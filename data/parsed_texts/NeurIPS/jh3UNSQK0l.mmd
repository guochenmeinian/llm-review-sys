# Finite-Time Analysis of Single-Timescale Actor-Critic

Xuyang Chen

National University of Singapore

chenxuyang@u.nus.edu &Lin Zhao

National University of Singapore

elezhli@nus.edu.sg

Corresponding author

###### Abstract

Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an \(\epsilon\)-approximate stationary point with \(\widetilde{\mathcal{O}}(\epsilon^{-2})\) sample complexity under standard assumptions, which can be further improved to \(\mathcal{O}(\epsilon^{-2})\) under the i.i.d. sampling. Our novel framework systematically evaluates and controls the error propagation between the actor and critic. It offers a promising approach for analyzing other single-timescale reinforcement learning algorithms as well.

## 1 Introduction

Actor-critic (AC) methods have achieved great success in solving many challenging reinforcement learning (RL) problems [17, 20, 24]. AC updates the actor (i.e., the policy) using the estimated policy gradient (PG), which is a function of the Q-value under the policy. Meanwhile, it employs a bootstrapping critic to estimate the Q-value, which often helps reduce variance and accelerates convergence in practice.

Despite the empirical success, the non-asymptotic convergence analysis of AC in the most practical single-timescale form remains underexplored. A large body of existing works consider the double-loop variants, where the critic runs many steps to accurately estimate the Q-value for a given actor [37, 16, 27, 2]. This leads to a decoupled convergence analysis of the critic and the actor, which involves a policy evaluation sub-problem in the inner loop and a perturbed gradient descent in the outer loop. Its finite-time convergence is relatively well understood [16, 37, 35]. Nevertheless, the double-loop setting is mainly for ease of analysis, which is barely adopted in practice. Since it requires an accurate critic estimation, it is typically sample inefficient. In fact, it is unclear whether an inner loop of accurate policy evaluation is really necessary since it only corresponds to one transient policy among many iterations.

Another body of works considers the (single-loop) two-timescale variants [31, 9, 36, 13], where the actor and the critic are updated simultaneously in each iteration with stepsizes of different timescales. The actor stepsize is typically smaller than that of the critic, with their ratio converging to zero as the iteration number goes to infinity. Hence, the actor is updated much slower than the critic. The two-timescale allows the critic to approximate the desired Q-value in an asymptotic way, which enables a decoupled convergence analysis of the actor and the critic. This variant is occasionallyadopted to improve learning stability. However, it is still considered inefficient as the actor update is artificially slowed down.

In this paper, we consider the more practical single-timescale AC algorithm, which is the one introduced in many works of literature as well as in [25] as a classic AC algorithm. In single-timescale AC, the stepsizes of the critic and the actor diminish at the same timescale. Unlike the aforementioned variants, which have specialized designs aimed at simplifying the convergence analysis, the analysis of the single-timescale AC presents a greater challenge. Due to the substantial errors in critic estimation and the close coupling between the parallel critic update and actor update, the algorithm is more prone to unstable error propagation. It remains unclear under what condition the errors will converge to zero. To study its finite-time convergence, we consider the challenging undiscounted time-average reward formulation [25; 31; 37], which consists of three parallel updates: the (time-average) reward estimator, the critic estimator, and the actor estimator. We keep track of the reward estimation error, the critic error, and the policy gradient norm (which measures the actor error) by deriving an implicit bound for each of them. They are then analyzed altogether as an interconnected system inspired by [21] to establish the convergence simultaneously. Specifically, we identify the (constant) ratio between the actor stepsize and the critic stepsize, below which all three errors will diminish to zero, despite the inaccurate estimation in all three updates (reward estimation, critic, actor).

### Main Contributions

We summarise our main contributions as follows:

\(\bullet\) We provide a finite-time analysis for the single-timescale AC under the Markovian sampling and prove an \(\widetilde{\mathcal{O}}(\epsilon^{-2})\) sample complexity, where \(\widetilde{\mathcal{O}}(\cdot)\) hides additional logarithmic terms. We further show that this sample complexity can be improved to \(\mathcal{O}(\epsilon^{-2})\) under i.i.d. sampling, which matches the state-of-the-art performance of SGD on general non-convex optimization problems. Our proof clearly shows that the additional logarithmic term under the Markovian sampling is introduced by the mixing time of the underlying Markov chain.

\(\bullet\) Our result compares favorably to existing works on single-timescale AC. To our knowledge, the only other results of single-timescale AC in the general MDP (Markov decision process) case are from [8] and [21], both of which obtain a sample complexity of \(O(\epsilon^{-2})\) under discounted reward setting. However, both [8] and [21] considered the i.i.d. sampling, where the transition tuples are independently sampled from stationary distribution and discounted state-action visitation distribution. In this paper, we consider the more practical Markovian sampling, where the transition tuples are generated from a single trajectory (see Table 1).

Furthermore, [8] follows an explicit Lyapunov analysis, where they leave a biased term in the critic and eliminated in the actor. Therefore, their proof framework cannot show the convergence of the critic. We instead give a neat proof framework to guarantee convergence for both the critic and the actor. Additionally, [21] only considered the tabular case (finite state-action space) where we allow the state space to be infinite (see Table 1). It is worth emphasizing that moving from a finite state space to an infinite state space takes significantly non-trivial effort in analysis. The analysis in [21] concatenates all state-action pairs to create a finite-dimensional feature matrix, which however becomes impossible in the infinite state space scenario. Consequently, their analysis technique and established results are not applicable in our context.

\(\bullet\) Technically, we develop a new analysis framework that can establish the finite-time convergence for single-timescale AC under the general setting. The existing analysis for double-loop AC [37] and two-timescale AC [31] hinge on decoupling the analysis of actor and critic, which typically establishes the convergence of critic first and then actor [37; 31; 8]. We instead investigate the evolution of

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Reference} & \multicolumn{2}{c|}{Setting} & \multicolumn{2}{c|}{Sampling} & \multirow{2}{*}{Sample Complexity} \\ \cline{2-2} \cline{4-6}  & State Space & Reward & Actor & & \multicolumn{1}{c|}{Critic} \\ \hline
[21] & Finite & Discounted & i.i.d. & i.i.d. & \(\mathcal{O}(\epsilon^{-2})\) \\ \hline
[8] & Infinite & Discounted & i.i.d. & i.i.d. & \(\mathcal{O}(\epsilon^{-2})\) \\ \hline This Paper & Infinite & Average & Markovian & Markovian & \(\mathcal{O}(\epsilon^{-2})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with related single-timescale actor-critic algorithmsthe coupled estimation errors of the time-average reward, the critic, and the policy gradient norm altogether as an interconnected system in a much less conservative way. We emphasize that our analysis framework includes the discounted setting as a simple special case where the interconnected system is only two-dimensional without the time-average reward estimation.

### Related Work

**Policy gradient methods.** Policy gradient methods [26; 25] learn a parameterized policy, constituting a departure from the value-based approach [28; 32; 40]. The asymptotic convergence of policy gradient methods has been well established in [29; 26; 3; 14] via stochastic approximation methods [6]. Some recent works have shown that PG methods can find the global optimum of some particular class of problems, such as LQR [11; 19] and tabular case problem [1]. Under general function approximation setting, finite-time convergence of PG methods was analyzed in [1; 38; 33; 34]. Specifically, [1] established the finite-time convergence of PG methods under both tabular policy parameterizations and general parametric policy classes. [38] showed that a variant of PG methods can attain an \(\epsilon\)-accurate stationary point at a sample complexity of \(\mathcal{O}(\epsilon^{-2})\), where they adopted Monte-Carlo sampling to find an unbiased estimation of policy gradient. [33; 34] studied the variance reduction PG and acceleration PG.

**Actor-Critic methods.** The AC algorithm was initially proposed by [15]. Later, [14] extended it to the natural AC algorithm. The asymptotic convergence of AC algorithms has been well established in [14; 5; 7; 39] under various settings. Many recent works focused on the finite-time convergence of AC methods. Under the double-loop setting, [37] established the global convergence of AC methods for solving linear quadratic regulator (LQR). [27] studied the global convergence of AC methods with both the actor and the critic parameterized by neural networks. [16] studied the finite-time local convergence of a few AC variants with linear function approximation.

Under the two-timescale AC setting, [31] established the finite-time local convergence to a stationary point at a sample complexity of \(\widetilde{\mathcal{O}}(\epsilon^{-2.5})\) under the undiscounted time-average reward setting. [36] studied both local convergence and global convergence for two-timescale (natural) AC, with \(\widetilde{\mathcal{O}}(\epsilon^{-2.5})\) and \(\widetilde{\mathcal{O}}(\epsilon^{-4})\) sample complexity, respectively, under the discounted accumulated reward. The algorithm collects multiple samples to update the critic. [13] proposed a two-timescale stochastic approximation algorithm for bilevel optimization and the algorithm was subsequently employed in the context of two-timescale AC. [9] established the global convergence of two-timescale AC methods for solving LQR, where only a single sample is used to update the critic in each iteration.

Under the single-timescale setting, [12] considered the least-squares temporal difference (LSTD) update for the critic and obtained the optimal policy within the energy-based policy class for both linear function approximation and nonlinear function approximation using neural networks. [41] studied single-timescale AC on LQR. In addition, [8] and [21] considered the single-timescale AC in general MDP cases, which have been reviewed and compared in Section 1.1.

**Notation.** We use non-bold letters to denote scalars and use lower and upper case bold letters to denote vectors and matrices respectively. Without further specification, we write \(x_{n}=\mathcal{O}(y_{n})\) if there exists an absolute positive constant \(C\) such that \(x_{n}\leq Cy_{n}\), for two sequences \(\{x_{n}\}\) and \(\{y_{n}\}\). We use \(\widetilde{\mathcal{O}}(\cdot)\) to hide logarithm factors. The total variation distance of two probability measure \(\mu\) and \(v\) is defined by \(d_{TV}(\mu,v):=\frac{1}{2}\int_{\mathcal{X}}|\mu(dx)-v(dx)|\). In addition, we use \(\mathbb{P}\) to denote a generic probability of some random event.

## 2 Preliminaries

In this section, we review the basics of the Markov decision process, policy gradient algorithm, and single-timescale AC with linear function approximation.

### Markov decision process

We consider the standard Markov Decision Process (MDP) characterized by \((\mathcal{S},\mathcal{A},\mathcal{P},r)\), where \(\mathcal{S}\) is the state space and \(\mathcal{A}\) is the action space. We consider a finite action space \(|\mathcal{A}|<\infty\), whereas the state space can be either a finite set or an (unbounded) real vector space \(\mathcal{S}\subset\mathbb{R}^{n}\). \(\mathcal{P}(s_{t+1}|s_{t},a_{t})\in[0,1]\) denotes the transition kernel. We consider a bounded reward \(r:\mathcal{S}\times\mathcal{A}\rightarrow[-U_{r},U_{r}]\), which is a function of the state \(s\) and action \(a\). A policy \(\pi_{\mathbf{\theta}}(\cdot|s)\in\mathbb{R}^{|\mathcal{A}|}\) parameterized by \(\mathbf{\theta}\) is defined as a mapping from a given state to a probability distribution over actions.

The RL problem of consideration aims to find a policy \(\pi_{\mathbf{\theta}}\) that maximizes the infinite-horizon time-average reward [26; 25; 37; 31], which is given by

\[J(\mathbf{\theta}):=\lim_{T\rightarrow\infty}\mathbb{E}_{\mathbf{\theta}}\frac{\sum_{t= 0}^{T-1}r(s_{t},a_{t})}{T}=\mathop{\mathbb{E}}_{s\sim\mu_{\mathbf{\theta}},a\sim \pi_{\mathbf{\theta}}}[r(s,a)],\]

where the expectation \(\mathbb{E}_{\mathbf{\theta}}\) is over the Markov chain under the policy \(\pi_{\mathbf{\theta}}\), and \(\mu_{\mathbf{\theta}}\) denotes the stationary state distribution induced by \(\pi_{\mathbf{\theta}}\). The existence of the stationary distribution can be guaranteed by the uniform ergodicity of the underlying MDP, which is a common assumption. Hereafter, we refer to \(J(\mathbf{\theta})\) as the time-average reward (or exchangeably, performance function), which can be evaluated by the expected reward over the stationary distribution \(\mu_{\mathbf{\theta}}\) and the policy \(\pi_{\mathbf{\theta}}\).

The state-value function is used to evaluate the overall rewards starting from a state \(s\) and following policy \(\pi_{\mathbf{\theta}}\) thereafter, which is defined as

\[V_{\mathbf{\theta}}(s):=\mathbb{E}_{\mathbf{\theta}}[\sum_{t=0}^{\infty}(r(s_{t},a_{t} )-J(\mathbf{\theta}))|s_{0}=s],\]

where the action follows the policy \(a_{t}\sim\pi_{\mathbf{\theta}}(\cdot|s_{t})\) and the next state comes from the transition kernel \(s_{t+1}\sim\mathcal{P}(\cdot|s_{t},a_{t})\). Similarly, we define the action-value (Q-value) function to evaluate the overall rewards starting from \(s\), taking action \(a\), and following policy \(\pi_{\mathbf{\theta}}\) thereafter:

\[Q_{\mathbf{\theta}}(s,a) =\mathbb{E}_{\mathbf{\theta}}[\sum_{t=0}^{\infty}(r(s_{t},a_{t})-J( \mathbf{\theta}))|s_{0}=s,a_{0}=a]\] \[\stackrel{{\text{(i)}}}{{=}}r(s,a)-J(\mathbf{\theta})+ \mathbb{E}[V_{\mathbf{\theta}}(s^{\prime})],\]

where the expectation in (i) is taken over \(s^{\prime}\sim\mathcal{P}(\cdot|s,a)\).

### Policy gradient theorem

The policy gradient theorem [26] provides an analytic expression for the gradient of the performance function \(J(\mathbf{\theta})\) with respect to the policy parameter \(\mathbf{\theta}\), which is given by:

\[\nabla_{\mathbf{\theta}}J(\mathbf{\theta})=\mathbb{E}_{s\sim\mu_{\mathbf{\theta}},a\sim \pi_{\mathbf{\theta}}}[Q_{\mathbf{\theta}}(s,a)\nabla_{\mathbf{\theta}}\log\pi_{\mathbf{\theta }}(a|s)]. \tag{1}\]

Evaluating this gradient requires the Q-value corresponding to the current policy \(\pi_{\mathbf{\theta}}\). The REINFORCE [29] is a Monte Carlo-based episodic algorithm, which uses all the rewards collected along the sample trajectory (that is, the differential return) as an approximation to the true Q-value.

Note that for any function \(b:\mathcal{S}\rightarrow\mathbb{R}\) that is independent of the action, we have

\[\sum_{a\in\mathcal{A}}b(s)\nabla\pi_{\mathbf{\theta}}(a|s)=b(s)\nabla(\sum_{a\in \mathcal{A}}\pi_{\mathbf{\theta}}(a|s))=b(s)\nabla 1=0.\]

Therefore, the policy gradient theorem can be written equivalently as:

\[\nabla J(\mathbf{\theta})=\mathbb{E}_{s\sim\mu_{\mathbf{\theta}},a\sim\pi_{\mathbf{\theta }}}[(Q_{\mathbf{\theta}}(s,a)-b(s))\nabla_{\mathbf{\theta}}\log\pi_{\mathbf{\theta}}(a|s )],\]

where \(b(s)\) is called the _baseline_ function. A popular choice of _baseline_ is the state-value function, which leads to the following advantage-based policy gradient

\[\nabla_{\mathbf{\theta}}J(\mathbf{\theta})=\mathbb{E}_{s\sim\mu_{\mathbf{\theta}},a\sim\pi _{\mathbf{\theta}}}[\Delta_{\mathbf{\theta}}(s,a)\nabla_{\mathbf{\theta}}\log\pi_{\mathbf{ \theta}}(a|s)],\]

where \(\Delta_{\mathbf{\theta}}=Q_{\mathbf{\theta}}(s,a)-V_{\mathbf{\theta}}(s)\) is known as the advantage function. This is the "REINFORCE with baseline" [29]. The baseline function can help reduce variance. However, like all Monte Carlo-based methods, it can still suffer from high variance and thus learns slowly. In addition, it is inconvenient to implement the algorithm online for continuing tasks [25].

AC algorithm instead employs a bootstrapping critic to estimate the Q-value. We describe the classic single-timescale AC in the next subsection.

### The single-timescale actor-critic algorithm

We consider the classic single-sample single-timescale AC method, where the critic is bootstrapping and uses a single sampled reward to update in each iteration. This directly accommodates online learning for continuing tasks. We consider the following linear function approximation of the state-value function:

\[\widehat{V}_{\mathbf{\theta}}(s;\mathbf{\omega})=\mathbf{\phi}(s)^{\top}\mathbf{\omega},\]

where \(\mathbf{\phi}(\cdot):\mathcal{S}\rightarrow\mathbb{R}^{d}\) is a known feature mapping, which satisfies \(\|\mathbf{\phi}(\cdot)\|\leq 1\). To drive \(\widehat{V}_{\mathbf{\theta}}(s;w)\) towards its true value \(V_{\mathbf{\theta}}(s)\), the semi-gradient TD(0) update is applied to estimate the linear coefficient \(\mathbf{\omega}\) (hereafter referred to as the critic):

\[\mathbf{\omega}_{t+1}=\mathbf{\omega}_{t}+\beta_{t}[(r_{t}-J(\mathbf{\theta})+\mathbf{\phi}(s_{ t+1})^{\top}\mathbf{\omega}_{t}-\mathbf{\phi}(s_{t})^{\top}\mathbf{\omega}_{t})]\mathbf{\phi}(s_{t}), \tag{2}\]

where \(\beta_{t}\) is the step size of the critic \(\mathbf{\omega}\) and \(r_{t}:=r(s_{t},a_{t})\). Since \(J(\mathbf{\theta})\) is unknown, the time-average reward setting introduces an additional estimator \(\eta\) to estimate it. Hereafter, we simply refer to \(\eta\) as the reward estimator. The temporal difference error can be defined as

\[\delta_{t}:=r_{t}-\eta_{t}+\mathbf{\phi}(s_{t+1})^{\top}\mathbf{\omega}_{t}-\mathbf{\phi}( s_{t})^{\top}\mathbf{\omega}_{t}.\]

Then, the update rule for the critic is given by

\[\eta_{t+1} =\eta_{t}+\gamma_{t}(r_{t}-\eta_{t}),\] \[\mathbf{\omega}_{t+1} =\mathbf{\omega}_{t}+\beta_{t}\delta_{t}\mathbf{\phi}(s_{t}),\]

where \(\gamma_{t}\) is the step size of the reward estimator \(\eta_{t}\).

Since \(\delta_{t}\) is an approximation of the advantage function, similar to REINFORCE with baseline, the corresponding update rule for the actor can be written as:

\[\mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}+\alpha_{t}\delta_{t}\nabla_{\mathbf{\theta}}\log \pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t}),\]

where \(\alpha_{t}\) is the actor stepsize. The above-described AC is summarized in Algorithm 1, which is introduced in [25] as a classic online one-step AC algorithm. Algorithm 1 can be efficiently implemented for continuing tasks due to its online nature.

```
1:Input initial actor parameter \(\mathbf{\theta}_{0}\), initial critic parameter \(\mathbf{\omega}_{0}\), initial reward estimator \(\eta_{0}\), stepsize \(\alpha_{t}\) for actor, \(\beta_{t}\) for critic, and \(\gamma_{t}\) for reward estimator.
2:Draw \(s_{0}\) from some initial distribution
3:for\(t=0,1,2,\cdots,T-1\)do
4: Take action \(a_{t}\sim\pi_{\mathbf{\theta}_{t}}(\cdot|s_{t})\)
5: Observe next state \(s_{t+1}\sim\mathcal{T}(\cdot|s_{t},a_{t})\) and reward \(r_{t}=r(s_{t},a_{t})\)
6:\(\delta_{t}=r_{t}-\eta_{t}+\mathbf{\phi}(s_{t+1})^{\top}\mathbf{\omega}_{t}-\mathbf{\phi}(s_ {t})^{\top}\mathbf{\omega}_{t}\)
7:\(\eta_{t+1}=\eta_{t}+\gamma_{t}(r_{t}-\eta_{t})\)
8:\(\mathbf{\omega}_{t+1}=\Pi_{U_{\mathbf{\omega}}}(\mathbf{\omega}_{t}+\beta_{t}\delta_{t}\mathbf{ \phi}(s_{t}))\)
9:\(\mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}+\alpha_{t}\delta_{t}\nabla_{\mathbf{\theta}}\log \pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\)
10:endfor
```

**Algorithm 1** Single-timescale Actor-Critic

Note that the "single-timescale" refers to the fact that the stepsizes \(\alpha_{t},\beta_{t},\gamma_{t}\) are only constantly proportional to each other. In addition, this is a "single-sample" algorithm, since only one sample is needed for the update in each iteration. We remark that Algorithm 1 is more common in practice than double loop variants. In Line 8 of Algorithm 1, a projection (\(\Pi_{U_{\mathbf{\omega}}}\)) is introduced to keep the critic norm-bounded by \(U_{\mathbf{\omega}}\), which is widely adopted in the literature [31; 37; 36; 8] for analysis. Note that the projection can be handled easily, which is relaxed using its non-expansive property in our analysis.

## 3 Main Results

We first present several standard assumptions that are common in the literature of analyzing AC with linear function approximation [12; 35; 31; 8; 21]. Insights into these conditions and connections with relevant works are also discussed.

### Assumptions

By taking the expectation of \(\mathbf{\omega}_{t+1}\) in (2) with respect to the stationary distribution, we have for any given \(\mathbf{\omega}_{t}\)

\[\mathbb{E}_{\mathbf{\theta}}[\mathbf{\omega}_{t+1}|\mathbf{\omega}_{t}]=\mathbf{ \omega}_{t}+\beta_{t}(\mathbf{b_{\theta}}+\mathbf{A_{\theta}}\mathbf{\omega}_{t}), \tag{3}\]

where

\[\mathbf{A_{\theta}} :=\mathbb{E}_{(s,a,s^{\prime})}[\mathbf{\phi}(s)(\mathbf{\phi}(s^{\prime} )-\mathbf{\phi}(s))^{\top})], \tag{4}\] \[\mathbf{b_{\theta}} :=\mathbb{E}_{(s,a)}[(r(s,a)-J(\mathbf{\theta}))\mathbf{\phi}(s)], \tag{5}\]

and \(s\sim\mu_{\mathbf{\theta}}(\cdot),a\sim\pi_{\mathbf{\theta}}(\cdot|s),s^{\prime}\sim \mathcal{P}(\cdot|s,a)\) is the subsequent state of the \((s,a)\). It can be easily shown that [25] the TD limiting point \(\mathbf{\omega}^{*}(\mathbf{\theta})\) satisfies:

\[\mathbf{b_{\theta}}+\mathbf{A_{\theta}}\mathbf{\omega}^{*}(\mathbf{\theta})=0. \tag{6}\]

Note that \(\mathbf{A_{\theta}}\) reflects the exploration of the policy. To see this, note that without sufficient exploration, \(\mathbf{A_{\theta}}\) can be rank deficient and (6) can be unsolvable. Consequently, the critic update (2) will not converge. Hence, the following assumption is made to guarantee the problem's solvability.

**Assumption 3.1** (Exploration).: For any \(\mathbf{\theta}\), the matrix \(\mathbf{A_{\theta}}\) defined in (4) is negative definite and its maximum eigenvalue can be upper bounded by \(-\lambda\).

Assumption 3.1 is commonly adopted in analyzing TD learning with linear function approximation [4; 42; 31; 23; 8; 21]. In particular, Assumption 3.1 holds if the policy \(\pi_{\mathbf{\theta}}\) can explore all state-action pairs in the tabular case [21]. In addition, with this assumption, we can choose \(U_{\mathbf{\omega}}=\frac{2U_{r}}{\lambda}\) so that all \(\mathbf{\omega}^{*}\) lie within the projection radius \(U_{\mathbf{\omega}}\) because \(\|\mathbf{b_{\theta}}\|\leq 2U_{r}\) and \(\|\mathbf{A_{\theta}}^{-1}\|\leq\lambda^{-1}\), which justifies the projection operator introduced in Line 8 of Algorithm 1.

**Assumption 3.2** (Uniform ergodicity).: For any \(\mathbf{\theta}\), denote \(\mu_{\mathbf{\theta}}(\cdot)\) as the stationary distribution induced by the policy \(\pi_{\mathbf{\theta}}(\cdot|s)\) and the transition probability measure \(\mathcal{P}(\cdot|s,a)\). For a Markov chain generated by the policy \(\pi_{\mathbf{\theta}}\) and transition kernel \(\mathcal{P}\), there exists \(m>0\) and \(\rho\in(0,1)\) such that

\[d_{TV}(\mathbb{P}(s_{\tau}\in\cdot|s_{0}=s),\mu_{\mathbf{\theta}}( \cdot))\leq m\rho^{\tau},\forall\tau\geq 0,\forall s\in\mathcal{S}.\]

Assumption 3.2 assumes the Markov chain is geometrically mixing, which can be implied by the uniform ergodicity. It is commonly employed to characterize the noise induced by Markovian sampling in RL algorithms [4; 42; 31; 8; 21].

**Assumption 3.3** (Lipschitz continuity of policy).: Let \(\pi_{\mathbf{\theta}}(a|s)\) be a policy parameterized by \(\mathbf{\theta}\in\mathbb{R}^{d}\). There exists positive constants \(B,L_{l}\) and \(L_{\pi}\) such that for any \(\mathbf{\theta},\mathbf{\theta}_{1},\mathbf{\theta}_{2}\in\mathbb{R}^{d}\), \(s\in\mathcal{S}\), and \(a\in\mathcal{A}\), it holds that:

* \(\|\nabla\log\pi_{\mathbf{\theta}}(a|s)\|\leq B\)
* \(\|\nabla\log\pi_{\mathbf{\theta}_{1}}(a|s)-\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\| \leq L_{l}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|\)
* \(|\pi_{\mathbf{\theta}_{1}}(a|s)-\pi_{\mathbf{\theta}_{2}}(a|s)|\leq L_{\pi}\|\mathbf{ \theta}_{1}-\mathbf{\theta}_{2}\|\)

Assumption 3.3 is standard in the literature of policy gradient methods [22; 42; 38; 34; 31; 8; 21]. This assumption holds for many policy classes such as Gaussian policy [10], Boltzmann policy [15], and tabular softmax policy [1].

**Assumption 3.4**.: For any \(\mathbf{\theta},\mathbf{\theta}^{\prime}\in\mathbb{R}^{d}\), there exists constant \(L_{\mu}\) such that \(\|\nabla\mu_{\mathbf{\theta}}-\nabla\mu_{\mathbf{\theta}^{\prime}}\|\leq L_{\mu}\|\mathbf{ \theta}-\mathbf{\theta}^{\prime}\|\), where \(\mu_{\mathbf{\theta}}(s)\) is the stationary distribution under the policy \(\pi_{\mathbf{\theta}}\).

Assumption 3.4 is introduced in [8] to show the smoothness of the optimal critic \(\mathbf{\omega}^{*}(\mathbf{\theta})\), which is critical to guarantee the convergence of single-timescale AC. This assumption holds for the finite state-action space setting [18].

### Finite-Time Analysis

We define the following uniform upper bound for the linear function approximation error of the critic:

\[\epsilon_{\mathrm{app}}:=\sup_{\mathbf{\theta}}\sqrt{\mathbb{E}_{s\sim \mu_{\mathbf{\theta}}}(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{*}(\mathbf{\theta})-V_{\mathbf{ \theta}}(s))^{2}}.\]The error \(\epsilon_{\mathrm{app}}\) is zero if \(V_{\mathbf{\theta}}\) is indeed a linear function for any \(\mathbf{\theta}\). Naturally, it can be expected that the learning errors of Algorithm 1 depend on \(\epsilon_{\mathrm{app}}\).

We define the following integer \(\tau_{T}\) that will be useful in the statement of the theorems, which depends on the number of total iterations \(T\):

\[\tau_{T}:=\min\{i\geq 0\mid m\rho^{i-1}\leq\frac{1}{\sqrt{T}}\},\]

where \(m,\rho\) are constants defined in Assumption 3.2. Therefore, we choose \(\tau_{T}=\frac{\log m\rho^{-1}}{\log\rho^{-1}}+\frac{\log T}{2\log\rho^{-1}}= \mathcal{O}(\log T)\) such that \(m\rho^{\tau_{T}-1}\leq\frac{1}{\sqrt{T}}\). The integer \(\tau_{T}\) represents the mixing time of an ergodic Markov chain, which will be used to control the Markovian noise in the analysis.

We quantify the learning errors by defining \(y_{t}:=\eta_{t}-J(\mathbf{\theta}_{t})\), which is the difference between the reward estimator and the true time-average reward \(J(\mathbf{\theta}_{t})\) at time \(t\). For the critic, we define, \(\mathbf{z}_{t}:=\mathbf{\omega}_{t}-\mathbf{\omega}_{t}^{*}\) with \(\mathbf{\omega}_{t}^{*}:=\mathbf{\omega}^{*}(\mathbf{\theta}_{t})\) to measure the error between the critic and its target value at iteration \(t\). The following two theorems summarize our main results.

**Theorem 3.5** (Markovian sampling).: _Consider Algorithm 1 with \(\alpha_{t}=\alpha=\frac{c}{\sqrt{T}},\beta_{t}=\beta=\frac{1}{\sqrt{T}},\gamma_ {t}=\gamma=\frac{1}{\sqrt{T}}\), where \(c\) is a constant depending on problem parameters. Suppose Assumptions 3.1-3.4 hold, we have for \(T\geq 2\tau_{T}\),_

\[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2} =\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}}),\] \[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t} \|^{2} =\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}}),\] \[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\nabla J( \mathbf{\theta}_{t})\|^{2} =\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}}).\]

We defer the interpretation of the above results a bit and present below the analysis results under the i.i.d. sampling first for better comparison. The major difference of i.i.d. from Markovian sampling is that at the \(t\)-th iteration, the state \(s_{t}\) is sampled from the stationary distribution \(\mu_{\mathbf{\theta}_{t}}\) instead of the evolving Markov chain (see Algorithm 2 in Appendix E). The i.i.d. sampling simplifies the analysis in the way that many Markovian noise terms reduce to zero effectively. This leads to a tighter sample complexity bound compared to the Markovian sampling by up to logarithmic factors.

**Theorem 3.6** (i.i.d. sampling).: _Consider Algorithm 2 (see Appendix E) with \(\alpha_{t}=\alpha=\frac{c}{\sqrt{T}},\beta_{t}=\beta=\frac{1}{\sqrt{T}},\gamma _{t}=\gamma=\frac{1}{\sqrt{T}}\), where \(c\) is a constant depending on problem parameters. Suppose Assumptions 3.1-3.4 hold, we have for \(T\geq 2\tau_{T}\),_

\[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2} =\mathcal{O}(\frac{1}{\sqrt{T}})+\mathcal{O}(\epsilon_{\mathrm{ app}}),\] \[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t} \|^{2} =\mathcal{O}(\frac{1}{\sqrt{T}})+\mathcal{O}(\epsilon_{\mathrm{ app}}),\] \[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\nabla J (\mathbf{\theta}_{t})\|^{2} =\mathcal{O}(\frac{1}{\sqrt{T}})+\mathcal{O}(\epsilon_{\mathrm{ app}}).\]

If the critic approximation error \(\epsilon_{\mathrm{app}}\) is zero, we see that the reward estimator, the critic, and the actor estimation errors all diminish at a sub-linear rate of \(\widetilde{\mathcal{O}}(T^{-\frac{1}{2}})\). The additional logarithmic term hidden by \(\widetilde{\mathcal{O}}(\cdot)\) is incurred by the mixing time of the Markov chain, which can be get rid of under the i.i.d. sampling. It also hides the polynomials of all other problem parameters. They are explicitly characterized in the proofs up to the last step of analyzing the overall interconnected error propagation system. One can easily keep and get the dependence orders of all parameters if needed. Here we focus on the dependence of the iteration number for ease of presentation.

To put the results into perspective, note that \(\mathcal{O}(T^{-\frac{1}{2}})\) is the rate one would obtain from stochastic gradient descent (SGD) on general non-convex functions with unbiased gradient updates. In terms of sample complexity, to obtain an \(\epsilon\)-approximate stationary point, it takes a number of \(\widetilde{\mathcal{O}}(\epsilon^{-2})\) samples for Markovian sampling (Algorithm 1) and \(\mathcal{O}(\epsilon^{-2})\) for i.i.d. sampling (Algorithm 2), which matches the state-of-the-art performance of SGD on non-convex optimization problems.

The obtained sample complexities are superior to those of other AC variants. Notably, [16] provided finite-time convergence for double-loop variant with a \(\mathcal{O}(\epsilon^{-4})\) sample complexity and [31] analysed two-timescale variant, yielding a \(\widetilde{\mathcal{O}}(\epsilon^{-2.5})\) sample complexity. The sample complexity gap is intrinsic to their inefficient usage of data. In the double-loop setting, the critic starts over to estimate the Q-value for an intermediate policy in the inner loop, ignoring the fact that the consecutive Q-values can be similar given a relatively minor policy update. The two-timescale setting artificially slows down the actor update by adopting an actor stepsize that decays faster than the critic. The single-timescale approach updates the critic and actor parallelly with proportional stepsizes and thus learns more efficiently.

Moreover, our result matches the \(\mathcal{O}(\epsilon^{-2})\) sample complexity of policy gradient methods such as REINFORCE [2; 22] under the i.i.d sampling. It is previously found in [31] that there is a sample complexity gap between Algorithm 1 adopting two-timescale stepsizes and (variance-reduced) REINFORCE [22]. In this paper, we close this gap by providing a single-timescale analysis for Algorithm 1 which shows that this practical single-timescale AC can have the same sample complexity as REINFORCE.

### Proof Sketch

The main challenge in the finite-time analysis lies in that the estimation errors of the time-average reward, the critic, and the policy gradient are strongly coupled. To overcome this difficulty, we view the propagation of these errors as an interconnected system and analyze them holistically. To better appreciate the advantage of our analysis framework over the decoupled methods that are traditionally adopted in analyzing double-loop and two-timescale variants, we sketch the main proof steps of Theorem 3.5 in the following. We also highlight the key challenges and techniques developed correspondingly. All supporting lemmas mentioned below can be found in Appendix.

We first derive implicit (coupled) upper bounds for the reward estimation error \(y_{t}\), the critic error \(\mathbf{z}_{t}\), and the policy gradient \(\nabla J(\mathbf{\theta}_{t})\), respectively. Then, we solve a system of inequalities to establish finite-time convergence.

**Step 1: Reward estimation error analysis.** Using the reward estimator update rule (Line 7 of Algorithm 1), we decompose the reward estimation error into:

\[\begin{split} y_{t+1}^{2}&=(1-2\gamma_{t})y_{t}^{2 }+2\gamma_{t}y_{t}(r_{t}-J(\mathbf{\theta}_{t}))\\ &\quad+2y_{t}(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1}))+(J(\mathbf{ \theta}_{t})-J(\mathbf{\theta}_{t+1})+\gamma_{t}(r_{t}-\eta_{t}))^{2}.\end{split} \tag{7}\]

The second term on the right-hand side of (7) is a bias term caused by the Markovian sample, which is characterized in Lemma C.1. As shown in Lemma E.1, this bias reduces to 0 under i.i.d. sampling after taking the expectation. The third term captures the variation of the moving targets \(J(\mathbf{\theta}_{t})\). The double-loop variant of AC runs a policy evaluation sub-problem in the inner loop for each target \(J(\mathbf{\theta}_{t})\) to estimate the policy gradient accurately. This easily ensures the monotonic decreasing of \(J(\mathbf{\theta}_{t})\) and consequently the convergence. The two-timescale variant utilizes the additional property of \(\lim_{t\to\infty}\alpha_{t}/\beta_{t}=0\) to annihilate this term and consequently can have a decoupled analysis. In the case of single-timescale AC, we do not have the aforementioned special algorithm designs and properties to ease the analysis. Instead, we utilize the smoothness of \(J(\mathbf{\theta})\) (see Lemma B.2) and derive an implicit upper bound for this term as a function of the norm of \(y_{t}\) and \(\nabla J(\mathbf{\theta}_{t})\). This bound will be combined with the implicit bounds derived in Step 2 and Step 3 below to establish the non-asymptotic convergence altogether. The last term in (7) reflects the variance in reward estimation, which is bounded by \(\widetilde{\mathcal{O}}(\gamma_{t})\).

**Step 2: Critic error analysis.** Using the critic update rule (Line 8 of Algorithm 1), we decompose the squared error by (we neglect the projection for the time being for the ease of comprehension. The complete analysis can be found in the appendix.)

\[\begin{split}\|\mathbf{z}_{t+1}\|^{2}=&\|\mathbf{z}_{t}\|^ {2}+2\beta_{t}\langle\mathbf{z}_{t},\bar{g}(\mathbf{\omega}_{t},\mathbf{\theta}_{t}) \rangle+2\beta_{t}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2\beta_{t} \langle\mathbf{z}_{t},\Delta g(O_{t},\eta_{t},\mathbf{\theta}_{t})\rangle\\ &\quad+2\langle\mathbf{z}_{t},\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^ {*}\rangle+\|\beta_{t}(g(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+\Delta g(O_{t },\eta_{t},\mathbf{\theta}_{t}))+\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*}\|^{2}, \end{split} \tag{8}\]where \(O_{t}:=(s_{t},a_{t},s_{t+1})\) is a tuple of observations and the definitions of \(g,\bar{g},\Delta g\), and \(\Psi\) can be found in (12) and (13) in Appendix A. Without diving into the detailed definitions, here we focus on illustrating the high-level insights of our proof. First of all, the second term on the right-hand side of (8) can be bounded by \(-2\lambda\beta_{t}\|\mathbf{z}_{t}\|^{2}\) under Assumption 3.1. It provides an explicit characterization of how sufficient exploration can help the convergence of learning. The third term is a Markovian noise, which is further bounded implicitly in Lemma C.3. For the i.i.d sampling case, as shown in Lemma E.1, this bias reduces to 0 after taking the expectation. The fourth term is caused by inaccurate reward and critic estimations, which can be bounded by the norm of \(y_{t}\) and \(\mathbf{z}_{t}\). The fifth term tracks both the critic estimation performance \(\mathbf{z}_{t}\) and the difference between the drifting critic targets \(\mathbf{\omega}_{t}^{*}\). Similar to the case of Step 1, the double-loop approach bounds this term relying on the accurate policy evaluation sub-problem in the inner loop for each target \(\mathbf{\omega}_{t}^{*}\), whereas the two-timescale approach ensures its convergence by additionally requiring \(\lim_{t\rightarrow\infty}\alpha_{t}/\beta_{t}=0\). In contrast, we establish an implicit upper bound for this term as a function of \(y_{t}\) and \(\mathbf{z}_{t}\) by utilizing the smoothness of the optimal critic proved in Lemma B.4. Finally, the last term reflects the variances of various estimations, which is bounded by \(\mathcal{O}(\beta_{t})\).

**Step 3: Policy gradient norm analysis**. Using the actor update rule (Line 9 of Algorithm 1) and the smoothness property of \(J(\mathbf{\theta})\) (see Lemma B.2), we derive

\[\begin{split}\|\nabla J(\mathbf{\theta}_{t})\|^{2}&\leq \frac{1}{\alpha_{t}}(J(\mathbf{\theta}_{t+1})-J(\mathbf{\theta}_{t}))+\Theta(O_{t},\bm {\theta}_{t})-\langle\nabla J(\mathbf{\theta}_{t}),\Delta h(O_{t},\eta_{t},\mathbf{ \omega}_{t},\mathbf{\theta}_{t})\rangle\\ &\quad-\langle\nabla J(\mathbf{\theta}_{t}),\mathbb{E}_{O_{t}^{\prime} }[\Delta h^{\prime}(O_{t}^{\prime},\mathbf{\theta}_{t})]\rangle+\frac{L_{J^{ \prime}}}{2}\alpha_{t}\|\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t}) \|^{2},\end{split} \tag{9}\]

where \(O_{t}^{\prime}\) is a shorthand for an independent sample from stationary distribution \(s\sim\mu_{\mathbf{\theta}_{t}},a\sim\pi_{\mathbf{\theta}_{t}},s^{\prime}\sim\mathcal{P }(\cdot|s,a)\), \(\Theta\) is defined in (13), and \(L_{J^{\prime}}\) is a constant. The first term on the right-hand side of (9) compares the actor's performances between consecutive updates, which can be bounded via Abel summation by parts. The second term is a noise term introduced by Markovian sampling, which is characterized in Lemma C.6. Again, as proven in Lemma E.1, this bias reduces to 0 under i.i.d. sampling after taking the expectation. The third term is an error introduced by the inaccurate estimations of both the time-average reward and the critic. This term was directly bounded to zero under both the double-loop setting and the two-timescale setting due to their particular algorithm design, to enable a decoupled analysis. We control this term by providing an implicit bound depending on \(y_{t}\), \(\mathbf{z}_{t}\), and \(\nabla J(\mathbf{\theta}_{t})\). The fourth term comes from the linear function approximation error. The last term captures the variance of the stochastic gradient update, which is bounded by \(\mathcal{O}(\alpha_{t})\).

**Step 4: Interconnected iteration system analysis.** Taking the expectation of and summing (7), (8), and (9) from \(\tau_{T}\) to \(T-1\), respectively, we obtain the following system of inequalities in terms of \(Y_{T},\ Z_{T},\ G_{T}\):

\[\begin{split} Y_{T}:=\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1} \mathbb{E}y_{t}^{2}&\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+ l_{1}\sqrt{Y_{T}G_{T}},\\ Z_{T}:=\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z }_{t}\|^{2}&\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{ O}(\epsilon_{\mathrm{app}})+l_{2}\sqrt{Y_{T}Z_{T}}+l_{3}\sqrt{Z_{T}(2Y_{T}+8Z_{T})},\\ G_{T}:=\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\| \nabla J(\mathbf{\theta}_{t})\|^{2}&\leq\mathcal{O}(\frac{\log^{2}T}{ \sqrt{T}})+\mathcal{O}(\epsilon_{\mathrm{app}})+l_{4}\sqrt{G_{T}(2Y_{T}+8Z_{T} )},\end{split}\]

where \(l_{1},l_{2},l_{3},l_{4}\) are positive constants. By solving the above system of inequalities, we further prove that if

\[l_{1}(1+2l_{4}^{2}+8l_{4}^{2}(2l_{2}^{2}+l_{3}))\leq 1\quad\text{and}\quad 16l_{3} \leq 1,\]

then \(Y_{T},Z_{T},G_{T}\) converge at a rate of \(\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})\). This condition can be easily satisfied by choosing the stepsize ratio \(c\) to be smaller than a threshold identified in Equation (28). Thus, it completes the proof.

The above proof applies to i.i.d sampling straightforwardly, with the corresponding terms pointed out in the above steps reducing to 0 in the analysis. The additional proof can be found in Lemma E.1.

Conclusion and Discussion

In this paper, we establish the finite-time analysis for single-timescale AC with Markovian sampling. Our work compares favorably to existing works in terms of analyzing online learning and considering the continuous state space. We developed a series of lemmas that characterize the propagation of errors, and establish their convergence simultaneously by solving a system of nonlinear inequalities. The proposed framework is general and can be applied to analyze other single-timescale stochastic approximation algorithms. Our future work includes further considering the continuous action space problems and developing new proof techniques that require fewer assumptions.

## Acknowledgements

This work was supported by the Singapore Ministry of Education Tier 1 Academic Research Funds (A0009030-00-00, 22-5460-A0001). The authors would like to thank the timely help from Yue Wu and Quanquan Gu for clarifying the proof of their seminar work on finite-time analysis of two-timescale actor-critic.

## References

* Agarwal et al. [2020] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. In _Conference on Learning Theory_, pages 64-66. PMLR, 2020.
* Agarwal et al. [2021] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _Journal of Machine Learning Research_, 22(98):1-76, 2021.
* Baxter and Bartlett [2001] Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. _Journal of Artificial Intelligence Research_, 15:319-350, 2001.
* Bhandari et al. [2018] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In _Conference on learning theory_, pages 1691-1692. PMLR, 2018.
* Bhatnagar et al. [2009] Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic algorithms. _Automatica_, 45(11):2471-2482, 2009.
* Borkar [2009] Vivek S Borkar. _Stochastic approximation: a dynamical systems viewpoint_, volume 48. Springer, 2009.
* Castro and Meir [2010] Dotan Di Castro and Ron Meir. A convergent online single time scale actor critic algorithm. _The Journal of Machine Learning Research_, 11:367-410, 2010.
* Chen et al. [2021] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. _Advances in Neural Information Processing Systems_, 34:25294-25307, 2021.
* Chen et al. [2022] Xuyang Chen, Jingliang Duan, Yingbin Liang, and Lin Zhao. Global convergence of two-timescale actor-critic for solving linear quadratic regulator. _arXiv preprint arXiv:2208.08744_, 2022.
* Doya [2000] Kenji Doya. Reinforcement learning in continuous time and space. _Neural computation_, 12(1):219-245, 2000.
* Fazel et al. [2018] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In _International Conference on Machine Learning_, pages 1467-1476. PMLR, 2018.
* Fu et al. [2020] Zuyue Fu, Zhuoran Yang, and Zhaoran Wang. Single-timescale actor-critic provably finds globally optimal policy. _arXiv preprint arXiv:2008.00483_, 2020.

* [13] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. _SIAM Journal on Optimization_, 33(1):147-180, 2023.
* [14] Sham M Kakade. A natural policy gradient. _Advances in neural information processing systems_, 14, 2001.
* [15] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. _Advances in neural information processing systems_, 12, 1999.
* [16] Harshat Kumar, Alec Koppel, and Alejandro Ribeiro. On the sample complexity of actor-critic method for reinforcement learning with function approximation. _arXiv preprint arXiv:1910.08412_, 2019.
* [17] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _Nature_, 521(7553):436-444, 2015.
* [18] Qijun Luo and Xiao Li. Finite-time analysis of fully decentralized single-timescale actor-critic. _arXiv preprint arXiv:2206.05733_, 2022.
* [19] Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter Bartlett, and Martin Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2916-2925. PMLR, 2019.
* [20] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pages 1928-1937. PMLR, 2016.
* [21] Alex Olshevsky and Bahman Gharesifard. A small gain analysis of single timescale actor critic. _SIAM Journal on Control and Optimization_, 61(2):980-1007, 2023.
* [22] Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. In _International conference on machine learning_, pages 4026-4035. PMLR, 2018.
* [23] Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of actor-critic algorithm. _IEEE Journal on Selected Areas in Information Theory_, 2(2):652-664, 2021.
* [24] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* [25] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [26] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12, 1999.
* [27] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. _arXiv preprint arXiv:1909.01150_, 2019.
* [28] Christopher JCH Watkins and Peter Dayan. Q-learning. _Machine learning_, 8(3):279-292, 1992.
* [29] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8(3):229-256, 1992.
* [30] Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor critic methods. _arXiv preprint arXiv:2005.01350_, 2020.

* [31] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic methods. _Advances in Neural Information Processing Systems_, 33:17617-17628, 2020.
* [32] Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Finite-time analysis for double q-learning. In _Advances in Neural Information Processing Systems (NeurIPS), 2020 (spotlight)_, 2020.
* [33] Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive variance reduction. _arXiv preprint arXiv:1909.08610_, 2019.
* [34] Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-reduced policy gradient. In _Uncertainty in Artificial Intelligence_, pages 541-551. PMLR, 2020.
* [35] Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural) actor-critic algorithms. _Advances in Neural Information Processing Systems_, 33:4358-4369, 2020.
* [36] Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms. _arXiv preprint arXiv:2005.03557_, 2020.
* [37] Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably global convergence of actor-critic: A case for linear quadratic regulator with ergodic cost. _Advances in neural information processing systems_, 32, 2019.
* [38] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to (almost) locally optimal policies. _SIAM Journal on Control and Optimization_, 58(6):3586-3612, 2020.
* [39] Shangtong Zhang, Bo Liu, Hengshuai Yao, and Shimon Whiteson. Provably convergent two-timescale off-policy actor-critic with function approximation. In _International Conference on Machine Learning_, pages 11204-11213. PMLR, 2020.
* [40] Lin Zhao, Huaqing Xiong, and Yingbin Liang. Faster non-asymptotic convergence for double q-learning. In _Advances in Neural Information Processing Systems (NeurIPS), 2021_, 2021.
* [41] Mo Zhou and Jianfeng Lu. Single timescale actor-critic method to solve the linear quadratic regulator with convergence guarantees. _Journal of Machine Learning Research_, 24(222):1-34, 2023.
* [42] Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approximation. _Advances in neural information processing systems_, 32, 2019.

## Appendix A Notation

We make use of the following auxiliary Markov chain to deal with the Markovian noise.

**Auxiliary Markov Chain:**

\[s_{t-\tau}\xrightarrow{\mathbf{\theta}_{t-\tau}}a_{t-\tau}\xrightarrow{\mathcal{P}}s _{t-\tau+1}\xrightarrow{\mathbf{\theta}_{t-\tau}}\widetilde{a}_{t-\tau+1} \xrightarrow{\mathcal{P}}\widetilde{s}_{t-\tau+2}\xrightarrow{\mathbf{\theta}_{t- \tau}}\widetilde{a}_{t-\tau+2}\cdots\xrightarrow{\mathcal{P}}\widetilde{s}_{t} \xrightarrow{\mathbf{\theta}_{t-\tau}}\widetilde{a}_{t}\xrightarrow{\mathcal{P}} \widetilde{s}_{t+1}. \tag{10}\]

For reference, we also show the original Markov chain.

**Original Markov Chain:**

\[s_{t-\tau}\xrightarrow{\mathbf{\theta}_{t-\tau}}a_{t-\tau}\xrightarrow{\mathcal{P} }s_{t-\tau+1}\xrightarrow{\mathbf{\theta}_{t-\tau+1}}\widetilde{a}_{t-\tau+1} \xrightarrow{\mathcal{P}}\widetilde{s}_{t-\tau+2}\xrightarrow{\mathbf{\theta}_{t- \tau+2}}\widetilde{a}_{t-\tau+2}\cdots\xrightarrow{\mathcal{P}}\widetilde{s}_ {t}\xrightarrow{\mathbf{\theta}_{t}}\widetilde{a}_{t}\xrightarrow{\mathcal{P}} \widetilde{s}_{t+1}. \tag{11}\]

In the sequel, we denote by \(\widetilde{O}_{t}:=(\widetilde{s}_{t},\widetilde{a}_{t},\widetilde{s}_{t+1})\) the tuple generated from the auxiliary Markov chain in (10) while \(O_{t}:=(s_{t},a_{t},s_{t+1})\) denotes the tuple generated from the original Markov chain in (11).

We define the following functions, which will benefit to decompose the errors and simplify the presentation.

\[\Delta g(O,\eta,\mathbf{\theta}) :=[J(\mathbf{\theta})-\eta]\mathbf{\phi}(s), \tag{12}\] \[g(O,\mathbf{\omega},\mathbf{\theta}) :=[r(s,a)-J(\mathbf{\theta})+(\mathbf{\phi}(s^{\prime})-\mathbf{\phi}(s))^{ \top}\mathbf{\omega}]\mathbf{\phi}(s),\] \[\bar{g}(\mathbf{\omega},\mathbf{\theta}) :=\mathbb{E}_{(s,a,s^{\prime})\sim(\mathbf{\mu}_{\mathbf{\theta}},\mathbf{ \pi}_{\mathbf{\theta}},\mathcal{P})}[[r(s,a)-J(\mathbf{\theta})+(\mathbf{\phi}(s^{\prime})- \mathbf{\phi}(s))^{\top}\mathbf{\omega}]\mathbf{\phi}(s)],\] \[\Delta h(O,\eta,\mathbf{\omega},\mathbf{\theta}) :=(J(\mathbf{\theta})-\eta+(\mathbf{\phi}(s^{\prime})-\mathbf{\phi}(s))^{\top} (\mathbf{\omega}-\mathbf{\omega}^{*}(\mathbf{\theta}))\nabla\log\pi_{\mathbf{\theta}}(a|s),\] \[\Delta h^{\prime}(O,\mathbf{\theta}) :=((\mathbf{\phi}(s^{\prime})\mathbf{\omega}^{*}(\mathbf{\theta})-V_{\mathbf{ \theta}}(s^{\prime}))-(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{*}(\mathbf{\theta})-V_{\bm {\theta}}(s)))\nabla\log\pi_{\mathbf{\theta}}(a|s),\] \[h(O,\mathbf{\theta}) :=(r(s,a)-J(\mathbf{\theta})+\mathbf{\phi}(s^{\prime})^{\top}\mathbf{\omega}^ {*}(\mathbf{\theta})-\mathbf{\phi}(s^{\top}\mathbf{\omega}^{*}(\mathbf{\theta}))\nabla\log\pi_ {\mathbf{\theta}}(a|s).\]

We also define the following functions, which characterize the Markovian noise.

\[\Phi(O,\eta,\mathbf{\theta}) :=(\eta-J(\mathbf{\theta}))(r(s,a)-J(\mathbf{\theta})), \tag{13}\] \[\Psi(O,\mathbf{\omega},\mathbf{\theta}) :=\langle\mathbf{\omega}-\mathbf{\omega}_{\mathbf{\theta}}^{*},g(O,\mathbf{\omega },\mathbf{\theta})-\bar{g}(\mathbf{\omega},\mathbf{\theta})\rangle,\] \[\Theta(O,O^{\prime},\mathbf{\theta}) :=\langle\nabla J(\mathbf{\theta}),\mathbb{E}_{O^{\prime}}[h(O^{ \prime},\mathbf{\theta})]-h(O,\mathbf{\theta})\rangle,\] \[\Xi(O,\mathbf{\omega},\mathbf{\theta}) :=\langle\mathbf{\omega}-\mathbf{\omega}_{\mathbf{\theta}}^{*},(\nabla\mathbf{ \omega}_{\mathbf{\theta}}^{*})^{\top}(\mathbb{E}_{O^{\prime}}[h(O^{\prime},\mathbf{ \theta})]-h(O,\mathbf{\theta}))\rangle,\]

where \(O^{\prime}\) is a shorthand for an independent sample from stationary distribution \(s\sim\mu_{\mathbf{\theta}},a\sim\pi_{\mathbf{\theta}},s^{\prime}\sim\mathcal{P}\). Define \(U_{\delta}:=2U_{r}+2U_{\mathbf{\omega}}\) so that we have \(|\delta_{t}|\leq U_{\delta}\), where \(\delta_{t}\) comes from Line 6 in Algorithm 1. Note that from Assumption 3.3, we have \(\|\delta\nabla\log\pi_{\mathbf{\theta}}\|\leq G:=U_{\delta}B\).

Preliminary Lemmas

**Lemma B.1** ([31], Lemma C.4).: _For any \(\mathbf{\theta}_{1},\mathbf{\theta}_{2}\), we have_

\[|J(\mathbf{\theta}_{1})-J(\mathbf{\theta}_{2})|\leq L_{J}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{ 2}\|,\]

_where \(L_{J}=2U_{r}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1}\rceil+\frac{1}{1- \rho})\)._

**Lemma B.2** ([38], Lemma 3.2).: _For the performance function \(J(\mathbf{\theta})\), there exists a constant \(L_{J^{\prime}}>0\) such that for all \(\mathbf{\theta}_{1},\mathbf{\theta}_{2}\in\mathbb{R}^{d}\), it holds that_

\[\|\nabla J(\mathbf{\theta}_{1})-\nabla J(\mathbf{\theta}_{2})\|\leq L_{J^{\prime}}\| \mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|, \tag{14}\]

which further implies

\[J(\mathbf{\theta}_{2})\geq J(\mathbf{\theta}_{1})+\langle\nabla J(\mathbf{ \theta}_{1}),\mathbf{\theta}_{2}-\mathbf{\theta}_{1}\rangle-\frac{L_{J^{\prime}}}{2}\| \mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|^{2}, \tag{15}\] \[J(\mathbf{\theta}_{2})\leq J(\mathbf{\theta}_{1})+\langle\nabla J(\mathbf{ \theta}_{1}),\mathbf{\theta}_{2}-\mathbf{\theta}_{1}\rangle+\frac{L_{J^{\prime}}}{2}\| \mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|^{2}. \tag{16}\]

**Lemma B.3** ([31], Proposition 4.4).: _There exists a constant \(L_{*}>0\) such that_

\[\|\mathbf{\omega}^{*}(\mathbf{\theta}_{1})-\mathbf{\omega}^{*}(\mathbf{\theta}_{2})\|\leq L_{* }\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|,\forall\mathbf{\theta}_{1},\mathbf{\theta}_{2}\in \mathbb{R}^{d},\]

_where \(L_{*}=(2\lambda^{-2}U_{r}+3\lambda^{-1}U_{r})|\mathcal{A}|L_{\pi}(1+\lceil\log_ {\rho}m^{-1}\rceil+\frac{1}{1-\rho})\)._

**Lemma B.4** ([8], Proposition 8).: _For any \(\mathbf{\theta}_{1},\mathbf{\theta}_{2}\in\mathbb{R}^{d}\), we have_

\[\|\nabla\mathbf{\omega}^{*}(\mathbf{\theta}_{1})-\nabla\mathbf{\omega}^{*}(\mathbf{\theta}_{2} )\|\leq L_{s}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|,\]

_where \(L_{s}\) is a positive constant._

**Lemma B.5** ([42],[31]).: _For any \(\mathbf{\theta}_{1}\) and \(\mathbf{\theta}_{2}\), it holds that_

\[d_{TV}(\mu_{\mathbf{\theta}_{1}},\mu_{\mathbf{\theta}_{2}})\leq|\mathcal{A}|(\lceil\log _{\rho}m^{-1}\rceil+\frac{1}{1-\rho})\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|,\]

\[d_{TV}(\mu_{\mathbf{\theta}_{1}}\otimes\pi_{\mathbf{\theta}_{1}},\mu_{\mathbf{\theta}_{2}} \otimes\pi_{\mathbf{\theta}_{2}})\leq|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{- 1}\rceil+\frac{1}{1-\rho})\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|,\]

\[d_{TV}(\mu_{\mathbf{\theta}_{1}}\otimes\pi_{\mathbf{\theta}_{1}}\otimes\mathcal{P},\mu _{\mathbf{\theta}_{2}}\otimes\pi_{\mathbf{\theta}_{2}}\otimes\mathcal{P})\leq| \mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1}\rceil+\frac{1}{1-\rho})\|\mathbf{ \theta}_{1}-\mathbf{\theta}_{2}\|.\]

**Lemma B.6** ([31], Lemma B.2).: _Given time indexes \(t\) and \(\tau\) such that \(t\geq\tau>0\), consider the auxiliary Markov chain in (10). Conditioning on \(s_{t-\tau+1}\) and \(\mathbf{\theta}_{t-\tau}\), we have_

\[d_{TV}(\mathbb{P}(s_{t+1}\in\cdot),\mathbb{P}(\widetilde{s}_{t+ 1}\in\cdot))\leq d_{TV}(\mathbb{P}(O_{t}\in\cdot),\mathbb{P}(\widetilde{O}_{t }\in\cdot)),\] \[d_{TV}(\mathbb{P}(O_{t}\in\cdot),\mathbb{P}(\widetilde{O}_{t}\in \cdot))=d_{TV}(\mathbb{P}((s_{t},a_{t})\in\cdot),\mathbb{P}((\widetilde{s}_{ t},\widetilde{a}_{t})\in\cdot)),\] \[d_{TV}(\mathbb{P}((s_{t},a_{t})\in\cdot),\mathbb{P}((\widetilde{s }_{t},\widetilde{a}_{t})\in\cdot))\leq d_{TV}(\mathbb{P}(s_{t}\in\cdot),\mathbb{ P}(\widetilde{s}_{t}\in\cdot))+\frac{1}{2}|\mathcal{A}|\mathbb{E}[\|\mathbf{\theta}_{t}- \mathbf{\theta}_{t-\tau}\|].\]

## Appendix C Proof of Main Theorem

### Step 1: Reward estimation error analysis

In this subsection, we will establish an implicit bound for estimator.

**Lemma C.1**.: _From any \(t\geq\tau>0\), we have_

\[\mathbb{E}[\Phi(O_{t},\eta_{t},\mathbf{\theta}_{t})] \leq 4U_{r}L_{J}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+2U_{r}| \eta_{t}-\eta_{t-\tau}|\] \[+2U_{r}^{2}|\mathcal{A}|L_{\pi}\sum_{i=t-\tau}^{t}\mathbb{E}\|\mathbf{ \theta}_{i}-\mathbf{\theta}_{t-\tau}\|+4U_{r}^{2}m\rho^{\tau-1}.\]

**Theorem C.2**.: _Choose \(\alpha_{t}=\frac{c}{\sqrt{T}},\beta_{t}=\gamma_{t}=\frac{1}{\sqrt{T}}\), we have_

\[Y_{T}\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+cG\sqrt{Y_{T}G_{T}}. \tag{17}\]Proof.: From the update rule of reward estimator in Line 7 of Algorithm 1, we have

\[\eta_{t+1}-J(\mathbf{\theta}_{t+1})=\eta_{t}-J(\mathbf{\theta}_{t})+J(\mathbf{\theta}_{t})-J( \mathbf{\theta}_{t+1})+\gamma_{t}(r_{t}-\eta_{t})\]

Then we have

\[y_{t+1}^{2} =(y_{t}+J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1})+\gamma_{t}(r_{t}- \eta_{t}))^{2}\] \[\leq y_{t}^{2}+2y_{t}(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1}))+2 \gamma_{t}y_{t}(r_{t}-\eta_{t})\] \[\quad+2(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1}))^{2}+2\gamma_{t}^ {2}(r_{t}-\eta_{t})^{2}\] \[= (1-2\gamma_{t})y_{t}^{2}+2\gamma_{t}y_{t}(r_{t}-J(\mathbf{\theta}_{t} ))+2y_{t}(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1}))\] \[\quad+2(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1}))^{2}+2\gamma_{t}^ {2}(r_{t}-\eta_{t})^{2}.\]

Taking expectation up to \(s_{t+1}\) (the whole trajectory), rearranging and summing from \(\tau_{T}\) to \(T-1\), we have

\[\sum_{t=r_{T}}^{T-1}\mathbb{E}[y_{t}^{2}] \leq\underbrace{\sum_{t=r_{T}}^{T}\frac{1}{2\gamma_{t}}\mathbb{E} (y_{t}^{2}-y_{t+1}^{2})}_{I_{1}}+\underbrace{\sum_{t=r_{T}}^{T-1}\mathbb{E}[y_ {t}(r_{t}-J(\mathbf{\theta}_{t}))]}_{I_{2}}+\underbrace{\sum_{t=r_{T}}^{T-1}\frac{ 1}{\gamma_{t}}\mathbb{E}[y_{t}(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1})]}_{I_{3}}\] \[\quad+\underbrace{\sum_{t=r_{T}}^{T-1}\frac{1}{\gamma_{t}}\mathbb{ E}[(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1}))^{2}]}_{I_{4}}+\underbrace{\sum_{t=r_{T} }^{T-1}\gamma_{t}\mathbb{E}[(r_{t}-\eta_{t})^{2}]}_{I_{5}}.\]

For term \(I_{1}\), from Abel summation by parts, we have

\[I_{1} =\sum_{t=r_{T}}^{T-1}\frac{1}{2\gamma_{t}}(y_{t}^{2}-y_{t+1}^{2})\] \[=\sum_{t=r_{T+1}}^{T-1}y_{t}^{2}(\frac{1}{2\gamma_{t}}-\frac{1}{2 \gamma_{t-1}})+\frac{1}{2\gamma_{\tau_{t}}}y_{\tau_{t}}^{2}-\frac{1}{\gamma_{T -1}}y_{T}^{2}\] \[\leq\frac{2U_{r}^{2}}{\gamma_{T-1}}\] \[=2U_{r}^{2}\sqrt{T}.\]

For term \(I_{2}\), from Lemma C.1, we have

\[\mathbb{E}[y_{t}(r_{t}-J(\mathbf{\theta}_{t}))] \leq 4U_{r}L_{J}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+2U_{r}| \eta_{t}-\eta_{t-\tau}|\] \[\quad+2U_{r}^{2}|\mathcal{A}|L_{\pi}\sum_{i=t-\tau}^{t}\mathbb{E }\|\mathbf{\theta}_{i}-\mathbf{\theta}_{t-\tau}\|+4U_{r}^{2}m\rho^{\tau-1}\] \[\leq 4U_{r}L_{J}G\tau\alpha_{t-\tau}+4U_{r}^{2}\tau\gamma_{t-\tau }+2U_{r}^{2}|\mathcal{A}|L_{\pi}\tau(\tau+1)G\alpha_{t-\tau}+4U_{r}^{2}m\rho^ {\tau-1}\] \[\leq(4U_{r}L_{J}G\tau+2U_{r}^{2}|\mathcal{A}|L_{\pi}G\tau(\tau+1 ))\alpha_{t-\tau}+4U_{r}^{2}\tau\gamma_{t-\tau}+4U_{r}^{2}m\rho^{\tau-1}.\]

Choose \(\tau=\tau_{T}\), we have

\[I_{2} =\sum_{t=r_{T}}^{T-1}\mathbb{E}[y_{t}(r_{t}-J(\mathbf{\theta}_{t}))]\] \[\leq(4U_{r}L_{J}G\tau_{T}+2U_{r}^{2}|\mathcal{A}|L_{\pi}G\tau_{ T}(\tau_{T}+1))\sum_{t=r_{T}}^{T-1}\alpha_{t}\] \[\quad+4U_{r}^{2}\tau_{T}\sum_{t=r_{T}}^{T-1}\gamma_{t}+4U_{r}^{2} \sum_{t=r_{T}}^{T-1}\frac{1}{\sqrt{T}}\] \[=(4U_{r}L_{J}G\tau_{T}+2U_{r}^{2}|\mathcal{A}|L_{\pi}G\tau_{T}( \tau_{T}+1)+4U_{r}^{2}\tau_{T}+4U_{r}^{2})\frac{T-\tau_{T}}{\sqrt{T}}.\]For \(I_{3}\), if \(y_{t}>0\), from (15), we have

\[y_{t}(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1})) \leq y_{t}(\frac{L_{J^{\prime}}}{2}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1 }\|^{2}+\langle\nabla J(\mathbf{\theta}_{t}),\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\rangle)\] \[\leq L_{J^{\prime}}U_{r}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\|^{2} +|y_{t}|\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\|\|\nabla J(\mathbf{\theta}_{t})\|.\]

If \(y_{t}\leq 0\), from (16), we have

\[y_{t}(J(\mathbf{\theta}_{t})-J(\mathbf{\theta}_{t+1})) \leq y_{t}(-\frac{L_{J^{\prime}}}{2}\|\mathbf{\theta}_{t}-\mathbf{\theta }_{t+1}\|^{2}+\langle\nabla J(\mathbf{\theta}_{t}),\mathbf{\theta}_{t}-\mathbf{\theta}_{t+ 1}\rangle)\] \[\leq L_{J^{\prime}}U_{r}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\|^{2} +|y_{t}|\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\|\|\nabla J(\mathbf{\theta}_{t})\|.\]

Overall, we get

\[I_{3} =\sum_{t=r_{T}}^{T-1}\frac{1}{\gamma_{t}}\mathbb{E}[y_{t}(J(\mathbf{ \theta}_{t})-J(\mathbf{\theta}_{t+1}))]\] \[\leq\sum_{t=r_{T}}^{T-1}\frac{1}{\gamma_{t}}\mathbb{E}[L_{J^{ \prime}}U_{r}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\|^{2}+|y_{t}|\|\mathbf{\theta}_{t} -\mathbf{\theta}_{t+1}\|\|\nabla J(\mathbf{\theta}_{t})\|]\] \[\leq\sum_{t=r_{T}}^{T-1}\mathbb{E}[cL_{J^{\prime}}U_{r}G^{2}\alpha _{t}+cG|y_{t}|\|\nabla J(\mathbf{\theta}_{t})\|]\] \[\leq cL_{J^{\prime}}U_{r}G^{2}\frac{T-\tau_{T}}{\sqrt{T}}+cG(\sum _{t=r_{T}}^{T-1}\mathbb{E}y_{t}^{2})^{\frac{1}{2}}(\sum_{t=r_{T}}^{T-1}\mathbb{ E}\|\nabla J(\mathbf{\theta}_{t})\|^{2})^{\frac{1}{2}}.\]

For term \(I_{4}\), we have

\[I_{4} =\sum_{t=r_{T}}^{T-1}\frac{1}{\gamma_{t}}\mathbb{E}[(J(\mathbf{\theta }_{t})-J(\mathbf{\theta}_{t+1}))^{2}]\] \[\leq\sum_{t=r_{T}}^{T-1}\frac{1}{\gamma_{t}}L_{J}^{2}\mathbb{E}\| \mathbf{\theta}_{t}-\mathbf{\theta}_{t+1}\|^{2}\] \[\leq\sum_{t=r_{T}}^{T-1}\frac{1}{\gamma_{t}}L_{J}^{2}G^{2}\alpha _{t}^{2}\] \[=L_{J}^{2}G^{2}c^{2}\frac{T-\tau_{T}}{\sqrt{T}}.\]

For term \(I_{5}\), we have

\[I_{5} =\sum_{t=r_{T}}^{T-1}\gamma_{t}\mathbb{E}[(r_{t}-J(\mathbf{\theta}_{t }))^{2}]\] \[\leq\sum_{t=r_{T}}^{T-1}4U_{r}^{2}\gamma_{t}\] \[=4U_{r}^{2}\frac{T-\tau_{T}}{\sqrt{T}}.\]

Therefore, we get

\[\sum_{t=r_{T}}^{T-1}\mathbb{E}[y_{t}^{2}] \leq(4U_{r}L_{J}G\tau_{T}+2U_{r}^{2}|\mathcal{A}|L_{\pi}G\tau_{T}( \tau_{T}+1)\] \[\quad+4U_{r}^{2}(\tau_{T}+2)+c^{2}G^{2}(L_{J^{\prime}}U_{r}+L_{J}^ {2}))\frac{T-\tau_{T}}{\sqrt{T}}\] \[\quad+2U_{r}^{2}\sqrt{T}+cG(\sum_{t=r_{T}}^{T-1}\mathbb{E}y_{t}^{ 2})^{\frac{1}{2}}(\sum_{t=r_{T}}^{T-1}\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^ {2})^{\frac{1}{2}}.\]Since \(\tau_{T}=\mathcal{O}(\log T)\), we have \(\frac{\sqrt{T}}{T-\tau_{T}}\leq\frac{2}{\sqrt{T}}\) for large \(T\). Then we get

\[\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}[y_{t}^{2}]\] \[\leq(4U_{r}L_{J}G\tau_{T}+2U_{r}^{2}|\mathcal{A}|L_{\pi}G\tau_{T}( \tau_{T}+1)\] \[\quad+4U_{r}^{2}(\tau_{T}+3)+c^{2}G^{2}(L_{J^{\prime}}U_{r}+L_{J}^ {2}))\frac{1}{\sqrt{T}}\] \[\quad+cG(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t }^{2})^{\frac{1}{2}}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\| \nabla J(\mathbf{\theta}_{t})\|^{2})^{\frac{1}{2}}\] \[=\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+cG(\frac{1}{T-\tau_{T}} \sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2})^{\frac{1}{2}}(\frac{1}{T-\tau_{T}} \sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2})^{\frac{1}{ 2}}.\]

Thus we finish the proof. 

### Step 2: Critic error analysis

In this subsection, we will establish an implicit upper bound for critic.

**Lemma C.3**.: _For any \(t\geq\tau>0\), we have_

\[\mathbb{E}[\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})]\leq C_{1} \|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+6U_{\delta}\|\mathbf{\omega}_{t}-\mathbf{ \omega}_{t-\tau}\|+U_{\delta}^{2}|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau}+2U_{\delta}^{2}m\rho^{\tau-1},\]

_where \(C_{1}=2U_{\delta}^{2}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1}\rceil+\frac {1}{1-\rho})+2U_{\delta}L_{J}+2U_{\delta}L_{*}\)._

**Lemma C.4**.: _Given the definition of \(\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\), for any \(t\geq\tau>0\), we have_

\[\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})] \leq C_{2}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+2U_{\delta}B \|\mathbf{\omega}_{t}-\mathbf{\omega}_{t-\tau}\|\] \[\quad+2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau}+4U_{\delta}^{2}Bm\rho^{\tau-1}.\]

_where \(C_{2}:=3BU_{\delta}^{2}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1}\rceil+ \frac{1}{1-\rho})+3U_{\delta}^{2}L_{l}+8U_{\delta}BL_{*}\)._

**Theorem C.5**.: _Choose \(\alpha_{t}=\frac{c}{\sqrt{T}},\beta_{t}=\gamma_{t}=\frac{1}{\sqrt{T}}\), we have_

\[Z_{T}\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}( \epsilon_{\mathrm{app}})+\frac{2}{\lambda}\sqrt{Y_{T}Z_{T}}+\frac{2cBL_{*}}{ \lambda}\sqrt{Z_{T}(2Y_{T}+8Z_{T})}. \tag{18}\]

Proof.: From the update rule of critic in Line 8 of Algorithm 1, we have

\[\|\mathbf{\omega}_{t+1}-\mathbf{\omega}_{t+1}^{*}\| =\|\Pi_{U_{\mathbf{\omega}}}(\mathbf{\omega}_{t}+\beta_{t}\delta_{t}\mathbf{ \phi}(s_{t}))-\mathbf{\omega}_{t+1}^{*}\|\] \[=\|\Pi_{U_{\mathbf{\omega}}}(\mathbf{\omega}_{t}+\beta_{t}\delta_{t}\mathbf{ \phi}(s_{t}))-\Pi_{U_{\mathbf{\omega}}}(\mathbf{\omega}_{t+1}^{*})\|\] \[\leq\|\mathbf{\omega}_{t}+\beta_{t}\delta_{t}\mathbf{\phi}(s_{t})-\mathbf{ \omega}_{t+1}^{*}\|\] \[=\|\mathbf{\omega}_{t}+\beta_{t}(g(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{ t})+\Delta g(O_{t},\eta_{t},\mathbf{\theta}_{t}))-\mathbf{\omega}_{t+1}^{*}\|\] \[=\|\mathbf{\omega}_{t}-\mathbf{\omega}_{t}^{*}+\beta_{t}(g(O_{t},\mathbf{ \omega}_{t},\mathbf{\theta}_{t})+\Delta g(O_{t},\eta_{t},\mathbf{\theta}_{t}))+\mathbf{ \omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*}\|.\]

Therefore, we have

\[\|\mathbf{z}_{t+1}\|^{2} =\|\mathbf{z}_{t}+\beta_{t}(g(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t} )+\Delta g(O_{t},\eta_{t},\mathbf{\theta}_{t}))+\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+ 1}^{*}\|^{2}\] \[=\|\mathbf{z}_{t}\|^{2}+2\beta_{t}\langle\mathbf{z}_{t},g(O_{t},\mathbf{ \omega}_{t},\mathbf{\theta}_{t})\rangle+2\beta_{t}\langle\mathbf{z}_{t},\Delta g(O_{t },\eta_{t},\mathbf{\theta}_{t})\rangle\] \[\quad+2\langle\mathbf{z}_{t},\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*} \rangle+\|\beta_{t}(g(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+\Delta g(O_{t}, \eta_{t},\mathbf{\theta}_{t}))+\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*}\|^{2}\] \[\leq\|\mathbf{z}_{t}\|^{2}+2\beta_{t}\langle\mathbf{z}_{t},\bar{g}(\mathbf{ \omega}_{t},\mathbf{\theta}_{t})\rangle+2\beta_{t}\langle\mathbf{O}_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t}\rangle+2\beta_{t}\langle\mathbf{z}_{t},\Delta g(O_{t},\eta_{t},\mathbf{ \theta}_{t})\rangle\] \[\quad+2\langle\mathbf{z}_{t},\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*} \rangle+2U_{\delta}^{2}\beta_{t}^{2}+2\|\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*} \|^{2}.\]Note that we have

\[\langle\mathbf{z}_{t},\bar{g}(\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle =\langle\mathbf{z}_{t},\bar{g}(\mathbf{\omega}_{t},\mathbf{\theta}_{t})-\bar{g }(\mathbf{\omega}_{t}^{*},\mathbf{\theta}_{t})\rangle\] \[=\langle\mathbf{z}_{t},\mathbb{E}[(\mathbf{\phi}(s^{\prime})-\mathbf{\phi}(s) )^{\top}(\mathbf{\omega}_{t}-\mathbf{\omega}_{t}^{*})\mathbf{\phi}(s)]\rangle\] \[=\mathbf{z}_{t}^{\top}\mathbb{E}[\mathbf{\phi}(s)(\mathbf{\phi}(s^{\prime})- \mathbf{\phi}(s))^{\top}]\mathbf{z}_{t}\] \[=\mathbf{z}_{t}^{\top}Az_{t}\] \[\leq\ -\lambda\|\mathbf{z}_{t}\|^{2},\]

where the first equality is due to the fact that \(\bar{g}(\mathbf{\omega}_{t}^{*},\mathbf{\theta}_{t})=0\) and the last inequality follows from Assumption 3.1.

Taking expectation up to \(s_{t+1}\), we have

\[\mathbb{E}\|\mathbf{z}_{t+1}\|^{2} \leq\mathbb{E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t}\mathbb{E}\langle\mathbf{z }_{t},\bar{g}(\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle+2\beta_{t}\mathbb{E} \Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2\beta_{t}\mathbb{E}\langle\mathbf{z}_ {t},\Delta g(O_{t},\eta_{t},\mathbf{\theta}_{t})\rangle\] \[\quad+2\mathbb{E}\langle\mathbf{z}_{t},\mathbf{\omega}_{t}^{*}-\mathbf{\omega }_{t+1}^{*}\rangle+2U_{\delta}^{2}\beta_{t}^{2}+2\mathbb{E}\|\mathbf{\omega}_{t}^{ *}-\mathbf{\omega}_{t+1}^{*}\|^{2}\] \[\leq(1-2\lambda\beta_{t})\mathbb{E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t} \mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2\beta_{t}\mathbb{E} \langle\mathbf{z}_{t},\Delta g(O_{t},\eta_{t},\mathbf{\theta}_{t})\rangle\] \[\quad+2\mathbb{E}\langle\mathbf{z}_{t},\mathbf{\omega}_{t}^{*}-\mathbf{\omega }_{t+1}^{*}\rangle+2U_{\delta}^{2}\beta_{t}^{2}+2\mathbb{E}\|\mathbf{\omega}_{t}^{ *}-\mathbf{\omega}_{t+1}^{*}\|^{2}\] \[\quad+2\mathbb{E}\langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{ \top}(\mathbf{\theta}_{t}-\mathbf{\theta}_{t+1})\rangle+2U_{\delta}^{2}\beta_{t}^{2}+2 \mathbb{E}\|\mathbf{\omega}_{t}^{*}-\mathbf{\omega}_{t+1}^{*}\|^{2}\] \[\stackrel{{(1)}}{{\leq}}(1-2\lambda\beta_{t})\mathbb{ E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t}\mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2 \beta_{t}\mathbb{E}\|\mathbf{z}_{t}\|\|\mathbf{\theta}_{t}+L_{s}\mathbb{E}\|\|\mathbf{z}_ {t}\|\|\mathbf{\theta}_{t+1}-\mathbf{\theta}_{t}\|^{2}\] \[\quad+2\alpha_{t}\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_ {t}^{*})^{\top}\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle +2U_{\delta}^{2}\beta_{t}^{2}+2L_{s}^{2}\mathbb{E}\|\mathbf{\theta}_{t}-\mathbf{\theta} _{t+1}\|^{2}\] \[\leq(1-2\lambda\beta_{t})\mathbb{E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t} \mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2\beta_{t}\sqrt{\mathbb{E }y_{t}^{2}}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\] \[\quad+\frac{L_{s}}{2}\mathbb{E}\|\mathbf{z}_{t}\|^{2}\|\mathbf{\theta}_{t+ 1}-\mathbf{\theta}_{t}\|^{2}+\frac{L_{s}}{2}\mathbb{E}\|\mathbf{\theta}_{t+1}-\mathbf{ \theta}_{t}\|^{2}+2U_{\delta}^{2}\beta_{t}^{2}+2L_{s}^{2}G^{2}\alpha_{t}^{2}\] \[\quad+2\alpha_{t}\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_ {t}^{*})^{\top}\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle\] \[\leq(1-2\lambda\beta_{t})\mathbb{E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t} \mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2\beta_{t}\sqrt{\mathbb{E }y_{t}^{2}}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}+\frac{L_{s}G^{2}}{2}\alpha_{t}^ {2}\mathbb{E}\|\mathbf{z}_{t}\|^{2}\] \[\quad+2U_{\delta}^{2}\beta_{t}^{2}+(2L_{s}^{2}+\frac{L_{s}}{2})G^{ 2}\alpha_{t}^{2}+2\alpha_{t}\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_ {t}^{*})^{\top}\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle\] \[\stackrel{{(2)}}{{\leq}}(1-\lambda\beta_{t})\mathbb{ E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t}\mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2 \beta_{t}\sqrt{\mathbb{E}y_{t}^{2}}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\] \[\quad+2U_{\delta}^{2}\beta_{t}^{2}+(2L_{s}^{2}+\frac{L_{s}}{2})G^ {2}\alpha_{t}^{2}+2\alpha_{t}\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_ {t}^{*})^{\top}\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle \tag{19}\]

where (1) follows from the \(L_{s}\)-smoothness of \(\mathbf{\omega}^{*}\) in Lemma B.4; (2) uses \(\frac{L_{s}G^{2}}{2}\alpha_{t}^{2}\leq\lambda\beta_{t}\) for large \(T\).

For term \(\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_{t}^{*})^{\top}\delta_{t}\nabla \log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle\), we have

\[\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_{t}^{*})^{\top} \delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle\] \[=\mathbb{E}\langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{\top}(- \Delta h(O_{t},\eta_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})-h(O_{t},\mathbf{\theta}_{t }))\rangle\] \[=-\mathbb{E}\langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{\top} \Delta h(O_{t},\eta_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle\] \[\quad+\mathbb{E}\langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{ \top}(\mathbb{E}_{O_{t}}[h(O_{t}^{\prime},\mathbf{\theta}_{t})]-h(O_{t},\mathbf{\theta}_ {t})-\mathbb{E}_{O_{t}^{\prime}}[h(O_{t}^{\prime},\mathbf{\theta}_{t})])\rangle\] \[=\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})]-\mathbb{E} \langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{\top}\Delta h(O_{t},\eta_{t},\mathbf{ \omega}_{t},\mathbf{\theta}_{t})\rangle-\mathbb{E}\langle\mathbf{z}_{t},(\nabla\mathbf{ \omega}_{t}^{*})^{\top}\mathbb{E}_{O_{t}^{\prime}}[h(O_{t}^{\prime},\mathbf{\theta}_{ t})]\rangle \tag{20}\]

Note that from Cauchy-Schwartz inequality and \(L_{s}\) is the Lipschitz constant of \(\mathbf{\omega}^{*}\) in Lemma B.3, we have

\[-\mathbb{E}\langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{\top}\Delta h(O_{t}, \eta_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle\leq BL_{*}\sqrt{\mathbb{E}\| \mathbf{z}_{t}\|^{2}}\sqrt{2\mathbb{E}y_{t}^{2}+8\mathbb{E}\|\mathbf{z}_{t}\|^{2}}. \tag{21}\]Furthermore, it holds that

\[\mathbb{E}_{O^{\prime}}\|\nabla h^{\prime}(O,\mathbf{\theta})\|^{2} =\mathbb{E}_{O^{\prime}}\|((\mathbf{\phi}(s^{\prime})^{\top}\mathbf{\omega}^ {*}-V_{\mathbf{\theta}}(s^{\prime}))-(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{*}-V_{\mathbf{ \theta}}(s)))\nabla\log\pi_{\mathbf{\theta}}(a|s)\|^{2}\] \[\leq\mathbb{E}_{O^{\prime}}[B^{2}(\mathbf{\phi}(s^{\prime})^{\top}\bm {\omega}^{*}-V_{\mathbf{\theta}}(s^{\prime}))-(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{*}- V_{\mathbf{\theta}}(s)))^{2}]\] \[\leq\mathbb{E}_{O^{\prime}}[2B^{2}(\mathbf{\phi}(s^{\prime})^{\top} \mathbf{\omega}^{*}-V_{\mathbf{\theta}}(s^{\prime}))^{2}+(\mathbf{\phi}(s)^{\top}\mathbf{ \omega}^{*}-V_{\mathbf{\theta}}(s))^{2}]\] \[=4B^{2}\mathbb{E}_{O^{\prime}}[(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{* }-V_{\mathbf{\theta}}(s))^{2}]\] \[=4B^{2}\epsilon_{\mathrm{app}}^{2}.\]

Therefore, we have

\[-\langle\mathbf{z}_{t},(\nabla\mathbf{\omega}_{t}^{*})^{\top}\mathbb{E}_{ O^{\prime}}[\Delta h^{\prime}(O^{\prime},\mathbf{\theta}_{t})]\rangle \leq U_{\delta}L_{*}\sqrt{\|\mathbb{E}_{O^{\prime}}[\Delta h^{ \prime}(O_{t},\mathbf{\theta}_{t})]\|^{2}}\] \[\leq U_{\delta}L_{*}\sqrt{\mathbb{E}_{O^{\prime}}\|\Delta h^{ \prime}(O_{t},\mathbf{\theta}_{t})\|^{2}}\] \[\leq 2BU_{\delta}L_{*}\epsilon_{\mathrm{app}}. \tag{22}\]

Substituting (21) and (22) into (20) yields

\[\mathbb{E}\langle\mathbf{z}_{t},-(\nabla\mathbf{\omega}_{t}^{*})^{\top} \delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle \leq\mathbb{E}\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2BU_{ \delta}L_{*}\epsilon_{\mathrm{app}}\] \[\quad+BL_{*}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\sqrt{2\mathbb{E} y_{t}^{2}+8\mathbb{E}\|\mathbf{z}_{t}\|^{2}}. \tag{23}\]

Plugging (23) into (19), we have

\[\mathbb{E}\|\mathbf{z}_{t+1}\|^{2} \leq(1-\lambda\beta_{t})\mathbb{E}\|\mathbf{z}_{t}\|^{2}+2\beta_{t} \mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})+2\alpha_{t}\mathbb{E} \Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\] \[\quad+2\beta_{t}\sqrt{\mathbb{E}y_{t}^{2}}\sqrt{\mathbb{E}\|\mathbf{ z}_{t}\|^{2}}+2BL_{*}\alpha_{t}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\sqrt{2 \mathbb{E}y_{t}^{2}+8\mathbb{E}\|\mathbf{z}_{t}\|^{2}} \tag{24}\] \[\quad+2U_{\delta}^{2}\beta_{t}^{2}+(2L_{*}^{2}+\frac{L_{s}}{2})G ^{2}\alpha_{t}^{2}+4BU_{\delta}\alpha_{t}\epsilon_{\mathrm{app}}.\]

Rearranging and summing from \(\tau_{T}\) to \(T\) gives

\[\lambda\sum_{\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2} \leq\underbrace{\sum_{t=\tau_{T}}^{T-1}\frac{1}{\beta_{t}}( \mathbb{E}\|\mathbf{z}_{t}\|^{2}-\mathbb{E}\|\mathbf{z}_{t+1}\|^{2})}_{I_{1}}+ \underbrace{2\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\bm {\theta}_{t})}_{I_{2}}+\underbrace{2c\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\Xi(O_ {t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})}_{I_{3}}\] \[\quad+\underbrace{2\sum_{t=\tau_{T}}^{T-1}\sqrt{\mathbb{E}y_{t}^ {2}}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}}_{I_{4}}+\underbrace{2cBL_{*}\sum_{t= \tau_{T}}^{T-1}\sqrt{\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\sqrt{2\mathbb{E}y_{t}^{2} +8\mathbb{E}\|\mathbf{z}_{t}\|^{2}}}_{I_{5}}\] \[\quad+\sum_{t=\tau_{T}}^{T-1}(2U_{\delta}^{2}\beta_{t}+c(2L_{*}^ {2}+\frac{L_{s}}{2})G^{2}\alpha_{t}+4cBU_{\delta}\epsilon_{\mathrm{app}}).\]

In the sequel, we will tackle \(I_{1},I_{2},I_{3},I_{4},I_{5}\) respectively.

For term \(I_{1}\), from Abel summation by parts, we have

\[I_{1} =\sum_{t=\tau_{T}}^{T-1}\frac{1}{\beta_{t}}(\mathbb{E}\|\mathbf{z}_{ t}\|^{2}-\mathbb{E}\|\mathbf{z}_{t+1}\|^{2})\] \[=\sum_{t=\tau_{T}+1}^{T-1}(\frac{1}{\beta_{t}}-\frac{1}{\beta_{t -1}})\mathbb{E}\|\mathbf{z}_{t}\|^{2}+\frac{1}{\beta_{\tau_{T}}}\mathbb{E}\|\mathbf{z}_ {\tau_{T}}\|^{2}-\frac{1}{\beta_{T-1}}\mathbb{E}\|\mathbf{z}_{T}\|^{2}\] \[\leq U_{\delta}^{2}(\sum_{t=\tau_{T}+1}^{T-1}(\frac{1}{\beta_{t}} -\frac{1}{\beta_{t-1}})+\frac{1}{\beta_{\tau_{T}}})\] \[=U_{\delta}^{2}\sqrt{T},\]

where the inequality is due to \(\mathbb{E}\|\mathbf{z}_{t}\|^{2}\leq U_{\delta}^{2}\) and discard the last term.

For term \(I_{2}\), from Lemma C.3, choose \(\tau=\tau_{T}\), we have

\[\mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t}) \leq C_{1}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau_{T}}\|+U_{\delta}^{ 2}|\mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1)\alpha_{t-\tau_{T}}\] \[\quad+2U_{\delta}^{2}m\rho^{\tau_{T}-1}+6U_{\delta}\|\mathbf{\omega}_ {t}-\mathbf{\omega}_{t-\tau_{T}}\|\] \[\leq C_{1}\sum_{k=t-\tau_{T}}^{t-1}G\alpha_{k}+U_{\delta}^{2}| \mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1)\alpha_{t-\tau_{T}}+\frac{2U_{\delta}^{ 2}}{\sqrt{T}}+6U_{\delta}\sum_{k=t-\tau_{T}}^{t-1}U_{\delta}\beta_{k}\] \[\leq(C_{1}G\tau_{T}+U_{\delta}^{2}|\mathcal{A}|L_{\pi}G\tau_{T}( \tau_{T}+1))\alpha_{t-\tau_{T}}+\frac{2U_{\delta}^{2}}{\sqrt{T}}+6U_{\delta}^ {2}\tau_{T}\beta_{t-\tau_{T}}.\]

Then we get

\[I_{2} =2\sum_{T=\tau_{T}}^{T-1}\mathbb{E}\Psi(O_{t},\mathbf{\omega}_{t}, \mathbf{\theta}_{t})\] \[\leq 2\sum_{T=\tau_{T}}^{T-1}((C_{1}G\tau_{T}+U_{\delta}^{2}| \mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1))\alpha_{t}+\frac{2U_{\delta}^{2}}{ \sqrt{T}}+6U_{\delta}^{2}\tau_{T}\beta_{t}).\]

For term \(I_{3}\), from Lemma C.4, choose \(\tau=\tau_{T}\), we have

\[\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})] \leq C_{2}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+2U_{\delta}B\| \mathbf{\omega}_{t}-\mathbf{\omega}_{t-\tau}\|\] \[\quad+2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau}+4U_{\delta}^{2}Bm\rho^{\tau-1}\] \[\leq C_{2}\sum_{k=t-\tau_{T}}^{t-1}G\alpha_{k}+2U_{\delta}B\sum_ {k=t-\tau_{T}}^{t-1}U_{\delta}\beta_{k}\] \[\quad+2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau}+4U_{\delta}^{2}Bm\rho^{\tau_{T}-1}\] \[\leq(C_{2}G\tau_{T}+2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau_{T} (\tau_{T}+1))\alpha_{t}+2U_{\delta}^{2}B\tau_{T}\beta_{t}+\frac{4U_{\delta}^{ 2}B}{\sqrt{T}}.\]

Therefore, we have

\[I_{3} =2c\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\Xi(O_{t},\mathbf{\omega}_{t},\bm {\theta}_{t})\] \[\leq 2c\sum_{t=\tau_{T}}^{T-1}((C_{2}G\tau_{T}+2U_{\delta}^{2}B| \mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1))\alpha_{t}+2U_{\delta}^{2}B\tau_{T} \beta_{t}+\frac{4U_{\delta}^{2}B}{\sqrt{T}}).\]

For term \(I_{4}\) and \(I_{5}\), from Cauchy-Schwartz inequality, we have

\[I_{4} \leq 2(\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2})^{\frac{1}{2}}( \sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}},\] \[I_{5} \leq 2cBL_{*}(\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2}) ^{\frac{1}{2}}(2\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2}+8\sum_{t=\tau_{T}}^{ T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}.\]Overall, we get

\[\lambda\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2}\leq 2(\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2})^{\frac{1}{2}}( \sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}\] \[+2cBL_{*}(\sum_{t=\tau_{t}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{ \frac{1}{2}}(2\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2}+8\sum_{t=\tau_{T}}^{T- 1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}\] \[+U_{\delta}^{2}\sqrt{T}+2\sum_{T=\tau_{T}}^{T-1}((C_{1}G\tau_{T}+ U_{\delta}^{2}|\mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1))\alpha_{t}+\frac{2U_{ \delta}^{2}}{\sqrt{T}}+6U_{\delta}^{2}\tau_{T}\beta_{t})\] \[+2c\sum_{t=\tau_{T}}^{T-1}((C_{2}G\tau_{T}+2U_{\delta}^{2}B| \mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1))\alpha_{t}+2U_{\delta}^{2}B\tau_{T} \beta_{t}+\frac{4U_{\delta}^{2}B}{\sqrt{T}})\] \[+\sum_{t=\tau_{T}}^{T-1}(2U_{\delta}^{2}\beta_{t}+c(2L_{*}^{2}+ \frac{L_{s}}{2})G^{2}\alpha_{t}+4cBU_{\delta}\epsilon_{\mathrm{app}}).\]

Therefore, we have

\[Z_{T}\stackrel{{\eqref{eq:Z_T}}}{{\leq}} \frac{2}{\lambda}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1} \mathbb{E}y_{t}^{2})^{\frac{1}{2}}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1} \mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}\] \[+\frac{2cBL_{*}}{\lambda}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{t}}^{ T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}(2\frac{1}{T-\tau_{T}}\sum_{t= \tau_{T}}^{T-1}\mathbb{E}y_{t}^{2}+8\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1 }\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}\] \[+\frac{1}{\lambda}(\frac{2U_{\delta}^{2}}{\sqrt{T}}+2(C_{1}G\tau _{T}+U_{\delta}^{2}|\mathcal{A}|L_{\pi}G\tau_{T}(\tau_{T}+1))\alpha_{t}+\frac{ 4U_{\delta}^{2}}{\sqrt{T}}+12U_{\delta}^{2}\tau_{T}\beta_{t}\] \[+2c(C_{2}G\tau_{T}+2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau_{T}( \tau_{T}+1))\alpha_{t}+4cU_{\delta}^{2}B\tau_{T}\beta_{t}+\frac{8cU_{\delta}^ {2}B}{\sqrt{T}}\] \[+2U_{\delta}^{2}\beta_{t}+c(2L_{*}^{2}+\frac{L_{s}}{2})G^{2} \alpha_{t}+4cBU_{\delta}\epsilon_{\mathrm{app}})\] \[=\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}})+\frac{2}{\lambda}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1} \mathbb{E}y_{t}^{2})^{\frac{1}{2}}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T-1} \mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}\] \[+\frac{2cBL_{*}}{\lambda}(\frac{1}{T-\tau_{T}}\sum_{t=\tau_{t}}^{ T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}(2\frac{1}{T-\tau_{T}}\sum_{t= \tau_{T}}^{T-1}\mathbb{E}y_{t}^{2}+8\frac{1}{T-\tau_{T}}\sum_{t=\tau_{T}}^{T- 1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}},\]

where (1) follows from \(\tau_{T}=\mathcal{O}(\log T)\) so that \(T-\tau_{T}\geq\frac{1}{2}T\) for large \(T\). Therefore, we have

\[Z_{T}\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{\mathrm{ app}})+\frac{2}{\lambda}\sqrt{Y_{T}Z_{T}}+\frac{2cBL_{*}}{\lambda}\sqrt{Z_{T}(2Y_{T}+8Z _{T})},\]

which completes the proof. 

### Step 3: Policy gradient norm analysis

In this subsection, we will establish an implicit upper bound for policy gradient norm.

**Lemma C.6**.: _For any \(t\geq\tau>0\), it holds that_

\[\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t})]\leq D_{1}\tau(\tau+1)G \alpha+D_{2}m\rho^{\tau-1},\]

_where \(D_{1}=\max\{U_{\delta}BL_{J^{\prime}}+3L_{J}L_{h},2U_{\delta}BL_{J}|\mathcal{A} |L_{\pi}\}\) and \(D_{2}=4U_{\delta}BL_{J}\)._

**Theorem C.7**.: _We have_

\[G_{T}\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}( \epsilon_{\mathrm{app}})+B\sqrt{G_{T}(2Y_{T}+8Z_{T})}. \tag{25}\]Proof.: From the update rule of actor in Line 9 of Algorithm 1 and 15, we have

\[J(\mathbf{\theta}_{t+1}) \geq J(\mathbf{\theta}_{t})+\langle\nabla J(\mathbf{\theta}_{t}),\mathbf{\theta }_{t+1}-\mathbf{\theta}_{t}\rangle-\frac{L_{J^{\prime}}}{2}\|\mathbf{\theta}_{1}-\mathbf{ \theta}_{2}\|^{2}\] \[=J(\mathbf{\theta}_{t})+\langle\nabla J(\mathbf{\theta}_{t}),\delta_{t} \nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\rangle-\frac{L_{J^{\prime}}}{2} \alpha_{t}^{2}\|\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\|^{2}\] \[=J(\mathbf{\theta}_{t})+\alpha_{t}\langle\nabla J(\mathbf{\theta}_{t}), \Delta h(O_{t},\eta_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle+\alpha_{t} \langle\nabla J(\mathbf{\theta}_{t}),h(O_{t},\mathbf{\theta}_{t})\rangle\] \[\quad-\frac{L_{J^{\prime}}}{2}\alpha_{t}^{2}\|\delta_{t}\nabla \log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\|^{2}\] \[=J(\mathbf{\theta}_{t})+\alpha_{t}\langle\nabla J(\mathbf{\theta}_{t}), \Delta h(O_{t},\eta_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle-\alpha_{t} \Theta(O_{t},\mathbf{\theta}_{t})\] \[\quad+\alpha_{t}\langle\nabla J(\mathbf{\theta}_{t}),\mathbb{E}_{O^{ \prime}}[h(O^{\prime},\mathbf{\theta}_{t})]\rangle-\frac{L_{J^{\prime}}}{2}\alpha_ {t}^{2}\|\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\|^{2}\] \[=J(\mathbf{\theta}_{t})+\alpha_{t}\langle\nabla J(\mathbf{\theta}_{t}), \Delta h(O_{t},\eta_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle-\alpha_{t} \Theta(O_{t},\mathbf{\theta}_{t})+\alpha_{t}\|\nabla J(\mathbf{\theta}_{t})\|^{2}\] \[\quad+\alpha_{t}\langle\nabla J(\mathbf{\theta}_{t}),\mathbb{E}_{O^{ \prime}}[\Delta h^{\prime}(O^{\prime},\mathbf{\theta}_{t})]\rangle-\frac{L_{J^{ \prime}}}{2}\alpha_{t}^{2}\|\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_{t}|s_ {t})\|^{2},\]

where the last equality is due to the fact

\[\mathbb{E}_{O^{\prime}}[h(O^{\prime},\mathbf{\theta})-\Delta h^{\prime}(O^{\prime},\mathbf{\theta})]=\mathbb{E}_{O^{\prime}}[(r(s,a)-J(\mathbf{\theta})+V_{\mathbf{\theta} }(s^{\prime})-V_{\mathbf{\theta}}(s))\nabla\log\pi_{\mathbf{\theta}}(a|s)]=\nabla J( \mathbf{\theta}).\]

Rearranging the above inequality and taking expectation, we have

\[\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2} \leq\frac{1}{\alpha_{t}}(\mathbb{E}[J(\mathbf{\theta}_{t+1})-J(\mathbf{ \theta}_{t})])-\mathbb{E}\langle\nabla J(\mathbf{\theta}_{t}),\Delta h(O_{t},\eta_ {t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\rangle+\mathbb{E}[\Theta(O_{t},\mathbf{ \theta}_{t})]\] \[\quad-\mathbb{E}\langle\nabla J(\mathbf{\theta}_{t}),\mathbb{E}_{O^{ \prime}}[\Delta h^{\prime}(O^{\prime},\mathbf{\theta}_{t})]\rangle+\frac{L_{J^{ \prime}}}{2}\alpha_{t}\mathbb{E}\|\delta_{t}\nabla\log\pi_{\mathbf{\theta}_{t}}(a_ {t}|s_{t})\|^{2}.\]

Note that from Cauchy-Schwartz inequality, we have

\[-\mathbb{E}\langle\nabla J(\mathbf{\theta}_{t}),\Delta h(O_{t},\eta_{t},\mathbf{ \omega}_{t},\mathbf{\theta}_{t})\rangle\leq B\sqrt{\mathbb{E}\|\nabla J(\mathbf{ \theta}_{t})\|^{2}}\sqrt{2\mathbb{E}y_{t}^{2}+8\mathbb{E}\|\mathbf{z}_{t}\|^{2}}.\]

From Lemma C.6 and choosing \(\tau=\tau_{T}\), we have

\[\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t})] \leq D_{1}(\tau_{T}+1)\sum_{k=t-\tau_{T}+1}^{t}\mathbb{E}\|\mathbf{ \theta}_{k}-\mathbf{\theta}_{k-1}\|+D_{2}m\rho^{\tau_{T}-1}\] \[\leq D_{1}(\tau_{T}+1)G\sum_{k=t-\tau_{T}}^{t-1}\alpha_{k}+D_{2} m\rho^{\tau_{T}-1}\] \[\leq GD_{1}(\tau_{T}+1)^{2}\alpha_{t-\tau_{T}}+D_{2}\frac{1}{\sqrt {T}}.\]

It has been shown that

\[\mathbb{E}_{O^{\prime}}\|\nabla h^{\prime}(O,\mathbf{\theta})\|^{2} =\mathbb{E}_{O^{\prime}}\|((\mathbf{\phi}(s^{\prime})^{\top}\mathbf{\omega }^{*}-V_{\mathbf{\theta}}(s^{\prime}))-(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{*}-V_{ \mathbf{\theta}}(s)))\nabla\log\pi_{\mathbf{\theta}}(a|s)\|^{2}\] \[\leq\mathbb{E}_{O^{\prime}}[B^{2}((\mathbf{\phi}(s^{\prime})^{\top} \mathbf{\omega}^{*}-V_{\mathbf{\theta}}(s^{\prime}))-(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{* }-V_{\mathbf{\theta}}(s)))^{2}]\] \[\leq\mathbb{E}_{O^{\prime}}[2B^{2}(\mathbf{\phi}(s^{\prime})^{\top} \mathbf{\omega}^{*}-V_{\mathbf{\theta}}(s^{\prime}))^{2}+(\mathbf{\phi}(s)^{\top}\mathbf{ \omega}^{*}-V_{\mathbf{\theta}}(s))^{2}]\] \[=4B^{2}\mathbb{E}_{O^{\prime}}[(\mathbf{\phi}(s)^{\top}\mathbf{\omega}^{* }-V_{\mathbf{\theta}}(s))^{2}]\] \[=4B^{2}\epsilon_{\mathrm{app}}^{2}.\]

Therefore, we have

\[-\langle\nabla J(\mathbf{\theta}_{t}),\mathbb{E}_{O^{\prime}}[\Delta h ^{\prime}(O^{\prime},\mathbf{\theta}_{t})]\rangle \leq L_{J}\sqrt{\|\mathbb{E}_{O^{\prime}}[\Delta h^{\prime}(O _{t},\mathbf{\theta}_{t})]\|^{2}}\] \[\leq L_{J}\sqrt{\mathbb{E}_{O^{\prime}}\|\Delta h^{\prime}(O_{t},\mathbf{\theta}_{t})\|^{2}}\] \[\leq 2BL_{J}\epsilon_{\mathrm{app}},\]

where we use \(\|\nabla J(\mathbf{\theta})\|\leq L_{J}\) which comes from Lemma B.1. Plugging the three terms yields

\[\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2} \leq\frac{1}{\alpha_{t}}(\mathbb{E}[J(\mathbf{\theta}_{t+1})]-\mathbb{E} [J(\mathbf{\theta}_{t})])+B\sqrt{\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2}} \sqrt{2\mathbb{E}y_{t}^{2}+8\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\] \[\quad+2BL_{J}\epsilon_{\mathrm{app}}+GD_{1}(\tau_{T}+1)^{2} \alpha_{t-\tau_{T}}+D_{2}\frac{1}{\sqrt{T}}+\frac{L_{J^{\prime}}}{2}G^{2} \alpha_{t}.\]Summing over \(t\) from \(\tau_{T}\) to \(T-1\) gives

\[\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2} \leq\underbrace{\sum_{t=\tau_{T}}^{T-1}\frac{1}{\alpha_{t}}(\mathbb{ E}[J(\mathbf{\theta}_{t+1})-\mathbb{E}[J(\mathbf{\theta}_{t})])}_{I_{1}}+B\sum_{t=\tau_{T}}^{ T-1}\sqrt{\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2}}\sqrt{2\mathbb{E}y_{t}^{2}+8 \mathbb{E}\|\mathbf{z}_{t}\|^{2}}\] \[\quad+(GD_{1}(\tau_{T}+1)^{2}+D_{2})\frac{T-\tau_{T}}{\sqrt{T}}+2 BL_{J}\epsilon_{\text{app}}(T-\tau_{T}).\]

For term \(I_{1}\), from Abel summation by parts, we have

\[I_{1} =\sum_{t=\tau_{T}}^{T-1}\frac{1}{\alpha_{t}}(\mathbb{E}[J(\mathbf{ \theta}_{t+1})-\mathbb{E}[J(\mathbf{\theta}_{t})])\] \[=\sum_{t=\tau_{T}+1}^{T-1}(\frac{1}{\alpha_{t-1}}-\frac{1}{\alpha_ {t}})\mathbb{E}[J(\mathbf{\theta}_{t})]-\mathbb{E}[J(\mathbf{\theta}_{\tau_{T}})]\frac {1}{\alpha_{\tau_{T}}}+\frac{1}{\alpha_{T-1}}\mathbb{E}[J(\mathbf{\theta}_{T})]\] \[\leq\sum_{t=\tau_{T}+1}^{T-1}(\frac{1}{\alpha_{t}}-\frac{1}{\alpha _{t-1}})U_{r}+\frac{1}{\alpha_{\tau_{T}}}U_{r}+\frac{1}{\alpha_{T-1}}U_{r}\] \[=\frac{2U_{r}}{\alpha_{T-1}}\] \[=\frac{2U_{r}}{c}\sqrt{T}.\]

Overall, we have

\[\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\nabla J(\mathbf{\theta}_{t})\|^{2} \leq\frac{2U_{r}}{c}\sqrt{T}+(GD_{1}(\tau_{T}+1)^{2}+D_{2})\frac{T -\tau_{T}}{\sqrt{T}}+2BL_{J}\epsilon_{\text{app}}(T-\tau_{T})\] \[\quad+B\sum_{t=\tau_{T}}^{T-1}\sqrt{\mathbb{E}\|\nabla J(\mathbf{ \theta}_{t})\|^{2}}\sqrt{2\mathbb{E}y_{t}^{2}+8\mathbb{E}\|\mathbf{z}_{t}\|^{2}}\] \[\leq\frac{2U_{r}}{c}\sqrt{T}+(GD_{1}(\tau_{T}+1)^{2}+D_{2})\frac {T-\tau_{T}}{\sqrt{T}}+2BL_{J}\epsilon_{\text{app}}(T-\tau_{T})\] \[\quad+B(\sum_{t=\tau_{T}}^{T-1}\mathbb{E}\|\nabla J(\mathbf{\theta}_{ t})\|^{2})^{\frac{1}{2}}(2\sum_{t=\tau_{T}}^{T-1}\mathbb{E}y_{t}^{2}+8\sum_{t= \tau_{T}}^{T-1}\mathbb{E}\|\mathbf{z}_{t}\|^{2})^{\frac{1}{2}}.\]

Therefore, we get

\[G_{T} \leq(\frac{4U_{r}}{c}+GD_{1}(\tau_{T}+1)^{2}+D_{2})\frac{1}{\sqrt {T}}+2BL_{J}\epsilon_{\text{app}}+B\sqrt{G_{T}(2Y_{T}+8Z_{T})}\] \[=\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \text{app}})+B\sqrt{G_{T}(2Y_{T}+8Z_{T})},\]

which concludes the proof. 

### Step 4: Interconnected iteration system analysis

In this subsection, we perform an interconnected iteration system analysis to prove Theorem 3.5.

**Proof of Theorem 3.5.**Proof.: Combining (17), (18), and (25), we have

\[Y_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+cG\sqrt{Y_{T}G_{T}},\] \[Z_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}})+\frac{2}{\lambda}\sqrt{Y_{T}Z_{T}}+\frac{2cBL_{*}}{\lambda}\sqrt{ Z_{T}(2Y_{T}+8Z_{T})},\] \[G_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}})+B\sqrt{G_{T}(2Y_{T}+8Z_{T})}.\]

Denote

\[l_{1} :=cG,\] \[l_{2} :=\frac{2}{\lambda},\] \[l_{3} :=\frac{2cBL_{*}}{\lambda},\] \[l_{4} :=B.\]

Then we have

\[Y_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+l_{1}\sqrt{Y_{T}G_{T}},\] \[Z_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_ {\mathrm{app}})+l_{2}\sqrt{Y_{T}Z_{T}}+l_{3}\sqrt{Z_{T}(2Y_{T}+8Z_{T})},\] \[G_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_ {\mathrm{app}})+l_{4}\sqrt{G_{T}(2Y_{T}+8Z_{T})}.\]

For \(G_{T}\), we get

\[G_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon _{\mathrm{app}})+\frac{1}{2}G_{T}+l_{4}^{2}(Y_{T}+4Z_{T}),\] \[G_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_ {\mathrm{app}})+2l_{4}^{2}(Y_{T}+4Z_{T}). \tag{26}\]

For \(Z_{T}\), we have

\[Z_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon _{\mathrm{app}})+\frac{1}{4}Z_{T}+l_{2}^{2}Y_{T}+4l_{3}Z_{T}+\frac{l_{3}}{2}Y_{ T}.\]

If it satisfies \(4l_{3}\leq\frac{1}{4}\), we further have

\[Z_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon _{\mathrm{app}})+(2l_{2}^{2}+l_{3})Y_{T}. \tag{27}\]

For \(Y_{T}\), we get

\[Y_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\frac{l_{1}}{2}(Y_{T }+G_{T}).\]

Plugging (26) and (27) into the above inequality gives

\[Y_{T} \leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon _{\mathrm{app}})+\frac{l_{1}}{2}(Y_{T}+2l_{4}^{2}Y_{T}+8l_{4}^{2}Z_{T})\] \[\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon _{\mathrm{app}})+\frac{l_{1}}{2}(Y_{T}+2l_{4}^{2}Y_{T}+8l_{4}^{2}(2l_{2}^{2}+l _{3})Y_{T})\] \[=\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}})+\frac{l_{1}}{2}(1+2l_{4}^{2}+8l_{4}^{2}(2l_{2}^{2}+l_{3}))Y_{T}.\]Therefore, if \(l_{1}(1+2l_{4}^{2}+8l_{4}^{2}(2l_{2}^{2}+l_{3})\leq 1\), we have

\[Y_{T}\leq\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}( \epsilon_{\mathrm{app}}).\]

Overall, we require

\[4l_{3} \leq\frac{1}{4},\] \[l_{1}(1+2l_{4}^{2}+8l_{4}^{2}(2l_{2}^{2}+l_{3})) \leq 1.\]

According to the definition of \(l_{1},l_{2},l_{3},l_{4}\), we have

\[\frac{8cBL_{*}}{\lambda} \leq\frac{1}{4},\] \[cG(1+2B^{2}+8B^{2}(\frac{8}{\lambda^{2}}+\frac{2cBL_{*}}{\lambda })) \leq 1.\]

Thus we choose

\[c=\min\{\frac{\lambda}{32BL_{*}},\frac{\lambda^{2}}{G(\lambda^{2} +3B^{2}\lambda^{2}+64B^{2})}\}, \tag{28}\]

which satisfies the above two inequalities. Therefore, we have

\[Y_{T}=\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}( \epsilon_{\mathrm{app}}),\]

and consequently,

\[Z_{T} =\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}}),\] \[G_{T} =\mathcal{O}(\frac{\log^{2}T}{\sqrt{T}})+\mathcal{O}(\epsilon_{ \mathrm{app}}).\]

Thus we conclude our proof. 

## Appendix D Proof of Supporting Lemmas

The following three lemmas only deal with the Markovian noise, which are originally proved in [31] and updated in [30]. We include the proof with slight modifications for proving Theorem 3.5.

**Proof of Lemma C.1**.

Proof.: We will divide the proof of this lemma into four steps.

**Step 1:** show that for any \(\mathbf{\theta}_{1},\mathbf{\theta}_{2},\eta,O=(s,a,s^{\prime})\), we have

\[|\Phi(O,\eta,\mathbf{\theta}_{1})-\Phi(O,\eta,\mathbf{\theta}_{2})| \leq 4U_{r}L_{J}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|. \tag{29}\]

By the definition of \(\Phi(O,\eta,\mathbf{\theta})\) in (13), we have

\[|\Phi(O,\eta,\mathbf{\theta}_{1})-\Phi(O,\mathbf{\theta},\mathbf{\theta}_{2})| =|(\eta-J(\mathbf{\theta}_{1}))(r-J(\mathbf{\theta}_{1}))-(\eta-J(\mathbf{ \theta}_{2}))(r-J(\mathbf{\theta}_{2}))|\] \[\leq|(\eta-J(\mathbf{\theta}_{1}))(r-J(\mathbf{\theta}_{1}))-(\eta-J(\bm {\theta}_{1}))(r-J(\mathbf{\theta}_{2}))|\] \[\quad\quad+|(\eta-J(\mathbf{\theta}_{1}))(r-J(\mathbf{\theta}_{2}))-(\eta -J(\mathbf{\theta}_{2}))(r-J(\mathbf{\theta}_{2}))|\] \[\leq 4U_{r}|J(\mathbf{\theta}_{1})-J(\mathbf{\theta}_{2})|\] \[\leq 4U_{r}L_{J}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|.\]

**Step 2:** show that for any \(\mathbf{\theta},\eta_{1},\eta_{2},O\), we have

\[|\Phi(O,\eta_{1},\mathbf{\theta})-\Phi(O,\eta_{2},\mathbf{\theta}) \leq 2U_{r}|\eta_{1}-\eta_{2}|. \tag{30}\]

By definition, we have

\[|\Phi(O,\eta_{1},\mathbf{\theta})-\Phi(O,\eta_{2},\mathbf{\theta})| =|(\eta_{1}-J(\mathbf{\theta}))(r-J(\mathbf{\theta}))-(\eta_{2}-J(\mathbf{ \theta}))(r-J(\mathbf{\theta}))|\] \[\leq 2U_{r}|\eta_{1}-\eta_{2}|.\]

**Step 3:** show that for original tuple \(O_{t}\) and the auxiliary tuple \(\widetilde{O}_{t}\), conditioned on \(s_{t-\tau-1}\) and \(\mathbf{\theta}_{t-\tau}\), we have

\[|\mathbb{E}[\Phi(O_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})-\mathbb{E}[\Phi( \widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]|\leq 2U_{r}^{2}|\mathcal{A}| L_{\pi}\sum_{k=t-\tau}^{t}\mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\|. \tag{31}\]

By definition, we have

\[\mathbb{E}[\Phi(O_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})-\mathbb{E}[\Phi( \widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]=(\eta_{t-\tau}-J(\mathbf{ \theta}_{t-\tau}))\mathbb{E}[r(s_{t},a_{t})-r(\widetilde{s}_{t},\widetilde{a} _{t})].\]

By definition of total variation norm, we have

\[\mathbb{E}[r(s_{t},a_{t})-r(\widetilde{s}_{t},\widetilde{a}_{t})]\leq 2U_{r} d_{TV}(\mathbb{P}(O_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}),\mathbb{P}( \widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau})). \tag{32}\]

By Lemma B.6, we get

\[d_{TV}(\mathbb{P}(O_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau }),\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}))\] \[=d_{TV}(\mathbb{P}((s_{t},a_{t})\in\cdot|s_{t-\tau+1},\mathbf{\theta} _{t-\tau}),\mathbb{P}((\widetilde{s}_{t},\widetilde{a}_{t})\in\cdot|s_{t-\tau+ 1},\mathbf{\theta}_{t-\tau}))\] \[\leq d_{TV}(\mathbb{P}(s_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t- \tau}),\mathbb{P}(\widetilde{s}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau})) +\frac{1}{2}L_{\pi}\mathbb{E}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|\] \[\leq d_{TV}(\mathbb{P}(O_{t-1}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t -\tau}),\mathbb{P}(\widetilde{O}_{t-1}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau }))+\frac{1}{2}L_{\pi}\mathbb{E}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|.\]

Repeat the above argument from \(t\) to \(t-\tau+1\), we have

\[d_{TV}(\mathbb{P}(O_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}),\mathbb{P} (\widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}))\leq\frac{1}{2} |\mathcal{A}|L_{\pi}\sum_{k=t-\tau}^{t}\mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_ {t-\tau}\|. \tag{33}\]

Plugging (33) into (32), we have

\[|\mathbb{E}[\Phi(O_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})-\mathbb{E}[\Phi( \widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]|\leq 2U_{r}^{2}|\mathcal{A}| L_{\pi}\sum_{k=t-\tau}^{t}\mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\|.\]

**Step 4:** show that conditioned on \(s_{t-\tau+1}\) and \(\mathbf{\theta}_{t-\tau}\), we have

\[\mathbb{E}[\Phi(\widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]\leq 4U_{ r}^{2}m\rho^{\tau-1}. \tag{34}\]

Note that according to definition, we have

\[\mathbb{E}[\Phi(O_{t-\tau}^{\prime},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})|\mathbf{ \theta}_{t-\tau}]=0,\]

where \(O_{t-\tau}^{\prime}=(s_{t-\tau}^{\prime},a_{t-\tau}^{\prime},s_{t-\tau+1}^{ \prime})\) is the tuple generated by \(s_{t-\tau}^{\prime}\sim\mu_{t-\tau},a_{t-\tau}^{\prime}\sim\pi_{\mathbf{\theta}_{ t-\tau}},s_{t-\tau+1}^{\prime}\sim\mathcal{P}\). From the uniform ergodicity in Assumption 3.2, it shows that

\[d_{TV}(\mathbb{P}(\widetilde{s}_{t}=\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}), \mu_{\mathbf{\theta}_{t-\tau}})\leq m\rho^{\tau-1}.\]

Then we have

\[\mathbb{E}[\Phi(\widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})] =\mathbb{E}[\Phi(\widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t- \tau})-\Phi(O_{t-\tau}^{\prime},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]\] \[=\mathbb{E}[(\eta_{t-\tau}-J(\mathbf{\theta}_{t-\tau}))(r(\widetilde {s}_{t},\widetilde{a}_{t})-r(s_{t-\tau}^{\prime},a_{t-\tau}^{\prime}))]\] \[\leq 4U_{r}^{2}d_{TV}(\mathbb{P}(\widetilde{O}_{t-\tau}=\cdot|s_{ t-\tau+1},\mathbf{\theta}_{t-\tau}),\mu_{\mathbf{\theta}_{t-\tau}}\otimes\pi_{\mathbf{\theta}_{ t-\tau}}\otimes\mathcal{P})\] \[\leq 4U_{r}^{2}m\rho^{\tau-1}.\]

Combing (29), (30), (31), and (34), we have

\[\mathbb{E}[\Phi(O_{t},\eta_{t},\mathbf{\theta}_{t})] =\mathbb{E}[\Phi(O_{t},\eta_{t},\mathbf{\theta}_{t})-\Phi(O_{t},\eta_{ t},\mathbf{\theta}_{t-\tau})]+\mathbb{E}[\Phi(O_{t},\eta_{t},\mathbf{\theta}_{t-\tau})- \Phi(O_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Phi(O_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})- \Phi(\widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]+\mathbb{E}[\Phi( \widetilde{O}_{t},\eta_{t-\tau},\mathbf{\theta}_{t-\tau})]\] \[\leq 4U_{r}L_{J}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+2U_{r}| \eta_{t}-\eta_{t-\tau}|+2U_{r}^{2}|\mathcal{A}|L_{\pi}\sum_{i=t-\tau}^{t} \mathbb{E}\|\mathbf{\theta}_{i}-\mathbf{\theta}_{t-\tau}\|\] \[\quad+4U_{r}^{2}m\rho^{\tau-1},\]

which concludes the proof.

[MISSING_PAGE_FAIL:27]

**Step 3:** show that for tuples \(O_{t}=(s_{t},a_{t},s_{t+1})\) and \(\widetilde{O}_{t}=(\widetilde{s}_{t},\widetilde{a}_{t},\widetilde{s}_{t+1})\). Conditioning on \(s_{t-\tau+1}\) and \(\boldsymbol{\theta}_{t-\tau}\), we have

\[\mathbb{E}[\Psi(O_{t},\boldsymbol{\omega}_{t-\tau},\boldsymbol{\theta}_{t-\tau })-\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau},\boldsymbol{\theta}_{t- \tau})]\leq U_{\delta}^{2}|\mathcal{A}|L_{\pi}\sum_{k=t-\tau}^{t}\mathbb{E}\| \boldsymbol{\theta}_{k}-\boldsymbol{\theta}_{t-\tau}\|. \tag{37}\]

By the definition of total variation norm, we have

\[\mathbb{E}[\Psi(O_{t},\boldsymbol{\omega}_{t-\tau},\boldsymbol{ \theta}_{t-\tau})-\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau}, \boldsymbol{\theta}_{t-\tau})]\] \[\leq\mathbb{E}[\langle\boldsymbol{\omega}_{t-\tau}-\boldsymbol{ \omega}_{t-\tau}^{*},g(O_{t},\boldsymbol{\omega}_{t-\tau},\boldsymbol{\theta}_ {t-\tau})-g(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau},\boldsymbol{\theta} _{t-\tau}))]\] \[\leq 2U_{\delta}^{2}d_{TV}(\mathbb{P}(O_{t}\in\cdot|s_{t-\tau+1},\boldsymbol{\theta}_{-\tau}),\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t-\tau+ 1},\boldsymbol{\theta}_{t-\tau}))\] \[\stackrel{{(1)}}{{\leq}}U_{\delta}^{2}|\mathcal{A}|L _{\pi}\sum_{k=t-\tau}^{t}\mathbb{E}\|\boldsymbol{\theta}_{k}-\boldsymbol{\theta }_{t-\tau}\|\] \[\leq U_{\delta}^{2}|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau},\]

where (1) follows from (33).

**Step 4:** show that conditioning on \(s_{t-\tau+1}\) and \(\boldsymbol{\theta}_{t-\tau}\),

\[\mathbb{E}[\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau},\boldsymbol{ \theta}_{t-\tau})]\leq 2U_{\delta}^{2}m\rho^{\tau-1} \tag{38}\]

From the definition of \(\Psi(O,\boldsymbol{\omega},\boldsymbol{\theta})\), we have

\[\mathbb{E}[\Psi(O^{\prime}_{t-\tau},\boldsymbol{\omega}_{t-\tau},\boldsymbol{ \theta}_{t-\tau})|s_{t-\tau+1},\boldsymbol{\theta}_{t-\tau}]=0,\]

where \(O^{\prime}_{t-\tau}\) is the tuple generated by \(s^{\prime}_{t-\tau}\sim\mu_{\boldsymbol{\theta}_{t-\tau}},a^{\prime}_{t-\tau }\sim\pi_{\boldsymbol{\theta}_{t-\tau}},s^{\prime}_{t-\tau+1}\sim\mathcal{P}\). From Assumption 3.2, we have

\[d_{TV}(\mathbb{P}(\widetilde{s}_{t}=\cdot|s_{t-\tau+1},\boldsymbol{\theta}_{t- \tau}),\mu_{\boldsymbol{\theta}_{t-\tau}})\leq m\rho^{\tau-1}.\]

Then, it holds that

\[\mathbb{E}[\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau}, \boldsymbol{\theta}_{t-\tau})] =\mathbb{E}[\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau}, \boldsymbol{\theta}_{t-\tau})-\Psi(O^{\prime}_{t-\tau},\boldsymbol{\omega}_{t -\tau},\boldsymbol{\theta}_{t-\tau})]\] \[=\mathbb{E}\langle\boldsymbol{\omega}_{t-\tau}-\boldsymbol{ \omega}_{t-\tau}^{*},g(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau}, \boldsymbol{\theta}_{t-\tau}-g(O^{\prime}_{t-\tau},\boldsymbol{\omega}_{t-\tau },\boldsymbol{\theta}_{t-\tau}))\] \[\leq 4U_{\boldsymbol{\omega}}U_{\delta}d_{TV}(\mathbb{P}(\widetilde {O}_{t}=\cdot|s_{t-\tau+1},\boldsymbol{\theta}_{t-\tau}),\mu_{\boldsymbol{ \theta}_{t-\tau}}\otimes\pi_{\boldsymbol{\theta}_{t-\tau}}\otimes\mathcal{P})\] \[\leq 2U_{\delta}^{2}d_{TV}(\mathbb{P}(\widetilde{O}_{t}=\cdot|s_{t- \tau+1},\boldsymbol{\theta}_{t-\tau}),\mu_{\boldsymbol{\theta}_{t-\tau}} \otimes\pi_{\boldsymbol{\theta}_{t-\tau}}\otimes\mathcal{P})\] \[=2U_{\delta}^{2}d_{TV}(\mathbb{P}((\widetilde{s}_{t},\widetilde{a }_{t})\in\cdot|s_{t-\tau+1},\boldsymbol{\theta}_{t-\tau}),\mu_{\boldsymbol{ \theta}_{t-\tau}}\otimes\pi_{\boldsymbol{\theta}_{t-\tau}})\] \[=2U_{\delta}^{2}d_{TV}(\mathbb{P}(\widetilde{s}_{t}=\cdot|s_{t- \tau+1},\boldsymbol{\theta}_{t-\tau}),\mu_{\boldsymbol{\theta}_{t-\tau}})\] \[\leq 2U_{\delta}^{2}m\rho^{\tau-1}.\]

Combining (35), (36), (37), and (38), we have

\[\mathbb{E}[\Psi(O_{t},\boldsymbol{\omega}_{t},\boldsymbol{\theta }_{t})] =\mathbb{E}[\Psi(O_{t},\boldsymbol{\omega}_{t},\boldsymbol{\theta }_{t})-\Psi(O_{t},\boldsymbol{\omega}_{t},\boldsymbol{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Psi(O_{t},\boldsymbol{\omega}_{t},\boldsymbol{ \theta}_{t-\tau})-\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t-\tau}, \boldsymbol{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Psi(\widetilde{O}_{t},\boldsymbol{\omega}_{t- \tau},\boldsymbol{\theta}_{t-\tau})]\] \[\leq C_{1}\|\boldsymbol{\theta}_{t}-\boldsymbol{\theta}_{t-\tau}\|+U _{\delta}^{2}|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t-\tau}\] \[\quad+2U_{\delta}^{2}m\rho^{\tau-1}+6U_{\delta}\|\boldsymbol{\omega }_{t}-\boldsymbol{\omega}_{t-\tau}\|,\]

where \(C_{1}=2U_{\delta}^{2}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1}\rceil+ \frac{1}{1-\rho})+2U_{\delta}(L_{J}+L_{*})\). 

**Proof of Lemma C.4**.

Proof.: We will divide the proof of this lemma into four steps.

**Step 1:** show that

\[\|\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})-\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta} _{t-\tau})\|\leq(3U_{\delta}L_{h}+2U_{\delta}BL_{*})\|\mathbf{\theta}_{t}-\mathbf{\theta }_{t-\tau}\| \tag{39}\]

Since \(\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})=\langle\mathbf{\omega}_{t}-\mathbf{\omega}_{ t}^{*},\mathbb{E}_{O^{\prime}}[h(O^{\prime},\mathbf{\theta})]-h(O,\mathbf{\theta})\rangle\), we define \(\mathbb{E}_{\mathbf{\theta}}[h(O^{\prime},\mathbf{\theta})]:=\mathbb{E}_{O^{\prime}}[h (O^{\prime},\mathbf{\theta})]\), where \(\mathbb{E}_{\mathbf{\theta}}\) is the shorthand of \(\mathbb{E}_{O^{\prime}\sim(\mathbf{\mu}_{\mathbf{\theta}},\mathbf{\pi}_{\mathbf{\theta}},\mathbf{ \mathcal{P}})}\). In the following, we will show that each term in \(\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})\) is Lipschitz.

Term \(\mathbf{\omega}_{t}\) is not related to \(\mathbf{\theta}\) and term \(\mathbf{\omega}_{t}^{*}:=\mathbf{\omega}^{*}(\mathbf{\theta}_{t})\) is \(L_{*}\)-Lipschitz.

For term \(h(O,\mathbf{\theta})\), denote \(\delta(O,\mathbf{\theta}):=r(s,a)-r(\mathbf{\theta})+(\mathbf{\phi}(s^{\prime})-\mathbf{\phi} (s))^{\top}\mathbf{\omega}^{*}\), we have

\[\|h(O,\mathbf{\theta}_{1})-h(O,\mathbf{\theta}_{2})\|\] \[=\|\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{1}}(a|s)- \delta(O_{t},\mathbf{\theta}_{2})\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\|\] \[\leq\|\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{1}}(a|s )-\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\|\] \[\quad+\|\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{2}}( a|s)-\delta(O,\mathbf{\theta}_{2})\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\|\] \[\leq U_{\delta}L_{l}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+B|\delta( O,\mathbf{\theta}_{1})-\delta(O,\mathbf{\theta}_{2})|\] \[\leq U_{\delta}L_{l}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+B(|r(\bm {\theta}_{1})-r(\mathbf{\theta}_{2})|+\|\mathbf{\phi}(s^{\prime})-\mathbf{\phi}(s)\|\cdot \|\mathbf{\omega}^{*}(\mathbf{\theta}_{1})-\mathbf{\omega}^{*}(\mathbf{\theta}_{2})\|)\] \[\leq(U_{\delta}L_{l}+2BL_{*})\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\| +B\|\mathbb{E}_{s\sim\mathbf{\mu}_{\mathbf{\theta}_{1}},a\sim\pi_{\mathbf{\theta}_{1}}}[r( s,a)]-\mathbb{E}_{s\sim\mu_{\mathbf{\theta}_{1}},a\sim\pi_{\mathbf{\theta}_{2}}}[r( s,a)]\|\] \[\leq(U_{\delta}L_{l}+2BL_{*})\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\| +2BU_{r}d_{TV}(\mu_{\mathbf{\theta}_{1}}\otimes\pi_{\mathbf{\theta}_{1}},\mu_{\mathbf{ \theta}_{2}}\otimes\pi_{\mathbf{\theta}_{2}})\] \[\leq(U_{\delta}L_{l}+2BL_{*}+2BU_{r}|\mathcal{A}|L_{\pi}(1+\lceil \log_{\rho}m^{-1}\rceil+\frac{1}{1-\rho}))\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|.\]

Hence we have \(h(O,\mathbf{\theta})\) is \(L_{h}\)-Lipschitz, where \(L_{h}\) denotes the above coefficient.

For term \(\mathbb{E}_{\mathbf{\theta}}[h(O^{\prime},\mathbf{\theta})]\), we have

\[\|\mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{1})]- \mathbb{E}_{\mathbf{\theta}_{2}}[h(O^{\prime},\mathbf{\theta}_{2})]\|\] \[\leq\|\mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{1})]- \mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{2})]\|+\|\mathbb{E}_{ \mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{2})]-\mathbb{E}_{\mathbf{\theta}_{2}}[h (O^{\prime},\mathbf{\theta}_{2})]\|\] \[\leq\mathbb{E}_{\mathbf{\theta}_{1}}[\|h(O^{\prime},\mathbf{\theta}_{1})-h (O^{\prime},\mathbf{\theta}_{2})\|]+\|\mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime}, \mathbf{\theta}_{2})]-\mathbb{E}_{\mathbf{\theta}_{2}}[h(O^{\prime},\mathbf{\theta}_{2})]\|\] \[\leq L_{h}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+\|\mathbb{E}_{\mathbf{ \theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{2})]-\mathbb{E}_{\mathbf{\theta}_{2}}[h(O^{ \prime},\mathbf{\theta}_{2})]\|\] \[\leq L_{h}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+2BU_{r}d_{TV}(\mu_ {\mathbf{\theta}_{1}}\otimes\pi_{\mathbf{\theta}_{1}},\mu_{\mathbf{\theta}_{2}}\otimes\pi_ {\mathbf{\theta}_{2}})\] \[\leq[L_{h}+2BU_{r}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1 }\rceil+\frac{1}{1-\rho})]\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|\] \[\leq 2L_{h}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|.\]

Then we have \(\mathbf{\omega}_{t}-\mathbf{\omega}_{t}^{*}\) is \(U_{\delta}\)-bounded and \(L_{*}\)-Lipschitz; \(h(O,\mathbf{\theta})-\mathbb{E}_{\mathbf{\theta}}[h(O^{\prime},\mathbf{\theta})]\) is \(3L_{h}\)-Lipschitz and \(2U_{\delta}B\)-bounded. By the triangle inequality, we have

\[\|\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})-\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{ \theta}_{t-\tau})\|\leq(3U_{\delta}L_{h}+2U_{\delta}BL_{*})\|\mathbf{\theta}_{t}- \mathbf{\theta}_{t-\tau}\|\leq C_{2}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|,\]

where \(C_{2}:=3BU_{\delta}^{2}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1}\rceil+ \frac{1}{1-\rho})+3U_{\delta}^{2}L_{l}+8U_{\delta}BL_{*}\).

**Step 2:** show that

\[\|\Xi(O,\mathbf{\omega}_{t},\mathbf{\theta})-\Xi(O,\mathbf{\omega}_{t-\tau},\mathbf{\theta})\| \leq 2U_{\delta}B\|\mathbf{\omega}_{t}-\mathbf{\omega}_{t-\tau}\|. \tag{40}\]

Actually, we have

\[\|\Xi(O,\mathbf{\omega}_{1},\mathbf{\theta})-\Xi(O,\mathbf{\omega}_{2},\mathbf{\theta})\|=\| \langle\mathbf{\omega}_{1}-\mathbf{\omega}_{2},\mathbb{E}_{O^{\prime}}[h(O^{\prime},\mathbf{ \theta})]-h(O,\mathbf{\theta})\rangle\|\leq 2U_{\delta}B\|\mathbf{\omega}_{1}-\mathbf{\omega}_{2}\|\]

**Step 3:** show that for tuples \(O_{t}=(s_{t},a_{t},s_{t+1})\) and \(\widetilde{O}_{t}=(\widetilde{s}_{t},\widetilde{a}_{t},\widetilde{s}_{t+1})\). Conditioning on \(s_{t-\tau+1}\) and \(\mathbf{\theta}_{t-\tau}\), we have

\[\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})-\Xi(\widetilde{O} _{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]\leq 2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}\sum_{k=t- \tau}^{t}\mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\|. \tag{41}\]

By definition of \(\Xi(O,\mathbf{\omega},\mathbf{\theta})\), we have

\[\|\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})-\Xi( \widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]\|\] \[=\|\mathbb{E}[\langle\mathbf{\omega}_{t-\tau}-\mathbf{\omega}_{t-\tau}^{*},h( \widetilde{O}_{t},\mathbf{\theta}_{t-\tau})-h(O_{t},\mathbf{\theta}_{t-\tau})]\|\] \[=\|\mathbb{E}[\langle\mathbf{\omega}_{t-\tau}-\mathbf{\omega}_{t-\tau}^{*},h( \widetilde{O}_{t},\mathbf{\theta}_{t-\tau})\rangle-\mathbb{E}[\langle\mathbf{\omega}_{t- \tau}-\mathbf{\omega}_{t-\tau}^{*},h(O_{t},\mathbf{\theta}_{t-\tau})\rangle]\|\] \[\leq 4U_{\delta}^{2}Bd_{TV}(\mathbb{P}(O_{t}\in\cdot|s_{t-\tau+1},\mathbf{ \theta}_{t-\tau}),\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t- \tau})), \tag{42}\]where the inequality comes from the definition of total variation distance. The total variation norm between \(O_{t}\) and \(\widetilde{O}_{t}\) has been computed in (33). Plugging (33) into (42), we get

\[\|\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})- \Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]\| \leq 2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}\sum_{k=t-\tau}^{t} \mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\|\] \[\leq 2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau}.\]

**Step 4:** Show that conditioning on \(s_{t-\tau+1}\) and \(\mathbf{\theta}_{t-\tau}\), we have

\[\|\mathbb{E}[\Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})] \|\leq 4U_{\delta}^{2}Bm\rho^{\tau-1}. \tag{43}\]

It can be shown that

\[\|\mathbb{E}[\Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta }_{t-\tau})]\| \overset{(1)}{=} \|\mathbb{E}[\Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta }_{t-\tau})-\Xi(O^{\prime}_{t-\tau},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]\|\] \[= \|\mathbb{E}[\langle\mathbf{\omega}_{t-\tau}-\mathbf{\omega}_{t-\tau}^{*},h(O^{\prime}_{t-\tau},\mathbf{\theta}_{t-\tau})\rangle-\langle\mathbf{\omega}_{t-\tau} -\mathbf{\omega}_{t-\tau}^{*},h(\widetilde{O}_{t},\mathbf{\theta}_{t-\tau})\rangle]\|\] \[\overset{(2)}{\leq} 4U_{\delta}^{2}Bd_{TV}(\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t- \tau+1},\mathbf{\theta}_{t-\tau}),\mu_{\mathbf{\theta}_{t-\tau}}\otimes\pi_{\mathbf{\theta }_{t-\tau}}\otimes\mathcal{P}),\]

where (1) is due to the fact that \(O^{\prime}_{t}\) is from the stationary distribution which satisfies \(\mathbb{E}[\Xi(O^{\prime}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]=0\) and (2) follows from the definition of total variation distance. From Assumption 3.2, we know that

\[d_{TV}(\mathbb{P}(\widetilde{s}_{t}\in\cdot),\mu_{\mathbf{\theta}_{t-\tau}})\leq m \rho^{\tau-1}.\]

We also have the fact that

\[\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau})= \mathbb{P}(\widetilde{s}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau})\otimes \pi_{\mathbf{\theta}_{t-\tau}}\otimes\mathcal{P}.\]

Therefore, we have

\[\|\mathbb{E}[\Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau}) \|\leq 4U_{\delta}^{2}Bm\rho^{\tau-1}.\]

Combining (39)-(43), we can decompose the Markovian bias as

\[\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})] =\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t})-\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t},\mathbf{\theta}_{t-\tau})- \Xi(O_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Xi(O_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t- \tau})-\Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Xi(\widetilde{O}_{t},\mathbf{\omega}_{t-\tau},\mathbf{ \theta}_{t-\tau})]\] \[\leq C_{2}\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|+2U_{\delta}B\| \mathbf{\omega}_{t}-\mathbf{\omega}_{t-\tau}\|\] \[\quad+2U_{\delta}^{2}B|\mathcal{A}|L_{\pi}G\tau(\tau+1)\alpha_{t- \tau}+4U_{\delta}^{2}Bm\rho^{\tau-1}.\]

Thus we conclude our proof. 

#### Proof of Lemma c.6.

Proof.: We will divide the proof of this lemma into three steps.

**Step 1:** show that

\[|\Theta(O_{t},\mathbf{\theta}_{t-\tau})-\Theta(\widetilde{O}_{t},\mathbf{\theta}_{t- \tau})|\leq(2U_{\delta}BL_{J^{\prime}}+3L_{J}L_{h})\|\mathbf{\theta}_{t}-\mathbf{\theta }_{t-\tau}\|, \tag{44}\]

where \(L_{h}=U_{\delta}L_{l}+(2+2\lambda^{-2}+3\lambda^{-1})BU_{r}|\mathcal{A}|L_{\pi} (1+\lceil\log_{\rho}m^{-1}\rceil+1/(1-\rho))\).

Since \(\Theta(O,\mathbf{\theta})=\langle\nabla J(\mathbf{\theta}),\mathbb{E}_{\mathbf{\theta}}[h( O^{\prime},\mathbf{\theta})]-h(O,\mathbf{\theta})\rangle\), we will show that each term in \(\Theta(O,\mathbf{\theta})\) is Lipschitz.

For the term \(\nabla J(\mathbf{\theta})\), by Lemma B.3 we know it's \(L_{J^{\prime}}\)-Lipschitz.

For term \(h(O,\mathbf{\theta})\), denote \(\delta(O,\mathbf{\theta}):=r(s,a)-r(\mathbf{\theta})+(\mathbf{\phi}(s^{\prime})-\mathbf{\phi}(s))^{ \top}\mathbf{\omega}^{*}\), we have

\[\|h(O,\mathbf{\theta}_{1})-h(O,\mathbf{\theta}_{2})\|\] \[=\|\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{1}}(a|s)- \delta(O_{t},\mathbf{\theta}_{2})\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\|\] \[\leq\|\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{1}}(a|s )-\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\|\] \[\quad+\|\delta(O,\mathbf{\theta}_{1})\nabla\log\pi_{\mathbf{\theta}_{2}}( a|s)-\delta(O,\mathbf{\theta}_{2})\nabla\log\pi_{\mathbf{\theta}_{2}}(a|s)\|\] \[\leq U_{\delta}L_{l}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+B|\delta (O,\mathbf{\theta}_{1})-\delta(O,\mathbf{\theta}_{2})|\] \[\leq U_{\delta}L_{l}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+B(|r(\bm {\theta}_{1})-r(\mathbf{\theta}_{2})|+\|\mathbf{\phi}(s^{\prime})-\mathbf{\phi}(s)\|\cdot \|\mathbf{\omega}^{*}(\mathbf{\theta}_{1})-\mathbf{\omega}^{*}(\mathbf{\theta}_{2})\|)\] \[\leq(U_{\delta}L_{l}+2BL_{*})\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+ B|\mathbb{E}_{s\sim\mu_{\mathbf{\theta}_{1}},a\sim\pi_{\mathbf{\theta}_{1}}}[r(s,a)]- \mathbb{E}_{s\sim\mu_{\mathbf{\theta}_{1}},a\sim\pi_{\mathbf{\theta}_{2}}}[r(s,a)]|\] \[\leq(U_{\delta}L_{l}+2BL_{*})\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+ 2BU_{r}d_{TV}(\mu_{\mathbf{\theta}_{1}}\otimes\pi_{\mathbf{\theta}_{1}},\mu_{\mathbf{ \theta}_{2}}\otimes\pi_{\mathbf{\theta}_{2}})\] \[\leq(U_{\delta}L_{l}+2BL_{*}+2BU_{r}|\mathcal{A}|L_{\pi}(1+\lceil \log_{\rho}m^{-1}\rceil+\frac{1}{1-\rho}))\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|.\]

Hence we have \(h(O,\mathbf{\theta})\) is \(L_{h}\)-Lipschitz, where \(L_{h}\) denotes the above coefficient.

For term \(\mathbb{E}_{\mathbf{\theta}}[h(O^{\prime},\mathbf{\theta})]\), we have

\[\|\mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{1})]- \mathbb{E}_{\mathbf{\theta}_{2}}[h(O^{\prime},\mathbf{\theta}_{2})]\|\] \[\leq\|\mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{1})]- \mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{2})]\|+\|\mathbb{E}_{ \mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{2})]-\mathbb{E}_{\mathbf{\theta}_{2}}[h( O^{\prime},\mathbf{\theta}_{2})]\|\] \[\leq\mathbb{E}_{\mathbf{\theta}_{1}}[\|h(O^{\prime},\mathbf{\theta}_{1})- h(O^{\prime},\mathbf{\theta}_{2})\|]+\|\mathbb{E}_{\mathbf{\theta}_{1}}[h(O^{\prime},\mathbf{ \theta}_{2})]-\mathbb{E}_{\mathbf{\theta}_{2}}[h(O^{\prime},\mathbf{\theta}_{2})]\|\] \[\leq L_{h}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+\|\mathbb{E}_{\mathbf{ \theta}_{1}}[h(O^{\prime},\mathbf{\theta}_{2})]-\mathbb{E}_{\mathbf{\theta}_{2}}[h(O^{ \prime},\mathbf{\theta}_{2})]\|\] \[\leq L_{h}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|+2BU_{r}d_{TV}(\mu_ {\mathbf{\theta}_{1}}\otimes\pi_{\mathbf{\theta}_{1}},\mu_{\mathbf{\theta}_{2}}\otimes\pi_ {\mathbf{\theta}_{2}})\] \[\leq[L_{h}+2BU_{r}|\mathcal{A}|L_{\pi}(1+\lceil\log_{\rho}m^{-1} \rceil+\frac{1}{1-\rho})]\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|\] \[\leq 2L_{h}\|\mathbf{\theta}_{1}-\mathbf{\theta}_{2}\|.\]

Then we have \(\nabla J(\mathbf{\theta})\) is \(L_{J}\)-bounded and \(L_{J^{\prime}}\)-Lipschitz; \(h(O,\mathbf{\theta})-\mathbb{E}_{\mathbf{\theta}}[h(O^{\prime},\mathbf{\theta})]\) is \(3L_{h}\)-Lipschitz and \(2U_{\delta}B\)-bounded. By the triangle inequality, we have

\[\Theta(O_{t},\mathbf{\theta}_{t})-\Theta(O_{t},\mathbf{\theta}_{t-\tau})\leq(2U_{ \delta}BL_{J^{\prime}}+3L_{J}L_{h})\|\mathbf{\theta}_{t}-\mathbf{\theta}_{t-\tau}\|\]

**Step 2:** show that for \(t\geq\tau>0\), we have

\[|\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t-\tau})-\Theta(\widetilde{O}_{t},\mathbf{ \theta}_{t-\tau})]|\leq 2U_{\delta}BL_{J}|\mathcal{A}|L_{\pi}\sum_{k=t-\tau}^{t}\| \mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\| \tag{45}\]

By definition of \(\Theta(O,\mathbf{\theta})\), we have

\[|\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t-\tau})-\Theta(\widetilde {O}_{t},\mathbf{\theta}_{t-\tau})]|\] \[=|\mathbb{E}[\langle\nabla J(\mathbf{\theta}_{t-\tau}),h(\widetilde{O }_{t},\mathbf{\theta}_{t-\tau})-h(O_{t},\mathbf{\theta}_{t-\tau})]|\] \[=|\mathbb{E}[\langle\nabla J(\mathbf{\theta}_{t-\tau}),h(\widetilde {O}_{t},\mathbf{\theta}_{t-\tau})\rangle-\mathbb{E}[\langle\nabla J(\mathbf{\theta}_{t- \tau}),h(O_{t},\mathbf{\theta}_{t-\tau})\rangle]|\] \[\leq 4U_{\delta}BL_{J}d_{TV}(\mathbb{P}(O_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}),\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{ \theta}_{t-\tau})), \tag{46}\]

where the inequality comes from the definition of total variation distance. The total variation norm between \(O_{t}\) and \(\widetilde{O}_{t}\) has been computed in (33). Plugging (33) into (46), we get

\[|\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t-\tau})-\Theta(\widetilde{O}_{t},\mathbf{ \theta}_{t-\tau})]|\leq 2U_{\delta}BL_{J}|\mathcal{A}|L_{\pi}\sum_{k=t-\tau}^{t}\| \mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\|.\]

**Step 3:** show that for \(t\geq\tau>0\), we have

\[|\mathbb{E}[\Theta(\widetilde{O}_{t},\mathbf{\theta}_{t-\tau})-\Theta(O^{\prime}_{t}, \mathbf{\theta}_{t-\tau})]|\leq 4U_{\delta}BL_{J}m\rho^{\tau-1}. \tag{47}\]

From the definition of \(\Theta(O,\mathbf{\theta})\), we have

\[|\mathbb{E}[\Theta(\widetilde{O}_{t},\mathbf{\theta}_{t-\tau})-\Theta(O ^{\prime}_{t},\mathbf{\theta}_{t-\tau})]| =|\mathbb{E}[\langle\nabla J(\mathbf{\theta}_{t-\tau}),h(O^{\prime}_{t },\mathbf{\theta}_{t-\tau})\rangle-\langle\nabla J(\mathbf{\theta}_{t-\tau}),h( \widetilde{O}_{t},\mathbf{\theta}_{t-\tau})\rangle]|\] \[\leq 4U_{\delta}BL_{J}d_{TV}(\mathbb{P}(\widetilde{O}_{t}\in \cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau}),\mu_{\mathbf{\theta}_{t-\tau}}\otimes\pi_{ \mathbf{\theta}_{t-\tau}}\otimes\mathcal{P}).\]The inequality is due to the definition of total variation distance. From Assumption 3.2, we know that

\[d_{TV}(\mathbb{P}(\widetilde{s}_{t}\in\cdot),\mu_{\mathbf{\theta}_{t-\tau}})\leq m\rho ^{\tau-1}.\]

We also have the fact that

\[\mathbb{P}(\widetilde{O}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau})=\mathbb{ P}(\widetilde{s}_{t}\in\cdot|s_{t-\tau+1},\mathbf{\theta}_{t-\tau})\otimes\pi_{\mathbf{ \theta}_{t-\tau}}\otimes\mathcal{P}.\]

Therefore, we have

\[|\mathbb{E}[\Theta(\widetilde{O}_{t},\mathbf{\theta}_{t-\tau}-\Theta(O^{\prime}_{t},\mathbf{\theta}_{t-\tau})]|\leq 4U_{\delta}BL_{J}m\rho^{\tau-1}.\]

Combining (44), (45), and (47), we can decompose the Markovian bias as

\[\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t})] =\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t})-\Theta(O_{t},\mathbf{\theta }_{t-\tau})]\] \[\quad+\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t-\tau})-\Theta( \widetilde{O}_{t},\mathbf{\theta}_{t-\tau})]\] \[\quad+\mathbb{E}[\Theta(\widetilde{O}_{t},\mathbf{\theta}_{t-\tau})],\]

where \(\widetilde{O}_{t}\) is from the auxiliary Markovian chain defined in (10) and \(O^{\prime}_{t}\) is from the stationary distribution which satisfies \(\mathbb{E}[\Theta(O^{\prime}_{t},\mathbf{\theta}_{t-\tau})]=0\).

Then we have

\[\mathbb{E}[\Theta(O_{t},\mathbf{\theta}_{t})] \leq(2U_{\delta}BL_{J^{\prime}}+3L_{J}L_{h})\mathbb{E}\|\mathbf{ \theta}_{t}-\mathbf{\theta}_{t-\tau}\|\] \[\quad+2U_{\delta}BL_{J}|\mathcal{A}|L_{\pi}\sum_{k=t-\tau}^{t}\| \mathbf{\theta}_{k}-\mathbf{\theta}_{t-\tau}\|+4U_{\delta}BL_{J}m\rho^{\tau-1}\] \[\leq(2U_{\delta}BL_{J^{\prime}}+3L_{J}L_{h})\sum_{k=t-\tau+1}^{t} \mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_{k-1}\|\] \[\quad+2U_{\delta}BL_{J}|\mathcal{A}|L_{\pi}\sum_{k=t-\tau+1}^{t} \sum_{j=t-\tau+1}^{k}\mathbb{E}\|\mathbf{\theta}_{j}-\mathbf{\theta}_{j-1}\|+4U_{ \delta}BL_{J}m\rho^{\tau-1}\] \[\leq(2U_{\delta}BL_{J^{\prime}}+3L_{J}L_{h})\sum_{k=t-\tau+1}^{t} \mathbb{E}\|\mathbf{\theta}_{k}-\mathbf{\theta}_{k-1}\|\] \[\quad+2U_{\delta}BL_{J}|\mathcal{A}|L_{\pi}\tau\sum_{j=t-\tau+1}^ {t}\mathbb{E}\|\mathbf{\theta}_{j}-\mathbf{\theta}_{j-1}\|+4U_{\delta}BL_{J}m\rho^{ \tau-1}\] \[\leq D_{1}(\tau+1)\sum_{k=t-\tau+1}^{t}\mathbb{E}\|\mathbf{\theta}_{k }-\mathbf{\theta}_{k-1}\|+D_{2}m\rho^{\tau-1}\] \[\leq D_{1}(\tau+1)^{2}G\alpha+D_{2}m\rho^{\tau-1}\]

where \(D_{1}=\max\{U_{\delta}BL_{J^{\prime}}+3L_{J}L_{h},2U_{\delta}BL_{J}|\mathcal{ A}|L_{\pi}\}\) and \(D_{2}=4U_{\delta}BL_{J}\). Thus we conclude the proof. 

## Appendix E IID Sampling Analysis

```
1:Input initial actor parameter \(\mathbf{\theta}_{0}\), initial critic parameter \(\mathbf{\omega}_{0}\), initial reward estimator \(\eta_{0}\), stepsize \(\alpha_{t}\) for actor, \(\beta_{t}\) for critic and \(\gamma_{t}\) for reward estimator.
2:for\(t=0,1,2,\cdots,T-1\)do
3: Sample \(s_{t}\sim\mu_{\mathbf{\theta}_{t}}\)
4: Take the action \(a_{t}\sim\pi_{\mathbf{\theta}_{t}}(\cdot|s_{t})\)
5: Observe next state \(s^{\prime}_{t}\sim\mathcal{P}(\cdot|s_{t},a_{t})\) and the reward \(r_{t}=r(s_{t},a_{t})\)
6:\(\delta_{t}=r_{t}-\eta_{t}+\mathbf{\phi}(s^{\prime}_{t})^{\top}\mathbf{\omega}_{t}-\mathbf{ \phi}(s_{t})^{\top}\mathbf{\omega}_{t}\)
7:\(\eta_{t+1}=\eta_{t}+\gamma_{t}(r_{t}-\eta_{t})\)
8:\(\mathbf{\omega}_{t+1}=\Pi_{U_{\infty}}(\mathbf{\omega}_{t}+\beta_{t}\delta_{t}\mathbf{\phi} (s_{t}))\)
9:\(\mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}+\alpha_{t}\delta_{t}\nabla_{\mathbf{\theta}}\log \pi_{\mathbf{\theta}_{t}}(a_{t}|s_{t})\)
10:endfor
```

**Algorithm 2** Single-timescale Actor-Critic (i.i.d. sampling)

[MISSING_PAGE_FAIL:33]