# Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models

 Xu Yang\({}^{1,2}\), Yingzhe Peng\({}^{1,2}\), Haoxuan Ma\({}^{1,2}\), Shuo Xu\({}^{1,2}\),

Chi Zhang\({}^{3}\), Yucheng Han\({}^{4}\), Hanwang Zhang\({}^{4}\)

\({}^{1}\) Southeast University

\({}^{2}\) Key Laboratory of New Generation Artificial Intelligence Technology & Its Interdisciplinary Applications, (Southeast University),Ministry of Education

\({}^{3}\) Westlake University

\({}^{4}\) Nanyang Technological University

{xuyang_palm, yingzhe.peng, haoxuan-ma, xushuo}@seu.edu.cn

chizhang@westlake.edu.cn, yucheng002@e.ntu.edu.sg, hanwangzhang@ntu.edu.sg

###### Abstract

As Archimedes famously said, "Give me a lever long enough and a fulcrum on which to place it, and I shall move the world", in this study, we propose to use a tiny Language Model (LM), _e.g._, a Transformer with 67M parameters, to lever much larger Vision-Language Models (LVLMs) with 9B parameters. Specifically, we use this tiny **Lever-LM** to configure effective in-context demonstration (ICD) sequences to improve the In-Context Learning (ICL) performance of LVLMs. Previous studies show that diverse ICD configurations like the selection and ordering of the demonstrations heavily affect the ICL performance, highlighting the significance of configuring effective ICD sequences. Motivated by this and by re-considering the the process of configuring ICD sequence, we find this is a mirror process of human sentence composition and further assume that effective ICD configurations may contain internal statistical patterns that can be captured by Lever-LM. Then a dataset with effective ICD sequences is constructed to train Lever-LM. After training, given novel queries, new ICD sequences are configured by the trained Lever-LM to solve vision-language tasks through ICL. Experiments show that these ICD sequences can improve the ICL performance of two LVLMs compared with some strong baselines in Visual Question Answering and Image Captioning, validating that Lever-LM can really capture the statistical patterns for levering LVLMs. The code is available at https://github.com/ForJadeForest/Lever-LM.

## 1 Introduction

With the escalation in model size and training data [1, 2, 3, 4, 5, 6], Large Language Models (LLMs) emerge the ability of In-Context Learning (ICL) [7, 8, 9]. ICL, akin to few-shot learning [10, 11, 12], utilizes a few exemplary In-Context Demonstrations (ICDs) to adapt LLMs to new tasks without gradient updates. This achievement in NLP has inspired researchers to similarly enhance Large Vision-Language Models (LVLMs) with ICL capabilities [13, 14]. However, just as in NLP, the effectiveness of ICL in LVLMs is significantly influenced by the configurations of ICDs, such as their selection and ordering [15, 16, 17, 18, 19, 20, 21, 22, 23]. Recent studies [24, 25, 26] have shown that this sensitivity in LVLMs is further exacerbated by the multimodal combinatorial complexity of vision and language data.

In NLP, researchers employ various strategies to optimize in-context sequences to improve ICL performance, including retrieving representative examples as the ICDs [15, 27, 28] and re-ordering these ICDs based on specific principles [29, 21]. While these methods have shown improvements,their application remains largely confined to NLP and is less explored in the vision-language domain. Moreover, as shown in Fig. 1(a), the independent operations of retrieval and reordering often result in sub-optimal outcomes. A critical reconsideration of the ICD sequence generation reveals that configuring an optimal ICD sequence should be a coherent process. Instead of independently selecting and re-ordering, each ICD should be chosen conditionally based on the previous ICDs. This mirrors the sequential nature of human sentence composition, where each word is sequentially selected to ensure overall fluency. Such fluency can be characterized by temporal statistical patterns, which allows the design of statistical learning methods to model and learn from data, where Language Model is one typical technique that demonstrates the effectiveness. This analogy supports the hypothesis that optimal ICD sequences may contain inherent temporal statistical patterns.

Although not explicitly stated, some previous studies in NLP work toward this direction by calculating statistical metrics like the perplexity [30] or the entropy [31, 32] to discover what statistical characteristics a good prompt should have. However, the requirement to know the probability of each token when calculating these metrics limits their applicability in VL. This is because recently proposed LVLMs [13, 14, 33] use continuous image patches rather than the tokenized discrete elements as the vision input, rendering these techniques used in NLP inapplicable in VL and the following two questions remain unaddressed: (1) whether effective VL ICDs exhibit certain statistical patterns and (2) whether such patterns can be leveraged to compose new ICD sequences for a given query. This study aims to address these questions. Specifically, we employ a tiny Language Model, _e.g._, a Transformer, to capture the inherent statistic patterns. This tiny LM is named as "**Lever-LM**" since it can lever/control a much larger VLM by composing suitable ICD sequence. Compared with the classic LM, the only difference is that the vocabulary of Lever-LM consists not of standard words, but of examples from the supporting set that will be used as ICDs.

Fig. 1 (b) shows the training pipeline for Lever-LM. Initially, a "ground-truth" dataset is constructed to indicate which examples or their orders can form good ICD sequences. Specifically, we employ a frozen LVLM to evaluate if an ICD sequence facilitates accurate predictions for a given query, _e.g._, answering questions correctly or generating appropriate captions. 1 Lever-LM is then trained to concurrently learn the selection and ordering of ICDs, streamlining the process by eliminating the need for two separate stages typical of previous methods. Our experiments, conducted with two LVLMs--Open-Flamingo [34] and IDEFICS [14]--on classic VL tasks--Image Captioning and Visual Question Answering-- demonstrate that Lever-LM surpasses several strong baselines, including those that retrieve ICDs based on image similarity. These results confirm that effective ICD sequences contain inherent temporal statistical patterns and such patterns can be learned for composing new ICD sequences for test queries.

Footnote 1: Note: This method requires ground-truth labels and thus can not be used at the test stage.

Besides the above-mentioned advantages, Lever LM emerges two interesting abilities. First, it has strong length extrapolation ability, _e.g._, when trained on a dataset with only 2-shot ICDs, Lever LM can generate 4 or more-shot ICDs that outperform several strong baselines. Second, Lever LM can construct a "golden" ICD sequence of 8 predetermined ICDs in a fixed order. This sequence can be uniformly applied across different test queries to assist LVLM in label generation, thereby reducing the computational overhead for configuring new ICD sequences for each query. Experiments in IC/VQA tasks show that "golden" ICD sequence achieves 6.91/1.24 improvements compared to

Figure 1: (a) The traditional ICD configuration methods separately select and order the ICDs, leading to sub-optimal ICL performance. (b) Our Lever-LM enables the step-by-step generation of ICD configurations and simultaneously considers the selection of ICDs and the ordering of ICD sequences.

a strong baseline. In addition, we use exhaustive ablations, including applying different ways to construct training set and changing the architecture of Lever-LM, to discover which factors and analyze why they will affect the ICL performance.

## 2 Related Work

**Models with In-Context Learning Ability.** Prompt engineering enables Large Language Models (LLMs) to address downstream tasks without the need for fine-tuning [35; 1; 36]. A variant, ICL, enhances this ability by constructing prompts with a few examples. This has been demonstrated in LLMs such as GPT-3 [1], LLaMA [5], and MPT [37]. Recently, witnessing such success in NLP, the VL domain has also developed numerous LVLMs with prompt engineering abilities [38; 39; 13; 40; 41; 42; 43; 44] and ICL ability like [40], Flamingo [13], and IDEFICS [14] Among them, we use Flamingo and IDEFICS as the LVLMs to explore the effectiveness of Lever-LM since they have stronger and more robust ICL ability by using better language encoders and more training data2.

Footnote 2: Since Flamingo does not open-source the model, we use an unofficial implementation, OpenFlamingo [34].

**Configuring In-Context Demonstrations.** Although ICL assists LLMs in better adapting to downstream tasks, its performance is highly sensitive to the selection [19; 20; 15] and ordering [21; 22; 23] of ICDs. Numerous studies have explored diverse methods to select ICDs in the NLP field [45; 46; 47; 48; 29]. For example, [15] selects ICDs based on the embedding similarity between ICDs and test samples where the embeddings are extracted from an existing language encoder. Such a method is further developed by training an encoder specifically for selection [49; 50; 51; 52; 53; 50].

Regarding the ordering of ICDs, researchers calculate diverse statistical-based metrics to measure the quality of ICD configurations, _e.g._, the Minimal Description Length [29] and Global and Local Entropy [21]. Besides them, researchers focus more on discovering the statistical patterns of good prompts. For example, [30] uses perplexity to measure which prompts can better help LLMs perform a task. Furthermore, [31] unifies diverse statistics-based prompt selection methods [54; 55] from the perspective of mutual information and discover that mutual information or its variants can uncover certain statistical patterns of effective prompts. However, these statistical-based methods require to calculate the token probabilities, making them infeasible to address continuous image patches, thus can not be used in VL.

Besides these NLP studies, in VL, [24] and [25] explore diverse ICD configurations in IC and VQA, while only the heuristic-based methods are used for selecting ICDs and do not consider the ordering. In contrast, our Lever-LM can simultaneously learn how to select and reorder the samples and moreover, our Lever-LM is model-specific.

## 3 Lever Language Model

In this section, we introduce how to build Lever Language Model (Lever-LM) for configuring the ICD sequence to lever a given LVLM. First, we briefly introduce the formulations of ICL for Vision-Language (VL) tasks. Then we introduce the construction of the dataset used to train Lever-LM. After that, we show the architecture of Lever-LM and briefly discuss how to train Lever-LM and use it to configure ICD sequences.

**The Formulation of In-Context Learning (ICL).** Given a query input \(\mathbf{x}^{\prime}\), ICL predicts the corresponding output \(\mathbf{y}^{\prime}\) using a well-trained foundation model \(\mathcal{M}\), conditioned on the concatenation of an in-context sequence \(\mathcal{S}\) and this query. We denote a in-context sequence with \(K\)-shot ICDs \(\hat{\bm{d}}\) as \(\mathcal{S}^{K}=\{\hat{\bm{d}}_{1},\hat{\bm{d}}_{2},...,\hat{\bm{d}}_{K}\}\). Then ICL can be formulated as:

\[\mathbf{y}^{\prime}\leftarrow\bm{P}_{\mathcal{M}}(\mathbf{y}^{\prime}\mid \mathcal{S}^{k},\mathbf{x}^{\prime}),\] (1)

where \(\bm{P}_{\mathcal{M}}\) denotes the predicted probability of \(\mathcal{M}\) and "\(\leftarrow\)" represents the decoding strategy, _e.g._, beam search. For each \(\hat{\bm{d}}\), it is selected from a supporting set \(\mathcal{D}_{S}=\{\bm{d}_{1},...,\bm{d}_{N}\}\), where each sample \(\bm{d}_{i}=(\mathbf{x}_{i},\mathbf{y}_{i})\): \(\mathbf{x}_{i}\) and \(\mathbf{y}_{i}\) respectively denote the input and the corresponding label. It is noteworthy that in diverse VL tasks, \(\mathbf{x}\) and \(\mathbf{y}\) have different forms. For instance, in Image Captioning (IC), \(\mathbf{x}\) is the image and \(\mathbf{y}\) is the caption; and in Vision Question Answering (VQA), \(\mathbf{x}\) contains the image and the question, while \(\mathbf{y}\) is the answer.

**Constructing the Training Dataset.** To train Lever-LM for generating effective ICD sequences for a given LVLM \(\mathcal{M}\), we should first construct a dataset \(\mathcal{D}_{\mathcal{M}}\) containing high-quality ICD sequences for different query inputs. Simply, we use one VL dataset--COCO [56] from IC-- to show how to construct \(\mathcal{D}_{\mathcal{M}}\), which is shown in Fig. 2. Formally, given a dataset \(\mathcal{D}\) which is already split into the training part \(\mathcal{D}_{R}\) and the test part \(\mathcal{D}_{E}\), we build \(\mathcal{D}_{\mathcal{M}}\) only from \(\mathcal{D}_{R}\). As Fig. 2 (a.1) shows, initially, we randomly select \(n\) samples from \(\mathcal{D}_{R}\) to form an anchor set \(\mathcal{A}\). Then for each sample \(\bm{a}_{m}=\{\bm{x}_{m},\bm{y}_{m}\}\in\mathcal{A}\), we construct a \(K\)-shot in-context sequence \(\mathcal{S}_{m}^{K}\) for it. Then \(\mathcal{D}_{\mathcal{M}}=\{(\bm{a}_{1},\mathcal{S}_{1}^{K}),(\bm{a}_{2}, \mathcal{S}_{2}^{K}),...,(\bm{a}_{M},\mathcal{S}_{M}^{K})\}\) where each training sample contains an query \(\bm{a}_{m}\) and the corresponding \(K\)-shot in-context sequence \(\mathcal{S}_{m}^{K}\).

To avoid confusion, we remove the subscript \(m\) in following texts. To construct \(\mathcal{S}^{K}=\{\bm{d}_{1},...,\bm{d}_{K}\}\), we need to select \(K\)-shot samples from the supporting set \(\mathcal{D}_{S}\), which is set to the complement set of \(\mathcal{A}\) in \(\mathcal{D}_{R}\): \(\mathcal{D}_{R}\backslash\mathcal{A}\). Meantime, we also need to decide which samples should be selected in turn. To achieve this, given the anchor sample \(\bm{a}=\{\bm{x},\bm{y}\}\) and the partially constructed in-context sequence, _e.g._, a \(k-1\)-shot \(\mathcal{S}^{k-1}\), we need to know that after adding which sample \(\bm{d}\in\mathcal{D}_{S}\), the ICL performance improvement can be maximized by applying the given LVLM \(\mathcal{M}\):

\[\hat{\bm{d}}_{k}=\operatorname*{arg\,max}_{\bm{d}\in\mathcal{D}_{S}}I_{ \mathcal{M}}(\{\bm{d},\mathcal{S}^{k-1}\},\bm{a})-I_{\mathcal{M}}(\mathcal{S }^{k-1},\bm{a}),\] (2)

where \(I_{\mathcal{M}}\) is one kind of ICL performance measurement related to \(\mathcal{M}\). Note that Eq. (2) actually uses the greedy sampling method to select the samples every time, while we can use beam search here to further achieve a better solution. Additionally, to improve the diversity of the dataset, we will keep the top-\(b\) highest-scoring ICD sequences \(\{S_{1}^{K},S_{2}^{K},...,S_{b}^{K}\}\) at the last iteration for each \(a\), where \(b\) is equal to the beam size. For example, when setting beam size to \(5\) as shown in Fig. 2 (a.3), we can get \(5\) diverse high-quality ICD sequences for an anchor sample.

Intuitively, for diverse tasks, we can use the corresponding "golden measurement" as \(I_{\mathcal{M}}\), _e.g._, setting it to CIDEr [57]/accuracy for IC/VQA. However, this strategy encounters two limitations. The first one is that for diverse VL tasks, we need diverse corresponding measurements, which is inconvenient. Second, some "golden measurements" may be impractical to deploy. For example, for IC, calculating CIDEr requires the LVLM to forward multiple times to sample an integral sentence, and then it costs expensive time burdens to construct the dataset. While for VQA, accuracy is a binary value (accuracy=1 when correct and 0 when wrong), then maybe lots of candidates in \(\mathcal{D}_{S}\) will make the accuracy change from 0 to 1 and then it is hard to judge which one of them is the most suitable one.

To overcome these two limitations, we use a relatively general measurement as \(I_{\mathcal{M}}\). Formally, since we have the ground-truth results \(\bm{y}\) of the anchor sample, we can use the given LVLM \(\mathcal{M}\) to measure the prediction confidence of \(\bm{y}\) given the input \(\bm{x}\) and the in-context sequence \(\mathcal{S}^{K}\):

\[I_{\mathcal{M}}(\mathcal{S}^{K},\bm{a})=P_{\mathcal{M}}(\bm{y}|\mathcal{S}^{K },\bm{x})=\prod_{t}P_{\mathcal{M}}(y^{(t)}|\mathcal{S}^{K},\bm{x},y^{(1:t-1)}).\] (3)

In VL tasks, the ground-truth label \(\bm{y}=\{y^{(1)},...,y^{(T)}\}\) is a sequence, thus we can decompose the probability distribution into a series of productions. Then Eq. (2) selects a sample that can further maximize the prediction confidence given the query input and the current in-context sequence.

Figure 2: (a): The pipeline of constructing \(\mathcal{D}_{\mathcal{M}}\). Darker color of \(S_{i,j}^{K}\) indicates a higher score given by Eq. 2. (b): Top: Lever-LM is a two-layer Transformer. Bottom: Each input embeddings is the sum of the random initialized learnable embeddings, the image and text embeddings extracted by CLIP. The dotted block means that some tasks do not exist the text input, _e.g._, IC.

In implementation, \(\mathcal{D}_{S}\) usually contains huge amounts of samples, _e.g._, \(\mathcal{D}_{S}\) in COCO [56] contains about \(10^{5}\) samples. However, we need to calculate Eq. (2) for each \(\bm{d}\in\mathcal{D}_{S}\) when selecting \(\hat{\bm{d}}_{k}\) for each \(\bm{a}\in\mathcal{A}\), which means the whole process of building \(\bm{D}_{\mathcal{M}}\) is quite time-consuming. To alleviate the cost, as shown in Fig. 2 (a.2), for each specific \(\bm{a}\), we narrow the set size by sampling a much smaller subset \(\mathcal{D}_{S}^{\bm{a}}\), _e.g._, containing 64 samples \(\mathcal{D}_{S}^{\bm{a}}=\{d_{\bm{a}}^{1},d_{\bm{a}}^{2},...,d_{\bm{a}}^{64}\}\), from \(\mathcal{D}_{S}\) for selecting \(\hat{\bm{d}}_{k}\). We use diverse sampling strategies to construct this subset, _e.g._, retrieving some samples similar to \(\bm{a}\), and implement exhaustive ablation studies to explore which strategies are useful in Section 4.3.

**Training Lever-LM.** After getting \(\mathcal{D}_{\mathcal{M}}\), we use it to train Lever-LM, as Fig. 2(b) shows, it is a two-layer tiny Transformer [58]. The primary difference between Lever-LM and the traditional LM lies in the tokens of the vocabulary, whose tokens are the samples from the supporting set \(\mathcal{D}_{S}\), _e.g._, the first token corresponds to the first sample in \(\mathcal{D}_{S}\). Then given the query sample, the ICDs can be selected one by one based on the token distribution produced by the trained Lever-LM, just as when composing a sentence, the words are selected one by one from the word vocabulary.

Besides the tokens from \(\mathcal{D}_{S}\), three special tokens are added into the vocabulary to help configure the ICD sequence, which are [BOS], [EOS], and [QUERY], respectively representing the beginning of a sequence, the end of a sequence, and the query sample. Given a data sample \((\mathcal{S}^{K}=\{\bm{d}_{1},...,\bm{d}_{K}\},\bm{x}^{\prime})\) from \(\mathcal{D}_{\mathcal{M}}\) where \(\mathcal{S}^{K}\) is the ICD sequence and \(\bm{x}^{\prime}\) is the query input, we reformulate it into \(\{\texttt{[BOS]},\texttt{[QUERY]}+\bm{x}^{\prime},\bm{d}_{1},...,\bm{d}_{K}, \texttt{[EOS]}\}\) where \(\texttt{[QUERY]}+\bm{x}^{\prime}\) denotes to add two embeddings. This reformulated sequence is input into Lever-LM for training.

To train Lever-LM, we should embed the tokens of the vocabulary to get dense embeddings. Since each token contains both image and text, we use the vision encoder \(F_{I}(\cdot)\) and the language encoder \(F_{T}(\cdot)\) of CLIP [59] to embed the image and text, respectively. Meanwhile, we add each of these embeddings with a learnable part \(\bm{r}_{i}\) that is randomly initialized. Then for the \(i\)-th token \(\bm{d}_{i}=(I_{i},T_{i})\) in the vocabulary where \(I_{i}/T_{i}\) are the corresponding image/ text, its token embedding is \(\bm{e}_{i}\):

\[\bm{e}_{i}=F_{I}(I_{i})+F_{T}(T_{i})+\bm{r}_{i}.\] (4)

Note that \(T_{i}\) varies between IC and VQA tasks where it denotes caption in IC and question in VQA.

For test query \(x^{\prime}\), we use the same vision and language encoders to embed it. For VQA, the image and question are embedded and summed, while for IC, only the image is embedded. Lastly, we use the cross-entropy loss for training as a standard LM that given the previously \(k-1\) tokens, we maximize the probability of the \(k\)-th ground-truth token.

**Configuring the ICD Sequence to Lever LVLM.** After training Lever-LM, we use it to configure the ICD sequence. Given a query sample \(x^{\prime}\), we initialize the input sequence as \(\{\texttt{[BOS]},\texttt{[QUERY]}+e_{x^{\prime}}\}\) and then generate the ICDs one by one, where \(e_{x^{\prime}}\) is the embedding of \(x^{\prime}\) computed by Eq. (4). After iteratively sampling \(K\)-shot ICDs, we can compose the corresponding in-context sequence \(\mathcal{S}^{K}\) for \(x^{\prime}\) and then use Eq. (1) to implement the ICL.

## 4 Experiments

### Datasets and implementation details

Our approach is evaluated on MS-COCO [56] for Image Captioning (IC) and VQAV2 [60] for Visual Question Answering (VQA). For each corresponding dataset, we use the train split to construct the \(\mathcal{D}_{\mathcal{M}}\) and use the validation split to evaluate the performance of ICD configurations generated by Lever-LM. More details are given in Appendix A.

To get \(\mathcal{D}_{\mathcal{M}}\), we select \(5000\) samples to get the anchor set \(\mathcal{A}\). For each anchor sample, we randomly choose \(64\) samples to build the sub-supporting set \(\mathcal{D}_{S}^{\bm{a}}\). The beam size for these processes is 5. To train Lever-LM, different strategies are employed for IC and VQA. In IC, the weight of CLIP model will be frozen, and an MLP adapter is introduced to its output. While, for VQA, the CLIP encoder remains trainable, and no adapter is appended. The training phase leverages the AdamW optimizer [61] and a cosine learning rate scheduler. We set the learning rate to \(1\times 10^{-4}\) and the batch size to 128. We train our Lever-LM for 20 epochs. To implement ICL, we use OpenFlamingoV2-9B [34] and IDEFICS-9B [14] as our LVLMs. We use beam search during inference where the beam size is set to 3. Besides, we set the maximum number of generated tokens as 20 in IC and 5 in VQA.

### Results and Analyses

#### 4.2.1 Comparison Methods

We compare Lever-LM with 4 ICD selection strategies:

**Random Sample (RS)**: RS constructs \(\mathcal{S}^{k}\) by randomly selecting and ordering \(k\) ICDs from \(\mathcal{D}_{S}\).

**Similarity-based Retrieval methods**: To date, only a few studies focus on configuring ICD sequence for solving VL tasks [24; 25], where both studies show that, despite their simplicity, similarity-based retrieval methods are effective for selecting ICDs. We therefore consider these strategies as current SOTA benchmarks to assess our effectiveness. 3. They form \(\mathcal{S}^{k}\) by computing the cosine similarity between the query input \(\mathbf{x}^{\prime}\) and ICDs in \(\mathcal{D}_{S}\) where CLIP is used to extract features. We follow [25] to sort examples in ascending order by their similarity to the query input, so the rightmost demonstration is the closest example. Similarity-based methods contain three variants: (1). **Similarity-based Image-Image Retrieval (SIIR):** We select \(k\) ICDs from \(\mathcal{D}_{S}\) with highest image similarity to the query image. (2). **Similarity-based Text-Text Retrieval (STTR):** We select \(k\) ICDs from \(\mathcal{D}_{S}\) with highest text similarity to the query text. This method is only applicable to VQA where question is used as text and not infeasible for IC. (3). **Similarity-based Image-Text Retrieval (SITR):** We compute the similarity between query image and all text of \(d_{i}\in\mathcal{D}_{S}\) and select ICDs whose texts have the top-\(k\) similarities with the query image. For IC/VQA, we use caption/question for IC/VQA.

Footnote 3: Note that we use different experiment settings from [24] and more details are given in Appendix A.4

#### 4.2.2 Main Result

The results for various ICD selection strategies are shown in Table 1 for IC and VQA. For Lever-LM, it is trained by \(\mathcal{D}_{\mathcal{M}}\) whose ICD length is set to 2. Due to increased inference time with more shots, we do not test the inference results for 5- and 7-shots. The table shows the length interpolation and extrapolation ability of Lever-LM. Interpolation refers to performance with ICDs shorter than those in the training set \(\mathcal{D}_{\mathcal{M}}\), which contains only 2-shot ICDs, and is denoted as "Avg:1\(\sim\)2". Extrapolation pertains to performance with ICDs longer than those in \(\mathcal{D}_{\mathcal{M}}\), represented as "Avg:3\(\sim\)8". The notation "Avg:1\(\sim\)8" indicates overall performance across 1 to 8 shots. Future analysis will focus on comparing these averages to minimize potential bias across methods.

Overall, Lever-LM achieves the best performance on most cases compared to other methods on both LVLMs. Notably, Lever-LM excels in Avg:1\(\sim\)2. Specifically, in VQA, Lever-LM surpasses the best performing SIIR method by 3.07 (48.75 vs. 45.68) and 0.57 (53.65 vs. 53.08) in accuracy on the IDEFICS and OpenFlamingo models, respectively. In IC, Lever-LM outperforms the best baseline, SIIR, by 6.03 (84.32 vs. 78.29) CIDEr on OpenFlamingo. Similarly, for IDEFICS, Lever-LM achieves a higher CIDEr of 3.2 (89.57 vs. 86.37) compared to the best baseline, RS.

Moreover, Lever-LM has remarkable extrapolation abilities. Regarding Avg:3\(\sim\)8, Lever-LM maintains the top performance in both IC and VQA. Specially, on OpenFlamingo, Lever-LM outperforms SIIR with a 0.8 higher CIDEr in IC (96.52 vs. 95.72) and a 0.75 greater accuracy in VQA (52.59 vs. 51.84). Meanwhile, on IDEFICS, compared with RS, Lever-LM achieves a 2.93 higher CIDEr in IC (105.79 vs. 102.86) and 0.49 higher accuracy in VQA (54.84 vs. 54.35). These results indicate that _Lever-LM can effectively identify and utilize internal statistical patterns to compose longer, high-quality ICD sequences, even from a dataset comprising only two shots._

When we delve deeper into the results in Table 1, we find that the relative performance of similarity-based methods and RS varies by LVLM and task. For example, for IC, SIIR outperforms RS ( Avg:1\(\sim\)8: 89.91 vs. 88.48) when OpenFlamingo is used to implement ICL while SIIR significantly lags behind RS (Avg:1\(\sim\)8: 88.19 vs. 97.36) when IDEFICS is used. Also, for VQA, STTR is comparable to RS (Avg:1\(\sim\)8: 47.98 vs. 47.94) on OpenFlamingo while STTR is defeated by RS (Avg:1\(\sim\)8: 49.75 vs. 53.54) when IDEFICS is used. These performance fluctuations demonstrates the instability of these heuristic-based methods. However, Lever-LM does not have such serious fluctuations where it outperforms both RS and similarity-based retrieval methods across various LVLMs and tasks on average. Such observations also suggest that _Lever-LM may capture the stable statistic patterns between ICDs_.

Besides the above-mentioned advantages, Fig. 3 shows that _Lever-LM is more robust to the Short-cut Inference brought by using similarity-based retrieval methods [24; 25]_. For example, in (a) and

[MISSING_PAGE_FAIL:7]

Eq. (3) to build \(D_{\mathcal{M}}\) may introduce certain in-domain bias which is beneficial for interpolation while detrimental for extrapolation on IC.

For different sample methods of constructing the \(\mathcal{D}_{S}^{\alpha}\) in table 2 (9) \(\sim\) (11), we find Random is the best in both IC and VQA. We suppose this is because selecting similar ICDs with the anchor sample from \(\mathcal{D}_{S}\) will damage the diversity. Previous study [62] in NLP validates that the diversity of the ICD sequences will also help improve the performance of LLMs.

**Diverse scorers \(I_{\mathcal{M}}\) for evaluating ICD sequences.** To evaluate the quality of ICD configurations, we can use task-specific metrics as \(I_{\mathcal{M}}\) to build \(\mathcal{D}_{\mathcal{M}}\), such as CIDEr in IC. Table 2 (2) and (12) compare the results between using prediction confidence Eq. 3 (2) and CIDEr (12) as \(I_{\mathcal{M}}\). We find that using CIDEr achieves 3.61 higher than Confidence in Avg:1\(\sim\)2, suggesting that CIDEr can assign a more accurate and reasonable score for ICD configurations. However, the length extrapolation capability decreases obviously, which is 3.0 lower than Confidence in Avg:3\(\sim\)8, validating the robustness of Confidence scorer. Moreover, it will cost more time to construct \(\mathcal{D}_{\mathcal{M}}\) by task-specific metric is used, _e.g._, CIDEr costs approximately 10 times of Confidence when constructing \(\mathcal{D}_{\mathcal{M}}\).

**Diverse LM Structures.** Table 2 (13) shows the results of using LSTM [63] as Lever-LM, we find that this still achieves excellent performance. For example, in IC, the overall performance improves by 3.64 (92.12 vs. 88.48) compared to the RS baseline, while in VQA, it is improved by 1.38 (49.32 vs. 47.94). However, due to the weak representation learning capability of LSTM, its performance is lower than Transformer,_e.g._, the scores decrease by 0.33/1.99 in IC/VQA, respectively. Overall, these results suggest that effective ICD configurations contain internal statistic patterns which can be captured by different temporal learner.

\begin{table}
\begin{tabular}{c|c|c c c|c c c} \hline \hline  & & \multicolumn{3}{c|}{IC} & \multicolumn{3}{c}{VQA} \\ \cline{3-8}  & & **Avg:1\(\sim\)2** & **Avg:3\(\sim\)8** & **Avg:1\(\sim\)8** & **Avg:1\(\sim\)2** & **Avg:3\(\sim\)8** & **Avg:1\(\sim\)8** \\ \hline (1) & RS & 78.14 & 93.65 & 88.48 & 43.95 & 49.94 & 47.94 \\ (2) & Lever-LM & 84.32 & 96.52 & 92.45 & 48.75 & 52.59 & 51.31 \\ \hline (3) & \(b=1\) & 79.91 & 94.53 & 89.66 & 46.47 & 51.13 & 49.58 \\ (4) & \(b=5\) & 84.32 & 96.52 & 92.45 & **48.75** & **52.59** & **51.31** \\ (5) & \(b=10\) & **84.96** & **97.12** & **93.06** & 48.58 & 52.49 & 51.19 \\ \hline (6) & \(n=1000\) & 83.94 & 96.74 & 92.48 & 45.39 & 50.44 & 48.76 \\ (7) & \(n=3000\) & 84.13 & **97.60** & **93.11** & 47.56 & 51.11 & 49.93 \\ (8) & \(n=5000\) & **84.32** & 96.52 & 92.45 & **48.75** & **52.59** & **51.31** \\ \hline (9) & Sim-I & 81.96 & 96.11 & 91.40 & 47.10 & 51.79 & 50.23 \\ (10) & Sim-T & 81.22 & 87.66 & 85.52 & 45.38 & 49.55 & 48.16 \\ (11) & Random & **84.32** & **96.52** & **92.45** & **48.75** & **52.59** & **51.31** \\ \hline (12) & CIDEr Scorer & 87.93 & 93.52 & 91.65 & - & - & - \\ (13) & Lever-LM LSTM & 83.93 & 96.21 & 92.12 & 46.60 & 50.68 & 49.32 \\ (14) & Golden-1 & 81.78 & 97.44 & 92.22 & 47.78 & 52.51 & 50.93 \\ (15) & Golden-2 & 91.20 & 99.63 & 96.82 & 45.32 & 49.05 & 47.80 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of diverse ablation studies on IC and VQA.

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{3}{c|}{IC} & \multicolumn{3}{c}{VQA} \\ \cline{2-7}  & **Avg:1\(\sim\)4** & **Avg:6\(\sim\)8** & **Avg:1\(\sim\)8** & **Avg:1\(\sim\)4** & **Avg:6\(\sim\)8** & **Avg:1\(\sim\)8** \\ \hline RS & 84.41 & 96.62 & 88.48 & 46.25 & 51.31 & 47.94 \\ SITR & 78.06 & 91.71 & 82.61 & 44.32 & 50.24 & 46.29 \\ SIIR & 85.16 & **99.40** & 89.91 & 47.83 & **53.41** & 49.69 \\ STTR & - & - & - & 47.08 & 49.77 & 47.98 \\ Lever-LM(4-shot \(\mathcal{D}_{\mathcal{M}}\)) & **87.35** & 97.96 & **90.88** & **48.56** & 52.68 & **49.93** \\ \hline Lever-LM(2-shot \(\mathcal{D}_{\mathcal{M}}\)) & 89.53 & 98.30 & 92.45 & 50.39 & 53.15 & 51.31 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results of Lever-LM with 4-shot \(\mathcal{D}_{\mathcal{M}}\) on IC and VQA.

**Golden ICD Sequence.** In experiments, we find that using a high learning rate and not freezing the CLIP model may make Lever-LM converge to a specific solution that for any query input, the ICD configuration is fixed while still return good ICL performance. For example, in IC, the best version, Golden-2, can outperform the non-Fixed case (Table 2 (2)) by 4.37 points in Avg:1\(\sim\)8. Such improvement suggest that if we do not have enough computation burdens to configure diverse ICD sequence for each query, we can preserve one Golden ICD Sequence for the latter usage. However, we also find that the performance of Golden ICD Sequence fluctuates significantly, \(e\)._g_., Golden-2 is poorer than RS in VQA in Avg:1\(\sim\)8. This also points out a new direction to study how to get more stable Golden ICD Sequence.

**Longer Few-shot \(\mathcal{D}_{\mathcal{M}}\).** We further explore the performance of Lever-LM using a 4-shot \(\mathcal{D}_{\mathcal{M}}\), as presented in Table 3. It is evident that Lever-LM continues to outperform other retrieval-based methods in Avg:1\(\sim\)8 metric. However, we observe a notable performance reduction when Lever-LM is trained with the 4-shot \(\mathcal{D}_{\mathcal{M}}\) compared to the 2-shot \(\mathcal{D}_{\mathcal{M}}\). Specifically, for IC, there is a performance decrease of approximately 1.57 in the Avg:1\(\sim\)8 metric when using the 4-shot \(\mathcal{D}_{\mathcal{M}}\) to train Lever-LM. One possible reason is that when constructing \(\mathcal{D}_{\mathcal{M}}\), some approximation operations are applied where sub-optimal ICD sequences are got. Then parts of statistic patterns may be salient is \(\mathcal{D}_{\mathcal{M}}\) that are more easily captured by Lever-LM where using longer ICD sequences may further encourage Lever-LM to capture such patterns, and thus causing less effective ICD sequences than the shorter \(\mathcal{D}_{\mathcal{M}}\). This points out a new direction to study how to build \(\mathcal{D}_{\mathcal{M}}\) with longer and more robust ICD sequences for better training Lever-LM.

**Random Order ICD sequence.** To validate that whether Lever-LM captures effective ICD orders, we randomly rearrange the ICD sequences generated by Lever-LM trained with 2-shot and 4-shot \(\mathcal{D}_{\mathcal{M}}\) and then evaluate the performance the 2-shot and 4-shot ICD configurations, respectively, in Table 4. It is evident that the original order of ICDs generated by Lever-LM attains the highest score in both VQA and IC, validating that Lever-LM can learn how to order the ICDs.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{2-Shot \(\mathcal{D}_{\mathcal{M}}\)} & \multicolumn{2}{c}{4-Shot \(\mathcal{D}_{\mathcal{M}}\)} \\ \cline{2-5}  & VQA & IC & VQA & IC \\ \hline Original & **50.83** & **88.63** & **51.12** & **85.97** \\ Random Order & 50.42 & 88.56 & 50.63 & 85.77 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of Random Order of Lever-LM generated ICDs.

Figure 3: Visualizations of diverse ICDs configurations, where the first and the last ICDs are given due to space limitation. We can find that Lever-LM use more diverse ICDs and thus not lead to short-cut inference.

Conclusion

After observing that configuring an ICD sequence is a mirror process of composing a sentence, we assume effective ICDs may contain statistic patterns that can be captured by temporal learner. Then we use a tiny LM named as Lever-LM to capture such patterns for configuring ICDs to lever LVLMs. To achieve this, we construct a dataset containing effective ICD sequences to train this Lever-LM. After training, we validate the effectiveness of Lever-LM by comparing it with similarity-based retrieval methods and find that Lever-LM can capture the statistic patterns between ICDs. Extensive ablations are deployed to discover which factors and why they will affect the results, which also pointing out a few future research directions.

## Acknowledgement

This work is supported by the National Science Foundation of China (62206048), the Natural Science Foundation of Jiangsu Province (BK20220819), the Young Elite Scientists Sponsorship Program of Jiangsu Association for Science and Technology (Tj-2022-027), and the Fundamental Research Funds for the Central Universities (2242024k30035). This research work is also supported by the Big Data Computing Center of Southeast University.

## References

* [1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [2] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _ArXiv preprint_, abs/2205.01068, 2022.
* [3] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _ArXiv preprint_, abs/2210.11416, 2022.
* [4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ArXiv preprint_, abs/2204.02311, 2022.
* [5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ArXiv preprint_, abs/2302.13971, 2023.
* [6] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. _ArXiv preprint_, abs/2210.02414, 2022.
* [7] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _ArXiv preprint_, abs/2206.07682, 2022.
* [8] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. _ArXiv preprint_, abs/2303.03846, 2023.
* [9] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _ArXiv preprint_, abs/2301.00234, 2023.
* [10] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with set-to-set functions. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020_, pages 8805-8814. IEEE, 2020.
* [11] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 9036-9046. IEEE, 2022.
* [12] Qiufeng Wang, Xu Yang, Shuxia Lin, and Xin Geng. Learninge: Inheriting condensed knowledge from the ancestry model to descendant models. _ArXiv_, abs/2305.02279, 2023.
* [13] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.
* [14] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.

* [15] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In _Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures_, pages 100-114, Dublin, Ireland and Online, 2022. Association for Computational Linguistics.
* [16] Jincen Jiang, Qianyu Zhou, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, and Jian Jun Zhang. Dg-pic: Domain generalized point-in-context learning for point cloud understanding. In _European Conference on Computer Vision (ECCV)_. Springer, 2024.
* [17] Chengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. In-context learning with iterative demonstration selection. _arXiv preprint arXiv:2310.09881_, 2023.
* [18] Chengwei Qin, Wenhan Xia, Fangkai Jiao, and Shafiq Joty. Improving in-context learning via bidirectional alignment. _arXiv preprint arXiv:2312.17055_, 2023.
* [19] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3816-3830, Online, 2021. Association for Computational Linguistics.
* [20] Feng Nie, Meixi Chen, Zhirui Zhang, and Xu Cheng. Improving few-shot performance of language models via nearest neighbor calibration. _ArXiv preprint_, abs/2212.02216, 2022.
* [21] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8086-8098, Dublin, Ireland, 2022. Association for Computational Linguistics.
* [22] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. _ArXiv preprint_, abs/2205.10625, 2022.
* [23] Sawan Kumar and Partha Talukdar. Reordering examples helps during priming-based few-shot learning. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 4507-4518, Online, 2021. Association for Computational Linguistics.
* [24] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Geng Xin. Exploring diverse in-context configurations for image captioning. _ArXiv preprint_, abs/2305.14800, 2023.
* [25] Li Li, Jiawei Peng, Huiyi Chen, Chongyang Gao, and Xu Yang. How to configure good in-context sequence for visual question answering. _ArXiv preprint_, abs/2312.01571, 2023.
* [26] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. _arXiv preprint arXiv:2306.13549_, 2023.
* [27] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. _ArXiv preprint_, abs/2209.01975, 2022.
* [28] Eshaan Tanwar, Manish Borthakur, Subhabrata Dutta, and Tanmoy Chakraborty. Multilingual llms are better cross-lingual in-context learners with alignment. _ArXiv preprint_, abs/2305.05940, 2023.
* [29] Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning. _ArXiv preprint_, abs/2212.10375, 2022.
* [30] Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. _ArXiv preprint_, abs/2212.04037, 2022.
* [31] Sohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon Ye, Hyunji Lee, and Minjoon Seo. Improving probability-based prompt selection through unified evaluation and analysis. _ArXiv preprint_, abs/2305.14877, 2023.

* [32] Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong Xu, and Chenguang Zhu. In-context demonstration selection with cross entropy difference. _ArXiv preprint_, abs/2305.14726, 2023.
* [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.
* [34] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. _ArXiv preprint_, abs/2308.01390, 2023.
* [35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. 2019.
* [36] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. _ArXiv preprint_, abs/2107.02137, 2021.
* [37] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. Accessed: 2023-05-05.
* [38] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _ArXiv preprint_, abs/2304.10592, 2023.
* [39] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. _ArXiv preprint_, abs/2305.03726, 2023.
* [40] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 200-212, 2021.
* [41] Congzhi Zhang, Linhai Zhang, and Deyu Zhou. Causal walk: Debiasing multi-hop fact verification with front-door adjustment. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 19533-19541, 2024.
* [42] Congzhi Zhang, Linhai Zhang, Jialong Wu, Deyu Zhou, and Yulan He. Causal prompting: Debiasing large language model prompting based on front-door adjustment, 2024.
* [43] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023.
* [44] Yixiao Yuan, Yawen Huang, and Yi Zhou. Rethinking a unified generative adversarial model for mri modality completion. In Anirban Mukhopadhyay, Ilkay Oksuz, Sandy Engelhardt, Dajiang Zhu, and Yixuan Yuan, editors, _Deep Generative Models_, pages 143-153, Cham, 2024. Springer Nature Switzerland.
* [45] Tai Nguyen and Eric Wong. In-context example selection with influences. _ArXiv preprint_, abs/2302.11042, 2023.
* [46] Xiaonan Li and Xipeng Qiu. Finding supporting examples for in-context learning. _ArXiv preprint_, abs/2302.13539, 2023.
* [47] Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9134-9148, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.

* [48] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [49] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2655-2671, Seattle, United States, 2022. Association for Computational Linguistics.
* [50] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. Unified demonstration retriever for in-context learning. _ArXiv preprint_, abs/2305.04320, 2023.
* [51] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exemplars for in-context learning. _ArXiv preprint_, abs/2302.05698, 2023.
* [52] Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models. _ArXiv preprint_, abs/2307.07164, 2023.
* [53] Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zornitsa Kozareva. Improving in-context few-shot learning via self-supervised training. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3558-3573, Seattle, United States, 2022. Association for Computational Linguistics.
* [54] Chonghua Liao, Yanan Zheng, and Zhilin Yang. Zero-label prompt selection. _ArXiv preprint_, abs/2211.04668, 2022.
* [55] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8086-8098, Dublin, Ireland, 2022. Association for Computational Linguistics.
* [56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [57] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015_, pages 4566-4575. IEEE Computer Society, 2015.
* [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017.
* [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.
* [60] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 6325-6334. IEEE Computer Society, 2017.

* [61] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [62] Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositional generalization. _ArXiv preprint_, abs/2212.06800, 2022.
* [63] Hasim Sak, Andrew Senior, and Francoise Beaufays. Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. _arXiv preprint arXiv:1402.1128_, 2014.
* [64] Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015_, pages 3128-3137. IEEE Computer Society, 2015.
* [65] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [66] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* [67] Yongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning. _arXiv preprint arXiv:2403.13164_, 2024.

Implementation Details.

### Lever-LM Hyperparameters

We provide additional implementation details for each experiment in Table 5. For all experiments, the batch size is set to 64, the warmup steps to \(5\%\) of total training steps, the scheduler as a cosine scheduler, and the optimizer as AdamW [61]. All experiments are deployed on an RTX 3090. All training processes are carried out with mixed precision and 2 RTX3090 GPUs. While for LVLM ICL experiments, we use BF16 mode.

### Datasets

**MS-COCO [56]** is widely used in IC, which is divided into \(118,287\) training, \(5,000\) validation, and \(5,000\) test image-caption pairs. Notably, each training image is associated with five distinct human-annotated captions.

**VQAV2 [60]** emphasizes open-ended VQA tasks, which encompasses \(4,437,570\) question-answer pairs in its training split, supplemented by an additional \(2,143,540\) pairs in the validation split.

### Prompt Template

For different LVLMs and tasks, the input format also varies. We followed the prompt templates provided by OpenFlamingo and IDEFICS for our experiments. We show the prompt templates in table 6. In the VQA tests of IDEFICS, an additional instruction needs to be added at the beginning of the input. Therefore, our final input format of k-shot is: [instruction] + [ICD1 prompt] + [ICD2 prompt] +... [ICDk prompt] + [Query prompt].

### Similarity-based retrieval method

In our study, we adopt the methodology proposed by [24] as a baseline, yet we incorporate several distinct experimental settings. Firstly, unlike their use of OpenFlamingoV1 [34] which employs LLaMA [5] as the underlying LLM, we utilize OpenFlamingoV2, based on the MPT LLM [37]. This version of OpenFlamingo leverages a more extensive dataset and a more advanced LLM, resulting in enhanced robustness and improved ICL capabilities.

Secondly, while they focus exclusively on ICL strategies in IC, which lacks textual input, our research extends to VQA. VQA involves textual queries; hence, we have adapted the STTR retrieval method for sourcing samples with similar textual prompts as ICDs, drawing inspiration from their approach.

Lastly, in the context of Image Captioning, our experiment utilizes the MSCOCO 2017 dataset, in contrast to their employment of the Karpathy split [64] of the MSCOCO 2014 dataset.

\begin{table}
\begin{tabular}{c|c|c c c c c c|c c c} \hline \hline  & & \multicolumn{6}{c}{Training Parameters} & \multicolumn{3}{c|}{\(\mathcal{D}_{\mathcal{A}}\) Parameters} & \multicolumn{1}{c}{**Avg1-8**} \\ \cline{3-11}  & & lr & weight decay & epoch & Freze & Adapter & n & \(b\) & \(l\) & \(\bm{\mathrm{Age1-8}}\) \\ \hline \multirow{6}{*}{IC} & 2-shot D\({}_{\mathcal{A}}\) (OpenFlamingo) & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✓ & ✓ & 5000 & 5 & 2 & 92.45 \\  & 2-shot D\({}_{\mathcal{A}}\) (DIFIFICS) & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✓ & ✓ & 5000 & 5 & 2 & 100.38 \\  & 4-shot D\({}_{\mathcal{A}}\) & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✓ & ✓ & 10000 & 5 & 4 & 90.88 \\  & Lever-LMLS/TD\({}_{\mathcal{A}}\) & \(1.0\times 10^{-3}\) & \(1.0\times 10^{-3}\) & 20 & ✓ & ✓ & 5000 & 5 & 2 & 92.12 \\  & CIDER Server & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✓ & ✓ & 5000 & 5 & 2 & 91.65 \\  & Fixed Set-1 & \(5.0\times 10^{-3}\) & \(1.0\times 10^{-3}\) & 10 & ✗ & ✗ & 5000 & 5 & 2 & 92.22 \\  & Fixed Set-2 & \(1.0\times 10^{-3}\) & \(1.0\times 10^{-3}\) & 20 & ✗ & ✗ & 5000 & 5 & 2 & 96.82 \\ \hline \multirow{6}{*}{VQA} & 2-shot D\({}_{\mathcal{A}}\) (OpenFlamingo) & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✗ & ✗ & 5000 & 5 & 2 & 51.31 \\  & 2-shot D\({}_{\mathcal{A}}\) (DIFIFICS) & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✗ & ✗ & 5000 & 5 & 2 & 54.44 \\  & 4-shot D\({}_{\mathcal{A}}\) & \(1.0\times 10^{-4}\) & \(1.0\times 10^{-3}\) & 20 & ✗ & ✗ & 10000 & 5 & 4 & 48.61 \\  & Lever-LMLS/TD\({}_{\mathcal{A}}\) & \(1.0\times 10^{-3}\) & \(1.0\times 10^{-3}\) & 30 & ✗ & ✗ & 5000 & 5 & 2 & 49.32 \\  & Fixed Set-1 & \(1.0\times 10^{-3}\) & \(1.0\times 10^{-3}\) & 20 & ✗ & ✗ & 5000 & 5 & 2 & 50.93 \\  & Fixed Set-2 & \(1.0\times 10^{-3}\) & \(1.0\times 10^{-3}\) & 10 & ✗ & ✗ & 5000 & 5 & 2 & 47.80 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Different settings of Lever-LM experiments, where the \(n\) is the number of anchor samples in \(\mathcal{A}\), \(b\) is the beam size, and \(l\) is the length of ICD configurations.

[MISSING_PAGE_FAIL:17]

tests. As shown in Table 12, it can be observed that randomly selecting a fixed set of ICD sequences results in relatively poor performance. The Golden-Set outperforms the best Fix Random set (seed 1) in Avg:1\(\sim\)8 by 16.14. This also demonstrates the importance of high-quality ICD sequences.

### Fine-grained Analysis of Diverse ICL Methods for VQAv2

Table 13 provides a detailed comparison of different ICL methods on the VQAv2 dataset using IDEFICSv1. We compare the accuracy of different question types in VQAv2 and find Lever-LM consistently outperforms other methods, highlighting its effectiveness in VQA tasks.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Type & Method & **Avg:1\(\sim\)2** & **Avg:4\(\sim\)8** & **Avg:1\(\sim\)8** \\ \hline \multirow{4}{*}{Yes/No} & Lever-LM & **0.5623** & **0.5939** & **0.5833** \\  & RS & 0.5402 & 0.5795 & 0.5664 \\  & SIIR & 0.5593 & 0.5853 & 0.5799 \\  & SITR & 0.5229 & 0.5652 & 0.5511 \\ \hline \multirow{4}{*}{Other} & Lever-LM & **0.3192** & **0.3574** & **0.3446** \\  & RS & 0.2852 & 0.3267 & 0.3129 \\  & SIIR & 0.2917 & 0.3349 & 0.3205 \\  & SITR & 0.2643 & 0.3049 & 0.2914 \\ \hline \multirow{4}{*}{Counting} & Lever-LM & **0.2782** & **0.3079** & **0.2980** \\  & RS & 0.1323 & 0.1719 & 0.1587 \\ \cline{1-1}  & SIIR & 0.1381 & 0.1917 & 0.1738 \\ \cline{1-1}  & SITR & 0.1051 & 0.1325 & 0.1234 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Fine-grained analysis of diverse ICL methods on VQAv2 with IDEFICSv1.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c} \hline \hline  & & \multicolumn{4}{c|}{Interceptance} & \multicolumn{4}{c|}{Expineineine{3-10}} & \multirow{2}{*}{**Avg1-8**} \\ \cline{3-10} \multirow{4}{*}{IC} & \multicolumn{2}{c|}{Shoc 1} & Shoc 2 & Shoc 3 & Shoc 4 & **Avg:1-4** & Shoc 6 & Shoc 8 & **Avg:6-8** & \\ \cline{3-10}  & & RS & 73.32 & 82.95 & 87.72 & 93.65 & 84.41 & 95.81 & 97.42 & 96.62 & 88.48 \\  & SITR & 66.06 & 77.69 & 83.46 & 85.05 & 87.36 & 98.34 & 93.57 & 97.17 & 82.61 \\  & SITR & 71.71 & 84.87 & 90.33 & 93.22 & 85.16 & **97.80** & **1601.10** & **99.49** & 89.91 \\  & Lever-LM-dist (_D_) & **75.73** & **89.71** & **92.864** & **94.173** & **97.35** & 97.56 & 98.35 & 97.96 & **98.88** \\  & Lever-LM-dist (_D_) & 80.02 & 88.63 & 93.41 & 90.66 & 89.53 & 97.26 & 99.35 & 98.30 & 92.45 \\ \hline \multirow{4}{*}{VQA} & RS & 41.97 & 45.92 & 48.17 & 48.95 & 46.25 & 51.18 & 51.44 & 51.31 & 47.94 \\  & SITR & 40.17 & 45.88 & 40.13 & 47.43 & 49.72 & 97.05 & 97.52 & 94.62 & 46.29 \\ \cline{1-1}  & SITR & 43.11 & 47.46 & 49.85 & 50.68 & 47.83 & **53.23** & **53.85** & **53.85** & 48.49 \\ \cline{1-1}  & SITR & 44.66 & 47.52 & 49.95 & 49.05 & 47.48 & 50.06 & 49.47 & 49.77 & 47.98 \\ \cline{1-1}  & Lever-LM-dist (_D_) & **44.55** & **48.05** & **50.65** & **50.98** & **48.56** & 52.43 & 53.92 & 52.68 & **49.93** \\ \cline{1-1}  & Lever-LM-dist (_D_) & 46.66 & 50.83 & 51.91 & 52.15 & 50.93 & 53.29 & 53.01 & 53.15 & 51.31 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Results of Lever-LM with 4-shot \(\mathcal{D}_{\mathcal{M}}\) on IC and VQA.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & & \multicolumn{4}{c|}{Interceptance} & \multicolumn{4}{c}{Expineineine{3-10}} & \multirow{2}{*}{**Avg1-8**} \\ \cline{3-10}  & & Shoc 1 & Shoc 2 & Shoc 3 & Shoc 4 & Shoc & Shoc & Shoc & **Avg:1-8** \\ \cline{3-10}  & & RS & 73.32 & 84.57 & 73.26 & 87.23 & 87.06 & 87.01 & 97.42 & 96.55 & 88.0 \\  & SITR & 71.31 & 84.57 & 73.29 & 87.23 & 87.23 & 87.06 & 110.2 & 96.72 & 88.0 \\  & SITR & 71.31 & 84.57 & 73.29 & 87.23 & 87.23 & 87.00 &

### CIDEr Results of Different Lever-LM Sizes in Image Captioning

Table 14 shows the impact of different Lever-LM model sizes on CIDEr scores for image captioning on IDEFICSv1. As shown in Table 14, we evaluate 1-layer/4-layer Transformer decoder layers. We find that the size of Lever-LM has minimal impact on performance. We believe that capturing the ICD sequence distribution is a simple task that can be learned by only a few Transformer Decoder layers. Moreover, our motivation is to use a small model to enhance the ICL performance of a large model, so it is not appropriate to design Lever-LM to be too large.

### Lever-LM in NLP domain.

To show Lever-LM is a general method, we train a Lever-LM in a NLP task. Specifically, we use Qwen1.5 [65] 1.8B and generate 2-shot ICD datasets for SST-2 [66], which is a sentiment classification task. The accuracy results are displayed in the Table 15. It can be observed that our Lever-LM outperforms the Random method and STTR in Avg:1\(\sim\)2 and Avg: 4\(\sim\)8, demonstrating the potential of Lever-LM in NLP.

### Inference Time Comparison between Lever-LM and SIIR.

Table 16 compares the inference time between SIIR and Lever-LM methods on IDEFICSv1. Both methods have similar retrieval times, indicating that Lever-LM's performance gains do not come at the cost of efficiency.

### Performance on VL-ICL Benchmark Tasks

We test Lever-LM on two tasks of VL-bench [67] due to computation limitation and show the results in Table 17. It can be found Lever-LM achieves higher performance than other retrieval-based methods, validating the generalizability of Lever-LM.

### Performance on IDEFICSv2-8B

We also validate Lever-LM's generalization ability using IDEFICSv2. IDEFICSv2 is an open-source model specifically designed for ICL. Besides, modern LVLMs with robust ICL ability belong to two mainstream architectures : (1) Flamingo-based (using cross-attention to fuse vision and language, like Open-Flamingo or IDEFICSv1 that are tested in our paper) and (2) LLaVA-based (directly concatenating image and language tokens like IDEFICSv2). Thus, testing with IDEFICSv2 assesses Lever-LM's generalization across both architectures. Since IDEFICSv2 directly concatenate vision

\begin{table}
\begin{tabular}{l c} \hline \hline Method & Retrieval Time (s) \\ \hline SIIR & 0.317 \\ Lever-LM & 0.328 \\ \hline \hline \end{tabular}
\end{table}
Table 16: The inference time of different ICL methods with IDEFICSv1.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Model Size & **Avg:1\(\sim\)2** & **Avg:4\(\sim\)8** & **Avg:1\(\sim\)8** \\ \hline
1-layer Transformer (64.2M) & 89.48 & 107.49 & 101.15 \\
2-layer Transformer (67.4M) & 89.58 & 105.79 & 100.05 \\
4-layer Transformer (73.7M) & 89.12 & 106.10 & 100.44 \\ \hline \hline \end{tabular}
\end{table}
Table 14: CIDEr results of different Lever-LM sizes in Image Captioning with IDEFICSv1.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & **Avg:1\(\sim\)2** & **Avg:4\(\sim\)8** & **Avg:1\(\sim\)8** \\ \hline RS & 0.6227 & 0.6579 & 0.6438 \\ STTR & 0.6807 & 0.7312 & 0.7110 \\ Lever-LM & **0.7087** & **0.7450** & **0.7305** \\ \hline \hline \end{tabular}
\end{table}
Table 15: Accuracy results of diverse ICL methods on SST2 with Qwen1.5-1.8B.

[MISSING_PAGE_FAIL:20]

Figure 4: 8-shot ICD configurations visualizations of IC Fixed Set.

## Appendix D Limitation

One major limitation of our study is the strategy used to build \(\mathcal{D}_{\mathcal{M}}\) is not optimal, which requires further improvement. This limitation is revealed by observing that the 4-shot \(\mathcal{D}_{\mathcal{M}}\) performs worse than the 2-shot one, highlighting the need for a more effective approach in searching for longer ICD sequences. To address this, we plan to design function \(I_{\mathcal{M}}\) to evaluate the effectiveness of ICD sequences; and use better sampling strategies in Eq. (2) to avoid the ICD sequence deviating from the global optimum.

Figure 5: 8-shot ICD configurations visualizations of VQA Fixed Set.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have elaborated on the contributions of our approach in the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in Appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: we do not have theoretical result. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We show all hyperparameters and detailed experimental procedures for all experiments in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We submit all code for reproducing all experiments of our paper in supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all the training and test details in our paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The number of our experiments is quite large, and the cost of repeating experiments is relatively high. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide all computer resources information in our paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our approach is primarily used to enhance the ICL capabilities of the model without modifying the parameters of the large model. Therefore, it does not involve social impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our approach is not related to this. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We respect all licenses of the model and datasets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not create new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our research is not related to the crowdsourcing experiments and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our research does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.