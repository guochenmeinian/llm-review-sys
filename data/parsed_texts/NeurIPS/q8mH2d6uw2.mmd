# Deep Contract Design via Discontinuous Networks

 Tonghan Wang

Harvard University

twang1@g.harvard.edu &Paul Dutting

Google Switzerland

duetting@google.com &Dmitry Ivanov

Israel Institute of Technology

divanov@campus.technion.ac.il &Inbal Talgam-Cohen

Israel Institute of Technology

italgam@cs.technion.ac.il &David C. Parkes

Harvard University

parkes@eecs.harvard.edu

Also DeepMind, London UK

###### Abstract

Contract design involves a _principal_ who establishes contractual agreements about payments for outcomes that arise from the actions of an _agent_. In this paper, we initiate the study of deep learning for the automated design of optimal contracts. We introduce a novel representation: the _Discontinuous ReLU (DeLU) network_, which models the principal's utility as a discontinuous piecewise affine function of the design of a contract where each piece corresponds to the agent taking a particular action. DeLU networks implicitly learn closed-form expressions for the incentive compatibility constraints of the agent and the utility maximization objective of the principal, and support parallel inference on each piece through linear programming or interior-point methods that solve for optimal contracts. We provide empirical results that demonstrate success in approximating the principal's utility function with a small number of training samples and scaling to find approximately optimal contracts on problems with a large number of actions and outcomes.

## 1 Introduction

Contract theory studies the setting where a _principal_ seeks to design a contract for rewarding an _agent_ on the basis of the uncertain outcomes caused by the agent's private actions [8, 45]. Typical examples include a landlord who enters into a summer rental with a contract that includes penalties in the case of damage; a homeowner who engages a firm to complete a kitchen renovation with a contract that conditions payments on timely completion or functioning appliances; or an individual who employs a freelancer to do some design work with a contract that includes bonuses for completing the job.

A contract specifies payments to the agent, conditioned on outcomes. The principal is self-interested, with a value for each outcome and a cost for making payments. The agent is also self-interested, and responds to a contract by choosing an action that maximizes its expected utility (expected payment minus the cost of an action). The problem is to find a contract that maximizes the principal's utility (expected value minus expected payment), given that the agent will best respond to the contract. In economics, this is referred to as a problem of _moral hazard_, in that the agent is willing to privately act in its best interest given the contract (the "hazard" is that the behavior of the agent may be to the detriment of the principal).

The importance of contract design is evidenced by the 2016 Nobel Prize awarded to O. Hart and B. Holmstrom [51] and its broad application to real-world problems. Contract design is one of the three fundamental problems in the realm of economics involving asymmetric information and incentives, along with _mechanism design_[9] and _signalling_ (Bayesian persuasion) [41]. However, whileboth mechanism design [10; 11; 12; 7; 36; 37] and signalling [26; 27; 18] have been studied extensively from a computational perspective, the contract design problem has only recently received attention and presented distinct computational challenges. For many combinatorial contract settings [6], the conventional approach based on linear programming becomes computationally infeasible and the problem of finding or even approximating the optimal contract becomes intractable [33; 28; 29]. In addition, learning-theoretic results give worst-case exponential sample complexity bounds for learning an approximately-optimal contract [40; 55]. As a result, there is a growing demand for a scalable, general purpose, and beyond-worst case approach for computing (near-)optimal contracts.

We are thus motivated to initiate the study of deep learning for optimal contract design (_deep contract design_). This falls within the broader framework of _differentiable economics_, which seeks to leverage parameterized representations of differentiable functions for the purpose of optimal economic design [30]. In regard to learning contracts, recent work [40; 21; 55; 31] considers the distinct setting of _online learning_ for contract design with bandit feedback, characterizing regret bounds without appeal to deep learning.

A first innovation of this paper is to introduce a neural network architecture that can well-approximate the principal's utility function. A close examination of the geometry of the principal's utility function (Sec. 3) reveals its similarity to fully-connected feed-forward neural networks with ReLU activations [1]: both of them are piecewise affine functions [22]. However, whereas a ReLU network models a continuous function, the principal's utility function is discontinuous at the boundary of linear regions, where the best response of the agent changes. Accurate approximation in the vicinity of boundaries is critical because an optimal contract is always located on the boundary (Lemma 3 in Sec. 3), but this is exactly where ReLU approximation error can be large. To handle this, we introduce the _Discontinuous ReLU (DeLU)_ network, which recognizes that the linear regions of a ReLU network are decided by activation patterns (the status of all activation units in the network), and conditions a _piecewise bias_ on these activation patterns. In this way, each linear region has a different bias parameter and can be discontinuous at the boundaries. Fig. 1 illustrates the DeLU and its use to represent the principal's utility function, contrasting this with a continuous ReLU function.

A second innovation in this paper is to introduce a scalable inference technique that can find the contract that maximizes the network output (i.e., the principal's utility). We show that linear regions (pieces) of DeLU networks implicitly learn closed-form expressions for the incentive constraints of the agent and the utility maximization objective of the principal, so that we can use linear programming (LP) on each piece to find the global optimum. However, the time efficiency of this LP method is impeded by the overhead of solving individual LPs, and worsens with problem size in the absence of suitable parallel computing resources. To increase the computational efficiency, we develop a gradient-based inference algorithm based on the interior-point method [49] that only requires a few forward and backward passes of the DeLU network to find the contract that maximizes the network output. This method scales well to large-scale problems and can be readily run in parallel on GPUs.

The effectiveness of introducing piecewise discontinuity into neural networks is demonstrated by our experimental results. The DeLU network well-approximates the principal's utility function even with

Figure 1: DeLU and ReLU approximation on a randomly generated contract design instance with 1,000 actions and 2 outcomes (see Sec. 4.3). The x- and y-coordinates are the payments for each of the two outcomes, and the z-coordinate is the utility of the principal. **(a)** The exact surface of the principalâ€™s utility function \(u^{p}\). Different colors represent the action selected by the agent upon receiving the contract. **(b)** A learned _ReLU network_ cannot model the discontinuity of the \(u^{p}\) function and yields an incorrect contract as shown in (a). Colors in (b) and (c) represent different activation patterns (linear regions) of the networks. **(c)** A learned DeLU network represents a discontinuous function and can well-approximate \(u^{p}\), yielding the optimal contract as shown in (a).

a small number of training samples, and the inference method remains accurate and efficient as the problem size grows. By synergistically harnessing these two innovations, our method consistently finds near-optimal solutions on a wide range of contract design problems, significantly surpassing the capability of conventional continuous networks.

**Related work.** A longstanding challenge in the deep learning community has been to approximate discontinuous functions with neural networks. While the Universal Approximation Theorem guarantees the approximation of continuous functions, many problems involve discontinuity, including solar flare imaging [46] and problems coming from mathematics [24]. However, establishing a network to represent discontinuous functions is not an easy task. Discontinuities were considered as early as in the 1950s, when Rosenblatt [50] introduced the perceptron model and its single-layer of step activation functions. Following this work, others modeled discontinuity through the use of different discontinuous activation functions [34; 2; 44]. However, training these models is more challenging than the more typical models that make use of continuous activation functions [24], and this has hindered their application. To the best of our knowledge, this paper presents the first discontinuous network architecture with continuous activation functions and stable optimization performance.

There is a vast and still-growing economics literature on contracts [see, e.g., 52; 15], to which the computational lens has recently been applied. In classic settings, computing the optimal contract is tractable by solving one LP per agent's action, and taking the overall best solution [see, e.g., 32]. However, LP becomes computationally infeasible for combinatorial contracts, including settings with exponentially-many outcomes [33], actions [28], or agent combinations [29]. Another issue with the LP-based approach is that it requires complete information in regard to the problem facing the agent (its _type_). If the agent has a hidden type, this makes the optimal contract hard to compute [16; 38; 17], or otherwise complex [4]. One way to deal with these complexities is to focus on simple contracts, in particular _linear contracts_ (commission-based), which are popular in practice [e.g., 13; 32; 14].

A distinct but related approach combines contract design with learning, where efforts have concentrated on online learning theory [40; 21; 55; 31]. These works show exponential lower bounds on the achievable regret for general contracts in worst-case settings, and sublinear regret bounds for linear contracts. Additionally, contracts have been studied from the perspective of multi-agent reinforcement learning [48; 19; 25]. The problem of _strategic classification_ has also established a formal connection between strategy-aware classifiers and contracts [43; 42; 3]. There is also interest in the application of contract theory to the AI alignment problem [39]. However, the use of learning methods for solving optimal contracts, as opposed to auctions [30], remains largely unexplored. This gap in the literature can be partly attributed to the inherent complexity of finding optimal contracts, as the principal utility depends on the agent's action, which must conform to incentive compatibility (IC) constraints that are not observable by the principal.

## 2 Preliminaries

**Offline contract learning problem.** The contract design problem is defined with elements \(\mathcal{C}=\langle\mathcal{A},\mathcal{O},\mathcal{F},c,p,v\rangle\), and involves a single principal and a single agent. The agent selects an action \(a\) in the finite action space \(\mathcal{A}\), \(|\mathcal{A}|\)=\(n\). Action \(a\) leads to a distribution \(p(\cdot|a)\) over the outcomes in \(\mathcal{O}\), \(|\mathcal{O}|\)=\(m\), and incurs a cost \(c(a)\in\mathbb{R}_{\geq 0}\) to the agent. The valuation of each outcome \(o_{j}\in\mathcal{O}\) for the principal is decided by the value function \(v:\mathcal{O}\rightarrow\mathbb{R}_{\geq 0}\). The principal sets up a _contract_, \(\bm{f}\in\mathcal{F}\subset\mathbb{R}_{\geq 0}^{m}\), which influences the action selected by the agent. A contract \(\bm{f}=(f_{1},f_{2},\cdots,f_{m})^{\top}\) specifies the payment \(f_{j}\geq 0\) made to the agent by the principal in the event of outcome \(o_{j}\). On receiving a contract \(\bm{f}\), the agent selects the action \(a^{*}(\bm{f})\) maximizing its utility \(u^{a}(\bm{f};a)=\mathbb{E}_{o\sim p(\cdot|a)}\left[f_{o}\right]-c(a)\). For a given action \(a\) of the agent, the principal gets utility \(u^{p}(\bm{f};a)=\mathbb{E}_{o\sim p(\cdot|a)}\left[v_{o}-f_{o}\right]\), which is the principal's expected value minus payment. In the case that the induced action is the best response of the agent given the contract, we write \(u^{p}(\bm{f})=\mathbb{E}_{o\sim p(\cdot|a^{*}(\bm{f}))}\left[v_{o}-f_{o}\right]\). The goal in optimal contract design is to find the contract that maximizes the principal's utility without access to \(p(\cdot|a)\) and the action taken by the agent:

\[\bm{f}^{*}=\arg\max_{\bm{f}}u^{p}\left(\bm{f}\right)=\arg\max_{\bm{f}}\mathbb{ E}_{o\sim p(\cdot|a^{*}(\bm{f}))}\left[v_{o}-f_{o}\right].\] (1)

**ReLU piecewise-affine networks and activation patterns.** A fully-connected neural network with a piecewise linear activation function (e.g., ReLU and leaky ReLU) and a linear output layer represents a _continuous_ piecewise affine function [5; 23]. A piecewise affine function is defined as follows.

**Definition 1** (Piecewise affine function).: _A function \(g:\mathbb{R}^{d}\to\mathbb{R}\) is piecewise affine if there exists a finite set of polytopes \(\{\mathcal{D}_{i}\}_{i=1}^{P}\) such that \(\cup_{i=1}^{P}\mathcal{D}_{i}=\mathbb{R}^{d}\), \(\mathcal{D}_{i}\cap\mathcal{D}_{j\neq i}=\varnothing\), and \(g\) is a linear function \(\rho_{i}:\mathcal{D}_{i}\to\mathbb{R}\) when restricted to \(\mathcal{D}_{i}\). We call \(\rho_{i}\) a linear piece of \(g\)._

We now follow [22] and introduce some local properties of the piecewise affine function that is represented by a ReLU network. Suppose there are \(L\) hidden layers in a network \(g\), with sizes \([n_{1},n_{2},\cdots,n_{L}]\). \(\bm{W}^{(l)}\in\mathbb{R}^{n_{l}\times n_{l-1}}\) and \(\bm{b}^{(l)}\in\mathbb{R}^{n_{l}}\) are the weights and biases of layer \(l\). Let \(n_{0}=d\) denote the input space dimension. We consider a ReLU network with one-dimensional outputs, and the output layer has weights \(\bm{W}^{(L+1)}\in\mathbb{R}^{1\times n_{L}}\) and a bias \(b^{(L+1)}\in\mathbb{R}\). With input \(\bm{x}\in\mathbb{R}^{d}\), we have the pre- and post-activation output of layer \(l\): \(\bm{h}^{(l)}(\bm{x})=\bm{W}^{(l)}\bm{o}^{(l-1)}(\bm{x})+\bm{b}^{(l)}\) and \(\bm{o}^{(l)}(\bm{x})=\sigma\left(\bm{h}^{(l)}(\bm{x})\right)\), where \(\sigma:\mathbb{R}\to\mathbb{R}\) is an _activation function_. In this paper, we consider ReLU activation \(\sigma(x)=\max\{x,0\}\)[54; 35], but the proposed method can be extended to other piecewise linear activation functions (e.g., LeakyReLU and PReLU [54]). For each hidden unit, the ReLU _activation status_ has two values, defined as \(1\) when pre-activation \(h\) is positive and \(0\) when \(h\) is strictly negative. The activation pattern of the entire network is defined as follows.

**Definition 2** (Activation Pattern).: _An activation pattern of a ReLU network \(g\) with \(L\) hidden layers is a binary vector \(\bm{r}=[\bm{r}^{(1)},\cdots,\bm{r}^{(L)}]\in\{0,1\}^{\sum_{l=1}^{L}n_{l}}\), where \(\bm{r}^{(l)}\) is a layer activation pattern indicating activation status of each unit in layer \(l\)._

The activation pattern depends on the input \(\bm{x}\), and we define function \(r:\mathbb{R}^{d}\to\{0,1\}^{\sum_{l=1}^{L}n_{l}}\) that maps the input to the corresponding activation pattern. For a ReLU network, inputs that have the same activation pattern lie in a polytope, and the activation pattern determines the boundaries of this polytope. To see this, we can write the output of layer \(l\), \(\bm{h}^{(l)}(\bm{x})\), as

\[\bm{h}^{(l)}(\bm{x})=\bm{W}^{(l)}\bm{R}^{(l-1)}(\bm{x})\left(\bm{W}^{(l-1)}\bm {R}^{(l-2)}(\bm{x})\left(\cdots\left(\bm{W}^{(1)}\bm{x}+\bm{b}^{(1)}\right) \cdots\right)+\bm{b}^{(l-1)}\right)+\bm{b}^{(l)},\] (2)

where \(\bm{R}^{(k)}\) is a diagonal matrix with diagonal elements equal to the layer activation pattern \(\bm{r}^{(k)}\). Eq. 2 indicates that, when \(\bm{r}\) is fixed, \(\bm{h}^{(l)}\) is a linear function \(\bm{h}^{(l)}(\bm{x})=\bm{M}^{(l)}\bm{x}+\bm{z}^{(l)}\), where \(\bm{M}^{(l)}=\bm{W}^{(l)}\left(\prod_{k=1}^{l-1}\bm{R}^{(l-k)}(\bm{x})\bm{W}^{ (l-k)}\right)\) and \(\bm{z}^{(l)}=\bm{b}^{(l)}+\sum_{k=1}^{l-1}\left(\prod_{j=1}^{l-k}\bm{W}^{(l+1- j)}\bm{R}^{(l-j)}(\bm{x})\right)\bm{b}^{(k)}\). We thus get \(\sum_{l=1}^{L}n_{l}\) half-spaces, with the half-space corresponding to unit \(i\) of layer \(l\) defined as:

\[\Gamma_{l,i}=\left\{\bm{y}\in\mathbb{R}^{d}|\Delta_{i}^{(l)}\left(\bm{M}_{i}^{ (l)}\bm{y}+\bm{z}_{i}^{(l)}\right)\geq 0\right\},\] (3)

where \(\bm{M}_{i}^{(l)}\bm{y}+\bm{z}_{i}^{(l)}\) is the output of unit \(i\) at layer \(l\), and \(\Delta_{i}^{(l)}\) is 1 if \(\bm{h}_{i}^{(l)}(\bm{x})\) is positive, and is -1 otherwise. The input \(\bm{x}\) is in the polytope that is defined by the intersection of these half-spaces: \(\mathcal{D}(\bm{x})=\cap_{l=1,\cdots,L}\cap_{i=1,\cdots,n_{l}}\Gamma_{l,i}\). When restricted to \(\mathcal{D}(\bm{x})\), the ReLU network is a linear function: \(g(\bm{x})=\bm{W}^{(L+1)}\bm{R}^{(L)}\bm{h}^{(L)}(\bm{x})+b^{(L+1)}\).

**Interior-point method for optimization problems with inequality constraints.** For a minimization problem with objective function \(q(\bm{x})\) and inequality constraints \(p_{i}(\bm{x})>0,i=1,\cdots,M\), the _interior-point method_[53] introduces a logarithmic _barrier function_, \(\phi(\bm{x})=-\sum_{i=1}^{M}\log(p_{i}(\bm{x}))\), and finds the minimizer of \(q(\bm{x})+\frac{1}{t}\phi(\bm{x})\), for some \(t>0\). This new objective function is defined on the set of strictly feasible points \(\{\bm{x}|p_{i}(\bm{x})>0,i=1,\cdots,M\}\), and approximates the original objective as \(t\) becomes large. Given this, we can solve for a series of optimization problems for increasing values of \(t\). In the \(k\)-th round, \(t^{(k)}\) is set to \(\mu\cdot t^{(k-1)}\), where \(\mu>1\) is a constant, \(t^{(0)}>0\) is an initial value, and the problem is solved (e.g., by Newton initialized at \(\bm{x}^{(k-1)}\)) to yield \(\bm{x}^{(k)}\). Assume that we solve the barrier problem exactly for each iterate, then to achieve a desired accuracy level of \(\epsilon>0\), we need \(n_{\mathtt{barrier}}=\log(M/(t^{(0)}\epsilon))/\log(\mu)\) rounds of optimization.

## 3 Geometry of Optimal Contracts

In this section, we introduce some properties of the principal's utility function, \(u^{p}:\mathcal{F}\to\mathbb{R}\), which will motivate our method in Sec. 4. First, we show that \(u^{p}\) is a piecewise affine function.

**Lemma 1**.: _The principal's utility function \(u^{p}\) is a piecewise affine function._Proof.: The principal utility depends on the agent action. For contracts in the intersection of the following \(n-1\) half spaces that represent _incentive compatibility constraints_, the agent's action is \(a_{i}\):

\[\Gamma_{i,j}=\left\{\bm{f}\in\mathcal{F}\mid\mathbb{E}_{o\sim p(\cdot|a_{i})} \left[f_{o}\right]-c(a_{i})\geq\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[f_{o} \right]-c(a_{j})\right\},\forall j\neq i.\] (4)

Moreover, when agent action \(a_{i}\) remains unchanged, \(u^{p}\) changes linearly with \(\bm{f}\): \(\alpha u^{p}(\bm{f};a_{i})+\beta u^{p}(\bm{f}^{\prime};a_{i})=\alpha\mathbb{E }_{o\sim p(\cdot|a_{i})}\left[v_{o}-f_{o}\right]+\beta\mathbb{E}_{o\sim p(\cdot |a_{i})}\left[v_{o}-f_{o}^{\prime}\right]=\mathbb{E}_{o\sim p(\cdot|a_{i})} \left[v_{o}-(\alpha f_{o}+\beta f_{o}^{\prime})\right]=u^{p}(\alpha\bm{f}+ \beta\bm{f}^{\prime};a_{i})\). Therefore, when restricted to the polytope \(\mathcal{Q}_{i}=\cap_{j\neq i}\Gamma_{i,j},\forall i\), \(u^{p}\) is linear. 

Based on Lemma 1, we define a _linear piece_ of function \(u^{p}\) as \(\mu_{i}^{p}:\mathcal{Q}_{i}\rightarrow\mathbb{R}\), where \(\mathcal{Q}_{i}\) defines the set of contracts that motivates the agent to take action \(i\). Lemma 1 can be easily extended to the agent's utility function \(u^{a}\), and the linear pieces of function \(u^{p}\) and \(u^{a}\) share the same set of domains \(\{\mathcal{Q}_{i}\}\). We define a linear piece of function \(u^{a}\) as \(\mu_{i}^{a}:\mathcal{Q}_{i}\rightarrow\mathbb{R}\). We next observe:

**Lemma 2**.: _The principal's utility function \(u^{p}\) can be discontinuous on the boundary of linear pieces._

The proof in Appx. A follows the idea that the agent is indifferent between action \(a_{i}\) and \(a_{j}\) given a contract \(\bm{f}\) on the boundary of two neighboring linear pieces \(\mu_{i}^{p}\) and \(\mu_{j\neq i}^{p}\): \(\mu_{i}^{a}(\bm{f})\)=\(\mu_{j}^{a}(\bm{f})\). However, the principal's utility equals the expected value minus the cost of an action minus \(u^{a}(\bm{f})\). As the expected value minus the cost can be different in \(\mu_{i}^{p}\) and \(\mu_{j}^{p}\), \(u^{p}\) can be discontinuous at \(\bm{f}\).

We then define the optimality of a contract and analyze the structure of optimal contracts.

**Definition 3** (Piecewise and Global Optimal Contracts).: _A contract \(\bm{f}_{i}^{*}\in\mathcal{Q}_{i}\) is piecewise optimal if \(u^{p}(\bm{f}_{i}^{*})\geq u^{p}(\bm{f}),\forall\bm{f}\in\mathcal{Q}_{i}\). A contract \(\bm{f}^{*}\) is global optimal if \(u^{p}(\bm{f}^{*})\geq u^{p}(\bm{f}),\forall\bm{f}\in\mathcal{F}\)._

**Lemma 3**.: _The global optimal contract is on the boundary of a linear piece._

Lemma 3 can be proved by contradiction (as detailed in Appx. A): if a global optimal contract is not on the boundary, we can always find a solution with greater principal utility. As analyzed in Sec. 4, Lemma 3 serves as a compelling rationale for introducing our discontinuous networks. Besides discontinuity, there is another network design consideration motivated by the following property.

**Lemma 4**.: _The principal's utility function \(u^{p}\) can be written as a summation of a concave function and a piecewise constant function._

The proof in Appx. A commences by establishing the convexity of function \(u^{a}\) and subsequently formulating \(u^{p}\) as a function of \(u^{a}\). This property motivates us to introduce _concavity_ into our network design. In Appx. C, we discuss how to achieve this by imposing non-negativity constrains on network weights and analyze the impact of this design choice on the performance.

## 4 DeLU Neural Networks

Given the piecewise-affine geometry, the preceding analysis shows a close connection between the principal's utility function \(u^{p}\) (Lemma 1) and fully-connected ReLU networks (Sec. 2). However, ReLU functions are continuous and cannot represent the abrupt changes in \(u^{p}\) at the boundaries of the linear pieces. This lack of representational capacity is problematic because the optimal contracts are on the boundary (Lemma 3), which is precisely where the ReLU approximation errors can be large. Consequently, ReLU networks are not well suited to deep contract design. In this section, we introduce the new _Discontinuous ReLU (DeLU) network_, which provides _a discontinuity at boundaries between pieces_, making it a suitable function approximator for the \(u^{p}\) function.

### Architecture

The DeLU network architecture supports different biases for different linear pieces. Since a linear piece can be identified by the corresponding activation pattern, we propose to condition these _piecewise biases_ on activation patterns. Specifically, we learn a _DeLU network_\(\xi:\mathcal{F}\rightarrow\mathbb{R}\) (Fig. 2) to approximate the principal's utility function, mapping a contract to the corresponding utility of the principal. The first part of a DeLU network is a sub-network similar to a conventional ReLU network. This sub-network \(\eta\) has \(L\) fully-connected hidden layers with ReLU activation and a weight matrix at the output layer, i.e., \(\eta\) is parameterized as \(\theta_{\eta}=\{\bm{W}^{(1)},\bm{b}^{(1)},\cdots,\bm{W}^{(L)},\bm{b}^{(L)},\bm{W }^{(L+1)}\}\), which includes weights and biases for \(L\) hidden layers and weights for the output (\((L+1)\)-th) layer. The DeLU network is different from a conventional ReLU network at the bias of the output (last) layer (\(b^{(L+1)}\)), and we introduce a new method to generate the last-layer biases.

Since there are no activation units at the output layer, given an input contract \(\bm{f}\), we can obtain the activation pattern \(r(\bm{f})\) by a forward pass of the network up until the last layer. To condition \(b^{(L+1)}\) on the activation pattern, we train another sub-network \(\zeta:[0,1]^{\sum_{i=1}^{L}n_{l}}\rightarrow\mathbb{R}\), that maps \(r(\bm{f})\) to the bias of the output layer. We use a two-layer fully-connected network with Tanh activation to represent \(\zeta\) and denote its parameters as \(\theta_{\zeta}\). In this way, contracts in the same linear piece of the DeLU network share the same bias value, enabling the network to express discontinuity at the boundaries while keeping other properties of network \(\eta\) unchanged. In Appx. B, we discuss why we adopt a neural network, instead of a simpler function, to model the bias term.

### Training and inference

The DeLU network \(\xi\), including sub-networks \(\eta\) and \(\zeta\), is end-to-end differentiable. For training, we randomly sample \(K\) contracts and the corresponding principal's utilities: \(\mathcal{T}_{K}=\{(\bm{f}_{i},u^{p}(\bm{f}_{i}))\}_{i=1}^{K}\). Feeding a training sample \(\bm{f}_{i}\) as input, we get an approximated principal's utility: \(\xi(\bm{f}_{i};\theta_{\eta},\theta_{\zeta})=\eta(\bm{f}_{i};\theta_{\eta})+ \zeta(r(\bm{f}_{i});\theta_{\zeta})\), and the network \(\xi\) is trained to minimize the following loss function in \(T\) epochs (Appx. D gives more details on network architecture, infrastructure, and training):

\[\mathcal{L}_{\mathcal{T}_{K}}(\theta_{\eta},\theta_{\zeta})=\frac{1}{K}\sum_{ i=1}^{K}\left[\xi(\bm{f}_{i};\theta_{\eta},\theta_{\zeta})-u^{p}(\bm{f}_{i}) \right]^{2}.\] (5)

Given a trained DeLU network, \(\xi:\mathcal{F}\rightarrow\mathbb{R}\), we have an approximation of the principal's utility function. The next step is to find a contract that maximizes the learned utility function, that is \(\bm{f}^{*}=\operatorname*{arg\,max}_{\bm{f}}\xi(\bm{f})\). We call this the _inference process_. Since the function represented by a DeLU network is discontinuous, conventional first- or second-order optimization methods are not applicable, and, formally, we need to develop a suitable optimization approach to solve

\[\xi(\bm{f}^{*})=\max_{\rho_{i}}\max_{\bm{f}\in\mathcal{D}_{i}}\rho_{i}(\bm{f}),\] (6)

where \(\rho_{i}:\mathcal{D}_{i}\rightarrow\mathbb{R}\) is a linear piece of network \(\xi\). This motivates us to first find the piecewise optimal contracts and then get the global optimum by comparing the piecewise optima.

#### 4.2.1 Linear programming based inference

A first approach finds the optimal contract for each piece using linear programming (LP). Contracts in the same linear piece \(\rho\) result in the same activation pattern \(\bm{r}\), and the DeLU network is a linear function on the piece. In particular, the optimization problem of finding the piecewise optimum is:

\[\operatorname*{arg\,max}_{\bm{f}}\ \ \bm{W}^{(L+1)}\bm{R}^{(L)}\left[\bm{M}^{(L)} \bm{f}+\bm{z}^{(L)}\right]\ \ s.t.\ \Delta_{i}^{(l)}\left(\bm{M}_{i}^{(l)}\bm{f}+\bm{z}_{i}^{(l)}\right)\geq 0,\forall l\in[L],\ i\in[n_{l}],\]

Figure 2: The DeLU architecture.

where \([L]=\{1,\cdots,L\}\), \([n_{l}]=\{1,\cdots,n_{l}\}\), the objective is the linear function represented by the DeLU network on piece \(\rho\), and the constraints require that the activation pattern remains \(\bm{r}\). The definitions of \(\Delta_{i}^{(l)}\), \(\bm{R}^{(L)}\), \(\bm{M}^{(l)}\), and \(\bm{z}^{(l)}\) are the same as in Sec. 2, but with the activation pattern fixed to \(\bm{r}\). We omit the last-layer bias in the objective because the activation pattern is fixed for this piece, and thus the bias is constant and does not influence the piecewise optimal solution.

For each LP, there are \(m\) decision variables, representing payments for \(m\) outcomes, and \(N=\sum_{l=1}^{L}n_{l}\) (the number of neurons) constraints. LPs for each piece can be solved in polynomial time of \(m\) and \(N\), and quickly in practice via the simplex method. However, a challenge is that there are \(2^{N}\) activation patterns.1 For a DeLU network with a moderate size, we can enumerate all these pieces. For a larger DeLU network, we can approximate by collecting random contract samples and solving LPs with the corresponding activation patterns. Parallelizing these LPs may achieve better time efficiency. However, this improvement is limited by the overhead of solving a single LP and the required amount of suitable parallel computational resources. In the next section, we introduce a gradient-based method that provides better efficiency in solving single problems and better parallelism through making use of GPUs. We compare these two inference methods in Sec. 5.1.

Footnote 1: Previous research [20] found some patterns are invalid, and there are actually \(n_{N,m}=\sum_{i=0}^{m}\binom{N}{m-i}\) pieces.

#### 4.2.2 Gradient-based inference

The gradient-based inference method is based on the interior-point method (Sec. 2) and finds piecewise optimal solutions via forward and backward passes of the DeLU network. Specifically, on each linear piece, \(\rho\), we adopt the following _barrier function_:

\[\phi(\bm{f})=-\sum_{l=1}^{L}\sum_{i=1}^{n_{l}}\log\left[\Delta_{i}^{(l)}\left( \bm{M}_{i}^{(l)}\bm{f}+\bm{z}_{i}^{(l)}\right)\right].\] (7)

At the \(k\)-th round, the objective function is

\[\phi^{(k)}(\bm{f})=-\bm{W}^{(L+1)}\bm{R}^{(L)}\left[\bm{M}^{(L)}\bm{f}+\bm{z}^ {(L)}\right]+\frac{1}{t^{(k)}}\phi(\bm{f}).\] (8)

We use gradient descent initialized at \(\bm{f}^{(k-1)}\), and update \(\bm{f}\) with \(\partial\phi^{(k)}(\bm{f})/\partial\bm{f}\) to find the minimizer. A forward pass of the DeLU network gives \(\phi^{(k)}(\bm{f})\) and a backward pass is sufficient to calculate \(\partial\phi^{(k)}(\bm{f})/\partial\bm{f}\). Since forward and backward passes are naturally parallelized in modern deep learning frameworks, this method can be parallelized by processing multiple \(\bm{f}\) simultaneously. Alg. 1 gives the matrix-form expression of this parallel computation. The input \(\bm{X}^{(0)}\in\mathbb{R}^{K\times m}\) can be the training set or a random sample set, and we discuss the difference of these two settings in Appx. E.

**Inference performance and boundary alignment degree**. The performance of the proposed inference algorithms depends on the DeLU approximation quality, especially on the degree of alignment between the DeLU boundaries and the ground-truth linear piece boundaries. An interesting question is whether our training setup can achieve a high _boundary alignment degree_. A reason to think this is possible comes from observing that the MSE training loss (Eq. 5) is sensitive to misalignment between the DeLU and true boundaries. In particular, given that the jump of the utility function at boundary points can be arbitrarily large, a slight misalignment between DeLU and true boundaries can lead to a large increase in the MSE loss. Related to this, we explore an extension of the gradient-based inference method to make it more robust to possible boundary misalignment. When

Figure 3: The gradient-based inference method for finding piecewise optima, illustrated on the same instance as Fig. 1.

annealing the coefficient of the barrier function, we can check whether the principal's utility increases for each \(t^{(k)}\) value. A non-increasing utility indicates that we encounter inaccurate boundaries, and we can stop the inference to seek more robustness. We call inference with this "early stop" mechanism the _sub-argmax gradient-based inference method_.

In Sec. 5.3, we empirically evaluate the boundary alignment degree achieved by DeLU, and demonstrate that near-optimal contract-design performance is closely associated with high boundary alignment. We also show that the sub-argmax variation can improve performance of gradient-based inference, especially on tasks where the boundary alignment degree is low.

### Illustrating DeLU-based contract design

We first illustrate the DeLU approach on a randomly generated contract design instance, in this case with 2 outcomes and 1,000 actions. In this instance, we sample the values for outcomes and costs for actions uniformly from \([0,10]\). The outcome distribution for an action is further generated by applying SoftMax to a Gaussian random vector in \(\mathbb{R}^{m}\). In Fig. 1 (a), we show the exact surface of the principal's utility function \(u^{p}\) on such a randomly generated instance. We observe that \(u^{p}\) is a discontinuous, piecewise affine function, where each piece corresponds to an action of the agent. In Fig. 1 (b) and (c), we show the \(u^{p}\) surface approximated by each of a ReLU and a DeLU network trained with 40\(K\) samples. These two networks have a similar architecture, with a single hidden layer of 8 ReLU units. The difference is the additional, last-layer biases of the DeLU network, which are dependent on activation patterns. Whereas the ReLU network cannot represent the discontinuity of \(u^{p}\), and thus gives an incorrect optimal contract, the DeLU network replicates the \(u^{p}\) surface, and gives an accurate optimal contract (Fig. 1 (a)). Another interesting observation is that the DeLU network may use multiple pieces to represent an original linear region (Fig. 1 (c)). In Fig. 3, we further illustrate the inference process of the gradient-based method on this instance.

## 5 Empirical Evaluation

In this section, we design experiments to study the following aspects of DeLU contract design: **(1) Optimality** (Sec. 5.1): Can DeLU networks give solutions close to the optimal contracts? How do the solutions compare against those generated by continuous neural networks? **(2) Sample efficiency** (Sec. 5.1): How many training samples are required for accurate DeLU approximation? Is the proposed method applicable to large-scale problems? **(3) Time efficiency** (Sec. 5.2): How does the computation overhead required for DeLU learning and inference compare to those of other solvers? **(4) Inference** (Sec. 5.2): Does the gradient-based inference method provide a good tradeoff between accuracy and time efficiency? **(5) Boundary alignment degree** (Sec. 5.3): How does the boundary alignment degree affect optimality?

**Problem generation.** Experiments are carried out on random synthetic examples. The outcome distributions \(p(\cdot|a)\) are generated by applying SoftMax on a Gaussian random vector in \(\mathbb{R}^{m}\). The outcome value \(v_{o}\) is uniform on \([0,10]\). The action cost is a mixture, \(c(a)=(1-\beta_{p})c_{r}(a)+\beta_{p}c_{i}(a)\), where \(c_{r}(a)=\alpha_{p}\mathbb{E}_{o\sim p(\cdot|a)}[v_{o}]\) for scaling factor \(\alpha_{p}>0\) is a correlated cost that is proportional to the expected value of the action, \(c_{i}(a)\) is an independent cost and uniform on [0, 1], and \(\beta_{p}\) controls the weight of the independent cost. We test different problem sizes by changing the number of outcomes \(m\) and actions \(n\). For each problem size, we test various combinations of \((\alpha_{p},\beta_{p})\) to consider the influence of correlation costs. Different methods are compared on the same set of problems.

Figure 4: Optimality (normalized principal utility) of DeLU, ReLU and a direct LP solver (Oracle LP), for problems with increasing sizes.

### Optimality and efficiency.

In Fig. 4, we compare DeLU against ReLU networks as well as a baseline _linear programming (LP)_ solver (Oracle LP). Oracle LP refers to the use of LP for directly solving the contract design problem, not for inference on a trained DeLU network. It solves \(n\) LP problems, one for each action \(a\), where the objective is to maximize \(u^{p}\) with the incentive compatibility (IC) constraints associated with the action \(a\). The best of these \(n\) solutions becomes the optimal contract. Oracle LP has access to the outcome distributions \(p(\cdot|a)\) (to construct the IC constraints) that are unobservable to the DeLU and ReLU learners, but gives a _benchmark_ for the optimality of the proposed method.

We test different problem sizes in Fig. 4. Specifically, we set the number of outcomes \(m\) to 25 (\(1^{\text{st}}\) column), 50 (\(2^{\text{nd}}\) column), and 100 (\(3^{\text{rd}}\) column) and increase the number of actions \(n\) from \(2^{2}\) to \(2^{8}\). For each problem size, 12 combinations of \(\alpha_{p}\) and \(\beta_{p}\) are tested, with \((\alpha_{p},\beta_{p})\in\{0.5,0.7,0.9\}\times\{0,0.3,0.6,0.9\}\). The median performance as well as the first and third quartile (shaped area) of these 12 combinations are shown. When reporting optimality, we normalize the principal's utility achieved by DeLU/ReLU LP-based inference via dividing them by the value returned by Oracle LP. To ensure a fair comparison, DeLU and ReLU networks have the same architecture, with one hidden layer of 32 hidden units. They are trained for 100 epochs with 50\(K\) random samples.

DeLU consistently achieves better **optimality** than ReLU networks (\(+28.29\%\)) and the best contract in the training set (\(+23.84\%\)) across all problem sizes. The performance gap is particularly large for large-scale problems. For example, when the number of outcomes is larger than \(1,000\) (Fig. 5 (b)), the ReLU networks return solutions much worse (\(<20\%\)) than training samples, while DeLU can obtain a solution at least \(1.7\) times better than the best training data. As for **sample efficiency**, as shown in Fig. 5 (c), DeLU achieves near-optimality even with a very small training set.

### Comparing the two DeLU inference methods.

In Fig. 6, we fix \(m\) to 25, 50, 100 and increase \(n\) from \(2^{2}\) to \(2^{8}\) to compare the optimality of the proposed inference methods. We again test the same 12 combinations of \((\alpha_{p},\beta_{p})\). The median performance and the first and third quartile are shown for each problem size. Gradient-based inference is parallelized for 50\(K\) training samples on GPUs, while LP inference is parallelized for 5 linear pieces on CPUs. We can see that LP inference consistently provides a better solution, as the optimality of gradient-based inference (\(-7.56\%\)) is limited by the number of gradient descent steps.

The advantage of gradient-based inference is its time efficiency. In Fig. 5 (a), we compare the inference time for different problem sizes (with \(m\) fixed to 25). Gradient-based inference saves around \(50\)-\(90\%\) overhead compared to LP inference. We also note that the DeLU training and

Figure 5: (a) DeLU training and inference time compared against Oracle LP. (b) DeLU performance on large-scale problems. (c) DeLU performance with a small number of training samples.

Figure 6: Comparing the optimality of the two inference methods for solving the global optimum given a learned DeLU network, considering increasing problem sizes.

inference costs do not increase with the problem size. By comparison, the overhead of the direct LP solver grows quickly with the problem scale.

### Boundary alignment degree and its influence on DeLU optimality

In Fig. 7, we study the relationship between DeLU performance and the degree of boundary alignment.

For each contract design problem, we first calculate the boundary alignment degree achieved by the DeLU network. For this, we test a large number (50K) of contracts, and check whether they are simultaneously on the DeLU model and true utility boundary. Specifically, for each contract we randomly sample 10K directions and assess linearity of the DeLU utility model and the true utility function in each direction. The principal's utility function is piecewise linear in the proximity of an interior point. Conversely, when a point lies on a boundary, there is a jump in utility within its proximity, rendering the utility unable to pass a linearity test in some direction. In particular, if a function is non-linear in >20% of random directions, we mark the contract sample as being on a boundary (we use the exact same approach to check for boundaries of the DeLU utility function and the principal's true utility function). The _boundary alignment degree_ is calculated as the percentage of overlapped boundary points (# contract samples on both DeLU and true boundaries / # contract samples on true boundaries).

Each point in Fig. 7 represents a contract design problem, and we use the same set of problems as in the previous experiments. The x-axis is the DeLU boundary alignment degree, and the y-axis is the optimality of DeLU with LP-based inference (Fig. 7 left) and gradient-based inference (Fig. 7 middle). The right plot gives the optimality improvement achieved by the sub-argmax inference method compared to standard gradient-based inference. From Fig. 7 left, we observe that DeLU achieves good boundary alignment degrees (>80%) for most contract design problems, and that a strong positive correlation exists between the boundary alignment degree and the optimality of LP-based inference. This result indicates that the alignment degree between DeLU and true boundaries is important in supporting good performance of LP-based inference. A similar observation can be made for gradient-based inference. Fig. 7-right shows that as the boundary alignment degree increases, the optimality improvement from the sub-argmax inference compared to standard gradient-based inference becomes less significant. This confirms that the sub-argmax inference is especially helpful for contract design problems where the DeLU boundaries are less accurate.

## 6 Closing Remarks

This paper initiates the investigation of contract design from a deep learning perspective, introducing a family of piecewise discontinuous networks and inference techniques that are tailored for deep contract learning. In future work, it will be interesting to take this framework to real-world settings, provide theory in regard to the expressiveness of the function class comprising these piecewise discontinuous functions, and extend to the setting of online learning. We expect that our exploration of discontinuous networks can also draw attention to other economics problems involving discontinuity and thereby contribute to advancing AI progress in computational economics.

Figure 7: Correlation between boundary alignment degree (x-axis) and DeLU optimality (y-axis; left: LP-based inference; middle: gradient-based inference); also, the improvement achieved by sub-argmax inference compared to gradient-based inference (right). Trend lines (linear regression) are shown. Point sizes (\(\log(mn)\)) indicate the corresponding problem size.

Acknowledgements

This work received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement: 101077862, project name ALGORCONTRACT). We extend our heartfelt appreciation to the anonymous NeurIPS reviewers for their insightful questions and constructive interactions during the review process, which has inspired us to delve deeper into critical inquiries, such as the boundary alignment issue. Their feedback has been important in shaping the quality and depth of our work.

## References

* [1] Abien Fred Agarap. Deep learning using rectified linear units (ReLU). _arXiv preprint arXiv:1803.08375_, 2018.
* [2] Marat Akhmet and Enes Yilmaz. _Neural networks with discontinuous/impact activations_. Springer, 2014.
* [3] Tal Alon, Magdalen Dobson, Ariel D. Procaccia, Inbal Talgam-Cohen, and Jamie Tucker-Foltz. Multiagent evaluation mechanisms. In _AAAI 2020_, pages 1774-1781, 2020.
* [4] Tal Alon, Paul Dutting, Yingkai Li, and Inbal Talgam-Cohen. Bayesian analysis of linear contracts. In _EC '23_, page 66. ACM, 2023.
* [5] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In _ICLR 2018_, 2018.
* [6] Moshe Babaioff, Michal Feldman, Noam Nisan, and Eyal Winter. Combinatorial agency. _Journal of Economic Theory_, 147(3):999-1034, 2012.
* [7] Moshe Babaioff, Nicole Immorlica, Brendan Lucier, and S Matthew Weinberg. A simple and approximately optimal mechanism for an additive buyer. _Journal of the ACM_, 67(4):1-40, 2020.
* [8] Patrick Bolton and Mathias Dewatripont. _Contract theory_. MIT press, 2004.
* [9] Tilman Borgers. _An introduction to the theory of mechanism design_. Oxford University Press, USA, 2015.
* [10] Yang Cai, Constantinos Daskalakis, and S Matthew Weinberg. An algorithmic characterization of multi-dimensional mechanisms. In _STOC 2012_, pages 459-478, 2012.
* [11] Yang Cai, Constantinos Daskalakis, and S Matthew Weinberg. Optimal multi-dimensional mechanism design: Reducing revenue to welfare maximization. In _FOCS 2012_, pages 130-139, 2012.
* [12] Yang Cai, Nikhil R Devanur, and S Matthew Weinberg. A duality based unified approach to Bayesian mechanism design. In _STOC 2016_, pages 926-939, 2016.
* [13] Gabriel Carroll. Robustness and linear contracts. _American Economic Review_, 105(2):536-63, 2015.
* [14] Gabriel Carroll. Robustness in mechanism design and contracting. _Annual Review of Economics_, 11:139-166, 2019.
* [15] Gabriel Carroll. Contract theory. In Federico Echenique, Nicole Immorlica, and Vijay V. Vazirani, editors, _Online and Matching-Based Market Design_. Cambridge University Press, 2022. To appear.
* [16] Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. Bayesian agency: Linear versus tractable contracts. _Artificial Intelligence_, 307:103684, 2022.
* [17] Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. Designing menus of contracts efficiently: The power of randomization. In _EC 2022_, pages 705-735, 2022.

* [18] Yu Cheng, Ho Yee Cheung, Shaddin Dughmi, Ehsan Emamjomeh-Zadeh, Li Han, and Shang-Hua Teng. Mixture selection, mechanism design, and signaling. In _FOCS 2015_, pages 1426-1445, 2015.
* [19] Phillip J.K. Christoffersen, Andreas A. Haupt, and Dylan Hadfield-Menell. Get it in writing: Formal contracts mitigate social dilemmas in multi-agent RL. In _AAMAS 2023_, page 448-456, 2023.
* [20] Lingyang Chu, Xia Hu, Juhua Hu, Lanjun Wang, and Jian Pei. Exact and consistent interpretation for piecewise linear neural networks: A closed form solution. In _KDD 2018_, pages 1244-1253, 2018.
* [21] Alon Cohen, Argyrios Deligkas, and Moran Koren. Learning approximately optimal contracts. In _SAGT 2022_, pages 331-346, 2022.
* [22] Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of ReLU networks via maximization of linear regions. In _AISTATS 2019_, pages 2057-2066, 2019.
* [23] Francesco Croce and Matthias Hein. A randomized gradient-free attack on ReLU networks. In _GCPR 2019_, pages 215-227, 2019.
* [24] Francesco Della Santa and Sandra Pieraccini. Discontinuous neural networks and discontinuity learning. _Journal of Computational and Applied Mathematics_, 419:114678, 2023.
* [25] Heng Dong, Tonghan Wang, Jiayuan Liu, Chi Han, and Chongjie Zhang. Birds of a feather flock together: A close look at cooperation emergence via multi-agent rl. _arXiv preprint arXiv:2104.11455_, 2021.
* [26] Shaddin Dughmi. On the hardness of signaling. In _FOCS 2014_, pages 354-363, 2014.
* [27] Shaddin Dughmi and Haifeng Xu. Algorithmic Bayesian persuasion. In _STOC 2016_, pages 412-425, 2016.
* [28] Paul Dutting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Combinatorial contracts. In _FOCS 2021_, pages 815-826, 2022.
* [29] Paul Dutting, Tomer Ezra, Michal Feldman, and Thomas Kesselheim. Multi-agent contracts. In _STOC 2023_, pages 1311-1324, 2023.
* [30] Paul Dutting, Zhe Feng, Harikrishna Narasimhan, David C. Parkes, and Sai Srivatsa Ravindranath. Optimal auctions through deep learning: Advances in differentiable economics. _Journal of the ACM_, Forthcoming 2023. First version, ICML 2019, pages 1706-1715. PMLR, 2019.
* [31] Paul Dutting, Guru Guruganesh, Jon Schneider, and Joshua Ruizhi Wang. Optimal no-regret learning for one-sided Lipschitz functions. In _ICML 2023_, volume 202, pages 8836-8850. PMLR, 2023.
* [32] Paul Dutting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In _EC 2019_, pages 369-387, 2019.
* [33] Paul Dutting, Tim Roughgarden, and Inbal Talgam-Cohen. The complexity of contracts. _SIAM Journal on Computing_, 50(1):211-254, 2021.
* [34] Mauro Forti and Paolo Nistri. Global convergence of neural networks with discontinuous neuron activations. _IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications_, 50(11):1421-1435, 2003.
* [35] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In _AISTATS 2011_, pages 315-323, 2011.
* [36] Yannai A Gonczarowski. Bounding the menu-size of approximately optimal auctions via optimal-transport duality. In _STOC 2018_, pages 123-131, 2018.
* [37] Yannai A Gonczarowski and S Matthew Weinberg. The sample complexity of up-to-\(\varepsilon\) multi-dimensional revenue maximization. _Journal of the ACM_, 68(3):1-28, 2021.

* [38] Guru Guruganesh, Jon Schneider, and Joshua R Wang. Contracts under moral hazard and adverse selection. In _EC 2021_, pages 563-582, 2021.
* [39] Dylan Hadfield-Menell and Gillian K. Hadfield. Incomplete contracting and AI alignment. In _AIES 2023_, pages 417-422, 2019.
* [40] Chien-Ju Ho, Aleksandrs Slivkins, and Jennifer Wortman Vaughan. Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems. _Journal of Artificial Intelligence Research_, 55:317-359, 2016.
* [41] Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. _American Economic Review_, 101(6):2590-2615, 2011.
* [42] Jon Kleinberg and Manish Raghavan. Algorithmic classification and strategic effort. _ACM SIGecom Exchanges_, 18(2):45-52, 2020.
* [43] Jon M. Kleinberg and Manish Raghavan. How do classifiers induce agents to invest effort strategically? _ACM Transactions on Economics and Computation_, 8(4):19:1-19:23, 2020.
* [44] Xiaoyang Liu and Jinde Cao. Robust state estimation for neural networks with discontinuous activations. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 40(6):1425-1437, 2010.
* [45] David Martimort and Jean-Jacques Laffont. _The theory of incentives: The principal-agent model_. Princeton University Press, 2009.
* [46] Paolo Massa, Sara Garbarino, and Federico Benvenuto. Approximation of discontinuous inverse operators with neural networks. _Inverse Problems_, 38(10):105001, 2022.
* [47] Stuart Mitchell, Michael OSullivan, and Iain Dunning. Pulp: a linear programming toolkit for python. _The University of Auckland, Auckland, New Zealand_, 65, 2011.
* [48] Tong Mu, Stephan Zheng, and Alexander Trott. Solving dynamic principal-agent problems with a rationally inattentive principal. _arXiv preprint arXiv:2202.01691_, 2022.
* [49] Florian A Potra and Stephen J Wright. Interior-point methods. _Journal of Computational and Applied Mathematics_, 124(1-2):281-302, 2000.
* [50] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. _Psychological review_, 65(6):386, 1958.
* [51] Royal Swedish Academy of Sciences. Scientific background on the 2016 Nobel Prize in Economic Sciences, 2016.
* [52] Bernard Salanie. _The Economics of Contracts: A Primer_. MIT press, 2017.
* [53] Stephen J Wright. On the convergence of the Newton/log-barrier method. _Mathematical Programming_, 90:71-100, 2001.
* [54] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. _arXiv preprint arXiv:1505.00853_, 2015.
* [55] Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I. Jordan. The sample complexity of online contract design. In _EC '23_, page 1188. ACM, 2023.

Proofs of Lemma 2-4

In Sec. 3, we study the geometric structure of optimal contracts by establishing four lemmas. While some of them have been stated and proved for linear contracts [32], and the first two lemmas, at least, are not surprising, we give formal proofs of these properties of the principal's utility and optimal contracts in the general case. An important property of the principal's utility function \(u^{p}\) that we state in Lemma 2 is that \(u^{p}\) can be a discontinuous function.

**Lemma 2**.: _The principal's utility function \(u^{p}\) can be discontinuous on the boundary of linear pieces._

Proof.: For a contract \(\bm{f}\) on the boundary of two neighboring linear pieces \(\mu_{i}^{p}\) and \(\mu_{j\neq i}^{p}\), the agent is indifferent between action \(a_{i}\) and \(a_{j}\) given \(\bm{f}\): \(\mu_{i}^{a}(\bm{f})=\mu_{j}^{a}(\bm{f})\). The principal's utility

\[\mu_{i}^{p}(\bm{f}) =\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]-\mathbb{E}_{o\sim p( \cdot|a_{i})}[f_{o}]\] (9) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]-c(a_{i})-(\mathbb{E}_{o \sim p(\cdot|a_{i})}[f_{o}]-c(a_{i}))\] (10) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]-c(a_{i})-\mu_{i}^{a}( \bm{f})\] (11) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]-c(a_{i})-\mu_{j}^{a}( \bm{f})\] (12) \[=\mu_{j}^{p}(\bm{f})+\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]-c(a _{i})-(\mathbb{E}_{o\sim p(\cdot|a_{j})}[v_{o}]-c(a_{j})).\] (13)

It is possible that

\[\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]-c(a_{i})\neq\mathbb{E}_{o\sim p( \cdot|a_{j})}[v_{o}]-c(a_{j}).\] (14)

Function \(u^{p}\) is discontinuous in this case. 

Another property of the principal's utility function \(u^{p}\) that motivates our discontinuous neural networks is Lemma 3.

**Lemma 3**.: _The global optimal contract is on the boundary of a linear piece._

Proof.: Suppose that the global optimal \(\bm{f}^{*}\) is not on a boundary but is an interior point of a linear piece \(\mathcal{Q}_{i}\) (contracts in \(\mathcal{Q}_{i}\) encourage the agent to take action \(a_{i}\).):

\[\bm{f}^{*}\in\mathcal{Q}_{i}-\partial\mathcal{Q}_{i},\] (15)

where

\[\mathcal{Q}_{i} =\cap_{j\neq i}\Gamma_{i,j};\] (16) \[\Gamma_{i,j} =\left\{\bm{f}\in\mathcal{F}\mid\mathbb{E}_{o\sim p(\cdot|a_{i})} \left[f_{o}\right]-c(a_{i})\geq\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[f_{o} \right]-c(a_{j})\right\},\forall j\neq i.\] (17)

It follows that

\[\bm{f}^{*}\in\left\{\bm{f}\in\mathcal{F}\mid\mathbb{E}_{o\sim p(\cdot|a_{i})} \left[f_{o}\right]-c(a_{i})>\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[f_{o} \right]-c(a_{j}),\forall j\neq i\right\},\] (18)

where the inequality is strict. Let

\[\epsilon_{i,j}=\mathbb{E}_{o\sim p(\cdot|a_{i})}\left[f_{o}^{*}\right]-c(a_{i} )-\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[f_{o}^{*}\right]+c(a_{j}).\] (19)

It holds that \(\epsilon_{i,j}>0,\forall j\neq i\).

Now we consider the contract \(\bm{f}^{\prime}=\bm{f}^{*}-\delta p(\cdot|a_{i})\) for some small \(\delta>0\). For any \(a_{j}\neq a_{i}\), we have

\[u^{a}(\bm{f}^{\prime};a_{i})-u^{a}(\bm{f}^{\prime};a_{j})\] \[= \mathbb{E}_{o\sim p(\cdot|a_{i})}\left[f_{o}^{*}-\delta p(o|a_{i} )\right]-c(a_{i})-\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[f_{o}^{*}-\delta p(o| a_{i})\right]+c(a_{j})\] (20) \[= \epsilon_{i,j}-\delta\mathbb{E}_{o\sim p(\cdot|a_{i})}\left[p(o|a _{i})\right]+\delta\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[p(o|a_{i})\right].\]

When

\[0<\delta<\min_{j\neq i}\frac{\epsilon_{i,j}}{\mathbb{E}_{o\sim p(\cdot|a_{i})} \left[p(o|a_{i})\right]-\mathbb{E}_{o\sim p(\cdot|a_{j})}\left[p(o|a_{i}) \right]},\] (21)

(where note the denominator is \(>0\)), we have

\[u^{a}(\bm{f}^{\prime};a_{i})-u^{a}(\bm{f}^{\prime};a_{j})>0,\forall j\neq i,\] (22)which means \(\bm{f}^{\prime}\) incentivizes the agent to take action \(a_{i}\). Therefore, for \(\delta\) in this range, the principal's utility given \(\bm{f}^{\prime}\) is:

\[\begin{split} u^{p}(\bm{f}^{\prime})&=\mathbb{E}_{o \sim p(\cdot|a_{i})}\left[v_{o}-f_{o}^{*}+\delta p(o|a_{i})\right]\\ &=\mathbb{E}_{o\sim p(\cdot|a_{i})}\left[v_{o}-f_{o}^{*}\right]+ \mathbb{E}_{o\sim p(\cdot|a_{i})}\left[\delta p(o|a_{i})\right]\\ &=u^{p}(\bm{f}^{*})+\delta\sum_{o}p^{2}(o|a_{i})\\ &>u^{p}(\bm{f}^{*}).\end{split}\] (23)

We thus find a contract \(\bm{f}^{\prime}\) that induces greater utility for the principal, contradicting with the fact that \(\bm{f}^{*}\) is the global optimal contract. This finishes the proof.

Note that here we consider the boundary resulting from changes in the agent's best responses. We can extend the proof to cover another type of boundary, which pertains to the requirement that contracts are non-negative, by defining \(\mathcal{Q}_{i}\) to be \(\mathcal{Q}_{i}=\{\bm{f}|\bm{f}\geq 0\}\cap\Gamma_{i,1}\cap\cdots\cap\Gamma_{i,i- 1}\cap\Gamma_{i,i+1}\cap\cdots\). 

Lemma 4 claims another property that may influence the design of DeLU networks.

**Lemma 4**.: _The principal's utility function \(u^{p}\) can be written as a summation of a concave function and a piecewise constant function._

Proof.: We first prove the agent's utility function \(u^{a}(\bm{f})\) is a convex function.

We need to prove that, for any two contracts \(\bm{f}^{(1)}\in\mathcal{F}\) and \(\bm{f}^{(2)}\in\mathcal{F}\), it holds that \(u^{p}(\lambda\bm{f}^{(1)}+(1-\lambda)\bm{f}^{(2)})\leq\lambda u^{p}(\bm{f}^{(1 )})+(1-\lambda)u^{p}(\bm{f}^{(2)})\), \(\forall\lambda\in[0,1]\). Denote \(\bm{d}=\bm{f}^{(2)}-\bm{f}^{(1)}\). It suffices to prove that the derivative of \(u^{p}(\bm{f}^{(1)}+\delta\bm{d})\) with respect to \(\delta\) is a non-decreasing function for \(\delta\in[0,1]\).

We have

\[\frac{\partial}{\partial\delta}u^{a}(\bm{f}^{(1)}+\delta\bm{d})=\frac{\partial }{\partial\delta}\left[\mathbb{E}_{o\sim p(\cdot|a_{\delta})}[f_{o}^{(1)}+ \delta d_{o}]-c(a_{\delta})\right],\] (24)

where \(a_{\delta}\) is the agent's action given the contract \(\bm{f}^{(1)}+\delta\bm{d}\).

Case 1: When \(a_{\delta}\) does not change,

\[\frac{\partial}{\partial\delta}u^{a}(\bm{f}^{(1)}+\delta\bm{d})=\mathbb{E}_{o \sim p(\cdot|a_{\delta})}[d_{o}]\] (25)

is a constant, which is a non-decreasing function.

Case 2: When \(a_{\delta}\) changes. Suppose that there exists \(\delta_{1}\in[0,1]\) such that \(\bm{f}^{(1)}+\delta_{1}\bm{d}\) is on the boundary of linear piece \(\mathcal{Q}_{i}\) and \(\mathcal{Q}_{j}\). Let

\[\delta_{1}^{+} =\delta_{1}+\epsilon,\] \[\delta_{1}^{-} =\delta_{1}-\epsilon,\] (26)

where \(\epsilon\) is a small number and

\[\bm{f}^{(1)}+\delta_{1}^{-}\bm{d} \in\mathcal{Q}_{i},\] \[\bm{f}^{(1)}+\delta_{1}^{+}\bm{d} \in\mathcal{Q}_{j}.\] (27)

Because the agent is self-interested, it follows that \(a_{j}\) is the best response when the contract is \(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d}\):

\[u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d})=u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm {d};a_{j})>u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d};a_{i}).\] (28)

It follows that

\[\frac{\partial}{\partial\delta}u^{a}(\bm{f}^{(1)}+\delta\bm{d}) \bigg{|}_{\delta=\delta_{1}^{+}} =\lim_{\epsilon\to 0}\frac{u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d})-u ^{a}(\bm{f}^{(1)}+\delta_{1}\bm{d})}{\epsilon}\] (29) \[>\lim_{\epsilon\to 0}\frac{u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d};a_ {i})-u^{a}(\bm{f}^{(1)}+\delta_{1}\bm{d})}{\epsilon}.\] (30)We now look at the two terms in the numerator of Eq. 30:

\[u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d};a_{i}) =\mathbb{E}_{o\sim p(\cdot|a_{i})}[f_{o}^{(1)}+\delta_{1}^{+}d_{o}]- c(a_{i})\] (31) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[f_{o}^{(1)}+(\delta_{1}+\epsilon )d_{o}]-c(a_{i})\] (32) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[f_{o}^{(1)}+\delta_{1}d_{o}]- c(a_{i})+\epsilon\mathbb{E}_{o\sim p(\cdot|a_{i})}[d_{o}]\] (33) \[=u^{a}(\bm{f}^{(1)}+\delta_{1}\bm{d})+\epsilon\mathbb{E}_{o\sim p (\cdot|a_{i})}[d_{o}],\] (34)

and

\[u^{a}(\bm{f}^{(1)}+\delta_{1}\bm{d}) =\mathbb{E}_{o\sim p(\cdot|a_{i})}[f_{o}^{(1)}+\delta_{1}d_{o}]- c(a_{i})\] (35) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[f_{o}^{(1)}+(\delta_{1}^{-}+ \epsilon)d_{o}]-c(a_{i})\] (36) \[=\mathbb{E}_{o\sim p(\cdot|a_{i})}[f_{o}^{(1)}+\delta_{1}^{-}d_{ o}]-c(a_{i})+\epsilon\mathbb{E}_{o\sim p(\cdot|a_{i})}[d_{o}]\] (37) \[=u^{a}(\bm{f}^{(1)}+\delta_{1}^{-}\bm{d})+\epsilon\mathbb{E}_{o \sim p(\cdot|a_{i})}[d_{o}].\] (38)

Therefore,

\[\frac{\partial}{\partial\delta}u^{a}(\bm{f}^{(1)}+\delta\bm{d}) \bigg{|}_{\delta=\delta_{1}^{+}} >\lim_{\epsilon\to 0}\frac{u^{a}(\bm{f}^{(1)}+\delta_{1}^{+}\bm{d};a_{i} )-u^{a}(\bm{f}^{(1)}+\delta_{1}\bm{d})}{\epsilon}\] \[=\lim_{\epsilon\to 0}\frac{u^{a}(\bm{f}^{(1)}+\delta_{1}\bm{d})-u^ {a}(\bm{f}^{(1)}+\delta_{1}^{-}\bm{d})}{\epsilon}\] (39) \[=\left.\frac{\partial}{\partial\delta}u^{a}(\bm{f}^{(1)}+\delta \bm{d})\right|_{\delta=\delta_{1}^{-}}.\]

This finishes the proof that \(u^{a}\) is a convex function. Furthermore, we have that

\[u^{p}(\bm{f})=-u^{a}(\bm{f})-c(a^{*}(\bm{f}))+\mathbb{E}_{o\sim p(\cdot|a^{*} (\bm{f}))}[v_{o}],\] (40)

where \(-u^{a}(\bm{f})\) is a concave function and \(-c(a^{*}(\bm{f}))+\mathbb{E}_{o\sim p(\cdot|a^{*}(\bm{f}))}[v_{o}]\) is a piecewise constant function with the value \(-c(a_{i})+\mathbb{E}_{o\sim p(\cdot|a_{i})}[v_{o}]\) when \(\bm{f}\in\mathcal{Q}_{i}\). 

## Appendix B Why we use another network to generate the last-layer bias?

To model model the dependency of the last-layer bias on the activation pattern, we use a neural network, rather than a simpler, linear, and learnable function. The reason is that the bias does not always depend linearly on the activation pattern. Here is an example to illustrate this. There are two outcomes with values \(\mathbf{v}=[20,1]\), four actions with costs \(\mathbf{c}=[1.0,2.1,2.3,4.7]\), and the action-outcome transition kernel is

\[P=\begin{bmatrix}0.211&0.789\\ 0.398&0.602\\ 0.430&0.570\\ 0.684&0.316\end{bmatrix}.\]

Suppose we consider linear contracts, where \(\mathbf{f}=\alpha\mathbf{v},\alpha>0\). Then the principal's utility function for different contracts is:

\[u^{p}(\alpha)=\begin{cases}-5\alpha+5&0.2<\alpha<0.3\\ -8.57\alpha+8.57&0.3<\alpha<0.4\\ -9.17\alpha+9.17&0.4<\alpha<0.5\\ -14\alpha+14&\alpha>0.5\end{cases}.\]

Suppose that we have a 2-dimensional activation pattern, and the linear function converting activation patterns to the bias has parameters \([b_{1},b_{2}]\). Then the bias for each of the four pieces would be 0, \(b_{1}\), \(b_{2}\), and \(b_{1}+b_{2}\), respectively. The difference between each piece's bias needs to model the discontinuity at contract parameter \(\alpha=0.3,0.4,0.5\), but this is impossible with this linear model. To see this, we first assume that the piece \(0.2<\alpha<0.3\) has bias 0. Then the differences of biases of the other 3 pieces would need to be 2.5, 2.86, and 5.28, which cannot be achieved with \(b_{1}\), \(b_{2}\), and \(b_{1}+b_{2}\). It can be easily verified that the cases where other pieces have a bias of 0 are similar, demonstrating that a linear bias function cannot express the discontinuity. By contrast, appealing to a second network allows for non-linear dependency on activation, and can handle this problem.

## Appendix C Introducing Concavity into the DeLU Network

From Lemma 4, the principal's utility function \(u^{p}\) is a summation of a concave function and a piecewise constant function. Further, our DeLU network, which is used to approximate the function \(u^{p}\), can be written as

\[\xi(\bm{f}_{i};\theta_{\eta},\theta_{\zeta})=\eta(\bm{f}_{i};\theta_{\eta})+ \zeta(r(\bm{f}_{i});\theta_{\zeta}).\] (41)

In particular, \(\zeta(r(\bm{f}_{i});\theta_{\zeta})\) is a piecewise constant function, because given an activation pattern \(r(\bm{f}_{i})\), \(\zeta\) is a constant. However, the first term in Eq. 41, in a general DeLU network, is an arbitrary function. This raises the question as to whether it is useful to further restrict the network architecture, constraining the sub-network \(\eta\) to be a concave function. In this section, we discuss how to introduce concavity into \(\eta\), and how this restriction affects the performance of DeLU-based contract design.

### Concave DeLU architecture

We can make \(\eta\) a concave function by (1) enforcing its weights (for all but the first layer) to be non-negative, and (2) taking the negation of its output. The first modification will make \(\eta\) a convex function because \(\eta\) uses ReLU activation, which is a convex and non-decreasing function. When the weights after a ReLU activation are non-negative, \(\eta\) becomes a composition of several convex, non-decreasing functions, which is still a convex function. Since the negation of a convex function is a concave function, the second modification will make \(\eta\) concave.

Formally, in Concave DeLU, the sub-network \(\eta\) calculates

\[\bm{h}^{(L)}(\bm{x}) =|\bm{W}^{(L)}|\bm{R}^{(L-1)}(\bm{x})\left(\cdots\left(|\bm{W}^{ (2)}|\bm{R}^{(1)}(\bm{x})\left(\bm{W}^{(1)}\bm{x}+\bm{b}^{(1)}\right)+\bm{b}^{ (2)}\right)\cdots\right)+\bm{b}^{(L)},\] \[\eta(\bm{x}) =-|\bm{W}^{(L+1)}|\bm{R}^{(L)}\bm{h}^{(L)}(\bm{x}),\] (42)

where \(\bm{R}^{(k)}\) is a diagonal matrix with diagonal elements equal to the layer activation pattern \(\bm{r}^{(k)}\). The other components, as well as the training process, of Concave DeLU are the same as a DeLU network. In the next sub-section, we evaluate the performance of Concave DeLU.

### Experiments on Concave DeLU networks

In Fig. 8, we compare Concave DeLU against DeLU and the direct LP solver (Oracle LP). For this, we fix DeLU and Concave DeLU to have the same size, and both use the LP inference algorithm. We test different problem sizes. In the first and second column, we set the number of actions \(n\) to 5 and 50, respectively, and increase the number of outcomes \(m\) from \(2^{1}\) to \(2^{8}\). In the third and fourth column, we set the number of outcomes \(m\) to 5 and 50, respectively, and increase the number of actions \(n\) from \(2^{1}\) to \(2^{8}\). For each problem size, the same 12 combinations of \(\alpha_{p}\) and \(\beta_{p}\) are tested. The median

Figure 8: Optimality (normalized principal utility, divided by the result obtained by the direct LP solver Oracle LP) and inference time of DeLU, Concave DeLU, and Oracle LP on increasing problem sizes.

performance as well as the first and third quartile (shaped area) of these 12 combinations are shown. The first row compares optimality, while the second row compares inference time efficiency. We again report normalized principal utilities when it comes to optimality.

A surprising result is that for most problem sizes, DeLU achieves better optimality than Concave DeLU. Although the function class represented by Concave DeLU is a better fit with \(u^{p}\), it seems that the larger function class of DeLU aids optimization and leads to better performance. At the same time, it is somewhat surprising that Concave DeLU can reduce inference time substantially. Unlike DeLU, the inference time of Concave DeLU remains relatively stable when the problem size increases. We conjecture that this behavior can be attributed to the non-negativity constraint on network weights, which reduces the number of valid activation patterns and speeds up LP inference.

## Appendix D Experimental Setups

### Network architecture and training

We use a simple architecture for the DeLU network. In all experiments, the sub-network \(\eta\) has one hidden layer with 32 hidden units and ReLU activations. We deliberately restrict the size of this sub-network to limit the number of valid activation patterns and speedup LP-based inference. However, this restriction may reduce the representational capacity of DeLU networks: it is in contrast to the common practice of overparameterization, which has contributed to the success of deep learning. To alleviate this concern, we employ a relatively larger network for the bias network \(\zeta\). In our experiments, \(\zeta\) has a hidden layer with 512 (Tanh-activated) neurons.

The two sub-networks \(\eta\) and \(\zeta\) are trained in an end-to-end manner by the MSE loss (Eq. 5). The optimization is conducted using RMSprop with a learning rate of \(1\times 10^{-3}\), \(\alpha\) of 0.99, and with no momentum or weight decay. For the DeLU and the baseline ReLU networks, training samples are randomly shuffled in each of the training epochs.

### Infrastructure

Across all experiments, DeLU, and the baseline ReLU networks are trained on a NVIDIA A100 GPU. Gradient-based inference is also parallelized on the A100 GPU. The direct LP solver (Oracle LP) and the LP-based inference algorithm are based on the linear programming toolkit PuLP [47], and we parallelize five LP solvers on CPUs.

## Appendix E More Experiments

In this section, we carry out experiments to study (1) using random samples to initialize gradient-based inference (Appx. E.1); and (2) the influence of cost correlation on the optimality of DeLU learners (Appx. E.2).

### Gradient-based inference initialized with random samples

In Sec. 4.2.2, we introduce a gradient-based inference algorithm, and Alg. 1 gives the matrix-form expression of its parallel implementation. There is a choice in using Alg. 1, as to whether the input ("probe points") \(\bm{X}^{(0)}\in\mathbb{R}^{K\times m}\) is taken from the training set or a different, random sample set. In this section, we report the results of experiments to empirically compare these two setups.

We start from the same trained DeLU networks on small (\(n=5,m=16\)), middle (\(n=32,m=50\)), and large (\(n=50,m=128\)) problem sizes, where \(n\) is the number of actions and \(m\) is the number of outcomes. For each problem size, we consider 12 different combinations of \((\alpha_{p},\beta_{p})\) as in other experiments and report the mean and variance of the performance. We run Alg. 1 with two different inputs \(\bm{X}^{(0)}\): Training Set uses 50\(K\) training samples while Random Set uses 50\(K\) randomly generated contracts. In Table. 1, we present the normalized principal utility (divided by the result obtained by Oracle LP) of these two setups.

We can observe that the optimality of these two setups are very close, especially when the problem size is large. We thus recommend running Alg. 1 initialized with the training set to reduce the possible time and memory overhead of generating a new random sample set.

### Influence of correlated costs

Across our experiments, we test 12 different combinations of \(\alpha_{p}\) and \(\beta_{p}\), where \((\alpha_{p},\beta_{p})\in\{0.5,0.7,0.9\}\times\{0,0.3,0.6,0.9\}\). Recall that a greater \(\beta_{p}\) value means we put more weights on the independent cost, and a greater \(\alpha_{p}\) value indicates that the correlated cost is more close to the expected value of the action. It is interesting to investigate the influence of these two parameters on the performance of DeLU contract designers.

In Table 2, we show the optimality of DeLU learners (using the LP inference algorithm) under different \(\alpha_{p}\) and \(\beta_{p}\) values. For each \((\alpha_{p},\beta_{p})\) combination, we test 24 different problem sizes: \((m,n)\in\{25,50,100\}\times\{2,4,8,16,32,64,128,256\}\). We give the median and standard deviation of these 24 instances. We observe that \(\beta_{p}\) exerts influence on the performance of DeLU networks: optimality of the learned contracts generally increases for greater values of \(\beta_{p}\), whatever the value of \(\alpha_{p}\), and we see that DeLU seems better suited to handle problems in which the costs of actions are relatively independent of the expected values of actions. This claim is also supported by the results regarding \(\alpha_{p}\), where the optimality under \(\alpha_{p}=0.5\) is typically better than those under other \(\alpha_{p}\) values for most \(\beta_{p}\) values. It will be interesting in future work to further study this phenomenon, and to see whether suitable modifications can be made to the DeLU architecture to further improve optimality in regimes with higher cost correlation.

\begin{table}
\begin{tabular}{c c c} \hline Problem size (\(n\), \(m\)) & Training Set & Random Set \\ \hline (5, 16) & 95.29 \(\pm\) 0.20 & 95.33 \(\pm\) 0.19 \\ (32, 50) & 88.43 \(\pm\) 0.97 & 88.46 \(\pm\) 0.98 \\ (50, 128) & 94.28 \(\pm\) 0.02 & 94.28 \(\pm\) 0.02 \\ \hline \end{tabular}
\end{table}
Table 1: Optimality (normalized principal utility %) of two setups of gradient-based inference: using the training set (Training Set) or a random sample set (Random Set) as input \(\bm{X}^{(0)}\) of Alg. 1. Mean and variance over 12 different combinations of \((\alpha_{p},\beta_{p})\) are shown. In this table, the problem size is defined by (\(n\), \(m\)), where \(n\) is the number of actions and \(m\) is the number of outcomes.

\begin{table}
\begin{tabular}{c|c c c} \hline  & \(\alpha_{p}=0.5\) & \(\alpha_{p}=0.7\) & \(\alpha_{p}=0.9\) \\ \hline \(\beta_{p}=0.0\) & 88.79\(\pm\)12.77 & 89.67\(\pm\)12.57 & 87.20\(\pm\)11.97 \\ \(\beta_{p}=0.3\) & 93.94\(\pm\)11.16 & 93.28\(\pm\)11.48 & 87.94\(\pm\)13.45 \\ \(\beta_{p}=0.6\) & 95.22\(\pm\)7.93 & 95.08\(\pm\)8.78 & 91.97\(\pm\)10.32 \\ \(\beta_{p}=0.9\) & 97.23\(\pm\)6.62 & 90.69\(\pm\)18.95 & 96.43\(\pm\)7.45 \\ \hline \end{tabular}
\end{table}
Table 2: Optimality (normalized principal utilities %) of DeLU networks under different combinations of \((\alpha_{p},\beta_{p})\). Mean and variance over 32 problem sizes are shown.