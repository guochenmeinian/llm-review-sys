# Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms

 Qian Yu

Princeton University

qy4628@princeton.edu

&Yining Wang

University of Texas at Dallas

yining.wang@utdallas.edu

Baihe Huang

University of California, Berkeley

baihe_huang@berkeley.edu

&Qi Lei

New York University

q1518@nyu.edu

&Jason D. Lee

Princeton University

Jasondl@princeton.edu

###### Abstract

In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called _energy allocation_, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributions, which are enabled by a truncation method.

## 1 Introduction

Stochastic optimization in the zeroth order (gradient-free) setting has attracted significant attention in recent decades. It naturally arises in various applications such as autonomous driving [12], AI gaming [22], robotics [14], healthcare [23], and education [13]. This setting is particularly important when the gradient of the objective function cannot be directly evaluated or is expensive.

An important case of interest in the zeroth order setting is the bandit optimization of smooth and strongly-convex functions. Although there are ample results regarding the minimax rates of this problem [1, 8, 21, 6, 3, 24], little is known about how its complexity depends on the geometry of the objective function \(f\) near the global optimum \(x^{*}\), as specified by the quadratic approximation \(\frac{1}{2}(x-x^{*})^{\top}\nabla^{2}f(x^{*})(x-x^{*})\). As an initial step, we investigate the following natural questions:

* For zeroth-order bandit optimization problems of quadratic functions of the form \(\frac{1}{2}(x-x_{0})^{\top}A(x-x_{0})\), what is the optimal instance-dependent upper bound with respect to \(A\)?
* Is there an algorithm that universally achieves the optimal instance-dependent bounds for all quadratic functions, but without the knowledge of Hessian?

As our main contributions, we fully addressed the above questions as follows. First, we established the tight Hessian-dependent upper and lower bounds of the simple regret. Our bounds indicate asymptotic sample complexity bounds of \(\text{Tr}^{2}(A^{-\frac{1}{2}})/\left(2\epsilon\right)\) to achieve \(\epsilon\) accuracy. This covers theminimax lower bound of \(\Omega(d^{2}/\epsilon)\) in [21]. Second, we prove the existence of a Hessian-independent algorithm with a matching upper bound of \(O\left(\text{Tr}^{2}(A^{-\frac{1}{2}})/\epsilon\right)\). Thus, we complete the theory of zeroth-order bandit optimization on quadratic functions.

Related works beyond linear or convex bandit optimization.Compared to their linear/convex counterparts, much less is known for finding optimal bandit under nonlinear/non-convex reward function. A natural next step beyond linear bandits is to look at quadratic reward functions, as studied in this paper and some prior work (see reference on bandit PCA [15; 4], its rank-1 special cases [18; 11], bilinear [10], low-rank linear [19] and some other settings [9; 5; 16]).

When getting beyond quadratic losses, prior work on non-convex settings mainly focuses on finding achieving \(\epsilon\)-stationary points instead of \(\epsilon\)-optimal reward [17], except for certain specific settings (see, e.g., [7; 25; 26; 20; 26; 20]).

## 2 Problem Formulation and Main Results

We first present a rigorous formulation for the stochastic zeroth-order optimization problem studied in this paper. Given a fixed dimension parameter \(d\), let \(f:\mathcal{B}\rightarrow\mathbb{R}\) be an unknown objective function defined on the closed unit \(L_{2}\)-ball \(\mathcal{B}\) of the Euclidean space \(\mathbb{R}^{d}\), which takes the following form for some positive-semidefinite (PSD) matrix \(A\).

\[f(\bm{x})=\frac{1}{2}(\bm{x}-\bm{x}_{0})^{\intercal}A(\bm{x}-\bm{x}_{0}).\] (1)

For each time \(t\in[T]\), an optimization algorithm \(\mathcal{A}\) produces a query point \(\bm{x}_{t}\in\mathcal{B}\), and receives

\[y_{t}=f(\bm{x}_{t})+w_{t}.\] (2)

The algorithm can be _adaptive_, so that \(\mathcal{A}\) is described by a sequence of conditional distributions for choosing each \(\bm{x}_{t}\) based on all prior observations \(\{\bm{x}_{\tau},y_{\tau}\}_{\tau<t}\). Then we only assume that the noises \(\{w_{t}\}_{t=1}^{T-1}\) are independent random variables with zero mean and unit variance, i.e., \(\mathbb{E}[w_{t}|\bm{x}_{t}]=0\) and \(\mathbb{E}[w_{t}^{2}|\bm{x}_{t}]\leq 1\).

For brevity, we use \(\mathcal{F}(A)\) to denote all functions \(f\) satisfying equation (1) for some \(\bm{x}_{0}\in\mathcal{B}\). We are interested in the following minimax _simple_ regret for any PSD matrix \(A\).

\[\mathfrak{R}(T;A):=\inf_{\mathcal{A}}\sup_{f\in\mathcal{F}(A)}\mathbb{E}\left[ f(\bm{x}_{T})\right].\] (3)

The above quantity characterizes the simple regrets achievable by algorithms with perfect Hessian information. We also aim to identify the existence of algorithms that universally achieves the minimax regret for all \(A\), without having access to that knowledge.

Our first result provides a tight characterization for the asymptotics of the minimax regret.

**Theorem 2.1**.: _For any PSD matrix \(A\), we have_

\[\limsup_{T\rightarrow\infty}\mathfrak{R}(T;A)\cdot T\leq\frac{1}{2}\left( \text{Tr}(A^{-\frac{1}{2}})\right)^{2},\] (4)

_where \(A^{-\frac{1}{2}}\) denotes the pseudo inverse of \(A^{\frac{1}{2}}\). Moreover, if the distributions of \(w_{t}\) are i.i.d. standard Gaussian, the above bound provides a tight characterization, i.e., there is a matching lower bound that the following equality is implied._

\[\lim_{T\rightarrow\infty}\mathfrak{R}(T;A)\cdot T=\frac{1}{2}\left(\text{Tr} (A^{-\frac{1}{2}})\right)^{2}.\]

We prove Theorem 2.1 in Section 3. More generally, we also provide a full characterization of \(\mathfrak{R}(T;A)\) for the non-asymptotic regime, stated as follows and proved in Appendix C.

**Theorem 2.2**.: _For any PSD matrix \(A\) and \(T>3\)\(\text{dim}A\), where \(\text{dim}A\) denotes the rank of \(A\), we have_

\[\mathfrak{R}(T;A)=\begin{cases}\Theta\left(\frac{\left(\sum_{k=1}^{k^{*}} \lambda_{k}^{-\frac{1}{2}}\right)^{2}}{T}+\lambda_{k^{*}+1}\right)\text{ if }k^{*}<\text{dim}A\\ \Theta\left(\frac{\left(\text{Tr}(A^{-\frac{1}{2}})\right)^{2}}{T}\right) \text{ otherwise}\end{cases}\]_where \(\lambda_{j}\) is the \(j\)th smallest eigenvalue of \(A\) and \(k^{*}\) is the largest integer in \(\{0,1,...,\text{dim}A\}\) satisfying \(T\geq\left(\sum_{k=1}^{k^{*}}\lambda_{k}^{-\frac{1}{2}}\right)\left(\sum_{k=1}^{ k^{*}}\lambda_{k}^{-\frac{3}{2}}\right)\)._

**Remark 2.3**.: _While our formulation requires the algorithm to return estimates from the bounded domain, i.e., \(\bm{x}_{T}\in\mathcal{B}\), our lower bound applies to algorithms without this constraint as well. This can be proved by showing that the worst-case simple regret over all \(f\in\mathcal{F}(A)\) is always achievable by estimators satisfying \(\bm{x}_{T}\in\mathcal{B}\). Their construction can be obtained using the projection step in Algorithm 1, and proof details can be found in Appendix A._

Finally, we provide a positive answer to the existence of universally optimal algorithms, stated in the following Theorem. We present the algorithm and prove its achievability guarantee in Section 4.

**Theorem 2.4**.: _There exists an algorithm \(\mathcal{A}\), which does not depend on the Hessian parameter \(A\), such that for \(A\) being any PSD matrix, the achieved minimax simple regret satisfies_

\[\limsup_{T\rightarrow\infty}\;\sup_{f\in\mathcal{F}(A)}\mathbb{E}\left[f(\bm {x}_{T})\right]\cdot T=O\left((\text{Tr}(A^{-\frac{1}{2}}))^{2}\right).\]

## 3 Proof of Theorem 2.1

To motivate the main construction ideas, we start by proving a weaker version of the lower bound in Theorem 2.1. We use the provided intuition to construct a matching upper bound. Then we complete the proof by strengthening the lower bound through a Bayes analysis and the uncertainty principle.

### Proof of a Weakened Lower Bound

Assuming \(A\) being positive definite, we prove the following inequality when the additive noises \(w_{t}\)'s are i.i.d. standard Gaussian.

\[\liminf_{T\rightarrow\infty}\mathfrak{R}(T;A)\cdot T\geq\Omega\left((\text{ Tr}(A^{-\frac{1}{2}}))^{2}\right).\] (5)

Throughout this section, we fix any basis such that \(A=\text{diag}(\lambda_{1},...,\lambda_{d})\). We construct a class of hard instances by letting

\[\bm{x}_{0}\in\mathcal{X}_{\text{H}}\triangleq\left\{(x_{1},x_{2},...,x_{d}) \;\middle|\;x_{k}=\pm\sqrt{\frac{\lambda_{k}^{-\frac{3}{2}}\left(\sum_{j=1}^{d }\lambda_{j}^{-\frac{1}{2}}\right)}{2T}},\forall k\in[d]\right\}.\]

We investigate a Bayes setting where the objective function is defined by equation (1) and \(\bm{x}_{0}\) is uniformly random on the above set. For convenience, let \(\bm{x}_{t}=(x_{1,t},...,x_{d,t})\) be the action of the algorithm at time \(t\). We define the _energy spectrum_ to be a vector \(\bm{R}=(R_{1},...,R_{d})\) with each entry given by

\[R_{k}=\mathbb{E}\left[\sum_{t=1}^{T}x_{k,t}^{2}\right].\]

Intuitively, our proof is to show that an allocation of at least \(R_{k}\geq\Omega(\lambda_{k}^{-2}x_{k}^{-2})\) energy is required to correctly estimate each \(x_{k}\) with \(\Theta(1)\) probability. Note that for any entry that is incorrectly estimated, a penalty of \(\Omega(\lambda_{k}x_{k}^{2})\) is applied to the simple regret. This penalty is proportional to the required energy, which is due to the design of each \(x_{k}\). Meanwhile, the total expected energy is upper bounded by \(T\), which is no greater than the summation of the individual requirements. Therefore, an \(\Omega(1)\) fraction of the penalty is guaranteed, resulting in an overall effect of \(\Omega\left(\sum_{k}\lambda_{k}x_{k}^{2}\right)=\Omega\left((\text{Tr}(A^{- \frac{1}{2}}))^{2}/T\right)\) on the simple regret.

Rigorously, consider any fixed algorithm, we define the estimation cost of each entry as follows.

\[E_{k}=\mathbb{E}\left[\frac{1}{2}\lambda_{k}(x_{k,T}-x_{k})^{2}\right].\]For brevity, let \(s_{k}=\text{sign}(x_{k})\). The above function is minimized by the minimum mean square error (MMSE) estimator (e.g., see [2, Sec. 4.6]), which depends on the following log-likelihood ratio (LLR) function.

\[L_{k}\triangleq\log\frac{\mathbb{P}\left[s_{k}=1|\{\bm{x}_{\tau},y_{\tau}\}_{ \tau<T}\right]}{\mathbb{P}\left[s_{k}=-1|\{\bm{x}_{\tau},y_{\tau}\}_{\tau<T} \right]}.\]

Specifically, the MMSE estimator is given by \(\widehat{x}_{k,T}\triangleq|x_{k}|\tanh\frac{L_{k}}{2}\), and the resulting error conditioned on any fixed \(\{\bm{x}_{\tau},y_{\tau}\}_{\tau<T}\) has an expectation of \(x_{k}^{2}\cdot\text{sech}^{2}\frac{L_{k}}{2}\). Hence, we have the following lower bound for each cost entry.

\[E_{k}\geq\mathbb{E}\left[\frac{1}{2}\lambda_{k}(\widehat{x}_{k,T}-x_{k})^{2} \right]=\frac{1}{2}\lambda_{k}x_{k}^{2}\cdot\mathbb{E}\left[\text{sech}^{2} \frac{L_{k}}{2}\right].\] (6)

By the Gaussian noise assumption, the conditional expectation of LLR can be written as follows.

\[\mathbb{E}\left[L_{k}|s_{k}\right] =\mathbb{E}\left[\sum_{t}-\frac{\left(y_{t}-\frac{1}{2}\lambda_{ k}(x_{k,t}-x_{k})^{2}\right)^{2}}{2}+\frac{\left(y_{t}-\frac{1}{2}\lambda_{k}(x_{k,t }+x_{k})^{2}\right)^{2}}{2}\bigg{|}s_{k}\right]\] \[=2s_{k}\left(\lambda_{k}x_{k}\right)^{2}\mathbb{E}\left[\sum_{t} x_{k,t}^{2}\bigg{|}s_{k}\right].\]

The above quantity equals the KL divergence between the distributions of action-reward sequences generated by \(s_{k}=\pm 1\). Clearly, it has the following connection to the energy spectrum.

\[\mathbb{E}\left[L_{k}s_{k}\right]=2\left(\lambda_{k}x_{k}\right)^{2}\mathbb{E }\left[\sum_{t}x_{k,t}^{2}\right]=2\left(\lambda_{k}x_{k}\right)^{2}R_{k}.\] (7)

Recall inequality (6), it also implies the following lower bound of the mean-squared error.

\[\mathbb{E}\left[L_{k}s_{k}\right]=\mathbb{E}\left[L_{k}\mathbb{E}\left[s_{k} |L_{k}\right]\right]=\mathbb{E}\left[L_{k}\text{tanh}\frac{L_{k}}{2}\right] \geq 2-2\mathbb{E}\left[\text{sech}^{2}\frac{L_{k}}{2}\right]\geq 2-\frac{4E_{k}}{ \lambda_{k}x_{k}^{2}}.\] (8)

Combine inequality (7) and (8), the overall simple regret can be bounded as follows.

\[\mathbb{E}\left[f(\bm{x}_{T})\right]=\sum_{k}E_{k}\geq\frac{1}{2}\sum_{k} \left(1-\lambda_{k}^{2}x_{k}^{2}R_{k}\right)\lambda_{k}x_{k}^{2}.\] (9)

Note that the definition of \(\mathcal{X}_{H}\) implies that the value of each \(x_{k}^{2}\) is fixed. Hence,

\[\mathbb{E}\left[f(\bm{x}_{T})\right]\geq\frac{\left(\sum_{j}\lambda_{j}^{- \frac{1}{2}}\right)^{2}}{4T}-\frac{\left(\sum_{j}\lambda_{j}^{-\frac{1}{2}} \right)^{2}}{8T^{2}}\sum_{k}R_{k}\geq\frac{\left(\sum_{j}\lambda_{j}^{-\frac{1 }{2}}\right)^{2}}{8T}=\frac{\left(\text{Tr}(A^{-\frac{1}{2}})\right)^{2}}{8T},\]

where the last inequality uses the fact that the total energy is upper bounded by the number of samples.

Finally, when \(T\) is sufficiently large, we have \(||\bm{x}_{0}||_{2}\leq 1\) almost surely, which implies that our hard-instance functions belong to the set of objective functions \(\mathcal{F}(A)\). Therefore, \(\mathbb{E}\left[f(\bm{x}_{T})\right]\) provides a lower bound of the asymptotic minimax regret, and we can conclude that

\[\liminf_{T\to\infty}\mathfrak{R}(T;A)\cdot T\geq\frac{1}{8}\left(\text{Tr}(A^ {-\frac{1}{2}})\right)^{2}.\]

**Remark 3.1**.: _The validity of the hard-instance functions requires \(T=\Omega\left(\left(\sum_{k}\lambda_{k}^{-\frac{1}{2}}\right)\left(\sum_{k} \lambda_{k}^{-\frac{3}{2}}\right)\right)\), which is consistent with the transition threshold in Theorem 2.2 to the non-asymptotic regimes._

### Proof of the Upper Bound

Now we provide a proof for equation (4), which is implied by the following result.

**Proposition 3.2**.: _For any PSD matrix \(A\), let \(k^{*}\) be the rank, \(\lambda_{1},...,\lambda_{k^{*}}\) be the non-zero eigenvalues, and \(\bm{e}_{1},\bm{e}_{2},...,\bm{e}_{k^{*}}\) be the associated orthonormal eigenvectors. Then the expected simple regret achieved by algorithm 1 satisfies_

\[\limsup_{T\to\infty}\sup_{f\in\mathcal{F}(A)}\mathbb{E}[f(\bm{x}_{T})]\cdot T \leq\frac{1}{2}\Big{(}\text{Tr}(A^{-\frac{1}{2}})\Big{)}^{2}.\] (10)Proof.: Consider an eigenbasis of \(A\) with \(\bm{e}_{1},...,\bm{e}_{k^{*}}\) being the first \(k^{*}\) vectors. For each \(k\in[k^{*}]\), the algorithm allocates \(R_{k}\) energy to estimate the \(k\)th entry of \(\bm{x}_{0}\). The value of \(R_{k}\) is chosen such that the hard instances in the earlier subsection maximizes (modulo a constant factor) the lower bound in inequality (9) (i.e., \(R_{k}\) satisfies \(x_{k}^{2}\asymp 1/(\lambda_{k}^{2}R_{k})\)) while ensuring the total number of samples does not exceed \(T-1\). By the zero-mean and unit-variance assumptions of the noise distribution, we have

\[\mathbb{E}[\left(\alpha_{k}-\bm{x}_{0}\cdot\bm{e}_{k}\right)^{2}]=\frac{1}{2 \lambda_{k}^{2}t_{k}}\leq\frac{\lambda_{k}^{-\frac{3}{2}}\cdot\sum_{j=1}^{k^{* }}\lambda_{j}^{-\frac{1}{2}}}{T-2d-1}.\]

This essentially provides an unbounded estimator \(\widehat{\bm{x}}\triangleq\sum_{k=1}^{k^{*}}\alpha_{k}\bm{e}_{k}\) that satisfies

\[\limsup_{T\to\infty}\sup_{f\in\mathcal{F}(A)}\mathbb{E}[f( \widehat{\bm{x}})]\cdot T =\limsup_{T\to\infty}\sup_{\bm{x}_{0}\in\mathcal{B}}\frac{1}{2} \sum_{k=1}^{k^{*}}\lambda_{k}\mathbb{E}[(\alpha_{k}-\bm{x}_{0}\cdot\bm{e}_{k}) ^{2}]\cdot T\] \[\leq\frac{\left(\sum_{j=1}^{k^{*}}\lambda_{j}^{-\frac{1}{2}} \right)^{2}}{2}\cdot\limsup_{T\to\infty}\frac{T}{T-2d-1}\] \[=\frac{1}{2}\Big{(}\text{Tr}(A^{-\frac{1}{2}})\Big{)}^{2}.\] (11)

Then, to obtain \(\bm{x}_{T}\) within the bounded constraint set, \(\widehat{\bm{x}}\) is projected to \(\mathcal{B}\) under a pseudometric defined by matrix \(A\). Due to the convexity of set \(\mathcal{B}\), we have \(f(\widehat{\bm{x}})\geq f(\bm{x}_{T})\) with probability 1 (see Appendix A for a proof). Therefore, inequality (10) is implied by inequality (11). 

### Proof of the Lower Bound with Tight Constant Factors

To complete the proof of Theorem 2.1, it remains to show that for standard Gaussian noise, we have

\[\liminf_{T\to\infty}\mathfrak{R}(T;A)\cdot T\geq\frac{1}{2}\Big{(}\text{Tr} (A^{-\frac{1}{2}})\Big{)}^{2}.\] (12)

Notice that the object function and reward feedbacks depend only on the projections of \(\bm{x}_{0}\) and query points onto the column (or row) space of \(A\). Hence, it suffices to focus on algorithms with actions that are constrained on this subspace. This reduces the original problem to an equivalent instance that is defined on a (possibly) lower dimensional space and by a non-singular \(A\). Therefore, we only need to prove inequality (12) for those reduced cases (i.e., when \(A\) is full-rank).

We lower bound the minimax regret by comparing them with a Bayes estimation error, where \(\bm{x}_{0}\) has a prior distribution that violates the bounded-norm constraint. Formally, for any fixed algorithm \(\mathcal{A}\), we can consider its expected simple regret \(\mathbb{E}[f(\bm{x}_{T})]\) over an extended class of objective functions where \(f\) is defined by equation (1) but \(\bm{x}_{0}\) is chosen from the entire Euclidian space. Then for any distribution of \(\bm{x}_{0}\), the overall expectation is upper bounded by the following inequality.

\[\mathbb{E}_{\bm{x}_{0}}\left[\mathbb{E}[f(\bm{x}_{T})]\right]\leq\mathbb{P}[ \bm{x}_{0}\in\mathcal{B}]\cdot\sup_{f\in\mathcal{F}(A)}\mathbb{E}[f(\bm{x}_{T} )]+\mathbb{E}\left[\mathbbm{1}(\bm{x}_{0}\notin\mathcal{B})\cdot\sup_{\bm{x} \in\mathcal{B}}f(\bm{x})\right],\] (13)where the first term on the RHS above is obtained by taking the supremum over all objective functions that satisfies \(\bm{x}_{0}\in\mathcal{B}\), then the second term is obtained from the adversarial choice over all estimators for \(\bm{x}_{0}\notin\mathcal{B}\). Compare the RHS of inequality (13) with equation (3), a lower bound of the minimax simple regret \(\mathfrak{R}(T;A)\) can be obtained by taking the infimum over algorithm \(\mathcal{A}\) on both sides. Hence, it remains to characterize the optimal Bayes estimation error on the LHS.

To provide a concrete analysis, let \(\bm{x}_{0}\) be a Gaussian vector with zero mean and a covariance of \(T^{-\frac{2}{3}}\cdot I_{d}\), where \(I_{d}\) denotes the identity matrix.1 Under this setting, conditioned on any realization of queries \(\bm{x}_{1},...,\bm{x}_{T-1}\) and feedbacks \(y_{1},...,y_{T-1}\), the posterior distribution of \(\bm{x}_{0}\) is proportional to

Footnote 1: For the purpose of our proof, the covariance of \(\bm{x}_{0}\) can be arbitrary as long as their inverse is asymptotically large but sublinear w.r.t. \(T\).

\[\exp\left(-\frac{||\bm{x}_{0}||_{2}^{2}\cdot T^{\frac{2}{3}}}{2}-\sum_{t=1}^{ T-1}\frac{(y_{t}-\frac{1}{2}(\bm{x}_{t}-\bm{x}_{0})^{\intercal}A(\bm{x}_{t}- \bm{x}_{0}))^{2}}{2}\right).\]

Clearly, the Bayesian error can be lower bounded by the expectation of the conditional covariance, which is further characterized by the following principle (see Appendix B for a proof).

**Proposition 3.3** (Uncertainty Principle).: _Let \(\bm{Z}\) be a random variable on any measurable space and \(\theta\) be a real-valued random variable dependent of \(\bm{Z}\). If the conditional distribution of \(\theta\) given \(\bm{Z}\) has a density function \(f_{\bm{Z}}(\theta)\), and \(\ln f_{\bm{Z}}(\theta)\) has a second derivative that is integrable over \(f_{\bm{Z}}\), then_

\[\mathbb{E}\left[\operatorname{Var}[\theta|\bm{Z}]\right]\geq\frac{1}{\mathbb{ E}\left[-\frac{\partial^{2}}{\partial\theta^{2}}\ln f_{\bm{Z}}(\theta)\right]}.\]

Hence, by taking the second derivative, the squared estimation error for the \(k\)th entry of \(\bm{x}_{0}\) is lower bounded by the inverse of the following expectation.

\[\mathbb{E}\left[T^{\frac{2}{3}}+\left(\sum_{t=1}^{T-1}A(\bm{x}_{t }-\bm{x}_{0})(\bm{x}_{t}-\bm{x}_{0})^{\intercal}A^{\intercal}-Aw_{t}\right)_{ kk}\right]\] \[\qquad=T^{\frac{2}{3}}+\mathbb{E}\left[\left(\sum_{t=1}^{T-1}A( \bm{x}_{t}-\bm{x}_{0})(\bm{x}_{t}-\bm{x}_{0})^{\intercal}A^{\intercal}\right)_ {kk}\right],\]

where \((\cdot)_{kk}\) denotes the \(k\)th diagonal entry of the given matrix. Recall that we chose a basis where \(A\) is diagonal. The overall Bayes error is bounded as follows

\[\mathbb{E}_{\bm{x}_{0}}\left[\mathbb{E}\left[f(\bm{x}_{T})\right]\right]\geq \sum_{k}\frac{\lambda_{k}}{2}/\left(T^{\frac{2}{3}}+\lambda_{k}^{2}\mathbb{E} \left[\left(\sum_{t=1}^{T-1}(\bm{x}_{t}-\bm{x}_{0})(\bm{x}_{t}-\bm{x}_{0})^{ \intercal}\right)_{kk}\right]\right),\]

where \(\lambda_{k}=(A)_{kk}\) is the \(k\)th eigenvalue of \(A\). By Cauchy's inequality, the RHS above can be further bounded by

\[\frac{\frac{1}{2}\left(\sum_{k}\lambda_{k}^{-\frac{1}{2}}\right)^{2}}{\sum_{k }\left(T^{\frac{2}{3}}\lambda_{k}^{-2}+\mathbb{E}\left[\left(\sum_{t=1}^{T-1}( \bm{x}_{t}-\bm{x}_{0})(\bm{x}_{t}-\bm{x}_{0})^{\intercal}\right)_{kk}\right] \right)}=\frac{\frac{1}{2}\left(\text{Tr}(A^{-\frac{1}{2}})\right)^{2}}{T^{ \frac{2}{3}}\text{Tr}(A^{-2})+\sum_{t=1}^{T-1}\mathbb{E}\left[\left||\bm{x}_{t }-\bm{x}_{0}||_{2}^{2}\right]\right]}.\]

Note that by triangle inequality

\[\mathbb{E}\left[||\bm{x}_{t}-\bm{x}_{0}||_{2}^{2}\right]\leq\mathbb{E}\left[ \left(1+||\bm{x}_{0}||_{2}\right)^{2}\right]\leq\left(1+\mathbb{E}\left[||\bm{x }_{0}||_{2}^{2}\right]^{\frac{1}{2}}\right)^{2}=\left(1+d^{\frac{1}{2}}T^{-\frac {1}{3}}\right)^{2},\]

where \(d\) is the dimension of the action space. We have obtained a lower bound of \(\mathbb{E}_{\bm{x}_{0}}\left[\mathbb{E}\left[f(\bm{x}_{T})\right]\right]\) that is independent of the algorithm. Formally,

\[\inf_{\mathcal{A}}\mathbb{E}_{\bm{x}_{0}}\left[\mathbb{E}\left[f(\bm{x}_{T}) \right]\right]\geq\frac{1}{2}\left(\text{Tr}(A^{-\frac{1}{2}})\right)^{2}\! \Big{/}\!\left(T\left(1+d^{\frac{1}{2}}T^{-\frac{1}{3}}\right)^{2}+T^{\frac{ 2}{3}}\text{Tr}(A^{-2})\right)\!.\]

We apply the above estimate to inequality (13). By taking the infimum over all algorithms,

\[\liminf_{T\to\infty}\mathfrak{R}(T;A)\cdot T\geq\liminf_{T\to\infty}\frac{\inf_ {\mathcal{A}}\mathbb{E}_{\bm{x}_{0}}\left[\mathbb{E}[f(\bm{x}_{T})]\right]- \mathbb{E}\left[\mathbb{1}(\bm{x}_{0}\notin\mathcal{B})\cdot\sup_{\bm{x}\in \mathcal{B}}f(\bm{x})\right]}{\mathbb{P}[\bm{x}_{0}\in\mathcal{B}]}\cdot T\]Observe that for our Gaussian prior,

\[\lim_{T\to\infty}\mathbb{P}[\bm{x}_{0}\in\mathcal{B}]=1,\] \[\lim_{T\to\infty}\mathbb{E}\left[\mathbbm{1}(\bm{x}_{0}\notin \mathcal{B})\cdot\sup_{\bm{x}\in\mathcal{B}}f(\bm{x})\right]\cdot T=0.\]

Hence, all terms above can be replace by closed-form functions. and we have

\[\liminf_{T\to\infty}\mathfrak{R}(T;A)\cdot T \geq\liminf_{T\to\infty}\inf_{\mathcal{A}}\mathbb{E}_{\bm{x}_{0 }}\left[\mathbb{E}[f(\bm{x}_{T})]\right]\cdot T\] \[\geq\liminf_{T\to\infty}\frac{1}{2}\left(\text{Tr}(A^{-\frac{1}{ 2}})\right)^{2}\Big{/}\Big{(}\Big{(}1+d^{\frac{1}{2}}T^{-\frac{1}{3}}\Big{)}^{2 }+T^{-\frac{1}{3}}\text{Tr}(A^{-2})\Big{)}\] \[=\frac{1}{2}\Big{(}\text{Tr}(A^{-\frac{1}{2}})\Big{)}^{2}.\]

## 4 Proof of Theorem 2.4

To prove the universal achievability result, we need to develop a new algorithm to learn and incorporate the needed Hessian information. Particularly, the procedure required for achieving the optimal rates is beyond simply adding an initial stage with an arbitrary Hessian estimator, for there are two challenges. First, any Hessian estimation algorithm would result in a mean squared error of \(\Omega(1/T)\) in the worst case, which translates to a cost of \(\Omega(1/T)\) in the minimax simple regret. This induced cost often introduces a multiplicative factor that is order-wise larger than the desired \(O\left((\text{Tr}(A^{-\frac{1}{2}}))^{2}\right)\). Second, to utilize any estimated Hessian, the analysis for the subsequent stages often requires the estimation error to have a light-tail distribution, which is not guaranteed for general linear estimators when the additive noise in the observation model has a heavy-tail distribution.

To overcome these challenges, we present two main algorithmic building blocks in the following subsections. The first uses \(o(T)\) samples to obtain a rough estimate of the Hessian, and achieves low-error guarantees with high probability through the introduction of a truncation method. The second estimates the global minimum with carefully designed sample points to minimize the error contributed from the Hessian estimation. We show that this sample phase can be written in the form of a two-step descent. Finally, we show the combination of two stages provides an algorithm that proves Theorem 2.4.

### Initial Hessian Estimate and Truncation Method

Consider any sufficiently large \(T\), we first use \(T_{0}=\lceil T^{0.8}\rceil\) samples to find a rough estimate of \(A\). Particularly, we rely on the following result.

**Proposition 4.1**.: _For any fixed dimension \(d\), there is an algorithm that samples on \(T_{0}\) predetermined points and returns an estimation \(\widehat{A}\), such that for any fixed \(\alpha\in(0,0.5)\), \(\beta\in(0,+\infty)\), and PSD matrix \(A\),_

\[\lim_{T_{0}\to\infty}\sup_{f\in\mathcal{F}_{A}}\mathbb{P}\left[|| \widehat{A}-A||_{\mathrm{F}}\geq T_{0}^{-\alpha}\right]\cdot T_{0}^{\beta}=0,\] (14)

_where \(||\cdot||_{\mathrm{F}}\) denotes the Frobenius norm._

Proof.: We first consider the 1D case. Let \(y_{+}\), \(y_{-}\), and \(y\) each be samples of \(f\) at \(1\), \(-1\), and \(0\), respectively. When the additive noises are subgaussian, we have that \((y_{+}+y_{-}-2y)\) is an unbiased estimator of \(A\) with an error that is subgaussian. By repeating and averaging over this process \(\lfloor T_{0}/3\rfloor\) times, the squared estimation error is reduced to \(O(1/T_{0})\), and the normalized error is subgaussian, which satisfies the statement in the proposition.

However, recall that in Section 2 we did not assume the subgaussianity of \(w_{t}\). A modification of the above estimator is required to achieve the same guanrantee in equation (14). Here we propose a truncation method, which projects each measurement \((y_{+}+y_{-}-2y)\) to a bounded interval \([-T_{0}^{0.5},T_{0}^{0.5}]\). Specifically, the returned \(\widehat{A}\) is the average of \(\lfloor T_{0}/3\rfloor\) samples of \(\max\{\min\{y_{+}+y_{-}-2y,T_{0}^{0.5}\},-T_{0}^{0.5}\}\), which provides a guaranteed superpolynomial tail bound. A more detailed discussion and analysis for the truncation method is provided in Appendix D.1.

For general \(d\), one can return an estimator that satisfies the same light-tail requirement, for example, by applying the above 1D estimator repetitively \(\text{poly}(d)\) times to obtain estimates of all entries of \(A\). Then the overall error probability is controlled by the union bound. 

### Two-Step Descent Stage

Let \(\widehat{A}\) be any estimator that satisfies the condition in Proposition 4.1 for \(\alpha=0.4\) and \(\beta=1.6\). Asymptotically, this implies the eigenvalues and eigenvectors of \(\widehat{A}\) are close to that of \(A\). We choose an eigenbasis of \(\widehat{A}\) and remove vectors with vanishingly small eigenvalues to approximate the row space of \(A\). Then, we estimate the entries of \(\bm{x}_{0}\) in these remaining directions following an energy allocation defined based on \(\widehat{A}\).

Specifically, consider any fixed realization of \(\widehat{A}\), let \(\widehat{\lambda}_{1},\widehat{\lambda}_{2},...\) be its eigenvalues in the non-increasing order, \(\widehat{\bm{e}}_{1},\widehat{\bm{e}}_{2},...\) be the corresponding eigenvectors, and \(k^{*}\) be the largest integer such that \(\widehat{\lambda}_{k^{*}}\geq T^{-0.2}\). We present the detailed steps in Algorithm 2, where \(T_{1}\) denotes the remaining available samples, i.e., \(T-T_{0}\).

``` procedureQuadraticSearch(\(\widehat{\lambda}_{1},\widehat{\lambda}_{2},...,\widehat{\lambda}_{k^{*}}\), \(\widehat{\bm{e}}_{1},...,\widehat{\bm{e}}_{k^{*}}\), \(T_{1}\)) for\(k\gets 1\) to \(k^{*}\)do  Let \(p_{k}=\frac{\widehat{\lambda}_{k}^{*}-\frac{1}{2}}{4\sum_{j=1}^{k^{*}} \widehat{\lambda}_{j}^{-\frac{1}{2}}}\), \(t_{k}=\lceil p_{k}\cdot(T_{1}-4d-1)\rceil\).  Let \(\alpha_{k}=-\frac{1}{2\lambda_{k}}\)Truncated Diff\((\widehat{\bm{e}}_{k},-\widehat{\bm{e}}_{k},t_{k})\). endfor  Let \(\widetilde{\bm{x}}=\sum_{k=1}^{k^{*}}\alpha_{k}\widehat{\bm{e}}_{k}\), \(\widehat{\bm{x}}=\widetilde{\bm{x}}\cdot\min\left\{1,1.5/||\widetilde{\bm{x} }||_{2}\right\}\qquad\triangleright\) Projecting to a \(\bm{0}\)-centered hyperball for\(k\gets 1\) to \(k^{*}\)do  Let \(\beta_{k}=-\frac{4}{\lambda_{k}}\)Truncated Diff\((\frac{\widehat{\bm{e}}_{k}+2\widehat{\bm{x}}}{4},\frac{-\widehat{\bm{e}}_{k}+2 \widehat{\bm{x}}}{4},t_{k})\). \(\triangleright\) Obtain an unbounded estimator endfor return\(\bm{x}_{T}=\text{argmin}_{\bm{x}=\sum_{k=1}^{k^{*}}\theta_{k}\widehat{\bm{e}}_{k} \in\mathcal{B}}\sum_{k=1}^{k^{*}}\widehat{\lambda}_{k}\left(\beta_{k}-\theta_ {k}\right)^{2}\qquad\qquad\qquad\triangleright\) Projection to \(\mathcal{B}\) endprocedureprocedureTruncatedDiff(\(\bm{x}_{0}\), \(\bm{x}_{1}\), \(t\)) for\(k\gets 1\) to \(t\)do  Let \(y_{+},y_{-}\) be a sample of \(f\) at \(\bm{x}_{0}\), \(\bm{x}_{1}\), respectively  Compute the projection of the difference \(y_{+}-y_{-}\) to the interval \([-t^{0.5},t^{0.5}]\), i.e., let \(z_{k}=\max\{\min\{y_{+}-y_{-},t^{0.5}\},-t^{0.5}\}\) endfor return\(\frac{1}{t}\sum_{k=1}^{t}z_{k}\) endprocedure ```

**Algorithm 2**

We use the eigenbasis of \(\widehat{A}\) and let \(\widehat{A}_{0}\) be the diagonal matrix given by \(\text{diag}(\widehat{\lambda}_{1},\widehat{\lambda}_{2},...,\widehat{\lambda }_{k^{*}},0,...,0)\). In the first for-loop, we essentially estimated \(A\bm{x}_{0}\), and then computed the first \(k^{*}\) entries of its product with the pseudo-inverse of \(\widehat{A}_{0}\) (note that the rest of the entries are all zero). Since \(\widehat{A}_{0}\) is a good estimate of \(A\) with high probability, we use it to compute the optimal energy spectrum similar to the instance-dependent case, and allocate the measurements accordingly.

Recall the proof in Section 3. By the analysis of the truncation method in Appendix D.1, the variable \(\widehat{\bm{x}}\) could have served as an estimator of \(\bm{x}_{0}\) if \(\widehat{A}_{0}=A\), where the estimation process reduces to the Hessian-dependent case. However, \(\widehat{A}_{0}\) relies only on \(O(T^{0.8})\) samples, which generally leads to an expected penalty of \(\Theta(T^{-0.8})=\omega(1/T)\) in simple regret if used in place of \(A\). We reserve a fraction of samples for a second for-loop to fine-tune the estimation in order to achieve the optimal Hessian-dependent simple regrets.

### Regret Analysis

Recall our assumption on the Hessian Estimator and \(T_{0}=O(T^{0.8})\). The probability for \(||\widehat{A}-A||_{\mathrm{F}}\geq T^{-0.32}\) is \(o(1/T)\). As our algorithm always returns \(\bm{x}_{T}\) with bounded norms, the simple regret contributed by this exceptional case is negligible. Hence, we can focus on instances where \(||\widehat{A}-A||_{\mathrm{F}}\leq T^{-0.32}\).

Under such scenario, any non-zero \(\widehat{\lambda}_{k}\) is close to a non-zero eigenvalue of \(A\). Specifically, we have \(|\widehat{\lambda}_{k}-\lambda_{k}|\leq T^{-0.32}\) for all \(k\), where each \(\lambda_{k}\) is the \(k\)th largest eigenvalue of \(A\). Recall the definition of \(\widehat{A}_{0}\). It implies that for sufficiently large \(T\), \(k^{*}\) equals the rank of \(A\), and all non-zero eigenvalues of \(\widehat{A}_{0}\) are bounded away from zero. Consequently, \(||\widehat{A}_{0}^{-1}||_{\mathrm{F}}=\|A^{-1}\|_{\mathrm{F}}(1+o(1))\), which does not scale w.r.t. \(T\), where \(M^{-1}\) denotes the pseudo-inverse for any symmetric \(M\). We also have \((\mathrm{Tr}(\widehat{A}_{0}^{-\frac{1}{2}}))^{2}=O\left((\mathrm{Tr}(A^{- \frac{1}{2}}))^{2}\right).\)

The closeness between \(\widehat{A}\) and \(A\) also implies information on the eigenvectors of \(\widehat{A}_{0}\). Recall that \(\widehat{A}_{0}\) is obtained by removing the diagonal entries of \(\widehat{A}\) (in their eigenbasis) that are below a threshold \(T^{-0.2}\). For sufficiently large \(T\), the removed entries are associated with the \(d-k^{*}\) smallest eigenvalues, which are no greater than \(T^{-0.32}\). Hence, as a rough estimate, we have \(||\widehat{A}_{0}-A||_{\mathrm{F}}=o(T^{-0.3})\).

These conditions can be used to characterize the distribution of \(\widehat{\bm{x}}\). As mentioned earlier, from the analysis of the truncation method, each \(\alpha_{k}\) concentrates around \(\widehat{\bm{e}}_{k}\widehat{A}_{0}^{-1}A\bm{x}_{0}\). Hence, \(\widetilde{\bm{x}}\) concentrates near \(\widehat{A}_{0}^{-1}A\bm{x}_{0}\), and the truncation method ensures that

\[\mathbb{E}\left[\left(\widetilde{\bm{x}}-\widehat{A}_{0}^{-1}A \bm{x}_{0}\right)^{\intercal}\widehat{A}_{0}\left(\widetilde{\bm{x}}-\widehat {A}_{0}^{-1}A\bm{x}_{0}\right)\right]=O\left((\mathrm{Tr}(\widehat{A}_{0}^{- \frac{1}{2}}))^{2}\right)/T=O\left((\mathrm{Tr}(A^{-\frac{1}{2}}))^{2}\right)/T,\] (15) \[\mathbb{P}\left[\left|\left|\widetilde{\bm{x}}-\widehat{A}_{0}^{- 1}A\bm{x}_{0}\right|\right|_{2}\geq T^{-0.4}\right]=o\left(1/T\right).\] (16)

Note that the \(L_{2}\)-norm of \(\widehat{A}_{0}^{-1}A\bm{x}_{0}\) is no greater than \(1+o(1)\) under the condition of \(||\widehat{A}_{0}-A||_{\mathrm{F}}=o(T^{-0.3})\). Formally, by triangle inequality

\[||\widehat{A}_{0}^{-1}A\bm{x}_{0}||_{2}\leq||\widehat{A}_{0}^{-1}\widehat{A} _{0}\bm{x}_{0}||_{2}+||\widehat{A}_{0}^{-1}(\widehat{A}_{0}-A)\bm{x}_{0}||_{2} \leq 1+||\widehat{A}_{0}^{-1}||_{\mathrm{F}}\cdot||\widehat{A}_{0}-A||_{ \mathrm{F}}=1+o(1).\]

We can apply inequality (16) to show that the \(L_{2}\)-norm of \(\widetilde{\bm{x}}\) is no greater than \(1+o(1)\) with \(1-o(1/T)\) probability. Under such high probability cases, the projection of \(\widetilde{\bm{x}}\) to the hyperball of radius \(1.5\) remains identical. Hence, by the PSD property of \(\widehat{A}_{0}\), which is due to the convergence of its eigenvalues, we can replace all \(\widetilde{\bm{x}}\) in inequality (15) with \(\widehat{\bm{x}}\) and obtain that

\[\mathbb{E}\left[\left(\widehat{\bm{x}}-\widehat{A}_{0}^{-1}A\bm{x}_{0}\right)^ {\intercal}\widehat{A}_{0}\left(\widehat{\bm{x}}-\widehat{A}_{0}^{-1}A\bm{x}_ {0}\right)\right]=O\left((\mathrm{Tr}(A^{-\frac{1}{2}}))^{2}\right)/T.\] (17)

The same analysis can also be performed for each \(\beta_{k}\), which concentrates near \(\widehat{\bm{e}}_{k}\widehat{A}_{0}^{-1}A(2\bm{x}_{0}-\widehat{x})\). Formally, let \(\widetilde{\bm{x}}_{T}\triangleq\sum_{k}\beta_{k}\widehat{\bm{e}}_{k}\), we have

\[\mathbb{E}\left[\left(\widetilde{\bm{x}}_{T}-\widehat{A}_{0}^{-1}A(2\bm{x}_{0}- \widehat{x})\right)^{\intercal}\widehat{A}_{0}\left(\widetilde{\bm{x}}_{T}- \widehat{A}_{0}^{-1}A(2\bm{x}_{0}-\widehat{x})\right)\right]=O\left((\mathrm{ Tr}(A^{-\frac{1}{2}}))^{2}\right)/T.\] (18)

The above results for the two descent steps can be combined, using triangle inequality and Proposition 4.2 below, to obtain the following inequality (See Appendix D for their proofs).

\[\mathbb{E}\left[\left(\widetilde{\bm{x}}_{T}-\bm{z}\right)^{\intercal}\widehat{ A}_{0}\left(\widetilde{\bm{x}}_{T}-\bm{z}\right)\right]=O\left((\mathrm{Tr}(A^{- \frac{1}{2}}))^{2}\right)/T.\] (19)

where we denote \(\bm{z}\triangleq\left(2\widehat{A}_{0}^{-1}A-\left(\widehat{A}_{0}^{-1}A \right)^{2}\right)\bm{x}_{0}\) for brevity.

**Proposition 4.2**.: _Let \(\widehat{A}_{0},Z,\bm{y}\) be variables dependent on a parameter \(T\in\mathbb{N}\), where \(\widehat{A}_{0},Z\) are PSD matrices and \(\bm{y}\) belongs to the column space of \(\widehat{A}_{0}\). If \(\limsup_{T\to\infty}||\widehat{A}_{0}^{-1}||_{\mathrm{F}}<\infty\) and \(\lim_{T\to\infty}||Z-\widehat{A}_{0}||_{\mathrm{F}}=0\), then_

\[\bm{y}^{\intercal}Z\bm{y}\leq(1+||Z-\widehat{A}_{0}||_{\mathrm{F}}||\widehat{A} _{0}^{-1}||_{\mathrm{F}})(\bm{y}^{\intercal}\widehat{A}_{0}\bm{y})=(1+o(1))(\bm {y}^{\intercal}\widehat{A}_{0}\bm{y}).\]

[MISSING_PAGE_FAIL:10]

## References

* [1] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In _Colt_, pages 28-40. Citeseer, 2010.
* [2] D. P. Bertsekas and J. N. Tsitsiklis. Introduction to probability vol. 1. 2002.
* [3] S. Bubeck, Y. T. Lee, and R. Eldan. Kernel-based methods for bandit convex optimization. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing_, pages 72-85, 2017.
* [4] D. Garber, E. Hazan, and T. Ma. Online learning of eigenvectors. In _International Conference on Machine Learning_, pages 560-568. PMLR, 2015.
* [5] A. Gopalan, O.-A. Maillard, and M. Zaki. Low-rank bandits with latent mixtures. _arXiv preprint arXiv:1609.01508_, 2016.
* [6] E. Hazan and K. Y. Levy. Bandit convex optimization: Towards tight bounds. In _MIPS_, 2014.
* [7] B. Huang, K. Huang, S. M. Kakade, J. D. Lee, Q. Lei, R. Wang, and J. Yang. Optimal gradient-based algorithms for non-concave bandit optimization. _arXiv preprint arXiv:2107.04518_, 2021.
* [8] K. G. Jamieson, R. Nowak, and B. Recht. Query complexity of derivative-free optimization. _Advances in Neural Information Processing Systems (NeurIPS)_, 2012.
* [9] N. Johnson, V. Sivakumar, and A. Banerjee. Structured stochastic linear bandits. _arXiv preprint arXiv:1606.05693_, 2016.
* [10] K.-S. Jun, R. Willett, S. Wright, and R. Nowak. Bilinear bandits with low-rank structure. In _International Conference on Machine Learning_, pages 3163-3172. PMLR, 2019.
* [11] S. Katariya, B. Kveton, C. Szepesvari, C. Vernade, and Z. Wen. Stochastic rank-1 bandits. In _Artificial Intelligence and Statistics_, pages 392-401. PMLR, 2017.
* [12] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani, and P. Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2022.
* [13] R. F. Kizilecec, J. Reich, M. Yeomans, C. Dann, E. Brunskill, G. Lopez, S. Turkay, J. J. Williams, and D. Tingley. Scaling up behavioral science interventions in online education. _Proceedings of the National Academy of Sciences_, 117(26):14900-14905, 2020.
* [14] J. Kober, A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. _Springer Tracts in Advanced Robotics_, 97:9-67, 2014.
* [15] W. Kotlowski and G. Neu. Bandit principal component analysis. In _Conference On Learning Theory_, pages 1994-2024. PMLR, 2019.
* [16] S. Lale, K. Azizzadenesheli, A. Anandkumar, and B. Hassibi. Stochastic linear bandits with hidden low rank structure. _arXiv preprint arXiv:1901.09490_, 2019.
* [17] J. Larson, M. Menickelly, and S. M. Wild. Derivative-free optimization methods. _Acta Numerica_, 28:287-404, 2019.
* [18] T. Lattimore and B. Hao. Bandit phase retrieval. _arXiv preprint arXiv:2106.01660_, 2021.
* [19] Y. Lu, A. Meisami, and A. Tewari. Low-rank generalized linear bandit problems. In _International Conference on Artificial Intelligence and Statistics_, pages 460-468. PMLR, 2021.
* [20] E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. _Advances in neural information processing systems_, 24, 2011.
* [21] O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In _Conference on Learning Theory_, pages 3-24. PMLR, 2013.
* [22] S. Yildirim and S. B. Stene. _A survey on the need and use of AI in game agents_. InTech, 2010.
* [23] C. Yu, J. Liu, and S. Nemati. Reinforcement learning in healthcare: A survey. In _arXiv preprint arXiv:1908.08796_, 2020.
* [24] X. Yu, I. King, M. R. Lyu, and T. Yang. A generic approach for accelerating stochastic zeroth-order convex optimization. _IJCAI_, 2018.
* [25] L. Zhang, T. Yang, R. Jin, and Z.-H. Zhou. Online bandit learning for a special class of non-convex losses. In _Twenty-Ninth AAAI Conference on Artificial Intelligence_, 2015.

* [26] P. Zhao and L. Lai. Optimal stochastic nonconvex optimization with bandit feedback. _arXiv preprint arXiv:2103.16082_, 2021.

Projection Lemma

**Proposition A.1**.: _For any PSD matrix \(A\) with dimension \(d\), any closed convex set \(\mathcal{B}\) in the Euclidian space \(\mathbb{R}^{d}\), and \(\widehat{\bm{x}}\in\mathbb{R}^{d}\), let_

\[\bm{x}^{*}=\operatorname*{argmin}_{\bm{x}\in\mathcal{B}}g(\widehat{\bm{x}},\bm{ x})\]

_where_

\[g(\bm{u},\bm{v})\triangleq(\bm{u}-\bm{v})^{\intercal}A(\bm{u}-\bm{v}),\]

_then_

\[g(\bm{x}^{*},\bm{x}_{0})\leq g(\widehat{\bm{x}},\bm{x}_{0}) \forall\bm{x}_{0}\in\mathcal{B}.\]

_More generally,_

\[g(\bm{x}^{*},\bm{z}_{0})\leq g(\widehat{\bm{x}},\bm{z}_{0})+ \min_{\bm{x}\in\mathcal{B}}g(\bm{z}_{0},\bm{x}) \forall\bm{z}_{0}\in\mathbb{R}^{d}.\]

Proof.: We first provide a proof for \(\bm{x}_{0}\in\mathcal{B}\). For any \(\alpha\in[0,1]\), let

\[\bm{x}_{\alpha}\triangleq\alpha\bm{x}^{*}+(1-\alpha)\bm{x}_{0}.\]

By convexity, we have \(\bm{x}_{\alpha}\in\mathcal{B}\) for any \(\alpha\). Note that \(g(\widehat{\bm{x}},\bm{x}_{\alpha})\) is differentiable. By the definition of \(x^{*}\), we have

\[(\bm{x}^{*}-\widehat{\bm{x}})^{\intercal}A(\bm{x}^{*}-\bm{x}_{0}) =\frac{1}{2}\frac{\partial}{\partial\alpha}g(\widehat{\bm{x}},\bm{x}_{ \alpha})\Big{|}_{\alpha=1}\leq 0.\]

Therefore,

\[g(\bm{x}^{*},\bm{x}_{0})=g(\widehat{\bm{x}},\bm{x}_{0})+2(\bm{x} ^{*}-\widehat{\bm{x}})^{\intercal}A(\bm{x}^{*}-\bm{x}_{0})-g(\bm{x}^{*}, \widehat{\bm{x}})\leq g(\widehat{\bm{x}},\bm{x}_{0}),\]

where the last inequality uses the PSD property of \(A\).

Now we consider the more general case and let \(\bm{x}\) be any vector in \(\mathcal{B}\). Following the same steps in the earlier case, we have

\[(\bm{x}^{*}-\widehat{\bm{x}})^{\intercal}A(\bm{x}^{*}-\bm{x}) \leq 0.\]

Hence,

\[g(\bm{x}^{*},\bm{z}_{0})-g(\widehat{\bm{x}},\bm{z}_{0}) =2(\bm{x}^{*}-\widehat{\bm{x}})^{\intercal}A(\bm{x}^{*}-\bm{z}_{0 })-g(\bm{x}^{*},\widehat{\bm{x}})\] \[\leq 2(\bm{x}^{*}-\widehat{\bm{x}})^{\intercal}A(\bm{x}-\bm{z}_{0 })-g(\bm{x}^{*},\widehat{\bm{x}})\] \[=g(\bm{z}_{0},\bm{x})-(\bm{x}-\bm{z}_{0}-\bm{x}^{*}+\widehat{\bm {x}})^{\intercal}A\left(\bm{x}-\bm{z}_{0}-\bm{x}^{*}+\widehat{\bm{x}}\right)\] \[\leq g(\bm{z}_{0},\bm{x}).\]

Note that the above inequality holds for any \(\bm{x}\in\mathcal{B}\). The proposition is proved by taking the minimum over \(\bm{x}\).

## Appendix B Proof of Proposition 3.3

Proof.: We first prove for the case where \(\bm{Z}\) is deterministic. Let \(\mu_{\bm{Z}}\) denote the conditional expectation of \(\theta\). By Cauchy's inequality,

\[\mathbb{E}[(\theta-\mu_{\bm{Z}})^{2}|\bm{Z}]\cdot\mathbb{E}\left[ \left(\frac{\partial}{\partial\theta}\ln f_{\bm{Z}}(\theta)\right)^{2}\left| \bm{Z}\right]\geq\mathbb{E}\left[\left|(\theta-\mu_{\bm{Z}})\cdot\frac{ \partial}{\partial\theta}\ln f_{\bm{Z}}(\theta)\right|\left|\bm{Z}\right|^{2}.\right.\] (22)The quantity on the RHS above can be bounded as follows.

\[\mathbb{E}\left[\left|(\theta-\mu_{\bm{Z}})\cdot\frac{\partial}{ \partial\theta}\ln f_{\bm{Z}}(\theta)\right|\left|\bm{Z}\right] =\int\left|(\theta-\mu_{\bm{Z}})\cdot\frac{\partial}{\partial \theta}\ln f_{\bm{Z}}(\theta)\right|f_{\bm{Z}}(\theta)d\theta\] \[=\int\left|(\theta-\mu_{\bm{Z}})\cdot\frac{\partial}{\partial \theta}f_{\bm{Z}}(\theta)\right|d\theta\] \[\geq\limsup_{T\rightarrow+\infty}\left|\int_{-T}^{T}(\theta-\mu_ {\bm{Z}})\cdot\frac{\partial}{\partial\theta}f_{\bm{Z}}(\theta)d\theta\right|\] \[=\limsup_{T\rightarrow+\infty}\left|\left((\theta-\mu_{\bm{Z}}) f_{\bm{Z}}(\theta)\right|_{\theta=-T}^{\theta=T}\right)-\mathbb{P}[\theta \in[-T,T]|\bm{Z}]\right|\] \[\geq 1,\]

where the last inequality uses the integrability of \(f_{\bm{Z}}\), which implies

\[\liminf_{T\rightarrow+\infty}(\theta-\mu_{\bm{Z}})f_{\bm{Z}}(\theta)\big{|}_ {\theta=-T}^{\theta=T}\leq 0.\]

Then we evaluate the second factor on the LHS of inequality (22). Recall that \(\frac{\partial^{2}}{\partial\theta^{2}}\ln f_{\bm{Z}}(\theta)\) is integrable, the following limit exists.

\[\mathbb{E}\left[\left.\frac{\partial^{2}}{\partial\theta^{2}}\ln f_{\bm{Z}}( \theta)\right|\bm{Z}\right]=\lim_{T\rightarrow+\infty}\int_{-T}^{T}f_{\bm{Z}} (\theta)\frac{\partial^{2}}{\partial\theta^{2}}\ln f_{\bm{Z}}(\theta)d\theta.\]

Then by positivity, we also have

\[\mathbb{E}\left[\left(\frac{\partial}{\partial\theta}\ln f_{\bm{Z}}(\theta) \right)^{2}\left|\bm{Z}\right]=\lim_{T\rightarrow+\infty}\int_{-T}^{T}f_{\bm {Z}}(\theta)\left(\frac{\partial}{\partial\theta}\ln f_{\bm{Z}}(\theta) \right)^{2}d\theta.\]

If we focus the non-trivial case where the first limit is not \(-\infty\), the above two equation implies the existence of the following limit.

\[\mathbb{E}\left[\left.\frac{\partial^{2}}{\partial\theta^{2}}\ln f _{\bm{Z}}(\theta)\right|\bm{Z}\right]+\mathbb{E}\left[\left(\frac{\partial}{ \partial\theta}\ln f_{\bm{Z}}(\theta)\right)^{2}\left|\bm{Z}\right]\right.\] \[=\lim_{T\rightarrow+\infty}\int_{-T}^{T}f_{\bm{Z}}(\theta)\left( \frac{\partial^{2}}{\partial\theta^{2}}\ln f_{\bm{Z}}(\theta)+\left(\frac{ \partial}{\partial\theta}\ln f_{\bm{Z}}(\theta)\right)^{2}\right)d\theta\] \[=\lim_{T\rightarrow+\infty}f_{\bm{Z}}(\theta)\frac{\partial}{ \partial\theta}\ln f_{\bm{Z}}(\theta)\left|_{\theta=-T}^{\theta=T}\right.\] \[=\lim_{T\rightarrow+\infty}\frac{\partial}{\partial\theta}f_{\bm {Z}}(\theta)\left|_{\theta=-T}^{\theta=T}.\]

The result of the above equation has to be zero, because the limit points of \(\frac{\partial}{\partial\theta}f_{\bm{Z}}(\theta)\) must contain zero on both ends of the real line, which is implied by the integrability of \(f_{\bm{Z}}\). Consequently, we have

\[\mathbb{E}\left[\left(\frac{\partial}{\partial\theta}\ln f_{\bm{Z}}(\theta) \right)^{2}\left|\bm{Z}\right]=\mathbb{E}\left[-\frac{\partial^{2}}{ \partial\theta^{2}}\ln f_{\bm{Z}}(\theta)\middle|\bm{Z}\right].\] (23)

Then, the special case of Proposition 3.3 with fixed \(\bm{Z}\) is implied by inequality (22).

When \(\bm{Z}\) is variable, we simply have

\[\mathbb{E}\left[\text{Var}[\theta|\bm{Z}]\right] \geq\mathbb{E}\left[1/\mathbb{E}\left[\left(\frac{\partial}{ \partial\theta}\ln f_{\bm{Z}}(\theta)\right)^{2}\left|\bm{Z}\right]\right]\] \[\geq\frac{1}{\mathbb{E}\left[\mathbb{E}\left[\left(\frac{\partial }{\partial\theta}\ln f_{\bm{Z}}(\theta)\right)^{2}\left|\bm{Z}\right]\right]}.\]

Then the proposition is implied by equation (23).

Proof of Theorem 2.2

We first investigate the lower bounds. Observe that the proof provided in Section 3.1 only fails when the constructed hard instances have \(\|\bm{x}_{0}\|_{2}>1\). Hence, we have already covered the \(T\geq\left(\sum_{k=1}^{d}\lambda_{k}^{-\frac{1}{2}}\right)\left(\sum_{k=1}^{d} \lambda_{k}^{-\frac{3}{2}}\right)\) case, i.e., when \(k^{*}=\text{dim}A=d\). It remains to consider the other scenarios, where \(k^{*}<d\) is satisfied.

By the assumption that \(T\geq\left(\sum_{k=1}^{k^{*}}\lambda_{k}^{-\frac{1}{2}}\right)\left(\sum_{k=1} ^{k^{*}}\lambda_{k}^{-\frac{3}{2}}\right)\), one can instead set the entries of \(\bm{x}_{0}\) in the earlier proof with indices greater than \(k^{*}\) to be zero, so that \(||\bm{x}_{0}||_{2}\leq 1\) is satisfied. Formally, let the hard-instance functions be constructed by the following set.

\[\bm{x}_{0}\in\mathcal{X}_{\text{H}}\triangleq\left\{(x_{1},x_{2},...,x_{k^{*} },0,...,0)\;\middle|\;x_{k}=\pm\sqrt{\frac{\lambda_{k}^{-\frac{3}{2}}\left( \sum_{j}\lambda_{j}^{-\frac{1}{2}}\right)}{2T}},\forall k\in[k^{*}]\right\}.\]

Then by the identical proof steps, we have \(\mathfrak{R}(T;A)=\Omega\left(\left(\sum_{k=1}^{k^{*}}\lambda_{k}^{-\frac{1}{2 }}\right)^{2}\middle/T\right).\)

Next, we show that \(\mathfrak{R}(T;A)=\Omega\left(\lambda_{k^{*}+1}\right)\). We assume the non-trivial case where \(\lambda_{k^{*}+1}\neq 0\). Note that \(\mathfrak{R}(T;A)\) is non-increasing w.r.t. \(T\). We can lower bound \(\mathfrak{R}(T;A)\) through the above steps but by replacing \(T\) with any larger quantity. Specifically, recall that \(k^{*}\) is largest integer satisfying \(T\geq\left(\sum_{k=1}^{k^{*}}\lambda_{k}^{-\frac{1}{2}}\right)\left(\sum_{k=1} ^{k^{*}}\lambda_{k}^{-\frac{3}{2}}\right)\), which implies \(T\leq\left(\sum_{k=1}^{k^{*}+1}\lambda_{k}^{-\frac{1}{2}}\right)\left(\sum_{k =1}^{k^{*}+1}\lambda_{k}^{-\frac{3}{2}}\right)\). We have,

\[\mathfrak{R}(T;A) \geq\mathfrak{R}\left(\left(\sum_{k=1}^{k^{*}+1}\lambda_{k}^{- \frac{1}{2}}\right)\left(\sum_{k=1}^{k^{*}+1}\lambda_{k}^{-\frac{3}{2}}\right); A\right).\]

Notice that this change of sampling time allows us to apply the earlier lower bound with \(k^{*}\) incremented by \(1\).

\[\mathfrak{R}(T;A) \geq\Omega\left(\frac{\left(\sum_{k=1}^{k^{*}+1}\lambda_{k}^{- \frac{1}{2}}\right)^{2}}{\left(\sum_{k=1}^{k^{*}+1}\lambda_{k}^{-\frac{1}{2}} \right)\left(\sum_{k=1}^{k^{*}+1}\lambda_{k}^{-\frac{3}{2}}\right)}\right)\] \[=\Omega\left(\frac{\sum_{k=1}^{k^{*}+1}\lambda_{k}^{-\frac{1}{2} }}{\sum_{k=1}^{k^{*}+1}\lambda_{k}^{-\frac{3}{2}}}\right)=\Omega(\lambda_{k^{* }+1}).\]

To conclude,

\[\mathfrak{R}(T;A)=\Omega\left(\max\left\{\frac{\left(\sum_{k=1}^{k^{*}} \lambda_{k}^{-\frac{1}{2}}\right)^{2}}{T},\lambda_{k^{*}+1}\right\}\right)= \Omega\left(\frac{\left(\sum_{k=1}^{k^{*}}\lambda^{-\frac{1}{2}}\right)^{2}}{ T}+\lambda_{k^{*}+1}\right),\]

which completes the proof of the lower bounds.

The needed upper bounds can be obtained by only estimating the first \(k^{*}\) entries of \(\bm{x}_{0}\).

**Remark C.1**.: _The requirement of \(T>3\,\text{dim}A\) in the Theorem statement is simply due to the integer constraints for the achievability bounds. Indeed, when \(\lambda_{\text{dim}A}\) is large, it requires at least \(\Omega(\text{dim}A)\) samples to achieve \(O(1)\) expected simple regret._

## Appendix D Proof Details for Theorem 2.4

### Truncation Method and Its Applications

The truncation method is based on the following facts.

**Proposition D.1**.: _For any sequence of independent random variables \(X_{1},X_{2},...,X_{n}\) and any fixed parameter \(m\) satisfying \(m>\max_{k}|\mathbb{E}[X_{k}]|\). Let \(Z_{k}=\max\{\min\{X_{k},m\},-m\}\) for any \(k\in[n]\), we have_

\[|\mathbb{E}[Z_{k}]-\mathbb{E}[X_{k}]|\leq\frac{1}{4}\cdot\frac{ \text{Var}[X_{k}]}{m-|\mathbb{E}[X_{k}]|},\] (24) \[\text{Var}[Z_{k}]\leq\mathbb{E}\left[\left(Z_{k}-\mathbb{E}[X_{k} ]\right)^{2}\right]\leq\text{Var}[X_{k}].\] (25)

_Moreover, for any \(z>0\), we have_

\[\mathbb{P}\left[\left|\sum_{k}Z_{k}-\sum_{k}\mathbb{E}[X_{k}] \right|\geq z\right]\leq 2\exp\left(\sum_{k}\frac{\text{Var}[X_{k}]}{m(m-| \mathbb{E}[X_{k}]|)}-\frac{z}{m}\right).\] (26)

Proof.: The first inequality is proved by expressing the LHS with piecewise linear functions. Note that by the definition of \(Z_{k}\), we have

\[|\mathbb{E}[Z_{k}]-\mathbb{E}[X_{k}]| =|\mathbb{E}[\max\{-m-X_{k},0\}]-\mathbb{E}[\max\{X_{k}-m,0\}]|\] \[\leq|\mathbb{E}[\max\{-m-X_{k},0\}]|+|\mathbb{E}[\max\{X_{k}-m,0 \}]|\] \[=\mathbb{E}[\max\{|X_{k}|-m,0\}].\]

We apply the following inequalities, which holds for any \(m\geq|\mathbb{E}[X_{k}]|\).

\[|X_{k}|-m\leq|X_{k}-\mathbb{E}[X_{k}]|-m+\mathbb{E}[X_{k}]\leq \frac{1}{4}\cdot\frac{|X_{k}-\mathbb{E}[X_{k}]|^{2}}{m-\mathbb{E}[X_{k}]}.\]

Therefore,

\[|\mathbb{E}[Z_{k}]-\mathbb{E}[X_{k}]| \leq\mathbb{E}\left[\frac{1}{4}\cdot\frac{|X_{k}-\mathbb{E}[X_{k }]|^{2}}{m-\mathbb{E}[X_{k}]}\right]\] \[=\frac{1}{4}\cdot\frac{\text{Var}[X_{k}]}{m-|\mathbb{E}[X_{k}]|}.\]

The second inequality is due to the following elementary facts,

\[\mathbb{E}[(Z_{k}-\mathbb{E}[X_{k}])^{2}]\leq \mathbb{E}[(X_{k}-\mathbb{E}[X_{k}])^{2}]=\text{Var}[X_{k}],\]

where the inequality step is implied by the definition of \(Z_{k}\) and the condition \(m>\max_{k}|\mathbb{E}[X_{k}]|\).

To prove the third inequality, we first investigate the following upper bound, which is due to Markov's inequality.

\[\mathbb{P}\left[\sum_{k}Z_{k}-\sum_{k}\mathbb{E}[X_{k}]\geq z \right] \leq\frac{\mathbb{E}[e^{\frac{1}{m}(\sum_{k}Z_{k}-\sum_{k}\mathbb{ E}[X_{k}])}]}{e^{\frac{1}{m}}}\] \[= \frac{\prod_{k}\mathbb{E}[e^{\frac{1}{m}(Z_{k}-\mathbb{E}[X_{k}]) }]}{e^{\frac{1}{m}}}\] (27)

The equality step above is by the fact that \(Z_{k}\)'s are jointly independent. For each \(k\), using the fact that \(Z_{k}\) is bounded, particularly, \(Z_{k}-\mathbb{E}[X_{k}]\leq m+|\mathbb{E}[X_{k}]|\), we have the following inequality

\[e^{\frac{1}{m}(Z_{k}-\mathbb{E}[X_{k}])}-1-\frac{1}{m}(Z_{k}- \mathbb{E}[X_{k}])\leq(Z_{k}-\mathbb{E}[X_{k}])^{2}\cdot\frac{e^{\frac{1}{m}( m+|\mathbb{E}[X_{k}]|)}-1-\frac{1}{m}(m+|\mathbb{E}[X_{k}]|)}{(m+|\mathbb{E}[X_{k}]|) ^{2}}.\]

For brevity, let \(\theta\triangleq\frac{|\mathbb{E}[X_{k}]|}{m}\). We combine the above bound with inequality (24) and (25) to obtain that

\[\mathbb{E}[e^{\frac{1}{m}(Z_{k}-\mathbb{E}[X_{k}])}] =1+\mathbb{E}\left[\frac{1}{m}(Z_{k}-\mathbb{E}[X_{k}])\right]+ \mathbb{E}\left[e^{\frac{1}{m}(Z_{k}-\mathbb{E}[X_{k}])}-1-\frac{1}{m}(Z_{k}- \mathbb{E}[X_{k}])\right]\] \[\leq 1+\frac{\text{Var}[X_{k}]}{m(m-|\mathbb{E}[X_{k}]|)}\cdot \left(\frac{1}{4}+(1-\theta)\cdot\frac{e^{1+\theta}-2-\theta}{(1+\theta)^{2}} \right).\] (28)Recall that \(\theta<1\) as assumed in the proposion. From elementary calculus, we have

\[\mathbb{E}[e^{\frac{1}{m}(Z_{k}-\mathbb{E}[X_{k}])}] \leq 1+\frac{\text{Var}[X_{k}]}{m(m-|\mathbb{E}[X_{k}]|)}\] \[\leq\exp\left(\frac{\text{Var}[X_{k}]}{m(m-|\mathbb{E}[X_{k}]|)} \right).\]

Therefore, recall inequality (27), we have

\[\mathbb{P}\left[\sum_{k}Z_{k}-\sum_{k}\mathbb{E}[X_{k}]\geq z\right]\leq\exp \left(\sum_{k}\frac{\text{Var}[X_{k}]}{m(m-|\mathbb{E}[X_{k}]|)}-\frac{z}{m} \right).\]

By symmetry, one can also prove the following bound through the same steps.

\[\mathbb{P}\left[\sum_{k}Z_{k}-\sum_{k}\mathbb{E}[X_{k}]\leq-z\right]\leq\exp \left(\sum_{k}\frac{\text{Var}[X_{k}]}{m(m-|\mathbb{E}[X_{k}]|)}-\frac{z}{m} \right).\]

Hence, the needed inequality is obtained by adding the two inequalities above. 

Now equation (14) for the 1D case is immediately implied by Proposition D.1. Recall the construction of \(\widehat{A}\) in the proof, for any sufficiently large \(T_{0}\), we have

\[\mathbb{P}\left[\left|\widehat{A}-A\right|\geq T_{0}^{-\alpha}\right]\leq 2 \exp\left(3-\frac{T_{0}^{0.5-\alpha}}{3}\right)=o\left(\frac{1}{T_{0}^{\beta}} \right).\]

**Remark D.2**.: _Instead of projecting to a bounded interval, the same achievability result can be obtained if the we average over any functions that map the samples to \([-T_{0}^{0.5},T_{0}^{0.5}]\) while imposing an additional error of \(o(T^{-\alpha})\) everywhere. This includes \(\Theta(\ln T)\)-bit uniform quantizers, which naturally appear in digital systems, over which exact computation can be performed to eliminate numerical errors. We present this simple generalization in the following corollary._

**Corollary D.3**.: _Consider the setting in Proposition D.1. Let \(Y_{1},...,Y_{n}\) be variables that satisfy \(|Y_{k}-Z_{k}|\leq b\) for all \(k\) with probability \(1\). We have_

\[\mathbb{P}\left[\left|\sum_{k}Y_{k}-\sum_{k}\mathbb{E}[X_{k}]\right|\geq z \right]\leq 2\exp\left(\sum_{k}\frac{\text{Var}[X_{k}]}{m(m-|\mathbb{E}[X_{k}]|) }-\frac{z-bn}{m}\right).\]

### Proof of Proposition 4.2

Proof.: \[\bm{y}^{\intercal}Z\bm{y}-\bm{y}^{\intercal}\widehat{A}_{0}\bm{y}=\bm{y}^{ \intercal}(Z-\widehat{A}_{0})\bm{y}\leq||Z-\widehat{A}_{0}||_{\text{F}}||\bm {y}||_{2}^{2}\leq||Z-\widehat{A}_{0}||_{\text{F}}||\widehat{A}_{0}^{-1}||_{ \text{F}}(\bm{y}^{\intercal}\widehat{A}_{0}\bm{y}).\]

### Proof of inequality (19)

We apply Proposition 4.2 to inequality (17) and let \(Z=A\widehat{A}_{0}^{-1}A\). Note that

\[||Z-\widehat{A}_{0}||_{\text{F}}\leq 2||A-\widehat{A}_{0}||_{\text{F}}+||(A- \widehat{A}_{0})\widehat{A}_{0}^{-1}(A-\widehat{A}_{0})||_{\text{F}}=o(1),\]

which satisfies the condition of Proposition 4.2. Using the fact that \(\widehat{A}_{0}^{-1}\widehat{A}_{0}\widehat{A}_{0}^{-1}=\widehat{A}_{0}^{-1}\), we have

\[\mathbb{E}\left[\left(\widehat{A}_{0}^{-1}A\left(\widehat{\bm{x} }-\widehat{A}_{0}^{-1}A\bm{x}_{0}\right)\right)^{\intercal}\widehat{A}_{0} \left(\widehat{A}_{0}^{-1}A\left(\widehat{\bm{x}}-\widehat{A}_{0}^{-1}A\bm{x} _{0}\right)\right)\right]\] \[=\mathbb{E}\left[\left(\widehat{\bm{x}}-\widehat{A}_{0}^{-1}A\bm {x}_{0}\right)^{\intercal}Z\left(\widehat{\bm{x}}-\widehat{A}_{0}^{-1}A\bm{x} _{0}\right)\right]=O\left(\left(\text{Tr}(A^{-\frac{1}{2}})\right)^{2}\right)/T.\] (29)

Then by the triangle inequality for the PSD matrix \(\widehat{A}_{0}\), the combination of the above inequality and inequality (18) gives

\[\mathbb{E}\left[\left(\widetilde{\bm{x}}_{T}-\bm{z}\right)^{\intercal}\widehat {A}_{0}\left(\widetilde{\bm{x}}_{T}-\bm{z}\right)\right]=O\left((\text{Tr}(A^{ -\frac{1}{2}}))^{2}\right)/T.\]

### Proof of Proposition 4.3

Proof.: When \(A_{1}\) and \(A\) has the same rank, the map \(P_{1}\) is invertible over the column space of \(A\). Under such condition, there exists a matrix \(X\) such that \(A=XP_{1}A\). Note that \(A_{1}A_{1}^{-1}=P_{1}\). We have \(XP_{1}=XP_{1}A_{1}A_{1}^{-1}=AA_{1}^{-1}\). Therefore, the needed \(A=AA_{1}^{-1}A\) is obtained by multiplying \(A\) on the right-hand sides in the above identity.