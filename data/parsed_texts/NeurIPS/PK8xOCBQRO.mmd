# Transfer Learning for Latent Variable Network Models

 Akhil Jalan

Department of Computer Science

UT Austin

akhiljalan@utexas.edu

&Arya Mazumdar

Halicioglu Data Science Institute & Dept of CSE

UC San Diego

arya@ucsd.edu

Soumendu Sundar Mukherjee

Statistics and Mathematics Unit (SMU)

Indian Statistical Institute, Kolkata

ssmukherjee@isical.ac.in

&Purnamrita Sarkar

Department of Statistics and Data Sciences

UT Austin

purna.sarkar@austin.utexas.edu

###### Abstract

We study transfer learning for estimation in latent variable network models. In our setting, the conditional edge probability matrices given the latent variables are represented by \(P\) for the source and \(Q\) for the target. We wish to estimate \(Q\) given two kinds of data: (1) edge data from a subgraph induced by an \(o(1)\) fraction of the nodes of \(Q\), and (2) edge data from all of \(P\). If the source \(P\) has no relation to the target \(Q\), the estimation error must be \(\Omega(1)\). However, we show that if the latent variables are shared, then vanishing error is possible. We give an efficient algorithm that utilizes the ordering of a suitably defined graph distance. Our algorithm achieves \(o(1)\) error and does not assume a parametric form on the source or target networks. Next, for the specific case of Stochastic Block Models we prove a minimax lower bound and show that a simple algorithm achieves this rate. Finally, we empirically demonstrate our algorithm's use on real-world and simulated network estimation problems.

## 1 Introduction

Within machine learning and statistics, the paradigm of _transfer learning_ describes a setup where data from a source distribution \(P\) is exploited to improve estimation of a target distribution \(Q\) for which a small amount of data is available. Transfer learning is quite well-studied in learning theory, starting with works such as Ben-David et al. (2006); Cortes et al. (2008); Crammer et al. (2008), and at the same time has found applications in areas such as computer vision (Tzeng et al., 2017) and speech recognition (Huang et al., 2013). A fairly large body of work in transfer learning considers different types of relations that may exist between \(P\) and \(Q\), for example, Mansour et al. (2009); Hanneke and Kpotufe (2019, 2022), with emphasis on model selection, multitask learning and domain adaptation. On the other hand, optimal nonparametric rates for transfer learning have very recently been studied, both for regression and classification problems (Cai and Wei, 2021; Cai and Pu, 2024).

In this paper, we study transfer learning in the context of _random network/graph models_. In our setting, we observe Bernoulli samples from the full \(n\times n\) edge probability matrix for the source \(P\) and only a \(n_{Q}\times n_{Q}\) submatrix of \(Q\) for \(n_{Q}\ll n\). We would like to estimate the full \(n\times n\) probability matrix \(Q\), using the full source data and limited target data, i.e., we are interested in the task of estimating \(Q\) in the partially observed target network, utilizing information from the fully observed source network. This is a natural extension of the transfer learning problem in classification/regressionto a network context. However, it is to be noted that network transfer is a genuinely different problem owing to the presence of edge correlations.

While transfer learning in graphs seems to be a fundamental enough problem to warrant attention by itself, we are also motivated by potential applications. For example, metabolic networks model the chemical interactions related to the release and utilization of energy within an organism (Christensen and Nielsen, 2000). Existing algorithms for metabolic network estimation (Sen et al., 2018; Baranwal et al., 2020) and biological network estimation more broadly (Fan et al., 2019; Li et al., 2022) typically assume that some edges are observed for every node in the target network. One exception is Kshirsagar et al. (2013), who leverage side information for host-pathogen protein interaction networks. For the case of metabolic networks, determining interactions _in vivo_1 requires metabolite balancing and labeling experiments, so only the edges whose endpoints are _both_ incident to the experimentally chosen metabolites are observed (Christensen and Nielsen, 2000). For a non-model organism, the experimentally tested metabolites may be a small fraction of all metabolites believed to affect metabolism. However, data for a larger set of metabolites might be available for a model organism.

Footnote 1: In the organism, as opposed to _in vitro_ (in the lab).

To study transfer learning on networks, one needs to fix a general enough class of networks that is appropriate for the applications (such as the biological networks mentioned above) and also suitable to capture the transfer phenomenon. The latent variable models defined below appear to be a natural candidate for that.

**Latent Variable Models.** Latent variable network models consist of a large class of models whose edge probabilities are governed by the latent positions of nodes. This includes latent distance models, stochastic block models, random dot product graphs and mixed membership block models (Hoff et al., 2002; Hoff, 2007; Handcock et al., 2007; Holland et al., 1983; Rubin-Delanchy et al., 2022; Airoldi et al., 2008). They can also be unified under graph limits or graphons (Lovasz, 2012; Bickel and Chen, 2009), which provide a natural representation of vertex exchangeable graphs (Aldous, 1981; Hoover, 1979). In addition to their theoretical breadth and usefulness, latent variable models are relevant and applicable to real-world settings such as neuroscience Ren et al. (2023), ecology Trifonova et al. (2015), international relations Cao and Ward (2014), political psychology Barbera et al. (2015), and education research Sweet et al. (2013).

For unseen latent variables \(\bm{x}_{1},\ldots,\bm{x}_{n}\in\mathcal{X}\subset\mathbb{R}^{d}\) and unknown function \(f_{Q}:\mathcal{X}\times\mathcal{X}\to[0,1]\) where \(\mathcal{X}\) is a compact set and \(d\) an arbitrary fixed dimension, the edge probabilities are:

\[Q_{ij}=f_{Q}(\bm{x}_{i},\bm{x}_{j}).\] (1)

Typically, in network estimation, one observes adjacency matrix \(\{A_{ij}\}\) distributed as \(\{\text{Bernoulli}(Q_{ij})\}\), and either has to learn \(\bm{x}_{i}\) or directly estimate \(f_{Q}\). There has been much work in the statistics community on estimating \(\bm{x}_{i}\) for specific models (usually up to rotation). For stochastic block models, see the excellent survey in Abbe (2017).

Estimating \(f_{Q}\) can be done with some additional assumptions (Chatterjee, 2015). When \(f_{Q}\) has appropriate smoothness properties, one can estimate it by a histogram approximation (Olhede and Wolfe, 2014; Chan and Airoldi, 2014). This setting has also been compared to nonparametric regression with an unknown design (Gao et al., 2015). Methods for network estimation include Universal Singular Value Thresholding (Chatterjee, 2015; Xu, 2018), combinatorial optimization (Gao et al., 2015; Klopp et al., 2017), and neighborhood smoothing (Zhang et al., 2017; Mukherjee and Chakrabarti, 2019).

**Transfer Learning on Networks.** We wish to estimate the target network \(Q\). However, we only observe \(f_{Q}\) on \(\binom{n_{Q}}{2}\) pairs of nodes, for a uniformly random subset of variables \(S\subset\{1,2,\ldots,n\}\). We assume \(S\) is vanishingly small, so \(n_{Q}:=|S|=o(n)\).

Absent additional information, we cannot hope to achieve \(o(1)\) mean-squared error. To see this, suppose \(f_{Q}\) is a stochastic block model with 2 communities of equal size. For a node \(i\not\in S\), no edges incident to \(i\) are observed, so its community cannot be learned. Since \(n_{Q}\ll n\), we will attain \(\Omega(1)\) error overall. To attain error \(o(1)\), we hope to leverage transfer learning from a source \(P\) if available. In fact, we give an efficient algorithm to achieve \(o(1)\) error, formally stated in Section 2.

**Theorem 1.1** (Theorem 2.3, Informal).: _There exists an efficient algorithm such that, if given source data \(A_{P}\in\{0,1\}^{n\times n}\) and target data \(A_{Q}\in\{0,1\}^{n_{Q}\times n_{Q}}\) coming from an appropriate pair \((f_{P},f_{Q})\)of latent variable models, outputs \(\widehat{Q}\in\mathbb{R}^{n\times n}\) such that_

\[\mathbb{P}\left[\frac{1}{n^{2}}\|Q-\widehat{Q}\|_{F}^{2}\leq o(1)\right]\geq 1- o(1).\]

There must be a relationship between P and Q for them to be an _appropriate_ pair for transfer learning. We formalize this relationship below.

**Relationship Between Source and Target.** It is natural to consider pairs \((f_{P},f_{Q})\) such that for all \(\bm{x},\bm{y}\in\mathcal{X}\), the difference \((f_{P}(\bm{x},\bm{y})-f_{Q}(\bm{x},\bm{y}))\) is small. For example, Cai and Pu (2024) study transfer learning for nonparametric regression when \(f_{P}-f_{Q}\) is close to a polynomial in \(\bm{x},\bm{y}\). But, requiring \(f_{P}-f_{Q}\) to be pointwise small does not capture a broad class of pairs in the network setting. For example, if \(f_{P}=\alpha f_{Q}\). Then \(f_{P}-f_{Q}=(\alpha-1)f_{Q}\) can be far from all polynomials if \(f_{Q}\) is, e.g. a Holder-smooth graphon.2 However, under the network model, this means \(A_{P}\) and \(A_{Q}\) are stochastically identical modulo one being \(\alpha\) times denser than the other.

Footnote 2: In fact, Cai and Pu (2024) highlight this exact setting as a direction for future work.

We will therefore consider pairs \((f_{P},f_{Q})\) that are close in some measure of local graph structure. With this in mind, we use a graph distance introduced in Mao et al. (2021) for a different inference problem.

**Definition 1.2** (Graph Distance).: _Let \(P\in[0,1]^{n\times n}\) be the probability matrix of a graph. For \(i,j\in[n],i\neq j\), we define the graph distance between them as follows:_

\[d_{P}(i,j):=\|(\bm{e}_{i}-\bm{e}_{j})^{T}P^{2}(I-\bm{e}_{i}\bm{e}_{i}^{T}-\bm{e }_{j}\bm{e}_{j}^{T})\|_{2}^{2},\]

_where \(\bm{e}_{i},\bm{e}_{j}\in\mathbb{R}^{n}\) are standard basis vectors._

Intuitively, this first computes the matrix \(P^{2}\) of common neighbors, and then computes the distance between two rows of the same (ignoring the diagonal elements). We will require that \(f_{P},f_{Q}\) satisfy a local similarity condition on the relative rankings of nodes with respect to this graph distance. Since we only estimate the probability matrix of \(Q\), the condition is on the latent variables \(\bm{x}_{1},\ldots,\bm{x}_{n}\) of interest. The hope is that the proximity in graph distance reflects the proximity in latent positions.

**Definition 1.3** (Rankings Assumption at Quantile \(h_{n}\)).: _Let \((P,Q)\) be a pair of graphs evaluated on \(n\) latent positions. We say \((P,Q)\) satisfy the rankings assumption at quantile \(h_{n}\leq 1\) if there exists constant \(C>0\) such that for all \(i\in[n]\) and all \(j\neq i\), if \(j\) belongs to the bottom \(h_{n}\)-quantile of \(d_{P}(i,\cdot)\), then \(j\) belongs to the bottom \(Ch_{n}\)-quantile of \(d_{Q}(i,\cdot)\)._

To further motivate Definition 1.3, recall our motivating example of biological network estimation. Previous works require some form of similarity between networks to enable transfer Sen et al. (2018); Fan et al. (2019); Baranwal et al. (2020). For example, Kshirsagar et al. (2013) require a _commonality hypothesis_: if pathogens A, B target the same neighborhoods in a protein interaction network, one can transfer from A to B. Our rankings assumption similarly posits that to transfer knowledge from A to B, A and B have similar 2-hop neighborhood structures.

Note that Definition 1.3 involves quantiles of graph distances; therefore it is a _relative_ condition, because it depends on a rank-ordering within both graphs \(P,Q\) before comparison. On the other hand, an _absolute_ condition would require that for nodes \(i,j\in[n]\), if e.g. \(d_{P}(i,j)<100\) then \(d_{Q}(i,j)<C\cdot 100\). Our condition is more flexible and will hold for a larger set of graph pairs \((P,Q)\), such as pairs where one graph is much more dense than the other.

Finally, to illustrate Definition 1.3, consider stochastic block models \(f_{P},f_{Q}\) with \(k_{P}\geq k_{Q}\) communities respectively. If nodes \(i,j\) are in the same communities then \(P\bm{e}_{i}=P\bm{e}_{j}\), so \(d_{P}(i,j)=0\). We require that \(j\) minimizes \(d_{Q}(i,\cdot)\). This occurs if and only if \(d_{Q}(i,j)=0\). Hence if \(i,j\) belong to the same community in \(P\), they are in the same community in \(Q\). Note that the converse is not necessary; we could have \(Q\) with \(1\) community and \(P\) with arbitrarily many communities.

With the relationship between the source and target defined by the rankings assumption, our contributions are as follows.

**(1) Algorithm for Latent Variable Models.** We provide an efficient Algorithm 1 for latent variable models with Holder-smooth \(f_{P},f_{Q}\). The benefit of this algorithm is that it does not assume a parametric form of \(f_{P}\) and \(f_{Q}\). We prove a guarantee on its error in Theorem 2.3.

**(2) Minimax Rates.** We prove a minimax lower bound for Stochastic Block Models (SBMs) in Theorem 3.2. Moreover, we provide a simple Algorithm 2 that attains the minimax rate for this class (Proposition 3.4).

**(3) Experimental Results on Real-World Data.** We test both of our algorithms on real-world metabolic networks and dynamic email networks, as well as synthetic data (Section 4).

All proofs are deferred to the Appendix.

### Other Related work

Transfer learning has recently drawn a lot of interest both in applied and theoretical communities. The notion of transferring knowledge from one domain with a lot of data to another with less available data has seen applications in epidemiology Apostolopoulos and Bessiana (2020), computer vision Long et al. (2015); Tzeng et al. (2017); Huh et al. (2016); Donahue et al. (2014); Neyshabur et al. (2020), natural language processing Daume (2007), etc. For a comprehensive survey see Zhuang et al. (2019); Weiss et al. (2016); Kim et al. (2022). Recently, there have also been advances in the theory of transfer learning Yang et al. (2013); Tripuraneni et al. (2020); Agarwal et al. (2023); Cai and Wei (2021); Cai and Pu (2024); Cody and Beling (2023).

In the context of networks, transfer learning is particularly useful since labeled data is typically hard to obtain. Tang et al. (2016) develop an algorithmic framework to transfer knowledge obtained using available labeled connections from a source network to do link prediction in a target network. Lee et al. (2017) proposes a deep learning framework for graph-structured data that incorporates transfer learning. They transfer geometric information from the source domain to enhance performance on related tasks in a target domain without the need for extensive new data or model training. The SGDA method Qiao et al. (2023) introduce adaptive shift parameters to mitigate domain shifts and propose pseudo-labeling of unlabeled nodes to alleviate label scarcity. Zou et al. (2021) proposes to transfer features from the previous network to the next one in the dynamic community detection problem. Simchowitz et al. (2023) work on combinatorial distribution shift for matrix completion, where only some rows and columns are given. A similar setting is used for link prediction in egocentrically sampled networks in Wu et al. (2018). Zhu et al. (2021) train a graph neural network for transfer based on an ego-graph-based loss function. Learning from observations of the full network and additional information from a game played on the network Leng et al. (2020); Rossi et al. (2022). Wu et al. (2024) study graph transfer learning for node regression in the Gaussian process setting, where the source and target networks are fully observed.

Levin et al. (2022) proposes an inference method from multiple networks all with the same mean but different variances. While our work is related, we do not assume \(\mathbb{E}[P_{ij}]=\mathbb{E}[Q_{ij}]\). Cao et al. (2010) do joint link prediction on a collection of networks with the same link function but different parameters.

Another line of related but different work deals with multiplex networks (Lee et al., 2014, 2015; Iacovacci and Bianconi, 2016; Cozzo et al., 2018) and dynamic networks Sarkar and Moore (2005); Kim et al. (2018); Sewell and Chen (2015); Sarkar et al. (2012); Chang et al. (2024); Wang et al. (2023). One can think of transfer learning in clustering as clustering with side information. Prior works consider stochastic block models with noisy label information (Mossel and Xu, 2016; Mazumdar and Saha, 2017) or oracle access to the latent structure (Mazumdar and Saha, 2017).

**Notation.** We use lowercase letters \(a,b,c\) to denote (real) scalars, boldface \(\bm{x},\bm{y},\bm{z}\) to denote vectors, and uppercase \(A,B,C\) to denote matrices. Let \(a\lor b:=\max\{a,b\}\) and \(a\wedge b:=\min\{a,b\}\). For integer \(n>0\), let \([n]:=\{1,2,\ldots,n\}\). For a subset \(S\subset[n]\) and \(A\in\mathbb{R}^{n\times n}\), let \(A[S,S]\in\mathbb{R}^{|S|\times|S|}\) be the principal submatrix with row and column indices in \(S\). We denote the \(\ell_{2}\) vector norm as \(\|\bm{x}\|=\|\bm{x}\|_{2}\), dot product as \(\langle\bm{x},\bm{y}\rangle\), and Frobenius norm as \(\|A\|=\|A\|_{F}\). For functions \(f,g:\mathbb{N}\rightarrow\mathbb{R}\) we let \(f\lesssim g\) denote \(f=O(g)\) and \(f\gtrsim g\) denote \(f=\Omega(g)\). All asymptotics \(O(\cdot),o(\cdot),\Omega(\cdot),\omega(\cdot)\) are with respect to \(n_{Q}\) unless specified otherwise.

## 2 Estimating Latent Variable Models with Rankings

In this section, we present a computationally efficient transfer learning algorithm for latent variable models. Algorithm 1 learns the local structure of \(P\) based on graph distances (Definition 1.2). Foreach node \(i\) of \(P\), it ranks the nodes in \(S\) with respect to the graph distance \(d_{P}(i,\cdot)\). For most nodes \(i,j\in[n]\), none of the edges incident to \(i\) or \(j\) are observed in \(Q\). Therefore, we estimate \(\widehat{Q}_{ij}\) by using the edge information about nodes \(r,s\in S\) such that \(d_{P}(i,r)\) and \(d_{P}(j,s)\) are small.

Formally, we consider a model as in Eq. (1) with a compact latent space \(\mathcal{X}\subset\mathbb{R}^{d}\) and latent variables sampled i.i.d. from the normalized Lebesgue measure on \(\mathcal{X}\). We set \(\mathcal{X}=[0,1]^{d}\) without loss of generality and assume that functions \(f:\mathcal{X}\times\mathcal{X}\to[0,1]\) are \(\alpha\)-Holder-smooth.

**Definition 2.1**.: _Let \(f:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) and \(\alpha>0\). We say \(f\) is \(\alpha\)-Holder-smooth if there exists \(C_{\alpha}>0\) such that for all \(\bm{x},\bm{x}^{\prime},\bm{y}\in\mathcal{X}\),_

\[\sum_{\kappa\in\mathbb{N}^{d}:\sum_{i}\kappa_{i}=\lfloor\alpha\rfloor}\left| \frac{\partial^{\sum_{i}\kappa_{i}}f}{\partial_{x_{1}}^{\kappa_{1}}\cdots \partial_{x_{d}}^{\kappa_{d}}}(\bm{x},\bm{y})-\frac{\partial^{\sum_{i}\kappa_ {i}}f}{\partial_{x_{1}}^{\kappa_{1}}\cdots\partial_{x_{d}}^{\kappa_{d}}}(\bm{ x}^{\prime},\bm{y})\right|\leq C_{\alpha}\|\bm{x}-\bm{x}^{\prime}\|_{2}^{ \alpha\wedge 1}.\]

To exclude degenerate cases where a node may not have enough neighbors in latent space, we require the following assumption.

**Assumption 2.2** (Assumption 3.2 of Mao et al. (2021)).: _Let \(G\) be a graph on \(\bm{x}_{1},\ldots,\bm{x}_{n}\). There exist \(c_{2}>c_{1}>0\) and \(\Delta_{n}=o(1)\) such that for all \(\bm{x}_{i},\bm{x}_{j}\),_

\[c_{1}\|\bm{x}_{i}-\bm{x}_{j}\|^{\alpha\wedge 1}-\Delta_{n}\leq\frac{1}{n^{3}}d _{G}(i,j)\leq c_{2}\|\bm{x}_{i}-\bm{x}_{j}\|^{\alpha\wedge 1}.\]

The second inequality follows directly from Holder-smoothness, and the first is shown to hold for e.g. Generalized Random Dot Product Graphs, among others (Mao et al., 2021).

We establish the rate of estimation for Algorithm 1 below.

**Theorem 2.3**.: _Let \(\widehat{Q}\) be as in Algorithm 1. Let \(f_{P}\) be \(\alpha\)-Holder-smooth and \(f_{Q}\) be \(\beta\)-Holder-smooth for \(\beta\geq\alpha>0\), and let \(c\) be an absolute constant. Suppose \((P,Q)\) satisfy Definition 1.3 at \(h_{n}=c\sqrt{\frac{\log n_{Q}}{n_{Q}}}\) and \(P\) satisfies Assumption 2.2 with \(\Delta_{n}=O((\frac{\log n}{n_{Q}})^{\frac{1}{2}\lor\frac{\alpha\wedge 1}{d}})\). Then there exists an absolute constant \(C>0\) such that_

\[\mathbb{P}\left[\frac{1}{n^{2}}\|\widehat{Q}-Q\|_{F}^{2}\lesssim\left(\frac{d }{2}\right)^{\frac{\beta\wedge 1}{2}}\left(\frac{\log n}{n_{Q}}\right)^{\frac{\beta \wedge 1}{2d}}\right]\geq 1-n_{Q}^{-C}.\]To parse Theorem 2.3, consider the effect of various parameter choices. First, observe that our upper bound scales quite slowly with \(n\). Even if \(n\) is superpolynomial in \(n_{Q}\), e.g. \(n=n_{Q}^{\log n_{Q}}\), then \(\log n=O((\log n_{Q})^{2})=n_{Q}^{o(1)}\), so the overall effect on the error is dominated by the \(n_{Q}\) term.

Second, the bound is worse in large dimensions, and scales exponentially in \(\frac{1}{d}\). This kind of scaling also occurs in minimax lower bounds for nonparametric regression (Tsybakov, 2009), and upper bounds for smooth graphon estimation (Xu, 2018). However, we caution that nonparametric regression can be quite different from network estimation; it would be very interesting to know the dependence of dimension on minimax lower bounds for network estimation, but to the best of our knowledge this is an open problem. Finally notice that a greater smoothness \(\beta\) results in a smaller error, up to \(\beta=1\), exactly as in (Gao et al., 2015; Klopp et al., 2017; Xu, 2018).

## 3 Minimax Rates for Stochastic Block Models

In this section, we will show matching lower and upper bounds for a very structured class of latent variable models, namely, Stochastic Block Models (SBMs).

**Definition 3.1** (Sbm).: _Let \(P\in[0,1]^{n\times n}\). We say \(P\) is an \((n,k)\)-SBM if there exist \(B\in[0,1]^{k\times k}\) and \(z:[n]\rightarrow[k]\) such that for all \(i,j\), \(P_{ij}=B_{z(i)z(j)}\). We refer to \(z^{-1}(\{j\})\) as community \(j\in[k]\)._

We first state a minimax lower bound, proved via Fano's method.

**Theorem 3.2** (Minimax Lower Bound for SBMs).: _Let \(k_{P}\geq k_{Q}\geq 1\) with \(k_{Q}\) dividing \(k_{P}\). Let \(\mathcal{F}\) be the family of pairs \((P,Q)\) where \(P\) is an \((n,k_{P})\)-SBM, \(Q\) is an \((n,k_{Q})\)-SBM, and \((P,Q)\) satisfy Definition 1.3 at \(h_{n}=1/k_{P}\). Moreover, suppose \(S\subset[n]\) is restricted to contain an equal number of nodes from communities \(1,2,\ldots,k_{P}\) of \(P\). Then the minimax rate of estimation is:_

\[\inf_{\widehat{Q}\in[0,1]^{n\times n}}\sup_{(P,Q)\in\mathcal{F}}\mathbb{E} \left[\frac{1}{n^{2}}\|\widehat{Q}-Q\|_{F}^{2}\right]\gtrsim\frac{k_{Q}^{2}}{ n_{Q}^{2}}.\]

Note that Definition 1.3 at \(h_{n}=1/k_{P}\) implies that the true community structure of \(Q\) coarsens that of \(P\). The condition that \(k_{Q}\) divides \(k_{P}\) is merely a technical one that we assume for simplicity.

We remark that minimax lower bounds for smooth graphon estimation are established by first showing lower bounds for SBMs, and then constructing a graphon with the same block structure using smooth mollifiers (Gao et al., 2015). Therefore, we expect that Theorem 3.2 can also be extended to the graphon setting, using the same techniques. However, sharp lower bounds for other classes such as Random Dot Product Graphs will likely require different techniques (Xie and Xu, 2020; Yan and Levin, 2023).

**Remark 3.3** (Clustering Regime).: _In Appendix A.4 we also prove a minimax lower bound of \(\frac{\log k_{Q}}{n_{Q}}\) in the regime where the error of recovering the true clustering \(z\) dominates. This matches the rate of Gao et al. (2015), but for estimating all \(n^{2}\) entries of \(Q\), rather than just the \(n_{Q}^{2}\) observed entries._

Theorem 3.2 suggests that a very simple algorithm might achieve the minimax rate. Namely, use both \(A_{P},A_{Q}\) to learn communities, and then use only \(A_{Q}\) to learn inter-community edge probabilities. If \((P,Q)\) are in the nonparametric regime where regression error dominates clustering error (called the _weak consistency_ or _almost exact recovery_ regime), then the overall error will hopefully match the minimax rate.

We formalize this approach in Algorithm 2, and prove that it does achieve the minimax error rate in the weak consistency regime. To this end, we define the signal-to-noise ratio of an SBM with parameter \(B\in[0,1]^{k\times k}\) as follows:

\[s:=\frac{p-q}{\sqrt{p(1-q)}},\]

where \(p=\min_{i}B_{ii},q=\max_{i\neq j}B_{ij}\).

**Proposition 3.4** (Error Rate of Algorithm 2).: _Suppose \(P,Q\in[0,1]^{n\times n}\) are \((n,k_{P}),(n,k_{Q})\)-SBMs with minimum community sizes \(n_{\min}^{(P)},n_{\min}^{(Q)}\) respectively. Suppose also that \((P,Q)\) satisfy 

**Definition 1.3**: _at \(h_{n}=n_{\min}^{(P)}/n\). Then if the signal-to-noise ratios are such that: \(s_{P}\geq C(\frac{\sqrt{n}}{n_{\min}^{(P)}}\vee\frac{\log^{2}(n)}{\sqrt{n_{\min} ^{(P)}}})\) and \(s_{Q}\geq C(\frac{\sqrt{n_{Q}}}{n_{\min}(Q)}\vee\frac{\log^{2}(n_{Q})}{\sqrt{n_{ \min}^{(Q)}}})\) for large enough constant \(C>0\), Algorithm 2 returns \(\widehat{Q}\) such that_

\[\mathbb{P}\left[\frac{1}{n^{2}}\|\widehat{Q}-Q\|_{F}^{2}\lesssim \frac{k_{Q}^{2}\log(n_{\min}^{(Q)})}{n_{Q}^{2}}\right]\geq 1-O\bigg{(}\frac{1}{n_{Q} }\bigg{)}.\]

## 4 Experiments

In this section, we test Algorithm 1 against several classes of simulated and real-world networks. We use quantile cutoff of \(h_{n}=\sqrt{\frac{\log n_{Q}}{n_{Q}}}\) for Algorithm 1 in all experiments.

**Baselines.** To the best of our knowledge, our exact transfer formulation has not been considered before in the literature. Therefore, we implement two algorithms as alternatives to Algorithm 1.

_(1) Algorithm 2_. Given \(A_{P}\in\{0,1\}^{n\times n},A_{Q}\in\{0,1\}^{n_{Q}\times n_{Q}}\), let \(k_{P}=\lceil\sqrt{n}\rceil,k_{Q}=\big{\lceil}\sqrt{n_{Q}}\big{\rceil}\). Compute spectral clusterings \(\widehat{Z}_{P},\widehat{Z}_{Q}\) with \(k_{P},k_{Q}\) clusters respectively. Let \(J_{S}\in\{0,1\}^{n_{Q}\times n}\) is such that \(J_{S;ij}=1\) if and only if \(i=j\) and \(i\in S\). The projection \(\widehat{\Pi}\in\mathbb{R}^{k_{P}\times k_{Q}}\) solves the least-squares problem \(\min_{\widehat{\Pi}\in\mathbb{R}^{k_{P}\times k_{Q}}}\|J_{S}\widehat{Z}_{P} \Pi-\widehat{Z}_{Q}\|_{F}^{2}\). We compute the \(\widehat{\Pi}\) differently from steps 4-7 in Algorithm 2 to account for cases where \(Q\) is not a true coarsening of \(P\). When \(Q\) is a true coarsening of \(P\), this reduces to the procedure in steps 4-7. Given \(\widehat{Z}_{P},\widehat{\Pi}\) we return \(\widehat{Q}\) as in Algorithm 2.

_(2) Oracle._ Suppose that an oracle can access data for \(Q\) on _all_\(n\gg n_{Q}\) nodes as follows. Fix an error probability \(p\in(0,1)\). The oracle is given symmetric \(A_{Q}^{\prime}\in\{0,1\}^{n\times n}\) with independent entries following a mixture distribution. For all \(i,j\in[n]\) with \(i<j\) let \(X_{ij}\sim\text{Bernoulli}(p)\) and \(Y_{ij}\sim\text{Bernoulli}(Q(\bm{x}_{i},\bm{x}_{j}))\). Then:

\[A_{Q;ij}^{\prime}=\mathds{1}_{i\in S,j\in S}Y_{ij}+(1-\mathds{1}_{i\in S,j\in S })((1-X_{ij})Y_{ij}+X_{ij}(1-Y_{ij})).\]

Given \(A_{Q}^{\prime}\), the oracle returns the estimate from Universal Value Thresholding on \(A_{Q}^{\prime}\) Chatterjee (2015). As \(p\to 0\), the error will approach \(O(n^{\frac{-2\beta}{2\beta+d}})\) for a \(\beta\)-smooth network on on \(d\)-dimensional latent variables (Xu, 2018), so the oracle will outperform any transfer algorithm.

**Simulations.** We first test on several classes of simulated networks. For \(n_{Q}=50,n=200\), we run \(50\) independent trials for each setting. We report results for each setting in Table 1, and visualize estimates for stylized examples in Figure 1.

At a glance, Figure 1 shows that Algorithms 1 and 2 both work well on Stochastic Block Models (first row), that only Algorithm 1 works well on graphons (second and third rows), and that the Oracle performs well in all cases.

_Smooth Graphons._ The latent space is \(\mathcal{X}=[0,1]\). We consider graphons of the form \(f_{\gamma}(x,y)=\frac{x^{\gamma}+y^{\gamma}}{2}\) where \(P,Q\) have different \(\gamma\). We denote this the \(\gamma\)-Smooth Graphon.

_Mixed-Membership Stochastic Block Model._ Set \(k_{P}=\lfloor\sqrt{n}\rfloor,k_{Q}=\left\lfloor\sqrt{n_{Q}}\right\rfloor\). The latent space \(\mathcal{X}\) is the probability simplex \(\mathcal{X}=\Delta_{k_{P}}:=\{x\in[0,1]^{k_{P}}:\sum_{i}x_{i}=1\}\subset\mathbb{ R}^{k_{P}}\). The latent variables \(\bm{x}_{1},\ldots,\bm{x}_{n}\) are iid-Dirichlet distributed with equal weights \(\frac{1}{k_{P}},\ldots,\frac{1}{k_{P}}\). Then \(P_{ij}=\bm{x}_{i}^{T}B_{P}\bm{x}_{j}\) and \(Q_{ij}=\Pi(\bm{x}_{i})^{T}B_{Q}\Pi(\bm{x}_{j})\), for connectivity matrices \(B_{P}\in[0,1]^{k_{P}\times k_{P}},B_{Q}\in[0,1]^{k_{Q}\times k_{Q}}\), and projection \(\Pi:\Delta_{k_{P}}\rightarrow\Delta_{k_{Q}}\) for a fixed subset of \([k_{P}]\). For parameters \(a,b,\epsilon\in[0,1]\) we generate \(B\in[0,1]^{k\times k}\) by sampling \(E\in\text{Uniform}(-\epsilon,\epsilon)^{k\times k}\) and set \(B=\text{clip}((a-b)I+b\bm{1}\bm{1}^{T}+E,0,1)\). We call this Noisy-MMSB\((a,b,\epsilon)\).

_Latent Distance Model._ The latent space is the unit sphere \(\mathcal{X}=\mathbb{S}^{d-1}\subset\mathbb{R}^{d}\). For scale parameter \(s>0\), we call \(f_{s}(\bm{x},\bm{y})=\exp(-s\|\bm{x}-\bm{y}\|_{2})\) the \(\mathbb{R}^{d}\)-Latent\((s)\) model.

**Discussion.** When the latent dimension is larger than \(1\) (the Noisy MMSB and Latent Variable Models), our Algorithm 1 is better than both Algorithm 2 and the Oracle with \(p=0.1\). Note that Algorithms 1 and 2 use \(\frac{n_{Q}^{2}}{n^{2}}\approx 0.06\) unbiased edge observations from \(Q\), while the Oracle with \(p=0.1\) observes \((1-p)\frac{n^{2}-n_{Q}^{2}}{n^{2}}\approx 0.9\) unbiased edge observations in expectation.

**Real-World Data.** Next, we test on two classes of real-world networks. We summarize our dataset characteristics in Table 2. See Appendix C for further details.

**Transfer Across Species in Metabolic Networks.** For a fixed organism, a metabolic network has a node for each metabolite, and an edge exists if and only if two metabolites co-occur in a metabolic

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Source & Target & Alg. 1 & Alg. 2 & Oracle & Oracle & Oracle \\  & & & & (\(p=0.1\)) & (\(p=0.3\)) & (\(p=0.5\)) \\ \hline Noisy- & Noisy- & **0.7473\(\pm\)** & \(1.3761\ \pm\) & _0.9556 \(\pm\)_ & \(2.2568\ \pm\) & \(4.2212\ \pm\) \\ MMSB & MMSB & **0.0648** & \(1.1586\) & _0.0633_ & \(0.3107\) & \(0.2825\) \\ \((0.7,0.3,0.01)\) & \((0.9,0.1,0.01)\) & & & & \\ \(0.1\)-Smooth & \(0.5\)-Smooth & _1.7656 \(\pm\)_ & \(4.5033\ \pm\) & \(4.5033\ \pm\) & **0.5016 \(\pm\)** & \(2.4423\ \pm\) & \(5.7774\ \pm\) \\ Graphon & Graphon & _0.7494_ & \(1.5613\) & **0.0562** & \(0.4574\) & \(0.7126\) \\ \(\mathbb{R}^{10}\) & \(\mathbb{R}^{10}\) & **0.5744 \(\pm\)** & \(1.1773\ \pm\) & _0.7715 \(\pm\)_ & \(2.1822\ \pm\) & \(4.3335\ \pm\) \\ Latent\((2.5)\) & Latent\((1.0)\) & **0.1086** & \(1.0481\) & _0.0456_ & \(0.2741\) & \(0.3476\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of different algorithms on simulated networks. Each cell reports \(\widehat{\mu}\pm 2\widehat{\sigma}\) of the mean-squared error over 50 independent trials. Error numbers are all scaled by \(1e2\) for ease of reading. Bold: Best algorithm. Emphasis: Second-best algorithm.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Name & \(n\) & Median Degree & Type \\ \hline BiGG Model iWFL1372 & \(251\) & \(15.00\) & Source \\ BiGG Model iPC815 & \(251\) & \(12.00\) & Source \\ BiGG Model iJN1463 & \(251\) & \(14.00\) & Target \\ Email-EU Days 1-80 & \(1005\) & \(6.92\) & Source \\ Email-EU Days 81-160 & \(1005\) & \(7.35\) & Target \\ Email-EU Days 561-640 & \(1005\) & \(7.66\) & Target \\ \hline \hline \end{tabular}
\end{table}
Table 2: Dataset Characteristicsreaction in that organism. We obtain the unweighted metabolic networks for multiple gram-negative bacteria from the BiGG genome-scale metabolic model dataset (King et al., 2016; Norsigian et al., 2020). In the left half of Figure 2, we compare two choices of source organism in estimating the network for BiGG model iJN1463 (_Pseudomonas putida_). For a good choice of source, Algorithm 1 is competitive with the Oracle at \(p=0.1\).

**Transfer Across Time in the Email Interaction Networks.** We use the Email-EU interaction network between \(n=1005\) members of a European research institution across \(803\) days Leskovec and Krevl (2014); Paranjape et al. (2017). The source graph \(A_{P}\) is the network from day \(1\) to \(\approx 80\) (\([1,80]\)). In Figure 2 we simulate transfer with targets \([81,160]\) (left) and \([561,640]\) (right). We visualize results for arbitrary target periods; similar results hold for other targets. Unlike metabolic networks, Algorithm 2 has comparable performance to both our Algorithm 1 and the oracle algorithm with \(p\in\{0.01,0.05\}\). Compared to the metabolic networks, this indicates that the email interaction networks are relatively well-approximated by SBMs, although Algorithm 1 is still the best.

**Additional Experiments and Baseline.** In Appendix B.1, we present additional ablation experiments that test the dependence of Algorithms 1 and 2 on all relevant parameters. We compare their performance to the Oracle baseline with \(p=0.0\) (the non-transfer setting), and an additional baseline adapted from Levin et al. (2022). We find that our Algorithms outperform this new baseline but are worse than the Oracle with \(p=0.0\), as expected. Further, in Appendix B.2, we test our Algorithms and original baselines on a link prediction task in the setting of Figure 2. We find that the relative accuracy of the methods for link prediction is qualitatively similar to that of Figure 2, and the Oracle performs even better with sparsity tuning.

Figure 1: Comparison of algorithms on three source-target pairs (\(n=2000,n_{Q}=500\)). Each row corresponds to a different source/target pair \((P,Q)\). For a fixed row, the upper triangular part on columns 2, 3, 4 corresponds a \(\widehat{Q}\) for a different algorithm. The upper triangular part of column 1 shows the true \(P\). The lower triangular part of columns 1, 2, 3, and 4 is identical for a fixed row, and shows the true \(Q\). In each heatmap, the lower triangle is the target \(Q\). Algorithm 2 performs best when \((P,Q)\) are SBMs (top), while Algorithm 1 is better for smooth graphons (2nd and 3rd rows).

## 5 Conclusion

In this paper, we study transfer learning for network estimation in latent variable models. We show that there exists an efficient Algorithm 1 that achieves vanishing error even when \(n\geq n_{Q}^{\omega(1)}\), and a simpler Algorithm 2 for SBMs that achieves the minimax rate.

There are several interesting directions for future work.

First, we believe that Algorithm 1 works for moderately sparse networks with population edge density \(\Omega(\frac{1}{\sqrt{n}})\). This is because the concentration of empirical graph distance (Algorithm 1 line 3) requires expected edge density \(\widetilde{\Omega}(n^{-1/2})\) Mao et al. (2021). It would be interesting to see if a similar approach can work for edge density \(\Omega(\frac{\log n}{n})\). For example, in the aforementioned paper it is shown that a variation of the graph distance of Definition 1.2 concentrates at expected edge density \(\widetilde{\Omega}(n^{-2/3})\). While is this still far from the \(\Omega(\frac{\log n}{n})\) regime, it suggests that variations on the graph distance might ensure our Algorithm 1 works for sparser graphs.

Second, the case of multiple sources is also interesting. We have focused on the case of one source distribution, as in Cai and Wei (2021); Cai and Pu (2024), but expect that our algorithms can be extended to multiple sources as long as they satisfy Definition 1.3.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their valuable feedback.

AJ and PS gratefully acknowledge NSF grants 2217069, 2019844, and DMS 2109155.

AM was supported by NSF awards 2217058 and 2133484.

SSM was partially supported by an INSPIRE research grant (DST/INSPIRE/04/2018/002193) from the Dept. of Science and Technology, Govt. of India and a Start-Up Grant from Indian Statistical Institute.

## References

* Abbe (2017) Abbe, E. (2017). Community detection and stochastic block models. _arXiv preprint arXiv:1703.10146_.
* Agarwal et al. (2023) Agarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., and Zhang, X. (2023). Provable benefits of representational transfer in reinforcement learning. In Neu, G. and Rosasco, L., editors, _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 2114-2187. PMLR.

Figure 2: Results of network estimation on real-world data. Shaded regions denote \([1,99]\) percentile outcomes from \(50\) trials.

_Left half_: Estimating metabolic network of iJN1463 (_Pseudomonas putida_) with source iWFL1372 (_Escherichia coli W_) leftmost, and source iPC815 (_Yersinia pestis_) second-left.

_Right half_: Using source data from days \(1-80\) of Email-EU to estimate target days \(81-160\) (third-left) and target days \(561-640\) (rightmost). Note that we use smaller values of \(p\) for the Oracle in Email-EU.

Airodli, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). Mixed membership stochastic blockmodels. _Journal of Machine Learning Research_, 9:1981-2014.
* Aldous (1981) Aldous, D. J. (1981). Representations for partially exchangeable arrays of random variables. _Journal of Multivariate Analysis_, 11:581-598.

* Baranwal et al. (2020) Baranwal, M., Magner, A., Elvati, P., Saldinger, J., Violi, A., and Hero, A. O. (2020). A deep learning architecture for metabolic pathway prediction. _Bioinformatics_, 36(8):2547-2553.
* Barbera et al. (2015) Barbera, P., Jost, J. T., Nagler, J., Tucker, J. A., and Bonneau, R. (2015). Tweeting from left to right: Is online political communication more than an echo chamber? _Psychological science_, 26(10):1531-1542.
* Ben-David et al. (2006) Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2006). Analysis of representations for domain adaptation. _Advances in neural information processing systems_, 19.
* Bickel and Chen (2009) Bickel, P. J. and Chen, A. (2009). A nonparametric view of network models and newman-girvan and other modularities. _Proceedings of the National Academy of Sciences_, 106(50):21068-21073.
* Cai and Pu (2024) Cai, T. T. and Pu, H. (2024). Transfer learning for nonparametric regression: Non-asymptotic minimax analysis and adaptive procedure. _arXiv preprint arXiv:2401.12272_.
* Cai and Wei (2021) Cai, T. T. and Wei, H. (2021). Transfer learning for nonparametric classification: Minimax rate and adaptive classifier. _The Annals of Statistics_, 49(1).
* Cao et al. (2010) Cao, B., Liu, N. N., and Yang, Q. (2010). Transfer learning for collective link prediction in multiple heterogenous domains. In _Proceedings of the 27th international conference on machine learning (ICML-10)_, pages 159-166. Citeseer.
* Cao and Ward (2014) Cao, X. and Ward, M. D. (2014). Do democracies attract portfolio investment? transnational portfolio investments modeled as dynamic network. _International Interactions_, 40(2):216-245.
* Chan and Airoldi (2014) Chan, S. and Airoldi, E. (2014). A consistent histogram estimator for exchangeable graph models. In _International Conference on Machine Learning_, pages 208-216. PMLR.
* Chang et al. (2024) Chang, S., Koehler, F., Qu, Z., Leskovec, J., and Ugander, J. (2024). Inferring dynamic networks from marginals with iterative proportional fitting. _arXiv preprint arXiv:2402.18697_.
* Chatterjee (2015) Chatterjee, S. (2015). Matrix estimation by universal singular value thresholding. _The Annals of Statistics_, pages 177-214.
* Chen et al. (2014) Chen, Y., Sanghavi, S., and Xu, H. (2014). Improved graph clustering. _IEEE Transactions on Information Theory_, 60(10):6440-6455.
* Christensen and Nielsen (2000) Christensen, B. and Nielsen, J. (2000). _Metabolic Network Analysis_, pages 209-231. Springer Berlin Heidelberg, Berlin, Heidelberg.
* Cody and Beling (2023) Cody, T. and Beling, P. A. (2023). A systems theory of transfer learning. _IEEE Systems Journal_, 17(1):26-37.
* Cortes et al. (2008) Cortes, C., Mohri, M., Riley, M., and Rostamizadeh, A. (2008). Sample selection bias correction theory. In _International conference on algorithmic learning theory_, pages 38-53. Springer.
* Cozzo et al. (2018) Cozzo, E., De Arruda, G. F., Rodrigues, F. A., and Moreno, Y. (2018). _Multiplex networks: basic formalism and structural properties_, volume 10. Springer.
* Crammer et al. (2008) Crammer, K., Kearns, M., and Wortman, J. (2008). Learning from multiple sources. _Journal of Machine Learning Research_, 9(8).
* Daume (2007) Daume, H. (2007). Frustratingly easy domain adaptation. _ArXiv_, abs/0907.1815.
* D'Ariri et al. (2015)Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In Xing, E. P. and Jebara, T., editors, _Proceedings of the 31st International Conference on Machine Learning_, volume 32 of _Proceedings of Machine Learning Research_, pages 647-655, Bejing, China. PMLR.
* Fan et al. (2019) Fan, J., Cannistra, A., Fried, I., Lim, T., Schaffner, T., Crovella, M., Hescott, B., and Leiserson, M. D. (2019). Functional protein representations from biological networks enable diverse cross-species inference. _Nucleic acids research_, 47(9):e51-e51.
* Gao et al. (2015) Gao, C., Lu, Y., and Zhou, H. H. (2015). Rate-optimal graphon estimation. _The Annals of Statistics_, pages 2624-2652.
* Guruswami et al. (2019) Guruswami, V., Rudra, A., and Sudan, M. (2019). Essential coding theory.
* Handcock et al. (2007) Handcock, M. S., Raftery, A. E., and Tantrum, J. M. (2007). Model-based clustering for social networks. _Journal of the Royal Statistical Society: Series A (Statistics in Society)_, 170(2):301-354.
* Hanneke and Kpotufe (2019) Hanneke, S. and Kpotufe, S. (2019). On the value of target data in transfer learning. _Advances in Neural Information Processing Systems_, 32.
* Hanneke and Kpotufe (2022) Hanneke, S. and Kpotufe, S. (2022). A no-free-lunch theorem for multitask learning. _The Annals of Statistics_, 50(6):3119-3143.
* Hoeffding (1994) Hoeffding, W. (1994). Probability inequalities for sums of bounded random variables. _The collected works of Wassily Hoeffding_, pages 409-426.
* Hoff (2007) Hoff, P. (2007). Modeling homophily and stochastic equivalence in symmetric relational data. _Advances in neural information processing systems_, 20.
* Hoff et al. (2002) Hoff, P. D., Raftery, A. E., and Handcock, M. S. (2002). Latent Space Approaches to Social Network Analysis. _Journal of the American Statistical Association_, 97(460):1090-1098.
* Holland et al. (1983) Holland, P. W., Laskey, K. B., and Leinhardt, S. (1983). Stochastic blockmodels: First steps. _Social Networks_, 5(2):109-137.
* Hoover (1979) Hoover, D. N. (1979). _Relations on probability spaces arrays of random variables_. Institute for Advanced Study,RI.
* Huang et al. (2013) Huang, J.-T., Li, J., Yu, D., Deng, L., and Gong, Y. (2013). Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In _2013 IEEE international conference on acoustics, speech and signal processing_, pages 7304-7308. IEEE.
* Huh et al. (2016) Huh, M., Agrawal, P., and Efros, A. A. (2016). What makes imagenet good for transfer learning? _arXiv preprint arXiv:1608.08614_.
* Iacovacci and Bianconi (2016) Iacovacci, J. and Bianconi, G. (2016). Extracting information from multiplex networks. _Chaos: An Interdisciplinary Journal of Nonlinear Science_, 26(6).

* Kim et al. (2022) Kim, H., Cosa-Linan, A., Santhanam, N., and et al. (2022). Transfer learning for medical image classification: a literature review. _BMC Medical Imaging_, 22(1):69.
* King et al. (2016) King, Z. A., Lu, J., Drager, A., Miller, P., Federowicz, S., Lerman, J. A., Ebrahim, A., Palsson, B. O., and Lewis, N. E. (2016). Bigg models: A platform for integrating, standardizing and sharing genome-scale models. _Nucleic acids research_, 44(D1):D515-D522.
* Klopp et al. (2017) Klopp, O., Tsybakov, A. B., and Verzelen, N. (2017). Oracle inequalities for network models and sparse graphon estimation. _Annals of Statistics_, 45(1):316-354.
* Kshirsagar et al. (2013) Kshirsagar, M., Carbonell, J., and Klein-Seetharaman, J. (2013). Multitask learning for host-pathogen protein interactions. _Bioinformatics_, 29(13):i217-i226.
* Koshino et al. (2017)Lee, J., Kim, H., Lee, J., and Yoon, S. (2017). Transfer learning for deep learning on graph-structured data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31.
* Lee et al. (2014) Lee, K.-M., Kim, J. Y., Lee, S., and Goh, K.-I. (2014). Multiplex networks. _Networks of networks: The last frontier of complexity_, pages 53-72.
* Lee et al. (2015) Lee, K.-M., Min, B., and Goh, K.-I. (2015). Towards real-world complexity: an introduction to multiplex networks. _The European Physical Journal B_, 88:1-20.
* Leng et al. (2020) Leng, Y., Dong, X., Wu, J., and Pentland, A. (2020). Learning quadratic games on networks. In _International Conference on Machine Learning_, pages 5820-5830. PMLR.
* Leskovec and Krevl (2014) Leskovec, J. and Krevl, A. (2014). SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data.
* Levin et al. (2022) Levin, K., Lodhia, A., and Levina, E. (2022). Recovering shared structure from multiple networks with unknown edge distributions. _Journal of machine learning research_, 23(3):1-48.
* Li et al. (2022) Li, L., Dannenfelser, R., Zhu, Y., Hejduk, N., Segarra, S., and Yao, V. (2022). Joint embedding of biological networks for cross-species functional alignment. _bioRxiv_, pages 2022-01.
* Long et al. (2015) Long, J., Shelhamer, E., and Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3431-3440.
* Lovasz (2012) Lovasz, L. (2012). _Large Networks and Graph Limits._, volume 60 of _Colloquium Publications_. American Mathematical Society.
* Mansour et al. (2009) Mansour, Y., Mohri, M., and Rostamizadeh, A. (2009). Domain adaptation: Learning bounds and algorithms. _arXiv preprint arXiv:0902.3430_.
* Mao et al. (2021) Mao, X., Chakrabarti, D., and Sarkar, P. (2021). Consistent nonparametric methods for network assisted covariate estimation. In _International Conference on Machine Learning_, pages 7435-7446. PMLR.
* Mazumdar and Saha (2017a) Mazumdar, A. and Saha, B. (2017a). Clustering with noisy queries. _Advances in Neural Information Processing Systems_, 30.
* Mazumdar and Saha (2017b) Mazumdar, A. and Saha, B. (2017b). Query complexity of clustering with side information. _Advances in Neural Information Processing Systems_, 30.
* Mossel and Xu (2016) Mossel, E. and Xu, J. (2016). Local algorithms for block models with side information. In _Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science_, pages 71-80.
* Mukherjee and Chakrabarti (2019) Mukherjee, S. S. and Chakrabarti, S. (2019). Graphon estimation from partially observed network data. _arXiv preprint arXiv:1906.00494_.
* Neyshabur et al. (2020) Neyshabur, B., Sedghi, H., and Zhang, C. (2020). What is being transferred in transfer learning? In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, _Advances in Neural Information Processing Systems_, volume 33, pages 512-523. Curran Associates, Inc.
* Norsigian et al. (2020) Norsigian, C. J., Pusarla, N., McConn, J. L., Yurkovich, J. T., Drager, A., Palsson, B. O., and King, Z. (2020). Bigg models 2020: multi-strain genome-scale models and expansion across the phylogenetic tree. _Nucleic acids research_, 48(D1):D402-D406.
* Olhede and Wolfe (2014) Olhede, S. C. and Wolfe, P. J. (2014). Network histograms and universality of blockmodel approximation. _Proceedings of the National Academy of Sciences_, 111(41):14722-14727.
* Paranjape et al. (2017) Paranjape, A., Benson, A. R., and Leskovec, J. (2017). Motifs in temporal networks. In _Proceedings of the tenth ACM international conference on web search and data mining_, pages 601-610.
* Qiao et al. (2023) Qiao, Z., Luo, X., Xiao, M., Dong, H., Zhou, Y., and Xiong, H. (2023). Semi-supervised domain adaptation in graph transfer learning. _ArXiv_, abs/2309.10773.
* Zhang et al. (2016)Ren, M., Zhang, S., and Wang, J. (2023). Consistent estimation of the number of communities via regularized network embedding. _Biometrics_, 79(3):2404-2416.
* Rossi et al. (2022) Rossi, E., Monti, F., Leng, Y., Bronstein, M., and Dong, X. (2022). Learning to infer structures of network games. In _International Conference on Machine Learning_, pages 18809-18827. PMLR.
* Rubin-Delanchy et al. (2022) Rubin-Delanchy, P., Cape, J., Tang, M., and Priebe, C. E. (2022). A statistical interpretation of spectral embedding: the generalised random dot product graph. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(4):1446-1473.
* Sarkar et al. (2012) Sarkar, P., Chakrabarti, D., and Jordan, M. I. (2012). Nonparametric link prediction in dynamic networks. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, ICML'12, page 1897-1904, Madison, WI, USA. Omnipress.
* Sarkar and Moore (2005) Sarkar, P. and Moore, A. (2005). Dynamic social network analysis using latent space models. In Weiss, Y., Scholkopf, B., and Platt, J., editors, _Advances in Neural Information Processing Systems_, volume 18. MIT Press.
* Sen et al. (2018) Sen, R., Tagore, S., and De, R. K. (2018). Asapp: Architectural similarity-based automated pathway prediction system and its application in host-pathogen interactions. _IEEE/ACM Transactions on Computational Biology and Bioinformatics_, 17(2):506-515.
* Sewell and Chen (2015) Sewell, D. K. and Chen, Y. (2015). Latent space models for dynamic networks. _Journal of the american statistical association_, 110(512):1646-1657.
* Simchowitz et al. (2023) Simchowitz, M., Gupta, A., and Zhang, K. (2023). Tackling combinatorial distribution shift: A matrix completion perspective. In Neu, G. and Rosasco, L., editors, _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 3356-3468. PMLR.
* Sweet et al. (2013) Sweet, T. M., Thomas, A. C., and Junker, B. W. (2013). Hierarchical network models for education research: Hierarchical latent space models. _Journal of Educational and Behavioral Statistics_, 38(3):295-318.
* Tang et al. (2016) Tang, J., Lou, T., Kleinberg, J., and Wu, S. (2016). Transfer learning to infer social ties across heterogeneous networks. _ACM Trans. Inf. Syst._, 34(2).
* Trifonova et al. (2015) Trifonova, N., Kenny, A., Maxwell, D., Duplisea, D., Fernandes, J., and Tucker, A. (2015). Spatio-temporal bayesian network models with latent variables for revealing trophic dynamics and functional networks in fisheries ecology. _Ecological Informatics_, 30:142-158.
* Tripuraneni et al. (2020) Tripuraneni, N., Jordan, M. I., and Jin, C. (2020). On the theory of transfer learning: the importance of task diversity. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS '20, Red Hook, NY, USA. Curran Associates Inc.
* Tsybakov (2009) Tsybakov, A. B. (2009). _Introduction to Nonparametric Estimation_. Springer series in statistics. Springer, Dordrecht.
* Tzeng et al. (2017a) Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017a). Adversarial discriminative domain adaptation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7167-7176.
* Tzeng et al. (2017b) Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017b). Adversarial discriminative domain adaptation. _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2962-2971.
* Vershynin (2018) Vershynin, R. (2018). _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press.
* Wang et al. (2023) Wang, H., Mao, Y., Sun, J., Zhang, S., Fan, Y., and Zhou, D. (2023). Dynamic transfer learning across graphs. _arXiv preprint arXiv:2305.00664_.
* Weiss et al. (2016) Weiss, K. R., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. _Journal of Big Data_, 3:1-40.
* Wang et al. (2017)Wu, J., Ainsworth, L., Leakey, A., Wang, H., and He, J. (2024). Graph-structured gaussian processes for transferable graph learning. _Advances in Neural Information Processing Systems_, 36.

* Xie and Xu (2020) Xie, F. and Xu, Y. (2020). Optimal bayesian estimation for random dot product graphs. _Biometrika_, 107(4):875-889.
* Xu (2018) Xu, J. (2018). Rates of convergence of spectral methods for graphon estimation. In _International Conference on Machine Learning_, pages 5433-5442. PMLR.
* Yan and Levin (2023) Yan, H. and Levin, K. (2023). Minimax rates for latent position estimation in the generalized random dot product graph. _arXiv preprint arXiv:2307.01942_.
* Yang et al. (2013) Yang, L., Hanneke, S., and Carbonell, J. (2013). A theory of transfer learning with applications to active learning. _Machine learning_, 90:161-189.
* Yu (1997) Yu, B. (1997). Assouad, fano, and le cam. In _Festschrift for Lucien Le Cam: research papers in probability and statistics_, pages 423-435. Springer.
* Zhang et al. (2017) Zhang, Y., Levina, E., and Zhu, J. (2017). Estimating network edge probabilities by neighbourhood smoothing. _Biometrika_, 104(4):771-783.
* Zhu et al. (2021) Zhu, Q., Yang, C., Xu, Y., Wang, H., Zhang, C., and Han, J. (2021). Transfer learning of graph neural networks with ego-graph information maximization. _Advances in Neural Information Processing Systems_, 34:1766-1779.
* Zhuang et al. (2019) Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. (2019). A comprehensive survey on transfer learning. _Proceedings of the IEEE_, 109:43-76.
* Zou et al. (2021) Zou, J., Lin, F., Gao, S., Deng, G., Zeng, W., and Alterovitz, G. (2021). Transfer learning based multi-objective genetic algorithm for dynamic community detection. _arXiv preprint arXiv:2109.15136_.

Proofs

### Preliminaries

Recall Hoeffding's inequality.

**Lemma A.1** (Hoeffding (1994)).: _Let \(X_{1},\ldots,X_{n}\) be independent random variables such that \(a_{i}\leq X_{i}\leq b_{i}\) almost surely for all \(i\in[n]\). Then_

\[\mathbb{P}\left[\left|\sum_{i=1}^{n}(X_{i}-\mathbb{E}[X_{i}])\right|\geq t \right]\leq 2\exp\bigg{(}\frac{-2t^{2}}{\sum\limits_{i=1}^{n}(b_{i}-a_{i})^{2}} \bigg{)}.\]

We also need Bernstein's inequality.

**Lemma A.2** (Bernstein's Inequality).: _Let \(X_{1},\ldots,X_{n}\) be independent mean-zero random variables with \(|X_{i}|\leq 1\) for all \(i\) and \(n\geq 5\). Then_

\[\mathbb{P}\left[\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}\right|\geq t\right]\leq 2 \exp\bigg{(}\frac{-nt^{2}}{2(1+\frac{t}{3})}\bigg{)}\leq 2\exp\bigg{(}- \frac{nt^{2}}{4}\bigg{)}.\]

### Proof of Theorem 2.3

Throughout this section, let \(\mathcal{X}=[0,1]^{d}\) and \(\mu:\mathcal{X}\to[0,1]\) be the normalized Lebesgue measure.

We require the following Lemmata.

**Lemma A.3**.: _Let \(\upsilon\in(0,1)\) and \(\mu:\mathcal{X}\to[0,1]\) be the normalized Lebesgue measure. Then for all \(\bm{x}\in\mathcal{X}\),_

\[\mu(\operatorname{Ball}(\bm{x},2\upsilon)\cap\mathcal{X})\geq\mu(\operatorname {Ball}(\bm{0},\upsilon)\cap\mathcal{X}).\]

Proof.: Recall \(\mathcal{X}=[0,1]^{d}\). Fix \(\bm{x}\in\mathcal{X},\upsilon>0\). Note that \(\mu(\operatorname{Ball}(\bm{x},\upsilon)\cap\mathcal{X})\) is smallest when \(\bm{x}\) is a vertex of the hypercube; therefore take \(\bm{x}\in\{0,1\}^{d}\) without loss of generality. Then, note that for each \(\bm{z}\in\operatorname{Ball}(\bm{x},\upsilon)\cap\mathcal{X}\), we can find \((2^{d}-1)\) other points \(\bm{z}^{\prime}\in\operatorname{Ball}(\bm{x},\upsilon)\setminus\mathcal{X}\) by reflecting subsets of coordinates of \(\bm{z}\) about \(\bm{x}\). There are \(2^{d}-1\) such nonempty subsets of coordinates. This shows that \(\mu(\operatorname{Ball}(\bm{x},\upsilon)\cap\mathcal{X})\geq\mu(\operatorname {Ball}(\bm{x},\upsilon))/2^{d}\) for all \(\bm{x}\). Since \(\mu(\operatorname{Ball}(\bm{x},\upsilon))\asymp\upsilon^{d}\), the conclusion follows. 

We will repeatedly make use of the concentration of latent positions.

**Lemma A.4** (Latent Concentration).: _Let \(\mathcal{X}=[0,1]^{d}\) and \(\mu\) denote the normalized Lebesgue measure on \(\mathcal{X}\). Suppose \(\bm{x}_{1},\ldots,\bm{x}_{n}\sim\mathcal{X}\) are sampled iid and uniformly at random from \(\mu\). Fix some \(T\subset\mathcal{X}\) such that \(\mu(T)=\upsilon\). Then_

\[\mathbb{P}\left[\left|\upsilon n-\left|\{j\in[n]:\bm{x}_{j}\in T\}\right| \right|\geq 10\sqrt{\frac{\log n}{n}}\right]\leq n^{-10}.\]

Proof.: Let \(X_{i}\) be an indicator variable that equals \(1\) if \(\bm{x}_{i}\in T\) and zero otherwise. Notice the \(X_{i}\) are iid and bounded within \([0,1]\). Moreover, \(\sum_{i}\mathbb{E}[X_{i}]=n\mu(T)\). Therefore by Hoeffding's inequality, for any \(t>0\),

\[\mathbb{P}[\left|\upsilon n-\left|\{j\in[n]:\bm{x}_{j}\in T\}\right|\right|\geq t ]\leq 2\exp\bigg{(}\frac{-2t^{2}}{n}\bigg{)}.\]

Setting \(t=10\sqrt{\frac{\log n}{n}}\) gives the result.

**Corollary A.5**.: _Let \(\epsilon>0\). For \(i\in[n]\) let \(\epsilon_{i}^{\prime}>0\) be \(\epsilon_{i}^{\prime}:=\sup\{v>0:\mu(\operatorname{Ball}(\bm{x}_{i},v)\cap \mathcal{X})\leq\epsilon\). Let \(T_{i}:=\operatorname{Ball}(\bm{x}_{i},\epsilon_{i}^{\prime})\cap\mathcal{X}\). Let \(u_{i}(S):=|\{j\in S:\bm{x}_{j}\in T_{i}\}|\) denote the number of members of \(S\) landing in \(T_{i}\). Then_

\[\mathbb{P}\left[\forall i\in[n]:|u_{i}(S)-n_{Q}\epsilon|\leq 10\sqrt{\frac{ \log n}{n_{Q}}}\right]\geq 1-n^{-8}.\]

Proof.: Notice that each \(T_{i}\) has Lebesgue measure \(\epsilon\) by definition. Therefore \(\mathbb{E}[u_{i}(S)]=n_{Q}\epsilon\). Since \(S\) has \(n_{Q}\) members, setting \(t=10\sqrt{\frac{\log n}{n_{Q}}}\) in the statement of Lemma A.4 and taking a union bound over all \(i\in[n]\) gives the conclusion. 

We will decompose the error of Algorithm 1 into two parts.

**Proposition A.6**.: _Let \(\widehat{Q}\in[0,1]^{n\times n}\) be the estimator from Algorithm 1. Then_

\[\frac{1}{n^{2}}\|Q-\widehat{Q}\|_{F}^{2}\leq\frac{2}{n^{2}}\sum_{i,j\in[n]}(J_ {S}(i,j)+J_{B}(i,j)),\]

_where \(J_{S},J_{B}\) are the smoothing and Bernoulli errors respectively:_

\[J_{S}(i,j) :=\frac{1}{\left|T_{i}\right|^{2}\left|T_{j}\right|^{2}}\bigg{(} \sum_{r\in T_{i},s\in T_{j}}Q_{ij}-Q_{rs}\bigg{)}^{2};\] \[J_{B}(i,j) :=\frac{1}{\left|T_{i}\right|^{2}\left|T_{j}\right|^{2}}\bigg{(} \sum_{r\in T_{i},s\in T_{j}}Q_{rs}-A_{Q;rs}\bigg{)}^{2}.\]

Controlling the Bernoulli errors is relatively straightforward.

**Proposition A.7**.: _Let \(h\) be the bandwidth of Algorithm 1. The Bernoulli error is at most \(O(\frac{\log n}{m})\) with probability \(\geq 1-n^{-8}\), where \(m=h^{2}n_{Q}^{2}\)._

Proof.: Fix \(i,j\in[n]\). We will bound the maximum Bernoulli error \(J_{S}(i,j)\) over \(i,j\), which suffices to bound the average. Let \(m=\left|T_{i}\right|\left|T_{j}\right|\). We want to bound:

\[\left|\frac{1}{\left|T_{i}\right|\left|T_{j}\right|}\sum_{r\in T_{i},s\in T_{ j}}(Q_{rs}-A_{Q;rs})\right|^{2}.\]

Notice each summand is bounded within \(\pm\frac{1}{m}\). Bernstein's inequality gives:

\[\mathbb{P}\left[\left(\frac{1}{\left|T_{i}\right|\left|T_{j}\right|}\sum_{r \in T_{i},s\in T_{j}}Q_{rs}-A_{Q;rs}\right)^{2}\geq t^{2}\right]\leq 2\exp(-0.5t^{2 }m).\]

Setting \(t=C\sqrt{\frac{\log n}{m}}\) for large enough \(C=O(1)\), a union bound tells us that with probability \(\geq 1-n^{-8}\), the Bernoulli error is bounded by \(t^{2}\). 

**Corollary A.8**.: _The Bernoulli error is at most \(O(\sqrt{\frac{\log n_{Q}}{n_{Q}}})\) with probability \(\geq 1-n_{Q}^{-4}\)._

The rest of this section is devoted to bounded the smoothing errors \(J_{S}(i,j)\).

#### a.2.1 Latent Distance to Graph Distance

We claim that if nodes are close in the latent space then they are close in graph distance.

**Proposition A.9**.: _Suppose that \(\|\bm{x}_{i}-\bm{x}_{r}\|\leq\epsilon\) and \(Q\) is \(\beta\)-smooth. Then \(d_{Q}(i,r)\leq C_{\beta}^{2}n^{3}\epsilon^{2(\beta\wedge 1)}\)._

Proof.: We the use smoothness of \(Q\). By definition there exists \(C_{\beta}>0\) such that \(Q_{ki}-Q_{kr}\leq C_{\beta}\|\bm{x}_{i}-\bm{x}_{r}\|^{\beta\wedge 1}\). Therefore,

\[d_{Q}(i,r) =\sum_{\ell\neq i,r}\left|(Q^{2})_{\ell i}-(Q^{2})_{\ell r}\right| ^{2}\] \[=\sum_{\ell\neq i,r}\bigg{(}\sum_{k\in[n]}Q_{\ell k}(Q_{ki}-Q_{kr} )\bigg{)}^{2}\] \[\leq\sum_{\ell\neq i,r}\sum_{k\in[n]}Q_{\ell k}^{2}C_{\beta}^{2} \epsilon^{2(\beta\wedge 1)}\] \[\leq n^{3}C_{\beta}^{2}\epsilon^{2(\beta\wedge 1)}.\qed\]

We can now bound the minimum sizes of the neighborhoods using the concentration of latent positions and the smoothness of the graphon.

**Lemma A.10** (Vershynin (2018)).: _The volume of a ball of radius \(r>0\) in \(\mathbb{R}^{d}\) is \(\frac{\sqrt{\pi}^{d}}{\Gamma(d/2+1)}r^{d}\), where \(\Gamma(\cdot)\) is the \(\Gamma\) function._

**Proposition A.11**.: _Let \(C_{d}=(\Gamma(\frac{d}{2}+1))^{1/d}\). Let \(C_{0},C^{\prime}\) be constants. If \(\upsilon_{n}\geq C\cdot C_{d}(\sqrt{\frac{\log n}{n_{Q}}})^{1/d}\) for large enough constant \(C>0\), and \(g_{n}=C_{0}C_{\beta}^{2}n^{2}(\upsilon_{n})^{2(\beta\wedge 1)}\), then with probability \(\geq 1-n^{-6}\) for all \(i\in[n]\) the neighborhood size is \(|\{r:d_{Q}(i,r)\leq g_{n}\}|\geq C^{\prime}n_{Q}\sqrt{\frac{\log n}{n_{Q}}}\)._

Proof.: Fix \(i\in[n]\) and \(\upsilon_{n}>0\). Let \(\epsilon_{i}\) denote the Lebesgue measure of \(\operatorname{\mathrm{Ball}}(\bm{x}_{i},\upsilon_{n})\cap\mathcal{X}\). By Lemma A.3 and Lemma A.10, for all \(i\), \(\epsilon_{i}\geq(\frac{\sqrt{\pi}\upsilon_{n}}{2C_{d}})^{d}=(\frac{0.5\sqrt{ \pi}\upsilon_{n}}{C_{d}})^{d}\). Let \(\epsilon=\min_{i\in[n]}\epsilon_{i}\).

By Corollary A.5, with probability \(\geq 1-n^{-8}\), there are \(n_{Q}\epsilon-C\sqrt{\frac{\log n}{n_{Q}}}\) members \(j\) of \(S\) such that \(\|\bm{x}_{i}-\bm{x}_{j}\|\leq\upsilon_{n}\). A union bound over \(i\) gives the result simultaneously for all \(i\) with probability \(\geq 1-n^{-6}\).

From Proposition A.9, it follows that for all \(i\in[n]\),

\[\left|\{r\in S:d_{Q}(i,r)\leq C_{\beta}^{2}n^{2}(2\upsilon_{n}^{\prime})^{2( \beta\wedge 1)}\}\right|\geq n_{Q}\epsilon-10\sqrt{\frac{\log n}{n_{Q}}}.\]

Choosing \(\upsilon_{n}\geq C\cdot C_{d}(\frac{\log n}{n_{Q}})^{\frac{1}{2d}}\) for large enough \(C>0\) gives the conclusion. 

#### a.2.2 Graph Distance Concentration

Next, we show that the empirical graph distance concentrates to the population distance.

**Proposition A.12**.: _For any arbitrary symmetric \(P\in[0,1]^{n\times n}\), we have, for all \(i,j\) simultaneously with probability at least \(\geq 1-O(n^{-8})\), that_

\[|d_{A_{P}}(i,j)-d_{P}(i,j)|\leq O(n^{2}\log n)+O(n^{2.5}\sqrt{\log n}).\]Proof.: Fix \(i,j\). Let \(C_{ij}:=(A_{P}^{2})_{ij}\). By Mao et al. (2021) A.1, we have \(C_{ij}=(P^{2})_{ij}+t_{ij}\) for an error term \(t_{ij}\) such that \(\mathbb{P}[\forall i,j:|t_{ij}|\leq 10\sqrt{n\log n}]\geq 1-n^{-10}\). Then,

\[|d_{A_{P}}(i,j)-d_{P}(i,j)| =\left|\sum_{\ell\neq i,j}\big{(}(C_{i\ell}-C_{j\ell})^{2}-((P^{2 })_{i\ell}-(P^{2})_{j\ell})^{2}\big{)}\right|\] \[=\sum_{\ell\neq i,j}\big{|}(t_{i\ell}+t_{j\ell})^{2}+2(t_{i\ell}+ t_{j\ell})((P^{2})_{i\ell}-(P^{2})_{j\ell})\big{|}\] \[\leq O(n^{2}\log n)+O\bigg{(}\sqrt{n\log n}\sum_{\ell\neq i,j} \big{(}(P^{2})_{i\ell}-(P^{2})_{j\ell}\big{)}\bigg{)}.\]

Finally, notice that all entries of \(P^{2}\) are of size \(O(n)\), so the conclusion follows. 

Finally, we will show that taking the restriction of the graph distance \(T_{i}^{P}\) to nodes in \(S\subset[n]\) does not incur too much error.

**Proposition A.13**.: _Suppose \(n=n_{Q}^{O(1)}\). Then there exists a constant \(C\) such that if \(h_{0}\geq C\sqrt{\frac{\log n}{n_{Q}}}+\Delta_{n}\), then for all \(i,r\) simultaneously, \(r\in T_{i}^{A_{P}}(h_{0})\) implies \(r\in T_{i}^{P}(h_{2})\) for some \(h_{2}=O(h)\) with probability \(\geq 1-O(n^{-5})\)._

Proof.: Let us introduce the notation \(T_{i}^{P,S}(h)\) to denote the bottom \(h\)-quantile of \(\{d_{P}(i,j):j\in S\}\). In this notation, \(T_{i}^{A_{P}}(h):=T_{i}^{A_{P},S}(h)\) since we restrict the quantile to nodes in \(S\). From Proposition A.12 and Assumption 2.2, we know that if \(n\geq n_{Q}\) then for \(h_{0}\leq h_{1}-20\sqrt{\frac{\log n}{n}}-\Delta_{n}\) we have \(T_{i}^{A_{P}}(h_{0})\subseteq T_{i}^{P,S}(h_{1})\) simultaneously for all \(i\in[n]\) with probability \(\geq 1-O(n^{-8})\). It remains to compare \(T_{i}^{P,S}(h_{1})\) with \(T_{i}^{P}(h_{2})\) for some \(h_{2}\).

We claim that if \(h_{2}\geq 30\sqrt{\frac{\log n_{Q}}{n_{Q}}}\) then \(\mathbb{P}[\forall i\,\big{|}T_{i}^{P}\cap S\big{|}\geq h_{2}n_{Q}-3\sqrt{n_{Q }\log n_{Q}}]\geq 1-O(n_{Q}^{-2})\). To see this, fix \(i\in[n]\) and consider \(T_{i}^{P}(h_{2})\). For \(j\in S\), let \(X_{j}\) be the indicator variable:

\[X_{j}=\begin{cases}1&\text{if }j\in T_{i}^{P}(h_{2}),\\ 0&\text{otherwise}.\end{cases}\]

Notice that \(\big{|}T_{i}^{P}(h_{2})\cap S\big{|}=\sum_{j\in S}X_{j}\). By Hoeffding's inequality, since \(\mathbb{E}[\sum_{j\in S}X_{j}]=h_{2}n_{Q}\) and \(|X_{j}-h_{2}|\leq 1\) for all \(j\), we have

\[\mathbb{P}\left[\,\big{|}\big{|}T_{i}^{P}(h_{2})\cap S\big{|}-h_{2}n_{Q}\big{|} \geq 3\sqrt{n_{Q}\log n}\right]\leq 2\exp\bigg{(}-\frac{6n_{Q}^{2}\log n}{n_{Q} ^{2}}\bigg{)}\leq 2n^{-6}.\]

Taking a union bound over all \(i\in[n]\) shows the claim holds with probability \(\geq 1-O(n^{-5})\). Therefore we set \(h_{1}\leq h_{2}-3.1\sqrt{\frac{\log n}{n_{Q}}}\) then \(j\in T_{i}^{P,S}(h_{1})\) implies \(j\in T_{i}^{P}(h_{2})\).

The conclusion follows with \(C=24\sqrt{\frac{\log n}{\log n_{Q}}}=O(1)\). 

The ranking condition (Definition 1.3) then allows us to translate between graph distances in \(A_{P}\) and \(Q\).

**Corollary A.14**.: _Suppose that Definition 1.3 holds for \((P,Q)\) at \(h_{n}=c\sqrt{\frac{\log n_{Q}}{n_{Q}}}+\Delta_{n}\), for large enough constant \(c>0\). Suppose \(n_{Q}\leq n\leq n_{Q}^{O(1)}\). Then for \(h>h_{n}\) and \(r\in T_{i}^{A_{P}}(h)\), it follows that \(r\in T_{i}^{Q}(h_{3})\) for some \(h_{3}=O(h)\). The statement holds simultaneously for all \(i,r\) with probability \(\geq 1-O(n^{-5})\)._

#### a.2.3 Control of Smoothing Error

We will decompose smoothing error into a sum of two terms called \(E_{S,1}\) and \(E_{S,2}\). The control of \(E_{S,1}\) is relatively straightforward.

**Lemma A.15**.: _The total smoothing error can be bounded with two terms:_

\[\frac{2}{n^{2}}\sum_{i,j\in[n]}J_{S}(i,j)\leq E_{S,1}+E_{S,2},\]

_where_

\[E_{S,1}:=\frac{C}{n}\max_{j\in[n],s\in T_{j}}\|Q(e_{j}-e_{s})\|_ {2}^{2};\] \[E_{S,2}:=\frac{4}{n^{2}}\sum_{i\in[n]}\frac{1}{|T_{i}|}\operatorname {\mathbb{E}}\bigg{[}\sum_{r\in T_{i}}\sum_{j\in[n]}\sum_{s\in T_{j}}(Q_{rj}-Q_ {rs})^{2}.\bigg{]}\]

Proof.: Note that

\[\frac{2}{n^{2}}\sum_{i,j\in[n]}J_{S}(i,j) =\frac{2}{n^{2}}\sum_{i,j\in[n]}\frac{1}{|T_{i}|^{2}\left|T_{j} \right|^{2}}\operatorname{\mathbb{E}}\bigg{[}\bigg{(}\sum_{r\in T_{i},s\in T _{j}}Q_{ij}-Q_{rs}\bigg{)}^{2}\bigg{]}\] \[\leq\frac{2}{n}\sum_{i\in[n]}\frac{1}{n\left|T_{i}\right|}\sum_{j \in[n]}\frac{2}{|T_{j}|}\operatorname{\mathbb{E}}\bigg{[}\sum_{r\in T_{i},s \in T_{j}}(Q_{ij}-Q_{rj})^{2}+(Q_{rj}-Q_{rs})^{2}\bigg{]}\] \[=\frac{4}{n}\sum_{i\in[n]}\frac{1}{n\left|T_{i}\right|} \operatorname{\mathbb{E}}\bigg{[}\sum_{j}\frac{1}{|T_{j}|}\bigg{(}\sum_{r\in T _{i}}(Q_{ij}-Q_{rj})^{2}+\sum_{r\in T_{i}}\sum_{s\in T_{j}}(Q_{rj}-Q_{rs})^{2} \bigg{)}\bigg{]}.\]

The second inner summand is precise \(E_{S,2}\). For \(E_{S,1}\), notice that \(|T_{i}|=|T_{j}|=h(n_{Q}-1)\) by definition. Therefore

\[\sum_{j}\frac{1}{|T_{j}|}\sum_{r\in T_{i}}(Q_{ij}-Q_{rj})^{2}=\frac{1}{h(n_{Q }-1)}\sum_{r\in T_{i}}\sum_{j}(Q_{ij}-Q_{rj})^{2}\leq 2\max_{r\in T_{i}}\|(e_{i} -e_{r})^{T}Q\|_{2}^{2}.\qed\]

We can now bound \(E_{S,1}\) in terms of graph distances.

**Lemma A.16**.: _The smoothing error term \(E_{S,1}\) can be bounded as follows:_

\[E_{S,1}\leq\frac{2}{n}\max_{i\in[n],r\in T_{i}}\sqrt{d_{Q}(i,r)}+\frac{2c}{ \sqrt{n}}\]

_for some constant \(c>0\)._

Proof.: Fix \(i\in[n]\) and \(r\in T_{i}\). We have

\[\|Q(e_{i}-e_{r})\|_{2}^{2} \leq\|e_{i}-e_{r}\|_{2}\|Q^{T}Q(e_{i}-e_{r})\|_{2}\] \[\leq 2\|Q^{2}(e_{i}-e_{r})\|_{2}.\]

Now we will pass to graph distances. Let \(e_{ab}:=((Q^{2})_{aa}-(Q^{2})_{ab})^{2}\) for \(a,b\in[n]\). Notice that \(\|Q^{2}(e_{i}-e_{r})\|_{2}=\sqrt{d_{Q}(i,r)+e_{ir}+e_{ri}}\). Moreover, \(\sqrt{e_{ir}+e_{ri}}\leq 2\sqrt{n}\) since the entries of \(Q^{2}\) are individually bounded by \(O(n)\). The conclusion follows. 

**Proposition A.17**.: _Suppose \(\Delta_{n}=O(\sqrt{\frac{\log n}{n_{Q}}})\). Let \(C_{d}\) be the constant of Proposition A.11. Then if the bandwidth of Algorithm 1 is \(h_{n}=C\sqrt{\frac{\log n}{n_{Q}}}\), for a constant \(C=O(1)\), then the smoothing error \(E_{S,1}\) is at most_

\[E_{S,1}\leq C_{2}C_{d}^{\beta\wedge 1}\bigg{(}\sqrt{\frac{\log n_{Q}}{n_{Q}}} \bigg{)}^{\frac{\beta\wedge 1}{d}}\]

_for some \(C_{2}=O(1)\), with probability \(\geq 1-O(n^{-6})\)._Proof.: Fix \(i\in[n]\) and \(r\in T_{i}^{A_{P}}(h_{n})\). By Corollary A.14, if \(h_{n}\geq C\sqrt{\frac{\log n}{n_{Q}}}+\Delta_{n}\) for a large enough constant \(C>0\), then there exists constant \(C_{2}>0\) such that the following holds. With probability \(\geq 1-O(n^{-5})\), for all \(i\in[n]\) and \(r\in S\), \(r\in T_{i}^{Q}(C_{2}h_{n})\),

Let \(\upsilon_{n}=CC_{d}(\sqrt{\frac{\log n}{n_{Q}}})^{1/d}\) for \(C_{d}\) as in Proposition A.11 and \(C>0\) large enough constant. Then by Proposition A.11 the set of \(s\in S\) such that \(d_{Q}(i,r)\leq C_{0}C_{\beta}^{2}n^{2}(\upsilon_{n})^{2(\beta\wedge 1)}\) has size at least \(C_{2}n_{Q}\sqrt{\frac{\log n}{n_{Q}}}\).The statement holds for all \(i\) simultaneously with probability at least \(1-O(n^{-6})\). Therefore for all \(i\in[n]\) and \(r\in T_{i}^{A_{P}}(h_{n})\), we have

\[d_{Q}(i,r)\leq C_{0}C_{\beta}^{2}n^{2}(\upsilon_{n})^{2(\beta\wedge 1)}\]

for some \(C_{0},C_{\beta}=O(1)\), with probability \(\geq 1-O(n^{-6})\). By Lemma A.16 we conclude that \(E_{S,1}\) is bounded by \(2\upsilon_{n}^{\beta\wedge 1}+\frac{2}{\sqrt{n}}\) with the same probability. 

#### a.2.4 Control of the Second Smoothing Error

In this section, we show that the second smoothing error can be controlled in terms of \(E_{S,1}\). We will need to track the following quantity.

**Definition A.18** (Membership Count).: _For \(r\in S\) and bandwidth \(h\), distance cutoff \(\epsilon\), the \(P\)-neighborhood count of \(r\) is \(\psi_{P}(r):=\big{|}\{j\in[n]:r\in T_{j}^{P}(h,\epsilon)\}\big{|}\)._

In words, \(\psi_{P}(r)\) counts the number of nodes \(j\in[n]\) such that \(r\) lands in the neighborhood of \(j\) in our algorithm. While we know that \(\big{|}T_{j}^{P}(h)\big{|}\leq hn_{Q}\) always, simply applying the pigeonhole principle gives too weak of a bound on membership counts. The base case is that there may be a "hub" node \(r\) lands in \(T_{j}^{P}(h)\) for all \(j\). We will show that there can be no such hub node.

Supposing that we can control of the empirical count \(\psi_{A_{P}}\), we show that the smoothing error can be bounded.

**Proposition A.19**.: _Let \(h_{n}\) be the bandwidth. Then_

\[E_{S,2}\leq O\bigg{(}\frac{E_{S,1}}{h_{n}n}\bigg{)}\cdot\max_{r\in[n]}(\psi_{A _{P}}(r)).\]

Proof.: Rearranging terms, we have

\[E_{S,2} =\frac{1}{n^{2}h^{2}n_{Q}^{2}}\sum_{i,j\in[n],r\in T_{i},s\in T_{ j}}(Q_{rj}-Q_{rs})^{2}\] \[=\frac{1}{n^{2}h^{2}n_{Q}^{2}}\sum_{r\in S}\psi_{A_{P}}(r)\sum_{j,s}(Q_{rj}-Q_{rs})^{2}\] \[=\frac{n_{Q}}{n^{2}h^{2}n_{Q}^{2}}\mathop{\mathbb{E}}_{r\in[n]} \bigg{[}\psi_{A_{P}}(r)\sum_{j,s}(Q_{rj}-Q_{rs})^{2}\bigg{]},\]

where the last step follows because \(j,s\) do not depend on \(i,r\) and because \(S\subset[n]\) is chosen uniformly at random. Now, we will control the expectation by passing to a row sum, which is handled by \(E_{S,1}\).

\[\mathop{\mathbb{E}}_{r\in[n]}\bigg{[}\psi_{A_{P}}(r)\sum_{j,s}(Q_{rj}-Q_{rs}) ^{2}\bigg{]}\leq\max_{r\in[n]}\bigg{(}\frac{\psi_{A_{P}}(r)}{n}\bigg{)}\cdot \sum_{j\in[n]}\sum_{s\in T_{j}}\|Q(e_{j}-e_{s})\|_{2}^{2}.\]

Recall that \(n^{2}n_{Q}h_{n}E_{S,1}=\Omega\bigg{(}\sum\limits_{j\in[n]}\sum\limits_{s\in T _{j}}\|Q(e_{j}-e_{s})\|_{2}^{2}\bigg{)}\). Hence we conclude that

\[E_{S,2}\leq O\bigg{(}\frac{E_{S,1}}{h_{n}n}\bigg{)}\cdot\max_{r\in[n]}(\psi_{A _{P}}(r)).\qed\]We therefore must show that \(\max\limits_{r\in S}\psi_{A_{P}}(r)\leq O(hn)\) with high probability.

**Proposition A.20** (Population Version).: _Suppose Assumption 2.2 holds for \(P\) with \(c_{1}<c_{2}\) and \(\Delta_{n}=O((\frac{\log n}{n_{Q}})^{\frac{1}{2}\lor\frac{\alpha\wedge 1}{d}})\). Then if \(h\leq C\sqrt{\frac{\log n}{n_{Q}}}\) for large enough constant \(C>0\), then we have \(\max\limits_{r\in S}\psi_{P}(r)\leq O(hn)\) with probability at least \(1-O(n_{Q}^{-8})\)._

Proof.: Fix \(r\in S\). Let \(C_{d}\) be as in Proposition A.11.Suppose that \(\epsilon=C_{d}(C+10)\sqrt{\frac{\log n_{Q}}{n_{Q}}}^{1/d}\) and \(h=C\sqrt{\frac{\log n_{Q}}{n_{Q}}}\). Now, we will claim that for large enough constant \(c>0\), that \(\psi_{P}(r)\) is at most the size of \(\operatorname{\mathrm{Ball}}(\bm{x}_{r},c\epsilon)\cap\{\bm{x}_{1},\dots,\bm{x }_{n}\}\).

Suppose that \(c>0\) is a large enough constant. Now suppose that \(\bm{x}_{j}\) is such that \(\|\bm{x}_{j}-\bm{x}_{r}\|\geq c\epsilon\). We can lower bound the graph distance using Assumption 2.2, as:

\[d_{P}(r,j):=\|(e_{r}-e_{j})^{T}P^{2}(I-e_{r}e_{r}^{T}-e_{j}e_{j}^{T})\|_{2}^{2} \geq c_{1}n^{3}(c\epsilon)^{2(\alpha\wedge 1)}-n^{3}\Delta_{n}.\]

On the other hand, suppose that \(i\in S\) is such that \(\|\bm{x}_{i}-\bm{x}_{j}\|\leq\epsilon\). Then \(d_{P}(i,j)\leq C_{2}^{2}n^{3}\epsilon^{2(\alpha\wedge 1)}\) by Proposition A.9. Therefore since \(\epsilon=C_{d}(C+10)\sqrt{\frac{\log n_{Q}}{n_{Q}}}^{1/d}\) and \(\Delta_{n}=O((\frac{\log n}{n_{Q}})^{\frac{1}{2}\lor\frac{\alpha\wedge 1}{d}})\), for large enough \(c_{1}>0\) we have

\[d_{P}(r,j):=\|(e_{r}-e_{j})^{T}P^{2}(I-e_{r}e_{r}^{T}-e_{j}e_{j}^{T})\|_{2}^{2 }\geq\frac{c_{1}}{2}n^{3}(c\epsilon)^{2(\alpha\wedge 1)}.\]

Then, if we choose \(c>0\) such that \(c^{2(\alpha\wedge 1)}>\frac{2C_{2}^{2}}{c_{1}}\), then \(d_{P}(i,j)<d_{P}(r,j)\).

Next, from our choices of \(h,\epsilon\), by Corollary A.5, simultaneously for all \(i\in[n]\) there are at least \(hn_{Q}\) nodes in \(S\) that have distance \(\leq\epsilon\) in latent space from \(\bm{x}_{i}\), with probablity \(\geq 1-O(n_{Q}^{-6})\).

Therefore, if \(\bm{x}_{r}\not\in\operatorname{\mathrm{Ball}}(\bm{x}_{j},c\epsilon)\cap\{\bm{ x}_{1},\dots,\bm{x}_{n}\}\) then \(r\not\in T_{j}^{P}(h)\). This implies that \(\psi_{P}(r)\leq|\{\operatorname{\mathrm{Ball}}(\bm{x}_{r},2c\epsilon)\cap\{ \bm{x}_{1},\dots,\bm{x}_{n}\}|\). We can bound the size of this ball with Lemma A.4. Notice the Lebesgue measure of \(\operatorname{\mathrm{Ball}}(\bm{x}_{r},2c\epsilon)\cap[0,1]\) is at most \((\frac{4c\epsilon}{C_{d}})^{d}\). Therefore, since \(\bm{x}_{i}\) are chosen iid from the Lebesgue measure on \(\mathcal{X}\), with probability at least \(\geq 1-O(n_{Q}^{-10})\), we have

\[\frac{1}{n}\left|\operatorname{\mathrm{Ball}}(\bm{x}_{r},2c\epsilon)\cap\{\bm{ x}_{1},\dots,\bm{x}_{n}\}\right|\leq 2c\epsilon+10\sqrt{\frac{\log n}{n}}.\]

The right-hand side is bounded by \(O(h)\) if \(n\geq n_{Q}\). Taking a union bound over all \(r\in S\) gives the conclusion. 

We conclude with the desired upper bound.

**Proposition A.21** (Bound on \(\psi_{A_{P}}(r)\)).: _Suppose Assumption 2.2 holds for \(P\) with \(c_{1}<c_{2}\) and \(\Delta_{n}=O((\frac{\log n}{n_{Q}})^{\frac{1}{2}\lor\frac{\alpha\wedge 1}{d}})\). Then if \(h\leq C_{0}\sqrt{\frac{\log n_{Q}}{n_{Q}}}\) for small enough constant \(C_{0}\), then we have \(\max\limits_{r\in S}\psi_{A_{P}}(r)\leq O(hn)\) with probability at least \(1-O(n_{Q}^{-8})\)._

Proof.: By Proposition A.12, with probability at least \(1-O(n_{Q}^{-8})\), we have for all \(r\in S,j\in[n]\) simultaneously that

\[d_{A_{P}}(r,j) \geq d_{P}(r,j)-O(n^{2.5}\sqrt{\log n})\] \[\geq(1-O(\frac{1}{\sqrt{n}}))d_{P}(r,j).\]

Similarly, \(d_{A_{P}}(r,j)\leq(1+O(\frac{1}{\sqrt{n}}))d_{P}(r,j)\). We conclude that \(\psi_{A_{P}}(r)\leq 2\psi_{P}(r)=O(hn)\) with probability \(\geq 1-O(n_{Q}^{-8})\)

#### a.2.5 Overall Error

We can bound \(C_{d}:=\Gamma(\frac{d}{2}+1)^{1/d}\) with the elementary inequality.

**Lemma A.22**.: _Let \(C_{d}:=\Gamma(\frac{d}{2}+1)^{1/d}\). Then \(C_{d}\leq\sqrt{d/2}\)._

Proof of Theorem 2.3.: By Proposition A.21 and Prop A.19, we have that \(E_{S,1}\leq O(E_{S,1})\) with probability at least \(1-O(n_{Q}^{-8})\). Therefore by Proposition A.17,

\[\mathbb{P}\left[E_{S,1}+E_{S,2}\leq O\bigg{(}C_{d}^{\beta\wedge 1}\bigg{(} \frac{\log n}{n_{Q}}\bigg{)}^{\frac{\beta\wedge 1}{2d}}\bigg{)}\right]\geq 1-O(n_{Q}^ {-6}).\]

By Lemma A.22, \(C_{d}\leq\sqrt{d/2}\). Finally, by Corollary A.8, the Bernoulli error is bounded by \(O(\sqrt{\frac{\log n_{Q}}{n_{Q}}})\) with probability \(\geq 1-O(n_{Q}^{-4})\). Applying a union bound over the two kinds of error and Lemma A.15 gives the result. 

### Proof of Theorem 3.2

Recall the Gilbert-Varshamov code (Guruswami et al., 2019).

**Theorem A.23** (Gilbert-Varshamov).: _Let \(q\geq 2\) be a prime power. For \(0<\epsilon<\frac{q-1}{q}\) there exists an \(\epsilon\)-balanced code \(C\subset\mathbb{F}_{q}^{n}\) with rate \(\Omega(\epsilon^{2}n)\)._

We will use the following version of Fano's inequality.

**Theorem A.24** (Generalized Fano Method, Yu (1997)).: _Let \(\mathcal{P}\) be a family of probability measures, \((\mathcal{D},d)\) a pseudo-metric space, and \(\theta:\mathcal{P}\rightarrow\mathcal{D}\) a map that extracts the parameters of interest. For a distinguished \(P\in\mathcal{P}\), let \(X\sim P\) be the data and \(\widehat{\theta}:=\widehat{\theta}(X)\) be an estimator for \(\theta(P)\)._

_Let \(r\geq 2\) and \(\mathcal{P}_{r}\subset\mathcal{P}\) be a finite hypothesis class of size \(r\). Let \(\alpha_{r},\beta_{r}>0\) be such that for all \(i\neq j\), and all \(P_{i},P_{j}\in\mathcal{P}_{r}\),_

\[d(\theta(P_{i}),\theta(P_{j})) \geq\alpha_{r};\] \[KL(P_{i},P_{j}) \leq\beta_{r}.\]

_Then_

\[\max_{j\in[r]}\mathbb{E}_{P_{j}}[d(\widehat{\theta}(X),\theta(P_{j}))]\geq \frac{\alpha_{r}}{2}\bigg{(}1-\frac{\beta_{r}+\log 2}{\log r}\bigg{)}.\]

**Definition A.25** (Relative Hamming Distance).: _For \(\bm{x},\bm{y}\in\{0,1\}^{m}\), we define their relative Hamming distance as follows:_

\[d_{H}(\bm{x},\bm{y}):=\frac{1}{m}\left|\{i\in[m]:x_{i}\neq y_{i}\}\right|.\]

We will need the following construction of coupled codes.

**Proposition A.26**.: _Let \(m_{P},m_{Q}\geq 2\) and \(m_{Q}\) divide \(m_{P}\). There exists a code \(C\subset\{0,1\}^{m_{P}}\) and a projection map \(\Pi:\{0,1\}^{m_{P}}\rightarrow\{0,1\}^{m_{Q}}\) such that if \(C^{\prime}=\{\Pi(w):w\in C\}\) then \(C^{\prime}\) is a code with relative Hamming distance \(\Omega(1)\). Moreover, \(|C|=|C^{\prime}|\geq 2^{0.1m_{Q}}\)_

Throughout the proof, we will identify the community assignment function \(z:[n]\rightarrow[k]\) of an SBM (Definition 3.1) with the matrix \(Z\in\{0,1\}^{n\times k}\) where \(Z_{ij}=1\) if and only if \(z(i)=j\).

Proof.: Begin with a Gilbert-Varshamov code \(B\subset\{0,1\}^{m_{Q}}\) as in Theorem A.23. We can "lift" \(B\) to a code on \(\{0,1\}^{m_{P}}\) simply by concatenation. If \(w\in B\), then the corresponding \(w^{\prime}\in C\) is just \(w^{\prime}=(w,w,\ldots,w)\in\{0,1\}^{m_{P}}\). Let \(\Pi:\{0,1\}^{m_{P}}\rightarrow\{0,1\}^{m_{Q}}\) simply select the first \(m_{Q}\) bits of a word. It is clear that \(B=\{\Pi(w):w\in C\}\), so we are done. 

Now we are ready to prove Theorem 3.2.

[MISSING_PAGE_FAIL:24]

By Prop A.26, \(d_{H}(\Pi(w),\Pi(w^{\prime}))=\Omega(m_{Q})=\Omega(k_{Q}^{2})\). Therefore \(\alpha\leq\Omega(\delta_{Q})\).

Next, by Prop A.27, the pair \((P_{w},Q_{w})\) satisfies Definition 1.3 for all \(w\in C\). Moreover, \(\log|C|\geq 0.1m_{Q}\) by Prop A.26.

Combining these results, by Theorem A.24 the overall lower bound is

\[\inf_{\widehat{Q}}\sup_{w}\frac{1}{n}\|\widehat{Q}-Q_{w}\|_{F} \gtrsim\alpha\bigg{(}1-\frac{\beta+\log 2}{0.1\binom{k_{Q}}{2}} \bigg{)}\] \[\geq\delta_{Q}\bigg{(}1-\frac{30n^{2}\delta_{P}^{2}}{k_{Q}^{2}}- \frac{30n_{Q}^{2}\delta_{Q}^{2}}{k_{Q}^{2}}-o(1)\bigg{)}.\]

If we choose \(\delta_{P}=0.01(\frac{k_{Q}}{n})\) and \(\delta_{Q}=0.01\frac{k_{Q}}{n_{Q}}\), then

\[\inf_{\widehat{Q}}\sup_{w}\frac{1}{n^{2}}\|\widehat{Q}-Q_{w}\|_{F} ^{2} \gtrsim\delta_{Q}^{2}\] \[\gtrsim\frac{k_{Q}^{2}}{n_{Q}^{2}}.\]

Note that \(k_{Q}\leq n_{Q}\leq n\), so \(\delta_{P},\delta_{Q}\in(0,1/4)\) as desired. 

**Proposition A.27**.: _If \(h_{n}=\min\{\frac{1}{k_{P}},\frac{1}{k_{Q}}\}\) then for all \(w\in C\), the pair \((P_{w},Q_{w})\) satisfies Defn 1.3 at \(h_{n}\)._

Proof.: Consider \(h=h_{n}\) and some node \(i\in[n]\). Suppose that \(j\neq i\) is in the same \(P_{w}\)-community as \(i\), and that \(\ell\neq i\) is in a different community. Then notice that \(d_{P_{w}}(i,\ell)\geq d_{P_{w}}(i,j)\). Therefore \(j\in T_{i}^{P_{w}}(h)\). Moreover, since \(h\leq\frac{1}{k_{P}}\) and since the nodes of \(S\subset[n]\) are equidistributed among the communities \(1,2,\ldots,k_{P}\), it follows that all members of \(T_{i}^{P_{w}}(h)\) must belong to the same \(P_{w}\)-community as \(i\).

Therefore, since the communities of \(Q_{w}\) are a coarsening of the communities of \(P_{w}\), \(j\in T_{i}^{Q_{w}}(\frac{1}{k_{Q}})\). Since \(h\leq\frac{1}{k_{Q}}\), we are done. 

### SBM Clustering Error

In this section, we prove a minimax lower bound in the clustering regime for stochastic block models.

**Theorem A.28**.: _Let \(\Pi\) denote the parameter space of pairs of SBMs \((P,Q)\) on \(n\) nodes with \(k_{P},k_{Q}\) communities respectively, such that the cluster structure of \(Q\) is a coarsening the cluster structure of \(P\). Then_

\[\inf_{\widehat{Q}}\sup_{(P,Q)\in\Pi}\mathbb{E}[\frac{1}{n^{2}}\|\widehat{Q}-Q _{i}\|_{F}^{2}]\gtrsim\frac{\log k_{Q}}{n_{Q}}.\]

Proof.: Let \(H_{m}\in[0,1]^{m\times m}\) be the Hadamard matrix of order \(m\) modified to replace all entries \(-1\) with \(0\). If \(m\) is not a power of two, let \(H_{m}\) be defined as follows. Let \(\ell=\lfloor\log_{2}m\rfloor\) and let \(H_{m^{\prime}}\in\mathbb{R}^{m/2\times m/2}\) contain \(H_{2^{\ell-1}}\) on its top left block and zeroes elsewhere. Let

\[H_{m}=\begin{bmatrix}\mathbf{00}^{T}&H_{m^{\prime}}\\ H_{m^{\prime}}^{T}&\mathbf{00}^{T}\end{bmatrix}.\]

Notice that at most \(\frac{7}{8}\) fraction of the entries of \(H_{m}\) are zero-padded, for any \(m\). Now, let \(B_{P}=\frac{1}{2}\mathbf{1}\mathbf{1}^{T}+\delta_{P}H_{k_{P}}\) and \(B_{Q}=\frac{1}{2}\mathbf{1}\mathbf{1}^{T}+\delta_{Q}H_{k_{Q}}\) for some \(\delta_{P},\delta_{Q}\in(0,1/4)\) to be chosen later.

We will define two families of matrices indexed by a finite set \(T\). For \(i\in T\), there are some \(Z_{i}\in\{0,1\}^{n\times k_{P}}\) and \(Y_{i}\in\{0,1\}^{n\times k_{Q}}\) to be specified later. Then

\[P_{i} =Z_{i}B_{P}Z_{i}^{T},\] \[Q_{i} =Y_{i}B_{Q}Y_{i}^{T}.\]Now, we define \(Y_{i}\) as follows. Let \(Z_{n,k_{Q}}\) denote the set of balanced clusterings \(z:[n]\to[k_{Q}]\) such that for all \(i,j\in[k_{Q}]\), \(\left|z^{-1}(\{i\})\right|=\left|z^{-1}(\{j\})\right|\). Let \(Z\subset Z_{n,k_{Q}}\) select the \(z\) such that for all \(j\leq k_{Q}/2\), \(z^{-1}(j)=\{\left\lfloor\frac{n(j-1)}{k_{Q}}\right\rfloor,\ldots,\left\lfloor \frac{nj}{k_{Q}}\right\rfloor\}\). Define a distance on \(Z\) as follows. For \(y,y^{\prime}\in Z\) let \(Y,Y^{\prime}\in\{0,1\}^{n\times k_{Q}}\) be the corresponding cluster matrices and let \(d(y,y^{\prime}):=\frac{1}{n}\|YB_{Q}Y^{T}-Y^{\prime}B_{Q}(Y^{\prime})^{T}\|_{F}\). By Theorem 2.2 of Gao et al. (2015), there exists a packing \(T_{0}\subset Z\) with respect to \(d\) such that for all \(y,y^{\prime}\in T_{0}\), we have \(|\{j:y^{\prime}(j)\neq y(j)\}|\geq n/6\). Moreover, \(\log|T_{0}|\geq\frac{1}{12}n\log k_{Q}\). Set \(T=T_{0}\). For any \(y_{i}\in T_{0}\), let \(Y_{i}\in\{0,1\}^{n\times k_{Q}}\) be the corresponding cluster matrix and then \(Q_{i}=Y_{i}B_{Q}Y_{i}^{T}\).

Now, to define \(Z_{i}\), take \(a\in[k_{Q}]\) and partition \(y_{i}^{-1}(\{a\})\subset[n]\) into \(k_{P}/k_{Q}\) equally sized communities in a uniformly random way. Number these \(1,\ldots,\frac{k_{P}}{k_{Q}}\). In this way, we split community \(1\) of \(y_{i}\) into communities \(1,\ldots,\frac{k_{P}}{k_{Q}}\) of \(z_{i}\), and so on. Define \(Z_{i}\) to be the matrix corresponding to \(z_{i}\). Notice that \(Z_{i},Y_{i}\) are both balanced clusterings and that the clustering \(Y_{i}\) coarsens that of \(Z_{i}\). Therefore \((P_{i},Q_{i})\) are a pair of heterogeneous symmetric SBMs satisfying Definition 1.3 at \(h=1/k_{Q}\).

Next, we apply Fano's Inequality (Theorem A.24). Recall \(\log|T|\geq\frac{1}{12}n\log k_{Q}\). Now, for \(i,j\in T\) distinct, Prop 4.2 of Gao et al. (2015) gives

\[D_{KL}((P_{i},Q_{i}),(P_{j},Q_{j}))\leq D_{KL}(P_{i},P_{j})+D_{KL}(Q_{i},Q_{j} )\leq O(n^{2}\delta_{P}^{2}+n_{Q}^{2}\delta_{Q}^{2})=:\gamma_{1}.\]

Finally, we can bound:

\[\frac{1}{n^{2}}\|Q_{i}-Q_{i^{\prime}}\|_{F}^{2} \geq\frac{1}{n^{2}}\sum_{n/2<j\leq n}\frac{n}{k_{Q}}\|(e_{y_{i}(j )}-e_{y_{i}^{\prime}(j)})B_{Q}\|^{2}\] \[\geq c_{0}\delta_{Q}^{2}=:\gamma_{2}^{2},\]

where \(c_{0}>1\) is some constant. This follows because there are a constant fraction of \(j>n/2\) such that \(y_{i}(j)\neq y_{i}^{\prime}(j)\), and any two rows of the Hadamard matrix differ on half their entries.

Now, set \(\delta_{Q}^{2}=\frac{n_{Q}\log k_{Q}}{10n_{Q}^{2}}\) and \(\delta_{P}^{2}=\frac{\log k_{Q}}{10n^{2}}\). Since \(n\geq n_{Q}\), we conclude that

\[\inf_{\widehat{Q}}\sup_{i\in T}\mathbb{E}\left[\frac{1}{n^{2}}\| \widehat{Q}-Q_{i}\|_{F}^{2}\right] \gtrsim\gamma_{2}^{2}\bigg{(}1-\frac{\gamma_{1}+\log 2}{(1/12)n \log k_{Q}}\bigg{)}\] \[\gtrsim\frac{\log k_{Q}}{n_{Q}}.\qed\]

### Proof of Proposition 3.4

We first argue that Algorithm 2 perfectly recovers \(Z_{P},Z_{Q}\) with high probability.

**Theorem A.29** (Implicit in Chen et al. (2014)).: _Let \(M=ZBZ^{T}\) be an \((n,n_{min},s)\)-HSBM. Then there exists absolute constant \(C>0\) such that the Algorithm of Chen et al. (2014) can recover \(Z\), up to permutation, with zero error with probability \(\geq 1-O(n^{-8})\) if_

\[s\geq C\bigg{(}\frac{\sqrt{n}}{n_{\min}}\vee\frac{\log^{2}(n)}{\sqrt{n_{\min} }}\bigg{)}.\]

Proof.: The algorithm of Chen et al. (2014) returns a matrix \(Y\in\{0,1\}^{n\times n}\) such that \(Y_{ij}=1\) if and only if \(i,j\) are in the same community, with probability \(\geq 1-O(n^{-8})\). Therefore, to construct a clustering from \(Y\), simply assign the cluster of node \(1\) to all \(j\in[n]\) such that \(Y_{1j}=1\), and so on. This returns the true \(Z\in\{0,1\}^{n\times k}\) up to permutation with probability \(\geq 1-O(n^{-8})\). Note that \(k\) is correctly chosen because \(Y\) is equal to a block-diagonal matrix of ones up to permutation, with \(k\) blocks. 

Theorem A.29 implies the following.

**Proposition A.30**.: _Let \(\widehat{Z}_{P},\widehat{Z}_{Q}\) be as in Algorithm 2. Let \(s_{P},s_{Q}\) be the signal to noise ratios of \(P,Q\) respectively. If \(s_{P},s_{Q}\) satisfy the conditions of Theorem A.29 with respect to \((n,n_{\min}^{(P)}\) and \((n_{Q},n_{\min}^{(Q)})\) respectively, then then with probability \(\geq 1-O(n_{Q}^{-8})\), there are permutation matrices \(U_{P}\in\{0,1\}^{k_{P}\times k_{P}},U_{Q}\in\{0,1\}^{k_{Q}\times k_{Q}}\) such that \(\widehat{Z}_{P}=Z_{P}U_{P}\) and \(\widehat{Z}_{Q}=Z_{Q}U_{Q}\)._

Next, we want to recover the clustering of \(Q\) on all \(n\) nodes, not just the \(n_{Q}\) nodes that we observe in \(A_{Q}\). This is given by the following.

**Proposition A.31**.: _1. If \(h_{n}=1/k_{P}\) and \(k_{Q}\leq k_{P}\) then there exists a unique \(\Pi\in\{0,1\}^{k_{P}\times k_{Q}}\) such that \(Z_{P}\Pi\) contains the \(Q\)-clustering of all nodes in \([n]\). Let \(\widetilde{Z}_{Q}:=Z_{P}\Pi\)._

_2. Let \(\widehat{\Pi}\) be as in Algorithm 2 and \(U_{P},U_{Q}\) be as in Proposition A.30. Then with probability \(1-O(\frac{1}{n_{Q}})\), \(Z_{P}U_{P}\widehat{\Pi}=\widetilde{Z}_{Q}U_{Q}\)._

Proof.: Part (1) follows immediately from the SBM structure of \(P,Q\) and definition of Definition 1.3. For Part (2), first notice that by Proposition A.30, with probability at least \(1-O(\frac{1}{n_{Q}})\), Algorithm 2 returns the true clusterings \(\widehat{Z}_{P}=Z_{P}\in\{0,1\}^{n\times k_{P}}\) and \(\widehat{Z}_{Q}=Z_{Q}\in\{0,1\}^{n_{Q}\times k_{Q}}\), up to permutation.

Now, Algorithm 2 simply takes unions of the clusters of \(Z_{P}\) to learn \(\widehat{\Pi}\). Therefore, let \(V:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n_{Q}}\) project onto coordinates in \(S\). Then \(V\widehat{Z}_{P}\widehat{\Pi}=\widetilde{Z}_{Q}\). Moreover, by Proposition A.30, \(\widehat{Z}_{P}=Z_{P}U_{P}\) and \(\widehat{Z}_{Q}=Z_{Q}U_{Q}\). Hence \(VZ_{P}U_{P}\widehat{\Pi}=V\widetilde{Z}_{Q}U_{Q}\). To remove dependence on \(V\), we need to argue that each \(Q\)-cluster has a representative in \(S\).

Let \(E\) be the event that at least one \(Q\)-cluster has no representative in \(S\). For a fixed \(j\in[k_{Q}]\), cluster \(j\) has no representative in \(S\) with probability \(\leq\left(1-\frac{n_{\min}^{(Q)}}{n_{Q}}\right)^{n_{Q}}\). A union bound implies that

\[\mathbb{P}[E]\leq k_{Q}\bigg{(}1-\frac{n_{\min}^{(Q)}}{n_{Q}}\bigg{)}^{n_{Q}} \leq k_{Q}\exp(-n_{\min}^{(Q)})\leq O(n_{Q}^{-1}).\]

The last inequality holds because the condition of Theorem A.29 implies that \(n_{\min}^{(Q)}\geq\Omega(\sqrt{n_{Q}})\) and \(k_{Q}\leq\frac{n_{Q}}{n_{\min}^{(Q)}}\).

Finally, we proceed by conditioning on \(\neg E\). Since \(\widehat{Z}_{P}=Z_{P}U_{P}\), we know that for all \(i\in S\), the unique \(j_{P}\in[k_{P}]\) such that row \(i\), column \(j_{P}\) of \(Z_{P}\) is nonzero contains its true \(P\)-community up to \(U_{P}\). Similarly since \(\widehat{Z}_{Q}=Z_{Q}U_{Q}\), the the unique \(j_{Q}\in[k_{Q}]\) such that row \(i\), column \(j_{Q}\) of \(Z_{P}\) is nonzero contains its true \(Q\)-community up to \(U_{Q}\). Therefore the nodes in community \(j_{P}\) in \(P\) are in community \(j_{Q}\) in \(Q\). So, up to permutations \(U_{P}\) and \(U_{Q}\), we have \(\Pi_{j_{P},j_{Q}}=1\). Since we condition on \(\neg E\), each cluster of \(Q\) has at least one representative in \(S\), so each columns of \(\Pi\) is nonzero. We conclude that \(Z_{P}U_{P}\widehat{\Pi}=\widetilde{Z}_{Q}U_{Q}\) with probability at least \(1-O(n_{Q}^{-1})\). 

We are ready to give the overall error of Proposition 2.

**Proposition A.32**.: _Suppose that \(\widehat{Z}_{P}=Z_{P},\widehat{\Pi}=\Pi\) in Algorithm 2. Then with probability \(\geq 1-O(\frac{1}{n_{Q}})\), Algorithm 2 returns a \(\widehat{Q}\in[0,1]^{n\times n}\) such that_

\[\frac{1}{n^{2}}\|\widehat{Q}-Q\|_{F}^{2}\lesssim\frac{k_{Q}^{2}\log(n_{\min} ^{(Q)})}{n_{Q}^{2}}.\]

Proof.: By Proposition A.31, with probability \(\geq 1-O(\frac{1}{n_{Q}})\), we have \(\widehat{Z}_{P}=Z_{P}U_{P}\), \(\widehat{Z}_{Q}=Z_{Q}U_{Q}\), and \(\widetilde{Z}_{Q}U_{Q}=Z_{P}U_{P}\widehat{\Pi}\). We proceed by conditioning on these events.

Next, let \(W_{Q}\in\mathbb{R}^{k_{Q}\times k_{Q}}\) be the population version of \(\widetilde{W}_{Q}\) with \(W_{Q;ii}=(\mathbf{1}^{T}Z_{Q}\bm{e}_{i})^{-1}\). Then since \(\widehat{Z}_{Q}=Z_{Q}U_{Q}\) we have \(\widehat{W}_{Q}=U_{Q}^{T}W_{Q}U_{Q}\). Hence

\[\widehat{Q} =(Z_{P}U_{P}\widehat{\Pi})(U_{Q}^{T}W_{Q}U_{Q}^{T})(Z_{Q}U_{Q})^{ T}A_{Q}(Z_{Q}U_{Q})(U_{Q}^{T}W_{Q}U_{Q})(Z_{P}U_{P}\widehat{\Pi})^{T}\] \[=\widetilde{Z}_{Q}(W_{Q}Z_{Q}^{T}A_{Q}Z_{Q}W_{Q})\widetilde{Z}_{ Q}^{T}.\]

Next, let \(z_{Q}:[n]\to[k_{Q}]\) be the ground truth clustering map given by \(\widetilde{Z}_{Q}\in\{0,1\}^{n\times k_{Q}}\). Let \(B_{Q}\) be defined analogously to \(\widehat{B}_{Q}\) in Algorithm 2, but using \(W_{Q},Z_{Q},\mathbb{E}[A_{Q}]\) in place of \(\widehat{W}_{Q},\widehat{Z}_{Q},A_{Q}\). Let \(m_{i}:=W_{Q;i}^{-1}\) be the the number of nodes in \(S\) belong to community \(i\), and let \(n_{i}\)be the the number of nodes in \([n]\) belonging to community \(i\) of \(Q\). Then the error of Algorithm 2 is then

\[\frac{1}{n^{2}}\|\widetilde{Z}_{Q}(\widehat{B}_{Q}-B_{Q}) \widetilde{Z}_{Q}^{T}\|_{F}^{2} =\frac{1}{n^{2}}\bigg{(}\sum_{i,j\in[k_{Q}]}n_{i}n_{j}\bigg{(} \sum_{\begin{subarray}{c}r\in z_{Q}^{-1}(\{i\})\cap S\\ s\in z_{Q}^{-1}(\{j\})\cap S\end{subarray}}\frac{B_{Q;ij}-A_{Q;rs}}{m_{i}m_{j }}\bigg{)}^{2}\bigg{)}\] \[=\frac{1}{n^{2}}\sum_{i,j\in[k_{Q}]}\frac{n_{i}n_{j}}{m_{i}^{2}m_ {j}^{2}}\bigg{(}\sum_{\begin{subarray}{c}r\in z_{Q}^{-1}(\{i\})\cap S\\ s\in z_{Q}^{-1}(\{j\})\cap S\end{subarray}}B_{Q;ij}-A_{Q;rs}\bigg{)}^{2}.\]

Next, fix \(i,j\in[k_{Q}]\) and let

\[X_{ij}=\sum_{\begin{subarray}{c}r\in z_{Q}^{-1}(\{i\})\cap S\\ s\in z_{Q}^{-1}(\{j\})\cap S\end{subarray}}B_{Q;ij}-A_{Q;rs}.\]

If we condition on the clusterings of \(P,Q\) being correct then \(\mathbb{E}[B_{Q;ij}-A_{Q;rs}]=0\). Therefore by Hoeffding's inequality,

\[\mathbb{P}(X_{ij}\geq t^{2})\leq 2\exp\bigg{(}-\frac{2t^{2}}{m_{i}m_{j}} \bigg{)}.\]

Setting \(t^{2}=10\log(m_{i}m_{j})m_{i}m_{j}\) implies that with probability at least \(1-k_{Q}^{2}\min_{i}(m_{i})^{-20}\), that the overall error is

\[\frac{1}{n^{2}}\|\widehat{Q}-Q\|_{F}^{2}\leq\frac{1}{n^{2}}\sum_{i,j\in[k_{Q} ]}\frac{10\log(m_{i}m_{j})n_{i}n_{j}}{m_{i}m_{j}}.\]

Finally, note that there exists a constant \(c_{0}>0\) such that for all \(i\in[k_{Q}]\), \(m_{i}\geq c_{0}\sqrt{n_{Q}}\) and \(n_{i}\geq c_{0}\sqrt{n}\), by assumption. Note that each \(m_{i}\) is a random quantity depending on the choice of \(S\subset[n]\) such that \(\mathbb{E}[m_{i}]=\frac{n_{Q}}{n}n_{i}\). Hoeffding's inequality and a union bound over all \(i\in[k_{Q}]\) imply that that with probability at least \(\geq 1-O(n_{Q}^{-8})\) that \(m_{i}\geq\mathbb{E}[m_{i}]-10\sqrt{\log n_{Q}}\geq\Omega(\mathbb{E}[m_{i}])\). We conclude that

\[\frac{1}{n^{2}}\|\widehat{Q}-Q\|_{F}^{2} \leq O\bigg{(}\frac{1}{n_{Q}^{2}}\sum_{i,j\in[k_{Q}]}10\log(m_{i}m _{j})\bigg{)}\] \[\leq O\bigg{(}\frac{k_{Q}^{2}\log(n_{\min}^{(Q)})}{n_{Q}}\bigg{)}.\qed\]

## Appendix B Additional Experiments

### Ablation Experiments

In this section, we discuss additional experiments that quantify the dependence of our algorithms on all relevant parameters. Our experiments also include a new baseline adapted from the estimator of Levin et al. (2022).

**Description of New Baseline.** Levin et al. (2022) assumes that full edge data from both P and Q are observed, and \(P=Q\). Since this is not true for us, we instead compute the following modified MLE based on their estimator from Section 3.3 of Levin et al. (2022).

\[\widetilde{Q}_{ij}=\begin{cases}\frac{w_{P}}{w_{P}+w_{Q}}A_{P;ij}+\frac{w_{Q}}{w _{P}+w_{Q}}A_{Q;ij}&\text{if }i,j\in S,\\ A_{P;ij}&\text{otherwise}.\end{cases}\]

Here \(w_{P},w_{Q}\) are computed as in their paper, based on estimated sub-gamma parameters of the noise for \(A_{P},A_{Q}\). Akin to their adjacency spectral embedding, which assumes known rank of \(Q\), we use Universal Singular Value Thresholding to obtain \(\widehat{Q}\) from \(\widetilde{Q}\) Chatterjee (2015).

**Oracle with \(p=0.0\).** In addition to testing the new baseline from Levin et al. (2022), we also test the Oracle baseline with \(p=0.0\). As noted in Section 4, this corresponds to the non-transfer setting where all edges from the target graph \(Q\) are observed. Note that in this case, the value of \(n_{Q}\) does not matter because edges incident to nodes outside of \(S\) never get flipped. The Oracle error for \(\beta\)-smooth graphons on \(d\)-dimensional latent variables will therefore be \(O(n^{-\frac{2\beta}{2\beta+d}})\) Xu (2018), which is less than the error bound of Theorem 2.3. Indeed, we will find that the Oracle our transfer algorithms in the regimes where its theoretical upper bound is better than our theoretical upper bounds.

Next, we describe the experimental results.

Figure 3 tests Algorithm 1 for general latent variable models. The error (Theorem 2.3) depends on the smoothness \(\beta\) of the target graph, the number of observed target nodes \(n_{Q}\), and the dimension of the latent variables \(d\).

Figure 4 tests Algorithm 2 for Stochastic Block Models. The error (Proposition 3.4) depends on the number of communities \(k_{Q}\) in the target graph, and the number of observed target nodes \(n_{Q}\). Note that Proposition 3.4 also depends logarithmically on the minimum community size of \(Q\), but this is less significant.

Figure 3: Testing parameters of Algorithm 1 (Transfer for Latent Variable Models). For most parameter settings, our method is better than the baseline and worse than the Oracle.

**Left**: Testing Hlder-smoothness of \(f_{Q}\) with \(n=200,n_{Q}=25,d=1\). All methods improve as \(\beta\to 1\). Here \(f_{P}(x,y)=\frac{x^{\alpha}+y^{\alpha}}{2},f_{Q}(x,y)=\frac{x^{\beta}+y^{ \beta}}{2}\) with \(\alpha=0.01\) and \(\beta\) varying.

**Middle**: Testing number of observed target nodes \(n_{Q}\) with \(n=200,d=1\). Here \(f_{P}(x,y)=\frac{x^{\alpha}+y^{\alpha}}{2},f_{Q}(x,y)=\frac{x^{\beta}+y^{ \beta}}{2}\) with \(\alpha=0.01,\beta=0.1\). Note that the oracle does not depend on \(n_{Q}\) because it observes the full adjacency matrix \(A_{Q}\in\{0,1\}^{n\times n}\).

**Right**: Testing dimension \(d\) of latent positions \(\bm{x}_{1},\dots,\bm{x}_{n}\in[0,1]^{d}\) (i.i.d. Lebesgue) with \(n=200,n_{Q}=25\). Here \(f_{P}(\bm{x},\bm{y})=\exp(-6\|\bm{x}-\bm{y}\|_{2})\) and \(f_{Q}(\bm{x},\bm{y})=\exp(-\left|x_{1}-y_{1}\right|)\).

Points are the median MSE across \(50\) trials, with with \([5,95]\) percentile outcomes shaded.

[MISSING_PAGE_FAIL:30]

For the second and third rows, the source function is \(Q(x,y)=\frac{1+\sin(\pi(1+3(x+y-1))))}{2}\) (modified from Zhang et al. (2017)). The sources are \(P(x,y)=1-Q(x,y)\) and \(P(x,y)=Q(\phi(x),y)\), where \(\phi(x)=0.5+|x-0.5|\) if \(x<0.5\), and \(0.5-|x-0.5|\) otherwise.

**Metabolic Networks.** We access metabolic models from King et al. (2016) at http://bigg.ucsd.edu. To construct a reasonable set of shared metabolites across the networks, we take the intersection of the node sets for the following BiGG models: iCHOv1, IJN1463, iMM1415, iPC815, iRC1080,

Figure 5: Link prediction results with the metabolic network of BiGG model iWFL1372 (_Escherichia coli W_) as the source and iJN1463 (_Pseudomonas putida_) the target. Shaded regions denote \([5,95]\) percentile outcomes from \(50\) independent trials.

Figure 6: Link prediction results with the metabolic network of BiGG model iPC815 (_Yersinia pestis_) as the source and iJN1463 (_Pseudomonas putida_) the target. Shaded regions denote \([5,95]\) percentile outcomes from \(50\) independent trials.

Figure 7: Link prediction results with Days 1-80 of Email-EU as the source, and Days 81-160 as target. Shaded regions denote \([5,95]\) percentile outcomes from \(50\) independent trials.

iSDY1059, iSFxv1172, iYL1228, iYS1720, and Recon3D. We obtain a set of \(n=251\) metabolites that are present in all of the listed models.

The resulting networks are undirected, unweighted graphs on \(251\) nodes. We construct the matrix \(A_{P}\in\{0,1\}^{n\times n}\) for species \(P\) by setting \(A_{P;uv}=1\) if and only if \(u\) and \(v\) co-occur in a metabolic reaction in the BiGG model for \(P\).

**Email-EU**. We use the "email-EU-core-temporal" dataset at https://snap.stanford.edu/data/email-Eu-core-temporal.html, as introduced in Paranjape et al. (2017). Note that we do not perform any node preprocessing, so we use all \(n=1005\) nodes present in the data, as opposed to Leskovec and Krevl (2014); Paranjape et al. (2017) who use only \(986\) nodes.

Data consist of triples \((u,v,t)\) where \(u,v\) are anonymized individuals and \(t>0\) is a timestamp. We split the data into \(10\) bins based on equally spaced timestamp percentiles. For simplicity we refer to these time periods as consisting of \(80\) days each in Section 4, but technically there are \(803\) days total. The network at time period \(\ell\) consists of an unweighted undirected graph with adjacency matrix entry \(A_{uv}=1\) if and only if \((u,v,t)\) or \((v,t,u)\) occurred in the data for an appropriate timestamp \(t\).

**Hyperparameters.** We do not tune any hyperparameters. For Algorithm 1 we use the quantile cutoff of \(h_{n}=\sqrt{\frac{\log n_{Q}}{n_{Q}}}\) in all experiments.

Figure 8: Link prediction results with Days 1-80 of Email-EU as the source, and Days 561-640 as target. Shaded regions denote \([5,95]\) percentile outcomes from \(50\) independent trials.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We discuss our main contributions in the abstract and introduction, and then develop them in the main body of the paper. Specifically, we point out Algorithm 1 and Algorithm 2, the upper bounds from Theorem 2.3 and Proposition 3.4, and the lower bound Theorem 3.2. Finally, Section 4 shows experimental results. Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes] Justification: We discuss the limitation of lacking a minimax lower bound for network estimation in \(d\)-dimensions after Theorem 2.3, the need for a different graph distance for graphs with sparsity \(O(\frac{\log n}{n})\) in the Conclusion, and adapting to multiple source distributions in the Conclusion. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We give self-contained theoretical statements with definitions and assumptions in the main body, and full proofs for all claims in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: We give experimental details in Section 4, and give further details on our exact data preparation methods in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example * If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. * If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. * If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). * We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We submit our code as a supplementary zip file in accordance with the NeurIPS code and data submission guidelines. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We describe our choices of source and target network, the network sizes and degrees, and the single hyperparameter (quantile cutoff) for Algorithm 1 in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For each experiment we run \(50\) independent trials. We report \(\pm 2\) standard deviations for the mean-squared error in Table 1, and \([1,99]\) percentile errors in Figure 2. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: As described in Appendix C, we run all experiments on a personal Linux machine with 378GB of CPU/RAM. The total compute time across all results in the paper was less than \(2\) hours. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with the NeurIPS Code of Ethics with regards to potential harms caused by the research process, societal impact, and impact mitigation measures. Moreover, we have anonymized our code and manuscript. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper presents work whose goal is to advance the field of Machine Learning through foundational research. There are many possible societal consequences of our work, none which we feel must be specifically highlighted here. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release data or models that have a high risk for misuse. We use only publicly available data from the academic literature. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We credit the authors of the BiGG metabolic models dataset [16, 17] and the Email-EU dataset [18, 19]. Both datasets are publicly released for academic research. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. See Appendix C for the exact details regarding our use of existing datasets and assets.

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not perform any crowdsourcing or research on human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not perform any research on living subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.