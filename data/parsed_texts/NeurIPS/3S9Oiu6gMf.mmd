# Learning bounded degree polytrees with samples

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We establish finite-sample guarantees for efficient proper learning of bounded-degree _polytrees_, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical models. Very recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We considerably extend their results by providing an efficient algorithm which learns \(d\)-polytrees in polynomial time and sample complexity when the in-degree \(d\) is constant, provided that the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic lower bound, showing that the dependence of our sample complexity is nearly tight in both the dimension and target accuracy parameters.

## 1 Introduction

Distribution learning, or density estimation, is the task of obtaining a good estimate of some unknown underlying probability distribution \(P\) from observational samples. Understanding which classes of distributions could be or could not be learnt efficiently is a fundamental problem in both computer science and statistics, where efficiency is measured in terms of _sample_ (data) and _computational_ (time) complexities.

_Bayesian networks_ (or _Bayes nets_ in short) represent a class of high-dimensional distributions that can be explicitly described by how each variable is be generated sequentially in a directed fashion. Being interpretable, Bayes nets have been used to model beliefs in a wide variety of domains (e.g. see Jensen and Nielsen, 2007; Koller and Friedman, 2009) and references therein). A fundamental problem in computational learning theory is to identify families of Bayes nets which can be learned efficiently from observational data.

Formally, a Bayes net is a probability distribution \(P\), defined over some directed acyclic graphs (DAG) \(G=(V,E)\) on \(|V|=n\) nodes that factorizes according to \(G\) (i.e. Markov with respect to \(G\)) in the following sense: \(P(v_{1},\ldots,v_{n})=\prod_{v_{1},\ldots,v_{n}}P(v\mid\pi(v))\), where \(\pi(v)\subseteq V\) are the parents of \(v\) in \(G\). While it is well-known that given the DAG (structure) of a Bayes net, there exists sample-efficient algorithms that output good hypotheses Dasgupta, 1997; Bhattacharyya et al., 2020), there is no known computationally efficient algorithms for obtaining the DAG of a Bayes net. In fact, it has long been understood that Bayes net structure learning is computationally expensive, in various general settings (Chickering et al., 2004). However, these hardness results fall short when the goal is learning the distribution \(P\) in the probabilistically approximately correct (PAC) (Valiant, 1984) sense (with respect to, say, KL divergence or total variation distance), rather than trying to recover an exact graph from the Markov equivalence class of \(P\).

_Polytrees_ are a subclass of Bayesian networks where the undirected graph underlying the DAG is a forest, i.e., there is no cycle for the undirected version of the DAG; a polytree with maximum in-degree \(d\) is also known as a \(d\)-polytree. With an infinite number of samples, one can recover theDAG of a non-degenerate polytree in the equivalence class with the Chow-Liu algorithm (Chow and Liu, 1968) and some additional conditional independence tests (Rebane and Pearl, 1988). However, this algorithm does _not_ work in the finite sample regime. The only known result for learning polytrees with finite sample guarantees is for 1-polytrees (Bhattacharyya et al., 2021). Furthermore, in the agnostic setting, when the goal is to find the closest polytope distribution to an arbitrary distribution \(P\), the learning problem becomes NP-hard (Dasgupta, 1999).

In this work, we investigate what happens when the given distribution is a \(d\)-polytree, for \(d>1\). _Are \(d\)-polytrees computationally hard to learn in the realizable PAC-learning setting?_ One motivation for studying polytrees is due to a recent work of Gao and Aragam (2021) which shows that polytrees are easier to learn than general Bayes nets due to the underlying graph being a tree, allowing typical causal assumptions such as faithfulness to be dropped when designing efficient learning algorithms.

**Contributions.** Our main contribution is a sample-efficient algorithm for proper Bayes net learning in the realizable setting, when provided with the ground truth skeleton (i.e., the underlying forest). Crucially, our result does not require any distributional assumptions such as strong faithfulness, etc.

**Theorem 1**.: _There exists an algorithm which, given \(m\) samples from a polytree \(P\) over \(\Sigma^{n}\), accuracy parameter \(\varepsilon>0\), failure probability \(\delta\), as well as its maximum in-degree \(d\) and the explicit description of the ground truth skeleton of \(P\), outputs a \(d\)-polytree \(\hat{P}\) such that \(d_{\mathrm{KL}}(P\parallel\hat{P})\leq\varepsilon\) with success probability at least \(1-\delta\), as long as_

\[m=\tilde{\Omega}\!\left(\frac{n\cdot|\Sigma|^{d+1}}{\varepsilon}\log\frac{1}{ \delta}\right)\.\]

_Moreover, the algorithm runs in time polynomial in \(m\), \(|\Sigma|^{d}\), and \(n^{d}\)._

We remark that our result holds when even given only an upper bound on the true in-degree \(d\). In particular, our result provides, for constant \(|\Sigma|\), \(d\), an upper bound of \(\tilde{O}(n/\varepsilon)\) on the sample complexity of learning \(O(1)\)-polytrees. Note that this dependence on the dimension \(n\) and the accuracy parameter \(\varepsilon\) are optimal, up to logarithmic factors: indeed, we establish in Theorem 15 an \(\Omega(n/\varepsilon)\) sample complexity lower bound for this question, even for \(d=2\) and \(|\Sigma|=2\).1

Footnote 1: We remark that (Bhattacharyya et al., 2021, Theorem 7.6) implies an \(\Omega(\frac{n}{\varepsilon}\log\frac{n}{\varepsilon})\) sample complexity lower bound for the analogous question when the skeleton is unknown and \(d=1\).

We also state sufficient conditions on the distribution that enable recovery of the ground truth skeleton. Informally, we require that the data processing inequality hold in a strong sense with respect to the edges in the skeleton graph. Under these conditions, combining with our main result in Theorem 1, we obtain a polynomial-time PAC algorithm to learn bounded-degree polytrees from samples.

**Other related work.** Structure learning of Bayesian networks is an old problem in machine learning and statistics that has been intensively studied; see, for example, Chapter 18 of Koller and Friedman (2009). Many of the early approaches required faithfulness, a condition which permits learning of the Markov equivalence class, e.g. Spirtes and Glymour (1991), Chickering (2002), Friedman et al. (2013). Finite sample complexity of such algorithms assuming faithfulness-like conditions has also been studied, e.g. Friedman and Yakhini (1996). An alternate line of more modern work has considered various other distributional assumptions that permits for efficient learning, e.g., Chickering and Meek (2002), Hoyer et al. (2008), Shimizu et al. (2006), Peters and Buhlmann (2014), Ghoshal and Honorio (2017), Park and Raskutti (2017), Aragam et al. (2019), with the latter three also showing analyzing finite sample complexity. Specifically for polytrees, Rebane and Pearl (1988), Geiger et al. (1990) studied recovery of the DAG for polytrees under the infinite sample regime. Gao and Aragam (2021) studied the more general problem of learning Bayes nets, and their sufficient conditions simplified in the setting of polytrees. Their approach emphasize more on the exact recovery, and thus the sample complexity has to depend on the minimum gap of some key mutual information terms. In contrast, we allow the algorithm to make mistakes when certain mutual information terms are too small to detect for the given sample complexity budget and achieve a PAC-type guarantee. As such, once the underlying skeleton is discovered, our sample complexity only depends on the \(d,n,\varepsilon\) and not on any distributional parameters.

There are existing works on Bayes net learning with tight bounds in total variation distance, with a focus on sample complexity (and not necessarily computational efficiency); for instance, (Canonne et al., 2020). Acharya et al. (2018) consider the problem of learning (in TV distance) a bounded-degree causal Bayesian network from interventions, assuming the underlying DAG is known.

**Outline of paper.** We begin with some preliminary notions and related work in Section 2. Section 3 then shows how to recover a polytree close in KL divergence, assuming knowledge of the skeleton and maximum in-degree. Section 4 gives sufficient conditions to recover the underlying skeleton from samples, while Section 5 provides our sample complexity lower bound. We conclude in Section 6 with some open directions and defer some full proofs to the appendix.

## 2 Preliminaries and tools from previous work

### Preliminary notions and notation

We write the disjoint union as \(\dot{\cup}\). For any set \(A\), let \(|A|\) denotes its size. We use hats to denote estimated quantities, e.g., \(\hat{I}(X;Y)\) will be the estimated mutual information of \(I(X;Y)\). We employ the standard asymptotic notation \(O(\cdot)\), \(\Omega(\cdot)\)\(\Theta(\cdot)\), and write \(\tilde{O}(\cdot)\) to omit polylogarithmic factors. Throughout, we identify probability distributions over discrete sets with their probability mass functions (pmf). We use \(d^{*}\) to denote the true maximum in-degree of the underlying polytree.

### Probability distribution definitions

We begin by defining KL divergence and squared Hellinger distances for a pair of discrete distributions with the same support.

**Definition 2** (KL divergence and squared Hellinger distance).: For distributions \(P,Q\) defined on the same discrete support \(\mathcal{X}\), their KL divergence and squared Hellinger distances are defined as \(d_{\mathrm{KL}}(P\parallel Q)=\sum_{x\in\mathcal{X}}P(x)\log\frac{P(x)}{Q(x)}\) and \(d_{\mathrm{H}}^{2}(P,Q)=1-\sum_{x\in\mathcal{X}}\sqrt{P(x)\cdot Q(x)}\) respectively.

Abusing notation, for a distribution \(P\) on variables \(X=\{X_{1},\ldots,X_{n}\}\), we write \(P_{S}\) to mean the projection of \(P\) to the subset of variables \(S\subseteq X\) and \(P_{G}\) to mean the projection of \(P\) onto a graph \(G\). More specifically, we have \(P_{G}(x_{1},\ldots,x_{n})=\prod_{x\in X}P(x\mid\pi_{G}(x))\) where \(\pi_{G}(x)\) are the parents of \(x\) in \(G\). Note that \(P_{G}\) is the closest distribution2 on \(G\) to \(P\) in \(d_{\mathrm{KL}}\), i.e. \(P_{G}=\operatorname*{argmin}_{Q\in G}d_{\mathrm{KL}}(P\parallel Q)\). By Chow and Liu (1968), we also know that

Footnote 2: One can verify this using Bhattacharyya et al. (2021, Lemma 3.3): For any distribution \(Q\) defined on graph \(G\), we have \(d_{\mathrm{KL}}(P\parallel Q)-d_{\mathrm{KL}}(P\parallel P_{G})=\sum_{v\in V}P (\pi_{G}(v))\cdot d_{\mathrm{KL}}(P(v\mid\pi_{G}(v))\parallel Q(v\mid\pi_{G}( v)))\geq 0\).

\[d_{\mathrm{KL}}(P,P_{G})=-\sum_{i=1}^{n}I(X_{i};\pi_{G}(X_{i}))-H(P_{X})+\sum_ {i=1}^{n}H(P_{X_{i}})\,\] (1)

where \(H\) is the entropy function. Note that only the first term depends on the graph structure of \(G\).

By maximizing the sum of mutual information (the negation of the first term in (1)), we can obtain an \(\varepsilon\)-approximated graph \(G\) such that \(d_{\mathrm{KL}}(P\parallel P_{G})\leq\varepsilon\). In the case of tree-structured distributions, this can be efficiently solved by using any maximum spanning tree algorithm; a natural generalization to bounded degree bayes nets remains open due to the hardness of solving the underlying optimization problem (Hoffgen, 1993). If any valid topological ordering of the target Bayes net \(P\) is present, then an efficient greedy approach is able to solve the problem.

**Definition 3** ((Conditional) Mutual Information).: Given a distribution \(P\), the mutual information of two random variables \(X\) and \(Y\), supported on \(\mathcal{X}\) and \(\mathcal{Y}\) respectively, is defined as

\[I(X;Y)=\sum_{x\in\mathcal{X},y\in\mathcal{Y}}P(x,y)\cdot\log\left(\frac{P(x,y )}{P(x)\cdot P(y)}\right)\.\]

Conditioning on a third random variable \(Z\), supported on \(\mathcal{Z}\), the conditional mutual information is defined as:

\[I(X;Y\mid Z)=\sum_{x\in\mathcal{X},y\in\mathcal{Y},z\in\mathcal{Z}}P(x,y,z) \cdot\log\left(\frac{P(x,y,z)\cdot P(z)}{P(x,z)\cdot P(y,z)}\right)\.\]

By adapting a known testing result from (Bhattacharyya et al., 2021, Theorem 1.3), we can obtain the following corollary, which we will use. We provide the full derivation in the supplementary materials.

**Corollary 4** (Conditional Mutual Information Tester, adapted from [Bhattacharyya et al., 2021, Theorem 1.3]).: _Fix any \(\varepsilon>0\). Let \((X,Y,Z)\) be three random variables over \(\Sigma_{X},\Sigma_{Y},\Sigma_{Z}\) respectively. Given the empirical distribution \((\hat{X},\hat{Y},\hat{Z})\) over a size \(N\) sample of \((X,Y,Z)\), there exists a universal constant \(0<C<1\) so that for any_

\[N\geq\Theta\left(\frac{|\Sigma_{X}|\cdot|\Sigma_{Y}|\cdot|\Sigma_{Z}|}{ \varepsilon}\cdot\log\frac{|\Sigma_{X}|\cdot|\Sigma_{Y}|\cdot|\Sigma_{Z}|}{ \delta}\cdot\log\frac{|\Sigma_{X}|\cdot|\Sigma_{Y}|\cdot|\Sigma_{Z}|\cdot\log( 1/\delta)}{\varepsilon}\right),\]

_the following statements hold with probability \(1-\delta\):_

_(1) If_ \(I(X;Y\mid Z)=0\)_, then_ \(\hat{I}(X;Y\mid Z)<C\cdot\varepsilon\)_._

_(2) If_ \(\hat{I}(X;Y\mid Z)\geq C\cdot\varepsilon\)_, then_ \(I(X;Y\mid Z)>0\)_._

_(3) If_ \(\hat{I}(X;Y\mid Z)\leq C\cdot\varepsilon\)_, then_ \(I(X;Y\mid Z)<\varepsilon\)_._

_Unconditional statements involving \(I(X;Y)\) and \(\hat{I}(X;Y)\) hold similarly by choosing \(|\Sigma_{Z}|=1\)._

### Graph definitions

Let \(G=(V,E)\) be a graph on \(|V|=n\) vertices and \(|E|\) nodes where adjacencies are denoted with dashes, e.g. \(u-v\). For any vertex \(v\in V\), we use \(N(v)\subseteq V\setminus\{v\}\) to denote the neighbors of \(v\) and \(d(v)=|N(v)|\) to denote \(v\)'s degree. An undirected cycle is a sequence of \(k\geq 3\) vertices such that \(v_{1}-v_{2}-\ldots-v_{k}-v_{1}\). For any subset \(E^{\prime}\subseteq E\) of edges, we say that the graph \(H=(V,E^{\prime})\) is the edge-induced subgraph of \(G\) with respect to \(E^{\prime}\).

For oriented graphs, we use arrows to denote directed edges, e.g. \(u\to v\). We denote \(\pi(v)\) to denote the parents of \(v\) and \(d^{in}(v)\) to denote \(v\)'s incoming degree. An interesting directed subgraph on three vertices is the v-structure, where \(u\to v\gets w\) and \(u\mathord{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{ \raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt }{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt }{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{ \raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075 pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt}{\raisebox{-1.075pt{-1.075pt}{\raisebox{-1.0 5pt}{\raisebox{-1.075pt{-1.075pt}{\raisebox{-1.075pt{-1.075pt}{\raisebox{-1.0 5pt{-1.075pt{-1.075pt{-1.075pt}\raisebox{-1.075pt{-1.075pt{-1.0 5pt{-1.075pt{-1.075pt{-1.075pt{-1.075pt}\raisebox{-1.0 5pt{-1.075pt{-1.075pt{-1.075pt{-1.075pt{-1.075pt{-1.0 5pt{-1.075pt{-1.075pt{-1.075pt{-1.075pt{- -1.075pt{-1.075pt{-1.075pt{-1.075pt{- -1.075pt{-1.075pt{-1.075pt{-1.075pt{- -1.075pt{-75pt{-1.075pt{-75.75pt{- -1.075pt{-75pt{-75.75pt{- -75pt{-75.75pt{-75pt-{ -75.75pt{ -75pt-{ -7.075pt{-75pt-{ -775pt-{ -7 5pt-{-77-75pt-{ \cdot 5pt-{-77-75pt-{  \cdot 5pt-{-75pt-{-7-7  \cdot

### Algorithm

At any point in the algorithm, let us define the following sets. Let \(N(v)\) be the set of all neighbors of \(v\) in \(\operatorname{skel}(G^{*})\). Let \(N^{in}(v)\subseteq N(v)\) be the current set of incoming neighbors of \(v\). Let \(N^{out}(v)\subseteq N(v)\) be the current set of outgoing neighbors of \(v\). Let \(N^{un}(v)\subseteq N(v)\) be the current set of unoriented neighbors of \(v\). That is,

\[N(v)=N^{in}(v)\ \dot{\cup}\ N^{out}(v)\ \dot{\cup}\ N^{un}(v)\]

``` Input: Skeleton \(\operatorname{skel}(G^{*})=(V,E)\), max in-degree \(d^{*}\), threshold \(\varepsilon>0\), universal constant \(C\) Output: A complete orientation of \(\operatorname{skel}(G^{*})\)
1: Run Phase 1: Orient strong v-structures (Algorithm 3) \(\triangleright\)\(\mathcal{O}(n^{d^{*}})\) time
2: Run Phase 2: Local search and Meek \(R1(d^{*})\) (Algorithm 4) \(\triangleright\)\(\mathcal{O}(n^{3})\) time
3: Run Phase 3: Freely orient remaining unoriented edges (Algorithm 5) \(\triangleright\)\(\mathcal{O}(n)\) time via DFS
4:return\(\hat{G}\) ```

**Algorithm 1** Algorithm for known skeleton and max in-degree.

There are three phases to our algorithm. In Phase 1, we orient strong v-structures. In Phase 2, we locally check if an edge is forced to orient one way or another to avoid incurring too much error. In Phase 3, we orient the remaining unoriented edges as a 1-polytree. Since the remaining edges were not forced, we may orient the remaining edges in an arbitrary direction (while not incurring "too much error") as long as the final incoming degrees of any vertex does not increase by more than 1. Subroutine Orient (Algorithm 2) performs the necessary updates when we orient \(u-v\) to \(u\to v\).

``` Input: Vertices \(u\) and \(v\) where \(u-v\) is currently unoriented
1: Orient \(u-v\) as \(u\to v\).
2: Update \(N^{in}(v)\) to \(N^{in}(v)\cup\{u\}\) and \(N^{un}(v)\) to \(N^{un}(v)\setminus\{u\}\).
3: Update \(N^{out}(u)\) to \(N^{out}(u)\cup\{v\}\) and \(N^{un}(u)\) to \(N^{un}(u)\setminus\{v\}\). ```

**Algorithm 2**Orient: Subroutine to orient edges

### Analysis

Observe that we perform \(\mathcal{O}(n^{d^{*}})\) (conditional) mutual information tests in Algorithm 1. The following lemma (Lemma 5) ensures us that _all_ our tests will behave as expected with sufficient samples. As such, in the rest of our analysis, we analyze under the assumption that our tests are correct.

**Lemma 5**.: _Suppose all variables in the Bayesian network has alphabet \(\Sigma\). For \(\varepsilon^{\prime}>0\), with_

\[m\in\mathcal{O}\left(\frac{|\Sigma|^{d^{*}+1}}{\varepsilon^{\prime}}\cdot\log \frac{|\Sigma|^{d^{*}+1}\cdot n^{d^{*}}}{\delta}\cdot\log\frac{|\Sigma|^{d^{*}+ 1}\cdot\log(n^{d^{*}}/\delta)}{\varepsilon^{\prime}}\right)\]

_empirical samples, \(\mathcal{O}(n^{d^{*}})\) statements of the following forms, where \(\mathbf{X}\) and \(\mathbf{Y}\) are variable sets of size \(|\mathbf{X}\ \dot{\cup}\ \mathbf{Y}|\leq d\) and \(Z\) is possibly \(\emptyset\), all succeed with probability at least \(1-\delta\):_

Figure 1: Running polytree example with \(d^{*}=3\) where \(I(a;b,c)=I(b;a,c)=I(c;a,b)=0\) due to the deg-3 v-structure centered at \(d\). Since \(I(a;f\mid d)=0\), Corollary 4 tells us that \(\hat{I}(a;f\mid d)\leq C\cdot\varepsilon\). Thus, we will _not_ detect \(a\to d\to f\) erroneously as a strong deg-2 v-structure \(a\to d\gets f\).

_(1) If \(I(\mathbf{X};\mathbf{Y}\mid Z)=0\), then \(\hat{I}(\mathbf{X};\mathbf{Y}\mid Z)<C\cdot\varepsilon^{\prime}\),_

_(2) If \(\hat{I}(\mathbf{X};\mathbf{Y}\mid Z)\geq C\cdot\varepsilon^{\prime}\), then \(I(\mathbf{X};\mathbf{Y}\mid Z)>0\),_

_(3) If \(\hat{I}(\mathbf{X};\mathbf{Y}\mid Z)\leq C\cdot\varepsilon^{\prime}\), then \(I(\mathbf{X};\mathbf{Y}\mid Z)<\varepsilon^{\prime}\)._

Proof.: Use Corollary 4 and apply union bound over \(\mathcal{O}(n^{d})\) tests. 

Recall that \(\pi(v)\) is the set of true parents of \(v\) in \(G^{*}\). Let \(H\) be the forest induced by the remaining unoriented edges after phase 2. Let \(\hat{G}\) be returned graph of the algorithm 1. Let us denote the final \(N^{in}(v)\) as \(\pi^{in}(v)\) at the end of Phase 2, just before freely orienting, i.e. the vertices pointing into \(v\) in \(\hat{G}\setminus H\). Let \(\pi^{un}(v)=\pi(v)\setminus\pi^{in}(v)\) be the set of ground truth parents that are not identified in Phase 1. Since the algorithm does not make mistakes for orientations in \(\hat{G}\setminus H\) (Lemma 6), all edges of in \(\pi^{un}(v)\) will be unoriented.

**Lemma 6**.: _Any oriented arc in \(\hat{G}\setminus H\) is a ground truth orientation. That is, any vertex parent set in \(\hat{G}\setminus H\) is a subset of \(\pi(v)\), i.e. \(\pi^{in}(v)\subseteq\pi(v)\), and \(N^{in}(v)\) at any time during the algorithm will have \(N^{in}(v)\subseteq\pi^{in}(v)\)._

Let \(\hat{\pi}(v)\) be the proposed parents of \(v\) output by Algorithm 1. The KL divergence between the true distribution and our output distribution is essentially \(\sum_{v\in V}I(v;\pi(v))-\sum_{v\in V}I(v;\hat{\pi}(v))\) as the structure independent terms will cancel out.

To get a bound on the KL divergence, we will upper bound \(\sum_{v\in V}I(v;\pi(v))\) and lower bound \(\sum_{v\in V}I(v;\hat{\pi}(v))\). To upper bound \(I(v;\pi(v))\) in terms of \(\pi^{in}(v)\subseteq\pi(v)\) and \(I(v;u)\) for \(u\in\pi^{un}(v)\), we use Lemma 8 which relies on repeated applications of Lemma 7. To lower bound \(\sum_{v\in V}I(v;\hat{\pi}(v))\), we use Lemma 9.

**Lemma 7**.: _Fix any vertex \(v\), any \(S\subseteq\pi^{un}(v)\), and any \(S^{\prime}\subseteq\pi^{in}(v)\). If \(S\neq\emptyset\), then there exists a vertex \(u\in S\cup S^{\prime}\) with_

\[I(v;S\cup S^{\prime})\leq I(v;S\cup S^{\prime}\setminus\{u\})+I(v;u)+ \varepsilon\;.\] (2)

**Lemma 8**.: _For any vertex \(v\) with \(\pi^{in}(v)\), we can show that_

\[I(v;\pi(v))\leq\varepsilon\cdot|\pi(v)|+I(v;\pi^{in}(v))+\sum_{u\in\pi^{un}(v )}I(v;u)\;.\]

**Algorithm 3** Phase 1: Orient strong v-structures

```
1:\(d\gets d^{*}\)
2:while\(d\geq 2\)do
3:for\(v\in V\)do\(\triangleright\) Arbitrary order
4: Let \(\mathcal{N}_{d}\subseteq 2^{N(v)}\) be the set of \(d\) neighbors of \(v\)\(\triangleright\)\(|\mathcal{N}_{d}|=\binom{|N(v)|}{d}\)
5:for\(S\in\mathcal{N}_{d}\) s.t. \(|S|=d\), \(|S\cup N^{in}(v)|\leq d^{*}\), and \(\hat{I}(u;S\setminus\{u\}\mid v)\geq C\cdot\varepsilon\), \(\forall u\in S\)do
6:for\(u\in S\)do\(\triangleright\) Strong deg-\(d\) v-structure
7: orient(\(u\), \(v\))\(\triangleright\) Decrement degree bound
8:\(d\gets d-1\) ```

**Algorithm 4** Phase 2: Orient strong v-structures

Figure 2: Suppose we have the following partially oriented graph in the execution of Algorithm 4 (after Phase 1). Since \(N^{in}(d)=\{a,b\}\), we will check the edge orientations of \(c-d\) and \(f-d\). Since \(I(f;\{a,b\}\mid d)=0\), we will have \(\hat{I}(f;\{a,b\}\mid d)\leq\varepsilon\), so we will _not_ erroneously orient \(f\to d\). Meanwhile, \(I(c;\{a,b\})=0\), we will have \(\hat{I}(c;\{a,b\})\leq\varepsilon\), so we will _not_ erroneously orient \(d\to c\).

In Phase 3, we increase the incoming edges to any vertex by at most one. The following lemma tells us that we lose at most4 an additive \(\varepsilon\) error per vertex.

Footnote 4: Orienting “freely” could also increase the mutual information score and this is considering the worst case.

**Lemma 9**.: _Consider an arbitrary vertex \(v\) with \(\pi^{in}(v)\) at the start of Phase 3. If Phase 3 orients \(u\to v\) for some \(u-v\in H\), then_

\[I(v;\pi^{in}(v)\cup\{u\})\geq I(v;\pi^{in}(v))+I(v;u)-\varepsilon.\]

By using Lemma 8 and Lemma 9, we can show our desired KL divergence bound (Lemma 10).

**Lemma 10**.: _Let \(\pi(v)\) be the true parents of \(v\). Let \(\hat{\pi}(v)\) be the proposed parents of \(v\) output by our algorithm. Then,_

\[\sum_{v\in V}I(v;\pi(v))-\sum_{v\in V}I(v;\hat{\pi}(v))\leq n\cdot(d^{*}+1) \cdot\varepsilon\;.\]

With these results in hand, we are ready to establish our main theorem:

Proof of Theorem 1.: We first combine Lemma 10 and Lemma 5 with \(\varepsilon^{\prime}=\frac{\varepsilon}{2n\cdot(d^{*}+1)}\) in order to obtain an orientation \(\hat{G}\) which is close to \(G^{*}\). Now, recall that there exist efficient algorithms for estimating the parameters of a Bayes net with in-degree-\(d\) (note that this includes \(d\)-polytrees) \(P\) once a close-enough graph \(\hat{G}\) is recovered (Dasgupta, 1997; Bhattacharyya et al., 2020), with sample complexity \(\tilde{\mathcal{O}}(|\Sigma|^{d}n/\varepsilon)\). Denote the final output \(\hat{P}_{\hat{G}}\), a distribution that is estimated using the conditional probabilities implied by \(\hat{G}\). One can bound the KL divergences as follows:

\[d_{\mathrm{KL}}(P\parallel P_{\hat{G}})-d_{\mathrm{KL}}(P\parallel P_{G^{*}}) \leq\varepsilon/2\quad\text{and}\quad d_{\mathrm{KL}}(P\parallel\hat{P}_{\hat {G}})-d_{\mathrm{KL}}(P\parallel P_{\hat{G}})\leq\varepsilon/2\;.\]

Thus, we see that

\[d_{\mathrm{KL}}(P\parallel\hat{P}_{\hat{G}})\leq\varepsilon+d_{\mathrm{KL}}(P \parallel P_{G^{*}})=\varepsilon\;.\]

## 4 Skeleton assumption

In this section, we present a set of _sufficient_ assumptions (Assumption 11) under which the Chow-Liu algorithm will recover the true skeleton even while with finite samples.

**Assumption 11**.: For any given distribution \(P\), there exists a constant \(\varepsilon_{P}>0\) such that:

(1) For every pair of nodes \(u\) and \(v\), if there exists a path \(u-\cdots-v\) of length greater than \(2\) in \(G^{*}\), then then \(I(u;v)+3\cdot\varepsilon_{P}\leq I(a;b)\) for every pair of adjacent vertices \(a-b\) in the path.

(2) For every pair of directly connected nodes \(a-b\) in \(G^{*}\), \(I(a;b)\geq 3\cdot\varepsilon_{P}\).

Suppose there is a large enough gap of \(\varepsilon_{P}\) between edges in \(G^{*}\) and edges outside of \(G^{*}\). Then, with \(\mathcal{O}(1/\varepsilon_{P}^{2})\) samples, each estimated mutual information \(\hat{I}(a;b)\) will be sufficiently close to the true mutual information \(I(a;b)\). Thus, running the Chow-Liu algorithm (which is essentially maximum spanning tree on the estimated mutual information on each pair of vertices) recovers \(\operatorname{skel}(G^{*})\).

**Lemma 12**.: _Under Assumption 11, running the Chow-Liu algorithm on the \(m\)-sample empirical estimates \(\{\hat{I}(u;v)\}_{u,v\in V}\) recovers a ground truth skeleton with high probability when \(m\geq\Omega(\frac{\log n}{\varepsilon_{P}^{2}})\)._

Combining Lemma 12 with our algorithm Algorithm 1, one can learn a polytree that is \(\varepsilon\)-close in KL with \(\tilde{\mathcal{O}}\left(\max\left\{\frac{\log(n)}{\varepsilon_{P}^{2}}, \frac{2^{d}\cdot n}{\varepsilon}\right\}\right)\) samples, where \(\varepsilon_{P}\) depends on the distribution \(P\).

## 5 Lower bound

In this section, we show that \(\Omega(n/\varepsilon)\) samples are necessary _even when a known skeleton is provided_. For constant in-degree \(d\), this shows that our proposed algorithm in Section 3 is sample-optimal up to logarithmic factors.

We first begin by showing a lower bound of \(\Omega(1/\varepsilon)\) on a graph with three vertices, even when the skeleton is given. Let \(G_{1}\) be \(X\to Z\to Y\) and \(G_{2}\) be \(X\to Z\gets Y\), such that \(\operatorname{skel}(G_{1})=\operatorname{skel}(G_{2})\) is \(X-Z-Y\). Now define \(P_{1}\) and \(P_{2}\) as follows:

Figure 4: The five different possible orientations of \(H\). Observe that the ground truth orientation of these edges is inconsistent with all five orientations shown here.

Figure 3: Consider the partially oriented graph before the final phase, where \(H\) is the edge-induced subgraph on the unoriented edges in red. Since \(d^{*}=3\) is known, we can conclude that \(g\to i\) was oriented due to a local search step and not due to Meek \(R1(3)\). We have the following sets before the final phase: \(\pi^{in}(c)=\{a,b\}\), \(\pi^{in}(g)=\{f,j\}\), \(\pi^{i}=\{g\}\), \(\pi^{un}(d)=\{c\}\), \(\pi^{un}(f)=\{d,e\}\), and \(\pi^{un}(e)=\{h\}\). With respect to the chosen orientation of \(H\) and the notation in Lemma 10, we have \(A=\{c,d,f,e,h\}\), \(a_{c}=d\), \(a_{d}=f\), \(a_{f}=e\), and \(a_{e}=h\). Observe that the \(\pi^{un}\)’s and \(a\)’s are two different ways to refer to the set of red edges of \(H\).

\[P_{1}:\begin{cases}X\sim\operatorname{Bern}(1/2)\\ Z=\begin{cases}X&\text{w.p. 1/2}\\ \operatorname{Bern}(1/2)&\text{w.p. 1/2}\\ Y=\begin{cases}Z&\text{w.p. }\sqrt{\varepsilon}\\ \operatorname{Bern}(1/2)&\text{w.p. }1-\sqrt{\varepsilon}\end{cases} \end{cases}P_{2}:\begin{cases}X\sim\operatorname{Bern}(1/2)\\ Y\sim\operatorname{Bern}(1/2)\\ Z=\begin{cases}X&\text{w.p. 1/2}\\ Y&\text{w.p. }\sqrt{\varepsilon}\\ \operatorname{Bern}(1/2)&\text{w.p. }1/2-\sqrt{\varepsilon}\end{cases}\end{cases}\] (3)

The intuition is that we keep the edge \(X\to Z\) "roughly the same" and tweak the edge \(Y-Z\) between the distributions. We define \(P_{i,G}\) as projecting \(P_{i}\) onto \(G\). One can check that the following holds (see Supplemental for the detailed calculations):

**Lemma 13**.: _Let \(G_{1}\) be \(X\to Z\to Y\) and \(G_{2}\) be \(X\to Z\gets Y\), such that \(\operatorname{skel}(G_{1})=\operatorname{skel}(G_{2})\) is \(X-Z-Y\). With respect to Eq. (3), we have the following:_

1. \(d_{\mathrm{H}}^{2}(P_{1},P_{2})\in\mathcal{O}(\varepsilon)\)__
2. \(d_{\mathrm{KL}}(P_{1}\parallel P_{1,G_{1}})=0\) _and_ \(d_{\mathrm{KL}}(P_{1}\parallel P_{1,G_{2}})\in\Omega(\varepsilon)\)__
3. \(d_{\mathrm{KL}}(P_{2}\parallel P_{2,G_{2}})=0\) _and_ \(d_{\mathrm{KL}}(P_{2}\parallel P_{2,G_{1}})\in\Omega(\varepsilon)\)__

Our hardness result (Lemma 14) is obtained by reducing the problem of finding an \(\varepsilon\)-close graph orientation of \(X-Z-Y\) to the problem of _testing_ whether the samples are drawn from \(P_{1}\) or \(P_{2}\): To ensure \(\varepsilon\)-closeness in the graph orientation, one has to correctly determine whether the samples come from \(P_{1}\) or \(P_{2}\) and then pick \(G_{1}\) or \(G_{2}\) respectively. However, it is well-known that distinguishing two distributions whose squared Hellinger distance is \(\varepsilon\) requires \(\Omega(1/\varepsilon)\) samples (see, e.g., [1, Theorem 4.7]).

**Lemma 14**.: _Even when given \(\operatorname{skel}(G^{*})\), it takes \(\Omega(1/\varepsilon)\) samples to learn an \(\varepsilon\)-close graph orientation of \(G^{*}\) for distributions on \(\{0,1\}^{3}\)._

Using the above construction as a gadget, we can obtain a dependency on \(n\) in our lower bound by constructing \(n/3\) independent copies of the above gadget, a la proof strategy of Bhattacharyya et al. [2021, Theorem 7.6]. For some constant \(c>0\), we know that a constant \(1/c\) fraction of the gadgets will incur an error or more than \(\varepsilon/n\) if less than \(cn/\varepsilon\) samples are used. The desired result then follows from the tensorization of KL divergence, i.e., \(d_{\mathrm{KL}}\left(\prod_{i}P_{i}\parallel\prod_{i}Q_{i}\right)=\sum_{i}d_{ \mathrm{KL}}(P_{i}\parallel Q_{i})\).

**Theorem 15**.: _Even when given \(\operatorname{skel}(G^{*})\), it takes \(\Omega(n/\varepsilon)\) samples to learn an \(\varepsilon\)-close graph orientation of \(G^{*}\) for distributions on \(\{0,1\}^{n}\)._

## 6 Conclusion

In this work, we studied the problem of estimating a distribution defined on a \(d\)-polytree \(P\) with graph structure \(G^{*}\) using finite observational samples. We designed and analyzed an efficient algorithm that produces an estimate \(\hat{P}\) such that \(d_{\mathrm{KL}}(P\parallel\hat{P})\leq\varepsilon\) assuming access to \(\operatorname{skel}(G^{*})\) and \(d\). The skeleton \(\operatorname{skel}(G^{*})\) is recoverable under Assumption 11 and we show that there is an inherent hardness in the learning problem even under the assumption that \(\operatorname{skel}(G^{*})\) is given. For constant \(d\), our hardness result shows that our proposed algorithm is sample-optimal up to logarithmic factors.

An interesting open question is whether one can extend the hardness result to arbitrary \(d\geq 1\), or design more efficient learning algorithms for \(d\)-polytrees.

## References

* Acharya et al. [2018] J. Acharya, A. Bhattacharyya, C. Daskalakis, and S. Kandasamy. Learning and testing causal models with interventions. In _NeurIPS_, pages 9469-9481, 2018.
* Aragam et al. [2019] B. Aragam, A. Amini, and Q. Zhou. Globally optimal score-based learning of directed acyclic graphs in high-dimensions. _Advances in Neural Information Processing Systems_, 32, 2019.
* Bar-Yossef [2002] Z. Bar-Yossef. _The Complexity of Massive Data Set Computations_. PhD thesis, UC Berkeley, 2002. Adviser: Christos Papadimitriou. Available at http://webee.technion.ac.il/people/zivby/index_files/Page1489.html.

A. Bhattacharyya, S. Gayen, K. S. Meel, and N. V. Vinodchandran. Efficient distance approximation for structured high-dimensional distributions via learning. In _NeurIPS_, 2020.
* Bhattacharyya et al. [2021] A. Bhattacharyya, S. Gayen, E. Price, and N. V. Vinodchandran. Near-optimal learning of tree-structured distributions by Chow-Liu. In _STOC '21--Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 147-160. ACM, New York, 2021. doi: 10.1145/3406325.3451066. URL https://doi.org/10.1145/3406325.3451066.
* Canonne et al. [2020] C. L. Canonne, I. Diakonikolas, D. M. Kane, and A. Stewart. Testing bayesian networks. _IEEE Trans. Inf. Theory_, 66(5):3132-3170, 2020. Preprint available at arXiv:1612.03156.
* Chickering [2002] D. M. Chickering. Optimal structure identification with greedy search. _Journal of machine learning research_, 3(Nov):507-554, 2002.
* Chickering and Meek [2002] D. M. Chickering and C. Meek. Finding optimal bayesian networks. In _Proceedings of the Eighteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI 02)_, pages 94-102, 2002.
* Chickering et al. [2004] D. M. Chickering, D. Heckerman, and C. Meek. Large-sample learning of bayesian networks is np-hard. _J. Mach. Learn. Res._, 5:1287-1330, 2004.
* Chow and Liu [1968] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. _IEEE Trans. Inf. Theory_, 14(3):462-467, 1968.
* Dasgupta [1997] S. Dasgupta. The sample complexity of learning fixed-structure bayesian networks. _Mach. Learn._, 29(2-3):165-180, 1997.
* Dasgupta [1999] S. Dasgupta. Learning polytrees. In _UAI_, pages 134-141. Morgan Kaufmann, 1999.
* Friedman and Yakhini [1996] N. Friedman and Z. Yakhini. On the sample complexity of learning bayesian networks. In _Proceedings of the Twelfth Annual Conference on Uncertainty in Articial Intelligence (UAI \(\{96\}\)), San Francisco, CA_, pages 274-282, 1996.
* Friedman et al. [2013] N. Friedman, I. Nachman, and D. Pe'er. Learning bayesian network structure from massive datasets: The" sparse candidate" algorithm. _arXiv preprint arXiv:1301.6696_, 2013.
* Gao and Aragam [2021] M. Gao and B. Aragam. Efficient bayesian network structure learning via local markov boundary search. _Advances in Neural Information Processing Systems_, 34:4301-4313, 2021.
* Geiger et al. [1990] D. Geiger, A. Paz, and J. Pearl. Learning causal trees from dependence information. In _AAAI_, pages 770-776. AAAI Press / The MIT Press, 1990.
* Ghoshal and Honorio [2017] A. Ghoshal and J. Honorio. Learning identifiable gaussian bayesian networks in polynomial time and sample complexity. _Advances in Neural Information Processing Systems_, 30, 2017.
* Hoffgen [1993] K.-U. Hoffgen. Learning and robust learning of product distributions. In _Proceedings of the sixth annual conference on Computational learning theory_, pages 77-83, 1993.
* Hoyer et al. [2008] P. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Scholkopf. Nonlinear causal discovery with additive noise models. _Advances in Neural Information Processing Systems_, 21, 2008.
* Jensen and Nielsen [2007] F. V. Jensen and T. D. Nielsen. _Bayesian networks and decision graphs_, volume 2. Springer, 2007.
* Principles and Techniques_. MIT Press, 2009.
* Meek [1995] C. Meek. Causal Inference and Causal Explanation with Background Knowledge. In _Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence_, UAI'95, page 403-410, San Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1558603859.
* Park and Raskutti [2017] G. Park and G. Raskutti. Learning quadratic variance function (qvf) dag models via overdispersion scoring (ods). _J. Mach. Learn. Res._, 18:224-1, 2017.
* Peters and Buhlmann [2014] J. Peters and P. Buhlmann. Identifiability of gaussian structural equation models with equal error variances. _Biometrika_, 101(1):219-228, 2014.
* Park et al. [2017]* Rebane and Pearl (1988) G. Rebane and J. Pearl. The recovery of causal poly-trees from statistical data. _Int. J. Approx. Reason._, 2(3):341, 1988.
* Shimizu et al. (2006) S. Shimizu, P. O. Hoyer, A. Hyvarinen, A. Kerminen, and M. Jordan. A linear non-gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7(10), 2006.
* Spirtes and Glymour (1991) P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. _Social science computer review_, 9(1):62-72, 1991.
* Valiant (1984) L. G. Valiant. A theory of the learnable. In _STOC_, pages 436-445. ACM, 1984.