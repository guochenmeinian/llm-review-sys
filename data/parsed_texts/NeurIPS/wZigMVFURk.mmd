# RoPINN: Region Optimized

Physics-Informed Neural Networks

 Haixu Wu, Huakun Luo, Yuezhou Ma, Jianmin Wang, Mingsheng Long\({}^{\text{\textcircled{3}}}\)

School of Software, BNRist, Tsinghua University, China

{wuhr23,luohk19,mayz20}@mails.tsinghua.edu.cn,{jimwang,mingsheng}@tsinghua.edu.cn

###### Abstract

Physics-informed neural networks (PINNs) have been widely applied to solve partial differential equations (PDEs) by enforcing outputs and gradients of deep models to satisfy target equations. Due to the limitation of numerical computation, PINNs are conventionally optimized on finite selected points. However, since PDEs are usually defined on continuous domains, solely optimizing models on scattered points may be insufficient to obtain an accurate solution for the whole domain. To mitigate this inherent deficiency of the default scatter-point optimization, this paper proposes and theoretically studies a new training paradigm as _region optimization_. Concretely, we propose to extend the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs. A practical training algorithm, **R**egion **O**ptimized **PINN** (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method. By calibrating the sampling process into trust regions, RoPINN finely balances optimization and generalization error. Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation. Code is available at this repository: https://github.com/thuml/RoPINN.

## 1 Introduction

Solving partial differential equations (PDEs) is the key problem in extensive areas, covering both engineering and scientific research [38; 40; 49]. Due to the inherent complexity of PDEs, they usually cannot be solved analytically [10]. Thus, a series of numerical methods have been widely explored, such as spectral methods [22; 40] or finite element methods [5; 7]. However, these numerical methods usually suffer from huge computational costs and can only obtain an approximate solution on discretized meshes [26; 43]. Given the impressive nonlinear modeling capability of deep models [4; 14], they have also been applied to solve PDEs, where physics-informed neural networks (PINNs) are proposed and have emerged as a promising and effective surrogate tool for numerical methods [44; 36; 35]. By formalizing PDE constraints (i.e. equations, initial and boundary conditions) as objective functions, the outputs and gradients of PINNs will be optimized to satisfy a certain PDE during training [36], which successfully instantiates the PDE solution as a deep model.

Although deep models have been proven to enjoy the universal approximation capability, the actual optimization process of PINNs still faces thorny challenges [8; 24; 35]. As a basic topic of PINNs, the optimization problem has been widely explored from various aspects [17; 44]. Previous methods attempt to mitigate this problem by using novel architectures to enhance model capacity [58; 3; 50], reweighting multiple loss functions for more balanced convergence [47], resampling data to improve important areas [51] or developing new optimizers to tackle the rough loss landscape [55; 37], etc.

Orthogonal to the above-mentioned methods, this paper focuses on a foundational problem, which is the objective function of PINNs. We notice that, due to the limitation of numerical calculation, it is almost impossible to optimize the loss function in the complete continuous domain. Thus, the conventional PINN loss is only defined on a series of selected points [36] (Figure 1). However, the scatter-point loss function obviously mismatches the PDE-solving objective, which is approximating the solution on a continuous domain. This mismatch may fundamentally limit the performance of PINNs. Several prior works also try to improve the canonical PINN loss function, which can be roughly categorized into the following two paradigms. One paradigm enhances the optimization by adding high-order derivatives of PDEs as a regularization term to the loss function [55]. However, calculating high-order gradients is numerically unstable and time-consuming, even with automatic differentiation in well-established deep learning frameworks [2; 34]. The other paradigm attempts to bypass the high-order derivative calculation in the PINN loss function with variational formulations [18; 19; 20]. Nevertheless, these variational methods still face difficulties in calculating the integral of deep models and will bring extra computations, thereby mainly limited to very shallow models or relying on massive sampled quadrature points and elaborative test functions [11; 57].

This paper proposes and studies a new training paradigm for PINNs as _region optimization_. As shown in Figure 1, we extend the optimization process from selected scatter points into their neighborhood regions, which can theoretically decrease the generalization error on the whole domain, especially for hidden high-order constraints of PDEs. In practice, we seamlessly transform this paradigm into a practical training algorithm, named **R**egion **O**ptimized **PINN** (RoPINN), which is implemented through simple but effective Monte Carlo sampling. In addition, to control the estimation error, we adaptively adjust the sampling region size according to the gradient variance among successive training iterations, which can constrain the sampling-based optimization into a neighborhood with low-variance loss gradients, namely _trust region_. In experiments, RoPINN demonstrates consistent and sharp improvement for diverse PINN backbones on extensive PDEs (19 different tasks) without any extra gradient calculation. Our contributions are summarized as follows:

* To mitigate the inherent deficiency of conventional PINN optimization, we propose the _region optimization_ paradigm, which extends the scatter-point optimization to neighborhood regions that theoretically benefits both generalization and high-order constraints satisfaction.
* We present RoPINN for PINN training based on Monte Carlo sampling, which can effectively accomplish the region optimization. A trust region calibration strategy is proposed to reduce the gradient estimation error caused by sampling for more trustworthy optimization.
* RoPINN can consistently improve the performance of various PINN backbones (i.e. canonical and Transformer-based) on a wide range of PDEs without extra gradient calculation.

## 2 Preliminaries

A PDE with equation constraints, initial (ICs) and boundary conditions (BCs) can be formalized as

\[\mathcal{F}(u)(\bm{x})=0,\bm{x}\in\Omega;\;\mathcal{I}(u)(\bm{x})=0,\bm{x}\in \Omega_{0};\;\mathcal{B}(u)(\bm{x})=0,\bm{x}\in\partial\Omega,\] (1)

where \(\mathcal{F},\mathcal{I},\mathcal{B}\) denote the PDE equations, ICs and BCs respectively [6]. \(u:\mathbb{R}^{d+1}\rightarrow\mathbb{R}^{m}\) is the target PDE solution. \(\bm{x}\in\Omega\subseteq\mathbb{R}^{d+1}\) represents the input coordinate, which is usually a composition of spatial and temporal positions, namely \(\bm{x}=(x_{1},\cdots,x_{d},t)\). \(\Omega_{0}\) corresponds to the \(t=0\) situation.

Figure 1: Comparison between previous methods and ours. Previous point optimization methods train PINNs via the loss on selected points, which is different from our region optimization paradigm.

Correspondingly, the PINN loss function (_point optimization_) is typically defined as follows [17; 36]:

\[\mathcal{L}(u_{\theta})= \frac{\lambda_{\Omega}}{N_{\Omega}}\sum_{i=1}^{N_{\Omega}}\| \mathcal{F}(u_{\theta})(\bm{x}_{i})\|^{2}+\frac{\lambda_{\Omega_{0}}}{N_{ \Omega_{0}}}\sum_{i=1}^{N_{\Omega_{0}}}\|\mathcal{I}(u_{\theta})(\bm{x}_{i})\|^ {2}+\frac{\lambda_{\partial\Omega}}{N_{\partial\Omega}}\sum_{i=1}^{N_{\partial \Omega}}\|\mathcal{B}(u_{\theta})(\bm{x}_{i})\|^{2},\] (2)

where \(u_{\theta}\) represents the neural network parameterized by \(\theta\). \(N_{\Omega},N_{\Omega_{0}},N_{\partial\Omega}\) are the numbers of sampled points in \(\Omega,\Omega_{0},\partial\Omega\) respectively. \(\lambda_{*}\) is the corresponding loss weight. Note that there is an additional data loss term in Eq. (2) when we can access the ground truth of some points [36]. Since we mainly focus on PDE constraints throughout this paper, we omit the data loss term in the above formalization, which is still maintained in our experiments. In this paper, we try to improve PINN solving by defining a new surrogate loss in place of the canonical definition of PINN loss in Eq. (2). In contrast, the relevant literature mainly improves the objective function in two different directions as follows. Appendix F provides a more comprehensive discussion on other relative topics.

High-order regularizationThe first direction is to add the high-order constraints of PDEs as regularization terms to the loss function [55]. Specifically, since PDEs are sets of identical relations, suppose that the solution \(u\) is a \(K\)-order differential function, Eq. (1) can naturally derive a branch of high-order equations, where the \(k\)-th derivative for the \(j\)-th dimension is \(\frac{\partial^{k}}{\partial x_{j}^{j}}\mathcal{F}(u)(\bm{x})=0,\)\(\bm{x}\in\Omega,1\leq j\leq(d+1),1\leq k\leq K\), corresponding to the following regularization:

\[\mathcal{L}_{k,j}^{\text{reg}}(u_{\theta})=\frac{\lambda_{k,j}}{N_{k,j}}\sum_ {i=1}^{N_{k,j}}\left\|\frac{\partial^{k}}{\partial x_{j}^{j}}\mathcal{F}(u_{ \theta})(\bm{x}_{i})\right\|^{2},\] (3)

where \(N_{k,j}\) denotes the number of sampled points with weight \(\lambda_{k,j}\). Although this design can explicitly enhance the model performance in satisfying high-order constraints, the calculation of high-order derivatives can be extremely time-consuming and unstable [39]. Thus, in practice, the previous methods [55; 31] only consider a small value of \(K\). In the next sections, we will prove that RoPINN can naturally incorporate high-order constraints. Besides, as presented in Eq. 3, this paradigm still optimizes PINNs on scattered points, while this paper extends optimization to neighborhood regions.

Variational formulationAs a classical tool in traditional PDE solvers, the variational formulation is widely used to reduce the smoothness requirements of the approximate solution [42]. Concretely, the target PDEs are multiplied with a set of predefined test functions \(\{v_{1},\cdots,v_{M}\}\) and then the PDE equation term of the loss function is transformed as follows [18; 19; 20]:

\[\mathcal{L}^{\text{equ}}(u_{\theta})=\frac{1}{M}\sum_{k=1}^{M}\left\|\left< \mathcal{F}^{(x_{j})}(u_{\theta})(\bm{x}),v_{k}(\bm{x})\right>\right|_{\partial _{(x_{j})}\Omega}-\int_{\Omega}\left<\mathcal{F}^{(x_{j})}(u_{\theta})(\bm{x}), \frac{\partial}{\partial x_{j}}v_{k}(\bm{x})\right>\!\!\mathrm{d}\bm{x}\!\right\| ^{2},\] (4)

where \(\mathcal{F}^{(x_{j})}\) defines the antiderivative of \(\mathcal{F}\) on the \(j\)-th dimension. Using integrals by parts, the derivative operation in \(\mathcal{F}\) is transferred to test functions \(\{v_{k}\}_{k=1}^{M}\), thereby able to bypass high-order derivatives. However, the integral on \(\Omega\) is still hard to compute, which requires massive quadrature points for approximation [18]. Besides, test function selection requires extra manual effort and will bring \(M\) times computation costs [57]. In contrast, RoPINN does not require test functions and will not bring extra gradient calculations. Also, RoPINN employs a trust region calibration strategy to limit the optimization in low-variance regions, which can control the estimation error of sampling.

## 3 Method

As aforementioned, we propose the region optimization paradigm to extend the optimization from scatter points to a series of corresponding neighborhood regions. This section will first present the region optimization and its theoretical benefits in both reducing generalization error and satisfying high-order PDE constraints. Then, we implement RoPINN in a simple but effective sampling-based way, along with a trust region calibration strategy to control the sampling estimation error.

### Region Optimization

For clarity, we record the point optimization loss defined in Eq. (2) at \(\bm{x}\) as \(\mathcal{L}(u_{\theta},\bm{x})\), where \(\bm{x}\in\Omega\cup\Omega_{0}\cup\partial\Omega\) denotes the point selected from inner domain, initial state or boundaries. We adopt \(\mathcal{S}\) to denote the finite set of selected points. Then Eq. (2) can be simplified as \(\mathcal{L}(u_{\theta},\mathcal{S})=\frac{1}{|\mathcal{S}|}\sum_{\bm{x}\in \mathcal{S}}\mathcal{L}(u_{\theta},\bm{x})\).

Correspondingly, we define the objective function of our _region optimization_ innovatively as

\[\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta},\mathcal{S})=\frac{1}{|\mathcal{S}|} \sum_{\bm{x}\in\mathcal{S}}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta},\bm{x}) =\frac{1}{|\Omega_{r}|\times|\mathcal{S}|}\sum_{\bm{x}\in\mathcal{S}}\int_{ \Omega_{r}}\mathcal{L}(u_{\theta},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi},\] (5)

where \(\Omega_{r}=[0,r]^{(d+1)}\) represents the extended neighborhood region with hyperparameter \(r\). Although this definition seems to require more sampling points than point optimization, we can develop an efficient algorithm to implement it without adding sampling points (see next section). Besides, this formalization also provides us with a convenient theoretical analysis framework. Next, we will discuss the theoretical properties of the two optimization paradigms. All proofs are in Appendix A.

Generalization boundHere we discuss the _generalization error in expectation_[13], which is independent of the point selection, thereby quantifying the error of PINN optimization more rigorously.

**Definition 3.1**.: _The generalization error in expectation of model trained on dataset \(\mathcal{S}\) is defined as_

\[\mathcal{E}_{\mathrm{gen}}=\big{|}\mathbb{E}_{\mathcal{S},\mathcal{A}}\left[ \mathcal{L}\left(u_{\mathcal{A}(\mathcal{S})},\Omega\right)-\mathcal{L}\left( u_{\mathcal{A}(\mathcal{S})},\mathcal{S}\right)\right]\big{|},\] (6)

_where \(\mathcal{A}\) denotes the training algorithm and \(\mathcal{A}(\mathcal{S})\) represents the optimized model parameters._

**Assumption 3.2**.: _The loss function \(\mathcal{L}\) is \(L\)-Lipschitz and \(\beta\)-smooth with respect to model parameters, which means that \(\forall\bm{x}\in\Omega\) the following inequalities hold:_

\[\|\mathcal{L}(u_{\theta_{1}},\bm{x})-\mathcal{L}(u_{\theta_{2}},\bm{x})\|\leq L \|\theta_{1}-\theta_{2}\|,\;\|\nabla_{\theta}\mathcal{L}(u_{\theta_{1}},\bm{x })-\nabla_{\theta}\mathcal{L}(u_{\theta_{2}},\bm{x})\|\leq\beta\|\theta_{1}- \theta_{2}\|.\] (7)

**Theorem 3.3** (**Point optimization**).: _Suppose that the loss function \(\mathcal{L}\) is \(L\)-Lipschitz-\(\beta\)-smooth for \(\theta\). If we run stochastic gradient descent with step size \(\alpha_{t}\) at the \(t\)-th step for \(T\) iterations, we have that: (1) If \(\mathcal{L}\) is convex for \(\theta\) and \(\alpha_{t}\leq\frac{2}{\beta}\), then \(\mathcal{E}_{\mathrm{gen}}\leq\frac{2L^{2}}{|\mathcal{S}|}\sum_{t=1}^{T} \alpha_{t}\) (proved by [13, 52]). (2) If \(\mathcal{L}\) is bounded by a constant \(C\) for all \(\theta,\bm{x}\) and is non-convex for \(\theta\) with monotonically non-increasing step sizes \(\alpha_{t}\leq\frac{1}{\beta t}\), then \(\mathcal{E}_{\mathrm{gen}}\leq\frac{C}{|\mathcal{S}|}+\frac{2L^{2}(T-1)}{\beta (|\mathcal{S}|-1)}\) (tighter bound than [13, 52])._

**Lemma 3.4**.: _If \(\mathcal{L}\) is bounded for all \(\theta,\bm{x}\) and is convex, \(L\)-Lipschitz-\(\beta\)-smooth with respect to model parameters \(\theta\), then \(\mathcal{L}_{r}^{\mathrm{region}}\) is also bounded for all \(\theta,\bm{x}\) and convex, \(L\)-Lipschitz-\(\beta\)-smooth for \(\theta\)._

**Theorem 3.5** (**Region optimization**).: _Suppose that the point optimization loss function \(\mathcal{L}\) is \(L\)-Lipschitz and \(\beta\)-smooth for \(\theta\). If we run stochastic gradient descent with step size \(\alpha_{t}\) for \(T\) iterations based on region optimization loss \(\mathcal{L}_{r}^{\mathrm{region}}\) in Eq. (5), the generalization error in expectation satisfies: (1) If \(\mathcal{L}\) is convex for \(\theta\) and \(\alpha_{t}\leq\frac{2}{\beta}\), then \(\mathcal{E}_{\mathrm{gen}}\leq(1-\frac{|\Omega_{r}|}{|\Omega|})\frac{2L^{2}}{| \mathcal{S}|}\sum_{t=1}^{T}\alpha_{t}\). (2) If \(\mathcal{L}\) is bounded by a constant \(C\) for all \(\theta,\bm{x}\) and is non-convex for \(\theta\) with monotonically non-increasing step sizes \(\alpha_{t}\leq\frac{1}{\beta t}\), then \(\mathcal{E}_{\mathrm{gen}}\leq\frac{C}{|\mathcal{S}|}+\frac{2L^{2}(T-1)}{\beta (|\mathcal{S}|-1)}-JL(\frac{|\Omega_{r}|}{|\Omega|})^{2}\), where \(J\) is a finite number that depends on the training property at the several beginning iterations._

Proof.: Based on the Lipschitz assumption, \(\mathcal{E}_{\mathrm{gen}}\) can be bounded by a term relating to the expectation of distance between parameter \(\theta\) optimized from different training sets. The region optimization paradigm brings a more "consistent" gradient optimization direction than point optimization at each iteration, thereby benefiting the generalization property. See Appendix A.3 for complete proof. 

From Theorems 3.3 and 3.5, we can observe that region optimization can reduce the generalization error \(\mathcal{E}_{\mathrm{gen}}\). Furthermore, the region optimization theorem also provides a more general theoretical framework. For example, the conventional point optimization is equivalent to the case of \(\Omega_{r}=0\), where only one single point is selected for each region. For another extreme case, enlarging the region size to the whole domain (i.e. \(\Omega_{r}=\Omega\)), Eq. (5) is equivalent to directly optimizing the loss defined on \(\Omega\), where the generalization error will be reduced to zero. Unfortunately, this ideal situation cannot be satisfied in practice, since Eq. (5) requires precise calculation of the integral over the whole domain. More discussions of practical implementation are deferred to the next section.

High-order PDE constraintsIn our proposed region optimization (Eq. (5)), the integral operation on the input domain can also relax the smoothness requirements of the loss function \(\mathcal{L}\). For example, without any additional assumption of the smoothness of \(\mathcal{L}(u_{\theta},\bm{x})\) on \(\bm{x}\), we can directly derive the generalization error for the first-order loss \(\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta},\bm{ x})\) on the \(j\)-th dimension as follows.

**Corollary 3.6** (**Region optimization for first-order constraints)**.: _Suppose that \(\mathcal{L}\) is bounded by \(C\) for all \(\theta,\bm{x}\) and is \(L\)-Lipschitz and \(\beta\)-smooth for \(\theta\). If we run stochastic gradient method based on first-order \(j\)-th dimension loss function \(\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}\) for \(T\) iterations, the generalization error in Theorem 3.5(2) still holds when we adopt the monotonically non-increasing step size \(\alpha_{t}\leq\frac{1}{2\beta t}\)._

Corollary 3.6 implies that the integral on the input domain in region optimization can help training PINNs with high-order constraints, which is valuable for high-order PDEs, such as wave equations. In contrast, this valuable property cannot be achieved by the classic point optimization. See Example 3.7.

**Example 3.7** (**Point optimization fails in optimizing with first-order constraints**).: _Under the same assumption with Corollary 3.6, we cannot obtain the Lipschitz and smoothness property of \(\frac{\partial}{\partial x_{j}}\mathcal{L}(u_{\theta},\bm{x})\). For example, suppose that \(\mathcal{L}(u_{\theta},\bm{x})=|\theta^{\mathsf{T}}\sqrt{\bm{x}}|,\bm{x}\in[0,1]^{(d+1)}\), which is 1-Lipschitz-1-smooth. However, \(\nabla_{\theta}\frac{\partial}{\partial x_{j}}\mathcal{L}(u_{\theta},\bm{x})\) is unbounded when \(\bm{x}\to\bm{0}\), thereby not Lipschitz constant._

### Practical Algorithm

Derived from our theoretical insights of region optimization, we implement RoPINN as a practical training algorithm. As elaborated in Algorithm 1, RoPINN involves the following two iterative steps: Monte Carlo approximation and trust region calibration, where the former can efficiently approximate the optimization objective and the latter can effectively control the estimation error. Next, we will discuss the details and convergence properties of RoPINN. All proofs can be found in Appendix B.

Monte Carlo approximationNote that the region integral in Eq. (5) cannot be directly calculated, so we adopt a straightforward implementation based on the Monte Carlo approximation. Concretely, to approximate the gradient descent on the region loss \(\mathcal{L}_{r}^{\mathrm{region}}\), we uniformly sample one point within the region \(\Omega_{r}\) for the gradient descent at each iteration, whose expectation is equal to the gradient descent of the original region optimization in Eq. (5):

\[\mathbb{E}_{\bm{\xi}\sim U(\Omega_{r})}\left[\nabla_{\theta}\mathcal{L}(u_{ \theta},\bm{x}+\bm{\xi})\right]=\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{ region}}(u_{\theta},\bm{x}).\] (8)

In addition to efficiently approximating region optimization without adding sampling points, our proposed sampling-based strategy is also equivalent to a high-order loss function, especially for the first-order term, which is essential in practice [55]. Concretely, with Taylor expansion, we have that:

\[\mathbb{E}_{\bm{\xi}\sim U(\Omega_{r})}\big{(}\nabla_{\theta}\mathcal{L}(u_{ \theta},\bm{x}+\bm{\xi})\big{)}=\mathbb{E}_{\bm{\xi}\sim U(\Omega_{r})}\big{(} \nabla_{\theta}\mathcal{L}(u_{\theta},\bm{x})+\nabla_{\theta}(\bm{\xi}^{ \mathsf{T}}\mathcal{L}_{1}(u_{\theta},\bm{x}))+\mathcal{O}(\|\bm{\xi}\|^{2}) \big{)},\] (9)

where \(\Omega_{r}=[0,r]^{d+1}\), and \(\mathcal{L}_{1}\) represents the first order of loss function, namely \(\frac{\partial}{\partial\bm{x}}\mathcal{L}(u_{\theta},\bm{x})\).

**Theorem 3.8** (**Convergence rate)**.: _Suppose that there exists a constant \(H\), s.t. \(\forall\bm{v}\) and \(\forall\bm{x}\in\Omega\), \(\big{|}\bm{v}^{\mathsf{T}}\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_ {\theta},\bm{x})\bm{v}\big{|}\leq H\|\bm{v}\|^{2}\). If the step size \(\alpha_{t}=\frac{1}{\sqrt{t+1}}\) decreases over time for \(T\) iterations, the region optimization based on Monte Carlo approximation will converge at the speed of_

\[\mathbb{E}\left[\big{\|}\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_{ \theta},\bm{x})\big{\|}^{2}\right]\leq\mathcal{O}\left(\frac{1}{\sqrt{T}} \right).\] (10)

**Theorem 3.9** (**Gradient estimation error)**.: _The estimation error of gradient descent between Monte Carlo approximation and the original region optimization satisfies:_

\[\mathbb{E}_{\boldsymbol{\xi}\sim U(\Omega_{r})}\left[\left\|\nabla_{\theta} \mathcal{L}(u_{\theta},\boldsymbol{x}+\boldsymbol{\xi})-\nabla_{\theta} \mathcal{L}_{r}^{\mathrm{region}}(u_{\theta},\boldsymbol{x})\right\|^{2} \right]^{\frac{1}{2}}=\left\|\sigma_{\boldsymbol{\xi}\sim U(\Omega_{r})}\left( \nabla_{\theta}\mathcal{L}(u_{\theta},\boldsymbol{x}+\boldsymbol{\xi})\right) \right\|,\] (11)

_where \(\sigma\) represents the standard deviation of gradients in region \(\Omega_{r}\)._

Trust region calibrationAlthough the expectation of the Monte Carlo sampling is equal to region optimization as shown in Eq. (8), this design will also bring estimation error in practice (Theorem 3.9). A large estimation error will cause unstable training and further affect convergence. To ensure a reliable gradient descent, we propose to control the sampling region size \(r\) towards a trustworthy value, namely _trust region calibration_. Unlike the notion in optimization [56], here _trust region_ is used to define the area of input domain where the variance of loss gradients for different points is relatively small. Formally, we adjust the region size in inverse proportion to gradient variance:

\[r\propto\frac{1}{\left\|\sigma_{\boldsymbol{\xi}\sim U(\Omega_{r})}\left( \nabla_{\theta}\mathcal{L}(u_{\theta},\boldsymbol{x}+\boldsymbol{\xi}) \right)\right\|}.\] (12)

In practice, we initialize the trust region size as a default value \(r\) and calculate the gradient estimation error during the training process for calibration (Algorithm 1). However, the calculation of the standard deviation of gradients usually requires multiple samples, which will bring times of computation overload. In pursuit of a practical algorithm, we propose to adopt the gradient variance among several successive iterations as an approximation. Similar ideas are widely used in deep learning optimizers, such as Adam [21] and AdaGrad [48], which adopt multi-iteration statistics as the momentum of gradient descent. The approximation process is guaranteed by the following theoretical results.

**Lemma 3.10** (**Trust region one-iteration approximation**).: _Suppose that loss function \(\mathcal{L}\) is \(L\)-Lipschitz and \(\beta\)-smooth for \(\theta\) and the \(t\)-th step parameter is \(\theta_{t}\). Two gradient difference sequences between successive iterations, \(\|\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{1})-\nabla_{ \theta}\mathcal{L}(u_{\theta_{t-1}},\boldsymbol{z}_{2})\|\) and \(\|\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{1})-\nabla_{ \theta}\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{2})\|\), share the same limit, as the difference of the two sequences is dominated by the following inequality:_

\[\big{|}\left\|\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{1})- \nabla_{\theta}\mathcal{L}(u_{\theta_{t-1}},\boldsymbol{z}_{2})\right\|-\| \nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{1})-\nabla_{\theta }\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{2})\|\big{|}\leq\beta L\alpha_{t-1},\] (13)

_where \(\alpha_{t-1}\) represents the step size at the \((t-1)\)-th iteration, which approaches 0 as \(t\) tends to \(\infty\)._

**Theorem 3.11** (**Trust region multi-iteration approximation**).: _Suppose that loss function \(\mathcal{L}\) is \(L\)-Lipschitz and \(\beta\)-smooth for \(\theta\) and the learning rate \(\alpha_{t}\leq\frac{1}{\beta L}\) converges to zero over time \(t\), then the estimation error can be approximated by the variance of optimization gradients in multiple successive iterations. Given hyperparameter \(T_{0}\), our multi-iteration approximation is guaranteed by_

\[\lim_{t\to\infty}\sigma\left(\left\{\nabla_{\theta}\mathcal{L}(u_{\theta_{t-i +1}},\boldsymbol{z}_{i})\right\}_{i=1}^{T_{0}}\right)=\sigma\left(\left\{ \nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\boldsymbol{z}_{i})\right\}_{i=1}^ {T_{0}}\right).\] (14)

It is worth noting that, as presented in Algorithm 1, since the gradient of each iteration has already been on the shelf, our design will not bring any extra gradient or backpropagation calculation in comparison with point optimization. Besides, our algorithm is not limited to a certain optimizer, and in general, we can effectively obtain the gradients of parameters by retrieving the computation graph.

Balance between generalization and optimizationRecall that in Theorem 3.5, we observe that a larger region size will benefit the generalization error, while Theorem 3.9 demonstrates that too large region size will also cause unstable training because it will result in excessive gradient estimation error of Monte Carlo sampling in our implementation. The above analysis reveals the underlying trade-off between generalization and optimization of PINN models, which is formally stated below.

**Theorem 3.12** (**Region Optimization with gradient estimation error**).: _Based on the same assumptions in Theorem 3.5 but optimizing the PINN model with the approximate region optimization loss \(\mathcal{L}_{r}^{\mathrm{approx}}(u_{\theta},\boldsymbol{x})=\nabla_{\theta} \mathcal{L}(u_{\theta},\boldsymbol{x}+\boldsymbol{\xi}),\boldsymbol{\xi} \sim U(\Omega_{r})\) for \(T\) iterations, we further denote the upper bound of gradient estimation error as \(\mathcal{E}_{r,\mathrm{grad}}=\max_{t\leq T}\|\nabla_{\theta}\mathcal{L}_{r} ^{\mathrm{approx}}-\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}\|\), then \(\mathcal{E}_{\mathrm{gen}}\) satisfies:_

_(1) If \(\mathcal{L}\) is convex for \(\theta\) and \(\alpha_{t}\leq\frac{2}{\beta}\), \(\mathcal{E}_{\mathrm{gen}}\leq\big{(}\begin{array}{cc}(1-|\Omega_{r}|/| \Omega|)L&+&\mathcal{E}_{r,\mathrm{grad}}\\ \text{inversely proportional to }|\Omega_{r}|&\end{array}\big{)}\frac{2L}{| \mathcal{S}|}\sum_{t=1}^{T}\alpha_{t}\)._

_(2) If \(\mathcal{L}\) is bounded by a constant \(C\) and is non-convex for \(\theta\) with monotonically non-increasing step sizes \(\alpha_{t}\leq\frac{1}{\beta t}\), then \(\mathcal{E}_{\mathrm{gen}}\leq\frac{C}{|\mathcal{S}|}+\frac{2L^{2}(T-1)}{\beta (|\mathcal{S}|-1)}\frac{-J^{\prime}L(|\Omega_{r}|/|\Omega|)^{2}}{\text{inversely proportional to }|\Omega_{r}|}+ \frac{J^{\prime}\mathcal{E}_{r,\mathrm{grad}}(1+|\Omega_{r}|/|\Omega|)}{\text{ generally}\propto|\Omega_{r}|}\), where \(J^{\prime}\) is a finite number that depends on the training property at the several beginning iterations._Proof.: In contrast to Theorem 3.5, here the gradient estimation error will bring extra optimization discrepancy between different training sets. See Appendix B.4 for complete proof. 

Based on Theorem 3.12, we have pinpointed that classical point optimization (\(\Omega_{r}=0\)) and sampling points globally (\(\Omega_{r}=\Omega\)) discussed in Theorem 3.5 correspond to two extreme cases. The former makes \(\mathcal{E}_{r,\mathrm{grad}}=0\) but cannot reduce the generalization error, while the latter holds a large gradient estimation error. Thus, neither case can yield perfect generalization for PINNs. In contrast, the design for calibrating trust regions in RoPINN provides an adaptive strategy to better balance generalization and optimization, which can adjust the region size according to multi-iteration training stability.

## 4 Experiments

To verify the effectiveness and generalizability of our proposed RoPINN, we experiment with a wide range of PDEs, covering diverse physics processes and a series of advanced PINN models.

BenchmarksFor a comprehensive evaluation, we experiment with four benchmarks: 1D-Reaction, 1D-Wave, Convection and PINNacle [12]. The first three benchmarks are widely acknowledged in investigating the optimization property of PINNs [47; 37]. Especially, 1D-Reaction and Convection are highly challenging and have been used to demonstrate "PINNs failure modes" [24; 33]. As for PINNacle [12], it is a comprehensive family of 20 tasks, including diverse PDEs, e.g. Burgers, Poisson, Heat, Navier-Stokes, Wave and Gray-Scott equations in 1D to 5D space and on complex geometries. In this paper, to avoid meaningless comparisons, we remove the tasks that all the methods fail and leave 16 tasks.

Base modelsTo verify the generalizability of RoPINN among different PINN models, we experiment with five base models, including canonical PINN [36], activation function enhanced models: QRes [3] and FLS [50], Transformer-based model PINNsFormer [58] and advanced physics-informed backbone KAN [28]. PINNsFormer [58] and KAN [28] are the most advanced PINN models.

BaselinesAs stated before, this paper mainly focuses on the objective function of PINNs. Thus, we only include the gradient-enhanced method gPINN [55] and variational-based method vPINN [18] as baselines. Notably, there are diverse training strategies for PINNs focusing on other aspects than objective function, such as sampling-based RAR [51] or neural tangent kernel (NTK) approaches [47]. We also experimented with them and demonstrated that they contribute orthogonally to RoPINN.

ImplementationsIn RoPINN (Algorithm 1), we select the multi-iteration hyperparameter \(T_{0}\) from \(\{5,10\}\) and set the initial region size \(r=10^{-4}\) for all datasets, where the trust region size will be adaptively adjusted to fit the PDE property during training. For 1D-Reaction, 1D-Wave and Convection, we follow [58] and train the model with L-BFGS optimizer [27] for 1,000 iterations. As for PINNacle, we strictly follow their official configuration [12] and train the model with Adam [21] for 20,000 iterations. Besides, for simplicity and fair comparison, we set the weights of PINN loss as equal, that is \(\lambda_{*}=1\) in Eq. (2). Canonical loss formalized in Eq. (2), relative L1 error (rMAE) and relative L2 error (rMSE) are recorded. All experiments are implemented in PyTorch [34] and trained on a single NVIDIA A100 GPU. See Appendix C for more implementation details.

### Main Results

ResultsAs shown in Table 2, we investigate the effectiveness of RoPINN on diverse tasks and base models and compare it with two well-acknowledged PINN objectives. Here are two key observations.

RoPINN can consistently boost performance on all benchmarks, justifying its generality on PDEs and base models. Notably, since the PDEs under evaluation are quite diverse, especially for PINNacle (Table 1), it is extremely challenging to obtain such a consistent improvement. We can find that the previous high-order regularization and variational-based methods could yield negative effects in many cases. For example, gPINN [55] performs badly on 1D-Wave, which may be due to second-order derivatives in the wave equation. Besides, vPINN [18] also fails in 1D-Reaction and QRes.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Benchmark & Dimension & Derivative & Property \\ \hline
1D-Reaction & 1D+Time & 1 (e.g. \(\frac{\partial\pi}{\partial t}\)) & Failure modes [24] \\
1D-Wave & 1D+Time & 2 (e.g. \(\frac{\partial\pi}{\partial\pi}\)) & / \\ Convection & 1D+Time & 1 (e.g. \(\frac{\partial\pi}{\partial\pi}\)) & Failure modes [24] \\ PINNacle [12] & 1D\(\sim\)5D+Time & 1\(\sim\)2 (e.g. \(\frac{\partial\pi}{\partial\pi}\)) & 16 different tasks \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of benchmarks. _Dimension_ means the input space and _Derivative_ is the highest derivative order.

As we stated in Table 1, 1D-Reaction and Convection are hard to optimize, so-called "PINNs failure modes" [24, 33]. In contrast, empowered by RoPINN, PINNs can mitigate this thorny challenge to some extent. Specifically, with RoPINN, canonical PINN [36], QRes [3] and FLS [50] achieve more than 90% improvements in 1D-Reaction. Besides, RoPINN can further enhance the performance of PINNsFormer [58] and KAN [28], which have already performed well in 1D-Reaction or Convection, further verifying its effectiveness in helping PINN optimization.

**Combining with other strategies** Since RoPINN mainly focuses on the objective function design, it can be integrated seamlessly and directly with other strategies. As shown in Table 3, we experiment with the widely-used loss-reweighting method NTK [47] and data-sampling strategy RAR [51]. Although NTK can consistently improve the performance, it will take extra computation costs due to the calculation of neural tangent kernels [15]. Based on NTK, our RoPINN can obtain better results with slightly more time cost. As for RAR, it performs unstable in different tasks, while RoPINN can also boost it. These results verify the orthogonal contribution and favorable efficiency of RoPINN w.r.t. other methods.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Base Model} & \multirow{2}{*}{Objective} & \multicolumn{3}{c}{1D-Reaction} & \multicolumn{3}{c}{1D-Wave} & \multicolumn{3}{c}{Convection} & \multicolumn{3}{c}{PINNacle (16 tasks)} \\ \cline{3-13}  & & Loss & rMAE & rMSE & Loss & rMAE & rMSE & Loss & rMAE & rMSE & rMAE & rMSE \\ \hline \multirow{6}{*}{PINN [36]} & Vanilla & 2.0e-1 & 0.982 & 0.981 & 1.9e-2 & 0.326 & 0.335 & 1.6e-2 & 0.778 & 0.840 & - & - \\  & gPINN & 2.0e-1 & 0.978 & 0.978 & 2.8e-2 & 0.399 & 0.399 & 3.1e-2 & 0.890 & 0.935 & 18.8\% & 18.8\% \\  & vPINN & 2.3e-1 & 0.985 & 0.982 & 7.3e-3 & 0.162 & 0.173 & 1.1e-2 & 0.663 & 0.743 & 25.0\% & 25.0\% \\ \cline{2-13}  & RoPINN & **4.7e-5** & **0.056** & **0.095** & **1.5e-3** & **0.063** & **0.064** & **1.0e-2** & **0.635** & **0.720** & **0.67** \\  & Promotion & 99\% & 94\% & 90\% & 92\% & 80\% & 80\% & 25\% & 18\% & 14\% & **93.8\%** & **100.0\%** \\ \hline \multirow{6}{*}{QRes [3]} & Vanilla & 2.0e-1 & 0.979 & 0.977 & 9.8e-2 & 0.523 & 0.515 & 4.2e-2 & 0.925 & 0.959 & - & - \\  & gPINN & 2.1e-2 & 0.984 & 0.984 & 1.3e-1 & 0.785 & 0.781 & 1.6e-1 & 1.111 & 1.222 & 12.5\% & 12.5\% \\  & vPINN & 2.2e-2 & 0.999 & 1.000 & 1.0e-1 & 0.709 & 0.721 & 5.5e-2 & 0.941 & 0.966 & 12.5\% & 12.5\% \\ \cline{2-13}  & RoPINN & **9.0e-6** & **0.007** & **0.013** & **1.7e-2** & **0.309** & **0.321** & **1.2e-2** & **0.819** & **0.870** & **81.3\%** & **81.3\%** \\  & Promotion & 99\% & 99\% & 99\% & 83\% & 41\% & 38\% & 71\% & 11\% & 9\% & **81.3\%** & **81.3\%** \\ \hline \multirow{6}{*}{FLS [50]} & Vanilla & 2.0e-1 & 0.984 & 0.985 & 3.6e-3 & 0.102 & 0.119 & 1.2e-2 & 0.674 & 0.771 & - & - \\  & gPINN & 2.0e-1 & 0.978 & 0.979 & 9.2e-2 & 0.500 & 0.489 & 3.8e-1 & 0.913 & 0.949 & 12.5\% & 18.8\% \\  & vPINN & 2.1e-1 & 1.000 & 0.994 & 2.1e-3 & 0.069 & 0.069 & 1.1e-2 & 0.688 & 0.765 & 25.0\% & 18.8\% \\ \cline{2-13}  & RoPINN & **2.2e-5** & **0.022** & **0.039** & **1.5e-4** & **0.016** & **0.017** & **9.6e-4** & **0.173** & **0.197** & **81.3\%** & **87.5\%** \\  & Promotion & 99\% & 98\% & 96\% & 96\% & 84\% & 86\% & 99\% & 74\% & 74\% & & & \\ \hline \multirow{6}{*}{PINNs-Former [58]} & Vanilla & 3.0e-6 & 0.015 & 0.030 & 1.4e-2 & 0.270 & 0.283 & 3.7e-5 & 0.023 & 0.027 & - & - \\  & gPINN & 1.5e-6 & 0.009 & 0.018 & OOM & OOM & OOM & 3.7e-2 & 0.914 & 0.950 & 0.0\% & 0.0\% \\  & vPINN & 1.6e-4 & 0.065 & 0.124 & 4.5e-2 & 0.411 & 0.400 & 5.1e-5 & 0.016 & 0.022 & 0.0\% & 0.0\% \\ \cline{2-13}  & RoPINN & **1.0e-6** & **0.007** & **0.017** & **6.5e-3** & **0.165** & **0.172** & **1.2e-5** & **0.005** & **0.006** & **100.0\%** \\  & Promotion & 66\% & 53\% & 43\% & 54\% & 39\% & 39\% & 68\% & 78\% & 78\% & & & \\ \hline \multirow{6}{*}{KAN [28]} & Vanilla & 7.3e-5 & 0.031 & 0.061 & 9.2e-2 & 0.499 & 0.489 & 5.8e-2 & 0.922 & 0.954 & - & - \\  & gPINN & 2.9e-4 & 0.030 & 0.061 & 2.6e-1 & 1.131 & 1.110 & 1.2e-1 & 1.006 & 1.041 & 31.3\% & 31.3\% \\ \cline{1-1}  & vPINN & 2.1e-1 & 0.998 & 0.996 & 9.0e-2 & 0.498 & 0.487 & 2.5e-2 & 0.853 & 0.853 & 43.8\% & 43.8\% \\ \cline{1-1} \cline{2-13}  & RoPINN & **4.9e-5** & **0.026** & **0.051** & **9.6e-3** & **0.177** & **0.191** & **2.2e-2** & **0.805** & **0.801** & **100\%** & **93.8\%** \\ \cline{1-1}  & Promotion & 33\% & 16\% & 16\% & 89\% & 65\% & 61\% & 62\% & 13\% & 16\% & & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison between RoPINN and other objective functions (gPINN [55] and vPINN [18]) under different base models. Metrics for PINNacle [12] are the proportions of improved tasks over 16 tasks, where full results can be found in Appendix E. A lower loss, rMAE or rMSE indicates better performance. For clarity, we highlight the value with \(\,\)\(\,\)\(\,\)if it surpasses the vanilla PINN and the best is in bold. _Promotion_ refers to the relative promotion of RoPINN over the vanilla version.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Method rMSE & 1D-Reaction & 1D-Wave & Convection & Time (s) \\ \hline PINN [36] & 0.981 & 0.335 & 0.840 & 18.47 \\ +gPINN [55] & 0.978 & 0.399 & 0.935 & 37.91 \\ +vPINN [18] & 0.982 & 0.173 & 0.743 & 38.78 \\ +RoPINN & **0.095** & **0.064** & **0.720** & 20.04 \\ \hline +NTK [47] & 0.098 & 0.149 & 0.798 & 27.99 \\ +NTK

### Algorithm Analysis

Initial region size in Algorithm 1To provide an intuitive understanding of RoPINN, we plot the curves of training statistics in Figure 2, including temporally adjusted region size \(\log(\frac{r}{\sigma_{t}})\), train loss, and test performance. From Figure 2(a), we can find that even though we initialize the region size as distinct values, RoPINN will progressively adjust the trust region size to similar values during training. This indicates that our algorithm can capture a potential "balance point" between training stability and generalization error, where the fluctuation of trust region size reveals the balancing process. Further, as shown in Figure 2(b-c), if \(r\) is initialized as a value closer to the balance point (e.g. 1e-4 and 1e-5 in this case), then the training process will converge faster. And too large a region size (e.g. 1e-3) will decrease the convergence speed due to the optimization noise (Theorem 3.9).

Number of sampling points in Eq. (8)For efficiency, RoPINN only samples one point within the trust region to approximate the region gradient descent. However, it is worth noticing that sampling more points will make the approximation in Eq. (8) more accurate, leading to a lower gradient estimation error. Further, since RoPINN employs an adaptive strategy to adjust region \(\Omega_{r}\), a lower gradient estimation error will also make the optimization process adapt to a larger region size \(r\). Therefore, we observe in Figure 3(a-b) that sampling more points will also increase the finally learned region size \(r\) and speed up the convergence. In addition, Figure 3(c) shows that adding sampled points can also improve the final performance, which has also been theoretically justified in Theorem 3.12 that the upper bound of generalization error is inversely proportion to gradient estimation error.

Efficiency analysisAs we discussed above, sampling more points can benefit the final performance, while we choose only sample one point as the default setting of RoPINN in the spirit of boosting PINNs without extra backpropagation or gradient calculation, which has already achieved significant promotion w.r.t. original PINNs (Table 2). To provide a more comprehensive understanding of algorithm property, we plot the efficiency-performance curve in Figure 4, where we can obtain the following observations. Firstly, computation costs will grow linearly when adding points. Secondly, more points will bring better performance but will saturate around 10 points, where the performance fluctuations of 9, 13, and 30 points are within three times the standard deviations (Appendix D.3).

AblationsTo verify the effectiveness of our design in RoPINN, we present ablations in Figure 5. It is observed that although we only sample one point, even fixed-size region optimization can also boost

Figure 3: Optimization of canonical PINN [36] on the 1D-Reaction under different sample points.

Figure 2: Optimization of canonical PINN [36] on the 1D-Reaction under different region sizes. To highlight the region size change, we adopt the moving average over time and mark the temporal standard deviation with shadow. The steep training loss is caused by the learning difficulty of PDE.

the performance of PINNs in most cases, demonstrating the effectiveness of introducing "region" to PINN optimization. However, as illustrated in Theorem 3.9, the sampling process may also cause gradient estimation error, so the relative promotion is inconsistent and unstable among different PDEs and base models. With our proposed trust region calibration, we can obtain a more significant and consistent improvement, indicating that achieving a better balance between optimization and generalization (formally stated in Theorem 3.12) performs an essential role in training PINN models.

Loss landscapePrevious research [24] has studied why PINN cannot solve the Convection equation and found that it is not caused by the limited model capacity but by the hard-to-optimize loss landscape. Here we also provide a loss landscape visualization in Figure 6, which is obtained by perturbing the trained model along the directions of the first two dominant Hessian eigenvectors [24, 25, 54]. We can find that vanilla PINN optimized by PINN loss in Eq. (2) presents sharp cones. In contrast, empowered by RoPINN, the loss landscape is significantly smoothed. This visualization intuitively interprets why RoPINN can mitigate "PINN failure modes". See Appendix D for more results.

## 5 Conclusion

This paper presents and analyzes a new PINN optimization paradigm: region optimization. Going beyond previous scatter-point optimization, we extend the optimization from selected points to their neighborhood regions. Based on this idea, RoPINN is implemented as a simple but effective training algorithm, where an efficient Monte Carlo approximation process is used along with a trust region calibration strategy to control the gradient estimation error caused by sampling, theoretically manifesting a better balance of generalization and optimization. In addition to theoretical advantages, RoPINN can consistently boost the performance of various PINN models without extra backpropagation or gradient calculation, demonstrating favorable efficiency, training stability and general capability.

Figure 4: Efficiency and model performance w.r.t. number of samples. Note that the default setting of RoPINN is just sampling one point, which will not bring extra gradient calculation costs.

Figure 5: Ablation study of RoPINN on different PDEs and diverse base models. rMSE is recorded.

Figure 6: Loss landscape of RoPINN and vanilla PINNs on the Convection equation. _Error Map_ refers to the distance between model prediction and the accurate solution, i.e. \((u_{\theta}-u)\).

[MISSING_PAGE_FAIL:11]

* [21] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [22] David A Kopriva. _Implementing spectral methods for partial differential equations: Algorithms for scientists and engineers_. Springer Science & Business Media, 2009.
* [23] Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett. Mitigating propagation failures in physics-informed neural networks using retain-resample-release (R3) sampling. In _ICML_, 2023.
* [24] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. _NeurIPS_, 2021.
* [25] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. _NeurIPS_, 2018.
* [26] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In _ICLR_, 2021.
* [27] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. _Mathematical programming_, 1989.
* [28] Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y Hou, and Max Tegmark. Kan: Kolmogorov-armold networks. _arXiv preprint arXiv:2404.19756_, 2024.
* [29] ES Lobanova and FI Ataullakhanov. Running pulses of complex shape in a reaction-diffusion model. _Physical review letters_, 2004.
* [30] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving differential equations. _SIAM Review_, 2021.
* [31] Suryanarayana Maddu, Dominik Sturm, Christian L Muller, and Ivo F Sbalzarini. Inverse dirichlet weighting enables reliable training of physics informed neural networks. _Machine Learning: Science and Technology_, 2022.
* [32] Siddhartha Mishra and Roberto Molinaro. Estimates on the generalization error of physics-informed neural networks for approximating pdes. _IMA Journal of Numerical Analysis_, 2023.
* [33] Rambod Mojgani, Maciej Balajewicz, and Pedram Hassanzadeh. Lagrangian pinns: A causality-conforming solution to failure modes of physics-informed neural networks. _arXiv preprint arXiv:2205.02902_, 2022.
* [34] Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, 2019.
* [35] Maziar Raissi. Deep hidden physics models: Deep learning of nonlinear partial differential equations. _JMLR_, 2018.
* [36] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _J. Comput. Phys._, 2019.
* [37] Pratik Rathore, Weimu Lei, Zachary Frangella, Lu Lu, and Madeleine Udell. Challenges in training pinns: A loss landscape perspective. _arXiv preprint arXiv:2402.01868_, 2024.
* [38] Tomas Roubicek. _Nonlinear partial differential equations with applications_. Springer Science & Business Media, 2013.
* [39] Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differential equations. _Journal of computational physics_, 2018.
* [40] Pavel Solin. _Partial differential equations and the finite element method_. John Wiley & Sons, 2005.
* [41] Thomas Stocker. _Introduction to climate modelling_. Springer Science & Business Media, 2011.
* [42] ENZO Tonti. Variational formulation for every nonlinear problem. _International Journal of Engineering Science_, 1984.

* [43] Nobuyuki Umetani and Bernd Bickel. Learning three-dimensional flow for interactive aerodynamic design. _ACM Transactions on Graphics (TOG)_, 2018.
* [44] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. _Nature_, 2023.
* [45] Sifan Wang, Shyam Sankaran, and Paris Perdikaris. Respecting causality for training physics-informed neural networks. _Computer Methods in Applied Mechanics and Engineering_, 2024.
* [46] Sifan Wang, Shyam Sankaran, Hanwen Wang, and Paris Perdikaris. An expert's guide to training physics-informed neural networks. _arXiv preprint arXiv:2308.08468_, 2023.
* [47] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. _Journal of Computational Physics_, 2022.
* [48] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. _JMLR_, 2020.
* [49] Abdul Majid Wazwaz. Partial differential equations: methods and applications. 2002.
* [50] Jian Cheng Wong, Chin Chun Ooi, Abhishek Gupta, and Yew-Soon Ong. Learning in sinusoidal spaces with physics-informed neural networks. _IEEE Transactions on Artificial Intelligence_, 2022.
* [51] Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks. _Computer Methods in Applied Mechanics and Engineering_, 2023.
* [52] Jiancong Xiao, Yanbo Fan, Ruoyu Sun, Jue Wang, and Zhi-Quan Luo. Stability analysis and generalization bounds of adversarial training. _NeurIPS_, 2022.
* [53] Jiachen Yao, Chang Su, Zhongkai Hao, Songming Liu, Hang Su, and Jun Zhu. Multiadam: Parameter-wise scale-invariant optimizer for multiscale training of physics-informed neural networks. In _ICML_, 2023.
* [54] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the hessian. In _2020 IEEE international conference on big data (Big data)_, 2020.
* [55] Jeremy Yu, Lu Lu, Xuhui Meng, and George Em Karniadakis. Gradient-enhanced physics-informed neural networks for forward and inverse pde problems. _Computer Methods in Applied Mechanics and Engineering_, 2022.
* [56] Ya-xiang Yuan. A review of trust region algorithms for optimization. In _Icaim_, 2000.
* [57] Yaohua Zang, Gang Bao, Xiaojing Ye, and Haomin Zhou. Weak adversarial networks for high-dimensional partial differential equations. _Journal of Computational Physics_, 2020.
* [58] Leo Zhiyuan Zhao, Xueying Ding, and B Aditya Prakash. Pinnsformer: A transformer-based framework for physics-informed neural networks. _ICLR_, 2024.

Generalization Analysis in Section 3.1

This section will present the proofs for the theorems in Section 3.1.

### Proof for Point Optimization Generalization Error (Theorem 3.3)

The proof for the convex case is derived from previous papers [13, 52] under the Assumption 3.2. We derive a more compact upper bound for generalization error in expectation for the non-convex setting.

**Lemma A.1**.: _Given two finite sets of selected points \(\mathcal{S}=(\bm{x}_{1},\cdots,\bm{x}_{N})\) and \(\mathcal{S}^{\prime}=(\bm{x}^{\prime}_{1},\cdots,\bm{x}^{\prime}_{N})\), let \(\mathcal{S}^{(i)}=(\bm{x}_{1},\cdots,\bm{x}_{i-1},\bm{x}^{\prime}_{i},\bm{x}_ {i+1},\cdots,\bm{x}_{N})\) be the set that is identical to \(\mathcal{S}\) except the \(i\)-th element, the generalization error in expectation is equal to the expectation of the error difference between these two sets, which can be formalized as follows:_

\[\mathcal{E}_{\mathrm{gen}}=\left|\mathbb{E}_{\mathcal{S},\mathcal{S}^{\prime},\mathcal{A}}\left[\frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\mathcal{A}(\mathcal{ S}^{(i)}),\bm{x}^{\prime}_{i})-\frac{1}{N}\sum_{i=1}^{N}\mathcal{L}( \mathcal{A}(\mathcal{S}),\bm{x}^{\prime}_{i})\right]\right|.\] (15)

Proof.: Directly deriving from the in-domain loss, we have:

\[\mathbb{E}_{\mathcal{S},\mathcal{A}}\left[\mathcal{L}(\mathcal{A }(\mathcal{S}),\mathcal{S})\right] =\mathbb{E}_{\mathcal{S},\mathcal{A}}\left[\frac{1}{N}\sum_{i=1} ^{N}\mathcal{L}(\mathcal{A}(\mathcal{S}),\bm{x}_{i})\right]\] (16) \[=\mathbb{E}_{\mathcal{S},\mathcal{S}^{\prime},\mathcal{A}}\left[ \frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\mathcal{A}(\mathcal{S}),\bm{x}^{\prime} _{i})\right]+\delta\] \[=\mathbb{E}_{\mathcal{S},\mathcal{A}}\left[\mathcal{L}(\mathcal{ A}(\mathcal{S}),\Omega)\right]+\delta.\]

Then, according to Definition 3.1, we have:

\[\mathcal{E}_{\mathrm{gen}}=\delta=\mathbb{E}_{\mathcal{S},\mathcal{S}^{\prime},\mathcal{A}}\left[\frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\mathcal{A}(\mathcal{ S}^{(i)}),\bm{x}^{\prime}_{i})-\frac{1}{N}\sum_{i=1}^{N}\mathcal{L}( \mathcal{A}(\mathcal{S}),\bm{x}^{\prime}_{i})\right].\] (17)

**Lemma A.2** (**Convex case**).: _Given the stochastic gradient method with an update rule as \(G_{\alpha,\bm{x}}(\theta)=\theta-\alpha\nabla_{\theta}\mathcal{L}(\theta,\bm{ x})\) and \(\mathcal{L}\) is convex in \(\theta\), then for \(\alpha\leq\frac{2}{\beta}\), we have \(\|G_{\alpha,\bm{x}}(\theta_{1})-G_{\alpha,\bm{x}}(\theta_{2})\|\leq\|\theta_ {1}-\theta_{2}\|\)._

Proof.: For clarity, we denote \(g=\|\nabla_{\theta}\mathcal{L}(\theta_{1},\bm{x})-\nabla_{\theta}\mathcal{L}( \theta_{2},\bm{x})\|\). Then we have:

\[\|G_{\alpha,\bm{x}}(\theta_{1})-G_{\alpha,\bm{x}}(\theta_{2})\|^ {2} =\|\theta_{1}-\theta_{2}-\alpha(\nabla_{\theta}\mathcal{L}(\theta _{1},\bm{x})-\nabla_{\theta}\mathcal{L}(\theta_{2},\bm{x}))\|^{2}\] (18) \[=\|\theta_{1}-\theta_{2}\|^{2}-2\alpha\left(\nabla_{\theta} \mathcal{L}(\theta_{1},\bm{x})-\nabla_{\theta}\mathcal{L}(\theta_{2},\bm{x}) \right)^{\mathsf{T}}(\theta_{1}-\theta_{2})+\alpha^{2}g^{2}\] \[\leq\|\theta_{1}-\theta_{2}\|^{2}-\frac{2\alpha}{\beta}g^{2}+ \alpha^{2}g^{2}\] (Convexity and Assumption 3.2) \[\leq\|\theta_{1}-\theta_{2}\|^{2}. (\alpha\leq\frac{2}{\beta})\]

Convex settingNext, we will give the proof for the convex case of Theorem 3.3(1).

Proof.: According to Lemma A.1, we attempt to bound the generalization error in expectation \(\mathcal{E}_{\mathrm{gen}}\) by analyzing the error difference between two selected sample sets. Denote \(\mathcal{S}\) and \(\mathcal{S}^{\prime}\) as two identical sample sets of size \(|\mathcal{S}|\) except for one sample. Suppose that with the stochastic gradient method on these two sets, we can obtain two optimization trajectories \(\{\theta_{t}\}_{t=1}^{T}\) and \(\{\theta^{\prime}_{t}\}_{t=1}^{T}\) respectively. According to Assumption 3.2, we have the following inequality:

\[\mathbb{E}\left[\left|\mathcal{L}(u_{\theta_{t}},\bm{x})-\mathcal{L}(u_{\theta^ {\prime}_{t}},\bm{x})\right|\right]\leq L\mathbb{E}\left[\left\|\theta_{t}- \theta^{\prime}_{t}\right\|\right].\] (19)

We assume two optimization trajectories, both obtained under the same random update rule and random permutation rule. Note that at the \(t\)-th step, with probability \((1-\frac{1}{|\mathcal{S}|})\), the example selected by the stochastic gradient method is the same in both \(\mathcal{S}\) and \(\mathcal{S}^{\prime}\). As for the other \(\frac{1}{|\mathcal{S}|}\) probability, we have to deal with different selected samples. Thus, according to Lemma A.2, we have:

\[\mathbb{E}\left[\left\|\theta_{t+1}-\theta^{\prime}_{t+1}\right\|\right] =(1-\frac{1}{|\mathcal{S}|})\mathbb{E}\left[\left\|G_{\alpha_{1},\bm{x}}(\theta_{t})-G_{\alpha_{t},\bm{x}}(\theta^{\prime}_{t})\right\|\right] +\frac{1}{|\mathcal{S}|}\mathbb{E}\left[\left\|G_{\alpha_{1},\bm{x}}(\theta_ {t})-G_{\alpha_{t},\bm{x}^{\prime}}(\theta^{\prime}_{t})\right\|\right]\] \[\leq(1-\frac{1}{|\mathcal{S}|})\mathbb{E}[\left\|\theta_{t}- \theta^{\prime}_{t}\right\|]+\frac{1}{|\mathcal{S}|}\mathbb{E}\left[\left\| \theta_{t}-\theta^{\prime}_{t}\right\|\right]+\left\|\alpha_{t}\nabla_{\theta }\mathcal{L}(u_{\theta},\bm{x})-\alpha_{t}\nabla_{\theta}\mathcal{L}(u_{ \theta},\bm{x}^{\prime})\right\|\right].\] (20)

Due to the \(L\)-Lipschitz assumption of \(\mathcal{L}\), the gradient \(\nabla_{\theta}\mathcal{L}(u_{\theta},\bm{x})\) is uniformly smaller than \(L\), then:

\[\mathbb{E}\left[\left\|\theta_{t+1}-\theta^{\prime}_{t+1}\right\|\right]\leq \mathbb{E}\left[\left\|\theta_{t}-\theta^{\prime}_{t}\right\|\right]+\frac{2 \alpha_{t}L}{|\mathcal{S}|}.\] (21)

In summary, since both optimization trajectories start from the same initialization, namely \(\theta_{0}=\theta^{\prime}_{0}\), the following inequality holds:

\[\mathbb{E}\left[\left|\mathcal{L}(u_{\theta_{T}},\bm{x})-\mathcal{L}(u_{ \theta^{\prime}_{T}},\bm{x})\right|\right]\leq\frac{2L^{2}}{|\mathcal{S}|}\sum _{t=1}^{T}\alpha_{t}.\] (22)

From Lemma A.1, we have \(\mathcal{E}_{\mathrm{gen}}\leq\frac{2L^{2}}{|\mathcal{S}|}\sum_{t=1}^{T}\alpha _{t}\). 

**Lemma A.3** (**Non-convex case**).: _Given the stochastic gradient method with an update rule as \(G_{\alpha,\bm{x}}(\theta)=\theta-\alpha\nabla_{\theta}\mathcal{L}(\theta,\bm{ x})\), then we have \(\|G_{\alpha,\bm{x}}(\theta_{1})-G_{\alpha,\bm{x}}(\theta_{2})\|\leq(1+\alpha \beta)\|\theta_{1}-\theta_{2}\|\)._

Proof.: This inequality can be easily obtained from the following:

\[\|G_{\alpha,\bm{x}}(\theta_{1})-G_{\alpha,\bm{x}}(\theta_{2})\| =\|\theta_{1}-\theta_{2}-\alpha(\nabla_{\theta}\mathcal{L}(\theta _{1},\bm{x})-\nabla_{\theta}\mathcal{L}(\theta_{2},\bm{x}))\|\] \[=\|\theta_{1}-\theta_{2}\|+\alpha\|\nabla_{\theta}\mathcal{L}( \theta_{1},\bm{x})-\nabla_{\theta}\mathcal{L}(\theta_{2},\bm{x})\|\] \[\leq(1+\alpha\beta)\|\theta_{1}-\theta_{2}\|.\] (Assumption 3.2 )

Non-convex settingFinally, we will give the proof for the non-convex case in Theorem 3.3(2).

Proof.: We also consider the optimization trajectory \(\{\theta_{t}\}_{t=1}^{T}\) and \(\{\theta^{\prime}_{t}\}_{t=1}^{T}\) from \(\mathcal{S}\) and \(\mathcal{S}^{\prime}\), which are identical except for one element. We assume two optimization trajectories, both obtained under the same random update rule and random permutation rule. Let \(\delta_{t}=\|\theta_{t}-\theta^{\prime}_{t}\|\) and \(t_{0}\in\{1,\cdots,|\mathcal{S}|\}\) be a considered iteration. Here, \(t\leq|\mathcal{S}|\) because for \(t>|\mathcal{S}|\), we must have \(\delta_{t_{0}}\neq 0\). Then we have:

\[\mathbb{E}\left[\left|\mathcal{L}(u_{\theta_{T}},\bm{x})-\mathcal{L}(u_{ \theta^{\prime}_{T}},\bm{x})\right|\right] =\mathbb{P}(\delta_{t_{0}}=0)\mathbb{E}\left[\left|\mathcal{L}(u_{ \theta_{T}},\bm{x})-\mathcal{L}(u_{\theta^{\prime}_{T}},\bm{x})\right|\right| \delta_{t_{0}}=0\right]\] \[+\mathbb{P}(\delta_{t_{0}}\neq 0)\mathbb{E}\left[\left|\mathcal{L}(u_{ \theta_{T}},\bm{x})-\mathcal{L}(u_{\theta^{\prime}_{T}},\bm{x})\right|\right| \delta_{t_{0}}\neq 0\right]\] \[\leq L\mathbb{E}\left[\|\theta_{T}-\theta^{\prime}_{T}\|\|\delta_{ t_{0}}=0\right]+\mathbb{P}(\delta_{t_{0}}\neq 0)C\] (Upper bound of \[\mathcal{L}\] ) \[=\frac{Ct_{0}}{|\mathcal{S}|}+L\mathbb{E}\left[\|\theta_{T}- \theta^{\prime}_{T}\|\|\delta_{t_{0}}=0\right].\] (24)

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

we have \(g\leq\frac{|\Omega_{\rm in}|}{|\Omega_{r}|}\beta\|\theta_{t}-\theta_{t}^{\prime} \|\leq\beta\|\theta_{t}-\theta_{t}^{\prime}\|\). Thus, the following inequality holds:

\[\begin{split}&\left\|\theta_{t}-\theta_{t}^{\prime}-\left(\alpha_{t} \frac{1}{|\Omega_{r}|}\int_{\Omega_{\rm in}}\nabla_{\theta}\mathcal{L}(u_{ \theta_{t}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}-\alpha_{t}\frac{1}{|\Omega_{r }|}\int_{\Omega_{\rm in}}\nabla_{\theta}\mathcal{L}(u_{\theta_{t}^{\prime}},\bm{ x}+\bm{\xi})\mathrm{d}\bm{\xi}\right)\right\|^{2}\\ &=\|\theta_{t}-\theta_{t}^{\prime}\|^{2}-2\alpha_{t}g^{\mathsf{T} }(\theta_{t}-\theta_{t}^{\prime})+\alpha_{t}^{2}g^{2}\\ &\leq\|\theta_{t}-\theta_{t}^{\prime}\|^{2}-2\frac{\alpha_{t}}{ \beta}g^{2}+\alpha_{t}^{2}g^{2}\\ &\leq\|\theta_{t}-\theta_{t}^{\prime}\|^{2}.\end{split}\] (32)

Thus, recursively accumulating the residual at the \(t\)-th step, we have:

\[\mathcal{E}_{\rm gen}\leq(1-\frac{|\Omega_{r}|}{|\Omega|})\frac{2L^{2}}{| \mathcal{S}|}\sum_{t=1}^{T}\alpha_{t}.\] (33)

For the **more general case**, we no longer assume that the domain \(\Omega\) can be projected to a torus, resulting in a non-symmetric scenario when \(|\Omega_{\rm in}|>0\). This asymmetry arises due to the presence of boundaries, since points that could potentially intersect with the set \(\bm{x}+\Omega_{r}\) may be truncated by the boundary. Specifically, we consider \(\Omega=[0,l]^{(d+1)}\) and \(\Omega_{r}=[0,r]^{(d+1)}\). The concrete probability \(\mathbb{P}(|\Omega_{\rm in}|>0)\) and the expectation \(\mathbb{E}_{|\Omega_{\rm in}|>0}(|\Omega_{\rm in}|)\) can be calculated as follows:

\[\mathbb{P}(|\Omega_{\rm in}|>0)=(\frac{r(2l-3r)}{(l-r)^{2}})^{(d+1)},\;\mathbb{ E}_{|\Omega_{\rm in}|>0}(|\Omega_{\rm in}|)=(\frac{r^{2}(3l-4r)}{3(l-r)^{2}})^{(d+1)}.\] (34)

Thus, the general case of Eq. 31 can be reformulated using the identities above. We assume \(\frac{r}{l}<0.5\), as when \(\frac{r}{l}\geq 0.5\), it follows that \(\mathbb{P}(|\Omega_{\rm in}|>0)=1\). Specifically, Eq. (31) can be rewrite as:

\[\begin{split}&\mathbb{E}\left[\|G_{\alpha_{t},\bm{x}}^{\rm region }(\theta_{t})-G_{\alpha_{t},\bm{x}^{\prime}}^{\rm region}(\theta_{t}^{ \prime})\|\right]\\ &=\mathbb{P}(|\Omega_{\rm in}|=0)\mathbb{E}_{\Omega_{\rm in}=0} \left[\|G_{\alpha_{t},\bm{x}}^{\rm region}(\theta_{t})-G_{\alpha_{t},\bm{x}^{ \prime}}^{\rm region}(\theta_{t}^{\prime})\|\right]+\mathbb{P}(|\Omega_{\rm in }|>0)\mathbb{E}_{\Omega_{\rm in}>0}\left[\|G_{\alpha_{t},\bm{x}}^{\rm region}( \theta_{t})-G_{\alpha_{t},\bm{x}^{\prime}}^{\rm region}(\theta_{t}^{\prime})\| \right]\right]\\ &\leq\mathbb{P}(|\Omega_{\rm in}|=0)\mathbb{E}_{\Omega_{\rm in}=0} \left[\|\theta_{t}-\theta_{t}^{\prime}\|+2\alpha_{t}L\right]\\ &\leq\mathbb{P}(|\Omega_{\rm in}|=0)\mathbb{E}_{\Omega_{\rm in}=0} \left[\|\theta_{t}-\theta_{t}^{\prime}\|+2\alpha_{t}L\right]+\mathbb{P}(| \Omega_{\rm in}|>0)\mathbb{E}_{\Omega_{\rm in}>0}\left[\frac{|\Omega_{r}|-| \Omega_{\rm in}|}{|\Omega_{r}|}2\alpha_{t}L\right]\\ &\leq\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+2\alpha_{ t}L-\mathbb{P}(|\Omega_{\rm in}|>0)\mathbb{E}_{\Omega_{\rm in}>0}\left[\frac{| \Omega_{\rm in}|}{|\Omega_{r}|}2\alpha_{t}L\right]\\ &\leq\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+2\alpha_ {t}L\left[1-\mathbb{P}(|\Omega_{\rm in}|>0)\frac{\mathbb{E}_{\Omega_{\rm in}>0} (|\Omega_{\rm in}|)}{|\Omega_{r}|}\right]\\ &=\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+2\alpha_{t}L \left[1-(\frac{r^{2}(2l-3r)(3l-4r)}{3(l-r)^{4}})^{(d+1)}\right].\end{split}\] (35)

Thus, recursively accumulating the residual at the \(t\)-th step, we have:

\[\mathcal{E}_{\rm gen}\leq\left[1-(\frac{r^{2}(2l-3r)(3l-4r)}{3(l-r)^{4}})^{(d+1 )}\right]\frac{2L^{2}}{|\mathcal{S}|}\sum_{t=1}^{T}\alpha_{t}.\] (36)

Although the specific forms differ and the general case is much more complex, these two inequalities both share the same intuitive meaning: region optimization benefits from the overlap between \(\bm{x}+\Omega_{r}\) and \(\bm{x}^{\prime}+\Omega_{r}\). Moreover, within a certain range, the benefit increases as the value of \(r\) becomes larger.

Thus, to keep the bound simple and easy to understand, the main text theorems are under the assumption that \(\Omega\) can be projected to a torus. Otherwise, \(\frac{|\Omega_{r}|}{|\Omega|}\) should be replaced by \((\frac{r^{2}(2l-3r)(3l-4r)}{3(l-r)^{4}})^{(d+1)}\). 

Non-convex settingNext, we will prove the non-convex setting. Similarly, we assume \(\Omega\) can be projected to a torus. Otherwise, the \(\frac{|\Omega_{r}|}{|\Omega|}\) in the final bound should be replaced by \((\frac{r^{2}(2l-3r)(3l-4r)}{3(l-r)^{4}})^{(d+1)}\).

Proof.: For clarity, let \(M=\frac{|\Omega_{r}|}{|\Omega|}\). Then, we can rewrite the Eq. (25) as follows.

If \(\mathbb{E}(\delta_{t})\leq\frac{2L}{\beta}\), we have:

\[\mathbb{E}\left[\|\theta_{t+1}-\theta_{t+1}^{\prime}\|\|\delta_ {t_{0}}=0\right] \leq(1-\frac{1}{|\mathcal{S}|})(1+\frac{1}{t})\mathbb{E}\left[ \|\theta_{t}-\theta_{t}^{\prime}\|\right]+\frac{1}{|\mathcal{S}|}\mathbb{E} \left[\|G_{\alpha_{t},\bm{x}}^{\text{region}}(\theta_{t})-G_{\alpha_{t},\bm{x }^{\prime}}^{\text{region}}(\theta_{t}^{\prime})\|\right]\] (37) \[\leq(1+\frac{1}{t}-\frac{1-M}{t|\mathcal{S}|})\mathbb{E}[\delta_ {t}]+\frac{2L}{\beta t|\mathcal{S}|}(1-M)\] \[\leq\exp\left(\frac{1}{t}-\frac{1-M}{t|\mathcal{S}|}\right) \mathbb{E}[\delta_{t}]+\frac{2L}{\beta t|\mathcal{S}|}(1-M),\]

where the second inequality is based on the following derivations:

\[\mathbb{E}\left[\|G_{\alpha_{t},\bm{x}}^{\text{region}}(\theta_{t })-G_{\alpha_{t},\bm{x}^{\prime}}^{\text{region}}(\theta_{t}^{\prime})\|\right]\] (38) \[=\mathbb{I}\left(\frac{|\Omega|-2^{(d+1)}|\Omega_{r}|}{|\Omega|} \right)\mathbb{E}_{\Omega_{\text{in}}=0}\left[\|G_{\alpha_{t},\bm{x}}^{\text{ region}}(\theta_{t})-G_{\alpha_{t},\bm{x}^{\prime}}^{\text{region}}(\theta_{t}^{ \prime})\|\right]\] \[+\mathbb{I}\left(\frac{2^{(d+1)}|\Omega_{r}|}{|\Omega|}\right) \mathbb{E}_{\Omega_{\text{in}}>0}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+2 \alpha_{t}L\right]\] \[\leq\mathbb{I}\left(\frac{|\Omega|-2^{(d+1)}|\Omega_{r}|}{|\Omega| }\right)\mathbb{E}_{\Omega_{\text{in}}=0}\left[\|\theta_{t}-\theta_{t}^{ \prime}\|+2\alpha_{t}L\right]\] \[+\mathbb{I}\left(\frac{2^{(d+1)}|\Omega_{r}|}{|\Omega|}\right) \mathbb{E}_{\Omega_{\text{in}}>0}\left[\left\|\theta_{t}-\theta_{t}^{\prime} \right\|+\left\|\left(\alpha_{t}\frac{1}{|\Omega_{r}|}\int_{\Omega_{\text{in} }}\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi }-\alpha_{t}\frac{1}{|\Omega_{r}|}\int_{\Omega_{\text{in}}}\nabla_{\theta} \mathcal{L}(u_{\theta_{t}^{\prime}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}\right) \right\|\right]\] \[+\mathbb{I}\left(\frac{2^{(d+1)}|\Omega_{r}|}{|\Omega|}\right) \mathbb{E}_{\Omega_{\text{in}}>0}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+ \left\|\left(\alpha_{t}\frac{1}{|\Omega_{r}|}\int_{\Omega_{\text{in}}}\nabla_ {\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}-\alpha_{ t}\frac{1}{|\Omega_{r}|}\int_{\Omega_{\text{in}}}\nabla_{\theta}\mathcal{L}(u_{ \theta_{t}^{\prime}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}\right)\right\|\right]\] \[+\mathbb{I}\left(\frac{2^{(d+1)}|\Omega_{r}|}{|\Omega|}\right) \mathbb{E}_{\Omega_{\text{in}}>0}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+ \left\|\left(\alpha_{t}\frac{1}{|\Omega_{r}|}\int_{\Omega_{\text{in}}}\nabla_ {\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}-\alpha_{ t}\frac{1}{|\Omega_{r}|}\int_{\Omega_{\text{in}}}\nabla_{\theta}\mathcal{L}(u_{ \theta_{t}^{\prime}},\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}\right)\right\|\right]\] \[+\mathbb{I}\left(\frac{2^{(d+1)}|\Omega_{r}|}{|\Omega|}\right) \mathbb{E}_{\Omega_{\text{in}}>0}\left[\|\theta_{t}-\theta_{t}^{\prime}\|+\frac{| \Omega_{\text{in}}|}{|\Omega_{r}|}\alpha_{t}\beta\|\theta_{t}-\theta_{t}^{ \prime}\|+\frac{|\Omega_{\text{in}}|-|\Omega_{\text{in}}|}{|\Omega_{r}|}2 \alpha_{t}L\right]\] \[\leq\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\|\right]+ \frac{\alpha_{t}\beta|\Omega_{r}|}{|\Omega|}\mathbb{E}\left[\|\theta_{t}-\theta_{t }^{\prime}\|+2\alpha_{t}L-\mathbb{I}\left(\frac{2^{(d+1)}|\Omega_{r}|}{| \Omega|}\right)\mathbb{E}_{\Omega_{\text{in}}>0}\left[\frac{|\Omega_{\text{in} }|}{|\Omega_{r}|}2\alpha_{t}L\right]\right.\] \[\leq(1+\frac{\alpha_{t}\beta|\Omega_{r}|}{|\Omega|})\mathbb{E} \left[\|\theta_{t}-\theta_{t}^{\prime}\|\right]+2\alpha_{t}L(1-\frac{|\Omega_{ r}|}{|\Omega|})\] \[=(1+\frac{M}{t})\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\| \right]+\frac{2L}{\beta t}(1-M).\]Notably, \(\mathbb{E}\left[\|G_{\alpha_{t},\bm{x}}^{\rm region}(\theta_{t})-G_{\alpha_{t},\bm{x }^{\prime}}^{\rm region}(\theta^{\prime}_{t})\|\right]\) has an obvious upper bound, i.e. \(\left(\mathbb{E}\left[\|\theta_{t}-\theta^{\prime}_{t}\|\right]+\frac{2L}{ \beta t}\right)\). And only when \(\mathbb{E}(\delta_{t})\leq\frac{2L}{\beta}\), the bound derived by Eq. (38) is tighter. Furthermore, the condition that \(\mathbb{E}(\delta_{t})\leq\frac{2L}{\beta}\) can be easily satisfied at the beginning several iterations since \(\mathbb{E}(\delta_{0})=0\).

Otherwise, we still take the following equation:

\[\mathbb{E}\left[\|\theta_{t+1}-\theta^{\prime}_{t+1}\|\|\delta_{t_{0}}=0\right] \leq\exp\left(\frac{1}{t}-\frac{1}{t|\mathcal{S}|}\right)\mathbb{E}[\delta_{ t}]+\frac{2L}{\beta t|\mathcal{S}|},\] (39)

where we do not consider the benefits brought by the overlap area of \(\bm{x}+\Omega_{r}\) and \(\bm{x}^{\prime}+\Omega_{r}\).

Suppose that at the first \(K\) steps \(\mathbb{E}[\delta_{t_{0}+K}]\leq\frac{2L}{\beta}\), Accumulating the above in equations recursively, we have the generalization error bound accumulated to the first \(K\) steps as follows:

\[\Delta =\sum_{t=t_{0}+1}^{t_{0}+K}\left\{\Pi_{k=t+1}^{t_{0}+K}\exp\left( \frac{1}{t}-\frac{1-M}{t|\mathcal{S}|}\right)\Pi_{k=t_{0}+K+1}^{T}\exp\left( \frac{1}{t}-\frac{1}{t|\mathcal{S}|}\right)\right\}\frac{2L}{\beta t|\mathcal{ S}|}(1-M)\] (40) \[\leq\sum_{t=t_{0}+1}^{t_{0}+K}\exp\left((1-\frac{1}{|\mathcal{S}| })\log\frac{T}{t_{0}+K}+(1-\frac{1-M}{|\mathcal{S}|})\log\frac{t_{0}+K}{t} \right)\frac{2L}{\beta t|\mathcal{S}|}(1-M)\] \[=\sum_{t=t_{0}+1}^{t_{0}+K}\exp\left((1-\frac{1}{|\mathcal{S}|}) \log\frac{T}{t}+\frac{M}{|\mathcal{S}|}\log\frac{t_{0}+K}{t}\right)\frac{2L}{ \beta t|\mathcal{S}|}(1-M)\] \[\leq\sum_{t=t_{0}+1}^{t_{0}+K}\exp\left((1-\frac{1}{|\mathcal{S}| })\log\frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}(1-M)(\frac{t_{0}+K}{t })^{\frac{M}{|\mathcal{S}|}}\] \[\leq\sum_{t=t_{0}+1}^{t_{0}+K}\exp\left((1-\frac{1}{|\mathcal{S}| })\log\frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}-\sum_{t=t_{0}+1}^{t_{0} +K}\exp\left((1-\frac{1}{|\mathcal{S}|})\log\frac{T}{t}\right)\frac{2L}{\beta t |\mathcal{S}|}(M^{2})\] \[=\sum_{t=t_{0}+1}^{t_{0}+K}\exp\left((1-\frac{1}{|\mathcal{S}|}) \log\frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}-JM^{2},\]

where \(J\) is a finite value that depends on the training property of beginning iterations, namely \(K\) and \(t_{0}\). The last inequality is from \((\frac{t_{0}+K}{t})^{\frac{M}{|\mathcal{S}|}}\leq(1+M)\), when \(|\mathcal{S}|\) is sufficient enough.

Then, considering the all \(T\) steps, we have

\[\mathbb{E}\left[\|\theta_{T}-\theta^{\prime}_{T}\|\|\delta_{t_{0} }=0\right] \leq\Delta+\sum_{t=t_{0}+K+1}^{T}\left\{\Pi_{k=t+1}^{T}\exp\left( \frac{1}{t}-\frac{1}{t|\mathcal{S}|}\right)\right\}\frac{2L}{\beta t|\mathcal{ S}|}\] (41) \[\leq\sum_{t=t_{0}+1}^{T}\exp\left((1-\frac{1}{|\mathcal{S}|}) \log\frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}-JM^{2}\] \[=\frac{2L}{\beta|\mathcal{S}|}T^{1-\frac{1}{|\mathcal{S}|}}\sum_{ t=t_{0}+1}^{T}t^{-(1-\frac{1}{|\mathcal{S}|})-1}-JM^{2}\] \[\leq\frac{2L}{\beta|\mathcal{S}|}T^{1-\frac{1}{|\mathcal{S}|}} \frac{1}{1-\frac{1}{|\mathcal{S}|}}\left(t_{0}^{-(1-\frac{1}{|\mathcal{S}|})}- T^{-(1-\frac{1}{|\mathcal{S}|})}\right)-JM^{2}.\]

Thus, following a similar proof process as Theorem 3.3(2), we can obtain:

\[\mathbb{E}\left[\|\theta_{T}-\theta^{\prime}_{T}\|\|\delta_{t_{0}}=0\right] \leq\frac{2L}{\beta(|\mathcal{S}|-1)}(\frac{T}{t_{0}})-\frac{2L}{\beta(| \mathcal{S}|-1)}-JM^{2}.\] (42)

With \(t_{0}=1\), we have the generalization error under the non-convex case satisfies:

\[\mathbb{E}\left[|\mathcal{L}(u_{\theta_{T}},\bm{x})-\mathcal{L}(u_{\theta^{ \prime}_{T}},\bm{x})|\right]\leq\frac{C}{|\mathcal{S}|}+\frac{2L^{2}(T-1)}{ \beta(|\mathcal{S}|-1)}-JLM^{2}.\] (43)

### Proof for High-order Constraint Optimization (Corollary 3.6)

First, we would like to prove the following Lemma.

**Lemma A.4**.: _Suppose that \(\mathcal{L}\) is bounded by \(C\) for all \(\theta,\bm{x}\) and is \(L\)-Lipschitz and \(\beta\)-smooth for \(\theta\), then the first-order \(j\)-th dimension loss function \(\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}\) is also bounded by \(C\) for all \(\theta,\bm{x}\) and is \(2L\)-Lipschitz and \(2\beta\)-smooth for \(\theta\)._

Proof.: For the bounded property, with the non-negative property of loss function, we have:

\[\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta}, \bm{x})=\frac{\partial}{\partial x_{j}}\int_{\Omega_{r}}\mathcal{L}(u_{\theta },\bm{x}+\bm{\xi})\mathrm{d}\bm{\xi}=\int_{\Omega_{r}\setminus x_{j}} \mathcal{L}(u_{\theta},\bm{x}+\bm{\xi}_{r})-\mathcal{L}(u_{\theta},\bm{x}+\bm {\xi}_{0})\mathrm{d}\bm{\xi}\leq C,\] (44)

where \(\bm{\xi}_{r}=(\cdots,r,\cdots)\in\Omega_{t}\backslash x_{j}\) and \(\bm{\xi}_{0}=(\cdots,0,\cdots)\in\Omega_{t}\backslash x_{j}\).

As for the Lipschitz and smoothness, we can obtain the following inequalities:

Lipschitz: \[\|\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region }}(u_{\theta_{1}},\bm{x})-\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{ \mathrm{region}}(u_{\theta_{2}},\bm{x})\|\] \[=\|\int_{\Omega_{r}\setminus x_{j}}\mathcal{L}(u_{\theta_{1}}, \bm{x}+\bm{\xi}_{r})-\mathcal{L}(u_{\theta_{1}},\bm{x}+\bm{\xi}_{0})-\mathcal{ L}(u_{\theta_{2}},\bm{x}+\bm{\xi}_{r})-\mathcal{L}(u_{\theta_{2}},\bm{x}+\bm{\xi}_{0}) \mathrm{d}\bm{\xi}\|\] \[\leq\int_{\Omega_{r}\setminus x_{j}}\|\mathcal{L}(u_{\theta_{1}}, \bm{x}+\bm{\xi}_{r})-\mathcal{L}(u_{\theta_{2}},\bm{x}+\bm{\xi}_{r})\|+\| \mathcal{L}(u_{\theta_{1}},\bm{x}+\bm{\xi}_{0})-\mathcal{L}(u_{\theta_{2}}, \bm{x}+\bm{\xi}_{0})\|\mathrm{d}\bm{\xi}\] \[\leq 2L\|\theta_{1}-\theta_{2}\|\] Smoothness: \[\|\nabla_{\theta}\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^ {\mathrm{region}}(u_{\theta_{1}},\bm{x})-\nabla_{\theta}\frac{\partial}{ \partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{2}},\bm{x})\|\] \[=\|\nabla_{\theta}\int_{\Omega_{r}\setminus x_{j}}\mathcal{L}(u_ {\theta_{1}},\bm{x}+\bm{\xi}_{r})-\mathcal{L}(u_{\theta_{1}},\bm{x}+\bm{\xi}_ {0})-\mathcal{L}(u_{\theta_{2}},\bm{x}+\bm{\xi}_{r})-\mathcal{L}(u_{\theta_{ 2}},\bm{x}+\bm{\xi}_{0})\mathrm{d}\bm{\xi}\|\] \[\leq\int_{\Omega_{r}\setminus x_{j}}\|\nabla_{\theta}\mathcal{L}( u_{\theta_{1}},\bm{x}+\bm{\xi}_{r})-\nabla_{\theta}\mathcal{L}(u_{\theta_{2}},\bm{x}+ \bm{\xi}_{r})\|\mathrm{d}\bm{\xi}\] \[+\int_{\Omega_{r}\setminus x_{j}}\|\nabla_{\theta}\mathcal{L}( u_{\theta_{1}},\bm{x}+\bm{\xi}_{0})-\nabla_{\theta}\mathcal{L}(u_{\theta_{2}},\bm{x}+\bm{ \xi}_{0})\|\mathrm{d}\bm{\xi}\] \[\leq 2\beta\|\theta_{1}-\theta_{2}\|.\] (45)

Thus, \(\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}\) is also bounded by \(C\) for all \(\theta,\bm{x}\) and is \(2L\)-Lipschitz and \(2\beta\)-smooth for \(\theta\). 

Next, we will give the proof for Corollary 3.6.

Proof.: According to Lemma A.3, we have the gradient update operator \(G_{\alpha_{t},\bm{x}}^{\mathrm{region},x_{j}}\) for \(\frac{\partial}{\partial x_{j}}\mathcal{L}_{r}^{\mathrm{region}}\) satisfies the following inequality:

\[\|G_{\alpha_{t},\bm{x}}^{\mathrm{region},x_{j}}(\theta_{1})-G_{\alpha_{t},\bm{ x}}^{\mathrm{region},x_{j}}(\theta_{2})\|\leq(1+2\alpha_{t}\beta)\|\theta_{1}- \theta_{2}\|.\] (46)

Let \(M=\frac{|\Omega_{r}|}{|\Omega|}\), since \(\alpha_{t}\leq\frac{1}{2\beta t}\), we can rewrite the Eq. (37) as follows:

\[\begin{split}&\mathbb{E}\left[\|\theta_{t+1}-\theta_{t+1}^{ \prime}\|\|\delta_{t_{0}}=0\right]\\ &\leq(1-\frac{1}{|\mathcal{S}|})(1+\frac{1}{t})\mathbb{E}\left[\| \theta_{t}-\theta_{t}^{\prime}\|\right]+\frac{1}{|\mathcal{S}|}\left((1+ \frac{M}{t})\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\|\right]+\frac{2 L}{\beta}(1-M)\right),\end{split}\] (47)

where the second term is derived from Eq. (38) by substituting \(L\) to \(2L\) and \(\beta\) to \(2\beta\), which is:

\[\begin{split}\mathbb{E}\left[\|G_{\alpha_{t},\bm{x}}^{\mathrm{ region},x_{j}}(\theta_{t})-G_{\alpha_{t},\bm{x}^{\prime}}^{\mathrm{region},x_{j}}( \theta_{t}^{\prime})\|\right]&\leq(1+\frac{2\alpha_{t}\beta| \Omega_{r}|}{|\Omega|})\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime}\|\right]+4 \alpha_{t}L(1-\frac{|\Omega_{r}|}{|\Omega|})\\ &=(1+\frac{M}{t})\mathbb{E}\left[\|\theta_{t}-\theta_{t}^{\prime} \|\right]+\frac{2L}{\beta t}(1-M).\end{split}\] (48)

Thus, following the same derivation as Theorem 3.5, we have Corollary 3.6 holds.

Algorithm Analysis in Section 3.2

This section contains the proof for the theoretical analysis of our proposed algorithm in Section 3.2.

### Proof for Convergence Rate of RoPINN (Theorem 3.8)

The crux of proof is to take expectation for Monte Carlo sampling.

Proof.: From Taylor expansion, there exist \(\bm{x}^{\prime}\) such that:

\[\begin{split}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t+1}}, \bm{x})&=\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}}-\alpha_{ t}\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi}),\bm{x})\\ &=\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})-\alpha _{t}\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi})^{\mathsf{T}} \nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})\\ &+\frac{1}{2}(\alpha_{t}\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}))^{\mathsf{T}}\nabla_{\theta}^{2}\mathcal{L}_{r}^{\mathrm{region}}(u_ {\theta_{t}},\bm{x}^{\prime})(\alpha_{t}\nabla_{\theta}\mathcal{L}(u_{\theta_ {t}},\bm{x}))\\ &\leq\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})- \alpha_{t}\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi})^{\mathsf{ T}}\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})+ \frac{\alpha_{t}^{2}L^{2}H}{2}.\end{split}\] (49)

Taking expectations to \(\bm{\xi}\) on both sides, since \(\mathbb{E}[\nabla_{\theta}\mathcal{L}(u_{\theta_{t}},\bm{x}+\bm{\xi})]=\nabla_ {\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x}+\bm{\xi})\), we have:

\[\begin{split}\mathbb{E}\left[\mathcal{L}_{r}^{\mathrm{region}}(u _{\theta_{t+1}},\bm{x})\right]&\leq\mathbb{E}\left[\mathcal{L}_{r }^{\mathrm{region}}(u_{\theta_{t}},\bm{x})-\alpha_{t}\nabla_{\theta}\mathcal{L }(u_{\theta_{t}},\bm{x}+\bm{\xi})^{\mathsf{T}}\nabla_{\theta}\mathcal{L}_{r}^ {\mathrm{region}}(u_{\theta_{t}},\bm{x})+\frac{\alpha_{t}^{2}L^{2}H}{2}\right] \\ &=\mathbb{E}\left[\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})\right]-\alpha_{t}\mathbb{E}\left[\left\|\nabla_{\theta}\mathcal{L}_{ r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})\right\|^{2}\right]+\frac{\alpha_{t}^{2}L^{2}H}{2}.\end{split}\] (50)

Rearranging the terms and accumulating over \(T\) iterations, we have the following sum:

\[\begin{split}\sum_{t=0}^{T-1}\alpha_{t}\mathbb{E}\left[\left\| \nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})\right\| ^{2}\right]&\leq\sum_{t=0}^{T-1}\left(\mathbb{E}\left[\mathcal{L} _{r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})\right]-\mathbb{E}\left[\mathcal{L }_{r}^{\mathrm{region}}(u_{\theta_{t+1}},\bm{x})\right]\right)+\sum_{t=0}^{T-1} \frac{\alpha_{t}^{2}L^{2}H}{2}\\ &\leq\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{0}},\bm{x})- \mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{T}},\bm{x})+\frac{L^{2}H}{2}\sum_{ t=0}^{T-1}\alpha_{t}^{2}\\ &\leq\mathcal{L}_{r}^{\mathrm{region}}(u_{\theta_{0}},\bm{x})- \mathcal{L}_{r}^{\mathrm{region}}(u_{*},\bm{x})+\frac{L^{2}H}{2}\sum_{t=0}^{T- 1}\alpha_{t}^{2},\end{split}\] (51)

where \(u_{*}\) represents the global optimum. Here we run the gradient descent for a random number of iterations \(\tau\). For \(\tau=t\) iterations with probability:

\[\mathbb{P}(\tau=t)=\frac{\alpha_{t}}{\sum_{k=0}^{T-1}\alpha_{k}},\] (52)

Thus, with \(\alpha_{t}=\frac{1}{\sqrt{t+1}}\), we have the gradient norm is bounded by:

\[\begin{split}\mathbb{E}\left[\left\|\nabla_{\theta}\mathcal{L}_{ r}^{\mathrm{region}}(u_{\theta_{t}},\bm{x})\right\|^{2}\right]&= \left(\sum_{t=0}^{T-1}\alpha_{t}\right)^{-1}\sum_{t=0}^{T-1}\alpha_{t} \mathbb{E}\left[\left\|\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(u_{ \theta_{t}},\bm{x})\right\|^{2}\right]\\ &\leq\left(\sum_{t=0}^{T-1}\alpha_{t}\right)^{-1}\left(\mathcal{L} _{r}^{\mathrm{region}}(u_{\theta_{0}},\bm{x})-\mathcal{L}_{r}^{\mathrm{region}}( u_{*},\bm{x})+\frac{L^{2}H}{2}\sum_{t=0}^{T-1}\alpha_{t}^{2}\right)\\ &\lesssim(2\sqrt{T})^{-1}\left(\mathcal{L}_{r}^{\mathrm{region}}(u_ {\theta_{0}},\bm{x})-\mathcal{L}_{r}^{\mathrm{region}}(u_{*},\bm{x})+\frac{L^{2 }H}{2}\log(T+1)\right)\\ &=\mathcal{O}(\frac{1}{\sqrt{T}}).\end{split}\] (53)

[MISSING_PAGE_EMPTY:23]

### Proof for Region Optimization with Gradient Estimation Error (Theorem 3.12)

Convex settingFirstly, we would like to prove the convex case as follows.

Proof.: Similar to the proof of region optimization in Appendix A.3, at the \(t\)-th step, we can obtain the following equation:

\[\mathbb{E}\left[\left\|\theta_{t+1}-\theta_{t+1}^{\prime}\right\|\right] =(1-\frac{1}{|\mathcal{S}|})\mathbb{E}\left[\left\|G_{\alpha_{t}, \boldsymbol{x}}^{\mathrm{approx}}(\theta_{t})-G_{\alpha_{t},\boldsymbol{x}}^ {\mathrm{approx}}(\theta_{t}^{\prime})\right\|\right]+\frac{1}{|\mathcal{S}|} \mathbb{E}\left[\left\|G_{\alpha_{t},\boldsymbol{x}}^{\mathrm{approx}}(\theta_ {t})-G_{\alpha_{t},\boldsymbol{x}^{\prime}}^{\mathrm{approx}}(\theta_{t}^{ \prime})\right\|\right]\] (60)

where \(G_{\alpha,\boldsymbol{x}}^{\mathrm{approx}}(\theta)=\theta-\alpha\nabla_{ \theta}\mathcal{L}_{r}^{\mathrm{approx}}(\theta,\boldsymbol{x})=\theta- \alpha\nabla_{\theta}\mathcal{L}(\theta,\boldsymbol{x}+\boldsymbol{\xi}), \boldsymbol{\xi}\sim U(\Omega_{r})\). Suppose that we have sampled \(\boldsymbol{\xi},\boldsymbol{\xi}^{\prime}\in\Omega_{r}\), the second term on the right part can be bounded as follows:

\[\mathbb{E}\left[\left\|G_{\alpha_{t},\boldsymbol{x}}^{\mathrm{ approx}}(\theta_{t})-G_{\alpha_{t},\boldsymbol{x}^{\prime}}^{\mathrm{approx}}( \theta_{t}^{\prime})\right\|\right]\] (61) \[\leq\mathbb{E}\left[\left\|\theta_{t}-\alpha_{t}\nabla_{\theta} \mathcal{L}(\theta_{t},\boldsymbol{x}+\boldsymbol{\xi})-\theta_{t}^{\prime}- \alpha_{t}\nabla_{\theta}\mathcal{L}(\theta_{t}^{\prime},\boldsymbol{x}^{ \prime}+\boldsymbol{\xi}^{\prime})\right\|\right]\] \[\leq\mathbb{E}\left[\left\|\theta_{t}-\alpha_{t}\nabla_{\theta} \mathcal{L}_{r}^{\mathrm{region}}(\theta_{t},\boldsymbol{x})-\theta_{t}^{ \prime}-\alpha_{t}\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(\theta_{t} ^{\prime},\boldsymbol{x}^{\prime})\right\|\right]\] \[+\mathbb{E}\left[\left\|\alpha_{t}\nabla_{\theta}\mathcal{L}( \theta_{t},\boldsymbol{x}+\boldsymbol{\xi})-\alpha_{t}\nabla_{\theta} \mathcal{L}_{r}^{\mathrm{region}}(\theta_{t},\boldsymbol{x})\right\|\right]\] \[+\mathbb{E}\left[\left\|\alpha_{t}\nabla_{\theta}\mathcal{L}( \theta_{t},\boldsymbol{x}^{\prime}+\boldsymbol{\xi}^{\prime})-\alpha_{t} \nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(\theta_{t}^{\prime}, \boldsymbol{x}^{\prime})\right\|\right]\] \[\leq\mathbb{E}\left[\left\|\theta_{t}-\theta_{t}^{\prime}\right\| \right]+2\alpha_{t}L(1-\frac{|\Omega_{r}|}{|\Omega|})+2\alpha_{t}\mathcal{E}_{r,\mathrm{grad}}\] (Based on Eq. ( 31 )) \[=\mathbb{E}\left[\left\|\theta_{t}-\theta_{t}^{\prime}\right\| \right]+2\alpha_{t}\left(L(1-\frac{|\Omega_{r}|}{|\Omega|})+\mathcal{E}_{r, \mathrm{grad}}\right)\]

Thus, recursively accumulating the residual at the \(t\)-th step, we have:

\[\mathcal{E}_{\mathrm{gen}}\leq\left(L(1-\frac{|\Omega_{r}|}{|\Omega|})+ \mathcal{E}_{r,\mathrm{grad}}\right)\frac{2L}{|\mathcal{S}|}\sum_{t=1}^{T} \alpha_{t}.\] (62)

Non-convex settingSimilarly, we can prove the non-convex setting as follows.

Proof.: It is easy to prove that \(\mathcal{L}_{r}^{\mathrm{approx}}(\theta,\boldsymbol{x})\) is still \(L\)-Lipchitz-\(\beta\)-smoothness for \(\theta\). For clarity, we define that \(M=\frac{|\Omega_{r}|}{|\Omega|}\). Thus, based on Eq. (38), we have the following derivations.

If \(\mathbb{E}(\delta_{t})\leq\frac{2L}{\beta}-\frac{2}{\beta M}\mathcal{E}_{r, \mathrm{grad}}\), we have:

\[\mathbb{E}\left[\|\theta_{t+1}-\theta_{t+1}^{\prime}\|\|\delta_{t _{0}}=0\right]\] (63) \[\leq(1-\frac{1}{|\mathcal{S}|})(1+\frac{1}{t})\mathbb{E}\left[\| \theta_{t}-\theta_{t}^{\prime}\|\right]+\frac{1}{|\mathcal{S}|}\mathbb{E} \left[\|G_{\alpha_{t},\boldsymbol{x}}^{\mathrm{approx}}(\theta_{t})-G_{\alpha _{t},\boldsymbol{x}^{\prime}}^{\mathrm{approx}}(\theta_{t}^{\prime})\|\right]\] \[\leq(1-\frac{1}{|\mathcal{S}|})(1+\frac{1}{t})\mathbb{E}\left[\| \theta_{t}-\theta_{t}^{\prime}\|\right]\] \[+\frac{1}{|\mathcal{S}|}\left(\mathbb{E}\left[\|G_{\alpha_{t}, \boldsymbol{x}}^{\mathrm{region}}(\theta_{t})-G_{\alpha_{t},\boldsymbol{x}^{ \prime}}^{\mathrm{region}}(\theta_{t}^{\prime})\|\right]\right)\] \[+\frac{1}{|\mathcal{S}|}\left(\mathbb{E}\left[\|\alpha_{t} \nabla_{\theta}\mathcal{L}(\theta_{t},\boldsymbol{x}+\boldsymbol{\xi})- \alpha_{t}\nabla_{\theta}\mathcal{L}_{r}^{\mathrm{region}}(\theta_{t}, \boldsymbol{x})\|\right]\right)\] \[\leq(1+\frac{1}{t}-\frac{1-M}{t|\mathcal{S}|})\mathbb{E}[\delta _{t}]+\frac{2\alpha_{t}}{|\mathcal{S}|}\left(L(1-M)+\mathcal{E}_{r,\mathrm{ grad}}\right)\] \[\leq\exp\left(\frac{1}{t}-\frac{1-M}{t|\mathcal{S}|}\right) \mathbb{E}[\delta_{t}]+\frac{2}{\beta t|\mathcal{S}|}\left(L(1-M)+\mathcal{E}_{r,\mathrm{grad}}\right).\]Otherwise, we still consider the following inequality:

\[\mathbb{E}\left[||\theta_{t+1}-\theta^{\prime}_{t+1}||\delta_{t_{0}}=0\right]\leq \exp\left(\frac{1}{t}-\frac{1}{t|\mathcal{S}|}\right)\mathbb{E}[\delta_{t}]+ \frac{2L}{\beta t|\mathcal{S}|},\] (64)

Suppose that at the first \(K^{\prime}\) steps \(\mathbb{E}[\delta_{t_{0}+K^{\prime}}]\leq\frac{2L}{\beta}-\frac{2}{\beta M} \mathcal{E}_{r,\mathrm{grad}}\).

Accumulating the above in equations recursively, we have the generalization error bound accumulated to the first \(K^{\prime}\) steps as follows:

\[\Delta =\sum_{t=t_{0}+1}^{t_{0}+K^{\prime}}\left\{\Pi_{k=t+1}^{t_{0}+K^{ \prime}}\mathrm{exp}\left(\frac{1}{t}-\frac{1-M}{t|\mathcal{S}|}\right)\Pi_{k= t_{0}+K^{\prime}+1}^{T}\mathrm{exp}\left(\frac{1}{t}-\frac{1}{t|\mathcal{S}|} \right)\right\}\frac{2}{\beta t|\mathcal{S}|}\left(L(1-M)+\mathcal{E}_{r, \mathrm{grad}}\right)\] (65) \[\leq\sum_{t=t_{0}+1}^{t_{0}+K^{\prime}}\exp\left((1-\frac{1}{| \mathcal{S}|})\log\frac{T}{t}\right)\frac{2}{\beta t|\mathcal{S}|}\left(L(1-M )+\mathcal{E}_{r,\mathrm{grad}}\right)(\frac{t_{0}+K^{\prime}}{t})^{\frac{M} {|\mathcal{S}|}}\] \[\leq\sum_{t=t_{0}+1}^{t_{0}+K^{\prime}}\exp\left((1-\frac{1}{| \mathcal{S}|})\log\frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}-\sum_{t=t_ {0}+1}^{t_{0}+K^{\prime}}\exp\left((1-\frac{1}{|\mathcal{S}|})\log\frac{T}{t} \right)\frac{2}{\beta t|\mathcal{S}|}\left(LM^{2}+\mathcal{E}_{r,\mathrm{grad }}(1+M)\right)\] \[=\sum_{t=t_{0}+1}^{t_{0}+K^{\prime}}\exp\left((1-\frac{1}{| \mathcal{S}|})\log\frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}-J^{\prime} LM^{2}+J^{\prime}\mathcal{E}_{r,\mathrm{grad}}(1+M),\]

where \(J^{\prime}\) is a finite value that depends on the training property of beginning iterations, namely \(K^{\prime}\) and \(t_{0}\). The last inequality is from \((\frac{t_{0}+K^{\prime}}{t})^{\frac{M}{|\mathcal{S}|}}\leq(1+M)\), when \(|\mathcal{S}|\) is sufficient enough.

Then, considering the all \(T\) steps, we have

\[\mathbb{E}\left[||\theta_{T}-\theta^{\prime}_{T}||\delta_{t_{0}}=0\right]\] (66) \[\leq\Delta+\sum_{t=t_{0}+K+1}^{T}\left\{\Pi_{k=t+1}^{T}\mathrm{ exp}\left(\frac{1}{t}-\frac{1}{t|\mathcal{S}|}\right)\right\}\frac{2L}{\beta t |\mathcal{S}|}\] \[\leq\sum_{t=t_{0}+1}^{T}\exp\left((1-\frac{1}{|\mathcal{S}|})\log \frac{T}{t}\right)\frac{2L}{\beta t|\mathcal{S}|}-J^{\prime}M^{2}+J^{\prime} \mathcal{E}_{r,\mathrm{grad}}(1+M)\hskip 28.452756pt(\sum_{k=t+1}^{T}\frac{1}{k} \leq\log\frac{T}{t})\] \[=\frac{2L}{\beta|\mathcal{S}|}T^{1-\frac{1}{|\mathcal{S}|}}\sum_{ t=t_{0}+1}^{T}t^{-(1-\frac{1}{|\mathcal{S}|})-1}-J^{\prime}M^{2}+J^{\prime} \mathcal{E}_{r,\mathrm{grad}}(1+M)\] \[\leq\frac{2L}{\beta|\mathcal{S}|}T^{1-\frac{1}{|\mathcal{S}|}} \frac{1}{1-\frac{1}{|\mathcal{S}|}}\left(t_{0}^{-(1-\frac{1}{|\mathcal{S}|})} -T^{-(1-\frac{1}{|\mathcal{S}|})}\right)-J^{\prime}M^{2}+J^{\prime}\mathcal{E }_{r,\mathrm{grad}}(1+M).\]

Next, following the proof in Appendix A.3, we can obtain the generalization bound as follows:

\[\mathbb{E}\left[|\mathcal{L}(u_{\theta_{T}},\bm{x})-\mathcal{L}(u_{\theta^{ \prime}_{T}},\bm{x})|\right]\leq\frac{C}{|\mathcal{S}|}+\frac{2L^{2}(T-1)}{ \beta(|\mathcal{S}|-1)}-J^{\prime}LM^{2}+J^{\prime}\mathcal{E}_{r,\mathrm{ grad}}(1+M).\] (67)

Note that \(K^{\prime}\) does not exist when \(\frac{2L}{\beta}<\frac{2}{\beta M}\mathcal{E}_{r,\mathrm{grad}}\), then \(J^{\prime}=0\), which corresponds to the situation that the region size is too large and brings serious gradient estimation error. Introducing "region" cannot bring a better generalization bound in this case.

Implementation Details

This section provides experiment details, including **benchmarks, metrics** and **implementations**.

### Benchmarks

To comprehensively test our algorithm, we include the following four benchmarks. The first three benchmarks cover three typical PDEs (plotted in Figure 7), which are widely used in exploring the PINN optimization [24, 37]. The last one is an advanced comprehensive benchmark with 20 different PDEs. Here are the details.

1D-ReactionThis problem is a one-dimensional non-linear ODE, which describes the chemical reactions. The concrete equation that we studied here can be formalized as follows:

\[\begin{split}\frac{\partial u}{\partial t}-\rho u(1-u)=0,& \,x\in(0,2\pi),t\in(0,1),\\ u(x,0)=\exp\left(-\frac{(x-\pi)^{2}}{2(\pi/4)^{2}}\right),& \,x\in[0,2\pi],\\ u(0,t)=u(2\pi,t),&\,t\in[0,1].\end{split}\] (68)

The analytical solution to this problem is \(u(x,t)=\frac{h(x)e^{\rho t}}{h(x)e^{\rho t}+1-h(x)}\) and \(h(x)=\exp\left(-\frac{(x-\pi)}{2(\pi/4)^{2}}\right)\). In our experiments, we set \(\rho=5\). This problem is previously studied as "PINN failure mode" [24], which is because of the non-linear term of the equation [29]. Besides, as shown in Figure 7(a), it contains sharp boundaries for the center high-value area, which is also hard to learn for deep models.

Following experiments in PINNsFormer [58], we uniformly sampled 101 points for initial state \(\Omega_{0}\) and boundary \(\partial\Omega\) and a uniform grid of 101\(\times\)101 mesh points for the residual domain \(\Omega\). For evaluation, we employed a 101\(\times\)101 mesh within the residual domain \(\Omega\). This strategy is also adopted for 1D-Wave and Convection experiments.

1D-WaveThis problem presents a hyperbolic PDE that is widely studied in acoustics, electromagnetism, and fluid dynamics [1]. Concretely, the PDE can be formalized as follows:

\[\begin{split}\frac{\partial^{2}u}{\partial t^{2}}-4\frac{ \partial^{2}u}{\partial x^{2}}=0,&\,x\in(0,1),t\in(0,1),\\ u(x,0)=\sin(\pi x)+\frac{1}{2}\sin(\beta\pi x),&\, x\in[0,1],\\ \frac{\partial u(x,0)}{\partial t}=0,&\,x\in[0,1],\\ u(0,t)=u(1,t)=0,&\,t\in[0,1].\end{split}\] (69)

The analytic solution for this PDE is \(u(x,t)=\sin(\pi x)\cos(2\pi t)+\frac{1}{2}\sin(\beta\pi x)\cos(2\beta\pi t)\). We set \(\beta=3\) for our experiments. As presented in Figure 7(b), the solution is smoother than the other two datasets, thereby easier for deep models to solve in some aspects. However, the equation contains second-order derivative terms, which also brings challenges in automatic differentiation. That is why gPINN [55] fails in this task (Table 2).

Figure 7: Visualization of the solution \(u\) for the first three benchmarks.

ConvectionThis problem is also a hyperbolic PDE that can be used to model fluid, atmosphere, heat transfer and biological processes [41]. The concrete PDE that we studied in this paper is:

\[\begin{split}\frac{\partial u}{\partial t}+\beta\frac{\partial u}{ \partial t}=0,&\,x\in(0,2\pi),t\in(0,1),\\ u(x,0)=\sin(x),&\,x\in[0,2\pi],\\ u(0,t)=u(2\pi,t),&\,t\in[0,1].\end{split}\] (70)

The analytic solution for this PDE is \(u(x,t)=\sin(x-\beta t)\), where \(\beta\) is set as 50 in our experiments. Note that although the final solution seems to be quite simple, it is difficult for PINNs in practice due to the highly complex and high-frequency patterns. And the previous research [24] has shown that the loss landscape of the Convection equation contains many hard-to-optimize sharp cones.

PINNacleThis benchmark [12] is built upon the DeepXDE [30], consisting of a wide range of PDEs and baselines. In their paper, the authors included 20 different PDE-solving tasks, covering diverse phenomena in fluid dynamics, heat conduction, etc and including PDEs with high dimensions, complex geometries, nonlinearity and multiscale interactions. To ensure a comprehensive evaluation, we also benchmark RoPINN with PINNacle.

During our experiments, we found that there are several subtasks that none of the previous methods can solve, such as the 2D Heat equation with long time (Heat 2d-LT), 2D Navier-Stokes equation with long time (NS 2d-LT), 2D Wave equation with long time (Wave 2d-MS) and Kuramoto-Sivashinsky equation (KS). In addition to the challenges of high dimensionality and complex geometry mentioned by PINNacle, we discover unique challenges in these tasks caused by long periods and high-order derivatives of governed PDEs, making them extremely challenging for current PINNs. To solve these problems, we might need more powerful PINN backbones. Since we mainly focus on the PINN training paradigm, we omit the abovementioned 4 tasks to avoid the meaningless comparison and experiment with the left 16 tasks. Our datasets are summarized in Table 4.

\begin{table}
\begin{tabular}{c c|c c c c c} \hline \hline \multicolumn{2}{c}{PDE} & \multicolumn{1}{c}{Dimension} & Order & \(N_{\text{train}}\) & \(N_{\text{test}}\) & Key Equations \\ \hline \multirow{2}{*}{Burges} & 1d-C & 1D+Time & 2 & 16384 & 12288 & \(\frac{\partial u}{\partial t}+\bm{u}\cdot\nabla\bm{u}-\nu\Delta\bm{u}=0\) \\  & 2d-C & 2D+Time & 2 & 98308 & 82690 & \(\frac{\partial u}{\partial t}+\bm{u}\cdot\nabla\bm{u}-\nu\Delta\bm{u}=0\) \\ \hline \multirow{4}{*}{Poisson} & 2d-C & 2D & 2 & 12288 & 10240 & \(-\Delta\bm{u}=0\) \\  & 2d-CG & 2D & 2 & 12288 & 10240 & \(-\Delta\bm{u}+k^{2}\bm{u}=f(x,y)\) \\  & 3d-CG & 3D & 2 & 49152 & 40960 & \(-\mu_{i}\Delta\bm{u}+k^{2}\bm{u}=f(x,y,z),i=1,2\) \\  & 2d-MS & 2D & 2 & 12288 & 10329 & \(-\nabla(a(x)\nabla\bm{u})=f(x,y)\) \\ \hline \multirow{4}{*}{Heat} & 2d-VC & 2D+Time & 2 & 65536 & 49189 & \(\frac{\partial u}{\partial t}-\nabla(a(x)\nabla\bm{u})=f(x,t)\) \\  & 2d-MS & 2D+Time & 2 & 65536 & 49189 & \(\frac{\partial u}{\partial t}-\frac{1}{(500x)^{2}}\bm{u}_{xx}-\frac{1}{\pi^{2} }\bm{u}_{yy}=0\) \\  & 2d-CG & 2D+Time & 2 & 65536 & 49152 & \(\frac{\partial u}{\partial t}-\Delta\bm{u}=0\) \\ \hline \multirow{2}{*}{NS} & 2d-C & 2D & 2 & 14337 & 12378 & \multirow{2}{*}{\(\bm{u}\cdot\nabla\bm{u}+\nabla p-\frac{1}{\hbar c}\Delta\bm{u}=0,\nabla \cdot\bm{u}=0\)} \\  & 2d-CG & 2D & 2 & 14055 & 12007 & \(\bm{u}\cdot\nabla\bm{u}+\nabla p-\frac{1}{\hbar c}\Delta\bm{u}=0,\nabla\cdot \bm{u}=0\) \\ \hline \multirow{2}{*}{Wave} & 1d-C & 1D+Time & 2 & 12288 & 10329 & \(\bm{u}_{tt}-4\bm{u}_{xx}=0\) \\  & 2d-CG & 2D+Time & 2 & 49170 & 42194 & \(\left[\nabla^{2}-\frac{1}{c(x)}\frac{\partial^{2}}{\partial t^{2}}\right]u(x,t )=0\) \\ \hline \multirow{2}{*}{Chaotic} & GS & 2D+Time & 2 & 65536 & 61780 & \(\bm{u}_{t}=\varepsilon_{1}\Delta\bm{u}+b(1-\bm{u})-\bm{u}\bm{v}^{2}\) \\  & \multirow{2}{*}{PNd} & 5D & 2 & 49152 & 67241 & \(-\Delta\bm{u}=\frac{\pi^{2}}{4}\sum_{i=1}^{n}\sin\left(\frac{\pi}{2}x_{i}\right)\) \\ \cline{1-1}  & HNd & 5D+Time & 2 & 65537 & 49152 & \(\frac{\partial\bm{u}}{\partial t}=k\Delta\bm{u}+f(x,t)\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Details of datasets in PINNacle [12] (16 different PDEs included in our experiments), including the dimension of inputs, highest order of PDEs, number of train/test points and concrete equations. Here we only present the simplified PDE formalizations for intuitive understanding. More detailed descriptions of PDE type and coefficient meanings can be found in their paper [12].

### Metrics

In our experiments, we adopt the following three metrics. Training loss, rMAE and rMSE. And the training loss has been defined in Eq. (2). Here are the calculations for rMSE and rMAE:

\[\text{rMAE:}\;\sqrt{\frac{\sum_{\bm{x}\in\mathcal{S}}|u_{\theta}(\bm{x})-u_{*}( \bm{x})|}{\sum_{\bm{x}\in\mathcal{S}}|u_{*}(\bm{x})|}}\quad\text{rMSE:}\; \sqrt{\frac{\sum_{\bm{x}\in\mathcal{S}}\left(u_{\theta}(\bm{x})-u_{*}(\bm{x}) \right)^{2}}{\sum_{\bm{x}\in\mathcal{S}}\left(u_{*}(\bm{x})\right)^{2}}},\] (71)

where \(u_{*}\) denotes the ground truth solution. Note that the model output and ground truth can be negative and positive, respectively. Thus, these two metrics could be larger than 1.

### Implementations

For classical base models PINN [36], QRes [3] and FLS [50], we adopt the conventional configuration from previous papers [58]. As for the latest model PINNsFormer [58] and KAN [28], we use their official code. Next, we will detail the implementations of optimization algorithms.

RoPINNAs we described in the main text, we set the initial region size \(r=10^{-4}\), past iteration number \(T_{0}\in\{5,10\}\) and only sample 1 point for each region at each iteration for all datasets. The corresponding analyses have been included in Figure 2 for \(r\), Figure 3 for sampling points and Appendix D.1 for \(T_{0}\) to demonstrate the algorithm property under different hyperparameter settings.

In addition, our formalization for region optimization in Eq. (8) only involves the equation, initial and boundary conditions, where we can still calculate their loss values after random sampling in the extended region. This definition perfectly matches the setting of 1D-Reaction, 1D-Wave and Convection. However, in PINNacle [12], some tasks also involve the data loss term, such as the inverse problem (Appendix D.2), which means we can only obtain the correct values for several observed or pre-calculated points. Since these points are pre-selected, we cannot obtain their new values after sampling. Thus, we do not apply region sampling to these points in our experiments. Actually, the data loss term only involves the forward process of deep models, which is a pure data-driven paradigm and is distinct from the other PDE-derived terms in PINNs. Therefore, the previous methods, gPINN and vPINN, also do not consider the data loss term in their algorithms.

gPINNFor the first three benchmarks, we add the first-order derivatives for spatial and temporal dimensions as the regularization term. We also search the weights of regularization terms in \(\{1,0.1,0.01\}\) and report the best results. As for the PINNacle, we report the results of canonical PINN following their paper [12] and experiment with other base models by only replacing the model.

vPINNWe follow the code base in PINNacle, and implement it to the first three benchmarks. The test functions are set as Legendre polynomials and the test function number is set as \(5\). The number of points used to compute the integral within domain \(\Omega\) is set as 10, and the number of grids is set differently for each subtask, with values of {4, 8, 16, 32} for PINNacle and the same to other baselines for the first three benchmarks.

Other baselinesIn Table 3 of the main text, we also experiment with the loss-reweighting method NTK [47] and data-resampling method RAR [51]. For NTK, we follow their official code and recalculate the neural tangent kernel to update loss weights every 10 iterations. And the kernel size is set as 300. As for RAR, we use the _residual-based adaptive refinement with distribution_ algorithm.

## Appendix D Additional Results

In this section, we provide more results as a supplement to the main text, including additional hyperparameter analysis, new experiments and more showcases.

### Hyperparameter Sensitivity on \(T_{0}\)

As we stated in Algorithm 1, we adopt the gradient variance of past \(T_{0}\) iterations to approximate the sampling error defined in Theorem 3.9. In our experiments, we choose \(T_{0}\) from \(\{5,10\}\), which canachieve consistently good and stable performance among different benchmarks and PDEs. To analyze the effect of this hyperparameter, we further add experiments with different choices in Figure 8.

As shown in Figure 8, we can find that under all the choices in \(\{1,5,10,15,20,25,30\}\), RoPINN performs better than the vanilla PINN. Specifically, in both 1D-Reaction and 1D-Wave (Figure 8(a-b)), the model performs quite stable under different choices of \(T_{0}\). As for Convection in Figure 8(c-d), the influence of \(T_{0}\) is relatively significant in PINN. This may caused by the deficiency of PINN in solving Convection, where all the PINN-based experiments fail to generate an accurate solution for Convection (rMSE\(>\)0.5, Table 2). If we adopt a more powerful base model, such as PINNsFormer [58], this sensitivity will be alleviated. Also, it is worth noticing that, even though in Convection, RoPINN surpasses the vanilla PINN under all hyperparameter settings of \(T_{0}\).

Besides, we can observe that the model performance slightly decreases when we set \(T_{0}\) with a relatively large value. This may come from the difference between parameters \(\theta_{t}\) and \(\theta_{t+29}\), which will make the gradient variance approximation less reliable (Eq. (58) in the Theorem 3.11 proof).

### Experiments with Data Loss (Inverse Problem)

As we stated in the implementations (Appendix C.3), RoPINN can also be applied to tasks with data loss. Here we also include an inverse problem in PINNacle to testify to the performance of RoPINN in this case, which requires the model to reconstruct the diffusion coefficients of the Poisson equation from observations on 2500 uniform grids with additional Gaussian noise.

As presented in Table 5, in this task, RoPINN can also boost the performance of PINN with over 10% in the rMSE metric and outperform the other baselines (gPINN and vPINN) that cannot bring improvements. Note that in this experiment, we failed to reproduce the performance of vPINN reported by PINNacle. Thus, we report the results of vPINN by directly running the official code in PINNacle.

### Standard Deviations

Considering the limited resources, we repeat all the experiments on the first three typical benchmarks and our method on the PINNacle three times and other experiments one time. The official paper of PINNacle has provided the standard deviations for PINN, gPINN and vPINN on all benchmarks.

We summarize the standard deviations of PINN in Table 6. As for other base models, the standard deviations of FLS, QRes and KAN are within 0.005 on 1D-Wave and Convection, and within 0.001 for 1D-Reaction. PINNsFormer's standard deviations are smaller than 0.001 for all three benchmarks.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline rMSE\(\pm\)Standard Deviations & 1D-Reaction & 1D-Wave & Convection \\ \hline PINN [36] & 0.981\(\pm\)5e-4 & 0.335\(\pm\)1e-3 & 0.840\(\pm\)5e-4 \\ +gPINN [55] & 0.978\(\pm\)3e-4 & 0.399\(\pm\)3e-3 & 0.935\(\pm\)3e-3 \\ +vPINN [18] & 0.982\(\pm\)3e-3 & 0.173\(\pm\)1e-3 & 0.743\(\pm\)2e-3 \\ \hline +RoPINN (Ours) & **0.095\(\pm\)**8e-4 & **0.064\(\pm\)**1e-3 & **0.720\(\pm\)**2e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Standard deviations for canonical PINN on three typical benchmarks. The confidence for RoPINN achieving the best performance is over 99% in all three benchmarks.

Figure 8: Hyperparameter analyses for \(T_{0}\) in RoPINN based on PINN [36] and PINNsFormer [58] on different benchmarks. We change \(T_{0}\) in \(\{1,5,10,15,20,25,30\}\) and record the rMSE.

### More Showcases

As a supplement to the main text, we provide the showcases of RoPINN in Figure 9. From these showcases, we can observe that RoPINN can consistently boost the model performance and benefit the solving process of boundaries, discontinuous phases and periodic patterns.

### Experiments with Advanced Quadrature Methods

In our implementation, RoPINN employs a simple Monte Carlo sampling to approximate integral. Obviously, we can adopt more advanced quadrature methods, such as Gaussian quadrature [16]. Thus, we also experiment with 2D space Gaussian quadrature, which requires square number points and the one-point-sampling situation will degenerate to the center value. As shown in Table 7, we can find that under our official setting (only sampling one point), Monte Carlo is better, while Gaussian quadrature is better in more points. Note that although Gaussian quadrature has the potential to achieve better performance, sampling more points may contradict our motivation of boosting PINNs without extra backpropagation or gradient calculation. Thus, we choose the Monte Carlo method, which works better under high-efficiency settings.

## Appendix E Full Results on PINNacle

In Table 2 of the main text, due to the context limitation, we only present the proportion of improved tasks over the total tasks. Here we provide the complete results for 5 based models for PINNacle (16 different tasks) in Table 8 and Table 9, where we can have the following observations:

* _RoPINN presents favorable generality in varied PDEs and base models._ As we described in Table 4, this benchmark contains of extensive physics phenomena. It is impressive that our proposed RoPINN can boost the performance of such extensive base models on a wide scope of PDEs, highlighting the generalizability of our algorithm.
* _RoPINN is numerically stable and efficient for computation._ As a training paradigm, RoPINN does not require extra gradient calculation and also does not add sampled points, which makes the algorithm computation efficient. In contrast, other baselines may generate poor results or encounter NaN or OOM problems in some PDEs.

\begin{table}
\begin{tabular}{l|c c} \hline \hline PINN+RoPINN & Monte Carlo & Gaussian \\ \hline Sample 1 Point & **0.095** & 0.109 \\ Sample 4 Points & 0.066 & **0.059** \\ Sample 9 Points & 0.033 & **0.030** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison between Monte Carlo approximation (_Monte Carlo_) and Gaussian quadrature (_Gaussian_) on 1D Reaction. rMSE is recorded.

Figure 9: Showcases of RoPINN on the first three datasets based on PINN and PINNsFormer.

\begin{table}
\begin{tabular}{c c c c|c c|c c|c c} \hline \hline (Part I) & PDE & \multicolumn{2}{c}{Vanilla} & \multicolumn{2}{c}{gPINN [55]} & \multicolumn{2}{c}{vPINN [18]} & \multicolumn{2}{c}{RoPINN (Ours)} \\ \cline{3-11}  & & rMAE & rMSE & rMAE & rMSE & rMAE & rMSE & rMAE & rMSE \\ \hline \multirow{8}{*}{\begin{tabular}{} \end{tabular} } & Bures & 1d-C & 1.1e-2 & 3.3e-2 & 3.7e-4 & 5.1e-1 & 4.0e-2 (2.27e.2%) & 3.5e-1(-952.3%) & **9.1e-3**(15.8\%) & **1.4e-2**(5.6\%) \\  & Bures & 2d-C & 4.5e-1 & 5.2e-1 & 4.9e-1(-8.7\%) & 5.4e-1(-3.8\%) & 6.6e-1(-46.4\%) & 6.4e-1(-23.0\%) & **4.3e-1**(-4.0\%) & **4.9e-15**(3.3\%) \\ \cline{2-11}  & & 2d-C & 7.5e-1 & 6.8e-1 & 7.7e-1(-3.0\%) & 7.0e-1(-4.0\%) & 4.6e-1(-38.0\%) & **4.9e-1**(27.4\%) & **4.1e-1**(4.49\%) & 6.6e-1(-12.5\%) \\  & Poisson & 2d-C & 5.4e-1 & 6.6e-1 & 7.4e-1(-3.7\%) & 7.9e-1(-2.0\%) & **2.4e-1**(54.7\%) & **2.9e-1(56.6\%) & 4.1e-1(-24.1\%) & 6.0e-18.1\%) \\  & Poisson & 3d-C & 4d-C & 2.4e-1 & 5.0e-1 & 4.3e-1(-4.2\%) & 5.2e-1(-3.5\%) & 8.0e-1(-4.7\%) & 7.4e-1(-4.6\%) & 7.4e-1(-1.9\%) & **4.6e-1**(-8.7\%) \\  & & 2d-MS & 7.8e-1 & 6.4e-1 & 6.7e-1(-13.2\%) & 6.2e-1(-3.9\%) & 9.6e-1(-23.6\%) & 7.9e-1(-2.5\%) & 7.7e-1(-0.0\%) & 6.4e-10.3\%) \\ \cline{2-11}  & & 2d-C & 1.2e+0 & 9.8e-1 & 1.0e+1 & 1.0e+1 & 8.8e-1(26.9\%) & 9.4e-1(-4.8\%) & **8.7e-1**(27.6\%) & **7.9e-1**(19.7\%) \\  & Heat & 2d-MS & 4.7e-2 & 6.9e-2 & 1.0e+0 & 8.5e-1 & 9.3e-1 & 9.3e-1 & **4.4e-2**(6.4\%) & **3.4e-2**(5.1\%) \\  & & 2d-CG & 2.7e-2 & 2.3e-2 & 1.9e-1(-58.9\%) & 2.1e-1(-17.3\%) & 3.1e+0 & 9.3e-1 & **1.5e-2**(43.5\%) & **2.0e-2**(12.6\%) \\ \cline{2-11}  & NS & 2d-C & 6.1e-2 & 5.1e-2 & 6.4e-1(95.6\%) & 4.9e-1(-617.5\%) & 2.0e-1(-22.2\%) & 2.9e-1(-17.8\%) & **4.1e-2**(32.2\%) & **4.2e-2**(16.9\%) \\  & 2d-CG & 1.8e-1 & 1.1e-1 & 4.2e-1(-34.5\%) & 2.9e-1(-167.6\%) & 9.9e-1(-45.6\%) & 9.9e-1(-81.2\%) & **1.5e-1**(19.0\%) & **9.8e-2**(12.5\%) \\ \cline{2-11}  & Wave & 1d-C & 5.5e-1 & 5.5e-1 & 7.0e-1(-27.0\%) & 7.2e-1(-31.9\%) & 1.4e+0(-155.3\%) & 8.4e-1(-53.5\%) & **3.8e-1**(31.1\%) & **3.9e-1**(28.0\%) \\  & 2d-CG & 2.3e+01 & 6.4e-0 & 9.9e-1(56.6\%) & 1.0e+0(-037.4\%) & 1.1e+0(-032.8\%) & 8.0e-1(50.8\%) & **7.1e-1**(168.8\%) & **7.9e-1**(15.3\%) \\ \cline{2-11}  & Chaotic & GS & 2.1e-2 & 9.4e-2 & 3.4e-2(-6.1\%) & 9.5e-2(-1.0\%) & 8.9e-1 & 1.2e+0 & **2.1e-2**(2.1\%) & **9.3e-2**(0.4\%) \\ \cline{2-11}  & High-PNd & 1.2e-3 & 1.1e-3 & 2.6e-3(-1.9\%) & 2.7e-3(-17.7\%) & NAN & NN & **8.7e-4**(42.9\%) & **6.4e-4**(3.6\%) \\  & dim & HN & 1.2e-2 & 5.3e-3(-36.0\%) & 4.6e-3(-313.6\%) & NAN & NN & **5.6e-4**(95.5\%) & **7.3e-4**(8.2\%) \\ \cline{2-11}  & **Proportion of improved tasks** & 18.8\% & 18.9\% & 25.0\% & 25.0\% & 25.0\% & 9.3\% & 8.6\% & 10.0\% \\ \hline \multirow{8}{*}{
\begin{tabular}{} \end{tabular} } & Bures & 2d-C & 5.8e-3 & 2.0e-2 & 3.6e-1 & 5.1e-2 & 2.0e-2(-24.0\%) & 8.1e-2(-30.3\%) & **5.7e-3**(4.4\%) & **1.8e-2**(6.8\%) \\  & Bures & 2d-C & 3.2e-1 & 4.8e-1 & 4.9e-1(-53.1\%) & 5.4e-1(-11.1\%) & 1.1e+0(-326.9\%) & 1.3e+0(-161.2\%) & **3.2e-1**(0.3\%) & **4.7e-1**(18.8\%) \\  & & 2d-CG & 2.9e-1 & 6.8e-1 & 7.6e-1(-159.4\%) & 7.2e-1(-5.9\%) & 3.2e-1(-7.6\%) & **3.0e-1(55.6\%)** & **2.9e-1**(0.6\%) & 7.0e-1(-3.7\%) \\  & Poisson & 2d-CG & 3.1e-1 & 7.4e-1 & 7.3e-1(-136.9\%) & 7.8e-1(-6.0\%) & 6.4e-1(-108.9\%) & 7.3e-1(0.8\%) & **2.9e-1**(4.0\%) & **7.2e-1**(2.5\%) \\  & Poisson & 3d-CG & 9.0e-2 & 5.8e-1 & 5.9e-1(-558.8\%) & 6.4e-1(-10.7\%) & 8.0e-1(-79.0\%) & 7.4e-1(-28.4\%) & **8.9e-2**(12.9\%) & **5.6e-1**(3.5\%) \\  & & 2d-MS & 7.4e+0 & 7.9e-1 & **5.5e-1(67.2\%)** & **5.0e-1(66.5\%)** & 9.7e-1(61.8\%) & 9.8e-1(-24.4\%) & 1.9e+0(-13.1\%) & 9.1e-1(-13.0\%) \\ \cline{2-11}  & & 2d-VC & 2.0e-1 & 1.3e-1 & **7.2e-1** & 5.9e-0(-36.0\%) & 3.3e-1(-6.3\%) & **3.2e-1**(74.9\%) & **1.6e-1**(9.6\%) & 1.1e-1(0.6\%) \\  & Heat & 2d-MS & 1.7e-1 & 2.4e-1 & **4.3e-1** & 3.5e-1(-15.6\%) & **4.0e-1** & 3.8e-1(-17.4\%) & **8.7e-3**(48.5\%) & **7.2e-

[MISSING_PAGE_FAIL:32]

approximations [39, 9], which can tackle the expensive computation cost caused by calculating high-order derivatives. However, this paradigm does not attempt to change the objection function definition, just focuses on the calculation of point optimization PINN loss, which is distinct from our proposed region optimization paradigm.

Sampling-based methodsStrategies for sampling collocation points perform an important role in training PINN models [46]. Previous sampling-based methods mainly focus on accumulating collocation points to high-residual areas [51, 23] or considering the temporal causality [45], whose theoretical analyses are usually based on the quadrature theorem [32]. Distinct from these methods, RoPINN is motivated by the optimization deficiency of PINN models and can be seamlessly integrated with sampling-based methods with significant promotion (Table 3), indicating that RoPINN works orthogonally to sampling methods. Besides, the theoretical analysis of RoPINN also starts from the optimization perspective, which reveals that one key advancement of RoPINN is a better balance between optimization error and generalization error (Theorem 3.12).

In addition, RoPINN is also distinct from data augmentation or adversarial training techniques in the following aspects: (1) Theorem difference: although our proposed practical algorithm is based on Monte Carlo sampling in a region, the underlying theoretical support and insights are a region-based objective function (Theorem 3.5). (2) Implementation difference: In our algorithm, not only is the input changed, but the objective is also correspondingly changed. Thus, this paper is foundationally different from augmentation and adversarial training in that the ground truth label is fixed. Our design is tailored to the physics-informed loss function of PINNs, where we can accurately calculate the equation residual at any point within the input domain.

## Appendix G Limitations

This paper presents region optimization as a new PINN training paradigm and provides both theoretical analysis and practical algorithms, supported by extensive experiments. However, there are still several limitations. In the theoretical analysis, we assume that the canonical loss function is \(L\)-Lipschitz-\(\beta\)-smooth, which may not be guaranteed in practice. Besides, RoPINN involves several hyperparameters, such as initial region size \(r\), and number of past iterations \(T_{0}\). Although we have studied the sensitivity w.r.t. them in Figures 2 and Appendix D.1 and demonstrate that they are easy to tune in most cases, we still need to adjust them for better performance in practice.

According to our experiments and theorems, we provide some recipes for hyperparameter tuning in the following, which may be helpful to the usage of RoPINN:

* As shown in Figure 2, region size \(r\) will be progressively adjusted by RoPINN. Setting \(r\) in \([10^{-6},10^{-4}]\) can work well. According to Theorem 3.12, the choice of \(r\) should balance optimization and generalization, which may be inherently decided by the PDE smoothness.
* As analyzed in Figures 3 and 4, sampling 1-30 points can gain consistent promotion but will linearly increase the computation costs. Following our default setting (sampling one point) can already achieve a competitive performance in a wide range of PDEs.
* As presented in Figure 8, number of past iterations \(T_{0}\) is easy to tune in \([1,20]\). Setting \(T_{0}\in\{5,10\}\) can be a good choice, which has been widely verified in our paper.
* Some hyperparameter tuning tools, such as Weights and Bias (Wandb1), may mitigate this limitation to some extent, which has already been used in previous related work [37].

Footnote 1: https://github.com/wandb/wandb

## Appendix H Broader Impacts

In this paper, we develop a new region optimization training paradigm for PINNs and provide both theorem analyses and practical algorithms. This new perspective may inspire the subsequent research of PINNs, especially rethinking the canonical objective function. In addition, our proposed RoPINN shows favorable efficiency and generalizes well in different base models and PDEs, which can be used to boost the precision of PINNs and generally benefit the downstream tasks, such as physics phenomenon simulation, biological property analysis, etc. Since we purely focus on the training algorithm of PINNs, there are no potential negative social impacts or ethical risks.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This paper proposes a new training paradigm for PINNs along with a practical algorithm. Theoretical analyses (Theorems 3.3-3.12), extensive experiments (Tables 2, 8 and 9) and detailed discussions of related work (Section 2 and Appendix F) are provided to verify the effectiveness of our design and clarify the scope of our paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed our limitations in Appendix G. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have provided complete and detailed proofs for all the lemmas, theorems and corollaries in Appendixes A and B. The previous works have also been properly cited. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have clearly formalized our method in Algorithm 1, which is easy to reproduce. We have also included every detail of datasets, baselines and hyperparameters of our algorithm in both the main text and Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is available at this repository: https://github.com/thuml/RoPINN. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have included all the details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have included the statistical significance in Appendix D.3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have included these details in the main text (Section 4) and Table 3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have strictly followed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have included the discussion in Appendix H. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the data and code are used with proper citation and license. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.