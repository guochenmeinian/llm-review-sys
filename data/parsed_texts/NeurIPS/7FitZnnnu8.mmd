# Learning Directed Graphical Models with

Optimal Transport

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on downstream applications, notably the non-trivial task of discrete representation learning.

## 1 Introduction

Learning probabilistic directed graphical models (DGMs, also known as Bayesian networks) with latent variables is an important ongoing challenge in machine learning and statistics. This paper focuses on parameter learning, i.e., estimating the parameters of a DGM given its known structure. Learning DGMs has a long history, dating back to classical indirect likelihood-maximization approaches such as expectation maximization [EM, 15]. However, despite all its success stories, EM is well-known to suffer from local optima issues. More importantly, EM becomes inapplicable when the posterior distribution is intractable, which arises fairly often in practice.

A large family of related methods based on variational inference [VI, 30, 27] have demonstrated tremendous potential in this case, where the evidence lower bound (ELBO) is not only used for posterior approximation but also for point estimation of the model parameters. Such an approach has proved surprisingly effective and robust to overfitting, especially when having a small number of parameters. From a high-level perspective, both EM and VI are based on likelihood maximization in the presence of latent variables, which ultimately requires carrying out expectations over the commonly intractable posterior. In order to address this challenge, a large spectrum of methods have been proposed in the literature and we refer the reader to [5] for an excellent discussion of these approaches. Here we characterize them between two extremes. At one extreme, restrictive assumptions about the structure (e.g., as in mean-field approximations) or the model class (e.g., using conjugate exponential families) must be made to simplify the task. At the other extreme, when no assumptions are made, most existing black-box methods exploit very little information about the structure of the known probabilistic model (for example, in black-box and stochastic variational inference [44, 27], hierarchical approaches [45] and normalizing flows [42]).

Addressing the problem at its core, we hereby propose an alternative strategy to likelihood maximization that does not require the estimation of expectations over the posterior distribution. Concretely, parameter learning is now viewed through the lens of _optimal transport_[54], where the data distribution is the source and the true model distribution is the target. Instead of minimizing a Kullback-Leibler (KL) divergence (which likelihood maximization methods are essentially doing), here we aim to find a point estimate \(\theta^{*}\) that minimizes the Wasserstein distance [WD, 31] between these two distributions.

This perspective allows us to leverage desirable properties of the WD in comparison with other metrics. These properties have motivated the recent surge in generative models, e.g., Wasserstein GANs [1, 9] and Wasserstein Auto-encoders [50]. Indeed, the WD is shown to be well-behaved in situations where standard metrics such as the KL or JS (Jensen-Shannon) divergences are either infinite or undefined [43, 4]. The WD thus characterizes a more meaningful distance, especially when the two distributions reside in low-dimensional manifolds [9]. Ultimately, this novel view enables us to pursue an ambitious goal towards a model-agnostic and scalable learning framework.

Contributions.We present an entirely different view that casts parameter estimation as an optimal transport problem [54], where the goal is to find the optimal plan transporting "mass" from the data distribution to the model distribution. To achieve this, our method minimizes the WD between these two distributions. This permits a flexible framework applicable to any type of variable and graphical structure. In summary, we make the following contributions:

* an **O**ptimal **T**ransport framework for **P**arameter Learning in **D**irected **A**cyclic **G**raphical models. OTP-DAG is an alternative line of thinking about parameter learning. Diverging from the existing frameworks, the underlying idea is to find the parameter set associated with the distribution that yields the lowest transportation cost from the data distribution.
* We present theoretical developments showing that minimizing the transport cost is equivalent to minimizing the reconstruction error between the observed data and the model generation. This renders a tractable training objective to be solved efficiently with stochastic optimization.
* We provide empirical evidence demonstrating the versatility of our method on various graphical structures. OTP-DAG is shown to successfully recover the ground-truth parameters and achieve competitive performance across a range of downstream applications.

## 2 Background and Related Work

We first introduce the notations and basic concepts used throughout the paper. We reserve bold capital letters (i.e., **G**) for notations related to graphs. We use calligraphic letters (i.e. \(\mathcal{X}\)) for spaces, italic capital letters (i.e. \(X\)) for random variables, and lower case letters (i.e. \(x\)) for their values.

A **directed graph**\(\mathbf{G}=(\mathbf{V},\mathbf{E})\) consists of a set of nodes \(\mathbf{V}\) and an edge set \(\mathbf{E}\subseteq\mathbf{V}^{2}\) of ordered pairs of nodes with \((v,v)\notin\mathbf{E}\) for any \(v\in\mathbf{V}\) (one without self-loops). For a pair of nodes \(i,j\) with \((i,j)\in\mathbf{E}\), there is an arrow pointing from \(i\) to \(j\) and we write \(i\to j\). Two nodes \(i\) and \(j\) are adjacent if either \((i,j)\in\mathbf{E}\) or \((j,i)\in\mathbf{E}\). If there is an arrow from \(i\) to \(j\) then \(i\) is a parent of \(j\) and \(j\) is a child of \(i\). A Bayesian network structure \(\mathbf{G}=(\mathbf{V},\mathbf{E})\) is a **directed acyclic graph** (DAG), in which the nodes represent random variables \(X=[X_{i}]_{i=1}^{n}\) with index set \(\mathbf{V}:=\{1,...,n\}\). Let \(\mathrm{PA}_{X_{i}}\) denote the set of variables associated with parents of node \(i\) in \(\mathbf{G}\).

In this work, we tackle the classic yet important problem of learning the parameters of a directed graph from _partially observed data_. Let \(\mathbf{O}\subseteq\mathbf{V}\) and \(X_{\mathbf{O}}=[X_{i}]_{i\in\mathbf{O}}\) be the set of observed nodes and \(\mathbf{H}:=\mathbf{V}\backslash\mathbf{O}\) be the set of hidden nodes. Let \(P_{\theta}\) and \(P_{d}\) respectively denote the distribution induced by the graphical model and the empirical one induced by the _complete_ (yet unknown) data. Given a fixed graphical structure \(\mathbf{G}\) and some set of i.i.d data points, we aim to find the point estimate \(\theta^{*}\) that best fits the observed data \(X_{\mathbf{O}}\). The conventional approach is to minimize the KL divergence between the model distribution and the _empirical_ data distribution over observed data i.e., \(D_{\mathbb{KL}}(P_{d}(X_{\mathbf{O}}),P_{\theta}(X_{\mathbf{O}}))\), which is equivalent to maximizing the likelihood \(P_{\theta}(X_{\mathbf{O}})\) w.r.t \(\theta\). In the presence of latent variables, the marginal likelihood, given as \(P_{\theta}(X_{\mathbf{O}})=\int_{X_{\mathbf{H}}}P_{\theta}(X)dX_{\mathbf{H}}\), is generally intractable. Standard approaches then resort to maximizing a bound on the marginal log-likelihood, known as the evidence lower bound (ELBO), which is essentially the objective of EM [38] and VI [30]. Optimization of the ELBO for parameter learning in practice requires many considerations. For vanilla EM, the algorithm only works if the true posterior density can be computed exactly. Furthermore, EM is originally a batch algorithm, thereby converging slowly on large datasets [36]. Subsequently, researchers have tried exploring other methods for scalability, including attempts to combine EM with approximate inference [56; 40; 14; 10; 13; 36; 41].

When exact inference is infeasible, a variational approximation is the go-to solution. Along this line, research efforts have concentrated on ensuring tractability of the ELBO via the mean-field assumption [11] and its relaxation known as structured mean field [47]. Scalability has been one of the main challenges facing the early VI formulations since it is a batch algorithm. This has triggered the development of stochastic variational inference (SVI) [27; 26; 16; 29; 8; 7] which applies stochastic optimization to solve VI objectives. Another line of work is collapsed VI that explicitly integrates out certain model parameters or latent variables in an analytic manner [23; 32; 48; 34]. Without a closed form, one could resort to Markov chain Monte Carlo [18; 19; 21], which however tends to be slow. More accurate variational posteriors also exist, namely, through hierarchical variational models [45], implicit posteriors [49; 58; 37; 49], normalizing flows [33], or copula distribution [51]. To avoid computing the ELBO analytically, one can obtain an unbiased gradient estimator using Monte Carlo and re-parameterization tricks [44; 57]. As mentioned in the introduction, an excellent summary of these approaches is discussed in [5; 86]. Extensions of VI to other divergence measures than KL divergence e.g., \(\alpha-\)divergence or \(f-\)divergence, also exist [35; 24; 55]. In the causal inference literature, a related direction is to learn both the graphical structure and parameters of the corresponding structural equation model [60; 17]. These frameworks are often limited to additive noise models while assuming no latent confounders.

## 3 Optimal Transport for Learning Directed Graphical Models

We begin by explaining how parameter learning can be reformulated into an optimal transport problem [53] and thereafter introduce our novel theoretical contribution.

We consider a DAG \(\mathbf{G}(\mathbf{V},\mathbf{E})\) over random variables \(X=[X_{i}]_{i=1}^{n}\) that represents the data generative process of an underlying system. The system consists of \(X\) as the set of endogenous variables and \(U=\{U_{i}\}_{i=1}^{n}\) as the set of exogenous variables representing external factors affecting the system. Associated with every \(X_{i}\) is an exogenous variable \(U_{i}\) whose values are sampled from a prior distribution \(P(U)\) independently from other exogenous variables. For the purpose of this work, our framework operates on an extended graph consisting of both endogenous and exogenous nodes (See Figure 0(b)). In the graph \(\mathbf{G}\), \(U_{i}\) is represented by a node with no ancestors that has an outgoing arrow towards node \(i\). Consequently, for every endogenous variable, its parent set \(\mathrm{PA}_{X_{i}}\) is extended to include an exogenous variable and possibly some other endogenous variables. Henceforth, every distribution \(P_{\theta_{i}}(X_{i}|\mathrm{PA}_{X_{i}})\) can be reparameterized into a deterministic assignment

\[X_{i}=\psi_{i}\big{(}\mathrm{PA}_{X_{i}},U_{i}\big{)},\text{ for }i=1,...,n.\]

The ultimate goal is to estimate \(\theta=\{\theta_{i}\}_{i=1}^{n}\) as the parameters of the set of deterministic functions \(\psi=\{\psi_{i}\}_{i=1}^{n}\). We will use the notation \(\psi_{\theta}\) to emphasize this connection from now on.

Figure 1: (a) A DAG represents a system of \(4\) endogenous variables where \(X_{1},X_{3}\) are observed (black-shaded) and \(X_{2}\), \(X_{4}\) are hidden variables (non-shaded). (b): The extended DAG that includes an additional set of independent exogenous variables \(U_{1},U_{2},U_{3},U_{4}\) (grey-shaded) acting on each endogenous variable. \(U_{1},U_{2},U_{3},U_{4}\sim P(U)\) where \(P(U)\) is a prior product distribution. (c) Visualization of our backward-forward algorithm, where the dashed arcs represent the backward maps involved in optimization.

Given the data distribution \(P_{d}(X_{\mathbf{O}})\) and the model distribution \(P_{\theta}(X_{\mathbf{O}})\) over the observed set \(\mathbf{O}\), the **optimal transport** (OT) goal is to find the parameter set \(\theta\) that minimizes the cost of transport between these two distributions. The Kantorovich's formulation of the problem is given by

\[W_{c}\big{(}P_{d};P_{\theta}\big{)}:=\inf_{\Gamma\sim\mathcal{P}(X\sim P_{d},Y \sim P_{\theta})}\mathbb{E}_{(X,Y)\sim\Gamma}\big{[}c(X,Y)\big{]}, \tag{1}\]

where \(\mathcal{P}(X\sim P_{d},Y\sim P_{\theta})\) is a set of all joint distributions of \(\big{(}P_{d};P_{\theta}\big{)}\) and \(c:\mathcal{X}_{\mathbf{O}}\times\mathcal{X}_{\mathbf{O}}\mapsto\mathcal{R}_{+}\) is any measurable cost function over \(\mathcal{X}_{\mathbf{O}}\) (i.e., the product space of the spaces of observed variables) that is defined as \(c(X_{\mathbf{O}},Y_{\mathbf{O}}):=\sum_{i\in\mathbf{O}}c_{i}(X_{i},Y_{i})\) where \(c_{i}\) is a measurable cost function over a space of a certain observed variable.

Let \(P_{\theta}(\mathrm{PA}_{X_{i}},U_{i})\) denote the joint distribution of \(\mathrm{PA}_{X_{i}}\) and \(U_{i}\) factorized according to the graphical model. Let \(\mathcal{U}_{i}\) denote the space over random variable \(U_{i}\). The key ingredient of our theoretical development is local backward mapping. For every observed node \(i\in\mathbf{O}\), we define a stochastic "backward" map \(\phi_{i}:\mathcal{X}_{i}\mapsto\Pi_{k\in\mathrm{PA}_{X_{i}}}\ \mathcal{X}_{k} \times\mathcal{U}_{i}\) such that \(\phi_{i}\in\mathfrak{C}(X_{i})\) where \(\mathfrak{C}(X_{i})\) is the constraint set given as

\[\mathfrak{C}(X_{i}):=\big{\{}\phi_{i}:\phi_{i}\#P_{d}(X_{i})=P_{\theta}( \mathrm{PA}_{X_{i}},U_{i})\big{\}}.\]

Essentially, \(\phi_{i}\) pushes the data marginal of \(X_{i}\) forward to the model marginal of its parent variables. If \(\mathrm{PA}_{X_{i}}\) are latent variables, \(\phi_{i}\) can be viewed as a stochastic decoder mapping \(X_{i}\) to the conditional density \(\phi_{i}(\mathrm{PA}_{X_{i}}|X_{i})\).

Theorem 1 presents the main theoretical contribution of our paper. Our OT problem is concerned with finding the optimal set of deterministic "forward" maps \(\psi_{\theta}\) and stochastic "backward" maps \(\big{\{}\phi_{i}\in\mathfrak{C}(X_{i})\big{\}}_{i\in\mathbf{O}}\) that minimizes the cost of transporting the mass from \(P_{d}\) to \(P_{\theta}\) over \(\mathbf{O}\). While the formulation in Eq. (1) is not trainable, we show that the problem is reduced to minimizing the reconstruction error between the data generated from \(P_{\theta}\) and the observed data. To understand how reconstruction works, let us examine Figure 0(c). Given \(X_{1}\) and \(X_{3}\) as observed nodes, we sample \(X_{1}\sim P_{d}(X_{1}),X_{3}\sim P_{d}(X_{3})\) and evaluate the local densities \(\phi_{1}(\mathrm{PA}_{X_{1}}|X_{1})\), \(\phi_{3}(\mathrm{PA}_{X_{3}}|X_{3})\) where \(\mathrm{PA}_{X_{1}}=\{X_{2},X_{4},U_{1}\}\) and \(\mathrm{PA}_{X_{3}}=\{X_{4},U_{3}\}\). The next step is to sample \(\mathrm{PA}_{X_{1}}\sim\phi_{1}(\mathrm{PA}_{X_{1}}|X_{1})\) and \(\mathrm{PA}_{X_{3}}\sim\phi_{3}(\mathrm{PA}_{X_{3}}|X_{3})\), which are plugged back to the model \(\psi_{\theta}\) to obtain the reconstructions \(\widetilde{X}_{1}=\psi_{\theta_{1}}(\mathrm{PA}_{X_{1}})\) and \(\widetilde{X}_{3}=\psi_{\theta_{3}}(\mathrm{PA}_{X_{3}})\). We wish to learn \(\theta\) such that \(X_{1}\) and \(X_{3}\) are reconstructed correctly. For a general graphical model, this optimization objective is formalized as

**Theorem 1**: _For every \(\phi_{i}\) as defined above and fixed \(\psi_{\theta}\),_

\[W_{c}\big{(}P_{d}(X_{\mathbf{O}});P_{\theta}(X_{\mathbf{O}})\big{)}=\inf_{ \big{[}\phi_{i}\in\mathfrak{C}(X_{i})\big{]}_{i\in\mathbf{O}}}\mathbb{E}_{X_{ \mathbf{O}}\sim P_{d}(X_{\mathbf{O}}),\mathrm{PA}_{X_{\mathbf{O}}}\sim\phi(X_ {\mathbf{O}})}\big{[}c\big{(}X_{\mathbf{O}},\psi_{\theta}(\mathrm{PA}_{X_{ \mathbf{O}}})\big{)}\big{]}, \tag{2}\]

_where \(\mathrm{PA}_{X_{\mathbf{O}}}:=\big{[}[X_{ij}]_{j\in\mathrm{PA}_{X_{i}}}\big{]} _{i\in\mathbf{O}}\)._

The proof is provided in Appendix A. It is seen that Theorem 1 set ups a trainable form for our optimization solution. Notice that the quality of the reconstruction hinges on how well the backward maps approximate the true local densities. To ensure approximation fidelity, every backward function \(\phi_{i}\) must satisfy its push-forward constraint defined by \(\mathfrak{C}\). In the above example, the backward maps \(\phi_{i}\) and \(\phi_{3}\) must be constructed such that \(\phi_{1}\#(X_{1})=P_{\theta}(X_{2},X_{4},U_{1})\) and \(\phi_{3}\#(X_{3})=P_{\theta}(X_{4},U_{3})\). This gives us a constraint optimization problem, and we relax the constraints by adding a penalty to the above objective.

The **final optimization objective** is therefore given as

\[J_{WS}=\inf_{\psi,\phi}\ \ \ \mathbb{E}_{X_{\mathbf{O}}\sim P_{d}(X_{\mathbf{O}}), \mathrm{PA}_{X_{\mathbf{O}}}\sim\phi(X_{\mathbf{O}})}\big{[}c\big{(}X_{ \mathbf{O}},\psi_{\theta}(\mathrm{PA}_{X_{\mathbf{O}}})\big{)}\big{]}+\eta\ D \big{(}\phi,P_{\theta}\big{)}, \tag{3}\]

where \(D\) is any arbitrary divergence measure and \(\eta>0\) is a trade-off hyper-parameter. \(D\big{(}\phi,P_{\theta}\big{)}\) is a short-hand for divergence between all pairs of backward and forward distributions.

This theoretical result provides us with several interesting properties: (1) to minimize the global OT cost between the model distribution and the data distribution, one only needs to characterize the local densities by specifying the backward maps from every observed node to its parents and optimizing them with appropriate cost metrics; (2) all model parameters are optimized simultaneously within a single framework whether the variables are continuous or discrete ; (3) the computational process can be automated without deriving an analytic lower bound or restricting to certain graph ical structures. In connection with VI, OTP-DAG is also optimization-based. We in fact leverage modern VI techniques of reparameterization and amortized inference [6] for solving it efficiently via stochastic gradient descent. However, unlike such advances as hierarchical VI, our method does not place any prior over the variational distribution on the latent variables underlying the variational posterior [45]. For providing a guarantee, OTP-DAG relies on the condition that the backward maps are sufficiently expressive to cover the push-forward constraints. We prove further in Appendix A that given a suitably rich family of backward functions, our algorithm OTP-DAG can converge to the ground-truth parameters. Details on our algorithm can be found in Appendix B. In the next section, we illustrate how OTP-DAG algorithm is realized in practical applications.

## 4 Applications

We apply OTP-DAG on \(3\) widely-used graphical models for a total of \(5\) different sub-tasks. Here we aim to demonstrate the versatility of OTP-DAG: OTP-DAG can be exploited for various purposes through a single learning procedure. In terms of estimation accuracy, OTP-DAG is capable of recovering the ground-truth parameters while achieving the comparable or better performance level of existing frameworks across downstream tasks.1

Footnote 1: Our code is anonymously published at [https://anonymous.4open.science/r/OTP-7944/](https://anonymous.4open.science/r/OTP-7944/).

We consider various directed probabilistic models with either continuous or discrete variables. We begin with (1) Latent Dirichlet Allocation [12] for topic modeling and (2) Hidden Markov Model (HMM) for sequential modeling tasks. We conclude with a more challenging setting: (3) Discrete Representation Learning (Discrete RepL) that cannot simply be solved by EM or MAP (maximum a posteriori). It in fact invokes deep generative modeling via a pioneering development called Vector Quantization Variational Auto-Encoder (VQ-VAE) [52]. We investigate an application of OTP-DAG algorithm to learning discrete representations by grounding it into a parameter learning problem.

Note that our goal is not to achieve the state-of-the-art performance, rather to prove OTP-DAG as a versatile approach for learning parameters of directed graphical models. Figure 2 illustrates the empirical DAG structures of the \(3\) applications. Unlike the standard visualization where the parameters are considered hidden nodes, our graph separates model parameters from latent variables and only illustrates random variables and their dependencies (except the special setting of Discrete RepL). We also omit the exogenous variables associated with the hidden nodes for visibility, since only those acting on the observed nodes are relevant for computation. There is also a noticeable difference between Figure 2 and Figure 0(c): the empirical version does not involve learning the backward maps for the exogenous variables. This stems from an experimental observation that sampling the noise from an appropriate prior distribution at random suffices to yield accurate estimation. We find it to be beneficial in that training complexity can be greatly reduced. In the following, we report the main experimental results, leaving the discussion of the formulation and technicalities in Appendix C. In all tables, we report the average results over \(5\) random initializations and the best ones are highlighted in bold. In addition, \(\uparrow\), \(\downarrow\) indicate higher/lower performance is better, respectively.

### Latent Dirichlet Allocation

Let us consider a corpus \(\mathcal{D}\) of \(M\) independent documents where each document is a sequence of \(N\) words denoted by \(W=(W_{1},W_{2},\cdots,W_{N})\). Documents are represented as random mixtures over \(K\) latent topics, each of which is characterized by a distribution over words. Let \(V\) be the size of a vocabulary indexed by \(\{1,\cdots,V\}\). Latent Dirichlet Allocation (LDA) [12] dictates the following generative process for every document in the corpus:

Figure 2: Empirical structure of (a) latent Dirichlet allocation model (in plate notation), (b) standard hidden Markov model, and (c) discrete representation learning.

1. Sample \(\theta\sim\text{Dir}(\alpha)\) with \(\alpha<1\),
2. Sample \(\gamma_{k}\sim\text{Dir}(\beta)\) where \(k\in\{1,\cdots,K\}\),
3. For each of the word positions \(n\in\{1,\cdots,N\}\), * Sample a topic \(Z_{n}\sim\text{Multi-nominal}(\theta)\), * Sample a word \(W_{n}\sim\text{Multi-nominal}(\gamma_{k})\),

where \(\text{Dir}(.)\) is a Dirichlet distribution. \(\theta\) is a \(K-\)dimensional vector that lies in the \((K-1)-\)simplex and \(\gamma_{k}\) is a \(V-\)dimensional vector represents the word distribution corresponding to topic \(k\). In the standard model, \(\alpha,\beta,K\) are hyper-parameters and \(\theta,\gamma\) are learnable parameters. Throughout the experiments, the number of topics \(K\) is assumed known and fixed.

Parameter Estimation.To test whether OTP-DAG can recover the true parameters, we generate synthetic data in a simplified setting: the word probabilities are parameterized by a \(K\times V\) matrix \(\gamma\) where \(\gamma_{kn}:=P(W_{n}=1|Z_{n}=1)\); \(\gamma\) is now a fixed quantity to be estimated. We set \(\alpha=1/K\) uniformly and generate small datasets for different number of topics \(K\) and sample size \(N\). Inspired by the setup of [20], for every topic \(k\), the word distribution \(\gamma_{k}\) can be represented as a square grid where each cell, corresponding to a word, is assigned an integer value of either \(0\) and \(1\), indicating whether a certain word is allocated to the \(k^{th}\) topic or not. As a result, each topic is associated with a specific pattern. For simplicity, we represent topics using horizontal or vertical patterns (See Figure 3). Following the above generative model, we sample \(3\) sets of data w.r.t \(3\) sets of configuration triplets \(\{K,M,N\}\): \(\{10,1000,100\}\), \(\{20,5000,200\}\) and \(\{30,10000,300\}\).

We compare OTP-DAG with Batch EM [38] and SVI [25, 27]. For the baselines, only \(\gamma\) is learnable whereas \(\alpha\) is set fixed to be uniform, whereas for our method OTP-DAG, we take on a more challenging task of **learning both parameters**. We report the fidelity of the estimation of \(\gamma\) in Table 1 wherein OTP-DAG is shown to yield estimates closest to the ground-truth values. At the same time, our estimates for \(\alpha\) (averaged over \(K\)) are nearly \(100\%\) faithful at \(0.10,0.049,0.033\) (recall that the ground-truth \(\alpha\) is uniform over \(K\) where \(K=10,20,30\) respectively).

Figure 3 illustrates the model topic distribution at the end of training. OTP-DAG recovers all of the ground-truth patterns, and as further shown Figure 4, most of the patterns in fact converge well before training ends.

[MISSING_PAGE_FAIL:7]

ure 4(a) reports the most probable state for each observation, inferred from our backward distribution \(\phi(X_{1:T})\). It can be seen that the partition overall aligns with the true generative process the data.

Polyphonic Music Modeling.We consider another application of HMM to model sequences of polyphonic music. The data under analysis is the corpus of \(382\) harmonized chorales by J. S. Bach [3]. The training set consists of \(N=229\) sequences, each of which has a maximum length of \(T=129\) and \(D=51\) notes. The data matrix is a Boolean tensor of size \(N\times T\times D\). We follow the standard preprocessing where \(37\) redundant notes are dropped.3

Footnote 3: [https://pyro.ai/examples/hmm.html](https://pyro.ai/examples/hmm.html).

The observation at each time step is modeled using a factored observation distribution of the form

\[P(X_{t}|Z_{t}=k)=\prod_{d=1}^{D}\operatorname{Ber}(X_{td}|B_{d}(k)),\]

where \(B_{d}(k)=P(X_{td}=1|Z_{t}=k)\) and \(k=1,\cdots,K\). Similarly, we use a uniform prior over the initial state. Following [39], the transition probabilities are sampled from a Dirichlet distribution with concentration parameters \(\alpha_{1:K}\), where \(\alpha_{k}=1\) if the state remains and \(0.1\) otherwise,

The parameter set \(\theta\) is a matrix size \(D\times K\) where each element \(\theta_{ij}\in[0,1]\) parameterizes \(B_{d}k(.)\). The goal is to learn these probabilities with underlying HMM sharing the same structure as Figure 1(b). The main difference is that the previous application only deals with one sequence, while here we consider a batch of sequences. For larger datasets, estimating MAP of an HMM can be expensive. Figure 4(b) reports negative log-likelihood of the learned models on the test set, along with training time (in minutes) at different values of \(K\). Our fitted HMM closely approaches the level of performance of MAP. Both models are optimized using mini-batch gradient descent, yet OTP-DAG runs in constant time (approx. \(3\) minutes), significantly faster than solving MAP with SGD.

### Learning Discrete Representations

Many types of data exist in the form of discrete symbols e.g., words in texts, or pixels in images. This motivates the need to explore the latent discrete representations of the data, which can be useful for planning and symbolic reasoning tasks. Viewing discrete representation learning as a parameter learning problem, we endow it with a probabilistic generative process as illustrated in Figure 1(c). The problem deals with a latent space \(\mathcal{C}\in\mathbb{R}^{K\times D}\) composed of \(K\) discrete latent sub-spaces of \(D\) dimensionality. The probability a data point belongs to a discrete sub-space \(c\in\{1,\cdots,K\}\) follows a \(K-\)way categorical distribution \(\pi=[\pi_{1},\cdots,\pi_{K}]\). In the language of VQ-VAE, each \(c\) is referred to as a _codeword_ and the set of codewords is called a _codebook_. Let \(Z\in\mathbb{R}^{D}\) denote the latent variable in a sub-space. On each sub-space, we impose a Gaussian distribution parameterized by \(\mu_{c},\Sigma_{c}\) where \(\Sigma_{c}\) is diagonal. The data generative process is described as follows:

1. Sample \(c\sim\text{Cat}(\pi)\),
2. Sample \(Z\sim\mathcal{N}(\mu_{c},\Sigma_{c})\)

Figure 5: (a) Segmentation of Poisson time series inferred from the backward distribution \(\phi(X_{1:T})\). (b) Training time \(\downarrow\) (in minutes) and Negative log-likelihood \(\downarrow\) on the test dataset at various \(K\).

3. Quantize \(\mu_{c}=Q(Z)\),
4. \(X=\psi_{\theta}(Z,\mu_{c})\).

where \(\psi\) is a highly non-convex function with unknown parameters \(\theta\) and often parameterized with a deep neural network. \(Q\) refers to the quantization of \(Z\) to \(\mu_{c}\) defined as \(\mu_{c}=Q(Z)\) where \(c=\operatorname*{argmin}_{c}d_{z}\big{(}Z;\mu_{c}\big{)}\) and \(d_{z}=\sqrt{(Z-\mu_{c})^{T}\Sigma_{c}^{-1}(Z-\mu_{c})}\) is the Mahalanobis distance.

The goal is to learn the parameter set \(\{\pi,\mu,\Sigma,\theta\}\) with \(\mu=[\mu_{k}]_{k=1}^{K},\Sigma=[\Sigma_{k}]_{k=1}^{K}\) such that the model captures the key properties of the data. Fitting OTP-DAG to the observed data requires constructing a backward map \(\phi:\mathcal{X}\mapsto\mathbb{R}^{D}\) from the input space back to the latent space. In connection with vector quantization, the backward map is defined via \(Q\) and an encoder \(f_{e}\) as

\[\phi(X)=\big{[}f_{e}(X),Q(f_{e}(X))\big{]},\quad Z=f_{e}(X),\quad\mu_{c}=Q(Z).\]

Following VQ-VAE [52], our practical implementation considers \(Z\) as an \(M-\)component latent embedding. We experiment with images in this application and compare OTP-DAG with VQ-VAE on \(3\) popular datasets: CIFAR10, MNIST and SVHN. Since the true parameters are unknown, we assess how well the latent space characterizes the input data through the quality of the reconstruction of the original images. Our analysis considers various metrics measuring the difference/similarity between the two images on patch (SSIM), pixel (PSNR), feature (LPIPS) and dataset (FID) levels. We also compute Perplexity to evaluate the degree to which the latent representations \(Z\) spread uniformly over \(K\) sub-spaces. Table 4 reports our superior performance in preserving high-quality information of the input images. VQ-VAE suffers from poorer performance mainly due to an issue called _codebook collapse_[59] where most of latent vectors are quantized to few discrete codewords, while the others are left vacant. Meanwhile, our framework allows for control over the number of latent representations assigned to each codeword through learning \(\pi\), ensuring all codewords are utilized. See Appendix C.3 for detailed formulation and qualitative examples.

## 5 Limitations

Our framework employs amortized optimization that requires continuous relaxation or reparameterization of the underlying model distribution to ensure the gradients can be back-propagated effectively. For discrete distributions and for some continuous ones (e.g., Gamma distribution), this is not easy to attain. To this end, a recent proposal on _Generalized Reparameterization Gradient_[46] is a viable solution. OTP-DAG also relies on the expressivity of the backward maps. Since our backward mapping only considers local dependencies, it is however simpler to find a good approximation compared to VI where the variational approximator should ideally characterize the entire global dependencies in the graph. We use neural networks to model the backward conditionals. With enough data, network complexity, and training time, the difference between the modeled distribution and the true conditional can be assumed to be smaller than an arbitrary constant \(\epsilon\) based on the universal approximation theorem [28].

## 6 Conclusion and Future Work

This paper contributes a novel approach based on optimal transport to learning parameters of directed graphical models. The proposed algorithm OTP-DAG is general and applicable to any directed graph with latent variables regardless of variable types and structural dependencies. As for future research, this new perspective opens up promising avenues, for instance applying OTP-DAG to structural learning problems where edge existence and directionality can be parameterized for continuous optimization, or extending it to learning undirected graphical models.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline Dataset & Method & Latent Size & SSIM \(\uparrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & Perplexity \(\uparrow\) \\ \hline CIFAR10 & **VQ-VAE** & \(8\times 8\) & 0.70 & 23.14 & 0.35 & 77.3 & 69.8 \\  & **OTP-DAG** (Ours) & \(8\times 8\) & **0.80** & **25.40** & **0.23** & **56.5** & **498.6** \\ \hline MNIST & **VQ-VAE** & \(8\times 8\) & **0.98** & 33.37 & 0.02 & 4.8 & 47.2 \\  & **OTP-DAG** (Ours) & \(8\times 8\) & **0.98** & **33.62** & **0.01** & **3.3** & **474.6** \\ \hline SVHN & **VQ-VAE** & \(8\times 8\) & 0.88 & 26.94 & 0.17 & 38.5 & 114.6 \\  & **OTP-DAG** (Ours) & \(8\times 8\) & **0.94** & **32.56** & **0.08** & **25.2** & **462.8** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Quality of the image reconstructions (\(K=512\)).

## References

* [1] Jonas Adler and Sebastian Lunz. Banach wasserstein gan. _Advances in neural information processing systems_, 31, 2018.
* [2] Nikolaos Aletras and Mark Stevenson. Evaluating topic coherence using distributional semantics. In _Proceedings of the 10th international conference on computational semantics (IWCS 2013)-Long Papers_, pages 13-22, 2013.
* [3] Moray Allan and Christopher Williams. Harmonising chorales by probabilistic inference. _Advances in neural information processing systems_, 17, 2004.
* [4] Luca Ambrogioni, Umut Guclu, Yagmur Gucluutz, Max Hinne, Marcel A.J. Van Gerven, and Eric Maris. Wasserstein variational inference. _Advances in Neural Information Processing Systems_, 2018-December(NeurIPS):2473-2482, 2018.
* [5] Luca Ambrogioni, Kate Lin, Emily Fertig, Sharad Vikram, Max Hinne, Dave Moore, and Marcel van Gerven. Automatic structured variational inference. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 676-684. PMLR, 13-15 Apr 2021.
* [6] Brandon Amos. Tutorial on amortized optimization for learning to optimize over continuous domains. _arXiv preprint arXiv:2202.00665_, 2022.
* [7] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. _Journal of machine learning research_, 15:2773-2832, 2014.
* [8] Animashree Anandkumar, Daniel Hsu, and Sham M Kakade. A method of moments for mixture models and hidden markov models. In _Conference on Learning Theory_, pages 33-1. JMLR Workshop and Conference Proceedings, 2012.
* [9] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pages 214-223. PMLR, 2017.
* [10] Matthew J Beal and Zoubin Ghahramani. Variational bayesian learning of directed graphical models with hidden variables. 2006.
* [11] Christopher M Bishop and Nasser M Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006.
* [12] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. _Journal of machine Learning research_, 3(Jan):993-1022, 2003.
* [13] Olivier Cappe and Eric Moulines. On-line expectation-maximization algorithm for latent data models. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 71(3):593-613, 2009.
* [14] Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochastic approximation version of the em algorithm. _Annals of statistics_, pages 94-128, 1999.
* [15] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the EM Algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, 1977.
* [16] Nick Foti, Jason Xu, Dillon Laird, and Emily Fox. Stochastic variational inference for hidden markov models. _Advances in neural information processing systems_, 27, 2014.
* [17] Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, et al. Deep end-to-end causal inference. _arXiv preprint arXiv:2202.02195_, 2022.
* [18] Alan E Gelfand and Adrian FM Smith. Sampling-based approaches to calculating marginal densities. _Journal of the American statistical association_, 85(410):398-409, 1990.
** [19] Walter R Gilks, Sylvia Richardson, and David Spiegelhalter. _Markov chain Monte Carlo in practice_. CRC press, 1995.
* [20] Thomas L Griffiths and Mark Steyvers. Finding scientific topics. _Proceedings of the National academy of Sciences_, 101(suppl_1):5228-5235, 2004.
* [21] John Hammersley. _Monte carlo methods_. Springer Science & Business Media, 2013.
* [22] Ernst Hellinger. Neue begrundung der theorie quadratischer formen von unendlichvielen veranderlichen. _Journal fur die reine und angewandte Mathematik_, 1909(136):210-271, 1909.
* [23] James Hensman, Magnus Rattray, and Neil Lawrence. Fast variational inference in the conjugate exponential family. _Advances in neural information processing systems_, 25, 2012.
* [24] Jose Hernandez-Lobato, Yingzhen Li, Mark Rowland, Thang Bui, Daniel Hernandez-Lobato, and Richard Turner. Black-box alpha divergence minimization. In _International conference on machine learning_, pages 1511-1520. PMLR, 2016.
* [25] Matthew Hoffman, Francis Bach, and David Blei. Online learning for latent dirichlet allocation. _advances in neural information processing systems_, 23, 2010.
* [26] Matthew D Hoffman and David M Blei. Structured stochastic variational inference. In _Artificial Intelligence and Statistics_, pages 361-369, 2015.
* [27] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. _Journal of Machine Learning Research_, 2013.
* [28] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* [29] Matthew Johnson and Alan Willsky. Stochastic variational inference for bayesian time series models. In _International Conference on Machine Learning_, pages 1854-1862. PMLR, 2014.
* [30] Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. _Machine learning_, 37:183-233, 1999.
* [31] Leonid V Kantorovich. Mathematical methods of organizing and planning production. _Management science_, 6(4):366-422, 1960.
* [32] Nathaniel J King and Neil D Lawrence. Fast variational inference for gaussian process models through kl-correction. In _Machine Learning: ECML 2006: 17th European Conference on Machine Learning Berlin, Germany, September 18-22, 2006 Proceedings 17_, pages 270-281. Springer, 2006.
* [33] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. _Advances in neural information processing systems_, 29, 2016.
* [34] Miguel Lazaro-Gredilla, Steven Van Vaerenbergh, and Neil D Lawrence. Overlapping mixtures of gaussian processes for the data association problem. _Pattern recognition_, 45(4):1386-1395, 2012.
* [35] Yingzhen Li and Richard E Turner. Renyi divergence variational inference. _Advances in neural information processing systems_, 29, 2016.
* [36] Percy Liang and Dan Klein. Online em for unsupervised models. In _Proceedings of human language technologies: The 2009 annual conference of the North American chapter of the association for computational linguistics_, pages 611-619, 2009.
* [37] Dmitry Molchanov, Valery Kharitonov, Artem Sobolev, and Dmitry Vetrov. Doubly semi-implicit variational inference. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2593-2602. PMLR, 2019.
** [38] Todd K Moon. The expectation-maximization algorithm. _IEEE Signal processing magazine_, 13(6):47-60, 1996.
* [39] Kevin P Murphy. _Probabilistic machine learning: Advanced topics_. MIT Press, 2023.
* [40] Radford M Neal and Geoffrey E Hinton. A view of the em algorithm that justifies incremental, sparse, and other variants. _Learning in graphical models_, pages 355-368, 1998.
* [41] Ronald C Neath et al. On convergence properties of the monte carlo em algorithm. _Advances in modern statistical theory and applications: a Festschrift in Honor of Morris L. Eaton_, pages 43-62, 2013.
* [42] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _Journal of Machine Learning Research_, 22:1-64, 2021.
* [43] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. _Center for Research in Economics and Statistics Working Papers_, (2017-86), 2017.
* [44] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In _Artificial intelligence and statistics_, pages 814-822. PMLR, 2014.
* [45] Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In _International conference on machine learning_, pages 324-333. PMLR, 2016.
* [46] Francisco R Ruiz, Titsias RC AUEB, David Blei, et al. The generalized reparameterization gradient. _Advances in neural information processing systems_, 29, 2016.
* [47] Lawrence Saul and Michael Jordan. Exploiting tractable substructures in intractable networks. _Advances in neural information processing systems_, 8, 1995.
* [48] Yee Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm for latent dirichlet allocation. _Advances in neural information processing systems_, 19, 2006.
* [49] Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 167-176. PMLR, 2019.
* [50] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders. _arXiv preprint arXiv:1711.01558_, 2017.
* [51] Dustin Tran, David Blei, and Edo M Airoldi. Copula variational inference. _Advances in neural information processing systems_, 28, 2015.
* [52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.
* [53] Cedric Villani. _Topics in optimal transportation_, volume 58. AMS Graduate Studies in Mathematics, 2003.
* [54] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [55] Neng Wan, Dapeng Li, and Naira Hovakimyan. F-divergence variational inference. _Advances in neural information processing systems_, 33:17370-17379, 2020.
* [56] Greg CG Wei and Martin A Tanner. A monte carlo implementation of the em algorithm and the poor man's data augmentation algorithms. _Journal of the American statistical Association_, 85(411):699-704, 1990.
* [57] Ming Xu, Matias Quiroz, Robert Kohn, and Scott A Sisson. Variance reduction properties of the reparameterization trick. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2711-2720. PMLR, 2019.

* [58] Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In _International Conference on Machine Learning_, pages 5660-5669. PMLR, 2018.
* [59] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In _International Conference on Learning Representations_.
* [60] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. In _International Conference on Machine Learning_, pages 7154-7163. PMLR, 2019.