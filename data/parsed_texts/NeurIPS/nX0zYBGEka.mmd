# FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning

 Jinyuan Jia

The Pennsylvania State University

jingyuan@psu.edu

&Zhuowen Yuan

UIUC

zhuowen3@illinois.edu

&Dinuka Sahabandu

University of Washington

sdinuka@uw.edu

&Luyao Niu

University of Washington

luyaoniu@uw.edu

&Arezoo Rajabi

University of Washington

rajabia@uw.edu

&Bhaskar Ramasubramanian

Western Washington University

ramasub@wwu.edu

&Bo Li

UIUC

lbo@illinois.edu

&Radha Poovendran

University of Washington

rp3@uw.edu

Equal contribution.

###### Abstract

Federated learning (FL) provides a distributed training paradigm where multiple clients can jointly train a global model without sharing their local data. However, recent studies have shown that FL offers an additional surface for backdoor attacks. For instance, an attacker can compromise a subset of clients and thus corrupt the global model to misclassify an input with a backdoor trigger as the adversarial target. Existing defenses for FL against backdoor attacks usually detect and exclude the corrupted information from the compromised clients based on a _static_ attacker model. However, such defenses are inadequate against _dynamic_ attackers who strategically adapt their attack strategies. To bridge this gap, we model the strategic interactions between the defender and dynamic attackers as a minimax game. Based on the analysis of the game, we design an interactive defense mechanism FedGame. We prove that under mild assumptions, the global model trained with FedGame under backdoor attacks is close to that trained without attacks. Empirically, we compare FedGame with multiple _state-of-the-art_ baselines on several benchmark datasets under various attacks. We show that FedGame can effectively defend against strategic attackers and achieves significantly higher robustness than baselines. Our code is available at: https://github.com/AI-secure/FedGame.

## 1 Introduction

Federated learning (FL) [21; 28] aims to train a global model over training data that are distributed across multiple clients (e.g., mobile phones, IoT devices) in an iterative manner. In each communication round, a cloud server shares its global model with selected clients. Then, each selected client uses the global model to initialize its local model, utilizes its local training dataset to train the local model, and sends the local model update to the server. Finally, the server uses an aggregation rule to integrate local model updates from clients to update its global model.

Due to the distributed nature of FL, many studies [3; 4; 5; 21; 44] have shown that FL is vulnerable to backdoor attacks [11; 18; 26; 27; 30; 33; 37; 40; 41; 55; 59]. For instance, an attacker can compromisea subset of clients and manipulate their local training datasets to corrupt the global model such that it predicts an attacker-chosen target class for any inputs embedded with a backdoor trigger [3]. To defend against such backdoor attacks, many defenses [7; 32; 34; 36; 39; 51] have been proposed (see Section 2 for details). However, all those defenses consider a _static_ attack model where an attacker sticks to a fixed strategy and does not adapt its attack strategies. As a result, they are less effective under adaptive attacks, e.g., Wang et al. [44] showed that defenses in [6; 39] can be bypassed by appropriately designed attacks. While the vulnerability of the FL against dynamic or adaptive attack is known, dynamic defense has not been well-studied yet.

In this work, we propose FedGame, a game-theoretic defense against backdoor attacks in FL. Specifically, we formulate FedGame as a minimax game between the server (defender) and attacker, enabling them to adapt their defense and attack strategies strategically. While the server has no prior knowledge about which client is compromised, our key idea is that the server can compute a _genuine score_ for each client, which should be large (or small) if the client is benign (or compromised) in each communication round. The genuine score serves as a weight for the local model update of the client when used to update the global model. The goal of the defender is to minimize the genuine scores for compromised clients and maximize them for benign ones. To solve the resulting minimax game for the defender, we follow a three-step process consisting of 1) building an auxiliary global model, 2) exploiting the auxiliary global model to reverse engineer a backdoor trigger and target class, and 3) testing whether the local model of a client will predict an input embedded with the reverse engineered backdoor trigger as the target class to compute a genuine score for the client. Based on the deployed defense, the goal of the attacker is to optimize its attack strategy by maximizing the effectiveness of the backdoor attack while remaining stealthy. Our key observation is that the attack effectiveness is determined by two factors: the genuine score and the local model of the client. We optimize the attack strategy with respect to those two factors to maximize the effectiveness of backdoor attacks against dynamic defense.

We perform both theoretical analysis and empirical evaluations for FedGame. Theoretically, we prove that the global model trained with our defense under backdoor attacks is close to that trained without attacks (measured by Euclidean distance of model parameters). Empirically, we perform comprehensive evaluations. In particular, we evaluate FedGame on benchmark datasets to demonstrate its effectiveness under _state-of-the-art_ backdoor attacks [3; 54; 59]. Moreover, we compare it with six _state-of-the-art_ baselines [6; 7; 36; 39; 56]. Our results indicate that FedGame outperforms them by a significant margin (e.g., the attack success rate for FedGame is less than 13% while it is greater than 95% for those baselines on CIFAR10 under Scaling attack [3]). We also perform comprehensive ablation studies and evaluate FedGame against adaptive attacks. Our results show FedGame is consistently effective. Our key contributions are as follows:

* We propose FedGame, the first game-theoretic defense against dynamic backdoor attacks to FL.
* We provide theoretical guarantees of FedGame. We show that the global model trained with FedGame under backdoor attacks is close to that without attacks.
* We perform a systematic evaluation of FedGame on benchmark datasets and demonstrate that FedGame significantly outperforms state-of-the-art baselines.

## 2 Related Work

Backdoor Attacks in FL.In backdoor attacks against FL [3; 4; 5; 31; 44; 50; 59], an attacker aims to make a global model predict a target class for any input embedded with a backdoor trigger via compromised clients. For instance, Bagdasaryan et al. [3] proposed the scaling attack in which an attacker first uses a mix of backdoored and clean training examples to train its local model and then scales the local model update by a factor before sending it to the server. In our work, we will leverage _state-of-the-art_ attacks [3; 54; 59] to perform strategic backdoor attacks against our defense.

Defenses against Backdoor Attacks in FL.Many defenses [39; 7; 34; 51; 36; 32; 57; 9] were proposed to mitigate backdoor attacks in FL. For instance, Sun et al. [39] proposed norm-clipping, which clips the norm of the local model update of a client to a certain threshold. Some work extended differential privacy [15; 1; 29] to mitigate backdoor attacks to federated learning. The idea is to clip the local model update and add Gaussian noise. Cao et al. [7] proposed FLTrust, which leveraged the similarity of the local model update of a client with that computed by the server itself on its clean dataset. DeepSight [36] performs clustering on local models and then filters out those from compromised clients. By design, DeepSight is ineffective when the fraction of compromised clients is larger than 50%. Other defenses include Byzantine-robust FL methods such as Krum [6], Trimmed Mean [56], and Median [56]. However, all of those defenses only consider a static attacker model. As a result, they become less effective against dynamic attackers who strategically adapt their attack strategies.

Another line of research focuses on detecting compromised clients [24; 58]. As those defenses need to collect many local model updates from clients to make confident detection, the global model may already be backdoored before those clients are detected. Some recent studies [8; 53; 57; 10] proposed certified defenses against compromised clients. However, they can only tolerate a moderate fraction of compromised clients (e.g., less than 10%) as shown in their results. In contrast, FedGame can tolerate a much higher fraction (e.g., 80%) of compromised clients.

In this work, we aim to prevent backdoor attacks to federated learning, i.e., our goal is to train a robust global model under compromised clients. The server could also utilize post-training defenses [43; 51] to remove the backdoor in a backdoored global model. Prevention-based defense could be combined with those post-training defenses to form a defense-in-depth. We note that the defense proposed in [43] requires the server to own some clean samples that have the same distribution as the local training data of clients. The defense proposed in [51] prunes filters based on pruning sequences collected from clients and thus is less effective when the fraction of compromised clients is large.

## 3 Background and Threat Model

Federated Learning.Let \(\mathcal{S}\) denote the set of clients and \(\mathcal{D}_{i}\) denote the local training dataset of client \(i\in\mathcal{S}\). In the \(t^{th}\) communication round, the server first sends the current global model \(\Theta^{t}\) to the selected clients. Then, each selected client \(i\) trains a local model (denoted as \(\Theta^{t}_{i}\)) by fine-tuning the global model \(\Theta^{t}\) using its local training dataset \(\mathcal{D}_{i}\). For simplicity, we use \(\mathbf{z}=(\mathbf{x},y)\) to denote a training example in \(\mathcal{D}_{i}\), where \(\mathbf{x}\) is the training input (e.g., an image) and \(y\) is its ground truth label. Given \(\mathcal{D}_{i}\) and the global model \(\Theta^{t}\), the local objective is defined as \(\mathcal{L}(\mathcal{D}_{i};\Theta^{t})=\frac{1}{|\mathcal{D}_{i}|}\sum_{ \mathbf{x}\in\mathcal{D}_{i}}\ell(\mathbf{z};\Theta^{t})\) where \(\ell\) is a loss function (e.g., cross-entropy loss). The client can use gradient descent to update its local model based on the global model and its local training dataset, i.e., \(\Theta^{t}_{i}=\Theta^{t}-\eta_{l}\frac{\partial\mathcal{L}(\mathcal{D}_{i}; \Theta^{t})}{\partial\Theta^{t}}\), where \(\eta_{l}\) is the local learning rate. Note that stochastic gradient descent can also be used in practice. After that, the client sends \(g^{t}_{i}=\Theta^{t}_{i}-\Theta^{t}\) (called _local model update_) to the server. Note that it is equivalent for the client to send a local model or local model update to the server as \(\Theta^{t}_{i}=\Theta^{t}+g^{t}_{i}\). After receiving the local model updates from all clients, the server can aggregate them based on an aggregation rule \(\mathcal{R}\) (e.g., FedAvg [28]) to update its global model, i.e., we have \(\Theta^{t+1}=\Theta^{t}+\eta\mathcal{R}(g^{t}_{1},g^{t}_{2},\cdots,g^{t}_{| \mathcal{S}|})\), where \(|\mathcal{S}|\) represents the number of clients and \(\eta\) is the learning rate of the global model.

Threat Model.We consider attacks proposed in previous work [3; 54; 59]. We assume an attacker can compromise a set of clients (denoted as \(\mathcal{S}_{a}\)). To perform backdoor attacks, the attacker first selects an arbitrary backdoor trigger \(\delta\) and a target class \(y^{tc}\). For each client \(i\in\mathcal{S}_{a}\) in the \(t^{th}\) (\(t=1,2,\dots\)) communication round, the attacker can choose an arbitrary fraction (denoted as \(r^{t}_{i}\)) of training examples from the local training dataset of the client, embed the backdoor trigger \(\delta\) to training inputs, and relabel them as the target class \(y^{tc}\). In _state-of-the-art_ backdoor attacks [3; 54; 59], the attacker leverages those trigger-embedded training examples to inject the backdoor into the local model of a compromised client to attack the global model.

Following [7], we consider that the server itself has a small clean training dataset (denoted as \(\mathcal{D}_{s}\)), which could be collected from the same or different domains of the local training datasets of clients. Note that the server does not have 1) information on compromised clients, and 2) the poisoning ratio \(r^{t}_{i}\) (\(i\in\mathcal{S}_{a}\)), backdoor trigger \(\delta\), and target class \(y^{tc}\) selected by the attacker. In our threat model, we do not make any other assumptions for the server compared with standard federated learning [28].

## 4 Methodology

Overview.We formulate FedGame as a minimax game between the defender and attacker, which enables them to optimize their strategies respectively. In particular, the defender computes a genuine score for each client in each communication round. The goal of the defender is to maximize the genuine score for a benign client and minimize it for a compromised one. Given the genuine score for each client, we use a weighted average over all the local model updates to update the global model, i.e., we have

\[\Theta^{t+1}=\Theta^{t}+\eta\frac{1}{\sum_{i\in\mathcal{S}}p_{i}^{ t}}\sum_{i\in\mathcal{S}}p_{i}^{t}g_{i}^{t},\] (1)

where \(p_{i}^{t}\) is the genuine score for client \(i\) in the \(t^{th}\) communication round and \(\eta\) is the learning rate of the global model. We compute a weighted average of local models because we aim to achieve robustness against attacks while making minimal changes to FedAvg [28], which has been shown to achieve high utility in diverse settings. In our experiments, we empirically show that the genuine scores are almost constantly 0 for malicious clients after a few communication rounds, which indicates that the malicious model updates will not be aggregated at all, reducing Equation 1 to a robust FedAvg against attacks. (See Figure 2(c)(d) in the Appendix for more details.) Based on Equation 1, the effectiveness of the attack is determined by two components: genuine scores and local models of compromised clients. In our framework, the attacker optimizes the tradeoff between these two components to maximize the effectiveness of backdoor attacks.

### Game Formulation

Computing Genuine Scores.The key challenge for the server to compute genuine scores for clients is that the server can only access their local model updates, i.e., the server can only leverage local models of clients to compute genuine scores. To tackle this issue, we observe that the local model of a compromised client is more likely to predict a trigger-embedded input as the target class. Therefore, the server can first reverse engineer the backdoor trigger \(\delta_{re}\) and target class \(y_{re}^{tc}\) (which we will discuss more in the next subsection) and then use them to compute the genuine score for each client. Since the client \(i\) sends its local model update \(g_{i}^{t}\) to the server, the server can compute the local model of the client \(i\) as \(\Theta_{i}^{t}=\Theta^{t}+g_{i}^{t}\). With \(\Theta_{i}^{t}\), the server can compute \(p_{i}^{t}\) as follows:

\[p_{i}^{t}=1-\frac{1}{|\mathcal{D}_{s}|}\sum_{\mathbf{x}\in \mathcal{D}_{s}}\mathbb{I}(G(\mathbf{x}\oplus\delta_{re};\Theta_{i}^{t})=y_{ re}^{tc}),\] (2)

where \(\mathbb{I}\) is an indicator function, \(\mathcal{D}_{s}\) is the clean training dataset of the server, \(\mathbf{x}\oplus\delta_{re}\) is a trigger-embedded input, and \(G(\mathbf{x}\oplus\delta_{re};\Theta_{i}^{t})\) represents the label of \(\mathbf{x}\oplus\delta_{re}\) predicted by \(\Theta_{i}^{t}\). Intuitively, the genuine score for client \(i\) is small if a large fraction of inputs embedded with the reverse-engineered backdoor trigger is predicted as the target class by its local model. We note that the defender only needs to query local models to compute their genuine scores. Moreover, our defense is still applicable when differential privacy [49; 14] is used to protect the local models.

Optimization Problem of the Defender.The server aims to reverse engineer the backdoor trigger \(\delta_{re}\) and target class \(y_{re}^{tc}\) such that the genuine scores for compromised clients are minimized while those for benign clients are maximized. Formally, we have the following optimization problem:

\[\min_{\delta_{re},y_{re}^{tc}}(\sum_{i\in\mathcal{S}_{a}}p_{i}^{ t}-\sum_{j\in\mathcal{S}\setminus\mathcal{S}_{a}}p_{j}^{t}).\] (3)

Note that \(\mathcal{S}_{a}\) is the set of malicious clients that is unknown to the server. In the next subsection, we will discuss how to address this challenge to solve the optimization problem.

Optimization Problem of the Attack.The goal of an attacker is to maximize its attack effectiveness. Based on Equation 1, the attacker needs to: 1) maximize the genuine scores for compromised clients while minimizing them for benign ones, i.e., \(\max(\sum_{i\in\mathcal{S}_{a}}p_{i}^{t}-\sum_{j\in\mathcal{S}\setminus\mathcal{ S}_{a}}p_{j}^{t})\), and 2) make the local models of compromised clients predict an input embedded with the attacker-chosen backdoor trigger \(\delta\) as the target class \(y^{tc}\). To perform the backdoor attack in the \(t^{\mathrm{th}}\) communication round, the attacker embeds the backdoor to a certain fraction (denoted as \(r_{i}^{t}\)) of training examples in the local training dataset of the client and uses them as data augmentation. Intuitively, a larger \(r_{i}^{t}\) encourages a higher attack success rate but can potentially result in a lower genuine score. In other words, \(r_{i}^{t}\) measures a tradeoff between these two factors. Formally, the attacker can solve the following optimization problem to find the desired tradeoff:

\[\max_{R^{t},\delta}(\sum_{i\in\mathcal{S}_{a}}p_{i}^{t}-\sum_{j\in\mathcal{S} \setminus\mathcal{S}_{a}}p_{j}^{t}+\lambda\sum_{i\in\mathcal{S}_{a}}r_{i}^{t}),\] (4)

where \(R^{t}=\{r_{i}^{t}\mid i\in\mathcal{S}_{a}\}\), \(\delta\) is the trigger, and \(\lambda\) is a hyperparameter to balance the two terms.

Minimax Game.Given the optimization problems of the defender and attacker, we have the following minimax game:

\[\min_{\delta_{res},y_{res}^{tc}}\max_{R^{t},\delta}(\sum_{i\in\mathcal{S}_{a}}p _{i}^{t}-\sum_{j\in\mathcal{S}\setminus\mathcal{S}_{a}}p_{j}^{t}+\lambda\sum_ {i\in\mathcal{S}_{a}}r_{i}^{t}).\] (5)

Note that \(r_{i}^{t}\) (\(i\in\mathcal{S}_{a}\)) and \(\delta\) are chosen by the attacker. Thus, we can add them to the objective function in Equation 3 without influencing its solution given the local model updates of clients. In Section A of Appendix, we provide an intuitive interpretation regarding how to connect our above objective to the ultimate goal of the defender and attacker (the defender aims to obtain a clean model, while the attacker wants the global model to be backdoored).

### Solving the Minimax Game by the Defender

The key challenge for the server to solve Equation 5 is that it does not know \(\mathcal{S}_{a}\) (set of compromised clients). To address the challenge, our idea is to construct an _auxiliary global model_ based on local models of all clients. Suppose \(g_{i}^{t}\) is the local model update sent by each client \(i\in\mathcal{S}\) to the server. Our auxiliary global model is constructed as follows: \(\Theta_{a}^{t}=\Theta^{t}+\frac{1}{|\mathcal{S}|}\sum_{i\in\mathcal{S}}g_{i}^{t}\), where \(\Theta^{t}\) is the global model. Our intuition is that such a naively aggregated auxiliary global model is inclined to predict a trigger-embedded input as the target class under backdoor attacks. As a result, given the auxiliary global model, we can use an arbitrary existing method [42; 46; 38; 48] to reverse engineer the backdoor trigger and target class based on it, which enables us to compute genuine scores for clients based on Equation 2. With those genuine scores, we can use Equation 1 to update the global model to protect it from backdoor attacks in every communication round. The complete algorithm of our FedGame is shown in Algorithm 1 of Appendix.

Our framework is compatible with any trigger reverse engineering methods, allowing off-the-shelf incorporation of techniques developed for centralized supervised learning into federated learning. Note that developing a new reverse engineering method is not the focus of this work. Instead, our goal is to formulate the attack and defense against backdoor attacks on federated learning as a minimax game, which enables us to defend against dynamic attacks.

### Solving the Minimax Game by the Attacker

The goal of the attacker is to find \(r_{i}^{t}\) for each client \(i\in\mathcal{S}_{a}\) such that the loss function in Equation 5 is maximized. As the attacker does not know the genuine scores of benign clients, the attacker can find \(r_{i}^{t}\) to maximize \(p_{i}^{t}+\lambda r_{i}^{t}\) for client \(i\in\mathcal{S}_{a}\) to approximately solve the optimization problem in Equation 5. However, the key challenge is that the attacker does not know the reverse engineered backdoor trigger \(\delta_{re}\) and the target class \(y_{re}^{tc}\) of the defender to compute the genuine score for the client \(i\). In response, the attacker can use the backdoor trigger \(\delta\) and target class \(y_{c}^{tc}\) chosen by itself. Moreover, the attacker reserves a certain fraction (e.g., 10%) of training data from its local training dataset \(\mathcal{D}_{i}\) as the validation dataset (denoted as \(\mathcal{D}_{i}^{rev}\)) to find the best \(r_{i}^{t}\).

Estimating the Genuine Score Given \(r_{i}^{t}\).For a given \(r_{i}^{t}\), the client \(i\) can embed the backdoor to \(r_{i}^{t}\) fraction of training examples in \(\mathcal{D}_{i}\setminus\mathcal{D}_{i}^{rev}\) and then use those backdoored training examples to augment \(\mathcal{D}_{i}\setminus\mathcal{D}_{i}^{rev}\) to train a local model (denoted as \(\tilde{\Theta}_{i}^{t}\)). Then, the genuine score can be estimated as \(\tilde{p}_{i}^{t}=1-\frac{1}{|\mathcal{D}_{i}^{rev}|}\sum_{\mathbf{x}\in \mathcal{D}_{i}^{rev}}\mathbbm{I}(G(\mathbf{x}\oplus\delta;\tilde{\Theta}_{i}^ {t})=y^{tc})\), where \(G(\mathbf{x}\oplus\delta;\tilde{\Theta}_{i}^{t})\) is the predicted label by the global model \(\tilde{\Theta}_{i}^{t}\) for the trigger-embedded input \(\mathbf{x}\oplus\delta\).

Finding the Optimal \(r_{i}^{t}\).The client can use grid search to find \(r_{i}^{t}\) that achieves the largest \(\tilde{p}_{i}^{t}+\lambda r_{i}^{t}\). After estimating the optimal \(r_{i}^{t}\), client \(i\) can embed the backdoor to \(r_{i}^{t}\) fraction of training examples and utilize them to perform backdoor attacks based on state-of-the-art methods [3; 54; 59].

Trigger Optimization.The attacker can choose an arbitrary static trigger to launch backdoor attacks, or dynamically optimize it to make the attack more effective. Given the fixed location and bounding box of the backdoor trigger, the attacker can optimize the trigger pattern with gradient descent such that it is more likely for a backdoored input to be predicted as the target class. For instance, the attacker can optimize the trigger pattern to minimize the cumulative loss on all training data of malicious clients, i.e., \(\delta^{t}=\operatorname*{argmin}_{\delta^{*}}\sum_{\mathbf{x}\in\cup_{i\in \mathcal{S}_{0}}\mathcal{D}_{i}}\ell(\Theta^{t}(\mathbf{x}\oplus\delta^{*}),y ^{tc})\), where \(\Theta^{t}(\mathbf{x}\oplus\delta^{*})\) is the output of the model which is a probability simplex of all possible classes and \(\ell\) is a standard loss function (e.g., cross-entropy loss). The attacker can then solve Equation 5 with the optimized trigger. Recall that the attacker aims to find the largest \(\tilde{p}_{i}^{t}+\lambda r_{i}^{t}\). Therefore, if the resulting \(\tilde{p}_{i}^{t}+\lambda r_{i}^{t}\) given \(\delta^{t}\) is lower than that given the previous trigger \(\delta^{t-1}\), we let \(\delta^{t}=\delta^{t-1}\).

The complete algorithm for the compromised clients is shown in Algorithm 2 in Appendix C.

## 5 Theoretical Analysis

This section provides a theoretical analysis of FedGame under backdoor attacks. Suppose the global model parameters are in a bounded space. We derive an upper bound for the \(L_{2}\)-norm of the difference between the parameters of the global models with and without attacks. To analyze the robustness of FedGame, we make the following assumptions, which are commonly used in the analysis of previous studies [25, 45, 16, 35, 53, 7, 9] on federated learning.

**Assumption 5.1**.: The loss function is \(\mu\)-strongly convex with \(L\)-Lipschitz continuous gradient. Formally, we have the following for arbitrary \(\Theta\) and \(\Theta^{\prime}\):

\[(\nabla_{\Theta}\ell(\mathbf{z};\Theta)-\nabla_{\Theta^{\prime} }\ell(\mathbf{z};\Theta^{\prime}))^{T}(\Theta-\Theta^{\prime})\geq\mu\left\| \Theta-\Theta^{\prime}\right\|_{2}^{2},\] (6) \[\left\|\nabla_{\Theta}\ell(\mathbf{z};\Theta)-\nabla_{\Theta^{ \prime}}\ell(\mathbf{z};\Theta^{\prime})\right\|_{2}\leq L\left\|\Theta- \Theta^{\prime}\right\|_{2},\] (7)

where \(\mathbf{z}\) is an arbitrary training example.

**Assumption 5.2**.: We assume the gradient \(\nabla_{\Theta}\ell(\mathbf{z};\Theta)\) is bounded with respect to \(L_{2}\)-norm for arbitrary \(\Theta\) and \(\mathbf{z}\), i.e., there exists some \(M\geq 0\) such that

\[\left\|\nabla_{\Theta}\ell(\mathbf{z};\Theta)\right\|_{2}\leq M.\] (8)

Suppose \(\Theta_{c}^{t}\) is the global model trained by FedGame without any attacks in the \(t\)th communication round, i.e., each client \(i\in\mathcal{S}\) uses its clean local training dataset \(\mathcal{D}_{i}\) to train a local model. Moreover, we assume gradient descent is used by each client to train its local model. Suppose \(q_{i}^{t}\) is the genuine score for client \(i\) without attacks. Moreover, we denote \(\beta_{i}^{t}=\frac{q_{i}^{t}}{\sum_{i\in\mathcal{S}}q_{i}^{t}}\) as the normalized genuine score for client \(i\). To perform the backdoor attack, we assume a compromised client \(i\) can embed the backdoor trigger to \(r_{i}^{t}\) fraction of training examples in the local training dataset of the client and relabel them as the target class. Those backdoored training examples are used to augment the local training dataset of the client. Suppose \(\Theta^{t}\) is the global model under the backdoor attack in the \(t\)th communication round with our defense. We denote \(\alpha_{i}^{t}=\frac{p_{i}^{t}}{\sum_{i\in\mathcal{S}}p_{i}^{t}}\) as the normalized genuine score for client \(i\) with attacks in the \(t\)th communication round. Formally, we have:

**Lemma 5.3** (Robustness Guarantee for One Communication Round).: _Suppose Assumptions 5.1 and 5.2 hold. Moreover, we assume \((1-r^{t})\beta_{i}^{t}\leq\alpha_{i}^{t}\leq(1+r^{t})\beta_{i}^{t}\), where \(i\in\mathcal{S}\) and \(r^{t}=\sum_{j\in\mathcal{S}_{a}}r_{j}^{t}\). Then, we have:_

\[\|\Theta^{t+1}-\Theta_{c}^{t+1}\|_{2}\] (9) \[\leq \sqrt{1-\eta\mu+2\eta\gamma^{t}+\eta^{2}L^{2}+2\eta^{2}L\gamma^{t} }\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}+\sqrt{2\eta\gamma^{t}(1+\eta L+ 2\eta\gamma^{t})}+2\eta r^{t}M,\] (10)

_where \(\eta\) is the learning rate of the global model, \(L\) and \(\mu\) are defined in Assumption 5.1, \(\gamma^{t}=\sum_{i\in\mathcal{S}_{a}}\alpha_{i}^{t}r_{i}^{t}M\), and \(M\) is defined in Assumption 5.2._

Proof sketch.: Our idea is to decompose \(\left\|\Theta^{t+1}-\Theta_{c}^{t+1}\right\|_{2}\) into two terms. Then, we derive an upper bound for each term based on the change of the local model updates of clients under backdoor attacks and the properties of the loss function. As a result, our derived upper bound relies on \(r_{i}^{t}\) for each client \(i\in\mathcal{S}_{a}\), parameters \(\mu\), \(L\), and \(M\) in our assumptions, as well as the parameter difference of the global models in the previous iteration, i.e., \(\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}\). Our complete proof is in Appendix B.1.

In the above lemma, we derive an upper bound of \(\left\|\Theta^{t+1}-\Theta_{c}^{t+1}\right\|_{2}\) with respect to \(\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}\) for one communication round. In the next theorem, we derive an upper bound of \(\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}\) as \(t\rightarrow\infty\). We iterative apply Lemma 5.3 for successive values of \(t\) and have the following theorem:

**Theorem 5.4** (Robustness Guarantee).: _Suppose Assumptions 5.1 and 5.2 hold. Moreover, we assume \((1-r^{t})\beta_{i}^{t}\leq\alpha_{i}^{t}\leq(1+r^{t})\beta_{i}^{t}\) for \(i\in\mathcal{S}\), \(\gamma^{t}\leq\gamma\) and \(r^{t}\leq r\) hold for all communication round \(t\), and \(\mu>2\gamma\), where \(r^{t}=\sum_{j\in\mathcal{S}_{a}}r_{j}^{t}\) and \(\gamma^{t}=\sum_{i\in\mathcal{S}_{a}}\alpha_{i}^{t}r_{i}^{t}M\). Let the global model learning rate by chosen as \(0<\eta<\frac{\mu-2\gamma}{L^{2}+2L\gamma}\). Then, we have:_

\[\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}\leq\frac{\sqrt{2\eta\gamma(1+ \eta L+2\eta\gamma)}+2\eta rM}{1-\sqrt{1-\eta\mu+2\eta\gamma+\eta^{2}L^{2}+2 \eta^{2}L\gamma}}\] (11)

_holds as \(t\rightarrow\infty\)._

Proof sketch.: Given the conditions that \(\gamma^{t}\leq\gamma\) and \(r^{t}\leq r\) as well as the fact that the right-hand side of Equation 10 is monotonic with respect to \(\gamma^{t}\) and \(r^{t}\), we can replace \(\gamma^{t}\) and \(r^{t}\) in Equation 10 with \(\gamma\) and \(r\). Then, we iterative apply the equation for successive values of \(t\). When \(0<\eta<\frac{\mu-2\gamma}{L^{2}+2L\gamma}\), we have \(0<1-\eta\mu+2\eta\gamma+\eta^{2}L^{2}+2\eta^{2}L\gamma<1\). By letting \(t\rightarrow\infty\), we can reach the conclusion. The complete proof can be found in Appendix B.2. 

Our theorem implies that the global model parameters under our defense against adaptive attacks do not deviate too much from those of the global model without attack when the fraction of backdoored training examples \(r^{t}\) is bounded.

## 6 Experiments

In order to thoroughly evaluate the effectiveness of FedGame, we conduct comprehensive experiments on 1) evaluation against three _state-of-the-art_ backdoor attacks: Scaling attack [3], DBA [54], and Neurotoxin [59], 2) comparison with six existing baselines including Krum, Median, Norm-Clipping, Differential Privacy, DeepSight, FLTrust, 3) evaluation against strong adaptive attacks, and 4) comprehensive ablation studies.

### Experimental Setup

Datasets and Models.We use two benchmark datasets: MNIST [23] and CIFAR10 [22] for FL tasks. MNIST has 60,000 training and 10,000 testing images, each of which has a size of 28 \(\times\) 28 belonging to one of 10 classes. CIFAR10 consists of 50,000 training and 10,000 testing images with a size of 32 \(\times\) 32. Each image is categorized into one of 10 classes. For each dataset, we randomly sample 90% of training data for clients, and the remaining 10% of training data is reserved to evaluate our defense when the clean training dataset of the server is from the same domain as those of clients. We use a CNN with two convolution layers (detailed architecture can be found in Table 2 in Appendix) and ResNet-18 [19] which is pre-trained on ImageNet [13] as the global models for MNIST and CIFAR10.

FL Settings.We consider two settings: local training data are independently and identically distributed (IID) among clients, and non-IID. We follow the previous work [17] to distribute training data to clients by using a parameter \(q\) to control the degree of non-IID, which models the probability that training images from one category are distributed to a particular client (or a set of clients). We set \(q=0.5\) by following [17]. Moreover, we train a global model based on 10 clients for 200 iterations with a global learning rate \(\eta=1.0\). In each communication round, we use SGD to train the local model of each client for two epochs with a local learning rate of 0.01.

Attack Settings.We consider three _state-of-the-art_ backdoor attacks on federated learning, i.e., Scaling attack [3], DBA [54], and Neurotoxin [59]. For Scaling attack, we set the scaling parameter to be #total clients/(\(\eta\times\)#compromised clients) by following [3]. For Neurotoxin, we set the ratio of masked gradients to be 1%, following the choice in [59]. We use the same backdoor trigger and target class as used in those works. By default, we assume 60% of clients are compromised by an attacker. When the attacker solves the minimax game in Equation 5, we set the default \(\lambda=1\). We explore the impact of \(\lambda\) in our experiments. We randomly sample 10% of the local data of each compromised client as validation data to search for an optimal \(r_{i}^{t}\). Moreover, we set the granularity of grid search to 0.1 when searching for \(r_{i}^{t}\).

Baselines.We compare our defense with the following methods: FedAvg [28], Krum [6], Median [56], Norm-Clipping [39], Differential Privacy (DP) [39], DeepSight [36], and FLTrust [7]. Please see Appendix D.2 for parameter settings for those baselines.

Evaluation Metrics.We use _testing accuracy (TA)_ and _attack success rate (ASR)_ as evaluation metrics. Concretely, TA is the fraction of clean testing inputs that are correctly predicted, and ASR refers to the fraction of backdoored testing inputs that are predicted as the target class.

Defense Settings.We consider two settings: in-domain and out-of-domain. For the in-domain setting, we consider the clean training dataset of the server is from the same domain as the local training datasets of clients. We use the reserved data as the clean training dataset of the server for each dataset. For the out-of-domain setting, we consider the server has a clean training dataset that is from a different domain than the FL task. In particular, we randomly sample 6,000 images from FashionMNIST [52] for MNIST and sample 5,000 images from GTSRB [20] for CIFAR10 as the clean training dataset of the server. Unless otherwise mentioned, we adopt Neural Cleanse [42] to reverse engineer the backdoor trigger and target class.

### Experimental Results

We show the results of FedGame compared with existing defenses under IID and non-IID settings in Table 1. We defer the results against Neurotoxin to Appendix D.3 due to space limitations. We have the following observations from the experimental results. First, FedGame outperforms all existing defenses in terms of ASR. In particular, FedGame can reduce ASR to random guessing (i.e., ASR of FedAvg under no attacks) in both IID and non-IID settings for clients as well as both in-domain and out-of-domain settings for the server. Intrinsically, FedGame performs better because our game-theoretic defense enables the defender to optimize its strategy against dynamic, adaptive attacks. We

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Attacks} & Client & \multicolumn{4}{c}{FedAvg} & \multicolumn{4}{c}{Defenses (Under attacks)} \\ \cline{3-13}  & Data & \multirow{3}{*}{Datasets} & \multirow{3}{*}{Metrics} & \multirow{3}{*}{(Without attacks)} & \multirow{3}{*}{FedAvg} & \multirow{3}{*}{Krum} & \multirow{3}{*}{Median} & \multirow{3}{*}{Norm-Clipping} & \multirow{3}{*}{DP} & \multirow{3}{*}{Depth} & \multirow{3}{*}{FLTrust} & \multicolumn{2}{c}{FedGame} \\ \cline{3-13}  & & & & & & & & & & & & & In Out-of \\ \cline{3-13}  & \multirow{3}{*}{IID} & \multirow{3}{*}{MNIST} & TA (\%) & 99.04 & 98.77 & 98.78 & 99.17 & 95.48 & 92.97 & 97.69 & 97.93 & 98.53 & 98.56 \\ \cline{3-13}  & & ASR (\%) & 9.69 & 99.99 & 99.99 & 99.97 & 98.54 & 99.45 & 20.03 & 16.01 & **9.72** & **9.68** \\ \cline{3-13}  & \multirow{3}{*}{CIFAR10} & \multirow{3}{*}{CIFAR10} & TA (\%) & 81.08 & 80.51 & 76.44 & 80.17 & 80.38 & 43.22 & 76.79 & 75.71 & 74.81 & 74.65 \\ \cline{3-13}  & & & ASR (\%) & 8.39 & 99.80 & 99.94 & 99.82 & 99.87 & 99.58 & 98.58 & 99.46 & **8.92** & **9.24** \\ \cline{3-13}  & \multirow{3}{*}{non-IID} & \multirow{3}{*}{MNIST} & TA (\%) & 98.98 & 99.15 & 96.88 & 99.12 & 94.54 & 91.52 & 97.39 & 97.68 & 98.28 & 98.34 \\ \cline{3-13}  & & ASR (\%) & 9.73 & 99.99 & 85.03 & 99.98 & 98.16 & 99.54 & 20.03 & 19.61 & **10.42** & **10.88** \\ \cline{3-13}  & \multirow{3}{*}{CIFAR10} & \multirow{3}{*}{CIFAR10} & TA (\%) & 80.25 & 75.35 & 67.66 & 79.54 & 70.18 & 50.79 & 77.76 & 75.08 & 73.88 & 73.57 \\ \cline{3-13}  & & ASR (\%) & 9.67 & 99.92 & 99.92 & 99.99 & 99.63 & 95.01 & 99.03 & 99.82 & **11.76** & **12.03** \\ \hline \multirow{3}{*}{DBA} & \multirow{3}{*}{IID} & \multirow{3}{*}{MNIST} & TA (\%) & 99.04 & 99.03 & 98.87 & 98.98 & 98.99 & 98.96 & 97.98 & 97.84 & 98.05 \\ \cline{3-13}  & & ASR (\%) & 9.69 & 100.00 & 10.06 & 99.81 & 99.75 & 99.73 & 15.02 & 10.02 & **9.56** & **9.68** \\ \cline{3-13}  & \multirow{3}{*}{CIFAR10} & \multirow{3}{*}{CIFAR10} & TA (\%) & 81.08 & 80.90 & 76.09 & 80.00 & 80.21 & 41.36 & 72.13 & 75.17 & 73.18 & 72.93 \\ \cline{3-13}  & & ASR (\%) & 8.39 & 93.44 & 94.97 & 91.60 & 91.90 & 86.96 & 83.26 & 66.58 & **8.81** & **9.00** \\ \cline{3-13}  & \multirow{3}{*}{non-IID} & \multirow{3}{*}{MNIST} & TA (\%) & 98.98 & 98.98 & 98.58 & 99.13 & 93.98 & 88.91 & 96.65 & 97.62 & 98.58 & 98.59 \\ \cline{3-13}  & & ASR (\%) & 9.73 & 100.00 & 10.52 & 99.85 & 55.97 & 99.66 & 13.21 & 10.19 & **9.97** & **9.83** \\ \cline{3-13}  & \multirow{3}{*}{CIFAR10} & \multirow{3}{*}{CIFAR10} & TA (\%) & 80.25 & 80.15 & 74.31 & 79.78 & 78.78 & 38.17 & 73.83 & 74.57 & 73.52 & 73.21 \\ \cline{3-13}  & & ASR (\%) & 9.67 & 95.03 & 60.06 & 95.00 & 92.51 & 99.51 & 80.75 & 74.35 & **10.62** & **10.67** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of FedGame with existing defenses under Scaling attack and DBA. The total number of clients is 10, where 60% are compromised. The best results for each setting among FedGame and existing defenses are bold.

note that FLTrust outperforms other defenses (except FedGame) in most cases since it exploits a clean training dataset from the same domain as local training datasets of clients. However, FLTrust is not applicable when the server only holds an out-of-domain clean training dataset, while FedGame can relax such an assumption and will still be applicable. Moreover, our experimental results indicate that FedGame achieves comparable performance even if the server holds an out-of-domain clean training dataset.

To further understand our results,we visualize the average genuine (or trust) scores computed by FedGame (or FLTrust) for compromised and benign clients in Appendix D.4. In particular, we find that the genuine scores produced by FedGame are much lower than those produced by FLTrust for compromised clients, which explains why FedGame outperforms FLTrust. Second, FedGame achieves comparable TA with existing defenses, indicating that FedGame preserves the utility of global models.

Furthermore, we show the comparison results of FedGame with existing defenses against the scaling attack when the total number of clients is 30 in Table 4 in Appendix D.5. Our observations are similar, which indicates that FedGame consistently outperforms existing defenses under different numbers of clients and backdoor attacks.

Impact of \(\lambda\).\(\lambda\) is a hyperparameter used by an attacker (see Eqn. 4) when searching for the optimal \(r_{i}^{t}\) for each compromised client \(i\) in each communication round \(t\). Figure 1(a) shows the impact of \(\lambda\) on ASR of FedGame. The results show that FedGame is insensitive to different \(\lambda\)'s. The reason is that the genuine score for a compromised client is small when \(\lambda\) is large, and the local model of a compromised client is less likely to predict a trigger-embedded input as the target class when \(\lambda\) is small. As a result, backdoor attacks with different \(\lambda\) are ineffective under FedGame.

Impact of the Fraction of Compromised Clients.Figure 1(b) shows the impact of the fraction of compromised clients on ASR. As the results show, FedGame is effective for a different fraction of compromised clients in both in-domain and out-of-domain settings. In contrast, FLTrust is ineffective when the fraction of compromised clients is large. For instance, FedGame can achieve 9.84% (in-domain) and 10.12% (out-of-domain) ASR even if 80% of clients are compromised on MNIST. Under the same setting, the ASR of FLTrust is 99.95%, indicating that the defense fails.

Impact of Client Selection.By default, we consider that all clients are selected in each communication round. We also consider only a subset of clients are selected in each communication round by the server. Figure 1(c) shows our experimental results. Our results suggest that our defense is effective and consistently outperforms FLTrust.

Computation Cost.FedGame computes a genuine score for each client in each communication round. Here we demonstrate its computational efficiency. On average, it takes 0.148s to compute a genuine score for each client in each communication round on a single NVIDIA 2080 Ti GPU. We note that the server could from a resourceful tech company (e.g., Google, Meta, Apple), which would have enough computation resources to compute it for millions of clients. Moreover, those local models can be evaluated in parallel.

Figure 1: Comparing FedGame and FLTrust under different variations of attack. (a) Different \(\lambda\). (b) Different fractions of malicious clients. (c) Different numbers of clients selected in each communication round. (d) Different trigger sizes.

Performance under Static Attacks.In our evaluation, we consider an attacker optimizing the fraction of backdoored training examples. We also evaluate FedGame under existing attacks where the attacker does not optimize it. Under the default setting, FedGame can achieve an ASR of 9.75%, indicating that our defense is effective under static attack.

Furthermore, we discuss the impact of trigger reverse engineering methods, the total number of clients, and the size of clean data of the server in Appendix D.6.

### Adaptive Attacks

In this subsection, we discuss some potential adaptive strategies that may be leveraged by the attacker to evade our defense.

Data Replacing Attack.By default, we consider an attacker optimizing the fraction of backdoored training examples _added_ to the local training dataset of a compromised client to maximize backdoor effectiveness. We also consider an attacker who _replaces_ a fraction of the local training data with backdoored samples and optimizes such fraction. Under our default setting, the ASR of FedGame is 9.71% and 10.69% on MNIST and CIFAR10, respectively, indicating that our defense is still effective under data replacing attacks.

Variation in Triggers.We demonstrate that our defense is robust to variation in triggers. In particular. we try triggers with sizes \(2\times 2\), \(4\times 4\), and \(6\times 6\) under the default setting. Figure 1(d) compares FedGame with FLTrust. The results demonstrate that FedGame is consistently effective under triggers with different sizes.

We note that although an attacker can slightly manipulate the parameters of local models such that they are more similar to those of benign clients, FedGame does not rely on model parameters for detection. Instead, FedGame leverages the model behaviors, i.e., whether the model predicts inputs with our reverse-engineered trigger as the target class. As a result, our defense would still be effective even if the change in the model parameters is small as long as the model has backdoor behavior (this is required to make the attack effective). This is also the reason why our defense is better than existing methods such as FLTrust which leverages model parameters for defense.

## 7 Conclusion and Future Work

In this work, we propose FedGame, a general game-theory based defense against adaptive backdoor attacks in federated learning. Our formulated minimax game enables the defender and attacker to dynamically optimize their strategies. Moreover, we respectively design solutions for both of them to solve the minimax game. We perform theoretical analysis and empirical evaluations for our framework. Our results demonstrate the effectiveness of FedGame under strategic backdoor attackers. Moreover, FedGame achieves significantly higher robustness than baselines in different settings.

We consider that an attacker could optimize the poisoning ratio and trigger pattern in our work. We believe it is an interesting future work to consider other factors for the attacker (e.g., trigger size, consideration of long-term goal, design of new loss functions [12], poisoned neurons selection [2]). Moreover, we consider a zero-sum Stackelberg game in this work. Another interesting future work is to consider other game formulations, e.g., Bayesian games.

## Acknowledgements

This work is partially supported by the National Science Foundation (NSF) under grants No.1910100, No.2046726, No.2153136, No.2229876, DARPA GARD, the National Aeronautics and Space Administration (NASA) under grant No.80NSSC20M0229, Alfred P. Sloan Fellowship, Office of Naval Research (ONR) under grant N00014-23-1-2386, Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, and the Amazon research award.

## References

* [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _ACM SIGSAC Conference on Computer and Communications Security_, pages 308-318, 2016.
* [2] Manaar Alam, Esha Sarkar, and Michail Maniatakos. Perdoor: Persistent non-uniform backdoors in federated learning using adversarial perturbations. _arXiv preprint arXiv:2205.13523_, 2022.
* [3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2938-2948. PMLR, 2020.
* [4] Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [5] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learning through an adversarial lens. In _International Conference on Machine Learning_, pages 634-643. PMLR, 2019.
* [6] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* [7] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. FLtrust: Byzantine-robust federated learning via trust bootstrapping. In _Network and Distributed Systems Security Symposium_, 2021.
* [8] Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against malicious clients. In _AAAI Conference on Artificial Intelligence_, volume 35, pages 6885-6893, 2021.
* [9] Xiaoyu Cao, Jinyuan Jia, Zaixi Zhang, and Neil Zhenqiang Gong. Fedrecover: Recovering from poisoning attacks in federated learning using historical information. In _2023 IEEE Symposium on Security and Privacy (SP)_, pages 1366-1383. IEEE, 2023.
* [10] Xiaoyu Cao, Zaixi Zhang, Jinyuan Jia, and Neil Zhenqiang Gong. FLcert: Provably secure federated learning against poisoning attacks. _IEEE Transactions on Information Forensics and Security_, 17:3691-3705, 2022.
* [11] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_, 2017.
* [12] Yanbo Dai and Songze Li. Chameleon: Adapting to peer images for planting durable backdoors in federated learning. _arXiv preprint arXiv:2304.12961_, 2023.
* [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [14] Cynthia Dwork. Differential privacy. In _Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33_, pages 1-12. Springer, 2006.
* [15] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography Conference_, pages 265-284. Springer, 2006.
* [16] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning approach. In _Conference on Neural Information Processing Systems_, 2020.
* [17] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to Byzantine-Robust federated learning. In _29th USENIX Security Symposium_, pages 1605-1622, 2020.

* [18] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [20] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In _International Joint Conference on Neural Networks_, number 1288, 2013.
* [21] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [22] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* [23] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* [24] Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients for robust federated learning. _arXiv preprint arXiv:2002.00211_, 2020.
* [25] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of FedAvg on non-iid data. In _International Conference on Learning Representations_, 2020.
* [26] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16463-16472, 2021.
* [27] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In _NDSS_, 2017.
* [28] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial Intelligence and Statistics_, pages 1273-1282. PMLR, 2017.
* [29] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In _International Conference on Learning Representations_, 2017.
* [30] Anh Nguyen and Anh Tran. Wanet-imperceptible warping-based backdoor attack. _arXiv preprint arXiv:2102.10369_, 2021.
* [31] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Mollering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, et al. Flame: Taming backdoors in federated learning. _Cryptology ePrint Archive_, 2021.
* [32] Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Mollering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, et al. FLAME: Taming backdoors in federated learning. In _31st USENIX Security Symposium_, pages 1415-1432, 2022.
* [33] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. _Advances in Neural Information Processing Systems_, 33:3454-3464, 2020.
* [34] Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel. Defending against backdoors in federated learning with robust learning rate. In _AAAI Conference on Artificial Intelligence_, volume 35, pages 9268-9276, 2021.
* [35] Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. _Advances in Neural Information Processing Systems_, 33:21554-21565, 2020.

* [36] Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, and Ahmad-Reza Sadeghi. DeepSight: Mitigating backdoor attacks in federated learning through deep model inspection. In _Network and Distributed System Security Symposium_, 2022.
* [37] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 11957-11965, 2020.
* [38] Mingjie Sun and Zico Kolter. Single image backdoor inversion via robust smoothed classifiers. _arXiv preprint arXiv:2303.00215_, 2023.
* [39] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? _arXiv preprint arXiv:1911.07963_, 2019.
* [40] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. _arXiv preprint arXiv:1912.02771_, 2019.
* [41] Akshaj Veldanda and Siddharth Garg. On evaluating neural network backdoor defenses. _arXiv preprint arXiv:2010.12186_, 2020.
* [42] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural Cleanse: Identifying and mitigating backdoor attacks in neural networks. In _2019 IEEE Symposium on Security and Privacy_, pages 707-723. IEEE, 2019.
* [43] Hang Wang, Zhen Xiang, David J Miller, and George Kesidis. Universal post-training backdoor detection. _arXiv preprint arXiv:2205.06900_, 2022.
* [44] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. _Advances in Neural Information Processing Systems_, 33:16070-16084, 2020.
* [45] Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms. _Journal of Machine Learning Research_, 22(213):1-50, 2021.
* [46] Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, and Shiqing Ma. Rethinking the reverse-engineering of trojan triggers. In _Advances in Neural Information Processing Systems_, 2022.
* [47] Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, and Shiqing Ma. Rethinking the reverse-engineering of trojan triggers. _arXiv preprint arXiv:2210.15127_, 2022.
* [48] Zhenting Wang, Kai Mei, Juan Zhai, and Shiqing Ma. Unicorn: A unified backdoor trigger inversion framework. In _The Eleventh International Conference on Learning Representations_, 2023.
* [49] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. _IEEE Transactions on Information Forensics and Security_, 15:3454-3469, 2020.
* [50] Yuxin Wen, Jonas Geiping, Liam Fowl, Hossein Souri, Rama Chellappa, Micah Goldblum, and Tom Goldstein. Thinking two moves ahead: Anticipating other users improves backdoor attacks in federated learning. _arXiv preprint arXiv:2210.09305_, 2022.
* [51] Chen Wu, Xian Yang, Sencun Zhu, and Prasenjit Mitra. Mitigating backdoor attacks in federated learning. _arXiv preprint arXiv:2011.01767_, 2020.
* [52] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [53] Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. CRFL: Certifiably robust federated learning against backdoor attacks. In _38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 11372-11382. PMLR, 2021.

* [54] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. DBA: Distributed backdoor attacks against federated learning. In _International Conference on Learning Representations_, 2020.
* [55] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In _Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security_, pages 2041-2055, 2019.
* [56] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In _International Conference on Machine Learning_, pages 5650-5659. PMLR, 2018.
* [57] Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, et al. Flip: A provable defense framework for backdoor mitigation in federated learning. _arXiv preprint arXiv:2210.12873_, 2022.
* [58] Zaixi Zhang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. FLDetector: Defending federated learning against model poisoning attacks via detecting malicious clients. In _28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2545-2555, 2022.
* [59] Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael Mahoney, Prateek Mittal, Ramchandran Kannan, and Joseph Gonzalez. Neurotoxin: Durable backdoors in federated learning. In _International Conference on Machine Learning_, pages 26429-26446. PMLR, 2022.

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_EMPTY:17]

\[\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}\leq A\left\|\Theta^{t-1}- \Theta_{c}^{t-1}\right\|_{2}+B.\] (58)

Then, we can iteratively apply the above equation to prove our theorem. In particular, we have:

\[\left\|\Theta^{t}-\Theta_{c}^{t}\right\|_{2}\] \[\leq A\left\|\Theta^{t-1}-\Theta_{c}^{t-1}\right\|_{2}+B\] (59) \[\leq A(A\left\|\Theta^{t-2}-\Theta_{c}^{t-2}\right\|_{2}+B)+B\] (60) \[= A^{2}\left\|\Theta^{t-2}-\Theta_{c}^{t-2}\right\|_{2}+(A^{1}+A^{ 0})B\] (61) \[\leq A^{t}\left\|\Theta^{0}-\Theta_{c}^{0}\right\|_{2}+(A^{t-1}+A^{ t-2}+\cdots+A^{0})B\] (62) \[= A^{t}\left\|\Theta^{0}-\Theta_{c}^{0}\right\|_{2}+\frac{1-A^{t} }{1-A}B\] (63) \[= (\sqrt{1-\eta\mu+2\eta\gamma+\eta^{2}L^{2}+2\eta^{2}L\gamma})^{t }\left\|\Theta^{0}-\Theta_{c}^{0}\right\|_{2}\] \[\quad\quad+\frac{1-(\sqrt{1-\eta\mu+2\eta\gamma+\eta^{2}L^{2}+2 \eta^{2}L\gamma})^{t}}{1-\sqrt{1-\eta\mu+2\eta\gamma+\eta^{2}L^{2}+2\eta^{2}L \gamma}}(\sqrt{2\eta\gamma(1+\eta L+2\eta\gamma)}+2\eta rM),\] (64)

When the learning rate satisfies \(0<\eta<\frac{\mu+2\gamma}{L^{2}+2L\gamma}\), we have that \(0<1-\eta\mu+2\eta\gamma+\eta^{2}L^{2}+2\eta^{2}L\gamma<1\). Therefore, the upper bound becomes \(\frac{\sqrt{2\eta\gamma(1+\eta L+2\eta\gamma)}+2\eta rM}{1-\sqrt{1-\eta\mu+2 \eta\gamma+\eta^{2}L^{2}+2\eta^{2}L\gamma}}\) as \(t\rightarrow\infty\). Hence, we prove our Theorem 5.4.

## Appendix C Complete Algorithms

### Complete Algorithm of FedGame

Algorithm 1 shows the complete algorithm of FedGame. In Line 3, we construct an auxiliary global model. In Line 4, the function ReverseEngineer is used to reverse engineer the backdoor trigger and target class. In Line 6, we compute the local model of client \(i\) based on its local model update. In Line 7, we compute a genuine score for client \(i\). In Line 9, we update the global model based on genuine scores and local model updates of clients.

``` Input:\(\Theta^{t}\) (global model in the \(t^{\mathrm{th}}\) communication round), \(g^{t}_{i},i\in\mathcal{S}\) (local model updates of clients), \(\mathcal{D}_{s}\) (clean training dataset of server), \(\eta\) (learning rate of global model). Output:\(\Theta^{t+1}\) (global model for the \((t+1)^{\mathrm{th}}\) communication round) \(\Theta^{t}_{a}=\Theta^{t}+\frac{1}{|\mathcal{S}|}\sum_{i\in\mathcal{S}}g^{t}_{i}\) \(\delta_{re},y^{tc}_{re}=\textsc{ReverseEngineer}(\Theta^{t}_{a})\) for\(i\in\mathcal{S}\)do \(\Theta^{t}_{i}=\Theta^{t}+g^{t}_{i}\) \(p^{t}_{i}=1-\frac{1}{|\mathcal{D}_{s}|}\sum_{\mathbf{x}\in\mathcal{D}_{s}} \mathbb{I}(G(\mathbf{x}\oplus\delta_{re};\Theta^{t}_{i})=y^{tc}_{re})\) endfor \(\Theta^{t+1}=\Theta^{t}+\eta\frac{1}{\sum_{i\in\mathcal{S}}p^{t}_{i}}\sum_{i \in\mathcal{S}}p^{t}_{i}g^{t}_{i}\) return\(\Theta^{t+1}\) ```

**Algorithm 1** FedGAME

### Complete Algorithm for a Compromised Client

Algorithm 2 shows the complete algorithm for a compromised client. In Line 3, we randomly subsample \(\rho_{i}\) fraction of training data from \(\mathcal{D}_{i}\). In Line 7, the function CreateBackdooredData is used to generate backdoored training examples by embedding the backdoor trigger \(\delta\) to \(\lfloor\min(j*\zeta,1)|\mathcal{D}_{i}\setminus\mathcal{D}^{rev}_{i}|\rfloor\) training examples in \(\mathcal{D}_{i}\setminus\mathcal{D}^{rev}_{i}\) and relabel them as \(y^{tc}\), where \(|\cdot|\) measures the number of elements in a set. In Line 8, the function TrainingLocalModel is used to train the local model on the training dataset \(\mathcal{D}^{\prime}_{i}\cup\mathcal{D}_{i}\setminus\mathcal{D}^{rev}_{i}\). In Line 9, we estimate a genuine score. In Line 15, we use the function CreateBackdooredData to generate backdoored training examples by embedding the backdoor trigger \(\delta\) to \(\lfloor\min(o*\zeta,1)|\mathcal{D}_{i}|\rfloor\) training examples in \(\mathcal{D}_{i}\) and relabel them as \(y^{tc}\). In Line 16, we use the function TrainingLocalModel to train a local model and utilize existing state-of-the-art attacks to inject the backdoor based on the training dataset \(\mathcal{D}^{\prime}_{i}\cup\mathcal{D}_{i}\).

```
1:Input:\(\Theta^{t}\) (global model in the \(t^{\mathrm{th}}\) communication round), \(\mathcal{D}_{i}\) (local training dataset of client \(i\)), \(\rho_{i}\) (fraction of reserved data to find optimal \(r^{t}_{i}\)), \(\zeta\) (granularity of searching for \(r^{t}_{i}\)), \(\delta\) (backdoor trigger), \(y^{tc}\) (target class), and \(\lambda\) (hyperparameter)
2:Output:\(g^{t}_{i}\) (local model update)
3:\(\mathcal{D}^{rev}_{i}\) = RandomSampling\((\mathcal{D}_{i},\rho_{i})\)
4:\(count=\lceil\frac{1}{\zeta}\rceil\)
5:\(max\_value,o\gets 0,0\)
6:for\(j\gets 0\) to count do
7:\(\mathcal{D}^{\prime}_{i}\) = CreateBackdooredData\((\mathcal{D}_{i}\setminus\mathcal{D}^{rev}_{i},\delta,y^{tc},\min(j*\zeta,1))\)
8:\(\Theta_{ij}=\textsc{TrainingLocalModel}(\Theta^{t},\mathcal{D}^{\prime}_{i} \cup\mathcal{D}_{i}\setminus\mathcal{D}^{rev}_{i})\)
9:\(p_{ij}=1-\frac{1}{|\mathcal{D}^{rev}_{i}|}\sum_{\mathbf{x}\in\mathcal{D}^{rev }_{i}}\mathbb{I}(G(\mathbf{x}\oplus\delta;\Theta_{ij})=y^{tc})\)
10:if\(p_{ij}+\lambda\min(j*\zeta,1)>max\_value\)then
11:\(o=j\)
12:\(max\_value=p_{ij}+\lambda\min(j*\zeta,1)\)
13:endif
14:endfor
15:\(\mathcal{D}^{\prime}_{i}\) = CreateBackdooredData\((\mathcal{D}_{i},\delta,y^{tc},\min(o*\zeta,1))\)
16:\(\Theta^{t}_{i}\) = TrainingLocalModel\((\Theta^{t},\mathcal{D}^{\prime}_{i}\cup\mathcal{D}_{i})\)
17:return\(\Theta^{t}_{i}-\Theta^{t}\) ```

**Algorithm 2** Algorithm for a compromised client
Additional Experimental Setup and Results

### Architecture of Global Model

Table 2 shows the global model architecture on MNIST dataset.

### Parameter Setting for Compared Baselines

Recall that we compare our defense with the following methods: FedAvg [28], Krum [6], Median [56], Norm-Clipping [39], Differential Privacy (DP) [39], DeepSight [36], and FLTrust [7]. FedAvg is non-robust while Krum and Median are two Byzantine-robust baselines. Norm-Clipping clips the \(L_{2}\)-norm of local model updates to a given threshold \(\mathcal{T}_{N}\). We set \(\mathcal{T}_{N}=0.01\) for MNIST and \(\mathcal{T}_{N}=0.1\) for CIFAR10. DP first clips the \(L_{2}\)-norm of a local model update to a threshold \(\mathcal{T}_{D}\) and then adds Gaussian noise. We set \(\mathcal{T}_{D}=0.05\) for MNIST and \(\mathcal{T}_{D}=0.5\) for CIFAR10. We set the standard deviation of noise to be \(0.01\) for both datasets. In FLTrust, the server uses its clean dataset to compute a server model update and assigns a trust score to each client by leveraging the similarity between the server model update and the local model update. We set the clean training dataset of the server to be the same as FedGame in our comparison. Note that FLTrust is not applicable when the clean training dataset of the server is from a different domain from those of clients.

### Performance of FedGame against Neurotoxin

In Table 3, we compare our FedGame with other defense baselines against Neurotoxin [59]. We can observe that our FedGame is consistently more effective than existing defenses. Our observation is consistent with the experimental results for Scaling attack and DBA attack in Table 1.

### Visualization of Genuine Score of FedGame and Trust Score of FLTrust

Our FedGame computes a genuine score for each client which quantifies the extent to which a client is benign in each communication round. Intuitively, our FedGame would be effective if the genuine score is small for a compromised client but is large for a benign one. FLTrust [7] computes a trust score for each client in each communication round. Similarly, FLTrust would be effective if the trust score is small for a compromised client but large for a benign one. Figure 2 visualizes the

\begin{table}
\begin{tabular}{c c} \hline \hline Type & Parameters \\ \hline Convolution & \(3\times 3\), stride=1, 16 kernels \\ Activation & ReLU \\ Max Pooling & \(2\times 2\) \\ \hline Convolution & \(4\times 4\), stride=2, 32 kernels \\ Activation & ReLU \\ Max Pooling & \(2\times 2\) \\ \hline Fully Connected & \(800\times 500\) \\ Activation & ReLU \\ Fully Connected & \(500\times 10\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Architecture of the convolutional neural network for MNIST.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Metrics} & \multirow{3}{*}{
\begin{tabular}{c} FedAvg \\ (No attacks) \\ \end{tabular} } & \multicolumn{6}{c}{Defenses (Under attacks)} & \multicolumn{6}{c}{FedGame} \\ \cline{3-11}  & & & & & & & & & & \\ \cline{3-11}  & FedAvg & Krum & Median & \multicolumn{2}{c}{Clipping} & DP & \multicolumn{2}{c}{Sight} & FLTrust & \multicolumn{2}{c}{In-} & \multicolumn{2}{c}{Out-of-domain domain domain} \\ \hline TA (\%) & 99.04 & 99.02 & 99.32 & 99.08 & 90.75 & 95.28 & 96.36 & 95.73 & 97.27 & 97.33 \\ ASR (\%) & 9.69 & 99.97 & 99.98 & 99.99 & 99.36 & 99.27 & 89.02 & 13.02 & **9.93** & **10.03** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of FedGame with existing defenses against Neurotoxin on MNIST under IID setting. The total number of clients is 10 with 60% compromised. The best results when respectively comparing FedGame in each setting with existing defenses are bold.

average genuine or trust scores for compromised and benign clients of FedGame and FLTrust on the MNIST dataset. We have the following observations from the figures. First, the average genuine score computed by FedGame drops to 0 quickly for compromised clients. In contrast, the average trust score computed by FLTrust drops slowly. Second, the average genuine score computed by FedGame for benign clients first increases and then becomes stable. In contrast, the average genuine score computed by FLTrust for benign clients decreases as the number of iterations increases. As a result, FedGame outperforms FLTrust.

### FedGame Performance in FL Consisting of 30 Clients

In Table 4, we report the performance of FedGame and baselines when the total number of clients is 30. The results also indicate that our FedGame outperforms all baselines in terms of ASR and achieves comparable TA with existing methods.

### Additional Ablation Studies

Impact of Trigger Reverse Engineering.Our framework is compatible with any trigger reverse engineer methods. To study the impact of different trigger reverse engineering methods, we use FeatureRE [47] to substitute Neural Cleanse in FedGame. We adopt the public implementation of FeatureRE. Under the default setting, FedGame achieves a 10.36% ASR, which indicates that our framework is consistently effective with different trigger reverse engineering methods.

Besides, FedGame would still be effective even if the reverse-engineered backdoor trigger and target label are of low quality, as long as they can distinguish benign and compromised clients. We validate this by experiment. In particular, we use 20 iterations (originally, it's 100) when reverse engineering the trigger/target label under the default setting to obtain a low-quality trigger/target label. We find that genuine scores for malicious clients are still low. The final ASR is 13.23%, which means our FedGame is still effective.

Impact of the Total Number of Clients.We study the impact of the total number of clients for our FedGame under the default setting. In particular, we consider the total number of clients to be 10, 30,

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Datasets} & \multirow{3}{*}{Metrics} & \multicolumn{6}{c}{Defenses (Under attacks)} \\ \cline{3-10}  & & \begin{tabular}{c} FedAvg \\ (No attacks) \\ \end{tabular} & FedAvg & \begin{tabular}{c} Krum \\ \end{tabular} & \begin{tabular}{c} Median \\ Clipping \\ \end{tabular} & DP & FLTrust & \begin{tabular}{c} 
\begin{tabular}{c} FedGame \\ \end{tabular} \\ \end{tabular} \\ \hline \multirow{2}{*}{MNIST} & TA (\%) & 99.02 & 99.09 & 98.16 & 99.01 & 92.77 & 89.77 & 95.27 & 97.81 & 97.64 \\ \cline{2-10}  & ASR (\%) & 9.74 & 99.98 & 99.98 & 99.98 & 98.20 & 98.83 & 11.04 & **9.95** & **9.95** \\ \hline \multirow{2}{*}{CIFAR10} & TA (\%) & 80.08 & 79.73 & 72.23 & 79.58 & 79.20 & 50.86 & 67.84 & 73.29 & 74.42 \\ \cline{2-10}  & ASR (\%) & 9.14 & 99.82 & 99.97 & 99.85 & 99.87 & 96.53 & 99.28 & **10.44** & **9.15** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of FedGame with existing defenses under Scaling attack. The total number of clients is 30 with 60% compromised. The best results when respectively comparing FedGame in each setting with existing defenses are bold.

Figure 2: (a)(b): Server-computed average trust scores for benign and compromised clients of FLTrust on MNIST under Scaling attack. (c)(d): Average genuine scores computed by the server for benign and compromised clients of FedGame on MNIST under Scaling attack. The clean sets of the server are the same for FLTrust and FedGame.

50, 70, and 100, where the fraction of malicious clients is 60%. We show the experimental results in Table 5. Our experimental results show that our FedGame is effective for different number of clients on different datasets.

Impact of the Size of Server Clean Data.By default, we set the ratio between the number of clean examples of the server and the total number of examples of clients to be 0.1. We conduct experiments with different ratios: 0.01, 0.02, and 0.05 under the default setting. The corresponding ASRs are 9.71%, 12.38%, and 9.75%, indicating that FedGame is effective even when the server only has 1% clean data.

Trigger Optimization.We consider an attacker optimizes trigger pattern such that a backdoored input is more likely to be predicted as the target class. We perform experiments under the default setting. The ASR is 12.43%, which indicates that our FedGame is consistently effective for trigger optimization.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{5}{c}{Total Number of Clients} \\ \cline{2-6}  & 10 & 30 & 50 & 70 & 100 \\ \hline MNIST & 9.72 & 9.95 & 10.03 & 10.01 & 9.89 \\ CIFAR10 & 8.92 & 10.44 & 10.62 & 9.79 & 10.82 \\ \hline \hline \end{tabular}
\end{table}
Table 5: ASRs of FedGame under different total number of clients on MNIST and CIFAR10. The fraction of compromised clients is 60%.