# DP-SGD Without Clipping:

The Lipschitz Neural Network Way

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) faces difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, our theoretical analysis of Lipschitz constrained networks reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters we guarantee DP training of these networks. This analysis not only allows the computation of the aforementioned sensitivities at scale but also provides leads on to how maximize the gradient-to-noise ratio for fixed privacy guarantees. To facilitate the application of Lipschitz networks and foster robust and certifiable learning under privacy guarantees, we provide a Python package that implements building blocks allowing the construction and private training of such networks.

## 1 Introduction

Machine learning relies more than ever on foundational models, and such practices raise questions about privacy. Differential privacy allows to develop methods for training models that preserve the privacy of individual data points in the training set. The field seeks to enable deep learning on sensitive data, while ensuring that models do not inadvertently memorize or reveal specific details about individual samples in their weights. This involves incorporating privacy-preserving mechanisms into the design of deep learning architectures and training algorithms, whose most popular example is Differentially Private Stochastic Gradient Descent (DP-SGD) [1]. One main drawback of classical DP-SGD methods is that they require costly per-sample backward processing and gradient clipping. In this paper, we offer a new method that unlocks fast differentially private training through the use of Lipschitz constrained neural networks. Additionally, this method offers new opportunities for practitioners that wish to easily "DP-fy" [2] the training procedure of a deep neural network.

**Differential privacy fundamentals.** Informally, differential privacy is a _definition_ that quantifies how much the change of a single sample in a dataset affects the range of a stochastic function (here the DP training), called _mechanism_ in this context. This quantity can be bounded in an inequality involving two parameters \(\epsilon\) and \(\delta\). A mechanism fulfilling such inequality is said \((\epsilon,\delta)\)-DP (see Definition 1). This definition is universally accepted as a strong guarantee against privacy leakages under various scenarii, including data aggregation or post-processing [3]. A popular rule of thumb suggests using \(\epsilon\leq 10\) and \(\delta<\frac{\Gamma}{N}\) with \(N\) the number of records [2] for mild guarantees. In practice, most classic algorithmic procedures (called _queries_ in this context) do not readily fulfill the definition for useful values of \((\epsilon,\delta)\), in particular the deterministic ones: randomization is mandatory. This randomizationcomes at the expense of "utility", i.e the usefulness of the output for downstream tasks [4]. The goal is then to strike a balance between privacy and utility, ensuring that the released information remains useful and informative for the intended purpose while minimizing the risk of privacy breaches. The privacy/utility trade-off yields a Pareto front, materialized by plotting \(\epsilon\) against a measurement of utility, such as validation accuracy for a classification task.

**Private gradient descent.** The SGD algorithm consists of a sequence of queries that (i) take the dataset in input, sample a minibatch from it, and return the gradient of the loss evaluated on the minibatch, before (ii) performing a descent step following the gradient direction. The sensitivity (see Definition 2) of SGD queries is proportional to the norm of the per-sample gradients. DP-SGD turns each query into a Gaussian mechanism by perturbing the gradients with a noise \(\zeta\). The upper bound on gradient norms is generally unknown in advance, which leads practitioners to clip it to \(C>0\), in order to bound the sensitivity manually. This is problematic for several reasons: **1.** Hyper-parameter search on the broad-range clipping value \(C\) is required to train models with good privacy/utility tradeoffs [5], **2.** The computation of per-sample gradients is expensive: DP-SGD is usually slower and consumes more memory than vanilla SGD, in particular for the large batch sizes often used in private training [6], **3.** Clipping the per-sample gradients biases their average [7]. This is problematic as the average direction is mainly driven by misclassified examples, that carry the most useful information for future progress.

**An unexplored approach: Lipschitz constrained networks.** We propose to train neural networks for which the parameter-wise gradients are provably and analytically bounded during the whole training procedure, in order to get rid of the clipping process. This allows for rapid training of models without a need for tedious hyper-parameter optimization.

The main reason why this approach has not been experimented much in the past is that upper bounding the gradient of neural networks is often intractable. However, by leveraging the literature of Lipschitz constrained networks [8], we show that these networks allows to estimate their gradient bound. This yields tight bounds on the sensitivity of SGD steps, making their transformation into Gaussian mechanisms inexpensive - hence the name **Clipless DP-SGD**.

Informally, the Lipschitz constant quantifies the rate at which the function's output varies with respect to changes in its input. A Lipschitz constrained network is one in which its weights and activations are constrained such that it can only represent \(l\)-Lipschitz functions. In this work, we will focus our attention on feed-forward networks (refer to Definition 3). Note that the most common architectures, such as Convolutional Neural Networks (CNNs), Fully Connected Networks (FCNs), Residual Networks (ResNets), or patch-based classifiers (like MLP-Mixers), all fall under the category of feed-forward networks. We will also tackle the particular case of Gradient Norm Preserving (GNP) networks, a subset of Lipschitz networks that enjoy tighter bounds (see appendix).

Figure 1: **An example of usage of our framework, illustrating how to create a small Lipschitz VGG and how to train it under \((\epsilon,\delta)\)-DP guarantees while reporting \((\epsilon,\delta)\) values.**

Contributions

While the properties of Lipschitz constrained networks regarding their inputs are well explored, the properties with respect to its parameters remain non-trivial. This work provides a first step to fill this gap: our analysis shows that under appropriate architectural constraints, a \(l\)-Lipschitz network has a tractable, finite Lipschitz constant with respect to its parameters. We prove that this Lipschitz constant allows for easy estimation of the sensitivity of the gradient computation queries. The prerequisite and details of the method to compute the sensitivities are explained in Section 2.

Our contributions are the following:

1. We extend the field of applications of Lipschitz constrained neural networks. So far the literature focused on Lipschitzness with respect to the _inputs_: we extend the framework to **compute the Lipschitzness with respect to the parameters**. This is exposed in Section 2.
2. We propose a **general framework to handle layer gradient steps as Gaussian mechanisms** that depends on the loss and the model structure. Our framework covers widely used architectures, including VGG and ResNets.
3. We show that SGD training of deep neural networks can be achieved **without gradient clipping** using Lipschitz layers. This allows the use of larger networks and larger batch sizes, as illustrated by our experiments in Section 4.
4. We establish connections between **Gradient Norm Preserving** (GNP) networks and **improved privacy/utility trade-offs** (Section 3.1).
5. Finally, a **Python package1** companions the project, with pre-computed Lipschitz constant and noise for each layer type, ready to be forked on any problem of interest (Section 3.2).

Footnote 1: Code and documentation are given as supplementary material during review process.

### Differential Privacy and Lipschitz Networks

The definition of DP relies on the notion of neighboring datasets, i.e datasets that vary by at most one example. We highlight below the central tools related to the field, inspired from [9].

**Definition 1** (\((\epsilon,\delta)\)-Differential Privacy).: _A labeled dataset \(\mathcal{D}\) is a finite collection of input/label pairs \(\mathcal{D}=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots....(x_{N},y_{N})\}\). Two datasets \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\) are said to be neighboring for the "replace-one" relation if they differ by at most one sample: \(\mathcal{D}^{\prime}=\mathcal{D}\cup\{(x_{i}^{\prime},y_{i}^{\prime})\} \setminus\{(x_{i},y_{i})\}\). Let \(\epsilon\) and \(\delta\) be two non-negative scalars. A mechanism \(\mathcal{A}\) is \((\epsilon,\delta)\)-DP if for any two neighboring datasets \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\), and for any \(S\subseteq\text{range}(\mathcal{A})\):_

\[\mathbb{P}[\mathcal{A}(\mathcal{D})\in S]\leq e^{\epsilon}\times\mathbb{P}[ \mathcal{A}(\mathcal{D}^{\prime})\in S]+\delta.\] (1)

A cookbook to create a \((\epsilon,\delta)\)-DP mechanism from a query is to compute its _sensitivity_\(\Delta\) (see Definition 2), and to perturb its output by adding a Gaussian noise of predefined variance \(\zeta^{2}=\Delta^{2}\sigma^{2}\), where the \((\epsilon,\delta)\)-DP guarantees depends on \(\sigma\). This yields what is called a _Gaussian mechanism_[3].

**Definition 2** (\(l_{2}\)-sensitivity).: _Let \(\mathcal{M}\) be a query mapping from the space of the datasets to \(\mathbb{R}^{p}\). Let \(\mathcal{N}\) be the set of all possible pairs of neighboring datasets \(\mathcal{D},\mathcal{D}^{\prime}\). The \(l_{2}\) sensitivity of \(\mathcal{M}\) is defined by:_

\[\Delta(\mathcal{M})=\max_{\mathcal{D},\mathcal{D}^{\prime}\in\mathcal{N}}\lVert \mathcal{M}(D)-\mathcal{M}(D^{\prime})\rVert_{2}.\] (2)

**Differentially Private SGD.** The classical algorithm keeps track of \((\epsilon,\delta)\)-DP values with a _moments accountant_[1] which allows to keep track of privacy guarantees at each epoch, by composing different sub-mechanisms. For a dataset with \(N\) records and a batch size \(b\), it relies on two parameters: the sampling ratio \(p=\frac{b}{N}\) and the "noise multiplier" \(\sigma\) defined as the ratio between effective noise strength \(\zeta\) and sensitivity \(\Delta\). Bounds on gradient norm can be turned into bounds on sensitivity of SGD queries. In "replace-one" policy for \((\epsilon,\delta)\)-DP accounting, if the gradients are bounded by \(K>0\), the sensitivity of the gradients averaged on a minibatch of size \(b\) is \(\Delta=2K/b\)..

Crucially, the algorithm requires a bound on \(\lVert\nabla_{\theta}\mathcal{L}(\hat{y},y)\rVert_{2}\leq K\). The whole difficulty lies in bounding tightly this value in advance for neural networks. Currently, gradient clipping serves as a patch to circumvent the issue [1]. Unfortunately, clipping individual gradients in the batch is costly and will bias the direction of their average, which may induce underfitting [7].

Lipschitz constrained networks.Our proposed solution comes from the observation that the norm of the gradient and the Lipschitz constant are two sides of the same coin. The function \(f:\mathbb{R}^{m}\to\mathbb{R}^{n}\) is said \(l\)-Lipschitz for \(l_{2}\) norm if for every \(x,y\in\mathbb{R}^{m}\) we have \(\|f(x)-f(y)\|_{2}\leq l\|x-y\|_{2}\). Per Rademacher's theorem [10], its gradient is bounded: \(\|\nabla_{x}f\|\leq l\). Reciprocally, continuous functions gradient bounded by \(l\) are \(l\)-Lipschitz.

In Lipschitz networks, the literature has predominantly concentrated on investigating the control of Lipschitzness with respect to the inputs (i.e bounding \(\nabla_{x}f\)), primarily motivated by concerns of robustness [11]. However, in this work, we will demonstrate that it is also possible to control Lipschitzness with respect to parameters (i.e bounding \(\nabla_{\theta}f\)), which is essential for ensuring privacy. Our first contribution will point out the tight link that exists between those two quantities.

**Definition 3** (Lipschitz feed-forward neural network).: _A feedforward neural network of depth \(D\), with input space \(\mathcal{X}\subset\mathbb{R}^{n}\), output space \(\mathcal{Y}\subset\mathbb{R}^{K}\) (e.g logits), and parameter space \(\Theta\subset\mathbb{R}^{p}\), is a parameterized function \(f:\Theta\times\mathcal{X}\to\mathcal{Y}\) defined by the sequential composition of layers \(f_{d}\):_

\[f(\theta,x):=\left(f_{D}(\theta_{d})\circ\ldots\circ f_{2}(\theta_{2})\circ f _{1}(\theta_{1})\right)(x).\] (3)

_The parameters of the layers are denoted by \(\theta=(\theta_{d})_{1\leq d\leq D}\in\Theta\). For affine layers, it corresponds to bias and weight matrix \(\theta_{d}=(W_{d},b_{d})\). For activation functions, there is no parameters: \(\theta_{d}=\varnothing\). Lipschitz networks are feed-forward networks, with the additionnal constraint that each layer \(x_{d}\mapsto f_{d}(\theta_{d},x_{d}):=y_{d}\) is \(l_{d}\)-Lipschitz for all \(\theta_{d}\). Consequently, the function \(x\mapsto f(\theta,x)\) is \(l\)-Lipschitz with \(l=l_{1}\times\ldots\times l_{d}\) for all \(\theta\in\Theta\)._

In practice, this is enforced by using activations with Lipschitz constant \(l_{d}\), and by applying a constraint \(\Pi:\mathbb{R}^{p}\to\Theta\) on the weights of affine layers. This corresponds to spectrally normalized matrices [12; 13], since for affine layers we have \(l_{d}=\|W_{d}\|_{2}:=\max\limits_{\|x\|\leq 1}\|W_{d}x\|_{2}\) hence \(\Theta=\{\|W_{d}\|\leq l_{q}\}\).

The seminal work of [8] proved that universal approximation in the set of \(l\)-Lipschitz functions was achievable by this family of architectures. Concurrent approaches are based on regularization (like in [14; 15; 16]) but they fail to produce formal guarantees. While they have primarily been studied in the context of adversarial robustness [11; 17], recent works have revealed additional properties of these networks, such as improved generalization [13; 18]. However, the properties of their parameter gradient \(\nabla_{\theta}f(\theta,x)\) remain largely unexplored.

## 2 Clipless DP-SGD with \(l\)-Lipschitz networks

Our framework consists of **1.** a method that computes the maximum gradient norm of a network with respect to its parameters to obtain a _per-layer_ sensitivity \(\Delta_{d}\), **2.** a moments accountant that relies on the per-layer sensitivities to compute \((\epsilon,\delta)\)-DP guarantees. The method 1. is based on the recursive formulation of the chain rule involved in backpropagation, while 2. keeps track of \((\epsilon,\delta)\)-DP values with RDP accounting. It requires some natural assumptions that we highlight below.

**Requirement 1** (Lipschitz loss.).: _The loss function \(\hat{y}\mapsto\mathcal{L}(\hat{y},y)\) must be \(L\)-Lipschitz with respect to the logits \(\hat{y}\) for all ground truths \(y\in\mathcal{Y}\). This is notably the case of Categorical Softmax-Crossentropy._

The Lipschitz constants of common classification losses can be found in the appendix.

**Requirement 2** (Bounded input).: _There exists \(X_{0}>0\) such that for all \(x\in\mathcal{X}\) we have \(\|x\|\leq X_{0}\)._

While there exist numerous approaches for the parametrization of Lipschitz networks (e.g differentiable re-parametrization [19; 8], optimization over matrix manifolds [20] or projections [21]), our framework only provides sensitivity bounds for projection-based algorithms (see appendix).

**Requirement 3** (Lipschitz projection).: _The Lipschitz constraints must be enforced with a projection operator \(\Pi:\mathbb{R}^{p}\to\Theta\). This corresponds to Tensorflow [22] constraints and Pytorch [23] hooks. Projection is a post-processing of private gradients: it induces no privacy leakage [3]._

To compute the per-layer sensitivities, our framework mimics the backpropagation algorithm, where _Vector-Jacobian_ products (VJP) are replaced by _Scalar-Scalar_ products of element-wise bounds. For an arbitrary layer \(x_{d}\mapsto f_{d}(\theta_{d},x_{d}):=y_{d}\) the operation is sketched below:

\[\nabla_{x_{d}}\mathcal{L}:=(\nabla_{y_{d}}\mathcal{L})\frac{\partial f_{d}}{ \partial x_{d}}\qquad\implies\underbrace{\|\nabla_{x_{d}}\mathcal{L}\|_{2} \leq\|\nabla_{y_{d}}\mathcal{L}\|_{2}\times\bigg{\|}\frac{\partial f_{d}}{ \partial x_{d}}\bigg{\|}}_{\text{Scalar-Scalar product: backpropagate bounds}}.\] (4)The notation \(\|\cdot\|_{2}\) must be understood as the spectral norm for Jacobian matrices, and the Euclidean norm for gradient vectors. The scalar-scalar product is inexpensive. For Lipschitz layers the spectral norm of the Jacobian \(\|\frac{\partial f}{\partial x}\|\) is kept constant during training with projection operator \(\Pi\). The bound of the gradient with respect to the parameters then takes a simple form:

\[\|\nabla_{\theta_{d}}\mathcal{L}\|_{2}=\|\nabla_{y_{d}}\mathcal{L}\|_{2}\times \left\|\frac{\partial f_{d}}{\partial\theta_{d}}\right\|_{2}.\] (5)

Once again the operation is inexpensive. The upper bound \(\left\|\frac{\partial f}{\partial\theta}\right\|_{2}\) typically depends on the supremum of \(\|x_{d}\|_{2}\), that can also be analytically bounded, as exposed in the following section.

### Backpropagation for bounds

The pseudo-code of **Clipless DP-SGD** is sketched in Algorithm 2. The algorithm avoids clipping by computing a _per-layer_ bound on the element-wise gradient norm. The computation of this _per-layer_ bound is described by Algorithm 1 (graphically explained in Figure 2). Crucially, it requires to compute the spectral norm of the Jacobian of each layer with respect to input and parameters.

**Input bound propagation (line 2).** We compute \(X_{d}=\max_{\|x\|\leq X_{d-1}}\|f_{d}(x)\|_{2}\). For activation functions it depends on their range. For linear layers, it depends on the spectral norm of the operator itself. This quantity can be computed with SVD or Power Iteration [24, 19], and constrained during training using projection operator \(\Pi\). In particular, it covers the case of convolutions, for which tight bounds are known [25]. For affine layers, it additionally depends on the amplitude of the bias \(\|b_{d}\|\).

**Remark 1** (Tighter bounds in literature.).: _Although libraries such as Decomon [26] or auto-LiRPA [27] provide tighter bounds \(X_{d}\) via linear relaxations [28, 29], our approach is capable of delivering practically tighter bounds than worst-case scenarios thanks to the projection operator \(\Pi\), while also being significantly less computationally expensive. Moreover, hybridizing our method with scalable certification methods can be a path for future extensions._

**Computing maximum gradient norm (line 6).** We bound the Jacobian \(\frac{\partial f_{d}(\theta_{d},x)}{\partial\theta_{d}}\). In neural networks, the parameterized layers \(f(\theta,x)\) (fully connected, convolutions) are bilinear operators. Hence we typically obtain bounds of the form:

\[\left\|\frac{\partial f_{d}(\theta_{d},x)}{\partial\theta_{d}}\right\|_{2} \leq K(f_{d},\theta_{d})\|x\|_{2}\leq K(f_{d},\theta_{d})X_{d-1},\] (6)

where \(K(f_{d},\Theta_{d})\) is a constant that depends on the nature of the operator. \(X_{d-1}\) is obtained in line 2 with input bound propagation. Values of \(K(f_{d},\theta_{d})\) for popular layers are pre-computed in the library.

Backpropagate octangeant vector bounds (line 7).We bound the Jacobian \(\frac{\partial f_{d}(\theta_{d},x)}{\partial x}\). For activation functions this value can be hard-coded, while for affine layers it is the spectral norm of the linear operator. Like before, this value is constrained with projection operator \(\Pi\).

### Privacy accounting for Clipless DP-SGD

Two strategies are available to keep track of \((\epsilon,\delta)\) values as the training progresses, based on accounting either a per-layer "local" sensitivity, either by aggregating them into a "global" sensitivity.

Figure 2: **Backpropagation for bounds, Algorithm 1. Compute the per-layer sensitivity \(\Delta_{d}\).**

**The "global" strategy.** Illustrated in the appendix,this strategy simply aggregates the individual sensitivities \(\Delta_{d}\) of each layer to obtain the global sensitivity of the whole gradient vector \(\Delta=\sqrt{\sum_{d}\Delta_{d}^{2}}\). The origin of the clipping-based version of this strategy can be traced back to [30]. With noise variance \(\sigma^{2}\Delta^{2}\) we recover the accountant that comes with DP-SGD. It tends to overestimate the true sensitivity (in particular for deep networks), but its implementation is straightforward with existing tools.

**The "local" strategy.** Recall that we are able to characterize the sensitivity \(\Delta_{d}\) of every layer of the network. Hence, we can apply a different noise to each of the gradients. We dissect the whole training procedure in Figure 3. At same noise multiplier \(\sigma\), it tends to produce a higher value of \(\epsilon\) per epoch than "global" strategy, but has the advantage over the latter to add smaller effective noise \(\zeta\) to each weight.

We rely on the autodp2 library [32, 33, 34] as it uses the Renyi Differential Privacy (RDP) adaptive composition theorem [35, 36], that ensures tighter bounds than naive DP composition.

Footnote 2: https://github.com/yuxiangw/autodp distributed under Apache License 2.0 licence.

## 3 From theory to practice

Beyond the application of Algorithms 1 and 2, our framework provides numerous opportunities to enhance our understanding of prevalent techniques identified in the literature. An in-depth exploration of these is beyond the scope of this work, so we focus on giving insights on promising tracks based on our theoretical analysis. In particular, we discuss how the tightness of the bound provided by Algorithm 1 can be influenced by working on the architecture, the input pre-processing and the loss post-processing.

### Gradient Norm Preserving networks

We can manually derive the bounds obtained from Algorithm 2 across diverse configurations. Below, we conduct a sensitivity analysis on \(l\)-Lipschitz networks.

**Theorem (informal) 1. Gradient Norm of Lipschitz Networks.**_Assume that every layer \(f_{d}\) is \(K\)-Lipschitz, i.e \(l_{1}=\cdots=l_{D}=K\). Assume that every bias is bounded by \(B\). We further assume that each activation is centered in zero (e.g ReLU, tanh, GroupSort). We recall that \(\theta=[\theta_{1},\theta_{2},\dots\theta_{D}]\). Then the global upper bound of Algorithm 2 can be expanded analytically._

**1. If \(K<1\) we have:**\(\|\nabla_{\theta}\mathcal{L}(f(\theta,x),y)\|_{2}=\mathcal{O}\left(L\left(K^{D}(X_ {0}+B)+1\right)\right).\)

Due to the \(K^{D}\ll 1\) term this corresponds to a vanishing gradient phenomenon [37]. The output of the network is essentially independent of its input, and the training is nearly impossible.

**2. If \(K>1\) we have:**\(\|\nabla_{\theta}\mathcal{L}(f(\theta,x),y)\|_{2}=\mathcal{O}\left(LK^{D} \left(X_{0}+B\right)\right).\)

Due to the \(K^{D}\gg 1\) term this corresponds to an exploding gradient phenomenon [38]. The upper bound becomes vacuous for deep networks: the added noise \(\zeta\) is at risk of being too high.

**3. If \(K=1\) we have:**\(\|\nabla_{\theta}\mathcal{L}(f(\theta,x),y)\|_{2}=\mathcal{O}\left(L\left(X_ {0}+\sqrt{D}+\sqrt{BX_{0}}D+BD^{3/2}\right)\right),\)

which for linear layers without biases further simplify to \(\mathcal{O}(L(X_{0}+\sqrt{D}))\).

The formal statement can be found in appendix. From Theorem 1 we see that most favorable bounds are achieved by 1-Lipschitz neural networks with 1-Lipschitz layers. In classification tasks, they are not less expressive than conventional networks [18]. Hence, this choice of architecture is not at the expense of utility. Moreover an accuracy/robustness trade-off exists, determined by the choice of loss function [18]. However, setting \(K=1\) merely ensures that \(\|\nabla_{x}f\|\leq 1\), and in the worst-case scenario we have \(\|\nabla_{x}f\|<1\) almost everywhere. This could result in a situation where the bound of case 3 in Theorem 1 is not tight, leading to an underfitting regime as in case \(K<1\). With Gradient Norm Preserving (GNP) networks [17], we expect to mitigate this issue.

Controlling \(K\) with Gradient Norm Preserving (GNP) networks.GNP networks are 1-Lipschitz neural networks with the additional constraint that the Jacobian of layers consists of orthogonal matrices. They fulfill the Eikonal equation \(\left\|\frac{\partial f_{d}(\theta_{d},x_{d})}{\partial x_{d}}\right\|_{2}=1\) for any intermediate activation \(f_{d}(\theta_{d},x_{d})\). Without biases these networks are also norm preserving: \(\|f(\theta,x)\|=\|x\|\).

As a consequence, the gradient of the loss with respect to the parameters is easily bounded by

\[\|\nabla_{\theta_{d}}\mathcal{L}\|=\|\nabla_{y_{D}}\mathcal{L}\|\times\left\| \frac{\partial f_{d}(\theta_{d},x_{d})}{\partial\theta_{d}}\right\|,\] (7)

which for weight matrices \(W_{d}\) further simplifies to \(\|\nabla_{W_{d}}\mathcal{L}\|\leq\|\nabla_{y_{D}}\mathcal{L}\|\times\|f_{d-1}( \theta_{d-1},x_{d-1})\|\). We see that this upper bound crucially depends on two terms than can be analyzed separately. On one hand, \(\|f_{d-1}(\theta_{d-1},x_{d-1})\|\) depends on the scale of the input. On the other, \(\|\nabla_{y_{D}}\mathcal{L}\|\) depends on the loss, the predictions and the training stage. We show below how to intervene on these two quantities.

Figure 3: **Account for locally enforced differential privacy.****(i)** The gradient query for each layer is turned into a Gaussian mechanism [9], **(ii)** their composition at the scale of the whole network is a non isotropic Gaussian mechanism, **(iii)** that benefits from amplification via sub-sampling [31], **(iv)** the train steps are composed over the course of training.

**Remark 2** (Implementation of GNP Networks).: _In practice, GNP are parametrized with GroupSort activation [8; 39], Householder activation [40], and orthogonal weight matrices [17; 41]. Strict orthogonality is challenging to enforce, especially for convolutions for which it is still an active research area (see [42; 43; 44; 45; 46] and references therein). Our line of work traces an additional motivation for the development of GNP and the bounds will strengthen as the field progresses._

**Controlling \(X_{0}\) with input pre-processing.** The weight gradient norm \(\|\nabla_{\theta_{\mathcal{E}}}\mathcal{L}\|\) indirectly depends on the norm of the inputs. This observation implies that the pre-processing of input data significantly influences the bounding of sensitivity. Multiple strategies are available to keep the input's norm under control: projection onto the ball ("norm clipping"), or projection onto the sphere ("normalization"). In the domain of natural images for instance, this result sheds light on the importance of color space such as RGB, HSV, YIQ, YUV or Grayscale. These strategies are natively handled by our library.

**Controlling \(L\) with the hybrid approach, loss gradient clipping.** As training progresses, the magnitude of \(\|\nabla_{f}\mathcal{L}\|\) tends to diminish when approaching a local minima, quickly falling below the upper bound and diminishing the gradient norm to noise ratio. To circumvent the issue, the gradient clipping strategy is still available in our framework. Crucially, instead of clipping the parameter gradient \(\nabla_{\theta}\mathcal{L}\), any intermediate gradient \(\nabla_{f_{d}}\mathcal{L}\) can be clipped during backpropagation. This can be achieved with a special "_clipping layer_" that behaves like the identity function at the forward pass, and clips the gradient during the backward pass. The resulting cotangent vector is not a true gradient anymore, but rather a descent direction [47]. In vanilla DP-SGD the clipping is applied on the batched gradient \(\nabla_{W_{d}}\mathcal{L}\) of size \(b\times h^{2}\) for matrix weight \(W_{d}\in\mathbb{R}^{h\times h}\) and clipping this vector can cause memory issues or slowdowns [6]. In our case, \(\nabla_{y_{D}}\mathcal{L}\) is of size \(b\times h\) which reduces overhead.

### Lip-dp library

To foster and spread accessibility, we provide an opensource tensorflow library for Clipless DP-SGD training, named lip-dp. It provides an exposed Keras API for seamless usability. It is implemented as a wrapper over the Lipschitz layers of deel-lip3 library [48]. Its usage is illustrated in Figure 1.

Footnote 3: https://github.com/deel-ai/deel-lip distributed under MIT License (MIT).

## 4 Experimental results

We validate our implementation with a speed benchmark against competing approaches, and we present the privacy/utility Pareto front that can be obtained with GNP networks.

Speed and memory consumption.We benchmarked the median runtime per epoch of vanilla DP-SGD against the one of Clipless DP-SGD, on a CNN architecture and its Lipschitz equivalent respectively. The experiment was run on a GPU with 48GB video memory. We compare against the implementation of tf_privacy, opacus and optax. In order to allow a fair comparison, when evaluating Opacus, we reported the runtime with respect to the logical batch size, while capping the physical batch size to avoid Out Of Memory error (OOM). Although our library does not implement logical batching yet, it is fully compatible with this feature.

An advantage of projection \(\Pi\) over per-sample gradient clipping is that the projection cost is independent of the batch size. Fig 4 validates that our method scales much better than vanilla DP-SGD, and is compatible with large batch sizes. It offers several advantages: firstly, a larger batch size contributes to a decrease of the sensitivity \(\Delta\propto 1/b\), which diminishes the ratio between noise and gradient norm. Secondly, as the batch size \(b\) increases, the variance decreases at the parametric rate \(\mathcal{O}(\sqrt{b})\) (as demonstrated in appendix), aligning with expectations. This observation does not apply to DP-SGD: gradient clipping biases the direction of the average gradient, as noticed by [7].

Figure 4: **Our approach outperforms concurrent frameworks in terms of runtime and memory:** we trained CNNs (ranging from 130K to 2M parameters) on CIFAR-10, and report the median batch processing time (including noise, and constraints application \(\Pi\) or gradient clipping).

Pareto front of privacy/utility trade-off.We performed a search over a broad range of hyperparameters values to cover the Pareto front between utility and privacy. Results are reported in Figure 5. We emphasize that our experiments did not use the elements behind the success of most recent papers (pre-training, data preparation, or handcrafted feature are examples). Hence our results are more representative of the typical performance that can be obtained in an "out of the box" setting. Future endeavors or domain-specific engineering can enhance the performance even further, but such improvements currently lie beyond the scope of our work. We also benchmarked architectures inspired from VGG [52], Resnet [53] and MLP_Mixers [54] see appendix for more details. Following standard practices of the community [2], we used _sampling without replacement_ at each epoch (by shuffling examples), but we reported \(\epsilon\) assuming _Poisson sampling_ to benefit from privacy amplification [31]. We also ignore the privacy loss that may be induced by hyper-parameter search, which is a limitation per recent studies [5], but is common practice.

## 5 Limitations and future work

Although this framework offers a novel approach to address differentially private training, it introduces new challenges. We primary rely on GNP networks, where high performing architectures are quite different from the usual CNN architectures. As emphasized in Remark 2, we anticipate that progress in these areas would greatly enhance the effectiveness of our approach. Additionally, to meet requirement 3, we rely on projections, necessitating additional efforts to incorporate recent advancements associated with differentiable reparametrizations [42, 43]. It is worth noting that our methodology is applicable to most layers. Another limitation of our approach is the accurate computation of sensitivity \(\Delta\), which is challenging due to the non-associativity of floating-point arithmetic and its impact on numerical stability [55]. This challenge is exacerbated on GPUs, where operations are inherently non-deterministic [56]. Finally, as mentioned in Remark 1, our propagation bound method can be refined.

## 6 Concluding remarks and broader impact

Besides its main focus on differential privacy, our work provides **(1) a motivation to further develop Gradient Norm Preserving architectures**. Furthermore, the development of networks with known Lipschitz constant with respect to parameters is a question of independent interest, **(2) a useful tool for the study of the optimization dynamics** in neural networks. Finally, Lipschitz networks are known to enjoy certificates against adversarial attacks [17, 57], and from generalization guarantees [13], without cost in accuracy [18]. We advocate for the spreading of their use in the context of robust and certifiable learning.

Figure 5: **Our framework paints a clearer picture of the privacy/utility trade-off. We trained models in an ”out of the box setting” (no pre-training, no data augmentation and no handcrafted features) on multiple tasks. While our results align with the baselines presented in other frameworks, we recognize the importance of domain-specific engineering. In this regard, we find the innovations introduced in [49, 50, 51] and references therein highly relevant. These advancements demonstrate compatibility with our framework and hold potential for future integration.**

## References

* Abadi et al. [2016] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* Ponomareva et al. [2023] Natalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Thakurta. How to dp-f ml: A practical guide to machine learning with differential privacy. _arXiv preprint arXiv:2303.00654_, 2023.
* Dwork et al. [2006] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3_, pages 265-284. Springer, 2006.
* Alvim et al. [2012] Mario S Alvim, Miguel E Andres, Konstantinos Chatzikokolakis, Pierpaolo Degano, and Catuscia Palamidessi. Differential privacy: on the trade-off between utility and information leakage. In _Formal Aspects of Security and Trust: 8th International Workshop, FAST 2011, Leuven, Belgium, September 12-14, 2011. Revised Selected Papers 8_, pages 39-54. Springer, 2012.
* Papernot and Steinke [2022] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In _International Conference on Learning Representations_, 2022.
* Lee and Kifer [2021] Jaewoo Lee and Daniel Kifer. Scaling up differentially private deep learning with fast per-example gradient clipping. _Proceedings on Privacy Enhancing Technologies_, 2021(1), 2021.
* Chen et al. [2020] Xiangyi Chen, Steven Z Wu, and Mingyi Hong. Understanding gradient clipping in private sgd: A geometric perspective. _Advances in Neural Information Processing Systems_, 33:13773-13782, 2020.
* Anil et al. [2019] Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In _International Conference on Machine Learning_, pages 291-301. PMLR, 2019.
* Dwork et al. [2014] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Simon et al. [1983] Leon Simon et al. _Lectures on geometric measure theory_. The Australian National University, Mathematical Sciences Institute, Centre..., 1983.
* Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations_, 2014.
* Yoshida and Miyato [2017] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. _arXiv preprint arXiv:1705.10941_, 2017.
* Bartlett et al. [2017] Peter L Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, pages 6241-6250, 2017.
* Gulrajani et al. [2017] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In _Advances in Neural Information Processing Systems_, volume 30, pages 5767-5777. Curran Associates, Inc., 2017.
* Cisse et al. [2017] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In _International Conference on Machine Learning_, pages 854-863. PMLR, 2017.
* Gouk et al. [2021] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. _Machine Learning_, 110:393-416, 2021.

* Li et al. [2019] Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Jorn-Henrik Jacobsen. Preventing gradient attenuation in lipschitz constrained convolutional networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, Cambridge, MA, 2019. MIT Press.
* Bethune et al. [2022] Louis Bethune, Thibaut Boissin, Mathieu Serrurier, Franck Mamalet, Corentin Friedrich, and Alberto Gonzalez Sanz. Pay attention to your loss : understanding misconceptions about lipschitz neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Miyato et al. [2018] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. _arXiv preprint arXiv:1802.05957_, 2018.
* Absil et al. [2008] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. _Optimization algorithms on matrix manifolds_. Princeton University Press, 2008.
* Arjovsky et al. [2017] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pages 214-223. PMLR, 2017.
* Abadi et al. [2015] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems, 2015.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Trefethen and Bau [2022] Lloyd N Trefethen and David Bau. _Numerical linear algebra_, volume 181. Siam, 2022.
* Singla and Feizi [2021] S Singla and S Feizi. Fantastic four: Differentiable bounds on singular values of convolution layers. In _International Conference on Learning Representations (ICLR)_, 2021.
* [26] Airbus. Decomon. https://github.com/airbus/decomon, 2023.
* Xu et al. [2020] Kaidi Xu, Zhoxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. _Advances in Neural Information Processing Systems_, 33:1129-1141, 2020.
* Singh et al. [2019] Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for certifying neural networks. _Proceedings of the ACM on Programming Languages_, 3(POPL):1-30, 2019.
* Zhang et al. [2018] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. _Advances in neural information processing systems_, 31, 2018.
* McMahan et al. [2018] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In _International Conference on Learning Representations_, 2018.
* Balle et al. [2018] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. _Advances in Neural Information Processing Systems_, 31, 2018.
* Wang et al. [2019] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled renyi differential privacy and analytical moments accountant. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1226-1235. PMLR, 2019.

* Zhu and Wang [2019] Yuqing Zhu and Yu-Xiang Wang. Possion subsampled renyi differential privacy. In _International Conference on Machine Learning_, pages 7634-7642. PMLR, 2019.
* Zhu and Wang [2020] Yuqing Zhu and Yu-Xiang Wang. Improving sparse vector technique with renyi differential privacy. _Advances in Neural Information Processing Systems_, 33:20249-20258, 2020.
* Mironov [2017] Ilya Mironov. Renyi differential privacy. In _2017 IEEE 30th computer security foundations symposium (CSF)_, pages 263-275. IEEE, 2017.
* Mironov et al. [2019] Ilya Mironov, Kunal Talwar, and Li Zhang. R\'enyi differential privacy of the sampled gaussian mechanism. _arXiv preprint arXiv:1908.10530_, 2019.
* Pascanu et al. [2013] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In _International conference on machine learning_, pages 1310-1318. PMLR, 2013.
* Bengio et al. [1994] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. _IEEE transactions on neural networks_, 5(2):157-166, 1994.
* Tanielian and Biau [2021] Ugo Tanielian and Gerard Biau. Approximating lipschitz continuous functions with groupsort neural networks. In _International Conference on Artificial Intelligence and Statistics_, pages 442-450. PMLR, 2021.
* Mhammedi et al. [2017] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. In _International Conference on Machine Learning_, pages 2401-2409. PMLR, 2017.
* Li et al. [2019] Shuai Li, Kui Jia, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks. _IEEE transactions on pattern analysis and machine intelligence_, 43(4):1352-1368, 2019.
* Trockman and Kolter [2021] Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform. In _International Conference on Learning Representations_, 2021.
* Singla and Feizi [2021] Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. In _International Conference on Machine Learning_, pages 9756-9766. PMLR, 2021.
* Achour et al. [2022] El Mehdi Achour, Francois Malgouyres, and Franck Mamalet. Existence, stability and scalability of orthogonal convolutional neural networks. _The Journal of Machine Learning Research_, 23(1):15743-15798, 2022.
* Singla and Feizi [2022] Sahil Singla and Soheil Feizi. Improved techniques for deterministic l2 robustness. _arXiv preprint arXiv:2211.08453_, 2022.
* Xu et al. [2022] Xiaojun Xu, Linyi Li, and Bo Li. Lot: Layer-wise orthogonal training on improving l2 certified robustness. _arXiv preprint arXiv:2210.11620_, 2022.
* Boyd and Vandenberghe [2004] Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Serrurier et al. [2021] Mathieu Serrurier, Franck Mamalet, Alberto Gonzalez-Sanz, Thibaut Boissin, Jean-Michel Loubes, and Eustasio Del Barrio. Achieving robustness in classification using optimal transport with hinge regularization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 505-514, 2021.
* Papernot et al. [2021] Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Ulfar Erlingsson. Tempered sigmoid activations for deep learning with differential privacy. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9312-9321, 2021.
* Tramer and Boneh [2021] Florian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data), 2021.
* De et al. [2022] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlocking high-accuracy differentially private image classification through scale. _arXiv preprint arXiv:2204.13650_, 2022.

* [52] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [54] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* [55] David Goldberg. What every computer scientist should know about floating-point arithmetic. _ACM computing surveys (CSUR)_, 23(1):5-48, 1991.
* [56] Hadi Jooybar, Wilson WL Fung, Mike O'Connor, Joseph Devietti, and Tor M Aamodt. Gpudet: a deterministic gpu architecture. In _Proceedings of the eighteenth international conference on Architectural support for programming languages and operating systems_, pages 1-12, 2013.
* [57] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient and accurate estimation of lipschitz constants for deep neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.