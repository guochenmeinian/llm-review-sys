# LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch

 Xiaoyuan Zhang, Liang Zhao, Yingying Yu, Xi Lin, Yifan Chen, Han Zhao, Qingfu Zhang

Corresponding to Prof. Qingfu Zhang. Contact: {xzhang2523-c@my.qingfu.zhang@}cityu.edu.hk.

CityUHK, HKBU, UIUC.

###### Abstract

Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, fairness, robustness, and more. Unlike single-objective optimization, which aggregates objectives into a scalar through weighted sums, MOPs focus on generating specific or diverse Pareto solutions and learning the entire Pareto set directly. Existing MOP benchmarks primarily focus on evolutionary algorithms, which are zeroth-order or meta-heuristic methods that fail to leverage higher-order objective information and cannot scale to large models. To address these challenges, we introduce LibMOON, the first multiobjective optimization library supporting state-of-the-art gradient-based methods, offering a fair and comprehensive benchmark, and open-sourced for the community.

## 1 Introduction

Multiobjective Optimization Problems (MOPs) are ubiquitous in machine learning. For instance, trustworthy machine learning (e.g., algorithmic fairness) problems balancing the fairness level and the accuracy level [1, 2]; in robotics, it is necessary to balance several objectives (e.g., forward speed and energy consumption) [3, 4]; similarly, recommendation systems face potentially conflicting objectives, such as novelty, accuracy, and diversity [5, 6, 7]. For all the applications above, the underlying optimization problem involves an MOP with \(m\) objectives and can be (informally) defined as:

\[\min_{\mathbf{\theta}}\mathbf{L}(\mathbf{\theta})=(L_{1}(\mathbf{\theta}),\ldots,L_{m}(\mathbf{ \theta})), \tag{1}\]

where \(L_{1}(\mathbf{\theta}),\ldots,L_{m}(\mathbf{\theta})\) denote \(m\) (potentially) conflicting objectives and we denote the size of the model parameter as \(n\coloneqq|\mathbf{\theta}|\). Note that as informally defined above, Equation (1) is a vector optimization problem that does not necessarily admit a total ordering. For a non-trivial MOP, no single solution can attain the minimum of all objectives simultaneously. To compare two solutions for an MOP, we introduce the concepts of _dominance_ and _Pareto optimality_[8]. We say that solution \(\mathbf{\theta}^{(a)}\) dominates \(\mathbf{\theta}^{(b)}\) when \(\mathbf{\theta}^{(a)}\) satisfies \(L_{i}(\mathbf{\theta}^{(a)})\leq L_{i}(\mathbf{\theta}^{(b)})\) for all \(1\leq i\leq m\), with at least one strict inequality. A solution is Pareto optimal if no other solution in the feasible region dominates it. The set of all Pareto optimal solutions is called the Pareto set (PS), and its image set is called the Pareto front (PF).

Over the last few decades, multiobjective evolutionary algorithms (MOEAs) emerged as a widely used methodology for addressing MOPs due to their ability to find diverse and approximate PS. Several popular MOEA libraries have emerged, including PlatEMO (Matlab) [9], Pagmo (C++) [10], and Pymoo (Python) [11]. Compared to MOEAs, gradient-based multiobjective optimization (MOO) methods are particularly suitable for large-scale machine learning tasks involving thousands to millions of neural network parameters. In contrast, gradient-based MOO methods can only findPareto _stationary_ solutions--solutions that cannot be _locally_ improved in all objectives, in practice, Pareto stationary solutions well approximate global Pareto solutions for deep learning tasks.

With the growing need for gradient-based MOO methods (e.g., [12; 13; 4; 14]) for large-scale neural networks, there is a pressing need for the development of a standard library to benchmark related algorithms and problems. For this reason, we introduce LibMOON, the first modern gradient-based MOO library supporting over twenty state-of-the-art (SOTA) methods. We summarize our contributions as follows:

1. We introduce the _first_ modern gradient-based MOO library which is implemented in PyTorch [15] and carefully designed to support GPU acceleration. LibMOON supports MO machine learning problems such as MO classification, MO regression, MO distribution matching, etc, along with their real-world applications.
2. LibMOON supports over twenty state-of-the-art (SOTA) gradient-based MOO methods for constructing the PS and PF, including MOO solvers that use _finite_ solutions to approximate the entire PS/PF [16; 17]; Pareto Set Learning (PSL) solvers [18; 19] that approximate the _entire_ PS/PF with a single neural model; and multi-objective Bayesian Optimization (MOBO) solvers, which are designed to minimize the need for avoiding frequent function evaluations.
3. We have open-ourced LibMOON on Github2 with document at LibMOON Docs3. LibMOON can be installed via "pip install libmoon" as an off-the-shelf gradient-based multiobjective package for easy use. Footnote 2: [https://github.com/xzhang2523/libmoon](https://github.com/xzhang2523/libmoon)

Footnote 3: [https://libmoondocs.readthedocs.io/en/latest/](https://libmoondocs.readthedocs.io/en/latest/)

Notation.Bold letters represent vector (e.g., \(\mathbf{\lambda}\) denotes a preference vector). \(\mathbf{x}^{(k)}\) denotes vector \(\mathbf{x}\) at \(k\)-th iteration and \(x_{k}\) denotes the \(k\)-th entry of \(\mathbf{x}\). The preference vector \(\mathbf{\lambda}\) lies in the \(m\)-dim simplex (\(\mathbf{\Delta}_{m}\)), satisfying \(\sum_{i=1}^{m}\lambda_{i}=1\) and \(\lambda_{i}\geq 0\). The decision network parameter \(\mathbf{\theta}\) has a size of \(n\). For two \(m\)-D vectors \(\mathbf{x}^{(a)}\) and \(\mathbf{x}^{(b)}\), \(\mathbf{x}^{(a)}\preceq\mathbf{x}^{(b)}\) means \(\mathbf{x}^{(a)}_{i}\leq\mathbf{x}^{(b)}_{i}\) for all \(i\in[m]\); \(\mathbf{x}^{(b)}\), \(\mathbf{x}^{(a)}\preceq_{\text{strict}}\mathbf{x}^{(b)}\) means that \(\mathbf{x}^{(a)}_{i}\leq\mathbf{x}^{(b)}_{i}\) for all \(i\in[m]\) and for at least one strict inequality. \(\mathbf{x}^{(a)}\prec\mathbf{x}^{(b)}\) means that \(\mathbf{x}^{(a)}_{i}<\mathbf{x}^{(b)}_{i}\) for all \(i\in[m]\). Refer to Table 11 for full notations.

## 2 Related works

### Gradient-based multiobjective optimization

Gradient-based MOO has a rich research literature. The well-known convex optimization book [20; Chap 4] outlines how linear scalarization can transform a MOO problem into a single-objective optimization (SOO) problem. However, for much of the past few decades, gradient-based methods have not been the primary focus for MOO. Instead, MOEAs have gained more attention due to their population-based nature, which is well-suited for approximating the PS and finding diverse solutions. In recent years, however, gradient-based MOO has experienced a resurgence, particularly in (deep) machine learning, where these methods scale better with the number of decision variables. A pivotal contribution in this area is the MODA-UB [12] method, which introduced MOO techniques into deep learning by casting multi-task learning (MTL) as a MOO problem. Since then, many approaches have followed, including EPO [16], Pareto Multi-Task Learning (PMTL) [13], MOO with Stein Variational Gradient Descent (MOO-SVGD) [17], and some methods for learning the entire PS (Pareto set learning) [19; 18; 21; 22; 23; 24].

### Multiobjective optimization libraries

A number of multiobjective libraries exist in the literature. We summarize the high-level comparison between LibMOON and existing libraries in Table 1 and discuss the detailed differences as follows.

**LibMTL**[25] is a Python-based library for multitask learning. LibMTL aims to find a single network to benefit all tasks, such as finding a benign updating direction or optimizing a network architecture. In contrast, LibMOON addresses inherent trade-offs in machine learning problems, where improving one objective inevitably worsens others, and explores the distribution of Pareto solutions.

**jMetal**[26], **Pymoo**[27] and **PlatEMO**[9] are Java, python and Matlab frameworks for MOEAs, supporting popular methods such as NSGA-III [28, 29], MOEA/D [30], and SMS-EMOA [31]. Pymoo allows flexible algorithm customization with user-defined operators and data visualization. PlatEMO is a MATLAB-based multiobjective optimization tool supporting over 160 MOEAs and a comprehensive test problems, including sparse, high-cost, large-scale, and multimodal. PlatEMO also contains a number of metrics and supporting visualization during the optimization process.

**Pagmo**[10] is a C++ library for parallel multiobjective global optimization, utilizing evolutionary algorithms and gradient-based methods like simplex, SQP, and interior-point techniques. It supports constrained, unconstrained, single- and multi-objective, continuous, integer, stochastic, and deterministic optimization problems.

**EvoTorch**[32] and **EvoX**[33]. EvoTorch accelerates evolutionary algorithms in PyTorch, while EvoX scales them to large, complex problems with GPU-accelerated parallel execution for single and multiobjective tasks, including synthetic problems and reinforcement learning.

## 3 LibMoon: A gradient-based MOO library in PyTorch

This section introduces LibMOON. We introduce its framework in Section 3.1, and briefly introducing its supporting **problems** and **metrics**. Then we introduce supported **solvers** in Sections 3.2 to 3.4.

### Framework

Figure 1 demonstrates the components of LibMOON, including three categories of solvers: MOO solvers aiming to find a finite set of Pareto solutions satisfying certain requirements, Pareto set learning (PSL) solvers aiming to learn whole PS with a single model, and MOBO solvers aiming to solve expensive MO problems. Each solver category is designed in a highly modularized way so that new solvers can be easily incorporated into LibMOON by rewriting only a small portion of code, e.g., the specific algorithm of gradient manipulations4. MOO and PSL solvers support all synthetic, MTL, and real-world (RE) problems, while MOBO solvers support synthetic and RE problems.

Footnote 4: An example of adding a new solver is provided in the LibMOON Doc.

**Supported problems.** LibMOON currently supports three categories of methods, synthetic problems, MTL problems, and RE problems.

\begin{table}
\begin{tabular}{l c c l} \hline \hline
**Name** & **Language** & **Year** & **Key Features** \\ \hline
**Pymoo** & Python & 2020 & (2) Euro-order methods \\  & & & (3) Diverse problem types \\ \hline
**JRetail** & Java & 2011 & (1) Single-domain objective optimization \\  & & & (3) Diverse problem types \\ \hline
**PlatEMO** & **Multi** & 2017 & (1) Over 160 MOEAs \\  & & & (3) Provident size demonstrations \\ \hline
**Pagmoo** & C++ & 2020 & (1) Global optimization \\  & & & (2) Parallel optimization \\ \hline
**LIMMTL** & Python & 2023 & (1) Unified codebase \\  & & & (3) Creative solution for new methods \\ \hline
**EvoTorch** & Python & N/A & (1) Distribution-based search algorithms \\  & & & (3) Participation based search algorithms \\  & & & (3) Multiple CPU, GPU, computers \\ \hline
**EvoX** & Python & 2024 & (2) Single-multi-objective optimization \\  & & & (3) Neuroscientist/RL tasks \\ \hline
**LibMOON** & Python & 2024 & (2) Pareto set learners \\  & & & (3) Large-scale (millions \# params.) ML tasks \\ \hline \hline \end{tabular}
\end{table}
Table 1: Previous MOO libraries and LibMOON.

Figure 1: **Supported solvers and problems in LibMOON: LibMOON addresses synthetic, real-world and MTL problems with three categories of solvers: MOO, PSL and MOBO solvers.**

**Supported metrics.** LibMOON supports several metrics for evaluation, (1) hypervolume (HV), (2) inverted general distance (IGD), (3) fill distance (FD), (4) minimal distance (l\({}_{\min}\)), (5) smooth minimal distance (sl\({}_{\min}\)), (5) Spacing, (6) Span, (7) penalty-based intersection (PBI), (8) inner product (IP), (9) cross angle (\(\vartheta\)). Full descriptions of these metitres are provided in Appendix A.3.

### MOO solvers

In this paper, MOO solvers refer to solvers that find a set of Pareto solutions. The simplest and most commonly used method is _linear scalarization_[35], which converts a MOO problem to a single objective optimization problem through some aggregation functions.

Aggregation-based methods.A straightforward way is to use some aggregation functions \(g_{\mathbf{\lambda}}(\cdot):\mathbb{R}^{m}\mapsto\mathbb{R}\) to convert a MOP to a single objective optimization problem. The reason that optimizing this converted single objective optimization problem will yield Pareto optimal solutions is due to the following two theorems.

**Theorem 1** (Adapted from Theorem 2.6.2 [37]).: _If \(g_{\mathbf{\lambda}}(\cdot)\) is **strictly decreasing** w.r.t vector \(\mathbf{L}(\mathbf{\theta})\), i.e., \(g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}^{(a)}))<g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{ \theta}^{(b)}))\) when \(\mathbf{L}_{i}(\mathbf{\theta}^{(a)})\preceq_{\mathrm{strict}}\mathbf{L}_{i}(\mathbf{\theta}^{ (b)})\), then the optimal solution \(\theta^{*}\) of \(g_{\mathbf{\lambda}}(\hat{\mathbf{L}}(\mathbf{\theta}))\) serves as a **Pareto optimal** solution for the original MOP._

Proof.: See Mitten's book [37], Page 22. Similarly, for decreasing aggregation functions, we have the following theorem.

**Theorem 2**.: _If \(g_{\mathbf{\lambda}}\) is **decreasing** w.r.t. vector \(\mathbf{L}(\mathbf{\theta})\) (i.e., \(g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}^{(a)}))\leq g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{ \theta}^{(b)}))\) when \(\mathbf{L}(\mathbf{\theta}^{(a)})\preceq_{\mathrm{strict}}\mathbf{L}(\mathbf{\theta}^{(b)})\), then the optimal solution \(\theta^{*}\) of \(g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))\) serves as a **weakly Pareto optimal** solution for the original MOP._

Proof.: Similar to the proof of Mitten's book [37], Page 22.

In Theorem 2, _weakly Pareto optimality_ means that for a solution \(\mathbf{\theta}^{(a)}\), no other solutions \(\mathbf{\theta}^{\prime}\) can strictly dominate it, i.e., \(\mathbf{L}_{i}(\mathbf{\theta}^{\prime})<\mathbf{L}_{i}(\mathbf{\theta}^{(a)})\) for all \(i\in[m]\). Some common aggregation functions include the linear scalarization (LS) function, where \(g_{\mathbf{\lambda}}^{\mathrm{LS}}(\mathbf{L}(\mathbf{\theta}))=\sum_{i=1}^{m}\lambda_{i}L _{i}(\mathbf{\theta})\), the Tchebycheff function, where \(g_{\mathbf{\lambda}}^{\mathrm{Tche}}(\mathbf{L}(\mathbf{\theta}))=\max_{i\in[m]}\lambda_{i }\cdot(L_{i}(\mathbf{\theta})-z_{i})\) (\(\mathbf{z}\) is a reference point), Penalty-Based Intersection (PBI) function [30], and COSMOS function [34]. For expressions and other aggregation functions, please refer to Appendix A.2.

For a preference vector \(\mathbf{\lambda}\succ\mathbf{0}\), \(g_{\mathbf{\lambda}}^{\mathrm{LS}}(\cdot)\) is a _strictly decreasing function_, hence directly optimizing \(g_{\mathbf{\lambda}}^{\mathrm{LS}}(\cdot)\) yields _Pareto optimal solutions_ (by Theorem 1). However, for any \(\mathbf{\lambda}\in\mathbf{\Delta}_{m}\), optimizing \(g_{\mathbf{\lambda}}^{\mathrm{Cheb}}(\cdot)\) only yields weakly _Pareto optimal solutions_ (by Theorem 2). For an improper setting of the weight factor \(\mu\) (see Appendix A.2 item 1 and 6), the optimal solution of \(g_{\mathbf{\lambda}}^{\mathrm{PBI}}(\cdot)\) or \(g_{\mathbf{\lambda}}^{\mathrm{COSMOS}}(\cdot)\) can be non-(weakly) Pareto optimal solutions of the original MOP.

An aggregation function is optimized by gradient descent in LibMOON via backpropagation, i.e, \(\mathbf{\theta}^{(k+1)}=\mathbf{\theta}^{(k)}-\eta\mathbf{d}_{k}=\mathbf{\theta}^{(k)}-\eta \frac{\partial g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))}{\partial\mathbf{\theta}}|_{ \mathbf{\theta}^{(k)}}\), where \(\mathbf{d}^{(k)}\) is called the updating direction at the \(k\)th iteration. The gradient term \(\frac{\partial g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))}{\partial\mathbf{\theta}}\) can be decomposed into two parts: \(\frac{\partial g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))}{\partial\mathbf{\theta}}= \frac{\partial g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))}{\partial\mathbf{\theta}} \frac{\partial\mathbf{L}(\mathbf{\theta})}{\partial\mathbf{\theta}}\), assuming that \(\frac{\partial\mathbf{L}(\mathbf{\theta})}{\partial\mathbf{\theta}}\) exists. If \(g_{\mathbf{\lambda}}(\cdot)\) is differentiable, then \(\frac{\partial g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))}{\partial\mathbf{L}(\mathbf{\theta })}\) is the standard gradient. In cases where \(g_{\mathbf{\lambda}}(\cdot)\) is non-differentiable, \(\frac{\partial g_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}))}{\partial\mathbf{L}(\mathbf{\theta })}\) could be taken as a sub-gradient.

Gradient manipulation-based methods.Besides directly optimizing the aggregation function, several so-called "_gradient manipulation methods"_ solve an updating direction \(\mathbf{d}^{(k)}\) using gradient information for each iteration for some specific purpose. For example, as listed in Table 3, EPO [16] aims to find "exact Pareto solutions" (the intersection points of the Pareto front and preference vectors), HVGrad [38] aims to maximize the hypervolume of a set of solutions, MOO-SVGD [17]

\begin{table}
\begin{tabular}{l l l} \hline \hline Method & \(L_{1}\) & \(L_{2}\) \\ \hline Fairness classification [34] & BCE & DEO \\ MO classification [13] & CE - BR & CE - UL \\ MO regression [35] & MSE & MSE \\ MO distribution matching & \(D(\cdot\|\cdot)\) & \(D(\cdot\|\cdot)\) \\ \hline BCE: Binary Cross Entropy; DBO [36]: Difference of Equality of Opportunity; CE: Cross Entropy; BR: Bottom Right; UL: Upper Left; MSE: Mean Square Error. \\ \hline \hline \end{tabular}
\end{table}
Table 2: Supported MO machine learning problems. \(L_{1}\): the first objective. \(L_{2}\): the second objective.

aims to find diverse solutions, PMTL [13] aims to identify sector-constrained Pareto solutions, and ExcessMTL [39] aims to find a Pareto solution with the same excess risk across all the objectives. Interestingly, until now, all these gradient manipulation methods can be implemented in two steps: 1 calculating a dynamic weight vector \(\tilde{\mathbf{\alpha}}\) and then 2 performing backpropagation on a generalized aggregation function \(\tilde{g}_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}^{(k)}))\), where \(\tilde{g}_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}^{(k)}))=\sum_{i=1}^{m}\tilde{\alpha }_{i}L_{i}(\mathbf{\theta})\). At each iteration, gradient manipulation methods can be equivalently expressed as updating the gradient of its induced generalization aggregation function \(\tilde{g}_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}^{(k)}))\). The weight vector \(\tilde{\mathbf{\alpha}}\) are achieved by solving a linear programming (LP) problem (e.g., [16, Eq. 24]) in EPO, a quadratic programming (QP) problem (e.g., [13, Eq. 14]) in PMTL, or other more complex algorithms as used to calculate the hypervolume gradient \(\mathbf{\lambda}\).

Some MOO solvers accept preference vectors \(\mathbf{\lambda}\) as input, termed _preference-based_, affecting Pareto solution positions. The others, called _preference-free_, do not accept preferences, such as those maximizing dominated hypervolume. A summary of these solvers is in Table 3.

Zero-order optimization.In the previous discussion, we assume that all the gradients of objective functions \(\nabla L_{i}(\mathbf{\theta})\) can be easily achieved via backward propagation. However, for some black-box optimization problems, \(\nabla L_{i}(\mathbf{\theta})\)'s may not easily be achieved. Therefore, LiMDON not only supports first-order optimization, but also supports zero-order optimization methods with estimated gradients \(\hat{\nabla}L_{i}(\mathbf{\theta})\) such as evolutionary strategy (ES) [44].

### Pareto set learning solvers

LibMOON also supports Pareto Set Learning (PSL) solvers, which trains a model with parameter \(\mathbf{\phi}\) to approximate the _entire_ PS/PF. A Pareto model is denoted as \(\mathbf{\theta}_{\mathbf{\phi}}(\cdot):\mathbf{\Delta}_{m}\mapsto\mathbb{R}^{n}\) with input as a preference vector and output as a Pareto solution.

**PSL Architecture.** Pareto models vary in structure. For synthetic problems, the simplest model is a fully-connected neural network that takes a preference as input and outputs the corresponding Pareto solution. In multitask learning, the input is a pair \((\mathbf{x},\mathbf{y})\) from dataset \(\mathcal{D}\), and the decision variable \(\mathbf{\theta}\) represents the target network's parameter, with \(\phi\) as the hypernetwork's parameter [45]. The loss vector is calculated as \(\mathbf{L}(\mathbf{\theta})=\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}}(\ell(\mathbf{t}_{ \mathbf{\theta}}(\mathbf{x}),\mathbf{y}))\), where \(\ell\) is a basic loss function like cross-entropy or mean square error. Structures of these two models are illustrated by Figure 2. Besides these two models, LiMDON also supports LoRA (low rank adaptation)-based PSL [46, 21, 47], which admits a low rank adaptation structure and other structures. PSL structures are decoupled from the training loss and used as plug-ins.

**PSL Training.** Goal of PSL is to find a model with parameter \(\mathbf{\phi}\) optimizing the PSL loss \(\ell_{\text{psl}}\),

\[\min_{\mathbf{\phi}}\ell_{\text{psl}}=\mathbb{E}_{\mathbf{\lambda}\sim\text{Dir}(\mathbf{ p})}\tilde{g}_{\mathbf{\lambda}}(\mathbf{L}(\mathbf{\theta}_{\mathbf{\phi}}(\mathbf{\lambda}))), \qquad\text{where}\quad\mathbf{L}(\mathbf{\theta})=\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim \mathcal{D}}(\ell(\mathbf{t}_{\mathbf{\theta}}(\mathbf{x}),\mathbf{y})).\]

In the above formulation, \(\text{Dir}(\mathbf{p})\) is a Dirichlet distribution with hyperparameter \(\mathbf{p}\). \(\tilde{g}_{\mathbf{\lambda}}(\cdot)\) can either be a generalized aggregation function as introduced in the previous section or a normal aggregation function. The gradient of \(\ell_{\text{psl}}\) can be estimated by the chain rule:

\begin{table}
\begin{tabular}{l l l c} \hline \hline
**Method** & **Solution Property** & **Complexity** & **Pref.** \\ \hline EPO [16] & Exact solutions & \(O(m^{2}nK)\) & \(\checkmark\) \\ HVChap [18] & Solutions with maximal HV & \(O(m^{2}nK^{2})\) & \(\times\) \\ MOGA-UB [12] & Random solutions & \(O(m^{2}nK)\) & \(\times\) \\ MOO-SOD [17] & Density by particles repulsion & \(O(m^{2}nK^{2})\) & \(\times\) \\ PMQCA [40] & Solutions under specific demands & \(O(m^{2}nK)\) & \(\checkmark\) \\ PMTL [13] & Solutions in sectors & \(O(m^{2}nK)\) & \(\times\) \\ Random [41] & Random solutions & \(O(m^{2}nK)\) & \(\times\) \\ Agg-LS [37] & Conver parts of a PF & \(O(mnK)\) & \(\checkmark\) \\ Agg-Tob [42] & Exact solutions & \(O(mnK)\) & \(\checkmark\) \\ Agg-MLrba [24] & Exact solutions & \(O(mnK)\) & \(\checkmark\) \\ ExecMMTL [39] & Exact solutions & \(O(mnK)\) & \(\checkmark\) \\ Agg-PH [30] & Approximate exact solutions & \(O(mnK)\) & \(\checkmark\) \\ Agg-SmoothSf [23] & Approximate exact solutions & \(O(mnK)\) & \(\checkmark\) \\ Agg-SmoothTob [22] & Approximate exact solutions & \(O(mnK)\) & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: MOO solvers, properties, and complexities.

Figure 2: Architecture of Pareto models.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

**Conclusion.** For synthetic problems, the most recommended method is _Agg-Tche_ since (1) it keeps function convexity, (2) it finds "exact" Pareto solutions under quite mild conditions (3) it does not need to calculate the Jacobian matrix for each iteration.

### Pareto set learning on synthetic problems

In this section, we present PSL results (Figure 4 and table 7) also on VLMOP2. 1. PSL with the COSMOS aggregation function fails to find all marginal Pareto solutions because COSMOS does not guarantee the discovery of the entire PS/PF. PSL with linear scalarization function could not fit the two endpoints of the PF. Those PSL results inherit from their base MOO solvers. 2. PSL with the smooth Tchebycheff function finds diverse but non-exact Pareto solutions. In contrast, PSL with Agg-Tche, EPO, and PMGDA as base solvers discovers the entire PS/PF, as all three methods find exact Pareto solutions. By traversing the preference simplex, the model accurately fits the entire PS.

**Conclusion.** The most recommended method is still _Agg-Tche-based PSL_ since its basic MOO solver Agg-Tche has attractive properties as mentioned in the previous section.

### MOO solvers for MTL problems

We evaluate the performance of finite Pareto solvers on the Adult dataset, a multitask fairness classification problem. The decision variable \(\mathbf{\theta}\) represents the parameters of a fully-connected neural network with \(|\mathbf{\theta}|=28033\). The first objective is cross-entropy loss, and the second is the DEO loss [34][Eq. 6. 1]. Agg-LS has two drawbacks: (1) it cannot identify the non-convex part of a PF (as previous section mentioned), and (2) the relationship between preference vectors and Pareto objectives is unknown; different preference vectors may yield duplicate solutions. Agg-PBI and Agg-COSMOS only find a small portion of the PF. 2. Agg-Smooth mTche and Agg-mTche perform well on this task, as they can find (approximate) "exact" Pareto solutions. Once the range of PF range is known, diverse solutions can be easily found using uniform preference vectors. The Random and MGDA-UB methods only find a single Pareto solution, since the position of this solution cannot be controlled by these methods. 3. Among the three methods that directly find a set of Pareto solutions (MOO-SVGD, PMTL, and HV-Grad), HV-Grad produces the most diverse solutions with the largest hypervolume. PMTL, being a two-stage method, may fail when solutions fall outside the sector due to stochastic factors. MOO-SVGD optimizes both convergence and diversity but is generally unstable based on our tests.

**Conclusion.** For convex Pareto fronts in MTL problems, _Agg-LS_ is recommended for computational efficiency. However, _PMGDA_ or _EPO_ may offer better convergence and preference-solution correspondence.

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l} \hline \hline Method & \(I_{\text{min}}\) & \(I_{\text{all}}\) & \(I_{\text{min}}\) & Sparsion & Sparsity & HV & IP & Cross Angle & PBI & Span \\ \hline COSMOS & 0.045 (0.000) & -0.127 (0.000) & 1.560 (0.000) & **0.525 (0.000)** & 0.318 (0.000) & 0.752 (0.000) & 0.950 (0.001) & 0.995 (0.000) & 0.907 (0.000) \\ Agg-LS & 0.000 (0.000) & -0.258 (0.000) & **1.158 (0.000)** & 1.32

[MISSING_PAGE_FAIL:9]

evaluations is set as 200. Our experimental results, illustrated in Figure 4, clearly demonstrate the rapid convergence capabilities of all three methods. DirHV-EGO, PSL-DirHV-EI, and PSL-MOBO not only efficiently navigate the solution space but also quickly reach optimal solutions. This highlights the robustness and effectiveness of our implemented algorithms in handling different types of MOPs.

## 5 Conclusion, limitations, and further works

Conclusion.We introduced the _first_ modern gradient-based MOO framework called LibMOON in PyTorch for the research community's convenience. LibMOON supports more than 20 mainstream gradient-based MOO methods; the modular design of LibMOON further allows the library to address various MOPs via various methods in a plug-and-play manner. LibMOON can thus be leveraged to quickly yet robustly test new MOO ideas.

Limitationsinclude: (1) rapid developments of gradient-based MOO methods makes it hard to incorporate all methods, so some effective methods may be missing; (2) gradient-based solvers may fail for problems with a number of local optimas.

Future Workincludes (1) maintaining a user and development community to address issues and (2) adding newly published methods as quickly as possible.

Figure 6: Training process for generating predicted Pareto solutions using different PSL solvers on MO-MNIST problem.

Figure 7: HV curves on MOBO problem. Results are averaged on five random seeds. Reference point to calculate HV : [1.2, 1.2].

## Acknowledge

The work described in this paper was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China [GRF Project No. CityU 11215622].

## Appendix A Appendix

### Full name and notation tables

This section lists the full names of optimization methods and terms for clarity (Table 10) and provides notation in Table 11.

### Aggregation functions

Aggregation function convert an MOP into a single-objective optimization problem under a specific preference vector \(\mathbf{\lambda}\). Some popular aggregation functions are:

1. **COSMOS**: \[g_{\mathbf{\lambda}}^{\rm cosmos}(\mathbf{\theta})=\mathbf{\lambda}^{\top}\mathbf{L}(\mathbf{ \theta})-\mu\frac{\mathbf{\lambda}^{\top}\mathbf{L}(\mathbf{\theta})}{\|\mathbf{\lambda}\|\| \mathbf{L}(\mathbf{\theta})\|},\] (3) where \(\mu\) is a positive weight factor to align the objective vector \(\mathbf{L}(\mathbf{\theta})\) with the preference vector \(\mathbf{\lambda}\).
2. **Linear scalarization (LS)**: \[g_{\mathbf{\lambda}}^{\rm LS}(\mathbf{\theta})=\sum_{i=1}^{m}\lambda_{i}L_{i}(\mathbf{ \theta}).\] (4)
3. **Tchebycheff (Tche)**: \[g_{\mathbf{\lambda}}^{\rm Tche}(\mathbf{\theta})=\max_{1\leq i\leq m}\left\{\lambda_{ i}(L_{i}(\mathbf{\theta})-z_{i})\right\},\] (5) where \(\mathbf{z}\) is a reference point, usually set as the nadir point the minimal value in each objective. Modified Tchebycheff is the same as the original one by simply choosing \(\lambda_{i}^{\prime}\) to be \(1/\lambda_{i}\), \(g_{\mathbf{\lambda}}^{\rm mTche}(\mathbf{\theta})=g_{\mathbf{\lambda}^{\prime}}^{\rm Tche }(\mathbf{\theta})\).
4. **Smooth Tchebycheff (STche)**: \[g_{\mathbf{\lambda}}^{\rm STche}(\mathbf{\theta})=\frac{1}{h}\log\left(\sum_{i=1}^{m} \exp(h\cdot\lambda_{i}(L_{i}(\mathbf{\theta})-z_{i}))\right).\] (6) The Smooth Tchebycheff function uses a relaxed Smooth \(\max\) operator. The advantage of this approach is that \(g_{\mathbf{\lambda}}^{\rm STche}(\mathbf{\theta})\) becomes a smooth function if each objective function \(L_{i}(\mathbf{\theta})\) is smooth, unlike the non-Smooth \(g_{\mathbf{\lambda}}^{\rm Tche}(\mathbf{\theta})\). Smooth functions generally have faster convergence rate compared to non-Smooth ones. Similarly, we can define the Smooth modified Tchebycheff function.
5. **Penalty-Based Intersection (PBI):** \[g_{\mathbf{\lambda}}^{\rm PBI}(\mathbf{\theta})=\underbrace{\frac{1}{\|\mathbf{\lambda}\| }\cdot\sum_{i=1}^{m}\lambda_{i}L_{i}(\mathbf{\theta})}_{d_{1}}+\mu\underbrace{\| \mathbf{L}(\mathbf{\theta})-\frac{d_{1}}{\|\mathbf{\lambda}\|}\cdot\mathbf{\lambda}\|}_{d_{2}},\] (7) where \(\mu\) is a positive weight factor that encourage a objective to align with a preference vector \(\mathbf{\lambda}\).
6. \(p\)**-norm**: \[g_{\mathbf{\lambda}}^{\rm pmorm}(\mathbf{\theta})=\|\mathbf{\lambda}\odot\mathbf{L}(\mathbf{\theta })-\mathbf{z}\|_{p}.\] (8) The symbol \(\odot\) denotes the element-wise product between two vectors.

7. **Augmented Achievement Scalarization Function (AASF)**: \[g_{\mathbf{\lambda}}^{\mathrm{AASF}}(\mathbf{\theta})=g_{\mathbf{\lambda}}^{\mathrm{mTche}}( \mathbf{\theta})+\mu g_{\mathbf{\lambda}}^{\mathrm{LS}}(\mathbf{\theta}),\] (9) where \(\mu\) is small positive coefficient, which is usually set as 0.1. Contour curves for this function for a bi-objective case can be found in the LibMOON Doc6. Footnote 6: [https://libmoodocs.readthedocs.io/en/latest/gallery/aggfuns.html](https://libmoodocs.readthedocs.io/en/latest/gallery/aggfuns.html)

### Metrics

Metrics used in LibMOON can be categorized into two groups. The first group evaluates the quality of a _set_ of solutions \(\mathbb{Y}=\{\mathbf{y}^{(1)},\ldots,\mathbf{y}^{(K)}\}\), with specific metrics such as IGD and FD relying on the known Pareto front for accuracy. The second group of metrics assesses the quality of _individual_ solutions \(\mathbf{y}\) when a preference vector \(\mathbf{\lambda}\) is provided.

Group 1: Metrics for a set of solutions.

1. Hypervolume (HV) (\(\uparrow\)) [55]: This metric evaluates both the convergence to the PF and the diversity of solutions. A low HV value indicates poor convergence, while high HV values

\begin{table}
\begin{tabular}{l l} \hline \hline Short Name & Full name \\ \hline \hline MOP & Multiobjective Optimization Problem \\ SOP & Singleobjective Optimization Problem \\ MOO & MultiObjective Optimization \\ MOEA & MultiObjective Evolutionary Algorithm \\ MOBO & MultiObjective Baysian Optimization \\ PSL & Pareto Set Learning \\ PS & Pareto Set \\ PF & Pareto Front \\ “exact” Pareto solution & The corresponding Pareto objective aligns with a given preference vector \\ \hline ES & Evolutionary strategy \\ BP & Backward propagation \\ \hline PMTL [13] & Pareto Mults-Task Learning \\ MOO-SVGD [17] & MultiObjective Optimization Stein Variational Gradient Descent \\ EPO [16] & Exact Pareto Optimization \\ PMGDA [40] & Preference based Multiple Gradient Descent Algorithm \\ Agg-LS [37] & Aggregation function with Linear Scalarization \\ Agg-PBI [30] & Aggregation function with Penalty Based Intersection \\ Agg-Tche [30] & Aggregation function with Tchebycheff scalarization \\ Agg-mTche [42] & Aggregation function with modified Tchebycheff scalarization \\ Agg-COSMOS [34] & Aggregation function with COSMOS scalarization \\ \hline RE problems & Realworld problems \\ \hline \hline \end{tabular}
\end{table}
Table 10: Short name to full name table

\begin{table}
\begin{tabular}{l l} \hline \hline Notation & Meaning \\ \hline \hline \(\mathbf{\theta}\) & The decision variable of an MOP. \\ \(\mathbf{\phi}\) & The decision variable of a Pareto set model. \\ \(m\) & Number of objectives. \\ \(n\) & Number of decision variables. \\ \(K\) & Number of finite Pareto solutions. \\ \(\alpha_{i}\) & Coefficients of objective functions. \\ \(\mathbf{\lambda}\) & A preference vector. \\ \hline \hline \end{tabular}
\end{table}
Table 11: Notations used in this paper imply better performance. The hypervolume is calculated as the volume dominated by at least one solution in the set \(\mathbb{S}\) with respect to a reference point \(\mathbf{r}\): \[\mathrm{HV}_{\mathbf{r}}(\mathbb{S})=\mathrm{Vol}(\mathbf{y}\mid\exists\mathbf{y}^{\prime} \in\mathbb{S},\mathbf{y}^{\prime}\preceq\mathbf{y}\preceq\mathbf{r}).\]
2. Inverted Generational Distance (IGD) [56]: IGD measures the average distance between points in a reference set \(\mathbb{Z}\) and the nearest solutions in the set \(\mathbb{S}\): \[\mathrm{IGD}(\mathbb{S})=\frac{1}{|\mathbb{Z}|}\left(\sum_{i=1}^{|\mathbb{Z} |}\min_{\mathbf{y}^{\prime}\in\mathbb{S}}\rho(\mathbf{y}^{(i)},\mathbf{y}^{\prime})^{2} \right)^{1/2}.\]
3. Fill Distance (FD) [57]: This metric calculates the covering radius of a set of solutions \(\mathbb{S}\), defined as the maximum minimum distance from any point in the reference set \(\mathbb{Z}\) to the nearest solution in \(\mathbb{Z}\): \[\mathrm{FD}(\mathbb{S})=\max_{\mathbf{y}^{\prime}\in\mathbb{Z}}\min_{\mathbf{y}^{ \prime}\in\mathbb{S}}\rho(\mathbf{y},\mathbf{y}^{\prime}).\] (10)
4. Minimal Distance (\(l_{\min}\)): This metric captures the smallest pairwise distance among all objectives: \[l_{\min}=\min_{1\leq i<j\leq K}\rho(\mathbf{y}^{(i)},\mathbf{y}^{(j)})\] where \(\rho()\) denotes the Euclidean distance.
5. Smooth Minimal Distance (\(sl_{\min}\)): This metric is a "smooth-min" version of the minimal distance function, defined as: \[sl_{\min}=-\frac{1}{h\cdot K(K-1)}\log\left(\sum_{1\leq i<j\leq K}\exp\left(- h\rho\left(\mathbf{y}^{(i)},\mathbf{y}^{(j)}\right)\right)\right).\] (11)
6. Spacing: This metric measures the standard deviation of the minimal distances from one solution to others, with lower values indicating a more uniform distribution of objective vectors: \[\mathrm{spacing}=\frac{1}{K}\sum_{i=1}^{K}(d_{i}-\bar{d})^{2},\qquad\bar{d}= \frac{1}{K}\sum_{i=1}^{K}d_{i},\qquad d_{i}=\min_{1\leq i\neq j\leq K}\rho(\bm {y}^{(i)},\mathbf{y}^{(j)}).\] (12)
7. Span: This metric evaluates the range (span) of solutions in their minimal dimension, defined as: \[\mathrm{Span}=\min_{1\leq i\leq m}\max_{1\leq k<l\leq K}|y_{i}^{(k)}-y_{i}^{( l)}|.\] (13)

Group 2: Metrics for individual solutions.

1. Penalty-based Intersection (PBI): This metric is a weighted sum of two distance functions \(d_{1}\) and \(d_{2}\), given by \(\mathrm{PBI}=d_{1}+\mu d_{2}\), where \[d_{1}=\frac{\left\langle\mathbf{y}-\mathbf{z},\mathbf{\lambda}\right\rangle}{\left\|\mathbf{ \lambda}\right\|},\qquad d_{2}=\left\|\mathbf{y}-(d_{1}\mathbf{\lambda}+\mathbf{z})\right\|.\] (14)
2. Inner Product (IP): This metric measures the alignment of the objective vector \(\mathbf{y}\) with the preference vector \(\mathbf{\lambda}\): \[\mathrm{IP}=\left\langle\mathbf{y},\mathbf{\lambda}\right\rangle.\] (15)
3. Cross Angle (\(\vartheta\)): For bi-objective problems, this metric measures the angular difference between the objective vector and the preference vector: \[\vartheta=\left\|\arctan(y_{2}/y_{1})-\arctan(\lambda_{2}/\lambda_{1})\right\|.\] (16)

Those metrics are summarized in Table 12 and also can be found in the LibMOON document.

### GPU acceleration

We evaluate LibMOON performance on Pareto set learning for the MO-MNIST problem across various platforms (CPU, RTX 3080, 4060, 4090). Running times are detailed in Table 13 and visualized in Figure 8. The table and figure show a significant reduction in time (about one-third) when using a personal GPU compared to a CPU. The RTX 4090 further reduces time by approximately 25% compared to the RTX 4060.

### License, usage, and code dependence

The license used for Adult/Compas/Credit follows Creative Commons Attribution 4.0 International (CC BY 4.0), Database Contents License (DbCL) v1.0, and CC0: Public Domain, respectively. For academic use of LibMOON, please cite our paper or GitHub. Commercial use requires author permission.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Platform** & **MO-MNIST** & **MO-Fashion** & **Fashion-MNIST** \\ \hline
**CPU** & 43.43 & 44.45 & 46.45 \\
**RTX 4060 (8G)** & 14.72 & 13.21 & 12.43 \\
**RTX 3080 (10G)** & 7.88 & 7.16 & 7.17 \\
**RTX 4090 (24G)** & 3.27 & 3.27 & 3.27 \\ \hline \hline \end{tabular}

* We run all datasets for 100 epochs using 3M parameters on a 13th Gen Intel(R) Core(TM) i9-13900HX CPU.

\end{table}
Table 12: Supported metrics.

Figure 8: Running time for Pareto set learning on the MO-MNIST problem using different devices using 3M parameters.

Footnote 6: [https://github.com/ruchtem/cosmos](https://github.com/ruchtem/cosmos)

Some part codes of LibMOON follows (1) COSMOS 7, (2) PHN 8, and (3) HVGrad 9.

Footnote 7: [https://github.com/riuchem/cosmos](https://github.com/riuchem/cosmos)

Footnote 8: [https://github.com/AviwNavon/pareto-hypernetworks](https://github.com/AviwNavon/pareto-hypernetworks)

Footnote 9: [https://github.com/timodeist/multi_objective_learning](https://github.com/timodeist/multi_objective_learning)

## References

* [1]H. Zhao and G. J. Gordon (2022) Inherent tradeoffs in learning fair representations. The Journal of Machine Learning Research23 (1), pp. 2527-2552. Cited by: SS1.
* [2]R. Xian, L. Yin, and H. Zhao (2023) Fair and optimal classification via post-processing. In International Conference on Machine Learning, pp. 37977-38012. Cited by: SS1.
* [3]M. Brandao, M. Fallon, and I. Havoutis (2019) Multi-controller multi-objective locomotion planning for legged robots. In 2019 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 4714-4721. Cited by: SS1.
* [4]J. Xu, Y. Tian, P. Ma, D. Rus, S. Sueda, and W. Matusik (2020) Prediction-guided multi-objective reinforcement learning for continuous robot control. In International conference on machine learning, pp. 10607-10616. Cited by: SS1.
* [5]D. Jannach (2022) Multi-objective recommender systems: survey and challenges. arXiv preprint arXiv:2210.10309. Cited by: SS1.
* [6]E. Yeo Keat, N. M. Sharef, R. Yaakob, K. Azhar Kasmiran, E. Marlisah, N. Mustapha, and M. Zolkepli (2022) Multiobjective deep reinforcement learning for recommendation systems. IEEE Access10, pp. 65011-65027. Cited by: SS1.
* [7]F. Ezzahra Zaizi, S. Qassimi, and S. Rakrak (2023) Multi-objective optimization with recommender systems: a systematic review. Information Systems117, pp. 102233. Cited by: SS1.
* [8]M. Ehrgott (2005) Multicriteria optimization. Vol. 491, Springer Science & Business Media. Cited by: SS1.
* [9]Y. Tian, R. Cheng, X. Zhang, and Y. Jin (2017) Platemo: a matlab platform for evolutionary multi-objective optimization [educational forum]. IEEE Computational Intelligence Magazine12 (4), pp. 73-87. Cited by: SS1.
* [10]F. Biscani and D. Izzo (2020) A parallel global multiobjective framework for optimization: pagmo. Journal of Open Source Software5 (53), pp. 2338. Cited by: SS1.
* [11]J. Blank, K. Deb, Y. Dhebar, S. Bandaru, and H. Seada (2020) Generating well-spaced points on a unit simplex for evolutionary many-objective optimization. IEEE Transactions on Evolutionary Computation25 (1), pp. 48-60. Cited by: SS1.
* [12]O. Sener and V. Koltun (2018) Multi-task learning as multi-objective optimization. Advances in neural information processing systems31. Cited by: SS1.
* [13]X. Lin, H. Zhen, Z. Li, Q. Zhang, and S. Kwong (2019) Pareto multi-task learning. Advances in neural information processing systems32. Cited by: SS1.
* [14]S. P. and M. Dellnitz (2018) Gradient-based multiobjective optimization with uncertainties. In NEO 2016: Results of the Numerical and Evolutionary Optimization Workshop NEO 2016 and the NEO Cities 2016 Workshop held on September 20-24, 2016 in Talnepantla, Mexico, pp. 159-182. Cited by: SS1.
* [15]S. Imambi, K. Bhanu Prakash, and G. Kanagachidambaresan (2021) PyTorch. Programming with TensorFlow: Solution for Edge Computing Applications, pp. 87-104. Cited by: SS1.
* [16]D. Mahapatra and V. Rajan (2020) Multi-task learning with user preferences: gradient descent with controlled ascent in pareto optimization. In International Conference on Machine Learning, pp. 6597-6607. Cited by: SS1.
* [17]X. Liu, X. Tong, and Q. Liu (2021) Profiling pareto front with multi-objective stein variational gradient descent. Advances in Neural Information Processing Systems34, pp. 14721-14733. Cited by: SS1.
* [18]A. Navon, A. Shamsian, G. Chechik, and E. Fetaya (2020) Learning the pareto front with hypernetworks. arXiv preprint arXiv:2010.04104. Cited by: SS1.
* [19]X. Lin, Z. Yang, Q. Zhang, and S. Kwong (2020) Controllable pareto multi-task learning. arXiv preprint arXiv:2010.06313. Cited by: SS1.
* [20]S. P. Boyd and L. Vandenberghe (2004) Convex optimization. Cambridge university press. Cited by: SS1.

* [21] Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Qingfu Zhang, Siyuan Qi, and Yaodong Yang. Panacea: Pareto alignment via preference adaptation for l1ms. _arXiv preprint arXiv:2402.02030_, 2024.
* [22] Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, and Qingfu Zhang. Smooth tchebychef scalarization for multi-objective optimization. In _International conference on machine learning_. PMLR, 2024.
* [23] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Pareto set learning for expensive multi-objective optimization. _Advances in Neural Information Processing Systems_, 35:19231-19247, 2022.
* [24] Xiaoyuan Zhang, Xi Lin, Bo Xue, Yifan Chen, and Qingfu Zhang. Hypervolume maximization: A geometric view of pareto set learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [25] Baijiong Lin and Yu Zhang. Libmtl: A python library for deep multi-task learning. _The Journal of Machine Learning Research_, 24(1):9999-10005, 2023.
* [26] Juan J. Durillo, Antonio J. Nebro, and Enrique Alba. The jmetal framework for multi-objective optimization: Design and architecture. In _IEEE Congress on Evolutionary Computation_, pages 1-8, 2010.
* [27] J. Blank and K. Deb. pymoo: Multi-objective optimization in python. _IEEE Access_, 8:89497-89509, 2020.
* [28] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: Solving problems with box constraints. _IEEE Transactions on Evolutionary Computation_, 18(4):577-601, 2014.
* [29] Himanshu Jain and Kalyanmoy Deb. An evolutionary many-objective optimization algorithm using reference-point based nondominated sorting approach, part ii: Handling constraints and extending to an adaptive approach. _IEEE Transactions on Evolutionary Computation_, 18(4):602-622, 2014.
* [30] Qingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposition. _IEEE Transactions on evolutionary computation_, 11(6):712-731, 2007.
* [31] Nicola Beume, Boris Naujoks, and Michael Emmerich. Sms-emoa: Multiobjective selection based on dominated hypervolume. _European Journal of Operational Research_, 181(3):1653-1669, 2007.
* [32] Nihat Engin Toklu, Timothy Atkinson, Vojtech Micka, Pawel Liskowski, and Rupesh Kumar Srivastava. Evotorch: Scalable evolutionary computation in python. _arXiv preprint arXiv:2302.12600_, 2023.
* [33] Beichen Huang, Ran Cheng, Zhuozhao Li, Yaochu Jin, and Kay Chen Tan. Evox: A distributed gpu-accelerated framework for scalable evolutionary computation. _IEEE Transactions on Evolutionary Computation_, 2024.
* [34] Michael Ruchte and Josif Grabocka. Scalable pareto front approximation for deep multi-objective learning. In _2021 IEEE international conference on data mining (ICDM)_, pages 1306-1311. IEEE, 2021.
* [35] Yuzheng Hu, Ruicheng Xian, Qilong Wu, Qiuling Fan, Lang Yin, and Han Zhao. Revisiting scalarization in multi-task learning: A theoretical perspective. _arXiv preprint arXiv:2308.13985_, 2023.
* [36] Kirtan Padh, Diego Antognini, Emma Lejal-Glaude, Boi Faltings, and Claudiu Musat. Addressing fairness in classification with a model-agnostic multi-objective algorithm. In _Uncertainty in artificial intelligence_, pages 600-609. PMLR, 2021.
* [37] Kaisa Miettinen. _Nonlinear multiobjective optimization_, volume 12. Springer Science & Business Media, 1999.
* [38] Timo M Deist, Monika Grewal, Frank JWM Dankers, Tanja Alderliesten, and Peter AN Bosman. Multi-objective learning to predict pareto fronts using hypervolume maximization. _arXiv preprint arXiv:2102.04523_, 2021.
* [39] Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng, Trishul Chilimbi, and Han Zhao. Robust multi-task learning with excess risks. _International conference on machine learning_, 2024.

* [40] Xiaoyuan Zhang, Xi Lin, and Qingfu Zhang. Pmgda: A preference-based multiple gradient descent algorithm. _IEEE Transactions on Emerging Topics in Computational Intelligence_, 2024.
* [41] Baijiong Lin, Feiyang Ye, Yu Zhang, and Ivor W Tsang. Reasonable effectiveness of random weighting: A litmus test for multi-task learning. _arXiv preprint arXiv:2111.10603_, 2021.
* [42] Xiaoliang Ma, Qingfu Zhang, Guangdong Tian, Junshan Yang, and Zexuan Zhu. On tchebycheff decomposition approaches for multiobjective evolutionary optimization. _IEEE Transactions on Evolutionary Computation_, 22(2):226-244, 2017.
* [43] Michael Emmerich and Andre Deutz. Time complexity and zeros of the hypervolume indicator gradient field. In _EVOLVE-a bridge between probability, set oriented numerics, and evolutionary computation III_, pages 169-193. Springer, 2014.
* [44] Hans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies-a comprehensive introduction. _Natural computing_, 1:3-52, 2002.
* [45] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. _arXiv preprint arXiv:1609.09106_, 2016.
* [46] Weiyu Chen and James T Kwok. Efficient pareto manifold learning with low-rank structure. _arXiv preprint arXiv:2407.20734_, 2024.
* [47] Nikolaos Dimitriadis, Pascal Frossard, and Francois Fleuret. Pareto low-rank adapters: Efficient multi-task learning with preferences. _arXiv preprint arXiv:2407.08056_, 2024.
* [48] Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, and Qingfu Zhang. Evolutionary pareto set learning with structure constraints. _arXiv preprint arXiv:2310.20426_, 2023.
* [49] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient Monte-Carlo Bayesian optimization. _Advances in neural information processing systems_, 33:21524-21538, 2020.
* [50] Alexander Cowen-Rivers, Wenlong Lyu, Rasul Tutunov, Zhi Wang, Antoine Grosnit, Ryan-Rhys Griffiths, Alexandre Maravel, Jianye Hao, Jun Wang, Jan Peters, and Haitham Bou Ammar. HEBO: Pushing the limits of sample-efficient hyperparameter optimisation. _Journal of Artificial Intelligence Research_, 74, 07 2022.
* [51] Liang Zhao and Qingfu Zhang. Hypervolume-guided decomposition for parallel expensive multiobjective optimization. _IEEE Transactions on Evolutionary Computation_, 28(2):432-444, 2024.
* [52] Xi Lin, Qingfu Zhang, and Sam Kwong. An efficient batch expensive multi-objective evolutionary algorithm based on decomposition. In _2017 IEEE Congress on Evolutionary Computation (CEC)_, pages 1343-1349, 2017.
* [53] Biswajit Paria, Kirthevasan Kandasamy, and Barnabas Poczos. A flexible framework for multi-objective bayesian optimization using random scalarizations. In _Uncertainty in Artificial Intelligence_, pages 766-776. PMLR, 2020.
* [54] Zhenkun Wang, Jingda Deng, Qingfu Zhang, and Qite Yang. On the parameter setting of the penalty-based boundary intersection method in moea/d. In _International Conference on Evolutionary Multi-Criterion Optimization_, pages 413-423. Springer, 2021.
* [55] Andreia P Guerreiro, Carlos M Fonseca, and Luis Paquete. The hypervolume indicator: Problems and algorithms. _arXiv preprint arXiv:2005.00515_, 2020.
* [56] Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance calculation in generational distance and inverted generational distance. In _Evolutionary Multi-Criterion Optimization: 8th International Conference, EMO 2015, Guimaraes, Portugal, March 29-April 1, 2015. Proceedings, Part II 8_, pages 110-125. Springer, 2015.
* [57] Xiaoyuan Zhang, Genghui Li, Xi Lin, Yichi Zhang, Yifan Chen, and Qingfu Zhang. Gliding over the pareto front with uniform designs. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]. 2. Did you describe the limitations of your work? [Yes], see Section 5. 3. Did you discuss any potential negative societal impacts of your work? [N/A]. LibMOON is a basic optimization library and we do not see direct societal impacts. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [N/A].
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [Yes]. The assumption is in the theorem itself, e.g., in Theorem 1. 2. Did you include complete proofs of all theoretical results? [No]. This theorem is a restatement of previous results. We have give both the thereom and its proof proper citations.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]. These instruments are provided in the LibMOON Github page: [https://github.com/xzhang2523/libmoon](https://github.com/xzhang2523/libmoon). 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]. See source code of LibMOON. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]. We have reported standard derivation results. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]. See first paragraph of Section 4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes]. See Appendix A.5. 2. Did you mention the license of the assets? [Yes]. See Appendix A.5. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes]. Code are provided in [https://github.com/xzhang2523/libmoon](https://github.com/xzhang2523/libmoon). 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]. LibMOON uses public data such as multiobjective classification and fairness classification. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A].
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]