# AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neural Activity

Jingyuan Li\({}^{1}\), Leo Scholl\({}^{1}\), Trung Le\({}^{1}\), Pavithra Rajeswaran\({}^{2}\), Amy Orsborn\({}^{1,2}\), Eli Shlizerman\({}^{1,3}\)

1. Department of Electrical & Computer Engineering,

2. Department of Bioengineering,

3. Department of Applied Mathematics

University of Washington, Seattle, WA

{jingyli6,lscholl,tle45,pavir,aorsborn,shlizee}@uw.edu

###### Abstract

Latent Variable Models (LVMs) propose to model the dynamics of neural populations by capturing low-dimensional structures that represent features involved in neural activity. Recent LVMs are based on deep learning methodology where a deep neural network is trained to reconstruct the same neural activity given as input and as a result to build the latent representation. Without taking past or future activity into account such a task is non-causal. In contrast, the task of forecasting neural activity based on given input extends the reconstruction task. LVMs that are trained on such a task could potentially capture temporal causality constraints within its latent representation. Forecasting has received less attention than reconstruction due to recording challenges such as limited neural measurements and trials. In this work, we address modeling neural population dynamics via the forecasting task and improve forecasting performance by including a prior, which consists of pairwise neural unit interaction as a multivariate dynamic system. Our proposed model--Additive, Multiplicative, and Adaptive Graph Neural Network (AMAG)--leverages additive and multiplicative message-passing operations analogous to the interactions in neuronal systems and adaptively learns the interaction among neural units to forecast their future activity. We demonstrate the advantage of AMAG compared to non-GNN based methods on synthetic data and multiple modalities of neural recordings (field potentials from penetrating electrodes or surface-level micro-electrocorticography) from four rhesus macaques. Our results show the ability of AMAG to recover ground truth spatial interactions and yield estimation for future dynamics of the neural population.

## 1 Introduction

The ability to encode, decode, and interpret neural activity could facilitate novel applications targeted toward neural activity, such as Brain-Computer Interfaces (BCIs) [15, 50, 52, 28, 49]. A promising methodology for neural activity encoding and decoding is the use of artificial Deep Neural Networks (DNN) [53, 84, 78]. These approaches encode neural activity into latent neural representations, facilitating decoding and interpretation of neural signals and correlating the activity with other modalities, e.g., brain-to-text translation [72].

Most existing DNN architectures, such as Latent Factor Analysis via Dynamical Systems (LFADS) and its variants, infer latent dynamics from single-trial population activities by encoding the neural time series into a summarized representation from which the original time series could be regenerated, a procedure known as reconstruction task [53, 65, 51]. While effective in capturing low-dimensional latent neural manifolds, the task does not consider causal aspects of neural dynamics. In contrast,the forecasting task, which aims to predict future activity based on past data, adhering to temporal causality constraints, could provide further insight into neural representations that are closer to representations employed by the brain [82; 55; 56; 18]. In addition, learning to forecast offers practical advantages since the signals that are predicted can help to reduce latency in real-time neural decoding applications (e.g., BCIs) or serve as a reference to detect real-time deviations of current recordings compared to earlier ones, indicating dynamic changes in the neuronal system or equipment malfunction [3].

Neural activity forecasting is inherently more challenging than reconstruction due to the requirement of a larger number of recorded neural signals, which are typically limited, and sensitivity to noise present in the signals. Incorporating priors into model structure design can be beneficial to ensure that instead of learning noisy information or overfitting the training data, the model learns features for forecasting future activity that are generalizable. A well-known prior is the spatial interaction among neurons, involving both addition [27; 9] and multiplication processes [19; 25] which could be modeled with Hodgkin-Huxley or Leaky Integrate-and-Fire [27; 1]. Previous works have utilized probabilistic graphical models to learn interactions [14; 42; 69; 36; 10] with a focus on capturing the neuron interaction itself rather than addressing downstream forecasting tasks. Models that learn the downstream task by considering spatial interaction have been proposed, such as Spatial Temporal Neural Data Transformer (STNDT) and Embedded Interaction Transformer (EIT), where the interaction between neurons is modeled using an attention mechanism [37; 44; 68]. Graph Neural Networks Networks (GNNs), designed to handle graph-structured data, are an alternative to the attention mechanism to model neuron interactions [75].

In this study, we propose a GNN with additive and multiplicative message-passing operations, called AMAG, for neuron signal forecasting. As shown in Fig. 1 (A), AMAG includes three modules: temporal encoding (TE), spatial interaction (SI), and temporal readout (TR). TE and TR model temporal dynamics for each recording channel independently. SI models the interaction between recording channels by leveraging the additive and multiplicative message-passing operations. We test the model on synthetic datasets and show the ability of AMAG to recover the ground truth interaction between channels. In addition, we demonstrate the effectiveness of the proposed model by testing it on two neural local field potential recording types (each with datasets from two animals). The first type is a publicly available dataset recorded by Utah array (penetrating probes) and the second type is a novel dataset of micro-electrocorticography (\(\mu\)ECoG) array recordings from the cortical surface. These four recordings are from the motor cortices of rhesus macaque monkeys performing reaching. We compare our model with other approaches, including GNN or non-GNN based. These computational evaluations demonstrate that AMAG outperforms other models regarding forecasting accuracy on neural recordings. We summarize the main contributions of our work below:

* _Modeling spatial-temporal relationship._ We present a graph neural network (GNN), called AMAG, that can forecast neuron activity by using sample-dependent additive and multiplicative message-passing operations with a learnable adjacency matrix.
* _Learning underlying spatial interaction._ We demonstrate the ability of AMAG to learn the underlying spatial interactions on synthetic datasets and show the importance of learning to forecast in discovering such interactions.

Figure 1: AMAG Overview. (A) The forecasting task and AMAG architecture with temporal Encoding (TE), Spatial Interaction (SI), and Temporal Readout (TR) modules to address the task. (B) Components of SI module.

* _Generating future neural signals with AMAG._ We apply AMAG to four neural recording datasets and demonstrate its reliability in generating future neural activity while recovering the channel spatial proximity in the learned adjacency matrix and aligning neural trajectories in the latent space.

## 2 Related work

**Modeling Dynamics of Neural Signals.** Studying the dynamics of neural signals provides insights into the computation performed in the brain [70]. Neural dynamics has been modeled by Gaussian Process [73; 79; 81] and linear dynamical systems [45; 46; 23; 41; 2]. More recently, a series of nonlinear models have been proposed, including Multi-Layer Perceptron (MLP) [43; 83; 7], RNNs [52; 34; 57; 32; 60], Neural ODEs [35], and Transformers [77; 37; 44; 47]. Latent representations of population dynamics are often extracted while these models optimize for a reconstruction objective, upon which interpretable features emerge naturally [54; 58; 4]. A recent model that could forecast future neural activities is Preferential Subspace IDentification algorithm (PSID)[56; 55]. PSID proposes to forecast future neural activity along with behavior variables in two stages. In the first stage, the model learns to predict behavior and the behaviorally-relevant portion of neural signals. In the second stage, PSID learns to forecast both behaviorally-relevant and behaviorally-irrelevant neural signals [56; 55].

**Time Series Forecasting.** Time series forecasting, i.e., the prediction of next values in a time series, is a well-studied topic that encompasses various applications, such as traffic flow forecasting, motion prediction, and electric load forecasting. Examples of proposed methods include methods that are CNN based [31; 8], RNN based [24; 13; 26], Multi-layer perceptron [80] and so on. Transformers with additional components for capturing long-term dependencies have also been proposed for time series forecasting [68; 16; 74]. Also, models such as Graph Neural Networks (GNNs) have been proposed for time series forecasting focusing on datasets with inherent structural organization, e.g., traffic flow [30; 77; 38]. For example, these methods have been applied to functional Magnetic Resonance Imaging (fMRI) forecasting [71] leveraging Diffusion Convolutional Recurrent Neural Network (DCRNN) and Graph WaveNet (GWNet) [76; 40]. In these applications, GNN has been embedded in GRU gating mechanism to learn the spatial-temporal relationship (DCRNN), or CNN and GNN have been proposed to be intertwined to perform spatial-temporal modeling (GWNet). In contrast, GraphS4mer, which combines graph neural networks and structured state space models, performs temporal embedding followed by spatial message-passing in a sequential manner [66]. These methods perform only the additive message-passing, which may not be sufficient for a complex system such as the brain.

**GNNs for Neural Signals.** In recent years, GNNs have been proposed as plausible models for analyzing brain activity across different modalities [62; 33; 12; 29]. In non-invasive neural recordings, GNNs have been applied to EEG and fMRI data to capture the relationships between different brain regions or features. For example, LGGNet learns local-global-graph representations of EEG for various cognitive classification tasks [17], and BrainGNN has been proposed to identify neurological biomarkers with GNN, leveraging the topological and functional information of fMRI [39]. Recently, the method has been extended to analyze ECoG recordings as a graph diffusion process [59]. A few applications of GNNs to invasive neural measurements have been introduced, although these are less ubiquitous than GNNs for other recording types. The Two-Stream GNN model, which has been designed to classify anesthetized states based on ECoG recording by building two fixed graphs, is a notable example of such a model [11]. The first graph in this model is the functional connectivity matrix computed from the phase lag index, and the second is the dual version of the first one. The Two-Stream GNN is not suitable for forecasting since it combines temporal signals into a unified vector as the model input.

In light of the related work, the proposed AMAG is a GNN based approach which explicitly models interactions between neural signals to learn the dynamic features and interactions that can generate future neural signals. This is different from earlier models of learning neural dynamics with the primary focus on reconstruction without explicit modeling neural signal interactions. AMAG incorporates novel additive and multiplicative message-passing operations, which distinguish it from methods focusing on time series forecasting to associate GNNs with neural dynamics.

Methods

Our objective is to develop a model that estimates future neural dynamics given past activities. We define the relationship between future and past as \(\hat{\mathbf{X}}_{t+1:t+\tau}=f_{\theta}(\mathbf{X}_{0:t})\), where \(\mathbf{X}\in\mathbb{R}^{T\times C\times D}\) represents neural signals recorded from \(C\) electrode channels over a time window of size \(T\), with each channel having \(D\) dimensional features. \(\mathbf{X}_{0:t}\) denotes neural signals from \(0\) to \(t\), and \(\hat{\mathbf{X}}_{t+1:t+\tau}\) corresponds to predicted future neural signals for time window \(t+1\) to \(t+\tau\).

To estimate \(f_{\theta}(\cdot)\), we formulate an optimization problem minimizing the loss \(\mathcal{L}\) between predicted future signals \(\hat{\mathbf{X}}_{t+1:t+\tau}\) and ground truth future signals \(\mathbf{X}_{t+1:t+\tau}\) as follows

\[\min_{f_{\theta}}\mathbb{E}_{\mathbf{X}}\bigg{[}\mathcal{L}(f_{\theta}\big{(}\mathbf{X }_{0:t}),\mathbf{X}_{t+1:t+\tau}\big{)}\bigg{]}. \tag{1}\]

Specifically, \(f_{\theta}(\cdot)\) is constructed from three sub-modules as shown in Fig. 1: Temporal Encoding (**TE**) Module, Spatial Interaction (**SI**) Module, and Temporal Readout (**TR**) Module. The TE module captures temporal features for each channel individually. Then SI facilitates information exchange between channels, followed by TR to generate future neural activities. We describe the three modules in detail in the sections below.

### SI with _Add_ and _Modulator_

SI is constructed as a GNN. We represent a Graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{A})\), where \(\mathcal{V}\) denotes a set of nodes \(|\mathcal{V}|=C\), \(\mathcal{E}\) is a set of edges, and weighted adjacency matrix \(\mathbf{A}\in\mathbb{R}^{C\times C}\) contains the weights of edges. The neighborhood of a node \(v\) is written as \(\mathbf{N}(v)\), where \(\mathbf{N}(v)=\{u|u\in V,(u,v)\in\mathcal{E}\}\). We denote neural signal from channel \(v\) at time \(t\) as \(X_{t}^{(v)}\). \(FC\), \(MLP\) and \(\sigma(\cdot)\) in the following sections represent a one-layer neural network, multi-layer perceptron, and sigmoid function, respectively.

Considering two types of interaction between neurons, additive [27] and multiplicative interactions [19; 25], we design the SI module to incorporate two sub-graphs for additive and multiplicative operations, namely Add and Modulator modules, respectively. In addition, we introduce a sample-dependent matrix, called Adaptor, to adjust the Add for each input sample, accounting for the complexity and dynamic nature of the brain as a system.

**Add Module**. The Add Module performs the additive interaction between channels with the message-passing function \(\mathcal{M}_{a}(\cdot)\), depending on the adjacency matrix \(\mathbf{A}_{a}\in\mathbb{R}^{C\times C}\). Assuming we have \(d\) dimensional channel features \(\mathbf{h}_{t}^{(v)}\) for node \(v\) at timestep \(t\) (\(\mathbf{h}_{t}^{(v)}\in\mathbb{R}^{d}\) ), \(\mathcal{M}_{a}(\cdot)\) updates \(\mathbf{h}_{t}^{(v)}\) with the additive message from neighbor node \(u\) as \(\mathcal{M}_{a}(\mathbf{h}_{t}^{(u)},\mathbf{h}_{t}^{(v)},\mathbf{A}_{a})=A_{a}^{(u,v)}\mathbf{ h}_{t}^{(u)}\), where \(u\in\mathbf{N}_{a}(v)\). Such that the updated channel feature of Add Module is the weighted feature of neighborhood,

\[\mathbf{a}_{t}^{(v)}=\sum_{u\in\mathbf{N}_{a}(v)}A_{a}^{(u,v)}\mathbf{h}_{t}^{(u)}. \tag{2}\]

The element \((u,v)\) in the adjacency matrix \(\mathbf{A}_{a}\) indicates the influence of channel \(u\) on \(v\). Since \(\mathbf{A}_{a}\) is shared for all the input sequences, while channel interaction can change across inputs, we introduce a sample-dependent _Adaptor_ Module. We treat \(\mathbf{A}_{a}\) as a fundamental adjacency matrix, and then we further learn a sample-dependent Adaptor as a matrix \(\mathbf{S}\in\mathbb{R}^{C\times C}\) to uniquely modulate \(\mathbf{A}_{a}\) depending on each sample. Then, the shared message-passing function Eq. 3 becomes a sample-dependent message-passing function,

\[\mathbf{a}_{t}^{(v)}=\sum_{u\in\mathbf{N}_{a}(v)}S^{(u,v)}A_{a}^{(u,v)}\mathbf{h}_{t}^{(u )}. \tag{3}\]

\(S^{(u,v)}\) represents interaction strength between channel \(u\) and \(v\) relying on temporal embeddings for each channel, e.g., \(\mathbf{H}^{(v)}=[\mathbf{h}_{1}^{(v)},\ldots,\mathbf{h}_{t}^{(v)}]\) where \(t\) is the context window for future neural signal generation. Specifically, we compute \(S^{(u,v)}\) as \(\sigma(MLP([\mathbf{H}^{(u)},\mathbf{H}^{(v)}]))\), such that \(S^{(u,v)}\) ranges from 0 to 1, similar to matrix construction for learning interpretable GNNs [48].

Since the fundamental adjacency matrix for the Add Module is unknown, \(\mathbf{A}_{a}\) needs to be learned during the optimization of the model parameters. We observe that direct learning of \(\mathbf{A}_{a}\) from random initialization could make the training process unstable and thus instead, we initialize the \(\mathbf{A}_{a}\) with the precomputed correlation matrix from the neural recordings to stabilize the learning process.

**Modulator Module**. The Modulator Module is the multiplicative message-passing operator, with function \(\mathcal{M}_{m}(\cdot)\). We incorporate the adjacency matrix \(\mathbf{A}_{m}\in\mathbf{R}^{C\times C}\) to encode the neighborhood information that performs a multiplicative operation. \(\mathcal{M}_{m}(\cdot)\) indicates how the feature of the neighborhood modulates the target node feature, which can be expressed as \(\mathcal{M}_{m}(\mathbf{h}_{t}^{(u)},\mathbf{h}_{t}^{(v)},\mathbf{A}_{m})=\mathbf{A}_{m}^{(u,v )}\mathbf{h}_{t}^{(u)}\odot\mathbf{h}_{t}^{(v)}\), where \(\odot\) represents Hadamard product. The Modulator Module includes all the multiplicative information from the neighbors to update feature for target channel \(u\) at time \(t\) as \(\mathbf{m}_{t}^{(u)}\)

\[\mathbf{m}_{t}^{(v)}=\sum_{u\in\mathbf{N}_{m}(v)}\mathcal{M}_{m}(\mathbf{h}_{t}^{(u)},\mathbf{h }_{t}^{(v)},\mathbf{A}_{m}) \tag{4}\]

Similarly to the adjacency matrix for the Add Module, the adjacency matrix in the Modulator Module is also trainable and is initialized from the correlation matrix.

In summary, at a specific timestep \(t\), the output of SI is

\[\mathbf{z}_{t}^{(v)}=\beta_{1}\mathbf{h}_{t}^{(v)}+\beta_{2}FC(\mathbf{a}_{t}^{(v)})+\beta_ {3}FC(\mathbf{m}_{t}^{(v)}), \tag{5}\]

with \(\beta_{1}\), \(\beta_{2}\), and \(\beta_{3}\) controlling the contributions of the self-connection, Add Module, and Modulator Module respectively.

### Temporal Processing with TE and TR Modules

TE and TR perform temporal encoding and decoding. In particular, TE embeds input neural signals \(\mathbf{X}^{(v)}\in\mathbb{R}^{T\times D}\) from channel \(v\) into the embedding space \(\mathbf{H}^{(v)}\in\mathbb{R}^{T\times d}\). For multi-step forecasting, we mask neural signals after the context window \(t\) with constant, i.e., \(\mathbf{X}_{t:T}^{(v)}=constant\). Then SI updates representation of channel \(v\) as \(\mathbf{Z}^{(v)}\in\mathbb{R}^{T\times d}\), followed by TR generating future neuron activity \(\hat{\mathbf{X}}_{t:T}^{(v)}\) taking \(\mathbf{Z}^{(v)}\) as inputs. TE and TR can be Transformer or GRU. For further details on TE and TR, see Appendix A.2.

**Parameter Learning**. In summary, AMAG estimates future neural signals with \(f_{\theta}\) implementing TE, SI, and TR sequentially to the past neural signals. The optimal parameters \(\theta\) are obtained through backpropagation minimizing the Mean Squared Error (MSE) loss,

\[\mathcal{L}=\mathbb{E}_{\mathbf{X}}\big{(}\big{\|}\hat{\mathbf{X}}_{t:T}-\mathbf{X}_{t:T} \big{\|}_{2}\big{)}. \tag{6}\]

## 4 Experiments

### Learning The Adjacency Matrix of a Synthetic Dataset

**Synthetic Dataset.** To explore the ability of AMAG to capture channel interactions, we generate synthetic datasets using linear non-Gaussian dynamic systems similar to previous works [61; 5; 6]. Specifically, with adjacency matrix \(\mathbf{A}\) describing the spatial interactions among variables (channels), datasets \(\mathbf{X}\) are generated recursively at each time step as \(X_{t}=X_{t-1}+\mathcal{G}(X_{t-1},\mathbf{A})+\mu\), where \(\mu\) is uniformly distributed noise term. We consider two scenarios where the connection ratio of the adjacency matrix is 0.2 and 0.5, respectively. The resulting datasets consist of multichannel time series, where the value of each channel at a given time step depends on its past and its neighboring channels, as defined by the adjacency matrix.

**Adjacency Matrix Recovery with AMAG.** We evaluate the ability of AMAG to recover the ground truth adjacency matrix and its dependency on tasks. We do so by additionally incorporating reconstruction and masked input reconstruction tasks as supplementary training objectives together with one-step and multi-step and forecasting tasks. The learned adjacency matrices, along with the ground truth matrix for 20 channels, are visualized in Fig.2 (Additional results with different numbers of channels in Appendix B.1). To assess the accuracy of recovering the underlying ground truth adjacency matrix, we employ the F1 Score as a quantitative measure. Our results show that the one-step forecasting task achieves the highest recovery of ground truth edges (\(F1=\mathbf{0.88}\) at 0.5 connection ratio), followed by the multi-step forecasting task (\(F1=\mathbf{0.82}\)). This is compared to the reconstruction task which yields a lower F1 score of \(\mathbf{0.63}\). The lower performance of reconstruction tasks could be due to the model's access to future signals, which potentially alleviates the need to learn the interactions with neighbors that future signals rely on. While multi-step forecasting generatesmultiple future signals, some adjacency matrices may generate more accurate signals than others at a given moment, but considering all future moments collectively, different adjacency matrices may achieve similar performance, resulting in learned adjacency matrices being less accurate than one-step forecasting.

### Evaluation on Forecasting with Neural Signals

**Dataset and Preprocessing** In this study, we collect neural signals from monkey Affogato (A), and Beignet (B) with \(\mu\)ECoG implanted in motor cortices, covering five different subregions (Primary Motor Cortex (M1), Premotor Cortex, Frontal Eye Field (FEF), Supplementary Motor Area (SMA), and Dorsolateral Prefrontal Cortex (DLPFC)) with 239 effective \(\mu\)ECoG electrodes arrays which have been used in [67]. During data collection, the subjects performed a reaching task towards one of eight different directions. All procedures were approved by the University of Washington Institutional Animal Care and Use Committee. In the following experiments, we include all electrodes from Monkey A and electrodes in M1 from Monkey B (More in the Appendix B.2). We additionally apply AMAG on two public datasets with field potential recorded with Utah arrays from the M1 of Monkey Mihili (M) and Chewie (C) where subjects performed similar hand-reaching task [22; 21]. M and C datasets contain the computed Local Motor Potential (LMP) and power band signal from eight frequency bands: 0.5-4 Hz, 4-8 Hz, 8-12 Hz, 12-25 Hz, 25-50 Hz, 50-100 Hz, 100-200 Hz, and 200-400 Hz. The resulting signal is downsampled to 30ms. Full broadband data from A and B were collected at 25 kHz. We then applied a preprocessing procedure to extract LMP and powerband signals from A and B datasets as used in M and C datasets [20; 64]. The powerband signals are included as additional feature dimensions. In experiments, we treat a target reaching trial as a sample for all four datasets. On M and C datasets, trials are temporally aligned to detect movement onset and include 300 ms before to 450 ms after movement onset. For the A and B datasets, trials are temporally aligned to the appearance of the peripheral target and include 600ms after the peripheral target onset.

**Benchmark Methods,** We compare AMAG with existing approaches, including variants of RNNs, Transformers, and GNNs. Specifically, we compare with the following methods: _LRNN_, RNN model without a non-linear activation function and gating mechanism [63]; _RNNf_, RNN model with non-linear activation and gating mechanism [32; 47]; _LFADS_, RNN based model with pre-computed initial states [65]. _NDT_, a decoder based transformer in temporal domain to estimate neural firing rate [78]; _STNDT_, a model that is similar to NDT with additional attention based spatial interaction between neurons [37]; _TERN_, a model that includes the GRU for encoding and temporal Transformer for decoding [32; 47]. _RNN PSID_, RNN based model with 2-stage training for learning behavior-related neural space and behavior-unrelated neural space combined to ensure accurate

Figure 2: Colormap visualization of AMAG learned adjacency matrices arranged by F1 score (right better), with yellow indicating Ground Truth recovery while gree-blue indicating lower accuracy, as reconstruction task (first column), reconstruction of masked input task (second column), multi-step forecasting (third column) and one-step forecasting task (fourth column), alongside ground truth adjacency matrices generating synthetic data (last column), demonstrating AMAGâ€™s ability to recover the underlying spatial interactions doing forecasting tasks.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

**Investigating Adjacency Matrix Learned by AMAG.** We hypothesize that the learned adjacency matrix in AMAG indicates interaction strength between channels and channels with high interaction strength are vital in forecasting. To verify the assumption, we group the channels into High (H), Mid-high (M-H), Mid-low (M-L), and Low (L) according to connection strength in \(Aa\). We estimate their importance as the degree of performance drop when channels in each group are masked in AMAG. As the testing of the generality of the channel importance, we performed the same procedure in NDT. The forecasting performance (\(R^{2}\)) of NDT and AMAG with masked channels as input are shown in Fig. 4 (A). Generally, masking of channels consistently leads to performance drop, particularly with more significant degradation of performance in both AMAG and NDT when masking happens in the H group. This confirms our assumption that the weights learned in adjacency matrices provide informative insights into channel importance. Additionally, we investigate the relationship between learned weights and channel proximity. Figure 4 (B) illustrates the projection of the interaction strength of a target channel (T) onto other channels in the space organized by the \(\mu\)ECoG arrangement. We observe that target channels exhibit stronger interactions with surrounding channels, which aligns with the properties of \(\mu\)ECoG arrays. These experiments collectively demonstrate that the learned adjacency matrix values are meaningful in terms of identifying important channels, and the learned strength of connections aligns with the spatial proximity among ECoG arrays.

**Neuron Trajectory Alignment.** Apart from learning meaningful adjacency matrices, it is also advantageous to study neural trajectories AMAG has learned during the forecasting task. For that purpose, we project the original neural signals and their hidden embeddings from AMAG into the 2D space with PCA in Fig. 4(C). We find that, while neural trajectories, projected from the original neural signals, are tangled with random start and endpoints, with the projection from the hidden embeddings of AMAG, the neural trajectories are disentangled for the corresponding time-step and exhibit a circular structure similar to [53]. This illustration indicates that AMAG learns informative features that can be visualized as coherent and interpretable neural trajectories in 2D space.

**Computation Complexity.** We additionally compare in Table 3 the number of parameters \(\#\)P(M), training time per epoch T(S), and maximum memory cost during training M(GB). We observe that GNN based methods use much fewer parameters than non-GNN methods, with AMAG consistently using a minimal or comparable (DCRNN for one-step forecasting) number of parameters when compared to graph model baselines (DCRNN, GWNet, GS4-G). This demonstrates that the source of the contribution to AMAG performance is its architecture rather than the number of parameters. The graph architecture of AMAG provides the topological constraint across channels. The absence of the topological constraint in non-GNN based methods could result in overfitting with limited training samples. Adding regularization terms, e.g., weight decay could help, but will also limit models' capacity. We demonstrate it in Fig. 3 (B). Adding weight decay to NDT improves performance (orange triangle vs purple circle), however, as the attention dimension increases, the effect of regularization diminishes, and for dim>1024, weight decay does not contribute to further improvement.

### Ablation Study

Here we examine how each component in AMAG contributes to forecasting. Specifically, we consider the following ablated versions of AMAG: (1) removing self-connection in AMAG (amAG), (2) removing both the "Add" and "Modulator" module (-AG), (3) Only sample dependent "Adaptor" is kept in "Add" Module (\(\bar{a}\)MAG), (4) Adjacency matrix is not learnable (AM-G), (5) The "Add" is ablated from the graph operation (-MAG), (6) "Modulator" is removed (A-AG), (7) The "Adaptor" in excluded from the "Add" (aMAG). The results are shown in Table.4.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & **AMAG** & LRNN & LFADS & STNDT & NDT & RNNf & TERN & DCRNN & GWNet & GS4-G \\ \hline \(\#\)P(M) & **0.27** & 4.59 & 5.53 & 2.38 & 1.06 & 3.55 & 9.93 & 0.60 & 1.12 & 1.52 \\ T(S) & **9.74** & **2.18** & **3.44** & **4.70** & **5.96** & **7.22** & **8.48** & 11.01 & 12.27 & 13.53 \\ M(GB) & **5.74** & **0.12** & **0.15** & **0.19** & **0.04** & **0.11** & **0.34** & **1.69** & **1.53** & **4.48** \\ \hline \multicolumn{11}{c}{One-step Forecasting} \\ \(\#\)P(M) & **0.22** & 1.25 & 5.84 & 8.31 & 3.70 & 3.55 & 9.93 & **0.15** & 1.12 & N/A \\ T(S) & **9.90** & **1.97** & **3.29** & **4.61** & **5.94** & **7.26** & **8.58** & 11.22 & 12.54 & N/A \\ M(GB) & **5.36** & **0.04** & **0.15** & **0.43** & **0.10** & **0.11** & **0.34** & **1.05** & **1.53** & N/A \\ \hline \hline \end{tabular}
\end{table}
Table 3: _**_Computation complexity estimation for multi-step and one-step forecasting task. Bold marks AMAG estimates and those methods whose estimates are better than AMAG._**We find that removing _Add_ or _Modulator_ modules can cause the most significant forecasting accuracy drop (-AG) as well as the self-connection part in the graph (amAG). When keeping sample-dependent Adaptor term only (\(\bar{a}\)MAG) in the Add module, the performance is very close to ablating the Add module (-MAG). The sample-dependent Adaptor term appears to be less important compared to other components but still reduces the accuracy if being ablated from AMAG (aMAG).

In addition, we investigate the effect of initialization (random versus correlation) when the adjacency matrices are not adaptable (AM-G), specifically, AM-G (Rand Init 1), AM-G (Rand Init 2), AM-G (Corr Init). The forecasting accuracy of AM-G compared to the other two variants demonstrates the importance of correlation initialization. In particular, random and correlation initialization achieve similar performance when the adjacency matrices are adaptable, while random initialization leads to a less stable training process, see Appendix B.4. In Fig. 5, we visualized the learned adjacency matrix with correlation initialization (top) and random initialization (bottom) for one-step (left) and multi-step forecasting (right). Globally, the learned adjacency matrices show similar connection patterns, while locally, the connection can be different with different initialization matrices. This indicates that regardless of the initialization method, AMAG can converge to graph patterns with globally similar connections while multiple local connections are available to achieve similar forecasting performance.

## 5 Discussion

In conclusion, we propose AMAG for modeling spatial-temporal interaction between neural signals via forecasting future neuron activity. Experiments with both synthetic and neural signals show potential for discovering underlying neural signal interactions. The learned neural features by AMAG appear to disentangle in the PCA space. While adjacency matrix recovery is promising as training AMAG with one-step forecasting, more animal experiments could be done together with AMAG to understand the biological meaning of the learned adjacency matrix. It is worth noting that the memory requirement for GNN training increases as the number of neural recording channels increases. For future development, improvements to adapt the model for multi-region, multi-channel recordings could be considered.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(R^{2}\uparrow\) & \(CORR\uparrow\) & \(MSE\downarrow\) \\ -AG & 0.424\(\pm\)1e-3 & 0.650\(\pm\)1e-3 & 0.0456\(\pm\)1e-4 \\ amAG & 0.427\(\pm\)6e-3 & 0.652\(\pm\)4e-3 & 0.0453\(\pm\)5e-4 \\ -MAG & 0.647\(\pm\)2e-3 & 0.805\(\pm\)6e-4 & 0.0274\(\pm\)2e-4 \\ \(\bar{a}\)MAG & 0.648\(\pm\)1e-3 & 0.806\(\pm\)4e-4 & 0.0273\(\pm\)6e-5 \\ -A-AG & 0.652\(\pm\)9e-4 & 0.807\(\pm\)3e-4 & 0.0268\(\pm\)1e-4 \\ aMAG & 0.655\(\pm\)2e-3 & 0.810\(\pm\)1e-3 & 0.0269\(\pm\)2e-4 \\ \hline AM-G Rand Init) & 0.575\(\pm\)2e-2 & 0.767\(\pm\)7e-3 & 0.0329\(\pm\)1e-3 \\ AM-G (Corr Init) & 0.617\(\pm\)2e-4 & 0.786\(\pm\)7e-4 & 0.0296\(\pm\)2e-5 \\ \hline AMAG (Rand Init) & 0.652\(\pm\)1e-3 & 0.807\(\pm\)8e-4 & 0.0270\(\pm\)1e-4 \\ AMAG (Corr Init) & **0.657\(\pm\)2e-3** & **0.811\(\pm\)2e-3** & **0.0266\(\pm\)2e-4** \\ \hline \hline \end{tabular}
\end{table}
Table 4: _Comparison between the Ablated versions of AMAG._

Figure 5: Visualization of the adjacency matrix of Add module (\(A_{a}\)) and Modulator module (\(A_{m}\)) learned by AMAG performing one-step forecasting (left) and multi-step forecasting (right) with correlation initialization (top) and random initialization (bottom).

Acknowledgements

Authors acknowledge the support in part by A3D3 National Science Foundation grant OAC-2117997 (ES,JL,TL,ALO), the Simons Foundation (LS,PR,ALO), the Washington Research Foundation Fund (ES), Departments of Electrical Computer Engineering (ES,JL,TL), Applied Mathematics (ES). Authors are thankful to the Center of Computational Neuroscience, and the eScience Center at the University of Washington. Authors would like to thank Rahul Biswas for discussions of synthetic dataset generation, Pan Li for discussions regarding GNNs, and the anonymous reviewers for insightful feedback.

## 7 Supplementary Information and Data

Supplementary information is available in the Appendix. Additional information and data will be made available through the project page [https://github.com/shlizee/AMAG](https://github.com/shlizee/AMAG).

## References

* [1] Larry F Abbott. "Lapicque's introduction of the integrate-and-fire model neuron (1907)". In: _Brain research bulletin_ 50.5-6 (1999), pp. 303-304.
* [2] Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. "Black box variational inference for state space models". In: _arXiv preprint arXiv:1511.07367_ (2015).
* [3] Navid Ayoobi and Elnaz Banan Sadeghian. "A self-paced BCI system with low latency for motor imagery onset detection based on time series prediction paradigm". In: _arXiv preprint arXiv:2204.05450_ (2022).
* [4] Edoardo Balzani, Jean Paul Noel, Pedro Herrero-Vidal, Dora E Angelaki, and Cristina Savin. "A probabilistic framework for task-aligned intra-and inter-area neural manifold estimation". In: _arXiv preprint arXiv:2209.02816_ (2022).
* [5] Rahul Biswas and Eli Shlizerman. "Statistical perspective on functional and causal neural connectomics: A comparative study". In: _Frontiers in Systems Neuroscience_ 16 (2022), p. 817962.
* [6] Rahul Biswas and Eli Shlizerman. "Statistical perspective on functional and causal neural connectomics: The Time-Aware PC algorithm". In: _PLOS Computational Biology_ 18.11 (2022), e1010653.
* [7] Martin Bjerke, Lukas Schott, Kristopher T Jensen, Claudia Battistin, David A Klindt, and Benjamin A Dunn. "Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles". In: _arXiv preprint arXiv:2210.03155_ (2022).
* [8] Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. "Conditional time series forecasting with convolutional neural networks". In: _arXiv preprint arXiv:1703.04691_ (2017).
* [9] Nicolas Brunel. "Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons". In: _Journal of computational neuroscience_ 8 (2000), pp. 183-208.
* [10] Luis Carrillo-Reid, Shuting Han, Darik O'Neil, Ekaterina Taralova, Tony Jebara, and Rafael Yuste. "Identification of pattern completion neurons in neuronal ensembles using probabilistic graphical models". In: _Journal of Neuroscience_ 41.41 (2021), pp. 8577-8588.
* [11] Kun Chen, Tianli Xie, Li Ma, Andrew E Hudson, Qingsong Ai, and Quan Liu. "A Two-Stream Graph Convolutional Network Based on Brain Connectivity for Anesthetized States Analysis". In: _IEEE Transactions on Neural Systems and Rehabilitation Engineering_ 30 (2022), pp. 2077-2087.
* [12] Yuquian Chen, Fan Zhang, Leo R Zekelman, Tengfei Xue, Chaoyi Zhang, Yang Song, Nikos Makris, Yogesh Rathi, Weidong Cai, and Lauren J O'Donnell. "TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography". In: _arXiv preprint arXiv:2301.01911_ (2023).
* [13] Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. "On the properties of neural machine translation: Encoder-decoder approaches". In: _arXiv preprint arXiv:1409.1259_ (2014).
* [14] Rainer Dahlhaus. "Graphical interaction models for multivariate time series". In: _Metrika_ 51 (2000), pp. 157-172.

* [15] Janis J Daly and Jonathan R Wolpaw. "Brain-computer interfaces in neurological rehabilitation". In: _The Lancet Neurology_ 7.11 (2008), pp. 1032-1043.
* [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding". In: _arXiv preprint arXiv:1810.04805_ (2018).
* [17] Yi Ding, Neethu Robinson, Chengxuan Tong, Qiuhao Zeng, and Cuntai Guan. "LGGNet: Learning from local-global-graph representations for brain-computer interface". In: _IEEE Transactions on Neural Networks and Learning Systems_ (2023).
* [18] Matthew Dowling, Yuan Zhao, and Il Memming Park. "Real-time variational method for learning neural trajectory and its dynamics". In: _The Eleventh International Conference on Learning Representations_.
* [19] Katie A Ferguson and Jessica A Cardin. "Mechanisms underlying gain modulation in the cortex". In: _Nature Reviews Neuroscience_ 21.2 (2020), pp. 80-92.
* [20] Robert D Flint, Christian Ethier, Emily R Oby, Lee E Miller, and Marc W Slutzky. "Local field potentials allow accurate decoding of muscle activity". In: _Journal of neurophysiology_ 108.1 (2012), pp. 18-24.
* [21] Cecilia Gallego-Carracedo, Matthew G Perich, Raeed H Chowdhury, Lee E Miller, and Juan Alvaro Gallego. "Dataset from "Local field potentials reflect cortical population dynamics in a region-specific and frequency-dependent manner"". In: (). doi: 10.5061/dryad.xd2547dkt.url: [https://doi.org/10.5061/dryad.xd2547dkt](https://doi.org/10.5061/dryad.xd2547dkt).
* [22] Cecilia Gallego-Carracedo, Matthew G Perich, Raeed H Chowdhury, Lee E Miller, and Juan Alvaro Gallego. "Local field potentials reflect cortical population dynamics in a region-specific and frequency-dependent manner". In: _Elife_ 11 (2022), e73155.
* [23] Yuanjun Gao, Evan W Archer, Liam Paninski, and John P Cunningham. "Linear dynamical neural population models through nonlinear embeddings". In: _Advances in neural information processing systems_ 29 (2016).
* [24] Felix A Gers, Nicol N Schraudolph, and Jurgen Schmidhuber. "Learning precise timing with LSTM recurrent networks". In: _Journal of machine learning research_ 3.Aug (2002), pp. 115-143.
* [25] Lukas N Groschner, Jonatan G Malis, Birte Zuidinga, and Alexander Borst. "A biophysical account of multiplication by a single neuron". In: _Nature_ 603.7899 (2022), pp. 119-123.
* [26] Sepp Hochreiter and Jurgen Schmidhuber. "Long short-term memory". In: _Neural computation_ 9.8 (1997), pp. 1735-1780.
* [27] Alan L Hodgkin and Andrew F Huxley. "A quantitative description of membrane current and its application to conduction and excitation in nerve". In: _The Journal of physiology_ 117.4 (1952), p. 500.
* [28] Kejia Hu, Mohsen Jamali, Ziev B Moses, Carlos A Ortega, Gabriel N Friedman, Wendong Xu, and Ziv M Williams. "Decoding unconstrained arm movements in primates using high-density electrocorticography signals for brain-machine interface use". In: _Scientific reports_ 8.1 (2018), pp. 1-13.
* [29] Yicong Huang and Zhuliang Yu. "Representation Learning for Dynamic Functional Connectivities via Variational Dynamic Graph Latent Variable Models". In: _Entropy_ 24.2 (2022), p. 152.
* [30] Weiwei Jiang, Jiayun Luo, Miao He, and Weixi Gu. "Graph Neural Network for Traffic Forecasting: The Research Progress". In: _ISPRS International Journal of Geo-Information_ 12.3 (2023), p. 100.
* [31] Weiwei Jiang and Lin Zhang. "Geospatial data to images: A deep-learning framework for traffic forecasting". In: _Tsinghua Science and Technology_ 24.1 (2018), pp. 52-64.
* [32] Bryan Jimenez. "Neural Network Models For Neurophysiology Data". PhD thesis. Purdue University Graduate School, 2022.
* [33] Ming Jin, Enwei Zhu, Changde Du, Huiguang He, and Jinpeng Li. "PGCN: Pyramidal Graph Convolutional Network for EEG Emotion Recognition". In: _arXiv preprint arXiv:2302.02520_ (2023).

* [34] Mohammad Reza Keshtkaran, Andrew R Sedler, Raeed H Chowdhury, Raghav Tandon, Diya Basrai, Sarah L Nguyen, Hansem Sohn, Mehrdad Jazayeri, Lee E Miller, and Chethan Pandarinath. "A large-scale neural network training framework for generalized estimation of single-trial population dynamics". In: _bioRxiv_ (2021).
* [35] Timothy D Kim, Thomas Z Luo, Jonathan W Pillow, and Carlos Brody. "Inferring latent dynamics underlying neural population activity via neural differential equations". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 5551-5561.
* [36] Natalie Klein, Josue Orellana, Scott L Brincat, Earl K Miller, and Robert E Kass. "Torus graphs for multivariate phase coupling analysis". In: (2020).
* [37] Trung Le and Eli Shlizerman. "STNDT: Modeling Neural Population Activity with a Spatiotemporal Transformer". In: _arXiv preprint arXiv:2206.04727_ (2022).
* [38] Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, and Qi Tian. "Multiscale spatio-temporal graph neural networks for 3d skeleton-based motion prediction". In: _IEEE Transactions on Image Processing_ 30 (2021), pp. 7760-7775.
* [39] Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. "Brainingn: Interpretable brain graph neural network for fmri analysis". In: _Medical Image Analysis_ 74 (2021), p. 102233.
* [40] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting". In: _arXiv preprint arXiv:1707.01926_ (2017).
* [41] Scott W Linderman, Andrew C Miller, Ryan P Adams, David M Blei, Liam Paninski, and Matthew J Johnson. "Recurrent switching linear dynamical systems". In: _arXiv preprint arXiv:1610.08466_ (2016).
* [42] Hexuan Liu, Jimin Kim, and Eli Shlizerman. "Functional connectomics from neural dynamics: probabilistic graphical models for neuronal network of Caenorhabditis elegans". In: _Philosophical Transactions of the Royal Society B: Biological Sciences_ 373.1758 (2018), p. 20170377.
* [43] Ran Liu, Mehdi Azabou, Max Dabagia, Chi-Heng Lin, Mohammad Gheshlaghi Azar, Keith Hengen, Michal Valko, and Eva Dyer. "Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity". In: _Advances in Neural Information Processing Systems_ 34 (2021).
* [44] Ran Liu, Mehdi Azabou, Max Dabagia, Jingyun Xiao, and Eva Dyer. "Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 2377-2391.
* [45] Jakob H Macke, Lars Buesing, John P Cunningham, Byron M Yu, Krishna V Shenoy, and Maneesh Sahani. "Empirical models of spiking in neural populations". In: _Advances in neural information processing systems_ 24 (2011).
* [46] Jakob H Macke, Lars Buesing, and Maneesh Sahani. "Estimating state and parameters in state space models of spike trains". In: _Advanced state space methods for neural and clinical data_ 137 (2015).
* [47] Ganga Meghanath, Bryan Jimenez, and Joseph G Makin. "Inferring Population Dynamics in Macaque Cortex". In: _arXiv preprint arXiv:2304.06040_ (2023).
* [48] Siqi Miao, Mia Liu, and Pan Li. "Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism". In: _International Conference on Machine Learning_ (2022).
* [49] Kai J Miller, Dora Hermes, and Nathan P Staff. "The current state of electrocorticography-based brain-computer interfaces". In: _Neurosurgical focus_ 49.1 (2020), E2.
* [50] Luis Fernando Nicolas-Alonso and Jaime Gomez-Gil. "Brain computer interfaces, a review". In: _sensors_ 12.2 (2012), pp. 1211-1279.
* [51] Michael Nolan, Bijan Pesaran, Eli Shlizerman, and Amy L Orsborn. "Multi-block RNN Autoencoders Enable Broadband ECoG Signal Reconstruction". In: _bioRxiv_ (2022), pp. 2022-09.
* [52] Chethan Pandarinath, Paul Nuyujukian, Christine H Blabe, Brittany L Sorice, Jad Saab, Francis R Willett, Leigh R Hochberg, Krishna V Shenoy, and Jaimie M Henderson. "High performance communication by people with paralysis using an intracortical brain-computer interface". In: _Elife_ 6 (2017), e18554.

* [53] Chethan Pandarinath, Daniel J O'Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. "Inferring single-trial neural population dynamics using sequential auto-encoders". In: _Nature methods_ 15.10 (2018), pp. 805-815.
* [54] Felix Pei, Joel Ye, David Zoltowski, Anqi Wu, Raeed H Chowdhury, Hansem Sohn, Joseph E O'Doherty, Krishna V Shenoy, Matthew T Kaufman, Mark Churchland, et al. "Neural Latents Benchmark"21: evaluating latent variable models of neural population activity". In: _arXiv preprint arXiv:2109.04463_ (2021).
* [55] Omid G Sani, Hamidreza Abbaspourazad, Yan T Wong, Bijan Pesaran, and Maryam M Shanechi. "Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification". In: _Nature Neuroscience_ 24.1 (2021), pp. 140-149.
* [56] Omid G Sani, Bijan Pesaran, and Maryam M Shanechi. "Where is all the nonlinearity: flexible nonlinear modeling of behaviorally relevant neural dynamics using recurrent neural networks". In: _bioRxiv_ (2021).
* [57] Marine Schimel, Ta-Chu Kao, Kristopher T Jensen, and Guillaume Hennequin. "iLQR-VAE: control-based learning of input-driven dynamics with applications to neural data". In: _bioRxiv_ (2021).
* [58] Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. "Learnable latent embeddings for joint behavioural and neural analysis". In: _Nature_ (2023), pp. 1-9.
* [59] Felix Schwock, Julien Bloch, Les Atlas, Shima Abadi, and Azadeh Yazdan-Shahmorad. "Estimating and Analyzing Neural Information flow using Signal Processing on Graphs". In: _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE. 2023, pp. 1-5.
* [60] Qi She and Anqi Wu. "Neural dynamics discovery via gaussian process recurrent neural networks". In: _Uncertainty in Artificial Intelligence_. PMLR. 2020, pp. 454-464.
* [61] Le Song, Mladen Kolar, and Eric Xing. "Time-varying dynamic bayesian networks". In: _Advances in neural information processing systems_ 22 (2009).
* [62] Tengfei Song, Wenming Zheng, Peng Song, and Zhen Cui. "EEG emotion recognition using dynamical graph convolutional neural networks". In: _IEEE Transactions on Affective Computing_ 11.3 (2018), pp. 532-541.
* [63] Frieder Stolzenburg, Sandra Litz, Olivia Michael, and Oliver Obst. "The Power of Linear Recurrent Neural Networks". In: _arXiv preprint arXiv:1802.03308_ (2018).
* [64] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. "A neural network that finds a naturalistic solution for the production of muscle activity". In: _Nature neuroscience_ 18.7 (2015), pp. 1025-1033.
* [65] David Sussillo, Rafal Jozefowicz, LF Abbott, and Chethan Pandarinath. "Lfads-latent factor analysis via dynamical systems". In: _arXiv preprint arXiv:1608.06315_ (2016).
* [66] Siyi Tang, Jared A Dunnmon, Liangqiong Qu, Khaled K Saab, Christopher Lee-Messer, and Daniel L Rubin. "Spatiotemporal Modeling of Multivariate Signals With Graph Neural Networks and Structured State Space Models". In: _arXiv preprint arXiv:2211.11176_ (2022).
* [67] Michael Trumpis, Chia-Han Chiang, Amy L Orsborn, Brinnae Bent, Jinghua Li, John A Rogers, Bijan Pesaran, Gregory Cogan, and Jonathan Viventi. "Sufficient sampling for kriging prediction of cortical potential in rat, monkey, and human \(\mu\)ECoG". In: _Journal of neural engineering_ 18.3 (2021), p. 036011.
* [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. "Attention is all you need". In: _Advances in neural information processing systems_ 30 (2017).
* [69] Giuseppe Vinci, Valerie Ventura, Matthew A Smith, and Robert E Kass. "Adjusted regularization in latent graphical models: Application to multiple-neuron spike count data". In: _The annals of applied statistics_ 12.2 (2018), p. 1068.
* [70] Saurabh Vyas, Matthew D Golub, David Sussillo, and Krishna V Shenoy. "Computation through neural population dynamics". In: _Annual review of neuroscience_ 43 (2020), pp. 249-275.
* [71] Simon Wein, Alina Schuller, Ana Maria Tome, Wilhelm M Malloni, Mark W Greenlee, and Elmar W Lang. "Forecasting brain activity based on models of spatiotemporal brain dynamics: A comparison of graph neural network architectures". In: _Network Neuroscience_ 6.3 (2022), pp. 665-701.

* [72] Francis R Willett, Donald T Avansino, Leigh R Hochberg, Jaimie M Henderson, and Krishna V Shenoy. "High-performance brain-to-text communication via handwriting". In: _Nature_ 593.7858 (2021), pp. 249-254.
* [73] Anqi Wu, Nicholas A Roy, Stephen Keeley, and Jonathan W Pillow. "Gaussian process based nonlinear latent structure discovery in multivariate spike train data". In: _Advances in neural information processing systems_ 30 (2017).
* [74] Neo Wu, Bradley Green, Xue Ben, and Shawn O'Banion. "Deep transformer models for time series forecasting: The influenza prevalence case". In: _arXiv preprint arXiv:2001.08317_ (2020).
* [75] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. "A comprehensive survey on graph neural networks". In: _IEEE transactions on neural networks and learning systems_ 32.1 (2020), pp. 4-24.
* [76] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. "Graph wavenet for deep spatial-temporal graph modeling". In: _arXiv preprint arXiv:1906.00121_ (2019).
* [77] Jiexia Ye, Juanjuan Zhao, Kejiang Ye, and Chengzhong Xu. "How to build a graph-based deep learning architecture in traffic domain: A survey". In: _IEEE Transactions on Intelligent Transportation Systems_ 23.5 (2020), pp. 3904-3924.
* [78] Joel Ye and Chethan Pandarinath. "Representation learning for neural population activity with Neural Data Transformers". en. In: _Neurons, Behavior, Data analysis, and Theory_ (Aug. 2021). issn: 2690-2664. doi: 10.51628/001c.27358. url: [https://nbdt.scholasticaho.com/article/27358-representation-learning-for-neural-population-activity-with-neural-data-transformers](https://nbdt.scholasticaho.com/article/27358-representation-learning-for-neural-population-activity-with-neural-data-transformers) (visited on 08/29/2021).
* [79] Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity". In: _Advances in neural information processing systems_ 21 (2008).
* [80] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. "Are transformers effective for time series forecasting?" In: _arXiv preprint arXiv:2205.13504_ (2022).
* [81] Yuan Zhao and Il Memming Park. "Variational latent gaussian process for recovering single-trial dynamics from population spike trains". In: _Neural computation_ 29.5 (2017), pp. 1293-1316.
* [82] Yuan Zhao and Il Memming Park. "Variational online learning of neural dynamics". In: _Frontiers in computational neuroscience_ 14 (2020), p. 71.
* [83] Ding Zhou and Xue-Xin Wei. "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 7234-7247.
* [84] Feng Zhu, Andrew Sedler, Harrison A Grier, Nauman Ahad, Mark Davenport, Matthew Kaufman, Andrea Giovannucci, and Chethan Pandarinath. "Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 2331-2345.

## Appendix A Methods Details

In the following, we provide additional details regarding the initialization of the adjacency matrix, and temporal modeling (TE and TR components of AMAG).

Across the paper, we define the input to the model as \(\mathbf{X}\in\mathbb{R}^{T\times C\times D}\), where \(T\) denotes the total number of time steps, \(C\) represents the number of channels in the neural recordings, and \(D\) signifies the feature dimension. To refer specifically to the channel \(v\) or the time step \(t\) of input neural signals, we use the notations \(\mathbf{X}^{(v)}\in\mathbb{R}^{T\times D}\) or \(\mathbf{X}_{t}\in\mathbb{R}^{C\times D}\), respectively. To distinguish different data structures, matrices are denoted by bold capital letters, such as \(\mathbf{X}\), \(\mathbf{X}^{(v)}\), \(\mathbf{X}_{t}\), while unbolded capital letters are used for vectors and scalars (e.g., \(X_{t}^{(v)}\in\mathbb{R}^{D}\)). \(FC\) denotes a single-layer fully connected neural network, \(MLP\) denotes a multi-layer fully connected neural network, and \(LN\) denotes the layer normalization technique. \(\mathbf{A}_{a}\) and \(\mathbf{A}_{m}\) are the adjacency matrices in Add and Modulator modules.

### Initialization of Adjacency Matrix

The adjacency matrices in AMAG are learnable parameters. We find that learning the adjacency matrix from scratch can be unstable. Hence we propose to initialize the adjacency matrix based on the correlation of each channel to stabilize the training. We flatten the features of channel \(v\) and \(u\), such that \(\mathbf{X}^{(u)},\,\mathbf{X}^{(v)}\in\mathbb{R}^{TD}\). Then the correlation between channel \(u\) and channel \(v\) is

\[Corr(u,v)=\frac{\mathbf{X}^{(u)}\mathbf{X}^{(v)T}}{\sqrt{|\mathbf{X}^{(u)}||\mathbf{X}^{(v)}|}}.\]

We then initialize each element in the adjacency matrices \(A_{a}^{(u,v)}\) and \(A_{m}^{(u,v)}\) as \(Corr(u,v)\).

### Recursive Time Series Forecasting with TE and TR

Since Recurrent Neural Networks (RNN) are common in modeling continuous time-series signals, we employ a variant of RNN, Gated Recurrent Unit (GRU) [13], to facilitate the temporal encoding (TE) and readout (TR) modules. The TE module consists of a single-layer GRU that takes input from channel \(v\) and computes the corresponding temporal embedding features \(\mathbf{h}_{t}^{(v)}\in\mathbb{R}^{d}\) for each step \(t\). These embedding features capture the underlying temporal dynamics of neural activity. To enable the exchange of information between the target channel \(v\) and its neighboring channels \(\mathbf{N}(v)\), specified by the adjacency matrices \(\mathbf{A}_{a}\) and \(\mathbf{A}_{m}\), the spatial interaction (SI) module updates the embedding features. This interaction among channels allows for modeling of spatial dependencies.

The TR module consists of a single-layer GRU and a fully connected (\(FC\)) layer. The module generates future neural signals. This pipeline of TE, SI and TR modules is shown below

\[\mathbf{h}_{t}^{(v)} =GRU_{TE}(\mathbf{X}_{t}^{(v)},\mathbf{h}_{t-1}^{(v)}),\,\,\,\mathbf{h}_{0}^{ (v)}=\mathbf{0}\] \[\mathbf{z}_{t}^{(v)} =SI(\mathbf{h}_{t}^{(v)},\{\mathbf{h}_{t}^{(u)}|u\in\mathbf{N}(v)\},\mathbf{A}_{a },\mathbf{A}_{m})\] \[\mathbf{r}_{t}^{(v)} =GRU_{TR}(\mathbf{z}_{t}^{(v)},\mathbf{r}_{t-1}^{(v)})\] \[\mathbf{X}_{t+1}^{(v)} =FC(\mathbf{r}_{t}^{(v)})\]

### Simultaneous Temporal Forecasting with TE and TR

The GRUs employed in TE and TR modules are particularly well-suited for recursive forecasting of future neural signals since they leverage the previously generated one-step signals to predict the next steps. In the multi-step forecasting scenario, however, where predictions for multiple future time steps are required, using GRUs in an autoregressive manner could lead to accumulation of errors. To address this issue and explore an alternative approach, we consider Transformers as an alternative to GRU. Transformers [68], simultaneously generate all future neural signals and could avoid the accumulation of errors. In this case, both TE and TR are implemented as decoder-based Transformers. The TR module predicts multiple future steps from a given initial time step \(t\) up to the target time step \(T\), based on the available neural recordings prior to \(t\).

[MISSING_PAGE_FAIL:17]

To ensure a fair comparison between these tasks, we employ the same model structure, specifically using GRUs for both the TE and TR modules in all four cases, and compute the loss term for each task over the time steps from \(t\) to the end of the sequence.

**Additional Results with the Synthetic Dataset.** In addition to the results in the main paper, we further assess AMAG ability to recover the learned ground truth adjacency matrix using the synthetic dataset, which consists of 40 channels with a connection ratio of 0.2. We visualize the learned adjacency matrices in Fig. 6, comparing the results obtained from AMAG computing various tasks: reconstruction, reconstruction with masking, multi-step forecasting, and one-step forecasting. The learned adjacency matrices have continuous values describing the strength of connections between channels. The color in the figure indicates the connection strength. We keep the top eight (\(40\times 0.2\)) connections as the recovery of the ground truth connections for each channel. To evaluate the accuracy of the discovered adjacency matrices, we use the F1 score as a metric that measures the harmonic mean between precision and recall, providing a comprehensive assessment of the matrix recovery. F1 scores for the four considered tasks are ordered from low to high and are arranged from left to right in Fig. 6. A higher F1 score indicates a higher degree of accuracy in capturing the ground truth adjacency matrix, highlighting the effectiveness of each method in this discovery process.

### Neural Recordings Datasets

**Details of Monkey A and B Datasets.** As described in the main paper, we collected two new datasets from monkey Affogato (A), and Beignet (B) with \(\mu\)ECoG implanted in motor cortices, apply and compare AMAG and other methods with these recordings. All procedures were approved by the University of Washington Institutional Animal Care and Use Committee. The \(\mu\)ECoG covers five different subregions (Primary Motor Cortex (M1), Premotor Cortex (PM), Frontal Eye Field (FEF), Supplementary Motor Area (SMA), and Dorsolateral Prefrontal Cortex (DLPFC)) with 239 effective electrodes which have been previously used in [67]. During data collection, the subjects performed a reaching task towards one of eight different directions. The subject's hand position was tracked in

Figure 6: Adjacency matrices learned by AMAG visualized as colormaps and arranged by their F1 score (from left to right is from lower to higher), with the rightmost map indicating Ground Truth. Yellow color indicates full recovery of ground truth value (100% accuracy) while green-blue colors indicate recovery with lower accuracy. In particular, we show outcome adjacency matrices that AMAG recovers when solving left-right: the reconstruction task, the reconstruction of masked input task, multi-step forecasting task, one-step forecasting task, and ground truth.

real-time using a marker-based camera system (Optitrak) and mapped to control the position of a cursor with which they performed the point-to-point movements. The collected neural signals can be segmented into trials based on the stage of the behavioral task. Within each trial, the subjects were trained first to reach the center target, followed by the appearance of the surrounding target. Subsequently, a delay period ranging from approximately 50ms to 150ms was introduced. After the delay period, the subjects moved from the center target to the surrounding target. A trial is considered successful if the subjects successfully reach the surrounding target, resulting in a reward. For our study, we include the neural signals recorded during the 600ms following the appearance of the surrounding target as our samples. Specifically, we treat the first 150ms of neural signals as a preparatory context for forecasting the future 450ms of neural signals. Consequently, trials with a duration shorter than 600ms after the appearance of the surrounding target are excluded from the study. In our study, we use data collected from monkeys A and B across 13 different sessions. These sessions are combined to form a comprehensive dataset for analysis. The dataset is split into 80% for training, 10% for validation, and 10% for testing. Specifically, dataset A contains 985 training samples, 122 validation, and 122 testing samples, while dataset B contains 700 training samples, 87 validation samples, and 87 testing samples. In experiments, we include all 239 electrodes from Monkey A and 87 electrodes, specifically from the M1 region of Monkey B. The neural time series for all train and test samples from monkeys A and B used in our analyses, along with meta-data of relative electrode locations within the array, will be shared to facilitate benchmarking of alternative algorithms.

**Details of Monkey M and C Datasets.** For dataset C and M [20; 64], we use neural signals recorded from the M1 of the two monkeys, including 96 channels. In these two datasets, we align the trials based on the movement onset, i.e., we include neural signals from 300ms before the movement onset and 450ms after the movement onset. Similarly, we split the datasets into 80% training data, 10% validation data, and 10% test data, collected in 6 sessions for Monkey C and 7 sessions for Monkey M. The number of training, validation and testing samples for Monkey C is 1285, 160, and 160, respectively. For dataset M, we have 1023 training samples, 127 validation, and 127 testing samples. All the trials have at least 450ms after the target onset.

**Details of Preprocessing.** M and C datasets contain the computed Local Motor Potential (LMP) and power band signal from eight frequency bands: 0.5-4 Hz, 4-8 Hz, 8-12 Hz, 12-25 Hz, 25-50 Hz, 50-100 Hz, 100-200 Hz, and 200-400 Hz. Full broadband data from A and B were collected at 25 kHz. We then applied a preprocessing procedure to extract LMP and powerband signals from A and B datasets as used in M and C datasets [20; 64]. All four datasets follow the normalization procedures to ensure consistency and comparability. Specifically, we normalize the data such that values for each channel within the range defined by mean minus four times the standard deviation (\(mean-4\times std\)) and mean plus four times the standard deviation (\(mean+4\times std\)) are linearly scaled to interval \([-1,1]\). The mean (\(mean\)) and standard deviation (\(std\)) are computed for each channel based on the training set.

### Hyperparameters and Additional Model Details

To optimize the performance of models, we conducted a hyperparameter search by exploring different values for the learning rate, hidden size, and weight decay. Specifically, we considered learning rates of \(10^{-3}\), \(5\times 10^{-4}\), and \(10^{-4}\), hidden sizes of 64, 128, 512, 1024, and 2048, and weight decay values of \(0\), \(10^{-4}\), \(10^{-5}\), \(10^{-6}\), and \(10^{-7}\). We used the Adam optimizer to update the model parameters during training. We evaluate the model performance in terms of the \(R^{2}\), correlation coefficient (\(Corr\)), and mean squared error (\(MSE\)) metrics on the validation set every 10 training epochs. The reported values in the tables of the main paper represent the testing set performance over the course of 500 epochs for most cases when the validation set achieves best performance. However, for the one-step forecasting task using the AMAG model and the STNDT-based models for both one-step and multi-step forecasting, we trained the models for 1000 epochs to ensure sufficient convergence and performance evaluation.

**Details of Models Used for Comparison**

* _LFADS_[65]. The LFADS model is a non-causal bi-directional GRU. We constrain the RNNs used in the LFADS to be uni-directional. Furthermore, since forecasting tasks typically require higher-dimensional features compared to reconstruction tasks, we set the factor dimension and the inferred input dimension to 256 for datasets A and B, and 64 for datasets M and C. The dimension of the encoder, controller, and generator in the model is 512. The model is trained with an initialized learning rate of \(10^{-4}\) reducing by 0.95 every 50 epochs and a weight decay rate of \(10^{-4}\).
* _LRNN._ We build a one-layer RNN model. For one-step forecasting, we use the hidden dimension 1024 without weight decay on all four datasets. For multi-step forecasting, the hidden dimension is 2048 with a weight decay rate of \(10^{-4}\) for M and C datasets and \(10^{-5}\) for A and B datasets. All models are trained with the initial learning rate \(10^{-4}\) decaying by 0.95 every 50 epochs.
* _RNNf_[47]. RNN_f is composed of a single-layer GRU. For one-step forecasting, the hidden dimension of GRU is 1024 with a weight decay rate \(10^{-4}\) on M and C datasets and no weight decay for A and B datasets. For multi-step forecasting, we use hidden dimensions as 2048 on A and B datasets and 1024 on M and C datasets and set the weight decay rate to 0. The initial learning rate is \(10^{-4}\), which decays by 0.95 every 50 epochs in all cases.
* _RNN PSID_[56]. We experiment with RNN PSID with the behavior-related hidden state of RNN PSID being 64 and the neuron-related hidden state being 512. The initial learning rate is \(10^{-4}\). No weight decay is used in all cases.
* _NDT_[78]. The NDT model we use is a three-layer decoder-based Transformer. On A and B datasets, the attention dimension of the Transformer is 1024 for both multi-step and one-step forecasting. The attention dimension on M and C datasets is 512 and 256 for one and multi-step forecasting, respectively. Weight decay is set to 0 for one-step forecasting on four datasets. For multi-step forecasting, weight decay is \(10^{-4}\) on the M and C datasets and \(10^{-5}\) on A and B datasets. For all cases, we set the initial learning rate \(10^{-4}\), which decays by 0.95 every 50 epochs in all cases.
* _STNDT_[37]. STNDT model we use has 10 layers of spatial-temporal blocks, each with one spatial attention Transformer and one temporal attention Transformer. The spatial Transformer has a hidden dimension as the temporal context length and the temporal attention has the dimension as the length of input neurons. We use the weight decay \(10^{-4}\) for multi-step forecasting, \(10^{-5}\) for one-step forecasting. We set the initial learning rate as \(10^{-4}\) decaying by 0.95 every 50 epochs in all cases.
* _TERN1_[47]. We experiment TERN as a one-layer GRU followed by a Transformer layer, the hidden state of both GRU and Transformer is 1024, and the model is trained with an initial learning rate \(10^{-4}\) decaying by 0.95 every 50 epochs. The weight decay rate is \(10^{-6}\) for M and C datasets for both one-step and multi-step forecasting. Weight decay on A and B datasets is \(0\) and \(10^{-4}\) and for one-step and multi-step forecasting.
* _GWNet_[76]. We use three layers GWNet where each layer has two blocks, including the three CNN kernels for dilation, residual, and skip convolution for A and B datasets. Before generating the future neural signals, there is one end CNN layer and one read-out CNN layer. The channel size is 64 for residual and dilatation CNN, 128 for skip CNN, and 64 for end CNN on A and B datasets. For M and C datasets, the channel size for residual, dilation, skip, and end CNNs are 128, 128, 256, and 512, respectively. To be noticed, for M and C datasets, each layer of GraphWaveNet has three blocks. The initial learning rate is \(5\times 10^{-4}\) decaying by 0.95 every 50 epochs. No weight decay is applied on A and B datasets, and \(10^{-7}\) weight decay rate on M and C datasets. Kernel size is 2 for all CNN layers, Footnote 1: TERN is also denoted as Neural RoBERTa, RoBERTa in short [32].
* _DCRNN_[40]. DCRNN we use is built as two diffusion convolutional layers, and each layer performs 2 diffusion steps. The hidden dimension of each diffusion layer is 64 for A and B datasets and 128 for M and C datasets, no weight decay is used. The learning rate is \(5\times 10^{-4}\) decaying by 0.95 every 50 epochs.
* _Graphs4mer_[66]. We use two variants of Graphs4mer, (i) with GRU (GS4-G) for temporal encoding, (ii) with structured state space model (GS4-S), to perform the temporal encoding for all four datasets. The hidden dimension size is 256 on A and B datasets with a weight decay rate of \(1e^{-5}\), and 128 on M and C datasets with a weight decay rate of \(0\). The learning rate is \(5\times 10^{-4}\), decaying by 0.95 every 50 epochs.
* _AMAG._ In a one-step forecasting scenario, we use single-layer GRU as a basic component for TR and TE with a 64-dimensional hidden size. For multi-step forecasting, we use Transformer layer as the basic component for TE and TR with hidden size 64. Sample-dependent matrix is computed using MLP taking the concatenated features from a pair of nodes as input, as used in [48]. The MLP encompasses four sequential fully connected layers whose input feature dimensions are \(64\times t\), \(64\times 2\), \(64\times 4\), and \(64\) respectively where \(t\) is the temporal context length. There is no weight decay on M and C datasets, \(10^{-5}\) decay rate on A and B datasets. The initial learning rate is \(5\times 10^{-4}\) decaying by 0.95 every 50 epochs.

### Additional Results

**GRU and Transformer Comparison as Base Components for TE and TR for Multi-step Forecasting.** We compare how different temporal learning components for TE and TR change model performance in multi-step forecasting. We show the results in Table 5. We find that by incorporating the Transformer for temporal learning, AMAG could potentially achieve slightly better performance. We thereby use Transformer as the main components of TE and TR for multi-step forecasting.

**Correlation Matrix as the Initialization of the Adjacency Matrix in AMAG.** A well-initialized adjacency matrix in AMAG contributes to a more stable training process. We investigate two initialization approaches: random and correlation matrix initialization. Both initialization methods result in a similar performance in terms of the final \(R^{2}\) scores, i.e., 0.659 (randomly initialized) _vs._ 0.658 (correlation matrix initialized). However, their learning curves exhibit distinct characteristics, as depicted in Fig. 7. When the adjacency matrix is initialized randomly (dashed curves), both training and testing curves display more fluctuations. In contrast, when the adjacency matrix is initialized with the correlation matrix (solid lines), the learning curves become smoother. This indicates that the correlation matrix initialization promotes a more stable and consistent learning process. Furthermore, examining the \(R^{2}\) values in the initial epochs, we observe that the model initialized with the correlation matrix achieved higher performance compared to the random initialization. This finding suggests that careful initialization with the correlation matrix can accelerate the training process, leading to improved performance during the early stages of training.

**Adjacency Matrix Before and After Training.** We visualize the learned adjacency matrix alongside the correlation matrix in Fig. 8. We observe that the learned adjacency matrix is more sparse compared to the original correlation matrix. This indicates that during the training process, AMAG prunes and

Figure 7: Visualization of \(R^{2}\) score during the training and testing with the initialization of correlation matrix (solid curves) and random initialization (dashed curves). The correlation matrix initialized adjacency matrix stabilizes the training process.

\begin{table}
\begin{tabular}{l c c c}  & \(R^{2}\uparrow\) & \(Corr\uparrow\) & \(MSE\downarrow\) \\
**AMAG-G** & 0.653 & 0.807 & **0.0283** \\
**AMAG-T** & **0.658** & **0.811** & 0.0285 \\ \end{tabular}
\end{table}
Table 5: _Multi-step forecasting performance of AMAG using GRU as the components for TE and TR modules (AMAG-G) versus AMAG using Transformer as the components for TE and TR modules (AMAG-T)._refines the correlation matrix, identifying the relevant and informative connections between neural channels for forecasting of neural signals.

## Appendix C Analysis of Forecasting Error

We provide the following proofs to understand what would be the upper bound error of the predicted neural activity. Considering the complexity of the AMAG model study, the theoretical guarantee of the temporal forecasting tasks could be a potential research direction. As a starting point, we simplify the problem as follows: considering the one-step forecasting case and removing the nonlinearities in the model, the sample-dependent weight matrix, and the multiplicative weight matrix.

Assume we have neural signal recorded from \(C\) channels at time step \(t\), \(\mathbf{X}_{t}\in\mathbb{R}^{C\times 1}\), output of TE layer, \(\mathbf{H}_{t}\in\mathbb{R}^{(C\times D)}\). (\(\mathbf{H}_{0}=0\)). The \(\mathbf{H}_{t}\) is updated with Linear RNN, such that \(\mathbf{H}_{t}=\mathbf{X}_{t}W_{1}^{e}+\mathbf{H}_{t-1}W_{2}^{e}\), where \(W_{1}^{e}\) and \(W_{2}^{e}\) are weights of linear RNN. Outputs of SI and TR are represented as \(\mathbf{Z}_{t}\) and \(\mathbf{R}_{t}\) respectively.

\[\mathbf{Z}_{t} =\mathbf{A}\mathbf{H}_{t}\] \[\mathbf{R}_{t} =\mathbf{Z}_{t}W_{1}^{r}+\mathbf{R}_{t-1}W_{2}^{r}\] \[\mathbf{R}_{0} =0\] \[\mathbf{Z}_{t},\ \mathbf{R}_{t}\in\mathbb{R}^{(C\times D)}\]

We rewrite the \(\mathbf{H}_{t}\) and \(\mathbf{Z}_{t}\) such that they only depend on \(\mathbf{X}\),

\[\mathbf{H}_{t} =\sum_{\tau=1}^{t}\mathbf{X}_{\tau}W_{1}^{e}(W_{2}^{e})^{t-\tau}\] \[\mathbf{Z}_{t} =\sum_{\tau=1}^{t}\mathbf{A}\mathbf{X}_{\tau}W_{1}^{e}(W_{2}^{e})^{t-\tau}\] \[\mathbf{R}_{t} =\sum_{\tau=1}^{t}\mathbf{Z}_{\tau}W_{1}^{e}(W_{2}^{e})^{t-\tau}\] \[=\sum_{\tau=1}^{t}(\sum_{q=1}^{\tau}\mathbf{A}\mathbf{X}_{q}W_{1}^{e}(W_{ 2}^{e})^{\tau-q})W_{1}^{r}(W_{2}^{r})^{t-\tau}\]

The Predicted signal at the time \(t+1\), \(\hat{\mathbf{X}}_{t+1}=R_{t}\mathbf{W}\). Since we assume the model is stable, such that all the weight matrices should be bounded by M, i.e., \(\|W_{1}^{e}\|_{2}\), \(\|W_{2}^{e}\|_{2}\), \(\|W_{1}^{r}\|_{2}\), \(\|W_{2}^{r}\|_{2}\leq 1\). Besides,

Figure 8: Illustration of correlation matrix as the initialization of the adjacency matrix in AMAG and the adjacency matrix after training. The learned adjacency matrix is more sparse compared with the correlation matrix (fewer values close to 1).

to make sure the RNN is stable, we need to constrain the eigenvalue of for \(W_{1}^{e},W_{2}^{e},W_{1}^{r},W_{2}^{r}\) to be smaller than 1, for example, \(\forall\lambda_{i}\in\text{eig}(W_{1}^{r}),\ \ \lambda_{i}<1\). With the assumption, we remove the term \((W_{2}^{e})^{n},\ (W_{2}^{r})^{n},n>2\) which corresponds to the contribution of previous t-2 steps to t+1 step prediction. Thus we have

\[\hat{\mathbf{X}}_{t+1}=(AX_{t-1}W_{1}^{e}W_{1}^{r}W_{2}^{r}+AX_{t-1}W_{1}^{e}W_{2}^ {e}W_{1}^{r}+AX_{t}W_{1}^{e}W_{1}^{r})W.\]

We rewrite the \(X_{t}\) and \(X_{t+1}\) in form of early step plus a variation term, such that

\[X_{t} =X_{t-1}+\delta_{1}\] \[X_{t+1} =X_{t}+\delta_{2}\] \[=X_{t-1}+\delta_{1}+\delta_{2}\]

The prediction error as \(\epsilon=\hat{\mathbf{X}}_{t+1}-\mathbf{X}_{t+1}\),

\[\epsilon=(AX_{t-1}W_{1}^{e}W_{1}^{r}W_{2}^{r}+AX_{t-1}W_{1}^{e}W_{2}^{e}W_{1}^ {r}+A(X_{t-1}+\delta_{1})W_{1}^{e}W_{1}^{r})W-X_{t-1}-\delta_{1}-\delta_{2}.\]

Considering the first three terms that decide the \(\epsilon\), we apply Cauchy-Schwarz inequality to get the upper bound,

\[\|(AX_{t-1}W_{1}^{e}W_{1}^{r}W_{2}^{r}+AX_{t-1}W_{1}^{e}W_{2}^{e}W _{1}^{r}+\] \[A(X_{t-1}+\delta_{1})W_{1}^{r}W_{1}^{r})W\|_{2}\leq 3\|A\|_{2}\|X_{t-1 }\|_{2}+\|A\|_{2}\|\delta_{1}\|_{2}\]

So we have:

\[\|\epsilon\|_{2}\leq 3\|A\|_{2}\|X_{t-1}\|_{2}+\|A\|_{2}\|\delta_{1}\|_{2}+\|X_ {t-1}\|_{2}+\|\delta_{1}\|_{2}+\|\delta_{2}\|_{2}\]

So if the signal is smooth enough that that \(||\delta_{1}||_{2}+||\delta_{2}||_{2}\) is small, then the prediction error is bounded by the adjacency matrix \(A\).

## References

* [1] Larry F Abbott. "Lapicque's introduction of the integrate-and-fire model neuron (1907)". In: _Brain research bulletin_ 50.5-6 (1999), pp. 303-304.
* [2] Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. "Black box variational inference for state space models". In: _arXiv preprint arXiv:1511.07367_ (2015).
* [3] Navid Ayoobi and Elnaz Banan Sadeghian. "A self-paced BCI system with low latency for motor imagery onset detection based on time series prediction paradigm". In: _arXiv preprint arXiv:2204.05450_ (2022).
* [4] Edoardo Balzani, Jean Paul Noel, Pedro Herrero-Vidal, Dora E Angelaki, and Cristina Savin. "A probabilistic framework for task-aligned intra-and inter-area neural manifold estimation". In: _arXiv preprint arXiv:2209.02816_ (2022).
* [5] Rahul Biswas and Eli Shlizerman. "Statistical perspective on functional and causal neural connectomics: A comparative study". In: _Frontiers in Systems Neuroscience_ 16 (2022), p. 817962.
* [6] Rahul Biswas and Eli Shlizerman. "Statistical perspective on functional and causal neural connectomics: The Time-Aware PC algorithm". In: _PLOS Computational Biology_ 18.11 (2022), e1010653.
* [7] Martin Bjerke, Lukas Schott, Kristopher T Jensen, Claudia Battistin, David A Klindt, and Benjamin A Dunn. "Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles". In: _arXiv preprint arXiv:2210.03155_ (2022).
* [8] Anastasia Borovykh, Sander Bohte, and Cornelis W Oosterlee. "Conditional time series forecasting with convolutional neural networks". In: _arXiv preprint arXiv:1703.04691_ (2017).
* [9] Nicolas Brunel. "Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons". In: _Journal of computational neuroscience_ 8 (2000), pp. 183-208.
* [10] Luis Carrillo-Reid, Shuting Han, Darik O'Neil, Ekaterina Taralova, Tony Jebara, and Rafael Yuste. "Identification of pattern completion neurons in neuronal ensembles using probabilistic graphical models". In: _Journal of Neuroscience_ 41.41 (2021), pp. 8577-8588.

* [11] Kun Chen, Tianli Xie, Li Ma, Andrew E Hudson, Qingsong Ai, and Quan Liu. "A Two-Stream Graph Convolutional Network Based on Brain Connectivity for Anesthetized States Analysis". In: _IEEE Transactions on Neural Systems and Rehabilitation Engineering_ 30 (2022), pp. 2077-2087.
* [12] Yuqian Chen, Fan Zhang, Leo R Zekelman, Tengfei Xue, Chaoyi Zhang, Yang Song, Nikos Makris, Yogesh Rathi, Weidong Cai, and Lauren J O'Donnell. "TractGraphCNN: anatomically informed graph CNN for classification using diffusion MRI tractography". In: _arXiv preprint arXiv:2301.01911_ (2023).
* [13] Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. "On the properties of neural machine translation: Encoder-decoder approaches". In: _arXiv preprint arXiv:1409.1259_ (2014).
* [14] Rainer Dahlhaus. "Graphical interaction models for multivariate time series". In: _Metrika_ 51 (2000), pp. 157-172.
* [15] Janis J Daly and Jonathan R Wolpaw. "Brain-computer interfaces in neurological rehabilitation". In: _The Lancet Neurology_ 7.11 (2008), pp. 1032-1043.
* [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding". In: _arXiv preprint arXiv:1810.04805_ (2018).
* [17] Yi Ding, Neethu Robinson, Chengxuan Tong, Qiuhao Zeng, and Cuntai Guan. "LGGNet: Learning from local-global-graph representations for brain-computer interface". In: _IEEE Transactions on Neural Networks and Learning Systems_ (2023).
* [18] Matthew Dowling, Yuan Zhao, and Il Memming Park. "Real-time variational method for learning neural trajectory and its dynamics". In: _The Eleventh International Conference on Learning Representations_.
* [19] Katie A Ferguson and Jessica A Cardin. "Mechanisms underlying gain modulation in the cortex". In: _Nature Reviews Neuroscience_ 21.2 (2020), pp. 80-92.
* [20] Robert D Flint, Christian Ethier, Emily R Oby, Lee E Miller, and Marc W Slutzky. "Local field potentials allow accurate decoding of muscle activity". In: _Journal of neurophysiology_ 108.1 (2012), pp. 18-24.
* [21] Cecilia Gallego-Carracedo, Matthew G Perich, Raeed H Chowdhury, Lee E Miller, and Juan Alvaro Gallego. "Dataset from "Local field potentials reflect cortical population dynamics in a region-specific and frequency-dependent manner"". In: (). DOI: 10.5061/dryad.xd2547dkt.url:[https://doi.org/10.5061/dryad.xd2547dkt](https://doi.org/10.5061/dryad.xd2547dkt).
* [22] Cecilia Gallego-Carracedo, Matthew G Perich, Raeed H Chowdhury, Lee E Miller, and Juan Alvaro Gallego. "Local field potentials reflect cortical population dynamics in a region-specific and frequency-dependent manner". In: _Elife_ 11 (2022), e73155.
* [23] Yuanjun Gao, Evan W Archer, Liam Paninski, and John P Cunningham. "Linear dynamical neural population models through nonlinear embeddings". In: _Advances in neural information processing systems_ 29 (2016).
* [24] Felix A Gers, Nicol N Schraudolph, and Jurgen Schmidhuber. "Learning precise timing with LSTM recurrent networks". In: _Journal of machine learning research_ 3.Aug (2002), pp. 115-143.
* [25] Lukas N Groschner, Jonatan G Malis, Birte Zuidinga, and Alexander Borst. "A biophysical account of multiplication by a single neuron". In: _Nature_ 603.7899 (2022), pp. 119-123.
* [26] Sepp Hochreiter and Jurgen Schmidhuber. "Long short-term memory". In: _Neural computation_ 9.8 (1997), pp. 1735-1780.
* [27] Alan L Hodgkin and Andrew F Huxley. "A quantitative description of membrane current and its application to conduction and excitation in nerve". In: _The Journal of physiology_ 117.4 (1952), p. 500.
* [28] Kejia Hu, Mohsen Jamali, Ziev B Moses, Carlos A Ortega, Gabriel N Friedman, Wendong Xu, and Ziv M Williams. "Decoding unconstrained arm movements in primates using high-density electrocorticography signals for brain-machine interface use". In: _Scientific reports_ 8.1 (2018), pp. 1-13.
* [29] Yicong Huang and Zhuliang Yu. "Representation Learning for Dynamic Functional Connectivities via Variational Dynamic Graph Latent Variable Models". In: _Entropy_ 24.2 (2022), p. 152.

* [30] Weiwei Jiang, Jiayun Luo, Miao He, and Weixi Gu. "Graph Neural Network for Traffic Forecasting: The Research Progress". In: _ISPRS International Journal of Geo-Information_ 12.3 (2023), p. 100.
* [31] Weiwei Jiang and Lin Zhang. "Geospatial data to images: A deep-learning framework for traffic forecasting". In: _Tsinghua Science and Technology_ 24.1 (2018), pp. 52-64.
* [32] Bryan Jimenez. "Neural Network Models For Neurophysiology Data". PhD thesis. Purdue University Graduate School, 2022.
* [33] Ming Jin, Enwei Zhu, Changde Du, Huiguang He, and Jinpeng Li. "PGCN: Pyramidal Graph Convolutional Network for EEG Emotion Recognition". In: _arXiv preprint arXiv:2302.02520_ (2023).
* [34] Mohammad Reza Keshtkaran, Andrew R Sedler, Raeed H Chowdhury, Raghav Tandon, Diya Basrai, Sarah L Nguyen, Hansem Sohn, Mehrdad Jazayeri, Lee E Miller, and Chethan Pandarinath. "A large-scale neural network training framework for generalized estimation of single-trial population dynamics". In: _bioRxiv_ (2021).
* [35] Timothy D Kim, Thomas Z Luo, Jonathan W Pillow, and Carlos Brody. "Inferring latent dynamics underlying neural population activity via neural differential equations". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 5551-5561.
* [36] Natalie Klein, Josue Orellana, Scott L Brincat, Earl K Miller, and Robert E Kass. "Torus graphs for multivariate phase coupling analysis". In: (2020).
* [37] Trung Le and Eli Shlizerman. "STNDT: Modeling Neural Population Activity with a Spatiotemporal Transformer". In: _arXiv preprint arXiv:2206.04727_ (2022).
* [38] Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, and Qi Tian. "Multiscale spatio-temporal graph neural networks for 3d skeleton-based motion prediction". In: _IEEE Transactions on Image Processing_ 30 (2021), pp. 7760-7775.
* [39] Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. "Braingnn: Interpretable brain graph neural network for fmri analysis". In: _Medical Image Analysis_ 74 (2021), p. 102233.
* [40] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. "Diffusion convolutional recurrent neural network: Data-driven traffic forecasting". In: _arXiv preprint arXiv:1707.01926_ (2017).
* [41] Scott W Linderman, Andrew C Miller, Ryan P Adams, David M Blei, Liam Paninski, and Matthew J Johnson. "Recurrent switching linear dynamical systems". In: _arXiv preprint arXiv:1610.08466_ (2016).
* [42] Hexuan Liu, Jimin Kim, and Eli Shlizerman. "Functional connectomics from neural dynamics: probabilistic graphical models for neuronal network of Caenorhabditis elegans". In: _Philosophical Transactions of the Royal Society B: Biological Sciences_ 373.1758 (2018), p. 20170377.
* [43] Ran Liu, Mehdi Azabou, Max Dabagia, Chi-Heng Lin, Mohammad Gheshlaghi Azar, Keith Hengen, Michal Valko, and Eva Dyer. "Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity". In: _Advances in Neural Information Processing Systems_ 34 (2021).
* [44] Ran Liu, Mehdi Azabou, Max Dabagia, Jingyun Xiao, and Eva Dyer. "Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 2377-2391.
* [45] Jakob H Macke, Lars Buesing, John P Cunningham, Byron M Yu, Krishna V Shenoy, and Maneesh Sahani. "Empirical models of spiking in neural populations". In: _Advances in neural information processing systems_ 24 (2011).
* [46] Jakob H Macke, Lars Buesing, and Maneesh Sahani. "Estimating state and parameters in state space models of spike trains". In: _Advanced state space methods for neural and clinical data_ 137 (2015).
* [47] Ganga Meghanath, Bryan Jimenez, and Joseph G Makin. "Inferring Population Dynamics in Macaque Cortex". In: _arXiv preprint arXiv:2304.06040_ (2023).
* [48] Siqi Miao, Mia Liu, and Pan Li. "Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism". In: _International Conference on Machine Learning_ (2022).
* [49] Kai J Miller, Dora Hermes, and Nathan P Staff. "The current state of electrocorticography-based brain-computer interfaces". In: _Neurosurgical focus_ 49.1 (2020), E2.
* [50] Luis Fernando Nicolas-Alonso and Jaime Gomez-Gil. "Brain computer interfaces, a review". In: _sensors_ 12.2 (2012), pp. 1211-1279.

* [51] Michael Nolan, Bijan Pesaran, Eli Shlizerman, and Amy L Orsborn. "Multi-block RNN Autoencoders Enable Broadband ECoG Signal Reconstruction". In: _bioRxiv_ (2022), pp. 2022-09.
* [52] Chethan Pandarinath, Paul Nuyujukian, Christine H Blabe, Brittany L Sorice, Jad Saab, Francis R Willett, Leigh R Hochberg, Krishna V Shenoy, and Jaimie M Henderson. "High performance communication by people with paralysis using an intracortical brain-computer interface". In: _Elife_ 6 (2017), e18554.
* [53] Chethan Pandarinath, Daniel J O'Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. "Inferring single-trial neural population dynamics using sequential auto-encoders". In: _Nature methods_ 15.10 (2018), pp. 805-815.
* [54] Felix Pei, Joel Ye, David Zoltowski, Anqi Wu, Raeed H Chowdhury, Hansem Sohn, Joseph E O'Doherty, Krishna V Shenoy, Matthew T Kaufman, Mark Churchland, et al. "Neural Latents Benchmark'21: evaluating latent variable models of neural population activity". In: _arXiv preprint arXiv:2109.04463_ (2021).
* [55] Omid G Sani, Hamidreza Abbaspourazad, Yan T Wong, Bijan Pesaran, and Maryam M Shanechi. "Modeling behaviorally relevant neural dynamics enabled by preferential subspace identification". In: _Nature Neuroscience_ 24.1 (2021), pp. 140-149.
* [56] Omid G Sani, Bijan Pesaran, and Maryam M Shanechi. "Where is all the nonlinearity: flexible nonlinear modeling of behaviorally relevant neural dynamics using recurrent neural networks". In: _bioRxiv_ (2021).
* [57] Marine Schimel, Ta-Chu Kao, Kristopher T Jensen, and Guillaume Hennequin. "iLQR-VAE: control-based learning of input-driven dynamics with applications to neural data". In: _bioRxiv_ (2021).
* [58] Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. "Learnable latent embeddings for joint behavioural and neural analysis". In: _Nature_ (2023), pp. 1-9.
* [59] Felix Schwock, Julien Bloch, Les Atlas, Shima Abadi, and Azadeh Yazdan-Shahmorad. "Estimating and Analyzing Neural Information flow using Signal Processing on Graphs". In: _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE. 2023, pp. 1-5.
* [60] Qi She and Anqi Wu. "Neural dynamics discovery via gaussian process recurrent neural networks". In: _Uncertainty in Artificial Intelligence_. PMLR. 2020, pp. 454-464.
* [61] Le Song, Mladen Kolar, and Eric Xing. "Time-varying dynamic bayesian networks". In: _Advances in neural information processing systems_ 22 (2009).
* [62] Tengfei Song, Wenming Zheng, Peng Song, and Zhen Cui. "EEG emotion recognition using dynamical graph convolutional neural networks". In: _IEEE Transactions on Affective Computing_ 11.3 (2018), pp. 532-541.
* [63] Frieder Stolzenburg, Sandra Litz, Olivia Michael, and Oliver Obst. "The Power of Linear Recurrent Neural Networks". In: _arXiv preprint arXiv:1802.03308_ (2018).
* [64] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. "A neural network that finds a naturalistic solution for the production of muscle activity". In: _Nature neuroscience_ 18.7 (2015), pp. 1025-1033.
* [65] David Sussillo, Rafal Jozefowicz, LF Abbott, and Chethan Pandarinath. "Lfads-latent factor analysis via dynamical systems". In: _arXiv preprint arXiv:1608.06315_ (2016).
* [66] Siyi Tang, Jared A Dunnmon, Liangqiong Qu, Khaled K Saab, Christopher Lee-Messer, and Daniel L Rubin. "Spatiotemporal Modeling of Multivariate Signals With Graph Neural Networks and Structured State Space Models". In: _arXiv preprint arXiv:2211.11176_ (2022).
* [67] Michael Trumpis, Chia-Han Chiang, Amy L Orsborn, Brinnae Bent, Jinghua Li, John A Rogers, Bijan Pesaran, Gregory Cogan, and Jonathan Viventi. "Sufficient sampling for kriging prediction of cortical potential in rat, monkey, and human \(\mu\)ECoG". In: _Journal of neural engineering_ 18.3 (2021), p. 036011.
* [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. "Attention is all you need". In: _Advances in neural information processing systems_ 30 (2017).
* [69] Giuseppe Vinci, Valerie Ventura, Matthew A Smith, and Robert E Kass. "Adjusted regularization in latent graphical models: Application to multiple-neuron spike count data". In: _The annals of applied statistics_ 12.2 (2018), p. 1068.

* [70] Saurabh Vyas, Matthew D Golub, David Sussillo, and Krishna V Shenoy. "Computation through neural population dynamics". In: _Annual review of neuroscience_ 43 (2020), pp. 249-275.
* [71] Simon Wein, Alina Schuller, Ana Maria Tome, Wilhelm M Malloni, Mark W Greenlee, and Elmar W Lang. "Forecasting brain activity based on models of spatiotemporal brain dynamics: A comparison of graph neural network architectures". In: _Network Neuroscience_ 6.3 (2022), pp. 665-701.
* [72] Francis R Willett, Donald T Avansino, Leigh R Hochberg, Jaimie M Henderson, and Krishna V Shenoy. "High-performance brain-to-text communication via handwriting". In: _Nature_ 593.7858 (2021), pp. 249-254.
* [73] Anqi Wu, Nicholas A Roy, Stephen Keeley, and Jonathan W Pillow. "Gaussian process based nonlinear latent structure discovery in multivariate spike train data". In: _Advances in neural information processing systems_ 30 (2017).
* [74] Neo Wu, Bradley Green, Xue Ben, and Shawn O'Banion. "Deep transformer models for time series forecasting: The influenza prevalence case". In: _arXiv preprint arXiv:2001.08317_ (2020).
* [75] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. "A comprehensive survey on graph neural networks". In: _IEEE transactions on neural networks and learning systems_ 32.1 (2020), pp. 4-24.
* [76] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. "Graph wavenet for deep spatial-temporal graph modeling". In: _arXiv preprint arXiv:1906.00121_ (2019).
* [77] Jiexia Ye, Juanjuan Zhao, Kejiang Ye, and Chengzhong Xu. "How to build a graph-based deep learning architecture in traffic domain: A survey". In: _IEEE Transactions on Intelligent Transportation Systems_ 23.5 (2020), pp. 3904-3924.
* [78] Joel Ye and Chethan Pandarinath. "Representation learning for neural population activity with Neural Data Transformers". en. In: _Neurons, Behavior, Data analysis, and Theory_ (Aug. 2021). issn: 2690-2664. doi: 10.51628/001c.27358. url: [https://nbdt.scholasticahq.com/article/27358-representation-learning-for-neural-population-activity-with-neural-data-transformers](https://nbdt.scholasticahq.com/article/27358-representation-learning-for-neural-population-activity-with-neural-data-transformers) (visited on 08/29/2021).
* [79] Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity". In: _Advances in neural information processing systems_ 21 (2008).
* [80] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. "Are transformers effective for time series forecasting?" In: _arXiv preprint arXiv:2205.13504_ (2022).
* [81] Yuan Zhao and Il Memming Park. "Variational latent gaussian process for recovering single-trial dynamics from population spike trains". In: _Neural computation_ 29.5 (2017), pp. 1293-1316.
* [82] Yuan Zhao and Il Memming Park. "Variational online learning of neural dynamics". In: _Frontiers in computational neuroscience_ 14 (2020), p. 71.
* [83] Ding Zhou and Xue-Xin Wei. "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 7234-7247.
* [84] Feng Zhu, Andrew Sedler, Harrison A Grier, Nauman Ahad, Mark Davenport, Matthew Kaufman, Andrea Giovannucci, and Chethan Pandarinath. "Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 2331-2345.