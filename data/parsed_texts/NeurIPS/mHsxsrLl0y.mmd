# Theoretically Guaranteed Bidirectional Data Rectification for Robust Sequential Recommendation

 Yatong Sun\({}^{1,2}\), Xiaochun Yang\({}^{1}\), Zhu Sun\({}^{3*}\), Bin Wang\({}^{1}\), Yan Wang\({}^{2}\)

\({}^{1}\)School of Computer Science and Engineering, Northeastern University, China

\({}^{2}\)School of Computing, Macquarie University, Australia

\({}^{3}\)Center for Frontier AI Research, Institute of High Performance Computing, A*STAR, Singapore

yatong@stumail.neu.edu.cn, yangxc@mail.neu.edu.cn, sunzhuntu@gmail.com, binwang@mail.neu.edu.cn, yan.wang@mq.edu.au

denotes the corresponding authors

###### Abstract

Sequential recommender systems (SRSs) are typically trained to predict the next item as the _target_ given its preceding (and succeeding) items as the _input_. Such a paradigm assumes that every input-target pair is reliable for training. However, users can be induced to click on items that are inconsistent with their true preferences, resulting in unreliable instances, i.e., mismatched input-target pairs. Current studies on mitigating this issue suffer from two limitations: (i) they discriminate instance reliability according to models trained with unreliable data, yet without theoretical guarantees that such a seemingly contradictory solution can be effective; and (ii) most methods can only tackle either unreliable input or targets but fail to handle both simultaneously. To fill the gap, we theoretically unveil the relationship between SRS predictions and instance reliability, whereby two error-bounded strategies are proposed to rectify unreliable targets and input, respectively. On this basis, we devise a model-agnostic **B**idirectional **D**ata **R**ectification (**B**irD**Rec**) framework, which can be flexibly implemented with most existing SRSs for robust training against unreliable data. Additionally, a rectification sampling strategy is devised and a self-ensemble mechanism is adopted to reduce the (time and space) complexity of BirDRec. Extensive experiments on four real-world datasets verify the generality, effectiveness, and efficiency of our proposed BirDRec.

## 1 Introduction

Recently, the study on sequential recommender systems (SRSs) [1; 2; 3; 4; 5] has garnered much attention as users' preferences are inherently dynamic and evolving in real-world scenarios. The goal of SRSs is learning to predict the next item a user interacts with given the preceding (and succeeding) items. Therefore, a training instance for SRSs is typically composed of an _input_ item sequence and its next item as the _target_. However, distractions in daily lives (e.g. recommendations from friends, account sharing, and accidental clicks) can induce users to click on items that are inconsistent with their true preferences, resulting in unreliable training instances with mismatched input-target pairs. The mismatch can be categorized into _Complete Mismatch_ and _Partial Mismatch_ when the item caused by distractions acts as an unreliable target and unreliable input of an instance, respectively. To illustrate, the romantic film 'La La Land', in the first instance of Figure 1, serves as an unreliable target, which is recommended by friends and completely mismatched with the previous superhero movies. By contrast, in the second instance, 'La La Land' acts as an unreliable input item which renders the input sequence partially mismatched with the target superhero film. Bothtypes of mismatch would cause unreliable training instances that mislead SRSs to learn sequential relationships between irrelevant items and eventually undermine the recommendation accuracy.

Although there are a number of studies aiming to combat such unreliable data for more robust SRSs, they suffer from two core limitations. (i) They discriminate instance reliability based on the intermediate or final output of a model (either the SRS itself [6; 7; 8; 9] or an additional corrector [10]) that is trained with unreliable data. However, there is no theoretical guarantee that such seemingly contradictory solutions can be trustworthy for detecting and correcting unreliable instances. (ii) Most prior studies only focus on tackling either unreliable input [6; 7; 11; 12; 13] or targets [8], but fail to handle both simultaneously. Only one recently proposed method [10] attempts to address this issue, but it relies on a corrector trained with unreliable data, yet without any theoretical guarantees.

As such, we, _for the first time_, theoretically unveil the relationship between the SRS predictions and instance reliability, proving that a target with consistently low prediction scores is unlikely to be reliable, assuming that the randomness of user behavior is limited. It then inspires us to devise two error-bounded rectification strategies to (1) detect consistently low-scored targets (i.e., unreliable targets) and replace them with steadily high-scored items and (2) detect and delete consistently low-scored items within the input (i.e., unreliable input), where the score is estimated by a backward SRS. Note that the unreliable input items, as the interruptions in the input, are not replaced but directly removed to bridge the preceding and succeeding items. Based on these strategies, we propose a model-agnostic **B**idirectional **D**ata **R**ectification (**BirDRec**) framework which contains two SRSs in opposite directions for tackling both unreliable targets and input. In addition, to reduce the time complexity, a rectification sampling strategy is devised to efficiently obtain consistently high-scored items; to reduce space complexity, a self-ensemble mechanism [14] is adopted to approximate the weighted average prediction scores across different training epochs.

**Contributions. (1)** We are the first to provide theoretically guaranteed data rectification strategies based on SRS predictions to tackle both unreliable input and targets for more robust SRSs. **(2)** We devise a model-agnostic bidirectional data rectification framework that can be flexibly implemented with most existing SRSs for robust training against unreliable data. **(3)** We devise a rectification sampling strategy and adopt a self-ensemble mechanism to ensure better scalability of BirDRec. **(4)** Extensive experiments with SRSs based on representative backbones and datasets across various domains validate the generality, effectiveness, and efficiency of BirDRec.

## 2 Related Works

Early SRSs [15; 16] adopt Markov Chains to encode users' interaction sequences, assuming users' latest interactions affect the future behavior linearly and independently. Later, powerful deep learning backbones such as recurrent neural networks (RNNs) [17; 18; 19], convolution neural networks (CNNs) [20; 21; 22], graph neural networks (GNNs) [23; 24; 25; 26; 27], and Transformers [28; 29] are employed to extract complex non-linear patterns within users' sequences [30]. They posit each training instance is a definitely matched input-target pair and thus cannot handle unreliable data.

To resist unreliable data, existing robust SRSs can be categorized into three types. The _first type_ focuses on handling the complete mismatch by identifying and eliminating instances with unreliable targets. For example, BERD [8] empirically finds that instances with high training loss and low uncertainty tend to have unreliable targets. This idea is relevant to studies on clean label selection [31; 32; 33; 34; 35] and label correction [36; 37; 38]. The _second type_ concentrates on addressing partial mismatch by reducing the importance of unreliable input when formulating users' dynamic preference representations. Accordingly, various advanced mechanisms are integrated into SRSs, such as memory networks [7], gating networks [39; 40; 41], autoencoders [12], reinforcement learning [6],

Figure 1: Examples of two types of mismatch caused by external distractions.

uncertainty modelling [11; 42], and Fast Fourier Transform [9]. To the best of our knowledge, there is only one recently proposed method STEAM [10] falling into the _third type_ which attempts to tackle both unreliable targets and input with an additional corrector producing reliable data. Nonetheless, existing robust SRSs all rely on models (either SRSs or additional correctors) trained with unreliable data, yet without theoretical proof that such seemingly contradictory solutions can be effective.

## 3 Theoretical Guarantees for Rectifying Unreliable Data

This section presents theoretical guarantees for rectifying unreliable data via SRS predictions for more robust SRSs. In particular, Section 3.1 introduces important preliminaries. Subsequently, Section 3.2 unveils the relationship between the prediction score of SRSs and the reliability of a target, inspiring us to propose an error-bounded strategy for handling unreliable targets; and Section 3.3 provides the error-bounded strategy for dealing with unreliable input.

### Preliminaries

**Problem Statement of SRSs**. Let \(\mathcal{U}\) and \(\mathcal{V}\) be the sets of users and items, respectively. Each user \(u\in\mathcal{U}\) chronologically interacts with a sequence of items \(\mathbf{s}^{u}=[\tilde{v}_{1}^{u},\tilde{v}_{2}^{u},...,\tilde{v}_{\mathbf{s} ^{u}}^{u}]\), where \(\tilde{v}_{t}^{u}\in\mathcal{V}\) is the \(t\)-th item user \(u\) interacts with and \(|\mathbf{s}^{u}|\) is the length of sequence \(\mathbf{s}^{u}\). The goal of SRSs is to predict the target item \(\tilde{v}_{t}^{u}\) given the input \(\tilde{\mathbf{x}}_{t}^{u}=\left\{u,[\tilde{v}_{t-L}^{u},...,\tilde{v}_{t-2} ^{u},\tilde{v}_{t-1}^{u}]\right\}\), where \(L\) is the length of \(\tilde{\mathbf{x}}_{t}^{u}\). Thus, the training instance of SRSs can be represented as an input-target pair \((\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u})\). Note that we use '-' to denote the observed data that may be unreliable due to external distractions.

**Core Assumptions**. Ideally, each user-item interaction should be drawn from users' true preference distribution \(\eta\) without any distractions, where \(\eta_{v_{i}}(\mathbf{x}_{t}^{u})=\mathbb{P}(v_{t}^{u}=v_{i}|\mathbf{x}_{t}^{u})\). We define \(p_{1}\) to be the true item for recommendation, i.e, the top-\(1\) item according to \(\eta\),

\[\sum\nolimits_{v_{i}\in\mathcal{V}}\mathbb{I}[\eta_{p_{1}}(\mathbf{x}_{t}^{u} )\geq\eta_{v_{i}}(\mathbf{x}_{t}^{u})]=|\mathcal{V}|,\] (1)

where \(\mathbb{I}[\cdot]\) is an indicator function that equals \(1\) if the condition is true; otherwise 0. Meanwhile, we define \(p_{2}\) to be the middle-ranked item (ranked \(\lfloor|\mathcal{V}|/2\rfloor\)-th) according to \(\eta\), namely,

\[\sum\nolimits_{v_{i}\in\mathcal{V}}\mathbb{I}[\eta_{p_{2}}(\mathbf{x}_{t}^{u })\geq\eta_{v_{i}}(\mathbf{x}_{t}^{u})]=\lfloor|\mathcal{V}|/2\rfloor.\] (2)

In general, SRSs are built upon the hypothesis that users usually select items with a tendency rather than randomly. In other words, the randomness of users' true preferences is restricted, i.e., the probability gap between the top-\(1\) and middle-ranked items regarding \(\eta\) is unlikely to be small. This assumption can be formally defined as follows.

**Assumption 1**.: _The users' true preference distribution \(\eta\) fulfills the relaxed Multiclass Tsybakov Condition [43] with constants \(C>0\), \(\lambda>0\), and \(\alpha_{0}\in(0,1]\), such that for all \(\alpha\in(0,\alpha_{0}]\),_

\[\mathbb{P}\big{[}\eta_{p_{1}}(\mathbf{x}_{t}^{u})-\eta_{p_{2}}(\mathbf{x}_{t} ^{u})\leq\alpha\big{]}\leq C\alpha^{\lambda}.\] (3)

The feasibility of Assumption 1 relies on small \(C\) and large \(\lambda\), which are satisfied on public (observed) datasets based on our empirical analysis in the Appendix with \(C\in(0.55,0.70)\) and \(\lambda\in(1.37,4.01)\).

**Connecting \(\eta\) with SRS Predictions**. Obviously, \(\eta\) is the ideal corrector to rectify unreliable data, however, due to its unavailability, many existing methods [8; 10] leverage SRS predictions as the substitution with no theoretical guarantees. This urges us to explore the connection between \(\eta\) and SRS predictions. To achieve this goal, we first investigate the relationship between \(\eta\) and users' observed preference distribution \(\tilde{\eta}\), since SRSs are trained with the observed data that may be distorted by external distractions. Formally, \(\tilde{\eta}_{v_{i}}(\tilde{\mathbf{x}}_{t}^{u})=\mathbb{P}(\tilde{v}_{t}^{u}= v_{i}|\tilde{\mathbf{x}}_{t}^{u})\), where \(\tilde{v}_{t}^{u}\) is the observed target that may be unreliable. We then define a transition probability \(\tau_{v_{i}v_{i}}(\tilde{\mathbf{x}}_{t}^{u})=\mathbb{P}(\tilde{v}_{t}^{u}=v_ {i}|v_{t}^{u}=v_{j},\tilde{\mathbf{x}}_{t}^{u})\) as the chance that a true target \(v_{t}^{u}\) is flipped from item \(v_{j}\) to item \(v_{i}\) owing to external distractions. Thus, for any pair \((v_{i},v_{j})\in\mathcal{V}\), there is a linear relationship between \(\eta\) and \(\tilde{\eta}\):

\[\tilde{\eta}_{v_{i}}(\tilde{\mathbf{x}}_{t}^{u})=\sum\nolimits_{v_{j}\in \mathcal{V}}\mathbb{P}(\tilde{v}_{t}^{u}=v_{i}|v_{t}^{u}=v_{j},\tilde{\mathbf{ x}}_{t}^{u})\mathbb{P}(v_{t}^{u}=v_{j}|\tilde{\mathbf{x}}_{t}^{u})=\sum \nolimits_{v_{j}\in\mathcal{V}}\tau_{v_{j}v_{i}}(\tilde{\mathbf{x}}_{t}^{u}) \eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u}).\] (4)

To bridge \(\eta\) and SRS predictions via \(\tilde{\eta}\), we then study the relationship between \(\tilde{\eta}\) and SRS predictions. Let \(f^{h}\) be an SRS at the \(h\)-th training epoch, and the prediction of \(f^{h}\) be \(\epsilon\)-close to \(\tilde{\eta}\),

\[\epsilon=\max_{\tilde{\mathbf{x}}_{t}^{h},v_{i}}|\tilde{\eta}_{v_{i}}(\tilde{ \mathbf{x}}_{t}^{u})-f_{v_{i}}^{h}(\tilde{\mathbf{x}}_{t}^{u})|,\] (5)where \(f_{v}^{h}(\tilde{\mathbf{x}}_{t}^{u})\) is the predicted probability (score) of the target being \(v_{i}\) given the input \(\tilde{\mathbf{x}}_{t}^{u}\) at the \(h\)-th training epoch. Eqs. (4-5) indicate that there is indeed a connection between \(\eta\) and SRS predictions, laying the foundation for the proposed strategies to rectify unreliable data as what follows.

### Theorems for Rectifying Unreliable Targets

We now explore how to properly use SRS predictions for rectifying unreliable targets. Prior works empirically find that unreliable targets tend to possess consistently low prediction scores at different epochs [8, 44]. Yet, there is no guarantee that such prediction scores given by an SRS trained with unreliable data can be trustworthy for detecting unreliable targets. This prompts us to theoretically unveil the relationship between target reliability and SRS predictions at different epochs. Specifically, Theorem 1 proves that a reliable target is unlikely to keep low prediction scores during training.

**Theorem 1**.: _Given Assumption 1, let \(\left\{w_{h}\mid 1\leq h\leq H,0\leq w_{h}\leq 1,\sum_{h=1}^{H}w_{h}=1\right\}\) be the weights1 for averaging prediction scores of different epochs. \(\forall\left\{\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\right\}\), assume \(\epsilon\leq\alpha_{\mathcal{G}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_ {t}^{u})\). Let \(\gamma=\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u}) \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau _{v_{j}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{ \mathbf{x}}_{t}^{u})\). We have: \(\mathbb{P}\Big{[}p_{1}=\tilde{v}_{t}^{u};\sum_{h=1}^{H}\left[w_{h}f_{\tilde{v}_ {t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]\leq\gamma\Big{]}\leq C( \mathcal{O}(\epsilon))^{\lambda}\)._

Footnote 1: The detailed setting of the weights is related to the self-ensemble mechanism introduced in Section 4.2

Proof.: \[\mathbb{P}\bigg{[}p_{1}=\tilde{v}_{t}^{u},\sum_{h=1}^{H}\left[w_{h }f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]\leq\gamma\bigg{]}\] \[\leq \mathbb{P}\bigg{[}p_{1}=\tilde{v}_{t}^{u},\sum_{h=1}^{H}w_{h} \big{[}\tilde{\eta}_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})-\epsilon \big{]}\leq\gamma\bigg{]}\] \[= \mathbb{P}\bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u} }(\tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}), \sum_{h=1}^{H}w_{h}\bigg{[}\sum_{v_{j}\geq\tilde{v}}\tau_{v_{j}\tilde{v}_{t}^{ u}}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})-\epsilon\bigg{]}\leq\gamma\bigg{]}\] (6) \[\leq \mathbb{P}\bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{v_{1}^{u}}( \tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\tau _{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j} }(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\leq \gamma+\epsilon\bigg{]}\] \[= \mathbb{P}\bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\eta_{v_{1}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\leq\frac {\gamma+\epsilon-\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau_{v_{j}\tilde{v}_{t}^{u} }(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}{\tau_{ v_{k}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})}\bigg{]}.\]

By replacing \(\gamma\) with \(\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{p_{ 2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau_{v_{j} \tilde{v}_{j}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}( \tilde{\mathbf{x}}_{t}^{u})\), we obtain:

\[\mathbb{P}\bigg{[}p_{1}=\tilde{v}_{t}^{u},\sum_{h=1}^{H}\left[w_{h}f_{\tilde{v }_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]\leq\gamma\bigg{]}\leq \mathbb{P}\bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta_{p_{1}}( \tilde{\mathbf{x}}_{t}^{u})\leq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+ \frac{\epsilon}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t }^{u})}\bigg{]}.\] (7)

Recall that \(\epsilon\leq\alpha_{\mathcal{G}_{t}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\), which indicates \(\frac{\epsilon}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t }^{u})}\leq\alpha_{0}\). Hence, the relaxed Multiclass Tsybakov Condition holds and the probability is bounded by \(C\big{(}\frac{\epsilon}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{ \mathbf{x}}_{t}^{u})}\big{)}^{\lambda}\), namely, \(C\big{(}\mathcal{O}(\epsilon)\big{)}^{\lambda}\). 

Note that a small \(\epsilon\) relies on large observed data and a powerful \(f\), so that (i) the observed data can accurately approximate \(\tilde{\eta}\), and (ii) \(f\) can closely fit the observed data. Both requirements can be satisfied thanks to the large-scale datasets and deep learning advancements in recommendation.

Theorem 1 indicates that the probability of a reliable target (\(p_{1}=\tilde{v}_{t}^{u}\)) keeping low prediction scores across different epochs (\(\sum_{h=1}^{H}\left[w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]\leq\gamma\)) is bounded to be low, i.e., no more than \(C\big{(}\mathcal{O}(\epsilon)\big{)}^{\lambda}\). This inspires us to rectify unreliable targets by replacing consistently low-scored targets with steadily high-scored items as the following strategy.

**DRUT: Detecting and Replacing Unreliable Targets**. Given an SRS \(f\) that is \(\epsilon\)-close to \(\tilde{\eta}\), an instance \(\langle\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\rangle\), the consistently high-scored item \(v_{m}=\arg\max_{v_{i}\neq\tilde{v}_{t}^{u}}\sum_{h=1}^{H}\left[w_{h}f_{v_{i}}^{h}( \tilde{\mathbf{x}}_{t}^{u})\right]\), and \(\beta\in(0,1]\), we stipulate that if \(\sum_{h=1}^{H}\left[w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]/\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]<\beta\), i.e., the target \(\tilde{v}_{t}^{u}\) is consistently lower-scored than \(v_{m}\) to some extent, then \(\tilde{v}_{t}^{u}\) should be replaced by \(v_{m}\) in the \(H\)-th epoch. We denote the output instance of DRUT as \(\langle\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\rangle\).

Different from existing methods, DRUT is theoretically error-bounded. Specifically, the error of DRUT, denoted as \(\boldsymbol{E}_{\mathrm{DRUT}}\), comes from three cases: (**Case-1**) the true target \(p_{1}\) is \(\tilde{v}_{t}^{u}\) but is replaced

[MISSING_PAGE_EMPTY:5]

**Theorem 3** (The Upper Bound of \(\bm{E}_{\mathrm{DDUI}}\)).: _Given Assumption 1 and the set of weights \(\{w_{h}\ |\ 1\leq h\leq H,0\leq w_{h}\leq 1,\sum_{h=1}^{H}w_{h}=1\}\), \(\forall\ \langle\bar{\mathbf{x}}_{t}^{u},\bar{\mathbf{v}}_{t-t}^{u}\rangle\), let \(\beta_{1}^{\prime}=\left[\frac{\tau_{\bar{\mathbf{x}}_{t-1}}^{\prime}(\bar{ \mathbf{x}}_{t}^{u}-(\bar{\mathbf{x}}_{t}^{u})\eta_{\bar{\mathbf{x}}_{t}^{ \prime}}^{\prime}(\bar{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq v_{l-1}^{\prime}} \tau_{v_{j}}^{\prime}\bar{\mathbf{x}}_{t-t}^{u}\eta_{v_{j}}^{\prime}(\bar{ \mathbf{x}}_{t}^{u})}{\sum_{h=1}^{H}[w_{h}\bar{\mathbf{x}}_{t}^{u}(\bar{ \mathbf{x}}_{t}^{u})]}\right]\), \(\beta_{2}^{\prime}=\left[\frac{\sum_{h=1}^{H}[w_{h}\bar{\mathbf{x}}_{t-1}^{h}( \mathbf{x}_{t}^{u})]}{\tau_{\bar{\mathbf{x}}_{t}m_{\bar{\mathbf{x}}}}^{\prime }(\bar{\mathbf{x}}_{t}^{u})\eta_{\bar{\mathbf{x}}_{t}^{\prime}}^{\prime}(\bar{ \mathbf{x}}_{t}^{u})}\right]\), \(\xi_{1}^{\prime}=|\beta^{\prime}-\beta_{1}^{\prime}|\), \(\xi_{2}^{\prime}=|\beta^{\prime}-\beta_{2}^{\prime}|\). Assume \(\xi_{2}^{\prime}<\beta_{2}^{\prime}\), \(\epsilon^{\prime}\leq\min\left[\alpha\sigma_{\bar{\mathbf{x}}_{t-1}^{\prime} \bar{\mathbf{x}}_{t}^{u}}-\bar{\mathbf{x}}_{t}^{\prime},\alpha\sigma_{\bar{ \mathbf{x}}_{t}^{u}}-\bar{\mathbf{x}}_{t}^{\prime}\right]\), \(\frac{\alpha\sigma_{\bar{\mathbf{x}}_{t}^{u}}}{\beta_{2}^{\prime}(\bar{ \mathbf{x}}_{t}^{u})^{2}(\bar{\mathbf{x}}_{t}^{u})^{2}-\bar{\xi}_{2}^{\prime} }{\beta_{2}^{\prime}(\bar{\mathbf{x}}_{t}^{u})^{2}(\bar{\mathbf{x}}_{t}^{u})^ {2}-\bar{\xi}_{2}^{\prime}}\frac{1}{2}\big{[}[\tau_{v_{1}^{\prime}\bar{\mathbf{ x}}_{t}^{u}}^{\prime}(\bar{\mathbf{x}}_{t}^{u})-\tau_{v_{1}^{\prime}\bar{ \mathbf{x}}_{m}}^{\prime}(\bar{\mathbf{x}}_{t}^{u})][\alpha_{0}+\eta_{p_{2}^{ \prime}}^{\prime}(\bar{\mathbf{x}}_{t}^{u})]-\sum_{v_{j}\neq p_{1}^{\prime}}[ \tau_{v_{j}\bar{\mathbf{x}}_{m}}^{\prime}(\bar{\mathbf{x}}_{t}^{u})-\tau_{v_{j }\bar{\mathbf{x}}_{t}^{\prime}}^{\prime}(\bar{\mathbf{x}}_{t}^{u})]\eta_{v_{j} }^{\prime}(\bar{\mathbf{x}}_{t}^{u})\big{]}\). We have: \(\bm{E}_{\mathrm{DDUI}}\leq C\big{(}\mathcal{O}(\epsilon^{\prime}+\xi_{1}^{ \prime})\big{)}^{\lambda}+C\big{(}\mathcal{O}(\epsilon^{\prime}+\xi_{2}^{ \prime})\big{)}^{\lambda}+C\big{(}\mathcal{O}(\epsilon^{\prime})\big{)}^{ \lambda}\)._

## 4 The Proposed BirDRec Framework

By integrating DRUT and DDUI into existing SRSs, we introduce BirDRec, a model-agnostic bidirectional data rectification framework, which can rectify both unreliable targets and input with theoretical guarantees for more robust SRSs. Yet, the complexity of BirDRec is prohibitively high due to the calculation and storage of prediction scores for each instance across different epochs in DRUT and DDUI. To ease this issue, we devise a rectification sampling strategy that avoids prediction on the full item set to replace unreliable targets or delete unreliable input, thereby reducing the time complexity of BirDRec. Meanwhile, we adopt the self-ensemble mechanism [14] to approximate the weighted average prediction scores of different epochs, thus avoiding preserving scores of all epochs and reducing the space complexity.

**Framework Overview**. Accordingly, the efficiency-improved BirDRec is depicted in Fig. 2. Specifically, BirDRec first leverages the self-ensembled forward SRS to rectify the target of an instance via DRUT, and then the input is rectified by the self-ensembled backward SRS via DDUI. Thereafter, the rectified instance and its \(L\) backward instances are respectively used to train the forward and backward SRSs, which are finally employed to update the corresponding self-ensembled SRSs.

### Reducing Time Complexity via Rectification Sampling

The time complexity of BirDRec is primarily attributed to the search for consistently high-scored items (\(v_{m}\) and \(\tilde{v}_{m}\)), which requires the calculation of prediction scores between every instance and all items. Concretely, the per-epoch time complexity of BirDRec is \(\mathcal{O}\big{(}N\cdot(|\mathcal{V}|+L)\big{)}\), where \(N\) denotes the number of instances. To mitigate this substantial computational burden, we propose a rectification sampling strategy that circumvents the need to search the entire item set.

**Rectification Sampling**. Each instance \(\langle\bar{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\rangle\) is assigned two rectification pools \(\mathcal{K}_{t}^{u}\) and \(\tilde{\mathcal{K}}_{t}^{u}\) to rectify the target and input, respectively. At first, \(\mathcal{K}_{t}^{u}\) is initialized by the \(K\) succeeding items of \(\tilde{v}_{t}^{u}\), i.e., \([\tilde{v}_{t+1}^{u},...,\tilde{v}_{t+K}^{u}]\), which are potentially better substitutions for \(\tilde{v}_{t}^{u}\). Similarly, \(\tilde{\mathcal{K}}_{t}^{u}\) is initialized with the \(K\) preceding items of \(\tilde{v}_{t-1}^{u}\). Next, in each epoch, the items in \(\mathcal{K}_{t}^{u}\), together with \(K\) additional items that are randomly sampled from \(\mathcal{V}\), are ranked in descending order w.r.t. their weighted average prediction scores over different epochs (i.e., \(\sum_{h=1}^{H}[w_{h}f_{v,i}^{h}(\tilde{\mathbf{x}}_{t}^{u})]\)). Then the top-\(1\) item, denoted as \(\hat{v}_{m}\), is adopted as the approximation of \(v_{m}\) in DRUT, while the top-\(K\) items are retained to update \(\mathcal{K}_{t}^{u}\) for the next epoch. The rationale behind approximating \(v_{m}\) with \(\hat{v}_{m}\) is supported by Theorem 4, which indicates that the relative rank of an item over a list of randomly sampled items can approximate this item's relative rank over the full item set. That is, the top-\(1\) item \(\hat{v}_{m}\) tends to be ranked highly over

Figure 2: The overall architecture of the proposed BirDRec framework.

\(\mathcal{V}\). The updating rules of \(\vec{K}^{u}_{t}\) for DDUI is defined similarly as \(\mathcal{K}^{u}_{t}\). As a result, the per-epoch time complexity of BirDRec is reduced from \(\mathcal{O}\big{(}N\cdot(|\mathcal{V}|+L)\big{)}\) to \(\mathcal{O}\big{(}N\cdot(K+L)\big{)}\).

**Theorem 4**.: _Let \(\mathfrak{R}\) be a list of \(K\) items randomly sampled from \(\mathcal{V}\) with replacement, \(\zeta\in(0,1)\), \(r_{H}(\mathbf{x}^{u}_{t},v_{i})=\sum_{v_{j}\in\mathcal{V}}\mathbb{I}\big{[} \sum_{h=1}^{h}[w_{h}f^{h}_{v_{j}}(\mathbf{x}^{u}_{t})]\big{]}\) be the rank of item \(v_{i}\) over the entire item set at the \(H\)-th epoch, and \(\hat{r}_{H}(\mathbf{x}^{u}_{t},v_{i})=\sum_{v_{j}\in\mathcal{R}}\mathbb{I}\big{[} \sum_{h=1}^{h}[w_{h}f^{h}_{v_{j}}(\mathbf{x}^{u}_{t})]>\sum_{h=1}^{H}[w_{h}f^{ h}_{v_{i}}(\mathbf{x}^{u}_{t})]\big{]}\) be the rank of \(v_{i}\) over \(\mathcal{R}\) at the \(H\)-th epoch. We have: \(\mathcal{P}[\frac{\hat{r}_{H}(\mathbf{x}^{u}_{t},v_{i})}{K}-\frac{r_{H}( \mathbf{x}^{u}_{t},v_{i})}{|\mathcal{V}|}|\geq\zeta]\leq\exp(-2K\zeta^{2})\)._

### Reducing Space Complexity via Self-ensemble Mechanism

The huge space cost of BirDRec is caused by the storage of prediction scores between each instance and all items in every epoch, for the sake of calculating the weighted average prediction scores.

Specifically, at the \(H\)-th epoch, the space complexity of preserving all prediction scores is \(\mathcal{O}(N\cdot|\mathcal{V}|\cdot H)\). To save space, we thus approximate the weighted average scores with the self-ensemble mechanism [14], thereby avoiding storing prediction scores of different epochs. Formally, let \(f^{H}\) be an SRS parameterized by \(\boldsymbol{\theta}_{H}\), and \(\overline{f}^{H}\) be a self-ensembled SRS parameterized by \(\overline{\boldsymbol{\theta}}_{H}=\sum_{h=1}^{H}w_{h}\boldsymbol{\theta}_{h}\). It has proven that [14] the difference between the weighted average prediction scores (\(\sum_{h=1}^{H}w_{h}f^{h}_{v_{i}}(\mathbf{x}^{u}_{t})\)) and the prediction of the self-ensembled SRS (\(\overline{f}^{H}_{v_{i}}(\mathbf{x}^{u}_{t})\)) is of the second order of smallness, if and only if \(w_{h}=\rho^{H-h}(1-\rho)^{1-\delta(h-1)}\), where \(\rho\in(0,1)\) denotes the exponential decay rate for ensembling; and \(\delta(\cdot)\) is the unit impulse function, i.e., \(\delta(0)=1\), otherwise \(0\). With such self-ensemble, there is no need to store SRSs of each epoch, as \(\overline{\boldsymbol{\theta}}_{H}\) can be efficiently derived from \(\overline{\boldsymbol{\theta}}_{H-1}\) and \(\boldsymbol{\theta}_{H}\) with the exponential moving average as follows:

\[\overline{\boldsymbol{\theta}}_{H}=\sum\nolimits_{h=1}^{H}\rho^{H-h}(1-\rho)^ {1-\delta(h-1)}\boldsymbol{\theta}_{h}=\rho\overline{\boldsymbol{\theta}}_{H -1}+(1-\rho)\boldsymbol{\theta}_{H}.\] (9)

By doing so, the burden of retaining prediction scores of different epochs is reduced to maintaining an extra self-ensembled SRS. As the parameters of an SRS mainly consist of the user and item embeddings, the per-epoch space complexity of BirDRec is reduced from \(\mathcal{O}\big{(}(|\mathcal{U}|+|\mathcal{V}|)\cdot d+(L+|\mathcal{V}|\cdot H )\cdot N\big{)}\) to \(\mathcal{O}\big{(}(|\mathcal{U}|+|\mathcal{V}|)\cdot d+(L+K)\cdot N\big{)}\), where \(d\) represents the embedding size. Furthermore, to reduce the number of parameters and mitigate overfitting, the (self-ensembled) forward and backward SRSs in BirDRec share the same user and item embeddings.

## 5 Experiments and Results

### Experimental Settings

**Datasets.** We adopt four real-world datasets with varying domains, sizes, sparsity, and average sequence lengths shown in Table 1. Specifically, ML-1M (ML)[45] is a popular movie recommendation benchmark. Beauty (Be) [46] is the product review dataset collected from Amazon.com. Yelp (Ye) [10] is a business recommendation dataset released by Yelp.com. QK-Video (QK) [47] is a video recommendation dataset crawled from Tencent.com. Following [3; 8; 9], we preprocess all datasets by removing users and items whose interactions are less than 5.

**Baselines.** To verify the generality of BirDRec, we implement it with _vanilla SRSs_ based on representative backbones. In particular, FPMC[15] is based on Markov Chain. GRU4Rec[17], Caser[20], and MAGNN[27] are built on RNN, CNN, and GNN, respectively. SASRec[28] and BERT4Rec[29] are based on Transformer. Meanwhile, to validate its effectiveness and efficiency, we compare BirDRec with state-of-the-art _robust SRSs_ including BERD[8], FMLP-Rec[9], and STEAM[10], which aim to tackle unreliable targets, unreliable input, and both, respectively.

**Evaluation Protocol.** Following [9; 48; 49], three widely-used metrics are adopted to evaluate the ranking quality, namely, HR, NDCG, and MRR. For all these metrics, higher metric values suggest

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Dataset** & \# Users & \# Items & \# Interactions & Avg. Length & Sparsity \\ \hline ML-1M (ML) & 6,040 & 3,417 & 999,611 & 165.5 & 95.16\% \\ Beauty (Be) & 22,362 & 12,102 & 198,502 & 8.9 & 99.93\% \\ Yelp (Ye) & 22,844 & 16,552 & 236,999 & 10.4 & 99.94\% \\ QK-VoidQK (QK) & 30,704 & 41,534 & 2,268,935 & 73.9 & 99.82\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of the datasets.

[MISSING_PAGE_FAIL:8]

BirDRec framework. We can note that all the baselines are boosted significantly on all datasets with the aid of BirDRec, which demonstrates the _generality_ of BirDRec.

Table 3 compares BirDRec with state-of-the-art robust SRSs. Considering BERD and STEAM are both built upon Transformer, we also adopt a transformer-based SRS, SASRec, as the default backbone of BirDRec in the following experiments to ensure fair comparisons. The results show that BirDRec dramatically outperforms existing robust SRSs, confirming the _effectiveness_ of our error-bounded rectification strategies in comparison to existing methods that lack theoretical guarantees.

Fig. 3 compares the time and storage cost of BirDRec and STEAM, both of which aim to handle unreliable input and targets simultaneously. In particular, Figs. 3(a) and (b) show that BirDRec executes \(4.28\) times faster than STEAM in each epoch and converges with fewer epochs. Meanwhile, Figs. 3(c) and (d) show that BirDRec incurs only half the storage cost of STEAM on most datasets. These results highlight the _efficiency_ of our sampling strategy and the self-ensemble mechanism. The storage cost of BirDRec is marginally higher than STEAM's on ML-1M. This is because the advantage of our rectification sampling strategy is less obvious on datasets with small item sets (see Table 1), indicating the superior scalability of our BirDRec on large-scale item sets.

**Ablation Study**. To check the efficacy of essential strategies of BirDRec (DRUT, DDUI, and the self-ensemble), we add these strategies incrementally to a plain representative SRS - SASRec. Note that without self-ensemble, DRUT and DDUI only consider the prediction scores at the latest epoch for data rectification. The results are presented in Fig. 4 and similar trends can be noted with the rest SRSs on the other metrics. First, adding either DRUT or DDUI to SASRec brings dramatic improvements, implying the effectiveness of DRUT and DDUI in rectifying unreliable targets and input, respectively. Second, using DRUT and DDUI together is better than leveraging each of them solely, indicating the necessity of rectifying both unreliable targets and input for more robust SRSs. Moreover, adding the self-ensemble mechanism can further boost the accuracy, which confirms the efficacy of considering prediction scores of different epochs for data rectification. Overall, the performance gain of separately adding DRUT, DDUI, and self-ensemble is \(45.7\%\), \(43.3\%\), and \(3.1\%\), respectively. That is, DRUT and DDUI contribute more than the self-ensemble to our BirDRec.

**Hyper-parameter Analysis.** We analyze the impact of key hyper-parameters, i.e., the rectification thresholds \(\beta\), \(\beta^{\prime}\), the size \(K\) of the rectification pool, and the exponential decay rate \(\rho\). The results are presented in Fig. 5 with several major findings (similar trends can be noted with the other metrics on the rest datasets). (i) The best choice for \(\beta\) and \(\beta^{\prime}\) is \(0.1\), showing that small thresholds tend to maintain unreliable instances while large ones may mistakenly rectify reliable data. (ii) \(K\geq 10\) is sufficient to gain better accuracy. (iii) \(\rho\geq 0.5\) yields better performance, that is, it is beneficial to consider predictions in earlier epochs. (iv) Even if the hyper-parameters of BirDRec are not optimally set, BirDRec still dramatically outperforms the best baseline (BERD) on ML-1M.

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline \hline Datasets & \multicolumn{6}{c}{**Bault**} \\ \hline Metrics & HR@5 & HR@10 & NDCG@5 & NDCG@10 & MRR & HR@5 & HR@10 & NDCG@5 & NDCG@10 & MRR \\ \hline BERD & 0.1922* & 0.2814* & 0.1267* & 0.1554* & 0.1335* & 0.0507* & 0.0745 & 0.0332 & 0.0406 & 0.0364 \\ FGMLP-Rec & 0.1789 & 0.2685 & 0.1207 & 0.1492 & 0.1299 & 0.0501 & 0.0743 & 0.0384* & 0.0448* & 0.0404* \\ STEAM & 0.1198 & 0.1950 & 0.0765 & 0.1006 & 0.0874 & 0.0495 & 0.0765* & 0.0324 & 0.0414 & 0.0371 \\ BiDRec & **2.3259** & **0.1631** & **0.1915** & **0.1647** & **0.0653** & **0.0903** & **0.0459** & **0.0537** & **0.0822** \\ _Improv._ & 22.379 & 15.815 & 28.737 & 23.232 & 23.373 & 30.347 & 18.044 & 19.535 & 19.878 & 19.31\% \\ \hline \hline Datasets & \multicolumn{6}{c}{**Yelp**} \\ \hline BERD & 0.0423 & 0.0636 & 0.0304 & 0.0360 & 0.0351 & 0.0548 & 0.0958 & 0.0352 & 0.0496 & 0.0461 \\ FGMLP-Rec & 0.0489 & 0.0702 & 0.0374 & 0.0433 & 0.0422 & 0.0604* & 0.0991 & 0.0951 & 0.0481* & 0.0480* \\ STEAM & 0.0556* & 0.0822* & 0.0387* & 0.0473* & 0.0448* & 0.0597 & 0.1021* & 0.0371 & 0.0507 & 0.0465 \\ BirDRec & **0.0771** & **0.0965** & **0.0626** & **0.0687** & **0.0663** & **0.0815** & **0.1306** & **0.0523** & **0.0882** & **0.0616** \\ _Improv._ & 38.675 & 17.400 & 61.76 & 45.246 & 47.99 & 34.93\% & 27.91\% & 33.76\% & 32.68\% & 28.33\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance comparison with existing robust SRSs, where the best performance is boldfaced and the runner up is marked by ‘\(*\)’. _Improv._ means the relative improvement of BirDRec over the runner up. The significance of the improvement is determined by a paired t-test with \(p\leq 0.001\).

Figure 4: Ablation study on different variants of BirDRec.

**Case Study.** Fig. 6 presents a real instance that is rectified by BirDRec on ML-1M. Specifically, in the DRUT module, the target movie 'Schindler's List' consistently gets lower scores than the movie 'Terminator 2' in the rectification pool, and is then replaced by 'Terminator 2'. This is reasonable, given that 'Terminator 2' aligns more closely with the input Sci-fi movies than 'Schindler's List'. Subsequently, in the DDUI module, the input movie 'American Beauty' is consistently lower-scored than the movie 'Star Wars IV' in the rectification pool and is thus deleted. This decision is justifiable, considering that the rectified target, 'Terminator 2', generally lacks relevance to the input 'American Beauty' across various aspects, including genres, actors, directors, tags, etc.

**Limitations of BirDRec.** Although BirDRec exhibits its superiority through our extensive experiments, its limitations are two-fold. Firstly, the improvement of BirDRec on storage cost is limited for smaller item sets. This is primarily because the rectification sampling strategy with self-ensemble has less apparent advantages on datasets with smaller item sets. To be specific, the storage cost reduction for calculating weighted average prediction scores is from \(O(|V|*H)\) to \(O(K)\) for each instance. Thus if \(|V|\) is small, the benefit of this reduction will be less obvious. Secondly, although BirDRec is significantly faster than the latest robust SRS (STEAM), it is worth noting that BirDRec is 1.6 times on average slower than its backbone model (as depicted in Fig. 3) in each training epoch. This increased training time could be a practical concern in systems with extremely large-scale datasets and real-time recommendation demands.

## 6 Conclusion

This work, _for the first time_, provides theoretically guaranteed data rectification strategies to tackle both unreliable input and targets for more robust SRSs. The proposed strategies are further integrated into a model-agnostic bidirectional data rectification framework, BirDRec, that can be flexibly implemented with most existing SRSs, for robust training against unreliable data. Additionally, we devise a rectification sampling strategy to reduce the computational cost of BirDRec; meanwhile, a self-ensemble mechanism is adopted to reduce the space complexity. Extensive experiments verify the generality, effectiveness, and efficiency of the proposed BirDRec.

## Acknowledgments

The work is partially supported by the National Key Research and Development Program of China (2020YFB1707900), the National Natural Science Foundation of China (Nos. U22A2025, 62072088, 62232007), Liaoning Provincial Science and Technology Plan Project - Key R&D Department of Science and Technology (No. 2023JH2/101300182), and ARC Discovery Projects DP200101441 and DP230100676.

Figure 5: Effect of key hyper-parameters \(\beta\), \(\beta^{\prime}\), \(K\), and \(\rho\) of BirDRec on ML-1M dataset.

Figure 6: The overall architecture of the proposed BirDRec framework.

## References

* [1]Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z Sheng, and Mehmet Orgun. Sequential recommender systems: challenges, progress and prospects. In _IJCAI_, pages 6332-6338, 2019.
* [2] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. Deep learning for sequential recommendation: Algorithms, influential factors, and evaluations. _ACM TOIS_, 39(1):1-42, 2020.
* [3] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. Contrastive learning for representation degeneration problem in sequential recommendation. In _WSDM_, pages 813-823, 2022.
* [4] Yu Zheng, Chen Gao, Jianxin Chang, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. Disentangling long and short-term interests for recommendation. In _TheWebConf_, pages 2256-2267, 2022.
* [5] Yongjun Chen, Zhiwei Liu, Jia Li, Julian J. McAuley, and Caiming Xiong. Intent contrastive learning for sequential recommendation. In _TheWebConf_, pages 2172-2182, 2022.
* [6] Jing Zhang, Bowen Hao, Bo Chen, Cuiping Li, Hong Chen, and Jimeng Sun. Hierarchical reinforcement learning for course recommendation in moocs. In _AAAI_, pages 435-442, 2019.
* [7] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and Hongyuan Zha. Sequential recommendation with user memory networks. In _WSDM_, pages 108-116, 2018.
* [8] Yatong Sun, Bin Wang, Zhu Sun, and Xiaochun Yang. Does every data instance matter? enhancing sequential recommendation by eliminating unreliable data. In _IJCAI_, pages 1579-1585, 2021.
* [9] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. Filter-enhanced mlp is all you need for sequential recommendation. In _TheWebConf_, pages 2388-2399, 2022.
* [10] Yujie Lin, Chenyang Wang, Zhumin Chen, Zhaochun Ren, Xin Xin, Qiang Yan, Maarten de Rijke, Xiuzhen Cheng, and Pengjie Ren. A self-correcting sequential recommender. In _TheWebConf_, pages 1283-1293, 2023.
* [11] Ziwei Fan, Zhiwei Liu, Yu Wang, Alice Wang, Zahra Nazari, Lei Zheng, Hao Peng, and Philip S Yu. Sequential recommendation via stochastic self-attention. In _TheWebConf_, pages 2036-2047, 2022.
* [12] Chi Zhang, Yantong Du, Xiangyu Zhao, Qilong Han, Rui Chen, and Li Li. Hierarchical item inconsistency signal learning for sequence denoising in sequential recommendation. In _CIKM_, pages 2508-2518, 2022.
* [13] Yuqi Qin, Pengfei Wang, and Chenliang Li. The world is binary: Contrastive learning for denoising next basket recommendation. In _SIGIR_, pages 859-868, 2021.
* [14] Hongjun Wang and Yisen Wang. Self-ensemble adversarial training for improved robustness. In _ICLR_, 2022.
* [15] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In _TheWebConf_, pages 811-820, 2010.
* [16] Chen Cheng, Haiqin Yang, Michael R Lyu, and Irwin King. Where you like to go next: successive point-of-interest recommendation. In _IJCAI_, pages 2605-2611, 2013.
* [17] Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. In _ICLR_, 2016.
* [18] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. A dynamic recurrent model for next basket recommendation. In _SIGIR_, pages 729-732, 2016.
* [19] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J. Smola, and How Jing. Recurrent recommender networks. In _WSDM_, pages 495-503, 2017.

* Tang and Wang [2018] Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In _WSDM_, pages 565-573, 2018.
* Tuan et al. [2017] Trinh Xuan Tuan and Minh Phuong Tu. 3d convolutional networks for session-based recommendation with content features. In _RecSys_, pages 138-146, 2017.
* Yuan et al. [2019] Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, and Xiangnan He. A simple convolutional generative network for next item recommendation. In _WSDM_, pages 582-590, 2019.
* Zhang et al. [2023] Mengqi Zhang, Shu Wu, Xueli Yu, Qiang Liu, and Liang Wang. Dynamic graph neural networks for sequential recommendation. _IEEE TKDE_, 35(5):4741-4753, 2023.
* Chang et al. [2021] Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. Sequential recommendation with graph neural networks. In _SIGIR_, pages 378-387, 2021.
* Xu et al. [2019] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. Graph contextualized self-attention network for session-based recommendation. In _IJCAI_, pages 3940-3946, 2019.
* Wu et al. [2019] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-based recommendation with graph neural networks. In _AAAI_, pages 346-353, 2019.
* Ma et al. [2020] Chen Ma, Liheng Ma, Yingxue Zhang, Jianing Sun, Xue Liu, and Mark Coates. Memory augmented graph neural networks for sequential recommendation. In _AAAI_, pages 5045-5052, 2020.
* Kang and McAuley [2018] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In _ICDM_, pages 197-206, 2018.
* Sun et al. [2019] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In _CIKM_, pages 1441-1450, 2019.
* Sun et al. [2019] Zhu Sun, Qing Guo, Jie Yang, Hui Fang, Guibing Guo, Jie Zhang, and Robin Burke. Research commentary on recommendations with side information: a survey and research directions. _Electronic Commerce Research and Applications (ECRA)_, 37, 2019.
* Song et al. [2022] Hwanjun Song, MINEok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE TNNLS_, 2022.
* Han et al. [2018] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In _NeurIPS_, pages 8536-8546, 2018.
* Yu et al. [2019] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In _ICML_, pages 7164-7173, 2019.
* Wei et al. [2020] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint training method with co-regularization. In _CVPR_, pages 13723-13732, 2020.
* Xiaobo Xia et al. [2022] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. In _ICLR_, 2022.
* Lu et al. [2022] Yangdi Lu, Yang Bo, and Wenbo He. Noise attention learning: Enhancing noise robustness by gradient scaling. In _NeurIPS_, pages 23164-23177, 2022.
* Lu and He [2022] Yangdi Lu and Wenbo He. SELC: self-ensemble label correction improves learning with noisy labels. In _IJCAI_, pages 3278-3284, 2022.
* Zheng et al. [2020] Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao Chen. Error-bounded correction of noisy labels. In _ICML_, pages 11447-11457, 2020.

* [39] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentive session-based recommendation. In _CIKM_, pages 747-755, 2017.
* [40] Jiaxi Tang, Francois Belletti, Sagar Jain, Minmin Chen, Alex Beutel, Can Xu, and Ed H. Chi. Towards neural mixture recommender for long range dependent user sequences. In _TheWebConf_, pages 1782-1793, 2019.
* [41] Chen Ma, Peng Kang, and Xue Liu. Hierarchical gating networks for sequential recommendation. In _KDD_, pages 825-833, 2019.
* [42] Ziwei Fan, Zhiwei Liu, Shen Wang, Lei Zheng, and Philip S Yu. Modeling sequences as distributions with uncertainty for sequential recommendation. In _CIKM_, pages 3019-3023, 2021.
* [43] Di-Rong Chen and Tao Sun. Consistency of multiclass empirical risk minimization methods based on convex loss. _JMLR_, 7:2435-2447, 2006.
* [44] Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, and Depeng Jin. Simplify and robustify negative sampling for implicit collaborative filtering. In _NeurIPS_, pages 1094-1105, 2020.
* [45] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. _ACM TIIS_, 5(4):1-19, 2015.
* [46] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In _RecSys_, pages 165-172, 2013.
* [47] Guanghu Yuan, Fajie Yuan, Yudong Li, Beibei Kong, Shujie Li, Lei Chen, Min Yang, Chenyun YU, Bo Hu, Zang Li, Yu Xu, and Xiaohu Qie. Tenrec: A large-scale multipurpose benchmark dataset for recommender systems. In _NeurIPS_, pages 11480-11493, 2022.
* [48] Zhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, and Cong Geng. Are we evaluating rigorously? benchmarking recommendation for reproducible evaluation and fair comparison. In _RecSys_, pages 23-32, 2020.
* [49] Zhu Sun, Hui Fang, Jie Yang, Xinghua Qu, Hongyang Liu, Di Yu, Yew-Soon Ong, and Jie Zhang. Daisyrec 2.0: Benchmarking recommendation for rigorous evaluation. _IEEE TPAMI_, 45(7):8206-8226, 2023.
* [50] Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. In _KDD_, pages 1748-1757, 2020.
* [51] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _AISTATS_, pages 249-256, 2010.
* [52] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.

**Theoretically Guaranteed Bidirectional Data Rectification for Robust Sequential Recommendation - Appendix -**

This Appendix is divided into three sections. First, Section A empirically validates the feasibility of Assumption 1. Next, in Section B, complete proofs of all the lemmas and theorems are presented. Finally, in Section C, we provide detailed settings of baselines and additional experimental results, including the hyper-parameter analysis and the percentage of rectified data. Our code is available at: https://github.com/AlchemistYT/BirDRec.

## Appendix A The Feasibility of Assumption 1

**Assumption 1**.: _The users' true preference distribution \(\eta\) fulfills the relaxed Multiclass Tsybakov Condition [1] with constants \(C>0\), \(\lambda>0\), and \(\alpha_{0}\in(0,1]\), such that for all \(\alpha\in(0,\alpha_{0}]\),_

\[\mathbb{P}\big{[}\eta_{p_{1}}(\mathbf{x}_{t}^{u})-\eta_{p_{2}}(\mathbf{x}_{t} ^{u})\leq\alpha\big{]}\leq C\alpha^{\lambda}.\] (1)

The feasibility of Assumption 1 relies on large \(C\) and small \(\lambda\). In order to estimate the values of \(C\) and \(\lambda\), the initial step is to approximate the true preference distribution \(\eta\). To achieve this, we first obtain a reliable dataset \(\mathcal{D}\) via the heuristic method proposed by [2]. The heuristic method measures the matching degree between the input and target of each instance from two aspects: item co-occurrence and item properties, then those instances with matching degrees lower than a threshold are filtered as unreliable data. We use a large threshold (\(0.9\)) to filter the dataset rigorously, ensuring the vast majority of the maintained instances in \(\mathcal{D}\) are reliable. Subsequently, as suggested by [3], we train a classic SRS, SASRec [4], on the filtered reliable dataset \(\mathcal{D}\) and then use the prediction of SASRec to approximate \(\eta\). Formally, for each reliable instance \(\langle\mathbf{x}_{t}^{u},v_{t}^{u}\rangle\) from the reliable dataset \(\mathcal{D}\), the SRS prediction score \(f_{v_{t}^{u}}(\mathbf{x}_{t}^{u})\) is employed as the approximation of \(\eta_{v_{t}^{u}}(\mathbf{x}_{t}^{u})\).

Next, we densely sample \(\alpha\) from \(0.05\) to \(0.9\) with step size \(0.005\) and calculate the corresponding left-hand-side probability of Eq. 1 with the following relative frequency \(F_{\alpha}\), and collect a series of \(\big{(}\log(\alpha),\log(F_{\alpha})\big{)}\) data points:

\[F_{\alpha}=\frac{1}{|\mathcal{D}|}\sum_{\langle\mathbf{x}_{t}^{u},v_{t}^{u} \rangle\in\mathcal{D}}\mathbb{I}\big{[}f_{\hat{p}_{1}}(\mathbf{x}_{t}^{u})-f _{\hat{p}_{2}}(\mathbf{x}_{t}^{u})\leq\alpha\big{]},\] (2)

where \(\hat{p}_{1}\) and \(\hat{p}_{2}\) are respectively the top- and middle-ranked items according to \(f\), namely, \(\sum_{v_{i}\in\mathcal{V}}\mathbb{I}\big{[}f_{\hat{p}_{1}}(\mathbf{x}_{t}^{u} )\geq f_{v_{i}}(\mathbf{x}_{t}^{u})\big{]}=|\mathcal{V}|\), \(\sum_{v_{i}\in\mathcal{V}}\mathbb{I}\big{[}f_{\hat{p}_{2}}(\mathbf{x}_{t}^{u} )\geq f_{v_{i}}(\mathbf{x}_{t}^{u})\big{]}=\lfloor|\mathcal{V}|/2\rfloor\). We then use \(\log(F_{\alpha})\) to approximate \(\log(C\alpha^{\lambda})\). As shown by the blue dots in Fig. 6, the collected \(\big{(}\log(\alpha),\log(F_{\alpha})\big{)}\) data points are generally linearly distributed, which allows us to estimate \(C\) and \(\lambda\) with linear regression according to \(\log(C\alpha^{\lambda})=\log(C)+\lambda\log(\alpha)\). As a result, Fig. 6 shows that the estimated \(C\) and \(\lambda\) are respectively restricted in \((0.55,0.70)\) and \((1.37,4.01)\) on real-world datasets from various domains, which validates the feasibility of Assumption 1.

## Appendix B Proofs of Lemmas and Theorems

### The Proof of Theorem 1

**Theorem 1**.: _Given Assumption 1, let \(\left\{w_{h}\ |\ 1\leq h\leq H,0\leq w_{h}\leq 1,\sum_{h=1}^{H}w_{h}=1\right\}\) be the weights for averaging prediction scores of different epochs. \(\forall\left(\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\right)\), assume \(\epsilon\leq\alpha_{0}\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{ x}}_{t}^{u})\). Let \(\gamma=\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u}) \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau _{v_{j}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{ \mathbf{x}}_{t}^{u})\). We have: \(\mathbb{P}\!\left[p_{1}=\tilde{v}_{t}^{u}\!\sum_{h=1}^{H}\left[w_{h}f_{\tilde{ v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]\leq\gamma\right] \leq C(\mathcal{O}(\epsilon))^{\lambda}\)._

Proof.: \[\mathbb{P}\!\left[p_{1}=\tilde{v}_{t}^{u},\sum_{h=1}^{H}\left[w_{h }f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]\leq\gamma\right]\] \[\leq \mathbb{P}\!\left[p_{1}=\tilde{v}_{t}^{u},\sum_{h=1}^{H}w_{h} \big{[}\tilde{\eta}_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})-\epsilon \big{]}\leq\gamma\right]\] \[= \mathbb{P}\!\left[p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u}} (\tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_ {h=1}^{H}w_{h}\bigg{[}\sum_{v_{j}\in\mathcal{V}}\tau_{v_{j}\tilde{v}_{t}^{u}} \eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})-\epsilon\bigg{]}\leq\gamma\right]\] (3) \[\leq \mathbb{P}\!\left[p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u} }(\tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\tau _{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{\tilde {v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau _{v_{j}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\gamma+\epsilon\right]\] \[= \mathbb{P}\!\left[p_{1}=\tilde{v}_{t}^{u},\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\eta_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u}) \leq\frac{\gamma+\epsilon-\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau_{v_{j}\tilde{v }_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u} )}{\tau_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})}\right]\] By replacing \(\gamma\) with \(\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{p_{ 2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau_{v_{j} \tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x} }_{t}^{u})\), we obtain:

Figure 1: The estimated constants \(C\) and \(\lambda\) on various datasets.

\[\mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t} ^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\leq\gamma\Bigg{]}\leq\mathbb{P} \Bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta_{p_{1}}(\tilde{\mathbf{ x}}_{t}^{u})\leq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\frac{\epsilon}{\tau_{ \tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}\] (4)

Recall that \(\epsilon\leq\alpha_{0}\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{ \mathbf{x}}_{t}^{u})\), which implies \(\frac{\epsilon}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_ {t}^{u})}\leq\alpha_{0}\). Hence, the relaxed Multiclass Tsybakov Condition holds and the probability is bounded by \(C\big{(}\frac{\epsilon}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{ \mathbf{x}}_{t}^{u})}\big{)}^{\lambda}\), namely, \(C\big{(}\mathcal{O}(\epsilon)\big{)}^{\lambda}\). 

### The Proof of Lemma 1

\[\small\begin{split}\small\bm{E}_{\rm DRUT}=&\underbrace{ \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},p_{1}\neq v_{m},\,\frac{\sum_{h=1}^{ H}\big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{ \sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u} )\big{]}}<\beta\Bigg{]}}_{\text{Case-1}}+&\underbrace{\mathbb{P} \Bigg{[}p_{1}\neq\tilde{v}_{t}^{u},p_{1}=v_{m},\,\frac{\sum_{h=1}^{H}\big{[}w_ {h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\sum_{h=1}^{ H}\big{[}w_{h}f_{\tilde{v}_{m}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}} \geq\beta\Bigg{]}}_{\text{Case-2}}\underbrace{\mathbb{P}\Bigg{[}p_{1}\neq\tilde {v}_{t}^{u},p_{1}\neq v_{m}\Bigg{]}}_{\text{Case-3}}.\end{split}\]

**Lemma 1**.: _Given Assumption 1 and the set of weights \(\{w_{h}\mid 1\leq h\leq H,0\leq w_{h}\leq 1,\sum_{h=1}^{H}w_{h}=1\},\ \forall\ \langle\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\rangle\), assume \(\epsilon\leq\min\big{[}\alpha_{0}\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}( \tilde{\mathbf{x}}_{t}^{u}),\alpha_{0}\tau_{w_{m}v_{m}}(\tilde{\mathbf{x}}_{t} ^{u})\big{]}\). Let \(\beta_{1}=\big{[}\frac{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{ \mathbf{x}}_{t}^{u})\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})}{\sum_{h=1}^{H} \big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\] and \(\beta_{2}=\big{[}\frac{\sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}( \tilde{\mathbf{x}}_{t}^{u})\big{]}}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t} ^{u})\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\Sigma_{v_{j}}\nu_{m}\tau_{v_{j }}\nu_{m}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}\big{]}\). We have: \(\beta\leq\beta_{1}\) guarantees the probability of Case-1 in \(\bm{E}_{\rm DRUT}\) is bounded by \(C(\mathcal{O}(\epsilon))^{\lambda}\), and \(\beta\geq\beta_{2}\) guarantees the probability of Case-2 in \(\bm{E}_{\rm DRUT}\) is bounded by \(C(\mathcal{O}(\epsilon))^{\lambda}\)._

Proof.: For Case-1 of \(\bm{E}_{\rm DRUT}\), we have:

\[\mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},p_{1}\neq v_{m},\frac{ \sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u} )\big{]}}{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}<\beta_{1}\Bigg{]}\] (5) \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\frac{\sum_{h=1}^{H} \big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{ \sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}<\beta_{1}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u}}( \tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{h=1 }^{H}w_{h}\big{[}\tilde{\eta}_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u} )-\epsilon\big{]}<\beta_{1}\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{ \mathbf{x}}_{t}^{u})\big{]}\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\eta_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})< \frac{\beta_{1}\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u} )\big{]}+\epsilon-\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\big{[}\tau_{v_{j}\tilde{v}_{t}^ {u}}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\tau_{\tilde{v}_{t}^{u} \tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\Bigg{]}.\]

By substituting \(\beta_{1}\) with \(\frac{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{ p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau_{v_{j}\tilde{v}_{t}^ {u}}\tau_{v_{j}\tilde{v}_{t}^{u}}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}{ \sum_{h=1}^{H}[w_{h}f_{\tilde{v}_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}\) in Eq. 7, we obtain:

\[\mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},p_{1}\neq v_{m},\frac{ \sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u} )\big{]}}{\sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u} )\big{]}}<\beta_{1}\Bigg{]}\] (6) \[\leq \mathbb{P}\Bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq \eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})<\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u} )+\frac{\epsilon}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u} )}\Bigg{]}.\]

Recall that \(\epsilon\leq\alpha_{0}\tau_{\tilde{v}

Thereafter, for Case-2 of \(\bm{E}_{\rm DRUT}\), we have:

\[\mathbb{P}\Bigg{[}p_{1}\neq\tilde{v}_{t}^{u},p_{1}=v_{m},\frac{\sum_ {h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]} }{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}} \geq\beta_{2}\Bigg{]}\] (7) \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\frac{\sum_{h=1}^{H}\big{[}w_{h}f_{ \tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\sum_{h=1}^{H} \big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\geq\beta_{2} \Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{m}(\tilde{\mathbf{x}}_{t}^{u} )\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{h=1}^{H}w_{h}\big{[}\tilde {\eta}_{m}(\tilde{\mathbf{x}}_{t}^{u})-\epsilon\big{]}\leq\frac{\sum_{h=1}^{H} \big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\beta_{2}} \Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{m}(\tilde{\mathbf{x}}_{t}^{u} )\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\tilde{\eta}_{m}(\tilde{\mathbf{ x}}_{t}^{u})\leq\frac{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{ u})\big{]}}{\beta_{2}}+\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{m}(\tilde{\mathbf{x}}_{t}^{u} )\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{v_{j}\in\mathcal{V}}\tau_{ v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u}) \geq\frac{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}{\beta_{2}}+\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{p_{2}}(\tilde{\mathbf{x}}_{t} ^{u})\leq\eta_{m}(\tilde{\mathbf{x}}_{t}^{u})\leq\frac{\sum_{h=1}^{h}[w_{h}f_{ v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]-\sum_{v_{j}\neq w_{m}}\big{[}\tau_{v_{j}v_{m}}( \tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{ \tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{\epsilon}{\tau_{v_{m}v_{ m}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}.\]

We replace \(\beta_{2}\) with \(\frac{\sum_{h=1}^{H}[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}{\tau_{ v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+ \sum_{v_{j}\neq v_{m}}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}} (\tilde{\mathbf{x}}_{t}^{u})}\) in Eq. 5 and continue the calculation:

\[\mathbb{P}\Bigg{[}p_{1}\neq\tilde{v}_{t}^{u},p_{1}=v_{m},\frac{ \sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{ \sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}} \geq\beta_{2}\Bigg{]}\] (8) \[\leq \mathbb{P}\Bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq \eta_{m}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u })+\frac{\epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}.\]

Recall that \(\epsilon\leq\alpha_{0}\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\), which implies \(\frac{\epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\leq\alpha_{0}\). Hence, the Multiclass Tsybakov Condition holds and the probability of Case-2 is bounded by \(C\big{(}\frac{\epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\big{)}^ {\lambda}\), namely, \(C\big{(}\mathcal{O}(\epsilon)\big{)}^{\lambda}\). 

### The Proof of Theorem 2

**Theorem 2** (The Upper Bound of \(\bm{E}_{\rm DRUT}\)).: _Given Assumption 1 and the set of weights \(\{w_{h}\bigm{|}1\leq h\leq H,0\leq w_{h}\leq 1,\sum_{h=1}^{H}w_{h}=1\}\), \(\forall\,\langle\tilde{\mathbf{x}}_{t}^{u},\tilde{v}_{t}^{u}\rangle\), let \(\beta_{1}=\big{[}\frac{\tau_{v_{m}}^{h}\tilde{v}_{t}^{u}(\tilde{\mathbf{x}}_{t}^ {u})\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq v_{j}^{u}}\tau_{ v_{j}}\tilde{v}_{t}^{u}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}{\sum_{h=1}^{H}[w_{h}f_{v _{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}\big{]}\), \(\beta_{2}=\big{[}\frac{\sum_{h=1}^{H}[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^ {u})]}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{p_{2}}(\tilde{\mathbf{ x}}_{t}^{u})+\sum_{v_{j}\neq v_{j}^{u}}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u}) \eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}\big{]}\), \(\xi_{1}=|\beta-\beta_{1}|\), and \(\xi_{2}=|\beta-\beta_{2}|\). Assume \(\xi_{2}<\beta_{2}\), \(\epsilon\leq\min\big{[}\alpha_{0}\tau_{v_{m}}^{h}\tilde{v}_{t}^{u}(\tilde{ \mathbf{x}}_{t}^{u})-\xi_{1},\frac{\alpha_{0}\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^ {u})\beta_{2}(\beta_{2}-\xi_{2})-\xi_{2}}{\beta_{2}(\beta_{2}-\xi_{2})},\frac{1}{2 }[\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})-\tau_{p_{1}v_{m}}(\tilde{\mathbf{x }}_{t}^{u})][\alpha_{0}+\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})]-\sum_{v_{j} \neq p_{1}}[\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})-\tau_{v_{j}p_{1}}( \tilde{\mathbf{x}}_{t}^{u})]\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\). We have: \(\bm{E}_{\rm DRUT}\leq C(\mathcal{O}(\epsilon+\xi_{1}))^{\lambda}+C(\mathcal{O}( \epsilon+\xi_{2}))^{\lambda}+C(\mathcal{O}(\epsilon))^{\lambda}\)._Proof.: For Case-1 of \(\bm{E}_{\rm DRUT}\), we have:

\[\mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},p_{1}\neq v_{m},\frac{ \sum_{h=1}^{H}\left[w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]}{\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]}<\beta\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\frac{\sum_{h=1}^{H} \left[w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]}{\sum_ {h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]}<\beta \Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u} }(\tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}), \sum_{h=1}^{H}w_{h}\big{[}\tilde{\eta}_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_ {t}^{u})-\epsilon\big{]}<\beta\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{ \mathbf{x}}_{t}^{u})\right]\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u} }(\tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}), \tilde{\eta}_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})<\beta\sum_{h=1}^ {H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]+\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{\tilde{v}_{t}^{u} }(\tilde{\mathbf{x}}_{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}), \sum_{v_{j}\in\mathcal{V}}\tau_{v_{j}\tilde{v}_{t}^{u}}\eta_{v_{j}}(\tilde{ \mathbf{x}}_{t}^{u})<\beta\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{ x}}_{t}^{u})\right]+\epsilon\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\eta_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})< \frac{\beta\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]+\epsilon-\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\big{[}\tau_{v_{j}\tilde{v}_{ t}^{u}}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\tau_{\tilde{v}_{t}^{u} \tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\eta_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})< \frac{(\beta_{1}+\xi_{1})\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{ \mathbf{x}}_{t}^{u})\right]+\epsilon-\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\big{[} \tau_{v_{j}\tilde{v}_{t}^{u}}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{ \tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})} \Bigg{]}.\] (9)

We substitute \(\beta_{1}\) with \(\frac{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})\eta _{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq\tilde{v}_{t}^{u}}\tau_{v_{ j}\tilde{v}_{t}^{u}}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}{\sum_{h=1}^{H}[w_{h}f_{v_{m}} ^{h}(\tilde{\mathbf{x}}_{t}^{u})]}\) and obtain:

\[\mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},p_{1}\neq v_{m},\frac{ \sum_{h=1}^{H}\left[w_{h}f_{\tilde{v}_{t}^{u}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]}{\sum_{h=1}^{H}\left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \right]}<\beta\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=\tilde{v}_{t}^{u},\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})\leq\eta_{\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})< \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+\frac{\epsilon}{\tau_{\tilde{v}_{t}^{u} \tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{\xi_{1}\sum_{h=1}^{H} \left[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\right]}{\tau_{\tilde{v}_{t }^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}\] (10) \[\leq \mathbb{P}\Bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta _{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})<\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})+ \frac{\epsilon+\xi_{1}}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{ x}}_{t}^{u})}\Bigg{]}.\]

Recall that \(\epsilon\leq\alpha_{0}\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{ x}}_{t}^{u})-\xi_{1}\), which implies \(\frac{\epsilon+\xi_{1}}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{\mathbf{x}}_{t}^{ u})}\leq\alpha_{0}\). Hence, the Multiclass Tsybakov Condition holds and the probabiliy of Case-2 is bounded by \(C\Big{(}\frac{\epsilon+\xi_{1}}{\tau_{\tilde{v}_{t}^{u}\tilde{v}_{t}^{u}}(\tilde{ \mathbf{x}}_{t}^{u})}\Big{)}^{\lambda}\), namely, \(C\big{(}\mathcal{O}(\epsilon+\xi_{1})\big{)}^{\lambda}\).

For Case-2 of \(\bm{E}_{\rm{DRTU}}\), we have:

\[\mathbb{P}\Bigg{[}p_{1}\neq\tilde{v}_{t}^{u},p_{1}=v_{m},\frac{\sum_{ h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{v}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{ \sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}\geq\beta\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\frac{\sum_{h=1}^{H}\big{[}w_{h}f_{ \tilde{v}_{t}^{v}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\sum_{h=1}^{H}\big{[} w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\geq\beta\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{m}(\tilde{\mathbf{x}}_{t}^{u} )\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\tilde{\eta}_{m}(\tilde{\mathbf{ x}}_{t}^{u})\leq\frac{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{ u})\big{]}}{\beta}+\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{m}(\tilde{\mathbf{x}}_{t}^{u} )\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{v_{j}\in\mathcal{V}}\tau_{ v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\leq \frac{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}{\beta}+\epsilon\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^ {u})\leq\eta_{m}(\tilde{\mathbf{x}}_{t}^{u})\leq\frac{\frac{\sum_{h=1}^{H}[w_{ h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}{\beta}-\sum_{v_{j}yv_{m}}\big{[}\tau_{ v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{\epsilon}{\tau_{ v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^ {u})\leq\eta_{m}(\tilde{\mathbf{x}}_{t}^{u})\leq\frac{\frac{\sum_{h=1}^{H}[w_{ h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}{\beta_{2}-\xi_{2}}-\sum_{v_{j}v_{m}} \big{[}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{ x}}_{t}^{u})\big{]}}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{ \epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^ {u})\leq\eta_{m}(\tilde{\mathbf{x}}_{t}^{u})\leq\frac{\frac{\sum_{h=1}^{H}[w_{ h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}{\beta_{2}}-\sum_{v_{j}yv_{m}}\big{[} \tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t }^{u})\big{]}}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{\epsilon}{ \tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\] \[\quad+\frac{\xi_{2}\sum_{h=1}^{H}[w_{h}f_{v_{m}}^{h}(\tilde{ \mathbf{x}}_{t}^{u})]}{\beta_{2}(\beta_{2}-\xi_{2})\tau_{v_{m}v_{m}}(\tilde{ \mathbf{x}}_{t}^{u})}\Bigg{]}\] (11)

We replace \(\beta_{2}\) with \(\frac{\sum_{h=1}^{H}[w_{h}f_{\tilde{v}_{t}^{v}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) ]}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{p_{2}}(\tilde{\mathbf{x} }_{t}^{u})+\sum_{v_{j}yv_{m}}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta _{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})}\) and continue the calculation:

\[\mathbb{P}\Bigg{[}p_{1}\neq\tilde{v}_{t}^{u},p_{1}=v_{m},\frac{ \sum_{h=1}^{H}\big{[}w_{h}f_{\tilde{v}_{t}^{v}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}{\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u}) \big{]}}\geq\beta\Bigg{]}\] (12) \[\leq \mathbb{P}\Bigg{[}p_{1}=v_{m},\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^ {u})\leq\eta_{m}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta_{p_{2}}(\tilde{\mathbf{x} }_{t}^{u})+\frac{\epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+ \frac{\xi_{2}\sum_{h=1}^{H}[w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})]}{ \beta_{2}(\beta_{2}-\xi_{2})\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})} \Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta _{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\leq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}) +\frac{\epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{\xi_{2}}{ \beta_{2}(\beta_{2}-\xi_{2})\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})} \Bigg{]}.\]

Recall that \(\epsilon\leq\frac{\alpha_{0}\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\beta_{2}( \beta_{2}-\xi_{2})-\xi_{2}}{\beta_{2}(\beta_{2}-\xi_{2})}\), which implies \(\frac{\epsilon}{\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}+\frac{\xi_{2}}{ \beta_{2}(\beta_{2}-\xi_{2})\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\leq \alpha_{0}\). Hence, the relaxed Multiclass Tsybakov Condition holds and the probability of Case-2 is bounded by \(C\Big{(}\frac{\epsilon\beta_{2}(\beta_{2}-\xi_{2})+\xi_{2}}{\beta_{2}(\beta_{2}- \xi_{2})\tau_{v_{m}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})}\Big{)}^{\lambda}\), namely, \(C\big{(}\mathcal{O}(\epsilon+\xi_{2})\big{)}^{\lambda}\).

Finally, for Case-3 of \(\bm{E}_{\rm DRUT}\), we have:

\[\mathbb{P}\Big{[}p_{1}\neq\bar{v}_{t}^{u},p_{1}\neq v_{m}\Big{]}\] \[\leq \mathbb{P}\Big{[}p_{1}\neq v_{m}\Big{]}\] \[= \mathbb{P}\Bigg{[}p_{1}\neq v_{m},\eta_{p_{1}}(\tilde{\mathbf{x}} _{t}^{u})\geq\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\frac{\sum_{h=1}^{H} \big{[}w_{h}f_{p_{1}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}{\sum_{h=1}^{H} \big{[}w_{h}f_{v_{m}}^{h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}<1\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{h=1}^{H}\big{[}w_{h}f_{p_{1}}^{ h}(\tilde{\mathbf{x}}_{t}^{u})\big{]}<\sum_{h=1}^{H}\big{[}w_{h}f_{v_{m}}^{h}( \tilde{\mathbf{x}}_{t}^{u})\big{]}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{h=1}^{H}w_{h}\big{[}\tilde{\eta }_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})-\epsilon\big{]}<\sum_{h=1}^{H}w_{h} \big{[}\tilde{\eta}_{m}(\tilde{\mathbf{x}}_{t}^{u})+\epsilon\bigg{]}\Bigg{]}\] \[= \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\tilde{\eta}_{p_{1}}(\tilde{\mathbf{ x}}_{t}^{u})-\epsilon<\tilde{\eta}_{m}(\tilde{\mathbf{x}}_{t}^{u})+\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\sum_{v_{j}\in\mathcal{V}}\tau_{v_{j }p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})< \sum_{v_{j}\in\mathcal{V}}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{ v_{j}}(\tilde{\mathbf{x}}_{t}^{u})+2\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_ {t}^{u})\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq p_{1}}\tau_{v _{j}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})<\] (13) \[\tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\eta_{p_{1}}(\tilde{ \mathbf{x}}_{t}^{u})+\sum_{v_{j}\neq p_{1}}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x} }_{t}^{u})\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})+2\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{ u})\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})-\tau_{p_{1}v_{m}}( \tilde{\mathbf{x}}_{t}^{u})\big{]}<\] \[\sum_{v_{j}\neq p_{1}}\big{[}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x} }_{t}^{u})-\tau_{v_{j}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{v_{j}}( \tilde{\mathbf{x}}_{t}^{u})+2\epsilon\Bigg{]}\] \[= \mathbb{P}\Bigg{[}\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\geq \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u}),\eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{ u})<\frac{\sum_{v_{j}\neq p_{1}}\big{[}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})- \tau_{v_{j}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{v_{j}}(\tilde{ \mathbf{x}}_{t}^{u})+2\epsilon}{\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t }^{u})-\tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\Bigg{]}\] \[= \mathbb{P}\Bigg{[}\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\leq \eta_{p_{1}}(\tilde{\mathbf{x}}_{t}^{u})<\eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{ u})+\] \[\frac{\sum_{v_{j}\neq p_{1}}\big{[}\tau_{v_{j}v_{m}}(\tilde{\mathbf{ x}}_{t}^{u})-\tau_{v_{j}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{v_{j}}( \tilde{\mathbf{x}}_{t}^{u})+2\epsilon-\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x} }_{t}^{u})-\tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{p_{2}}( \tilde{\mathbf{x}}_{t}^{u})}{\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})- \tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\Bigg{]}\]

Recall that \(\epsilon\leq\frac{1}{2}\Big{[}\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t}^{ u})-\tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\big{[}\alpha_{0}+ \eta_{p_{2}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}-\sum_{v_{j}\neq p_{1}}\big{[} \tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})-\tau_{v_{j}p_{1}}(\tilde{\mathbf{x}}_{t }^{u})\big{]}\eta_{v_{j}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\), which implies \(\frac{\sum_{v_{j}\neq p_{1}}\big{[}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{ u})-\tau_{v_{j}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{v_{j}}(\tilde{ \mathbf{x}}_{t}^{u})+2\epsilon-\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t}^{ u})-\tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{p_{2}}(\tilde{ \mathbf{x}}_{t}^{u})}{\big{[}\tau_{p_{1}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})- \tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}}\leq\alpha_{0}\). Hence, the relaxed Multiclass Tsybakov Condition holds and the probability of Case-3 is bounded by \(C\bigg{(}\frac{\sum_{v_{j}\neq p_{1}}\big{[}\tau_{v_{j}v_{m}}(\tilde{\mathbf{x}}_{t}^{ u})-\tau_{v_{j}p_{1}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{v_{j}}( \tilde{\mathbf{x}}_{t}^{u})+2\epsilon-\big{[}\tau_{p_{1}p_{1}}(\tilde{ \mathbf{x}}_{t}^{u})-\tau_{p_{1}v_{m}}(\tilde{\mathbf{x}}_{t}^{u})\big{]}\eta_{p_{2}}( \tilde{\mathbf{x}}_{t}^{u})}{\big{[}\tau_{p_{1}p_{1

**Theorem 3** (The Upper Bound of \(\bm{E}_{\rm{DDUI}}\)).: _Given Assumption 1 and the set of weights \(\{w_{h}\mid 1\leq h\leq H,0\leq w_{h}\leq 1,\sum_{h=1}^{H}w_{h}=1\}\), \(\forall\,\langle\bar{\bm{x}}_{t}^{w},\bar{\bm{v}}_{t-l}^{u}\rangle\), let \(\beta^{\prime}_{1}=\left[\frac{\tau^{\prime}_{\bar{\bm{v}}_{t-l}^{u}}(\bar{\bm{x }}_{t}^{u})\tau^{\prime}_{\bar{\bm{v}}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u})+\sum_{v_ {j}\neq\bar{\bm{v}}_{t-l}^{u}}\tau^{\prime}_{\bar{\bm{v}}_{j}^{u}}\bar{\bm{x}}_ {t-l}^{u}\tau^{\prime}_{\bar{\bm{v}}_{j}^{u}}(\bar{\bm{x}}_{t}^{u})}{\sum_{h=1 }^{H}[w_{h}\bar{\bm{v}}_{h}^{h}(\bar{\bm{x}}_{t}^{u})]}\right]\), \(\beta^{\prime}_{2}=\left[\frac{\sum_{h=1}^{H}[w_{h}\bar{\bm{v}}_{t-l}^{u}(\bar{ \bm{x}}_{t}^{u})]}{\tau^{\prime}_{\bar{\bm{v}}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u}) \tau^{\prime}_{\bar{\bm{v}}_{j}^{u}}(\bar{\bm{x}}_{t}^{u})\tau^{\prime}_{\bar{ \bm{v}}_{j}^{u}}(\bar{\bm{x}}_{t}^{u})}\right]\), \(\xi^{\prime}_{1}=|\beta^{\prime}-\beta^{\prime}_{1}|\), \(\xi^{\prime}_{2}=|\beta^{\prime}-\beta^{\prime}_{2}|\). Assume \(\xi^{\prime}_{2}<\beta^{\prime}_{2}\), \(\epsilon^{\prime}\leq\min\left[\alpha\tau^{\prime}_{\bar{\bm{v}}_{t-l}^{u}( \bar{\bm{x}}_{t}^{u})}-\xi^{\prime}_{1},\frac{\alpha\sigma^{\prime}_{\bar{\bm{ v}}_{t-l}}\bar{\bm{v}}_{t-l}\bar{\bm{v}}_{t-l}}{\beta^{\prime}_{2}(\bar{\bm{v}}_{t}^{u} )\beta^{\prime}_{2}(\beta^{\prime}_{2}-\xi^{\prime}_{2})-\xi^{\prime}_{2}}, \frac{1}{2}\left[\tau^{\prime}_{\bar{\bm{v}}_{1}^{\prime}\bar{\bm{v}}_{1}^{ \prime}}(\bar{\bm{x}}_{t}^{u})-\tau^{\prime}_{\bar{\bm{v}}_{1}^{\prime}\bar{\bm {v}}_{m}}(\bar{\bm{x}}_{t}^{u})\right]\)\(\left[\alpha_{0}+\eta^{\prime}_{\bar{\bm{v}}_{t}^{\prime}}(\bar{\bm{x}}_{t}^{u})- \tau^{\prime}_{\bar{\bm{v}}_{j}^{u}}(\bar{\bm{x}}_{t}^{u})\right]-\sum_{v_{j} \neq\bar{\bm{v}}_{1}^{\prime}}[\tau^{\prime}_{v_{j}^{u}}(\bar{\bm{x}}_{t}^{u}) -\tau^{\prime}_{v_{j}^{\prime}\bar{\bm{v}}_{j}^{u}}(\bar{\bm{x}}_{t}^{u})] \eta^{\prime}_{v_{j}^{u}}(\bar{\bm{x}}_{t}^{u})]\). We have: \(\bm{E}_{\rm{DDUI}}\leq C\big{(}\mathcal{O}(\epsilon^{\prime}+\xi^{\prime}_{1}) \big{)}^{\lambda}+C\big{(}\mathcal{O}(\epsilon^{\prime}+\xi^{\prime}_{2})\big{)} ^{\lambda}+C\big{(}\mathcal{O}(\epsilon^{\prime})\big{)}^{\lambda}\)._

Proof.: For the first term of \(\bm{E}_{\rm{DDUI}}\), we have:

\[\mathbb{P}\Bigg{[}p^{\prime}_{1}=\bar{v}_{t-l}^{u},\frac{\sum_{h=1 }^{H}\left[w_{h}\bar{\bm{v}}_{\bar{v}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u})\right]}{ \sum_{h=1}^{H}\left[w_{h}\bar{\bm{f}}_{\bar{v}_{t-l}^{u}}^{h}(\bar{\bm{x}}_{t}^ {u})\right]}<\beta^{\prime}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p^{\prime}_{1}=\bar{v}_{t-l}^{u},\eta^{\prime}_{ \bar{v}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u})\geq\eta^{\prime}_{\bar{\bm{v}}_{2}^{ \prime}}(\bar{\bm{x}}_{t}^{u}),\sum_{h=1}^{H}w_{h}\big{[}\bar{\bm{f}}_{\bar{v} _{t-l}^{u}}^{h}(\bar{\bm{x}}_{t}^{u})-\epsilon^{\prime}\big{]}<\beta^{\prime} \sum_{h=1}^{H}\left[w_{h}\bar{\bm{f}}_{v_{m}}^{h}(\bar{\bm{x}}_{t}^{u})\right] \right]\] \[= \mathbb{P}\Bigg{[}p^{\prime}_{1}=\bar{v}_{t-l}^{u},\eta^{\prime}_{ \bar{v}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u})\geq\eta^{\prime}_{\bar{\bm{v}}_{2}^{ \prime}}(\bar{\bm{x}}_{t}^{u}),\tilde{\tau}^{\prime}_{\bar{v}_{t-l}^{u}}(\bar{ \bm{x}}_{t}^{u})<\beta^{\prime}\sum_{h=1}^{H}\left[w_{h}\bar{\bm{f}}_{\bar{v}_{ m}}^{h}(\bar{\bm{x}}_{t}^{u})\right]+\epsilon^{\prime}\Bigg{]}\] \[= \mathbb{P}\Bigg{[}p^{\prime}_{1}=\bar{v}_{t-l}^{u},\eta^{\prime}_{ \bar{\bm{v}}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u})\geq\eta^{\prime}_{\bar{\bm{v}}_{2}^ {\prime}}(\bar{\bm{x}}_{t}^{u}),\sum_{v_{j}\in\mathcal{V}}\tau^{\prime}_{\bar{v} _{j}^{u}\bar{v}_{t-l}^{u}}\eta^{\prime}_{v_{j}}(\bar{\bm{x}}_{t}^{u})<\beta^{ \prime}\sum_{h=1}^{H}\left[w_{h}\bar{\bm{f}}_{\bar{v}_{m}}^{h}(\bar{\bm{x}}_{t}^ {u})\right]+\epsilon^{\prime}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p^{\prime}_{1}=\bar{v}_{t-l}^{u},\eta^{\prime}_{ \bar{\bm{v}}_{2}^{\prime}}(\bar{\bm{x}}_{t}^{u})\leq\eta^{\prime}_{\bar{v}_{t-l }^{u}}(\bar{\bm{x}}_{t}^{u})<\frac{\beta^{\prime}\sum_{h=1}^{H}\left[w_{h}\bar{ \bm{f}}_{\bar{v}_{m}}^{h}(\bar{\bm{x}}_{t}^{u})\right]+\epsilon^{\prime}-\sum_{v_ {j}\neq\bar{v}_{t-l}^{u}}\left[\tau^{\prime}_{v_{j}\bar{v}_{t-l}^{u}}\eta^{ \prime}_{v_{j}}(\bar{\bm{x}}_{t}^{u})\right]}{\tau^{\prime}_{\bar{v}_{t-l}^{u} \bar{\bm{x}}_{t}^{u}}(\bar{\bm{x}}_{t}^{u})}\Bigg{]}.\] (15)

We substitute \(\beta^{\prime}_{1}\) with \(\frac{\tau^{\prime}_{\bar{v}_{t-l}^{u}\bar{\bm{v}}_{t-l}^{u}}(\bar{\bm{x}}_{t}^{u}) \eta^{\prime}_{\bar{\bm{v}}_{j}^{u}}(\bar{\bm{x}}_{t}^{u})+\sum_{v_{j}\neq\bar{v} _{t-l}^{u}}\tau^{\prime}_{\bar{\bm{v}}_{j}^{u}\bar{v}_{t-l}^{u}}\eta^{\prime}_{ v_{j}}(\bar{\bm{x}}_{t}^{u})}{\sum_{h=1}^{H}\left[w_{h}\bar{\bm{f}}_{\bar{v}_{m}}^{h}(\bar{\bm{x}}_{t}^ {u})\right]}\) and obtain:

\[\mathbb{P}\Bigg{[}p^{\prime}_{1}=\bar{v}_{t-l}^{u},\frac{\sum_{h=1 }^{H}\left[w_{h}\bar{\bm{f}}_{\bar{v}_{t-l}^{u}}^{h}(\bar{\bm{x}}_{t}^{u}) \For the first term of Eq. 17, we have:

\[\mathbb{P}\Bigg{[}p_{1}^{\prime}\neq\tilde{v}_{t-l}^{u},p_{1}^{ \prime}=\tilde{v}_{m},\frac{\sum_{h=1}^{H}\big{[}w_{h}\tilde{f}_{\tilde{v}_{m}^{ u}}^{h}(\mathbf{\tilde{x}}_{t}^{u})\big{]}}{\sum_{h=1}^{H}\big{[}w_{h}\tilde{f}_{ \tilde{v}_{m}}^{h}(\mathbf{\tilde{x}}_{t}^{u})\big{]}}\geq\beta^{\prime}\Bigg{]}\] (18) \[\leq \mathbb{P}\Bigg{[}p_{1}^{\prime}=\bar{v}_{m},\frac{\sum_{h=1}^{H} \big{[}w_{h}\tilde{f}_{\tilde{v}_{m}^{u}}^{h}(\mathbf{\tilde{x}}_{t}^{u})\big{]} }{\sum_{h=1}^{H}\big{[}w_{h}\tilde{f}_{\tilde{v}_{m}}^{h}(\mathbf{\tilde{x}}_{ t}^{u})\big{]}}\geq\beta^{\prime}\Bigg{]}\] \[\leq \mathbb{P}\Bigg{[}p_{1}^{\prime}=\bar{v}_{m},\eta_{\tilde{v}_{m} }^{\prime}(\mathbf{\tilde{x}}_{t}^{u})\geq\eta_{\tilde{v}_{m}}^{\prime}( \mathbf{\tilde{x}}_{t}^{u})\leq\eta_{\tilde{v}_{m}^{\prime}}^{\prime}( \mathbf{\tilde{x}}_{t}^{u})+\frac{\epsilon^{\prime}}{\gamma_{\tilde{v}_{m} \tilde{v}_{m}}^{\prime}(\mathbf{\tilde{x}}_{t}^{u})}+\frac{\xi_{2}^{\prime} \sum_{h=1}^{H}[w_{h}\tilde{f}_{\tilde{v}_{m}}^{h}(\mathbf{\tilde{x}}_{t}^{u}) ]}{\beta^{\prime}(\beta_{2}^{\prime}-\xi_{2}^{\prime})\tau_{\tilde{v}_{m} \tilde{v}_{m}}^{\prime}(\mathbf{\tilde{x}}_{t}^{u})}\Bigg{]}\] (19) \[\leq \mathbb{P}\Bigg{[}\eta_{\tilde{v}_{m}^{\prime}}^{\prime}( \mathbf{\tilde{x}}_{t}^{u})\leq\eta_{\tilde{v}_{m}}^{\prime}(\mathbf{\tilde{x }}_{t}^{u})\leq\eta_{\tilde{v}_{m}^{\prime}}^{\prime}(\mathbf{\tilde{x}}_{t}^ {u})+\frac{\epsilon^{\prime}}{\gamma_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime}( \mathbf{\tilde{x}}_{t}^{u})}+\frac{\xi_{2}^{\prime}}{\beta_{2}^{\prime}(\beta _{2}^{\prime}-\xi_{2}^{\prime})\tau_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime}( \mathbf{\tilde{x}}_{t}^{u})}\Bigg{]}.\]

Recall that \(\epsilon^{\prime}\leq\frac{\alpha_{0}\tau_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime }\varepsilon_{m}(\mathbf{\tilde{x}}_{t}^{u})\beta_{2}^{\prime}(\beta_{2}^{ \prime}-\xi_{2}^{\prime})-\xi_{2}^{\prime}}{\beta_{2}^{\prime}(\beta_{2}^{ \prime}-\xi_{2}^{\prime})\tau_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime}(\mathbf{ \tilde{x}}_{t}^{u})}\), which implies \(\frac{\epsilon^{\prime}}{\gamma_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime}(\mathbf{ \tilde{x}}_{t}^{u})}+\frac{\xi_{2}^{\prime}}{\beta_{2}^{\prime}(\beta_{2}^{ \prime}-\xi_{2}^{\prime})\tau_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime}(\mathbf{ \tilde{x}}_{t}^{u})}\leq\alpha_{0}\). Hence, the relaxed Multiclass Tsybakov Condition holds and the probability of the first term of Eq. 17 is bounded by \(C\Big{(}\frac{\epsilon^{\prime}\beta_{2}^{\prime}(\beta_{2}^{\prime}-\xi_{2}^{ \prime})+\xi_{2}^{\prime}}{\beta_{2}^{\prime}(\beta_{2}^{\prime}-\xi_{2}^{ \prime})\tau_{\tilde{v}_{m}\tilde{v}_{m}}^{\prime}(\mathbf{\tilde{x}}_{t}^{u}) }\Big{)}^{\lambda}\), namely, \(C\big{(}\mathcal{O}(\epsilon^{\prime}+\xi_{2}^{\prime})\big{)}^{\lambda}\).

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_FAIL:24]

Figure 5: Testing accuracy (NDCG@10) of our BirDRec and SASRec with increasing epochs. The training stops if the best accuracy does not increase in 25 consecutive epochs.

Figure 6: The percentage of instances that are rectified with increasing epochs.

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l} \hline \hline  & Parameter & ML & Be & Ye & QK & Search Space \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & learning\_rate & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & L2\_regularization\_coefficient & \(10^{-2}\) & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & input\_length & \(10\) & \(5\) & \(5\) & \(10\) & \([5,10,20,30,40,50]\) \\  & batch size & \(1024\) & \(512\) & \(512\) & \(1024\) & \([128,256,512,1024]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & learning\_rate & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & L2\_regularization\_coefficient & \(10^{-2}\) & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & input\_length & \(5\) & \(5\) & \(5\) & \(5\) & \([5,10,20,30,40,50]\) \\  & batch\_size & \(512\) & \(256\) & \(256\) & \(512\) & \([128,256,512,1024]\) \\  & horizontal\_filter\_num & \(16\) & \(16\) & \(16\) & \(32\) & \([4,8,16,32,64]\) \\  & vertical\_filter\_num & \(4\) & \(4\) & \(4\) & \(8\) & \([1,2,4,8,16]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & learning\_rate & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & L2\_regularization\_coefficient & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & input\_length & \(10\) & \(5\) & \(5\) & \(10\) & \([5,10,20,30,40,50]\) \\  & batch\_size & \(1024\) & \(512\) & \(512\) & \(1024\) & \([128,256,512,1024]\) \\  & GRU\_unit\_number & \(256\) & \(256\) & \(256\) & \(512\) & \([128,256,512,1024]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & learning\_rate & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & 12\_regularization\_coefficient & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & input\_length & \(50\) & \(20\) & \(20\) & \(40\) & \([5,10,20,30,40,50]\) \\  & batch\_size & \(256\) & \(256\) & \(512\) & \([128,256,512,1024]\) \\  & self\_attention\_bact\_num & \(4\) & \(1\) & \(2\) & \(4\) & \([1,2,3,4]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & learning\_rate & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & L2\_regularization\_coefficient & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & input\_length & \(50\) & \(20\) & \(20\) & \(40\) & \([5,10,20,30,40,50]\) \\  & batch\_size & \(256\) & \(256\) & \(512\) & \([1242\) & \([128,256,512,1024]\) \\  & self\_attention\_bact\_num & \(4\) & \(1\) & \(2\) & \(4\) & \([1,2,3,4]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & L2\_regularization\_coefficient & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & L2\_regularization\_coefficient & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \(10^{-2}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & input\_length & \(5\) & \(5\) & \(5\) & \(5\) & \([5,10,20,30,40,50]\) \\  & LSTM\_size & \(1024\) & \(512\) & \(512\) & \(1024\) & \([128,256,512,1024]\) \\  & GRU\_size & \(2\) & \(2\) & \(2\) & \(2\) & \([1,2,3,4]\) \\  & GRU\_size & \(0.05\) & \(0.05\) & \(0.05\) & \(0.10\) & \([0.05,0.10,0.15,0.20,0.25]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(4\) & \(4\) & \(4\) & \(4\) & \([1,2,3,4]\) \\  & embedding\_size & \(4\) & \(4\) & \(4\) & \(4\) & \(4\) & \([1,2,3,4]\) \\ \hline \multirow{6}{*}{**Fault**} & embedding\_size & \(64\) & \(64\) & \(64\) & \(128\) & \([16,32,64,128]\) \\  & learning\_rate & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) & \(10^{-3}\) & \([10^{-4},10^{-3},10^{-2},10^{-1}]\) \\  & LSTM\_size & \(1024\) & \(512\) & \(512\) & \(1024\) & \([128,256,512,1024]\) \\  & GRU\_size & \(0.05\) & \

## References

* [1] Di-Rong Chen and Tao Sun. Consistency of multiclass empirical risk minimization methods based on convex loss. _The Journal of Machine Learning Research_, 7:2435-2447, 2006.
* [2] Yatong Sun, Bin Wang, Zhu Sun, and Xiaochun Yang. Does every data instance matter? enhancing sequential recommendation by eliminating unreliable data. In _IJCAI_, pages 1579-1585, 2021.
* [3] Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, and Chao Chen. Error-bounded correction of noisy labels. In _ICML_, pages 11447-11457, 2020.
* [4] Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In _ICDM_, pages 197-206, 2018.
* [5] Wassily Hoeffding. _Probability Inequalities for sums of Bounded Random Variables_, pages 409-426. 1994.
* [6] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _AISTATS_, pages 249-256, 2010.
* [7] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.