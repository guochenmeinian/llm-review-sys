# Implicit Regularization Paths of

Weighted Neural Representations

 Jin-Hong Du

Carnegie Mellon University

jinhongd@andrew.cmu.edu

&Pratik Patil

University of California Berkeley

pratikpatil@berkeley.edu

###### Abstract

We study the implicit regularization effects induced by (observation) weighting of pretrained features. For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels. Specifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms. These paths can be interpreted as matching the effective degrees of freedom of ridge estimators fitted with weighted features. For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in [50]. We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity. As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pre-trained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).

## 1 Introduction

In recent years, neural networks have become state-of-the-art models for tasks in computer vision and natural language processing by learning rich representations from large datasets. Pretrained neural networks, such as ResNet, which are trained on massive datasets like ImageNet, serve as valuable resources for new, smaller datasets [32]. These pretrained models reduce computational burden and generalize well in tasks such as image classification and object detection due to their rich feature space [32, 69]. Furthermore, pretrained features or neural embeddings, such as the neural tangent kernel, extracted from these models, serve as valuable representations of diverse data [33, 66].

However, despite their usefulness, fitting models based on pretrained features on large datasets can be challenging due to computational and memory constraints. When dealing with high-dimensional pretrained features and large sample sizes, direct application of even simple linear regression may be computationally infeasible or memory-prohibitive [23, 44]. To address this issue, subsampling has emerged as a practical solution that reduces the dataset size, thereby alleviating the computational and memory burden. Subsampling involves creating smaller datasets by randomly selecting a subset of the original data points. Beyond these computational and memory advantages, subagging can also greatly improve predictive performance in overparameterized regimes, especially near model interpolation thresholds [53]. Moreover, through distributed learning, models fitted on multiple subsampled datasets can be aggregated as an ensemble to provide more stable predictions [20, 21, 51].

There has been growing interest in understanding the effects of subsampling (without replacement) [16, 25, 37, 50, 51]. These works relate subsampling to explicit ridge regularization, assuming eitherGaussian features \(\bm{\phi}\sim\mathcal{N}(\bm{0}_{p},\bm{\Sigma})\) or linearly decomposable features (referred to as _linear features_ in this paper) \(\bm{\phi}=\bm{\Sigma}^{1/2}\bm{z}\), where \(\bm{\Sigma}\in\mathbb{R}^{p\times p}\) is the covariance matrix and \(\bm{z}\in\mathbb{R}^{p}\) contains i.i.d. entries with zero means and bounded \(4+\delta\) moments for some \(\delta>0\). Specifically, [50] establish a connection between implicit regularization induced by subsampling and explicit ridge regularization through a _path_ defined by the tuple \((k/n,\lambda)\), where \(k\) and \(n\) are the subsample size and the full sample size, respectively, and \(\lambda\) is the ridge regularization level. Along this path, any subsample estimator with the corresponding ridge regularization exhibits the same first-order (or estimator equivalence) and second-order (or risk equivalence) asymptotic limits. Moreover, the endpoints of all such paths along the two axes of \(k=n\) (no subsampling) and \(\lambda=0\) (no regularization) span the same range. Although these results have been demonstrated for linear features, [50] also numerically observe similar equivalence behavior in more realistic contexts and propose conjectures for random features and kernel features based on heuristic "universality" justifications. However, extending these results to encompass more general feature structures and other sampling schemes remains an open question.

Towards answering this question, in this paper, we view subsampling as a weighted regression problem [67]. This perspective allows us to study the equivalence in its most general form, considering arbitrary feature structures and weight structures. The general weight matrix approach used in this study encompasses various applications, including subsampling, bootstrapping, variance-adaptive weighting, survey, and importance weighting, among others. By interpreting subsampling as a weighted regression problem, we leverage recent tools from free probability theory, which have been developed to analyze feature sketching [39; 42; 54]. Building on these theoretical tools, we establish implicit regularization paths for general weighting and feature structures. We summarize our main results below and provide an overview of our results in the context of recent related work in Table 1.

### Summary of results and paper outline

We summarize our main results and provide an outline for the paper below.

* _Paths of weighted representations._ In Section 3, we demonstrate that general weighted models exhibit first-order equivalence along a path (Theorem 1) when the weight matrices are asymptotically independent of the data matrices. This path of equivalence can be computed directly from the data using the formula provided in Equation (2). Furthermore, we provide a novel interpretation of this path in terms of matching effective degrees of freedom of models along the path for general feature structures when the weights correspond to those arising from subsampling (Theorem 2).
* _Paths of subsampled representations._ We further specialize our general result in Theorem 2 for the weights induced by subsampling without replacement to structured features in Section 3.2. These include results for linear random features, nonlinear random features, and kernel features, as shown in Propositions 3-5, respectively. The latter two results also resolve Conjectures 7 and 8 raised by [50] regarding subsampling regularization paths for random and kernel features, respectively.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Main analysis** & **Feature structure** & **Weight structure** & **Reference** \\ \hline \multirow{4}{*}{Risk characterization} & Gaussian & subsampling & [37] \\  & linear & subsampling & [51] \\  & Gaussian & bootstrapping & [5; 16; 17] \\ \cline{2-4}  & linear & subsampling & [50] \\ \multirow{2}{*}{Estimator equivalence} & general & general & Theorem 1 \\  & general & subsampling & Theorem 2 \\ \multirow{2}{*}{Risk equivalence} & linear, random, kernel & subsampling & Propositions 3â€“5 \\ \cline{2-4}  & linear & subsampling & [50] \\ \multirow{2}{*}{Risk equivalence} & general & general & Theorem 6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Overview of related work on the equivalence of implicit regularization and explicit ridge regularization.

* _Risk equivalences and tuning._ In Section 4, we demonstrate that an ensemble of weighted models has general quadratic risk equivalence on the path, with an error term that decreases inversely as \(1/M\) as the number of ensemble size \(M\) increases (Theorem 6). The risk equivalence holds for both in-distribution and out-of-distribution settings. For subsampling general features, we derive an upper bound for the optimal subsample size (Proposition 7) and propose a cross-validation method to tune the subsample and ensemble sizes (Algorithm 1), validated on real datasets in Section 4.3.

This level of generality is achievable because we do not analyze the risk of either the full model or the weighted models in isolation. Instead, we relate these two sets of models, allowing us to maintain weak assumptions about the features. The key assumption underlying our results is the asymptotic freeness of weight matrices with respect to the data matrices. While directly testing this assumption is generally challenging, we verify its validity through its consequences on real datasets in Section 4.3.

### Related literature

We provide a brief account of other related work below to place our work in a better context.

_Linear features._ Despite being overparameterized, neural networks generalize well in practice [70; 71]. Recent work has used high-dimensional "linearized" networks to investigate the various phenomena that arise in deep learning, such as double descent [12; 46; 48], benign overfitting [10; 35; 45], and scaling laws [7; 19; 66]. This literature analyzes linear regression using statistical physics [14; 60] and random matrix theory [22; 30]. Risk approximations hold under random matrix theory assumptions [6; 30; 66] in theory and apply empirically on a variety of natural data distributions [43; 60; 66].

_Random and kernel features._ Random feature regression, initially introduced in [56] as a way to scale kernel methods, has recently been used for theoretical analysis of neural networks and trends of double descent in deep networks [1; 46]. The generalization of kernel ridge regression has been studied in [11; 40; 57]. The risks of kernel ridge regression are also analyzed in [9; 19; 29]. The neural representations we study are motivated by the neural tangent kernel (NTK) and related theoretical work on ultra-wide neural networks and their relationships to NTKs [34; 68].

_Resampling analysis._ Resampling and weighted models are popular in distributed learning to provide more stable predictions and handle large datasets [20; 21; 51]. Historically, for ridge ensembles, [36; 61] derived risk asymptotics under Gaussian features. Recently, there has been growing interest in analyzing the effect of subsampling in high-dimensional settings. [37] considered least squares ensembles obtained by subsampling, where the final subsampled dataset has more observations than the number of features. For linear models in the underparameterized regime, [59] also provide certain equivalences between subsampling and iterative least squares approaches. The asymptotic risk characterization for general data models has been derived by [51]. [25; 50] extended the scope of these results by characterizing risk equivalences for both optimal and suboptimal risks and for arbitrary feature covariance and signal structures. Very recently, different resampling strategies for high-dimensional supervised regression tasks have been analyzed by [17] under isotropic Gaussian features. Cross-validation methods for tuning the ensemble of ridge estimators and other penalized estimators are discussed in [13; 25; 26]. Our work adds to this literature by considering ensembles of models with general weighting and feature structures.

## 2 Preliminaries

In this section, we formally define our weighted estimator and state the main assumption on the weight matrix. Let \(f_{\text{nn}}\colon\mathbb{R}^{d}\to\mathbb{R}^{p}\) be a pretrained model. Let \(\{(\bm{x}_{i},y_{i})\colon i=1,\ldots,n\}\) in \(\mathbb{R}^{d}\times\mathbb{R}\) be the given dataset. Applying \(f_{\text{nn}}\) to the raw dataset, we obtain the pretrained features \(\bm{\phi}_{i}=f_{\text{nn}}(\bm{x}_{i})\) for \(i=1,\ldots,n\) as the resulting neural representations or neural embeddings. In matrix notation, we denote the pretrained feature matrix by \(\bm{\Phi}=[\bm{\phi}_{1},\ldots,\bm{\phi}_{n}]^{\top}\in\mathbb{R}^{n\times p}\). Let \(\bm{W}\in\mathbb{R}^{n\times n}\) be a general weight matrix used for weighting the observations. The weight matrix \(\bm{W}\) is allowed to be asymmetric, in general.

We consider fitting ridge regression on the weighted dataset \((\bm{W}\bm{\Phi},\bm{W}\bm{y})\). Given a ridge penalty \(\lambda\), the ridge estimator fitted on the weighted dataset is given by:

\[\widehat{\bm{\beta}}_{\bm{W},\lambda}:=\operatorname*{argmin}_{\bm{\beta}\in \mathbb{R}^{p}}\left(\frac{\|\bm{W}\bm{y}-\bm{W}\bm{\Phi}\bm{\beta}\|_{2}^{2}} {n}+\lambda\|\bm{\beta}\|_{2}^{2}\right)=(\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W} \bm{\Phi}+n\lambda\bm{I}_{p})^{\dagger}\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W}\bm{y}.\] (1)

In the definition above, we allow for \(\lambda=0\), in which case the corresponding ridgeless estimator is defined as the limit \(\lambda\to 0^{+}\). For \(\lambda<0\), we use the Moore-Penrose pseudoinverse. An important special case is where \(\bm{W}\) is a diagonal matrix, in which case the above estimator reduces to weighted ridge regression. This type of weight matrix encompasses various applications, such as resampling, bootstrapping, and variance weighting. Our main application in this paper will be subsampling.

For our theoretical results, we assume that the weight matrix \(\bm{W}\) preserves some spectral structure of the feature matrix \(\bm{\Phi}\). This assumption is captured by the condition of _asymptotic freeness_ between \(\bm{W}^{\top}\bm{W}\) and the feature Gram matrix \(\bm{\Phi}\bm{\Phi}^{\top}\). Asymptotic freeness is a concept from free probability theory [64].

**Assumption A** (Weight structure).: Let \(\bm{W}^{\top}\bm{W}\) and \(\bm{\Phi}\bm{\Phi}^{\top}/n\) converge almost surely to bounded operators that are infinitesimally free with respect to \((\operatorname{tr}[\cdot],\operatorname{tr}[\bm{C}(\cdot)])\) for any \(\bm{C}\) independent of \(\bm{W}\) with \(\|\bm{C}\|_{\operatorname{tr}}\) uniformly bounded. Additionally, let \(\bm{W}^{\top}\bm{W}\) have a limiting \(S\)-transform that is analytic on the lower half of the complex plane.

At a high level, Assumption A captures the notion of independence but is adapted for non-commutative random variables of matrices. We provide background on free probability theory and asymptotic freeness in Appendix A.3. Here, we briefly list a series of invertible transformations from free probability to help define the \(S\)-transform [47]. The Cauchy transform is given by \(\mathcal{G}_{\bm{A}}(z)=\operatorname{tr}[(z\bm{I}-\bm{A})^{-1}]\). The moment generating series is given by \(\mathcal{M}_{\bm{A}}(z)=z^{-1}\mathcal{G}_{\bm{A}}(z^{-1})-1\). The \(S\)-transform is given by \(\mathcal{S}_{\bm{A}}(w)=(1+w^{-1})\mathcal{M}_{\bm{A}}^{(-1)}(w)\). These are the Cauchy transform (negative of the Stieltjes transform), moment generating series, and \(S\)-transform of \(\bm{A}\), respectively. Here, \(\mathcal{M}_{\bm{A}}^{(-1)}\) denotes the inverse under the composition of \(\mathcal{M}_{\bm{A}}\). The notation \(\operatorname{tr}[\bm{A}]\) denotes the average trace \(\operatorname{tr}[\bm{A}]/p\) of \(\bm{A}\in\mathbb{R}^{p\times p}\).

The freeness of a pair of matrices \(\bm{A}\) and \(\bm{B}\) means that the eigenvectors of one are completely unaligned or incoherent with those of the other. For example, if \(\bm{A}=\bm{U}\bm{R}\bm{U}^{\top}\) for a uniformly random unitary matrix \(\bm{U}\) drawn independently of the positive semidefinite \(\bm{B}\) and \(\bm{R}\), then \(\bm{A}\) and \(\bm{B}\) are almost surely asymptotically infinitesimally free [15]. Other well-known examples include Wigner matrices, which are asymptotically free with respect to deterministic matrices [4, Theorem 5.4.5]. Gaussian matrices, where the Gram matrix \(\bm{G}=\bm{\Phi}\bm{\Phi}^{\top}/n=\bm{U}(\bm{V}\bm{V}^{\top}/n)\bm{U}^{\top}\) and any deterministic \(\bm{S}\), are almost surely asymptotically free [47, Chapter 4, Theorem 9]. Although not proven in full generality, it is expected that diagonal matrices are asymptotically free from data Gram matrices constructed using i.i.d. data. In Section 3.2, we will provide additional examples of feature matrices, such as random and kernel features from machine learning, for which our results apply.

Our results involve the notion of degrees of freedom from statistical optimism theory [27, 28]. Degrees of freedom in statistics count the number of dimensions in which a statistical model may vary, which is simply the number of variables for ordinary linear regression. To account for regularization, this notion has been extended to _effective degrees of freedom_ (Chapter 3 of [31]). Under some regularity conditions, from Stein's relation [63], the degrees of freedom of a predictor \(\widehat{f}\) are measured by the trace of the operators \(\bm{y}\mapsto(\partial/\partial\bm{y})\widehat{f}(\bm{\Phi})\). For the ridge estimator \(\widehat{\bm{\beta}}_{\bm{I},\mu}\) fitted on \((\bm{\Phi},\bm{y})\) with penalty \(\mu\), the degrees of freedom is consequently the trace of its prediction operator \(\bm{y}\mapsto\bm{\Phi}(\bm{\Phi}^{\top}\bm{\Phi}+\mu I_{p})^{\dagger}\bm{\Phi }^{\top}\bm{y}\), which is also referred to as the ridge smoother matrix. That is, \(\operatorname{df}(\widehat{\bm{\beta}}_{\bm{I},\mu})=\operatorname{tr}[\bm{\Phi }^{\top}\bm{\Phi}(\bm{\Phi}^{\top}\bm{\Phi}+\mu I_{p})^{\dagger}]\). We denote the normalized degrees of freedom by \(\overline{\operatorname{df}}=\operatorname{df}/n\). Note that \(\overline{\operatorname{df}}(\widehat{\bm{\beta}}_{\bm{I},\mu})\leq\min\{n,p \}/n\leq 1\).

Finally, we express our asymptotic results using the asymptotic equivalence relation. Consider sequences \(\{\bm{A}_{n}\}_{n\geq 1}\) and \(\{\bm{B}_{n}\}_{n\geq 1}\) of (random or deterministic) matrices (which includes vectors and scalars). We say that \(\bm{A}_{n}\) and \(\bm{B}_{n}\) are _equivalent_ and write \(\bm{A}_{n}\simeq\bm{B}_{n}\) if \(\lim_{p\to\infty}|\operatorname{tr}[\bm{C}_{n}(\bm{A}_{n}-\bm{B}_{n})]|=0\) almost surely for any sequence \(\bm{C}_{n}\) of matrices with bounded trace norm such that \(\limsup\|\bm{C}_{n}\|_{\operatorname{tr}}<\infty\) as \(n\to\infty\). Our forthcoming results apply to a sequence of problems indexed by \(n\). For notational simplicity, we omit the explicit dependence on \(n\) in our statements.

Implicit regularization paths

We begin by characterizing the implicit regularization induced by weighted pretrained features. We will show that the degrees of freedom of the unweighted estimator \(\widehat{\bm{\beta}}_{\bm{I},\mu}\) on the full data \((\bm{\Phi}_{\bm{\lambda}}\bm{y})\) with regularization parameter \(\mu\) are equal to the degrees of freedom of the weighted estimator \(\widehat{\bm{\beta}}_{\bm{W},\lambda}\) for some regularization parameter \(\lambda\). For estimator equivalence, our data-dependent set of weighted ridge estimators \((\bm{W},\lambda)\) that connect to the unweighted ridge estimator \((\bm{I},\mu)\) is defined in terms of "matching" effective degrees of freedom of component estimators in the set.

To state the upcoming result, denote the Gram matrix of the weighted data as \(\bm{G}_{\bm{W}}=\bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}/n\) and the Gram matrix of the unweighted data as \(\bm{G}_{\bm{I}}=\bm{\Phi}\bm{\Phi}^{\top}/n\). Furthermore, let \(\lambda^{+}_{\min}(\bm{A})\) denote the minimum positive eigenvalue of a symmetric matrix \(\bm{A}\).

**Theorem 1** (Implicit regularization of weighted representations).: _For \(\bm{G}_{\bm{I}}\in\mathbb{R}^{n\times n}\), suppose that the weight matrix \(\bm{W}\in\mathbb{R}^{n\times n}\) satisfies Assumption 1 and \(\limsup\|\bm{y}\|_{2}^{2}/n<\infty\) as \(n\to\infty\). For any \(\mu>-\liminf_{n\to\infty}\lambda^{+}_{\min}(\bm{G}_{\bm{I}})\), let \(\lambda>-\lambda^{+}_{\min}(\bm{G}_{\bm{W}})\) be given by the following equation:_

\[\lambda=\mu/\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\overline{\mathrm{df}}( \widehat{\bm{\beta}}_{\bm{I},\mu})),\] (2)

_where \(\mathcal{S}_{\bm{W}^{\top}\bm{W}}\) is the \(S\)-transform of the operator \(\bm{W}^{\top}\bm{W}\). Then, as \(n\to\infty\), it holds that:_

\[\mathrm{df}(\widehat{\bm{\beta}}_{\bm{W},\lambda})\simeq\mathrm{df}(\widehat{ \bm{\beta}}_{\bm{I},\mu})\quad\text{and}\quad\widehat{\bm{\beta}}_{\bm{W}, \lambda}\simeq\widehat{\bm{\beta}}_{\bm{I},\mu}.\] (3)

In other words, to achieve a target regularization of \(\mu\) on the unweighted data, Theorem 1 provides a method to compute the regularization penalty \(\lambda\) with given weights \(\bm{W}\) from the available data using (2). The weighted estimator then has asymptotically the same degrees of freedom as the unweighted estimator. This means that the level of effective regularization of the two estimators is the same. Moreover, the estimators themselves are structurally equivalent; that is, \(\bm{c}^{\top}(\widehat{\bm{\beta}}_{\bm{W},\lambda}-\widehat{\bm{\beta}}_{\bm {I},\mu})\xrightarrow{\text{a.s.}}0\) for every constant vector \(\bm{c}\) with bounded norm. The estimator equivalence in Theorem 1 is a "first-order" result, while we will also characterize the "second-order" effects in Section 4.

The notable aspect of Theorem 1 is its generality. The equivalence results hold for a wide range of weight matrices and allow for negative values for the regularization levels. Furthermore, we have not made any direct assumptions about the feature matrix \(\bm{\Phi}\), the weight matrix \(\bm{W}\), and the response vector \(\bm{y}\) (other than mild bounded norms). The main underlying ingredient is the asymptotic freeness between \(\bm{W}\) and \(\bm{\Phi}\), which we then exploit using tools developed in [39] in the context of feature sketching. We discuss special cases of interest for \(\bm{W}\) and \(\bm{\Phi}\) in the upcoming Sections 3.1 and 3.2.

### Examples of weight matrices

There are two classes of weighting matrices that are of practical interest:

* **Non-diagonal weighting matrices.** One can consider observation sketching, which involves some random linear combinations of the rows of the data matrix. Such observation sketching is beneficial for privacy, as it scrambles the rows of the data matrix, which may contain identifiable information about individuals. It also helps in reducing the effect of non-i.i.d. data that arise in time series or spatial data, where one wants to smooth away the impact of irregularities or non-stationarity.
* **Diagonal weighting matrices.** When observations are individually weighted, \(\bm{W}\) is a diagonal matrix, which includes scenarios such as resampling, bootstrapping, and subsampling. Note that even with subsampling, one can have a non-binary diagonal weighting matrix. For example, one can consider sampling with replacement or sampling with a particular distribution, which yields non-binary diagonal weighting matrices. Other examples of non-binary diagonal weighting matrices include inverse-variance weighting sampling to mitigate the effects of heterogeneous variations if the responses have different variances for different units.

In general, the set of equivalent weighted estimators depends on the corresponding \(S\)-transform as in (2), and it can be numerically evaluated. When focusing on subsampling without replacement, the data-dependent path for equivalent estimators with associated subsampling and regularization levels can be explicitly characterized in the following result by analyzing the \(S\)-transform of subsampling operators.

**Theorem 2** (Regularization paths due to subsampling).: _For a subsampling matrix \(\bm{W}^{(k)}\) consisting of \(k\) unit diagonal entries, the path (2) in terms of \((k,\lambda)\) simplifies to:_

\[(1-\mathsf{df}/n)\cdot(1-\lambda/\mu)=(1-k/n),\] (4)

_where we denote by \(\mathsf{df}=\mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu})=\mathsf{df}(\widehat {\bm{\beta}}_{\bm{W},\lambda})\) for notational simplicity._

The relation (4) is remarkably simple, yet quite general! It provides an interplay between the normalized target complexity \(\mathsf{df}/n\), regularization inflation \(\lambda/\mu\), and subsample fraction \(k/n\):

\[(1-\text{normalized complexity})\cdot(1-\text{regularization inflation})=(1- \text{subsample fraction}).\] (5)

Since the normalized target complexity and subsample fraction are no greater than one, (5) also implies that the regularization level \(\lambda\) for the subsample estimator is always lower than the regularization level \(\mu\) for the full estimator. In other words, subsampling induces (positive) implicit regularization, reducing the need for explicit ridge regularization. This is verified numerically in Figure 1.

For a fixed target regularization amount \(\mu\), the degrees of freedom \(\mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu})\) of the ridge estimator on full data is fixed. Thus, we can observe that the path in the \((k/n,\lambda)\)-plane is a line. There are two extreme cases: (1) when the subsample size \(k\) is close to \(n\), we have \(\mu\approx\lambda\); and (2) when the subsample size is near \(0\), we have \(\mu\approx\infty\). When \(\lambda=0\), the effective regularization level \(\lambda\) is such that \(\mathsf{df}(\widehat{\bm{\beta}}_{\bm{W}^{(k)},\lambda})=\mathsf{df}(\widehat {\bm{\beta}}_{\bm{I},\mu})=k\), which we find to be a neat relation!

Beyond subsampling without replacement, one can also consider other subsample matrixs. For example, for bootstrapping \(k\) entries, we observe a similar equivalent path in Figure 5. Additionally, for random sample reweighting, as shown in Figure 6, we also observe certain equivalence behaviors of degrees of freedom. This indicates that Theorem 1 also applies to more general weighting schemes.

### Examples of feature matrices

As mentioned in Section 2, when the feature matrix \(\bm{\Phi}\) consists of i.i.d. Gaussian features, any deterministic matrix \(\bm{W}\) satisfies the condition stated in Assumption A. However, our results are not limited to Gaussian features. In this section, we will consider more general families of features commonly analyzed in machine learning and demonstrate the applicability of our results to them.

_(1) Linear features._ As a first example, we consider linear features composed of (multiplicatively) transformed i.i.d. entries with sufficiently bounded moments by a deterministic covariance matrix.

**Proposition 3** (Regularization paths with linear features).: Suppose the feature \(\bm{\phi}\) can be decomposed as \(\bm{\phi}=\bm{\Sigma}^{1/2}\bm{z}\), where \(\bm{z}\in\mathbb{R}^{p}\) contains i.i.d. entries \(z_{i}\) for \(i=1,\dots,p\) with mean \(0\), variance \(1\), and satisfies \(\mathbb{E}[|z_{i}|^{4+\mu}]\leq M_{\mu}<\infty\) for some \(\mu>0\) and a constant \(M_{\mu}\), and \(\bm{\Sigma}\in\mathbb{R}^{p\times p}\) is a deterministic

Figure 1: Equivalence under subsampling. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection \(\mathbb{E}_{\bm{W}}|\bm{a}^{\top}\widehat{\bm{\beta}}_{\bm{W},\lambda}|\) where \(\bm{a}\sim\mathcal{N}(\bm{0}_{p},\bm{I}_{p}/p)\). In both heatmaps, the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths by matching empirical degrees of freedom. The data is generated according to Appendix F.1 with \(n=10000\) and \(p=1000\), and the results are averaged over \(M=100\) random weight matrices \(\bm{W}\).

symmetric matrix with eigenvalues uniformly bounded between constants \(r_{\min}>0\) and \(r_{\max}<\infty\). Then, as \(n,p\to\infty\) such that \(p/n\to\gamma>0\), the equivalences in (3) hold along the path (4).

Features of this type are common in random matrix theory [8] and in a wide range of applications, including statistical physics [14, 60], high-dimensional statistics [22, 55, 58], machine learning [18], among others. The generalized path (2) in Theorem 2 recovers the path in Proposition 4 of [50]. Although the technique in this paper is quite different and more general than that of [50].

_(2) Kernel features._ As the second example, Theorem 2 also applies to kernel features. Kernel features are a generalization of linear features and lift the input feature space to a high- or infinite-dimensional feature space by applying a feature map \(\bm{x}\mapsto\bm{\phi}(\bm{x})\). Kernel methods use the kernel function \(K(\bm{x}_{i},\bm{x}_{j})=\langle\bm{\phi}(\bm{x}_{i}),\bm{\phi}(\bm{x}_{j})\rangle\) to compute the inner product in the lifted space.

**Proposition 4** (Regularization paths with kernel features).: Suppose the same conditions as in Proposition 3 and the kernel function is of the form \(K(\bm{x}_{i},\bm{x}_{j})=g({\left\|\bm{x}_{i}\right\|}_{2}^{2}/p,{\langle\bm{ x}_{i},\bm{x}_{j}\rangle}/p,{\left\|\bm{x}_{j}\right\|}_{2}^{2}/p)\), where \(g\) is \(\mathcal{C}^{1}\) around \((\tau,\tau,\tau)\) and \(\mathcal{C}^{3}\) around \((\tau,0,\tau)\) and \(\tau:=\lim_{p\to\infty}\operatorname{tr}[\bm{\Sigma}]/d\). Then, as \(n\to\infty\), the equivalences in (3) hold in probability along the path (4).

The assumption in Proposition 4 is commonly used in the risk analysis of kernel ridge regression [9, 19, 29, 57], among others. Here, \(\mathcal{C}^{k}\) denotes the class of functions that are \(k\)-times continuously differentiable. It includes neural tangent kernels (NTKs) as a special case. Proposition 4 confirms Conjecture 8 of [50] for these types of kernel functions.

_(3) Random features._ Finally, we consider random features that were introduced by [56] as a way to scale kernel methods to large datasets. Linked closely to two-layer neural networks [46], the random feature model has \(f_{\text{m}}(\bm{x})=\sigma(\bm{F}\bm{x})\), where \(\bm{F}\in\mathbb{R}^{d\times p}\) is some randomly initialized weight matrix, and \(\sigma:\mathbb{R}\to\mathbb{R}\) is a nonlinear activation function applied element-wise to \(\bm{F}\bm{x}\).

**Proposition 5** (Regularization paths with random features).: Suppose \(\bm{x}_{i}\sim\mathcal{N}(\bm{0},\bm{\Sigma})\) and the activation function \(\sigma:\mathbb{R}\to\mathbb{R}\) is differentiable almost everywhere and there are constants \(c_{0}\) and \(c_{1}\) such that \(|\sigma(x)|,|\sigma^{\prime}(x)|\leq c_{0}e^{c_{1}x}\), whenever \(\sigma^{\prime}(x)\) exists. Then, as \(n,p,d\to\infty\) such that \(p/n\to\gamma>0\) and \(d/n\to\xi>0\), the equivalences in (3) hold in probability along the path (4).

As mentioned in the related work, random feature models have recently been used as a standard model to study various generalization phenomena observed in neural networks theoretically [1, 46]. Proposition 5 resolves Conjecture 7 of [50] under mild regularity conditions on the activation function.

It is worth noting that the prior works mentioned above, including [50], have focused on first characterizing the risk asymptotics in terms of various population quantities for each of the cases above. In contrast, our work in this paper deviates from these approaches by not expressing the risk in population quantities but rather by directly relating the estimators at different regularization levels. In the next section, we will explore the relationship between their squared prediction risks.

Figure 2: Equivalence of degrees of freedom for various feature structures under subsampling. The three panels correspond to linear features, random features with ReLU activation function (2-layer), and kernel features (polynomial kernel with degree 3 and without intercept), respectively. In all heatmaps, the red color lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths by matching the empirical degrees of freedom. The data is generated according to Appendix F.1 with \(n=5000\) and \(p=500\), and the results are averaged over \(M=100\) random weight matrices \(\bm{W}\).

Prediction risk asymptotics and risk estimation

The results in the previous section provide first-order equivalences of the estimators, which are related to the bias of the estimators. In practice, we are also interested in the predictive performance of the estimators. In this section, we investigate the second-order equivalence of weighting and ridge regularization through ensembling. Specifically, we show that aggregating estimators fitted on different weighted datasets also reduces the additional variance. Furthermore, the prediction risks of the full-ensemble weighted estimator and the unweighted estimator also match along the path.

Before presenting our risk equivalence result, we first introduce some additional notation. Assume there are \(M\) i.i.d. weight matrices \(\bm{W}_{1},\ldots,\bm{W}_{M}\in\mathbb{R}^{n\times n}\). The _\(M\)-ensemble_ estimator is defined as:

\[\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}=M^{-1}\sum_{m=1}^{M} \widehat{\bm{\beta}}_{\bm{W}_{m},\lambda},\] (6)

and its performance is quantified by the conditional squared prediction risk, given by:

\[R(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda})=\mathbb{E}_{\bm{ x}_{0},y_{0}}[(y_{0}-\bm{\phi}_{0}^{\top}\widehat{\bm{\beta}}_{\bm{W}_{1:M}, \lambda})^{2}\ |\ \bm{\Phi},\bm{y},\{\bm{W}_{m}\}_{m=1}^{M}],\] (7)

where \((\bm{x}_{0},y_{0})\) is a test point sampled independently from some distribution \(P_{\bm{x}_{0},y_{0}}\) that may be different from the training distribution \(P_{\bm{x},y}\), and \(\bm{\phi}_{0}=f_{\text{m}}(\bm{x}_{0})\) is the pretrained feature at the test point. The covariance matrix of the test features \(\bm{\phi}_{0}\) is denoted by \(\bm{\Sigma}_{0}\). When \(P_{\bm{x}_{0},y_{0}}=P_{\bm{x},y}\), we refer to it as the in-distribution risk. On the other hand, when \(P_{\bm{x}_{0},y_{0}}\) differs from \(P_{\bm{x},y}\), we refer to it as the out-of-distribution risk. Note that the conditional risk \(R_{M}\) is a scalar random variable that depends on both the dataset \((\bm{\Phi},\bm{y})\) and the weight matrix \(\bm{W}_{m}\) for \(m\in[M]\). Our goal in this section is to analyze the prediction risk of the ensemble estimator (6) for any ensemble size \(M\).

**Theorem 6** (Risk equivalence along the path).: _Under the setting of Theorem 1, assume that the operator norm of \(\bm{\Sigma}_{0}\) is uniformly bounded in \(p\) and that each response variable \(y_{i}\) for \(i=1,\ldots,n\) has mean \(0\) and satisfies \(\mathbb{E}[|y_{i}|^{4+\mu}]\leq M_{\mu}<\infty\) for some \(\mu,M_{\mu}>0\). Then, along the path (4),_

\[R(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda})\simeq R(\widehat{ \bm{\beta}}_{\bm{I},\mu})+\frac{C}{M}\operatorname{\overline{tr}}[(\bm{G}_{ \bm{I}}+\mu\bm{I})^{\dagger}\bm{y}\bm{y}^{\top}(\bm{G}_{\bm{I}}+\mu\bm{I}_{n}) ^{\dagger}],\] (8)

_where the constant \(C\) is given by:_

\[C=-\partial\mu/\partial\lambda\cdot\lambda^{2}\mathcal{S}^{\prime}_{\bm{W}^{ \top}\bm{W}}(-\mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu}))\operatorname{ \overline{tr}}[(\bm{G}_{\bm{I}}+\mu\bm{I})^{\dagger}(\bm{\Phi}\bm{\Sigma}_{0} \bm{\Phi}^{\top}/n)(\bm{G}_{\bm{I}}+\mu\bm{I})^{\dagger}].\] (9)

At a high level, Theorem 6 provides a bias-variance-like risk decomposition for both the squared risks of weighted ensembles. The risk of the weighted predictor is equal to the risk of the unweighted equivalent implicit ridge regressor (bias) plus a term due to the randomness due to weighting (variance). The inflation factor \(C\) controls the magnitude of this term, and it decreases at a rate of \(1/M\) as the ensemble size \(M\) increases (see Figure 7 for a numerical verification of this rate). Therefore, by using a resample ensemble with a sufficiently large size \(M\), we can retain the statistical properties of the full ridge regression while reducing memory usage and increasing parallelization.

Theorem 6 extends the risk equivalence results in [50; 52]. Compared to previous results, Theorem 6 provides a broader risk equivalence that holds for general weight and feature matrices, as well as an arbitrary ensemble size \(M\). It is important to note that Theorem 6 holds even when the test distribution differs from the training data, making it applicable to out-of-distribution risks. Furthermore, our results do not rely on any specific distributional assumptions for the response vector, making them applicable in a model-free setting. The key idea behind this result is to exploit asymptotic freeness between the subsample and data matrices. Next, we will address the question of optimal tuning.

### Optimal oracle tuning

As in Theorem 2, we next analyze various properties related to optimal subsampling weights and their implications for the risk of optimal ridge regression. Recall that the subsampling matrix \(\bm{W}^{(k)}\) is a diagonal matrix with \(k\in\{1,\ldots,n\}\) nonzero diagonal entries, which is parameterized by the subsample size \(k\). Note that the optimal regularization parameter \(\mu^{*}\) for the full data (\(\bm{W}^{(k)}=\bm{I}\) or \(k=n\)) is a function of the distribution of pretrained data and the test point. Based on the risk equivalence in Theorem 6, there exists an optimal path of \((k,\lambda)\) with the corresponding full-ensembleestimator \(\widehat{\bm{\beta}}_{\bm{W}^{(k)}_{1:\infty},\lambda}:=\lim_{M\to\infty}\widehat{ \bm{\beta}}_{\bm{W}^{(k)}_{1:M},\lambda}\) that achieves the optimal predictive performance at \((n,\mu^{*})\). In particular, the ridgeless ensemble with \(\lambda^{*}=0\) happens to be on the path. From previous work [25; 50], the optimal subsample size \(k^{*}\) for \(\lambda^{*}=0\) has the property that \(k^{*}\leq p\) under linear features. We show in the following that this property can be extended to include general features.

**Proposition 7** (Optimal subsample ratio).: Assume the subsampling matrix \(\bm{W}\) as defined in Theorem 2. Let \(\mu^{*}=\operatorname*{argmin}_{\mu\geq 0}R(\widehat{\bm{\beta}}_{\bm{W}^{(k)}_{ 1:\infty},\mu})\). Then the corresponding subsample size satisfies:

\[k^{*}=\mathsf{df}(\widehat{\bm{\beta}}_{\bm{W}^{(k)}_{1:\infty},\mu^{*}})\leq \operatorname*{rank}(\bm{G}_{\bm{I}}).\] (10)

The optimal subsample size \(k^{*}\) obtained from Proposition 7 is asymptotically optimal. For linear features, in the underparameterized regime where \(n>p\), [25; 50] show that the optimal subsample size \(k^{*}\) is asymptotically no larger than \(p\). This result is covered by Proposition 7 by noting that \(\operatorname*{rank}(\bm{G}_{\bm{I}})\leq p\) under linear features. It is interesting and somewhat surprising to note that in the underparameterized regime (when \(p\leq n\)), we do not need more than \(p\) observations to achieve the optimal risk. In this sense, the optimal subsampled dataset is always overparameterized.

When the limiting risk profiles \(\mathscr{R}(\gamma,\psi,\mu):=\lim_{p/n\to\gamma,p/k\to\Psi}R(\widehat{\bm{ \beta}}_{\bm{W}^{(k)}_{1:\infty},\mu})\) exist for subsample ensembles, the limiting risk of the optimal ridge predictor \(\inf_{\mu\geq 0}\mathscr{R}(\gamma,\gamma,\mu)\) is monotonically decreasing in the limiting sample aspect ratio \(\gamma\)[50]. This also (provably) confirms the sample-wise monotonicity of optimally-tuned risk for general features in an asymptotic sense [48]. Due to the risk equivalence in Theorem 6, for any \(\mu>0\), there exists \(\psi\) such that \(\mathscr{R}(\gamma,\gamma,\mu)=\mathscr{R}(\gamma,\psi,0)\). This implies that \(\inf_{\mu\geq 0}\mathscr{R}(\gamma,\gamma,\mu)=\inf_{\psi\geq\gamma} \mathscr{R}(\gamma,\psi,0)\). In other words, tuning over subsample sizes with sufficiently large ensembles is equivalent to tuning over the ridge penalty on the full data.

### Data-dependent tuning

As suggested by Proposition 7, the optimal subsample size is smaller than the rank of the Gram matrix. This result has important implications for real-world datasets where the number of observations (\(n\)) is much larger than the number of features (\(p\)). In such cases, instead of using the entire dataset, we can efficiently build small ensembles with a subsample size \(k\leq p\). This approach is particularly beneficial when \(n\) is significantly higher than \(p\), for example, when \(n=1000p\). By fitting ensembles with only \(M=100\) base predictors, we can potentially reduce the computational burden while still achieving optimal predictive performance. Furthermore, this technique can be especially valuable in scenarios where computational resources are limited or when dealing with massive datasets that cannot be easily processed in their entirety.

In the following, we propose a method to determine the optimal values of the regularization parameter \(\mu^{*}\) for the full ridge regression, as well as the corresponding subsample size \(k^{*}\) and the optimal ensemble size \(M^{*}\). According to Theorem 6, the optimal value of \(M^{*}\) is theoretically infinite. However, in practice, the prediction risk of the \(M\)-ensemble predictor decreases at a rate of \(1/M\) as \(M\) increases. Therefore, it is important to select a suitable value of \(M\) that achieves the desired level of performance while considering computational constraints and the specified error budget. By carefully choosing an appropriate \(M\), we can strike a balance between model accuracy and efficiency, ensuring that the subsampled neural representations are effectively used in downstream tasks.

Consider a grid of subsample size \(\mathcal{K}_{n}\subseteq\{1,\dots,n\}\); for instance, \(\mathcal{K}_{n}=\{0,k_{0},2k_{0},\dots,n\}\) where \(k_{0}\) is a subsample size unit. For a prespecified subsample size \(k\in\mathcal{K}_{n}\) and ensemble size

Figure 3: Equivalence in pretrained features of pretrained ResNet-50 on Flowers-102 datasets.

\(\mathbb{N}\), suppose we have multiple risk estimates \(\widehat{R}_{m}\) of \(R_{m}\) for \(m=1,\ldots,M_{0}\). The squared risk decomposition (51, Eq (7)) along with the equivalence path (8) implies that \(R_{m}=m^{-1}R_{1}+\left(1-m^{-1}\right)R_{\infty}\), for \(m=1,\ldots,M_{0}\). Summing these equations yields \(\sum_{m=1}^{M_{0}}R_{m}=\sum_{m=1}^{M_{0}}\frac{1}{m}R_{1}+\sum_{m=1}^{M_{0}} \left(1-m^{-1}\right)R_{\infty}\). Thus, we can estimate \(R_{\infty}\) by:

\[\widehat{R}_{\infty}=\Big{(}\sum_{m=1}^{M_{0}}\widehat{R}_{m}-\sum_{m=1}^{M_{0 }}m^{-1}\widehat{R}_{1}\Big{)}\ /\sum_{m=1}^{M_{0}}\big{(}1-m^{-1}\big{)}.\] (11)

Then, the extrapolated risk estimates \(\widehat{R}_{m}\) (with \(m>M_{0}\)) are defined as:

\[\widehat{R}_{m}:=m^{-1}\widehat{R}_{1}+\left(1-m^{-1}\right)\widehat{R}_{ \infty}\quad\text{for}\quad m>M_{0}.\] (12)

The meta-algorithm that implements the above cross-validation procedure is provided in Algorithm 1. To efficiently tune the parameters of ridge ensembles, we use and combine the corrected generalized cross-validation (CGCV) method [13] and the extrapolated cross-validation (ECV) method [26]. The improved CV method is implemented in the Python library [24].

### Validation on real-world datasets

In this section, we present numerical experiments to validate our theoretical results on real-world datasets. Figure 3 provides evidence supporting Assumption A on pretrained features extracted from commonly used neural networks applied to real-world datasets. The first panel of the figure demonstrates the equivalence of degrees of freedom for these pretrained features. Furthermore, we also observe consistent behavior across different neural network architectures and different datasets (see Figures 8 and 9). Remarkably, the path of equivalence can be accurately predicted, offering valuable insight into the underlying dynamics of these models. This observation suggests that the pretrained features from widely used neural networks exhibit similar properties when applied to real-world data, regardless of the specific architecture employed. The ability to predict the equivalence path opens up new possibilities for optimizing the performance of these models in practical applications.

One implication of the equivalence results explored in Theorems 1 and 6 is that instead of tuning for the full ridge penalty \(\mu\) on the large datasets, we can fix a small value of the ridge penalty \(\lambda\), fit subsample ridge ensembles, and tune for an optimal subsample size \(k\). To illustrate the validity of the tuning procedure described in Algorithm 1, we present both the actual prediction errors and their estimates by Algorithm 1 in Figure 4. We observe that the risk estimates closely match the prediction risks at different ensemble sizes across different datasets. Even with a subsampling ratio \(k/n\) of \(0.01\) and a sufficiently large \(M\), the risk estimate is close to the optimal risk. A smaller subsample size could also yield even smaller prediction risk in certain datasets.

## 5 Limitations and outlook

While our results are quite general in terms of applying to a wide variety of pretrained features, they are limited in that they only apply to ridge regression fitted on the pretrained features. The key challenge for extending the analysis based on Assumption A to general estimators beyond ridge regression is the characterization of the effect of subsampling general resolvents as additional ridge regularization. To extend to generalized linear models, one approach is to view the optimization as iteratively reweighted least squares [38] in combination with the current results. Another approach is to combine our results with the techniques in [41] to obtain deterministic equivalents for the Hessian, enabling an understanding of implicit regularization due to subsampling beyond linear models.

Beyond implicit regularization due to subsampling, there are other forms of implicit regularization, such as algorithmic regularization due to early stopping in gradient descent [2, 3, 49], dropout regularization [62, 65], among others. In some applications, multiple forms of implicit regularization are present simultaneously. For instance, during a mini-batch gradient step, implicit regularization arises from both iterative methods and mini-batch subsampling. The results presented in this paper may help to make explicit the combined effect of various forms of implicit regularization.

## Acknowledgments and Disclosure of Funding

We thank Benson Au, Daniel LeJeune, Ryan Tibshirani, and Alex Wei for the helpful conversations surrounding this work. We also thank the anonymous reviewers for their valuable feedback and suggestions.

We acknowledge the computing support the ACCESS allocation MTH230020 provided for some of the experiments performed on the Bridges2 system at the Pittsburgh Supercomputing Center. The code for reproducing the results of this paper can be found at https://jaydu1.github.io/overparameterized-ensembling/weighted-neural.

## References

* (1)
* Adlam et al. (2022) Adlam, B., Levinson, J. A., and Pennington, J. (2022). A random matrix perspective on mixtures of nonlinearities in high dimensions. In _International Conference on Artificial Intelligence and Statistics_.
* Ali et al. (2020) Ali, A., Dobriban, E., and Tibshirani, R. J. (2020). The implicit regularization of stochastic gradient flow for least squares. In _International conference on machine learning_.
* Ali et al. (2019) Ali, A., Kolter, J. Z., and Tibshirani, R. J. (2019). A continuous-time view of early stopping for least squares regression. In _International Conference on Artificial Intelligence and Statistics_.
* Anderson et al. (2010) Anderson, G. W., Guionnet, A., and Zeitouni, O. (2010). _An Introduction to Random Matrices_. Cambridge University Press.

Figure 4: Risk estimation by corrected and extrapolated generalized cross-validation. The risk estimates are computed based on \(M_{0}=25\) base estimators using Algorithm 1 with \(\lambda=10^{-3}\).

* Ando and Komaki [2023] Ando, R. and Komaki, F. (2023). On high-dimensional asymptotic properties of model averaging estimators. _arXiv preprint arXiv:2308.09476_.
* Bach [2023] Bach, F. (2023). High-dimensional analysis of double descent for linear regression with random projections. _arXiv:2303.01372_.
* Bahri et al. [2021] Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2021). Explaining neural scaling laws. _arXiv:2102.06701_.
* Bai and Silverstein [2010] Bai, Z. and Silverstein, J. W. (2010). _Spectral Analysis of Large Dimensional Random Matrices_. Springer. Second edition.
* Barthelme et al. [2023] Barthelme, S., Amblard, P.-O., Tremblay, N., and Usevich, K. (2023). Gaussian process regression in the flat limit. _The Annals of Statistics_, 51(6):2471-2505.
* Bartlett et al. [2020] Bartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. (2020). Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070.
* Bartlett et al. [2021] Bartlett, P. L., Montanari, A., and Rakhlin, A. (2021). Deep learning: A statistical viewpoint. _Acta Numerica_, 30:87-201.
* Belkin et al. [2020] Belkin, M., Hsu, D., and Xu, J. (2020). Two models of double descent for weak features. _SIAM Journal on Mathematics of Data Science_, 2(4):1167-1180.
* Bellec et al. [2023] Bellec, P., Du, J.-H., Koriyama, T., Patil, P., and Tan, K. (2023). Corrected generalized cross-validation for finite ensembles of penalized estimators. _arXiv preprint arXiv:2310.01374_.
* Bordelon et al. [2020] Bordelon, B., Canatar, A., and Pehlevan, C. (2020). Spectrum dependent learning curves in kernel regression and wide neural networks. In _International Conference on Machine Learning_.
* Cebron et al. [2022] Cebron, G., Dahlqvist, A., and Gabriel, F. (2022). Freeness of type \(B\) and conditional freeness for random matrices. _arXiv preprint arXiv:2205.01926_.
* Chen et al. [2023] Chen, X., Zeng, Y., Yang, S., and Sun, Q. (2023). Sketched ridgeless linear regression: The role of downsampling. In _International Conference on Machine Learning_.
* Clarke et al. [2024] Clarke, L., Vandenbroucque, A., Dalle, G., Loureiro, B., Krzakala, F., and Zdeborova, L. (2024). Analysis of bootstrap and subsampling in high-dimensional regularized regression. _arXiv preprint arXiv:2402.13622_.
* Couillet and Liao [2022] Couillet, R. and Liao, Z. (2022). _Random Matrix Methods for Machine Learning_. Cambridge University Press.
* Cui et al. [2021] Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. (2021). Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. _Advances in Neural Information Processing Systems_, 34:10131-10143.
* Dobriban and Sheng [2020] Dobriban, E. and Sheng, Y. (2020). Wonder: Weighted one-shot distributed ridge regression in high dimensions. _Journal of Machine Learning Research_, 21(66):1-52.
* Dobriban and Sheng [2021] Dobriban, E. and Sheng, Y. (2021). Distributed linear regression by averaging. _The Annals of Statistics_, 49(2):918-943.
* Dobriban and Wager [2018] Dobriban, E. and Wager, S. (2018). High-dimensional asymptotics of prediction: Ridge regression and classification. _The Annals of Statistics_, 46(1):247-279.
* Drineas et al. [2006] Drineas, P., Mahoney, M. W., and Muthukrishnan, S. (2006). Sampling algorithms for \(\ell_{2}\) regression and applications. _Proceedings of the ACM-SIAM Symposium on Discrete Algorithm_.
* Du and Patil [2023] Du, J.-H. and Patil, P. (2023). Python package sklearn_ensemble_cv v0.2.1. PyPI.

* Du et al. [2023] Du, J.-H., Patil, P., and Kuchibhotla, A. K. (2023). Subsample ridge ensembles: Equivalences and generalized cross-validation. In _International Conference on Machine Learning_.
* Du et al. [2024] Du, J.-H., Patil, P., Roeder, K., and Kuchibhotla, A. K. (2024). Extrapolated cross-validation for randomized ensembles. _Journal of Computational and Graphical Statistics_.
* Efron [1983] Efron, B. (1983). Estimating the error rate of a prediction rule: improvement on cross-validation. _Journal of the American Statistical Association_, 78(382):316-331.
* Efron [1986] Efron, B. (1986). How biased is the apparent error rate of a prediction rule? _Journal of the American Statistical Association_, 81(394):461-470.
* Ghorbani et al. [2020] Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2020). When do neural networks outperform kernel methods? _Advances in Neural Information Processing Systems_.
* Hastie et al. [2022] Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. _The Annals of Statistics_, 50(2):949-986.
* Hastie and Tibshirani [1990] Hastie, T. and Tibshirani, R. (1990). _Generalized Additive Models_. Chapman & Hall.
* He et al. [2016] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In _Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Jacot et al. [2018] Jacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_.
* Jacot et al. [2020] Jacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel, F. (2020). Kernel alignment risk estimator: Risk prediction from training data. _Advances in Neural Information Processing Systems_.
* Koehler et al. [2021] Koehler, F., Zhou, L., Sutherland, D. J., and Srebro, N. (2021). Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting. _Advances in Neural Information Processing Systems_.
* Krogh and Sollich [1997] Krogh, A. and Sollich, P. (1997). Statistical mechanics of ensemble learning. _Physical Review E_, 55(6):811.
* LeJeune et al. [2020] LeJeune, D., Javadi, H., and Baraniuk, R. (2020). The implicit regularization of ordinary least squares ensembles. In _International Conference on Artificial Intelligence and Statistics_.
* LeJeune et al. [2021] LeJeune, D., Javadi, H., and Baraniuk, R. G. (2021). The flip side of the reweighted coin: Duality of adaptive dropout and regularization. In _Advances in Neural Information Processing Systems_.
* LeJeune et al. [2024] LeJeune, D., Patil, P., Javadi, H., Baraniuk, R. G., and Tibshirani, R. J. (2024). Asymptotics of the sketched pseudoinverse. _SIAM Journal on Mathematics of Data Science_, 6(1):199-225.
* Liang and Rakhlin [2020] Liang, T. and Rakhlin, A. (2020). Just interpolate: Kernel "ridgeless" regression can generalize. _The Annals of Statistics_.
* Liao and Mahoney [2021] Liao, Z. and Mahoney, M. W. (2021). Hessian eigenspectra of more realistic nonlinear models. In _Advances in Neural Information Processing Systems_, volume 34.
* Liu and Dobriban [2020] Liu, S. and Dobriban, E. (2020). Ridge regression: Structure, cross-validation, and sketching. In _International Conference on Learning Representations_.
* Loureiro et al. [2021] Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., Mezard, M., and Zdeborova, L. (2021). Learning curves of generic features maps for realistic datasets with a teacher-student model. In _Advances in Neural Information Processing Systems_.

* Mahoney (2011) Mahoney, M. W. (2011). Randomized algorithms for matrices and data. _Foundations and Trends in Machine Learning_, 3(2):123-224.
* Mallinar et al. (2022) Mallinar, N., Simon, J. B., Abedsoltan, A., Pandit, P., Belkin, M., and Nakkiran, P. (2022). Benign, tempered, or catastrophic: A taxonomy of overfitting. _arXiv:2207.06569_.
* Mei and Montanari (2022) Mei, S. and Montanari, A. (2022). The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75(4):667-766.
* Mingo and Speicher (2017) Mingo, J. A. and Speicher, R. (2017). _Free Probability and Random Matrices_, volume 35. Springer.
* Nakkiran et al. (2021) Nakkiran, P., Venkat, P., Kakade, S. M., and Ma, T. (2021). Optimal regularization can mitigate double descent. In _International Conference on Learning Representations_.
* Neu and Rosasco (2018) Neu, G. and Rosasco, L. (2018). Iterate averaging as regularization for stochastic gradient descent. In _Conference On Learning Theory_.
* Patil and Du (2023) Patil, P. and Du, J.-H. (2023). Generalized equivalences between subsampling and ridge regularization. _Advances in Neural Information Processing Systems_.
* Patil et al. (2023) Patil, P., Du, J.-H., and Kuchibhotla, A. K. (2023). Bagging in overparameterized learning: Risk characterization and risk monotonization. _Journal of Machine Learning Research_, 24(319):1-113.
* Patil et al. (2024) Patil, P., Du, J.-H., and Tibshirani, R. J. (2024). Optimal ridge regularization for out-of-distribution prediction. _arXiv preprint arXiv:2404.01233_.
* Patil et al. (2022) Patil, P., Kuchibhotla, A. K., Wei, Y., and Rinaldo, A. (2022). Mitigating multiple descents: A model-agnostic framework for risk monotonization. _arXiv preprint arXiv:2205.12937_.
* Patil and LeJeune (2024) Patil, P. and LeJeune, D. (2024). Asymptotically free sketched ridge ensembles: Risks, cross-validation, and tuning. In _International Conference on Learning Representations_.
* Paul and Aue (2014) Paul, D. and Aue, A. (2014). Random matrix theory in statistics: A review. _Journal of Statistical Planning and Inference_, 150:1-29.
* Rahimi and Recht (2007) Rahimi, A. and Recht, B. (2007). Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20.
* Sahraee-Ardakan et al. (2022) Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A. K. (2022). Kernel methods and multi-layer perceptrons learn linear models in high dimensions. _arXiv preprint arXiv:2201.08082_.
* Serdobolskii (2007) Serdobolskii, V. I. (2007). _Multiparametric Statistics_. Elsevier.
* Slagel et al. (2019) Slagel, J. T., Chung, J., Chung, M., Kozak, D., and Tenorio, L. (2019). Sampled tikhonov regularization for large linear inverse problems. _Inverse Problems_, 35(11):114008.
* Sollich (2001) Sollich, P. (2001). Gaussian process regression with mismatched models. _Advances in Neural Information Processing Systems_, 14.
* Sollich and Krogh (1995) Sollich, P. and Krogh, A. (1995). Learning with ensembles: How overfitting can be useful. In _Advances in Neural Information Processing Systems_.
* Srivastava et al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. _The Journal of Machine Learning Research_, 15(1):1929-1958.

* Stein [1981] Stein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. _The Annals of Statistics_, pages 1135-1151.
* Voiculescu [1997] Voiculescu, D. V. (1997). _Free Probability Theory_. American Mathematical Society.
* Wager et al. [2013] Wager, S., Wang, S., and Liang, P. S. (2013). Dropout training as adaptive regularization. In _Advances in Neural Information Processing Systems_.
* Wei et al. [2022] Wei, A., Hu, W., and Steinhardt, J. (2022). More than a toy: Random matrix models predict how real-world neural representations generalize. In _International Conference on Machine Learning_.
* Weisberg [2005] Weisberg, S. (2005). _Applied Linear Regression_. John Wiley & Sons.
* Yang [2019] Yang, G. (2019). Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. _arXiv preprint arXiv:1902.04760_.
* Yosinski et al. [2014] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In _Advances in Neural Information Processing Systems_.
* Zhang et al. [2017] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_.
* Zhang et al. [2021] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115.

This serves as an appendix to the paper "Implicit Regularization Paths of Weighted Neural Representations." The beginning (unlabeled) section of the appendix provides an organization for the appendix, followed by a summary of the general notation used in both the paper and the appendix. Any other specific notation is explained inline where it is first used.

Organization
* In Appendix A, we provide a brief technical background on free probability theory and various transforms that we need and collect known asymptotic ridge equivalents that we use in our proofs.
* In Appendix B, we present proofs of the theoretical results in Section 3 (Theorems 1 and 2 and Propositions 3-5).
* In Appendix C, we present proofs of the theoretical results in Section 4 (Theorem 6 and Proposition 7).
* In Appendix D, we provide additional illustrations for the results in Section 3 (Figures 5 and 6).
* In Appendix E, we provide additional illustrations for the results in Section 4 (Figures 7-9), including our meta-algorithm for tuning (Algorithm 1) that is not included in the main text due to space constraints.
* In Appendix F, we provide additional details on the experiments in both Section 3 and Section 4.

NotationWe use blackboard letters to denote some special sets: \(\mathbb{N}\) denotes the set of natural numbers, \(\mathbb{R}\) denotes the set of real numbers, \(\mathbb{R}_{+}\) denotes the set of positive real numbers, \(\mathbb{C}\) denotes the set of complex numbers, \(\mathbb{C}^{+}\) denotes the set of complex numbers with positive imaginary part, and \(\mathbb{C}^{-}\) denotes the set of complex numbers with negative imaginary part. We use \([n]\) to denote the index set \(\{1,2,\ldots,n\}\).

We denote scalars and vectors using lower-case letters and matrices using upper-case letters. For a vector \(\bm{\beta}\), \(\bm{\beta}^{\top}\) denotes its transpose, and \(\|\bm{\beta}\|_{2}\) denotes its \(\ell_{2}\) norm. For a pair of vectors \(\bm{u}\) and \(\bm{v}\), \(\langle\bm{u},\bm{v}\rangle\) denotes their inner product. For a matrix \(\bm{X}\in\mathbb{R}^{n\times p}\), \(\bm{X}^{\top}\in\mathbb{R}^{p\times n}\) denotes its transpose, and \(\bm{X}^{\dagger}\in\mathbb{R}^{p\times n}\) denotes its Moore-Penrose inverse. For a square matrix \(\bm{A}\in\mathbb{R}^{p\times p}\), \(\operatorname{tr}[\bm{A}]\) denotes its trace, \(\operatorname{tr}[\bm{A}]\) denotes its average trace \(\operatorname{tr}[\bm{A}]/p\), and \(\bm{A}^{-1}\) denotes its inverse, provided that \(\bm{A}\) is invertible. For a symmetric matrix \(\bm{A}\), \(\lambda^{+}_{\min}(\bm{A})\) denotes its minimum nonzero eigenvalue. For a positive semidefinite matrix \(\bm{G}\), \(\bm{G}^{1/2}\) denotes its principal square root. For a matrix \(\bm{X}\), we denote by \(\|\bm{X}\|_{\mathrm{op}}\) its operator norm with respect to the \(\ell_{2}\) vector norm. It is also the spectral norm of \(\bm{X}\). For a matrix \(\bm{X}\), we denote by \(\|\bm{X}\|_{\operatorname{tr}}\) its trace norm. It is given by \(\operatorname{tr}[(\bm{X}^{\top}\bm{X})^{1/2}]\), and is also the nuclear norm \(\bm{X}\). We denote \(p\times p\) identity matrix by \(\bm{I}_{p}\), or simply by \(\bm{I}\) when it is clear from the context.

For symmetric matrices \(\bm{A}\) and \(\bm{B}\), we use \(\bm{A}\preceq\bm{B}\) to denote the Loewner ordering to mean that \(\bm{A}-\bm{B}\) is a positive semidefinite matrix. For two sequences of matrices \(\bm{A}_{p}\) and \(\bm{B}_{p}\), we use \(\bm{A}_{p}\simeq\bm{B}_{p}\) to denote a certain asymptotic equivalence; see Appendix A.3 for a precise definition.

## Appendix A Technical background

### Basics of free probability theory

In this section, we briefly review definitions from free probability theory and its applications to random matrices. This review will help set the stage by introducing the various mathematical structures and spaces we are working with. It will also introduce some of the notation used throughout the text.

Free probability is a mathematical framework that deals with non-commutative random variables [19]. The use of free probability theory has appeared in various recent works in statistical machine learning, including [1, 2, 11, 12, 17]. Good references on free probability theory include [3, 13], from which we borrow some basic definitions in the following. All the material in this section is standard in free probability theory and mainly serves to keep the definitions self-contained.

**Definition 8** (Non-commutative algebra).: A set \(\mathcal{A}\) is called a (complex) algebra (over the field of complex numbers \(\mathbb{C}\)) if it is a vector space (over \(\mathbb{C}\) with addition \(+\)), equipped with a bilinear multiplication \(\cdot\), such that for all \(x,y,z\in\mathcal{A}\) and \(\alpha\in\mathbb{C}\),

1. \(x\cdot(y\cdot z)=(x\cdot y)\cdot z\),
2. \((x+y)\cdot z=x\cdot z+y\cdot z\),
3. \(x\cdot(y+z)=x\cdot y+x\cdot z\),
4. \(\alpha(x\cdot y)=(\alpha x)\cdot y=x\cdot(\alpha y)\).

In addition, an algebra is called unital if a multiplicative identity element exists. We will use \(1_{\mathcal{A}}\) to denote this identity element. We will drop the "\(\cdot\)" symbol to denote multiplication over the algebra.

**Definition 9** (Non-commutative probability space).: Let \(\mathcal{A}\) over \(\mathbb{C}\) be a unital algebra with identity \(1_{\mathcal{A}}\). Let \(\varphi:\mathcal{A}\to\mathbb{C}\) be a linear functional which is unital (that is, \(\varphi(1_{\mathcal{A}})=1\)). Then \((\mathcal{A},\varphi)\) is called a non-commutative probability space, and \(\varphi\) is called a state. A state \(\varphi\) is said to be tracial if \(\varphi(xy)=\varphi(yx)\) for all \(x,y\in\mathcal{A}\).

**Definition 10** (Moments).: Let \((\mathcal{A},\varphi)\) be a non-commutative probability space. The numbers \(\{\varphi(x^{k})\}_{k=1}^{\infty}\) are called the moments of the variable \(x\in\mathcal{A}\).

**Definition 11** (\(*\)-algebra).: An algebra \(\mathcal{A}\) is called a \(*\)-algebra if there exists a mapping \(x\to x^{*}\) from \(\mathcal{A}\to\mathcal{A}\) such that, for all \(x,y\in\mathcal{A}\) and \(\alpha\in\mathbb{C}\),

1. \((x+y)^{*}=x^{*}+y^{*}\),
2. \((\alpha x)^{*}=\bar{\alpha}x^{*}\),
3. \((xy)^{*}=y^{*}x^{*}\),
4. \((x^{*})^{*}=x\).

A variable \(x\) of a \(*\)-algebra is called self-adjoint if \(x=x^{*}\). A unital linear functional \(\varphi\) on a \(*\)-algebra is said to be positive if \(\varphi(x^{*}x)\geq 0\) for all \(x\in\mathcal{A}\).

**Definition 12** (\(*\)-probability space).: Let \(\mathcal{A}\) be a unital \(*\)-algebra with a positive state \(\varphi\). Then \((\mathcal{A},\varphi)\) is called a \(*\)-probability space.

**Example 1**.: Denote by \(\mathcal{M}_{p}(\mathbb{C})\) the collection of all \(p\times p\) matrices with complex entries. Let the multiplication and addition operations be defined in the usual way. The \(*\)-operation is the same as taking the conjugate transpose. Let \(\operatorname{tr}:\mathcal{M}_{p}(\mathbb{C})\to\mathbb{C}\) be the normalized trace defined by:

\[\operatorname{\widetilde{tr}}(\boldsymbol{A})=\frac{1}{p}\operatorname{tr}[ \boldsymbol{A}].\]

The state \(\operatorname{tr}\) is tracial and positive.

**Definition 13** (Free independence).: Suppose \((\mathcal{A},\varphi)\) is a \(*\)-probability space. Then, the \(*\)-sub-algebras \(\{\mathcal{A}_{i}\}_{i\in I}\) of \(\mathcal{A}\) are said to be \(*\)-freely independent (or simply \(*\)-free) if, for all \(n\geq 2\) and all \(x_{1},x_{2},\cdots,x_{n}\) from \(\{\mathcal{A}_{i}\}_{i\in I}\), \(\kappa_{n}(x_{1},x_{2},\cdots,x_{n})=0\) whenever at least two of the \(x_{i}\) are from different \(\mathcal{A}_{i}\). In particular, any collection of variables is said to be \(*\)-free if the sub-algebras generated by these variables are \(*\)-free.

**Lemma 14**.: Suppose \((\mathcal{A},\varphi)\) is a \(*\)-probability space. If \(x\) and \(y\) are free in \((\mathcal{A},\varphi)\), then for all non-negative integers \(n\) and \(m\),

\[\varphi(x^{n}y^{m})=\varphi(x^{n})\varphi(y^{m})=\varphi(y^{m}x^{n}).\]

In other words, elements of the algebra are considered free if any alternating product of centered polynomials is also centered.

In this work, we will consider \(\varphi\) to be the normalized trace. The normalized trace is the generalization of \(\frac{1}{p}\operatorname{tr}[\boldsymbol{A}]\) for \(\boldsymbol{A}\in\mathbb{C}^{p\times p}\) to elements of a \(C^{*}\)-algebra \(\mathcal{A}\). Specifically, for any self-adjoint \(a\in\mathcal{A}\) and any polynomial \(p\), we have

\[\varphi(p(a))=\int p(z)\,\mathrm{d}\mu_{a}(z),\]where \(\mu_{a}\) is the probability measure that characterizes the spectral distribution of \(a\).

**Definition 15** (Convergence in spectral distribution).: Let \((\mathcal{A},\varphi)\) be a \(C^{*}\)-probability space. We say that \(\bm{A}_{1},\dots,\bm{A}_{m}\in\mathbb{C}^{p\times p}\)_converge in spectral distribution_ to elements \(a_{1},\dots,a_{m}\in\mathcal{A}\) if, for all \(1\leq\ell<\infty\) and \(1\leq i_{j}\leq m\) for \(1\leq j\leq\ell\), we have

\[\frac{1}{p}\operatorname{tr}[\bm{A}_{i_{1}}\cdots\bm{A}_{i_{\ell}}]\to\varphi( a_{i_{1}}\cdots a_{i_{\ell}}).\]

Then, with slight abuse of notation, two matrices \(\bm{A},\bm{B}\in\mathbb{R}^{p\times p}\) are said to be free if

\[\frac{1}{p}\operatorname{tr}\left[\prod_{\ell=1}^{L}\operatorname{poly}_{ \ell}^{\bm{A}}(\bm{A})\operatorname{poly}_{\ell}^{\bm{B}}(\bm{B})\right]=0,\]

for all \(L\geq 1\) and all centered polynomials, that is, \(\operatorname{\widetilde{tr}}[\operatorname{poly}_{\ell}^{\bm{A}}(\bm{A})]=0\). This notation is an abuse of notation because finite matrices cannot satisfy this condition. However, they can satisfy it asymptotically as \(p\to\infty\), and in this case, we say that \(\bm{A}\) and \(\bm{B}\) are _asymptotically free_.

Note: With some abuse of notation, we will let matrices in boldface denote both the finite matrix and the limiting element in the free probability space. The limiting element can be understood, for example, as a bounded linear operator on a Hilbert space. We also remark that all notions we need are well-defined in this limit as well, as long as they are appropriately normalized.

### Useful transforms and their relationships

In this section, we review the key transforms used in free probability theory and their interrelationships.

**Definition 16** (Cauchy transform).: Let \(a\) be an element of a \(*\)-probability space \((\mathcal{A},\varphi)\). Suppose there exists some \(C>0\) such that \(|\varphi(a^{n})|\leq C^{n}\) for all \(n\in\mathbb{N}\). Then the Cauchy transform of \(a\) is defined as:

\[\mathcal{G}_{a}(z)=\sum_{n=0}^{\infty}\frac{\varphi(a^{n})}{z^{n+1}}\]

for all \(z\in\mathbb{C}\) with \(|z|>C\).

Note that the Cauchy transform is the negative of the Stieltjes transform. In this paper, we will focus only on the Cauchy transform. Recall that for a probability measure \(\nu\) on \(\mathbb{R}\) and for \(z\notin\mathbb{R}\), the Cauchy transform of \(\nu\) is defined as:

\[\mathcal{G}(z)=\int_{\mathbb{R}}\frac{1}{z-x}\,\mathrm{d}\nu(x).\]

The definition above is motivated by the following property of the Cauchy transform of a measure. Suppose \(\nu\) is a probability measure whose support is contained in \([-C,C]\) for some \(C>0\) and which has moments \(\{m_{k}(\nu)\}_{k=0}^{\infty}\). Then the Cauchy transform of \(\nu\) is defined for \(z\in\mathbb{C}\) with \(|z|>C\) as:

\[\mathcal{G}_{\nu}(z)=\sum_{k=0}^{\infty}\frac{m_{k}(\nu)}{z^{k+1}}.\]

**Definition 17** (Moment generating function).: Let \(a\) be an element of a \(*\)-probability space \((\mathcal{A},\varphi)\). The moment generating function of \(a\) is defined as:

\[\mathcal{M}_{a}(z)=1+\sum_{k=1}^{\infty}\varphi(a^{k})z^{k}\]

for \(z\in\mathbb{C}\) such that \(|z|<r_{a}\). Here, \(r_{a}\) is the radius of convergence of the series.

For a probability measure \(\nu\), the moment generating function is defined analogously. (Note: The definition above is not to be confused with the moment generating function of a random variable in probability theory.) The Cauchy transform is related to the moment series via:

\[\mathcal{G}_{a}(z)=\frac{1}{z}\mathcal{M}_{a}\left(\frac{1}{z}\right).\] (13)In the other direction, we have:

\[\mathcal{M}_{a}(z)=\frac{1}{z}\mathcal{G}_{a}\left(\frac{1}{z}\right)-1.\] (14)

**Definition 18** (\(S\)-transform).: For

\[\mathcal{M}_{a}(z)=\sum_{m=0}^{\infty}\varphi(a^{m})z^{m},\]

we define the \(S\)-transform of \(a\) by:

\[\mathcal{S}_{a}(w)=\frac{1+w}{w}\mathcal{M}_{a}^{(-1)}(w),\] (15)

where \(\mathcal{M}^{(-1)}\) denotes the inverse under composition of \(\mathcal{M}\).

Finally, in terms of operator \(\bm{A}\), we summarize the series of invertible transformations between the various transforms introduced in this section.

* _Cauchy transform_: \[\mathcal{G}_{\bm{A}}(z)=\overline{\mathrm{tr}}[(z\bm{I}-\bm{A})^{-1}].\]
* _Moment generating series_: \[\mathcal{M}_{\bm{A}}(z)=\frac{1}{z}\mathcal{G}_{\bm{A}}\left(\frac{1}{z} \right)-1.\]
* \(S\)_-transform_: \[\mathcal{S}_{\bm{A}}(w)=\frac{1+w}{w}\mathcal{M}_{\bm{A}}^{(-1)}(w).\]

Here:

* \(\mathcal{M}_{\bm{A}}(z)=\sum_{k=1}^{\infty}\overline{\mathrm{tr}}[\bm{A}^{k}] z^{k}\) is the moment generating series.
* \(\mathcal{M}_{\bm{A}}^{(-1)}\) denotes the inverse under composition of \(\mathcal{M}_{\bm{A}}\).
* \(\overline{\mathrm{tr}}[\bm{A}]\) denotes the average trace \(\mathrm{tr}[\bm{A}]/p\) of a matrix \(\bm{A}\in\mathbb{R}^{p\times p}\).

### Asymptotic ridge resolvents

In this section, we provide a brief background on the language of asymptotic equivalents used in the proofs throughout the paper. We will state the definition of asymptotic equivalents and point to useful calculus rules. For more details, see [16, Appendix S.7].

To concisely present our results, we will use the framework of asymptotic equivalence [5, 6, 16], defined as follows. Let \(\bm{A}_{p}\) and \(\bm{B}_{p}\) be sequences of matrices of arbitrary dimensions (including vectors and scalars). We say that \(\bm{A}_{p}\) and \(\bm{B}_{p}\) are _asymptotically equivalent_, denoted as \(\bm{A}_{p}\simeq\bm{B}_{p}\), if \(\lim_{p\to\infty}|\mathrm{tr}[\bm{C}_{p}(\bm{A}_{p}-\bm{B}_{p})]|=0\) almost surely for any sequence of random matrices \(\mathcal{C}_{p}\) with bounded trace norm that are independent of \(\bm{A}_{p}\) and \(\bm{B}_{p}\). Note that for sequences of scalar random variables, the definition simply reduces to the typical almost sure convergence of sequences of random variables involved.

The notion of deterministic equivalents obeys various calculus rules such as sum, product, differentiation, conditioning, and substitution. We refer the reader to [16] for a comprehensive list of these calculus rules, their proofs, and other related details.

Next, we collect first- and second-order asymptotic equivalents for sketched ridge resolvents from [11, 17], which will be useful for our extensions to weighted ridge resolvents.

**Assumption B** (Sketch structure).: Let \(\bm{S}\in\mathbb{R}^{p\times q}\) be the feature sketching matrix and \(\bm{X}\in\mathbb{R}^{n\times p}\) be the data matrix. Let \(\bm{S}\bm{S}^{\top}\) and \(\frac{1}{n}\bm{X}^{\top}\bm{X}\) converge almost surely to bounded operators that are infinitesimally free with respect to \((\frac{1}{p}\mathrm{tr}[\cdot],\mathrm{tr}[\bm{\Theta}(\cdot)])\) for any \(\bm{\Theta}\) independent of \(\bm{S}\) with \(\|\bm{\Theta}\|_{\mathrm{tr}}\) uniformly bounded. Additionally, let \(\bm{S}\bm{S}^{\top}\) have a limiting \(S\)-transform that is analytic on the lower half of the complex plane.

For the statement to follow, let us define \(\widehat{\bm{\Sigma}}:=\frac{1}{n}\bm{X}^{\top}\bm{X}\). Let \(\widetilde{\lambda}_{0}:=-\liminf_{p\to\infty}\lambda_{\min}^{+}(\bm{S}^{\top} \widehat{\bm{\Sigma}}\bm{S})\). Here, recall that \(\lambda_{\min}^{+}(\bm{A})\) represents the minimum nonzero eigenvalue of a symmetric matrix \(\bm{A}\).

**Theorem 19** (Free sketching equivalence; [11], Theorem 7.2).: _Under Assumption B, for all \(\lambda>\widetilde{\lambda}_{0}\),_

\[\bm{S}(\bm{S}^{\top}\widehat{\bm{\Sigma}}\bm{S}+\lambda\bm{I}_{q})^{\dagger} \bm{S}^{\top}\simeq(\widehat{\bm{\Sigma}}+\nu\bm{I}_{p})^{\dagger},\] (16)

_where \(\nu>-\lambda_{\min}^{+}(\widehat{\bm{\Sigma}})\) is increasing in \(\lambda>\widetilde{\lambda}_{0}\) and satisfies:_

\[\nu\simeq\lambda\mathcal{S}_{\bm{S}\bm{S}^{\top}}(-\operatorname{\widetilde {tr}}[\widehat{\bm{\Sigma}}\bm{S}(\bm{S}^{\top}\widehat{\bm{\Sigma}}\bm{S}+ \lambda\bm{I}_{q})^{\dagger}\bm{S}^{\top}])\simeq\lambda\mathcal{S}_{\bm{S} \bm{S}^{\top}}(-\operatorname{\widetilde{tr}}[\widehat{\bm{\Sigma}}(\widehat {\bm{\Sigma}}+\nu\bm{I}_{p})^{\dagger}]).\] (17)

**Lemma 20** (Second-order equivalence for sketched ridge resolvents; [17], Lemma 15).: Under the settings of Lemma 21, for any positive semidefinite \(\bm{\Psi}\) with uniformly bounded operator norm, for all \(\lambda>\lambda_{0}\),

\[\bm{S}(\bm{S}^{\top}\widehat{\bm{\Sigma}}\bm{S}+\lambda\bm{I}_{q})^{\dagger} \bm{S}^{\top}\bm{\Psi}\bm{S}(\bm{S}^{\top}\widehat{\bm{\Sigma}}\bm{S}+\lambda \bm{I}_{q})^{\dagger}\bm{S}^{\top}\simeq(\widehat{\bm{\Sigma}}+\nu\bm{I}_{p})^ {\dagger}(\bm{\Psi}+\nu^{\prime}_{\bm{\Psi}}\bm{I}_{p})(\widehat{\bm{\Sigma}}+ \nu\bm{I}_{p})^{\dagger},\] (18)

where \(\nu^{\prime}_{\bm{\Psi}}\geq 0\) is given by:

\[\nu^{\prime}_{\bm{\Psi}}=-\frac{\partial\nu}{\partial\lambda}\lambda^{2} \mathcal{S}^{\prime}_{\bm{S}\bm{S}^{\top}}(-\operatorname{\widetilde{tr}}[ \widehat{\bm{\Sigma}}(\widehat{\bm{\Sigma}}+\nu\bm{I}_{p})^{\dagger}]) \operatorname{\widetilde{tr}}[(\widehat{\bm{\Sigma}}+\nu\bm{I}_{p})^{\dagger }\bm{\Psi}(\widehat{\bm{\Sigma}}+\nu\bm{I}_{p})^{\dagger}].\] (19)

## Appendix B Proofs in Section 3

### Proof of Theorem 1

Our main ingredient in the proof is Lemma 21. We will first show estimator equivalence and then show degrees of freedom equivalence.

_Estimator equivalence._ Recall from (1) the ridge estimator on the weighted data is:

\[\widehat{\bm{\beta}}_{\bm{W},\lambda}=(\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W} \bm{\Phi}/n+\lambda\bm{I}_{p})^{\dagger}\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W} \bm{y}/n.\]

This is the "primal" form of the ridge estimator. Using the Woodbury matrix identity, we first write the estimator into its "dual" form.

\[\widehat{\bm{\beta}}_{\bm{W},\lambda} =\bm{\Phi}^{\top}\bm{W}^{\top}(\bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm {W}^{\top}/n+\lambda\bm{I}_{n})^{\dagger}\bm{W}\bm{y}/n\] \[=\bm{\Phi}^{\top}\bm{W}^{\top}(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n} )^{\dagger}\bm{W}\bm{y}/n.\]

Now, we can apply the first part of Lemma 21 to the matrix \(\bm{W}^{\top}(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n})^{\dagger}\bm{W}\). From (31), we then have the following equivalence:

\[\widehat{\bm{\beta}}_{\bm{W},\lambda} \simeq\bm{\Phi}^{\top}(\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{\dagger} \bm{y}/n\] \[=\bm{\Phi}^{\top}(\bm{\Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{ \dagger}\bm{y}/n\] \[=(\bm{\Phi}^{\top}\bm{\Phi}/n+\mu\bm{I}_{n})^{\dagger}\bm{\Phi}^{ \top}\bm{y}/n=\widehat{\bm{\beta}}_{\bm{I},\mu},\]

where \(\mu\) satisfies the following equation:

\[\mu=\lambda\mathcal{S}_{\bm{W}^{\top}\bm{W}}\left(-\frac{\operatorname{tr}[\bm {G}_{\bm{I}}(\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{\dagger}]}{n}\right)=\lambda \mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\operatorname{\widetilde{df}}(\widehat{\bm{ \beta}}_{\bm{I},\mu})).\]

Note that in the simplification, we used the Woodbury identity again to go back from the dual form into the primal form for the ridge estimator based on the full data. Rearranging, we obtain the desired estimator equivalence. We next move on to showing the degrees of freedom equivalence.

_Degrees of freedom equivalence._ For the subsampled estimator \(\widehat{\bm{\beta}}_{\bm{W},\lambda}\), the effective degrees of freedom is given by:

\[\operatorname{df}(\widehat{\bm{\beta}}_{\bm{W},\lambda}) =\operatorname{tr}[\bm{\Phi}^{\top}\bm{\Phi}/n(\bm{\Phi}^{\top} \bm{\Phi}/n+\lambda\bm{I}_{p})^{\dagger}]\] \[=\operatorname{tr}[\bm{\Phi}(\bm{\Phi}^{\top}\bm{\Phi}/n+\lambda \bm{I}_{p})^{\dagger}\bm{\Phi}^{\top}/n]\] \[=\operatorname{tr}[(\bm{\Phi}\bm{\Phi}^{\top}/n+\lambda\bm{I}_{n })^{\dagger}\bm{\Phi}\bm{\Phi}^{\top}/n].\]The second equality above follows from the push-through identity \(\bm{\Phi}(\bm{\Phi}^{\top}\bm{\Phi}/n+\lambda\bm{I}_{p})^{\dagger}\bm{\Phi}^{\top }=(\bm{\Phi}\bm{\Phi}^{\top}+\lambda\bm{I}_{n})^{\dagger}\bm{\Phi}\bm{\Phi}^{\top}\). Recognizing the quantity inside the trace as the degrees of freedom of the full ridge estimator, we have

\[\mu=\lambda\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\mathsf{df}(\widehat{\bm{\beta}}_ {\bm{I},\mu})).\]

We can equivalently write the equation above as

\[-\mathcal{S}_{\bm{W}^{\top}\bm{W}}^{-1}\left(\frac{\mu}{\lambda}\right)= \mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu})\quad\text{or}\quad\frac{\mu}{ \lambda}=\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\mathsf{df}(\widehat{\bm{\beta}}_ {\bm{I},\mu})).\]

Rearranging the display above provides the desired degrees of freedom equivalence and finishes the proof.

### Proof of Theorem 2

We will apply Theorem 1 to the subsampling weight matrix \(\bm{W}\). The main ingredient that we need is the \(S\)-transform of the spectrum of the matrix \(\bm{W}^{\top}\bm{W}\). As summarized in Appendix A.2, one approach to compute the \(S\)-transform is to go through the following chain of transforms. First, we apply the Cauchy transform, then the moment-generating series, and finally, take the inverse to obtain the \(S\)-transform. We will do this in the following steps.

_Cauchy transform._ Recall that the Cauchy transform from Definition 16 can be computed as:

\[\mathcal{G}_{\bm{W}^{\top}\bm{W}}(z)=\overline{\operatorname{tr}}[(z\bm{I}_{n }-\bm{W}^{\top}\bm{W})^{-1}].\]

_Moment generating series._ We can then compute the moment series from Definition 17 using (14) as follows:

\[\mathcal{M}_{\bm{W}^{\top}\bm{W}}(z) =\frac{1}{z}\,\overline{\operatorname{tr}}\left[\left(\frac{1}{ z}\bm{I}_{n}-\bm{W}^{\top}\bm{W}\right)^{-1}\right]-1\] \[=\overline{\operatorname{tr}}[(\bm{I}_{n}-z\bm{W}^{\top}\bm{W}) ^{-1}]-\overline{\operatorname{tr}}[\bm{I}_{n}]\] \[=-\overline{\operatorname{tr}}[\bm{I}_{n}]+\overline{ \operatorname{tr}}[(\bm{I}_{n}-z\bm{W}^{\top}\bm{W})^{-1}]\] \[=\overline{\operatorname{tr}}[(z\bm{W}^{\top}\bm{W}-\bm{I}_{n}+ \bm{I}_{n})(\bm{I}_{n}-z\bm{W}^{\top}\bm{W})^{-1}]\] \[=\overline{\operatorname{tr}}[z\bm{W}^{\top}\bm{W}(\bm{I}_{n}-z \bm{W}^{\top}\bm{W})^{-1}].\]

We now note that the matrix \(\bm{W}^{\top}\bm{W}\) has \(k\) eigenvalues of \(1\) and \(n-k\) eigenvalues of \(0\). Therefore, we have

\[\mathcal{M}_{\bm{W}^{\top}\bm{W}}(z) =\overline{\operatorname{tr}}[z\bm{W}^{\top}\bm{W}(\bm{I}_{n}-z \bm{W}^{\top}\bm{W})^{-1}]\] \[=\frac{1}{n}\left(\sum_{i=1}^{n}\frac{zd_{i}}{1-zd_{i}}\right)\] \[=\frac{k}{n}\cdot\frac{z}{1-z}.\] (20)

\(S\)_-transform._ The inverse of the moment generating series map \(z\mapsto\mathcal{M}_{\bm{W}^{\top}\bm{W}}(z)\) from (20) is:

\[\mathcal{M}^{\langle-1\rangle}(w)=\frac{w}{w+k/n}.\] (21)

Therefore, from Definition 18 and using (21), we have

\[\mathcal{S}(w)=\frac{1+w}{w}\cdot\frac{w}{w+k/n}=\frac{1+w}{w+k/n}.\] (22)

Now, we are ready to apply Theorem 1 to the subsampling matrix \(\bm{W}\).

Substituting (22) into (2), we get

\[\frac{\mu}{\lambda}=\mathcal{S}(-\overline{\mathsf{df}}(\widehat{\bm{\beta}}_ {\bm{I},\mu}))=\frac{1-\overline{\mathsf{df}}(\widehat{\bm{\beta}}_{\bm{I},\mu })}{-\overline{\mathsf{df}}(\widehat{\bm{\beta}}_{\bm{I},\mu})+k/n}.\]Rearranging, we obtain

\[\overline{\operatorname{df}}(\widehat{\bm{\beta}}_{\bm{I},\mu})\cdot(\mu-\lambda)= \mu\cdot(k/n)-\lambda.\]

Thus, we get

\[\overline{\operatorname{df}}(\widehat{\bm{\beta}}_{\bm{I},\mu})=-\frac{\lambda -\mu\cdot(k/n)}{\mu-\lambda}.\]

In other words, we have

\[1-\overline{\operatorname{df}}(\widehat{\bm{\beta}}_{\bm{I},\mu})=\left(\frac{ \mu}{\mu-\lambda}\right)\cdot\left(1-\frac{k}{n}\right).\]

Multiplying \((1-\lambda/\mu)\) on both sides, we arrive at the desired relation. This completes the proof.

### Proof of Proposition 3

We prove this by matching the path (4) with the one in [15]. Let \(\gamma=p/n\), \(\psi=p/k\), \(H_{p}\) be the spectral distribution of \(\widehat{\bm{\Sigma}}=\bm{X}^{\top}\bm{X}/n\). The path from Equation (5) of [15] is given by the following equation:

\[\mu=(\psi-\gamma)\int\frac{r}{1+v(\mu,\gamma)r}\operatorname{d}\!H_{p}(r),\] (23)

where \(v(\mu,\gamma)\) is the unique solution to the following fixed-point equation:

\[\frac{1}{v(\mu,\gamma)}=\mu+\gamma\int\frac{r}{1+v(\mu,\gamma)r}\operatorname {d}\!H_{p}(r)=\psi\int\frac{r}{1+v(\mu,\gamma)r}\operatorname{d}\!H_{p}(r).\] (24)

For given \(\gamma\) and \(\mu\), we will show that \(\psi\) that solves (23) gives rise \(k=p/\psi\) that also solves (4) with \(\lambda=0\):

\[-\tfrac{1}{n}\operatorname{tr}\big{[}\tfrac{1}{n}\bm{X}\bm{X}^{\top}\big{(} \tfrac{1}{n}\bm{X}\bm{X}^{\top}+\mu\bm{I}_{n}\big{)}^{\dagger}\big{]}=-\frac{k }{n}.\]

Rearranging the above equation yields:

\[\frac{k}{n}=1-\mu\operatorname{\overline{tr}}\big{[}\big{(}\tfrac{1}{n}\bm{X} \bm{X}^{\top}+\mu\bm{I}_{n}\big{)}^{\dagger}\big{]}=1-\mu v(\mu,\gamma),\]

where the second equality is from Lemma B.2 of [15]. This implies that

\[\mu =\left(1-\frac{k}{n}\right)\frac{1}{v(\mu,\gamma)}\] \[=\left(1-\frac{k}{n}\right)\psi\int\frac{r}{1+v(\mu,\gamma)r} \operatorname{d}\!H_{p}(r)\] \[=(\psi-\gamma)\int\frac{r}{1+v(\mu,\gamma)r}\operatorname{d}\!H_ {p}(r),\]

where the second equality follows from (24). The above is the same as the path (23) in [15]. This finishes the proof.

### Proof of Proposition 4

We first describe the setup for the kernel ridge regression formulation and then show the desired equivalence.

_Setup._ Let \(K(\cdot,\cdot):\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) be a kernel function. Let \(\mathcal{H}\) denote the reproducing kernel Hilbert space associated with kernel \(K\). Kernel ridge regression with the subsampling matrix \(\bm{W}\) solves the following problem with tuning parameter \(\lambda\geq 0\):

\[\widehat{f}_{\bm{W},\lambda}=\operatorname*{argmin}_{f\in\mathcal{H}}\|\bm{W} \bm{y}-\bm{W}f(\bm{X})\|_{2}^{2}/n+\lambda\|f\|_{\mathcal{H}}^{2},\]

where \(f(\bm{X})=[f(\bm{x}_{1}),\dots,f(\bm{x}_{n})]^{\top}\). Kernel ridge regression predictions have a closed-form expression:

\[\widehat{f}_{\bm{W},\lambda}(\bm{x})=K(\bm{x},\bm{X})^{\top}\bm{W}^{\top}( \bm{W}K(\bm{X},\bm{X})\bm{W}^{\top}+\lambda\bm{I}_{n})^{\dagger}\bm{W}\bm{y}.\]Here, \(K(\bm{x},\bm{X})\in\mathbb{R}^{n}\) with \(i\)-th entry \(K(\bm{x},\bm{x}_{i})\), and \(K(\bm{X},\bm{X})\in\mathbb{R}^{n\times n}\) with the \(ij\)-th entry \(K(\bm{x}_{i},\bm{x}_{j})\).

The predicted values on the training data \(\bm{X}\) are given by

\[\widehat{f}_{\bm{W},\lambda}(\bm{X})=K(\bm{X},\bm{X})^{\top}\bm{W}^{\top}(\bm{ W}K(\bm{X},\bm{X})\bm{W}^{\top}+\lambda\bm{I}_{n})^{\dagger}\bm{W}\bm{y}.\]

Here, the matrix \(K(\bm{X},\bm{X})^{\top}\bm{W}^{\top}(\bm{W}K(\bm{X},\bm{X})\bm{W}^{\top}+ \lambda\bm{I}_{n})^{\dagger}\bm{W}\) is the smoothing matrix.

Define \(\bm{G}_{\bm{I}}=K(\bm{X},\bm{X})\) and \(\bm{G}_{\bm{W}}=\bm{W}K(\bm{X},\bm{X})\bm{W}^{\top}\). Leveraging the kernel trick, the preceding optimization problem translates into solving the following problem (in the dual domain):

\[\widehat{\bm{\alpha}}_{\bm{W},\lambda}=\operatorname*{argmin}_{\bm{\alpha}\in \mathbb{R}^{n}}\bm{\alpha}^{\top}\left(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n}\right) \bm{\alpha}+2\bm{\alpha}^{\top}\bm{W}\bm{y},\]

where the dual solution is given by \(\widehat{\bm{\alpha}}_{\bm{W},\lambda}=(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n})^{ \dagger}\bm{W}\bm{y}\). The correspondence between the dual and primal solutions is simply given by: \(\widehat{\bm{\beta}}_{\bm{W},\lambda}=\bm{\Phi}^{\top}\bm{W}^{\top}\widehat{ \bm{\alpha}}_{\bm{W},\lambda}\) where \(\bm{\Phi}=[\phi(\bm{x}_{1}),\ldots,\phi(\bm{x}_{n})]^{\top}\) is the feature matrix and \(\phi\colon\mathbb{R}^{d}\mapsto\mathcal{H}\) is the feature map of the Hilbert space \(\mathcal{H}\) with kernel \(K\). Thus, \(\widehat{f}_{\bm{W},\lambda}(\bm{X})=\bm{W}\bm{\Phi}\widehat{\bm{\beta}}_{\bm {W},\lambda}=\bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}\widehat{\bm{\alpha} }_{\bm{W},\lambda}=\bm{G}_{\bm{W}}(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n})^{ \dagger}\bm{W}\bm{y}\) and the degrees of freedom is given by \(\text{df}(\widehat{\bm{\beta}}_{\bm{I},\mu})=\operatorname{tr}[\bm{G}_{\bm{W} }(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n})^{\dagger}]\).

Next, we show that (3) holds. Alternatively, one can also show that

\[\widehat{\bm{\alpha}}_{\bm{W},\lambda}\simeq\widehat{\bm{\alpha}}_{\bm{I}, \mu},\quad\text{and}\quad\widehat{f}_{\bm{W},\lambda}(\bm{x}_{0})\simeq \widehat{f}_{\bm{I},\mu}(\bm{x}_{0}),\]

which we omit due to similarity. Our proof strategy consists of two steps. We first show that it suffices to establish the desired result for the linearized version. We then show that we can suitably adapt our result for the linearized version.

_Linearization of kernels._ In the below, we will show that for \(\mu\geq 0\),

\[\bm{W}^{\top}(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n})^{\dagger}\bm{W} \simeq(\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{\dagger},\] \[\operatorname{\overline{tr}}[\lambda(\bm{G}_{\bm{W}}+\lambda\bm{ I}_{n})^{\dagger}] \simeq\operatorname{\overline{tr}}[\mu(\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{ \dagger}],\]

where \(\bm{W}\) and \(\lambda\geq 0\) satisfy that

\[\mu=\lambda\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\tfrac{1}{n} \operatorname{tr}[\bm{G}_{\bm{I}}(\bm{G}_{\bm{I}}+\lambda\bm{I}_{n})^{\dagger}] )=\lambda\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\tfrac{1}{n}\operatorname{tr}[ \bm{G}_{\bm{W}}(\bm{G}_{\bm{W}}+\mu\bm{I}_{n})^{\dagger}]).\] (25)

Using assumptions of Proposition 3 and the assumption in Proposition 4, by [18, Proposition 5.1],2

Footnote 2: Assumption A1 of [18] requires finite \(5+\delta\)-moments, which can be relaxed to only finite \(4+\delta\)-moments as in the assumption of Proposition 3, by a truncation argument as in the proof of Theorem 6 of [7, Appendix A.4].

\[\|\bm{G}_{\bm{I}}-\bm{G}_{\bm{I}}^{\text{lin}}\|_{\mathrm{op}}\xrightarrow{ \mathrm{P}}0,\]

where

\[\bm{G}_{\bm{I}}^{\text{lin}}=c_{0}\bm{I}_{n}+c_{1}\bm{1}_{n}\bm{1}_{n}^{\top}+c _{2}\bm{X}\bm{X}^{\top}\]

and \((c_{0},c_{1},c_{2})\) associated with function \(g\) in Proposition 4 and \(\tau=\lim_{p\to\infty}\operatorname{tr}[\bm{\Sigma}]/p\) are defined as

\[c_{0} =g(\tau,\tau,\tau)-g(\tau,0,\tau)-c_{2}\frac{\operatorname{tr}[ \bm{\Sigma}]}{p},\] (26) \[c_{1} =g(\tau,0,\tau)+g^{\prime\prime}(\tau,0,\tau)\frac{\operatorname{ tr}[\bm{\Sigma}^{2}]}{2p^{2}},\] (27) \[c_{2} =g^{\prime}(\tau,0,\tau).\] (28)

Assume \(\bm{C}_{n}\) is a sequence of random matrices with bounded trace norm. Note that

\[\operatorname{tr}[\bm{C}_{p}((\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{ \dagger}-(\bm{G}_{\bm{I}}^{\text{lin}}+\mu\bm{I}_{n})^{\dagger})]\] \[\leq\operatorname{tr}[\bm{C}_{p}]\|(\bm{G}_{\bm{I}}+\mu\bm{I}_{n} )^{\dagger}-(\bm{G}_{\bm{I}}^{\text{lin}}+\mu\bm{I}_{n})^{\dagger}\|_{\mathrm{op}}\] \[\leq\operatorname{tr}[\bm{C}_{p}]\|(\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{ \dagger}\|_{\mathrm{op}}\|(\bm{G}_{\bm{I}}^{\text{lin}}+\mu\bm{I}_{n})^{ \dagger}\|_{\mathrm{op}}\|\bm{G}_{\bm{I}}-\bm{G}_{\bm{I}}^{\text{lin}}\|_{ \mathrm{op}}\xrightarrow{\text{a.s.}}0.\]where in the last inequality, we use a matrix identity \(\bm{A}^{-1}-\bm{B}^{-1}=\bm{A}^{-1}(\bm{B}-\bm{A})\bm{B}^{-1}\) for two invertible matrices \(\bm{A}\) and \(\bm{B}\). Thus, we have

\[(\bm{G}_{\bm{I}}+\mu\bm{I}_{n})^{\dagger}\simeq_{p}(\bm{G}_{\bm{I}}^{\rm lin}+ \mu\bm{I}_{n})^{\dagger}.\] (29)

Hence, combining (29) and the transition property of asymptotic equivalence (15, Lemma S.7.4 (1)), it suffices to show

\[\bm{W}^{\top}(\bm{G}_{\bm{W}}^{\rm lin}+\lambda\bm{I}_{n})^{\dagger}\bm{W} \simeq(\bm{G}_{\bm{I}}^{\rm lin}+\mu\bm{I}_{n})^{\dagger},\]

where \(\bm{G}_{\bm{W}}^{\rm lin}=\bm{W}\bm{G}_{\bm{I}}^{\rm lin}\bm{W}^{\top}\), and \(\lambda\) and \(\mu\) satisfy (25). Similarly, we can also show that the path (25) is asymptotically equivalent to

\[\mu=\lambda\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\tfrac{1}{n}\operatorname{tr}[ \bm{G}_{\bm{I}}^{\rm lin}(\bm{G}_{\bm{I}}^{\rm lin}+\lambda\bm{I}_{n})^{ \dagger}])=\lambda\mathcal{S}_{\bm{W}^{\top}\bm{W}}(-\tfrac{1}{n}\operatorname {tr}[\bm{G}_{\bm{W}}^{\rm lin}(\bm{G}_{\bm{W}}^{\rm lin}+\mu\bm{I}_{n})^{ \dagger}]).\] (30)

_Equivalence for linearized kernels._ We next show that the resolvent equivalence result holds for \(\bm{K}^{\rm lin}\). This follows from additional manipulations building on Lemma 21.

### Proof of Proposition 5

In the below, we will show that for \(\mu\geq 0\),

\[\bm{W}^{\top}(\bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}/n+ \lambda\bm{I}_{n})^{\dagger}\bm{W} \simeq(\bm{\Phi}\bm{\Phi}^{\top}/n+\mu\bm{I}_{n})^{\dagger},\] \[\operatorname{\widetilde{tr}}[\lambda(\bm{W}\bm{\Phi}\bm{\Phi}^{ \top}\bm{W}^{\top}/n+\lambda\bm{I}_{n})^{\dagger}] \simeq\operatorname{\widetilde{tr}}[\mu(\bm{\Phi}\bm{\Phi}^{\top}/n+ \mu\bm{I}_{n})^{\dagger}].\]

Under assumptions in Proposition 5, the linearized features take the form

\[\bm{\Phi}^{\rm lin}=\sqrt{\frac{\rho_{s}}{d}}\bm{F}\bm{X}+\sqrt{\rho_{s}\omega _{s}}\bm{U},\]

where the constants \(\rho_{s}\) and \(\omega_{s}\) are given in Proposition 5 and \(\bm{U}\in\mathbb{R}^{n\times p}\) has i.i.d. standard normal entries. From Claim A.13 of [10], the linear functionals of the estimators \(\widehat{\bm{\beta}}_{\bm{D},\lambda}\) and \(\widehat{\bm{\beta}}_{\bm{I},\mu}\) with random features \(\bm{\Phi}\) and \(\bm{\Phi}^{\rm lin}\) are asymptotically equivalent. Now, following the proof of Proposition 4, we apply Lemma 21 on \(\bm{\Phi}^{\rm lin}\) to yield the desired result.

### Technical lemmas

In preparation for the forthcoming statement, define \(\lambda_{0}=-\liminf_{n\to\infty}\lambda_{\min}^{+}(\bm{G}_{\bm{W}})\). Recall the Gram matrices \(\bm{G}=\bm{\Phi}\bm{\Phi}^{\top}/n\) and \(\bm{G}_{\bm{W}}=\bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}/n\).

**Lemma 21** (General first-order equivalence for freely subsampled ridge resolvents).: For \(\bm{W}\in\mathbb{R}^{n\times n}\), suppose Assumption A holds for \(\bm{W}\bm{W}^{\top}\). Then, for all \(\lambda>\lambda_{0}\),

\[\bm{W}^{\top}(\bm{G}_{\bm{W}}+\lambda\bm{I})^{\dagger}\bm{W} \simeq(\bm{G}+\mu\bm{I})^{\dagger},\] (31) \[\operatorname{\widetilde{tr}}[\lambda(\bm{G}_{\bm{W}}+\lambda \bm{I}_{n})^{\dagger}] \simeq\operatorname{\widetilde{tr}}[\mu(\bm{G}+\mu\bm{I}_{n})^{ \dagger}],\] (32)

where \(\mu>-\lambda_{\min}^{+}(\bm{G})\) solves the equation:

\[\mu=\lambda\mathcal{S}_{\bm{W}\bm{W}^{\top}}(-\operatorname{\widetilde{tr}}[ \bm{G}(\bm{G}+\mu\bm{V})^{\dagger}])\simeq\lambda\mathcal{S}_{\bm{W}\bm{W}^{ \top}}(-\operatorname{\widetilde{tr}}[\bm{G}_{\bm{W}}(\bm{G}_{\bm{W}}+\lambda \bm{V})^{\dagger}]).\] (33)

Proof of Lemma 21.: The first result follows from using Theorem 19 by suitably changing the roles of \(\bm{X}\) and \(\bm{\Phi}\). In particular, we set \(\bm{\Phi}\) to be \(\bm{X}^{\top}\) and \(\bm{W}\) to be \(\bm{S}\) and apply Theorem 19 to obtain

\[\bm{W}^{\top}(\bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}/n+\lambda\bm{I}_{n} )^{\dagger}\bm{W}\simeq(\bm{\Phi}\bm{\Phi}^{\top}/n+\mu\bm{I}_{n})^{\dagger}.\] (34)

Writing in terms of \(\bm{G}\) and \(\bm{G}_{\bm{W}}\), this proves the first part (31).

For the second part, we use the result (34) in the first part and multiply both sides by \(\bm{\Phi}\bm{\Phi}^{\top}/n\) to get

\[(\bm{\Phi}\bm{\Phi}^{\top}/n)\cdot\bm{W}^{\top}(\bm{W}\bm{\Phi}\bm{\Phi}^{\top} \bm{W}^{\top}/n+\lambda\bm{I}_{n})^{\dagger}\bm{W} \simeq(\bm{\Phi}\bm{\Phi}^{\top}/n)\cdot(\bm{\Phi}\bm{\Phi}^{\top}/n+\mu\bm{ I}_{n})^{\dagger}.\]

Using the trace property of asymptotic equivalence (15, Lemma S.7.4 (4)), we have

\[\operatorname{\widetilde{tr}}[(\bm{\Phi}\bm{\Phi}^{\top}/n)\cdot\bm{W}^{\top}( \bm{W}\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}/n+\lambda\bm{I}_{n})^{\dagger}\bm{ W}]\simeq\operatorname{\widetilde{tr}}[(\bm{\Phi}\bm{\Phi}^{\top}/n)\cdot(\bm{\Phi}\bm{ \Phi}^{\top}/n+\mu\bm{I}_{n})^{\dagger}].\]Using the cyclic property of the trace operator yields

\[\overline{\mathrm{tr}}[(\bm{W}\bm{\Phi}\bm{\Phi}^{\top}/n)\cdot\bm{W}^{\top}(\bm{W }\bm{\Phi}\bm{\Phi}^{\top}\bm{W}^{\top}/n+\lambda\bm{I}_{n})^{\dagger}]\simeq \overline{\mathrm{tr}}[(\bm{\Phi}\bm{\Phi}^{\top}/n)\cdot(\bm{\Phi}\bm{\Phi}^{ \top}/n+\mu\bm{I}_{n})^{\dagger}].\]

In terms of \(\bm{G}\) and \(\bm{G}_{\bm{W}}\), this is the same as

\[\overline{\mathrm{tr}}[\bm{G}_{\bm{W}}(\bm{G}_{\bm{W}}+\lambda\bm{I}_{n})^{ \dagger}]\simeq\overline{\mathrm{tr}}[\bm{G}(\bm{G}+\mu\bm{I}_{n})^{\dagger}].\]

Adding and subtracting \(\lambda\bm{I}_{n}\) and \(\mu\bm{I}_{n}\) on the left- and right-hand resolvents, we arrive at the second part (32). This completes the proof. 

## Appendix C Proofs in Section 4

### Proof of Theorem 6

The main ingredients of the proof are Lemmas 21 and 22. We begin by decomposing the unknown response \(y_{0}\) into its linear predictor and residual. Specifically, let \(\bm{\beta}_{0}\) be the optimal projection parameter given by \(\bm{\beta}_{0}=\bm{\Sigma}_{0}^{-1}\mathbb{E}[\bm{\phi}_{0}y_{0}]\). Then, we can express the response as the sum of its best linear predictor, \(\bm{\phi}_{0}^{\top}\bm{\beta}_{0}\), and the residual, \(y_{0}-\bm{\phi}_{0}^{\top}\bm{\beta}_{0}\). Denote the variance of this residual by \(\sigma_{0}^{2}=\mathbb{E}[(y_{0}-\bm{\phi}_{0}^{\top}\bm{\beta}_{0})^{2}]\). It is easy to see that the risk decomposes as follows:

\[R(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}) =\mathbb{E}\big{[}(y_{0}-\bm{\phi}_{0}^{\top}\widehat{\bm{\beta}} _{\bm{W}_{1:M},\lambda})^{2}\mid\bm{\Phi},\bm{y},\{\bm{W}_{m}\}_{m=1}^{M}\big{]}\] \[=(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}-\bm{\beta}_{0})^{ \top}\bm{\Sigma}(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}-\bm{\beta}_{0})+ \sigma_{0}^{2}.\]

Here, we used the fact that \((y_{0}-\bm{\phi}_{0}^{\top}\bm{\beta}_{0})\) is uncorrelated with \(\bm{\phi}_{0}\), that is, \(\mathbb{E}[\bm{\phi}_{0}(y_{0}-\bm{\phi}_{0}^{\top}\bm{\beta}_{0})]=\bm{0}_{p}\). We note that \(\|\bm{\beta}_{0}\|_{2}<\infty\) and \(\bm{\Sigma}_{0}\) has uniformly bounded operator norm.

Observe that

\[R(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}) =(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}-\bm{\beta}_{0})^{ \top}\bm{\Sigma}_{0}(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}-\bm{\beta}_{ 0})+\sigma_{0}^{2}\] \[=\bigg{(}\frac{1}{M}\sum_{m=1}^{M}\widehat{\bm{\beta}}_{\bm{W}_{ m},\lambda}-\bm{\beta}_{0}\bigg{)}^{\top}\bm{\Sigma}_{0}\bigg{(}\frac{1}{M}\sum_{m= 1}^{M}\widehat{\bm{\beta}}_{\bm{W}_{m},\lambda}-\bm{\beta}_{0}\bigg{)}+\sigma_ {0}^{2}\] \[=\frac{1}{M^{2}}\sum_{k,\ell=1}^{M}\widehat{\bm{\beta}}_{\bm{W}_{ k},\lambda}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{W}_{\ell},\lambda}- \frac{2}{M}\sum_{m=1}^{M}\bm{\beta}_{0}^{\top}\bm{\Sigma}_{0}\widehat{\bm{ \beta}}_{\bm{W}_{m},\lambda}+\bm{\beta}_{0}^{\top}\bm{\Sigma}_{0}\bm{\beta}_{ 0}+\sigma_{0}^{2}\] \[=\frac{1}{M^{2}}\sum_{k,\ell=1}^{M}(\widehat{\bm{\beta}}_{\bm{W}_ {k},\lambda}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{W}_{\ell},\lambda}- \widehat{\bm{\beta}}_{\bm{I},\mu}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{ \bm{I},\mu})+\widehat{\bm{\beta}}_{\bm{I},\mu}^{\top}\bm{\Sigma}_{0}\widehat{ \bm{\beta}}_{\bm{I},\mu}\] \[\quad-\frac{2}{M}\sum_{m=1}^{M}\bm{\beta}_{0}^{\top}\bm{\Sigma}_{0 }\widehat{\bm{\beta}}_{\bm{W}_{m},\lambda}+\bm{\beta}_{0}^{\top}\bm{\Sigma}_{0 }\bm{\beta}_{0}+\sigma_{0}^{2}.\]

By Lemma 21, note that

\[\frac{1}{M}\sum_{k=1}^{M}\widehat{\bm{\beta}}_{\bm{W}_{m},\lambda}\simeq \widehat{\bm{\beta}}_{\bm{I},\mu}.\]

Thus, we have

\[R(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}) \simeq\frac{1}{M^{2}}\sum_{k,\ell=1}^{M}\big{(}\widehat{\bm{ \beta}}_{\bm{W}_{k},\lambda}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{W} _{\ell},\lambda}-\widehat{\bm{\beta}}_{\bm{I},\mu}^{\top}\bm{\Sigma}_{0} \widehat{\bm{\beta}}_{\bm{I},\mu}\big{)}\] \[\quad+\widehat{\bm{\beta}}_{\bm{I},\mu}^{\top}\bm{\Sigma}_{0} \widehat{\bm{\beta}}_{\bm{I},\mu}-\frac{2}{M}\sum_{m=1}^{M}\bm{\beta}_{0}^{ \top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{I},\mu}+\bm{\beta}_{0}^{\top} \bm{\Sigma}_{0}\bm{\beta}_{0}+\sigma_{0}^{2}.\]

Now, by two applications of Lemma 21, we know that \(\widehat{\bm{\beta}}_{\bm{W}_{k},\lambda}^{\top}\bm{\Sigma}_{0}\widehat{\bm{ \beta}}_{\bm{W}_{\ell},\lambda}-\widehat{\bm{\beta}}_{\bm{I},\mu}\bm{\Sigma}_{0} \widehat{\bm{\beta}}_{\bm{I},\mu}\xrightarrow{\text{a.s.}}0\) when \(k\neq\ell\) since \(\bm{W}_{k}\) and \(\bm{W}_{\ell}\) are independent. Hence, we have

\[R(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda})\simeq\frac{1}{M^{2}}\sum_{m=1}^{M }\big{(}\widehat{\bm{\beta}}_{\bm{W}_{m},\lambda}^{\top}\bm{\Sigma}_{0}\widehat{ \bm{\beta}}_{\bm{W}_{m},\lambda}-\widehat{\bm{\beta}}_{\bm{I},\mu}^{\top}\bm{ \Sigma}_{0}\widehat{\bm{\beta}}_{\bm{I},\mu}\big{)}+(\widehat{\bm{\beta}}_{\bm{I},\mu}-\bm{\beta}_{0})^{\top}\bm{\Sigma}_{0}(\widehat{\bm{\beta}}_{\bm{I},\mu}- \bm{\beta}_{0})+\sigma_{0}^{2}\]\[\simeq\frac{1}{M}\big{(}\widehat{\bm{\beta}}_{\bm{W},\lambda}^{\top} \bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{W},\lambda}-\widehat{\bm{\beta}}_{\bm{ I},\mu}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{I},\mu}\big{)}+( \widehat{\bm{\beta}}_{\bm{I},\mu}-\bm{\beta}_{0})^{\top}\bm{\Sigma}_{0}( \widehat{\bm{\beta}}_{\bm{I},\mu}-\bm{\beta}_{0})+\sigma_{0}^{2}\] \[=\frac{1}{M}\big{(}\widehat{\bm{\beta}}_{\bm{W},\lambda}^{\top} \bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{W},\lambda}-\widehat{\bm{\beta}}_{\bm {I},\mu}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{I},\mu}\big{)}+R( \widehat{\bm{\beta}}_{\bm{I},\mu})\] (35)

where we used the fact that the \(M\) terms where \(k=\ell\) converge identically in the second to last line and a risk decomposition similar to that for \(\widehat{\bm{\beta}}_{\bm{W}_{1:M},\lambda}\) in the last line. Thus, it suffices to evaluate the difference \(\widehat{\bm{\beta}}_{\lambda}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{ \lambda}-\widehat{\bm{\beta}}_{\mu}^{\top}\bm{\Sigma}_{0}\widehat{\bm{\beta}}_ {\mu}\) to finish the proof.

We have

\[\widehat{\bm{\beta}}_{\bm{W},\lambda}^{\top}\bm{\Sigma}_{0} \widehat{\bm{\beta}}_{\bm{W},\lambda}-\widehat{\bm{\beta}}_{\bm{I},\mu}^{\top }\bm{\Sigma}_{0}\widehat{\bm{\beta}}_{\bm{I},\mu}\] \[=(\bm{y}^{\top}\bm{W}^{\top}/n)\bm{W}\bm{\Phi}(\bm{\Phi}^{\top}\bm {W}^{\top}\bm{W}\bm{\Phi}/n+\lambda\bm{I}_{p})^{\dagger}\bm{\Sigma}_{0}(\bm{ \Phi}^{\top}\bm{W}^{\top}\bm{W}\bm{\Phi}/n+\lambda\bm{I}_{p})^{\dagger}\bm{ \Phi}^{\top}\bm{W}^{\top}(\bm{W}\bm{y}/n)\] \[\quad-(\bm{y}^{\top}\bm{\Phi}/n)(\bm{\Phi}^{\top}\bm{\Phi}/n+\mu \bm{I}_{p})^{\dagger}\bm{\Sigma}_{0}(\bm{\Phi}^{\top}\bm{\Phi}/n+\mu\bm{I}_{p })^{\dagger}(\bm{\Phi}^{\top}\bm{y}/n)\] \[=\operatorname{\widetilde{tr}}[\bm{W}^{\top}\bm{W}\bm{\Phi}(\tfrac {1}{n}\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W}\bm{\Phi}+\lambda\bm{I}_{p})^{\dagger} \bm{\Sigma}_{0}(\tfrac{1}{n}\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W}\bm{\Phi}+ \lambda\bm{I}_{p})^{\dagger}\bm{\Phi}^{\top}\bm{W}^{\top}\bm{W}/n\cdot(\bm{y} \bm{y}^{\top})]\] \[\quad-\operatorname{\widetilde{tr}}[\bm{\Phi}(\bm{\Phi}^{\top} \bm{\Phi}/n+\mu\bm{I}_{p})^{\dagger}\bm{\Sigma}_{0}(\bm{\Phi}^{\top}\bm{\Phi} /n+\mu\bm{I}_{p})^{\dagger}\bm{\Phi}^{\top}/n\cdot(\bm{y}\bm{y}^{\top})]\] \[\simeq\operatorname{\widetilde{tr}}[(\bm{\Phi}\bm{\Phi}^{\top}/n+ \mu\bm{I}_{n})^{\dagger}(\bm{\Phi}\bm{\Sigma}_{0}\bm{\Phi}^{\top}/n+\mu^{ \prime}_{\bm{\Sigma}_{0}}\bm{I}_{n})(\bm{\Phi}\bm{\Phi}^{\top}/n+\mu\bm{I}_{n })^{\dagger}(\bm{y}\bm{y}^{\top})]\] \[\quad-\operatorname{\widetilde{tr}}[\bm{\Phi}(\bm{\Phi}^{\top}\bm {\Phi}/n+\mu\bm{I}_{p})^{\dagger}\bm{\Sigma}_{0}(\bm{\Phi}^{\top}\bm{\Phi}^{ \top}\bm{\Phi}/n+\mu\bm{I}_{p})^{\dagger}\bm{\Phi}^{\top}/n\cdot(\bm{y}\bm{y}^ {\top})]\] \[=\operatorname{\widetilde{tr}}[(\bm{\Phi}\bm{\Phi}^{\top}/n+\mu \bm{I}_{n})^{\dagger}(\bm{\Phi}\bm{\Sigma}_{0}\bm{\Phi}^{\top}/n)(\bm{\Phi}\bm{ \Phi}^{\top}/n+\mu\bm{I}_{n})^{\dagger}(\bm{y}\bm{y}^{\top})]\] \[\quad+\mu^{\prime}_{\bm{\Sigma}_{0}}\operatorname{\widetilde{tr}} [(\bm{\Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger}(\bm{y}\bm{y}^{\top})( \bm{\Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger}]\] \[\quad-\operatorname{\widetilde{tr}}[(\bm{\Phi}\bm{\Phi}^{\top}/n+ \mu\bm{I}_{n})^{\dagger}(\bm{\Phi}\bm{\Sigma}_{0}\bm{\Phi}^{\top}/n)(\bm{\Phi}\bm{ \Phi}^{\top}/n+\mu\bm{I}_{n})^{\dagger}(\bm{y}\bm{y}^{\top})]\] \[=\mu^{\prime}_{\bm{\Sigma}_{0}}\operatorname{\widetilde{tr}}[(\bm{ \Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger}(\bm{y}\bm{y}^{\top})(\bm{\Phi} \bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger}],\] (36)

where in the third line, we used the second-order equivalence for freely weighted ridge resolvents from Lemma 22; in the fourth line, we employed the push-through identity multiple times. Substituting for \(\mu^{\prime}_{\bm{\Sigma}_{0}}\) from Lemma 22 in (36) and substituting this back into (35), we arrive at the desired decomposition. This completes the proof.

### Proof of Proposition 7

We use the path (4) with \(k^{*}\) and \(\mu^{*}\), and setting \(\lambda^{*}=0\):

\[\left(1-\frac{\mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu^{*}})}{n}\right)= \left(1-\frac{k^{*}}{n}\right).\]

This suggests that

\[k^{*}=\mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu^{*}}).\]

Note that \(r:=\operatorname{rank}(\bm{X}^{\top}\bm{X})=\operatorname{rank}(\bm{G}_{\bm{ I}})\). By the definition of degrees of freedom, it follows that

\[\mathsf{df}(\widehat{\bm{\beta}}_{\bm{I},\mu^{*}}) =\operatorname{tr}[\bm{X}^{\top}\bm{X}(\bm{X}^{\top}\bm{X}+\mu^{ *}\bm{I}_{p})^{\dagger}]\] \[=\sum_{i=1}^{r}\frac{s_{i}}{s_{i}+\mu^{*}}\leq r=\operatorname{ rank}(\bm{G}_{\bm{I}}),\]

where \(s_{1},\dots,s_{r}\) are non-zero eigenvalues of \(\bm{X}^{\top}\bm{X}\). This finishes the proof.

### Technical lemmas

Recall from Appendix B.6 that we define \(\lambda_{0}=-\liminf_{n\to\infty}\lambda_{\min}^{+}(\bm{W}\bm{\Phi}\bm{\Phi}^{ \top}\bm{W}^{\top}/n)\).

**Lemma 22** (General second-order equivalence for freely weighted ridge resolvents).: Under the settings of Lemma 21, for any positive semidefinite \(\bm{\Sigma}_{0}\) with uniformly bounded operator norm, for all \(\lambda>\lambda_{0}\),

\[\tfrac{1}{n}\bm{W}^{\top}\bm{W}\bm{\Phi}(\tfrac{1}{n}\bm{\Phi}^{\top}\bm{W}^{ \top}\bm{W}\bm{\Phi}+\lambda\bm{I}_{p})^{\dagger}\bm{\Sigma}_{0}(\tfrac{1}{n} \bm{\Phi}^{\top}\bm{W}^{\top}\bm{W}\bm{\Phi}+\lambda\bm{I}_{p})^{\dagger}\bm{ \Phi}^{\top}\bm{W}^{\top}\bm{W}\]\[\simeq(\tfrac{1}{n}\bm{\Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger}( \tfrac{1}{n}\bm{\Phi}\bm{\Sigma}_{0}\bm{\Phi}^{\top}+\mu^{\prime}_{\bm{\Sigma}_{ 0}}\bm{I}_{n})(\tfrac{1}{n}\bm{\Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger},\] (37)

where \(\mu^{\prime}_{\bm{\Sigma}_{0}}\geq 0\) is given by:

\[\mu^{\prime}_{\bm{\Sigma}_{0}} =-\frac{\partial\mu}{\partial\lambda}\lambda^{2}\mathcal{S}^{ \prime}_{\bm{W}\bm{W}^{\top}}\big{(}-\tfrac{1}{n}\operatorname{tr}\big{[} \tfrac{1}{n}\bm{\Phi}\bm{\Phi}^{\top}(\tfrac{1}{n}\bm{\Phi}\bm{\Phi}^{\top}+\mu \bm{I}_{n})^{\dagger}\big{]}\big{)}\] \[\quad\cdot\tfrac{1}{p}\operatorname{tr}\big{[}(\tfrac{1}{n}\bm{ \Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{\dagger}(\tfrac{1}{n}\bm{\Phi}\bm{\Sigma} _{0}\bm{\Phi}^{\top})(\tfrac{1}{n}\bm{\Phi}\bm{\Phi}^{\top}+\mu\bm{I}_{n})^{ \dagger}\big{]}.\] (38)

Proof.: We use the Woodbury matrix identity to write

\[\tfrac{1}{n}\bm{W}\bm{W}^{\top}\bm{\Phi}(\tfrac{1}{n}\bm{\Phi}^{ \top}\bm{W}\bm{W}^{\top}\bm{\Phi}+\lambda\bm{I}_{p})^{\dagger}\bm{\Sigma}_{0}( \tfrac{1}{n}\bm{\Phi}^{\top}\bm{W}\bm{W}^{\top}\bm{\Phi}+\lambda\bm{I}_{p})^{ \dagger}\bm{\Phi}^{\top}\bm{W}\bm{W}^{\top}\] \[=\tfrac{1}{n}\bm{W}(\tfrac{1}{n}\bm{W}^{\top}\bm{\Phi}\bm{\Phi}^{ \top}\bm{W}+\lambda\bm{I}_{m})^{\dagger}\bm{W}^{\top}\bm{\Phi}\bm{\Sigma}_{0} \bm{\Phi}^{\top}\bm{W}(\tfrac{1}{n}\bm{W}^{\top}\bm{\Phi}\bm{\Phi}^{\top}\bm{W} +\lambda\bm{I}_{m})^{\dagger}\bm{W}^{\top}.\]

The equivalence in (37) and the inflation parameter in (38) now follow from the second-order result for feature sketch by substituting \(\bm{W}\) for \(\bm{S}\), \(\bm{\Phi}\) for \(\bm{\Phi}^{\top}\), and \(\tfrac{1}{n}\bm{\Phi}\bm{\Sigma}_{0}\bm{\Phi}^{\top}\) for \(\bm{\Sigma}_{0}\) in (18).

Additional illustrations for Section 3

### Implicit regularization paths for bootstrapping

### Implicit regularization paths with non-uniform weights

Figure 5: Equivalence under bootstrapping. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection \(\mathbb{E}_{\bm{W}}[\bm{\alpha}^{\top}\widehat{\bm{\beta}}_{\bm{W},\lambda}]\) where \(\bm{\alpha}\sim\mathcal{N}(\bm{0}_{p},\bm{I}_{p}/p)\). In both heatmaps, the red lines indicate the predicted paths using Equation (4), and the black dashed lines indicate the empirical paths obtained by matching empirical degrees of freedom. Despite the complexity of the theoretical path for bootstrapping, we observe that the empirical paths closely resemble it. Therefore, the theoretical path for sampling without replacement from (4) serves as a good approximation.

Figure 6: Equivalence under non-uniform weighting. The left panel shows the heatmap of degrees of freedom, and the right panel shows the random projection \(\mathbb{E}_{\bm{W}}[\bm{\alpha}^{\top}\widehat{\bm{\beta}}_{\bm{W},\lambda}]\), where \(\bm{\alpha}\sim\mathcal{N}(\bm{0}_{p},\bm{I}_{p}/p)\). The weights (\(\mathrm{diag}(\bm{W})\)) for observations are initially generated as \((9/10)^{i}\) for \(i=0,\dots,n-1\), subsample \(k\) entries from \(\{1,\dots,n\}\), zero out the other \(n-k\) entries, and then normalized to have norm \(k\). The black dashed lines indicate the empirical paths obtained by matching the empirical degrees of freedom.

Additional illustrations for Section 4

### Rate illustration for ensemble risk against ensemble size

### Real data illustrations for implicit regularization paths

## Appendix F Details of experiments

### Simulation details

The simulation settings are as follows.

* _Covariance model._ The covariance matrix of an auto-regressive process of order 1 (AR(1)) is given by \(\mathbf{\Sigma}_{\mathrm{ar1}}\in\mathbb{R}^{d\times d}\), where \((\mathbf{\Sigma}_{\mathrm{ar1}})_{ij}=\rho_{\mathrm{ar1}}^{|i-j|}\) for some parameter \(\rho_{\mathrm{ar1}}\in(0,1)\). For the simulations, we set \(\rho_{\mathrm{ar1}}=0.25\).

Figure 8: Equivalence in pretrained features of pretrained ResNet-18 on CIFAR-10 dataset.

Figure 7: Risk equivalence for random feature structures when sampling without replacement. The solid lines represent the prediction risks and their estimates of the subsample ridge ensemble, and the red dashed lines indicate the prediction error of the full ridge predictor. The data and random features with the ReLU activation function are generated according to Appendix F.1 with \(n=5000\) and \(p=500\). The regularization level for the full ridge is set as \(\mu=1\), and each subsampled ridge ensemble is fitted with \(M=100\) randomly sampled subsampling matrices. For each value of \(\lambda\), the subsample ratio is determined by solving Equation (4).

Figure 9: Equivalence in features of randomly initialized ResNet-18 on Fashion-MNIST dataset.

* _Signal model._ Define \(\bm{\beta}_{0}=\frac{1}{5}\sum_{j=1}^{5}\bm{w}_{(j)}\) where \(\bm{w}_{(j)}\) is the eigenvector of \(\bm{\Sigma}_{\mathrm{ar1}}\) associated with the top \(j\)th eigenvalue \(r_{(j)}\).
* _Response model._ We generated data \((\bm{x}_{i},y_{i})\in\mathbb{R}^{d}\times\mathbb{R}\) for \(i=1,\ldots,n\) from a nonlinear model: \[y_{i}=\bm{x}_{i}^{\top}\bm{\beta}_{0}+\frac{1}{p}(\|\bm{x}_{i}\|_{2}^{2}- \mathrm{tr}[\bm{\Sigma}_{\mathrm{ar1}}])+\varepsilon_{i},\quad\bm{x}_{i}=\bm{ \Sigma}_{\mathrm{ar1}}^{\frac{1}{2}}\bm{z}_{i},\quad z_{ij}\overset{iid}{ \sim}\frac{t_{5}}{\sigma_{5}},\quad\varepsilon_{i}\sim\frac{t_{5}}{\sigma_{5}},\] (M-AR1) where \(\sigma_{5}=\sqrt{5/3}\) is the standard deviation of \(t_{5}\) distribution.

The benefit of using the above nonlinear model is that we can clearly separate the linear and the nonlinear components and compute the quantities of interest because \(\bm{\beta}_{0}\) happens to be the best linear projection.

The linear, random, and kernel features are generated as follows.

* _Linear features._ For a given feature dimension \(p\), we use \(d=p\) raw features from (M-AR1) as linear features.
* _Random features._ For generating random features, we use \(d=2p\) raw features from (M-AR1) and sample a randomly initialized weight matrix \(\bm{F}\in\mathbb{R}^{p\times d}\) whose entries are i.i.d. samples from \(\mathcal{N}(0,d^{-1/2})\). Then the transform feature is given by \(\widetilde{\bm{x}}_{i}=\varphi(\bm{F}\bm{x}_{i})\in\mathbb{R}^{p}\), where \(\varphi\) is a nonlinear transformation and set to be ReLU function in our experiment.
* _Kernel features._ For kernel features, we use \(d=p\) raw features from (M-AR1) to construct the kernel matrix.

In the simulations, the estimates are averaged across \(20\) simulations with different random seeds.

### Experimental details in Section 4.3

Following the similar experimental setup in [20], we use residual networks to extract features on several computer vision datasets, both at random initialization and after pretraining. More specifically, we consider ResNet-{18, 34, 50, 101} applied to the CIFAR-{10,100} [9], Fashion-MNIST [21], Flowers-102 [14], and Food-101 [4] datasets. All random initialization was done following [8]; pretrained networks (obtained from PyTorch) were pretrained on ImageNet, and the outputs of the last pretrained layer on each dataset mentioned above were used as the embedding feature \(\bm{\Phi}\).

After obtaining the embedding features from the last layer of the neural network model, we further normalize each row of the pretrained feature to have a norm of \(p\), and center the one-hot labels to have zero means. To reduce the computational burden, we only consider the first 10 one-hot labels of all datasets. For datasets with different data aspect ratios, we stratify 10% of the training samples as the training set for the CIFAR-100 dataset. The training and predicting errors are the mean square errors on the training and test sets, respectively, aggregated over all the labels.

## References

* [1] Adlam, B., Levinson, J. A., and Pennington, J. (2022). A random matrix perspective on mixtures of nonlinearities in high dimensions. In _International Conference on Artificial Intelligence and Statistics_.
* [2] Adlam, B. and Pennington, J. (2020). The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In _International Conference on Machine Learning_.
* [3] Bose, A. (2021). _Random Matrices and Non-Commutative Probability_. CRC Press.
* [4] Bossard, L., Guillaumin, M., and Van Gool, L. (2014). Food-101-mining discriminative components with random forests. In _Computer Vision-ECCV 2014: 13th European Conference_. Springer.

* Dobriban and Sheng (2020) Dobriban, E. and Sheng, Y. (2020). Wonder: Weighted one-shot distributed ridge regression in high dimensions. _Journal of Machine Learning Research_, 21(66):1-52.
* Dobriban and Sheng (2021) Dobriban, E. and Sheng, Y. (2021). Distributed linear regression by averaging. _The Annals of Statistics_, 49(2):918-943.
* Hastie et al. (2022) Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. _The Annals of Statistics_, 50(2):949-986.
* He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _International Conference on Computer Vision_.
* Krizhevsky and Hinton (2009) Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images.
* Lee et al. (2023) Lee, D., Moniri, B., Huang, X., Dobriban, E., and Hassani, H. (2023). Demystifying disagreement-on-the-line in high dimensions. In _International Conference on Machine Learning_.
* LeJeune et al. (2024) LeJeune, D., Patil, P., Javadi, H., Baraniuk, R. G., and Tibshirani, R. J. (2024). Asymptotics of the sketched pseudoinverse. _SIAM Journal on Mathematics of Data Science_, 6(1):199-225.
* Mel and Pennington (2021) Mel, G. and Pennington, J. (2021). Anisotropic random feature regression in high dimensions. In _International Conference on Learning Representations_.
* Mingo and Speicher (2017) Mingo, J. A. and Speicher, R. (2017). _Free Probability and Random Matrices_, volume 35. Springer.
* Nilsback and Zisserman (2008) Nilsback, M.-E. and Zisserman, A. (2008). Automated flower classification over a large number of classes. In _Indian Conference on Computer Vision, Graphics and Image Processing_.
* Patil and Du (2023) Patil, P. and Du, J.-H. (2023). Generalized equivalences between subsampling and ridge regularization. _Advances in Neural Information Processing Systems_.
* Patil et al. (2023) Patil, P., Du, J.-H., and Kuchibhotla, A. K. (2023). Bagging in overparameterized learning: Risk characterization and risk monotonization. _Journal of Machine Learning Research_, 24(319):1-113.
* Patil and LeJeune (2024) Patil, P. and LeJeune, D. (2024). Asymptotically free sketched ridge ensembles: Risks, cross-validation, and tuning. In _International Conference on Learning Representations_.
* Sahraee-Ardakan et al. (2022) Sahraee-Ardakan, M., Emami, M., Pandit, P., Rangan, S., and Fletcher, A. K. (2022). Kernel methods and multi-layer perceptrons learn linear models in high dimensions. _arXiv preprint arXiv:2201.08082_.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Dataset** & **Model** & **Number of train samples** & **Number of test samples** & **Number of pretrained features** \\ \hline \multirow{2}{*}{Fashion-MNIST} & ResNet-18 & \multirow{2}{*}{6000} & \multirow{2}{*}{10000} & \multirow{2}{*}{512} \\  & init. & & & \\ \multirow{2}{*}{CIFAR-10} & ResNet-18 & \multirow{2}{*}{50000} & \multirow{2}{*}{10000} & \multirow{2}{*}{512} \\  & pretr. & & & \\ \multirow{2}{*}{CIFAR-100 (subset)} & ResNet-50 & \multirow{2}{*}{5000} & \multirow{2}{*}{10000} & \multirow{2}{*}{2048} \\  & pretr. & & & \\ \multirow{2}{*}{Flowers-102} & ResNet-50 & \multirow{2}{*}{2040} & \multirow{2}{*}{6149} & \multirow{2}{*}{2048} \\  & pretr. & & & \\ \multirow{2}{*}{Food-101} & ResNet-101 & \multirow{2}{*}{75750} & \multirow{2}{*}{25250} & \multirow{2}{*}{2048} \\  & pretr. & & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of pretrained features from different real datasets.

* [19] Voiculescu, D. V. (1997). _Free Probability Theory_. American Mathematical Society.
* [20] Wei, A., Hu, W., and Steinhardt, J. (2022). More than a toy: Random matrix models predict how real-world neural representations generalize. In _International Conference on Machine Learning_.
* [21] Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All the claims made the abstract are justified by both theoretical and experimental results in Sections 3 and 4 and the appendix. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The assumptions are discussed and explained after their first mention (either in Section 2 or right after the theoretical result that uses them). The main limitations of the paper are discussed in the last section (Section 5). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions for each of the results are included in the main text, and the complete proofs for each of the results are included in the appendix. The beginning of the appendix provides an organization for the proofs of all the results mentioned in the main text. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The code and instructions for reproducing experimental results in this paper are included in the supplementary materials. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We use publicly available data. The code and instructions for reproducing experimental results in this paper are included in the supplementary materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental details are included in the appendix, and the source code is provided in the supplementary materials. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The standard errors across multiple random seeds are included for Figure 7. Note that for the heatmaps, we only report the mean statistics because of visual constraints. However, the standard errors for the heatmaps are small enough not to impact the regularization paths indicated.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The compute resources are described in the README file of the submitted code in the supplementary materials. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [NA] Justification: The paper provides a theoretical analysis and does not have immediate societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not pose any risks that require safeguards. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper correctly cites papers of related assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.