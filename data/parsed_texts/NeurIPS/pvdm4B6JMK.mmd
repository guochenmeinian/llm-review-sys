# ChessGPT: Bridging Policy Learning and Language Modeling

 Xidong Feng

University College London

&Yicheng Luo

University College London

&Ziyan Wang

King's College London

&Hongrui Tang

University College London

&Mengyue Yang

University College London

&Kun Shao

Huawei Noah's Ark Lab

&David Mguni

Huawei Noah's Ark Lab

&Yali Du

King's College London

&Jun Wang

University College London

###### Abstract

When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose **ChessGPT**, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples **ChessCLIP** and **ChessGPT**, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model's chess ability. Experimental results validate our model and dataset's effectiveness. We open source our code, model, and dataset at https://github.com/waterhorsel/ChessGPT.

## 1 Introduction

In recent years, large language models (LLMs) based on transformer architectures [57] have show-cased remarkable capabilities far exceeding their original design as simple language modeling tools. This was especially notable following the advent of ChatGPT [38]. Stemming from causal language modeling, a plethora of recent studies have concentrated on developing efficient and powerful LLM base models [16, 6, 55, 53, 5], supervised fine-tuned models [52, 12, 3, 24] and models [38, 23, 64, 51] leveraging Reinforcement Learning from Human Feedback (RLHF) [13].

Concurrently, there has been a growing trend in employing Large Language Models (LLMs) as foundational elements for decision-making systems. These systems either depend on the expressive capacity of transformer architectures to execute imitation learning, thereby modeling complex behaviors [11, 22, 4], or they harness the common knowledge embedded within LLMs to facilitate the policy learning process [60, 17, 15, 2]. However, the dynamic interplay between policy learning and language modeling has been scarcely addressed. Human decision-making typically involves both: we draw upon historical policy interaction to refine our policy and also employ our thoughts for strategic

[MISSING_PAGE_EMPTY:2]

format, as well as a mixed game-language dataset, which offers the most straightforward interrelated data including articles, discussion, or commentary (language) on specific chess game replay (game).

**Models.** We introduce two models, ChessCLIP and ChessGPT, leveraging our datasets. These models showcase the potential for AI to learn from a mixture of replay data and language knowledge.

**Evaluations.** We design an extensive set of tasks to evaluate our models' abilities from three distinct perspectives: modeling ability, to gauge the model's proficiency in tracking game state; value judgement ability, measuring the model's capacity for value assessment and chess knowledge; and policy ability, to test the model's capability in decision-making. Our experimental results confirm that our models consistently outperform other LLM baselines in all evaluation tasks.

We illustrate our full pipeline in fig. 1. Our work primarily pursues two objectives. Firstly, we construct the whole pipeline on chess as an initial step in promoting research on the interaction/interplay between policy learning and language learning, as well as on the potential of language as a tool for action and understanding. Secondly, our efforts have yielded valuable by-products: the ChessGPT/CLIP models. These models possess practical applicability - they could potentially serve as effective Chess AI assistants for humans.

## 2 Related work

The pursuit of creating artificial intelligence capable of playing chess can be traced back to the very beginning of the history of computer science [56]. Chess engines today achieve superhuman-level performance by utilizing human knowledge [9] or self-play [47]. Recently, there has been increasing interest in improving the interpretability [32] of these systems and their alignment with human behavior [33] besides strong performance. A chess engine that aligns with human behavior may unlock many exciting opportunities, for example, they can be used as a personalized tutor for chess beginners [33]. Some research efforts also concentrated on employing LLMs to learn policies in Chess [36; 50]. However, these studies mainly center on small-scale datasets or limited training.

There has been increasing interest in leveraging Internet-scale knowledge for creating agents capable of generalizing across many tasks and capabilities [60; 17; 44]. For example, MineDojo [17] introduced a framework on Minecraft for understanding how to enable artificial agents to learn in an open-ended environment. More recently, there has been a surge in research that treats LLMs as agents, aiming to harness their internet-scale knowledge for decision-making tasks [31; 58; 62; 21]. In contrast to these studies which typically rely on powerful LLMs like GPT-4 [8], our paper concentrates more on training, especially the interplay between language modeling and policy learning.

## 3 A large-scale game and language dataset for chess

We introduce a large-scale game & language dataset by collecting chess-related materials from the Internet. Our dataset can be mainly divided into four categories: (1) The Game dataset, encompassing online chess match replay data involving worldwide human players and chess engines of varying skill levels. (2) The Language dataset, principally recording chess-associated knowledge, analyses, discussions, and news in the form of natural language (3) The Mixed Game-Language dataset, incorporating both game data and human natural language elements (such as game analysis or comments) in alignment. (4) The instruction-tuning and conversation dataset, consisting of instruction data and conversation data related to chess. We include comprehensive dataset descriptions, statistics, and text examples in Appendix D, and the procedure of data collection and pre-processing in Appendix E.

### Game dataset

Game replay data provide the most direct method for both humans and machines to grasp the play mechanics of Chess. In chess, these data are commonly stored in the Portable Game Notation (PGN2) format which is a standard plain text format as illustrated in Figure 2. A PGN starts with some headers that include metadata about the game. These headers include information such as the name of players, the Elo ratings, the opening play, and the game outcome. The headers are followed by a move text section that records the moves played by the two players in turn. The moves may be further annotated with comments enclosed in braces.

Footnote 2: We refer to Appendix B for readers who are not familiar with chess notation format.

Previous work [33] uses the moves recorded in PGNs for policy learning. The moves are interpreted as actions in a Markov Decision Process and the state position can be reconstructed by loading the PGN into a chess engine. However, PGNs may contain additional useful information beyond the individual moves made. For example, the Elo ratings in the headers may inform us about the relative strength of the players. Additional information included in the comments of the move text section can also be useful - some of the moves are annotated with evaluations generated by computer chess programs that predict the current advantage of the players. These additional annotations may be useful from a reinforcement learning perspective, e.g., for value function learning. For this reason, we curated the game dataset with all of this information intact to facilitate policy learning.

**Lichess dataset** We collect five months of online game data from the Lichess database [30], culminating in a total of 17.5 million game replay records for online game players.

**Pro-player dataset** In the Lichess dataset, the majority of player Elo-ratings range between 1000 and 2000. To diversify our game dataset with more skilled matches, we also incorporated an additional 440,000 game records from 245 professional chess players. These professionals typically hold notably higher Elo ratings within the range of 2000 to 2800.

**CCRL** Chess engines like StockFish and LeelaZero have attained a proficiency level far beyond what any human player can currently reach. Considering this, we additionally incorporate the _Computer Chess Rating Lists_ (CCRL) [10], which is a dataset of chess games played by computer chess engines. The CCRL dataset comprises a considerable collection of chess games, specifically 3 million, all of which are played by computer chess engines and stored in PGN format. The Elo-ratings of chess engines fall in the range of 2800-3700.

**Chess puzzles** A chess puzzle represents a particular chessboard configuration, designed to present a distinct challenge or objective for the solver. Chess puzzles often require players to find the best move or sequence of moves to achieve a specific goal, such as checkmating the opponent's king, or finding a tactical combination. In our game dataset, we integrate 3.2M puzzles sourced from the Lichess puzzle dataset. Each puzzle within this collection is annotated with its rating, theme and solution.

**Chess modeling dataset** We observe that most chess rule descriptions are conveyed in natural language, posing a challenge for machine learning models since they statistically require a large volume of model data to accurately comprehend the chess rules [45]. To address this issue, we build a synthetic chess modeling dataset leveraging the python-ches library [41]. We collect chess game data from a one-month dump of the Lichess dataset, deliberately distinct from the month used in our own Lichess dataset. we design several model-based tasks including converting PGN to FEN, transferring UCI to FEN, and predicting legal moves, etc, resulting in 1.9M data samples.

### Language dataset

**Existing dataset** Numerous existing datasets comprise general internet crawl data from platforms like CommonCrawl or Wikipedia. We establish a filtering pipeline to extract only chess-related language corpus from pre-existing language corpus, including C4 [43], Pile [19], Oscar [37], Wikipedia [18] and RedPajama [53]. These datasets extend the scope of our language data beyond mere game-play.

**Chess blogs** Numerous chess websites often publish insightful blogs, sharing their analyses and perspectives on various aspects of chess gameplay. Such blog data is incredibly valuable, as it encompasses game-specific analysis, forming a vital link between the concrete chess game data and its interpretation in natural language form. We manually select approximately 30 chess-related websites and scrape 73.2k blog articles.

**Chess books** Similar to chess blogs, chess books can provide long and detailed analysis of the game. We extract approximately 8k chess-related books from online library to enrich our language dataset.

**Chess forums** Chess forum serves as a platform for a large amount of chess-related dialogues and conversations involving a diverse range of users. These platforms encompass high-quality question-and-answer pairs, as seen in platforms like StackExchange, or more generalized discussions on

Figure 2: Replay example in Portable Game Notation (PGN) format.

various chess-related topics, commonly found in dedicated chess-specific forums. We mainly scrape chess forum data from 5 chess-specific forum platforms and StackExchange, using requests and playwright. This process results in a collection of 140K posts, representing a wealth of diverse views, queries, and discourses related to the world of chess.

### Mixed game-language dataset

**Annotated chess game** An annotated chess game is a chess game accompanied by written commentary and analysis. In an annotated game, each move made by the players is explained and evaluated, providing insights into the thought process, strategic considerations, and tactical ideas behind the moves. Here is an example of an annotated PGN with Sicilian Defense opening:

_1.e4 c5 [The game starts with the Sicilian Defense, one of the most popular and aggressive responses to 1.e4. Black aims to control the center and create imbalances early on.]_

These annotated games inherently maintain the correspondence between board state and human language, serving as an exceptionally high-quality data source to align a model with complex human intentions and judgements. We amassuated games from seven sources, five of which are collected from the internet while the rest two are commercial datasets. In total, we collect 245K annotated games with 1.3M board-language pairs.

**Youtube transcripts** Drawing inspiration similarly from MineDoJo [17], a YouTube video can naturally serve as a mixed game-language dataset by aligning video clips with natural language transcripts based on timestamps. Rather than generating image-language pairs directly, we develop a pipeline that accurately applies OCR (Optical Character Recognition) to chessboard screenshots to generate FEN (Forsyth-Edwards Notation), a system that describes the chess state in a language format. We gathered around 83k chess videos, resulting in million-scale English transcripts and board-language pairs, thus establishing a substantial mixed game-language dataset.

### Instruction-tuning & conversation dataset

Supervised fine-tuning is a crucial component to train large language model (LLM) to follow instructions [38; 34; 61]. In addition to the comprehensive chess materials mentioned before, we also collect instruction-tuning and conversation datasets which can be used to finetune the pre-trained LLM base model, thereby enhancing its instruction-following and dialogue capability.

**Instruction-tuning data from GPT-4** Inspired by Alpaca [52], we use the self-instruct technique [59] to generate high-quality, instruction-following data through GPT-4 [8]. Specifically, we manually construct 200 seed prompts for chess-related questions or instructions. These prompts serve as few-shot examples, guiding GPT-4 towards more coherent and relevant generation. Finally, we generate around 4k instruction-response pairs using this pipeline.

**Conversation data from Reddit** The instruction data collected from GPT-4 are mainly in a single-step form, which means only one round of question-answer pair is included. To mitigate this issue, we collect multi-step conversation data about chess on Reddit. Reddit allows users to interact by commenting on posts and responding to other comments, creating a nested structure of responses. This nested structure can be easily converted to a conversation tree by treating the comment's reply as a child node for that reply. A rich source of conversation data can then be acquired by navigating from the root node to each leaf node via every available path. In all, we choose 6 chess-related sub-reddits and collect 410k human conversations about chess.

## 4 Large-scale pretraining

We will showcase two models - **ChessCLIP** and **ChessGPT** trained on the large-scale dataset.

### ChessCLIP

CLIP (Contrastive Language-Image Pre-Training) [42] is a neural network trained on a variety of modalities (e.g. image, text). By conducting contrastive learning on a large amount of paired data, CLIP bridges the image and language modality, enabling the model to understand vision by language information and vice versa. Our mixed game-language dataset in Section 3.3 has a similar paired structure because the annotation is naturally paired with its preceding game trajectories. Based on this subset, we can train a **ChessCLIP** to bridge the modality of policy and language. Specifically, by denoting the chessboard state \(S\) at timestep \(t\) as \(S_{t}\), and the annotation language as \(L_{t}\), the data pair at timestep \(T\) can be represented by \(((\{S_{t}\}_{t=T-k}^{t=T},a_{T}),L_{T})\) where \(\{S_{t}\}_{t=T-k}^{t=T}\) is a stacked \(k\) history states and \(a_{T}\) is the last move.

We want to emphasize more on what ChessCLIP can do by aligning the policy modality and the language modality. Firstly, ChessCLIP offers a similarity metric given one PGN and a text description. Just like the application of large-scale image/text retrieval using CLIP, ChessCLIP can help users conduct PGN/text retrieval - search for game based on text or search for comments based on specific game. In addition, because of the low-dimensional feature of action space compared to vision or language space (there only exists a few legal moves for a given chess state ), we can directly conduct search algorithms to maximize the similarity to generate action based on one text description using ChessCLIP. For example, given a chessboard state and a text description, ChessCLIP can generate a move by iterating through all legal moves and finding one move that returns the largest similarity. By the same logic, ChessCLIP can directly generate move sequences (multiple actions) using greedy search or beam search. We refer the reader to Appendix F.1.1 for more discussions.

**Implementation details** We preprocess the annotated PGNs to produce board/text pairs which we feed separately to the board and text encoders. In particular, for every move in the PGN, we extract the comments attached to the move as well as the board state. We encode the board positions and moves using the same scheme as those used by Leela Chess Zero (lc0) [26], which is similar to the encoding used by AlphaZero [47] for encoding positions and moves in chess. Concretely, the board positions are encoded as a \(\mathcal{R}^{8\times 8\times 112}\) feature map and the actions are encoded as a \(\mathcal{R}^{1858}\) vector. We instantiate a ChessCLIP model with a pair of text encoder and a board/action encoder. For the text encoder, we only fine-tune the last two layers of pretrained text encoder from OpenAI CLIP model. For the board/action encoder, we use a ResNet [20] architecture that conditions on the action encoding via a modified FiLM layer [40]. Please refer to Appendix F.1.1 for implementation details.

### ChessGPT

The Generative Pretraining Transformer (GPT-3) [7] is an autoregressive language model that uses deep learning techniques to generate human-like text. GPT-3 is trained by casual language modeling, which aims to predict the next word in a sentence given all the previous words. Following the same logic, we train a GPT-like model using all chess materials introduced in Section 3. Unlike other policy behavior data in robots [27] or video games [35], the chess state and move data can be represented in merely textual format. Thanks to this feature, we can directly treat chess as a text game and the imitation learning objective for policy learning can be directly covered by casual language modeling over the game dataset provided in Section 3.1.

**Implementation details** We follow common implementations of training a domain-specific instruction-following LLM. Firstly we conduct base-model fine-tuning using chess corpus introduced in section 3.1, 3.2 and 3.3. Due to computational constraints, we choose to finetune the RedPajama-3B-base [53] model, which is an open-source replication of LLaMA [55]. The base model adopts the GPT-NeoX [6] architecture, a GPT-3 [7] variant with a few modifications such as rotary positional embedding, parallel attention computation, and different initialization. The base-finetuning brings us our base model: **ChessGPT-Base**. After base-finetuning, we conduct supervised fine-tuning by supervised learning on question/conversation response using data introduced in section 3.4 and general conversation data from OASST1 [24], Dolly2 [14], Alpaca-GPT4 [39], and Sharegpt [46], forming our chat model: **ChessGPT-Chat**. We leave further RLHF (Reinforcement Learning from Human Feedback) training for future work. Refer to Appendix F.1.2 for more details.

## 5 Evaluation and benchmark

In this section, we present a comparative analysis between ChessGPT trained on our database with other baseline LLMs. The purpose of our experiments is to assess the performance of ChessGPT in three primary dimensions: Chess modeling ability, Value judgement ability, and Policy ability. The Chess Modeling capability focuses on the language model's proficiency in accurately tracking the game state and predicting valid moves. Regarding the Value judgement ability, we assess the model's precision in evaluating the worth of a chess game, encompassing the identification of advantageous positions and the calculation of situation scores. Lastly, the Policy capability gauges the model's aptitude for generating optimal moves based on a given position. By thoroughly examining these sub-categories, we can comprehensively evaluate and contrast the efficacy of different models in chess-related tasks. We choose the following models as baselines: LLaMA-7B [55], RedPajama-Base-3B [53], and compare them with ChessCLIP, ChessGPT-Base-3B3, and ChessGPT-Chat-3B. To help readers who are not familiar with chess, we provide task examples and illustrative figures to elucidate these evaluation tasks in Appendix G.1.

Footnote 3: The model size is 2.8B and 3B is a rounded representation

### Chess modeling ability

**Chess state tracking** We utilized Big-bench's State Tracking in Chess task [49; 54] to evaluate language models' ability to track the state of chess games encoded in UCI notation. The task involves predicting the legal ending square given the game prefix and starting square of the current move. For example, if the input UCI notation is "\(f2f4\)\(d7d5\)\(g1\)", the expected output would be \(["h3","f3"]\), as the chess piece on square \(g1\) can only move to those two positions. The task dataset includes real and synthetic games, divided into short, medium, and long categories based on move count. The evaluation measures correctness across all games using a specified output regex. Notably, the ChessCLIP is unsuitable for modeling tasks, so we do not include it in the comparison.

Table 1 presents a performance analysis of all models on the task. Our Base and Chat models consistently outperformed baselines in all tasks. This indicates their strong ability to track the state of chess games. However, the ChessGPT-Chat model exhibited slightly lower performance, suggesting a potential trade-off between language capabilities and state tracking. Nevertheless, the results underscore the effectiveness of our dataset-trained LLM models for chess state tracking.

**Board state tracking** We performed additional evaluations involving UCI to FEN and PGN to FEN conversions. In the UCI to FEN experiment, the target was replaced with FEN format, while in the PGN to FEN experiment, UCI was converted to PGN format as input and the target was replaced with FEN format. The similarity was measured using Levenshtein distance, which was normalized to a range of 0 to 1 [63]. These evaluations focused on assessing the model's capability to track the overall state of the chessboard by representing the state of each chess piece using FEN notation.

Table 2 illustrates the results of these evaluations. It is evident that compared to tracking the state of an individual chess piece, tracking the entire chessboard state becomes more challenging. The similarity scores between the two baselines were consistently below \(10\%\), indicating a lack of global chess piece state tracking ability. However, the ChessGPT achieves an average similarity score

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & & \multicolumn{4}{c}{LLM Models (\%)} \\ \cline{2-5} Tasks & LLAMA-7B & RedPajama-Base & ChessGPT-Base & ChessGPT-Chat \\ \hline Real Short & 29.5 \(\pm\) 1.4 & 23.2 \(\pm\) 1.3 & **99.5 \(\pm\) 0.2** & **98.5 \(\pm\) 0.4** \\ Real Med & 39.3 \(\pm\) 1.5 & 38.2 \(\pm\) 1.5 & **97.7 \(\pm\) 0.5** & **97.8 \(\pm\) 0.4** \\ Real Long & 53.0 \(\pm\) 1.6 & 51.9 \(\pm\) 1.6 & **98.1 \(\pm\) 0.4** & **97.6 \(\pm\) 0.4** \\ Syn Short & 31.3 \(\pm\) 1.4 & 24.9 \(\pm\) 1.3 & **94.2 \(\pm\) 0.7** & **92.3 \(\pm\) 0.8** \\ Syn Med & 39.9 \(\pm\) 1.6 & 37.7 \(\pm\) 1.5 & **94.6 \(\pm\) 0.7** & **88.9 \(\pm\) 1.0** \\ Syn Long & 45.8 \(\pm\) 1.5 & 42.2 \(\pm\) 1.5 & **92.8 \(\pm\) 0.8** & **85.1 \(\pm\) 1.1** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Bigbench State Tracking in Chess

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & & \multicolumn{4}{c}{LLM Models (\%)} \\ \cline{3-6}  & Tasks & LLAMA & RedPajama-Base & ChessGPT-Base & ChessGPT-Chat \\ \hline \multirow{3}{*}{UCI to FEN} & Real Short & 2.2 \(\pm\) 0.0 & 5.2 \(\pm\) 0.0 & **95.1 \(\pm\) 0.3** & **95.7 \(\pm\) 0.1** \\  & Real Med & 2.3 \(\pm\) 0.1 & 4.0 \(\pm\) 0.1 & **89.9 \(\pm\) 0.2** & **88.6 \(\pm\) 0.3** \\  & Real Long & 1.8 \(\pm\) 0.0 & 3.8 \(\pm\) 0.1 & **85.7 \(\pm\) 0.2** & **81.4 \(\pm\) 0.5** \\ \hline \multirow{3}{*}{PGN to FEN} & Real Short & 6.0 \(\pm\) 0.1 & 2.8 \(\pm\) 0.1 & **96.7 \(\pm\) 0.1** & **95.8 \(\pm\) 0.1** \\  & Real Med & 5.6 \(\pm\) 0.1 & 3.4 \(\pm\) 0.1 & **94.8 \(\pm\) 0.1** & **93.6 \(\pm\) 0.1** \\ \cline{1-1}  & Real Long & 5.7 \(\pm\) 0.0 & 8.9 \(\pm\) 0.1 & **95.4 \(\pm\) 0.2** & **82.7 \(\pm\) 1.0** \\ \hline \hline \end{tabular}
\end{table}
Table 2: UCI to FEN test higher than \(90\%\). These results demonstrate that our dataset-trained model excels in capturing and reproducing the global chess piece state in both UCI to FEN and PGN to FEN conversions.

### Value judgement ability

In this part, we evaluate the model's ability of value judgement. Specifically, we want to assess the model from two perspectives: (1) its ability to align with the true value function given a chessboard state (the true value are evaluated by chess engines in enough search depths) in the evaluation of **State value multi-choice**, and (2) its ability to align with human judgement and human knowledge in the evaluation of **Chess Annotation Multi-choice** and **Opening multi-choice**.

**State value multi-choice** Here we evaluate the model's ability to see whether it can determine which side holds the advantage for a given PGN. We construct an evaluation dataset consisting of \(3000\) game snippets and utilize Stockfish 15 with a depth of 18 to calculate the winning rate for the white pieces. By categorizing the winning rate into three intervals: \(0-33\%\) for black advantage, \(34-66\%\) representing a balanced state, and \(67-100\%\) for white advantage, we construct the state-value multiple-choice task. During experiments, we discovered that an additional '{' suffix to the prompt can significantly enhance the performance of the base model. This is due to '{' consistently serving as the initial symbol for annotation in annotated PGNs. Consequently, we carried out our evaluation under two distinct prompt settings and report our results w.r.t multi-choice grade shown in Table 3.

**Chess annotation multi-choice** The annotations within an annotated PGN can be viewed as a reflection of human evaluative judgement. To examine the degree to which the model's value aligns with human value, we extract 3k game-language pairs from the annotation dataset as the test set. By randomly selecting three annotations from the test set as incorrect options, we construct the chess annotation four-choice task. We report the multi-choice grade results over two prompts in Table 4.

**Opening multi-choice** A chess opening refers to the initial moves made by players at the beginning of a chess game. There are numerous chess openings, each with its own name, characteristics, and strategic goals. For example, the Sicilian defense: _1. e4 c5_ is one of the most popular and aggressive chess openings for Black. We use the Lichess opening dataset [29] including 3.5k opening PGNs and their corresponding names, to formulate two tasks: (1) PGN2Opening five-choice task, which aims at choosing the correct opening name for a given PGN, and reversely, (2) Opening2PGN five-choice task, aiming at choosing the correct PGN for a given opening name. We report the result in Table 5.

In general, our trio of models surpasses the performance of two baseline language models across these four tasks in all settings. This result confirms that our models are more effectively aligned with both the true value function and human judgement/knowledge. Both ChessGPT-Base and ChessGPT-chat deliver outstanding performance in the state-value task and the opening task. Notably, ChessCLIP displays a surprisingly high level of proficiency in the annotation task and the opening task. This result reveals the model's capacity to extract human judgement and knowledge solely from annotations, even without training in any actual chess games.

### Policy evaluation

**Checkmate in one** We incorporate the checkmate-in-one task from Big-Bench [49] into our evaluation methods. This task is designed to challenge the model's ability to identify a move in a given PGN

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{4}{c}{Models (\%)} \\ \cline{2-6} Prompt Setting & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\ \hline W/O \{ suffix & 29.8 \(\pm\) 0.8 & 27.4 \(\pm\) 0.7 & **33.2 \(\pm\) 0.9** & **35.7 \(\pm\) 0.9** & N/A \\ With \{ suffix & 29.6 \(\pm\) 0.8 & 28.4 \(\pm\) 0.8 & 38.8 \(\pm\) 0.9 & 34.7 \(\pm\) 0.9 & **63.6 \(\pm\) 0.9** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Chess Annotation Multi-choice

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{4}{c}{Models (\%)} \\ \cline{2-6} Prompt Setting & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\ \hline W/O \{ suffix & 33.2 \(\pm\) 0.7 & 31.1 \(\pm\) 0.7 & **43.1 \(\pm\) 0.8** & **52.8 \(\pm\) 0.8** & N/A \\ With \{ suffix & 26.9 \(\pm\) 0.7 & 29.7 \(\pm\) 0.8 & **53.7 \(\pm\) 0.8** & **53.5 \(\pm\) 0.8** & **38.1 \(\pm\) 0.8** \\ \hline \hline \end{tabular}
\end{table}
Table 3: State value multi-choicethat would result in a checkmate. By doing so, it measures the model's capacity to comprehend and apply the rules of chess. The model is essentially required to discern a move that not only places the opponent's king under attack but also ensures that the king cannot evade capture in the next move.

We also find adding an additional instruction suffix like _[Now white/black can checkmate in one]_ can largely enhance the base model performance. We report the result in two prompts with two metrics (exact-string match as ESM and multi-choice-grade as MC) in Table 6. our ChessGPT-Base model and ChessGPT-Chat model show a really great checkmate ability by surpassing two LLM baselines by a large margin. ChessCLIP does not perform well in this task, because there does not exist much annotation data regarding checkmate-in-one behavior in the annotation dataset.

**General policy** In order to assess the model's generalization ability, we introduced Elo Rating as a factor in the task, aiming to evaluate its capacity to identify PGN and related keywords and generate the appropriate next move within the specified skill level. Model's selection of the next legal move is assigned a move score, which is normalized based on the win rate observed in the raw data. Table 7 presents the results representing the performance of different models in selecting the most suitable move for white chess. Notably, all models surpassed the performance of the random policy (\(\approx 50\%\)) as the Elo Ratings correspond to relatively high skill levels among human players.

Further analyzing the performance of different models across varying Elo Ratings is crucial for understanding the observed results. The minor variations in move scores for different Elo Rating scenarios in Table 8 indicate that ChessGPT-Base may struggle to effectively incorporate Elo Rating information into its decision-making process. This could be due to the model's limited understanding of the nuanced characteristics associated with distinct Elo Ratings. The complexity of the task and the challenges in accurately accounting for diverse playing styles further contribute to the limited variations in move scores across different Elo Ratings. Consequently, neglecting this information can lead to the model learning an average policy for each Elo Rating, resulting in subpar overall performance. Similar findings were observed in the black chess test, and to further validate this viewpoint, we conducted input attention visualization. Refer to Appendix F.1.2 for more details.

To clarify, the dataset we have presented encompasses a wide range of games and varying Elo ratings, as shown in Figure 2, which possesses the potential to effectively capture and generalize intricate patterns and policies associated with different Elo levels. However, the current training method might not sufficiently emphasize these nuanced features. This highlights a potential direction for future research, which involves enhancing the model's ability to better integrate and utilize metadata such as Elo Rating and other auxiliary data. By addressing these aspects, the model's overall generalization can be further improved.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Setting} & \multicolumn{4}{c}{Models (\%)} \\ \cline{2-5}  & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\ \hline With suffix (ESM) & 1.6 \(\pm\) 0.2 & 0.0 \(\pm\) 0.0 & **71.4 \(\pm\) 0.7** & 56.8 \(\pm\) 0.8 & N/A \\ With suffix (MC) & 2.6 \(\pm\) 0.3 & 0.0 \(\pm\) 0.0 & **66.1 \(\pm\) 0.8** & 11.3 \(\pm\) 0.5 & 2.9 \(\pm\) 0.3 \\ W/O suffix (ESM) & 1.7 \(\pm\) 0.2 & 0.0 \(\pm\) 0.0 & 26.5 \(\pm\) 0.8 & **59.4 \(\pm\) 0.8** & N/A \\ W/O suffix (MC) & 2.2 \(\pm\) 0.3 & 0.0 \(\pm\) 0.0 & 13.6 \(\pm\) 0.6 & **15.4 \(\pm\) 0.6** & N/A \\ \hline \hline \end{tabular}
\end{table}
Table 6: Checkmate in One

\begin{table}
\begin{tabular}{c c} \hline \hline Elo Rating & Move Score \\ \hline
700-1000 & 59.4 \(\pm\) 1.0 \\
1200-1500 & 58.9 \(\pm\) 0.9 \\
1700-2000 & 59.6 \(\pm\) 1.0 \\
2700-3000 & 59.8 \(\pm\) 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table 8: ChessGPT-Base in Different Elo Rating Results

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Prompt Setting} & \multicolumn{4}{c}{Models (\%)} \\ \cline{2-5}  & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat & ChessCLIP \\ \hline Opening2PGN & 43.0 \(\pm\) 0.9 & 26.5 \(\pm\) 0.8 & **92.2 \(\pm\) 0.5** & **94.7 \(\pm\) 0.4** & 73.0 \(\pm\) 0.8 \\ PGN2Opening & 20.0 \(\pm\) 0.7 & 20.7 \(\pm\) 0.7 & 49.3 \(\pm\) 0.9 & 55.8 \(\pm\) 0.9 & **80.5 \(\pm\) 0.7** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Opening2PGN and PGN2Opening

### Qualitative results

We also perform qualitative comparison between our models (ChessGPT-Chat and ChessGPT-Base) and the baselines. We ask the language models a series of questions ranging from factual knowledge of chess as well as requesting the models to perform some operational tasks related to chess. We found that ChessGPT-base performed similarly to RedPajama: both models can sometimes produce factual answers for some of the questions but they failed to generate coherent answers when asked to perform tasks such as providing commentary on chess moves or converting the PGN notation to FEN. ChessGPT-Chat gives more factual answers and demonstrates better performance when prompted to generate analysis and perform other chess-related tasks. Refer to Appendix H for qualitative analysis.

## 6 Conclusion

In this paper, we introduce a new large-scale dataset and benchmark on chess to encourage of study of the interplay between historical policy data and natural language knowledge. We accompany our dataset with an evaluation framework for assessing language models' capability in chess. We showcase two models, **ChessCLIP** and **ChessGPT**, that demonstrate promising results for learning the interplay between language and action. Nevertheless, our results indicate that we are only beginning to understand how to bridge the gap between policy learning and language modeling and we discuss more on the future directions of our dataset in Appendix J. We hope that our dataset and benchmark can make future research on policy and language alignment more accessible.

## References

* [1] Playwright. https://github.com/microsoft/playwright.
* [2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.
* [3] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo. https://github.com/nomic-ai/gpt4all, 2023.
* [4] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. _Advances in Neural Information Processing Systems_, 35:24639-24654, 2022.
* [5] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, 2023.
* [6] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. _arXiv preprint arXiv:2204.06745_, 2022.
* [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [8] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [9] Murray Campbell, A. Joseph Hoane, and Feng-hsiung Hsu. Deep Blue. _Artificial Intelligence_, 134(1):57-83, January 2002.
* [10] CCRL. https://www.computerches.org.uk/ccrl/.
* [11] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, March 2023.
* [13] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* [14] Dahoas. Dolly2. https://huggingface.co/datasets/databricks/databricks-dolly-15k.
* [15] Yuqing Du, Olivia Watkins, Zihan Wang, Cedric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language models. _arXiv preprint arXiv:2302.06692_, 2023.
* [16] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GIm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, 2022.

* [17] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. _arXiv preprint arXiv:2206.08853_, 2022.
* [18] Wikimedia Foundation. Wikimedia Downloads. https://dumps.wikimedia.org.
* [19] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, Las Vegas, NV, USA, June 2016. IEEE.
* [21] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.
* [22] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286, 2021.
* [23] Dai Juntao, Ji Jiaming, Pan Xuehai, Sun Ruiyang, Wang Yizhou, and Yang Yaodong. Constrained Value-Aligned LLM via Safe RLHF, May 2023.
* [24] Andreas Kupf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. OpenAssistant Conversations-Democratizing Large Language Model Alignment. _arXiv preprint arXiv:2304.07327_, 2023.
* [25] Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, Jorg Frohberg, Mario Sako, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Luchona Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaien Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Romero Munoz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite. The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [26] Leela Chess Zero. https://lczero.org/.
* [27] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. _The Journal of Machine Learning Research_, 17(1):1334-1373, 2016.
* [28] Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In _CVPR_, 2022.
* [29] Lichess chess opening names. https://github.com/licenses-org/chess-openings.
* [30] Lichess Developers. Lichess. https://lichess.org/.
* [31] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. _arXiv preprint arXiv:2308.03688_, 2023.
* [32] Thomas McGrath, Andrei Kapishnikov, Nenad Tomasev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik. Acquisition of chess knowledge in AlphaZero. _Proceedings of the National Academy of Sciences_, 119(47):e2206625119, November 2022.

* [33] Reid McIlroy-Young, Siddhartha Sen, Jon Kleinberg, and Ashton Anderson. Aligning Superhuman AI with Human Behavior: Chess as a Model System. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, KDD '20, pages 1677-1687, New York, NY, USA, August 2020. Association for Computing Machinery.
* [34] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. _arXiv preprint arXiv:2104.08773_, 2021.
* [35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* [36] David Noever, Matt Ciolino, and Josh Kalin. The chess transformer: Mastering play using generative language models. _arXiv preprint arXiv:2008.04057_, 2020.
* 16, Mannheim, 2019. Leibniz-Institut f"ur Deutsche Sprache.
* [38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [39] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction Tuning with GPT-4. _arXiv preprint arXiv:2304.03277_, 2023.
* [40] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual reasoning with a general conditioning layer. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, AAAI'18/IAAI'18/EAAI'18, pages 3942-3951, New Orleans, Louisiana, USA, February 2018. AAAI Press.
* [41] python-chess: a chess library for python. https://github.com/niklasf/python-chess.
* [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, February 2021.
* [43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019.
* [44] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. _arXiv preprint arXiv:2205.06175_, 2022.
* [45] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering Atari, Go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* [46] ShareGPT. https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered.

* [47] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. _Science (New York, N.Y.)_, 362(6419):1140-1144, 2018.
* [48] Alexei Barantsev Simon Stewart. Selenium, March 2023.
* [49] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* [50] Andreas Stockl. Watching a language model learning chess. In _Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)_, pages 1369-1379, 2021.
* [51] Tianxiang Sun and Xipeng Qiu. Moss, March 2023.
* [52] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* [53] Together Computer. RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset. https://github.com/togethercomputer/RedPajama-Data, april 2023.
* [54] Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel. Chess as a Testbed for Language Model State Tracking. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022.
* [55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [56] Alan Turing. Digital computers applied to games. _Faster than thought_, 1953.
* [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [58] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv: Arxiv-2305.16291_, 2023.
* [59] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Model with Self Generated Instructions, 2022.
* [60] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. _arXiv preprint arXiv:2302.01560_, 2023.
* [61] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [62] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. _arXiv preprint arXiv:2306.02224_, 2023.
* [63] Li Yujian and Liu Bo. A normalized levenshtein distance metric. _IEEE transactions on pattern analysis and machine intelligence_, 29(6):1091-1095, 2007.
* [64] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. ChatGLM-6B, March 2023.

## Appendix

* Acknowledgement
* Different Chess Formats
* B.1 Universal Chess Interface (UCI)
* B.2 Standard Algebraic Notation (SAN)
* B.3 Portable Game Notation (PGN)
* B.4 Forsyth-Edwards Notation (FEN)
* Image sources
* Dataset analysis
* D.1 Dataset statistics and metrics
* D.2 Dataset examples
* Data Collection and Preprocessing
* E.1 Online Chess websites
* E.2 Online Chess forums
* E.3 Annotated PGN
* E.4 Existing Datasets
* E.5 YouTube transcripts dataset
* E.6 Lichess dataset
* E.7 Pro-player dataset
* E.8 Chess books
* E.9 CCRL
* E.10 Chess puzzles
* E.11 Chess modeling data
* E.12 Preprocessing
* E.13 Licenses and dataset cards
* Implementation and Evaluation Details
* F.1 Implmenation details
* Evaluation details
* G.1 Evaluation task examples and plots
* Qualitative Analysis
* H.1 Experimental set-up
* H.2 General knowledge
* H.3 Chess modeling
* ChessCLIP visualization
Potential directions
* K Limitations and Potential Societal ImpactAcknowledgement

We thank Jiacheng Liu for his work on collecting chess-related data and chess book list.

## Appendix B Different Chess Formats

### Universal Chess Interface (UCI)

The UCI format is widely used for communication between chess engines and user interfaces. It represents chess moves by combining the starting and ending squares of a piece, such as "e2e4" to indicate moving the pawn from e2 to e4. For example, the UCI notation for a full game would be:

``` e2e4c7c6g1f3d7d5e4d5c6d5d2d4b8c6c2c4g8f6b1c3e8e6c4c5g7g6c1f4f8g7 fie2f6e4e1g1e6g4f3e5g4e2d1e2c6e5c3e4e5c6e4d6e8f8f1e1g7d4f4h6d4g7 h6g7f8g7d6b5a7f46b5c3d54d3e4d8d5a2a3a8d8b2b4h7h6e2 ```

### Standard Algebraic Notation (SAN)

SAN (Standard Algebraic Notation) is a widely used notation system in the game of chess for recording and describing moves. It provides a standardized and concise representation of moves that is easily understood by chess players and enthusiasts. In SAN, each move is represented by two components: the piece abbreviation and the destination square. The piece abbreviation is a letter that represents the type of piece making the move, such as "K" for king, "Q" for queen, "R" for rook, "B" for bishop, "N" for knight, and no abbreviation for pawns. The destination square is denoted by a combination of a letter (a-h) representing the column and a number (1-8) representing the row on the chessboard. Additional symbols may be used to indicate specific move types. The symbol "+" is used to indicate a check, while "#" denotes a checkmate. Castling moves are represented by "O-O" for kingside castling and "O-O-O" for queenside castling.

### Portable Game Notation (PGN)

PGN is a widely adopted format for recording chess games. It includes not only the SAN moves but also additional information like player names, event details, and game results. PGN files are human-readable and can be easily shared and analyzed. Here is an example of a PGN representation of a full game:

``` [Event "World Chess Championship"] [Site "London, England"] [Date "2023.05.20"] [Round "1"] [White "Carlsen, Magnus"] [Black "Nepomniachtchi, Ian"] [Result "1/2-1/2"] ```

1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6
8. c3 O-O 9. h3 Nb8 10. d4 Nbd7 11. Nbd2b 8b7 12. Bec2 Re8 13. Nf1 Br8
14. Ng3 g6 15. a4 c5 16. d5 c4 17. Bg5 h6 18. Be3 Nc5 19. Qd2 h5 20. Bg5 Bg7
21. Nh2 Qc7 22. Rif1 Nh7 23. Bh6 Bh8 24. f4 exf4 25. Bxf4 Nf6 26. Rae1 bxa4
27. Nf3 Nf47 28. Bh6 Ne5 29. Nxe5 Bxe5 30. Rf3 Qb6 31. Kh1 (xb2 32. Ref1 Re7
33. Bg5 Rd7 34. Bf6 Bxf6 35. Rxf6 e3 36. Ixh5 a2 37. Qh6 gxh5 38. Ref3 h4
39. Rf4 15 40. Rxf5 Rg7 41. Rh5 1-0 ```

### Forsyth-Edwards Notation (FEN)

FEN is a notation system used to describe the state of a chess game. It represents the positions of pieces on the chessboard, active color, castling rights, en passant targets, and the half-move and full-move counters. Here is an example of a FEN notation representing the starting position:

``` rnbqkbnr/pppppppp/8/8/8/8/PPPPPPP/RNBQKBNRwKQkq-01 ```

In this FEN notation, each letter represents a piece on the board, with uppercase letters representing white pieces and lowercase letters representing black pieces. The forward slash ("/") separates ranks, and the number after each rank indicates the number of empty squares. The active color is represented by "w" for white or "b" for black. The casting rights are denoted by "K" (white kinside), "Q" (white queenside), "k" (black kinside), and "q" (black quesenide). The en passant target square is indicated with the corresponding square, or "-" if there is no en passant target. The half-move and full-move counters specify the number of half-moves since the last pawn move or capture and the current move number, respectively.

These different chess formats serve various purposes, from representing individual moves (UCI) to recording entire games (PGN) and describing specific positions (FEN). Understanding and working with these formats is essential for tasks like parsing, analyzing, and exchanging chess game data in different contexts.

## Appendix C Image sources

Here we provide the sources for images used in our Figure 1.

* Online game: https://lichess.org/tv
* Pro game: https://www.insidethegames.biz/articles/1110959/magnus-carlsen-ches-world-cup-duda
* Computer game: https://www.chess.com/news/view/updated-alphazero-crushes-stockfish-in-new-1-000-game-match
* Modeling: https://www.chess.com/terms/fen-ches
* Puzzle: https://chespuzzle.net/
* General text: https://www.bbc.co.uk/newsround/66233770
* Wiki: https://en.wikipedia.org/wiki/Chess
* Forum: https://www.chess.com/forum
* Blog: https://www.chess.com/blogs
* Book: https://www.amazon.co.uk/Chess-Strategy-Action-John-Watson/dp/1901983692
* Annotated game: https://www.chess.com/forum/view/game-analysis/kingside-pawn-rush
* Youtube video: https://www.youtube.com/watch?v=NAIQyoPcjNM
* Reddit: https://www.reddit.com/r/chess/comments/15s65hb/white_to_play_and_reach_2000_chesscom/jwdqpze/?context=3
* Value judgement: https://www.thenewatlantis.com/wp-content/uploads/legacy/20190820_TNA58Wilkenfeldbanner.jpg

## Appendix D Dataset analysis

### Dataset statistics and metrics

InTable 9, we present the dataset statistics breakdown for each data subset, including its raw size, document count, and subset type.

Table 10 shows the properties of the chess-specific language dataset that we use for training ChessGPT. For these datasets, we computed the average number of words (num. words) per example, character repetition ratio (char. rep. ratio), word repetition ratio (word. rep. ratio.), special character ratio (special char. ratio), stopwords ratio and perplexity of the first shard for each subset in the language dataset. These metrics are based on some of the criteria employed by Bloom [25] in their pre-processing pipelines.

### Dataset examples

In this subsection, we offer dataset examples for each subset to help the readers to understand its contents more intuitively and clearly.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline dataset & num. words & char. rep. & word rep. & special char & stopwords & perplexity \\  & & ratio & ratio & ratio & ratio & \\ \hline Chess puzzles & 49.6618 & 0.0104 & 0.0000 & 0.3246 & 0.4771 & 988.3734 \\ Oscar-Chess & 1441.4341 & 0.0596 & 0.0499 & 0.2242 & 0.4119 & 665.6499 \\ Pile-Chess & 2105.2454 & 0.0626 & 0.0205 & 0.2409 & 0.4227 & 497.3883 \\ RedPajama-Chess & 1581.5825 & 0.0532 & 0.0163 & 0.2273 & 0.4218 & 410.1236 \\ StackExchange-Chess & 578.3733 & 0.0591 & 0.0816 & 0.2617 & 0.4835 & 520.1257 \\ Wikipedia-Chess & 463.4980 & 0.0876 & 0.0052 & 0.2604 & 0.3079 & 236.9671 \\ C4-Chess & 510.6041 & 0.0479 & 0.0082 & 0.2131 & 0.4418 & 548.9471 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Metrics for the language dataset.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Component & Raw size & Document count & Subset type \\ \hline Lichess & 19.9 GB & 17.5 M & Game \\ Pro-player & 0.37 GB & 0.44 M & Game \\ CCRL & 3.60 GB & 3.00 M & Game \\ Chess puzzles & 1.53 GB & 3.19 M & Game \\ Chess modeling & 0.94 GB & 1.90 M & Game \\ \hline C4-Chess & 0.59 GB & 0.18 M & Language \\ Pile-Chess & 1.10 GB & 0.10 M & Language \\ RedPajama-Chess & 5.65 GB & 0.52 M & Language \\ Oscar-Chess & 3.13 GB & 0.33 M & Language \\ WikiPedia-Chess & 40.3 MB & 11.4 K & Language \\ Chess Blogs & 0.59 GB & 73.2 K & Language \\ Chess Books & 1.86 GB & 8.36 K & Language \\ Chess Forums & 1.05 GB & 0.14 M & Language \\ \hline Annotated Chess Games & 0.66 GB & 245 K & Mixed \\ Youtube transcripts & 0.98 GB & 83.0K & Mixed \\ \hline GPT-4-Chess & 0.95 MB & 3.91 K & Conversation \\ Reddit & 0.86 GB & 0.41 M & Conversation \\ \hline Overall & 42.3 GB & 28.1 M & N/A \\ \hline \hline \end{tabular}
\end{table}
Table 9: Dataset statistics[Event "Rated Bullet tournament https://lichess.org/tournament/yc1WW20x"]
[Site "https://lichess.org/PpwPO2XMq"]
[Date "2017.04.01"]
[Round "-"]
[White "Abbot"]
[Black "Costello"]
[Result "0-1"]
[UTCDate "2017.04.01"]
[UTCTime "11:32:01"]
[WhiteElo "2100"]
[BlackElo "2000"]
[WhiteRatingDiff "-4"]
[BlackRatingDiff "+1"]
[WhiteTitle "FM"]
[ECO "B30"]
[Opening "Sicilian Defense: Old Sicilian"]
[TimeControl "300+0"]
[Termination "Time forfeit"]
1. e4 { [Yeval 0.17] } 1... c5 { [Yeval 0.19] }
2. Nf3 { [Yeval 0.25] } 2... Nc6 { [Yeval 0.33] }
3. Be4 { [Yeval -0.13] } 3... e6 { [Yeval -0.04] }
4. c3 { [Yeval -0.4] } 4... b5? { [Yeval 1.18] }
5. bb3? { [Yeval 0.21] } 5... c4 { [Yeval 0.32] }
6. bc2 { [Yeval 0.2] } 6... a5 { [Yeval 0.6] }
7. d4 { [Yeval 0.29] } 7... cxd3 { [Yeval 0.6] }
8. dxd3 { [Yeval 0.12] } 8... Nf6 { [Yeval 0.52] }
9. e5 { [Yeval 0.39] } 9... Nd5 { [Yeval 0.45] }
10. Bg5?! { [Yeval -0.44] } 10... Qc7 { [Yeval -0.12] }
11. Nbd2?? { [Yeval -3.15] } 11... h6 { [Yeval -2.99] }
12. Bh4 { [Yeval -3.0] } 12... Ba6? { [Yeval -0.12] }
13. b3?? { [Yeval -4.14] } 13... Nf4? { [Yeval -2.73] } 0-1

[Event "URS-ch34"]
[Site "Tilisi"]
[Date "1966.??.??"]
[Round "9"]
[White "Bronstein, David I"]
[Black "Suetin, Alexey S"]
[Result "1/2-1/2"]
[WhiteElo "1"]
[BlackElo "="]
[ECO "B97"]
1. e4 c5 2.Nf3 d6 3.d4 cxd4 4.Nxd4 Nf6 5.Nc3 a6
6.Bg5 e6 7.f4 Qb6 8.0d2 Qxb2
9.Rb1 Qa3 10.Bxf6 gxf6 11.Be2 Bg7 12.0-0 Nc6
13.Nx6c bxc6 14.Rb3 Qc5+ 15.Kh1 f5
16.exf5 exf5 17.Na4 Qd4 18.Qxd4 Bxd4 19.Rd1 Bf2
20.Rxd6 O-0 21.Nb6 Bxb6 22.Rxb6 Re8
23.Bf1 Be6 24.Kgl Braa2 25.Rxa6 Rxa6 26.Bxa6 Bd5
27.Kf2 Red 28.g3 BC4 29.Rxc6 Re2+
30.Kgl Braa 31.Nx6 Rxc2 32.Ra5 Kg7 33.Rxf5 Kg6
34.Rg5+ Kf6 1/2-1/2[Event "CCRL 40/15"] [Site "CCRL"] [Date "2022.01.08"] [Round "806.6.381"] [White "Stockfish 060122 64-bit"] [Black "Dragon by Komodo 2.6 64-bit"] [Result "1/2-1/2"] [ECO "D30"] [Opening "Queen's gambit declined"] [PlyCount "115"] [WhiteElo "3505"] [BlackElo "3480"]

 1. d4 {book} d5 {book} 2. c4 {book} e6 {book} 3. Nf3 {book} Nf6 {book} 4. g3 {book} a6 {book} 5. c5 {book} b6 {book} 6. cxb6 {+0.23/33 28s} c5 {-0.23/30 40s} 7. Bg2 {+0.24/30 11s} cxd4 {-0.15/29 17s} 8. Nxd4 {+0.08/32 15s} Nbd7 {-0.21/29 18s} 9. Nc3 {+0.29/30 18s} Nxb6 {-0.31/29 21s} 10. -0 {+0.27/30 12s} Bb7 {-0.21/28 20s} 11. Be3 {+0.41/31 14s} Nxd {-0.12/27 19s} 12. Beg {+0.31/30 18s} Be7 {-0.16/29 29s} 13. 24s+ {+0.24/29 18s} 047 {-0.08/30 20s} 14. b3 {+0.07/33 22s} Nd6 {+0.09/29 17s} 13. Ref1 {+0.19/31 28s} Rc8 {-0.02/31 15s} 16. e3 {+0.13/33 24s} Nde4 {-0.02/33 16s} 17. Qxd7+ {+0.27/34 21s} Kxd7 {-0.05/34 31s} 18. Nxe4 {+0.25/37 13s} Nxe4 {-0.17/32 22s} 19. Bxe7 {+0.29/32 22s} Kxe7 {-0.24/32 20s} 20. Exet {+0.19/36 36s} dxe4 {-0.12/34 37s} 21. Kr1 {+0.13/38 38s} g5 {-0.03/33 27s} 22. Ne2 {+0.00/37 63s} Kdd {-0.03/36 21s} 23. Rxc8 {-0.08/40 12s} Rxc8 {-0.08/37 25s} 24. Ke1 {+0.12/39 20s} Bd5 {-0.05/34 40s} 25. Rc1 {+0.07/41 17s} Ra8 {-0.06/32 27s} 26. Kd2 {+0.05/33 16s} a5 {+0.00/35 32s} 27. Nc3 {+0.10/35 18s} Rb8 {-0.05/37 20s} 28. Rh1 {+0.04/39 16s} Ke5 {+0.00/39 67s} 29. Rcl {+0.07/43 56s} f5 {+0.00/39 21s} 30. Na4 {+0.00/41 13s} Kdd {+0.00/40 19s} 31. Rc5 {+0.00/46 34s} Ra8 {+0.00/42 57s} 32. Rc2 {+0.00/49 20s} Rb8 {+0.00/40 13s} 33. Nb2 {+0.12/37 17s} h5 {+0.00/41 14s} 34. Nc4 {+0.11/41 29s} Bxc4 {+0.00/42 14s} 35. Rxc4 {+0.00/46 23s} h4 {+0.00/43 21s} 36. Ra4 {+0.00/48 20s} Rb5 {+0.00/43 16s} 37. Nc3 {+0.00/50 29s} Ke5 {+0.00/44 18s} 38. b4 {+0.00/51 24s} xab4+ {+0.00/41 26s} 39. Rxb4 {+0.00/53 30s} Rc5 {+0.00/43 14s} 40. Rb3 {+0.00/53 26s} Rd5 {+0.00/53 20s} Kdf6 {+0.00/45 31s} 42. Rc4 {+0.00/50 16s} 31s} 43. Nc2 {+0.00/53 34s} Kd7 {+0.00/47 23s} 44. Rb3 {+0.00/47 17s} Rd3+ {+0.00/49 22s} 45. Rc2 {+0.00/50 19s} Rd5 {+0.00/51 28s} 46. Rc3 {+0.00/52 35s} 64 {+0.00/53 22s} 47. Rc1 {+0.00/57 23s} Ra5 {+0.00/55 25s} 48. Rc4 {+0.00/58 21s} Raf6 {+0.00/57 22s} 49. Ke2 {+0.00/59 59s} RdG {+0.00/57 28s} 50. s {+0.00/58 14s} Rd5 {+0.00/53 23s} St1. Ra4 {+0.00/60 27s} Rc7 {+0.00/57 16s} 52. ag {+0.00/60 17s} Rb8 {+0.00/60 18s} 53. a7+ {+0.00/59 25s} Ra8 {+0.00/65 42s} 54. Ra6 {+0.00/59 20s} Rc5+ {+0.00/66 16s} 55. Rb2 {+0.00/65 25s} Rd5 {+0.00/66 14s} 56. Nc2 {+0.00/62 17s} Rc5+ {+0.00/66 18s} 57. Kd1 {+0.00/67 17s} Rd5+ {+0.00/63 14s} 58. Rc2 {+0.00/101 20s, Draw by 3-fold repetition} 1/2-1/2

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

``` Q:Isthereawaytousehandicapsinchestobridgethegapbetweenplayersoftdifferentskilllevels?HandicappingisroutineintheJapanesegameGo(mybestgame).Thebasicstrengthunitisonestone,andaone-stonedifferencerepresentsafulllevelofdifferenceinstrength.I(abouta1500player)onceaskaded2100playerhowmuchofah handicapshewouldneedtogivemesothatwewouldhaveanequalchancetowin."Probablyaknight,maybecmore,"sheanswered.Ioncetookaknighthandicapagainstain2200playerandlost,butitwasamultigther,closergamethanonewithho handicap.Thatmightsuggesthatapawnisequivalenttoabout200pointsofrating.Apparentlyhandicappingdoesn'tdomuchforsay,a50pointdifferenceinstrength(youjustplayandtakeyourchances).Butabovethat,theremightbewaystousehandicaps.Evengivingsomeonethefirstmovetwotimesoutofthere(aswasearlierdoneinprofessionalGo)mightdosomething.Orwouldit?Whyhasn'thandicappingbeendonemuchformallyinches,asinGo? ```

``` Automated_open [Event"AllabouttheTraxlerCounter-Attack:WhytoplayTraxlerinsteadofapassivemove"] [Site"https://lichess.org/study/WLyfoXTJ/xdA6Lwme"] [Date"????????????"] [Round"?"] [White?"] [Black"?"] [Result"] [Annotator"https://lichess.org/@/Dat_Das"] [ECO"<57"] [Opening"ItalianGame:TwoKnightsDefense,FriedLiverAttack"] [UTCDate"2017.10.13"] [UTCTime"16:33:43"] [Variant"Standard"] ```

``` {Helloeveryone,pleaseclickthelittlehearttoshowhatthisstudywashelpfultoyou,tospreadthewordandtoshowyourappreciation.}1.e4e5.2.Mf3Nc63.Bc4Mf64.Ng5{YoumaywonderwhyyoushouldplayBc5insteadofd5.Thisisjusttoshowyouexactlywhatwhistheirvingtodo.}4...d55.exd5{Itseemsyou'llwintheexchange.}5...NxdS{5...NxdS{Thisisthebestdefenseifyoudoplayd5.}6.Bb5+c67.dxc6bxc68.Qf3)6.Nxf7{Whitehasaacrificoftheirown.Thisisthefriedliverattack.}6...Kxf7.Qf3+Ke68.Nc3{Thehightispinned.}8...Nb49.a3Nxc2+{Sacrificingarook.}10.Kd1Nxa11.Nx45{Amovewithpotentialforadangerousdiscoveredattack}11...Qh412.Nxc7+{Doublecheck.}12...Kd713.Qf7+Qe714.Nxa8Qxf715.Bx7{AndBlack'skingpositionisdestroy,andwhiteisapawnup.White'sknightmaybehanging,butsoisblack's.} ```

[MISSING_PAGE_FAIL:25]

* "author": "Human 0", "text": "Honest question for those with higher ELOs. First, sorry if this is a confusing or vague question but I'll try my best to word it: At what rating, in your opinions, do you find people stop making \"silly" mistakes? I'm at 1500 rapid and 1300 blitz, trying to improve my play. I found that at these ratings blunders are rare (assuming adequate time). It seems like mostly people just don't see/miss tactics or play inaccuracies rather than playing straight up mistakes/blunders. Do you higher rated elo players feel the same way? Or do you think the inaccuracies we make/tactics we miss are quite obvious, the same way I can see a blunder is obvious? Curious on the perspective.", "score": 8, "other_data": false, "url": "/r/ches/comments/nhh2mn/honest_question_for_those_with_higher_elos/" } {  "author": "Human 1", "text": "Im 2200 in blitz on lichess, and of my last 10 blitz games, around 8 were decided by major blunders. In rapid, the amount of major blunders decreases a lot, but they are still very common. When you get higher rated, you will still blunder because you are also going to be facing higher rated opponents. If I played a 1200, I would rarely blunder but I blunder very easily against 2300+ people.", "score": 22, "other_data": false, "url": "/r/ches/comments/nhh2mn/honest_question_for_those_with_higher_elos/ gywapvb/" }

## Appendix E Data Collection and Preprocessing

### Online Chess websites

We choose around 26 chess websites, including chess.com and Lichess.com to scrape chess-related language corpus. We gather a diverse range of chess and language data from various platforms, including blogs, news, and articles. Specifically, we focused on extracting relevant information from several topics including blogs, news, openings, chess terms, forums, and articles. These topics were carefully chosen as they contain valuable texts pertaining to chess background knowledge, news, and also PGN games in some instances.

We utilize Beautiftulsoup4and Playwright [1] to parse HTML pages and locate all the texts and PGNs. We further transfer those PGNs into text which helps us build mixed game-language datasets from these sources. We record crucial details such as the URL, author, title, time, and the actual content of the articles.

Footnote 4: https://www.crummy.com/software/BeautifulSoup/

### Online Chess forums

We choose 5 chess forums and follow basically similar way with Appendix E.1 to scrape forum text data.

### Annotated PGN

We collect our annotated PGN from 7 sources: Lichess studies, Chess publishing, Megabase, Pgnlib, path-tochems mastery and gamechnot.

**Lichess studies** Lichess Study provides a rich collection of annotated PGNs. The annotations are embedded in PGNs, explaining the insight of the moves. Users can conveniently search for studies based on keywords or specific topics like Sicilian Defense, Puzzles, or d5. To enhance the searching process, we collect a comprehensive set of 54 popular keywords. Our implementation leverages Selenium's [48] built-in functions to efficiently parse HTML pages and simulate the searching process. Additionally, we use Lichess APIs5to request for PGN games associated with a specific study ID.

Footnote 5: https://lichess.org/api

**Chess publishing** This contains commercial annotated PGNs from Chesspublishing.com6, so we do not open source this source of data.

Footnote 6: https://www.chesspublishing.com/content/

**Megabase** This contains commercial annotated PGNs from Megabase20237, so we do not open source this source of data.

Footnote 7: https://shop.chesbase.com/en/products/mega_database_2023

**Pgnlib** We collect annotated PGN from Pgnlib8.

**Paththotechsmastery** We collect annotated PGNs from Path to Chess Master9.

**Gameknot** We use Selenium [48] to scrape annotated PGNs from gameknot10.

Footnote 8: https://www.angelifire.com/games3/smartbridge/

Footnote 9: https://www.pathotechsmastery.com/

Footnote 10: https://gameknot.com/list_annotated.pl?u=all

### Existing Datasets

We mainly extract all chess-related language corpus from existing dataset C4 [43], Pile [19], Oscar [37], Wikipedia [18] and RedPajama [53]. To extract chess-related language corpus, we first filter language corpus that contains the keyword 'chess'. We further utilize the deberta-v3-base-tasksource-nli model11, which can conduct zero-shot classification based on classes specified by users. We set two classes: chess-related and non-ches-related for this model. We input the first 2000 characters of each text example to the model and set the threshold as 0.5.

Footnote 11: https://huggingface.co/sileod/deberta-v3-base-tasksource-nli

### YouTube transcripts dataset

To collect the youtube transcripts dataset, we first gather 19 keywords and utilize scrapetube12 to gather all related channels. We extract all videos from these channels and use the same deberta-v3-base-tasksource-nli model mentioned in Appendix E.4 to filter all video transcripts and also the corresponding videos that are not relevant to chess. It is fairly easy to extract transcripts from videos and the main difficulty is to extract the FEN chessboard representations from videos. Here we mainly utilize two steps to extract specific FEN from chess videos.

Footnote 12: https://github.com/dermasmid/scrapetube

#### e.5.1 Extract Chess Board from videos

The first step is to extract the chessboard from videos. We utilize GLIP [28], a zero-shot language-based detection model. By setting the prompt as 'Chess, chessboard', we can extract chess-related detection bounding boxes from videos. We conduct further filterings such as length-width ratio filtering to guarantee it is a valid chessboard bounding box in most cases, which will be processed based on the second step.

#### e.5.2 Convert Chess board image to FEN text format

Our second step involves converting the chessboard image into FEN text. FEN format serves as a great way to describe the current chess board state in text-based format. The pipeline that converts the chess board image to FEN format includes three main procedures - chess board decomposition, piece classification, and the prediction of which player is the next turn.

**Chess board decomposition** The aim of this section is to breakdown a whole chess board into 64 small tiles (8 rows * 8 columns), where each tile contains only one chess piece. To achieve this, we initially convert the RGB image to grayscale, preparing for the line detection process. Subsequently, we make two convolutional kernels to find horizontal and vertical gradients13. The Hough Transform is then applied to detect vertical and horizontal lines and filter out seven vertical and seven horizontal lines that fit the demand. Finally, we divide the board into 64 tiles by having the position of the 14 lines.

Footnote 13: https://github.com/Elucidation/tensorflow_chesbot/blob/master/tensorflow_compvision.ipynb

**Piece classification** To facilitate model training and evaluation, we employ an open source chess image dataset on Kaggle14 which contains 80k training images and 20k testing images. Each tile can be classified into one of the 13 categories (p, r, b, n, k, q, P, R, B, N, K, Q, and space) which is detailed in Appendix B.3. We implement a model in PyTorch which uses a pre-trained ResNet8 [20] due to its well-established performance in image classification tasks. To adapt the model to our specific problem, we replaced the original fully connected layer with a new layer consisting of 13 output neurons, corresponding to the 13 pieces categories. We train the model on 40000 images with hyperparameters shown in Table 11.

Footnote 14: https://www.kaggle.com/datasets/koryakinp/chess-positions

After the training process, we evaluate the model on a testset with 20k images (equivalent to 128k tiles). Please refer to Table 12 for the final accuracy of each category.

**Prediction of next turn** As FEN format also includes the prediction of the next turn which is indicated by "w" for white, and "b" for black, the prediction of the next turn is accomplished by analyzing the main color of each tile. We use Colorthief, a library for grabbing the color palette from images, to extract the main color from each tile since the background color of a tile will be highlighted if a move is played on that tile. Hence, we find the highlighted tile by analyzing the tile color to know who is the current player and naturally infer who is the next turn.

Finally, we also provide a final certainty percentile to evaluate to what extent the generated FEN is correct by calculating the product of the accuracy of the 64 tiles.

### Lichess dataset

We collect 5 months of Lichess dataset from the Lichess database [30]: 2013-02, 2014-02, 2015-02, 2016-02, and 2017-02. In fact, there are much more data available and we leave more game data for future work.

### Pro-player dataset

We collect our pro-player dataset from PGN Mentor15.

\begin{table}
\begin{tabular}{l r} \hline \hline \multicolumn{1}{l}{Hyperparameter} & Value \\ \hline Batch size & 32 \\ Number of epochs & 10 \\ Learning rate & 0.001 \\ Optimizer & SGD \\ Momentum & 0.9 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Training hyperparameters for chess board to FEN

\begin{table}
\begin{tabular}{l l r} \hline \hline Color & Piece & Accuracy (\(\%\)) \\ \hline Black & **p**awn & \(99.98\) \\  & **rook** & \(99.99\) \\  & **knight** & \(99.98\) \\  & **b**ishop & \(99.98\) \\  & **q**u**en & \(100.00\) \\  & **king** & \(99.97\) \\ \hline White & **P**awn & \(100.00\) \\  & **R**ook & \(99.98\) \\  & k**N**ight & \(99.98\) \\  & **B**ishop & \(99.99\) \\  & **Queen & \(99.95\) \\  & **King** & \(99.98\) \\ \hline  & Space & \(100.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Validation accuracy of each chess piece

### Chess books

We select 100 chess-related keywords and search for all related chess books (around 9k books) on the online PDF library. Because of the legal issues about books' copyright, we choose not to open-source this source of data. Instead, we only open source the list of books we use.

### Ccrl

We collect our CCRL dataset without comments from the official website16 for three settings of time control: CCRL BLITZ, CCRL 40/2 FRC and CCRL 40/15.

Footnote 16: https://ccrl.chessdom.com/ccrl/4040/

### Chess puzzles

We collect our chess puzzles from the Lichess puzzle dataset17.

Footnote 17: https://database.lichess.org/#puzzles

### Chess modeling data

We design 11 modeling tasks to generate data:

* Given PGN, generate FEN representation.
* Given a list of UCI moves, generate FEN representation.
* Given FEN and a UCI move, transfer the move to SAN format.
* Given FEN and a SAN move, transfer the move to UCI format.
* Given FEN, generate an ASCII board.
* Given FEN and a UCI move, generate the next FEN.
* Given FEN and a SAN move, generate the next FEN.
* Given FEN, generate all legal moves in SAN format.
* Given FEN, generate all legal moves in UCI format.
* Given PGN, generate all legal moves in SAN format.
* Given PGN, generate all legal moves in UCI format.

To generate the synthetic modeling dataset for these tasks, we utilize PGN data extracted from the first 1 million games of the Lichess database dump from March 2017. In order to encompass a wider range of ELO ratings, we divide the elo-rating into 9 intervals: 0-1000, 1000-1200, 1200-1400, 1400-1600, 1600-1800, 1800-2000, 2000-2200, 2200-2400, and 2400-3000. Random sampling is employed to select games from each interval, ensuring that our dataset contains approximately 10,000 games for each ELO interval. Consequently, the dataset achieves a balanced representation across different ELO ratings. Then we further utilize the python-ches library [41] to complete all the tasks we design to generate our final synthetic modeling dataset.

### Preprocessing

We preprocess the data sources in three levels of granularity. For sources where existing **preprocessed dataset** are available, we filter out the subset that contains chess-related information without performing additional preprocessing. For sources that we retrieve from the **Internet**, we only parse portions of the HTML that contains information about chess. We implement different parsers for the different sources we consider. As a result, our data preprocessing can be more light-weight compared to previous work that extracts corpora from raw HTML web pages. For **PGN games**, we use the original PGN but filter out some annotations that are not useful for learning the model.

The different sources contain data in different formats. To facilitate training on all datasets, we preprocess all datasets to have the same jsonI format.

We implement the data-preprocessing step for each source as a separate Apache Beam pipeline which allows us to process the datasets in parallel into multiple shards. We note that a simple Apache Beam pipeline implementation provides no guarantees that data processing will be in the same order as they were read from the files. As a result, running the same pipeline twice will produce a set of shards that are shuffled differently. To provide determinism in our data-processing pipeline, we adopt a deterministic shuffling strategy similar to the implementation in TensorFlow Datasets (TFDS) to ensure reproducible data processing while maintaining scalability.

We provide further details on the preprocessing used for each individual source below:

**C4, Oscar, The Pile, RedPajama, Wikipedia** Since these datasets are available in processed format, we do not perform any additional preprocessing.

**StackExchange** We use the forums Chess StackExchange18. We preprocess the StackExchange data in the same way as done in RedPajama. Concretely, for each question, we first clean up the text via simple regular expressions and remove some HTML tags in the text. Then we prefix the question with Q: and the answers with A: and then concatenate the questions with all answers.

Footnote 18: https://chess.stackexchange.com/

**Chess puzzle and Chess modeling data** The original data format is in CSV format with key data, such as puzzle FEN and puzzle answer. We leverage some language templates to transfer the CSV as natural language text.

**Lichess database, CCRL and pro-player dataset** We keep only games with known finish, i.e., (win, lose or draw). We remove clk, arrow, evp annotations from the comments. We further remove Emojis from the comments. Afterward, each PGN is considered a single string of text that is used for downstream training.

**Medium article, Chess forum, Chess books** We run the same preprocessing pipeline as in [25].

**Annotated PGNs** We conduct language filtering using Fasttext19 in ChessCLIP preprocessing. And we conduct the same preprocessing as we do in Lichess database in ChessGPT training preprocessing.

Footnote 19: https://fasttext.cc/docs/en/language-identification.html

**Instruction data from GPT-4** No-further pre-processing.

**Conversational data from Reddit** We filter the conversations based on the language, response length, number of emojis, blacklist words, and scores.

We initially applied the same data-processing procedure described in [25] for all of the data that we collected. However, we found that the filtering used in [25] can be too aggressive in removing useful examples as many of our data sources include a significant portion of chess notation that does not resemble natural language (e.g., chess puzzles). Therefore, we opted for more light-weight pre-processing and use the processing from [25] only in cases where the text includes a significant portion of natural language description (blogs for example). In addition, for further protection of privacy, we anonymize user names and replace them with terms like 'Human 0' in all conversation-like data, especially in chess forums and Reddit conversational data.

### Licenses and dataset cards

For specific Licenses and dataset cards, refer to our open-source dataset repository: https://huggingface.co/datasets/Waterhorse/chess_data.

## Appendix F Implementation and Evaluation Details

We open source all our models: ChessCLIP (https://huggingface.co/Waterhorse/ChessCLIP), ChessGPT-Base (https://huggingface.co/Waterhorse/chessgpt-base-v1) and ChessGPT-Chat (https://huggingface.co/Waterhorse/chessgpt-chat-v1). Refer to these URLs for model licenses and model cards.

### Implmenation details

#### f.1.1 ChessCLIP

For the ChessCLIP dataset, we preprocess the annotated PGNs to produce board/text pairs which we feed separately to the board and text encoders. In particular, for every move in the PGN, we extract the comments attached to the move as well as the board state. While our YouTube transcripts dataset can also serve as training data for ChessCLIP, we have discovered that it consistently contains more noise compared to the annotated PGN dataset. To ensure the stability of our training process, we have chosen to exclusively utilize the annotated PGN datasets. The task of refining the YouTube transcripts for future training remains a part of our ongoing work.

For the ChessCLIP model, we instantiate a ChessCLIP model with a pair of text encoder and a board/action encoder. For board/action encoder, we use a ResNet [20] architecture that conditions the action encoding via a modified FiLM layer [40]. We encode the board positions and moves using the same scheme as those used by Leela Chess Zero (lcl) [26], which is similar to the encoding used by AlphaZero [47] for encoding positions and moves in chess. Concretely, the board positions are encoded as a \(\mathcal{R}^{8\times 8\times 112}\) feature map and the actions are encoded as a \(\mathcal{R}^{1858}\) vector. For the text encoder, we follow the same architecture as with the original OpenAI

[MISSING_PAGE_FAIL:31]

Here we also show more evaluation results.

**General policy result.** Table 16 presents the results of the general policy experiment using black chess, which align with the findings from the previous white chess experiment. The comparison between the two ChessGPT models across different Elo ratings reveals a lack of noticeable distinctions, indicating the model's limited sensitivity to the key information provided in the prompt. A more intuitive illustration of this observation will be provided in the subsequent paragraph. There are two notable points to highlight. Firstly, ChessGPT demonstrates improvements compared to its base model RedPajama and performs on par with LLAMA. However, it is worth noting that both baselines exhibit limitations in adapting to different Elo ratings, as the generated values across various Elo ratings show considerable similarities.

**Words attention visualization.** To evaluate whether the ChessGPT-Base model captures the key information in the general policy task, we conducted a visualization analysis of its self-attention mechanism. The visualization, as shown in Figure 3, reveals that the model does attend to the "WhiteElo" and "BlackElo" values to some extent. However, the level of attention dedicated to these important features appears to be relatively weak. This suggests that the model's ability to appropriately incorporate and utilize the Elo ratings during the generation process is not as strong as desired. Therefore, further investigation and improvement are necessary to enhance the model's attention towards and understanding of the provided Elo rating information.

#### f.1.3 Compute resources

We use 8\(\times\)80G A100 GPUs for all our experiments. It takes 5 hours to train ChessCLIP using all A100 GPUs. And it takes 60 hours to train ChessGPT-Base model and 18 hours to train ChessGPT-Chat.

## Appendix G Evaluation details

### Evaluation task examples and plots

Here we show task examples and plots for our evaluation tasks. Basically, the evaluation tasks consist of three parts: Task Prefix, which can be regarded as a description of the task and is also the main prompt we use for LLMs. Input, which is the question and the input of LLMs. And Target, which contains the answer of the question for exact string match tasks, or target score, which provides the score for each available answer for multi-choice tasks.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{4}{c}{Move Scores (\%)} \\ \cline{2-5} Elo Rating & LLAMA & RedPajama & ChessGPT-Base & ChessGPT-Chat \\ \hline
700-1000 & 52.9 \(\pm\) 0.9 & 46.2 \(\pm\) 1.0 & 51.9 \(\pm\) 0.1 & 52.1 \(\pm\) 0.9 \\
1200-1500 & 53.2 \(\pm\) 0.9 & 46.9 \(\pm\) 0.9 & 53.0 \(\pm\) 1.0 & 52.4 \(\pm\) 1.0 \\
1700-2000 & 52.1 \(\pm\) 0.8 & 46.6 \(\pm\) 1.0 & 52.0 \(\pm\) 1.0 & 52.0 \(\pm\) 1.0 \\
2700-3000 & 53.6 \(\pm\) 0.9 & 47.3 \(\pm\) 1.0 & 52.2 \(\pm\) 0.9 & 52.1 \(\pm\) 1.1 \\ \hline \hline \end{tabular}
\end{table}
Table 16: General policy evaluation in Black

Figure 3: Visualization of ChessGPT-Base Attention: The figure illustrates the attention space of ChessGPT for the General Policy experiment, generating a compound level next move based on **Elo rating**. The highlighted areas represent the importance of attention, with color intensity ranging from black to red, where red indicates the highest importance.

* **Task Prefix**: For each of the following (in-progress) chess games, please complete the notation for the last shown move by filling in the destination square. Input: e2e4 e7e6 d2d4 d7d5 e4e5 c7c5 c2c3 b8c6 g1f3 g8e7 a2a3 a7a5 f1d3 c8d7 c1e3 d8 Target: ["b8", "b6", "c7", "c8"]

* 0 36
* 0 26

Figure 4: **Example in Chess state tracking.22** The input leads to the following board configuration.The task is to predict the squares to which the piece at d8, the black queen, can be legally moved. Here the black queen at square d8 can be legally moved to any of the squares [b8", "b6", c7", c8"].

* **Task Prefix:** Evaluate the following PGN to see whether black or white takes advantage.
* **Input:** 1. e4 e6 2. d4 d5 3. e5 c5 4. Nt3 cxd4 5. Nxd4 Nc6 6. Nxc6 bxc6 7. Nc3 Ne7 8. g3 Ng6 9. 14 Be7 10. Be3 h5 11. Bd3 f5 12. exf6 Bxf6 13. Bd2 Ne7 14. Oe2 Od6 15. O-O-O Bd7 16. Kh1 Rb8 17. b3 Qa3 18. Bc1 Qa5 19. Bd2 Bxc3 20. Bxc3 Qxc3 21. Qd2 Of6 22. Rhe1 a5 23. a4 c5
* **Target Score:** ("Black has advantage.": 1, "The game is equal.": 0, "White has advantage.": 0)

* **Task Prefix:** Annotate the last step of the following PGN.
* **Input:** 1. d4 Nf6 2. c4 e6 3. Nf3 Bb4+ 4. Bd2 a5 5. g3 O-O 6. Bg2 b6 7. O-O Ba6 8. Bg5 Be7 9. Qc2 Nc6 10. a3 h6 11. Bxf6 Bxf6 12. Rd1 Qe7 13. e3 Raes 14. Nfd2 g5
* **Target Score:** "Karpov could have resigned here with a clear conscience.": 0, "White intends to further weaken Black's kingside with 19.h5.": 0, "20...Kh7 21.Bx96+ kpg6 22.Oxce6 Gives White a winning attack.": 0, "Black overreacts to the positional strength of White's game. 14...g6 would have been more solid.": 1

* **Task Prefix:** Show me the PGN of the following opening.
* **Input:** Amar Gambit Opening
* **Target Score:** "1. Nh3 d5 2. g3 e5 3. 14 Bxh3 4. Bxh3 ext4": 1, "1. d4 d5 2. c4 e6 3. Nc3 c5 4. cxd5 exd5 5. dxc5 d4 6. Na4 b5": 0, "1. d4 Nf6 2. g4 Nxq4 3. f3 Nf6 4. e4": 0, "1. Nc3 c5 2. b4": 0, "1. d4 Nf6 2. c4 g6 3. Nc3 Bg7 4. e4 d6 5. f3 O-O 6. Nge2": 0

* **Task Prefix:** Show me the opening name of the following PGN.
* **Input:** 1. Nh3 d5 2. g3 e5 3. 14 Bxh3 4. Bxh3 ext4. The opening name of this PGN is.
* **Target Score:** ("Amar Gambit": 1, "Tarrasch Defense: Tarrasch Gambit": 0, "Indian Defense: Gibbins-Weidenhagen Gambit, Malteese Falcon": 0, "Van Geet Opening: D00fcsseldorf Gambit": 0, "King's Indian Defense: S00e4misch Variation, Bobotsov-Korchnoi-Petrosian Variation": 0)

* **1. e4 e6 2. Ke2 d5 3. e5 c5 4. f4 Nc6**
* **5. Nf3 Qb6 6. g4 Bd7 7. h4 Nge7 8. c3 Ng6**
* **9. d4 cxd4 10. cxd4 Be7 11. Kf2 0-0 12. h5 Nh8 ----> Bxh7+**
* **13. Be3 Qxb2+ 14. Kg3 Qxa1 15. Bd3 Qxa2 16. Rh2 Qa1**
* **17. Qc2 Nb4 18.**

Figure 5: **Example for Checkmate in one.23 The goal of this task is to probe the ability of language models to play chess in standard algebraic notation. The input to the model is a sequence of moves such that a next possible move is a checkmate. For example, the chess game shown in the figure can checkmate the opponent in one step is "Bxh7+"**

* **Input** :1. e4 62. Ke2 d5 3. e5 c5 4. 14 Nc6 5. NI3 Qb6 6. g4 Bd7 7. h4 Nge7 8. c3 Ng6 9. d4 cxd4 10. cxd4 Be7 11. Kf2 O-O 12. h5 Nh8 13. Be3 Oxb2+ 14. Kg3 Oxa1 15. Bd3 Oxa2 16. Rh2 Qa1 17. Qc2 Nb4 (Now white has checkmate in one move.}
* **Target Score**: {"Oxa1": 0.0, "Bxh7"+: 1.0, "Qd2": 0.0, "Qe2": 0.0, "Qd1": 0.0, "Oc3": 0.0, "Oc7": 0.0, "Bb1": 0.0, "Bc2": 0.0, "Bf5": 0.0, "Bg6": 0.0, "Bf1": 0.0, "Bb5": 0.0, "Bxa6": 0.0, "Bc4": 0.0, "Bb3": 0.0, "Bc1": 0.0, "Bd2": 0.0, "Bc3": 0.0, "Bxb4": 0.0, "Nb42": 0.0, "Nc3": 0.0, "Na3": 0.0, "Rc2": 0.0, "Bg2": 0.0, "Rh1": 0.0, "Rm2": 0.0, "R62": 0.0, "Rh3": 0.0, "Rh4": 0.0, "Rh2": 0.0, "Rh4": 0.0, "Rh2": 0.0, "Rh3": 0.0, "Rh4": 0.0, "Rm5": 0.0, "Rm6": 0.0, "Nf2": 0.0, "Nf2": 0.0, "Nf2": 0.0, "Nf2": 0.0, "Nf2": 0.0, "Nf2": 0.0.)

* **Task Prefix**: In the following chess game, you play black.
* **Input**: [Date "2017.04.01"] [White "????"] [Black "????"] [Result "0-1"] [WhiteElo "983"] [BlackElo "2983"] [WhiteRatingDiff "???"] [BlackRatingDiff "???"] [ECO "???"] [Opening "???"] [TimeControl "300+0"] [Termination "Time forfeit"] [1. b3 e5 2. Bb2 Nc6 3. a3 Nite 4. h3 d5 5. g3 Bd6 6. Bg2 O-O 7. e3 e4 8. d3 Be5 9. d4 Bd6 10. Ne2 Nc7 11. c4 c6 12. Nb5 Nf5 13. Qd2 e5 14. Qc2 Be6 15. cxd5 cxd5 16. Nb5 Rc8 17. Qd2 Qb6 18. Nk6 19. Qd1 Rc7 20. a4 Rf68 21. Ba3 Nf5 22. Rc5 Rx5 23. dx5 [Target Score: {"Rx5": 0.0, "Qa7": 0.02, "Qd8": 0.05, "Qc7": 0.083, "Qb4+": 0.11, "Qxc5": 0.13, "Oc6": 0.16, "Qa6": 0.19, "Nx6": 0.22, "Nx63": 0.25, "Nd4": 0.27, "Rc6": 0.30, "Ng4": 0.33, "Qxb3": 0.36, "d4": 0.38, "Nh4": 0.41, "Rh8": 0.44, "Nd7": 0.47, "h6": 0.5, "96": 0.52, "Rf8": 0.55, "Ra8": 0.58, "Ne7": 0.61, "Qd6": 0.63, "h5": 0.66, "Re8": 0.69, "Kf8": 0.72, "Qb5": 0.75, "Rf8": 0.77, "Bd7": 0.80, "Ne8": 0.83, "Rb8": 0.86, "Nd6": 0.88, "95": 0.91, "Rc7": 0.94, "Nh5": 0.97, "Nh6": 1.0]

#### g.1.1 Evaluation on chess modeling ability

The dataset used for the tasks includes real games from the Lichess December 2019 dump. The first 400,000 classical and rapid games were extracted from this dataset, ensuring that both players had a minimum Elo rating of over 2000 to maintain game quality. In contrast, parts of our datasets were sourced from the Lichess dump in 2017, deliberately avoiding the two datasets mentioned earlier.

#### g.1.2 Evaluation on task construction

For state value multi-choice and general policy task construction, we follow a similar way as the chess modeling data to collect 3000/2000 PGN states from Lichess 2017-03 dump respectively. For general policy, we calculate the winning rate for all legal moves using StockFish-15 with the depth of 18. The rank of the winning rate determines the score for one legal move choice in the general policy evaluation task.

Qualitative Analysis

In this section, we present some qualitative results between RedPajama, ChessGPT-Base and ChessGPT-Chat.

### Experimental set-up

We use the Beam search implementation in HuggingFace transformers for generating samples from each model. Please refer to Table 17 for the hyperparameters used for the qualitative experiments. For each question, we sample three responses from each model. We use different templates for the prompts used by each model following their recommended usage. Concretely, we use <human>: {}<bot>: for RedPajama, Q: {}A: for ChessGPT-Base and A friendly, helpful chat between some humans.<|endoftext|>Human 0: {}<|endoftext|>Human 1: for ChessGPT-Chat. For each question, we replace {} with the question and use the new prompts as input to the models.

\begin{table}
\begin{tabular}{l c} \hline \hline Hyperparameter & Value \\ \hline max new tokens & \(128\) \\ top-\(k\) & \(50\) \\ top-\(p\) & \(0.7\) \\ temperature & \(0.7\) \\ \hline \hline \end{tabular}
\end{table}
Table 17: Hyperparameters for qualitative analysis.

[MISSING_PAGE_EMPTY:37]

[MISSING_PAGE_EMPTY:38]

[MISSING_PAGE_EMPTY:39]

* DeepBlue was a chess-playing computer developed by IBM, that became the first computer to beat a reigning world champion in a match, when it defeated Garry Kasparov in Game 6 of their 1997 match.

* DeepBlue was a chess-playing computer developed by IBM, that became the first computer to beat a reigning world champion in a match, when it defeated Garry Kasparov in Game 6 of their 1997 match.

* DeepBlue defeated Garry Kasparov in a chess match in 1997.

* No, DeepBlue defeated Garry Kasparov in 1997.

* **ChessGPT-Base**: Mixed responses were provided. The first and third answers are "no", while the second answer is "yes". This is inconsistent and without further explanation.
* **ChessGPT-Chat**: All three answers mentioned a 1997 game that DeepBlue had won against Garry Kasparov. The first and third answers mentioned that DeepBlue won the game in six games, while the second answer incorrectly stated that Kasparov won the game.
* **RedPajama**: Both the first and second answers were "yes", pointing out that DeepBlue beat Garry Kasparov in 1997. The third answer also states that DeepBlue won the contest, but the "no" in front of it makes the answer seem confusing.

\begin{table}
\begin{tabular}{|p{284.5pt}|} \hline \hline
**Problem** \\ \hline

[MISSING_PAGE_POST]

 \end{tabular}
\end{table}
Table 21: Question on Deep Blue.

### Chess modeling

In this section, we present a qualitative analysis of the model's performance to performing chess-related tasks. We provide qualitative comparison for three tasks:

#### Opening recognition

In this task, the models were tasked to detect a chess opening. Example responses can be found in table 22.

#### 1.e4 e5 2.14

What is the name of this opening? _(Answer: King's Gambit)_

[TABLE:A1.T2][E

\begin{table}
\begin{tabular}{|p{142.3pt}|} \hline \hline \multicolumn{1}{|c|}{Methoding} \\ \hline \multicolumn{1}{|c|}{The Sicilian Defence.} \\ \hline \multicolumn{1}{|c|}{The French Defense} \\ \hline \multicolumn{1}{|c|}{The French Defense.} \\ \hline \multicolumn{1}{|c|}{Human Error Analysis} \\ \hline \multicolumn{1}{|c|}{**ChessGPT-Base**:The first and second answers called it "Kings Gambit Accepted", which is the name when the opposing side accepts a sacrificed pawn. And the third answer is Kings Gambit, which is a broader category. Technically, its only called Accepted after 2...ex4. \\ \multicolumn{1}{|c|}{**ChessGPT-Chat**: All three responses call it Kings Gambit, which is correct.} \\ \multicolumn{1}{|c|}{**RedPajama**: All responses are wrong, calling it Sicilian Defense and French Defense, neither of which have anything to do with the openings offered. \\ \hline \end{tabular}
\end{table}
Table 22: Question on chess openings

### PGN to FEN conversion

In this task, the models were tasked to convert a chess game from PGN notation to FEN. Example responses can be found in table 23.

\begin{table}
\begin{tabular}{|c|} \hline \hline  [width=0.25cm] 
\begin{tabular}{} \end{tabular} \\ \hline \hline \end{tabular} \\ \hline \end{tabular} \\ \hline \end{tabular}
\end{table}
Table 23: Question on PGN to FEN conversion.

### UCI to FEN conversion.

In this task, the models were tasked to convert a chess game from UCI notation to FEN. Example responses can be found in table 24.

#### 4.1.1 Unset of the 2016

The first two answers have subtle errors and the chess colors (w/b) are mislabeled.

**RedPajama**: Completely wrong, it gives the game record instead of FEN.

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Group** \\ \hline 
\begin{tabular}{l} Convert the following UCI to FEN: g2g3 d7d6 f1g2 e7e5 g1f3 g8f6 d2d4 b8 \\ _(Answer: rnbqkb1r/pppp2pp/3p1n2/4p3/3P4/5NP1/PPP1PPBP/RNBQK2R b KOkq - 0 4)_ \\ \end{tabular} \\ \hline \hline
**Class GPT-Base** \\ \hline \begin{tabular}{l} You can use the following code to convert the FEN string to a UCI string: #!/usr/bin/python import chess import chess.pygame import chess.uci import chess.pgn fen = rnbqkbm/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KGkq - 0 1 board = chess.Board(fen) board.parse\_san(fen) # print(board.san(board.fen)) \\ \end{tabular} \\ \begin{tabular}{l} The FEN is rnbqkb1r/pp2pppp/3p1n2/4p3/3PP3/5NP1/PPP2PBP/RNBQK2R b KOkq - 0 5. \\ \end{tabular} \\ 
\begin{tabular}{l} Here is a link to the FEN representation of the above position: m-bqkb1r/ppp2pp/3p1n2/4p3/3PP3/5NP1/PPP2PBP/RNBQK2R b KOkq - 0 5 \\ \end{tabular} \\ \hline \hline
**Class GPT-Base** \\ \hline \begin{tabular}{l} rnbqkb1r/ppp2pp/3p1n2/4p3/3P4/5NP1/PPP1PPBP/RNBQK2R w KGkq - 0 5 \\ \end{tabular} \\ \begin{tabular}{l} rnbqkb1r/ppp2pppp/3p1n2/4p3/3P4/5NP1/PPP1PPBP/RNBQK2R w KGkq - 0 5 \\ \end{tabular} \\ 
\begin{tabular}{l} rnbqkb1r/ppp2pppp/3p1n2/4p3/3P4/5NP1/PPP1PPBP/RNBQK2R b KOkq - 0 4 \\ \end{tabular} \\ \hline \hline
**RedPajama** \\ \hline \begin{tabular}{l} Here is the FEN:g2k4f4 b8k4b5 d7k6d6 e7k6e5 f1k1f2 g8k8f6 UCI: g2k4f4 b8k4b5 d7k6d6 e7k6e5 f1k1f2 g8k8f6 Note that the FEN is a different notation than the UCI. \\ \end{tabular} \\ \begin{tabular}{l} Here is the FEN: g2g3 d7d6 f1g2 e7e5 g1f3 g8f6 d2d4 b8 The following are the chess pieces: g - knight g2 - bishop g3 - knight d - queen d7 - queen d6 - queen f - bishop f1 - bishop e - knight e7 - knight e5 - knight h - knight h2 - knight h3 - knight h4 - knight h5 - knight h6 - knight \\ \end{tabular} \\ 
\begin{tabular}{l} FEN: g2g3 d7d6 f1g2 e7e5 g1f3 g8f6 d2d4 b8 This is a standard position in chess, but not in FEN. \\ \end{tabular} \\ \hline \hline
**Class GPT-Base**: The first answer gives a code snippet rather than a direct answer. The second and third answers try to convert UCI to FEN, but the result is wrong. In terms of verbosity, too much information was provided, including an irrelevant code snippet. \\ \begin{tabular}{l} **Class GPT-Chat**: The third answer is correct. The first two answers have subtle errors and the chess colors (w/b) are mislabeled. \\ \end{tabular} \\ 
\begin{tabular}{l} **RedPajama**: Completely wrong, it gives the game record instead of FEN. \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 24: Question on UCI to FEN conversion.

[MISSING_PAGE_EMPTY:44]

**ChessGPT-Base**: The first answer is a short review of the Scotch Game, but not a step-by-step analysis. The second answer mentions other software for game analysis, which is not what the user asked for. The third answer just repeats the PGN given.
**ChessGPT-Chat**: The first two answers started a step-by-step game analysis but were interrupted and not completed, but the analysis given is reasonable. The third answer provides a follow-up move for 15...Oxb2, but the user does not provide this step.
**RedPajama**: The first and third answers just repeat the PGN given, no analysis is provided. The second answer tries to provide a follow-up move, which is also irrelevant to the question, since the user just requested an analysis, not a follow-up move.

## Appendix I ChessCLIP visualization

We present a visualization demo to better illustrate ChessCLIP's capability. Here we choose the chess opening as a test scenario and choose ten different chess openings. Ten chess opening PGNs and their corresponding names are arranged in sequence, so the diagonal cells in the entire similarity matrix should have the highest similarity. We present such a similarity matrix generated by ChessCLIP in Figure 6. The results exactly correspond to our expectations, which successfully validate the effectiveness of our ChessCLIP model.

## Appendix J Potential directions

In this section we will describe several potential directions based on our dataset.

1. Dataset Augmentation and Fine-tuning: Researchers can explore dataset augmentation techniques to further enhance the diversity and size of the provided dataset. By introducing variations in game conditions, player strategies, or opening positions, researchers can create augmented datasets that can be used to fine-tune existing language models or train new models specifically tailored for chess-related tasks. This can potentially lead to improved performance in state tracking, value judgement, and policy evaluation.
2. Transfer Learning to Chess Variants: The dataset and benchmark provided in this study can serve as a valuable resource for transfer learning experiments to chess variants. Researchers can leverage the knowledge learned from the base chess task and apply it to chess variants such as Fischer Random Chess or Chess960. By fine-tuning or adapting pre-trained language models on the provided dataset, researchers can explore the capabilities of these models in handling different chess variants, including state tracking, value judgement, and policy evaluation
3. Reinforcement Learning/Planning with Language Models: Combining reinforcement learning or planning with language models trained on the provided dataset opens up exciting possibilities for improving chess-playing agents. Researchers can develop reinforcement learning algorithms that

Figure 6: Similarity matrix of different chess opening PGN and text using ChessCLIP.

utilize the language model's state tracking ability to build more sophisticated and strategic agents. By training agents to interact with the language model in a dialogue-like manner, researchers can explore the potential of language-based reinforcement learning for chess-related tasks, such as move generation and evaluation..
4. Explainable AI in Chess: Given the language model's ability to generate human-readable outputs, researchers can investigate the application of explainable AI techniques in chess. By interpreting the model's generated moves or predictions, researchers can gain insights into the reasoning behind the model's decisions. This can lead to the development of explainable AI systems that provide justifications or explanations for their chess moves, aiding both players and analysts in understanding and learning from the model's decision-making process.
5. Multi-modal Approaches: Researchers can explore multi-modal approaches that combine textual and visual information for chess-related tasks. By incorporating board visualizations, game position images, or move sequence visualizations along with textual inputs, researchers can develop models that leverage both textual and visual cues to improve state tracking, value judgement, and policy evaluation in chess. This can open up avenues for multi-modal analysis and understanding of chess games, allowing models to capture and reason over both textual and visual representations simultaneously.
6. Chess Education and Tutorial Systems: The dataset can be utilized to develop educational tools and tutorial systems for chess players of different skill levels. Researchers can leverage the language model's expertise in state tracking, value judgement, and policy evaluation to provide interactive and personalized learning experiences. By tailoring the tutorial content and feedback based on individual player performance, researchers can create intelligent systems that assist in skill development and strategic improvement in chess.
7. Adversarial Attacks in Chess: With the increasing use of language models in critical applications like chess analysis and decision-making, it becomes essential to investigate potential vulnerabilities and develop defenses against adversarial attacks. Researchers can explore techniques to generate adversarial examples specifically targeted at chess-related tasks. By identifying weaknesses in language models' state tracking or policy evaluation abilities, researchers can enhance the robustness and security of these models.
8. Chess Game Generation: Researchers can utilize the provided dataset to develop models capable of generating new chess game sequences. By leveraging the language model's understanding of chess moves and game structures, researchers can explore generative models that can produce novel and diverse chess game sequences. This can be beneficial for various applications, including chess game analysis, training data generation, and even game generation for chess variants.

Overall, our dataset and benchmark offer numerous potential directions, ranging from dataset expansion and transfer learning to exploring chess variants, education, analysis, and game generation. These directions have the potential to advance the field of chess-related language modeling and provide valuable tools and resources for chess players and enthusiasts.

## Appendix K Limitations and Potential Societal Impact

The availability of a comprehensive and diverse chess dataset presents both limitations and potential societal impacts that researchers should consider.

**Limitations.** While the chess dataset provided in this study is valuable, it is important to acknowledge its limitations. One limitation is the potential bias introduced by relying on historical Lichess matches from different time periods. This may result in variations in player strategies, popular openings, and game trends over time, potentially impacting the generalizability of the dataset. Additionally, it is worth noting that the dataset predominantly focuses on standard chess, and may not encompass the full spectrum of chess variants. Researchers interested in exploring niche or less-popular variants may need to gather additional data from specific sources to ensure comprehensive coverage of different chess variants. These considerations are crucial to ensure the validity and applicability of research findings based on the provided dataset.

**Potential Societal Impact.** The availability of a comprehensive and diverse chess dataset can have a significant societal impact. First and foremost, it can contribute to the development of more advanced and intelligent chess-playing agents. These agents can be utilized in various applications, such as chess analysis, training tools for players of different skill levels, and even as opponents for chess enthusiasts. The dataset can also facilitate the advancement of chess education by providing valuable resources for tutorials, interactive learning platforms, and strategic guidance. Additionally, the dataset can inspire research in the field of artificial intelligence, contributing to the development of innovative techniques that can be applied beyond the domain of chess. Lastly, the dataset can encourage the exploration of explainable AI methods in chess, enabling players to understand and learn from the reasoning behind the model's moves, thereby promoting transparency and trust in AI systems.