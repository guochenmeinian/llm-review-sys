# Learning a 1-layer conditional generative model in total variation

Ajil Jalal  Justin Kang

UC Berkeley

{ajiljalal, justin_kang}@berkeley.edu &Ananya Uppal

UT Austin

ananya.uppal09@gmail.com &Kannan Ramchandran

UC Berkeley

kannanr@eecs.berkeley.edu &Eric Price

UT Austin

ecprice@cs.utexas.edu

###### Abstract

A conditional generative model is a method for sampling from a conditional distribution \(p(y\mid x)\). For example, one may want to sample an image of a cat given the label "cat". A feed-forward conditional generative model is a function \(g(x,z)\) that takes the input \(x\) and a random seed \(z\), and outputs a sample \(y\) from \(p(y\mid x)\). Ideally the distribution of outputs \((x,g(x,z))\) would be close in total variation to the ideal distribution \((x,y)\).

Generalization bounds for other learning models require assumptions on the distribution of \(x\), even in simple settings like linear regression with Gaussian noise. We show these assumptions are unnecessary in our model, for both linear regression and single-layer ReLU networks. Given samples \((x,y)\), we show how to learn a 1-layer ReLU conditional generative model in total variation. As our result has no assumption on the distribution of inputs \(x\), if we are given access to the internal activations of a deep generative model, we can compose our 1-layer guarantee to progressively learn the deep model using a near-linear number of samples.

## 1 Introduction

Generative models are in the midst of an explosion in accessibility, as models like DALL-E [31] or Stable Diffusion [32] capture the attention of millions. In many cases, these generative models can be succinctly represented by a fundamental mathematical object--the conditional distribution \(p(y\mid x)\). In the example of text-to-image generative models, \(x\) can represent a text prompt or its Word2Vec embedding [26], and the model can be seen as sampling an image \(y\) from its conditional distribution \(p(y\mid x)\). With large numbers of people accessing these models, a massive amount of sample pairs \((y_{i},x_{i})\), are becoming available online. A natural question to ask is: How many samples \((y_{i},x_{i})\) does it take to learn the conditional generative model \(p(y\mid x)\)?

Recent empirical studies such as Stanford Alpaca [34], which attempt to learn GPT-3.5 from limited samples, indicate that the number of samples needed may be within a practical range. In this paper, we attempt to address this problem from a fundamental perspective grounded in a concept from classical theoretical statistics: the Maximum Likelihood Estimator (MLE). Specifically, we focus on feed-forward generative models from a relatively simple family and ask: with _no_ assumptions on \(x\), how many samples are required to efficiently learn the conditional generative model \(p(y\mid x)\)?

Linear Regression.Consider ordinary linear regression with Gaussian noise: you observe independent samples \((x_{i},y_{i})\in\mathbb{R}^{k}\times\mathbb{R}\) of the form

\[y=x\cdot w^{*}+\eta\qquad\text{ for }\eta\sim N(0,1).\]How many samples does it take to get a "good" solution? For standard metrics--such as parameter distance in \(w^{*}\) or prediction error on \(y\)--the sample complexity of linear regression depends on properties of the distribution (such as the conditioning of \(x\), or the variance of \(y\)) that could be unbounded and cannot be tested. For example, in the classic analysis (see [7], Chapter 3), sample complexity depends on the design matrix, which in turn results in a dependence on the expectation of \(\|x\|_{2}\). The typical approach to deal with this would require bounded moments for \(x\). For learning the conditional distribution, the more natural metric is total variation (TV) distance: the parameters \(w\) induce a distribution \(p_{w}(x,y)=p(x)p_{w}(y\mid x)\) where \(p(x)\) is the _true_ distribution of \(x\), and we would like to find \(\widehat{w}\) such that

\[d_{TV}(p_{w^{*}}(x,y),p_{\widehat{w}}(x,y))\leq\varepsilon.\] (1)

This ensures that when the input \(x\) come from the user in the true unknown distribution \(p(x)\), the model generates a conditional sample that is close in TV to the true model. It turns out that this goal, unlike parameter distance, _can_ be solved with no assumption on the distribution.

**Theorem 1.1** (Informal version of Theorem 4.1).: _The MLE (i.e., least squares regression) achieves (1) with \(O(\frac{1}{\varepsilon^{2}}k\log\frac{1}{\varepsilon})\) samples, regardless of the distribution of \(x\)._

To our knowledge, all previous guarantees for linear regression either require some assumptions on, or give guarantees in terms of, the distribution of \(x\) or \(y\). We avoid this dependence on the \(x\) distribution by adopting a similar analysis to Theorem 11.2 in [20].

Main Result: 1-Layer Networks.Modern feed-forward conditional generative models (e.g., StyleGAN2) are more complicated than linear regression. They are stochastic neural networks with layers of the form:

\[y=\phi(W^{*}x+\eta)\qquad\text{for }\eta\sim\mathcal{N}(0,\Sigma^{*})\] (2)

for \(x\in\mathbb{R}^{k}\), \(y\in\mathbb{R}^{d}\), \(\phi(x)=\max(x,0)\) is the ReLU activation, and some weights \(W^{*}\in\mathbb{R}^{d\times k}\), \(\Sigma^{*}\in\mathbb{R}^{d\times d}\).

We show that 1-layer generative models of the form (2) can _also_ be estimated using the MLE (which is concave), with small TV error and without any assumption on \(x\).

**Theorem 1.2** (Informal version of Theorem 4.5).: _Suppose \(y\) is drawn according to (2) where \(\Sigma^{*}\) has condition number at most \(\kappa\). Using \(O\Big{(}\frac{kd+d^{2}}{\varepsilon^{2}}\log\frac{kd\kappa}{\varepsilon}\Big{)}\) samples, the distribution generated by the MLE has TV error \(\varepsilon\), regardless of the distribution of \(x\)._

Like Theorem 1.1, Theorem 1.2 shows that a conditional generative model can be learned in TV regardless of the distribution of the input \(x\). It generalizes Theorem 1.1 in three ways: (a) the output \(y\) is \(d\)-dimensional rather than 1-dimensional; (b) the covariance of the noise \(\Sigma^{*}\) is learned rather than specified; and (c) the ReLU nonlinearity \(\phi\) imparts additional complex structure.

Indeed, the point (c) means that parameter distances are poor metrics for this problem. For example, when an element of \(W^{*}x\) has a large negative bias, the corresponding coordinate in \(y\) will almost

Figure 1: A conditional distribution defined by a conditional generative model. To sample from the conditional distribution \(p(y\mid x)\), we perform inference. Due to the stochastic nature of the model, each output is different.

always be \(0\). This means sample pairs \((x_{i},y_{i})\) provide little information about \(W^{*}\), but still provide useful information about the conditional distribution. As we will later see, exploiting this valuable information in the truncated samples allows us to significantly outperform prior works such as [35], which do not exploit these \(0\)-valued samples.

Multilayer Networks Given Activations.Given our distribution-free results on \(1\)-layer networks, it is possible to extend our results to deep multi-layer networks. Given access to the internal activations of a neural network (but not the weights), our results can be applied layer-wise. Intermediate layers may have poorly conditioned input distributions, but since our result does not depend on the input distribution, we achieve strong guarantees for layer-wise learning in Theorem 4.6.

### Proof Approach

In this outline we focus on the case of \(1\)-dimensional \(y\), and and standard Gaussian noise \(\eta\sim\mathcal{N}(0,1)\). Our proof approach is inspired by learning bounds that exploit finite VC dimension. We would like to show (1), or equivalently,

\[d(w^{*},\widehat{w}):=\operatorname*{\mathbb{E}}_{x}[d_{TV}(p_{w^{*}}(y\mid x ),p_{\widehat{w}}(y\mid x))]\leq\varepsilon,\] (3)

when we see \(n\) samples \(x_{i}\) of \(x\), and one sample \(y_{i}\) for each \(x_{i}\). We do this in two stages. First, we show that the empirical distance between \(w\) and \(\widehat{w}\) is small i.e.,

\[\widetilde{d}(w^{*},\widehat{w}):=\operatorname*{\mathbb{E}}_{x}[d_{TV}(p_{w^ {*}}(y\mid x),p_{\widehat{w}}(y\mid x))]\leq 0.5\varepsilon,\] (4)

where \(\operatorname*{\mathbb{E}}_{x}[f(x)]=\frac{1}{n}\sum_{i=1}^{n}f(x_{i})\) denotes the empirical expectation over \(x\).

Second, we show that the empirical distance is a good proxy for the true distance, i.e.,

\[d(w^{*},\widehat{w})\leq\widetilde{d}(w^{*},\widehat{w})+0.5\varepsilon\leq\varepsilon,\] (5)

which gives (3).

Linear Case.In the linear case, both stages are straightforward. The linear regression solution has an explicit form, and it is well known and easy to show that

\[\operatorname*{\mathbb{E}}(x^{T}(w^{*}-\widehat{w}))^{2}\propto k/n.\]

Since \(d_{TV}(p_{w^{*}}(y\mid x),p_{\widehat{w}}(y\mid x))=\Theta(\min(1,|x\cdot(w^{ *}-\widehat{w})|))\), Jensen's inequality implies (4) for \(n>k/\varepsilon^{2}\).

Secondly, \(f_{w}(x):=d_{TV}(p_{w^{*}}(y\mid x),p_{w}(y\mid x))\) is bounded and unimodal in \(w\). Thus, it suffices to bound the deviation of the empirical average from the true \(f_{w}(x)\) with Chernoff's inequality.

ReLU Case.In the ReLU case, we have \(y=\phi(w^{*}\cdot x+\eta)\), and both stages of the previous analysis are more difficult.

The most interesting part of our proof is showing the first stage for the ReLU case, which states that the \(\widehat{w}\) maximizing

\[L(w):=\frac{1}{n}\sum_{i=1}^{n}\log p_{w}(y_{i}\mid x_{i})\]

satisfies (4). Now, for any \(w\) not satisfying (4),

\[\operatorname*{\mathbb{E}}_{y}[L(w)-L(w^{*})] =-\operatorname*{\mathbb{E}}_{x}d_{KL}(p_{w^{*}}(y\mid x)\|p_{w} (y\mid x))\] \[\leq-2\operatorname*{\mathbb{E}}_{x}[d_{TV}(p_{w^{*}}(y\mid x),p _{w}(y\mid x))^{2}]\leq-2\varepsilon^{2},\]

where the first inequality follows from Pinsker's inequality. Unfortunately, \(L(w)-L(w^{*})\) does not concentrate well, by virtue of the KL-divergence being unbounded. However, we can upper bound it via the Bernstein inequality, such that for a fixed \(w\) not satisfying (4), and given \(n=\frac{1}{\varepsilon^{2}}\log(\frac{1}{\delta})\) samples, we have

\[L(w)-L(w^{*})\leq-\varepsilon^{2},\]with probability \(1-\delta\). Using a careful covering argument and \(n=(k/\varepsilon^{2})\log(1/\delta)\) samples, we can uniformly extend this to _all_\(w\) not satisfying Eq (4). By definition, the MLE has \(L(\widehat{w})\geq L(w^{*})\), and by our uniform bound, it must satisfy (4).

The second stage changes because \(f_{w}(x)\) depends on \(x\cdot w\) and \(x\cdot w^{*}\) in a more complicated way than through \(x\cdot(w-w^{*})\). This makes showing a bounded VC dimension more difficult; however, unpacking the proof that VC implies generalization, we can still show that the net (normally given by Sauer's lemma) is bounded. This generalization holds as long as \(f_{w}(x)\) is unimodal in \(x\cdot w\).

## 2 Contributions

1. We show that MLE can perform distribution learning in the setting of linear regression and multi-layer ReLU networks. Our bounds do not make assumptions on the distribution of \(x\) or the condition number of \(W^{*}\), and achieve a sample complexity polynomial in the system parameters.
2. We improve the sample complexity bound in [35], which estimates the parameters of a one-layer ReLU network but suffers an exponential dependence on the \(W^{*}\) term. In contrast, as we seek to estimate the _distribution_ of \((x,y)\), rather than the parameter \(W^{*}\), we are able to avoid this. See Section 4.2 for more details.
3. Our algorithm for learning multi-layer ReLU networks is considerably simpler than [1], who learn discriminators that are engineered to perform moment-matching on the output of each layer of the network. Furthermore, [1] impose a strong requirement on the sparsity and independence of the activations at each layer, which essentially allows standard techniques in sparse coding to recover these activations.

## 3 Related Work

Generative adversarial networks (GANs) [19; 2; 30] are a popular family of generative models that train a generator and discriminator in an adversarial training framework. The seminal result by [22] proposed _progressive_ growing of GANs (PGGANs) as a way to stabilize and accelerate the training phase of these models. Future results, such as StyleGAN [23] introduce more complicated architectures and "style" variables. Additionally, these models add noise at each layer of the generator in order to introduce greater stochasticity in the generated images, which is important for textures such as hair and skin.

Distributional LearningMost theoretical results have focused on the min-max optimality of GANs [15; 28; 29], characterizing their stationary points [21; 17], or characterizing their generalization once they have reached a global minimum [3; 5]. The closest result to ours is [35]. Setting \(x\) to a deterministic scalar in our problem statement reduces it to [35], who consider \(y=\phi(b+\eta)\) s.t. \(b,\eta\in\mathbb{R}^{d}\), and they seek to learn the covariance of \(\eta\) along with the bias vector \(b\). However, their sample complexity bound suffers an exponential dependence on \(\|b\|_{\infty}^{2}\).

Single layer networks have attracted recent attention, as they provide a tractable formulation for studying the dynamics and generalization of adversarial learning [24; 18; 25; 11]. The recent results of [1] show that multi-layered models that satisfy a property known as _forward super-resolution_ (such as PGGANs) can be learned in polynomial time and sample complexity using stochastic gradient descent-ascent. In this case, the discriminator is designed to detect differences between higher order moments of the generated and training distribution. Deep models have also been considered in [12; 10].

## 4 Main Results

In this section we first show that the MLE of the parameters learns the input-output joint distribution for linear regression. Then, we extend this guarantee to the case where the ReLU activation function \(\phi\) is applied to the multi-dimensional output \(y\). Finally, we show that we can compose the 1-layer ReLU guarantee to learn the distribution generated by a multi-layer model.

### Linear Regression

We begin with the classic linear regression problem of learning the parameter \(w^{*}\) from a linear model

\[y=x\cdot w^{*}+\eta,\] (6)

where \(\eta\sim\mathcal{N}(0,\sigma^{2})\), \(w\in\mathbb{R}^{k}\), \(\sigma\) is known and \(x\in\mathbb{R}^{k}\) with some distribution. This problem has been studied for centuries. Our novelty is that we view (6) as a conditional generative process, and instead of studying error in Euclidean distance in parameter space, i.e., minimizing \(\|\widehat{w}\cdot x-w^{*}\cdot x\|_{2}\), we focus on error in \(d(w^{*},\widehat{w})\) as defined in (3), which only captures the error in \(\widehat{w}\) insofar as it impacts our distribution estimate. Given data \(\{(x_{i},y_{i})\}_{i=1}^{n}\) generated by (6), the MLE is:

\[\widehat{w}:=\operatorname*{arg\,max}_{w\in\mathbb{R}^{k}}\sum_{i\in[n]}\log p _{w}(y_{i}|x_{i})\equiv\operatorname*{arg\,min}_{w\in\mathbb{R}^{k}}\sum_{i\in[ n]}\frac{(y_{i}-w\cdot x_{i})^{2}}{\sigma^{2}}.\]

The following theorem establishes that the MLE is close in TV distance. The proofs for all results in this section are in Appendix A.

**Theorem 4.1**.: _Let \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be i.i.d. random variables generated from the linear model (6), and assume that \(\sigma\) is known. Then, for a sufficiently large constant \(C>0\),_

\[n=C\frac{k}{\varepsilon^{2}}\log\frac{1}{\varepsilon}\]

_samples suffice to ensure that with probability \(1-e^{-\Omega(\varepsilon^{2}n)}\) over the data,_

\[d(\widehat{w},w^{*})\leq\varepsilon.\]

Note that one cannot hope to get such a guarantee in the classical setting where error is measured in \(\|\widehat{w}\cdot x-w^{*}\cdot x\|_{2}\) without additional assumptions on the distribution of \(x\) because if \(x\) is badly conditioned, the error may be dominated by very rare directions of \(x\) that we never sample. For example, the bounds in [7], Chapter 3, require the second moment of \(x\) to be bounded.

Since we only wish to learn the distribution of \(y\) in total variation, no single \(x\) can contribute much to our loss and we get a distribution-free result. This is possible as the total variation distance is bounded, and we can invoke Theorem 11.2 in Gyorfi et al [20].

We now state two lemmas needed to prove Thereom 4.1, assuming without loss of generality that \(\sigma^{2}=1\). We split the proof into two stages. In the first stage, we bound \(\widetilde{d}(\widehat{w},w^{*})\), which denotes the empirical TV distance (4) over the training set \(\{x_{i}\}_{i=1}^{n}\).

**Lemma 4.2**.: _Let \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be i.i.d. random variables such that \(y_{i}=x_{i}\cdot w^{*}+\mathcal{N}(0,1)\). Then, for \(n\geq\frac{k}{2}\), with probability \(1-e^{-\Omega(n)}\), the MLE \(\widehat{w}\) satisfies_

\[\widetilde{d}(\widehat{w},w^{*})\leq\sqrt{\frac{k}{2n}}.\]

The proof relies on the fact that \(p_{w^{*}}(y|x_{i})\) and \(p_{\widehat{w}}(y|x_{i})\) are Gaussian distributions, so

\[d(p_{\widehat{w}}(y|x_{i}),p_{w^{*}}(y|x_{i}))=\Theta(\min\{1,|x_{i}^{T}( \widehat{w}-w^{*})|\}).\]

Using the explicit form of the MLE, we can show that, with high probability,

\[\frac{1}{n}\sum_{i}\bigl{(}x_{i}^{T}(\widehat{w}-w^{*})\bigr{)}^{2}\leq\frac{ k}{2n},\]

and Lemma 4.2 follows from Jensen's inequality.

The second stage shows that the empirical average of the TV distance \(\widetilde{d}(\widehat{w},w^{*})\) is close to the population average \(d(\widehat{w},w^{*})\).

**Lemma 4.3**.: _Let \(\{x_{i}\}_{i=1}^{n}\) be i.i.d. random variables such that \(x_{i}\sim\mathcal{D}_{x}\). For a sufficiently large constant \(C>0\), and for \(n=C\frac{k}{\varepsilon^{2}}\log\frac{1}{\varepsilon}\) with \(n\geq\frac{k}{2}\), we have:_

\[\Pr_{x_{i}\sim\mathcal{D}_{x}}\biggl{[}\sup_{w\in\mathbb{R}^{k}}\left| \widetilde{d}(w,w^{*})-d(w,w^{*})\right|>\varepsilon\biggr{]}\leq e^{-\Omega( ne^{2})}.\]Note the probability in the above statement is with respect to the distribution of \(x\), and does not depend on \(y\). The proof follows Theorem 11.2 in Gyorfi et al [20]: it relies on the fact that the TV distance is bounded and a unimodal function of \(w\cdot x\). This implies that for each \(x_{i}\), we are able to partition the space of \(w\) with \(O(1/\varepsilon)\) hyperplanes such that within each cell the TV distance varies by at most \(\varepsilon\). As we have \(n\) samples and \(w\in\mathbb{R}^{k}\), the number of cells induced in \(\mathbb{R}^{k}\) is \(\propto(n/\varepsilon)^{k}\), and it is sufficient to provide concentration bounds for one representative in each cell. This approach is similar to bounding the VC dimension of a set of binary functions. Setting \(n=\Theta(\frac{k}{\varepsilon^{2}}\log\frac{1}{\varepsilon})\) and combining Lemma 4.2 with Lemma 4.3 gives \(d(\widehat{w},w^{*})\leq\varepsilon\).

### ReLU Case

Now consider the single-layer ReLU. We observe \((x,y)\in\mathbb{R}^{k}\times\mathbb{R}^{d}\) such that:

\[y=\phi(W^{*}x+\eta),\quad\eta\sim\mathcal{N}(0,\Sigma^{*}),\] (7)

where \(\eta\in\mathbb{R}^{d}\), \(W^{*}\) and \(\phi(\cdot)=\max(\cdot,0)\) is applied coordinate-wise. The matrices \(W^{*}\in\mathbb{R}^{d\times k}\) and \(\Sigma^{*}\in\mathbb{R}^{d\times d}\) are _unknown_, and we do not observe \(\eta\). The variable \(x\) is drawn from an arbitrary probability distribution \(\mathcal{D}_{x}\), and we make _no additional assumptions_ on \(\mathcal{D}_{x}\): this is important, as we will progressively cascade layers, and one should think of \(\mathcal{D}_{x}\) being the distribution of activations at each layer.

Given a sample \((x,y)\in\mathbb{R}^{k}\times\mathbb{R}^{d}\), let \(S\) denote the co-ordinates of \(y\) that are zero-valued, and let \(S^{c}\) denote the compliment of \(S\). Then, the log-likelihood of \(W,\Sigma,\) on this sample is given by

\[\log p_{W,\Sigma}(y|x)=c-\frac{\log\lvert\Sigma\rvert}{2}+\log\int\limits_{ \begin{subarray}{c}t_{S}\leq 0\\ t_{S^{c}}=y_{S^{c}}\end{subarray}}\exp\biggl{\{}-\frac{(t-Wx)^{T}\Sigma^{-1}( t-Wx)}{2}\biggr{\}}dt_{S}.\] (8)

where \(c\) is a normalization constant which does not depend on \(W\) or \(\Sigma\). This function is a mixed density: in the coordinates of \(y\) that are 0, i.e., in the set \(S\), we integrate the Gaussian density over the negative orthant, as \(W^{*}x+\eta\) could have been any negative value in those coordinates.In Lemma F.1 in the Appendix, we show that Eqn (8) is concave after an invertible reparameterization of \(W,\Sigma\).

In this setting, proving an analogue of Theorem 4.1 poses multiple challenges:

* The output \(y\) is \(d\)-dimensional rather than a scalar, and \(\eta\) in Eqn (7) introduces correlations between the coordinates of \(W^{*}x\), such that we cannot decompose the log-likelihood in Eqn (8) per coordinate.
* We do not know the covariance matrix \(\Sigma^{*}\), and it must be estimated.
* Lemma 4.2 requires the explicit form of the MLE in linear regression. In the absence of such a closed-form solution, we need to directly analyze the log-likelihood, which is a mixed density and involves integrating the Gaussian likelihood over the zero-valued coordinates of \(y\). In order to handle this, we use the Gyorfi approach again on the log-likelihood. This is challenging because the variables we concentrate are KL-divergences, which are unbounded and require Bernstein type inequalities.
* otherwise, their sample complexity scales as \(e^{\lVert W\rVert_{\infty}^{2}}\).

Nonetheless, we can handle most of these difficulties, and the only assumption we make is that the condition number of \(\Sigma^{*}\) is bounded and known to our estimator.

**Assumption 4.4** (Condition number bound).: _Let \(\lambda_{\max}^{*},\lambda_{\min}^{*}\) denote the largest and smallest singular values of \(\Sigma^{*}\). We assume there exists \(\kappa<\infty\) such that \(\frac{\lambda_{\max}^{*}}{\lambda_{\min}^{*}}\leq\kappa\). We further assume that the value of \(\kappa\) is known to our estimator._

Note that the condition number only allows us to control the correlation between the coordinates of \(W^{*}x+\eta\). The other challenges introduce by the ReLU, such as the lack of a closed form MLE, the need to estimate \(\Sigma^{*}\) and a mixed density log-likelihood remain.

Under Assumption 4.4, the following theorem shows that the MLE \(\widehat{W},\widehat{\Sigma}\) achieves a small total variation distance. The proof of this theorem is in Appendix C.

**Theorem 4.5**.: _Let \(\mathbb{R}^{d\times d}_{\kappa}\) denote the set of positive definite matrices with condition number \(\kappa\). Given \(n\) samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) satisfying Assumption 4.4, where \(x_{i}\sim\mathcal{D}_{x}\) i.i.d., and \(y_{i}\) is generated according to (7), let \(\widehat{W},\widehat{\Sigma}:=\arg\max_{W\in\mathbb{R}^{d\times k},\Sigma\in \mathbb{R}^{d\times d}_{\kappa}}\frac{1}{n}\sum_{i}\log p_{W,\Sigma}(y_{i}\mid x _{i})\). Then, for a sufficiently large constant \(C>0\),_

\[n=C\cdot\left(\frac{kd+d^{2}}{\varepsilon^{2}}\right)\log\biggl{(}\frac{\kappa kd }{\varepsilon\delta}\biggr{)}\]

_samples suffice to ensure that with probability \(1-\delta\), we have_

\[d_{TV}\Bigl{(}(\widehat{W},\widehat{\Sigma}),(W^{*},\Sigma^{*})\Bigr{)}\leq\varepsilon.\]

Comparison to [35].Our result is closely related to [35]. Our ReLU model reduces to their model by setting \(W^{*}\in\mathbb{R}^{d}\) as a vector, and \(x=1\). In order to learn the distribution of \(y\), they first estimate the parameters \(W^{*},\Sigma^{*}\), in \(\ell_{2}\) norm and then convert the \(\ell_{2}\) error to a TV error. The parameter recovery is done per coordinate of \(W^{*},\Sigma^{*}\), by performing MAP estimation on the positive samples in \(y\). This crucially assumes that each coordinate has enough positive samples, and to that end, they assume that each entry in \(W^{*}\) is positive--otherwise, their sample complexity scales as \(e^{\|W^{*}\|_{\infty}^{2}}\). Our results _do not make any assumptions_ on \(W^{*}\) and handles a wider class of matrix valued \(W^{*}\). Additionally, the objective function (8) does not discard the zero valued samples in \(y\), making it more sample efficient.

We assumed that the covariance matrix \(\Sigma^{*}\) has condition number \(\kappa\), and our sample complexity scales as \(\log\kappa\). Hence, even if \(\kappa=e^{\operatorname{poly}(d,k)}\), we only pay a \(\operatorname{poly}(d,k)\) penalty. This improves on the result in [35], where the sample complexity scales as \(\kappa^{2}\). While our statistical guarantees are strictly better, [35] gives poly-time and poly-space algorithmic guarantees for their estimator. We discuss the empirical limitations of our algorithm in Section 6.

Lower BoundsIgnoring log factors, the complexity factor of \(kd\) is obviously required. Furthermore, learning a Gaussian with unknown covariance matrix in total variation takes \(\widehat{\Omega}(d^{2})\) samples; see [6]. Our Theorem 4.5 would solve their lower bound instance, the same lower bound applies to our problem.

Extension to Multi-Layer Generative Models.Consider the following \((L+1)\)-layered generative model.

\[x_{L+1} =W_{L}^{*}x_{L}+\eta_{L},\quad\text{ where }\quad x_{\ell}=\phi(W_{ \ell-1}^{*}x_{\ell-1}+\eta_{\ell-1})\in\mathbb{R}^{d_{\ell}}\quad\forall\quad \ell\in[1,L],\] (9) \[x_{0} \sim\mathcal{D}_{0}\quad\text{ and }\quad\eta_{\ell}\sim \mathcal{N}(0,\Sigma_{\ell}^{*})\quad\forall\quad\ell\in[0,L].\] (10)

We can compose the guarantees provided by Theorem 4.1 and 4.5 to show that we can learn this model.

**Theorem 4.6**.: _Given \(n\) i.i.d. samples of \((x_{0},\dots,x_{L+1})\), such that each matrix \(\Sigma_{\ell}^{*}\) satisfies Assumption 4.4, let \(\widehat{W}_{\ell},\widehat{\Sigma}_{\ell},\) be the MLE estimates of \(W_{\ell}^{*},\Sigma_{\ell}^{*}\) learned from samples of \((x_{\ell},x_{\ell+1})\). Define \(m:=\max_{\ell}d_{\ell}^{2}\). Then,_

\[n=O\biggl{(}\frac{l^{2}m}{\varepsilon^{2}}\log\biggl{(}\frac{lm\kappa}{ \varepsilon\delta}\biggr{)}\biggr{)},\]

_samples suffice to ensure that with probability \(1-\delta\),_

\[d_{TV}\Bigl{(}\{\widehat{W}_{\ell},\widehat{\Sigma}_{\ell}\}_{\ell=0}^{L},\{( W_{\ell}^{*},\Sigma_{\ell}^{*})\}_{\ell=0}^{L}\Bigr{)}\leq\varepsilon.\]

Comparison to [1].The modelling assumptions in [1] are similar to ours - the authors learn a generative model per layer, using images produced per layer. The key differences of their model are: (i) each layer is deterministic (there is no \(\eta_{\ell}\)), (ii) their learning algorithm does not require access to the activations of each layer, (iii) their algorithm performs moment-matching by crafting the discriminator strategically.

In order to avoid requiring activations at each layer, [1] imposes a sparsity assumption on the activations: this allows them to leverage tools from existing results in sparse coding [4], such thatsparse activations at each layer can be recovered using images produced by the layer. This assumption can be somewhat strong, as it implies that the activations are roughly independent of one another, and the sparsity remains constant over layers, despite the layers themselves expanding by a factor of 4, i.e., \(d_{\ell}\geq 4d_{\ell-1}\ \forall\ \ell\leq L-1\).

## 5 Simulations

We now numerically verify our theoretical claims and compare against other approaches. A detailed description of simulation methods are included in the appendix. Our code is available at https://github.com/basics-lab/learningGenerativeModels.git.

### Scaling in \(n\) and \(k\)

Figure 2 numerically investigates how TV distance of the MLE scales with the number of samples \(n\) and the input dimension \(k\). We consider a model with \(1\)-dimensional output and a \(k\)-dimensional input: \(y=\phi(x\cdot w^{*}+\eta)\), for \(w^{*}\in\mathbb{R}^{k}\) and \(\eta\sim\mathcal{N}(0,\sigma^{2})\). We set \(\sigma^{\mathcal{I}}=1\) and \(w^{*}=\mathds{1}_{k\times 1}\), both unknown to the optimizer, which has samples \((y_{i},x_{i})_{i=1}^{n}\). Figure 1(a), which plots the error in TV distance against \(n\) on a log-log plot, has a slope of roughly \(-1/2\) as predicted by our theory. Similarly, Figure 1(b) has a slope of \(1/2\), which is in line with our theory on scaling with respect to input dimension \(k\). We defer simulations involving scaling in \(d\) to the appendix because computing TV distance becomes increasingly difficult as \(d\) becomes large, and we must resort to using upper bounds.

### Distribution Independence

The fact that our guarantee does not depend on the distribution of \(x\) suggests that the expected TV error of the distribution learned from the MLE may be similar for all distributions over \(x\). To test this we consider both \(x\sim\mathcal{N}(0,I_{k})\) and \(x\sim\bigotimes_{k}\mathrm{Lap}(0,1)\), i.e., each element of \(x\) is drawn independently standard Laplace. Figure 1(a) verifies our hypothesis, showing only very slight differences in our observed empirical average TV error between the two distributions over a wide range of \(n\), and for \(k=5\) and \(k=20\).

Figure 2: (a) Plot of TV distance vs. \(n\). \(\sigma^{2}=1\), \(w^{*}=\mathds{1}_{k\times 1}\), for two different values of \(k=5\) and \(k=20\). Plot includes data for two different distributions of \(x\). Note that distribution has little impact on TV distance, and in both cases, we see the error decreasing with a slope of \(-1/2\) in alignment with our theory. (b) Plot of TV distance vs. input dimension \(k\). For both \(n=10^{3}\) and \(n=10^{4}\), the error grows with a slope of roughly \(1/2\), in alignment with our theory. In both plots \(2000\) runs are used to compute the mean. Error bars represent 95% confidence intervals.

### Scaling with Bias and Condition Number of Covariance Matrix

A key feature of the MLE is that it makes use of truncated samples. This is in contrast to [35], which leverages results on learning truncated normal distributions [14] where truncated samples are not observed. This leads to a stark difference in performance as the number of truncated samples becomes large. To show this, we consider a model with a \(d\)-dimensional output and \(1\)-dimensional input. We let \(x=1\) almost surely, and then take \(w^{*}=b\mathds{1}_{d\times 1}\) for some _bias_\(b\in\mathbb{R}\) and \(\eta=\mathcal{N}(0,\Sigma)\) with \(\Sigma=I_{d}\), thus \(y=\phi(\eta+b\mathds{1}_{d\times 1})\). As \(b\) becomes more negative, the number of truncated samples increases. Figure 2(a) shows the differing behavior of MLE and that of [35] as \(b\) becomes negative. For ease of computing the TV distance we set \(d=3\), and restrict optimization over diagonal \(\Sigma\). The solid blue line depicts the performance of the MLE. We observe that the TV error is constant for \(b>0\) and begins to decrease rapidly for \(b<0\). This happens because as \(b\) becomes more negative, the truncation places more probability mass at \(y=0\). Indeed, the dashed blue lines indicate that even as the TV error is decreasing rapidly, the mean square estimation error of the covariance and mean increase, however, since most of the probability mass is at zero, this does not significantly impact the TV. In contrast, the method of [35] rapidly deteriorates as \(b<1\) as the number of untruncated samples decreases. We also point out that even when the bias is large, [35] is still significantly worse. We attribute this to the fact that even when there is no truncated samples, [35] is still minimizing a different MAP objective. More discussion of this is provided in the appendix.

Robustness to Condition Number of \(\Sigma\).Another concern is how TV error scales as a function of the condition number of \(\Sigma^{*}\). Poorly conditioned \(\Sigma^{*}\) can put significant probability masses on small sets, and potentially cause large error. We consider a similar environment to the one described above, but fix \(b=1\) and alter diagonal entries of \(\Sigma^{*}\) such that one entry is \(\sqrt{\kappa}\), another is \(\sqrt{\kappa^{-1}}\) and the rest are \(1\), making the condition number \(\kappa\). Figure 2(b) shows that the MLE is not measurably impacted by the changing condition number over the range plotted. This is not true of [35], where we observe that TV does grow with condition number.

## 6 Limitations

This work is only a first step to understanding fundamental limits of learning generative models. We showed that our theorems for single-layer networks can be composed to get sample complexity bounds on deep networks, with a critical caveat: we require access to not just the input and output pairs, but also the intermediate activations. This is not practical in many scenarios, and removing this restriction will be an important direction for future research. Beyond this, we assume that the learner has an understanding of the model architecture. In many cases, however, a learner may not be aware

Figure 3: (a) Left hand axis shows TV distance vs. bias vector \(b\) with \(y=\phi(\eta+b\mathds{1}_{d\times 1})\), \(d=3\), \(\eta=\mathcal{N}(0,\Sigma)\), and \(\Sigma=I_{d}\). Note that MLE (blue) has error going to zero as bias becomes negative, while the opposite is true for the baseline (red). Right hand axis shows the mean-squared error of the parameters \(\Sigma\) and the mean \(\mu\), each point was run a total of \(2000\) times. (b) TV distance vs. condition number, \(d=3\). MLE does not exhibit trend with condition number, but baseline does. Error bars are 95% confidence intervals, over 20000 runs.

of the number of layers a network has or a vast number of other architectural details. Additionally, we inherit known problems with the MLE, such as exacerbation of biases that exist in the training data (Chapter 24.1.3 in [33]).

Our results place emphasis on sample complexity over _computational complexity_. Though we show that the MLE problem is concave, this work does not provide a thorough analysis of the optimization problem. It is possible that similar results to [14, 35] can be derived for the MLE problem. Indeed, empirically, we find that a similar projected stochastic gradient ascent performs well in our problem. A careful analysis must consider factors like the distribution over \(x\), the condition number of \(\Sigma^{*}\) and the truncation probability, all of which are likely to impact the optimization.

## 7 Conclusions

We have studied the problem of learning conditional generative models from a limited number of samples. We have shown that it is possible to learn a 1-layer ReLU conditional generative model in total variation, with no assumption on the distribution of the conditioning variable \(x\) using the MLE. We have also shown that this result can be extended to multi-layer ReLU networks, given access to the internal activations. Our results suggest that MLE is a promising approach for learning feed-forward generative models from limited samples.

## 8 Acknowledgements

Ajit Jalal and Kannan Ramchandran are supported by ARO 051242-002. Justin Kang and Kannan Ramchandran are supported by NSF EAGER Award 2232146. Eric Price is supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).

## References

* [1] Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical generative models for real-world distributions. _arXiv preprint arXiv:2106.02619_, 2021.
* [2] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. _arXiv preprint arXiv:1701.07875_, 2017.
* [3] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In _International Conference on Machine Learning_, pages 224-232. PMLR, 2017.
* [4] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In _Conference on learning theory_, pages 113-149. PMLR, 2015.
* [5] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and empirics. In _International Conference on Learning Representations_, 2018.
* [6] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. _Journal of the ACM (JACM)_, 67(6):1-42, 2020.
* [7] Francis Bach. Learning theory from first principles. _Draft of a book, version of Sept_, 6:2021, 2021.
* [8] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration inequalities: A nonasymptotic theory of independence_. Oxford university press, 2013.
* [9] Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* [10] Sitan Chen, Jerry Li, and Yuanzhi Li. Learning (very) simple generative models is hard. _arXiv preprint arXiv:2205.16003_, 2022.

* [11] Sitan Chen, Jerry Li, Yuanzhi Li, and Raghu Meka. Minimax optimality (probably) doesn't imply distribution learning for gans. _arXiv preprint arXiv:2201.07206_, 2022.
* [12] Sitan Chen, Jerry Li, Yuanzhi Li, and Anru R Zhang. Learning polynomial transformations. _arXiv preprint arXiv:2204.04209_, 2022.
* [13] Annie AM Cuyt, Vigdis Petersen, Brigitte Verdonk, Haakon Waadeland, and William B Jones. _Handbook of continued fractions for special functions_. Springer Science & Business Media, 2008.
* [14] Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient statistics, in high dimensions, from truncated samples. In _2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 639-649. IEEE, 2018.
* [15] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. _arXiv preprint arXiv:1711.00141_, 2017.
* [16] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-dimensional gaussians. _arXiv preprint arXiv:1810.08693_, 2018.
* [17] Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria? In _International Conference on Machine Learning_, pages 3029-3039. PMLR, 2020.
* [18] Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans in the lqg setting: Formulation, generalization and stability. _IEEE Journal on Selected Areas in Information Theory_, 1(1):304-311, 2020.
* [19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _Advances in neural information processing systems_, pages 2672-2680, 2014.
* [20] Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. _A distribution-free theory of nonparametric regression_, volume 1. Springer, 2002.
* [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* [22] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In _International Conference on Learning Representations_, 2018.
* [23] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4401-4410, 2019.
* [24] Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. Sgd learns one-layer networks in wgans. In _International Conference on Machine Learning_, pages 5799-5808. PMLR, 2020.
* [25] Yuanzhi Li and Zehao Dou. Making method of moments great again?-how can gans learn distributions. _arXiv preprint arXiv:2003.04033_, 2020.
* [26] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. _arXiv preprint arXiv:1301.3781_, 2013.
* [27] John P Mills. Table of the ratio: area to bounding ordinate, for any portion of normal curve. _Biometrika_, pages 395-400, 1926.
* [28] Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of o(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 30(4):3230-3251, 2020.
* [29] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. _Advances in neural information processing systems_, 30, 2017.

* [30] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. _arXiv preprint arXiv:1511.06434_, 2015.
* [31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [33] Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [34] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model. 2023.
* [35] Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer relu networks. _Advances in neural information processing systems_, 32, 2019.

Proofs of Linear Case

Throughout the appendix, for ease of notation, we overload the definition of the function \(d_{TV}(\cdot,\cdot)\). When inputs are random variables, it represent the TV distance between the distributions of those random variables.

**Lemma 4.2**.: _Let \(\{(x_{i},y_{i})\}_{i=1}^{n}\) be i.i.d. random variables such that \(y_{i}=x_{i}\cdot w^{*}+\mathcal{N}(0,1)\). Then, for \(n\geq\frac{k}{2}\), with probability \(1-e^{-\Omega(n)}\), the MLE \(\hat{w}\) satisfies_

\[\widetilde{d}(\hat{w},w^{*})\leq\sqrt{\frac{k}{2n}}.\]

The proof of this lemma requires Lemma A.1, which characterizes the distribution of the residual error of the MLE.

**Lemma A.1**.: _Given \(y\in\mathbb{R}^{n},X\in\mathbb{R}^{n\times k}\) satisfying \(y=Xw^{*}+\eta\), where \(\eta\sim\mathcal{N}(0,\sigma^{2}I_{n})\), the least square solution \(\hat{w}\) satisfies_

\[Xw^{*}-X\hat{w}\sim\mathcal{N}(0,\sigma^{2}X(X^{T}X)^{-1}X^{T}) \Rightarrow\mathbb{E}[\|X\hat{w}-Xw^{*}\|^{2}]=\sigma^{2}k.\]

Proof.: The least squares solution is given by

\[\hat{w} =(X^{T}X)^{-1}X^{T}y,\] \[=(X^{T}X)^{-1}X^{T}(Xw^{*}+\eta),\] \[=w^{*}+(X^{T}X)^{-1}X^{T}\eta.\]

Multiplying on the left by \(X\), we have

\[X\hat{w}=Xw^{*}+X(X^{T}X)^{-1}X^{T}\eta.\]

Since \(\eta\) is i.i.d. Gaussian with variance \(\sigma^{2}\), we have,

\[X(X^{T}X)^{-1}X^{T}\eta \sim\mathcal{N}(0,\sigma^{2}X(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}X^{T})\] \[\sim\mathcal{N}(0,\sigma^{2}X(X^{T}X)^{-1}X^{T})\]

This implies

\[\mathbb{E}[\|X\hat{w}-Xw^{*}\|^{2}] =\sigma^{2}\text{Tr}[X(X^{T}X)^{-1}X^{T}],\] \[=\sigma^{2}\text{Tr}[(X^{T}X)^{-1}X^{T}X],\] \[=\sigma^{2}k.\]

Proof of Lemma 4.2.: The KL divergence between two Gaussians \(P=\mathcal{N}(\mu_{1},\Sigma)\) and \(Q=\mathcal{N}(\mu_{2},\Sigma)\) is:

\[d_{KL}(P\|Q)=\frac{1}{2}(\mu_{1}-\mu_{2})\Sigma^{-1}(\mu_{1}-\mu_{2}).\]

By Pinsker's inequality, this implies

\[d_{TV}(P\|Q)\leq\min\biggl{\{}1,\frac{1}{2}\sqrt{(\mu_{1}-\mu_{2})\Sigma^{-1} (\mu_{1}-\mu_{2})}\biggr{\}}.\]Hence, the empirical TV on the dataset can be bounded by

\[\frac{1}{n}\sum_{i}d_{TV}(p_{\widehat{w}}(y|x_{i}),p_{w^{*}}(y|x_{i})) \leq\frac{1}{n}\sum_{i}\min\Biggl{\{}1,\frac{1}{2}\frac{|x_{i}^{T }(\widehat{w}-w^{*})|}{\sigma}\Biggr{\}},\] \[\leq\sqrt{\min\Biggl{\{}1,\frac{1}{4n}\sum_{i}\frac{\left(x_{i}^{ T}(\widehat{w}-w^{*})\right)^{2}}{\sigma^{2}}\Biggr{\}}},\] \[=\sqrt{\min\biggl{\{}1,\frac{1}{4n}\frac{1}{\sigma^{2}}\|X( \widehat{w}-w^{*})\|^{2}\biggr{\}}}.\]

where the second line follows from Jensen's inequality.

By Lemma A.1, we have

\[\mathbb{E}[\|X(\widehat{w}-w^{*})\|^{2}]=\sigma^{2}k.\]

which implies that with probability \(1-e^{-\Omega(n)}\), we have

\[\|X(\widehat{w}-w^{*})\|^{2}\leq 2\sigma^{2}k.\]

Substituting in the earlier inequality, we get

\[\frac{1}{n}\sum_{i}d_{TV}(p_{\widehat{w}}(y|x_{i}),p_{w^{*}}(y|x_{i}))\leq \sqrt{\min\biggl{\{}1,\frac{k}{2n}\biggr{\}}}=\sqrt{\frac{k}{2n}}\text{ for }n\geq\frac{k}{2}.\]

**Lemma 4.3**.: _Let \(\{x_{i}\}_{i=1}^{n}\) be i.i.d. random variables such that \(x_{i}\sim\mathcal{D}_{x}\). For a sufficiently large constant \(C>0\), and for \(n=C\frac{k}{c^{2}}\log\frac{1}{c}\) with \(n\geq\frac{k}{2}\), we have:_

\[\Pr_{x_{i}\sim\mathcal{D}_{x}}\left[\sup_{w\in\mathbb{R}^{k}}\left|\widetilde {d}(w,w^{*})-d(w,w^{*})\right|>\varepsilon\right]\leq e^{-\Omega(ne^{2})}.\]

Proof.: The proof is inspired by Theorem 11.2 in [20], with modifications to our setting.

Let Since \(f_{w}(x)\) is bounded, for any fixed \(w\), the Chernoff bound gives

\[\Pr\Bigl{[}\left|\widetilde{d}(w,w^{*})-d(w,w^{*})\right|>\alpha\Bigr{]}\leq e ^{-2n\alpha^{2}}.\] (11)

for any \(\alpha>0\). The challenge lies in constructing a "net" to be able to union bound over \(\mathbb{R}^{k}\) without assuming any bound on \(w\) or the covariate \(x\). A net is a partitioning of an space, where within each part, points are close together in some way. In this case, we construct a net using what we will refer to as "ghost" samples.

Ghost samples.First, we construct a "ghost" dataset \(D_{x}^{\prime}\) consisting of \(n\) new samples, drawn i.i.d. \(\{x_{i}^{\prime}\}_{i\in[n]}\) of \(\mathcal{D}_{x}\). This gives another metric \(\widetilde{d}^{\prime}(\cdot,\cdot)\). Instead of directly considering the distance between \(\widetilde{d}(w,w^{*})\) and \(d(w,w^{*})\), it is sufficient to consider the difference between \(\widetilde{d}(w,w^{*})\) and \(\widetilde{d}^{\prime}(w,w^{*})\) i.e.,

\[\Pr\biggl{[}\sup_{w}\Bigl{|}d(w,w^{*})-\widetilde{d}(w,w^{*})\Bigr{|}> \varepsilon\biggr{]}\leq 2\Pr\biggl{[}\sup_{w}\Bigl{|}\widetilde{d}(w,w^{*})- \widetilde{d}^{\prime}(w,w^{*})\Bigr{|}>\varepsilon/2\biggr{]}.\] (12)

To see this, let \(\bar{w}\) maximize \(\widetilde{d}(w,w^{*})-\widetilde{d}^{\prime}(w,w^{*})\). Since \(\bar{w}\) and \(\{x_{i}^{\prime}\}_{i\in[n]}\) are independent, by the Chernoff bound,

\[\Pr\Bigl{[}\Bigl{|}\widetilde{d}^{\prime}(\bar{w},w^{*})-d(\bar{w},w^{*}) \Bigr{|}>\varepsilon/2|D_{x}\Bigr{]}\leq e^{-n\varepsilon^{2}/2}\leq 1/2.\]for any \((D_{x},\bar{w})\) and large enough \(n\). Thus,

\[\Pr\Bigl{[}\Bigl{|}\widetilde{d}^{\prime}(\bar{w},w^{*})- \widetilde{d}(\bar{w},w^{*})\Bigr{|}>\varepsilon/2\Bigr{]} \geq\Pr\Bigl{[}\Bigl{|}d(\bar{w},w^{*})-\widetilde{d}(\bar{w},w^{* })\Bigr{|}>\varepsilon\cap\Bigl{|}d(\bar{w},w^{*})-\widetilde{d}^{\prime}( \bar{w},w^{*})\Bigr{|}<\varepsilon/2\Bigr{]}\] \[=\operatorname*{\mathbb{E}}_{D_{x}}\Bigl{[}1_{\{|d(\bar{w},w^{*})- \widetilde{d}(\bar{w},w^{*})|>\varepsilon\}}\Pr\Bigl{[}\Bigl{|}d(\bar{w},w^{*} )-\widetilde{d}^{\prime}(\bar{w},w^{*})\Bigr{|}<\varepsilon/2|D_{x}\Bigr{]} \Bigr{]}\] \[\geq(1-1/2)\Pr\Bigl{[}\Bigl{|}d(w,w^{*})-\widetilde{d}(w,w^{*}) \Bigr{|}>\varepsilon\Bigr{]},\]

which implies (12).

Symmetrization.Since \(D_{x}\) and \(D_{x}^{\prime}\) each have \(n\) independent samples, we could instead draw the datasets by first sampling \(2n\) elements \(x_{1},\ldots,x_{2n}\) from \(\mathcal{D}_{x}\), then randomly partition this sample into two equal datasets. Let \(s_{i}\in\{\pm 1\}\) so \(s_{i}=1\) if \(z_{i}\) lies in \(D_{x}^{\prime}\) and \(-1\) if it lies in \(D_{x}\). Then

\[\widetilde{d}^{\prime}(\bar{w},w^{*})-\widetilde{d}(\bar{w},w^{*})=\frac{1}{n }\sum_{i=1}^{2n}s_{i}\cdot d_{TV}(p_{w}(y|x_{i}),p_{w^{*}}(y|x_{i})).\]

For a fixed \(w\) and \(x_{1},\ldots,x_{2n}\), the random variables \((s_{1},\ldots,s_{2n})\) are a permutation distribution, so negatively associated. Then the variables \(s_{i}\cdot d_{TV}(p_{w}(y|x_{i}),p_{w^{*}}(y|x_{i}))\) are monotone functions of \(s_{i}\), so also negatively associated. They are also bounded in \([-1,1]\). Hence we can apply a Chernoff bound:

\[\Pr\Bigl{[}\Bigl{|}\widetilde{d}^{\prime}(\bar{w},w^{*})-\widetilde{d}(\bar{w},w^{*})\Bigr{|}>\varepsilon\Bigr{]}<e^{-n\varepsilon^{2}/2}\] (13)

for any fixed \(w\).

Constructing a net.We partition \(\mathbb{R}^{k}\) the space of \(w\) s.t. if \(w,w^{\prime}\) are in the same partition then,

\[\bigl{|}d_{TV}(p_{w}(y|x),p_{w^{*}}(y|x))-d_{TV}(p_{w^{\prime}}(y|x),p_{w^{*}} (y|x))\bigr{|}<\alpha.\]

for each \(x\) in the dataset \(x_{1},\ldots,x_{2n}\). Then take the intersection of all \(2n\) partitions to construct a net over \(\mathbb{R}^{k}\).

As the total variation distance is a unimodal function of \(x_{i}\cdot w-x_{i}\cdot w^{*}\), we partition \(w\) the sets

\[\{w:d_{TV}(p_{w}(y|x_{i}),p_{w^{*}}(y|x_{i}))\in[j\alpha,(j+1)\alpha]\]

where \(j\) goes from \(0\) to \(1/\alpha-1\). So the space of \(w\), \(\mathbb{R}^{k}\) is partitioned by \(2n\) sets of \(1/\alpha\) parallel hyper-planes. Then the total number of cells is at most

\[\sum_{i=0}^{k}\binom{2n}{i}(2/\alpha)^{i}\leq 2\biggl{(}\frac{4en}{\alpha k} \biggr{)}^{k}\]

We define a net \(N\) by choosing one representative of each cell in the partition, so \(|N|\leq e^{2k\log\frac{\alpha}{\alpha k}}\). By (13),

\[\Pr\biggl{[}\max_{w\in N}\biggl{|}\widetilde{d}^{\prime}(\bar{w},w^{*})- \widetilde{d}(\bar{w},w^{*})\Bigr{|}>\varepsilon\biggr{]}<|N|e^{-n\varepsilon ^{2}/2}\leq e^{2k\log\frac{n}{\alpha k}-\varepsilon^{2}n/2}.\]

Finally, for any \(w\in\mathbb{R}^{d}\) let \(\bar{w}\in N\) be the representative of its cell. By definition of the cells,

\[|d_{TV}(p_{w}(y|x_{i}),p_{w^{*}}(y|x_{i}))-d_{TV}(p_{\bar{w}}(y|x_{i}),p_{w^{ *}}(y|x_{i}))|<\alpha\]

for all \(i\in[2n]\). Thus

\[\Bigl{|}\Bigl{(}\widetilde{d}^{\prime}(w,w^{*})-\widetilde{d}(w,w^{*})\Bigr{)} -\Bigl{(}\widetilde{d}^{\prime}(\bar{w},w^{*})-\widetilde{d}(\bar{w},w^{*}) \Bigr{|}\leq\Bigl{|}\widetilde{d}(w,w^{*})-\widetilde{d}(\bar{w},w^{*})\Bigr{|} +\Bigl{|}\widetilde{d}^{\prime}(w,w^{*})-\widetilde{d}^{\prime}(\bar{w},w^{*}) \Bigr{|}\leq 2\alpha\]

and so

\[\Pr\biggl{[}\sup_{w\in\mathbb{R}^{d}}\Bigl{|}\widetilde{d}^{\prime}(w,w^{*})- \widetilde{d}(w,w^{*})\Bigr{|}>\varepsilon\biggr{]}\leq\Pr\biggl{[}\max_{w \in N}\biggl{|}\widetilde{d}^{\prime}(w,w^{*})-\widetilde{d}(w,w^{*})\Bigr{|}> \varepsilon-2\alpha\biggr{]}\leq e^{2k\log\frac{\alpha}{\alpha k}-(\varepsilon-2 \alpha)^{2}n/2}\]

Setting \(\alpha=\varepsilon/4\), we have that

\[n\lesssim\frac{1}{\varepsilon^{2}}k\log\frac{1}{\varepsilon}\]

suffices for

\[\Pr\biggl{[}\max_{w\in\mathbb{R}^{k}}\widetilde{d}^{\prime}(w,w^{*})- \widetilde{d}(w,w^{*})>\varepsilon\biggr{]}<e^{-\Omega(\varepsilon^{2}n)}.\]ReLU Activation with Scalar \(y\)

In this section, we consider the model of

\[y=\phi(w^{*}\cdot x+\eta),\quad\eta\sim\mathcal{N}(0,1),\]

where \(w^{*},x\in\mathbb{R}^{k}\), \(y,\eta\in\mathbb{R}\). We are given samples \((x,y)\in\mathbb{R}^{k}\times\mathbb{R}\), and want to estimate a \(\widehat{w}\) that estimates the distribution of \(y\) in TV.

The most challenging aspect of the ReLU setting is that we do not have an expression for the TV suffered by the MLE, such as Lemma 4.2 in the linear case. This forces us to directly analyze the log-likelihood.

For a fixed \(x,w,\), the expectation of the log-likelihood ratio over \(y\) is

\[\operatorname*{\mathbb{E}}_{y}\!\left[\log\frac{p_{w}(y\mid x)}{p_{w^{*}}(y \mid x)}\right]=-d_{KL}(w^{*}\|w)\leq-2d_{TV}^{2}(w^{*},w),\]

where the last inequality is via Pinsker's inequality. This equation implies that if \(w\) is \(\varepsilon\)-far from \(w^{*}\), then the expected log-likelihood ratio(LLR) is \(<-2\varepsilon^{2}\). By definition, the MLE has a non-negative LLR. Hence, if the empirical LLR is close to the expectation, this would imply that the MLE has small TV.

However, we only receive a single sample of \(y\) per \(x\). For a fixed \(w\), we can prove a Bernstein inequality, showing that given \(1/\varepsilon^{2}\log(1/\delta)\) samples, the empirical LLR is \(<-\varepsilon^{2}\) for \(w\) that are \(\varepsilon\)-far.

**Lemma B.1**.: _Let \(p_{1},\ldots,p_{n}\) and \(q_{1},\ldots,q_{n}\) be distributions with \(\operatorname*{\mathbb{E}}_{i}[d_{TV}(p_{i},q_{i})]\geq\varepsilon\), where we use the uniform measure on \(i\in[n]\). Let \(x_{i}\sim p_{i}\) for \(i\in[n]\). Then w.p. \(1-\delta\), \(\operatorname*{\mathbb{E}}_{i}[\log\frac{q_{i}(x_{i})}{p_{i}(x_{i})}]\leq- \frac{\varepsilon^{2}}{4}\) for \(n\geq O\big{(}\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta}\big{)}\)._

The proof of this Lemma, as well as other Lemmas in this section, can be found in Appendix B.1.

In order to extend this to _all_\(w\in\mathbb{R}^{k}\) that are \(\varepsilon\)-far, we will construct a cover over \(\mathbb{R}^{k}\) depending on the values the log-likelihood ratio can take, and then apply the Bernstein inequality to each element in the cover.

In order to construct the cover, we first show that the log-likelihood ratio is bounded above by the magnitude of noise in \(y\). For ease of notation, for a fixed \(x\in\mathbb{R}^{k}\), and each \(w\in\mathbb{R}^{k}\), define

\[\theta=\langle x,w\rangle\in\mathbb{R},\]

and let \(\theta^{*}=\langle x,w^{*}\rangle\).Similar to the notation for \(w\), for each \(\theta\in\mathbb{R}\), define \(p_{\theta}\) as the distribution of \(\phi(\theta+\eta)\) for \(\eta\sim N(0,1)\). Define the log likelihood ratio

\[\gamma_{\theta}(y):=\log\frac{p_{\theta}(y|x)}{p_{\theta^{*}}(y|x)}.\]

The following Lemma states that for a fixed datapoint \((x,y)\), the log-likelihood ratio is bounded by the noise in \(y\):

**Lemma B.2**.: _For any \(y=\phi(\theta+\eta)\),_

\[\gamma_{\theta}(y)=\begin{cases}\log\Phi(-\theta)-\log\Phi(-\theta^{*})&\text {if }y=0\\ \eta(\theta-\theta^{*})-\frac{(\theta-\theta^{*})^{2}}{2}&\text{if }y>0\end{cases}\]

_and therefore, for all \(y\),_

\[\gamma_{\theta}(y)\leq\left|\eta\right|^{2}/2.\]

Now, as \(\gamma\) is bounded above by \(\frac{\left|\eta\right|^{2}}{2}\), and it is concave wrt \(\theta\), the following Lemma shows that we can partition \(\theta\) into \(O\big{(}\frac{A}{\varepsilon}\big{)}\) intervals, such that in each interval, \(\gamma\) changes by atmost \(\varepsilon\), or is very negative, i.e., \(\gamma<-A\).

**Lemma B.3** (One-dimensional net).: _Let \(A>B^{2}>1\). There exists a partition of \(\mathbb{R}\) into \(O(A/\varepsilon)\) intervals such that, for each interval \(I\) in the partition and every \(y=\phi(\theta^{*}+\eta)\) with \(\left|\eta\right|\leq B\), one of the following holds:_* _For all_ \(\theta\in I\)_,_ \(\gamma_{\theta}(y)\leq-A\)__
* _For all_ \(\theta,\theta^{\prime}\in I\)_,_ \(|\gamma_{\theta}(y)-\gamma_{\theta^{\prime}}(y)|\leq\varepsilon\)__

Using Lemma B.2 and Lemma B.3, we can form a uniform bound, such that all \(w\) that are \(\varepsilon\)-far from \(w^{*}\) in distribution will have log-likelihood ratio smaller than \(-\frac{\varepsilon^{2}}{4}\) on the training set. With some additional arguments, we can now show that as the MLE has positive log-likelihood ratio, it has small empirical TV.

**Lemma B.4**.: _Let \(x_{1},\ldots,x_{n}\) be fixed, and \(y_{i}\sim\phi(x_{i}^{T}w^{*}+\eta_{i})\) for \(\eta_{i}\sim\mathcal{N}(0,1)\). For \(n\geq\frac{1}{\varepsilon^{2}}k\log\frac{1}{\varepsilon}\), the MLE \(\widehat{w}\) satisfies_

\[\widetilde{d}(\widehat{w},w^{*})\leq\varepsilon.\]

This sample complexity guarantees that the MLE is good for the set of empirical \(x_{i}\sim\mathcal{D}_{x}\), and we need to extend this to the expectation over \(x\sim\mathcal{D}_{x}\), for which we use a Lemma similar to Lemma 4.3 in the linear case, and this completes the proof.

A straight forward combination of Lemma 4.3 and Lemma B.4 gives the following Theorem.

**Theorem B.5**.: _Let \(y=\phi\big{(}x^{T}w^{*}+\eta\big{)}\), for \(w^{*}\in\mathbb{R}^{k}\), \(x\sim\mathcal{D}_{x}\), and \(\eta\sim\mathcal{N}(0,1)\). Then for a sufficiently large constant \(C>0\),_

\[n=C\cdot\frac{k}{\varepsilon^{2}}\log\frac{1}{\varepsilon}\]

_samples of \(\left\{(y_{i},x_{i})\right\}_{i=1}^{n}\) suffices to guarantee that the MLE \(\widehat{w}\) satisfies_

\[d(\widehat{w},w^{*})\leq\varepsilon.\]

### Proofs

**Lemma B.1**.: _Let \(p_{1},\ldots,p_{n}\) and \(q_{1},\ldots,q_{n}\) be distributions with \(\mathbb{E}_{i}[d_{TV}(p_{i},q_{i})]\geq\varepsilon\), where we use the uniform measure on \(i\in[n]\). Let \(x_{i}\sim p_{i}\) for \(i\in[n]\). Then w.p. \(1-\delta\), \(\mathbb{E}_{i}[\log\frac{q_{i}(x_{i})}{p_{i}(x_{i})}]\leq-\frac{\varepsilon^{ 2}}{4}\) for \(n\geq O\big{(}\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta}\big{)}\)._

Proof.: Define \(\gamma_{i}(x)=\log\frac{q_{i}(x)}{p_{i}(x)}\) and \(a_{i}(x):=\max(\gamma_{i}(x),-2)\). We have that

\[\mathop{\mathbb{E}}_{i,x}[\gamma_{i}(x)]=-\mathop{\mathbb{E}}_{i}[d_{KL}(p_{ i},q_{i})]\leq-\mathop{\mathbb{E}}_{i}[2d_{TV}(p_{i},q_{i})^{2}]\leq-2\varepsilon^{2}\]

and want to show that \(\mathbb{E}_{i}[\gamma_{i}(x)]\leq-\varepsilon^{2}/4\) with high probability. Note that \(a_{i}(x)\geq\gamma_{i}(x)\), so it suffices to show \(\mathbb{E}_{i}[a_{i}(x)]\leq-\varepsilon^{2}/4\). We will do this with Bernstein's inequality, for which we need bounds on the moments of \(a_{i}(x)\).

To simplify notation, fix a particular \(i\) and consider \(p=p_{i},q=q_{i},a=a_{i}\), and \(x\sim p\).

For a random variable \(v\), define \(v_{+},v_{-}\) to be the positive/negative parts of \(v\), respectively, so \(v=v_{-}+v_{+}\). Define \(\Delta(x)=\frac{q(x)}{p(x)}-1\). We have that \(\mathbb{E}_{x\sim p}[\Delta(x)]=0\), and

\[\mathop{\mathbb{E}}_{x\sim p}[\Delta_{+}(x)]=\mathop{\mathbb{E}}_{x\sim p}[- \Delta_{-}(x)]=d_{TV}(p,q).\] (14)

Now, consider the function \(b(z):=\max(\log(1+z),-2)-z\). This function is nonpositive over \(z\geq-1\), and \(b(z)\leq-z^{2}/2\) for \(z\leq 0\). Since

\[a(x)=b(\Delta(x))+\Delta(x)\]

and \(\mathbb{E}_{x\sim p}[\Delta(x)]=0\), \(\mathbb{E}_{x\sim p}[-a(x)]=\mathbb{E}_{x\sim p}[-b(\Delta(x))]\). This means

\[\mathop{\mathbb{E}}_{x\sim p}[-a(x)] =\mathop{\mathbb{E}}_{x\sim p}[-b(\Delta(x))]\geq\mathop{ \mathbb{E}}_{x\sim p}[-b(\Delta(x))1_{\Delta(x)<0}]\] \[\geq\mathop{\mathbb{E}}_{x\sim p}[\Delta_{-}^{2}(x)/2]\]

or by (14),

\[\mathop{\mathbb{E}}_{x}[-a(x)]\geq\mathop{\mathbb{E}}_{x}[\Delta_{-}^{2}(x)/2] \geq\frac{1}{2}d_{TV}(p,q)^{2}.\] (15)Bounding the positive higher moments.We have that \(p(x)e^{a(x)}=\max(q(x),e^{-2}p(x))\) so

\[\mathbb{E}[e^{a(x)}] =\int\max(q(x),e^{-2}p(x))dx\] \[\leq 1+e^{-2}\Pr[a(x)=-2].\]

In the following, we use that \(e^{t}\geq 1+t\) for all \(t\), as well as \(e^{t}=1+t+\sum_{k=2}^{\infty}\frac{1}{k!}t^{k}\). Therefore

\[1+e^{-2}\Pr[a(x)=-2]\] \[\geq\mathbb{E}[e^{a(x)}] =\mathbb{E}[e^{a_{-}(x)}1_{a(x)\leq 0}+e^{a_{+}(x)}1_{a(x)>0}]\] \[\geq\mathbb{E}[1+(a_{-}(x)+a_{+}(x))+\sum_{k=2}^{\infty}\frac{1}{ k!}a_{+}^{k}(x)]\] \[=1+\mathbb{E}[a(x)]+\sum_{k=2}^{\infty}\frac{1}{k!}\,\mathbb{E}[a_ {+}^{k}(x)]\]

so

\[\sum_{k=2}^{\infty}\frac{1}{k!}\,\mathbb{E}[a_{+}^{k}(x)]\leq\mathbb{E}[-a(x) ]+e^{-2}\Pr[a(x)=-2].\]

We now show that the \(\Pr[a(x)=-2]\) is smaller than the \(\mathbb{E}[-a(x)]\) term, by relating to \(-b\). When \(a(x)=-2\), \(\Delta(x)\leq-1+1/e^{2}\), and \(b(\Delta(x))=-2-\Delta(x)\leq-1\). Since \(-b(\Delta(x))\) is non-negative, and at least \(1\) whenever \(a(x)=-2\),

\[\mathbb{E}[-a(x)]=\mathbb{E}[-b(\Delta(x))]\geq\Pr[a(x)=-2]\cdot 1\]

and hence

\[\sum_{k=2}^{\infty}\frac{1}{k!}\,\mathbb{E}[a_{+}^{k}(x)]\leq(1+\frac{1}{e^{2 }})\,\mathbb{E}[-a(x)].\] (16)

In particular, \(\mathbb{E}[a_{+}^{k}(x)]\leq 2k!\,\mathbb{E}[-a(x)]\) for all \(k\geq 2\).

Bounding the second moment of \(a\).We have that

\[\mathbb{E}[a(x)^{2}]=\mathbb{E}[a_{+}^{2}(x)+a_{-}^{2}(x)]\]

and \(\mathbb{E}[a_{+}^{2}(x)]\leq 4\,\mathbb{E}[-a(x)]\) by (16). We now bound \(\mathbb{E}[a_{-}^{2}(x)]\). Note that \(|a_{-}(x)|\leq\frac{2}{1-1/e^{2}}|\Delta_{-}(x)|\) by the construction of \(a\). Therefore

\[a_{-}^{2}(x)\leq 6\Delta_{-}^{2}(x)\]

and so by (15),

\[\mathbb{E}[a_{-}^{2}(x)]\leq 6\,\mathbb{E}[\Delta_{-}^{2}(x)]\leq 12\,\mathbb{E} [-a(x)].\]

Thus

\[\mathbb{E}[a^{2}(x)]\leq 16\,\mathbb{E}[-a(x)].\] (17)

Bernstein Concentration.Now we can apply Bernstein's inequality (Theorem 2.10 of [8]).

We apply the theorem to \(X_{i}:=a_{i}(x_{i})\), which are independent. The theorem uses that

\[\sum_{i=1}^{n}\mathbb{E}[X_{i}^{2}]=n\,\underset{i,x}{\mathbb{E}}[a_{i}(x)^{2 }]\leq 16n\,\underset{i,x}{\mathbb{E}}[-a_{i}(x)]=:v\]

by (17), and since

\[\sum_{i=1}^{n}\mathbb{E}[(X_{i})_{+}^{k}]=n\,\underset{i,x}{\mathbb{E}}[a_{i, +}(x)^{k}]\leq 2k!\,\underset{i,x}{\mathbb{E}}[-a_{i}(x)]\leq\frac{1}{2}vk!\]

so we can set \(c=1\). Applying the theorem, we have that \(S=\sum a_{i}(x_{i})-\mathbb{E}[a_{i}(x_{i})]\) satisfies

\[S\leq\sqrt{2v\log\frac{1}{\delta}}+\log\frac{1}{\delta}\]with probability \(1-\delta\). Plugging in \(v\) and rescaling by \(n\), with probability \(1-\delta\) we have:

\[\operatorname*{\mathbb{E}}_{i}[a_{i}(x_{i})]\leq\operatorname*{\mathbb{E}}_{i,x}[a _{i}(x)]+O(1)\cdot\sqrt{\operatorname*{\mathbb{E}}[-a(x)]\frac{1}{n}\log\frac{1 }{\delta}}+\frac{1}{n}\log\frac{1}{\delta}\]

By our assumption on \(n\) for a sufficiently large constant in the big \(O\), this implies

\[\operatorname*{\mathbb{E}}_{i}[a_{i}(x_{i})]\leq\operatorname*{\mathbb{E}}_{i,x}[a_{i}(x)]+\frac{1}{6}\varepsilon\sqrt{\operatorname*{\mathbb{E}}_{i,x}[-a_ {i}(x)]}+\varepsilon^{2}/8\]

Since by (15), \(\varepsilon\leq\sqrt{\operatorname*{\mathbb{E}}_{i}[d_{TV}(p_{i},q_{i})^{2}]}\leq \sqrt{\operatorname*{\mathbb{E}}_{i,x}[-2a_{i}(x)]}\), this means

\[\operatorname*{\mathbb{E}}_{i}[\gamma_{i}(x_{i})]\leq\operatorname*{ \mathbb{E}}_{i}[a_{i}(x_{i})] \leq(-1+\frac{\sqrt{2}}{6})\operatorname*{\mathbb{E}}_{i,x}[-a_ {i}(x)]+\varepsilon^{2}/8\] \[\leq(-1+\frac{\sqrt{2}}{6})\frac{1}{2}\varepsilon^{2}+\frac{1}{8} \varepsilon^{2}\] \[\leq-\frac{1}{4}\varepsilon^{2}\]

as desired. 

**Lemma B.2**.: _For any \(y=\phi(\theta+\eta)\),_

\[\gamma_{\theta}(y)=\begin{cases}\log\Phi(-\theta)-\log\Phi(-\theta^{*})&\text {if }y=0\\ \eta(\theta-\theta^{*})-\frac{(\theta-\theta^{*})^{2}}{2}&\text{if }y>0\end{cases}\]

_and therefore, for all \(y\),_

\[\gamma_{\theta}(y)\leq\left|\eta\right|^{2}/2.\]

Proof.: Let \(\Phi(x)\) be the cdf of a standard Gaussian. For \(y>0\),

\[\gamma_{\theta}(y) =\frac{1}{2}((y-\theta^{*})^{2}-(y-\theta)^{2})\] \[=\frac{1}{2}(\eta^{2}-(\eta+\theta^{*}-\theta)^{2})\] \[=\eta(\theta-\theta^{*})-\frac{(\theta-\theta^{*})^{2}}{2}\]

Thus:

\[\gamma_{\theta}(y)=\begin{cases}\log\Phi(-\theta)-\log\Phi(-\theta^{*})&\text {if }y=0\\ \eta(\theta-\theta^{*})-\frac{(\theta-\theta^{*})^{2}}{2}&\text{if }y>0\end{cases}\]

Now suppose \(\left|\eta\right|\leq B\). We can upper bound \(\gamma_{\theta}(y)\) for all \(\theta\):

* If \(y=0\), then \(-\theta^{*}\geq-B\), so \[\gamma_{\theta}(0)\leq-\log\Phi(-\theta^{*})\leq-\log e^{-B^{2}/2}=B^{2}/2.\]
* If \(y>0\), then \[\gamma_{\theta}(y)=(\theta-\theta^{*})\eta-\frac{(\theta-\theta^{*})^{2}}{2} \leq\eta^{2}/2\leq B^{2}/2.\]

as desired. 

**Lemma B.3** (One-dimensional net).: _Let \(A>B^{2}>1\). There exists a partition of \(\mathbb{R}\) into \(O(A/\varepsilon)\) intervals such that, for each interval \(I\) in the partition and every \(y=\phi(\theta^{*}+\eta)\) with \(\left|\eta\right|\leq B\), one of the following holds:_

* _For all_ \(\theta\in I\)_,_ \(\gamma_{\theta}(y)\leq-A\)__
* _For all_ \(\theta,\theta^{\prime}\in I\)_,_ \(\left|\gamma_{\theta}(y)-\gamma_{\theta^{\prime}}(y)\right|\leq\varepsilon\)__Proof.: To define our partition, we actually define two partitions, depending on whether \(y=0\), then intersect them for our final partition.

First, consider \(y=0\). By Lemma B.2, \(\gamma_{\theta}(0)\) is monotonically decreasing in \(\theta\), from its maximum of at most \(B^{2}/2\). We can thus define a partition \(P_{1}\) consisting of intervals of the form \(I_{i}:=\{\theta\mid\gamma_{\theta}(0)\in(B^{2}/2-(i+1)\varepsilon,B^{2}/2-i \varepsilon)\}\), for \(i\in\{0,1,\ldots,(A+B^{2}/2)/\varepsilon\}\), plus a special interval \(I^{\prime}\) of \(\{\theta\mid\gamma_{\theta}(0)<-A\}\). When \(y=0\), this partition satisfies the desired conclusion to the lemma: \(|\gamma_{\theta}(0)-\gamma_{\theta^{\prime}}(0)|\leq\varepsilon\) for all \(\theta,\theta^{\prime}\in I_{i}\), while \(\gamma_{\theta}(0)<-A\) for \(\theta\in I^{\prime}\). Call this partition \(P_{0}\), which has size \(O(A/\varepsilon)\).

Second, consider \(y>0\). Define \(R=\sqrt{2A}+B\). Note that \(R^{2}\lesssim A\) and \((R-B)^{2}\geq 2A\). Therefore for \(|\theta-\theta^{*}|\geq R\),

\[\gamma_{\theta}(y)\leq-\frac{1}{2}\max(0,|\theta-\theta^{*}|-\eta)^{2}\leq-A.\]

Consider any \(\theta,\theta^{\prime}\in[\theta^{*}-R,\theta^{*}+R]\) with \(\alpha:=|\theta-\theta^{\prime}|\). We have

\[|\gamma_{\theta}(y)-\gamma_{\theta^{\prime}}(y)| \leq|\eta(\theta-\theta^{\prime})|+\frac{1}{2}|(\theta^{\prime}- \theta^{*})^{2}-(\theta-\theta^{*})^{2}\big{|}\] \[\leq B\alpha+\frac{1}{2}|(\theta^{\prime}-\theta)(-2\theta^{*}+( \theta^{\prime}+\theta))|\] \[\leq B\alpha+\frac{1}{2}\alpha(2R)=\alpha(B+R).\]

Thus, for \(\alpha=\frac{\varepsilon}{2R}\), this is at most \(\varepsilon\). If we partition \([\theta^{*}-R,\theta^{*}+R]\) into length-\(\alpha\) intervals, we get a size \(O(R^{2}/\varepsilon)=O(A/\varepsilon)\) partition \(P_{1}\) of \(\mathbb{R}\) that has the desired property for all \(y>0\).

Our final partition is defined by all endpoints in either \(P_{0}\) and \(P_{1}\). This has size \(O(A/\varepsilon)\), and within each interval the conclusion holds for both \(y=0\) and \(y>0\), as needed.

**Lemma B.4**.: _Let \(x_{1},\ldots,x_{n}\) be fixed, and \(y_{i}\sim\phi(x_{i}^{T}w^{*}+\eta_{i})\) for \(\eta_{i}\sim\mathcal{N}(0,1)\). For \(n\geq\frac{1}{\varepsilon^{2}}k\log\frac{1}{\varepsilon}\), the MLE \(\widehat{w}\) satisfies_

\[\widetilde{d}(\widehat{w},w^{*})\leq\varepsilon.\]

Proof.: For any \(w\in\mathbb{R}^{k}\), and a sample \((x_{i},y_{i})\), let \(p_{w}(y|x_{i})\) be the conditional distribution of \(y=\phi(\langle x_{i},w\rangle+\eta)\), and let \(\gamma_{i,w}\) be the log-likelihood ratio between \(w\) and \(w^{*}\) on this sample:

\[\gamma_{i,w}(y):=\log\frac{p_{w}(y|x_{i})}{p_{w^{*}}(y|x_{i})}.\]

Then

\[\mathop{\mathbb{E}}_{y}[\gamma_{i,w}(y)]=-d_{KL}(p_{i,w^{*}}(y|x_{i})||p_{i,w }(y|x_{i})).\]

Define

\[d_{KL}(w^{*},w):=\frac{1}{n}\sum_{i=1}^{n}d_{KL}(p_{i,w^{*}}(y|x_{i})||p_{i,w }(y|x_{i})).\]

Concentration.From Lemma B.1, we see that if \(\widetilde{d}(w^{*},w)\geq\varepsilon\), then for \(n\geq O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta})\),

\[\overline{\gamma}_{w}:=\frac{1}{n}\sum_{i=1}^{n}\gamma_{i,w}(y_{i})<-\frac{ \varepsilon^{2}}{4},\] (18)

with probability \(1-\delta\).

Of course, whenever \(\overline{\gamma}_{w}<0\), the likelihood under \(w^{*}\) is larger than the likelihood under \(w\). Thus, for each _fixed_\(w\) with \(\widetilde{d}(w^{*},w)\geq\varepsilon\), maximizing likelihood would prefer \(w^{*}\) to \(w\) with probability \(1-\delta\) if \(n\geq O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta})\).

Nothing above is specific to our ReLU-based distribution. But to extend to the MLE over all \(w\), we need to build a net using properties of our distribution.

Building a net.First, with high probability, \(|\eta_{i}|\leq B=O(\sqrt{\log n})\) for all \(i\). Suppose this happens. For each \(i\), by an abuse of notation, let \(\gamma_{i,w}(y)=\gamma_{\langle x_{i},w\rangle}(y)\) where the value of \(\theta^{*}\) when considering \(i\) is \(\langle x_{i},w^{*}\rangle\). By Lemma B.2,

\[\gamma_{i,w}(y_{i})\leq B^{2}/2\]

for all \(i\). Let \(A=O(n\log n)>nB^{2}\). By Lemma B.3, for each \(i\in[n]\), there exists a partition \(P_{i}\) of \(\mathbb{R}\) into \(O(A/\varepsilon^{2})\) intervals, such that for interval \(I\in P_{i}\), and any \(w,w^{\prime}\) with \(x_{i}^{T}w,x_{i}^{T}w^{\prime}\in I\), either

\[|\gamma_{i,w}(y_{i})-\gamma_{i,w^{\prime}}(y_{i})|\leq\varepsilon^{2}/2\] (19)

or \(\gamma_{i,w}(y_{i})<-A\).

These individual partitions \(P_{i}\) on \(\langle x_{i},w\rangle\) induce a partition \(P\) on \(\mathbb{R}^{k}\), where \(w,w^{\prime}\) lie in the same cell of \(P\) if \(\langle x_{i},w\rangle\) and \(\langle x_{i},w^{*}\rangle\) are in the same cell of \(P_{i}\) for all \(i\in[n]\). Since \(P\) is defined by \(n\) sets of \(O\big{(}\frac{A}{\varepsilon^{2}}\big{)}\) parallel hyperplanes in \(\mathbb{R}^{k}\), the number of cells in \(P\) is:

\[2\bigg{(}\frac{2Aen}{\varepsilon^{2}k}\bigg{)}^{k}.\]

We choose a net \(\mathcal{N}\) to contain, for each cell in \(P\), the \(w\) in the cell maximizing \(\widetilde{d}(w^{*},w)\). This has size

\[\log|\mathcal{N}|\lesssim k\log\frac{n}{\varepsilon}.\]

By (18), for our \(n\geq O\big{(}\frac{1}{\varepsilon^{2}}k\log\frac{k}{\varepsilon}\big{)}\), we have with high probability that \(\overline{\gamma}_{w}\leq-\frac{\varepsilon^{2}}{4}\), for all \(w\in\mathcal{N}\) with \(\widetilde{d}(w^{*},w)\geq\varepsilon\). Suppose that both this happens, and \(|\eta_{i}|\leq B\) for all \(i\). We claim that the MLE \(\widehat{w}\) must have \(\widetilde{d}(w^{*},\widehat{w})<\varepsilon\).

Consider any \(w\in\mathbb{R}^{d}\) with \(\widetilde{d}_{TV}(w^{*},w)\geq\varepsilon\). Let \(w^{\prime}\in\mathcal{N}\) lie in the same cell of \(P\). By our choice of \(\mathcal{N}\), we know \(\widetilde{d}_{TV}(w^{*},w^{\prime})\geq\widetilde{d}_{TV}(w^{*},w)\geq\varepsilon\), so \(\overline{\gamma}_{w^{\prime}}\leq-\varepsilon^{2}\). Now we consider two cases. In the first case, there exists \(i\) with \(\gamma_{i,w}(y_{i})<-A\). Then

\[\overline{\gamma}_{w}=\frac{1}{n}\sum_{i}\gamma_{i,w}(y_{i})\leq-\frac{A}{n} +B^{2}/2<0.\]

Otherwise, by (19),

\[\overline{\gamma}_{w}\leq\overline{\gamma}_{w^{\prime}}+|\overline{\gamma}_{w }-\overline{\gamma}_{w^{\prime}}|\leq-\varepsilon^{2}+\max_{i}\lvert\gamma_{ i,w}(y_{i})-\gamma_{i,w^{\prime}}(y_{i})\rvert\leq-\varepsilon^{2}/2.\]

In either case, \(\overline{\gamma}_{w}<0\) and the likelihood under \(w^{*}\) exceeds that under \(w\). Hence the MLE \(\widehat{w}\) must have \(\widetilde{d}(w^{*},\widehat{w})\leq\varepsilon\). 

**Theorem B.5**.: _Let \(y=\phi\big{(}x^{T}w^{*}+\eta\big{)}\), for \(w^{*}\in\mathbb{R}^{k}\), \(x\sim\mathcal{D}_{x}\), and \(\eta\sim\mathcal{N}(0,1)\). Then for a sufficiently large constant \(C>0\),_

\[n=C\cdot\frac{k}{\varepsilon^{2}}\log\frac{1}{\varepsilon}\]

_samples of \(\left\{(y_{i},x_{i})\right\}_{i=1}^{n}\) suffices to guarantee that the MLE \(\widehat{w}\) satisfies_

\[d(\widehat{w},w^{*})\leq\varepsilon.\]

Proof.: Let \(D_{x}\) denote the dataset \(\{x_{i}\}_{i\in[n]}\) that is used to find the MLE. Notice that the MLE is found using this finite subset, but we would like to make a claim about \(\mathcal{D}_{x}\) without making any parametric or simplifying assumptions on the distribution \(\mathcal{D}_{x}\).

An application of Lemma 4.3 tells us that with probability \(1-e^{-\Omega(ne^{2})}\), the expectation over the distribution \(\mathcal{D}_{x}\) and the dataset \(D_{x}\) are within \(\varepsilon/2\) of one another:

\[d(\widehat{w},w^{*})\leq\widetilde{d}(\widehat{w},w^{*})+\varepsilon/2.\]

Now, all we need to show is that the MLE has a small TV distance on the finite dataset, and Lemma B.4 tells us that with probability \(1-e^{-\Omega(ne^{2})}\),

\[\widetilde{d}(\widehat{w},w^{*})\leq\varepsilon/2.\]

Substituting in the above inequality, we get \(d(\widehat{w},w^{*})\leq\varepsilon\)ReLU Activations with \(d>1\), Unknown Covariance

We recommend the reader review Appendix B, which contains the proof recipe for the case of scalar \(y\). The proofs in this section generalize those of Appendix B.

Consider a sample \((x,y)\in\mathbb{R}^{k\times d}\), with

\[y=\phi(W^{*}x+\eta),\] (20)

where \(W^{*}\in\mathbb{R}^{d}\times k\), and noise \(\eta\sim\mathcal{N}(0,\Sigma^{*})\). The matrices \(W^{*}\) and \(\Sigma^{*}\) are unknown. For each matrix \(W\in\mathbb{R}^{d\times k}\), let \(\theta=Wx\in\mathbb{R}^{d}\), denote a reparametrization of \(W\), and let \(\theta^{*}\) denote \(\theta^{*}=W^{*}x\). Let \(S\) denote the co-ordinates of \(y\) that are zero-valued. Then the log-likelihood for each \(\theta,\Sigma\) is given by

\[f_{\theta,\Sigma}(y):=\log p_{W,\Sigma}(y\mid x)=c-\frac{1}{2}\log|\Sigma|+ \log\int_{t:t_{S}\leq 0,t_{S^{c}}=y_{S^{c}}}\exp\Bigl{\{}-(t-\theta)^{T} \Sigma^{-1}(t-\theta)/2\Bigr{\}}.\]

where \(c\) is a normalization constant which does not depend on \(\theta\) or \(\Sigma\). Let

\[P:=\Sigma^{-1}\]

and let \(P^{*}\) be the precision matrix of the noise \(\eta\), and \(P_{S},P_{SS^{c}},P_{S^{c}S},P_{S^{c}}\) be the block matrices of \(P\) corresponding to the index sets \(S\) and its complement \(S^{c}\).

By some arithmetic involving completion of squares, we can decompose the integral in \(f\) into the sum of two functions \(g,h\), such that

\[f_{\theta,\Sigma}(y)=c-\frac{1}{2}\log|\Sigma|+g_{\theta,\Sigma}(y)+h_{\theta, \Sigma}(y).\]

The first term \(g\) corresponds to the quadratic term involving the observed positive-valued coordinates \(y_{S^{c}}\):

\[g_{\theta,\Sigma}(y)=-(y_{S^{c}}-\theta_{S^{c}})^{T}(P_{S^{c}}-P_{S^{c}S}(P_{S })^{-1}P_{SS^{c}})(y_{S^{c}}-\theta_{S^{c}})/2.\]

As the matrix \(P_{S^{c}}-P_{S^{c}S}(P_{S})^{-1}P_{SS^{c}}=((P^{-1})_{S^{c}})^{-1}=\Sigma_{S^{c }}^{-1}\) is the precision matrix of \(\eta_{S}\), if \(\Sigma\) were the covariance of \(\eta\), we can simplify the above equation as

\[g_{\theta,\Sigma}(y)=-(y_{S^{c}}-\theta_{S^{c}})^{T}(\Sigma_{S^{c}})^{-1}(y_{ S^{c}}-\theta_{S^{c}})/2.\] (21)

The second term corresponds to the probability under \(\theta,P\) of observing zero-valued coordinates corresponding to the index set \(S\), given the positive coordinates \(y_{S^{c}}\):

\[h_{\theta,\Sigma}(y)=\log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{\frac{1}{2}}(t- \theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}})\|^{2}/2\Bigr{)}.\] (22)

The _log-likelihood ratio_ is the difference between \(f_{\theta,\Sigma}\) and \(f_{\theta^{*},\Sigma^{*}}\), which we denote by

\[\gamma_{\theta,\Sigma}(y):=f_{\theta,\Sigma}(y)-f_{\theta^{*},\Sigma^{*}}(y)\]

Over a dataset \(\{(x_{i},y_{i})\}_{i\in[n]}\), the average log-likelihood ratio is given by

\[\bar{\gamma}_{W,\Sigma}:=\frac{1}{n}\sum_{i}\gamma_{Wx_{i},\Sigma}(y_{i}).\]

**Remark C.1**.: _For ease of analysis, we will interchange between the precision matrix \(P\) in \(\gamma_{\theta,P}\) and the covariance matrix \(\Sigma\) in \(\gamma_{\theta,\Sigma}\), and it should be understood that \(P=\Sigma^{-1}\). The same applies to the functions \(g_{\theta,\Sigma}\) and \(h_{\theta,\Sigma}\). Finally, the matrix \(P^{*}\) refers to the ground truth precision matrix \((=\Sigma^{*-1})\)._

Analogous to Appendix B, we start by showing that the log-likelihood ratio is bounded by the noise in the sample. The proofs of results in this Section are in Subsection C.1.

**Lemma C.2**.: _Assume \(P^{*}:=\Sigma^{*-1}\) satisfies Assumption 4.4. For all \(y=\phi(\theta^{*}+\eta)\) such that \(S\) denotes the zero-coordinates of \(y\), and \(\eta\) such that \(\|P_{S}^{*\frac{1}{2}}\eta_{S}\|,\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|\leq B\), if the max eigenvalue \(\lambda_{\max}(P)\) satisfies_

\[\frac{\lambda_{\max}(P)}{\lambda_{\min}(P^{*})}\leq C,\]

_then for all \(\theta\in\mathbb{R}^{d}\), we have_

\[\gamma_{\theta,P}\leq\frac{d}{2}\log(C)+3B^{2}.\]

For the ease of stating the next Lemma, we assume that across the samples of \(y\) in the training data, at least one coordinate has sufficiently many positive samples. The proof of our theorem separately handles cases violating this assumption.

**Assumption C.3**.: _Let \(\delta\in(0,1)\) be a parameter corresponding to the failure probability of our algorithm. Then, there exists a coordinate \(j\in[d]\), such that for at least \(n^{\prime}=O\big{(}\log\frac{1}{\delta}\big{)}\) samples \(y_{i_{1}},\ldots,y_{i_{n^{\prime}}}\) in the dataset, the \(j\)-th coordinate is positive._

This is a very weak assumption: if it is violated, then \(W=0_{d\times k},\Sigma=0\) will achieve a TV distance smaller than \(\frac{2\varepsilon^{2}}{d}\).

Appendix B assumed that the variance in \(y\) was 1. Since Section 4.2 considers an unknown \(\Sigma^{*}\), we need the following Lemma to show that the MLE will select a precision matrix \(P\), whose eigenvalues are reasonably bounded wrt \(\Sigma^{*-1}\).

**Lemma C.4**.: _Under Assumption 4.4, C.3, consider \(P\in\mathbb{R}^{d\times d}_{+}\) such that \(\frac{\lambda_{\max}(P)}{\lambda_{\min}(P)}\leq\kappa\) and_

\[\frac{\lambda_{\max}(P)}{\lambda_{\max}(P^{*})}\geq O\bigg{(}\frac{\kappa^{3} d^{2}n^{2}}{k^{2}}+\frac{B^{2}n\kappa}{k}\bigg{)}.\]

_Then, for all \(W\in\mathbb{R}^{d\times k}\), and for all \(y_{i}=\phi(W^{*}x_{i}+\eta_{i})\) with \(\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|,\|P_{S}^{*\frac{1}{2}}\eta_{S}\|\leq B\), we have_

\[\bar{\gamma}_{W,P}:=\frac{1}{n}\sum_{i\in[n]}\gamma_{Wx_{i},P}(y_{i})<0.\]

Lemma C.2 and Lemma C.4 show that the MLE will only select precision matrices \(P\) that have max eigenvalues in a certain range of the true precision matric \(P^{*}\).

Now, for matrices in the above eigenvalue range, we first construct a geometric net over the max eigenvalue \(\rho\) of the precision matrix, and then cover the matrices whose max eigenvalue is smaller than \(\rho\).

**Lemma C.5** (\(\Sigma\) cover).: _For \(B>1\), and \(0<L<U\), let \(A>\max\Bigl{\{}\sqrt{\log\frac{1}{\varepsilon}},B^{2}U\kappa,\frac{d}{2}\log \bigl{(}\frac{\kappa U}{L}\bigr{)},1\Bigr{\}}\). Let \(P^{*}:=\Sigma^{*-1}\) be the precision matrix of \(\eta\). Let \(\Omega\subset\mathbb{R}^{d\times d}_{+}\) denote the set of positive definite matrices \(P\in\mathbb{R}^{d\times d}_{+}\) with condition number \(\kappa\) and whose maximum eigenvalue lies in \([L\lambda_{\min}(P^{*}),U\cdot\lambda_{\max}(P^{*})]\). Then, there exists a partition of \(\Omega\) of size_

\[\left(\operatorname{poly}\biggl{(}A,\frac{1}{\varepsilon}\biggr{)}\right)^{d^{2}}\]

_such that for all \(\theta\in\mathbb{R}^{d}\) and all \(y=\phi(\theta^{*}+\eta)\in\mathbb{R}^{d}\) with \(\|P_{S}^{*\frac{1}{2}}\eta_{S}\|,\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|\leq B\), and each cell \(I\) in the partition, one of the following holds:_

* _for all_ \(P\in I\)_,_ \(\gamma_{\theta,P}(y)<-A\)_, or_
* _for all_ \(P,P^{\prime}\in I\)_, we have_ \(|\gamma_{\theta,P}(y)-\gamma_{\theta,P^{\prime}}(y)|\leq\epsilon\)Analogous to Appendix B, we now construct a partition over \(W\) for a fixed precision matrix \(P\), such that each cell in the partition has very small log-likelihood (in which case the MLE will not choose it) or the log-likelihood changes slowly.

**Lemma C.6** (\(W\)-net).: _Let \(\eta_{S^{c}},\eta_{S}\) be such that_

\[\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|\leq B_{1},\|P_{S}^{*\frac{1}{2}}\eta_{ S}\|\leq B_{2},\]

_for \(B_{1},B_{2}\geq 0\)._

_Let \(A>\max\{B_{1}^{2},B_{2}^{2},\mathrm{poly}(C,\kappa)\}\). Let \(P^{*}=\Sigma^{*-1}\) be the precision matrix of \(\eta\). For a fixed matrix \(P\in\mathbb{R}^{d\times d}\) whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy \(\lambda_{\max}(P)\in[e^{-\frac{2d}{d}}\lambda_{\min}(P^{*}),C\lambda_{\max}(P^ {*})]\), there exists a partition \(\mathcal{I}\) of \(\mathbb{R}^{d}\) with size_

\[\left(\mathrm{poly}\bigg{(}A,\frac{1}{\varepsilon}\bigg{)}\right)^{3d}\]

_such that for each interval \(I\in\mathcal{I}\), we have one of the following:_

* _for all_ \(\theta\in I\)_,_ \(\gamma_{\theta,P}(y)<-A\)_, or_
* _for all_ \(\theta,\theta^{\prime}\in I\)_,_ \(|\gamma_{\theta,P}(y)-\gamma_{\theta^{\prime},P}(y)|\leq\epsilon\)_._

Using the above lemmas, we can show that the MLE will only pick out \(\widehat{W},\widehat{P}\) such that they have small TV on the dataset of \(\{x_{i}\}\).

**Lemma C.7**.: _Let \(x_{1},\ldots,x_{n}\) be fixed, and \(y_{i}=\phi(W^{*}x_{i}+\eta_{i})\) for \(\eta_{i}\sim\mathcal{N}(0,\Sigma^{*})\), and \(W^{*}\in\mathbb{R}^{d\times k}\) with \(\Sigma^{*}\in\mathbb{R}^{d\times d}\) satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant \(C>0\),_

\[n=C\cdot\frac{(d^{2}+kd)}{\varepsilon^{2}}\log\frac{kd\kappa}{\varepsilon}\]

_samples suffice to guarantee that with high probability, the MLE \(\widehat{W},\widehat{\Sigma}\) satisfies_

\[\widetilde{d}\Big{(}(\widehat{W},\widehat{\Sigma}),(W^{*},\Sigma^{*})\Big{)} \leq\varepsilon.\]

**Lemma C.8**.: _Let \(\{x_{i}\}_{i=1}^{n}\) be i.i.d. random variables such that \(x_{i}\sim\mathcal{D}_{x}\)._

_Let \(P^{*}:=\Sigma^{*-1}\). Let \(\lambda_{\min}^{*},\lambda_{\max}^{*}\) be the minimum and maximum eigenvalues of \(P^{*}\). For \(0<L<U\), let \(\Omega\) denote the following set of precision matrices_

\[\Omega:=\bigg{\{}P\in\mathbb{R}_{+}^{d\times d}:\frac{\lambda_{\max}(P)}{ \lambda_{\min}(P)}\leq\kappa\text{ and }\lambda_{\max}(P)\in[L\cdot\lambda_{\min}^{*},U \cdot\lambda_{\max}^{*}]\bigg{\}}.\]

_Then, for a sufficiently large constant \(C>0\), and for_

\[n=C\cdot\bigg{(}\frac{kd+d^{2}}{\varepsilon^{2}}\bigg{)}\log\bigg{(}\frac{kd \kappa}{\varepsilon}\log\bigg{(}\frac{U}{L}\bigg{)}\bigg{)},\]

_we have:_

**Theorem 4.5**.: _Let \(\mathbb{R}_{+}^{d\times d}\) denote the set of positive definite matrices with condition number \(\kappa\). Given \(n\) samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) satisfying Assumption 4.4, where \(x_{i}\sim\mathcal{D}_{x}\) i.i.d., and \(y_{i}\) is generated according to (7), let \(\widehat{W},\widehat{\Sigma}:=\operatorname*{arg\,max}_{W\in\mathbb{R}^{d \times k},\Sigma\in\mathbb{R}_{n}^{d\times d}}\frac{1}{n}\sum_{i}\log p_{W, \Sigma}(y_{i}\mid x_{i})\). Then, for a sufficiently large constant \(C>0\),_

\[n=C\cdot\bigg{(}\frac{kd+d^{2}}{\varepsilon^{2}}\bigg{)}\log\bigg{(}\frac{ \kappa kd}{\varepsilon\delta}\bigg{)}\]

_samples suffice to ensure that with probability \(1-\delta\), we have_

\[d_{TV}\Big{(}(\widehat{W},\widehat{\Sigma}),(W^{*},\Sigma^{*})\Big{)}\leq\varepsilon.\]Proof of Theorem 4.5.: First, we consider the cases violating Assumption C.3.

As \(n\propto\frac{d^{2}}{\varepsilon^{2}}\log\frac{1}{\delta}\), if assumption C.3 is violated, then it implies that each coordinate is non-zero in atmost a \(\varepsilon^{2}/d^{2}\) fraction of the samples, and a union bound implies that the probability of seeing a non-zero vector is atmost \(\varepsilon^{2}/d\). Hence, with high probability over the draws of the data, returning the all-zeros vector always will achieve a TV distance smaller than \(\frac{2\varepsilon^{2}}{d}\).

Let \(\widehat{P},P^{*}=\widehat{\Sigma}^{-1},\Sigma^{*-1}\). Now, if Assumption C.3 holds, Lemma C.7 guarantees that the MLE has small TV on the \(x_{i}\) observed in the dataset:

\[\widetilde{d}((\widehat{W},\widehat{P}),(W^{*},P^{*}))\leq\varepsilon.\]

The above result is over the finite \(x_{i}\) observed in our dataset. To generalize it over \(x\sim\mathcal{D}_{x}\), we use Lemma C.8, which gives

\[d((\widehat{W},\widehat{P}),(W^{*},P^{*}))-\widetilde{d}(( \widehat{W},\widehat{P}),(W^{*},P^{*}))\leq\varepsilon.\]

Rescaling \(\varepsilon\) gives the conclusion of the Theorem. 

### Proofs of Appendix C.

**Lemma C.2**.: _Assume \(P^{*}:=\Sigma^{*-1}\) satisfies Assumption 4.4._

_For all \(y=\phi(\theta^{*}+\eta)\) such that \(S\) denotes the zero-coordinates of \(y\), and \(\eta\) such that \(\|P_{S}^{*\frac{1}{2}}\eta_{S}\|,\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|\leq B\), if the max eigenvalue \(\lambda_{\max}(P)\) satisfies_

\[\frac{\lambda_{\max}(P)}{\lambda_{\min}(P^{*})}\leq C,\]

_then for all \(\theta\in\mathbb{R}^{d}\), we have_

\[\gamma_{\theta,P}\leq\frac{d}{2}\log(C)+3B^{2}.\]

Proof.: We have

\[\gamma_{\theta,\Sigma}\leq\frac{1}{2}\log\frac{|\Sigma^{*}|}{| \Sigma|}+g_{\theta,\Sigma}-g_{\theta^{*},\Sigma^{*}}+h_{\theta,\Sigma}-h_{ \theta^{*},\Sigma^{*}}.\] (23)

From Lemma C.9, C.10, we have

\[g_{\theta,\Sigma}-g_{\theta^{*},\Sigma^{*}}+h_{\theta,\Sigma}-h _{\theta^{*},\Sigma^{*}}\leq g_{\theta,\Sigma}+\frac{1}{2}\log\frac{|P_{S}^{* }|}{|P_{S}|}+3B^{2}.\]

Substituting in Eqn (23), we get

\[\gamma_{\theta,\Sigma}\leq g_{\theta,\Sigma}+\frac{1}{2}\log \frac{|\Sigma^{*}|}{|\Sigma|}+\frac{1}{2}\log\frac{|P_{S}^{*}|}{|P_{S}|}+3B^{2}.\]

As \((P_{S}^{*})^{-1}=\Sigma_{S}^{*}-\Sigma_{SS^{*}}^{*}\Sigma_{S^{c}}^{*-1}\Sigma _{S^{c}S}^{*}\), by the matrix determinant rule, we have

\[\log|\Sigma^{*}|+\log|P_{S}^{*}|=\log|\Sigma_{S^{c}}^{*}|.\]

This gives

\[\gamma_{\theta,\Sigma}\leq g_{\theta,\Sigma}+\frac{1}{2}\log \frac{|\Sigma_{S^{c}}^{*}|}{|\Sigma_{S^{c}}|}+3B^{2}.\]

This gives

\[\gamma_{\theta,\Sigma} \leq g_{\theta,\Sigma}+\frac{d}{2}\log\frac{\lambda_{\max}(\Sigma ^{*})}{\lambda_{\min}(\Sigma)}+3B^{2},\] \[=g_{\theta,\Sigma}+\frac{d}{2}\log\frac{\lambda_{\max}(P)}{ \lambda_{\min}(P^{*})}+3B^{2}.\] (24)As the matrix \(\Sigma_{S^{c}}^{-1}\) is positive definite, we trivially get

\[g_{\theta_{i},\Sigma}(y)=-(y_{i,S^{c}}-\theta_{i,S^{c}})^{T}(\Sigma_{S^{c}})^{-1} (y_{i,S^{c}}-\theta_{i,S^{c}})/2\leq 0.\]

Substituting in Eqn (24), we get

\[\gamma_{\theta,\Sigma}\leq\frac{d}{2}\log\frac{\lambda_{\max}(P)}{\lambda_{ \min}(P^{*})}+3B^{2}.\]

As the Lemma assumes

\[\lambda_{\max}(P)\leq C\lambda_{\min}(P^{*}),\]

we get

\[\gamma_{\theta,\Sigma}\leq\frac{d}{2}\log(C)+3B^{2}.\]

**Lemma C.9**.: _Consider the function \(g\) defined in Eq (21). For the ground truth parameters \(\theta^{*},\Sigma^{*}\), the function \(g_{\theta^{*},\Sigma^{*}}\) satisfies_

\[-g_{\theta^{*},\Sigma^{*}}\leq\frac{1}{2}\|\eta_{S^{c}}\|_{\Sigma_{S^{c}}}^{2},\]

_which is, with probability \(1-e^{-\Omega(d)}\),_

\[-g_{\theta^{*},\Sigma^{*}}\leq O(d).\]

Proof.: As \(y_{S^{c}}\) are the positive valued coordinates in \(y\), we have

\[y_{S^{c}}-\theta_{S^{c}}^{*}=\eta_{S^{c}},\]

which gives

\[g_{\theta^{*},\Sigma^{*}}(y) =-(y_{S^{c}}-\theta_{S^{c}}^{*})^{T}(\Sigma_{S^{c}}^{*})^{-1}(y_{ S^{c}}-\theta_{S^{c}}^{*})/2,\] \[=-\|\eta_{S^{c}}\|_{\Sigma_{S^{c}}}^{2}/2.\]

As \(\eta_{S^{c}}\) is Gaussian with covariance \(\Sigma_{S^{c}}^{*}\), the expected norm is \(\frac{|S^{c}|}{2}\), which implies that with probability \(1-e^{-\Omega(|S^{c}|)}\), we have

\[-g_{\theta^{*},\Sigma^{*}}(y)\leq O(|S^{c}|).\]

**Lemma C.10**.: _Consider \(y\) generated according to Eqn (20) by_

\[y=\phi(\theta^{*}+\eta),\quad\eta\sim\mathcal{N}(0,\Sigma^{*}).\]

_For all \(\theta\in\mathbb{R}^{d},\Sigma\in\mathbb{R}^{d\times d}_{+}\), and the function \(h_{\theta,\Sigma}\) defined in Eqn (22), the difference \(h_{\theta,\Sigma}(y)-h_{\theta^{*},\Sigma^{*}}(y)\) satisfies_

\[h_{\theta,\Sigma}(y)-h_{\theta^{*},\Sigma^{*}}(y)\leq\ \frac{1}{2}\log\frac{|P_{S}^{*}|}{|P_{S}|}+\|P_{S^{c}}^{* \frac{1}{2}}\eta_{S^{c}}\|^{2}+2\|P_{S}^{*\frac{1}{2}}\eta_{S}\|^{2}-\|\eta_{S ^{c}}\|_{\Sigma_{S^{c}}}^{2}+O(|S|),\] (25)

where \(P^{*}=\Sigma^{*-1}\) is the precision matrix of \(\eta\).

Proof.: For \(\theta\in\mathbb{R}^{d},\Sigma\in\mathbb{R}^{d\times d}_{+}\), and \(P=\Sigma^{-1}\), we have

\[h_{\theta,\Sigma}(y) =\log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{\frac{1}{2}}(t-\theta_{S}) +(P_{S})^{-1/2}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}})\|^{2}/2\Bigr{)},\] \[\leq\log\int_{t\in\mathbb{R}^{|S|}}\exp\Bigl{(}-\|P_{S}^{\frac{1} {2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}})\|^{2}/2 \Bigr{)},\] \[\leq\frac{|S|}{2}\log(2\pi)-\frac{1}{2}\log|P_{S}|,\] (26)where the last step follows from the integral of a Gaussian pdf. This gives a sufficient upper bound on \(h_{\theta,\Sigma}(y)\), and now we will focus on lower bounding \(h_{\theta^{*},\Sigma^{*}}(y)\).

For the coordinates of \(y\) in \(S^{c}\), we have \(y_{S^{c}}-\theta^{*}_{S^{c}}=\eta_{S^{c}}\). Substituting in Eqn (22), we get

\[h_{\theta^{*},\Sigma^{*}}(y)= \log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{*\frac{1}{2}}(t-\theta^{*} _{S})+(P_{S}^{*})^{-1/2}P_{SS^{c}\eta_{S^{c}}}^{*}\|^{2}/2\Bigr{)},\] \[= \log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{*\frac{1}{2}}(t-\theta^{* }_{S})+(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}/2\Bigr{)}.\]

Using \(\|a+b\|^{2}\leq 2a^{2}+2b^{2}\), we get

\[h_{\theta^{*},\Sigma^{*}}(y)\geq-\|(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c} }\|^{2}+\log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{*\frac{1}{2}}(t-\theta^{*} _{S})\|^{2}\Bigr{)}.\]

Set \(u:=P_{S}^{*\frac{1}{2}}(t-\theta^{*}_{S})\), and by the change of variables formula, we get:

\[h_{\theta^{*},\Sigma^{*}}(y)\geq -\|(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}+\log\int_{P _{S}^{*-\frac{1}{2}}u+\theta^{*}_{S}\leq 0}\Bigl{|}(P_{S}^{*})^{-1/2}\Bigr{|} \cdot\exp\bigl{(}-\|u\|^{2}\bigr{)},\] \[= -\|(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}+\frac{1}{2} \log\bigl{|}P_{S}^{*-1}\bigr{|}+\log\int_{P_{S}^{*-\frac{1}{2}}u+\theta^{*}_{S }\leq 0}\exp\bigl{(}-\|u\|^{2}\bigr{)}.\]

For \(i\in S\), we have \(\theta^{*}_{i}+\eta_{i}\leq 0\). This gives

\[P_{S}^{*-\frac{1}{2}}u\leq\eta_{S}\Rightarrow P_{S}^{*-\frac{1}{2}}u+\theta^{*}_{S}\leq 0 \Rightarrow\log\int_{P_{S}^{*-\frac{1}{2}}u+\theta^{*}_{S}\leq 0}\exp \bigl{(}-\|u\|^{2}\bigr{)}\geq\log\int_{P_{S}^{*-\frac{1}{2}}u\leq\eta_{S}} \exp\bigl{(}-\|u\|^{2}\bigr{)},\]

using which we get

\[h_{\theta^{*},\Sigma^{*}}(y)\geq-\|(P_{S})^{*-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\| ^{2}-\frac{1}{2}\log\bigl{|}P_{S}^{*}\bigr{|}+\log\int_{P_{S}^{*-\frac{1}{2}} u\leq\eta_{S}}\exp\bigl{(}-\|u\|^{2}\bigr{)}.\]

By another change of variables via \(v:=P_{S}^{*-\frac{1}{2}}u-\eta_{S}\), we get

\[h_{\theta^{*},\Sigma^{*}}(y)\geq -\|(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}-\frac{1}{2} \log\bigl{|}P_{S}^{*}\bigr{|}+\log\int_{v\leq 0}\Bigl{|}P_{S}^{*\frac{1}{2}} \Bigl{|}\exp\Bigl{(}-\|P_{S}^{*\frac{1}{2}}(v+\eta_{S})\|^{2}\Bigr{)},\] \[\geq -\|(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}-\frac{1}{2} \log\bigl{|}P_{S}^{*}\bigr{|}-2\|P_{S}^{*\frac{1}{2}}\eta_{S}\|^{2}+\log\int_{ v\leq 0}\Bigl{|}P_{S}^{*\frac{1}{2}}\Bigl{|}\exp\Bigl{(}-2\|P_{S}^{*\frac{1} {2}}v\|^{2}\Bigr{)},\] \[= -\|(P_{S}^{*})^{-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}-\frac{1}{2} \log\bigl{|}P_{S}^{*}\bigr{|}-2\|P_{S}^{*\frac{1}{2}}\eta_{S}\|^{2}+O(|S|).\]

As \((\Sigma_{S^{c}}^{*})^{-1}=P_{S^{c}}^{*}-P_{S^{c}}^{*}(P_{S}^{*})^{-1}P_{SS^{c}}^ {*}\), we have

\[-\|(P_{S})^{*-1/2}P_{SS^{c}}^{*}\eta_{S^{c}}\|^{2}=\|\eta_{S^{c}}\|_{\Sigma_{S^{ c}}^{*}}^{2}-\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|^{2},\]

which gives

\[h_{\theta^{*},\Sigma^{*}}(y)\geq \|\eta_{S^{c}}\|_{\Sigma_{S^{c}}^{*}}^{2}-\|P_{S^{c}}^{*\frac{1}{ 2}}\eta_{S^{c}}\|^{2}-\frac{1}{2}\log\bigl{|}P_{S}^{*}\bigr{|}-2\|P_{S}^{* \frac{1}{2}}\eta_{S}\|^{2}+O(|S|).\] (27)

From Eqn (26) \(-\) Eqn (27), we get

\[h_{\theta,\Sigma}(y)-h_{\theta^{*},\Sigma^{*}}(y)\leq\ \|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|^{2}+2\|P_{S}^{* \frac{1}{2}}\eta_{S}\|^{2}-\|\eta_{S^{c}}\|_{\Sigma_{S^{c}}^{*}}^{2}+\frac{1}{2} \log\frac{\bigl{|}P_{S}^{*}\bigr{|}}{|P_{S}|}+O(|S|).\] (28)

**Lemma C.4**.: _Under Assumption 4.4, C.3, consider \(P\in\mathbb{R}_{+}^{d\times d}\) such that \(\frac{\lambda_{\max}(P)}{\lambda_{\min}(P)}\leq\kappa\) and_

\[\frac{\lambda_{\max}(P)}{\lambda_{\max}(P^{*})}\geq O\biggl{(}\frac{\kappa^{3}d^{2}n^{2}}{k^{2}} +\frac{B^{2}n\kappa}{k}\biggr{)}.\]

_Then, for all \(W\in\mathbb{R}^{d\times k}\), and for all \(y_{i}=\phi(W^{*}x_{i}+\eta_{i})\) with \(\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|,\|P_{S}^{*\frac{1}{2}}\eta_{S}\|\leq B\), we have_

\[\bar{\gamma}_{W,P}:=\frac{1}{n}\sum_{i\in[n]}\gamma_{Wx_{i},P}(y_{i})<0.\]Proof of Lemma c.4.: For each \(W\in\mathbb{R}^{d\times k}\), let

\[\theta_{i}:=Wx_{i}.\]

From Eqn (24) in Lemma C.2, for each \(i\in[n]\), we have,

\[\gamma_{\theta_{i},\Sigma} \leq g_{\theta_{i},\Sigma}+\frac{d}{2}\log\frac{\lambda_{\max}(P)} {\lambda_{\min}(P^{*})}+3B^{2},\] \[\leq g_{\theta_{i},\Sigma}+\frac{d}{2}\log\frac{\kappa\lambda_{ \max}(P)}{\lambda_{\max}(P^{*})}+3B^{2}.\]

Now consider

\[g_{\theta_{i},\Sigma}(y) =-\frac{1}{2}(y_{i,S^{c}}-\theta_{i,S^{c}})^{T}(\Sigma_{S^{c}})^{ -1}(y_{i,S^{c}}-\theta_{i,S^{c}}),\] \[\leq-\frac{1}{2}\|y-\theta\|^{2}\lambda_{\min}\big{(}\Sigma_{S^{c }}^{-1}\big{)},\] \[\leq-\frac{1}{2}\|y-\theta\|^{2}\lambda_{\min}\big{(}\Sigma^{-1} \big{)}=-\frac{1}{2}\|y-\theta\|^{2}\lambda_{\min}(P),\] \[\leq-\frac{1}{2}\|y-\theta\|^{2}\frac{\lambda_{\max}(P)}{\kappa},\]

where the second inequality comes from the eigenvalue interlacing Theorem, and the last line follows from the condition number assumption on \(\Sigma,P\).

By Assumption C.3, there exist at least \(\varepsilon^{2}n\) samples for a coordinate \(j\) such that \((y_{i})_{j}>0\). Averaging \(g_{\theta_{i},\Sigma}\), by Lemma A.1, we get that with high probability,

\[\sum_{i}\|y_{i}-\theta_{i}\|^{2}\geq\frac{\sigma_{j}^{*2}k}{2},\]

which gives

\[\frac{1}{n}\sum_{i}g_{\theta_{i},\Sigma}(y_{i}) \leq-\frac{\sigma_{j}^{*2}k\lambda_{\max}(P)}{4n\kappa},\] \[\leq-\frac{k\lambda_{\max}(P)}{4n\kappa\lambda_{\max}(P^{*})}.\]

This gives

\[\bar{\gamma}_{W,\Sigma} \leq-\frac{\lambda_{\max}(P)k}{4n\kappa\lambda_{\max}(P^{*})}+ \frac{d}{2}\log\!\left(\kappa\cdot\frac{\lambda_{\max}(P)}{\lambda_{\max}(P^{ *})}\right)+3B^{2},\] \[\leq-\frac{\lambda_{\max}(P)k}{4n\kappa\lambda_{\max}(P^{*})}+d \sqrt{\kappa\cdot\frac{\lambda_{\max}(P)}{\lambda_{\max}(P^{*})}}+3B^{2}.\]

Completing the squares, we get

\[\bar{\gamma}_{W,\Sigma}\leq-\Bigg{(}\sqrt{\frac{\lambda_{\max}(P)k}{4n\kappa \lambda_{\max}(P^{*})}}-\kappa d\sqrt{\frac{n}{k}}\Bigg{)}^{2}+\frac{\kappa^{ 2}d^{2}n}{k}+3B^{2}.\]

For

\[\frac{\lambda_{\max}(P)}{\lambda_{\max}(P^{*})}\geq O\bigg{(}\frac{\kappa^{3}d ^{2}n^{2}}{k^{2}}+\frac{B^{2}n\kappa}{k}\bigg{)},\]

the above inequality satisfies

\[\bar{\gamma}_{W,\Sigma}\leq 0.\]

**Lemma C.11**.: _Assume \(P^{*}:=\Sigma^{*-1}\) satisfies Assumption 4.4 with condition number \(\kappa\). For all \(y=\phi(\theta^{*}+\eta)\) such that \(S\) denotes the zero-coordinates of \(y\), and \(\eta\) such that \(\|P_{S}^{*\frac{1}{2}}\eta_{S}\|,\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|\leq B\), consider precision matrices \(P\) whose max eigenvalue \(\lambda_{\max}(P)\) satisfies_

\[\frac{\lambda_{\max}(P)}{\lambda_{\min}(P^{*})}\leq C.\]

_Let \(A\geq 4\max\{\frac{d}{2}\log C,3B^{2}\}\). Then for \(V:=(P^{-1})_{S^{c}}\) and \(R_{P}\) defined as_

\[R_{P}:=2B\sqrt{C}+\sqrt{\frac{3}{2}A},\] (29)

_we have_

\[\|\theta_{S^{c}}-\theta_{S^{c}}^{*}\|_{V}\geq R_{P}\implies\gamma_{\theta,P} \leq-A.\]

Proof of Lemma C.11.: Consider Eqn (24) in Lemma C.2. We have

\[\gamma_{\theta,P} \leq g_{\theta,P}+\frac{d}{2}\log\frac{\lambda_{\max}(P)}{\lambda _{\min}(P^{*})}+3B^{2},\] \[\leq g_{\theta,P}+\frac{d}{2}\log C+3B^{2},\]

where the last inequality follows from \(\frac{\lambda_{\max}(P)}{\lambda_{\min}(P^{*})}\leq C\) in the statement of the Lemma.

By the definition of \(g_{\theta,P}\), we have

\[g_{\theta,P}:=-\frac{1}{2}(y_{S^{c}}-\theta_{S^{c}})^{T}(P_{S^{c}}-P_{S^{c}S} P_{S}^{-1}P_{SS^{c}})(y_{S^{c}}-\theta_{S^{c}}).\]

We can rewrite the matrix \((P_{S^{c}}-P_{S^{c}S}P_{S}^{-1}P_{SS^{c}})\) as

\[(P_{S^{c}}-P_{S^{c}S}P_{S}^{-1}P_{SS^{c}})=\big{(}(P^{-1})_{S^{c}}\big{)}^{-1}.\]

By setting

\[V:=(P^{-1})_{S^{c}},\]

we can rewrite \(g_{\theta,P}\) as

\[g_{\theta,P}:=-\frac{1}{2}\|y_{S^{c}}-\theta_{S^{c}}\|_{V}^{2}=-\frac{1}{2}(y_ {S^{c}}-\theta_{S^{c}})^{T}V^{-1}(y_{S^{c}}-\theta_{S^{c}}).\]

Now, as \(y_{S^{c}}=\eta_{S^{c}}+\theta_{S^{c}}^{*}\), we have

\[g_{\theta,P} =-\frac{1}{2}\|\eta_{S^{c}}+\theta_{S^{c}}^{*}-\theta_{S^{c}}\|_ {V}^{2},\] \[=-\frac{1}{2}\|\theta_{S^{c}}^{*}-\theta_{S^{c}}\|_{V}^{2}+\|\eta _{S^{c}}\|_{V}\|\theta_{S^{c}}^{*}-\theta_{S}\|-\frac{1}{2}\|\eta_{S^{c}}\|_ {V}^{2}.\]

Ignoring the \(\|\eta_{S^{c}}\|_{V}^{2}\) term, we get

\[g_{\theta,P}\leq-\frac{1}{2}\|\theta_{S^{c}}^{*}-\theta_{S^{c}}\|_{V}^{2}+\| \eta_{S^{c}}\|_{V}\|\theta_{S^{c}}^{*}-\theta_{S}\|_{V}.\]

By the Cauchy-Schwartz inequality and the eigenvalue interlacing theorem, we have

\[\|\eta_{S^{c}}\|_{V}\leq\lambda_{\max}^{\frac{1}{2}}(V^{-1})\cdot\|\eta_{S^{c} }\|_{2}=\frac{\|\eta_{S^{c}}\|_{2}}{\lambda_{\min}^{\frac{1}{2}}(V)}=\frac{\| \eta_{S^{c}}\|_{2}}{\lambda_{\min}^{\frac{1}{2}}(P_{S^{c}}^{-1})}\leq\frac{\| \eta_{S^{c}}\|_{2}}{\lambda_{\min}^{\frac{1}{2}}(P^{-1})}=\lambda_{\max}^{ \frac{1}{2}}(P)\cdot\|\eta_{S^{c}}\|_{2}\]By the statement of the Lemma, we have \(\|P^{*\frac{1}{2}}_{S^{c}}\eta_{S^{c}}\|\leq B\implies\|\eta_{S^{c}}\|_{2}\leq \frac{B}{\lambda_{\min}^{\frac{1}{2}}(P^{*c}_{S^{c}})}\). Substituting in the above inequality, we get

\[\|\eta_{S^{c}}\|_{V}\leq\frac{\lambda_{\max}^{\frac{1}{2}}(P)\cdot\|\eta_{S^{c} }\|_{2}}{\lambda_{\min}^{\frac{1}{2}}(P^{*}_{S^{c}})}\leq\sqrt{C}B.\]

Substituting in the function \(g_{\theta,P}\), we get

\[g_{\theta,P}\leq-\frac{1}{2}\|\theta^{*}_{S^{c}}-\theta_{S^{c}}\|_{V}^{2}+B \sqrt{C}\|\theta^{*}_{S^{c}}-\theta_{S}\|_{V}.\]

Hence, for \(\theta\) satisfying

\[\|\theta_{S^{c}}-\theta^{*}_{S^{c}}\|_{V}\geq R_{P}:=2B\sqrt{C}+2\sqrt{A},\]

we get

\[\gamma_{\theta,P}\leq-A.\]

In order to cover our precision matrices, we will consider a subset of matrices whose entries are quantized by an interval size \(\beta\):

**Definition C.12** (Quantized Precision Matrices).: _For \(\kappa>0\), define \(\Omega\subset\mathbb{R}^{d\times d}\) as the set of positive definite matrices with condition number \(\kappa\). For \(\rho>0\), define the set \(\Omega_{\rho}\subset\Omega\) as_

\[\Omega_{\rho}:=\Big{\{}P\in\Omega:\lambda_{\max}(P)\in\Big{[}\frac{\rho}{2}, \rho\Big{]}\Big{\}}\]

_For a quantization size \(\beta>0\), define \(\widetilde{\Omega}_{\rho,\beta}\subset\Omega_{\rho}\) as:_

\[\widetilde{\Omega}_{\rho,\beta}:=\{P\in\Omega_{\rho}:P_{ij}\in\{-\rho,-\rho(1 -\beta),-\rho(1-2\beta),\cdots,\rho(1-2\beta),\rho(1-\beta),\rho\}.\}\]

**Lemma C.5** (\(\Sigma\) cover).: _For \(B>1\), and \(0<L<U\), let \(A>\max\Bigl{\{}\sqrt{\log\frac{1}{\varepsilon}},B^{2}U\kappa,\frac{d}{2}\log \bigl{(}\frac{\kappa U}{L}\bigr{)},1\Bigr{\}}\)._

_Let \(P^{*}:=\Sigma^{*-1}\) be the precision matrix of \(\eta\). Let \(\Omega\subset\mathbb{R}^{d\times d}_{+}\) denote the set of positive definite matrices \(P\in\mathbb{R}^{d\times d}_{+}\) with condition number \(\kappa\) and whose maximum eigenvalue lies in \([L\lambda_{\min}(P^{*}),U\cdot\lambda_{\max}(P^{*})]\)._

_Then, there exists a partition of \(\Omega\) of size_

\[\left(\operatorname{poly}\biggl{(}A,\frac{1}{\varepsilon}\biggr{)}\right)^{d^ {2}}\]

_such that for all \(\theta\in\mathbb{R}^{d}\) and all \(y=\phi(\theta^{*}+\eta)\in\mathbb{R}^{d}\) with \(\|P^{*\frac{1}{2}}_{S}\eta_{S}\|,\|P^{*\frac{1}{2}}_{S^{c}}\eta_{S^{c}}\|\leq B\), and each cell \(I\) in the partition, one of the following holds:_

* _for all_ \(P\in I\)_,_ \(\gamma_{\theta,P}(y)<-A\)_, or_
* _for all_ \(P,P^{\prime}\in I\)_, we have_ \(|\gamma_{\theta,P}(y)-\gamma_{\theta,P^{\prime}}(y)|\leq\epsilon\)_._

Proof.: In order to construct the net over the precision matrices, we will consider geometrically spaced values of \(\rho\in[L\cdot\lambda_{\min}(P^{*}),U\cdot\lambda_{\max}(P^{*})]\), and for each \(\rho\), we will construct a net over matrices that have max eigenvalue \(\leq\rho\).

Now consider \(\rho>0\) that lies in the following discrete set:

\[\bigl{\{}\lambda_{\min}(P^{*})2^{j},j\in\lceil\log_{2}(\kappa\frac{U}{L}) \rceil\bigr{\}}\]

This set is a geometric partition over the possible max eigenvalues that the MLE can return.

For the current \(\rho\), let \(\Omega_{\rho}\) follow Definition C.12. Now consider \(P\in\Omega_{\rho}\).

Constructing the interval for which \(\gamma_{\theta,P}<-A\).By Lemma C.11, for \(V=(P^{-1})_{S^{c}}\), and \(R_{P}=O\Big{(}B\sqrt{\frac{\rho}{\lambda_{\min}(P^{*})}}+\sqrt{A}\Big{)}=O(\sqrt{A})\), we have

\[\|\theta_{S^{c}}-\theta_{S^{c}}^{*}\|_{V}\geq R_{P}\implies\gamma_{\theta,P}<-A.\]

For any \(\theta\), notice that the set of matrices \(P\) satisfying \(\|\theta_{S^{c}}-\theta_{S^{c}}^{*}\|_{V}\geq R_{P}\) is connected (as its complement is compact). This forms the set \(I\) for which \(\gamma_{\theta,P}<-A\).

Constructing intervals for which \(|\gamma_{\theta,P}-\gamma_{\theta,P^{\prime}}|\leq\varepsilon\).We will now construct a partition over those \(P\) which satisfy \(\|\theta_{S^{c}}-\theta_{S^{c}}^{*}\|_{V}<R_{P}\), and show that the log-likelihood changes by atmost \(\varepsilon\) for each cell in this partition.

If \(P\in\Omega_{\rho}\), then each of its elements \(P_{ij}\in[-\rho,\rho]\). For a parameter \(\beta>0\) that we will specify later, consider the partition \(\widetilde{\Omega}_{\rho,\beta}\) of \(\Omega_{\rho}\), following Definition C.12. Clearly, the size of \(\widetilde{\Omega}_{\rho,\beta}\) can be upper bounded by

\[\Big{|}\widetilde{\Omega}_{\rho,\beta}\Big{|}\leq\bigg{(}\frac{2}{\beta}\bigg{)} ^{d^{2}}.\]

We will now analyze the effect of rounding down \(P\in\Omega_{\rho}\) to its nearest element in \(\widetilde{\Omega}_{\rho,\beta}\).

By Claim C.13, for \(\gamma=2\kappa\beta d^{2}\), we have

\[(1-\gamma)\|t-\theta\|_{\Sigma}^{2}\leq\|t-\theta\|_{\Sigma^{\prime}}^{2}\leq (1+\gamma)\|t-\theta\|_{\Sigma}^{2},\] (30)

Consider the log-likelihood at \(\theta,P^{\prime}\):

\[f_{\theta,P^{\prime}}(y)=\frac{1}{2}\log\lvert P^{\prime}\rvert+\log\int_{t:t _{S}\leq 0,t_{S^{c}}=y_{S^{c}}}\exp\big{(}-\|t-\theta\|_{\Sigma^{\prime}}^{2} \big{)}.\]

We will use the LHS of Eqn (30) to show that

\[f_{\theta,P^{\prime}}(y)-\frac{1}{2}\log\lvert P^{\prime}\rvert\leq f_{ \theta,P}(y)-\frac{1}{2}\log\lvert P\rvert+\varepsilon,\]

and deal with the \(\log\lvert P^{\prime}\rvert\) term later. The lower bound for the log-likelihood at \(P^{\prime}\) can be obtained via analogous proof using the RHS of Eqn (30).

By the LHS of Eqn (30), we get

\[f_{\theta,P^{\prime}}(y)-\frac{1}{2}\log\lvert P^{\prime}\rvert\leq\log\int_ {t:t_{S}\leq 0,t_{S^{c}}=y_{S^{c}}}\exp\big{(}-(1-\gamma)\|t-\theta\|_{\Sigma}^{2}\big{)}.\]

Rearranging the terms, we get

\[f_{\theta,P^{\prime}}(y)-\frac{1}{2}\log\lvert P^{\prime}\rvert\leq -\frac{(1-\gamma)}{2}\|y_{S^{c}}-\theta_{S^{c}}\|_{\Sigma_{S^{c}}}^ {2}\] \[+\log\int_{t\leq 0}\exp\bigg{(}-\frac{(1-\gamma)}{2}\|P_{S}^{ \frac{1}{2}}(t-\theta)_{S}+P_{S}^{-\frac{1}{2}}P_{SS^{c}}(y_{S^{c}}-\theta_{S^ {c}})\|^{2}\bigg{)}\]

The non-integral term corresponds to \(g_{\theta,P}\) in Eqn (21), while the integral term corresponds to \(h_{\theta,P}\) in Eqn (22).

Handling the non-integral term.As we are only considering \(\theta\) such that \(\|y_{S^{c}}-\theta_{S^{c}}\|_{\Sigma_{S^{c}}}\leq R_{P}\), we have that for

\[\beta=O\bigg{(}\frac{\varepsilon}{R_{P}^{2}d^{2}\kappa}\bigg{)}=O\bigg{(}\frac {\varepsilon}{\operatorname{poly}(A)}\bigg{)},\]

the non-integral term corresponds to \(g_{\theta,P}+\varepsilon\), which gives

\[f_{\theta,P^{\prime}}(y)-\frac{1}{2}\log\lvert P^{\prime}\rvert\leq g_{\theta,P}+\varepsilon+\log\int_{t\leq 0}\exp\bigg{(}-\frac{(1-\gamma)}{2}\|P_{S}^{\frac{1}{2}}(t- \theta)_{S}+P_{S}^{-\frac{1}{2}}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}})\|^{2} \bigg{)}\] (31)Handling the integral.Now we consider the integral term. Define the integral

\[I_{1}=\log\int_{t\leq 0}\exp\left(-\frac{(1-\gamma)}{2}\|P_{S}^{\frac{1}{2}}(t- \mu)\|^{2}\right)\]

for \(\gamma=2d^{2}\kappa\beta\) and \(\mu=\theta_{S}-P_{S}^{-1}P_{S{S^{c}}}(y_{{S^{c}}}-\theta_{{S^{c}}})\).

Define the analogous integral that does not have the \((1-\gamma)\) term in the exponential:

\[I_{2}=\log\int_{t\leq 0}\exp\left(-\frac{1}{2}\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}\right)\]

Clearly, \(I_{1}\geq I_{2}\).

We only need to consider \(\mu\) such that \(\|\mu\|_{\infty}\leq O(\sqrt{A}\rho)\): otherwise the likelihood will be smaller than \(-A\).

By Lemma C.14, for \(\gamma=O\Big{(}\frac{\varepsilon}{\operatorname{poly}(A)}\Big{)}\), we have

\[I_{1}\leq I_{2}+\varepsilon.\]

Handling the log-determinant term.Now consider the \(\log|P|\) term. As we are decreasing each element by atmost \(\beta\rho\), none of the eigenvalues can increase. Moreover, as

\[\text{Tr}(P^{\prime})-\text{Tr}(P)\geq-d\beta\rho,\]

we can conclude that each eigenvalue decreases by at most \(-d\beta\rho\). Also, as \(\rho\leq\kappa\lambda_{j}(P)\ \forall\ j\in[d]\), we can conclude that each eigenvalue satisfies

\[\lambda_{j}(P^{\prime})\geq\lambda_{j}(P)(1-d\beta\kappa).\]

Hence, the log-determinant satisfies

\[\log|P^{\prime}|\geq\log|P|+d\log(1-\beta d\kappa)\geq\log|P|-\frac{d^{2} \beta\kappa}{1-d\beta\kappa}\geq\log|P|-O(\varepsilon)\text{ for }\beta\leq\frac{ \varepsilon}{\kappa d^{2}}\leq\frac{\varepsilon}{\kappa r^{2}d^{2}}.\]

This finally gives

\[|\gamma_{\theta,P}-\gamma_{\theta,P^{\prime}}|\leq O(\varepsilon).\]

Bounding the size of the netAs \(\beta=O\Big{(}\frac{\varepsilon}{\operatorname{poly}(A)}\Big{)}\), and the max radius is also \(O(\operatorname{poly}(A))\), we have a cover of size \(\Big{(}\frac{\operatorname{poly}(A)}{\varepsilon}\Big{)}\) per entry of the precision matrix (for a fixed \(\Omega_{\rho}\)).

Intersecting the \(d^{2}\) nets means that for each \(\Omega_{\rho}\), we have a net of size

\[\left(\operatorname{poly}\!\left(A,\frac{1}{\varepsilon}\right)\right)^{d^{2}}.\]

As we are considering \(\operatorname{poly}(A)\) many \(\Omega_{\rho}\)s, the size of the net remains the same as the above.

**Claim C.13**.: _In the setting of Lemma C.5, if \(P\in\Omega_{\rho}\) and \(P^{\prime}\in\widetilde{\Omega}_{\rho,\beta}\) is its nearest neighbor, then for \(\gamma=2\kappa\beta d^{2}\), we have_

\[(1-\gamma)\|t-\theta\|_{\Sigma}^{2}\leq\|t-\theta\|_{\Sigma^{\prime}}^{2}\leq (1+\gamma)\|t-\theta\|_{\Sigma}^{2},\] (32)

_where \(\Sigma:=P^{-1},\Sigma^{\prime}:=P^{{}^{\prime}-1}\)._

Proof.: Consider \(P\in\Omega_{\rho}\) and \(P^{\prime}\in\widetilde{\Omega}_{\rho,\beta}\) such that \(P=P^{\prime}+\Delta\). Since \(P^{\prime}\) is the rounding down of \(P\), we have \(\Delta_{ij}\in[0,\beta\rho]\).

As \(\text{Tr}(\Delta)\in[0,\rho\beta d]\), and \(\|\Delta\|_{F}\leq\rho\beta d\), we have

\[\lambda_{\max}(\Delta)\leq\rho\beta d\text{ and }\lambda_{\min}(\Delta)\geq-\rho \beta d^{2}.\]This implies that when considering untruncated Gaussians with precision matrices \(P,P^{\prime}\), we have that for all \(t,\theta\in\mathbb{R}^{d}\),

\[\|t-\theta\|_{\Sigma}^{2}-\rho\beta d^{2}\|t-\theta\|^{2}\leq\|t-\theta\|_{ \Sigma^{\prime}}^{2}\leq\|t-\theta\|_{\Sigma}^{2}+\rho\beta d\|t-\theta\|^{2}.\]

Since \(\lambda_{\min}(P)\geq\frac{\rho}{2\kappa}\), we have

\[\rho\|t-\theta\|^{2}\leq 2\kappa\|t-\theta\|_{\Sigma}^{2}.\]

Substituting in the previous inequality, we get

\[\|t-\theta\|_{\Sigma}^{2}-\rho\beta d^{2}\|t-\theta\|^{2} \leq\|t-\theta\|_{\Sigma^{\prime}}^{2}\leq\|t-\theta\|_{\Sigma}^{2}+ \rho\beta d\|t-\theta\|^{2},\] \[\implies(1-2\kappa\beta d^{2})\|t-\theta\|_{\Sigma}^{2} \leq\|t-\theta\|_{\Sigma^{\prime}}^{2}\leq(1+2\kappa\beta d)\|t- \theta\|_{\Sigma}^{2},\]

For the sake of symmetry, we will use the weaker bound of

\[(1-2\kappa\beta d^{2})\|t-\theta\|_{\Sigma}^{2}\leq\|t-\theta\|_{\Sigma^{ \prime}}^{2}\leq(1+2\kappa\beta d^{2})\|t-\theta\|_{\Sigma}^{2},\]

Setting \(\gamma=2\kappa\beta d^{2}\) completes the proof. 

**Lemma C.14**.: _Consider a bounded mean vector \(\mu\) with \(\|\mu\|_{\infty}\leq\alpha\) and precision matrix \(P\) with max eigenvalue \(\rho\) and condition number \(\kappa\)._

_For \(\gamma=O\Big{(}\min\Bigl{\{}\frac{\varepsilon}{\alpha\rho^{1/2}d^{3/2}},\frac {\varepsilon}{\alpha^{2}d^{3}\rho}\Bigr{\}}\Big{)}\), we have_

\[\log\int_{t\leq 0}\exp\left(-\frac{(1-\gamma)}{2}\|P^{\frac{1}{2}}(t- \mu)\|^{2}\right)\leq\varepsilon+\log\int_{t\leq 0}\exp\left(-\frac{1}{2}\|P^{ \frac{1}{2}}(t-\mu)\|^{2}\right).\]

Proof of Lemma C.14.: Wlog, consider \(\mu\geq 0\). The case where the entries are possibly negative follow a similar proof.

Define the integral on the LHS and RHS of the Lemma statement by \(I_{1}\) and \(I_{2}\) respectively.

By a change of variables, we set \(t^{\prime}=\sqrt{1-\gamma}(t-\mu)+\mu\) in \(I_{1}\), to get

\[I_{1}=\log\frac{1}{\sqrt{1-\gamma}}+\log\int_{t^{\prime}\leq(1- \sqrt{1-\gamma})\mu}\exp\left(-\frac{1}{2}\|P^{\frac{1}{2}}(t^{\prime}-\mu)\|^ {2}\right)\!.\]

Since \(\gamma<1\), we have \((1-\sqrt{1-\gamma})\mu<\gamma\mu\). Substituting in \(I_{1}\), and for \(\gamma=O(\varepsilon)\), we get

\[I_{1} \leq\log\frac{1}{\sqrt{1-\gamma}}+\log\int_{t^{\prime}\leq\gamma \mu}\exp\left(-\frac{1}{2}\|P^{\frac{1}{2}}(t^{\prime}-\mu)\|^{2}\right)\!,\] \[\leq O(\varepsilon)+\log\int_{t^{\prime}\leq\gamma\mu}\exp\left(- \frac{1}{2}\|P^{\frac{1}{2}}(t^{\prime}-\mu)\|^{2}\right)\!.\]

The integrating set in the above inequality can be split into two parts: one over the negative orthant (which is exactly to \(e^{I_{2}}\)) and another over the shell

\[C=\{t^{\prime}\leq\gamma\mu\}\setminus\{t^{\prime}\leq 0\}.\]

This gives

\[I_{1}\leq O(\varepsilon)+\log\!\left(e^{I_{2}}+\int_{t^{\prime}\in C}\exp \left(-\frac{1}{2}\|P^{\frac{1}{2}}(t^{\prime}-\mu)\|^{2}\right)\right)\!.\]

In the above inequality, let \(e^{I_{3}}\) denote the integral over the shell \(C\). We will now show that \(I_{3}\) satisfies

\[e^{I_{3}}\leq\varepsilon e^{I_{2}}.\]

Let \(f(x)\) denote the Gaussian density with mean \(\mu\) and precision matrix \(P\).

For a subset of co-ordinates \(S\subseteq[d],S\neq\emptyset\), and \(t\in\mathbb{R}^{d}\), let \(x_{+},x_{-}\in\mathbb{R}^{d}\) be such that

\[x_{+,S}(i)=\begin{cases}\gamma\mu_{i}&\text{ if }i\in S,\\ t_{i}&\text{ if }i\notin S,\end{cases}\quad,\quad x_{-,S}(i)=\begin{cases}-\frac{ \gamma}{\varepsilon}\mu_{i}&\text{ if }i\in S,\\ t_{i}&\text{ if }i\notin S.\end{cases}\]By the monotonicity of the Gaussian density, the integral over the shell \(C\) can be upper bounded by breaking up into a sum of integrals over lower-dimensional strips, where for a fixed subset \(S\in[d]\), the variables \(t_{S^{c}}\) are integrated over \((-\infty,\gamma\mu_{S^{c}})\), while the variables in \(S\) are fixed to \(\gamma\mu_{S}\).

This gives

\[e^{I_{3}} \leq\sum_{S\subseteq[d]}\int_{t_{S^{c}}\leq\gamma\mu_{S^{c}}}f(x_ {+,S})\prod_{i\in S}\gamma\mu_{i},\] \[\leq\sum_{S\subseteq[d]}\int_{t_{S^{c}}\leq\gamma\mu_{S^{c}}}f(x_ {+,S})(\gamma\alpha)^{|S|},\] \[\leq\sum_{k=1}^{d}\binom{d}{k}(\gamma\alpha)^{k}\max_{S\subseteq[d ]:|S|=k}\int_{t_{S^{c}}\leq\gamma\mu_{S^{c}}}f(x_{+,S}).\]

By Claim C.15, for any \(S\), and \(\gamma=O\Big{(}\min\Bigl{\{}\frac{\varepsilon}{\alpha\sqrt{\rho}},\frac{ \varepsilon}{\alpha^{2}d^{2}\rho}\Bigr{\}}\Bigr{)}\) each summand satisfies

\[f(x_{+,S})\leq 2f(x_{-,S})\]

Furthermore, for any \(S\subseteq[d]\), we have

\[\int_{t_{S^{c}}\leq\gamma\mu_{S^{c}}}f(x_{-,S})\Bigl{(}\frac{ \gamma}{\varepsilon}\alpha\Bigr{)}^{|S|}\leq e^{I_{2}}.\]

This gives

\[e^{I_{3}}\leq\sum_{k=1}^{d}d^{k}2\varepsilon^{k}e^{I_{2}}\leq 3 \varepsilon de^{I_{2}}\quad\text{ if }\varepsilon d\leq\frac{1}{3}.\]

Rescaling \(\varepsilon\leftarrow\frac{\varepsilon}{3d}\) completes the proof. 

**Claim C.15**.: _Let \(f\) be the Gaussian density with mean \(\mu\in[0,\alpha]^{d}\) and precision matrix \(P\in\mathbb{R}^{d\times d}\) with max eigenvalue \(\rho\) and condition number \(\kappa\)._

_Let \(\gamma=O\Big{(}\min\Bigl{\{}\frac{\varepsilon}{\alpha\sqrt{\rho}d},\frac{ \varepsilon}{\alpha^{2}d^{2}\rho}\Bigr{\}}\Bigr{)}\). For any subset of co-ordinates \(S\subseteq[d],S\neq\emptyset\), and \(t\in\mathbb{R}^{d}\), let \(x_{+},x_{-}\in\mathbb{R}^{d}\) be such that_

\[x_{+}(i)=\begin{cases}\gamma\mu_{i}&\text{ if }i\in S,\\ t_{i}&\text{ if }i\notin S,\end{cases},\quad x_{-}(i)=\begin{cases}-\frac{ \gamma}{\varepsilon}\mu_{i}&\text{ if }i\in S,\\ t_{i}&\text{ if }i\notin S.\end{cases}\]

_we have_

\[f(x_{+})\leq 2f(x_{-})\]

Proof.: WLOG, let \(S\) be a contiguous set such that we can separate the coordinates of \(x_{+}\) and \(x_{-}\) into disjoint sets. For the coordinates belonging to \(S\), let \(\mu_{S}\) denote the coordinates of \(\mu\) belonging to \(\mu\), and \(\mu_{S^{c}}\) the coordinates not belonging to \(S\) (similarly for \(t_{S}\) and \(t_{S^{c}}\)).

Taking the logarithm on both sides of the claimed inequality, we want to show that

\[-\frac{1}{2}\Bigl{\|}P^{\frac{1}{2}}\begin{bmatrix}\gamma\mu_{S}- \mu_{S}\\ t_{S^{c}}-\mu_{S^{c}}\end{bmatrix}\Bigr{\|}^{2}\leq-\frac{1}{2}\Bigl{\|}P^{ \frac{1}{2}}\begin{bmatrix}-\frac{\gamma}{\varepsilon}\mu_{S}-\mu_{S}\\ t_{S^{c}}^{-}-\mu_{S^{c}}\end{bmatrix}\Bigr{\|}^{2}+\log 2\]

Let \(a\) and \(b\) denote the vectors whose norms correspond to the log-densities in the claimed inequality, and let \(\delta=a-b\).

This gives

\[b=P^{\frac{1}{2}}\begin{bmatrix}-\mu_{S}(1-\gamma)\\ t_{S^{c}}-\mu_{S^{c}}\end{bmatrix},\qquad a\quad=P^{\frac{1}{2}}\begin{bmatrix}- \mu_{S}(1+\frac{\gamma}{\varepsilon})\\ t_{S^{c}}-\mu_{S^{c}}\end{bmatrix},\qquad\delta:=a-b=P^{\frac{1}{2}}\begin{bmatrix}- \mu_{S}\gamma\bigl{(}\frac{1}{\varepsilon}+1\bigr{)}\\ 0_{S^{c}}\end{bmatrix}\]We want to show that

\[-\frac{1}{2}\|b\|^{2}\leq-\frac{1}{2}\|a\|^{2}+\log 2,\] \[\Leftrightarrow\langle\delta,b\rangle+\frac{1}{2}\|\delta\|^{2}\leq \log 2.\] (33)

As \(\|P\|\leq\rho\) and \(\|\mu\|_{\infty}\leq\alpha\), we have

\[\|\delta\|_{2}^{2}\leq\rho(\alpha^{2}|S|)\gamma^{2}\bigg{(}1+\frac{1}{ \varepsilon}\bigg{)}^{2}.\]

For \(\gamma=O\Big{(}\frac{\varepsilon}{\alpha\sqrt{\rho d}}\Big{)}\), we get

\[\|\delta\|_{2}^{2}\leq\frac{1}{2}\log 2.\] (34)

Similarly, consider the inner product \(\langle\delta,b\rangle\) in Eqn (33). By the trace trick, we get

\[\langle\delta,b\rangle=\text{Tr}(\Delta P),\]

where

\[\Delta=\begin{bmatrix}-\mu_{S}(1-\gamma)\\ t_{S^{c}}-\mu_{S^{c}}\end{bmatrix}\begin{bmatrix}-\mu_{S}\gamma\big{(}\frac{1}{ \varepsilon}+1\big{)}\\ 0_{S^{c}}\end{bmatrix}^{T}\]

Notice that the diagonal elements of \(\Delta\) are all non-negative. This implies that all singular values are non-negative. The trace of \(\Delta\) is

\[\text{Tr}(\Delta)=\|\mu_{S}\|_{2}^{2}(1-\gamma)\gamma\bigg{(}\frac{1}{ \varepsilon}+1\bigg{)}.\]

Hence, by Von Neumann's trace inequality, we get

\[\langle\delta,b\rangle\leq\text{Tr}(\Delta)\text{Tr}(P)\leq\|\mu_{S}\|_{2}^{2} (1-\gamma)\gamma\bigg{(}\frac{1}{\varepsilon}+1\bigg{)}\rho d.\]

For \(\gamma=O\Big{(}\frac{\varepsilon}{\alpha^{2}|S|\rho d}\Big{)}\), this gives

\[\langle\delta,b\rangle\leq\frac{1}{2}\log 2.\] (35)

Substituting Eqn (34) and Eqn (35) in Eqn (33) completes the proof. 

**Lemma C.6** (\(W\)-net).: _Let \(\eta_{S^{c}},\eta_{S}\) be such that_

\[\|P_{S^{c}}^{*\frac{1}{2}}\eta_{S^{c}}\|\leq B_{1},\|P_{S}^{*\frac{1}{2}}\eta _{S}\|\leq B_{2},\]

_for \(B_{1},B_{2}\geq 0\)._

_Let \(A>\max\{B_{1}^{2},B_{2}^{2},\mathrm{poly}(C,\kappa)\}\). Let \(P^{*}=\Sigma^{*-1}\) be the precision matrix of \(\eta\). For a fixed matrix \(P\in\mathbb{R}^{d\times d}\) whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy \(\lambda_{\max}(P)\in[e^{-\frac{2\delta}{d}}\lambda_{\min}(P^{*}),C\lambda_{ \max}(P^{*})]\), there exists a partition \(\mathcal{I}\) of \(\mathbb{R}^{d}\) with size_

\[\bigg{(}\mathrm{poly}\bigg{(}A,\frac{1}{\varepsilon}\bigg{)}\bigg{)}^{3d}\]

_such that for each interval \(I\in\mathcal{I}\), we have one of the following:_

* _for all_ \(\theta\in I\)_,_ \(\gamma_{\theta,P}(y)<-A\)_, or_
* _for all_ \(\theta,\theta^{\prime}\in I\)_,_ \(|\gamma_{\theta,P}(y)-\gamma_{\theta^{\prime},P}(y)|\leq\epsilon\)_._Proof of Lemma c.6.: Recall that the log-likelihood ratio \(\gamma_{\theta}\) can be decomposed into the difference of two terms that depend on \(\theta\):

\[\gamma_{\theta,P}(y)-\frac{1}{2}\log\frac{|P|}{|P^{\prime}|}= g_{\theta,P}(y)-g_{\theta^{*},P^{*}}(y)+h_{\theta,P}(y)-h_{\theta^{*},P^{*} }(y).\]

Without loss of generality, consider the net for the first coordinate \(\theta_{1}\). The final net will be the intersection of the per-coordinate nets.

We will construct three partitions: the first is \(\mathcal{I}_{h,0}\) for \(h\) when \(y_{1}=0\), the second is is \(\mathcal{I}_{h,1}\) for \(h\) when \(y_{1}>0\), and the last is \(\mathcal{I}_{g}\) for \(g\) when \(y_{1}>0\). The final partition will be the intersection of these partitions.

Case 1: Net over \(h\), \(y_{1}=0\).As \(y_{1}=0\), we have \(1\in S\). For \(\theta\in\mathbb{R}^{d}\), we have

\[h_{\theta,P}(y)=\log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{\frac{1}{2}}(t-\theta _{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}})\|^{2}/2\Bigr{)},\]

By Claim c.16, if \(\theta_{1}\geq\Theta(\frac{\sqrt{C\kappa A}}{p_{1}})\), then the log-likelihood is smaller than \(-A\).

Now, consider \(\theta_{1}<O(\frac{\sqrt{C\kappa A}}{p_{1}})\). Let \(\theta^{\prime}=\theta+\alpha e_{1}\) for \(\alpha>0\). As \(h\) is monotonically decreasing per coordinate, \(\theta_{1}<\theta_{1}^{\prime}\implies h_{\theta,P}\geq h_{\theta^{\prime},P}\). We would like to now upper bound \(h_{\theta,P}\) in terms of \(h_{\theta^{\prime},P}\).

Let

\[\mu:= \theta_{S}+(P_{S})^{-1}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}}),\] \[\mu^{\prime}:= \theta_{S}^{\prime}+(P_{S})^{-1}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c }})\]

In the function \(h_{\theta}\), break the integrating set into two domains: one where \(t-\mu\) is small,

\[\Omega_{1}=\Bigl{\{}t\in\mathbb{R}^{|S|}:\|P_{S}^{\frac{1}{2}}(t-\mu)\|\leq r \Bigr{\}},\]

and another where it is large:

\[\Omega_{2}=\Bigl{\{}t\in\mathbb{R}^{|S|}:\|P_{S}^{\frac{1}{2}}(t-\mu)\|>r \Bigr{\}},\]

for some \(r>0\) that we will specify later.

Let \(I_{1}\) and \(I_{2}\) denote the integrals over \(\Omega_{1}\) and \(\Omega_{2}\) respectively.

\(I_{2}\) corresponds to the tail of an unnormalized Gaussian distribution, and hence we have

\[h_{\theta,P}(y) =\log\Bigg{(}I_{2}+\int_{t\leq 0,t\in\Omega_{1}}\exp\Bigl{(}-\|P_{S }^{\frac{1}{2}}(t-\mu)\|^{2}/2\Bigr{)}\Bigg{)},\] \[\text{where }I_{2} \leq(2\pi)^{\frac{|S|}{2}}\Big{|}P_{S}^{-\frac{1}{2}}\Big{|}e^{- r^{2}}.\]

We can simplify \(I_{2}\) be comparing \(|P|\) to \(|P^{*}|\):

\[I_{2} \leq(2\pi)^{\frac{|S|}{2}}\frac{\Big{|}P_{S}^{-1/2}\Big{|}}{ \Big{|}P_{S}^{*-1/2}\Big{|}}\Big{|}P_{S}^{*-1/2}\Big{|}e^{-r^{2}}\leq(2\pi)^{ \frac{|S|}{2}}\bigg{(}\frac{\lambda_{\max}^{*}}{\lambda_{\min}(P)}\bigg{)}^{|S| }\Big{|}P_{S}^{*-\frac{1}{2}}\Big{|}e^{-r^{2}},\] \[\leq(2\pi)^{\frac{|S|}{2}}\Big{(}\kappa e^{\frac{A}{d}}\Big{)}^{|S| }\Big{|}P_{S}^{*-\frac{1}{2}}\Big{|}e^{-r^{2}}.\]

By Lemma c.10, we have

\[(2\pi)^{\frac{|S|}{2}}\Big{|}P_{S}^{*-\frac{1}{2}}\Big{|}\leq e^{h_{\theta^{*},P^{*}}(y)+O(d+B_{2}^{2}+B_{3}^{2})}\]As we are only consider \(\theta^{\prime}\) such that \(h_{\theta^{\prime},P^{*}}(y)-A<h_{\theta^{\prime},P}(y)\), for

\[r^{2}=O(d\log\kappa+A+\log\frac{1}{\varepsilon})=O(A),\]

we have

\[I_{2}\leq\varepsilon e^{h_{\theta^{\prime},P}(y)}\]

Subsituting in \(h_{\theta,P}(y)\), we get

\[h_{\theta,P}(y)\leq\log\biggl{(}\varepsilon e^{h_{\theta^{\prime},P}(y)}+\int_ {t\leq 0,t\in\Omega_{1}}\exp\left(-\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}/2\right) \biggr{)}\]

Now consider the integral \(I_{1}=\int_{t\leq 0,t\in\Omega_{1}}\exp\left(-\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}/2 \right)\).

By Claim C.17, as \(\Omega_{1}\) is defined for \(t\) bounded by \(r\), and \(\mu-\mu^{\prime}=\alpha e_{1}\), we have

\[I_{1} \leq\exp\left(2\alpha p_{1}r+\alpha^{2}p_{1}^{2}\right)\cdot I_{ 1}^{\prime},\] \[\text{where }I_{1}^{\prime} =\int_{t\leq 0,t\in\Omega_{1}}\exp\left(-\|P_{S}^{\frac{1}{2}}(t- \mu^{\prime})\|^{2}/2\right)\leq e^{h_{\theta^{\prime},P}(y)}.\]

Substituting in the expression for \(h_{\theta,P}\), we get

\[h_{\theta,P}(y)\leq\log\Bigl{(}e^{h_{\theta^{\prime},P}(y)}\cdot\bigl{(} \varepsilon+\exp\left(2\alpha p_{1}r+\alpha^{2}p_{1}^{2}\right)\bigr{)}\Bigr{)}\]

As \(\log(\varepsilon+e^{x})\leq\varepsilon+x\) for \(x\geq 0\), we have

\[h_{\theta,P}(y)\leq h_{\theta^{\prime},P}(y)+\varepsilon+2\alpha p_{1}r+ \alpha^{2}p_{1}^{2}.\]

Setting \(\alpha=O(\frac{\varepsilon}{p_{1}r})\), we get

\[h_{\theta,P}(y)\leq h_{\theta^{\prime},P}(y)+2\varepsilon.\]

This shows that \(h_{\theta,P}\) changes by at most \(\varepsilon\) for the considered net. We need to defined the other end point for the net. By a similar argument to the positive end point, if \(\theta_{1}=-O\biggl{(}\frac{\sqrt{C\kappa\log\left(\frac{1}{\varepsilon} \right)}}{p_{1}}\biggr{)}\), the log-likelihood ratio changes by at most \(\varepsilon\) until \(\theta_{1}=-\infty\).

As we are only trying to cover \(\theta\) such that \(|\theta_{1}|\leq O(\frac{\sqrt{C\kappa A}}{p_{1}})\), this net has size

\[O\biggl{(}\frac{\sqrt{C\kappa A}}{p_{1}\alpha}\biggr{)}=O\biggl{(}\frac{\sqrt {C\kappa A}}{p_{1}}\frac{p_{1}r}{\varepsilon}\biggr{)}=\frac{A}{\varepsilon}.\]

Case 2: Net over \(h\), \(y>0\).A similar argument to Case 1 works here as well.

Case 3: Net over \(g\), \(y_{1}>0\).By Lemma C.11, if \(|\theta-\theta_{*}|_{P_{S^{c}}}>R_{1}\) for

\[R_{1}=O(\sqrt{A}),\]

then

\[g_{\theta,P}-g_{\theta^{*},P}<-A.\]

Now consider \(\theta\) such that

\[|\theta_{1}-\theta_{1}^{*}|\leq R,\]

and \(\theta,\theta^{\prime}\) such that \(\theta-\theta^{\prime}=\alpha e_{1}\).

The difference in \(g_{\theta}-g_{\theta^{\prime}}\) is

\[g_{\theta}(y)-g_{\theta^{\prime}}(y)=\eta_{S^{c}}^{T}(\Sigma_{S^{c}})^{-1}(\theta _{S^{c}}-\theta_{S^{c}}^{\prime})-\frac{1}{2}\|\theta_{S^{c}}^{*}-\theta_{S^{c}} \|_{Z_{S^{c}}}^{2}+\frac{1}{2}\|\theta_{S^{c}}^{*}-\theta_{S^{c}}^{\prime}\|_{Z _{S^{c}}}^{2},\]

The second and third terms in the RHS can bounded by observing that

\[|2\theta^{*}-\theta^{\prime}-\theta|\leq 2R_{1}=O(\sqrt{A}),|\theta^{\prime}- \theta|\leq\alpha,\]

and hence we get

\[-\frac{1}{2}\|\theta_{S^{c}}^{*}-\theta_{S^{c}}\|_{\Sigma_{S^{c}}}^{2}+\frac{ 1}{2}\|\theta_{S^{c}}^{*}-\theta_{S^{c}}^{\prime}\|_{\Sigma_{S^{c}}}^{2}\leq O (\sqrt{A})\alpha.\]

Now, for the first term in the RHS, we have

\[\|\eta_{S^{c}}\|_{P_{S^{c}}^{*}}\leq B_{1}\implies\|\eta_{S^{c}}\|\leq\frac{B _{1}}{\lambda_{\min}^{\frac{1}{2}}(P^{*})}\leq\frac{B_{1}\sqrt{\kappa}}{ \lambda_{\max}^{\frac{1}{2}}(P^{*})}.\]

This further implies that

\[\eta_{S^{c}}^{T}(\Sigma_{S^{c}})^{-1}(\theta_{S^{c}}-\theta_{S^{c}}^{\prime}) \leq\frac{B_{1}\sqrt{\kappa}}{\lambda_{\max}^{\frac{1}{2}}(P^{*})}\sqrt{p_{1}} \alpha=\mathrm{poly}(A)\alpha.\]

Setting

\[\alpha=O\bigg{(}\frac{\epsilon}{\mathrm{poly}(A)}\bigg{)},\]

we get

\[|g_{\theta}(y)-g_{\theta^{\prime}}(y)|\leq\frac{\epsilon}{d}.\]

As we are covering a set of size \(R_{1}\) using a grid size of \(\alpha\), the size of this partition is

\[O(\frac{R_{1}}{\alpha})=\frac{\mathrm{poly}(A)}{\varepsilon}.\]

**Claim C.16**.: _In the setting of Lemma C.6, we have \(\lambda_{\max}(P)\leq C\lambda_{\max}(P^{*})\). Let \(p_{1}\) denote the first diagonal element of \(P\)._

_If \(\theta_{1}\geq\Theta(\frac{\sqrt{C\kappa A}}{p_{1}})\), then the function \(h_{\theta,P}\) is such that_

\[h_{\theta,P}-h_{\theta^{*},P}<-A.\]

Proof of Claim C.16.: Recall that the function \(h_{\theta,P}\) is defined as:

\[h_{\theta,P}(y)=\log\int_{t\leq 0}\exp\Bigl{(}-\|P_{S}^{\frac{1}{2}}(t- \theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}})\|^{2}/2\Bigr{)}.\]

Consider the term \(\|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^{c}}-\theta_ {S^{c}})\|\). By the triangle inequality, we have

\[\|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{ S^{c}}-\theta_{S^{c}})\|\] \[\geq \|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}( \theta_{S^{c}}^{\prime}-\theta_{S^{c}}^{*})\|-\|P_{S}^{-\frac{1}{2}}P_{SS^{c}}( y_{S^{c}}-\theta_{S^{c}}^{*})\|,\] \[\geq \|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}( \theta_{S^{c}}^{\prime}-\theta_{S^{c}}^{*})\|-\sqrt{C\kappa A},\]where the last inequality follows as

\[\|P_{S}^{-\frac{1}{2}}P_{SS^{c}}(y_{S^{c}}-\theta_{S^{c}}^{*})\|=\|P_{S}^{-\frac{1 }{2}}P_{SS^{c}}(\eta_{S^{c}})\|\leq\|P_{S^{c}}^{\frac{1}{2}}\|\|\eta_{S^{c}}\| \leq\frac{\sqrt{C\lambda_{\max}^{*}}}{\sqrt{\lambda_{\min}^{*}}}B\leq\sqrt{C \kappa A}.\]

Similarly, the function \(g_{\theta,P}\) only considers \(\theta\) such that \(\|P_{S^{c}}^{\frac{1}{2}}(\theta_{S^{c}}-\theta_{S^{c}}^{*})\|\leq\sqrt{C \kappa A}\) ( otherwise, the log-likelihood ratio is smaller than \(-A\) by virtue of \(g_{\theta,P}\), irrespective of \(h_{\theta,P}\)). For these \(\theta\), we have

\[\|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}( \theta_{S^{c}}^{*}-\theta_{S^{c}})\|\] \[\geq \|P_{S}^{\frac{1}{2}}(t-\theta_{S})\|-\sqrt{C\kappa A},\]

which gives

\[\|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^ {c}}-\theta_{S^{c}})\|\geq \|P_{S}^{\frac{1}{2}}(t-\theta_{S})\|-2\sqrt{C\kappa A}.\]

Hence, if \(\theta_{1}\geq O(\frac{\sqrt{C\kappa A}}{p_{1}})\), then we have

\[\|P_{S}^{\frac{1}{2}}(t-\theta_{S})+(P_{S})^{-1/2}P_{SS^{c}}(y_{S^ {c}}-\theta_{S^{c}})\|\geq \Omega(\sqrt{C\kappa A})\ \forall\ t\leq 0,\]

and hence the Gaussian integral is at most \((2\pi)^{|S|/2}\Big{|}P_{S}^{-\frac{1}{2}}\Big{|}e^{-\Omega(C\kappa A)}\).

By Lemma C.10, we have \(h_{\theta^{*},P^{*}}\geq-\frac{1}{2}\log|P_{S}^{*}|-O(A)\), which gives

\[h_{\theta,P}(y)-h_{\theta^{*},P^{*}}(y) <\frac{1}{2}\log\frac{|P_{S}^{*}|}{|P_{S}|}-\Omega(C\kappa A)\] \[<\frac{d}{2}\log\frac{\lambda_{\max}^{*}}{\lambda_{\min}(P)}- \Omega(C\kappa A)<O(A\log\kappa)-\Omega(C\kappa A)=-\Omega(C\kappa A).\]

This gives a contiguous interval over \(\theta_{1}\) for which \(\gamma_{\theta,P}<-A\). 

**Claim C.17**.: _In the setting of Lemma C.6, let \(\mu,\mu^{\prime}\) be such that \(\mu-\mu^{\prime}=\alpha e_{1}\) Then, for all \(t\) such that_

\[\|P_{S}^{\frac{1}{2}}(t-\mu)\|\leq r,\]

_and \(p_{1}:=P_{11}\), we have_

\[\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}\geq -2\alpha p_{1}r-\alpha^{2}p_{1}^{2}+\|P_{S}^{\frac{1}{2}}(t-\mu^{ \prime})\|^{2}.\]

Proof of Claim c.17.: Consider the term \(\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}\).

Adding and subtracting \(\|P_{S}^{\frac{1}{2}}(t-\mu^{\prime})\|^{2}\), we get

\[\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2} =\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}-\|P_{S}^{\frac{1}{2}}(t-\mu^{ \prime})\|^{2}+\|P_{S}^{\frac{1}{2}}(t-\mu^{\prime})\|^{2},\] \[=\langle P_{S}^{\frac{1}{2}}(2t-\mu^{\prime}-\mu),P_{S}^{\frac{1} {2}}(\mu^{\prime}-\mu)\rangle+\|P_{S}^{\frac{1}{2}}(t-\mu^{\prime})\|^{2},\] \[=2\langle P_{S}^{\frac{1}{2}}(t-\mu),P_{S}^{\frac{1}{2}}(\mu^{ \prime}-\mu)\rangle-\|P_{S}^{\frac{1}{2}}(\mu-\mu^{\prime})\|^{2}+\|P_{S}^{ \frac{1}{2}}(t-\mu^{\prime})\|^{2}.\]

As \(\mu-\mu^{\prime}=\alpha e_{1}\), we have

\[\|P_{S}^{\frac{1}{2}}(\mu-\mu^{\prime})\|^{2}=\alpha^{2}p_{1}^{2}.\]

By the Cauchy-Schwartz inequality, and since \(\mu-\mu^{\prime}=\alpha e_{1}\), the inner product can be lower bounded as

\[2\langle P_{S}^{\frac{1}{2}}(t-\mu),P_{S}^{\frac{1}{2}}(\mu^{\prime}-\mu) \rangle\geq-2\|P_{S}^{\frac{1}{2}}(t-\mu)\|\|P_{S}^{\frac{1}{2}}(\mu^{\prime}- \mu)\|\geq-2\alpha p_{1}r.\]

Substituting, we get

\[\|P_{S}^{\frac{1}{2}}(t-\mu)\|^{2}\geq-2\alpha p_{1}r-\alpha^{2}p_{1}^{2}+\|P_{ S}^{\frac{1}{2}}(t-\mu^{\prime})\|^{2}.\]

This completes the proof.

**Lemma C.18**.: _Following Definition C.12 let \(\Omega_{\rho}\) be the set of precision matrices with condition number \(\kappa\) satisfying \(\lambda_{\max}(P)\in[\frac{\rho}{2},\rho]\), and let \(\widetilde{\Omega}_{\rho,\beta}\) be the quantized net with quantization level \(\beta\)._

_For any \(P\in\Omega_{\rho}\), let \(\widetilde{P}\in\widetilde{\Omega}_{\rho,\beta}\) be its element-wise rounding down. Then, for any \(\mu\in\mathbb{R}^{d}\), we have_

\[d_{TV}(\mathcal{N}(\mu;P),\mathcal{N}(\mu;\widetilde{P}))\leq O\big{(}d^{2} \beta\kappa\big{)}.\] (36)

Proof of Lemma C.18.: Let \(\Sigma=P^{-1}\) and \(\widetilde{\Sigma}=\widetilde{P}^{-1}\).

By Theorem 1.1 in [16], the TV between two Gaussians with the same mean is

\[d_{TV}(\mathcal{N}(\mu;\Sigma),\mathcal{N}(\mu;\widetilde{\Sigma}))=\Theta \Bigg{(}\min\Bigg{\{}1,\sqrt{\sum_{i}\xi_{i}^{2}}\Bigg{\}}\Bigg{)},\]

where \(\xi_{i}\) are the eigenvalues of \(\widetilde{\Sigma}^{-1}\Sigma-I_{d}\).

We can convert the bound on the eigenvalues to the Frobenius norm of \(\widetilde{\Sigma}^{-1}\Sigma-I_{d}\):

\[\sqrt{\sum_{i}\xi_{i}^{2}}\leq||\widetilde{\Sigma}^{-1}\Sigma-I_{d}\|_{F}.\]

Recall that \(\widetilde{P}\) is the rounding down per entry of \(P\). Hence,

\[\widetilde{\Sigma}^{-1}\Sigma-I_{d} =(\Sigma^{-1}-[\nu_{ij}])\Sigma-I_{d},\text{ where }0\leq\nu_{ij}<\beta\rho,\] \[=-\nu\Sigma.\]

Taking the Frobenius norm, we get

\[\|\widetilde{\Sigma}^{-1}\Sigma-I_{d}\|_{F} =\|\nu\Sigma\|_{F},\] \[\leq(d\beta\rho)(d\rho_{\max}(\Sigma))=(d\beta\rho)\bigg{(}\frac{ d}{\rho_{\min}(P)}\bigg{)},\] \[\leq(d\beta\rho)\bigg{(}\frac{d\kappa}{\rho_{\max}(P)}\bigg{)} \leq 2d^{2}\beta\kappa.\]

where the first inequality follows as each element of \(\nu\) is at most \(\beta\rho\), the second inequality follows as \(P\) has condition number \(\kappa\), and the third follows as \(P\in\Omega_{\rho}\implies\rho_{\max}(P)\geq\frac{\rho}{2}\).

This completes the proof. 

**Lemma C.7**.: _Let \(x_{1},\ldots,x_{n}\) be fixed, and \(y_{i}=\phi(W^{*}x_{i}+\eta_{i})\) for \(\eta_{i}\sim\mathcal{N}(0,\Sigma^{*})\), and \(W^{*}\in\mathbb{R}^{d\times k}\) with \(\Sigma^{*}\in\mathbb{R}^{d\times d}\) satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant \(C>0\),_

\[n=C\cdot\frac{(d^{2}+kd)}{\varepsilon^{2}}\log\frac{kd\kappa}{\varepsilon}\]

_samples suffice to guarantee that with high probability, the MLE \(\widehat{W},\widehat{\Sigma}\) satisfies_

\[\widetilde{d}\Big{(}(\widehat{W},\widehat{\Sigma}),(W^{*},\Sigma^{*})\Big{)} \leq\varepsilon.\]

Proof of Lemma c.7.: For any \(W\in\mathbb{R}^{d\times k}\), \(\Sigma\in\mathbb{R}^{d\times d}\) and a sample \((x_{i},y_{i})\), let \(p_{i,W,\Sigma}(y|x_{i})\) be the conditional distribution of \(y=\phi(Wx+\eta)\), and let \(\gamma_{i,W,\Sigma}\) be the log-likelihood ratio between \((W,\Sigma)\) and \((W^{*},\Sigma^{*})\) on this sample:

\[\gamma_{i,W,\Sigma}(y):=\log\frac{p_{i,W,\Sigma}(y\mid x_{i})}{p_{i,W^{*}, \Sigma^{*}}(y\mid x_{i})}.\]

Then

\[\mathop{\mathbb{E}}_{y}[\gamma_{i,W,\Sigma}(y)]=-KL(p_{i,W^{*},\Sigma^{*}}(y \mid x_{i})\|p_{i,W,\Sigma}(y\mid x_{i})).\]Concentration.From Lemma B.1, we see that if \(d_{TV}((W^{*},\Sigma^{*}),(W,\Sigma))\geq\varepsilon\), then for \(n\geq O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta})\),

\[\bar{\gamma}_{W,\Sigma}:=\frac{1}{n}\sum_{i=1}^{n}\gamma_{i,W,\Sigma}(y_{i})<- \frac{\varepsilon^{2}}{2},\] (37)

with probability \(1-\delta\).

Of course, whenever \(\bar{\gamma}_{W,\Sigma}<0\), the likelihood under \(W^{*},\Sigma^{*}\) is larger than the likelihood under \(W,\Sigma\). Thus, for each _fixed_\(W,\Sigma\) with \(d_{TV}((W^{*},\Sigma^{*}),(W,\Sigma))\geq\varepsilon\), maximizing likelihood would prefer \(W^{*},\Sigma^{*}\) to \(W,\Sigma\) with probability \(1-\delta\) if \(n\geq O(\frac{1}{\varepsilon^{2}}\log\frac{1}{\delta})\).

Nothing above is specific to our ReLU-based distribution. But to extend to the MLE over all \(W,\Sigma\), we need to build a net using properties of our distribution.

Building a net.First, for a given sample \(y_{i}\), let \(S_{i}\) be the set of coordinates of \(y_{i}\) that are zero, and \(S_{i}^{c}\) be its complement. Then, with high probability,

\[\|P_{S_{i}}^{*\frac{1}{2}}(\eta_{i})_{S_{i}}\|,\|P_{S_{i}^{c}}^{*\frac{1}{2}} (\eta_{i})_{S_{i}}\|\leq B=O(\sqrt{\kappa d\log n})\ \forall\ i\in[n],\]

where \(P^{*}=\Sigma^{*-1}\), and \(P_{S}^{*},P_{S^{c}}^{*}\) are the block matrices in \(P^{*}\).

Supposing the above event happens, we will construct a net over the precision matrices \(P=\Sigma^{-1}\). Note that as we are only considering matrices with bounded condition number, this is a bijective mapping.

Net over \(\Sigma^{-1}\).By Lemma C.4, any precision matrix \(P=\Sigma^{-1}\) satisfying Assumption 4.4 and whose max eigenvalue satisfies

\[\lambda_{\max}(P)\geq U\cdot\lambda_{\max}(P^{*}),\]

for \(U=O\Big{(}\frac{\kappa^{3}d^{2}n^{2}}{k^{2}}+\frac{\kappa^{2}dn\log n}{k} \Big{)}\), will have \(\bar{\gamma}_{W,P^{-1}}<0\), irrespective of \(W\).

Similarly, by Lemma C.2, any precision matrix satisfying Assumption 4.4 and whose max eigenvalue satisfies

\[\lambda_{\max}(P)\leq L\cdot\lambda_{\min}(P^{*})\]

for \(L=e^{-O(\kappa\log n)}\) has \(\gamma_{i,W,P^{-1}}\leq 0\) for all \(i\in[n]\), and hence its average \(\bar{\gamma}_{W,P^{-1}}\) is also \(<0\).

This shows that for all precision matrices \(P\) whose max eigenvalue is extremely small / large when compared to the min / max eigenvalues of \(P^{\star}\), has

\[\bar{\gamma}_{W,P^{-1}}<0,\]

irrespective of \(W\), and the MLE, which has non-negative \(\bar{\gamma}\), will never pick these \(P\).

Let \(A=\operatorname{poly}(n,d,\kappa,\frac{1}{\varepsilon})\) be large enough such that \(A>n(d\log(U\kappa)+B^{2})\), and such that it meets the requirements of Lemma C.5 and Lemma C.6.

Then, by Lemma C.5 with \(U=\operatorname{poly}(d,\kappa,n)\) and \(\log(\frac{1}{L})=\operatorname{poly}(\kappa,n)\), there exists a partition \(\mathcal{P}\) of precision matrices whose max-eigenvalue lies in \([L\cdot\lambda_{\max}(P^{*}),U\cdot\lambda_{\max}(P^{*})]\) into \(\big{(}\operatorname{poly}\big{(}d,\kappa,n,\frac{1}{\varepsilon}\big{)}\big{)} ^{d^{2}}\) cells, such that for each cell \(I\in\mathcal{P}\), and \(P,P^{\prime}\in I\), the following holds for all \(i\in[n]\) and \(W\in\mathbb{R}^{d\times k}\):

\[|\gamma_{i,W,P}(y_{i})-\gamma_{i,W,P^{\prime}}(y_{i})|\leq\frac{\varepsilon^{ 2}}{16}\] (38)

or \(\gamma_{i,W,P}(y_{i})<-A\).

Using Lemma C.18, we also have that for all \(W\),

\[d_{TV}((W,P),(W,P^{\prime}))\leq\frac{\varepsilon^{2}}{16}.\] (39)We can choose a net \(N\) consisting of precision matrices from each cell in \(\mathcal{P}\). This net has size

\[\log\lvert N\rvert\lesssim d^{2}\log\biggl{(}\frac{d\kappa n}{\varepsilon}\biggr{)}.\]

This gives a sufficient net over the precision matrices.

Now we will construct a net over \(W\) for each precision matrix in the net.

\(W\)-net.Now, for each \(\widetilde{P}\in N_{P}\), by Lemma C.6, for each \(i\in[n]\), there exists a partition \(\mathcal{P}_{\widetilde{P},i}\) of \(\mathbb{R}^{d}\) into \(\bigl{(}\operatorname{poly}\bigl{(}d,k,\kappa,n,\frac{1}{\varepsilon}\bigr{)} \bigr{)}^{d}\) cells such that for each cell \(I\in\mathcal{P}_{\widetilde{P},i}\), and \(W,W^{\prime}\in I\), one of the following holds:

\[\Bigl{|}\gamma_{i,W,\widetilde{P}}(y_{i})-\gamma_{i,W^{\prime},\widetilde{P}}( y_{i})\Bigr{|}\leq\frac{\varepsilon^{2}}{16}\] (40)

or \(\gamma_{i,W,\widetilde{P}}(y_{i})<-A\).

Let \(W_{j}\) be the \(j\)-th row of \(W\). The individual partitions \(\mathcal{P}_{\widetilde{P},i}\) on \(\langle x_{i},W_{j}\rangle\) induce a partition \(\mathcal{P}_{\widetilde{P},i,j}\) on \(\mathbb{R}^{k}\), where \(W_{j},W^{\prime}_{j}\) lie in the same cell of \(\mathcal{P}_{\widetilde{P},i,j}\) if \(\langle x_{i},W_{j}\rangle\) and \(\langle x_{i},W^{\prime}_{j}\rangle\) are in the same cell of \(\mathcal{P}_{\widetilde{P},i}\) for all \(i\in[n]\). Since \(\mathcal{P}_{\widetilde{P},i,j}\) is defined by \(n\) sets of \(\bigl{(}\operatorname{poly}\bigl{(}d,k,\kappa,n,\frac{1}{\varepsilon}\bigr{)} \bigr{)}\) parallel hyperplanes in \(\mathbb{R}^{k}\), the number of cells in \(\mathcal{P}_{\widetilde{P},i,j}\) is

\[\biggl{(}\operatorname{poly}\biggl{(}d,k,\kappa,n,\frac{1}{\varepsilon}\biggr{)} \biggr{)}^{k}.\]

As there are \(d\) rows in \(W\), we can intersect \(\mathcal{P}_{\widetilde{P},i,j}\) over \(j\in[d]\), which induces \(\operatorname{poly}(d,k,\kappa,n,\frac{1}{\varepsilon})^{kd}\) cells in \(\mathbb{R}^{k}\). We choose a net \(N_{\widetilde{P}}\) to contain, for each cell in \(\bigcap_{j\in[d]}\mathcal{P}_{\widetilde{P},i,j}\), the \(W\) in the cell maximizing \(d_{TV}((W^{*},P^{*}),(W,\widetilde{P}))\). This has size

\[\log\bigl{\lvert}N_{\widetilde{P}}\bigr{\rvert}\lesssim kd\log\biggl{(}\frac{ \kappa dkn}{\varepsilon}\biggr{)}.\]

Proving MLE works.By (37), for our \(n\geq O\Bigl{(}\frac{(kd+d^{2})}{\varepsilon^{2}}\log\frac{kd\kappa}{ \varepsilon}\Bigr{)}\), we have with high probability that

\[\overline{\gamma}_{W,P}\leq-\frac{\varepsilon^{2}}{2},\]

for all \(P\in N\) and for all \(W\in N_{\widetilde{P}}\) with \(d_{TV}((W^{*},P^{*}),(W,P))\geq\varepsilon\). Suppose that both this happens, and

\[\lVert P_{S_{i}}^{*\frac{1}{2}}(\eta_{i})_{S_{i}}\rVert,\lVert P_{S_{i}^{* \frac{1}{2}}}^{*\frac{1}{2}}(\eta_{i})_{S_{i}}\rVert\leq B=O(\sqrt{\kappa d \log n})\ \forall\ i\in[n].\]

We claim that the MLE \(\widehat{W},\widehat{\Sigma}\) must have \(d_{TV}((W^{*},\Sigma^{*}),(\widehat{W},\widehat{\Sigma}))<\frac{17}{16}\varepsilon\).

Consider any \(W\in\mathbb{R}^{d\times k}\) and \(P\in\mathbb{R}^{d\times d}\) with \(d_{TV}((W^{*},P^{*}),(W,P))\geq\frac{17}{16}\varepsilon\). Using our net on precision matrices, we can find \(\widetilde{P}\in N\) such that

\[d_{TV}((W^{*},P^{*}),(W,\widetilde{P}))\geq d_{TV}((W^{*},P^{*}),(W,P))-d_{TV} ((W,P),(W,\widetilde{P})).\]

Recall that we are only currently considering \(W,P\) such that \(d_{TV}((W^{*},P^{*}),(W,P))\geq\frac{17}{16}\varepsilon\). By Eqn (39), we have \(d_{TV}((W,P),(W,\widetilde{P}))\leq\frac{\varepsilon^{2}}{16}\), which gives

\[d_{TV}((W^{*},P^{*}),(W,\widetilde{P}))\geq\varepsilon\frac{17}{16}-\frac{ \varepsilon^{2}}{16}\geq\varepsilon.\]

Now, for this \(\widetilde{P}\), we can find a \(\widetilde{W}\in N_{\widetilde{P}}\), and by our choice of \(N_{\widetilde{P}}\), we know that

\[d_{TV}((W^{*},P^{*}),(\widetilde{W},\widetilde{P}))\geq d_{TV}((W^{*},P^{*}),( W,\widetilde{P}))\geq\varepsilon,\]and by (37), we have \(\bar{\gamma}_{\widetilde{W},\widetilde{P}}\leq-\frac{\varepsilon^{2}}{2}\).

Now we consider two cases. In the first case, there exists \(i\) with \(\gamma_{i,W,P}(y_{i})<-A\). Then

\[\overline{\gamma}_{W,P}=\frac{1}{n}\sum_{i}\gamma_{i,W,P}(y_{i})\leq-\frac{A}{ n}+B^{2}/2<0.\]

Otherwise, by Eqn (38) and Eqn (40), we have

\[\overline{\gamma}_{W,P} \leq\overline{\gamma}_{\widetilde{W},\widetilde{P}}+\Big{|} \overline{\gamma}_{\widetilde{W},\widetilde{P}}-\overline{\gamma}_{W, \widetilde{P}}\Big{|}+\Big{|}\overline{\gamma}_{W,\widetilde{P}}-\overline{ \gamma}_{W,P}\Big{|},\] \[\leq-\frac{\varepsilon^{2}}{2}+\max_{i}\Bigl{|}\overline{\gamma} _{i,\widetilde{W},\widetilde{P}}-\overline{\gamma}_{i,W,\widetilde{P}}\Big{|} +\max_{i}\Bigl{|}\overline{\gamma}_{i,W,\widetilde{P}}-\overline{\gamma}_{i,W,P}\Bigr{|},\] \[\leq-\frac{\varepsilon^{2}}{2}+\frac{\varepsilon^{2}}{16}+\frac{ \varepsilon^{2}}{16}<0.\]

In either case, \(\overline{\gamma}_{W,P}<0\) and the likelihood under \(w^{*}\) exceeds that under \(w\). Hence the MLE \(\widehat{w}\) must have \(d_{TV}(w^{*},w)\leq\frac{17}{16}\varepsilon\). Rescaling \(\varepsilon\) gives the conclusion of the Lemma.

**Lemma C.8**.: _Let \(\{x_{i}\}_{i=1}^{n}\) be i.i.d. random variables such that \(x_{i}\sim\mathcal{D}_{x}\)._

_Let \(P^{*}:=\Sigma^{*-1}\). Let \(\lambda_{\min}^{*},\lambda_{\max}^{*}\) be the minimum and maximum eigenvalues of \(P^{*}\). For \(0<L<U\), let \(\Omega\) denote the following set of precision matrices_

\[\Omega:=\bigg{\{}P\in\mathbb{R}_{+}^{d\times d}:\frac{\lambda_{\max}(P)}{ \lambda_{\min}(P)}\leq\kappa\text{ and }\lambda_{\max}(P)\in[L\cdot\lambda_{\min}^{*},U \cdot\lambda_{\max}^{*}]\bigg{\}}.\]

_Then, for a sufficiently large constant \(C>0\), and for_

\[n=C\cdot\bigg{(}\frac{kd+d^{2}}{\varepsilon^{2}}\bigg{)}\log\biggl{(}\frac{kd \kappa}{\varepsilon}\log\biggl{(}\frac{U}{L}\biggr{)}\bigg{)},\]

_we have:_

\[\Pr_{x_{i}\sim\mathcal{D}_{x}}\Biggl{[}\sup_{W\in\mathbb{R}^{d\times k},P\in \Omega}\Bigl{|}\widetilde{d}((W,P),(W^{*},P^{*}))-d((W,P),(W^{*},P^{*}))\Bigr{|} >\varepsilon\Biggr{]}\leq e^{-\Omega(n\varepsilon^{2})}.\]

Proof of Lemma c.8.: For \(P=\Sigma^{-1}\) and \(P^{*}=\Sigma^{*-1}\), let

\[f(W,P):=d((W,\Sigma),(W^{*},\Sigma^{*}))\]

and

\[f_{n}(W,P):=\widetilde{d}((W,\Sigma),(W^{*},\Sigma^{*}))=\frac{1}{n}\sum_{i}[ d_{TV}(p_{W,P}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i}))].\]

Since the function is bounded, for any fixed \(W,P\), the Chernoff bound gives

\[\Pr[|f_{n}(W,P)-f(W,P)|>\alpha]\leq e^{-2n\alpha^{2}}.\] (41)

for any \(\alpha>0\). The challenge lies in constructing a net to be able to union bound over \(\mathbb{R}^{k}\) without assuming any bound on \(W\) or the covariate \(x\). As before, we do so by constructing a "ghost" sample, symmetrizing, and constructing a net based on these samples.

Ghost sample.First, we construct a "ghost" dataset \(D_{x}^{\prime}\) consisting of \(n\) fresh samples IID samples \(\{x_{i}^{\prime}\}_{i\in[n]}\) of \(\mathcal{D}_{x}\). This gives another metric

\[f_{n}^{\prime}(W,P):=\widetilde{d}^{\prime}((W,\Sigma),(W^{*},\Sigma^{*}))= \frac{1}{n}\sum_{i}[d_{TV}(p_{W,P}(y|x_{i}^{\prime}),p_{W^{*},P^{*}}(y|x_{i}^{ \prime}))].\]

Similar to the proof in Lemma 4.3, it is sufficient to consider the difference between \(f_{n}(W,P)\) and \(f_{n}^{\prime}(W,P)\) i.e.,

\[\Pr\biggl{[}\sup_{W,P}|f(W,P)-f_{n}(W,P)|>\varepsilon\biggr{]}\leq 2\Pr\biggl{[} \sup_{W,P}|f_{n}(W,P)-f_{n}^{\prime}(W,P)|>\varepsilon/2\biggr{]}.\] (42)Symmetrization.Since \(D_{x}\) and \(D^{\prime}_{x}\) each have \(n\) independent samples, we could instead draw the datasets by first sampling \(2n\) elements \(x_{1},\ldots,x_{2n}\) from \(\mathcal{D}_{x}\), then randomly partition this sample into two equal datasets. Let \(s_{i}\in\{\pm 1\}\) so \(s_{i}=1\) if \(z_{i}\) lies in \(D^{\prime}_{x}\) and \(-1\) if it lies in \(D_{x}\). Then

\[f_{n}(W,P)-f^{\prime}_{n}(W,P)=\frac{1}{n}\sum_{i=1}^{2n}s_{i}\cdot d_{TV}(p_{W,P}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i})).\]

For a fixed \(W,P\) and \(x_{1},\ldots,x_{2n}\), the random variables \((s_{1},\ldots,s_{2n})\) are a permutation distribution, so negatively associated. Then the variables \(s_{i}\cdot d_{TV}(p_{W,P}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i}))\) are monotone functions of \(s_{i}\), so also negatively associated. They are also bounded in \([-1,1]\). Hence we can apply a Chernoff bound:

\[\Pr[|f_{n}(W,P)-f^{\prime}_{n}(W,P)|>\varepsilon]<e^{-n\varepsilon^{2}/2},\] (43)

for any fixed \(W,P\).

Constructing a net.We will first construct a net over the precision matrices \(P\) (independent of \(W\)), and then for each element in the \(P\)-net, we will construct a net over \(W\).

Net over \(\Sigma^{-1}\).In the following, \(\lambda_{\max}(P)\) denotes max eigenvalue of a matrix \(P\), \(\lambda_{\min}(P)\) denotes the min eigenvalue, and \(\lambda_{i}(P)\) denotes the \(i\)-th eigenvalue, in decreasing order.

In order to construct the net over the precision matrices, we will consider geometrically spaced values of \(\lambda\in[L\cdot\lambda_{\min}(P^{*}),U\cdot\lambda_{\max}(P^{*})]\), and for each \(\lambda\), we will construct a net over matrices that have max eigenvalue \(\leq\lambda\).

Now consider \(\lambda>0\) that lies in the following discrete set:

\[\big{\{}\lambda_{\min}(P^{*})2^{j},j\in\lceil\log_{2}(\kappa\frac{U}{L}) \rceil\big{\}}\]

This set is a geometric partition over the possible max eigenvalues that the MLE can return.

Following definition C.12, let \(\Omega_{\lambda}\) denote the subset of positive definite matrices in \(\mathbb{R}^{d\times d}\) that have condition number \(\kappa\) and max-eigenvalue in \(\left\lceil\frac{\lambda}{2},\lambda\right\rceil\). Similarly, following Definition C.12, let \(\widetilde{\Omega}_{\lambda,\beta}\) denote the gridded version of \(\Omega_{\lambda}\), where entries in the matrix are multiples of \(\lambda\beta\).

For any \(P\in\Omega_{\lambda}\), let \(\widetilde{P}\in\widetilde{\Omega}_{\lambda,\beta}\) be the matrix obtained by rounding down every element in \(P\).

By the Data Processing Inequality, for any \(W\in\mathbb{R}^{d\times k}\), we have

\[d_{TV}\Big{(}p_{W,P}(y|x),p_{W,\widetilde{P}}(y|x)\Big{)}\leq d_{TV}(\mathcal{ N}(Wx;P),\mathcal{N}(Wx;\widetilde{P})).\]

By Lemma C.18, we can upper bound the RHS of the above inequality by

\[d_{TV}(\mathcal{N}(Wx;P),\mathcal{N}(Wx;\widetilde{P}))\leq O(d^{2}\beta \kappa).\]

Setting

\[\beta=O\Big{(}\frac{\varepsilon}{d^{2}\kappa}\Big{)},\]

we have a partition of size \(O\Big{(}(d^{2}\kappa/\varepsilon)^{d^{2}}\Big{)}\) per \(\lambda\) such that:

\[d_{TV}\Big{(}p_{W,P}(y|x),p_{W,\widetilde{P}}(y|x)\Big{)}\leq O(\varepsilon).\]

We will now construct a net over \(W\), so as to show Eqn (43) for all \(W,P\).

\(W\)-net.By repeated triangle inequalities, we have

\[|f_{n}(W,P)-f^{\prime}_{n}(W,P)|\leq\Big{|}f_{n}(W,P)-f_{n}(W, \widetilde{P})\Big{|}+\Big{|}f_{n}(W,\widetilde{P})-f^{\prime}_{n}(W, \widetilde{P})\Big{|}+\Big{|}f^{\prime}_{n}(W,\widetilde{P})-f^{\prime}_{n}(W, P)\Big{|}.\]

Using the cover over \(P\), the first and last term on the RHS are \(O(\varepsilon)\). This gives

\[|f_{n}(W,P)-f^{\prime}_{n}(W,P)|\leq O(\varepsilon)+\Big{|}f_{n}(W, \widetilde{P})-f^{\prime}_{n}(W,\widetilde{P})\Big{|}.\] (44)For a fixed \(\widetilde{W},\widetilde{P}\), we will have (43) using a Chernoff bound. Since \(\widetilde{P}\) is already finite, we will now construct a net over \(W\)_for each_\(\widetilde{P}\).

It is sufficient to bound \(d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{\prime},\widetilde{P}}(y|x_{i}))\) as the triangle inequality implies that this is larger than the RHS above.

We want, for any \(W,W^{\prime}\) in a cell,

\[|d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i}))-d_{TV}(p_{W^{ \prime},\widetilde{P}}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i})|\leq O(\varepsilon).\]

for all \(i\in[2n]\). It is also sufficient to bound \(d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{\prime},\widetilde{P}}(y|x_{i}))\) as the triangle inequality implies that this is larger than the left hand side above.

Lemma C.19 implies that we can find \(\mathcal{O}\!\left(\frac{d}{\epsilon^{3/2}}\sqrt{\log(\frac{2d}{\epsilon})}\right)\) per row of \(W\) such that for any \(W,W^{\prime}\) in a cell, either

\[d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i}))\geq d_{TV}(p_{W,\widetilde{P}}(y_{j}|x_{i}),p_{W^{*},P^{*}}(y_{j}|x_{i})\geq 1-\varepsilon\]

which implies

\[d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{\prime},\widetilde{P}}(y|x_{i})\leq\epsilon\]

or

\[d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{\prime},\widetilde{P}}(y|x_{i})\leq \sum_{j}d_{TV}(p_{W,\widetilde{P}}(y|x_{i})_{j}|z_{i},p_{W^{\prime},\widetilde {P}}(y_{j}|x_{i}))\leq\epsilon/d.\]

Therefore, for each \(i\) regardless of the value of \(W^{*}_{j}z_{i}\) there are at most \(\mathcal{O}\!\left(\frac{d}{\epsilon^{3/2}}\sqrt{\log(\frac{2d}{\epsilon})}\right)\) partitioning hyperplanes.

We then take the intersection of all \(2n\) partitions (for each data point \(z_{i}\)). The cells of this partition are defined by \(2n\) sets of \(\mathcal{O}\!\left(\frac{d}{\epsilon^{3/2}}\sqrt{\log(\frac{2d}{\epsilon})}\right)\) parallel hyperplanes. Since \(z\in\mathbb{R}^{k}\), the number of cells is at most \(\mathcal{O}\!\left(\left(\frac{nd}{\epsilon^{3/2}}\sqrt{\log(\frac{2d}{\epsilon })}\right)^{k}\right)\).

Hence the total number of cells for \(d\) rows is at most \(\mathcal{O}\!\left(\left(\frac{nd}{\epsilon^{3/2}}\sqrt{\log(\frac{2d}{\epsilon })}\right)^{dk}\right)\).

Putting everything together.Finally, for any \(W\in\mathbb{R}^{d}\) let \(\widetilde{W}\in N_{\widetilde{P}}\) be the representative of its cell. Recall that each representative \(\widetilde{P}\) of \(P\) induces a different cover \(N_{\widetilde{P}}\) over \(W\). Let \(N\) be the net over the precision matrices \(P\).

By definition of the cells,

\[\big{|}d_{TV}(p_{W,\widetilde{P}}(y|x_{i}),p_{W^{*},P^{*}}(y|x_{i}))-d_{TV}(p_ {\widetilde{W},\widetilde{P}}(y|x_{i}),p_{W^{*},\Sigma^{*}}(y|x_{i}))\big{|}< O(\varepsilon).\]

for all \(i\in[2n]\). Thus

\[\Big{|}\Big{(}f^{\prime}_{n}(W,\widetilde{P})-f_{n}(W,P)\Big{)}-\Big{(}f^{ \prime}_{n}(\widetilde{W},\widetilde{P})-f_{n}(\widetilde{W},P)\Big{)}\Big{|} \leq O(\varepsilon).\]

and so

\[\Pr[ \sup_{W\in\mathbb{R}^{d\times k},P\in\mathbb{R}^{d\times d}}|f^{ \prime}_{n}(W,P)-f_{n}(W,P)|>\varepsilon]\] \[\leq\Pr\!\left[\max_{w\in N_{P},P\in N}\!|f^{\prime}_{n}(W,P)-f_ {n}(W,P)|>\frac{\varepsilon}{4}\right]\] \[\leq e^{\log|N|+\log|N_{P}|-(\frac{\varepsilon}{4})^{2}n/2}\]

As there are \(\log\kappa\frac{U}{L}\) partitions over \(P\) (corresponding to the maximum possible eigenvalue of \(P\)), each with \((O(\frac{d^{2}\kappa}{\varepsilon}))^{d^{2}}\) elements, we have

\[\log|N|\lesssim d^{2}\log\!\left(\frac{d^{2}\kappa}{\varepsilon}\right)+\log \log\!\left(\kappa\frac{U}{L}\right)\!.\]and each cover \(N_{P}\) over \(W\) has size

\[\log|N_{P}|=2kd\log\Biggl{(}\frac{d}{\epsilon^{3/2}}\sqrt{\log\biggl{(}\frac{2d}{ \epsilon}\biggr{)}}\Biggr{)}.\]

This implies that

\[n=C\cdot\biggl{(}\frac{kd+d^{2}}{\varepsilon^{2}}\biggr{)}\log\biggl{(}\frac{kd \kappa}{\varepsilon}\log\biggl{(}\frac{U}{L}\biggr{)}\biggr{)},\]

suffices for

\[\Pr[\sup_{W,P}f^{\prime}_{n}(W,P)-f_{n}(W,P)>\varepsilon]<e^{-\Omega( \varepsilon^{2}n)}.\]

**Lemma C.19**.: _Let \(y=\phi(\mu^{*}+\eta_{\sigma^{*}})\) where \(\mu^{*},\sigma^{*}\) are fixed, and \(y_{\mu,\sigma}=\phi(\mu+\eta_{\sigma})\). We partition the space \(\mathbb{R}\) of \(\mu\) s.t. for \(\mu,\mu^{\prime}\) in a cell, either_

\[d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{\prime},\sigma}(y))\leq\epsilon/2d.\]

_or_

\[d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))\geq 1-\epsilon\quad\text{ and } \quad d_{TV}(p_{\mu^{\prime},\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))\geq 1-\epsilon.\]

_Then the number of cells is at most \(\mathcal{O}(\frac{d}{\epsilon^{3/2}}\sqrt{\log(\frac{2d}{\epsilon})})\)._

Proof.: In one dimension

\[d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))=d_{TV}(p_{c\mu,c\sigma}(y),p_{c\mu^{*},c\sigma^{*}}(y))\]

where \(c\) is a constant. So, we can assume WLOG that \(\sigma^{*}=1\). The number of cells in the grid only depends on \(\sigma/\sigma^{*}\).

Now, we show that, regardless of the value of \(\mu^{*}\) we only need to make a grid on a segment of length at most \(3\max(\sigma,1)\sqrt{\log(2d/\epsilon)}\). This is because for any \(\mu\) outside the ranges specified below the \(d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))\geq 1-\epsilon\).

* If \(\mu^{*}\leq-\sqrt{\log(2d/\epsilon)}\) and for any \(\mu\) such that \(\mu\geq\sigma\sqrt{\log(2d/\epsilon)}\), the \(d_{TV}(p_{\mu^{*},\sigma^{*}}(y),p_{\mu,\sigma}(y))\geq\) the difference in the probabilities at \(0\) which is bigger than \(1-\epsilon\).
* If \(0\geq\mu^{*}\geq-\sqrt{\log(2d/\epsilon)}\) and for any \(\mu\) s.t. \(\mu\geq\max(\sigma,1)\sqrt{\log(2d/\epsilon)}\), the \(d_{TV}(p_{\mu^{*},\sigma^{*}}(y),p_{\mu,\sigma}(y))\) is the same as in the linear case and since, \(\mu-\mu^{*}\geq\max(\sigma,1)\sqrt{\log(2d/\epsilon)}\), the \(d_{TV}(p_{\mu^{*},\sigma^{*}}(y),p_{\mu,\sigma}(y))\geq 1-\epsilon\).
* If \(0\leq\mu^{*}\leq\sqrt{\log(2d/\epsilon)}\), for any \(\mu\) s.t. \(\mu-\mu^{*}\geq\max(\sigma,1)\sqrt{\log(2d/\epsilon)}\), the \(d_{TV}(p_{\mu^{*},\sigma^{*}}(y),p_{\mu,\sigma}(y))\geq 1-\epsilon\).
* If \(\mu^{*}\geq\sqrt{\log(2d/\epsilon)}\) then for any \(\mu\) s.t. \(\mu-\mu^{*}\geq\max(\sigma,1)\sqrt{\log(2d/\epsilon)}\) using the same argument as above, we have, the \(d_{TV}(p_{\mu^{*},\sigma^{*}}(y),p_{\mu,\sigma}(y))\geq 1-\epsilon\). Moreover, this is also true for \(\mu\) s.t. \(-\sigma\sqrt{\log(2d/\epsilon)}\leq\mu\leq\mu^{*}-\max(\sigma,1)\sqrt{\log(2 d/\epsilon)}\). Therefore, in this case, we have an additional cell.

In addition to the above, for any \(\mu,\mu^{\prime}\leq-\sigma\sqrt{\log(2d/\epsilon)}\), the \(d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{\prime},\sigma}(y))\leq\epsilon/2d\) since both \(y_{\mu,\sigma},y_{\mu^{\prime},\sigma}\) are only non-zero with probability at most \(\epsilon/2d\). Therefore, for all the above cases, we only need to partition a segment of length at most \(3\max(\sigma,1)\sqrt{\log(2d/\epsilon)}\)

Moreover, for \(\sigma\) sufficiently small we can do better. We only need to partition a space of \(\sigma\sqrt{\log(2d/\epsilon)}\). This is primarily because when \(\sigma\) sufficiently small, for any \(\mu\) in the linear case we have that \(d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))\geq 1-\epsilon\).

It is easy to see that \(d_{TV}(p_{\mu,\sigma}(y),p_{0,1}(y))\geq d_{TV}(p_{0,\sigma}(y),p_{0,1}(y))\). The PDFs of \(\mathcal{N}(0,\sigma)\) and \(\mathcal{N}(0,1)\) intersect at \(x=\pm\sigma\sqrt{\frac{\log(1/\sigma^{2})}{1-\sigma^{2}}}\). To show that \(d_{TV}(p_{0,\sigma}(y),p_{0,1}(y))\geq 1-\epsilon\), it is now sufficient to show that

\[1-2\Phi(-|x|/\sigma)\geq 1-\epsilon\quad\text{ and }\quad 1-2\phi(-|x|)\leq\epsilon,\]

where \(\Phi(x)\) is the CDF of the standard normal distribution. By using classical bounds on \(\Phi(x)\), we have that

\[\Phi(-|x|/\sigma)\leq\frac{\exp^{-x^{2}/2\sigma^{2}}}{|x|/\sigma}=\frac{\exp ^{-\frac{\log(1/\sigma^{2})}{2(1-\sigma^{2})}}}{|x|/\sigma}\]

which is \(\leq\epsilon/2\) if \(\sigma^{2}\leq\epsilon/2\). And,

\[\Phi(-|x|)\geq\frac{|x|\exp^{-x^{2}/2}}{x^{2}+1}\]

which is \(\geq(1-\epsilon)/2\) if

When \(\mu^{*}\leq-\sqrt{\log(2d/\epsilon)}\) the same argument as above shows that if \(\mu>\sigma\sqrt{\log(2d/\epsilon)}\) then the \(d_{TV}(p_{\mu^{*},\sigma^{*}}(y),p_{\mu,\sigma}(y))\geq\) the difference in the probabilities at \(0\) which is bigger than \(1-\epsilon\).We consider the case where \(\mu^{*}\geq-\sqrt{\log(2d/\epsilon}\).

Since, when \(\sigma\) is small, for any \(\mu\) in the linear case the TV distance is large it is sufficient to have \(\mu\) large enough so that the intersection of the PDFs are positive and we are in the linear case.

The point of intersection assuming \(\sigma^{*}=1\) is given by

\[x=\frac{\mu_{1}-\mu_{2}/\sigma^{2}\pm\sqrt{(\mu_{1}-\mu_{2})^{2}/\sigma^{2}+ \left(\frac{1}{\sigma^{2}}-1\right)\log(\sigma^{2})}}{\frac{1}{\sigma^{2}}-1}\]

which is positive whenever

\[\mu_{2}\geq\sigma\sqrt{\log(1/\sigma^{2}+\mu_{1}^{2}}\geq\sigma\sqrt{\log(2d/ \epsilon}.\]

For the rest of the space, we partition \(\mu\in\mathbb{R}^{k}\) s.t. for any \(\mu,\mu^{\prime}\) in a cell,

\[|\mu-\mu^{\prime}|\leq\sigma\epsilon/2d.\]

This implies that for any \(\mu,\mu^{\prime}\) in a cell, either,

\[d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{\prime},\sigma}(y))\leq\epsilon/2d\]

or

\[d_{TV}(p_{\mu,\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))\geq 1-\epsilon\text{ and }d_{TV}(p_{\mu^{ \prime},\sigma}(y),p_{\mu^{*},\sigma^{*}}(y))\geq 1-\epsilon.\]

Then the number of cells is the max of \(\mathcal{O}(d\sqrt{\log(2d/\epsilon)}/\epsilon)\) (when \(\sigma\) small) or \(\mathcal{O}(d\sqrt{\log(2d/\epsilon)}/\sigma\epsilon)\leq d\sqrt{\log(2d/ \epsilon)}/\epsilon^{3/2}\) (when \(\sigma\) large). 

## Appendix D Proof of composition of layers

Proof.: We can use the triangle inequality to compose our single layer guarantees. Suppose, for layer \(j\) and \(j+1\) we have

\[d_{TV}(X^{j},\hat{X}^{j})\leq\epsilon/2\quad\text{and}\]

\[d_{TV}(\phi(\hat{W}^{j}_{MLE}X_{j}+\eta_{j}),\phi(W^{j}X_{j}+\eta_{j}))\leq \epsilon/2\]

then,

\[d_{TV}(\phi(\hat{W}^{j}_{MLE}\hat{X}_{j}+\eta_{j}),\phi(W^{j}X_{j}+\eta_{j}))\]

\[\leq d_{TV}(\phi(\hat{W}^{j}_{MLE}\hat{X}_{j}+\eta_{j}),\phi(\hat{W}^{j}_{MLE} X_{j}+\eta_{j}))\]

\[+d_{TV}(\phi(\hat{W}^{j}_{MLE}X_{j}+\eta_{j}),\phi(W^{j}X_{j}+\eta_{j}))\]

\[\leq\epsilon\]

where we use the fact that \(d_{TV}(f(X),f(Y))\leq d_{TV}(X,Y)\).

## Appendix E Simulations

### Additional Simulations

In this section, we provide additional simulations to supplement some of the discussion in Section 5.

### Simulation Details

#### e.2.1 Figure 2

In these experiment, we set \(d=1\), and plot the results for various values of the number of samples \(n\) in Figure 1(a) and various values of the input dimension \(k\) in Figure 1(b). For each plot, we fix the true \(\sigma^{*}=1\) and the \(w^{*}=\mathds{1}_{k\times 1}\). In each case the MLE is solved via gradient descent with backtracking line search, and we check a first order condition \(\|\nabla_{w,\sigma}\log p_{w,\sigma}((y\mid x))\|^{2}<\delta=10^{-3}\) as the exit condition. We verify that increasing or decreasing \(\delta\) by one order of magnitude makes no difference to the Figure.

The expected total variation distance for the two distributions is calculated as follows. We sample \(x\) according to the true distribution (in this case either Laplace or Normal). Then we compute \(d_{TV}(p_{\hat{w},\hat{\sigma}}(y\mid x),p_{w,\sigma}(y\mid x))\) via the MATLAB integral function which uses vectorized adaptive quadrature. We repeat this a total of \(100\) times and take the average to compute our expected total variation. We then repeat the entire process \(2000\) times, each time optimizing to find an MLE, and then compute its average total variation distance. Lines indicate the average of these experiments, and the error bars, (not easily visible due to their size) indicates one standard error.

#### e.2.2 Figure 3

In these experiments we fix \(d=3\) to retain reasonable complexity for computing the TV distance, and take input dimension \(k=1\) with deterministic \(x\) in order to compare with [35]. In Figure 2(a) we fix \(\Sigma^{*}=I_{d}\) and take let \(W^{*}=b\mathds{1}_{d\times 1}\), where \(b\) will vary across our experiments. We set \(n=10000\). In Figure 2(b) we set \(n=5000\) and adjust \(\Sigma\) such that one diagonal entry is \(\kappa^{1/2}\), and the other is \(\kappa^{-1/2}\), making the total condition number \(\kappa\).

In both of these experiments, we restrict the MLE computation to be over diagonal \(\Sigma\) only. This is not because computation of the MLE is too difficult, but rather because computing the TV distance is greatly simplified in this case. The algorithm of Wu et al. is hence modified to use the knowledge that the output must be diagonal. This is simply done, because the procedure of Wu et al. essentially first

Figure 4: (a) Plots Pinsker’s upper bound on the TV distance as \(d\) gets large. We set \(\Sigma^{*}=I_{d}\) and \(W^{*}=\mathds{1}_{d\times 1}\), thus setting input dimension \(k=1\). \(n=5000\) samples are taken. As we might expect, the upper bound is increasing in \(d\). each point is determined by 2000 samples. (b) Plot of TV vs. \(n\) for additional distributions of \(x\). All three distributions follow roughly the same trend, each point is determined by 2500 samples.

estimates the diagonal entries of the matrix as if it were diagonal and then computes the correlations. Removing this second phase allows us to achieve our goal.

Since \(x\) is deterministic, we do not need to consider randomness in computing the expected TV distance, though other challenges remain. Since our distribution is degenerate, we must be very careful in computing the TV distance in higher dimensions. Specifically, in the diagonal case, the TV may be written as:

\[d_{TV}\Big{(}(\widehat{W},\widehat{\Sigma}),(W^{*},\Sigma^{*}) \Big{)} =\frac{1}{2}\int_{\mathbb{R}^{d}_{\geq 0}}\Big{|}p_{\widehat{W}, \widehat{\Sigma}}(y\mid x)-p_{W^{*},\Sigma^{*}}(y\mid x)\Big{|}dy\] \[=\frac{1}{2}\int_{\mathbb{R}^{3}_{\geq 0}}\left|\prod_{i=1}^{d}p_{ \widehat{W},\widehat{\Sigma}}(y_{i}\mid x)-\prod_{i=1}^{d}p_{W^{*},\Sigma^{*}} (y_{i}\mid x)\right|dy,\]

where \(y_{i}\) is the \(i^{\text{th}}\) element of \(y\). Though at first glace it seems that this is a single high-dimensional integral, the reality is that due to the truncation, the probability mass on the boundary of the non-negative orthant cone \(\mathbb{R}^{3}_{\geq 0}\) has a complex structure that cannot be ignored. Instead we perform a series of integrals of continuous bounded functions, which are much more amenable to Monte-Carlo integration techniques:

\[d_{TV}\Big{(}(\widehat{W},\widehat{\Sigma}),(W^{*},\Sigma^{*}) \Big{)}=\] \[\frac{1}{2}\sum_{S^{\prime}\in 2^{[d]}}\int_{\mathbb{R}^{[S^{ \prime}]}_{\geq 0}}\left|\prod_{i\in S^{\prime}}p_{\widehat{W},\widehat{\Sigma}}(y_ {i}\mid x)\prod_{i\in(S^{\prime})^{c}}\Phi\Big{(}\widehat{\Sigma}^{-1/2}_{ii} \widehat{W}_{i}\Big{)}-\prod_{i\in S^{\prime}}p_{W^{*},\Sigma^{*}}(y_{i}\mid x )\prod_{i\in(S^{\prime})^{c}}\Phi\Big{(}\Sigma^{-1/2}_{ii}W_{i}\Big{)}\right| dy_{S^{\prime}}.\] (45)

Essentially, for each possible support of \(y\), \(S^{\prime}\), we integrate over the absolute deviation in those coordinates.

#### e.2.3 Figure 4

In Figure 4, we plot an upper bounds for the TV distance of the MLE as the output dimension \(d\) grows. We set the input dimension \(k=1\) with deterministic \(x\) and fix the number of samples \(n=5000\). To estimate the KL divergence, we repeatedly sample \(y\) according to the true distribution, and compute the empirical average log-likelihood ratio.

In Figure 4 we fix the output dimension \(d=1\) and input dimension \(k=5\), and compute the TV over a range of values of \(n\). In addition to \(x\) sampled i.i.d. from the Normal and Laplace distributions, we also plot a performance with a Normal mixture, where with probability \(0.01\), the normal distribution has mean shifted by 100. We observe, as our theory suggests, that in all cases, there is only very minor differences in the expected TV distance.

Note that in the case where \(x\) is distributed according to a Normal mixture, we observe that the optimization may become very challenging, and in the plot above, we have omitted some of the instances where optimization failed due to lack of smoothness in the objective and numerical imprecision. Omitting these point may lead to a small systematic error in the figure, which may explain why it is lower than the other plots. In practice, for a fixed optimization budget, we may observe meaningful differences in TV for different distributions of \(x\), since computing the MLE becomes more challenging for more complex heavy-tailed distributions.

## Appendix F The Likelihood Function

In this section, we discuss the likelihood function, proving log-concavity, as well as discussing computational challenges.

### One Dimensional Case

In this section, we consider the case where the output dimension \(d=1\), with some \(\sigma^{*}\) and some \(W^{*}\in\mathbb{R}^{1,k}\) and describe how to compute the likelihood function. We defer the proof of the log-concavity to the follwing section, which covers the more general case with \(d\geq 1\) When we re-parameterize as \(u=-W/\sigma\), \(v=1/\sigma\), the likelihood function is written as:

\[f_{u,v}(y)=-\frac{1}{2}\sum_{i\in S^{\prime}}(vy_{i}-u\cdot x_{i})^{2}+|S^{ \prime}|\log(v)+\sum_{i\in S}\log\Phi(-u\cdot x_{i})\] (46)

where in this case we let \(S=\{i\ \mid\ y_{i}=0\}\), where \(y_{i}\) is the \(i^{\text{th}}\) sample in the set \(\{y_{i},x_{i}\}_{i=1}^{n}\). Note this is distinct from how we define \(S\) and \(S^{\prime}\) in the multidimensional case, where it corresponds to the zero and non-zero coordinates of a single sample \(y_{i}\). The case of \(d>0\) with uncorrelated \(\eta\) follows a similar approach.

Numerical concerns.In (46), the term \(\log\Phi(-u\cdot x_{i})\) presents some numerical concerns when \(u\cdot x_{i}\gg 0\) if we naively compute \(\Phi(-u\cdot x_{i})\) and then take the logarithm. Instead we compute it from the _mills ratio_\(m(x)\)[27], defined to be the ratio of the standard normal pdf and the complementary cdf. The mills ratio is easily computed, with many well-known expansions, see for example, [13]. Then we can write:

\[\log\Phi(-x)=-\log m(x)-\frac{1}{2}\log(2\pi)-\frac{1}{2}x^{2},\ \ x>0.\]

Since \(m(x)\) changes relatively slowly in \(x\) compared to \(\Phi(-x)\), this greatly improves numerical stability.

### Multidimensional Case

In the multi-dimensional case, we will generally use the more standard natural parameters:

\[U :=\frac{\Sigma^{-1}}{2},\] \[v :=-\Sigma^{-1}Wx.\]

Note that in the one-dimensional case, we could have also use the natural parameters, but due to the truncation structure, the parameters we used make the computation simpler, in a way that does not apply to the multidimensional case. Also note that here we are considering a fixed \(x\) and writing \(v\) as a vector. In full generality, we should take \(V=-\Sigma^{-1}W\), however, this is a simple extension which we omit here for readability. It turns out that density is log-concave in these natural parameters:

**Lemma F.1**.: _The log-likelihood function in Eqn (8) is concave in the natural parameter space._

Proof.: First, let's write the un-truncated density in terms of these parameters:

\[f_{W,\Sigma}(y|x) = \exp\biggl{(}-\frac{1}{2}(y-Wx)^{T}\Sigma^{-1}(y-Wx)-\frac{1}{2} \log|2\pi\Sigma|\biggr{)},\] (47) \[= \exp\biggl{(}-\frac{1}{2}y^{T}\Sigma^{-1}y+x^{T}W^{T}\Sigma^{-1} y-x^{T}W^{T}\Sigma^{-1}Wx-\frac{1}{2}\log|2\pi\Sigma|\biggr{)}\] (48) \[= \exp\biggl{(}-\frac{1}{2}y^{T}Uy-v^{T}y-v^{T}U^{-1}v-\frac{1}{2} \log((2\pi)^{n}/|U|)\biggr{)}\] (49)

Thus, the untruncated conditional density can be written as:

\[f_{U,v}(y)=\exp\biggl{(}-\frac{1}{2}y^{T}Uy+y^{T}v-A(U,v)\biggr{)},\]

where \(A(U,v)\) is the cumulant function (note this is distinct from the related cumulant generating function). A well known result is that \(A\) is jointly convex in its arguments, \(U\) and \(v\). Taking logs and using this fact, shows us that \(f_{U,v}(y)\) is log-concave in \(U,v\).

Our truncated density is simply:

\[f_{U,v}(y|x)=\int_{y\leq 0}p_{U,v}(y|x)dy_{S},\]

For any log-concave density \(f(x)\), integration over a convex subset of the coordinates preserves log-concavity ([9], Example 3.42-3.44). Thus the objective is log-concave.

Then the likelihood function at \(U,v\) can be rewritten as

\[f_{U,v}(y) =\log\int_{t_{S}\leq 0,t_{S^{\prime}}=y_{S^{\prime}}}\exp{\left(-t^{T} Ut-t^{T}v-\frac{v^{T}U^{-1}v}{4}+\frac{1}{2}\log\lvert 2U\rvert\right)},\] \[=-\frac{v^{T}U^{-1}v}{4}+\frac{1}{2}\log\lvert 2U\rvert+\log\int_{t_ {S}\leq 0,t_{S^{\prime}}=y_{S^{\prime}}}\exp{\left(-t^{T} Ut-t^{T}v\right)},\]

Separating the terms corresponding to \(S\) and \(S^{\prime}\), we get

\[f_{U,v}(y)=-\frac{v^{T}U^{-1}v}{4}+\frac{1}{2}\log\lvert 2U\rvert-y_{S^{ \prime}}^{T}U_{S^{\prime}}y_{S^{\prime}}-y_{S^{\prime}}^{T}v_{S^{\prime}}+ \log\int_{t_{S}\leq 0}\exp{\left(-t_{S}^{T}U_{S}t_{S}-2y_{S^{\prime}}^{T}U_{S^{ \prime}}t_{S}-v_{S}^{T}t_{S}\right)}.\]

The last term resembles the \(\log\Phi\) term that appears in the univariate case. This resemblance can be made more clear as follows. Let \(r_{S}=U_{SS^{\prime}}y_{S}^{\prime}+\frac{1}{2}v_{S}\) and \(M^{T}M=U_{S}\).

\[=\log\int_{t_{S}\leq 0}\exp{\left(-\big{(}(Mt_{S})^{T}Mt_{S}+2t _{S}^{T}r_{S}\big{)}\right)}\\ =\log\int_{t_{S}\leq 0}\exp{\left(-\big{(}(Mt_{S})^{T}Mt_{S}+2t _{S}^{T}r_{S}+r_{S}^{T}M^{-1}M^{-T}r_{S}-r_{S}^{T}M^{-1}M^{-T}r_{S}\big{)} \right)}\\ =\log\int_{t_{S}\leq 0}\exp{\left(-\big{\|}Mt_{S}+M^{-T}r_{S} \big{\|}^{2}+r_{S}^{T}M^{-1}M^{-T}r_{S}\right)}\\ =r_{S}^{T}M^{-1}M^{-T}r_{S}+\log\int_{t_{S}\leq 0}\exp{\left(- \big{\|}t_{S}+U_{S}^{-1}r_{S}\big{\|}^{2}_{U_{S}}\right)}\\ =r_{S}^{T}U_{s}^{-1}r_{S}+\log\int_{t_{S}\leq 0}\frac{(2 \pi)^{d/2}\big{\lvert}U_{S}^{-1}/2\big{\rvert}^{1/2}}{(2\pi)^{d/2}\big{\lvert} U_{S}^{-1}/2\big{\rvert}^{1/2}}\exp{\left(-\big{\|}t_{S}+U_{S}^{-1}r_{S} \big{\|}^{2}_{U_{S}}\right)}\\ =r_{S}^{T}U_{s}^{-1}r_{S}-\frac{1}{2}\log\lvert 2U_{S}\rvert+ \log\int_{t_{S}\leq 0}\frac{1}{(2\pi)^{d/2}\big{\lvert}U_{S}^{-1}/2\big{\rvert}^{1/2}} \exp{\left(-\frac{1}{2}\big{\|}t_{S}+U_{S}^{-1}r_{S}\big{\|}^{2}_{2U_{S}} \right)}+c\\ =r_{S}^{T}U_{s}^{-1}r_{S}-\frac{1}{2}\log\lvert 2U_{S}\rvert+ \log\Phi{\left(0;\mu=-U_{S}^{-1}r_{S},\Sigma=\frac{1}{2}U_{S}^{-1}\right)}+c\]

Putting this together, \(f_{U,v}\) can be written as:

\[f_{U,v}(y)=-\frac{v^{T}U^{-1}v}{4}+\frac{1}{2}\log\lvert U\rvert -y_{S^{\prime}}^{T}U_{S^{\prime}}y_{S^{\prime}}-y_{S^{\prime}}^{T}v_{S^{\prime }}+r_{S}^{T}U_{S}^{-1}r_{S}-\frac{1}{2}\log\lvert U_{S}\rvert+\lvert S^{\prime }\rvert\frac{\log(2)}{2}\\ +\log\Phi{\left(0;\mu=-U_{S}^{-1}r_{S},\Sigma=\frac{1}{2}U_{S}^{- 1}\right)}+c\] (51)

Thus, it appears that evaluating the likelihood for even a single sample involves the high-dimensional integral that is the rectangular cdf in equation (51).

### Computing Gradients

#### f.3.1 One Dimensional Case

In the one-dimensional case, the gradient with respect to \(u\) is easily computed as:

\[\nabla_{u}f_{u,v}(y)=\sum_{i\in S^{\prime}}(vy_{i}-u\cdot x_{i})x_{i}-\sum_{i \in S}\frac{1}{m(u\cdot x_{i})}x_{i},\]

where we have previously defined \(m(x)\) as the mills ratio. Furthermore, we have:

\[\nabla_{v}f_{u,v}(y)=\lvert S^{\prime}\rvert\frac{1}{v}-\sum_{i\in S^{\prime }}y_{i}(vy_{i}-u\cdot x_{i})\]

#### f.3.2 Multidimensional Case

First we consider the non-integral terms in the likelihood. Differentiating each term wrt \(U\), we get

\[-\nabla_{U}\frac{v^{T}U^{-1}v}{4} =\frac{1}{4}(U^{-1}vv^{T}U^{-1}),\] \[\nabla_{U}\frac{1}{2}\log\lvert 2U\rvert =\frac{1}{2}U^{-1},\] \[-\nabla_{U}y_{S^{\prime}}^{T}U_{S^{\prime}}y_{S^{\prime}} =\begin{pmatrix}0&0\\ 0&-y_{S^{\prime}}y_{S^{\prime}}^{T}\end{pmatrix}\]

Differentiating each term wrt \(v\), we get

\[-\nabla_{v}\frac{v^{T}U^{-1}v}{4} =-\frac{1}{2}U^{-1}v,\] \[-\nabla_{v}y_{S^{\prime}}^{T}v_{S^{\prime}} =\begin{pmatrix}0\\ -y_{S^{\prime}}\end{pmatrix}\]

Now consider the integral term. Differentiating wrt \(U\), we get

\[\nabla_{U}\log\int_{t_{S}\leq 0}\exp\left(-t_{S}^{T}U_{S}t_{S}-2 y_{S^{\prime}}^{T}U_{S^{\prime}S}t_{S}-v_{S}^{T}t_{S}\right)\\ =\frac{\int_{t_{S}\leq 0}\begin{pmatrix}-t_{S}t_{S}^{T}&-t_{S}y_ {S^{\prime}}^{T}\\ -y_{S^{\prime}}t_{S}^{T}&0\end{pmatrix}\exp\left(-t_{S}^{T}U_{S}t_{S}-2y_{S^ {\prime}}^{T}U_{S^{\prime}S}t_{S}-v_{S}^{T}t_{S}\right)}{\int_{t_{S}\leq 0}\exp \left(-t_{S}^{T}U_{S}t_{S}-2y_{S^{\prime}}^{T}U_{S^{\prime}S}t_{S}-v_{S}^{T}t_ {S}\right)}\]

Let \(M\) be a matrix such that

\[M^{T}M=U_{S}\]

Then via completion of squares in the exponential term, we get

\[\nabla_{U}\log\int_{t_{S}\leq 0}\exp\left(-t_{S}^{T}U_{S}t_{S}-2 y_{S^{\prime}}^{T}U_{S^{\prime}S}t_{S}-v_{S}^{T}t_{S}\right)\] (52) \[= \frac{\int_{t_{S}\leq 0}\begin{pmatrix}-t_{S}t_{S}^{T}&-t_{S}y_ {S^{\prime}}^{T}\\ -y_{S^{\prime}}t_{S}^{T}&0\end{pmatrix}\exp\left(-\lVert Mt_{S}+(M^{-1})^{T} \big{(}U_{SS^{\prime}}y_{S^{\prime}}+\frac{v_{S}}{2}\big{)}\rVert^{2}\right) }{\int_{t_{S}\leq 0}\exp\left(-\lVert Mt_{S}+(M^{-1})^{T}\big{(}U_{SS^{ \prime}}y_{S^{\prime}}+\frac{v_{S}}{2}\big{)}\rVert^{2}\right)}\] (53)

Notice that this density is Gaussian, with mean and covariance:

\[\mathcal{N}\bigg{(}-M^{-1}\big{(}M^{-1}\big{)}^{T}\Big{(}U_{SS^{\prime}}y_{S^ {\prime}}+\frac{v_{S}}{2}\Big{)};\frac{U_{S}^{-1}}{2}\bigg{)}.\]

And hence, the gradient can be estimated as

\[\nabla_{U}\log\int_{t_{S}\leq 0}\exp\left(-t_{S}^{T}U_{S}t_{S}-2 y_{S^{\prime}}^{T}U_{S^{\prime}S^{\prime}}t_{S}-v_{S}^{T}t_{S}\right)= \underset{t_{S}}{\mathbb{E}}\left[\begin{pmatrix}-t_{S}t_{S}^{T}&-t_{S}y_{S^{ \prime}}^{T}\\ -y_{S^{\prime}}t_{S}^{T}&0\end{pmatrix}\right]\]

where \(t_{S}\) is the truncation of

\[z\sim\mathcal{N}\bigg{(}-M^{-1}\big{(}M^{-1}\big{)}^{T}\Big{(}U_{SS^{\prime}}y _{S^{\prime}}+\frac{v_{S}}{2}\Big{)};\frac{U_{S}^{-1}}{2}\bigg{)}.\]

to the negative quadrant. A similar calculation gives the gradient for \(v\) as

\[\nabla_{v}\log\int_{t_{S}\leq 0}\exp\left(-t_{S}^{T}U_{S}t_{S}-2 y_{S^{\prime}}^{T}U_{S^{\prime}S}t_{S}-v_{S}^{T}t_{S}\right)=\underset{t_{S}}{ \mathbb{E}}\left[\begin{pmatrix}-t_{S}\\ 0\end{pmatrix}\right]\]