# Differentiable Random Partition Models

Thomas M. Sutter, Alain Ryser, Joram Liebeskind, Julia E. Vogt

Department of Computer Science

ETH Zurich

Equal Contribution. Correspondence to {thomas.sutter,alain.ryser}@inf.ethz.ch

###### Abstract

Partitioning a set of elements into an unknown number of mutually exclusive subsets is essential in many machine learning problems. However, assigning elements, such as samples in a dataset or neurons in a network layer, to an unknown and discrete number of subsets is inherently non-differentiable, prohibiting end-to-end gradient-based optimization of parameters. We overcome this limitation by proposing a novel two-step method for inferring partitions, which allows its usage in variational inference tasks. This new approach enables reparameterized gradients with respect to the parameters of the new random partition model. Our method works by inferring the number of elements per subset and, second, by filling these subsets in a learned order. We highlight the versatility of our general-purpose approach on three different challenging experiments: variational clustering, inference of shared and independent generative factors under weak supervision, and multitask learning.

## 1 Introduction

Partitioning a set of elements into subsets is a classical mathematical problem that attracted much interest over the last few decades (Rota, 1964; Graham et al., 1989). A partition over a given set is a collection of non-overlapping subsets such that their union results in the original set. In machine learning (ML), partitioning a set of elements into different subsets is essential for many applications, such as clustering (Bishop and Svensen, 2004) or classification (De la Cruz-Mesia et al., 2007).

Random partition models (RPM, Hartigan, 1990) define a probability distribution over the space of partitions. RPMs can explicitly leverage the relationship between elements of a set, as they do not necessarily assume _i.i.d._ set elements. On the other hand, most existing RPMs are intractable for large datasets (MacQueen, 1967; Plackett, 1975; Pitman, 1996) and lack a reparameterization scheme, prohibiting their direct use in gradient-based optimization frameworks.

In this work, we propose the differentiable random partition model (DRPM), a fully-differentiable relaxation for RPMs that allows reparametrizable sampling. The DRPM follows a two-stage procedure: first, we model the number of elements per subset, and second, we learn an ordering of the elements with which we fill the elements into the subsets. The DRPM enables the integration of partition models into state-of-the-art ML frameworks and learning RPMs from data using stochastic optimization.

We evaluate our approach in three experiments, demonstrating the proposed DRPM's versatility and advantages. First, we apply the DRPM to a variational clustering task, highlighting how the reparametrizable sampling of partitions allows us to learn a novel kind of Variational Autoencoder (VAE, Kingma and Welling, 2014). By leveraging potential dependencies between samples in a dataset, DRPM-based clustering overcomes the simplified _i.i.d._ assumption of previous works, which used categorical priors (Jiang et al., 2016). In our second experiment, we demonstrate how to retrieve sets of shared and independent generative factors of paired images using the proposed DRPM. Incontrast to previous works (Bouchacourt et al., 2018; Hosoya, 2018; Locatello et al., 2020), which rely on strong assumptions or heuristics, the DRPM enables end-to-end inference of generative factors. Finally, we perform multitask learning (MTL) by using the DRPM as a building block in a deterministic pipeline. We show how the DRPM learns to assign subsets of network neurons to specific tasks. The DRPM can infer the subset size per task based on its difficulty, overcoming the tedious work of finding optimal loss weights (Kurin et al., 2022; Xin et al., 2022).

To summarize, we introduce the DRPM, a novel differentiable and reparametrizable relaxation of RPMs. In extensive experiments, we demonstrate the versatility of the proposed method by applying the DRPM to clustering, inference of generative factors, and multitask learning.

## 2 Related Work

Random Partition ModelsPrevious works on RPMs include product partition models (Hartigan, 1990), species sampling models (Pitman, 1996), and model-based clustering approaches (Bishop and Svensen, 2004). Further, Lee and Sang (2022) investigate the balancedness of subset sizes of RPMs. They all require tedious manual adjustment, are non-differentiable, and are, therefore, unsuitable for modern ML pipelines. A fundamental RPM application is clustering, where the goal is to partition a given dataset into different subsets, the clusters. In contrast to many existing approaches (Yang et al., 2019; Sarfraz et al., 2019; Cai et al., 2022), we consider cluster assignments as random variables, allowing us to treat clustering from a variational perspective. Previous works in variational clustering (Jiang et al., 2016; Dilokthanakul et al., 2016; Manduchi et al., 2021) implicitly define RPMs to perform clustering. They compute partitions in a variational fashion by making _i.i.d._ assumptions about the samples in the dataset and imposing of assignments of the clusters to data points during training. A problem related to set partitioning is the earth mover's distance problem (EMD, Monge, 1781; Rubner et al., 2000). However, EMD aims to assign a set's elements to different subsets based on a cost function and given subset sizes. Iterative solutions to the problem exist (Sinkhorn, 1964), and various methods have recently been proposed, e.g., for document ranking (Adams and Zemel, 2011) or permutation learning (Santa Cruz et al., 2017; Mena et al., 2018; Cuturi et al., 2019).

Differentiable and Reparameterizable Discrete DistributionsFollowing the proposition of the Gumbel-Softmax trick (GST, Jang et al., 2016; Maddison et al., 2017), interest in research around continuous relaxations for discrete distributions and non-differentiable algorithms rose. The GST enabled the reparameterization of categorical distributions and their integration into gradient-based optimization pipelines. Based on the same trick, Sutter et al. (2023) propose a differentiable formulation for the multivariate hypergeometric distribution. Multiple works on differentiable sorting procedures and permutation matrices have been proposed, e.g., Linderman et al. (2018); Prillo and Eisenschlos (2020); Petersen et al. (2021). Further, Grover et al. (2019) described the distribution over permutation matrices \(p(\pi)\) for a permutation matrix \(\pi\) using the Plackett-Luce distribution (PL, Luce, 1959; Plackett, 1975). Prillo and Eisenschlos (2020) proposed a computationally simpler variant of Grover et al. (2019). More examples of differentiable relaxations include the top-\(k\) elements selection procedure (Xie and Ermon, 2019), blackbox combinatorial solvers (Pogancic et al., 2019), implicit likelihood estimations (Niepert et al., 2021), and \(k\)-subset sampling (Ahmed et al., 2022).

Figure 1: Illustration of the proposed DRPM method. We first sample a permutation matrix \(\pi\) and a set of subset sizes \(\bm{n}\) separately in two stages. We then use \(\bm{n}\) and \(\pi\) to generate the assignment matrix \(Y\), the matrix representation of a partition \(\rho\).

## 3 Preliminaries

Set PartitionsA partition \(\rho=(\mathcal{S}_{1},\ldots,\mathcal{S}_{K})\) of a set \([n]=\{1,\ldots,n\}\) with \(n\) elements is a collection of \(K\) subsets \(\mathcal{S}_{k}\subseteq[n]\) where \(K\) is _a priori_ unknown (Mansour and Schork, 2016). For a partition \(\rho\) to be valid, it must hold that

\[\mathcal{S}_{1}\cup\cdots\cup\mathcal{S}_{K}=[n]\ \ \ \text{and}\ \ \forall k\neq l:\ \mathcal{S}_{k}\cap\mathcal{S}_{l}=\emptyset\] (1)

In other words, every element \(i\in[n]\) has to be assigned to precisely one subset \(\mathcal{S}_{k}\). We denote the size of the \(k\)-th subset \(\mathcal{S}_{k}\) as \(n_{k}=|\mathcal{S}_{k}|\). Alternatively, we can describe a partition \(\rho\) through an assignment matrix \(Y=[\bm{y}_{1},\ldots,\bm{y}_{K}]^{T}\in\{0,1\}^{K\times n}\). Every row \(\bm{y}_{k}\in\{0,1\}^{1\times n}\) is a multi-hot vector, where \(\bm{y}_{ki}=1\) assigns element \(i\) to subset \(\mathcal{S}_{k}\).

Within the scope of our work, we view a partition of a set of \(n\) elements as a special case of the urn model. Here, the urn contains marbles with \(n\) different colors, where each color corresponds to a subset in the partition. For each color, there are \(n\) marbles corresponding to the potential elements of their color/subset. To derive a partition, we sample \(n\) marbles without replacement from the urn and register the order in which we draw the colors. The color of the \(i\)-th marble then determines the subset to which element \(i\) corresponds. Furthermore, we can constrain the partition to only \(K\) subsets by taking an urn with only \(K\) different colors.

Probability distribution over subset sizesThe multivariate non-central hypergeometric distribution (MVHG) describes sampling without replacement and allows to skew the importance of groups with an additional importance parameter \(\bm{\omega}\)(Fisher, 1935; Wallenius, 1963; Chesson, 1976). The MVHG is an urn model and is described by the number of different groups \(K\in\mathbb{N}\), the number of elements in the urn of every group \(\bm{m}=[m_{1},\ldots,m_{K}]\in\mathbb{N}^{K}\), the total number of elements in the urn \(\sum_{k=1}^{K}m_{k}\in\mathbb{N}\), the number of samples to draw from the urn \(n\in\mathbb{N}_{0}\), and the importance factor for every group \(\bm{\omega}=[\omega_{1},\ldots,\omega_{K}]\in\mathbb{R}_{0+}^{K}\)(Johnson, 1987). Then, the probability of sampling \(\bm{n}=\{n_{1},\ldots,n_{K}\}\), where \(n_{k}\) describes the number of elements drawn from group \(K\) is

\[p(\bm{n};\bm{\omega},\bm{m})=\frac{1}{P_{0}}\prod_{k=1}^{K}\binom{m_{k}}{n_{ k}}\omega_{k}^{n_{k}}\] (2)

where \(P_{0}\) is a normalization constant. Hence, the MVHG \(p(\bm{n};\bm{\omega},\bm{m})\) allows us to model dependencies between different elements of a set since drawing one element from the urn influences the probability of drawing one of the remaining elements, creating interdependence between them. For the rest of the paper, we assume \(\forall\ m_{k}\in\bm{m}:m_{k}=n\). We thus use the shorthand \(p(\bm{n};\bm{\omega})\) to denote the density of the MVHG. We refer to Appendix A.1 for more details.

Probability distribution over Permutation MatricesLet \(p(\pi)\) denote a distribution over permutation matrices \(\pi\in\{0,1\}^{n\times n}\). A permutation matrix \(\pi\) is doubly stochastic (Marcus, 1960), meaning that its row and column vectors sum to \(1\). This property allows us to use \(\pi\) to describe an order over a set of \(n\) elements, where \(\pi_{ij}=1\) means that element \(j\) is ranked at position \(i\) in the imposed order. In this work, we assume \(p(\pi)\) to be parameterized by scores \(\bm{s}\in\mathbb{R}_{+}^{n}\), where each score \(s_{i}\) corresponds to an element \(i\). The order given by sorting \(\bm{s}\) in decreasing order corresponds to the most likely permutation in \(p(\pi;\bm{s})\). Sampling from \(p(\pi;\bm{s})\) can be achieved by resampling the scores as \(\tilde{s}_{i}=\beta\log s_{i}+g_{i}\) where \(g_{i}\sim\text{Gumbel}(0,\beta)\) for fixed scale \(\beta\), and sorting them in decreasing order. Hence, resampling scores \(\bm{s}\) enables the resampling of permutation matrices \(\pi\). The probability over orderings \(p(\pi;\bm{s})\) is then given by (Thurstone, 1927; Luce, 1959; Plackett, 1975; Yellott, 1977)

\[p(\pi;\bm{s})=p((\pi\tilde{\bm{s}})_{1}\geq\cdots\geq(\pi\tilde{\bm{s}})_{n})= \frac{(\pi\bm{s})_{1}}{Z}\frac{(\pi\bm{s})_{2}}{Z-(\pi\bm{s})_{1}}\cdots\frac{ (\pi\bm{s})_{n}}{Z-\sum_{j=1}^{n-1}(\pi\bm{s})_{j}}\] (3)

where \(\pi\) is a permutation matrix and \(Z=\sum_{i=1}^{n}s_{i}\). The resulting distribution is a Plackett-Luce (PL) distribution (Luce, 1959; Plackett, 1975) if and only if the scores \(\bm{s}\) are perturbed with noise drawn from Gumbel distributions with identical scales (Yellott, 1977). For more details, we refer to Appendix A.2).

## 4 A two-stage Approach to Random Partition Models

We propose the DRPM \(p(Y;\bm{\omega},\bm{s})\), a differentiable and reparameterizable two-stage Random Partition Model (RPM). The proposed formulation separately infers the number of elements \(i\) per subset\(\bm{n}\in\mathbb{N}_{0}^{K}\), where \(\sum_{k=1}^{K}n_{k}=n\), and the assignment of elements to subsets \(\mathcal{S}_{k}\) by inducing an order on the \(n\) elements and filling \(\mathcal{S}_{1},...,\mathcal{S}_{K}\) sequentially in this order. To model the order of the elements, we use a permutation matrix \(\pi=[\bm{\pi}_{1},\ldots,\bm{\pi}_{n}]^{T}\in\{0,1\}^{n\times n}\), from which we infer \(Y\) by sequentially summing up rows according to \(\bm{n}\). Note that the doubly-stochastic property of all permutation matrices \(\pi\) ensures that the columns of \(Y\) remain one-hot vectors, assigning every element \(i\) to precisely one of the \(K\) subsets. At the same time, the \(k\)-th row of \(Y\) corresponds to an \(n_{k}\)-hot vector \(\bm{y}_{k}\) and therefore serves as a subset selection vector, i.e.

\[\bm{y}_{k}=\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i},\quad\text{ where }\ \ \nu_{k}=\sum_{\iota=1}^{k-1}n_{\iota}\] (4)

such that \(Y=[\bm{y}_{1},\ldots,\bm{y}_{K}]^{T}\). Additionally, Figure 1 provides an illustrative example. Note that \(K\) defines the maximum number of possible subsets, and not the effective number of non-empty subsets, because we allow \(\mathcal{S}_{k}\) to be the empty set \(\emptyset\)(Mansour and Schork, 2016). We base the following Proposition 4.1 on the MVHG distribution \(p(\bm{n};\bm{\omega})\) for the subset sizes \(\bm{n}\) and the PL distribution \(p(\pi;\bm{s})\) for assigning the elements to subsets. However, the proposed two-stage approach to RPMs is not restricted to these two classes of probability distributions.

**Proposition 4.1** (Two-stage Random Partition Model).: _Given a probability distribution over subset sizes \(p(\bm{n};\bm{\omega})\) with \(\bm{n}\in\mathbb{N}_{0}^{K}\) and distribution parameters \(\bm{\omega}\in\mathbb{R}_{+}^{K}\) and a PL probability distribution over random orderings \(p(\pi;\bm{s})\) with \(\pi\in\{0,1\}^{n\times n}\) and distribution parameters \(\bm{s}\in\mathbb{R}_{+}^{n}\), the probability mass function \(p(Y;\bm{\omega},\bm{s})\) of the two-stage RPM is given by_

\[p(Y;\bm{\omega},\bm{s})=p(\bm{y}_{1},\ldots,\bm{y}_{K};\bm{\omega},\bm{s})=p( \bm{n};\bm{\omega})\sum_{\pi\in\Pi_{Y}}p(\pi;\bm{s})\] (5)

_where \(\Pi_{Y}=\{\pi:\bm{y}_{k}=\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i},k=1, \ldots,K\}\), and \(\bm{y}_{k}\) and \(\nu_{k}\) as in Equation (4)._

In the following, we outline the proof of Proposition 4.1 and refer to Appendix B for a formal derivation. We calculate \(p(Y;\bm{\omega},\bm{s})\) as a probability of subsets \(p(\bm{y}_{1},\ldots,\bm{y}_{K};\bm{\omega},\bm{s})\), which we compute sequentially over subsets, i.e.

\[p(\bm{y}_{1},\ldots,\bm{y}_{K};\bm{\omega},\bm{s})=p(\bm{y}_{1};\bm{\omega}, \bm{s})\cdots p(\bm{y}_{K}\mid\bm{y}_{<K};\bm{\omega},\bm{s}),\] (6)

where \(\bm{y}_{<k}=[\bm{y}_{1},\ldots,\bm{y}_{k-1}]\) and

\[p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})=p(n_{k}\mid n_{<k};\bm{\omega })\sum_{\bar{\pi}\in\Pi_{\bm{y}_{k}}}p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s}),\] (7)

where \(\Pi_{\bm{y}_{k}}\) in Equation (7) is the set of all subset permutations of elements \(i\in\mathcal{S}_{k}\). A subset permutation matrix \(\bar{\pi}\) represents an ordering over only \(n_{k}\) out of the total \(n\) elements. The probability \(p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})\) describes the probability of a subset of a given size \(n_{k}\) by marginalizing over the probabilities of all subset permutations \(p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})\). Hence, the sum over all \(p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})\) makes \(p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})\) invariant to the ordering of elements \(i\in\mathcal{S}_{k}\)(Xie and Ermon, 2019). Note that in a slight abuse of notation, we use \(p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{\omega},\bm{s})\) as the probability of a subset permutation \(\bar{\pi}\) given that there are \(n_{k}\) elements in \(\mathcal{S}_{k}\) and thus \(\bar{\pi}\in\{0,1\}^{n_{k}\times n}\).

The probability of a subset permutation matrix \(p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})\) describes the probability of drawing the elements \(i\in\mathcal{S}_{k}\) in the order defined by the subset permutation matrix \(\bar{\pi}\) given that the elements in \(\mathcal{S}_{<k}\) are already determined. Hence, we condition on the subsets \(\bm{y}_{<k}\). This property follows from Luce's choice axiom (LCA, Luce, 1959). Additionally, we condition on \(n_{k}\), the size of the subset \(\mathcal{S}_{k}\). The probability of a subset permutation is given by

\[p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})=\prod_{i=1}^{n_{k}}\frac{(\bar{\pi} \bm{s})_{i}}{Z_{k}-\sum_{j=1}^{i-1}(\bar{\pi}\bm{s})_{j}}\] (8)

In contrast to the distribution over permutations matrices \(p(\pi;\bm{s})\) in Equation (3), we compute the product over \(n_{k}\) terms and have a different normalization constant \(Z_{k}\), which is the sum over the scores \(s_{i}\) of all elements \(i\in\mathcal{S}_{k}\). Although we induce an ordering over all elements \(i\) by using a permutation matrix \(\pi\), the probability \(p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})\) is invariant to intra-subset orderings of elements \(i\in\mathcal{S}_{k}\). Finally, we arrive at Equation (5) by substituting Equation (7) into Equation (6),and applying the definition of the conditional probability \(p(\bm{n};\bm{\omega})=\prod_{k=1}^{K}p(n_{k}\mid n_{<k};\bm{\omega})\) and by reshuffling indices \(\sum_{\pi\in\Pi_{Y}}p(\pi;\bm{s})=\prod_{k=1}^{K}\sum_{\pi\in\Pi_{\bm{y}_{k}}}p( \tilde{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})\).

Note that in contrast to previous RPMs, which often need exponentially many distribution parameters (Plackett, 1975), the proposed two-stage approach to RPMs only needs \((n+K)\) parameters to create an RPM for \(n\) elements: the score parameters \(\bm{s}\in\mathbb{R}_{+}^{n}\) and the group importance parameters \(\bm{\omega}\in\mathbb{R}_{+}^{K}\).

Finally, to sample from the two-stage RPM of Proposition 4.1 we apply the following procedure: First sample \(\pi\sim p(\pi;\bm{s})\) and \(\bm{n}\sim p(\bm{n};\bm{\omega})\). From \(\pi\) and \(\bm{n}\), compute partition \(Y\) by summing the rows of \(\pi\) according to \(\bm{n}\) as described in Equation (4) and illustrated in Figure 1.

### Approximating the Probability Mass Function

The number of permutations per subset \(|\Pi_{\bm{y}_{k}}|\) scales factorially with the subset size \(n_{k}\), i.e. \(|\Pi_{\bm{y}_{k}}|=n_{k}!\). Consequently, the number of valid permutation matrices \(|\Pi_{Y}|\) is given as a function of \(\bm{n}\), i.e.

\[|\Pi_{Y}|=\prod_{k=1}^{K}|\Pi_{\bm{y}_{k}}|=\prod_{k=1}^{K}n_{k}!\] (9)

Although Proposition 4.1 describes a well-defined distribution for \(p(Y;\bm{\omega},\bm{s})\), it is in general computationally intractable due to Equation (9). In practice, we thus approximate \(p(Y;\bm{\omega},\bm{s})\) using the following Lemma.

**Lemma 4.2**.: \(p(Y;\bm{\omega},\bm{s})\) _can be upper and lower bounded as follows_

\[\forall\pi\in\Pi_{Y}:\;p(\bm{n};\bm{\omega})p(\pi;\bm{s})\ \leq\ p(Y;\bm{ \omega},\bm{s})\ \leq\ |\Pi_{Y}|p(\bm{n};\bm{\omega})\max_{\tilde{\pi}}p(\tilde{\pi};\bm{s})\] (10)

We provide the proof in Appendix B. Note that from Equation (3) we see that \(\max_{\tilde{\pi}}p(\tilde{\pi};\bm{s})=p(\pi_{\bm{s}};\bm{s})\), where \(\pi_{\bm{s}}\) is the permutation that results from sorting the unperturbed scores \(\bm{s}\).

### The Differentiable Random Partition Model

To incorporate our two-stage RPM into gradient-based optimization frameworks, we require that efficient computation of gradients is possible for every step of the method. The following Lemma guarantees differentiability, allowing us to train deep neural networks with our method in an end-to-end fashion:

**Lemma 4.3** (DRPM).: _A two-stage RPM is differentiable and reparameterizable if the distribution over subset sizes \(p(\bm{n};\bm{\omega})\) and the distribution over orderings \(p(\pi;\bm{s})\) are differentiable and reparameterizable._

We provide the proof in Appendix B. Note that Lemma 4.3 enables us to learn variational posterior approximations and priors using Stochastic Gradient Variational Bayes (SGVB, Kingma and Welling, 2014). In our experiments, we apply Lemma 4.3 using the recently proposed differentiable formulations of the MVHG (Sutter et al., 2023) and the PL distribution (Grover et al., 2019), though other choices would also be valid.

## 5 Experiments

We demonstrate the versatility and effectiveness of the proposed DRPM in three different experiments. First, we propose a novel generative clustering method based on the DRPM, which we compare against state-of-the-art variational clustering methods and demonstrate its conditional generation capabilities. Then, we demonstrate how the DRPM can infer shared and independent generative factors under weak supervision. Finally, we apply the DRPM to multitask learning (MTL), where the DRPM enables an adaptive neural network architecture that partitions layers based on task difficulty2.

### Variational Clustering with Random Partition Models

In our first experiment, we introduce a new version of a Variational Autoencoder (VAE, Kingma and Welling, 2014), the DRPM Variational Clustering (DRPM-VC) model. The DRPM-VC enables clustering and unsupervised conditional generation in a variational fashion. To that end, we assume that each sample \(\bm{x}\) of a dataset \(X\) is generated by a latent vector \(\bm{z}\in\mathbb{R}^{l}\), where \(l\in\mathbb{N}\) is the latent space size. Traditional VAEs would then assume that all latent vectors \(\bm{z}\) are generated by a single Gaussian prior distribution \(\mathcal{N}(\bm{0},\mathbb{I}_{l})\). Instead, we assume every \(\bm{z}\) to be sampled from one of \(K\) different latent Gaussian distributions \(\mathcal{N}(\bm{\mu}_{k},\mathrm{diag}(\bm{\sigma}_{k})),k=1,\ldots,K\), with \(\bm{\mu}_{k},\bm{\sigma}_{k}\in\mathbb{R}^{l}\). Further, note that similar to an urn model (Section 3), if we draw a batch from a given finite dataset with samples from different clusters, the cluster assignments within that batch are not entirely independent. Since there is only a finite number of samples per cluster, drawing a sample from a specific cluster decreases the chance of drawing a sample from that cluster again, and the distribution of the number of samples drawn per cluster will follow an MVHG distribution. Previous work on variational clustering proposes to model the cluster assignment \(\bm{y}\in\{0,1\}^{K}\) of each sample \(\bm{x}\) through independent categorical distributions (Jiang et al., 2016), which might thus be over-restrictive and not correctly reflect reality. Instead, we propose explicitly modeling the dependency between the \(\bm{y}\) of different samples by assuming they are drawn from an RPM. Hence, the generative process leading to \(X\) can be summarized as follows: First, the cluster assignments are represented as a partition matrix \(Y\) and sampled from our DRPM, i.e., \(Y\sim p(Y;\bm{\omega},\bm{s})\). Given an assignment \(\bm{y}\) from \(Y\), we can sample the respective latent variable \(\bm{z}\), where \(\bm{z}\sim\mathcal{N}(\bm{\mu}_{\bm{y}},\mathrm{diag}(\bm{\sigma}_{\bm{y}}))\), \(\bm{z}\in\mathbb{R}^{l}\). Note that we use the notational shorthand \(\bm{\mu}_{\bm{y}}\coloneqq\bm{\mu}_{\arg\max(\bm{y})}\). Like in vanilla VAEs, we infer \(\bm{x}\) by independently passing the corresponding \(\bm{z}\) through a decoder model. Assuming this generative process, we derive the following evidence lower bound (ELBO) for \(p(X)\):

\[\mathcal{L}_{ELBO}=\sum_{\bm{x}\in X}\mathbb{E}_{q(\bm{z}|\bm{x})}\left[\log p (\bm{x}|\bm{z})\right]-\sum_{\bm{x}\in X}\mathbb{E}_{q(Y|X)}\left[KL[q(\bm{z} |\bm{x})||p(\bm{z}|Y)]\right]-KL[q(Y|X)||p(Y)]\]

Note that computing \(KL[q(Y|X)||p(Y)]\) directly is computationally intractable, and we need to upper bound it according to Lemma 4.2. For an illustration of the generative assumptions and more details on the ELBO, we refer to Appendix C.2.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{MNIST} & \multicolumn{3}{c}{FMNIST} \\ \cline{2-7}  & NMI & ARI & ACC & NMI & ARI & ACC \\ \hline GMM & \(0.32_{\pm 0.01}\) & \(0.22_{\pm 0.02}\) & \(0.41_{\pm 0.01}\) & \(0.49_{\pm 0.01}\) & \(0.33_{\pm 0.00}\) & \(0.44_{\pm 0.01}\) \\ Latent GMM & \(0.86_{\pm 0.02}\) & \(0.83_{\pm 0.06}\) & \(0.88_{\pm 0.07}\) & \(0.60_{\pm 0.00}\) & \(0.47_{\pm 0.01}\) & \(0.62_{\pm 0.01}\) \\ VaDE & \(0.84_{\pm 0.01}\) & \(0.76_{\pm 0.05}\) & \(0.82_{\pm 0.04}\) & \(0.56_{\pm 0.02}\) & \(0.40_{\pm 0.04}\) & \(0.56_{\pm 0.03}\) \\ DRPM-VC & \(\bm{0.89}_{\pm 0.01}\) & \(\bm{0.88}_{\pm 0.03}\) & \(\bm{0.94}_{\pm 0.02}\) & \(\bm{0.64}_{\pm 0.00}\) & \(\bm{0.51}_{\pm 0.01}\) & \(\bm{0.65}_{\pm 0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: We compare the clustering performance of the DRPM-VC on test sets of MNIST and FMNIST between Gaussian Mixture Models (GMM), GMM in latent space (Latent GMM), and Variational Deep Embedding (VaDE). We measure performance in terms of the Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and cluster accuracy (ACC) over five seeds and put the best model in bold.

Figure 2: A sample drawn from a DRPM-VC model trained on FMNIST. On top is the sampled partition with the cluster assignments, and on the bottom are generated images corresponding to the sampled assignment matrix. The DRPM-VC learns consistent clusters for different pieces of clothing and can generate new samples of each cluster with great variability.

To assess the clustering performance, we train our model on two different datasets, namely MNIST (LeCun et al., 1998) and Fashion-MNIST (FMNIST, Xiao et al., 2017), and compare it to three baselines. Two of the baselines are based on a Gaussian Mixture Model, where one is directly trained on the original data space (GMM), whereas the other takes the embeddings from a pretrained encoder as input (Latent GMM). The third baseline is Variational Deep Embedding (VaDE, Jiang et al., 2016), which is similar to the DRPM-VC but assumes _i.i.d._ categorical cluster assignments. For all methods except GMM, we use the weights of a pretrained encoder to initialize the models and priors at the start of training. We present the results of these experiments in Table 1. As can be seen, we outperform all baselines, indicating that modeling the inherent dependencies implied by finite datasets benefits the performance of variational clustering. While achieving decent clustering performance, another benefit of variational clustering methods is that their reconstruction-based nature intrinsically allows unsupervised conditional generation. In Figure 2, we present the result of sampling a partition and the corresponding generations from the respective clusters after training the DRPM-VC on FMNIST. The model produces coherent generations despite not having access to labels, allowing us to investigate the structures learned by the model more closely. We refer to Appendix C.2 for more illustrations of the learned clusters, details on the training procedure, and ablation studies.

### Variational Partitioning of Generative Factors

Data modalities not collected as _i.i.d._ samples, such as consecutive frames in a video, provide a weak-supervision signal for generative models and representation learning (Sutter et al., 2023). Here, on top of learning meaningful representations of the data samples, we are also interested in discovering the relationship between coupled samples. If we assume that the data is generated from underlying generative factors, weak supervision comes from the fact that we know that certain factors are shared between coupled pairs while others are independent. The supervision is weak because we neither know the underlying generative factors nor the number of shared and independent factors. In such a setting, we can use the DRPM to learn a partition of the generative factors and assign them to be either shared or independent.

In this experiment, we use paired frames \(\bm{X}=[\bm{x}_{1},\bm{x}_{2}]\) from the _mpi3d_ dataset (Gondal et al., 2019). Every pair of frames shares a subset of its seven generative factors. We introduce the DRPM-VAE, which models the division of the latent space into shared and independent latent factors as RPM. We add a posterior approximation \(q(Y\mid\bm{X})\) and additionally a prior distribution of the form \(p(Y)\). The model maximizes the following ELBO on the

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \(n_{s}=0\) & \multicolumn{2}{c}{\(n_{s}=1\)} & \multicolumn{2}{c}{\(n_{s}=3\)} & \multicolumn{2}{c}{\(n_{s}=5\)} \\ \cline{2-7}  & I & S & I & S & I & S & I \\ \hline Label & \(0.14\pm 0.01\) & \(0.19\pm 0.03\) & \(0.16\pm 0.01\) & \(0.10\pm 0.00\) & \(0.23\pm 0.01\) & \(0.34\pm 0.00\) & \(0.00\pm 0.00\) \\ \hline Ada & \(0.12\pm 0.01\) & \(0.19\pm 0.01\) & \(0.15\pm 0.01\) & \(0.10\pm 0.03\) & \(0.22\pm 0.02\) & \(0.33\pm 0.03\) & \(0.00\pm 0.00\) \\ \hline HG & \(0.18\pm 0.01\) & \(0.22\pm 0.05\) & \(0.19\pm 0.01\) & \(0.08\pm 0.02\) & \(0.28\pm 0.01\) & \(0.28\pm 0.01\) & \(0.01\pm 0.00\) \\ \hline DRPM & \(\bm{0.26}\pm 0.02\) & \(\bm{0.39}\pm 0.07\) & \(\bm{0.2}\pm 0.01\) & \(\bm{0.15}\pm 0.01\) & \(\bm{0.29}\pm 0.02\) & \(\bm{0.42}\pm 0.03\) & \(0.01\pm 0.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Partitioning of Generative Factors. We evaluate the learned latent representations of the four methods (Label-VAE, Ada-VAE, HG-VAE, DRPM-VAE) with respect to the shared (S) and independent (I) generative factors. We do this by fitting linear classifiers on the shared and independent dimensions of the representation, predicting the respective generative factors. We report the results in adjusted balanced accuracy (Sutter et al., 2023) across five seeds.

Figure 3: The mean squared errors between the estimated number of shared factors \(\hat{n}_{s}\) and the true number of shared factors \(n_{s}\) across five seeds for the Label-VAE, Ada-VAE, HG-VAE, and DRPM-VAE.

marginal log-likelihood of images through a VAE (Kingma and Welling, 2014):

\[\mathcal{L}_{ELBO} =\sum_{j=1}^{2}\mathbb{E}_{q(\bm{z}_{s},\bm{z}_{j},Y|\bm{X})}\left[ \log p(\bm{x}_{j}\mid\bm{z}_{s},\bm{z}_{j})\right]\] (11) \[\quad-\mathbb{E}_{q(Y|\bm{X})}\left[KL\left[q(\bm{z}_{s},\bm{z}_{ 1},\bm{z}_{2}\mid Y,\bm{X})||p(\bm{z}_{s},\bm{z}_{1},\bm{z}_{2})\right]\right] -KL\left[q(Y\mid\bm{X})||\;p(Y)\right]\]

Similar to the ELBO for variational clustering in Section 5.1, computing \(KL\left[q(Y\mid\bm{X})||\;p(Y)\right]\) directly is intractable, and we need to bound it according to Lemma 4.2.

We compare the proposed DRPM-VAE to three methods, which only differ in how they infer shared and latent dimensions. While the Label-VAE (Bouchacourt et al., 2018; Hosoya, 2018) assumes that the number of independent factors is known, the Ada-VAE (Locatello et al., 2020) relies on a heuristic-based approach to infer shared and independent latent factors. Like in Locatello et al. (2020) and Sutter et al. (2023), we assume a single known factor for Label-VAE in all experiments. HG-VAE (Sutter et al., 2023) also relies on the MVHG to model the number of shared and independent factors. Unlike the proposed DRPM-VAE approach, HG-VAE must rely on a heuristic to assign latent dimensions to shared factors, as the MVHG only allows to model the number of shared and independent factors but not their position in the latent vector. We use the code from Locatello et al. (2020) and follow the evaluation in Sutter et al. (2023). We refer to Appendix C.3 for details on the ELBO, the setup of the experiment, the implementation, and an illustration of the generative assumptions.

We evaluate all methods according to their ability to estimate the number of shared generative factors (Figure 3) and how well they partition the latent representations into shared and independent factors (Table 2). Because we have access to the data-generating process, we can control the number of shared \(n_{s}\) and independent \(n_{i}\) factors. We compare the methods on four different datasets with \(n_{s}\in\{0,1,3,5\}\). In Figure 3, we demonstrate that the DRPM-VAE accurately estimates the true number of shared generative factors. It matches the performance of HG-VAE and outperforms the other two baselines, which consistently overestimate the true number of shared factors. In Table 2, we see a considerable performance improvement compared to previous work when assessing the learned latent representations. We attribute this to our ability to not only estimate the subset sizes of latent and shared factors like HG-VAE but also learn to assign specific latent dimensions to the corresponding shared or independent representations. Thus, the DRPM-VAE dynamically learns more meaningful representations and can better separate and infer the shared and independent subspaces for all dataset versions.

The DRPM-VAE provides empirical evidence of how RPMs can leverage weak supervision signals by learning to maximize the data likelihood while also inferring representations that capture the relationship between coupled data samples. Additionally, we can explicitly model the data-generating process in a theoretically grounded fashion instead of relying on heuristics.

### Multitask Learning

Many ML applications aim to solve specific tasks, where we optimize for a single objective while ignoring potentially helpful information from related tasks. Multitask learning (MTL) aims to improve the generalization across all tasks, including the original one, by sharing representations between related tasks (Caruana, 1993; Caruana and de Sa, 1996) Recent works (Kurin et al., 2022; Xin et al., 2022) show that it is difficult to outperform a convex combination of task losses if the task losses are appropriately scaled. I.e., in case of equal difficulty of the two tasks, a classifier with equal weighting of the two classification losses serves as an upper bound in terms of performance. However, finding suitable task weights is a tedious and inefficient approach to MTL. A more automated way of weighting multiple tasks would thus be vastly appreciated.

In this experiment, we demonstrate how the DRPM can learn task difficulty by partitioning a network layer. Intuitively, a task that requires many neurons is more complex than a task that can be solved using a single neuron. Based on this observation, we propose the DRPM-MTL. The DRPM-MTL learns to partition the neurons of the last shared layer such that only a subset of the neurons are used for every task. In contrast to the other experiments (Sections 5.1 and 5.2), we use the DRPM without resampling and infer the partition \(Y\) as a deterministic function. This can be done by applying the two-step procedure of Proposition 4.1 but skipping the resampling step of the MVHG and PL distributions. We compare the DRPM-MTL to the unitary loss scaling method (ULS, Kurin et al.,2022), which has a fixed architecture and scales task losses equally. Both DRPM-MTL and ULS use a network with shared architecture up to some layer, after which the network branches into two task-specific layers that perform the classifications. Note the difference between the methods. While the task-specific branches of the ULS method access all neurons of the last shared layer, the task-specific branches of the DRPM-MTL access only the subset of neurons reserved for the respective task.

We perform experiments on MultiMNIST (Sabour et al., 2017), which overlaps two MNIST digits in one image, and we want to classify both numbers from a single sample. Hence, the two tasks, classification of the left and the right digit (see Appendix C.4 for an example), are approximately equal in difficulty by default. To increase the difficulty of one of the two tasks, we introduce the noisyMultiMNIST dataset. There, we control task difficulty by adding salt and pepper noise to one of the two digits, subsequently increasing the difficulty of that task with increasing noise ratios. Varying the noise, we evaluate how our DRPM-MTL adapts to imbalanced difficulties, where one usually has to tediously search for optimal loss weights to reach good performance. We base our pipeline on (Sener and Koltun, 2018). For more details and additional CelebA MTL experiments we refer to Appendix C.4.

We evaluate the DRPM-MTL concerning its classification accuracy on the two tasks and compare the inferred subset sizes per task for different noise ratios \(\alpha\in\{0.0,\dots,0.9\}\) of the noisyMultiMNIST dataset (see Figure 4). The DRPM-MTL achieves the same or better accuracy on both tasks for most noise levels (upper part of Figure 4). It is interesting to see that, the more we increase \(\alpha\), the more the DRPM-MTL tries to overcome the increased difficulty of the right task by assigning more dimensions to it (lower part of Figure 4, noise ratio \(\alpha\)\(0.6\)-\(0.8\)). Note that for the maximum noise ratio of \(\alpha=0.9\), it seems that the DRPM-MTL basically surrenders and starts neglecting the right task, instead focusing on getting good performance on the left task, which impacts the average accuracy.

## Limitations & Future Work

The proposed two-stage approach to RPMs requires distributions over subset sizes and permutation matrices. The memory usage of the permutation matrix used in the two-stage RPM increases quadratically in the number of elements \(n\). Although we did not experience memory issues in our experiments, this may lead to problems when partitioning vast sets. Furthermore, learning subsets by first inferring an ordering of all elements can be a complex optimization problem. Approaches based on minimizing the earth mover's distance (Monge, 1781) to learn subset assignments could be an alternative to the ordering-based approach in our DRPM and pose an interesting direction for future work. Finally, note that we compute the probability mass function (PMF) \(p(Y;\bm{\omega},\bm{s})\) by approximating it with the bounds in Lemma 4.2. While the upper bound is tight when all scores have similar magnitude, the bound loosens if scores differ a lot, leading Equation (10) to overestimate the value of the PMF. In practice, we thus reweight the respective terms in the loss function, but in the future, we will investigate better estimates for the PMF.

Ultimately, we are interested in exploring how to apply the DRPM to multimodal learning under weak supervision, for instance, in medical applications. Section 5.2 demonstrated the potential of learning from coupled samples, but further research is needed to ensure fairness concerning underlying, hidden attributes when working with sensitive data.

Figure 4: Results for noisyMultiMNIST experiment. In the upper plot, we compare the task accuracy of the two methods ULS and the DRPM-MTL. We see that the DRPM-MTL can reach higher accuracy for most of the different noise ratios \(\alpha\) while it assigns the number of dimensions per task according to their difficulty.

## Conclusion

In this work, we proposed the differentiable random partition model, a novel approach to random partition models. Our two-stage method enables learning partitions end-to-end by separately controlling subset sizes and how elements are assigned to subsets. This new approach to partition learning enables the integration of random partition models into probabilistic and deterministic gradient-based optimization frameworks. We show the versatility of the proposed differentiable random partition model by applying it to three vastly different experiments. We demonstrate how learning partitions enables us to explore the modes of the data distribution, infer shared and independent generative factors from coupled samples, and learn task-specific sub-networks in applications where we want to solve multiple tasks on a single data point.

## Acknowledgements

AR is supported by the StimuLoop grant #1-007811-002 and the Vontobel Foundation. TS is supported by the grant #2021-911 of the Strategic Focal Area "Personalized Health and Related Technologies (PHRT)" of the ETH Domain (Swiss Federal Institutes of Technology).

## References

* Abadi et al. [2016] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. G. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker, V. Vanhoucke, V. Vasudevan, F. B. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. _CoRR_, abs/1603.0, 2016.
* Adams and Zemel [2011] R. P. Adams and R. S. Zemel. Ranking via sinkhorn propagation. _arXiv preprint arXiv:1106.1925_, 2011.
* Ahmed et al. [2022] K. Ahmed, Z. Zeng, M. Niepert, and G. V. d. Broeck. SIMPLE: A Gradient Estimator for k-Subset Sampling. In _The Eleventh International Conference on Learning Representations_, Sept. 2022.
* Bengio et al. [2013] Y. Bengio, N. Leonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _arXiv preprint arXiv:1308.3432_, 2013.
* Bishop and Svensen [2004] C. M. Bishop and M. Svensen. Robust Bayesian Mixture Modelling. _To appear in the Proceedings of ESANN_, page 1, 2004. Publisher: Citeseer.
* Bouchacourt et al. [2018] D. Bouchacourt, R. Tomioka, and S. Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. In _Thirty-Second AAAI Conference on Artificial Intelligence_, 2018.
* Cai et al. [2022] J. Cai, J. Fan, W. Guo, S. Wang, Y. Zhang, and Z. Zhang. Efficient deep embedded subspace clustering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1-10, 2022.
* Caruana [1993] R. Caruana. Multitask learning: A knowledge-based source of inductive bias. In _Machine learning: Proceedings of the tenth international conference_, pages 41-48, 1993.
* Caruana and de Sa [1996] R. Caruana and V. R. de Sa. Promoting poor features to supervisors: Some inputs work better as outputs. _Advances in Neural Information Processing Systems_, 9, 1996.
* Chesson [1976] J. Chesson. A non-central multivariate hypergeometric distribution arising from biased sampling with application to selective predation. _Journal of Applied Probability_, 13(4):795-797, 1976. Publisher: Cambridge University Press.
* Coates et al. [2011] A. Coates, A. Ng, and H. Lee. An Analysis of Single-Layer Networks in Unsupervised Feature Learning. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, June 2011. ISSN: 1938-7228.
* Coates et al. [2012]M. Cuturi, O. Teboul, and J.-P. Vert. Differentiable Ranks and Sorting using Optimal Transport, Nov. 2019. arXiv:1905.11885 [cs, stat].
* De la Cruz-Mesia et al. (2007) R. De la Cruz-Mesia, F. A. Quintana, and P. Muller. Semiparametric Bayesian Classification with longitudinal Markers. _Journal of the Royal Statistical Society Series C: Applied Statistics_, 56(2):119-137, Mar. 2007. ISSN 0035-9254. doi: 10.1111/j.1467-9876.2007.00569.x.
* Deng et al. (2009) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Dilokthanakul et al. (2016) N. Dilokthanakul, P. A. M. Mediano, M. Garnelo, M. C. H. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan. Deep unsupervised clustering with Gaussian mixture variational autoencoders. _arXiv preprint arXiv:1611.02648_, 2016.
* Fisher (1935) R. A. Fisher. The logic of inductive inference. _Journal of the royal statistical society_, 98(1):39-82, 1935. Publisher: JSTOR.
* Fog (2008) A. Fog. Calculation methods for Wallenius' noncentral hypergeometric distribution. _Communications in Statistics--Simulation and Computation(r)_, 37(2):258-273, 2008. Publisher: Taylor & Francis.
* Gondal et al. (2019) M. W. Gondal, M. Wuthrich, D. Miladinovic, F. Locatello, M. Breidt, V. Volchkov, J. Akpo, O. Bachem, B. Scholkopf, and S. Bauer. On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Graham et al. (1989) R. L. Graham, D. E. Knuth, O. Patashnik, S. L.-C. i. Physics, and undefined 1989. Concrete mathematics: a foundation for computer science. _aip.scitation.org_, 3(5):165, 1989. doi: 10.1063/1.4822863. Publisher: AIP Publishing.
* Grover et al. (2019) A. Grover, E. Wang, A. Zweig, and S. Ermon. Stochastic Optimization of Sorting Networks via Continuous Relaxations. In _International Conference on Learning Representations_, 2019.
* Theory and Methods_, 19(8):2745-2756, 1990. doi: 10.1080/03610929008830345. Publisher: Taylor & Francis.
* He et al. (2015) K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition, Dec. 2015.
* Higgins et al. (2016) I. Higgins, L. Matthey, A. Pal, C. Burgess, and X. Glorot. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
* Hosoya (2018) H. Hosoya. A simple probabilistic deep generative model for learning generalizable disentangled representations from grouped data. _CoRR_, abs/1809.0, 2018.
* Jang et al. (2016) E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.
* Jiang et al. (2016) Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: An unsupervised and generative approach to clustering. _arXiv preprint arXiv:1611.05148_, 2016.
* Johnson (1987) M. E. Johnson. _Multivariate statistical simulation: A guide to selecting and generating continuous multivariate distributions_, volume 192. John Wiley & Sons, 1987.
* Kingma and Welling (2014) D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In _International Conference on Learning Representations_, 2014.
* Kurin et al. (2022) V. Kurin, A. D. Palma, I. Kostrikov, S. Whiteson, and M. P. Kumar. In Defense of the Unitary Scalarization for Deep Multi-Task Learning. _CoRR_, abs/2201.04122, 2022.
* LeCun et al. (1998) Y. LeCun, C. Cortes, and C. J. Burges. Gradient-based learning applied to document recognition. In _Proceedings of the IEEE_, volume 86, pages 2278-2324, 1998. Issue: 11.
* Lee and Sang (2022) C. J. Lee and H. Sang. Why the Rich Get Richer? On the Balancedness of Random Partition Models. _arXiv preprint arXiv:2201.12697_, 2022.
* Lee et al. (2016)Y. Li, M. Yang, D. Peng, T. Li, J. Huang, and X. Peng. Twin Contrastive Learning for Online Clustering. _International Journal of Computer Vision_, 130(9):2205-2221, Sept. 2022. ISSN 0920-5691, 1573-1405. doi: 10.1007/s11263-022-01639-z. arXiv:2210.11680 [cs].
* Linderman et al. [2018] S. W. Linderman, G. E. Mena, H. J. Cooper, L. Paninski, and J. P. Cunningham. Reparameterizing the Birkhoff Polytope for Variational Permutation Inference. In A. J. Storkey and F. Perez-Cruz, editors, _International Conference on Artificial Intelligence and Statistics, [AISTATS] 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain_, volume 84 of _Proceedings of Machine Learning Research_, pages 1618-1627. PMLR, 2018.
* Liu et al. [2015] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep Learning Face Attributes in the Wild, Sept. 2015. arXiv:1411.7766 [cs].
* Locatello et al. [2020] F. Locatello, B. Poole, G. Ratsch, B. Scholkopf, O. Bachem, and M. Tschann. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, pages 6348-6359. PMLR, 2020.
* Loshchilov and Hutter [2019] I. Loshchilov and F. Hutter. Decoupled Weight Decay Regularization, Jan. 2019. arXiv:1711.05101 [cs, math].
* Luce [1959] R. D. Luce. _Individual choice behavior: A theoretical analysis_. Courier Corporation, 1959.
* MacQueen [1967] J. MacQueen. Classification and analysis of multivariate observations. In _5th Berkeley Symp. Math. Statist. Probability_, pages 281-297, 1967.
* Maddison et al. [2017] C. Maddison, A. Mnih, and Y. Teh. The concrete distribution: A continuous relaxation of discrete random variables. In _International Conference on Learning Representations_, 2017.
* Manduchi et al. [2021] L. Manduchi, K. Chin-Cheong, H. Michel, S. Wellmann, and J. E. Vogt. Deep Conditional Gaussian Mixture Model for Constrained Clustering. _ArXiv_, abs/2106.0, 2021.
* Mansour and Schork [2016] T. Mansour and M. Schork. _Commutation relations, normal ordering, and Stirling numbers_. CRC Press Boca Raton, 2016.
* Marcus [1960] M. Marcus. Some Properties and Applications of Doubly Stochastic Matrices. _The American Mathematical Monthly_, 67(3):215-221, 1960. ISSN 00029890, 19300972. Publisher: Mathematical Association of America.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018.
* Monge [1781] G. Monge. Memoire sur la theorie des deblais et des remblais. _Mem. Math. Phys. Acad. Royale Sci._, pages 666-704, 1781.
* Niepert et al. [2021] M. Niepert, P. Minervini, and L. Franceschi. Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions. In _Advances in Neural Information Processing Systems_, volume 34, pages 14567-14579. Curran Associates, Inc., 2021.
* Paszke et al. [2019] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. _CoRR_, abs/1912.0, 2019.
* Petersen et al. [2021] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen. Monotonic Differentiable Sorting Networks. In _International Conference on Learning Representations_, 2021.
* Pitman [1996] J. Pitman. Some Developments of the Blackwell-Macqueen URN Scheme. _Lecture Notes-Monograph Series_, 30:245-267, 1996. ISSN 07492170. Publisher: Institute of Mathematical Statistics.
* Plackett [1975] R. L. Plackett. The analysis of permutations. _Journal of the Royal Statistical Society: Series C (Applied Statistics)_, 24(2):193-202, 1975. Publisher: Wiley Online Library.
* Pogogor [1975]M. V. Pogancic, A. Paulus, V. Musil, G. Martius, and M. Rolinek. Differentiation of Blackbox Combinatorial Solvers. In _International Conference on Learning Representations_, Sept. 2019.
* Prillo and Eisenschlos [2020] S. Prillo and J. Eisenschlos. SoftSort: A Continuous Relaxation for the argsort Operator. In _Proceedings of the 37th International Conference on Machine Learning, [ICML] 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 7793-7802. PMLR, 2020.
* Rota [1964] G.-C. Rota. The Number of Partitions of a Set. _The American Mathematical Monthly_, 71(5):498, May 1964. ISSN 00029890. doi: 10.2307/2312585. Publisher: JSTOR.
* Rubner et al. [2000] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth mover's distance as a metric for image retrieval. _International journal of computer vision_, 40(2):99, 2000. Publisher: Springer Nature BV.
* Sabour et al. [2017] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic Routing Between Capsules. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Santa Cruz et al. [2017] R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould. Deeppermnet: Visual permutation learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3949-3957, 2017.
* Sarfraz et al. [2019] S. Sarfraz, V. Sharma, and R. Stiefelhagen. Efficient parameter-free clustering using first neighbor relations. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8934-8943, 2019.
* Sener and Koltun [2018] O. Sener and V. Koltun. Multi-task learning as multi-objective optimization. _Advances in neural information processing systems_, 31, 2018.
* Sinkhorn [1964] R. Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. _The annals of mathematical statistics_, 35(2):876-879, 1964. Publisher: JSTOR.
* Sutter et al. [2023] T. M. Sutter, L. Manduchi, A. Ryser, and J. E. Vogt. Learning Group Importance using the Differentiable Hypergeometric Distribution. In _The Eleventh International Conference on Learning Representations_, 2023.
* Thurstone [1927] L. L. Thurstone. A law of comparative judgment. In _Scaling_, pages 81-92. Routledge, 1927.
* Wallenius [1963] K. T. Wallenius. Biased sampling; the noncentral hypergeometric probability distribution. Technical report, Stanford Univ Ca Applied Mathematics And Statistics Labs, 1963.
* Xiao et al. [2017] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms, Sept. 2017. arXiv:1708.07747 [cs, stat].
* Xie and Ermon [2019] S. M. Xie and S. Ermon. Reparameterizable subset sampling via continuous relaxations. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, IJCAI'19, pages 3919-3925, Macao, China, Aug. 2019. AAAI Press. ISBN 978-0-9992411-4-1.
* Xin et al. [2022] D. Xin, B. Ghorbani, A. Garg, O. Firat, and J. Gilmer. Do Current Multi-Task Optimization Methods in Deep Learning Even Help? _arXiv preprint arXiv:2209.11379_, 2022.
* Yang et al. [2019] X. Yang, C. Deng, F. Zheng, J. Yan, and W. Liu. Deep spectral clustering using dual autoencoder network. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4066-4075, 2019.
* Yellott [1977] J. I. Yellott. The relationship between Luce's choice axiom, Thurstone's theory of comparative judgment, and the double exponential distribution. _Journal of Mathematical Psychology_, 15(2):109-144, 1977. Publisher: Elsevier.

Preliminaries

### Hypergeometric Distribution

This part is largely based on Sutter et al. (2023).

Suppose we have an urn with marbles in different colors. Let \(K\in\mathbb{N}\) be the number of different classes or groups (e.g. marble colors in the urn), \(\bm{m}=[m_{1},\ldots,m_{K}]\in\mathbb{N}^{K}\) describe the number of elements per class (e.g. marbles per color), \(N=\sum_{k=1}^{K}m_{K}\) be the total number of elements (e.g. all marbles in the urn) and \(n\in\{0,\ldots,N\}\) be the number of elements (e.g. marbles) to draw. Then, the multivariate hypergeometric distribution describes the probability of drawing \(\bm{n}=[n_{1},\ldots,n_{K}]\in\mathbb{N}^{K}\) marbles by sampling without replacement such that \(\sum_{k=1}^{K}n_{k}=n\), where \(n_{k}\) is the number of drawn marbles of class \(k\).

In the literature, two different versions of the noncentral hypergeometric distribution exist, Fisher's (Fisher, 1935) and Wallenius' (Wallenius, 1963; Chesson, 1976) distribution. Sutter et al. (2023) restrict themselves to Fisher's noncentral hypergeometric distribution due to limitations of the latter (Fog, 2008). Hence, we will also talk solely about Fisher's noncentral hypergeometric distribution.

**Definition A.1** (Multivariate Fisher's Noncentral Hypergeometric Distribution (Fisher, 1935)).: _A random vector \(\bm{X}\) follows Fisher's noncentral multivariate distribution, if its joint probability mass function is given by_

\[P(\bm{N}=\bm{n};\bm{\omega})=p(\bm{n};\bm{\omega})=\frac{1}{P_{0}} \prod_{k=1}^{K}\binom{m_{k}}{n_{k}}\omega_{k}^{n_{k}}\] (12)

\[\text{where}\ \ \ P_{0}=\sum_{(n_{1},\ldots,n_{K})\in\mathcal{S}}\prod_{k=1}^{K} \binom{m_{k}}{\eta_{k}}\omega_{k}^{\eta_{k}}\] (13)

_The support \(S\) of the PMF is given by \(S=\{\bm{n}\in\mathbb{N}^{K}:\forall k\ \ \ n_{k}\leq m_{k},\sum_{k=1}^{K}n_{k}=n\}\) and \(\binom{n}{k}=\frac{n!}{k!(n-k)!}\)._

The class importance \(\bm{\omega}\) is a crucial modeling parameter in applying the noncentral hypergeometric distribution (see (Chesson, 1976)).

#### a.1.1 Differentiable MVHG

Their reparameterizable sampling for the differentiable MVHG consists of three parts:

1. Reformulate the multivariate distribution as a sequence of interdependent and conditional univariate hypergeometric distributions.
2. Calculate the probability mass function of the respective univariate distributions.
3. Sample from the conditional distributions utilizing the Gumbel-Softmax trick.

Following the chain rule of probability, the MVHG distribution allows for sequential sampling over classes \(k\). Every step includes a merging operation, which leads to biased samples compared to groundtruth non-differentiable sampling with equal class weights \(\bm{\omega}\). Given that we intend to use the differentiable MVHG in settings where we want to learn the unknown class weights, we do not expect a negative effect from this sampling procedure. For details on how to merge the MVHG into a sequence of unimodal distributions, we refer to Sutter et al. (2023).

The probability mass function calculation is based on unnormalized log-weights, which are interpreted as unnormalized log-weights of a categorical distribution. The interpretation of the class-conditional unimodal hypergeometric distributions as categorical distributions allows applying the Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2017). Following the use of the Gumbel-Softmax trick, the class-conditional version of the hypergeometric distribution is differentiable and reparameterizable. Hence, the MVHG has been made differentiable and reparameterizable as well. Again, for details we refer to the original paper (Sutter et al., 2023).

### Distribution over Random Orderings

Yellott (1977) show that the distribution over permutation matrices \(p(\pi;\bm{s})\) follows a Plackett-Luce (PL) distribution (Plackett, 1975; Luce, 1959), if and only of the perturbed scores \(\bm{\tilde{s}}\) are sampled independently from Gumbel distributions with identical scales. For each item \(i\), sample \(g_{i}\sim\text{Gumbel}(0,\beta)\) independently with zero mean and and fixed scale \(\beta\). Let \(\bm{\tilde{s}}\) be the vector of Gumbel perturbed log-weights such that \(\tilde{s}_{i}=\beta\log s_{i}+g_{i}\). Hence,

\[q(\tilde{s}_{1}\geq\cdots\geq\tilde{s}_{n})=\frac{s_{1}}{Z}\cdot\frac{s_{2}}{ Z-s_{1}}\cdot\cdots\cdot\frac{s_{n}}{Z-\sum_{i=1}^{n-1}s_{i}}\] (14)

We refer to Yellott (1977) or Grover et al. (2019) for the proof. However, Grover et al. (2019) provide only an adapted proof sketch from Yellott (1977). The probability of sampling element \(i\) first is given by its score \(s_{i}\) divided by the sum of all weights in the set

\[q(\tilde{s}_{i})=\frac{s_{i}}{Z}\] (15)

For \(z_{i}=\log s_{i}\), the right hand side of Equation (15) is equal to the softmax distribution \(\text{softmax}(z_{i})=\exp(z_{i})/\sum_{j}\exp(z_{j})\) as already described in (Xie and Ermon, 2019). Hence, Equation (15) directly leads to the Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2017).

#### a.2.1 Differentiable Sorting

In the main text of the paper we rely on a differentiable function \(f_{\pi}(\bm{\tilde{s}})\), which sorts the resampled version of the scores \(\bm{s}\)

\[\pi=f_{\pi}(\bm{\tilde{s}})=\text{sort}(\bm{\tilde{s}})\] (16)

Here, we summarise the findings from Grover et al. (2019) on how to construct such a differentiable sorting operator. As already mentioned in Section 2, there are multiple works on the topic (Prillo and Eisenschlos, 2020; Petersen et al., 2021; Mena et al., 2018), but we restrict ourselves to the work of Grover et al. (2019) as we see the differentiable generation of permutation matrices as a tool in our pipeline.

**Corollary A.2** (Permutation Matrix (Grover et al., 2019)).: _Let \(\bm{s}=[s_{1},\ldots,s_{n}]^{T}\) be a real-valued vector of length \(n\). Let \(A_{\bm{s}}\) denote the matrix of absolute pairwise differences of the elements of \(\bm{s}\) such that \(A_{\bm{s}}[i,j]=|s_{i}-s_{j}|\). The permutation matrix \(\pi\) corresponding to sort\((\bm{s})\) is given by:_

\[\pi=\begin{cases}1&\text{if }j=\arg\max[(n+1-2i)\bm{s}-A_{\bm{s}}\mathbbm{1}]\\ 0&\text{otherwise}\end{cases}\] (17)

_where \(\mathbbm{1}\) denotes the column vector of all ones._

As we know, the \(\arg\max\) operator is non-differentiable which prohibits the direct use of Corollary A.2 for gradient computation. Hence, Grover et al. (2019) propose to replace the \(\arg\max\) operator with \(\text{softmax}\) to obtain a continuous relaxation \(\pi(\tau)\) similar to the GS trick (Jang et al., 2016; Maddison et al., 2017). In particular, the \(i\)th row of \(\pi(\tau)\) is given by:

\[\pi(\tau)[i,:]=\text{softmax}[(n+1-2i)\bm{s}-A_{\bm{s}}\mathbbm{1}/\tau]\] (18)

where \(\tau>0\) is a temperature parameter. We adapted this section from Grover et al. (2019) and we also refer to their original work for more details on how to generate differentiable permutation matrices.

In this, work we remove the temperature parameter \(\tau\) to reduce clutter in the notation. Hence, we only write \(\pi\) instead of \(\pi(\tau)\), although it is still needed for the generation of the matrix \(\pi\). For details on how we select the temperature parameter \(\tau\) in our experiments, we refer to Appendix C.

## Appendix B Detailed Derivation of the Differentiable Two-Stage Random Partition Model

### Two-Stage Partition Model

We want to partition \(n\) elements \([n]=\{1,\ldots,n\}\) into \(K\) subsets \(\{\mathcal{S}_{1},\ldots,\mathcal{S}_{K}\}\) where \(K\) is _a priori_ unknown.

**Definition B.1** (Partition).: _A partition \(\rho\) of a set of elements \([n]=\{1,\ldots,n\}\) is a collection of subsets \((\mathcal{S}_{1},\ldots,\mathcal{S}_{K})\) such that_

\[\mathcal{S}_{1}\cup\cdots\cup\mathcal{S}_{K}=[n]\ \ \text{and}\ \ \forall i\neq j:\ \mathcal{S}_{i}\cap \mathcal{S}_{j}=\emptyset\] (19)

Put differently, every element \(i\) has to be assigned to precisely one subset \(\mathcal{S}_{k}\). We denote the size of the \(k\)-th subset \(\mathcal{S}_{k}\) as \(n_{k}=|\mathcal{S}_{k}|\). Alternatively, we describe a partition \(\rho\) as an assignment matrix \(Y=[\bm{y}_{1},\ldots,\bm{y}_{K}]^{T}\in\{0,1\}^{K\times n}\). Every row \(\bm{y}_{k}\in\{0,1\}^{1\times n}\) is a multi-hot vector, where \(\bm{y}_{ki}=1\) assigns element \(i\) to subset \(\mathcal{S}_{k}\).

In this work, we propose a new two-stage procedure to learn partitions. The proposed formulation separately infers the number of elements per subset \(n_{k}\) and the assignment of elements to subsets \(\mathcal{S}_{k}\) by inducing an order on the \(n\) elements and filling \(\mathcal{S}_{1},...,\mathcal{S}_{K}\) sequentially in this order. See Figure 1 for an example.

**Definition B.2** (Two-stage partition model).: _Let \(\bm{n}=[n_{1},\ldots,n_{K}]\in\mathbb{N}_{0}^{K}\) be the subset sizes in \(\rho\), with \(\mathbb{N}_{0}\) the set of natural numbers including \(0\) and \(\sum_{k=1}^{K}n_{k}=n\), where \(n\) is the total number of elements. Let \(\pi\in\{0,1\}^{n\times n}\) be a permutation matrix that defines an order over the \(n\) elements. We define the two-stage partition model of \(n\) elements into \(K\) subsets as an assignment matrix \(Y=[\bm{y}_{1},\ldots,\bm{y}_{K}]^{T}\in\{0,1\}^{K\times n}\) with_

\[\bm{y}_{k}=\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i},\ \ \ \ \text{ where}\ \ \nu_{k}=\sum_{i=1}^{k-1}n_{\iota}\] (20)

_such that \(Y=[\{\bm{y}_{k}\ |\ n_{k}>0\}_{k=1}^{K}]^{T}\)._

Note that in contrast to previous work on partition models (Mansour and Schork, 2016), we allow \(\mathcal{S}_{k}\) to be the empty set \(\emptyset\). Hence, \(K\) defines the maximum number of possible subsets, not the effective number of non-empty subsets.

To model the order of the elements, we use a permutation matrix \(\pi=[\bm{\pi}_{1},\ldots,\bm{\pi}_{n}]^{T}\in\{0,1\}^{n\times n}\) which is a square matrix where every row and column sums to \(1\). This doubly-stochastic property of all permutation matrices \(\pi\)(Marcus, 1960) thus ensures that the columns of \(Y\) remain one-hot vectors. At the same time, its rows correspond to \(n_{k}\)-hot vectors \(\bm{y}_{k}\) in Definition B.2 and therefore serve as subset assignment vectors.

**Corollary B.3**.: _A two-stage partition model \(Y\), which follows Definition B.2, is a valid partition satisfying Definition B.1._

Proof.: By definition, every row \(\bm{\pi}_{i}\) and column \(\bm{\pi}_{j}\) of \(\pi\) is a one-hot vector, hence every \(\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i}\) results in different, non-overlapping \(n_{k}\)-hot encodings, ensuring \(\mathcal{S}_{i}\cap\mathcal{S}_{j}=\emptyset\ \ \forall\ \ i,j\) and \(i\neq j\). Further, since \(n_{k}\)-hot encodings have exactly \(n_{k}\) entries with \(1\), we have \(\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\sum_{j=1}^{n}\bm{\pi}_{ij}=n_{k}\). Hence, since \(\sum_{k=1}^{K}n_{k}=n\), every element \(i\) is assigned to a \(\bm{y}_{k}\), ensuring \(\mathcal{S}_{1}\cup\cdots\cup\mathcal{S}_{K}=[n]\). 

### Two-Stage Random Partition Models

An RPM \(p(Y)\) defines a probability distribution over partitions \(Y\). In this section, we derive how to extend the two-stage procedure from Definition B.2 to the probabilistic setting to create a two-stage RPM. To derive the two-stage RPM's probability distribution \(p(Y)\), we need to model distributions over \(\bm{n}\) and \(\pi\). We choose the MVHG distribution \(p(\bm{n};\bm{\omega})\) and the PL distribution \(p(\pi;\bm{s})\) (see Section 3).

We calculate the probability \(p(Y;\bm{\omega},\bm{s})\) sequentially over the probabilities of subsets \(p_{\bm{y}_{k}}:=p(\bm{y}_{k}\ |\ \bm{y}_{<k};\bm{\omega},\bm{s})\). \(p_{\bm{y}_{k}}\) itself depends on the probability over subset permutations \(p_{\bar{\pi}_{k}}:=p(\bar{\pi}\ |\ n_{k},\bm{y}_{<k};\bm{s})\), where a subset permutation matrix \(\bar{\pi}\) represents an ordering over \(n_{k}\) out of \(n\) elements.

**Definition B.4** (Subset permutation matrix \(\bar{\pi}\)).: _A subset permutation matrix \(\bar{\pi}\in\{0,1\}^{n_{k}\times n}\), where \(n_{k}\leq n\), must fulfill_

\[\forall i\leq n_{k}:\ \sum_{j=1}^{n}\bar{\pi}_{ij}=1\ \ \text{and}\ \ \forall j\leq n:\ \sum_{i=1}^{n_{k}}\bar{\pi}_{ij}\leq 1.\]We describe the probability distribution over subset permutation matrices \(p_{\pi_{k}}\) using Definition B.4 and Equation (3).

**Lemma B.5** (Probability over subset permutations \(p_{\bar{\pi}_{k}}\)).: _The probability \(p_{\bar{\pi}_{k}}\) of any subset permutation matrix \(\bar{\pi}=[\bar{\bm{\pi}}_{1},\ldots,\bar{\bm{\pi}}_{n_{k}}]^{T}\in\{0,1\}^{n_{ k}\times n}\) is given by_

\[p_{\bar{\pi}_{k}}:=p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})=\prod_{i=1}^{n_{k} }\frac{(\bar{\pi}\bm{s})_{i}}{Z_{k}-\sum_{j=1}^{i-1}(\bar{\pi}\bm{s})_{j}}\] (21)

_where \(\bm{y}_{<k}=\{\bm{y}_{1},...,\bm{y}_{k-1}\}\), \(Z_{k}=Z-\sum_{j\in\mathcal{S}_{<k}}\bm{s}_{j}\) and \(\mathcal{S}_{<k}=\bigcup_{j=1}^{k-1}\mathcal{S}_{j}\)._

Proof.: We provide the proof for \(p_{\bar{\pi}_{1}}\), but it is equivalent for all other subsets. Without loss of generality, we assume that there are \(n_{1}\) elements in \(\mathcal{S}_{1}\). Following Equation (3), the probability of a permutation matrix \(p(\pi;\bm{s})\) is given by

\[p(\pi;\bm{s})= \frac{(\pi\bm{s})_{1}}{Z}\frac{(\pi\bm{s})_{2}}{Z-(\pi\bm{s})_{1 }}\cdots\frac{(\pi\bm{s})_{n}}{Z-\sum_{j=1}^{n-1}(\pi\bm{s})_{j}}\] (22)

At the moment, we are only interested in the ordering of the first \(n_{1}\) elements. The probability of the first \(n_{1}\) is given by marginalizing over the remaining \(n-n_{1}\) elements:

\[p(\bar{\pi}\mid n_{1};\bm{\omega})=\sum_{\pi\in\Pi_{1}}p(\pi\mid\bm{s})\] (23)

where \(\Pi_{1}\) is the set of permutation matrices such that the top \(n_{1}\) rows select the elements in a specific ordering \(\bar{\pi}\in\{0,1\}^{n_{1}\times n}\), i.e. \(\Pi_{1}=\{\pi:[\bm{\pi}_{1},\ldots,\bm{\pi}_{n_{1}}]^{T}=\bar{\pi}\}\). It follows

\[p(\bar{\pi}\mid n_{1};\bm{\omega}) =\sum_{\pi\in\Pi_{1}}p(\pi\mid\bm{s})\] (24) \[= \sum_{\pi\in\Pi_{1}}\prod_{i=1}^{n}\frac{(\pi\bm{s})_{i}}{Z-\sum_ {j=1}^{i-1}(\pi\bm{s})_{j}}\] (25) \[= \prod_{i=1}^{n_{1}}\frac{(\bar{\pi}\bm{s})_{i}}{Z-\sum_{j=1}^{i-1 }(\bar{\pi}\bm{s})_{j}}\sum_{\pi\in\Pi_{1}}\prod_{i=1}^{n-n_{1}}\frac{(\pi\bm{s })_{n_{1}+i}}{Z-\sum_{j=1}^{n_{1}}(\bar{\pi}\bm{s})_{j}-\sum_{j=1}^{i-1}(\bar{ \pi}\bm{s})_{j}}\] (26) \[= \prod_{i=1}^{n_{1}}\frac{(\bar{\pi}\bm{s})_{i}}{Z-\sum_{j=1}^{i-1 }(\bar{\pi}\bm{s})_{j}}\sum_{\pi\in\Pi_{1}}\prod_{i=1}^{n-n_{1}}\frac{(\pi\bm{s })_{n_{1}+i}}{Z_{1}-\sum_{j=1}^{i-1}(\bar{\pi}\bm{s})_{j}}\] (27)

where \(Z_{1}=Z-\sum_{j=1}^{n_{1}}(\bar{\pi}\bm{s})_{j}\). It follows

\[p(\bar{\pi}\mid n_{1};\bm{\omega})= \prod_{i=1}^{n_{1}}\frac{(\bar{\pi}\bm{s})_{i}}{Z-\sum_{j=1}^{i-1 }(\bar{\pi}\bm{s})_{j}}\] (28)

Lemma B.5 describes the probability of drawing the elements \(i\in\mathcal{S}_{k}\) in the order described by the subset permutation matrix \(\bar{\pi}\) given that the elements in \(\mathcal{S}_{<k}\) are already determined. Note that in a slight abuse of notation, we use \(p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{\omega},\bm{s})\) as the probability of a subset permutation \(\bar{\pi}\) given that there are \(n_{k}\) elements in \(\mathcal{S}_{k}\) and thus \(\bar{\pi}\in\{0,1\}^{n_{k}\times n}\). Additionally, we condition on the subsets \(\bm{y}_{<k}\) and \(n_{k}\), the size of subset \(\mathcal{S}_{k}\). In contrast to the distribution over permutations matrices \(p(\pi;\bm{s})\) in Equation (3), we take the product over \(n_{k}\) terms and have a different normalization constant \(Z_{k}\). Although we induce an ordering over all elements \(i\) in Definition B.2, the probability \(p_{\bm{y}_{k}}\) is invariant to intra-subset orderings of elements \(i\in\mathcal{S}_{k}\).

**Lemma B.6** (Probability distribution \(p_{\bm{y}_{k}}\)).: _The probability distribution over subset assignments \(p_{\bm{y}_{k}}\) is given by_

\[p_{\bm{y}_{k}}:=p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})=p(n_{k}\mid n_ {<k};\bm{\omega})\sum_{\bar{\pi}\in\Pi_{\bm{y}_{k}}}p(\bar{\pi}\mid n_{k},\bm{y }_{<k};\bm{s})\]

_where \(\Pi_{\bm{y}_{k}}=\{\bar{\pi}\in\{0,1\}^{n_{k}\times n}:\bm{y}_{k}=\sum_{i=1}^{n _{k}}\bar{\bm{\pi}}_{i}\}\) and \(p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})\) as in Lemma B.5._Proof.: We can proof the statement of Lemma B.6 as follows:

\[p_{\bm{y}_{k}} =p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})\] \[=\sum_{n_{k}^{\prime}}p(\bm{y}_{k},n_{k}^{\prime}\mid\bm{y}_{<k}; \bm{\omega},\bm{s})\] (29) \[=\sum_{n_{k}^{\prime}}p(n_{k}^{\prime}\mid\bm{y}_{<k};\bm{\omega},\bm{s})p(\bm{y}_{k}\mid n_{k}^{\prime},\bm{y}_{<k};\bm{\omega},\bm{s})\] (30) \[=\sum_{n_{k}^{\prime}}p(n_{k}^{\prime}\mid n_{<k};\bm{\omega},\bm {s})p(\bm{y}_{k}\mid n_{k}^{\prime},\bm{y}_{<k};\bm{s})\] (31) \[=p(n_{k}\mid n_{<k};\bm{\omega},\bm{s})p(\bm{y}_{k}\mid n_{k},\bm {y}_{<k};\bm{s})\] (32) \[=p(n_{k}\mid n_{<k};\bm{\omega})\sum_{\bar{\pi}\in\Pi_{\bm{y}_{k} }}p(\bar{\pi}\mid n_{k},\bm{y}_{<k};\bm{s})\] (33)

Equation (29) holds by marginalization, where \(n_{k}^{\prime}\) denotes the random variable that stands for the size of subset \(\mathcal{S}_{k}\). By Bayes' rule, we can then derive Equation (30). The next derivations stem from the fact that we can compute \(n_{<k}\) if \(\bm{y}_{<k}\) is given, as the assignments \(\bm{y}_{<k}\) hold information on the size of subsets \(\mathcal{S}_{<k}\). More explicitly, \(n_{i}=\sum_{j=1}^{n}y_{ij}\). Further, \(\bm{y}_{k}\) is independent of \(\bm{\omega}\) if the size \(n_{k}^{\prime}\) of subset \(\mathcal{S}_{k}\) is given, leading to Equation (31). We further observe that \(p(\bm{y}_{k}\mid n_{k}^{\prime},\bm{y}_{<k};\bm{s})\) is only non-zero, if \(n_{k}^{\prime}=\sum_{i=1}^{n}y_{ki}=n_{k}\). Dropping all zero terms from the sum in Equation (31) thus results in Equation (32). Finally, by Definition B.2, we know that \(\bm{y}_{k}=\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i}\), where \(\nu_{k}=\sum_{\iota=1}^{k-1}n_{\iota}\) and \(\pi\in\{0,1\}^{n\times n}\) a permutation matrix. Hence, in order to get \(\bm{y}_{k}\) given \(\bm{y}_{<k}\), we need to marginalize over all permutations of the elements of \(\bm{y}_{k}\) given that the elements in \(\bm{y}_{<k}\) are already ordered, which corresponds exactly to marginalizing over all subset permutation matrices \(\bar{\pi}\) such that \(\bm{y}_{k}=\sum_{i=1}^{n_{k}}\bm{\bar{\pi}}_{i}\), resulting in Equation (33). 

In Lemma B.6, we describe the set of all subset permutations \(\bar{\pi}\) of elements \(i\in\mathcal{S}_{k}\) by \(\Pi_{\bm{y}_{k}}\). Put differently, we make \(p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})\) invariant to the ordering of elements \(i\in\mathcal{S}_{k}\) by marginalizing over the probabilities of subset permutations \(p_{\bar{\pi}_{k}}\)(Xie and Ermon, 2019).

Using Lemmas B.5 and B.6, we propose the two-stage random partition \(p(Y;\bm{\omega},\bm{s})\). Since \(Y=[\bm{y}_{1},\ldots,\bm{y}_{K}]^{T}\), we calculate \(p(Y;\bm{\omega},\bm{s})\), the PMF of the two-stage RPM, sequentially using Lemmas B.5 and B.6, where we leverage the PL distribution for permutation matrices \(p(\pi;\bm{s})\) to describe the probability distribution over subsets \(p(\bm{y}_{k}\mid\bm{y}_{<k};\bm{\omega},\bm{s})\).

**Proposition 4.1** (Two-Stage Random Partition Model).: Given a probability distribution over subset sizes \(p(\bm{n};\bm{\omega})\) with \(\bm{n}\in\mathbb{N}_{0}^{K}\) and distribution parameters \(\bm{\omega}\in\mathbb{R}_{+}^{K}\) and a PL probability distribution over random orderings \(p(\pi;\bm{s})\) with \(\pi\in\{0,1\}^{n\times n}\) and distribution parameters \(\bm{s}\in\mathbb{R}_{+}^{n}\), the probability mass function \(p(Y;\bm{\omega},\bm{s})\) of the two-stage RPM is given by

\[p(Y;\bm{\omega},\bm{s})=p(\bm{y}_{1},\ldots,\bm{y}_{K};\bm{\omega},\bm{s})=p( \bm{n};\bm{\omega})\sum_{\pi\in\Pi_{Y}}p(\pi;\bm{s})\] (34)

where \(\Pi_{Y}=\{\pi:\bm{y}_{k}=\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i},k=1, \ldots,K\}\), and \(\bm{y}_{k}\) and \(\nu_{k}\) as in Definition B.2.

Proof.: Using Lemmas B.5 and B.6, we write

\[p(Y)= p(\bm{y}_{1},\dots,\bm{y}_{K};\bm{\omega},\bm{s})=p(\bm{y}_{1};\bm{ \omega},\bm{s})\cdots p(\bm{y}_{K}\mid\{\bm{y}_{j}\}_{j<K};\bm{\omega},\bm{s})\] \[= \left(p(n_{1};\bm{\omega})\sum_{\bar{\pi}_{1}\in\Pi_{\bm{y}_{1}}}p (\bar{\pi}_{1}\mid n_{1};\bm{s})\right)\] \[\cdots\left(p(n_{K}\mid\{n_{j}\}_{j<K};\bm{\omega})\sum_{\bar{\pi }_{K}\in\Pi_{\bm{y}_{K}}}p(\bar{\pi}_{K}\mid\{n_{j}\}_{j\leq K};\bm{s})\right)\] (35) \[= p(n_{1};\bm{\omega})\cdots p(n_{K}\mid\{n_{K}\}_{j<K};\bm{\omega})\] \[\cdot\left(\sum_{\bar{\pi}_{1}\in\Pi_{\bm{y}_{1}}}p(\bar{\pi}_{1 }\mid n_{1};\bm{s})\cdots\sum_{\pi_{K}\in\Pi_{\bm{y}_{K}}}p(\bar{\pi}_{K}\mid\{ n_{j}\}_{j\leq K};\bm{s})\right)\] (36) \[= p(\bm{n};\bm{\omega})\left(\sum_{\bar{\pi}_{1}\in\Pi_{\bm{y}_{1 }}}\cdots\sum_{\pi_{K}\in\Pi_{\bm{y}_{K}}}p(\bar{\pi}_{1}\mid n_{1};\bm{s}) \cdots p(\bar{\pi}_{K}\mid\{n_{j}\}_{j\leq K};\bm{s})\right)\] (37) \[= p(\bm{n};\bm{\omega})\sum_{\pi\in\Pi_{Y}}p(\pi\mid\bm{n};\bm{s})\] (38) \[= p(\bm{n};\bm{\omega})\sum_{\pi\in\Pi_{Y}}p(\pi;\bm{s})\] (39)

### Approximating the Probability Mass Function

**Lemma 4.2**.: \(p(Y;\bm{\omega},\bm{s})\) can be upper and lower bounded as follows

\[\forall\pi\in\Pi_{Y}:\ p(\bm{n};\bm{\omega})p(\pi;\bm{s})\ \leq\ p(Y;\bm{\omega},\bm{s})\ \leq\ |\Pi_{Y}|p(\bm{n};\bm{\omega})\max_{\bar{\pi}}p(\bar{\pi};\bm{s})\] (40)

Proof.: Since \(p(\pi;\bm{s})\) is a probability we know that \(\forall\pi\in\{0,1\}^{n\times n}\ \ p(\pi;\bm{s})\geq 0\). Thus, it follows directly that:

\[\forall\pi\in\Pi_{Y}:\ \ p(Y;\bm{\omega},\bm{s})=p(\bm{n};\bm{\omega})\sum_{ \pi^{\prime}\in\Pi_{Y}}p(\pi^{\prime};\bm{s})\geq p(\bm{n};\bm{\omega})p(\pi; \bm{s}),\]

proving the lower bound of Lemma 4.2.

On the other hand, can prove the upper bound in Lemma 4.2 by:

\[p(Y;\bm{\omega},\bm{s})=p(\bm{n};\bm{\omega})\sum_{\pi^{\prime} \in\Pi_{Y}}p(\pi^{\prime};\bm{s})\] \[\leq p(\bm{n};\bm{\omega})\sum_{\pi^{\prime}\in\Pi_{Y}}\max_{\pi\in \Pi_{Y}}p(\pi;\bm{s})\] \[= p(\bm{n};\bm{\omega})\max_{\pi\in\Pi_{Y}}p(\pi;\bm{s})\sum_{\pi^ {\prime}\in\Pi_{Y}}1\] \[= |\Pi_{Y}|\cdot p(\bm{n};\bm{\omega})\max_{\pi\in\Pi_{Y}}p(\pi;\bm {s})\] \[\leq |\Pi_{Y}|\cdot p(\bm{n};\bm{\omega})\max_{\pi}p(\pi;\bm{s})\]

We can compute the maximum probability \(\max_{\pi}p(\pi;\bm{s})\) with the probability of the permutation matrix \(f_{\pi}(\bm{s})\), which sorts the unperturbed scores in decreasing order. 

### The Differentiable Random Partition Model

We propose the DRPM \(p(Y;\bm{\omega},\bm{s})\), a differentiable and reparameterizable two-stage RPM.

**Lemma 4.3** (DRPM).: A two-stage RPM is differentiable and reparameterizable if the distribution over subset sizes \(p(\bm{n};\bm{\omega})\) and the distribution over orderings \(p(\pi;\bm{s})\) are differentiable and reparameterizable.

Proof.: To prove that our two-stage RPM is differentiable we need to prove that we can compute gradients for the bounds in Lemma 4.2 and to provide a reparameterization scheme for the two-stage approach in Definition B.2.

**Gradients for the bounds:** Since we assume that \(p(\bm{n};\bm{\omega})\) and \(p(\pi;\bm{s})\) are differentiable and reparameterizable, we only need to show that we can compute \(|\Pi_{Y}|\) and \(\max_{\tilde{\pi}}p(\tilde{\pi};\bm{s})\) in a differentiable manner to prove that the bounds in Lemma 4.2 are differentiable. By definition (see Section 4.1),

\[|\Pi_{Y}|=\prod_{k=1}^{K}|\Pi_{\bm{y}_{k}}|=\prod_{k=1}^{K}n_{k}!.\]

Hence, \(|\Pi_{Y}|\) can be computed given a reparametrized version \(n_{k}\), which is provided by the reparametrization trick for the MVHG \(p(\bm{n};\bm{\omega})\). Further, from Equation (14), we immediately see that the most probable permutation is given by the order induced by sorting the original, unperturbed scores \(\bm{s}\) from highest to lowest. This implies that \(\max_{\tilde{\pi}}p(\tilde{\pi};\bm{s})=p(\pi_{\bm{s}};\bm{s})\), which we can compute due to \(p(\pi_{\bm{s}};\bm{s})\) being differentiable according to our assumptions.

**Reparametrization of the two-stage approach:** Given reparametrized versions of \(\bm{n}\) and \(\pi\), we compute a partition as follows:

\[\bm{y}_{k}=\sum_{i=\nu_{k}+1}^{\nu_{k}+n_{k}}\bm{\pi}_{i},\quad\text{ where }\ \nu_{k}=\sum_{i=1}^{k-1}n_{\iota}\] (41)

The challenge here is that we need to be able to backpropagate through \(n_{k}\), which appears as an index in the sum. Let \(\bm{\alpha}_{k}=\{0,1\}^{n}\), such that

\[(\bm{\alpha}_{k})_{i}=\begin{cases}1&\text{if }\nu_{k}<i\leq\nu_{k+1}\\ 0&\text{otherwise}\end{cases}\]

Given such \(\bm{\alpha}_{k}\), we can rewrite Equation (41) with

\[\bm{y}_{k}=\sum_{i=1}^{n}(\bm{\alpha}_{k})_{i}\bm{\pi}_{i}.\] (42)

While this solves the problem of propagating through sum indices, it is not clear how to compute \(\bm{\alpha}_{k}\) in a differentiable manner. Similar to other works on continuous relaxations (Jang et al., 2016; Maddison et al., 2017), we can compute a relaxation of \(\bm{\alpha}_{k}\) by introducing a temperature \(\tau\). Let us introduce auxiliary function \(f:\mathbb{N}\to[0,1]^{n}\), that maps an integer \(x\) to a vector with entries

\[f_{i}(x;\tau)=\sigma\left(\frac{x-i+\epsilon}{\tau}\right),\]

such that \(f_{i}(x;\tau)\approx 0\) if \(\frac{x-i}{\tau}<0\) and \(f_{i}(x;\tau)\approx 1\) if \(\frac{x-i}{\tau}\geq 0\). Note that \(\sigma(\cdot)\) is the standard sigmoid function and \(\epsilon<<1\) is a small positive constant to break the tie at \(\sigma(0)\). We then compute an approximation of \(\bm{\alpha}_{k}\) with

\[\tilde{\bm{\alpha}}_{k}(\tau)=f(\nu_{k};\tau)-f(\nu_{k-1};\tau),\]

\(\tilde{\bm{\alpha}}_{k}(\tau)\in[0,1]^{n}\). Then, for \(\tau\to 0\) we have \(\tilde{\bm{\alpha}}_{k}(\tau)\to\bm{\alpha}_{k}\). In practice, we cannot set \(\tau=0\) since this would amount to a division by \(0\). Instead, we can apply the straight-through estimator (Bengio et al., 2013) to the auxiliary function \(f(x;\tau)\) in order to get \(\tilde{\bm{\alpha}}_{k}\in\{0,1\}^{n}\) and use it to compute Equation (42). 

Note that in our experiments, we use the MVHG relaxation of Sutter et al. (2023) and can thus leverage that they return one-hot encodings for \(n_{k}\). This allows a different path for computing \(\bm{\alpha}_{k}\) which circumvents introducing yet another temperature parameter altogether. We refer to our code in the supplement for more details.

## Appendix C Experiments

In the following, we describe each of our experiments in more detail and provide additional ablations. All our experiments were run on RTX2080Ti GPUs. Each run took 6h-8h (Variational Clustering), \(4\)h-6h (Generative Factor Partitioning), or \(\sim 1\)h (Multitask Learning) respectively. We report the training and test time per model. Please note that we can only report the numbers to generate the final results but not the development time.

Code ReleaseThe official code can be found under https://github.com/thomassutter/drpm. Please note that the results reported in the main text slightly differ from the ones being generated from the official code. For the main paper, we based our own code for the experiments in Section 5.2 on the disentanglement_lib(Locatello et al., 2020). However, the library is based on Tensorflow v1(Abadi et al., 2016), which makes it more and more difficult to maintain and install. Therefore, we decided to re-implement everything in PyTorch(Paszke et al., 2019).

While the metrics of our method and the baselines slightly change, the relative performance between them remains the same.

The code and results for the remaining two experiments in Sections 5.1 and 5.3 are the same as in the main text.

### Approximation quality of Lemma 4.2

To provide intuitive understanding of the bounds introduced in Lemma 4.2, we present an experiment in this subsection to demonstrate the behavior of the upper and lower bounds. It is important to note that RPMs are discrete distributions, as the number of possible samples is finite for a given number of elements \(n\) and subsets \(K\). Therefore, we can estimate the probability of a fixed partition \(\tilde{Y}\) under given \(\bm{\omega}\) and \(\bm{s}\) by sampling \(M\) partitions from \(p(Y;\bm{\omega},\bm{s})\), counting the occurrences of \(\tilde{Y}\) in the samples, and dividing the count by \(M\). As \(M\) approaches infinity, we can obtain the true probability mass function (PMF) \(p(Y;\bm{\omega},\bm{s})\) for every partition \(Y\).

In our experiment, we set \(n=5\) and aim to evaluate the quality of our bounds for all possible subset combinations of \(5\) elements, we thus set \(K=n=5\). To obtain a reliable estimate of the true PMF, we set \(M=10^{8}\). From Lemma 4.2, we know that:

\[\forall\pi\in\Pi_{Y}:\ p(\bm{n};\bm{\omega})p(\pi;\bm{s})\ \leq\ p(Y;\bm{\omega},\bm{s})\ \leq\ |\Pi_{Y}|p(\bm{n};\bm{\omega})\max_{\tilde{\pi}}p( \tilde{\pi};\bm{s})\]

Let us define

\[p_{U}(Y;\bm{\omega},\bm{s}) \coloneqq|\Pi_{Y}|p(\bm{n};\bm{\omega})\max_{\tilde{\pi}}p( \tilde{\pi};\bm{s})\] \[p_{L}(Y;\bm{\omega},\bm{s}) \coloneqq\max_{\pi\in\Pi_{Y}}p(\bm{n};\bm{\omega})p(\pi;\bm{s})\]

In Figure 5, we present the estimated PMF along with the corresponding upper bounds (\(p_{U}\)) and lower bounds (\(p_{L}\)) for four different combinations of RPM parameters \(\bm{\omega}\) and \(\bm{s}\). We observe that when all scores \(\bm{s}\) are equal, as in the priors of the experiments in Sections 5.1 and 5.2, \(p_{U}(Y;\bm{\omega},\bm{s})\) approximates \(p(Y;\bm{\omega},\bm{s})\) well and serves as a reliable estimate of the PMF. However, when the scores vary, the upper bound becomes looser, particularly for lower probability partitions, as it is dominated by the term \(\max_{\tilde{\pi}}p(\tilde{\pi};\bm{s})\). Although \(p_{L}(Y;\bm{\omega},\bm{s})\) appears looser than \(p_{U}(Y;\bm{\omega},\bm{s})\) for certain configurations of \(\bm{\omega}\) and \(\bm{s}\), it provides more consistent results across all hyperparameter combinations.

\begin{table}
\begin{tabular}{l c} \hline \hline Experiment & Computation Time (h) \\ \hline Clustering (Section 5.1) & 100 \\ Partitioning of Generative Factors (Section 5.2) & 480 \\ MTL (Section 5.3) & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Total GPU hours per experiment. We report the cumulative training and testing hours to generate the results shown in the main part of this manuscript. We relied on our internal cluster infrastructure equipped with RTX2080Ti GPUs. Hence, we report the number of compute hours for this GPU-type.

### Variational Clustering with Random Partition Models

#### c.2.1 Loss Function

As mentioned in Section 5.1, for a given dataset \(X\) with \(N\) samples, let \(Z\) and \(Y\) contain the respective latent vectors and cluster assignments for each sample in \(X\). The generative process can then be summarized as follows: First, we sample the cluster assignments \(Y\) from an RPM, i.e., \(Y\sim P(Y;\bm{\omega},\bm{s})\). Given \(Y\), we can sample the latent variables \(Z\), where for each \(\bm{y}\) we have \(\bm{z}\sim\mathcal{N}(\bm{\mu}_{\bm{y}},\bm{\sigma}_{\bm{y}}^{T}\mathbb{I}_{ \mathbb{I}})\), \(\bm{z}\in\mathbb{R}^{l}\). Finally, we sample \(X\) by passing each \(\bm{z}\) through a decoder like in vanilla VAEs. Using Bayes rule and Jensen's inequality, we can then derive the following evidence lower bound (ELBO):

\[\log(p(X)) =\log\left(\int\sum_{Y}p(X,Y,Z)dZ\right)\] \[\geq\mathbb{E}_{q(Z,Y|X)}\left[\log\left(\frac{p(X|Z)p(Z|Y)p(Y)}{ q(Z,Y|X)}\right)\right]\] \[:=\mathcal{L}_{ELBO}(X)\]

We then assume that we can factorize the approximate posterior as follows:

\[q(Z,Y|X)=q(Y|X)\prod_{\bm{x}\in X}q(\bm{z}|\bm{x})\]

Note that while we do assume conditional independence between \(\bm{z}\) given its corresponding \(\bm{x}\), we model \(q(Y|X)\) with the DRPM and do not have to assume conditional independence between

Figure 5: Partitions with \(n=5\) and \(K=5\) for different \(\bm{\omega}\) and \(\bm{s}\). Each point in the plots corresponds to one of the \(n^{K}\) different partitions and their respective estimated probability mass and its upper/lower bounds according to Lemma 4.2.

different cluster assignments. This allows us to leverage dependencies between samples from the dataset. Hence, we can rewrite the ELBO as follows:

\[\mathcal{L}_{ELBO}(X)= \mathbb{E}_{q(Z|X)}\left[\log(p(X|Z))\right]\] \[-\mathbb{E}_{q(Y|X)}\left[KL[q(Z|X)||p(Z|Y)]\right]\] \[-KL[q(Y|X)||p(Y)]\] \[= \sum_{\bm{x}\in X}\mathbb{E}_{q(\bm{z}|\bm{x})}\left[\log p(\bm{x }|\bm{z})\right]\] \[-\sum_{\bm{x}\in X}\mathbb{E}_{q(Y|X)}\left[KL[q(\bm{z}|\bm{x})|| p(\bm{z}|Y)]\right]\] \[-KL[q(Y|X)||p(Y)]\]

See Figure 6 for an illustration of the generative process and the assumed inference model. Since computing \(P(Y)\) and \(q(Y|X)\) is intractable, we further apply Lemma 4.2 to approximate the KL-Divergence term in \(\mathcal{L}_{ELBO}\), leading to the following lower bound:

\[\mathcal{L}_{ELBO}\geq \sum_{\bm{x}\in X}\mathbb{E}_{q(\bm{z}|\bm{x})}\left[\log p(\bm{x }|\bm{z})\right]\] (43) \[-\sum_{\bm{x}\in X}\mathbb{E}_{q(Y|X)}\left[KL[q(\bm{z}|\bm{x})|| p(\bm{z}|Y)]\right]\] (44) \[-\mathbb{E}_{q(Y|X)}\left[\log\frac{||\Pi_{Y}|\cdot q(\bm{n}; \bm{\omega}(X))}{p(\bm{n};\bm{\omega})p(\pi_{Y};\bm{s})}\right]\] (45) \[-\log\left(\max_{\tilde{\pi}}q(\tilde{\pi};\bm{s}(X))\right),\] (46)

where \(\pi_{Y}\) is the permutation that lead to \(Y\) during the two-stage resampling process. Further, we want to control the regularization strength of the KL divergences similar to the \(\beta\)-VAE (Higgins et al., 2016). Since the different terms have different regularizing effects, we rewrite Equations (45)

Figure 6: Generative model of the DRPM clustering model. Generative paths are marked with thin arrows, whereas inference is in bold.

and (46) and weight the individual terms as follows, leading to our final loss:

\[\mathcal{L}\coloneqq -\sum_{\bm{x}\in X}\mathbb{E}_{q(\bm{z}|\bm{x})}\left[\log p(\bm{x} |\bm{z})\right]\] (47) \[+\beta\cdot\sum_{\bm{x}\in X}\mathbb{E}_{q(Y|X)}\left[KL[q(\bm{z} |\bm{x})||p(\bm{z}|Y)]\right]\] (48) \[+\gamma\cdot\mathbb{E}_{q(Y|X)}\left[\log\left(\frac{|\Pi_{Y}| \cdot q(\bm{n};\bm{\omega}(X))}{p(\bm{n};\bm{\omega})}\right)\right]\] (49) \[+\delta\cdot\mathbb{E}_{q(Y|X)}\left[\log\left(\frac{\max_{\hat{ \bm{x}}}q(\hat{\bm{\pi}};\bm{s}(X))}{p(\pi_{Y};\bm{s})}\right)\right]\] (50)

#### c.2.2 Architecture

The model for our clustering experiments is a relatively simple, fully-connected autoencoder with a structure as seen in Figure 7. We have a fully connected encoder \(E\) with three layers mapping the input to \(500\), \(500\), and \(2000\) neurons, respectively. We then compute each parameter by passing the encoder output through a linear layer and mapping to the respective parameter dimension in the last layer. In our experiments, we use a latent dimension size of \(l=10\) for MNIST and \(l=20\) for FMNIST, such that \(\bm{\mu}(\bm{x}),\bm{\sigma}(\bm{x})\in\mathbb{R}^{l}\). To understand the architecture choice for the DRPM parameters, let us first take a closer look at Equation (48). For each sample \(\bm{x}\), this term minimizes the expected KL divergence between its approximate posterior \(q(\bm{z}|\bm{x})=\mathcal{N}(\bm{\mu}(\bm{x}),\mathrm{diag}(\bm{\sigma}(\bm{x })))\) and the prior at index \(\bm{y}\) given by the partition \(Y\) sampled from the DRPM \(q(Y|X;\bm{s},\bm{\omega})\), i.e., \(\mathcal{N}(\bm{\mu}_{\bm{y}},\mathrm{diag}(\bm{\sigma}_{\bm{y}}))\). Ideally, the most likely partition should assign the approximate posterior to the prior that minimizes this KL divergence. We can compute such \(\bm{s}(X)\) and \(\bm{\omega}(X)\) given the parameters of the approximate posterior and priors as follows:

\[\forall\bm{x}_{i}\in X:s_{i}(\bm{x}_{i}) =u\cdot(K-\operatorname*{arg\,min}_{k}\left(KL[\mathcal{N}(\bm{ \mu}(\bm{x}_{i}),\mathrm{diag}(\bm{\sigma}(\bm{x}_{i}))||\mathcal{N}(\bm{\mu} _{k},\mathrm{diag}(\bm{\sigma}_{k}))]\right))\] \[\bm{\omega}(X) =\frac{1}{|X|}\sum_{\bm{x}\in X}^{N}\left\{\frac{\mathcal{N}(\bm {x}|\bm{\mu}_{k},\mathrm{diag}(\bm{\sigma}_{k}))}{\sum_{k^{\prime}=1}^{K} \mathcal{N}(\bm{x}|\bm{\mu}_{k^{\prime}},\mathrm{diag}(\bm{\sigma}_{k^{\prime }}))}\right\}_{k=1}^{K},\]

where \(u\) is a scaling constant that controls the probability of sampling the most likely partition. Note that \(\bm{\omega}\) and \(\bm{s}\) minimize Equation (48) if defined this way when given the distribution parameters of the approximate posterior and the priors. The only thing that is left unclear is how much \(u\) should scale the scores \(\bm{s}\). Ultimately, we leave \(u\) as a learnable parameter but detach the rest of the computation of \(\bm{s}\) and \(\bm{\omega}\) from the computational graph to improve stability during training. Finally, once we resample \(z\sim\mathcal{N}(\mu(\bm{x}),\sigma(\bm{x}))\), we pass it through a fully connected decoder \(D\) with four layers mapping \(z\) to \(2000\), \(500\), and \(500\) neurons in the first three layers and then finally back to the input dimension in the last layer to end up with the reconstructed sample \(\bm{\hat{x}}\).

Figure 7: Autoencoder architecture of the DRPM-VC model.

#### c.2.3 Training

As in vanilla VAEs, we can estimate the reconstruction term in Equation (47) with MCMC by applying the reparametrization trick (Kingma and Welling, 2014) to \(q(\bm{z}|\bm{x})\) to sample \(M\) samples \(\bm{z}^{(i)}\sim q(\bm{z}|\bm{x})\) and compute their reconstruction error to estimate Equation (47). Similarly, we can sample from \(q(Y|X)\)\(L\) times to estimate the terms in Equations (48) to (50), such that we minimize

\[\tilde{\mathcal{L}}\coloneqq -\sum_{\bm{x}\in X}\frac{1}{M}\sum_{i=1}^{M}\log p(\bm{x}|\bm{z}^ {(i)})\] \[+\frac{\beta}{L}\cdot\sum_{\bm{x}\in X}\sum_{i=1}^{L}KL[q(\bm{z}| \bm{x})||p(\bm{z}|Y^{(i)})]\] \[+\frac{\gamma}{L}\cdot\sum_{i=1}^{L}\log\left(\frac{|\Pi_{Y^{(i)} }|\cdot q(\bm{n}^{(i)};\bm{\omega}(X))}{p(\bm{n}^{(i)};\bm{\omega})}\right)\] \[+\frac{\delta}{L}\cdot\sum_{i=1}^{L}\log\left(\frac{\max_{\tilde{ \pi}}q(\tilde{\pi};\bm{s}(X))}{p(\pi_{Y^{(i)}};\bm{s})}\right)\]

In our experiments, we set \(M=1\) and \(L=100\) since the MVHG and PL distributions are not concentrated around their mean very well, and more Monte Carlo samples thus lead to better approximations of the expectation terms. We further set \(\beta=1\) for MNIST and \(\beta=0.1\) for FMNIST, and otherwise \(\gamma=1\), and \(\delta=0.01\) for all experiments.

To resample \(\bm{n}\) and \(\pi\) we need to apply temperature annealing (Grover et al., 2019; Sutter et al., 2023). To do this, we applied the exponential schedule that was originally proposed together with the Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2017), i.e., \(\tau=\max(\tau_{final},exp(-rt))\), where \(t\) is the current training step and \(r\) is the annealing rate. For our experiments, we choose \(r=\frac{\log(\tau_{final})-\log(\tau_{init})}{100000}\) in order to annealing over \(100000\) training step. Like Jang et al. (2016), we set \(\tau_{init}=1\) and \(\tau_{final}=0.5\).

Similar to Jiang et al. (2016), we quickly realized that proper initialization of the cluster parameters and network weights is crucial for variational clustering. In our experiments, we pretrained the autoencoder structure by adapting the contrastive loss of (Li et al., 2022), as they demonstrated that their representations manage to retain clusters in low-dimensional space. Further, we also added a reconstruction loss to initialize the decoder properly. To initialize the prior parameters, we fit a GMM to the pretrained embeddings of the training set and took the resulting Gaussian parameters to initialize our priors. Note that we used the same initialization across all baselines. See Appendix C.2.4 for an ablation where we pretrain with only a reconstruction loss similar to what was proposed with the VaDE baseline.

To optimize the DRPM-VC in our experiments, we used the AdamW (Loshchilov and Hutter, 2019) optimizer with a learning rate of \(0.0001\) with a batch size of \(256\) for \(1024\) epochs. During initial experiments with the DRPM-VC, we realized that the pretrained weights of the encoder would often lose the learned structure in the first couple of training epochs. We suspect this to be an artifact of instabilities induced by temperature annealing. To deal with these problems, we decided to freeze the first three layers of the encoder when training the DRPM-VC, giving us much better results. See Appendix C.2.5 for an ablation where we applied the same optimization procedure to VaDE.

Finally, when training the VaDE baseline and the DRPM-VC on FMNIST, we often observe a local optimum where the prior distributions collapse and become identical. We can solve this problem by refitting the GMM in the latent space every \(10\) epochs and by using the resulting parameters to reinitialize the prior distributions.

#### c.2.4 Reconstruction Pretraining

While the results of our variational clustering method depend a lot on the specific pretraining, we want to demonstrate that improvements over the baselines do not depend on the chosen pretraining method. To that end, we repeat our experiments but initialize the weights of our model with an autoencoder that has been trained to minimize the mean squared error between the input and the reconstruction. This initialization procedure was originally proposed in (Jiang et al., 2016). Wepresent the results of this ablation in Table 4. Simply minimizing the reconstruction error does not necessarily retain cluster structures in the latent space. Thus, it does not come as a surprise that overall results get about \(10\%\) to \(20\%\) worse across most metrics, especially for MNIST, while results on FMNIST only slightly decrease. However, we still beat the baselines across most metrics, suggesting that modeling the implicit dependencies between cluster assignments helps to improve variational clustering performance.

#### c.2.5 Baselines with fixed Encoder

For the experiments in the main text, we wanted to implement the VaDE baseline similar to the original method proposed in Jiang et al. (2016). This means, in contrast to our method, we used their optimization procedure, i.e., Adam with a learning rate of \(0.002\) with a decay of \(0.95\) every \(10\) steps, and did not freeze the encoder as we do for the DRPM-VC. To ensure our results do not stem from this minor discrepancy, we perform an ablation experiment on VaDE using the same optimizer and learning rate as with the DRPM-VC and freeze the encoder backbone. The results of this additional experiment can be found in Appendix C.2.5. As can be seen, VaDE results do improve when adjusting the optimization procedure in this way. However, we still match or improve upon the results of VaDE in most metrics, especially in ARI and ACC, suggesting purer clusters compared to VaDE. We suspect this is because we assign samples to fixed clusters when sampling from the DRPM, whereas VaDE performs soft assignments by marginalizing over a categorical distribution.

#### c.2.6 Additional Partition Samples

In Section 5.1, we have seen a sample of a partition of the DRPM-VC trained on FMNIST. We provide additional samples for both MNIST and FMNIST at the end of the appendix in Figures 15 and 16. We can see that for both datasets, the DRPM-VC learns coherent representations of each cluster that easily allow us to generate new samples from each class.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{MNIST} & \multicolumn{3}{c}{FMNIST} \\ \cline{2-7}  & NMI & ARI & ACC & NMI & ARI & ACC \\ \hline Latent GMM & \(0.75\pm 0.00\) & \(0.66\pm 0.01\) & \(\mathbf{0.75\pm 0.01}\) & \(0.56\pm 0.02\) & \(0.41\pm 0.03\) & \(0.57\pm 0.02\) \\ VaDE & \(\mathbf{0.77\pm 0.02}\) & \(0.62\pm 0.04\) & \(0.69\pm 0.04\) & \(0.53\pm 0.07\) & \(0.35\pm 0.08\) & \(0.47\pm 0.09\) \\ DRPM-VC & \(0.74\pm 0.00\) & \(\mathbf{0.67\pm 0.01}\) & \(\mathbf{0.75\pm 0.02}\) & \(\mathbf{0.59\pm 0.01}\) & \(\mathbf{0.47\pm 0.02}\) & \(\mathbf{0.62\pm 0.01}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: We compare the clustering performance of the DRPM-VC on test sets of MNIST and FMNIST between GMM in latent space (Latent GMM) and Variational Deep Embedding (VaDE) initializing weights using an autoencoder trained on a reconstruction objective. We measure performance in terms of the Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and cluster accuracy (ACC) over five seeds and put the best model in bold.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{MNIST} & \multicolumn{3}{c}{FMNIST} \\ \cline{2-7}  & NMI & ARI & ACC & NMI & ARI & ACC \\ \hline Latent GMM & \(0.86\pm 0.02\) & \(0.83\pm 0.06\) & \(0.88\pm 0.07\) & \(0.60\pm 0.00\) & \(0.47\pm 0.01\) & \(0.62\pm 0.01\) \\ VaDE & \(\mathbf{0.90\pm 0.02}\) & \(\mathbf{0.88\pm 0.06}\) & \(0.92\pm 0.06\) & \(\mathbf{0.64\pm 0.01}\) & \(0.47\pm 0.01\) & \(0.59\pm 0.03\) \\ DRPM-VC & \(0.89\pm 0.01\) & \(\mathbf{0.88\pm 0.03}\) & \(\mathbf{0.94\pm 0.02}\) & \(\mathbf{0.64\pm 0.00}\) & \(\mathbf{0.51\pm 0.01}\) & \(\mathbf{0.65\pm 0.00}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: We compare the clustering performance of the DRPM-VC on test sets of MNIST and FMNIST between GMM in latent space (Latent GMM), and Variational Deep Embedding (VaDE) when freezing the encoder. We measure performance in terms of the Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and cluster accuracy (ACC) over five seeds and put the best model in bold.

#### c.2.7 Samples per cluster

In addition to sampling partitions and then generating samples according to the sampled cluster assignments, we can also directly sample from each of the learned priors. We show some examples of this for both MNIST and FMNIST at the end of the appendix in Figures 17 and 18. We can again see that the DRPM-VC learns accurate cluster representations since each of the samples seems to correspond to one of the classes in the datasets. Further, the clusters also seem to capture the diversity in each cluster, as we see a lot of variety across the generated samples.

#### c.2.8 Varying the number of Clusters

In previous experiments, we assumed that we had access to the true number of clusters of the dataset, which is, of course, not true in practice. We thus also want to investigate the behavior of the DRPM-VC when varying the number of partitions \(K\) of the DRPM and compare it to our baselines. In Figure 8, we show the performance of Latent GMM, VaDE, and DRPM-VC for \(K\in\{6,8,10,12,14,16,18,20\}\) across \(5\) different seeds on FMNIST. DRPM-VC clearly outperforms the two baselines for all \(K\), except for the extreme case of \(K=20\), where all models seem to perform similarly. Expectedly, DRPM-VC performs well when \(K\) is close to the true number of clusters, but performance decreases the farther we are from it. To investigate whether the model still learns meaningful patterns when we are far from the true number of clusters, we additionally generate samples from each prior of the DRPM-VC in Figure 9. Interestingly, we can see that DRPM-VC still detects certain structures in the dataset but starts breaking specific FMNIST categories apart. For instance, it splits the clusters sandals/boots into clusters with (Priors \(6/15\)) and without (Priors \(4/9\)) heels or the cluster T-shirt into clothing of lighter (Prior \(0\)) and darker (Prior \(3\)) color. Thus, DRPM-VC allows us to investigate clusters in datasets hierarchically, where low values of \(K\) detect more coarse and higher values of \(K\) more fine-grained patterns in the data.

#### c.2.9 Clustering of STL-10

We include an additional variational clustering ablation on the STL-\(10\) dataset (Coates et al., 2011) that follows the experimental setup of Jiang et al. (VaDE, 2016). As noted by Jiang et al. (2016),

Figure 8: Clustering performance of Latent GMM, VaDE, and DRPM-VC when varying the number of clusters \(K\) for \(5\) seeds on Fashion MNIST. DRPM-VC consistently outperforms the baselines except for \(K=20\), where all methods perform similarly.

variational clustering algorithms have difficulties clustering in raw pixel space for natural images, which is why we apply DRPM-VC to representations extracted using an Imagenet (Deng et al., 2009) pretrained ResNet-\(50\)(He et al., 2015) as done in Jiang et al. (VaDE, 2016). Note that these results are hard to interpret, as STL-\(10\) is a subset of Imagenet, meaning that representations are relatively easy to cluster as the pretraining task already encourages separation by label. For this reason, we list this experiment separately in the appendix instead of including it in the main text. We present the results of this ablation in Table 6, where we again confirm that modeling cluster assignments with our DRPM can improve upon previous work that modeled the assignments independently.

### Variational Partitioning of Generative Factors

We assume that we have access to multiple instances or views of the same event, where only a subset of generative factors changes between views. The knowledge about the data collection process provides a form of weak supervision. For example, we have two images of a robot arm as depicted here on the left side (see (Gondal et al., 2019)), which we would describe using high-level concepts such as color, position or rotation degree. From the data collection process, we know that a subset of these generative factors is shared between the two views We do not know how many generative factors there are in total nor how many of them are shared. More precisely, looking at the robot arm, we do not know that the views share two latent factors, depicted in red, out of a total of four factors. Please note that we chose four generative in Figure 10 only for illustrative reason as there are seven generative factors in the _mpi3d_ toy dataset. Hence, the goal of learning under weak supervision is not

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{STL-10} \\ \cline{2-4}  & NMI & ARI & ACC \\ \hline VADE & \(0.80\pm 0.03\) & \(0.76\pm 0.04\) & \(0.88\pm 0.02\) \\ DRPM-VC & \(\mathbf{0.83}\pm 0.01\) & \(\mathbf{0.80}\pm 0.02\) & \(\mathbf{0.91}\pm 0.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: We compare the clustering performance of the DRPM-VC and VaDE on the test set of STL-10 across five seeds. We measure performance in terms of the Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and cluster accuracy (ACC) over five seeds and put the best model in bold.

Figure 9: We generate \(32\) images from each prior when training DRPM-VC on Fashion MNIST with \(K=20\). DRPM-VC starts detecting more fine-grained patterns in the data and splits some of the original clusters, such as sandals, boots, t-shirts, or handbags, into more specific sub-categories.

only to infer good representations, but also inferring the number of shared and independent generative factors. Learning what is shared and what is independent lets us reason about the group structure without requiring explicit knowledge in the form of expensive labeling. Additionally, leveraging weak supervision and, hence, the underlying group structure holds promise for learning more generalizable and disentangled representations (see (e.g., Locatello et al., 2020)).

#### c.3.1 Generative Model

We assume the following generative model for DRPM-VAE

\[p(\bm{X})= \int_{\bm{z}}p(\bm{X},\bm{z})d\bm{z}\] (51) \[= \int_{\bm{z}}p(\bm{X}\ |\ \bm{z})p(\bm{z})d\bm{z}\] (52)

where \(\bm{z}=\{\bm{z}_{s},\bm{z}_{1},\bm{z}_{2}\}\). The two frames share an unknown number \(n_{s}\) of generative latent factors \(\bm{z}_{s}\), and an unknown number, \(n_{1}\) and \(n_{2}\), of independent factors \(\bm{z}_{1}\) and \(\bm{z}_{2}\). The RPM infers \(n_{k}\) and \(\bm{z}_{k}\) using \(Y\). Hence, the generative model extends to

\[p(\bm{X})= \int_{\bm{z}}p(\bm{X}\ |\ \bm{z})\sum_{Y}p(\bm{z}\ |\ Y)p(Y)d\bm{z}\] \[= \int_{\bm{z}}p(\bm{x}_{1},\bm{x}_{2}\ |\ \bm{z}_{s},\bm{z}_{1},\bm{z}_{2}) \sum_{Y}p(\bm{z}\ |\ Y)p(Y)d\bm{z}\] \[= \int_{\bm{z}_{s},\bm{z}_{1},\bm{z}_{2}}p(\bm{x}_{1}\ |\ \bm{z}_{s},\bm{z}_{1})p(\bm{x}_{2}\ |\ \bm{z}_{s},\bm{z}_{2}) \sum_{Y}p(\bm{z}_{s},\bm{z}_{1},\bm{z}_{2}\ |\ Y)p(Y)d\bm{z}_{s}d\bm{z}_{1}d\bm{z}_{2}\] (53)

Figure 11 shows the generative and inference models assumptions in a graphical model.

Figure 10: Motivation for the Partitioning of Generative Factors under weak supervision. The knowledge about the data collection process provides a weak supervision signal. We observe access to a dataset of pairs of images of the same robot arm with a subset of shared generative factors (in red). We want to learn the shared and independent generative factors in addition to learning from the data. The images of the robot arms are taken from Locatello et al. (2020) but originate from the mpi3d toy dataset (see https://github.com/rr-learning/disentanglement_dataset). The image is from Sutter et al. (2023) and their ICLR 2023 presentation video (see https://iclr.cc/virtual/2023/poster/10707).

#### c.3.2 Drpm Elbo

We derive the following ELBO using the posterior approximation \(q(\bm{z},Y\mid\bm{X})\)

\[\mathcal{L}_{ELBO}(\bm{X}) =\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{X}\mid\bm{z},Y)- \log\frac{q(\bm{z},Y\mid\bm{X})}{p(\bm{z},Y)}\right]\] (54) \[=\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{X}\mid\bm{z})- \log\frac{q(\bm{z}\mid Y,\bm{X})q(Y\mid\bm{X})}{p(\bm{z})p(Y)}\right]\] (55) \[=\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{x}_{1},\bm{x}_{2 }\mid\bm{z})-\log\frac{q(\bm{z}\mid Y,\bm{X})}{p(\bm{z})}-\log\frac{q(Y\mid \bm{X})}{p(Y)}\right]\] (56) \[=\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{x}_{1}\mid\bm{z }_{s},\bm{z}_{1})\right]-\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{x}_{ 2}\mid\bm{z}_{s},\bm{z}_{2})\right]\] \[\quad-\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log\frac{q(\bm{z}_{s },\bm{z}_{1},\bm{z}_{2}\mid Y,\bm{X})}{p(\bm{z}_{s},\bm{z}_{1},\bm{z}_{2})} \right]-\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log\frac{q(Y\mid\bm{X})}{p(Y)}\right]\] (57)

Following Lemma 4.2, we are able to optimize DRPM-VAE using the following ELBO \(\mathcal{L}_{ELBO}(\bm{X})\):

\[\mathcal{L}_{ELBO} \geq\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{x}_{1}\mid \bm{z}_{s},\bm{z}_{1})\right]-\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{ x}_{2}\mid\bm{z}_{s},\bm{z}_{2})\right]\] (58) \[\quad-\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log\frac{q(\bm{z}_{s },\bm{z}_{1},\bm{z}_{2}\mid Y,\bm{X})}{p(\bm{z}_{s},\bm{z}_{1},\bm{z}_{2})}\right]\] (59) \[\quad-\mathbb{E}_{q(Y|\bm{X})}\left[\log\left(\frac{\left|\Pi_{Y }\right|\cdot q(\bm{n}\mid\bm{X};\bm{\omega})}{p(\bm{n};\bm{\omega}_{p})p(\pi _{Y};\bm{s}_{p})}\right)\right]\] (60) \[\quad-\log\left(\max_{\tilde{\pi}}q(\tilde{\pi}\mid\bm{X};\bm{s}) \right),\] (61)

where \(\pi_{Y}\) is the permutation that lead to \(Y\) during the two-stage resampling process. Further, we want to control the regularization strength of the KL divergences similar to the \(\beta\)-VAE (Higgins et al., 2016). The ELBO \(\mathcal{L}(\bm{X})\) to be optimized can be written as

\[\mathcal{L}_{ELBO} =\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{x}_{1}\mid\bm{z }_{s},\bm{z}_{1})\right]+\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log p(\bm{x}_{2 }\mid\bm{z}_{s},\bm{z}_{2})\right]\] (62) \[\quad-\beta\cdot\mathbb{E}_{q(\bm{z},Y|\bm{X})}\left[\log\frac{q( \bm{z}_{s},\bm{z}_{1},\bm{z}_{2}\mid Y,\bm{X})}{p(\bm{z}_{s},\bm{z}_{1},\bm{z} _{2})}\right]\] (63) \[\quad-\gamma\cdot\mathbb{E}_{q(Y|\bm{X})}\left[\log\left(\frac{ \left|\Pi_{Y}\right|\cdot q(\bm{n};\bm{\omega}(\bm{X}))}{p(\bm{n};\bm{\omega}_ {p})}\right)\right]\] (64) \[\quad-\delta\cdot\mathbb{E}_{q(Y|\bm{X})}\left[\log\left(\frac{ \max_{\tilde{\pi}}q(\tilde{\pi};\bm{s}(\bm{X}))}{p(\pi_{Y};\bm{s}_{p})}\right)\right]\] (65)

where \(\bm{s}(\bm{X})\) and \(\bm{\omega}(\bm{X})\) denote distribution parameters, which are inferred from \(\bm{X}\) (similar to the Gaussian parameters in the vanilla VAE).

Figure 11: Graphical Models for DRPM-VAE models in the weakly-supervised experiment.

As in vanilla VAEs, we can estimate the reconstruction term in Equation (58) with MCMC by applying the reparametrization trick (Kingma and Welling, 2014) to sample \(L\) samples \(\bm{z}^{(l)}\sim q(\bm{z}\mid Y,\bm{X})\) and compute their reconstruction error to estimate Equation (58). Similarly, we can sample from \(q(Y\mid\bm{X})\)\(L\) times. We use \(L=1\) to estimate all expectations in \(\mathcal{L}_{ELBO}\).

#### c.3.3 Implementation and Hyperparameters

In this experiment, we use the disentanglement_lib from Locatello et al. (2020). We use the same architectures proposed in the original paper for all methods we compare to. The baseline algorithms, LabelVAE (Bouchacourt et al., 2018; Hosoya, 2018) and AdaVAE (Locatello et al., 2020) are already implemented in disentanglement_lib. For details on the implementation of these methods we refer to the original paper from Locatello et al. (2020). HGVAE is implemented in Sutter et al. (2023). We did not change any hyperparameters or network details. All experiments were performed using \(\beta=1\) as this is the best performing \(\beta\) (according to Locatello et al. (2020). For DRPMVAE we chose \(\gamma=0.25\) for all runs. All models are trained on 5 different random seeds and the reported results are averaged over the 5 seeds. We report mean performance with standard deviations.

We adapted Figure 12 from Sutter et al. (2023). It shows the baseline architecture, which is used for all methods. As already stated in the main part of the paper, the methods only differ in the View Aggregation module, which determines the shared and independent latent factors. Given a subset \(S\) of shared latent factors, we have

\[q_{\phi}(z_{i}\mid\bm{x}_{j}) =avg(q_{\phi}(z_{i}\mid\bm{x}_{1}),q_{\phi}(z_{i}\mid\bm{x}_{2})) \forall\;\;i\in S\] (66) \[q_{\phi}(z_{i}\mid\bm{x}_{j}) =q_{\phi}(z_{i}\mid\bm{x}_{j}) \text{ else}\] (67)

where \(avg\) is the averaging function of choice (Locatello et al., 2020; Sutter et al., 2023) and \(j\in\{1,2\}\). The methods used (i. e. Label-VAE, Ada-VAE, HG-VAE, DRPM-VAE) differ in how to select the subset S.

For DRPM-VAE, we infer \(\bm{\omega}\) from the pairwise KL-divergences \(KL_{pw}\) between the latent vectors of the two views.

\[KL_{pw}(\bm{x}_{1},\bm{x}_{2})=\frac{1}{2}KL[q(\bm{z}_{1}\mid\bm{x}_{1})||q( \bm{z}_{2}\mid\bm{x}_{2})]+\frac{1}{2}KL[q(\bm{z}_{2}\mid\bm{x}_{2})||q(\bm{ z}_{1}\mid\bm{x}_{1})]\] (68)

where \(q(\bm{z}_{j}\mid\bm{x}_{j})\) are the encoder outputs of the respective images. We do not average or sum across dimensions in the computation of \(KL_{pw}(\cdot)\) such that the \(KL_{pw}(\cdot)\) is \(d\)-dimensional, where \(d\) is the latent space size. The encoder \(E\) in Figure 12 maps to \(\bm{\mu}(\bm{x}_{j})\) and \(\bm{\sigma}(\bm{x}_{j})\) of a Gaussian distribution. Hence, we can compute the KL divergences above in closed form. Afterwards, we feed the pairwise KL divergence \(KL_{pw}\) to a single fully-connected layer, which maps from \(d\) to \(K\) values

\[\log\bm{\omega}=FC(KL_{pw}(\bm{x}_{1},\bm{x}_{2}))\] (69)

where \(d=10\) and \(K=2\) in this experiment. \(d\) is the total number of latent dimensions and \(K\) is the number of groups in the latent space. To infer the scores \(\bm{s}(\bm{X})\) we again rely on the pairwise KL divergence \(KL_{pw}\). Instead of using another fully-connected layer, we directly use the log-values of the pairwise KL divergence

\[\log\bm{s}=\log KL_{pw}(\bm{x}_{1},\bm{x}_{2})\] (70)

Similar to the original works, we also anneal the temperature parameter for \(p(\bm{n};\bm{\omega})\) and \(p(\pi;\bm{s})\)(Grover et al., 2019; Sutter et al., 2023). We use the same annealing function as in the clustering experiment (see Appendix C.2). We anneal the temperature \(\tau\) from \(1.0\) to \(0.5\) over the complete training time.

Figure 12: Setup for the weakly-supervised experiment. The three methods differ only in the View Aggregation module.

### Multitask Learning

#### c.4.1 MultiMNIST Dataset

The different tasks in multitask learning often vary in difficulty. To measure the effect of discrepancies in task difficulties on DRPM-MTL, we introduce the noisyMultiMNIST dataset.

The noisyMultiMNIST dataset modifies the MultiMNIST dataset (Sabour et al., 2017) as follows. In the right image, we set each pixel value to zero with probability \(\alpha\in[0,1]\). This is done before merging the left and right image in order to only affect the difficulty of the right task. Note that for \(\alpha=0\) noisyMultiMNIST is equivalent to MultiMNIST and for \(\alpha=1\) the right task can no longer be solved. This allows us to control the difficulty of the right task, without changing the difficulty of the left. A few examples are shown in Figure 13.

#### c.4.2 Implementation & Architecture

The multitask loss function for the _MultiMNIST_ dataset is

\[\mathbb{L}=w_{L}\mathbb{L}_{L}+w_{R}\mathbb{L}_{R}\] (71)

where \(w_{L}\) and \(w_{L}\) are the loss weights, and \(\mathbb{L}_{L}\) and \(\mathbb{L}_{R}\) are the individual loss terms for the respective tasks \(L\) and \(R\). In our experiments, we set the task weights to be equal for all dataset versions, i.e. \(w_{L}=w_{R}=0.5\). We use these loss weights for the DRPM-MTL and ULS method. For the ULS method, it is by definition and to see the influence of a mismatch in loss weights. The DRPM-MTL method on the other hand does not need additional weighting of loss terms. The task losses are defined as cross-entropy losses

\[\mathbb{L}_{t}=-\sum_{c=1}^{C_{t}}\bm{gt}_{c}\log\bm{p}_{c}=-\bm{gt}^{T}\log \bm{p}\] (72)

where \(C_{L}=C_{R}=10\) for MultiMNIST, \(\bm{gt}\) is a one-hot encoded label vector and \(\bm{p}\) is a categorical vector of estimated class assignments probabilities, i.e. \(\sum_{c}\bm{p}_{c}=1\).

The predictions for the individual tasks \(\bm{p}_{t}\) are given as

\[\bm{p}_{t} =h_{\theta_{t}}(\bm{z}),\;\;\;\text{where}\] (73) \[\bm{z} =\text{enc}_{\theta}(\bm{x})\] (74)

for a sample \(\bm{x}\in\bm{X}\) (see also Figure 14). We use an adaptation of the LeNet-5 architecture LeCun et al. (1998) to the multitask learning problem (Sener and Koltun, 2018). Both DRPM-MTL and ULS use the same network \(\text{enc}_{\theta}(\cdot)\) with shared architecture up to some layer for both tasks, after which the network branches into two task-specific sub-networks that perform the classifications. Different to the ULS method, the task-specific networks in the DRPM-MTL pipeline predict the digit using only a subset of \(\bm{z}\). DRPM-MTL uses the following prediction scheme

\[\bm{p}_{t} =h_{\theta_{t}}(\bm{z}_{t}),\;\;\text{where}\] (75) \[\bm{z}_{t} =\bm{z}\odot\bm{y}_{t}\] (76) \[\bm{y}_{t} =\text{DRPM}(\bm{\omega},\bm{s})_{t}=\text{DRPM}(\text{enc}_{ \varphi}(\bm{x}))_{t}\] (77)

The DRPM-MTL encoder first predicts a latent representation \(\bm{z}\leftarrow\text{enc}_{\theta}(\bm{x})\), where \(\bm{x}\) is the input image. Using the same encoder architecture but different parameters \(\varphi\), we predict a partitioning encoding \(\bm{z}^{\prime}\leftarrow\text{enc}_{\varphi}(\bm{x})\). With a single linear layer per DRPM log-parameter \(\log\bm{\omega}\) and \(\log\bm{s}\) are computed. Next we infer the partition masks \(\bm{y}_{\text{L}},\bm{y}_{\text{R}}\sim p(\bm{y}_{\text{L}},\bm{y}_{\text{R}} ;\bm{\omega},\bm{s})\). We then feed the masked latent representations \(\bm{z}_{\text{L}}\leftarrow\bm{z}\odot\bm{y}_{\text{L}}\) and \(\bm{z}_{\text{R}}\leftarrow\bm{z}\odot\bm{y}_{\text{R}}\) into the task specific classification networks \(h_{\theta_{\text{L}}}(\bm{z}_{\text{L}})\) and \(h_{\theta_{\text{R}}}(\bm{z}_{\text{R}})\) respectively to obtain the task specific predictions. Since the two tasks in the MultiMNIST dataset are of similar nature, the task-specifc networks \(h_{\theta_{\text{L}}}\) and \(h_{\theta_{\text{R}}}\) share the same architecture, but have different parameters.

Figure 13: Samples from the noisyMultiMNIST dataset with increasing noise ratio in the right task.

#### c.4.3 Training

For both the ULS and the DRPM-MTL model, we use the Adam optimizer with learning rate \(0.0005\) and train them for 200 epochs with a batch size of 256. We again choose an exponential schedule for the temperature \(\tau\) and anneal it over the training time, as is explained in Appendix C.2.3.

In our ablation we use \(\alpha\in\{0,0.1,0.2,\ldots,0.9\}\) and train each model with five different seeds. The reported accuracies and partition sizes are then means over the five seeds with the error bands indicating the variance and standard deviation respectively. We evaluate each model after the epoch with the best average test accuracy.

#### c.4.4 CelebA for MTL

In addition to the experiment shown in Section 5.3, we show additional results for DRPM-MTL on the CelebA dataset (Liu et al., 2015). In MTL, each of the \(40\) attributes of the CelebA dataset serves as an individual task. Hence, using CelebA for MTL results is a \(40\) task learning problem making the scaling of different task losses more difficult compared to MultiMNIST (see Section 5.3) where we only need to scale two different tasks.

We again use the newly introduced DRPM-MTL method and compare it to the ULS model. We use the same pipeline as for MultiMNIST dataset but with different encoders and hyperparameters (see Appendices C.4.2 and C.4.3). We use the pipeline of Sener and Koltun (2018) with a ResNet-based encoder to map an image to a representation of \(d=64\) dimensions. For architectural details, we refer to Sener and Koltun (2018) and https://github.com/isl-org/MultiObjectiveOptimization.

Again, ULS inputs all \(d=64\) dimensions to the task-specific sub-networks whereas DRPM-MTL partitions the intermediate representations into \(n_{T}\) different subsets, which are then fed to the respective task networks. \(n_{T}\) is the number of tasks.

Compared to the MultiMNIST experiment (see Appendix C.4.2), we introduce an additional regularization for the DRPM-MTL method. The additional regularization is based on the upper bound in Lemma 4.2 and is penalizing size of \(|\Pi_{Y}|\) for a given \(\bm{n}\). Hence, the loss function changes to

\[\mathbb{L} =\frac{1}{n_{T}}\sum_{t=1}^{n_{T}}\mathbb{L}_{t}+\lambda\cdot \mathbb{L}_{\text{reg}}\] (78) \[\text{where}\ \ \mathbb{L}_{\text{reg}} =\log\left(\prod_{t=1}^{n_{T}}n_{t}!\right)=\sum_{t=1}^{n_{T}} \log\Gamma(n_{t}+1)\] (79)

For all versions of the experiment (i.e. \(n_{T}\in 10,20,40\)), we set \(\lambda=0.015\approx\frac{1}{64}\), which is the number of elements we want to partition. The task losses \(\mathbb{L}_{t}\) are simple BCE losses similar to the MultiMNIST experiments but with two classes per task only.

We perform two different experiments based on the CelebA experiment. First, we use form a MTL experiments using the first \(10\) attributes out of the \(40\) attributes. Second, we increase the number of different tasks to \(20\). Because we sort the attributes alphabetically in both cases, the first \(10\) tasks are

Figure 14: Overview of the multitask learning pipeline of the DRPM-MTL method.

shared between the two experiment versions. And third, we set \(n_{T}=40\), where the first \(20\) tasks are the shared with the previous experiment.

Table 7 shows the results of all CelebA experiments for both methods, ULS and DRPM-MTL. We see that the DRPM-MTL scales better to a larger number of tasks compared to the ULS method, highlighting the importance of finding new ways of automatic scaling between tasks. Interestingly, the DRPM-MTL outperforms the ULS method on most tasks for the \(20\)-tasks experiment even though it has only access to \(d/n_{T}=64/20=3.2\) dimensions on average. And even more extrem for \(n_{T}=40\), DRPM-MTL on average has only access to \(d/n_{T}=64/40=1.6\) dimensions. On the other hand, the ULS method can access the full set of \(64\) dimensions for every single task.

### Supervised Learning

Given the true partition \(Y\), we can also adapt the DRPM to learn partitions in a supervised fashion. One instance where we know the true partition of elements is in the case of classification. There, for a given batch \(X\) containing \(B\) samples, we have \(Y:=(\bm{y}_{1},\ldots,\bm{y}_{B})\) where \(\bm{y}_{i}\in\mathbb{R}^{K}\) is the one-hot encoding of the label of the \(i\)-th sample in the batch. In this ablation, we infer \(\hat{Y}:=DRPM(\bm{\omega}_{\theta_{1}}(X),\bm{s}_{\theta_{2}}(X))\), where we compute \(\bm{\omega}(X)\) and \(\bm{s}(X)\) as in the variational clustering experiment (Appendix C.2.2) and use the DRPM without resampling as in the multitask experiment (Section 5.3). We optimize

\begin{table}

\end{table}
Table 7: Results for the MTL experiment on the CelebA dataset. We compare the DRPM-MTL again to the ULS method. We assess the performance of both methods on two sub-experiment of the CelebA experiment. In Table 6(a), we form a MTL experiment with \(10\) different tasks. In Table 6(b), we form a MTL experiment with \(20\) different tasks where the first \(10\) tasks are the same as in the \(10\) tasks experiment. And in Table 5(c), a MTL experiment with all \(40\) tasks from the dataset. We train both methods for \(50\) epochs using a learning rate of \(0.0001\) and a batch size of \(128\). The temperature annealing schedule remains the same as in the MultiMNIST experiment. We report the per task classification accuracy in percentages (%) as well as the average task accuracy in the bottow row of both subtables.

parameters \(\theta_{1}\) and \(\theta_{2}\) by minimizing the following loss:

\[\mathcal{L}(X,Y): =\mathcal{L}_{1}(X,Y)+\alpha\mathcal{L}_{2}(X,Y)\] \[\mathcal{L}_{1}(X,Y): =\frac{1}{B}\mathcal{L}_{CE}(\hat{Y},Y)\] \[\mathcal{L}_{2}(X,Y): =\frac{1}{K}\|\bm{n}(X)-\sum_{i=1}^{B}\bm{y}_{i}\|^{2},\]

where \(\mathcal{L}_{CE}\) denotes the standard cross-entropy loss, \(\bm{n}\) denotes the output of the MVHG leading to \(\hat{Y}\), and \(\mathcal{L}_{2}\) ensures that \(\bm{n}_{i}\) matches the number of appearances of label \(i\) in the current batch. Using this simple training scheme, we achieve an f1-score of \(96.43\pm 0.02\) and \(82.71\pm 0.03\) on MNIST and FMNIST, respectively, further demonstrating the applicability and versatility of the DRPM to a number of different problems.

Figure 15: Additional partition samples from the DRPM-VC trained on MNIST. The different sets of each partition match each of the digits very well, even after repeatedly sampling from the model.

Figure 16: Additional partition samples from the DRPM-VC trained on FMNIST. Most clusters accurately represent one of the clothing categories and generate new samples very well. The only problem is with the handbag class, where the DRPM-VC learns two different clusters for different kinds of handbags (cluster \(5\) and \(6\)).

Figure 17: Various samples from each of the generative priors. Each prior learns to represent one of the digits. Further, we see a lot of variation between the different samples, suggesting that the clusters of the DRPM-VC manage to capture some of the diversity present in the dataset.

Figure 18: Various samples from each of the generative priors. Each prior learns to represent one of the digits. The DRPM-VC learns nice representations that provide coherent generations of most classes. For high-heels (cluster \(4\)), generating new samples seems difficult due to the heterogeneity within that class.