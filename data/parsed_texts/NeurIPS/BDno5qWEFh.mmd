# Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization

 Alex Foo Wynne Hsu Mong Li Lee

School of Computing

National University of Singapore

{alexfoo,whsu,leeml}@comp.nus.edu.sg

###### Abstract

Discovering object-centric representations from images has the potential to greatly improve the robustness, sample efficiency and interpretability of machine learning algorithms. Current works on multi-object images typically follow a generative approach that optimizes for input reconstruction and fail to scale to real-world datasets despite significant increases in model capacity. We address this limitation by proposing a novel method that leverages feature connectivity to cluster neighboring pixels likely to belong to the same object. We further design two object-centric regularization terms to refine object representations in the latent space, enabling our approach to scale to complex real-world images. Experimental results on simulated, real-world, complex texture and common object images demonstrate a substantial improvement in the quality of discovered objects compared to state-of-the-art methods, as well as the sample efficiency and generalizability of our approach. We also show that the discovered object-centric representations can accurately predict key object properties in downstream tasks, highlighting the potential of our method to advance the field of multi-object representation learning.

## 1 Introduction

Human understanding of the world relies on objects as compositional building blocks [24], and emulating this through object-centric representations can improve robustness, sample efficiency, generalization to out-of-domain distributions, and interpretability of machine learning algorithms [18; 9]. Recent work utilizes a generative approach, optimizing pixel-based reconstruction to learn object-centric representations [8; 17; 33; 4; 35; 12; 13; 11; 45; 46; 51]. This approach has limitations as it prioritizes pixel accuracy over object discovery and functional feature extraction [30; 11]. This may lead to the failure of discovering objects [35], or obtaining useful object features, such as position, shape, or boundaries between overlapping objects [23]. Additionally, pixel-based reconstruction tends to waste model capacity on less important visual features, such as complex backgrounds [25], making scaling these methods to real-world images a challenge.

To address the fundamental limitations of pixel-based reconstruction, we propose a framework that leverages feature connectivity and design two object-centric regularization terms to directly refine object representations, ensuring sufficient separation and high disentanglement between dimensions. Our method utilizes visual connectedness principles [38], where similar pixels that are connected should belong to the same object, to guide object discovery. The two regularization terms promote disentangled representations and prevent sub-optimal clustering.

We demonstrate that our approach outperforms state-of-the-art methods in discovering multiple objects from simulated, real-world, complex texture and common object images in a fine-grained manner without supervision. The proposed solution attains sample efficiency and is generalizable to out-of-domain images. The learned object representations also accurately predict key objectproperties in downstream tasks. Our contributions include: (1) a framework that leverages feature connectivity for fine-grained object discovery, (2) introduction of object-centric regularization terms as an alternative to pixel-based reconstruction loss, (3) experimental validation of our solution's superior performance, and (4) demonstration of the usefulness of discovered object representations in downstream tasks.

## 2 Related Work

Numerous works have demonstrated remarkable success in segmenting real-world images. Most of these works focus on semantic segmentation and object detection by utilizing supervised signals [19; 6]. In contrast, unsupervised approaches like SLIC [1] employ a modified k-means algorithm to cluster pixels into superpixels, similar to Felzenszwalb's algorithm [15] that relies on hand-crafted features for clustering. However, these methods do not focus on learning useful representations for the segmented components.

Various unsupervised methods for learning object-centric representations have been proposed, and can be categorized into three main approaches: spatial attention, sequential attention, and iterative attention. Spatial attention approach utilizes spatial transformer networks [21] to crop out rectangular regions from an image and extract object attributes such as position and scale [14; 31; 8; 33]. They rely on a fixed-size sampling grid which may not be suitable for scenes with varying object sizes, and may compromise training when the sampling grid does not overlap with any object.

The sequential attention approach uses RNN-based models such as MONet [5] and GENESIS [12] to sequentially attend to different regions in an image. These methods employ a deterministic network to perform the attention process, which allows them to capture and represent objects in the scene. However, these methods may neglect smaller objects as they tend to produce a weaker signal during the attention process. This can lead to incomplete or biased representations of scenes with objects of varying sizes. To overcome this, GENESIS-V2 [13] uses a stochastic stick-breaking process to perform attention randomly.

In the iterative attention approach, a set of object representations is randomly initialized and then iteratively refined to bind these objects to different regions of an image. IODINE [17] is a model that can discover objects with disentangled representations. However, it requires long training times and many samples. Slot Attention [35] introduces competition among the object representations by utilizing cross-attention along the object dimension. While this method is fast, versatile and can be extended to handle videos [29], it may fail to discover objects when the training set is diverse, and the resulting representations are highly entangled.

EfficientMORL [11], SLATE [45], SysBinder [46] and BO-QSA [22] are recent developments in the iterative attention approach aimed at addressing some limitations of earlier methods like Slot Attention. EfficientMORL presents an hierarchical variational autoencoder and a lightweight iterative refinement network to increase efficiency without sacrificing representation quality. SLATE increases the non-linear interaction between the slots in Slot Attention with an autoregressive decoder that is conditioned on the slots, resulting in improved reconstructions and object-centric representations. SysBinder enhances the slots of Slot Attention with factor representations called block-slots, which provides within-slot disentanglement between learned factors. BO-QSA initializes the slots of Slot Attention as learnable embeddings instead of sampling from a Gaussian distribution and uses bi-level optimization, resulting in more stable training. Despite the advancements, one drawback remains: the number of clusters are fixed a priori which limits the applicability in real-world scenarios where the number of objects or clusters is not known beforehand.

## 3 Methodology

Our proposed method OC-Net is designed to extract objects in an image without relying on labeled data or specifying the number of objects present in the image. By not requiring the number of objects to be specified beforehand, OC-Net can generalize better to real-world scenes with varying numbers of objects and handle more complex scenarios.

Figure 1 shows the main components of OC-Net. The main idea behind OC-Net is to learn pixel embeddings that can be clustered to discover objects and their respective object masks and object

representations. This is achieved by passing the input image through a \(1\times 1\) convolutional layer to obtain a set of \(N\) pixel embeddings \(\mathcal{P}=\{\mathbf{p}_{1},\ldots,\mathbf{p}_{N}\}\) of \(D\) dimensions each. We leverage on feature connectivity and iteratively cluster the embeddings of neighbouring pixels based on the likelihood that they belong to the same object. The output is a set of objects \(\mathcal{O}=\{O_{1},\ldots,O_{M}\}\) where each object is a set of pixel embeddings. We derive the object mask \(\mathbf{m}_{j}\) of each object by setting the pixel corresponding to the embedding in \(O_{j}\):

\[\mathbf{m}_{j}[i]=\begin{cases}1&\text{if }\mathbf{p}_{i}\in O_{j}\\ 0&\text{otherwise}\end{cases}\] (1)

where \(\mathbf{m}_{j}[i]\) denotes the \(i^{th}\) pixel value and \(i\in\{1,\ldots,N\}\).

With this, we obtain the matrix of object representations \(\mathbf{Z}=[\mathbf{z}_{1},\ldots,\mathbf{z}_{M}]\) where each \(\mathbf{z}_{j}\) is the sum of extracted mask information and the average of the pixel embeddings in \(O_{j}\):

\[\mathbf{z}_{j}[d]=(\mathbf{A}\cdot\mathbf{m}_{j})[d]+\frac{1}{|O_{j}|}\sum_{ \mathbf{p}_{i}\in O_{j}}\mathbf{p}_{i}[d]\] (2)

where \(\mathbf{z}_{j}[d]\) denotes the \(d^{th}\) value of vector \(\mathbf{z}_{j}\), \(d\in\{1,\ldots,D\}\), \(\mathbf{A}\) is the mask transformation matrix.

### Object Discovery

The object discovery process iteratively clusters the pixel embeddings based on their feature connectivity and similarity. LayerNorm [2] is applied to normalize all pixel embeddings, and positional encodings are added to the pixel embeddings. The neighbors of a pixel embedding \(\mathbf{p}\) are the set of embeddings of the 8 neighbours in the input image. We use Dijkstra's algorithm to compute the shortest distance of a sampled pixel embedding to all other embeddings as follows.

Let \(\mathcal{U}\) be the set of pixel embeddings that have not been assigned to an object yet. We uniformly sample a pixel embedding \(\mathbf{p}_{i}\in\mathcal{U}\). The distance from \(\mathbf{p}_{i}\) to itself is set to zero, and the distance to all the other pixels is set to infinity. We select an unvisited pixel embedding \(\mathbf{p}_{m}\) that has the minimum distance to \(\mathbf{p}_{i}\). Let \(\mathbf{p}_{k}\) be the embedding of a neighbour of the pixel corresponding to \(\mathbf{p}_{m}\). We compute the distance between a pair of neighbouring pixels as the similarity between their corresponding embeddings given by:

\[\text{sim}(\mathbf{p}_{m},\mathbf{p}_{k})=\sqrt{\sum_{d=1}^{D}(\mathbf{p}_{m} [d]-\mathbf{p}_{k}[d])^{2}}\] (3)

Figure 1: Overview of OC-Net.

where \(\mathbf{p}_{m}[d]\) denotes the \(d^{th}\) value of the embedding \(\mathbf{p}_{m}\).

If the distance between \(\mathbf{p}_{i}\) and \(\mathbf{p}_{k}\) is shorter through \(\mathbf{p}_{m}\), we update the shortest distance accordingly. This ensures that we consider the most efficient path between pixel embeddings, leading to better object discovery. We mark \(\mathbf{p}_{m}\) as visited and consider it to be part of the same object as \(\mathbf{p}_{i}\) according to a threshold. The process is repeated for the next unvisited pixel embedding until all pixels have been visited.

### Object-Centric Regularization

We design two object-centric regularization terms \(\mathcal{L}_{sep}\) and \(\mathcal{L}_{ent}\) to improve the quality of the learned object representations for downstream generalization and object discovery. Given the matrix of object representations \(\mathbf{Z}=[\mathbf{z}_{1},\dots,\mathbf{z}_{M}]\) corresponding to the object training samples \(\{(\mathbf{x}_{i},\mathbf{y}_{i})\}_{i=1}^{M}\), we quantify downstream generalization performance with the prediction error:

\[\delta_{\mathbf{Z}}=\frac{1}{M}\sum_{i=1}^{M}||\mathbf{W}\cdot\mathbf{z}_{i}- \mathbf{y}_{i}||\] (4)

where \(\mathbf{W}\) is the minimum-norm solution of the downstream predictor.

Since labels \(\mathbf{y}_{i}\) are unknown in the unsupervised setting, \(\delta_{\mathbf{Z}}\) cannot be directly minimized. Theorem 3.1 below shows that we can minimize an upper bound of \(\delta_{\mathbf{Z}}\) by using the projection matrix \(\mathbf{P}_{\mathbf{Z}}\). Proof of the theorem is provided in the supplementary material.

**Theorem 3.1**.: _Let \(\mathbf{Y}\) be the matrix of training sample labels and \(\mathbf{P}_{\mathbf{Z}}\) be the projection matrix of \(\mathbf{Z}\):_

\[\mathbf{P}_{\mathbf{Z}}=\mathbf{I}-\mathbf{Z}^{\top}(\Sigma_{\mathbf{Z}})^{ \dagger}\mathbf{Z}\] (5)

_where \(\mathbf{I}\) is the identity matrix, \((.)^{\dagger}\) is the pseudoinverse and \(\Sigma_{\mathbf{Z}}=\mathbf{Z}\mathbf{Z}^{\top}\) is the unnormalized covariance matrix of \(\mathbf{Z}\). Let \(||.||_{F}\) be the Frobenius norm. Then, the following relation holds:_

\[\delta_{\mathbf{Z}}\leq||\mathbf{P}_{\mathbf{Z}}||_{F}||\mathbf{Y}||_{F}\] (6)

Since \(||\mathbf{Y}||_{F}\) in Equation 6 is unknown but fixed, we can minimize \(\delta_{\mathbf{Z}}\) by minimizing \(||\mathbf{P}_{\mathbf{Z}}||_{F}\). From the definition of \(\mathbf{P}_{\mathbf{Z}}\) in Equation 5, \(||\mathbf{P}_{\mathbf{Z}}||_{F}\) is minimized when the rank of \(\Sigma_{\mathbf{Z}}\) is maximized [44; 3]. We achieve this by maximizing the diagonal entries of \(\Sigma_{\mathbf{Z}}\) with a separation term \(\mathcal{L}_{sep}\) while simultaneously minimizing its off-diagonal entries with an entanglement term \(\mathcal{L}_{ent}\), in effect regularizing \(\Sigma_{\mathbf{Z}}\) to be a diagonal matrix with a maximum number of nonzero entries.

Maximizing the diagonal entries of \(\Sigma_{\mathbf{Z}}\) via \(\mathcal{L}_{sep}\) consequently maximizes the distance between object representations in the latent space. This encourages the model to learn distinct and non-overlapping object representations. Expanding the representation space also ensures that objects with varying features and properties can be accurately represented and distinguished from one another, enhancing both downstream generalization and fine-grained object discovery. We define \(\mathcal{L}_{sep}\) as:

\[\mathcal{L}_{sep}=\frac{1}{D}\sum_{d=1}^{D}\max(0,1-\sqrt{\sigma_{d}+\tau})\] (7)

where \(\sigma_{d}\) is the variance of the \(d^{th}\) dimension across the vectors \(\mathbf{z}_{1},\cdots,\mathbf{z}_{M}\) and \(\tau\) is a small constant to maintain numerical stability.

The entanglement term \(\mathcal{L}_{ent}\) minimizes the off-diagonal entries of \(\Sigma_{\mathbf{Z}}\) and consequently minimizes the correlation between dimensions in the latent space \(\mathbf{Z}\), thereby achieves more disentangled object representations. Such representations are easier to manipulate and analyze, as each dimension captures a distinct object property, such as position, scale, or color. \(\mathcal{L}_{ent}\) is defined as follows:

\[\mathcal{L}_{ent}=\frac{1}{D\times(M-1)}\sum_{i\neq j}\Sigma_{\mathbf{Z}}[i,j]\] (8)

## 4 Performance Study

We conduct experiments to evaluate the performance of OC-Net in terms of quality, sample efficiency and generalizability. We use a diverse range of datasets to demonstrate its effectiveness across various scenarios:1. Simulated datasets Multi-dSprites [23] and Tetrominoes-NM. The former consists of multiple oval, heart or square-shaped sprites with some occlusions, while the latter is a subset of the original Tetrominoes dataset [23] where images whose ground truth segmentation requires knowledge of the object shapes are filtered out.
2. Real-world multi-object datasets SVHN [36] and IDRiD [40]. SVHN consists of street view images of house numbers while IDRiD is the Indian Diabetic Retinopathy Image Segmentation Dataset.
3. Complex texture datasets CLEVRTEX [26] and CLEVRTEX-OOD. CLEVRTEX features scenes with diverse shapes, textures and photo-mapped materials while CLEVRTEX-OOD is the CLEVRTEX out-of-distrbution test set with 25 new materials and 4 new shapes.
4. Common object datasets Flowers [37], Birds [48] and COCO [50]. The Flowers dataset features 17 diverse flower classes with large variations viewpoint, scale, illumination and background. Birds is the most widely-used CUB-200-2011 dataset for fine-grained visual categorization. COCO is the variant of the Microsoft Common Objects in Context dataset used for large-scale object segmentation [32].

Table 1 shows the dataset characteristics. Following [35, 11], we use the first 60K samples in Multi-dSprites, Tetrominoes-NM and SVHN for training and hold out the next 320 samples for testing. For IDRiD, we split this dataset into 54 images for training and 27 images for testing. For CLEVRTEX, we use the first 40K samples for training and last 5K samples for testing. For CLEVRTEX-OOD, we use 10K samples for testing. For Flowers, we use the first 6K samples for training and last 1K samples for testing. For Birds, we use the first 10K samples for training and last 1K samples for testing. For COCO, we use the first 10K samples for training and last 2K samples for testing.

We compare OC-Net with SLIC [1], Felzenszwalb [11], Slot Attention [35], EfficientMORL [11], GENESIS-V2 [13], SLATE [45], SysBinder [46] and BO-QSA [22]. We train OC-Net for 1000 iterations with a batch size of 64 using Adam [28] with a learning rate of \(1\times 10^{-3}\). We carried out an initial experiment to choose the clustering threshold. The results show that the value can range from 0.2 to 2.0 without affecting the performance of OC-Net. As such, we set the threshold to \(\epsilon=0.7\) so that two pixels will belong to the same object if their normalized feature similarity is more than 50%. If a pixel is assigned to multiple objects, we assign it to the mask of the first object in that list and ignore its membership in other objects. Training on 64-by-64 images from Multi-dSprites on a single V100 GPU with 32GB of RAM takes about 10 minutes.

For all methods, we set the maximum number of foreground objects to 6 and 4 for Multi-dSprites and Tetrominoes respectively. Training is carried out for 300,000 iterations with a batch size of 64, using the Adam optimizer with a base learning rate of \(4\times 10^{-4}\). We set the size of the latent space to be \(D=64\) for all models. For SVHN and COCO, the number of objects is set to 6. For IDRiD, the number of objects is set to 20 and we train them for 100,000 iterations. For CLEVRTEX and CLEVRTEX-OOD, the number of objects is set to 11. For Flowers and Birds, the number of objects is set to 2.

We use the Adjusted Rand Index (ARI) to measure the quality of objects discovered [20]. The ARI is a measure of similarity between two data clusterings that takes into account the permutation-invariant nature of the predicted segmentation masks and their corresponding ground-truth masks. We also

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Type & Ground Truth & Image Size & \# Samples \\ \hline Multi-dSprites & Simulated & Pixel Mask & \(64\times 64\) & 1M \\ Tetrominoes-NM & Simulated & Pixel Mask & \(35\times 35\) & 1M \\ SVHN & Real-World & Bounding Box & Varied & 530K \\ IDRiD & Real-World & Pixel Mask & \(4288\times 2848\) & 81 \\ CLEVRTEX & Complex Texture & Pixel Mask & \(128\times 128\) & 50K \\ CLEVRTEX-OOD & Complex Texture & Pixel Mask & \(128\times 128\) & 10K \\ Flowers & Common Object & Pixel Mask & \(128\times 128\) & 7K \\ Birds & Common Object & Pixel Mask & \(128\times 128\) & 11K \\ COCO & Common Object & Pixel Mask & \(128\times 128\) & 12K \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of dataset characteristicsuse the Dice similarity coefficient, along with the Intersection-over-Union (IoU) between the best matching object masks \(X\) and \(Y\) as follows:

\[\text{Dice}(X,Y)=\frac{2|X\cap Y|}{|X|+|Y|}\quad\text{;}\quad\text{IoU}(X,Y)= \frac{|X\cap Y|}{|X\cup Y|}\] (9)

where \(X\) the set of object pixels extracted and \(Y\) is the set of annotated object pixels in the ground truth. We compute the mean Dice and the mean IoU scores, denoted as mDice and mIoU respectively, by averaging the individual Dice and IoU scores across all matches. For the complex textures dataset, background discovery is included in the computation of the scores.

### Experiments on Quality of Discovered Objects

We first evaluate the ability of OC-Net to discover objects from images with multiple objects. Table 2(a) shows the average ARI, mDice and mIoU scores based on the discovered foreground objects in the simulated datasets after 3 runs. We observe that OC-Net outperforms all other methods by a large margin in Multi-dSprites, and even achieves perfect score for all Tetrominoes-NM test samples.

Table 2(b) shows the results on the real-world multi-object image datasets. For SVHN, OC-Net outperforms all methods, even when the ground truth is provided in the form of bounding boxes. This implies that we need to expand the discovered object masks into their corresponding bounding boxes which are often rough fits, and is the reason for the close difference in mDice and mIoU scores between OC-Net and BO-QSA. For IDRiD, which contains multiple small objects, OC-Net significantly improves the ARI scores and more than doubles the mDice and mIoU scores over all methods, demonstrating its robustness in challenging object discovery tasks.

Table 2(c) shows the results on CLEVRTEX and CLEVRTEX-OOD, which contains complex textured objects and backgrounds. Here, OC-Net again shows superior performance in all metrics, illustrating its capability to effectively segment complex objects. Although a general decrease in performance is observed across all methods in the CLEVRTEX-OOD test set, likely due to a change in data distribution, OC-Net's performance drop is slight and it still outperforms the closest baseline.

Finally, Table 2(d) shows the results on Flowers, Birds and COCO common object datasets. Here, OC-Net again shows superior performance in all metrics, illustrating its capability to effectively segment commonly seen natural objects. Notably, OC-Net outperforms all other methods by a large margin in COCO, demonstrating its robustness in handling objects with highly varied appearances.

Figure 2 visualizes the objects discovered by the various methods for sample images. OC-Net is able to identify large and small objects in Multi-dSprites even when these objects are significantly occluded. Moreover, in the Tetrominoes-NM dataset, despite the presence of shadow effects that often confuse existing methods, OC-Net still manages to separate each tile. For SVHN, only OC-Net is able to segment the character objects out in a fine-grained manner. EfficientMORL tend to group all the characters together while the other methods segment the objects in a coarse-grained manner. For IDRiD, OC-Net is able to segment out the optic disc and small lesions which other methods fail to discover. For CLEVRTEX and CLEVRTEX-OOD, OC-Net is able to segment out the various objects from the complex-textured backgrounds in a fine-grained manner. Finally, for Flowers, Birds, and COCO, only OC-Net is able to segment out the complex-shaped and multi-part objects from the backgrounds in a fine-grained manner.

### Experiments on Sample Efficiency

One obstacle to unsupervised object discovery is the availability of a sufficiently large number of suitable training samples. Sample efficiency refers to a model's ability to learn effectively from a relatively small number of examples. Figure 3 shows the mIoU scores as we decrease the number of training samples in Multi-dSprites, SVHN and CLEVRTEX. OC-Net is able to achieve near-optimal performance even with a significantly smaller training set (1,000 samples) compared to all the other methods. The high sample efficiency of OC-Net reduces the need for large, potentially costly or difficult-to-obtain datasets. This makes OC-Net a more practical solution for real-world applications.

\begin{table}

\end{table}
Table 2: Evaluation scores for the discovered foreground objects.

### Experiments on Model Generalizability

Ideally, an unsupervised object discovery model should be trained to understand common visual appearances such as the difference between foreground vs background, so as to discover objects from

Figure 2: Visualization of discovered objects.

out-of-domain images. In this set of experiments, we compare the generalization ability of OC-Net with the baselines by training the models on Multi-dSprites and testing them on the other datasets.

Table 3 shows the results. For Tetrominoes-NM, there is a decrease in performance for all methods while OC-Net still obtains perfect score. For SVHN, CLEVRTEX and CLEVRTEX-OOD, the performance of all models decrease due to the shift in data distribution. However, OC-Net experiences the smallest drop in performance and still significantly surpasses the best performing method. For IDRiD, the methods show improvement in performance. One possible reason is that the larger training set in Multi-dSprites enables the circular shape of the optic disc to be better segmented. Despite this, OC-Net remains the top performer.

### Ablation Studies

Next, we examine the effect of feature connectivity and regularization terms \(\mathcal{L}_{sep}\) and \(\mathcal{L}_{ent}\) on the performance of OC-Net. We implemented three variants of OC-Net: (a) w/o connectivity. Here, we do not require that a path should exist between \(i\) and \(k\) when computing \(\text{dist}[i,k]\); (b) w/o \(\mathcal{L}_{ent}\). This network is trained without the entanglement regularization term; (c) w/o \(\mathcal{L}_{sep}\). The separation regularization term is not used in the training of OC-Net.

Table 4 shows the mIoU scores for all the datasets. Without feature connectivity, we observe a drop in performance across all datasets since it is common for images to have multiple identical objects, that is, same color and shape. As such, OC-Net w/o connectivity tend to cluster these blocks as a single object.

Removing the entanglement term (OC-Net w/o \(\mathcal{L}_{ent}\)) leads to a slight decrease in the performance as the object representations may still be separated even when the dimensions are entangled. The largest performance drop in all datasets is seen when the object representation separation term is removed (OC-Net w/o \(\mathcal{L}_{sep}\)), indicating the importance of having a well-separated object representation space for effective object discovery.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Method & Tetrominoes-NM & SVHN & IDRiD & CLEVRTEX & CLEVRTEX-OOD \\ \hline Slot Attention & 21.8\(\pm\)3.5 & 19.5\(\pm\)3.8 & 7.5\(\pm\)2.5 & 12.2\(\pm\)2.2 & 12.3\(\pm\)2.2 \\ EfficientMORL & 21.2\(\pm\)3.8 & 23.4\(\pm\)2.8 & 6.5\(\pm\)2.6 & 12.7\(\pm\)3.2 & 15.2\(\pm\)2.0 \\ GENESIS-V2 & 42.9\(\pm\)4.9 & 31.1\(\pm\)2.8 & 8.5\(\pm\)2.4 & 21.9\(\pm\)1.6 & 21.3\(\pm\)2.5 \\ SLATE & 51.4\(\pm\)1.6 & 21.1\(\pm\)2.0 & 10.0\(\pm\)1.7 & 12.7\(\pm\)2.2 & 12.9\(\pm\)1.8 \\ SysBinder & 28.5\(\pm\)1.8 & 23.8\(\pm\)1.1 & 13.9\(\pm\)1.8 & 10.6\(\pm\)1.5 & 11.6\(\pm\)1.9 \\ BO-QSA & 41.8\(\pm\)1.8 & 24.3\(\pm\)2.0 & 4.0\(\pm\)1.5 & 24.4\(\pm\)1.4 & 22.8\(\pm\)2.5 \\ OC-Net & **100.0\(\pm\)0.0** & **47.5\(\pm\)0.5** & **29.1\(\pm\)0.5** & **31.7\(\pm\)0.6** & **31.3\(\pm\)0.6** \\ \hline \hline \end{tabular}
\end{table}
Table 3: mIoU scores for model generalizability after training on Multi-dSprites.

Figure 3: mIoU scores vs decreasing number of training samples.

## 5 Prediction based on Learned Object Representation

One characteristic of an effective object-centric representation is its ability to encode object properties such as color, position and shape [42]. In this section, we show that the learned object representations are disentangled and can be used to predict the values of these properties.

Given the representations and their corresponding ground truth values of a target object property, we employ them as features to train a gradient boosted tree (GBT) [16]. To evaluate how well the properties of unseen objects are predicted by the GBT, we use the coefficient of determination \(R^{2}\)[49]. We perform an experiment using the learned object representations from the simulated datasets to predict the properties of object such as color, position and shape. We use the mIoU score to match the discovered object to the ground truth object. We split the 320 test images equally into two sets, one for training the GBT model for each object property, and the other for evaluation.

Table 5 shows the average \(R^{2}\) scores of the GBT models on the evaluation set. The GBT models trained with OC-Net representations achieved the highest \(R^{2}\) scores compared to the models trained using the representations from other methods. This suggests that the object representations learned by OC-Net are effective in encoding the object properties.

## 6 Conclusion

In this work, we have described a framework called OC-Net that learns object-centric representations in a fine-grained manner without supervision. OC-Net leverages on feature connectivity and two new regularization terms to learn disentangled representations and to ensure the representations of different objects are well-separated. From the results of experiments conducted on simulated, real-world, complex texture and common object images, we have demonstrated the superior quality of the object representations over current state-of-the-art. Moreover, we have highlighted the sample efficiency and generalizability of OC-Net. Finally, we have shown how the discovered object representations can be used to predict object properties in a downstream task, indicating its potential use for other computer vision applications where samples and ground truth labels are limited.

There are still obstacles that have to be overcome for successful application of our framework to the full visual complexity of the real world. A natural next step would be to extend OC-Net to handle real-world scenes containing objects with more complex part-whole hierarchies. It is also promising to explore explicit representation of the discovered objects into a dictionary of prototypes to better handle occlusion between objects. Lastly, real-world scenes with multiple objects is still of higher visual complexity than the datasets considered here and reliably bridging this gap is an open problem.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Method & Multi-dSprites & Tetro-NM & SVHN & IDRiD & CTEX & CTEX-OOD \\ \hline OC-Net & **99.1\(\pm\)0.0** & **100.0\(\pm\)0.0** & **49.9\(\pm\)0.1** & **31.2\(\pm\)0.2** & **37.5\(\pm\)0.7** & **35.0\(\pm\)0.6** \\ w/o connectivity & 98.8\(\pm\)0.1 & 89.2\(\pm\)0.1 & 36.6\(\pm\)0.1 & 17.3\(\pm\)0.1 & 20.3\(\pm\)0.9 & 17.7\(\pm\)0.9 \\ w/o \(\mathcal{L}_{ent}\) & 98.7\(\pm\)0.3 & 99.6\(\pm\)0.1 & 48.8\(\pm\)0.1 & 25.8\(\pm\)0.2 & 26.4\(\pm\)0.8 & 18.9\(\pm\)0.4 \\ w/o \(\mathcal{L}_{sep}\) & 49.3\(\pm\)0.3 & 51.2\(\pm\)0.2 & 35.0\(\pm\)0.2 & 4.0\(\pm\)0.1 & 18.5\(\pm\)0.6 & 11.0\(\pm\)0.4 \\ \hline \hline \end{tabular}
\end{table}
Table 4: mIoU scores for variants of OC-Net.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Multi-dSprites} & \multicolumn{3}{c}{Tetrominoes-NM} \\ \cline{2-7} Method & Color & Position & Shape & Color & Position & Shape \\ \hline Slot Attention & 72.2\(\pm\)12 & 96.8\(\pm\)0.1 & 38.2\(\pm\)0.0 & 86.5\(\pm\)6.5 & 98.7\(\pm\)0.6 & 36.3\(\pm\)0.0 \\ EfficientMORL & 86.5\(\pm\)6.2 & 95.8\(\pm\)0.1 & 61.7\(\pm\)0.0 & 94.9\(\pm\)3.2 & 97.9\(\pm\)0.7 & 68.5\(\pm\)0.0 \\ GENESIS-V2 & 78.1\(\pm\)7.5 & 97.1\(\pm\)0.7 & 75.8\(\pm\)0.0 & 88.1\(\pm\)5.8 & 94.6\(\pm\)2.6 & 37.9\(\pm\)0.0 \\ SLATE & 87.5\(\pm\)0.7 & 90.6\(\pm\)4.4 & 31.7\(\pm\)0.0 & 85.5\(\pm\)3.9 & 89.6\(\pm\)0.7 & 10.5\(\pm\)0.0 \\ SysBinder & 73.6\(\pm\)1.0 & 69.3\(\pm\)3.4 & 33.3\(\pm\)0.0 & 97.9\(\pm\)0.6 & 77.8\(\pm\)2.7 & 19.9\(\pm\)0.0 \\ BO-QSA & 96.3\(\pm\)1.6 & 97.4\(\pm\)0.1 & 75.2\(\pm\)0.0 & 98.1\(\pm\)0.7 & 98.9\(\pm\)0.2 & 52.5\(\pm\)0.0 \\ OC-Net & **98.0\(\pm\)0.6** & **98.3\(\pm\)0.1** & **78.1\(\pm\)0.0** & **100.0\(\pm\)0.0** & **99.4\(\pm\)0.1** & **98.7\(\pm\)0.0** \\ \hline \hline \end{tabular}
\end{table}
Table 5: \(R^{2}\) scores for object property prediction on simulated datasets

## Acknowledgments and Disclosure of Funding

This research is supported by the National Research Foundation Singapore under its AI Singapore Programme (Award Number: AISG-GC-2019-001-2A).

## References

* [1]A. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk (2012) Slic superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on Pattern Analysis and Machine Intelligence34 (11), pp. 2274-2282. External Links: Document, ISSN 1558-0221 Cited by: SS1.
* [2]J. L. Ba, J. R. Kiros, and G. E. Hinton (2016) Layer normalization. Advances in NIPS 2016 Deep Learning Symposium. Cited by: SS1.
* [3]A. Bardes, J. Ponce, and Y. LeCun (2022) VICREG: variance-invariance-covariance regularization for self-supervised learning. In The Tenth International Conference on Learning Representations, ICLR, Cited by: SS1.
* [4]D. Bear, C. Fan, D. Mrowca, Y. Li, S. Alter, A. Nayebi, J. Schwartz, J. Fei-Fei, J. Wu, J. Tenenbaum, et al. (2020) Learning physical graph representations from visual scenes. Advances in Neural Information Processing Systems33, pp. 6027-6039. Cited by: SS1.
* [5]C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. M. Botvinick, and A. Lerchner (2019) Monet: unsupervised scene decomposition and representation. CoRR, pp. 1901.11390. Cited by: SS1.
* [6]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko (2020) End-to-end object detection with transformers. In European conference on computer vision, pp. 213-229. Cited by: SS1.
* [7]M. Caron, H. Touvron, I. Misra, H. Jegou, P. Mairal, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9650-9660. Cited by: SS1.
* [8]E. Crawford and J. Pineau (2019) Spatially invariant unsupervised object detection with convolutional neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, pp. 3412-3420. Cited by: SS1.
* [9]A. Dittadi, S. S. Papa, M. D. Vita, B. Scholkopf, O. Winther, and F. Locatello (2022) Generalization and robustness implications in object-centric learning. In International Conference on Machine Learning, ICML, Cited by: SS1.
* [10]C. Eastwood and C. K. I. Williams (2018) A framework for the quantitative evaluation of disentangled representations. In International Conference on Learning Representations, Cited by: SS1.
* [11]P. Emami, P. He, S. Ranka, and A. Rangarajan (2021) Efficient iterative amortized inference for learning symmetric and disentangled multi-object representations. In International Conference on Machine Learning, pp. 2970-2981. Cited by: SS1.
* [12]M. Engelcke, A. R. Kosiorek, O. P. Jones, and I. Posner (2020) GENESIS: generative scene inference and sampling with object-centric latent representations. In 8th International Conference on Learning Representations, ICLR 2020, Cited by: SS1.
* [13]M. Engelcke, O. Parker Jones, and I. Posner (2021) Genesis-v2: inferring unordered object representations without iterative refinement. Advances in Neural Information Processing Systems34, pp. 8085-8094. Cited by: SS1.
* [14]S. Eslami, N. Heess, T. Weber, Y. Tassa, D. Zepesvari, G. E. Hinton, et al. (2016) Attend, infer, repeat: fast scene understanding with generative models. Advances in Neural Information Processing Systems29. Cited by: SS1.
* [15]P. F. Felzenszwalb and D. P. Huttenlocher (2004) Efficient graph-based image segmentation. International journal of computer vision59 (2), pp. 167-181. Cited by: SS1.
* [16]P. F. Felzenszwalb (2004) Learning physical graph representations from visual scenes. Advances in Neural Information Processing Systems33, pp. 1000-1015. Cited by: SS1.

[MISSING_PAGE_POST]

* [16] Friedman, J. H. Greedy function approximation: a gradient boosting machine. _Annals of statistics_, pp. 1189-1232, 2001.
* [17] Greff, K., Kaufman, R. L., Kabra, R., Watters, N., Burgess, C., Zoran, D., Matthey, L., Botvinick, M., and Lerchner, A. Multi-object representation learning with iterative variational inference. In _International Conference on Machine Learning_, pp. 2424-2433, 2019.
* [18] Greff, K., Van Steenkiste, S., and Schmidhuber, J. On the binding problem in artificial neural networks. _arXiv preprint arXiv:2012.05208_, 2020.
* [19] He, K., Gkioxari, G., Dollar, P., and Girshick, R. Mask r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pp. 2961-2969, 2017.
* [20] Hubert, L. and Arabie, P. Comparing partitions. _Journal of classification_, 2(1):193-218, 1985.
* [21] Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial transformer networks. _Advances in neural information processing systems_, 28, 2015.
* [22] Jia, B., Liu, Y., and Huang, S. Improving object-centric learning with query optimization. In _The Eleventh International Conference on Learning Representations_, 2023.
* [23] Kabra, R., Burgess, C., Matthey, L., Kaufman, R. L., Greff, K., Reynolds, M., and Lerchner, A. Multi-object datasets. https://github.com/deepmind/multi-object-datasets/, 2019.
* [24] Kahneman, D., Treisman, A., and Gibbs, B. J. The reviewing of object files: Object-specific integration of information. _Cognitive psychology_, 24(2):175-219, 1992.
* [25] Karazija, L., Laina, I., and Rupprecht, C. Clevrtex: A texture-rich benchmark for unsupervised multi-object segmentation. In _Proceedings of the Neural Information Processing Systems_, 2021.
* [26] Karazija, L., Laina, I., and Rupprecht, C. Clevrtex: A texture-rich benchmark for unsupervised multi-object segmentation. In Vanschoren, J. and Yeung, S. (eds.), _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks_. Curran, 2021.
* [27] Kim, J., Choi, J., Choi, H.-J., and Kim, S. J. Shepherding slots to objects: Towards stable and robust object-centric learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 19198-19207, 2023.
* [28] Kingma Diederik, P. and Adam, J. B. A method for stochastic optimization. _International Conference on Learning Representations ICLR_, 2015.
* [29] Kipf, T., Elsayed, G. F., Mahendran, A., Stone, A., Sabour, S., Heigold, G., Jonschkowski, R., Dosovitskiy, A., and Greff, K. Conditional Object-Centric Learning from Video. In _International Conference on Learning Representations (ICLR)_, 2022.
* [30] Kipf, T. N., van der Pol, E., and Welling, M. Contrastive learning of structured world models. In _8th International Conference on Learning Representations, ICLR_, 2020.
* [31] Kosiorek, A., Kim, H., Teh, Y. W., and Posner, I. Sequential attend, infer, repeat: Generative modelling of moving objects. _Advances in Neural Information Processing Systems_, 31, 2018.
* [32] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.
* [33] Lin, Z., Wu, Y., Peri, S. V., Sun, W., Singh, G., Deng, F., Jiang, J., and Ahn, S. SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition. In _8th International Conference on Learning Representations, ICLR 2020_, 2020.
* [34] Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Scholkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. In _international conference on machine learning_, pp. 4114-4124. PMLR, 2019.

* [35] Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf, T. Object-centric learning with slot attention. _Advances in Neural Information Processing Systems_, 33:11525-11538, 2020.
* [36] Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. _NIPS Workshop on Deep Learning and Unsupervised Feature Learning_, 2011.
* [37] Nilsback, M.-E. and Zisserman, A. Delving deeper into the whorl of flower segmentation. _Image and Vision Computing_, 28(6):1049-1062, 2010.
* [38] Palmer, S. and Rock, I. Rethinking perceptual organization: The role of uniform connectedness. _Psychonomic bulletin & review_, 1(1):29-55, 1994.
* [39] Paszke, A. e. a. Pytorch: An imperative style, high-performance deep learning library. _Advances in Neural Information Processing Systems_, 2019.
* [40] Porwal, P., Pachade, S., Kamble, R., Kokare, M., Deshmukh, G., Sahasrabuddhe, V., and Meriaudeau, F. Indian diabetic retinopathy image dataset (idrid), 2018.
* [41] Rezende, D. J. and Viola, F. Taming vaes. _ArXiv_, abs/1810.00597, 2018.
* [42] Scholkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., and Bengio, Y. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021. doi: 10.1109/JPROC.2021.3058954.
* [43] Shi, J. and Malik, J. Normalized cuts and image segmentation. _IEEE Transactions on pattern analysis and machine intelligence_, 22(8):888-905, 2000.
* [44] Shwartz-Ziv, R., Balestriero, R., Kawaguchi, K., Rudner, T. G., and LeCun, Y. An information-theoretic perspective on variance-invariance-covariance regularization. _arXiv preprint arXiv:2303.00633_, 2023.
* [45] Singh, G., Deng, F., and Ahn, S. Illiterate dall-e learns to compose. In _International Conference on Learning Representations_, 2021.
* [46] Singh, G., Kim, Y., and Ahn, S. Neural systematic binder. In _The Eleventh International Conference on Learning Representations_, 2023.
* [47] Wang, X., Girdhar, R., Yu, S. X., and Misra, I. Cut and learn for unsupervised object detection and instance segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3124-3134, 2023.
* [48] Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., and Perona, P. Caltech-ucsd birds 200. 2010.
* [49] Wright, S. Correlation and causation. _Journal of agricultural research_, 1921.
* [50] Yang, Y. and Yang, B. Promising or elusive? unsupervised object segmentation from real-world single images. _Advances in Neural Information Processing Systems_, 35:4722-4735, 2022.
* [51] Yuan, J., Chen, T., Li, B., and Xue, X. Compositional scene representation learning via reconstruction: A survey. _arXiv preprint arXiv:2202.07135_, 2022.