# Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models

Salma Abdel Magid, Weiwei Pan, Simon Warchol, Grace Guo,

**Junsik Kim, Wanhua Li, Mahia Rahman, Hanspeter Pfister**

Harvard University

Cambridge, MA, USA

Correspondence to: sabdelmagid@g.harvard.edu, weiweipan@g.harvard.edu

###### Abstract

Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose _Concept2Concept_, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user defined distributions or empirical, real world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool facilitating use by non-technical end-users.

_Warning: This paper contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers._

## 1 Introduction

Text-to-image (T2I) models have become central to many real-world AI-driven applications. However, the complexity of these models makes it difficult to understand how they associate concepts in images with textual prompts. Existing works have shown that T2I models can resolve prompts in unexpected ways (Bianchi et al. (2023)). Furthermore, the training datasets for T2I models are often large, uncurated, and may contain undesirable prompt to image associations that models can learn to internalize (Birhane et al. (2024)). Thus, without robust auditing frameworks that help us detect these undesirable associations, we risk deploying T2I models that generate unexpected and inappropriate content for a given task.

However, auditing T2I models is challenging because it is difficult to systematically, efficiently, and intuitively explore the vast space of prompts and possible outputs. Because raw pixel values alone are difficult to semantically reason about, previous works learn mappings from raw inputs to high-level concepts. This can be achieved post-hoc (Kim et al. (2018); Zhou et al. (2018); Ghorbani et al. (2019)) or as an intervention during training (Koh et al. (2020);Chen et al. (2020). Although these methods were designed for classification networks, it is this general intuition which motivates our work.

In this paper, we propose a framework for producing interpretable characterizations of the conditional distribution of generated images given a prompt, \(p(\texttt{image}|\texttt{prompt})\). We do so by extracting high-level concepts from each image and summarizing \(p(\texttt{image}|\texttt{prompt})\) in terms of such concepts. Here, we define concepts as a class of objects/nouns, ideas, open vocabulary detected classes or labels.

Our contributions are as follows:

(1) We propose **an interpretable framework** for concept-association based auditing of conditional distributions. Specifically, in our framework: we sample images from a T2I model under audit, given a prior distribution over prompts-either a user defined distribution or an empirical real-world distribution. Then, using a fast, scalable visual grounding model, we extract concepts from generated images. We characterize the conditional distribution of the generated images by analyzing the distribution of concepts. This framework allows users to systemically investigate associations of conditional distributions at varying levels of granularity, from broad concept trends and co-occurrences, to detailed visual features. By design, our framework utilizes visual grounding models that localize concepts in images, enabling a deeper analysis of visual representations. Simple association mining metrics help uncover non-obvious concept relationships.

(2) We demonstrate a wide range of concrete use-cases for our framework, by applying it to audit models and prompt datasets. In addition to demonstrating the effectiveness of the framework, our analysis unearthed **new findings** that are independently significant. In particular, _we discovered child-sexual abuse material (CSAM)_ in a human-preferences prompt dataset and misaligned classes in a synthetically generated ImageNet dataset. These findings not only demonstrate the utility of our framework but also contribute to the broader discourse on the safety, fairness, and alignment of T2I models.

(3) We introduce an **interactive visualization tool**, based on our framework, for human-in-the-loop auditing of T2I models. Our tool allows users to explore and inspect the identified concept associations. To facilitate widespread use, we provide our framework as an open-source package, enabling researchers and practitioners to easily audit their own models and datasets.

## 2 Related Work

**Biases in T2I models.** There is a body of works that have qualitatively investigated biases in T2I models, focusing on social biases related to gender, race, and other identity attributes. For example, Bianchi et al. (2023) qualitatively demonstrated a range of social biases in T2I models, including biases related to basic traits, social roles, and everyday objects. Similarly, Ungless et al. (2023) manually analyzed images generated by T2I models and found that certain non-cigsender identities were misrepresented, often depicted in stereotyped or sexualized ways. Through several focus groups, Mack et al. (2024) found that T2I models repeatedly presented "reductive archetypes for different disabilities". Qualitative evaluations play a critical role in exposing instances where generative models can be biased. However, given the large space of possible prompts and images, instance-based bias probing alone cannot paint a systematic picture of how T2I models may (mis)behave in application.

A number of works have focused on automating bias detection at scale. For example, in Cho et al. (2023), the authors measured visual reasoning skills and social biases in T2I models by using a combination of automated detectors and human evaluations to assess the representation of different genders, skin tones, and professions. Likewise, Luccioni et al. (2024) utilized Visual Question Answering (VQA) models and clustering-based evaluations to measure correlations between social attributes and identity characteristics. TIBET (Chinchure et al. (2023)) dynamically generated axes of bias and counterfactual prompts based on a single input prompt. However, these works either do not operate on the general concept-level (e.g. only specifically probe for concepts related to social attributes) and/or do not leverage the rich information in the concept co-occurrences, the stability of concepts, nor do they pinpoint and extract specific concepts. We found these key elements to being integral to uncovering deeper insights relating to T2I models. Most closely related to our work is Try Before You Bias (TBYB) (Vice et al. (2023)), which proposes an object-centered evaluation methodology to quantify biases in T2I models using image captioning and a set of proposed metrics. Also like us, CUPID (Zhao et al. (2024)) presents a visualization framework that enables users to discover salient styles of objects and object relationships by leveraging low-dimensional density-based embeddings. Our approach generalizes and builds upon these previous works. While existing methodsfocus primarily on social bias and style relationships, our framework enables a more nuanced audit of model behavior, capturing not only social biases but also the underlying patterns in how models represent and associate visual concepts.

**Important use cases of synthetic data.** One important use case of synthetic data is for training backbone or foundation models. Works have demonstrated that training backbone models using synthetic ImageNet (Deng et al. (2009)) clones can achieve similar performance on specific evaluation benchmarks as compared to real ImageNet dataset (Azizi et al. (2023);He et al. (2022);Saryldiz et al. (2023)). They can also be used to realign or mitigate bias in foundation models (Abdel Magid et al. (2024);Howard et al. (2024)) or evaluate vision-language models (Fraser and Kiritchenko (2024); Smith et al. (2023)). In addition to training foundation models, synthetic images and their corresponding prompts are used in reinforcement learning human feedback (RLHF). Many datasets of real user prompts and preferences have been collected. Examples include RichHF-18K (Liang et al. (2024)), ImageReward (Xu et al. (2024)), and Pick-a-Pic (Kirstain et al. (2023)). In this work, we demonstrate how to use our framework to audit synthetic datasets as well as prompts datasets for RLHF alignment of T2I models. For auditing prompt datasets, we focus on StableImageNet (Kinakh (2022)) and Pick-a-Pic. The latter is used to train the PickScore which is then used as an evaluation metric and to better align T2I models with human preferences.

Concept2Concept: An Intuitive Framework for Characterizing the Conditional Distribution of T2I Models

We propose _Concept2Concept_, a novel framework to provide systematic and interpretable characterizations of the conditional distribution of images generated by a T2I model given a prompt, \(p(\texttt{image}|\texttt{prompt})\). We do so by first extracting high-level concepts from generated images and then characterizing the conditional distribution of these concepts given prompts, \(p(\texttt{concept}|\texttt{prompt})\).

**Obtaining Concept Distributions from T2I Models.** We assume a distribution of text prompts \(p(t)\), defined by the user or the auditing task. We empirically represent \(p(t)\) with \(N\) sampled prompts \(\{t_{i}\}_{i=1}^{N}\) from \(p(t)\):

\[t_{i}\sim p(t),\quad\text{for }i=1,2,\dots,N.\] (1)

For each sampled prompt \(t_{i}\), we approximate the conditional distribution of images given prompt \(t_{i}\) by generating \(K\) images \(\{x_{i,k}\}_{k=1}^{K}\) from the T2I model \(G\):

\[x_{i,k}\sim p_{G}(x_{i,k}|t_{i}),\quad\text{for }k=1,2,\dots,K.\] (2)

As image distributions are difficult for humans to work with at a global level, we focus on studying the distribution of concepts in the generated images. Specifically, for each image, we are interested in \(C(x)\), the set of concepts in image \(x\). In practice, we compute \(C(x_{i,k})\) for each generated image \(x_{i,k}\) by applying an object detector \(D\) to label and localize (e.g., bounding box) the concepts in the image \(C_{i,k}=D(x_{i,k})\). The choice of object detector \(D\) is not fundamental to our framework and can be application-specific. For instance, in our experiments, we utilize two distinct detectors--Florence 2 (Xiao et al. (2023)) and BLIP VQA (Li et al. (2022))--each offering different levels of detection capabilities. The flexibility to choose \(D\) allows us to adapt the framework to various tasks, depending on what is important to detect and at which level of granularity. Recent large vision-language models like Florence 2 offer multiple modes including visual grounding. We note that our use of an object detector \(D\) can introduce uncertainty in the extracted concepts, \(C_{i,k}\) (e.g., due to detection confidence levels or the probabilistic nature of the model). Thus, we consider \(C_{i,k}\) as samples from a distribution \(C_{i,k}\sim p(C|x_{i,k})\). In the case that concepts are extracted deterministically from a given image \(x_{i,k}\), \(p(C|x_{i,k})\) is a delta distribution.

Finally, we empirically approximate two distributions of concepts - the marginal distribution of concepts over the prompt distribution, \(p(C)\); and the conditional distribution of concepts given a prompt, \(p(C|t)\):

\[p(C)=\int_{t}p(C|t)p(t)\,dt,\quad p(C|t)=\int_{x}p(C|x)p_{G}(x|t)\,dx.\] (3)

**Summarizing Concept Distributions.** We further summarize the concepts distributions \(p(C)\) and \(p(C|t)\) we obtain from the T2I model to enable end-users in exploring and discovering associations between concepts in the prompt and concepts in the generated images. Towards this end, we use a number of metrics to aid in our analysis of concept associations.

_Concept Frequency \(P(c)\)._ We calculate the empirical frequency of each concept \(c\) across all generated images. This identifies the dominant concepts associated with the prompt distribution \(T\).

_Concept Stability._ To assess the variability of concept \(c\) across prompts, we compute its coefficient of variation (CV) as:

\[CV(c)=\frac{\sigma_{c}}{P(c)},\quad\sigma_{c}=\sqrt{\frac{1}{N} \sum\limits_{i=1}^{N}{(P(c\mid t_{i})-P(c))}^{2}}.\] (4)

We set a threshold \(\tau\) to focus on concepts that occur with sufficient frequency: \(\mathcal{C}_{\tau}=\{c\in\mathcal{C}\mid P(c)>\tau\}\). Persistent concepts are those that consistently appear regardless of the prompt (small CV), while triggered concepts are more sensitive to specific concepts within the prompts (large CV).

_Concept Co-Occurrence._ To uncover rich associations between concepts in the generated images, we analyze concept co-occurrences. For each pair of concepts \((c,c^{\prime})\), we compute the co-occurrence probability:

\[P(c,c^{\prime})=\frac{\sum\limits_{i=1}^{N}{\sum\limits_{k=1}^{ K}\mathbb{I}[c,c^{\prime}\in C_{i,k}]}}{N\times K}.\] (5)

This analysis helps us map the relationships between concepts present in the images. We refer the reader to the appendix for additional details.

**Choosing Task-Relevant Prompt Distributions \(p(t)\).** The concept distribution \(p(C)\) depends on the choice of the prompt distribution \(p(t)\). Generally, the choice of \(p(t)\) should be informed by the task, e.g. auditing models for social biases. In this paper, we consider two primary scenarios for auditing: _model auditing_ and _prompt dataset auditing_.

_Model Auditing._ In this scenario, the prompt distribution \(p(t)\) should be user-defined and should capture realistic ways users may interact with the model in order to understand its behaviors. Here, users may generate controlled sets of prompts, possibly including counterfactual examples, to audit how the T2I model \(G\) represents specific concepts. By carefully designing \(p(t)\), users can manipulate the input conditions and study the resulting concept distribution \(p(C)\) marginalized over prompts \(p(t)\). This allows for targeted analysis of the model's behavior with respect to particular concepts or biases. We provide several experiments in section 4.

_Prompt Dataset Auditing._ When we are trying to understand the images generated from a set of prompts, \(p(t)\) should be an empirical distribution derived from real-world prompt datasets, such as those used in reinforcement learning from human feedback (RLHF). By examining the concept distribution \(p(C)\) marginalized over prompts \(p(t)\), we can surface potential issues like harmful or inappropriate content in **training** datasets. We provide several experiments in section 5.

## 4 Application 1: Auditing the Model

### Case Study 1: Replicating Bias Probing Results from Literature

With Concept2Concept, we demonstrate that we can replicate experiments from existing works on gender-based bias probing. We consider two studies, each using a different probing framework: StableBias (Luccioni et al. (2024)) and Try Before You Bias (TBYB) (Vice et al. (2023)). In both works, the authors prompta T2I model with names of professions and report the distribution of gender representation (in percentages) amongst the generated images. Our findings are summarized in Table 1. Consistent with the two existing studies, we found that the concept [10] is underrepresented across most professions, with only about 30% of the images depicting the concept [10], while approximately 70% of the images depicted the concept [10]. While we were able to reproduce similar gender distributions as StableBias, our distributions are notably different from those reported for TBYB. We provide a discussion for this discrepancy in A.6.

### Case Study 2: Scaling Up Qualitative Studies on Disability Representation

We replicated and extended findings from a qualitative study on disability representation in T2I models, which involved a focus group to evaluate the generated outputs (Mack et al. (2024)). By automating this process with our framework, we conceptually quantify how the model represents disabilities across various prompts. Concretely, \(\mathcal{T}_{\text{disability}}=\{t_{i}=\text{``A person with [value] }\}\) where [value] \(\in\) {a disability, bipolar disorder, a chronic illness, cerebral palsy, a limb difference, hearing loss}. Figure 7 (top left) shows that for the prompt "a person with a disability," **nearly 100% of the generated images depicted [10], despite not being explicitly stated in the prompt.** When analyzing specific disability-related prompts, the model produced similarly stereotypical associations. For instance, the prompt "cereb palsy" primarily generated images of [10] and [10], while "a limb difference" shown in Figure 7 (bottom left) resulted in images with the concepts [10] and [10]; individuals in the images are typically dressed in shorts to emphasize the disability. Unexpectedly, [10], co-occurred with [10]. We visualize this in Figure 7 (bottom right) and find that the model produces [10]-like sticks, perhaps to represent crutches. In the case of "chronic illness," the model often depicted people in [10], [10], with their [10]. Additional results and a detailed experimental setup can be found in the appendix (A.8).

## 5 Application 2: Auditing Prompt Datasets

### Case Study 3: Detecting Unexpected Issues in Pick-A-Pic

_Warning: This section contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers._ The Pick-a-Pic dataset (Kirstain et al. (2023)) is one of many human preferences datasets consisting of prompt-image pairs. Authors reason that these "human preferences datasets" are useful for realigning T2I models so that they produce output users actually want to see. Kirstain et al. (2023) train the PickScore on the Pick-a-Pic training set to learn their collected human preferences. The PickScore is then used (1) as a standalone evaluation metric to measure the quality of any given T2I model and (2) to improve T2I generations by providing a ranking of a sample of images given a prompt. It is clear that these two use cases are incredibly safety-critical. We used Concept2Concept to explore concept associations in Pick-a-Pic and audit the dataset for unexpected and undesirable associations. Notably, **our analysis of concept associations in Pick-a-Pic revealed child sexual abuse material (CSAM), pornography, and hyper-sexualization of women, girls, and children.**

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline
**Model** & \multicolumn{2}{c|}{**Concept Detected**} & \multicolumn{2}{c}{**U.S. Labor Bureau**} \\ \cline{2-5}  & \% woman & \% man & \% woman & \% man \\ \hline StableBias (Luccioni et al. (2024)) & 31.10 \% & 68.90\% & \multirow{3}{*}{47.03\%} & \multirow{3}{*}{52.97\%} \\ \cline{1-1} Ours & 28.41 \% & 71.59\% & & \\ \cline{1-1} TBYB (Vice et al. (2023)) & 31.64\% & 68.36\% & & \\ \cline{1-1} Ours & 19.56 \% & 80.44 \% & & \\ \hline \end{tabular}
\end{table}
Table 1: The average percentage of detections of [10] and [10] generated by a concept detector in our framework for the StableBias (Luccioni et al. (2024)) and TBYB (Vice et al. (2023)) case studies. Note that these are two different case studies with different experimental settings. U.S. Bureau of Labor Statistics.

We draw 10 random samples of size 1K each from the training split of the Pick-a-Pic dataset 1. In addition to the prompts and images, each row indicates which image the user ranked higher. When sampling, we save

Figure 1: Co-occurrences of concepts with the detected concepts \(\ttypoint\) and \(\ttypoint\) in 10 random samples of the Pick-A-Pic DatasetKirstain et al. (2023). Most frequent concepts overall can be found in A.9.

Figure 2: Prompts in the Pick-a-Pic dataset that trigger the \(\ttypoint\), \(\ttypoint\), and \(\ttypoint\), concept associations. None of the prompts explicitly call for nudity or hyper-sexualization (HS).

[MISSING_PAGE_FAIL:7]

Similarly, for the class redbone, the intended ImageNet class refers to "A variety or breed of American bound with a predominantly red coat..."3 However, the model instead generated images of human individuals. Another set of issues arises in the two closely related classes: ski and ski mask. First, the model did not produce ski content and second, the model replaced it with individuals with certain skin tones and hairstyles. The issue is thus two fold; one of prompt adherence and one of fairness. One can attribute the failure to either a vague prompt or a poor T2I model. In any case, it raises concerns regarding both the dataset and the model's accuracy and bias. It is also important to note that while this exact dataset was not published in a specific paper, the recipe for generation is replicated in other works as a comparison point (Bansal and Grover (2023); Sarvilduz et al. (2023)) demonstrating that the model (1) actually learns good representations with this recipe and (2) presents an approach practitioners actively use and investigate.

## 6 Interactive Tool

Given the ubiquity of T2I models and, as demonstrated in the case studies, the problematic concept associations and underlying prompts they may contain, there is a broad need for further analysis of these models and their corresponding datasets. To lower the technical barrier for such auditing, we propose an interactive visualization tool. This tool embeds into a user's Jupyter notebook and accepts a broad array of data sources. A user can investigate specific concepts, their stability, and co-occurrence with other concepts (Figure 23). Additionally, users may search for specific concepts to identify the prompts used to generate the concept, the distribution of these prompts, and, localize how the concept is depicted in different images (Figure 24).

## 7 Conclusion

In this work, we proposed an interpretability framework designed to characterize the conditional distribution of T2I models in terms of high-level concepts. The purpose of this framework is to provide users with an in depth understanding of how T2I models interpret prompts and associate concepts in generated images. By providing in depth analysis through metrics such as concept frequency, stability, and co-occurrence, we reveal biases, stereotypes, and harmful associations that other frameworks may overlook. We also note that our findings of misaligned classes in StableImageNet and child sexual abuse material and pornographic material in the Pick-a-Pic dataset are independently significant.

Figure 3: Sample of misaligned synthetic ImageNet images detected through the conceptual characterization of conditional distributions through our framework. The first 9 images from each class. Clear misalignment. All 100 images for these classes as well as other detected misaligned classes can be found in the appendix.

## References

* Magid et al. (2024) Salma Abdel Magid, Jui-Hsien Wang, Kushal Kafle, and Hanspeter Pfister. They're all doctors: Synthesizing diverse counterfactuals to mitigate associative bias. _arXiv preprint arXiv:2406.11331_, 2024.
* Azizi et al. (2023) Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Synthetic data from diffusion models improves imagenet classification. _arXiv preprint arXiv:2304.08466_, 2023.
* Bansal and Grover (2023) Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via generated datasets. _International Conference on Learning Representations Workshop on Trustworthy and Reliable Large-Scale ML Models_, 2023.
* Bianchi et al. (2023) Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. In _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency_, pp. 1493-1504, 2023.
* Birhane et al. (2024) Abeba Birhane, Sanghyun Han, Vishnu Boddeti, Sasha Luccioni, et al. Into the laion's den: Investigating hate in multimodal datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* Carlini et al. (2023) Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In _32nd USENIX Security Symposium (USENIX Security 23)_, pp. 5253-5270, 2023.
* Chen et al. (2020) Zhi Chen, Yijie Bei, and Cynthia Rudin. Concept whitening for interpretable image recognition. _Nature Machine Intelligence_, 2(12):772-782, 2020.
* Chinchure et al. (2023) Aditya Chinchure, Pushkar Shukla, Gaurav Bhatt, Kiri Salij, Kartik Hosanagar, Leonid Sigal, and Matthew Turk. Tibet: Identifying and evaluating biases in text-to-image generative models. _arXiv preprint arXiv:2312.01261_, 2023.
* Cho et al. (2023) Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 3043-3054, 2023.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pp. 248-255. Ieee, 2009.
* Fraser and Kiritchenko (2024) Kathleen C Fraser and Svetlana Kiritchenko. Examining gender and racial bias in large vision-language models using a novel dataset of parallel images. _arXiv preprint arXiv:2402.05779_, 2024.
* Ghorbani et al. (2019) Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. _Advances in neural information processing systems_, 32, 2019.
* He et al. (2022) Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? _arXiv preprint arXiv:2210.07574_, 2022.
* Howard et al. (2024) Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Anahita Bhiwandiwalla, and Vasudev Lal. Socialcounterfactuals: Probing and mitigating intersectional social biases in vision-language models with counterfactual examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 11975-11985, 2024.

* Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In _International conference on machine learning_, pp. 2668-2677. PMLR, 2018.
* Kinakh (2022) V. Kinakh. Stable imagenet-1k dataset. https://www.kaggle.com/datasets/vitaliykinakh/stable-imagenet1k, 2022. Accessed: 2024-09-30.
* Kirstain et al. (2023) Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. _Advances in Neural Information Processing Systems_, 36:36652-36663, 2023.
* Koh et al. (2020) Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In _International conference on machine learning_, pp. 5338-5348. PMLR, 2020.
* Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International conference on machine learning_, pp. 12888-12900. PMLR, 2022.
* Liang et al. (2024) Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 19401-19411, 2024.
* Luccioni et al. (2024) Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Evaluating societal representations in diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Mack et al. (2024) Kelly Avery Mack, Rida Qadri, Remi Denton, Shaun K Kane, and Cynthia L Bennett. They only care to show us the wheelchair": disability representation in text-to-image ai models. In _Proceedings of the CHI Conference on Human Factors in Computing Systems_, pp. 1-23, 2024.
* Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* Sarvildiz et al. (2023) Mert Bulent Sarvildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic imagenet clones. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 8011-8021, 2023.
* Smith et al. (2023) Brandon Smith, Miguel Farinha, Siobhan Mackenzie Hall, Hannah Rose Kirk, Aleksandar Shtedritski, and Max Bain. Balancing the picture: Debiasing vision-language datasets with synthetic contrast sets. _arXiv preprint arXiv:2305.15407_, 2023.
* Unpless et al. (2023) Eddie L Unpless, Bjorn Ross, and Anne Lauscher. Stereotypes and smut: The (mis) representation of non-cisgender identities by text-to-image models. _arXiv preprint arXiv:2305.17072_, 2023.
* Vice et al. (2023) Jordan Vice, Naveed Akhtar, Richard Hartley, and Ajmal Mian. Quantifying bias in text-to-image generative models. _arXiv preprint arXiv:2312.13053_, 2023.
* Xiao et al. (2023) Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. _arXiv preprint arXiv:2311.06242_, 2023.

* Xu et al. (2024) Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhao et al. (2024) Yayan Zhao, Mingwei Li, and Matthew Berger. Cupid: Contextual understanding of prompt-conditioned image distributions. In _Computer Graphics Forum_, pp. e15086. Wiley Online Library, 2024.
* Zhou et al. (2018) Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba. Interpretable basis decomposition for visual explanation. In _Proceedings of the European Conference on Computer Vision (ECCV)_, September 2018.

## Appendix A Appendix

An overview of our method can be found in Figure 4.

### Case Study: Toy Examples

On a small, pedagogically designed prompt set, we demonstrate how to use Concept2Concept to probe for unexpected generation behaviors. We design a prompt set which varies along a social attribute of interest, age, and a second prompt set which adds an axis of variation along semantically similar words

Figure 4: _Concept2Concept_ enables users to systematically analyze the conditional distributions by investigating concept frequency and stability, co-occurrences, and detailed visual features using simple association mining metrics. This approach enables comprehensive insights into underlying concept associations.

Figure 5: Top concepts detected by our framework. Concepts are curated to highlight the effectiveness of the framework for user-defined prompt distributions in Section A.1(e.g. jogging vs running). Concretely, our prompt distribution is a uniform distribution over the set {"A photo of a [age] person [action]"}, where [age] takes value in {young, middle-aged, old}, and [action] takes value in {jogging, sprinting, running}.

**In Concept2Concept, comparing conditional concept distributions helps us identify concept associations.** Figure 5 shows the conditional concept distributions \(p(C|t)\) we obtain through Concept2Concept. By contrasting these distributions, we find that the concept [age] is largely associated with the concept [count] (the concept of "woman" occurs in roughly 60% of the generations). Conversely, [count] is associated with [count] in about 80% of the generations. We are also able to discover that different attires are associated to the concept of [logang] and [count], respectively (see Figure 6).

**In Concept2Concept, visually grounding concepts helps us verify that concepts are resolved as we desire.** Figure 6 provides a small example of concept co-occurrences. Even seemingly concrete concepts can be visually resolved in diverse ways, in Concept2Concept, we visually ground each concept (see Figure 6). The localization of our framework is highly precise, even for small objects like [discuss], which occupy only a small fraction of the entire image. Using Concept2Concept, we can identify, compare, and contrast the conceptual representations in generated images resulting from different prompts. This enables us to uncover unexpected concept associations (e.g. [count] and [count] vs. [count] and [logang]). Additional results, including concept stability are included in A.6.

### Case Study 3: Disability Representation

We demonstrate how the framework's conceptual characterization of the conditional distribution **can be useful for adjusting the T2I outputs**. Figure 7 (top right) shows how we can use the concepts and apply negative prompting with concepts we wish to attenuate and/or amplify. Suppose we want to exclude the concept [discuss] and emphasize [discuss] and [count] in the images. Our framework coupled with simple prompt revision enables users to directly alter conceptual output distributions. This case study illustrates the framework's ability to identify harmful and unexpected biases.

### Ethics Statement

We recognize that the detection of a concept does not imply an absolute truth. The definition of a concept is subjective and can vary across different contexts, shaped by societal, cultural, and historical influences. Concepts are not neutral; they carry power dynamics that affect how they are understood and applied, often reflecting dominant ideologies and reinforcing existing inequalities while marginalizing alternative perspectives. For instance, when the framework detects [count] or [discuss] it is important to recognize that these

Figure 6: Examples of concepts as they are extracted from our framework along with sample co-occurrences.

attributes cannot be inferred from visual cues alone--especially in the context of synthetic images. Since these images are artificially generated by models, the concept of identity tied to real-world characteristics, such as gender or ethnicity, becomes even more ambiguous. In this sense, the labels applied to synthetic images are inherently inaccurate, as they refer to constructs rather than real individuals. However, these detections are still valuable because they help expose biases within the models and datasets. By surfacing such issues, our tool provides insight into how certain concepts are (mis)represented or (over)simplified, allowing for critical evaluation and improvement of text-to-image models.

Similarly, other design choices in our work reflect inherent biases. For example, our use of U.S. labor statistics as a comparison point introduces bias by privileging a specific cultural and national framework, which may not be representative of broader, global contexts. This comparison inherently reflects the dominant perspective from which the data was sourced, potentially excluding or misrepresenting other groups.

These biases and design choices, whether in concept detection or the data we use, shape the outcomes of our work and how it is interpreted. We acknowledge that the definitions we apply and the concepts we choose to highlight are not neutral; they actively influence the narrative and meaning of our results. Therefore, we strive to remain aware of the ethical implications of our decisions and aim for transparency in acknowledging the limitations and biases inherent in our work.

### Limitations

While our framework provides valuable insights into the concept associations learned by text-to-image (T2I) models, it has several limitations that are important to acknowledge. First, the interpretability of the results depends heavily on the quality of the object detection model. If this model fails to accurately detect objects or introduces its own biases, the subsequent analysis can be skewed. Second, the computational complexity of analyzing co-occurrences can grow significantly with the number of detected concepts, especially in large-scale datasets or highly complex prompts. Another important direction is to explore active mitigation

Figure 7: Concept distributions for two examples on disability representation. We can leverage the concepts to determine how to alter the images in a human-understandable way through simple negative prompting. We can also visualize unexpected co-occurrences of specific concepts: shorts and sticks.

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

### Interactive Tool

Figures 24 and 23 show screenshots of the Concept2Concept interactive tool.

Figure 12: Harmful prompts in the Pick-a-Pic dataset.

## Appendix A

\begin{table}
\begin{tabular}{c|c}
**Hyperparameter** & **Value** \\ \hline Object Detector & Florence 2 \\ \hline Object Detector Mode & caption + grounding \\ \hline Text-to-Image Model & Stable Diffusion v1.4 \\ \hline T2I Model Hyperparameters & inference steps= 50; guidance scale= 7.5 \\ \hline Number of Images Per Prompt & 100 \\ \hline Prompt Distribution & a photo of class, realistic, high quality \\ \hline Image Size & \(512\times 512\) \\ \hline \end{tabular}
\end{table}
Table 7: Case study StableImageNet: hyperparameters and their corresponding values

Figure 13: The top detected concepts over 10 random samples of size 1k drawn from the Pick-a-Pic dataset.

Figure 14

Figure 14

Figure 15

Figure 15

Figure 16

Figure 16

Figure 16

Figure 17

Figure 17

Figure 18:

Figure 19:

Figure 20:

Figure 21:

Figure 22

Figure 23: Users can interactively inspect concept metrics, such as the persistence and fickleness scores, their stability, and co-occurrence with other concepts. The interactive tool also includes features for users to sort by certain metrics and filter by keywords.

Figure 24: The interactive tool also includes a concept inspection tab that allows users to search for concepts of interest. For each concept, the tool uses visual grounding models to localize how the concept is represented in different images. Localized concepts are displayed as thumbnails. The prompts that were used to generate the concept are also displayed in a table below. Users can search for and compare two concepts in this panel.