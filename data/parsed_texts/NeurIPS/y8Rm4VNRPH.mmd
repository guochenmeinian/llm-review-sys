# Parallelizing Linear Transformers with the Delta Rule over Sequence Length

 Songlin Yang\({}^{\diamond}\)  Bailin Wang\({}^{\diamond}\)  Yu Zhang\({}^{\dagger}\)  Yikang Shen\({}^{\ddagger}\)  Yoon Kim\({}^{\diamond}\)

\({}^{\diamond}\)Massachusetts Institute of Technology \({}^{\dagger}\)Soochow University \({}^{\ddagger}\)MIT-IBM Watson AI Lab

yangsl66@mit.edu

###### Abstract

Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule [2] have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices [11]. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba [31] and GLA [124] in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.

## 1 Introduction

The attention mechanism [8, 116] has been shown to be an important primitive for accurate sequence modeling. Attention is moreover efficient during training as it is rich in matrix multiplications and can thus take advantage of highly parallel processing capabilities and specialized accelerators on modern GPUs. However, the complexity of attention is quadratic in sequence length, and hence it is a fundamentally expensive primitive. And while recent techniques have made it possible to scale attention to longer sequences through hardware-aware restructuring of the intermediate computations [20, 18, 59, 14], these methods still require storing the key/value vectors of previous elements, and this "KV cache" (whose size grows linearly) can be unwieldy to manage for long sequences.

Linear attention transformers [48] replace the exponential kernel in softmax attention with a dot-product over (possibly transformed) key and query vectors. This makes it possible to formulate linear attention as a linear RNN with matrix-valued hidden states, thus obviating the need for a KV cache and enabling constant-memory inference. While initial variants of linear attention generally underperformed softmax attention on language modeling, gated variants of linear attention which incorporate a data-dependent gating factor have recently been shown to be competitive against strong transformer baselines [124, 92, 9, 79]. These gated linear transformers, along with time-varying state space models such as Mamba [31, 19] (which can be reparameterized as a gated linear transformer [124]), have been suggested as a potential alternative to ordinary transformers. However, despitethe competitive language modeling performance, these models have been shown to underperform transformers on recall-intensive tasks [6; 7], which is important for many practical downstream tasks of interest (e.g., in retrieval-augmented generation [53]).

To enhance associative recall over long contexts, Schlag et al. [101] propose DeltaNet, a variant of a linear transformer which uses a delta rule-like update [121] to retrieve and update a value vector that is associated with the current key. DeltaNet was found to be effective on synthetic tasks and small scale language modeling/machine translation. However, the original work used a sequential algorithm that did not parallelize across sequence length, thus resulting in hardware-inefficient training, and it has not been clear how to scale DeltaNet to larger models and datasets.

This work describes a hardware-efficient training algorithm for DeltaNets which parallelizes the forward/backward passes across sequence length. We reparameterize the DeltaNet as a matrix-valued RNN whose recurrence is given by a generalized Householder transformation. This reparameterization enables the use of the compact WY representation [11] for products of Householder matrices, eliminating the need to materialize the hidden states of matrix size at each time step during parallel training, which would otherwise result in high I/O costs. The memory-efficient representation makes it possible to straightforwardly extend the chunkwise parallel strategy for training linear attention models [34; 108; 124] to the DeltaNet case. We scale DeltaNets to moderate-scale language modeling benchmarks (1.3B models trained on 100B tokens), where DeltaNet is found to obtain better language modeling and zero-shot downstream task performance than strong linear recurrent models such as Mamba [31] and GLA [124]. For in-context retrieval and learning evaluation, we evaluate DeltaNet on synthetic and real benchmarks [4; 2; 84; 6], where it is again found to perform well against linear recurrent baselines. Finally, we experiment with a hybrid approach where we combine DeltaNet layers with sliding attention layers or global attention layers, and find that these hybrid models can improve upon ordinary transformers, as well as the pure DeltaNet transformer.

## 2 Background

### Linear Transformer: Transformers with Linear Attention

Given a sequence of \(d\)-dimensional input vectors \(\bm{x}_{1},\ldots,\bm{x}_{L}\), transformers use the softmax attention mechanism to attend over the entire past,

\[\bm{q}_{t},\ \bm{k}_{t},\ \bm{v}_{t}=\bm{W}_{Q}\bm{x}_{t},\bm{W}_{K}\bm{x }_{t},\bm{W}_{V}\bm{x}_{t}, \bm{o}_{t}=\sum_{i=1}^{t}\frac{\exp(\bm{k}_{i}^{\top}\bm{q}_{t})}{ \sum_{j=1}^{t}\exp(\bm{k}_{j}^{\top}\bm{q}_{t})}\bm{v}_{i},\]

where \(\bm{W}_{Q},\bm{W}_{K},\bm{W}_{V}\in\mathbb{R}^{d\times d},\bm{q}_{t},\bm{k}_{t },\bm{v}_{t},\bm{o}_{t}\in\mathbb{R}^{d}\). (Here we assume a single attention head for simplicity). Linear attention [48] replaces the exponential kernel \(\exp(\bm{k}_{i}^{\top}\bm{q}_{t})\) with the dot-product \(\phi(\bm{k}_{i})^{\top}\phi(\bm{q}_{t})\) where \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\) is a feature map. This makes it possible to rearrange computations to represent linear attention as a linear RNN with matrix-valued hidden states,

\[\bm{o}_{t}=\sum_{i=1}^{t}\frac{\phi(\bm{k}_{i})^{\top}\phi(\bm{q}_{t})}{\sum_ {j=1}^{t}\phi(\bm{k}_{j})^{\top}\phi(\bm{q}_{t})}\bm{v}_{i}=\frac{\left(\sum_{ i=1}^{t}\bm{v}_{i}\phi(\bm{k}_{i})^{\top}\right)\phi(\bm{q}_{t})}{\left(\sum_{j=1}^{t} \phi(\bm{k}_{j})^{\top}\right)\phi(\bm{q}_{t})}=\frac{\mathbf{S}_{t}\phi(\bm{ q}_{t})}{\mathbf{z}_{t}^{\top}\phi(\bm{q}_{t})},\]

where \(\mathbf{S}_{t}=\sum_{i=1}^{t}\bm{v}_{i}\phi(\bm{k}_{i})^{\top}\in\mathbb{R}^{d \times n}\) and \(\mathbf{z}_{t}=\sum_{i=1}^{t}\phi(\bm{k}_{i})\in\mathbb{R}^{n}\). If we allow \(n\) to go to infinity, linear attention can use feature maps associated with polynomial kernels to compute a polynomial approximation to the exponential kernel as a dot product, and can thus approximate softmax attention arbitrarily well [6]. The denominator \(\mathbf{z}_{t}^{\top}\phi(\bm{q}_{t})\in\mathbb{R}\) can result in numerical instabilities [86] and is removed in recent works [101; 63]. It is also common to use the identity mapping for \(\phi\)[63; 108], which results in the following simplified linear transformer: \(\mathbf{S}_{t}=\mathbf{S}_{t-1}+\bm{v}_{t}\bm{k}_{t}^{\top}\), \(\bm{o}_{t}=\mathbf{S}_{t}\bm{q}_{t}\).

Efficient training.Let \(\mathbf{Q},\mathbf{K},\mathbf{V}\in\mathbb{R}^{L\times d}\) be the stacked query, key, value vectors, e.g., \(\mathbf{Q}_{i}=\bm{q}_{i}\). We can then compute the output \(\mathbf{O}\in\mathbb{R}^{L\times d}\) in parallel via \(\mathbf{O}=\left(\mathbf{Q}\mathbf{K}^{\top}\odot\mathbf{M}_{L}\right)\mathbf{V}\), where \(\mathbf{M}_{L}\in\mathbb{R}^{L\times L}\) is the causal mask. This fully "parallel form" and the above "recurrent form" have different FLOPs and parallelization tradeoffs. The parallel form takes \(O(L^{2}d+Ld^{2})\) and thus requires more FLOPs than the recurrent form, which takes \(O(Ld^{2})\). However, the parallel form is often much faster in practice for moderate-length sequences as it can be done in \(O(1)\) steps. This sequence-level parallelellism also enables high GPU occupancy. The recurrent form requires fewerFLOPs but cannot be parallelized across sequence length1 and the elementwise operations involved in recurrence moreover cannot make use of specialized matmul accelerators (e.g., tensor cores).

Footnote 1: It is possible in theory to use parallel scan [13] to parallelize the recurrent form, which would enable the computations to be performed in \(O(\log L)\) steps and \(O(Ld^{2})\) FLOPs. However, this approach requires materializing the 2D hidden state for each time step, which would incur significant memory I/O cost unless the state size is small enough such that materialization can happen in faster memory (i.e., as in Mamba [31]).

Chunkwise parallel form.The chunkwise parallel form [108, 124, 34] strikes a balance between the parallel and recurrent forms, allowing for fewer FLOPs than the parallel form and more sequence-level parallelism than the recurrent form. Concretely, suppose the query/key/value vectors are split into \(\frac{L}{C}\) chunks where each chunk is of length \(C\). Let \(\mathbf{Q}_{[t]}\in\mathbb{R}^{C\times d}\) be all the query vectors for chunk \(t\), and let \(\bm{q}_{[t]}^{i}=\bm{q}_{tC+i}\) be the \(i\)-th query vector within the \(t\)'th chunk; the key/value chunks are defined similarly. Note that \(t\in[0,L/C)\), \(i\in[1,C]\). The state matrices are also re-indexed such that \(\mathbf{S}_{[t]}^{i}=\mathbf{S}_{tC+i}\), and we additionally define \(\mathbf{S}_{[t]}^{0}=\mathbf{S}_{[t-1]}^{C}\), i.e., the initial state of a chunk is the last state of the previous chunk. We can then obtain the following identity for the hidden state and output vector for the \(r\)-th element within the \(t\)-th chunk,

\[\mathbf{S}_{[t]}^{r}=\mathbf{S}_{[t]}^{0}+\sum_{i=1}^{r}\bm{v}_{[t]}^{i}\bm{k }_{[t]}^{\textsc{r}},\qquad\bm{o}_{[t]}^{r}=\mathbf{S}_{[t]}^{0}\bm{q}_{[t]}^{ r}+\sum_{i=1}^{r}\bm{v}_{[t]}^{i}\left(\bm{k}_{[t]}^{i\textsc{r}}\bm{q}_{[t]}^{r} \right).\]

By further rewriting the intra-chunk computation based on the parallel form, we obtain following,

\[\mathbf{S}_{[t+1]} =\mathbf{S}_{[t]}+\mathbf{V}_{[t]}^{\textsc{r}}\mathbf{K}_{[t]} \in\mathbb{R}^{d\times d},\] (1) \[\mathbf{O}_{[t]} =\mathbf{Q}_{[t]}\mathbf{S}_{[t]}^{\textsc{r}}+\left(\mathbf{Q}_{ [t]}\mathbf{K}_{[t]}^{\textsc{r}}\odot\mathbf{M}_{C}\right)\mathbf{V}_{[t]} \in\mathbb{R}^{C\times d}\] (2)

where we let \(\mathbf{S}_{[t]}=\mathbf{S}_{[t]}^{0}\) to reduce notational clutter. With this "chunkwise parallel form", information is propagated chunk-to-chunk through \(\mathbf{S}_{[t]}\), and the intra-chunk states \(\mathbf{S}_{[t]}^{i}\) for \(i\in[1,C]\) need not be materialized, thus saving memory.

The complexity of the chunkwise parallel form is \(O(LCd+Ld^{2})\), and the number of steps (without chunk-level parallel scan) is \(O(\frac{L}{C})\). Hence, \(C=L\) recovers the fully parallel form and \(C=1\) recovers the recurrent form. The chunkwise parallel form allows us to interpolate between the two forms, in essence trading off the number of sequential computations against sequence-level parallelism. In practice \(C\) is set to a small constant (usually 64 or 128), allowing for subquadratic training. This chunkwise form enables practical speed-ups against parallel-form-only softmax attention even on moderate-length sequences, as demonstrated by FlashLinearAttention [124, 123]

### DeltaNet: Linear Transformers with the Delta Update Rule

The above linear transformer employs a simple linear recurrence: \(\mathbf{S}_{t}=\mathbf{S}_{t-1}+\bm{v}_{t}\bm{k}_{t}^{\textsc{r}}\). This can be seen as additively updating the memory \(\mathbf{S}_{t-1}\) with new key-value associations at each time step. However, a purely additive update rule makes it difficult to deallocate past key-value associations, eventually leading to key "collisions" when \(L>d\), as pointed out by Schlag et al. [101]. A model should ideally learn to remove less important key-value associations to make room for new ones, and this removal should depend on the interaction between the new key and the memory content.

From a fast weight programming [102] perspective, the recurrent hidden state of linear attention is the fast weight mapping the input \(\bm{q}_{t}\) to output \(\bm{o}_{t}\) with a Hessian-like update rule, which has limited memory capacity [68]. DeltaNet, on the other hand, uses the delta update rule or the Widrow-Hoff learning rule [121] for fast weight update: \(\mathbf{S}_{t}=\mathbf{S}_{t-1}-\beta_{t}(\mathbf{S}_{t-1}\bm{k}_{t}-\bm{v}_{t })\bm{k}_{t}^{\top}\), where \(\beta_{t}\) is the learning rate, \(\mathbf{S}_{t-1}\bm{k}_{t}\) represents the current prediction, \(\bm{v}_{t}\) is the target value. The method derives its name from the core principle of updating weights based on the "delta" (difference) between the prediction \(\mathbf{S}_{t-1}\bm{k}_{t}\) and the target \(\bm{v}_{t}\), which has been shown better memory capacity [28, 85, 56, 101]. This process can also be regarded as optimizing an online regression loss using a single step of SGD,2

Footnote 2: This perspective was discussed as early as Widrow et al. [121], which later became known as the Least Mean Squares (LMS) algorithm and has found widespread applications in signal processing [97].

\[\mathcal{L}_{t}(\mathbf{S})=\frac{1}{2}\|\mathbf{S}\bm{k}_{t}-\bm{v}_{t}\|^{2},\quad\mathbf{S}_{t}=\mathbf{S}_{t-1}-\beta_{t}\nabla_{\mathbf{S}_{t-1}} \mathcal{L}_{t}(\mathbf{S}_{t-1})=\mathbf{S}_{t-1}-\beta_{t}(\mathbf{S}_{t-1} \bm{k}_{t}-\bm{v}_{t})\bm{k}_{t}^{\top},\]

which has been discussed in several recent works [110, 58, 125, 10]. In contrast, linear transformerscan be regarded as optimizing an online linear (negative inner-product) loss \(\mathcal{L}_{t}=-\langle\mathbf{S}\bm{k}_{t},\bm{v}_{t}\rangle\).3

Footnote 3: While quadratic loss (used in DeltaNet) provides gradients that scale with error magnitude, offering stronger corrections when predictions are far from the target, the gradient of linear loss remains constant regardless of prediction error. This gradient behavior provides an intuitive explanation for why linear transformers underperform DeltaNet in in-context retrieval tasks.

An alternative interpretation for DeltaNet is from the perspective of key-value retrieval. It first retrieves the old value using the current key, \(\bm{v}_{t}^{\text{old}}=\mathbf{S}_{t-1}\bm{k}_{t}\). It then obtains a new value \(\bm{v}_{t}^{\text{new}}\) by interpolating between the old value and the current value \(\bm{v}_{t}\), which replaces \(\bm{v}_{t}^{\text{old}}\) in the memory:

\[\bm{v}_{t}^{\text{new}} =\beta_{t}\bm{v}_{t}+\left(1-\beta_{t}\right)\bm{v}_{t}^{\text{ old}}, \mathbf{S}_{t} =\mathbf{S}_{t-1}\underbrace{-\bm{v}_{t}^{\text{old}}\bm{k}_{t}^{ \text{r}}}_{\text{remove}}\underbrace{+\bm{v}_{t}^{\text{new}}\bm{k}_{t}^{ \text{r}}}_{\text{write}}\]

Here \(\beta_{t}=\sigma(\mathbf{W}_{\beta}\bm{x}_{t})\in(0,1)\) is a soft "writing strength": when \(\beta_{t}=1\), the old value is completely removed and \(\bm{v}_{t}^{\text{new}}=\bm{v}_{t}\); when \(\beta_{t}=0\), the memory remains unmodified and we have \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\). The output computation is the same as vanilla linear attention, i.e., \(\bm{o}_{t}=\mathbf{S}_{t}\bm{q}_{t}\). The complexity of this recurrent form is the same as that of vanilla linear attention, i.e., \(\mathcal{O}(Ld^{2})\).

Schlag et al. [101] demonstrate that DeltaNet outperforms ordinary linear transformers on small-scale language modeling and synthetic in-context retrieval tasks. However, their training algorithm, an extension of the memory-efficient recurrent implementation for linear Transformers [48, SS3.3.1], is strictly sequential and thus hardware inefficient, as discussed in [124, SS3.2], motivating us to derive an equivalent chunkwise algorithm to train DeltaNet at scale, as introduced below.

## 3 Parallelizing DeltaNet Across the Sequence Dimension

### A Memory-efficient Reparameterization

We first observe that \(\mathbf{S}_{t}\) admits a purely additive representation of the form \(\mathbf{S}_{t}=\sum_{i=1}^{t}\bm{u}_{t}\bm{k}_{i}^{\text{r}}\) for \(\bm{u}_{i},\bm{k}_{i}\in\mathbb{R}^{d}\), since we can simply set \(\bm{u}_{i}=\bm{v}_{i}^{\text{new}}-\bm{v}_{i}^{\text{old}}=\beta_{i}(\bm{v}_{i }-\bm{v}_{i}^{\text{old}})\). Recall from SS2.1 that simple linear attention has the form \(\mathbf{S}_{t}=\sum_{i=1}^{t}\bm{v}_{i}\bm{k}_{i}^{\text{r}}\). Thus, DeltaNet simply replaces the value vector \(\bm{v}_{i}\) in linear attention with the "pseudo" value vector \(\bm{u}_{i}\). Once the \(\bm{u}_{i}\)'s have been constructed, the rest of computation can proceed as in ordinary linear attention, i.e., \(\mathbf{O}=\left(\mathbf{Q}\mathbf{K}^{\text{r}}\odot\mathbf{M}\right)\mathbf{U}\) where \(\mathbf{U}\in\mathbb{R}^{L\times d}\) is the row-wise concatenation of the \(\bm{u}_{i}\) vectors.

However, computing \(\bm{u}_{t}\) naively requires explicitly materializing \(\mathbf{S}_{t-1}\) to compute \(\bm{v}_{t}^{\text{old}}\), which would require \(\mathcal{O}(d^{2})\) memory. We now show that we can obtain the \(\bm{u}_{t}\)'s _without_ explicitly materializing \(\mathbf{S}_{t-1}\) in \(\mathcal{O}(d)\) memory. Our simple proof (by induction) relies on an application of the WY representation for products of Householder matrices [11]. The base case is clear since we have \(\mathbf{S}_{1}=\beta_{1}\bm{v}_{1}\bm{k}_{1}^{\text{r}}\), so \(\bm{u}_{1}=\beta_{1}\bm{v}_{1}\). For the inductive step, we first observe that the DeltaNet update is given by,

\[\mathbf{S}_{t}=\mathbf{S}_{t-1}-\bm{v}_{t}^{\text{old}}\bm{k}_{t}^{\text{r}}+ \bm{v}_{t}^{\text{new}}\bm{k}_{t}^{\text{r}}=\mathbf{S}_{t-1}-\beta_{t}\left( \mathbf{S}_{t-1}\bm{k}_{t}\right)\bm{k}_{t}^{\text{r}}+\beta_{t}\bm{v}_{t}\bm{k }_{t}^{\text{r}}=\mathbf{S}_{t-1}(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{ \text{r}})+\beta_{t}\bm{v}_{t}\bm{k}_{t}^{\text{r}},\]

which can be seen as applying a generalized Householder transformation (i.e., matmul with an identity plus rank-one matrix) to the previous state. The inductive step is then given by,

\[\mathbf{S}_{t}=\mathbf{S}_{t-1}(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{ \text{r}})+\beta_{t}\bm{v}_{t}\bm{k}_{t}^{\text{r}}=\sum_{i=1}^{t-1}\bm{u}_{i} \bm{k}_{i}^{\text{r}}+\underbrace{\beta_{t}\left(\bm{v}_{t}-\sum_{i=1}^{t-1} \bm{u}_{i}\left(\bm{k}_{i}^{\text{r}}\bm{k}_{t}\right)\right)}_{\text{defined as $\bm{u}_{t}$}}\bm{k}_{t}^{\text{r}}=\sum_{i=1}^{t}\bm{u}_{i}\bm{k}_{i}^{ \text{r}}\] (3)

Note that \(\bm{u}_{t}\) does not require materializing any of the hidden states and requires \(\mathcal{O}(d)\) memory to compute, thus completing the proof. While we have avoided materializing \(\mathbf{S}_{t}\)'s, computing \(\bm{u}_{t}\)'s for all \(L\) (that is, \(\mathbf{U}\)) takes \(\mathcal{O}(L^{2}d)\) and moreover cannot be fully parallelized, unlike in linear attention where we can calculate all the value vectors \(\mathbf{V}\) in parallel in \(\mathcal{O}(1)\) steps. We now show that the above trick still enables an efficient chunkwise parallel form for DeltaNet.

### Chunkwise Parallel Form for DeltaNet

To derive the chunkwise parallel form, we first unroll the recurrence,

\[\mathbf{S}_{t}=\mathbf{S}_{t-1}(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{ \text{r}})+\beta_{t}\bm{v}_{t}\bm{k}_{t}^{\text{r}}=\sum_{i=1}^{t}\beta_{i}( \bm{v}_{i}\bm{k}_{i}^{\text{r}})\left(\prod_{j=i+1}^{t}(\mathbf{I}-\beta_{j} \bm{k}_{j}\bm{k}_{j}^{\text{r}})\right).\] (4)We then define the following variables: \(\mathbf{P}^{j}_{i}=\prod_{t=i}^{d}(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{ \intercal})\in\mathbb{R}^{d\times d}\), \(\mathbf{H}^{j}_{i}=\sum_{t=i}^{j}\beta_{t}(\bm{v}_{t}\bm{k}_{t}^{\intercal}) \mathbf{P}^{j}_{t+1}\in\mathbb{R}^{d\times d}\), where we let \(\mathbf{P}^{j}_{i}=\mathbf{I}\) whenever \(i>j\). Intuitively, \(\mathbf{P}^{j}_{i}\) is the "decay factor" to be applied to \(\mathbf{S}_{i}\) for obtaining \(\mathbf{S}_{j}\), and \(\mathbf{H}^{j}_{i}\) represents the contributions to \(\mathbf{S}_{j}\) starting from token \(i\). (Hence \(\mathbf{S}_{t}=\mathbf{H}^{\intercal}_{1}\)). The chunkwise recurrence can then be written as,

\[\mathbf{S}^{r}_{[t]}=\mathbf{S}^{0}_{[t]}\mathbf{P}^{r}_{[t]}+ \mathbf{H}^{r}_{[t]}\] (5)

where we define the chunkwise variables \(\mathbf{S}^{i}_{[t]}=\mathbf{S}_{tC+i}\), \(\mathbf{P}^{r}_{[t]}=\mathbf{P}^{tC+r}_{tC+1}\), \(\mathbf{H}^{r}_{[t]}=\mathbf{H}^{tC+r}_{tC+1}\). Here we have \(\frac{L}{C}\) chunks of size \(C\). The trick is to now efficiently represent the \(\mathbf{P}^{r}_{[t]},\mathbf{H}^{r}_{[t]}\in\mathbb{R}^{d\times d}\) matrices using a similar approach described in SS3.1, so that these matrices can be stored in \(\mathcal{O}(d)\) memory,

\[\mathbf{P}^{r}_{[t]}=\mathbf{I}-\sum_{i=1}^{r}\bm{w}^{i}_{[t]} \bm{k}^{\intercal}_{[t]},\qquad\mathbf{H}^{r}_{[t]}=\sum_{i=1}^{r}\bm{u}^{i}_ {[t]}\bm{k}^{\intercal}_{[t]}\quad\in\mathbb{R}^{d\times d}\] (6) \[\bm{w}^{r}_{[t]}=\beta^{r}_{[t]}\left(\bm{k}^{r}_{[t]}-\sum_{i=1}^ {r-1}\bm{w}^{i}_{[t]}(\bm{k}^{\intercal}_{[t]}\bm{k}^{\intercal}_{[t]}) \right),\quad\bm{u}^{r}_{[t]}=\beta^{r}_{[t]}\left(\bm{v}^{r}_{[t]}-\sum_{i=1} ^{r-1}\bm{u}^{i}_{[t]}(\bm{k}^{\intercal}_{[t]}\bm{k}^{\intercal}_{[t]}\bm{k}^ {r}_{[t]})\right)\quad\in\mathbb{R}^{d}\] (7)

The derivations for the above can be found in the appendix. Subsequently, based on Eq. 5, we can obtain the chunk-level recurrence for hidden states and outputs as,

\[\mathbf{S}^{r}_{[t]}=\mathbf{S}^{0}_{[t]}-\left(\mathbf{S}^{0}_{[ t]}\sum_{i=1}^{r}\bm{w}^{i}_{[t]}\bm{k}^{\intercal}_{[t]}\right)+\sum_{i=1}^{r} \bm{u}^{i}_{[t]}\bm{k}^{\intercal}_{[t]}=\mathbf{S}^{0}_{[t]}+\sum_{i=1}^{r} \left(\bm{u}^{i}_{[t]}-\mathbf{S}^{0}_{[t]}\bm{w}^{i}_{[t]}\right)\bm{k}^{ \intercal}_{[t]},\] \[\bm{o}^{r}_{[t]}=\mathbf{S}^{r}_{[t]}\bm{q}^{r}_{[t]}=\mathbf{S}^ {0}_{[t]}\bm{q}^{r}_{[t]}+\sum_{i=1}^{r}\left(\bm{u}^{i}_{[t]}-\mathbf{S}^{0}_ {[t]}\bm{w}^{i}_{[t]}\right)\left(\bm{k}^{\intercal}_{[t]}\bm{q}^{i}_{[t]} \right).\]

Letting \(\mathbf{S}_{[t]}=\mathbf{S}^{0}_{[t]}\), the above can be simplified to matrix notations similarly to Eq.1-2,

\[\mathbf{S}_{[t+1]}=\mathbf{S}_{[t]}+\left(\mathbf{U}_{[t]}-\mathbf{W}_{[t]} \mathbf{S}^{\intercal}_{[t]}\right)^{\intercal}\mathbf{K}_{[t]},\] (8)

\[\mathbf{O}_{[t]}=\mathbf{Q}_{[t]}\mathbf{S}^{\intercal}_{[t]}+ \left(\mathbf{Q}_{[t]}\mathbf{K}^{\intercal}_{[t]}\odot\mathbf{M}\right)\left( \mathbf{U}_{[t]}-\mathbf{W}_{[t]}\mathbf{S}^{\intercal}_{[t]}\right)\] (9)

where \(\square_{[t]}=\square_{[t]}^{1:C}\in\mathbb{R}^{C\times d}\) for \(\square\in\{\mathbf{Q},\mathbf{K},\mathbf{V},\mathbf{O},\mathbf{U},\mathbf{W}\}\) defines the chunkwise matrices that are formed from stacking the \(\bm{q}_{t},\bm{k}_{t},\bm{v}_{t},\bm{o}_{t},\bm{u}_{t},\bm{w}_{t}\) vectors.

Practical considerations.In the above, Eq. 7 is fully recurrent and thus cannot use tensor cores written as is. To solve this, we further leverage the _UT transform_[44, 23] (see SSB.2 for derivations):

\[\mathbf{T}_{[t]} =\left(\mathbf{I}+\mathrm{tril}(\mathrm{diag}(\beta_{[t]}) \mathbf{K}_{[t]}\mathbf{K}^{\intercal}_{[t]},-1)\right)^{-1}\mathrm{diag}\left( \beta_{[t]}\right)\] (10) \[\mathbf{W}_{[t]} =\mathbf{T}_{[t]}\mathbf{K}_{[t]},\qquad\qquad\mathbf{U}_{[t]}= \mathbf{T}_{[t]}\mathbf{V}_{[t]}\] (11)

to rewrite most operations in matmutls. The inverse of lower triangular matrices could be solved efficiently using forward substitution. Once computed, the hidden state updates (Eq. 8) and the output computations (Eq. 9) are largely the same as in vanilla linear attention. We adapt FlashLinearAttention [124] to implement Eq. 8 and 9 with hidden states recomputed during the backward pass for saving GPU memory. The PyTorch pseudocode for the forward pass is shown in Listing 1.

Speed comparison.We implement both the pure recurrent form4 and the chunkwise parallel form in Triton [112] and show the speed-ups for various sequence lengths (L) and head dimensions (\(d_{\text{head}}\)) in the right figure, where the model dimension \(d\) is 2048 and we vary batch size and sequence length so that they multily to 16384.5 Our chunkwise algorithm achieves greater speed-ups as sequence length \(L\) and head dimension \(d_{\text{head}}\) increase, where the use of sequence-level parallelism (for high GPU occupancy) and tensor core (for fast matmuls) become more important [124, SS3].

Footnote 4: Note that our recurrent kernel is already \(2\times\) faster than the original CUDA kernel from Schlag et al. [101].

Footnote 5: So far we have been assuming a single head (\(d_{\text{head}}=d\)) for easier exposition. In practice we use multiple heads where the head dimension \(d_{\text{head}}\) is smaller than the model dimension \(d\). We thus have \(\mathbf{S}_{t}\in\mathbb{R}^{d\times d_{\text{head}}}\).

Figure 1: Speed-up of the chunkwise parallel form vs. the recurrent form.

Fully Parallel Form for DeltaNet.For completeness, we also discuss the fully parallel form of DeltaNet. While we use the concept of a "pseudo" value, it is possible to avoid modifying values. From Eq. 4, it is straightforward to compute the attention matrix \(\mathbf{A}\): \(\mathbf{A}_{ij}=\bm{k}_{j}^{\mathsf{T}}\mathbf{D}_{j+1}^{i}\bm{q}_{i}\) if \(j\leq i\) and \(0\) otherwise. Notably, \(\mathbf{A}\) has the matrix form \(\mathbf{A}=\left(\mathbf{Q}\mathbf{K}^{T}\odot\mathbf{M}\right)\mathbf{T}\), obtained by combining Eq. 3 and 11. However, computing \(\mathbf{T}\) requires a matrix inverse (Eq. 10), which scales cubically with sequence length without further algorithmic changes. Due to the above we avoid using the fully parallel form for training DeltaNet; however the "attention" matrix derived from this form could be of interest to the interpretability study for RNNs [3, 134].

### DeltaNet Transformer

We describe how the DeltaNet layer primitive is used to build up a transformer-like model using standard modules. We largely follow the LLaMA-architecture [Transformer++, 114] and simply replace the self-attention layer with the DeltaNet layer. We also apply normalization before output projection for stable training [86, 69]. As the additional parameters for computing scalar \(\beta_{t}\) terms are negligible, parameter allocation is roughly the same as in Transformer++, i.e., \(4d^{2}\) for the DeltaNet layer and \(8d^{2}\) for the SwiGLU FFN layer [103].

Feature map and normalization.Our key/query vectors are given by \(\bm{k}_{t}=\frac{\operatorname{SILU}(\mathbf{W}_{K}\bm{x}_{t})}{\| \operatorname{SILU}(\mathbf{W}_{K}\bm{x}_{t})\|_{2}}\), \(\bm{q}_{t}=\frac{\operatorname{SILU}(\mathbf{W}_{Q}\bm{x}_{t})}{\| \operatorname{SILU}(\mathbf{W}_{Q}\bm{x}_{t})\|_{2}}\). Schlag et al. [101] originally follow Katharopoulos et al. [48] and apply a "ELU + 1" [17] to nonlinearly transform the key/query vectors. We instead use the SiLU activation [24], which was found to perform better [88, 19]. For stability, it is crucial to ensure that the norm of each eigenvalue of the transition matrices does not exceed one. The eigenvalues of \(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{\mathsf{T}}\) are 1 with multiplicity \(d-1\) and \(1-\beta_{t}\|\bm{k}_{t}\|_{2}\) with multiplicity 1. Schlag et al. [101] used the \(L_{1}\) norm to normalize query/key vectors, ensuring that \(0\leq 1-\beta_{t}\|\bm{k}_{t}\|_{2}\leq 1\). We instead apply \(L_{2}\) normalization, which we found to perform better and offers a more intuitive interpretation: when \(\beta_{t}=1\), \(\mathbf{I}-\bm{k}_{t}\bm{k}_{t}^{\mathsf{T}}\) becomes a projection matrix, erasing information in one subspace while preserving the other \(d-1\) subspaces. This is beneficial for retaining information while enabling more _targeted_ forgetting.

### Hybrid Models

Following recent work on combining subquadratic token-mixing layers with existing neural network primitives [6, 21, 55], we also experiment with hybridizing DeltaNet models.

Convolutional layers.Recent linear recurrent models typically incorporate a lightweight depthwise-separable convolution layer after the query/key/value projections [31, 9, 19]. This "short convolution" layer [83] generalizes the shift SSM [26], and is efficient in both number of parameters and computational cost. We also add a short convolution layer after the query/key/value projections.

Local sliding window and global attention.Linear attention largely uses a content-based addressing mechanism [29] and lacks positional information [129]. Arora et al. [6] also argue that linear attention lacks the ability to perform precise local token shifts and comparisons, thus facing difficulties on retrieval-intensive tasks. Motivated by this, we experiment with two different hybrid architectures that incorporate softmax attention. We first explore _sliding window attention_ (SWA) which has been shown to significantly improve linear attention [86, 6, 57, 75]; we follow Griffin [21] and Samba [95] to interleave DeltaNet layers and SWA layers. We also experiment with _global attention_, which has been found to be helpful [51, 35] even if only few of the recurrent layers are replaced with global attention [55]. We follow Fu et al. [26] to replace only two layers with global attention: the second layer and the \((\frac{N}{2}+1)\)-th layer, where \(N\) is total number of layers.

## 4 Empirical Study

We compare the DeltaNet against strong baselines in both synthetic and real-world language modeling settings. Our main baselines include: LLaMA-architecture Transformer++ [114]; RetNet [108], a linear attention Transformer with non-data-dependent exponential decay and large head dimension;

Figure 2: An illustration of DeltaNet neural architecture.

GLA [124], a linear attention Transformer with data-dependent decay; and Mamba [31], a selective state-space model with data-dependent decay.

### Synthetic Benchmarks

We evaluate on three synthetic benchmarks: Multi-query associative recall [MQAR; 4], Mechanistic Architecture Design [MAD; 84], and in-context language learning [RegBench; 2].

MQAR evaluates language models' ability to (in-context) recall information within a context when faced with multiple recall queries. We use Arora et al. [4]'s training setting and for DeltaNet we use 2 heads. We do not use convolutions for these experiments. Figure 4 shows that DeltaNet performs perfectly (even without convolution) in the hardest setting and outperforms Mamba (which uses convolutions) in the low-dimension setting. Next, we consider the MAD benchmark [84], a suite of synthetic token manipulation tasks designed to probe capabilities of model architectures. The results are shown in Table 3. Compared with other architectures, including MHA, DeltaNet is better at recalling tasks, especially on Fuzzy Recall as expected, although it somehow struggles on the "Memorize" task. We defer the RegBench results to the SSA.2 due to space constraints.

### Language Modeling

Experimental setup.Following prior work [31; 124], we evaluate on Wikitext perplexity and zero-shot common sense reasoning tasks, including LAMBADA [LMB.; 77], PiQA [12], HellaSwag [Hella.; 127], WinoGrande [Wino.; 99], ARC-easy (ARC-e) and ARC-challenge (Arc-c) [16]. Following Arora et al. [6], we also evaluate the models real-world recall-intensive tasks, including FDA [5], SWDE [60], and SQUAD [93]. Both SWDE and FDA focus on extracting structured information: SWDE from raw HTML to identify semi-structured relationships, and FDA from PDFs to retrieve key-value pairs. SQUAD evaluates language models on reading comprehension by providing a text passage and a related question. See SSA.1 for hyperparameter settings.

Results.Our main language modeling results are shown in Table 1. Since Mamba uses convolutions by default while GLA does not, we retrain the GLA with convolution, and also train DeltaNet without convolution. For the 1.3B setting we only train the DeltaNet with convolution due to limited compute resources. In general we find that DeltaNet outperforms the strong Mamba/GLA baselines in terms of both perplexity and downstream task performance. For recall-intensive tasks (i.e., SWDE, SQuAD, FDA), we find that under the same state size at the 340M scale, DeltaNet outperforms GLA, confirming the effectiveness of the delta rule. However, at the 1.3B scale, DeltaNet underperforms GLA due to its poorer state size scability (see SS5.3), since state size plays an important role in recall-intensive tasks. Finally, we confirm the benefits of hybrid architectures [21; 55]: both the sliding window and global attention hybrids work well, outperforming the strong Transformer++ baselines. We also scale DeltaNet to the 3B parameter scale trained with 1T tokens using the same settings as Shen et al. [104]. The results are shown in Table 5, where 3B DeltaNet slightly underperforms a Transformer architecture trained with the same setting (PowerLM-3B), but outperforms other RNN baselines in the 2B-3B range (though these are trained for a different number of tokens so are not exactly comparable).

Ablations.In Table 1 (bottom) we ablate the choice of feature map and normalization. We find that simply replacing the \(L_{1}\)-norm with the \(L_{2}\)-norm greatly increases performance. For the feature map, we experiment with \(\{\texttt{ReLU},1+\texttt{ELU},\texttt{SilU}\}\) and find that SiLU performs the best, consistent with prior work [88].

Training throughput.Figure 6 compares the training throughputs of different 1.3B models in different training lengths and batch size settings. The training speed of DeltaNet is close to GLA

Figure 4: Accuracy (%) on MQAR.

Figure 3: Results on the synthetic MAD benchmark. Results other than DeltaNet are directly borrowed from Poli et al. [84]. (Multi-head) Hyena, DeltaNet and Mamba make use of convolutions, whereas GLA does not.

and significantly faster than Mamba. All linear-time models outperform Transformers for longer-sequence training.

## 5 Discussion and Related Work

### DeltaNet vs. State Space Models / Linear RNNs

To discuss DeltaNet against existing linear RNNs (including state-space models) we first introduce a general class of associative RNNs with matrix-valued hidden states. Given a matrix-valued hidden state \(\mathbf{S}_{t}\in\mathbb{R}^{d\times n}\) and current input \(\boldsymbol{x}_{t}\in\mathbb{R}^{d}\), these models have the following form:

\[\mathbf{S}_{t} =\mathbf{S}_{t-1}\bullet\mathbf{M}_{t}+\boldsymbol{v}_{t} \boldsymbol{k}_{t}^{\top},\] (recurrence) \[\boldsymbol{o}_{t} =\mathbf{S}_{t}\boldsymbol{q}_{t},\] (memory read-out)

where \(\bullet\) is an associative operator (e.g., Hadamard product, matrix multiplication, etc.). The matrix \(\mathbf{M}_{t}\) and vectors \(\boldsymbol{v}_{t}\), \(\boldsymbol{k}_{t}\), \(\boldsymbol{q}_{t}\) are (potentially non-linear) functions of the current input \(\boldsymbol{x}_{t}\).

As is the case in vector-valued linear RNNs [64, 106], the use of an associative operator enables the use of parallel scan [13] to calculate \(\mathbf{S}_{1},\ldots,\mathbf{S}_{L}\) in \(O(\log L)\) steps and \(O(L)\) work (ignoring the terms associated with the associative operation) if the inputs \(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{L}\) are given (though see our discussion in footnote 1). Hence, as long as the associative operator is not too expensive, training

\begin{table}
\begin{tabular}{l|c|c c c c c c c c|c c c} \hline \hline
**Model** & **Wiki** & **LMB** & **LMB** & **PIQA** & **Hella** & **Wino** & **ARC-e** & **ARC-e** & **Avg.** & **SWDE** & **SQuAD** & **FDA** & **Sate** \\  & ppl \(\downarrow\) & ppl \(\downarrow\) & ppl \(\downarrow\) & \(\downarrow\) & acc \(\uparrow\) & acc \(\uparrow\) & acc\(\uparrow\) & acc \(\uparrow\) & acc\(\uparrow\) & acc\(\uparrow\) & acc\(\uparrow\) & acc\(\uparrow\) & exp. \\ \hline _340M params / 15B tokens_ & & & & & & & & & & & & & & \\ Transformer++ & 28.39 & 42.69 & 31.0 & 63.3 & 34.0 & 50.4 & 44.5 & 24.2 & 41.2 & 42.2 & 22.1 & 21.4 & N/A \\ ResNet (_w/o. com_) & 32.33 & 49.19 & 28.6 & 63.5 & 33.5 & 52.5 & 44.5 & 23.4 & 41.0 & 13.3 & 27.6 & 2.9 & 512x \\ Mamba (_w. com_) & 28.39 & 39.66 & 30.6 & 65.0 & 35.4 & 50.1 & 46.3 & 23.6 & 41.8 & 12.4 & 23.0 & 2.1 & 64x \\ GLA (_w. com_) & 28.65 & 43.53 & 30.3 & 64.8 & 34.5 & 51.4 & 45.1 & 22.7 & 41.5 & 18.6 & 27.2 & 8.1 & 128x \\ (_w. com_) & 29.47 & 45.53 & 31.3 & 65.1 & 33.8 & 51.6 & 44.4 & 24.6 & 41.8 & 24.0 & 24.7 & 7.3 & 128x \\ DeltaNet (_w. com_) & 29.08 & 50.87 & 30.0 & 63.6 & 33.6 & 51.7 & 46.0 & 23.0 & 41.3 & 24.6 & 26.9 & 4.5 & 128x \\ DeltaNet (_w. com_) & 28.24 & 37.37 & 32.1 & 64.8 & 34.3 & 52.2 & 45.8 & 23.5 & 42.1 & 26.4 & 28.9 & 12.8 & 128x \\ + Sliding Atm & 27.06 & 38.17 & 33.4 & 64.0 & 35.3 & 50.9 & 45.9 & 23.2 & 42.1 & 39.3 & 32.5 & 18.8 & N/A \\ + Global Attn (2 layers) & 27.51 & 35.04 & 33.5 & 64.0 & 34.5 & 51.7 & 46.0 & 23.3 & 42.1 & 42.9 & 32.1 & 23.1 & N/A \\ \hline _1.3B params / 100B tokens_ & & & & & & & & & & & & & \\ Transformer++ & 16.85 & 13.44 & 48.9 & 70.8 & 49.6 & 53.6 & 56.0 & 26.5 & 50.9 & 66.6 & 31.5 & 27.4 & N/A \\ ResNet (_w/o. com_) & 18.64 & 17.27 & 43.3 & 70.0 & 47.3 & 52.5 & 54.8 & 25.6 & 48.9 & 42.8 & 34.7 & 14.3 & 512x \\ Mamba (_w. com_) & 17.06 & 13.89 & 46.2 & 72.2 & 40.1 & 54.1 & 59.0 & 28.2 & 50.0 & 41.4 & 35.2 & 6.2 & 64x \\ GLA (_w. com_) & 17.22 & 14.47 & 46.9 & 71.8 & 49.8 & 53.9 & 57.2 & 26.6 & 51.0 & 50.6 & 42.6 & 19.9 & 256x \\ (_w. com_) & 17.25 & 14.92 & 46.2 & 70.6 & 49.9 & 53.0 & 55.3 & 27.0 & 50.4 & 52.4 & 37.4 & 22.3 & 256x \\ DeltaNet (_w. com_) & 16.87 & 12.21 & 48.9 & 71.2 & 50.2 & 53.6 & 57.2 & 28.3 & 51.6 & 49.5 & 37.4 & 17.2 & 128x \\ + Sliding Atm & 16.56 & 11.74 & 49.2 & 71.8 & 51.1 & 52.8 & 58.9 & 28.8 & 52.1 & 53.3 & 43.3 & 22.3 & N/A \\ + Global Attn (2 layers) & 16.55 & 12.40 & 48.8 & 70.8 & 50.7 & 54.2 & 58.4 & 28.1 & 51.8 & 71.0 & 43.0 & 29.8 & N/A \\ \hline _DeltaNet Ablation (340M)_ & & & & & & & & & & & & & \\ _w: L\({}_{1}\)-norm \& 1+tELM_ & 31.12 & 55.96 & 26.3 & 63.9 & 33.0 & 50.9 & 44.3 & 21.8 & 40.1 & 14.5 & 23.9 & 6.2 & 128x \\ \({}_{w: L\_{2}\)-norm \& 1+tELM} & 28.03 & 37.62 & 32.2 & 65.7 & 34.7 & 51.8 & 45.4 & 22.5 & 42.1 & 23.8 & 28.6 & 13.1 & 128x \\ \({}_{w: L\_{2}\)-norm \& ReLU & 28.75 & 43.53 & 30.2 & 64.0 & 33.9 & 48.9 & 45.6 & 22.8 & 40.9 & 27.2 & 26.7 & 9.0 & 128x \\ \hline \hline \end{tabular}
\end{table}
Table 1: Main language modeling results against Transformer++, RetNet [108], Mamba [31], and GLA [124]. All models are trained on the same subset of the SlimPajama dataset with the Mistral tokenizer. The Transformer++, RetNet, Mamba, GLA (_w/o. com_) results are taking from Yang et al. [124]. For hybrid models, “Sliding Attn” interleaves a sliding window attention every other layer, and “Global Attn” uses full global attention on two layers. The 340M/1.3B models are trained for 15B/100B tokens respectively. All results are obtained through 1m-evaluation-harness[27]. The last column denotes the expansion ratio of the recurrent state size relative to the product of the number of layers and model dimension (see Zhang et al. [131, App. C]).

Figure 5: Zero-shot model performance across selected benchmarks for 3B models. Llama-3.2-3B and PowerLM-3B are Transformer models, while the others are recurrent models. ARC results are averaged over normalized accuracy across ARC-Easy and ARC-Challenge.

can be efficient. However, parallel scan by itself is not sufficient for training language models at practical scale due to some associative operator's being too expensive. Recent models such as such as Mamba [31] and gated linear attention Transformers [108; 124; 92; 79; 9] thus make use of cheap element-wise recurrence updates, in particular the Hadamard product, i.e., \(\bullet=\odot\). See Table 2 for how recent models can be cast into this form.

Standard matrix multiplications (i.e., \(\mathbf{S}_{t-1}\bullet\mathbf{M}_{t}=\mathbf{S}_{t-1}\mathbf{M}_{t}\)) on the other hand can model richer interactions that go beyond elementwise recurrence. Without any structural assumptions on \(\mathbf{M}_{t}\) however, these operations would take \(O(dn^{2})\) for each update (as opposed to \(O(dn)\) for elementwise products), which would be prohibitively expensive. Hence, DeltaNet's use of \(\mathbf{M}_{t}=\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{\top}\) can be seen as exploiting structured matrices to efficiently model interactions beyond elementwise recurrences. Our chunkwise algorithm could generalize to a broader class of matrices in the Diagonal-Plus-Low-Rank (DPLR) form \(\mathbf{M}_{t}=\mathbf{D}-\bm{a}_{t}\bm{b}_{t}^{\top}\), which has been explored in S4 [32], although their DPLR transition matrices are data-independent. We adopt DeltaNet's parameterization in this work (i.e., \(\mathbf{D}=\mathbf{I},\bm{a}_{t}=\beta_{t}\bm{k}_{t},\bm{b}_{t}=\bm{k}_{t}\)) as we are primarily interested in improving recall (through DeltaNet's key-value update rule) while maintaining parameter efficiency. We leave the exploration of more generalized parameterizations for future work.

### Towards a Unifying Framework for Efficient Autoregressive Sequence Transformations

While the above class of models makes it possible to unify recent models, we do not claim that it is the "right" level at which view (autoregressive) sequence transformations of the form \(\{\bm{x}_{t}\}_{t=1}^{L}\mapsto\{\bm{o}_{t}\}_{t=1}^{L}\), where \(\bm{o}_{t}\) cannot depend on any \(\bm{x}_{j}\) if \(j>t\). For example, this framing makes it difficult to (neatly) capture other subquadratic models that have been shown to be effective [126; 49; 83; 83]. An alternative unifying framework might be to view the above sequence transformations as a discretization of a continuous state space model [32; 106; 31], or as a matrix multiplication with a masked structured matrix [76; 87; 46; 19]. What does seem important, however, is that a framework should ideally expose efficient algorithms for training, and the algorithm should be hardware-efficient, which, in the case of modern GPUs, means that it should be rich in matrix multiplications. From this perspective, the state-space duality (SSD) framework recently proposed by Dao and Gu [19], which provides a connection between SSM-based sequence transformations and structured matrix multiplications with a semiseparable matrix, seems a promising candidate. However, this framework may not capture an important class of models, e.g., models where the as

\begin{table}
\begin{tabular}{l l l} \hline \hline Model & Recurrence & Memory read-out \\ \hline Linear Attention [48; 47] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}+\bm{v}_{t}\bm{k}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}\bm{q}_{t}\) \\ + Kernel & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}+\bm{v}_{t}(\bm{o}_{t})^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}\phi(\bm{q}_{t})\) \\ + Normalization & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}+\bm{v}_{t}(\bm{k}_{t})^{\top}\), \(\bm{z}_{t}=\bm{z}_{t-1}+\phi(\bm{k}_{t})\) & \(\bm{o}_{t}=\mathbf{S}_{t}\phi(\bm{q}_{t})/(\bm{z}_{t}^{\top}\phi(\bm{q}_{t}))\) \\ DeltaNet [101] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{ \top})+\beta_{t}\bm{v}_{t}\bm{k}_{t}\) & \(\bm{o}_{t}=\mathbf{S}_{t}\bm{q}_{t}/(\bm{z}_{t}^{\top}\phi(\bm{q}_{t}))\) \\ Gaed RFA [81] & \(\mathbf{\beta}_{t}=\mathbf{S}_{t-1}+(1-\beta_{t}\bm{v}_{t}\bm{k}_{t}^{\top}, \ \bm{z}_{t}=g_{t}\bm{z}_{t-1}+(1-g_{t})\bm{k}_{t}\) & \(\bm{o}_{t}=\mathbf{S}_{t}(\mathbf{S}_{t}\odot\mathbf{C})+\bm{d}\odot\bm{v}_{t}\) \\ S4 [32; 106] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\odot\exp(-\alpha(t)^{\top}\odot\exp(\bm{A}))+ \bm{B}\odot(\bm{v}_{t}\bm{1}^{\top})\) & \(\bm{o}_{t}=\mathbf{S}_{t}^{\top}\phi_{t}^{\top}\) \\ ABC [82] & \(\mathbf{S}_{t}^{\top}=\mathbf{S}_{t-1}^{\top}+\bm{k}_{t}\bm{o}_{t}^{\top}\), \(\mathbf{S}_{t}^{\top}=\mathbf{S}_{t-1}^{\top}+\bm{v}_{t}\bm{o}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}^{\top}\) \\ DFW [63] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\odot(\bm{\beta}_{t}\bm{\alpha}_{t}^{\top})+ \bm{v}_{t}\bm{k}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}\bm{q}_{t}\) \\ RetNet [108] & \(\mathbf{S}_{t}=\gamma\mathbf{S}_{t-1}+\bm{v}_{t}\bm{k}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}\) \\ Mamba [31] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\odot\exp(-(\alpha_{t}\bm{1}^{\top})\odot \exp(\bm{A}))+(\bm{\alpha}_{t}\odot\bm{v}_{t})\bm{k}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}+\bm{\alpha}\odot\bm{v}_{t}\) \\ GLA [124] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\odot(\bm{1}\bm{\alpha}_{t})+\bm{v}_{t}\bm{k}_{t }^{\top}=\mathbf{S}_{t-1}\text{Diag}(\bm{\alpha}_{t})+\bm{v}_{t}\bm{k}_{t}^{ \top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}\) \\ RWKv-6 [79] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\text{Diag}(\bm{\alpha}_{t})+\bm{v}_{t}\bm{k}_{t }^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t-1}+(\bm{d}\odot\bm{v}_{t})\bm{k}_{t}^{\top}\) \\ HGRN-2 [92] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\text{Diag}(\bm{\alpha}_{t})+\bm{v}_{t}(\mathbf{ I}-\bm{\alpha}_{t})^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}\) \\ mLSTM [9] & \(\mathbf{S}_{t}=f_{t}\mathbf{S}_{t-1}+i_{t}\bm{v}_{t}\bm{k}_{t}^{\top}\) & \(\bm{z}_{t}=f_{t}\bm{z}_{t-1}+i_{t}\bm{k}_{t}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}/\max\{1,|\bm{z}_{t}^{\top}\bm{q}_{t}|\}\) \\ Mamba-2 [19] & \(\mathbf{S}_{t}=\gamma_{t}\mathbf{S}_{t-1}+\bm{v}_{t}\bm{k}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}\) \\ GSA [131] & \(\mathbf{S}_{t}^{\top}=\mathbf{S}_{t-1}^{\top}\text{Diag}(\bm{\alpha}_{t})+\bm{k }_{t}\bm{o}_{t}^{\top}\), \(\mathbf{S}_{t}^{\top}=\mathbf{S}_{t-1}^{\top}\text{Diag}(\bm{\alpha}_{t})+\bm{v }_{t}\bm{\phi}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}^{\top}\text{softmax}\left(\mathbf{S}_{t}^{\top}\bm{q}_{t}\right)\) \\ Gaed DeltaNet [125] & \(\mathbf{S}_{t}=\mathbf{S}_{t-1}\left(\alpha_{t}(\bm{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t }^{\top})\right)+\beta_{t}\bm{v}_{t}\bm{k}_{t}^{\top}\) & \(\bm{o}_{t}=\mathbf{S}_{t}q_{t}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overview of recent linear recurrent models that have been proposed and applied to autoregressive language modeling (ordered in rough chronological order). These works make use of a matrix-valued hidden state \(\mathbf{S}_{t}\in\mathbb{R}^{d\times n}\) (or two sociative recurrence involves matrix multiplication with an unstructured matrix, or models that make use of more exotic associative operators (e.g., in Peng et al. [80]).

Finally, we observe that there have been many recent linear-time models that have been proposed which purportedly match or outperform classic transformers. As can be seen in Table 2, the "sequence mixing" component of these works are closely related to one another. However, the way in which the token-mixing primitive is used to build up a transformer-like model varies widely. For example, while most recent works make use of depthwise-separable convolution layers (not shown in Table 2) [31; 9; 15; 125], earlier works generally do not [48; 101; 81]. There are also differences in the parameterizations of the feedforward layers used for the "channel mixing" component. Such variations should be taken into account before declaring a particular model layer superior to another.

### Limitations and Future Work

Our work has several limitations. First, in terms of computation, although we propose a new hardware-efficient algorithm, the training speed still lags behind that of GLA. This is due to the overhead caused by modeling state-to-state dependencies as described above, which requires "marginalizing" over the head dimension inside the kernel, similar to the case of softmax attention. However, for GLA since there are no intra-state dependencies (everything is elementwise), and thus it is easy to use tiling to support arbitrary size of head dimension, as implemented in Yang and Zhang [123]. This limitation would potentially limit DeltaNet's memory size, consequently lowering the recall-intensive task performance as we observed in SS4.2. However, it may be feasible to adopt block diagonal generalized Householder transition matrices with block sizes fitting GPU SRAM (e.g., 128) while maintaining a overall large head dimension (and thus a large recurrent state size).

We also found that the length generalization of DeltaNet was limited, while GLA and RetNet (and Mamba to an extent) have been found to be able to extrapolate beyond the training length [124]. We speculate that this is because DeltaNet lacks explicit decay factors. This could be improved through incorporating a gating term in the recurrence, as demonstrated in a recent work by Yang et al. [125].

## 6 Related Work

We briefly discuss related work here and give an extended discussion in Appendix C.

Linear transformers can be seen as a type of iterated Hopfield networks [72], and this connection can provide perspectives on the limitations and improvements of linear attention transformers. For example, vanilla linear transformers use a Hebbian-like update rule, which has been shown to have limited memory capacity [68]. Later works in Hopfield networks use higher-order polynomials [22] and exponential kernels [94; 50] to enhance the memory capacity, which is also related to linear attention with polynomial kernels [45; 6; 1]. On the other hand, the delta rule has been shown to have better memory capacity [28; 85; 56; 101]. In this sense, given the fixed size recurrent state, using the delta rule is able to achieve a better frontier of the recall-memory tradeoff curve [6], and has recently been applied to enhance real-world retrieval tasks [74; 96]. Moreover, it outperforms the additive rule used in vanilla linear transformers across multiple domains [101; 38; 40; 36; 42].

Despite these advantages, Irie et al. [42] revealed theoretical limitations of the delta update rule in terms of expressiveness. Recurrent enhancements of DeltaNet, such as Recurrent DeltaNet [37] and the Modern Self-Referential Weight Matrix [41], and the mesa-layer [117] were proposed and found superior. However, these models extend beyond linear RNNs and cannot be parallelized across sequence length. This suggests a fundamental trade-off between parallelism and expressiveness [70]. How to further enhance DeltaNet without sacrificing parallelism remains an open question, and the hybrid cross-chunk nonlinear and intra-chunk linear strategy used in TTT [110] might provide a suitable middle ground. Finally, we remark that delta rule is closely related to meta or online learning via gradient descent [73; 39], which has been revisited in recent works like Longhorn [58] and TTT [110]. Recently, Titans [10] improves TTT by introducing a momentum and weight decay term.

## 7 Conclusion

We describe an algorithm that parallelizes DeltaNet training across the sequence length dimension, achieving significant speed-ups against existing implementations on modern hardware. This makes it possible to scale up DeltaNet to moderate-scale language modeling settings, where we find that it performs well compared to recent linear-recurrent baselines.

## Acknowledgements

This study was supported by funds from MIT-IBM Watson AI Lab and the MIT-Google Program for Computing Innovation. We are grateful to Mayank Mishra for assistance with training and evaluating the 3B models, to Kazuki Irie for valuable feedback on the draft, and to Simran Arora, Liliang Ren and Eric Alcaide for their insightful discussions. We also thank Michael Poli and Armin Thomas for sharing the raw results from the MAD benchmark experiment, as well as to Fan Zhou for identifying an error in Eq. 10.

## References

* Aksenov et al. [2024] Y. Aksenov, N. Balagansky, S. M. L. C. Vaina, B. Shaposhnikov, A. Gorbatovski, and D. Gavrilov. Linear Transformers with Learnable Kernel Functions are Better In-Context Models, 2024. URL https://arxiv.org/abs/2402.10644.
* Akyurek et al. [2024] E. Akyurek, B. Wang, Y. Kim, and J. Andreas. In-context language learning: Architectures and algorithms. _ArXiv preprint_, abs/2401.12973, 2024. URL https://arxiv.org/abs/2401.12973.
* Ali et al. [2024] A. Ali, I. Zimerman, and L. Wolf. The hidden attention of mamba models, 2024.
* Arora et al. [2023] S. Arora, S. Eyuboglu, A. Timalsina, I. Johnson, M. Poli, J. Zou, A. Rudra, and C. Re. Zoology: Measuring and improving recall in efficient language models. _ArXiv preprint_, abs/2312.04927, 2023. URL https://arxiv.org/abs/2312.04927.
* Arora et al. [2023] S. Arora, B. Yang, S. Eyuboglu, A. Narayan, A. Hojel, I. Trummer, and C. Re. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes, 2023. URL https://arxiv.org/abs/2304.09433.
* Arora et al. [2024] S. Arora, S. Eyuboglu, M. Zhang, A. Timalsina, S. Alberti, D. Zinsley, J. Zou, A. Rudra, and C. Re. Simple linear attention language models balance the recall-throughput tradeoff. _ArXiv preprint_, abs/2402.18668, 2024. URL https://arxiv.org/abs/2402.18668.
* Arora et al. [2024] S. Arora, A. Timalsina, A. Singhal, B. Spector, S. Eyuboglu, X. Zhao, A. Rao, A. Rudra, and C. Re. Just read twice: closing the recall gap for recurrent language models, 2024. URL https://arxiv.org/abs/2407.05483.
* Bahdanau et al. [2015] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Y. Bengio and Y. LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1409.0473.
* Beck et al. [2024] M. Beck, K. Poppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. _ArXiv preprint_, abs/2405.04517, 2024. URL https://arxiv.org/abs/2405.04517.
* Behrouz et al. [2024] A. Behrouz, P. Zhong, and V. Mirrokni. Titans: Learning to memorize at test time, 2024. URL https://arxiv.org/abs/2501.00663.
* Bischof and Loan [1985] C. H. Bischof and C. V. Loan. The WY representation for products of householder matrices. In _SIAM Conference on Parallel Processing for Scientific Computing_, 1985. URL https://api.semanticscholar.org/CorpusID:36094006.
* Bisk et al. [2020] Y. Bisk, R. Zellers, R. LeBras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 7432-7439. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6239.
* Blelloch [1990] G. E. Blelloch. Prefix sums and their applications. 1990.

* Brandon et al. [2023] W. Brandon, A. Nrusimha, K. Qian, Z. Ankner, T. Jin, Z. Song, and J. Ragan-Kelley. Striped Attention: Faster Ring Attention for Causal Transformers. _ArXiv preprint_, abs/2311.09431, 2023. URL https://arxiv.org/abs/2311.09431.
* Chou et al. [2024] Y. Chou, M. Yao, K. Wang, Y. Pan, R.-J. Zhu, J. Wu, Y. Zhong, Y. Qiao, B. XU, and G. Li. MetaLA: Unified optimal linear approximation to softmax attention map. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024. URL https://openreview.net/forum?id=Y8YVCOMEpz.
* Clark et al. [2018] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _ArXiv preprint_, abs/1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.
* Clevert et al. [2016] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). In Y. Bengio and Y. LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1511.07289.
* Dao [2023] T. Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. _ArXiv preprint_, abs/2307.08691, 2023. URL https://arxiv.org/abs/2307.08691.
* Dao and Gu [2024] T. Dao and A. Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. _arXiv preprint arXiv: 2405.21060_, 2024.
* December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.
* De et al. [2024] S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N. De Freitas, and C. Gulcehre. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models, 2024. URL https://arxiv.org/abs/2402.19427.
* Demircigil et al. [2017] M. Demircigil, J. Heusel, M. Lowe, S. Upgang, and F. Vermet. On a model of associative memory with huge storage capacity. _ArXiv preprint_, abs/1702.01929, 2017. URL https://arxiv.org/abs/1702.01929.
* Dominguez and Orti [2018] A. E. T. Dominguez and E. S. Q. Orti. Fast blocking of household reflectors on graphics processors. _2018 26th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)_, pages 385-393, 2018. URL https://api.semanticscholar.org/CorpusID:46960439.
* Elfwing et al. [2017] S. Elfwing, E. Uchibe, and K. Doya. Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning, 2017. URL https://arxiv.org/abs/1702.03118.
* Fathi et al. [2023] M. Fathi, J. Pilault, P.-L. Bacon, C. Pal, O. Firat, and R. Goroshin. Block-state transformer. _ArXiv preprint_, abs/2306.09539, 2023. URL https://arxiv.org/abs/2306.09539.
* Fu et al. [2023] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=C0ZDy0WYGg.
* Gao et al. [2021] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 2021.
* Gardner [1988] E. Gardner. The space of interactions in neural network models. _Journal of Physics A_, 21:257-270, 1988.

* Graves et al. [2014] A. Graves, G. Wayne, and I. Danihelka. Neural Turing Machines, 2014. URL http://arxiv.org/abs/1410.5401. arXiv:1410.5401 [cs].
* Griffin and Teams [2024] R. Griffin and G. Teams. Recurrentgemma: Moving past transformers for efficient open language models. _ArXiv preprint_, abs/2404.07839, 2024. URL https://arxiv.org/abs/2404.07839.
* Gu and Dao [2023] A. Gu and T. Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. In _Proceedings of COLM_, 2023.
* Gu et al. [2022] A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022._ OpenReview.net, 2022. URL https://openreview.net/forum?id=uYLFo2lvlAC.
* Helfrich et al. [2018] K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled cayley transform. In J. G. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 1974-1983. PMLR, 2018. URL http://proceedings.mlr.press/v80/helfrich18a.html.
* Hua et al. [2022] W. Hua, Z. Dai, H. Liu, and Q. V. Le. Transformer quality in linear time. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 9099-9117. PMLR, 2022. URL https://proceedings.mlr.press/v162/hua22a.html.
* Huang et al. [2023] F. Huang, K. Lu, Y. Cai, Z. Qin, Y. Fang, G. Tian, and G. Li. Encoding recurrence into transformers. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023._ OpenReview.net, 2023. URL https://openreview.net/pdf?id=YYfHla7lxBJ.
* Irie and Schmidhuber [2023] K. Irie and J. Schmidhuber. Images as weight matrices: Sequential image generation through synaptic learning rules. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023._ OpenReview.net, 2023. URL https://openreview.net/pdf?id=dda0PNUvV.
* Irie et al. [2021] K. Irie, I. Schlag, R. Csordas, and J. Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 7703-7717, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html.
* Irie et al. [2021] K. Irie, I. Schlag, R. Csordas, and J. Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 7703-7717, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef90805d-Abstract.html.
* Irie et al. [2022] K. Irie, R. Csordas, and J. Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 9639-9659. PMLR, 2022. URL https://proceedings.mlr.press/v162/irie22a.html.
* Irie et al. [2021] K. Irie, F. Faccio, and J. Schmidhuber. Neural differential equations for learning to program neural nets through continuous learning rules. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/fc09b26b85ab3abb2832bd555a2e4215-Abstract-Conference.html.
* Irie et al. [2022] K. Irie, I. Schlag, R. Csordas, and J. Schmidhuber. A modern self-referential weight matrix that learns to modify itself. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 9660-9677. PMLR, 2022. URL https://proceedings.mlr.press/v162/irie22b.html.
* Irie et al. [2023] K. Irie, R. Csordas, and J. Schmidhuber. Practical computational power of linear transformers and their recurrent and self-referential extensions. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9455-9465, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emmlp-main.588. URL https://aclanthology.org/2023.emmlp-main.588.
* Jing et al. [2019] L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated Orthogonal Recurrent Units: On Learning to Forget. _Neural Computation_, 31(4):765-783, 2019. ISSN 0899-7667, 1530-888X. doi: 10.1162/neco_a_01174. URL https://direct.mit.edu/neco/article/31/4/765-783/8458.
* Joffrain et al. [2006] T. Joffrain, T. M. Low, E. S. Quintana-Orti, R. A. van de Geijn, and F. G. V. Zee. Accumulating householder transformations, revisited. _ACM Trans. Math. Softw._, 32:169-179, 2006. URL https://api.semanticscholar.org/CorpusID:15723171.
* Kacham et al. [2023] P. Kacham, V. Mirrokni, and P. Zhong. Polysketchformer: Fast transformers via sketches for polynomial kernels. _ArXiv preprint_, abs/2310.01655, 2023. URL https://arxiv.org/abs/2310.01655.
* Kang et al. [2023] Y. Kang, G. Tran, and H. De Sterck. Fast multipole attention: A divide-and-conquer attention mechanism for long sequences. _ArXiv preprint_, abs/2310.11960, 2023. URL https://arxiv.org/abs/2310.11960.
* Kasai et al. [2021] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas, Y. Mao, W. Chen, and N. A. Smith. Finetuning pretrained transformers into RNNs. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10630-10643, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emmlp-main.830. URL https://aclanthology.org/2021.emmlp-main.830.
* Katharopoulos et al. [2020] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 5156-5165. PMLR, 2020. URL http://proceedings.mlr.press/v119/katharopoulos20a.html.
* Kitaev et al. [2020] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB.
* Krotov and Hopfield [2021] D. Krotov and J. J. Hopfield. Large associative memory problem in neurobiology and machine learning. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=X4y_100X-hX.
* Lei [2021] T. Lei. When attention meets fast recurrence: Training language models with reduced compute. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7633-7648, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emmlp-main.602. URL https://aclanthology.org/2021.emmlp-main.602.

* Lei et al. [2022] T. Lei, R. Tian, J. Bastings, and A. P. Parikh. Simple recurrence improves masked language models. _ArXiv preprint_, abs/2205.11588, 2022. URL https://arxiv.org/abs/2205.11588.
* Lewis et al. [2020] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html.
* Liao et al. [2024] B. Liao, X. Wang, L. Zhu, Q. Zhang, and C. Huang. Vig: Linear-complexity visual sequence learning with gated linear attention. _ArXiv_, abs/2405.18425, 2024. URL https://api.semanticscholar.org/CorpusID:270068351.
* Lieber et al. [2024] O. Lieber, B. Lenz, H. Bata, G. Cohen, J. Osin, I. Dalmedigos, E. Safahi, S. Meirom, Y. Belinkov, S. Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language model. _ArXiv preprint_, abs/2403.19887, 2024. URL https://arxiv.org/abs/2403.19887.
* Lingashetty [2010] K. C. Lingashetty. Delta learning rule for the active sites model. _arXiv preprint arXiv:1007.0417_, 2010.
* Lingle [2023] L. D. Lingle. Transformer-vq: Linear-time transformers via vector quantization. _ArXiv preprint_, abs/2309.16354, 2023. URL https://arxiv.org/abs/2309.16354.
* Liu et al. [2024] B. Liu, R. Wang, L. Wu, Y. Feng, P. Stone, and Q. Liu. Longhorn: State space models are amortized online learners. _ArXiv preprint_, abs/2407.14207, 2024. URL https://arxiv.org/abs/2407.14207.
* Liu et al. [2023] H. Liu, M. Zaharia, and P. Abbeel. Ring Attention with Blockwise Transformers for Near-Infinite Context. _ArXiv preprint_, abs/2310.01889, 2023. URL https://arxiv.org/abs/2310.01889.
* Lockard et al. [2019] C. Lockard, P. Shiralkar, and X. L. Dong. OpenCeres: When open information extraction meets the semi-structured web. In J. Burstein, C. Doran, and T. Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 3047-3056, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1309. URL https://aclanthology.org/N19-1309.
* Ma et al. [2023] X. Ma, C. Zhou, X. Kong, J. He, L. Gui, G. Neubig, J. May, and L. Zettlemoyer. Mega: Moving average equipped gated attention. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=qNLe3iq2E1.
* Ma et al. [2024] X. Ma, X. Yang, W. Xiong, B. Chen, L. Yu, H. Zhang, J. May, L. Zettlemoyer, O. Levy, and C. Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. _ArXiv preprint_, abs/2404.08801, 2024. URL https://arxiv.org/abs/2404.08801.
* Mao [2022] H. H. Mao. Fine-tuning pre-trained transformers into decaying fast weights. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 10236-10242, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.697. URL https://aclanthology.org/2022.emnlp-main.697.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=HyUNwulC-.
- May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=HyUNwu1C-.
* Mathiasen et al. [2020] A. Mathiasen, F. Hvilshoj, J. R. Jorgensen, A. Nasey, and D. Mottin. What if neural networks had svds? _ArXiv_, abs/2009.13977, 2020. URL https://api.semanticscholar.org/CorpusID:221995651.
* Mathiasen et al. [2020] A. Mathiasen, F. Hvilshoj, J. R. Jorgensen, A. Nasey, and D. Mottin. Faster orthogonal parameterization with household matrices. In _ICML, Workshop Proceedings_, 2020.
* McEliece et al. [1987] R. J. McEliece, E. C. Posner, E. R. Rodemich, and S. S. Venkatesh. The capacity of the hopfield associative memory. _IEEE Trans. Inf. Theory_, 33:461-482, 1987.
* Mercat et al. [2024] J. Mercat, I. Vasiljevic, S. Keh, K. Arora, A. Dave, A. Gaidon, and T. Kollar. Linearizing large language models. _ArXiv preprint_, abs/2405.06640, 2024. URL https://arxiv.org/abs/2405.06640.
* Merrill et al. [2024] W. Merrill, J. Petty, and A. Sabharwal. The Illusion of State in State-Space Models, 2024. URL https://arxiv.org/abs/2404.08819.
* Mhammedi et al. [2017] Z. Mhammedi, A. D. Hellicar, A. Rahman, and J. Bailey. Efficient orthogonal parametrisation of recurrent neural networks using householder reflections. In D. Precup and Y. W. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 2401-2409. PMLR, 2017. URL http://proceedings.mlr.press/v70/mhammedi17a.html.
* Millidge [2019] B. Millidge. Linear Attention as Iterated Hopfield Networks. URL http://www.beren.io/2024-03-03-Linear-Attention-as-Iterated-Hopfield-Networks/.
* Munkhdalai et al. [2019] T. Munkhdalai, A. Sordoni, T. Wang, and A. Trischler. Metalearned neural memory. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. B. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 13310-13321, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/182bd81ea25270b7d1c2fe8353d17fe6-Abstract.html.
* Munkhdalai et al. [2024] T. Munkhdalai, M. Faruqui, and S. Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. _ArXiv preprint_, abs/2404.07143, 2024. URL https://arxiv.org/abs/2404.07143.
* Nahshan et al. [2023] Y. Nahshan, J. Kampeas, and E. Haleva. Linear Log-Normal Attention with Unbiased Concentration, 2023. URL https://arxiv.org/abs/2311.13541.
* Nguyen et al. [2021] T. M. Nguyen, V. Suliafu, S. J. Osher, L. Chen, and B. Wang. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 29449-29463, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/f621585df244e9596dc70a39b579efb1-Abstract.html.
* Paperno et al. [2016] D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In K. Erk and N. A. Smith, editors, _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1525-1534, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.
* Park et al. [2024] J. Park, J. Park, Z. Xiong, N. Lee, J. Cho, S. Oymak, K. Lee, and D. Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. _ArXiv preprint_, abs/2402.04248, 2024. URL https://arxiv.org/abs/2402.04248.

* Peng et al. [2024] B. Peng, D. Goldstein, Q. Anthony, A. Albalak, E. Alcaide, S. Biderman, E. Cheah, X. Du, T. Ferdinan, H. Hou, P. Kazienko, K. K. GV, J. Kocon, B. Koptyra, S. Krishna, R. McClelland Jr., N. Muennighoff, F. Obeid, A. Saito, G. Song, H. Tu, S. Wozniak, R. Zhang, B. Zhao, Q. Zhao, P. Zhou, J. Zhu, and R.-J. Zhu. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence, 2024. URL https://arxiv.org/abs/2404.05892.
* Peng et al. [2018] H. Peng, R. Schwartz, S. Thomson, and N. A. Smith. Rational recurrences. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 1203-1214, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1152. URL https://aclanthology.org/D18-1152.
* Peng et al. [2021] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong. Random feature attention. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB.
* Peng et al. [2022] H. Peng, J. Kasai, N. Pappas, D. Yogatama, Z. Wu, L. Kong, R. Schwartz, and N. A. Smith. ABC: Attention with bounded-memory control. In S. Muresan, P. Nakov, and A. Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7469-7483, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.515. URL https://aclanthology.org/2022.acl-long.515.
* Poli et al. [2023] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re. Hyena hierarchy: Towards larger convolutional language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 28043-28078. PMLR, 2023. URL https://proceedings.mlr.press/v202/poli23a.html.
* Poli et al. [2024] M. Poli, A. W. Thomas, E. Nguyen, P. Ponnusamy, B. Deiseroth, K. Kersting, T. Suzuki, B. Hie, S. Ermon, C. Re, C. Zhang, and S. Massaroli. Mechanistic Design and Scaling of Hybrid Architectures, 2024. URL https://arxiv.org/abs/2403.17844.
* Prados and Kak [1989] D. Prados and S. Kak. Neural network capacity using delta rule. _Electronics Letters_, 3(25):197-199, 1989.
* Qin et al. [2022] Z. Qin, X. Han, W. Sun, D. Li, L. Kong, N. Barnes, and Y. Zhong. The devil in linear transformer. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 7025-7041, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.473. URL https://aclanthology.org/2022.emnlp-main.473.
* Qin et al. [2023] Z. Qin, X. Han, W. Sun, B. He, D. Li, D. Li, Y. Dai, L. Kong, and Y. Zhong. Toeplitz neural network for sequence modeling. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=IxmWsm4xrua.
* Qin et al. [2023] Z. Qin, D. Li, W. Sun, W. Sun, X. Shen, X. Han, Y. Wei, B. Lv, F. Yuan, X. Luo, et al. Scaling transnormer to 175 billion parameters. _ArXiv preprint_, abs/2307.14995, 2023. URL https://arxiv.org/abs/2307.14995.
* Qin et al. [2023] Z. Qin, W. Sun, K. Lu, H. Deng, D. Li, X. Han, Y. Dai, L. Kong, and Y. Zhong. Linearized Relative Positional Encoding, 2023. URL https://arxiv.org/abs/2307.09270.
* 16, 2023_, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html.
* Qin et al. [2024] Z. Qin, W. Sun, D. Li, X. Shen, W. Sun, and Y. Zhong. Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. 2024.
* Qin et al. [2024] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y. Zhong. HGRN2: Gated Linear RNNs with State Expansion. 2024. URL https://api.semanticscholar.org/CorpusID:269043328.
* Rajpurkar et al. [2018] P. Rajpurkar, R. Jia, and P. Liang. Know what you don't know: Unanswerable questions for SQuAD. In I. Gurevych and Y. Miyao, editors, _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 784-789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology.org/P18-2124.
* Ramsauer et al. [2021] H. Ramsauer, B. Schafl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, T. Adler, D. P. Kreil, M. K. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield networks is all you need. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=tL89RnzIiCd.
* Ren et al. [2024] L. Ren, Y. Liu, Y. Lu, Y. Shen, C. Liang, and W. Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. _ArXiv preprint_, abs/2406.07522, 2024. URL https://arxiv.org/abs/2406.07522.
* Rodkin et al. [2024] I. Rodkin, Y. Kuratov, A. Bulatov, and M. Burtsev. Associative recurrent memory transformer. _ArXiv preprint_, abs/2407.04841, 2024. URL https://arxiv.org/abs/2407.04841.
* Rogers [1996] S. Rogers. Adaptive filter theory. _Control Engineering Practice_, 4:1629-1630, 1996. URL https://api.semanticscholar.org/CorpusID:109523942.
* Roy et al. [2021] A. Roy, M. Saffar, A. Vaswani, and D. Grangier. Efficient content-based sparse attention with routing transformers. _Transactions of the Association for Computational Linguistics_, 9:53-68, 2021. doi: 10.1162/tacl_a_00353. URL https://aclanthology.org/2021.tacl-1.
* Sakaguchi et al. [2020] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 8732-8740. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6399.
* Schiff et al. [2024] Y. Schiff, C.-H. Kao, A. Gokaslan, T. Dao, A. Gu, and V. Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling. _ArXiv_, abs/2403.03234, 2024. URL https://api.semanticscholar.org/CorpusID:268253280.
* Schlag et al. [2021] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9355-9366. PMLR, 2021. URL http://proceedings.mlr.press/v139/schlag21a.html.
* Schmidhuber [1992] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. _Neural Computation_, 4(1):131-139, 1992.
* Shazeer [2020] N. Shazeer. Glu variants improve transformer. _ArXiv preprint_, abs/2002.05202, 2020. URL https://arxiv.org/abs/2002.05202.
* Shen et al. [2024] Y. Shen, M. Stallone, M. Mishra, G. Zhang, S. Tan, A. Prasad, A. M. Soria, D. D. Cox, and R. Panda. Power scheduler: A batch size and token number agnostic learning rate scheduler. _ArXiv preprint_, abs/2408.13359, 2024. URL https://arxiv.org/abs/2408.13359.

* Smith et al. [2023] J. T. H. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence modeling. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=Ai8Hw3AXqks.
* Smith et al. [2023] J. T. H. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence modeling. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=Ai8Hw3AXqks.
* Sun et al. [2024] W. Sun, Z. Qin, D. Li, X. Shen, Y. Qiao, and Y. Zhong. Linear attention sequence parallelism. _ArXiv preprint_, abs/2404.02882, 2024. URL https://arxiv.org/abs/2404.02882.
* Sun et al. [2023] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models. _ArXiv preprint_, abs/2307.08621, 2023. URL https://arxiv.org/abs/2307.08621.
* Sun et al. [2024] Y. Sun, L. Dong, Y. Zhu, S. Huang, W. Wang, S. Ma, Q. Zhang, J. Wang, and F. Wei. You only cache once: Decoder-decoder architectures for language models. _ArXiv preprint_, abs/2405.05254, 2024. URL https://arxiv.org/abs/2405.05254.
* Sun et al. [2024] Y. Sun, X. Li, K. Dalal, J. Xu, A. Vikram, G. Zhang, Y. Dubois, X. Chen, X. Wang, O. Koyejo, T. Hashimoto, and C. Guestrin. Learning to (learn at test time): Rnn's with expressive hidden states. _ArXiv preprint_, abs/2407.04620, 2024. URL https://arxiv.org/abs/2407.04620.
* Team [2024] L. Team. The llama 3 herd of models. _ArXiv preprint_, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783.
* Tillet et al. [2019] P. Tillet, H. Kung, and D. D. Cox. Triton: an intermediate language and compiler for tiled neural network computations. In _Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, MAPL@PLDI 2019_, pages 10-19. ACM, 2019. doi: 10.1145/3315508.3329973.
* Tomczak and Welling [2016] J. M. Tomczak and M. Welling. Improving Variational Auto-Encoders using Householder Flow, 2016. URL https://arxiv.org/abs/1611.09630.
* Touvron et al. [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _ArXiv preprint_, abs/2302.13971, 2023. URL https://arxiv.org/abs/2302.13971.
* van den Berg et al. [2018] R. van den Berg, L. Hasenclever, J. M. Tomczak, and M. Welling. Sylvester normalizing flows for variational inference. In A. Globerson and R. Silva, editors, _Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018_, pages 393-402. AUAI Press, 2018. URL http://auai.org/uai2018/proceedings/papers/156.pdf.
* Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
* von Oswald et al. [2023] J. von Oswald, E. Niklasson, M. Schlegel, S. Kobayashi, N. Zucchet, N. Scherrer, N. Miller, M. Sandler, B. A. y Arcas, M. Vladymyrov, R. Pascanu, and J. Sacramento. Uncovering mesa-optimization algorithms in transformers. _CoRR_, abs/2309.05858, 2023. URL https://doi.org/10.48550/arXiv.2309.05858.
* Vorontsov et al. [2017] E. Vorontsov, C. Trabelsi, S. Kadoury, and C. Pal. On orthogonality and learning recurrent networks with long term dependencies. In D. Precup and Y. W. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 3570-3578. PMLR, 2017. URL http://proceedings.mlr.press/v70/vortontsov17a.html.
* Wang et al. [2024] C. X. Wang, O. Tsepa, J. Ma, and B. Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. _ArXiv_, abs/2402.00789, 2024. URL https://api.semanticscholar.org/CorpusID:267364853.
* Wang et al. [2023] J. Wang, J. N. Yan, A. Gu, and A. Rush. Pretraining without attention. In H. Bouamor, J. Pino, and K. Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 58-69, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.5. URL https://aclanthology.org/2023.findings-emnlp.5.
* Widrow et al. [1960] B. Widrow, M. E. Hoff, et al. Adaptive switching circuits. In _IRE WESCON convention record_, volume 4, pages 96-104. New York, 1960.
* Yan et al. [2023] J. N. Yan, J. Gu, and A. M. Rush. Diffusion models without attention. 2023.
* Yang and Zhang [2024] S. Yang and Y. Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, 2024. URL https://github.com/sustcsonglin/flash-linear-attention. original-date: 2023-12-20T06:50:18Z.
* Yang et al. [2023] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training. _ArXiv preprint_, abs/2312.06635, 2023. URL https://arxiv.org/abs/2312.06635.
* Yang et al. [2024] S. Yang, J. Kautz, and A. Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule, 2024. URL https://arxiv.org/abs/2412.06464.
* Zaheer et al. [2020] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big bird: Transformers for longer sequences. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html.
* Zellers et al. [2019] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. Traum, and L. Marquez, editors, _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.
* Zhang et al. [2018] J. Zhang, Q. Lei, and I. S. Dhillon. Stabilizing gradients for deep neural networks via efficient SVD parameterization. In J. G. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 5801-5809. PMLR, 2018. URL http://proceedings.mlr.press/v80/zhang18g.html.
* Zhang et al. [2023] J. Zhang, S. Jiang, J. Feng, L. Zheng, and L. Kong. Linear Attention via Orthogonal Memory, 2023. URL https://arxiv.org/abs/2312.11135.
* Zhang et al. [2023] Q. Zhang, D. Ram, C. Hawkins, S. Zha, and T. Zhao. Efficient long-range transformers: You need to attend more, but not necessarily at every layer. In H. Bouamor, J. Pino, and K. Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2775-2786, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.183. URL https://aclanthology.org/2023.findings-emnlp.183.
* Zhang et al. [2024] Y. Zhang, S. Yang, R. Zhu, Y. Zhang, L. Cui, Y. Wang, B. Wang, F. Shi, B. Wang, W. Bi, P. Zhou, and G. Fu. Gated slot attention for efficient linear-time sequence modeling. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.

* Zhu et al. [2024] L. Zhu, Z. Huang, B. Liao, J. H. Liew, H. Yan, J. Feng, and X. Wang. Dig: Scalable and efficient diffusion models with gated linear attention. _ArXiv_, abs/2405.18428, 2024. URL https://api.semanticscholar.org/CorpusID:270068366.
* Zhu et al. [2024] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. _ArXiv_, abs/2401.09417, 2024. URL https://api.semanticscholar.org/CorpusID:267028142.
* Zimerman et al. [2024] I. Zimerman, A. Ali, and L. Wolf. A unified implicit attention formulation for gated-linear recurrent sequence models. _ArXiv preprint_, abs/2405.16504, 2024. URL https://arxiv.org/abs/2405.16504.

Experiments Continued

### Hyperparameters

We used 8 H100 GPUs for 340M and 1.3B language modeling experiments. Each model uses AdamW for optimization, with a peak learning rate of \(3\times 10^{-4}\). The 340M models are trained using 15 billion tokens and a batch size of 0.5M tokens, while the 1.3B models are trained with 100 billion tokens and a batch size of 2M tokens. We use a cosine learning rate schedule, starting with a warm-up phase of 0.5 billion tokens for the 340M models and 1 billion tokens for the 1.3B models. Both configurations have initial and final learning rates set at \(3\times 10^{-5}\). We apply a weight decay of 0.01 and use gradient clipping at a maximum of 1.0. The head dimension of DeltaNet is set to 128, and the kernel size for convolution layers is set at 4.

### Synthetic tasks

We additional conduct experiments on RegBench [2], a synthetic data set designed to assess the in-context language learning capability of different model architectures. Each input sequence in this benchmark consists of 10 to 20 strings drawn from a distinct language defined by a probabilistic finite automaton (PFA), so that a model needs to infer the underlying language from the context on the fly. During testing, a model is evaluated on predicting the next token of testing sequences generated from held-out PFAs. Here again we find that DeltaNet performs strongly compared to baselines, as shown in Figure 7.

## Appendix B Method Continued

### WY representation derivation

To reduce notational clutter, we discuss only the first chunk here.

We first show \(\mathbf{P}_{n}=\mathbf{I}-\sum_{t=1}^{n}\bm{w}_{t}\bm{k}_{t}^{\top}\) by induction,

\[\mathbf{P}_{n} =\prod_{t=1}^{n}(\mathbf{I}-\beta_{t}\bm{k}_{t}\bm{k}_{t}^{\top})\] \[=\mathbf{P}_{n-1}(\mathbf{I}-\beta_{n}\bm{k}_{n}\bm{k}_{n}^{\top})\] \[=(\mathbf{I}-\sum_{t=1}^{n-1}\bm{w}_{t}\bm{k}_{t}^{\top})( \mathbf{I}-\beta_{n}\bm{k}_{n}\bm{k}_{n}^{\top})\] \[=\mathbf{I}-\sum_{t=1}^{n-1}\bm{w}_{t}\bm{k}_{t}^{\top}-\beta_{n} \bm{k}_{n}\bm{k}_{n}^{\top}+(\sum_{t=1}^{n-1}\bm{w}_{t}\bm{k}_{t}^{\top})\beta _{n}\bm{k}_{n}\bm{k}_{n}^{\top}\] \[=\mathbf{I}-\sum_{t=1}^{n-1}\bm{w}_{t}\bm{k}_{t}^{\top}-\underbrace {\left(\beta_{n}\bm{k}_{n}-\beta_{n}\sum_{t=1}^{n-1}\left(\bm{w}_{t}(\bm{k}_{ t}^{\top}\bm{k}_{n})\right)\right)}_{\bm{w}_{n}}\bm{k}_{n}^{\top}\] \[=\mathbf{I}-\sum_{t=1}^{n}\bm{w}_{t}\bm{k}_{t}^{\top}\]

Figure 7: Accuracy (%) on RegBench.

[MISSING_PAGE_EMPTY:23]

Related Work Continued

Chunkwise linear attention.Hua et al. [34] first proposed chunkwise form for linear attention; however, they used a hybrid linear and nonlinear attention model similar to Munkhdalai et al. [74]. It is possible to adapt their algorithm to compute the _exact_ output of the pure linear attention, as shown in Sun et al. [108] and Yang et al. [124]. The chunkwise linear attention algorithm has also been independently discovered in several works [108; 45; 19]. Yang et al. [124] and Qin et al. [91] discuss I/O-aware hardware optimization for chunkwise linear attention and Sun et al. [107] make generalization to multi-node distributed training. Inspired by the chunkwise form, we propose a new algorithm for hardware-efficient DeltaNet training, significantly improving the training efficiency and allowing for large-scale experiments.

Hybrid models.Linear recurrent models - including state-space models [32; 105; 31; 120], gated linear RNNs [65; 90], and linear attention mechanisms [48; 124] - have demonstrated remarkable potential as alternatives to traditional softmax attention across diverse domains [122; 132; 54; 133; 119; 100]. Given their complementary strengths, recent research has increasingly focused on developing hybrid architectures that combine linear recurrent layers with local chunk attention [61; 130; 25; 62; 74] or sliding window attention [130; 6; 21; 95] or global attention [51; 52; 35; 26; 55; 78; 109]. Poli et al. [84] systematically study the scaling law of hybrid models. We similarly show that combining DeltaNet with classic attention is an effective strategy.

Householder matrices.Householder matrices, known for preserving norms, are a type of orthogonal matrix extensively used in machine learning [67; 71; 128; 113; 89; 115]. These matrices allow for efficient computation of inverses and their Jacobian determinant of one, making them particularly suitable for applications in normalizing flows [67; 115]. Notably, Mathiasen et al. [67] and Mathiasen et al. [66] developed a chunkwise fast algorithm for computing the cumulative product of Householder matrices for normalizing flows, leveraging the WY representation. Our approach, while sharing the same high-level concept, tackles a different problem and is arguably more general.

There has also been significant interest in using orthogonal matrices to parameterize the transition matrices of RNNs [71; 43; 118; 33] for mitigating vanishing gradients. Mhammedi et al. [71] use the WY representation to reduce the memory footprint when training nonlinear RNNs with Householder transition matrices.

## Appendix D Pseudo code

```
1defchunk_delta_rule_forward(Q,K,V,beta,C):
2"""
3Q/K/V:query,key,valueofshape[L,d]
4beta:betaofshape[L]
5C:chunksize
6"""
7#L:sequencelength,d:headdimension
8L,d=Q.shape
9
10#chunking
11Q,K,V=map(lambdax:x.reshape(-1,C,d),[Q,K,V])
12beta=beta.reshape(-1,C)
13K_beta=K*beta.unsqueezeeze(-1)
14V_beta=V*beta.unsqueezeeze(-1)
15
16#computeeq.10withvectorizedforwordsubstitutionforfastinverse
17T=-(K_beta@K.t()).tril(-1)
18foriinrange(1,C):
19T[i,:i]=T[i,:i]+(T[i,:,None]*T[:,:i]).sum(-2)
20T+=torch.eye(C)
21#computeEq.11
22W=T@K_beta
23U=T@V_beta
24#chunkwiseparallel.Eq.8-9
25S=torch.zeros(d,d)
26O=torch.empty_like(V)
27foriinrange(L//C):
28q_i,k_i,w_i=Q[i],K[i],W[i]
29u_i=U[i]-w_i@S
30o_inter=q_i@S
31A_i=(q_i@k_i.t()).tril()
32o_intra=A_i@u_i
33S+=k_i.t()@u_i
34Q[i]=o_intra+o_inter
35returnO.reshape(L,d) ```

Listing 1: Pytorch-like code snippet of the forward pass of our chunkwise algorithm for training DeltaNet. We omit the dimensions of batch size and number of heads for clarity.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This paper's contributions and scope are reflected in abstract and introduction part clearly. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations**Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of this work in SS5.3. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical results that require a full proof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: The paper provides sufficient details on hyperparameters and training procedures in SSA.1 to reproduce the results supporting its main conclusions. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code is publicly available at https://github.com/sustcsonglin/flash-linear-attention. Our primary training corpus is Slimpajama, an open-source dataset. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have detailed all the training and evaluation settings before the main results in the experimental part. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do not have enough resources to obtain error bars as running the experiments multiple times is computationally expensive due to the large model size. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: We provide information of GPU type and number of GPUs used for running our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work follows the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We foresee no potential societal impact of this work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We foresee no such risks posed by this work. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **License for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All of the datasets we use are publicly available at huggingface site, and we have properly cited all the training and evaluation datasets we used. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This work does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.