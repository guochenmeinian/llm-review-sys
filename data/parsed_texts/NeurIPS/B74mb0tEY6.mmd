# Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits

 Dorian Baudry\({}^{1,2,}\)

Equal contribution.

Hugo Richard\({}^{2,*}\)

Maria Cherifa\({}^{2,*}\)

Clement Calauzenes\({}^{2}\)

&Vianney Perchet\({}^{2}\)

\({}^{1}\) Department of Statistics, University of Oxford. \({}^{2}\)

\({}^{2}\) Inria Fairplay Joint team, CREST, ENSAE Paris, Criteo AI Lab

Equal contribution.

###### Abstract

Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function \(F\). In each auction \(t\), a decision-maker bound by limited observations selects \(n_{t}\) agents from a coalition of \(N\) to compete for a prize with \(p\) other agents, aiming to maximize the cumulative reward of the coalition across all auctions. The problem is framed as an \(N\)-armed structured bandit, each number of player sent being an arm \(n\), with expected reward \(r(n)\) fully characterized by \(F\) and \(p+n\). We present two algorithms, Local-Greedy (LG) and Greedy-Grid (GG), both achieving _constant_ problem-dependent regret. This relies on three key ingredients: **1.** an estimator of \(r(n)\) from feedback collected from any arm \(k\), **2.** concentration bounds of these estimates for \(k\) within an estimation neighborhood of \(n\) and **3.** the unimodality property of \(r\) under standard assumptions on \(F\). Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees. However, by avoiding to rely on confidence intervals, LG practically outperforms GG, as well as standard unimodal bandit algorithms such as OSUB or multi-armed bandit algorithms.

## 1 Introduction

The online display advertising has seen remarkable evolution in recent decades [14, 30, 33, 25]. Publishers, who are the suppliers of digital ad space on the internet, sell display spots for ads to advertisers through real-time bidding in spot auctions, with many of these auctions being conducted using first or second-price mechanisms [20]. Due to the technological complexity of online advertising, advertisers usually delegate the task of buying ad placements to demand-side platforms (DSP) that operate many advertising campaigns. This interaction between DSP and the publisher, can be simplified as the publisher acting as multiple ad auctions selling ad impressions (online displays), while the DSP acts as a _centralized coalition_: at each time step, it determines which campaign(s) from the coalition participate to the auction to maximize their total gain. The chosen campaign(s) then compete with others to secure impressions. The primary goal of advertising companies is then to maximize the cumulative utility: the total value of impressions won minus their costs. This raises a fundamental question: _how many ad campaigns should participate in the auction to optimize the overall utility?_ In the _interim_ setting, where the DSP observes current bidder values before deciding, it's known that only the highest value bidder should be sent. However, online privacy enhancements in browsers necessitate _ex-ante_ decisions from DSPs [8], without exact value knowledge. Here, the problem becomes challenging: choosing a small number of campaigns can make it difficult to secure impressions, while securing the spot with a large number of bidders inevitably raises the price dueto competition. In this paper, this problem is formalized and solved via novel Multi-Armed-Bandit (MAB) algorithms.

Problem statementConsider a sequence of \(T\) ad impressions sold through _second price auctions_ (see [20] for a survey). At auction \(t\in[T]\), each participant (bidder) bids on the item based on its own (stochastic) value for the item. The highest bidder wins the item and pays a price equal to the second highest bid. The _decision maker_ (the DSP) runs \(N\in\mathbb{N}^{*}\) advertising campaigns forming a _coalition_. At time \(t\), two groups of bidders participate: (1) \(n_{t}\in[N]\) bidders from the coalition chosen by the decision maker _ex-ante_ - without knowing the realization of the bidders' values - and (2) \(p\in\mathbb{N}^{*}\) other bidders, that we call the _competition_. When a bidder from the coalition wins the auction, the decision maker observes the realized value for the winner (also called _winning bid_). In the rest of the paper, the following assumptions about the behavior of bidders is made.

**Assumption 1**.: _All bidders are identical, their values are sampled i.i.d. from a distribution supported on \([0,1]\) characterized by its cumulative distribution function (c.d.f.) \(F\). All bidders bid their value._

Assuming identical bidders with i.i.d values is a strong but widespread assumption in auction theory [20, 22], known as the symmetric bidders case. It is particularly relevant in online advertising, notably in homogeneous impression markets where advertisers compete for similar ad displays due to shared objectives, target demographics, or placement competition. The bounded support assumption is also standard, as letting an automated system bid arbitrarily large values is unrealistic. Finally, bidders bid their value as this is a weakly dominant strategy in this case. Lastly, assuming a known number of competitors \(p\) is frequently seen in auction models (see for instance [20] chapter 3.2.2). Under Assumption 1, the expected reward received by the decision maker at time \(t\) is given by \(r(n_{t})\), where \(r\) is the _expected reward function_, defined by

\[r:n\in[N]\mapsto r(n)\coloneqq\mathbb{E}_{\mathbf{v}=(v_{i})_{t\in[n+p]} \sim F\times\dots\times F}\bigg{[}(\mathbf{v}_{(1)}-\mathbf{v}_{(2)})\mathds{1 }\bigg{\{}\operatorname*{argmax}_{i\in[n+p]}v_{i}\in[n]\bigg{\}}\bigg{]} \tag{1}\]

where \(\mathbf{v}_{(1)}\) and \(\mathbf{v}_{(2)}\) are respectively the first and second maximum of \(\mathbf{v}\), and \([n]\) is used to abbreviate \(\{1,\dots,n\}\). The problem therefore reduces to a MAB where the decision maker chooses _arms_\(n_{1},\dots,n_{T}\in[N]\) sequentially and aims to minimize its cumulative _expected regret_\(\mathcal{R}(T)\) defined by

\[\mathcal{R}(T)=\sum_{t\leq T}r(n^{*})-r(n_{t})\,\quad\text{with}\quad n^{*}= \operatorname*{argmax}_{n\in[N]}r(n), \tag{2}\]

given that privacy constraints from the browser [8] only let the decision maker observes (1) if the coalition won, (2) the realization of the maximum value when winning.

Related worksFollowing (2), the problem presented in this paper can be formulated as a Multi-Arm Bandits (MAB, see [21] for a survey). In MAB, a learner repeatedly selects from a set of actions, or "arms", each yielding a reward. The goal is to maximize total rewards by striking a balance between exploration (sampling various arms to learn their rewards) and exploitation (picking the arms with the highest anticipated rewards based on collected feedback). While the literature has known a significant development in the last years ([2, 7, 17], to name a few), the most popular approaches arguably remain _exponential weights algorithms_ (EXP3, [4]) in adversarial settings, and _optimism in face of uncertainty_ (UCB, [3]) when rewards are stochastic.

While UCB and EXP3 can both tackle the regret minimization problem presented here, they inevitably achieve sub-optimal performance due to not using the inherent _structure_ of the expected reward function. Several types of structure have been explored in the bandit literature, some notable examples being linear bandits [1], Lipschitz bandits [23], or unimodal bandits [9, 28, 29]. The problem considered here is novel in the literature of structured bandits, arising from the observability restrictions coming with privacy-enhancing systems. Still, in the next section we show that unimodality - in this paper the fact that \(r\) admits only one local (hence global) maximum - is in many cases inherited from this stronger structure. A typical strategy to exploit unimodality - also used in this work - consists in playing a standard bandit policy (such as UCB) on a well chosen subset of arms (OSUB, [9]).

Last, the use of online learning algorithms to tackle repeated auction problems have been explored in various contexts ([26, 12, 5, 32, 27, 11]). However, none of these works approach the problem through the perspective of a coalition of bidders, and are thus not applicable to this setting.

Outline and contributions.Section 3 presents two novel bandit algorithms: LG (Local Greedy) which is inspired by OSUB[9], and GG (Greedy Grid) which combines Local Greedy and a successive elimination strategy. Theorem 2 and Theorem 3 provide upper bounds on the regret of LG and GG respectively, which are summarized in Table 1. Both algorithms achieve problem-dependent regret independent of \(T\). However, their scaling differs: the regret of LG depends on the _worst local gap_\(\Delta=\min_{n\in[N]}|r(n+1)-r(n)|\), while for GG it only depends on the gaps \(\Delta_{n}=r(n^{*})-r(n)\). Furthermore, w.h.p. GG only suffers regret for arms in a _reference grid_\(\mathcal{S}\) containing \(\mathcal{O}(\log(N))\) arms and in a _neighborhood_\(\mathcal{B}^{*}\) of the optimal arm. All these quantities, as well as the notation \(\widetilde{\mathcal{O}}\) and \(\widetilde{\mathcal{O}}_{N}\) (hiding logarithmic factors), are defined in Section 3. These regret upper bounds rely on three key ingredients presented in Section 2: (1) an estimator of \(r(n)\) from feedback collected from any arm \(k\) (2) novel concentration bounds on these estimates for \(k\) within an estimation neighborhood of \(n\) (Theorem 1) and (3) the unimodality property of \(r\) under standard assumptions on \(F\). Lastly, Appendix D provides an experimental benchmark comparison of the performance of GG, LG and their competitors: LG has the lowest expected regret among the algorithms tested. Indeed, LG avoids the explicit use of the confidence bounds in the algorithm which makes it more practical, even though GG admits better theoretical guarantees.

## 2 Estimating the reward function from samples of powers of \(F\)

In this part, we put aside the sequential nature of the repeated auction setting that we introduced and consider the problem of estimating the expected reward as a function of the number of bidders, given a stream of collected data. We first present a formulation of the expected reward function in terms of powers of the c.d.f. \(F\). Then, we leverage this formula to introduce _power estimates_, as a solution to estimate the expected reward of an arm \(n\in[N]\) from samples collected from an arm \(k\in[N]\). Lastly, we discuss the theoretical properties of these estimates, introducing upper and lower confidence bounds on the expected reward in Theorem 1.

### Properties of the expected reward

The expected reward function \(r\) (Eq. (1)) can be expressed as a function of \(n\), \(p\) and the c.d.f. \(F\).

**Lemma 1**.: _The expected reward function defined in Equation (1) satisfies,_

\[n\in[N]\mapsto r(n)=n\int_{0}^{1}F^{p+n-1}(x)-F^{p+n}(x)\mathrm{d}x \tag{3}\]

The proof can be found in Appendix A.1 and is based on properties of order statistics.

This particular definition of \(r(n)\), which is a product of \(n\) and a function that decreases with \(n\), suggests that \(r\) could be unimodal for some choices of \(F\). In the rest of the paper, we restrict ourselves to distributions that guarantees unimodal reward functions.

**Assumption 2**.: \(F\) _and \(p\) are such that the reward function \(r\) in Equation (3) is unimodal_

As the next lemma shows, many classical distributions lead to unimodal rewards for all \(p\in\mathbb{N}\).

**Lemma 2**.: _Let \(F\) be the cumulative distribution function of a Bernoulli, truncated exponential or Complementary Beta distribution. Then, for any \(p\in\mathbb{N}^{*}\), \(r\) in Equation (3) unimodal._

\begin{table}
\begin{tabular}{l l} \hline \hline Algorithm & Regret upper bound \\ \hline EXP3[4] & \(\mathcal{O}(\sqrt{NT})\) \\ UCB1[3] & \(\mathcal{O}\left(\sum_{n\in[N]}\frac{\log(T)}{\Delta_{n}}\wedge\sqrt{NT}\right)\) \\ OSUB[9] & \(\mathcal{O}\left(\frac{\log(T)}{\Delta_{n+1}}+\frac{\log(T)}{\Delta_{n}-1}+ \sum_{n\in[N]}\frac{\Delta_{n}\log\log(T)}{\Delta^{2}}\right)\) \\ LG (this paper) & \(\widetilde{\mathcal{O}}_{N}(\sum_{n\in[N]}\frac{\Delta_{n}}{\Delta^{2}})\) \\ GG (this paper) & \(\widetilde{\mathcal{O}}_{N}(\sum_{n\in\mathcal{B}^{*}}\frac{1}{\Delta_{n}}+ \sum_{n\in\mathcal{S}}\frac{\Delta_{n}}{\Delta^{2}})\wedge\widetilde{\mathcal{ O}}(\sqrt{(\log(N)+|\mathcal{B}^{*}|)T})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of regret guarantees for different algorithmsThe proof of Lemma 2 can be found in Appendix A.2. Note that the Complementary Beta distributions [19], chosen for technical reasons, are similar to Beta distributions and any Beta distribution can be approached by a Complementary Beta. Furthermore, in Appendix A.3 we present experiments suggesting that \(r\) is unimodal for all \(p\in\mathbb{N}^{*}\) if \(F\) is the c.d.f of Beta or Kumaraswamy distributions. However, we also show in Appendix A.4 that this is not always the case, by providing a counter example. Nonetheless, we argue that (complementary) beta or truncated exponentials are flexible models for real world data, so Assumption 2 is reasonable in practice. We furthermore discuss in Section 3.2 the adaptation of our algorithms if this was not the case.

### Estimation of powers of \(F\)

Consider the feedback \(\overline{W_{k}}=(w_{k,1},\ldots,w_{k,m_{k}})\) gathered after playing arm \(k\) and winning the auction \(m_{k}\) times. \(\overline{W_{k}}\), represents the sequence of first values (value of the winning bid) which has been _collected by arm \(k\)_.

It is well known that the marginal distribution of any order statistic can be expressed as a function of the c.d.f. \(F\) (see Section 2.1 of [10]). The distribution of any element of \(\overline{W}_{k}\) has cumulative distribution function \(F_{k}:x\in[0,1]\to F^{k+p}(x)\), which clearly exhibits a one-to-one mapping between \(F_{k}(x)\) and \(F(x)\). Hence, given \(\overline{W}_{k}\), for any \(\ell\in\mathbb{N}\) we can estimate \(F^{\ell}\) by

\[\widetilde{F}_{k+p}^{\ell}:x\mapsto(\widehat{F}_{k+p}(x))^{\frac{\ell}{k+p}}, \text{ where }\widehat{F}_{k+p}:x\mapsto\frac{1}{m_{k}}\sum_{j=1}^{m_{k}} \mathds{1}\{w_{k,j}\leq x\}\text{ (emp. c.d.f. of }\overline{W_{k}}). \tag{4}\]

Estimation of \(r\)Consider any arm \(n\in[N]\). Following Equation (3), it appears that estimating both \(F^{n+p}\) and \(F^{n+p-1}\) is sufficient to construct an estimate of \(r(n)\). According to Equation (4), this can be done from samples originated from any arm \(k\in[N]\), by using the _simple estimate_

\[\widehat{r}_{k}(n)=n\int_{0}^{1}\left(\widetilde{F}_{k+p}^{n+p-1}(x)- \widetilde{F}_{k+p}^{n+p}(x)\right)\mathrm{d}x. \tag{5}\]

Furthermore, it also clear that any convex combination of estimates can become a new estimate, however in the rest of the paper we focus on simple estimates for simplicity.

**Remark 1** (Adaptation to different feedback).: _A similar procedure can be derived for a setting where the sequence of second prices would be observed instead. Indeed, their distribution would be \(G_{k}:x\in[0,1]\mapsto(k+p)F(x)^{k+p-1}-(k+p-1)F(x)^{k+p}\), which can lead to a reward estimate similar to (5) by using a suitable inversion formula. The same can be said for the case where both first and second prices are observed, with additional complexity because the joint distribution should be considered since for each auction the first and second price are dependent variables._

### Concentration of estimates of the reward function

We now introduce the first theoretical contribution of this paper: confidence bounds on the deviations of an empirical estimate \(\widehat{r}_{k}(n)\) w.r.t. the true expected reward \(r(n)\).

Importance of (relatively) local estimationIn principle, (5) suggests that samples from any arm \(k\in[N]\) can provide a simple estimate of the reward function of any other arm \(n\in[N]\). However, we establish that the position of \(k\) w.r.t. \(n\) significantly impacts the concentration of \(\widehat{r}_{k}(n)\). Intuitively, the ratio\((n+p)/(k+p)\) determines how the uncertainty on \(F^{k+p}\) propagates on the reward after performing the inversion to obtain an estimate of \(F^{n+p}\). Indeed, considering any \(i\in\mathbb{N}\), if for some \(x\in[0,1]\) the deviation \(F(x)^{i}-\widehat{F}_{i}(x)\) is small then a first order approximation provides that

\[\forall j\in\mathbb{N}\ :\ \ (F(x)^{i})^{\frac{i}{4}}-\widehat{F}_{i}(x)^{\frac{i}{4}} \approx(F(x)^{i}-\widehat{F}_{i}(x))\times\frac{j}{i}F_{i}(x)^{\frac{i}{4}-1}. \tag{6}\]

Hence, a small error on \(F(x)^{i}\) is multiplied by \(\frac{i}{4}F_{i}(x)^{\frac{i}{4}-1}\) to obtain the resulting error on \(F(x)^{j}\). For \(j\geq i\) this term can be as large as \(j/i\) while for \(j<i\) it can be arbitrarily large if \(F^{i}(x)\) is very small. This observation motivates a restriction on the range of arms that can be used to estimate the reward of a given arm \(n\), that we call its _estimation neighborhood_. We use the convention that arms smaller than \(1\) or greater than \(N\) exist but have not collected any sample and have a known reward of \(0\).

**Definition 1** (Estimation neighborhood of an arm \(n\)).: _Assume3 that \(p\geq 4\) Then, the estimation neighborhood of \(n\) is the range \(\mathcal{V}(n)=[v_{\ell}(n),v_{r}(n)]=\big{\{}k\in[N]:\ k+p\in\big{[}\frac{n+p}{2},\frac{3}{2}(n+p-1)\big{]}\big{\}}\). We call \(v_{\ell}(n)\) and \(v_{r}(n)\) respectively the furthest left and right neighbor of \(n\)._

Footnote 3: This assumption simplifies the presentation but our theoretical results can easily be adapted for \(p<4\) if \(n-1\) and \(n+1\) are included by default in \(\mathcal{V}(n)\).

**Theorem 1** (Concentration of simple estimates).: _Consider any \(n\in[N]\) and \(k\in\mathcal{V}(n)\). Let \(\widehat{\tau}_{k}(n)\) be defined according to (5) from \(m_{k}\) samples collected by \(k\). Then, there exists some constants \(\beta_{k,n}\) (depending on \(n,k,p\)) and \(\xi_{k,n,F}\) (additionally depending on \(F\)) such that, with probability \(1-\delta\),_

\[|\widehat{r}_{k}(n)-r(n)|\leq\beta_{k,n}\sqrt{\frac{\log\left(\frac{2\lceil n \sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}}}+n\times\xi_{k,n,F}\left(\frac{\log \left(\frac{2\lceil n\sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}}\right)^{\frac{n +p-1}{k+p}}. \tag{7}\]

_Furthermore, the constants admit universal upper bounds for any \(n,k,p,F\). For instance if \(m_{k}\geq 4\) it holds that \(\beta_{k,n}\leq 33\) and \(\gamma_{k,n,F}\leq 100\)._

Proof sketch (see Appendix B for the detailed proof).: The first ingredient consists in approximating the reward formulation (3) by a Riemann sum: for some step size \(D^{-1}>0\), it holds that \(\widehat{r}_{k}(n)-r(n)=\frac{n}{D}\sum_{s=0}^{D-1}\mathcal{E}(x_{s})+\text{ err}_{D}\), with \(x_{s}=s/D\) for all \(s\in\{0,\ldots,D-1\}\). In Lemma 4 we use elementary properties of \(F\) to show that the approximation error satisfies \(\text{err}_{D}\in[0,nD^{-1}]\). Next, we upper and lower bound \(\mathcal{E}(x_{s})\) with different concentration bounds according to the value of \(F_{k,s}\coloneqq F(x_{s})^{k+p}\). More precisely, for any \(\delta\in(0,1)\) the following bounds hold each with probability at least \(1-\delta\),

\[\begin{cases}|\widehat{F}_{k}(x_{s})-F_{k,s}|\leq\sqrt{F_{k,s}}\times\sqrt{ \frac{3\log\left(\frac{3}{2}\right)}{m_{k}}}\quad\text{if }F_{k,s}\in I_{0} \coloneqq\left[\frac{3\log(2/\delta)}{m_{k}},1\right]&\text{( Chernoff) },\\ \widehat{F}_{k}(x_{s})\leq\frac{6\log(2/\delta)}{m_{k}}\quad\text{ if }F_{k,s} \in I_{1}\coloneqq\left(\frac{\delta}{m_{k}},\frac{3\log(2/\delta)}{m_{k}} \right)&\text{( Chernoff) },\\ \widehat{F}_{k}(x_{s})=0\quad\text{if }F_{k,s}\in I_{2}\coloneqq\left[0,\frac{ \delta}{m_{k}}\right]&\text{(union bound) }.\end{cases}\]

These results are derived in Lemma 5 from a well-known multiplicative form of the Chernoff bound for Bernoulli random variables [16]. Then, the analysis consists in using the appropriate bound for each point \(s\in\{0,\ldots,D-1\}\). The interval \(I_{0}\) provides the first term in (7), which is dominant in terms of \(m_{k}\), and we make \(\beta_{k,n}\) fully independent of \(F\) by carefully using some properties of the reward function. The two remaining intervals \(I_{1}\) and \(I_{2}\) provide the second term in (7), and \(\gamma_{k,n,F}\) depend on \(F\) through the boundaries of the interval \(I_{1}\). The corresponding factor in \(\xi_{k,n,F}\) can be bounded by \(1\) or estimated in practice (see Appendix B.4). 

In Appendix B, we give the expression of \(\beta_{k,n}\) and \(\xi_{k,n,F}\) and provide in (17) and (19) fully explicit upper and lower confidence bounds on \(\widehat{r}_{k}(n)\), depending on all problem parameters, and that are much tighter than what the universal constants provided in the theorem suggest. These universal constants are purely indicative, in order to assess that \(\beta_{k,n}\) and \(\xi_{k,n,F}\) do not diverge for any value of the problem parameters. We now provide more high-level comments on the derivation of this result.

DiscussionThe proof of Theorem 1 is non-trivial, and the careful usage of the Chernoff bounds that we introduced is crucial to obtain tight bounds on \(\widehat{r}_{k}(n)\) for two reasons. First, it seems necessary to concentrate estimates from arms \(k>n\) (see the discussion below (6)), which are instrumental to the performance of the bandit algorithms presented in the next section. Secondly, by exhibiting powers of \(F\), they make \(\beta_{k,n}\)**not** increasing linearly in \(n\), which is not easy to achieve. Indeed, it is clear from the analysis that this cost would be inevitable with standard Hoeffding bounds. However, completely avoiding \(n\) seems difficult in general, so our proof provides a way to mitigate its cost by multiplying it by a higher power of \(m_{k}^{-1}\), at least \(m_{k}^{-\frac{2}{3}}\) (if \(k+p=\frac{3}{2}(n+p-1)\)). This is the theoretical motivation for the definition of \(\mathcal{V}(n)\) (Definition 1): while \(k+p=2(n+p-1)\) would lead to theoretically valid results, it would not ensure that the linear term in \(n\) is second-order in \(m_{k}\).

We now conclude this section by exhibiting a condition on \(F\) that allows to reduce the scaling of the confidence bound in \(n\) to logarithmic terms.

**Lemma 3** (Improved bound for Lipschitz quantile function).: _Assume that \(k\in\mathcal{V}(n)\) and \(F^{-1}\) is \(L\)-Lipschitz, then there exists an absolute constant \(\xi\) such that with probability \(1-\delta\) it holds that_

\[|\widehat{r}_{k}(n)-r(n)|\leq\beta_{k,n}\sqrt{\frac{\log\left(\frac{2\lceil n \sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}}}+\xi L\log\left(\frac{4\lceil n\sqrt {m_{k}}\rceil}{\delta}\right)\left(\frac{\log\left(\frac{2\lceil n\sqrt{m_{k}} \rceil}{\delta}\right)}{m_{k}}\right)^{\frac{n+p-1}{k+p}} \tag{8}\]

This result is proved in Appendix B.3, and shows that for some distributions (e.g. "close" to uniform) the confidence bounds converge relatively fast to standard sub-Gaussian type of bounds, even for very large \(n\). Whether this result holds in general remains open.

## 3 Bandit algorithms

### Bandit algorithms: Local-Greedy (Lg) and Greedy-Grid (Gg)

We now detail the two novel bandit algorithms proposed to tackle the problem presented in Section 1. Both rely on the use of simple estimates of \(r(n)\) (see Section 2) by arms present in its _estimation neighborhood_\(\mathcal{V}(n)\) (see Definition 1) and theoretically motivated by Theorem 1. In this section, for ease of exposition, we describe algorithms as if feedback was collected at every time steps. In Appendix C.1, we show that the algorithms and their guarantees only require a slight adaptation when the feedback is collected only when the auction is won.

#### 3.1.1 Local-Greedy

We first present Local-Greedy (Lg), which is a natural adaptation of a standard policy in unimodal bandits, QSUB[9].The main idea of QSUB is to play UCB locally around a reference arm, and eventually reach the optimal arm \(n^{*}\) by gradually moving the reference arm in its direction. With Lg, we adapt this principle to efficiently exploit the structure of the problem considered: at each round \(t\), Lg defines a reference arm \(\ell_{t}\), called _leader_, but plays _greedily_ in the _neighborhood_\(\mathcal{V}(\ell_{t})\), based on simple power estimates computed with samples from \(\ell_{t}\) only. In addition a _sampling requirement_, implemented by a parameter \(\alpha\in(0,1)\), is used in order to ensure the good concentration of these estimates. We detail Local-Greedy in Algorithm 1 below.

```
Input: exploration parameter \(\alpha\), neighborhoods \((\mathcal{V}(n))_{n\in[N]}\) (Definition 1)  Play \(n_{1}=1\) and observe \(w\sim F^{1+p}\) ; for\(t\geq 2\)do  Set \(\ell_{t}=n_{t-1}\), compute \((\widehat{r}_{\ell_{t}}(n))_{n\in\mathcal{V}(\ell_{t})}\) (Eq.(5)) ; \(\triangleright\) Compute estimates from the leader If\(m_{t}\coloneqq\lfloor\{s\in[t-1],n_{s}=\ell_{t}\}\rfloor\leq\alpha t\): play \(n_{t}=\ell_{t}\) ; \(\triangleright\) Linear sampling requirement Else: play \(n_{t}\in\operatorname*{argmax}_{n\in\mathcal{V}(\ell_{t})}\widehat{r}_{\ell_ {t}}(n)\) ; \(\triangleright\) Greedy play in \(\mathcal{V}(\ell_{t})\) Observe \(w\sim F^{n_{t}+p}\) ; \(\triangleright\) Record feedback
```

**Algorithm 1** Local Greedy (Lg)

High-level properties of LgFirst, using Greedy instead of UCB is only possible because of the structure of the problem: when \(\ell_{t}\) is well-explored the estimates of arms in \(\mathcal{V}(\ell_{t})\) computed with samples from \(\ell_{t}\) are sufficiently close to the true reward, so that _no exploration is needed_. The sampling requirement then guarantees that all greedy plays are made when \(\ell_{t}\) is well explored.

A second property is that since \(|\mathcal{V}(\ell_{t})|\) grows with \(\ell_{t}\), a sequence of _locally optimal moves_ (best play in a given neighborhood) allows to reach the optimal arm exponentially fast (in \(\mathcal{O}(\log(N))\) steps), which is particularly interesting in practice if \(N\) is large. On the other hand, Lg might suffer from the inherent drawback of any "local" policy: identifying a high-rewarding arm in a neighborhood can take a long time if the reward curve in this area is flat (depending on how small are the "local" gaps). This problem can be attenuated, but not solved, by adding an initial exploration phase. We propose Greedy-Grid, presented in the next section, as a way to fully address this issue.

Lastly, requiring only the computation of empirical reward estimates is a strength of Local-Greedy. Indeed, deriving tighter confidence bounds would improve its analysis, but not the practical implementation (and performance) of the algorithm.

#### 3.1.2 Greedy-grid

The concept of Greedy-Grid is very intuitive: it plays a Local-Greedy strategy only if it can tell which segment of the reward function contains the best arm with high probability. To implement this idea, GG uses a Successive-Elimination procedure [13] on a _subset_ of arms forming a _reference grid_, denoted by \(\mathcal{S}\).

Reference gridThe grid \(\mathcal{S}\) is designed so that two of its successive arms belong to their respective neighborhood (Definition 1), and can hence mutually estimate themselves and all arms in between (Theorem 1). In particular, the optimal arm can be well-estimated at least by its two closest neighbors on the grid, so its neighborhood can be "discovered" with high probability simply by sampling the points in the grid in a round-robin fashion for a sufficiently long time.

Following Definition 1, we construct \(\mathcal{S}\coloneqq\{s_{i}\}_{i\geq 1}\) recursively: \(s_{1}=1\), and for \(i\geq 2\) we set \(s_{i+1}=\max\left\{s\geq s_{i}:s\in[N],\ s\in\mathcal{V}(s_{i}),s_{i}\in \mathcal{V}(s)\right\}\). We provide an illustrative example below.

**Example 1**.: _For \(N=2000\) and \(p=100\) the grid is \(\mathcal{S}=\{1,50,123,233,398,645,1016,1572\}\)._

Any arm \(n\in[N]\) admits a left and right "neighbor in the grid", denoted respectively by \(v_{l}^{\mathcal{S}}(n)\) and \(v_{r}^{\mathcal{S}}(n)\) and defined by: \(v_{l}^{\mathcal{S}}(n)=0\) if \(n<\min\mathcal{S}\), \(v_{r}^{\mathcal{S}}(n)=N+1\) if \(n>\max\mathcal{S}\) and \((v_{l}^{\mathcal{S}}(n),v_{r}^{\mathcal{S}}(n))=\operatorname*{argmin}_{(x,y) \in\mathcal{S}\setminus\{n\}:\ n\in[x,y]}(y-x)\) otherwise. We call the "bin" of arm \(n\) all arms between its left and right neighbors: \(\mathcal{B}(n)=\{n\in[N],v_{l}^{\mathcal{S}}(n^{\prime})<n<v_{r}^{\mathcal{S}} (n^{\prime})\}\). For simplicity we use the notation \(\mathcal{B}^{\star}=\mathcal{B}(n^{\star})\)4.

Footnote 4: We assume a unique optimal arm for simplicity, but the analysis holds if several successive arms are optimal.

Greedy-GridWe provide the detailed implementation in Algorithm 2 below, and now describe the general principle of the algorithm. At each round, it operates in two steps. In the first step, it decides whether to play arms on the grid \(\mathcal{S}\) (play the grid, to simplify), or to focus on a specific bin (and, as we will see, _play greedy_). This choice depends on an elimination procedure: an arm \(k\) in \(\mathcal{S}\) should be _eliminated_ for this round if their _upper confidence bound_ (UCB) is smaller than the best _lower confidence bound_ (LCB) among all other arms. Furthermore, if there exists an eliminated arm whose index is closer to the index \(i_{t}^{*}\) of the arm with the best LCB, then the unimodality assumption implies that \(k\) should also be eliminated. The set of arms not eliminated at \(t\) is called \(\mathcal{C}_{t}\) in Algorithm 2.

To compute the UCB (\(U_{n}\)) and LCB (\(L_{n}\)) of an arm \(n\), we elect a leader \(\ell_{n}\) which is the arm in \([v_{l}^{\mathcal{S}}(n),v_{r}^{\mathcal{S}}(n)]\) that was played the most in the last \(t\) rounds and then compute the bounds based on \(\ell_{n}\), using Theorem 1. We show in the proof of Theorem 3 that this procedure ensures that a linear number of samples in \(t\) is used to compute the UCB and LCB of arms in \(\mathcal{B}^{\star}\) with high probability.

If at least one arm is not eliminated (\(\mathcal{C}_{t}\) is not empty), arms in \(\mathcal{C}_{t}\) are played one after the other (Round Robin). If all arms in the grid are eliminated, GG plays greedily in the bin \(B(i_{t}^{*})\) of the arm with the highest LCB. The empirical reward of each arm \(n\in B(i_{t}^{*})\) is computed similarly as \(U_{n}\) and \(L_{n}\) using samples from the leader \(\ell_{n}\). GG then plays the best empirical arm \(\alpha t\) times which is the same sampling requirement as LG.

The careful design of Greedy-Grid prevents the main theoretical drawback of Local-Greedy: since the algorithm has a very low probability to play in a sub-optimal bin, it almost never pays "local gaps" in a sub-optimal part of the reward function. However this guarantee comes at a cost: if \(n^{\star}\) is not in the grid, it will never be played until the confidence intervals shrink "sufficiently" to eliminate the entire grid. Hence, GG might be more conservative than LG in practice, while offering better theoretical guarantees. We express this trade-off in the next section.

### Regret upper bounds

We now present the theoretical results obtained for the two algorithms presented in Section 3. We first establish the regret bounds and sketch their proofs, before discussing and comparing the results. We introduce some notation, that considerably simplifies the presentation of the results.

Notation:\(\widetilde{\mathcal{O}}\) and \(\widetilde{\mathcal{O}}_{n}\)For any \(x>0\), we use the notation \(\widetilde{\mathcal{O}}(x)\) to describe a quantity that scales in \(x\), up to logarithmic terms **in \(x\) and \(N\)** (hence the notation is linked to the problem). Furthermore,for \(n\in[N]\) we also use \(\widetilde{\mathcal{O}}_{n}\) as a shorthand notation for \(\widetilde{\mathcal{O}}(\{n^{6}\lor x\}\wedge n^{2}x)\). This type of constants emerges from using (7) (Theorem 1) in the analysis. Indeed, we proved that the simple estimate of an arm \(n\) by an arm \(k\in\mathcal{V}(n)\) admit sub-Gaussian ("square-root") confidence intervals, independent of \(n\), when the sample size of \(k\) is larger than \(\Omega(n^{6})\).

**Theorem 2** (Regret bound for Local-Greedy).: _Let \(\Delta\coloneqq\min_{n\in[N-1]}|r(n+1)-r(n)|\) (worst local gap). Under Assumption 2 and with \(\alpha=(\log_{3/2}N+1)^{-1}\), the regret of LG is upper bounded by a problem-dependent constant: there exists \((C_{n})_{n\in[N]\setminus\{n^{\star}\}}\), each satisfying \(C_{n}=\widetilde{\mathcal{O}}_{N}\left(\frac{\Delta_{n}}{\Delta^{2}}\right)\), such that \(\mathcal{R}_{T}\leq\sum_{n\in[N]\setminus n^{\star}}C_{n}\)._

_Additionally, if the arm set forms a single estimation neighborhood, that is \(\forall n\in[N]:\ \mathcal{V}(n)\supset[N]\), then each constant \(C_{n}\) can be refined to \(\widetilde{\mathcal{O}}_{n}\left(\Delta_{n}^{-1}\right)\), providing \(\mathcal{R}_{T}=\widetilde{\mathcal{O}}(\sqrt{NT})\), which holds even when the reward function is not unimodal._

Proof sketch (see Appendix C.3 for the detailed proof).: We start by the case where the arm set forms a single neighborhood. Since LG is guaranteed that any arm it selects will provide an estimate for all the other arms, this context is very similar to a full information scenario. This explains why GG achieves both constant regret depending on the gaps, and a gap-independent bound in \(\sqrt{NT}\). Furthermore, the hidden logarithmic constants come from carefully using Theorem 1 to separate the linear term in \(n\) from the gaps when they are small.

The general case presents an additional complexity. Indeed, it is possible that playing arm \(n\neq n^{\star}\) is _locally optimal_, if \(n\) is the best arm in the neighborhood of the current leader: playing \(n\) in that context would not be unlikely. To tackle that scenario, we prove that pulling arm \(n\) at time \(t\) necessarily implies a _locally sub-optimal play_, in some estimation neighborhood, at some point in the past (maximized by the chosen value of \(\alpha\)). We then show that this cannot happen after some deterministic time w.h.p., leading to constant regret. However, since the sub-optimal play might be any arm the constant now depends on the _worst local gap_\(\Delta^{2}\). 

**Theorem 3** (Regret upper bound for Greedy-Grid).: _Suppose that GG is tuned with confidence level \(\delta_{t}=\frac{1}{N^{2}\epsilon^{3}}\), and \(\alpha=1/4\). Then, for any \(T\in\mathbb{N}\) it holds that_

\[\mathcal{R}_{T}=\widetilde{\mathcal{O}}_{N}\left(\sum_{n\in\mathcal{B}^{\star }}\frac{1}{\Delta_{n}}+\sum_{n\in\mathcal{S}}\frac{\log(T)}{\Delta_{n}}\wedge \Delta_{n}\left(\frac{\mathds{1}\{n<n^{\star}\}}{\Delta_{v_{l}(n^{\star})}^{2} }+\frac{\mathds{1}\{n>n^{\star}\}}{\Delta_{v_{r}(n^{\star})}^{2}}\right) \right)\.\]

_Additionally, it holds that \(\mathcal{R}_{T}=\widetilde{\mathcal{O}}\left(\sqrt{(K+|\mathcal{B}^{\star}|)T }\right)\), for \(K=\lfloor\log_{3/2}(N)\rfloor\)._

Proof sketch (see Appendix C.4 for the detailed proof).: First we prove that, w.h.p., during a linear time range in \(t\) GG either played the grid or in \(\mathcal{B}^{\star}\). Hence, arms \(n\in[N]\setminus\{\mathcal{S}\cup\mathcal{B}^{\star}\}\) are played a (universal!) constant number of times by GG in expectation. Then, for \(n\in\mathcal{S}\) the term in \(\frac{\log(T)}{\Delta_{n}}\) comes from the standard analysis of UCB [3]; while the constant bound comes from exploiting that after a constant time \(n\) the LCB of \(n^{*}\) eliminates its neighboors w.h.p., and by extension the entire grid. Finally, the constant bound \(n\in\mathcal{B}^{*}\) is derived similarly as the first bound of Theorem 2. 

DiscussionFirst, we show that being able to estimate \(r(n)\) from the feedback obtained after playing an arm \(k\) in its estimation neighborhood leads to a regret independent of \(T\) for both LG and GG. For the former, the bound depends in general on the worst _local gap_\(\Delta\), while for the latter only the actual gaps \(\Delta_{n}\) (with \(n^{\star}\)) are involved. This difference permits to obtain a problem-independent guarantee for GG for any configuration of \(p\) and \(N\). Furthermore, its scaling \(\sqrt{K+|\mathcal{B}^{*}|}\leq\sqrt{2n^{*}+\lfloor\log_{3/2}(N)\rfloor}\) can be much smaller than \(\sqrt{N}\) if \(n^{*}\) is small.

Then, we would like to discuss the impact of the concentration bound presented in Theorem 1 on the regret of both GG and LG. Indeed, a naive approach with Hoeffding bounds would not allow to remove \(n\) from the first order term of the concentration bound, because of the multiplicative factor \(n\) in the definition of \(r(n)\). A feature of our concentration bound is that the linear scaling in \(n\) does not appear in the first order term. Informally, this allows to exhibit terms of order \(\widetilde{\mathcal{O}}_{N}(\Delta_{n}^{-1})\) in the regret analysis instead of \(\widetilde{\mathcal{O}}(N^{2}\Delta_{n}^{-1})\), which can be significantly better for small gaps. A remark here is that the size of the grid in GG could be optimized as a larger grid makes the second order term in Theorem 1 smaller but is paid linearly in the regret.

We nevertheless highlight some potential for improvement in the analysis of LG. First, the local gaps \(\Delta\) in the bound of LG could be replaced by (in spirit, referring to \(\mathcal{S}\) for simplicity) \(\min_{n\in[N]}|r(v_{l}^{S}(n))-r(v_{r}^{S}(n))|\). It is clear though that this gap remains "local" and can be arbitrarily smaller than \(\Delta_{n}\) for some arms \(n\in[N]\), so the general interpretation of the results would be unchanged. Second, for simplicity, the analysis of LG was carried out using the constant upper bound of \(\beta_{k,n}\) and \(\xi_{k,n,F}\) but a tighter analysis could lead to a better dependency with respect to \(N\).

We now justify the use of simple estimates in GG and LG. In practice, combining estimates would allow to use more samples for the estimation. However, this would make the algorithm slower, and we believe that the sampling requirement implemented in the algorithms makes the use of simple estimates efficient: potential uniform exploration in a neighborhood is replaced by a focus on a single arm, but the same quality of information is accrued. Furthermore, from a theoretical perspective union bounds over the samples collected by each arm might also cost a factor \(N\) in the analysis.

Lastly, while GG admits better theoretical guarantees, LG might be more appealing in practice because it does not require to explicitly compute confidence intervals. This means that the regret bounds provided for LG are conservative, and might be refined with tighter confidence bounds without changing the algorithm.

Adaptation for non-unimodal rewardsWhile LG relies heavily on Assumption 2, GG can be readily adapted to handle non-unimodal reward functions. This is done by modifying the definition of the set of non-eliminated grid arms \(\mathcal{C}_{t}\) to \(\{s\in\mathcal{S},U_{s}\geq L_{i_{t}^{*}}\}\) in Algorithm 2. In that case, the algorithm can no longer eliminate arms on the grid based on the elimination of other arms. This naturally induces that the number of plays of sub-optimal arms is no longer bounded by a constant. In Theorem 4 (see Appendix), we show that only the \(\mathcal{O}(\log(T))\) term persists for \(n\in\mathcal{S}\) in Theorem 3, while the problem-independent bound remains unchanged. Although we believe unimodality is necessary for achieving constant regret, this result demonstrates that, even without that assumption, GG can still provide the same logarithmic regret guarantees as UCB. However, it does so on a \(|\mathcal{S}|\)-armed bandit, rather than an \(N\)-armed bandits with \(|\mathcal{S}|=\mathcal{O}(\log(N))\ll N\) for large \(N\).

Experimental resultsIn Appendix D we present a benchmark of LG, GG, UCB, EXP3 and OSUB on synthetic data in terms of the expected regret \(\mathcal{R}(T)\). This benchmark illustrates the strong performance of LG relative to the other approaches. Although GG offers more robust theoretical guarantees, particularly with sub-linear problem-independent bounds, LG proves to be more effective in practice. Several factors may explain this gap between theoretical guarantees and empirical performance. First, as discussed in the previous section, the worst-case local gap in the analysis of Local Greedy (Theorem 2) might be overly conservative. This worst-case scenario could occur under a combination of unfavorable conditions, such as poor initialization far from the optimal arm and a flat reward function, paired with bad luck in exploration. However, such a scenario is likely rare in practice and was not encountered in our experiments. Additionally, Local Greedy benefits from scenarioswhere it starts playing in the optimal neighborhood only after a few steps, a situation GG cannot exploit due to its need for sufficient statistical evidence to eliminate all suboptimal neighborhoods. While GG's caution leads to stronger theoretical guarantees, this comes at the cost of empirical performance. Moreover, GG's results are tied to the tightness of the confidence intervals in Theorem 2, a limitation that does not apply to LG. An interesting and challenging open problem remains whether LG can be modified to achieve the same theoretical guarantees as GG without sacrificing its performance. We leave this question for future work.

## 4 Conclusion

The bandit problem studied in this work is structured since playing arm \(n\) gives a reward \(r(n)\) determined by \(n\), \(p\) and the unknown c.d.f \(F\) and with probability \(\frac{n}{n+p}\) an observation of a sample of the distribution with c.d.f \(F^{n+p}\).

While traditional bandit approaches give problem dependent bounds depending on \(T\), algorithms GG and LG presented in this work have constant problem dependent bounds. Furthermore, GG and LG avoid a quadratic dependency in \(N\) for large \(T\) thanks to new concentration bounds introduced in Theorem 1. Overall, while GG has the best theoretical guarantees, LG has better constants and is therefore better suited for most practical problems (see the discussion at the end of Section 3 and experimental results in Appendix D).

Whether an algorithm that has the theoretical guarantees of GG and the practical performance of LG can be designed is an interesting question. We believe that the main leverage to improve the practical performance of GG might be to derive tighter concentration bounds. Possible directions to improve over Theorem 1 might include: further refining the decomposition of the integral in (3) according to the value of \(F\), further use "empirical" components (depending on estimates of \(F\)), or even using ideas from the proof of the DKW inequality [24] to avoid the union bounds over the points of each interval in the decomposition. We leave these directions for future work.

To conclude, since in practice, a DSP can launch campaigns through multiple auctions, an interesting question is whether the current analysis could be extended to the case of \(A\) auctions where a play at time \(t\) is \((n_{a,t})_{a\in[A]}\) where \(\sum_{a\in[A]}n_{a,t}=N\) and the reward is \(\sum_{a\in[A]}r_{a}(n_{a,t})\) with \(r_{a}\) determined by integers \(p_{a}\), \(n_{a,t}\) and \(F_{a}\) in the same way that \(r\) depends on \(p,n_{t}\) and \(F\). How to explore each auction in parallel in an efficient manner and how to handle the case where some auctions must be assigned zero players are then the main questions to solve.

## Acknowledgments

Dorian Baudry thanks the support of the French National Research Agency: ANR-19-CHIA-02 SCAI, ANR-22-SRSE-0009 Ocean, and ANR-23-CE23-0002 Doom. Dorian Baudry was partially funded by UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee [grant number EP/Y028333/1].

Vianney Perchet's research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

## References

* Abbasi-yadkori et al. [2011] Y. Abbasi-yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011. URL [https://proceedings.neurips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2011/file/e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf).
* Agrawal and Goyal [2012] S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In _Proceedings of the 25th Annual Conference on Learning Theory_, 2012.

* Auer et al. [2002] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine Learning_, 47:235-256, 2002. URL [https://api.semanticscholar.org/CorpusID:207609497](https://api.semanticscholar.org/CorpusID:207609497).
* Auer et al. [2002] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. _SIAM Journal on Computing_, 32(1):48-77, 2002. doi: 10.1137/S0097539701398375. URL [https://doi.org/10.1137/S0097539701398375](https://doi.org/10.1137/S0097539701398375).
* Babaioff et al. [2009] M. Babaioff, Y. Sharma, and A. Slivkins. Characterizing truthful multi-armed bandit mechanisms: extended abstract. In _Proceedings of the 10th ACM Conference on Electronic Commerce_, EC '09, page 79-88, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605584584. doi: 10.1145/1566374.1566386. URL [https://doi.org/10.1145/1566374.1566386](https://doi.org/10.1145/1566374.1566386).
* Baudry et al. [2024] D. Baudry, N. Merlis, M. B. Molina, H. Richard, and V. Perchet. Multi-armed bandits with guaranteed revenue per arm. In _International Conference on Artificial Intelligence and Statistics, 2-4 May 2024, Palau de Congressos, Valencia, Spain_, Proceedings of Machine Learning Research, 2024.
* Cappe et al. [2013] O. Cappe, A. Garivier, O.-A. Maillard, R. Munos, G. Stoltz, et al. Kullback-leibler upper confidence bounds for optimal sequential allocation. _The Annals of Statistics_, 41(3):1516-1541, 2013.
* Chatterjee and Sharfi [2024] P. Chatterjee and I. Sharfi. Bidding and auction services in the privacy sandbox. [https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_api.md](https://github.com/privacysandbox/protected-auction-services-docs/blob/main/bidding_auction_services_api.md), 2024.
* Combes and Proutiere [2014] R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. _ArXiv_, abs/1405.5096, 2014. URL [https://api.semanticscholar.org/CorpusID:15210470](https://api.semanticscholar.org/CorpusID:15210470).
* David and Nagaraja [2004] H. David and H. Nagaraja. _Order Statistics_. Wiley Series in Probability and Statistics. Wiley, 2004. ISBN 9780471654018. URL [https://books.google.fr/books?id=bdhzFXg6xFkC](https://books.google.fr/books?id=bdhzFXg6xFkC).
* Deng et al. [2023] Y. Deng, N. Golrezaei, P. Jaillet, J. C. N. Liang, and V. S. Mirrokni. Multi-channel autobidding with budget and roi constraints. _ArXiv_, abs/2302.01523, 2023. URL [https://api.semanticscholar.org/CorpusID:256598278](https://api.semanticscholar.org/CorpusID:256598278).
* Devanur and Kakade [2009] N. R. Devanur and S. M. Kakade. The price of truthfulness for pay-per-click auctions. In _Proceedings of the 10th ACM Conference on Electronic Commerce_, EC '09, page 99-106, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605584584. doi: 10.1145/1566374.1566388. URL [https://doi.org/10.1145/1566374.1566388](https://doi.org/10.1145/1566374.1566388).
* Even-Dar et al. [2006] E. Even-Dar, S. Mannor, Y. Mansour, and S. Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _Journal of machine learning research_, 7(6), 2006.
* Ha [2012] L. Ha. Online advertising research in advertising journals: A review. _Journal of Current Issues and Research in Advertising_, 30, 05 2012. doi: 10.1080/10641734.2008.10505236.
* Harris et al. [2020] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del Rio, M. Wiebe, P. Peterson, P. Gerard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL [https://doi.org/10.1038/s41586-020-2649-2](https://doi.org/10.1038/s41586-020-2649-2).
* Hoeffding [1994] W. Hoeffding. Probability inequalities for sums of bounded random variables. _The collected works of Wassily Hoeffding_, pages 409-426, 1994.
* Honda and Takemura [2015] J. Honda and A. Takemura. Non-asymptotic analysis of a new bandit algorithm for semi-bounded rewards. _Journal of Machine Learning Research_, 16:3721-3756, 2015.

* [18] J. D. Hunter. Matplotlib: A 2d graphics environment. _Computing in Science & Engineering_, 9(3):90-95, 2007. doi: 10.1109/MCSE.2007.55.
* [19] M. Jones. The complementary beta distribution. _Journal of Statistical Planning and Inference_, 104(2):329-337, 2002. ISSN 0378-3758. doi: [https://doi.org/10.1016/S0378-3758](https://doi.org/10.1016/S0378-3758)(01)00260-9. URL [https://www.sciencedirect.com/science/article/pii/S0378375801002609](https://www.sciencedirect.com/science/article/pii/S0378375801002609).
* [20] V. Krishna. _Auction Theory_. Academic Press, 2009.
* [21] T. Lattimore and C. Szepesvari. Bandit algorithms. 2017. URL [https://tor-lattimore.com/downloads/book/book.pdf](https://tor-lattimore.com/downloads/book/book.pdf).
* [22] J. Levin. Auction theory. _Manuscript available at www. stanford. edu/jdlevin/Econ_, 20286, 2004.
* [23] S. Magureanu, R. Combes, and A. Proutiere. Lipschitz bandits: Regret lower bound and optimal algorithms. In M. F. Balcan, V. Feldman, and C. Szepesvari, editors, _Proceedings of The 27th Conference on Learning Theory_, volume 35 of _Proceedings of Machine Learning Research_, pages 975-999, Barcelona, Spain, 13-15 Jun 2014.
* [24] P. Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality. _The Annals of Probability_, 18(3):1269-1283, 1990. ISSN 00911798. URL [http://www.jstor.org/stable/224426](http://www.jstor.org/stable/224426).
* [25] S. Muthukrishnan. Ad exchanges: Research issues. In _Workshop on Internet and Network Economics_, 2009. URL [https://api.semanticscholar.org/CorpusID:10046036](https://api.semanticscholar.org/CorpusID:10046036).
* [26] H. Nazerzadeh, A. Saberi, and R. Vohra. Dynamic cost-per-action mechanisms and applications to online advertising. WWW '08, page 179-188, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605580852. doi: 10.1145/1367497.1367522. URL [https://doi.org/10.1145/1367497.1367522](https://doi.org/10.1145/1367497.1367522).
* [27] T. Nedelec, C. Calauzenes, N. El Karoui, and V. Perchet. 2022.
* [28] S. Paladino, F. Trovo, M. Restelli, and N. Gatti. Unimodal thompson sampling for graph-structured arms. In S. Singh and S. Markovitch, editors, _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA_. AAAI Press, 2017.
* [29] H. Saber, P. M'enard, and O.-A. Maillard. Forced-exploration free strategies for unimodal bandits. _ArXiv_, abs/2006.16569, 2020. URL [https://api.semanticscholar.org/CorpusID:220265988](https://api.semanticscholar.org/CorpusID:220265988).
* [30] A. S. Sayedi-Roshkhar. Real-time bidding in online display advertising. _Mark. Sci._, 37:553-568, 2018. URL [https://api.semanticscholar.org/CorpusID:52277027](https://api.semanticscholar.org/CorpusID:52277027).
* [31] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
* [32] J. Weed, V. Perchet, and P. Rigollet. Online learning in repeated auctions. In V. Feldman, A. Rakhlin, and O. Shamir, editors, _29th Annual Conference on Learning Theory_, volume 49 of _Proceedings of Machine Learning Research_, pages 1562-1583, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL [https://proceedings.mlr.press/v49/weed16.html](https://proceedings.mlr.press/v49/weed16.html).
* [33] Y. Yuan, J. Li, and R. Qin. A survey on real time bidding advertising. _Proceedings of 2014 IEEE International Conference on Service Operations and Logistics, and Informatics, SOLI 2014_, pages 418-423, 11 2014. doi: 10.1109/SOLI.2014.6960761.

Properties of the expected reward function

In this appendix we prove the results presented in Section 2.1 of the paper, and discuss the shape of the expected reward.

### Proof of Lemma 1

**Lemma 1**.: _The expected reward function defined in Equation (1) satisfies,_

\[n\in[N]\mapsto r(n)=n\int_{0}^{1}F^{p+n-1}(x)-F^{p+n}(x)\mathrm{d}x \tag{3}\]

Proof.: Given \(\mathbf{v}=(v_{i})_{i\in[n+p]}\sim F\times\cdots\times F\), we have

\[r(n) =\mathbb{E}\bigg{[}(\mathbf{v}_{(1)}-\mathbf{v}_{(2)})\mathds{1} \bigg{\{}\operatorname*{argmax}_{i\in[n+p]}v_{i}\in[n]\bigg{\}}\bigg{]}\] \[\stackrel{{(i)}}{{=}}\mathbb{E}\bigg{[}(\mathbf{v} _{(1)}-\mathbf{v}_{(2)})\bigg{]}\mathbb{E}\bigg{[}\mathds{1}\bigg{\{} \operatorname*{argmax}_{i\in[n+p]}v_{i}\in[n]\bigg{\}}\bigg{]}\] \[\stackrel{{(ii)}}{{=}}\bigg{(}\mathbb{E}\bigg{[} \mathbf{v}_{(1)}\bigg{]}-\mathbb{E}\bigg{[}\mathbf{v}_{(2)}\bigg{]}\bigg{)} \times\frac{n}{n+p}\] \[=\frac{n}{n+p}\times\int_{0}^{1}\mathbb{P}(\mathbf{v}_{(1)}>x)- \mathbb{P}(\mathbf{v}_{(2)}>x)\mathrm{d}x\] \[=\frac{n}{n+p}\times\int_{0}^{1}\mathbb{P}(\mathbf{v}_{(2)}\leq x )-\mathbb{P}(\mathbf{v}_{(1)}\leq x)\mathrm{d}x\] \[\stackrel{{(iii)}}{{=}}\frac{n}{n+p}\times\int_{0}^{1 }((n+p)F^{n+p-1}(x)-(n+p-1)F^{n+p}(x)-F^{n+p}(x))\mathrm{d}x\] \[=n\int_{0}^{1}(F^{n+p-1}(x)-F^{n+p}(x))\mathrm{d}x\;.\]

The first equality is the definition of \(r(n)\) in Equation (1). Equality \((i)\) follows by independence of the index of the maximum and the value of the maximum and second maximum. This is itself a consequence of the fact that the values are i.i.d.. Then equality \((ii)\) follows since the distribution of the index of the maximum is uniform over \(n+p\). This is also a consequence of the fact that the values are i.i.d. Lastly, equality \((iii)\) follows from [10] (Equation 2.1.3) where for \(k\in\{1,2\}\), it is shown that

\[\mathbb{P}(\mathbf{v}_{(k)}\leq x)=\sum_{i=n+p-k+1}^{n+p}\binom{n+p}{i}(1-F(x)) ^{n+p-i}F(x)^{i},\]

and the proof is concluded by substitution. 

### Proof of Lemma 2

As a preliminary, we formally define the non-usual distributions considered in Lemma 2.

Truncated exponential distributionLet \(a>0\) be some parameter. Then, we define a truncated exponential distribution of parameter \(a\) as the distribution with c.d.f. \(F:x\mapsto\frac{1-e^{-ax}}{1-e^{-a}}\). Hence, \(F(0)=0\) and \(F(1)=1\), and the density of this distribution is the same as the density of the exponential distribution with same parameter on the segment \([0,1]\), up to a normalization constant.

**Complementary Beta distribution**

**Lemma 2**.: _Let \(F\) be the cumulative distribution function of a Bernoulli, truncated exponential or Complementary Beta distribution. Then, for any \(p\in\mathbb{N}^{*}\), \(r\) in Equation (3) unimodal._

Proof.: We consider each family of distributions separately.

Bernoulli distributionsIf \(F\) is the c.d.f. of \(\mathcal{B}(q)\) (a Bernoulli distribution of parameter \(q\)), then \(r(n)\) is equal to the probability that exactly one player from the coalition draws a value of \(1\), and every other player draw a value of \(0\). Hence, we obtain that \(r(n)=nq(1-q)^{n+p-1}\), which is trivially unimodal and maximized in \(n^{\star}=\frac{-1}{\log(1-q)}\lor 1\), regardless of the size of the competition.

Truncated exponential distributionsLet \(a>0\) be the parameter of the distribution. Let \(Q(x)\) be the inverse function of \(F\) (the quantile function), defined by \(Q(x)=\frac{1}{a}\log\left(\frac{1}{1-x(1-e^{-a})}\right)=\frac{1}{a}\sum_{k=1} ^{+\infty}\frac{x^{k}(1-e^{-a})^{k}}{k}\).

Let's denote by \(q(x)\) the derivative of \(Q(x)\), denoted by \(q(x)=\sum_{k=0}^{+\infty}\lambda_{k}x^{k}\) where \(\lambda_{k}=\frac{1}{a}(1-e^{-a})^{k}\). Introducing theses functions allows us to rewrite \(r(n)\) as follows,

\[r(n) =n\int_{0}^{1}F(v)^{p+n-1}(1-F(v))\mathrm{d}v\] \[=n\int_{0}^{1}x^{p+n-1}(1-x)q(x)\mathrm{d}x\quad\quad\text{using }F(v)=x\] \[=n\int_{0}^{1}x^{p+n-1}(1-x)\left(\sum_{k=0}^{+\infty}\lambda_{k} x^{k}\right)\mathrm{d}x\] \[=\sum_{k=0}^{+\infty}\lambda_{k}\left(\frac{n}{p+n+k}-\frac{n}{p+ n+k+1}\right)\] \[=\frac{n}{n+p}\lambda_{0}+n\sum_{j=1}^{+\infty}\frac{1}{n+p+j}( \lambda_{j}-\lambda_{j-1})\] \[=\lambda_{0}(1-\frac{p}{n+p})+\sum_{j=1}^{+\infty}(1-\frac{p+j}{ n+p+j})(\lambda_{j}-\lambda_{j-1})\] \[=\lambda_{0}\Big{(}-\frac{p}{n+p}+\sum_{j=1}^{+\infty}\underbrace{ \frac{\lambda_{j-1}-\lambda_{j}}{\lambda_{0}}}_{\theta_{j}}\frac{p+j}{n+p+j} \Big{)}\]

where the last inequality follows since \(\lim_{j\to\infty}\lambda_{j}=0\). Remark that \(\theta_{j}\geq 0\) since \(\lambda_{j}\) is decreasing and \(\sum_{j=1}^{\infty}\theta_{j}=1\).

The derivative of \(r(n)\) is given by,

\[r^{\prime}(n) =\lambda_{0}\Big{(}\frac{p}{(n+p)^{2}}-\sum_{j=1}^{+\infty}\theta _{j}\frac{p+j}{(n+p+j)^{2}}\Big{)}\] \[=\lambda_{0}\Big{(}\Theta_{p}(n)-\sum_{j=1}^{\infty}\theta_{j} \Theta_{p+j}(n)\Big{)}\] \[=\lambda_{0}\Theta_{p}(n)\Big{(}1-\sum_{j=1}^{\infty}\theta_{j} \Gamma_{p,p+j}(n)\Big{)}\quad\quad\text{where }\Gamma_{p,p+j}(n)=\frac{\Theta_{p+j}(n)}{ \Theta_{p}(n)}\]

the functions \(\Gamma_{p,p+j}(n)\) are non-decreasing hence it is the same for their convex combination. As \(\mathrm{sign}(r^{\prime}(n))=\mathrm{sign}(1-\sum_{j=1}^{\infty}\theta_{j} \Gamma_{p,p+j}(n))\) it follows that \(\mathrm{sign}(r^{\prime}(n))\) is decreasing meaning \(r(n)\) is unimodal.

Complementary beta distributionsUsing the same change of variable as in the previous proof (\(Y=F(X)\)), we express the reward as follows,

\[r(n)=n\times\mathbb{E}_{Y\sim Q}\left[Y^{n+p-1}(1-Y)\right]\,\]

where \(Q\) denotes the quantile function associated with c.d.f \(F\) (i.e. \(F^{-1}\)). By definition of \(F\), \(Y\) follows a Beta distribution of parameters \((a,b)\). We can thus compute the expected reward by usingthe explicit formula for moments of the Beta distribution,

\[\mathbb{E}_{X\sim\mathsf{B}(a,b)}[X^{n+p-1}-X^{n+p}] =\prod_{k=0}^{n+p-2}\frac{a+k}{a+b+k}-\prod_{k=0}^{n+p-1}\frac{a+k} {a+b+k}\] \[=\prod_{k=0}^{n+p-2}\frac{a+k}{a+b+k}\times\left(1-\frac{a+n+p-1}{ a+b+n+p-1}\right)\] \[=\prod_{k=0}^{n+p-2}\frac{a+k}{a+b+k}\times\frac{b}{a+b+n+p-1}\;.\]

Thanks to this expression, we prove the unimodality by analyzing the ratio \(\frac{r(n+1)}{r(n)}\), that we first write as

\[\frac{r(n+1)}{r(n)} =\frac{n+1}{n}\times\frac{a+n+p-1}{a+b+n+p-1}\times\frac{a+b+n+p- 1}{a+b+n+p}\] \[=\frac{n+1}{n}\times\frac{a+n+p-1}{a+b+n+p}\;,\]

and then obtain that this ratio is larger than \(1\) if and only if

\[(n+1)(a+n+p-1)\geq n(a+b+n+p) \iff n(a+n+p)+a+p-1\geq n(a+n+p)+bn\] \[\iff n\geq\frac{a+p-1}{b}\;,\]

which concludes the proof by showing the unimodality and expressing the value of the critical point. 

The proof of Lemma 2 highlights that the unimodality assumption is satisfied as soon as the quantile function, expressed as a power series, has its coefficients that slowly decrease (indeed, the \(k\)-th coefficient just needs to be smaller than \(1-\frac{1}{k}\) times the \(k-1\)-th one).

Similarly, the second proof technique highlights (up to standard algebraic manipulations) that unimodularity is guaranteed as soon as the function \(n\mapsto 1-E[X^{n+p-1}]/E[X^{n+p-2}]\) is log-concave.

### Additional discussion on the unimodality of \(r\)

In this section, we plot the shape of \(r(n)\) for some additional families of distribution that we conjecture to be unimodal from the plots.

Beta distributionThe following figure, illustrate the unimodal shape of \(r(n)\) for different parameters for the Beta distribution and \(p\).

Kumaraswamy distributionThe cumulative distribution is defined by \(F(x)=1-(1-x^{a})^{b}\) for some parameters \((a,b)\) (we use the notation \(K(a,b)\)). The following figure, illustrate the unimodal shape of \(r(n)\) for different parameters of \(K(a,b)\) and \(p\).

We now provide and discuss an example where Assumption 2 is not satisfied.

### An example of distribution with non-unimodal rewards

Let us consider a discrete distribution supported on \(\{0,0.5,1\}\). The counter-example emerges from putting all the probability mass in \(0.5\): let us consider a small \(\epsilon>0\), identify \(F\) with \(\{\epsilon,1-\epsilon,1\}\) and assume that there is no competition (\(p=0\)). Then, we can verify with (3) that

\[r(n) =\frac{n}{2}(\epsilon^{n-1}+(1-\epsilon)^{n-1}-(\epsilon^{n}+(1- \epsilon)^{n}))\] \[=\frac{n}{2}(\epsilon^{n-1}(1-\epsilon)+\epsilon(1-\epsilon)^{n- 1})\]

Consider \(\epsilon=0.15\), we get up to a precision \(0.001\) the values:

\[(r(n))_{n=1}^{7}=(0.5,0.255,0.191,0.190,0.197,0.200,0.198)\]

showing that \(r(n)\) is not unimodal in this case.

Figure 1: Shape of \(r(n)\) when \(F\) is Beta

Figure 2: Shape of \(r(n)\) when \(F\) is Kumaraswamy distribution

Concentration bounds on simple reward estimates

### Auxiliary results

Before presenting the proof of the theorem, we present two auxiliary results that are essential to its development.

#### b.1.1 Riemann sum approximation of the expected reward

The first result consists in upper bounding the deviation of a Riemann sum approximation of \(r(n)\) (for some \(n\in[N]\)) with respect to its exact integral formulation. This result is also of practical interest, since it can prevent computing exact integrals at each step of the algorithms without altering their theoretical guarantees with an appropriate tuning of the approximation error.

**Lemma 4** (Riemann sum approximation of \(r(n)\)).: _Let \(n\in[N]\), \(D\in\mathbb{N}\), and define the grid \((x_{s})_{s\in\{0,\ldots,D-1\}}=\big{\{}0,\frac{1}{D},\ldots,\frac{D-1}{D}\big{\}}\). Then, the expected reward approximation_

\[\widetilde{r}(n)=n\times\frac{1}{D}\sum_{s=0}^{D-1}\big{\{}F(x_{s})^{n+p-1}-F( x_{s})^{n+p}\big{\}}\]

_satisfies_

\[|r(n)-\widetilde{r}(n)|\leq\frac{n}{D}\;.\]

Proof.: For any \(j\in\mathbb{N}\) we consider

\[S_{j}=\frac{1}{D}\sum_{s=0}^{D-1}F^{j}(x_{s})\quad\text{as an approximation of}\quad I_{j}=\int_{0}^{1}F^{j}(x)\mathrm{d}x\;.\]

We recall that since \(F^{j}\) is a c.d.f., it is monotone, increasing and satisfies \(F^{j}(0)=0\) and \(F^{j}(1)=1\). This means that for any \(s\in\{0,\ldots,D-1\}\), it holds that \(\forall x\in[x_{s},x_{s+1}],F^{j}(x_{s})\leq F^{j}(x)\leq F^{j}(x_{s+1})\). The linearity of the integral first provides that

\[I_{j}=\int_{0}^{1}F^{j}(x)\mathrm{d}x=\sum_{s=0}^{D-1}\int_{\frac{s+1}{D}}^{ \frac{s+1}{D}}F^{j}(x)\mathrm{d}x\]

Then, using this decomposition and the monotony of \(F^{j}\) we obtain that

\[S_{j}\leq\frac{1}{D}\sum_{s=0}^{D-1}F^{j}\left(\frac{s}{D}\right) \leq\int_{0}^{1}F^{j}(x)\mathrm{d}x \leq\frac{1}{D}\sum_{s=0}^{D-1}F^{j}\left(\frac{s+1}{D}\right)\] \[\leq\frac{1}{D}\sum_{s=0}^{D-1}F^{j}\left(\frac{s}{D}\right)+ \frac{1}{D}\sum_{k=0}^{D-1}\left(F^{j}\left(\frac{s+1}{D}\right)-F^{j}\left( \frac{s}{D}\right)\right)\] \[\leq\frac{1}{D}\sum_{s=0}^{D-1}F^{j}\left(\frac{s}{D}\right)+ \frac{F^{j}(1)-F^{j}(0)}{D}\] \[=S_{j}+\frac{1}{D}\;.\]

Therefore, we obtained that for any \(j\in\mathbb{N}\) it holds that \(S_{j}\leq I_{j}\leq S_{j}+\frac{1}{D}\). We conclude by using this result after splitting the reward as a difference of two integrals that can be expressed in this form, respectively with \(j=n+p-1\) and \(j=n+p\). 

#### b.1.2 Chernoff bounds for Bernoulli random variables

In the following lemma, we summarize the different concentration bounds that we use in the proof of Theorem 1.

**Lemma 5** (Muttiplicative Chernoff bounds).: _Let \(\widehat{\mu}_{m}\) be the empirical average of \(m\) i.i.d. Bernoulli random variables \(X_{1},\ldots,X_{m}\) with expectation \(\mu\). Then, for any \(\delta>0\), each of the following bounds holds with probability at least \(1-\delta\),_

\[\begin{cases}|\widehat{\mu}_{m}-\mu|\leq\sqrt{\mu}\times\sqrt{\frac{3\log\left( \frac{2}{\delta}\right)}{m}}&\text{if }\mu\in I_{0}:=\left[\frac{3\log(2/\delta)}{m},1 \right]\,\\ \mu_{m}\leq\frac{6\log(2/\delta)}{m}&\text{if }\mu\in I_{1}:=\left(\frac{\delta}{m}, \frac{3\log(2/\delta)}{m}\right)\,\\ \mu_{m}=0&\text{if }\mu\in I_{2}:=\left[0,\frac{\delta}{m}\right]\.\end{cases}\]

Proof.: We first tackle the case \(\mu\in I_{2}\), where the bound is obtained by remarking that \(\mathbb{P}(\mu_{m}>0)\leq\mathbb{P}(\exists i\in[m]:X_{i}=1)\leq m\mu\leq\delta\). The two other cases are obtained by using the multiplicative form of the well-known Chernoff bounds [16], that provide that

\[\forall\gamma>0,\quad\mathbb{P}(\widehat{\mu}_{m}\geq(1+\gamma)\mu)\leq e^{-m \frac{\gamma^{2}\mu}{2+\gamma}}\quad,\text{ and}\]

\[\forall\gamma\in[0,1],\ \mathbb{P}(\widehat{\mu}_{m}\leq(1-\gamma)\mu)\leq e^{-m \frac{\gamma^{2}\mu}{2}}\.\]

When considering \(\gamma\leq 1\) we can further write that

\[\mathbb{P}(|\widehat{\mu}_{m}-\mu|\geq\gamma\mu)\leq 2e^{-m\frac{\gamma^{2}\mu}{ 3}}\.\]

On the other hand, for \(\gamma\geq 1\) the bound for the lower deviation is trivially \(0\) while the bound for the upper deviation can be written as follows,

\[\gamma\geq 1\Rightarrow\mathbb{P}(\widehat{\mu}_{m}\geq(1+\gamma)\mu)\leq e^{-m \frac{\gamma^{2}\mu}{2+\gamma}}\leq e^{-m\frac{\gamma^{2}\mu}{3\gamma}}=e^{-m \mu\frac{\gamma}{3}}\.\]

Hence, the case separation between the intervals \(I_{0}\) and \(I_{1}\) simply consist in identifying the value \(\mu\) for which a probability \(1-\delta\) can be obtained by setting an appropriate \(\gamma\in[0,1]\) or for \(\gamma>1\) in the above inequalities. More precisely, inverting the first bound provides \(\gamma=\mu^{\frac{-1}{2}}\sqrt{\frac{3\log\left(\frac{2}{\delta}\right)}{m}}\), which is valid only if \(\mu^{\frac{-1}{2}}\sqrt{\frac{3\log\left(\frac{2}{\delta}\right)}{m}}\leq 1 \Rightarrow\mu\geq\frac{3\log\left(\frac{2}{\delta}\right)}{m}\). This leads to the first confidence interval when \(\mu\in I_{0}\). The same procedure for \(\mu\in I_{1}\) leads to \(\gamma=\mu^{-1}\frac{3\log\left(\frac{2}{\delta}\right)}{m}\), which provides the result stated in the lemma. This concludes the proof. 

### Proof of Theorem 1

**Theorem 1** (Concentration of simple estimates).: _Consider any \(n\in[N]\) and \(k\in\mathcal{V}(n)\). Let \(\widehat{r}_{k}(n)\) be defined according to (5) from \(m_{k}\) samples collected by \(k\). Then, there exists some constants \(\beta_{k,n}\) (depending on \(n,k,p\)) and \(\xi_{k,n,F}\) (additionally depending on \(F\)) such that, with probability \(1-\delta\),_

\[|\widehat{r}_{k}(n)-r(n)|\leq\beta_{k,n}\sqrt{\frac{\log\left(\frac{2\left[n \sqrt{m_{k}}\right]}{\delta}\right)}{m_{k}}}+n\times\xi_{k,n,F}\left(\frac{ \log\left(\frac{2\left[n\sqrt{m_{k}}\right]}{\delta}\right)}{m_{k}}\right)^{ \frac{n+p-1}{k+p}}. \tag{7}\]

_Furthermore, the constants admit universal upper bounds for any \(n,k,p,F\). For instance if \(m_{k}\geq 4\) it holds that \(\beta_{k,n}\leq 33\) and \(\gamma_{k,n,F}\leq 100\)._

Proof.: We build the proof from the Riemann sum approximation of the reward presented in Lemma 4 and the Chernoff bounds presented in Lemma 5. Defining some parameter \(D\in\mathbb{N}\) that will be fixed later, we use the first result to consider the following approximation of the empirical reward estimate \(\widehat{r}_{k}(n)\) by

\[\widetilde{r}_{k}(n)=\frac{1}{D}\sum_{s=0}^{D-1}\left(\widehat{F}_{k+p,m_{k}} \left(\frac{s}{D}\right)^{\frac{n+p-1}{k+p}}-\widehat{F}_{k+p,m_{k}}\left( \frac{s}{D}\right)^{\frac{n+p}{k+p}}\right)\.\]

Thanks to Lemma 4, we know that \(\widehat{r}_{k}(n)\in\left[\widetilde{r}_{k}(n)-\frac{n}{D},\widetilde{r}_{k }(n)+\frac{n}{D}\right]\). For the rest of proof, we introduce the notation \(F_{s}^{j}=F\left(\frac{s}{D}\right)^{j}\) for any \(j\in[N]\), \(\widehat{F}_{s}^{k+p}=\widehat{F}_{k+p,m_{k}}\left(\frac{s}{D}\right)\), and\(\widehat{F}_{k+p,m_{k}}\left(\frac{s}{D}\right)^{\frac{n+p}{k+p}}\), so that

\[\widetilde{r}_{k}(n)\coloneqq\frac{1}{D}\sum_{s=0}^{D-1}\left(\widehat{F}_{s,k,n -1}-\widehat{F}_{s,k,n}\right)\,\]

that we want to relate with

\[\widetilde{r}(n)\coloneqq\frac{1}{D}\sum_{s=0}^{D-1}\left(F_{s}^{n+p-1}-F_{s}^{ n+p}\right)\.\]

We use that each variable \(\widehat{F}_{s,k,n}\) can be expressed as the expectation of \(m_{k}\) i.i.d. Bernoulli random variables of expectation \(F_{s}^{k+p}\), since \(\widehat{F}_{k+p,m_{k}}=\frac{1}{m_{k}}\sum_{j=1}^{m_{k}}\mathds{1}\{X_{k,j} \leq x\}\). Hence, we can use the confidence intervals providing by Lemma 5, according to the value of \(F_{s}^{k+p}\). We define two critical values, corresponding to the switch between the different intervals \(I_{0},I_{1},I_{2}\) in the lemma, and their closest upper point in the discretization grid. The first is

\[x_{0,k}=F^{-1}\left(\left(\frac{\delta}{m_{k}}\right)^{\frac{1}{k+p}}\right), \quad\text{and }s_{0,k}=\lceil Dx_{0,k}\rceil\,\]

and we recall that below \(x_{0,k}\) it holds that \(\widehat{F}_{k,i,n-1}=0\) with probability larger than \(1-\delta\). Then, we define

\[x_{1,k}\coloneqq F^{-1}\left(1\wedge\left(4\frac{\log\left(\frac{2}{\delta} \right)}{m_{k}}\right)^{\frac{1}{k+p}}\right),\ \text{and }s_{1,k}\coloneqq\lceil Dx_{1,k}\rceil\.\]

We remark that we use a multiplicative factor \(4\) inside of \(F^{-1}\), while Lemma 5 might suggest to use \(3\). We do that for technical reasons, that we will motivate at one stage of the proof. These terms depend both on the sample size \(m_{k}\) and the confidence level \(\delta\), but we omit them in the notation for simplicity. Then, for fixed values of these constants we decompose the estimator between the intervals \(I_{0}=\{s_{1,k},\ldots,D-1\}\), \(I_{1}=\{s_{0,k},\ldots,s_{1,k}-1\}\), and \(I_{2}=\{0,\ldots,s_{0,k}-1\}\). Note that for the second interval to be non-empty it must hold that \(s_{0,k}\leq s_{1,k}-1\), that we assume in the following, otherwise we can just remove this interval from the analysis.

For \(s\geq s_{1,k}\), Lemma 5 guarantees that with probability larger than \(1-\delta\) it holds that

\[\widehat{F}_{s}^{k+p}\in\left[(1-\gamma_{s})F_{s}^{k+p},(1+\gamma_{s})F_{s}^{ k+p}\right]\quad\text{for }\gamma_{s}=F_{s}^{-\frac{k+p}{2}}\sqrt{3\frac{\log\left(\frac{2}{\delta} \right)}{m_{k}}}\,\]

while for \(k\leq s_{1,k}-1\) we can use one of the two other bounds provided in the lemma. Using a union bound, all the confidence intervals hold simultaneously for the points in the sum and in \(x_{1,k}\) with probability larger than \(1-(D+1)\delta\), which defines a "good" event

\[\mathcal{G}= \left\{\forall k\in I_{2},\widehat{F}_{s}^{k+p}=0,\ \ \forall k\in I _{1},\ \widehat{F}_{s}^{k+p}\leq 8\frac{\log\left(\frac{2}{\delta}\right)}{m_{k}},\right.\] \[\left.\forall k\in I_{0},\widehat{F}_{s}^{k+p}\in\left[(1-\gamma_ {s})F_{s}^{k+p},(1+\gamma_{s})F_{s}^{k+p}\right]\ \right\}. \tag{9}\]

For the rest of the analysis, we assume that \(\mathcal{G}\) holds. In particular, in that context there exists \(D-s_{1,k}\) constants \((z_{s})_{s\in\{s_{1,k},\ldots,D-1\}}\) such that \(\forall s\in I_{0},\widehat{F}_{s}^{k+p}=(1+z_{s})F_{s}^{k+p}\) and \(z_{s}\in[-\gamma_{s},\gamma_{s}]\). We now upper and lower bound \(\widehat{r}_{k}(n)\) using these constants, first writing that under \(\mathcal{G}\) it holds that

\[\widetilde{r}_{k}(n)= \frac{1}{D}\sum_{s=0}^{D-1}\left(\widehat{F}_{s,k,n-1}-\widehat{F }_{s,k,n}\right)\] \[= \frac{1}{D}\sum_{s=s_{1,k}}^{D-1}\left(\widehat{F}_{s,k,n-1}- \widehat{F}_{s,k,n}\right)+\frac{1}{D}\sum_{s=0}^{s_{1,k}-1}\left(\widehat{F} _{s,k,n-1}-\widehat{F}_{s,k,n}\right)\] \[= \frac{1}{D}\sum_{s=s_{1,k}}^{D-1}\left((1+z_{s})^{\frac{n+p-1}{k +p}}F_{s}^{n+p-1}-(1+z_{s})^{\frac{n+p}{k+p}}F_{s}^{n+p}\right)+\frac{1}{D} \sum_{s=s_{0,k}}^{s_{1,k}-1}\left(\widehat{F}_{s,k,n-1}-\widehat{F}_{s,k,n} \right)\,\]where we used that all the terms are zero for indices smaller than \(s_{0,k}\). We can thus express \(\widehat{r}_{k}(n)\) as follows,

\[\widehat{r}_{k}(n)=\widetilde{r}(n)+n\mathcal{E}_{0}+n\mathcal{E}_{1}\,\]

with

\[\mathcal{E}_{0} \coloneqq\frac{1}{D}\sum_{s=s_{1,k}}^{D-1}((1+z_{s})^{\frac{n+p-1 }{k+p}}-1)F_{s}^{n+p-1}-\frac{1}{D}\sum_{s=s_{1,k}}^{D-1}((1+z_{s})^{\frac{n+p }{k+p}}-1)F_{s}^{n+p}\,\quad\text{and}\] \[\mathcal{E}_{1} \coloneqq\frac{1}{D}\sum_{s=s_{0,k}}^{s_{1,k}-1}\left(\widehat{F }_{s,k,n-1}-\widehat{F}_{s,k,n}\right)-\sum_{s=0}^{s_{1,k}-1}\left(F_{s}^{n+p -1}-F_{s}^{n+p}\right)\,\]

so we can upper and lower bound \(\widehat{r}_{i}(n)\) by upper and lower bounding \(\mathcal{E}_{0}\) and \(\mathcal{E}_{1}\) separately.

Bounding the individual terms of \(\mathcal{E}_{0}\)For any \(k\geq s_{1,k}\) we consider the term

\[\mathcal{E}_{0,s}\coloneqq((1+z_{s})^{\frac{n+p-1}{k+p}}-1)F_{s}^{n+p-1}-((1+z _{s})^{\frac{n+p}{k+p}}-1)F_{s}^{n+p}\.\]

We first re-arrange it in a more convenient way, remarking that

\[F_{s}^{n+p-1}=F_{s}^{n+p-1}(F_{s}+(1-F_{s}))=F_{s}^{n+p}+F_{s}^{n+p-1}(1-F_{s}).\]

Using this result, we obtain that

\[\mathcal{E}_{0,s} \coloneqq((1+z_{s})^{\frac{n+p-1}{k+p}}-1)F_{s}^{n+p-1}-((1+z_{s} )^{\frac{n+p}{k+p}}-1)F_{s}^{n+p}\] \[=((1+z_{s})^{\frac{n+p-1}{k+p}}-1)F_{s}^{n+p-1}(F_{s}+(1-F_{s}))- ((1+z_{s})^{\frac{n+p}{k+p}}-1)F_{s}^{n+p}\] \[=F_{s}^{n+p}((1+z_{s})^{\frac{n+p-1}{k+p}}-1)-(1+z_{s})^{\frac{n+ p}{k+p}}+1)+F_{s}^{n+p-1}(1-F_{s})((1+z_{s})^{\frac{n+p-1}{k+p}}-1)\,\]

which simplifies to

\[\mathcal{E}_{0,s}=\underbrace{F_{s}^{n+p}((1+z_{s})^{\frac{n+p-1}{k+p}}-(1+z_{ s})^{\frac{n+p}{k+p}})}_{\mathcal{E}_{0,s}^{-}}+\underbrace{F_{s}^{n+p-1}(1-F_{s} )((1+z_{s})^{\frac{n+p-1}{k+p}}-1)}_{\mathcal{E}_{0,s}^{+}} \tag{10}\]

We remark that these two terms have opposite sign, \(\mathcal{E}_{0,s}^{+}\) having the same sign as \(z_{s}\). We first upper bound \(\mathcal{E}_{0,s}\), starting with the case \(z_{s}>0\), for which it holds that

\[z_{s}\geq 0\Rightarrow\mathcal{E}_{0,s}\leq\mathcal{E}_{0,s}^{+}\leq\underbrace {((1+\gamma_{s})^{\frac{n+p-1}{k+p}}-1)}_{c_{s}}F_{s}^{n+p-1}(1-F_{s})\.\]

The constant \(c_{k}\) is explicit from the definition of \(\gamma_{s}\), and the bound holds for any \(i\in[N+p]\) without restriction. However, if we only consider the case \(\frac{n+p-1}{k+p}\leq 2\) then we can further write that

\[c_{s}\leq(1+\gamma_{s})^{2}-1\leq 2\gamma_{s}+\gamma_{s}^{2}\leq 3\gamma_{s}\,\]

since \(\gamma_{s}\leq 1\) for the values of \(s\) considered. Then, for \(z_{s}\leq 0\) we use that

\[z_{s}\leq 0\Rightarrow\mathcal{E}_{0,s} \leq\mathcal{E}_{0,s}^{-}\leq F_{s}^{n+p}((1+z_{s})^{\frac{n+p-1} {k+p}}-(1+z_{s})^{\frac{n+p}{k+p}})\] \[=F_{s}^{n+p}(1+z_{s})^{\frac{n+p-1}{k+p}}\left(1-(1+z_{s})^{\frac{ 1}{k+p}}\right)\.\]

Using the notation \(y_{s}=-z_{s}\) for convenience, we upper bound the last multiplicative term as follows,

\[(1-y_{s})^{\frac{1}{k+p}}=e^{\frac{\log(1-y_{s})}{k+p}} \geq 1+\frac{\log(1-y_{s})}{k+p}\] \[=1-\frac{1}{k+p}\log\left(1+\frac{y_{s}}{1-y_{s}}\right)\] \[\geq 1-\frac{1}{k+p}\times\frac{y_{s}}{1-y_{s}}\,\]

[MISSING_PAGE_FAIL:21]

The first sum can be trivially upper bounded by \(2^{\frac{n+p-1}{k+p}}\frac{n}{k+p}\), which cannot be refined without more restrictive assumptions on \(F\). For the second term, we use that \(k+p\leq\frac{3}{2}(n+p)\) to exhibit the reward function associated with a number of players \(n^{\prime}\coloneqq n-\frac{n(k+p)}{2(n+p)}\geq\frac{n}{4}\) and a competition of size \(p^{\prime}\coloneqq p-\frac{p(k+p)}{2(n+p)}\geq\frac{p}{4}\).

\[\frac{n}{D}\sum_{s=s_{1,k}}^{D-1}F_{s}^{n+p-1-\frac{k+p}{2}}(1-F_{ s}) =n\times\frac{1}{D}\sum_{s=s_{1,k}}^{D-1}F_{s}^{n^{\prime}+p^{ \prime}-1}(1-F_{s})\] \[\leq n\times\int_{0}^{1}F(x)^{\frac{n+p-1}{4}}(1-F(x))\mathrm{d}x +\frac{n}{D}\] \[=\frac{n}{n^{\prime}}\times n^{\prime}\int_{0}^{1}F(x)^{n^{\prime }+p^{\prime}-1}(1-F(x))\mathrm{d}x+\frac{n}{D}\] \[\leq\frac{n}{n^{\prime}+p^{\prime}-1}+\frac{n}{D}=\frac{n}{n+p- \frac{k+p}{2}}+\frac{n}{D}\;,\]

where we used that the reward is smaller than the probability that the coalition wins the auction, which is easily generalized even if \(i/2\) is not integer.

We thus conclude the proof of the lower bound by writing that

\[n\mathcal{E}_{0}\geq-4\left\{\left(\frac{n}{2(n+p)-(k+p)}+\frac{n}{2D}\right) \ +\ 2^{\frac{n+p-1}{k+p}-2}\times\frac{n}{k+p}\right\}\times\sqrt{\frac{3 \log\left(\frac{2}{\delta}\right)}{m_{k}}}\;, \tag{13}\]

where the worst scaling for the left-hand term in the maximum is attained in \(k+p=3\frac{n+p}{2}\) and provide \(2\frac{n}{n+p}\), while for the right-hand term it is achieved in \(k+p=\frac{n+p-1}{2}\) and provides \(2\frac{n}{n+p-1}\).

As we already discussed, the upper bound can be expressed very similarly, remarking that the bound involving the terms \(\gamma_{s}/i\) have to be divided by two, and the other term have to be multiplied by \(3/2\). We hence directly obtain that

\[n\mathcal{E}_{0}\leq 6\left\{\left(\frac{n}{2(n+p)-(k+p)}+\frac{n}{2D}\right) \ +\ \frac{n}{3(k+p)}\right\}\times\sqrt{\frac{3\log\left(\frac{2}{\delta} \right)}{m_{k}}}\;. \tag{14}\]

Bounds on \(\mathcal{E}_{1}\)We start by upper bounding the second sum by \(0\). Under \(\mathcal{G}\) we hence obtain that

\[n\mathcal{E}_{1} \leq\frac{n}{D}\sum_{k\in I_{1}}(\widehat{F}_{s}^{k+p})^{\frac{n+ p-1}{k+p}}\] \[\leq\frac{n}{D}\sum_{k\in I_{1}}\left(8\frac{\log(2/\delta)}{m_{k }}\right)^{\frac{n+p-1}{k+p}}\] \[\leq n\left(x_{1,k}-x_{0,k}+\frac{1}{D}\right)\left(8\frac{\log(2 /\delta)}{m_{k}}\right)^{\frac{n+p-1}{k+p}}\;,\]

which has a worst possible power of \(2/3\) when \(m_{k}\geq 8\log(2/\delta)\), corresponding to \(k+p=\frac{3}{2}(n+p-1)\). Replacing \(x_{1,k}\) by its expression, we further obtain that

\[n\mathcal{E}_{1}\leq n\left(8\frac{\log(2/\delta)}{m_{k}}\right)^{\frac{n+p-1} {k+p}}\times\left\{F^{-1}\left(1\wedge\left(\frac{4\log\left(\frac{2}{\delta }\right)}{m_{k}}\right)^{\frac{1}{k+p}}\right)-F^{-1}\left(\left(\frac{\delta} {m_{k}}\right)^{\frac{1}{k+p}}\right)+\frac{1}{D}\right\}\;. \tag{15}\]

For the lower bound on \(n\mathcal{E}_{1}^{1}\), we apply the exact same steps, remarking that the constant \(8\) at the very first step can be replaced by \(4\), since we now use the exact value of \(F^{k+p}\) in the upper bound. Furthermore, we have to remove the term \(x_{0,k}\). We finally obtain

\[n\mathcal{E}_{1}\geq-n\left(4\frac{\log(2/\delta)}{m_{k}}\right)^{\frac{n+p-1} {k+p}}\times\left\{F^{-1}\left(1\wedge\left(\frac{4\log\left(\frac{2}{\delta }\right)}{m_{k}}\right)^{\frac{1}{k+p}}\right)+\frac{1}{D}\right\}\;. \tag{16}\]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_FAIL:24]

[MISSING_PAGE_FAIL:25]

which is sufficient to conclude, since the multiplicative constants to \(m_{k}^{-\frac{n+p-1}{k+p}}\) are clearly dominated by \(\log\left(\frac{4\lceil n\sqrt{m_{k}}\rceil}{\delta}\right)\). 

### Empirical UCB and LCB

The UCB and LCB in Equation (17) and Equation (19) depend explicitly on the unknown \(F\) via \(\xi_{k,n,F}^{+}\) and \(\xi_{k,n,F}^{-}\) and therefore cannot be used in the implementation of GG.

Below, we give empirical UCB (\(\widehat{U}_{k}(n,\delta)\)) and LCB (\(\widehat{L}_{k}(n,\delta)\)) by replacing \(\xi_{k,n,F}^{+}\) and \(\xi_{k,n,F}^{-}\) by empirical estimates \(\widehat{\xi}_{k,n}^{+}\) and \(\widehat{\xi}_{k,n}^{-}\):

\[\widehat{U}_{k}(n,\delta)=\widehat{r}_{k}(n)+\frac{1}{\sqrt{m_{k}}}+\beta_{k,n }^{-}\sqrt{\frac{\log\left(\frac{2\lceil n\sqrt{m_{k}}\rceil}{\delta}\right)} {m_{k}}}+n\times\widehat{\xi}_{k,n}^{-}\left(\frac{\log\left(\frac{2\lceil n \sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}}\right)^{\frac{n+p-1}{k+p}} \tag{21}\]

\[\widehat{L}_{k}(n,\delta)=\widehat{r}_{k}(n)-\frac{1}{\sqrt{m_{k}}}-\beta_{k, n}^{+}\sqrt{\frac{\log\left(\frac{2\lceil n\sqrt{m_{k}}\rceil}{\delta}\right)} {m_{k}}}-n\times\widehat{\xi}_{k,n}^{+}\left(\frac{\log\left(\frac{2\lceil n \sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}}\right)^{\frac{n+p-1}{k+p}} \tag{22}\]

where \(\beta_{k,n}^{-}\) and \(\beta_{k,n}^{+}\) are defined in Equation (18) and Equation (20), and

\[\widehat{\xi}_{k,n}^{-} =4^{\frac{n+p-1}{k+p}}\left\{\frac{\hat{d}_{k+p}+1}{\lceil n\sqrt {m_{k}}\rceil-1}\right\},\] \[\widehat{\xi}_{k,n}^{+} =8^{\frac{n+p-1}{k+p}}\left\{\frac{\hat{d}_{k+p}+1}{\lceil n\sqrt {m_{k}}\rceil-1}\right\}\]

for

\[\hat{d}_{k+p}=\inf\{d\in\{0,\ldots,\lceil n\sqrt{m_{k}}\rceil-1\},\hat{F}_{d}^ {k+p}\geq 8\frac{\log(2\lceil n\sqrt{m_{k}}\rceil/\delta)}{m_{k}}\}\]

if the infimum exists and \(\hat{d}_{k+p}=1\) otherwise.

It is a corollary of Theorem 1 that \(\widehat{U}_{k}(n)\) and \(\widehat{L}_{k}(n)\) are indeed high probability upper and lower bounds of the true reward:

**Corollary 1** (Explicit upper and lower bounds).: _It holds that_

\[\mathbb{P}(\widehat{L}_{k}(n,\delta)\leq r(n)\leq\widehat{U}_{k}(n,\delta)) \geq 1-\delta\]

Proof.: With \(D=\lceil n\sqrt{m_{k}}\rceil\) and \(x_{1,k}=F^{-1}\left(1\wedge\left(\frac{4\log\left(\frac{2\lceil n\sqrt{m_{k}} \rceil}{\delta}\right)}{m_{k}}\right)^{\frac{1}{k+p}}\right)\) the good event \(\mathcal{G}\) defined in Equation (9)implies that with probability \(1-\delta\) the following event \(\mathcal{H}\) holds:

\[\mathcal{H}=\left\{\forall k\leq\lceil Dx_{1,k}\rceil,\widehat{F}_{s}^{k+p} \leq 8\frac{\log\left(\frac{2\lceil n\sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}} \right\}.\]

Under \(\mathcal{H}\), and since by definition \(\hat{F}_{d_{k+p}}^{k+p}\geq 8\frac{\log(2\lceil n\sqrt{m_{k}}\rceil/\delta)}{m_{k}}\), it holds that

\[\frac{\hat{d}_{k+p}}{D}\geq x_{1,k}=F^{-1}\left(1\wedge\left(\frac{4\log \left(\frac{2\lceil n\sqrt{m_{k}}\rceil}{\delta}\right)}{m_{k}}\right)^{\frac {1}{k+p}}\right)\]

This implies that \(\widehat{\xi}_{k,n}^{-}\geq\xi_{k,n,F}^{-}\) and \(\widehat{\xi}_{k,n}^{+}\geq\xi_{k,n,F}^{+}\).

Therefore, we can incorporate in the proof of Theorem 1 the fact that the good event \(\mathcal{G}\) implies \(\widehat{\xi}_{k,n}^{-}\geq\xi_{k,n,F}^{-}\) and \(\widehat{\xi}_{k,n}^{+}\geq\xi_{k,n,F}^{+}\) and obtain the stated result.

Regret analysis of Local-Greedy and Greedy-Grid

### Clarification on the feedback received by the algorithms

In this section, we consider the case where a feedback (in the form of a sample from a power of \(F\)) is gathered only when an auction is won. If this is not the case, the decision-maker only knows that the coalition lost the auction. Therefore, if at time \(t\), \(n_{t}\) agents are assigned to an auction and the auction is lost, it makes sense to continue assigning \(n_{t}\) agents to the auction at time \(t+1\), in order to gather the information that the algorithm wanted to obtain. The meta algorithm called CoMAB for coalition multi-armed bandits described in Algorithm 3 implements this strategy.

```
Init:\(\mathcal{J}_{0}=\varnothing\), \(m=1\) Input: Algo for\(t=1\dots T\)do if\(t>1\) and auction at \(t-1\) is not wonthen  Play \(n_{t}=n_{t-1}\) ; \(\triangleright\) Play same arm until an auction is won else  Play \(n_{t}=\text{Algo}(\mathcal{J}_{m-1})\) ; \(\triangleright\) When an auction is won play as prescribed by input Algo ifAuction is wonthen  Observe \(w_{n_{t}}\) a sample from \(F^{n_{t}+p}\)  Set \(\mathcal{J}_{m}=\mathcal{J}_{m-1}\cup\{(w_{n_{t}},n_{t},m)\}\) ; \(\triangleright\) Record feedback obtained when winning Update \(m=m+1\)
```

**Algorithm 3** CoMAB

Local-Greedy and Greedy-Grid are then defined as CoMAB applied on \(\pi_{\tt Lg}\) and \(\pi_{\tt GG}\) for local greedy and greedy-grid respectively. These policies associate a play \(n_{m}\) to an history \(\mathcal{J}_{m-1}\). In Algorithm 1 and Algorithm 2, the policies are called sequentially \(T\) times and feedback is observed after each request. This gives an implicit definition of the policies.

Lemma 6 expresses the regret of CoMAB in function of the behavior of any policy \(\pi\) when feedback is observed after each request.

**Lemma 6** (Regret of CoMAB).: _Consider a policy \(\pi\) that associate to every \(\mathcal{J}_{m-1}\) a play \(n_{m}^{\pi}\). Define \(\mathcal{J}_{0}=\varnothing\) and \(\mathcal{J}_{m}=\mathcal{J}_{m-1}\cup\{w_{n_{m}^{\pi}},n_{m}^{\pi},m\}\) where \(w_{n_{m}}^{\pi}\) is a sample from a distribution with c.d.f \(F^{n_{m}^{\pi}+p}\) and \(n_{m}^{\pi}=\pi(\mathcal{J}_{m-1})\). Consider \(m_{n}^{\pi}(m)\) the number of times \(\pi\) returns \(n\) after \(m\) calls of \(\pi\). After \(T\) iterations, CoMAB based on \(\pi\) has regret:_

\[\mathcal{R}_{T}\leq\sum_{n=1}^{N}\mathbb{E}[m_{n}^{\pi}(T)]\frac{p+n}{n}(r(n^{* })-r(n))\]

Proof.: Call \(n_{t}\) the play chosen by CoMAB at time \(t\),

\[\eta_{t}=\mathds{1}\{\text{The auction is won at time }t\}\,,\]

\[m_{n}(t)=|\{\rho\leq t,n_{\rho}=n\text{ and }\eta_{\rho}=1\}|\]

the number of times that \(n\) is played and the auction is won up to time \(t\) and

\[Z_{n,m}(t)=|\{\rho\leq t,n_{\rho}=n\text{ and }m\leq m_{n}(\rho)<m+1\}|\]

the number of times that \(n\) has been played between the \(m\)-th time \(n\) won an auction and the \(m+1\)-th time. Note that \(m_{n}(t)\leq m_{n}^{\pi}(t)\) since at time \(t\), \(\pi\) has been called at most \(t\) times.

The regret of CoMAB satisfies:

\[\mathcal{R}_{T} =\sum_{t=1}^{T}\mathbb{E}[r(n^{*})-r(n_{t})]\] \[=\sum_{n=1}^{N}\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{n_{t}=n\} \right](r(n^{*})-r(n))\] \[=\sum_{n=1}^{N}\mathbb{E}\left[\sum_{m=1}^{m_{m}(T)}Z_{n,m}(T) \right](r(n^{*})-r(n))\] \[=\sum_{n=1}^{N}\mathbb{E}\left[\sum_{m=1}^{m_{m}(T)}Z_{n,m}(T) \right](r(n^{*})-r(n))\] \[\leq\sum_{n=1}^{N}\mathbb{E}[m_{n}^{\pi}(T)]\frac{p+n}{n}(r(n^{*}) -r(n))\]

where in the second to last inequality, we used the independence between \(n_{t}=n\) and \(Z_{n,m_{t}(n)}(T)\) as \(n_{t}=n\) depends only on the history at times \(t<m_{t}(n)\) while \(Z_{n,m_{t}(n)}(T)\) depends only on times \(t\geq m_{t}(n)\).

### Auxiliary result

Before proving the theorems, we present an auxiliary result from [6] that we to derive upper bounds that can be recovered explicit in the proof of the theorems. Since the proof is simple, we recall it for completeness.

**Lemma 7** (Lemma 4 from [6]).: _For any \(\zeta\geq 1\), the mapping_

\[f_{\zeta}:x\in[(\zeta+2)^{\zeta}\lor 3,\infty)\mapsto\sup\left\{t\in\mathbb{N}: \frac{t}{\log(t)^{\zeta}}\leq x\right\}\]

_satisfies_

\[f_{\zeta}(x)\leq(\zeta+2)^{\zeta}\times\log(x)^{\zeta}x.\]

Proof.: We start by remarking that the function \(g(x)=\frac{x}{\log(x)^{\zeta}}\) is strictly increasing for all \(x\geq e^{\zeta}\). Now, consider a value \(s=Ax\log(x)^{\zeta}\) for some \(A>0\), such that \(s\geq 3\lor e^{\zeta}\). By the monotonicity of \(\frac{t}{(\log t)^{\zeta}}\), we have that

\[t>s\Rightarrow\frac{t}{(\log(t)^{\zeta}}>\frac{s}{(\log(s)^{\zeta}}=x\times \frac{A\log(x)^{\zeta}}{(\log(A)+\log(x)+\zeta\log(\log(x)))^{\zeta}}\.\]

Then, for \(x\geq A\geq 3\), it holds that \(\log(A)+\log(x)+\zeta\log(\log(x))\leq(\zeta+2)\log(x)\), so we can simply choose \(A=(\zeta+2)^{\zeta}\) to obtain the result.

All that is left is to verify that for this choice, \(s=(\zeta+2)^{\zeta}\times\log(x)^{\zeta}x\geq 3\lor e^{\zeta}\), but this clearly holds for all \(x\geq 3\) and \(\zeta>0\). 

### Proof of Theorem 2

**Theorem 2** (Regret bound for Local-Greedy).: _Let \(\Delta:=\min_{n\in[N-1]}|r(n+1)-r(n)|\) (worst local gap). Under Assumption 2 and with \(\alpha=(\log_{3/2}N+1)^{-1}\), the regret of LG is upper bounded by a problem-dependent constant: there exists \((C_{n})_{n\in[N]\setminus\{n^{*}\}}\), each satisfying \(C_{n}=\widetilde{\mathcal{O}}_{N}\left(\frac{\Delta_{n}}{\Delta^{2}}\right)\), such that \(\mathcal{R}_{T}\leq\sum_{n\in[N]\setminus n^{*}}C_{n}\)._

_Additionally, if the arm set forms a single estimation neighborhood, that is \(\forall n\in[N]:\ \mathcal{V}(n)\supset[N]\), then each constant \(C_{n}\) can be refined to \(\widetilde{\mathcal{O}}_{n}\left(\Delta_{n}^{-1}\right)\), providing \(\mathcal{R}_{T}=\widetilde{\mathcal{O}}(\sqrt{NT})\), which holds even when the reward function is not unimodal._

Proof.: We first prove the theorem by induction on \(\alpha\). We first prove the theorem by induction on \(\alpha\).

Proof.: First, we denote by \(\widetilde{r}_{t}(n)\) the reward estimate used for arm \(n\) at time \(t\), and by \(\widehat{r}_{k,t}(n)\) its value when the arm used to compute the estimate is fixed to \(k\in\mathcal{V}(n)\). The proofs rely on concentration bounds on \(\widetilde{r}_{t}(n)\) derived from Theorem 1, with a confidence level \(\delta_{t}\) that will be fixed later. However, we will use this result with extra care given that the identity of the arm \(k\) used to compute the estimate is a random variable, as well as its sample size \(m_{k}(t)\). This issue is tackled with appropriate union bounds. Furthermore, in order to simplify the presentation we denote by \(\mathcal{E}(m,\delta)\) the maximal diameter (as a function of \(k\)) of the confidence interval provided by Equation (7), defined by a number of plays \(m\) of the arm used to estimate, and by a confidence level \(\delta\). More precisely, with notation of Theorem 1, for any \((k,n)\in[N]^{2}\) we write that \(|\widehat{r}_{k,t}(n)-r(n)|\leq\mathcal{E}(m_{k}(t),\delta_{t})\) with probability at least \(1-\delta_{t}\). Furthermore, \(\mathcal{E}(m_{k}(t),\delta_{t})\) is increasing in \(\delta_{t}\) and decreasing in \(m_{k}(t)\). Finally, we use the notation \(K=\lceil\log_{3/2}(N)\rceil\), so that \(\alpha=\frac{1}{K+1}\).

We now prove the first statement of the theorem, by upper bounding the number of plays of each sub-optimal arm.

Single neighborhoodConsider any sub-optimal arm \(n\). The main ingredient of the proof is to tackle the forced sampling by using that if \(n\) is pulled at time \(t\), then it is either pulled "on purpose" or due to forced sampling. However, it it forced sampled then it must have been selected "on purpose" by being the best empirical arm in the neighborhood at some previous point in time. We hence consider the following good event

\[\mathcal{G}_{t}=\left\{\forall s\in\{t-\lfloor\alpha t\rfloor,\ldots,t\},\;( \forall k\in\mathcal{V}(n):m_{k}(s)\geq\alpha t),\;|\widehat{r}_{k,s}(n)-r(n)| \leq\mathcal{E}(m_{k}(s),\delta_{s})\right\}\;.\]

Using Theorem 1, \(\mathcal{G}_{t}\) holds with probability at least \(1-|\mathcal{V}(n)|t^{2}\delta_{t}\), where we used a crude union bound on the values of \(s\), \(k\) and \(m_{k}(t)\) (\(t\) could be replaced by \(t-\lfloor\alpha\rfloor t\)). Using this result, we first upper bound the number of plays of \(n\) up to horizon \(T\) as follows,

\[\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{n_{t}=n\}\right] \leq\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{\exists s\in\{t- \lfloor\alpha t\rfloor,\ldots,t\}:\;\widehat{r}_{s}(n)\geq\widehat{r}_{s}(n^ {\star})\}\right]\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{\exists s\in\{t- \lfloor\alpha t\rfloor,\ldots,t\}:\;\widehat{r}_{s}(n)\geq\widehat{r}_{s}(n^ {\star})\}\mathds{1}\{\mathcal{G}_{t}\}\right]+\sum_{t=1}^{T}\mathbb{P}( \vec{\mathcal{G}}_{t})\;.\]

As discussed above, the second term satisfies \(\sum_{t=1}^{T}\mathbb{P}(\vec{\mathcal{G}}_{t})\leq\sum_{t=1}^{+\infty}| \mathcal{V}(n)|t\delta_{t}\). In order to make it constant, we choose \(\delta_{t}=\frac{1}{|\mathcal{V}(n)|t^{\star}}\). For the first term, we use that \(\forall s\in[\alpha t,t],\mathcal{E}(m_{k}(s),\delta_{t})\leq\mathcal{E}( \alpha(1-\alpha)t,\delta_{t})\) and that \(n\) can be played only if the two confidence intervals overlap. Hence, we further obtain that

\[\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{n_{t}=n\}\right] \leq\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{2\mathcal{E}(\alpha (1-\alpha)t,\delta_{t})\geq\Delta_{n}\}\right]+\sum_{t=1}^{+\infty}|\mathcal{ V}(n)|t^{2}\delta_{t-\lfloor\alpha t\rfloor}\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{2\mathcal{E}(\alpha (1-\alpha)t,\delta_{t})\geq\Delta_{n}\}\right]+\sum_{t=1}^{+\infty}(1-\alpha)^ {-4}t^{-2}\] \[=t_{\alpha,n}+\frac{\pi^{2}}{6(1-\alpha)^{4}}\;,\] \[=t_{\alpha,n}+\frac{\pi^{2}}{6}\left(1+\frac{1}{K}\right)^{4}\;,\quad\text{since }\alpha=\frac{1}{K+1},\]

and for a deterministic constant \(t_{\alpha,n}\) defined by

\[t_{\alpha,n}=\sup\{t\in\mathbb{N}:2\mathcal{E}(\alpha(1-\alpha)t,\delta_{t}) \geq\Delta_{n}\}\;.\]

We recall that \(K=\lceil\log_{3/2}(N)\rceil\) is used to simplify the notation. The last step consists in upper bounding the value of \(t_{\alpha,n}\) explicitly according to \(n\) and \(\Delta_{n}\). Considering some \(t\geq 4\lor n+1\), from Theorem 1 we know that there exist universal constants \(\beta\) and \(\xi\), and we obtain that

\[\mathcal{E}(\alpha(1-\alpha)t,\delta_{t}) \leq\beta\sqrt{\frac{\log\left(\frac{2\lceil n\sqrt{m_{k}(s)}\rceil }{\delta_{t}}\right)}{m_{k}(s)}}+n\times\xi\left(\frac{\log\left(\frac{2\lceil n \sqrt{m_{k}(s)}\rceil}{\delta_{t}}\right)}{m_{k}(s)}\right)^{\frac{2}{3}}\] \[\leq\beta\sqrt{\frac{\log\left(2t^{4}(n+1)\sqrt{t}|\mathcal{V}(n) |\right)}{\alpha(1-\alpha)t}}+n\times\xi\left(\frac{\log\left(2t^{4}(n+1)\sqrt {t}|\mathcal{V}(n)|\right)}{\alpha(1-\alpha)t}\right)^{\frac{2}{3}}\] \[\leq\beta\sqrt{\frac{\log\left(t^{5}(n+1)^{2}\right)}{\alpha(1- \alpha)t}}+n\times\xi\left(\frac{\log\left(t^{5}(n+1)^{2}\right)}{\alpha(1- \alpha)t}\right)^{\frac{2}{3}}\] \[\leq\beta\sqrt{\frac{7\log\left(t\right)}{\alpha(1-\alpha)t}}+n \times\xi\left(\frac{7\log\left(t\right)}{\alpha(1-\alpha)t}\right)^{\frac{2}{ 3}}\;.\]

Since \(\alpha=\frac{1}{K+1}\) it holds that \(\alpha(1-\alpha)=\frac{1}{K+1}\frac{K}{K+1}\geq\frac{1}{2(K+1)}\). We then get that for some universal constants \(\beta^{\prime}\) and \(\xi^{\prime}\), it first holds that:

\[\mathcal{E}(\alpha(1-\alpha)t,\delta_{t})\leq\left(\beta^{\prime}\sqrt{K+1}+n(K +1)^{\frac{2}{3}}\xi^{\prime}\right)\sqrt{\frac{\log(t)}{t}}\;, \tag{23}\]

where we bounded \(\left(\frac{\log(t)}{t}\right)^{\frac{2}{3}}\) by \(\sqrt{\frac{\log(t)}{t}}\). Without this simplification, we also obtain that

\[\mathcal{E}(\alpha(1-\alpha)t,\delta_{t})\leq(K+1)^{\frac{2}{3}}\left\{\beta^{ \prime}+n\left(\frac{\log(t)}{t}\right)^{\frac{1}{6}}\xi^{\prime}\right\} \sqrt{\frac{\log(t)}{t}}\;. \tag{24}\]

The different scaling proposed in the theorem then come from using Lemma 7 on (23) and (24), taking the minimum between the two (since both bounds are valid simultaneously), and for the latter splitting cases depending on \(\frac{t}{\log(t)}\leq n^{6}\) being satisfied or not (taking this time the maximum between the two cases).

We provide the right-hand term of the result using (23), applying Lemma 7 with \(\zeta=1\) and

\[x=\frac{4}{\Delta_{n}^{2}}\times\left(\beta^{\prime}\sqrt{K+1}+n(K+1)^{\frac{2 }{3}}\xi^{\prime}\right)^{2}\;,\]

which leads to \(t_{\alpha,n}\leq 3x\log(x)\). This provides the term \(\mathcal{O}\left(\frac{n^{2}}{\Delta_{n}^{2}}\right)\) of the result, and constants in the logarithmic terms can be recovered explicitly by recovering the values of \(\beta^{\prime}\) and \(\xi^{\prime}\).

We then obtain the left-hand term of the result by considering (24). Lemma 7 first provides that \(n\left(\frac{\log(t)}{t}\right)^{\frac{t}{6}}\leq 1\) for \(t\geq 18n^{6}\log(n)\) (first bound). Still using Lemma 7, this simplification permits to use \(\zeta=1\) and the threshold

\[y=\frac{4}{\Delta_{n}^{2}}\times\left(\beta^{\prime}\sqrt{K+1}+(K+1)^{\frac{2 }{3}}\xi^{\prime}\right)^{2}\;,\]

and an upper bound of \(t_{\alpha,n}\leq 3y\log(y)\), but only if this term is larger than \(18n^{6}\log(n)\). This provides the remaining terms of the bound, and again the logarithms can be easily recovered by computing \(\beta^{\prime}\) and \(\xi^{\prime}\).

This concludes the proof for the problem-dependent in the favorable case where all arms are neighbors, remarking that these upper bounds just have to be multiplied by \(\Delta_{n}\) and summed over \(n\in[N]\) to convert into the regret bound. Furthermore, the problem-independent guarantee can be derived from taking the minimum between the bound and \(\Delta_{n}T\), remarking that the worst case is \(\Delta_{n}=T^{-1/2}\) if we omit the logarithms.

General caseWe now provide the regret bound for the general case, where at least some arms do not include \([N]\) in their neighborhood. We recall that two main ingredients of Local-Greedy are (1) that the arm \(n_{t}\) played in \(t\) is the best empirical arm in the neighborhood of \(\mathcal{V}(n_{t-1})\), according to the simple estimates computed with samples from \(n_{t-1}\), and (2) that \(n_{t}=n_{t-1}\) if \(m_{n_{t-1}}(t)<\alpha t\) (forced sampling). Hence, similarly to the previous proof we use that \(n_{t}\) is either pulled thanks to a "greedy play" or because of forced sampling. Furthermore, the two cases can be merged because forced sampling can only come after \(n_{t}\) being pulled because of a greedy play in the recent rounds. More precisely, we use that

\[\{n_{t}=n\}\subset\{\exists s\in[t-\lfloor\alpha t\rfloor]:n_{s}=n,m_{s}(\ell_ {s})\geq\lceil\alpha s\rceil\}\enspace. \tag{25}\]

This argument is at the core of our analysis, but before going further we need to introduce the notion of _locally optimal plays_.

**Definition 2** (Locally optimal plays and optimal path).: _Given a reference arm \(\ell\), playing \(n\in\mathcal{V}(\ell)\) is **locally optimal** if \(n=\operatorname*{argmax}_{k\in\mathcal{V}(\ell)}r(k)\). In that case, \(n\) is the best neighbor of \(\ell\), and we use the notation \(n=v^{+}(n)\)._

_Furthermore, a sequence of successive locally optimal plays is an **optimal path** towards \(n^{\star}\). By construction of \(\mathcal{V}\), an optimal path contains at most \(K\coloneqq\mathcal{O}(\log(N))\) sub-optimal arms._

The last fact presented in the definition is trivial: in the worst case the path start at one of the extremes of the interval \([N]\) and \(n^{\star}\) is at the other. By design of \(\mathcal{V}\) (Definition 1) we obtain that \(n^{\star}\) is reached in \(\lceil\log_{3/2}(N)\rceil\) steps at most. The rest of the proof is based on the idea that, when \(t\) is large enough, the algorithm starts following an optimal path with high probability, so a sub-optimal arm can be played only if it is located on an optimal path from another sub-optimal arm to \(n^{\star}\). We formalize it with the following result.

**Lemma 8** (Existence of a "recent" sub-optimal play).: _For any time step \(t\in[T]\) and arm \(n\neq n^{\star}\), it holds that_

\[\{n_{t}=n\}\subset\mathcal{A}_{t}\coloneqq \left\{\exists s\in\left[(1-\alpha)^{K}t,t\right],l\in[N],l^{\prime }\in\mathcal{V}(l):\;\widehat{r}_{s}(l)\leq\widehat{r}_{s}(l^{\prime}),\right.\] \[\left.m_{s}(l)\geq\alpha(1-\alpha)^{K}t\quad\text{and}\quad r(l)> r(l^{\prime})\right\}\enspace,\]

Proof.: Starting from (25), we first use that either \(n\neq v^{+}(\ell_{s})\), and in that case playing \(n\) is locally sub-optimal so this event belongs to \(\mathcal{A}_{t}\), or \(n=v^{+}(\ell_{s})\). Let us now consider this second case: by definition of the leader, \(\ell_{s}\) was played right before the sequence of forced plays of \(n\) started, which must have happened at least as recently as \(t-\lfloor\alpha t\rfloor-1\). From that point, the recursion pattern is clear: \(\ell_{s}\) must have been selected in the last \(\lfloor\alpha(s-1)\rfloor\), by either being a locally sub-optimal play or not. The first case is included in \(\mathcal{A}_{t}\), why the second requires to add another step in the analysis. Furthermore, the arm used to estimate \(\ell_{s}\) was itself samples at least proportionally to \(s\). Using that this can happen \(K\) times, and that the worst number of steps to look into the past at each step is at most a fraction \((1-\alpha)\) of the number in the previous step, we finally obtain that there have been a sub-optimal greedy play in the last \(\left[(1-\alpha)^{K}t,t\right]\) steps. 

Before going further, we justify the tuning \(\alpha=\frac{1}{K+1}\), by stating that it maximizes \(\alpha(1-\alpha)^{K}\) (used later in the proof). At this step, we can simplify the notation by remarking that

\[(1-\alpha)^{K}=\left(\frac{K}{K+1}\right)^{K}\geq e^{-1}\enspace,\]

hence we replace \((1-\alpha)^{K}\) by \(e^{-1}\) in the rest of the proof.

In words Lemma 8 states that, even if forced sampling slows down the ascension towards \(n^{\star}\), since optimal paths contain at most \(K\) sub-optimal arm then \(n^{\star}\) is relatively fast to reach from any arm in \([N]\). Next, we use this result in the regret analysis by considering its occurrences with the following event,

\[\mathcal{H}_{t}=\left\{\forall s\in\left[e^{-1}t,t\right],\, \forall n\in[N],\;\forall k\in\mathcal{V}(n):m_{k}(s)\geq\frac{e^{-1}}{1+K}t,\right.\] \[\left.\left|\widehat{r}_{k,s}(n)-r(n)\right|\leq\mathcal{E}(m_{k} (s),\delta_{s})\right\}\enspace.\]Then, for any \(t_{K}\in\mathbb{N}\) we can upper bound the number of plays of each sub-optimal arm \(n\in[N]\) as follows,

\[\mathbb{E}\left[\sum_{t=1}^{T}\mathds{1}\{n_{t}=n\}\right] \leq\mathbb{E}\left[\sum_{t\geq 1}\mathds{1}\{\mathcal{A}_{t}\}\right]\] \[\leq t_{K}+\mathbb{E}\left[\sum_{t\geq t_{K}}\mathds{1}\{ \mathcal{A}_{t},\mathcal{H}_{t}\}\right]+\sum_{t\geq t_{K}}\mathbb{P}(\bar{ \mathcal{H}}_{t})\,\]

with the slight abuse of notation that \(\mathcal{E}\) is now defined with the coalition size \(N\) and not the local value of \(n\) considered. The rest of the proof is analogous to the simple case, where all arms are in a single neighborhood. We first choose \(\delta_{t}=\frac{1}{N^{2}t^{2}}\), and obtain that \(\sum_{t\geq t_{K}}\mathbb{P}(\bar{\mathcal{H}}_{t})\leq\sum_{t=1}^{+\infty} \frac{1}{e^{-4}t^{2}}\leq\frac{\pi^{2}}{6e^{-4}}\).

Next, we tune \(t_{K}\) large enough so that \(\mathbb{E}\left[\sum_{t\geq t_{K}}\mathds{1}\{\mathcal{A}_{t},\mathcal{H}_{t }\}\right]=0\). This can be done by choosing

\[t_{K}=\sup\left\{t\in\mathbb{N}:2\mathcal{E}\left(\frac{e^{-1}}{1+K}t,\delta_ {t}\right)\right)\leq\Delta\right\}\.\]

where \(\Delta=\min_{n\in[N-1]}\{|r(n+1)-r(n)|\}\).

We then deduce the result by applying the exact same steps as for the upper bound of \(t_{\alpha,n}\), by carefully replacing \(\delta_{t}=\frac{1}{|\mathcal{V}(n)|t^{4}}\) by \(\delta_{t}=\frac{1}{N^{2}t^{4}}\), and \(\alpha(1-\alpha)t\) by \(\frac{e^{-1}}{1+K}t\). Lemma 7 then allows to easily obtain the desired scaling, and to make the upper bound explicit by substitution. Since \(K\) is logarithmic in \(N\), it is clear that it only contributes to the bound only by logarithmic factors. 

### Proof of Theorem 3

**Theorem 3** (Regret upper bound for Greedy-Grid).: _Suppose that \(\mathfrak{GG}\) is tuned with confidence level \(\delta_{t}=\frac{1}{N^{2}t^{3}}\), and \(\alpha=1/4\). Then, for any \(T\in\mathbb{N}\) it holds that_

\[\mathcal{R}_{T}=\widetilde{\mathcal{O}}_{N}\left(\sum_{n\in\mathcal{B}^{\star }}\frac{1}{\Delta_{n}}+\sum_{n\in\mathcal{S}}\frac{\log(T)}{\Delta_{n}}\wedge \Delta_{n}\left(\frac{\mathds{1}\{n<n^{\star}\}}{\Delta_{v_{l}(n^{\star})}^{2} }+\frac{\mathds{1}\{n>n^{\star}\}}{\Delta_{v_{r}(n^{\star})}^{2}}\right) \right)\.\]

_Additionally, it holds that \(\mathcal{R}_{T}=\widetilde{\mathcal{O}}\left(\sqrt{(K+|\mathcal{B}^{\star}|)T }\right)\), for \(K=\lfloor\log_{3/2}(N)\rfloor\)._

Proof.: As the theorem suggests, we will use different arguments depending on the position of \(n\) with respect to the grid and the optimal bin \(\mathcal{B}^{\star}\). Before that, we introduce the crucial result of this proof: for each time \(t\) large enough, thanks to the design of Greedy-Grid, all arms in optimal bin \(\mathcal{B}^{\star}\) are estimated with a simple estimate computed with a _linear_ number of samples in \(t\).

Following the implementation of \(\mathfrak{GG}\), at each time \(t\) and for each arm \(n\in[N]\), a confidence interval \([\mathtt{LCB}_{t}(n),\mathtt{UCB}_{t}(n)]\) is computed so that \(r(n)\in[\mathtt{LCB}_{t}(n),\mathtt{UCB}_{t}(n)]\) with probability at least \(1-\delta_{t}\). Similarly to the proof of Theorem 2, we consider a "good event" stating that all confidence intervals where valid on a given time range before \(t\),

\[\mathcal{G}_{t}=\left\{\forall s\in\left[\left\lfloor\frac{3t}{16}\right\rfloor,t\right],\forall n\in[N],\ r_{t}(n)\in[\mathtt{LCB}_{t}(n),\mathtt{UCB}_{t}( n)]\right\}\.\]

It is clear that \(\sum_{t=1}^{+\infty}\mathbb{P}(\bar{\mathcal{G}}_{t})\leq\sum_{t=1}^{+\infty} N^{2}t\delta_{\left\lceil\frac{3t}{16}\right\rceil}\), where the second union bound on \(N\) comes from considering the (random) identity of the arm whose samples are used to compute the interval. The following results proves that, under \(\mathcal{G}_{t}\), there is at least one arm in the bin \(\mathcal{B}^{\star}\) that was played a linear number of times in \(t\).

**Lemma 9** (Linear number of plays in \(\mathcal{B}^{\star}\) under \(\mathcal{G}_{t}\)).: _Under \(\mathcal{G}_{t}\), there exists an arm \(n\in\mathcal{B}^{\star}\cup\{v_{\ell}^{\mathcal{S}}(n^{\star}),v_{r}^{\mathcal{ S}}(n^{\star})\}\) satisfying \(m_{n}(3t/4)\geq\frac{t}{4K}\wedge\frac{t}{8}\), where we call \(K=|\mathcal{S}|=\lfloor\log_{3/2}(N)\rfloor\)._

Proof.: \(\mathcal{G}_{t}\) guarantees that any play during the interval \(\left[\left\lfloor\frac{t}{4}\right\rfloor,t\right]\) (including those due to forced sampling), were decided with valid confidence intervals. Indeed, starting at \(\frac{3t}{16}\) we are sure that all forced exploration launched before that time is completed in \(t/4\). Furthermore, it is also direct from the design of the algorithms that, if \(r_{s}(n)\in[\texttt{LCB}_{s}(n),\texttt{UCB}_{s}(n)]\) and Greedy-Grid is not forced to sample the previous arm it must hold that (1) it is playing the grid, or (2) it is playing an arm in \(\mathcal{B}^{\star}\). Indeed, no arm from a sub-optimal bin would eliminate its best neighbor.

We consider two cases. First, if an arm \(n\in\mathcal{B}^{\star}\) was played between rounds \(\left\lfloor\frac{t}{4}\right\rfloor\) and \(\left\lfloor\frac{t}{2}\right\rfloor\). In that case it has collected at least \(\frac{t}{8}\) samples before \(t\), thanks to forced sampling. In the alternative case, the grid was played between those these two rounds, which incurs \(\frac{t}{4K}\) plays of \(\operatorname*{argmax}_{s\in\mathcal{S}}r(s)\) since by \(\mathcal{G}\), \(\operatorname*{argmax}_{s\in\mathcal{S}}r(s)\) is not eliminated when the grid is played. Then, notice that \(\operatorname*{argmax}_{s\in\mathcal{S}}r(s)\subset\{n^{\star},v_{\ell}^{ \mathcal{S}}(n^{\star}),v_{r}^{\mathcal{S}}(n^{\star})\}\). The result follows by combining the two cases.

Without loss of generality, we assume in the following that \(K\geq 2\) (if this is not the case, just replace \(K\) by \(\max(K,2)\)). As a direct consequence of Lemma 9, using Theorem 1,under \(\mathcal{G}_{t}\) there exists some constant \(\beta\) and \(\xi\) (coming from the bounds of the theorem multiplied by \(2\sqrt{4}=4\)) such that the LCB of arm \(n^{\star}\) satisfies

\[\forall s\in\left[\frac{3t}{4},t\right]:\;\texttt{LCB}_{s}(n^{\star})\geq r(n ^{\star})-\left\{\beta\sqrt{K\frac{\log\left(\frac{2\lceil N\sqrt{t}\rceil}{ \delta_{t}}\right)}{t}}+N\times\xi\left(K\frac{\log\left(\frac{2\lceil N\sqrt{ t}\rceil}{\delta_{t}}\right)}{t}\right)^{\frac{2}{3}}\right\}\;. \tag{26}\]

To simplify the notation, we denote the right-hand term by \(\mathcal{E}(t)\) in the rest of the proof. Using this result, we can now consider all the sub-cases presented in the theorem. We fix a sub-optimal arm \(n\), and upper bound \(\sum_{t=1}^{T}\mathds{1}\{n_{t}=n,\mathcal{G}_{t}\}\) depending on the position of \(n\) with respect to the grid \(\mathcal{S}\). Similarly to what we did in the proof of Theorem 2, we relate the pulls due to forced sampling to actual decisions by stating that, if \(n_{t}=n\), then there exists a round \(s\) between \(t-\left\lfloor t/4\right\rfloor\) and \(t\) such that \(\mathtt{GG}\) requested a pull of arm \(n\) from a "grid play" or a "greedy play".

**Case 1:**\(n\in\mathcal{S}\). In that case if \(n\) is pulled then, since there is no forced sampling for the arms of the grid it directly holds that

\[\texttt{UCB}_{t}(n)\geq\max_{n^{\prime}\in[N]}\texttt{LCB}_{t}(n^{\prime}) \geq\texttt{LCB}_{t}(n^{\star})\geq r(n^{\star})-\mathcal{E}(t)\;. \tag{27}\]

On the other hand, under \(\mathcal{G}_{t}\) it holds that

\[\texttt{UCB}_{t}(n)\leq r(n)+\underbrace{\left\{\beta\sqrt{K\frac{\log\left( \frac{2\lceil N\sqrt{t}\rceil}{\delta_{t}}\right)}{m_{n}(t)}}+N\times\xi\left( K\frac{\log\left(\frac{2\lceil N\sqrt{t}\rceil}{\delta_{t}}\right)}{m_{n}(t)} \right)^{\frac{2}{3}}\right\}}_{\mathcal{E}^{\prime}(t)},\]

so pulling arm \(n\) is possible only if \(\mathcal{E}(t)+\mathcal{E}^{\prime}(t,m_{n}(t))\geq\Delta_{n}\), that we simplify to \(2\mathcal{E}^{\prime}(t,m_{n}(t))\geq\Delta_{n}\) or \(2\mathcal{E}(t)\geq\Delta_{n}\). Considering the second term leads to a constant problem-dependent bound, analogous to \(t_{\alpha,n}\) in the proof of Theorem 2. Hence, we focus on the first term, that provide the \(\log(T)\) bound.

This time, we don't know if \(m_{n}(t)\) is large or not. This explains why we obtain a UCB-like (\(\widetilde{\mathcal{O}}(\log(T))\)) upper bound with this technique. We use that

\[t\leq T\Rightarrow\log\left(t\frac{2\lceil N\sqrt{t}\rceil}{\delta_{t}} \right)\leq\log\left(T\frac{2\lceil N\sqrt{T}\rceil}{\delta_{T}}\right)= \widetilde{\mathcal{O}}(\log(T))\;.\]

Then, similarly to proof of Theorem 2, we mitigate the asymptotic scaling in \(N\) by noticing that if \(m_{n}(t)=\Omega(N^{6})\) then \(\frac{N}{m_{n}(t)^{\frac{1}{6}}}\) simplifies. In that case, we obtain a sub-Gaussian confidence interval, and similarly to the analysis of UCB [3] we obtain that arm \(n\) is only pulled at most \(\widetilde{\mathcal{O}}\left(\frac{\log(T)}{\Delta_{n}^{2}}\lor N^{6}\right)\) times with high probability. This is the first part of the result for \(n\in\mathcal{S}\).

We then use another analysis to derive the constant problem-dependent bound. We remark that, when the confidence intervals are valid, the best arm between \(v^{\mathcal{S}}_{\ell}(n^{*})\) and \(v^{\mathcal{S}}_{r}(n^{*})\) can only be eliminated by an arm \(i^{*}_{t}\in\mathcal{B}^{*}\). By design of the algorithm (exploiting the unimodality assumption), it furthermore holds that if \(i^{*}_{t}\in\mathcal{B}^{*}\) and those two arms are eliminated then GG does not play on the grid. Hence, the constant bound in this case comes from upper bounding the time required for this event to happen under \(\mathcal{G}_{t}\). If \(v^{\mathcal{S}}_{\ell}(n^{*})\) is not eliminated it must hold that \(\mathsf{UCB}_{t}(v^{\mathcal{S}}_{l}(n^{*}))\geq\mathsf{LCB}_{t}(n^{*})\) (if \(n\leq n^{*}\)). Furthermore, Lemma 9 also guarantees that

\[\mathsf{UCB}_{t}(v_{\ell}(n^{*}))\leq r(v_{\ell}(n^{*}))+\mathcal{E}(t)\,\]

therefore the event that \(v_{\ell}(n^{*})\) is not eliminated is only possible if \(2\mathcal{E}(t)\geq\Delta^{2}_{v_{\ell}(n^{*})}\). We can then use Lemma 7, following the same as in the proof of Theorem 2 right after (23) and (24). When \(T\) is large enough, the derivation provides the scaling \(\widetilde{\mathcal{O}}\left(\frac{1}{\Delta^{2}_{v^{\mathcal{S}}_{\ell}(n^{*} )}}\right)\). We can then can follow the same steps for \(v^{\mathcal{S}}_{r}(n^{*})\), and obtain \(\widetilde{\mathcal{O}}\left(\frac{1}{\Delta^{2}_{v^{\mathcal{S}}_{\ell}(n^{* })}}\right)\). Furthermore, it is clear by analogy with the proof of Theorem 2 that for small values of \(T\) the upper bounds have to be multiplied by a \(N^{2}\) factor. Finally, we can remark that if \(n<n^{*}\) the first bound is used, while the second is used for \(n>n^{*}\). This concludes the derivation of the upper bound for \(n\in\mathcal{S}\).

**Case 2: \(n\notin\mathcal{S}\), \(\mathcal{B}(n)\neq\mathcal{B}^{*}\)**. We prove that this case is actually impossible under the good event, which explains the surprising constant upper bound independent of any gap. Indeed, if \(n\notin\mathcal{S}\) is played, then it must hold that its right and left neighbors in the grid are eliminated. Since \(\mathcal{B}(n)\neq\mathcal{B}^{*}\) then at least one of them has a reward at least as good as any arm in \(\mathcal{B}(n)\). However, if playing arm \(n\) was possible under \(\mathcal{G}_{t}\) it would hold that

\[\exists\ell\in\mathcal{B}(n):\quad r(\ell) \geq\mathsf{LCB}_{t}(\ell)\] \[>\max\{\mathsf{UCB}_{t}(v^{\mathcal{S}}_{\ell}(n)),\mathsf{UCB}_ {t}(v^{\mathcal{S}}_{r}(n))\}\] \[\geq\max\{r(v^{\mathcal{S}}_{\ell}(n),r(v^{\mathcal{S}}_{r}(n)) \}\geq r(\ell)\,\]

which is a contradiction due to the strict inequality in the second line. Hence, the number of time such arm \(n\) is played is simply upper bounded by \(\sum_{t=1}^{+\infty}\mathbb{P}(\bar{\mathcal{G}}_{t})\), which is (by design) bounded by a universal constant.

**Case 3: \(n\notin\mathcal{S}\), \(\mathcal{B}(n)=\mathcal{B}^{*}\)** We use that \(n_{t}=n\) implies that \(n_{s}=n\)_due to a greedy play_ at some round \(s\in[3t/4,t]\). Under \(\mathcal{G}_{t}\), we can thus directly use (26), and obtain that if \(2\mathcal{E}(t)\leq\Delta^{2}_{n}\) this event is not possible anymore. Using the same derivation as in the other cases (involving Lemma 7), we obtain the upper bound scaling in \(\mathcal{O}\left(\frac{1}{\Delta^{2}_{n}}\right)\) for \(T\) large enough, and by \(\mathcal{O}\left(\frac{N^{2}}{\Delta^{2}_{n}}\right)\) in general.

### Regret of Greedy-Grid adapted for non-unimodal rewards

In this section we develop the result presented in Section 3, regarding the adaptation of Greedy-Grid in the case when the reward function is no longer assumed to be unimodal. We recall that the adaptation consists in simplifying the definition of the set \(\mathcal{C}_{t}\) in Algorithm 2 by

\[\mathcal{C}_{t}=\{s\in\mathcal{S},U_{s}\geq L_{i^{*}_{t}}\}\.\]

We call the resulting algorithm GG-NU, for _Greedy-Grid Non Unimodal_, to differentiate it from the original version of GG introduced in the paper. In the following, we formalize the upper bound of the regret of GG-NU, and discuss how this result is obtained by adapting the proof of Theorem 3 from the previous section.

**Theorem 4** (Regret of GG-NU).: _Suppose that GG is tuned with confidence level \(\delta_{t}=\frac{1}{N^{2}\ell^{3}}\), and \(\alpha=1/4\). Then, for any \(T\in\mathbb{N}\) it holds that_

\[\mathcal{R}_{T}=\widetilde{\mathcal{O}}_{N}\left(\sum_{n\in\mathcal{B}^{*}} \frac{1}{\Delta_{n}}+\sum_{n\in\mathcal{S}}\frac{\log(T)}{\Delta_{n}}\right)\.\]

_Additionally, it holds that \(\mathcal{R}_{T}=\widetilde{\mathcal{O}}\left(\sqrt{(K+|\mathcal{B}^{*}|)T}\right)\), for \(K=\lfloor\log_{3/2}(N)\rfloor\)._Proof.: The proof follows the exact same steps as the proof of Theorem 3 presented in Appendix C.4. To adapt the arguments to GG-NU, we first remark that it suffices to identify which part of the proof uses the definition of the set \(\mathcal{C}_{t}\). We then find that this is the case when analyzing the _Case 1_ in the proof, namely the regret caused by sub-optimal arms in the grid \(|\mathcal{S}|\). More precisely, to obtain the bound of Theorem 3 for GG we provide two simultaneously valid upper bounds: a logarithmic (\(\log(T)\)) upper bound with a reasoning akin to the standard UCB analysis, and then a constant upper bound that carefully leverages the definition of \(\mathcal{C}_{t}\). We easily verify that the steps for the logarithmic bound remain valid with the new definition of \(\mathcal{C}_{t}\), while the second bound clearly does not hold. This completes the adaptation of Theorem 3 (for GG) into Theorem 4 (for GG-NU). 

## Appendix D Experiments

All the code for the experiments is written in Python. We use Matplotlib for plotting [18], Numpy [15] for numerical computing and Scipy [31] for scientific and technical computing. Scipy and Numpy are distributed under the BSD 3-Clause, and Matplotlib is distribution under BSD-style license. Theses licenses allow free use, modification, and distribution of the library. All the experiments were conducted on a single standard laptop, with an execution time shorter than 24 hours.

In a first simulation, we consider a coalition of size \(N=100\) and a competition of size \(p=4\). At each timestep, \(t\), the algorithm decide a number of bidders \(n_{t}\) to send to the auction and the values of all bidders (coalition and competition) \(\mathbf{v}\in\{0,1\}^{n_{t}+p}\) are sampled according to \(\mathcal{B}(0.05)\). With probability \(\frac{n_{t}}{n_{t}+p}\), the reward \(\mathbf{v}_{(1)}-\mathbf{v}_{(2)}\) is received and \(\mathbf{v}_{(1)}\) is observed. The (pseudo) regret at time \(t\) is then computed as the sum of reward obtained up to time \(t\). The simulation above is repeated \(20\) times with random seeds and the mean value across seeds is reported as the expected regret \(\mathcal{R}(t)\) in Figure 3. Error bars represent the first and the last decile.

In this simulation, the parameters are chosen to allow for having a significant number of players while keeping a gap \(\Delta\) large enough (about \(2\times 10^{-4}\)) to be able to observe logarithmic regrets for the baselines. LG practically outperforms other approaches by a large gap. The two algorithms that ignore the structure (UCB and EXP3) end up exhibiting a worse regret than LG, QSUB and GG, which is expected. However GG has a much higher regret than LG and only outperform UCB and EXP3 for horizons greater than \(10^{5}\), when it starts to eliminate points from the logarithmic grid \(\mathcal{S}\). Indeed, due to the explicit use of concentration bounds in the algorithm, which multiplicative constants are not optimized for practical implementations, GG does not practically reach the constant regret regime in the horizon of these simulations.

To illustrate the practical performance of LG, we perform additional simulations which are identical to the first one except for the parameters \(N\), \(p\), and the distribution of value that are set according to Table 2. The results are plot in Figure 4 where it is shown that LG reaches the constant regret regime after only a couple hundreds or thousands time steps while the other algorithms are still in the transient linear regime.

\begin{table}
\begin{tabular}{c c c c c} Position & \(N\) & \(p\) & Value distribution & \(n^{*}\) & \(\Delta\) \\ \hline \hline Top & 5 & 2 & \(\mathcal{B}eta(a=0.35,b=0.63)\) & 3 & \(5\times 10^{-5}\) \\ Middle & 5 & 2 & \(Trunc.\ Exp(0,1)\) & 3 & \(4\times 10^{-4}\) \\ Bottom & 20 & 4 & \(Trunc.\ Exp(0,1)\) & 5 & \(3\times 10^{-5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Configuration of additional experiments presented in Figure 4.

Figure 3: An empirical illustration of Table 1 with simulations in the following setting: values are distributed according to \(\mathcal{B}(0.05)\), \(N=100\) and \(p=4\). We benchmark LG and GG (this paper), OSUB[9], UCB[3] and EXP3[4] in terms of \(\mathcal{R}(T)\) computed over \(20\) trajectories. Error bars represent the first and last decile.

Complexity analysisThe time-complexity of both GG and LG mainly comes from the computation of reward estimates. They are computed by replacing the integral in Equation (5) by a Riemann sum

Figure 4: Illustration of additional experiments. Details of parameters are provided in Table 2.

with \(\lfloor N\sqrt{T}\rfloor\) terms (the reasoning behind the number of terms needed is the same as in the proof of Theorem 1. Therefore, whenever reward estimates of a neighborhood of size \(\mathcal{O}(N)\) is needed, it costs \(\mathcal{O}(N^{2}\sqrt{T})\) operations. Note that during forced exploration steps, reward estimates are not needed and therefore the associated cost is not paid. The total time complexity therefore depends on the number of times reward estimates are needed which itself depends on the trajectory. However, our algorithms could be modified to guarantee that reward estimates are needed at most \(\mathcal{O}(\log(T))\), times for instance by only performing updates at the end of phases of exponentially increasing length. This would lead to a mean complexity per iteration of \(\mathcal{O}(N\log(T))\), and similar theoretical guarantees by a slight adaptation of the analysis. This would reduce the burden of using of incorporating the structure in the algorithm, compared to the \(\mathcal{O}(N)\) cost of the baselines (which is even \(\mathcal{O}(1)\) for OSUB).

## Appendix E Broader Impact

The collection of user data should be carried out with the preservation of user privacy in mind. This issue is at the forefront of recent, ongoing developments, such as the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA).

In online advertising, maintaining privacy presents new challenges, as decisions must be made without complete access to user data.

This paper tackles one such challenge by detailing Multi-Armed Bandit (MAB) algorithms that Demand Side Platforms (DSPs) can use to determine the number of ad campaigns that should partake in the repeated auction for ad placements, without the need for prior knowledge of each campaign's value. This study is therefore a step towards the realization of practical user privacy. It is important to note that the assumptions we make require that the actions of the DSP have a limited impact on the market, which should be carefully verified in practical applications.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The theoretical and experimental results provided in (Theorems 1 to 3 and appendix D ) correspond to the claims made in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See the discussion at the end of Section 3 and future work directions in Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: In Theorems 1 to 3, the assumptions are referenced in the statement of each theorem, sketch of proofs are provided below each theorem and the detailed proofs are available in Appendices B, C.3 and C.4 respectively.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experiments are described in the text in Appendix D and can be reproduced using the code provided in supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: yes the paper provide open access to the code (as a zip folder in the supplementary materials) with all the instructions needed to reproduce it. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes all the details for the experimental setting are provided by the paper (see Appendix D). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars are reported and defined in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Experiments were run on a laptop in less than a day (see also Appendix D). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Yes the paper is conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[Yes] Justification: The discussion on the Broader Impacts of this work is in Appendix E Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The python packages were cited and use open source licenses (see Appendix D). Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code for the experiments is provided in the supplementary materials as a zip folder. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.