# Joint Prompt Optimization of Stacked LLMs

using Variational Inference

 Alessandro Sordoni\({}^{ab}\)\({}^{*}\) Xingdi Yuan\({}^{a}\) Marc-Alexandre Cote\({}^{a}\) Matheus Pereira\({}^{a}\)

Adam Trischler\({}^{a}\) Ziang Xiao\({}^{a}\) Arian Hosseini\({}^{b}\) Friederike Niedtner\({}^{a}\) Nicolas Le Roux\({}^{ab}\)

Microsoft Research Montreal\({}^{a}\) MILA\({}^{b}\)

Corresponding author: alsordon@microsoft.com

###### Abstract

Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful. The DLN code is open source.1

Footnote 1: Code: https://github.com/microsoft/deep-language-networks.

## 1 Introduction

The size of large language models (LLMs) has grown significantly over the last few years, mainly because of emerging capabilities [5; 31], but at considerable technical and societal costs [49; 2; 4]. Recent efforts have focused either on learning smaller models matching the abilities of larger ones on some tasks using distillation [43; 36; 29; 13], or offloading part of the computation to other dedicated components [28; 22; 25; 18]. In the latter case, this is done through carefully crafted instructions to retrieve the necessary information from these additional modules [48; 41; 6; 54; 24].

Instruction-tuned LLMs map an input sequence to a distribution over output sequences conditioned on an instruction, or _prompt_. In this paper, we view such LLMs as stochastic language layers, whose learnable parameters are the prompts. Multiple layers can be stacked to form a Deep Language Network (DLN) whose learnable parameters are the prompts associated to each layer. Specifically, each layer uses a template to organize both its prompt and the inputs coming from the layer below into a single sequence before producing the output (see Figure 1). This layering induces a learnable decomposition of the task into a series of smaller sub-tasks, each of which might be more easily solvable by an LLM. This view shares similarities to recent works that chain LLM calls [41; 6; 54]. In this work, we move towards integrating learnable components in the pipeline: each prompt can be learned to maximize the final objective.

We first show how to perform prompt optimization in a shallow 1-layer language network (DLN-1) which parametrizes a distribution \(p_{\text{LM}}(y|x,\pi)\), where \(x\) and \(y\) are string input and output respectively, and \(\pi\) is the learnable prompt (Figure 1, _left_). Our prompt optimization techniques extend the Automatic Prompt Engineer (APE) procedure from Zhou et al. [57]. We show how our prompts can include a verbalization of difficult examples from the task: the final prompts are a combination of instruction directives, akin to zero-shot learning [17], and task examples, akin to in-context learning [20]. This significantly improves downstream performance, surpassing APE on several tasks.

Then, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability distribution:

\[p_{\text{DLN-2}}(y|x)=\sum_{h}p_{\text{LM}}(y|h,x,\pi_{1})\,p_{\text{LM}}(h|x, \pi_{0})\,,\]

where \(h\) is the string output of the first LLM layer (Figure 1, _right_). We consider \(h\) as a latent variable: to maximize the marginal log-likelihood, we formulate a variational inference algorithm that uses an approximate posterior over \(h\). Note that this formalism easily encompasses more than two layers.

Considering outputs of the hidden language layers as latent variables allows us to encompass various established prompting methods, such as Chain-Of-Thought (CoT) [48] and self-consistency (SC-CoT) [46]. Particularly, CoT can be seen as a particular DLN-2 with the first layer prompt set to "Let's think step by step" and the second layer prompt set to "The answer is"; we can either learn such prompts or learn a supplement to those as in Figure 1. SC-CoT can be seen as marginalizing over CoT strings sampled from a task-agnostic prior: when using the template in Figure 1 (_right_), our method generalizes this perspective by learning a task-specific prior distribution over successful CoTs.

The rest of the paper is organized as follows. First, we provide an interpretation of LLMs as shallow networks, drawing a number of analogies with standard parametric and non-parametric models and explaining how best to train them. After exploring their limitations, we propose to stack two such

Figure 1: _Left_: An illustration of a DLN-1 performing a sentiment analysis task: input and the trainable prompt are merged using a template and fed to the LM for answer generation. _Right_: a DLN-2 with a residual connection, performing the date understanding task: two prompts need to be learned. In this example, the hidden template extends Chain-Of-Thought [48] with a learnable prefix; we consider the output of the first layer, hidden, as a _latent variable_\(h\). We use _variational inference_ to learn \(\pi_{0},\pi_{1}\). Templates can be considered as an hyperparameter of the network.

LLMs to form a DLN-2. We show how they can be trained using a form of variational inference, then demonstrate their performance on a series of reasoning and language understanding tasks.

## 2 One-Layer Language Networks

A pre-trained LLM with frozen weights might be thought of as a complete _function class_ indexed by prompts. The output \(y\) of an LLM for an input \(x\) can be modulated through a prompt \(\pi\) by feeding a combination of \(\pi\) and \(x\) to the LLM. Hence, from a low-level perspective, the function class of an LLM is defined by its architecture, i.e., its depth, number of heads, context size, etc., and training happens at the parameter level. It is data and compute intensive and should be done rarely. From a high-level perspective, the function class of an LLM is defined by the pre-trained model chosen (LLAMA [44], text-davinci-003, GPT-4, etc.), and training happens by fine-tuning the model or by choosing the prompt.

There are two ways of optimizing an LLM at the prompt level. The first one is _prompt engineering_, a parametric optimization, where the optimization space is independent of the size of the dataset. Because this optimization usually happens in discrete space, gradient-based techniques do not apply and most efforts rely on a combination of random or local search and human heuristics [21; 55]. The second one is _in-context learning_ (ICL), a non-parametric optimization technique where the solution is a direct function of a subset of examples [5; 20]. This approach works well for few-shot learning but scaling it to larger datasets has both performance and computational issues. We shall now generalize previous work in discrete prompt optimization [57; 21] with the ultimate goal of learning a set of prompts in a language network.

### Language Layers

We use _language layer_ to refer to a (stochastic) computation that takes as input a string \(x\) and outputs a string \(y\). This computation is modulated by another string, \(\pi\), generally called a _prompt_ or _instruction_[57]. The string transduction is performed by an operator LM, by feeding \(x\) and \(\pi\) as context and generating a continuation \(y\). _Templates_ describe the way \(x\) and \(\pi\) are combined prior to being fed to the LM operator. These are functions that accept strings as variables and output a string. We will denote such templates with this font T. A simple forward template F is the concatenation, i.e. \(\mathtt{F}(x,\pi)\) = "\(\{\pi\}\{x\}\)". We also explore more complex ones, examples of which can be seen in Figure 1.

Given an input \(x\), a prompt \(\pi\), and a template F, a language layer defines a probability distribution \(p_{\mathtt{LM}}(y|\mathtt{F}(x,\pi))\) over output strings \(y\) as computed by the LM. In the next section, we describe a generic framework for optimizing the weights \(\pi\) for a language layer.

### Prompt Optimization: Improved APE

Because the search for the best prompt happens over a discrete space, we will rely on a local search method, using an LLM to implement a distance measure between prompts. The procedure can be seen as an extension of Automatic Prompt Engineer (APE), recently proposed by Zhou et al. [57], and will serve as a stepping stone towards introducing our algorithm for training deep language networks. Our prompt optimization algorithm can be structured as follows:

1. Given the current prompt \(\pi\) and a current batch of examples \(\{x,y\}\), generate \(N\) "local" candidates \(\pi^{1},\ldots,\pi^{N}\) using a prompt _proposal_ distribution;
2. Score each candidate using a (potentially stochastic) _scoring_ function \(s\), then choose \(\pi=\arg\max_{\pi^{n}}s(\pi^{n})\).

Prompt ProposalLocal search algorithms assume a distance measure between inputs to crawl the search space. In this setting, we rely on LLMs to generate local modifications to the prompts. Our prompt proposal distribution takes as conditioning information i) the batch given as input to the layer, ii) its corresponding output \(\{x,y,\hat{y}\}\), and iii) the current prompt \(\pi\). The proposal distribution \(p_{\mathtt{LM}}(\pi^{n}|\mathtt{B}_{\pi}(\{x,y,\hat{y}\},\pi))\) wraps this information using a particular "backward" template \(\mathtt{B}_{\pi}\), which can be found in Appendix D. This approach is similar to the instruction template used by Zhang et al. [55], with the exception that we also integrate information about the model's own predictions, which we found to empirically help performance given that the model tends to propose prompts that correct its own errors. We sample from the prompt proposal distribution to generate a set of \(N\) prompts. A particularly important aspect is ensuring the diversity of the candidate pool \(\pi^{1},\ldots,\pi^{N}\). We devise several strategies to improve the diversity and the usefulness of the candidate samples in Section 4.

**Prompt Selection** Once a set of \(N\) prompts has been generated, we use a scoring function to select the updated prompt. We assume access to the log-likelihoods of the LM operator and we rank the candidate prompts to maximize data log-likelihood \(\pi=\operatorname*{arg\,max}_{\pi^{n}}\log p_{\text{LM}}(y|\text{F}(x;\pi^{n}))\). In practice, we normalize this log-probability by the length of the output string. While we focus on that metric in this work, there is no restriction on the scoring function that can be used. We use backtracking to increase the robustness of our selection mechanism, as well as a memory of well-performing prompts for efficiency. We present both strategies in Section 4. The sketch of a 1-layer prompt optimization algorithm is described in Algorithm 1, ignoring backtracking and memory for simplicity.

```
0:\(\hat{y}\sim p_{\text{LM}}^{t}(y|c)\)\(\triangleright\) generates a completion of prefix \(c\) with temperature \(t\)
0:\(\log p_{\text{LM}}(h|c)\)\(\triangleright\) return log-prob of \(h\) following \(c\)
0:\(N\): prompt samples, \(I\): iterations, \(\mathcal{D}\): dataset
0:F: template for the inference/forward pass
0:\(\texttt{B}_{\pi}\): template for prompt proposal/backward pass.
1: Initialize \(\pi\) with a task description or empty
2:for\(i\) in \([1,I]\)do
3:\(x,y\sim\mathcal{D}\)\(\triangleright\) Sample minibatch
4:\(\hat{y}\gets p_{\text{LM}}^{0}(y|\text{F}(x,\pi))\)\(\triangleright\) Do inference pass
5:\(\pi^{1},\ldots,\pi^{N}\sim p_{\text{LM}}^{0,\pi}(\pi|\texttt{B}_{\pi}(\{x,y, \hat{y}\},\pi))\)\(\triangleright\) Sample \(N\) candidate prompts
6:\(s^{1},\ldots,s^{N}\leftarrow\log p_{\text{LM}}(y|\text{F}(x,\pi^{n}))\)\(\triangleright\) Score all prompts
7:\(\pi\leftarrow\operatorname*{arg\,max}_{\pi^{n}}\{s^{1},\ldots,s^{N}\}\)\(\triangleright\) Select prompt with best score
8:endfor ```

**Algorithm 1** One-Layer Language Network (DLN-1) Training Algorithm

The results of our prompt optimization may be found in Table 1 and will be discussed in detail in Section 5.2. We now turn to extending prompt optimization to architectures with two layers.

## 3 Two-Layer Deep Language Networks (DLN-2)

The natural extension of DLN-1 is DLN-2, in which language layers are stacked, i.e. the output of the first language layer is the input to the second one. A 2-layer network induces a distribution over outputs of the form:

\[p_{\texttt{DLN-2}}(y|x)=\sum_{h}p_{\texttt{LM}}(y|\texttt{F}_{r}(h,x,\pi_{1}) )p_{\texttt{LM}}(h|\text{F}(x,\pi_{0}))\] (1)

where \(h\) is a latent variable that potentially makes it easier to explain the target \(y\). The output layer is also conditioned on \(x\) through \(\texttt{F}_{r}\), forming a residual connection (Figure 1). This formulation is reminiscent of past work using latent language representations to guide document summarization [26]. In our case, however, the encoding/decoding distributions are parameterized by natural language prompts \(\Pi=\{\pi_{0},\pi_{1}\}\), and we do not assume access to the LLM parameters.

While this architecture has more expressive power than a shallow language network, the prompt optimization problem becomes harder now that we have to _jointly_ search over both \(\pi_{0}\) and \(\pi_{1}\). Doing random search in this space is impractical [8] and manual tuning of the weights is also exponentially harder than with a single prompt. We turn to variational inference to address this issue.

### Variational Inference Objective

The layerwise decomposition of our system allows us to leverage tools from approximate inference in probabilistic models to learn \(\Pi\). In particular, we propose to use variational inference to learn \(\Pi\)[3, 16]. We posit an approximate posterior \(q(h)\) over the latent variable \(h\), and bound the marginal log-likelihood of \(y\) given \(x\) by computing the ELBO:

\[\log p_{\texttt{DLN-2}}(y|x)\geq\sum_{h}q(h)\left[\log p_{\texttt{LM}}(y| \mathtt{F}_{r}(h,x,\pi_{1}))p_{\texttt{LM}}(h|\mathtt{F}(x,\pi_{0}))\right]+H \left[q(h)\right],\] (2)

which allows us to decompose the optimization over \(\Pi\) in two independent optimization problems, over both \(\pi_{0}\) and \(\pi_{1}\):

\[\pi_{0}^{*}=\arg\max_{\pi_{0}}\sum_{x,\,h}w_{h}\log p(h|\mathtt{F}(x,\pi_{0})), \ \ \pi_{1}^{*}=\arg\max_{\pi_{1}}\sum_{(x,y),\,h}w_{h}\log p(y|\mathtt{F}(h,x,\pi_ {1}))\.\] (3)

The search over \(\pi_{1}\) is identical to the prompt optimization described in Section 2, with the difference that the inputs now depend on the approximate posterior samples \(h\) in addition to the inputs \(x\). The search over \(\pi_{0}\) uses a similar strategy but uses the posterior samples \(h\) as targets, instead of \(y\).

Although this bound allows us to decompose the optimization w.r.t. \(\Pi\), it is only useful if it is close to the true value. Since its looseness is the KL divergence between \(q(h)\) and the true posterior, \(\text{KL}(q(h)||p(h|y,x))\): we need to find an approximate posterior \(q\) closely matching the true posterior. In what follows, we specify how we parametrize the approximate posterior and how we tighten the approximation via posterior sharpening.

**Hidden Proposal** We will also be using an LLM to sample candidate hidden states from \(q(h)\). Unless specified, for simplicity, we use the same LM operator used in our language layers. The approximate posterior can condition on arbitrary amount of information but especially useful might be to condition on the true target \(y\). If it conditions on the hidden state coming from the "forward pass", \(\hat{h}\sim p_{\texttt{LM}}(h|\mathtt{F}(x,\pi_{0}))\), then \(q_{\texttt{edit}}(h)=p_{\texttt{LM}}(h|\mathtt{B}_{h}(\hat{h},y,\pi_{1}))\). \(\mathtt{B}_{h}\) is a specifically tailored hidden proposal template (Appendix D). \(q_{\texttt{edit}}\) performs a sort of edit operation, where the LM is tasked to rewrite the hidden variable \(\hat{h}\) given some extra knowledge of the ground-truth label \(y\) and of \(\pi_{1}\). Alternatively, we can set the posterior to be equal to the prior, i.e. \(q_{\texttt{pri}}(h)=p_{\texttt{LM}}(h|\mathtt{F}(x,\pi_{0}))\), or the prior with additional information about the label \(y\), \(q_{\texttt{pri}*}(h)=p_{\texttt{LM}}(h|\mathtt{B}_{y}(x,\pi_{0},y))\) (Appendix D). This amounts to re-computing the hidden state knowing privileged information about the label. We found most effective to sample hidden states from a mixture of \(q_{\texttt{pri}}\) and \(q_{\texttt{pri}*}\).

**Posterior Sharpening** Given the absence of learnable parameters in \(q(h)\), the induced approximate posterior might still be far from the true posterior. To bridge this gap, we reweigh each sample \(h^{i}\) based on its probability under the true posterior distribution. More precisely, we compute \(\tilde{w}^{i}=\log p_{\texttt{LM}}(y|\mathtt{F}_{r}(h^{i},x,\pi_{1}))+\log p_{ \texttt{LM}}(h^{i}|\mathtt{F}(x,\pi_{0}))\), then assign to each \(h_{i}\) the probability \(w_{i}=\exp(\alpha\tilde{w}_{i})/\sum_{j}\exp(\alpha\tilde{w}_{j})\), where \(\alpha\) is a tunable temperature parameter that controls the entropy of the posterior weights. The full algorithm for training a DLN-2 is presented in Algorithm 2.

## 4 Practical Instantiation

Although our method aim to learning prompts in stacked LLM architectures, we do rely on a good amount of prompt engineering for our templates. Hereafter, we detail some choices that were fundamental to make our approach work in practice.

**Proposal Diversity** To ensure a diversity of the samples for both the prompt proposal distribution, we found helpful to use two strategies. The first is to modify the backward templates \(\mathtt{B}_{\pi}\) before drawing a sample from the proposal distribution \(p_{\texttt{LM}}(\pi)\). To achieve so, we parametrize the basic templates with a "[message]" variable that we instantiate from a pool of hand-written instructions, that describe different behaviors the model should follow to propose new \(\pi\), e.g. "[short the previous instruction","give useful examples", etc. These can be interpreted as _meta-instructions_, i.e. high-level directives that inform the model on how to create a better instruction for the task, and extend instruction-induction templates used in [12; 55]. These can be found in Appendix D. In the future, we could envision to extend learning to these instructions. In the case of \(\mathtt{B}_{\pi}\), they could function as parameters for a _prior_ over the weights of the DLN. The second strategy to ensure more diversity is that we instantiate \(\mathtt{B}_{\pi}\) with a different random subset of examples in the current batch, before drawing each sample \(\pi^{n}\). This effectively modifies the generation context for each sample \(\pi^{n}\).

**Learning In-Context Learning** One strategy we found particularly effective is to integrate in the pool of meta-instructions an additional instruction that asks the LM to give useful examples to improve its current prompt \(\pi\). Empirically, we observed that this allows the model to sample candidate prompts \(\pi^{n}\) that contain synthetic examples for the task, embedded in natural language. Examples of this interesting behavior can be found in Appendix F. We found that this behavior is particularly interesting as the resulting prompts often perform better than standard ICL. We hypothesize this is due to both _i)_ the "verbalization" of the example in the prompt, which modifies the dataset syntax into a more suitable one, and _ii)_ the fact that the model can dynamically select which examples are most important to integrate in the prompts, given the errors made during training. Therefore, we suspect that DLN achieves a similar effect to recent techniques that select important examples for ICL [20, 35, 40, 18, 47] with the improvement of naturally conditioning the selection on the end task performance via end-to-end training.

Backtracking and MemoryOptimization of both DLN-1 and DLN-2 is challenging due to the fact that we do not have gradient information and we sample a restricted set of candidates \(\pi^{n}\) at each optimization step due to computational reasons. We deploy multiple strategies to allow the network to be robust to sampling/selection errors. First, we include the current prompt \(\pi\) into the set of candidate prompts to be scored at the current iteration \(\pi^{n}\). This allows the model to not take the step if the previous prompt performed better. Second, we keep a memory of \(M=5\) best prompts found by tracking validation set performance.

Exploration RewardWhen training a DLN-2, we empirically observed that the first layer prompt \(\pi_{0}\) was updating very slowly. Due to the fact that the approximate posterior shares templates with the prior used in the forward pass, the posterior samples \(h^{i}\) are close to \(\hat{h}\) and the maximizer of Equation (3) remains \(\pi_{0}\). To address this issue, we add to the scores of each candidate prompt an exploration reward that is proportional to the negative log-probability of those \(\hat{h}\) that led to an incorrect prediction: \(r=-\lambda\,\log p_{\texttt{LR}}(\hat{h}|\mathbb{F}(x,\pi^{n}))\), if \(\hat{y}\neq y\). This encourages the model to both find prompts that maximize the log-probability of high-probability posterior samples and at the same time minimize the log-probability of prior samples that led to incorrect predictions. We anneal \(\lambda\) to 0 during training with a constant schedule and we select the initial \(\lambda\) by monitoring validation performance for each task.

Experiments and Results

We design and conduct a set of experiments to help answer two main research questions:

* **Q1:** Can we outperform APE and In-Context Learning (ICL) with a DLN-1?
* **Q2:** Does network depth provide further improvement upon DLN-1?

### Experimental Setup

Datasets and TasksWe adopt a set of nine NLP and reasoning tasks commonly used in prior work studying zero- or few-shot learning capabilities of LLMs [23; 10; 39; 42; 1]. We focus on classification tasks. For tasks adopted from BigBench-Hard (BBH) [42] (Hyper., Nav., Date. and Logic.72), we use the 250 data points provided by BBH as test set. We take the remaining data points from BigBench [39] that were not included in BBH, and randomly split them (evenly) into training and validation sets. For tasks adopted from [23] (Mpqa, Trec, and Subj), we randomly sample 400 and 250 data points from their training and test sets, respectively. We use the original validation sets. For tasks adopted from Leopard [1] (Disaster and Airline), we randomly sample 400, 250, and 250 data points as training, valid, and test. We list all tasks and their statistics in Table 3 in the Appendix.

Footnote 2: We only use the variant with seven objects.

We use accuracy as the evaluation metric. Specifically, given an input, we compare a system's output string against the ground-truth output string provided by the dataset. We score 1 if the two strings are identical and 0 otherwise. Before the comparison, we process the strings from both the model output and the ground-truth to deal with issues like tokenization and capitalization. In all our DLN experiments, we perform a hyperparameter search and run the same hyperparameter setting with three random seeds. We report the test accuracy averaged over three seeds corresponding to the hyperparameter setting that achieves the highest average validation accuracy. We report details of the hyperparameter search in the Appendix I.

Throughout this paper, we use OpenAI's models, specifically GPT-3 (text-davinci-003) and GPT-4, as the backbone to our proposed systems unless otherwise specified. For DLNs, we use a batch size of 20 and train for 20 iterations by early-stopping on validation performance evaluated every 2 iterations. We then report test scores. We sample \(N=20\) prompt proposals and \(K=5\) hidden samples.

BaselinesWe compare the DLN against two classes of baseline systems. First, we test a set of systems equipped with the same backbone (i.e., GPT-3):

* 0-shot: Given an input, the LLM is required to generate the answer in a zero-shot manner.
* 5-shot (ICL): Given an input as well as five data points as in-context examples, the LLM is queried to generate an answer. The five examples are randomly sampled from the training set.
* KATE [20]: Given an input, we retrieve the five most similar data points from the training set using an off-the-shelf sentence encoder, and use them as in-context examples.
* APE [57]: The LLM is queried to generate a pool of candidate prompts for the task given few input-output pair examples. The candidate prompts are evaluated on a validation set to find the best performing instruction prompt. The best instruction is then used for 0-shot evaluation. We optimize the prompt over both 15 and 400 examples (APE-15 and APE-400 respectively).
* CoT [48]: Given an input, the LLM is first queried to generate a reasoning path with the prompt "Let's think step by step". Then, conditioned on the input and its first output, the LLM is queried to generate an answer. This is the zero-shot version of CoT and is a natural baseline for DLN-2: it performs two LLM calls and can be seen as DLN-2 without optimization. We will report performance of this baseline when comparing to DLN-2.

Additionally, we compare against one of the most advanced LLMs to date, GPT-4. We test 0-shot and ICL settings with GPT-4.

### Dln-1

Our first set of experiments evaluates the 1-layer language network (DLN-1) described in Section 2. Table 1 presents results on the full suite of test tasks. We see that it matches the performance of the best GPT-3-based method on Disaster, Mpqa and Airline and narrowly beats the best GPT-3 baseline on Logic.7 and Nav.. On Hyper., Trec, and Subj, DLN-1 significantly outperforms the best GPT-3 baseline (by about 20, 10, and 7 percentage points, respectively). On Hyper., Trec, and Disaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other tasks. DLN-1's excellent performance on Hyper., a BBH task about ordering adjectives according to linguistic convention, is a surprise. To better understand this result, we show the final prompt in Figure 2. We see that the prompt contains both instructions and a list of examples from the training set. These examples were automatically chosen by the optimizer based on their impact on the performance. This can be seen as a combination of KATE, which selects training examples to put in context based on their similarity with the test example, and APE, which selects the prompt based on its performance. On Date., DLN-1 tends to systematically under-perform the 0-shot baseline both for GPT-3 and GPT-4. We observed that DLN-1 overfits due to paucity of examples in the validation set.

\begin{table}
\begin{tabular}{l|c c c|c c c c|c c c} \hline \hline  & \multicolumn{4}{c|}{**BigBench Hard**} & \multicolumn{4}{c|}{**NLU**} & \multicolumn{4}{c}{**Leopard**} \\ \hline
**Method** & **Hyper.** & **Nav.** & **Date.** & **Logic.7** & **Mpqa** & **Tree** & **Subj** & **Disaster** & **Airline** \\ \hline
**GPT-3** & & & & & & & & & & & \\
0-shot & 60.8 & 64.1 & 56.4 & 45.9 & 88.0 & 61.9 & 61.7 & 81.6 & 75.6 \\
5-shot & 55.6 & 56.5 & 62.1 & 36.7 & 87.2 & 80.0 & 76.4 & 81.2 & 82.7 \\ KATE & 71.1 & 56.9 & 61.1 & 44.4 & 88.4 & 77.6 & 71.3\(\pm\)5.5 & 61.3\(\pm\)7.2 & 54.8\(\pm\)14.6 & 81.6 \\ APE-15 & 68.5\(\pm\)5.5 & 67.3\(\pm\)7.7 & 33.1\(\pm\)2.8 & 45.5\(\pm\)4.7 & 85.5\(\pm\)4.6 & 71.3\(\pm\)5.5 & 61.3\(\pm\)7.2 & 54.8\(\pm\)14.6 & 83.5\(\pm\)3.5 \\ APE-400 & 65.5\(\pm\)4.7 & 56.9\(\pm\)3.2 & 23.5\(\pm\)14.1 & 45.6\(\pm\)12.4 & 84.9\(\pm\)9.7 & 72.0\(\pm\)1.7 & 63.7\(\pm\)9.2 & 60.3\(\pm\)37.4 & 82.3\(\pm\)10.0 \\ \hline DLN-1 & 91.9\(\pm\)3.0 & 68.5\(\pm\)4.7 & 55.7\(\pm\)4.5 & 47.5\(\pm\)2.1 & 88.5\(\pm\)2.5 & 89.7\(\pm\)3.2 & 83.2\(\pm\)6.5 & 81.7\(\pm\)6.5 & 83.2\(\pm\)5.5 \\ \hline \hline
**GPT-4** & & & & & & & & & \\
0-shot & 64.0\(\pm\)1.0 & 74.0\(\pm\)1.0 & 79.2\(\pm\)2.6 & 68.5\(\pm\)3.5 & 86.3\(\pm\)0.6 & 64.8\(\pm\)1.7 & 72.5\(\pm\)1.5 & 47.7\(\pm\)0.6 & 84.5\(\pm\)0.6 \\
5-shot & 88.4\(\pm\)2.6 & 75.7\(\pm\)1.5 & 79.3\(\pm\)1.1 & 62.8\(\pm\)1.7 & 88.0\(\pm\)3.0 & 82.5\(\pm\)3.8 & 94.7\(\pm\)3.5 & 63.6\(\pm\)8.5 & 88.0\(\pm\)1.0 \\
16-shot & 93.3\(\pm\)2.3 & 75.5\(\pm\)5.1 & 80.9\(\pm\)5.0 & 66.4\(\pm\)3.6 & 91.3\(\pm\)1.5 & 83.7\(\pm\)0.6 & 96.5\(\pm\)2.5 & 67.1\(\pm\)4.0 & 88.3\(\pm\)2.1 \\ \hline DLN-1 & 95.2\(\pm\)5.0 & 77.1\(\pm\)4.7 & 76.7\(\pm\)3.0 & 69.1\(\pm\)2.5 & 91.1\(\pm\)3.2 & 89.5\(\pm\)2.1 & 93.1\(\pm\)5.0 & 82.1\(\pm\)3.8 & 85.9\(\pm\)1.5 \\ \hline \hline \end{tabular} 
\begin{tabular}{l|c|c|c|c|c|c|c|c|c} \hline \hline
**IDN-1** & **prom on Hyperation (GPT-4)** & & & & & & & & \\ \hline To determine the correct adjective order, follow this sequence: opinion, size, shape, age, color, origin, material, and purpose. For example, choose “large red plastic ball” over “red large plastic ball” since it follows the order: size (large), color (red), and material (plastic). Not all adjectives may be present, but the order should still be maintained. If the options are “ancient prismlike white leather” admitting match” and “leather white ancient prismlike whiting match”, choose the first option, as it follows the order: age (ancient), shape (prismlike), color (white), material (leather), and purpose (whitting). Remember that opinion always comes before age, so “on

### Dln-2

We investigate the effectiveness of depth through experiments with 2-layer language networks (DLN-2) on tasks where we expect depth to be most useful, and on which DLN-1 significantly underperforms the GPT-4 0-shot baseline, i.e., Nav., Date., and Logic.7 [42]. Since the Nav., Date. and Logic.7 tasks from BBH require more complex spatial and temporal reasoning, they are the ones where we most expect a decomposition into subtasks to be helpful. We also include Subj and Disaster as an example where DLN-1 performs well (even outperforming the GPT-4 0-shot baseline), since we are interested to see to what extent DLN-2 can further push performance.

Results for DLN-2 can be found in Table 2. Compared to DLN-1, DLN-2 provides an average boost of 7.2% absolute score. On Nav. and Date., DLN-2 largely improves the performance of DLN-1, outperforming all single layer networks. On Logic.7, all methods appear to perform similarly. This could point to the fact that the task might be too hard for the base LLM and thus highlights the limits of prompt optimization of a weak base model. On Subj and Disaster, DLN-2 achieves further improvement over DLN-1. Compared to 0-shot GPT-4 results in Table 1, on Subj and Disaster, DLN-2 on average provides more than 20% in absolute improvement. We encourage readers to find additional experimental results in Appendix C.

## 6 Related Work

**Prompt-Based Machine Learning** GPT-3 [5] launched a new paradigm in NLP called in-context learning (ICL), now applied beyond traditional NLP tasks [21]. The discovery of chain-of-thought prompts (CoT) marked a major advance in prompting: LLM performance improves markedly when the prompt includes examples of intermediate reasoning steps [48] (few-shot CoT), or simply instructs the model to "think step by step" [17] (zero-shot CoT). Like CoT, DLNs break a problem down into intermediate steps but they operationalize these steps as separate LLM calls, each defined by its own learned prompt. Since the introduction of CoT, prompting techniques have evolved to be more dynamic and iterative. Recent methods often operate recursively. Examples include RECITE [41], Self-ask [33], and related methods for question-answering Creswell et al. [6], Zhou et al. [56]. A similar class of methods relies on "introspection" [14], where an LLM is prompted to ingest, evaluate then possibly act on its own previous output. Self-critique [46], ReAct [54], Reflexion [38], Self-refine [24] fit this mould along with Hao et al. [11], Du et al. [9], Yao et al. [53].

**Prompt Optimization** Techniques based on notions of self-talk and self-evaluation align naturally with automatic prompt optimization--a core function in DLNs. Early work in this category includes Autoprompt [37] and GRIPS [32]. Deng et al. [7] argue that 'enumeration-then-selection' heuristics for optimizing discrete prompts do not explore the prompt space systematically. They take an RL approach to overcome this problem, training a policy network, via soft Q-learning with a suitably designed and stabilized reward function, to generate effective prompts. Through Gibbs sampling, Repropmpting [52] iteratively searches CoT recipes to improve prompt performance automatically. Most relevant to DLNs, Zhou et al. [57] present Automatic prompt engineer (APE). APE optimizes an initial prompt by searching over a pool of candidates to maximize a score function. We use an APE-inspired approach in DLNs and we cast the proposal/scoring functions as elements of variational inference. In a concurrent work, Pryzant et al. [34] proposed using textual gradients in automatic prompt optimization. This algorithm uses LLM's nonparametric feedback to guide prompt generation and selection.

\begin{table}
\begin{tabular}{l|l l l l l} \hline \hline
**Method** & **Nav.** & **Date.** & **Logic.7** & **Disaster** & **Subj** \\ \hline
0-shot & 64.1 & 56.4 & 45.9 & 81.6 & 61.7 \\ CoT & 69.3 & 72.4 & 41.1 & 54.4 & 59.3 \\ APE & 67.3\(\pm\)7.7 & 32.1\(\pm\)28.5 & 45.5\(\pm\)4.7 & 54.8\(\pm\)14.6 & 61.3\(\pm\)7.2 \\ APE-400 & 56.9\(\pm\)32.9 & 23.5\(\pm\)14.1 & 45.6\(\pm\)12.4 & 60.3\(\pm\)37.4 & 63.7\(\pm\)9.2 \\ \hline DLN-1 & 68.5\(\pm\)4.7 & 55.7\(\pm\)4.5 & 47.5\(\pm\)2.1 & 81.7\(\pm\)6.5 & 83.2\(\pm\)5.5 \\ DLN-2 & 83.1\(\pm\)24.7 & 75.2\(\pm\)14.8 & 45.7\(\pm\)3.5 & 82.8\(\pm\)2.5 & 85.9\(\pm\)8.7 \\ \hline \hline \end{tabular}
\end{table}
Table 2: DLN-2 test accuracy using GPT-3 as LLM.

**Multi-Layer LLM systems** Several recent works compose LLMs as nodes in a computational graph, which is the core idea of DLNs. Some work cited above can be seen as instances of this idea. Similarly, Khot et al. [15] induce an LLM to generate a basic "control flow" that calls distinct LLM modules. Wu et al. [50] propose AI chains, an interactive system of chained LLMs based on a set of "LLM primitive" operations. They conduct a 20-person user study in which participants modify chains, and find this process to improve task performance, transparency, and controllability. Dohan et al. [8] unify LLMs and graphical models as "language model cascades". Specifically, they cast LLM compositions as graphical models with string-valued random variables.3 They show how scratchpad [30], chain-of-thought [48], tool use [25], and several other prompting strategies fit their formalism. DLNs can likewise be considered an instance of language model cascade, because of that framework's generality. However, going beyond the conceptual work of Dohan et al. [8], we present an effective technique for doing inference in an LLM-based graphical model and we apply learned networks of LLMs to several downstream tasks.

Footnote 3: Earlier work by Miao and Blunsom [27] also treated strings as random variables.

## 7 Conclusion and Future Work

In this paper we introduced an algorithm for joint prompt optimization in deep networks where each layer is an LLM. To do so, we consider outputs of each hidden LLM layer as a latent variable we need to do inference over. From a conceptual perspective, we demonstrated how CoT can be seen as a DLN-2 with a residual connection. Similarly, Generated Knowledge Prompting [19] could be considered as a fixed forward-only DLN-2 where, in the first layer, an LLM generates related knowledge, and in the second layer, another LLM takes the generated knowledge as input and generates the final answer. Other prompting techniques like ReAct [54], Reflexicon [38], and Self-Consistency [46] could all be ensembles of DLN-1s with different prompt initializations.

Although we only tested 1-layer and 2-layer LNs so far, we already show that the performance of smaller LLMs can be boosted when stacked and prompted properly. We believe the modularity of these architectures will make them more adaptable and reusable to new use cases. While accuracy on downstream tasks is an appealing metric, we argue that other considerations are just as important, for example the ease of adapting a model to one's own use case, or the ability to leverage multiple existing models.

We noticed that GPT-3 has a tendency to always produce an answer given an example: this could be due to the particular 0-shot fine-tuning procedure, which biases the model towards generating useful responses. This raises the question of whether we can fine-tune "stackable" LLMs and whether DLNs can be used as a framework to generate training data for that purpose. Second, we engineered our backward and forward templates; in the future, we wish to expand our work to learn parts of such templates: we expect this to make the variational bound tighter and thus easing DLN's optimization. Additionally, while we only proposed 2-layer DLNs, the framework accommodates arbitrary directed acyclic graphs.

Impact statementWhile we are fully aware of the limitations of addressing societal issues through technical work, we hope that modular approaches like ours will alleviate some of the issues associated with LLMs, like the concentration of power associated with the difficulty to train them. We also hope that, by facilitating the reusability and adaptivity of such models, we shall make them more amenable to a wider variety of use cases. However, while we discuss the performance of these models on artificial benchmarks, we do not address the question of when and how such models should be deployed, nor do we offer additional guarantees against their misuse. We also emphasize that performance on artificial tasks, even if realistic, is neither representative of performance in uncontrolled environments, nor enough to justify the deployment of these models in high stakes situations.

AcknowledgementsWe would like to acknowledge Silviu Pitis for the useful feedback on the draft, Nikolay Malkin and Tong Wang for their advice during the first steps of this project.

## References

* [1]S. Bansal, R. Jha, and A. McCallum (2020) Learning to few-shot learn across diverse natural language classification tasks. In Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain, pp. 5108-5123. Cited by: SS1.
* [2]E. M. Bender, T. Gebru, A. McMillan-Major, and M. Mitchell (2021) On the dangers of stochastic parrots: can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610-623. Cited by: SS1.
* [3]D. M. Blei, A. Kucukelbir, and J. D. McAuliffe (2017) Variational inference: a review for statisticians. Journal of the American statistical Association112 (518), pp. 859-877. Cited by: SS1.
* [4]S. L. Blodgett, Q. V. Liao, A. Olteanu, M. Muller, M. K. Scheuerman, C. Tan, and Q. Yang (2022) Responsible language technologies: foreseeing and mitigating harms. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI EA '22, New York, NY, USA, pp. 610-623. Cited by: SS1.
* [5]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [6]A. Creswell, M. Shanahan, and I. Higgins (2022) Selection-inference: exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712. Cited by: SS1.
* [7]M. Deng, J. Wang, C. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu (2022) Rlprompt: optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 3369-3391. Cited by: SS1.
* [8]D. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous, J. Sohl-Dickstein, et al. (2022) Language model cascades. arXiv preprint arXiv:2207.10342. Cited by: SS1.
* [9]Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch (2023) Improving factuality and reasoning in language models through multiagent debate. Cited by: SS1.
* [10]T. Gao, A. Fisch, and D. Chen (2021) Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online, pp. 3816-3830. Cited by: SS1.
* [11]S. Hao, Y. Gu, H. Ma, J. J. Wang, Z. Wang, and Z. Hu (2023) Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Cited by: SS1.
* [12]O. Honovich, U. Shaham, S. R. Bowman, and O. Levy (2022) Instruction induction: from few examples to natural language task descriptions. Cited by: SS1.
* [13]A. Hosseini, S. Reddy, D. Bahdanau, R. D. Hjelm, A. Sordoni, and A. C. Courville (2021) Understanding by understanding not: modeling negation in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 1301-1312. Cited by: SS1.

[MISSING_PAGE_POST]

* [15] Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. (2023). Decomposed prompting: A modular approach for solving complex tasks. _International Conference on Learning Representations_.
* [16] Kingma, D. P. and Welling, M. (2022). Auto-encoding variational bayes.
* [17] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models are zero-shot reasoners. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, _Advances in Neural Information Processing Systems_, volume 35, pages 22199-22213. Curran Associates, Inc.
* [18] Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. (2022). Internet-augmented language models through few-shot prompting for open-domain question answering. _arXiv preprint arXiv:2203.05115_.
* [19] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Le Bras, R., Choi, Y., and Hajishirzi, H. (2022a). Generated knowledge prompting for commonsense reasoning. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3154-3169.
* [20] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. (2021). What makes good in-context examples for gpt-3?
* [21] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, 55(9):1-35.
* [22] Liu, R., Wei, J., Gu, S. S., Wu, T.-Y., Vosoughi, S., Cui, C., Zhou, D., and Dai, A. M. (2022b). Mind's eye: Grounded language model reasoning through simulation. _arXiv preprint arXiv:2210.05359_.
* [23] Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. (2022). Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.
* [24] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Welleck, S., Majumder, B. P., Gupta, S., Yazdanbakhsh, A., and Clark, P. (2023). Self-refine: Iterative refinement with self-feedback.
* [25] Mialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y., and Scialom, T. (2023). Augmented language models: a survey.
* [26] Miao, Y. and Blunsom, P. (2016a). Language as a latent variable: Discrete generative models for sentence compression. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 319-328, Austin, Texas. Association for Computational Linguistics.
* [27] Miao, Y. and Blunsom, P. (2016b). Language as a latent variable: Discrete generative models for sentence compression. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 319-328.
* [28] Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes in-context learning work? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 11048-11064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
* [29] Mukherjee, S. and Awadallah, A. H. (2020). Xtremedistil: Multi-stage distillation for massive multilingual models. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R., editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 2221-2234. Association for Computational Linguistics.

* [30] Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. (2021). Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_.
* [31] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.
* [32] Prasad, A., Hase, P., Zhou, X., and Bansal, M. (2022). Gripps: Gradient-free, edit-based instruction search for prompting large language models. _arXiv preprint arXiv:2203.07281_.
* [33] Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. (2022). Measuring and narrowing the compositionality gap in language models. _arXiv preprint arXiv:2210.03350_.
* [34] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. (2023). Automatic prompt optimization with gradient descent and beam search. _arXiv preprint arXiv:2305.03495_.
* [35] Rubin, O., Herzig, J., and Berant, J. (2022). Learning to retrieve prompts for in-context learning. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2655-2671.
* [36] Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. _CoRR_, abs/1910.01108.
* [37] Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. (2020). Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4222-4235.
* [38] Shinn, N., Labash, B., and Gopinath, A. (2023). Reflexion: an autonomous agent with dynamic memory and self-reflection. _arXiv preprint arXiv:2303.11366_.
* [39] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_.
* [40] Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N. A., et al. (2023). Selective annotation makes language models better few-shot learners. _International Conference on Learning Representations_.
* [41] Sun, Z., Wang, X., Tay, Y., Yang, Y., and Zhou, D. (2022). Recitation-augmented language models. _arXiv preprint arXiv:2210.01296_.
* [42] Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D.,, and Wei, J. (2022). Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_.
* [43] Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin, J. (2019). Distilling task-specific knowledge from BERT into simple neural networks. _CoRR_, abs/1903.12136.
* [44] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. (2023a). Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.
* [45] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esibou, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. (2023b). Llama 2: Open foundation and fine-tuned chat models.

* [46] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2023a). Self-consistency improves chain of thought reasoning in language models. _International Conference on Learning Representations_.
* [47] Wang, X., Zhu, W., Saxon, M., Steyvers, M., and Wang, W. Y. (2023b). Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning.
* [48] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_.
* [49] Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., et al. (2022). Taxonomy of risks posed by language models. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 214-229.
* [50] Wu, T., Terry, M., and Cai, C. J. (2022). Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pages 1-22.
* [51] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. (2023a). Wizardlm: Empowering large language models to follow complex instructions.
* [52] Xu, W., Banburski-Fahey, A., and Jojic, N. (2023b). Reprompting: Automated chain-of-thought prompt inference through gibbs sampling. _arXiv preprint arXiv:2305.09993_.
* [53] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. (2023a). Tree of thoughts: Deliberate problem solving with large language models.
* [54] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023b). React: Synergizing reasoning and acting in language models. _International Conference on Learning Representations_.
* [55] Zhang, Z., Zhang, A., Li, M., and Smola, A. (2023). Automatic chain of thought prompting in large language models. _International Conference on Learning Representations_.
* [56] Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. (2023a). Least-to-most prompting enables complex reasoning in large language models. _International Conference on Learning Representations_.
* [57] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023b). Large language models are human-level prompt engineers. _International Conference on Learning Representations_.

#### Contents in Appendices:

* In Appendix A, we list the contribution of each author to this work.
* In Appendix B, we provide additional experimental details including task statistics and the prompt strings we used to initialize DLN.
* In Appendix C, we provide additional experiments and baselines we compare to.
* In Appendix D, we provide forward and backward templates being used in DLN.
* In Appendix E, we provide an algorithm that generalizes DLN training in a multiple layer setting.
* In Appendix F, we show examples of learned weights that exhibit behavior similar to in-context learning.
* In Appendix G, we show examples of learned weights by 2-Layer DLNs.
* In Appendix H, we show an example of the hidden states produced by a 2-Layer DLN.
* In Appendix I, we provide implementation details, including hyperparameter information.
* In Appendix J, we discuss resource used in DLN development and their pricing.

## Appendix A Contributions

Alessandro Sordoni proposed the general idea of DLN, where multiple prompts are learnt at each layer through backward natural language operations; they proposed to generate synthetic in-context examples and the exploration reward for DLN-2; they wrote the code and ran the experiments; they focused on Sections 2, 3, 4 and contribute writing the rest of the sections.

Xingdi Yuan co-developed the basic idea of DLN and wrote part of the code, they also co-designed and helped conducting experiments. They contributed to the writing of the paper, mainly Sections 5 and 6.

Marc-Alexandre Cote helped with the experiments and the infrastructure to make calls to OpenAI models. They also built a demo to visualize the evolution of DLN's prompts during training and contributed to the writing of the paper, mainly focusing on the algorithms and the appendix.

Matheus Pereira co-coded an earlier, non-variational backwards operator with AT, helped with the APE and DLN-2 layers experiments, implemented the method for estimating the total cost of experiments, build a demo to visualize the evolution of DLN's prompts during training, and contributed to the release of the DLN code.

Adam Trischler helped with template conception and iteration, co-coded an earlier, non-variational backwards operator with MP, and contributed to paper writing, mainly the literature review.

Ziang Xiao helped with the model evaluation and experiment setup and contributed to the paper writing, mainly the literature review and discussion.

Arian Hosseini participated in the development discussions throughout the project and contributed to writing the literature review of the paper.

Friederike Niedtner organized and managed the project, helping the team focus on the right priorities.

Nicolas Le Roux proposed the variational inference formulation and the posterior sharpening. They offered guidance and mentorship for the project. They also contributed to the writing of the paper, mainly sections 1, 2, and 3.

## Appendix B Additional Experimental Details

### Additional Task Information

In Table 3, we provide short descriptions for all tasks we use and their statistics.

### Prompt Initialization

We initialize the "classification" layer of the DLNs, i.e. the first layer of the 1-Layer LN and the second layer of the 2-layer DLN, with a question or a task description as reported in Table 4. We use these same initializations to compute the 0-shot performance. Therefore, at initialization, a 1-layer LN is equivalent to the 0-shot baseline. For the hidden layer of the 2-layer DLN, we initialize the prompt to "Decompose the problem to make it simpler:" for Nav. and Subj, and "

\begin{table}
\begin{tabular}{l|c c c c l} \hline \hline Task & train & valid & test & class & Description \\ \hline Mpqa & 400 & 256 & 250 & 2 & Sentiment analysis. \\ Tree & 400 & 256 & 250 & 6 & Question type classification. \\ Subj & 400 & 256 & 250 & 2 & Determine whether a sentence is subjective or objective. \\ Disaster & 400 & 250 & 250 & 2 & Determine whether a sentence is relevant to a disaster. \\ Airline & 400 & 250 & 250 & 3 & Airline tweet sentiment analysis. \\ Hyper. & 400 & 1000 & 250 & 2 & Order adjectives correctly in English sentences. \\ Nav. & 375 & 375 & 250 & 2 & Spatial reasoning given navigation instructions. \\ Date. & 59 & 60 & 250 & 6 & Infer a date from context. \\ Logic.7 & 225 & 225 & 250 & 7 & Deduce the order of seven objects given instruction. \\ \hline \hline \end{tabular}
\end{table}
Table 3: Tasks used in this work.

[MISSING_PAGE_FAIL:17]

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline
**Method** & **Nav.** & **Date.** & **Logic.7** & **Subj** \\ \hline LCL - 5-shot & 56.5 & 62.1 & 36.7 & 76.4 \\ LCL - 10-shot & 61.3 & 62.9 & 38.9 & 72.0 \\ LCL - 32-shot & 66.0 & 63.5 & - & 83.2 \\ KATE - 5-shot & 56.9 & 61.1 & 44.4 & 71.1 \\ KATE - 10-shot & 59.5 & 62.0 & 41.6 & 73.9 \\ KATE - 32-shot & 67.5 & 62.8 & - & 80.4 \\ \hline DLN-1 & 68.5\(\pm\) 4.7 & 55.7\(\pm\) 4.5 & 47.5\(\pm\) 2.1 & 83.2\(\pm\)5.5 \\ DLN-2 & 83.1\(\pm\) 24.7 & 75.2\(\pm\) 14.8 & 45.7\(\pm\) 3.5 & 85.9\(\pm\)8.7 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Test accuracy averaged over three random seeds with 95% confidence interval (where applicable). All models use GPT-3. Increasing the number of LCL examples helps performance, but cannot match DLN-1 and DLN-2 in general. Context length limit is an issue for ICL and KATE 32-shot.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
**Method** & **Nav.** & **Logic.7** & **Subj** \\ \hline
0-shot & 58.0 & 0.0 & 65.8 \\
5-shot & 56.0 & 28.0 & 50.8 \\ \hline DLN-1 & 61.1 & 31.0 & 79.8 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Test accuracy using WizardLM-v1.2 13B as LLM. This open source model seems significantly less able to capture few-shot examples from the context. DLN-1 outperforms ICL on all tasks here.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
**Method** & **Nav.** & **Date.** & **Logic.7** & **Subj** \\ \hline
0-shot & 42.0\(\pm\)0.0 & 25.2\(\pm\)0.0 & 14.4\(\pm\)0.0 & 62.4\(\pm\)0.0 \\
5-shot & 43.2\(\pm\)12.5 & 21.1\(\pm\)9.7 & 16.4\(\pm\)2.6 & 67.7\(\pm\)15.5 \\ \hline DLN-1 + GPT3 & 43.6\(\pm\)4.0 & 21.9\(\pm\)5.7 & 33.1\(\pm\)10.9 & 80.9\(\pm\)11.5 \\ DLN-1 & 44.9\(\pm\)6.4 & 31.6\(\pm\)10.5 & 38.4\(\pm\)3.6 & 76.1\(\pm\)4.5 \\ \hline DLN-2 + GPT3 & 43.7\(\pm\)3.0 & 51.1\(\pm\)4.0 & 21.9\(\pm\)4.9 & 59.1\(\pm\)16.7 \\ DLN-2 & 68.9\(\pm\)14.4 & 61.7\(\pm\)17.6 & 20.0\(\pm\)13.7 & 63.1\(\pm\)25.4 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test accuracy averaged over three random seeds with 95% confidence interval. All methods use LLaMA2-70B-Chat as LLM, with DLN + GPT3 employing text-davinci-003 as the backward LLM for prompt and hidden proposals.

Templates

### Forward Templates

"Classification" Template F for 1-Layer LNIn the 1-layer LN, we use the following template to elicit the output \(y\) given the input \(x\). prompt is substituted with the value of the current prompt.

[background]

Classification Template F

template:

(| prompt|)

[(input)]

Answer:

[background]

Residual Classification Template F for 2-Layer DLNIn the 2-layer LN, the last layer just concatenates the input to the output of the first layer, \(\hat{h}\), before eliciting an answer.

[background]

Restful classification template B

template:

(| prompt|)

[(| prompt|)

Your thoughts were:

(| h|)

Answer:

[background]

Hidden Layer FThe variable prompt is substituted with the value of the current prompt \(\pi_{0}\). This has the effect of providing additional information about how "Let's think step by step" should behave.

[background]

template:

(| prompt|) Let's think step by step.

For 2-Layer DLN on Subj, we use the following hidden template. We couldn't run with the previous template due to lack of time, as we observed that the step by step trigger tended to generate lengthy hidden states.

[background]

Hidden Layer F

template:

(| prompt|)

[(| input)]

Brief Analysis:

[background]

### Backward Templates (Prompt and Hidden Proposals)

Prompt Proposal Template B\({}_{\pi}\)B\({}_{\pi}\) is used to propose new candidate prompts. The template takes as input the current prompt, prompt, and a mini-batch of examples stored in backward_infos. backward_info.input stores the input to the layer (\(x\) if we are proposing prompts for the first layer or \(h\) if it is the second layer); backward_info.target stores the target to the layer (\(h^{*}\) if it is the first layer or \(y\) otherwise); backward_info.output stores the predictions of the model during the forward pass (\(\hat{h}\) if it is the first layer, and \(\hat{y}\) if it is the second layer). message is substituted with one of the message_alternatives, sampled at random during the DLN training. This induces diversity in the generated prompts and allows emergence of learning to ICL behaviors, where the prompts contain synthetic examples that can help solve the task.

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

## Appendix I Implementation Details

We report hyperparameter search space in Table 10. A brief description of the hyperparameters is as follows:

* bh_tpl is the type of backward prompt template we use B\({}_{\pi}\). v3.5 is equal to the B\({}_{\pi}\) we report in Section D. In v3.0, we remove "Be concise." at the end of each message_alternatives. We noticed that in general v3.5 works better as it implements a sort of regularization on the length of the found prompts. Future work could address length regularization in a more principled manner.
* logp_penalty is the coefficient for the exploration reward we mentioned in the paper.
* num_h_samples is the number of \(h\) samples to generate from the approximate posterior distribution.
* use_memory is whether or not we use the backtracking mechanism. Usually 2 works well across tasks.
* held_out_prompt_ranking describes whether we use only half of the mini-batch examples for each prompt proposal, as described in the main paper.
* tolerance describes after how many iterations we reload the best weights found during the last validation if the current validation score is lower than the best score obtained so far.

For the 2-Layer experiments, we have to restrict this search space due to computational costs. We use bh_tpl = "v3.5", tolerance = 2, use_memory = 2, held_out_prompt_ranking = True, logp_penalty = 0.5.

## Appendix J Pricing

We keep track the number of tokens we interact with GPT-3 via its online API. According to OpenAI's pricing policy, user pays for both the input tokens (prompts) and the output tokens. Using the Hyperbaton task as an example, while training a 1-layer LN, the total number of tokens we use is

\begin{table}
\begin{tabular}{l|l} \hline \hline
**hyperparam** & **search space** \\ \hline
**1-Layer LN** \\ \hline bh\_tpl & q\_action\_prompt:v3.0, q\_action\_prompt:v3.5 \\ \hline tolerance & -1, 0, 2 \\ \hline use\_memory & 0, 2 \\ \hline held\_out\_prompt\_ranking & True, False \\ \hline
**2-Layer DLN PT + fix 2nd layer** \\ \hline bh\_tpl & q\_action\_prompt:v3.0, q\_action\_prompt:v3.5 \\ \hline logp\_penalty & 0., 0.5, 2. \\ \hline
**2-Layer DLN PT + fine-tune 2nd layer** \\ \hline bh\_tpl & q\_action\_prompt:v3.0, q\_action\_prompt:v3.5 \\ \hline
**logp\_penalty** & 0., 0.5, 2. \\ \hline
**2-Layer DLN end-to-end** \\ \hline num\_h\_samples & 5, 10 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Hyperparameter search space.

2,941,360. For a 2-layer DLN, the total number of tokens we use is 13,654,962. According to the current price for GPT-3 (50.02/1k tokens), a single run of a 1-layer and 2-layer DLN cost roughly 59 USD and 273 USD, respectively.

In Table 11, we report the cost (lower is better) in terms of total number of tokens for the test set (prompts included). We emphasize the cost at the testing time because it is more relevant in real-world deployment and the training cost is one-off. We can see DLN-1 improves over ICL on 5 out of 9 tasks on GPT-4 at a comparable token cost. Some tasks do not benefit from ICL (i.e. reasoning tasks) while other tasks like Subj, Trec, and Hyper. benefit significantly.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c} \hline \hline  & \multicolumn{4}{c|}{**BigBench Hard**} & \multicolumn{2}{c|}{**NLU**} & \multicolumn{2}{c}{**Leopard**} \\ \hline
**Method** & **Hyper.** & **Nav.** & **Date.** & **Logic.7** & **Mpqa** & **Trec** & **Subj** & **Disaster** & **Airline** \\ \hline
**GPT-4** & & & & & & & & & \\
0-shot & 64.0\(\pm\)1.0 & 74.0\(\pm\)1.0 & 79.2\(\pm\)2.6 & 68.5\(\pm\)3.5 & 86.3\(\pm\)0.6 & 64.8\(\pm\)1.7 & 72.5\(\pm\)1.5 & 47.7\(\pm\)0.6 & 84.5\(\pm\)0.6 \\  & (7.63) & (12.9\(\pm\)3.6) & (23.6\(\pm\)2.0) & (3.7\(\pm\)0.6) & (7.6\(\pm\)0.2) & (10.2\(\pm\)0.1) & (0.9\(\pm\)0.8) \\
5-shot & 88.4\(\pm\)2.6 & 75.7\(\pm\)1.5 & 79.3\(\pm\)3.1 & 62.8\(\pm\)1.7 & 88.0\(\pm\)3.0 & 82.5\(\pm\)3.8 & 94.7\(\pm\)3.5 & 63.6\(\pm\)8.5 & 80.0\(\pm\)1.0 \\  & (45.0\(\pm\)3) & (79.2\(\pm\)2.0) & (143.3\(\pm\)3) & (287.5\(\pm\)0.5) & (24.5\(\pm\)3.5) & (52.8\(\pm\)0.6) & (63.5\(\pm\)3.5) & (61.7\(\pm\)3.5) \\
16-shot & 93.3\(\pm\)2.3 & 75.5\(\pm\)5.1 & 80.9\(\pm\)5.0 & 66.4\(\pm\)3.6 & 91.1\(\pm\)1.5 & 83.7\(\pm\)0.6 & **96.5\(\pm\)2.5** & **67.1\(\pm\)4.0** & **83.3\(\pm\)2.1** \\  & (13.5\(\pm\)3.0) & (29.9\(\pm\)5.0) & (40.5\(\pm\)10) & (17.6\(\pm\)0.6) & (70.3\(\pm\)0.3) & (41.0\(\pm\)0.6) & (17.9\(\pm\)0.3) & (17.9\(\pm\)0.3) & (17.5\(\pm\)3.0) \\ \hline DLN-1 & 92.5\(\pm\)5.0 & 77.1\(\pm\)4.7 & 74.3\(\pm\)1.5 & 69.1\(\pm\)2.5 & 91.1\(\pm\)3.2 & 89.5\(\pm\)2.1 & 93.1\(\pm\)5.0 & 82.1\(\pm\)3.8 & **85.9\(\pm\)1.5** \\  & (77.2\(\pm\)3) & (29.9\(\pm\)5.0) & (52.3\(\pm\)3.0) & (68.5\(\pm\)3.0) & (65.4\(\pm\)3.0) & (120.7\(\pm\)0.7) & (66.5\(\pm\)0.5) & (47.1\(\pm\)3.8) & (38.2\(\pm\)3.6) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Test accuracy along with inference cost expressed in tokens (in gray) averaged over three random seeds of a shallow, 1-layer language network (DLN-1) compared to baselines on GPT-4. We also report the 95% confidence interval on the test accuracy. We emphasize the cost at the testing time because it is more relevant in real-world deployment and the training cost is one-off.