WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts

Jiahuan Cao\({}^{1,3}\), Yang Liu\({}^{1,3}\), Yongxin Shi\({}^{1,3}\), Kai Ding\({}^{2,3}\), Lianwen Jin\({}^{*}\)\({}^{1,3}\)

\({}^{1}\)South China University of Technology

\({}^{2}\)INTSIG Information Co., Ltd

\({}^{3}\)INTSIG-SCUT Joint Lab on Document Analysis and Recognition

jiahuanc@foxmail.com, ly10061105@gmail.com, yongxin_shi@foxmail.com

danny_ding@intsig.net, eelwjin@scut.edu.cn

Equal contribution

###### Abstract

Large Language Models (LLMs) have made significant advancements across numerous domains, but their capabilities in Chinese Classical Literature and Language Arts (CCLLA) remain largely unexplored due to the limited scope and tasks of existing benchmarks. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Based on WenMind, we conduct a thorough evaluation of 31 representative LLMs, including general-purpose models and ancient Chinese LLMs. The results reveal that even the best-performing model, ERNIE-4.0, only achieves a total score of 64.3, indicating significant room for improvement of LLMs in the CCLLA domain. We also provide insights into the strengths and weaknesses of different LLMs and highlight the importance of pre-training data in achieving better results. Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research. Our benchmark and related code are available at https://github.com/SCUT-DLVCLab/WenMind.

## 1 Introduction

The emergence of Large Language Models (LLMs) has led to significant advancements in natural language processing and understanding across a wide range of domains, from finance [1; 2; 3] and law [4; 5; 6] to healthcare [7; 8; 9]. However, the capabilities of LLMs in the domain of Chinese Classical Literature and Language Arts (CCLLA) have not been fully explored, largely due to the lack of comprehensive benchmarks. CCLLA, which encompasses the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, serves as an essential bridge connecting ancient Chinese culture to the modern world. Evaluating and improving the performance of LLMs in this domain is crucial for furthering research and applications in CCLLA. Unfortunately, existing benchmarks focus primarily on the Ancient Prose sub-domain or contain only a limited number of tasks, making it challenging to provide a thorough and holistic assessment of LLMs' capabilities in CCLLA.

To address this gap, we introduce the WenMind benchmark for a comprehensive evaluation of LLMs' CCLLA capacities. Our WenMind offers several benefits over existing CCLLA benchmarks. **(a)**

**Comprehensive coverage**: Unlike current benchmarks that concentrate primarily on Ancient Prose, WenMind provides a holistic approach by encompassing all three sub-domains. **(b) Substantial number of tasks**: As depicted in Figure 1, WenMind consists of 42 fine-grained tasks, which is a remarkable twofold increase over the largest existing benchmark. **(c) Variety of question formats**: WenMind incorporates a wide array of question formats, including multiple choice, fill-in-the-blank, and open-ended questions. This diversity allows for a more detailed analysis of an LLM's grasp of CCLLA. The comparison between WenMind and other benchmarks is shown in Table 1. Figure 2 presents samples of WenMind's data.

Using WenMind, we thoroughly evaluate 31 representative LLMs, including general models in Chinese and English, as well as specialized models for ancient Chinese. Our findings indicate that even the top-performing model, ERNIE-4.0 [10], only achieves the highest score of 64.3, suggesting considerable room for improvement in the CCLLA domain. Further analysis shows that the lack of knowledge of CCLLA is the main reason for existing LLMs' poor performance. Unexpectedly, LLMs specifically for ancient Chinese rather underperform the general ones. We think this is because the incremental pre-training corpus is not extensive enough to cover the scope of CCLLA, and concurrently leads to a catastrophic forgetting of the generic knowledge. Furthermore, we compare traditional metrics with model scoring metric in translation and punctuation tasks, providing a comparative analysis of their respective merits and limitations.

In summary, our contributions are as follows:

* We introduce WenMind, a novel comprehensive evaluation benchmark specific for LLMs in CCLLA, which covers all three sub-domains in this domain and contains a diverse range of tasks and question formats, facilitating a rigorous and thorough evaluation of LLMs' capabilities.

Figure 1: Overview of WenMind Benchmark, which covers 3 sub-domains and 42 fine-gained tasks.

Figure 2: Samples of WenMind’s data.

* Using WenMind, we conduct an extensive and thorough evaluation of 31 representative LLMs, revealing and quantifying their performance in the CCLLA domain.
* We perform an in-depth analysis of the evaluation results and obtain valuable insights, offering significant guidance and profound understanding for future research on LLMs in the CCLLA domain.

## 2 Related Work

### General Chinese Benchmark for LLMs

To evaluate the performance of LLMs across diverse Chinese language tasks, several benchmarks have been proposed [24; 25; 26]. CLUE [27] is the first large-scale Chinese comprehension benchmark, including nine tasks such as sentence classification and reading comprehension. SuperCLUE [28] expands the evaluation to include user queries, open-ended dialogues, and closed-ended questions, focusing on real-world applications. CMMLU [13] aims to comprehensively assess the knowledge and reasoning capabilities of LLMs in Chinese, covering 67 subjects from basic to advanced levels. C-Eval [11] offers a thorough evaluation suite with 13,948 multiple-choice questions spanning 52 subjects, targeting foundational knowledge and reasoning skills. HalluQA [29] addresses the hallucination phenomenon in Chinese LLMs with 450 adversarial questions encompassing cultural and social aspects. AlignBench [30] evaluates the alignment of models across multiple dimensions. CBBQ [31], a bias benchmark, covers stereotypes and social biases relevant to Chinese culture with over 100 expert-constructed questions.

### CCLLA Benchmark for LLMs

Unlike general Chinese benchmarks, CCLLA benchmarks are limited in number. C-CLUE [17] provides a dataset for evaluating named entity recognition and relation extraction. CCLUE [18] offers five evaluation tasks, including sequence labeling and sentence classification. ACLUE [16] proposes a benchmark to evaluate classical Chinese understanding capabilities of LLMs, with the form of multiple-choice questions. WYWEB [23] offers nine tasks, such as text classification, punctuation and machine translation. However, these existing CCLLA benchmarks contain only a small number of tasks or are organized using only a single multiple-choice question format. Compared to these datasets, WenMind covers a wider variety of tasks and formats, and employs evaluation methods that align more closely with human intuition, allowing it to more accurately and comprehensively reflect the capabilities of LLMs in CCLLA.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**Dataset** & **Year** & **Domain** & **License** & **Scale** & **\# Tasks** & **\# QF** & \multicolumn{3}{c}{**Metric**} & \multicolumn{3}{c}{**Method**} \\ \cline{1-1} \cline{6-11} C-Eval [11] & 2023 & General & CC BY-NC-SA-4.0 & 457 & 2 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ CIF-Bench [12] & 2024 & General & - & 150 & 3 & 1 & ✓ & ✓ & ✓ & ✗ & ✗ \\ CMMLU [13] & 2023 & General & CC BY-NC-4.0 & 620 & 3 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ GAOK-Bench [14] & 2023 & General & Apache-2.0 & 145 & 4 & 3 & ✓ & ✓ & ✗ & ✗ & ✗ \\ XiezhiBenchmark [15] & 2023 & General & CC BY-NC-SA-4.0 & 2,060 & 2 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ ACLUE [16] & 2023 & CCLLA & CC BY-NC-4.0 & 4,967 & 15 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ C-CLUE [17] & 2021 & CCLLA & CC BY-SA-4.0 & 1,122 & 3 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ CCLUE [18] & 2021 & CCLLA & Apache-2.0 & 36,319 & 5 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ CPM [19] & 2021 & CCLLA & - & 2,720 & 1 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ THIAIber [20; 21; 22] & 2020 & CCLLA & - & 5,173 & 2 & 2 & ✓ & ✗ & ✗ & ✗ & ✗ \\ WYWEB [23] & 2023 & CCLLA & - & 69,700 & 9 & 2 & ✓ & ✗ & ✗ & ✗ & ✗ \\ \hline
**WenMind (Ours)** & 2024 & CCLLA & CC BY-NC-SA-4.0 & 4,875 & **42** & 3 & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of existing datasets. “CCLLA” represents “Chinese Classical Literature and Language Arts”; “QF” represents “Question Fromat”; “TM” represents “Traditional Metrics”; “MSM” represents “Model Scoring Metric”; “Method” represents “Construction Method of dataset”; “HG” represents “Human Generated”; “CI” represents “Collection and Improvement of existing datasets”; “MC” represents “Model Constructed”. Datasets with the domain “General” only count relevant data entries in the CCLLA field.

## 3 WenMind Benchmark

### Task Definition

This study aims to construct a comprehensive benchmark for the evaluation of LLMs' capabilities in Chinese Classical Literature and Language Arts (CCLLA). In general, the CCLLA contains three sub-domains: Ancient Prose, Ancient Poetry, and Ancient Literary Culture. Therefore, we define the following tasks for these domains, respectively.

**Ancient Prose** Ancient prose refers to a form of classical Chinese literature, which characterized by simplicity, elegance and a high degree of freedom. It is commonly used in historical records, philosophical texts, agricultural books, _etc_. To evaluate the models' capabilities in Ancient Prose, we design a set of tasks that assess the understanding and generation, respectively. We focus on the models' **understanding capability at both sentence level and word level** through 11 tasks: Tasks that pertain to the sentence level include sentence structure, classical Chinese to modern Chinese, modern Chinese to classical Chinese, topic classification, and reading comprehension. On the other hand, tasks that pertain to the word level encompass named entity recognition, punctuation, word explanation, function words, homophones, and polysemy. For **generation** capability, we directly utilize ancient prose writing as the evaluation task.

**Ancient Poetry** Ancient poetry is a special form of classical Chinese literature, which follows a strict rhyme scheme and often expresses rich emotions in refined language. Considering the characteristics of ancient poetry, we design the following tasks to assess the ability in three dimensions of LLMs. (a) Appreciation, ancient poetry translation, sentiment classification, and ancient poetry to English, for **understanding** capability. (b) Ancient poetry writing for **generation** capability. (c) Basic Q&A, poet introduction, and analysis of imagery, for **knowledge** capability.

**Ancient Literary Culture** Ancient Literary Culture refers to literary forms other than Ancient Prose and Ancient Poetry, such as riddle, idiom, _etc_. We primarily assess the generation and knowledge capabilities of LLMs in this sub-domain. (a) For **generation** capability, we utilize couplet as the evaluation task. (b) For **knowledge** capability, we utilize 5 tasks, including idiom, riddle, Xiehouyu, historical Chinese phonology, and sinology.

### Data Construction

The construction pipeline of WenMind includes data collection and data processing, as illustrated in Figure 3.

Data CollectionOur data collection process includes three main sources. **(a) Internet:** We collect authentic examination questions from Chinese language exams and poetry competitions, which are carefully curated by experts and scholars, to serve as part of our data. Additionally, we gather texts that are enriched with CCLLA knowledge from the Internet, such as introductions of poets and interpretations of idioms. Utilize the knowledge in these texts, we manually construct question-answer pairs for various tasks. **(b) Open-Source Datasets:** Leveraging open-source corpora like C2MChn [32] and the Daizhige Corpus [33] as foundational resources, we perform a series of operations, including text filtering, question crafting, and format standardization, to generate question-answer pairs. Given the challenge of acquiring data for certain tasks and to ensure a thorough and holistic evaluation within the CCLLA domain, we also incorporate test cases from other open-source datasets, such as ACLUE [16], WYWEB [23], and THU-FSPC [22]. **(c) LLM:** Tasks such as ancient poetry and prose writing are inherently open-ended and do not have fixed correct answers. For these tasks, we initially design a range of questions and then employ the ERNIE-3.5 model [10] to generate reference answers. These answers are further reviewed, filtered, and refined through a manual process. It is notable that the WenMind benchmark is released under the CC-BY-NC-SA-4.0 license and strictly adheres to the agreements of the original data sources. For more details, please refer to Appendix B.

Data ProcessingTo ensure the data quality, we perform a series of data processing on the collected data. **(a) Question Segmentation.** Some question-answer pairs consist of multiple questions intertwined. We employ the ERNIE-3.5 model [10] to distinguish between multiple questions and answers mixed together. **(b) Conversion and Standardization of Question-Answer Pairs.** Weensure that the data is presented in question-answer pairs, standardized in a dictionary format. Each sample is accompanied by metadata, including the task name and the capability being assessed. **(c) Data Deduplication.** A combination of MinHashLSH [34] and field-matching methods is employed to remove duplicate questions. **(d) Removal of Irrelevant Symbols and Content.** We utilize regular expressions, detection of irrelevant characters, and other methods to eliminate unnecessary English characters, abnormal symbols, erroneous data, _etc_[35, 36, 37]. **(e) Manual Proofreading.** We manually verify all question-answer pairs to ensure the integrity of the questions and the correctness of the answers.

### Data Statistics

We present the statistics of WenMind in Figure 4 and Table 2. The WenMind benchmark comprises a total of 4,875 entries, encompassing 26 coarse-grained tasks and 42 fine-grained tasks. Tasks involving Ancient Prose, Ancient Poetry, and Ancient Literary Culture respectively account for 39%, 38%, and 23% of the benchmark, showing a relatively balanced distribution. Among these, the task with the highest number of entries is basic Q&A on ancient poetry, constituting approximately 15% of the benchmark, while tasks such as English translations of ancient poetry have the fewest entries, at around 1%. This reflects the benchmark's design, which prioritizes tasks based on their common occurrence and general applicability. From the question format perspective, WenMind comprises 3,928 open-ended questions, 917 multiple-choice questions, and 30 fill-in-the-blank

Figure 4: Data statistics of WenMind: Distributions of (a) sentence length, (b) sub-domains and (c) capabilities. Zoom in for better view.

Figure 3: Construction pipeline of WenMind Benchmark. Zoom in for better view.

questions. Regarding cognitive capabilities, questions assessing understanding, generation, and knowledge account for 51%, 10%, and 38% respectively.

## 4 Experiment

### Models

We conduct an extensive evaluation of 31 models, both proprietary and open-source, encompassing English-centric models such as GPT-4 [38] and LLaMA [39; 40], Chinese-centric models like Qwen [41], Baichuan [42] and ChatGLM [43; 44], models fine-tuned from English to Chinese such as LLaMA-Chinese [45; 46], and specialized models for ancient Chinese, including Xunzi [47] and Chunhua [48]. Additionally, we assess different-sized variants within the same model family to reveal the effect of model size on their performance in the CCLLA domain. Details of the evaluated models are presented in Appendix C.

### Experiment Setup

For open-source and closed-source models we evaluate them through local access and API calls, respectively. To ensure a fair comparison, we standardize the inference settings for all evaluated LLMs. Specifically, we employ half-precision inference with bf16 and greedy decoding strategy with a maximum generation length set to 2048. The temperature parameter, Top-p sampling, and Top-k sampling are set to 1, 1, and 50, respectively. To evaluate the knowledge of LLM itself, we prohibit the use of external search engines for closed-source models. For model scoring, we randomly sample 100 instances and score them using LLMs three times, achieving an average error within 2%. This level of precision indicates that LLMs exhibit scoring stability, and therefore, we decide not to pursue multiple averaging scores due to cost considerations. All experiments are conducted on a NVIDIA A6000 GPU.

### Evaluation Metrics

Model Scoring MetricThe evaluation of the WenMind benchmark mainly employs model scoring. An optimal scoring model should meet the following criteria: (a) Exhibit excellent instruction-following capabilities, generating scores based on specified prompts and outputting them in the required format. (b) Possess extensive knowledge in the CCLLA domain to aid the scoring process. (c) Balance scoring effectiveness and cost efficiency. (d) Align closely with human subjective judgment in scoring results. After experimentation, the ERNIE-3.5 model [10] has been selected as the experimental scoring model, as it exhibits a consistency of approximately 89.4% with human evaluation, meeting the required criteria. More details can be found in Appendix C.

Furthermore, the scoring requirements vary for different formats of questions. (a) For multiple-choice questions with only one correct option, the model gives a score of 0 (incorrect) or 1 (correct). (b) For multiple-choice questions with multiple correct options, the model gives a score of 0 (incorrect options present), 0.5 (partially correct options), or 1 (all options correct). (c) For subjective questions without a standard answer, the model scores between 0 and 1 based on the given requirements. (d) For Q&A questions with a standard answer, the model identifies several scoring points (\(P_{all}\)) based on the reference answer and then determines the number of these points present in the LLM's response (\(P_{obtain}\)) to calculate the score for the question(\(P_{obtain}\)/\(P_{all}\)). All indicators are multiplied by one hundred to obtain final scores. The scoring prompts for various tasks refer to Appendix C.2.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Domain** & **Tasks** & **\#Q** & **Max. \#Q** & **Min. \#Q** & **Avg. Q Tokens** & **Avg. A Tokens** \\ \hline Ancient Prose & 15 & 1,900 & 200 & 7 & 107.51 & 62.12 \\ Ancient Poetry & 16 & 1,845 & 200 & 20 & 73.42 & 94.93 \\ Ancient Literary Culture & 11 & 1,130 & 100 & 100 & 26.68 & 14.26 \\ \hline
**Overall** & 42 & 4,875 & 200 & 7 & 75.87 & 63.44 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The statistics of the WenMind Benchmark. “Q” represents “Question” and “A” represents “Answer”.

Traditional MetricsIn tasks with strong objectivity or where scoring models struggle to extract the correct answers, we provide traditional metric as a reference. For punctuation task, we use the F1-score [49]. For the translation tasks of classical Chinese to modern Chinese, modern Chinese to classical Chinese, classical poetry to modern Chinese, and classical poetry to English, we utilize BLEU [50].

### Results and Analysis

This experiment evaluates the performance of 31 models on 26 coarse-grained tasks, as shown in Table 3 and 4. Detailed metrics for LLMs on 42 fine-grained tasks are provided in Appendix C. The tasks represented by T1-T26 are as follows: T1-Sentence structure, T2-Classical Chinese to modern Chinese, T3-Modern Chinese to classical Chinese, T4-Named entity recognition, T5-Punctuation, T6-Topic classification, T7-Word explanation, T8-Reading comprehension, T9-Function words, T10-Homophones, T11-Polysemy, T12-Ancient prose writing, T13-Ampreciation, T14-Ancient poetry writing, T15-Basic Q&A, T16-Ancient poetry translation, T17-Sentiment classification, T18-Ancient poetry to English, T19-Poot introduction, T20-Analysis of imagery, T21-Couplet, T22-Idiom, T23-Riddle, T24-Xiehouyu, T25-Historical Chinese phonology, T26-Knowledge of sinology Q&A. Based on the results, we draw the following insights.

**There is considerable room for improvement of existing LLMs' CCLLA capabilities.** ERNIE-4.0 [10] performs the best with a score of 64.3. LLaMA2-7B-Chat [40] has the lowest score, only 13.0. The average score of the 31 LLMs is approximately 41.2. Most models have scores between 20 and 60, with over 64% of the LLMs scoring below 50. Overall, the scores are relatively low, suggesting there is considerable room for improvement.

**Pre-training data matters.** General-purpose Chinese models demonstrate superior performance in the CCLLA domain. Additionally, fine-tuning general-purpose English models with Chinese data improves their performance, yet such enhancements do not fully match the level of proficiency of the general-purpose Chinese model, highlighting the significance of Chinese pre-training data in the CCLLA domain.

**Incremental pre-training in the CCLLA domain may not be effective.** Unexpectedly, the models specifically designed for ancient Chinese with incremental pre-training and fine-tuning, namely Ancient-Chat-LLM-7B [51], Bloom-7B-Chunhua [48], and Xunzi-Qwen1.5-7B [47], show subpar performance in the CCLLA domain, achieving an average score of 34.1. This indicates that incremental pre-training in the CCLLA domain may not be effective. The possible reason could be the incremental pre-training and fine-tuning data being insufficient to cover a wide range of knowledge and tasks, which concurrently leads to more profound catastrophic forgetting of CCLLA-related knowledge from the pre-training corpus.

**Large Language Models lack sufficient knowledge in the CCLLA domain.** As shown in Table 4, LLMs exhibit substantially different performance across various domains and capabilities dimension. On different capability dimensions, LLMs perform significantly worse in knowledge than in generation and understanding, indicating that LLMs lack sufficient knowledge in the CCLLA domain. Moreover, the fields of Ancient Poetry and Ancient Literary Culture contain more tasks with a knowledge dimension, which leads to LLMs performing noticeably worse in these two domains compared to Ancient Prose.

**The principle of scaling law remains valid in the CCLLA domain.** We present the relationship between model performance and parameters of Qwen [41] and Yi [52] in Figure 5, which shows that the performance improves with the parameters increasing, illustrating the scaling law [53] for LLMs in CCLLA domain.

For the tasks of translation and punctuation, we further utilize BLEU [50] and F1-score [49] as a traditional metric, respectively, assessing the strengths and weaknesses of traditional metrics versus model scoring metric. Specifically, in punctuation and four translation tasks, we select five models and for each model, we randomly choose 300 samples for manual evaluation to determine which metric aligns more closely with human evaluation. The comparative results between traditional metrics and model scoring metric are illustrated in Figure 6. It can be observed that for tasks with singular and definitive answers such as punctuation, traditional metrics are more appropriate as the scoring model struggle to extract the correct answer from responses of LLMs. In contrast, for tasks with non-unique answers like translation, model scoring metric demonstrates a high degree 

[MISSING_PAGE_EMPTY:8]

31 representative LLMs. The results reveal insights into their performance levels, highlighting areas for improvement. WenMind provides a standardized and detailed assessment, enabling researchers to assess and compare the performance of LLMs in the CCLLA domain effectively. Our study highlights the importance of knowledge enrichment in LLMs for the CCLLA domain and provides valuable insights for future research and development.

## 6 Limitations

The main limitation of our work is the use of an aligned LLMs for scoring, which might introduce some degree of error. However, we take several measures to minimize this issue. We standardize the behavior of the scoring LLMs to ensure fairness and consistency, resulting in a high level of consistency with human evaluation (89.4%). This minimizes the impact of error and ensures a reliable comparison between LLMs and human scoring. Additionally, while our evaluation tasks are designed to align closely with real-world applications of LLMs in the field of CCLLA, some tasks may not fully capture the complexity of specific application needs, such as T5 punctuation and T6 theme classification. These tasks serve more as general evaluations rather than targeted assessments of nuanced requirements in practical scenarios.

## 7 Ethical Statement

Our evaluation dataset contains content of significant historical and academic value, designed to promote scholarly research and educational applications related to historical texts, language transfor

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Overall**} & \multicolumn{4}{c}{**Domain**} & \multicolumn{4}{c}{**Capability**} \\ \cline{3-10}  & & \multicolumn{2}{c}{Accident Pose} & \multicolumn{2}{c}{Anticolumn{2}{c}} & \multicolumn{2}{c}{Anticolumn{2}{c}} & \multicolumn{2}{c}{Fore} & \multicolumn{2}{c}{Anticolumn{2}{c}} & \multicolumn{2}{c}{Fore} & \multicolumn{2}{c}{Ancentration} & \multicolumn{2}{c}{Fore} & \multicolumn{2}{c}{Knowledge} \\ \hline Bioichuan2-7B-Chat [42] & 41.2 & 49.5 & 33.6 & 39.5 & 47.8 & 58.2 & 27.7 \\ Bioichuan2-13B-Chat [42] & 45.5 & 53.4 & 39.8 & 41.6 & 53.7 & 58.4 & 31.2 \\ Fuerb-Bahrami-38b [54] & 38.7 & 44.7 & 33.1 & 37.8 & 45.2 & 50.2 & 26.9 \\ CurdM2-Chat [68] & 35.4 & 43.9 & 29.9 & 30.0 & 43.8 & 52.3 & 19.6 \\ CurdM2-Chat [68] & 39.5 & 50.9 & 32.4 & 32.0 & 50.9 & 55.7 & 20.0 \\ Intermedi-DAn-Chat [78] & 50.2 & 53.4 & 47.5 & 49.3 & 54.7 & 63.3 & 40.8 \\ Quent 5-D5-Chat [41] & 26.1 & 36.7 & 17.0 & 23.4 & 37.2 & 43.4 & 6.7 \\ Quent 5-D4-Chat [41] & 39.6 & 48.5 & 32.5 & 36.1 & 48.0 & 32.5 & 24.9 \\ Quent 5-D5-Dhat [41] & 50.3 & 55.5 & 48.2 & 44.7 & 57.9 & 65.0 & 36.2 \\ Quent 5-14B-Chat [41] & 54.9 & 60.5 & 52.8 & 49.1 & 62.5 & 65.3 & 42.0 \\ Quent 5-132B-Chat [41] & 57.0 & 63.3 & 52.6 & 53.4 & 64.6 & 65.7 & 44.4 \\ Quent 5-72B-Chat [41] & 58.5 & 64.0 & 55.6 & 54.0 & 65.9 & 67.4 & 46.3 \\ Vi-1-56-Dhat [52] & 47.2 & 53.4 & 42.9 & 43.7 & 54.7 & 61.9 & 33.3 \\ Vi-1-59-Dhat [52] & 51.7 & 58.4 & 46.6 & 48.6 & 59.1 & 65.0 & 38.1 \\ Vi-1-53-Dhat [52] & 57.4 & 63.0 & 52.0 & 56.6 & 63.2 & 69.6 & 46.4 \\ ENRISE-38-Chat [29] & 62.2 & 63.5 & 55.7 & 70.7 & 64.4 & 72.8 & 55.9 \\ ENRISE-40-83-Chat [29] & **64.3** & **66.3** & **56.6** & 27.4 & **66.8** & **76.1** & 57.8 \\ Sport-3-15-Dhat [60] & 60.9 & 59.8 & 54.1 & **78.7** & 60.2 & 66.9 & **60.2** \\ German-1-7B-T [57] & 25.2 & 32.4 & 21.8 & 18.6 & 34.9 & 47.7 & 6.2 \\ ZyyyLALA-13-B-11 [58] & 34.1 & 42.5 & 28.2 & 9.5 & 43.5 & 50.2 & 17.2 \\ LLLALALA-7B-Chat [40] & 13.0 & 14.0 & 14.3 & 9.2 & 16.8 & 26.9 & 4.2 \\ LLLALALA-13B-Chat [40] & 23.7 & 29.7 & 21.6 & 17.1 & 32.2 & 40.5 & 7.9 \\ LLLALALA-Chinese-T-Bhat [45] & 18.1 & 29.6 & 11.2 & 10.0 & 27.5 & 25.1 & 3.6 \\ LLLALALA-Chinese-13B-Chat [46] & 23.7 & 36.4 & 15.3 & 16.0 & 35.7 & 35.3 & 4.5 \\ LLLALALA-38-INTINT [34] & 34.7 & 45.0 & 27.5 & 29.1 & 46.1 & 57.4 & 13.4 \\ ILLALALA38-Chinese-8B-Chat [60] & 37.3 & 49.9 & 30.1 & 27.7 & 50.2 & 55.7 & 15.2 \\ GPT-5 [61] & 35.3 & 46.1 & 30.5 & 25.1 & 47.1 & 50.7 & 15.6 \\ GPT-4 [62] & 50.2 & 60.3 & 44.2 & 43.1 & 61.3 & 61.7 & 32.4 \\ Ancient Chinese-LLLM-7B [51] & 32.7 & 42.6 & 23.9 & 30.5 & 41.1 & 39.1 & 19.9 \\ Bloom-7B-Chumbas [3] & 32.5 & 42.7 & 24.0 & 29.3 & 42.2 & 41.4 & 17.3 \\ Xunai-Queen1.5-7B [47] & 37.0 & 44.8 & 29.4 & 36.2 & 44.9 & 46.8 & 23.8 \\ Average & & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of all evaluated models on different domains and capabilities. Details of the evaluated models are presented in Appendix C.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c}{**T2**} & \multicolumn{2}{c}{**T3**} & \multicolumn{2}{c}{**T16**} & \multicolumn{2}{c}{**T18**} & \multicolumn{2}{c}{**T5**} \\ \cline{2-10}  & MSM & TM & MSM & TM & MSM & TM & MSM & TM & MSM & TM \\ \hline Baiichuan2-13B-Chat [42] & 53.8 & 13.3 & 62.5 & 7.8 & 66.9 & 8.8 & 51.3 & 38.2 & 72.5 & 59.8 \\ ERNIE-4.0-8K-0329 [10] & 62.8 & 19.8 & 48.0 & 28.0 & 65.0 & 5.6 & 55.9 & 34.1 & 85.2 & 61.2 \\ LLLAM3-Chinese-8B-Chat [60] & 46.3 & 12.2 & 61.2 & 8.4 & 62.4 & 7.3 & 52.3 & 35.2 & 68.5 & 57.7 \\ GPT-3-5 [61] & 46.8 & 12.3 & 54.4 & 7.2 & 59.4 & 8.5 & 49.7 & 38.2 & 72.0 & 58.0 \\ Xunai-Queen1.5-7B [47] & 63.4 & 25.4 & 51.0 & 29.1 & 60.6 & 10.7 & 43.9 & 27.0 & 84.2 & 75.6 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Model scoring metric and traditional metrics on translation and punctuation tasks. “TM” represents “Traditional Metrics”; “MSM” represents “Model Scoring Metric”. Details of the evaluated models are presented in Appendix C. “TM” is either BLEU or F1.

mation, and ancient scenarios. It is not intended to reinforce or propagate societal biases. Throughout the data collection and processing phases, we have taken measures to minimize the presence of unsafe content. For tasks involving content generation, we emphasize that the generated materials should be limited to academic research and educational use, and must not be employed for commercial purposes, political propaganda, or any objectives that may lead to bias or discrimination. In light of potential misuse risks, we explicitly restrict the use of the dataset and generated texts to prevent inappropriate applications. We acknowledge the social, cultural, and historical risks that may be inherent in the dataset, and we are committed to actively exploring and implementing strategies to mitigate such risks. With this statement, we aim to provide academic value while addressing potential ethical concerns, underscoring that the dataset must not be misused.

## Acknowledgement

This research is supported in part by National Natural Science Foundation of China (Grant No.: 62441604, 62476093).

## References

* [1] Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. The FinBen: An holistic financial benchmark for large language models. _arXiv preprint arXiv:2402.12659_, 2024.
* [2] Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, et al. FinEval: A Chinese financial domain knowledge evaluation benchmark for large language models. _arXiv preprint arXiv:2308.09975_, 2023.
* [3] Dakuan Lu, Hengkui Wu, Jiaqing Liang, Yipei Xu, Qianyu He, Yipeng Geng, Mengkun Han, Yingsi Xin, and Yanghua Xiao. BBT-Fin: Comprehensive construction of Chinese financial domain pre-trained language model, corpus and benchmark. _arXiv preprint arXiv:2302.09432_, 2023.
* [4] Yongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han, Wei Tian, and Hao Wang. LAiW: A Chinese legal large language models benchmark. _arXiv preprint arXiv:2310.05620_, 2023.
* [5] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. LawBench: Benchmarking legal knowledge of large language models. _arXiv preprint arXiv:2309.16289_, 2023.
* [6] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Re, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. _Advances in Neural Information Processing Systems_, 36, 2023.
* [7] Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koras, Constantin Marc Seibold, Kaleb E Smith, and Jens Kleesiek. CLUE: A clinical language understanding evaluation for LLMs. _arXiv preprint arXiv:2404.04067_, 2024.
* [8] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, et al. CMB: A comprehensive medical benchmark in Chinese. _arXiv preprint arXiv:2308.08833_, 2023.
* [9] Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and Benyou Wang. Huatuo-26M, a large-scale Chinese medical QA dataset. _arXiv preprint arXiv:2305.01526_, 2023.
* [10] Baidu. ERNIE. https://yiyan.baidu.com/.
* [11] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A multi-level multi-discipline Chinese evaluation suite for foundation models. _Advances in Neural Information Processing Systems_, 2023.

* [12] Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, et al. CIF-Bench: A Chinese instruction-following benchmark for evaluating the generalizability of large language models. _arXiv preprint arXiv:2402.13109_, 2024.
* [13] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. _arXiv preprint arXiv:2306.09212_, 2023.
* [14] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. _arXiv preprint arXiv:2305.12474_, 2023.
* [15] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, et al. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 18099-18107, 2024.
* [16] Yixuan Zhang and Haonan Li. Can large langauge model comprehend ancient Chinese? A preliminary test on ACLUE. In _Proceedings of the Ancient Language Processing Workshop_, pages 80-87, Varna, Bulgaria, September 2023. INCOMA Ltd., Shoumen, Bulgaria.
* [17] Zijing Ji, Yuxin Shen, Yining Sun, Tian Yu, and Xin Wang. C-CLUE: A benchmark of classical Chinese based on a crowdsourcing system for knowledge graph construction. In _Knowledge Graph and Semantic Computing: Knowledge Graph Empowers New Infrastructure Construction: 6th China Conference, CCKS 2021, Guangzhou, China, November 4-7, 2021, Proceedings 6_, pages 295-301. Springer, 2021.
* [18] Ethan. CCLUE: Classical Chinese language understanding evaluation benchmark: datasets, baselines, pre-trained models, corpus and leaderboard. https://github.com/Ethan-yt/CCLUE.
* [19] Wenhao Li, Fanchao Qi, Maosong Sun, Xiaoyuan Yi, and Jiarui Zhang. CCPM: A Chinese classical poetry matching dataset. _arXiv preprint arXiv:2106.01979_, 2021.
* [20] Zhipeng Guo, Xiaoyuan Yi, Maosong Sun, Wenhao Li, Cheng Yang, Jiannan Liang, Huimin Chen, Yuhui Zhang, and Ruoyu Li. Jiuge: A human-machine collaborative Chinese classical poetry generation system. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 25-30, Florence, Italy, 2019.
* [21] Xiaoyuan Yi, Maosong Sun, Ruoyu Li, and Wenhao Li. Automatic poetry generation with mutual reinforcement learning. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3143-3153, Brussels, Belgium, 2018.
* [22] Huimin Chen, Xiaoyuan Yi, Maosong Sun, Cheng Yang, Wenhao Li, and Zhipeng Guo. Sentiment-controllable Chinese poetry generation. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, Macao, China, 2019.
* [23] Bo Zhou, Qianglong Chen, Tianyu Wang, Xiaomi Zhong, and Yin Zhang. WYWEB: A NLP evaluation benchmark for classical Chinese. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 3294-3319, Toronto, Canada, July 2023. Association for Computational Linguistics.
* [24] Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3Exam: A multilingual, multimodal, multilevel benchmark for examining large language models. _Advances in Neural Information Processing Systems_, 36, 2023.
* [25] Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, and Irwin King. CLongEval: A Chinese benchmark for evaluating long-context large language models, 2024.
* [26] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. SafetyBench: Evaluating the safety of large language models with multiple choice questions, 2023.

* [27] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. CLUE: A Chinese language understanding evaluation benchmark. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 4762-4772, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.
* [28] Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, and Zhenzhong Lan. SuperCLUE: A comprehensive Chinese large language model benchmark. _arXiv preprint arXiv:2307.15020_, 2023.
* [29] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqi Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu. Evaluating hallucinations in Chinese large language models. _CoRR_, abs/2310.03368, 2023.
* [30] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. AlignBench: Benchmarking Chinese alignment of large language models, 2023.
* [31] Yufei Huang and Deyi Xiong. CBBQ: A Chinese bias benchmark dataset curated with human-AI collaboration for large language models. _arXiv preprint arXiv:2306.16244_, 2023.
* [32] Zongyuan Jiang, Jiapeng Wang, Jiahuan Cao, Xue Gao, and Lianwen Jin. Towards better translations from classical to modern Chinese: A new dataset and a new method. In _CCF International Conference on Natural Language Processing and Chinese Computing_, pages 387-399. Springer, 2023.
* [33] Daizhigev20. https://github.com/garychowcmu/daizhigev20.
* [34] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The Refined-Web dataset for Falcon LLM: Outperforming curated corpora with web data only. _Advances in Neural Information Processing Systems_, 36, 2024.
* [36] Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Runyu Peng, et al. WanJuan-CC: A safe and high-quality open-sourced English webtext dataset. _arXiv preprint arXiv:2402.19282_, 2024.
* [37] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. Wudaocorpora: A super large-scale Chinese corpora for pre-training language models. _AI Open_, 2:65-68, 2021.
* [38] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. LLaMA 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [41] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [42] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. _arXiv preprint arXiv:2309.10305_, 2023.

* [43] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_, 2022.
* [44] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-trained model. In _The Eleventh International Conference on Learning Representations_, 2022.
* [45] FlagAlpha. Llama2-Chinese-7b-Chat. https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat, 2024. Hugging Face.
* [46] FlagAlpha. Llama2-Chinese-13b-Chat. https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat, 2024. Hugging Face.
* [47] Xunzi-LLM of Chinese-classics. XunziALLM. https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM, 2024. GitHub.
* [48] Wptoux. Bloom-7B-Chunhua. https://huggingface.co/wptoux/bloom-7b-chunhua, 2024. Hugging Face.
* [49] Zhe Zhang, Jie Liu, Lihua Chi, and Xinhai Chen. Word-level BERT-CNN-RNN model for Chinese punctuation restoration. In _2020 IEEE 6th International Conference on Computer and Communications (ICCC)_, pages 1629-1633. IEEE, 2020.
* [50] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* [51] PeterH0323. Ancient-Chat-LLM. https://github.com/PeterH0323/ancient-chat-llm, 2024. GitHub.
* [52] 01. AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, et al. Yi: Open foundation models by 01.AI, 2024.
* [53] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
* [54] YeungNLP. Firefly-Baichuan2-13B. https://huggingface.co/YeungNLP/firefly-baichuan2-13b, 2024. Hugging Face.
* [55] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. InternLM2 technical report, 2024.
* [56] Iflytek. Spark-v3.5. https://xinghuo.xfyun.cn/.
* [57] Google. Gemma-1.1-7B-IT. https://huggingface.co/google/gamma-1.1-7b-it, 2024. Hugging Face.
* [58] IDEA-CCNL. Ziya-LLaMA-13B-v1.1. https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1.
* [59] Meta. LLaMA3. https://ai.meta.com/blog/meta-llama-3.
* [60] Shenzhi Wang. Llama3-8B-Chinese-Chat. https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat, 2024. Hugging Face.
* [61] OpenAI. GPT-3.5. https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates.
* [62] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. GPT-4 technical report, 2024.
* [63] Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language models: A comprehensive survey. _arXiv preprint arXiv:2402.18041_, 2024.

* [64] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language models. _ACM Trans. Intell. Syst. Technol._, 15(3), mar 2024.
* [65] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176B-parameter open-access multilingual language model, 2023.
* [66] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. _Advances in Neural Information Processing Systems_, 36, 2023.
* [67] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, et al. Evaluating hallucinations in Chinese large language models. _arXiv preprint arXiv:2310.03368_, 2023.
* [68] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using GPT-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 2511-2522, Singapore, December 2023. Association for Computational Linguistics.
* [69] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. _Advances in Neural Information Processing Systems_, 36, 2023.
* [70] Silvia Stopponi, Saskia Peels-Matthey, and Malvina Nissim. AGREE: A new benchmark for the evaluation of distributional semantic models of ancient Greek. _Digital Scholarship in the Humanities_, 39(1):373-392, 2024.
* [71] Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, and Taylor Berg-Kirkpatrick. LogogramNLP: Comparing visual and textual representations of ancient logographic writing systems for NLP. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 14238-14254, 2024.
* [72] Amrith Krishna, Pavankumar Satuluri, and Pawan Goyal. A dataset for Sanskrit word segmentation. In _Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature_, pages 105-114, 2017.
* [73] Ann Taylor. The York-Toronto-Helsinki parsed corpus of old English prose. In _Creating and digitizing language corpora: Volume 2: Diachronic Databases_, pages 196-227. Springer, 2003.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] The abstract and section 1 accurately reflect the contribution of this paper in the field of Chinese Classical Literature and Language Arts, providing the evaluation benchmark WenMind. 2. Did you describe the limitations of your work? [Yes] Section 6. 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] Appendix F.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Appendix D. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Section 4.2. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Section 4.2. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Section 4.2.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] The main text contains relevant citations. For detailed information, please refer to the Appendix B.3. 2. Did you mention the license of the assets? [Yes] Table 1 and Appendix B.3. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] Appendix D. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Section 3.2 and Appendix F. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Datasheet for WenMind

### Motivation

**1. For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.**

A1: The purpose of WenMind is to quantify the capabilities of existing large language models (LLMs) on Chinese classical literature and language arts (CCLLA), providing a reference for the development of this field. Existing benchmarks focus primarily on the Ancient Prose sub-domain or contain only a limited number of tasks, making it challenging to provide a thorough and holistic assessment of LLMs' capabilities in CCLLA. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research.

**2. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?**

A2: The WenMind benchmark is created by the Deep Learning and Vision Computing Lab (DLVC-Lab) of South China University of Technology, INTSIG Information Co., Ltd, and INTSIG-SCUT Joint Lab on Document Analysis and Recognition.

### Composition

**1. What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.**

A1: The Wenmind benchmark consists of 4,875 plain text questions. Each question is represented in a dictionary format, containing the following keys: ID, domain, capability, question format, coarse-grained task, fine-grained task, question, and answer. The dataset is entirely stored in a JSON file.

**2. How many instances are there in total (of each type, if appropriate)?**

A2: The WenMind benchmark comprises a total of 4,875 instances, which are categorized into three distinct sections: 1,900 instances in the Ancient Prose section, 1,845 instances in the Ancient Poetry section, and 1,130 instances in the Ancient Literature Culture section.

**3. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this represent ativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances because instances were withheld or unavailable).**

A3: The WenMind benchmark encompasses all possible instances.

**4. What data does each instance consist of?**

A4: The WenMind benchmark consists of textual data. Each text instance includes the following information: ID, domain, capability, question format, coarse-grained task, fine-grained task, question, and answer.

**5. Is there a label or target associated with each instance?**

A5: The WenMind benchmark includes questions and their corresponding answers, along with additional information such as the question format and task name.

**6. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.**A6: No, each of our data instances contains comprehensive information.

**7. Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.**

A7: No, our data instances are independent of each other and have no interrelations.

**8. Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.**

A8: No, our data is utilized solely for the purpose of testing and not for training.

**9. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.**

A9: In WenMind, we have incorporated extensive manual calibration to minimize errors and noise as much as possible, yet we cannot guarantee that every answer will be flawless.

**10. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?**

A10: Yes, the WenMind benchmark is self-contained.

**11. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctorpatient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description.**

A11: No, we construct the WenMind benchmark from publicly accessible sources.

**12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.**

A12: No, the WenMind benchmark does not contain any content that is offensive, insulting, threatening, or might otherwise cause anxiety.

**13. Does the dataset relate to people?**

A13: No, the WenMind benchmark has nothing to do with people.

### Collection Process

**1. How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.**

A1: The data construction and specific sources are thoroughly described in Section 3.2 and Appendix B.3. The text we gather from the Internet is sourced from several open copyright websites, such as https://www.sou-yun.cn/ and https://www.zdic.net/.

**2. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?**

A2: Our data collection is carried out through three mechanisms: manual human curation, a web crawling program, and the API of LLMs.

**3. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)**

A3: The WenMind benchmark is created by the researchers from Deep Learning and Vision Computing Lab (DLVC-Lab) of South China University of Technology, INTSIG Information Co., Ltd, and INTSIG-SCUT Joint Lab on Document Analysis and Recognition.

**4. Over what timeframe was the data collected?**

A4: Our data collection spans a period of two months.

**5. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?**

A5: We mainly collect the data from the Internet, open-source datasets, and LLM generation.

### Preprocessing/cleaning/labeling

**1. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.**

A1: Yes, the detail is discussed in Section 3.2.

**2. Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.**

A2: No, the data we provide has undergone processing and transformation, combining original texts with annotations for storage.

### Uses

**1. Has the dataset been used for any tasks already? If so, please provide a description.**

A1: No, the WenMind benchmark has not appeared in previous works.

**2. What (other) tasks could the dataset be used for?**

A2: No, the WenMind benchmark is solely used for the evaluation of LLMs.

**3. Are there tasks for which the dataset should not be used? If so, please provide a description.**

A3: To prevent data leakage and ensure the fairness of the evaluation [63, 64], the WenMind benchmark cannot be used for training LLMs.

### Distribution

**1. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?**

A1: Our dataset is publicly available at https://github.com/SCUT-DLVCLab/WenMind.

**2. When will the dataset be distributed?**

A2: Our dataset has been already distributed.

**3. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?**

A3: For details, please refer to Appendix F.

### Maintenance

**1. Who will be supporting/hosting/maintaining the dataset?**

A1: The Deep Learning and Vision Computing Lab (DLVC-Lab) of South China University of Technology.

**2. How can the owner/curator/manager of the dataset be contacted (e.g., email address)?**

A2: The manager can be contacted through the email address or Github.

**3. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?**

A3: We will publish a correction list for the WenMind benchmark on GitHub every quarter.

**4. Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.**

A4: We will maintain all versions of the WenMind benchmark on GitHub.

**5. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.**

A5: If other researchers or individuals are interested in extending, augmenting, building on, or contributing to the dataset, they should contact us via email, clearly articulating their intentions and requesting our consent prior to any further actions.

[MISSING_PAGE_FAIL:20]

### Task Example

Figure 7, 8, 9, 10, 11 and 12 respectively present examples of question-answer pairs for the 42 fine-grained tasks.

Figure 7: Task 1-1 to Task 4 examples.

Figure 8: Task 5 to Task 11 examples.

Figure 9: Task 12 to Task 15-1 examples.

Figure 10: Task 15-2 to Task 17 examples.

Figure 11: Task 18 to Task 22-1 examples.

Figure 12: Task 22-2 to Task 26 examples.

[MISSING_PAGE_FAIL:27]

### The Specific Source of the Data

Table 7 presents the data construction methods, data sources, links to the sources, and the licenses of the original data for 42 fine-grained tasks. Data without a specified source fall into three categories: (a) Questions and answers pairs manually constructed using knowledge content from the Internet; (b) Publicly available examination questions; (c) Questions and answers pairs generated by models.

In the data construction method column of Table 7, the following methods are described:

M1: Scraping and creating Q&A pairs from Internet sources; M2: Directly collecting relevant questions from various Internet platforms; M3: Manually constructing Q&A pairs based on existing CCLLA corpora; M4: Utilizing evaluation dataset questions from the CCLLA domain; M5: Using LLM-generated answers for assessment data.

For tasks without specified links, the sources fall into two categories: (a) Data generated by LLMs, providing responses based on training and inference capabilities. (b) Original data derived from widely accessible online knowledge texts, such as high school Chinese language resources, which are not restricted to specific webpages.

### The Construction of Dataset

#### b.4.1 Specific Process

The process of constructing the dataset is divided into two stages: the data collection process and the data review process.

**(1) Data Collection Process**

**Data collected from the Internet.** Collectors: Volunteer A, Volunteer B, and Graduate Student C. Collection process: Based on common tasks in Chinese language exams (reading comprehension, comprehension dictation, _etc._), collect high-quality exam questions made by experts and scholars from the Internet. We require that the questions should be closely matched with the tasks.

**Other open-source datasets.** Collector: Graduate Student D. Collection process: The data collector conducts research and collection of existing open-source datasets, selects classic tasks and high-quality data in the CCLLA field, and assesses the scarcity and construction difficulty of task data. Ultimately, valuable and currently scarce data are selected for reuse to supplement and improve the evaluation benchmark. Post-processing: (a) Text filtering: Perform operations such as handling of abnormal symbols (_e.g._, blank squares), deletion of irrelevant content, and after manual inspection, obtain complete and high-quality data. (b) Question writing: Construct Q&A pairs oriented by the needs of different tasks. (c) Standardization of format: Questions for the same task are unified into the same questioning method. We directly set up corresponding question templates according to different tasks.

**LLMs.** Collector: Graduate Student E. Collection process: Tasks such as ancient poetry and prose writing are inherently open-ended and do not have fixed correct answers. For these tasks, we initially design a range of questions and then employ the ERNIE-3.5 [10] model to generate reference answers. These answers are further reviewed, filtered, and refined through a manual process.

**(2) Data Review Process**

a. Reviewers: The review is conducted by three individuals, namely Volunteer A, Graduate Student D, and Graduate Student E.

b. A total of approximately 7,000 data entries are collected. The reviewers manually verify the Q&A pairs from three dimensions: the standardization and accuracy of the questions, the correctness and comprehensiveness of the answers, and whether the text content contains ethical issues or unsafe content.

c. Data with non-standard Q&A are deleted or revised, and entries with safety issues are excluded.

d. Finally, the remaining data are rebalanced in terms of task and quality, followed by a second round of proofreading. In this second round, in addition to reviewing the safety and ethical aspects of the data, it is essential to ensure the accuracy and consistency of the Q&A content, with particular attention to grammar, punctuation, and adherence to proper expression standards. The data for the26 tasks are also rebalanced to prevent excessive or insufficient data for certain tasks. For tasks with less data, additional backup data are incorporated to increase their volume, while for tasks with excessive data, low-quality entries are streamlined to maintain consistent overall quality. Each task's data undergoes at least two rounds of proofreading to ensure relevance, a balanced distribution of task difficulty, and ultimately, the comprehensiveness, accuracy, and safety of the dataset.

#### b.4.2 Review Details

During the dataset construction process, two rounds of rigorous manual proofreading are conducted to ensure the accuracy and safety of all data. First, the initially collected 7,000 entries undergo the first round of review, where data containing non-standard Q&A pairs, inaccurate or incomplete answers, and entries with potential ethical issues or unsafe content are identified and filtered out. At this stage, 25.20% of the data are manually removed, leaving 5,236 entries. In the second round, the review standards are further refined, focusing not only on identifying additional safety and ethical concerns but also on ensuring the accuracy and consistency of the Q&A pairs, with special attention to the standardization of grammar, punctuation, and expression. An additional 6.89% of the data are removed during this phase, resulting in 4,875 entries that are comprehensive and secure. During this process, supplementary adjustments are made to address any data imbalances across individual tasks.

It is important to note that, despite our strict data review and filtering process, we cannot guarantee the complete absence of errors or safety issues in the dataset. Users should exercise caution and refrain from misusing the data. We emphasize the cultural value of this dataset and hope it provides valuable support and reference for research in the CCLLA field.

#### b.4.3 Task Collection Requirements and Question Template Explanation

The following presents a simplified version of the task collection requirements and question template explanation adhered to by the collectors during the data collection process for 26 different tasks.

* **T1-Sentence Structure Collection Requirements:** Questions are constructed based on specific subcategories (inverted sentence structure, elliptical sentence, inverted sentence types, sentence structure identification). The questions should focus on identifying or analyzing particular features of the sentence structure, such as rearranging the order of inverted sentences, identifying omitted elements, or determining the sentence type. The questions must be clear and explicitly indicate the sentence features or components being examined. **Template Explanation:** The questions are in MCQ format. A classical Chinese sentence is provided, followed by a question that requires selecting the correct answer based on the features of the sentence.
* **T2-Classical Chinese to Modern Chinese Collection Requirements:** Select a sentence of classical Chinese and require its translation into modern Chinese. The question must clearly specify the classical Chinese text to be translated. The selected text should have a certain level of difficulty to assess translation skills and comprehension of the classical language. **Template Explanation:** The questions are in QA format. First, the translation task is presented, followed by the classical Chinese sentence that needs to be translated.
* **T3-Modern Chinese to Classical Chinese Collection Requirements:** Select sentences in modern Chinese and require their translation into classical Chinese. The question must clearly specify the modern Chinese text to be translated to assess the ability to convert it into classical Chinese. **Template Explanation:** The questions are in QA format. First, the translation task requirements are presented, followed by the modern Chinese text that needs to be converted.
* **T4-Named Entity Recognition Collection Requirements:** A sentence of classical Chinese is provided, and the task is to identify named entities within it. The entities include personal names, place names, book titles, or other proper nouns.

**Template Explanation:** The questions are in QA format. First, the entity recognition task is specified, followed by the classical Chinese text to be analyzed. The answer section lists all identified named entities.
* **T5-Punctuation Collection Requirements:** A sentence of classical Chinese that lacks punctuation is selected, and the task requires the subject to add appropriate punctuation marks. The question must clearly specify the text to be punctuated in order to assess understanding of the structure and content of the classical language. Template Explanation:** The questions are in QA format. The prompt presents a punctuation-free classical Chinese sentence and requires the addition of the corresponding punctuation marks. The answer section provides the complete text with punctuation added.
* **T6-Topic Classification Collection Requirements:** A sentence of classical Chinese is provided, and the task requires classifying its content into the provided topic options. The question must include a clear list of topic classification options to assess the subject's understanding of the text's theme. Template Explanation:** The questions are in QA format. The prompt includes a classical Chinese sentence without a theme label, and the subject is required to select the most relevant topic type from the given multiple-choice options. The answer section should contain the correct topic classification.
* **T7-Word Explanation Collection Requirements:** A word or phrase that requires explanation is selected from a classical Chinese sentence, and the task requires the subject to provide its modern Chinese explanation. The question must include the complete classical Chinese sentence to provide context for the word meaning explanation. Template Explanation:** The questions are in QA format. First, a request for the explanation of the word or phrase is presented, followed by the classical Chinese sentence containing the word or phrase to be explained. Finally, the answer should provide the meaning of the selected word or phrase.
* **T8-Reading Comprehension Collection Requirements:** A passage of classical Chinese is selected, and relevant questions are designed to require the subject to respond based on the content of the text. The questions should focus on comprehension skills, such as extracting key information, analyzing character actions, or interpreting event outcomes. Template Explanation:** The questions are in QA format. The prompt includes a passage of classical Chinese text along with a question related to its content, and the answer should provide a clear explanation or conclusion.
* **T9-Function Words Collection Requirements:** A sentence of classical Chinese containing key function words is selected, and the task requires the subject to determine the specific usage of the function word. The question must provide multiple options covering different usages of the function word to test understanding of its specific meaning in the context of classical Chinese. Template Explanation:** The questions are in MCQ format. A classical Chinese sentence containing the target function word is presented, and the subject is required to choose the most appropriate usage from several explanation options. The answer section should include the correct option.
* **T10-Homophones Collection Requirements:** A sentence of classical Chinese containing homophones is selected, and the task requires the subject to clearly identify which character in the provided options is a homophone. The question must present multiple options, each containing a character that needs to be identified as a homophone. Template Explanation:** The questions are in MCQ format. Several classical Chinese sentences containing homophones are provided, and the subject is required to choose the homophone from the options. The character to be identified is highlighted in the options, and the answer section includes the correct option.
* **T11-Polysemy

**Collection Requirements:** A Chinese character with multiple meanings is selected, and questions are constructed based on the various interpretations of this character in different sentences. The questions may require selecting the explanation that corresponds to a given meaning from a specific sentence or identifying the usage that expresses a particular meaning among multiple sentences. This type of question assesses students' ability to understand the context of polysemous words.

**Template Explanation:** The questions are in MCQ format. The first format presents a specific sentence containing the polysemous character and requires the subject to choose the option that best matches a specific explanation of the character in that sentence. The options list the different meanings of the polysemous character. The second format provides multiple sentences with the polysemous character highlighted and requires the subject to identify the sentence that corresponds to the meaning described in the question. Each option demonstrates the usage of the polysemous character in different contexts.
* **T12-Ancient Prose Writing Collection Requirements:** Tasks should be designed to involve writing in classical Chinese across various scenarios or themes, requiring the subject to compose in classical Chinese. These scenarios can include storytelling, writing social media copy, or marketing content. The aim is to assess the subject's ability to create prose in classical Chinese and their skill in using ancient language to express modern ideas flexibly.

**Template Explanation:** The questions are in QA format. The prompt is typically presented in the form of a first-person request or command, asking for the creation of a specific type of content, such as a story or copywriting, with the requirement to use classical Chinese. The question structure includes a detailed scenario setup and specific writing instructions, and the answer usually provides a complete prose text in classical Chinese.
* **T13-Appreciation Collection Requirements:** Questions should focus on the interpretation and analysis of ancient Chinese poetry. The questions may ask participants to identify incorrect interpretations or analyze imagery, metaphors, and the emotions conveyed in the poem. Through these tasks, subjects need to demonstrate a deep understanding of the language, structure, and cultural background of the poetry.

**Template Explanation:** The questions are in QA and MCQ formats. In the first format (MCQ), the question provides a classical poem and annotations, with the options presenting different interpretations or analyses. The subject must select the best answer. In the second format (QA), the question provides a poem along with an open-ended question, asking the subject to analyze the metaphors, imagery, and emotions expressed in the poem. The answer should include an analysis from one or more perspectives, explaining the deeper meanings and emotions of the poem.
* **T14-Ancient Poetry Writing Collection Requirements:** Tasks should involve the creation of ancient Chinese poetry or lyrical works. The question must provide a clear theme and specify the required form, such as the genre of the poem, the name of the tune, or the title of the melody. Participants are required to compose within the traditional metrical structure, assessing their language expression skills and mastery of ancient poetry forms.

**Template Explanation:** The questions are in QA format. The prompt usually takes the form of a direct request, requiring the creation of a poem, lyric, or song based on a specified theme and format. The question description includes the theme and specific creative form, while the answer should provide a complete example that meets the task requirements.
* **T15-Basic Q&A Collection Requirements:** Tasks should cover fundamental questions on various aspects of ancient Chinese poetry. These may include questions on full-text content, identifying the poet and title, completing the next or previous line, understanding-based completion, and determining the genre. The questions must clearly specify the requirements, demonstrating the participant's knowledge of specific aspects of classical poetry.

**Template Explanation:** (a) Content Q&A: The questions are in QA format. The question directly asks for the full content of a given poem and requires participants to write it out completely. (b) Title and Author Q&A: The questions are in QA format. The task presents a part of a classical poem and asks the participant to identify its title and author. (c) Write the Next Sentence: The questions are in QA format. The question provides the first line and asks the participant to write the next line of the poem. (d) Write the Previous Sentence: The questions are in QA format. The question givesthe second line and asks the participant to fill in the previous one. (e) Comprehension Dictation: The questions are in FB format. The task provides a hint, such as the poem's theme or function, and asks for a key line from the poem to be filled in. (f) Genre Judgment: The questions are in QA format. The question presents a poem and requires the participant to determine its genre.
* **T16-Ancient Poetry Translation Collection Requirements:** The task should involve translating ancient Chinese poetry into modern vernacular Chinese. The question aims to assess the understanding of ancient poetic language and the ability to accurately convert it into more accessible modern language, ensuring the translation conveys the original imagery and emotions.
* **Template Explanation:** The questions are in QA format. The question asks subject to translate the given classical poem into vernacular Chinese. It includes the classical poem text, and the answer provides the modern vernacular translation of the poem.
* **T17-Sentiment Classification Collection Requirements:** The task should assess the ability to understand and classify the sentiment of ancient Chinese poetry. Each question provides a poem and asks the subject to choose the most appropriate sentiment classification from the given options. These options typically include negative, slightly negative, neutral, slightly positive, and positive.
* **Template Explanation:** The questions are in MCQ format. The question provides the title, author, and content of the poem, then requires the subject to select the sentiment classification that best matches the emotional tone. The options range from "negative" to "positive", and the answer identifies the best choice.
* **T18-Ancient poetry to English Collection Requirements:** The task is designed to test the ability to translate ancient Chinese poetry into English. Each question should provide a specific ancient poem and require the subject to translate it into modern English, ensuring that the translation preserves the original imagery and emotional expression of the poem.
* **Template Explanation:** The questions are in QA format. The task presents an ancient poem and asks for its translation into English. The question section includes the given poem, while the answer section provides a complete English translation.
* **T19-Poet introduction Collection Requirements:** The task is designed to guide the introduction of the biographies, achievements, and influences of ancient poets. Each question should present a clear request for detailed information about the specified poet, including their background, representative works, and literary contributions. The questions are in QA format. The question asks for the background and achievements of a particular poet. The question section consists of a simple request for an introduction, with the specific poet designated by the creator. The answer provides a detailed biography of the poet, including birth and death years, major experiences, literary achievements, and their historical and cultural significance.
* **T20-Analysis of imagery Collection Requirements:** The task is designed to analyze the multiple meanings of common imagery found in ancient poetry or prose. The questions should highlight specific imagery and require an explanation of its traditional and symbolic significance in different historical texts, thereby assessing the depth of understanding of the imagery. The questions are in QA format. The question asks for a list of the multiple meanings typically associated with the imagery. The question section includes the name of the imagery, while the answer section provides a detailed explanation of the imagery within the context of ancient texts, potentially listing various meanings along with historical background and classic citation examples.
* **T21-Couplet Collection Requirements:** The task is designed to test or inspire the understanding and skills in couplet following, couplet writing, or HengPi writing. The questions may involve providing the lower line for a given upper line, composing a couplet based on a festival or theme, or creating a suitable HengPi for an existing couplet. The task assesses mastery of parallel structure and creative interpretation of the couplet's theme.

**Template Explanation:** The questions are in QA format. The task may ask for couplet following by matching a lower line to an upper line, couplet writing based on a specific festival or theme, or HengPi writing for a given couplet. The question provides the necessary information and challenge, while the answer presents a couplet, matching lower line, or HengPi that meets the specified requirements.
* **T22-Idiom** Collection Requirements:** The task is designed to assess understanding, explanation, origin, and association of idioms. These questions may involve identifying synonyms of idioms, providing an idiom explanation, finding the origin of the idiom, or locating idioms embedded in a text and explaining their meaning. This task demonstrates the subject's familiarity with idioms and their ability to apply them. **Template Explanation:** The questions are in QA format. The question format involves identifying synonyms of a given idiom, providing an idiom explanation, stating the origin of idiom, or finding idioms in a text and explaining them. Each question provides an idiom or text, and the answer includes relevant idiom information such as synonyms, explanations, origin, or the idioms embedded in the text and their meanings.
* **T23-Riddle Collection Requirements:** The task is designed to present engaging and challenging riddles, covering a variety of subjects or themes. The goal is to test the subject's associative thinking, breadth of knowledge, and problem-solving skills. Each riddle should provide sufficient clues for logical reasoning to arrive at the correct answer. **Template Explanation:** The questions are in QA format. The subject is typically asked to guess an ancient historical figure, place name, country, or other objects. The question consists of the riddle and specific requirements, while the answer section provides the correct solution.
* **T24-Xiehouyu Collection Requirements:** The task is designed to assess the understanding and recall of Xiehouyu. The question usually provides the first part of the Xiehouyu, and the subject is required to complete the second part. This type of question evaluates familiarity with traditional Chinese cultural expressions of humor and wisdom, as well as the ability to interpret metaphors. **Template Explanation:** The questions are in QA format. The question is presented as an incomplete Xiehouyu, usually giving only the first part, and the subject is asked to complete the saying. The question clearly specifies the need for completion, and the answer section provides the second part of the Xiehouyu, optionally with an explanation.
* **T25-Historical Chinese phonology Collection Requirements:** The task involves designing multiple-choice questions on the topic of historical Chinese phonology. These questions require the subject to have a basic understanding of ancient Chinese phonetic structures, including initials and finals, as well as the Qieyun system. The subject must be able to accurately recognize and evaluate phonological phenomena and classical works in ancient Chinese phonology. **Template Explanation:** The questions are in MCQ format. Each question presents a specific knowledge point related to ancient Chinese phonology, along with several options, requiring the subject to select the correct answer. Each question includes a prompt and several answer choices, and the answer section identifies the correct option.
* **T26-Knowledge of Sinology Q&A Collection Requirements:** The task involves designing questions covering a wide range of common knowledge in Sinology, including history, geography, linguistics, philosophy, and other aspects of ancient Chinese culture. The questions should be concise and clearly formulated to assess the subject's understanding of traditional Chinese culture. **Template Explanation:** The questions are in QA and MCQ formats. Each question should clearly present a specific topic related to Sinology in a straightforward manner. The question section contains a clear prompt, while the answer section provides a concise and accurate response.

Evaluation

### Evaluated Model

Details of all evaluated models are shown in Table 8.

### Scoring Prompt

Scoring prompts of various tasks are illustrated in Figure 13.

### Scoring Consistency Analysis

When evaluating models using scoring methods, it is common to conduct an analysis of the consistency between model scores and human assessments [66; 67; 68; 69]. For the model scoring results on the WenMind benchmark, we have chosen to use the "Agreement Rate" metric to determine whether the model scores align with human expectations.

**Agreement Rate.** We present the question, reference answer, model output, model score, and the reasoning behind the model's score to humans. The humans then decide whether they agree based on the "score" and the "reasoning". If the model's score is reasonable and aligns with human understanding and expectations, they indicate "Agreement". If there are scoring errors or contradictory reasons, they indicate "Disagreement". The overall consistency between the model's scores and human expectations is determined by the proportion of "Agreements". This method, compared to having humans score or rank directly for comparison, more directly reflects consistency, reduces the influence of human subjectivity, speeds up the annotation process, and minimizes errors due to the difficulty of human ranking or scoring.

We randomly sample 417 data points (stratified by tasks) from the scoring results of five representative LLMs, and three volunteers perform "Consistency Judgments" on these samples. We then average the "Agreement Rate" of all volunteers. According to Table 9, the average agreement rate across the five LLMs is **89.4%**. This indicates that our model scoring method has a high level of consistency with human expectations, providing results that are of significant reference value. In the end, we opts for ERNIE-3.5 [10] as the scoring model, with the cost of completing one round of scoring being approximately $3.8, striking a balance between cost and effectiveness.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline
**Model** & **\#Parameter** & **Rene Model** & **Censformer** & **Agreements** & **Methods** & **Domain** \\ \hline \hline \multirow{3}{*}{
\begin{tabular}{} \end{tabular} } & 70 & Backlass & Words & Words & \(\text{{}}\)\(\text{{}^{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{{\text{{\texttexttexttext{{{                  }}}}}}}}}}}}}}} ) \\  & 70 & - & - & - & - \\  & & & & & - \\  & & & & & & - \\  & & & & & & \\  & & & & & & \\ \end{tabular} \end{table 8: Details of all evaluated models. Zoom in for better view.

Figure 13: Scoring prompt samples.

[MISSING_PAGE_FAIL:36]

[MISSING_PAGE_FAIL:37]

## Appendix A

Figure 15: The sample responses of LLMs on the sentence structure task.

Figure 16: The sample responses of LLMs on the basic Q&A task.

Figure 17: The sample responses of LLMs on the knowledge of Sinology Q&A task.

Figure 19: The sample responses of LLMs on the classical Chinese to modern Chinese task.

Figure 18: The sample responses of LLMs on the xiehouyu task.

### The Analysis of Fine-Grained Tasks and Performance

Due to space limitations, the main body of the paper only presents the metrics of LLMs on 26 coarse-grained tasks. In fact, we have subdivided six of these tasks (T1-Sentence structure, T13-Appreciation, T14-Ancient poetry writing, T15-Basic Q&A, T21-Couplet, T22-Idiom) into subtasks, resulting in a total of 42 fine-grained tasks. Table 12 provides the performance of LLMs on these subtasks (fine-grained). Different subtasks involve more detailed knowledge points and methods of examination, hence there are significant performance differences among the subtasks. Overall, LLMs generally perform poorly on tasks involving elliptical sentence, content Q&A of ancient poetry, title and author Q&A of ancient poetry, write the previous sentence, and the origin of idiom. Researchers can use it as a reference to focus on improving the capabilities of LLMs in certain areas.

The task numbers presented in Table 12 correspond to the following meanings, with detailed information available in Appendix B.1.

T1-1: Inverted Sentence Structure; T1-2: Elliptical Sentence; T1-3: Inverted Sentence Types; T1-4: Sentence Structure Identification; T13-1: Apreciation Exam Questions; T13-2: Free Apreciation; T14-1: Poetry Writing; T14-2: Ci Writing; T14-3: Qu Writing; T15-1: Content Q&A; T15-2: Title and Author Q&A; T15-3: Write the Next Sentence; T15-4: Write the Previous Sentence; T15-5: Comprehension Dictation; T15-6: Genre Judgment; T21-1: Couplet Following; T21-2: Couplet Writing; T21-3: HengPi Writing; T22-1: Synonyms; T22-2: The Origin of Idiom; T22-3: Idiom Finding; T22-4: Idiom Explanation.

### Performance Radar Chart

Performance radar charts of all evaluated LLMs are depicted from Figure 20 through 50. The radar charts from left to right represent the fields of Ancient Prose, Ancient Poetry, and Ancient Literary Culture. The meanings of T1-T26 are detailed in Appendix B.1.

### Analysis of Model Scoring Preferences

We design two methods to analyze the preference bias in model scoring.

**Use another model to score.** GPT-3.5-Turbo-0125 [61] is used as an alternative scoring model for the five evaluated models. The results are shown in Table 13. The results show that the ranking of models remain the same as when scored by ERNIE-3.5-8K-0329 [10], with ERNIE-4.0-8K-0329 [10] still ranking first. This suggests that its leading position is genuine and less affected by scoring bias.

**Manual analysis.** We categorize the questions in WenMind into objective and subjective types, and conduct manual analysis on the model's scoring results for each. We find that for objective questions with standard answers, ERNIE essentially judges and scores each one strictly according to the scoring points of the answer, showing a weaker bias. For subjective questions, we select classical Chinese

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l l l l l l l l l l} \hline \hline \multicolumn{1}{l}{**Model**} & \multicolumn{1}{c}{**Task**} & \multicolumn{1}{c}{**Task**} & \multicolumn{1}{c}{**T13**} & \multicolumn{1}{c}{T14} & \multicolumn{1}{c}{T15} & \multicolumn{1}{c}{T16} & \multicolumn{1}{c}{T17} & \multicolumn{1}{c}{T18} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T18} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T17} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} & \multicolumn{1}{c}{T19} \\ \hline \multicolumn{11}{l}{**HeadHead**} & 70.1 & 70.1 & 70.0 & 70.1 & 70.1 & 70.0 & 70.

writing and Qu writing tasks for analysis. (1) In classical Chinese writing task, ERNIE-4.0-8K-0329 scores 70.8, ranking second among all models. The first place is the Spark-3.5 [56] model with a score of 71.2. We review the response of ERNIE-4.0-8K-0329 and find that ERNIE-4.0-8K-0329 indeed has strong capabilities in classical Chinese writing, but this does not rule out the influence of model bias. (2) In the Qu writing task, ERNIE-4.0-8K-0329 scores 67, ranking 11th among all models. This also indirectly indicates that in subjective tasks, ERNIE does not necessarily give high scores to its own answers.

In summary, the preference bias in model scoring is relatively weak, making the use of model scoring a viable alternative to manual scoring and traditional metrics.

### Error Analysis of LLMs

We observe significant performance differences in the model across different tasks. We conduct an error analysis on T1 (Sentence Structure), T13 (Appreciation), T14 (Ancient Poetry Writing), T15 (Basic Q&A), T21 (Couplet), and T22 (Idiom), which involve more specialized sub-tasks.

**T1-Sentence Structure.** The model performs weakest on Elliptical Sentence (T1-2), with a score of only 14.2. It struggles with identifying the omitted parts and restoring sentence completeness. In contrast, its performance on Sentence Structure Identification (T1-4) is better, with a score of 47.3, indicating a relative strength in recognizing simpler sentence structures. However, the model's overall understanding and application of various sentence types remain incomplete, especially when dealing with the implicit semantics and grammatical features of classical Chinese.

**T13-Appreciation.** The model achieves a higher score in Free Appreciation (T13-2) compared to Appreciation Exam Questions (T13-1), with scores of 53.3 and 49.4, respectively. This suggests that the model performs better in less restrictive contexts, showing adaptability in textual analysis. In structured exam questions, however, its limitations become apparent, likely due to the deeper cultural understanding and appreciation skills required, which the model has not yet fully mastered.

**T14-Ancient Poetry Writing.** The model scores well in Ci Writing (T14-2) and Qu Writing (T14-3), with scores of 54.0 and 54.3, indicating strong performance in writing tasks with fixed formats. However, its score for Poetry Writing (T14-1) is lower, at 44.1, reflecting the challenge of producing creative and artistically expressive poetry, particularly when dealing with diverse themes and styles.

**T15-Basic Q&A.** The lowest score is observed in Title and Author Q&A (T15-2), with only 12.7, highlighting challenges in recalling and recognizing specific textual information, particularly in large datasets. In contrast, Comprehension Dictation (T15-5) achieves the highest score, at 45.8, indicating the model's relative strength in language comprehension and summarization, despite its struggles with more specific questions.

**T21-Couplet.** The model excels in Couplet Writing (T21-2) and HengPi Writing (T21-3), scoring 61.7 and 60.2, respectively, demonstrating its capability in generating structured and formatted content. However, it performs less well in Couplet Following (T21-1), with a score of 46.2, indicating difficulties in completing couplets, where maintaining contextual harmony and creativity is crucial.

**T22-Idiom.** The lowest score is in The Origin of Idiom (T22-2), with only 6.4, revealing the model's significant shortcomings in associating idioms with their origins, reflecting a lack of historical knowledge. By contrast, it performs better in Idiom Explanation (T22-4), with a score of 50.3, showing that the model can provide accurate explanations for common idioms, though deeper cultural and contextual understanding still needs improvement.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & **ERNIE-3.5-8K-0329** & **GPT-3.5-Turbo-0125** \\ \hline ERNIE-4.0-8K-0329[10] & 64.3 & 79.7 \\ Spark-3.5[56] & 60.9 & 77.4 \\ Qwen1.5-14B-Chat[41] & 54.9 & 73.7 \\ Baichuan2-13B-chat[42] & 45.5 & 66.4 \\ Xunzi-Qwen1.5-7B[47] & 37.0 & 59.8 \\ \hline \hline \end{tabular}
\end{table}
Table 13: The results of scoring five models using ERNIE-3.5-8K-0329 and GPT-3.5-Turbo-0125, respectively.

[MISSING_PAGE_FAIL:43]

[MISSING_PAGE_FAIL:44]

Statement of Responsibility

The WenMind benchmark is released under the CC-BY-NC-SA-4.0 license and strictly adheres to the agreements of the original data sources. The licenses for the original data sources are detailed in Appendix B.3. We have reviewed the ethical guidelines and ensured that the content of the paper and the benchmark are in compliance with the guidelines.

Figure 21: Performance of Baichuan2-13B-Chat.

Figure 22: Performance of Firefly-Baichuan2-13B.

Figure 20: Performance of Baichuan2-7B-Chat.

Figure 21: Performance of Baichuan2-13B-Chat.

Figure 23: Performance of ChatGLM2-6B.

Figure 24: Performance of ChatGLM3-6B.

Figure 25: Performance of InternLM2-Chat-7B.

Figure 26: Performance of Qwen1.5-0.5B-Chat.

Figure 23: Performance of ChatGLM2-6B.

Figure 30: Performance of Qwen1.5-32B-Chat.

Figure 28: Performance of Qwen1.5-7B-Chat.

Figure 29: Performance of Qwen1.5-14B-Chat.

Figure 27: Performance of Qwen1.5-4B-Chat.

Figure 31: Performance of Qwen1.5-72B-Chat.

Figure 34: Performance of Yi-1.5-34B-Chat.

Figure 32: Performance of Yi-1.5-6B-Chat.

Figure 33: Performance of Yi-1.5-9B-Chat.

Figure 37: Performance of Spark-3.5.

Figure 38: Performance of Gamma-1.1-7B-IT.

Figure 35: Performance of ERNIE-3.5-8K-0329.

Figure 36: Performance of ERNIE-4.0-8K-0329.

Figure 37: Performance of Spark-3.5.

Figure 41: Performance of LLaMA2-13B-Chat.

Figure 42: Performance of LLaMA2-Chinese-7B-Chat.

Figure 39: Performance of Ziya-LLaMA-13B-v1.1.

Figure 40: Performance of LLaMA2-7B-Chat.

Figure 43: Performance of LLaMA2-Chinese-13B-Chat.

Figure 44: Performance of LLaMA3-8B-Instruct.

Figure 45: Performance of LLaMA3-Chinese-8B-Chat.

Figure 46: Performance of GPT-3.5.

Figure 43: Performance of LLaMA2-Chinese-13B-Chat.

Figure 47: Performance of GPT-4.

Figure 48: Performance of Ancient-Chat-LLM-7B.

Figure 50: Performance of Xunzi-Qwen1.5-7B.