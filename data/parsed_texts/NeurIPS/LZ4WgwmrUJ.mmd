# High-dimensional Contextual Bandit Problem

without Sparsity

 Junpei Komiyama

New York University

junpei@komiyama.info

&Masaaki Imaizumi

The University of Tokyo / RIKEN Center for AIP

imaizumi@g.ecc.u-tokyo.ac.jp

Typically, the number of feature \(p\) can be exponential to the number of datapoints \(T\).

###### Abstract

In this research, we investigate the high-dimensional linear contextual bandit problem where the number of features \(p\) is greater than the budget \(T\), or it may even be infinite. Differing from the majority of previous works in this field, we do not impose sparsity on the regression coefficients. Instead, we rely on recent findings on overparameterized models, which enables us to analyze the performance of the minimum-norm interpolating estimator when data distributions have small effective ranks. We propose an explore-then-commit (EtC) algorithm to address this problem and examine its performance. Through our analysis, we derive the optimal rate of the ETC algorithm in terms of \(T\) and show that this rate can be achieved by balancing exploration and exploitation. Moreover, we introduce an adaptive explore-then-commit (AEtC) algorithm that adaptively finds the optimal balance. We assess the performance of the proposed algorithms through a series of simulations.

## 1 Introduction

The multi-armed bandit problem (Robbins, 1952; Lai and Robbins, 1985) has been widely studied in the field of sequential decision-making problems in uncertain environments, and it can be applied to a variety of real-world scenarios. This problem involves an agent selecting one of \(K\) arms in each round and receiving a corresponding reward. The agent aims to maximize the cumulative reward over rounds by using a clever algorithm that balances exploration and exploitation. In particular, a version of this problem called the contextual bandit problem (Abe and Long, 1999; Li et al., 2010) has attracted significant attention in the machine learning community. By observing the contexts associated with the arms, the agent can choose the best arm as a function of the contexts. This extension enables us to model many personalized machine learning scenarios, such as recommendation systems (Li et al., 2010; Wang et al., 2022) and online advertising (Tang et al., 2013), and personalized treatments (Chakraborty and Murphy, 2014).

Most of the papers about stochastic linear bandits assume that the number of features \(p\) is moderate (Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011). When \(p=o(\sqrt{T})\) to the number of rounds \(T\), the model is identifiable, and the agent can choose the best arm for most rounds. However, recent machine learning models desire to utilize an even larger number of features, and the theory of bandit models under the identifiability assumption does not necessarily reflect the modern use of machine learning. Several recent papers have overcome this limitation by considering sparse linear bandit models (Wang et al., 2018; Kim and Paik, 2019; Bastani and Bayati, 2020; Hao et al., 2020; Oh et al., 2021; Li et al., 2022; Jang et al., 2022). Sparse linear bandit models accept a very large number of features1 and suppress most of the coefficients by introducing the \(\ell 1\) regularize.

That said, the sparsity imposed by such models limits the applicability of these models. For example, in the case of recommendation models based on factorization, each user is associated with a dense latent vector (Rendle, 2010; Agarwal et al., 2012; Wang et al., 2022), which implies the sparsity is not unlikely the case. Another possible drawback of sparse models is that it requires the condition number to be close to one (e.g., the restricted isometry property, see Van De Geer and Buhlmann (2009) for review). This implies that the quality of the estimator is compromised by the noise on the features that correspond to small eigenvalues. Furthermore, it is still non-trivial to select a proper value of the penalty coefficient as a hyper-parameter. For example, Hara and Maehara (2017) claims that small changes in the choice of coefficients significantly alter feature selection, and Miolane and Montanari (2021) show a limitation of the conventional theory on the choice of penalty coefficients.

In this paper, We consider an alternative high-dimensional linear bandit problem without sparsity. We allow \(p\) to be as large as desired, and in fact, we even allow \(p\) to be infinitely large. Such an overparameterized model has more parameters than the number of training data points. A natural estimator in such a case is an _interpolating_ estimator, which perfectly fits the training data. We adopt recent results that bound the error of the estimator in terms of the _effective rank_(Koltchinskii and Lounici, 2017; Bartlett et al., 2020) on the covariance of the features. When the eigenvalues of the covariance decay moderately fast, we can obtain a concentration inequality on the squared error of the estimator.

The contributions of this paper are as follows: First, We consider explore-then-commit (EtC) strategy for the stochastic bandit problem based on the minimum-norm interpolating estimator. We derive the optimal rate of exploration that minimizes regret. However, EtC requires model-dependent parameters on the covariance, which limits the practical utility. To address this limitation, we propose an adaptive explore-then-commit (AEtC) strategy, which adaptively estimates these parameters and achieves the optimal rate. We conduct simulations to verify the efficiency of the proposed method.

## 2 Preliminary

### Notation

For \(z\in\mathbb{N}\), \([z]:=\{1,2,\ldots,z\}\). For \(x\in\mathbb{R}\), the notation \(\lfloor x\rfloor\) here denotes the largest integer that is less than or equal to a scalar \(x\). For vectors \(X,X^{\prime}\in\mathbb{R}^{P}\), \(\langle X,X^{\prime}\rangle:=X^{\top}X^{\prime}\) is an inner product, \(\|X\|_{2}^{2}:=\langle X,X\rangle\) is an \(\ell 2\)-norm. For a positive-definite matrix \(A\in\mathbb{R}^{p}\times\mathbb{R}^{P}\), \(\|X\|_{\mathbf{A}}^{2}:=\langle X,\mathbf{A}X\rangle\) is a weighted \(\ell 2\)-norm. \(\|\mathbf{A}\|_{\text{op}}\) denotes an operator norm of \(\mathbf{A}\). \(O(\cdot),o(\cdot),\Omega(\cdot),\omega(\cdot)\) and \(\Theta(\cdot)\) denotes Landau's Big-O, little-o, Big-Omega, little-omega, and Big-Theta notations, respectively. \(\widetilde{O}(\cdot),\widetilde{\Omega}(\cdot)\), and \(\widetilde{\Theta}(\cdot)\) are the notations that ignore polylogarithmic factors.

### Problem Setup

This paper considers a linear contextual bandit problem with \(K\) arms. We consider the fully stochastic setting, where the contexts, as well as the rewards, are drawn from fixed distributions. For each round \(t\in[T]\) and arm \(i\in[K]\), we define \(X^{(i)}(t)\) as a \(p\)-dimensional zero-mean sub-Gaussian vector. We assume \(X^{(i)}(t)\) is independent among rounds (i.e., vectors in two different rounds \(t,t^{\prime}\) are independent) but allow vectors \(X^{(1)}(t),X^{(2)}(t),\ldots,X^{(K)}(t)\) to be correlated with each other. The forecaster chooses an arm \(I(t)\in[K]\) based on the \(X^{(i)}(t)\) values of all the arms, and then observes a reward that follows a linear model as shown in

\[Y^{(I(t))}(t)=\langle X^{(I(t))}(t),\theta^{(I(t))}\rangle+\xi(t).\] (1)

The unknown true parameters \(\theta^{(i)}\) for each arm \(i\in[K]\) lie in a parameter space \(\Theta\subset\mathbb{R}^{P}\), and the independent noise term \(\xi(t)\) has zero mean and variance \(\sigma^{2}>0\). We assume that \(\xi(t)\) is sub-Gaussian, and for the sake of simplicity, we assume that it does not depend on the choice of the arm. However, our results can be extended to the case where \(\xi(t)\) varies among arms. We assume that each \(\theta^{(i)}\) are bounded \(\|\theta^{(i)}\|_{2}\leq\theta_{\text{max}}\). For each \(i\in[K]\), we define a covariance matrix \(\Sigma^{(i)}=\mathbb{E}[X^{(i)}(t)X^{(i)}(t)^{\top}]\in\mathbb{R}^{P\times P}\).

We define \(i^{*}(t):=\operatorname*{argmax}_{i\in[K]}\langle X^{(i)}(t),\theta^{(i)}\rangle\) as the (ex ante) optimal arm at round \(t\). Our goal is to design an algorithm that maximizes the total reward, which is equivalent to minimizing the following expected regret (Lai and Robbins, 1985; Auer et al., 2002);

\[R(T):=\mathbb{E}\left[\sum_{t=1}^{T}\left(\langle X^{(i^{\prime}(t))}(t),\theta^{ (i^{\prime}(t))}\rangle-\langle X^{(I(t))}(t),\theta^{(I(t))}\rangle\right) \right],\] (2)

where the expectation is taken with respect to the randomness of the contexts and (possibly) on the choice of arm \(I(t)\).

### Theory of Overparametrized Models

The primary focus of this paper is on scenarios where the number of arms \(K\) is moderate, but the number of features \(p\) is greater than the budget \(T\), possibly to the point of being infinite. The efficiency of linear regression models in such scenarios depends significantly on the covariance matrix, \(\Sigma^{(i)}\). Unlike sparsity, the theory of benign overfitting tightly examines errors using the decay of the eigenvalues of the covariance matrix of the context. In particular, if the decay rate of the eigenvalues is at a certain level, the error in linear regression converges to zero, even in high-dimensional spaces. To provide a more rigorous analysis of eigenvalue decay, the concept of _effective rank_ is introduced in Section 2.4.

**Remark 1**.: (Dependence on \(T\)) In accordance with Bartlett et al. (2020), we permit the covariance matrix \(\Sigma^{(i)}\) to depend on \(T\). In other words, we consider the sequence of covariances \(\Sigma^{(i)}(1)\), \(\Sigma^{(i)}(2)\),... for each \(T=1,2,\dots\). The linear regression is consistent if the effective rank of \(\Sigma^{(i)}(T)\) grows sufficiently slow as a function of \(T\).

### Effective Ranks of Covariance Matrix

For a covariance matrix \(\Sigma^{(i)}\) for \(i\in[K]\) and \(k\in[p]\), let \(\lambda_{k}^{(i)}\) be its \(k\)-th largest eigenvalue, such that \(\Sigma^{(i)}=\sum_{k=1}^{p}\lambda_{k}^{(i)}u_{k}^{(i)}(u_{k}^{(i)})^{\top}\) with order \(\lambda_{1}^{(i)}\geq\lambda_{2}^{(i)}\geq\cdots\geq\lambda_{p}^{(i)}\) and eigenvectors \(u_{k}^{(i)}\). We define the concept of _effective rank_ as

\[r_{k}(\Sigma^{(i)}):=\frac{\sum_{j>k}\lambda_{j}^{(i)}}{\lambda_{k+1}^{(i)}}, \ \text{and}\ \ R_{k}(\Sigma^{(i)}):=\frac{(\Sigma_{j>k}\lambda_{j}^{(i)})^{2}}{\Sigma_{j >k}(\lambda_{j}^{(i)})^{2}}.\] (3)

The first quantity \(r_{k}(\Sigma^{(i)})\) is related to the trace of \(\Sigma^{(i)}\), and the second quantity \(R_{k}(\Sigma^{(i)})\) is related to the decay rate of the eigenvalues.

The effective rank is used as a measure of the intrinsic complexity of high-dimensional data, by rigorously capturing a decay rate of eigenvalues. If the covariance matrix \(\Sigma^{(i)}\) is an identity matrix of size \(p\), then \(r_{0}(\Sigma^{(i)})\) and \(R_{0}(\Sigma^{(i)})\) are both equal to \(p\) (the rank of \(\Sigma^{(i)}\)). However, we anticipate that these quantities will be less than \(p\), which enables learning with fewer samples. In Figure 1, we plot of the effective rank in \(k\) with certain cases of eigenvalues \(\{\lambda_{k}\}_{k}\) of \(\Sigma\). Even though \(\Sigma\) is full-rank, the effective rank \(R_{k}(\Sigma)\) is sublinear in \(k\), which reflects the intrinsic low-complexity of data.

Because of this property, the effective ranks are used in modern high-dimensional statistics. Koltchinskii and Lounici (2017) evaluated concentration inequalities for high-dimensional random matrices using the notion. Bartlett et al. (2020); Tsigler and Bartlett (2020) evaluated the prediction error of a high-dimensional linear regression using the effective ranks and showed the consistency of the prediction.

## 3 Explore-then-Commit with Interpolation

The _Explore-then-Commit_ (EtC) algorithm is a well-known approach for solving high-dimensional linear bandit problems, and it has been shown to be effective in previous studies such as Hao et al. (2020); Li et al. (2022). The EtC algorithm operates by first conducting \(T_{0}=NK<T\) rounds of exploration, during which it uniformly explores all available arms to construct an estimator \(\widehat{\theta}^{(i)}\) for each arm \(i\in[K]\). After the exploration phase, the algorithm proceeds with exploitation. Let \(N\) be the number of the draws of arm \(i\), and \(\mathbf{X}^{(i)}=(X_{1}^{(i)},...,X_{N}^{(i)})^{\top}\in\mathbb{R}^{N\times p}\) and \(\mathbf{Y}^{(i)}=(Y_{1}^{(i)},...,Y_{N}^{(i)})^{\top}\in\mathbb{R}^{N}\)be the observed contexts and rewards of arm \(i\), where \((X_{n}^{(i)},Y_{n}^{(i)})\) is the corresponding values on the \(n\)-th draw of arm \(i\). Since we choose \(I(t)\) uniformly during the exploration phase, these are independent and identically drawn samples from the corresponding distribution.

For estimating the parameter \(\theta^{(i)}\), we consider the minimum-norm _interpolating estimator_ that perfectly fits the data, which reveals its advantage in recent results on high-dimensions (Bartlett et al., 2020). Rigorously, we assume \(p>N\) and and consider the following definition:

\[\widehat{\theta}^{(i)}:=\text{argmin}\left\{\|\theta\|_{2}\left|\theta\in \mathbb{R}^{p},0=\sum_{(Y_{n}^{(i)},X_{n}^{(i)}):n\leq N}(Y_{n}^{(i)}-\langle \theta,X_{n}^{(i)}\rangle)^{2}\right\}.\]

This estimator has a simple representation \(\widehat{\theta}^{(i)}=(\mathbf{X}^{(i)})^{\top}(\mathbf{X}^{(i)}(\mathbf{X} ^{(i)})^{\top})^{-1}\mathbf{Y}^{(i)}\). The EtC algorithm is presented in Algorithm 1, which utilizes the aforementioned interpolating estimator. In the subsequent sections, we will first discuss the assumptions on the data-generating process, followed by an analysis of the accuracy of the estimator. We then present an upper bounds on the regret of the EtC algorithm.

## 4 Theory of Explore-then-Commit

This section analyzes the EtC algorithm. If we were to fix the number of samples, this theory would largely align with the existing literature (Bartlett et al., 2020; Tsigler and Bartlett, 2020). However, the unique challenge in our case arises from the fact that the EtC selects the number of samples used to construct an estimator to balance between exploration and exploitation.

### Assumption and Notion

We consider a spectral decomposition \(\Sigma^{(i)}=V^{(i)}\Lambda^{(i)}(V^{(i)})^{\top}\) with the matrices (operators) \(V^{(i)}\) and \(\Lambda^{(i)}\) for each arm \(i\in[K]\), then impose the following assumptions.

**Assumption 1** (sub-Gaussianity).: _For all \(t\in\mathbb{N}\) and \(i\in[K]\), the followings hold:_

Figure 1: Eigenvalues \(\lambda_{k}\) (left), the effective rank \(r_{k}(\Sigma)\) (middle), and the effective rank \(R_{k}(\Sigma)\) (right) with \(k=1,...,15\) and \(T=3\); Case 1: \(\lambda_{k}=Ck^{-1+1/{P^{0.99}}}\), and Case 2: \(\lambda_{k}=C(\exp(-k)+T\exp(-T)/p)\). Case 1 considers a decay that is slightly faster than \(k^{-1}\), whereas Case 2 considers an exponentially fast decay. The slower increase of \(r_{k}(\Sigma)\) and \(R_{k}(\Sigma)\) in Case 2 reflects the impact of the eigenvalues’ faster decay.

* _There exists a random vector_ \(Z^{(i)}(t)\) _such that_ \(X^{(i)}(t)=V^{(i)}(\Lambda^{(i)})^{1/2}Z^{(i)}(t)\) _which is independent elements and sub-Gaussian with a parameter_ \(\kappa_{x}^{2}>0\)_, that is, for all_ \(\lambda\in\mathbb{R}^{p}\)_, we have_ \(\mathbb{E}[\exp(\langle\lambda,Z^{(i)}(t)\rangle)]\leq\exp(\kappa_{x}^{2}\| \lambda\|_{2}^{2}/2)\)_._
* _Moreover,_ \(\xi(t)\) _is conditionally sub-Gaussian with a parameter_ \(\kappa_{\xi}^{2}>0\)_, that is, for all_ \(\lambda\in\mathbb{R}\)_, we have_ \(\mathbb{E}[\exp(\lambda\xi(t))\mid X^{(i)}(t)]\leq\exp(\kappa_{\xi}^{2} \lambda^{2}/2)\)_._

The first bullet point of Assumption 1 requires that the features are multivariate sub-Gaussian and their tail is not too heavy, which is important for concentration inequalities on random variables (Vershynin, 2018). The second bullet point of Assumption 1, which states that rewards involve sub-Gaussian noise, is a common assumption in contextual bandit problems.

### Benign Covariance

We impose a condition to be benign on the covariance matrix \(\Sigma^{(i)}\) for all \(i\in[K]\). The condition is described using the notion of effective/coherent rank, which is commonly applied to the study of benign overfitting (Bartlett et al., 2020; Tsigler and Bartlett, 2020). However, unlike those papers that estimate using all \(T\) samples, we only use a portion of the data for learning.

We first define the _coherent rank_ of \(\Sigma^{(i)}\) with a number of samples \(N\) as

\[k_{N}^{*}=k_{N}^{*}(\Sigma^{(i)},N):=\min\{k\in\mathbb{N}\cup\{0\}\mid r_{k}( \Sigma^{(i)})\geq N\},\]

where we define \(\min\{\emptyset\}=+\infty\). By using the coherent rank, we decompose the error into two quantities called _effective bias/variance_ denoted as \(B_{N,T}^{(i)}\) and \(V_{N,T}^{(i)}\), based on a budget of \(T\) and the number of samples \(N\) used for estimation.

\[B_{N,T}^{(i)}:=\lambda_{k_{N}^{*}}^{(i)},\text{ and }V_{N,T}^{(i)}:=\left( \frac{k_{N}^{*}}{N}+\frac{N}{R_{k_{N}^{*}}(\Sigma^{(i)})}\right).\] (4)

**Definition 1** (Benign covariance).: Under Assumption 1, a covariance matrix \(\Sigma^{(i)}\) is _benign_, if \(B_{N,T}^{(i)}=o(1)\) and \(V_{N,T}^{(i)}=o(1)\) hold as \(N,T\rightarrow\infty\) while \(N=\Theta(T)\).

To put it differently, Assumption 1 and Definition 1 state that, if we can achieve consistent estimation by using all the data for estimation, then the covariance matrix is considered benign.

Technically, the benign property implies that eigenvalues decay fast enough compared with \(T\). In particular, the following examples have been considered in the literature.

**Proposition 1** (Example of Benign Covariance, Theorem 31 in Bartlett et al. (2020)2 ).: _A covariance matrix \(\Sigma^{(i)}\) with eigenvalues \(\lambda_{1}^{(i)},\lambda_{2}^{(i)},...\) is benign if it satisfies one of the following._

Footnote 2: It should be noted that Bartlett et al. (2020) only provided the variance term. Later on, Tsigler and Bartlett (2020) described both the variance and bias terms, which we follow.

* [leftmargin=*]
* _Example 1:_ _Let_ \(a\in(0,1)\)_,_ \(p=\infty\) _and_ \(\lambda_{k}^{(i)}=k^{-(1+1/T^{a})}\)_. In this case, we have_ \(B_{N,T}^{(i)}=O((T^{a}/N)^{1+1/T^{a}})\) _and_ \(V_{N,T}^{(i)}=O(T^{a}/N+T^{-a})\)_. For this model to be learnable,_ \(N\gg T^{a}\) _is required, and the variance dominates the bias._
* _Example 2:_ _Let_ \(\lambda_{k}^{(i)}=k^{-b}\) _and_ \(p=p_{T}=O(T^{c})\) _with_ \(b\in(0,1)\) _and_ \(c\in\left(\max\left(1,\frac{2}{2-b},\frac{1}{1-b^{2}}\right),\frac{1}{1-b}\right)\)_. In this case, we have_ \(B_{N,T}^{(i)}=O(T^{c\,(1-b)}/N)\) _and_ \(V_{N,T}^{(i)}=O((N/T^{c})^{\max\{1,\frac{1-b}{b}\}})\)_. For this model to be learnable,_ \(N\gg T^{c\,(1-b)}\) _is required, and the bias dominates the variance._

The first example is when the decay rate is near \(k^{-1}\), but the trace is \(O(T^{a})\). The second example is when the decay rate is smaller than \(k^{-1}\), and the trace is \(O(T^{c\,(1-b)})\). Note that Bartlett et al. (2020) provided two other examples of the benign covariance matrices.3 Our analysis mainly focuses on the two examples in Proposition 1, but another example is also empirically tested in Section 6.

Footnote 3: One of the omitted examples is the case where eigenvalues decay slightly slower than Example 1. The other example is the case where eigenvalues decay exponentially but has a noise term.

### Estimation Error by Exploration

We evaluate an error in the estimator \(\widehat{\theta}^{(i)}\) by its prediction quality. That is, with a covariance matrix \(\Sigma^{(i)}\) and an identical copy \(X_{i}^{(i)}\) of \(X_{i}^{(i)}\), we study

\[\|\theta^{(i)}-\widehat{\theta}^{(i)}\|_{\Sigma^{(i)}}^{2}=\mathbb{E}_{X_{i}^ {(i)}}\left[(\langle\theta^{(i)},X_{*}^{(i)}\rangle-\langle\widehat{\theta}^{( i)},X_{*}^{(i)}\rangle)^{2}\right],\]

The following result bounds the error of the estimator \(\widehat{\theta}^{(i)}\) in terms of bias and variance, which is a slight extension of Tsigler and Bartlett (2020). For \(k\in[p]\), we define an empirical submatrix as \(\mathbf{X}_{k:\infty}^{(i)}\in\mathbb{R}^{N\times(p-k)}\) as the \(p-k\) columns to the right of \(\mathbf{X}^{(i)}\), and define a Gram sub-matrix \(A_{k}^{(i)}=\mathbf{X}_{k:\infty}^{(i)}(\mathbf{X}_{k:\infty}^{(i)})^{\top} \in\mathbb{R}^{N\times N}\).

**Theorem 2**.: _Suppose Assumption 1. If there exists \(c_{U}>1\) such that \(k_{N}^{*}<N/c_{U}\) and a condition number of \(A_{k}^{(i)}\) is positive with probability at least \(1-c_{U}e^{-N/c_{U}}\), then we have_

\[\|\widehat{\theta}^{(i)}-\theta^{(i)}\|_{\Sigma^{(i)}}^{2}\leq C_{U}\left(B_{ N,T}^{(i)}+V_{N,T}^{(i)}\right),\] (5)

_with some constant \(C_{U}>0\) and probability at least \(1-2c_{U}e^{-N/c_{U}}\)._

Theorem 2 implies that the estimation error converges to zero if \(\Sigma^{(i)}\) has the benign property. The assumption on the condition number of \(A_{k}^{(i)}\) has a sufficient condition, which is provided in Lemma 3 in Tsigler and Bartlett (2020).

Moreover, the following lemma implies the tightness of the analysis in Theorem 2.

**Lemma 3** (Lower bound of estimation error, Theorem 10 in Tsigler and Bartlett (2020)).: _Suppose Assumption 1. These exist some constants \(c_{L},C_{L}>0\) such that, with probability at least \(1-c_{L}e^{-N/c_{L}}\), we have_

\[\|\widehat{\theta}^{(i)}-\theta^{(i)}\|_{\Sigma^{(i)}}^{2}\geq C_{L}\left(B_{ N,T}^{(i)}+V_{N,T}^{(i)}\right).\] (6)

In other words, the upper bound in Theorem 2 is optimal up to a constant.4

Footnote 4: Note that the constant here can depend on model parameters.

### Regret Bound of Explore-then-Commit

This section analyzes the EtC algorithm. We introduce an error function \(\mathrm{E}(N,T)\) that characterizes the rate of regret, which can be obtained by considering \((B_{N,T}^{(i)}+V_{N,T}^{(i)})^{1/2}\).

**Assumption 2**.: (Error function5) _There exists a continuous function \(\mathrm{E}:\mathbb{R}^{2}\rightarrow\mathbb{R}^{+}\) that is decreasing in \(N\) such that \(\|\widehat{\theta}^{(i)}-\theta^{(i)}\|_{\Sigma^{(i)}}=\widehat{\Theta}\left( \mathrm{E}(N,T)\right)\) as \(N,T\rightarrow\infty\)._Assumption 2 is satisfied in Examples 1 and 2. In particular, for Example 1 in Proposition 1, the error function is given as \(\operatorname{E}(N,T)=\sqrt{T^{a}/N+T^{-a}}\), while for Example 2 in Proposition 1, it is given as \(\operatorname{E}(N,T)=\sqrt{T^{c\,(1-b)}/N}\).

**Theorem 4**.: _Suppose Assumptions 1-2. Suppose that we run the EtC algorithm (Algorithm 1). Then, regret (2) satisfies_

\[R(T)=\widetilde{O}(L(T,K)),\]

_as \(T\to\infty\) with some \(\alpha>0\). where \(L(T,K)\) is such that_

\[N^{*}=\min_{N}\{N:N\geq T\operatorname{E}(N/K,T)\}.\] (7)

Specifically, for Example 1 in Proposition 1, we have \(L(T,K)=\max(T^{(2+a)/3}K^{2/3},T^{1-a/2})\), whereas for Example 2 in Proposition 1, we have \(L(T,K)=T^{(2+c(1-b))/3}K^{2/3}\).

Proof sketch of Theorem 4.: We show that the regret-per-round is \(O(1)\) during the exploration phase. Moreover, regret-per-round is \(\widetilde{O}(\operatorname{E}(T_{0}/K,T))\) during the exploitation phase. The total regret is \(T_{0}+\widetilde{O}(\operatorname{E}(T_{0}/K,T))T\), and optimizing \(T_{0}\) by using the decreasing property of \(\operatorname{E}(N,T)\) in \(N\) yields the desired result. 

### Matching Lower Bound

We show that this choice of \(T_{0}\) as a function of \(T\) is indeed optimal.

**Theorem 5**.: _Suppose Assumptions 1-2. Assume that we run the EtC algorithm (Algorithm 1). For any choice of \(T_{0}\), the following event occurs with strictly positive probability as \(T\to\infty\):_

\[R(T)=\widetilde{\Omega}\left(L(T,3)\right).\]

Proof sketch of Theorem 5.: We explicitly construct a model with \(K=3\). Let \(\epsilon_{T}=\operatorname{E}(T_{0}/K,T)\). In the model, the \(\Sigma^{(1)},\Sigma^{(2)},\Sigma^{(3)}\) are identical, and the only difference is that the first coefficients \(\theta_{1}^{(1)}=1,\theta_{1}^{(2)}=\Theta(\epsilon_{T}),\theta_{1}^{(3)}=0\). All other coefficients are set to zero. Roughly speaking, the gap is

\[\left|\langle X^{(i)}(t),\widetilde{\theta}^{(i)}\rangle-\langle X^{(j)}(t), \widetilde{\theta}^{(j)}\rangle\right|=\left\{\begin{array}{ll}\Theta(1)& (i=1,j=2,3)\\ \Theta(\epsilon_{T})&(i=2,j=3)\end{array}\right..\]

The regret-per-round during the exploration phase is \(\Theta(1)\) due to a misidentification of the best arm between arm 1 and arms \(\{2,3\}\). The regret-per-round during the exploitation phase is \(\Theta(\epsilon_{T})\) due to a misidentification of the best arm between arm 2 and arm 3. 

## 5 Adaptive Explore-then-Commit (AEtC) Algorithm

In the prior section, we demonstrated that the optimal way to minimize EtC's regret is by selecting \(T_{0}\), with \(T_{0}\) balancing the exploration and the exploitation. However, this requires knowledge of the covariance matrix's spectrum, which can be difficult to obtain in advance in certain scenarios. This section explores the way to adaptively determines the extent of exploration required.

### Estimator

We assume that \(\Sigma^{(i)}\) follows the data-generating process of Example 1 or 2 in Proposition 1. We use \(\beta_{T}\) to denote that

\[\lambda_{j}^{(i)}=C_{\lambda}j^{-\beta_{T}},\] (8)

with some constant \(C_{\lambda}>0\). We have \(\beta_{T}=1+1/T^{a}\) for Example 1, and \(\beta_{T}=b(<1)\) for Example 2. Balancing exploration and exploitation in an overparameterized model is challenging for the following reasons. First, the number of features \(p\) is very large or even infinite6. Second, the trace is heavy-tailbecause the decay is not very fast. As a result, a naive use of a traditional method does not work. We address this issue by utilizing two estimators. The first estimator is on the trace \(\text{tr}(\widehat{\Sigma}^{(i)})\) that we extracted from overparameterization theory (Bartlett et al., 2020), whereas the second estimator is on the estimated decay rate \(\widehat{\beta}_{T}\) that derives from Bosq (2000); Koltchinskii and Lounici (2017).

For an arm \(i\in[K]\) with \(N=T_{0}/K\) draws, we define an estimated eigenvalues \(\widehat{\lambda}_{1}^{(i)},...,\widehat{\lambda}_{N}^{(i)}>0\) from an empirical covariance matrix \(\widehat{\Sigma}^{(i)}:=N^{-1}\sum_{j=1}^{N}X_{j}^{(i)}(t)X_{j}^{(i)}(t)^{\top}\). We define the empirical trace to be \(\text{tr}(\widehat{\Sigma}^{(i)})=\sum_{j}\widehat{\lambda}_{j}^{(i)}\). The following lemma states the consistency of the estimated trace under very mild conditions.

**Lemma 6**.: (Error bound of empirical trace) _Suppose Assumption 1. Suppose the data generating process (DGP) of Example 1 or Example 2 in Proposition 1. Then, for any \(C_{\text{poly}}>0\), with probability at least \(1-1/T^{C_{\text{poly}}}\), we have the following as \(T\to\infty\):_

\[\frac{|\text{tr}(\Sigma^{(i)})-\text{tr}(\widehat{\Sigma}^{(i)})|}{\text{tr}( \Sigma^{(i)})}=\widehat{O}\left(\frac{\sqrt{\sum_{j}(\lambda_{j}^{(i)})^{2}}} {\sum_{j}\lambda_{j}^{(i)}}\right)=o(1).\] (9)

Moreover, the following bounds the estimation error of each eigenvalue.

**Lemma 7**.: _Suppose Assumption 1. For any \(\delta\in(0,1)\), for any \(C_{\text{poly}}>0\), with probability at least \(1-1/T^{C_{\text{poly}}}\), we have the following for any \(j=1,...,p\) as \(T\to\infty\):_

\[\left|\widehat{\lambda}_{j}^{(i)}-\lambda_{j}^{(i)}\right|=O\left(\sqrt{\frac {\text{tr}(\Sigma^{(i)})}{N}}\right).\] (10)

Lemma 7 implies the estimator of each eigenvalue is \(o(1)\) if we choose \(N\gg\text{tr}(\Sigma^{(i)})\). However, even if we choose a large \(N\), the error is still non-negligible for the tail of eigenvalues where \(|\widehat{\lambda}_{j}^{(i)}-\lambda_{j}^{(i)}|\) is very small7. Consequently, \(\widehat{\lambda}_{j}^{(i)}\) for large \(j\) are not necessarily consistent, so as to the effective rank and the coherent rank estimated from them. To address this, we estimate the decay rate \(\beta_{T}\), and then estimate the subsequent statistics.

Footnote 7: Remember that we consider \(\Sigma^{(i)}=\Sigma^{(i)}(T)\) that depends on \(T\). Tail of eigenvalues are \(o(1)\) to \(T\) as well.

Let the estimated decay rate be \(\widehat{\beta}_{T}=(1/\tau)\sum_{k=1}^{\tau}\log(\widehat{\lambda}_{k}/ \widehat{\lambda}_{k+1})/\log((k+1)/k)\). Theoretically, \(\widehat{\beta}_{T}\) is consistent for any constant \(\tau>1\). In practice, we can use a reasonable constant \(\tau\), such as \(\tau=10\), for robustness. To estimate the effective ranks, we use the following form

\[\widehat{\lambda}_{k}^{(i)}=\widehat{\lambda}_{1}^{(i)}k^{-\widehat{\beta}_{T }}.\]

Namely, we consider empirical analogues of the effective rank for \(k\) as

\[\widehat{r}_{k}(\Sigma^{(i)}):=\frac{\text{tr}(\widehat{\Sigma}^{(i)})}{ \widehat{\lambda}_{k+1}^{(i)}},\text{ and }\widehat{R}_{k}(\Sigma^{(i)}):=\frac{(\text{tr}(\widehat{\Sigma}^{(i)}))^{2}}{ \sum_{j>k}(\widehat{\lambda}_{j}^{(i)})^{2}}.\]

We also define an estimator for the coherent rank as \(\widehat{k}_{N}:=\min\{k\in\mathbb{N}\cup\{0\}\mid\widehat{r}_{k}(\Sigma^{(i) })\geq N\}\). Further, we define estimators of \(B_{N,T}^{(i)}\), \(V_{N,T}^{(i)}\) of Eq. (4) as

\[\widehat{B}_{N,T}^{(i)}:=\widehat{\lambda}_{\widehat{k}_{N}}^{(i)},\text{ and }\widehat{V}_{N,T}^{(i)}:=\left(\frac{\widehat{k}_{N}}{N}+\frac{N}{\widehat{R}_{ \widehat{k}_{N}}(\Sigma^{(i)})}\right),\]

Note that the estimators \(\widehat{r}_{k}\) and \(\widehat{R}_{k}\) use the trace \(\text{tr}(\widehat{\Sigma}^{(i)})\), which is a sum of eigenvalues, instead of the partial sum of the eigenvalues that constitutes the effective rank of Eq. (3). Despite this change, this does not affect8 asymptotic consistency of the coherent rank estimator \(\widehat{k}_{N}\).

Footnote 8: This is because \(\sum_{j}j^{-\beta_{T}}\) with \(\beta_{T}<1\) or \(\beta_{T}\approx 1\) is tail-heavy.

The following lemma states that small \(\tau\) suffices to assure the quality of \(\widehat{\beta}^{(i)}\). We study a convergence rate of \(\widehat{\beta}^{(i)}\) and the other estimators as follows.

``` while\(t=1,2,3,...\)do  Observe \(X^{(i)}(t)\) for all \(i\in[K]\).  Choose \(I(t)=t-K\big{\lfloor}t/K\big{\rfloor}\).  Receive a reward \(Y^{(I(t))}(t)\). if\(t\in\{\lfloor e^{N}\rfloor\mid N\in\mathbb{N},N\geq\log T\}\)then if\(\text{Stop}(t/K)\)then \(T_{0}\gets t\).  Break. endif endif endwhile for\(i\in[K]\)do \(\widetilde{\theta}^{(i)}\leftarrow(\mathbf{X}^{(i)})^{\top}(\mathbf{X}^{(i)}( \mathbf{X}^{(i)})^{\top})^{-1}\mathbf{Y}^{(i)}\). endfor for\(t=T_{0}+1,...,T\)do  Observe \(X^{(i)}(t)\) for all \(i\in[K]\).  Choose \(I(t)=\operatorname*{argmax}_{i\in[K]}\langle X^{(i)}(t),\widetilde{\theta}^{( i)}\rangle\).  Receive a reward \(Y^{(I(t))}(t)\). endfor ```

**Algorithm 2** Adaptive Explore-then-Commit (AEiC)

**Lemma 8**.: _Suppose Assumption 1. Suppose DGP of Example 1 or Example 2 in Proposition 1.9 Assume that we choose \(N=N(T)\) such that \(\operatorname{tr}(\Sigma^{(i)})/N=o(1)\) for all \(i\). Then, for any \(C_{\mathrm{poly}}>0\), with probability at least \(1-1/T^{C_{\mathrm{poly}}}\), we have_

Footnote 9: Note that DGP of Example 1 or 2 implies the existence of the error function of Assumption 2.

\[\frac{|\widetilde{\lambda}_{k}^{(i)}-\lambda_{k}^{(i)}|}{\lambda_{k}^{(i)}}=o(1),\]

_as \(T\to\infty\) for all \(k\in[\tau]\). Moreover, it implies \(|\beta_{T}-\widehat{\beta}_{T}|=o(1)\) and_

\[\max\left\{\frac{\widetilde{B}_{N,T}^{(i)}}{\widetilde{B}_{N,T}^{(i)}},\frac{ \widetilde{V}_{N,T}^{(i)}}{V_{N,T}^{(i)}}\right\}=T^{o(1)}.\] (11)

Eq. (11) states that the estimated rate of error is accurate as \(T\to\infty\). To see this, for Example 1 in Proposition 1, the estimated error rate is \(T^{o(1)}(\sqrt{T^{a}/N+T^{-a}})\), whose exponent approaches to \(\sqrt{T^{a}/N+T^{-a}}\) as \(T\to\infty\).

### Adaptive Explore-then-Commit (AEtC)

The Adaptive Explore-then-Commit (AEtC) algorithm is described in Algorithm 2. Unlike EtC, the amount of exploration in AEtC is adaptively determined based on the stopping condition:

\[\mathrm{Stop}^{(i)}(N)=\left\{N>C_{T}\operatorname{tr}(\widetilde{\Sigma}^{(i )})\cap NK\geq T\sqrt{\widehat{B}_{N,T}^{(i)}+\widehat{V}_{N,T}^{(i)}}\right\},\]

and \(\mathrm{Stop}=\cap_{i\in[K]}\mathrm{Stop}^{(i)}\), where \(C_{T}=C_{T}(T)\) is a function of \(T\) that slowly diverges to \(+\infty\). The first condition \(N>C_{T}\operatorname{tr}(\widetilde{\Sigma}^{(i)})\) ensures that \(N\) is large enough to have consistency on the estimators, whereas the second condition \(NK\geq T\sqrt{\widehat{B}_{N,T}^{(i)}+\widehat{V}_{N,T}^{(i)}}\) balances the exploration and exploitation. The following theorem bounds the regret of AEtC.

**Theorem 9**.: _Suppose Assumption 1. Suppose DGP of Example 1 or Example 2 in Proposition 1. Then, for any \(c>0\), the regret (2) when we run the AEtC algorithm (Algorithm 2) with a sufficiently slowly diverging \(C_{T}\) is bounded as follows as \(T\to\infty\):_

\[R(T)=\widetilde{O}(L(T,K)T^{c}),\]

_where \(L(T,K)\) is the function defined in Theorem 4._Theorem 9 states that, we can choose arbitrarily small \(c>0\) and the exponent of the regret of AEtC approaches that of EtC (Theorem 4). Note that the condition \(N\geq C_{T}\text{tr}(\tilde{\Sigma}^{(i)})\) should not dominate the balance between exploration and exploitation: For example, \(C_{T}=C\log T\) for any \(C>0\) suffices. In practice, a constant \(C_{T}\) works.

Proof sketch of Theorem 9.: We can derive that Eqs. (9), (10), and (11) for all \(N\in[T]\) that hold with probability at least \(1-O(1/T)\) by setting \(C_{\text{poly}}=2\) and taking a union bound over \(N\), and thus

\[\bigcap_{i\in[K]}\bigcap_{N\geq C_{T}\text{trtr}(\tilde{\Sigma}^{(i)})}\left\{ \log\left(\frac{\sqrt{\tilde{B}_{N,T}^{(i)}+\widehat{V}_{N,T}^{(i)}}}{\sqrt{B _{N,T}^{(i)}+V_{N,T}^{(i)}}}\right)=\log(T^{o(1)})\right\}\] (12)

holds. Under this event, the stopping time of AEtC and EtC are at most \(T^{o(1)}\) times different. Given this, the proof of the theorem is easily modified from the proof of Theorem 4. 

## 6 Simulation

We consider two setups: \((K,p,T)=(2,200,500)\) and \((K,p,T)=(10,200,1000)\). For each setup, we consider a covariance matrix \(\Sigma^{(i)}=c^{(i)}\tilde{\Sigma}\) where \(c^{(i)}\sim\text{Uni}\left([0.5,1.5]\right)\) and a base covariance \(\tilde{\Sigma}\) for each \(i\in[K]\), which represents a heterogeneous covariance among the arms. The base covariance \(\tilde{\Sigma}\) follows the following configurations: DGP 1: \(\tilde{\Sigma}=\text{diag}(\lambda_{1},...,\lambda_{p}),\lambda_{k}=k^{-0.5}\), DGP 2: \(\tilde{\Sigma}=\text{diag}(\lambda_{1},...,\lambda_{p}),\lambda_{k}=\exp(-k)+T \exp(-T)/p\), DGP 3: \(\tilde{\Sigma}=\text{diag}(\lambda_{1},...,\lambda_{p}),\lambda_{k}=k^{-1+1/T}\), and DGP 4: \(\tilde{\Sigma}_{i,j}=0.3\) and \(\tilde{\Sigma}_{i,i}=0.7\) for \(j\neq i\in[p]\). Note that the DGPs 1 and 3 correspond to Examples 2 and 1 in Proposition 1. DGP 3 is benign as well, while DGP 4 is not. We generate \(X^{(i)}(t)\) from a centered \(p\)-dimensional Gaussian with covariance \(\Sigma^{(i)}\), \(\theta^{(i)}\) from a standard normal distribution which yields non-sparse parameter vectors, and \(Y^{(i)}(t)\) by the linear model (1) with the noise with variance \(\sigma^{2}=0.01\). We compare proposal (AEtC, \(C_{T}=2\)) to ESTC (Hao et al., 2020), LinUCB (Li et al., 2010; Abbasi-Yadkori et al., 2011), and DR Lasso Bandit (Kim and Paik, 2019).

Figure 2 illustrates the accumulated regret \(R(t)\) in relation to the round \(t\in[T]\). In the first three benign DGPs, the AEtC consistently outperforms the other methods. In DGP 2, the advantage of AEtC is insignificant because the regret of any method is already small. This is because in DGP 2, a model with exponentially decaying behavior, only a small fraction of eigenvalues are important. However, AEtC performs significantly better than its competitors in DGP 1 and DGP 3, where the eigenvalues have a heavy-tail distribution. In a non-benign example of DGP 4, the proposed method is still comparable to LinUCB, which demonstrates the utility of an interpolating estimator.

Figure 2: Comparison of methods on four DGPs. Smaller regret indicates better performance. The solid lines are the mean of 10 repetitions and the bands represent the standard deviation.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abe and Long (1999) Naoki Abe and Philip M. Long. Associative reinforcement learning using linear probabilistic concepts. In _Proceedings of the Sixteenth International Conference on Machine Learning_, ICML, page 3-11. Morgan Kaufmann Publishers Inc., 1999.
* Agarwal et al. (2012) Deepak K. Agarwal, Bee-Chung Chen, Pradheep Elango, and Xuanhui Wang. Personalized click shaping through lagrangian duality for online recommendation. In _SIGIR '12_, 2012.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Mach. Learn._, 47(2-3):235-256, 2002. doi: 10.1023/A:1013689704352.
* Bartlett et al. (2020) Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020.
* Bastani and Bayati (2020) Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. _Oper. Res._, 68(1):276-294, 2020.
* Bosq (2000) Denis Bosq. _Linear processes in function spaces: theory and applications_, volume 149. Springer Science & Business Media, 2000.
* Chakraborty and Murphy (2014) Bibhas Chakraborty and Susan A. Murphy. Dynamic treatment regimes. _Annual Review of Statistics and Its Application_, 1(1):447-464, 2014.
* Chu et al. (2011) Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Geoffrey Gordon, David Dunson, and Miroslav Dudik, editors, _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15 of _Proceedings of Machine Learning Research_, pages 208-214. PMLR, 11-13 Apr 2011.
* Foster et al. (2019) Dylan J. Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. In _Advances in Neural Information Processing Systems 32_, pages 14714-14725, 2019.
* Hao et al. (2020) Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. _Advances in Neural Information Processing Systems_, 33:10753-10763, 2020.
* Hara and Maehara (2017) Satoshi Hara and Takanori Maehara. Enumerate lasso solutions for feature selection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* Jang et al. (2022) Kyoungseok Jang, Chicheng Zhang, and Kwang-Sung Jun. Popart: Efficient sparse regression and experimental design for optimal sparse linear bandits. In _NeurIPS_, 2022.
* Kim and Paik (2019) Gi-Soo Kim and Myunghee Cho Paik. Doubly-robust lasso bandit. _Advances in Neural Information Processing Systems_, 32, 2019.
* Koltchinskii and Lounici (2017) Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample covariance operators. _Bernoulli_, 23(1):110-133, 2017.
* Lai and Robbins (1985) T.L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. _Advances in Applied Mathematics_, 6(1):4-22, 1985. ISSN 0196-8858.
* Li et al. (2010) Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th International Conference on World Wide Web_, WWW '10, page 661-670. Association for Computing Machinery, 2010.
* Li et al. (2022) Wenjie Li, Adarsh Barik, and Jean Honorio. A simple unified framework for high dimensional bandit problems. In _International Conference on Machine Learning_, pages 12619-12655. PMLR, 2022.
* Miolane and Montanari (2021) Leo Miolane and Andrea Montanari. The distribution of the lasso: Uniform control over sparse balls and adaptive parameter tuning. _The Annals of Statistics_, 49(4):2313-2335, 2021.
* Miolane et al. (2019)Shogo Nakakita and Masaaki Imaizumi. Benign overfitting in time series linear model with over-parameterization. _arXiv preprint arXiv:2204.08369_, 2022.
* Oh et al. (2021) Min-hwan Oh, Garud Iyengar, and Assaf Zeevi. Sparsity-agnostic lasso bandit. In _International Conference on Machine Learning_, pages 8271-8280. PMLR, 2021.
* Rendle (2010) Steffen Rendle. Factorization machines. In Geoffrey I. Webb, Bing Liu, Chengqi Zhang, Dimitrios Gunopulos, and Xindong Wu, editors, _ICDM 2010, The 10th IEEE International Conference on Data Mining, Sydney, Australia, 14-17 December 2010_, pages 995-1000. IEEE Computer Society, 2010.
* 535, 1952.
* Tang et al. (2013) Liang Tang, Romer Rosales, Ajit Singh, and Deepak Agarwal. Automatic ad format selection via contextual bandits. In _22nd ACM International Conference on Information and Knowledge Management, CIKM'13_, pages 1587-1594. ACM, 2013.
* Tsigler and Bartlett (2020) Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. _arXiv preprint arXiv:2009.14286_, 2020.
* Van De Geer and Buhlmann (2009) Sara A Van De Geer and Peter Buhlmann. On the conditions used to prove oracle results for the lasso. _Electronic Journal of Statistics_, 3:1360-1392, 2009.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wang et al. (2018) Xue Wang, Mingcheng Wei, and Tao Yao. Minimax concave penalized multi-armed bandit model with high-dimensional covariates. In _International Conference on Machine Learning_, pages 5200-5208. PMLR, 2018.
* Wang et al. (2022) Yuyan Wang, Long Tao, and Xian Xing Zhang. Recommending for a multi-sided marketplace with heterogeneous contents. In _Proceedings of the 16th ACM Conference on Recommender Systems_, RecSys '22, page 456-459. Association for Computing Machinery, 2022.