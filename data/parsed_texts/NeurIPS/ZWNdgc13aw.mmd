# NeoRL: Efficient Exploration for Nonepisodic RL

Bhavya Sukhija, Lenart Treven, Florian Dorfler, Stelian Coros, Andreas Krause

ETH Zurich, Switzerland

Correspondence to sukhijab@ethz.ch

###### Abstract

We study the problem of nonepisodic reinforcement learning (RL) for nonlinear dynamical systems, where the system dynamics are unknown and the RL agent has to learn from a single trajectory, i.e., adapt online and without resets. This setting is ubiquitous in the real world, where resetting is impossible or requires human intervention. We propose _Nonepisodic Optimistic RL_ (NeoRL), an approach based on the principle of optimism in the face of uncertainty. NeoRL uses well-calibrated probabilistic models and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics. Under continuity and bounded energy assumptions on the system, we provide a first-of-its-kind regret bound of \(\mathcal{O}(\beta_{T}\sqrt{T\Gamma_{T}})\) for general nonlinear systems with Gaussian process dynamics. We compare NeoRL to other baselines on several deep RL environments and empirically demonstrate that NeoRL achieves the optimal average cost while incurring the least regret.

## 1 Introduction

In recent years, data-driven control approaches, such as reinforcement learning (RL), have demonstrated remarkable achievements. However, most RL algorithms are devised for an episodic setting, where during each episode, the agent interacts in the environment for a predetermined episode length or until a termination condition is met. After the episode, the agent is reset back to an initial state from where the next episode commences. Episodes prevent the system from blowing up, i.e., maintain stability, while also restricting exploration to states that are relevant to the task at hand. Moreover, resets ensure that the agent explores close to the initial states and does not end up at undesirable parts of the state space that exhibit low reward. In simulation, resetting is typically straightforward. However, if we wish to enable agents to learn and adapt by interacting online with the real world, resets are often prohibitive since they typically involve manual intervention. Instead, agents should be able to learn autonomously (Sharma et al., 2021) i.e., from a single trajectory. This problem is extensively studied in adaptive control (Astrom and Wittenmark, 2013), where classical works focus on controller design (Lai and Wei, 1982, 1987; Krstic et al., 1992, 1995; Annaswamy, 2023) and not on the exploration/learning aspect of the problem. Only a few works consider these two aspects jointly (Abbasi-Yadkori and Szepesvari, 2011; Cohen et al., 2019; Dean et al., 2020; Simchowitz and Foster, 2020; Zhao et al., 2024). However, these works study linear systems with quadratic costs, i.e., the LQR setting. While several works in the Deep RL community have also studied this problem, (c.f., Section 5), the theoretical results for this setting are fairly limited. In particular, theoretical results mostly exist for the finite state and action spaces (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Jaksch et al., 2010) and the extension to nonlinear systems with continuous spaces is much less understood. In our work, we address this gap and propose a practical RL algorithm that is grounded in theory. In particular, we make the following contributions.

**Contributions**

1. We propose, NeoRL, a novel model-based RL algorithm based on the principle of optimism in the face of uncertainty. NeoRL operates in a nonepisodic setting and picks average cost optimal policies optimistically w.r.t. to the model's epistemic uncertainty.

2. We show that when the dynamics lies in a reproducing kernel Hilbert space (RKHS) of kernel \(k\), NeoRL exhibits a regret of \(\mathcal{O}(\beta_{T}\sqrt{\mathrm{T}\Gamma_{T}})\), where the regret, akin to prior work, is measured w.r.t to the optimal average cost under known dynamics, \(T\) is the number of environment steps, \(\beta_{T}\) the calibration coefficient (Chowdhury and Gopalan, 2017; Srinivas et al., 2012) and \(\Gamma_{T}\) the maximum information gain of kernel \(k\)(Srinivas et al., 2012). Our regret bound is similar to the ones obtained in the episodic setting (Kakade et al., 2020; Curi et al., 2020; Sukhjia et al., 2024; Treven et al., 2024) and Gaussian process (GP) bandit optimization (Srinivas et al., 2012; Chowdhury and Gopalan, 2017; Scarlett et al., 2017) and is sublinear for common kernel such as the exponential kernel. To the best of our knowledge, we are the first to obtain regret bounds for the setting.
3. We evaluate NeoRL on several RL benchmarks against common model-based RL baselines. Our experimental results demonstrate that NeoRL consistently achieves sublinear regret, also when neural networks are employed instead of GPs for modeling dynamics. Moreover, in all our experiments, NeoRL converges to the optimal average cost.

## 2 Problem Setting

We consider a discrete-time dynamical system with running costs \(c\).

\[\bm{x}_{t+1}=\bm{f}^{*}(\bm{x}_{t},\bm{u}_{t})+\bm{w}_{t}, (\bm{x}_{t},\bm{u}_{t})\in\mathcal{X}\times\mathcal{U},\ \bm{x}(0)=\bm{x}_{0}\] (1) \[c(\bm{x},\bm{u})\in\mathbb{R}_{\geq 0}\] (Running cost)

Here \(\bm{x}_{t}\in\mathcal{X}\subseteq\mathbb{R}^{d_{\bm{u}}}\) is the state, \(\bm{u}_{t}\in\mathcal{U}\subseteq\mathbb{R}^{d_{\bm{u}}}\) the control input, and \(\bm{w}_{t}\in\mathcal{W}\subseteq\mathbb{R}^{\bm{w}}\) the process noise. The dynamics \(\bm{f}^{*}\) are unknown and the cost \(c\) is assumed to be known.

TaskIn this work, we study the average cost RL problem (Puterman, 2014), i.e., we want to learn the solution to the following minimization problem

\[A(\bm{\pi}^{*},\bm{x}_{0})=\min_{\bm{\pi}\in\Pi}A(\bm{\pi},\bm{ x}_{0})=\min_{\bm{\pi}\in\Pi}\limsup_{T\to\infty}\frac{1}{T}\mathbb{E}_{\bm{ \pi}}\left[\sum_{t=0}^{T-1}c(\bm{x}_{t},\bm{u}_{t})\right].\] (2)

Moreover, we consider the nonepisodic RL setting where the system starts at an initial state \(\bm{x}_{0}\in\mathcal{X}\) but never resets back during learning, that is, we seek to learn online from a single trajectory. After each step \(t\) in the environment, the RL system receives a transition tuple \((\bm{x}_{t},\bm{u}_{t},\bm{x}_{t+1})\) and updates its policy based on the data \(\mathcal{D}_{t}\) collected thus far during learning. The average cost formulation is common for the nonepisodic setting (Jaksch et al., 2010; Abbasi-Yadkori and Szepesvari, 2011; Cohen et al., 2019; Dean et al., 2020; Simchowitz and Foster, 2020), and the cumulative regret for the learning algorithm in this case is defined as

\[R_{T}=\sum_{t=0}^{T-1}\mathbb{E}_{\bm{x}_{t},\bm{u}_{t}|\bm{x}_{0}}[c(\bm{x}_{ t},\bm{u}_{t})-A(\bm{\pi}^{*},\bm{x}_{0})].\] (3)

Studying the average cost criterion for general continuous state-action spaces is challenging even when the dynamics are known, since the average cost exists only for special classes of nonlinear systems (Arapostathis et al., 1993). In the following, we impose assumptions on the dynamics and policy class \(\Pi\) that enable our theoretical analysis.

### Assumptions

Imposing continuity on \(\bm{f}^{*}\) is quite common in the control theory (Khalil, 2015) and reinforcement learning literature (Curi et al., 2020; Sussex et al., 2023; Sukhjia et al., 2024). To this end, for our analysis, we make the following assumption.

**Assumption 2.1** (Continuity of \(\bm{f}^{*}\) and \(\bm{\pi}\)).: The dynamics model \(\bm{f}^{*}\) and all \(\bm{\pi}\in\Pi\) are continuous.

Next, we make an assumption on the system's stochastic disturbances.

**Assumption 2.2** (Process noise distribution).: The process noise is i.i.d. Gaussian with variance \(\sigma^{2}\), i.e., \(\bm{w}_{t}\overset{i.i.d}{\sim}\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\).

Our analysis can be extended for the more general heteroscedastic case, where \(\sigma\) depends on \(\bm{x}\). However, for simplicity, we focus on the homoscedastic setting. In the following, we make assumptions on our policy class. To this end, we first introduce the class of \(\mathcal{K}_{\infty}\) functions.

**Definition 2.3** (\(\mathcal{K}_{\infty}\)-functions).: The function \(\xi:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}\) is of class \(\mathcal{K}_{\infty}\), if it is continuous, strictly increasing, \(\xi(0)=0\) and \(\xi(s)\to\infty\) for \(s\to\infty\).

**Assumption 2.4** (Policies with bounded energy).: We assume there exists \(\kappa,\xi\in\mathcal{K}_{\infty}\), positive constants \(K,C_{u},C_{l}\) with \(C_{u}>C_{l}\), and \(\gamma\in(0,1)\) such that for each \(\bm{\pi}\in\Pi\) we have,

_Bounded energy:_ There exists a Lyapunov function \(V^{\bm{\pi}}:\mathcal{X}\to[0,\infty)\) for which \(\forall\bm{x},\bm{x}^{\prime}\in\mathcal{X}\),

\[\begin{split}|V^{\bm{\pi}}(\bm{x})-V^{\bm{\pi}}(\bm{x}^{\prime})|& \leq\kappa(\|\bm{x}-\bm{x}^{\prime}\|)&\text{(uniform continuity)}\\ C_{l}\xi(\|\bm{x}\|)&\leq V^{\bm{\pi}}(\bm{x}) \leq C_{u}\xi(\|\bm{x}\|)&\text{(positive definiteness)}\\ \mathbb{E}_{\bm{x}_{+}|\bm{x},\bm{\pi}}[V^{\bm{\pi}}(\bm{x}_{+})]& \leq\gamma V^{\bm{\pi}}(\bm{x})+K&\text{(drift condition)}\end{split}\]

where \(\bm{x}_{+}=\bm{f}^{*}(\bm{x},\bm{\pi}(\bm{x}))+\bm{w}\).

_Bounded norm of cost:_

\[\sup_{\bm{x}\in\mathcal{X}}\frac{c(\bm{x},\bm{\pi}(\bm{x}))}{1+V^{\bm{\pi}}( \bm{x})}<\infty\]

_Boundedness of the noise with respect to \(\kappa\):_

\[\mathbb{E}_{\bm{w}}\left[\kappa(\|\bm{w}\|)\right]<\infty,\ \mathbb{E}_{\bm{w}} \left[\kappa^{2}(\|\bm{w}\|)\right]<\infty\]

The drift condition states that the energy between two timesteps can increase at most by \(K\). In particular, the Lyapunov function \(V^{\bm{\pi}}\) can be viewed as an energy function for the dynamical system, and the bounded energy condition above ensures that the system is not "blowing up". We do not perceive this as restrictive for real-world engineered systems. Other works that study learning nonlinear dynamics (Foster et al., 2020; Sattar and Oymak, 2022; Lale et al., 2021) in the nonepisodic setting also make stability assumptions such as global exponential stability for their analysis. In similar spirit, we make the bounded energy assumption for our policy class. The drift condition on the Lyapunov function is also used to study the ergodicity of Markov chains for continuous state spaces (Meyn and Tweedie, 2012; Hairer and Mattingly, 2011), which is crucial for our analysis of the infinite horizon behavior of the system. Moreover, for a very rich class of problems, the drift condition is satisfied. We highlight this in the corollary below.

**Lemma 2.5**.: _Assume \(\bm{f}^{*}\) is uniformly continuous and for all \(\bm{\pi}\in\Pi\), \(\bm{x}\in\mathcal{X}\), \(\|\bm{\pi}(\bm{x})\|\leq u_{\max}\). Further assume, there exists \(\bm{\pi}_{s}\in\Pi\) such that we have constants \(K,C_{u},C_{l}\) with \(C_{u}>C_{l}\), \(\gamma\in(0,1)\), \(\kappa,\alpha\in\mathcal{K}_{\infty}\) and a Lyapunov function \(V:\mathcal{X}\to[0,\infty)\) for which \(\forall\bm{x},\bm{x}^{\prime}\in\mathcal{X}\),_

\[\begin{split}|V(\bm{x})-V(\bm{x}^{\prime})|& \leq\kappa(\|\bm{x}-\bm{x}^{\prime}\|)\\ C_{l}\xi(\|\bm{x}\|)&\leq V(\bm{x})\leq C_{u}\xi( \|\bm{x}\|)\\ \mathbb{E}_{\bm{x}_{+}|\bm{x},\bm{\pi}_{s}}[V(\bm{x}_{+})]& \leq\gamma V(\bm{x})+K,\end{split}\]

_where \(\bm{x}_{+}=\bm{f}^{*}(\bm{x},\bm{\pi}(\bm{x}))+\bm{w}\). Then, \(V\) also satisfies the drift condition for all \(\bm{\pi}\in\Pi\), i.e., is a Lyapunov function for all policies._

We prove this lemma in Appendix A. Intuitively, if the inputs are bounded, the energy inserted into the system by another policy is also bounded. Nearly all real-world systems have bounded inputs due to the physical limitations of actuators. For these systems, it suffices if only one policy in \(\Pi\) satisfies the drift condition.

The boundedness assumptions for the cost and the noise in Assumption 2.4 are satisfied for a rich class of cost and \(\mathcal{K}_{\infty}\) functions.

Under these assumptions, we can show the existence of the average cost solution.

**Theorem 2.6** (Existence of Average Cost Solution).: _Let Assumption 2.1 - 2.4 hold. Consider any \(\bm{\pi}\in\Pi\) and let \(P^{\bm{\pi}}\) denote its transition kernel, i.e., \(P^{\bm{\pi}}(\bm{x},\mathcal{A})=\mathbb{P}(\bm{x}_{+}\in\mathcal{A}|\bm{x}, \bm{\pi}(\bm{x}))\) for \(\mathcal{A}\subseteq\mathcal{X}\). Then \(P^{\bm{\pi}}\) admits a unique invariant measure \(\widetilde{P}^{\bm{\pi}}\), and there exists \(C_{2},C_{3}\in(0,\infty)\), \(\lambda\in(0,1)\) such that_

Average Cost;

\[A(\bm{\pi})=\lim_{T\to\infty}\frac{1}{T}\mathbb{E}_{\bm{\pi}}\left[\sum_{t=0}^{ T-1}c(\bm{x}_{t},\bm{u}_{t})\right]=\mathbb{E}_{\bm{x}\sim\widetilde{P}^{\bm{\pi}}} \left[c(\bm{x},\bm{\pi}(x))\right]\]Bias Cost; _Letting \(B(\bm{\pi},\bm{x}_{0})=\lim_{T\to\infty}\mathbb{E}_{\bm{\pi}}\left[\sum_{t=0}^{T-1 }c(\bm{x}_{t},\bm{u}_{t})-A(\bm{\pi})\right]\) denote the bias, we have_

\[|B(\bm{\pi},\bm{x}_{0})|\leq C_{2}(1+V^{\bm{\pi}}(\bm{x}_{0}))\frac{1}{1-\lambda}\]

_for all \(\bm{x}_{0}\in\mathcal{X}\)._

Theorem 2.6 is a crucial result for our analysis since it implies that the average cost is bounded and _independent of the initial state_\(\bm{x}_{0}\). Furthermore, it also shows that the bias is bounded. The average cost criterion satisfies the following Bellman equation (Puterman, 2014) below

\[B(\bm{\pi},\bm{x})+A(\bm{\pi})=c(\bm{x},\bm{\pi}(\bm{x}))+\mathbb{E}_{\bm{x}_{ +}}[B(\bm{\pi},\bm{x}_{+})|\bm{x},\bm{\pi}]\] (4)

Accordingly, the bias term plays an important role in the regret analysis (also notice its similarity to our regret term in Equation (3)).

Thus far, we have only made assumptions that make the average cost problem tractable. In the following, we make an assumption on the dynamics that allow us to learn it from data. Moreover, we assume that at each step \(n\) we learn a mean estimate \(\bm{\mu}_{n}\) of \(\bm{f}^{*}\) and can quantify our uncertainty \(\bm{\sigma}_{n}\) over the estimate. More formally, we learn a well-calibrated statistical model of \(\bm{f}^{*}\) as defined below.

**Definition 2.7** (Well-calibrated statistical model of \(\bm{f}^{*}\), Rothfuss et al. (2023)).: Let \(\mathcal{Z}\stackrel{{\mathrm{def}}}{{=}}\mathcal{X}\times\mathcal{U}\). An all-time well-calibrated statistical model of the function \(\bm{f}^{*}\) is a sequence \(\{\mathcal{M}_{n}(\delta)\}_{n\geq 0}\), where

\[\mathcal{M}_{n}(\delta)\stackrel{{\mathrm{def}}}{{=}}\left\{ \bm{f}:\mathcal{Z}\to\mathbb{R}^{d_{x}}\mid\forall\bm{z}\in\mathcal{Z},\forall j \in 1,\ldots,d_{x}:|\mu_{n,j}(\bm{z})-f_{j}(\bm{z})|\leq\beta_{n}(\delta)\sigma_{n, j}(\bm{z})\right\},\]

if, with probability at least \(1-\delta\), we have \(\bm{f}^{*}\in\bigcap_{n\geq 0}\mathcal{M}_{n}(\delta)\). Here, \(f_{j}\), \(\mu_{n,j}\) and \(\sigma_{n,j}\) denote the \(j\)-th element in the vector-valued functions \(\bm{f}\), \(\bm{\mu}_{n}\) and \(\bm{\sigma}_{n}\) respectively, and \(\beta_{n}(\delta)\in\mathbb{R}_{\geq 0}\) is a scalar function that depends on the confidence level \(\delta\in(0,1]\) and which is monotonically increasing in \(n\).

Next, we assume that \(\bm{f}^{*}\) resides in a Reproducing Kernel Hilbert Space (RKHS) of vector-valued functions and show that this is sufficient for us to obtain a well-calibrated model.

**Assumption 2.8**.: We assume that the functions \(f_{j}^{*}\), \(j\in 1,\ldots,d_{x}\) lie in a RKHS with kernel \(k\) and have a bounded norm \(B\), that is \(\bm{f}^{*}\in\mathcal{H}_{k,B}^{d_{x}}\), with \(\mathcal{H}_{k,B}^{d_{x}}=\left\{\bm{f}\mid\left\|f_{j}\right\|_{k}\leq B,j=1, \ldots,d_{x}\right\}\). Moreover, we assume that \(k(\bm{x},\bm{x})\leq\sigma_{\max}\) for all \(\bm{x}\in\mathcal{X}\).

Assumption 2.8 allows us to model \(\bm{f}^{*}\) with GPs for which the mean and epistemic uncertainty (\(\bm{\mu}_{n}(\bm{z})=[\mu_{n,j}(\bm{z})]_{j\leq d_{x}}\), and \(\bm{\sigma}_{n}(\bm{z})=[\sigma_{n,j}(\bm{z})]_{j\leq d_{x}}\)) have an analytical formula

\[\mu_{n,j}(\bm{z}) =\bm{k}_{n}^{\top}(\bm{z})(\bm{K}_{n}+\sigma^{2}\bm{I})^{-1}\bm{y }_{1:n}^{j},\] (5) \[\sigma_{n,j}^{2}(\bm{z}) =k(\bm{x},\bm{x})-\bm{k}_{n}^{\top}(\bm{z})(\bm{K}_{n}+\sigma^{2} \bm{I})^{-1}\bm{k}_{n}(\bm{x}),\]

Here, \(\bm{y}_{1:n}^{j}\) corresponds to the noisy measurements of \(f_{j}^{*}\), i.e., the observed next state from the transitions dataset \(\mathcal{D}_{1:n}\), \(\bm{k}_{n}=[k(\bm{z},\bm{z}_{i})]_{i\leq nT}\), \(\bm{z}_{i}\in\mathcal{D}_{1:n}\), and \(\bm{K}_{n}=[k(\bm{z}_{i},\bm{z}_{l})]_{i,l\leq nT}\), \(\bm{z}_{i},\bm{z}_{l}\in\mathcal{D}_{1:n}\) is the data kernel matrix. The restriction on the kernel \(k(\bm{x},\bm{x})\leq\sigma_{\max}\) implies boundedness of \(\bm{f}^{*}\) and has also appeared in works studying the episodic setting for nonlinear systems (Mania et al., 2020; Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Wagenmaker et al., 2023). We can also define \(\bm{f}^{*}\) such that \(\bm{x}_{k}=\bm{x}_{k-1}+\bm{f}^{*}(\bm{x}_{k-1},\bm{u}_{k-1})+\bm{w}_{k-1}\) in which case the boundedness of \(\bm{f}^{*}\) captures many real-world systems.

**Lemma 2.9** (Well calibrated confidence intervals for RKHS, Rothfuss et al. (2023)).: _Let \(\bm{f}^{*}\in\mathcal{H}_{k,B}^{d_{x}}\). Suppose \(\bm{\mu}_{n}\) and \(\bm{\sigma}_{n}\) are the posterior mean and variance of a GP with kernel \(k\), c.f., Equation (5). There exists \(\beta_{n}(\delta)\propto\sqrt{\Gamma_{n}}\), for which the tuple \((\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n}(\delta))\) is a well-calibrated statistical model of \(\bm{f}^{*}\)._

In summary, in the RKHS setting, a GP is a well-calibrated model. For more general models like Bayesian neural networks (BNNs), methods such as Kuleshov et al. (2018) can be used for calibration. Our results can also be extended beyond the RKHS setting to other classes of well-calibrated models similar to Curi et al. (2020).

## 3 NeoRL

In the following, we present our algorithm: **N**onepisodic **O**ptimistic **RL** (NeoRL) for efficient nonepisodic exploration in continuous state-action spaces. NeoRL builds on recent advances in episodic RL (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Treven et al., 2024) and leverages the optimism in the face of uncertainty paradigm to pick policies that are optimistic w.r.t. the dynamics within our calibrated statistical model as follows

\[(\bm{\pi}_{n},\bm{f}_{n})\operatorname*{\stackrel{{ \mathrm{def}}}{{=}}}\operatorname*{\arg\min}_{\bm{\pi}\in\Pi,\,\bm{f}\in \mathcal{M}_{n-1}\cap\mathcal{M}_{0}}A(\bm{\pi},\bm{f}).\] (6)

Here, \(\bm{f}_{n}\) is a dynamical system such that the cost by controlling \(\bm{f}_{n}\) with its optimal policy \(\bm{\pi}_{n}\) is the lowest among all the plausible systems from \(\mathcal{M}_{n-1}\cap\mathcal{M}_{0}\). Note, from Lemma 2.9 we have that \(\bm{f}^{*}\in\mathcal{M}_{n-1}\cap\mathcal{M}_{0}\) (with high probability) and therefore the solution to Equation (6) gives an optimistic estimate for the average cost. We take the intersection of \(\mathcal{M}_{n-1}\) with \(\mathcal{M}_{0}\) to ensure that we maintain at least the same confidence about our model as at the beginning, i.e., \(n=0\), during learning. NeoRL proceeds in the following manner. Similar to Jaksch et al. (2010), we bin the total time \(T\) the agent spends interacting in the environment into \(N\) "artificial" episodes. At each episode, we pick a policy according to Equation (6) and roll it out for \(H_{n}\) steps on the system. Next, we use the data collected during the rollout to update our statistical model. Finally, we double the horizon \(H_{n+1}=2H_{n}\), akin to Simchowitz and Foster (2020), and continue to the next episode _without resetting_ the system back to the initial state \(\bm{x}_{0}\). Intuitively, in the beginning, when our model estimate is not accurate, we update our model more frequently, and with more episodes as our model gets better we reduce the frequency of updates. The algorithm is summarized in Algorithm 1.

``` Init: Aleatoric uncertainty \(\sigma\), Probability \(\delta\), Statistical model \((\bm{\mu}_{0},\bm{\sigma}_{0},\beta_{0}(\delta))\), \(H_{0}\) for\(n=1,\dots,N\)do \(\bm{\pi}_{n}=\operatorname*{\arg\min}_{\bm{\pi}\in\Pi}\,\min_{\bm{f}\in \mathcal{M}_{n-1}\cap\mathcal{M}_{0}}A(\bm{\pi},\bm{f})\) \(H_{n}=2H_{n-1}\) \(\mathcal{D}_{n}\leftarrow\textsc{Rollout}(\bm{\pi}_{n})\) \(\triangleright\) Select measurements for horizon \(H_{n}\)  Update \((\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n})\leftarrow\mathcal{D}_{n}\) \(\triangleright\) Update statistical model \(\mathcal{M}_{n}\) endfor ```

**Algorithm 1**NeoRL:**N**onepisodic Optimistic **RL**

### Theoretical Results

In the following, we study the theoretical properties for NeoRL and provide a first-of-its-kind bound on the cumulative regret for the average cost criterion for general nonlinear dynamical systems. Our bound depends on the _maximum information gain_ of kernel \(k\)(Srinivas et al., 2012), defined as

\[\Gamma_{T}(k)=\max_{\mathcal{A}\subset\mathcal{X}\times\mathcal{U};|\mathcal{A} |\leq T}\frac{1}{2}\log\left|\bm{I}+\sigma^{-2}\bm{K}_{T}\right|.\]

\(\Gamma_{T}\) represents the complexity of learning \(\bm{f}^{*}\) from \(T\) data points and is sublinear for a very rich class of kernels (e.g., \(\mathcal{O}(\log^{d_{x}+d_{u}+1}(T))\) for the exponential (RBF) kernel, \(\mathcal{O}((d_{x}+d_{u})\log(T))\) for the linear kernel). In Appendix A, we report the dependence of \(\Gamma_{T}\) on \(T\) in Table 1.

**Theorem 3.1** (Cumulative Regret of NeoRL).: _Let Assumption 2.1 - 2.8 hold, and define \(H_{0}\) as the smallest integer such that_

\[H_{0}>\frac{\log\left(C_{u}/C_{t}\right)}{\log\left(1/\gamma\right)}.\]

_Then with probability at least \(1-\delta\), we have the following regret for NeoRL_

\[R_{T}\leq D_{4}(\bm{x}_{0},K,\gamma)\beta_{T}\sqrt{T\Gamma_{T}}+D_{5}(\bm{x}_{ 0},K,\gamma)\log_{2}\left(\frac{T}{H_{0}}+1\right).\] (7)

_with \(D_{4}(\bm{x}_{0},K,\gamma)\), \(D_{5}(\bm{x}_{0},K,\gamma)\) being bounded constants for bounded \(\|\bm{x}_{0}\|\), \(K\), and \(\gamma<1\)._From Lemma 2.9 we have that \(\beta_{T}\propto\sqrt{\Gamma_{T}}\) and therefore Theorem 3.1 gives sublinear regret for a rich class of RKHS functions. Moreover, it also gives a minimal horizon \(H_{0}\) that we need to maintain before switching to the next policy. Even for the linear case, fast switching between stable controllers can destabilize the closed-loop system. We ensure this does not happen in our case by having a minimal horizon of \(H_{0}\). Theorem 3.1 can also be derived beyond the RKHS setting for a more general class of well-calibrated models. In this case, the maximum information gain is replaced by the model complexity from Curi et al. (2020) (c.f., Curi et al. (2020); Sukhija et al. (2024) for further detail).

In the following, we give an intuitive proof sketch for Theorem 3.1. The detailed proof is provided in Appendix A.

Proof sketchThe proof can be split into three main steps. First, we show the ergodicity of the closed-loop system, a sufficient condition for showing the existence of the average cost and bias term, i.e., Theorem 2.6, for every policy \(\bm{\pi}\in\Pi\) under Assumption 2.1 - 2.4. For this, we use elementary results on Markov chains in measurable spaces from Meyn and Tweedie (2012); Hairer and Mattingly (2011). Second, we show that under Assumption 2.8, the optimistic system selected in Equation (6), retains the same properties as the true system \(\bm{f}^{*}\), e.g., stability, and therefore also is ergodic. Crucial to show this is that the true system \(\bm{f}^{*}\) and the optimistic system \(\bm{f}_{n}\) are at most \(\beta_{n}\bm{\sigma}_{n}\) apart. Finally, in the third step, we show that as we update our model and policy every \(H_{n}\) steps, the doubling of the horizon retains the system properties from above, and our accumulated model uncertainties across \(T\) environment steps grow with the rate \(\Gamma_{T}\). For the latter, we use the analysis from Kakade et al. (2020) for the episodic case, to bound the deviation between the optimistic average cost and the true average cost.

### Practical Modifications

For testing NeoRL, we make three modifications that simplify its deployment in practice in terms of implementation and computation time. First, instead of doubling the horizon \(H_{n}\) we pick a fixed horizon \(H\) during the experiment. This makes the planning and training of the agent easier. Next, we use a receding horizon controller, i.e., model predictive control (MPC) (Garcia et al., 1989), instead of directly optimizing for the average cost in Equation (6). MPC is widely used to obtain a feedback controller for the infinite horizon setting. Moreover, while for linear systems, the Riccati equations (Anderson and Moore, 2007) provide an analytical solution to Equation (2), no such solution exists for the nonlinear case and MPC is commonly used as an approximation. Further, under additional assumptions on the cost and dynamics, MPC also obtains a policy with bounded average cost, which is crucial for the nonepisodic case (c.f., Assumption 2.4). We use the iCEM optimizer for planning (Pinneri et al., 2021). Finally, instead of optimizing over \(\mathcal{M}_{n}\cap\mathcal{M}_{0}\), we optimize directly over \(\mathcal{M}_{n}\). This allows us to use the reparameterization trick from Curi et al. (2020) and obtain a simple and tractable optimization problem. In summary, for each step \(t\) in the environment, we solve the following optimization problem

\[\min_{\bm{u}_{0:H_{\text{MPC}^{-1}}},\bm{\eta}_{0:H_{\text{MPC}^ {-1}}}}\mathbb{E}\left[\sum_{h=0}^{H_{\text{MPC}^{-1}}}c(\hat{\bm{x}}_{h},\bm {u}_{h})\right],\] (8) \[\text{s.t.}\ \hat{\bm{x}}_{h+1}=\bm{\mu}_{n-1}(\hat{\bm{x}}_{h},\bm {u}_{h})+\beta_{n-1}(\delta)\bm{\sigma}_{n-1}(\hat{\bm{x}}_{h},\bm{u}_{h})\bm{ \eta}_{h}+\bm{w}_{h}\ \text{ and }\hat{\bm{x}}_{0}=\bm{x}_{t}.\]

Here \(H_{\text{MPC}}\) is the MPC horizon. We take the first input from the solution of the problem above, i.e., \(\bm{u}_{0}^{*}\), and execute this in the system. We then repeat this procedure for \(H\) steps and then update our statistical model \(\mathcal{M}_{n}\). The resulting optimization above considers a larger action space as it includes the hallucinated controls \(\bm{\eta}\) as additional input variables. The hallucinated controls are introduced through the reparameterization trick from (Curi et al., 2020) and are used to directly optimize over models in \(\bm{f}\in\mathcal{M}_{n-1}\). Moreover, the final algorithm can be seen as a natural extension to H-UCRL (Curi et al., 2020) for the nonepisodic setting. We summarize the algorithm in Appendix B Algorithm 2. Note while these modifications deviate from our theoretical analysis, empirically they work well for GP and BNN models, c.f., Section 4.

## 4 Experiments

We evaluate NeoRL on the Pendulum-v1 and MountainCar environment from the OpenAI gym benchmark suite (Brockman et al., 2016), Cartpole, Reacher, and Swimmer from the DeepMind control suite (Tassa et al., 2018), the racecar simulator from Kabzan et al. (2020), and a soft robotic arm from Teknialp et al. (2024). The swimmer and the soft robotic arm are fairly high-dimensional systems - the swimmer has a 28-dimensional state and 5-dimensional action space, and the soft arm is represented by a 58-dimensional state and has a 12-dimensional action space. All environments are never reset during learning. Moreover, the Pendulum-v1, MountainCar, CartPole, and Reacher environments operate within a bounded domain and thus inherently satisfy Assumption 2.4. The swimmer, racecar, and soft arm can operate in an unbounded domain but have a cost function that penalizes the distance between the system's state \(\bm{x}_{t}\) and a target state \(\bm{x}^{*}\). Therefore, the cost encourages the system to move towards the target and remain within a bounded domain.

**Baselines** In the episodic setting, resets can be used to control the exploration space for the agent. However, in the absence of resets, the agent can explore arbitrarily and end up in states that are irrelevant to the task at hand. Moreover, the agent has to follow an uninterrupted chain of experience, which makes the nonepisodic setting the most challenging one in RL (Kakade, 2003). Accordingly, there are only a few algorithms that consider this setting (c.f., Section 5). In this work, we focus on model-based RL (MBRL) algorithms due to their sample efficiency. In particular, we adopt common MBRL methods for our setting. MBRL algorithms typically differentiate in three ways; (\(t\)) propagating dynamics for planning (Chua et al., 2018; Osband and Van Roy, 2017; Kakade et al., 2020; Curi et al., 2020), (\(ii\)) representation of the dynamics model (Ha and Schmidhuber, 2018; Hafner et al., 2019; Kipf et al., 2019), and (\(iii\)) types of planners (Williams et al., 2017; Hafner et al., 2020; Pinneri et al., 2021). NeoRL is independent to the choice of representation or planners. Therefore, we focus on (\(i\)) and use probabilistic ensembles (Lakshminarayanan et al., 2017) and GPs for modeling our dynamics and MPC with iCEM (Pinneri et al., 2021) as the planner. Common techniques to propagate the dynamics for planning are using the mean, trajectory sampling (Chua et al., 2018), and Thompson

Figure 1: Average reward \(A(\bm{\pi})\) and cumulative regret \(R_{T}\) over ten different seeds for all environments. We report the mean performance with one standard error as shaded regions. During all experiments, the environment is never reset. For all baselines, we model the dynamics with probabilistic ensembles, except in the Pendulum-GP experiment, where GPs are used instead. NeoRL significantly outperforms all baselines and converges to the optimal average reward, \(A(\bm{\pi}^{*})=0\), showing sublinear cumulative regret \(R_{T}\) for all environments.

sampling (Osband and Van Roy, 2017). We adapt these three for our setting similar to as discussed in Section 3.2. For all experiments with probabilistic ensembles, we consider TS1 from Chua et al. (2018) for trajectory sampling, and for the GP experiment, we use distribution sampling from Chua et al. (2018). We call the three baselines NeMean (nonepisodic mean), NePETS (nonepisodic PETS), and NeTS (nonepisodic Thompson sampling). NeMean and NePETS are greedy w.r.t. the current estimate of the dynamics, i.e., do not explicitly encourage exploration. In our experiments, we show that being greedy does not suffice to converge to the optimal average cost, that is, obtain sublinear regret. The code for our experiments is available online.2

Footnote 2: https://github.com/lasgroup/opax/tree/neorl

Convergence to the optimal average costIn Figure 1 we report the normalized average cost and cumulative regret of NeoRL, NeMean, NePETS, and NeTS. The normalized average cost is defined such that \(A(\bm{\pi}^{*})=0\) for all environments. We observe that NeMean fails to converge to the optimal average cost for the Pendulum-v1 environment for both probabilistic ensembles and a GP model. It also fails to solve the MountainCar environment and is unstable for the Reacher and CartPole. In general, NeMean performs the worst among all methods. This is similar to the episodic case, where using the mean model often leads to the policy "overfitting" to the model inaccuracies (Chua et al., 2018). NePETS performs better than the mean, however still significantly worse than NeoRL. Even in the episodic setting, PETS tends to underexplore (Curi et al., 2020). We observe the same for the nonepisodic case, especially for the MountainCar task, which is a challenging RL environment with a sparse cost. Here NePETS is also not able to achieve the optimal average cost and thus does not have sublinear cumulative regret. NeTS performs similarly to NePETS and is also not able to solve the MountainCar task.

NeoRL performs the best among the baselines for all experiments and converges to the optimal average cost achieving sublinear cumulative regret using only \(\sim 10^{3}\) environment interactions. Moreover, this observation is consistent between different dynamics models (GPs and probabilistic ensembles) and environments. Even in environments that are unbounded, i.e., Swimmer, SoftArm, and RaceCar, we observe that NeoRL converges to the optimal average cost the fastest. We believe this is due to the feedback control from MPC, which has a stabilizing effect.

Calling reset when neededAll the experiments in Figure 1 considered the nonepisodic setting where the system was never reset during learning. A special case of our theoretical analysis is the class of policies \(\Pi\) that may call for a reset / "ask for help" whenever they end up in an undesirable part of the state space. In this setting, the system is typically restricted to a compact subset of the state space \(\mathcal{X}\), and the policy class satisfies Assumption 2.4. For many real-world applications, such a policy class can be derived. To simulate this experiment, we consider the CartPoleBalance task in Figure 2, where the goal is to balance the pole in the upright position. A reset is triggered whenever the pole drops. We again observe that NeoRL achieves the best performance, i.e., lowest cumulative regret and thus learns to solve the task the fastest. Moreover, it also requires fewer resets than NeMean, NePETS, and NeTS.

## 5 Related Work

Average cost RL for finite state-action spacesA significant amount of work studies the average cost/reward RL setting for finite-state action spaces. Moreover, seminal algorithms such as \(\mathrm{E}^{3}\)(Kearns and Singh, 2002) and \(\mathrm{R}\operatorname{-}\max\)(Brafman and Tennenholtz, 2002) have established PAC bounds for the nonepisodic setting. These bounds are further improved for communicating MDPs by the UCRL2 (Jaksch et al., 2010) algorithm, which, similar to NeoRL, is based on the optimism in the face of uncertainty paradigm and picks policies that are optimistic w.r.t. to the estimated dynamics. Their result is extended for weakly-communicating MDPs by REGAL (Bartlett and Tewari, 2012), similar results are derived for Thompson sampling based exploration (Ouyang et al., 2017), and for factored-MDP (Xu and Tewari, 2020). Albeit the significant amount of work for the finite case, progress for continuous state-action spaces has mostly been limited to linear dynamical systems.

Nonepisodic RL for linear systemsThere is a large body of work for nonepisodic learning with linear systems (Abbasi-Yadkori and Szepesvari, 2011; Cohen et al., 2019; Simchowitz and Foster, 2020; Dean et al., 2020; Lale et al., 2020; Faradonbeh et al., 2020; Abeille and Lazaric, 2020; Treven et al., 2021). For linear systems with quadratic costs, the average reward problem, also known as the linear quadratic-Gaussian (LQG), has a closed-form solution which is obtained via the Riccati equations (Anderson and Moore, 2007). Moreover, for LQG, stability and optimality are intertwined,making studying linear systems much easier than their nonlinear counterpart. For studying nonlinear systems, additional assumptions on their stability are usually made.

**Episodic RL for nonlinear systems** In the case of nonlinear systems, guarantees have mostly been established for the episodic setting (Mania et al., 2020; Kakade et al., 2020; Curi et al., 2020; Wagenmaker et al., 2023; Sukhija et al., 2024; Treven et al., 2024). In this setting, the agent begins each episode from an initial state \(\mathfrak{s}_{0}\) (or initial state distribution) and interacts with the environment for a fixed horizon \(H\). It uses the data collected from the interactions to update its model. After each episode, the agent is reset back to \(\mathfrak{s}_{0}\). The works mentioned above theoretically study this setting for finite-horizon MDPs and establish regret bounds for general nonlinear systems. Particularly Kakade et al. (2020); Curi et al. (2020); Sukhija et al. (2024); Treven et al. (2024) also use an optimism-based approach similar to ours. Compared to the nonepisodic case, the analysis of episodic RL methods is simpler as resets restrict the agent's exploration around the initial state \(\bm{s}_{0}\) and prevent the system from blowing up or visiting states from which the agent cannot recover. However, as discussed in Section 1, resets are often prohibitive and RL agents that learn non-episodically are preferred for many real-world applications.

**Nonepisodic RL beyond linear systems** Only a few works consider the nonepisodic/single-trajectory case. For instance, a line of work studies data-driven MPC approaches focusing mostly on establishing system-theoretic guarantees such as closed-loop stability and robustness (Berberich and Allgower, 2024). From the learning side, Foster et al. (2020); Sattar and Oymak (2022) study the problem of system identification of a closed-loop globally exponentially stable dynamical system from a single trajectory. Lale et al. (2021) study the nonepisodic setting for nonlinear systems with MPC. Moreover, they consider finite-order or exponentially fading NARX systems that lie in the RKHS of infinitely smooth functions, which they further approximate with random Fourier features (Rahimi and Recht, 2007)\(\bm{\phi}\) with feature size \(D\). Further, they assume access to bounded persistently exciting inputs w.r.t. the feature matrix \(\bm{\Phi}_{t}\bm{\Phi}_{t}^{\intercal}\). This assumption is generally tough to verify and common excitation strategies such as random exploration often don't perform well for nonlinear systems (Sukhija et al., 2024). The algorithm also operates in two stages, where in the first stage it performs pure exploration for system identification and in the second stage exploitation, i.e., acting greedily w.r.t. the estimated dynamics, akin to NeMean. Additionally, the algorithm requires the feature size \(D\) to increase with the horizon \(T\). They give a regret bound of \(\mathcal{O}\left(T^{2/s}\right)\) where the regret is measured w.r.t. to the oracle MPC with access to the true dynamics. Lale et al. (2021) also assume exponential input-to-output stability of the system to avoid blow-up during exploration. Our work considers more general RKHS, naturally trades-off exploration and exploitation, does not require apriori knowledge of persistently exciting inputs and gives a regret bound of \(\mathcal{O}(\beta_{T}\sqrt{T\Gamma_{T}})\) w.r.t. the optimal average cost criterion. Moreover, our regret bound is similar to the ones obtained for nonlinear systems in the episodic case and Gaussian process bandits (Srinivas et al., 2012; Chowdhury and Gopalan, 2017; Scarlett et al., 2017). To the best of our knowledge, we are the first to give such a regret bound for nonlinear systems.

Figure 2: Total number of resets and cumulative regret \(R_{T}\) for the cart pole balancing task over ten different seeds. We report the mean performance with one standard errors as the shaded region. The environment is automatically reset whenever the agent drops the pole. All baselines solve the task, but NeoRL converges the fastest requiring fewer resets and suffering smaller regret.

Nonepisodic Deep RLStandard deep RL approaches often fail in the nonepisodic setting (Sharma et al., 2021). To this end, deep RL algorithms have also been developed for the nonepisodic case. Mostly, these works focus on learning to reset and formulate it from the perspective of safety (Eysenbach et al., 2018) (avoiding undesirable states), chaining multiple controllers (Han et al., 2015), skill discovery/intrinsic exploration (Zhu et al., 2020; Xu et al., 2020), curriculum learning (Sharma et al., 2021), and learning initial state distributions from demonstrations (Sharma et al., 2022). However, in contrast to us, none of the works above provide any theoretical guarantees.

There are several extensions of model-free deep RL algorithms to the average reward setting (TRPO (Zhang and Ross, 2021), PPO (Ma et al., 2021), and DDPG (Saxena et al., 2023)). However, they mostly focus on maximizing the long-term behavior of the RL agent and allow for resets during learning. Overall, extending RL algorithms for the discounted case to the average one is still an open problem (Dewanto et al., 2020). However, future work in this direction will benefit NeoRL. Since average-reward optimizers can be used in combination with NeoRL to directly minimize the average cost in a model-based policy optimization (Janner et al., 2019) manner.

## 6 Conclusion

We propose, NeoRL, a novel model-based RL algorithm for the nonepisodic setting with nonlinear dynamics and continuous state and action spaces. NeoRL seeks for average-cost optimal policies and leverages the model's epistemic uncertainty to perform optimistic exploration. Similar to the episodic case (Kakade et al., 2020; Curi et al., 2020), we provide a regret bound for NeoRL of \(\mathcal{O}(\beta_{T}\sqrt{TT_{T}})\) for Gaussian process dynamics. To our knowledge, we are the first to obtain this result in the nonepisodic setting. We compare NeoRL to other model-based RL methods on standard deep RL benchmarks. Our experiments demonstrate that NeoRL, converges to the optimal average cost of \(A(\bm{\pi}^{*})=0\) across all environments, suffering sublinear regret even when Bayesian neural networks are used to model the dynamics. Moreover, NeoRL outperforms all our baselines across all environments requiring only \(\sim 10^{3}\) samples for learning.

Future work may consider deriving lower bounds on the regret of NeoRL, studying different assumptions on \(\bm{f}^{*}\) and \(\Pi\), and investigating different notions of optimality such as bias optimality in the nonepisodic setting (Mahadevan, 1996).

## Acknowledgments and Disclosure of Funding

We would like to thank Mohammad Reza Karimi, Scott Sussex, and Armin Lederer for the insightful discussions and feedback on this work. This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and the Microsoft Swiss Joint Research Center.

## References

* Abbasi-Yadkori and Szepesvari (2011) Abbasi-Yadkori, Y. and Szepesvari, C. Regret bounds for the adaptive control of linear quadratic systems. In _Conference on Learning Theory_, 2011.
* Abeille and Lazaric (2020) Abeille, M. and Lazaric, A. Efficient optimistic exploration in linear-quadratic regulators via lagrangian relaxation. In _International Conference on Machine Learning_, 2020.
* Anderson and Moore (2007) Anderson, B. D. and Moore, J. B. _Optimal control: linear quadratic methods_. Courier Corporation, 2007.
* Annaswamy (2023) Annaswamy, A. M. Adaptive control and intersections with reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_, 2023.
* Arapostathis et al. (1993) Arapostathis, A., Borkar, V. S., Fernandez-Gaucherand, E., Ghosh, M. K., and Marcus, S. I. Discrete-time controlled markov processes with average cost criterion: A survey. _SIAM Journal on Control and Optimization_, 1993.
* Astrom and Wittenmark (2013) Astrom, K. J. and Wittenmark, B. _Adaptive Control_. Courier Corporation, 2013.
* Bartlett and Tewari (2012) Bartlett, P. L. and Tewari, A. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. _arXiv preprint arXiv:1205.2661_, 2012.
* Berberich and Allgower (2024) Berberich, J. and Allgower, F. An overview of systems-theoretic guarantees in data-driven model predictive control, 2024. URL https://arxiv.org/abs/2406.04130.
* Borkar et al. (2019)Brafman, R. I. and Tennenholtz, M. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. _Journal of Machine Learning Research_, 2002.
* Brockman et al. (2016) Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Chowdhury & Gopalan (2017) Chowdhury, S. R. and Gopalan, A. On kernelized multi-armed bandits. In _ICML_, 2017.
* Chua et al. (2018) Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In _NeurIPS_, 2018.
* Cohen et al. (2019) Cohen, A., Koren, T., and Mansour, Y. Learning linear-quadratic regulators efficiently with only \(\sqrt{T}\) regret. In _International Conference on Machine Learning_, 2019.
* Curi et al. (2020) Curi, S., Berkenkamp, F., and Krause, A. Efficient model-based reinforcement learning through optimistic policy search and planning. _NeurIPS_, 33:14156-14170, 2020.
* Dean et al. (2020) Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S. On the sample complexity of the linear quadratic regulator. _Foundations of Computational Mathematics_, 20(4):633-679, 2020.
* Dewanto et al. (2020) Dewanto, V., Dunn, G., Eshragh, A., Gallagher, M., and Roosta, F. Average-reward model-free reinforcement learning: a systematic review and literature mapping. _arXiv preprint arXiv:2010.08920_, 2020.
* Eysenbach et al. (2018) Eysenbach, B., Gu, S., Ibarz, J., and Levine, S. Leave no trace: Learning to reset for safe and autonomous reinforcement learning. _International Conference on Learning Representations_, 2018.
* Faradonbeh et al. (2020) Faradonbeh, M. K. S., Tewari, A., and Michailidis, G. Optimism-based adaptive regulation of linear-quadratic systems. _IEEE Transactions on Automatic Control_, 2020.
* Foster et al. (2020) Foster, D., Sarkar, T., and Rakhlin, A. Learning nonlinear dynamical systems from a single trajectory. In _Learning for Dynamics and Control_, 2020.
* a survey. _Automatica_, pp. 335-348, 1989.
* Ha & Schmidhuber (2018) Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. _Advances in neural information processing systems_, 31, 2018.
* Hafner et al. (2019) Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In _International conference on machine learning_, 2019.
* Hafner et al. (2020) Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. _ICLR_, 2020.
* Hairer & Mattingly (2011) Hairer, M. and Mattingly, J. C. Yet another look at harris' ergodic theorem for markov chains. In _Seminar on Stochastic Analysis, Random Fields and Applications VI: Centro Stefano Franscini, Ascona, May 2008_, pp. 109-117. Springer, 2011.
* Han et al. (2015) Han, W., Levine, S., and Abbeel, P. Learning compound multi-step controllers under unknown dynamics. In _Intelligent Robots and Systems (IROS)_, 2015.
* Jaksch et al. (2010) Jaksch, T., Ortner, R., and Auer, P. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 2010.
* Janner et al. (2019) Janner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_, 2019.
* Kabzan et al. (2020) Kabzan, J., Valls, M. I., Reijgwart, V. J., Hendrikx, H. F., Ehmke, C., Prajapat, M., Buhler, A., Gosala, N., Gupta, M., Sivanesan, R., et al. Amz driverless: The full autonomous racing system. _Journal of Field Robotics_, 2020.
* Kakade et al. (2020) Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. Information theoretic regret bounds for online nonlinear control. _NeurIPS_, 33:15312-15325, 2020.
* Kakade (2003) Kakade, S. M. _On the sample complexity of reinforcement learning_. University of London, University College London (United Kingdom), 2003.
* Kearns & Singh (2002) Kearns, M. and Singh, S. Near-optimal reinforcement learning in polynomial time. _Machine learning_, 2002.
* Khalil (2015) Khalil, H. K. _Nonlinear control_, volume 406. Pearson New York, 2015.
* Krizhevsky & Sutskever (2014)Kipf, T., Van der Pol, E., and Welling, M. Contrastive learning of structured world models. _arXiv preprint arXiv:1911.12247_, 2019.
* Krstic et al. (1992) Krstic, M., Kanellakopoulos, I., and Kokotovic, P. Adaptive nonlinear control without overparametrization. _Systems & Control Letters_, 1992.
* Krstic et al. (1995) Krstic, M., Kokotovic, P. V., and Kanellakopoulos, I. _Nonlinear and adaptive control design_. John Wiley & Sons, Inc., 1995.
* Kuleshov et al. (2018) Kuleshov, V., Fenner, N., and Ermon, S. Accurate uncertainties for deep learning using calibrated regression. In _ICML_, pp. 2796-2804. PMLR, 2018.
* Lai & Wei (1982) Lai, T. L. and Wei, C. Z. Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems. _The Annals of Statistics_, 1982.
* Lai & Wei (1987) Lai, T. L. and Wei, C.-Z. Asymptotically efficient self-tuning regulators. _SIAM Journal on Control and Optimization_, 1987.
* Lakshminarayanan et al. (2017) Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles, 2017.
* Lale et al. (2020) Lale, S., Azizzadenesheli, K., Hassibi, B., and Anandkumar, A. Logarithmic regret bound in partially observable linear dynamical systems. _Advances in Neural Information Processing Systems_, 2020.
* Lale et al. (2021) Lale, S., Azizzadenesheli, K., Hassibi, B., and Anandkumar, A. Model learning predictive control in nonlinear dynamical systems. In _Conference on Decision and Control (CDC)_. IEEE, 2021.
* Ma et al. (2021) Ma, X., Tang, X., Xia, L., Yang, J., and Zhao, Q. Average-reward reinforcement learning with trust region methods. _International Joint Conference on Artificial Intelligence_, 2021.
* Mahadevan (1996) Mahadevan, S. Average reward reinforcement learning: Foundations, algorithms, and empirical results. _Machine learning_, 1996.
* Mania et al. (2020) Mania, H., Jordan, M. I., and Recht, B. Active learning for nonlinear system identification with guarantees. _arXiv preprint arXiv:2006.10277_, 2020.
* Meyn & Tweedie (2012) Meyn, S. P. and Tweedie, R. L. _Markov chains and stochastic stability_. Springer Science & Business Media, 2012.
* Osband & Van Roy (2017) Osband, I. and Van Roy, B. Why is posterior sampling better than optimism for reinforcement learning? In _International conference on machine learning_, 2017.
* Ouyang et al. (2017) Ouyang, Y., Gagrani, M., Nayyar, A., and Jain, R. Learning unknown markov decision processes: A thompson sampling approach. _Advances in neural information processing systems_, 30, 2017.
* Pinneri et al. (2021) Pinneri, C., Sawant, S., Blaes, S., Achterhold, J., Stueckler, J., Rolinek, M., and Martius, G. Sample-efficient cross-entropy method for real-time planning. In _CORL_, Proceedings of Machine Learning Research, pp. 1049-1065, 2021.
* Puterman (2014) Puterman, M. L. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Rahimi & Recht (2007) Rahimi, A. and Recht, B. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* Rothfuss et al. (2023) Rothfuss, J., Sukhija, B., Birchler, T., Kassraie, P., and Krause, A. Hallucinated adversarial control for conservative offline policy evaluation. _UAI_, 2023.
* Sattar & Oymak (2022) Sattar, Y. and Oymak, S. Non-asymptotic and accurate learning of nonlinear dynamical systems. _Journal of Machine Learning Research_, 2022.
* Saxena et al. (2023) Saxena, N., Khastagir, S., Kolathaya, S., and Bhatnagar, S. Off-policy average reward actor-critic with deterministic policy search. In _International Conference on Machine Learning_, 2023.
* Scarlett et al. (2017) Scarlett, J., Bogunovic, I., and Cevher, V. Lower bounds on regret for noisy Gaussian process bandit optimization. In _Conference on Learning Theory_, 2017.
* Sharma et al. (2021a) Sharma, A., Gupta, A., Levine, S., Hausman, K., and Finn, C. Autonomous reinforcement learning via subgoal curricula. _Advances in Neural Information Processing Systems_, 2021a.

Sharma, A., Xu, K., Sardana, N., Gupta, A., Hausman, K., Levine, S., and Finn, C. Autonomous reinforcement learning: Formalism and benchmarking. _arXiv preprint arXiv:2112.09605_, 2021b.
* Sharma et al. (2022) Sharma, A., Ahmad, R., and Finn, C. A state-distribution matching approach to non-episodic reinforcement learning. _International Conference on Machine Learning_, 2022.
* Simchowitz & Foster (2020) Simchowitz, M. and Foster, D. Naive exploration is optimal for online lqr. In _International Conference on Machine Learning_. PMLR, 2020.
* Srinivas et al. (2012) Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. _IEEE Transactions on Information Theory_, 2012.
* Sukhija et al. (2024) Sukhija, B., Treven, L., Sancaktar, C., Blaes, S., Coros, S., and Krause, A. Optimistic active exploration of dynamical systems. _NeurIPS_, 2024.
* Sussex et al. (2023) Sussex, S., Makarova, A., and Krause, A. Model-based causal bayesian optimization. In _ICLR_, May 2023.
* Tassa et al. (2018) Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* Tekinalp et al. (2024) Tekinalp, A., Kim, S. H., Bhosale, Y., Parthasarathy, T., Naughton, N., Albazroun, A., Joon, R., Cui, S., Nasiriziba, I., Stolzle, M., Shih, C.-H. C., and Gazzola, M. Gazzolala/pyelastica: v0.3.2, 2024.
* Treven et al. (2021) Treven, L., Curi, S., Mutny, M., and Krause, A. Learning stabilizing controllers for unstable linear quadratic regulators from a single trajectory. In _Learning for Dynamics and Control_, 2021.
* Treven et al. (2024) Treven, L., Hubotter, J., Sukhija, B., Dorfler, F., and Krause, A. Efficient exploration in continuous-time model-based reinforcement learning. _NeurIPS_, 2024.
* Vakili et al. (2021) Vakili, S., Khezeli, K., and Picheny, V. On information gain and regret bounds in gaussian process bandits. In _AISTATS_, 2021.
* Wagenmaker et al. (2023) Wagenmaker, A., Shi, G., and Jamieson, K. Optimal exploration for model-based rl in nonlinear systems. _arXiv preprint arXiv:2306.09210_, 2023.
* Williams et al. (2017) Williams, G., Wagener, N., Goldfain, B., Drews, P., Rehg, J. M., Boots, B., and Theodorou, E. A. Information theoretic mpc for model-based reinforcement learning. In _ICRA_, 2017.
* Xu et al. (2020) Xu, K., Verma, S., Finn, C., and Levine, S. Continual learning of control primitives: Skill discovery via reset-games. _Advances in Neural Information Processing Systems_, 2020.
* Xu & Tewari (2020) Xu, Z. and Tewari, A. Reinforcement learning in factored mdps: Oracle-efficient algorithms and tighter regret bounds for the non-episodic setting. _Advances in Neural Information Processing Systems_, 2020.
* Zhang & Ross (2021) Zhang, Y. and Ross, K. W. On-policy deep reinforcement learning for the average-reward criterion. In _International Conference on Machine Learning_, 2021.
* Zhao et al. (2024) Zhao, F., Dorfler, F., Chiuso, A., and You, K. Data-enabled policy optimization for direct adaptive learning of the lqr. _arXiv preprint arXiv:2401.14871_, 2024.
* Zhu et al. (2020) Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V., and Levine, S. The ingredients of real-world robotic reinforcement learning. _arXiv preprint arXiv:2004.12570_, 2020.

[MISSING_PAGE_FAIL:14]

Through the minorisation condition and Assumption 2.4, we can prove the ergodicity of the closed-loop system for a given policy \(\bm{\pi}\in\Pi\).

**Theorem A.2** (Ergodicity of closed-loop system).: _Let Assumption 2.1 - 2.4, consider any probability measures \(\zeta_{1}\), \(\zeta_{2}\), and \(\theta>0\), define \(P^{\bm{\pi}}\zeta\), \(\left\lVert\varphi\right\rVert_{1+\theta V^{\bm{\pi}}}\), \(\rho^{\bm{\pi}}_{\theta}\) as_

\[(P^{\bm{\pi}}\zeta)\left(\mathcal{A}\right) =\int_{\mathcal{X}}P^{\bm{\pi}}(\bm{x},\mathcal{A})\zeta(d\bm{x})\] \[\left\lVert\varphi\right\rVert_{1+\theta V^{\bm{\pi}}} =\sup_{\bm{x}\in\mathcal{X}}\frac{|\varphi(\bm{x})|}{1+\theta V^{ \bm{\pi}}(\bm{x})}\] \[\rho^{\bm{\pi}}_{\theta}(\zeta_{1},\zeta_{2}) =\sup_{\varphi:\left\lVert\varphi\right\rVert_{1+\theta V^{\bm{ \pi}}}\leq 1}\int_{\mathcal{X}}\varphi(\bm{x})(\zeta_{1}-\zeta_{2})(d\bm{x})= \int_{\mathcal{X}}(1+\theta V^{\bm{\pi}}(\bm{x}))|\zeta_{1}-\zeta_{2}|(d\bm{x }).\]

_We have for all \(\bm{\pi}\in\Pi\), that \(P^{\bm{\pi}}\) admits a unique invariant measure \(\bar{P}^{\bm{\pi}}\). Furthermore, there exist constants \(C_{1}>0\), \(\theta>0\), \(\lambda\in(0,1)\) such that_

\[\rho^{\bm{\pi}}_{\theta}(P^{\bm{\pi}}\zeta_{1},P^{\bm{\pi}}\zeta_ {2}) \leq\lambda\rho^{\bm{\pi}}_{\theta}(\zeta_{1},\zeta_{2})\] (1) \[\left\lVert\mathbb{E}_{\bm{\pi}\sim(P^{\bm{\pi}})^{t}}\left[ \varphi(\bm{x})\right]\right\rVert_{1+V^{\bm{\pi}}} \leq C_{1}\lambda^{t}\left\lVert\varphi-\mathbb{E}_{\bm{x}\sim \bar{P}^{\bm{\pi}}}\left[\varphi(\bm{x})\right]\right\rVert_{1+V^{\bm{\pi}}}.\] (2)

_holds for every measurable function \(\varphi:\mathcal{X}\to\mathcal{R}\) with \(\left\lVert\varphi\right\rVert_{1+V^{\bm{\pi}}}<\infty\). Here \((P^{\bm{\pi}})^{t}\) denotes the \(t\)-step transition kernel under the policy \(\bm{\pi}\). Moreover, \(\theta=\nicefrac{{\alpha_{0}}}{{k}}\), and_

\[\lambda=\max\left\{1-(\alpha-\alpha_{0}),\frac{2+\nicefrac{{R}}{{K}}\alpha_{ 0}\gamma_{0}}{2+\nicefrac{{R}}{{K}}\alpha_{0}}\right\}\] (10)

_for any \(\alpha_{0}\in(0,\alpha)\) and \(\gamma_{0}\in(\gamma+2\nicefrac{{K}}{{R}},1)\)._

Proof.: From Assumption 2.4, we have a value function for each policy that satisfies the drift condition. Furthermore, in Lemma A.1 we show that our system also satisfies the minorisation condition for all policies. Under these conditions, we can use the results from Hairer & Mattingly (2011, Theorem 1.2, - 1.3.). 

Note that \(\left\lVert\cdot\right\rVert_{1+\theta V^{\bm{\pi}}}\) represents a family of equivalent norms for any \(\theta>0\). Now we prove Theorem 2.6.

Proof of Theorem 2.6.: From Theorem A.2, we have

\[\rho^{\bm{\pi}}_{\theta}((P^{\bm{\pi}})^{t+1},(P^{\bm{\pi}})^{t})=\rho^{\bm{ \pi}}_{\theta}(P^{\bm{\pi}}(P^{\bm{\pi}})^{t},P^{\bm{\pi}}(P^{\bm{\pi}})^{t-1 })\leq\lambda^{t}\rho^{\bm{\pi}}_{\theta}(P^{\bm{\pi}}\delta_{\bm{x}_{0}}, \delta_{\bm{x}_{0}}),\]

where \(\delta_{\bm{x}_{0}}\) is the dirac measure. Therefore, \((P^{\bm{\pi}})^{t}\) is a Cauchy sequence. Furthermore, \(\rho^{\bm{\pi}}_{\theta}\) is complete for the set of probability measures integrating \(V\), thus \(\rho^{\bm{\pi}}_{\theta}((P^{\bm{\pi}})^{t},\bar{P}^{\bm{\pi}})\to 0\) for \(t\to\infty\) (c.f., Hairer & Mattingly (2011) for more details). In particular, we have for \(\varphi\) such that \(\left\lVert\varphi\right\rVert_{1+\theta V^{\bm{\pi}}}\leq 1\),

\[\lim_{t\to\infty}\int_{\mathcal{X}}\varphi(\bm{x})(P^{\bm{\pi}})^{t}(d\bm{x})= \int_{\mathcal{X}}\varphi(\bm{x})\bar{P}^{\bm{\pi}}(d\bm{x}).\]

Note that since all \(\left\lVert\cdot\right\rVert_{1+\theta V^{\bm{\pi}}}\) norms are equivalent for \(\theta>0\), if \(\left\lVert c\right\rVert_{1+V^{\bm{\pi}}}\leq C\) (Assumption 2.4), then \(\left\lVert c\right\rVert_{1+\theta V^{\bm{\pi}}}\leq C^{\prime}\) for some \(C^{\prime}\in(0,\infty)\). Furthermore, note that \(c(\cdot)\geq 0\). Therefore,

\[\int_{\mathcal{X}}c(\bm{x})\bar{P}^{\bm{\pi}}(d\bm{x}) =\lim_{t\to\infty}\int_{\mathcal{X}}c(\bm{x})(P^{\bm{\pi}})^{t}(d \bm{x})\] \[\leq C\lim_{t\to\infty}\int_{\mathcal{X}}(1+V^{\bm{\pi}}(\bm{x}))(P ^{\bm{\pi}})^{t}(d\bm{x})\] \[=C+C\lim_{t\to\infty}\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t}}[V^ {\bm{\pi}}(\bm{x})]\] \[=C+C\lim_{t\to\infty}\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t-1}}[ \mathbb{E}_{\bm{x}^{\prime}\sim(P^{\bm{\pi}})}[V^{\bm{\pi}}(\bm{x}^{\prime})| \bm{x}]]\] \[\leq C+C\left(\lim_{t\to\infty}\gamma\mathbb{E}_{\bm{x}\sim(P^{ \bm{\pi}})^{t-1}}[V^{\bm{\pi}}(\bm{x})]+K\right)\] (Assumption 2.4)\[\leq C+C\lim_{t\to\infty}\gamma^{t}V^{\pi}(\bm{x}_{0})+K\frac{1-\gamma^{t}}{1-\gamma}\] \[=C\left(1+K\frac{1}{1-\gamma}\right)\]

In summary, we have \(\mathbb{E}_{\bm{x}\sim\tilde{P}^{\bm{\pi}}}\left[c(\bm{x})\right]\leq C\left(1+K \frac{1}{1-\gamma}\right)\)

Consider any \(t>0\), and note that from Theorem A.2 we have

\[\left\|\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t}}\left[c(\bm{x}) \right]-\mathbb{E}_{\bm{x}\sim\tilde{P}^{\bm{\pi}}}\left[c(\bm{x})\right]\right\| _{1+V^{\bm{\pi}}} =\sup_{\bm{x}_{0}\in\mathcal{X}}\frac{\left|\mathbb{E}_{\bm{x} \sim(P^{\bm{\pi}})^{t}}\left[c(\bm{x})\right]-\mathbb{E}_{\bm{x}\sim P^{\bm{ \pi}}}\left[c(\bm{x})\right]\right|}{1+V^{\bm{\pi}}(\bm{x}_{0})}\] \[\leq C_{1}\lambda^{t}\left\|c-\mathbb{E}_{\bm{x}\sim P^{\bm{\pi} }}\left[c(\bm{x})\right]\right\|_{1+V^{\bm{\pi}}}\quad\text{ (Theorem A.2)}\] \[\leq C_{1}\lambda^{t}\left\|c\right\|_{1+V^{\bm{\pi}}}+C_{1} \lambda^{t}\mathbb{E}_{\bm{x}\sim\tilde{P}^{\bm{\pi}}}\left[c(\bm{x})\right]\] \[=C_{2}\lambda^{t},\]

where \(C_{2}=C_{1}(\left\|c\right\|_{1+V^{\bm{\pi}}}+CK\frac{1}{1-\gamma})\).

Moreover, since the inequality holds for all \(\bm{x}_{0}\), we have

\[\frac{\left|\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t}}\left[c(\bm{x})\right]- \mathbb{E}_{\bm{x}\sim P^{\bm{\pi}}}\left[c(\bm{x})\right]\right|}{1+V^{\bm{ \pi}}(\bm{x}_{0})}\leq C_{2}\lambda^{t}.\]

In summary,

\[\left|\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t}}\left[c(\bm{x})\right]-\mathbb{ E}_{\bm{x}\sim\tilde{P}^{\bm{\pi}}}\left[c(\bm{x})\right]\right|\leq C_{2}(1+V^{ \bm{\pi}}(\bm{x}_{0}))\lambda^{t}.\]

Consider any \(T\geq 0\), and define with \(\bar{c}=\mathbb{E}_{\bm{x}\sim\tilde{P}^{\bm{\pi}}}\left[c(\bm{x},\bm{\pi}(x))\right]\).

\[\mathbb{E}_{\bm{\pi}}\left[\sum_{t=0}^{T-1}c(\bm{x}_{t},\bm{u}_{t })-\bar{c}\right] =\sum_{t=0}^{T-1}\mathbb{E}_{(P^{\bm{\pi}})^{t}}\left[c(\bm{x}_{t },\bm{u}_{t})\right]-\bar{c}\] \[\leq\sum_{t=0}^{T-1}\left|\mathbb{E}_{(P^{\bm{\pi}})^{t}}\left[c( \bm{x}_{t},\bm{u}_{t})\right]-\bar{c}\right|\] \[\leq C_{2}(1+V^{\bm{\pi}}(\bm{x}_{0}))\sum_{t=0}^{T-1}\lambda^{t}\] \[=C_{2}(1+V^{\bm{\pi}}(\bm{x}_{0}))\frac{1-\lambda^{T}}{1-\lambda}\]

Hence, we have

\[\lim_{T\to\infty}\left|\mathbb{E}_{\bm{\pi}}\left[\sum_{t=0}^{T-1}c(\bm{x}_{t },\bm{u}_{t})-\bar{c}\right]\right|\leq C_{2}(1+V^{\bm{\pi}}(\bm{x}_{0}))\frac {1}{1-\lambda},\]

and for any \(\bm{x}_{0}\) in a compact subset of \(\mathcal{X}\)

\[\lim_{T\to\infty}\frac{1}{T}\mathbb{E}_{\bm{\pi}}\left[\sum_{t=0}^{T-1}c(\bm{x }_{t},\bm{u}_{t})-\bar{c}\right]=0.\]

Moreover,

\[\left|B(\bm{\pi},\bm{x}_{0})\right|\leq C_{2}(1+V^{\bm{\pi}}(\bm{x}_{0}))\frac {1}{1-\lambda}.\]

Another interesting, inequality that follows from the proof above is the difference in bias inequality.

\[\left|\mathbb{E}_{\bm{x}_{0}\sim\zeta_{1}}[B(\bm{\pi},\bm{x}_{0})]-\mathbb{E} _{\bm{x}_{0}\sim\zeta_{2}}[B(\bm{\pi},\bm{x}_{0})]\right|\leq\frac{C_{3}}{1- \lambda}\int_{\mathcal{X}}(1+V^{\bm{\pi}}(\bm{x}))\left|\zeta_{1}-\zeta_{2} \right|(d\bm{x})\]for all probability measures \(\zeta_{1},\zeta_{2}\). To show this holds, define \(C^{\prime}=\max_{\bm{\pi}\in\Pi}\left\|c(\bm{x},\bm{\pi}(\bm{x}))\right\|_{1+ \theta V^{\bm{\pi}}}\). Furthermore, note that \(C^{\prime}<\infty\) from Assumption 2.4 and \(\left\|\nicefrac{{c(\bm{x},\bm{\pi}(\bm{x}))}}{{C^{\prime}}}\right\|_{1+\theta V ^{\bm{\pi}}}\leq 1\).

\[\left|\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t}\zeta_{1}}c(\bm{x}, \bm{\pi}(\bm{x}))-\mathbb{E}_{\bm{x}\sim(P^{\bm{\pi}})^{t}\zeta_{2}}c(\bm{x}, \bm{\pi}(\bm{x}))\right|=\left|\int_{\mathcal{X}}c(\bm{x},\bm{\pi}(\bm{x}))((P^ {\bm{\pi}})^{t}\zeta_{1}-(P^{\bm{\pi}})^{t}\zeta_{2})(d\bm{x})\right|\] \[=C^{\prime}\left|\int_{\mathcal{X}}\frac{1}{C^{\prime}}c(\bm{x}, \bm{\pi}(\bm{x}))((P^{\bm{\pi}})^{t}\zeta_{1}-(P^{\bm{\pi}})^{t}\zeta_{2})(d \bm{x})\right|\] \[\leq C^{\prime}\sup_{\varphi:\|\varphi\|_{1+\theta V^{\bm{\pi}}} \leq 1}\int_{\mathcal{X}}\varphi(\bm{x})((P^{\bm{\pi}})^{t}\zeta_{1}-(P^{ \bm{\pi}})^{t}\zeta_{2})(d\bm{x})=C^{\prime}\rho_{\theta}^{\bm{\pi}}((P^{\bm{ \pi}})^{t}\zeta_{1},(P^{\bm{\pi}})^{t}\zeta_{2})\] \[\leq C^{\prime}\lambda\rho_{\theta}^{\bm{\pi}}((P^{\bm{\pi}})^{t- 1}\zeta_{1},(P^{\bm{\pi}})^{t-1}\zeta_{2})\] (Theorem A.2) \[\leq C^{\prime}\lambda^{t}\rho_{\theta}^{\bm{\pi}}(\zeta_{1},\zeta _{2}).\]

Also, note that there exists \(C_{\theta}\in(0,\infty)\) such that \(C_{\theta}\left\|\varphi\right\|_{1+\theta V^{\bm{\pi}}}\geq\left\|\varphi \right\|_{1+V^{\bm{\pi}}}\) due to the equivalence of the two norms.

\[\rho_{\theta}^{\bm{\pi}}(\zeta_{1},\zeta_{2}) =\sup_{\varphi:\|\varphi\|_{1+\theta V^{\bm{\pi}}}\leq 1}\int_{ \mathcal{X}}\varphi(\bm{x})(\zeta_{1}-\zeta_{2})(d\bm{x})\] \[\leq\sup_{\varphi:\|\varphi\|_{1+V^{\bm{\pi}}}\leq C_{\theta}} \int_{\mathcal{X}}\varphi(\bm{x})(\zeta_{1}-\zeta_{2})(d\bm{x})\] \[=C_{\theta}\sup_{\varphi:\|\varphi\|_{1+V^{\bm{\pi}}}\leq 1}\int_{ \mathcal{X}}\varphi(\bm{x})(\zeta_{1}-\zeta_{2})(d\bm{x})\] \[=C_{\theta}\rho_{1}^{\bm{\pi}}(\zeta_{1},\zeta_{2})\]

Therefore, for the bias we have

\[\left|\mathbb{E}_{\bm{x}_{0}\sim\zeta_{1}}[B(\bm{\pi},\bm{x}_{0}) ]-\mathbb{E}_{\bm{x}_{0}\sim\zeta_{2}}[B(\bm{\pi},\bm{x}_{0})]\right|\] \[\leq\lim_{T\to\infty}\sum_{t=0}^{T-1}\left|\mathbb{E}_{\bm{x}_{ \sim(P^{\bm{\pi}})^{t}\zeta_{1}}}c(\bm{x},\bm{\pi}(\bm{x}))-\mathbb{E}_{\bm{x }\sim(P^{\bm{\pi}})^{t}\zeta_{2}}c(\bm{x},\bm{\pi}(\bm{x}))\right|\] \[\leq C^{\prime}\rho_{\theta}^{\bm{\pi}}(\zeta_{1},\zeta_{2})\lim_ {T\to\infty}\sum_{t=0}^{T-1}\lambda^{t}=\frac{C^{\prime}}{1-\lambda}\rho_{ \theta}^{\bm{\pi}}(\zeta_{1},\zeta_{2})\] \[\leq\frac{C^{\prime}C_{\theta}}{1-\lambda}\rho_{1}^{\bm{\pi}}( \zeta_{1},\zeta_{2})=\frac{C^{\prime}C_{\theta}}{1-\lambda}(1+V^{\bm{\pi}}( \bm{x}))\left|\zeta_{1}-\zeta_{2}\right|(d\bm{x})\]

Set \(C_{3}=C^{\prime}C_{\theta}\).

### Proof of bounded average cost for the optimistic system

In this section, we show that the results from Theorem 2.6 also transfer over to the optimistic dynamics.

**Theorem A.3** (Existence of Average Cost Solution for the Optimistic System).: _Let Assumption 2.1 - 2.8 hold. Consider any \(n>0\) and let \(\bm{\pi}_{n},\bm{f}_{n}\) denote the solution to Equation (6), \(P^{\bm{\pi},\bm{f}_{n}}\) its transition kernel. Then \(P^{\bm{\pi},\bm{f}_{n}}\) admits a unique invariant measure \(\bar{P}^{\bm{\pi}_{n},\bm{f}_{n}}\) and there exists \(C_{2},C_{3}\in(0,\infty)\), \(\hat{\lambda}\in(0,1)\) such that_

Average Cost;

\[A(\bm{\pi}_{n},\bm{f}_{n})=\lim_{T\to\infty}\frac{1}{T}\mathbb{E}_{\bm{\pi}_{n}, \bm{f}_{n}}\left[\sum_{t=0}^{T-1}c(\bm{x}_{t},\bm{u}_{t})\right]=\mathbb{E}_{ \bm{x}\sim\bar{P}^{\bm{\pi}_{n},\bm{f}_{n}}}\left[c(\bm{x},\bm{\pi}_{n}(x))\right]\]

Bias Cost;

\[\left|B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{0})\right|=\left|\lim_{T\to\infty} \mathbb{E}_{\bm{\pi}_{n},\bm{f}_{n}}\left[\sum_{t=0}^{T-1}c(\bm{x}_{t},\bm{u}_{t })-A(\bm{\pi}_{n},\bm{f}_{n})\right]\right|\leq C_{2}(1+V^{\bm{\pi}_{n}}(\bm{x }_{0}))\frac{1}{1-\hat{\lambda}}\]

_for all \(\bm{x}_{0}\in\mathcal{X}\)._Difference in Bias;

\[|\mathbb{E}_{\bm{x}_{0}\sim\zeta_{1}}[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{0})]- \mathbb{E}_{\bm{x}_{0}\sim\zeta_{2}}[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{0})]|\leq \frac{C_{3}}{1-\hat{\lambda}}\int_{\mathcal{X}}(1+V^{\bm{\pi}}(\bm{x}))\,|\zeta_ {1}-\zeta_{2}|\,(d\bm{x})\]

_for all probability measures \(\zeta_{1},\zeta_{2}\)._

Theorem A.3 shows that the optimistic dynamics \(\bm{f}_{n}\) retain the boundedness property from the true dynamics \(\bm{f}^{*}\) and give a well-defined solution w.r.t. average cost and the bias cost. To prove Theorem A.3 we show that the optimistic system also satisfies the drift and minorisation condition. Then we can invoke the result from Hairer & Mattingly (2011) similar to the proof of Theorem 2.6.

**Lemma A.4** (Stability of optimistic system).: _Let Assumption 2.1 - 2.8 hold, then we have with probability at least \(1-\delta\) for all \(n\geq 0\), \(\bm{\pi}\in\Pi\), \(\bm{f}\in\mathcal{M}_{n}\cap\mathcal{M}_{0}\), that there exists a constant \(\widehat{K}>0\) such that_

\[\mathbb{E}_{\bm{x}_{+}|\bm{x},\bm{f},\bm{\pi}}[V^{\bm{\pi}}(\bm{x}_{+})]\leq \gamma V^{\bm{\pi}}(\bm{x})+\widehat{K},\]

_where \(\bm{x}_{+}=\bm{f}(\bm{x},\bm{\pi}(\bm{x})+\bm{w}\)._

Proof.: Note, that \(V^{\bm{\pi}}\) is uniformly continuous w.r.t. \(\kappa\)

\[|V^{\bm{\pi}}(\bm{x})-V^{\bm{\pi}}(\bm{x}^{\prime})|\leq\kappa(\|\bm{x}-\bm{x }^{\prime}\|).\]

Furthermore, since \(\bm{f}\in\mathcal{M}_{n}\cap\mathcal{M}_{0}\) and therefore \(\bm{f}\in\mathcal{M}_{0}\), we have that there exists some \(\bm{\eta}\in[-1,1]^{dx}\) such that

\[\bm{f}(\bm{x},\bm{\pi}(\bm{x}))=\bm{\mu}_{0}(\bm{x}.\bm{\pi}(\bm{x}))+\beta_{ 0}\bm{\sigma}_{0}(\bm{x},\bm{\pi}(\bm{x}))\bm{\eta}(\bm{x}).\]

\[\mathbb{E}_{\bm{w}} [V^{\bm{\pi}}(\bm{\mu}_{0}(\bm{x}.\bm{\pi}(\bm{x}))+\beta_{0}\bm {\sigma}_{0}(\bm{x},\bm{\pi}(\bm{x}))\bm{\eta}(\bm{x})+\bm{w})]-\mathbb{E}_{ \bm{w}}[V^{\bm{\pi}}(\bm{f}^{*}(\bm{x}.\bm{\pi}(\bm{x}))+\bm{w})]\] \[\leq\kappa\left(\|\bm{\mu}_{0}(\bm{x}.\bm{\pi}(\bm{x}))+\beta_{0} \bm{\sigma}_{0}(\bm{x},\bm{\pi}(\bm{x}))\bm{\eta}(\bm{x})-\bm{f}^{*}(\bm{x}. \bm{\pi}(\bm{x}))\|\right)\] \[\leq\kappa\left(\|\bm{\mu}_{0}(\bm{x}.\bm{\pi}(\bm{x}))-\bm{f}^{* }(\bm{x}.\bm{\pi}(\bm{x}))\|+\|\beta_{0}\bm{\sigma}_{0}(\bm{x},\bm{\pi}(\bm{x} ))\bm{\eta}(\bm{x})\|\right)\] \[\leq\kappa\left(\left(1+\sqrt{d_{x}}\right)\beta_{0}\sqrt{d_{x}} \sigma_{\max}\right).\] (Assumption 2.8 )

Therefore,

\[\mathbb{E}_{\bm{x}_{+}|\bm{x},\bm{f},\bm{\pi}}[V^{\bm{\pi}}(\bm{x }_{+})] \leq\mathbb{E}_{\bm{x}_{+}|\bm{x},\bm{f}^{*},\bm{\pi}}[V^{\bm{ \pi}}(\bm{x}_{+}^{*})]+\kappa\left(\left(1+\sqrt{d_{x}}\right)\beta_{0}\sqrt{d _{x}}\sigma_{\max}\right)\] \[=\mathbb{E}_{\bm{x}_{+}|\bm{x},\bm{f}^{*},\bm{\pi}}[V^{\bm{\pi}}( \bm{x}_{+}^{*})]+\kappa\left(\left(1+\sqrt{d_{x}}\right)\beta_{0}\sqrt{d_{x}} \sigma_{\max}\right)\] \[\leq\gamma V^{\bm{\pi}}(\bm{x})+K+\kappa\left(\left(1+\sqrt{d_{x} }\right)\beta_{0}\sqrt{d_{x}}\sigma_{\max}\right),\] (Assumption 2.4 )

where we denoted \(\bm{x}_{+}^{*}=\bm{f}^{*}(\bm{x},\bm{\pi}(\bm{x})+\bm{w}\). Define \(\widehat{K}=K+\kappa\left(\left(1+\sqrt{d_{x}}\right)\beta_{0}\sqrt{d_{x}} \sigma_{\max}\right)\). 

**Lemma A.5** (Minorisation condition optimistic system).: _Consider the system_

\[\bm{x}_{+}=\bm{f}(\bm{x}.\bm{\pi}(\bm{x}))+\bm{w}\]

_for any \(n\geq 0\), \(\bm{\pi}\in\Pi\) and \(\bm{f}\in\mathcal{M}_{n}\cap\mathcal{M}_{0}\). Let Assumption 2.1 - 2.8 hold. Let \(P^{\bm{\pi},\bm{f}}\) denote the transition kernel for the policy \(\bm{\pi}\in\Pi\) i.e., \(P^{\bm{\pi},\bm{f}}(\bm{x},\bm{\mathcal{A}})=\mathbb{P}(\bm{x}_{+}\in\mathcal{ A}|\bm{x},\bm{\pi}(\bm{x}),\bm{f})\). Then, there exists a constant \(\hat{\alpha}\in(0,1)\) and a probability measure \(\hat{\zeta}(\cdot)\) independent of \(n\) s.t.,_

\[\inf_{\bm{x}\in\mathcal{C}}P^{\bm{\pi},\bm{f}}(\bm{x},\cdot)\geq\hat{\alpha} \hat{\zeta}(\cdot)\] (11)

_with \(\mathcal{C}\mathop{=}^{\mathrm{def}}\{\bm{x}\in\mathcal{X};V^{\bm{\pi}}(\bm{x}) <\hat{R}\}\) for some \(\hat{R}>{}^{2\hat{R}}\!/_{1-\gamma}\)_

Proof.: First, we show that \(\mathcal{C}\) is contained in a compact domain. From the Assumption 2.4 we pick the function \(\xi\in\mathcal{K}_{\infty}\). Since \(C_{l}\xi(0)=0,\lim_{s\to\infty}\xi(s)=+\infty\) and \(C_{l}\xi\) is continuous, there exists \(M\) such that \(C_{l}\xi(M)=\hat{R}\). Then for \(\|\bm{x}\|>M\) we have:

\[V^{\bm{\pi}}(\bm{x})\geq C_{l}\xi(\|\bm{x}\|)>\xi(M)=\hat{R}.\]Therefore we have: \(\mathcal{C}\subseteq\mathcal{B}(\mathbf{0},M)\mathop{\mathrm{def}}\limits^{\rm{ def}}\{\bm{x}\mid\|\bm{x}-\bm{0}\|\leq M\}\). Since for any \(\bm{x}\in\mathcal{C}\) we have \(\|\bm{f}(\bm{x},\bm{\pi}(\bm{x}))\|\leq\|\bm{f}^{*}(\bm{x},\bm{\pi}(\bm{x}))\| +\beta_{0}\sigma_{\max}\). Since \(\bm{f}^{*}\) is continuous, there exists a \(B\) such that \(\bm{f}^{*}(\mathcal{C},\bm{\pi}(\mathcal{C}))\subset\mathcal{B}(\mathbf{0},B)\). Therefore we have: \(\bm{f}(\mathcal{C},\bm{\pi}(\mathcal{C}))\subset\mathcal{B}(\mathbf{0},B_{1})\), where \(B_{1}=B+\beta_{0}\sigma_{\max}\). In the last step we prove that \(\alpha\mathop{\mathrm{def}}\limits^{\rm{def}}2^{-d_{\bm{x}_{\rm{e}}}}e^{-B_{1 }^{2}/\sigma^{2}}\) and \(\zeta\) with law of \(\mathcal{N}\left(0,\frac{\sigma^{2}}{2}\right)\) satisfy condition of Lemma A.1. It is enough to show that \(\forall\bm{\mu}\in\mathcal{B}(\mathbf{0},B_{1}),\forall\bm{x}\in\mathbb{R}^{d _{\bm{x}}}\) we have:

\[\alpha\frac{1}{(2\pi)^{\frac{d_{\bm{x}}}{2}}\left(\frac{\sigma^{2}}{2}\right) ^{\frac{d_{\bm{x}}}{2}}}e^{-\frac{\|\bm{x}\|^{2}}{\sigma^{2}}}\leq\frac{1}{(2 \pi)^{\frac{d_{\bm{x}}}{2}}(\sigma^{2})^{\frac{d_{\bm{x}}}{2}}}e^{-\frac{\|\bm {x}-\bm{\mu}\|^{2}}{2\sigma^{2}}}\]

which can be proven with simple algebraic manipulations. 

Proof of Theorem a.3.: As for the true system, the drift condition from Lemma A.4 and the minorisation condition from Lemma A.5 are sufficient to show ergodicity of the optimistic system (c.f., Theorem A.2 or Hairer & Mattingly (2011)). The rest of the proof is similar to Theorem 2.6. 

### Proof of Theorem 3.1

Since NeoRL works in artificial episodes \(n\in\{0,N-1\}\) of varying horizons \(H_{n}\). We denote with \(\bm{x}_{k}^{n}\) the state visited during episode \(n\) at time step \(k\leq H_{n}\). Crucial, to our regret analysis is bounding the first and second moment of \(V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})\) for all \(n,k\). Given the nature of Assumption 2.4, this requires analyzing geometric series. Thus, we start with the following elementary result of geometric series.

**Corollary A.6**.: _Consider the sequence \(\{S_{n}\}_{n\geq 0}\) with \(S_{n}\geq 0\) for all \(n\). Let the following hold_

\[S_{n}\leq\rho S_{n-1}+C\]

_for \(\rho\in(0,1)\) and \(C>0\). Then we have_

\[S_{n}\leq\rho^{n}S_{0}+C\frac{1}{1-\rho}.\]

Proof.: \[S_{n}\leq\rho S_{n-1}+C\leq\rho^{2}S_{n-2}+C(1+\rho)\leq\rho^{n}S_{0}+C\sum_{i =0}^{n}\rho^{i}\leq\rho^{n}S_{0}+C\frac{1}{1-\rho}.\]

**Lemma A.7**.: _Let Assumption 2.1 - 2.8 hold and let \(H_{0}\) be the smallest integer such that_

\[H_{0}>\frac{\log\left(\nicefrac{{C_{n}}}{{C_{l}}}\right)}{\log\left(\nicefrac{{ 1}}{{\gamma}}\right)}.\]

_Moreover, define \(\nu=\frac{C_{n}}{C_{l}}\gamma^{H_{0}}\). Note, by definition of \(H_{0}\), \(\nu<1\). Then we have for all \(k\in\{0,\dots,H_{n}\}\) and \(n>0\)_

Bounded expectation over horizon

\[\mathbb{E}_{\bm{x}_{k}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n}}( \bm{x}_{k}^{n})]\leq\gamma^{k}\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1}^{0}| \bm{x}_{0}}[V^{\bm{\pi}_{n}}(\bm{x}_{0}^{n})]+K/(1-\gamma).\] (12)

Bounded expectation over episodes

\[\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n}}( \bm{x}_{0}^{n})]\leq\nu^{n}V^{\bm{\pi}_{0}}(\bm{x}_{0})+\frac{C_{u}}{C_{l}}K/( 1-\gamma)\frac{1}{1-\nu}.\] (13)

_Moreover, we have_

\[\mathbb{E}_{\bm{x}_{k}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n}}( \bm{x}_{k}^{n})]\leq D(\bm{x}_{0},K,\gamma,\nu),\] (14)

_with \(D(\bm{x}_{0},K,\gamma,\nu)=V^{\bm{\pi}_{0}}(\bm{x}_{0})+K/(1-\gamma)\left( \frac{C_{u}}{C_{l}}\frac{1}{1-\nu}+1\right)\)_Proof.: We start with proving the first claim

\[\mathbb{E}_{\bm{x}_{k}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})] =\mathbb{E}_{\bm{x}_{k-1}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[ \mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}[V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})]]\] \[\leq\mathbb{E}_{\bm{x}_{k-1}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}} [\gamma V^{\bm{\pi}_{n}}(\bm{x}_{k-1}^{n})+K]\] (Assumption 2.4) \[=\gamma\mathbb{E}_{\bm{x}_{k-1}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0 }}[V^{\bm{\pi}_{n}}(\bm{x}_{k-1}^{n})]+K\]

We can apply Corollary A.6 to prove the claim. For the second claim, we note that for any \(\bm{\pi},\bm{\pi}^{\prime}\) and \(\bm{x}\in\mathcal{X}\) we have from Assumption 2.4

\[V^{\bm{\pi}}(\bm{x})\leq C_{u}\alpha(\|\bm{x}\|)\leq\frac{C_{u}}{C_{l}}V^{\bm {\pi}^{\prime}}(\bm{x}).\]

Therefore,

\[\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{ \bm{\pi}_{n}}(\bm{x}_{0}^{n})]\] \[\leq\frac{C_{u}}{C_{l}}\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1 }^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n-1}}(\bm{x}_{0}^{n})]\] \[=\frac{C_{u}}{C_{l}}\mathbb{E}_{\bm{x}_{H_{n}}^{n-1},\dots,\bm{x} _{1}^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n-1}}(\bm{x}_{H_{n}}^{n-1})]\] (Since \[\bm{x}_{0}^{n}=\bm{x}_{H_{n}}^{n-1}\] ) \[\leq\left(\frac{C_{u}}{C_{l}}\gamma^{H_{n}}\right)\mathbb{E}_{\bm {x}_{0}^{n-1},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n-1}}(\bm{x}_{0}^{ n-1})]+\frac{C_{u}}{C_{l}}K/(1-\gamma)\] (Equation ( 12 ))

For our choice of \(H_{0}\), we have for all \(n\geq 0\) that \(\frac{C_{u}}{C_{l}}\gamma^{H_{n}}\leq\frac{C_{u}}{C_{l}}\gamma^{H_{0}}\leq\nu<1\). From Corollary A.6, we get

\[\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{ \bm{\pi}_{n}}(\bm{x}_{0}^{n})] \leq\left(\frac{C_{u}}{C_{l}}\gamma^{H_{n}}\right)\mathbb{E}_{\bm {x}_{0}^{n-1},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{\bm{\pi}_{n-1}}(\bm{x}_{0}^{ n-1})]+\frac{C_{u}}{C_{l}}K/(1-\gamma)\] \[\leq\nu\mathbb{E}_{\bm{x}_{0}^{n-1},\dots,\bm{x}_{1}^{0}|\bm{x}_ {0}}[V^{\bm{\pi}_{n-1}}(\bm{x}_{0}^{n-1})]+\frac{C_{u}}{C_{l}}K/(1-\gamma)\] \[\leq\nu^{n}V^{\bm{\pi}_{0}}(\bm{x}_{0})+\frac{C_{u}}{C_{l}}K/(1- \gamma)\frac{1}{1-\nu}.\] (Corollary A.6 )

\[\mathbb{E}_{\bm{x}_{k}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V^{ \bm{\pi}_{n}}(\bm{x}_{k}^{n})] \leq\gamma^{k}\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1}^{0}|\bm {x}_{0}}[V^{\bm{\pi}_{n}}(\bm{x}_{0}^{n})]+K/(1-\gamma)\] (Equation ( 12 )) \[\leq\mathbb{E}_{\bm{x}_{0}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}[V ^{\bm{\pi}_{n}}(\bm{x}_{0}^{n})]+K/(1-\gamma)\] \[\leq\nu^{n}V^{\bm{\pi}_{0}}(\bm{x}_{0})+\frac{C_{u}}{C_{l}}K/(1- \gamma)\frac{1}{1-\nu}+K/(1-\gamma)\] (Equation ( 13 )) \[\leq V^{\bm{\pi}_{0}}(\bm{x}_{0})+\frac{C_{u}}{C_{l}}K/(1- \gamma)\frac{1}{1-\nu}+K/(1-\gamma)\]

**Lemma A.8**.: _Let Assumption 2.1 - 2.8 hold and let \(H_{0}\) be the smallest integer such that_

\[H_{0}>\frac{\log\left(\nicefrac{{C_{u}}}{{C_{l}}}\right)}{\log\left(\nicefrac{{1 }}{{\gamma}}\right)}.\]

_Moreover, define \(\nu=\frac{C_{u}}{C_{l}}\gamma^{H_{0}}\). Note, by definition of \(H_{0}\), \(\nu<1\)._

_Then we have for all \(k\in\{0,\dots,H_{n}\}\) and \(n>0\)_

Bounded second moment over horizon

\[\mathbb{E}_{\bm{x}_{k}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\left(V^{\bm {\pi}_{n}}(\bm{x}_{k}^{n})\right)^{2}\right]\leq\gamma^{2k}\mathbb{E}_{\bm{x}_ {0}^{n},\dots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n}}(\bm{x}_{0}^ {n})\right)^{2}\right]+\frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{2}}\] (15)

_with \(D_{2}(\bm{x}_{0},K,\gamma,\nu)=2K\gamma D(\bm{x}_{0},K,\gamma,\nu)+K^{2}+C_{ \bm{w}}\), and \(C_{\bm{w}}=\mathbb{E}_{\bm{w}}\left[\kappa^{2}(\|w\|)\right]+3(\mathbb{E}_{\bm{w} }\left[\kappa(\|w\|)\right])^{2}\)._Bounded second moment over episodes

\[\mathbb{E}_{\bm{x}_{0}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[(V^{\bm{\pi}_{ n}}(\bm{x}_{0}^{n}))^{2}\right]\leq\nu^{2n}\left(V^{\bm{\pi}_{0}}(\bm{x}_{0}) \right)^{2}+\left(\frac{C_{u}}{C_{l}}\right)^{2}\frac{D_{2}(\bm{x}_{0},K, \gamma,\nu)}{1-\gamma^{2}}\frac{1}{1-\nu^{2}}.\] (16)

_Moreover, let \(D_{3}(\bm{x}_{0},K,\gamma,\nu)=(V^{\bm{\pi}_{0}}(\bm{x}_{0}))^{2}+D_{2}(\bm{x}_ {0},K,\gamma,\nu)\left(\left(\frac{C_{u}}{C_{l}}\right)^{2}\frac{1}{1-\gamma^{2 }}\frac{1}{1-\nu^{2}}+\frac{1}{1-\gamma^{2}}\right)\)._

\[\mathbb{E}_{\bm{x}_{k}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[(V^{\bm{\pi}_ {n}}(\bm{x}_{k}^{n}))^{2}\right]\leq D_{3}(\bm{x}_{0},K,\gamma,\nu)\]

Proof.: Note that,

\[\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[(V^{\bm{\pi}_{n} }(\bm{x}_{k}^{n}))^{2}\right] =\left(\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})\right]\right)^{2}\] \[+\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[\left(V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})-\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[V ^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})\right]\right)^{2}\right].\]

We first bound the second term. Let \(\bar{\bm{x}}_{k}^{n}=\bm{f}^{*}(\bm{x}_{k-1}^{n},\bm{\pi}_{n}(\bm{x}_{k-1}^{n}))\), i.e., the next state in the absence of transition noise.

\[\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[\left(V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})-\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[V ^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})\right]\right)^{2}\right]\] \[=\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[\left(V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})-V^{\bm{\pi}_{n}}(\bar{\bm{x}}_{k}^{n})+V^{\bm{\pi}_{ n}}(\bar{\bm{x}}_{k}^{n})-\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})\right]\right)^{2}\right]\] \[=\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[\left(V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})-V^{\bm{\pi}_{n}}(\bar{\bm{x}}_{k}^{n})+\mathbb{E}_{ \bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[V^{\bm{\pi}_{n}}(\bar{\bm{x}}_{k}^{n})-V ^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})\right]\right)^{2}\right]\] \[\leq\mathbb{E}_{\bm{w}}\left[(\kappa(\|w\|)+\mathbb{E}_{\bm{w}}[ \kappa(\|w\|)])^{2}\right]\] (uniform continuity of \[V^{\bm{\pi}_{n}}\] ) \[=\mathbb{E}_{\bm{w}}\left[\kappa^{2}(\|\bm{w}\|)\right]+3( \mathbb{E}_{\bm{w}}\left[\kappa(\|w\|)\right])^{2}\] \[=C_{\bm{w}}\] (Assumption 2.4)

Therefore we have

\[\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[(V^{\bm{\pi}_{n}} (\bm{x}_{k}^{n}))^{2}\right] =\left(\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[V^{\bm{ \pi}_{n}}(\bm{x}_{k}^{n})\right]\right)^{2}+C_{\bm{w}}\] \[\leq(\gamma V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})+K)^{2}+C_{\bm{w}}\] \[=\gamma^{2}\left(V^{\bm{\pi}_{n}}(\bm{x}_{k-1}^{n})\right)^{2}+2K \gamma V^{\bm{\pi}_{n}}(\bm{x}_{k-1}^{n})+K^{2}+C_{\bm{w}}.\]

\[\mathbb{E}_{\bm{x}_{k}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[ \left(V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})\right)^{2}\right]\] \[=\mathbb{E}_{\bm{x}_{k-1}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}} \left[\mathbb{E}_{\bm{x}_{k}^{n}|\bm{x}_{k-1}^{n}}\left[(V^{\bm{\pi}_{n}}(\bm{x }_{k}^{n}))^{2}\right]\right]\] \[\leq\gamma^{2}\mathbb{E}_{\bm{x}_{k-1}^{n},\ldots,\bm{x}_{1}^{0}| \bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n}}(\bm{x}_{k-1}^{n})\right)^{2}\right]+2K \gamma\mathbb{E}_{\bm{x}_{k-1}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[V^{ \bm{\pi}_{n}}(\bm{x}_{k-1}^{n})\right]+K^{2}+C_{\bm{w}}\] \[\leq\gamma^{2}\mathbb{E}_{\bm{x}_{k-1}^{n},\ldots,\bm{x}_{1}^{0}| \bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n}}(\bm{x}_{k-1}^{n})\right)^{2}\right]+2K \gamma D(\bm{x}_{0},K,\gamma,\nu)+K^{2}+C_{\bm{w}}.\] (Lemma A.7)

Let \(D_{2}(\bm{x}_{0},K,\gamma,\nu)=2K\gamma D(\bm{x}_{0},K,\gamma,\nu)+K^{2}+C_{\bm{ w}}\). Applying Corollary A.6 we get

\[\mathbb{E}_{\bm{x}_{k}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[(V^{\bm{\pi}_{n} }(\bm{x}_{k}^{n}))^{2}\right]\leq\gamma^{2k}\mathbb{E}_{\bm{x}_{0}^{n}, \ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[(V^{\bm{\pi}_{n}}(\bm{x}_{0}^{n}))^{2} \right]+\frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{2}}\]

Similar to the first moment, we leverage that \(V^{\bm{\pi}_{n}}(\bm{x})\leq\frac{C_{u}}{C_{l}}V^{\bm{\pi}_{n-1}}(\bm{x})\) for all \(\bm{x}\in\mathcal{X}\), \(\frac{C_{u}}{C_{l}}\gamma^{H_{n-1}}\leq\nu\), and get,\[\leq\left(\frac{C_{u}}{C_{l}}\right)^{2}\mathbb{E}_{\bm{x}_{0}^{n-1},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n-1}}(\bm{x}_{0}^{n}) \right)^{2}\right]\] \[=\left(\frac{C_{u}}{C_{l}}\right)^{2}\mathbb{E}_{\bm{x}_{H_{n}}^{ n-1},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n-1}}(\bm{x}_{H_{n} }^{n-1})\right)^{2}\right]\] (Since \[\bm{x}_{0}^{n}=\bm{x}_{H_{n}}^{n-1}\] ) \[\leq\left(\frac{C_{u}}{C_{l}}\gamma^{H_{n}}\right)^{2}\mathbb{E}_ {\bm{x}_{0}^{n-1},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n- 1}}(\bm{x}_{0}^{n-1})\right)^{2}\right]+\left(\frac{C_{u}}{C_{l}}\right)^{2} \frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{2}}\] (Equation (15)) \[\leq\nu^{2}\mathbb{E}_{\bm{x}_{0}^{n-1},\ldots,\bm{x}_{1}^{0}|\bm {x}_{0}}\left[\left(V^{\bm{\pi}_{n-1}}(\bm{x}_{0}^{n-1})\right)^{2}\right]+ \left(\frac{C_{u}}{C_{l}}\right)^{2}\frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1- \gamma^{2}}\] \[\leq\nu^{2n}\left(V^{\bm{\pi}_{0}}(\bm{x}_{0})\right)^{2}+\left( \frac{C_{u}}{C_{l}}\right)^{2}\frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^ {2}}\frac{1}{1-\nu^{2}}\] (Corollary A.6)

Moreover,

\[\mathbb{E}_{\bm{x}_{k}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[ \left(V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})\right)^{2}\right]\] \[\leq\gamma^{2k}\mathbb{E}_{\bm{x}_{0}^{n},\ldots,\bm{x}_{1}^{0}| \bm{x}_{0}}\left[\left(V^{\bm{\pi}_{n}}(\bm{x}_{0}^{n})\right)^{2}\right]+ \frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{2}}\] (Equation (15)) \[\leq\mathbb{E}_{\bm{x}_{0}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}} \left[\left(V^{\bm{\pi}_{n}}(\bm{x}_{0}^{n})\right)^{2}\right]+\frac{D_{2}( \bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{2}}\] \[\leq\nu^{2n}\left(V^{\bm{\pi}_{0}}(\bm{x}_{0})\right)^{2}+\left( \frac{C_{u}}{C_{l}}\right)^{2}\frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{ 2}}\frac{1}{1-\nu^{2}}+\frac{D_{2}(\bm{x}_{0},K,\gamma,\nu)}{1-\gamma^{2}}\] (Equation (16)) \[\leq\left(V^{\bm{\pi}_{0}}(\bm{x}_{0})\right)^{2}+D_{2}(\bm{x}_{0},K,\gamma,\nu)\left(\left(\frac{C_{u}}{C_{l}}\right)^{2}\frac{1}{1-\gamma^{2} }\frac{1}{1-\nu^{2}}+\frac{1}{1-\gamma^{2}}\right)\]

Finally, we prove the regret bound of NeoRL.

Proof of Theorem 3.1.: In the following, let \(\hat{\bm{x}}_{k+1}^{n}=\bm{f}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))+ \bm{w}_{k}^{n}\) denote the state predicted under the optimistic dynamics and \(\bm{x}_{k+1}^{n}=\bm{f}_{n}^{*}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))+ \bm{w}_{k}^{n}\) the true state.

\[\mathbb{E}\left[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}c(\bm{x}_{k}^{ n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))-A(\bm{\pi}^{*})\right]\] \[\leq\mathbb{E}\left[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}c(\bm{x}_ {k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))-A(\bm{\pi}_{n},\bm{f}_{n})\right]\] (Optimism) \[=\mathbb{E}\left[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}B(\bm{\pi}_ {n},\bm{f}_{n},\bm{x}_{k}^{n})-B(\bm{\pi}_{n},\bm{f}_{n},\hat{\bm{x}}_{k+1}^{n})\right]\] (Bellman equation ( Equation ( 4 ))) \[=\mathbb{E}\left[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}B(\bm{\pi}_ {n},\bm{f}_{n},\bm{x}_{k}^{n})-B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})+B( \bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n},\hat{\bm{f}}_{n},\hat{ \bm{x}}_{k+1}^{n})\right]\] \[=\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}\left[B(\bm{\pi}_ {n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n},\bm{f}_{n},\hat{\bm{x}}_{k+1}^{n})\right]\] (A) \[+\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}\left[B(\bm{\pi}_ {n},\bm{f}_{n},\bm{x}_{k}^{n})-B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})\right]\] (B)

First, we study the term (A).

**Proof for (A)**: Note that because \(\bm{f}_{n}\in\mathcal{M}_{n}\), there exists a \(\bm{\eta}\in[-1,1]^{d_{x}}\) such that \(\hat{\bm{x}}_{k+1}^{n}=\bm{\mu}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))+ \beta_{n}\bm{\sigma}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))\bm{\eta}( \bm{x}_{k}^{n})+\bm{w}_{k}^{n}\). Furthermore, \(\bm{x}_{k+1}^{n}=\bm{f}^{*}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))+\bm{w}_ {k}^{n}\) and the transition noise is Gaussian. Let \(\zeta_{2,k}^{n}\) and \(\zeta_{1,k}^{n}\) denote the respective distributions of the two random variables, i.e., \(\zeta_{1,k}^{n}\sim\mathcal{N}(\bm{f}^{*}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^ {n})),\sigma^{2}\bm{I})\) and \(\zeta_{2,k}^{n}\sim\mathcal{N}(\bm{f}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k }^{n})),\sigma^{2}\bm{I})\). Next, define \(\bar{B}=\mathbb{E}_{\bm{x}\sim\zeta_{2,k}^{n}}\left[B(\bm{\pi}_{n},\bm{f}_{n}, \bm{x})\right]\), and consider the function \(h(\bm{x})=B(\bm{\pi}_{n},\bm{f}_{n},\bm{x})-\bar{B}\). Then we have

\[\mathbb{E}_{\bm{w}_{k}^{n}} \left[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n}, \bm{f}_{n},\hat{\bm{x}}_{k+1}^{n})\right]\] \[=\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[B(\bm{\pi}_{n},\bm{f }_{n},\bm{x})\right]-\mathbb{E}_{\bm{x}\sim\zeta_{2,k}^{n}}\left[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x})\right]\] \[=\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[B(\bm{\pi}_{n},\bm{f }_{n},\bm{x})-\bar{B}\right]-\mathbb{E}_{\bm{x}\sim\zeta_{2,k}^{n}}\left[B(\bm{ \pi}_{n},\bm{f}_{n},\bm{x})-\bar{B}\right]\] \[=\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}[h(\bm{x})]-\mathbb{E}_{\bm {x}\sim\zeta_{2,k}^{n}}[h(\bm{x})].\]

Note that \(\mathbb{E}_{\bm{x}\sim\zeta_{2,k}^{n}}[h(\bm{x})]=0\) by the definition of \(h\) and thus,

\[\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}[h(\bm{x})]-\mathbb{E}_{\bm{x}\sim\zeta_ {2,k}^{n}}[h(\bm{x})]=\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}[h(\bm{x})]\leq \sqrt{\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}[h^{2}(\bm{x})]}.\] (17)

In the following, we bound the term above w.r.t. the Chi-squared distance

\[\mathbb{E}_{\bm{w}_{k}^{n}} \left[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n}, \bm{f}_{n},\hat{\bm{x}}_{k+1}^{n})\right]=\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{ n}}[h(\bm{x})]-\mathbb{E}_{\bm{x}\sim\zeta_{2,k}^{n}}[h(\bm{x})]\] \[=\int_{\mathcal{X}}h(\bm{x})\left(1-\frac{\zeta_{2,k}^{n}}{\zeta_ {1,k}^{n}}\right)\zeta_{1,k}^{n}(d\bm{x})\leq\sqrt{\mathbb{E}_{\bm{x}\sim \zeta_{1,k}^{n}}\left[h^{2}(\bm{x})\right]}\sqrt{d_{\chi}(\zeta_{2,k}^{n}, \zeta_{1,k}^{n})}\] ((Kakade et al., 2020, Lemma C.2.,))

With \(d_{\chi}(\zeta_{2,k}^{n},\zeta_{1,k}^{n})\) being the Chi-squared distance.

\[d_{\chi}(\zeta_{2,k}^{n},\zeta_{1,k}^{n})=\int_{\mathcal{X}}\frac{\left(\zeta_ {1,k}^{n}-\zeta_{2,k}^{n}\right)^{2}}{\zeta_{1,k}^{n}}(d\bm{x})\]

Since both bounds from Equation (17) and bound we got by applying (Kakade et al., 2020, Lemma C.2.,), we can apply minimum and have:

\[\mathbb{E}_{\bm{w}_{k}^{n}}\left[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B (\bm{\pi}_{n},\bm{f}_{n},\hat{\bm{x}}_{k+1}^{n})\right]\leq\sqrt{\mathbb{E}_{ \bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm{x})\right]}\sqrt{\min\left\{d_{\chi}( \zeta_{2,k}^{n},\zeta_{1,k}^{n}),1\right\}}\]

Therefore, following Kakade et al. (2020, Lemma C.2.,) we get

\[\mathbb{E}_{\bm{w}_{k}^{n}} \left[B(\bm{\pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n}, \bm{f}_{n},\hat{\bm{x}}_{k+1}^{n})\right]\] \[\leq\sqrt{\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm {x})\right]}\min\left\{\nicefrac{{1}}{{\sigma}}\left\|\bm{f}^{*}(\bm{x}_{k}^{n },\bm{\pi}_{n}(\bm{x}_{k}^{n}))-\bm{f}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_ {k}^{n}))\right\|,1\right\}\] \[\leq\sqrt{\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm {x})\right]}(1+\sqrt{d_{x}})^{\nicefrac{{\beta_{n}}}{{\sigma}}}\left\|\bm{ \sigma}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))\right\|.\] (Sukhija et al., 2024, Cor. 3) (Sukhija et al., 2024, Cor. 3) (Sukhija et al., 2024, Cor. 3)

Therefore, we have

\[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n-1}}\mathbb{E}_{\bm{x}_{k}^{n}\ldots \bm{x}_{1}^{0}|\bm{x}_{0}}\left[\mathbb{E}_{\bm{w}_{k}^{n}}\left[B(\bm{\pi}_{n}, \bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n},\bm{f}_{n},\hat{\bm{x}}_{k+1}^{n}) \right]\right]\] \[\leq\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n-1}}\mathbb{E}_{\bm{x}_{k}^{n }\ldots\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\sqrt{\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n} }\left[h^{2}(\bm{x})\right]}(1+\sqrt{d_{x}})^{\nicefrac{{\beta_{n}}}{{\sigma}}} \left\|\bm{\sigma}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))\right\|\right]\] \[\leq\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n-1}}(1+\sqrt{d_{x}})^{ \nicefrac{{\beta_{n}}}{{\sigma}}}\sqrt{\mathbb{E}_{\bm{x}_{k}^{n}\ldots\bm{x}_{1}^{0}| \bm{x}_{0}}\left[\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm{x}) \right]\right]\mathbb{E}_{\bm{x}_{k}^{n}\ldots\bm{x}_{1}^{0}|\bm{x}_{0}} \left[\left\|\bm{\sigma}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{x}_{k}^{n})) \right\|^{2}\right]}\] \[\leq(1+\sqrt{d_{x}})^{\nicefrac{{\beta_{T}}}{{\sigma}}}\sqrt{ \sum_{n=0}^{N-1}\sum_{k=0}^{H_{n-1}}\mathbb{E}_{\bm{x}_{k}^{n}\ldots\bm{x}_{1}^{0}| \bm{x}_{0}}\left[\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm{x}) \right]\right]\] \[\times\sqrt{\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n-1}}\mathbb{E}_{\bm{x}_{k}^{n }\ldots\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\left\|\bm{\sigma}_{n}(\bm{x}_{k}^{n}, \bm{\pi}_{n}(\bm{x}_{k}^{n}))\right\|^{2}\right]}\]Here, for the second and third inequality, we use Cauchy-Schwarz. Now we bound the two terms above individually.

First we bound \(\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm{x})\right]\).

\[\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm{x})\right] =\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[(B(\bm{\pi}_{n},\bm{f }_{n},\bm{x})-\bar{B})^{2}\right]\] \[=\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[(B(\bm{\pi}_{n},\bm{ f}_{n},\bm{x})-\mathbb{E}_{\bm{x}\sim\zeta_{2,k}^{n}}\left[B(\bm{\pi}_{n},\bm{f }_{n},\bm{x})\right])^{2}\right]\] \[\leq\left(\frac{C_{2}}{1-\tilde{\lambda}}\right)^{2}\mathbb{E}_{ \bm{x}\sim\zeta_{1,k}^{n}}\left[(2+V^{\bm{\pi}_{n}}(\bm{x})+\mathbb{E}_{\bm{x} \sim\zeta_{2,k}^{n}}\left[V^{\bm{\pi}_{n}}(\bm{x})\right])^{2}\right]\] (Theorem A.3) \[\leq\left(\frac{C_{2}}{1-\tilde{\lambda}}\right)^{2}\mathbb{E}_{ \bm{x}\sim\zeta_{1,k}^{n}}\left[(2+V^{\bm{\pi}_{n}}(\bm{x})+\gamma V^{\bm{\pi} _{n}}(\bm{x}_{k}^{n})+\hat{K})^{2}\right]\] (Lemma A.4) \[\leq\left(\frac{\sqrt{2}C_{2}}{1-\tilde{\lambda}}\right)^{2} \mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[(V^{\bm{\pi}_{n}}(\bm{x}))^{2}+(2+ \gamma V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n})+\hat{K})^{2}\right]\] \[\leq\left(\frac{\sqrt{2}C_{2}}{1-\tilde{\lambda}}\right)^{2} \left(\mathbb{E}_{\bm{x}_{k+1}^{n}|\bm{x}_{k}^{n}}\left[(V^{\bm{\pi}_{n}}(\bm{ x}_{k+1}))^{2}\right]+2\gamma^{2}(V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n}))^{2}+2(2+\hat{K})^{2}\right)\]

Furthermore, we have from Lemma A.8.

\[\mathbb{E}_{\bm{x}_{k}^{n},\ldots,\bm{x}_{1}^{0}|\bm{x}_{0}}\left[ \mathbb{E}_{\bm{x}_{k+1}^{n}|\bm{x}_{k}^{n}}\left[(V^{\bm{\pi}_{n}}(\bm{x}_{k+ 1}))^{2}\right]+2\gamma^{2}(V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n}))^{2}\right]\] \[=\mathbb{E}_{\bm{x}_{k+1}^{n}|\bm{x}_{0}}\left[(V^{\bm{\pi}_{n}}( \bm{x}_{k+1}))^{2}\right]+2\gamma^{2}\mathbb{E}_{\bm{x}_{k}^{n},\ldots\bm{x}_{ 1}^{0}|\bm{x}_{0}}\left[(V^{\bm{\pi}_{n}}(\bm{x}_{k}^{n}))^{2}\right]\leq(1+2 \gamma^{2})D_{3}(\bm{x}_{0},K,\gamma,\nu).\]

In the end, we get

\[\sqrt{\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}_{\bm{x}_{k}^ {n},\ldots\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n }}\left[h^{2}(\bm{x})\right]\right]}\] \[\leq\left(\frac{\sqrt{2}C_{2}}{1-\tilde{\lambda}}\right)\sqrt{ \sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}(1+2\gamma^{2})D_{3}(\bm{x}_{0},K,\gamma, \nu)+2(2+\hat{K})^{2}}\] \[=\left(\frac{\sqrt{2}C_{2}}{1-\tilde{\lambda}}\right)\sqrt{(1+2 \gamma^{2})D_{3}(\bm{x}_{0},K,\gamma,\nu)+2(2+\hat{K})^{2}}\sqrt{\sum_{n=0}^{ N-1}H_{n}}\] \[=\left(\frac{\sqrt{2}C_{2}}{1-\tilde{\lambda}}\right)\sqrt{(1+2 \gamma^{2})D_{3}(\bm{x}_{0},K,\gamma,\nu)+2(2+\hat{K})^{2}}\sqrt{T}.\]

Next, we use the bound from Curi et al. (2020, Lemma 17.) for the second term.

\[\sqrt{\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}_{\bm{x}_{k}^{n},\ldots\bm {x}_{1}^{0}|\bm{x}_{0}}\left[\|\bm{\sigma}_{n}(\bm{x}_{k}^{n},\bm{\pi}_{n}(\bm{ x}_{k}^{n}))\|^{2}\right]}\leq C^{\prime}\sqrt{\Gamma_{T}}\]

Here \(\Gamma_{T}\) is the maximum information gain.

If we set \(D_{4}(\bm{x}_{0},K,\gamma)=\frac{C^{\prime}(1+\sqrt{d_{x}})}{\sigma}\left( \frac{\sqrt{2}C_{2}}{1-\tilde{\lambda}}\right)\sqrt{(1+2\gamma^{2})D_{3}(\bm{x }_{0},K,\gamma,\nu)+2(2+\hat{K})^{2}}\), we have

\[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}_{\bm{x}_{k}^{n}, \ldots\bm{x}_{1}^{0}|\bm{x}_{0}}\left[\mathbb{E}_{\bm{w}_{k}^{n}}\left[B(\bm{ \pi}_{n},\bm{f}_{n},\bm{x}_{k+1}^{n})-B(\bm{\pi}_{n},\bm{f}_{n},\hat{\bm{x}}_{k +1}^{n})\right]\right]\] \[\leq(1+\sqrt{d_{x}})^{\beta_{T}/\sigma}\sqrt{\sum_{n=0}^{N-1} \sum_{k=0}^{H_{n}-1}\mathbb{E}_{\bm{x}_{k}^{n},\ldots\bm{x}_{1}^{0}|\bm{x}_{0}} \left[\mathbb{E}_{\bm{x}\sim\zeta_{1,k}^{n}}\left[h^{2}(\bm{x})\right]\right]}\]\[\times\sqrt{\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}_{\bm{x}_{k}^ {n},\ldots\bm{x}_{l}^{n}|\bm{x}_{0}}\left[\|\bm{\sigma}_{n}(\bm{x}_{k}^{n},\bm{ \pi}_{n}(\bm{x}_{k}^{n}))\|^{2}\right]}\] \[\leq(1+\sqrt{d}_{x})^{\beta_{T}/\sigma}\left(\frac{\sqrt{2}C_{2}} {1-\tilde{\lambda}}\right)\sqrt{(1+2\gamma^{2})D_{3}(\bm{x}_{0},K,\gamma, \nu)+2(2+\tilde{K})^{2}}\sqrt{T}C^{\prime}\sqrt{\Gamma_{T}}\] \[\leq D_{4}(\bm{x}_{0},K,\gamma)\beta_{T}\sqrt{T\Gamma_{T}}\]

**Proof for (B)**:

\[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}\mathbb{E}\left[B(\bm{\pi}, \bm{f}_{n},\bm{x}_{k}^{n})-B(\bm{\pi},\bm{f}_{n},\bm{x}_{k+1}^{n})\right]=\sum_ {n=0}^{N-1}\mathbb{E}\left[B(\bm{\pi},\bm{f}_{n},\bm{x}_{0}^{n})-B(\bm{\pi}, \bm{f}_{n},\bm{x}_{H_{n}}^{n})\right]\] \[\leq\frac{C_{2}}{1-\tilde{\lambda}}\sum_{n=0}^{N-1}\left(2+ \mathbb{E}\left[V^{\bm{\pi}}(\bm{x}_{0}^{n})+V^{\bm{\pi}}(\bm{x}_{H_{n}}^{n}) \right]\right)\] (Theorem A.3) \[\leq\frac{2C_{2}}{1-\tilde{\lambda}}\sum_{n=0}^{N-1}\left(1+D(\bm {x}_{0},K,\gamma)\right)\] (Lemma A.7) \[=\frac{2C_{2}}{1-\tilde{\lambda}}(1+D(\bm{x}_{0},K,\gamma))N\] \[=D_{5}(\bm{x}_{0},K,\gamma)N.\]

Here \(D_{5}(\bm{x}_{0},K,\gamma)=\frac{2C_{2}}{1-\tilde{\lambda}}(1+D(\bm{x}_{0},K, \gamma))\). Finally, for our choice, \(H_{n}=H_{0}2^{n}\), we get

\[\sum_{n=0}^{N-1}H_{n}=H_{0}\sum_{n=0}^{N-1}2^{n}=H_{0}(2^{N}-1)=T.\]

Therefore, \(N=\log_{2}\left(\frac{T}{H_{0}}+1\right)\). To this end, we get for our regret

\[R_{T} =\mathbb{E}\left[\sum_{n=0}^{N-1}\sum_{k=0}^{H_{n}-1}c(\bm{x}_{k}^ {n},\bm{\pi}_{n}(\bm{x}_{k}^{n}))-A(\bm{\pi}^{*})\right]\] \[\leq D_{4}(\bm{x}_{0},K,\gamma)\beta_{T}\sqrt{T\Gamma_{T}}+D_{5}( \bm{x}_{0},K,\gamma)N\] \[\leq D_{4}(\bm{x}_{0},K,\gamma)\beta_{T}\sqrt{T\Gamma_{T}}+D_{5}( \bm{x}_{0},K,\gamma)\log_{2}\left(\frac{T}{H_{0}}+1\right)\]

This regret is sublinear for a very rich class of functions. We summarize bounds on \(\Gamma_{T}\) from Vakili et al. (2021) in Table 1. Furthermore, note that \(D_{4}(\bm{x}_{0},K,\gamma)\in(0,\infty)\) for all \(\bm{x}_{0}\in\mathcal{X}\) with \(\|\bm{x}_{0}\|<\infty\), \(K<\infty\), \(\gamma\in(0,1)\). The same holds for \(D_{5}(\bm{x}_{0},K,\gamma)\). Moreover, since \(V^{\bm{\pi}}(\bm{x})\) is \(\Theta(\zeta(\|\bm{x}\|))\), both \(D_{4}\) and \(D_{5}\) are \(\Theta(\zeta(\|\bm{x}_{0}\|))\).

\begin{table}
\begin{tabular}{l l l} \hline Kernel & \(k(\bm{x},\bm{x}^{\prime})\) & \(\Gamma_{T}\) \\ \hline Linear & \(\bm{x}^{\top}\bm{x}^{\prime}\) & \(\mathcal{O}\left(d\log(T)\right)\) \\ RBF & \(e^{-\frac{\|\bm{x}-\bm{x}^{\prime}\|^{2}}{2t^{2}}}\) & \(\mathcal{O}\left(\log^{d+1}(T)\right)\) \\ Matrn & \(\frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}\left\|\bm{x}-\bm{x}^{ \prime}\right\|}{l}\right)^{\nu}B_{\nu}\left(\frac{\sqrt{2\nu}\left\|\bm{x}- \bm{x}^{\prime}\right\|}{l}\right)\) & \(\mathcal{O}\left(T^{\frac{d}{2\nu+d}}\log^{\frac{2\nu}{2\nu+d}}(T)\right)\) \\ \hline \end{tabular}
\end{table}
Table 1: Maximum information gain bounds for common choice of kernels.

[MISSING_PAGE_FAIL:26]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We highlight the problem setting, algorithm, and theoretical and empirical results in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 2 we highlight the assumptions of our work, which also correspond to the limitations of our theoretical analysis and also the setting for which our algorithm yields theoretical guarantees. Further, in [2] we discuss an alternative set of assumptions to the one made in the main paper. Limitations to the theoretical algorithm are discussed in Section 3.2, where also practical modifications are proposed. In our experiments (Section 4) we evaluate our algorithm on settings where the assumptions are not necessarily satisfied. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All theoretical results are accompanied by the relevant assumptions that are listed in Section 2 and we provide all proofs in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We use open-source benchmarks, disclose all hyperparameters in Appendix B, and the practical algorithm is explained in Section 3.2. Furthermore, we provide the code as supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code as supplementary material and the hyperparameters used in our experiments in Appendix B. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experiment details are explained thoroughly in Section 3.2 and Section 4. Furthermore, all hyperparameters are listed in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments are run with 10 seeds and mean performance with standard error is reported in all our plots. Guidelines: ** The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We give details on computation time in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms, in every respect, to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: This paper proposes a method to improve exploration in reinforcement learning in the nonepisodic setting, and is not tied to specific applications. As such, it shares the many potential societal consequences that are associated with reinforcement learning and automation as a whole, spanning from environmental impact to concerns on ethics and alignment. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release high-risk data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite all creators whose code we used in our experiments in Section 4. Guidelines: * The answer NA means that the paper does not use existing assets.

* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the code as supplementary material including a readme that explains how to install and run the code. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.