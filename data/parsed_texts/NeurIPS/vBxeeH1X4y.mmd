# 2D-OOB: Attributing Data Contribution Through

Joint Valuation Framework

 Yifan Sun

University of Illinois Urbana-Champaign

yifan50@illinois.edu

&Jingyan Shen

Columbia University

js5544@columbia.edu

&Yongchan Kwon

Columbia University

yk3012@columbia.edu

Equal contribution.Corresponding author.

###### Abstract

Data valuation has emerged as a powerful framework for quantifying each datum's contribution to the training of a machine learning model. However, it is crucial to recognize that the quality of _cells_ within a single data point can vary greatly in practice. For example, even in the case of an abnormal data point, not all cells are necessarily noisy. The single scalar score assigned by existing data valuation methods blurs the distinction between noisy and clean cells of a data point, making it challenging to interpret the data values. In this paper, we propose 2D-OOB, an out-of-bag estimation framework for jointly determining helpful (or detrimental) samples as well as the particular cells that drive them. Our comprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art performance across multiple use cases while being exponentially faster. Specifically, 2D-OOB shows promising results in detecting and rectifying fine-grained outliers at the cell level, and localizing backdoor triggers in data poisoning attacks.

## 1 Introduction

From customer behavior prediction and medical image analysis to autonomous driving and policy making, machine learning (ML) systems process ever increasing amounts of data. In such data-rich regimes, a fraction of the samples is often noisy, incorrect annotations are likely to occur, and uniform data quality standards become difficult to enforce. To address these challenges, data valuation emerges as a research field receiving increasing attention, focusing on properly assessing the contribution of each datum to ML training [13, 26]. These methods have proven useful in identifying low-quality samples that can be detrimental to model performance, as well as selecting subsets of data that are representative of enhanced model performance [52, 30, 49]. Furthermore, they are widely applicable in data marketplace for fair revenue allocation and incentive design [55, 48, 43, 5].

Nevertheless, existing data valuation methods assign a scalar score to each datum, thereby failing to account for the varied roles of individual cells. This leaves the valuation rationale unclear and can be unsatisfactory and sub-optimal in various practical scenarios. Firstly, whenever a score is assigned to a data point by a particular data valuation method, it is crucial to understand the underlying justifications to ensure transparency and reliability, especially in high-stakes decision making [42]. Secondly, it is important to recognize the fact that even if a data point is of low quality, it is rarely the case that all the cells within this data point are noisy [40, 29, 46]. The absence of detailed insightsinto how individual cells contribute to ML training inevitably leads to discarding the entire data point. This can result in substantial data waste, particularly when only a few cells are noisy and data acquisition is expensive. Finally, in data markets, different cells within a data point may originate from different data sellers [3; 11]. Consequently, a singular valuation for the entire point fails to offer equitable compensation to all contributing parties.

Our contributionsIn this paper, we propose 2D-OOB, a powerful and efficient joint valuation framework that can attribute a data point's value to its features. 2D-OOB quantifies the importance of each _cell_ in a dataset, as illustrated in Figure 1, providing interpretable insights into which cells are associated with influential data points. Our method is computationally efficient as well as theoretically supported by its connections with Data-OOB[26]. Moreover, our extensive empirical experiments demonstrate the practical effectiveness of 2D-OOB in various use cases. 2D-OOB accurately identifies cell outliers and pinpoints which cells to fix to improve model performance. Additionally, 2D-OOB enables inspection of data poisoning attacks by precisely localizing the backdoor trigger, an artifact inserted into a training sample to induce malicious model behavior [14; 6]. On average, 2D-OOB is \(200\) times faster than state-of-the-art methods across all datasets examined.

## 2 Preliminaries

NotationsThroughout this paper, we focus on supervised learning settings. For \(d\in\mathbb{N}\), we denote an input space and an output space by \(\mathcal{X}\subseteq\mathbb{R}^{d}\) and \(\mathcal{Y}\subseteq\mathbb{R}\), respectively. We denote a training dataset with \(n\) data points by \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{n}\) where \((x_{i},y_{i})\) is the \(i\)-th pair of the input covariates \(x_{i}\in\mathcal{X}\) and its output label \(y_{i}\in\mathcal{Y}\). For an event \(A\), an indicator function \(\mathds{1}(A)\) is \(1\) if \(A\) is true, otherwise \(0\). For \(j\in\mathbb{N}\), we set \([j]:=\{1,\dots,j\}\). For a set \(S\), we denote its power set by \(2^{S}\) and its cardinality by \(|S|\).

DataShapleyThe primary goal of data valuation is to quantify the contribution of individual data points to a model's performance. Leveraging the Shapley value in cooperative game theory [41], DataShapley[13] measures the average change in a utility function \(U:2^{\mathcal{D}}\rightarrow\mathbb{R}\) over all possible subsets of the dataset that either include or exclude the data point. For \(i\in[n]\), DataShapley of \(i\)-th datum is defined as follows.

\[\phi_{i}^{\mathrm{Shap}}:=\frac{1}{n}\sum_{k=1}^{n}\frac{1}{\binom{n-1}{k-1}} \sum_{S\in\mathcal{P}_{k}^{(i)}}[U(S\cup\{(x_{i},y_{i})\})-U(S)],\] (1)

Figure 1: **Comparison of data valuation and joint valuation. (a) Data valuation evaluates the quality of individual data points, whereas (b) joint valuation evaluates the quality of individual cells. Both panels illustrate the same hypothetical dataset, while darker colors indicate higher quality or importance. As illustrated in panel (a), data valuation can only identify that the third and fifth data points are of low quality, but it lacks further feature-level attribution. This limitation may result in discarding the entire data point, even when only certain cells are problematic. In contrast, joint valuation provides a finer level of attribution than data valuation and aims to reveal how individual features contribute to data values. As shown in panel (b), the joint valuation framework can identify outlier cells (highlighted by blue boxes), such as \(-1\) in “Income” and \(100\) in “Education”, providing detailed interpretations of data values.**

where \(\mathcal{D}_{i}^{(i)}:=\{S\subseteq\mathcal{D}|(x_{i},y_{i})\notin S,|S|=k-1\}\). DataShapley\(\phi_{i}^{\mathrm{snap}}\) in (1) considers every set \(S\in\mathcal{D}_{k}^{(i)}\) and computes the average difference in utility \(U(S\cup\{(x_{i},y_{i})\})-U(S)\). It characterizes the impact of a data point, but its computation requires evaluating \(U\) for all possible subsets of \(\mathcal{D}\), rendering precise calculations infeasible. Many efficient computation algorithms have been proposed [16; 28; 54], and in these studies, Shapley-based methods have demonstrated better effectiveness in detecting low-quality samples than standard attribution approaches, such as leave-one-out and influence function methods [22; 10].

Data-OOBAs an alternative efficient data valuation method, Kwon and Zou [26] propose Data-OOB, which leverages a bagging model and measures the similarity between the nominal label and weak learners' predictions. To be more specific, consider a bagging model consisting of \(B\) weak learners, where for \(b\in[B]\), the \(b\)-th weak learner \(\hat{h}_{b}\) is given as a minimizer of the weighted empirical risk,

\[\hat{h}_{b}:=\mathrm{argmin}_{h}\sum_{i=1}^{n}w_{bi}\ell(y_{i},h(x_{i})),\]

where \(\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) is a loss function and \(w_{bi}\in\mathbb{N}\) is the number of times the \(i\)-th datum \((x_{i},y_{i})\) is selected by the \(b\)-th bootstrap dataset. Let \(\mathbf{w}_{b}\) be a weight vector \(\mathbf{w}_{b}:=(w_{b1},\ldots,w_{bn})\) for all \(b\in[B]\). For \(i\in[n]\) and \(\{(\mathbf{w}_{b},\hat{h}_{b})\}_{b=1}^{B}\), Data-OOB of the \(i\)-th datum is defined as follows.

\[\phi_{i}^{\mathrm{OOB}}:=\frac{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)T(y_{i},\hat{ h}_{b}(x_{i}))}{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)},\] (2)

where \(T(y_{i},\hat{h}_{b}(x_{i}))\) is a score function evaluated at \((x_{i},y_{i})\). We assume that the higher \(T\), the better the prediction. In classification settings, a common choice for \(T\) is \(\mathds{1}(y_{i}=\hat{h}_{b}(x_{i}))\), and in this case, Data-OOB\(\phi_{i}^{\mathrm{OOB}}\) measures the average similarity between a nominal label \(y_{i}\) and weak learners' predictions \(\hat{h}_{b}(x_{i})\) when a datum \((x_{i},y_{i})\) is _not_ sampled in a bootstrap dataset. It intuitively captures the quality of a data point. For instance, when \((x_{i},y_{i})\) is a mislabeled sample or an outlier, the label \(y_{i}\) is likely to differ from \(\hat{h}_{b}(x_{i})\), resulting in \(\phi_{i}^{\mathrm{OOB}}\) being close to zero.

It is noteworthy that Data-OOB in (2) can be computed by training a single bagging model, making it computationally efficient. Kwon and Zou [26] show that Data-OOB can easily scale to millions of data points, but for DataShapley this is often very impractical. In addition, Data-OOB is typically comparable to or even more effective than DataShapley in detecting mislabeled data points and selecting helpful data points [26; 17].

## 3 Attributing Data Contribution through Joint Valuation Framework

Data valuation quantifies the utility of data points, however, it fails to identify which features contribute to these data values and to what extent. For instance, in anomaly detection tasks, data valuation methods can be deployed to detect anomalous data points but cannot explain why they are considered abnormal, which is generally not desirable in practice. To address this challenge, we introduce a joint valuation framework that assigns _a cell score_ to each feature of a data point. Here, a cell score is designed to quantify how each feature affects the value of an individual data point, thereby attributing the data value to specific features.

To the best of the author's knowledge, Liu et al. [32] were the first to consider the concept of joint valuation in the literature, proposing 2D-Shapley as a means to quantitatively interpret DataShapley. To formalize this, we denote a 2D utility function by \(u:[n]\times[d]\rightarrow\mathbb{R}\), which takes as input a subset of data points \(S\subseteq[n]\) and a subset of features \(F\subseteq[d]\), measuring the utility of a fragment of the given dataset consisting of cells \(\{(i,j)\}_{i\in S,j\in F}\), where a tuple \((i,j)\) denotes a cell at the \(i\)-th datum and the \(j\)-th column. Then, 2D-Shapley is defined as

\[\psi_{ij}^{\mathrm{2D-Shap}}:=\frac{1}{nd}\sum_{k=1}^{n}\sum_{l=1}^{d}\frac{1} {\binom{n-1}{k-1}\binom{d-1}{l-1}}\sum_{(S,F)\in\mathcal{D}_{k,l}^{(i,j)}}M_{u} ^{i,j}(S,F)\] (3)

where \(\mathcal{D}_{k,l}^{(i,j)}:=\{(S,F)|S\subseteq[n]\backslash\{i\},F\subseteq[d] \backslash\{j\},|S|=k-1,|F|=l-1\}\) and

\[M_{u}^{i,j}(S,F)=u(S\cup\{i\},F\cup\{j\})+u(S,F)-u(S\cup\{i\},F)-u(S,F\cup\{j \}).\]The function \(M_{u}^{i,j}\) allows us to quantify how much removing a specific cell at \((i,j)\) from a given set \((S\cup\{i\},F\cup\{j\})\) affects the overall utility, and \(\tt{2D\text{-}Shapley}\)\(\psi_{ij}^{\tt{2D-Shap}}\) evaluates the average \(M_{u}^{i,j}\) across all possible data fragments \((S,F)\in\mathcal{D}_{k,l}^{(i,j)}\).

Similar to DataShapley, the permutation of all rows and columns required for exact \(\tt{2D\text{-}Shapley}\) calculations presents significant computational challenges. To address this, Liu et al. [32] develop \(\tt{2D\text{-}KNN}\), which utilizes \(k\)-nearest-neighbors models as surrogates to approximate \(\tt{2D\text{-}Shapley}\) values. However, the approximation methods can compromise the accuracy of valuations [26; 17]. Additionally, \(\tt{2D\text{-}KNN}\) still faces challenges scaling to large-scale datasets and high-dimensional settings.

To address these limitations, we propose \(\tt{2D\text{-}OOB}\), an _efficient_ and _model-agnostic_ joint valuation framework that leverages out-of-bag estimation to attribute data contribution. We also illustrate how \(\tt{2D\text{-}OOB}\) is connected to Data-OOB, thereby facilitating sample-wise feature-level interpretation for data valuation, as discussed in Section 3.2.

### 2D-OOB: an efficient joint valuation framework

Our idea builds upon the subset bagging model [15], which is well recognized as an earlier version of Breiman's random forest model [4]. A key distinction from a standard bagging model is that a weak learner in a subset bagging model is trained on a randomly selected subset of features. For \(b\in[B]\), we denote the \(b\)-th random feature subset by \(S_{b}\subseteq[d]\). Then, the \(b\)-th weak learner of a subset bagging model is given as follows.

\[\hat{f}_{b}:=\operatorname*{argmin}_{f}\sum_{i=1}^{n}w_{bi}\ell(y_{i},f(x_{i,S_ {b}})),\]

where \(x_{i,S_{b}}\) is a subvector of \(x_{i}\) that only takes elements in a subset \(S_{b}\). This difference enables us to assess the impact of which features are more influential: if \(S_{b}\) includes a helpful (or detrimental) feature, we can expect the out-of-bag prediction \(\hat{f}(x_{i,S_{b}})\) to be good (or poor). We formalize this intuition and propose \(\tt{2D\text{-}OOB}\). For \(i\in[n]\), \(j\in[d]\) and \(\{(\mathbf{w}_{b},S_{b},\hat{f}_{b})\}_{b=1}^{B}\), the \(\tt{2D\text{-}OOB}\) for the \(j\)-th cell of the \(i\)-th data point is defined as follows,

\[\psi_{ij}^{\tt{2D-OOB}}:=\frac{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0,j\in S_{b})T( y_{i},\hat{f}_{b}(x_{i,S_{b}}))}{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0,j\in S_{b})},\] (4)

where \(T:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) is a utility function that scores the performance of the weak learner \(\hat{f}_{b}(x_{i,S_{b}})\) on the \(i\)-th datum \((x_{i},y_{i})\). Specifically, for binary or multi-class classification problems, we can adopt \(T(y_{i},\hat{f}_{b}(x_{i,S_{b}}))=\mathds{1}(y_{i}=\hat{f}_{b}(x_{i,S_{b}}))\). In this case, \(\tt{2D\text{-}OOB}\) measures the average accuracy score of out-of-bag predictions (specifically, when the \(i\)-th data point is out-of-bag) if the \(j\)-th feature is used in training \(\hat{f}_{b}\). For regression problems, we can use the negative squared error loss function, defined as \(T(y_{i},\hat{f}_{b}(x_{i,S_{b}}))=-(y_{i}-\hat{f}_{b}(x_{i,S_{b}}))^{2}\). In practice, \(\mathcal{X}\) could also be incorporated into \(T\) to suit the specific use case.

While Data-OOB in (2) aims to assess the impact of the \(i\)-th datum, \(\tt{2D\text{-}OOB}\) in (4) further provides interpretable insights by evaluating the data point with various combinations of features, revealing which cells are influential to model performance. By leveraging the subset bagging scheme, \(\tt{2D\text{-}OOB}\) only requires a single training of the bagging model, making it computationally efficient.

### Connection to Data-OOB

We now present interpretable expressions of how \(\tt{2D\text{-}OOB}\) connects to Data-OOB in the following proposition. To begin with, we denote a set of subsets of \([d]\) by \(\mathcal{S}:=\{S\subseteq[d]\}\). With \(\{(\mathbf{w}_{b},\hat{f}_{b})\}_{b=1}^{B}\), we define the \(i\)-th Data-OOB when a particular subset \(S\) is used as follows and denote it by \(\phi_{i}^{\rm{OOB}}(S)\).

\[\phi_{i}^{\rm{OOB}}(S):=\frac{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)T(y_{i},\hat{f }_{b}(x_{i,S}))}{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)}.\]

**Proposition 3.1**.: _For all \(i\in[n]\) and \(j\in[d]\), \(\psi_{ij}^{\tt{2D-OOB}}\) can be expressed as follows._

\[\psi_{ij}^{\tt{2D-OOB}}=\mathbb{E}_{\hat{F}_{S}}[\phi_{i}^{\rm{OOB}}(S)\mid j \in S],\]_where \(\hat{F}_{S}\) is an empirical distribution with respect to \(S\) induced by the sampling process._

A proof is given in Appendix C. Proposition 3.1 shows that \(\mathtt{2D-OOB}\psi_{ij}^{\mathtt{2D-OOB}}\) can be expressed as a conditional empirical expectation of \(\mathtt{Data-OOB}\) provided that the \(j\)-th feature is used in \(\mathtt{Data-OOB}\) computation. It provides intuitive interpretations: for a fixed \(i\) and \(j\neq k\), \(\psi_{ij}^{\mathtt{2D-OOB}}>\psi_{ik}^{\mathtt{2D-OOB}}\) implies that the cell \(x_{ij}\) is more helpful to achieve a higher OOB score than the cell \(x_{ik}\), where the OOB score serves as an indicator of model performance. By distinguishing the contributions of individual cells, \(\mathtt{2D-OOB}\) effectively realizes joint valuation, providing a finer granularity of analysis that links feature-level importance to individual data quality.

## 4 Experiments

In this section, we empirically show the effectiveness of \(\mathtt{2D-OOB}\) across multiple use cases of the joint valuation: _cell-level outlier detection_, _cell fixation_, and _backdoor trigger detection_. To the best of our knowledge, the latter two use cases are introduced here for the first time within the joint valuation framework. As a summary, \(\mathtt{2D-OOB}\) can precisely identify anomalous cells that should be prioritized for examination and subsequent fixation to improve model performance. In the context of backdoor trigger detection, \(\mathtt{2D-OOB}\) demonstrates its efficacy by accurately identifying different types of triggers within poisoned data, showcasing its proficiency in detecting non-random, targeted anomalies. Our method also exhibits high computational efficiency through run-time comparison.

Throughout all of our experiments, \(\mathtt{2D-OOB}\) uses a subset bagging model with \(B=1000\) decision trees. We randomly select a fixed ratio of features to build each decision tree. Unless otherwise specified, we utilize half of the features for each weak learner and set \(T(y_{i},\hat{f}(x_{i,S_{0}}))=\mathds{1}(y_{i}=\hat{f}(x_{i,S_{0}}))\). The run time is measured on a single Intel Xeon Gold 6226 2.9 GHz CPU processor. We provide a detailed ablation study of key hyperparameters in Section 4.4.

### Cell-level outlier detection

Experimental settingIn practical situations, even when dealing with abnormal data points, it is not always the case that all cells are noisy [40; 32; 23]. To simulate more realistic settings, we introduce noise to certain _cells_ in the following two-step process: First, we randomly select \(20\%\) rows for each dataset. We then select \(20\%\) columns uniformly at random, allowing each selected row to have a different set of perturbed cells. We inject noises sampled from the low-probability region into these cells, following Du et al. [9] and Liu et al. [32]. Details on the outlier injection process can be found in Appendix A.3.

We use \(12\) publicly accessible binary classification datasets from OpenML, encompassing a range of both low and high-dimensional datasets, which have been widely used in the literature [13; 25; 26]. Details on these datasets are presented in Appendix A.1. For each dataset, \(1000\) and \(3000\) data points are randomly sampled for training and test datasets, respectively. For the baseline method, we consider \(\mathtt{2D-KNN}\), a fast and performant variant of \(\mathtt{2D-Shapley}\)[32]. We incorporate a distance regularization term in the utility function \(T\) for enhanced performance.

ResultsWe calculate the valuations for each cell using our joint valuation framework. Ideally, the outlier cells should receive a low valuation. We then arrange the cell valuations in _ascending_ order and inspect those cells with the lowest values first.

The detection rate curve of inserted outlier is shown in Figure 2. For all datasets, \(\mathtt{2D-OOB}\) successfully identifies over \(90\%\) of the outlier cells by inspecting only \(30\%\) of the bottom cells. In comparison, \(\mathtt{2D-KNN}\) requires examining nearly \(90\%\) of the cells to achieve the same detection level.

We also evaluate the area under the curve (AUC) as a quantitative metric and measure the run-time. As Table 1 shows, \(\mathtt{2D-OOB}\) achieves an average AUC of \(0.83\) across \(12\) datasets, compared to \(0.67\) for \(\mathtt{2D-KNN}\), while being significantly faster. For high-dimensional datasets such as the musk dataset, which comprises \(166\) features, \(\mathtt{2D-KNN}\) would take more than an hour to process, while \(\mathtt{2D-OOB}\) can finish in seconds. Furthermore, we present additional results on **multi-class classification** datasets in Appendix B.1, demonstrating the consistently superior performance and efficiency of \(\mathtt{2D-OOB}\).

### Cell fixation experiment

Experimental settingA naive strategy to handle cell-level outliers is to eliminate data points that contain outliers. This method, however, risks substantial data loss, particularly when outliers are scattered and data points are costly to collect. We instead consider a cell fixation experiment, where we assume that the ground-truth annotations of outlier cells can be restored with external expert knowledge. At each step, we "fix" a certain number of cells by substituting them with their ground-truth annotations, prioritizing cells that have the lowest valuations. Then we fit a logistic model3 and evaluate the model's performance with a test set of \(3000\) samples. It is important to note that correcting normal cells has no effect, whereas fixing outlier cells is expected to enhance the model's performance. We adopt the same datasets and implementations as in Section 4.1.

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{**AUC \(\uparrow\)**} & \multicolumn{2}{c}{**Run-time \(\downarrow\)**} \\  & 2D-OOB (ours) & 2D-KNN & 2D-OOB (ours) & 2D-KNN \\ \hline lawschool & **0.88\(\pm\) 0.0027** & 0.75\(\pm\) 0.0011 & **3.33 \(\pm\) 0.06** & 177.56 \(\pm\) 1.92 \\ electricity & **0.77\(\pm\) 0.0072** & 0.68\(\pm\) 0.0014 & **3.39 \(\pm\) 0.07** & 191.38 \(\pm\) 2.60 \\ fired & **0.91\(\pm\) 0.0015** & 0.61\(\pm\) 0.0005 & **3.97 \(\pm\) 0.10** & 322.79 \(\pm\) 2.98 \\
2dplanes & **0.87\(\pm\) 0.0015** & 0.62\(\pm\) 0.0005 & **3.46 \(\pm\) 0.05** & 295.25 \(\pm\) 2.37 \\ creditcard & **0.72\(\pm\) 0.0028** & 0.69\(\pm\) 0.0011 & **4.56 \(\pm\) 0.10** & 662.34 \(\pm\) 7.12 \\ pol & **0.82\(\pm\) 0.0014** & 0.67\(\pm\) 0.0006 & **4.34 \(\pm\) 0.05** & 759.33 \(\pm\) 4.37 \\ MiniBooNE & **0.77\(\pm\) 0.0058** & 0.63\(\pm\) 0.0019 & **7.46 \(\pm\) 0.06** & 1507.83 \(\pm\) 14.50 \\ jannis & **0.83\(\pm\) 0.0042** & 0.55\(\pm\) 0.0004 & **7.98 \(\pm\) 0.07** & 1753.01 \(\pm\) 12.35 \\ nomao & **0.79\(\pm\) 0.0021** & 0.67\(\pm\) 0.0009 & **7.69 \(\pm\) 0.11** & 2564.58 \(\pm\) 23.11 \\ vehicle\_sensIT & **0.81\(\pm\) 0.0014** & 0.61\(\pm\) 0.0005 & **9.87 \(\pm\) 0.08** & 3113.65 \(\pm\) 24.54 \\ gas\_drift & **0.86\(\pm\) 0.0010** & 0.84\(\pm\) 0.0017 & **11.28 \(\pm\) 0.10** & 3878.31 \(\pm\) 40.72 \\ musk & **0.88\(\pm\) 0.0008** & 0.71\(\pm\) 0.0006 & **14.09 \(\pm\) 0.11** & 4415.45 \(\pm\) 22.96 \\ \hline Average & **0.83** & 0.67 & **6.78** & 1636.80 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Cell-level outlier detection results.** AUC and run-time comparison between 2D-OOB and 2D-KNN across twelve binary classification datasets. The average and standard error of the AUC and run-time (in seconds) based on \(30\) independent experiments are denoted by “average \(\pm\) standard error”. Bold numbers denote the best method. The AUC value for the Random method consistently remains at \(0.5\) across all datasets. In every dataset, 2D-OOB achieves a significantly higher AUC while being orders of magnitude faster than 2D-KNN.

Figure 2: **Cell-level outlier detection rate curves for 2D-OOB, 2D-KNN, and Random. The x-axis represents the percentage of inspected cells. The y-axis represents the detection rate, defined as the ratio of the number of detected outlier cells to the total number of outlier cells present in a dataset. The error bars show a \(95\%\) confidence interval based on \(30\) independent experiments. We examine the cells in ascending order, starting from those with the lowest values, and thus a curve closer to the left-top corner indicates better performance. 2D-OOB efficiently detects the majority of outlier cells by examining only a small fraction of the total cells, while 2D-KNN and Random require scanning nearly all the cells.**ResultsFigure 3 illustrates the anticipated trend in the performance of 2D-OOB, validating our method's capability to accurately identify and prioritize the most impactful outliers for correction. As cells with the lowest valuations are progressively fixed, 2D-OOB demonstrates a consistent improvement in model accuracy. In contrast, when applying the same procedure with 2D-KNN, such notable performance enhancements are not observed.

Additionally, we investigate a scenario where ground-truth annotations remain unavailable. We adopt the setup from Liu et al. [32], where we replace the outlier cells with the average of other cells in the same feature column. 2D-OOB uniformly demonstrates significant superiority over its counterparts. Results are provided in Appendix B.2.

### Backdoor trigger detection

A common strategy of data poisoning attacks involves inserting a predefined trigger (e.g., a specific pixel pattern in an image) into a subset of the training data [14; 6; 31]. These malicious manipulations can be challenging to detect as they only infect specific, targeted samples. Even when poisoned data are present, it could be difficult to discern the root cause of the attacks, since manually reviewing the images is expensive and time-consuming. In this experiment, we introduce a novel task enabled by the joint valuation framework: localizing backdoor triggers in data poisoning attacks. Distinct from the random outliers investigated previously, this type of cell contamination is targeted and deliberate.

Figure 4: **Backdoor trigger detection rate curves for 2D-OOB, 2D-KNN, and Random. Panels A (top) and B (bottom) correspond to the Trojan square and BadNets square, respectively. We inspect the cells within each poisoned sample in descending order of their valuation scores. The detection rate curve shows the average detection rate across all poisoned samples, with error bars representing a \(95\%\) confidence interval based on \(15\) independent runs. 2D-OOB demonstrates superior performance in detecting the cells implanted with triggers.**

Figure 3: **Cell fixation experiment results (test accuracy curves) for 2D-OOB, 2D-KNN, and Random. We replace cells with their ground-truth annotations, starting with those cells assigned the lowest valuations. The results for \(6\) datasets are presented, and additional results for other datasets are available in Appendix B.2. We conduct \(30\) independent trials and report the average results. A higher curve indicates better performance. 2D-OOB demonstrates a superior capability in accurately identifying and rectifying cell-level outliers.**

We consider two popular backdoor attack algorithms: BadNets [14] and Trojan Attack [31]. During training, the poisoned samples, relabeled as the adversarial target class, are mixed up with clean data. As a result, the model learns to incorrectly associate the trigger with the target class. At test time, inputs containing the trigger are misclassified to the target class. In this context, our goal is to effectively pinpoint the triggers by recognizing them as influential features through our joint valuation framework.

**Experimental setting** We select 5 pairs of classes from CIFAR-10 [24]. For each pair, one class is designated as the target attack class, while the other serves as the source class. The training dataset comprises \(1000\) images. For each attack, we contaminate \(15\%\) of the training samples from the source class and relabel them to the target class. Two types of attack triggers are implemented: the Trojan square and the BadNets square [14, 38, 31]. These triggers are placed in the lower right corner of the original images to minimize occlusion. Further details about these attacks are available in Appendix A.4. In our experiment, the ratio of poisoned cells is approximately \(1\%\), and each weak learner in the subset bagging model is built by sampling \(25\)% of the features.

ResultsWe adopt the same detection scheme and baseline methods as in Section 4.1. Ideally, the poisoned cells should receive high valuation scores given that such data points have been relabeled. We present the detection rate curves for five datasets in Figure 4. 2D-OOB significantly outperforms 2D-KNN in detecting both types of triggers. Overall, 2D-OOB achieves an average detection AUC of \(0.95\) across all datasets and attack types, compared to \(0.83\) for 2D-KNN. It is worth noting that conventional data valuation methods are fundamentally unable to localize backdoor triggers; at most, they can only identify poisoned data points.

Qualitative examplesFigure 5 displays the heatmaps for poisoned samples based on cell valuations of 2D-OOB. Areas with higher cell valuations (marked in dark red) precisely indicate the trigger locations within these samples, demonstrating the effectiveness of our joint valuation framework. Additional examples can be found in Appendix B.3.

### Ablation study

We conduct ablation studies on the cell-level outlier detection task, as outlined in Section 4.1, to examine the impact of key hyperparameters on 2D-OOB estimations, including the selection and number of weak learners, as well as the feature subset ratio.

Selection of weak learnersAlthough our study primarily employs decision trees as weak learners, it is important to note that 2D-OOB is **model-agnostic**, enabling the use of any class of machine learning models as weak learners. Specifically, we examine decision trees, logistic regression, a single-layer MLP with \(64\) units, and a two-layer MLP with \(64\) and \(32\) units, respectively, as weak learners to compare their performance.

Table 2 presents a comparison of detection AUC across \(12\) datasets with different choices of weak learners, indicating that 2D-OOB is not model-free. The selection of weak learners _slightly_ affects the

Figure 5: **Qualitative examples for 2D-OOB in the backdoor trigger detection task. Each pair of images consists of a poisoned image and its corresponding cell valuation heatmap. The color of the heatmap indicates importance, with red cells representing higher importance and blue cells representing lower importance. In the first two pairs, the class ‘bird” is relabeled as “cat”, while in the latter two pairs, the class “deer” is relabeled as “cat”. The heatmaps clearly show that higher cell valuations are predominantly concentrated in the regions containing triggers, while areas featuring actual objects receive lower valuations. This pattern suggests that 2D-OOB effectively captures the triggers as the impactful features responsible for the misclassification of the poisoned samples.**

valuation results, with more complex models generally yielding better performance. Nonetheless, all variations of 2D-OOB outperform 2D-KNN, highlighting the clear advantages of the 2D-OOB approach.

Number of weak learnersIncreasing the number of weak learners allows for a greater number of data-feature subset pairs to be explored, potentially leading to more accurate estimates. However, as shown in Table (a)a, when we increase the number of base learners from \(500\) to \(3000\), the detection AUC for each dataset remains relatively unchanged, suggesting convergence of the estimation beyond a certain threshold. Typically, \(1000\) base learners are sufficient to achieve an equitable joint valuation.

Feature subset ratio \(K/d\)The feature subset ratio \(K/d\) refers to the fraction of the total number of features \(d\) that are randomly selected to build each weak learner, where \(K\) is the number of selected features. In previous experiments, we used a fixed ratio of \(0.50\) (unless otherwise specified). To further investigate the impact of this ratio, we now test two additional values: \(0.25\) and \(0.75\). The results in Table (b)b suggest that in general, the joint valuation capacity of our method is robust to the choice of feature subset ratio.

Apart from the experiments discussed above, we showcase that marginalization of 2D-OOB can either match or surpass state-of-the-art data valuation methods on standard benchmarks in Appendix D.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Dataset & Decision Tree & Logistic Regression & MLP (single-layer) & MLP (two-layer) & 2D-KNN (Baseline) \\ \hline lawschool & **0.88 \(\pm\) 0.0027** & 0.81 \(\pm\) 0.0014 & 0.83 \(\pm\) 0.0023 & 0.86 \(\pm\) 0.0049 & 0.75 \(\pm\) 0.0011 \\ electricity & **0.77 \(\pm\) 0.0072** & 0.75 \(\pm\) 0.0029 & 0.75 \(\pm\) 0.0039 & 0.74 \(\pm\) 0.0064 & 0.68 \(\pm\) 0.0014 \\ fried & **0.91 \(\pm\) 0.0015** & 0.82 \(\pm\) 0.0023 & 0.85 \(\pm\) 0.0020 & 0.88 \(\pm\) 0.0027 & 0.61 \(\pm\) 0.0005 \\ Zdplanes & 0.87 \(\pm\) 0.0015 & 0.82 \(\pm\) 0.0026 & 0.86 \(\pm\) 0.0026 & **0.88 \(\pm\) 0.0037** & 0.62 \(\pm\) 0.0005 \\ creditcard & 0.72 \(\pm\) 0.0028 & **0.74 \(\pm\) 0.0023** & **0.74 \(\pm\) 0.0026** & **0.74 \(\pm\) 0.0071** & 0.69 \(\pm\) 0.0011 \\ pol & 0.82 \(\pm\) 0.0014 & 0.79 \(\pm\) 0.0029 & 0.85 \(\pm\) 0.0014 & **0.86 \(\pm\) 0.0019** & 0.67 \(\pm\) 0.0006 \\ MiniBoeNE & 0.77 \(\pm\) 0.0058 & 0.77 \(\pm\) 0.0059 & 0.80 \(\pm\) 0.0057 & **0.81 \(\pm\) 0.0119** & 0.63 \(\pm\) 0.0019 \\ jannis & **0.83 \(\pm\) 0.0042** & 0.76 \(\pm\) 0.0040 & 0.79 \(\pm\) 0.0048 & 0.80 \(\pm\) 0.0108 & 0.55 \(\pm\) 0.0004 \\ nomao & 0.79 \(\pm\) 0.0021 & 0.82 \(\pm\) 0.0012 & **0.83 \(\pm\) 0.0010** & **0.83 \(\pm\) 0.0017** & 0.67 \(\pm\) 0.0009 \\ vehicle-sensIT & 0.81 \(\pm\) 0.0014 & 0.81 \(\pm\) 0.0026 & 0.80 \(\pm\) 0.0025 & **0.82 \(\pm\) 0.0037** & 0.61 \(\pm\) 0.0005 \\ gas-drift & 0.86 \(\pm\) 0.0010 & **0.89 \(\pm\) 0.0005** & 0.88 \(\pm\) 0.0005 & 0.88 \(\pm\) 0.0006 & 0.84 \(\pm\) 0.0017 \\ musk & 0.88 \(\pm\) 0.0008 & 0.87 \(\pm\) 0.0005 & **0.88 \(\pm\) 0.0005** & **0.88 \(\pm\) 0.0008** & 0.71 \(\pm\) 0.0006 \\ \hline Average & **0.83** & 0.80 & 0.82 & **0.83** & 0.67 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Ablation study results of weak learner types. The average and standard error of the detection AUC based on \(30\) independent experiments are denoted by “average \(\pm\) standard error”. Results from 2D-KNN are included for comparison. The choice of weak learner leads to variations in cell values, yet the performance of the detection task remains robust.**

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Dataset & \multicolumn{3}{c}{**AUC \(\uparrow\)**} \\  & \(B=500\) & \(B=1000\) & \(B=3000\) \\ \hline lawschool & 0.86 \(\pm\) 0.0035 & 0.88 \(\pm\) 0.0027 & 0.88 \(\pm\) 0.0026 \\ electricity & 0.77 \(\pm\) 0.0025 & 0.77 \(\pm\) 0.0072 & 0.77 \(\pm\) 0.0070 \\ fried & 0.87 \(\pm\) 0.0022 & 0.91 \(\pm\) 0.0015 & 0.91 \(\pm\) 0.0014 \\ \(2d\)planes & 0.87 \(\pm\) 0.0016 & 0.87 \(\pm\) 0.0015 & 0.87 \(\pm\) 0.0015 \\ creditcard & 0.72 \(\pm\) 0.0025 & 0.72 \(\pm\) 0.0028 & 0.72 \(\pm\) 0.0028 \\ pol & 0.87 \(\pm\) 0.0022 & 0.85 \(\pm\) 0.0014 & 0.82 \(\pm\) 0.0014 \\ MiniBoeNE & 0.77 \(\pm\) 0.0042 & 0.77 \(\pm\) 0.0058 & 0.77 \(\pm\) 0.0058 \\ jannis & 0.78 \(\pm\) 0.0045 & 0.83 \(\pm\) 0.0042 & 0.83 \(\pm\) 0.0039 \\ nomao & 0.79 \(\pm\) 0.0018 & 0.79 \(\pm\) 0.0021 & 0.79 \(\pm\) 0.0020 \\ vehicle\_sensIT & 0.80 \(\pm\) 0.0021 & 0.81 \(\pm\) 0.0014 & 0.81 \(\pm\) 0.0014 \\ gas\_drift & 0.86 \(\pm\) 0.0007 & 0.86 \(\pm\) 0.0010 & 0.86 \(\pm\) 0.0010 \\ musk & 0.88 \(\pm\) 0.0008 & 0.88 \(\pm\) 0.0008 & 0.88 \(\pm\) 0.0008 \\ \hline Average & 0.81 & **0.83** & 0.82 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Ablation study results of (a) the number of base learners \(B\) and (b) the feature subset ratio \(K/d\). The average and standard error of the detection AUC based on \(30\) independent runs are denoted by “average \(\pm\) standard error.” (a) Increasing the number of base learners from \(1000\) to \(3000\) does not yield a notable performance improvement. (b) Our method’s joint valuation capacity remains relatively stable regardless of the selected feature subset ratio**Related work

Data contribution estimationIn addition to the marginal contribution-based methods discussed in Section 2, many other approaches are emerging in the area of data valuation. Just et al. [19] develop a non-conventional class-wise Wasserstein distance between the training and validation sets and use the gradient information to evaluate each data point, an approach that has also been applied to data selection [20]. Wu et al. [51] extend data valuation to deep neural networks, introducing a training-free data valuation framework based on neural tangent kernel theory. Yoon et al. [52] leverage reinforcement learning techniques to automatically learn data valuation scores by training a regression model. However, all these data valuation methods do not assign importance scores to cells, whereas our method provides additional insights into how individual cells contribute to the data valuations.

Feature attributionFeature attribution is a pivotal research domain in explainable machine learning that primarily aims to provide insights into how individual features influence model predictions. Various effective methods have been proposed, including SHAP-based explanation [33; 34; 27; 8; 7], counterfactual explanation [47; 18; 39; 35; 36], and backpropagation-based explanation [1; 2; 45; 44; 53]. Among these methods, the SHAP-based explanation stands out as the most widely adopted approach, utilizing cooperative game theory principles to compute the Shapley value [41]. While feature attribution offers a potential method to attribute data valuation scores across individual cells, our empirical experiments in Appendix B.1 reveal that this two-stage scheme falls short in efficacy compared to our proposed joint valuation paradigm, which integrates data valuation and feature attribution in a simultaneous process.

## 6 Conclusion

We propose 2D-OOB, an efficient joint valuation framework that assigns a score to each cell in a dataset, thereby facilitating finer attribution of data contributions and enabling a deeper understanding of the dataset. Through comprehensive experiments, we show that 2D-OOB is computationally efficient and competitive over state-of-the-art methods in multiple joint valuation use cases.

DiscussionWe emphasize that the primary objective of the joint valuation framework is to evaluate the quality of cells within the dataset, rather than to optimize model performance. The model used serves as a proxy for this evaluation, and it is important to note that a high-performing machine learning method does not necessarily ensure a justified valuation framework.

Limitation and future workWhile our study primarily explores random forest models applied to tabular datasets and simple image datasets, the potential application of neural network models within the 2D-OOB framework for more complex vision and language tasks presents a promising avenue for future investigation. For instance, in text datasets, tokens or words can be treated as cells. 2D-OOB can be easily integrated into any bagging training scheme that uses language models.

Overall, we believe that our work will inspire further exploration in the field of joint valuation, with the broader goal of improving the transparency and interpretability of machine learning, as well as developing an equitable incentive mechanism for data sharing.

## Acknowledgement

We acknowledge computing resources from Columbia University's Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010.

## References

* [1] Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. _arXiv preprint arXiv:1711.06104_, 2017.
* [2] Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. _PloS one_, 10(7):e0130140, 2015.
* [3] Jens Bleiholder and Felix Naumann. Data fusion. _ACM computing surveys (CSUR)_, 41(1):1-41, 2009.
* [4] Leo Breiman. Random forests. _Machine learning_, 45:5-32, 2001.
* [5] Lingjiao Chen, Bilge Acun, Newsha Ardalani, Yifan Sun, Feiyang Kang, Hanrui Lyu, Yongchan Kwon, Ruoxi Jia, Carole-Jean Wu, Matei Zaharia, et al. Data acquisition: A new frontier in data-centric ai. _arXiv preprint arXiv:2311.13712_, 2023.
* [6] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_, 2017.
* [7] Ian Covert and Su-In Lee. Improving kernelshap: Practical shapley value estimation using linear regression. In _International Conference on Artificial Intelligence and Statistics_, pages 3457-3465. PMLR, 2021.
* [8] Ian Covert, Scott M Lundberg, and Su-In Lee. Understanding global feature contributions with additive importance measures. _Advances in Neural Information Processing Systems_, 33:17212-17223, 2020.
* [9] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don't know by virtual outlier synthesis. _arXiv preprint arXiv:2202.01197_, 2022.
* [10] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. _Advances in Neural Information Processing Systems_, 33:2881-2891, 2020.
* [11] Raul Castro Fernandez, Pranav Subramaniam, and Michael J Franklin. Data market platforms: Trading data assets to solve data problems. _arXiv preprint arXiv:2002.01047_, 2020.
* [12] Matthias Feurer, Jan N Van Rijn, Arlind Kadra, Pieter Gijsberg, Neeratyoy Mallik, Sahithya Ravi, Andreas Muller, Joaquin Vanschoren, and Frank Hutter. Openml-python: an extensible python api for openml. _The Journal of Machine Learning Research_, 22(1):4573-4577, 2021.
* [13] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In _International Conference on Machine Learning_, pages 2242-2251. PMLR, 2019.
* [14] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* [15] Tin Kam Ho. Random decision forests. In _Proceedings of 3rd international conference on document analysis and recognition_, volume 1, pages 278-282. IEEE, 1995.
* [16] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihre Merve Gurel, Bo Li, Ce Zhang, Costas J Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor algorithms. _arXiv preprint arXiv:1908.08619_, 2019.
* [17] Kevin Fu Jiang, Weixin Liang, James Zou, and Yongchan Kwon. Opendataval: a unified benchmark for data valuation. _arXiv preprint arXiv:2306.10577_, 2023.
* [18] Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. Towards realistic individual recourse and actionable explanations in black-box decision making systems. _arXiv preprint arXiv:1907.09615_, 2019.
* [19] Hoang Anh Just, Feiyang Kang, Jiachen T Wang, Yi Zeng, Myeongseob Ko, Ming Jin, and Ruoxi Jia. Lava: Data valuation without pre-specified learning algorithms. _arXiv preprint arXiv:2305.00054_, 2023.
* [20] Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, and Ruoxi Jia. Get more for less: Principled data selection for warming up fine-tuning in llms. _arXiv preprint arXiv:2405.02774_, 2024.
* [21] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. https://archive.ics.uci.edu, 2017. The UCI Machine Learning Repository.

* Koh and Liang [2017] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _International conference on machine learning_, pages 1885-1894. PMLR, 2017.
* Kriegel et al. [2009] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In _Advances in Knowledge Discovery and Data Mining: 13th Pacific-Asia Conference, PAKDD 2009 Bangkok, Thailand, April 27-30, 2009 Proceedings 13_, pages 831-838. Springer, 2009.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Kwon and Zou [2021] Yongchan Kwon and James Zou. Beta shapley: a unified and noise-reduced data valuation framework for machine learning. _arXiv preprint arXiv:2110.14049_, 2021.
* Kwon and Zou [2022] Yongchan Kwon and James Zou. Data-OOB: Out-of-bag estimate as a simple and efficient data value. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 18135-18152. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/kwon23e.html.
* Kwon and Zou [2022] Yongchan Kwon and James Y Zou. Weightedshap: analyzing and improving shapley based feature attributions. _Advances in Neural Information Processing Systems_, 35:34363-34376, 2022.
* Kwon et al. [2021] Yongchan Kwon, Manuel A. Rivas, and James Zou. Efficient computation and analysis of distributional shapley values. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 793-801. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/kwon21a.html.
* Leung et al. [2016] Andy Leung, Hongyang Zhang, and Ruben Zamar. Robust regression estimation and inference in the presence of cellwise and casewise contamination. _Computational Statistics & Data Analysis_, 99:1-11, 2016.
* Liang et al. [2022] Weixin Liang, Girmaw Abebe Tadesse, Daniel Ho, L Fei-Fei, Matei Zaharia, Ce Zhang, and James Zou. Advances, challenges and opportunities in creating data for trustworthy ai. _Nature Machine Intelligence_, 4(8):669-677, 2022.
* Liu et al. [2018] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and X. Zhang. Trojaning attack on neural networks. In _Network and Distributed System Security Symposium_, 2018. URL https://api.semanticscholar.org/CorpusID:31806516.
* Liu et al. [2023] Zhihong Liu, Hoang Anh Just, Xiangyu Chang, Xi Chen, and Ruoxi Jia. 2d-shapley: A framework for fragmented data valuation. _arXiv preprint arXiv:2306.10473_, 2023.
* Lundberg and Lee [2017] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017.
* Lundberg et al. [2018] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. _arXiv preprint arXiv:1802.03888_, 2018.
* Mahajan et al. [2019] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in counterfactual explanations for machine learning classifiers. _arXiv preprint arXiv:1912.03277_, 2019.
* Mothilal et al. [2020] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In _Proceedings of the 2020 conference on fairness, accountability, and transparency_, pages 607-617, 2020.
* Nicolae et al. [2018] Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian Molloy, and Ben Edwards. Adversarial robustness toolbox v1.2.0. _CoRR_, 1807.01069, 2018. URL https://arxiv.org/pdf/1807.01069.
* Pang et al. [2022] Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, and Ting Wang. Trojanzoo: Towards unified, holistic, and practical evaluation of neural backdoors. In _Proceedings of IEEE European Symposium on Security and Privacy (Euro S&P)_, 2022.
* Poyiadzi et al. [2020] Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. Face: feasible and actionable counterfactual explanations. In _Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society_, pages 344-350, 2020.

* [40] Peter J Rousseeuw and Wannes Van Den Bossche. Detecting deviating data cells. _Technometrics_, 60(2):135-145, 2018.
* [41] Lloyd S Shapley et al. A value for n-person games. 1953.
* [42] Rachael Hwee Ling Sim, Xinyi Xu, and Bryan Kian Hsiang Low. Data valuation in machine learning:"ingredients", strategies, and open challenges. In _Proc. IJCAI_, 2022.
* [43] Rachael Hwee Ling Sim, Yehong Zhang, Trong Nghia Hoang, Xinyi Xu, Bryan Kian Hsiang Low, and Patrick Jaillet. Incentives in private collaborative machine learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [44] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. _arXiv preprint arXiv:1312.6034_, 2013.
* [45] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. _arXiv preprint arXiv:1412.6806_, 2014.
* [46] Peng Su, Garth Tarr, and Samuel Muller. Robust variable selection under cellwise contamination. _Journal of Statistical Computation and Simulation_, pages 1-17, 2023.
* [47] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the gdpr. _Harv. JL & Tech._, 31:841, 2017.
* [48] Jiachen T Wang, Prateek Mittal, and Ruoxi Jia. Efficient data shapley for weighted nearest neighbor algorithms. _arXiv preprint arXiv:2401.11103_, 2024.
* [49] Jiachen T Wang, Tianji Yang, James Zou, Yongchan Kwon, and Ruoxi Jia. Rethinking data shapley for data selection tasks: Misleads and merits. _arXiv preprint arXiv:2405.03875_, 2024.
* [50] Tianhao Wang and Ruoxi Jia. Data bankhaf: A data valuation framework with maximal robustness to learning stochasticity. _arXiv preprint arXiv:2205.15466_, 2022.
* [51] Zhaoxuan Wu, Yao Shu, and Bryan Kian Hsiang Low. Davinz: Data valuation using deep neural networks at initialization. In _International Conference on Machine Learning_, pages 24150-24176. PMLR, 2022.
* [52] Jinsung Yoon, Sercan Arik, and Tomas Pfister. Data valuation using reinforcement learning. In _International Conference on Machine Learning_, pages 10842-10851. PMLR, 2020.
* [53] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13_, pages 818-833. Springer, 2014.
* [54] Jiayao Zhang, Qiheng Sun, Jinfei Liu, Li Xiong, Jian Pei, and Kui Ren. Efficient sampling approaches to shapley value approximation. _Proc. ACM Manag. Data_, 1(1), may 2023. doi: 10.1145/3588728. URL https://doi.org/10.1145/3588728.
* [55] Boxin Zhao, Boxiang Lyu, Raul Castro Fernandez, and Mladen Kolar. Addressing budget allocation and revenue allocation in data market environments using an adaptive sampling algorithm. _arXiv preprint arXiv:2306.02543_, 2023.

## Supplementary Materials

In the supplementary materials, we provide implementation details, additional experimental results, rigorous formalized proofs and data valuation experiment results. Code repository can be found at https://github.com/yifansun99/2D-OOB-Joint-Valuation.

## Appendix A Implementation details

### Datasets

Tabular datasetsWe use \(12\) binary classification datasets obtained from OpenML [12]. A summary of all the datasets is provided in Table 4. These datasets are used in Section 4.1, 4.2, 4.4, and Appendix D.

For each dataset, we first employ a standard normalization procedure, where each feature is normalized to have zero mean and unit standard deviation. After preprocessing, we randomly partition a subset of the data into two non-overlapping sets: a training dataset and a test dataset, which consists of \(1000\) and \(3000\) samples respectively. The training dataset is used to obtain the joint (or marginal) valuation for each cell (or data point). The test dataset is exclusively used for the cell fixation (or point removal) experiment when evaluating the test accuracy. Note that for methods that need a validation dataset such as KNNShapley and DataShapley, we additionally sample a separate validation dataset (disjoint from training dataset and test dataset) to evaluate the utility function. The size of the validation dataset is set to \(10\%\) of the training sample size.

Image datasetsWe create datasets by pairing CIFAR-10 classes, each pair consisting of a target attack class and a source class. The training and test dataset comprises \(1000\) and \(2000\) samples, respectively. The size of the validation dataset is set to \(10\%\) of the training sample size. To manage the computational challenges posed by the baseline method, we employ the super-pixel technique to transform the (\(32\),\(32\),\(3\)) image into a \(256\)-dimensional vector. Specifically, we first average the pixel values across the three channels for each pixel. Then, we partition these transformed images into equal-sized \(2\times 2\) grids. Average pooling is applied within each grid to reduce pixel values to a single cell value, which is then arranged into a flattened input vector. A cell is annotated as poisoned if at least \(25\%\) of its corresponding grid area contains the trigger.

### Implementation details for different methods

2d-OOB2d-OOB involves fitting a _subset_ random forest model with \(B=1000\) decision trees based on the package "scikit-learn". When constructing each decision tree, we fix the feature subset size ratio as \(0.5\) (unless otherwise specified). For Section 4.3 and Appendix D, we simply adopt \(T(y_{i},\hat{f}(x_{i,S_{b}}))=\mathds{1}(y_{i}=\hat{f}(x_{i,S_{b}}))\). For Section 4.1 and 4.2, we further calculate the normalized negative \(L_{2}\) distance between covariates and the class-specific mean in the bootstrap dataset, denoted as \(d_{norm}(x_{i},y_{i})\). Then we use \(T(y_{i},\hat{f}(x_{i,S_{b}}))=\mathds{1}(y_{i}=\hat{f}(x_{i,S_{b}}))+d_{norm }(x_{i},y_{i})\).

2d-KNN2d-KNN employs KNN as a surrogate model to approximate 2D-Shapley. We set the number of nearest neighbors as \(10\) and the number of permutations as \(1000\). The hyperparameters are selected based on convergence behavior, and the run time is measured until the values converge.

### Implementation details for cell-level outlier generation

Following Du et al. [9] and Liu et al. [32], we replace a given cell with an outlier value. Here, the outlier value is randomly generated from the two-sided "tails" of the Gaussian distribution fitted to the column's mean and standard deviation, where the probability of the two-sided tail area is set to be \(1\)%. In total, \(4\%\) (\(20\%\times 20\%\)) of the cells are replaced with the corresponding outlier values.

### Implementation details for backdoor trigger generation

Following the prior work [14; 31], we generate the BadNets square and the Trojan square trigger. For BadNets, we adopt the implementation in Nicolae et al. [37]. For Trojan Attack, we use apretrained ResNet-18 model on the CIFAR-10 dataset and employ the implementation in Pang et al. [38]. For each attack, we evaluate its effectiveness by training a decision tree model on the poisoned dataset. The accuracy on a clean test set remains nearly unchanged compared to a model trained on an uncontaminated training set, while the attack success rate on a held-out poisoned test set is guaranteed to exceed \(75\%\).

## Appendix B Additional experimental results

### Additional results for cell-level outlier detection

Additional results on multi-class classification datasetsWe conducted cell-level outlier detection experiments (as described in Section 4.1) on three multi-class classification datasets from the UCI Machine Learning repository [21]. As shown in the Table 5, 2D-OOB displays superior detection performance and efficiency.

Additional baseline: two-stage attributionOnce we obtain the data valuation scores, an alternative solution approach to determining cell-level attributions involves leveraging feature attribution methods such as SHAP [33]. We explore an additional baseline method building upon this idea: initially, Data-OOB (or any other data valuation method) is computed for the \(i\)-th data point, denoted as \(dv_{i}\). Subsequently, TreeSHAP [34] is fitted, using \(dv_{i}\) as the target and the concatenation of \(x_{i}\) and \(y_{i}\) (denoted as \(x_{i}\oplus y_{i}\)) as the predictor. The derived local feature attributions are then interpreted as joint valuation results. We refer to this method as "two-stage attribution".

Table 6 indicates that 2D-OOB substantially outperforms its two-stage counterpart. We hypothesize that the superiority of our method stems from integrating data valuation and feature attribution into a cohesive framework. Conversely, the two-stage method treats data valuation and feature attribution as separate processes, potentially resulting in sub-optimal outcomes. Furthermore, due to the computational complexity of TreeSHAP, the two-stage approach is notably slower compared to our method.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Name & Total sample size & Input dimension & Majority class proportion & OpenML ID \\ \hline lawschool & 20800 & 6 & 0.679 & 43890 \\ electricity & 38474 & 6 & 0.500 & 44080 \\ fried & 40768 & 10 & 0.502 & 901 \\
2dplanes & 40768 & 10 & 0.501 & 727 \\ creditcard & 30000 & 23 & 0.779 & 42477 \\ pol & 15000 & 48 & 0.664 & 722 \\ MiniBooNE & 72998 & 50 & 0.500 & 43974 \\ jannis & 57580 & 54 & 0.500 & 43977 \\ nomao & 34465 & 89 & 0.715 & 1486 \\ vehicle\_sensIT & 98528 & 100 & 0.500 & 357 \\ gas\_drift & 5935 & 128 & 0.507 & 1476 \\ musk & 6598 & 166 & 0.846 & 1116 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **A summary of all the datasets used in 4.1, 4.2, and Appendix D.** These datasets have been commonly used in previous literature [13, 25, 26]

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{**AUC \(\uparrow\)**} & \multicolumn{2}{c}{**Run-time \(\downarrow\)**} \\  & 2D-OOB (ours) & 2D-KIN & 2D-OOB (ours) & 2D-KNN \\ \hline Covertype & **0.81\(\pm\)0.0156** & 0.63\(\pm\)0.0183 & **3.98\(\pm\)0.5774** & 962.34\(\pm\)1.3383 \\ Dry Bean & **0.88\(\pm\)0.0059** & 0.85\(\pm\)0.0192 & **3.31\(\pm\)0.4586** & 347.80\(\pm\)2.0212 \\ Wine Quality & **0.86\(\pm\)0.0178** & 0.57\(\pm\)0.0252 & **2.90\(\pm\)0.1240** & 269.14\(\pm\)1.1825 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Cell-level outlier detection results on multi-class classification datasets.** The average and standard error of the AUC and run-time (in seconds) based on \(30\) independent experiments are denoted by “average \(\pm\) standard error”.

A noisy setting with more outlier cellsWe consider a more challenging scenario with increased outlier levels, where both the row outlier ratio and column outlier ratio increase from \(20\%\) (as in Section 4.1) to \(50\%\). Consequently, this leads to \(25\%\) (\(50\%\times 50\%\)) of the cells being replaced with outlier values. We follow the same outlier generation procedure outlined in Appendix A.3. The findings, presented in Table 7, demonstrate that our method maintains a significantly superior performance over 2D-KNN, even under such a noisy setting.

### Additional results for cell fixation experiment

Figure 6 presents the results for the cell fixation experiment on \(6\) additional datasets. 2D-OOB excels in precisely detecting and correcting relevant cell outliers.

The scenario without ground-truth knowledgeFollowing Liu et al. [32], we examine a situation where external information on the ground-truth annotations of outlier cells is not accessible. In this scenario, we address these outliers by substituting them with the average of other cells in the same feature column. This procedure starts by addressing cells with the lowest valuations, based on the hypothesis that correcting these cells is likely to maintain or potentially improve the model's performance. As depicted in Figure 7, 2D-OOB conforms to this expected trend, demonstrating the

\begin{table}
\begin{tabular}{l|c|c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{**AUC \(\uparrow\)**} \\  & 2D-OOB (ours) & Two-stage attribution \\ \hline lawschool & **0.88\(\pm\) 0.0027** & 0.83\(\pm\) 0.0064 \\ electricity & **0.77\(\pm\) 0.0072** & 0.64\(\pm\) 0.0093 \\ fried & **0.91\(\pm\) 0.0015** & 0.82\(\pm\) 0.0068 \\
2dplanes & **0.87\(\pm\) 0.0015** & 0.80\(\pm\) 0.0058 \\ creditcard & **0.72\(\pm\) 0.0028** & 0.67\(\pm\) 0.0051 \\ pol & **0.82\(\pm\) 0.0014** & 0.78\(\pm\) 0.0042 \\ MiniBooNE & **0.77\(\pm\) 0.0058** & 0.70\(\pm\) 0.0041 \\ jannis & **0.83\(\pm\) 0.0042** & 0.62\(\pm\) 0.0043 \\ nomao & **0.79\(\pm\) 0.0021** & 0.71\(\pm\) 0.0041 \\ vehicle\_sensIT & **0.81\(\pm\) 0.0014** & 0.64\(\pm\) 0.0033 \\ gas\_drift & **0.86\(\pm\) 0.0010** & 0.73\(\pm\) 0.0143 \\ musk & **0.88\(\pm\) 0.0008** & 0.68\(\pm\) 0.0028 \\ \hline Average & **0.83** & 0.72 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Cell-level outlier detection results (AUC) of 2D-OOB and the two-stage attribution.** Our method shows a better performance than the alternative method by a significant performance margin.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{**AUC \(\uparrow\)**} \\  & 2D-OOB (ours) & 2D-KNN \\ \hline lawschool & **0.75\(\pm\) 0.0084** & 0.60\(\pm\) 0.0144 \\ electricity & **0.64\(\pm\) 0.0155** & 0.60\(\pm\) 0.0106 \\ fried & **0.74\(\pm\) 0.0087** & 0.54\(\pm\) 0.0027 \\
2dplanes & **0.74\(\pm\) 0.0063** & 0.55\(\pm\) 0.0033 \\ creditcard & **0.63\(\pm\) 0.0055** & 0.61\(\pm\) 0.0053 \\ pol & **0.69\(\pm\) 0.0069** & 0.60\(\pm\) 0.0042 \\ MiniBooNE & **0.67\(\pm\) 0.0128** & 0.60\(\pm\) 0.0048 \\ jannis & **0.70\(\pm\) 0.0113** & 0.53\(\pm\) 0.0014 \\ nomao & **0.70\(\pm\) 0.0088** & 0.58\(\pm\) 0.0052 \\ vehicle\_sensIT & **0.70\(\pm\) 0.0075** & 0.55\(\pm\) 0.0031 \\ gas\_drift & **0.73\(\pm\) 0.0077** & 0.65\(\pm\) 0.0114 \\ musk & **0.77\(\pm\) 0.0063** & 0.64\(\pm\) 0.0038 \\ \hline Average & **0.71** & 0.59 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Cell-level outlier detection results (AUC) of different joint valuation methods when the row outlier ratio and column outlier ratio are both \(50\%\). Our method consistently outperforms 2D-KNN even in the presence of significant noise.**effectiveness of our method in joint valuation. Conversely, 2D-KNN fails to show similar performance improvements.

### Additional results for backdoor trigger experiment

We provide additional qualitative examples of the backdoor trigger detection experiment in Figure 8.

## Appendix C Proof of Proposition 3.1

Proof.: For simplicity, we denote \(\phi_{i}^{\mathrm{OOB}}(S)\) as \(\phi_{i}(S)\) and \(\psi_{ij}^{\mathrm{2D-OOB}}\) as \(\psi_{ij}\) in the proof. Let \(\mathcal{S}:=\{S\subseteq[d]\}\) represent the set of all feature subsets, where \(S\) is a feature subset. We denote the cardinality of \(\mathcal{S}\) as \(L:=|\mathcal{S}|=2^{d}\). Let \(\bm{\gamma}_{b}\) be a weight vector \(\bm{\gamma}_{b}:=(\gamma_{b1},\dots,\gamma_{bL})\) for all \(b\in[B]\), where \(\gamma_{bl}\in\{0,1\}\) and \(\gamma_{bl}=1\) indicates the \(l\)-th subset is used in the \(b\)-th weak learner. With \(\{\mathbf{w}_{b},\bm{\gamma}_{b},\hat{f}_{b}\}_{b=1}^{B}\), we can denote the \(i\)-th Data-OOB on the \(l\)-th feature subset \(S_{l}\) as

\[\phi_{i}(S_{l})=\frac{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)\mathds{1}(\gamma_{bl} =1)T(y_{i},\hat{f}_{b}(x_{i,S_{l}}))}{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0) \mathds{1}(\gamma_{bl}=1)}.\]

Figure 8: **Qualitative results on more datasets for the backdoor trigger detection experiment. The first two images are from the class “airplane” but have been relabeled as “automobile”, while the latter two images are from the class “frog” and have been relabeled as “horse”.**

Figure 6: **Cell fixation experiment results (test accuracy curves) for 2D-OOB, 2D-KNN and a random baseline. We replace cell values with ground-truth values from the cells with the lowest valuation to the highest valuation. The results from \(6\) additional datasets are displayed. We conduct \(30\) independent trials and report the average results. A higher curve indicates better performance. 2D-OOB sets itself apart by its remarkable precision in detecting and rectifying relevant cell outliers.**

Figure 7: **Cell fixation experiment (without ground-truth knowledge) results (test accuracy curves) for 2D-OOB, 2D-KNN and a random baseline. We replace cell values with column mean imputations from cells with the lowest value to the highest value. We report the average results of \(30\) independent trials from \(12\) datasets. A higher curve indicates better performance.**

With slight abuse of notation, the formulation of 2D-OOB in (4) can be expressed as follows.

\[\psi_{ij} =\frac{\sum_{l=1}^{L}\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)\mathds{1}( \gamma_{bl}=1)\mathds{1}(j\in S_{l})T(y_{i},\hat{f}_{b}(x_{i,S_{l}}))}{\sum_{l=1 }^{L}\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)\mathds{1}(\gamma_{bl}=1)\mathds{1}(j \in S_{l})}\] \[=\sum_{l=1}^{L}\mathds{1}(j\in S_{l})\frac{\sum_{b=1}^{B}\mathds{1 }(w_{bi}=0)\mathds{1}(\gamma_{bl}=1)}{\sum_{l=1}^{L}\sum_{b=1}^{B}\mathds{1}(w _{bi}=0)\mathds{1}(\gamma_{bl}=1)\mathds{1}(j\in S_{l})}\] \[=\sum_{l=1}^{L}\mathds{1}(j\in S_{l})\frac{\sum_{b=1}^{B}\mathds{1 }(w_{bi}=0)\mathds{1}(\gamma_{bl}=1)}{\sum_{l=1}^{L}\sum_{b=1}^{B}\mathds{1}(w _{bi}=0)\mathds{1}(\gamma_{bl}=1)\mathds{1}(j\in S_{l})}\frac{\sum_{b=1}^{B} \mathds{1}(w_{bi}=0)\mathds{1}(\gamma_{bl}=1)T(y_{i},\hat{f}_{b}(x_{i,S_{l}})) }{\sum_{b=1}^{B}\mathds{1}(w_{bi}=0)\mathds{1}(\gamma_{bl}=1)}\] \[=\sum_{l=1}^{L}\alpha_{i,j,l}\phi_{i}(S_{l}),\]

where \(\alpha_{i,j,l}\propto\mathds{1}(j\in S_{l})\sum_{b=1}^{B}\mathds{1}(w_{bi}=0 )\mathds{1}(\gamma_{bl}=1),\forall i\in[n],j\in[d],l\in[L]\) and \(\sum_{l=1}^{L}\alpha_{i,j,l}=1\). Define \(P_{i}(S_{l}|j\in S_{l},\{w_{bi}\}_{b=1}^{B})=\alpha_{i,j,l}\), which specifies an empirical distribution of the feature subset \(S\), conditioned on \(j\in S\) and the bootstrap sampling process. Here, \(\mathds{1}(j\in S_{l})\) indicates that the distribution is conditioned on the inclusion of the \(j\)-th feature within the feature subset \(S_{l}\). \(w_{bi}\) indicates whether the \(i\)-th sample is out-of-bag in the \(b\)-th bootstrap, and \(\gamma_{bl}\) indicates whether the \(l\)-th feature subset is selected in the \(b\)-th weak learner. Thus, the point mass is determined by the sampling process, leading to:

\[\psi_{ij}=\mathbb{E}_{\hat{F}_{S}}[\phi_{i}(S)\mid j\in S].\]

## Appendix D Data valuation experiment

In this section, we show that 2D-OOB-data, the marginalization of 2D-OOB, offers an effective approach to data valuation. This serves as the basis of our enhanced performance in joint valuation.

**Marginalization** 2D-OOB aims to attribute data contribution through cells. Consequently, by summing up 2D-OOB over all columns, we can derive data contribution values. For \(i\in[n]\), we define the 2D-OOB-data\(\psi_{i}^{data}\) as follows.

\[\psi_{i}^{data}:=\frac{1}{d}\sum_{j=1}^{d}\psi_{ij}^{\rm 2D-OOB}.\] (5)

Based on discussions in Section 3.2, the marginalizations also connect with Data-OOB:

**Proposition D.1**.: _For all \(i\in[n]\), the marginalizations \(\psi_{i}^{data}\) can be expressed as follows._

\[\psi_{i}^{data}=\mathbb{E}_{\hat{P}_{S}}[\phi_{i}^{OOB}(S)],\]

_where the notations follow the same definitions as Proposition 3.1._

Proof.: Based on the definition of 2D-OOB-Data, for \(i\in[n]\),

\[\psi_{i}^{data}:=\frac{1}{d}\sum_{j=1}^{d}\psi_{ij}^{\rm 2D-OOB} =\frac{1}{d}\sum_{j=1}^{d}\sum_{l=1}^{L}\alpha_{i,j,l}\phi_{i}^{ \rm OOB}(S_{l})\] \[=\sum_{l=1}^{L}(\frac{1}{d}\sum_{j=1}^{d}\alpha_{i,j,l})\phi_{i}^ {\rm OOB}(S_{l}),\]

where \(\alpha_{i,j,l}\) is defined in Appendix C. We have \(\sum_{l=1}^{L}(\frac{1}{d}\sum_{j=1}^{d}\alpha_{i,j,l})=\frac{1}{d}\sum_{j=1}^ {d}\sum_{l=1}^{L}\alpha_{i,j,l}=1\). Denote \(\mathbb{P}_{i}(S_{l}|\{w_{bi}\}_{b=1}^{B})=\frac{1}{d}\sum_{j=1}^{d}\alpha_{i, j,l}\), which induces the empirical expectation of Data-OOB with respect to \(S_{l}\).

Proposition D.1 indicates 2D-OOB-data \(\psi^{data}_{i}\) can be expressed as the average Data-OOB value for the \(i\)-th data point. As a result, 2D-OOB-data is expected to inherit the advanced ability of Data-OOB in terms of data valuation, as will be empirically examined next.

Experimental settingFollowing the standard protocol in Kwon and Zou [25; 26] and Jiang et al. [17], we randomly select \(10\)% of the data points and flip its label to the other class. For joint valuation methods, we calculate the valuation of each cell and perform the marginalization over features to obtain the data valuation scores. Mislabeled data detection and data removal experiments are examined based on this setting. For the baseline methods, we further incorporate several state-of-the-art data valuation methods including DataShapley [13], KNNShapley [16], DataBanzhaf [50], LAVA [19], and Data-OOB [26]. Implementation details are listed below. To guarantee a fair comparison, we also employ the decision tree as the base model in DataShapley and DataBanzhaf. We adopt the same \(12\) datasets as outlined in Section 4.1.

Data-OOB Data-OOB involves fitting a random forest model without feature subset sampling, consisting of \(1000\) decision trees.

DataShapleyWe use a Monte Carlo-based algorithm. The Gelman-Rubin statistics is computed to determine the termination criteria of the algorithm. Following Jiang et al. [17], We adopt the threshold to be \(1.05\).

KNNShapleyWe set the number of nearest neighbors to be \(10\%\) of the sample size following Jia et al. [16].

LAVAWe calculate the class-wise Wasserstein distance following Just et al. [19]. The "OTDD" framework is adopted to complete the optimal transport calculation.

DataBanzhafWe adopt the implementation from Jiang et al. [17]. We set "the number of models to train" as \(1000\).

### Mislabeled data detection

We calculate the precision-recall curve by comparing the actual annotations, which denote whether data points are mislabeled, against the data valuation scores computed by different methods. Mislabeled data typically have a detrimental impact on model performance. Therefore, data points that receive a lower valuation score are considered to have a higher chance of being mislabeled. We then determine AUCPR (the AUC of the precision-recall curve) as a quantitative metric to assess the detection efficacy.

As shown in Table 8, 2D-OOB-data consistently outperforms 2D-KNN-data across all datasets, suggesting its superior ability to detect mislabeled data points. It is worth noting that 2D-OOB-data's results are on par with Data-OOB, while significantly exceeding the performance of other data valuation methods. These results are in line with our theoretical analysis regarding the resemblance between Data-OOB and 2D-OOB-data. However, it is important to highlight that applying Data-OOB to the joint tasks is not feasible as mentioned earlier, underscoring the necessity for the development of 2D-OOB.

### Point removal experiment

Removing low-quality data points has the potential to enhance model performance. Based on this idea, we employ the point removal experiment, a widely used benchmark in data valuation [26; 13; 25]. According to the calculated data valuation scores, we progressively remove data points from the dataset in _ascending_ order. Specifically, we begin by removing the data points with the lowest data valuations. Each time we remove a datum, we fit a logistic model and use the held-out test set consisting of \(3000\) instances to evaluate the model performance. The expected behavior is that the model performance will improve initially as the detrimental data points are gradually eliminated from the training process. Removing an excessive number of data points may result in a drastically altered dataset. Consequently, we opt to remove the bottom \(20\%\) data points.

[MISSING_PAGE_FAIL:20]

Figure 9: **Point removal experiment results (test accuracy curves) of \(7\) data valuation methods – 2D-00B-data, 2D-KNN-data, Data-00B, LAVA, DataBanzhaf, DataShapley, KNNShapley and a random baseline. We remove data points from the lowest valuation to the highest valuation. The results from \(6\) binary classification datasets are displayed. For each dataset, we conduct \(30\) independent trials and report the average results. A higher curve indicates better performance. 2D-00B-data demonstrates superior ability in finding a set of helpful data points.**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper's contributions have been clearly stated in the abstract and section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper has included discussion about limitations in section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper discusses theoretical interpretation in section 3.2 and the proof has been included in Appendix C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All implementation details have been included in section 4 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The paper uses open-source datasets, detailed in Appendix A.1, and the code repository is included in the supplementary materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experiment settings have been clearly stated in section 4 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars for all experiment results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides information on compute resources in section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper fully adheres to the NeurIPS Code of Ethics as outlined in the provided guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Appendix A for dataset citations. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.