# Transformers learn to implement preconditioned gradient descent for in-context learning

 Kwangjun Ahn

MIT EECS/LIDS

kjahn@mit.edu

&Xiang Cheng

MIT LIDS

chengx@mit.edu

&Hadi Daneshmand1

MIT LIDS/FODSI

hdanesh@mit.edu

&Suvrit Sra

TU Munich / MIT

suvrit@mit.edu

Equal contribution, alphabetical order.

###### Abstract

Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: _Can transformers learn to implement such algorithms by training over random problem instances?_ To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with \(L\) attention layers, we prove certain critical points of the training objective implement \(L\) iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.

## 1 Introduction

In-context learning (ICL) is the striking capability of large language models: Given a prompt containing examples and a query, the transformer produces the correct output based on the context provided by the examples, _without adapting its parameters_(Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Black et al., 2022). This property has become the focus of body of recent research that aims to shed light on the underlying mechanism of large language models (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023; Li and Malik, 2017; Min et al., 2021; Xie et al., 2021; Elhage et al., 2021; Olsson et al., 2022).

A line of research studies ICL via the expressive power of transformers. Transformer architectures are powerful Turing machines, capable of implementing various algorithms (Perez et al., 2021; Wei et al., 2022). Given an in-context prompt, Edelman et al. (2022); Olsson et al. (2022) argue that transformers are able to implement algorithms through the recurrence of multi-head attentions to extract coarse information from raw input prompts. Akyurek et al. (2022); von Oswald et al. (2023) assert that transformers can implement gradient descent on linear regression encoded in a given input prompt. It is thought provoking that transformers can implement such algorithms.

Although transformers are universal machines to implement algorithms, they need specific parameter configurations for achieving these implementations. In practice, their parameters are adjusted via training using non-convex optimization over random problem instances. Hence, it remains unclear whether this non-convex optimization can be used to learn algorithms. The present paper investigates _the possibility of learning algorithms via training over random problem instances._More specifically, we investigate the learning of gradient-based methods. It is hard to mathematically formulate what it means to learn gradient descent for general functions with transformers. Yet, Garg et al. (2022) elegantly examine it in the specific setting of ICL for learning functions. Empirical evidence suggests that transformers indeed learn to implement gradient descent, after training on random instances of linear regression (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023). Motivated by these observations, we theoretically investigate the loss landscape of a simple transformer architecture based on _attention without softmax_(Schlag et al., 2021; von Oswald et al., 2023) (see Section 2 for details).

_Summary of our main results._ Our main contributions are the following:

* We provide a complete characterization of the global optimum of a single-layer linear transformer. In particular, we observe that, with the optimal parameters, the transformer implements a single step of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the distribution of input data but also to the variance caused by data inadequacy. We present this result in Theorem 1 in Section 3.
* Next, we focus on a subset of the transformer parameter space, defined by a special sparsity condition (8). Such a parameter configuration allows us to formulate training transformers as a search over \(k\)_-step adaptive gradient-based algorithms_. Theorem 2 characterizes the global minimizers of the training objective of a two-layer linear transformer over isotropic regression instances, and shows that the optima correspond to gradient descent with adaptive stepsizes. For multilayer transformers, Theorem 3 demonstrates that gradient descent, with a data-dependent preconditioning, can be derived from a critical point of the training objective.
* Finally, we study the loss landscape in the absence of the sparsity condition (8), which goes beyond searching over conventional gradient-based optimization methods. In this case, we prove and interpret the structure of a critical point of the training objective. We show that a certain critical point in parameter space leads to an intriguing gradient-based algorithm that simultaneously takes gradient steps preconditioned by data covariance, and applies a linear transformation to further improve the conditioning. In the specific case when data covariance is isotropic, this algorithm corresponds to the GD++ algorithm of von Oswald et al. (2023) which is experimentally observed to be the outcome of training.

We empirically validate the critical points analyzed in Theorem 3 and Theorem 4. For a transformer with three layers, our experimental results confirm the structural of critical points. Furthermore, we observed the objective value associated with these critical points is close to \(0\), suggesting that the critical points might be global optima. These experiments substantiate our theoretical analysis and suggests that our theory indeed _aligns with practice_. Code for our experiments is available at https://github.com/chengxiang/LinearTransformer.

### Related works

The ability of neural network architectures to implement algorithms has been investigated in various context. The seminal work by Siegelmann and Sontag (1992) investigate the Turing completeness of recurrent neural networks. Despite this computational power, training recurrent networks remains a challenge. Graves et al. (2014) design an alternative neural architecture known as the _neural Turing machine_, building on _attention layers_ introduced by Hochreiter and Schmidhuber (1997). Leveraging attention, Vaswani et al. (2017) propose transformers as powerful neural architectures, capable of solving various tasks in natural language processing (Devlin et al., 2019). This capability inspired a line of research that examines the algorithmic power of transformers (Perez et al., 2021; Wei et al., 2022; Giannou et al., 2023; Akyurek et al., 2022; Olsson et al., 2022). What sets transformers apart from conventional neural networks is their impressive performance after training. In this work, we focus on understanding _how transformers learn to implement algorithms_ by training over problem instances.

A line of research investigates how deep neural networks process data across their layers. The seminal work by Jastrzebski et al. (2018) observes that hidden representations across the layers of deep neural networks approximately implement gradient descent. Recent observations provide novel insights into the working mechanism of ICL for large language models, showing they can implement optimization algorithms across their layers (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023). Moreover, Zhao et al. (2023); Allen-Zhu and Li (2023) observe transformer perform dynamic programming to generate text. In this work, we theoretically study how transformer learns gradient-based algorithms for ICL.

We discuss here two related works (Zhang et al., 2023; Mahankali et al., 2023) that appeared shortly after publication of our original draft. Both of these studies focus on a single layer attention network (see Section 3). Zhang et al. (2023) prove the global convergence of gradient descent to the global optimum whose structure is analyzed independently from this study and it the same as that in Theorem 1. Mahankali et al. (2023) also characterize the global minimizer of a single layer attention without softmax for a different data distribution. In addition to results for a single-layer attention, we analyze the landscape of two and multi-layer transformers.

## 2 Setting: training linear transformers over random linear regression

In order to understand the mechanism of ICL, we consider the setting of training transformers over the random instances of linear regression, following (Garg et al., 2022; Akyurek et al., 2022; von Oswald et al., 2023). In particular, the random instances of linear regression are formalized as follows.

_Data distribution: random linear regression instances._ Let \(x^{(i)}\in\mathbb{R}^{d}\) be the covariates drawn i.i.d. from a distribution \(D_{\mathcal{X}}\), and \(w_{\star}\in\mathbb{R}^{d}\) be drawn from \(D_{\mathcal{W}}\). Let \(X\in\mathbb{R}^{(n+1)\times d}\) be the matrix of covariates whose row \(i\) contains tokens \(x^{(i)}\). Given \(x^{(i)}\)'s and \(w_{\star}\), the responses are defined as \(y=[\langle x^{(1)},w_{\star}\rangle,\ldots,\langle x^{(n)},w_{\star}\rangle] \in\mathbb{R}^{n}\). Define the _input matrix_\(Z_{0}\) as

\[Z_{0}=\begin{bmatrix}z^{(1)}\:z^{(2)}\;\cdots\;z^{(n)}\:z^{(n+1)} \end{bmatrix}=\begin{bmatrix}x^{(1)}&x^{(2)}&\cdots&x^{(n)}&x^{(n+1)}\\ y^{(1)}&y^{(2)}&\cdots&y^{(n)}&0\end{bmatrix}\in\mathbb{R}^{(d+1)\times(n+1)},\] (1)

where zero in the above matrix is used to replace the unknown response variable corresponding to \(x^{(n+1)}\). Then, our goal is to predict \(w_{\star}^{\top}x^{(n+1)}\) given \(Z_{0}\). In other words, the training data consists of pairs \((Z_{0},w_{\star}^{\top}x^{(n+1)})\) for \(x^{(i)}\sim D_{\mathcal{X}}\) and \(w_{\star}\sim D_{\mathcal{W}}\). We then consider training transformers over this data distribution.

_Self-attention layer without softmax._ Following (Schlag et al., 2021; von Oswald et al., 2023), we consider the linear self-attention layer. To motivate, we first briefly review the standard self-attention layer (Vaswani et al., 2017). Letting \(Z\in\mathbb{R}^{(d+1)\times(n+1)}\) be the input matrix with \(n+1\) tokens in \(\mathbb{R}^{d+1}\), a single-head self-attention layer denoted by \(\mathrm{Attn}^{\mathsf{smax}}\) is a parametric map defined as

\[\mathrm{Attn}^{\mathsf{smax}}_{W_{v},W_{q,v}}(Z)=W_{v}ZM\cdot \mathsf{smax}(Z^{\top}W_{k}^{\top}W_{q}Z)\,,\quad M\coloneqq\begin{bmatrix}I_ {n}&0\\ 0&0\end{bmatrix}\in\mathbb{R}^{(n+1)\times(n+1)},\] (2)

where \(W_{v},W_{k},W_{q}\in\mathbb{R}^{(d+1)\times(d+1)}\) are the (value, key and query) weight matrices, and \(\mathrm{smax}(\cdot)\) is the softmax operator which applies softmax operation to each column of the input matrix. Note that the prompt is asymmetric since the label for \(x^{(n+1)}\) is excluded from the input. To reflect this asymmetric structure, the mask matrix \(M\) is included in the attention. In our setting, we consider the self-attention layer that omits the softmax operation in (2). In particular, we reparameterize weights as \(P\coloneqq W_{v}\in\mathbb{R}^{(d+1)\times(d+1)}\) and \(Q\coloneqq W_{k}\,^{\top}W_{q}\in\mathbb{R}^{(d+1)\times(d+1)}\) and consider

\[\mathrm{Attn}_{P,Q}(Z)=PZM(Z^{\top}QZ)\,.\] (3)

At first glance, the omission of the softmax operation (3) might seem over-simplified. But, (von Oswald et al., 2023) proves such attention can implement gradient descent, and we will prove in Lemma 1 that it can also implement various algorithms to solve linear regression in-context.

_Architecture for prediction._ We now present the neural network architecture that will be used throughout this paper. For the number of layers \(L\), we define an _\(L\)-layer transformer_ as a stack of \(L\) linear self-attention blocks. Formally, denoting by \(Z_{\ell}\) the output of the \(\ell^{\text{th}}\) layer attention, we define

\[Z_{\ell+1}=Z_{\ell}+\frac{1}{n}\mathrm{Attn}_{P_{\ell},Q_{\ell}}(Z_{\ell})\quad \text{for }\ell=0,1,\ldots,L-1,\] (4)

The scaling factor \(\nicefrac{{1}}{{n}}\) is used only for ease of notation and does not influence the expressive power of the transformer. Given \(Z_{L}\), we define \(\mathsf{TF}_{L}(Z_{0};\{P_{\ell},Q_{\ell}\}_{\ell=0,1,\ldots L-1})=-[Z_{L}]_{(d+ 1),(n+1)}\), i.e., the \((d+1,n+1)\)-th entry of \(Z_{L}\). The reason for the minus sign is to be consistent with (von Oswald et al., 2023), and we will clarify such a choice in Lemma 1. For training, the parameters are optimized to minimize in-context loss as

\[f\left(\{P_{\ell},Q_{\ell}\}_{\ell=0}^{L}\right)=\mathbb{E}_{(Z_{0},w_{\star})} \Big{[}\Big{(}\mathsf{TF}_{L}(Z_{0},\{P_{\ell},Q_{\ell}\}_{\ell=0}^{L})+w_{ \star}^{\top}x^{(n+1)}\Big{)}^{2}\Big{]}.\] (5)

_Goal: the landscape analysis of the training objective functions._ We are interested in understanding how the optimization of \(f\) leads to in-context learning. We investigate this question by analyzing its loss landscape. Such analysis is challenging due to two major reasons: _(i) \(f\) is non-convex in parameters \(\{P_{i},Q_{i}\}\) even for a single layer transformer. (ii) The cross-product structures in attention makes \(f\) a highly nonlinear function in its parameters._ Hence, we analyze a spectrum of settings from single-layer transformers to multi-layer transformers. For simpler settings such as single-layer transformers, we prove stronger results such as the full characterization of the global minimizers. For networks with more layers, we characterize the structure of critical points. Furthermore, we provide algorithmic interpretations of the critical points. Table 1 summarizes our results for various parameteric models.

**Remark 1** (_Optimizing_ (5) _vs. practical transformer optimization_).: _Interestingly, a recent work by Ahn et al. (2023) reports that common optimization algorithms such as SGD/ADAM behave remarkably similarly on the (linear Transformers + linear regression) problem as they do on (practical transformers + real language modeling tasks). In particular, they reproduce several distinctive features of transformer optimization under a simple shallow linear transformer. This work suggests that (linear transformer + linear regression) may serve as a good proxy for understanding practical transformer optimization._

## 3 The global optimum for a single-layer transformer

For the single layer case of \(L=1\), the following result characterizes the optimal parameters \(P_{0}\) and \(Q_{0}\) for the in-context loss (5).

**Theorem 1** (**Single-layer; non-isotropic data**).: _Assume that vector \(x^{(i)}\) is sampled from \(\mathcal{N}(0,\Sigma)\), i.e., a Gaussian with covariance \(\Sigma=U\Lambda U^{\top}\) where \(\Lambda=\mathrm{diag}(\lambda_{1},\ldots,\lambda_{d})\). Moreover, assume that \(w_{\star}\) is sampled from \(\mathcal{N}(0,I_{d})\). Then, the following choice of parameters_

\[P_{0}=\begin{bmatrix}0_{d\times d}&0\\ 0&1\end{bmatrix},\quad Q_{0}=-\begin{bmatrix}U\mathrm{diag}\left(\left\{ \frac{1}{\frac{n+1}{n}\lambda_{i}+\frac{1}{n}\cdot\left(\sum_{k}\lambda_{k} \right)}\right\}_{i=1,\ldots,d}\right)U^{\top}&0\\ 0&0\end{bmatrix}.\] (6)

_is a global minimizer of \(f(P,Q)\) up to re-scaling, i.e., \(P_{0}\leftarrow\gamma P_{0}\) and \(Q_{0}\leftarrow\gamma^{-1}Q_{0}\) for a scalar \(\gamma\)._

See Appendix A for the proof of Theorem 1. In the specific case when the Gaussian is isotropic, i.e., \(\Sigma=I_{d}\), the optimal \(Q_{0}\) has the following simple form

\[Q_{0}=-\frac{1}{\left(\frac{n-1}{n}+(d+2)\frac{1}{n}\right)}\begin{bmatrix}I _{d}&0\\ 0&0\end{bmatrix}.\] (7)

Up to scaling, the above parameter configuration is equivalent to the parameters used by von Oswald et al. (2023) to perform one step of gradient descent. Thus, in the single-layer setting, the in-context loss is indeed minimized by a transformer that implements the gradient descent algorithm.

\begin{table}
\begin{tabular}{c|c l l l} Results & \(x^{(i)}\) & \(w_{\star}\) & Setting & Guarantees \\ \hline \hline Theorem 1 & \(\mathcal{N}(0,\Sigma)\) & \(\mathcal{N}(0,I)\) & single-layer & global minimizers \\ Theorem 2 & \(\mathcal{N}(0,I)\) & \(\mathcal{N}(0,I)\) & two-layer + symmetric (8) & global minimizers \\ Theorem 3 & \(\mathcal{N}(0,\Sigma)\) & \(\mathcal{N}(0,\Sigma^{-1})\) & multi-layer + (8) & critical points \\ Theorem 4 & \(\mathcal{N}(0,\Sigma)\) & \(\mathcal{N}(0,\Sigma^{-1})\) & multi-layer + (11) & critical points \\ Theorem 5 & \(\mathcal{N}(0,I)\) & \(\mathcal{N}(0,I)\) & single-layer + ReLU activation & global minimizers \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of our analyses for various models and input distributions. The additional conditions (8) and (11) are about the sparsity structure of parameters. In addition, “symmetric (8)” means we additionally impose the weights to be symmetric.

More generally, when the in-context samples are non-isotropic, the transformer learns to implement one step of a _preconditioned_ gradient descent as we shall detail in Lemma 1. Here the "preconditioning matrix" given in (6) has interesting properties:

* When the number of samples \(n\) is large, the first \(d\times d\) submatrix of \(Q_{0}\) approximates \(\Sigma^{-1}\), the inverse of the data covariance matrix, which is also close to the Gram matrix formed from \(x^{(1)},\ldots,x^{(n)}\). Hence the preconditioning can lead to considerably faster convergence rate when \(\Sigma\) is ill-conditioned.
* Moreover, \(\frac{1}{n}\sum_{k}\lambda_{k}\) in (6) acts as a regularizer. It becomes more significant when \(n\) is small and variance of the \(x^{(i)}\)'s is high. Such an adjustment resembles structural risk minimization (Vapnik, 1999) where the regularization strength is adapted to the sample size.

## 4 Multi-layer transformers with sparse parameters

Theorem 1 proves a single layer of linear attention can implement a single step of preconditioned gradient descent. Inspired by this result, we investigate the algorithmic power of the linear transformer architecture. We show that the model can implement various optimization methods even under sparsity constraints. In particular, we impose the following restrictions on the parameters:

\[P_{i}=\begin{bmatrix}0_{d\times d}&0\\ 0&1\end{bmatrix},\quad Q_{i}=-\begin{bmatrix}A_{i}&0\\ 0&0\end{bmatrix}\quad\text{where }A_{i}\in\mathbb{R}^{d\times d}.\] (8)

The next lemma proves that a forward-pass of a \(L\)-layer transformer, with the parameter configuration (8) is the same as taking \(L\) steps of gradient descent, preconditioned by \(A_{\ell}\).

**Lemma 1** (_Forward pass as a preconditioned gradient descent_).: _Consider the \(L\)-layer linear transformer parameterized by \(A_{0},\ldots,A_{L-1}\) as in (8). Let \(y_{\ell}^{(n+1)}\) be the \((d+1,n+1)\)-th entry of the \(\ell\)-th layer output, i.e., \(y_{\ell}^{(n+1)}=[Z_{\ell}]_{(d+1),(n+1)}\) for \(\ell=1,\ldots,L\). Then, it holds that \(y_{\ell}^{(n+1)}=-\langle x^{(n+1)},w_{\ell}^{\mathsf{gd}}\rangle\) where \(\{w_{\ell}^{\mathsf{gd}}\}\) is defined as \(w_{0}^{\mathsf{gd}}=0\) and as follows for \(\ell=1,\ldots,L-1\):_

\[w_{\ell+1}^{\mathsf{gd}}=w_{\ell}^{\mathsf{gd}}-A_{\ell}\nabla R_{w_{\star}} \left(w_{\ell}^{\mathsf{gd}}\right)\quad\text{where}\quad R_{w_{\star}}(w):= \frac{1}{2n}\sum_{i=1}^{n}(w^{\top}x_{i}-w_{\star}{}^{\top}x_{i})^{2}.\] (9)

See Subsection C.1 for a proof. The iterative scheme (9) includes various optimization methods including gradient descent with \(A_{\ell}=\gamma_{\ell}I_{d}\), and (adaptive) preconditioned gradient descent, where the preconditioner \(A_{\ell}\) depends on the time step. In the upcoming sections, we characterize how the optimal \(\{A_{\ell}\}\) are linked to the input distribution.

### Warm-up: optimal two-layer transformer with symmetric weights

For the rest of this section, we will study the optimal parameters for the in-context loss under the constraint of Eq. (8). Later in Section 5, we analyze the optimal model for a more general parameters. For a two-layer transformer, the next Theorem proves the optimal in-context loss obtains the simple gradient descent with adaptive coordinate-wise stepsizes.

**Theorem 2** (Global optimality for the two-layer (symmetric) transformer).: _Consider the optimization of in-context loss for a two-layer transformer with the parameter configuration in Eq. (8), and additionally assume that \(A_{1},A_{2}\) are symmetric matrices. More formally, consider_

\[\min_{A_{1},A_{2}\text{ are symmetric}}f\left\{P_{\ell}=\begin{bmatrix}0_{d \times d}&0\\ 0&1\end{bmatrix},\;Q_{\ell}=\begin{bmatrix}-A_{\ell}&0\\ 0&0\end{bmatrix}\right\}_{\ell=1,2}.\]

_Assume \(x^{(i)}\stackrel{{ i.d.}}{{\sim}}N(0,I_{d})\) and \(w_{\star}\sim N(0,I_{d})\); then, there are diagonal matrices \(A_{1}\) and \(A_{2}\) that are a global minimizer of \(f\)._

Combining the above result with Lemma 1 concludes that the two iterations of gradient descent with _coordinate-wise adaptive stepsizes_ achieve the minimal in-context loss for isotropic Gaussian inputs. Gradient descent with adaptive stepsizes such as Adagrad (Duchi et al., 2011) are widely used in machine learning. While Adagrad adjusts its stepsize based on the individual problem instance, the algorithm learned adjusts its stepsize to the underlying data distribution.

### Multi-layer transformers

We now turn to the setting of general \(L\)-layer transformers, for any positive integer \(L\). The next theorem proves that certain critical points of the in-context loss effectively implement a specific preconditioned gradient algorithm, where the preconditioning matrix is the inverse covariance of the input distribution. Before stating this result, let us first consider a motivating scenario in which the data-covariance matrix is non-identity:

_Linear regression with distorted view of the data:_ Suppose that \(\overline{w}_{\star}\sim\mathcal{N}(0,I)\) and the _latent_ covariates are \(\overline{x}^{(1)},\ldots,\overline{x}^{(n+1)}\), drawn i.i.d from \(\mathcal{N}(0,I)\). We are given \(y^{(1)},\ldots,y^{(n)}\), with \(y^{(i)}=\left\langle\overline{x}^{(i)},\overline{w}_{\star}\right\rangle\). However, we _do not observe_ the latent covariates \(\overline{x}^{(i)}\). Instead, we observe the _distorted_ covariates \(x^{(i)}=W\overline{x}^{(i)}\), where \(W\in\mathbb{R}^{d\times d}\) is a distortion matrix. Thus the prompt consists of \((x^{(1)},y^{(1)}),\ldots,(x^{(n)},y^{(n)})\), as well as \(x^{(n+1)}\). The goal is still to predict \(y^{(n+1)}\). Note that this setting is quite common in practice, when covariates are often represented in an arbitrary basis.

Assume that \(\Sigma:=WW^{\top}\succ 0\). We verify from our definitions that for \(w_{\star}:=\Sigma^{-1/2}\overline{w}_{\star}\), \(y^{(i)}=\left\langle x^{(i)},w_{\star}\right\rangle\). Furthermore, \(x^{(i)}\sim\mathcal{N}(0,\Sigma)\) and \(w_{\star}\sim\mathcal{N}(0,\Sigma^{-1})\). From Lemma 1, the transformer with weight matrices \(\{A_{0},\ldots,A_{L-1}\}\) implements preconditioned gradient descent with respect to \(R_{w_{\star}}(w)=\frac{1}{2n}(w-w_{\star})^{T}XX^{\top}(w-w_{\star})\), with \(X=\left[x^{(1)},\ldots,x^{(n)}\right]\). Under this loss, the Hessian matrix \(\nabla^{2}R_{w_{\star}}(w)=\frac{1}{2n}XX^{\top}\) (at least in the case of large \(n\)). For any fixed prompt, Newton's method corresponds to \(A_{i}\propto\left(XX^{\top}\right)^{-1}\), which makes the problem well-conditioned even if \(\Sigma\) is very degenerate. As we will see in Theorem 3 below, the choice of \(A_{i}\propto\Sigma^{-1}=\mathbb{E}\left[XX^{\top}\right]^{-1}\) appears to be a _stationary point_ of the loss landscape, in expectation over prompts.

Before stating the theorem, we introduce the following simplified notation: let \(A:=\{A_{i}\}_{i=0}^{L-1}\in\mathbb{R}^{L\times d\times d}\). We use \(f(A)\) to denote the in-context loss of \(f\left(\{P_{i},Q_{i}\}_{i=0}^{L-1}\right)\) as defined in (5), when \(Q_{i}\) depends on \(A_{i}\), and \(P_{i}\) is a constant matrix, as described in (8).

**Theorem 3**.: _Assume that \(x^{(i)}\overset{iid}{\sim}\mathcal{N}(0,\Sigma)\) and \(w_{\star}\sim\mathcal{N}(0,\Sigma^{-1})\), for \(i=1,\ldots,n\), and for some \(\Sigma\succ 0\). Consider the optimization of in-context loss for a \(k\)-layer transformer with the the parameter configuration in Eq. (8) given by:_

\[\min_{\{A_{i}\}_{i=0}^{L-1}}f\left(A\right).\]

_Let \(\mathcal{S}\subset\mathbb{R}^{L\times d\times d}\) be defined as follows: \(A\in\mathcal{S}\) if and only if for all \(i=0,\ldots,L-1\), there exists scalars \(a_{i}\in\mathbb{R}\) such that \(A_{i}=a_{i}\Sigma^{-1}\). Then_

\[\inf_{(A,B)\in\mathcal{S}}\sum_{i=0}^{L-1}\left\|\nabla_{A_{i}}f(A,B)\right\| _{F}^{2}=0,\]

_where \(\nabla_{A_{i}}f\) denotes derivative wrt the Frobenius norm \(\left\|A_{i}\right\|_{F}\)._

As discussed in the motivation above, under the setting of \(A_{i}=a_{i}\Sigma^{-1}\), the linear transformer implements an algorithm that is reminiscent of Newton's method (as well as a number of other adaptive algorithms such as the full-matrix variant of Adagrad); these can converge significantly faster than vanilla gradient descent when the problem is ill-conditioned. The proposed parameters \(A_{i}\) in Theorem 3 are also similar to \(A_{i}\)'s in Theorem 1 when \(n\) is large. However, in contrast to Theorem 1, there is no trade-off with statistical robustness; this is because \(w_{\star}\) has covariance matrix \(\Sigma^{-1}\) in the Theorem 3, while Theorem 1 has isotropic \(w_{\star}\).

Unlike our prior results, Theorem 3 only guarantees that the set \(\mathcal{S}\) of transformer prameters satisfying \(\left\{A_{i}\propto\Sigma^{-1}\right\}_{i=0}^{L-1}\)_essentially2_ contains critical points of the in-context loss. However, in the next section, we show experimentally that this choice of \(A_{i}\)'s does indeed seem to be recovered by training.

Footnote 2: A subtle issue is that the infimum may not be attained, so it is possible that \(\mathcal{S}\) contains points with arbitrarily small gradient, but does not contain a point with exactly \(0\) gradient.

We defer the proof of Theorem 3 to Subsection B.2. Due to the complexity of the transformer function, even verifying critical points can be challenging. We show that the in-context loss can be equivalently written as (roughly) a matrix polynomial involving the weights at each layer. Byexploiting invariances in the underlying distribution of prompts, we construct a flow, contained entirely in \(\mathcal{S}\), whose objective value decreases as fast as gradient flow. Since \(f\) is lower bounded, we conclude that there must be points in \(\mathcal{S}\) whose gradient is arbitrarily small.

### Experimental validations for Theorem 3

We present here an empirical verification of our results in Theorem 3. We consider the ICL loss for linear regression. The dimension is \(d=5\), and the number of training samples in the prompt is \(n=20\). Both \(x^{(i)}\sim\mathcal{N}(0,\Sigma)\) and \(w_{\star}\sim\mathcal{N}(0,\Sigma^{-1})\), where \(\Sigma=U^{T}DU\), where \(U\) is a uniformly random orthogonal matrix, and \(D\) is a fixed diagonal matrix with entries \((1,1,0.25,0.0625,1)\).

We optimizes \(f\) for a three-layer linear transformer using ADAM, where the matrices \(A_{0},A_{1},\) and \(A_{2}\) are initialized by i.i.d. Gaussian matrices. Each gradient step is computed from a minibatch of size 20000, and we resample the minibatch every 100 steps. We clip the gradient of each matrix to 0.01. All plots are averaged over \(5\) runs with different \(U\) (i.e. \(\Sigma\)) sampled each time.

Figure 0(d) plots the average loss. We observe that the training converges to an almost \(0\) value, suggesting the convergence to global minimum. The parameters at convergence match the stationary point introduced in Theorem 3, and indeed appear to be globally optimal.

To quantify the similarity between \(A_{0},A_{1},A_{2}\) and \(\Sigma^{-1}\) (up to scaling), we use the _normalized Frobenius norm distance_: \(\mathrm{Dist}(M,I):=\min_{\alpha}\frac{\|M-\alpha\cdot I\|}{\|M\|_{F}}\), (equivalent to choosing \(\alpha:=\frac{1}{d}\sum_{i=1}^{d}M[i,i]\)). This is essentially the projection distance of \(M/\|M\|_{F}\) onto the space of scaled identity matrices.

We plot \(\mathrm{Dist}\left(A_{i},I\right)\), averaged over \(5\) runs, against iteration in Figures 0(a),0(b),0(c). In each plot, the blue line represents \(\mathrm{Dist}(\Sigma^{1/2}A_{i}\Sigma^{1/2},I)\), and we verify that the optimal parameters are converging to the critical point introduced in Theorem 3, which implements preconditioned gradient descent. The red line represents \(\mathrm{Dist}(A_{i},I)\); it remains constant indicating that the trained transformer is not implementing plain gradient descent. Figures 1(a)-1(c) visualize each \(\Sigma^{1/2}A_{i}\Sigma^{1/2}\) matrix at the end of training to further validate that the learned parameter is as described in Theorem 3.

## 5 Multi-layer transformers beyond standard optimization methods

In this section, we study the more general setting of

\[P_{i}=\begin{bmatrix}B_{i}&0\\ 0&1\end{bmatrix},\quad Q_{i}=\begin{bmatrix}A_{i}&0\\ 0&0\end{bmatrix}\quad\text{where }A_{i},B_{i}\in\mathbb{R}^{d\times d}.\] (11)

Note that \(A_{i},B_{i}\) are not constrained to be symmetric. Similar to Section 4, we introduce the following simplified notation: let \(A:=\left\{A_{i}\right\}_{i=0}^{L-1}\in\mathbb{R}^{L\times d\times d}\) and \(B:=\left\{B_{i}\right\}_{i=0}^{L-1}\in\mathbb{R}^{L\times d\times d}\). We use \(f(A,B)\) to denote the in-context loss of \(f\left(\left\{P_{i},Q_{i}\right\}_{i=0}^{L-1}\right)\) as defined in (5), when \(P_{i}\) and \(Q_{i}\) depend on \(B_{i}\) and \(A_{i}\) as described in (11).

Figure 1: Plots for verifying convergence of general linear transformer, defined in Theorem 3. Figure (d) shows convergence of loss to \(0\). Figures (a),(b),(c) illustrate convergence of \(A_{i}\)’s to identity. More specifically, the blue line represents \(\mathrm{Dist}(\Sigma^{1/2}A_{i}\Sigma^{1/2},I)\), which measures the convergence to the critical point introduced in Theorem 3 (corresponding to \(\Sigma^{-1}\)-preconditioned gradient descent). The red line represents \(\mathrm{Dist}(A_{i},I)\); it remains constant indicating that the trained transformer is not implementing plain gradient descent.

With this relaxed parameter configuration, it turns out transformers can learn algorithms beyond the conventional preconditioned gradient descent. The next theorem asserts the possibility of learning a novel preconditioned gradient method. Let \(L\) be a fixed but arbitrary number of layers.

**Theorem 4**.: _Let \(\Sigma\) denote any PSD matrix. Assume that \(x^{(i)}\overset{iid}{\sim}\mathcal{N}(0,\Sigma)\) and \(w_{\star}\sim\mathcal{N}(0,\Sigma^{-1})\), for \(i=1,\ldots,n\), and for some \(\Sigma\succ 0\). Consider the optimization of in-context loss for a \(L\)-layer linear transformer with the the parameter configuration in Eq. (11) given by:_

\[\min_{\{A_{i},B_{i}\}_{i=0}^{L-1}}f\left(A,B\right).\]

_Let \(\mathcal{S}\subset\mathbb{R}^{2\times L\times d\times d}\) be defined as follows: \((A,B)\in\mathcal{S}\) if and only if for all \(i\in\{0,\ldots,k\}\), there exists scalars \(a_{i},b_{i}\in\mathbb{R}\) such that \(A_{i}=a_{i}\Sigma^{-1}\) and \(B_{i}=b_{i}I\). Then_

\[\inf_{(A,B)\in\mathcal{S}}\sum_{i=0}^{L-1}\left\|\nabla_{A_{i}}f(A,B)\right\| _{F}^{2}+\left\|\nabla_{B_{i}}f(A,B)\right\|_{F}^{2}=0,\] (12)

_where \(\nabla_{A_{i}}f\) denotes derivative wrt the Frobenius norm \(\left\|A_{i}\right\|_{F}\)._

In words, parameter matrices in \(\mathcal{S}\) implement the following algorithm: \(\left\{A_{i}=a_{i}\Sigma^{-1}\right\}_{i=0}^{L-1}\) plays the role of a distribution-dependent preconditioner for the gradient steps. At the same time, \(B_{i}=b_{i}I\) transforms the covariates themselves to make the Gram matrix have better condition number with each iteration. When the \(\Sigma=I\), the algorithm implemented by \(A_{i}\propto I,b_{i}\propto I\) is exactly the GD++ algorithm proposed in (von Oswald et al., 2023) (up to stepsize).

The result in (12) says that the set \(\mathcal{S}\)_essentially3_ contains critical points of the in-context loss \(f(A,B)\). In the next section, we provide empirical evidence that the trained transformer parameters do in fact converge to a point in \(\mathcal{S}\).

Footnote 3: Once again, similar to the case of Theorem 3, the infimum may not be attained, so it is possible that \(\mathcal{S}\) contains points with arbitrarily small gradient, but does not contain a point with exactly \(0\) gradient.

### Experimental validations for Theorem 4

The experimental setup is similar to Subsection 4.3: we consider ICL for linear regression with \(n=10,d=5\), with \(x^{(i)}\sim\mathcal{N}(0,\Sigma)\) and \(w_{\star}\sim\mathcal{N}(0,\Sigma^{-1})\), where \(\Sigma=U^{T}DU\), where \(U\) is a uniformly random orthogonal matrix, and \(D\) is a fixed diagonal matrix with entries \((1,1,0.25,0.0625,1)\). We train a three-layer linear transformer, under the constraints in (11) which is less restrictive than (8) in Subsection 4.3. We train the matrices \(A_{0},A_{1},A_{2},B_{0},B_{1}\)4 using ADAM with the same setup as in Section Subsection 4.3. We repeat this experiment 5 times with different random seeds, each time we sample a different \(U\) (i.e. \(\Sigma\)).

Figure 2: Visualization of learned weights for the setting of Theorem 3. We visualize each \(\Sigma^{1/2}A_{i}\Sigma^{1/2}\) matrix at the end of training. Note that the optimized weights match the stationary point discussed in Theorem 3.

In Figure 2(c), we plot the in-context loss through the iterations of ADAM; the loss appears to be converging to 0, suggesting that parameters are converging to the global minimum.

We next verify that the parameters at convergence are consistent with Theorem 4. We will once again use \(\mathrm{Dist}(M,I)\) to measure the distance from \(M\) to the identity matrix, up to scaling (see Subsection 4.3 for definition of \(Dist\)). Figures 2(a) and 2(b) show that \(B_{0}\) and \(B_{1}\) are close to identity, as \(\mathrm{Dist}(B_{i},I)\) appears to be decreasing to 0. Figures 2(d), 2(e) and 2(f) plot \(\mathrm{Dist}(A_{i},I)\) (red line) and \(\mathrm{Dist}(\Sigma^{1/2}A_{i}\Sigma^{1/2},I)\) (blue line); the results here suggest that \(A_{i}\) is converging to \(\Sigma^{-1}\), up to scaling. In Figures 2(a) and 2(b), we observe that \(B_{0}\) and \(B_{1}\) also converge to the identity matrix (_without_ left and right multiplication by \(\Sigma^{1/2}\)), consistent with Theorem 4.

We visualize each of \(B_{0},B_{1}\) in Figure 3(d) and \(A_{0},A_{1},A_{2}\) in Figure 4(a)-4(c) at the end of training. We highlight two noteworthy observations:

1. Let \(X_{k}\in\mathbb{R}^{d\times n}\) denote the first \(d\) rows of \(Z_{k}\), which are the output at layer \(k-1\) defined in (4). Then the update to \(X_{k}\) is \(X_{k+1}=X_{k}+B_{k}X_{k}MX_{k}^{T}A_{k}X_{k}\approx X_{k+1}=X_{k}\left(I-|a_{ k}b_{k}|MX_{k}^{T}X_{k}\right)\), where \(M\) is a mask defined in (2). As noted by von Oswald et al. (2023), this may be motivated by curvature correction.
2. As seen in Figures 4(a)-4(c) in the Appendix, \(\|A_{0}\|\leq\|A_{1}\|\leq\|A_{2}\|\) that implies the transformer implements gradient descent with a small stepsize at the beginning and a large stepsize at the end. This makes intuitive sense as \(X_{2}\) is better-conditioned compared to \(X_{1}\), due to the choice of \(B_{0},B_{1}\). This can be contrasted with the plots in Figures (2a)-(2c), where similar trends are not as pronounced because \(B_{i}\)'s are constrained to be 0.

## 6 Discussion

We take a first step toward proving that transformers can learn algorithms when trained over a set of random problem instances. Specifically, we investigate the possibility of learning gradient based methods when training on the in-context loss for linear regression. For a single layer transformer, we prove that the global minimum corresponds to a single iteration of preconditioned gradient descent. For multiple layers, we show that certain parameters that correspond to the critical points of the in-context loss can be interpreted as a broad family of adaptive gradient-based algorithms.

Figure 3: Plots for verifying convergence of general linear transformer, defined in Theorem 4. Figure (c) shows convergence of loss to \(0\). Figures (a),(b) illustrate convergence of \(B_{0},B_{1}\) to identity. Figures (d),(e),(f) illustrate convergence of \(A_{i}\)’s to \(\Sigma^{-1}\).

We discuss below two interesting future directions.

**Beyond linear attention.** The standard transformer architecture comes with nonlinear activations in attention. Hence, the natural question here is to ask the effect of nonlinear activations for our main results. Empirically, von Oswald et al. (2023) have observed that for linear regression task, softmax activations generally degrade the prediction performance, and in particular, softmax transformers typically need more attention heads to match their performance with that of linear transformers.

As a first step analysis, we consider the nonlinear attention defined as

\[\mathrm{Attn}_{P,Q}^{\sigma}(Z)\coloneqq PZM\;\sigma(Z^{\top}QZ)\quad\text{ where }\sigma:\mathbb{R}\to\mathbb{R}\text{ is applied entry-wise}.\]

The following result is an analog of Theorem1 for single-layer nonlinear attention. It characterizes a global minimizer for this setting with ReLU activation. Here, our choice of ReLU activation was motivated by Wortsman et al. (2023) who observed that ReLU attention matches the performance of softmax attention for vision transformers.

**Theorem 5**.: _Consider the single layer nonlinear attention setting with \(\sigma=\mathrm{ReLU}\). Assume that vector \(x^{(i)}\) is sampled from \(\mathcal{N}(0,I_{d})\). Moreover, assume that \(w_{\star}\) is sampled from \(\mathcal{N}(0,I_{d})\). Consider the parameter configuration \(P_{0},Q_{0}\) where we additionally assume that the last row of \(Q_{0}\) is zero. Then, the following parameters form a global minimizer of the corresponding in-context loss:_

\[P_{0}=\begin{bmatrix}0_{d\times d}&0\\ 0&1\end{bmatrix},\quad Q_{0}=-\frac{1}{\frac{1}{2}\frac{n-1}{n}+(d+2)\frac{1}{ n}}\cdot\begin{bmatrix}I_{d}&0\\ 0&0\end{bmatrix}.\]

The proof of Theorem5 involves an instructive argument and leverages tools from Erdogdu et al. (2016); we defer it to SubsectionA.4. Thus, for isotropic Gaussian data, the structure of global minimum under ReLU attention is similar to the global minimum with linear attention, established in Theorem1 (specifically the minimizer for the isotropic date given in (7)).

**Refined landscape analysis for multilayer transformer.** Theorem4 proves that a stationary point of the in-context loss corresponds to implementing a preconditioned gradient method. However, we do not prove that all critical points of the non-convex objective lead to similar optimization methods. In fact, in Lemma4 in AppendixB, we prove that the in-context loss can have multiple critical points. It will be interesting to analyze the set of all critical points and try to understand their algorithmic interpretations, as well as quantify their (sub)optimality.

## Acknowledgments and Disclosure of Funding

We thank Ekin Akyurek, Johannes von Oswald, Alex Gu and Joshua Robinson for helpful discussions. Kwangjun Ahn was supported by the ONR grant (N00014-20-1-2394) and MIT-IBM Watson as well as a Vannevar Bush fellowship from Office of the Secretary of Defense. Kwangjun Ahn also acknowledges support from the Kwanjeong Educational Foundation. Xiang Cheng acknowledges support from NSF CCF-2112665 (TILOS AI Research Institute). Hadi Daneshmand acknowledges support from NSF TRIPODS program (award DMS-2022448). Suvrit Sra acknowledges support from an NSF CAREER grant (1846088), and NSF CCF-2112665 (TILOS AI Research Institute).

Figure 4: Visualization of optimized weight matrices \(B_{0}\) (left) and \(B_{1}\) (right). One can see that the weight pattern matches the stationary point analyzed in Theorem4. Matrices \(A_{0}\), \(A_{1}\) and \(A_{2}\) are similar to Figure2, and are visualized in Figure5 in AppendixD.

## References

* Ahn et al. (2023) Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). _arXiv preprint arXiv:2310.01082_, 2023.
* Akyurek et al. (2022) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _International Conference on Learning Representations_, 2022.
* Allen-Zhu and Li (2023) Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. _arXiv preprint arXiv:2305.13673_, 2023.
* Alon and Spencer (2016) Noga Alon and Joel H Spencer. _The probabilistic method_. John Wiley & Sons, 2016.
* Workshop on Challenges & Perspectives in Creating Large Language Models_, 2022.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Neural Information Processing Systems_, 2020.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1_, 2019.
* Duchi et al. (2011) John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7), 2011.
* Edelman et al. (2022) Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning (ICML)_, 2022.
* Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. https://transformer-circuits.pub/2021/framework/index.html.
* Erdogdu et al. (2016) Murat A Erdogdu, Lee H Dicker, and Mohsen Bayati. Scaled least squares estimator for glms in large-scale problems. _Advances in Neural Information Processing Systems_, 29, 2016.
* Garg et al. (2022) Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* Giannon et al. (2023) Angeliki Giannon, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. _arXiv preprint arXiv:2301.13196_, 2023.
* Graves et al. (2014) Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. _arXiv preprint arXiv:1410.5401_, 2014.
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 1997.
* Jastrzebski et al. (2018) Stanislaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual connections encourage iterative inference. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=SJa9iHgAZ.
* Li and Malik (2017) Ke Li and Jitendra Malik. Learning to optimize. In _International Conference on Learning Representations_, 2017.
* Li et al. (2018)* Lieber et al. (2021) Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. _White Paper. AI21 Labs_, 2021.
* Mahankali et al. (2023) Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. _arXiv preprint arXiv:2307.03576_, 2023.
* Min et al. (2021) Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. _Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021.
* Olsson et al. (2022) Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _Transformer Circuits Thread_, 2022.
* Perez et al. (2021) Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. _The Journal of Machine Learning Research_, 2021.
* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021.
* Schlag et al. (2021) Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In _International Conference on Machine Learning_, pages 9355-9366. PMLR, 2021.
* Siegelmann and Sontag (1992) Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. In _Proceedings of Workshop on Computational learning theory_, 1992.
* Vapnik (1999) Vladimir Vapnik. _The nature of statistical learning theory_. Springer science & business media, 1999.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 2017.
* Oswald et al. (2023) Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Wei et al. (2022) Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022.
* Wortsman et al. (2023) Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Replacing softmax with relu in vision transformers. _arXiv preprint arXiv:2309.08586_, 2023.
* Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _International Conference on Learning Representations_, 2021.
* Zhang et al. (2023) Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_, 2023.
* Zhao et al. (2023) Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023.

**Appendix**

* Proofs for the single layer case
* 1 Rewriting the loss function
* 2 Warm-up: proof for the isotropic data
* 3 Proof for the non-isotropic case
* 4 Proof for non-linear attentions (Theorem 5)
* Proofs for the multi-layer case
* 1 Proof of Theorem 2
* 2 Proof of Theorem 3
* 3 Proof of Theorem 4
* 4 Equivalence under permutation
* Auxiliary Lemmas
* 1 Proof of Lemma 1 (Equivalence to Preconditioned Gradient Descent)
* 2 Reformulating the in-context loss
* 2 Additional experimental results

## Appendix A Proofs for the single layer case

In this section, we prove our characterization of global minima for the single layer case (Theorem 1). We begin by simplifying the loss into a more concrete form. Throughout the proof, we will write \(P,Q\) instead of \(P_{0},Q_{0}\) for brevity.

### Rewriting the loss function

Recall the in-context loss (5) for the single layer case \(f(P,Q)\) is defined as:

\[f\left(P,Q\right)=\mathbb{E}_{Z_{0},w_{*}}\left[\left(Z_{0}+\frac{1}{n} \mathrm{Attn}_{P,Q}(Z_{0})\right)_{(d+1),(n+1)}+w_{*}^{\top}x^{(n+1)}\right]^{2}\]

From the definition of attention given in (3), one can further spell out the expression \(Z_{0}+\frac{1}{n}\mathrm{Attn}_{P,Q}(Z_{0})\) using the notation \(Z_{0}=[z^{(1)}\;z^{(2)}\;\cdots\;z^{(n+1)}]\) as follows:

\[[z^{(1)}\;\cdots\;z^{(n+1)}]+\frac{1}{n}P[z^{(1)}\;\cdots\;z^{(n+1 )}]M\left([z^{(1)}\;\cdots\;z^{(n+1)}]^{\top}Q[z^{(1)}\;\cdots\;z^{(n+1)}]\right)\] \[=[z^{(1)}\;\cdots\;z^{(n+1)}]+\frac{1}{n}P\left(\sum_{i=1}^{n}z^{ (i)}{z^{(i)}}^{\top}\right)Q[z^{(1)}\;\cdots\;z^{(n+1)}]\,.\]

Thus, the last column of the above matrix can be expressed as

\[\begin{bmatrix}x^{(n+1)}\\ 0\end{bmatrix}+\frac{1}{n}P\left(\sum_{i=1}^{n}z^{(i)}{z^{(i)}}^{\top}\right)Q \begin{bmatrix}x^{(n+1)}\\ 0\end{bmatrix}\,,\]

where note that the summation is for \(i=1,2,\ldots,n\) due to the mask matrix \(M\). Therefore, letting \(b^{\top}\) be the last row of \(P\), and \(A\in\mathbb{R}^{d+1,d}\) be the first \(d\) columns of \(Q\) (as we did in (14)), then \(\left[Z_{0}+\frac{1}{n}\mathrm{Attn}_{P,Q}(Z_{0})\right]_{(d+1),(n+1)}\) can be written as

\[\frac{1}{n}b^{\top}\left(\sum_{i=1}^{n}z^{(i)}{z^{(i)}}^{\top}\right)A \begin{bmatrix}x^{(n+1)}\\ 0\end{bmatrix}\,,\] (13)in other words, \(f(P,Q)\) only depends on the parameter \(b\) and \(A\). Henceforth, we will write \(f(P,Q)\) as \(f(b,A)\). Let us summarize our conclusion so far since it's crucial for the analysis to follow.

**Conclusion so far:** A careful inspection reveals that the in-context loss only depends on the last row of \(P\) and the first \(d\) columns of \(Q\). Thus, consider the following parametrization

\[P=\begin{bmatrix}0\\ b^{\top}\end{bmatrix}\quad\text{and}\quad Q=[A\quad 0]\,\text{ where }b\in\mathbb{R}^{d+1}\text{ and }A\in\mathbb{R}^{(d+1)\times d}.\] (14)

Now with this parametrization, the in-context loss can be written as \(f(b,A)\coloneqq f([0\;\;b]^{\top},[A\;0])\).

Now, let us spell out \(f(b,A)\) based on (13) as follows:

\[f(b,A) =\mathbb{E}_{Z_{0},w_{\star}}\left[b^{\top}\underbrace{\frac{1}{n }\sum_{i}{z^{(i)}{z^{(i)}}}^{\top}}_{=:\mathsf{G}}Ax^{(n+1)}+w_{\star}^{\top} x^{(n+1)}\right]^{2}\] \[=:\mathbb{E}_{Z_{0},w_{\star}}\left[b^{\top}\mathsf{G}Ax^{(n+1)} +w_{\star}^{\top}x^{(n+1)}\right]^{2}=\mathbb{E}_{Z_{0},w_{\star}}\left[(b^{ \top}\mathsf{G}A+w_{\star}^{\top})x^{(n+1)}\right]^{2}\,\] (15)

where we used the notation \(\mathsf{G}\coloneqq\frac{1}{n}\sum_{i}{z^{(i)}{z^{(i)}}}^{\top}\) to simplify. We now analyze the global minima of this loss function. To illustrate the proof idea clearly, we begin with the proof for the simpler case of isotropic data.

### Warm-up: proof for the isotropic data

As a warm-up, we first prove the result for the special case where \(x^{(i)}\) is sampled from \(\mathcal{N}(0,I_{d})\).

_1. Decomposing the loss function into components._ Writing \(A=[a_{1}\;a_{1}\;\cdots\;a_{d}]\), and use the fact that \(\mathbb{E}[x^{(n+1)}[j]x^{(n+1)}[j^{\prime}]]=0\) for \(j\neq j^{\prime}\) and \(\mathbb{E}[x^{(n+1)}[j]^{2}]=1\), we get

\[f\left(b,A\right)=\sum_{j=1}^{d}\mathbb{E}_{Z_{0},w_{\star}}\left[b^{\top} \mathsf{G}\,a_{j}+w_{\star}[j]\right]^{2}\mathbb{E}[x^{(n+1)}[j]^{2}]=\sum_{j =1}^{d}\mathbb{E}_{Z_{0},w_{\star}}\left[b^{\top}\mathsf{G}\,a_{j}+w_{\star}[ j]\right]^{2}\.\]

The key idea is to characterize the global minima of each component in the summation separately. Another key idea is to reparametrize the cost function given the following identity:

\[\mathbb{E}_{Z_{0},w_{\star}}\left[b^{\top}\mathsf{G}\,a_{j}+w_{\star}[j] \right]^{2}=\mathbb{E}_{Z_{0},w_{\star}}\left[\operatorname{Tr}(\mathsf{G}\,a_ {j}b^{\top})+w_{\star}[j]\right]^{2}=\mathbb{E}_{Z_{0},w_{\star}}\left[\left< \mathsf{G},ba_{j}^{\top}\right>+w_{\star}[j]\right]^{2}\,\]

where we use the notation \(\left<X,Y\right>\coloneqq\operatorname{Tr}(XY^{\top})\) for two matrices \(X\) and \(Y\) here and below. Given the above identity, we define each component in the summation as follows.

_2. Characterizing global minima of each component._ To characterize the global minima of each objective, we prove the following result.

**Lemma 2** (**Global minima of each component**).: _Suppose that \(x^{(i)}\) is sampled from \(\mathcal{N}(0,I_{d})\) and \(w_{\star}\) is sampled from \(\mathcal{N}(0,I_{d})\). Consider the following objective (here, \(\left<X,Y\right>\coloneqq\operatorname{Tr}(XY^{\top})\) for two matrices \(X\) and \(Y\))_

\[f_{j}(X)=\mathbb{E}_{Z_{0},w_{\star}}\left[\left<\mathsf{G},X\right>+w_{\star} [j]\right]^{2}\.\]

_Then a global minimum is given as_

\[X_{j}=-\frac{1}{\left(\frac{n-1}{n}+(d+2)\frac{1}{n}\right)}E_{d+1,j}\,\]

_where \(E_{i_{1},i_{2}}\) is the matrix whose \((i_{1},i_{2})\)-th entry is \(1\), and the other entries are zero._

_3.

Proof of Lemma 2.: Note first that \(f_{j}\) is convex in \(X\). Hence, in order to show that matrix \(X_{j}\) is the global optimum of \(f_{j}\), it suffices to show that the gradient vanishes at that point, in other words, \(\nabla f_{j}(X_{j})=0\). To verify this, let us compute the gradient of \(f_{j}\): for a matrix \(X\),

\[\nabla f_{j}(X)=2\,\mathbb{E}\left[\left\langle\mathsf{G},X\right\rangle \mathsf{G}\right]+2\,\mathbb{E}\left[w_{\star}[j]\;\mathsf{G}\right]\,,\]

where we recall that \(\mathsf{G}\) is defined as

\[\mathsf{G}=\frac{1}{n}\sum_{i}\begin{bmatrix}{x^{(i)}}{x^{(i)}}^{\top}&{y^{( i)}}{x^{(i)}}\\ {y^{(i)}}{x^{(i)}}^{\top}&{y^{(i)}}^{2}\end{bmatrix}.\]

To verify that the gradient is equal to zero, let us first compute \(\mathbb{E}\left[w_{\star}[j]\;\mathsf{G}\right]\). For each \(i=1,\ldots,n\), note that \(\mathbb{E}[w_{\star}[j]\;{x^{(i)}}{x^{(i)}}^{\top}]=O\) because \(\mathbb{E}[w_{\star}]=0\). Moreover, \(\mathbb{E}[w_{\star}[j]\;{y^{(i)}}^{2}]=0\) because \(w_{\star}\) is symmetric, i.e., \(w_{\star}\stackrel{{ d}}{{=}}-w_{\star}\), and \(y^{(i)}=\left\langle w_{\star},x^{(i)}\right\rangle\). Lastly, for \(k=1,2,\ldots,d\), we have

\[\mathbb{E}[w_{\star}[j]\;{y^{(i)}}\;{x^{(i)}}[k]]=\mathbb{E}[w_{\star}[j]\; \left\langle w_{\star},x^{(i)}\right\rangle{x^{(i)}}[k]]=\mathbb{E}\left[w_{ \star}[j]^{2}\;{x^{(i)}}[j]\;{x^{(i)}}[k]\right]=\mathds{1}_{[j=k]}\] (16)

because \(\mathbb{E}[w_{\star}[i]\;w_{\star}[j]]=0\) for \(i\neq j\). Combining the above calculations, it follows that

\[\left\lceil\frac{\mathbb{E}\left[w_{\star}[j]\;\mathsf{G}\right]}{\mathsf{E} }=E_{d+1,j}+E_{j,d+1}\,.\right\rceil\] (17)

In order to compute \(\mathbb{E}\left[\left\langle\mathsf{G},X\right\rangle\mathsf{G}\right]\), let us compute \(\mathbb{E}\left[\left\langle\mathsf{G},E_{i,i^{\prime}}\right\rangle\mathsf{G}\right]\) for \(i,i^{\prime}=1,\ldots,d+1\). Without loss of generality, \(i\geq i^{\prime}\). First of all We now compute \(\mathbb{E}\left[\left\langle\mathsf{G},E_{d+1,j}\right\rangle\mathsf{G}\right]\). Note first that

\[\left\langle\mathsf{G},E_{d+1,j}\right\rangle=\sum_{i}\langle w_{\star},x^{( i)}\rangle\;{x^{(i)}}[j]\,.\]

Hence, it holds that

\[\mathbb{E}\left[\left\langle\mathsf{G},E_{d+1,j}\right\rangle\left(\sum_{i}{ x^{(i)}}{x^{(i)}}^{\top}\right)\right]=\mathbb{E}\left[\left(\sum_{i}\langle w_{ \star},x^{(i)}\rangle\;{x^{(i)}}[j]\right)\left(\sum_{i}{x^{(i)}}^{\top}\right) \right]=O\,.\]

because \(\mathbb{E}[w_{\star}]=0\). Next, we have

\[\mathbb{E}\left[\left\langle\mathsf{G},E_{d+1,j}\right\rangle\left(\sum_{i}{ y^{(i)}}^{2}\right)\right]=\mathbb{E}\left[\left(\sum_{i}\langle w_{\star},x^{(i)} \rangle\;{x^{(i)}}[j]\right)\left(\sum_{i}{y^{(i)}}^{2}\right)\right]=0\]

because \(w_{\star}\stackrel{{ d}}{{=}}-w_{\star}\). Lastly, we compute

\[\mathbb{E}\left[\left\langle\mathsf{G},E_{d+1,j}\right\rangle\left(\sum_{i}{ y^{(i)}}^{\top}\right)\right]\,.\]

To that end, note that for \(j\neq j^{\prime}\),

\[\mathbb{E}\left[\left\langle w_{\star},x^{(i)}\right\rangle\;{x^{(i)}}[j]\; \left\langle w_{\star},x^{(i^{\prime})}\right\rangle\;{x^{(i^{\prime})}}^{ \top}[j^{\prime}]\right]=\begin{cases}\mathbb{E}[\left\langle x^{(i)},x^{(i^{ \prime})}\right\rangle x^{(i)}[j]\;{x^{(i^{\prime})}}[j^{\prime}]]=0&\text{if }i \neq i^{\prime},\\ \mathbb{E}[\left\|x^{(i)}\right\|^{2}\;{x^{(i)}}[j]\;{x^{(i)}}[j^{\prime}]]=0& \text{if }i=i^{\prime},\end{cases}\]

and

\[\mathbb{E}\left[\left\langle w_{\star},x^{(i)}\right\rangle\;{x^{(i)}}[j]\; \left\langle w_{\star},x^{(i^{\prime})}\right\rangle\;{x^{(i^{\prime})}}[j] \right]=\begin{cases}\mathbb{E}[\left\langle x^{(i)}[j]\right\rangle^{2}\;{(x^{ (i^{\prime})}}[j]\right)^{2}]=1&\text{if }i\neq i^{\prime},\\ \mathbb{E}\left[\langle w_{\star},x^{(i)}\rangle^{2}\;{(x^{(i)}}[j]\right)^{2} \right]=d+2&\text{if }i=i^{\prime},\end{cases}\] (18)

where the last case follows from the fact that the fourth moment of Gaussian is \(3\) and

\[\mathbb{E}\left[\left\langle w_{\star},x^{(i)}\right\rangle^{2}\;{(x^{(i)}}[j ]\right)^{2}\right]=\mathbb{E}\left[\|x^{(i)}\|^{2}\;{(x^{(i)}[j])}^{2}\right]=3+ d-1=d+2.\]

Combining the above calculations together, we arrive at

\[\mathbb{E}\left[\left\langle\mathsf{G},E_{d+1,j}\right\rangle \mathsf{G}\right] =\frac{1}{n^{2}}\cdot\left(n(n-1)+(d+2)n\right)\left(E_{d+1,j}+E_{j,d+1}\right)\] \[=\left(\frac{n-1}{n}+(d+2)\frac{1}{n}\right)\left(E_{d+1,j}+E_{j,d +1}\right).\] (19)

Therefore, combining (17) and (19), the results follows.

\[X_{j}=-\frac{1}{\left(\frac{n-1}{n}+(d+2)\frac{1}{n}\right)}E_{d+1,j}\,,\]

is the unique global minimum of \(f_{j}\). Hence, \(b\) and \(A=[a_{1}\;a_{1}\;\cdots\;a_{d}]\) achieve the global minimum of \(f(b,A)=\sum_{j=1}^{d}f_{j}(ba_{j}^{\top})\) if they satisfy

\[ba_{j}^{\top}=-\frac{1}{\left(\frac{n-1}{n}+(d+2)\frac{1}{n}\right)}E_{d+1,j} \quad\text{for all }i=1,2,\ldots,d.\]

This can be achieve by the following choice:

\[b^{\top}=\mathbf{e}_{d+1},\quad a_{j}=-\frac{1}{\left(\frac{n-1}{n}+(d+2) \frac{1}{n}\right)}\mathbf{e}_{j}\quad\text{for }i=1,2,\ldots,d\,,\]

where \(\mathbf{e}_{j}\) is the \(j\)-th coordinate vector. This choice precisely corresponds to

\[b=\mathbf{e}_{d+1},\quad A=-\frac{1}{\left(\frac{n-1}{n}+(d+2)\frac{1}{n} \right)}\begin{bmatrix}I_{d}\\ 0\end{bmatrix}\,.\]

We next move on to the non-isotropic case.

### Proof for the non-isotropic case

_1. Diagonal covariance case._ We first consider the case where \(x^{(i)}\) is sampled from \(\mathcal{N}(0,\Lambda)\) where \(\Lambda=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{d})\) and \(w_{\star}\) is sampled from \(\mathcal{N}(0,I_{d})\). We prove the following generalization of Lemma 2.

**Lemma 3**.: _Suppose that \(x^{(i)}\) is sampled from \(\mathcal{N}(0,\Lambda)\) where \(\Lambda=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{d})\) and \(w_{\star}\) is sampled from \(\mathcal{N}(0,I_{d})\). Consider the following objective_

\[f_{j}(X)=\mathbb{E}_{Z_{0},w_{\star}}\left[\left\langle\mathsf{G},X\right\rangle +w_{\star}[j]\right]^{2}\,.\]

_Then a global minimum is given as_

\[X_{j}=-\frac{1}{\frac{n+1}{n}\lambda_{j}+\frac{1}{n}\cdot(\sum_{k}\lambda_{k} )}E_{d+1,j}\,,\]

_where \(E_{i_{1},i_{2}}\) is the matrix whose \((i_{1},i_{2})\)-th entry is \(1\), and the other entries are zero._

Proof of Lemma 3.: Similarly to the proof of Lemma 2, it suffices to check that

\[2\,\mathbb{E}\left[\left\langle\mathsf{G},X_{0}\right\rangle\mathsf{G}\right] +2\,\mathbb{E}\left[w_{\star}[j]\;\mathsf{G}\right]=0\,,\]

where we recall that \(\mathsf{G}\) is defined as

\[\mathsf{G}=\frac{1}{n}\sum_{i}\begin{bmatrix}{x^{(i)}}{x^{(i)}}^{\top}&{y^{(i )}}{x^{(i)}}^{\top}\\ {y^{(i)}}^{\top}&{y^{(i)}}^{2}\end{bmatrix}\,.\]

A similar calculation as the proof of Lemma 2 yields

\[\mathbb{E}\left[w_{\star}[j]\;\mathsf{G}\right]=\lambda_{j}(E_{d+1,j}+E_{j,d+1 }).\] (20)

Here the factor of \(\lambda_{j}\) comes from the following generalization of (16):

\[\mathbb{E}[w_{\star}[j]\;{y^{(i)}}\;{x^{(i)}}[k]]=\mathbb{E}[w_{\star}[j]\; \langle w_{\star},{x^{(i)}}\rangle\;{x^{(i)}}[k]]=\mathbb{E}\left[w_{\star}[j] ^{2}\;{x^{(i)}}[j]\;{x^{(i)}}[k]\right]=\lambda_{j}\mathbbm{1}_{[j=k]}\,.\]

Next, we compute \(\mathbb{E}\left[\left\langle\mathsf{G},E_{d+1,j}\right\rangle\mathsf{G}\right]\). Again, we follow a similar calculation to the proof of Lemma 2 except that this time we use the following generalization of (18):

\[\mathbb{E}\left[\left\langle w_{\star},{x^{(i)}}\right\rangle\,{x^{(i)}}[j]\; \langle w_{\star},{x^{(i^{\prime})}}\rangle\;{x^{(i^{\prime})}}[j]\right]= \begin{cases}\mathbb{E}[{x^{(i)}}[j]^{2}\;{x^{(i^{\prime})}}[j]^{2}]=\lambda_ {j}^{2}&\text{if }i\neq i^{\prime},\\ \mathbb{E}\left[\left\langle w_{\star},{x^{(i)}}\right\rangle^{2}\,{x^{(i)}}[ j]^{2}\right]=\lambda_{j}\sum_{k}\lambda_{k}+2\lambda_{j}^{2}&\text{if }i=i^{\prime},\end{cases}\]where the last line follows since

\[\mathbb{E}\left[\langle w_{\star},x^{(i)}\rangle^{2}\;x^{(i)}[j]^{2}\right]= \mathbb{E}\left[\|x^{(i)}\|^{2}\;x^{(i)}[j]^{2}\right]=\mathbb{E}\left[x^{(i)}[j ]^{2}\;\sum_{k}x^{(i)}[k]^{2}\right]=\lambda_{j}\sum_{k}\lambda_{k}+2\lambda_{j }^{2}\,.\]

Therefore, we have

\[\mathbb{E}\left[\langle\mathsf{G},E_{d+1,j}\rangle\,\mathsf{G}\right] =\frac{1}{n^{2}}\cdot\left(n(n-1)\lambda_{j}^{2}+n\lambda_{j}\sum _{k}\lambda_{k}+2n\lambda_{j}^{2}\right)(E_{d+1,j}+E_{j,d+1})\] \[=\left(\frac{n+1}{n}\lambda_{j}^{2}+\frac{1}{n}(\lambda_{j}\sum _{k}\lambda_{k})\right)(E_{d+1,j}+E_{j,d+1})\,.\] (21)

Therefore, combining (20) and (21), the results follows. 

Now we finish the proof. From Lemma 2, it follows that

\[X_{j}=-\frac{1}{\frac{n+1}{n}\lambda_{j}+\frac{1}{n}\cdot(\sum_{k}\lambda_{k} )}E_{d+1,j}\]

is the unique global minimum of \(f_{j}\). Hence, \(b\) and \(A=[a_{1}\;a_{1}\;\cdots\;a_{d}]\) achieve the global minimum of \(f(b,A)=\sum_{j=1}^{d}f_{j}(b,A_{j})\) if they satisfy

\[ba_{j}^{\top}=X_{j}=-\frac{1}{\frac{n+1}{n}\lambda_{j}+\frac{1}{n}\cdot(\sum_{ k}\lambda_{k})}E_{d+1,j}\quad\text{for all $i=1,2,\ldots,d$}.\]

This can be achieve by the following choice:

\[b^{\top}=\mathbf{e}_{d+1},\quad a_{j}=-\frac{1}{\frac{n+1}{n}\lambda_{j}+\frac {1}{n}\cdot(\sum_{k}\lambda_{k})}\mathbf{e}_{j}\quad\text{for $i=1,2,\ldots,d$}\,,\]

where \(\mathbf{e}_{j}\) is the \(j\)-th coordinate vector. This choice precisely corresponds to

\[b=\mathbf{e}_{d+1},\quad A=-\left[\operatorname{diag}\left(\left\{\frac{1}{ \frac{n+1}{n}\lambda_{j}+\frac{1}{n}\cdot(\sum_{k}\lambda_{k})}\right\}_{j} \right)\right].\]

_2. Non-diagonal covariance case (the setting of Theorem 1)._ We finally prove the general result of Theorem 1, namely \(x^{(i)}\) is sampled from a Gaussian with covariance \(\Sigma=U\Lambda U^{\top}\) where \(\Lambda=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{d})\) and \(w_{\star}\) is sampled from \(\mathcal{N}(0,I_{d})\). The proof works by reducing this case to the previous case. For each \(i\), define \(\widetilde{x}^{(i)}:=U^{T}x^{(i)}\). Then \(\mathbb{E}[\widetilde{x}^{(i)}(\widetilde{x}^{(i)})^{\top}]=\mathbb{E}[U^{ \top}(U\Lambda U^{\top})U]=\Lambda\). Now let us write the loss function (15) with this new coordinate system: since \(x^{(i)}=U\widetilde{x}^{(i)}\), we have

\[f(b,A)=\mathbb{E}_{Z_{0},w_{\star}}\left[(b^{\top}\mathsf{G}A+w_{\star}^{\top })U\widetilde{x}^{(n+1)}\right]^{2}=\sum_{j=1}^{d}\lambda_{j}\,\mathbb{E}_{Z_ {0},w_{\star}}\left[\left((b^{\top}\mathsf{G}A+w_{\star}^{\top})U\right)[j] \right]^{2}\,.\]

Hence, let us consider the vector \((b^{\top}\mathsf{G}A+w_{\star}^{\top})U\). By definition of \(\mathsf{G}\), we have

\[(b^{\top}\mathsf{G}A+w_{\star}^{\top})U =\frac{1}{n}\sum_{i}b^{\top}\begin{bmatrix}x^{(i)}\\ \left\langle x^{(i)},w_{\star}\right\rangle\end{bmatrix}^{\otimes 2}AU+w_{\star}^{ \top}U\] \[=\frac{1}{n}\sum_{i}b^{\top}\begin{bmatrix}U&0\\ 0&1\end{bmatrix}\begin{bmatrix}\tilde{x}_{i}\\ \left\langle Ux^{(i)},w_{\star}\right\rangle\end{bmatrix}^{\otimes 2}\begin{bmatrix}U^{ \top}&0\\ 0&1\end{bmatrix}AU+w_{\star}^{\top}U\] \[=\frac{1}{n}\sum_{i}\widetilde{b}^{\top}\begin{bmatrix}\tilde{x}_ {i}\\ \left\langle x^{(i)},\widetilde{w}_{\star}\right\rangle\end{bmatrix}^{\otimes 2} \widetilde{A}+\widetilde{w}_{\star}^{\top}\]where we define \(\widetilde{b}^{\top}\coloneqq b^{\top}\begin{bmatrix}U&0\\ 0&1\end{bmatrix}\), \(\widetilde{A}\coloneqq\begin{bmatrix}U^{\top}&0\\ 0&1\end{bmatrix}AU\), and \(\widetilde{w}_{\star}\coloneqq U^{\top}w_{\star}\). By the rotational symmetry, \(\widetilde{w}_{\star}\) is also distributed as \(\mathcal{N}(0,I_{d})\). Hence, this reduces to the previous case, and a global minimum is given as

\[\widetilde{b}=\mathbf{e}_{d+1},\quad\widetilde{A}=-\left[\operatorname{diag} \left(\left\{\frac{1}{\frac{n+1}{n}\lambda_{i}+\frac{1}{n}\cdot\left(\sum_{k} \lambda_{k}\right)}\right\}_{j}\right)\right].\]

From the definition of \(\widetilde{b}\), \(\widetilde{A}\), it thus follows that a global minimum is given by

\[b^{\top}=\mathbf{e}_{d+1},\quad A=-\left[U\operatorname{diag}\left(\left\{ \frac{1}{\frac{n+1}{n}\lambda_{i}+\frac{1}{n}\cdot\left(\sum_{k}\lambda_{k} \right)}\right\}_{i}\right)U^{\top}\right]\,,\]

as desired.

### Proof for non-linear attentions (Theorem 5)

As mentioned in Theorem 5, we focus on the setting where the last row of \(Q\) is zero, i.e., let

\[Q=\begin{bmatrix}A&a\\ 0^{\top}&0\end{bmatrix}\quad\text{for }A\in\mathbb{R}^{d\times d}\text{ and }a\in\mathbb{R}^{d}.\]

We first rewrite the loss function and simplify it following Subsection A.1. Moreover, for simple notation we will often write \(z,x\) instead of \(z^{(n+1)},x^{(n+1)}\).

_1. Rewriting loss function._

Following Subsection A.1, let us write down the in-context loss (5). for the single-layer nonlinear attention denoted by \(f(P,Q)\):

\[f\left(P,Q\right)=\mathbb{E}_{Z_{0},w_{\star}}\left(\left[Z_{0}+\frac{1}{n} \mathrm{Attn}_{P,Q}^{\sigma}(Z_{0})\right]_{d+1,n+1}+w_{\star}^{\top}x\right) ^{2}\]

Recalling the definition of the ReLU attention \(\mathrm{Attn}_{P,Q}^{\sigma}(Z)\coloneqq PZM\)\(\sigma(Z^{\top}QZ)\), the data matrix \(Z\coloneqq[z^{(1)}\;\cdots\;z^{(n+1)}]\), and the mask matrix \(M\), the term \(\mathrm{Attn}_{P,Q}^{\sigma}(Z_{0})\) can be written as:

\[\mathrm{Attn}_{P,Q}^{\sigma}(Z_{0})=\underset{\mathbb{R}^{(d+1)\times(n+1)}} {\underbrace{PZ_{0}}}\cdot\begin{bmatrix}I_{n\times n}&0\\ 0&0\end{bmatrix}\cdot\frac{\sigma\left(Z_{0}^{\top}QZ_{0}\right)}{\mathbb{R}^ {(n+1)\times(n+1)}}\cdot\]

Hence, it follows that the \((d+1,n+1)\)-th entry of \(\mathrm{Attn}_{P,Q}^{\sigma}(Z_{0})\) is equal to the product of the \((d+1)\)-th row of \(PZ_{0}\), the mask matrix \(M\), and the \((n+1)\)-th column of \(\sigma\left(Z_{0}^{\top}QZ_{0}\right)\). Hence, let us write them down explicitly:

* Letting \(b^{\top}\) be the last row of the matrix \(P\), it holds that the \((d+1)\)-th row of \(PZ_{0}\) is equal to \([\langle b,z^{(i)}\rangle]_{i=1,\ldots,n+1}\).
* The \((n+1)\)-th column of \(\sigma\left(Z_{0}^{\top}QZ_{0}\right)\) is equal to \(\left[\sigma\left((z^{(i)})^{\top}Qz^{(n+1)}\right)\right]_{i=1,\ldots,n+1}\). Letting \(A\) the first \(d\) columns of \(Q\), this vector is equal to \(\left[\sigma\left((x^{(i)})^{\top}Ax^{(n+1)}\right)\right]_{i=1,\ldots,n+1}\) because the last row of \(Q\) is zero and the last row of \(z^{(n+1)}\) is zero (since \((z^{(n+1)})^{\top}=[(x^{(n+1)})^{\top}\;0]\)).

Thus, the product of \([\langle b,z^{(i)}\rangle]_{i=1,\ldots,n+1}\), the mask matrix \(M\), and \(\left[\sigma\left((x^{(i)})^{\top}Ax^{(n+1)}\right)\right]_{i=1,\ldots,n+1}\) results in the following expression of the attention (writing \(z,x\) instead of \(z^{(n+1)},x^{(n+1)}\)):

\[\left[\mathrm{Attn}_{P,Q}^{\sigma}(Z_{0})\right]_{d+1,n+1}=\sum_{i=1}^{n} \left[\langle b,z^{(i)}\rangle\cdot\sigma((x^{(i)})^{\top}Ax)\right]\,.\]Since \([Z_{0}]_{d+1,n+1}=0\), we therefore have

\[\left[Z_{0}+\frac{1}{n}\mathrm{Attn}_{P,Q}^{\sigma}(Z_{0})\right]_{d+1,n+1}=\frac {1}{n}\sum_{i=1}^{n}\left[\langle b,z^{(i)}\rangle\cdot\sigma((x^{(i)})^{\top} Ax)\right]\,.\]

Therefore, it follows that the in-context loss \(f(P,Q)\) only depends on \(b\) and \(A\). Henceforth, let us write \(f(b,A)\) instead of \(f(P,Q)\) following Subsection A.1. In particular, writing \(b^{\top}=[b_{0}^{\top},b_{1}]\) for \(b_{0}\in\mathbb{R}^{d}\) and \(b_{1}\in\mathbb{R}\), the loss function can be expressed as

\[f(b,A)\coloneqq\mathbb{E}\left(\frac{1}{n}\sum_{i=1}^{n}\left[\langle\langle b _{0},x^{(i)}\rangle+b_{1}y^{(i)}\rangle\cdot\sigma((x^{(i)})^{\top}Ax)\right] +\langle w_{\star},x\rangle\right)^{2}\,.\] (22)

**2**.: _Simplifying the loss function with symmetry._

Now, we use the fact that both \(x^{(i)}\)'s and \(w_{\star}\) are sampled from the isotropic Gaussian, i.e., \(\mathcal{N}(0,I_{d})\) in order to further simplify the loss function in (22). In particular, we use the following facts:

1. For orthonormal matrices \(U,V\in\mathbb{R}^{d\times d}\), it holds that \(Ux^{(i)},\,Vx\) and \(Uw_{\star}\) have the same distributions as \(\mathcal{N}(0,I_{d})\).
2. Moreover, for a diagonal matrix \(\Xi=\mathrm{diag}(\xi_{i})\in\mathbb{R}^{d\times d}\) with the diagonal entries being random signs \(\xi_{i}\sim\{\pm 1\}\), it holds that \(\Xi x^{(i)}\), \(\Xi x\) and \(\Xi w_{\star}\) have the same distributions as \(\mathcal{N}(0,I_{d})\).

Now let us fix a matrix \(A\in\mathbb{R}^{d\times d}\) and \(b^{\top}=[b_{0}^{\top},b_{1}]\) for \(b_{0}\in\mathbb{R}^{d}\) and \(b_{1}\in\mathbb{R}\). Letting \(A=U\Sigma V^{\top}\) be the SVD of the matrix \(A\), it follows that

\[f(b,A) =\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left[(b_{0}^{\top}x^{( i)}+b_{1}w_{\star}^{\top}x^{(i)})\cdot\sigma((x^{(i)})^{\top}Ax)\right]+w_{ \star}^{\top}x\right]^{2}\] \[\overset{(a)}{=}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left[( b_{0}^{\top}Ux^{(i)}+b_{1}w_{\star}^{\top}U^{\top}Ux^{(i)})\cdot\sigma((x^{(i)})^{ \top}U^{\top}AVx)\right]+w_{\star}^{\top}U^{\top}Vx\right]^{2}\] \[\overset{(b)}{=}\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left[( b_{0}^{\top}U\Xi x^{(i)}+b_{1}w_{\star}^{\top}x^{(i)})\cdot\sigma((x^{(i)})^{\top} \Sigma x)\right]+w_{\star}^{\top}\Xi U^{\top}V\Xi x\right]^{2}\] \[\geq\mathbb{E}\left[\mathbb{E}_{\Xi}\left\{\frac{1}{n}\sum_{i=1} ^{n}\left[(b_{0}^{\top}U\Xi x^{(i)}+b_{1}w_{\star}^{\top}x^{(i)})\cdot\sigma(( x^{(i)})^{\top}\Sigma x)\right]+w_{\star}^{\top}\Xi U^{\top}V\Xi x\right\} \right]^{2}\] \[=\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left[b_{1}w_{\star}^{ \top}x^{(i)}\cdot\sigma((x^{(i)})^{\top}\Sigma x)\right]+w_{\star}^{\top} \mathrm{diag}(U^{\top}V)x\right]^{2}\] \[=:f_{\text{lower}}(b_{1},\Sigma,D\coloneqq\mathrm{diag}(U^{\top}V ))\,.\]

where in the third line we use the fact that \(\Xi^{\top}\Sigma\Xi=\Xi\Sigma\Xi=\Sigma\); and the fourth line follows from the Jensen's inequality. Hence for the remainder of the proof, we will characterize the global minimizer of the lower bound, i.e., \(f_{\text{lower}}\) and then we will connect it back to the original objective.

**3**.: _Computation of the lower bound \(f_{\text{lower}}\)._

Let us now explicitly compute \(f_{\text{lower}}\). Let us rewrite the definition of \(f_{\text{lower}}\). In fact since, \(\sigma=\mathrm{ReLU}\) is homogenous, one can further simplify the lower bound by pushing the constant \(b_{1}\) inside and write \(b_{1}\Sigma\) as \(\Sigma\). Hence, for two diagonal matrices \(\Sigma,D\in\mathbb{R}^{d\times d}\), \(f_{\text{lower}}\) is defined as:

\[f_{\text{lower}}(\Sigma,D)\coloneqq\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n} \left[\langle w_{\star},x^{(i)}\rangle\cdot\sigma(x^{\top}\Sigma x^{(i)}) \right]+w_{\star}^{\top}Dx\right]^{2}\,.\]

In particular, \(D\) is constrained to be the diagonal part of an orthogonal matrix (since \(D=\mathrm{diag}(U^{\top}V)\) in the above derivation). Now we focus on characterizing the global minimizers of \(f_{\text{lower}}\).

The main part of the argument is inspired by the elegant observation of Erdogdu et al. (2016), which says that the solution of least squares and generalized linear models are collinear for Gaussian inputs. We leverage the same proof technique (_a la_ Stein's Lemma) to prove that the presence of ReLU only changes the scaling of global optimum.

First, since \(w_{\star}\) is isotropic Gaussian, we can take the expectation over \(w_{\star}\) to obtain

\[f_{\mathsf{lower}}(\Sigma,D)=\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left[ \sigma(x^{\top}\Sigma x^{(i)})\,x^{(i)}\right]+Dx\right]^{2}\,,\]

which after a careful expansion becomes

\[\mathbb{E}\left[\frac{1}{n^{2}}\sum_{i,j}\sigma(x^{\top}\Sigma x^{(i)})\sigma (x^{\top}\Sigma x^{(j)})\langle x^{(i)},x^{(j)}\rangle+\frac{2}{n}\sum_{i} \sigma(x^{\top}\Sigma x^{(i)})\langle x^{(i)},Dx\rangle\right]+\text{const.}\] (23)

In order to compute (23), we will rely on the aforementioned argument of Erdogdu et al. (2016). In particular, from integration by parts, or Stein's lemma (Erdogdu et al., 2016) (since \(x\sim\mathcal{N}(0,I_{d})\)), we have

\[\mathbb{E}_{x}[\sigma(x^{\top}v)x]=\mathbb{E}_{x}[\sigma^{\prime}(x^{\top}v)] v\quad\text{for a fixed }v\in\mathbb{R}^{d}.\]

We use this to compute all the terms in (23) as follows:

* We first apply Stein's lemma to the first term of (23) for \(i\neq j\). This results in \[\mathbb{E}_{x^{(i)},x^{(j)},x}\,\sigma(x^{\top}\Sigma x^{(i)}) \sigma(x^{\top}\Sigma x^{(j)})\langle x^{(i)},x^{(j)}\rangle\] \[=\mathbb{E}_{x^{(j)},x}[\mathbb{E}_{x^{(i)}}[\sigma^{\prime}(x^{ \top}\Sigma x^{(i)})]\,\sigma(x^{\top}\Sigma x^{(j)})\,x^{\top}\Sigma x^{(j)}]\] Using the fact that \(x^{(i)}\) is a symmetric random variable, one can compute the expectation above as follows: one the one hand, we know \(\mathbb{E}_{x^{(i)}}[\sigma^{\prime}(x^{\top}\Sigma x^{(i)})]=\mathbb{E}_{x^{ (i)}}[\sigma^{\prime}(-x^{\top}\Sigma x^{(i)})]\). On the other hand, we also know that for any scalar \(\alpha\), \(\sigma^{\prime}(-\alpha)+\sigma^{\prime}(\alpha)=1\). Therefore, we conclude that \(\mathbb{E}_{x^{(i)}}[\sigma^{\prime}(x^{\top}\Sigma x^{(i)})]=1/2\). Thus, applying this technique twice, we obtain the following \[\mathbb{E}_{x^{(i)},x^{(j)},x}\,\sigma(x^{\top}\Sigma x^{(i)})\sigma(x^{\top }\Sigma x^{(j)})\langle x^{(i)},x^{(j)}\rangle=\frac{1}{4}\mathbb{E}_{x}[x^{ \top}\Sigma^{2}x]=\frac{1}{4}\operatorname{Tr}(\Sigma^{2})\,.\]
* Similarly, we can use Stein's lemma to the second term of (23) to conclude \[\mathbb{E}\,\sigma(x^{\top}\Sigma x^{(i)})\langle x^{(i)},Dx\rangle=\frac{1}{ 2}\,\mathbb{E}_{x}\,x^{\top}\Sigma Dx=\frac{1}{2}\operatorname{Tr}(\Sigma D)\,.\]
* Lastly, the computation of the first term of (23) for \(i=j\) is straightforward. Using the fact that \(\forall\alpha\in\mathbb{R}\), \(\sigma^{2}(\alpha)+\sigma^{2}(-\alpha)=a^{2}\), we get \[\mathbb{E}\left[\sigma^{2}(x^{\top}\Sigma x^{(i)})\,\left\|x^{(i)} \right\|^{2}\right]=\frac{1}{2}\mathbb{E}[(x^{\top}\Sigma x^{(i)})^{2}\,\left\| x^{(i)}\right\|^{2}]\] \[=\frac{1}{2}\,\mathbb{E}\left[(x^{(i)})^{\top}\Sigma^{2}x^{(i)} \,\left\|x^{(i)}\right\|^{2}\right]=\frac{d+2}{2}\operatorname{Tr}(\Sigma^{2})\,.\] Putting things all together (and ignoring the constant part in (23)), we have \[f_{\mathsf{lower}}(\Sigma,D)=\frac{2(d+2)+(n-1)}{4n}\operatorname{Tr}(\Sigma^ {2})+\operatorname{Tr}(\Sigma D)\,.\] (24)

_4. Connecting back to the original loss function._

One can in fact write (24) solely in terms of \(b_{1}\) and \(A\) as follows:

\[\frac{2(d+2)+(n-1)}{4n}\operatorname{Tr}(\Sigma^{2})+\operatorname{Tr}(\Sigma D )=\frac{2(d+2)+(n-1)}{4n}\left\|b_{1}A\right\|_{F}^{2}+\operatorname{Tr}(b_{1} A)\,.\]

Since the latter is a convex function in the matrix \(b_{1}A\), it follows that the minimizer corresponds to

\[b_{1}A=-\frac{2n}{2(d+2)+(n-1)}\cdot I_{d}\]

In fact the choice \(b_{1}=1\), \(b_{0}=0\), and \(A=-\frac{2n}{2(d+2)+(n-1)}\cdot I_{d}\) achieves this, and more crucially, satisfies the property that \(f_{\mathsf{lower}}=f\) for the corresponding parameters. Therefore, this shows that such choice is a global minimizer.

Proofs for the multi-layer case

### Proof of Theorem 2

The proof is based on probabilistic methods (Alon and Spencer, 2016). According to Lemma 5, the objective function can be written as (for more details check the derivations in (25))

\[f(A_{1},A_{2}) =\mathbb{E}\operatorname{Tr}\left(\mathbb{E}\left[\prod_{i=1}^{2}( I-X_{0}^{\top}A_{i}X_{0}M)X_{0}^{\top}w_{\star}w_{\star}^{\top}X_{0}\prod_{i=1}^{2}( I-MX_{0}^{T}A_{i}X_{0})\right]\right)\] \[=\mathbb{E}\operatorname{Tr}\left(\mathbb{E}\left[\prod_{i=2}^{1 }(I-X_{0}^{\top}A_{i}X_{0}M)X_{0}^{\top}X_{0}\prod_{j=1}^{2}(I-MX_{0}^{T}A_{j} X_{0})\right]\right),\]

where we use the isotropy of \(w_{\star}\) and the linearity of trace to get the last equation. Suppose that \(A_{0}^{\ast}\) and \(A_{1}^{\ast}\) denote the global minimizer of \(f\) over symmetric matrices. Since \(A_{1}^{\ast}\) is a symmetric matrix, it admits the spectral decomposition \(A_{1}=UD_{1}U^{\top}\) where \(D_{1}\) is a diagonal matrix and \(U\) is an orthogonal matrix. Remarkably, the distribution of \(X_{0}\) is invariant to a linear transformation by an orthogonal matrix, i.e, \(X_{0}\) has the same distribution as \(X_{0}U^{\top}\). This invariance yields

\[f(UD_{1}U^{\top},A_{2}^{\ast})=f(D_{1},U^{\top}A_{2}^{\ast}U).\]

Thus, we can assume \(A_{1}^{\ast}\) is diagonal without loss of generality. To prove \(A_{2}^{\ast}\) is also diagonal, we leverage a probabilistic proof technique. Consider the random diagonal matrix \(S\) whose diagonal elements are either \(1\) or \(-1\) with probability \(\frac{1}{2}\). Since the input distribution is invariant to orthogonal transformations, we have

\[f(D_{1},A_{2}^{\ast})=f(SD_{1}S,SA_{2}^{\ast}S)=f(D_{1},SA_{2}^{\ast}S).\]

Note that we use \(SD_{1}S=D_{1}\) in the last equation, which holds due to \(D_{1}\) and \(S\) are diagonal matrices and \(S\) has diagonal elements in \(\{+1,-1\}\). Since \(f\) is convex in \(A_{2}\), a straightforward application of Jensen's inequality yields

\[f(D_{1},A_{2}^{\ast})=\mathbb{E}\left[f(D_{1},SA_{2}^{\ast}S)\right]\geq f(D_ {1},\mathbb{E}\left[SA_{2}^{\ast}S\right])=f(D_{1},\operatorname{diag}(A_{2} ^{\ast})).\]

Thus, there are diagonal \(D_{1}\) and \(\operatorname{diag}(A_{2}^{\ast})\) for which \(f(D_{1},\operatorname{diag}(A_{2}^{\ast}))\leq f(A_{1}^{\ast},A_{2}^{\ast})\) holds for an optimal \(A_{1}^{\ast}\) and \(A_{2}^{\ast}\). This concludes the proof.

### Proof of Theorem 3

Let us drop the factor of \(\nicefrac{{1}}{{n}}\) which was present in the original update (56). This is because the constant \(1/n\) can be absorbed into \(A_{i}\)'s. Doing so does not change the theorem statement, but reduces notational clutter.

Let us consider the reformulation of the in-context loss \(f\) presented in Lemma 5. Specifically, let \(\overline{Z}_{0}\) be defined as

\[\overline{Z}_{0}=\begin{bmatrix}x^{(1)}&x^{(2)}&\cdots&x^{(n)}&x^{(n+1)}\\ y^{(1)}&y^{(2)}&\cdots&y^{(n)}&y^{(n+1)}\end{bmatrix}\in\mathbb{R}^{(d+1) \times(n+1)},\]

where \(y^{(n+1)}=\left\langle w_{\star},x^{(n+1)}\right\rangle\). Let \(\overline{Z}_{i}\) denote the output of the \((i-1)^{th}\) layer of the linear transformer (as defined in (56), initialized at \(\overline{Z}_{0}\)). For the rest of this proof, we will drop the bar, and simply denote \(\overline{Z}_{i}\) by \(Z_{i}\).5 Let \(X_{i}\in\mathbb{R}^{d\times(n+1)}\) denote the first \(d\) rows of \(Z_{i}\) and let \(Y_{i}\in\mathbb{R}^{1\times(n+1)}\) denote the \((d+1)^{th}\) row of \(Z_{k}\). Under the sparsity pattern enforced in (8), we verify that, for any \(i\in\{0,\ldots,k\}\),

Footnote 5: This use of \(Z_{i}\) differs the original definition in (1). But we will not refer to the original definition anywhere in this proof.

\[X_{i}=X_{0},\] \[Y_{i+1}=Y_{i}+Y_{i}MX_{i}^{\top}A_{i}X_{i}=Y_{0}\prod_{\ell=0}^{i} \left(I+MX_{0}^{\top}A_{\ell}X_{0}\right).\] (25)where \(M=\begin{bmatrix}I_{n\times n}&0\\ 0&0\end{bmatrix}\). We adopt the shorthand \(A=\left\{A_{i}\right\}_{i=0}^{k}\).

We adopt the shorthand \(A=\left\{A_{i}\right\}_{i=0}^{k}\). Let \(\mathcal{S}\subset\mathbb{R}^{(k+1)\times d\times d}\), and \(A\in\mathcal{S}\) if and only if for all \(i\in\{0,\ldots,k\}\), there exists scalars \(a_{i}\in\mathbb{R}\) such that \(A_{i}=a_{i}\Sigma^{-1}\) and \(B_{i}=b_{i}I\). We use \(f(A)\) to refer to the in-context loss of Theorem 3, that is,

\[f(A):=f\left(\left\{Q_{i}=\begin{bmatrix}A_{i}&0\\ 0&0\end{bmatrix},P_{i}=\begin{bmatrix}0_{d\times d}&0\\ 0&1\end{bmatrix}\right\}_{i=0}^{k}\right).\]

Throughout this proof, we will work with the following formulation of the _in-context loss_ from Lemma 5:

\[f(A)= \operatorname{\mathbb{E}}_{(X_{0},w_{*})}\left[\operatorname{Tr} \left((I-M)\,Y_{k+1}^{\top}Y_{k+1}\,(I-M)\right)\right].\] (26)

The theorem statement is equivalent to the following:

\[\inf_{A\in\mathcal{S}}\sum_{i=0}^{k}\left\|\nabla_{A_{i}}f(A)\right\|_{F}^{2} =0,\] (27)

where \(\nabla_{A_{i}}f\) denotes derivative wrt the Frobenius norm \(\left\|A_{i}\right\|_{F}\). Towards this end, we establish the following intermediate result: if \(A\in\mathcal{S}\), then for any \(R\in\mathbb{R}^{(k+1)\times d\times d}\), there exists \(\tilde{R}\in\mathcal{S}\), such that, at \(t=0\),

\[\frac{d}{dt}f(A+t\tilde{R})\leq\frac{d}{dt}f(A+tR).\] (28)

In fact, we show that \(\tilde{R}_{i}:=r_{i}I\), for \(r_{i}=\frac{1}{d}\operatorname{Tr}\left(\Sigma^{1/2}R_{i}\Sigma^{1/2}\right)\). This implies (27) via the following simple argument: Consider the "\(\mathcal{S}\)-constrained gradient flow": let \(A(t):\mathbb{R}^{+}\to\mathbb{R}^{(k+1)\times d\times d}\) be defined as

\[\frac{d}{dt}A_{i}(t)=-r_{i}(t)\Sigma^{-1},\quad r_{i}(t):=\operatorname{Tr}( \Sigma^{1/2}\nabla_{A_{i}}f(A(t))\Sigma^{1/2})\]

for \(i=0,\ldots,k\). By (28), we verify that

\[\frac{d}{dt}f(A(t))\leq-\sum_{i=0}^{k}\left\|\nabla_{A_{i}}f(A(t))\right\|_{F} ^{2}.\] (29)

We verify from its definition that \(f(A)\geq 0\); if the infimum in (27) fails to be zero, then inequality (29) will ensure unbounded descent as \(t\to\infty\), contradicting the fact that \(f(A)\) is lower-bounded. This concludes the proof.

_Proof outline._ The remainder of the proof will be devoted to showing (28), which we outline as follows:

* In Step 1, we reduce the condition in (29) to a more easily verified _layer-wise_ condition. Specifically, we only need to verify (29) when \(R_{i}\) are all zero except for \(R_{j}\) for some fixed \(j\) (see (30)) At the end of Step 1, we set up some additional notation, and introduce an important matrix \(G\), which is roughly "a product of attention layer matrices". In (31), we study the evolution of \(f(A(t))\) when \(A(t)\) moves in the direction of \(R\), as \(X_{0}\) is (roughly speaking) randomly transformed.
* In Step 2, we use the results of Step 2 to to study \(G\) (see (32)) and \(\frac{d}{dt}G(A(t))\) (see (33)) under random transformation of \(X_{0}\). The idea in (33) is that "randomly transforming \(X_{0}\)" has the same effect as "randomly transforming \(S\)" (recall \(S\) is the perturbation to \(B\)).
* In Step 3, we apply the result from Step 2 to the expression of \(\frac{d}{dt}f(A(t))\) in (31). We verify that \(\tilde{R}\) in (28) is exactly the expected matrix after "randomly transforming \(S\)". This concludes our proof.

_1. Reduction to layer-wise condition._ To prove (28), it suffices to show the following simpler condition: Let \(j\in\{0,\ldots,k\}\). Let \(R_{j}\in\mathbb{R}^{d\times d}\) be arbitrary matrices. For \(C\in\mathbb{R}^{d\times d}\), let \(A(tC,j)\) denote the collection of matrices, where \(\left[A(tC,j)\right]_{j}=A_{j}+tC\), and for \(i\neq j\), \(A(tC,j)_{i}=A_{i}\). We show that for all \(j\in\left\{0,\ldots,k\right\},R_{j}\in\mathbb{R}^{d\times d}\), there exists \(\tilde{R}_{j}=r_{j}\Sigma^{-1}\), such that, at \(t=0\),

\[\frac{d}{dt}f(A(t\tilde{R}_{j},j))\leq\frac{d}{dt}f(A(tR_{j},j))\] (30)

We can verify that (28) is equivalent to (30) by noticing that for any \(R\), at \(t=0\), \(\frac{d}{dt}f(A+tR)=\sum_{j=0}^{k}\frac{d}{dt}f(A(tR_{j},j))\). We will now work towards proving (30) for some index \(j\) that is arbitrarily chosen but fixed throughout.

Let us define, for any \(C\in\mathbb{R}^{d\times d}\), \(G(X,A_{j}+C):=X\prod_{i=0}^{k}\left(I-MX^{\top}\left[A(C,j)\right]_{i}X\right)\). By (25) and (26),

\[f(A(tR_{j},j))\] \[= \,\mathbb{E}\left[\operatorname{Tr}\left(\left(I-M\right)G(X_{0 },A_{j}+tR_{j})^{\top}w_{\star}^{\top}w_{\star}G(X_{0},A_{j}+tR_{j})\left(I-M \right)\right)\right]\] \[= \,\mathbb{E}\left[\operatorname{Tr}\left(\left(I-M\right)G(X_{0 },A_{j}+tR_{j})^{\top}\Sigma^{-1}G(X_{0},A_{j}+tR_{j})\left(I-M\right)\right)\right]\]

The second equality follows from plugging in (25). For the rest of this proof, let \(U\) denote a uniformly randomly sampled orthogonal matrix. Let \(U_{\Sigma}:=\Sigma^{1/2}U\Sigma^{-1/2}\). Using the fact that \(X_{0}\stackrel{{ d}}{{=}}U_{\Sigma}X_{0}\), we can verify

\[\left.\frac{d}{dt}f(A(tR_{j},j))\right|_{t=0}\] \[= \,\frac{d}{dt}\left.\mathbb{E}\left[\operatorname{Tr}\left( \left(I-M\right)G(X_{0},A_{j}+tR_{j})^{\top}\Sigma^{-1}G(X_{0},A_{j}+tR_{j}) \left(I-M\right)\right)\right]\right|_{t=0}\] \[= \,\frac{d}{dt}\left.\mathbb{E}_{X_{0},U}\left[\operatorname{Tr} \left(\left(I-M\right)G(U_{\Sigma}X_{0},A_{j}+tR_{j})^{\top}\Sigma^{-1}G(U_{ \Sigma}X_{0},A_{j}+tR_{j})\left(I-M\right)\right)\right]\right|_{t=0}\] \[= \,2\,\mathbb{E}_{X_{0},U}\left[\operatorname{Tr}\left(\left(I-M \right)G(U_{\Sigma}X_{0},A_{j})^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(U_{\Sigma }X_{0},A_{j}+tR_{j})\right|_{t=0}\left(I-M\right)\right)\right].\] (31)

_2. \(G\) and \(\frac{d}{dt}G\) under random transformation of \(X_{0}\)._ We will now verify that \(G(U_{\Sigma}X_{0},A_{j})=U_{\Sigma}G(X_{0},A_{j})\):

\[G(U_{\Sigma}X_{0},A_{j})\] \[= U_{\Sigma}X_{0}\prod_{i=0}^{k}\left(I+MX_{0}^{T}U_{\Sigma}^{ \top}A_{i}U_{\Sigma}X_{0}\right)\] \[= U_{\Sigma}G(X_{0},A_{j}),\] (32)

where we use the fact that \(U_{\Sigma}^{\top}A_{i}U_{\Sigma}=U_{\Sigma}^{\top}(a_{i}\Sigma^{-1})U_{\Sigma }=A_{i}\). Next, we verify that

\[\left.\frac{d}{dt}G(U_{\Sigma}X_{0},A+tR_{j})\right|_{t=0}\] \[= U_{\Sigma}X_{0}\left(\prod_{i=0}^{j-1}(I+MX_{0}^{T}A_{i}X_{0}) \right)MX_{0}^{T}U_{\Sigma}^{\top}R_{j}U_{\Sigma}X_{0}\prod_{i=j+1}^{k}(I+MX_{ 0}^{T}A_{i}X_{0})\] \[= U_{\Sigma}\frac{d}{dt}G(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{ \Sigma})\] (33)

where the first equality again uses the fact that \(U_{\Sigma}^{\top}A_{i}U_{\Sigma}=A_{i}\).

_3. Putting everything together._ Let us continue from (31). Plugging (32) and (33) into (31),\[= 2\,\mathbb{E}_{X_{0},U}\left[\mathrm{Tr}\left((I-M)\,G(U_{\Sigma}X_{0 },A_{j})^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(U_{\Sigma}X_{0},A_{j}+tR_{j}) \right|_{t=0}(I-M)\right)\right]\] \[\overset{(i)}{=} 2\,\mathbb{E}_{X_{0},U}\left[\mathrm{Tr}\left((I-M)\,G(X_{0},A_{j })^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{ \Sigma})\right|_{t=0}(I-M)\right)\right]\] \[= 2\,\mathbb{E}_{X_{0}}\left[\mathrm{Tr}\left((I-M)\,G(X_{0},A_{j })^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{ \Sigma})\right|_{t=0}(I-M)\right)\right]\] \[\overset{(ii)}{=} 2\,\mathbb{E}_{X_{0}}\left[\mathrm{Tr}\left((I-M)\,G(X_{0},A_{j })^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},A_{j}+t\,\mathbb{E}_{U}\left[U_ {\Sigma}^{\top}R_{j}U_{\Sigma}\right])\right|_{t=0}(I-M)\right)\right]\] \[= 2\,\mathbb{E}_{X_{0}}\left[\mathrm{Tr}\left((I-M)\,G(X_{0},A_{j })^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},A_{j}+t\cdot r_{j}\Sigma^{-1}) \right|_{t=0}(I-M)\right)\right]\] \[= \left.\frac{d}{dt}f(A(t\cdot r_{j}\Sigma^{-1},j))\right|_{t=0},\]

where \(r_{j}:=\frac{1}{d}\,\mathrm{Tr}\left(\Sigma^{1/2}R_{j}\Sigma^{1/2}\right)\). In the above, \((i)\) uses 1. (32) and (33), as well as the fact that \(U_{\Sigma}^{\top}\Sigma^{-1}U_{\Sigma}=\Sigma^{-1}\). \((ii)\) uses the fact that \(\left.\frac{d}{dt}G(X_{0},A_{j}+tC)\right|_{t=0}\) is affine in \(C\). To see this, one can verify from the definition of \(G\), e.g. using similar algebra as (33), that \(\frac{d}{dt}G(X_{0},A_{j}+C)\) is affine in \(C\). Thus \(\mathbb{E}_{U}\left[G(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{\Sigma})\right]=G( X_{0},A_{j}+t\,\mathbb{E}_{U}\left[U_{\Sigma}^{\top}R_{j}U_{\Sigma}\right]\).

### Proof of Theorem 4

The proof of Theorem 4 is similar to that of Theorem 3, and with a similar setup. However to keep the proof self-contained, we will restate the setup. Once again, we drop the factor of \(\frac{1}{n}\) which was present in the original update (56). This is because the constant \(1/n\) can be absorbed into \(A_{i}\)'s. Doing so does not change the theorem statement, but reduces notational clutter.

Let us consider the reformulation of the in-context loss \(f\) presented in Lemma 5. Specifically, let \(\overline{Z}_{0}\) be defined as

\[\overline{Z}_{0}=\begin{bmatrix}x^{(1)}&x^{(2)}&\cdots&x^{(n)}&x^{(n+1)}\\ y^{(1)}&y^{(2)}&\cdots&y^{(n)}&y^{(n+1)}\end{bmatrix}\in\mathbb{R}^{(d+1) \times(n+1)},\]

where \(y^{(n+1)}=\left\langle w_{*},x^{(n+1)}\right\rangle\). Let \(\overline{Z}_{i}\) denote the output of the \((i-1)^{th}\) layer of the linear transformer (as defined in (56), initialized at \(\overline{Z}_{0}\)). For the rest of this proof, we will drop the bar, and simply denote \(\overline{Z}_{i}\) by \(Z_{i}\).6 Let \(X_{i}\in\mathbb{R}^{d\times n+1}\) denote the first \(d\) rows of \(Z_{i}\) and let \(Y_{i}\in\mathbb{R}^{1\times n+1}\) denote the \((d+1)^{th}\) row of \(Z_{k}\). Under the sparsity pattern enforced in (11), we verify that, for any \(i\in\{0,\dots,k\}\),

Footnote 6: This use of \(Z_{i}\) differs the original definition in (1). But we will not refer to the original definition anywhere in this proof.

\[X_{i+1}=X_{i}+B_{i}X_{i}MX_{i}^{\top}A_{i}X_{i}\] \[Y_{i+1}=Y_{i}+Y_{i}MX_{i}^{\top}A_{i}X_{i}=Y_{0}\prod_{\ell=0}^ {i}\left(I+MX_{\ell}^{T}A_{\ell}X_{\ell}\right).\] (34)

We adopt the shorthand \(A=\left\{A_{i}\right\}_{i=0}^{k}\) and \(B=\left\{B_{i}\right\}_{i=0}^{k}\). Let \(\mathcal{S}\subset\mathbb{R}^{2\times(k+1)\times d\times d}\), and \((A,B)\in\mathcal{S}\) if and only if for all \(i\in\{0,\dots,k\}\), there exists scalars \(a_{i},b_{i}\in\mathbb{R}\) such that \(A_{i}=a_{i}\Sigma^{-1}\) and \(B_{i}=b_{i}I\). Throughout this proof, we will work with the following formulation of the _in-context loss_ from Lemma 5:

\[f(A,B):=\mathbb{E}_{(X_{0},w_{*})}\left[\mathrm{Tr}\left((I-M)\,Y_{k+1}^{\top}Y_ {k+1}\left(I-M\right)\right)\right].\] (35)

(note that the only randomness in \(Z_{0}\) comes from \(X_{0}\) as \(Y_{0}\) is a deterministic function of \(X_{0}\)). The theorem statement is equivalent to the following:

\[\inf_{(A,B)\in\mathcal{S}}\sum_{i=0}^{k}\left\|\nabla_{A_{i}}f(A,B)\right\|_{F} ^{2}+\left\|\nabla_{B_{i}}f(A,B)\right\|_{F}^{2}=0\] (36)where \(\nabla_{A_{i}}f\) denotes derivative wrt the Frobenius norm \(\left\|A_{i}\right\|_{F}\).

Our goal is to show that, if \((A,B)\in\mathcal{S}\), then for any \((R,S)\in\mathbb{R}^{2\times(k+1)\times d\times d}\), there exists \((\tilde{R},\tilde{S})\in\mathcal{S}\), such that, at \(t=0\),

\[\frac{d}{dt}f(A+t\tilde{R},B+t\tilde{S})\leq\frac{d}{dt}f(A+tR,B+tS).\] (37)

In fact, we show that \(\tilde{R}_{i}:=r_{i}I\), for \(r_{i}=\frac{1}{d}\operatorname{Tr}\left(\Sigma^{1/2}R_{i}\Sigma^{1/2}\right)\) and \(\tilde{S}_{i}=s_{i}I\), for \(s_{i}=\frac{1}{d}\operatorname{Tr}\left(\Sigma^{-1/2}S_{i}\Sigma^{1/2}\right)\). This implies (36) via the following simple argument: Consider the "\(\mathcal{S}\)-constrained gradient flow": let \(A(t):\mathbb{R}^{+}\to\mathbb{R}^{(k+1)\times d\times d}\) and \(B(t):\mathbb{R}^{+}\to\mathbb{R}^{(k+1)\times d\times d}\) be defined as

\[\frac{d}{dt}A_{i}(t) =-r_{i}(t)\Sigma^{-1},\quad r_{i}(t):=\operatorname{Tr}(\Sigma^ {1/2}\nabla_{A_{i}}f(A(t),B(t))\Sigma^{1/2})\] \[\frac{d}{dt}B_{i}(t) =-s_{i}(t)\Sigma^{-1},\quad s_{i}(t):=\operatorname{Tr}(\Sigma^{ -1/2}\nabla_{B_{i}}f(A(t),B(t))\Sigma^{1/2}),\]

for \(i=0,\dots,k\). By (37), we verify that

\[\frac{d}{dt}f(A(t),B(t))\leq-\left(\sum_{i=0}^{k}\left\|\nabla_{A_{i}}f(A(t), B(t))\right\|_{F}^{2}+\left\|\nabla_{B_{i}}f(A(t),B(t))\right\|_{F}^{2}\right).\] (38)

We verify from its definition that \(f(A,B)\geq 0\); if (36) does not hold then (38) will ensure unbounded descent as \(t\to\infty\), contradicting the fact that \(f(A,B)\) is lower-bounded. This concludes the proof.

_Proof outline._ The remainder of the proof will be devoted to showing (37), which we outline as follows:

* In Step 1, we reduce the condition in (37) to a more easily verified _layer-wise_ condition. Specifically, we only need to verify (37) in one of the two cases: (I) when \(R_{i},S_{i}\) are all zero except for \(R_{j}\) for some fixed \(j\) (see (40)), or (II) when \(R_{i},S_{i}\) are all zero except for \(S_{j}\) for some fixed \(j\) (see (39)). We focus on the proof of (II), as the proof of (I) is almost identical. At the end of Step 1, we set up some additional notation, and introduce an important matrix \(G\), which is roughly "a product of attention layer matrices". In (41), we study the evolution of \(f(A,B(t))\) when \(B(t)\) moves in the direction of \(S\), as \(X_{0}\) is (roughly speaking) randomly transformed. This motivates the subsequent analysis in Steps 2 and 3 below.
* In Step 2, we study how outputs of each layer (34) changes when \(X_{0}\) is randomly transformed. There are two main results here: First we provide the expression for \(X_{i}\) in (42). Second, we provide the expression for \(\frac{d}{dt}X_{i}(B(t))\) in (43).
* In Step 3, we use the results of Step 2 to to study \(G\) (see (47)) and \(\frac{d}{dt}G(B(t))\) (see (48)) under random transformation of \(X_{0}\). The idea in (48) is that "randomly transforming \(X_{0}\)" has the same effect as "randomly transforming \(S\)" (recall \(S\) is the perturbation to \(B\)).
* In Step 4, we use the results from Steps 2 and 3 to the expression of \(\frac{d}{dt}f(A,B(t))\) in (41). We verify that \(\tilde{S}\) in (37) is exactly the expected matrix after "randomly transforming \(S\)". This concludes our proof of (II).
* In Step 5, we sketch the proof of (I), which is almost identical to Steps 2-4.

_1. Reduction to layer-wise condition._ To prove (37), it suffices to show the following simpler condition: Let \(j\in\{0,\dots,k\}\). Let \(R_{j},S_{j}\in\mathbb{R}^{d\times d}\) be arbitrary matrices. For \(C\in\mathbb{R}^{d\times d}\), let \(A(tC,j)\) denote the collection of matrices, where \(A(tC,j)_{j}=A_{j}+tC\), and for \(i\neq j\), \(A(tC,j)_{i}=A_{i}\). Define \(B(tC,j)\) analogously. We show that for all \(j\in\{0,\dots,k\}\) and all \(R_{j},S_{j}\in\mathbb{R}^{d\times d}\), there exists \(\tilde{R}_{j}=r_{j}\Sigma^{-1}\) and \(\tilde{S}_{j}=s_{j}\Sigma^{-1}\), such that, at \(t=0\),

\[\frac{d}{dt}f(A(t\tilde{R}_{j},j),B) \leq\frac{d}{dt}f(A(tR_{j},j),B)\] (39) \[\text{and} \frac{d}{dt}f(A,B(t\tilde{S}_{j},j)) \leq\frac{d}{dt}f(A,B(tS_{j},j)).\] (40)We can verify that (37) is equivalent to (39)+(40) by noticing that for any \((R,S)\in\mathbb{R}^{2\times(k+1)\times d\times d}\), at \(t=0\), \(\frac{d}{dt}f(A+tR,B+tS)=\sum_{j=0}^{k}\big{(}\frac{d}{dt}f(A(tR_{j},j),B)+\frac{ d}{dt}f(A,B(tS_{j},j))\big{)}\).

We will first focus on proving (40) (the proof of (39) is similar, and we present it in Step 5 at the end), for some index \(j\) that is arbitrarily chosen but fixed throughout. Notice that \(X_{i}\) and \(Y_{i}\) in (34) are in fact functions of \(A,B\) and \(X_{0}\). For most of our subsequent discussion, \(A_{i}\) (for all \(i\)) and \(B_{i}\) (for all \(i\neq j\)) can be treated as constant matrices. We will however make the dependence on \(X_{0}\) and \(B_{j}\) explicit (as we consider the curve \(B_{j}+tS\)), i.e. we use \(X_{i}(X,C)\) (resp \(Y_{i}(X,C)\)) to denote the value of \(X_{i}\) (resp \(Y_{i}\)) from (34), with \(X_{0}=X\), and \(B_{j}=C\).

By (35) and (34),

\[f(A,B(tS_{j},j))\] \[= \mathbb{E}\left[\operatorname{Tr}\left(\left(I-M\right)Y_{k+1}(X _{0},B_{j}+tS)^{\top}Y_{k+1}(X_{0},B_{j}+tS_{j})\left(I-M\right)\right)\right]\] \[= \mathbb{E}\left[\operatorname{Tr}\left(\left(I-M\right)G(X_{0},B _{j}+tS_{j})^{\top}w_{\star}^{\top}w_{\star}G(X_{0},B_{j}+tS_{j})\left(I-M \right)\right)\right]\] \[= \mathbb{E}\left[\operatorname{Tr}\left(\left(I-M\right)G(X_{0},B _{j}+tS_{j})^{\top}\Sigma^{-1}G(X_{0},B_{j}+tS_{j})\left(I-M\right)\right)\right]\]

where \(G(X,C):=X\prod_{i=0}^{k}\big{(}I-MX_{i}(X,C)^{T}A_{i}X_{i}(X,C)\big{)}\). The second equality follows from plugging in (34).

For the rest of this proof, let \(U\) denote a uniformly randomly sampled orthogonal matrix. Let \(U_{\Sigma}:=\Sigma^{1/2}U\Sigma^{-1/2}\). Using the fact that \(X_{0}\overset{d}{=}U_{\Sigma}X_{0}\), we can verify

\[\frac{d}{dt}f(A,B(tS_{j},j))\bigg{|}_{t=0}\] \[= \frac{d}{dt}\left.\mathbb{E}_{X_{0}}\left[\operatorname{Tr}\left( \left(I-M\right)G(X_{0},B_{j}+tS_{j})^{\top}\Sigma^{-1}G(X_{0},B_{j}+tS_{j}) \left(I-M\right)\right)\right]\right|_{t=0}\] \[= \frac{d}{dt}\left.\mathbb{E}_{X_{0},U}\left[\operatorname{Tr} \left(\left(I-M\right)G(U_{\Sigma}X_{0},B_{j})^{\top}\Sigma^{-1}\left.\frac{d }{dt}G(U_{\Sigma}X_{0},B_{j}+tS_{j})\right|_{t=0}\left(I-M\right)\right)\right] \right|_{t=0}\] \[= 2\left.\mathbb{E}_{X_{0},U}\left[\operatorname{Tr}\left(\left(I -M\right)G(U_{\Sigma}X_{0},B_{j})^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(U_{ \Sigma}X_{0},B_{j}+tS_{j})\right|_{t=0}\left(I-M\right)\right)\right].\] (41)

_2. \(X_{i}\) and \(\frac{d}{dt}X_{i}\) under random transformation of \(X_{0}\)._ In this step, we prove that when \(X_{0}\) is transformed by \(U_{\Sigma}\), \(X_{i}\) for \(i\geq 1\) are likewise transformed in a simple manner. The first goal of this step is to show

\[X_{i}(U_{\Sigma}X_{0},B_{j})=U_{\Sigma}X_{i}(X_{0},B_{j}).\] (42)

We will prove this by induction. When \(i=0\), this clearly holds by definition. Suppose that (42) holds for some \(i\). Then

\[X_{i+1}(U_{\Sigma}X_{0},B_{j})\] \[= X_{i}(U_{\Sigma}X_{0},B_{j})+B_{i}X_{i}(U_{\Sigma}X_{0},B_{j})MX_ {i}(U_{\Sigma}X_{0},B_{j})^{T}A_{i}X_{i}(U_{\Sigma}X_{0},B_{j})\] \[= U_{\Sigma}X_{i}(X_{0},B_{j})+U_{\Sigma}B_{i}X_{i}(X_{0},B_{j})MX_ {i}(X_{0},B_{j})^{T}A_{i}X_{i}(X_{0},B_{j})\] \[= U_{\Sigma}X_{i+1}(X_{0},B_{j})\]

where the second equality uses the inductive hypothesis, and the fact that \(A_{i}=a_{i}\Sigma^{-1}\), so that \(U_{\Sigma}^{T}A_{i}U_{\Sigma}=A_{i}\), and the fact that \(B_{i}=b_{i}I\), from the definition of \(\mathcal{S}\) and our assumption that \((A,B)\in\mathcal{S}\). This concludes the proof of (42).

We now present the second main result of this step. Let \(U_{\Sigma}^{-1}:=\Sigma^{1/2}U^{T}\Sigma^{-1/2}\), so that it satisfies \(U_{\Sigma}U_{\Sigma}^{-1}=U_{\Sigma}^{-1}U_{\Sigma}=I\). For all \(i\),

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{i}(U_{\Sigma}X_{0},B_{j}+tS_{j}) \bigg{|}_{t=0}=\left.\frac{d}{dt}X_{i}(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{ \Sigma})\right|_{t=0}.\] (43)

To reduce notation, we will not write \(\cdot\big{|}_{t=0}\) explicitly in the subsequent proof. We first write down the dynamics for the right-hand-side term of (43): From (34), for any \(\ell\leq j\), and for any \(i\geq j+1\), and for any \(C\in\mathbb{R}^{d\times d}\),

\[\frac{d}{dt}X_{\ell}\left(X_{0},B_{j}+tC\right)=0\] \[\frac{d}{dt}X_{j+1}\left(X_{0},B_{j}+tC\right)=CX_{j}\left(X_{0},B _{j}\right)MX_{j}\left(X_{0},B_{j}\right)^{\top}A_{j}X_{j}\left(X_{0},B_{j}\right)\] \[\frac{d}{dt}X_{i+1}\left(X_{0},B_{j}+tC\right)=\frac{d}{dt}X_{i} \left(X_{0},B_{j}+tC\right)\] \[\qquad\qquad\qquad\qquad+B_{i}\left(\frac{d}{dt}X_{i}\left(X_{0},B_{j}+tC\right)\right)MX_{i}\left(X_{0},B_{j}\right)^{\top}A_{i}X_{i}\left(X_ {0},B_{j}\right)\] \[\qquad\qquad\qquad\qquad+B_{i}X_{i}\left(X_{0},B_{j}\right)MX_{i }\left(X_{0},B_{j}\right)^{\top}A_{i}\left(\frac{d}{dt}X_{i}\left(X_{0},B_{j} +tC\right)\right)\] (44)

We are now ready to prove (43) using induction. For the base case, we verify that for \(\ell\leq j\), \(U_{\Sigma}^{-1}\frac{d}{dt}X_{\ell}\left(U_{\Sigma}X_{0},B_{k}+tS_{j}\right)= 0=\frac{d}{dt}X_{\ell}\left(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma}\right)\) (see first equation in (44)). For index \(j+1\), we verify that

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{j+1}\left(U_{\Sigma}X_{0},B_{j}+tS _{j}\right)\] \[= U_{\Sigma}^{-1}S_{j}U_{\Sigma}X_{j}(X_{0},B_{j})MX_{j}(U_{\Sigma }X_{0},B_{j})^{\top}A_{j}X_{j}(U_{\Sigma}X_{0},B_{j})\] \[= \frac{d}{dt}X_{j+1}\left(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{ \Sigma}\right)\] (45)

where we use two facts: 1. \(X_{i}(U_{\Sigma}X_{0},B_{j})=U_{\Sigma}X_{i}(X_{0},B_{j})\) from (42), 2. \(A_{i}=a_{i}\Sigma^{-1}\), so that \(U_{\Sigma}^{\top}A_{i}U_{\Sigma}=A_{i}\). We verify by comparison to the second equation in (44) that \(U_{\Sigma}^{-1}\frac{d}{dt}X_{j}\left(U_{\Sigma}X_{0},B_{j}+tS_{j}\right)=0= \frac{d}{dt}X_{j}\left(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma}\right)\). These conclude the proof of the base case.

Now suppose that (43) holds for some \(i\). We will now prove (43) holds for \(i+1\). From (34),

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{i+1}\left(U_{\Sigma}X_{0},B_{j}+tS _{j}\right)\] \[= U_{\Sigma}^{-1}\frac{d}{dt}\left(X_{i}\left(U_{\Sigma}X_{0},B_{j }+tS_{j}\right)\right)\] \[\qquad+U_{\Sigma}^{-1}\frac{d}{dt}\left(B_{i}X_{i}\left(U_{\Sigma }X_{0},B_{j}+tS_{j}\right)MX_{i}\left(U_{\Sigma}X_{0},B_{j}+tS_{j}\right)^{ \top}A_{i}X_{i}\left(U_{\Sigma}X_{0},B_{j}+tS_{j}\right)\right)\] \[= U_{\Sigma}^{-1}\frac{d}{dt}\left(X_{i}\left(U_{\Sigma}X_{0},B_{j }+tS_{j}\right)\right)\] \[\qquad+U_{\Sigma}^{-1}B_{i}\left(\frac{d}{dt}X_{i}\left(U_{\Sigma }X_{0},B_{j}+tS_{j}\right)\right)MX_{i}\left(U_{\Sigma}X_{0},B_{j}\right)^{ \top}A_{i}X_{i}\left(U_{\Sigma}X_{0},B_{j}\right)\] \[\qquad+U_{\Sigma}^{-1}B_{i}X_{i}\left(U_{\Sigma}X_{0},B_{j}\right)MX _{i}\left(U_{\Sigma}X_{0},B_{j}\right)^{\top}A_{i}\left(\frac{d}{dt}X_{i}\left( U_{\Sigma}X_{0},B_{j}+tS_{j}\right)\right)\] \[\overset{(i)}{=} U_{\Sigma}^{-1}\frac{d}{dt}X_{i}\left(U_{\Sigma}X_{0},B_{j}+tS _{j}\right)\] \[\overset{(i)}{=} U_{\Sigma}^{-1}\frac{d}{dt}X_{i}\left(U_{\Sigma}X_{0},B_{j}+tS _{j}\right)\] \[\qquad+B_{i}X_{i}\left(X_{0},B_{j}\right)M\left(U_{\Sigma}^{-1} \frac{d}{dt}X_{i}\left(U_{\Sigma}X_{0},B_{j}+tS_{j}\right)\right)^{\top}A_{i} X_{i}\left(X_{0},B_{j}\right)\] \[\overset{(i)}{=} U_{\Sigma}^{-1}

\[\stackrel{{(ii)}}{{=}} \frac{d}{dt}X_{i}\left(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma}\right)\] \[\qquad+B_{i}X_{i}\left(X_{0},B_{j}\right)M\left(\frac{d}{dt}X_{i} \left(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma}\right)\right)^{\top}A_{i}X_{ i}\left(X_{0},B_{j}\right)\] \[\qquad+B_{i}X_{i}\left(X_{0},B_{j}\right)MX_{i}\left(X_{0},B_{j} \right)^{\top}A_{i}\left(\frac{d}{dt}X_{i}\left(X_{0},B_{j}+tU_{\Sigma}^{-1}S_ {j}U_{\Sigma}\right)\right)\] (46)

In \((i)\) above, we crucially use the following facts: 1. \(B_{i}=b_{i}I\) so that \(U_{\Sigma}^{-1}B_{i}=B_{i}U_{\Sigma}^{-1}\), 2. \(X_{i}(U_{\Sigma}X_{0},B_{j})=U_{\Sigma}X_{i}(X_{0},B_{j})\) from (42), 3. \(A_{i}=a_{i}\Sigma^{-1}\), so that \(U_{\Sigma}^{\top}A_{i}U_{\Sigma}=A_{i}\), 4. \(U_{\Sigma}U_{\Sigma}^{-1}=U_{\Sigma}^{-1}U_{\Sigma}=I\). \((ii)\) follows from our inductive hypothesis. The inductive proof is complete by verifying that (46) exactly matches the third equation of (44) when \(C=U_{\Sigma}^{-1}SU_{\Sigma}\).

_3. \(G\) and \(\frac{d}{dt}G\) under random transformation of \(X_{0}\)._ We now verify that \(G(U_{\Sigma}X_{0},B_{j})=U_{\Sigma}G(X_{0},B_{j})\). This is a straightforward consequence of (42) as

\[G(U_{\Sigma}X_{0},B_{j})\] \[= U_{\Sigma}X_{0}\prod_{i=0}^{k}\left(I+MX_{i}(U_{\Sigma}X_{0},B_ {j})^{T}A_{i}X_{i}(U_{\Sigma}X_{0},B_{j})\right)\] \[= U_{\Sigma}X_{0}\prod_{i=0}^{k}\left(I+MX_{i}(X_{0},B_{j})^{T}A_ {i}X_{i}(X_{0},B_{j})\right)\] \[= U_{\Sigma}G(X_{0},B_{j}),\] (47)

where the second equality uses (42), as well as the fact that \(U_{\Sigma}^{\top}A_{i}U_{\Sigma}=A_{i}\). Next, we will show that

\[U_{\Sigma}^{-1}\left.\frac{d}{dt}G(U_{\Sigma}X_{0},B_{j}+tS_{j})\right|_{t=0} =\left.\frac{d}{dt}G(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma})\right|_{t=0}.\] (48)

To see this, we can expand

\[U_{\Sigma}^{-1}\frac{d}{dt}G(U_{\Sigma}X_{0},B_{j}+tS_{j})\] \[= U_{\Sigma}^{-1}\frac{d}{dt}\left(U_{\Sigma}X_{0}\prod_{i=0}^{k} \left(I+MX_{i}(U_{\Sigma}X_{0},B_{j}+tS_{j})^{T}A_{i}X_{i}(U_{\Sigma}X_{0},B_ {j}+tS_{j})\right)\right)\] \[= X_{0}\sum_{i=0}^{k}\left(\prod_{\ell=0}^{i-1}\left(I+MX_{\ell} (U_{\Sigma}X_{0},B_{j})^{T}A_{\ell}X_{i}(U_{\Sigma}X_{0},B_{\ell})\right)\right)\] \[\cdot M\frac{d}{dt}\left(X_{i}(U_{\Sigma}X_{0},B_{j}+tS_{j})^{T}A_ {i}X_{i}(U_{\Sigma}X_{0},B_{j})\right)\] \[\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(U_{\Sigma}X_{0}, B_{j})^{T}A_{\ell}X_{i}(U_{\Sigma}X_{0},B_{\ell})\right)\right)\] \[\stackrel{{(i)}}{{=}} X_{0}\sum_{i=0}^{k}\left(\prod_{\ell=0}^{i-1}\left(I+MX_{\ell}(X_{0},B_ {j})^{T}A_{\ell}X_{\ell}(X_{0},B_{\ell})\right)\right)\] \[\cdot M\left(\left(U_{\Sigma}^{-1}\frac{d}{dt}X_{i}(U_{\Sigma}X_{0},B _{j}+tS_{j})\right)^{T}A_{i}X_{i}(X_{0},B_{j})+MX_{i}(X_{0},B_{j})^{T}A_{i} \left(U_{\Sigma}^{-1}\frac{d}{dt}X_{i}(U_{\Sigma}X_{0},B_{j}+tS_{j})\right)\right)\] \[\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(X_{0},B_{j})^{T }A_{\ell}X_{\ell}(X_{0},B_{\ell})\right)\right)\] \[\stackrel{{(ii)}}{{=}} X_{0}\sum_{i=0}^{k}\left(\prod_{\ell=0}^{i-1}\left(I+MX_{\ell}(X_{0},B_ {j})^{T}A_{\ell}X_{\ell}(X_{0},B_{\ell})\right)\right)\]\[\cdot M\left(\left(\frac{d}{dt}X_{i}(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j} U_{\Sigma})\right)^{T}A_{i}X_{i}(X_{0},B_{j})+MX_{i}(X_{0},B_{j})^{T}A_{i}\left( \frac{d}{dt}X_{i}(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma})\right)\right)\] \[\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(X_{0},B_{j})^{T }A_{\ell}X_{\ell}(X_{0},B_{\ell})\right)\right)\] \[\stackrel{{(iii)}}{{=}}\frac{d}{dt}G(X_{0},B_{j}+tU_ {\Sigma}^{-1}S_{j}U_{\Sigma})\]

In \((i)\) above, we the following facts: 1. \(X_{i}(U_{\Sigma}X_{0},B_{j})=U_{\Sigma}X_{i}(X_{0},B_{j})\) from (42), 2. \(A_{i}=a_{i}\Sigma^{-1}\), so that \(U_{\Sigma}^{\top}A_{i}U_{\Sigma}=A_{i}\), 3. \(U_{\Sigma}U_{\Sigma}^{-1}=U_{\Sigma}^{-1}U_{\Sigma}=I\). \((ii)\) follows from (43). \((iii)\) is by definition of \(G\).

_4. Putting everything together._ Let us now continue from (41). We can now plug (47) and (48) into (41):

\[\left.\frac{d}{dt}f(A,B(tS_{j},j))\right|_{t=0}\] \[= 2\,\mathbb{E}_{X_{0},U}\left[\mbox{Tr}\left((I-M)\,G(U_{\Sigma}X _{0},B_{j})^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(U_{\Sigma}X_{0},B_{j}+tS_{j}) \right|_{t=0}(I-M)\right)\right]\] \[\stackrel{{(i)}}{{=}} 2\,\mathbb{E}_{X_{0},U}\left[\mbox{Tr}\left((I-M)\,G(X_{0},B_{j}) ^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{ \Sigma})\right|_{t=0}(I-M)\right)\right]\] \[= 2\,\mathbb{E}_{X_{0}}\left[\mbox{Tr}\left((I-M)\,G(X_{0},B_{j}) ^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{ \Sigma})\right|_{t=0}(I-M)\right)\right]\] \[\stackrel{{(ii)}}{{=}} 2\,\mathbb{E}_{X_{0}}\left[\mbox{Tr}\left((I-M)\,G(X_{0},B_{j}) ^{\top}\Sigma^{-1}\left.\frac{d}{dt}G(X_{0},B_{j}+t\,\mathbb{E}_{U}\left[U_{ \Sigma}^{-1}S_{j}U_{\Sigma}\right])\right|_{t=0}(I-M)\right)\right]\] \[= \left.\frac{d}{dt}f(A,B(ts_{j}I,j))\right|_{t=0}\]

where \(s_{j}:=\frac{1}{d}\,\mbox{Tr}\left(\Sigma^{-1/2}S_{j}\Sigma^{1/2}\right)\). In the above, \((i)\) uses 1. (47) and (48), as well as the fact that \(U_{\Sigma}^{\top}\Sigma^{-1}U_{\Sigma}=\Sigma^{-1}\). \((ii)\) uses the fact that \(\frac{d}{dt}G(X_{0},B_{j}+tC)\big{|}_{t=0}\) is affine in \(C\). To see this, one can verify from (44), using a simple induction argument, that \(\frac{d}{dt}X_{i}(X_{0},B_{j}+tC)\) is affine in \(C\) for all \(i\). We can then verify from the definition of \(G\), e.g. using similar algebra as the proof of (48), that \(\frac{d}{dt}G(X_{0},B_{j}+C)\) is affine in \(\frac{d}{dt}X_{i}(X_{0},B_{j}+tC)\). Thus \(\mathbb{E}_{U}\left[G(X_{0},B_{j}+tU_{\Sigma}^{-1}S_{j}U_{\Sigma})\right]=G(X_{ 0},B_{j}+t\,\mathbb{E}_{U}\left[U_{\Sigma}^{-1}S_{j}U_{\Sigma})\right]\).

With this, we conclude our proof of (40).

_5. Proof of_ (39). We will now prove (39) for fixed but arbitrary \(j\), i.e. there is some \(r_{j}\) such that

\[\frac{d}{dt}f(A(t\cdot r_{j}\Sigma^{-1},j),B)\leq\frac{d}{dt}f(A(tR_{j},j),B).\]

The proof is very similar to the proof of (40) that we just saw, and we will essentially repeat the same steps from Step 2-4 above.

Since we now consider perturbations to \(A\) instead of to \(B\), we will need to redefine some notation: let \(X_{i}(X,C)\) (resp \(Y_{i}(X,C)\)) to denote the value of \(X_{i}\) (resp \(Y_{i}\)) from (34), with \(X_{0}=X\), and \(A_{j}=C\) (previously it was with \(B_{j}=C\)). Let \(G(X,A_{j}+C):=X\prod_{i=0}^{i}\left(I+M\left(X_{i}(X,A_{j}+C)^{T}A(C,j)_{i}X_{i} (X,A_{j}+C)\right)\right)\), where recall that \(A(C,j):=A_{j}+C\), and \(A(C,j)_{\ell}:=A_{\ell}\) for all \(\ell\in\{0...k\}\setminus\{j\}\).

We first verify that

\[X_{i}(U_{\Sigma}X_{0},A_{j})=U_{\Sigma}X_{i}(X_{0},A_{j})\] \[G(U_{\Sigma}X_{0},A_{j})=U_{\Sigma}G(X_{0},A_{j}).\] (49)The proofs are identical to the proofs of (42) and (47) so we omit them. Next, we show that for all \(i\),

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{i}(U_{\Sigma}X_{0},A_{j}+tR_{j})\bigg{|}_{t=0}= \left.\frac{d}{dt}X_{i}(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{\Sigma})\right|_{t= 0}.\] (50)

We establish the dynamics for the right-hand-side of (50):

\[\frac{d}{dt}X_{\ell}\left(X_{0},A_{j}+tC\right)=0\] \[\frac{d}{dt}X_{j+1}\left(X_{0},A_{j}+tC\right)=B_{j}X_{j}\left(X _{0},A_{j}\right)MX_{j}\left(X_{0},A_{j}\right)^{\top}CX_{j}\left(X_{0},A_{j}\right)\] \[\frac{d}{dt}X_{i+1}\left(X_{0},A_{j}+tC\right)=\frac{d}{dt}X_{i} \left(X_{0},A_{j}+tC\right)\] \[\qquad\qquad\qquad\qquad+B_{i}X_{i}\left(X_{0},A_{j}\right)M \left(\frac{d}{dt}X_{i}\left(X_{0},A_{j}+tC\right)\right)^{\top}A_{i}X_{i} \left(X_{0},A_{j}\right)\] \[\qquad\qquad\qquad\qquad+B_{i}X_{i}\left(X_{0},A_{j}\right)MX_{ i}\left(X_{0},A_{j}\right)^{\top}A_{i}\left(\frac{d}{dt}X_{i}\left(X_{0},A_{j}+tC \right)\right)\] (51)

Similar to (45), we show that for \(i\leq j\),

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{i}\left(U_{\Sigma}X_{0},A_{j}+tR_{j}\right)= 0=U_{\Sigma}^{-1}\frac{d}{dt}X_{i}\left(U_{\Sigma}X_{0},A_{j}+tU_{ \Sigma}R_{j}U_{\Sigma}\right)\]

and

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{j+1}\left(U_{\Sigma}X_{0},A_{j}+tR _{j}\right)\] \[= U_{\Sigma}^{-1}B_{j}U_{\Sigma}X_{j}(X_{0},A_{j})MX_{j}(U_{\Sigma }X_{0},A_{j})^{\top}A_{j}X_{j}(U_{\Sigma}X_{0},A_{j})\] \[= \frac{d}{dt}X_{j+1}\left(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{ \Sigma}\right).\]

Finally, for the inductive step, we follow identical steps leading up to (46) to show that

\[U_{\Sigma}^{-1}\frac{d}{dt}X_{i+1}\left(U_{\Sigma}X_{0},A_{j}+tR _{j}\right)\] \[= \frac{d}{dt}X_{i}\left(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{ \Sigma}\right)\] \[\qquad+B_{i}X_{i}\left(X_{0},A_{j}\right)M\left(\frac{d}{dt}X_{i} \left(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{\Sigma}\right)\right)^{\top}A_{i} X_{i}\left(X_{0},A_{j}\right)\] \[\qquad+B_{i}X_{i}\left(X_{0},A_{j}\right)MX_{i}\left(X_{0},A_{j} \right)^{\top}A_{i}\left(\frac{d}{dt}X_{i}\left(X_{0},A_{j}+tU_{\Sigma}^{\top }R_{j}U_{\Sigma}\right)\right)\] (52)

The inductive proof is complete by verifying that (52) exactly matches the third equation of (51) when \(C=U_{\Sigma}^{-1}SU_{\Sigma}\). This concludes the proof of (50).

Next, we study the time derivative of \(G(U_{\Sigma}X_{0},A_{j}+tR_{j})\) and show that

\[U_{\Sigma}^{-1}\frac{d}{dt}G(U_{\Sigma}X_{0},A_{j}+tR_{j})= \frac{d}{dt}G(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{\Sigma}).\] (53)

This proof differs significantly from that of (48) in a few places, so we provide the whole derivation below. By chain-rule, we can write

\[U_{\Sigma}^{-1}\frac{d}{dt}G(U_{\Sigma}X_{0},A_{j}+tR_{j})=\spadesuit+\heartsuit\]where

\[\spadesuit :=U_{\Sigma}^{-1}\frac{d}{dt}\left(U_{\Sigma}X_{0}\prod_{i=0}^{k} \left(I+MX_{i}(U_{\Sigma}X_{0},A_{j}+tR_{j})^{T}A_{i}X_{i}(U_{\Sigma}X_{0},A_{j} +tR_{j})\right)\right)\] \[\quad\quad\cdot MX_{j}(U_{\Sigma}X_{0},A_{j})^{T}R_{j}X_{j}(U_{ \Sigma}X_{0},A_{j})\] \[\quad\quad\left.\left(\prod_{i=j+1}^{k}\left(I+MX_{i}(U_{\Sigma}X _{0},A_{j})^{T}A_{i}X_{i}(U_{\Sigma}X_{0},A_{j})\right)\right)\right..\]

We will separately simplify \(\spadesuit\) and \(\heartsuit\), and verify at the end that summing them recovers the right-hand-side of (53). We begin with \(\spadesuit\), and the steps are almost identical to the proof of (48).

\(\spadesuit\)

\[= U_{\Sigma}^{-1}\frac{d}{dt}\left(U_{\Sigma}X_{0}\prod_{i=0}^{k} \left(I+MX_{i}(U_{\Sigma}X_{0},A_{j}+tR_{j})^{T}A_{i}X_{i}(U_{\Sigma}X_{0},A_ {j}+tR_{j})\right)\right)\] \[= X_{0}\sum_{i=0}^{k}\left(\prod_{\ell=0}^{i-1}\left(I+MX_{\ell} (U_{\Sigma}X_{0},A_{j})^{T}A_{\ell}X_{i}(U_{\Sigma}X_{0},A_{\ell})\right)\right)\] \[\quad\cdot M\frac{d}{dt}\left(X_{i}(U_{\Sigma}X_{0},A_{j}+tR_{j} )^{T}A_{i}X_{i}(U_{\Sigma}X_{0},A_{j}+tR_{j})\right)\] \[\quad\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(U_{\Sigma} X_{0},A_{j})^{T}A_{\ell}X_{i}(U_{\Sigma}X_{0},A_{\ell})\right)\right)\] \[\quad\cdot M\left(\left(U_{\Sigma}^{-1}\frac{d}{dt}X_{i}(U_{ \Sigma}X_{0},A_{j}+tR_{j})\right)^{T}A_{i}X_{i}(X_{0},A_{j})+MX_{i}(X_{0},A_{ j})^{T}A_{i}\left(U_{\Sigma}^{-1}\frac{d}{dt}X_{i}(U_{\Sigma}X_{0},A_{j}+tR_{j}) \right)\right)\] \[\quad\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(X_{0},A_{j })^{T}A_{\ell}X_{\ell}(X_{0},A_{\ell})\right)\right)\] \[\quad\cdot M\left(\left(\frac{d}{dt}X_{i}(X_{0},A_{j}+tU_{\Sigma} ^{\top}R_{j}U_{\Sigma})\right)^{T}A_{i}X_{i}(X_{0},A_{j})+MX_{i}(X_{0},A_{j}) ^{T}A_{i}\left(\frac{d}{dt}X_{i}(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{\Sigma })\right)\right)\] \[\quad\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(X_{0},A_{j })^{T}A_{\ell}X_{\ell}(X_{0},A_{\ell})\right)\right)\] \[= X_{0}\sum_{i=0}^{k}\left(\prod_{\ell=0}^{i-1}\left(I+MX_{\ell} (X_{0},A_{j})^{T}A_{\ell}X_{\ell}(X_{0},A_{\ell})\right)\right)\] \[\quad\cdot M\frac{d}{dt}\left(X_{i}(X_{0},A_{j}+tU_{\Sigma}^{\top} R_{j}U_{\Sigma})^{T}A_{i}X_{i}(X_{0},A_{j}+tU_{\Sigma}^{\top}R_{j}U_{\Sigma})\right)\] \[\quad\cdot\left(\prod_{\ell=i+1}^{k}\left(I+MX_{\ell}(X_{0},A_{j })^{T}A_{\ell}X_{\ell}(X_{0},A_{\ell})\right)\right)\] (54)

[MISSING_PAGE_FAIL:32]

### Equivalence under permutation

**Lemma 4**.: _Consider the same setup as Theorem 3. Let \(A=\left\{A_{i}\right\}_{i=0}^{k}\), with \(A_{i}=a_{i}\Sigma^{-1}\). Let_

\[f(A):=f\left(\left\{Q_{i}=\begin{bmatrix}A_{i}&0\\ 0&0\end{bmatrix},P_{i}=\begin{bmatrix}0_{d\times d}&0\\ 0&1\end{bmatrix}\right\}_{i=0}^{k}\right).\]

_Let \(i,j\in\{0,\ldots,k\}\) be any two arbitrary indices, and let \(\tilde{A}_{i}=A_{j}\), \(\tilde{A}_{j}=A_{i}\), and let \(\tilde{A}_{\ell}=A_{\ell}\) for all \(\ell\in\{0,\ldots,k\}\setminus\{i,j\}\). Then \(f(A)=f(\tilde{A})\)_

Proof.: Following the same setup leading up to (26) in the proof of Theorem 3, we verify that the in-context loss is

\[f(A)=\mathbb{E}\left[\operatorname{Tr}\left((I-M)\,G(X_{0},A)^{\top}\Sigma^{- 1}G(X_{0},A)\,(I-M)\right)\right]\]

where \(G(X_{0},A):=X_{0}\prod_{\ell=0}^{k}\left(I+MX_{0}^{T}A_{\ell}X_{0}\right)\).

Consider any fixed index \(\ell\). We will show that

\[\left(I+MX_{0}^{T}A_{\ell}X_{0}\right)\left(I+MX_{0}^{T}A_{\ell+1}X_{0}\right) =\left(I+MX_{0}^{T}A_{\ell+1}X_{0}\right)\left(I+MX_{0}^{T}A_{\ell}X_{0} \right).\]

The lemma can then be proven by repeatedly applying the above, so that indices of \(A_{i}\) and \(A_{j}\) are swapped.

To prove the above equality,

\[\left(I+MX_{0}^{T}A_{\ell}X_{0}\right)\left(I+MX_{0}^{T}A_{\ell+1 }X_{0}\right)\] \[= I+MX_{0}^{T}A_{\ell}X_{0}+MX_{0}^{T}A_{\ell+1}X_{0}+MX_{0}^{T}A _{\ell}X_{0}MX_{0}^{T}A_{\ell+1}X_{0}\] \[= I+MX_{0}^{T}A_{\ell}X_{0}+MX_{0}^{T}A_{\ell+1}X_{0}+MX_{0}^{T} a_{\ell}\Sigma^{-1}X_{0}MX_{0}^{T}a_{\ell+1}\Sigma^{-1}X_{0}\] \[= I+MX_{0}^{T}A_{\ell}X_{0}+MX_{0}^{T}A_{\ell+1}X_{0}+MX_{0}^{T} a_{\ell+1}\Sigma^{-1}X_{0}MX_{0}^{T}a_{\ell}\Sigma^{-1}X_{0}\] \[= \left(I+MX_{0}^{T}A_{\ell+1}X_{0}\right)\left(I+MX_{0}^{T}A_{ \ell}X_{0}\right).\]

This concludes the proof. Notice that we crucially used the fact that \(A_{\ell}\) and \(A_{\ell+1}\) are the same matrix up to scaling. 

## Appendix C Auxiliary Lemmas

### Proof of Lemma 1 (Equivalence to Preconditioned Gradient Descent)

Consider fixed samples \(x^{(1)},\ldots,x^{(n)}\), and fixed \(w_{*}\). Let \(P=\left\{P_{i}\right\}_{i=0}^{k},Q=\left\{Q_{i}\right\}_{i=0}^{k}\) denote fixed weights. Let \(Z_{i}\) evolve as described in (4). Let \(X_{i}\) denote the first \(d\) rows of \(Z_{k}\) (under (8), \(X_{i}=X_{0}\) for all \(I\)) and let \(Y_{i}\) denote the \((d+1)^{th}\) row of \(Z_{i}\). Let \(g(x,y,k):\mathbb{R}^{d}\times\mathbb{R}\times\mathbb{Z}\to\mathbb{R}\) be a function defined as follows: let \(x^{n+1}=x\) and let \(y_{0}^{n+1}=y\), then \(g(x,y,k):=y_{k}^{n+1}\). Note that \(y_{k}^{n+1}=\left[Y_{k}\right]_{n+1}\).

We verify that, under (8), the formula for updating \(y_{k}^{(n+1)}\) is given by

\[Y_{k+1}=Y_{k}-\frac{1}{n}Y_{k}MX_{0}^{\top}A_{k}X_{0}.\]

where \(M\) is a mask given by \(\begin{bmatrix}I&0\\ 0&0\end{bmatrix}\). We can verify the following facts

1. \(g(x,y,k)=g(x,0,k)+y\). To see this, notice first that for all \(i\in\{1,\ldots,n\}\), \[y_{k+1}^{(i)}=y_{k}^{(i)}-\frac{1}{n}\sum_{j=1}^{n}{x^{(i)}}^{T}A_{k}x^{(j)}y_ {k}^{(j)}.\]In other words, \(y_{k}^{(i)}\) does not depend on \(y_{t}^{(n+1)}\) for any \(t\). Next, for \(y_{k}^{(n+1)}\) itself, \[y_{k+1}^{(n+1)}=y_{k}^{(n+1)}-\frac{1}{n}\sum_{j=1}^{n}{x^{(n+1)}}^{T}A_{k}x^{(j )}y_{k}^{(j)},\] which depends on \(y_{k}^{n+1}\) only additively. We can verify under a simple induction that \(g(x,y,k+1)-y=g(x,y,k)-y\).
2. \(g(x,0,k)\) is linear in \(x\). To see this, notice first that for \(j\neq n+1\), \(y_{k}^{(j)}\) is does not depend on \(x_{t}^{(n+1)}\) for all \(t,j,k\). Consequently, the update formula for \(y_{k+1}^{(n+1)}\) depends only linearly on \(x^{(n+1)}\) and \(y_{k}^{(n+1)}\). Finally, \(y_{0}^{(n+1)}=0\) is linear in \(x\), so the conclusion follows by induction.

With these two facts in mind, we verify that for each \(k\), there exists a \(\theta_{k}\in\mathbb{R}^{d}\), such that

\[g(x,y,k)=g(x,0,k)+y=\left\langle\theta_{k},x\right\rangle+y\]

for all \(x,y\). It follows from definition that \(g(x,y,0)=y\), so that \(\left\langle\theta_{0},x\right\rangle=g(x,y,0)-y=0\), so that \(\theta_{0}=0\).

We now turn our attention to the third crucial fact: for all \(i\),

\[g(x^{(i)},y^{(i)},k)=y_{k}^{(i)}=\left\langle\theta_{k},x^{(i)}\right\rangle+ y^{(i)}\]

To see this, suppose that we let \(x^{(n+1)}:=x^{(i)}\) for some \(i\in 1,\ldots,n\). Then

\[y_{k+1}^{(i)}=y_{k}^{(i)}-\frac{1}{n}\sum_{j=1}^{n}{x^{(i)}}^{T}A_{k}x^{(j)}y_ {k}^{(j)}\]

\[y_{k+1}^{(n+1)}=y_{k}^{(n+1)}-\frac{1}{n}\sum_{j=1}^{n}{x^{(n+1)}}^{T}A_{k}x^{( j)}y_{k}^{(j)},\]

thus \(y_{k+1}^{(i)}=y_{k+1}^{(n+1)}\) if \(y_{k}^{(i)}=y_{k}^{(n+1)}\), and the induction proof is completed by noting that \(y_{0}^{(i)}=y_{0}^{(n+1)}\) by definition. Let \(\bar{X}\in R^{d\times n}\) be the matrix whose columns are \(x^{(1)},\ldots,x^{(n)}\), leaving out \(x^{(n+1)}\). Let \(\bar{Y}_{k}\in\mathbb{R}^{1\times n}\) denote the vector of \(y_{k}^{(1)},\ldots,y_{k}^{(n)}\). Then it follows that

\[\bar{Y}_{k}=\bar{Y}_{0}+\theta_{k}^{T}\bar{X}.\]

Using the above fact, the update formula for \(y_{k}^{(n+1)}\) can be written as

\[y_{k+1}^{(n+1)}= y_{k}^{(n+1)}-\frac{1}{n}\left\langle A_{k}X^{\top}Y_{k},x^{(n+1)}\right\rangle\] \[\Rightarrow \left\langle\theta_{k+1},x^{(n+1)}\right\rangle= \left\langle\theta_{k},x^{(n+1)}\right\rangle-\frac{1}{n}\left\langle A _{k}\bar{X}\left(\bar{X}^{T}\theta_{k}+\bar{Y}_{0}\right),x^{(n+1)}\right\rangle\] \[= \left\langle\theta_{k},x^{(n+1)}\right\rangle-\frac{1}{n}\left\langle A _{k}\bar{X}\left(\bar{X}^{T}\left(\theta_{k}+w_{\star}\right)\right),x^{(n+1) }\right\rangle\]

Since the choice of \(x^{(n+1)}\) is arbitrary, we get the more general update formula

\[\theta_{k+1}=\theta_{k}-\frac{1}{n}A_{k}\bar{X}\bar{X}^{T}\left(\theta_{k}+w_ {\star}\right).\]

We can treat \(A_{k}\) as a preconditioner. Let \(f(\theta):==\frac{1}{2n}\left(\theta+w_{\star}\right)^{T}\bar{X}\bar{X}^{T}( \theta+w_{\star})\), then

\[\theta_{k+1}=\theta_{k}-\frac{1}{n}A_{k}\nabla f(\theta).\]

Finally, let \(w_{k}^{\text{gd}}:=-\theta_{k}\). We verify that \(f(-w)=R_{w_{\star}}(w)\), so that

\[w_{k+1}^{\text{gd}}=w_{k}^{\text{gd}}-\frac{1}{n}A_{k}\nabla R_{w_{\star}}(w_{ k}^{\text{gd}}).\]

We also verify that for any \(x^{(n+1)}\), the prediction of \(y_{k}^{(n+1)}\) is

\[g\left(x^{(n+1)},y^{(n+1)},k\right)=y^{(n+1)}-\left\langle\theta,x^{(n+1)} \right\rangle=y^{(n+1)}+\left\langle w_{k}^{\text{gd}},x^{(n+1)}\right\rangle.\]

This concludes the proof.

### Reformulating the in-context loss

In this section, we will develop a re-formulation in-context loss, defined in (5), in a more convenient form (see Lemma 5).

For the entirety of this section, we assume that the transformer parameters \(\left\{P_{i},Q_{i}\right\}_{i=0}^{k}\) are of the form defined in (11), which we reproduce below for ease of reference:

\[P_{i}=\begin{bmatrix}B_{i}&0\\ 0&1\end{bmatrix},\quad Q_{i}=\begin{bmatrix}A_{i}&0\\ 0&0\end{bmatrix}.\]

Recall the update dynamics in (4), which we reproduce below:

\[Z_{i+1}=Z_{i}+\frac{1}{n}PZ_{i}MZ_{i}^{\top}QZ_{i},\] (56)

where \(M\) is a mask matrix given by \(M:=\begin{bmatrix}I_{n\times n}&0\\ 0&0\end{bmatrix}\). Let \(X_{k}\in\mathbb{R}^{d\times n+1}\) denote the first \(d\) rows of \(Z_{k}\) and let \(Y_{k}\in\mathbb{R}^{1\times n+1}\) denote the \((d+1)^{th}\) (last) row of \(Z_{k}\). Then the dynamics in (56) is equivalent to

\[X_{i+1}=X_{i}+\frac{1}{n}B_{i}X_{i}MX_{i}^{T}A_{i}X_{i}\] \[Y_{i+1}=Y_{i}+\frac{1}{n}Y_{i}MX_{i}^{T}A_{i}X_{i}.\] (57)

We present below an equivalent form for the in-context loss from (5):

**Lemma 5**.: _Let \(p_{x}\) and \(p_{w}\) denote distributions over \(\mathbb{R}^{d}\). Let \(x^{(1)},\dots,x^{(n+1)}\stackrel{{ iid}}{{\sim}}p_{x}\) and \(w_{\star}\sim p_{w}\). Let \(Z_{0}\in\mathbb{R}^{d+1\times n+1}\) be as defined in (1):_

\[Z_{0}=\begin{bmatrix}x^{(1)}&x^{(2)}&\cdots&x^{(n)}&x^{(n+1)}\\ y^{(1)}&y^{(2)}&\cdots&y^{(n)}&0\end{bmatrix}\in\mathbb{R}^{(d+1)\times(n+1)}.\]

_Let \(Z_{k}\) denote the output of the \((k-1)^{th}\) layer of the linear transformer (as defined in (56), initialized at \(Z_{0}\)). Let \(f\left(\left\{P_{i},Q_{i}\right\}_{i=0}^{k}\right)\) denote the in-context loss defined in (5), i.e._

\[f\left(\left\{P_{i},Q_{i}\right\}_{i=0}^{k}\right)=\mathbb{E}_{(Z_{0},w_{\star })}\Big{[}\Big{(}[Z_{k}]_{(d+1),(n+1)}+w_{\star}^{\top}x^{(n+1)}\Big{)}^{2} \Big{]}.\] (58)

_Let \(\overline{Z}_{0}\) be defined as_

\[\overline{Z}_{0}=\begin{bmatrix}x^{(1)}&x^{(2)}&\cdots&x^{(n)}&x^{(n+1)}\\ y^{(1)}&y^{(2)}&\cdots&y^{(n)}&y^{(n+1)}\end{bmatrix}\in\mathbb{R}^{(d+1) \times(n+1)},\]

_where \(y^{(n+1)}=\left\langle w_{\star},x^{(n+1)}\right\rangle\). Let \(\overline{Z}_{k}\) denote the output of the \((k-1)^{th}\) layer of the linear transformer (as defined in (56), initialized at \(\overline{Z}_{0}\)). Assume \(\left\{P_{i},Q_{i}\right\}_{i=0}^{k}\) be of the form in (11). Then the loss in (5) has the equivalent form_

\[f\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k}\right):=f\left(\left\{P_{i},Q_{i} \right\}_{i=0}^{k}\right)=\mathbb{E}_{(\overline{Z}_{0},w_{\star})}\left[ \operatorname{Tr}\left(\left(I-M\right)\overline{Y}_{k+1}^{\top}\overline{Y} _{k+1}\left(I-M\right)\right)\right],\]

_where \(\overline{Y}_{k+1}\in\mathbb{R}^{1\times n+1}\) is the \((d+1)^{th}\) row of \(\overline{Z}_{k}\)._

Before proving Lemma 5, we first establish an intermediate result (Lemma 6 below). To facilitate discussion, let us define a function \(F_{X}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}\right)\) and \(F_{Y}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}\right)\) to be the outputs, after \(k\) layers of linear transformers respectively. I.e.

\[F_{X}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}\right) =X_{k+1}\] \[F_{Y}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0} \right) =Y_{k+1},\]

as defined in (57), given initialization \(X_{0},Y_{0}\).

We now prove a useful lemma showing that \(\left[Y_{0}\right]_{n+1}=y^{(n+1)}\) influences \(X_{i},Y_{i}\) in a very simple manner:

**Lemma 6**.: _Let \(X_{i},Y_{i}\) follow the dynamics in (57). Then_

1. \(\left[X_{i}\right]\) _is are independent of_ \(\left[Y_{0}\right]_{n+1}\)_._
2. _For_ \(j\neq n+1\)_,_ \(\left[Y_{i}\right]_{j}\) _is independent of_ \(\left[Y_{0}\right]_{n+1}\)_._
3. \(\left[Y_{i}\right]_{n+1}\) _depends additively on_ \(\left[Y_{0}\right]_{n+1}\)_._

_In other words, for \(C:=\left[0,0,0,\ldots,0,c\right]\in\mathbb{R}^{1\times(n+1)}\),_

\[1:F_{X}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}+C \right) =F_{X}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}\right)\] \[2+3:F_{Y}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}+ C\right) =F_{Y}\left(\left\{A_{i},B_{i}\right\}_{i=0}^{k},X_{0},Y_{0}\right)+C\]

Proof of Lemma 6.: The first and second items follows directly from observing that the dynamics for \(X_{i}\) and \(Y_{i}\) in (57) do not involve \(\left[Y_{i}\right]_{n+1}\), due to the effect of \(M\).

The third item again uses the fact that \(\left[Y_{i+1}-Y_{i}\right]_{n+1}\) does not depend on \(\left[Y_{i}\right]_{n+1}\). 

We are now ready to prove Lemma 5

Proof of Lemma 5.: Let \(Z_{0}\), \(Z_{k}\), \(\overline{Z}_{0}\), \(\overline{Z}_{k}\) be as defined in the lemma statement. Let \(\overline{X}_{k}\) and \(\overline{Y}_{k}\) denote first \(d\) rows and last row of \(\overline{Z}_{k}\). Then by Lemma 6, \(\overline{X}_{k+1}=X_{k+1}\) and \(\overline{Y}_{k+1}=Y_{k+1}+\left[0\quad 0\quad\cdots\quad 0\quad\left\langle w_{ \star},x^{(n+1)}\right\rangle\right]\). Therefore, (58) is equivalent to

\[\mathbb{E}_{(\overline{Z}_{0},w_{\star})}\Big{[}\big{(}[\overline {Z}_{k+1}]_{(d+1),(n+1)}\big{)}^{2}\Big{]}\] \[= \,\mathbb{E}_{(\overline{Z}_{0},w_{\star})}\Big{[}\big{(}[ \overline{Y}_{k+1}]_{(n+1)}\big{)}^{2}\Big{]}\] \[= \,\mathbb{E}_{(\overline{Z}_{0},w_{\star})}\left[\left\|\left(I-M \right)\overline{Y}_{k+1}^{\top}\right\|^{2}\right]\] \[= \,\mathbb{E}_{(\overline{Z}_{0},w_{\star})}\left[\operatorname{Tr }\left(\left(I-M\right)\overline{Y}_{k+1}^{\top}\overline{Y}_{k+1}\left(I-M \right)\right)\right].\]

This concludes the proof. 

## Appendix D Additional experimental results

In this section, we present a few addition experimental results. We first present in Figure 5 a visualization of learned weights \(A_{0},A_{1},A_{2}\) for the setting of Theorem 4. One can see that the weight pattern matches the stationary point analyzed in Theorem 4; hence, combining Figure 4 and Figure 5, we corroborate our results from Theorem 4. Interestingly, it appears that the transformer implements a tiny gradient step using \(X_{0}\) (as \(A_{0}\) is small), and a large gradient step using \(X_{2}\) (as \(A_{2}\) is large). We believe that this is due to \(X_{2}\) being better-conditioned than \(X_{1}\), due to the effects of \(B_{0},B_{1}\).

Figure 5: Visualization of learned weights for the setting of Theorem 4. One can see that the weight pattern matches the stationary point analyzed in Theorem 4.

We next present some additional experiments that investigates the properties of the learned predictors of various algorithms. First, we plot the **test losses against the number of examples provided in the prompt** ("the number of ICL examples"). We compare four different algorithms: (i) the predictor learned by a three-layered of linear transformer, (ii) three steps of GD, (iii) three steps of preconditioned GD, and (iv) the ordinary least-squared solution (OLS). For GD and preconditioned GD, the optimal stepsizes are found by gridsearch. For preconditioned GD, preconditioner is fixed to be \(\Sigma^{-1}\) for comparison. In all cases, the dimension \(d=5\), and for each \(N\), the linear Transformer is trained using Adam. The result is presented in Figure 6.

Lastly, in Figure 7, we plot the **test losses against the number of layer \(L\)** (or the number of steps in the case of gradient-based algorithms). For \(L=1,2,3,4\), we compare between (i) the predictor learned by \(L\)-linear transformer and (i) \(L\)-steps of GD, (ii) \(L\)-steps of preconditioned GD. Again, the optimal stepsize is found by gridsearch, and for preconditioned GD, the preconditioner is fixed to be \(\Sigma^{-1}\). In all cases, the dimension \(d=5\), and context length \(N=20\). The linear transformer is trained with Adam.

Figure 6: Test loss comparison between (i) the predictor learned by a three-layered of linear transformer, (ii) three steps of GD, (iii) three steps of preconditioned GD, and (iv) the ordinary least-squared solution (OLS).

Figure 7: Test loss comparison between (i) the predictor learned by a \(L\)-layered linear transformer and (i) \(L\)-steps of GD, (ii) \(L\)-steps of preconditioned GD, for \(L=1,2,3,4\).