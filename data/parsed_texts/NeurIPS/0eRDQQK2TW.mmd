# A Finite-Particle Convergence Rate for

Stein Variational Gradient Descent

 Jiaxin Shi

Stanford University

Stanford, CA 94305

jiaxins@stanford.edu

&Lester Mackey

Microsoft Research New England

Cambridge, MA 02474

lmackey@microsoft.com

Part of this work was done at Microsoft Research New England.

###### Abstract

We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with \(n\) particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order \(1/\sqrt{\log\log n}\) rate. We suspect that the dependence on \(n\) can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements.

## 1 Introduction

Stein variational gradient descent [SVGD, 18] is an algorithm for approximating a target probability distribution \(P\) on \(\mathbb{R}^{d}\) with a collection of \(n\) particles. Given an initial particle approximation \(\mu_{0}^{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{x_{i}}\) with locations \(x_{i}\in\mathbb{R}^{d}\), SVGD (Algorithm 1) iteratively evolves the particle locations to provide a more faithful approximation of the target \(P\) by performing optimization in the space of probability measures. SVGD has demonstrated encouraging results for a wide variety of inferential tasks, including approximate inference [18, 28, 27], generative modeling [26, 13], and reinforcement learning [11, 21].

Despite the popularity of SVGD, relatively little is known about its approximation quality. A first analysis by Liu [17, Thm. 3.3] showed that _continuous SVGD_--that is, Algorithm 2 initialized with a continuous distribution \(\mu_{0}^{\infty}\) in place of the discrete particle approximation \(\mu_{0}^{n}\)--converges to \(P\) in kernel Stein discrepancy [KSD, 4, 19, 9]. KSD convergence is also known to imply weak convergence [9, 3, 12, 1] and Wasserstein convergence [14] under various conditions on the target \(P\) and the SVGD kernel \(k\). Follow-up work by Korba et al. [15], Salim et al. [22], Sun et al. [24] sharpened the result of Liu with path-independent constants, weaker smoothness conditions, and explicit rates of convergence. In addition, Duncan et al. [6] analyzed the continuous-time limit of continuous SVGD to provide conditions for exponential convergence. However, each of these analyses applies only to continuous SVGD and not to the finite-particle algorithm used in practice.

To bridge this gap, Liu [17, Thm. 3.2] showed that \(n\)-particle SVGD converges to continuous SVGD in bounded-Lipschitz distance but only under boundedness assumptions violated by most applications of SVGD. To provide a more broadly applicable proof of convergence, Gorham et al. [10, Thm. 7] showed that \(n\)-particle SVGD converges to continuous SVGD in \(1\)-Wasserstein distance under assumptions commonly satisfied in SVGD applications. However, both convergence results are asymptotic, providing neither explicit error bounds nor rates of convergence. Korba et al. [15, Prop. 7] explicitly bounded the expected squared Wasserstein distance between \(n\)-particle and continuous SVGD but only under the assumption of bounded \(\nabla\log p\), an assumption that rules out all stronglylog concave or dissipative distributions and all distributions for which the KSD is currently known to control weak convergence [9; 3; 12; 1]. In addition, Korba et al. [15] do not provide a unified bound for the convergence of \(n\)-particle SVGD to \(P\) and ultimately conclude that "the convergence rate for SVGD using [\(\mu_{0}^{n}\)] remains an open problem." The same open problem was underscored in the later work of Salim et al. [22], who write "an important and difficult open problem in the analysis of SVGD is to characterize its complexity with a finite number of particles."

In this work, we derive the first unified convergence bound for finite-particle SVGD to its target. To achieve this, we first bound the \(1\)-Wasserstein discretization error between finite-particle and continuous SVGD under assumptions commonly satisfied in SVGD applications and compatible with KSD weak convergence control (see Theorem 1). We next bound KSD in terms of \(1\)-Wasserstein distance and SVGD moment growth to explicitly control KSD discretization error in Theorem 2. Finally, Theorem 3 combines our results with the established KSD analysis of continuous SVGD to arrive at an explicit KSD error bound for \(n\)-particle SVGD.

## 2 Notation and Assumptions

Throughout, we fix a nonnegative step size sequence \((\epsilon_{s})_{s\geq 0}\), a target distribution \(P\) in the set \(\mathcal{P}_{1}\) of probability measures on \(\mathbb{R}^{d}\) with integrable first moments, and a _reproducing kernel_\(k\)--a symmetric positive definite function mapping \(\mathbb{R}^{d}\times\mathbb{R}^{d}\) to \(\mathbb{R}\)--with reproducing kernel Hilbert space (RKHS) \(\mathcal{H}\) and product RKHS \(\mathcal{H}^{d}\triangleq\bigotimes_{i=1}^{d}\mathcal{H}\)[2]. We will use the terms "kernel" and "reproducing kernel" interchangeably. For all \(\mu,\nu\in\mathcal{P}_{1}\), we let \(\Gamma(\mu,\nu)\) be the set of all _couplings_ of \(\mu\) and \(\nu\), i.e., joint probability distributions \(\gamma\) over \(\mathcal{P}_{1}\times\mathcal{P}_{1}\) with \(\mu\) and \(\nu\) as marginal distributions of the first and second variable respectively. We further let \(\mu\otimes\nu\) denote the independent coupling, the distribution of \((X,Z)\) when \(X\) and \(Z\) are drawn independently from \(\mu\) and \(\nu\) respectively. With this notation in place we define the \(1\)-Wasserstein distance between \(\mu,\nu\in\mathcal{P}_{1}\) as \(W_{1}(\mu,\nu)\triangleq\inf_{\gamma\in\Gamma(\mu,\nu)}\mathbb{E}_{(X,Z)\sim \gamma}[\left\|Z-X\right\|_{2}]\) and introduce the shorthand \(m_{\mu,x^{*}}\triangleq\mathbb{E}_{\mu}[\left\|\cdot-x^{*}\right\|\|_{2}\) for each \(x^{*}\in\mathbb{R}^{d}\), \(m_{\mu,P}\triangleq\mathbb{E}_{(X,Z)\sim\mu\otimes P}[\left\|X-Z\right\|_{2}]\), and \(M_{\mu,P}\triangleq\mathbb{E}_{(X,Z)\sim\mu\otimes P}[\left\|X-Z\right\|_{2} ^{2}]\). We further define the Kullback-Leibler (KL) divergence as \(\text{KL}(\mu\|\nu)\triangleq\mathbb{E}_{\mu}[\log(\frac{d\mu}{d\nu})]\) when \(\mu\) is absolutely continuous with respect to \(\nu\) (denoted by \(\mu\ll\nu\)) and as \(\infty\) otherwise.

Our analysis will make use of the following assumptions on the SVGD kernel and target distribution.

**Assumption 1** (Lipschitz, mean-zero score function).: _The target distribution \(P\in\mathcal{P}_{1}\) has a differentiable density \(p\) with an \(L\)-Lipschitz score function \(s_{p}\triangleq\nabla\log p\), i.e., \(\left\|s_{p}(x)-s_{p}(y)\right\|_{2}\leq L\|x-y\|_{2}\) for all \(x,y\in\mathbb{R}^{d}\). Moreover, \(\mathbb{E}_{P}[s_{p}]=0\) and \(s_{p}(x^{*})=0\) for some \(x^{*}\in\mathbb{R}^{d}\)._

**Assumption 2** (Bounded kernel derivatives).: _The kernel \(k\) is twice differentiable and \(\sup_{x,y\in\mathbb{R}^{d}}\max(|k(x,y)|,\|\nabla_{x}k(x,y)\|_{2},\left\| \nabla_{y}\nabla_{x}k(x,y)\right\|_{\text{op}},\left\|\nabla_{x}^{2}k(x,y) \right\|_{\text{op}})\leq\kappa_{1}^{2}\) for \(\kappa_{1}>0\). Moreover, for all \(i,j\in\{1,2,\ldots,d\}\), \(\sup_{x\in\mathbb{R}^{d}}\nabla_{y_{i}}\nabla_{y_{j}}\nabla_{x_{i}}\nabla_{ x_{j}}k(x,y)|_{y=x}\leq\kappa_{2}^{2}\) for \(\kappa_{2}>0\)._

**Assumption 3** (Decaying kernel derivatives).: _The kernel \(k\) is differentiable and admits a \(\gamma\in\mathbb{R}\) such that, for all \(x,y\in\mathbb{R}^{d}\) satisfying \(\left\|x-y\right\|_{2}\geq 1\),_

\[\left\|\nabla_{x}k(x,y)\right\|_{2}\leq\gamma/\left\|x-y\right\|_{2}.\]Assumptions 1, 2, and 3 are both commonly invoked and commonly satisfied in the literature. For example, the Lipschitz score assumption is consistent with prior SVGD convergence analyses [17, 15, 22] and, by Gorham and Mackey [8, Prop. 1], the score \(s_{p}\) is mean-zero under the mild integrability condition \(\mathbb{E}_{X\sim P}[\left\|s_{p}(X)\right\|_{2}]<\infty\). The bounded and decaying derivative assumptions have also been made in prior analyses [15, 9] and, as we detail in Appendix A, are satisfied by the kernels most commonly used in SVGD, like the Gaussian and inverse multiquadric (IMQ) kernels. Notably, in these cases, the bounds \(\kappa_{1}\) and \(\kappa_{2}\) are independent of the dimension \(d\).

To leverage the continuous SVGD convergence rates of Salim et al. [22], we additionally assume that the target \(P\) satisfies Talagrand's \(T_{1}\) inequality [25, Def. 22.1]. Remarkably, Villani [25, Thm. 22.10] showed that Assumption \(4\) is _equivalent_ to \(P\) being a sub-Gaussian distribution. Hence, this mild assumption holds for all strongly log concave \(P\)[23, Def. 2.9], all \(P\) satisfying the log Sobolev inequality [25, Thm. 22.17], and all _distantly dissipative_\(P\) for which KSD is known to control weak convergence [9, Def. 4].

**Assumption 4** (Talagrand's \(T_{1}\) inequality [25, Def. 22.1]).: _For \(P\in\mathcal{P}_{1}\), there exists \(\lambda>0\) such that, for all \(\mu\in\mathcal{P}_{1}\),_

\[W_{1}(\mu,P)\leq\sqrt{2\mathrm{KL}(\mu\|P)/\lambda}.\]

Finally we make use of the following notation specific to the SVGD algorithm.

**Definition 1** (Stein operator).: _For any differentiable vector-valued function \(g:\mathbb{R}^{d}\to\mathbb{R}^{d}\), the Langevin Stein operator [8] for \(P\) satisfying Assumption 1 is defined by_

\[(\mathcal{T}_{P}g)(x)\triangleq\langle s_{p}(x),g(x)\rangle+\nabla\cdot g(x) \quad\text{for all}\quad x\in\mathbb{R}^{d}.\]

**Definition 2** (Vector-valued Stein operator).: _For any differentiable function \(h:\mathbb{R}^{d}\to\mathbb{R}\), the vector-valued Langevin Stein operator [19] for \(P\) satisfying Assumption 1 is defined by_

\[(\mathcal{A}_{P}h)(x)\triangleq s_{p}(x)h(x)+\nabla h(x)\quad\text{for all} \quad x\in\mathbb{R}^{d}.\]

**Definition 3** (SVGD transport map and pushforward).: _The SVGD transport map [18] for a target \(P\) satisfying Assumption 1, a kernel \(k\) satisfying Assumption 2, a step size \(\epsilon\geq 0\), and an approximating distribution \(\mu\in\mathcal{P}_{1}\) takes the form_

\[T_{\mu,\epsilon}(x)\triangleq x+\epsilon\mathbb{E}_{X\sim\mu}[(\mathcal{A}_{P }k(\cdot,x))(X)]\quad\text{for all}\quad x\in\mathbb{R}^{d}.\]

_Moreover, the SVGD pushforward \(\Phi_{\epsilon}(\mu)\) represents the distribution of \(T_{\mu,\epsilon}(X)\) when \(X\sim\mu\)._

**Definition 4** (Kernel Stein discrepancy).: _The Langevin kernel Stein discrepancy [KSD, 4, 19, 9] for \(P\) satisfying Assumption 1, \(k\) satisfying Assumption 2, and measures \(\mu,\nu\in\mathcal{P}_{1}\) is given by_

\[\text{KSD}_{P}(\mu,\nu)\triangleq\sup_{\left\|g\right\|_{\mathbb{R}^{d}\leq 1 }}\mathbb{E}_{\mu}[\mathcal{T}_{P}g]-\mathbb{E}_{\nu}[\mathcal{T}_{P}g].\]

Notably, the KSD so-defined is symmetric in its two arguments and satisfies the triangle inequality.

**Lemma 1** (KSD symmetry and triangle inequality).: _Under Definition 4, for all \(\mu,\nu,\pi\in\mathcal{P}_{1}\),_

\[\text{KSD}_{P}(\mu,\nu)=\text{KSD}_{P}(\nu,\mu)\quad\text{and}\quad\text{KSD }_{P}(\mu,\nu)\leq\text{KSD}_{P}(\mu,\pi)+\text{KSD}_{P}(\pi,\nu).\]

Proof.: Fix any \(\mu,\nu,\pi\in\mathcal{P}_{1}\). For symmetry, we note that \(g\in\mathcal{H}^{d}\Leftrightarrow f=-g\in\mathcal{H}^{d}\), so

\[\text{KSD}_{P}(\mu,\pi)=\sup_{\left\|g\right\|_{\mathcal{H}^{d}\leq 1}} \mathbb{E}_{\mu}[\mathcal{T}_{P}g]-\mathbb{E}_{\pi}[\mathcal{T}_{P}g]=\sup_{ \left\|f\right\|_{\mathcal{H}^{d}\leq 1}}\mathbb{E}_{\pi}[\mathcal{T}_{P}f]- \mathbb{E}_{\mu}[\mathcal{T}_{P}f]=\text{KSD}_{P}(\pi,\mu).\]

To establish the triangle inequality, we write

\[\text{KSD}_{P}(\mu,\nu) =\sup_{\left\|g\right\|_{\mathcal{H}^{d}\leq 1}}\mathbb{E}_{\mu}[ \mathcal{T}_{P}g]-\mathbb{E}_{\pi}[\mathcal{T}_{P}g]+\mathbb{E}_{\pi}[ \mathcal{T}_{P}g]-\mathbb{E}_{\nu}[\mathcal{T}_{P}g]\] \[\leq\sup_{\left\|g\right\|_{\mathcal{H}^{d}\leq 1}}(\mathbb{E}_{\mu}[ \mathcal{T}_{P}g]-\mathbb{E}_{\pi}[\mathcal{T}_{P}g])+\sup_{\left\|h\right\|_{ \mathcal{H}^{d}\leq 1}}(\mathbb{E}_{\pi}[\mathcal{T}_{P}h]-\mathbb{E}_{\nu}[ \mathcal{T}_{P}h])\] \[\leq\text{KSD}_{P}(\mu,\pi)+\text{KSD}_{P}(\pi,\nu).\]

## 3 Wasserstein Discretization Error of SVGD

Our first main result concerns the discretization error of SVGD and shows that \(n\)-particle SVGD remains close to its continuous SVGD limit whenever the step size sum \(b_{r-1}=\sum_{s=0}^{r-1}\epsilon_{s}\) is sufficiently small.

**Theorem 1** (Wasserstein discretization error of SVGD).: _Suppose Assumptions 1, 2, and 3 hold. For any \(\mu_{0}^{n},\mu_{0}^{\infty}\in\mathcal{P}_{1}\), the outputs \(\mu_{r}^{n}=\text{SVGD}(\mu_{0}^{n},r)\) and \(\mu_{r}^{\infty}=\text{SVGD}(\mu_{0}^{\infty},r)\) of Algorithm 2 satisfy_

\[\log\biggl{(}\frac{W_{1}(\mu_{r}^{n},\mu_{r}^{\infty})}{W_{1}(\mu_{0}^{n},\mu_ {0}^{\infty})}\biggr{)}\leq b_{r-1}(A+B\exp(Cb_{r-1}))\]

_for \(b_{r-1}\triangleq\sum_{s=0}^{r-1}\epsilon_{s}\), \(A=(c_{1}+c_{2})(1+m_{P,x^{*}})\), \(B=c_{1}m_{\mu_{0}^{\infty},P}+c_{2}m_{\mu_{0}^{\infty},P}\), \(C=\kappa_{1}^{2}(3L+1)\), \(c_{1}=\max(\kappa_{1}^{2}L,\kappa_{1}^{2})\), and \(c_{2}=\kappa_{1}^{2}(L+1)+L\max(\gamma,\kappa_{1}^{2})\)._

We highlight that Theorem 1 applies to _any_\(\mathcal{P}_{1}\) initialization of SVGD: the initial particles supporting \(\mu_{0}^{n}\) could, for example, be drawn i.i.d. from a convenient auxiliary distribution \(\mu_{0}^{\infty}\) or even generated deterministically from some quadrature rule. To marry this result with the continuous SVGD convergence bound of Section 5, we will ultimately require \(\mu_{0}^{\infty}\) to be a continuous distribution with finite \(\text{KL}(\mu_{0}^{\infty}\|P)\). Hence, our primary desideratum for SVGD initialization is that \(\mu_{0}^{n}\) have small Wasserstein distance to some \(\mu_{0}^{\infty}\) with \(\text{KL}(\mu_{0}^{\infty}\|P)<\infty\). Then, by Theorem 1, the SVGD discretization error \(W_{1}(\mu_{r}^{n},\mu_{r}^{\infty})\) will remain small whenever the step size sum is not too large.

The proof of Theorem 1 in Section 6 relies on two lemmas. The first, due to Gorham et al. [10], shows that the one-step SVGD pushforward map \(\Phi_{\epsilon}\) (Definition 3) is pseudo-Lipschitz with respect to the \(1\)-Wasserstein distance2 whenever the score function \(\nabla\log p\) and kernel \(k\) fulfill a commonly-satisfied pseudo-Lipschitz condition. Here, for any \(g:\mathbb{R}^{d}\to\mathbb{R}^{d}\), we define the Lipschitz constant \(\operatorname{Lip}(g)\triangleq\sup_{x,z\in\mathbb{R}^{d}}\lVert g(x)-g(z) \rVert_{2}/\lVert x-z\rVert_{2}\).

Footnote 2: We say a map \(\Phi:\mathcal{P}_{1}\to\mathcal{P}_{1}\) is _pseudo-Lipschitz_ with respect to \(1\)-Wasserstein distance if, for a constant \(C_{\Phi}\in\mathbb{R}\), some \(x^{*}\in\mathbb{R}^{d}\), and all \(\mu,\nu\in\mathcal{P}_{1}\), \(W_{1}(\Phi(\mu),\Phi(\nu))\leq W_{1}(\mu,\nu)\left(1+m_{\mu,x^{*}}+m_{\nu,x^{*} }\right)C_{\Phi}\).

**Lemma 2** (Wasserstein pseudo-Lipschitzness of SVGD [10, Lem. 12]).: _For \(P\) satisfying Assumption 1, suppose that the following pseudo-Lipschitz bounds hold_

\[\operatorname{Lip}(s_{p}(x)k(x,\cdot)+\nabla_{x}k(x,\cdot)) \leq c_{1}(1+\lVert x-x^{*}\rVert_{2}),\] \[\operatorname{Lip}(s_{p}k(\cdot,z)+\nabla_{x}k(\cdot,z)) \leq c_{2}(1+\lVert z-x^{*}\rVert_{2}).\]

_for some constants \(c_{1},c_{2}\in\mathbb{R}\) and all \(x,z\in\mathbb{R}^{d}\). Then, for any \(\mu,\nu\in\mathcal{P}_{1}\),_

\[W_{1}(\Phi_{\epsilon}(\mu),\Phi_{\epsilon}(\nu))\leq W_{1}(\mu,\nu)(1+\epsilon c _{\mu,\nu}),\]

_where \(\Phi_{\epsilon}\) is the one-step SVGD pushforward (Definition 3) and \(c_{\mu,\nu}=c_{1}(1+m_{\mu,x^{*}})+c_{2}(1+m_{\nu,x^{*}})\)._

In Section 6, we will show that, under Assumptions 1, 2, and 3, the preconditions of Lemma 2 are fulfilled with \(c_{1}\) and \(c_{2}\) exactly as in Theorem 1. The second lemma, proved in Section 7, controls the growth of the first and second absolute moments under SVGD.

**Lemma 3** (SVGD moment growth).: _Suppose Assumptions 1 and 2 hold, and let \(C=\kappa_{1}^{2}(3L+1)\). Then the SVGD output \(\mu_{r}\) of Algorithm 2 with \(b_{r-1}\triangleq\sum_{s=0}^{r-1}\epsilon_{s}\) satisfies_

\[m_{\mu_{r},x^{*}}-m_{P,x^{*}}\leq m_{\mu_{r},P} \leq m_{\mu_{0},P}\prod_{s=0}^{r-1}(1+\epsilon_{s}C)\leq m_{\mu_{0},P}\exp(Cb_{r-1}),\] \[M_{\mu_{r},P} \leq M_{\mu_{0},P}\prod_{s=0}^{r-1}(1+\epsilon_{s}C)^{2}\leq M_{ \mu_{0},P}\exp(2Cb_{r-1}).\]

The key to the proof of Lemma 3 is that we show the norm of any SVGD update, i.e., \(\lVert T_{\mu,\epsilon}(x)-x\rVert_{2}\), is controlled by \(m_{\mu,P}\), the first absolute moment of \(\mu\) measured against \(P\). This is mainly due to the Lipschitzness of the score function \(s_{p}\) and our assumptions on the boundedness of the kernel and its derivatives. Then, we can use the result to control the growth of \(m_{\mu_{r},P}\) across iterations since \(m_{\mu_{r+1},P}=\mathbb{E}_{(X,Z)\sim\mu_{r}\otimes P}\lVert T_{\mu_{r}, \epsilon_{r}}(X)-Z\rVert_{2}\rVert_{2}\). The same strategy applies to the second absolute moment \(M_{\mu,P}\). The proof of Theorem 1 then follows directly from Lemma 2 where we plug in the first moment bound of Lemma 3.

KSD Discretization Error of SVGD

Our next result translates the Wasserstein error bounds of Theorem 1 into KSD error bounds.

**Theorem 2** (KSD discretization error of SVGD).: _Suppose Assumptions 1, 2, and 3 hold. For any \(\mu_{0}^{n},\mu_{0}^{\infty}\in\mathcal{P}_{1}\), the outputs of Algorithm 2, \(\mu_{r}^{n}=\text{SVGD}(\mu_{0}^{n},r)\) and \(\mu_{r}^{\infty}=\text{SVGD}(\mu_{0}^{\infty},r)\), satisfy_

\[\text{KSD}_{P}(\mu_{r}^{n},\mu_{r}^{\infty}) \leq(\kappa_{1}L+\kappa_{2}d)w_{0,n}\exp(b_{r-1}(A+B\exp(Cb_{r-1} )))\] \[+\kappa_{1}d^{1/4}L\sqrt{2M_{\mu_{0}^{\infty},P}w_{0,n}}\exp(b_{r -1}(2C+A+B\exp(Cb_{r-1}))/2)\]

_for \(w_{0,n}\triangleq W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})\) and \(A,B,C\) defined as in Theorem 1._

Our proof of Theorem 2 relies on the following lemma, proved in Section 8, that shows that the KSD is controlled by the \(1\)-Wasserstein distance.

**Lemma 4** (KSD-Wasserstein bound).: _Suppose Assumptions 1 and 2 hold. For any \(\mu,\nu\in\mathcal{P}_{1}\),_

\[\text{KSD}_{P}(\mu,\nu)\leq(\kappa_{1}L+\kappa_{2}d)W_{1}(\mu,\nu)+\kappa_{1} d^{1/4}L\sqrt{2M_{\nu,P}W_{1}(\mu,\nu)}.\]

Lemma 4 is proved in two steps. We first linearize \((\mathcal{T}_{P}g)(x)\) in the KSD definition through the Lipschitzness of \(s_{p}\) and the boundedness and Lipschitzness of RKHS functions. Then, we assign a 1-Wasserstein optimal coupling of \((\mu,\nu)\) to obtain the Wasserstein bound on the right.

Proof of Theorem 2The result follows directly from Lemma 4, the second moment bound of Lemma 3, and Theorem 1. 

## 5 A Finite-particle Convergence Rate for SVGD

To establish our main SVGD convergence result, we combine Theorems 1 and 2 with the following descent lemma for continuous SVGD error due to Salim et al. [22] which shows that continuous SVGD decreases the KL divergence to \(P\) and drives the KSD to \(P\) to zero.

**Lemma 5** (Continuous SVGD descent lemma [22, Thm. 3.2]).: _Suppose Assumptions 1, 2, and 4 hold, and consider the outputs \(\mu_{r}^{\infty}=\text{SVGD}(\mu_{0}^{\infty},r)\) and \(\mu_{r+1}^{\infty}=\text{SVGD}(\mu_{0}^{\infty},r+1)\) of Algorithm 2 with \(\mu_{0}^{\infty}\ll P\). If \(\max_{0\leq s\leq r}\epsilon_{s}\leq R_{\alpha,2}\) for some \(\alpha>1\) and_

\[R_{\alpha,p}\!\triangleq\!\min\!\left(\tfrac{p}{\kappa_{1}^{2}(L+\alpha^{2})},(\alpha-1)(1+Lm_{\mu_{0}^{\infty},x^{\star}}+2L\sqrt{\tfrac{2}{\lambda}\text {KL}(\mu_{0}^{\infty}\|P)})\right)\text{ for }p\in\{1,2\},\] (1)

_then_

\[\text{KL}(\mu_{r+1}^{\infty}\|P)-\text{KL}(\mu_{r}^{\infty}\|P)\leq-\epsilon _{r}\Big{(}1-\tfrac{\kappa_{1}^{2}(L+\alpha^{2})}{2}\epsilon_{r}\Big{)}\text {KSD}_{P}(\mu_{r}^{\infty},P)^{2}.\] (2)

By summing the result (2) over \(r\in\{0,\ldots,t\}\), we obtain the following corollary.

**Corollary 1**.: _Under the assumptions and notation of Lemma 5, suppose \(\max_{0\leq r\leq t}\epsilon_{r}\leq R_{\alpha,1}\) for some \(\alpha>1\), and let \(\pi_{r}\triangleq\tfrac{c(\epsilon_{r})}{\sum_{r=0}^{r}c(\epsilon_{r})}\) for \(c(\epsilon)\triangleq\epsilon\Big{(}1-\tfrac{\kappa_{1}^{2}(L+\alpha^{2})}{2} \epsilon\Big{)}\). Since \(\tfrac{\epsilon}{2}\leq c(\epsilon)<\epsilon\), we have_

\[\sum_{r=0}^{t}\pi_{r}\text{KSD}_{P}(\mu_{r}^{\infty},P)^{2}\leq\tfrac{1}{\sum_ {r=0}^{r}c(\epsilon_{r})}\text{KL}(\mu_{0}^{\infty}\|P)\leq\tfrac{2}{\sum_{r=0 }^{r}\epsilon_{r}}\text{KL}(\mu_{0}^{\infty}\|P).\]

Finally, we arrive at our main result that bounds the approximation error of \(n\)-particle SVGD in terms of the chosen step size sequence and the initial discretization error \(W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})\).

**Theorem 3** (KSD error of finite-particle SVGD).: _Suppose Assumptions 1, 2, 3, and 4 hold, fix any \(\mu_{0}^{\infty}\ll P\) and \(\mu_{0}^{n}\in\mathcal{P}_{1}\), and let \(w_{0,n}\triangleq W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})\). If \(\max_{0\leq r<t}\epsilon_{r}\leq\epsilon_{t}\triangleq R_{\alpha,1}\)3 for some \(\alpha>1\) and \(R_{\alpha,1}\) defined in Lemma 5, then the Algorithm 2 outputs \(\mu_{r}^{n}=\text{SVGD}(\mu_{0}^{n},r)\) satisfy_

Footnote 3: Note that the value assigned to \(\epsilon_{t}\) does not have any impact on the algorithm that generates \(\mu_{r}^{n}\) when \(r\leq t\).

\[\min_{0\leq r\leq t}\text{KSD}_{P}(\mu_{r}^{n},P)\leq\sum_{r=0}^{t}\pi_{r} \text{KSD}_{P}(\mu_{r}^{n},P)\leq a_{t-1}+\sqrt{\tfrac{2}{R_{\alpha,1}+b_{t-1}} \text{KL}(\mu_{0}^{\infty}\|P)},\] (3)

_for \(\pi_{r}\) as defined in Lemma 5, \((A,B,C)\) as defined in Theorem 1, \(b_{t-1}\triangleq\sum_{r=0}^{t}\epsilon_{r}\), and_

\[a_{t-1} \triangleq(\kappa_{1}L+\kappa_{2}d)w_{0,n}\exp(b_{t-1}(A+B\exp(Cb_{ t-1})))\] (4) \[+\kappa_{1}d^{1/4}L\sqrt{2M_{\mu_{0}^{\infty},P}w_{0,n}}\exp(b_{t- 1}(2C+A+B\exp(Cb_{t-1}))/2).\]Proof.: By the triangle inequality (Lemma 1) and Theorem 2 we have

\[|\text{KSD}_{P}(\mu_{r}^{n},P)-\text{KSD}_{P}(\mu_{r}^{\infty},P)|\leq\text{KSD}_ {P}(\mu_{r}^{n},\mu_{r}^{\infty})\leq a_{r-1}\]

for each \(r\). Therefore

\[\sum_{r=0}^{t}\pi_{r}(\text{KSD}_{P}(\mu_{r}^{n},P)-a_{r-1})^{2}\leq\sum_{r=0}^ {t}\pi_{r}\text{KSD}_{P}(\mu_{r}^{\infty},P)^{2}\leq\tfrac{2}{R_{\alpha,1}+b_{ t-1}}\text{KL}(\mu_{0}^{\infty}\|P),\] (5)

where the last inequality follows from Corollary 1. Moreover, by Jensen's inequality,

\[\sum_{r=0}^{t}\pi_{r}(\text{KSD}_{P}(\mu_{r}^{n},P)-a_{r-1})^{2}\geq\Big{(} \sum_{r=0}^{t}\pi_{r}\text{KSD}_{P}(\mu_{r}^{n},P)-\sum_{r=0}^{t}\pi_{r}a_{r-1 }\Big{)}^{2}.\] (6)

Combining (5) and (6), we have

\[\sum_{r=0}^{t}\pi_{r}\text{KSD}_{P}(\mu_{r}^{n},P)\leq\sum_{r=0}^{t}\pi_{r}a_ {r-1}+\sqrt{\tfrac{2}{R_{\alpha,1}+b_{t-1}}\text{KL}(\mu_{0}^{\infty}\|P)}.\]

We finish the proof by noticing that \(\sum_{r=0}^{t}\pi_{r}a_{r-1}\leq\max_{0\leq r\leq t}a_{r-1}=a_{t-1}\). 

The following corollary, proved in Appendix B, provides an explicit SVGD convergence bound and rate by choosing the step size sum to balance the terms on the right-hand side of (3). In particular, Corollary 2 instantiates an explicit SVGD step size sequence that drives the kernel Stein discrepancy to zero at an order \(1/\sqrt{\log\log(n)}\) rate.

**Corollary 2** (A finite-particle convergence rate for SVGD).: _Instantiate the notation and assumptions of Theorem 3, let \((\bar{w}_{0,n},\bar{A},\bar{B},\bar{C})\) be any upper bounds on \((w_{0,n},A,B,C)\) respectively, and define the growth functions_

\[\phi(w)\triangleq\log\log(e^{e}+\tfrac{1}{w})\quad\text{and}\quad\psi_{\bar{ B},\bar{C}}(x,y,\beta)\triangleq\tfrac{1}{\bar{C}}\log(\tfrac{1}{B}\max(\bar{B}, \tfrac{1}{\beta}\log\tfrac{1}{x}-y)).\]

_If the step size sum \(b_{t-1}=\sum_{r=0}^{t-1}\epsilon_{r}=s_{n}^{\star}\) for_

\[s_{n}^{\star} \triangleq\min\Big{(}\psi_{\bar{B},\bar{C}}\big{(}\bar{w}_{0,n} \sqrt{\phi(\bar{w}_{0,n})},\bar{A},\beta_{1}\big{)},\psi_{\bar{B},\bar{C}} \big{(}\bar{w}_{0,n}\,\phi(\bar{w}_{0,n}),\bar{A}+2\bar{C},\beta_{2}\big{)} \Big{)},\] \[\beta_{1} \triangleq\max\Big{(}1,\psi_{\bar{B},\bar{C}}\big{(}\bar{w}_{0,n} \sqrt{\phi(\bar{w}_{0,n})},\bar{A},1\big{)}\Big{)},\quad\text{and}\] \[\beta_{2} \triangleq\max\big{(}1,\psi_{\bar{B},\bar{C}}\big{(}\bar{w}_{0,n} \,\phi(\bar{w}_{0,n}),\bar{A}+2\bar{C},1\big{)}\big{)}\]

_then_

\[\min_{0\leq r\leq t}\text{KSD}_{P}(\mu_{r}^{n},P)\] \[\leq\begin{cases}(\kappa_{1}L+\kappa_{2}d)\bar{w}_{0,n}+\kappa_{1} d^{1/4}L\sqrt{2M_{\mu_{0}^{\infty},P}\bar{w}_{0,n}}+\sqrt{\tfrac{2}{R_{ \alpha,1}}\text{KL}(\mu_{0}^{\infty}\,\|P)}&\text{if }s_{n}^{\star}=0\\ \tfrac{(\kappa_{1}L+\kappa_{2}d)+\kappa d^{1/4}L\sqrt{2M_{\mu_{0}^{\infty},P}} }{\sqrt{\phi(\bar{w}_{0,n})}}+\sqrt{\tfrac{2\text{KL}(\mu_{0}^{\infty}\,\|P)} {R_{\alpha,1}+\frac{1}{\bar{C}}\log(\frac{1}{R}(\frac{\cos(1/(\alpha_{0},n, \,\vartheta(\bar{w}_{0,n})))}{\max(1,\gamma_{B},\bar{C}^{\prime}(\bar{w}_{0,n},0,1))}-\bar{A}-2\bar{C}))}}&\text{otherwise}\end{cases}\] (7) \[=O\bigg{(}\tfrac{1}{\sqrt{\log\log(e^{e}+\frac{1}{\bar{w}_{0,n}} )}}\bigg{)}.\] (8)

_If, in addition, \(\mu_{0}^{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{x_{i}}\) for \(x_{i}\overset{i\in d}{\sim}\mu_{0}^{\infty}\) with \(M_{\mu_{0}^{\infty}}\triangleq\mathbb{E}_{\mu_{0}^{\infty}}\big{[}\|\cdot\|_{2} ^{2}\big{]}<\infty\), then_

\[\bar{w}_{0,n}\triangleq\tfrac{M_{\mu_{0}^{\infty}}\log(n)^{[d-2]}}{\delta\,n^{ 1/(2\bar{\sigma}d)}}\geq w_{0,n}\] (9)

_with probability at least \(1-c\delta\) for a universal constant \(c>0\). Hence, with this choice of \(\bar{w}_{0,n}\),_

\[\min_{0\leq r\leq t}\text{KSD}_{P}(\mu_{r}^{n},P)=O\bigg{(}\tfrac{1}{\sqrt{\log \log(n\delta)}}\bigg{)}\]

_with probability at least \(1-c\delta\)._

Specifically, given any upper bounds \((\bar{w}_{0,n},\bar{A},\bar{B},\bar{C})\) on the quantities \((w_{0,n},A,B,C)\) defined in Theorem 3, Corollary 2 specifies a recommended step size sum \(s_{n}^{\star}\) to achieve an order \(1/\sqrt{\log\log(e^{e}+\frac{1}{\bar{w}_{0,n}})}\) rate of SVGD convergence in KSD. Several remarks are in order. First,the target step size sum \(s_{n}^{\star}\) is easily computed given \((\bar{w}_{0,n},\bar{A},\bar{B},\bar{C})\). Second, we note that the target \(s_{n}^{\star}\) can equal \(0\) when the initial Wasserstein upper bound \(\bar{w}_{0,n}\) is insufficiently small since \(\log(\frac{1}{B}\max(\bar{B},\frac{1}{\beta}\log\frac{1}{x}-y))=\log(\frac{ \bar{B}}{B})=0\) for small arguments \(x\). In this case, the setting \(b_{t-1}=0\) amounts to not running SVGD at all or, equivalently, to setting all step sizes to \(0\).

Third, Corollary 2 also implies a time complexity for achieving an order \(1/\sqrt{\log\log(e^{e}+\frac{1}{\bar{w}_{0,n}})}\) error rate. Recall that Theorem 3 assumes that \(\max_{0\leq r<t}\epsilon_{r}\leq R_{\alpha,1}\) for \(R_{\alpha,1}\) defined in (1). Hence, \(t^{\star}\triangleq\lceil s_{n}^{\star}/R_{\alpha,1}\rceil\) rounds of SVGD are necessary to achieve the recommended setting \(\sum_{r=0}^{t^{\star}-1}\epsilon_{r}=s_{n}^{\star}\) while also satisfying the constraints of Theorem 3. Moreover, \(t^{\star}\) rounds are also sufficient if each step size is chosen equal to \(s_{n}^{\star}/t^{\star}\). In addition, since \(s_{n}^{\star}=O(\log\log(e^{e}+\frac{1}{\bar{w}_{0,n}}))\), Corollary 2 implies that SVGD can deliver \(\min_{0\leq r<t^{\star}}\). KSD\({}_{P}(\mu_{r}^{n},P)\leq\Delta\) in \(t^{\star}=O(1/\Delta^{2})\) rounds. Since the computational complexity of each SVGD round is dominated by \(\Theta(n^{2})\) kernel gradient evaluations (i.e., evaluating \(\nabla_{x}k(x_{i},x_{j})\) for each pair of particles \((x_{i},x_{j})\)), the overall computational complexity to achieve the order \(1/\sqrt{\log\log(e^{e}+\frac{1}{\bar{w}_{0,n}})}\) error rate is \(O(n^{2}\lceil s_{n}^{\star}/R_{\alpha,1}\rceil)=O(n^{2}\log\log(e^{e}+\frac{1 }{\bar{w}_{0,n}}))\).

## 6 Proof of Theorem 1: Wasserstein discretization error of SVGD

In order to leverage Lemma 2, we first show that the pseudo-Lipschitzness conditions of Lemma 2 hold given our Assumptions 1 to 3. Recall that \(s_{p}\) is Lipschitz and \(x^{\star}\) satisfies \(s_{p}(x^{\star})=0\) by Assumption 1. Then, by the triangle inequality, the definition of \(\left\|\cdot\right\|_{\mathrm{op}}\), \(\left\|k(x,z)\right\|_{2}\leq\kappa_{1}^{2}\) and \(\left\|\nabla_{z}\nabla_{x}k(x,z)\right\|_{\mathrm{op}}\leq\kappa_{1}^{2}\) from Assumption 2, and Cauchy-Schwartz,

\[\mathrm{Lip}(s_{p}(x)k(x,\cdot)+\nabla_{x}k(x,\cdot))\] \[\leq\left\|s_{p}(x)-s_{p}(x^{\star})\right\|_{2}\left\|\nabla_{z} k(x,z)\right\|_{2}+\left\|\nabla_{z}\nabla_{x}k(x,z)\right\|_{\mathrm{op}}\] \[=\sup_{\left\|u\right\|_{2}\leq 1}(\left\|s_{p}(x)-s_{p}(x^{ \star})\right\|_{2}\left\|\nabla_{z}k(x,z)^{\top}u\right\|)+\left\|\nabla_{z} \nabla_{x}k(x,z)\right\|_{\mathrm{op}}\] \[\leq L\|x-x^{\star}\|_{2}\left\|\nabla_{z}k(x,z)\right\|_{2}+ \left\|\nabla_{z}\nabla_{x}k(x,z)\right\|_{\mathrm{op}}\] \[\leq\max(\kappa_{1}^{2}L,\kappa_{1}^{2})(1+\left\|x-x^{\star} \right\|_{2}).\]

Letting \(c_{1}=\max(\kappa_{1}^{2}L,\kappa_{1}^{2})\) and taking supremum over \(z\) proves the first pseudo-Lipschitzness condition. Similarly, we have

\[\mathrm{Lip}(s_{p}k(\cdot,z)+\nabla_{x}k(\cdot,z))\] \[\leq\sup_{x\in\mathbb{R}^{d}}\mathrm{Lip}(s_{p})|k(x,z)|+\left\| s_{p}(x)-s_{p}(x^{\star})\right\|_{2}\left\|\nabla_{x}k(x,z)\right\|_{2}+\left\| \nabla_{x}^{2}k(x,z)\right\|_{\mathrm{op}}\] \[\leq\kappa_{1}^{2}L+\sup_{x\in\mathbb{R}^{d}}L\|x-x^{\star}\|_{2} \left\|\nabla_{x}k(x,z)\right\|_{2}+\kappa_{1}^{2},\] (10)

where we used the Lipschitzness of \(s_{p}\) from Assumption 1 and \(\left|k(x,z)\right|\leq\kappa_{1}^{2},\left\|\nabla_{x}^{2}k(x,z)\right\|_{ \mathrm{op}}\leq\kappa_{1}^{2}\) from Assumption 2. Now we consider two cases separately: when \(\left\|x-z\right\|_{2}\geq 1\) and \(\left\|x-z\right\|_{2}<1\).

* Case 1: \(\left\|x-z\right\|_{2}\geq 1\). Recall that there exists \(\gamma>0\) such that \(\left\|\nabla_{x}k(x,z)\right\|_{2}\leq\gamma/\left\|x-z\right\|_{2}\) by Assumption 3. Then, using this together with the triangle inequality, we have \[\left\|x-x^{\star}\right\|_{2}\left\|\nabla_{x}k(x,z)\right\|_{2}\leq\gamma \frac{\left\|x-z\right\|_{2}+\left\|z-x^{\star}\right\|_{2}}{\left\|x-z\right\|_ {2}}\leq\gamma(1+\left\|z-x^{\star}\right\|_{2}).\] (11)
* Case 2: \(\left\|x-z\right\|_{2}<1\). Then, using \(\left\|\nabla_{x}k(x,z)\right\|_{2}\leq\kappa_{1}^{2}\) from Assumption 2 and by the triangle inequality, we have \[\left\|x-x^{\star}\right\|_{2}\left\|\nabla_{x}k(x,z)\right\|_{2}\leq\kappa_{1} ^{2}(\left\|x-z\right\|_{2}+\left\|z-x^{\star}\right\|_{2})<\kappa_{1}^{2}(1+ \left\|z-x^{\star}\right\|_{2}).\] (12) Combining (11) and (12) and using the triangle inequality, we get \[\left\|x-x^{\star}\right\|_{2}\left\|\nabla_{x}k(x,z)\right\|_{2}\leq\max( \gamma,\kappa_{1}^{2})(1+\left\|z-x^{\star}\right\|_{2}).\] (13) Plugging (13) back into (10), we can show the second pseudo-Lipschitzness condition holds for \(c_{2}=\max(\kappa_{1}^{2}(L+1)+L\max(\gamma,\kappa_{1}^{2}),L\max(\gamma, \kappa_{1}^{2}))=\kappa_{1}^{2}(L+1)+L\max(\gamma,\kappa_{1}^{2})\).

Now we have proved that both pseudo-Lipschitzness preconditions of Lemma 2 hold under our Assumptions 1 to 3. By repeated application of Lemma 2 and the inequality \((1+x)\leq e^{x}\), we have

\[W_{1}(\mu_{r+1}^{n},\mu_{r+1}^{\infty}) =W_{1}(\Phi_{\epsilon_{r}}(\mu_{r}^{n}),\Phi_{\epsilon_{r}}(\mu_{r} ^{\infty}))\leq(1+\epsilon_{r}D_{r})W(\mu_{r}^{n},\mu_{r}^{\infty})\] \[\leq W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})\prod_{s=0}^{r}(1+ \epsilon_{s}D_{s})\leq W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})\exp(\sum\nolimits_{ s=0}^{r}\epsilon_{s}D_{s})\] (14)

for \(D_{s}=c_{1}(1+m_{\mu_{r}^{n},x^{*}})+c_{2}(1+m_{\mu_{r}^{\infty},x^{*}})\).

Using the result from Lemma 3, we have

\[D_{s+1}\leq A+B\exp(Cb_{s})\]

for \(A=(c_{1}+c_{2})(1+m_{P,x^{*}})\), \(B=c_{1}m_{\mu_{0}^{\infty},P}+c_{2}m_{\mu_{0}^{\infty},P}\), and \(C=\kappa_{1}^{2}(3L+d)\). Therefore

\[\sum_{s=0}^{r}\epsilon_{s}D_{s}\leq\max_{0\leq s\leq r}D_{s}\sum_{s=0}^{r} \epsilon_{s}\leq b_{r}(A+B\exp(Cb_{r-1}))\leq b_{r}(A+B\exp(Cb_{r})).\]

Plugging this back into (14) proves the result.

## 7 Proof of Lemma 3: SVGD moment growth

From Assumption 2 we know \(|k(y,x)|\leq\kappa_{1}^{2}\) and \(\big{\|}\nabla_{y}^{2}k(y,x)\big{\|}_{op}\leq\kappa_{1}^{2}\). The latter implies

\[\|\nabla_{y}k(y,x)-\nabla_{z}k(z,x)\|_{2}\leq\kappa_{1}^{2}\|y-z\|_{2}.\]

Recall that \(s_{p}\) is Lipschitz and satisfies \(\mathbb{E}_{P}[s_{p}(\cdot)]=0\) by Assumption 1. Let \(\mu\) be any probability measure. Using the above results, Jensen's inequality and the fact that \(\mathbb{E}_{Z\sim P}[(\mathcal{A}_{P}k(\cdot,x))(Z)]=0\), we have

\[\|T_{\mu,\epsilon}(x)-x\|_{2}\leq\epsilon\|\mathbb{E}_{X\sim\mu}[ (\mathcal{A}_{P}k(\cdot,x))(X)]\|_{2}\] \[=\epsilon\|\mathbb{E}_{X\sim\mu}[(\mathcal{A}_{P}k(\cdot,x))(X)]- \mathbb{E}_{Z\sim P}[(\mathcal{A}_{P}k(\cdot,x))(Z)]\|_{2}\] \[=\epsilon\|\mathbb{E}_{(X,Z)\sim\mu\otimes P}[k(Z,x)(s_{p}(X)-s_ {p}(Z))+(k(X,x)-k(Z,x))(s_{p}(X)-\mathbb{E}_{P}[s_{p}(\cdot)])\] \[\quad+(\nabla_{X}k(X,x)-\nabla_{Z}k(Z,x))]\|_{2}\] \[\leq\epsilon\mathbb{E}_{(X,Z)\sim\mu\otimes P}[|k(Z,x)|\|_{Sp}(X )-s_{p}(Z)\|_{2}+(|k(X,x)|+|k(Z,x)|)\|s_{p}(X)-\mathbb{E}_{Y\sim P}[s_{p}(Y)] \|_{2}\] \[\quad+\|\nabla_{X}k(X,x)-\nabla_{Z}k(Z,x)\|_{2}]\] \[\leq\epsilon\mathbb{E}_{(X,Z)\sim\mu\otimes P}[\kappa_{1}^{2}(L+1 )\|X-Z\|_{2}]+\epsilon\cdot 2\kappa_{1}^{2}L\mathbb{E}_{(X,Y)\sim\mu\otimes P}[\|X-Y\|_{2}]\] \[=\epsilon\kappa_{1}^{2}(3L+1)\mathbb{E}_{(X,Z)\sim\mu\otimes P}[ \|X-Z\|_{2}]\] \[=\epsilon Cm_{\mu,P}.\] (15)

The last step used the definitions \(m_{\mu,P}\triangleq\mathbb{E}_{(X,Z)\sim\mu\otimes P}[\|X-Z\|_{2}]\) and \(C=\kappa_{1}^{2}(3L+1)\). Then, applying the triangle inequality and (15), we have

\[m_{\mu_{r+1},P} =\mathbb{E}_{(X,Z)\sim\mu_{r+1}\otimes P}[\|X-Z\|_{2}]=\mathbb{E} _{(X,Z)\sim\mu_{r}\otimes P}[\|T_{\mu_{r},\epsilon_{r}}(X)-Z\|_{2}]\] \[\leq\mathbb{E}_{(X,Z)\sim\mu_{r}\otimes P}[\|T_{\mu_{r},\epsilon_ {r}}(X)-X\|_{2}+\|X-Z\|_{2}]\leq(1+\epsilon_{r}C)m_{\mu_{r},P},\] (16) \[M_{\mu_{r+1},P} =\mathbb{E}_{(X,Z)\sim\mu_{r+1}\otimes P}[\|X-Z\|_{2}^{2}]=\mathbb{ E}_{(X,Z)\sim\mu_{r}\otimes P}[\|T_{\mu_{r},\epsilon_{r}}(X)-Z\|_{2}^{2}]\] \[\leq\mathbb{E}_{(X,Z)\sim\mu_{r}\otimes P}[\|T_{\mu_{r},\epsilon_ {r}}(X)-X\|_{2}^{2}+2\|T_{\mu_{r},\epsilon_{r}}(X)-X\|_{2}\|X-Z\|_{2}+\|X-Z\|_{2 }^{2}]\] \[\leq(\epsilon_{r}^{2}C^{2}+2\epsilon_{r}C)m_{\mu_{r},P}^{2}+M_{ \mu_{r},P}\leq(1+2\epsilon_{r}C+\epsilon_{r}^{2}C^{2})M_{\mu_{r},P}\] \[=(1+\epsilon_{r}C)^{2}M_{\mu_{r},P},\] (17)

where the second last step used Jensen's inequality \(m_{\mu_{r},P}^{2}\leq M_{\mu_{r},P}\). Then, we repeatedly apply (16) and (17) together with the triangle inequality and the bound \(1+x\leq e^{x}\) to get

\[M_{\mu_{r},P} \leq M_{\mu_{0},P}\prod_{s=0}^{r-1}(1+\epsilon_{s}C)^{2}\leq M_{ \mu_{0},P}\exp(2C\sum_{s=0}^{r-1}\epsilon_{s})\leq M_{\mu_{0},P}\exp(2Cb_{r-1})\] \[m_{\mu_{r},x^{*}}-m_{P,x^{*}} \leq m_{\mu_{r},P}\leq m_{\mu_{0},P}\prod_{s=0}^{r-1}(1+\epsilon_{s}C)\leq m _{\mu_{0},P}\exp(Cb_{r-1}).\]

## 8 Proof of Lemma 4: KSD-Wasserstein bound

Our proof generalizes that of Gorham and Mackey [9, Lem. 18]. Consider any \(g\in\mathcal{H}^{d}\) satisfying \(\|g\|_{\mathcal{H}^{d}}^{2}\triangleq\sum_{i=1}^{d}\|g_{i}\|_{\mathcal{H}}^{2}\leq 1\). From Assumption 2 we know

\[\|g(x)\|_{2}^{2} \leq k(x,x)\sum_{i=1}^{d}\|g_{i}\|_{\mathcal{H}}^{2}\leq\kappa_{1} ^{2},\] (18) \[\left\|\nabla g(x)\right\|_{op}^{2} \leq\left\|\nabla g(x)\right\|_{F}^{2}=\sum_{i=1}^{d}\sum_{j=1}^{d }|\nabla_{x},g_{j}(x)|^{2}\leq\|g\|_{\mathcal{H}^{d}}^{2}\mathrm{tr}(\nabla_{y }\nabla_{x}k(x,y)|_{y=x})\] \[\leq d\|\nabla_{y}\nabla_{x}k(x,y)|_{y=x}\|_{\mathrm{op}}\leq \kappa_{1}^{2}d,\text{ and }\] (19) \[\left\|\nabla(\nabla\cdot g(x))\right\|_{2}^{2} =\sum_{i=1}^{d}\Bigl{(}\sum_{j=1}^{d}\nabla_{x_{i}}\nabla_{x_{j}} g_{j}(x)\Bigr{)}^{2}\leq d\sum_{i=1}^{d}\sum_{j=1}^{d}|\nabla_{x_{i}}\nabla_{x_{j}} g_{j}(x)|^{2}\] \[\leq d\sum_{i=1}^{d}\sum_{j=1}^{d}\|g_{j}\|_{\mathcal{H}}^{2}( \nabla_{y_{i}}\nabla_{y_{j}}\nabla_{x_{i}}\nabla_{x_{j}}k(x,y)|_{y=x})\leq \kappa_{2}^{2}d^{2},\]

Suppose \(X,Y,Z\) are distributed so that \((X,Y)\) is a \(1\)-Wasserstein optimal coupling of \((\mu,\nu)\) and \(Z\) is independent of \((X,Y)\). Since \(s_{p}\) is \(L\)-Lipschitz with \(\mathbb{E}_{P}[s_{p}]=0\) (Assumption 1), \(g\) is bounded (18), and \(g\) and \(\nabla\cdot g\) are Lipschitz (19), repeated use of Cauchy-Schwarz gives

\[\mathbb{E}_{\mu}[\mathcal{T}_{P}g]-\mathbb{E}_{\nu}[\mathcal{T}_ {P}g]\] \[=\mathbb{E}[\nabla\cdot g(X)-\nabla\cdot g(Y)]+\mathbb{E}[\langle s _{p}(X)-s_{p}(Y),g(X)\rangle]+\mathbb{E}[\langle s_{p}(Y)-s_{p}(Z),g(X)-g(Y)\rangle]\] \[\leq(\kappa_{2}d+\kappa_{1}L)W_{1}(\mu,\nu)+L\mathbb{E}[\left\|Y -Z\right\|_{2}\min(2\kappa_{1},\kappa_{1}\sqrt{d}\|X-Y\|_{2})].\]

Since our choice of \(g\) was arbitrary, the first advertised result now follows from the definition of KSD (Definition 4). The second claim then follows from Cauchy-Schwarz and the inequality \(\min(a,b)^{2}\leq ab\) for \(a,b\geq 0\), since

\[\mathbb{E}[\left\|Y-Z\right\|_{2}\min(2\kappa_{1},\kappa_{1} \sqrt{d}\|X-Y\|_{2})]\leq M_{\nu,P}^{1/2}\,\mathbb{E}[\min(2\kappa_{1},\kappa_ {1}\sqrt{d}\|X-Y\|_{2})^{2}]^{1/2}\] \[\leq\sqrt{2M_{\nu,P}}\kappa_{1}d^{1/4}\mathbb{E}[\left\|X-Y\right\| _{2}]^{1/2}=\sqrt{2M_{\nu,P}W_{1}(\mu,\nu)}\kappa_{1}d^{1/4}.\]

## 9 Conclusions and Limitations

In summary, we have proved the first unified convergence bound and rate for finite-particle SVGD. In particular, our results show that with a suitably chosen step size sequence, SVGD with \(n\)-particles drives the KSD to zero at an order \(1/\sqrt{\log\log(n)}\) rate. The assumptions we have made on the target and kernel are mild and strictly weaker than those used in prior work to establish KSD weak convergence control [9, 3, 12, 1]. However, we suspect that, with additional effort, the Lipschitz score assumption (Assumption 1) can be relaxed to accommodate pseudo-Lipschitz scores as in Erdogdu et al. [7] or weakly-smooth scores as in Sun et al. [24]. A second limitation of this work is that the obtained rate of convergence is quite slow. However, we hope that this initial recipe for explicit, non-asymptotic convergence will serve as both a template and a catalyst for the field to develop refined upper and lower bounds for SVGD error. To this end, we leave the reader with several open challenges. First, can one establish a non-trivial minimax lower bound for the convergence of SVGD? Second, can one identify which types of target distributions lead to worst-case convergence behavior for SVGD? Finally, can one identify commonly met assumptions on the target distribution and kernel under which the guaranteed convergence rate of SVGD can be significantly improved? Promising follow-up work has already begun investigating speed-ups obtainable by focusing on the convergence of a finite set of moments [20] or by modifying the SVGD algorithm [5].

## References

* [1] Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, and Lester Mackey. Targeted separation and convergence with kernel discrepancies. _arXiv preprint arXiv:2209.12835_, 2022.
* [2] Alain Berlinet and Christine Thomas-Agnan. _Reproducing kernel Hilbert spaces in probability and statistics_. Springer Science & Business Media, 2011.
* [3] Wilson Ye Chen, Lester Mackey, Jackson Gorham, Francois-Xavier Briol, and Chris Oates. Stein points. In _International Conference on Machine Learning_, pages 844-853, 2018.

* [4] Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In _International Conference on Machine Learning_, pages 2606-2615, 2016.
* [5] Aniket Das and Dheeraj Nagaraj. Provably fast finite particle variants of svgd via virtual particle stochastic approximation. _arXiv preprint arXiv:2305.17558_, 2023.
* [6] Andrew Duncan, Nikolas Nusken, and Lukasz Szpruch. On the geometry of Stein variational gradient descent. _arXiv preprint arXiv:1912.00894_, 2019.
* [7] Murat A. Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with discretized diffusions. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roma Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 9694-9703, 2018.
* [8] Jackson Gorham and Lester Mackey. Measuring sample quality with Stein's method. In _Advances in Neural Information Processing Systems_, pages 226-234, 2015.
* [9] Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In _International Conference on Machine Learning_, pages 1292-1301, 2017.
* [10] Jackson Gorham, Anant Raj, and Lester Mackey. Stochastic Stein discrepancies. _Advances in Neural Information Processing Systems_, 33:17931-17942, 2020.
* [11] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In _International Conference on Machine Learning_, pages 1352-1361, 2017.
* [12] Jonathan Huggins and Lester Mackey. Random feature Stein discrepancies. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, pages 1903-1913. 2018.
* [13] Priyank Jaini, Lars Holdijk, and Max Welling. Learning equivariant energy based models with equivariant Stein variational gradient descent. _Advances in Neural Information Processing Systems_, 34:16727-16737, 2021.
* [14] Heishiro Kanagawa, Arthur Gretton, and Lester Mackey. Controlling moments with kernel stein discrepancies. _arXiv preprint arXiv:2211.05408_, 2022.
* [15] Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymptotic analysis for Stein variational gradient descent. _Advances in Neural Information Processing Systems_, 33, 2020.
* [16] Jing Lei. Convergence and concentration of empirical measures under wasserstein distance in unbounded functional spaces. _Bernoulli_, 26(1):767-798, 2020.
* [17] Qiang Liu. Stein variational gradient descent as gradient flow. In _Advances in Neural Information Processing Systems_, pages 3115-3123, 2017.
* [18] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm. _Advances in Neural Information Processing Systems_, 29:2378-2386, 2016.
* [19] Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In _International Conference on Machine Learning_, pages 276-284, 2016.
* [20] Tianle Liu, Promit Ghosal, Krishnakumar Balasubramanian, and Natesh Pillai. Towards understanding the dynamics of gaussian-stein variational gradient descent. _arXiv preprint arXiv:2305.14076_, 2023.
* [21] Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. In _33rd Conference on Uncertainty in Artificial Intelligence_, 2017.
* [22] Adil Salim, Lukang Sun, and Peter Richtarik. A convergence theory for SVGD in the population limit under Talagrand's inequality T1. In _International Conference on Machine Learning_, pages 19139-19152. PMLR, 2022.
* [23] Adrien Saumard and Jon A Wellner. Log-concavity and strong log-concavity: a review. _Statistics surveys_, 8:45, 2014.
* [24] Lukang Sun, Avetik Karagulyan, and Peter Richtarik. Convergence of Stein variational gradient descent under a weaker smoothness condition. _arXiv preprint arXiv:2206.00508_, 2022.
** [25] Cedric Villani. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [26] Dilin Wang and Qiang Liu. Learning to draw samples: With application to amortized MLE for generative adversarial learning. _arXiv preprint arXiv:1611.01722_, 2016.
* [27] Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical models. In _International Conference on Machine Learning_, pages 5219-5227, 2018.
* [28] Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang. Message passing Stein variational gradient descent. In _International Conference on Machine Learning_, pages 6018-6027, 2018.

[MISSING_PAGE_FAIL:12]

* When \(\left\|x-y\right\|_{2}\geq 1\), \[\left|\phi^{\prime}(\left\|x-y\right\|_{2}^{2}/2)\right|\left\|x-y \right\|_{2} \leq-2\beta\left\|x-y\right\|_{2}^{2\beta-2}\left\|x-y\right\|_{2}=-2 \beta\left\|x-y\right\|_{2}^{2\beta-1}\] \[\leq-2\beta/\left\|x-y\right\|_{2}\leq-2\beta.\]
* When \(\left\|x-y\right\|_{2}<1\), \(\left|\phi^{\prime}(\left\|x-y\right\|_{2}^{2}/2)\right|\left\|x-y\right\|_{2 }<\left|\phi^{\prime}(\left\|x-y\right\|_{2}^{2}/2)\right|\leq-2\beta c^{2 \beta-2}\).

Therefore, \(\left\|\nabla_{x}k(x,y)\right\|_{2}\) is also bounded (Assumption 2) by constants independent of dimension, and Assumption 3 holds with \(\gamma=-2\beta\).

The original SVGD paper [18] used Gaussian kernels in all experiments, and they remain perhaps the most common choice in the literature. In this case, \(\phi\) satisfies

\[\phi(t)=e^{-2\alpha t}\quad\text{for}\quad\alpha>0,\quad\phi^{\prime}(t)=-2 \alpha e^{-2\alpha t}=-2\alpha\phi(t),\quad\text{and}\quad\phi^{\prime\prime} (t)=4\alpha^{2}\phi(t).\]

Using the inequality \(x\leq e^{x-1}\), we find that

\[\left|\phi^{\prime}(\left\|x-y\right\|_{2}^{2}/2)\right| =2\alpha e^{-\alpha\left\|x-y\right\|_{2}^{2}}\leq\min(2\alpha,2 /(e\|x-y\|_{2}^{2}))\quad\text{and}\] \[\left|\phi^{\prime\prime}(\left\|x-y\right\|_{2}/2)\right|\left\|x -y\right\|_{2}^{2} =4\alpha^{2}e^{-\alpha\left\|x-y\right\|_{2}^{2}}\left\|x-y\right\|_{2}^{2 }\leq 4\alpha/e\]

so that \(\left\|\nabla_{y}\nabla_{x}k(x,y)\right\|_{\text{op}}\), \(\left\|\nabla_{x}^{2}k(x,y)\right\|_{\text{op}}\), and \(\left\|\nabla_{x}k(x,y)\right\|_{2}\) are bounded (Assumption 2) by constants independent of dimension, and Assumption 3 holds with \(\gamma=2/e\).

## Appendix B Proof of Corollary 2: A finite-particle convergence rate for SVGD

We begin by establishing a lower bound on \(b_{t-1}\). Let

\[b_{t-1}^{(1)}=\psi_{\bar{B},\bar{C}}(\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n})}, \bar{A},\beta_{1})\quad\text{and}\quad b_{t-1}^{(2)}=\psi_{\bar{B},\bar{C}}( \bar{w}_{0,n}\,\phi(\bar{w}_{0,n}),\bar{A}+2\bar{C},\beta_{2})\]

so that \(b_{t-1}=\min(b_{t-1}^{(1)},b_{t-1}^{(2)})\). Since \(\beta_{1},\beta_{2},\phi(\bar{w}_{0,n})\geq 1\), we have

\[\beta_{1} =\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log\tfrac{1}{\bar{w}_{0, n}\sqrt{\phi(\bar{w}_{0,n})}}-\bar{A})))\] \[\leq\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log\tfrac{1}{\bar{w} _{0,n}\sqrt{\phi(\bar{w}_{0,n})}})))\] \[\leq\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log\tfrac{1}{\bar{w} _{0,n}})))\quad\text{and}\] \[\beta_{2} =\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log\tfrac{1}{\bar{w}_{ 0,n}\phi(\bar{w}_{0,n})}-\bar{A}-2\bar{C})))\] \[\leq\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log\tfrac{1}{\bar{w} _{0,n}\phi(\bar{w}_{0,n})})))\] \[\leq\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log\tfrac{1}{\bar{w} _{0,n}\phi(\bar{w}_{0,n})})).\]

Hence, \(\phi(\bar{w}_{0,n})\geq 1\) implies that

\[b_{t-1}^{(1)} \geq\tfrac{1}{C}\log(\tfrac{1}{B}(\tfrac{\log\tfrac{1}{\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n})}}}{\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log \tfrac{1}{\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n})}}))}-\bar{A}))\] \[\geq\tfrac{1}{C}\log(\tfrac{1}{B}(\tfrac{\log\tfrac{1}{\bar{w}_{ 0,n}\phi(\bar{w}_{0,n})}}{\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{\bar{B}}(\log \tfrac{1}{\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n})}}))}-\bar{A}-2\bar{C}))\quad \text{and}\] (20) \[b_{t-1}^{(2)} \geq\tfrac{1}{C}\log(\tfrac{1}{B}(\tfrac{\log\tfrac{1}{\bar{w}_{ 0,n}\phi(\bar{w}_{0,n})}}{\max(1,\tfrac{1}{C}\,\log(\tfrac{1}{B}(\log \tfrac{1}{\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n})}}))}-\bar{A}-2\bar{C})).\]

We divide the remainder of our proof into four parts. First we prove each of the two cases in the generic KSD bound (7) in Appendices B.1 and B.2. Next we show in Appendix B.3 that these two cases yield the generic convergence rate (8). Finally, we prove the high probability upper estimate (9) for \(w_{0,n}\) under i.i.d. initialization in Appendix B.4.

### Case \(b_{t-1}=0\)

In this case, the error bound (7) follows directly from Theorem 3.

### Case \(b_{t-1}>0\)

We first state and prove a useful lemma.

**Lemma 6**.: _Suppose \(x=f(\beta)\) for a non-increasing function \(f:\mathbb{R}\to\mathbb{R}\) and \(\beta=\max(1,f(1))\). Then \(x\leq\beta\) and \(x\leq f(x).\)_

Proof.: Because \(f\) is non-increasing and \(\beta\geq 1\), \(x=f(\beta)\leq f(1)\leq\beta\). Since \(x\leq\beta\) and \(f\) is non-increasing, we further have \(f(x)\geq f(\beta)=x\) as advertised. 

Since \(\psi_{\bar{B},\bar{C}}\) is non-increasing in its third argument, Lemma 6 implies that \(b_{t-1}^{(1)}\leq\beta_{1}\) and

\[b_{t-1}^{(1)}\leq\psi_{\bar{B},\bar{C}}(\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n}) },\bar{A},b_{t-1}^{(1)}).\]

Rearranging the terms and noting that

\[\bar{B}<\tfrac{1}{\beta_{1}}\log\tfrac{1}{\bar{w}_{0,n}\sqrt{\phi(\bar{w}_{0,n })}}-\bar{A}\leq\tfrac{1}{b_{t-1}^{(1)}}\log\tfrac{1}{\bar{w}_{0,n}\sqrt{\phi( \bar{w}_{0,n})}}-\bar{A}\]

since \(b_{t-1}^{(1)}\geq b_{t-1}>0\), we have

\[\bar{w}_{0,n}\exp(b_{t-1}^{(1)}(\bar{A}+\bar{B}\exp(\bar{C}b_{t-1}^{(1)}))) \leq\tfrac{1}{\sqrt{\phi(\bar{w}_{0,n})}}.\] (21)

Similarly, we have \(b_{t-1}^{(2)}\leq\psi_{\bar{B},\bar{C}}(\bar{w}_{0,n}\log\log\tfrac{1}{\bar{w} _{0,n}},\bar{A}+2\bar{C},b_{t-1}^{(2)})\) and

\[\sqrt{\bar{w}_{0,n}}\exp(b_{t-1}^{(2)}(2\bar{C}+\bar{A}+\bar{B}\exp(\bar{C}b_{ t-1}^{(2)}))/2)\leq\tfrac{1}{\sqrt{\phi(\bar{w}_{0,n})}}.\] (22)

Since \(b_{t-1}=\min(b_{t-1}^{(1)},b_{t-1}^{(2)})\), the inequalities (21) and (22) are also satisfied when \(b_{t}\) is substituted for \(b_{t-1}^{(1)}\) and \(b_{t-1}^{(2)}\). Since the error term \(a_{t-1}\) (4) is non-decreasing in each of \((w_{0,n},A,B,C)\), we have

\[a_{t-1}\leq(\kappa_{1}L+\kappa_{2}d+\kappa_{1}d^{1/4}L\sqrt{2M_{\mu_{0}^{ \infty},P}})/\sqrt{\phi(\bar{w}_{0,n})}.\]

Since \(b_{t-1}=\min(b_{t-1}^{(1)},b_{t-1}^{(2)})\), the claim (7) follows from this estimate, the lower bounds (20), and Theorem 3.

### Generic convergence rate

The generic convergence rate (8) holds as, by the lower bounds (20), \(b_{t-1}=\min(b_{t-1}^{(1)},b_{t-1}^{(2)})>0\) whenever

\[e^{-(\bar{B}+\bar{A}+2\bar{C})}>\bar{w}_{0,n}\phi(\bar{w}_{0,n})\quad\text{and} \quad\bar{B}^{(\bar{B}+\bar{A}+2\bar{C})/\bar{C}}\quad>\bar{w}_{0,n}\phi(\bar {w}_{0,n})(\log(1/\bar{w}_{0,n}))^{(\bar{B}+\bar{A}+2\bar{C})/\bar{C}},\]

a condition which occurs whenever \(\bar{w}_{0,n}\) is sufficiently small since the right-hand side of each inequality converges to zero as \(\bar{w}_{0,n}\to 0\).

### Initializing with i.i.d. particles

We begin by restating an expected Wasserstein bound due to Lei [16].

**Lemma 7** (Lei [16, Thm. 3.1]).: _Suppose \(\mu_{0}^{n}=\tfrac{1}{n}\sum_{i=1}^{n}\delta_{x_{i}}\) for \(x_{i}\overset{i\text{\tiny{\it def}}}{\sim}\mu_{0}^{\infty}\) with \(M_{\mu_{0}^{\infty}}\triangleq\mathbb{E}_{\mu_{0}^{\infty}}[\|\cdot\|_{2}^{2}]<\infty\). Then, for a universal constant \(c>0,\)_

\[\mathbb{E}[W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})]\leq cM_{\mu_{0}^{\infty}}\frac {\log(n)^{\lceil\lceil d-2\rceil}}{n^{1/(2\lor d)}}.\]

Together, Lemma 7 and Markov's inequality imply that

\[W_{1}(\mu_{0}^{n},\mu_{0}^{\infty})\leq\mathbb{E}[W_{1}(\mu_{0}^{n},\mu_{0}^{ \infty})]/(c\delta)\leq M_{\mu_{0}^{\infty}}\frac{\log(n)^{\lceil d-2\rceil}} {n^{1/(2\lor d)}}/\delta\]

with probability at least \(1-c\delta\), proving the high probability upper estimate (9).