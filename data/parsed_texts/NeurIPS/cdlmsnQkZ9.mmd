# Learning non-Markovian Decision-Making

from State-only Sequences

 Aoyang Qin\({}^{\star,1,2}\) Feng Gao\({}^{3}\) Qing Li\({}^{2}\) Song-Chun Zhu\({}^{1,2,4}\) Sirui Xie\({}^{\star,5}\)

\({}^{1}\) Department of Automation, Tsinghua University

\({}^{2}\) Beijing Institute for General Artificial Intelligence (BIGAI)

\({}^{3}\) Department of Statistics, UCLA \({}^{4}\) School of Artificial Intelligence, Peking University

\({}^{5}\) Department of Computer Science, UCLA

indicates equal contribution. Correspondence: Sirui Xie (srxie@ucla.edu). Code and data are available at https://github.com/qayqaq/LanMDP

###### Abstract

Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to learn both the transition and the policy, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables _decision-making as inference_: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Markovian constraints and show that the learned model exhibits strong performances in challenging domains from the MuJoCo suite.

## 1 Introduction

Imitation from others is a prevalent phenomenon in humans and many other species, where individuals learn by observing and mimicking the actions of others. An intriguing aspect of this process is the brain's ability to extract motor signals from sensory input. This remarkable capability is facilitated by _mirror neurons_[1, 2], which respond to observations as if the imitator is performing the actions themselves. In conventional imitation learning [3, 4] and offline reinforcement learning [5], action labels have served as proxies for mirror neurons. But it is important to recognize that they are actually productions of human interventions. Given the recent advancements in AI, now is probably an opportune time to explore imitation learning in a more naturalistic setting.

While the setting of state-only demonstrations is not common, there are certain exceptions. For example, Inverse Reinforcement Learning (IRL) initially formulated the problem as state visitation matching [6], where demonstrations consist solely of state sequences. Subsequently, this state-only setting was rewarded as Imitation Learning from Observations (ILFO), which introduced the generalized formulation of matching marginal state distributions [7, 8]. These methods typically rely on the Markov assumption and Temporal Difference (TD) learning techniques [9]. One consequence of this assumption, previously believed to be advantageous, is that sequences with different state orders are treated as equivalent. However, the success of general sequence modeling [10] has challengedthis belief, leading to deep reflections. Notable progresses since then include an analysis of the expressivity of Markovian rewards [11] and a series of sequence models tailored for decision-making problems [12; 13; 14; 15]. Aligning with this evolving trend, we extend the state-only imitation learning problem to encompass non-Markovian domains.

In this work, we propose a generative model based on non-Markov Decision Process (nMDP), in which states are fully observable and actions are latent. Unlike existing monolithic sequence models, we factorize the joint state-action distribution into policy and causal transition according to the standard Markov Decision Process (MDP). To further extend to the non-Markovian domain, we condition the policy on sequential contexts. The density families of policy and transition are consistent with conventional IRL [4]. We refer to this model as Latent-action non-Markov Decision Process (LanMDP). Because the actions are latent variables following Boltzmann distribution, the present model is closely related to the Latent-space Energy-Based Model (LEBM) [16]. To learn the latent policy by Maximum Likelihood Estimation (MLE), we need to sample from the prior and the posterior. We sample the prior using short-run Markov Chain Monte Carlo (MCMC) [17], and the posterior using importance sampling. Specifically, the proposed importance sampling sidesteps back-propagation through time in posterior MCMC with a single-step lookahead of the Markov transition. The transition is learned from self-interaction.

Once the LanMDP is learned, it can be used for policy execution and planning through prior and posterior sampling, or in other words, _policy as prior, planning as posterior inference_[18; 19]. In our analysis, we derive an objective of the non-Markovian decision-making problem induced from the MLE. We show that the prior sampling at each step can indeed lead to optimal expected returns. Almost surprisingly, we find that the entire family of maximum entropy reinforcement learning [4; 20; 21; 22; 23; 24] naturally emerges from the algebraic structures in the MLE of latent policies. This formulation avoids the peculiarities of maximizing state transition entropy in prior arts [20; 24]. We also show that when a target goal state is in-distribution, the posterior sampling is optimizing a conditional variant of the objective, realizing model-based planning. In our experiments, we validate the necessity and efficacy of our model in learning to sequentially plan cubic curves, and illustrate an _over-imitation_ phenomenon [25; 26] when the learned model is repurposed for goal-reaching. We also test the proposed modeling, learning, and computing method in MuJoCo, a domain with higher-dimensional state and action spaces, and achieve performance competitive to existing methods, even those that learn with action labels.

## 2 Non-Markov Decision Process

The most well-known sequence model of a decision-making process is Markov Decision Process. A MDP is a tuple \(\mathcal{M}=\langle S,A,T,R,\rho,H\rangle\) that contains a set \(S\) of states, a set \(A\) of actions, a transition \(T:S\times A\mapsto\Pi(S)\) that returns for every state \(s_{t}\) and action \(a_{t}\) a distribution over the next state \(s_{t+1}\); a reward function \(R:S\times A\mapsto\mathbb{R}\) that specifies the real-valued reward received by the agent when taking action \(a_{t}\) in state \(s_{t}\); an initial state distribution \(\rho:\Pi(S)\); and a horizon \(H\) that is the maximum number of actions/steps the agent can execute in one episode. A solution to an MDP is a policy that maps states to actions, \(\pi:S\mapsto\Pi(A)\). The value of policy \(\pi\), \(V^{\pi}(s)=\mathbb{E}_{T,\pi}[\sum_{t=0}^{H}R(s_{t})|s_{0}=s]\) is the expected cumulative reward (_i.e_. return) when executing with this policy starting from state \(s\). The state-action value of policy \(\pi\) is \(Q^{\pi}(s_{t},a_{t})=R(s_{t},a_{t})+\mathbb{E}_{T(s_{t+1}|s_{t},a_{t})}[V^{\pi }(s_{t+1})]\). The optimal policy \(\pi^{*}\) can maximize either \(E_{\rho(s_{0})}[V^{\pi}(s_{0})]\), or the same objective plus the policy entropy [27; 4; 22]. The Markovian assumption supports the convergence of a series of TD-learning methods [9], whose reliability in non-Markovian domains is still an open problem.

Figure 1: **Graphical model of policy and transition in standard Markov Decision Process and non-Markov Decision Process. Reward variables are omitted in the probabilistic graph to emphasize the difference in dependency between actions and states. nMDP is a natural generalization of standard MDP.**

A non-Markov Decision Process is also a tuple \(\mathcal{M}=\langle S,A,T,R,\rho,H\rangle\). It generalizes MDP by allowing for non-Markovian transitions and rewards [28]. Notably, assuming Markovian transition and non-Markovian reward is usually sufficient since a state space with non-Markovian transition can be represented with its Markov abstraction [29]. Markov abstraction can be done either by treating the original space as observations generated from the latent belief state in a Partially Observable Markov Decision Process (POMDP) [30], or by projecting historic contexts into an embedding space for sequence pattern detection [31; 32; 28]. Presumably, it is statistically more interesting in deep learning to focus our attention on non-Markovian domains where the temporal dependencies in transition and reward differ. Therefore, without loss of generality, we assume that the state transition is Markovian \(T:S\times A\mapsto\Pi(S)\), while the reward is not [33; 11], _i.e._\(R:S^{+}\mapsto\mathbb{R}\), with \(S^{+}\) denotes the set of all finite non-empty state sequences with length smaller than \(H\). Obviously, the policy should also be non-Markovian \(\pi:S^{+}\mapsto\Pi(A)\). Check Figure 1 for a probabilistic graphical model of the generation process of state sequences from a policy.

## 3 Learning and Sampling

### Latent-action nMDP

A complete trajectory is denoted by

\[\zeta=\{s_{0},a_{0},s_{1},a_{1},\cdots,a_{T-1},s_{T}\},\] (1)

where \(T\) is the maximum length of all observed trajectories and \(T\leq H\). The joint distribution of state and action sequences can be factorized according to the causal assumptions in nMDP:

\[\begin{split} p_{\theta}(\zeta)&=p(s_{0})p_{\alpha }(a_{0}|s_{0})p_{\beta}(s_{1}|s_{0},a_{0})\cdots p_{\alpha}(a_{T-1}|s_{0:T-1}) p_{\beta}(s_{T}|s_{T-1},a_{T-1})\\ &=p(s_{0})\prod_{t=0}^{T-1}p_{\alpha}(a_{t}|s_{0:t})p_{\beta}(s_ {t+1}|s_{t},a_{t}),\end{split}\] (2)

where \(p_{\alpha}(a_{t}|s_{0:t-1})\) is the policy model with parameter \(\alpha\), \(p_{\beta}(s_{t}|s_{t-1},a_{t-1})\) is the transition model with parameter \(\beta\), both of which are parameterized with neural networks, \(\theta=(\alpha,\beta)\). \(p(s_{0})\) is the initial state distribution, which can be sampled as a black box.

The density families of policy and transition are consistent with the conventional setting of IRL [4], where the transition describes the predictable change in state as a single-mode Gaussian, \(s_{t+1}\sim\mathcal{N}(g_{\beta}(s_{t},a_{t}),\sigma^{2})\), and the policy accounts for bounded rationality as a Boltzmann distribution with state-action value as the unnormalized energy:

\[p_{\alpha}(a_{t}|s_{0:t})=\frac{1}{Z(\alpha,s_{0:t})}\exp{(f_{\alpha}(a_{t};s_ {0:t}))},\] (3)

where \(f_{\alpha}(a_{t};s_{0:t})\) is the negative energy, \(Z(\alpha,s_{0:t})=\int\exp(f_{\alpha}(a_{t};s_{0:t}))da_{t}\) is the normalizing constant given the contexts \(s_{0:t}\). We discuss a general push-forward transition in Appx A.3.

Since we can only observe state sequences, the aforementioned generative model can be understood as a sequential variant of LEBM [16], where the transition serves as the generator and the policy is a history-conditioned latent prior. The marginal distribution of state sequences and the posterior distribution of action sequences are:

\[p_{\theta}(s_{0:T})=\int p_{\theta}(s_{0:T},a_{0:T-1})da_{0:T-1},\quad p_{ \theta}(a_{0:T-1}|s_{0:T})=\frac{p_{\theta}(s_{0:T},a_{0:T-1})}{p_{\theta}(s_ {0:T})}.\] (4)

### Maximum likelihood learning

We need to estimate \(\theta=(\alpha,\beta)\). Suppose we observe _offline_ training examples: \(\{\xi^{i}\},i=1,2,\cdots,n,\quad\xi^{i}=\begin{bmatrix}s_{0}^{i},s_{1}^{i},...,s_{T}^{i}\end{bmatrix}\). The log-likelihood function is:

\[L_{off}(\theta)=\sum_{i=1}^{n}\log p_{\theta}(\xi^{i}).\] (5)

Denote posterior distribution of action sequence \(p_{\theta}(a_{0:T-1}|s_{0:T})\) as \(p_{\theta}(A|S)\) for convenience where \(A\) and \(S\) means the complete action and state sequences in a trajectory. The full derivation of the learning method can be found in Appx A.2, which results in the following gradient:

\[\nabla_{\theta}\log p_{\theta}(\xi)=\mathbb{E}_{p_{\theta}(A|S)}[\sum_{t=0}^{ T-1}(\underbrace{\nabla_{\alpha}\log p_{\alpha}(a_{t}|s_{0:t})}_{\text{policy/prior}},\underbrace{\nabla_{\beta}\log p_{ \beta}(s_{t+1}|s_{t},a_{t})}_{\text{transition}})].\] (6)Due to the normalizing constant \(Z(\alpha,s_{0:t})\) in the energy-based prior \(p_{\alpha}\), the gradient for the policy term involves both posterior and prior samples:

\[\delta_{\alpha,t}(S)=\mathbb{E}_{p_{\theta}(A|S)}\left[\nabla_{\alpha}\log p_{ \alpha}(a_{t}|s_{0:t})\right]=\mathbb{E}_{p_{\theta}(A|S)}\left[\nabla_{\alpha} f_{\alpha}(a_{t};s_{0:t})\right]-\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[ \nabla_{\alpha}f_{\alpha}(a_{t};s_{0:t})\right],\] (7)

where \(\delta_{\alpha,t}(S)\) denotes the expected gradient of policy term for time step \(t\). Intuition can be gained from the perspective of adversarial training [34; 35]: On one hand, the model utilizes action samples from the posterior \(p_{\theta}(A|S)\) as pseudo-labels to supervise the unnormalized prior at each step. On the other hand, it discourages action samples directly sampled from the prior. The model converges when prior samples and posterior samples are indistinguishable.

To ensure the transition model's validity, it needs to be grounded in real-world dynamics \(Tr\) when jointly learned with the policy. Otherwise, the agent would be purely hallucinating based on the demonstrations. Throughout the training process, we allow the agent to periodically collect _on-policy_ data \(\{(s_{t}^{i},a_{t}^{i},s_{t+1}^{i})\}\), \(i=1,2,\cdots,m\), \(t=1,2,\cdots,T\) with \(p_{\alpha}(a_{t}|s_{0:t})\) and update the transition with a _composite likelihood_[36]

\[L_{comp}(\beta)=L_{off}(\theta)+L_{on}(\beta),\quad L_{on}(\beta)=\sum_{i=1}^ {m}\sum_{t=1}^{T}\log p_{\beta}(s_{t+1}^{i}|s_{t}^{i},a_{t}^{i}).\] (8)

### Prior and posterior sampling

The maximum likelihood estimation requires samples from the prior and the posterior distributions of actions. It would not be a problem if the action space is quantized. However, since we target general latent action learning, we proceed to introduce sampling techniques for continuous actions.

When sampling from a continuous energy space, short-run Langevin dynamics [17] can be an efficient choice. For a target distribution \(\pi(a)\), Langevin dynamics iterates \(a_{k+1}=a_{k}+s\nabla_{a_{k}}\log\pi(a_{k})+\sqrt{2s}\epsilon_{k}\), where \(k\) indexes the number of iteration, \(s\) is a small step size, and \(\epsilon_{k}\) is the Gaussian white noise. \(\pi(a)\) can be either the prior \(p_{\alpha}(a_{t}|s_{0:t})\) or the posterior \(p_{\theta}(A|S)\). One property of Langevin dynamics that is particularly amenable for EBM is that we can get rid of the normalizing constant. So for each \(t\) the iterative update for prior samples is

\[a_{t,k+1}=a_{t,k}+s\nabla_{a_{t,k}}f_{\alpha}(a_{t,k};s_{0:t})+\sqrt{2s} \epsilon_{k}.\] (9)

Given a state sequence \(s_{0:T}\) from the demonstrations, the posterior samples at each time step \(a_{t}\) come from the conditional distribution \(p(a_{t}|s_{0:T})\). Notice that with Markov transition, we can derive

\[p_{\theta}(a_{0:T-1}|s_{0:T})=\prod_{t=0}^{T-1}p_{\theta}(a_{t}|s_{0:T})=\prod _{t=0}^{T-1}p_{\theta}(a_{t}|s_{0:t+1}).\] (10)

Eq. (10) reveals that given the previous and the next subsequent state, the posterior can be sampled at each step independently. So the posterior iterative update is

\[a_{t,k+1}=a_{t,k}+s\nabla_{a_{t,k}}(\underbrace{\log p_{\alpha}(a_{t,k}|s_{0: t})}_{\text{policy/prior}}+\underbrace{\log p_{\beta}(s_{t+1}|s_{t},a_{t,k})}_{ \text{transition}})+\sqrt{2s}\epsilon_{k}.\] (11)

Intuitively, action samples at each step are updated by back-propagation from its prior energy and a single-step lookahead. While gradients from the transition term are analogous to the inverse dynamics in Behavior Cloning from Observations (BCO) [37], it may lead to poor training performance due to non-injectiveness in forward dynamics [38].

We develop an alternative posterior sampling method with importance sampling to overcome this challenge. Leveraging the learned transition, we have

\[p_{\theta}(a_{t}|s_{0:t+1})=\frac{p_{\beta}(s_{t+1}|s_{t},a_{t})}{\mathbb{E}_{ p_{\alpha}(a_{t}|s_{0:t})}\left[p_{\beta}(s_{t+1}|s_{t},a_{t})\right]}p_{ \alpha}(a_{t}|s_{0:t}).\] (12)

Let \(c(a_{t};s_{0:t+1})=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[p_{\beta}(s_{t+ 1}|s_{t},a_{t})\right]\), posterior sampling from \(p_{\theta}(a_{0:T-1}|s_{0:T})\) can be realized by adjusting importance weights of independent samples from the prior \(p_{\alpha}(a_{t}|s_{0:t})\), in which the estimation of weights involves another prior sampling. In this way, we avoid back-propagating through non-injective dynamics and save some computation overhead.

To train the policy, Eq. (7) can now be rewritten as

\[\delta_{\alpha,t}(S)=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\frac{p_{ \beta}(s_{t+1}|s_{t},a_{t})}{c(a_{t};s_{0:t+1})}\nabla_{\alpha}f_{\alpha}(a_{ t};s_{0:t})\right]-\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\nabla_{ \alpha}f_{\alpha}(a_{t};s_{0:t})\right].\] (13)Decision-making as Inference

In Section 3, we present our method within the framework of probabilistic inference, providing a self-contained description. However, from a decision-making perspective, the learned policy may appear arbitrary. In this section, we establish a connection between probabilistic inference and decision-making, contributing a novel analysis that incorporates the latent action setting, the non-Markovian assumption, and maximum likelihood learning. This analysis is inspired by, but distinct from, previous studies on the relationship between these two fields [39, 20, 40, 41, 24].

### Policy execution with prior sampling

Let the ground-truth distribution of demonstrations be \(p*(s_{0:T})\), and the learned marginal distributions of state sequences be \(p_{\theta}(s_{0:T})\). Eq. (5) in Section 3.2 is an empirical estimate of

\[\mathbb{E}_{p^{\bullet}(s_{0:T})}[\log p_{\theta}(s_{0:T})]=\mathbb{E}_{p^{ \bullet}(s_{0})}\left[\log p^{\bullet}(s_{0})+\mathbb{E}_{p^{\bullet}(s_{1:T}| s_{0})}[\log p_{\theta}(s_{1:T}|s_{0})]\right].\] (14)

We can show that a sequential decision-making problem can be constructed to maximize the same objective. Our main result is summarized as Theorem 1.

**Theorem 1**.: _Assuming the Markovian transition \(p_{\beta*}(s_{t+1}|s_{t},a_{t})\) is known, the ground-truth conditional state distribution \(p^{*}(s_{t+1}|s_{0:t})\) for demonstration sequences is accessible, we can construct a sequential decision-making problem, based on a reward function \(r_{\alpha}(s_{t+1},s_{0:t}):=\log\int p_{\alpha}(a_{t}|s_{0:t})p_{\beta*}(s_{t +1}|s_{t},a_{t})da_{t}\) for an arbitrary energy-based policy \(p_{\alpha}(a_{t}|s_{0:t})\). Its objective is_

\[\sum_{t=0}^{T}\mathbb{E}_{p^{\bullet}(s_{0:t})}[V^{p_{\alpha}}(s_{0:t})]= \mathbb{E}_{p^{\bullet}(s_{0:T})}\left[\sum_{t=0}^{T}\sum_{k=t}^{T}r_{\alpha}( s_{k+1};s_{0:k})\right],\]

_where \(V^{p_{\alpha}}(s_{0:t}):=E_{p^{\bullet}(s_{t+1:T}|s_{0:t})}[\sum_{k=t}^{T}r_{ \alpha}(s_{k+1};s_{0:k})]\) is the value function for \(p_{\alpha}\). This objective yields the same optimal policy as the Maximum Likelihood Estimation \(\mathbb{E}_{p^{\bullet}(s_{0:T})}[\log p_{\theta}(s_{0:T})]\)._

_If we further define a reward function \(r_{\alpha}(s_{t+1},a_{t},s_{0:t}):=r_{\alpha}(s_{t+1},s_{0:t})+\log p_{\alpha} (a_{t}|s_{0:t})\) to construct a \(Q\) function for \(p_{\alpha}\)_

\[Q^{p_{\alpha}}(a_{t};s_{0:t}):=\mathbb{E}_{p^{\bullet}(s_{t+1}|s_{0:t})}\left[ r_{\alpha}(s_{t+1},a_{t},s_{0:t})+V^{p_{\alpha}}(s_{0:t+1})\right].\]

_The expected return of \(Q^{p_{\alpha}}(a_{t};s_{0:t})\) forms an alternative objective_

\[\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}[Q^{p_{\alpha}}(a_{t};s_{0:t})]=V^{p_{ \alpha}}(s_{0:t})-\mathcal{H}_{\alpha}(a_{t}|s_{0:t})-\sum_{k=t+1}^{T-1} \mathbb{E}_{p^{\bullet}(s_{t+1:k}|s_{0:t})}[\mathcal{H}_{\alpha}(a_{k}|s_{0:k })]\]

_that yields the same optimal policy, for which the optimal \(Q^{\bullet}(a_{t};s_{0:t})\) can be the energy function._

_Only under certain conditions, this sequential decision-making problem is solvable through non-Markovian extensions of the maximum entropy reinforcement learning algorithms._

Proof.: See Appx B. 

The constructive proof above offers profound insights. By starting with the hypothesis of latent actions and MLE, and then considering known transition and accessible ground-truth conditional state distribution, we witness the _automatic emergence_ of the entire family of maximum entropy (inverse) RL. This includes prominent algorithms such as soft policy iteration [20], soft Q learning [22] and soft Actor-Critic (SAC) [23]. Among them, SAC is the best-performing off-policy RL algorithm in practice. Unlike the formulation with joint state-action distribution [20, 24], our formulation avoids the peculiarities associated with maximizing state transition entropy. The choice of the maximum entropy policy aligns naturally with the objective of capturing uncertainty in latent actions, and it offers inherent advantages for exploration in model-free learning [42, 22].

### Model-based planning with posterior sampling

Lastly, with the learned model, we can do posterior sampling given any complete or incomplete state sequences. The computation involved is analogous to model-based planning. In Section 3.3, we introduce posterior sampling with short-run MCMC and importance sampling when we have the target next state, which generalizes all cases where the targets of immediate subsequent states are given. Here we introduce the complementary case, where the goal state \(s_{T}\) is given as the target.

The posterior of actions given the sequential context \(s_{0:t}\) and a target goal state \(s_{T}\) is

\[p_{\theta}(a_{t:T}|s_{0:t},s_{T})\times p_{\theta}(a_{t:T},s_{T}|s_ {0:t})\] (15) \[= \int\prod\nolimits_{k=0}^{T-t-1}\left[p_{\beta}(s_{t+k+1}|a_{t+k},s_{t+k})p_{\alpha}(a_{t+k}|s_{0:t+k})\right]p_{\beta}(s_{T}|a_{T-1},s_{T-1}) ds_{t+1:T-1},\]

in which all Gaussian expectation \(\mathbb{E}_{p_{\beta}}[\cdot]\) can be approximated with the mean [43]. Therefore, \(a_{t:T}\) can be sampled via short-run MCMC with \(\nabla_{a_{t:T}}\log p_{\theta}(a_{t:T},s_{T}|s_{0:t})\) back propagated through time. The learned prior can be used to initialize these samples and facilitate the MCMC mixing.

## 5 Experiments

### Cubic curve planning

To demonstrate the necessity of non-Markovian value and test the efficacy of the proposed model, we designed a motivating experiment. Path planning is a prototypical decision-making problem, in which actions are taken in a 2D space, with the x-y coordinates as states. To simplify the problem without loss of generality, we can further assume \(x_{t}\) to change with constant speed \(h\), such that the action is \(\Delta y_{t}\). Obviously, the transition model \((x_{t+1},y_{t+1})=(x_{t}+h,y_{t}+\Delta y_{t})\) is Markovian.

Path planning can have various objectives. Imagining you are a passenger of an autonomous driving vehicle. You would not only care about whether the vehicle reaches the goal without collision but also how comfortable you feel. To obtain comforting smoothness and curvature, consider \(y\) is constrained to be a cubic polynomial \(F(x)=ax^{3}+bx^{2}+cx+d\) of \(x\), where \((a,b,c,d)\) are polynomial coefficients. Then the policy for this decision-making problem is non-Markovian.

To see that, suppose we are at \((x_{t},y_{t})\) at this moment, and the next state should be \((x_{t}+h,F(x_{t}+h))\). With Taylor expansion, we know \(F(x_{t}+h)\approx F(x_{t})+F^{\prime}(x_{t})h+\frac{F^{\prime\prime}(x_{t})}{2!}h^{2}+\frac{F^{\prime\prime\prime}(x_{t})}{3!}h^{3}\), so we can have a representation for the policy, \(\pi(\Delta y_{t}|x_{t},y_{t})=F^{\prime}(x_{t})h+\frac{F^{\prime\prime}(x_{t} )}{2!}h^{2}+\frac{F^{\prime\prime\prime}(x_{t})}{3!}h^{3}\). However, our representation of state only gives us \((x_{t},y_{t})\), so we will need to estimate those derivatives. This can be done with the finite difference method if we happen to remember the previous states \((x_{t-1},y_{t-1})\),..., \((x_{t-3},y_{t-3})\). Taking the highest order derivative for example, \(F^{\prime\prime\prime}(x_{t})=(y_{t}-3y_{t-1}+3y_{t-2}-y_{t-3})/h^{3}\). It is thus apparent that the policy would not be possibly represented if we are Markovian or don't remember sufficiently many prior states.

This representation of policy is what models should learn through imitation. However, they should not know the polynomial structure a priori. Given a sufficient number of demonstrations with different combinations of polynomial coefficients, models are expected to discover this rule by themselves. This experiment is a minimum viable prototype for general non-Markovian decision-making. It can be easily extended to higher-order and higher-dimensional state sequences.

SettingWe employ multi-layer perception (MLP) for this experiment. Demonstrations can be generated by rejection sampling. We constrain the demonstration trajectories to the \((x,y)\in(-1,1)\times(-1,1)\) area, and randomly select \(y\) and \(y^{\prime}\) at \(x=-1\) and \(x=1\). Curves with third-order coefficients less than 1 are rejected. Otherwise, the models may be confused in learning the cubic characteristics.

Non-Markovian dependency and latent energy-based policy are two prominent features of the proposed model. To test the causal role of non-Markovianness, we experiment with context length \(\{1,2,4,6\}\). Context length refers to how many prior states the policy is conditioned on. When it is 1, the policy is Markovian. From our analysis above, we know that context length 4 should be the ground truth, which helps categorize context lengths 2 and 6 into insufficient and excessive expressivity. With these four context lengths, we also train Behavior Cloning (BC) models as the control group. In a deterministic environment, there should not be a difference between BC and BCO, as the latter basically employs inverse dynamics to recover action labels. For our model, this simple transition can either be learned or implanted. Empirically, we don't notice a significant difference.

Performance is evaluated both qualitatively and quantitatively. As a 2D planning task, a visualization of the planned curves says a thousand words. In our experiment, we take \(h=0.1\), so the planned paths are rather discretized. We use mean squared error to fit a cubic polynomial and use the residual error as a metric. When calculating the residual error, we exclude those with a third-order coefficient is less than 0.5. Actually, the acceptance rate itself is also a viable metric. It is the number of accepted trajectories divided by the total number of testing trajectories. It is complementary to the residual error because it directly measures the understanding of cubic polynomials.

ResultsFig. 2(a-c) show paths generated with LanMDP after training for 3000 steps. They have context lengths 1, 2, 4 respectively. Compared with demonstrations in Fig. 2(d), only paths from the policy with context length 4 exhibit cubic characteristics. The Markovian policy totally fails this task. But it still generates curves, rather than straight lines from Markovian BC (see Fig. A1). The policy with context length 2 can plan cubic-like curves at times. But some of its generated paths are very different from demonstrations. To investigate this interesting phenomenon, we plot the training curves in Fig. 2(e)(f). While LanMDP policies with sufficient and excessive expressivity achieve high acceptance rates at the very beginning of the training, policies with Markovian and insufficient expressivity struggle to generate expected curves at the same time. Remarkably, as training goes by, the policy with context length 2, which can only approximate the ground-truth action in the first order, gradually improves in acceptance rate and residual error. This observation is consistent with Fig. 2(b).

Continuing our investigation, we plot curves generated by its BC counterparts in Fig. 2(g) but only see straight lines like the Markovian BC. Therefore, we conjecture that the LanMDP policy with length context 2 leverages its energy-based multi-modality to capture the uncertainty induced by marginalizing part of the necessary contexts. The second-order error in Taylor expansion is possibly remedied by this, especially after long-run training. The Markovian LanMDP policy, however, fails to unlock such potential because it cannot even figure out the first-order derivative.

There are some other note-worthy observations. (i) Excessive expressivity does not impair performance, it just requires more training. As shown in Fig. 2(e)(f), at the end of training, LanMDP policies with context length 6 perform as well as ones with context length 4. This demonstrates LanMDP's potential in inducing proper state abstraction from sequential contexts. TD learning, however, has been shown to be incapable of such abstraction in a prior work [44]. (ii) BC policies with sufficient contexts do not perform as well as LanMDP, as shown in Fig. 2(e)(f). We conjecture that this might be attributed to the larger compounding error in BC. To shield the influence of compounding errors, we design an experiment where we measure the residual error of the next state after filling the historical contexts in the learned LanMDP context 4 and BC context 4 with expert states, rather than sampled states. The errors are both around 0.0004 for LanMDP and BC, closing the gap in Fig. 2f. The implication seems to be LanMDP is more robust to compounding errors than BC.

Figure 2: **Results for cubic curve generation. (a-c) show curves generated at training step 3000 with context lengths 1, 2, 4. Starting points are randomly selected, and all following are sampled from the policy model. Only models with context length 4 learn the cubic characteristic. (d) shows curves from demonstrations. (e) and (f) present the smoothed acceptance rate and fitting residual of trajectories from policies with context lengths 1, 2, 4, 6. The x-axis is the training steps. (e)(f) are better to be viewed together because residual errors will only be calculated if the acceptance rate is above a threshold. For context length 1, the acceptance rate is always zero for BC, so it is not plotted here. (g) shows curves planned by BC with context length 2. It can be compared with (b). Interestingly, LanMDP with context length 2 demonstrates certain cubic characteristics when trained sufficiently long, while the BC counterpart only plans straight lines. (h) is the result of goal-reaching planning, where the dashed line comes from a hand-designed Markov reward, the solid line from the trained LanMDP.**

To verify our analysis in Section 4, we visualize the non-Markovian value function defined in Theorem 1 in Fig. 3. The value increases monotonically when the policy generates the cubic curve step by step. In an animation we included on the project homepage1, we further show that the action sampling at each state yields the highest value in reachable next states.

Footnote 1: https://sites.google.com/view/non-markovian-decision-making

At last, we study repurposing the learned sequence model for goal-reaching. This is inspired by a surprising phenomenon, over-imitation, from psychology. Over-imitation occurs when imitators copy actions unnecessary for goal-reaching. In a seminal study [25], 3- to 4-year-old children and young chimpanzees were presented with a puzzle box containing a hidden treat. An experimenter demonstrated a goal-reaching sequence with both causally necessary and unnecessary actions. When the box was opaque, both chimpanzees and children tended to copy all actions. However, when a transparent box was used such that the causal mechanisms became apparent, chimpanzees omitted unnecessary actions, while human children imitated them. As shown in Fig. 2(h), planning with the learned non-Markovian value indeed leads to casually unnecessary states, consistent with the demonstrations. Planning with designed Markov rewards produces causally shortest paths.

### Mujoco control tasks

We also report the empirical results of our model and baseline models on MuJoCo control tasks: Cartpole-v1, Reacher-v2, Swimmer-v3, Hopper-v2 and Walker2d-v2. We train an expert for each task using PPO [45]. They are then used to generate 10 trajectories for each task as demonstrations. Actions are deleted in the state-only setting.

SettingWe conduct a comparative analysis of LanMDP against several established imitation learning baselines including BC [46], BCO [37], GAIL [35], GAIFO [7], and OPOLO [38]. Note that BC and GAIL have access to action labels, positioning them as the control group. The experimental group includes state-only methods such as LanMDP, BCO, GAIFO, and OPOLO. The expert is the idealized baseline. For all tasks, we adopt the MLP architecture for both transition and policy. The input and output dimensions are adapted to the state and action spaces in different tasks, and so are short-run sampling steps. Sequential contexts are extracted from stored episodic memory. The number of neurons in the input and hidden layer in the policy MLP varies according to the context length. We use replay buffers to store the self-interaction experiences for training the transition model offline. See Appendix D for detailed information on network architectures and hyper-parameters.

ResultsResults for context length 1 are illustrated through learning curves and a bar plot in Fig. 4. These learning curves are the average progress across 5 seeds. Scores in the bar plot are normalized relative to the expert score. Our model demonstrates significantly steeper learning curves compared to the state-only GAIFO baselines, especially in Cartpole and Walker2d. This illustrates the remarkable data efficiency of model-based methods. Additionally, LanMDP consistently matches or surpasses the performance of BC and GAIL, despite the latter having access to action labels. In comparison to the expert, LanMDP only lags behind in the most complex Walker2d task. However, it still maintains a noticeable margin over other state-only baselines.

Figure 3: **Mapping a generated curve to a trajectory in the value landscape. We train a neural network to approximate the non-Markovian value function constructed with the learned policy and transition following Theorem 1, and then visualize the landscape by projecting all history-augmented states to a 2D space with a top view (left) and a front view (middle). Starting from a random initial state, decisions are sequentially made according to the learned policy, leaving a curve in the original state space (right) and a trajectory on the value landscape. It is evident that the non-Markovian value increases monotonically along the trajectory.**

Results for longer context lengths, _i.e_. the non-Markovian setting, are reported in Table 1, in which the highest return across the training process is listed. Originally invented for studying differentiable dynamics, MuJoCo offers state features that are inherently Markovian. Though a MDP is sufficiently expressive, learning a more generalized nMDP does not impair the performance. Sometimes it can even improve a little bit. Due to the limit of time, the maximum context length is only 3. Within the investigated regime, our result is consistent with that reported by Janner et al. [12]. We leave the experiments with longer memory and more sophisticated neural networks to future research.

Table 2 is a study of the computational overhead for the sampling techniques involved. The short-run MCMC for posterior inference takes longer than a single step of gradient descent. Replacing it with the proposed importance sampling improves training efficiency by a large margin.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Task & context 3 & context 2 & context 1 & BC \\ \hline CartPole & **500.00\(\pm\)0.00** & **500.00\(\pm\)0.00** & **500.00\(\pm\)0.00** & 474.80\(\pm\)18.87 \\ Reacher & -10.91\(\pm\)0.73 & -9.70\(\pm\)0.64 & -9.00\(\pm\)0.87 & **-8.76\(\pm\)0.12** \\ Swimmer & **42.67\(\pm\)4.66** & **43.52\(\pm\)4.31** & 41.22\(\pm\)2.67 & 38.64\(\pm\)1.76 \\ Hopper & 3051.16\(\pm\)111.78 & 3053.91\(\pm\)176.5 & 3045.27\(\pm\)240.45 & **3083.32\(\pm\)156.61** \\ Walker2d & 1703.02\(\pm\)228.86 & **1811.77\(\pm\)369.54** & 1753.46\(\pm\)193.69 & **1839.94\(\pm\)376.87** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison between Markovian and non-Markovian policy in MuJoCo control task.** Context length is the number of prior sequential states that the policy depends on, with the current one included. Recall that these MuJoCo tasks are inherently Markovian, thanks to highly specified state features. Nevertheless, non-Markovian policies perform on par with Markovian ones and BC, despite having higher expressivity than sufficient. The best and the second-best results are highlighted. Results are averaged over 5 random seeds.

Figure 4: **Results in MuJoCo.** for our LanMDP (red), BC (orange), BCO (green), GAIL (purple), GAIFO (cyan), OPOLO (gray), expert (blue). The learning curves are obtained by averaging progress over 5 seeds. We only plot curves for interactive learning methods. The scores of all other methods are plotted as horizontal lines. LanMDP does not have performance scores in the first \(K\) steps because this data is collected with random policy to fill the replay buffer, which is then used to train the transition model. \(K=0\) for Cartpole, \(2e4\) for Reacher and Swimmer, \(2e5\) for Hopper and Walker2d. We include these steps for fair comparisons. LanMDP outperforms existing state-only methods and matches BC, the best-performing state-action counterpart. The bar plot presents the scores from the best-performing policy during the training process, averaged across 5 seeds and normalized with the expert mean. The score in Reacher is offset by a constant before the division of the expert mean to align with the positive scores in all other tasks. The expert mean is plotted as a horizontal line. Our model clearly stands out in state-only methods, while matching and even outperforming those with action labels. Its scores only lag behind the expert mean in the most complex task. Better viewed in color.

## 6 Discussion

Related work in imitation learningEarliest works in imitation learning utilized BC [3; 47]. When the training data is limited, temporal drifting in trajectories [48; 49] may occur, which led to the development of IRL [6; 50; 51; 4; 34; 35]. In recent years, the availability of abundant sequence/video data is not the primary concern, but rather the difficulty in obtaining action labels. There has since been increasing attention in ILfO [52; 53; 7; 38; 8], a setting similar to ours. Distinguished from existing ILfO solutions, our model probabilistically describes the entire trajectory. In particular, the energy-based model [54; 55] in the latent policy space [16] has been relatively unexplored. Additionally, the capability for model-based planning is also a novel contribution.

Limitation and potential impactThe proposed model factorizes the joint distribution of state-action sequences into a time-invariant causal transition and a latent policy modulated by sequential contexts. While this model requires sampling methods, and can be non-negligible for higher-dimensional actions, it is worth noting that action quantization, as employed in transformer-based models [12; 13], has the potential to reduce the computation overhead. In our experiments, a measure of the diversity of behavior is omitted, similar to other works in the literature of reinforcement learning. However, it deserves further investigation since multi-modal density matching is a crucial metric in generative modeling. Importantly, our training objective and analysis are independent of specific modeling and sampling techniques, as long as the state transition remains time-invariant. Given the ability of neural networks to learn approximate invariance through data augmentation [56; 57; 58; 59], we anticipate that our work will inspire novel training and inference techniques for monolithic sequential decision-making models [12; 13; 14; 15].

Implications in neuroscience and psychologyThe proposed latent model is an amenable framework for studying the emergent patterns in the mirror neurons [60; 61], echoing recent studies in grid cells and place cells [62; 63]. When the latent action is interpreted as an internal intention, the inference process is a manifestation of Theory of Mind (ToM) [64]. The phenomenon of over-imitation [25; 26; 65] can also be relevant. As shown in Section 5, although the proposed model learns a causal transition and hence understands causality, when repurposed for goal-reaching tasks, the learned non-Markovian value can result in "unnecessary" state visitation. It would be interesting to explore if over-imitation is simply an overfitting due to excessive expressivity in sequence models.

## 7 Conclusion

In this study, we explore deep generative modeling of state-only sequences in non-Markovian domains. We propose a model, LanMDP, in which the policy functions as an energy-based prior within the latent space of the state transition generator. This model learns by EM-style maximum likelihood estimation. Additionally, we demonstrate the existence of a decision-making problem inherent in such probabilistic inference, providing a fresh perspective on maximum entropy reinforcement learning. To showcase the importance of non-Markovian dependency and evaluate the effectiveness of our proposed model, we introduce a specific experiment called cubic curve planning. Our empirical results also demonstrate the robust performance of LanMDP across the MuJoCo suite.

AcknowledgmentAQ, QL and SZ are supported by the National Key R&D Program of China (2021ZD0150200). FG contributed to this work during his PhD study at UCLA. SX is supported by a Teaching Assistantship from UCLA CS. The authors thank Prof. Ying Nian Wu at UCLA, Baoxiong

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Task & \(a\) dim & \(s\) dim & Architecture & 10 steps & 50 steps \\ \hline Reacher & 2 & 11 & MLP(150;4) & 0.0108/0.0076/0.0014 & 0.0480/0.0350/0.0014 \\ Swimmer & 2 & 8 & MLP(150;4) & 0.0100/0.0071/0.0014 & 0.0463/0.0340/0.0014 \\ Hopper & 3 & 12 & MLP(512;4) & 0.0268/0.0170/0.0074 & 0.1403/0.0836/0.0073 \\ Walker2d & 6 & 18 & MLP(512;4) & 0.0282/0.0184/0.0077 & 0.1487/0.0899/0.0076 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Computational overheads for posterior sampling, importance sampling, and gradient descent (in seconds) in one training step. MLP(\(n\),\(m\)) means that we implement the policy model as an MLP with \(m\) layers and \(n\) hidden neurons each layer. Results are averaged over 10 epochs. The number of MCMC steps is set to 10, 50 respectively. Replacing posterior sampling with importance sampling improves training efficiency.

Jia and Xiaojian Ma at BIGAI for their useful discussion. The authors would also like to thank the reviewers for their valuable feedback.

## References

* Di Pellegrino et al. [1992] Giuseppe Di Pellegrino, Luciano Fadiga, Leonardo Fogassi, Vittorio Gallese, and Giacomo Rizzolatti. Understanding motor events: a neurophysiological study. _Experimental brain research_, 91:176-180, 1992.
* Rizzolatti et al. [2001] Giacomo Rizzolatti, Leonardo Fogassi, and Vittorio Gallese. Neurophysiological mechanisms underlying the understanding and imitation of action. _Nature reviews neuroscience_, 2(9):661-670, 2001.
* Hayes and Demiris [1994] Gillian M Hayes and John Demiris. _A robot controller using learning by imitation_. University of Edinburgh, Department of Artificial Intelligence, 1994.
* Ziebart et al. [2008] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In _Aaai_, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Ng and Russell [2000] Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In _Icml_, volume 1, page 2, 2000.
* Torabi et al. [2018] Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. _arXiv preprint arXiv:1807.06158_, 2018.
* Sun et al. [2019] Wen Sun, Anirudh Vemula, Byron Boots, and Drew Bagnell. Provably efficient imitation learning from observation alone. In _International conference on machine learning_, pages 6036-6045. PMLR, 2019.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Abel et al. [2021] David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and Satinder Singh. On the expressivity of markov reward. _Advances in Neural Information Processing Systems_, 34:7799-7812, 2021.
* Janner et al. [2021] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286, 2021.
* Chen et al. [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* Janner et al. [2022] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In _International Conference on Machine Learning_, pages 9902-9915. PMLR, 2022.
* Ajay et al. [2023] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=sP1fo2K9DFG.
* Pang et al. [2020] Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space energy-based prior model. _Advances in Neural Information Processing Systems_, 33:21994-22008, 2020.

* [17] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run mcmc toward energy-based model. _Advances in Neural Information Processing Systems_, 32, 2019.
* [18] Hagai Attias. Planning by probabilistic inference. In _International workshop on artificial intelligence and statistics_, pages 9-16. PMLR, 2003.
* [19] Matthew Botvinick and Marc Toussaint. Planning as inference. _Trends in cognitive sciences_, 16(10):485-488, 2012.
* [20] Brian D Ziebart. _Modeling purposeful adaptive behavior with the principle of maximum causal entropy_. Carnegie Mellon University, 2010.
* [21] Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. In _32nd Conference on Uncertainty in Artificial Intelligence 2016, UAI 2016_, pages 202-211. Association For Uncertainty in Artificial Intelligence (AUAI), 2016.
* [22] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In _International conference on machine learning_, pages 1352-1361. PMLR, 2017.
* [23] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* [24] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. _arXiv preprint arXiv:1805.00909_, 2018.
* [25] Victoria Horner and Andrew Whiten. Causal knowledge and imitation/emulation switching in chimpanzees (pan troglodytes) and children (homo sapiens). _Animal cognition_, 8:164-181, 2005.
* [26] Derek E Lyons, Andrew G Young, and Frank C Keil. The hidden structure of overimitation. _Proceedings of the National Academy of Sciences_, 104(50):19751-19756, 2007.
* [27] Emanuel Todorov. Linearly-solvable markov decision problems. _Advances in neural information processing systems_, 19, 2006.
* [28] Ronen I Brafman and Giuseppe De Giacomo. Regular decision processes: A model for non-markovian domains. In _IJCAI_, pages 5516-5522, 2019.
* [29] Alessandro Ronca, Gabriel Paludo Licks, and Giuseppe De Giacomo. Markov abstractions for pac reinforcement learning in non-markov decision processes. _arXiv preprint arXiv:2205.01053_, 2022.
* [30] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. _Artificial intelligence_, 101(1-2):99-134, 1998.
* [31] Marcus Hutter. Feature reinforcement learning: Part i. unstructured mdps. _Journal of Artificial General Intelligence_, 1(1):3-24, 2009.
* [32] Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, and Sheila McIlraith. Learning reward machines for partially observable reinforcement learning. _Advances in neural information processing systems_, 32, 2019.
* [33] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In _International Conference on Machine Learning_, pages 2107-2116. PMLR, 2018.
* [34] Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. _arXiv preprint arXiv:1611.03852_, 2016.
* [35] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. _Advances in neural information processing systems_, 29, 2016.

* [36] Cristiano Varin, Nancy Reid, and David Firth. An overview of composite likelihood methods. _Statistica Sinica_, pages 5-42, 2011.
* [37] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. _arXiv preprint arXiv:1805.01954_, 2018.
* [38] Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-policy imitation learning from observations. _Advances in Neural Information Processing Systems_, 33:12402-12413, 2020.
* [39] Emanuel Todorov. General duality between optimal control and estimation. In _2008 47th IEEE Conference on Decision and Control_, pages 4286-4292. IEEE, 2008.
* [40] Marc Toussaint. Robot trajectory optimization using approximate inference. In _Proceedings of the 26th annual international conference on machine learning_, pages 1049-1056, 2009.
* [41] Hilbert J Kappen, Vicenc Gomez, and Manfred Opper. Optimal control as a graphical model inference problem. _Machine learning_, 87:159-182, 2012.
* [42] Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Machine learning proceedings 1990_, pages 216-224. Elsevier, 1990.
* [43] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [44] Jonathan Ferrer-Mestres, Thomas G Dietterich, Olivier Buffet, and Iadine Chades. Solving k-mdps. In _Proceedings of the International Conference on Automated Planning and Scheduling_, volume 30, pages 110-118, 2020.
* [45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [46] Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 661-668. JMLR Workshop and Conference Proceedings, 2010.
* [47] Ramesh Amit and Maja Matari. Learning movement sequences from demonstration. In _Proceedings 2nd International Conference on Development and Learning. ICDL 2002_, pages 203-208. IEEE, 2002.
* [48] Christopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In _ICML_, volume 97, pages 12-20. Citeseer, 1997.
* [49] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. _Robotics and autonomous systems_, 57(5):469-483, 2009.
* [50] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the 21st international conference on Machine learning_, page 1, 2004.
* [51] Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. Maximum margin planning. In _Proceedings of the 23rd international conference on Machine learning_, pages 729-736, 2006.
* [52] Rahul Kidambi, Jonathan Chang, and Wen Sun. Mobile: Model-based imitation learning from observation alone. _arXiv preprint arXiv:2102.10769_, 2021.
* [53] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1118-1125. IEEE, 2018.
* [54] Song Chun Zhu, Yingnian Wu, and David Mumford. Filters, random fields and maximum entropy (frame): Towards a unified theory for texture modeling. _International Journal of Computer Vision_, 27(2):107-126, 1998.
* [55] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.

* [56] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* [57] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [58] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. _Advances in neural information processing systems_, 33:19884-19895, 2020.
* [59] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for offline reinforcement learning in robotics. In _Conference on Robot Learning_, pages 907-917. PMLR, 2022.
* [60] Richard Cook, Geoffrey Bird, Caroline Catmur, Clare Press, and Cecilia Heyes. Mirror neurons: from origin to function. _Behavioral and brain sciences_, 37(2):177-192, 2014.
* [61] Cecilia Heyes and Caroline Catmur. What happened to mirror neurons? _Perspectives on Psychological Science_, 17(1):153-168, 2022.
* [62] Ruiqi Gao, Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning grid cells as vector representation of self-position coupled with matrix representation of self-motion. _arXiv preprint arXiv:1810.05597_, 2018.
* [63] Rajkumar Vasudeva Raju, J Swaroop Guntupalli, Guangyao Zhou, Miguel Lazaro-Gredilla, and Dileep George. Space is a latent sequence: Structured sequence learning as a unified theory of representation in the hippocampus. _arXiv preprint arXiv:2212.01508_, 2022.
* [64] Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. Action understanding as inverse planning. _Cognition_, 113(3):329-349, 2009.
* [65] Michael Tomasello. _Becoming human: A theory of ontogeny_. Harvard University Press, 2019.
* [66] Edwin T Jaynes. Information theory and statistical mechanics. _Physical review_, 106(4):620, 1957.
* [67] Sultan Javed Majeed and Marcus Hutter. On q-learning convergence for non-markov decision processes. In _IJCAI_, volume 18, pages 2546-2552, 2018.
* [68] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. _Advances in neural information processing systems_, 33:21810-21823, 2020.
* [69] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. _arXiv preprint arXiv:1802.10592_, 2018.
* [70] Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. _arXiv preprint arXiv:1807.03858_, 2018.
* [71] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 7559-7566. IEEE, 2018.
* [72] Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model based reinforcement learning. In _International conference on machine learning_, pages 7953-7963. PMLR, 2020.
* [73] Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. _arXiv preprint arXiv:1912.03263_, 2019.

* Du and Mordatch [2019] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Florence et al. [2022] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In _Conference on Robot Learning_, pages 158-168. PMLR, 2022.

Learning and Sampling

### Deep generative modelling

A complete trajectory is denoted by

\[\zeta=\{s_{0},a_{0},s_{1},a_{1},\cdots,a_{T-1},s_{T}\},\] (1)

where \(T\) is the maximum length of all observed trajectories. The joint distribution of state and action sequences can be factorized according to the causal assumptions in nMDP:

\[\begin{split} p_{\theta}(\zeta)&=p(s_{0})p_{\alpha }(a_{0}|s_{0})p_{\beta}(s_{1}|s_{0},a_{0})\cdots p_{\alpha}(a_{T-1}|s_{0:T-1}) p_{\beta}(s_{T}|s_{T-1},a_{T-1})\\ &=p(s_{0})\prod_{t=0}^{T-1}p_{\alpha}(a_{t}|s_{0:t})p_{\beta}(s_{t +1}|s_{t},a_{t}),\end{split}\] (2)

where \(p_{\alpha}(a_{t}|s_{0:t-1})\) is the policy model with parameter \(\alpha\), \(p_{\beta}(s_{t}|s_{t-1},a_{t-1})\) is the transition model with parameter \(\beta\), both of which are parameterized with neural networks, \(\theta=(\alpha,\beta)\). \(p(s_{0})\) is the initial state distribution, which can be sampled as a black box.

The density families of policy and transition are consistent with the conventional setting of IRL [4]. The transition describes the predictable change in state space, which is often possible to express the random variable \(s_{t+1}\) as a deterministic variable \(s_{t+1}=g_{\beta}(s_{t},a_{t},\epsilon)\), where \(\epsilon\) is an auxiliary variable with independent marginal \(p(\epsilon)\), and \(g_{\beta}(.)\) is some vector-valued function parameterized by \(\beta\). The policy accounts for bounded rationality as a Boltzmann distribution with state-action value as the unnormalized energy:

\[p_{\alpha}(a_{t}|s_{0:t})=\frac{1}{Z(\alpha,s_{0:t})}\exp{(f_{\alpha}(a_{t};s_ {0:t}))},\] (3)

where \(f_{\alpha}(a_{t};s_{0:t})\) is the negative energy, \(Z(\alpha,s_{0:t})=\int\exp(f_{\alpha}(a_{t};s_{0:t}))da_{t}\) is the normalizing constant given the history \(s_{0:t}\).

Since we can only observe state sequences, the aforementioned generative model can be understood as a sequential variant of LEBM [16], where the transition serves as the generator and the policy is a history-conditioned latent prior. The marginal distribution of state sequences and the posterior distribution of action sequences are:

\[p_{\theta}(s_{0:T})=\int p_{\theta}(s_{0:T},a_{0:T-1})da_{0:T-1},\quad p_{ \theta}(a_{0:T-1}|s_{0:T})=\frac{p_{\theta}(s_{0:T},a_{0:T-1})}{p_{\theta}(s_ {0:T})}.\] (4)

### Maximum likelihood learning

We need to estimate \(\theta=(\alpha,\beta)\). Suppose we observe training examples: \(\{\xi^{i}\},i=1,2,\cdots,n,\quad\xi^{i}=[s^{i}_{0},s^{i}_{1},...,s^{i}_{T}]\). The log-likelihood function is:

\[L(\theta)=\sum_{i=1}^{n}\log p_{\theta}(\xi^{i}).\] (5)

Denote posterior distribution of action sequence \(p_{\theta}(a_{0:T-1}|s_{0:T})\) as \(p_{\theta}(A|S)\) for convenience where \(A\) and \(S\) means the complete action and state sequences in a trajectory. The gradient of log-likelihood is:

\[\begin{split}\nabla_{\theta}\log p_{\theta}(\xi)&= \nabla_{\theta}\log p_{\theta}(s_{0},s_{1},\cdots,s_{T})\\ &=\mathbb{E}_{p_{\theta}(A|S)}[\nabla_{\theta}\log p_{\theta}(s_{ 0},s_{1},\cdots,s_{T})]\\ &=\mathbb{E}_{p_{\theta}(A|S)}[\nabla_{\theta}\log p_{\theta}(s_ {0},s_{1},\cdots,s_{T})]+\mathbb{E}_{p_{\theta}(A|S)}[\nabla_{\theta}\log p_{ \theta}(A|S)]\\ &=\mathbb{E}_{p_{\theta}(A|S)}[\nabla_{\theta}\log p_{\theta}(s_ {0},a_{0},s_{1},a_{1},\cdots,a_{T-1},s_{T})]\\ &=\mathbb{E}_{p_{\theta}(A|S)}[\nabla_{\theta}\log p(s_{0})p_{ \alpha}(a_{0}|s_{0})\cdots p_{\alpha}(a_{T-1}|s_{0:T-1})p_{\beta}(s_{T}|s_{T-1 },a_{T-1})]\\ &=\mathbb{E}_{p_{\theta}(A|S)}[\nabla_{\theta}\sum_{t=0}^{T-1}( \log p_{\alpha}(a_{t}|s_{0:t})+\log p_{\beta}(s_{t+1}|s_{t},a_{t}))]\\ &=\mathbb{E}_{p_{\theta}(A|S)}[\sum_{t=0}^{T-1}(\underbrace{ \nabla_{\alpha}\log p_{\alpha}(a_{t}|s_{0:t})}_{\text{policy/prior}}+ \underbrace{\nabla_{\beta}\log p_{\beta}(s_{t+1}|s_{t},a_{t})}_{\text{ transition}})],\end{split}\] (6)where the third equation is because of a simple identity \(\mathrm{E}_{\pi_{\theta}(a)}\left[\nabla_{\theta}\log\pi_{\theta}(a)\right]=0\) for any probability distribution \(\pi_{\theta}(a)\). Applying this simple identiy, we also have:

\[\begin{split} 0&=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})} \left[\nabla_{\alpha}\log p_{\alpha}(a_{t}|s_{0:t})\right]\\ &=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\nabla_{\alpha}f_{ \alpha}(a_{t};s_{0:t})-\nabla_{\alpha}\log Z(\alpha,s_{0:t})\right]\\ &=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\nabla_{\alpha}f_{ \alpha}(a_{t};s_{0:t})\right]-\nabla_{\alpha}\log Z(\alpha,s_{0:t}).\end{split}\] (7)

Due to the normalizing constant \(Z(\alpha,s_{0:t})\) in the energy-based prior \(p_{\alpha}\), the gradient for the policy term involves both posterior and prior samples:

\[\begin{split}\delta_{\alpha,t}(S)&=\mathbb{E}_{p_ {\theta}(A|S)}\left[\nabla_{\alpha}\log p_{\alpha}(a_{t}|s_{0:t})\right]\\ &=\mathbb{E}_{p_{\theta}(A|S)}\left[\nabla_{\alpha}f_{\alpha}(a_ {t};s_{0:t})-\nabla_{\alpha}\log Z(\alpha,s_{0:t})\right]\\ &=\mathbb{E}_{p_{\theta}(A|S)}\left[\nabla_{\alpha}f_{\alpha}(a_ {t};s_{0:t})-\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\nabla f_{\alpha}(a_{ t};s_{0:t})\right]\right]\\ &=\mathbb{E}_{p_{\theta}(A|S)}\left[\nabla_{\alpha}f_{\alpha}(a_ {t};s_{0:t})\right]-\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\nabla_{\alpha }f_{\alpha}(a_{t};s_{0:t})\right],\end{split}\] (8)

where \(\delta_{\alpha,t}(S)\) denotes the surrogate loss of policy term for time step \(t\). Intuition can be gained from the perspective of adversarial training [34, 35]: On one hand, the model utilizes action samples from the posterior \(p_{\theta}(A|S)\) as pseudo-labels to supervise the unnormalized prior at each step \(p_{\alpha}(a_{t}|s_{0:t})\). On the other hand, it discourages action samples directly sampled from the prior. The model converges when prior samples and posterior samples are indistinguishable.

To ensure the transition model's validity, it needs to be grounded in real-world dynamics when jointly learned with the policy. Otherwise, the agent would be purely hallucinating based on the demonstrations. Throughout the training process, we allow the agent to periodically collect self-interaction data with \(p_{\alpha}(a_{t}|s_{0:t})\) and mix transition data from two sources with weight \(w_{\beta}\):

\[\delta_{\beta,t}(S)=w_{\beta}\mathbb{E}_{p_{\theta}(A|S)}\left[\nabla_{\beta }\log p_{\beta}(s_{t+1}|s_{t},a_{t})\right]+(1-w_{\beta})\mathbb{E}_{p_{\alpha }(a_{t}|s_{0:t}),Tr}\left[\nabla_{\beta}\log p_{\beta}(s_{t+1}|s_{t},a_{t}) \right].\] (9)

### General transition model

We need to compute the gradient of \(\beta\) for the logarithm of transition probability in Equation 9, and as we will see in section 3.3, we also need to compute the gradient of the action during sampling actions. The reparameterization [43] is useful since it can be used to rewrite an expectation w.r.t \(p_{\beta}(s_{t+1}|s_{t},a_{t})\) such that the Monte Carlo estimate of the expectation is differentiable, so we use delta function \(\delta(.)\) to rewrite probability as an expectation:

\[\begin{split} p_{\beta}(s_{t+1}|s_{t},a_{t})&=\int \delta(s_{t+1}-s^{\prime}_{t+1})p_{\beta}(s^{\prime}_{t+1}|s_{t},a_{t})ds^{ \prime}_{t+1}\\ &=\int\delta(s_{t+1}-g_{\beta}(s_{t},a_{t},\epsilon))p(\epsilon) d\epsilon.\end{split}\] (10)

Taking advantage of the properties of \(\delta(.)\):

\[\int f(x)\delta(x)dx=f(0),\quad\delta(f(x))=\Sigma_{n}\frac{1}{|f^{\prime}(x_ {n})|}\delta(x-x_{n}),\] (11)

where \(f\) is differentiable and have isolated zeros, which is \(x_{n}\), we can rewrite the transition probability as:

\[\begin{split} p_{\beta}(s_{t+1}|s_{t},a_{t})&=\int \sum_{n}\frac{1}{|\frac{\partial}{\partial\epsilon}g_{\beta}(s_{t},a_{t}, \epsilon)|_{\epsilon=\epsilon_{n}}}\delta(\epsilon-\epsilon_{n})p(\epsilon)d \epsilon\\ &=\sum_{n}\frac{p(\epsilon_{n})}{|\frac{\partial}{\partial \epsilon}g_{\beta}(s_{t},a_{t},\epsilon)|_{\epsilon=\epsilon_{n}}},\end{split}\] (12)

where \(\epsilon_{n}\) is the zero of \(s_{t+1}=g_{\beta}(s_{t},a_{t},\epsilon)\). Therefore, if we have a differentiable simulator \(\nabla_{a_{t}}\log p_{\beta}(s_{t+1}|s_{t},a_{t})\) and the analytical form of \(p(\epsilon)\), then gradient of both \(a_{t}\) and \(\beta\) for \(\log p_{\beta}(s_{t+1}|s_{t},a_{t})\) can be computed.

The simplest situation is:

\[s_{t+1}=g_{\beta}(s_{t},a_{t})+\epsilon,\epsilon\sim p(\epsilon)=\mathcal{N}( 0,\sigma^{2}).\] (13)In this case, there is only one zero \(\epsilon^{*}\) for the transition function, \(s_{t+1}=g_{\beta}(s_{t},a_{t})+\epsilon^{*}\), and the gradient of log probability is:

\[\begin{split}\nabla\log p_{\beta}(s_{t+1}|s_{t},a_{t})& =\nabla\log\frac{p(\epsilon^{*})}{|\frac{\epsilon}{\epsilon_{c} }(g_{\beta}(s_{t},a_{t})+\epsilon)|_{\epsilon=\epsilon^{*}}}\\ &=\nabla\log p(\epsilon^{*})\\ &=\nabla\log p(s_{t+1}-g_{\beta}(s_{t},a_{t}))\\ &=\frac{1}{\sigma^{2}}(s_{t+1}-g_{\beta}(s_{t},a_{t}))\nabla g _{\beta}(s_{t},a_{t}).\end{split}\] (14)

### Prior and posterior sampling

The maximum likelihood estimation requires samples from the prior and the posterior distributions of actions. It would not be a problem if the action space is quantized. However, since we target general latent action learning, we proceed to introduce sampling techniques for continuous actions.

When sampling from a continuous energy space, short-run Langevin dynamics [17] can be an efficient choice. For a target distribution \(\pi(a)\), Langevin dynamics iterates \(a_{k+1}=a_{k}+s\nabla_{a_{k}}\log\pi(a_{k})+\sqrt{2s}\epsilon_{k}\), where \(k\) indexes the number of iteration, \(s\) is a small step size, and \(\epsilon_{k}\) is the Gaussian white noise. \(\pi(a)\) can be either the prior \(p_{\alpha}(a_{t}|s_{0:t})\) or the posterior \(p_{\theta}(A|S)\). One property of Langevin dynamics that is particularly amenable for EBM is that we can get rid of the normalizing constant. So for each \(t\) the iterative update for prior samples is

\[a_{t,k+1}=a_{t,k}+s\nabla_{a_{t,k}}f_{\alpha}(a_{t,k};s_{0:t})+\sqrt{2s} \epsilon_{k}.\] (15)

Given a state sequence \(s_{0:T}\) from the demonstrations, the posterior samples at each time step \(a_{t}\) come from the conditional distribution \(p(a_{t}|s_{0:T})\). Notice that with Markov transition, we can derive

\[p_{\theta}(a_{0:T-1}|s_{0:T})=\prod\nolimits_{t=0}^{T-1}p_{\theta}(a_{t}|s_{ 0:T})=\prod\nolimits_{t=0}^{T-1}p_{\theta}(a_{t}|s_{0:t+1}).\] (16)

The point is, given the previous and the next subsequent state, the posterior can be sampled at each step independently. So the posterior iterative update is

\[\begin{split} a_{t,k+1}&=a_{t,k}+s\nabla_{a_{t,k}} \log p_{\theta}(a_{t,k}|s_{0:t+1})+\sqrt{2s}\epsilon_{k}\\ &=a_{t,k}+s\nabla_{a_{t,k}}\log p_{\theta}(s_{0:t},a_{t,k},s_{t+ 1})+\sqrt{2s}\epsilon_{k}\\ &=a_{t,k}+s\nabla_{a_{t,k}}(\underbrace{\log p_{\alpha}(a_{t,k}| s_{0:t})}_{\text{policy/prior}})+\underbrace{\log p_{\beta}(s_{t+1}|s_{t},a_{t})}_{ \text{transition}})+\sqrt{2s}\epsilon_{k}.\end{split}\] (17)

Intuitively, action samples at each step are updated with the energy of all subsequent actions and a single-step forward by back-propagation. However, while gradients from the transition term are analogous to the inverse dynamics in BCO [37], it may lead to poor training performance due to non-injectiveness in forward dynamics [38].

We develop an alternative posterior sampling method with importance sampling to overcome this challenge. Leveraging the learned transition, we have

\[p_{\theta}(a_{t}|s_{0:t+1})=\frac{p_{\beta}(s_{t+1}|s_{t},a_{t})}{\mathbb{E}_ {p_{\alpha}(a_{t}|s_{0:t})}\left[p_{\beta}(s_{t+1}|s_{t},a_{t})\right]}p_{ \alpha}(a_{t}|s_{0:t}).\] (18)

Let \(c(a_{t};s_{0:t+1})=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[p_{\beta}(s_{t+ 1}|s_{t},a_{t})\right]\), posterior sampling from \(p_{\theta}(a_{0:T-1}|s_{0:T})\) can be realized by adjusting importance weights of independent samples from the prior \(p_{\alpha}(a_{t}|s_{0:t})\), in which the estimation of weights involves another prior sampling. In this way, we avoid back-propagating through non-injective dynamics and save some computation overhead in Eq. (17).

To train the policy, Eq. (8) can now be rewritten as

\[\delta_{\alpha,t}(S)=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\frac{p_{ \beta}(s_{t+1}|s_{t},a_{t})}{c(a_{t};s_{0:t+1})}\nabla_{\alpha}f_{\alpha}(a_ {t};s_{0:t})\right]-\mathbb{E}_{p_{\theta}(a_{t}|s_{0:t})}\left[\nabla_{\alpha }f_{\alpha}(a_{t};s_{0:t})\right].\] (19)

### Algorithm

The learning and sampling algorithms with MCMC and with importance sampling for posterior sampling are described in Algorithm 1 and Algorithm 2.

``` Input: Learning iterations \(N\), learning rate for energy-based policy \(\eta_{\alpha}\), learning rate for transition model \(\eta_{\beta}\), initial parameters \(\theta_{0}=(\alpha_{0},\beta_{0})\), expert demonstrations \(\{s_{0:H}\}\), context length \(L\), batch size \(m\), number of prior and posterior sampling steps \(\{K_{0},K_{1}\}\), prior and posterior sampling step sizes \(\{s_{0},s_{1}\}\). Output:\(\theta_{N}=(\alpha_{N},\beta_{N})\).  Reorganize \(\{s_{0:H}\}\) to to state sequenec segments \((s_{t-L+1},\cdots,s_{t+1})\) with length \(L+1\).  Use energy-based policy with \(\alpha_{0}\) collect transitions to fill in the replay buffer.  Use transitions in replay buffer to pre-train transition model \(\beta_{0}\). for\(t=0\)to\(N-1\)do Demo sampling Sample observed examples \(\left(s_{t-L+1},\cdots,s_{t+1}\right)_{i=1}^{m}\). Posterior sampling: Sample \(\{a_{t}\}_{i=1}^{m}\) using Eq. (17) with \(K_{1}\) iterations and stepsize \(s_{1}\). Prior sampling: Sample \(\{\hat{a}_{t}\}_{i=1}^{m}\) using Eq. (15) with \(K_{0}\) iterations and stepsize \(s_{0}\). Policy learning: Update \(\alpha_{t}\) to \(\alpha_{t+1}\) by Eq. (8) with learning rate \(\eta_{\alpha}\). Transition learning: Update replay buffer with trajectories from current policy model \(\alpha_{t+1}\), then update \(\beta_{t}\) to \(\beta_{t+1}\) by Eq. (9) with learning rate \(\eta_{\beta}\). endfor ```

**Algorithm 1**LanMDP without importance sampling

``` Input: Learning iterations \(N\), learning rate for energy-based policy \(\eta_{\alpha}\), learning rate for transition model \(\eta_{\beta}\), initial parameters \(\theta_{0}=(\alpha_{0},\beta_{0})\), expert demonstrations \(\{s_{0:H}\}\), context length \(L\), batch size \(m\), number of prior sampling steps \(K\) and step sizes \(s\). Output:\(\theta_{N}=(\alpha_{N},\beta_{N})\).  Reorganize \(\{s_{0:H}\}\) to to state sequenec segments \((s_{t-L+1},\cdots,s_{t+1})\) with length \(L+1\).  Use energy-based policy with \(\alpha_{0}\) collect transitions to fill in the replay buffer.  Use transitions in replay buffer to pre-train transition model \(\beta_{0}\). for\(t=0\)to\(N-1\)do Demo sampling Sample observed examples \(\left(s_{t-L+1},\cdots,s_{t+1}\right)_{i=1}^{m}\). Prior sampling: Sample \(\{\hat{a}_{t}\}_{i=1}^{m}\) using Eq. (15) with \(K_{0}\) iterations and stepsize \(s_{0}\). Policy learning: Update \(\alpha_{t}\) to \(\alpha_{t+1}\) by Eq. (19) with learning rate \(\eta_{\alpha}\). Transition learning: Update replay buffer with trajectories from current policy model \(\alpha_{t+1}\), then update \(\beta_{t}\) to \(\beta_{t+1}\) by Eq. (9) with learning rate \(\eta_{\beta}\). endfor ```

**Algorithm 2**LanMDP with importance sampling A Decision-making Problem in MLE

Let the ground-truth distribution of demonstrations be \(p*(s_{0:T})\), and the learned marginal distributions of state sequences be \(p_{\theta}(s_{0:T})\). Eq. (5) is an empirical estimate of

\[\mathbb{E}_{p^{\bullet}(s_{0:T})}[\log p_{\theta}(s_{0:T})]=\mathbb{E}_{p^{ \bullet}(s_{0})}\left[\log p^{*}(s_{0})+\mathbb{E}_{p^{\bullet}(s_{1:T}|s_{0}) }[\log p_{\theta}(s_{1:T}|s_{0})]\right].\] (20)

We can show that a sequential decision-making problem can be constructed to maximize the same objective. To start off, suppose the MLE yields the maximum, we will have \(p_{\theta}*=p^{*}\).

Define \(V*(s_{0}):=\mathbb{E}_{p^{\bullet}(s_{1:T}|s_{0})}[\log p*(s_{1:T}|s_{0})]\), we can generalize it to have a \(V\)_function_

\[V*(s_{0:t}):=\mathbb{E}_{p^{*}(s_{t+1:T}|s_{0:t})}[\log p^{*}(s_{t+1:T}|s_{0:t} )],\] (21)

which comes with a Bellman optimality equation:

\[V*(s_{0:t})=\mathbb{E}_{p^{\bullet}(s_{t+1}|s_{0:t})}\left[r(s_{t+1},s_{0:t})+ V^{*}(s_{0:t+1})\right],\] (22)

with \(r(s_{t+1},s_{0:t}):=\log p^{*}(s_{t+1}|s_{0:t})=\log\left\{p_{\alpha}*(a_{t}| s_{0:t})p_{\beta}*(s_{t+1}|s_{t},a_{t})da_{t}\right.\), \(V*(s_{0:T}):=0\). It is worth noting that the \(r\) defined above involves the optimal policy, which may not be known a priori. We can resolve this by replacing it with \(r_{\alpha}\) for an arbitrary policy \(p_{\alpha}(a_{t}|s_{0:t})\). All Bellman identities and updates should still hold. Anyways, involving the current policy in the reward function should not appear to be too odd given the popularity of maximum entropy RL [20, 24].

The entailed Bellman update, _value iteration_, for arbitrary \(V\) and \(\alpha\) is

\[V(s_{0:t})=\mathbb{E}_{p^{*}(s_{t+1}|s_{0:t})}\left[r_{\alpha}(s_{0:t},s_{t+1} )+V(s_{0:t+1})\right].\] (23)

We then define \(r(s_{t+1},a_{t},s_{0:t}):=r(s_{t+1},s_{0:t})+\log p_{\alpha}*(a_{t}|s_{0:t})\) to construct a \(Q\) function:

\[Q*(a_{t};s_{0:t}):=\mathbb{E}_{p^{*}(s_{t+1}|s_{0:t})}\left[r(s_{t+1},a_{t},s_ {0:t})+V^{*}(s_{0:t+1})\right],\] (24)

which entails a Bellman update, _Q backup_, for arbitrary \(\alpha\), \(Q\) and \(V\)

\[Q(a_{t};s_{0:t})=\mathbb{E}_{p^{*}(s_{t+1}|s_{0:t})}\left[r_{\alpha}(s_{0:t}, a_{t},s_{t+1})+V(s_{0:t+1})\right].\] (25)

Also note that the \(V\) and \(Q\) in identities Eq. (23) and Eq. (25) respectively are not necessarily associated with the policy \(p_{\alpha}(a_{t}|s_{0:t})\). Slightly overloading the notations, we use \(Q^{\alpha}\), \(V^{\alpha}\) to denote the expected returns from policy \(p_{\alpha}(a_{t}|s_{0:t})\).

By now, we finish the construction of atomic algebraic components and move on to check if the relations between them align with the algebraic structure of a sequential decision-making problem [9].

We first prove the construction above is valid at optimality.

**Lemma 1**.: _When \(f_{\alpha}(a_{t};s_{0:t})=Q*(a_{t};s_{0:t})-V^{*}(s_{0:t})\), \(p_{\alpha}(a_{t}|s_{0:t})\) is the optimal policy._

Proof.: Note that the construction gives us

\[Q*(a_{t};s_{0:t}) =\mathbb{E}_{p^{*}(s_{t+1}|s_{0:t})}\left[r(s_{t+1},s_{0:t})+\log p _{\alpha}*(a_{t}|s_{0:t})+V^{*}(s_{0:t+1})\right]\] \[=\log p_{\alpha}*(a_{t}|s_{0:t})+\mathbb{E}_{p^{*}(s_{t+1}|s_{0: t})}\left[r(s_{t+1},s_{0:t})+V^{*}(s_{0:t+1})\right]\] (26) \[=\log p_{\alpha}*(a_{t}|s_{0:t})+V^{*}(s_{0:t}).\]

Obviously, \(Q*(a_{t};s_{0:t})\) lies in the hypothesis space of \(f_{\alpha}(a_{t};s_{0:t})\). 

Lemma 1 indicates that we need to either parametrize \(f_{\alpha}(a_{t};s_{0:t})\) or \(Q(a_{t};s_{0:t})\).

While \(Q^{\alpha}\) and \(V^{\alpha}\) are constructed from the optimality, the derived \(Q^{\alpha}\) and \(V^{\alpha}\) measure the performance of an interactive agent when it executes with the policy \(p_{\alpha}(a_{t}|s_{0:t})\). They should be consistent with each other.

**Lemma 2**.: \(V^{\alpha}(s_{0:t})\) _and \(\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}[Q^{\alpha}(a_{t};s_{0:t})]\) yield the same optimal policy \(p_{\alpha}*(a_{t}|s_{0:t})\)._

Proof.: \[\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}[Q^{\alpha}(a_{t};s_{0:t}) ]:=\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\mathbb{E}_{p^{\bullet}(s_{t+1}| s_{0:t})}\left[r(s_{t+1},a_{t},s_{0:t})+V^{\alpha}(s_{0:t+1})\right]\right]\] \[= \mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}\left[\mathbb{E}_{p^{\bullet }(s_{t+1}|s_{0:t})}\left[\log p_{\alpha}(a_{t}|s_{0:t})+r(s_{t+1},s_{0:t})+V^{ \alpha}(s_{0:t+1})\right]\right]\] (27) \[= \mathbb{E}_{p^{\bullet}(s_{t+1}|s_{0:t})}\left[r(s_{t+1},s_{0:t}) -\mathcal{H}_{\alpha}(a_{t}|s_{0:t})+V^{\alpha}(s_{0:t+1})\right]\] \[= V^{\alpha}(s_{0:t})-\mathcal{H}_{\alpha}(a_{t}|s_{0:t})-\sum_{k=t +1}^{T-1}\mathbb{E}_{p^{\bullet}(s_{t+1:k}|s_{0:t})}[\mathcal{H}_{\alpha}(a_{k}|s_{ 0:k})],\]where the last line is derived by recursively applying the Bellman equation in the line above until \(s_{0:T}\) and then applying backup with Eq. (23). As an energy-based policy, \(p_{\alpha}(a_{t}|s_{0:t})\)'s entropy is inherently maximized [66]. Therefore, within the hypothesis space, \(p_{\alpha^{\bullet}}(a_{t}|s_{0:t})\) that optimizes \(V^{\alpha}(s_{0:t})\) also leads to optimal expected return \(\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}[Q^{\alpha}(a_{t};s_{0:t})]\). 

If we parametrize the policy as \(p_{\alpha}(a_{t}|s_{0:t})\propto\exp(Q^{\alpha}(a_{t};s_{0:t}))\), the logarithmic normalizing constant \(\log Z^{\alpha_{t}}(s_{0:t})\) will be the _soft V function_ in maximum entropy RL [21, 22, 23]

\[V^{\alpha}_{soft}(s_{0:t}):=\log\int\exp(Q^{\alpha}(a_{t};s_{0:t}))da_{t},\] (28)

even if the reward function is defined differently. We can further show that Bellman identities and backup updates above can entail RL algorithms that achieve optimality of the decision-making objective \(V^{\alpha}\), including _soft policy iteration_[20]

\[p_{\alpha_{k+1}}(a_{t}|s_{0:t})\leftarrow\frac{\exp(Q^{\alpha_{k}}(a_{t};s_{0: t}))}{Z^{\alpha_{k}}(s_{0:t})},\forall s_{0:t},k\in[0,1,...M];\] (29)

and _soft Q iteration_[21]

\[\begin{split} Q^{\alpha_{k+1}}(a_{t};s_{0:t})& \leftarrow\mathbb{E}_{p^{\bullet}(s_{t+1}|s_{0:t})}\left[r_{\alpha}(s_{0:t},a_ {t},s_{t+1})+V^{\alpha_{k}}_{soft}(s_{0:t+1})\right],\forall s_{0:t},a_{t},\\ V^{\alpha_{k+1}}_{soft}(s_{0:t})&\leftarrow\log\int \exp(Q^{\alpha_{k}}(a;s_{0:t}))da,\forall s_{0:t},k\in[0,1,...M].\end{split}\] (30)

**Lemma 3**.: _If \(p^{\bullet}(s_{t+1}|s_{0:t})\) is accessible and \(p_{\beta^{\bullet}}(s_{t+1}|s_{t},a_{t})\) is known, soft policy iteration and soft Q learning both converge to \(p_{\alpha^{\bullet}}(a_{t}|s_{0:t})=p_{Q^{\bullet}}(a_{t}|s_{0:t})\propto \exp(Q^{\bullet}(a_{t};s_{0:t}))\) under certain conditions._

Proof.: See the convergence proof by Ziebart [20] for _soft policy iteration_ and the proof by Fox et al. [21] for _soft Q learning_. The latter requires Markovian assumption. But under some conditions, it can be extended to non-Markovian domains in the same way as proposed by Majeed and Hutter [67]. 

Lemma 3 means given \(p^{\bullet}(s_{t+1}|s_{0:t})\) and \(p_{\beta^{\bullet}}(s_{t+1}|s_{t},a_{t})\), we can recover \(p_{\alpha^{\bullet}}\) through reinforcement learning methods, instead of the proposed MLE. So \(p_{\alpha}(a_{t}|s_{0:t})\) is a viable policy space for the constructed sequential decision-making problem.

Together, Lemma 1, Lemma 2 and Lemma 3 provide constructive proof for a valid sequential decision-making problem that maximizes the same objective of MLE, described by Theorem 1.

**Theorem 1**.: _Assuming the Markovian transition \(p_{\beta^{\bullet}}(s_{t+1}|s_{t},a_{t})\) is known, the ground-truth conditional state distribution \(p^{\bullet}(s_{t+1}|s_{0:t})\) for demonstration sequences is accessible, we can construct a sequential decision-making problem, based on a reward function \(r_{\alpha}(s_{t+1},s_{0:t}):=\log\int p_{\alpha}(a_{t}|s_{0:t})p_{\beta^{ \bullet}}(s_{t+1}|s_{t},a_{t})da_{t}\) for an arbitrary energy-based policy \(p_{\alpha}(a_{t}|s_{0:t})\). Its objective is_

\[\sum_{t=0}^{T}\mathbb{E}_{p^{\bullet}(s_{0:t})}[V^{p_{\alpha}}(s_{0:t})]= \mathbb{E}_{p^{\bullet}(s_{0:T})}\left[\sum_{t=0}^{T}\sum_{k=t}^{T}r_{\alpha}( s_{k+1};s_{0:k})\right],\]

_where \(V^{p_{\alpha}}(s_{0:t}):=E_{p^{\bullet}(s_{t+1:T}|s_{0:t})}[\sum_{k=t}^{T}r_{ \alpha}(s_{k+1};s_{0:k})]\) is the value function for \(p_{\alpha}\). This objective yields the same optimal policy as the Maximum Likelihood Estimation \(\mathbb{E}_{p^{\bullet}(s_{0:T})}[\log p_{\theta}(s_{0:T})]\)._

_If we further define a reward function \(r_{\alpha}(s_{t+1},a_{t},s_{0:t}):=r_{\alpha}(s_{t+1},s_{0:t})+\log p_{\alpha}( a_{t}|s_{0:t})\) to construct a \(Q\) function for \(p_{\alpha}\)_

\[Q^{p_{\alpha}}(a_{t};s_{0:t}):=\mathbb{E}_{p^{\bullet}(s_{t+1}|s_{0:t})}\left[ r_{\alpha}(s_{t+1},a_{t},s_{0:t})+V^{p_{\alpha}}(s_{0:t+1})\right].\]

_The expected return of \(Q^{p_{\alpha}}(a_{t};s_{0:t})\) forms an alternative objective_

\[\mathbb{E}_{p_{\alpha}(a_{t}|s_{0:t})}[Q^{p_{\alpha}}(a_{t};s_{0:t})]=V^{p_{ \alpha}}(s_{0:t})-\mathcal{H}_{\alpha}(a_{t}|s_{0:t})-\sum_{k=t+1}^{T-1} \mathbb{E}_{p^{\bullet}(s_{t+1:k}|s_{0:t})}[\mathcal{H}_{\alpha}(a_{k}|s_{0:k})]\]

_that yields the same optimal policy, for which the optimal \(Q^{\bullet}(a_{t};s_{0:t})\) can be the energy function._

_Only under certain conditions, this sequential decision-making problem is solvable through non-Markovian extensions of the maximum entropy reinforcement learning algorithms._More results on Curve Planning

The energy function is parameterized by a small MLP with one hidden layer and \(4*L\) hidden neurons, where \(L\) is the context length. In short-run Langevin dynamics, the number of samples, the number of sampling steps, and the stepsize are 4, 20 and 1 respectively. We use Adam optimizer with a learning rate 1e-4 and batch size 64. Here we present the complete result in Fig. A1 with different training steps under context length 1 2 4 6, the acceptance rate and residual error of the testing trajectories, as well as the behavior cloning results. We can see that even with sufficient context, BC performs worse than LanMDP. Also, from the result of context length 6 we can see that excessive expressivity does not impair performance, it just requires more training.

## Appendix D Implementation Details of MuJoCo Environment

This section delineates the configurations for the MuJoCo environments utilized in our research. In particular, we employ standard environment horizons of 500 and 50 for Cartpole-v1 and Reacher-v2, respectively. Meanwhile, for Swimmer-v2, Hopper-v2, and Walker2d-v2, we operate within an environment horizon set at 400 as referenced in previous literature [68, 69, 70, 71, 72]. Additional specifications are made for Hopper-v2 and Walker2d-v2, where the velocity of the center of mass was integrated into the state parameterization [72, 68, 70, 52]. We leverage PPO [45] approach to train the expert policy until it reaches (approximately) 450, -10, 40, 3000, 2000 for Cartpole-v1, Reacher-v2,Swimmer-v2, Hopper-v2, Walker2d-v2 respectively. It should be noted that all results disclosed in the experimental section represent averages over five random seeds. Comparative benchmarks include BC [46], BCO [37], GAIL [35], and GAIFO [7]. MobILE [52] is a recent method for Markovian model-based imitation from observation. However, we failed to reproduce the expected performance utilizing various sets of demonstrations, so it is prudently omitted from the present displayed result. We specifically point out that BC/GAIL algorithms are privy to expert actions, however, our algorithm is not. We report the mean of the best performance achieved by BC/BCO with five random seeds, even though these peak performances may transpire at varying epochs. For BC, we executed the supervised learning algorithm for 200 iterations. The BCO/GAIL algorithms are run with an equivalent number of online samples as LanMDP for a fair comparison. All benchmarking is performed using a single 3090Ti GPU and implemented using the PyTorch framework. Notably, in our codebase, the modified environments of Hopper-v2 and Walker2d-v2 utilize MobILE's implementation [52]. Referring to the results in the main text, our presentation of normalized results in bar graph form is derived by normalizing each algorithm's performance (mean/standard deviation) against the expert mean. For Reacher-v2, due to the inherently negative rewards, we first add a constant offset of 20 to each algorithm's performance, thus converting all values to positive before normalizing them against the mean of expert policy.

We parameterize both the policy model and the transition model as MLPs, and the non-linear activation function is Swish and LeakyReLU respectively. We use AdamW to optimize both policy and transition. To stabilize training, we prefer using actions around which the transition model is more certain for computing the expectation over importance-weighted prior distribution in Eq. (19). Therefore, we use a model ensemble with two transition models and use the disagreement between these two models to measure the uncertainty of the sampled actions. We implement Algorithm 2 for all experiments to avoid expensive computation of the gradient for the transition model in posterior sampling. As for better and more effective short-run Langevin sampling, we use a polynomially decaying schedule for the step size as recommended in [73]. We also use weakly L2 regularized energy magnitudes and clip gradient steps like [74], choosing to clip the total amount of change value, i.e. after the gradient and noise have been combined. To realize more delicate decision-making, another trick in Implicit Behavior Clone [75] is also adopted for the inference/testing stage that we continue running the MCMC chain after the step size reaches the smallest in the polynomial schedule until we get twice as many inference Langevin steps as were used during training.

Hyper-parameters are listed in Table 3. Other hyperparameters that are not mentioned here are left as default in PyTorch. Also, note that the Cartpole-v1 task has no parameters for sampling because expectation can be calculated analytically.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Parameter & Cartpole-v1 & Reacher-v2 & Swimmer-v2 & Hopper-v2 & Walker2d-v2 \\ \hline
**Environment Specification** & & & & & \\ \hline Horizon & 500 & 50 & 400 & 400 & 400 \\ \hline Expert Performance (\(\approx\)) & 450 & -10 & 40 & 3000 & 2000 \\ \hline
**Transition Model** & & & & & \\ \hline Architecture(hidden,layers) & MLP(64;4) & MLP(64;4) & MLP(128;4) & MLP(512;4) & MLP(512;4) \\ \hline Optimizer(LR) & 3e-3 & 3e-3 & 3e-3 & 3e-3 & 3e-3 \\ \hline Batch Size & 2500 & 20000 & 20000 & 32768 & 32768 \\ \hline Repplay Buffer Size & 2500 & 20000 & 20000 & 200000 & 200000 \\ \hline
**Policy Model (with context length \(L\))** & & & & & \\ \hline Architecture(hidden,layers) & MLP(150\({}^{\star}\)(\(L\);4) & MLP(150\({}^{\star}\)(\(L\);4) & MLP(150\({}^{\star}\)(L\);4) & MLP(512\({}^{\star}\)(L\);4) \\ \hline Learning rate & 1e-3 & 1e-2 & 1e-2 & 1e-2 & 5e-3 \\ \hline Batch Size & 2500 & 20000 & 20000 & 32768 & 32768 \\ \hline Number of test trajectories & 5 & 20 & 20 & 50 & 50 \\ \hline
**Sampling Parameters** & & & & & \\ \hline Number of prior samples & \(\backslash\) & 8 & 8 & 8 & 8 \\ \hline Number of Langevin steps & \(\backslash\) & 100 & 100 & 100 & 100 \\ \hline Langevin initial stepsize & \(\backslash\) & 10 & 10 & 10 & 10 \\ \hline Langevin ending stepsize & \(\backslash\) & 1 & 1 & 1 & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyper-parameter list of MuJoCo experiments