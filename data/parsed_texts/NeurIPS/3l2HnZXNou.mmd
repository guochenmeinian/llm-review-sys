[MISSING_PAGE_EMPTY:1]

[Busoniu et al., 2008]. Inspired by this, we reconsider the cooperative game from an asynchronous perspective. In other words, each agent is assigned a priority (_i.e._ order) of decision-making at each step, thus the Stackelberg equilibrium (SE) [Von Stackelberg, 2010] is naturally set up as the learning objective. Specifically, the upper-level agents make decisions before the lower-level agents (Each agent represents a unique level, with upper and lower levels being relative.) Therefore, the lower-level agents can acquire the actual actions of the upper-level agents by communication and make their decisions conditioned on what the upper-level agents would do. Importantly, **we never break the fundamental dynamic, \(p(s_{t+1}|s_{t},\bm{a}^{1:k-1})\), in the multi-agent system.** The agents make decisions asynchronously but perform actions simultaneously as the default environment setting.

Under this setting, the SE is likely to be Pareto superior to the average Nash equilibrium (NE) in games that require a high cooperation level [Zhang et al., 2020]. However, _is it necessary to decide a specific priority of decision-making for each agent?_ Ideally, the optimal joint policy can be decomposed by any orders [Wen et al., 2019], _e.g._, \(\pi^{*}(a_{1},a_{2}|s)=\pi^{*}(a_{1}|s)\pi^{*}(a_{2}|s,a_{1})=\pi^{*}(a_{2}|s) \pi^{*}(a_{1}|s,a_{2})\). But during the learning process, agents are unlikely to use other agents' optimal actions for gradient calculation, making it still vulnerable to the relative overgeneralization problem [Wei et al., 2018]. This means there is no guarantee that different orders will converge to the same suboptimal. We also claim that the different priorities of decision-making may affect the optimality of the convergence of the learning algorithm in Section 3. Note that relative overgeneralization occurs when a suboptimal NE in the joint space of actions is preferred over an optimal NE because each agent's action in the suboptimal equilibrium is a better choice when matched with arbitrary actions from the cooperative agents.

This work proposes a novel multi-level communication scheme for cooperative MARL, _Sequential Communication_ (SeqComm), to enable agents to coordinate with each other explicitly. Specifically, SeqComm has two-phase communication, negotiation phase and launching phase. In the negotiation phase, agents communicate their hidden states of observations with others simultaneously. Then, they can generate multiple predicted trajectories, called _intention_, by modeling the environmental dynamics and other agents' actions. In addition, the priority of decision-making is determined by communicating and comparing the agents' intentions, which are evaluated by their state-value functions. **The value of each intention represents the predicted rewards obtained by treating that agent as the first mover of the order sequence.** The sequence of others follows the same procedure as aforementioned with the upper-level agents fixed. In the launching phase, the upper-level agents take the lead in decision-making and communicate their actual actions with the lower-level agents. The actual actions will be executed simultaneously in the environment without changes.

SeqComm is currently built on MAPPO [Yu et al., 2021]. Theoretically, we prove the policies learned by SeqComm are guaranteed to improve monotonically and converge. Empirically, we evaluate SeqComm on StarCraft multi-agent challenge v2 (SMACv2) [Samvelyan et al., 2019]. We demonstrate that SeqComm outperforms existing communication-free and communication-based methods in various maps in SMACv2. By ablation studies, we confirm that treating agents asynchronously is a more effective way to promote coordination, and SeqComm can provide the proper priority of decision-making for agents to develop better coordination.

## 2 Related Work

**Communication.** Existing work [Jiang and Lu, 2018, Kim et al., 2019, Singh et al., 2019, Das et al., 2019, Zhang et al., 2019, Jiang et al., 2020, Ding et al., 2020, Konan et al., 2022] in this realm mainly focus on how to extract valuable messages. ATOC [Jiang and Lu, 2018] and IC3Net [Singh et al., 2019] utilize gate mechanisms to decide when to communicate with other agents. Several studies [Das et al., 2019, Konan et al., 2022] employ multi-round communication to fully reason the intentions of others and establish complex collaboration strategies. Social influence [Jaques et al., 2019] uses communication to influence the behaviors of others. I2C [Ding et al., 2020] only communicates with agents that are relevant and influential which are determined by causal inference. However, all these methods focus on how to exploit valuable information from current or past partial observations effectively and properly. More recently, some studies [Kim et al., 2021, Du et al., 2021, Pretorius et al., 2021] begin to answer the question: can we favor cooperation beyond sharing partial observation? They allow agents to imagine their future states with a world model and communicate those with others. IS [Pretorius et al., 2021], as the representation of this line of research, enables each agent to share its intention with other agents in the form of the encoded imagined trajectory and use the attention module to figure out the importance of the received intention. However, two concerns arise. On one hand, circular dependencies can lead to inaccurate predicted future trajectories as long as the multi-agent system treats agents synchronously. On the other hand, MARL struggles in extracting useful information from numerous messages, not to mention more complex and dubious messages, _i.e._ predicted future trajectories. Unlike these studies, we treat the agents from an asynchronous perspective, therefore, circular dependencies can be naturally resolved. Moreover, agents send actions to lower-level agents, making the messages compact and informative.

**Coordination.** The agents are essentially independent decision-makers in execution and may break ties between equally good actions randomly. Thus, in the absence of additional mechanisms, different agents may break ties in different ways, and the resulting joint actions may be suboptimal. Coordination graphs (Guestrin et al., 2002; Bohmer et al., 2020; Wang et al., 2021) simplify the coordination when the global Q-function can be additively decomposed into local Q-functions that only depend on the actions of a subset of agents. Typically, a coordination graph expresses a higher-order value decomposition among agents. This improves the representational capacity to distinguish other agents' effects on local utility functions, which addresses the miscoordination problems caused by partial observability.

Another general approach to solving the coordination problem is to make sure that ties are broken by all agents in the same way, requiring that random action choices are somehow coordinated or negotiated. Social conventions (Boutilier, 1996) or role assignments (Prasad et al., 1998) encode prior preferences towards certain joint actions and help break ties during action selection. Communication (Fischer et al., 2004; Vlassis, 2007) can be used to negotiate action choices, either alone or in combination with the aforementioned techniques. Our method follows this line of research by utilizing the ordering of agents and actions to break the ties, other than the enhanced representational capacity of the local value function.

More discussions of related work are in Appendix C.

## 3 Problem Formulation

**Cost-Free Communication.** The decentralized partially observable Markov decision process (DecPOMDP) can be extended to multi-agent POMDP (Oliehoek et al., 2016) by sharing observations among agents via communication. The joint observations are not necessarily equivalent to the state. However, joint observations can be used to represent the state better than single observations.

Previous work (Pynadath and Tambe, 2002) shows that under cost-free communication, agents would share optimal messages for mutual interest. If the communication cost is high, there is a balance between delivering all the useful messages for greater benefits and keeping the amount of communication as low as possible. In addition, analyzing this extreme case gives us some understanding of the benefit of communication, even if the results do not apply across all domains. However, even under multi-agent POMDP where agents can get joint observations, coordination problems can still arise (Busoniu et al., 2008). Suppose the centralized critic has learnt actions pairs \([a_{1},a_{2}]\) and \([b_{1},b_{2}]\) that are equally optimal. Without any prior information, the individual policies \(\pi_{1}\) and \(\pi_{2}\) learned from the centralized critic can break the ties randomly and may choose \(a_{1}\) and \(b_{2}\), respectively.

**Multi-Agent Sequential Decision-Making.** We consider fully cooperative multi-agent tasks that are modeled as multi-agent POMDP, where \(n\) agents interact with the environment according to the following procedure, which we refer to as _multi-agent sequential decision-making_.

At each timestep \(t\), assume the priority (_i.e._ order) of decision-making for all agents is given and each priority level has only one agent (_i.e._, agents make decisions one by one). Note that the smaller the level index, the higher priority of decision-making is. The agent at each level \(k\) gets its own observation \(o_{t}^{k}\) drawn from the state \(s_{t}\), and receives messages \(\bm{m}_{t}^{-k}\) from all other agents, where \(\bm{m}_{t}^{-k}\triangleq\{\{o_{t}^{1},a_{t}^{1}\},\ldots,\{o_{t}^{k-1},a_{t}^ {k-1}\},o_{t}^{k+1},\ldots,o_{t}^{n}\}\). Equivalently, \(\bm{m}_{t}^{-k}\) can be written as \(\{\bm{o_{t}^{-k}},\bm{a}_{t}^{1:k-1}\}\), where \(\bm{o_{t}^{-k}}\) denotes the joint observations of all agents except \(k\) (in practice, agents communicate the hidden states/encodings of observations), and \(\bm{a}_{t}^{1:k-1}\) denotes the joint actions of agents \(1\) to \(k-1\). For the agent at the first level (_i.e._, \(k=1\)), \(\bm{a}_{t}^{1:k-1}=\varnothing\). Then, the agent determines its action \(a_{t}^{k}\) sampled from its policy \(\pi_{k}(\cdot|o_{t}^{k},\bm{m}_{t}^{-k})\) or equivalently \(\pi_{k}(\cdot|o_{t},\bm{a}_{t}^{1:k-1})\) and sends it to the lower-level agents. After all, agents have determined their actions, they perform the joint actions\(\bm{a}_{t}\), which can be seen as sampled from the joint policy \(\bm{\pi}(\cdot|s_{t})\)_factorized_ as \(\prod_{k=1}^{n}\pi_{k}(\cdot|\bm{o}_{t},\bm{a}_{t}^{1:k-1})\), in the environment and get a shared reward \(r(s_{t},\bm{a}_{t})\) and the state transitions to next state \(s^{\prime}\) according to the transition probability \(p(s^{\prime}|s_{t},\bm{a}_{t})\). All agents aim to maximize the expected return \(\sum_{t=0}^{\infty}\gamma^{t}r_{t}\), where \(\gamma\) is the discount factor. The state-value function and action-value function of the level-\(k\) agent are defined as follows:

\[V_{\pi_{k}}(s,\bm{a}^{1:k-1})\triangleq\mathop{\mathbb{E}}_{ \begin{subarray}{c}\bm{a}_{0}^{k,n}\sim\bm{\pi}_{k:n}\\ \bm{a}_{1:0}\sim\bm{\pi}\end{subarray}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}|s_ {0}=s,\bm{a}_{0}^{1:k-1}=\bm{a}^{1:k-1}]\] \[Q_{\pi_{k}}(s,\bm{a}^{1:k})\triangleq\mathop{\mathbb{E}}_{ \begin{subarray}{c}\bm{a}_{0}^{k+1}\sim\bm{\pi}_{k+1:n}\\ \bm{a}_{1:0}\sim\bm{\pi}\end{subarray}}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}|s_ {0}=s,\bm{a}_{0}^{1:k}=\bm{a}^{1:k}].\]

For the setting of multi-agent sequential decision-making discussed above, we have the following proposition.

**Proposition 1**.: _If all the agents update their policy with individual TRPO (Schulman et al., 2015) sequentially in multi-agent sequential decision-making, then the joint policy of all agents are guaranteed to improve monotonically and converge._

Proof.: The proof is given in Appendix A. 

Proposition 1 indicates that SeqComm has the performance guarantee regardless of the priority of decision-making in multi-agent sequential decision-making. However, the priority of decision-making indeed affects the optimality of the converged joint policy, and we have the following claim.

**Claim 1**.: _The different priorities of decision-making affect the optimality of the convergence of the learning algorithm due to the relative overgeneralization problem._

We use a one-step matrix game as an example, as illustrated in Figure 1(a), to demonstrate the influence of the priority of decision-making on the learning process. Due to relative overgeneralization (Wei et al., 2018), agent \(B\) tends to choose \(b_{2}\) or \(b_{3}\). Specifically, \(b_{2}\) or \(b_{3}\) in the suboptimal equilibrium is a better choice than \(b_{1}\) in the optimal equilibrium when matched with arbitrary actions from agent \(A\). Therefore, as shown in Figure 1(b), \(B\to A\) (_i.e._, agent \(B\) makes decisions before \(A\), and \(A\)'s policy conditions on the action of \(B\)) and _Simultaneous_ (_i.e._, two agents make decisions simultaneously and independently) are easily trapped into local optima. However, if agent \(A\) goes first, things can be different, as \(A\to B\) achieves the optimum. As long as agent \(A\) does not suffer from relative overgeneralization, it can help agent \(B\) get rid of local optima by narrowing down the search space of \(B\). Besides, a policy that determines the priority of decision-making can be learned under the

Figure 1: (a) Payoff matrix for a one-step game. There are multiple local optima. (b) Evaluations of different methods for the game in terms of the mean reward and standard deviation of ten runs. \(A\to B\), \(B\to A\), _Simultaneous_, and _Learned_ represent that agent \(A\) makes decisions first, agent \(B\) makes decisions first, two agents make decisions simultaneously, and there is another learned policy determining the priority of decision making, respectively. MAPPO (Yu et al., 2021) is used as the backbone.

guidance of the state-value function, denoted as _Learned_. It obtains better performance than \(B\to A\) and _Simultaneous_, which indicates that dynamically determining the order during policy learning can be beneficial as we do not know the optimal priority in advance.

**Remark 1**.: The priority (_i.e._ order) of decision-making affects the optimality of the converged joint policy in multi-agent sequential decision-making, thus it is critical to determine the order. However, learning the order directly requires an additional centralized policy in execution, which is not generalizable in a scenario where the number of agents varies. Moreover, its learning complexity exponentially increases with the number of agents, making it infeasible in many cases.

## 4 Sequential Communication

In this paper, we cast our eyes in another direction and resort to the world model, which is the dynamic model of the environment. Ideally, we can randomly sample candidate order sequences, evaluate them under the world model (see Section 4.1), and choose the order sequence that is deemed the most promising under the true dynamic. SeqComm is designed based on this principle to determine the priority of decision-making via communication.

In SeqComm, communication is separated into phases serving different purposes and multi-round communication in one phase is possible. One is the _negotiation_ phase for agents to determine the priority of decision-making. Another is the _launching_ phase for agents to act conditioning on actual actions upper-level agents will take to implement _explicit coordination via communication_. The overview of SeqComm is illustrated in Figure 2. Each SeqComm agent consists of a policy, a critic, and a world model, as illustrated in Figure 3, and the parameters of all networks are shared across agents (Gupta et al., 2017).

**World Model.** The world model is needed to predict and evaluate future trajectories. SeqComm, unlike previous works (Kim et al., 2021; Du et al., 2021; Pretorius et al., 2021), can utilize received hidden states of other agents in the first round of communication to model more precise environment dynamics for the explicit coordination in the next round of communication. Once an agent can access other agents' hidden states, it shall have adequate information to estimate their actions since all agents are homogeneous and parameter-sharing. Therefore, the world model \(\mathcal{M}(\cdot)\) takes as input the joint hidden states \(\bm{h}_{t}=\{h_{t}^{1},\dots,h_{t}^{n}\}\) and actions \(\bm{a}_{t}\), and predicts the next joint observations and reward. In practice, before the inputs pass into the world model, the attention module \(\mathrm{AM_{w}}\) is utilized to process the input.

\[\hat{\bm{o}}_{t+1},\hat{r}_{t+1}=\mathcal{M}_{i}(\mathrm{AM_{w}}(\bm{h}_{t}, \bm{a}_{t})).\]

The reason that we adopt the attention module is to entitle the world model to be generalizable in the scenarios where additional agents are introduced or existing agents are removed.

Figure 2: Overview of SeqComm. SeqComm has two communication phases, the negotiation phase (_left_) and the launching phase (_right_). In the negotiation phase, agents communicate hidden states of observations with others and obtain their own intention. The priority of decision-making is determined by sharing and comparing the value of all the intentions. In the launching phase, the agents who hold the upper-level positions will make decisions prior to the lower-level agents. Besides, their actions will be shared with anyone that has not yet made decisions.

### Negotiation Phase

In the negotiation phase, the observation encoder first takes \(o_{t}\) as input and outputs a hidden state \(h_{t}\) to compress the information, which is used to communicate with others. Note that many studies (Ding et al., 2020; Jiang and Lu, 2018) found that redundant messages may impair the learning process empirically. In more detail, the model can converge slowly or sometimes lead to a worse sub-optimal. Agents then determine the priority of decision-making by _intentions_ which is established and evaluated based on the world model.

**Priority of Decision-Making.** Intention is the key element in determining the priority of decision-making. The notion of intention is described as an agent's future behavior in previous works (Rabinowitz et al., 2018; Raileanu et al., 2018; Kim et al., 2021). However, we define the _intention_ as an agent's future behavior _without considering others_.

As mentioned before, an agent's intention considering others can lead to circular dependencies and cause miscoordination. By our definition, the intention of an agent should be depicted as all future trajectories considering that agent as the first mover and ignoring the others. However, there are many possible future trajectories as the priority of the rest of the agents is _unfixed_. In practice, we use the Monte Carlo method to estimate the intention value based on all future trajectories. Note that it is uniform across priorities for unfixed agents. Each order should be treated equally since we do not have any prior for the distribution.

Taking agent \(i\) at timestep \(t\) to illustrate, it firstly considers itself as the first-mover and produces its action only based on the joint hidden states, \(\hat{a}^{i}_{t}\sim\pi_{i}(\cdot|\mathrm{AM}_{\mathrm{a}}(\bm{h}_{t},\emptyset)\), where we again use an attention module \(\mathrm{AM}_{\mathrm{a}}\) to handle the input. For the order sequence of lower-level agents, we randomly sample a set of order sequences from unfixed agents. Assume agent \(j\) is the second-mover, agent \(i\) models \(j\)'s action by considering the upper-level action following its own policy \(\hat{a}^{j}_{t}\sim\pi_{i}(\cdot|\mathrm{AM}_{\mathrm{a}}(\bm{h}_{t},\hat{a} ^{i}_{t}))\). The same procedure is applied to predict the actions of all other agents following the sampled order sequence. Based on the joint hidden states and predicted actions, the next joint observations \(\hat{\bm{o}}_{t+1}\) can be predicted by the world model \(\mathcal{M}\). The length of the predicted future trajectory is \(H\) and it can then be written as \(\tau^{t}=\{\hat{\bm{o}}_{t+1},\hat{\bm{o}}_{t+1},\dots,\hat{\bm{o}}_{t+H}, \hat{\bm{a}}_{t+H}\}\) by repeating the procedure aforementioned.

Then, the agent uses its critic (state-value function) to evaluate the future trajectory and output value \(v_{\tau^{t}}\). The intention value is defined as the average value of \(F\) future trajectories with different sampled order sequences. _Through the critic, we have linked the order and agent performance together._

After all the agents have computed their intentions and the corresponding value, they again communicate their intention values to others. Then, agents would compare and choose the agent with the highest intention value to be the first mover. The priority of lower-level decision-making follows the same procedure with the upper-level agents fixed. Note that some agents may communicate intention values with others multiple times until the priority of decision-making is finally determined.

### Negotiation Phase for Local Communication

The full communication version of SeqComm is constructed based on theoretical derivation. It has a theoretical guarantee to some extent, but some assumptions, e.g., broadcast communication, can be unrealistic and incur lots of communication overhead. Therefore, we provide another version of SeqComm in scenarios where agents can only communicate with nearby agents (agents within a limited communication range).

In more detail, the agent first calculates its intention value based only on the hidden states of nearby agents. After comparing with the intention values of nearby agents (intention values are communicated with the nearby agents), the agent can determine the upper-level and lower-level nearby agents. Unlike the previous version of SeqComm, agents cannot distinguish the detailed order sequence of the upper-level nearby agents since their communication ranges may not overlap. Therefore, the intention values are calculated and communicated among agents for _only one time_. The local communication version greatly reduces communication overhead, making it more suitable for many real applications.

For more details of the algorithms, please refer to the Appendix D for the pseudo-code.
As for the launching phase, agents communicate to obtain additional information to make decisions. Apart from the received hidden states from the last phase, we allow agents to get what _actual_ actions the upper-level agents will take in execution, while other studies can only infer others' actions by opponent modeling (Rabinowitz et al., 2018; Raileanu et al., 2018) or communicating intentions (Kim et al., 2021). Therefore, miscoordination can be naturally avoided, and a better cooperation strategy is possible since lower-level agents can adjust their behaviors accordingly.

A lower-level agent \(i\) make a decision following the policy \(\pi_{i}(\cdot|\mathrm{AM}_{a}(\bm{h}_{t},\bm{a}_{t}^{upper}))\), where \(\bm{a}_{t}^{upper}\) means received actual actions from all upper-level agents. As long as the agent has decided on an action, it will send the action to all other lower-level agents through the communication channel. _Note that the actions are executed simultaneously and distributedly in execution, though agents make decisions sequentially._

### Theoretical Analysis

As intention values determine the priority of decision-making, SeqComm is likely to choose different orders _at different timesteps_ during training. However, we have the following proposition that theoretically guarantees the performance of the learned joint policy under SeqComm.

**Proposition 2**.: _The monotonic improvement and convergence of the joint policy in SeqComm are independent of the priority of decision-making of agents at each timestep._

Proof.: The proof is given in Appendix A. 

The priority of decision-making is chosen under the world model, thus the compounding errors in the world model can result in discrepancies between the predicted returns of the same order under the world model and the true dynamics. We then analyze the monotonic improvement for the joint policy under the world model based on Janner et al. (2019).

**Theorem 1**.: _Let the expected total variation between two transition distributions be bounded at each timestep as \(\max_{t}\mathbb{E}_{s\sim\pi_{s,t}}[D_{TV}(p(s^{\prime}|s,\bm{a})||\hat{p}(s^ {\prime}|s,\bm{a}))]\leq\epsilon_{m}\), and the policy divergences at level \(k\) be bounded as \(\max_{s,\bm{a}^{1:k-1}}D_{TV}(\pi_{\beta,k}(a^{k}|s,\bm{a}^{1:k-1})||\pi_{k}(a^ {k}|s,\bm{a}^{1:k-1}))\leq\epsilon_{\pi_{k}}\), where \(\bm{\pi}_{\beta}\) is the data collecting policy for the model and \(\hat{p}(s^{\prime}|s,\bm{a})\) is the transition distribution under the model. Then the model return \(\hat{\eta}\) and true return \(\eta\) of the policy \(\bm{\pi}\) are bounded as:_

\[\hat{\eta}[\bm{\pi}] \geq\eta[\bm{\pi}]-\] \[\underbrace{\frac{[2\gamma\tau_{\max}(\epsilon_{m}+2\sum_{k=1}^{n }\epsilon_{\pi_{k}})}{(1-\gamma)^{2}}+\frac{4r_{\max}\sum_{k=1}^{n}\epsilon_{ \pi_{k}}}{(1-\gamma)}]}_{C(\epsilon_{m},\bm{\epsilon}_{\pi_{1:n}})}.\]

Proof.: The proof is given in Appendix B. 

**Remark 2**.: Theorem 1 provides a useful relationship between the compounding errors and the policy update. As long as we improve the return under the true dynamic by more than the gap, \(C(\epsilon_{m},\bm{\epsilon}_{\pi_{1:n}})\), we can guarantee the policy improvement under the world model. If no such policy exists to overcome the gap, it implies the model error is too high, that is, there is a large discrepancy between the world model and true dynamics. Thus the order sequence obtained under the world model is not reliable. Such an order sequence is almost the same as a random one. Though a random order sequence also has the theoretical guarantee of Proposition 2, we will show in Section 5.2 that a random order sequence leads to a poor local optimum empirically.

Figure 3: Architecture of SeqComm. The critic and policy of each agent take input as its own observation and received messages. The world model takes as input the joint hidden states and predicted joint actions.

## 5 Experiments

SeqComm is currently instantiated based on MAPPO (Yu et al., 2021). We evaluate SeqComm nine maps in StarCraft multi-agent challenge v2 (SMACv2) (Ellis et al., 2024).

In the experiments, SeqComm and baselines are parameter-sharing for fast convergence (Gupta et al., 2017; Terry et al., 2020). We have fine-tuned the baselines for a fair comparison. _The world model in the SMACv2 environment is trained from scratch and kept fine-tuned in the learning process._ Therefore, no extra prior knowledge is provided. Please refer to the Appendix for the hyperparameter settings. All results are presented in terms of the mean and standard deviation of five runs with different random seeds.

### Results

**SMACv2.** We have evaluated our method on the _most representative and challenging_ multi-agent environment currently available. Compared with SMAC (Samvelyan et al., 2019), SMACv2 has some better properties, _i.e._ stochasticity and partial observability. In other words, agents need to cooperate more in the new environment to complete tasks, whereas they could achieve a certain success rate without cooperation in the original environment.

We have chosen nine maps for extensive evaluation and made some minor changes to the observation part of agents to make it more difficult. Specifically, the sight range of agents is reduced from \(9\) to \(3\), and agents cannot perceive any information about their allies even if they are within the sight range. NDQ (Wang et al., 2020) adopts a similar change to increase the difficulty of action coordination. The rest of the settings remain the same as the default. In summary, _we require the environment to be one where a high success rate cannot be achieved solely based on individual observations._

We also evaluate the local communication version of SeqComm. Agents can only communicate with nearby agents (agents within their communication range). Note that the map size and the total number

Figure 4: Learning curves of SeqComm and baselines in nine SMACv2 maps.

of agents restrict the number of nearby agents. As the task progresses, the number of nearby agents is from 2 to 4.

**Analysis.** The learning curves of SeqComm and the baselines in terms of the win rate are illustrated in Figure 4. All communication-based methods perform better than communicaion-free method (MAPPO). In easy scenarios, communication may not be very useful, but experiments have shown that in cases with significant partial observability and stochasticity, communication can greatly enhance agent ability.

We compare our method with TarMAC [10], which holds a similar position in communication settings to that of MAPPO in communication-free settings. SeqComm outperforms TarMAC in all maps, which verifies the gain of explicit action coordination. Moreover, the full-communication version performs better than the local-communication version because the former can access more information. However, it also costs more communication overhead.

### Ablation Studies

**Priority of Decision-Making.** We primarily want to contribute a practical version to the community. Moreover, _the fewer communicative agents there are, the fewer possible orders there are, thus increasing the probability of randomly obtaining a good order_. It would be more meaningful to demonstrate that devoting effort to finding a good order is still important in such a scenario. Therefore, we do the ablation study for the local version of the SeqComm. In more detail, we compare SeqComm with two ablation baselines: the priority of decision-making is determined randomly at each timestep, denoted as _Random_, and agents only access the observations of others during training and execution, denoted as _No action_.

As depicted in Figure 7, SeqComm achieves a higher win rate than _Random_ and _No action_ in all the maps. These results verify the importance of the priority of decision-making and the necessity to adjust it continuously during one episode. It is also demonstrated that SeqComm can provide a proper priority of decision-making. As discussed in Section 4.4, although _Random_ also has the theoretical guarantee, they converge to poor local optima in practice. Surprisingly, in most tasks, _Random_ performs worse than _No action_. It again verifies that a bad order may fail to improve coordination or even impair it.

**Communication Range.** We also conduct experiments to demonstrate the impact of different communication ranges. We set communication ranges to {1, 3, 9}, in addition to the default range of 6. We notice a steady improvement in performance as the communication range increases. Therefore, the choice of communication range is a trade-off between communication overhead and agent performance. In our previous experiments, we choose a compromise value of 3 for the local version to validate the effectiveness of our method. Results refer to Figure 5.

Figure 5: Ablation studies of the communication ranges.

Figure 6: Ablation studies on the network mechanisms.

**Network Mechanism.** We replaced the attention mechanism for local communication with an aggregation method. In more detail, messages are concatenated and passd into a five-layer linear neural networks. The curve is based on 3 random seeds and tested on the terran 10v10 map. The results refer to Figure 6.

## 6 Conclusions

We have proposed SeqComm, which enables agents to coordinate well and explicitly with each other, and it, from an asynchronous perspective, allows agents to make decisions sequentially. A two-phase communication scheme has been adopted to determine the priority of decision-making and transfer messages accordingly. Empirically, it is demonstrated that SeqComm outperforms baselines in a variety of cooperative multi-agent scenarios.

**Limitations.** The assumption of accessing the local observation of any other agent could be strong since it is unsuitable for all applications. Thus, we provide a local communication version of SeqComm for assumption relaxation in the experiment.

## Acknowledgements

This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404 and the NSFC under Grants 62450001 and 62476008.

Figure 7: Ablation studies under local communication in SMACv2.

## References

* Bohmer et al. (2020) Wendelin Bohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In _International Conference on Machine Learning (ICML)_, 2020.
* Boutilier (1996) Craig Boutilier. Planning, learning and coordination in multiagent decision processes. In _Conference on Theoretical Aspects of Rationality and Knowledge_, 1996.
* Busoniu et al. (2008) Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. _IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)_, 38(2):156-172, 2008.
* Das et al. (2019) Abhishek Das, Theophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In _International Conference on Machine Learning (ICML)_, 2019.
* Ding et al. (2020) Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent cooperation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* Du et al. (2021) Yali Du, Yifan Zhao, Meng Fang, Jun Wang, Gangyan Xu, and Haifeng Zhang. Learning predictive communication by imagination in networked system control, 2021.
* Ellis et al. (2024) Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Fischer et al. (2004) Felix Fischer, Michael Rovatsos, and Gerhard Weiss. Hierarchical reinforcement learning in communication-mediated multiagent coordination. In _International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS)_, 2004.
* Greenwald et al. (2003) Amy Greenwald, Keith Hall, and Roberto Serrano. Correlated q-learning. In _A comprehensive survey of multiagent reinforcement learning_, 2003.
* Guestrin et al. (2002) Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2002.
* Gupta et al. (2017) Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In _International Conference on Autonomous Agents and Multiagent Systems (AAMAS)_, 2017.
* Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Jaques et al. (2019) Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2019.
* Jiang et al. (2024) Haobin Jiang, Ziluo Ding, and Zongqing Lu. Settling decentralized multi-agent coordinated exploration by novelty sharing. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 17444-17452, 2024.
* Jiang and Lu (2018) Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* Jiang et al. (2020) Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. In _International Conference on Learning Representation (ICLR)_, 2020.
* Kim et al. (2019) Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee, Kyunghwan Son, and Yung Yi. Learning to schedule communication in multi-agent reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2019.
* Kim et al. (2021) Woojun Kim, Jongeui Park, and Youngchul Sung. Communication in multi-agent reinforcement learning: Intention sharing. In _International Conference on Learning Representations (ICLR)_, 2021.
* Kim et al. (2019)Sachin Konan, Esmaeil Seraj, and Matthew Gombolay. Iterated reasoning with mutual information in cooperative and byzantine decentralized teaming. In _International Conference on Learning Representations (ICLR)_, 2022.
* Kononen (2004) Ville Kononen. Asymmetric multiagent reinforcement learning. _Web Intelligence and Agent Systems: An international journal_, 2(2):105-121, 2004.
* Lowe et al. (2017) Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Ma et al. (2019) Hang Ma, Daniel Harabor, Peter J Stuckey, Jiaoyang Li, and Sven Koenig. Searching with consistent prioritization for multi-agent path finding. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2019.
* Oliehoek et al. (2016) Frans A Oliehoek, Christopher Amato, et al. _A concise introduction to decentralized POMDPs_, volume 1. Springer, 2016.
* Prasad et al. (1998) MV Nagendra Prasad, Victor R Lesser, and Susan E Lander. Learning organizational roles for negotiated search in a multiagent system. _International Journal of Human-Computer Studies_, 48(1):51-67, 1998.
* Pretorius et al. (2021) Arnu Pretorius, Scott Cameron, Andries Petrus Smit, Elan van Biljon, Lawrence Francis, Femi Azeez, Alexandre Latterre, and Karim Beguir. Learning to communicate through imagination with model-based deep multi-agent reinforcement learning, 2021.
* Pynadath and Tambe (2002) David V. Pynadath and Milind Tambe. The communicative multiagent team decision problem: Analyzing teamwork theories and models. _J. Artif. Intell. Res._, 16:389-423, 2002.
* Rabinowitz et al. (2018) Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. Machine theory of mind. In _International Conference on Machine Learning (ICML)_, 2018.
* Raileanu et al. (2018) Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2018.
* Samvelyan et al. (2019) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. _arXiv preprint arXiv:1902.04043_, 2019.
* Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International Conference on Machine Learning (ICML)_, 2015.
* Singh et al. (2019) Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Individualized controlled continuous communication model for multiagent cooperative and competitive tasks. In _International Conference on Learning Representations (ICLR)_, 2019.
* Sodomka et al. (2013) Eric Sodomka, Elizabeth Hilliard, Michael Littman, and Amy Greenwald. Coco-q: Learning in stochastic games with side payments. In _International Conference on Machine Learning (ICML)_, 2013.
* Terry et al. (2020) Justin K. Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, Benjamin Black, and Dinesh Manocha. Parameter sharing is surprisingly useful for multi-agent deep reinforcement learning. _arXiv preprint arXiv:2005.13625_, 2020.
* Van Den Berg and Overmars (2005) Jur P Van Den Berg and Mark H Overmars. Prioritized motion planning for multiple robots. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, 2005.
* Vlassis (2007) Nikos Vlassis. A concise introduction to multiagent systems and distributed artificial intelligence. _Synthesis Lectures on Artificial Intelligence and Machine Learning_, 1(1):1-71, 2007.
* Von Stackelberg (2010) Heinrich Von Stackelberg. _Market structure and equilibrium_. Springer Science & Business Media, 2010.
* Vlassis et al. (2018)Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value functions via communication minimization. In _International Conference on Learning Representation (ICLR)_, 2020.
* Wang et al. (2021) Tonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, and Chongjie Zhang. Context-aware sparse deep coordination graphs. _arXiv preprint arXiv:2106.02886_, 2021.
* Wei et al. (2018) Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent soft q-learning. In _AAAI Spring Symposium Series_, 2018.
* Wen et al. (2019) Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and W Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2019.
* Yu et al. (2021) Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. _arXiv preprint arXiv:2103.01955_, 2021.
* Zhang et al. (2020) Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun Wang. Bi-level actor-critic for multi-agent coordination. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2020.
* Zhang et al. (2019) Sai Qian Zhang, Qi Zhang, and Jieyu Lin. Efficient communication in multi-agent reinforcement learning via variance based control. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.

Proofs of Proposition 1 and Proposition 2

**Lemma 1** (Agent-by-Agent PPO).: _If we update the policy of each agent \(i\) with TRPO Schulman et al. (2015) (or approximately PPO) when fixing all the other agent's policies, then the joint policy will improve monotonically._

Proof.: We consider the joint surrogate objective in TRPO \(L_{\bm{\pi}_{\mathrm{old}}}(\bm{\pi}_{\mathrm{new}})\) where \(\bm{\pi}_{\mathrm{old}}\) is the joint policy before updating and \(\bm{\pi}_{\mathrm{new}}\) is the joint policy after updating.

Given that \(\pi_{\mathrm{new}}^{-i}=\pi_{\mathrm{old}}^{-i}\), we have:

\[L_{\bm{\pi}_{\mathrm{old}}}(\bm{\pi}_{\mathrm{new}}) =\mathbb{E}_{a\sim\bm{\pi}_{\mathrm{new}}}[A_{\bm{\pi}_{\mathrm{ old}}}(s,\bm{a})]\] \[=\mathbb{E}_{a\sim\bm{\pi}_{\mathrm{old}}}[\frac{\bm{\pi}_{ \mathrm{new}}(\bm{a}|s)}{\bm{\pi}_{\mathrm{old}}(\bm{a}|s)}A_{\bm{\pi}_{ \mathrm{old}}}(s,\bm{a})]\] \[=\mathbb{E}_{a\sim\bm{\pi}_{\mathrm{old}}}[\frac{\pi_{\mathrm{ new}}^{i}(a^{i}|s)}{\pi_{\mathrm{old}}^{i}(a^{i}|s)}A_{\bm{\pi}_{\mathrm{old}}}(s, \bm{a})]\] \[=\mathbb{E}_{a^{i}\sim\pi_{\mathrm{old}}^{i}}\left[\frac{\pi_{ \mathrm{new}}^{i}(a^{i}|s)}{\pi_{\mathrm{old}}^{i}(a^{i}|s)}\mathbb{E}_{a^{i} \sim\pi_{\mathrm{old}}^{-i}}[A_{\bm{\pi}_{\mathrm{old}}}(s,a^{i},a^{-i})]\right]\] \[=\mathbb{E}_{a^{i}\sim\pi_{\mathrm{old}}^{i}}\left[\frac{\pi_{ \mathrm{new}}^{i}(a^{i}|s)}{\pi_{\mathrm{old}}^{i}(a^{i}|s)}A_{\bm{\pi}_{ \mathrm{old}}}^{i}(s,a^{i})\right]\] \[=L_{\pi_{\mathrm{old}}^{i}}(\pi_{\mathrm{new}}^{i}),\]

where \(A_{\bm{\pi}_{\mathrm{old}}}^{i}(s,a^{i})=\mathbb{E}_{a^{-i}\sim\pi_{\mathrm{old }}^{-i}}[A_{\bm{\pi}_{\mathrm{old}}}(s,a^{i},a^{-i})]\) is the individual advantage of agent \(i\), and the third equation is from the condition \(\pi_{\mathrm{new}}^{-i}=\pi_{\mathrm{old}}^{-i}\).

With the result of TRPO, we have the following conclusion:

\[J(\pi_{\mathrm{new}})-J(\pi_{\mathrm{old}}) \geq L_{\bm{\pi}_{\mathrm{old}}}(\bm{\pi}_{\mathrm{new}})-\mathrm{ CD}_{\mathrm{KL}}^{\mathrm{max}}(\bm{\pi}_{\mathrm{new}}||\bm{\pi}_{\mathrm{ old}})\] \[=L_{\pi_{\mathrm{old}}^{i}}(\pi_{\mathrm{new}}^{i})-\mathrm{CD}_{ \mathrm{KL}}^{\mathrm{max}}(\pi_{\mathrm{new}}^{i}||\pi_{\mathrm{old}}^{i}) \quad(\text{from }\pi_{\mathrm{new}}^{-i}=\pi_{\mathrm{old}}^{-i})\]

This means the individual objective is the same as the joint objective so the monotonic improvement is guaranteed. 

Then we can show the proof of Proposition 1.

Proof.: We will build a new MDP \(\tilde{M}\) based on the original MDP. We keep the action space \(\tilde{A}=A=\times_{i=1}^{n}A^{i}\), where \(A^{i}\) is the original action space of agent \(i\). The new state space contains multiple layers. We define \(\tilde{S}^{k}=S\times(\times_{i=1}^{k}A^{i})\) for \(k=1,2,\cdots,n-1\) and \(\tilde{S}^{0}=S\), where \(S\) is the original state space. Then a new state \(\tilde{s}^{k}\in\tilde{S}^{k}\) means that \(\tilde{s}^{k}=(s,a^{1},a^{2},\cdots,a^{k})\). The total new state space is defined as \(\tilde{S}=\cup_{i=0}^{n-1}\tilde{S}^{i}\). Next we define the transition probability \(\tilde{P}\) as following:

\[\tilde{P}(\tilde{s}^{\prime}|\tilde{s}^{k},a^{k+1},a^{-(k+1)}) =\mathbbm{1}\left(\tilde{s}^{\prime}=(\tilde{s}^{k},a^{k+1})\right),\ k<n-1\] \[\tilde{P}(\tilde{s}^{\prime}|\tilde{s}^{k},a^{k+1},a^{-(k+1)}) =\mathbbm{1}\left(\tilde{s}^{\prime}\in\tilde{S}^{0}\right)P( \tilde{s}^{\prime}|\tilde{s}^{k},a^{k+1}),\ k=n-1.\]

This means that the state in the layer \(k\) can only transition to the state in the layer \(k+1\) with the corresponding action, and the state in the layer \(n-1\) will transition to the layer 0 with the probability \(P\) in the original MDP. The reward function \(\tilde{r}\) is defined as following:

\[\tilde{r}(\tilde{s},\bm{a})=\mathbbm{1}\left(\tilde{s}\in\tilde{S}_{0}\right)r( \tilde{s},\bm{a}).\]

This means the reward is only obtained when the state in layer 0 and the value is the same as the original reward function. Now we obtain the total definition of the new MDP \(\tilde{M}=\{\tilde{S},\tilde{A},\tilde{P},\tilde{r},\gamma\}\).

Then we claim that if all agents learn in multi-agent sequential decision-making by PPO, they are actually taking agent-by-agent PPO in the new MDP \(\tilde{M}\). To be precise, one update of multi-agentsequential decision-making in the original MDP \(\tilde{M}\) equals to a round of update from agent 1 to agent \(n\) by agent-by-agent PPO in the new MDP \(\tilde{M}\). Moreover, the total reward of a round in the new MDP \(\tilde{M}\) is the same as the reward in one timestep in the original MDP \(M\). With this conclusion and Lemma 1, we complete the proof.

The proof of Proposition 2 can be seen as a corollary of the proof of Proposition 1.

Proof.: From Lemma 1 we know that the monotonic improvement of the joint policy in the new MDP \(\tilde{M}\) is guaranteed for each update of one single agent's policy. So even if the different round of updates in the new MDP \(\tilde{M}\) is with different order of the decision-making, the monotonic improvement of the joint policy is still guaranteed. Finally, from the proof of Proposition 1, we know that the monotonic improvement in the new MDP \(\tilde{M}\) equals to the monotonic improvement in the original MDP \(M\). These complete the proof. 

## Appendix B Proofs of Theorem 1

**Lemma 2** (TVD of the joint distributions).: _Suppose we have two distribution \(p_{1}(x,y)=p_{1}(x)p_{1}(x|y)\) and \(p_{2}(x,y)=p_{2}(x)p_{2}(x|y)\). We can bound the total variation distance of the joint as:_

\[D_{TV}(p_{1}(x,y)||p_{2}(x,y))\leq D_{TV}(p_{1}(x)||p_{2}(x))+\max_{x}D_{TV}(p _{1}(y|x)||p_{2}(y|x))\]

Proof.: See [Janner et al., 2019] (Lemma B.1). 

**Lemma 3** (Markov chain TVD bound, time-varing).: _Suppose the expected KL-divergence between two transition is bounded as \(\max_{t}\mathbb{E}_{s\sim p_{1,t}(s)}D_{KL}(p_{1}(s^{\prime}|s)||p_{2}(s^{ \prime}|s))\leq\delta\), and the initial state distributions are the same \(p_{1,t=0}(s)=p_{2,t=0}(s)\). Then the distance in the state marginal is bounded as:_

\[D_{TV}(p_{1,t}(s)||p_{2,t}(s))\leq t\delta\]

Proof.: See [Janner et al., 2019] (Lemma B.2). 

**Lemma 4** (Branched Returns Bound).: _Suppose the expected KL-divergence between two dynamics distributions is bounded as \(\max_{t}\mathbb{E}_{s\sim p_{1,t}(s)}[D_{TV}(p_{1}(s^{\prime}|s,\bm{a})||p_{2}( s^{\prime}|s,\bm{a}))]\), and the policy divergences at level \(k\) are bounded as \(\max_{s,\bm{a}^{1:k-1}}D_{TV}(\pi_{1}(a^{k}|s,\bm{a}^{1:k-1})||\pi_{2}(a^{k}|s, \bm{a}^{1:k-1}))\leq\epsilon_{\pi_{k}}\). Then the returns are bounded as:_

\[|\eta_{1}-\eta_{2}|\leq\frac{2r_{\max}\gamma(\epsilon_{m}+\sum_{k=1}^{n} \epsilon_{\pi_{k}})}{(1-\gamma)^{2}}+\frac{2r_{\max}\sum_{k=1}^{n}\epsilon_{ \pi_{k}}}{1-\gamma},\]

_where \(r_{\max}\) is the upper bound of the reward function._

Proof.: Here, \(\eta_{1}\) denotes the returns of \(\bm{\pi}_{1}\) under dynamics \(p_{1}(s^{\prime}|s,\bm{a})\), and \(\eta_{2}\) denotes the returns of \(\bm{\pi}_{2}\) under dynamics \(p_{2}(s^{\prime}|s,\bm{a})\). Then we have

\[|\eta_{1}-\eta_{2}| =|\sum_{s,\bm{a}}(p_{1}(s,\bm{a})-p_{2}(s,\bm{a}))r(s,\bm{a})|\] \[=|\sum_{t}\sum_{s,\bm{a}}\gamma^{t}(p_{1,t}(s,\bm{a})-p_{2,t}(s, \bm{a}))r(s,\bm{a})|\] \[\leq\sum_{t}\sum_{s,\bm{a}}\gamma^{t}|p_{1,t}(s,\bm{a})-p_{2,t}(s,\bm{a})|r(s,\bm{a})\] \[\leq r_{\max}\sum_{t}\sum_{s,\bm{a}}\gamma^{t}|p_{1,t}(s,\bm{a})-p _{2,t}(s,\bm{a})|.\]By Lemma 2, we get

\[\max_{s}D_{TV}(\pi_{1}(\bm{a}|s)||\pi_{2}(\bm{a}|s)) \leq\max_{s,a_{1}}D_{TV}(\pi_{1}(\bm{a}^{-1}|s,a^{1})||\pi_{2}(\bm {a}^{-1}|s,a^{1}))\] \[+\max_{s}D_{TV}(\pi_{1}(a^{1}|s)||\pi_{2}(a^{1}|s))\] \[\leq\cdots\] \[\leq\sum_{k=1}^{n}\max_{s,\bm{a}^{1:k-1}}D_{TV}(\pi_{1}(a^{k}|s, \bm{a}^{1:k-1})||\pi_{2}(a^{k}|s,\bm{a}^{1:k-1}))\] \[\leq\sum_{k=1}^{n}\epsilon_{\pi_{k}}.\]

We then apply Lemma 3, using \(\delta=\epsilon_{m}+\sum_{k=1}^{n}\epsilon_{\pi_{k}}\) (via Lemma 3 and 2) to get

\[D_{TV}(p_{1,t}(s)||p_{2,t}(s)) \leq t\max_{t}E_{s\sim p_{1,t}(s)}D_{TV}(p_{1,t}(s^{\prime}|s)||p _{2,t}(s^{\prime}|s))\] \[\leq t\max_{t}E_{s\sim p_{1,t}(s)}D_{TV}(p_{1,t}(s^{\prime},\bm{ a}|s)||p_{2,t}(s^{\prime},\bm{a}|s))\] \[\leq t(\max_{t}E_{s\sim p_{1,t}(s)}D_{TV}(p_{1,t}(s^{\prime}|s, \bm{a})||p_{2,t}(s^{\prime}|s,\bm{a}))\] \[+\max_{t}E_{s\sim p_{1,t}(s)}\max_{s}D_{TV}(\bm{\pi}_{1,t}(\bm{a} |s)||\bm{\pi}_{2,t}(\bm{a}|s)))\] \[\leq t(\epsilon_{m}+\sum_{k=1}^{n}\epsilon_{\pi_{k}})\]

And we also get \(D_{TV}(p_{1,t}(s,\bm{a})||p_{2,t}(s,\bm{a}))\leq t(\epsilon_{m}+\sum_{k=1}^{n} \epsilon_{\pi_{k}})+\sum_{k=1}^{n}\epsilon_{\pi_{k}}\) by Lemma 2. Thus, by plugging this back, we get:

\[|\eta_{1}-\eta_{2}| \leq r_{\max}\sum_{t}\sum_{s,\bm{a}}\gamma^{t}|p_{1,t}(s,\bm{a}) -p_{2,t}(s,\bm{a})|\] \[\leq 2r_{\max}\sum_{t}\gamma^{t}(t(\epsilon_{m}+\sum_{k=1}^{n} \epsilon_{\pi_{k}})+\sum_{k=1}^{n}\epsilon_{\pi_{k}})\] \[\leq 2r_{\max}(\frac{\gamma(\epsilon_{m}+\sum_{k=1}^{n}\epsilon_{ \pi_{k}}))}{(1-\gamma)^{2}}+\frac{\sum_{k=1}^{n}\epsilon_{\pi_{k}}}{1-\gamma})\]

Then we can show the proof of Theorem 1.

Proof.: Let \(\bm{\pi}_{\beta}\) denote the data collecting policy. We use Lemma 4 to bound the returns, but it will require bounded model error under the new policy \(\bm{\pi}\). Thus, we need to introduce \(\bm{\pi}_{\beta}\) by adding and subtracting \(\eta[\bm{\pi}_{\beta}]\), to get:

\[\hat{\eta}[\bm{\pi}]-\eta[\bm{\pi}]=\hat{\eta}[\bm{\pi}]-\eta[\bm{\pi}_{\beta} ]+\eta[\bm{\pi}_{\beta}]-\eta[\bm{\pi}].\]

we can bound \(L_{1}\) and \(L_{2}\) both using Lemma 4 by using \(\delta=\sum_{k=1}^{n}\epsilon_{\pi_{k}}\) and \(\delta=\epsilon_{m}+\sum_{k=1}^{n}\epsilon_{\pi_{k}}\) respectively, and obtain:

\[L_{1}\geq-\frac{2\gamma r_{\max}\sum_{k=1}^{n}\epsilon_{\pi_{k}}}{(1-\gamma)^ {2}}-\frac{2r_{\max}\sum_{k=1}^{n}\epsilon_{\pi_{k}}}{(1-\gamma)}\]

\[L_{2}\geq-\frac{2\gamma r_{\max}(\epsilon_{\pi_{m}}+\sum_{k=1}^{n}\epsilon_{ \pi_{k}})}{(1-\gamma)^{2}}-\frac{2r_{\max}\sum_{k=1}^{n}\epsilon_{\pi_{k}}}{(1 -\gamma)}.\]

Adding these two bounds together yields the conclusion.

[MISSING_PAGE_FAIL:17]

/* Agents in \(\mathcal{A}\) communicate the intention values with each other*/  Set \(p_{i}\) to be the agent \(j\) with the maximum \(v_{j}\)  Append \(p_{i}\) to \(\mathcal{P}\) and remove it from \(\mathcal{A}\) endfor ```

**Algorithm 2**Intention Value Calculation of Agent \(a\)

```
0: Already determined priority \(\mathcal{P}\), remaining agents \(\mathcal{A}\), number of sampling trajectories \(F\), length of predicted future trajectory \(H\), policy \(\pi\) and attention module \(\mathrm{AM_{a}}\), world model \(\mathcal{M}\) and attention module \(\mathrm{AM_{w}}\), discount factor \(\gamma\) for\(i=1,2,...,F\)do  Randomly shuffle \(\mathcal{A}\setminus\{a\}\) to sample a decision-making priority \(\mathcal{P}_{\mathcal{A}\setminus\{a\}}\) of the remaining agents except agent \(a\) for\(j=0,1,...,H-1\)do \(\hat{\bm{a}}^{upper}=\{\}\): predicted actions from all upper-level agents for\(k\)in\(\mathrm{Concat}(\mathcal{P},[a],\mathcal{P}_{\mathcal{A}\setminus\{a\}})\)do  Sample \(\hat{a}^{k}\) following \(\pi(\cdot|\mathrm{AM_{a}}(\bm{h}_{t+j},\bm{a}^{upper}))\)  Append \(\hat{a}^{k}\) to \(\hat{\bm{a}}^{upper}\) endfor  Rollout one step with the world model \(\hat{\bm{o}}_{t+j+1},\hat{r}_{t+j+1}=\mathcal{M}(\mathrm{AM_{w}}(\bm{h}_{t+j},\bm{a}^{upper}))\) endfor  Compute the return of the trajectory \(v_{i}=\sum_{t^{\prime}=t+1}^{t+H}\gamma^{t^{\prime}-t-1}\hat{r}_{t^{\prime}}\) via the critic endfor  Compute the average return \(v=\frac{1}{F}\sum_{i=1}^{F}v_{i}\) ```

**Algorithm 3**Launching Phase

```
0: Decision-making priority \(\mathcal{P}\), policy \(\pi\) and \(\mathrm{AM_{a}}\) \(\bm{a}_{t}^{upper}=\{\}\): actions from all upper-level agents for\(i\)in\(\mathcal{P}\)do  Sample \(a_{t}^{i}\) following \(\pi_{i}(\cdot|\mathrm{AM_{a}}(\bm{h}_{t},\bm{a}_{t}^{upper}))\)  Append \(a_{t}^{i}\) to \(\bm{a}_{t}^{upper}\) /* Send \(\bm{a}_{t}^{upper}\) to the lower agent*/ endfor  Interact with the environment with \(\bm{a}_{t}\) ```

**Algorithm 4**Local Negotiation Phase of Agent \(a\)

We also provide the pseudo-code of the local communication version as below:

```
0: Neighbouring agents \(\mathcal{N}\) /* Agents communicate the hidden state \(\bm{h}\) of their observations with neighbouring agents*/  Compute local intention \(v_{a}\) via Algorithm 5 /* Send \(v_{a}\) to neighbouring agents and receive \(\{v_{i}\}_{i\in\mathcal{N}}\) from them */  Set upper-level neighbouring agents \(\mathcal{N}^{upper}=\{i\mid v_{i}>v_{a},i\in\mathcal{N}\}\)  Set lower-level neighbouring agents \(\mathcal{N}^{lower}=\{i\mid v_{i}<v_{a},i\in\mathcal{N}\}\) ```

**Algorithm 5**Local Intention Value Calculation of Agent \(a\)

```
0: Neighbouring agents \(\mathcal{N}\), number of sampling trajectories \(F\), length of predicted future trajectory \(H\), policy \(\pi\) and \(\mathrm{AM_{a}}\), world model \(\mathcal{M}\) and \(\mathrm{AM_{w}}\), discount factor \(\gamma\) for\(i=1,2,...,F\)do  Randomly shuffle \(\mathcal{N}\) to sample a local decision-making priority \(\mathcal{P}_{\mathcal{N}}\) for\(j=0,1,...,H-1\)do \(\hat{\bm{a}}^{upper}=\{\}\): predicted actions from all upper-level agents for\(k\)in\(\mathrm{Concat}([a],\mathcal{P}_{\mathcal{N}})\)do  Sample \(\hat{a}^{k}\) following \(\pi(\cdot|\mathrm{AM_{a}}(\bm{h}_{t+j},\bm{a}^{upper}))\)  Append \(\hat{a}^{k}\) to \(\hat{\bm{a}}^{upper}\) endfor  Rollout one step with the world model \(\hat{\bm{o}}_{t+j+1},\hat{r}_{t+j+1}=\mathcal{M}(\mathrm{AM_{w}}(\bm{h}_{t+j},\bm{a}^{upper}))\) ```

**Algorithm 6**Local Intention Value Calculation of Agent \(a\)

**end for**

Compute the return of the trajectory \(v_{i}=\sum_{t^{\prime}=t+1}^{t+H}\gamma^{t^{\prime}-t-1}\hat{r}_{t^{\prime}}\)

**end for**

Compute the average return \(v=\frac{1}{F}\sum_{i=1}^{F}v_{i}\) via the critic

```
0: Upper-level neighbouring agents \(\mathcal{N}^{upper}\), lower-level neighbouring agents \(\mathcal{N}^{lower}\), policy \(\pi\) and \(\mathrm{AM}_{\mathrm{a}}\) /* Receive upper-level actions \(\bm{a}_{t}^{upper}\) from all upper-level neighbouring agents \(\mathcal{N}^{upper}\) /*  Sample \(a_{t}^{i}\) following \(\pi_{i}(\cdot|\mathrm{AM}_{\mathrm{a}}(\bm{h}_{t},\bm{a}_{t}^{upper}))\) /* Send \(a_{t}^{i}\) to all lower-level neighbouring agents \(\mathcal{N}^{lower}\) */  Interact with the environment with \(\bm{a}_{t}\) ```

**Algorithm 6** Local Launching Phase of Agent \(a\)

### Architecture and Hyperparameters

Our models, including SeqComm and its ablations, are implemented based on MAPPO. Two fully connected layers realize the critic and policy network. As for the attention module, key, query, and value have one fully connected layer each. The size of the hidden layers is 100. Tanh functions are used as nonlinearity. As there is no released code of TarMAC, we implement TarMAC by ourselves, following the instructions mentioned in the original papers (Das et al., 2019).

For the world model, observations and actions are firstly encoded by a fully connected layer. The output size for the observation encoder is 48, and the output size for the action encoder is 16. Then, the outputs of the encoder will be passed into the attention module using the same structure aforementioned. Finally, we use a fully connected layer to decode. In these layers, Tanh is used as the nonlinearity.

SeqComm and its ablation baseline share the same hyperparameters. For Protoss, the learning rate is \(1\mathrm{e}^{-5}\), while for Terran and Zerg, the learning rate is \(2.5\mathrm{e}^{-5}\). \(H\) and \(F\) for calculating intention value is set to 20 and 2. For TarMAC, the learning rate is tuned as \(5\mathrm{e}^{-5}\). TarMAC adopts MAPPO as the backbone and two-round communication mechanism. For MAPPO, we follow the default settings of the official code (Yu et al., 2021).

### Attention Module

Attention module (AM) is applied to process messages in the world model, critic network, and policy network. AM consists of three components: query, key, and values. The output of AM is the weighted sum of values, where the weight of value is determined by the dot product of the query and the corresponding key.

For AM in the world model denoted as \(\mathrm{AM}_{\mathrm{w}}\), agent \(i\) gets messages \(\bm{m}_{t}^{-i}=\bm{h}_{t}^{-i}\) from all other agents at timestep \(t\) in negotiation phase, and predicts a query vector \(q_{t}^{i}\) following \(\mathrm{AM}_{\mathrm{w},q}^{i}(h_{t}^{i})\). The query is used to compute a dot product with keys \(\bm{k}_{t}=[k_{t}^{1},\cdots,k_{t}^{n}]\). Note that \(k_{t}^{j}\) is obtained by the message from agent \(j\) following \(\mathrm{AM}_{\mathrm{a},k}^{i}(h_{t}^{j})\) for \(j\neq i\), and \(k_{t}^{i}\) is from \(\mathrm{AM}_{\mathrm{neg},k}^{i}(h_{t}^{i})\). Besides, it is scaled by \(1/\sqrt{d_{k}}\) followed by a softmax to obtain attention weights \(\alpha\) for each value vector:

\[\alpha_{i}=\mathrm{softmax}\left[\frac{{q_{t}^{iT}}^{k_{1}}}{\sqrt{d_{k}}} \cdots\underbrace{\frac{{q_{t}^{iT}}^{k_{t}^{j}}}{\sqrt{d_{k}}}}_{\alpha_{ij}} \cdots\frac{{q_{t}^{iT}}^{k_{t}^{n}}}{\sqrt{d_{k}}}\right]\] (1)

The output of attention module is defined as: \(c_{t}^{i}=\sum_{j=1}^{n}\alpha_{ij}v_{t}^{j}\), where \(v_{t}^{j}\) is obtained from messages or its own hidden state of observation following \(\mathrm{AM}_{\mathrm{w},v}^{i}(\cdot)\).

As for AM in the policy and critic network denoted as \(\mathrm{AM}_{\mathrm{a}}\), agent \(i\) gets additional messages from upper-level agent in the launching phase. The message from upper-level and lower-level agent can be expanded as \(\bm{m}_{t}^{upper}=[\bm{h}_{t}^{upper},\bm{a}_{t}^{upper}]\) and \(\bm{m}_{t}^{lower}=[\bm{h}_{t}^{lower},0]\), respectively. In addition, the query depends on agent's own hidden state of observation \(h_{t}^{i}\), but keys and values are only from messages of other agents.

### Training

The training of SeqComm is an extension of MAPPO. The observation encoder \(e\), the critic \(V\), and the policy \(\pi\) are respectively parameterized by \(\theta_{e}\), \(\theta_{v}\), \(\theta_{\pi}\). Besides, the attention module \(\mathrm{AM_{a}}\) is parameterized by \(\theta_{a}\) and takes as input the agent's hidden state, the messages (hidden states of other agents) in the negotiation phase, and the messages (the actions of upper-level agents) in launching phase. Let \(\mathcal{D}=\{\tau_{k}\}_{k=1}^{K}\) be a set of trajectories by running policy in the environment. Note that we drop time \(t\) in the following notations for simplicity.

The value function is fitted by regression on mean-squared error:

\[\mathcal{L}(\theta_{v},\theta_{a},\theta_{e})=\frac{1}{KT}\sum_{\tau\in \mathcal{D}}\sum_{t=0}^{T-1}\left\|V(\mathrm{AM_{a}}(e(\bm{o}),\bm{a}^{upper}) )-\hat{R}\right\|_{2}^{2}\] (2)

where \(\hat{R}\) is the discount rewards-to-go.

We update the policy by maximizing the PPO-Clip objective:

\[\mathcal{L}(\theta_{\pi},\theta_{a},\theta_{e})=\frac{1}{KT}\sum_{\tau\in \mathcal{D}}\sum_{t=0}^{T-1}\min(\frac{\pi(a|\mathrm{AM_{a}}(e(\bm{o}),\bm{a}^ {upper}))}{\pi_{old}(a|\mathrm{AM_{a}}(e(\bm{o}),\bm{a}^{upper}))}A_{\pi_{old} },g(\epsilon,A_{\pi_{old}}))\] (3)

where \(g(\epsilon,A)=\begin{cases}(1+\epsilon)A&A\geq 0\\ (1-\epsilon)A&A\leq 0\end{cases}\), and \(A_{\pi_{old}}(\bm{o},\bm{a}^{upper},a)\) is computed using the GAE method.

The world model \(\mathcal{M}\) is parameterized by \(\theta_{w}\) is trained as a regression model using the training data set \(\mathcal{S}\). It is updated with the loss:

\[\mathcal{L}(\theta_{w})=\frac{1}{|\mathcal{S}|}\sum_{\bm{o},\bm{a},\bm{o}^{ \prime},r\in\mathcal{S}}\left\|(\bm{o}^{\prime},r)-\mathcal{M}(\mathrm{AM_{w} }(e(\bm{o}),\bm{a}))\right\|_{2}^{2}.\] (4)

We trained our model on one GeForce GTX 1050 Ti and Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz.

### Additional Ablation Studies

We conduct a comparison of SeqComm against MAIC and CommFormer across six different maps: Protoss, Terran, and Zerg in 5v5 scenarios (first row) and Protoss, Terran, and Zerg in 10v10 scenarios (second row). The evaluation uses the official codebase for each method, with three random seeds per map under a full communication setting. The results refer to Figure 8.

### Emergence of Behavioral Patterns

We have visualized several key frames in Figure 9 to highlight the observed behavioral patterns. In the combat game, concentrating attacks on a single enemy is consistently more effective than dispersing them. In frames 1-3, the agents lack specific targets until one agent, located at the end of the orange arrow, approaches an enemy in the bottom right corner. By frame 4, following the negotiation phase, this agent is designated as the highest-level agent (level 5), given its advantageous position to choose an enemy to attack. Once lower-level agents receive the actions from higher-level agents (represented by the white dashed line), all the red units cease random roaming and instead coordinate a unified attack on the blue units. A similar pattern can be observed in frames 7-9.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the Abstract, we outlined our contributions and reiterated our scope in the introduction. Guidelines:

Figure 8: Results of experiments with extra baseline algorithms.

Figure 9: Illustration of the Emergence of Behavioral Patterns* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss our limitations in the Conclusions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The Method or Appendix includes all assumptions and proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ** Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We fully disclose all the information needed to reproduce the main experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We plan to release data and code ASAP/upon acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all the training and test details in the Experiments and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars that are suitably and correctly defined, along with other appropriate information about the statistical significance of the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]. Justification: We provide sufficient information on the computer resources in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All the research conducted conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: SeqComm is a MARL method that does not have potential societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credit all the assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets.

* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.