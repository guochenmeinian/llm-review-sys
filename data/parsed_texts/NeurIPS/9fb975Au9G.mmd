# Django: Detecting Trojans in Object Detection Models via Gaussian Focus Calibration

 Guangyu Shen

Purdue University

West Lafayette, IN, 47907

shen447@purdue.edu

&Siyuan Cheng1

Purdue University

West Lafayette, IN 47907

cheng535@purdue.edu

&Guanhong Tao

Purdue University

West Lafayette, IN 47907

taog@purdue.edu

&Kaiyuan Zhang

Purdue University

West Lafayette, IN, 47907

zhan4057@purdue.edu

&Yingqi Liu

Microsoft

Redmond, Washington 98052

yingqiliu@microsoft.com

&Shengwei An

Purdue University

West Lafayette, IN, 47907

an93@purdue.edu

&Shiqing Ma

University of Massachusetts at Amherst

Amherst, MA, 01003

shiqingma@umass.edu

&Xiangyu Zhang

Purdue University

West Lafayette, IN, 47907

xyzhang@cs.purdue.edu

Equal Contribution

###### Abstract

Object detection models are vulnerable to backdoor or trojan attacks, where an attacker can inject malicious triggers into the model, leading to altered behavior during inference. As a defense mechanism, trigger inversion leverages optimization to reverse-engineer triggers and identify compromised models. While existing trigger inversion methods assume that each instance from the support set is equally affected by the injected trigger, we observe that the poison effect can vary significantly across bounding boxes in object detection models due to its dense prediction nature, leading to an undesired optimization objective misalignment issue for existing trigger reverse-engineering methods. To address this challenge, we propose the first object detection backdoor detection framework Django (_Detecting Trojans in Object Detection Models via Gaussian Focus Calibration_). It leverages a dynamic Gaussian weighting scheme that prioritizes more vulnerable victim boxes and assigns appropriate coefficients to calibrate the optimization objective during trigger inversion. In addition, we combine Django with a novel label proposal pre-processing technique to enhance its efficiency. We evaluate Django on 3 object detection image datasets, 3 model architectures, and 2 types of attacks, with a total of 168 models. Our experimental results show that Django outperforms 6 state-of-the-art baselines, with up to 38% accuracy improvement and 10x reduced overhead. The code is available at https://github.com/PurduePAML/DJGO.

## 1 Introduction

Object detection is an extensively studied computer vision application that aims to identify multiple objects in a given image. It has been widely integrated into various real-world systems, including public surveillance [51], autonomous driving [14, 25], optical character recognition (OCR)[41], etc.

State-of-the-art object detection systems, e.g., SSD [32] and YOLO [44], are primarily built on Deep Learning (DL) models [13], owing to their remarkable feature representation capabilities.

Despite the impressive performance of existing object detection approaches, they are vulnerable to backdoor attacks. Backdoor attack is a form of training-time attack, in which a malicious adversary strategically embeds an imperceptible _trigger_ (e.g., a small image patch) onto a small set of training data. DL models trained on this poisoned dataset will predict the attacker-chosen _target label_ whenever an input is stamped with the trigger. There are two major backdoor attacks in object detection, namely, _misclassification attack_[3] and _evasion attack_. As shown in Figure 1, misclassification attack causes the object detection model to misclassify an object as a target object, whereas evasion attack makes the model fail to recognize certain objects (i.e., classify traffic signs as the background). Such attacks can significantly hinder the deployment of object detection systems in real world as they may cause severe security issues or even life-threatening incidents.

To counter the backdoor threat, researchers have proposed a multitude of defense techniques aimed at safeguarding the model against backdoor attacks throughout its entire life cycle [59, 33, 69, 16, 26, 22, 18, 10, 54, 50, 36, 72, 27, 65, 31, 35]. Among them, trigger inversion [59, 55, 18] is one of the most popular backdoor scanning methods and has demonstrated its effectiveness in various tasks [59, 35]. Given a small set of clean samples and the subject model, trigger inversion techniques aims to determine if the model is backdoored or not by reverse-engineering the underlying triggers. The task of trigger inversion can be formulated as an optimization problem with specific constraints, such as trigger size [59, 18] or particular stylistic attributes [9, 42].

In order to detect backdoors in object detection, a direct solution is to adapt trigger inversion from the classification task to object detection. We however find that a simple adaptation of trigger inversion fails short. This attributes to the significantly denser output of object detection models due to the nature of the task. In specific, an object detection model, such as SSD [32], typically produces thousands of bounding boxes with corresponding class probabilities to capture objects of varying scales within a single image. Attackers can exploit this extensive output space to conceal injected triggers and mislead the optimization process during trigger inversion, causing the _loss misalignment issue_. That is, throughout the entire trigger inversion process, the loss values consistently remain either significantly high or extremely low, regardless of the _Attack Success Rate (ASR)_ that denotes the ratio of misclassified samples when the trigger is applied. This misalignment property hinders the identification of high-quality triggers, leading to low backdoor detection performance.

Our study reveals that the underlying cause of the _loss misalignment issue_ stems from the unequal impact of poisoning on individual bounding boxes within the backdoored model. More specifically, the backdoor trigger affects only a small fraction of bounding boxes, while leaving the majority of them largely unaffected. Consequently, the aggregated gradient information is obfuscated by remaining benign bounding boxes during inversion, thereby impeding the optimizer from accurately estimating the gradient direction. Based on this observation, we propose the first trigger inversion-based backdoor detection framework for object detection: Django (_Detecting Trojans in Object Detection Models via Gaussian Focus Calibration_). The overview is shown in Figure 2. It features a novel _Gaussian Focus Loss_ to calibrate the misaligned loss during inversion by dynamically assigning weights to individual boxes based on their vulnerability. Equipped with a label proposal pre-processor, Django is able to quickly identify malicious victim-target labels and effectively invert the injected trigger lies in the backdoored model. Extensive evaluation of 3 object detection datasets, utilizing 3

Figure 1: Triggers with different effects. The blue patch in Figure 1(a) is an evasion trigger, causing the bounding box to disappear. Figure 1(b) represents a misclassification trigger (yellow), leading to the misclassification of “pizza” as “giraffe”.

different model architectures with a total of 168 models, demonstrates the superiority of Django over six existing state-of-the-art backdoor scanning techniques. Specifically, we achieve up to 0.38 ROC improvement and a substantial reduction in scanning time.

## 2 Background & Threat Model

**Deep Learning based Object Detection Techniques.** Extensive research efforts have been devoted to deep learning (DL) object detection, encompassing various aspects such as neural network architecture design [32; 45; 2], training optimization [29; 61; 74], and robustness [71; 66; 6]. These endeavors contribute to a broad body of work aimed at advancing the field of DL object detection. Specifically, given an image \(x\in\mathcal{X}\) that contains \(p\) objects, an object detection neural network \(g_{\theta}\) (parameterized by \(\theta\)) outputs \(K\) (\(K\gg p\)) 5-tuples: \(\hat{y}=g_{\theta}(x)=\{\hat{b}_{k}^{x},\hat{b}_{k}^{y},\hat{b}_{k}^{h},\hat{b} _{k}^{w},\hat{p}_{k}\}_{k=1}^{K}=\{\hat{b}_{k},\hat{p}_{k}\}_{k=1}^{K}\), shorten as \(\hat{b}_{k}(x)\) and \(\hat{p}_{k}(x)\), which denote the center coordinates, height, width, and class probability of the \(k\)-th predicted bounding box. Note that \(K\) is usually much larger than \(p\) (\(K=8732\) in SSD [32], \(p\approx 7.7\) in COCO dataset [30]). Box matching algorithms [32; 46] and _Non-Maximum Suppression_(NMS) [20] were introduced to address the challenge posed by the substantial box number discrepancy between the prediction and ground-truth in both training and inference stages.

**Backdoor Attack & Defense.** Backdoor attacks can be carried out through data poisoning with modified or clean labels [17; 34; 7; 58] and model parameter hijacking [47]. Initially, small image patches were used as backdoor triggers [17; 8; 48], and later on more complex triggers have been successfully injected in DL models [28; 64; 33; 37]. These attacks are not limited to image classification models; they also affect Large Language Models [70], code models [73; 52], Reinforcement Learning agents [60], pre-trained image encoders [24; 56], and object detectors [3; 39]. Researchers have proposed various approaches from defense perspectives. During the training stage, methods such as poisoned sample identification and filtering have been proposed [40; 53; 4; 57; 19; 43]. In the post-training stage, backdoor scanning techniques aim to recover injected triggers from trained models [59; 49; 54; 62; 10; 63; 35; 50; 15]. Backdoor removal techniques are employed to eliminate triggers with minimal resource consumption and performance impact. Real-time rejection of trigger-carrying samples during inference can be achieved using methods like [12; 16]. Furthermore, theoretical analyses have been conducted to understand backdoor attacks [67; 68; 72; 23]. In this work, we prioritize the practical application of trigger inversion-based backdoor detection, with a specific focus on object detection models.

## 3 Methodology

**Threat Model.** Our work considers the standard setting used in existing backdoor scanning literature [59; 33; 18; 10], where the defender has a small set of clean samples from each class in the validation set (10 in our paper), but no poison samples and white-box access to the model under scanning. The defense goal is to classify the benignity of the subject model. Our focus in this study is on _label specific_ polygon triggers with two types of effects: _misclassification_ and _evasion_, which cause the model to predict an object as a target object and the background, respectively.

### Misalignment of CE Loss and ASR in Object Detection Model Trigger Inversion

**Backdoor Detection via Trigger Inversion.** Trigger inversion aims to decide whether the model is backdoored or not by reverse-engineering the injected trigger through optimization. Consider an image classification model \(f_{\theta}\) parameterized by \(\theta\) with \(C\) classes and a small support set \(\mathcal{S}_{c_{v}}\) from

Figure 2: Overview of Django

each class \(c_{v}\). Trigger inversion [49; 59] aims to solve the following optimization objective.

\[\min_{m,p}\mathbb{E}_{x\in\mathcal{S}_{c_{v}}}[\mathcal{L}(f_{\theta}(\phi(x)),c _{t})]+\beta||m||_{1},\text{where }\phi(x)=(1-m)\odot x+m\odot p.\] (1)

Variables \(m\) and \(p\) are the trigger mask and pattern respectively, to be optimized. The loss function \(\mathcal{L}(\cdot)\) typically rescues the task loss function as a measurement of the degree of the misbehaved model. In classification and object detection, it will be the cross entropy (CE) loss. \(\beta\) is the coefficient on the regularization term, and \(||m||_{1}\) represents the \(\ell_{1}\) norm of the mask. Since the defender lacks the knowledge of the exact poisoned victim and target labels, the backdoor scanning requires optimizing Eq. 1 for every label pair \((c_{v},c_{j})\). If any of these pairs yields a trigger with an exceptionally small size, it is indicative that the model is trojaned.

Intuitively, it shall be effortless to adapt existing trigger inversion methods from image classification to object detection. Considering the disparity of these two tasks, object detection models generate much denser output for each input, i.e., thousands of bounding boxes versus one label in classification. We can make adjustments to Eq. 1 to optimize a trigger that can impact samples at the box level rather than the entire image. Given a sample containing a victim object, one naive modification is to optimize a trigger that can influence all bounding boxes that are close to the victim object. The objective function can be reformulated as follows:

\[\begin{split}&\min_{m,p}\mathbb{E}_{x\in\mathcal{S}_{c_{v}}} \mathbb{E}_{\hat{o}\in\mathcal{T}_{x}}[\mathcal{L}(\hat{o},c_{t})]+\beta||m|| _{1},\\ &\text{where }\mathcal{T}_{x}=\{\hat{p}_{k}(\phi(x))\mid \mathcal{J}[\hat{b}_{k}(\phi(x)),b^{*}]\geq\alpha\}\text{ and }\phi(x)=(1-m)\odot x+m\odot p.\end{split}\] (2)

We denote the predicted probability and coordinates of the \(k\)-th bounding box of sample \(\phi(x)\) as \(\hat{p}_{k}(\phi(x))\) and \(\hat{b}_{k}(\phi(x))\), respectively. The function \(\mathcal{J}(\cdot)\) refers to the Jaccard Overlap, also known as Intersection over Union. \(b^{*}\) denotes the ground-truth box coordinates. Intuitively, \(\mathcal{T}_{x}\) indicates the class confidence of a set of predicted boxes surrounding the victim object. Additionally, \(\alpha\) is a pre-defined threshold used to quantify the degree of overlap between predictions and the ground-truth box and set to 0.5 by default. In this naive extension, the objective is to consider every predicted box surrounding the victim object as the target of the attack and aim to reverse-engineer a trigger that compromises all of them. We explain this limitation in the following.

**Mis-aligned Loss and ASR.**_Attack Success Rate (ASR)_ denotes the percentage of victim samples misclassified as the target label when the inverted trigger is applied (e.g., set at 90%). For _misclassification trigger_, a successful attack is defined as the presence of at least one box predicts the target class in the final detection. _Evasion Attack_ is a particular case of misclassification attack where the target label is the background class \(\emptyset\). _Cross-entropy (CE)_ loss measures the numerical difference between model output (confidence) and the desired prediction. In general, ASR and the CE loss are negatively correlated throughout the optimization process because minimizing the CE loss increases the probability of the target label. If the target label probability surpasses those of other labels, the input is misclassified as the target label. Figure 3(a) illustrates the characteristics of the CE loss and the ASR during trigger inversion for an image classification task.

Such a correlation however may not hold for object detection using a naive trigger inversion Eq. 2. Specifically, Figure 3(b) shows the inversion process on a Faster-RCNN [45] object detection model poisoned by an evasion trigger (TrojAI Round13#91, victim class 47). Note that an evasion trigger

Figure 3: ASR vs. CE loss during trigger inversion

will cause all bounding boxes surrounding the victim object to disappear. Observe that even when the CE loss is small (\(\leq 0.1\)), the ASR remains low (0%). For a model poisoned by misclassification attack (Round13#12 victim class 3), as illustrated in Figure 3(c), the CE loss remains incredibly high (\(\geq 10\)) and decreases only slightly during trigger inversion, while the ASR remains low or even decreases. Therefore, the naive trigger inversion fails to find a trigger with high ASR and cannot identify the poisoned model. The observations are similar when extending several state-of-the-art trigger inversion techniques [33, 59, 55, 18] to object detection. For instance, NC [59] only achieves 58% detection accuracy on the TrojAI dataset [1].

**Causes of Misalignment.** We study the misalignment problem by using differential analysis to compare the intermediate outputs of the object detection model with and without the ground-truth trigger, specifically the target label confidence of predicted bounding boxes. Given a sample with a victim object, we collect all predicted boxes with _IoU_\(\geq\alpha\) (\(\alpha=0.5\) means a box is close to the object) around the victim object and track their target label probability change before and after applying the ground-truth trigger. The key observation is that _not every box is equally poisoned_. Figure 4(a) shows the per-box confidence discrepancy for a poisoned model with a misclassification trigger. Although there are a total number of 84 boxes surrounding the victim object, only approximately 12% of them (10 boxes, shown in high red bars) are infected by the ground-truth backdoor trigger. The rest of the boxes only have an average of \(\leq 10^{-10}\) probability increase on the target label. For the evasion trigger in Figure 4(b), there is a slight difference, where 85% of the boxes already have high confidence (on the background class \(\emptyset\)) even without triggers. The backdoor trigger only elevates the target probability for the remaining 15% of the boxes.

This intuitively explains the misalignment issue of naive trigger reverse-engineering methods. During optimization, the cross-entropy loss of each box is equally aggregated, and the averaged gradient is used to mutate the trigger via back-propagation. However, in the poisoned model, there are only a small fraction of bounding boxes affected by the backdoor trigger. The gradient information from unaffected boxes dominates the optimization direction, causing the inversion to fail. This observation highlights the significance of considering individual bounding boxes separately when devising inversion methods for object detection, which shall _focus on boxes that have a higher likelihood of being compromised but have not been considered yet._

### Our Solution: Trigger Inversion via Gaussian Focus Loss

As discussed in the last section, the goal of our trigger inversion is to discriminate different bounding boxes during optimization. Particularly, we aim to find easy-to-flip boxes that are vulnerable and targeted by injected backdoors. However, it is non-trivial to adaptively select boxes during trigger inversion as the box output is constantly changing.

_Focal Loss_[29] is a widely used metric in object detection for assisting better training. It separates objects into hard-to-classify and easy-to-classify objects based on their training errors, and assigns different weights on their training losses. For the notation simplicity, we use a binary object detection model. Note that it is straightforward to extend it to multi-class cases. Recall \(\hat{p}_{k}\) denotes the output probability of the \(k\)-th victim box. The CE loss in Eq. 2 for the \(k\)-th victim box can be simplified as \(\mathcal{L}(\hat{p}_{k},c_{t})=-\log(\hat{p}_{k})\cdot c_{t}=-\log(\hat{p}_{k})\). _Focal Loss_ is a refined weighted CE loss aiming to solve the imbalanced learning difficulty issue in object detection models. In detail, it assigns larger weights to boxes that perform poorly (_hard examples_), i.e., small \(\hat{p}_{k}\), to enlarge its contribution to the aggregated

Figure 4: Target confidence discrepancy w/o the ground-truth trigger

loss. It assigns smaller weights to boxes that have already learned well (_easy examples_), i.e., large \(\hat{p}_{k}\):

\[\mathcal{L}_{fl}(\hat{p}_{k},c_{t})=-\lambda(1-\hat{p}_{k})^{\gamma}\log(\hat{p} _{k}),\] (3)

where \(\lambda\geq 0,\gamma\geq 1\) (\(\lambda=0.25,\gamma=2\) suggested in [29]). The coefficient of the \(k\)-th box will exponentially grow as its probability \(\hat{p}_{k}\) gets close to 0 and vice versa.

Eq. 3 provides insights to address the misalignment problem. Inspired by this, we propose _Gaussian Focus Loss_ to calibrate the misaligned loss. As illustrated in Figure 4(a), the backdoor trigger tend to infect only a small portion of victim boxes and leaves remaining largely unchanged. In other words, a trigger will mainly focus on compromising more vulnerable boxes that are easy to flip (_easy example_). Therefore, our goal is essentially opposite to the _Focal Loss_ as it aims to pay more attention to the _hard samples_. To avoid the gradient vanishing issue shown in Figure 4(b), a box shall also not be focused as long as it reaches the attack criterion, i.e., target label confidence \(\hat{p}_{k}\geq 0.1\). To summarize, the desired loss shall be able to dynamically capture a set of vulnerable boxes that have not been flipped yet, and assign a large coefficient to encourage the transition. The natural bell shape of the Gaussian Distribution perfectly satisfies our constraints*. We formulate our proposed novel _Gaussian Focus Loss_ as follows:

Footnote *: Any distributions characterized by centralized peaks are suitable for our intended purpose. We explore alternatives in Appendix D.

\[\begin{split}\mathcal{L}_{gf}(\hat{p}_{k},c_{t})& =-\mathcal{J}(\hat{b}_{k},b^{*})\phi(\hat{p}_{k}^{\gamma};\hat{ \sigma}_{k},\hat{\mu}_{k})\log(\hat{p}_{k})\\ where\ \mathcal{J}(\hat{b}_{k},b^{*})&=\textit{IoU}(\hat{b} _{k},b^{*}),\ \phi(x;\sigma,\mu)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{z-\mu}{ \sigma})^{2}}\end{split}\] (4)

It involves two key attributes to measure the vulnerability of a victim box during optimization: the overlapping with the ground-truth object and the confidence towards the target label. Namely, boxes with larger overlapping and higher target label confidence tend to be more vulnerable. The \(\hat{\mu}_{k}\) and \(\hat{\sigma}_{k}\) are the mean and standard deviation of the Gaussian Distribution. Note that we allow two parameters to be co-optimized with the inverted trigger such that we can adjust the range of focused boxes dynamically through the entire trigger inversion procedure. We initialize \(\hat{\mu}_{k}=0.1\) and \(\hat{\sigma}_{k}=2\) in this paper. Figure 5 provides a visual explanation of the coefficient dynamics during various optimization steps. Initially, when the victim boxes exhibit low confidence towards the target label, the _Gaussian Focus Loss_ assigns slightly larger weights to a considerable number of bounding boxes surrounding the initial value (0.1). As the optimization progresses, the _Gaussian Focus Loss_ prioritizes more vulnerable boxes that have higher target label probabilities by decreasing the value of \(\sigma\) and increasing the value of \(\mu\). In the later stages, the weight coefficients of boxes that have already been flipped are reduced to mitigate the issue of gradient vanishing.

### Compromised Label Proposal via Backdoor Leakage

Without knowing the exact victim and target labels, scanning backdoors is time-consuming. To reduce the time cost, pre-processing is proposed [49, 54] to quickly select a small set of promising compromised label pairs based on the widely existing _backdoor leakage_ phenomena: the poisoned

Figure 5: Coefficient dynamics during reverse-engineering a misclassification trigger

model's behavior on victim samples tends to shift towards the target label even without the appearance of the backdoor trigger. Therefore, we can feed clean samples from each class and pick the most likely target label by observing the model's output distribution shift. Specifically, given a set of samples \(\{x_{i}\}_{i=1}^{n}\) from class \(c_{i}\), we collect top-\(h\) predicted classes of each predicted box for each sample \(x_{i}\). Denote \(\omega_{j}\) as the frequency of class \(c_{j}\) appearing in the total box predictions. We consider a pair \(\{c_{i},c_{j}\}\) as a compromised label pair if \(\omega_{j}\geq\omega\). In this paper, we make a trade-off by setting \(h=5\) and \(\omega=0.5\). We also evaluate the sensitivity of hyper-parameters in Section 4.2.

## 4 Evaluation

All the experiments are conducted on a server equipped with two Intel Xeon Silver 4214 2.40GHz 12-core processors, 192 GB of RAM, and a NVIDIA RTX A6000 GPU.

**Models and Datasets.** We conduct the experiments on models from TrojAI [1] round 10 and round 13, which consists of a total of 168 models. Approximately half of these models are clean, while the other half are attacked. Our evaluation covers 3 existing object detection image datasets, including COCO [30], Synthesized Traffic Signs [1], and DOTA_v2 [11] on three representative object detection architectures: single-stage detector: SSD [32], two-stage detector: Faster-RCNN [45] and vision transformer based detector: DETR [2]. Please refer to Appendix A for more detailed description.

**Attack Settings.** We conducted an evaluation of Django by subjecting it to two types of backdoor attacks commonly observed in object detection models: Misclassification and Evasion attacks [3; 38] Misclassification attack means to flip the prediction of objects from the victim class to the target class, when the backdoor trigger is stamped on the input image. On the other hand, evasion triggers tend to suppress the recognition of victim objects, such that the model will consider them as the background. It is noteworthy that the evasion trigger can be regarded as a particular instance of the misclassification trigger, where the target label is set as the background class \(\emptyset\). As a result, its detection can be performed uniformly. Besides, we leverage stealthy polygons with different colors and textures as backdoor triggers, which are widely evaluated in many backdoor detection papers [59; 17; 49].

**Evaluation Metrics.** In our evaluation of backdoor detection methods, we employ four well-established metrics: Precision, Recall, ROC-AUC, and Average Scanning Overheads for each model. The unit of measurement we use is seconds (s), and we set a maximum threshold of 1 hour (3600 s) for all the methods being evaluated. If the scanning process exceeds 3600 seconds, it is terminated, and we rely on the existing results for making predictions. Appendix B present more details.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline D & M & Metric & ABS & NC & NC\({}^{*}\) & Pixel & Pixel\({}^{*}\) & Tabor & Tabor & Tabor \\ \hline \multirow{6}{*}{**Datasets**} & \multirow{6}{*}{**Datasets**} & Precision & 0.7143 & 0.6250 & 0.6522 & 0.4400 & 0.6666 & 0.3300 & 0.4000 & 0.8000 \\  & & Recall & 0.6250 & 0.9375 & 0.9375 & 0.5000 & 0.2500 & 0.3750 & 0.6250 & 1.0000 \\  & & ROC-AUC & 0.7109 & 0.6250 & 0.6641 & 0.6250 & 0.7083 & 0.5400 & 0.6725 & **0.9160** \\  & & Overhead(s) & 409.23 & -3660 & 1180.52 & -3660 & 953.42 & -3660 & 902.10 & 861.30 \\ \cline{2-11}  & & Precision & 0.6667 & 0.3478 & 0.6364 & 0.5512 & 0.6250 & 0.6621 & 0.7125 & 0.8571 \\  & & Recall & 0.3750 & 1.0000 & 0.8750 & 0.6110 & 0.6250 & 0.6012 & 0.7500 \\  & & ROC-AUC & 0.5352 & 0.6094 & 0.6953 & 0.6721 & 0.7500 & 0.6425 & 0.6820 & **0.8750** \\  & & Overhead(s) & 753.89 & -3660 & 2799.42 & -3600 & 2466.08 & -3660 & 2562.59 & 2128.99 \\ \cline{2-11}  & & \multirow{6}{*}{**Datasets**} & Precision & - & 0.3478 & 0.3478 & 0.4000 & 0.5000 & 0.2307 & 0.5000 & 0.8750 \\  & & Recall & - & 1.0000 & 1.0000 & 0.5000 & 0.3750 & 0.5270 & 0.6250 & 0.8750 \\  & & ROC-AUC & - & 0.5312 & 0.5312 & 0.5000 & 0.6660 & 0.2500 & 0.7600 & **0.9160** \\  & & Overhead(s) & - & 3600 & 691.27 & \(>\)3600 & 294.32 & \(>\)3600 & 275.50 & 228.92 \\ \hline \multirow{6}{*}{**Datasets**} & \multirow{6}{*}{**Datasets**} & \multirow{6}{*}{**Datasets**} & Precision & 0.6000 & 0.3636 & 1.0000 & 0.3300 & 0.3000 & 0.3330 & 1.0000 \\  & & Recall & 0.7500 & 1.0000 & 0.1250 & 0.2500 & 0.7500 & 0.2500 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.5000 & 0.5312 & 0.6875 & 0.3750 & 0.3333 & 0.5000 & 0.6250 & **1.0000** \\  & & Overhead(s) & 424.16 & \(>\)3600 & 675.32 & \(>\)3600 & 529.11 & \(>\)3600 & 618.72 & 678.48 \\ \cline{2-11}  & & \multirow{6}{*}{**Datasets**} & \multirow{6}{*}{**Datasets**} & Precision & 0.8000 & 0.3636 & 0.3636 & 0.3330 & 0.5000 & 0.3330 & 0.3300 & 0.8000 \\  & & Recall & 1.0000 & 1.0000 & 1.0000 & 0.5000 & 0.7500 & 0.5000 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.8750 & 0.5625 & 0.5625 & 0.5000 & 0.6666 & 0.5000 & 0.5000 & **0.9160** \\  & & Overhead(s) & 1127.01 & \(>\)3600 & 2866.14 & \(>\)3600 & 1565.82 & \(>\)3600 & 1602.10 & 1425.25 \\ \hline \multirow{6}{*}{**Datasets**} & \multirow{6}{*}{**Datasets**} & \multirow{6}{*}{**Datasets**} & Precision & 0.5167 & 0.5231 & 0.8333 & 0.7500 & 1.0000 & 0.6120 & 0.7500 & 0.9696 \\  & & Recall & 0.9688 & 1000 & 0.1471 & 0.2500 & 0.2500 & 0.5620 & 0.5300 & 0.8888 \\  & & ROC-AUC & 0.5584 & 0.5404 & 0.5579 & 0.6200 & 0.7000 & 0.6419 & 0.6820 & **0.9305** \\  & & Overhead(s) & 273.23 & \(>\)3600 & 2119.34 & \(>\)3600 & 1788.50 & \(>\)3600 & 1688.19 & 1476.01 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison to trigger inversion based methods

**Baseline Methods** We compare Django against 6 baseline methods, including 4 trigger inversion based methods (i.e., NC [59], ABS [33], Pixel [55] and Tabor [18]), and 2 meta-classification based methods (i.e., MNTD [69] and MF [21]). For meta classification based methods that involve training, we have performed 5-fold cross-validation and reported the validation results exclusively. For the 4 inversion-based scanners, we adopt their objective functions as described in Eq. 2 and keep their other designs unchanged. To ensure a fair comparison, we use the same setting for common optimization hyper-parameters for all inversion based methods. We determine the optimal threshold for the size of inverted triggers as the detection rule for[59, 55, 18]. For ABS, we follow its original technique and use REASR as the decision score.

### Detection Performance

**Comparison to Trigger Inversion based Methods.** Table 1 shows the results for trigger inversion based methods, where the first two columns denote the datasets and model architectures, the third column the evaluation metrics, and the subsequent columns the baselines. Additionally, considering that NC [59], Pixel [55], and Tabor [18] need to scan all possible victim-target pairs, which can exceed the time limit, we have employed our warm-up pre-processing technique (Section 3.3). We compare the enhanced versions of these methods (NC\({}^{*}\), Pixel\({}^{*}\), Tabor\({}^{*}\)) with Django. Note that we have not evaluated ABS on DETR models, as the original version is specifically designed for CNN-based models and not transformers.

Django consistently outperforms all the baselines, with the best results are highlighted in bold. The average ROC-AUC of Django is 0.913, while the highest ROC-AUC achieved by the baselines is 0.875, which is the result obtained by ABS on DOTA and F-RCNN. The average ROC-AUC of these baselines is nearly 0.592. Furthermore, our warm-up pre-processing technique has proven effective in enhancing the scanning performance of the baselines, resulting in an improvement of 0.05 to 0.15 in terms of ROC-AUC and a significant reduction in overheads from over 3600 seconds to a range of 275 to 2800 seconds. Despite these advancements, the enhanced versions of the baselines are still unable to surpass the performance of Django, with ROC-AUC gaps ranging from 0.15 to 0.50.

Additionally, we have observed that the overheads incurred by Django are consistently lower than those of NC, Pixel, and Tabor, even when these methods are equipped with our warm-up pre-processing technique. However, ABS generally achieves lower overheads than Django, primarily because ABS only performs trigger inversion for the top-10 target classes according to the original configuration. Nonetheless, it is important to note that Django significantly outperforms ABS in terms of ROC-AUC, which is the more critical metric for our evaluation. We further illustrate the inverted trigger by Django in Figure 6. It is evident that the inverted trigger produced by Django closely resembles the ground-truth injected trigger in terms of both shape and color. The overall success of Django can be attributed to our well-designed warm-up pre-processing and the novel _Gaussian Focus Loss_ 4. These techniques play a crucial role in achieving the superior performance and efficiency demonstrated by Django compared to the baselines.

**Comparison to Meta Classifiers.** Table 2 provides a comparison between meta classifiers and Django. Since meta classifiers require only a few seconds to scan a model, we have omitted the overheads from the table. Observe that baselines achieve decent results in the COCO dataset with over 0.81 ROC-AUC, while only make nearly 0.6 ROC-AUC on other datasets. This discrepancy can be attributed to the fact that there are over 60 models available in the COCO dataset, whereas there are only 30 models in the other datasets. Consequently, the performance of the meta classifiers heavily relies on the availability of a large number of training models. Unfortunate

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline D & M & Metric & MNTD & MF & Django \\ \hline \multirow{4}{*}{**Category**} & \multirow{3}{*}{SSD} & Precision & 0.4545 & 0.4000 & 0.8000 \\  & & Recall & 0.3125 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.3750 & 0.6563 & **0.9160** \\ \cline{2-6}  & \multirow{3}{*}{F-RCNN} & Precision & 0.5652 & 0.5000 & 0.8571 \\  & & Recall & 0.8125 & 0.1250 & 0.7500 \\  & & ROC-AUC & 0.7695 & 0.6797 & **0.8750** \\ \cline{2-6}  & \multirow{3}{*}{DETR} & Precision & 0.1250 & 0.1250 & 0.8750 \\  & & Recall & 0.1250 & 0.1250 & 0.8750 \\  & & ROC-AUC & 0.1719 & 0.2890 & **0.9160** \\ \hline \multirow{4}{*}{**Category**} & \multirow{3}{*}{SSD} & Precision & 0.3333 & 0.3333 & 1.0000 \\  & & Recall & 0.1250 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.3906 & 0.4375 & **1.0000** \\ \cline{2-6}  & \multirow{3}{*}{F-RCNN} & Precision & 1.0000 & 0.3333 & 0.8000 \\  & & Recall & 0.5000 & 0.5000 & 1.0000 \\  & & ROC-AUC & 0.6562 & 0.6875 & **0.9160** \\ \hline \multirow{4}{*}{**Category**} & \multirow{3}{*}{SSD} & Precision & 0.8182 & 0.7000 & 0.9969 \\  & & Recall & 0.5625 & 1.0000 & 0.8888 \\ \cline{1-1}  & & ROC-AUC & 0.8144 & 0.8163 & **0.9305** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison to meta classifierssignificant number of models in real-world scenarios is a challenging task. In contrast, Django does not require access to training models. Moreover, it achieves a ROC-AUC of over 0.91, outperforming the meta classifiers even when they are trained on a sufficient number of models in the COCO dataset. This highlights the effectiveness and robustness of Django in detecting backdoor attacks, surpassing the performance of meta classifiers.

**Evaluation on Advanced Object Detection Backdoor Attacks.** We also evaluate Django on two advanced object detection backdoor attacks [3; 5; 28]. Django achieves 0.8 and 0.9 ROC-AUC on detecting these attacks. Appendix E presents more discussion.

### Evaluation of Label Proposal Pre-processing

In this section, we evaluate the effectiveness of our label proposal pre-processing introduced in Section 3.3. Results are presented in Figure 7. As shown in Figure 7(a), where the x-axis denotes the dataset, the left y-axis denotes the average number of pairs and the right one the TPR (True Positive Rate) of the selected pairs. In the case of an attacked model, if the selected pairs after pre-processing contain the ground-truth victim-target pair, it is considered a true positive. TPR is calculated as the ratio of true positives to all positives. The green bars represent the average number of pairs without pre-processing, while the red bars represent the number of pairs after pre-processing. We observe that across all datasets, our pre-processing technique significantly reduces the number of scanning pairs by 83% to 98%. The curves represent the TPR, showing that our pre-processing results in TPRs of over 88% across different datasets. These results indicate that our pre-processing technique effectively reduces the number of pairs to a remarkably low level while maintaining a high detection rate. The experiment is conducted by selecting various values of \(h\) ranging from 1 to 10 (representing the top \(h\) class labels) and \(\omega\) ranging from 0.1 to 0.8 (representing the frequency threshold). We record the average number of pairs and true positive rate (TPR) under this configuration. The results are depicted in Figure 7(b). we observe that SSD and DETR models require fewer than 100 pairs to reach high TPR, while Faster-RCNN models require around 200 pairs. One potential reason for this difference is that Faster-RCNN requires a two-stage training process where the backdoor signal is less prominent compared to single-stage training methods in SSD and DETR.

Figure 6: Visual similarity between GT and Django inverted triggers.

Figure 7: Evaluation of label proposal pre-processing

### Ablation Study

Our ablation experiments are conducted on the synthesized traffic sign dataset with 3 different model architectures. We include 8 models for each architecture, with half clean and half attacked.

There are two key designs of Django to ensure the detection effectiveness, i.e., GF Loss and Pre-Processing. We conduct ablation study to assess the contribution of these components respectively. Results are shown in Table 3, where each row corresponds to a specific ablation method. The results clearly demonstrate the critical importance of both GF Loss and Pre-Processing in Django. Without these components, the detection performance of Django significantly degrades by more than 0.13 ROC-AUC. Pre-Processing also plays a crucial role in reducing the detection overhead. Please refer to sec D for hyper-parameter sensitivity analysis.

### Adaptive Attack

In this section, we evaluate the performance of Django in the context of adaptive attacks. The effectiveness of Django relies on the confidence levels assigned to the potential boxes, as these determine the coefficients used for trigger inversion. Consequently, an adaptive attack strategy could involve reducing the confidence of objects containing the trigger, thereby potentially causing Django to assign lower coefficients to these poisoned boxes. To evaluate the adaptive attack scenario, we conduct experiments using a 5-class synthesized traffic sign dataset with the SSD model. We train several models with misclassification attacks, where the victim class is to 0 and the target class 4. In order to simulate low confidence attacks, we reduced the ground-truth confidence of the target class to values of 0.8, 0.6, and 0.51, while setting the confidence of the victim class to 0.2, 0.4, and 0.49, respectively. After sufficient training, we applied Django to these models. The results are presented in Table 4, where we recorded the clean mAP and the ASRs at different score thresholds. For instance, ASR_0.1 indicates the ASR when the prediction confidence is higher than 0.1. From the results, we can see that low confidence attacks lead to lower ASRs, especially when the score threshold for measuring the ASR is set high. Furthermore, Django is successful in detecting attacked models with target confidences of 0.8 and 0.6, but fails when the target confidence is 0.51 (threshold set at 0.8). These results suggest that Django is effective against most low-confidence attacks, but its performance may degrade to some extent when the target confidence is very low.

## 5 Limitation

Our primary focus in this paper is on attacks that use static polygon triggers, which are more feasible in real-world scenarios. How to effectively inject more complex triggers [42; 9; 48] in object detection models is still an open question. We leave it to future work.

## 6 Conclusion

In this paper, we present Django, the first trigger inversion framework on detecting backdoors in object detection models. It is based on a novel _Gaussian Focus Loss_ to tackle the loss misalignment issue in object detection trigger inversion. Extensive experiments demonstrate the effectiveness and efficiency of Django compared to existing baselines.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Attack Conf.** & **Clean mAP** & **ASR\_0.1** & **ASR\_0.6** & **ASR\_0.8** & **Django** \\ \hline No Attack & 0.8766 & 0.0047 & 0.0023 & 0.0000 & 0.0 \\ \hline
0.80 & 0.8700 & 1.0000 & 1.0000 & 0.9812 & 1.0 \\
0.60 & 0.8708 & 1.0000 & 1.0000 & 0.4014 & 1.0 \\
0.51 & 0.8685 & 1.0000 & 0.5728 & 0.0446 & 0.6 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Adaptive attack

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{SSD} & \multicolumn{2}{c}{Faster-RCNN} & \multicolumn{2}{c}{DETR} \\ \cline{2-7}  & ROC-AUC & Overhead(s) & ROC-AUC & Overhead(s) & ROC-AUC & Overhead(s) \\ \hline
**Django** & **1.0000** & **859.59** & **0.8750** & **2120.10** & **0.8750** & 227.55 \\
**Django** - GF Loss & 0.7500 & 860.24 & 0.7500 & 2135.59 & 0.7500 & **226.88** \\
**Django** - Pre-Processing & 0.6250 & \textgreater{}3600 & 0.5000 & \textgreater{}3600 & 0.6250 & \textgreater{}3600 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation studyAcknowledgment

We thank the anonymous reviewers for their constructive comments. We are grateful to the Center for AI Safety for providing computational resources. This research was supported, in part by IARPA TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300, ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.

## References

* [1]T. Leaderboard (2020) https://pages.nist.gov/trojai/. Cited by: SS1.
* [2]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko (2020) End-to-end object detection with transformers. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part 1 16, pp. 213-229. Cited by: SS1.
* [3]S. Chan, Y. Dong, J. Zhu, X. Zhang, and J. Zhou (2023) Baddet: backdoor attacks on object detection. In Computer Vision-ECCV 2022 Workshops: Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part I, pp. 396-412. Cited by: SS1.
* [4]B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava (2018) Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728. Cited by: SS1.
* [5]K. Chen, X. Lou, G. Xu, J. Li, and T. Zhang (2022) Clean-image backdoor: attacking multi-label models with poisoned labels only. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* [6]S. Chen, C. Cornelius, J. Martin, and D. H. Chau (2018) Shapeshifter: robust physical adversarial attack on faster r-cnn object detector. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 10-14, 2018, Proceedings, Part I 18, pp. 52-68. Cited by: SS1.
* [7]X. Chen, C. Liu, B. Li, K. Lu, and D. Song (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526. Cited by: SS1.
* [8]X. Chen, C. Liu, B. Li, K. Lu, and D. Song (2017) Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526. Cited by: SS1.
* [9]S. Cheng, Y. Liu, S. Ma, and X. Zhang (2021) Deep feature space trojan attack of neural networks by controlled detoxification. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35, pp. 1148-1156. Cited by: SS1.
* [10]S. Cheng, G. Tao, Y. Liu, S. An, X. Xu, S. Feng, G. Shen, K. Zhang, Q. Xu, S. Ma, et al. (2023) Beagle: forensics of deep learning backdoor attack for better defense. arXiv preprint arXiv:2301.06241. Cited by: SS1.
* [11]J. Ding, N. Xue, G. Xia, X. Bai, W. Yang, M. Yang, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and L. Zhang (2021) Object detection in aerial images: a large-scale benchmark and challenges. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1. Cited by: SS1.
* [12]B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe (2020) Februus: input purification defense against trojan attacks on deep neural network systems. In Annual Computer Security Applications Conference, pp. 897-912. Cited by: SS1.
* [13]D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov (2014) Scalable object detection using deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2147-2154. Cited by: SS1.
* [14]D. Feng, C. Haase-Schutz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm, W. Wiesbeck, and K. Dietmayer (2020) Deep multi-modal object detection and semantic segmentation for autonomous driving: datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems22 (3), pp. 1341-1360. Cited by: SS1.
** [15] Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, and Xiangyu Zhang. Detecting backdoors in pre-trained encoders. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16352-16362, 2023.
* [16] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In _Proceedings of the 35th Annual Computer Security Applications Conference_, pages 113-125, 2019.
* [17] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. _IEEE Access_, 2019.
* [18] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems. _arXiv preprint arXiv:1908.01763_, 2019.
* [19] Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending against backdoor attacks using robust statistics. In _International Conference on Machine Learning_, pages 4129-4139. PMLR, 2021.
* [20] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learning non-maximum suppression. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4507-4515, 2017.
* [21] Khandoker Murad Hossain and Tim Oates. Backdoor attack detection in computer vision by applying matrix factorization on the weights of deep networks. _arXiv preprint arXiv:2212.08121_, 2022.
* [22] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. _arXiv preprint arXiv:2202.03423_, 2022.
* [23] Jinyuan Jia, Yupei Liu, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of nearest neighbors against data poisoning and backdoor attacks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9575-9583, 2022.
* [24] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 2043-2059. IEEE, 2022.
* [25] Bijun Lee, Yang Wei, and I Yuan Guo. Automatic parking of self-driving car based on lidar. _Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci_, 42:241-246, 2017.
* [26] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. In _NeurIPS_, 2021.
* [27] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. _arXiv preprint arXiv:2101.05930_, 2021.
* [28] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Composite backdoor attack for deep neural network by mixing existing benign features. In _Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security_, pages 113-131, 2020.
* [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _Proceedings of the IEEE international conference on computer vision_, pages 2980-2988, 2017.
* [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. pages 740-755, 2014.
* [31] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In _International Symposium on Research in Attacks, Intrusions, and Defenses_, pages 273-294. Springer, 2018.
* [32] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14_, pages 21-37. Springer, 2016.
* [33] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs: Scanning neural networks for back-doors by artificial brain stimulation. In _Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security_, pages 1265-1282, 2019.

* [34] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.
* [35] Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu Zhang. Piccolo: Exposing complex backdoors in nlp transformer models. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 2025-2042. IEEE, 2022.
* [36] Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, and Xiangyu Zhang. Complex backdoor detection by symmetric feature differencing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15003-15013, 2022.
* [37] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In _European Conference on Computer Vision_, pages 182-199. Springer, 2020.
* [38] Chengxiao Luo, Yiming Li, Yong Jiang, and Shu-Tao Xia. Untargeted backdoor attack against object detection. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.
* [39] Hua Ma, Yinshan Li, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Anmin Fu, Hyoungshick Kim, Said F Al-Sarawi, Nepal Surya, and Derek Abbott. Dangerous cloaking: Natural trigger based backdoor attacks on object detectors in the physical world. _arXiv preprint arXiv:2201.08619_, 2022.
* [40] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang. Nic: Detecting adversarial samples with neural network invariant checking. In _Proceedings of the 26th Network and Distributed System Security Symposium (NDSS 2019)_, 2019.
* [41] Shunji Mori, Hirobumi Nishida, and Hiromitsu Yamada. _Optical character recognition_. John Wiley & Sons, Inc., 1999.
* [42] Anh Nguyen and Anh Tran. Wanet-imperceptible warping-based backdoor attack. _arXiv preprint arXiv:2102.10369_, 2021.
* [43] Xiangyu Qi, Tinghao Xie, Jiachen T Wang, Tong Wu, Saeed Mahloujifar, and Prateek Mittal. Towards a proactive {ML} approach for detecting backdoor poison samples. In _32nd USENIX Security Symposium (USENIX Security 23)_, pages 1685-1702, 2023.
* [44] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. _arXiv preprint arXiv:1804.02767_, 2018.
* [45] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. volume 28, 2015.
* [46] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 658-666, 2019.
* [47] Ahmed Salem, Michael Backes, and Yang Zhang. Get a model! model hijacking attack against machine learning models. _arXiv preprint arXiv:2111.04394_, 2021.
* [48] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic backdoor attacks against machine learning models. _arXiv preprint arXiv:2003.03675_, 2020.
* [49] Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang. Backdoor scanning for deep neural networks through k-arm optimization. In _International Conference on Machine Learning_, 2021.
* [50] Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An, Shiqing Ma, and Xiangyu Zhang. Constrained optimization with dynamic bound-scaling for effective nlp backdoor defense. In _International Conference on Machine Learning_, pages 19879-19892. PMLR, 2022.
* [51] G Sreenu and Saleem Durai. Intelligent video surveillance: a review through deep learning techniques for crowd analysis. _Journal of Big Data_, 6(1):1-27, 2019.
* [52] Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, and Bin Luo. Backdooring neural code search. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_, 2023.
* [53] Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. Demon in the variant: Statistical analysis of dnns for robust backdoor contamination detection. In _30th USENIX Security Symposium (USENIX Security 21)_, 2021.

* [54] Guanhong Tao, Yingqi Liu, Guangyu Shen, Qiuling Xu, Shengwei An, Zhuo Zhang, and Xiangyu Zhang. Model orthogonalization: Class distance hardening in neural networks for better security. In _2022 IEEE Symposium on Security and Privacy (SP). IEEE_, volume 3, 2022.
* [55] Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma, Pan Li, and Xiangyu Zhang. Better trigger inversion optimization in backdoor scanning. In _2022 Conference on Computer Vision and Pattern Recognition (CVPR 2022)_, 2022.
* [56] Guanhong Tao, Zhenting Wang, Shiwei Feng, Guangyu Shen, Shiqing Ma, and Xiangyu Zhang. Distribution preserving backdoor attack in self-supervised learning. In _2024 IEEE Symposium on Security and Privacy (SP)._ IEEE Computer Society, 2024.
* [57] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. _Advances in neural information processing systems_, 31, 2018.
* [58] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.
* [59] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In _2019 IEEE Symposium on Security and Privacy (SP)_, pages 707-723. IEEE, 2019.
* [60] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Backdoor attack against competitive reinforcement learning. _arXiv preprint arXiv:2105.00579_, 2021.
* [61] Tong Wang, Yousong Zhu, Chaoyang Zhao, Wei Zeng, Yaowei Wang, Jinqiao Wang, and Ming Tang. Large batch optimization for object detection: Training coco in 12 minutes. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXI 16_, pages 481-496. Springer, 2020.
* [62] Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, and Shiqing Ma. Rethinking the reverse-engineering of trojan triggers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [63] Zhenting Wang, Kai Mei, Juan Zhai, and Shiqing Ma. Unicorn: A unified backdoor trigger inversion framework. _arXiv preprint arXiv:2304.02786_, 2023.
* [64] Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15074-15084, June 2022.
* [65] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. _Advances in Neural Information Processing Systems_, 34:16913-16925, 2021.
* [66] Zuxuan Wu, Ser-Nam Lim, Larry S Davis, and Tom Goldstein. Making an invisibility cloak: Real world adversarial attacks on object detectors. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV 16_, pages 1-17. Springer, 2020.
* [67] Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. Patchguard: A provably robust defense against adversarial patches via small receptive fields and masking. In _USENIX Security Symposium_, pages 2237-2254, 2021.
* [68] Chong Xiang, Saeed Mahloujifar, and Prateek Mittal. {PatchCleaner}: Certifiably robust defense against adversarial patches for any image classifier. In _31st USENIX Security Symposium (USENIX Security 22)_, pages 2065-2082, 2022.
* [69] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans using meta neural analysis. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 103-120. IEEE, 2021.
* [70] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. Rethinking stealthiness of backdoor attack against nlp models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 5543-5557, 2021.
* [71] Mingjun Yin, Shasha Li, Chengyu Song, M Salman Asif, Amit K Roy-Chowdhury, and Srikanth V Krishnamurthy. Adc: Adversarial attacks against object detection that evade context consistency checks. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3278-3287, 2022.

* [72] Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, and Xiangyu Zhang. FLIP: A provable defense framework for backdoor mitigation in federated learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [73] Zhuo Zhang, Guanhong Tao, Guangyu Shen, Shengwei An, Qiuling Xu, Yingqi Liu, Yapeng Ye, Yaoxuan Wu, and Xiangyu Zhang. Pelican: Exploiting backdoors of naturally trained deep learning models in binary code analysis. 2023.
* [74] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review. _IEEE transactions on neural networks and learning systems_, 30(11):3212-3232, 2019.

## Appendix A Details of datasets and architectures

### Object Detection Image Dataset

**COCO (Common Objects in Context) [30]** dataset is widely used for object detection tasks. It contains 80 object categories, including people, animals, vehicles and more. Each image can contain multiple instances of objects, providing ample opportunities for training and evaluating models capable of detecting and segmenting objects in complex scenes.

**Synthesized Traffic Sign** dataset is designed by TrojAI [1] which focuses on traffic sign detection, featuring various types of traffic signs commonly encountered in real-world scenarios. There are in total over 4000 different traffic signs. Each model is trained on a randomly sampled subset of classes. The number of classes within these subsets exhibits variability, ranging from as few as 2 to a maximum of 128.

**DOTA (Detection in Aerial Images)** dataset is designed for object detection in aerial images which consists of high-resolution images captured by aerial platforms. It contains 18 categories, including plane, ship, storage tank, baseball diamond and more. Its large-scale, fine-grained annotations, and challenging scenarios make it an ideal benchmark for evaluating and developing algorithms capable of detecting objects in aerial images accurately.

### Architecture

We evaluate our method on three well-known model architectures:, i.e., SSD [32], Faster-RCNN [45], and DETR [2]. SSD (Single Shot MultiBox Detector) [32] is a popular object detection model which utilizes a series of convolutional layers to detect objects at multiple scales and aspect ratios. Faster-RCNN [45] is another widely adopted object detection model that combines region proposal generation with a region-based CNN for object detection. DETR (DEtection TRansformer) [2] is a state-of-the-art object detection model that utilizes a transformer-based architecture. It replaces the conventional two-stage approach with a single-stage end-to-end detection framework.

### Model Dataset

**TrojAI**[1] initiative, spearheaded by IARPA, encompasses a multi-year, multi-round program. Its overarching objective revolves around the development of scalable and dependable automatic backdoor detection tools, specifically targeting the identification of backdoor trojans within Deep Learning models across diverse modalities. Presently, the program consists of a total of 13 rounds, each with distinct focuses and tasks. The initial four rounds and the eleventh round center their efforts on detecting trojans present in image classification models. In contrast, rounds five through nine concentrate on transformer models employed in various NLP tasks, including Sentiment Analysis, Named Entity Recognition, and Question Answering. Round twelve dedicates itself to the detection of backdoors in neural network-based PDF malware detection. Finally, rounds ten and thirteen direct their attention towards object detection models. For the evaluation of models, we exclusively utilize the training sets from rounds 10 and 13. As shown in Table 5, our evaluation entails 72 models trained on the Synthesis Traffic Sign dataset, encompassing all three model architectures. Among these models, 48 are benign, while 24 are deliberately poisoned, with an equal distribution of triggers for misclassification and evasion. Concerning the DOTA models, there exist two architectures, namely SSD and Faster-RCNN, resulting in a total of 24 models, including 16 benign models and 4 each poisoned with misclassification and evasion triggers. All COCO models adopt the SSD architecture, with a distribution of 36 clean models and 18 models poisoned by both misclassification and evasion triggers. To provide further elucidation,it is important to note that the poison rate varies within the range of 0.1% to 8% across diverse models sourced from TrojAI r10 and r13. Similarly, the trigger size exhibits a range of 1x1 to 22x22, representing a scale of 0.001% to 0.7% relative to the input dimensions. Pertaining to the hyper-parameters utilized in model training, the learning rate is stochastically assigned, spanning from \(1.56\mathrm{e}-8\) to \(1\mathrm{e}-4\) across different models. The number of epochs for training spans from 6 to 100, while the batch size ranges from 4 to 32. As for the model performance metrics, the average clean mAP across the models attains a value of 0.7979, while the average poison mAP stands at 0.7680. The trigger's polygonal structure is characterized by varyingedge counts, ranging from 3 to 8. Furthermore, each individual trigger is endowed with randomly generated color and texture attributes. Concrete settings can be found Round13 and Round101.

Footnote 1: https://pages.nist.gov/trojai/docs/object-detection-feb2023.html#object-detection-feb2023

## Appendix B Details of evaluation metrics

In our evaluation of backdoor detection methods, we employ four well-established metrics: Precision, Recall, ROC-AUC, and Average Scanning Overheads for each model. Precision quantifies the accuracy of a detection method by measuring the proportion of correctly identified positive instances among all predicted positives. In our case, we consider attacked models as positive instances and benign models as negatives. A higher precision indicates a lower rate of falsely identifying benign models as attacked. Recall, on the other hand, assesses the effectiveness of the detection method in correctly identifying positive instances. It measures the proportion of true positives among all actual positives. A higher recall suggests that the detection method is capable of identifying a significant portion of attacked models. ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) plots the true positive rate against the false positive rate at various threshold values and calculates the area under the curve. A value of 1 indicates perfect classification, while a value of 0.5 indicates that the method is no better than random guessing. We also consider the overhead of the detection method, which quantifies the average time required to scan a single model. We use seconds (s) as the unit of measurement and set a maximum threshold of 1 hour (3600 s). If the scanning process exceeds 3600 seconds, it is terminated, and we rely on the existing results for making predictions. Low overhead signifies high efficiency of the method. By employing these four metrics, we aim to comprehensively evaluate the performance and efficiency of the backdoor detection methods. It is worth noting that the time limit we have set for scanning models is deliberately conservative when compared to the thresholds established in different rounds of the TrojAI competition. For example, in round 13, participants are granted a generous 30-minute duration for scanning a single model. To surpass the official benchmarks set in each round, a more aggressive and precise pre-processing approach may be necessary.

## Appendix C Details of Baseline Methods

In this section, we introduce more details of baseline methods, including NC [59], Tabor [18], ABS [33], Pixel [55], Matrix Factorization(MF) [21] and MNTD [69].

**NC**[59] adopts a specific trigger inversion approach for each class and considers a model to be attacked if it is able to generate an effective yet extremely small trigger for a target class. **Tabor**[18] enhances NC by incorporating additional well-designed regularization terms, such as penalties for scattered triggers, overlaying triggers, and blocking triggers. These additions aim to improve the reconstruction of injected triggers. **Pixel**[55] introduces a novel inversion function that generates a pair of positive and negative trigger patterns. This approach achieves better detection performance compared to NC. **ABS**[33] employs a stimulation analysis to identify compromised neurons, which serves as guidance for trigger inversion. ABS considers a model to be attacked if it can invert a trigger that achieves a high reconstructed ASR (REASR).

To the best of our knowledge, there is no existing detection methods for object detection models. Therefore, we perform straight-forward but reasonable adaption to these existing methods designed on image classification tasks, such that they are able to work against backdoor attacks on object detection models. Specifically, the original objective of NC, Tabor, and Pixel is to invert small triggers while maintaining their effectiveness (high ASR). In our adaptation, we retain their design principles but re-define the ASR to align with object detection models, as explained in Section 3.1. Additionally, we introduce a threshold for the size of inverted triggers, enabling the differentiation between benign and attacked models. For ABS, we adhere to its original technique but employ the re-defined ASR as the optimization goal, and use REASR as the decision score. By employing these adaptations, we aim to enhance the detection capabilities of these existing methods specifically for backdoor attacks on object detection models.

No modifications or adaptations are needed for meta classification-based methods when applied to object detection models. MNTD [69] trains a set of queries and a classifier to discern the feature-spacedistinctions between clean and attacked models. MF [21] directly trains a classifier on model weight features using specialized feature extraction techniques, i.e., matrix factorization. These methods solely rely on the feature extraction networks commonly utilized in both image classification and object detection models. As a result, MNTD and MF can be directly employed to detect backdoor attacks in object detection models without the need for additional adjustments or modifications.

We collect the Precision, Recall, ROC-AUC and Overheads for each method across various datasets and model architectures. To ensure a fair comparison, we have conducted a search to determine the optimal thresholds for different decision scores associated with each method (trigger size for NC, Tabor, Pixel, REASR for ABS and output confidence for meta-classifiers). These thresholds are chosen to maximize accuracy. Besides, we set a fixed number of optimization steps for scanning a pair of victim-target label (100) for all inversion based baselines. For meta classification based methods that involve training, we have performed 5-fold cross-validation and reported the validation results exclusively.

## Appendix D Hyper-parameter Sensitivity Analysis

To assess the sensitivities of the hyper-parameters used in Django, we conduct experiments as described in Section 4.3.

**IoU Thresholds.** We evaluate the IoU threshold used to calculate the ASR of inverted triggers. The results are summarized in Figure 8(a), where each row corresponds to a different model architecture, and each column represents a different choice of IoU threshold. It can be observed that IoU thresholds of 0.3 and 0.5 generally yield good performance. However, a threshold of 0.7 tends to degrade the performance, possibly due to the inverted triggers interfering with the bounding box predictions.

**Region Size.** The impact of different regional initialization sizes is evaluated and the results are presented in Figure 8(b). Among the various choices, a region size of 30\(\times\)30 consistently achieved the best performance. This is because larger initialization sizes tend to result in more false positive cases.

**Score Threshold.** Different score thresholds are tested when computing the ASR of inverted triggers. The results, shown in Figure 8(c), indicate that a score threshold of 0.5 generally leads to the best performance across all model architectures. This choice represents a trade-off between false positives and false negatives. Higher score thresholds may introduce more false negatives, as the inverted trigger may not have high confidence similar to the injected one. On the other hand, lower score thresholds may result in more false positives. Thus, a moderate value of 0.5 provides the optimal balance.

Figure 8: Hyper-parameter sensitivity.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Model Source} & \multicolumn{2}{c}{Architecture} & \multicolumn{3}{c}{Number of Models} \\ \cline{2-7} Image Dataset & Round10 & Round13 & SSD & Faster-RCNN & DETR & Benign & Miscs Attack & Evasion Attack \\ \hline Synthesis Traffic Sign & ✗ & ✓ & ✓ & ✓ & ✓ & 48 & 12 & 12 \\ DOTA & ✗ & ✓ & ✓ & ✓ & ✗ & 16 & 4 & 4 \\ COCO & ✓ & ✗ & ✓ & ✗ & ✗ & 36 & 18 & 18 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Dataset details

[MISSING_PAGE_FAIL:19]

Instead, it leverages a clean object A to serve as the trigger for attacking another object B. Interestingly, Django is capable of effectively reversing this process, essentially identifying a trigger that closely mimics the pattern of object A. This ability enables Django to detect composite backdoors with a high level of accuracy.