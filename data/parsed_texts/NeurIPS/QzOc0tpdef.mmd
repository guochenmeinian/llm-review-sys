# NLIR: Natural Language Intermediate Representation

for Mechanized Theorem Proving

Laetitia Teodorescu

Adaptive ML

Guillaume Baudart

Universite Paris Cite

CNRS, Inria, IRIF

Emilio Jesus Gallego Arias

Universite Paris Cite

CNRS, Inria, IRIF

&Marc Lelarge

DI ENS, PSL University

Inria

###### Abstract

Formal theorem proving is challenging for humans as well as for machines. Thanks to recent advances in LLM capabilities, we believe natural language can serve as a universal interface for reasoning about formal proofs. In this paper, 1) we introduce _Petanque_, a new lightweight environment to interact with the Coq theorem prover; 2) we present two interactive proof protocols leveraging natural language as an intermediate representation for designing proof steps; 3) we implement beam search over these interaction protocols, using natural language to rerank proof candidates; and 4) we use Petanque to benchmark our search algorithms. Using our method with GPT-4o we can successfully synthesize proofs for 58% of the first 100/260 lemmas from the newly published Busy Beaver proofs.

## 1 Introduction

The general knowledge and reasoning abilities of frontier large language models (LLMs) makes them practical as a backbone for building agents able to interact with interactive theorem provers (ITP). These agents should iteratively build proofs with help from proof engine feedback. While previous work (e.g. Yang et al. [2023]) used a costly data collection procedure to finetune modestly sized language models, we believe that reasoning in natural language before outputting tactics will lead to better and more interpretable results. Recently, Thakur et al. [2024] showed promising preliminary results by using GPT-4 as an agent proposing tactics inside a backtracking search and using rich feedback from the proof environment.

In this work, we develop infrastructure to allow communication between a GPT-4o-based agent and the Coq proof environment [The Coq Development Team, 2024]. Our key idea is to rely on natural language as much as possible when generating proofs. Using natural language leverages the strength of LLMs, and allows us to use chain-of-thought [Wei et al., 2022] by asking for an informal mathematical proof before generating the formal proof, making it more intuitive and comprehensible compared to purely automatic formal techniques. Additionally, partial proofs expressed in natural language are easier for humans to understand, adapt, or reuse, allowing for greater flexibility and collaboration between machine-generated suggestions and human mathematicians.

We present the following contributions: 1) _Petanque_: A new fast and lightweight environment to interact with the Coq theorem prover. 2) Two interactive proof protocols both leveraging natural language reasoning: tactic-by-tactic proof construction, and hierarchical proof templating. 3) We couple both protocols with standard search algorithms leveraging feedback from the ITP and using natural language to rerank proof candidates. 4) We evaluate this agent on a new dataset of textbookexercises and intermediate theorems from the recent Busy Beaver proof formalized in Coq of BB\((4)=107\), [30, 31]. NLIR is open source (https://github.com/llm4coq/nlir).

## 2 Petanque: a lightweight interactive environment for Coq

A common difficulty when interacting with interactive proof assistants in the context of machine learning is inadequate tooling (see for example [18]). Following existing work [1, 19, 20], we have built a new environment for machine to machine interaction for the Coq proof assistant, particularly tailored for interactive, high-throughput, low-latency learning applications. Petanque is based on Fleche [19], a new document manager for Coq. We extend Fleche by enabling Petanque to access the Coq proof engine directly without requiring edits in the associated document. This makes our environment fast and lightweight. A Python interface provides easy access to the API. See Appendix B for more information on Fleche and Petanque.

## 3 Proof interaction protocols

In this section, we present two approaches leveraging LLMs' ability to reason in natural language in order to find a formal proof with the help of a proof assistant. _Tactic-by-tactic proof construction_ mimics the typical behavior of a standard Coq user: given the current goals, the agent generates one or several tactics that updates the goals and repeats this process until the proof is complete. By contrast, _hierarchical proof templating_ tries to generate full proofs directly. Failed tactics are then replaced with _holes_ to obtain a proof _template_. The agent repeats the process of filling each hole until the proof is complete. Our approach's originality is that although both protocols' inputs (goals) and outputs (tactics) are Coq code, the agent internally uses natural language as an intermediate representation to analyze the input and guide the code generation.

### Tactic-by-tactic proof construction

An overview of the tactic-by-tactic proof construction agent is presented in Figure 1. Given a Coq theorem, the agent first uses natural language to describe the goal and explain how to continue the proof (chain-of-thought). The last step synthesizes the corresponding Coq tactics. For instance, in Figure 1, the goal is to prove that addition over natural numbers is commutative. The agent decides to try a proof by induction and correctly synthesizes a sequence of two tactics: intros n m. introduces two variables n and m of type nat (natural numbers), and induction n. starts an induction over n.

The tactics are sent to the Petanque environment, which parses and executes each tactic to update the current goal. A textual representation of the new goal is then fed back to the agent to make further progress in the proof. If the execution returns an error, the current goal does not change, but we augment the prompt with the failed tactics and ask the LLM to try something else for the next attempt. For instance, in Figure 1, both tactics succeed and generate two new subgoals: the base case (for n=0, prove m + 0 = 0 + m) and the induction case (given

Figure 1: Tactic-by-tactic proof construction.

Ith: n + m = m + n, prove (n + 1) + m = m + (n + 1) ). The textual representation of a goal uses the the symbol \(\vdash\) to separate hypotheses from the conclusion, and S \(n\) denotes n + 1.

**Model Interface.** In early experiments, we observed that conversation-style reasoning often diverges: after a few rounds, the output makes very little sense, and the agent never recovers. Following (Yang et al., 2024) - and similarly to (Thakur et al., 2024) - we use a synthetic interface to summarize at each goal the global objective (initial theorem), the current goal (in the middle of a proof), and failed attempts to solve the same goal.

### Hierarchical proof templating

An example execution of the hierarchical proof templating agent is presented in Figure 2. The agent pipeline is similar to the tactic-by-tactic method, but instead of focusing only on the next step, the agent generates a complete proof in natural language, before translating the proof in Coq syntax. For instance, in Figure 2, the agent uses the inversion tactics on the hypothesis H which generate two subgoals with a simpler hypothesis H0, and then tries to solve each subgoals using this H0 hypothesis.

The Petanque environment then repairs the proof, replacing failed tactics by _holes_ which admit and close the current subgoal, removing subsequent tactics until the focus moves to the next subgoal. Petanque then checks that the resulting _template_ is correct, i.e., assuming a valid proof for each holes, the proof is complete. A textual representation of each holes is then fed back to the agent which repeat the process to fill the holes one by one. For instance, in Figure 2, apply H0 fails on both subgoals. The agent then repeats the process for each holes, using focused fine-grain reasoning to prove the corresponding subgoal. The proof is complete when there are no more holes.

## 4 Proof search

We combine our interactive protocol with the classic beam search algorithm. Inspired by (Yao et al., 2023), we use the LLM to rank and sort the proposals at each step of the search. A simplified version of the code is presented on the right. At each step, agent.generate generates n_actions possible steps (tactics or proofs). Each step is then validated with petanque.step and the state of all the resulting candidates is stored. Then agent.sort calls the LLM to discuss, compare and finally rank and sort the candidates for the next step.

``` defbeam_search(n_steps,n_actions,beam_size): s = petanque.start(Ith) = [s] # Initial state for step in range(n_steps): candidates = [] for s in beam: # Try multiple actions for each state for a in agent.generate(s,n_actions): sa = petanque.steps,a) if petanque.proof_finished(sa): returns ss.proof # Proof found else: candidates = candidates + [sa] # Rank and sort candidates  beam agent.sort(Candidates)[:beam_size] returnNone# No proof found

Figure 2: Hierarchical proof templating.

[MISSING_PAGE_FAIL:4]

training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities.

Reasoning in LLMsThis work is also related to recent investigations on the reasoning abilities of LLMs (Plaat et al., 2024). Chain-of-Thought (CoT) prompting (Wei et al., 2022) was shown to improve LLM's answers; subsequent work found that these reasoning abilities could be elicited zero-shot (Kojima et al., 2022). Further work interleaved CoT with decision-making (Yao et al., 2022), added search and complex control flow to reasoning (Chen et al., 2022; Yao et al., 2023; Besta et al., 2024), incorporated refinement and feedback (Madaan et al., 2024; Shinn et al., 2024), and learned to generate novel reasoning traces that proved beneficial for further training (Zelikman et al., 2022, 2024). Like our work, many of these methods - especially the ones using search and refinement - make use of LLM-based scoring or ranking functions (Zheng et al., 2023).

ConclusionIn this work, we have presented a new agent for building proofs leveraging chain of thought as an intermediate representation, and generating proofs by outputting step-by-step tactics or hierarchical proof templates. We couple this with beam search and natural language reranking and obtain good performance on a new evaluation set built with the help of our novel proof environment, _Petanque_. Future work could investigate how one could use reinforcement learning to obtain better reasoning and performance with smaller models (OpenAI, 2024).

AcknowledgementsWe thank Cyril Cohen and Pierre Boutillier for many insightful discussions. We also thank Alex Sanchez-Stern for his feedback on early versions of Petanque. This work is supported by the Inria Defi LLM4Code and the project ReaLiSe, Emergence Ville de Paris 2021-2025.

## References

* Acar et al. (2005) Umut A. Acar, Guy E. Blelloch, Matthias Blume, Robert Harper, and Kanat Tangwongsan. A library for self-adjusting computation. In Nick Benton and Xavier Leroy, editors, _ML Workshop_, 2005.
* Besta et al. (2024) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In _AAAI_, 2024.
* Accz181078 (2024)ccz181078. https://github.com/ccz181078/Coq-BB5/tree/main, 2024.
* Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _arXiv preprint arXiv:2211.12588_, 2022.
* First et al. (2023) Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models. _CoRR_, abs/2303.04910, 2023.
* Arias et al. (2016) Emilio Jesus Gallego Arias, Benoit Pin, and Pierre Jouvelot. jscoq: Towards hybrid theorem proving interfaces. In Serge Autexier and Pedro Quaresma, editors, _UITP_, 2016.
* Arias (2019) Emilio Jesus Gallego Arias. SerAPI: Machine-friendly, data-centric serialization for Coq. preprint, 01 2019. URL https://github.com/ejallego/coq-serapi/.
* Arias (2024) Emilio Jesus Gallego Arias. Fleche: Incremental validation for hybrid formal documents. under revision, 2024.
* Han et al. (2021) Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models. _arXiv preprint arXiv:2102.06203_, 2021.
* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _NeurIPS_, 2022.
* Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. _NeurIPS_, 2022.

* Lin et al. (2024) Haohan Lin, Zhiqing Sun, Yiming Yang, and Sean Welleck. Lean-star: Learning to interleave thinking and proving. _arXiv preprint arXiv:2407.10040_, 2024.
* Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _NeurIPS_, 2024.
* Newell et al. (1957) Allen Newell, John Clifford Shaw, and Herbert A Simon. Empirical explorations of the logic theory machine: a case study in heuristic. In _Western joint computer conference: Techniques for reliability_, pages 218-230, 1957.
* OpenAI (2024) OpenAI. Learning to Reason with LLMs. https://openai.com/o1/, 2024.
* Pierce et al. (2024) Benjamin C. Pierce, Arthur Azevedo de Amorim, Chris Casinghino, Marco Gaboardi, Michael Greenberg, Catalin Hricu, Vilhelm Sjoberg, and Brent Yorgey. _Logical Foundations_. Software Foundations. 2024. Version 6.7, http://softwarefoundations.cis.upenn.edu.
* Plaat et al. (2024) Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, a survey. _arXiv preprint arXiv:2407.11511_, 2024.
* Polu and Sutskever (2020) Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. _CoRR_, abs/2009.03393, 2020.
* Reichel et al. (2014) Tom Reichel, R. Wesley Henderson, Andrew Touchet, Andrew Gardner, and Talia Ringer. Proof repair infrastructure for supervised models: Building a large proof repair dataset. In Adam Naumowicz and Rene Thiemann, editors, _LIPIcs_.
* Sanchez-Stern et al. (2020) Alex Sanchez-Stern, Yousef Alhessi, Lawrence K. Saul, and Sorin Lerner. Generating correctness proofs with neural networks. In _MAPL@PLDI_, 2020.
* Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. _NeurIPS_, 2024.
* Thakur et al. (2024) Amitayush Thakur, George D. Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri. An in-context learning agent for formal theorem-proving. In _COLM_, 2024.
* release 8.19.0. https://coq.inria.fr/doc/V8.19.0/refman, 2024.
* Wang et al. (2024) Haiming Wang, Huajian Xin, Zhengying Liu, Wenda Li, Yinya Huang, Jianqiao Lu, Zhicheng Yang, Jing Tang, Jian Yin, Zhenguo Li, and Xiaodan Liang. Proving theorems recursively. _CoRR_, abs/2405.14414, 2024.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022.
* Wu et al. (2022) Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. In _NeurIPS_, 2022.
* Yang et al. (2024) John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. _CoRR_, abs/2405.15793, 2024.
* Yang and Deng (2019) Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In _ICML_, 2019.
* Yang et al. (2023) Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J. Prenger, and Animashree Anandkumar. Leandoj: Theorem proving with retrieval-augmented language models. In _NeurIPS_, 2023.
* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* Yang et al. (2024)Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In _NeurIPS_, 2023.
* Zelikman et al. [2022] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. _NeurIPS_, 2022.
* Zelikman et al. [2024] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. Quiet-star: Language models can teach themselves to think before speaking. _arXiv preprint arXiv:2403.09629_, 2024.
* Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In _NeurIPS_, 2023.

Detailed results

### Logical Foundations

For the template agent, the gray numbers indicate the proportion of proofs that are correct at the first try (no holes). We also report the average length of the generated proofs (number of tactics) and the size of the smallest and the biggest proofs. Details are presented in Table 1.

### \(\text{Bb}(4)\)

For each methods, we also report the original proof sizes (mean, min, and max) on the set of lemmas that were successfully proved. Details are presented in Table 2.

## Appendix B From Fleche to Petanque

In this section we will describe Petanque, a new environment for lightweight interaction with formal proof documents. Petanque targets machine-learning applications such as reinforcement learning and other agent-based use cases, providing _zero-overhead_, _purely functional_1 access to Coq's proof engine, along with some utilities to implement custom proof search routines.

Footnote 1: computations are treated as stateless functions, i.e., for equal inputs, we obtain equal outputs

FlechePetanque is built on top of Fleche [1], a new document manager for Coq. Fleche is both a formal document interpreter and a build system for Coq proof documents.

A schematic view of Fleche's behavior when the document is edited is presented in Figure 3. Fleche maintains an enriched representation of Coq proof documents, including the relevant Coq states associated to the interactive proofs and their dependencies. When an edit occurs, Fleche only invalidate the parts of the document that depend on that change, following standard incremental computing practices [1].

At any point, users can query Fleche for data about the document -- for example information about the current proof obligations at a given point of the document -- and Fleche will compute the requested information on-demand, as fast as possible.

coq-lspFleche's edit / query interface accommodates seamlessly the Language Server Protocol (LSP) protocol, the standard way to provide programming language support in modern Integrated Development Environments (IDEs). The LSP server coq-lsp2 built on top of Fleche thus provides continuous real-time checking for Coq documents inside popular editors such as Emacs or VSCode.

PetanqueUnfortunately the edit/query document model turns out to be too expensive for high-throughput, proof-search applications: while Fleche invalidation on edits is very efficient, the associated overhead starts to become a problem when the edit frequency is higher than a few times per second. Moreover, using IDE protocols such as LSP means that agents need to exchange message with the server multiple times per step, which again creates non-trivial overhead. To overcome the previous problems, Petanque provides one-shot direct access to Coq's proof state and tactic engine.

A simplified view of the Petanque API is presented in Figure 4. Using this API, agents can perform _speculative proof checking_ without altering the original document.

The start methods initialize a proof session where the initial Coq proof state correspond to the theorem statement thn in the document file. Then, given a state, the run_tac method executes a tactic tac and return the resulting state if successful. The goals method can be used to retrieve a human readable version of the proof goals (e.g., as in Figures 1 and 2).

Figure 4: A simplified view of the pytanque API

Figure 3: Fleche: a document manager for Coq. Fleche maintains a _decorated document_ where each atom (definitions and proof steps) are associated with the Coq state (green dots). When an edit happens in the editor, Fleche retrieves the corresponding state, execute the code with the Coq interpreter, stores the new state (blue dot) in the decorated document, and returns the next goal that can be visualized in the editor. Communication with the editor relies on the LSP protocol.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & \multicolumn{2}{c}{tactics} & \multicolumn{2}{c}{template} \\ \cline{2-5}  & \multicolumn{1}{c}{naive} & \multicolumn{1}{c}{beam} & \multicolumn{1}{c}{naive} & \multicolumn{1}{c}{beam} \\ \hline \multicolumn{5}{l}{IndProp:subsea\_trans} & 4 & 4 & x & 6 \\ \multicolumn{5}{l}{IndProp:reflect\_iff} & 11 & 12 & 20 & 18 \\ \multicolumn{5}{l}{IndProp:eeb\_practice} & x & x & x & x \\ \multicolumn{5}{l}{IndProp:merge\_filter} & 19 & 4 & 29 & 4 \\ \multicolumn{5}{l}{IndProp:pal\_app\_rew} & x & x & x \\ \multicolumn{5}{l}{IndProp:pal\_re} & 4 & 4 & 4 \\ \multicolumn{5}{l}{IndProp:pal\_informe\_converse} & x & x & x \\ \multicolumn{5}{l}{IndProp:pisal\_informe\_converse} & x & x & x \\ \multicolumn{5}{l}{IndProp:pigenoth\_principle} & x & x & x \\ \multicolumn{5}{l}{IndProp:piss\_informe\_principle} & x & x \\ \multicolumn{5}{l}{IndProp:piss\_informe\_correct} & x & x \\ \multicolumn{5}{l}{IndProp:le\_true\_trans} & 12 & 11 & 11 \\ \multicolumn{5}{l}{IndProp:piss\_informe\_for} & \multicolumn{1}{c}{x} & x \\ \hline \hline \end{tabular} 
\begin{tabular}{l r r r r} \hline \hline  & \multicolumn{2}{c}{tactics} & \multicolumn{2}{c}{template} \\ \cline{2-5}  & \multicolumn{1}{c}{naive} & \multicolumn{1}{c}{beam} & \multicolumn{1}{c}{naive} & \multicolumn{1}{c}{beam} \\ \hline \multicolumn{5}{l}{IndProp:subsea\_trans} & 4 & 4 & x & 6 \\ \multicolumn{5}{l}{IndProp:reflect\_iff} & 11 & 12 & 20 & 18 \\ \multicolumn{5}{l}{IndProp:eeb\_practice} & x & x & x \\ \multicolumn{5}{l}{IndProp:merge\_filter} & 19 & 4 & 29 & 4 \\ \multicolumn{5}{l}{IndProp:pal\_app\_rew} & x & x & x \\ \multicolumn{5}{l}{IndProp:pal\_re} & 4 & 4 & 4 \\ \multicolumn{5}{l}{IndProp:pisal\_informe\_converse} & x & x & x \\ \multicolumn{5}{l}{IndProp:pigenoth\_principle} & x & x & x \\ \multicolumn{5}{l}{IndProp:piss\_informe\_correct} & x & x \\ \multicolumn{5}{l}{AlAuto:eub\_exchange} & 14 & 9 & 12 & 12 \\ \multicolumn{5}{l}{AlAuto:eub\_true\_elim2} & 4 & 6 & 10 \\ \multicolumn{5}{l}{AlAuto:eub\_exchange} & 12 & 23 \\ \multicolumn{5}{l}{AlAuto:eub\_true\_elim2} & 4 & 6 & 10 \\ \multicolumn{5}{l}{AlAuto:eub\_exchange} & 12 & 23 \\ \multicolumn{5}{l}{AlAuto:nor\_com} & 12 & 10 & x & 10 \\ \multicolumn{5}{l}{AlAuto:eub\_not\^{}} & 11 & x & 10 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Detailed results for the Logical Foundations benchmark.

[MISSING_PAGE_FAIL:11]