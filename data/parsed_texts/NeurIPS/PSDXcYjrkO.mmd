# Towards Comprehensive Detection of Chinese

Harmful Memes

 Junyu Lu1, Bo Xu1, Xiaokun Zhang1, Hongbo Wang1,

**Haohao Zhu1, Dongyu Zhang2, Liang Yang1,3, Hongfei Lin1**

1 School of Computer Science and Technology, 2 School of Foreign Languages,

3 Key Laboratory of Social Computing and Cognitive Intelligence,

Dalian University of Technology, China

dutljy,zhuhh@mail.dlut.edu.cn, dawnknun1993,1846742523a@gmail.com

xubo,zhangdongyu,liang,hflin@dlut.edu.cn

Corresponding author

###### Abstract

Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors. To this end, we focus on the comprehensive detection of Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with fine-grained annotations for various meme types. Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), incorporating contextual information of meme content generated by the LLM to enhance the understanding of Chinese memes. During the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. The experimental results indicate that detecting Chinese harmful memes is challenging for existing models while demonstrating the effectiveness of MKE.1

Footnote 1: Resources of this paper are available at https://github.com/DUT-lujunyu/ToxiCN_MM.

_Disclaimer: The samples presented by this paper may be considered offensive._

## 1 Introduction

With the development of the Internet, harmful memes on the web have become increasingly rampant. Harmful memes are typically defined as multimodal units consisting of an image and embedded text that cause harm to an individual, an organization, a community, or a social group by specifically targeting social entities (Pramanick et al., 2021; Sharma et al., 2022). They may exacerbate social divisions, trigger discriminatory behaviors, and harm social harmony and unity (Kiela et al., 2020). Due to their negative impact on society, the widespread dissemination of harmful memes has been widely recognized as a growing concern.

In recent years, researchers have made substantial progress in detecting harmful memes. Several datasets, including HMC (Kiela et al., 2020), MMHS (Gomez et al., 2020), Harm-C, and Harm-P (Pramanick et al., 2021), have been established, and various detectors have been proposed Hee et al. (2022); Aggarwal et al. (2023); Pramanick et al. (2021); Sharma et al. (2022); Cao et al. (2022). However, most existing studies only focus on English memes. In contrast, Chinese harmful meme detection remains largely unexplored, presenting challenges in building reliable datasets and developing effective detectors.

On one hand, the types of Chinese harmful memes are diverse. In addition to those targeting specific social entities, many memes on Chinese platforms contain general offense, sexual innuendo, ordispirited culture (Liu and Xu, 2016), as shown in Figure 1. Despite lacking specific targets, they still exhibit potential toxicity, subtly propagating negative values that could lead to serious consequences like violent acts and sexual harassment (Lin and Zhang, 2019). To adapt to the online environment, it is crucial to consider this diversity when constructing the dataset.

On the other hand, understanding the semantics of Chinese harmful memes presents a significant challenge for detectors, necessitating contextual information from both textual and visual elements. For example, Exp. (a) of Figure 1 illustrates gender bias through the height difference between the man and the woman in the image. In contrast, the inline text of Exp. (b) introduces a Chinese insult, _"vegetable dog"_, to tease others, which implies incompetent people. Therefore, incorporating such information is necessary for the effective detection of Chinese harmful memes.

In this paper, we facilitate the detection of Chinese harmful memes, primarily focusing on two aspects: **dataset construction** and **detector development**. For the dataset construction, we first propose the definition of "_Chinese harmful memes_" as guidance, accurately adapting to the Chinese online environment. Based on the definition, we focus on both targeted harmful memes and those exhibiting potential toxicity without specific targets. We conduct fine-grained annotation for harmful memes collected from Chinese online platforms, analyzing their harmful types and combination features of textual and visual information. ToxiCN MM dataset is then constructed, encompassing 12,000 samples containing different harmful types. Two progressive tasks are established using the dataset: (I) Detect if a meme is harmful, and (II) If harmful, further identify its harmful type.

For the detector development, we present a Multimodal Knowledge Enhancement (MKE) detector, integrating the contextual information of meme content for effective detection. We first utilize the large language model (LLM) to capture the context of both the text and image of the meme, leveraging its extensive knowledge acquired through pre-training Zhao et al. (2023). This information is then integrated into a trainable detector as enhanced captions to improve the understanding of memes. In the experimental phase, we evaluate the detection performance of various baselines, including both traditional pre-trained language models (PLMs) and large language models (LLMs), providing a benchmark for evaluation. Experimental results show the effectiveness of our MKE. The main contributions of this paper are summarized as follows:

* We construct, to the best of our knowledge, the first Chinese harmful meme dataset ToxiCN MM, which comprises 12,000 diverse samples. We conduct a fine-grained annotation to analyze their harmful types and modality combination features.
* We present Multimodal Knowledge Enhancement (MKE) as a baseline detector, integrating contextual information of meme content generated by the LLM to improve the detector's understanding of Chinese harmful memes.
* We utilize ToxiCN MM as a benchmark to evaluate the detection performance of various baseline models. Extensive quantitative experiments and qualitative analysis illustrate the effect of MKE. We summarize the challenges of Chinese harmful memes detection.

Figure 1: Illustration of memes in Chinese. (a) is a harmless meme that humorously expresses concern about math homework. (b) is a targeted harmful meme conveying gender bias through the height differences between the man and woman. (c) contains general offense without specific targets, where “_vegetable dog_” is a Chinese insult, implying incompetent people. (d) subtly conveys sexual innuendo with an idiom ”_shy things_” (alluding to ”_sexual intercourse_”). (e) spreads dispirited culture by comparing oneself to ”_garbage_”.

## 2 Related Work

**Harmful Meme.** In recent years, researchers have noticed the importance of detecting harmful memes. Several datasets have been established Kiela et al. (2020); Gomez et al. (2020); Pramanick et al. (2021). Nevertheless, most current studies only focus on English, while research on detecting Chinese harmful memes remains unexplored. To this end, we present the first Chinese harmful meme dataset ToxiCN MM. Here we list Table 1 to compare existing datasets with ToxiCN MM.

Existing studies define "_harmful memes_" as memes that cause harm to specific social entities, based on their social attributes such as religion, race, and gender (Pramanick et al., 2021). However, this definition does not fully apply to the Chinese Internet, where harmful memes often exhibit potential toxicity without specific targets but still perpetuate negative cultural values (Liu and Xu, 2016; Zhang and Zhao, 2021). In this paper, we consider different harmful types for comprehensive detection.

Several methods have been proposed for harmful meme detection, primarily focusing on modeling based on targets of memes Pramanick et al. (2021); Sharma et al. (2022); Koutlis et al. (2023); Ji et al. (2023). However, they do not apply to Chinese harmful memes, which often lack specific targets. Despite this limitation, some methods enhance the detector's understanding of memes by integrating image descriptions to make decisions Wang et al. (2024); Cao et al. (2022). These studies still inspire us to integrate more comprehensive contextual information of meme content into detectors.

**Toxic Language.** Harmful memes are closely linked to toxic language (Kiela et al., 2020), which are rude, disrespectful, or unreasonable, and can drive people away from conversations (Dixon et al., 2018). Chinese harmful memes frequently feature toxic language in their inline text, utilizing slang and linguistic phenomena like homophony. Therefore, understanding Chinese memes necessitates the incorporation of linguistic knowledge.

Additionally, labeling both harmful memes and toxic language is often subjective. Several studies have addressed this issue by focusing on mitigating the subjective bias of annotators during the construction of toxic language datasets (Waseem and Hovy, 2016; Ross et al., 2016; Zeinert et al., 2021; Lu et al., 2023; Wang et al., 2023, 2024), enhancing the reliability of datasets. In this paper, we adopt these measures as a reference in the annotation process of our ToxiCN MM.

## 3 Dataset Construction

### Overview

In this section, we detail the construction of our ToxiCN MM dataset. We first define "Chinese harmful memes" to guide the dataset annotation. We then conduct fine-grained annotation for memes collected from Chinese platforms. In addition to the basic binary labels, we analyze harmful memes from both harmful types and combinations of inline text and image information. After the annotation, statistics of ToxiCN MM are presented. The diagram of dataset construction is shown in Figure 2.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Work** & **Language** & **Size** & **Ratio** & **Pot. Tox.** \\ \hline HarMeme (Pramanick et al., 2021) & English & 3,544 & 34.96\% & ✗ \\ Harm-C (Pramanick et al., 2021) & English & 3,013 & 35.31\% & ✗ \\ Harm-P (Pramanick et al., 2021) & English & 3,020 & 49.21\% & ✗ \\ HMC (Kiela et al., 2020) & English & 10,000 & 38.00\% & ✗ \\ MMHS150K (Gomez et al., 2020) & English & 150,000 & 24.68\% & ✗ \\ TamilMemes (Suryawanshi et al., 2020) & Indian & 2,969 & 65.71\% & ✗ \\ MUTE (Hossain et al., 2022) & Bengali & 4,158 & 37.87\% & ✗ \\ MAMI (Fersini et al., 2022) & English & 11,000 & 50.00\% & ✗ \\ \hline ToxiCN MM (ours) & Chinese & 12,000 & 31.89\% & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Information of harmful meme datasets, in terms of _Language_, _Size_, _Ratio_ of harmful samples, and whether containing memes exhibiting potential toxicity (_Pot. Tox._) without specific targets.

### Definition Development

The recognized definition of _"harmful meme"_ typically pertains to memes that target specific social entities. However, numerous memes on the Chinese Internet diverge from this definition by only propagating negative values without specific targets, which can be equally detrimental to society. To adapt to the Chinese online environment, a refined definition is necessary. Here we introduce the definition of Chinese harmful memes:

_Chinese harmful memes are multimodal units consisting of an image and Chinese inline text that have the potential to cause harm to an individual, an organization, a community, a social group, or society as a whole. These memes can range from offense or joking that perpetuate harmful stereotypes towards specific social entities, to memes that are more subtle and general but still have the potential to cause harm. It is important to note that Chinese harmful memes can be created and spread intentionally or unintentionally. They often reflect and reinforce underlying negative values and cultural attitudes on the Chinese Internet, which are detrimental from legal or moral perspectives._

According to the definition, we further identified the most common harmful types of memes on Chinese platforms based on the consensus of social psychology (Liu and Xu, 2016; Lin and Zhang, 2019) and communication (Peng, 2019; Zheng, 2016) studies. Specifically, it mainly includes _targeted harmful_, _general offense_, _sexual innuendo_, and _dispirited culture_. The harm of these memes to individuals and society has been widely discussed. In this study, we focus on these harmful types when constructing the dataset.

### Data Collection and Filtering

Data collection is the basic work of constructing datasets, and its breadth and quality greatly affect the subsequent research. To ensure a comprehensive dataset, we collect Chinese memes from two well-known public online platforms, _Weibo_ and _Baidu Tieba_, both widely representative of local users in China with active meme communities. We first conduct a random crawl to obtain a diverse set of memes. To maximize the inclusion of harmful memes, we further focus our data crawl on sensitive topics commonly debated online (_e.g._, "_gender_" and "_region_"). Additionally, we also target memes expressing negative emotions and attitudes (_e.g._, "_crazy_" and "_Dispirited Culture_") to enrich the dataset with samples potentially exhibiting toxicity. A total of about 14k memes are collected. We then de-duplicate the data and filter out dirty samples including unreadable memes. The final dataset contains 12k refined memes.

Subsequently, we utilize Baidu-OCR to extract inline text from memes, which offers a high-precision service for Chinese text recognition. To further enhance the sample quality, we also introduce a manual review process to examine the accuracy of the extracted text. Specifically, we normalize the text by adding appropriate separators and removing additional line breaks and spaces.

Figure 2: Illustration of ToxiCN MM construction procedure. According to the definition (Section 3.2), data collection and filtering (Section 3.3) and fine-grained annotations (Section 3.4) are conducted sequentially. The final statistics of ToxiCN MM are presented in Section 3.5.

### Data Annotation

#### 3.4.1 Annotator Selection and Training

Before the formal annotation process, it is crucial to select annotators carefully and mitigate their subjective bias, as this can significantly impact the quality of the dataset (Waseem and Hovy, 2016). To this end, we adopt the following measures: The majority of active users on Chinese platforms are between 12 and 35 years old. Considering Chinese laws that restrict individuals under 18 from engaging in activities that could harm their physical or mental health, we selected annotators aged 18 to 35. We assessed the annotators' proficiency in Chinese meme culture through questionnaires and ensured diversity in terms of gender, region, and education level to enhance reproducibility. The demographics of annotators are shown in Table 2.

During the training of annotators, we provided definitions of Chinese harmful memes, their various types, and conducted case analyses with diverse examples. To evaluate the annotators' abilities, we introduced three test groups of 100 memes each. Annotators labeled the memes independently, with researchers finalizing the labels. Post-round discussions were held to reduce errors, and detailed criteria, including edge cases, were established. Annotators improved from 63% accuracy in the first round to 78% in the final round, demonstrating the effectiveness of the training.

#### 3.4.2 Label Annotation

To guarantee the consistency of label annotation, we establish a comprehensive annotation framework as a guideline. The specific process includes the following three stages.

**Whether Harmful**. The foundation of the labeling is to determine whether a meme is harmful or benign, which is a binary annotation. We strictly follow the definition of "_Chinese harmful memes_", focusing not only on targeted harmful memes but also on samples exhibiting potential toxicity without specific targets.

**Harmful Type**. In the second stage, we further refine the categorization of harmful memes, including targeted harmful, general offensive, sexual minuendo, and dispirited culture. The annotation criteria for each harmful type are provided below. **Targeted Harmful** memes express disgust, prejudice, or stereotypes towards specific individuals or social groups. In contrast, **General Offensive** memes encompass sarcastic or rude content but lack specific targets. We also adhere to psychological and sociological definitions to classify the other two types: **Sexual Innundedo** refers to memes that imply sexual intent to provoke sexual arousal (Bell, 1997). Here we label memes that contain suggestive elements but not sexism or sexual assault as such samples to distinguish them from targeted harmful memes. And **Dispirited Culture** is characterized by the integration of decadent and desperate emotions, conveying a self-negative attitude (Dong et al., 2017).

**Modality Combination**. As multimodal units, harmful memes consist of both textual and visual modality, expressing toxicity through fused or independent features (Kiela et al., 2020). To gain a more comprehensive understanding of how harmful content is propagated via memes, we classify them based on the toxic manifestation of these two modalities, exploring their individual and combined effects. Among them, **Text-Image Fusion** memes exhibit toxicity only through the combined effect of both modalities, while the text and image separately remain benign. In contrast, **Harmful Text** and **Harmful Image** categories refer to one modality (either the text or the image) that independently exhibits toxicity.

During the label annotation, each meme is labeled by at least three annotators. And we use a majority vote to assign the final label. In addition, specific targets of targeted harmful memes are provided. We then discuss the Inter-Annotator Agreement (IAA) of each granularity, as shown in Appendix B.3.

### Statistics Description

For subsequent model training and evaluation, all samples in ToxiCN MM are divided into a training set and a test set at a ratio of 8:2, as detailed in Table 3. We note that there exists a sample imbalance

\begin{table}
\begin{tabular}{c c} \hline \hline
**Characteristic** & **Demographics** \\ \hline Gender & Male: 5, Female: 5 \\ Age & \textless{}20: 3, 20–30: 5, \textgreater{}30: 2 \\ Race \& Region & Asians: 7, Others: 3 \\ Education & UG: 3, PG: 4, PhD: 3 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Annotators demographics.

among different categories of harmful samples. Memes containing general offensive content (_Off._) constitute nearly 40% of harmful memes. Regarding modality combinations, over 50% of the inline text of harmful memes is harmful. Given that the data distribution accurately reflects the real state of platforms, we do not introduce supplementary sampling to address existing imbalances.

We then analyze the modality combinations across different harmful types, as shown in Table 4. Each type displays distinct patterns in its modality combinations. For example, memes containing general offense or dispirited culture (_Disp._) mainly feature inherently harmful inline text. Contrast, over 50% of targeted harmful memes (_Conv._) integrate multimodal features to express toxicity, where both text and image are individually benign. Moreover, there are 63 memes in which both text and image exhibit toxicity.

## 4 Detector Development

### Overview

To improve the detector's understanding of memes, we present a baseline detector, Multimodal Knowledge Enhancement (MKE), which integrates contextual information of meme content for more accurate predictions. We first leverage the LLM to capture the contextual context of memes and generate enhanced captions. Then, we fine-tune the detector by integrating the original inputs (i.e., text-image pairs) with the generated captions. The illustration of MKE is shown in Figure 3.

For a given meme, its inline text and image are encoded by modality-specific encoders, represented as \(S\in\mathbb{R}^{d_{s}}\) for text and \(V\in\mathbb{R}^{d_{v}}\) for the image, where \(d_{s}\) and \(d_{v}\) denote the dimensions of the textual and visual vector spaces, respectively.

### Knowledge Mining

We instruct the LLM to generate enhanced captions for the meme by designing the instruction template, which respectively captures the contextual information from both the inline text and image. To improve the understanding of the inline text that may contain slang, we enable LLM to incorporate language features unique to Chinese for semantic analysis. The template is as follows: "_Considering Chinese linguistic characteristics, please analyze the meaning of the text <Texry._" We further convert the image into textual descriptions with the multimodal large language model (MLLM), capturing

\begin{table}
\begin{tabular}{c|c c c|c} \hline \hline  & **T-I** & **Harm.T** & **Harm.I** & **Total** \\ \hline
**Tg.** & 575 & 404 & 47 & 1,016 \\
**Off.** & 198 & 1,247 & 93 & 1,498 \\
**Sex.** & 431 & 307 & 186 & 914 \\
**Disp.** & 149 & 234 & 19 & 399 \\ \hline
**Total** & 1,353 & 2,192 & 345 & 12,000 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Modality combination distribution of different harmful types in ToxiCN MM.

Figure 3: Overview of MKE. The translation of the inline text is ”what vegetable dog are you”.

\begin{table}
\begin{tabular}{c|c|c|c c c|c c c|c} \hline \hline \multirow{2}{*}{**Split**} & \multirow{2}{*}{**N-Harm.**} & \multirow{2}{*}{**Harm.**} & \multicolumn{4}{c|}{**Harmful Type Category**} & \multicolumn{4}{c|}{**Combination Category**} & \multirow{2}{*}{**Total**} \\ \cline{3-4} \cline{6-10}  & & & **Tg.** & **Off.** & **Sex.** & **Disp.** & **T-I** & **Harm.T** & **Harm.I** \\ \hline
**Train** & 6,538 & 3,062 & 813 & 1,198 & 731 & 320 & 1,082 & 1,754 & 276 & 9,600 \\
**Test** & 1,635 & 765 & 203 & 300 & 183 & 79 & 271 & 438 & 69 & 2,400 \\ \hline
**Total** & 8,173 & 3,827 & 1,016 & 1,498 & 914 & 399 & 1,353 & 2,192 & 345 & 12,000 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Basic statistics of ToxiCN MM, listing the number of non-harmful (_N-Harm._) and harmful (_Harm._) samples, containing targeted harmful memes (_Tg._), general offense (_Off._), sexual innuendo (_Sex._), and dispirited culture (_Disp._), as well as each modality combination category (including text-image fusion (_T-I_), harmful text (_Harm.T_) and harmful image (_Harm.I_)).

harmful elements in the context of Chinese culture. The template is designed as "_Considering Chinese cultural background, please describe the content of the image <Image>_."

In the process of knowledge mining, all parameters of LLMs are frozen. To facilitate knowledge integration for subsequent detectors, the captions of inline text and images are represented by the text encoder, denoted as \(K_{s}\) and \(K_{v}\in\mathbb{R}^{d_{s}}\).

### Knowledge Integration

To leverage contextual information, we employ a cross-attention mechanism to integrate the inline text with two types of caption information, due to the consistency of the textual vector space. The feature introducing the textual caption \(K_{s}\) is defined as \(S_{K_{s}}=\mathrm{Softmax}\left(SK_{s}^{\ T}/\sqrt{d_{s}}\right)S\). Similarly, the feature introducing the visual caption \(K_{v}\) is obtained and denoted as \(S_{K_{v}}\). We then incorporate these features into a knowledge-enhanced representation \(S_{K}=\text{Mean}(S,S_{K_{s}},S_{K_{v}})\), where \(S_{K}\in\mathbb{R}^{d_{s}}\). Next, we concatenate \(S_{K}\) with the original image feature \(V\) to obtain the final representation of a meme, denoted as \(C\in\mathbb{R}^{d_{c}}\), where \(d_{c}=d_{s}+d_{v}\). \(C\) is then processed by a trainable classifier, which applies a linear transformation followed by a softmax function to produce detection probabilities.

## 5 Experiments

### Tasks and Baselines

We utilize ToxiCN MM as the benchmark for Chinese harmful meme detection. Specifically, we establish two progressive tasks. (I) **Harmful Meme Detection**, a binary classification task, detects if a meme is harmful; (II) **Harmful Type Identification**, a multi-classification, further identifies its harmful type, including targeted harmful memes, general offense, sexual innuendo, or dispirited culture. In addition to MKE, we evaluate the performance of various baselines, including both unimodal and multimodal models. For unimodal models, we utilize RoBERTa (Liu et al., 2019), GPT-3.5, and GPT-4 (text input) as text-only models, while ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2021) serve as image-only models. For multimodal models, we employ CLIP (Radford et al., 2021), the fusion of RoBERTa and ViT, which concatenates representations of the text and image for classification, and GPT-4 (text and image input).

### Implementation

We adopt precision (\(P\)), recall (\(R\)), and macro \(F_{1}\)-score (\(F_{1}\)) as metrics. We also report the \(F_{1}\) of harmful memes and each harmful type. We respectively utilize CLIP and the fusion of RoBERTa and ViT as the backbones of MKE, and we use GPT-4 to generate enhanced captions. For conventional PLMs, we fine-tune their parameters and select the best-performing model based on test set outcomes. For LLMs, we evaluate their performance in a zero-shot scenario, using instruction templates in Chinese. More details are provided in Appendix B.5.

### Results and Discussions

In this section, we present our experimental results and conduct a detailed analysis. The performance of baselines is evaluated across two tasks, as shown in Table 5. From the results, we can observe that: (1) In contrast to LLMs, conventional fine-tuned pre-trained baselines (i.e., CLIP and the combination of RoBERTa and ViT) achieve better detection performance, indicating their effectiveness in specific tasks. When considering the modality of input information, RoBERTa, which solely utilizes the inline text of memes, achieves a significantly higher \(F_{1}\) score (average increase of 8.4%) than vision-based methods such as ResNet and ViT, which solely utilize images. This result supports the conclusion drawn in (Hee et al., 2022), namely that text comprehension plays a more crucial role than image understanding in the detection of harmful memes.

(2) GPT-4 and GPT-3.5 show similar performance in binary _harmful meme detection_ when only the inline text is provided, and there is a clear enhancement in the multiclass _harmful type identification_ task. After incorporating the image input, GPT-4 shows the best detection performance for sexual innuendo (_Sex._) memes, while its performance decreases for general offense (_Off._) and dispirited culture (_Disp._). Referring to Table 4, we observe that most samples of _Sex._ exhibit toxicity through image-text fusion or harmful images, whereas images of _Off._ and _Disp._ are mostly benign. This suggests that the toxicity of visual information has a significant impact on the decisions of GPT-4. We will further explain this in the following case study.

(3) Our MKE demonstrates superior performance, with an average macro-F1 -score improvement of 0.73% and 3.22% over the backbone models for both tasks. This improvement illustrates the effectiveness of introducing contextual information of meme content for detecting Chinese harmful memes. Ablation studies show that both enhanced captions for inline text and images contribute to the detector's deeper understanding of memes, leading to more precise classifications. Additionally, the degree of performance enhancement varies depending on the type of harmful meme. For instance, for targeted harmful memes (\(T_{g}\).), where toxicity often relies on the combination of image and text, image captions provide a greater boost (2.07%). In contrast, for memes expressing dispirited culture (_Disp._), where the text is typically harmful, inline text captions lead to a larger improvement (2.58%).

### Case Study

To further illustrate the rationales of MKE, we provide several case studies, as shown in Table 6. We list enhanced captions of harmful memes and the predictions of other models for reference. We also instruct GPT-4 to generate reasons for its detection decisions. We do not introduce additional templates to standardize its reasoning to reflect GPT-4's true understanding of memes more accurately.

Exp. (a) is a targeted harmful meme towards Asians. Through the caption, we observe that GPT-4 understands the meme's meaning solely through the inline text, recognizing the high standard of Asian parents on their children's academic performance. This highlights GPT-4's strong contextual understanding. After integrating this information, compared to the backbone (Fusion), our MKE model makes the correct decision, illustrating that incorporating contextual information of meme content enhances the model's understanding of memes.

For more insight into the challenges of detecting Chinese harmful memes, we manually inspected the samples misclassified by most baselines. Two main types of errors are summarized.

**Type I error**: Benign information contained in harmful memes can influence the judgment of models, resulting in incorrect detection. In Exp. (b), when presented solely with the inline text, GPT-4 accurately interprets the meme's meaning, comparing "\(I\)" with a "_little mouse_" to convey a dispirited culture. However, upon introducing the image, GPT-4 mistakenly interprets the mouse as being "_gently stroked_" and incorrectly categorizes the meme as harmless. This suggests that the model may overlook the potential toxicity of memes due to the seemingly benign nature of a certain modal. In contrast, MKE integrates original sample and caption information to make the correct judgment.

\begin{table}
\begin{tabular}{c c c|c c c|c c c c c c} \hline \hline \multirow{2}{*}{Modality} & \multicolumn{4}{c|}{Harmful Meme Detection} & \multicolumn{4}{c}{Harmful Type Identification} \\ \cline{3-11}  & Model & P & R & F1 & F1\({}_{\text{Hum}}\) & P & R & F1 & F1\({}_{\text{T}_{g}}\) & F1\({}_{\text{O}T}\) & F1\({}_{\text{S}C}\) & F1\({}_{\text{Dip.}}\) \\ \hline \multirow{4}{*}{Text} & GPT3.5 & 69.46 & 66.59 & 67.46 & 53.25 & 36.93 & 23.60 & 24.35 & 27.42 & 29.01 & 10.52 & 14.63 \\  & GPT4 & 74.52 & 65.59 & 68.01 & 51.78 & 58.29 & 41.81 & 44.86 & 11.43 & 62.22 & 32.36 & 34.15 \\  & RoBERTa & 75.52 & 77.54 & 76.36 & 66.48 & 53.24 & 60.06 & 55.85 & 48.79 & 71.81 & 42.70 & 29.23 \\ \hline \multirow{3}{*}{Image} & ResNet & 66.61 & 66.92 & 66.76 & 53.76 & 35.66 & 36.23 & 36.46 & 16.67 & 51.28 & 20.93 & 12.28 \\  & ViT & 68.97 & 68.61 & 68.78 & 57.24 & 43.38 & 37.86 & 39.10 & 24.17 & 54.57 & 34.32 & 9.01 \\ \hline \multirow{4}{*}{Multimodal} & GPT4 & 74.67 & 68.64 & 70.11 & 55.77 & 58.87 & 41.77 & 43.89 & 23.53 & 32.56 & 55.74 & 23.53 \\ \cline{2-11}  & Fusion & 77.77 & 79.18 & 78.39 & 69.61 & 58.93 & 60.85 & 59.35 & 50.24 & 73.35 & 47.85 & 38.71 \\ \cline{1-1}  & + \(K_{s}\) & 78.17 & 79.32 & 79.04 & 70.33 & 60.09 & 63.38 & 61.28 & 51.02 & 75.60 & 48.75 & **43.06** \\ \cline{1-1}  & + \(K_{v}\) & 77.93 & 79.32 & 78.55 & 69.85 & 59.16 & 60.67 & 59.61 & 51.41 & 75.00 & 48.68 & 36.23 \\ \cline{1-1}  & + MKE & 77.96 & 80.96 & 79.16 & 70.22 & **62.41** & 62.08 & **62.17** & 53.27 & 77.45 & 56.10 & 39.24 \\ \cline{1-1} \cline{2-11}  & CLIP & 78.95 & 80.26 & 79.54 & 71.28 & 54.85 & 64.95 & 57.85 & 49.58 & 74.65 & 49.64 & 26.92 \\ \cline{1-1}  & + \(K_{s}\) & 79.23 & 80.71 & 79.89 & 71.72 & 56.24 & **66.45** & 59.23 & 47.80 & 77.17 & 54.72 & 27.78 \\ \cline{1-1}  & + \(K_{v}\) & 79.39 & **80.93** & 80.07 & 71.96 & 57.20 & 64.27 & 59.24 & **53.41** & 76.97 & 52.53 & 25.24 \\ \cline{1-1}  & + MKE & **79.76** & 80.79 & **80.23** & **72.35** & 60.38 & 63.52 & 61.47 & 51.52 & **77.19** & **57.14** & 33.33 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Detection performance of baselines. Results show the mean of \(P\), \(R\), and macro \(F_{1}\), where the **bold** and underline scores respectively represent the optimal and suboptimal values. _Fusion_ refers to the fusion of RoBERTa and ViT, and \(K_{s}\) and \(K_{v}\) respectively denote introducing enhanced captions of inline text and image. All results are statistically significant, as determined by a \(t\)-test (\(p<0.01\)).

**Type II error**: Harmful memes containing unique cultural backgrounds are easily missed by models. Among them, the most challenging samples are the memes whose inline text contains specific expressions in Chinese, such as homophones and metaphors. Take Exp. (c) for example, where the term "_dog-banana_" is a homonym of "_bark_" in Chinese. Therefore, this meme is essentially a harmful meme containing general offense, implicitly expressing dissatisfaction with another person. Due to the lack of related knowledge, current models fail to comprehend the underlying semantics of these memes, making them difficult to detect successfully.

These case studies further illustrate that Chinese harmful memes detection is a complex multimodal semantic understanding task, which is challenging for existing models. The error analysis shows that fully integrating image and text information and introducing more comprehensive knowledge about Chinese culture are both crucial for effectively detecting harmful memes.

## 6 Conclusions and Future Work

In this paper, we focus on the comprehensive detection of Chinese harmful memes. We present the first Chinese harmful meme dataset ToxiCN MM. It has 12k samples including not only targeted harmful memes but also those only exhibiting potential toxicity without specific targets, adapting to the Chinese online environment. In addition to binary labels, ToxiCN MM provides harmful types and modality combination categories of memes. To improve the understanding of Chinese harmful memes, we present a Multimodal Knowledge Enhancement (MKE) detector, introducing the contextual information of inline text and images. In the experimental phase, we evaluate multiple baseline models for their performance in detecting Chinese harmful memes. Our case study suggests that integrating multimodal information and comprehensive background knowledge is crucial for effective detection.

In future work, we aim to design more effective methods for Chinese harmful memes detection. Meanwhile, we notice that the accuracy of LLMs in detecting Chinese harmful memes is still limited. Considering the potential harm that these memes may cause, this task can be used to evaluate the safety of LLMs. We will employ prompt engineering and instruction fine-tuning methods to explore and enhance the detection performance of LLMs. Additionally, we will continuously evaluate state-of-the-art models to ensure the effectiveness of ToxiCN MM. We expect our dataset, benchmark, and insights will assist researchers in related fields.

## 7 Limitations

In this study, we focus on several most common harmful types of memes on the Chinese online environment. Due to the filtering mechanism, some harmful memes, such as those containing _fake news_, are extremely scarce on Chinese platforms. As a result, our ToxiCN MM does not encompass all harmful types. The techniques we used to boost the percentage of harmful content during the dataset construction process may introduce problematic bias. In future work, we plan to broaden the scope and increase the number of meme cravks, focusing on more Chinese platforms to mitigate sampling bias. While we have implemented several measures to mitigate annotation bias, we acknowledge that our dataset may still contain mislabeled data due to the subjective understanding of annotators for Chinese harmful memes. Furthermore, our current study primarily focuses on predicting whether a given meme is harmful. We will further evaluate the ability of baselines to generate explanations for Chinese harmful memes with quantitative experiments.

## 8 Ethics Statement

Our study aims to facilitate the comprehensive detection of Chinese harmful memes and raise researchers' attention to non-English memes. The social psychological community has recognized the harms of the harmful types we selected in the dataset. We acknowledge the risk of malicious actors attempting to reverse-engineer memes. We strongly discourage and denounce such practices, emphasizing the necessity of human moderation to prevent them. All resources are intended solely for scientific research and are prohibited from commercial use. We believe the benefits of our proposed resources outweigh the associated risks. We strictly follow the data use agreements of each public online social platform. The opinions and findings contained in the samples of our presented dataset should not be interpreted as representing the views expressed or implied by the authors.

To mitigate the potential psychological impact on annotators evaluating harmful content, we implement the following protective measures: 1) obtain explicit consent regarding exposure to potentially abusive content, 2) limit weekly evaluations to manage exposure and ensure reasonable daily workloads, and 3) recommend discontinuing reviews if they experience distress. Additionally, we conduct regular well-being checks to monitor their mental health.

## Acknowledgment

This research is supported by the Natural Science Foundation of China (No. 62376051, 62076046, 62076051), the National Language Commission Key Program (No.ZDI145-80), the Liaoning Province Applied Basic Research Program (No. 2022JH2/101300270), the Liaoning Provincial Natural Science Foundation Joint Fund Program (2023-MSBA-003), and the Fundamental Research Funds for the Central Universities (DUT24LAB123, DUT24MS003). We would like to thank all reviewers for their constructive comments.

## References

* 4 May 2023_, pages 3734-3743. ACM.
* Bai et al. (2023) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_.
* Bell (1997) David M Bell. 1997. Innuendo. _Journal of Pragmatics_, 27(1):35-59.
* Blaier et al. (2021) Efrat Blaier, Itzik Malkiel, and Lior Wolf. 2021. Caption enriched samples for improving hateful memes detection. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, pages 9350-9358. Association for Computational Linguistics.
* Blaier et al. (2015)Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and Jing Jiang. 2022. Prompting for multimodal hateful meme classification. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 321-332. Association for Computational Linguistics.
* Dixon et al. (2018) Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018. Measuring and mitigating unintended bias in text classification. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018, New Orleans, LA, USA, February 02-03, 2018_, pages 67-73. ACM.
* Dong et al. (2017) Ziyang Dong, Jinfeng Chang, and Jian Sun. 2017. A study on "dispirited culture" of online youth from the perspective of social psychology. _Youth and Adolescent Studies_, (3-7+31).
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net.
* Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335.
* Fersini et al. (2022) Elisabetta Fersini, Francesca Gasparini, Giulia Rizzi, Aurora Saibene, Berta Chulvi, Paolo Rosso, Alyssa Lees, and Jeffrey Sorensen. 2022. Semeval-2022 task 5: Multimedia automatic misography identification. In _Proceedings of the 16th International Workshop on Semantic Evaluation, SemEval@NAACL 2022, Seattle, Washington, United States, July 14-15, 2022_, pages 533-549. Association for Computational Linguistics.
* Gomez et al. (2020) Raul Gomez, Jaume Gibert, Lluis Gomez, and Dimosthenis Karatzas. 2020. Exploring hate speech detection in multimodal publications. In _IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020_, pages 1459-1467. IEEE.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pages 770-778. IEEE Computer Society.
* 29, 2022_, pages 3651-3655. ACM.
* Student Research Workshop, Online, November 20, 2022_, pages 32-39. Association for Computational Linguistics.
* Hu et al. (2023) Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. Large multilingual models pivot zero-shot multimodal learning across languages. _CoRR_, abs/2308.12038.
* 4 May 2023_, pages 3868-3872. ACM.
* Kiela et al. (2020) Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.
* Krizhevsky et al. (2014)Eric Koukounas and Nicole M Letch. 2001. Psychological correlates of perception of sexual intent in women. _The Journal of social psychology_, 141(4):443-456.
* Koutlis et al. (2023) Christos Koutlis, Manos Schinas, and Symeon Papadopoulos. 2023. Memefier: Dual-stage modality fusion for image meme classification. In _Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, ICMR 2023, Thessaloniki, Greece, June 12-15, 2023_, pages 586-591. ACM.
* 11th CCF International Conference, NLPCC 2022, Guilin, China, September 24-25, 2022, Proceedings, Part I_, volume 13551 of _Lecture Notes in Computer Science_, pages 527-538. Springer.
* Lin and Zhang (2019) Aijun Lin and Bo Zhang. 2019. Emojis as discourse: Symbolic consumption and sociological reflection on internet emojis. _Modern Communication (Journal of Communication University of China)_, 41(35-40).
* Liu and Xu (2016) Min Liu and Shuai Xu. 2016. An instant'meme battle': Communication and identification in the spread of emoji packs". _Journal of News Research_, 7(339).
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692.
* Lu et al. (2023) Junyu Lu, Bo Xu, Xiaokun Zhang, Changrong Min, Liang Yang, and Hongfei Lin. 2023. Facilitating fine-grained detection of chinese toxic language: Hierarchical taxonomy, resources, and benchmarks. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 16235-16250. Association for Computational Linguistics.
* Miao and Xu (2022) Cunlong Miao and Maohua Xu. 2022. The representation and guidance path of youth "dispirited culture". _Journal of Socialist Theory Guide_, (123-128).
* Otsri (2020) Magi Otsri. 2020. Non-sexist sexual humor as quid pro quo sexual harassment. _Sexuality & Culture_, 24(1):94-112.
* Peng (2019) Lan Peng. 2019. Emotion icon: Password, label and mask. _Journal of Xi'an Jiaotong University (Social Sciences)_, 39(104-110+153).
* Pramanick et al. (2021a) Shraman Pramanick, Dimitar Dimitrov, Rituparna Mukherjee, Shivam Sharma, Md. Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2021a. Detecting harmful memes and their targets. In _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, volume ACL/IJCNLP 2021 of _Findings of ACL_, pages 2783-2796. Association for Computational Linguistics.
* Pramanick et al. (2021b) Shraman Pramanick, Shivam Sharma, Dimitar Dimitrov, Md. Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2021b. MOMENTA: A multimodal framework for detecting harmful memes and their targets. In _Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021_, pages 4439-4455. Association for Computational Linguistics.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR.
* Ross et al. (2016) Bjorn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki. 2016. Measuring the reliability of hate speech annotations: The case of the european refugee crisis. In _3rd Workshop on Natural Language Processing for Computer-Mediated Communication/Social Media_, pages 6-9. Ruhr-Universitat Bochum.
* Ross et al. (2017)Shivam Sharma, Md. Shad Akhtar, Preslav Nakov, and Tanmoy Chakraborty. 2022a. DISARM: detecting the victims targeted by harmful memes. In _Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, pages 1572-1588. Association for Computational Linguistics.
* Sharma et al. (2022b) Shivam Sharma, Firoj Alam, Md. Shad Akhtar, Dimitar Dimitrov, Giovanni Da San Martino, Hamed Firooz, Alon Y. Halevy, Fabrizio Silvestri, Preslav Nakov, and Tanmoy Chakraborty. 2022b. Detecting and understanding harmful memes: A survey. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pages 5597-5606. ijcai.org.
* Suryawanshi et al. (2020) Shardul Suryawanshi, Bharathi Raja Chakravarthi, Pranav Verma, Mihael Arcan, John Philip McCrae, and Paul Buitelaar. 2020. A dataset for troll classification of TamilMemes. In _Proceedings of the WILDRE5- 5th Workshop on Indian Language Data: Resources and Evaluation_, pages 7-13, Marseille, France. European Language Resources Association (ELRA).
* Thornhill and Thornhill (1983) Randy Thornhill and Nancy Wilmsen Thornhill. 1983. Human rape: An evolutionary analysis. _Ethology and sociobiology_, 4(3):137-173.
* 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12-15, 2023, Proceedings, Part II_, volume 14303 of _Lecture Notes in Computer Science_, pages 640-652. Springer.
* Wang et al. (2024a) Hongbo Wang, Junyu Lu, Yan Han, Kai Ma, Liang Yang, and Hongfei Lin. 2024a. Towards patronizing and condescending language in chinese videos: A multimodal dataset and detector. _CoRR_, abs/2409.05005.
* 13th National CCF Conference, NLPCC 2024, Hangzhou, China, November 1-3, 2024, Proceedings, Part V_, volume 15363 of _Lecture Notes in Computer Science_, pages 95-106. Springer.
* Waseem and Hovy (2016) Zeerak Waseem and Dirk Hovy. 2016. Hateful symbols or hateful people? predictive features for hate speech detection on twitter. In _Proceedings of the Student Research Workshop, SRW@HLT-NAACL 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016_, pages 88-93. The Association for Computational Linguistics.
* 15, 2022_, pages 2887-2899. ACM.
* Yang (2018) Jianhua Yang. 2018. The guidance and norms of the meme culture. _People's Tribune_, (140-141).
* Zeinert et al. (2021) Philine Zeinert, Nanna Inie, and Leon Derczynski. 2021. Annotating online misogyny. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pages 3181-3197. Association for Computational Linguistics.
* Zhang and Zhao (2021) Shengnan Zhang and Linyun Zhao. 2021. The communication mechanism and rational reflection of internet "dispirited culture". _Youth Journalist_, (40-42).
* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_.
* Zheng (2016) Manning Zheng. 2016. Research on the popularity of network expression meme and its turning of discourse space. _Editorial Friend_, (42-46).
* Zhang et al. (2017)

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Appendix A. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix B. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? See Section 5, Appendix C, and supplemental material. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 3.5 and 4.2, and Appendix D.6. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See Section 4.2. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix D.6.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [N/A] 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See Section 5 and Appendix C. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix B. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix B.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Section 3.4. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] See Appendix B. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Appendix D.3.

Research Background

Various harmful memes propagate on Chinese platforms (Peng, 2019; Zheng, 2016). While their creators and disseminators intend to simply express emotions or humor, and their original intent may be harmless, these memes have a significant negative impact on society when abused (Liu and Xu, 2016; Lin and Zhang, 2019). In this section, we individually explore the detrimental impacts of various harmful meme types, highlighting the importance of detecting these memes.

**Targeted Harmful.** Memes targeting specific individuals or groups can perpetuate hate speech, discrimination, and prejudice, fostering an environment of intolerance (Sharma et al., 2022b). They fuel online toxicity and create hostile environments. Furthermore, they have the potential to incite real-world violence or harassment and worsen social divisions.

**General Offensive.** Memes containing general offenses breed an aggressive culture prone to controversy and online violence (Zheng, 2016). Their influence extends beyond the individual, shaping the overall tone of the online environment. Additionally, such offensive content negatively impacts the development of correct values and healthy personalities among children (Yang, 2018).

**Sexual Innuendo.** While sexual innuendo content generally does not involve coercion in a sexual relationship, it can still be misconstrued due to gender and cultural differences, thereby contributing to sexualization (Thornhill and Thornhill, 1983; Koukounas and Letch, 2001). Moreover, inappropriate sexual innuendo may be considered sexual harassment (Otsri, 2020).

**Dispirited Culture.** Memes containing dispirited culture often evoke negative emotions and contribute to feelings of social isolation. This leads to an increase in personal depression, making it difficult for individuals to have positive interactions and relationships with others (Miao and Xu, 2022). Furthermore, the spread of these memes inadvertently undermines the value of positive thinking and promotes social anxiety (Zhang and Zhao, 2021).

## Appendix B Implementation Details

### Details of Data Filtering

For data filtering, we refer to the existing Chinese meme datasets (Li et al., 2022; Xu et al., 2022) and apply the following criteria:

* The meme text must contain Chinese (including code-switching); memes only containing other languages are not allowed.
* The meme text must have actual semantics; Thus, samples where the text is too brief, e.g. containing only modal particles, are removed.
* The meme must be readable. Hence, low-resolution samples that cannot be extracted inline text are excluded.
* The meme must be multimodal, meaning it should contain both the inline text and image information.

Here we present some examples of memes that were removed during the filtering process for failing to satisfy some of the above criteria, as shown in Figure B1.

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

### Evaluation of Chinese MLLMs

In this section, we utilize ToxiCN MM to evaluate the performance of Chinese LLMs in detecting Chinese harmful memes under the zero-shot scenario. Due to the unavailability of APIs for multimodal dialogue in existing commercial Chinese LLMs (e.g., Wenxin Yiyan), we only evaluate the effect of several open-source Chinese LLMs, including VisualGLM (Du et al., 2022), Qwen-VL (Bai et al., 2023), and VisCPM (Hu et al., 2023). The results are shown in Table C1.

Based on the result, we note that the detection performance of these open-source Chinese LLMs is not satisfactory compared to other baselines shown in Table 5. This is because these models have weak multimodal reasoning capabilities and contain limited background knowledge required for detection.

## Appendix D Licensing and Maintenance Plan

### Licensing

We confirm that the dataset is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license.

[MISSING_PAGE_FAIL:19]