# Learning Interatomic Potentials at Multiple Scales

 Xiang Fu

MIT CSAIL

xiangfu@mit.edu &Albert Musaelian

Harvard John A. Paulson

School of Engineering and Applied Sciences

albym@g.harvard.edu &Anders Johansson

Harvard John A. Paulson

School of Engineering and Applied Sciences

andersjohansson@g.harvard.edu &Tommi Jaakkola

MIT CSAIL

tommi@csail.mit.edu &Boris Kozinsky

Harvard John A. Paulson School of Engineering and Applied Sciences and

Robert Bosch Research and Technology Center

bkoz@g.harvard.edu

###### Abstract

The need to use a short time step is a key limit on the speed of molecular dynamics (MD) simulations. Simulations governed by classical potentials are often accelerated by using a multiple-time-step (MTS) integrator that evaluates certain potential energy terms that vary more slowly than others less frequently. This approach is enabled by the simple but limiting analytic forms of classical potentials. Machine learning interatomic potentials (MLPs), in particular recent equivariant neural networks, are much more broadly applicable than classical potentials and can faithfully reproduce the expensive but accurate reference electronic structure calculations used to train them. They still, however, require the use of a single short time step, as they lack the inherent term-by-term scale separation of classical potentials. This work introduces a method to learn a scale separation in complex interatomic interactions by co-training two MLIPs. Initially, a small and efficient model is trained to reproduce short-time-scale interactions. Subsequently, a large and expressive model is trained jointly to capture the remaining interactions not captured by the small model. When running MD, the MTS integrator then evaluates the smaller model for every time step and the larger model less frequently, accelerating simulation. Compared to a conventionally trained MLIP, our approach can achieve a significant speedup (\(\sim\)3x in our experiments) without a loss of accuracy on the potential energy or simulation-derived quantities.

## 1 Introduction

The interatomic potential energy that governs the dynamics of a system of atoms has long been both understood and modeled as a combination of atomic interactions of various strengths and scales. In a system containing a comparatively stiff molecule in a soft fluid, for example, the intramolecular forces are much stronger than the intermolecular forces from the solvent. Classical potentials, such as the popular optimized potentials for liquid simulations (OPLS, [1; 2]), explicitly define the potential as such a sum over simple analytic terms:

\[E(\bm{r})=E_{\mathrm{bonds}}(\bm{r})+E_{\mathrm{angles}}(\bm{r})+E_{\mathrm{ dihedrals}}(\bm{r})+E_{\mathrm{nb}}(\bm{r})\]where \(\bm{r}\) denotes the atom positions; \(E_{\rm bonds}(\bm{r}),E_{\rm angles}(\bm{r})\), and \(E_{\rm dihedrals}(\bm{r})\) are intramolecular (or "bonded") bond, angle, and torsional potentials; and \(E_{\rm nb}\) denotes the nonbonded, including intermolecular potential term. Rigid interactions such as bond vibrations, governed by \(E_{\rm bonds}\), occur at fast time scales, while the nonbonded interactions \(E_{\rm nb}\) are slower and smoother. Due to the greater number of atoms involved in nonbonded interactions, however, the computational expense of calculating the \(E_{\rm nb}\) term can be meaningfully larger.

Essentially, the intramolecular and intermolecular nonbonded forces are of different scales. While the intramolecular forces require a short time step, integrating the intermolecular forces at the same step size is often overkill. To harness this separation of both scales and computational cost, multiple-time-step (MTS) integrators [3; 4; 5; 6; 7] integrate the fast-evolving terms with a short time step and the slow-evolving terms with a long time step. MTS integrators are theoretically principled and have been widely used in various classical MD workflows [8; 9; 10; 11; 12], usually bringing a speedup of two to four times.

The approximations and limited functional forms of classical potentials are not sufficient for many applications. Ab initio molecular dynamics (AIMD) simulations--governed by a potential energy surface computed with electronic structure methods such as Density Functional Theory (DFT)--are widely used but suffer from dramatic computational limitations on time- and length-scale due to the expense and unfavorable scaling of the electronic structure calculations. The application of MTS schemes to AIMD has been limited by the difficulties in decomposing the ab initio potential into components of separate scales in the absence of a simple analytical form [13; 14].

Machine learning interatomic potentials (MLPs) [15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48] are increasingly used to run MD simulations orders of magnitude cheaper than AIMD while preserving near-AIMD accuracy. MLIPs also largely avoid classical potentials' assumption of an explicit discrete covalent bond topology, which greatly broadens their applicability, notably in materials science and reactive simulations. While MLIPs have enabled many previously impractical or impossible simulations, further improvements in the speed and cost of MLIP-driven MD are extremely valuable. MTS methods remain largely unexplored for MLIPs: although recent research [49] has combined an infrequently evaluated MLIP with a classical potential evaluated at every time step, a methodology for machine learning a scale-separated potential model from data is, to the best of our knowledge, lacking in the literature. This work presents an approach for learning complementary scale-separated MLIPs from data, yielding a \(\sim\)3x speedup in MD simulations using MTS integration.

## 2 Preliminaries

**MD simulation** involves integrating a Newtonian equation of motion \(\ddot{\bm{r}}=d^{2}\bm{r}/dt^{2}=\bm{m}^{-1}\bm{F}(\bm{r})\) with atomic positions \(\bm{r}\), masses \(\bm{m}\), and forces \(\bm{F}\). The forces are obtained by differentiating the potential energy of a molecular system \(E(\bm{r})\) with respect to the atomic positions \(\bm{r}\): \(\bm{F}(\bm{r})=-\partial E/\partial\bm{r}\).

**Allegro**[37] is an \(E(3)\)-equivariant [50] neural network MLIP architecture for predicting \(E(\bm{r})\) that enforces strictly local interactions while achieving state-of-the-art performance. Its strict locality enables efficient parallel scaling across GPUs to reach larger length- and time-scales [51]. In Allegro, every pair of atoms within a chosen fixed cutoff distance \(r\) are considered neighbors. For each ordered pair of neighboring atoms \(i\) and \(j\), an Allegro model produces a learnable, \(E(3)\)-invariant, many-body final latent representation \(\bm{x}^{ij}\) of the geometry of \(i\), \(j\), and all other neighbors of \(i\). An edge energy \(E_{ij}\) is then predicted from it via the output block, a multi-layer perception \(\mathrm{MLP}_{\rm out}\) without bias; the sum over edge energies gives the total potential energy:

\[E_{\rm ML}(\bm{r})=\sum_{(i,j):||\bm{r}_{i}-\bm{r}_{j}||\leq r}E_{ij}=\sum_{(i,j):||\bm{r}_{i}-\bm{r}_{j}||\leq r}\mathrm{MLP}_{\rm out}(\bm{x}^{ij})\] (1)

An Allegro model's predicted \(E_{\rm ML}(\bm{r})\) is trained to reproduce the potential energy and forces from a reference method such as DFT using a loss function like:

\[\mathcal{L}=\lambda_{E}\left\|\frac{E_{\rm ML}-E}{N}\right\|^{2}+\lambda_{F} \frac{1}{3N}\left\|-\frac{\partial E_{\rm ML}}{\partial\bm{r}}-\bm{F}\right\| ^{2}\] (2)

where \(\lambda_{E}\) and \(\lambda_{F}\) are loss coefficients for energy and forces and \(N\) is the number of atoms. For complete details on Allegro and its training see [37].

**MTS Integrators** accelerate MD simulations by propagating different parts of the dynamics with different time steps suited to their respective characteristic time scales. In this paper, we focus on the reversible reference system propagator algorithms (rRESPA [5; 52]), an MTS integrator popular for its rigorous derivation, time-reversibility, and symplectic properties. We show the rRESPA algorithm in Algorithm 1 and refer interested readers to [5] for detailed derivations.

```
1:Input: Inner time step \(\Delta t\), number of inner time steps per outer time step \(N_{\mathrm{inner}}\), short-range force \(\bm{F}_{s}\), long-range force \(\bm{F}_{l}\), atom masses \(\bm{m}\), initial positions \(\bm{r}\), initial velocities \(\hat{\bm{r}}\)
2:\(\hat{\bm{r}}\leftarrow\hat{\bm{r}}+\frac{1}{2}(N_{\mathrm{inner}}\Delta t) \cdot\bm{m}^{-1}\bm{F}_{l}(\bm{r})\)
3:for step \(i=1,\ldots,N_{\mathrm{inner}}\)do
4:\(\hat{\bm{r}}\leftarrow\hat{\bm{r}}+\frac{1}{2}\Delta t\cdot\bm{m}^{-1}\bm{F}_ {s}(\bm{r})\)
5:\(\hat{\bm{r}}\leftarrow\hat{\bm{r}}+\frac{1}{2}\Delta t\cdot\bm{m}^{-1}\bm{F}_ {s}(\bm{r})\)
6:endfor
7:\(\hat{\bm{r}}\leftarrow\hat{\bm{r}}+\frac{1}{2}(N_{\mathrm{inner}}\Delta t) \cdot\bm{m}^{-1}\bm{F}_{l}(\bm{r})\) ```

**Algorithm 1** An integration step of the MTS integrator (rRESPA)

## 3 Learning Scale Separation

To achieve scale separation and harness the MTS integrator, we need to enable efficient calculation of stiff and fast-evolving force terms. Meanwhile, we still need to capture the smooth and slow-evolving force terms so that the overall machine learning potential remains accurate. For many molecular systems of interest, short-range interactions, such as covalent bonds, are strong and induce stiff motions, while long-range interactions, such as non-bonded interactions, can be integrated with a longer time step.

The above observation motivates separating scales by combining MLIPs with different receptive fields. Allegro's unique combination of the leading accuracy of equivariant techniques with strict locality particularly lends itself to such a scale separation scheme. We train two models:

* **Inner model** (\(E_{\mathrm{inner}}\)): An efficient model with fewer parameters and interaction layers and a small radial cutoff that captures short-time-scale interactions.
* **Outer model** (\(E_{\mathrm{outer}}\)): An expressive model with a larger number of parameters and interaction layers and a larger radial cutoff that fits the remaining interactions not learned by the inner model.

We let the two models jointly predict the potential energy: \(E_{\mathrm{ML}}(\bm{r})=E_{\mathrm{inner}}(\bm{r})+E_{\mathrm{outer}}(\bm{r})\). Training the two models together from scratch, however, may not induce scale separation: the outer model has sufficient capacity to learn the entire reference potential energy surface, and so the short-range interactions may not be attributed to the inner model. To avoid such degeneracy, at the beginning of training, we first freeze all parameters in the outer model to let the inner model fit the force and energy within its capacity. We then later start training the outer model with a zero-initialization over its output block \(\mathrm{MLP}_{\mathrm{out}}\). This zero initialization ensures \(E_{ij}^{\mathrm{outer}}=0\) and \(\partial E_{ij}^{\mathrm{outer}}/\partial\bm{r}=0\) for all atom pairs \((i,j)\), preventing the initial noise of the outer model from interfering with the interactions already learned by the inner model. Zero-initialization has been widely used in previous works for finetuning pretrained models [53]. The training procedure is presented in Algorithm 2. We refer to our scale-separated Allegro model as **MTS-Allegro**.

## 4 Experiments

Our experiments consider an ab initio water system [54]. This dataset contains 1593 reference calculations of bulk liquid water at the revPBE0-D3 level of accuracy. Each structure contains 192 atoms (64 water molecules). We randomly sample 1000 structures for training, 100 structures for validation, and the rest for testing1. Both the Allegro model and the outer model of MTS-Allegro havetwo interaction layers, a 6 A cutoff, and the same parameter count; the inner model of MTS-Allegro has one interaction layer, a 4 A cutoff, and half the width in each layer compared to the outer model. Detailed hyperparameters are included in Appendix A.

We compare the accuracy in recovering the reference potential and various simulation-derived quantities between (1) our proposed MTS-Allegro model with rRESPA integration, (2) a conventionally trained Allegro model with standard Velocity Verlet integration, and (3) the inner model of MTS-Allegro alone with Velocity Verlet integration. We use a time step of \(0.5\) fs for Velocity Verlet integration and the inner loop of MTS integration, which is standard for water simulations [55, 56, 57]. For MTS-Allegro, we experiment with outer time steps of \([1.0,2.0,3.0,4.0]\) fs, corresponding to [2x, 4x, 6x, 8x] multiples of the inner time step.

**Force and energy prediction accuracy.** We report the force and energy prediction errors in Table 1. The MTS-Allegro model has a very similar performance to a conventionally trained Allegro model. Unsurprisingly, the inner model of MTS-Allegro alone obtains higher errors due to its limited receptive field and capacity.

**Structure and dynamics.** To investigate MTS-Allegro's ability to reproduce structural and dynamical observables, we simulate the water system in a canonical (NVT) ensemble (constant volume and temperature) at 300 K and compute the element-wise radial distribution functions (RDFs, structural) and the mean squared displacement (MSD, dynamical). We simulate for 400 picoseconds (ps) and remove the first 50 ps for equilibration when computing the observables. All models remain stable throughout the entire simulation. Figure 1 (a-d) shows the RDFs and MSD (in log-log space) for Allegro, the inner model of MTS-Allegro, and MTS-Allegro under different outer step sizes. MTS-Allegro simulations with 2x to 8x outer time steps all achieve excellent agreement with Allegro on the RDFs and MSD, while the inner model alone produces erroneous dynamics and observables due to its limited capacity and, thereby, accuracy in recovering the potential.

**Energy conservation under the microcanonical ensemble.** For a correct MD simulation in the microcanonical (NVE) ensemble, the total energy (sum of potential and kinetic energies) should remain constant over time. To evaluate the energy conservation property of the MLIPs, we initialize the water system using a structure from the test dataset and a temperature of 300 K, run energy minimization, and then simulate for 100 ps in the NVE ensemble. Figure 1 (e) shows the drift of total energy from the first frame (after removing the first 20 ps for equilibration). We observe that Allegro and MTS-Allegro with 2x and 4x outer time steps are energy-conserving. MTS-Allegro with 6x and 8x outer time steps yield drifts of \(0.12\) meV/(atom\(\cdot\)ps) and \(0.18\) meV/(atom\(\cdot\)ps), respectively.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Allegro & MTS-Allegro & MTS-Allegro, inner \\ \hline Energy MAE [meV/Atom] & 2.4 & 1.6 & 12.5 \\ Forces MAE [meV/Å] & 40.3 & 35.5 & 72.5 \\ Forces RMSE [meV/Å] & 77.4 & 76.9 & 118.3 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Energy and Force prediction mean absolute error (MAE) and root mean square error (RMSE).

**Speedup.** Figure 2 (a) shows the relative speedup of MTS-Allegro compared to Allegro at two system sizes as measured in MD simulation2. MTS-Allegro with a 4x outer time step, which preserves simulation-derived quantities and maintains energy conservation, achieves an MD speedup of 2.6x for a system size of 192 atoms and 2.9x for a system size of 1536 atoms. MTS-Allegro 6x and 8x, which obtain a further speedup, remain reliably stable in our experiments and faithfully reproduce RDFs and MSDs but do not maintain energy conservation. Such larger outer time step simulations may still find use in preliminary or other calculations whose requirements are less stringent.

Footnote 2: All simulations were run in LAMMPS [12] using pair_allegro on a single NVIDIA Tesla V100-PCIe 32GB GPU. The speed of Allegro is 0.51 and 0.072 ns/day for 192 and 1536 atoms, respectively.

**Analysis of scale separation.** We investigate how well MTS-Allegro separates scales by inspecting the time-smoothness and strength of the learned intentions. Figure 2 (b) shows the average relative frame-to-frame force difference (defined as \(\mathbb{E}_{t}\left[\left\|\bm{F}_{t+\Delta\tau}-\bm{F}_{t}\right\|/\left\|\bm{F }_{t}\right\|\right]\) for each component) in \(0.5\) fs time step Velocity Verlet MD for different models. Compared to the conventional Allegro model and the inner model, the outer model of MTS-Allegro outputs forces with a much lower \(\mathbb{E}_{t}\left[\left\|\bm{F}_{t+\Delta\tau}-\bm{F}_{t}\right\|/\left\| \bm{F}_{t}\right\|\right]\) for all \(\Delta\tau\), indicating that it learns interactions that change much more slowly in time. Figure 2 (c) shows the average force norm (defined as \(\mathbb{E}_{t}\left[\left\|\bm{F}_{t}\right\|\right]\)) of different models. The outer model learns interactions that are much weaker in magnitude than Allegro and the inner model of MTS-Allegro. Weak and slow-varying interactions are exactly what allows for a longer time step. This analysis confirms the effectiveness of MTS-Allegro in learning scale separation.

Figure 1: (a-c) O-O, H-H, and H-O RDFs of NVT simulations. (d) MSD of NVT simulations. (e) Total energy drift of NVE simulations. In the legend, MTS-Allegro is shortened for “MTS” along with the outer-inner time step ratio.

Figure 2: (a) Relative speedup factor compared to Allegro (1x in the plot). (b) Average relative frame-to-frame force difference in \(0.5\) fs time step Velocity Verlet MD for different models. (c) Average norm of forces for different models.

Conclusion

We have developed a method to accelerate MLIP-driven MD simulations by learning a scale separation and using an MTS integrator. In an ab initio water system, our approach achieves around three times speedup without loss in accuracy on the potential energy or simulation-derived quantities. MTS integrators can also be used with more than two levels, and learning finer-grained scale separations with more than two MLIPs is a direction for future work. It is also possible to co-train different model architectures, such as kernel-based methods [18] and message-passing MLIPs [36], for accuracy-speed trade-offs. The presented technique promises a direction for significant practical speed gains when running MLIP-driven MD, including in large and complex systems.

## Acknowledgments and Disclosure of Funding

We thank Simon Batzner, Cameron Owen, Yilun Xu, and Wujie Wang for valuable feedback and insightful discussions. X. F. and T. J. acknowledge support from the MIT-GIST collaboration. The work at Harvard was supported by Bosch Research, DOE Office of Basic Energy Sciences Award No. DE-SC0022199 and the Harvard University Materials Research Science and Engineering Center Grant No. DMR-2011754. A. M. was supported by DOE, Scientific Computing Research, Computational Science Graduate Fellowship under Award Number DE-SC0021110. Computing resources were provided by the MIT SuperCloud and Lincoln Laboratory Supercomputing Center and the Harvard University FAS Division of Science Research Computing Group.

## References

* [1] William L Jorgensen, David S Maxwell, and Julian Tirado-Rives. Development and testing of the opls all-atom force field on conformational energetics and properties of organic liquids. _Journal of the American Chemical Society_, 118(45):11225-11236, 1996.
* [2] William L Jorgensen and Julian Tirado-Rives. Potential energy functions for atomic-level simulations of water and organic and biomolecular systems. _Proceedings of the National Academy of Sciences_, 102(19):6665-6670, 2005.
* [3] Helmut Grubmuller, Helmut Heller, Andreas Windemuth, and Klaus Schulten. Generalized verlet algorithm for efficient molecular dynamics simulations with long-range interactions. _Molecular Simulation_, 6(1-3):121-142, 1991.
* [4] Mark E Tuckerman and Bruce J Berne. Molecular dynamics in systems with multiple time scales: Systems with stiff and soft degrees of freedom and with short and long range forces. _The Journal of chemical physics_, 95(11):8362-8364, 1991.
* [5] MBBJM Tuckerman, Bruce J Berne, and Glenn J Martyna. Reversible multiple time scale molecular dynamics. _The Journal of chemical physics_, 97(3):1990-2001, 1992.
* [6] Bosco Garcia-Archilla, Jesus Maria Sanz-Serna, and Robert D Skeel. Long-time-step methods for oscillatory differential equations. _SIAM Journal on Scientific Computing_, 20(3):930-963, 1998.
* [7] Jesus A Izaguirre, Daniel P Catarello, Justin M Wozniak, and Robert D Skeel. Langevin stabilization of molecular dynamics. _The Journal of chemical physics_, 114(5):2090-2098, 2001.
* [8] Darryl D Humphreys, Richard A Friesner, and Bruce J Berne. A multiple-time-step molecular dynamics algorithm for macromolecules. _The Journal of Physical Chemistry_, 98(27):6885-6892, 1994.
* [9] Laxmikant Kale, Robert Skeel, Milind Bhandarkar, Robert Brunner, Attila Gursoy, Neal Krawetz, James Phillips, Aritomo Shinozaki, Krishnan Varadarajan, and Klaus Schulten. Namd2: greater scalability for parallel molecular dynamics. _Journal of Computational Physics_, 151(1):283-312, 1999.
* [10] Kevin J Bowers, Edmond Chow, Huafeng Xu, Ron O Dror, Michael P Eastwood, Brent A Gregersen, John L Klepeis, Istvan Kolossvary, Mark A Moraes, Federico D Sacerdoti, et al. Scalable algorithms for molecular dynamics simulations on commodity clusters. In _Proceedings of the 2006 ACM/IEEE Conference on Supercomputing_, pages 84-es, 2006.
* [11] David A Case, H Metin Aktulga, Kellon Belfon, Ido Ben-Shalom, Scott R Brozell, David S Cerutti, Thomas E Cheatham III, Vinicius Wilian D Cruzeiro, Tom A Darden, Robert E Duke, et al. _Amber 2021_. University of California, San Francisco, 2021.
* a flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales. _Comp. Phys. Comm._, 271:108171, 2022.
* [13] Nathan Luehr, Thomas E Markland, and Todd J Martinez. Multiple time step integrators in ab initio molecular dynamics. _The Journal of chemical physics_, 140(8), 2014.
* [14] Elisa Liberatore, Rocco Meli, and Ursula Rothlisberger. A versatile multiple time step scheme for efficient ab initio molecular dynamics simulations. _Journal of chemical theory and computation_, 14(6):2834-2842, 2018.
* [15] Jorg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional potential-energy surfaces. _Physical review letters_, 98(14):146401, 2007.
* [16] Alireza Khorshidi and Andrew A Peterson. Amp: A modular approach to machine learning in atomistic simulations. _Computer Physics Communications_, 207:310-324, 2016.

* Smith et al. [2017] Justin S Smith, Olexandr Isayev, and Adrian E Roitberg. Ani-1: an extensible neural network potential with dft accuracy at force field computational cost. _Chemical science_, 8(4):3192-3203, 2017.
* Chmiela et al. [2017] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schutt, and Klaus-Robert Muller. Machine learning of accurate energy-conserving molecular force fields. _Science advances_, 3(5):e1603015, 2017.
* Artrith et al. [2017] Nongnuch Artrith, Alexander Urban, and Gerbrand Ceder. Efficient and accurate machine-learning interpolation of atomic energies in compositions with many species. _Physical Review B_, 96(1):014112, 2017.
* Unke and Meuwly [2018] Oliver T Unke and Markus Meuwly. A reactive, scalable, and transferable model for molecular energies from a neural network approach based on local information. _The Journal of chemical physics_, 148(24):241708, 2018.
* Zhang et al. [2018] Linfeng Zhang, Jiequn Han, Han Wang, Wissam Saidi, Roberto Car, et al. End-to-end symmetry preserving inter-atomic potential energy model for finite and extended systems. _Advances in Neural Information Processing Systems_, 31, 2018.
* Zhang et al. [2018] Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and EJPRL Weinan. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. _Physical review letters_, 120(14):143001, 2018.
* Zubatyuk et al. [2019] Roman Zubatyuk, Justin S Smith, Jerzy Leszczynski, and Olexandr Isayev. Accurate and transferable multitask prediction of chemical properties with an atoms-in-molecules neural network. _Science advances_, 5(8):eaav6490, 2019.
* Kovacs et al. [2021] David Peter Kovacs, Cas van der Oord, Jiri Kucera, Alice EA Allen, Daniel J Cole, Christoph Ortner, and Gabor Csanyi. Linear atomic cluster expansion force fields for organic molecules: beyond rmse. _Journal of chemical theory and computation_, 17(12):7696-7711, 2021.
* Tholke and De Fabritiis [2021] Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In _International Conference on Learning Representations_, 2021.
* Takamoto et al. [2022] So Takamoto, Chikashi Shinagawa, Daisuke Motoki, Kosuke Nakago, Wenwen Li, Iori Kurata, Taku Watanabe, Yoshihiro Yayama, Hiroki Iriguchi, Yusuke Asano, et al. Towards universal neural network potential for material discovery applicable to arbitrary combination of 45 elements. _Nature Communications_, 13(1):1-11, 2022.
* Gilmer et al. [2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* Schutt et al. [2017] Kristof Schutt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Muller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. _Advances in neural information processing systems_, 30, 2017.
* Smith et al. [2020] Justin S Smith, Roman Zubatyuk, Benjamin Nebgen, Nicholas Lubbers, Kipton Barros, Adrian E Roitberg, Olexandr Isayev, and Sergei Tretiak. The ani-1ccx and ani-1x data sets, coupled-cluster and density functional theory properties for molecules. _Scientific data_, 7(1):134, 2020.
* Devereux et al. [2020] Christian Devereux, Justin S Smith, Kate K Huddeston, Kipton Barros, Roman Zubatyuk, Olexandr Isayev, and Adrian E Roitberg. Extending the applicability of the ani deep learning molecular potential to sulfur and halogens. _Journal of Chemical Theory and Computation_, 16(7):4192-4202, 2020.
* Gasteiger et al. [2020] Johannes Gasteiger, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. In _International Conference on Learning Representations_, 2020.
* Gasteiger et al. [2021] Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. _Advances in Neural Information Processing Systems_, 34:6790-6802, 2021.

* [33] Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d molecular graphs. In _International Conference on Learning Representations_, 2021.
* [34] Oliver T Unke, Stefan Chmiela, Michael Gastegger, Kristof T Schutt, Huziel E Sauceda, and Klaus-Robert Muller. Spookynet: Learning force fields with electronic degrees of freedom and nonlocal effects. _Nature communications_, 12(1):1-14, 2021.
* [35] Cheol Woo Park, Mordechai Kornbluth, Jonathan Vandermause, Chris Wolverton, Boris Kozinsky, and Jonathan P Mailoa. Accurate and scalable graph neural network force field and molecular dynamics with direct force architecture. _npj Computational Materials_, 7(1):1-9, 2021.
* [36] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. _Nature communications_, 13(1):1-11, 2022.
* [37] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. _arXiv preprint arXiv:2204.05249_, 2022.
* [38] Oliver T Unke, Stefan Chmiela, Huziel E Sauceda, Michael Gastegger, Igor Poltavsky, Kristof T Schutt, Alexandre Tkatchenko, and Klaus-Robert Muller. Machine learning force fields. _Chemical Reviews_, 121(16):10142-10186, 2021.
* [39] Chi Chen and Shyue Ping Ong. A universal graph deep learning interatomic potential for the periodic table. _Nature Computational Science_, 2(11):718-728, 2022.
* [40] Bowen Deng, Peichen Zhong, KyuJung Jun, Janosh Riebesell, Kevin Han, Christopher J Bartel, and Gerbrand Ceder. Chgnet as a pretrained universal neural network potential for charge-informed atomistic modelling. _Nature Machine Intelligence_, pages 1-11, 2023.
* [41] Ruggero Lot, Franco Pellegrini, Yusuf Shaidu, and Emine Kucukbenli. Panna: Properties from artificial neural network architectures. _Computer Physics Communications_, 256:107402, 2020.
* [42] Xiang Fu, Zhenghao Wu, Wujie Wang, Tian Xie, Sinan Keten, Rafael Gomez-Bombarelli, and Tommi S. Jaakkola. Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations. _Transactions on Machine Learning Research_, 2023. Survey Certification.
* [43] Philipp Tholke and Gianni De Fabritiis. Torchmd-net: equivariant transformers for neural network based molecular potentials. _arXiv preprint arXiv:2202.02541_, 2022.
* [44] Samuel Schoenholz and Ekin Dogus Cubuk. Jax md: a framework for differentiable physics. _Advances in Neural Information Processing Systems_, 33:11428-11441, 2020.
* [45] Stefan Doerr, Maciej Majewski, Adria Perez, Andreas Kramer, Cecilia Clementi, Frank Noe, Toni Giorgino, and Gianni De Fabritiis. Torchmd: A deep learning framework for molecular simulations. _Journal of chemical theory and computation_, 17(4):2355-2363, 2021.
* [46] Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gabor Csanyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. _Advances in Neural Information Processing Systems_, 35:11423-11436, 2022.
* [47] Hikaru Ibayashi, Taufeq Mohammed Razakh, Liqiu Yang, Thomas Linker, Marco Olguin, Shinnosuke Hattori, Ye Luo, Rajiv K Kalia, Aiichiro Nakano, Ken-ichi Nomura, et al. Algebra-legato: Scalable, fast, and robust neural-network quantum molecular dynamics via sharpness-aware minimization. In _International Conference on High Performance Computing_, pages 223-239. Springer, 2023.
* [48] Arthur Kosmala, Johannes Gasteiger, Nicholas Gao, and Stephan Gunnemann. Ewald-based long-range message passing for molecular graphs. _arXiv preprint arXiv:2303.04791_, 2023.

* [49] Theo Jaffreplot Inizan, Thomas Ple, Olivier Adjoua, Pengyu Ren, Hatice Gokcan, Olexandr Isayev, Louis Lagardere, and Jean-Philip Piquemal. Scalable hybrid deep neural networks/polarizable potentials biomolecular simulations including long-range effects. _Chemical Science_, 14(20):5438-5452, 2023.
* [50] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. _arXiv preprint arXiv:2207.09453_, 2022.
* [51] Albert Musaelian, Anders Johansson, Simon Batzner, and Boris Kozinsky. Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size, 2023.
* [52] Steve Plimpton. Particle-mesh ewald and rrespa for parallel molecular dynamics simulations. In _PPSC_. Citeseer, 1997.
* [53] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* [54] Bingqing Cheng, Edgar A Engel, Jorg Behler, Christoph Dellago, and Michele Ceriotti. Ab initio thermodynamics of liquid and solid water. _Proceedings of the National Academy of Sciences_, 116(4):1110-1115, 2019.
* [55] Joost VandeVondele, Fawzi Mohamed, Matthias Krack, Jurg Hutter, Michiel Sprik, and Michele Parrinello. The influence of temperature and density functional models in ab initio molecular dynamics simulation of liquid water. _The Journal of chemical physics_, 122(1), 2005.
* [56] Weile Jia, Han Wang, Mohan Chen, Denghui Lu, Lin Lin, Roberto Car, E Weinan, and Linfeng Zhang. Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning. In _SC20: International conference for high performance computing, networking, storage and analysis_, pages 1-14. IEEE, 2020.
* [57] Mark Abraham, Andrey Alekseenko, Cathrine Bergh, Christian Blau, Eliane Briand, Mahesh Doijade, Stefan Fleischmann, Vytautas Gapsys, Gaurav Garg, Sergey Gorelov, Gilles Gouailardet, Alan Gray, M. Eric Irrgang, Farzaneh Jalalypour, Joe Jordan, Christoph Junghans, Prashanth Kanduri, Sebastian Keller, Carsten Kutzner, Justin A. Lemkul, Magnus Lundborg, Pascal Merz, Vedran Miletic, Dmitry Morozov, Szilard Pall, Roland Schulz, Michael Shirts, Alexey Shvetsov, Balint Soproni, David van der Spoel, Philip Turner, Carsten Uphoff, Alessandra Villa, Sebastian Wingbermuhle, Artem Zhmurov, Paul Bauer, Berk Hess, and Erik Lindahl. Gromacs 2023.2 manual, July 2023.

[MISSING_PAGE_EMPTY:11]