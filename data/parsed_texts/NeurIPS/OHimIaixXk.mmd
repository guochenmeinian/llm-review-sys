# Building the Bridge of Schrodinger:

A Continuous Entropic Optimal Transport Benchmark

 Nikita Gushchin

Skoltech

Moscow, Russia

n.gushchin@skoltech.ru

&Alexander Kolesov

Skoltech1

Moscow, Russia

a.kolesov@skoltech.ru

&Petr Mokrov

Skoltech1

Moscow, Russia

petr.mokrov@skoltech.ru

Polina Karpikova

Skoltech1

Moscow, Russia

polina.karpikova@skoltech.ru

&Andrey Spiridonov

Skoltech1

Moscow, Russia

andrew.spiridonov@skoltech.ru

&Evgeny Burnaev

Skoltech1

AIRI2

Moscow, Russia

e.burnaev@skoltech.ru

&Alexander Korotin

Skoltech1

AIRI2

Moscow, Russia

a.korotin@skoltech.ru

Footnote 1: Skoltech1

Footnote 2: Artificial Intelligence Research Institute

###### Abstract

Over the last several years, there has been significant progress in developing neural solvers for the Schrodinger Bridge (SB) problem and applying them to generative modelling. This new research field is justifiably fruitful as it is interconnected with the practically well-performing diffusion models and theoretically grounded entropic optimal transport (EOT). Still, the area lacks non-trivial tests allowing a researcher to understand how well the methods solve SB or its equivalent continuous EOT problem. We fill this gap and propose a novel way to create pairs of probability distributions for which the ground truth OT solution is known by the construction. Our methodology is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB (the main interest of our study). This development allows us to create continuous benchmark distributions with the known EOT and SB solutions on high-dimensional spaces such as spaces of images. As an illustration, we use these benchmark pairs to test how well existing neural EOT/SB solvers actually compute the EOT solution. Our code for constructing benchmark pairs under different setups is available at:

https://github.com/ngushchin/EntropicOTBenchmark

Diffusion models are a powerful tool to solve image synthesis [25; 45] and image-to-image translation [50; 47] tasks. Still, they suffer from the time-consuming inference which requires modeling thousands of diffusion steps. Recently, the **Schrodinger Bridge** (SB) has arisen as a promising framework to cope with this issue [15; 9; 54]. Informally, SB is a special diffusion which has rather _straight trajectories_ and _finite time horizon_. Thus, it may require fewer discretization steps to infer the diffusion.

In addition to promising practical features, SB is known to have good and well-studied theoretical properties. Namely, it is _equivalent_[38] to the **Entropic Optimal Transport** problem (EOT, [13, 20]) about moving the mass of one probability distribution to the other in the most efficient way. This problem has gained a genuine interest in the machine learning community thanks to its nice sample complexity properties, convenient dual form and a wide range of applications [28, 6, 44].

Expectiedly, recent **neural EOT/SB** solvers start showing promising performance in various tasks [15, 52, 9, 14, 23, 42]. However, it remains unclear to which extent this success is actually attributed to the fact that these methods properly solve EOT/SB problem rather than to a good choice of parameterization, regularization, tricks, etc. This ambiguity exists because of the **lack of ways to evaluate the performance of solvers qualitatively** in solving EOT/SB. Specifically, the class of continuous distributions with the analytically known EOT/SB solution is narrow (Gaussians [10, 41, 26, 7]) and these solutions have been obtained only recently. Hence, although papers in the field of neural EOT/SB frequently appear, we never know how well they actually solve EOT/SB.

**Contributions**. We develop a generic methodology for evaluating continuous EOT/SB solvers.

1. We propose a generic method to create continuous pairs of probability distributions with analytically known (by our construction) EOT solution between them (SS3.1, SS3.2).
2. We use log-sum-exp of quadratic functions (SS3.3, SS3.4) to construct pairs of distributions (SS4) that we use as a benchmark with analytically-known EOT/SB solution for the quadratic cost.
3. We use these **benchmark pairs** to evaluate (SS5) many popular neural EOT/SB solvers (SS2) in high-dimensional spaces, including the space of \(64\times 64\) celebrity faces.

In the field of neural OT, there already exist several _benchmarks_ for the Wasserstein-2 [32], the Wasserstein-1 [31] and the Wasserstein-2 barycenter [31] OT tasks. Their benchmark construction methodologies work **only** for specific OT formulations and **do not** generalize to EOT which we study.

## 1 Background: Optimal Transport and Schrodinger Bridges Theory

We work in Euclidean space \(\mathcal{X}=\mathcal{Y}=\mathbb{R}^{D}\) equipped with the standard Euclidean norm \(\|\cdot\|\). We use \(\mathcal{P}(\mathcal{X})=\mathcal{P}(\mathcal{Y})=\mathcal{P}(\mathbb{R}^{D})\) to denote the sets of Borel probability distributions on \(\mathcal{X},\mathcal{Y}\), respectively.

**Classic (Kantorovich) OT formulation**[27, 53, 48]. For two distributions \(\mathbb{P}_{0}\in\mathcal{P}(\mathcal{X})\), \(\mathbb{P}_{1}\in\mathcal{P}(\mathcal{Y})\) and a cost function \(c:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\), consider the following problem (Fig. 0(a)):

\[\text{OT}_{c}(\mathbb{P}_{0},\mathbb{P}_{1})\stackrel{{\text{ def}}}{{=}}\inf_{\pi\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})}\int_{\mathcal{X}\times \mathcal{Y}}c(x,y)d\pi(x,y),\] (1)

where the optimization is performed over the set \(\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\) of transport plans, i.e., joint distributions on \(\mathcal{X}\times\mathcal{Y}\) with marginals \(\mathbb{P}_{0}\), \(\mathbb{P}_{1}\), respectively. The set \(\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\) is non-empty as it always contains the trivial plan \(\mathbb{P}_{0}\times\mathbb{P}_{1}\). With mild assumptions, a minimizer \(\pi^{*}\) of (1) exists and is called an _OT plan_. Typical examples of \(c\) are powers of Euclidean norms, i.e., \(c(x,y)=\frac{1}{q}\|x-y\|^{q}\), \(q\geq 1\).

**Weak OT formulation**[22, 2, 3]. Let \(C:\mathcal{X}\times\mathcal{P}(\mathcal{Y})\to\mathbb{R}\cup\{+\infty\}\) be a weak cost which takes a point \(x\in\mathcal{X}\) and a distribution of \(y\in\mathcal{Y}\) as inputs. The weak OT cost between \(\mathbb{P}_{0}\), \(\mathbb{P}_{1}\) is (Fig. 0(b))

\[\text{WOT}_{C}(\mathbb{P}_{0},\mathbb{P}_{1})\stackrel{{\text{ def}}}{{=}}\inf_{\pi\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})}\int_{\mathcal{X}}C(x,\pi( \cdot|x))d\pi_{0}(x)=\inf_{\pi\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})}\int_{ \mathcal{X}}C(x,\pi(\cdot|x))d\mathbb{P}_{0}(x),\] (2)

where \(\pi(\cdot|x)\) denotes the conditional distribution of \(y\in\mathcal{Y}\) given \(x\in\mathcal{X}\) and \(\pi_{0}\) is the projection of \(\pi\) to \(\mathcal{X}\) which equals \(\mathbb{P}_{0}\) since \(\pi\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\). Weak OT formulation (2) generalizes classic OT (1): it suffices to pick \(C\big{(}x,\pi(\cdot|x)\big{)}=\int_{\mathcal{Y}}c(x,y)d\pi(y|x)\) to obtain (1) from (2). A more general case of a

Figure 1: Classic (Kantorovich’s) and weak OT formulations.

weak cost is \(C\big{(}x,\pi(\cdot|x)\big{)}=\int_{\mathcal{Y}}c(x,y)d\pi(y|x)+\epsilon\mathcal{R }\big{(}\pi(\cdot|x)\big{)},\) where \(\epsilon>0\) and \(\mathcal{R}:\mathcal{P}(\mathcal{Y})\to\mathbb{R}\) is some functional (a.k.a. regularizer), e.g., variance [3, 35], kernel variance [34] or entropy [3]. With mild assumptions on the weak cost function \(C\), an OT plan \(\pi^{*}\) in (2) exists. We say that the family of its conditional distributions \(\{\pi^{*}(\cdot|x)\}_{x\in\mathcal{X}}\) is the _conditional OT plan_.

**Entropic OT formulation**[13, 20]. It is common to consider entropy-based regularizers for (1):

\[\left\{\begin{aligned} &\text{EOT}^{(1)}_{c,c}(\mathbb{P}_{0}, \mathbb{P}_{1})\\ &\text{EOT}^{(2)}_{c,c}(\mathbb{P}_{0},\mathbb{P}_{1})\\ &\text{EOT}_{c,c}(\mathbb{P}_{0},\mathbb{P}_{1})\end{aligned} \right.\quad\stackrel{{\text{def}}}{{=}}\min_{\pi\in\Pi(\mathbb{P }_{0},\mathbb{P}_{1})}\int_{\mathcal{X}\times\mathcal{Y}}c(x,y)\pi(x,y)+ \left\{\begin{aligned} &+\epsilon\text{KL}\left(\pi\|\mathbb{P}_{0} \times\mathbb{P}_{1}\right),\\ &-\epsilon H(\pi),\\ &-\epsilon\int_{\mathcal{X}}\!H\big{(}\pi(\cdot|x)\big{)}d \mathbb{P}_{0}(x).\end{aligned}\right.\] (3)

Here KL is the Kullback-Leibler divergence and \(H\) is the differential entropy, i.e., the minus KL divergence with the Lebesgue measure. Since \(\pi\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\), it holds that \(\text{KL}\left(\pi\|\mathbb{P}_{0}\times\mathbb{P}_{1}\right)=H(\mathbb{P}_{0 })-\int_{\mathcal{X}}H\big{(}\pi(y|x)\big{)}d\mathbb{P}_{0}(x)=-H(\pi)+H( \mathbb{P}_{0})+H(\mathbb{P}_{1})\), i.e., these formulations are equal up to an additive constant when \(\mathbb{P}_{0}\in\mathcal{P}_{ac}(\mathcal{X})\) and \(\mathbb{P}_{1}\in\mathcal{P}_{ac}(\mathcal{Y})\) and have finite entropy. Here we introduce "\(ac\)" subscript to indicate the subset of absolutely continuous distributions. With mild assumptions on \(c,\mathbb{P}_{0},\mathbb{P}_{1}\), the minimizer \(\pi^{*}\) exists, it is **unique** and called the entropic OT plan. It is important to note that _entropic OT_ (5) _is a case of weak OT_ (2). Indeed, for the weak cost

\[C_{c,\epsilon}(x,\pi(\cdot|x))\stackrel{{\text{def}}}{{=}}\int_{ \mathcal{Y}}c(x,y)d\pi(y|x)-\epsilon H\big{(}\pi(\cdot|x)\big{)},\] (4)

formulation (2) immediately turns to (5). This allows us to apply the theory of weak OT to EOT.

**Dual OT formulation.** There exists a wide range of dual formulations of OT [53, 48], WOT [2, 22] and EOT [20, 44]. We only recall the particular dual form for WOT from [3, 2] which serves as the main theoretical ingredient for our paper. For technical reasons, from now on we consider only \(\mathbb{P}_{1}\in\mathcal{P}_{p}(\mathcal{Y})\subset\mathcal{P}(\mathcal{Y})\) for some \(p\geq 1\), where subscript "\(p\)" indicates distributions with a finite \(p\)-th moment. We also assume that the weak cost \(C:\mathcal{X}\times\mathcal{P}_{p}(\mathcal{Y})\to\mathbb{R}\cup\{+\infty\}\) is lower bounded, _convex_ in the second argument and jointly lower-semicontinuous in \(\mathcal{X}\times\mathcal{P}_{p}(\mathcal{Y})\). In this case, a minimizer \(\pi^{*}\) of WOT (2) exists [3, Theorem 3.2] and the following dual formulation holds [3, Eq. 3.3]:

\[\text{WOT}_{C}(\mathbb{P}_{0},\mathbb{P}_{1})=\sup_{f}\bigg{\{}\int_{\mathcal{ X}}f^{C}(x)d\mathbb{P}_{0}(x)+\int_{\mathcal{Y}}f(y)d\mathbb{P}_{1}(y) \bigg{\}},\] (5)

where \(f\in\mathcal{C}_{p}(\mathcal{Y})\stackrel{{\text{def}}}{{=}}\{f: \mathcal{Y}\to\mathbb{R}\) continuous s.t. \(\exists\alpha,\beta\in\mathbb{R}:\;|f(\cdot)|\leq\alpha\|\cdot\|^{p}+\beta\}\) and \(f^{C}\) is the so-called _weak \(C\)-transform_ of \(f\) which is defined by

\[f^{C}(x)\stackrel{{\text{def}}}{{=}}\inf_{\nu\in\mathcal{P}_{p}( \mathcal{Y})}\{C(x,\nu)-\int_{\mathcal{Y}}f(y)d\nu(y)\}.\] (6)

Function \(f\) in (5) is typically called the dual variable or the _Kantorovich potential_.

**SB problem with Wiener prior**[38, 11]. Let \(\Omega\) be the space of \(\mathbb{R}^{D}\)-valued functions of time \(t\in[0,1]\) describing trajectories in \(\mathbb{R}^{D}\), which start at time \(t=0\) and end at time \(t=1\). We use \(\mathcal{P}(\Omega)\) to denote the set of probability distributions on \(\Omega\), i.e., stochastic processes.

Consider two distributions \(\mathbb{P}_{0}\in\mathcal{P}_{2,ac}(\mathcal{X})\) and \(\mathbb{P}_{1}\in\mathcal{P}_{2,ac}(\mathcal{Y})\) with finite entropy. Let \(\mathcal{F}(\mathbb{P}_{0},\mathbb{P}_{1})\subset\mathcal{P}(\Omega)\) be the subset of processes which have marginals \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) at times \(t=0\) and \(t=1\), respectively. Let \(dW_{t}\) be the differential of the standard \(\mathbb{R}^{D}\)-valued Wiener process. Let \(W^{\epsilon}\in\mathcal{P}(\Omega)\) be the Wiener process with the variance \(\epsilon>0\) which starts at \(\mathbb{P}_{0}\) at time \(t=0\). It can be represented via the following stochastic differential equation (SDE): \(dX_{t}=\sqrt{\epsilon}dW_{t}\) with \(X_{0}\sim\mathbb{P}_{0}\).

The Schrodinger Bridge problem with the **Wiener prior** is the following:

\[\inf_{T\in\mathcal{F}(\mathbb{P}_{0},\mathbb{P}_{1})}\text{KL}\left(T\|W^{ \epsilon}\right).\] (7)

The \(\inf\) is attained uniquely at some process \(T^{*}\)[38, Proposition 4.1]. This process turns out to be a **diffusion** process and can be (uniquely) represented as the following SDE:

\[T^{*} : dX_{t}=v^{*}(X_{t},t)dt+\sqrt{\epsilon}dW_{t},\] (8)

Figure 2: The bridge of Schrödinger.

where \(v^{*}:\mathbb{R}^{D}\times[0,1]\to\mathbb{R}^{D}\) is its drift function which we call the _optimal drift_. Hence, in (9), one may consider only diffusion processes \(\subset\mathcal{F}(\mathbb{P}_{0},\mathbb{P}_{1})\) with the volatility \(\epsilon\) coinciding with the volatility of the Wiener prior \(W^{\epsilon}\). In turn, solving SB can be viewed as finding the optimal drift \(v^{*}\).

**Link between SB and EOT problem.** The process \(T^{*}\) solving SB (9) is related to the solution \(\pi^{*}\) of EOT problem (5) _with the quadratic cost function \(c(x,y)=\frac{1}{2}\|x-y\|^{2}\)_. We start with some notations. For a process \(T\in\mathcal{P}(\Omega)\), denote the joint distribution at time moments \(t=0,1\) by \(\pi^{T}\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})\). Let \(T_{|x,y}\) be the distribution of \(T\) for \(t\in(0,1)\) conditioned on \(T\)'s values \(x,y\) at \(t=0,1\).

For the solution \(T^{*}\) of SB (9), it holds that \(\pi^{T^{*}}=\pi^{*}\), where \(\pi^{*}\) is the EOT plan solving (5).

Moreover, \(T^{*}_{|x,y}=W^{\epsilon}_{|x,y}\), i.e., informally, the _"inner"_ part of \(T^{*}\) matches that of the prior \(W^{\epsilon}\).

Conditional process \(W^{\epsilon}_{|x,y}\) is well-known as the **Brownian Bridge**. Due to this, given \(x,y\), simulating the trajectories of \(W^{\epsilon}_{|x,y}\) is rather straightforward. Thanks to this aspect, SB and EOT can be treated as _nearly_ equivalent problems. Still EOT solution \(\pi^{*}\) does not directly yield the optimal drift \(v^{*}\). However, it is known that the density \(\frac{d\pi^{*}(x,y)}{d(x,y)}\) of \(\pi^{*}\) has the specific form [38, Theorem 2.8], namely, \(\frac{d\pi^{*}(x,y)}{d(x,y)}=\widetilde{\varphi}^{*}(x)\mathcal{N}(y|x, \epsilon I)\varphi^{*}(y)\), where functions \(\varphi^{*},\widetilde{\varphi}^{*}:\mathbb{R}^{D}\to\mathbb{R}\) are called the _Schrodinger potential_. From this equality one gets the expression for \(\varphi^{*}(\cdot)\) and the density of \(\pi^{*}(\cdot|x)\):

\[\frac{d\pi^{*}(y|x)}{dy}\propto\mathcal{N}(y|x,\epsilon I)\varphi^{*}(y) \qquad\Longrightarrow\qquad\varphi^{*}(y)\propto\frac{d\pi^{*}(y|x)}{dy}\cdot \big{[}\mathcal{N}(y|x,\epsilon I)\big{]}^{-1}\] (11)

up to multiplicative constants. One may recover the optimal drift \(v^{*}\) via [38, Proposition 4.1]

\[v^{*}(x,t)=\epsilon\nabla\log\int_{\mathbb{R}^{D}}\mathcal{N}(y|x,(1-t) \epsilon I_{D})\varphi^{*}(y)dy.\] (12)

Here the normalization constant vanishes when one computes \(\nabla\log(\cdot)\). Thus, technically, knowing the (unnormalized) density of \(\pi^{*}\), one may recover the optimal drift \(v^{*}\) for SB (9).

## 2 Background: Solving Continuous OT and SB Problems

Although OT (2), EOT (5) and SB (9) problems are well-studied in theory, solving them in practice is challenging. Existing OT solvers are of two main types: _discrete_[44] and _continuous_[32]. **Our benchmark is designed for continuous EOT solvers**; discrete OT/EOT is out of the scope of the paper.

Continuous OT assumes that distributions \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) are continuous and accessible only via their random samples \(X=\{x_{1},\ldots,x_{N}\}\sim\mathbb{P}_{0}\) and \(Y=\{y_{1},\ldots,y_{M}\}\sim\mathbb{P}_{1}\). The goal is to recover an OT plan \(\pi^{*}\) between _entire_\(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) but using only \(X\) and \(Y\). Most continuous OT solvers do this via employing neural networks to implicitly learn the conditional distributions \(\widehat{\pi}(\cdot|x)\approx\pi^{*}(\cdot|x)\). In turn, SB solvers learn the optimal drift \(\widehat{v}\approx v^{*}\) but it is anyway used to produce samples \(y\sim\widehat{\pi}(\cdot|x)\) via solving SDE \(dX_{t}=\widehat{v}(x,t)dt+\sqrt{\epsilon}dW_{t}\) starting from \(X_{0}=x\) (sampled from \(\mathbb{P}_{0}\)) at time \(t=0\).

After training on available samples \(X\) and \(Y\), continuous solvers may produce \(y\sim\widehat{\pi}(\cdot|x_{\text{test}})\) for previously unseen samples \(x_{\text{test}}\sim\mathbb{P}_{0}\). This is usually called the out-of-sample estimation. It allows applying continuous OT solver to generative modelling problems such as the **image synthesis** (_noise-to-data_) and **translation** (_data-to-data_). In both these cases, \(\mathbb{P}_{1}\) is a data distribution, and \(\mathbb{P}_{0}\) is either a noise (in synthesis) or some other data distribution (in translation). Many recent OT solvers achieve competitive performance in synthesis [12; 15; 46] and translation [35; 34] tasks.

Continuous OT/SB solvers are usually referred to as **neural OT/SB** because they employ neural networks. There exist a lot of neural OT solvers for classic OT (1) [46; 17; 56; 40; 30; 19], see also [32; 31] for **surveys**, weak OT (2) [34; 35; 1], entropic OT (5) [49; 14; 42] and SB (9) [52; 15; 9; 23]. Providing a concise but still explanatory overview of them is nearly impossible as the _underlying principles of many of them are rather different and non-trivial_. We list only EOT/SB solvers which are relevant to our benchmark and provide a brief summary of them in Table 1. In SS5, we test all these solvers on our continuous benchmark distributions which we construct in subsequent SS3.

**Approaches to evaluate solvers.** As seen from Table 1, each paper usually tests its solver on a restricted set of examples which rarely intersects with those from the other papers. In particular, some papers consider _data\(\rightarrow\)data_ tasks, while the others focus on _noise\(\rightarrow\)data_. Due to this, there is no clear understanding of the superiority of one solver over the other. Importantly, in many cases, the _quantitative_ evaluation is done exclusively via the metrics of the downstream task. For example, [15, 9, 23, 14] consider image generation or translation tasks and test the quality of generated images via FID [24]. That is, they compare generated _marginal_ distribution \(\widehat{\pi}_{1}\) with target \(\mathbb{P}_{1}\). This allows to access the generative performance of solvers but gives **no hint whether they actually learn the true EOT/SB solution**. Works [14, 23, 42, 15] do a step toward opening the veil of secrecy and test their solvers in the Gaussian case. Unfortunately, it is rather trivial and may not be representative.

## 3 Constructing Benchmark Pairs for OT and SB: Theory

In this section, we present our theoretical results allowing us to construct pairs of distributions with the EOT/SB solution known by the construction. We provide proofs in Appendix A.

### Generic Optimal Transport Benchmark Idea

For a given distribution \(\mathbb{P}_{0}\in\mathcal{P}(\mathcal{X})\), we want to construct a distribution \(\mathbb{P}_{1}\in\mathcal{P}_{p}(\mathcal{Y})\) such that some OT plan \(\pi^{*}\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\) for a given weak OT cost function \(C\) between them is known by the construction. That is, \(\pi^{*}_{0}=\mathbb{P}_{0}\), \(\pi^{*}_{1}=\mathbb{P}_{1}\) and \(\pi^{*}\) minimizes (2). In this case, \((\mathbb{P}_{0},\mathbb{P}_{1})\) may be used as a **benchmark pair** with a known OT solution. Our following main theorem provides a way to do so.

**Theorem 3.1** (Optimal transport benchmark constructor).: _Let \(\mathbb{P}_{0}\in\mathcal{P}(\mathcal{X})\) be a given distribution, \(f^{*}\in\mathcal{C}_{p}(\mathcal{Y})\) be a given function and \(C:\mathcal{X}\times\mathcal{P}_{p}(\mathcal{Y})\to\mathbb{R}\) be a given jointly lower semi-continuous, convex in the second argument and lower bounded weak cost. Let \(\pi^{*}\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})\) be a distribution for which \(\pi^{*}_{0}=\mathbb{P}_{0}\) and for all \(x\in\mathcal{X}\) it holds that_

\[\pi^{*}(\cdot|x)\in\operatorname*{argmin}_{\mu\in\mathcal{P}_{p}(\mathcal{Y} )}\{C(x,\mu)-\int_{\mathcal{Y}}f^{*}(y)d\mu(y)\}.\] (13)

_Let \(\mathbb{P}_{1}\stackrel{{\text{def}}}{{=}}\pi^{*}_{1}\) be the second marginal of \(\pi^{*}\) and assume that \(\mathbb{P}_{1}\in\mathcal{P}_{p}(\mathcal{Y})\). Then \(\pi^{*}\) is an **OT plan** between \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) (it minimizes (2)) and \(f^{*}\) is an **optimal dual potential** (it maximizes (7))._

Thanks to our theorem, given a pair \((\mathbb{P}_{0},f^{*})\in\mathcal{P}(\mathcal{X})\times\mathcal{C}_{p}( \mathcal{Y})\) of a distribution and a potential, one may produce a distribution \(\mathbb{P}_{1}\) for which an OT plan between them is known by the construction. This may be done by picking \(\pi^{*}\in\Pi(\mathbb{P})\) whose conditionals \(\pi^{*}(\cdot|x)\) minimize (13).

While our theorem works for rather general costs \(C\), it may be non-trivial to compute a minimizer \(\pi^{*}(\cdot|x)\) in the weak \(C\)-transform (8), e.g., to sample from it or to estimate its density. Also, we note that our theorem states that \(\pi^{*}\) is optimal but does not claim that it is the unique OT plan. These aspects may complicate the usage of the theorem for constructing the benchmark pairs \((\mathbb{P}_{0},\mathbb{P}_{1})\) for general costs \(C\). Fortunately, both these issues vanish when we consider EOT, see below.

### Entropic Optimal Transport Benchmark Idea

For \(C=C_{c,\epsilon}\) (6) with \(\epsilon>0\), the characterization of minimizers \(\pi^{*}(\cdot|x)\) in (13) is almost explicit.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \hline  & **Solver** & \multicolumn{2}{c|}{**Underlying principle**} & **Evaluated as** & \begin{tabular}{c} **tested in generation** \\ _(noise -data)_ \\ \end{tabular} & \begin{tabular}{c} **Tested in translation** \\ _(data -data)_ \\ \end{tabular} \\ \hline \multirow{6}{*}{**EOT** solvers} & \multirow{3}{*}{LSOT [49, 21]} & Solves classic dual EOT [20, \(\widehat{3}\).1] with 2 NNs. & \multirow{3}{*}{\begin{tabular}{c} \(\mathcal{X}\) \\ \end{tabular} } & \multirow{3}{*}{MNIST (32x32)} & \multirow{3}{*}{\begin{tabular}{c} MNIST\(\cdot\)USPS(16x16), \\ USPS\(\to\)MNIST (16x16), \\ SVHN\(\to\)MNIST (3x32x32) \\ \end{tabular} } \\ \cline{2-2} \cline{6-6}  & \multirow{3}{*}{SCONES [14]} & Combines LSOT’s potentials with a score model for & \multirow{3}{*}{Gaussian} & \multirow{3}{*}{\begin{tabular}{c} \(\mathcal{X}\) \\ \end{tabular} } & & \multirow{3}{*}{\begin{tabular}{c} CelebA Upscale (**3x6x64**) \\ \end{tabular} } \\ \cline{2-2} \cline{6-6}  & & & & & & \\ \cline{2-2} \cline{6-6}  & \multirow{3}{*}{NOT* [35]} & Solves max-min reformulation of weak OT & \multirow{3}{*}{\begin{tabular}{c} \(\star\) This is a generic neural solver for weak OT \\ but it has **not** been tested with the **entropic** cost function. \\ \end{tabular} } \\ \cline{2-2} \cline{6-6}  & & dual (7) with 2 NNs (transport map and potential). & & & \multirow{3}{*}{\begin{tabular}{c} \(\star\) This is a generic neural solver for weak OT \\ but it has **not** been tested with the **entropic** cost function. \\ \end{tabular} } \\ \cline{2-2} \cline{6-6}  & \multirow{3}{*}{ESOOT [42]} & Employs energy-based modeling (EBM [37]) & & & & \\ \cline{2-2} \cline{6-6}  & & to solve weak EOT dual (7); non-minimax; 1NN. & & & & \\ \hline \multirow{6}{*}{**SB** solvers} & \multirow{3}{*}{ENOT [23]} & Solves max-min reformulation of SB with 2 NNs (potential and SDE drift). & \multirow{3}{*}{Gaussian} & \multirow{3}{*}{\begin{tabular}{c} CelebA Upscale (**3x6x64**), \\ Colored MNIST 2\(\to\)3 (3x32x32) \\ \end{tabular} } \\ \cline{2-2} \cline{6-6}  & \multirow{3}{*}{\begin{tabular}{c} Alletime solving of two half Bridle (HB) problems. \\ HB is solved via full estimation with GP [55]. \\ \end{tabular} } & & & & \\ \cline{1-1} \cline{2-2} \cline{6-6}  & \multirow{3}{*}{DiffSB [15]} & Iterative Mean-Matching Proportional Fitting & \multirow{3}{*}{\begin{tabular}{c} CelebA (32x32) \\ CelebA (32x32) \\ CIFAR-10 (3x32x32) \\ \end{tabular} } \\ \cline{1-1} \cline{2-2} \cline{6-6}  & & \multirow{3}{*}{FB-SDE [9]} & Likelihood training of SB & \multirow{3}{*}{
\begin{tabular}{c} MNIST (32x32), \\ CelebA (3x32x32), \\ CIFAR-10 (3x32x32) \\ \end{tabular} } \\ \cline{1-1} \cline{2-2} \cline{6-6}  & & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Table of existing continuous (neural) solvers for EOT/SB.

**Theorem 3.2** (Entropic optimal transport benchmark constructor).: _Let \(\mathbb{P}_{0}\in\mathcal{P}(\mathcal{X})\) be a given distribution and \(f^{*}\in\mathcal{C}_{p}(\mathcal{Y})\) be a given potential. Assume that \(c:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\) is lower bounded and \((x,\mu)\mapsto\int_{\mathcal{Y}}c(x,y)d\mu(y)\) is lower semi-continuous in \(\mathcal{X}\times\mathcal{P}_{p}(\mathcal{Y})\). Furthermore, assume that there exists \(M\in\mathbb{R}_{+}\) such that for all \(x\in\mathcal{X}\) it holds that \(M_{x}\stackrel{{\text{def}}}{{=}}\int_{\mathcal{Y}}\exp\big{(}- \frac{c(x,y)}{\epsilon}\big{)}dy\leq M\). Assume that for all \(x\in\mathcal{X}\) value \(Z_{x}\stackrel{{\text{def}}}{{=}}\int_{\mathcal{Y}}\exp\big{(} \frac{f^{*}(y)-c(x,y)}{\epsilon}\big{)}dy\) is finite. Consider the joint distribution \(\pi^{*}\in\mathcal{P}(\mathcal{X}\times\mathcal{Y})\) whose first marginal distribution satisfies \(\pi^{*}_{0}=\mathbb{P}_{0}\) and for all \(x\in\mathcal{X}\) it holds that_

\[\frac{d\pi^{*}(y|x)}{dy}=\frac{1}{Z_{x}}\exp\bigg{(}\frac{f^{*}(y)-c(x,y)}{ \epsilon}\bigg{)}\] (14)

_and \(\pi^{*}(\cdot|x)\!\in\!\mathcal{P}_{p}(\mathcal{Y})\). Then if \(\mathbb{P}_{1}\stackrel{{\text{def}}}{{=}}\pi^{*}_{1}\) belongs to \(\mathcal{P}_{p}(\mathcal{Y})\), the distribution \(\pi^{*}\) is an **EOT plan** for \(\mathbb{P}_{0},\mathbb{P}_{1}\) and cost \(C_{c,\epsilon}\). Moreover, if \(\int_{\mathcal{X}}C_{c,\epsilon}\big{(}x,\pi^{*}(\cdot|x)\big{)}d\mathbb{P}_{0 }(x)\!<\!\infty\), then \(\pi^{*}\) is the **unique EOT plan**._

Our result above requires some technical assumptions on \(c\) and \(f^{*}\) but a reader should not worry as they are easy to satisfy in popular cases such as the quadratic cost \(c(x,y)=\frac{1}{2}\|x-y\|^{2}\) (SS3.3). The important thing is that our result allows **sampling**\(y\sim\pi^{*}(\cdot|x)\) from the conditional EOT plan by using MCMC methods [5, SS11.2] since (14) provides the **unnormalized density** of \(\pi^{*}(y|x)\). Such sampling may be time-consuming, which is why we provide a clever approach to avoid MCMC below.

### Fast Sampling With LogSumExp Quadratic Potentials.

In what follows, we propose a way to overcome the challenging sampling problem by considering the case \(c(x,y)=\frac{\|x-y\|^{2}}{2}\) and the special family of functions \(f^{*}\). For brevity, for a matrix \(A\in\mathbb{R}^{D\times D}\) and \(b\in\mathbb{R}^{D}\), we introduce \(\mathcal{Q}(y|b,A)\!\stackrel{{\text{def}}}{{=}}\!\exp\big{[}- \frac{1}{2}(y-b)^{T}A(y-b)\big{]}\). Henceforth, we choose the potential \(f^{*}\) to be a weighted log-sum-exp (LSE) of \(N\) quadratic functions:

\[f^{*}(y)\!\stackrel{{\text{def}}}{{=}}\!\epsilon\log\sum_{n=1}^{ N}w_{n}\mathcal{Q}(y|b_{n},\epsilon^{-1}A_{n})\] (15)

Here \(w_{n}\geq 0\) and we put \(A_{n}\) to be a symmetric matrix with eigenvalues in range \((-1,+\infty)\). We say that such potentials \(f^{*}\) are **appropriate**. One may also check that \(f^{*}\in\mathcal{C}_{2}(\mathcal{Y})\) as it is just the LSE smoothing of quadratic functions. Importantly, for this potential \(f\) and the quadratic cost, \(\pi^{*}(\cdot|x)\) is a Gaussian mixture, from which one can efficiently sample **without using MCMC methods**.

**Proposition 3.3** (Entropic OT solution for LSE potentials).: _Let \(f^{*}\) be a given appropriate LSE potential (15) and let \(\mathbb{P}_{0}\!\in\!\mathcal{P}_{2}(\mathcal{X})\!\subset\!\mathcal{P}( \mathcal{X})\). Consider the plan \(d\pi^{*}(x,y)=d\pi^{*}(y|x)d\mathbb{P}_{0}(x)\), where_

\[\frac{d\pi^{*}(y|x)}{dy} =\sum_{n=1}^{N}\gamma_{n}\,\mathcal{N}(y|\mu_{n}(x),\Sigma_{n})\text { with }\Sigma_{n}\stackrel{{\text{def}}}{{=}}\epsilon(A_{n}+I)^{-1}\text {, }\mu_{n}(x)\stackrel{{\text{def}}}{{=}}(A_{n}+I)^{-1}(A_{n}b_{n}+x),\] \[\gamma_{n}\stackrel{{\text{def}}}{{=}}\widetilde{w} _{n}/\sum_{n=1}^{N}\widetilde{w}_{n},\qquad\widetilde{w}_{n}\stackrel{{ \text{def}}}{{=}}w_{n}(2\pi)^{\frac{D}{2}}\sqrt{\det(\Sigma_{n})}\mathcal{Q}( x|b_{n},\frac{1}{\epsilon}I-\frac{1}{\epsilon^{2}}\Sigma_{n}).\]

_Then it holds that \(\mathbb{P}_{1}\stackrel{{\text{def}}}{{=}}\pi^{*}_{1}\) belongs to \(\mathcal{P}_{2}(\mathcal{Y})\) and the joint distribution \(\pi^{*}\) is the **unique EOT plan** between \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) for cost \(C_{c,\epsilon}\) with \(c(x,y)=\frac{1}{2}\|x-y\|^{2}\)._

We emphasize that although each conditional distribution \(\pi^{*}\left(\cdot|x\right)\) is a Gaussian mixture, in general, this **does not** mean that \(\pi^{*}\) or \(\mathbb{P}_{1}=\pi^{*}_{1}\) is a Gaussian mixture, even when \(\mathbb{P}_{0}\) is Gaussian. This aspect does not matter for our construction, and we mention it only for the completeness of the exposition.

### Schrodinger Bridge Benchmark Idea

Since there is a link between EOT and SB, our approach allows us to immediately obtain a solution to the Schrodinger Bridge between \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) (constructed with an LSE potential \(f^{*}\)).

**Corollary 3.4** (Solution for SB between \(\mathbb{P}_{0}\) and constructed \(\mathbb{P}_{1}\)).: _In the context of Theorem 3.2, let \(p=2\) and consider \(c(x,y)=\frac{1}{2}\|x-y\|^{2}\). Assume that \(\mathbb{P}_{0}\in\mathcal{P}_{2,ac}(\mathcal{X})\subset\mathcal{P}(\mathcal{X})\) and both \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) (constructed with a given \(f^{*}\)) have finite entropy. Then it holds that \(\varphi^{*}(y)\stackrel{{\text{def}}}{{=}}\exp\big{(}\frac{f^{*}(y) }{\epsilon}\big{)}\) is a Schrodinger potential providing the **optimal drift**\(v^{*}\) for SB via formula (12)._

Although the drift is given in the closed form, its computation may be challenging, especially in high dimensions. Fortunately, as well as for EOT, for the quadratic cost \(c(x,y)=\frac{\|x-y\|^{2}}{2}\) and our LSE (15) potentials \(f^{*}\), we can derive the optimal drift explicitly.

**Corollary 3.5** (SB solution for LSE potentials).: _Let \(f^{*}\) be a given appropriate LSE potential (15) and consider a distribution \(\mathbb{P}_{0}\in\mathcal{P}_{2,\text{ac}}(\mathcal{X})\) with finite entropy. Let \(\mathbb{P}_{1}\) be the one constructed in Proposition 3.3. Then it holds that \(\mathbb{P}_{1}\) has finite entropy, belongs to \(\mathcal{P}_{2,\text{ac}}(\mathcal{Y})\) and_

\[v^{*}(x,t)=\nabla_{x}\log\sum_{n=1}^{N}w_{n}\sqrt{\det(\Sigma_{n}^{t})} \mathcal{Q}(x|b_{n},\frac{1}{\epsilon(1-t)}I-\frac{1}{\epsilon^{2}(1-t)}\Sigma _{n}^{t})\] (16)

_is the optimal drift for the SB between \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\). Here \(A_{n}^{t}\stackrel{{\text{def}}}{{=}}(1-t)A_{n}\) and \(\Sigma_{n}^{t}\stackrel{{\text{def}}}{{=}}\epsilon(A_{n}^{t}+I)^ {-1}\)._

## 4 Constructing Benchmark Pairs for OT and SB: Implementation

Our benchmark is implemented using PyTorch framework and is publicly available at

https://github.com/ngushchin/EntropicOTBenchmark

It provides code to sample from our constructed continuous benchmark pairs \((\mathbb{P}_{0},\mathbb{P}_{1})\) for various \(\epsilon\), see SS4.1 for details of these **pairs**. In SS4.2, we explain the **intended usage** of these pairs.

### Constructed Benchmark Pairs

We construct various pairs in dimensions \(D\) up to \(12288\) and \(\epsilon\in\{0.1,1,10\}\). Our mixtures pairs simulate _noise\(\rightarrow\)data_ setup and images pairs simulate _data\(\rightarrow\)data_ case.

**Mixtures benchmark pairs.** We consider EOT with \(\epsilon\in\{0.1,1,10\}\) in space \(\mathbb{R}^{D}\) with dimension \(D\in\{2,16,64,128\}\). We use a centered Gaussian as \(\mathbb{P}_{0}\) and we use LSE function (15) with \(N=5\) for constructing \(\mathbb{P}_{1}\) (Proposition 3.3). In this case, the constructed distribution \(\mathbb{P}_{1}\) has \(5\) modes (Fig. 2(a), 2(f)). Details of particular parameters (\(A_{n},b_{n},w_{n}\), etc.) are given in Appendix B.

**Images benchmark pairs.** We consider EOT with \(\epsilon\in\{0.1,1,10\}\). As distribution \(\mathbb{P}_{0}\), we use the approximation of the distribution of \(64\times 64\) RGB images (\(D=12288\)) of CelebA faces dataset [39]. Namely, we train a normalizing flow with Glow architecture [29]. It is absolutely continuous by the construction and allows straightforward sampling from \(\mathbb{P}_{0}\). For constructing distribution \(\mathbb{P}_{1}\), we also use LSE function \(f^{*}\). We fix \(N=100\) random samples from \(\mathbb{P}_{0}\) for \(b_{n}\) and choose all \(A_{n}\equiv I\). Details of \(w_{n}\) are given in Appendix C. For these parameters, samples from \(\mathbb{P}_{1}\) look like noised samples from \(\mathbb{P}_{0}\) which are shifted to one of \(\mu_{n}\) (Fig. 4).

By the construction of distribution \(\mathbb{P}_{1}\), obtaining the conditional EOT plan \(\pi^{*}(y|x)\) between \((\mathbb{P}_{0},\mathbb{P}_{1})\) may be viewed as learning the _noising_ model. From the practical perspective, this looks less interesting than learning the _de-noising_ model \(\pi^{*}(x|y)\). Due to this, working with images in SS5, we always test EOT solvers in \(\mathbb{P}_{1}\rightarrow\mathbb{P}_{0}\) direction, i.e., recovering \(\pi^{*}(x|y)\) and generating clean samples \(x\) form noised \(y\). The reverse conditional OT plans \(\pi^{*}(x|y)\) are not as tractable as \(\pi^{*}(y|x)\). We overcome this issue with MCMC in the latent space of the normalizing flow (Appendix C).

### Intended Usage of the Benchmark Pairs

The EOT/SB solvers (SS2) provide an approximation of the conditional OT plan \(\widehat{\pi}(\cdot|x)\approx\pi^{*}(\cdot|x)\) from which one can sample (given \(x\sim\mathbb{P}_{0}\)). In particular, SB solvers recover the approximation of the optimal drift \(\widehat{v}\approx v^{*}\); it is anyway used to produce samples \(y\sim\widehat{\pi}(\cdot|x)\) via solving SDE \(dX_{t}=\widehat{v}(x,t)dt+\sqrt{\epsilon}dW_{t}\) starting from \(X_{0}=x\) at time \(t=0\). Therefore, **the main goal of our benchmark** is to provide a way to compare such approximations \(\widehat{\pi},\widehat{v}\) with the ground truth. _Prior to our work, this was not possible_ due to the lack of non-trivial pairs \((\mathbb{P}_{0},\mathbb{P}_{1})\) with known \(\pi^{*},v^{*}\).

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{\(\epsilon\)} & \multirow{2}{*}{Metric} & \multicolumn{4}{c|}{**EOT solvers**} & \multicolumn{4}{c|}{**SB solvers**} \\ \cline{2-10}  & & **LSOT** & **SCONES** & **NOT** & **EgNOT** & **[ENOT]** & **[MLE-SB]** & **[DiffSB]** & **[**FB-SDE-A**] & **[**FB-SDE-J**] \\ \hline \multirow{2}{*}{\(0.1\)} & \multicolumn{2}{c|}{\(\mathbb{W}_{2}^{2}\)-UVP} & \multirow{2}{*}{\(\star\)} & \multirow{2}{*}{\(\odot\)} & \multirow{2}{*}{\(\odot\)} & \multirow{2}{*}{\(\odot\)} & \multirow{2}{*}{\(\odot\)} & \multirow{2}{*}{\(\odot\)} & \multirow{2}{*}{\(\odot\)} & \multirow{2}{*}{\(\odot\)} \\  & & & & do not work for small \(\epsilon\) & & & & & & \\  & \multicolumn{2}{c|}{} & due to numerical instability & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \cline{2-10}  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{[14, §5.1]} & \multicolumn{2}{c|}{\(\odot\)} & \multicolumn{2}{c|}{\(\odot\)} & \multicolumn{2}{c|}{\(\odot\)} & \multicolumn{2}{c|}{\(\odot\)} & \multicolumn{2}{c|}{\(\odot\)} & \multicolumn{2}{c|}{\(\odot\)} \\ \hline \multirow{2}{*}{\(1\)} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \cline{2-10}  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \hline \multirow{2}{*}{\(10\)} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \hline \end{tabular}
\end{table}
Table 2: The summary of EOT/SB solvers’ quantitative performance in \(\text{cB}\mathbb{W}_{2}^{2}\)-UVP and \(\text{B}\mathbb{W}_{2}^{2}\)-UVP metrics on our mixtures pairs. Detailed evaluation and coloring principles are given in Appendix B.

For each of the constructed pairs \((\mathbb{P}_{0},\mathbb{P}_{1})\), we provide the code to do 5 main things: **(a)** sample \(x\sim\mathbb{P}_{0}\); **(b)** sample \(y\sim\pi^{*}(\cdot|x)\) for any given \(x\); **(c)** sample pairs \((x,y)\sim\pi^{*}\) from the EOT plan; **(d)** sample \(y\sim\mathbb{P}_{1}\); **(e)** compute the optimal drift \(v^{*}(x,t)\). Function **(c)** is just a combination of **(a)** and **(b)**. For images pairs we implement the extra functionality **(f)** to sample \(x\sim\pi^{*}(\cdot|y)\) using MCMC. Sampling **(d)** is implemented via discarding \(x\) (not returning it to a user) in \((x,y)\) in **(c)**.

When **training** a neural EOT/SB solver, one should use random batches from **(a,d)**. Everything coming from **(b,c,e,f)** should be considered as **test** information and used only for evaluation purposes. Also, for each of the benchmark pairs (mixtures and images), we provide a hold-out **test** for evaluation.

## 5 Experiments: Testing EOT and SB Solvers on Our Benchmark Pairs

Now we train various existing EOT/SB solvers from Table 1 on our benchmark pairs \((\mathbb{P}_{0},\mathbb{P}_{1})\) to showcase how well they capture the ground truth EOT plan. For solvers' details, see Appendix D.

**Mixtures benchmark pairs**. For quantitative analysis, we propose the following metric:

\[\text{cB}\mathbb{W}_{2}^{2}\text{-UVP}\big{(}\widehat{\pi},\pi^{*}\big{)} \stackrel{{\text{def}}}{{=}}\frac{100\%}{\frac{1}{2}\text{Var}( \mathbb{P}_{1})}\int_{\mathcal{X}}\text{B}\mathbb{W}_{2}^{2}\big{(}\widehat{ \pi}(\cdot|x),\pi^{*}(\cdot|x)\big{)}d\mathbb{P}_{0}(x).\] (17)

For each \(x\) we compare **conditional** distributions \(\widehat{\pi}(\cdot|x)\) and \(\pi^{*}(\cdot|x)\) with each other by using the Bures-Wasserstein metric [16], i.e., the Wasserstein-2 distance between Gaussian approximations of distributions. Then we average this metric w.r.t. \(x\sim\mathbb{P}_{0}\). The final normalization \(\frac{1}{2}\text{Var}(\mathbb{P}_{1})\) is chosen so that the trivial baseline which maps the entire \(\mathbb{P}_{0}\) to the mean of \(\mathbb{P}_{1}\) provides \(100\%\) error. Metric (17) is a modification of the standard \(\text{B}\mathbb{W}_{2}^{2}\)-UVP [14, 23, 42, 33, 18] for the conditional setting. For completeness, we also report the **standard**\(\text{B}\mathbb{W}_{2}^{2}\)-UVP to check how well \(\widehat{\pi}_{1}\) matches \(\pi_{1}^{*}\).

**Disclaimer.** We found that most solvers' performance **significantly** depends on the selected hyper-parameters. We neither have deep knowledge of many solvers nor have the resources to tune them to achieve the best performance on our benchmark pairs. Thus, _we kindly invite the interested authors of solvers to improve the results for their solvers._ Meanwhile, we report the results of solvers with their default configs and/or with limited tuning. Nevertheless, we present a hyperparameter study in Appendix E to show that the chosen hyperparameters are a reasonable fit for the considered tasks. Our goal here is to find out and explain the issues of the methods which are due to their principle rather than non-optimal hyperparameter selection.

The detailed results are in Appendix B. Here we give their concise summary (Table 2) and give a qualitative example (Fig. 3) of solvers' performance on our mixtures pair with \((D,\epsilon)=(16,1)\).

**EOT solvers.**[**LSOT**] and [**SCONES**] solvers work only for medium/large \(\epsilon=1,10\). \(\lceil\text{{LSOT}}\rfloor\) learns only the barycentric projection [49, Def. 1] hence naturally experiences large errors and even collapses (Fig. 3b). \(\lceil\text{{SCONES}}\rfloor\) solver works better but recovers the plan with a large error. We think

Figure 3: Qualitative results of EOT/SB solvers on our mixtures benchmark pair with \((D,\epsilon)=(16,1)\). The distributions are visualized using \(2\) PCA components of target distribution \(\mathbb{P}_{1}\). Additional examples of performance on pairs with other \(\epsilon\in\{0.1,10\}\) are given in Appendix B.

this is due to using the Langevin dynamic [14, Alg. 2] during the inference which gets stuck in modes plus the imprecise target density approximation which we employed (Appendix D). \(\left\lceil\mathbf{EqNOT}\right\rceil\) solver also employs Langevin dynamic and possibly experiences the same issue (Fig. 2(e)). Interestingly, our evaluation shows that it provides a better metric in matching the target distribution \(\mathbb{P}_{1}\). \(\left\lceil\mathbf{NOT}\right\rceil\) was originally not designed for EOT because it is non-trivial to estimate entropy from samples. To fix this issue, we modify the authors' code for EOT by employing _conditional normalizing flow_ (CNF) as the generator. This allows us to estimate the entropy from samples and hence apply the solver to the EOT case. It scores good results despite the restrictiveness of the used architecture (Fig. 2(d)).

**SB solvers.** For \(\left\lceil\mathbf{MLE-SB}\right\rceil\), the original authors' implementation uses Gaussian processes as parametric approximators instead of neural nets [52]. Since other SB solvers \(\left\lceil\mathbf{DiffSB}\right\rceil,\)\(\left\lceil\mathbf{FB-SDE-A(J)}\right\rfloor\) and \(\left\lceil\mathbf{ENOT}\right\rceil)\) use neural nets, after discussion with the authors of \(\left\lceil\mathbf{MLE-SB}\right\rceil\), we decided to use neural nets in their solver as well. All SB solvers work reasonably well, but their performance drops as the \(\epsilon\) increases. This is because it becomes more difficult to model the entire diffusion (with volatility \(\epsilon\)); these solvers may require more discretization steps. Still, the case of large \(\epsilon\) is not very interesting since, in this case, the EOT is almost equal to the trivial independent plan \(\mathbb{P}_{0}\times\mathbb{P}_{1}\).

**Images benchmark pairs.** There are only two solvers which have been tested by the authors in their papers in such a large-scale _data\(\rightarrow\)data_ setup (\(64\times 64\) RGB images), see Table 1. Namely, these are \(\left\lceil\mathbf{SCONES}\right\rceil\) and \(\left\lceil\mathbf{ENOT}\right\rceil.\) Unfortunately, we found that \(\left\lceil\mathbf{SCONES}\right\rceil\) yields unstable training on our benchmark pairs, probably due to too small \(\epsilon\) for it, see [14, SS5.1]. Therefore, we only report the results of \(\left\lceil\mathbf{ENOT}\right\rceil\) solver. For completeness, we tried to run \(\left\lfloor\mathbf{DiffSB}\right\rfloor,\)\(\left\lceil\mathbf{FB-SDE-A}\right\rceil\) solvers with their configs from _noise\(\rightarrow\)data_ generative modelling setups but they diverged. We also tried \(\left\lceil\mathbf{NOT}\right\rceil\) with convolutional CNF as the generator but it also did not converge. We leave adapting these solvers for high-dimensional _data\(\rightarrow\)data_ setups for future studies. Hence, here we test only \(\left\lceil\mathbf{ENOT}\right\rceil\).

In Fig. 4, we qualitatively see that \(\left\lceil\mathbf{ENOT}\right\rceil\) solver _only for small \(\epsilon\)_ properly learns the EOT plan \(\pi^{*}\) and sufficiently well restores images from the input noised inputs. As there is anyway the lack of baselines in the field of neural EOT/SB, we plan to release these \(\left\lceil\mathbf{ENOT}\right\rceil\) checkpoints and expect them to become a **baseline for future works** in the field. Meanwhile, in Appendix C, we discuss possible metrics which we recommend to use to compare with these baselines.

## 6 Discussion

**Potential Impact.** Despite the considerable growth of the field of EOT/SB, there is still no standard way to test existing neural (continuous) solvers. In our work, we fill this gap. Namely, _we make a step towards bringing clarity and healthy competition to this research area by proposing the first-ever theoretically-grounded EOT/SB benchmark_. We hope that our constructed benchmark pairs will

Figure 4: Qualitative comparison of ground truth samples \(x\sim\pi^{*}(\cdot|y)\) with samples produced by \(\left\lceil\mathbf{ENOT}\right\rceil\). With the increase of \(\epsilon\), the diversity increases but the precision of image restoration drops.

become the standard playground for testing continuous EOT/SB solvers as part of the ongoing effort to advance computational OT/SB, in particular, in its application to generative modelling.

**Limitations (benchmark).** We employ LSE quadratic functions (15) as optimal Kantorovich potentials to construct benchmark pairs. It is unclear whether our benchmark sufficiently reflects the practical scenarios in which the EOT/SB solvers are used. Nevertheless, our methodology is generic and can be used to construct new benchmark pairs but may require MCMC to sample from them.

To show that the family of EOT plans which can be produced with LSE potentials is rich enough, we provide a heuristic recipe on how to construct benchmark pairs simulating given real-world datasets, see Appendix H. As we show there, the recipe works on several non-trivial single-cell datasets [36, 8]. Thus, we conjecture that LSE potentials may be sufficient to represent any complex distribution just like the well-celebrated Gaussian mixtures are capable of approximating any density [43]. We leave this inspiring theoretical question open for future studies.

For completeness, we note that our images benchmark pairs use LSE potentials and do not require MCMC for sampling from marginals \(\mathbb{P}_{0},\mathbb{P}_{1}\), i.e., to get clean and noisy images, respectively. However, for computing the test conditional FID (Appendix C) of EOT/SB solvers, MCMC is needed to sample clean images \(x\sim\pi^{*}(\cdot|y)\) conditioned on noisy inputs \(y\). This may introduce extra sources of error.

**Limitations (evaluation).** We employ \(\mathbb{BV}_{2}^{2}\)-UVP for the quantitative evaluation (SS5) as it is popular in OT field [33, 14, 18, 23, 42]. However, it may not capture the full picture as it only compares the 1st and 2nd moments of distributions. We point to developing of novel evaluation metrics for neural OT/SB solvers as an important and helpful future research direction.

Following our disclaimer in SS5, we acknowledge one more time, that the hyper-parameters tuning of the solvers which we test on our proposed benchmark is not absolutely comprehensive. It is possible that we might have missed something and did not manage to achieve the best possible performance in each particular case. At the same time, our Appendix E shows the extensive empirical study of the key hyper-parameters and it seems that the metrics reported are reasonably close to the optimal ones.

## 7 Acknowledgements

This work was partially supported by the Skoltech NGP Program (Skoltech-MIT joint project).

## References

* Asadulaev et al. [2022] Arip Asadulaev, Alexander Korotin, Vage Egiazarian, and Evgeny Burnaev. Neural optimal transport with general cost functionals. _arXiv preprint arXiv:2205.15403_, 2022.
* Backhoff-Verguas et al. [2019] Julio Backhoff-Verguas, Mathias Beiglbock, and Gudmun Pammer. Existence, duality, and cyclical monotonicity for weak transport costs. _Calculus of Variations and Partial Differential Equations_, 58(6):1-28, 2019.
* Backhoff-Verguas and Pammer [2022] Julio Backhoff-Verguas and Gudmund Pammer. Applications of weak transport theory. _Bernoulli_, 28(1):370-394, 2022.
* Bianchini et al. [2014] Stefano Bianchini, Alexander Dabrowski, et al. Existence and uniqueness of the gradient flow of the entropy in the space of probability measures. _RENDICONTI DELL'ISTITUTO DI MATEMATICA DELL'UNIVERSITA DI TRIESTE_, 46(1):43-70, 2014.
* Bishop and Nasrabadi [2006] Christopher M Bishop and Nasser M Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006.
* Bonneel and Digne [2023] Nicolas Bonneel and Julie Digne. A survey of optimal transport for computer graphics and computer vision. In _Computer Graphics Forum_, volume 42, pages 439-460. Wiley Online Library, 2023.
* Bunne et al. [2023] Charlotte Bunne, Ya-Ping Hsieh, Marco Cuturi, and Andreas Krause. The schrodinger bridge between gaussian measures has a closed form. In _International Conference on Artificial Intelligence and Statistics_, pages 5802-5833. PMLR, 2023.
* Bunne et al. [2023] Charlotte Bunne, Stefan G Stark, Gabriele Gut, Jacobo Sarabia Del Castillo, Mitch Levesque, Kjong-Van Lehmann, Lucas Pelkmans, Andreas Krause, and Gunnar Ratsch. Learning single-cell perturbation responses using neural optimal transport. _Nature Methods_, pages 1-10, 2023.

* [9] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schrodinger bridge using forward-backward SDEs theory. In _International Conference on Learning Representations_, 2022.
* [10] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. Optimal steering of a linear stochastic system to a final probability distribution, part i. _IEEE Transactions on Automatic Control_, 61(5):1158-1169, 2015.
* [11] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. On the relation between optimal transport and schrodinger bridges: A stochastic control viewpoint. _Journal of Optimization Theory and Applications_, 169:671-691, 2016.
* [12] Jaemoo Choi, Jaewoong Choi, and Myungjoo Kang. Generative modeling through the semi-dual formulation of unbalanced optimal transport. In _Advances in Neural Information Processing Systems_, 2023.
* [13] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in neural information processing systems_, 26, 2013.
* [14] Max Daniels, Tyler Maunu, and Paul Hand. Score-based generative neural networks for large-scale optimal transport. _Advances in neural information processing systems_, 34:12955-12965, 2021.
* [15] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* [16] D.C Dowson and B.V Landau. The frechet distance between multivariate normal distributions. _Journal of Multivariate Analysis_, 12(3):450-455, 1982.
* [17] Jiaojiao Fan, Shu Liu, Shaojun Ma, Hao-Min Zhou, and Yongxin Chen. Neural monge map estimation and its applications. _Transactions on Machine Learning Research_, 2023. Featured Certification.
* [18] Jiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Scalable computations of wasserstein barycenter via input convex neural networks. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1571-1581. PMLR, 18-24 Jul 2021.
* [19] Milena Gazdieva, Litu Rout, Alexander Korotin, Alexander Filippov, and Evgeny Burnaev. Unpaired image super-resolution with optimal transport maps. _arXiv preprint arXiv:2202.01116_, 2022.
* [20] Aude Genevay. _Entropy-regularized optimal transport for machine learning_. PhD thesis, Paris Sciences et Lettres (ComUE), 2019.
* [21] Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-scale optimal transport. In _Advances in neural information processing systems_, pages 3440-3448, 2016.
* [22] Nathael Gozlan, Cyril Roberto, Paul-Marie Samson, and Prasad Tetali. Kantorovich duality for general transport costs and applications. _Journal of Functional Analysis_, 273(11):3327-3405, 2017.
* [23] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. In _Advances in Neural Information Processing Systems_, 2023.
* [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in neural information processing systems_, pages 6626-6637, 2017.
* [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.

* [26] Hicham Janati, Boris Muzellec, Gabriel Peyre, and Marco Cuturi. Entropic optimal transport between unbalanced gaussian measures has a closed form. _Advances in neural information processing systems_, 33:10468-10479, 2020.
* [27] Leonid V Kantorovich. On the translocation of masses. In _Dokl. Akad. Nauk. USSR (NS)_, volume 37, pages 199-201, 1942.
* [28] Abdelwahed Khamis, Russell Tsuchida, Mohamed Tarek, Vivien Rolland, and Lars Petersson. Earth movers in the big data era: A review of optimal transport in machine learning. _arXiv preprint arXiv:2305.05080_, 2023.
* [29] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. _Advances in neural information processing systems_, 31, 2018.
* [30] Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev. Wasserstein-2 generative networks. In _International Conference on Learning Representations_, 2021.
* [31] Alexander Korotin, Alexander Kolesov, and Evgeny Burnaev. Kantorovich strikes back! Wasserstein GANs are not optimal transport? In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [32] Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. _Advances in Neural Information Processing Systems_, 34:14593-14605, 2021.
* [33] Alexander Korotin, Lingxiao Li, Justin Solomon, and Evgeny Burnaev. Continuous wasserstein-2 barycenter estimation without minimax optimization. In _International Conference on Learning Representations_, 2021.
* [34] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport. In _International Conference on Learning Representations_, 2023.
* [35] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. In _International Conference on Learning Representations_, 2023.
* [36] Takeshi Koshizuka and Issei Sato. Neural lagrangian schr\(\backslash\)"{o}dinger bridge: Diffusion modeling for population dynamics. In _The Eleventh International Conference on Learning Representations_, 2022.
* [37] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* [38] Christian Leonard. A survey of the schr\(\backslash\)"dinger problem and some of its connections with optimal transport. _arXiv preprint arXiv:1308.0215_, 2013.
* [39] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* [40] Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport mapping via input convex neural networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6672-6681. PMLR, 13-18 Jul 2020.
* [41] Anton Mallasto, Augusto Gerolin, and Ha Quang Minh. Entropy-regularized 2-wasserstein distance between gaussian measures. _Information Geometry_, 5(1):289-323, 2022.
* [42] Petr Mokrov, Alexander Korotin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. _arXiv preprint arXiv:2304.06094_, 2023.
* [43] T Tin Nguyen, Hien D Nguyen, Faiecl Chamroukhi, and Geoffrey J McLachlan. Approximation by finite mixtures of continuous density functions that vanish at infinity. _Cogent Mathematics & Statistics_, 7(1):1750861, 2020.

* [44] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [46] Litu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In _International Conference on Learning Representations_, 2022.
* [47] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH 2022 Conference Proceedings_, pages 1-10, 2022.
* [48] Filippo Santambrogio. Optimal transport for applied mathematicians. _Birkauser, NY_, 55(58-63):94, 2015.
* [49] Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large scale optimal transport and mapping estimation. In _International Conference on Learning Representations_, 2018.
* [50] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [51] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [52] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.
* [53] Cedric Villani. _Optimal transport: old and new_, volume 338. Springer Science & Business Media, 2008.
* [54] Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schrodinger bridge. In _International Conference on Machine Learning_, pages 10794-10804. PMLR, 2021.
* [55] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* [56] Yujia Xie, Minshuo Chen, Haoming Jiang, Tuo Zhao, and Hongyuan Zha. On scalable and efficient computation of large scale optimal transport. volume 97 of _Proceedings of Machine Learning Research_, pages 6882-6892, Long Beach, California, USA, 09-15 Jun 2019. PMLR.

Proofs

Proof of Theorem 3.1.: By the definition of \(\mathbb{P}_{1}\), it holds that \(\pi^{*}\in\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\). It suffices to show that \(\pi^{*}\) attains the optimal cost. Let \(\textbf{Cost}(\pi)\) be the value of weak OT functional for a plan \(\pi\), i.e.,

\[\textbf{Cost}(\pi)\stackrel{{\text{def}}}{{=}}\int_{\mathcal{X}}C (x,\pi(\cdot|x))d\mathbb{P}(x).\]

We consider weak OT (2) between \(\mathbb{P}_{0}\in\mathcal{P}(\mathcal{X})\) and \(\mathbb{P}_{1}\in\mathcal{P}_{p}(\mathcal{X})\) and use its dual form (7):

\[\textbf{Cost}(\mathbb{P}_{0},\mathbb{P}_{1})=\sup_{f}\bigg{\{} \int_{\mathcal{X}}f^{C}(x)d\mathbb{P}_{0}(x)+\int_{\mathcal{Y}}f(y)d\mathbb{P} _{1}(y)\bigg{\}}=\] \[\sup_{f}\bigg{\{}\int_{\mathcal{X}}\inf_{\nu\in\mathcal{P}_{p}( \mathcal{Y})}\{C(x,\nu)-\int_{\mathcal{Y}}f(y)d\nu(y)\}d\mathbb{P}_{0}(x)+\int _{\mathcal{Y}}f(y)d\mathbb{P}_{1}(y)\bigg{\}}\geq\] \[\int_{\mathcal{X}}\inf_{\nu\in\mathcal{P}_{p}(\mathcal{Y})}\{C(x,\nu)-\int_{\mathcal{Y}}f^{*}(y)d\nu(y)\}d\mathbb{P}_{0}(x)+\int_{\mathcal{Y}} f^{*}(y)d\mathbb{P}_{1}(y).\]

Now we use the fact that \(\pi^{*}(\cdot|x)\) minimizes (8) for all \(x\in\mathcal{X}\):

\[\int_{\mathcal{X}}\inf_{\nu\in\mathcal{P}_{p}(\mathcal{Y})}\{C(x,\nu)-\int f^{*}(y)d\nu(y)\}d\mathbb{P}_{0}(x)+\int_{\mathcal{Y}}f^{*}(y)d \mathbb{P}_{1}(y)=\] \[=\int_{\mathcal{X}}\big{\{}C(x,\pi^{*}(\cdot|x))-\int_{\mathcal{Y }}f^{*}(y)d\pi^{*}(y|x)\big{\}}d\mathbb{P}_{0}(x)+\int_{\mathcal{Y}}f^{*}(y)d \mathbb{P}_{1}(y)=\] \[\int_{\mathcal{X}}C(x,\pi^{*}(\cdot|x))d\mathbb{P}_{0}(x)-\int_{ \mathcal{X}}\int_{\mathcal{Y}}f^{*}(y)d\pi^{*}(x|x)\underbrace{d\mathbb{P}_{0} (x)}_{=d\pi^{*}_{0}(x)}+\int_{\mathcal{Y}}f^{*}(y)d\mathbb{P}_{1}(y)=\] \[\int_{\mathcal{X}}C(x,\pi^{*}(\cdot|x))d\mathbb{P}_{0}(x)-\int_{ \mathcal{X}\times\mathcal{Y}}f^{*}(y)d\pi^{*}(x,y)+\int_{\mathcal{Y}}f^{*}(y)d \mathbb{P}_{1}(y)=\] \[\int_{\mathcal{X}}C(x,\pi^{*}(\cdot|x))d\mathbb{P}_{0}(x)-\int_{ \mathcal{Y}}f^{*}(y)d\pi^{*}_{1}(y)+\int_{\mathcal{Y}}f^{*}(y)d\mathbb{P}_{1} (y)=\] \[\int_{\mathcal{X}}C(x,\pi^{*}(\cdot|x))d\mathbb{P}_{0}(x)+ \underbrace{\int_{\mathcal{Y}}f^{*}(y)d(\mathbb{P}_{1}-\pi^{*}_{1})(y)}_{=0 \text{ since }\pi^{*}_{1}=\mathbb{P}_{1}}=\] \[\int_{\mathcal{X}}C(x,\pi^{*}(\cdot|x))d\mathbb{P}_{0}(x)=\textbf {Cost}(\pi^{*}).\] (18)

We see that \(\textbf{Cost}(\pi^{*})\) is not greater than the optimal \(\textbf{Cost}(\mathbb{P}_{0},\mathbb{P}_{1})\), i.e., \(\pi^{*}\) is optimal. At the same time, from the derivations above, it directly follows that \(f^{*}\) is an optimal potential. 

Proof of Theorem 3.2.: We are going to use our Theorem 3.1. First, we check that (13) holds for \(\pi^{*}(\cdot|x)\) defined by (14). Analogously to [42, Theorem 1], for each \(x\in\mathcal{X}\), we derive

\[\inf_{\nu\in\mathcal{P}_{p}(\mathcal{Y})}\{C_{c,\epsilon}(x,\nu)-\int_{ \mathcal{Y}}f^{*}(y)d\nu(y)\}=\inf_{\nu\in\mathcal{P}_{p}(\mathcal{Y})} \underbrace{\big{\{}\int_{\mathcal{Y}}\big{[}c(x,y)-f^{*}(y)\big{]}d\nu(y)- \epsilon H(\nu)\big{\}}}_{\stackrel{{\text{def}}}{{=}}\mathcal{G} _{\sigma}(\nu)}.\]

Minimizing \(\mathcal{G}_{\pi}\), one should consider only \(\nu\in\mathcal{P}_{p,ac}(\mathcal{Y})\subset\mathcal{P}_{p}(\mathcal{Y})\). Indeed, for \(\nu\notin\mathcal{P}_{p,ac}(\mathcal{Y})\), it holds that \(\mathcal{G}_{\pi}(\nu^{*})=+\infty\) since \(c(x,y)\) is lower bounded and \(-H(\nu)=+\infty\). We continue

\[\inf_{\nu\in\mathcal{P}_{p,ac}(\mathcal{Y})}\big{\{}-\epsilon \int_{\mathcal{Y}}\log\exp\bigg{(}\frac{f^{*}(y)-c(x,y)}{\epsilon}\bigg{)}d\nu (y)+\epsilon\overbrace{\int_{\mathcal{Y}}\log\frac{d\nu(y)}{dy}d\nu(y)}^{=-H( \nu)}=\] \[\inf_{\nu\in\mathcal{P}_{p,ac}(\mathcal{Y})}\big{\{}-\epsilon \int_{\mathcal{Y}}\log\big{(}Z_{x}\cdot\frac{d\pi^{*}(y|x)}{dy}\big{)}d\nu(y)+ \epsilon\int_{\mathcal{Y}}\log\frac{d\nu(y)}{dy}d\nu(y)\big{\}}=\] \[-\epsilon\log Z_{x}+\inf_{\nu\in\mathcal{P}_{p,ac}(\mathcal{Y})} \big{\{}-\epsilon\int_{\mathcal{Y}}\log\frac{d\pi^{*}(y|x)}{dy}d\nu(y)+ \epsilon\int_{\mathcal{Y}}\log\frac{d\nu(y)}{dy}d\nu(y)\big{\}}=\]\[-\epsilon\log Z_{x}+\inf_{\nu\in\mathcal{P}_{p,ac}}\epsilon\text{KL}\left(\nu \|\pi^{*}(\cdot|x)\right).\] (19)

Since \(\pi^{*}(\cdot|x)\in\mathcal{P}_{p,ac}(\mathcal{Y})\), by the assumption of the current Theorem, we conclude that it is the unique minimum of \(\mathcal{G}_{x}(\nu)\) in \(\mathcal{P}_{p,ac}(\mathcal{Y})\). Now to apply our Theorem 3.1, it remains to check that all its assumptions hold. We only have to check that \(C_{c,\epsilon}\) given by (6) is lower bounded, jointly lower semi-continuous and convex in the second argument.

Analogously to (19), we derive

\[C_{c,\epsilon}(x,\nu)=\int_{\mathcal{Y}}c(x,y)d\nu(y)-\epsilon H(\nu)=\underbrace {-\epsilon\log M_{x}}_{\geq-\epsilon\log M}+\epsilon\underbrace{\text{KL} \left(\nu\|\nu_{x}\right)}_{\geq 0}\geq-\epsilon\log M,\] (20)

where \(\frac{d\nu_{x}(y)}{dy}\stackrel{{\text{def}}}{{=}}M_{x}^{-1} \exp\big{(}-\frac{c(x,y)}{\epsilon}\big{)}\). This provides a lower bound on the cost \(C_{c,\epsilon}\). From the first equality in (20), we see that \(C_{c,\epsilon}\) is jointly lower semi-continuous because the first term \(\int_{\mathcal{Y}}c(x,y)d\nu(y)\) is jointly lower semi-continuous by the assumptions and the entropy term \(-H(\nu)\) is lower semi-continuous in \(\mathcal{P}_{1}(\mathcal{Y})\)[48, Ex. 45] and hence in \(\mathcal{P}_{p}(\mathcal{Y})\) as well (\(p\geq 1\)). The last step is to note that \(C_{c,\epsilon}(x,\nu)\) is convex in \(\nu\) thanks to the convexity of \(-H(\nu)\).

Finally, if \(\int_{\mathcal{X}}C_{c,\epsilon}\big{(}x,\pi^{*}(\cdot|x)\big{)}d\mathbb{P}_{ 0}(x)<\infty\), then \(-\int_{\mathcal{X}}H\big{(}\pi^{*}(\cdot|x)\big{)}\) is finite. Let \(U\) be the subset of plans \(\pi\subset\Pi(\mathbb{P}_{0},\mathbb{P}_{1})\) where \(-\int_{\mathcal{X}}H\big{(}\pi(\cdot|x)\big{)}\) is finite. It is not empty since \(\pi^{*}\in U\). At the same time, it is a convex set and functional \(\pi\mapsto-\int_{\mathcal{X}}H\big{(}\pi(\cdot|x)\big{)}d\mathbb{P}_{0}(x)\) is **strictly** convex in \(U\) thanks to the strict convexity of the (negative) entropy \(\nu\mapsto-H(\nu)\) on the set of distributions where it is finite. Thus, \(\pi\mapsto\int_{\mathcal{X}}C_{c,\epsilon}\big{(}x,\pi(\cdot|x)\big{)}d \mathbb{P}_{0}(x)\) is strictly convex in \(U\) and \(\pi^{*}\) is the unique minimum.

For completeness, we note that if \(\int_{\mathcal{X}}C_{c,\epsilon}\big{(}x,\pi^{*}(\cdot|x)\big{)}d\mathbb{P}_{ 0}(x)=+\infty\), this situation is trivial, as the cost of every plan turns to be equal to \(+\infty\). As a result, every plan is optimal.

Proof of Proposition 3.3.: Deriving the actual form of \(\pi^{*}(\cdot|x)\) is an easy exercise. We substitute (15) into (14) and use the quadratic cost \(c(x,y)=\frac{||y-x||^{2}}{2}\):

\[\frac{d\pi^{*}(y|x)}{dy}=\frac{1}{Z_{x}}\exp\bigg{(}\frac{f^{*}(y)-c(x,y)}{ \epsilon}\bigg{)} =\]

\[\frac{1}{Z_{x}}\exp\bigg{(}\frac{\epsilon\log\sum_{n=1}^{N}w_{n}\mathcal{Q}(y| b_{n},\epsilon^{-1}A_{n})-\frac{||y-x||^{2}}{2}}{2}\bigg{)} =\]

\[\frac{1}{Z_{x}}\bigg{(}\sum_{n=1}^{N}w_{n}\mathcal{Q}(y|b_{n},\epsilon^{-1}A_ {n})\bigg{)}\exp(-\frac{||y-x||^{2}}{2\epsilon}) =\]

\[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}\bigg{(}\exp\big{[}-\frac{1}{2}(y-b_{n})^{T} \frac{A_{n}}{\epsilon}(y-b_{n})-\frac{||y-x||^{2}}{2\epsilon}\big{]}\bigg{)} =\]

\[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}\bigg{(}\exp\big{[}-\frac{1}{2}(y-b_{n})^{T} \frac{A_{n}}{\epsilon}(y-b_{n})-\frac{1}{2}(y-x)^{T}\frac{I}{\epsilon}(y-x) \big{]}\bigg{)} =\]

\[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}\bigg{(}\exp\big{[}-\frac{1}{2}(y-b_{n})^{T} \frac{A_{n}}{\epsilon}(y-b_{n})+(y-x)^{T}\frac{I}{\epsilon}(y-x)\big{]}\bigg{)}.\] (21)

Next, we prove that (we write just \(\mu_{n}\) instead of \(\mu_{n}(x)\) for simplicity):

\[(y-b_{n})^{T}\frac{A_{n}}{\epsilon}(y-b_{n})+(y-x)^{T}\frac{I}{\epsilon}(y-x)=\]

[MISSING_PAGE_EMPTY:16]

\[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+b_{n}^{T}\frac{A_{n}-A_{n}^{T} \frac{\Sigma_{n}}{\epsilon}A_{n}}{\epsilon}h_{n}-2b_{n}^{T}(\frac{I}{\epsilon}- \frac{\Sigma_{n}}{\epsilon^{2}})x+x^{T}(\frac{I}{\epsilon}-\frac{\Sigma_{n}}{ \epsilon^{2}})x=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+b_{n}^{T}\frac{A_{n}-A _{n}^{T}\frac{\Sigma_{n}}{\epsilon}A_{n}}{\epsilon}b_{n}-\] \[b_{n}^{T}(\frac{I}{\epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})b_{ n}+(x-b_{n})^{T}(\frac{I}{\epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})+\] \[b_{n}^{T}\frac{A_{n}-A_{n}^{T}\frac{\Sigma_{n}}{\epsilon}A_{n}} {\epsilon}b_{n}-b_{n}^{T}(\frac{I}{\epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})b _{n}=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})+\] \[b_{n}^{T}(\frac{A_{n}-A_{n}^{T}\frac{\Sigma_{n}}{\epsilon}A_{n}} {\epsilon}-\frac{I}{\epsilon}+\frac{\Sigma_{n}}{\epsilon^{2}})b_{n}=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})+\] \[b_{n}^{T}(\frac{A_{n}(I-\frac{\Sigma_{n}}{\epsilon}A_{n})}{ \epsilon}-\frac{I}{\epsilon}+\frac{\Sigma_{n}}{\epsilon^{2}})b_{n}=\] \[b_{n}^{T}(\frac{A_{n}(I-\frac{\Sigma_{n}}{\epsilon}A_{n})}{ \epsilon}-\frac{I}{\epsilon}+\frac{\Sigma_{n}}{\epsilon^{2}})b_{n}=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})+\] \[b_{n}^{T}(\frac{A_{n}(\frac{\Sigma_{n}}{\epsilon})-I+\frac{ \Sigma_{n}}{\epsilon}}{\epsilon})b_{n}=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})+\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})+\] \[b_{n}^{T}(\frac{(\epsilon\Sigma_{n}^{-1}-I)\frac{\Sigma_{n}}{ \epsilon}-I+\frac{\Sigma_{n}}{\epsilon}}{\epsilon})b_{n}=\] \[(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n}).\]

Next, we substitute (22) into (21)

\[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}\bigg{(}\exp\big{[}-\frac{1}{2} \big{\{}(y-b_{n})^{T}\frac{A_{n}}{\epsilon}(y-b_{n})+(y-x)^{T}\frac{I}{ \epsilon}(y-x)\big{\}}\big{]}\bigg{)}=\] \[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}\bigg{(}\exp\big{[}-\frac{1}{2} \big{\{}(y-\mu_{n})^{T}\Sigma_{n}^{-1}(y-\mu_{n})+(x-b_{n})^{T}(\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n})\big{\}}\big{]}\bigg{)}=\] \[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}\exp(-\frac{1}{2}(y-\mu_{n})^{T }\Sigma_{n}^{-1}(y-\mu_{n}))\exp(-\frac{1}{2}(x-b_{n})^{T}(\frac{I}{\epsilon} -\frac{\Sigma_{n}}{\epsilon^{2}})(x-b_{n}))=\] \[\frac{1}{Z_{x}}\sum_{n=1}^{N}w_{n}(2\pi)^{\frac{D}{2}}\sqrt{\det( \Sigma_{n})}\mathcal{N}(y|\mu_{n},\Sigma_{n})\mathcal{Q}(x|b_{n},\frac{I}{ \epsilon}-\frac{\Sigma_{n}}{\epsilon^{2}})=\] \[\frac{1}{Z_{x}}\sum_{n=1}^{N}\underbrace{w_{n}(2\pi)^{\frac{D}{2} }\sqrt{\det(\Sigma_{n})}\mathcal{Q}(x|b_{n},\frac{I}{\epsilon}-\frac{\Sigma_{n }}{\epsilon^{2}})}_{\widehat{w}_{n}}\mathcal{N}(y|\mu_{n},\Sigma_{n}))=\]\[\frac{1}{Z_{x}}\sum_{n=1}^{N}\widetilde{w}_{n}\mathcal{N}(y|\mu_{n}, \Sigma_{n})=\frac{1}{\sum_{n=1}^{N}\widetilde{w}_{n}}\sum_{n=1}^{N}\widetilde{w} _{n}\mathcal{N}(y|\mu_{n},\Sigma_{n})=\] \[\sum_{n=1}^{N}\frac{\widetilde{w}_{n}}{\sum_{n=1}^{N}\widetilde{w }_{n}}\mathcal{N}(y|\mu_{n},\Sigma_{n})=\sum_{n=1}^{N}\gamma_{n}\mathcal{N}(y| \mu_{n},\Sigma_{n}).\]

which finishes the derivation of the expression for the density of \(\pi^{*}(\cdot|x)\).

Now we prove that \(\mathbb{P}_{1}\stackrel{{\mathrm{def}}}{{=}}\pi_{1}^{*}\in \mathcal{P}_{2}(\mathcal{Y})\). For each \(x\), consider \(\frac{d\pi^{*}(y|x)}{dy}=\sum_{n=1}^{N}\gamma_{n}\mathcal{N}(y|\mu_{n}(x), \Sigma_{n})\). Its second moment is given by \(\sum_{n=1}^{N}\gamma_{n}\big{(}\|\mu_{n}(x)\|^{2}+\operatorname{Tr}\Sigma_{ n}\big{)}\). Note that

\[\|\mu_{n}(x)\|=\|(A_{n}+I)^{-1}(A_{n}b_{n}+x)\|\leq\] \[\|(A_{n}+I)^{-1}\|\cdot\|A_{n}b_{n}+x\|\leq\|(A_{n}+I)^{-1}\| \cdot(\|A_{n}b_{n}\|+\|x\|),\]

where \(\|\cdot\|\) applied to matrix means the operator norm. Hence, one may conclude that \(\|\mu_{n}(x)\|^{2}\) is upper bounded by some quadratic polynomial of \(\|x\|\), i.e., there exist constants \(\alpha_{n}\in\mathbb{R},\beta_{n}\in\mathbb{R}_{+}\) such that \(\|\mu_{n}(x)\|^{2}\leq\alpha_{n}+\beta_{n}\cdot\|x\|^{2}\). We derive

\[\int_{\mathcal{Y}}\|y\|^{2}d\pi_{1}^{*}(y)=\int_{\mathcal{X}}\int _{\mathcal{Y}}\|y\|^{2}d\pi^{*}(y|x)\underbrace{d\pi_{0}^{*}}_{=d\mathbb{P}_{ 0}(x)}(x)=\int_{\mathcal{X}}\sum_{n=1}^{N}\gamma_{n}\big{(}\|\mu_{n}(x)\|^{2} +\operatorname{Tr}\Sigma_{n}\big{)}d\mathbb{P}_{0}(x)\leq\] \[\int_{\mathcal{X}}\sum_{n=1}^{N}\gamma_{n}\big{(}\alpha_{n}+\beta _{n}\|x\|^{2}+\operatorname{Tr}\Sigma_{n}\big{)}d\mathbb{P}_{0}(x)=\] \[\sum_{n=1}^{N}\gamma_{n}\big{(}\alpha_{n}+\operatorname{Tr}\Sigma _{n}\big{)}+\big{(}\sum_{n=1}^{N}\beta_{n}\gamma_{n}\big{)}\int_{\mathcal{X}} \|x\|^{2}d\mathbb{P}_{0}(x)<\infty\]

since \(\mathbb{P}_{0}\in\mathcal{P}_{2}(\mathcal{X})\) by the assumption of the proposition.

It remains to prove that \(\pi^{*}\) is the unique EOT plan. According to our Theorem 3.2, one only has to ensure that \(\int_{\mathcal{X}}C_{c,\epsilon}(x,\pi^{*}(\cdot|x))d\mathbb{P}_{0}(x)<\infty\). Just for completeness, we highlight that \(\int_{\mathcal{X}}C_{c,\epsilon}\big{(}x,\pi^{*}(\cdot|x)\big{)}d\mathbb{P}_{0 }(x)\) is _lower_-bounded since \(C_{c,\epsilon}\) is lower bounded, see the proof of Theorem 3.2. Anyway, this is indifferent for us. We recall that \(\pi^{*}\) is an optimal plan between \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}=\pi_{1}^{*}\) and \(f^{*}\) is an optimal potential by our construction. Thanks to the duality, we have

\[\int_{\mathcal{X}}C_{c,\epsilon}\big{(}x,\pi^{*}(\cdot|x)\big{)}d \mathbb{P}_{0}(x)=\int_{\mathcal{X}}(f^{*})^{C_{c,\epsilon}}(x)d\mathbb{P}_{0}(x )+\int_{\mathcal{Y}}f^{*}(y)d\mathbb{P}_{1}(y)=\] \[\int_{\mathcal{X}}\big{[}-\epsilon\log Z_{x}\big{]}d\mathbb{P}_{0 }(x)+\int_{\mathcal{Y}}f^{*}(y)d\mathbb{P}_{1}(y),\] (23)

where in transition to (23) we used our findings of line (19). Note that \(\int_{\mathcal{Y}}f^{*}(y)d\mathbb{P}_{1}(y)\) is finite since \(f^{*}\in\mathcal{C}_{2}(\mathcal{Y})\) is dominated by a quadratic polynomial, and we have already proved that \(\mathbb{P}_{1}\) has finite second moment. It remains to upper bound the first term in (23). We note that

\[Z_{x}=\int_{\mathcal{Y}}\exp\bigg{(}\frac{f^{*}(y)-\frac{1}{2}\|x- y\|^{2}}{\epsilon}\bigg{)}dy=(\sqrt{2\pi\epsilon})^{D}\int_{\mathcal{Y}}\exp \bigg{(}\frac{f^{*}(y)}{\epsilon}\bigg{)}\mathcal{N}(y|x,\epsilon I)dy\geq\] \[(\sqrt{2\pi\epsilon})^{D}\exp\bigg{(}\int_{\mathcal{Y}}\frac{f^{*} (y)}{\epsilon}\mathcal{N}(y|x,\epsilon I)dy\bigg{)}\geq(\sqrt{2\pi\epsilon})^{D }\exp\bigg{(}\int_{\mathcal{Y}}\frac{\beta+\alpha\|y\|^{2}}{\epsilon}\mathcal{N }(y|x,\epsilon I)dy\bigg{)}=\] (24) \[(\sqrt{2\pi\epsilon})^{D}\exp\bigg{(}\frac{\beta+\alpha(\|x\|^{2} +\epsilon D)}{\epsilon}\bigg{)},\] (25)

where in transition to line (24) we used the Jessnen's inequality and \(\alpha,\beta\in\mathbb{R}\) are some constants for which \(f^{*}(\cdot)\geq\beta+\alpha\|\cdot\|^{2}\). They exist since \(f^{*}\in\mathcal{C}_{2}(\mathcal{Y})\). Indeed, there exist \(\tilde{\alpha},\tilde{\beta}:\,|f^{*}(\cdot)|\leq\tilde{\beta}+\tilde{\alpha} \|\cdot\|^{2}\geq f^{*}(\cdot)\geq-\tilde{\beta}-\tilde{\alpha}\|\cdot\|^{2}\), and we set \(\alpha=-\tilde{\alpha},\beta=-\tilde{\beta}\). In turn, line (25) uses the explicit formula for the second moment of \(\mathcal{N}(y|x,\epsilon I)\). We use (25) to upper bound the first term in (23):

\[\int_{\mathcal{X}}\big{[}-\epsilon\log Z_{x}\big{]}d\mathbb{P}_{0}(x)\leq\int_{ \mathcal{X}}\big{[}-\epsilon\log\bigg{(}\{(\sqrt{2\pi\epsilon})^{D}\exp\bigg{(} \frac{\beta+\alpha\|x\|^{2}+\alpha\epsilon D}{\epsilon}\bigg{)}\bigg{)}\bigg{]}d \mathbb{P}_{0}(x)=\]\[-\frac{\epsilon D}{2}\log(2\pi\epsilon)-\beta-\alpha\epsilon D-\alpha\int_{\mathcal{X }}\|x\|^{2}d\mathbb{P}_{0}(x).\]

It remains to note that the last value is finite, since \(\mathbb{P}_{0}\in\mathcal{P}_{2}(\mathcal{X})\) by the assumption. 

Proof of Corollary 3.4.: We note that \(\frac{d\pi^{*}(y\|x)}{dy}\propto\exp\big{(}\frac{f^{*}(y)-\frac{1}{2}\|x-y\|^{ 2}}{\epsilon}\big{)}\). Therefore,

\[\exp\big{(}\frac{f^{*}(y)}{\epsilon}\big{)}\propto\frac{d\pi^{*}(y|x)}{dy}\exp \big{(}\frac{1}{2\epsilon}\|x-y\|^{2}\big{)}\propto\frac{d\pi^{*}(y|x)}{dy} \cdot\big{[}\mathcal{N}(y|x,\epsilon I)\big{]}^{-1}.\] (26)

By comparing (26) with (11), we see that \(\exp\big{(}\frac{f^{*}(y)}{\epsilon}\big{)}\) indeed coincides with the Schrodinger potential \(\phi^{*}(y)\). Formula (12) for the optimal drift follows from [38, Proposition 4.1]3. 

Footnote 3: The authors of [38] consider SB with the _reversible_ Wiener prior \(R\), i.e., the standard Brownian motion starting at the Lebesgue measure. They deal with \(\inf_{T\in\mathcal{F}(\mathbb{P}_{0},\mathbb{P}_{1})}\text{KL }(T\|R)\) which matches (up to an additive constant) our formulation (9) for \(\epsilon=1\). Indeed, using the measure disintegration theorem, one can derive \(\text{KL }(T\|R)=-H(\mathbb{P}_{0})+\text{KL }(T\|W^{\epsilon})\). For other \(\epsilon>0\), the analogous equivalence holds true.

Proof of Corollary 3.5.: First, we prove that constructed \(\mathbb{P}_{1}\overset{\text{def}}{=}\pi_{1}^{*}\) actually has finite entropy. This is needed to ensure that the assumptions of [38, Proposition 4.1]. This proposition provides the formula for the optimal drift (12) via the Schrodinger potential. We write

\[0\leq\text{KL }(\pi_{1}^{*}\|\mathcal{N}(\cdot|0,I))=-H(\pi_{1}^ {*})-\int_{\mathcal{Y}}\log\mathcal{N}(y|0,I)d\pi_{1}^{*}(y)=\] \[-H(\pi_{1}^{*})+\frac{D}{2}\log(2\pi)+\frac{1}{2}\int_{\mathcal{ Y}}\|y\|^{2}d\pi_{1}^{*}(y).\] (27)

From our Proposition 3.3 it follows that \(\mathbb{P}_{1}=\pi_{1}^{*}\) has finite second moment. Hence, the latter constant in (27) is finite. Therefore, \(H(\pi_{1}^{*})\) is upper bounded. To lower bound \(H(\pi_{1}^{*})\), recall that each \(\pi^{*}(\cdot|x)\) is a mixture of \(N\) Gaussians (Proposition 3.3) with (\(x\)-independent) covariances \(\Sigma_{n}\). Thus, its density \(\frac{d\pi^{*}(y|x)}{dy}\) is upper bounded by \(\xi\overset{\text{def}}{=}\max_{n}\big{[}(2\pi)^{-D/2}\big{]}(\det\Sigma_{n}) ^{-1/2}>0\) which also means that

\[\frac{d\pi_{1}^{*}(y)}{dy}=\int_{\mathcal{X}}\frac{d\pi^{*}(y|x)}{dy}d\pi_{0}^ {*}(x)\leq\int_{\mathcal{X}}\xi d\pi_{0}^{*}(x)\leq\xi.\]

We conclude that

\[H(\pi_{1}^{*})=-\int\log\frac{d\pi_{1}^{*}(y)}{dy}d\pi_{1}^{*}(y)\geq-\int\log \xi d\pi_{1}^{*}(y)=-\log\xi,\] (28)

i.e., \(H(\pi_{1}^{*})\) is lower-bounded as well.

Having in mind our previous Corollary, we just substitute \(\exp\big{(}\frac{f^{*}(y)}{\epsilon}\big{)}\) of LSE (15) potential \(f^{*}\) as the Schrodinger potential \(\phi^{*}(y)\) to (12). We derive

\[v^{*}(x,t)=\epsilon\nabla\log\int_{\mathbb{R}^{D}}\mathcal{N}(y|x,(1-t)\epsilon I)\varphi^{*}(y)dy=\] \[\epsilon\nabla\log\int_{\mathbb{R}^{D}}\mathcal{N}(y|x,(1-t) \epsilon I)\exp(\frac{f^{*}(y)}{\epsilon})dy=\] \[\epsilon\nabla\log\int_{\mathbb{R}^{D}}\mathcal{N}(y|x,(1-t) \epsilon I)\exp(\frac{\epsilon\log\sum_{n=1}^{N}w_{n}\mathcal{Q}(y|b_{n}, \epsilon^{-1}A_{n})}{\epsilon})dy=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}} \mathcal{N}(y|x,(1-t)\epsilon I)\mathcal{Q}(y|b_{n},\epsilon^{-1}A_{n}))dy=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}} \big{(}2\pi\epsilon(1-t)\big{)}^{-\frac{D}{2}}\exp(-(y-x)^{T}\frac{I}{2 \epsilon(1-t)}(y-x))\mathcal{Q}(y|b_{n},\epsilon^{-1}A_{n})dy=\]\[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}}\exp(-(y-x)^{T} \frac{I}{2\epsilon(1-t)}(y-x))\mathcal{Q}(y|b_{n},\epsilon^{-1}A_{n})dy+\] \[\underbrace{\epsilon\nabla\log\left(\big{(}2\pi\epsilon(1-t)\big{)} ^{-\frac{D}{2}}\right)}_{=0}=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}}\exp(-( y-x)^{T}\frac{I}{2\epsilon(1-t)}(y-x))\exp(-(y-b_{n})^{T}\frac{A_{n}}{2\epsilon}(y-b _{n}))dy=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}}\exp \big{(}-\frac{1}{2(1-t)}\{(y-x)^{T}\frac{I}{\epsilon}(y-x)+(y-b_{n})^{T}\frac{ \overbrace{(1-t)A_{n}}^{A_{n}^{t}}}{\epsilon}(y-b_{n})\}\big{)}dy\]

Next, we use (22) but with \(A_{n}^{t}\) instead of \(A_{n}\) and \(\Sigma_{n}^{t}\) instead of \(\Sigma_{n}\). Also, we denote \(\mu_{n}^{t}=(A_{n}^{t}+I)^{-1}(A_{n}^{t}b_{n}+x)\):

\[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}}\exp \Big{(}-\frac{1}{2(1-t)}\{(y-x)^{T}\frac{I}{\epsilon}(y-x)+(y-b_{n})^{T} \frac{\overbrace{(1-t)A_{n}}^{A_{n}^{t}}}{\epsilon}(y-b_{n})\}\big{)}dy=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\int_{\mathbb{R}^{D}}\exp \bigg{(}-\frac{1}{2(1-t)}\{(y-\mu_{n}^{t})^{T}\left(\Sigma_{n}^{t}\right)^{- 1}(y-\mu_{n}^{t})+\] \[(x-b_{n})^{T}(\frac{I}{\epsilon}-\frac{\Sigma_{n}^{t}}{\epsilon^ {2}})(x-b_{n})\}\bigg{)}dy=\] \[\epsilon\nabla\log\sum_{n=1}^{N}\Big{\{}w_{n}\exp\big{(}-\frac{1 }{2}(x-b_{n})^{T}\frac{\epsilon I-\Sigma_{n}^{t}}{\epsilon^{2}(1-t)}(x-b_{n}) \big{)}\] \[\int_{\mathbb{R}^{D}}\exp\big{(}-\frac{1}{2}(y-\mu_{n}^{t})^{T} \frac{\left(\Sigma_{n}^{t}\right)^{-1}}{(1-t)}(y-\mu_{n}^{t})\big{)}dy\Big{\}}=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\mathcal{Q}\big{(}x\big{|}b_ {n},\frac{\epsilon I-\Sigma_{n}^{t}}{\epsilon^{2}(1-t)}\big{)}\int_{\mathbb{R }^{D}}(2\pi)^{\frac{D}{2}}\det((1-t)\Sigma_{n}^{t})^{\frac{1}{2}}\mathcal{N}( y|\mu_{n}^{t},(1-t)\Sigma_{n}^{t})dy=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\mathcal{Q}\big{(}x\big{|}b_ {n},\frac{\epsilon I-\Sigma_{n}^{t}}{\epsilon^{2}(1-t)}\big{)}(2\pi(1-t))^{ \frac{D}{2}}\det(\Sigma_{n}^{t})^{\frac{1}{2}}\underbrace{\int_{\mathbb{R}^{D }}\mathcal{N}(y|\mu_{n}^{t},(1-t)\Sigma_{n}^{t})dy}_{=1}=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\mathcal{Q}\big{(}x\big{|}b_ {n},\frac{\epsilon I-\Sigma_{n}^{t}}{\epsilon^{2}(1-t)}\big{)}(2\pi(1-t))^{ \frac{D}{2}}\det(\Sigma_{n}^{t})^{\frac{1}{2}}=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\mathcal{Q}\big{(}x\big{|}b_ {n},\frac{\epsilon I-\Sigma_{n}^{t}}{\epsilon^{2}(1-t)}\big{)}\det(\Sigma_{n}^ {t})^{\frac{1}{2}}+\underbrace{\epsilon\nabla\log\big{(}(2\pi(1-t))^{\frac{D}{ 2}}\big{)}}_{=0}=\] \[\epsilon\nabla\log\sum_{n=1}^{N}w_{n}\sqrt{\det(\Sigma_{n}^{t})} \mathcal{Q}\big{(}x\big{|}b_{n},\frac{\epsilon I-\Sigma_{n}^{t}}{\epsilon^{2}(1- t)}\big{)},\]

which finishes the proof. 

## Appendix B Mixtures Benchmark Pairs: Details and Results

**Parameters for constructing benchmark pairs.** In our benchmark pairs, we choose all their hyperparameters manually to make sure the constructed distributions \(\mathbb{P}_{0},\mathbb{P}_{1}\) are visually pleasant and distinguishable. As \(\mathbb{P}_{0}\), we always use the centered Gaussian whose covariance matrix is \(0.25I\). We use LSE function (15) with \(N=5\) for constructing the distribution \(\mathbb{P}_{1}\). In each setup, all \(A_{n}\) are the same and given in Table 3. We pick \(w_{n}\) such that \(\gamma_{n}=\frac{1}{5}\mathcal{N}(x|b_{n},(\frac{1}{e}I-\frac{1}{e^{2}}\Sigma_ {n})^{-1})\). We sample \(b_{n}\) randomly from a uniform distribution on a sphere with the radius \(R=5\).

**Evaluation details.** For computing \(\text{B}\mathbb{W}_{2}^{2}\text{-UVP}(\widehat{\pi}_{1},\mathbb{P}_{1})\), we use \(10^{5}\) random samples from \(\mathbb{P}_{1}\) and \(10^{5}\) random samples from learned distribution \(\widehat{\pi}_{1}\). For computing \(\text{c}\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\big{(}\widehat{\pi},\pi^{*} \big{)}\), we use the hold-out test set containing \(1000\) samples \(x\sim\mathbb{P}_{0}\). We compute the expectation and covariance matrices of \(\pi^{*}(\cdot|x)\) analytically (Proposition 3.3) and we estimate the expectation and covariance matrix of \(\widehat{\pi}(\cdot|x)\) by using \(10^{3}\) samples. We present results of evaluation in Table 4 and Table 5.

We present an additional _trivial_ baseline for the conditional metric \(\text{c}\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\big{(}\widehat{\pi},\pi^{*} \big{)}\), which is given by the independent plan \(\mathbb{P}_{0}\times\mathbb{P}_{1}\). We compare other methods with this baseline in Table 5.

**Colors for the Table 2.** To assign a color for the metric \(\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\) and \(\text{c}\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\) for each \(\epsilon\) in the Table 2, we use the following rule: we assign the rank \(1\) if a method's metric for a given dimension \(D\) has the color green, the rank \(2\) if a method's metric \(\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\) has the color orange and the rank \(3\) if a method's metric \(\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\) has the color red. To get the average rank, we take the mean of \(4\) ranks obtained for each dimension \(D\) and round it (\(1.5\) and \(2.5\) are rounded to \(1\) and \(2\) respectively).

**Extra qualitative results of EOT/SB solvers**. In Figure 5 and Figure 6, we present the additional qualitative comparison of solvers on our mixtures benchmark pairs in \(D=16\) with \(\epsilon\in\{0.1,10\}\). The figures are designed similarly to Figure 3 for \((D,\epsilon)=(16,1)\) in the main text. Note that case \(\epsilon=10\) (Figure 6) is extremely challenging; only \(\lfloor\text{\bf EgNOT}\rceil\) provides more-or-less reasonable results.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{\(\epsilon\!=\!0.1\)} & \multicolumn{4}{c}{\(\epsilon\!=\!1\)} & \multicolumn{4}{c}{\(\epsilon\!=\!10\)} \\ \cline{2-13}  & \(D\!=\!2\) & \(D\!=\!16\) & \(D\!=\!64\) & \(D\!=\!128\) & \(D\!=\!2\) & \(D\!=\!16\) & \(D\!=\!64\) & \(D\!=\!128\) & \(D\!=\!2\) & \(D\!=\!16\) & \(D\!=\!64\) & \(D\!=\!128\) \\ \hline \(\lfloor\text{\bf LSOT}\rceil\) & - & - & - & - & - & - & - & - & - & - & - & - \\ \(\lfloor\text{\bf SCOMES}\rceil\) & - & - & - & - & - & - & - & - & - & - & - & - \\ \(\lfloor\text{\bf SCOMES}\rceil\) & - & - & - & - & - & - & - & - & - & - & - & - \\ \(\lfloor\text{\bf SCOMES}\rceil\) & - & - & - & - & - & - & - & - & - & - & - & - \\ \(\lfloor\text{\bf EGNOT}\rceil\) & 0.016 & 0.63 & 1.53 & 2.62 & 0.08 & 1.13 & 1.62 & 2.62 & 0.225 & 2.603 & 1.872 & 6.12 \\ \(\lfloor\text{\bf EgNOT}\rceil\) & 0.09 & 0.31 & 0.88 & 0.22 & 0.46 & 0.3 & 0.85 & 0.12 & 0.077 & 0.02 & 0.15 & 0.23 \\ \(\lfloor\text{\bf ENOT}\rceil\) & 0.2 & **2.9** & **1.8** & 1.4 & 0.22 & 0.4 & 7.8 & 29 & 1.2 & 2 & 18.9 & 28 \\ \(\lfloor\text{\bf MLE\_SB}\rceil\) & 0.01 & 0.14 & 0.97 & 2.08 & 0.005 & 0.09 & 0.56 & **1.46** & 0.01 & 1.02 & 6.65 & 23.4 \\ \(\lfloor\text{\bf DIMSB}\rceil\) & **2.85** & **2.81** & 153.22 & 232.67 & 0.87 & 0.99 & 1.12 & 1.56 & - & - & - & - \\ \(\lfloor\text{\bf FB-SDE-A}\rceil\) & 2.37 & 2.55 & **68.19** & 27.11 & 0.6 & 0.63 & 0.65 & 0.71 & - & - & - & - \\ \(\lfloor\text{\bf FB-SDE-J}\rceil\) & 0.03 & 0.05 & 0.25 & **2.96** & 0.07 & 0.13 & 1.52 & 0.48 & - & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparisons of \(\text{B}\mathbb{W}_{2}^{2}\text{-UVP}\downarrow\) (%) between the target \(\mathbb{P}_{1}\) and learned marginal \(\pi_{1}\). Colors indicate the metric value: \(\text{BW}_{2}^{2}\text{-UVP}\leq 0.5,\text{BW}_{2}^{2}\text{-UVP}\in(0.5,1],\text{BW}_{2}^{2}\text{-UVP}>1.0\).

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & \(D=2\) & \(D=16\) & \(D=64\) & \(D=128\) \\ \hline \(\epsilon=0.1\) & \(\frac{1}{16}I\) & \(\frac{1}{16}I\) & \(\frac{1}{16}I\) & \(\frac{1}{16}I\) \\ \hline \(\epsilon=1\) & \(\frac{1}{16}I\) & \(\frac{1}{16}I\) & \(\frac{1}{16}I\) & \(\frac{1}{16}I\) \\ \hline \(\epsilon=10\) & \(\frac{1}{160

**Computational complexity**. Sampling from \(\mathbb{P}_{0}\) is lightspeed as it is just sampling a Normal noise. Sampling from \(\mathbb{P}_{1}\) is also fast, as it is the Gaussian mixture (Proposition 3.3).

## Appendix C Images Benchmark Pairs: Details and Results

**Parameters for constructing image benchmark pairs.** We fix \(N=100\) random samples from \(\mathbb{P}_{0}\) for \(b_{n}\) and choose all \(A_{n}\equiv I\). We use \(w_{n}\) such that \(\gamma_{n}=\frac{1}{100}\mathcal{N}(x|b_{n},(\frac{1}{\epsilon}I-\frac{1}{ \epsilon^{2}}\Sigma_{n})^{-1})\).

**GLOW details**. We use the code from the repository with the default parameters:

https://github.com/rosinality/glow-pytorch

After training, the latent variable \(z\) is sampled from \(N(0,\sigma^{2}I)\) with \(\sigma^{2}=0.49\) for image generation. That is, the image distribution \(\mathbb{P}_{0}\) is produced by the mapping \(z\sim N(0,\sigma^{2}I)\) to the image space with the learned normalizing flow \(G\), i.e., \(\mathbb{P}_{0}\stackrel{{\text{def}}}{{=}}G\sharp\mathcal{N}( \cdot|\sigma^{2}I)\) in our construction.

Figure 5: Qualitative results of EOT/SB solvers on our mixtures benchmark pair with \((D,\epsilon)=(16,0.1)\). The distributions are visualized using \(2\) PCA components of target distr. \(\mathbb{P}_{1}\).

Figure 6: Qualitative results of EOT/SB solvers on our mixtures benchmark pair with \((D,\epsilon)=(16,10)\). The distributions are visualized using \(2\) PCA components of target distr. \(\mathbb{P}_{1}\).

**MCMC in the latent space of the normalizing flow.** We test EOT/SB solvers in \(\mathbb{P}_{1}\rightarrow\mathbb{P}_{0}\) direction, i.e., recovering \(\pi^{*}(x|y)\) and generating clean samples \(x\) from noised \(y\). Unfortunately, the reverse conditional OT plans \(\pi^{*}(x|y)\) are not as tractable as \(\pi^{*}(y|x)\). However, we note that

\[\frac{d\pi^{*}(x|y)}{dy}\propto\frac{d\pi^{*}(y|x)}{dy}\frac{d\mathbb{P}_{0}(x )}{dx},\] (29)

i.e., the density of \(\pi^{*}(\cdot|y)\) it known up to the normalizing constant. Recall that here \(\mathbb{P}_{0}\) is constructed using the normalizing flow and \(\pi^{*}(\cdot|x)\) is a Gaussian mixture (Proposition 3.3), i.e., we indeed know the values of both terms. Therefore, one may use the well-celebrated Langevin dynamics to sample from \(\pi^{*}(y|x)\). Unfortunately, we found that such sampling in the image space is rather slow.

To overcome this issue, we employ the Langevin sampling in the latent space of the normalizing flow. It is possible since the normalizing flow is a bijection between the space of images and the latent space. We use the standard notation \(z\) for the latent variable and \(G:\mathbb{R}^{D}\rightarrow\mathbb{R}^{D}\) for the normalizing flow, i.e., \(x=G(z)\sim\mathbb{P}_{0}\) for \(z\sim p(z)\stackrel{{\text{def}}}{{=}}\mathcal{N}(z|0,\sigma^{2 }I)\). In this case, we have

\[\frac{d\pi^{*}(z|y)}{dz}=\frac{d\pi^{*}(x|y)}{dx}|\det J_{G^{-1}}( x)|\propto\frac{d\pi^{*}(y|x)}{dy}\frac{d\mathbb{P}_{0}(x)}{dx}|\det J_{G^{-1}}( x)|=\] \[\frac{d\pi^{*}(y|G(z))}{dx}\underbrace{\frac{d\mathbb{P}_{0}(x)}{ dx}|\det J_{G^{-1}}(x)}_{p(z)}|=\frac{d\pi^{*}\big{(}y|G(z)\big{)}}{dy}p(z),\]

and we can derive the _score function_\(\nabla_{z}\log\frac{d\pi^{*}(z|y)}{dz}\) which is needed for the Langevin dynamic as

\[\nabla_{z}\log\frac{d\pi^{*}(z|y)}{dz}=\nabla_{z}\frac{d\pi^{*}(y|G(z))}{dy}+ \nabla_{z}\log p(z).\] (30)

Hence, instead of doing non-trivial Langevin in the data space with \(\nabla_{x}\frac{d\pi^{*}(x|y)}{dx}\), one may equivalently do the sampling in the latent space by using the score (30) and then get \(x=G(z)\). We empirically found this approach works much better, presumably due to the fact that (30) is just the score of the Normal distribution which is slightly adjusted with the information coming from \(\pi^{*}\big{(}y|G(z)\big{)}\).

For sampling, we employ the **Metropolis-adjusted Langevin algorithm** with the time steps \(10^{-3},10^{-4}\) and \(10^{-5}\) for \(\epsilon=10\), \(\epsilon=1\) and \(\epsilon=0.1\), respectively. It provides the theoretical guarantees that the constructed Markov chain \(z_{1},z_{2},\ldots,...\) converges to the distribution \(\frac{d\pi^{*}(z|y)}{dz}\). For initializing the Markov chain, we sample a pair \((x,y)\sim\pi^{*}\) and use \(z=G^{-1}(x)\) as the initial state for the Langevin sampling to get new samples from \(\pi^{*}(\cdot|y)\). This trick allows for improving the stability of sampling and the convergence speed since it provides a good starting point. We use \(N=200\) steps for all the setups for the Metropolis-adjusted Langevin algorithm.

In Figures 7 and 8, we provide additional examples of the samples from the ground truth plan \(\pi^{*}\).

**Metric 1.** For each \(\epsilon=0.1,1,10\) we prepare a test set with \(10^{4}\) samples from \(\mathbb{P}_{0}\). We use this set to calculate the FID [24] metric between the ground truth distribution \(\mathbb{P}_{0}\) and the model's marginal distribution \(\pi_{1}\) to estimate how well the model restores the target distribution. This allows to access the generative performance of solvers, i.e., the quality of generated images and matching the target distribution. However, _this metric does not assess the accuracy of the recovered EOT plan._

**Metric 2.** For each \(\epsilon=0.1,1,10\), we prepare a test set containing \(100\) "noised" samples \(y\sim\mathbb{P}_{1}\) and \(5\)K samples \(x\sim\pi^{*}(\cdot|y)\) for each "noised" sample \(y\), i.e., \(5\)K \(\times 100\) images for each \(\epsilon\) in consideration. We propose to compute **conditional FID** to evaluate the difference between the conditional plans \(\pi^{*}(\cdot|y)\) and \(\widehat{\pi}(\cdot|y)\). That is, for each \(y\) we compute FID between \(\pi^{*}(\cdot|y)\) and \(\widehat{\pi}(\cdot|y)\), and then average the result for all test \(y\). Clearly, such an evaluation is approximately 100\(\times\)times times more consuming than computing the base FID. However, _it allows us to fairly assess the quality of the recovered EOT solution, and we recommend this metric as the main for future EOT/SB studies_.

In Tables 6, 7, we present the evaluation results for \(\lfloor\textbf{ENOT}\rceil\)[23]. We again emphasize that, to the best of our knowledge, there is no scalable _data_\(\rightarrow\)_data_ EOT/SB solver to compare against. Hence, we report the results as-is for future methods to be able to compare with them as the baseline.

**Computational complexity**. Sampling \(x\sim\mathbb{P}_{0}\) is just applying the trained GLOW neural network to noise vectors \(z\sim\mathcal{N}(\cdot|0,\sigma^{2}I)\). Sampling \(y\sim\mathbb{P}_{1}\) (or \(y|x\)) takes comparable time, as it is just extrasampling from the Gaussian mixture with \(x\)-dependent parameters (Proposition 3.3). In turn, as we noted above, sampling \(x|y\) requires using the Langevin dynamic and takes considerable time. To obtain 3 **test** sets of \(5\)K samples \(y\sim\pi^{*}(\cdot|x)\) per each of 100 samples \(x\sim\mathbb{P}_{0}\), we employed \(8\times\)A100 GPUs. This generation of test datasets took approximately \(1\) week.

Figure 8: Ground truth samples \(y\sim\pi^{*}(\cdot|x)\) on images benchmark pairs.

Figure 7: Ground truth samples \(x\sim\pi^{*}(\cdot|y)\) on images benchmark pairs.

[MISSING_PAGE_FAIL:25]

for approximately 50K iterations and reporting the best-obtained metric. We understand that such an evaluation procedure is not ideal and does not provide statistically significant results. However, the qualitative results reported in Table 2 seem to show the behaviour of \(\lfloor\textbf{EgNOT}\rceil\) solver on our benchmark setup and reveal the key properties of the approach.

\(\lfloor\textbf{ENOT}\rceil\)[23] We use the official code from

https://github.com/ngushchin/EntropicNeuralOptimalTransport

We use the same hyperparameters for this setup as the authors [23, Appendix E], except the number of discretization steps N, which we set to 200 as well as for other Schrodinger Bridge based methods. We also change the learning rate of the potential to \(3\cdot 10^{-4}\) for the setups with \(\epsilon=10\).

\(\lfloor\textbf{MLE-SB}\rceil\)[52]. We tested the official code from

https://github.com/fraciscovargas/GP_Sinkhorn

Instead of Gaussian processes, we used a neural network as for \(\lfloor\textbf{ENOT}\rceil\). We use \(N=200\) discretization steps as for other SB solvers, \(5000\) IPF iterations, and \(512\) samples from distributions \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\) in each of them. We use the Adam optimizer with \(lr=10^{-4}\) for optimization.

\(\lfloor\textbf{DiffSB}\rceil\)[15]. We utilize the official code from

https://github.com/JTT94/diffusion_schrodinger_bridge

with their configuration blob/main/conf/dataset/2d.yaml for toy problems. We increase the number of steps of dynamics to 200 and the number of steps of the IPF procedure for dimensions 16, 64 and 128 to 30, 40 and 60, respectively.

\(\lfloor\textbf{FB-SDE-J}\rceil\)[9]. We utilize the official code from

https://github.com/ghiu/SB-FBSDE

with their configuration blob/main/configs/default_checkerboard_config.py for the checkerboard-to-noise toy experiment, changing the number of steps of dynamics from 100 to 200 steps. Since their hyper-parameters are developed for their 2-dimensional experiments, we increase the number of iterations for dimensions 16, 64 and 128 to 15 000.

\(\lfloor\textbf{FB-SDE-A}\rceil\)[9]. We also take the code from the same repository as above. We base our configuration on the authors' one (blob/main/configs/default_moon_to_spiral_config.py) for the moon-to-spiral experiment. As earlier, we increase the number of steps of dynamics up to 200. Also, we change the number of training epochs during one IPF procedure for dimensions 16, 64 and 128 to 2,4 and 8 correspondingly.

### Images Benchmark Pairs

\(\lfloor\textbf{ENOT}\rceil\)[23] As well as for the mixtures benchmark pairs, we use the official code from

https://github.com/ngushchin/EntropicNeuralOptimalTransport

We use the same hyperparameters for this setup as the authors [23, Appendix F] except the batch size which we set to 16 (/blob/main/notebooks/Image_experiments.ipynb).

## Appendix E Additional Study of Hyperparameters of Solvers

To show that the default solvers parameters described in Appendix D are already a good choice, we additionally try different values of some of the most important hyperparameters. We consider each of the solvers except \(\lfloor\textbf{LSOT}\rceil\) because it is anyway known to poorly perform due to the systematic bias in its solutions [31, 32]. For the evaluation, we consider the mixtures benchmark pair with \(D=64\) and \(\epsilon=1\) where most of the solvers perform reasonably well. In the tables below, we use "\(*\)" to mark the hyperparameters that we use for comparisons in SS4.1.

For \(\lfloor\textbf{ENOT}\rceil\) solver, we consider the number of inner and outer problem iterations during the optimization and present the results in Table 10. The obtained results show that the performance increases slowly with increasing number of iterations of both types.

For IPF-based SB solvers \(\left[\textbf{{MLE-SB}}\right],\left[\textbf{{DiffSB}}\right],\left[\textbf{{FB-SDE-A}}\right]\) and \(\left[\textbf{{FB-SDE-J}}\right]\), we try different numbers of IPF iterations and the number of samples used in each iteration. We present the results in Tables 11, 12, 13, 14. All of the IPF-based solvers learn an inversion of a diffusion process at each IPF step but they differ in the way how this is done. The typical number of IPF steps used by each algorithm is affected by this difference. The performance increases slowly with the increase of the two hyperparameters considered, at the cost of a proportional increase in iterations or in the number of samples used.

For \(\left[\textbf{{SCONES}}\right]\) and \(\left[\textbf{{EgNOT}}\right]\) solvers, we consider the number of Langevin steps and the Langevin step size and present the results in Table 15 and Table 16. For \(\left[\textbf{{SCONES}}\right]\) the results obtained show that the performance increases slowly with increasing Langevin steps and decreasing Langevin step size. For \(\left[\textbf{{EgNOT}}\right]\) the trends are slightly different, since the optimal Langevin step size seems to be in the interval \([0.1,0.2]\). Anyway, our selected parameters are reasonable ones because specifying an enormously large number of Langevin steps for these solvers is sort of impractical.

Finally, for \(\left[\textbf{{NOT}}\right]\) we consider the number of inner problem steps and the hidden size of the used neural network (conditional normalizing flow). We present results in Table 17.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline 
\begin{tabular}{c} IPF iters \\ \end{tabular} & \(64\) & \(256\) & \(512\) \\ \hline \(15000\) & \(173.16\) & \(163.04\) & \(160.5\)* \\ \hline \(30000\) & \(168.86\) & \(165.06\) & \(\textbf{156.5}\) \\ \hline \end{tabular}
\end{table}
Table 13: Comparison of cB\(\mathbb{W}_{2}^{2}\)-UVP \(\downarrow\) (%) for \(\left[\textbf{{FB-SDE-J}}\right]\) on mixtures benchmark pairs for \(D=64\), \(\epsilon=1\) and different hyperparameters.

**Discussion.** From the results it can be seen that for the most solvers' dependence on the considered hyperparameters is almost monotonic and the hyperparameters chosen for the solver comparison on the mixtures setup are in the region where the metric growth is almost saturated.

## Appendix F Qualitative Evaluation of the Drift Learned with SB methods

Our benchmark primarily aimed at quantifying the recovered **conditional EOT plan**\(\widehat{\pi}(\cdot|x)\). Thanks to our Proposition 3.5, our benchmark provides not only the ground truth conditional EOT plan \(\pi^{*}(\cdot|x)\), but the **optimal SB drift**\(v^{*}(x,t)\) as well. This means that for SB solvers we may additionally compare their recovered SB drift \(\widehat{v}\) with the ground truth drift \(v^{*}\). Here we do this for \([\)**MLE-SB\(],[\)DiffSB\(],[\)ENOT\(],[\)FB-SDE-A\(],[\)FB-SDE-J\(]\)** solvers by using our mixtures pairs.

Metrics.Recall that \(T_{v^{*}}\) is the Schrodinger bridge (10) and let \(T_{\widehat{v}}\) denote the learned process:

\[dX_{t}=\widehat{v}(x,t)dt+\sqrt{\epsilon}dW_{t},\qquad X_{0}\sim\mathbb{P}_{0}.\]

Both \(T_{v^{*}}\) and \(T_{\widehat{v}}\) are diffusion processes which start at distribution \(\mathbb{P}_{0}\) at \(t=0\) and have fixed volatility \(\epsilon\). Their respective drifts are \(v^{*}\) and \(\widehat{v}\). For each time \(t\in[0,1]\), consider

\[\mathcal{L}^{2}_{\text{fwd}}[t]\stackrel{{\text{def}}}{{=}} \mathbb{E}_{T_{v^{*}}}\|v^{*}(X_{t},t)-\widehat{v}(X_{t},t)\|^{2},\] (31)

\[\mathcal{L}^{2}_{\text{rev}}[t]\stackrel{{\text{def}}}{{=}} \mathbb{E}_{T_{\widehat{v}}}\|v^{*}(X_{t},t)-\widehat{v}(X_{t},t)\|^{2}.\] (32)

which are the expected squared differences between the ground truth \(v^{*}\) and learned \(\widehat{v}\) drifts at the time \(t\). In (31), the expectation is w.r.t. \(X_{t}\) coming from the true SB trajectories of \(T_{v^{*}}\), while in (32) - w.r.t. the learned trajectories from \(T_{\widehat{v}}\). Reporting this metric for all the time steps, all the mixtures pairs and solvers would be an overkill. In what follows, we use this metric for quantitative analysis.

**First**, for \(D=16\) and \(\epsilon\in\{0.1,10\}\), we plot these metrics (as a function of time \(t\)). The results for all the solvers are shown in Figure 9. **Second**, we provide Table 19 where for \(D\in\{2,16,64,128\}\) and \(\epsilon\in\{0.1,10\}\) report \(\mathcal{L}^{2}\) metrics averaged over \(t\in[0,1]\). Namely, we report

\[\text{KL}\left(T_{v^{*}}\|T_{\widehat{v}}\right)\stackrel{{ \text{def}}}{{=}}\frac{1}{2\epsilon}\int_{0}^{1}\mathcal{L}^{2}_{ \text{fwd}}[t]dt\qquad\text{and}\qquad\text{RKL}\left(T_{v^{*}}\|T_{\widehat {v}}\right)\stackrel{{\text{def}}}{{=}}\frac{1}{2\epsilon}\int_{0 }^{1}\mathcal{L}^{2}_{\text{rev}}[t]dt.\] (33)

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Langevin step size & Langevin steps & \(100\) & \(200\) & \(500\) & \(1000\) \\ \hline \(0.01\) & \(70.9\) & \(70.98\) & \(72.9\) & \(68.13\) \\ \hline \(0.02\) & \(71.31\) & \(67.14\) & \(69.11\) & \(69.02\) \\ \hline \(0.05\) & \(68.78\) & \(68.59\) & \(63.73\)\(\ast\) & \(56.84\) \\ \hline \(0.1\) & \(64.52\) & \(57.45\) & \(52.35\) & \(51.9\) \\ \hline \(0.2\) & \(58.22\) & \(60.08\) & \(58.93\) & \(\mathbf{41.31}\) \\ \hline \end{tabular}
\end{table}
Table 16: Comparison of c\(\text{B}\mathbb{W}^{2}_{2}\)-UVP \(\downarrow\) (%) for \(\lfloor\)**EqNOT** on mixtures benchmark pairs for \(D=64\), \(\epsilon=1\) and different hyperparameters.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline Inner steps & Hidden size & \(64\) & \(128\) & \(192\) & \(256\) & \(320\) & \(384\) & \(448\) & \(512\) \\ \hline
1 & \(93.14\) & \(167.05\) & \(149.52\) & \(189.0\) & \(89.1\) & \(161.66\) & \(176.43\) & \(175.67\) \\ \hline
5 & \(82.64\) & \(86.09\) & \(82.18\) & \(190.04\) & \(147.31\) & \(105.46\) & \(103.5\) & \(150.76\) \\ \hline
10 & \(163.47\) & \(146.68\) & \(53.26\) & \(137.47\) & \(100.84\) & \(171.65\) & \(115.84\) & \(126.96\) \\ \hline
100 & \(18.68\) & \(21.4\) & \(\mathbf{14.64}\) & \(18.08\) & \(16.66\) & \(20.64\ast\) & \(18.71\) & \(15.15\) \\ \hline
200 & \(61.99\) & \(52.74\) & \(58.63\) & \(53.89\) & \(52.44\) & \(55.3\) & \(55.02\) & \(54.75\) \\ \hline \end{tabular}
\end{table}
Table 17: Comparison of c\(\text{B}\mathbb{W}^{2}_{2}\)-UVP \(\downarrow\) (%) for \(\lfloor\)**NOT** on mixtures benchmark pairs for \(D=64\), \(\epsilon=1\) and different hyperparameters.

[MISSING_PAGE_FAIL:29]

to explain them. We hope that the question of the interpretation of the KL and RKL values will be addressed in future SB studies.

## Appendix G Potential Societal Impact

Our proposed approach deals with generative models based on Entropic Optimal Transport and Schrodinger Bridge principles. Such models form and emergent subarea in the field of machine learning research and could be used for various purposes in the industry including image manipulation, artificial content rendering, graphical design, etc. Our benchmark is a step towards improving the reliability, robustness and transparency of these models. One potential negative of our work is that improving generative models may lead to transforming some jobs in the industry.

## Appendix H Building Benchmarks from Real Data

In this section, we present a simple heuristic recipe to build benchmark pairs similar to some given real-world data. To illustrate the recipe, we consider toy 2D data example and several single-cell datasets [36, 8]. Code and data for the experiments in this section can be found in the benchmark_construction_examplesdata folder of our repository.

### Recipe for Building Benchmark Pairs form Data.

For constructing distribution pairs similar to some given data, we consider a pair of original and target datasets obtained from the true distributions \(\mathbb{P}_{0}\) and \(\mathbb{P}_{1}\), respectively. We heuristically initialize the LSE potential (15) \(f^{*}(y)=\epsilon\log\sum_{n=1}^{N}w_{n}\mathcal{Q}(y|b_{n},\epsilon^{-1}A_{n})\) with \(b_{n}\) as cluster centers obtained from the K-means clustering algorithm applied to the target data from \(\mathbb{P}_{1}\). The weights \(w_{n}\) are chosen to be \(1/N\) and matrices \(A_{n}=\lambda I\) are diagonal where \(\lambda\) is a manually-chosen parameter (shared between all \(A_{n}\)). For any \(x\) the conditional plan \(\pi^{*}(\cdot|x)\) for LSE potential \(f^{*}\) is just a Gaussian mixture and the mean of each its component is largely determined by \(b_{n}\) (Proposition 3.3). We empirically found that the resulting constructed distribution \(d\widehat{\mathbb{P}}_{1}(y)=d\pi_{1}^{*}(y)=\int d\pi^{*}(y|x)d\mathbb{P}_{0} (x)\) from \(\mathbb{P}_{0}\) resembles the Gaussian mixture approximation of the target dataset if one managed to find proper value of \(\lambda\).

In the rest of this section, we use the described recipe to construct benchmark pairs from data to show that the LSE parameterization of the potential provides a wide class of EOT/SB solutions and even allows constructing a benchmark similar to real data.

### Benchmark Pairs for 2D data.

Code and data for the experiment described in this section can be found in the folder benchmark_construction_examples/2d_data of our repository.

To begin with, we present the results of constructing a benchmark pair from 2D data. We consider a Gaussian distribution \(\mathbb{P}_{0}\) as the source distribution and two moons \(\mathbb{P}_{1}\) as the target distribution. We aim to use the previously described recipe SSH.1 to find parameters of the LSE potential to construct an EOT solution between \(\mathbb{P}_{0}\) and an approximation of \(\mathbb{P}_{1}\) denoted as \(\widehat{\mathbb{P}}_{1}\). Here we consider EOT with \(\epsilon=0.05\), use \(N=100\) for LSE potentials, and choose \(\lambda=50\). The result is in Figure 10.

As seen from the figure, the constructed target benchmark distribution \(\widehat{\mathbb{P}}_{1}\) is similar to the target distribution \(\mathbb{P}_{1}\). In turn, the EOT plan maps \(x\sim\mathbb{P}_{0}\) to the close regions of the target distribution.

### Single-cell RNA Data

Code and data for the experiment described in this section can be found in the folder benchmark_construction_examples/single_cell_rna of our repository.

We consider the same setup as in [36, SS5.2]. We use their data from the supplementary materials.4 The provided data displays the progression of human embryonic stem cells as they differentiate from embryoid bodies into a range of cell types, such as mesoderm, endoderm, neuroectoderm, and neural crest, throughout a span of 27 days. The cell samples (approximately \(2000\) ones per each time period) were gathered at five distinct intervals (\(t_{0}\): day 0 to 3, \(t_{1}\): day 6 to 9, \(t_{2}\): day 12 to 15, \(t_{3}\): day 18 to 21, \(t_{4}\): day 24 to 27). These collected cells were evaluated via scRNAseq, subjected to quality control filtering, and then projected onto a \(5\)-dimensional feature space utilizing principal component analysis (PCA).

To construct the benchmark pair using the LSE potential, we consider \(N=250\), \(\epsilon=100\) and \(\lambda=100\) and employ the train data at times \(t_{0}\) and \(t_{4}\). Then we use the constructed benchmark plan \(\pi^{*}(\cdot|x)\) to map source data at time \(t_{0}\) to the data at time \(t_{4}\) and obtain benchmark target distribution samples \(\widehat{\mathbb{P}}_{1}\). Finally, we fit TSNE [51] to the combined dataset of samples from \(\mathbb{P}_{1}\) and \(\widehat{\mathbb{P}}_{1}\) and then plot their projections in Figure 11. The resulting plots are very similar, confirming that the constructed benchmark target data resembles the considered single-cell target data.

### Single-cell Drugs Data

Code and data for the experiment described in this section can be found in the folder benchmark_construction_examples/single_cell_drugs of our repository.

In [8], the authors consider the problem of predicting single-cell drug responses for drugs with different molecular effects, using melanoma cell lines profiled by 4i technology (single-cell technology). Utilizing a blend of two melanoma tumor cell lines at a 1:1 ratio, a total of 21,650 cells were imaged. Within this dataset, 11,526 cells existed in the untreated control state, 2,364 received Erlotinib treatment, 2,650 underwent Imatinib treatment, 2,683 were subjected to T

Figure 11: TSNE visualization of Single-cell RNA target data and our constructed target data.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Method & scGen & cAE & CellOT [8] & EOT Benchmark (ours) \\ \hline MMD\(\downarrow\) & \(0.0241\) & \(0.0074\) & \(0.0013\) & \(0.0036\) \\ \hline \end{tabular}
\end{table}
Table 20: MMD\(\downarrow\) distances (on the test data) between the observed perturbed cells \(\mathbb{P}_{1}\) and predicted responses from control cells \(\widehat{\mathbb{P}}_{1}\).

Figure 10: _Gaussian \(\rightarrow\) Two Moons_ benchmark pair.

2,417 were treated with a combination of Trametinib and Erlotinib. After preprocessing, each cell is described by 78 features. The train-test split with each drug is 80:20.

In this example, we consider cell data before treatment \((\mathbb{P}_{0})\) and after treatment with Erlotinib \((\mathbb{P}_{1})\). For the construction of the benchmark pair using an LSE potential, we consider \(N=250\), \(\epsilon=1\) and \(\lambda=20\). As with the single cell RNA data \(\lx@sectionsign\)H.3, we fit the TSNE [51] on a combined dataset of samples from \(\mathbb{P}_{1}\) and \(\widehat{\mathbb{P}}_{1}\) and then plot their projections in Figure 11. As seen from the visualizations, the TSNE projections of the real data and the mapped data are similar.

In addition, we quantitatively evaluate on the test data how well the constructed target distribution \(\widehat{P}_{1}\) matches the true data distribution \(\mathbb{P}_{1}\). We employ the same MMD metric as the authors and present the results in Table 20. The data for the baselines scGen, cAE and the authors' method CellOT are taken from [8]. As one can see, our approach is even better than two of the baselines considered.

Figure 12: TSNE visualization of Single-cell Drugs target data and our constructed target data.