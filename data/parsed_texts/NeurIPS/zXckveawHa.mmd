# Statistical Limits of Adaptive Linear Models:

Low-Dimensional Estimation and Inference

 Licong Lin

Department of Statistics

University of California, Berkeley

liconglin@berkeley.edu

&Mufang Ying

Department of Statistics

Rutgers University - New Brunswick

my426@scarletmail.rutgers.edu

&Suvrojit Ghosh

Department of Statistics

Rutgers University - New Brunswick

sg1565@scarletmail.rutgers.edu

&Koulik Khamaru

Department of Statistics

Rutgers University - New Brunswick

kk1241@stat.rutgers.edu

&Cun-Hui Zhang

Department of Statistics

Rutgers University - New Brunswick

czhang@stat.rutgers.edu

###### Abstract

Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of \(\sqrt{d}\) when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity. We show that OLS or OLS on centered data can achieve this matching error. In addition, we propose a novel estimator for single coordinate inference via solving a Two-stage Adaptive Linear Estimating equation (TALE). Under a weaker form of adaptivity in data collection, we establish an asymptotic normality property of the proposed estimator.

## 1 Introduction

Estimating a low-dimensional parameter component in a high-dimensional model is a fundamental problem in statistics and machine learning that has been widely studied in e.g., semiparametric statistics [35, 8], causal inference [16, 15] and bandit algorithms [1, 27]. When data are independently and identically distributed (i.i.d.), it is often possible to derive estimators that are asymptotically normal with a rate of convergence of \(\sqrt{n}\), and that achieve the semi-parametric variance lower boundthat is independent of the dimension. There is now a rich body of literature that studies this problem under various scenarios [5, 6, 33, 36, 4, 8, 41].

In this work we are interested in the same estimation and inference problem but under the setting where the i.i.d. data assumption fails. Specifically, we consider an adaptive collection framework where the data collected at time \(i\) is allowed to be dependent on the historical data collected up to time \(i-1\). This adaptive framework incorporates datasets originated from applications in many fields, including sequential experimental design [15], bandit algorithm [27], time series modeling [7], adaptive stochastic approximation schemes [13, 24].

### An interesting lower bound

To see the intrinsic difference between the i.i.d. and the adaptive data collection settings, we consider the canonical example of linear model \(y=\bm{x}^{\top}\bm{\theta}^{*}+\varepsilon\), where the parameter \(\bm{\theta}^{*}=(\theta_{1}^{*},\bm{\theta}_{2}^{*})\in\mathbb{R}^{1}\times \mathbb{R}^{d-1}\), \(\varepsilon^{iid}\mathcal{N}(0,1)\). Clearly, when the covariates \(\{\bm{x}_{i}\}_{i\leq n}\) are deterministic, a straightforward calculation yields

\[\widehat{\theta}_{\mathrm{ols},1}-\theta_{1}^{*}\overset{d}{=}\mathcal{N}(0,(\mathbf{S}_{n}^{-1})_{11}),\quad\text{ and }\quad\mathbb{E}[(\mathbf{S}_{n}^{-1})_{11}^{-1} \cdot(\widehat{\theta}_{\mathrm{ols},1}-\theta_{1}^{*})^{2}]=1,\] (1)

where \(\widehat{\theta}_{\mathrm{ols}}\) is the OLS estimator and \(\mathbf{S}_{n}:=\sum_{t=1}^{n}\bm{x}_{i}\bm{x}_{i}^{\top}\) is the sample covariance matrix.

However, somewhat surprisingly, when the covariates \(\{\bm{x}_{i}\}_{i\leq n}\) are allowed to be collected in an _arbitrary_ adaptive manner, in a recent work [20] the authors proved the following (informal) counter-intuitive minimax lower bound on the scaled-MSE (defined in Definition 2.2)

\[\min_{\widehat{\theta}}\max_{\bm{\theta}^{*}}\mathbb{E}[(\mathbf{S}_{n}^{-1} )_{11}^{-1}\cdot(\widehat{\theta}-\theta_{1}^{*})^{2}]\geq cd\cdot\log(n),\] (2)

where the extra \(d\)-factor enters the estimation of a _single_ coordinate. This lower bound indicates that a dimension independent single coordinate estimation is _infeasible_ when the data are collected _arbitrarily_ adaptively. This is undesirable especially in the high dimensional scenario where \(d\to\infty\), since a \(\sqrt{n}\)-consistent estimation is unattainable. Motivated by the contrast between i.i.d. and adaptive data collection, we pose the following question in this work:

_Can we bridge the gap between iid and adaptive data collection, and obtain an estimator for a low-dimensional parameter component in linear models, such that its performance depends on the degree of adaptivity?_

### Contributions

In this work, we initiate the study of how the adaptivity of the collected data affects low-dimensional estimation in a high-dimensional linear model. We explore the previously posed question and provide an affirmative answer.

We begin by introducing a general data collection assumption, which we term _\((k,d)\)-adaptivity_. Broadly speaking, \((k,d)\)-adaptivity implies that the data pairs \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\in\mathbb{R}^{d}\times\mathbb{R}\) are collected in a way that the first \(k\) coordinates of \(\bm{x}_{i}\) (denoted by \(\bm{x}_{i}^{\mathrm{ad}}\)) are chosen adaptively based on the historical data, while the remaining \(d-k\) coordinates of \(\bm{x}_{i}\) (denoted by \(\bm{x}_{i}^{\mathrm{nad}}\)) are i.i.d. across time \(i\in[n]\).

Assume the collected data are \((k,d)-\)adaptive from a linear model \(y=\bm{x}^{\top}\bm{\theta}^{*}+\varepsilon\). We analyze the lower-dimensional estimation problem under the scenarios where the i.i.d. non-adaptive components \(\bm{x}_{i}^{\mathrm{nad}}\) are either zero-mean or nonzero-mean. In the zero mean case, we show that the ordinary least squares estimator (OLS) for the first \(k\)-coordinate yields a scaled mean squared error (scaled-MSE) of \(k\log(n)\) (Theorem 3.1). For the nonzero-mean case, a similar result is achieved using the OLS estimator on centered data (Theorem 3.2). Consequently, we find that the degree of adaptivity significantly impacts the performance of single coordinate estimation, in the sense that the scaled-MSE is inflated by a factor of \(k\), where \(k\) denotes the number of adaptive coordinates (see Corollary 3.3).

Although OLS for a single coordinate has a dimension independent scaled-MSE when the collected data are \((1,d)\)-adaptive, it should be noted that OLS may exhibit non-normal asymptotic behavior [13, 20] when data are adaptively collected. Therefore, we propose a novel estimator by solving a Two-stage Adaptive Linear Estimating Equation (TALE). When the collected data are \((1,d)\)-adaptive and the non-adaptive component is zero mean, we show that our new estimator is asymptotically normal and has a comparable scaled-MSE as the naive OLS estimator (see Theorem 3.4).

## 2 Problem set up

Consider a linear model

\[y=\bm{x}^{\top}\bm{\theta}^{*}+\varepsilon,\] (3)

where the parameter \(\bm{\theta}^{*}\in\mathbb{R}^{d},\) and \(\varepsilon\) is a zero mean noise variable. Given access to a data set \(\{(\bm{x}_{i},y_{i})\}_{i\leq n}\) from the model (3), we are interested in the estimation and inference problem of a low-dimensional parameter component \(\bm{\theta}^{*}_{\rm ad}\in\mathbb{R}^{k}\), where \(\bm{\theta}^{*}=(\bm{\theta}^{*\top}_{\rm ad},\bm{\theta}^{*\top}_{\rm nad})^ {\top}\).

In this paper, we are interested in adaptive data collection regime. Concretely, we assume that the data are collected adaptively in the following way

**Definition 2.1** (\((k,d)\)-adaptivity): _The collected samples \(\{(\bm{x}_{i},y_{i})\}_{i\leq n}\) forms a filtration \(\{\mathcal{F}\}_{i=0}^{\infty}\) with \(\mathcal{F}_{0}=\emptyset\) and \(\mathcal{F}_{i}=\sigma(\bm{x}_{1},y_{1},\ldots,\bm{x}_{i},y_{i})\). Let \(P\) be an unknown distribution on \(\mathbb{R}^{d-k}\). We assume that at each stage, \(i\geq 1\)_

* _The adaptive component_ \(\bm{x}^{\rm ad}_{i}=\bm{x}_{i,1:k}\) _is collected from some unknown distribution that could depend on_ \(\mathcal{F}_{i-1}\)_._
* _The non-adaptive component_ \(\bm{x}^{\rm nad}_{i}=\bm{x}_{i,k+1:d}\) _is a sample from_ \(P\) _and independent of_ \((\bm{x}^{\rm ad}_{i},\mathcal{F}_{i-1})\)_._

When \(k=0\), Definition 2.1 reduces to an i.i.d. data collection strategy; when \(k=d\), it corresponds to the case where the data are allowed to be collected arbitrarily adaptively. Consequently, \((k,d)\)-adaptivity connects two extreme scenarios, and the degree of adaptivity increases as \(k\) increases.

**Example 2.1** (Treatment assignment): _As a concrete example, consider the problem of treatment assignment to patients. At round \(i\), we observe the health profile of the patient \(i\), which we denote by \(\bm{x}_{i}\in\mathbb{R}^{d-1}\). Our job to assign a treatment \(A_{i}\in\{0,1\}\) based on the patient's health profile \(\bm{x}_{i}\) and also our prior knowledge of effectiveness of the treatments. It is natural to capture our prior knowledge using \(\mathcal{F}_{i}=\sigma(A_{1},\bm{x}_{1},y_{1},\ldots,A_{i-1},\bm{x}_{i-1},y_{ i-1})\) -- the sigma field generated by previous data-points. As already pointed out in (2), in the adaptive regime the estimator error for treatment effect scales as \(\sqrt{d/n}\) ; in words, we have to pay for a dimension factor \(\sqrt{d}\) even if we are only interested in estimating a one-dimensional component. While for our treatment assignment example, the dimension \(d-1\) of the covariate vector \(\bm{x}_{i}\) is large in practice, it is natural to assume that the treatment assignment is dependent on \(k-1\ll d-1\), a few (unknown) components. Under this assumption, it is easy to see that this treatment assignment problem is \((k,d)\)-adaptive. We show that the treatment effect can be estimated at a rate \(\sqrt{k/n}\ll\sqrt{d/n}\)._

### Statistical limits

Before we discuss how to obtain estimators for a low-dimensional parameter component of \(\theta^{\star}\), we establish some baselines by recalling existing lower bounds. Throughout this section, we assume the noise \(\epsilon_{i}\overset{iid}{\sim}\mathcal{N}(0,\sigma^{2})\). We start with defining the metric for comparison.

**Definition 2.2** (scaled mean squared error (scaled-MSE)): _Given a subset \(\mathcal{I}\subseteq[d]\). We define the scaled-MSE of an estimator \(\widehat{\bm{\theta}}_{\mathcal{I}}\) for \(\bm{\theta}^{*}_{\mathcal{I}}\in\mathbb{R}^{|\mathcal{I}|}\) to be \(\mathbb{E}[(\widehat{\bm{\theta}}_{\mathcal{I}}-\widehat{\bm{\theta}}_{ \mathcal{I}})^{\top}[(\mathbf{S}^{-1}_{n})_{\mathcal{I}\mathcal{I}}]^{-1}( \widehat{\bm{\theta}}_{\mathcal{I}}-\widehat{\bm{\theta}}_{\mathcal{I}})],\) where \(\mathbf{S}_{n}=\sum_{i=1}^{n}\bm{x}_{i}\bm{x}_{i}^{\top}\) is the sample Gram matrix._

Roughly speaking, when the covariates \(\bm{x}_{i}\) are all fixed, the scaled-MSE compares the performance of \(\widehat{\bm{\theta}}_{\mathcal{I}}\) against the estimator with minimal variance (OLS). Moreover, we have the following result:

**Proposition 2.2** (A simplified version of Theorem 2 in Khamaru et al. (2018)): __

1. _Given a set_ \(\mathcal{I}\subseteq[d]\)_. Suppose the data_ \(\{(\bm{x}_{i},y_{i})\}_{i=1^{n}}\) _are i.i.d. (_\((0,d)\)_-adaptive) from model (_3_). Then the scaled-MSE satisfies_ \[\inf_{\widehat{\bm{\theta}}}\sup_{\bm{\theta}^{*}\in\mathbb{R}^{d}}\mathbb{E} \Big{\|}\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{\theta}^{*}_{\mathcal{I}} \Big{\|}_{[(\mathbf{S}^{-1}_{n})_{\mathcal{I}\mathcal{I}}]^{-1}}^{2}\geq\sigma ^{2}|\mathcal{I}|.\] (4) _Furthermore, the equality holds when choosing_ \(\widehat{\bm{\theta}}_{\mathcal{I}}\) _to be the OLS estimator for_ \(\bm{\theta}^{*}_{\mathcal{I}}\)_._2. _Suppose the data points_ \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) _are allowed to be arbitrarily adaptive (_\((d,d)\)_-adaptive). For any_ \((n,d)\) _with_ \(d\geq 2\) _and_ \(n\geq c\cdot d^{3}\)_, and any non-empty set_ \(\mathcal{I}\in[d]\)_, there exists a data collection algorithm such that_ \[\inf_{\bm{\widehat{\theta}}}\sup_{\bm{\theta}^{\ast}\in\mathbb{R}^{d}}\mathbb{ E}\Big{\|}\bm{\widehat{\theta}}_{\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^{ \ast}\Big{\|}_{[(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm }}}}}}}}}}}}})_{ \mathcal{I}^{c}}}^{2}\geq c^{\prime}\cdot d\sigma^{2}\log(n),\] (5) _where_ \(c,c^{\prime}>0\) _are some universal constants._

Proposition 2.2 exhibits the striking difference between two extreme data collection mechanisms. While the scaled-MSE scales as \(O(|\mathcal{I}|)\) when data are i.i.d., the scaled-MSE for even a single coordinate (e.g., setting \(\mathcal{I}=\{1\}\)) can be of the order \(O(d)\) if the data are allowed to be collected arbitrarily adaptively.

Let \(\mathcal{I}^{c}=[d]\setminus\mathcal{I}\). By the matrix inverse formula, we have

\[[(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{ \bm{ }}}}}}}}}}}}}}}} [(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}} [(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} [(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} -1}}(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}}} -1}(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}}} -1}(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}}} 1}(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ }}}}}}}}}}}}}}}}}}} 1}}(\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{ \bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bm{\bmbmbmbmbmbmbmbmbmbmbmbmbmbmbmbmbmbmbmbm {\bm

### Notations

In the paper, we use the bold font to denote vectors and matrices (e.g., \(\bm{x},\mathbf{x},\mathbf{X},\bm{\theta},\bm{\varepsilon}\)), and the regular font to denote scalars (e.g., \(x,\theta,\varepsilon\)). Given data \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) from the linear model (3) that are \((k,d)\)-adaptive, we use \(\bm{x}_{i}^{\rm ad}\in\mathbb{R}^{k},\bm{x}_{i}^{\rm ad}\in\mathbb{R}^{d-k}\) to denote the adaptive and non-adaptive covariates. We also write \(\bm{\theta}^{*}=(\bm{\theta}_{\rm ad}^{*},\bm{\theta}_{\rm nad}^{*\top})^{\top}\) to denote the components that correspond to the adaptive and non-adaptive covariates. Let \(\mathbf{X}=(\bm{x}_{1}^{\top},\ldots,\bm{x}_{n}^{\top})^{\top}\in\mathbb{R}^{n \times d}\) be the covariate matrix, with \(\mathbf{X}_{\rm ad}\) (or \(\mathbf{X}_{\rm nad}\)) representing the submatrices consisting of the adaptive (or non-adaptive) columns. We use \(\mathbf{x}_{j}\) to denote the \(j\)-th column of the covariate matrix.

For a matrix \(\mathbf{M}\) with \(n\) rows, let \(\mathbf{M}_{-j}\) be the matrix obtained by deleting the \(j\)-th column of \(\mathbf{M}\). We define the _projection operator_\(\mathbf{P}_{\mathbf{M}}:=\mathbf{M}(\mathbf{M}^{\top}\mathbf{M})^{-1}\mathbf{ M}^{\top}\) and the (columnwise) centered matrix \(\widetilde{\mathbf{M}}:=(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{1}_{n}})\mathbf{M}\), where \(\mathbf{I}\in\mathbb{R}^{n\times n}\) is the identity matrix and \(\mathbf{1}_{n}\in\mathbb{R}^{n}\) is the all-one vector. For a symmetric \(\mathbf{M}\succeq 0\), we define \(\|\bm{x}\|_{\mathbf{M}}:=\sqrt{\bm{x}^{\top}\mathbf{M}\bm{x}}\). Lastly, we use \(c,c^{\prime},c^{\prime\prime}>0\) to denote universal constants and \(C,\widetilde{C}^{\prime},C^{\prime\prime}>0\) to denote constants that may depend on the problem specific parameters but not on \(k,d,n\). We allow the values of the constants to vary from place to place.

## 3 Main results

This section is devoted to our main results on low-dimensional estimation and inference. In Section 3.1 and 3.2 we discuss the problem of estimating a low-dimensional component of \(\theta^{*}\), and Section 3.3 is devoted to inference of low-dimensional components.

### Low-dimensional estimation

Suppose the collected data \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) are \((k,d)\)-adaptive. In this section, we are interested in estimating the adaptive parameter component \(\bm{\theta}_{\rm ad}^{*}\in\mathbb{R}^{k}\).

In addition to \((k,d)\)-adaptivity, we introduce the following assumptions on the collected data \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\).

**Assumption A**:
1. There exists a constant \(\mathrm{U_{x}}>0\) such that \[1\leq\sigma_{\min}(\mathbf{X}_{\rm ad}^{\top}\mathbf{X}_{\rm ad})\leq\sigma_{ \max}(\mathbf{X}_{\rm ad}^{\top}\mathbf{X}_{\rm ad})\leq n\mathrm{U_{x}}.\]
2. The non-adaptive components \(\{\bm{x}_{i}^{\rm nad}\}_{i=1}^{n}\) are i.i.d. sub-Gaussian vectors with parameter \(\nu>0\), that is, for any unit direction \(\bm{u}\in\mathbb{S}^{d-1}\), \[\mathbb{E}[\exp\{\lambda\langle\bm{u},\,\bm{x}_{i}^{\rm nad}-\mathbb{E}[\bm{x }_{i}^{\rm nad}]\rangle\}]\leq e^{\lambda^{2}\nu^{2}/2}\qquad\forall\lambda \in\mathbb{R}.\]
3. There exist some constants \(0\leq\sigma_{\min}\leq\sigma_{\max}\) such that the covariance matrix of the non-adaptive component \(\bm{\Sigma}:=\mathrm{Cov}[\bm{x}_{i}^{\rm nad}]\) satisfies, \[0<\sigma_{\min}\leq\sigma_{\min}(\bm{\Sigma})\leq\sigma_{\max}(\bm{\Sigma})\leq \sigma_{\max}.\]
4. Conditioned on \((\bm{x}_{i},\mathcal{F}_{i-1})\), the noise variable \(\varepsilon_{i}\) in (3) is zero mean sub-Gaussian with parameter \(v>0\), i.e., \[\mathbb{E}[\varepsilon_{i}|\bm{x}_{i}^{\rm ad},\mathcal{F}_{i-1}]=0,\;\;\text{ and}\;\;\mathbb{E}[e^{\lambda\varepsilon_{i}}|\bm{x}_{i}^{\rm ad},\mathcal{F}_{i-1}]\leq e^{ \lambda^{2}v^{2}/2}\qquad\forall\lambda\in\mathbb{R},\] and has conditional variance \(\sigma^{2}=\mathbb{E}[\varepsilon_{i}^{2}|\bm{x}_{i},\mathcal{F}_{i-1}]\) for all \(i\in[n]\).

Let us clarify the meaning of the above assumptions. Assumption (A1) is the regularity assumption on the adaptive component. Roughly speaking, we allow the adaptive component to be _arbitrarily adaptive_ as long as \(\mathbf{X}_{\rm ad}^{\top}\mathbf{X}_{\rm ad}\) is not close to be singular and \(\bm{x}_{i}^{\rm ad}\) has bounded \(\ell_{2}-\)norm. This is weaker than the assumptions made in [15, 43, 30], which assume that the conditional distribution of \(\mathbf{X}_{\rm ad}\) is known. Assumption (A2), (A3) on the non-adaptive component, assume its distribution is non-singular and light-tailed. Assumption (A4) is a standard assumption that characterizes the tail behavior of the zero-mean noise variable. We remark that the equal conditional variance assumption in Assumption (A4) is mainly required in Theorem 3.4, while it is sufficient to assume \(\sigma^{2}\) being a uniform upper bound of the conditional variance in Theorem 3.1 and 3.2.

#### 3.1.1 Warm up: zero-mean non-adaptive component

We start with discussing a special case where the non-adaptive component \(\bm{x}_{i}^{\mathrm{nad}}\) is zero-mean. In this case, we prove that the Ordinary Least Squares (OLS) estimator on \((\mathbf{X},\bm{y})\) for \(\bm{\theta}_{\mathrm{ad}}^{\star}\) is near-optimal; see the discussion in Section 2.1. Denote the OLS estimator by \(\widehat{\bm{\theta}}=(\widehat{\bm{\theta}}_{\mathrm{ad}}^{\top},\widehat{ \bm{\theta}}_{\mathrm{nad}}^{\top})^{\top}\). Throughout, we assume that the sample size \(n\) and dimension \(d\) satisfies the relation

\[\frac{n}{\log^{2}(n/\delta)}\geq Cd^{2},\] (6)

where \(C\) is an independent of \((n,d)\) but may depend on other problem specific parameters. With this set up, our first theorem states

**Theorem 3.1**: _Given data points \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) from a \((k,d)\)-adaptive model, and tolerance level \(\delta\in(0,1/2)\). Let, assumption (A1)-(A4) and the bound (6) in force, and the non-adaptive component \(\bm{x}_{i}^{\mathrm{nad}}\) is drawn from a zero-mean distribution. Then, we have_

\[\|\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{ \star}\|_{\mathbf{X}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{ X}_{\mathrm{nad}}})\mathbf{X}_{\mathrm{ad}}}^{2} \leq C^{\prime}\log(n\det(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{ \mathrm{ad}})/\delta)\] (7a) \[\leq C^{\prime\prime}k\log(n/\delta).\] (7b)

_with probability at least \(1-\delta\)._

See Appendix A.3 for a detailed proof. A few comments regarding Theorem 3.1 are in order. One might integrate both sides of the last bound to get a bound on the scaled-MSE. Comparing the bound (7b) with the lower bound from Proposition 2.2, we see this bound is tight in a minimax sense, up to some logarithmic factors.

It is now worthwhile to compare this bound with the existing best upper bounds in the literature. Invoking the concentration bounds from (26, Lemma 16) one have that

\[\|\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{ \star}\|_{\mathbf{X}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{ X}_{\mathrm{nad}}})\mathbf{X}_{\mathrm{ad}}}^{2}\leq\|\widehat{\bm{\theta}}_{ \mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{\star}\|_{\mathbf{X}_{\mathrm{ad}}^{ \top}\mathbf{X}_{\mathrm{ad}}}^{2}\leq c\cdot d\log(n/\delta)\] (8)

One might argue that the first inequality is loose as we only want to estimate a low-dimensional component \(\bm{\theta}_{\mathrm{ad}}^{\star}\in\mathbb{R}^{k}\). However, invoking the lower bound from Proposition 2.2, we see that the bound (8) is the best you can hope for if we do not utilize the \((k,d)\)-adaptivity structure present in the data. See also the scaled-MSE bound for a single coordinate estimation in (26, Theorem 8) which also has a dimension dependence in the scaled-MSE bound.

#### 3.1.2 Nonzero-mean non-adaptive component

In practice, the assumption that the non-adaptive covariates are drawn i.i.d. from a distribution \(P\) with _zero mean_ is unsatisfactory. One would like to have a similar result where the distribution \(P\) has an _unknown_ non-zero mean.

```
1:\(\hat{\bm{\mu}}_{\mathrm{ad}}\leftarrow\frac{\mathbf{X}_{\mathrm{ad}}^{\top} \mathbf{I}_{n}}{n}\), \(\hat{\bm{\mu}}_{\mathrm{nad}}\leftarrow\frac{\mathbf{X}_{\mathrm{nad}}^{\top} \mathbf{I}_{n}}{n}\)
2:\(\widetilde{\mathbf{X}}_{\mathrm{ad}}=\mathbf{X}_{\mathrm{ad}}-\mathbf{I}_{n} \hat{\bm{\mu}}_{\mathrm{ad}}^{\top}\), \(\widetilde{\mathbf{X}}_{\mathrm{nad}}=\mathbf{X}_{\mathrm{nad}}-\mathbf{1}_{n} \hat{\bm{\mu}}_{\mathrm{nad}}^{\top}\)
3:Run OLS on centered response vector \(\bm{y}-\overline{\bm{y}}\cdot\mathbf{1}_{n}\) and centered covariate matrix \(\widetilde{\mathbf{X}}=(\widetilde{\mathbf{X}}_{\mathrm{ad}},\widetilde{ \mathbf{X}}_{\mathrm{nad}})\in\mathbb{R}^{n\times d}\); obtain the estimator \(\widetilde{\bm{\theta}}=(\widetilde{\bm{\theta}}_{\mathrm{ad}}^{\top},\widetilde {\bm{\theta}}_{\mathrm{nad}}^{\top})^{\top}\). ```

**Algorithm 1** Centered OLS for \(k\) adaptive coordinates (\(\mathbf{X},\bm{y}\))

Before we state our estimator for the nonzero-mean case, it is helpful to understand the proof intuition of Theorem 3.1. A simple expansion yields

\[\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{\star} =(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}}-\mathbf{ X}_{\mathrm{ad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{X}_{\mathrm{ad}})^{-1}( \mathbf{X}_{\mathrm{ad}}^{\top}\bm{\varepsilon}-\mathbf{X}_{\mathrm{ad}}^{\top} \mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon})\] \[\approx(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})^ {-1}\mathbf{X}_{\mathrm{ad}}^{\top}\bm{\varepsilon}+\text{smaller order terms}\]

We show that the interaction term \(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\) is small compared to the other terms under \((k,d)\)-adaptivity and _zero-mean_ property of \(\widetilde{\mathbf{X}}_{\mathrm{nad}}\). In particular, under _zero-mean_ property, each entry ofthe matrix \(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{rad}}\) is a martingale difference sequence, and can be controlled via concentration inequalities [1]. This martingale property is _not true_ when the columns of \(\mathbf{X}_{\mathrm{nad}}\) have a nonzero mean.

As a remedy, we consider the mean-centered linear model:

\[\mathbf{y}-\bar{y}\cdot\mathbf{1}_{n}=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{ \top}\boldsymbol{\theta}_{\mathrm{ad}}^{*}+\widetilde{\mathbf{X}}_{\mathrm{ nad}}^{\top}\boldsymbol{\theta}_{\mathrm{nad}}^{*}+(\epsilon-\tilde{\epsilon}\cdot \mathbf{1}_{n})\] (9)

where \(\widetilde{\mathbf{X}}_{\mathrm{ad}}=\mathbf{X}_{\mathrm{ad}}-\frac{\mathbf{1 }_{n}\mathbf{1}_{n}^{\top}}{n}\mathbf{X}_{\mathrm{ad}}\), and \(\widetilde{\mathbf{X}}_{\mathrm{nad}}=\mathbf{X}_{\mathrm{nad}}-\frac{\mathbf{1 }_{n}\mathbf{1}_{n}^{\top}}{n}\mathbf{X}_{\mathrm{nad}}\) are centered version of the matrices \(\mathbf{X}_{\mathrm{ad}}\) and \(\mathbf{X}_{\mathrm{nad}}\), respectively. The centering in (9) ensures that \(\mathbf{X}_{\mathrm{nad}}\) is _approximately_ zero-mean, but unfortunately, it breaks the martingale structure present in the data. For instance, the elements of \(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}}\) are not a sum of martingale difference sequence because we have subtracted the column mean from each entry. Nonetheless, it turns out that subtracting the sample mean, while breaks the martingale difference structure, does not break it in an adversarial way, and we can still control the entries of \(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}}\). See Lemma A.1 part (b) for one of the key ingredient in the proof. We point out that this finding is not new. Results of this form are well understood in various forms in sequential prediction literature, albeit in a different context. Such results can be found in earlier works of Lai, Wei and Robbins [21; 22; 24] and also in the later works by several authors [11; 12; 10; 1] and the references therein.

Our following Theorem 3.2 ensures that the intuition developed in this section so far is useful to characterize the performance of the solution obtained from the centered OLS.

**Theorem 3.2**: _Given data points \(\{(\boldsymbol{x}_{i},y_{i})\}_{i=1}^{n}\) from a \((k,d)\)-adaptive model, and tolerance level \(\delta\in(0,1/2)\). Let, assumption (A1)-(A4) and the bound (6) be in force. Then, \(\widetilde{\boldsymbol{\theta}}_{\mathrm{ad}}\) obtained from Algorithm 1, satisfies_

\[\|\widetilde{\boldsymbol{\theta}}_{\mathrm{ad}}-\boldsymbol{\theta }_{\mathrm{ad}}^{*}\|_{\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_ {n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\widetilde{\mathbf{X}}_ {\mathrm{ad}}}^{2} \leq C^{\prime}\log(n\det(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{ \top}\widetilde{\mathbf{X}}_{\mathrm{ad}})/\delta)\] (10) \[\leq C^{\prime\prime}k\log(n/\delta).\]

_with probability at least \(1-\delta\)._

See Appendix A.4 for a proof. Note that the variance of \(\widetilde{\boldsymbol{\theta}}_{\mathrm{ad}}\) is given by \(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{ \widetilde{\mathbf{X}}_{\mathrm{nad}}})\widetilde{\mathbf{X}}_{\mathrm{ad}}\). The covariance matrix is the same as \(\mathbf{X}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{X}_{\mathrm{ nad}}})\mathbf{X}_{\mathrm{ad}}\) when the all one vector \(\mathbf{1}_{n}\) belongs to the column space of \(\widetilde{\mathbf{X}}_{\mathrm{nad}}\).

### Single coordinate estimation: Application to treatment assignment

Let us now come back to Example 2.1 that we started. Let, at every round \(i\), the treatment assignment \(A_{i}\) depends on \(k-1\) (unknown) coordinate of the covariates \(\boldsymbol{x}_{i}\in\mathbb{R}^{d-1}\). We assume that the covariates \(\boldsymbol{x}_{i}^{\prime}s\) are drawn i.i.d. from some unknown distribution \(\mathcal{P}\). Assuming the response is related to the treatment and covariates via a linear model, it is not hard to see that this problem satisfies a \((k,d)\)-adaptivity property. The following corollary provides a bound on the estimation error of estimating the (homogeneous) treatment effect.

Figure 1: The plot depicts the empirical relation between the scaled MSE of the OLS and centered OLS estimate from Algorithm 1 and the number of adaptive covariates \((k)\) for a carefully constructed problem. See Section B.1 for simulation details.

**Corollary 3.3**: _Suppose the assumptions from Theorem 3.2 are in force, and \(\ell\in[k]\) be an index corresponding to one of the adaptive coordinates. Then, the \(\ell^{th}\) coordinate of the the centered OLS estimator from Algorithm 1 satisfies_

\[|\widetilde{\theta}_{\mathrm{ad},\ell}-\theta^{*}_{\mathrm{ad},\ell}|\leq\frac{ \sqrt{C\log(n\det(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}\widetilde{ \mathbf{X}}_{\mathrm{ad}})/\delta)}}{\sqrt{\widetilde{\mathbf{x}}_{\ell}^{\top }(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{-\ell}})\widetilde{ \mathbf{x}}_{\ell}}}\leq\frac{C\sqrt{k\log(n/\delta)}}{\sqrt{\widetilde{ \mathbf{x}}_{\ell}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{ -\ell}})\widetilde{\mathbf{x}}_{\ell}}}.\]

_The bounds above hold with probability at least \(1-\delta\), and \(\widetilde{\mathbf{X}}_{-\ell}\) denote the matrix obtained by removing the \(\ell^{th}\) column from \(\widetilde{\mathbf{X}}\)._

See Appendix A.5 for a proof of this Corollary. We verify the result of Corollary 3.3 via simulations as shown in Figure 1. From the figure we see that the scaled MSE increases linearly as the number of adaptive covariates increases, matching with our theoretical predictions. See Appendix B for more details about the simulation.

### Inference with one adaptive arm

In this section, we provide a method for constructing valid confidence intervals for \(\boldsymbol{\theta}^{*}_{\mathrm{ad}}\). To simplify the problem, we restrict our attention to the case of \((1,d)\)-adaptivity and \(\mathbb{E}[\boldsymbol{x}^{\mathrm{nad}}_{i}]=\mathbf{0}\).

#### A two-stage estimator

Our goal is to derive an asymptotically normal estimator for a target parameter in presence of a nuisance component. We call our estimator a "Two-stage-adaptive-linear-estimating-equation" based estimator, or \(\mathtt{TALE-estimator}\) for short. We start with a prior estimate \(\widehat{\boldsymbol{\theta}}^{\text{Tr}}_{\mathrm{nad}}\) of \(\boldsymbol{\theta}^{*}_{\mathrm{nad}}\), and define our estimate \(\widehat{\boldsymbol{\theta}}_{\mathrm{TALE}}\) for \(\boldsymbol{\theta}^{*}_{\mathrm{ad}}\) as a solution of this

\[\mathtt{TALE-estimator}\colon\qquad\sum_{i=1}^{n}w_{i}(y_{i}-x_{i}^{\mathrm{ ad}}\cdot\widehat{\boldsymbol{\theta}}_{\mathrm{TALE}}-\boldsymbol{x}_{i}^{ \mathrm{nad}\top}\widehat{\boldsymbol{\theta}}_{\mathrm{nad}}^{\text{Tr}})=0.\] (11)

Recall that \(\widehat{\boldsymbol{\theta}}_{\mathrm{TALE}}\) is a scalar and the equation has a unique solution as long as \(\sum_{1\leq i\leq n}w_{i}^{\mathrm{ad}}x_{i}\neq 0\).

The weights \(\{w_{i}\}_{i\leq n}\) in equation (11) are a set of predictable random scalars (i.e. \(w_{i}\in\sigma(\boldsymbol{x}_{i}^{\mathrm{ad}},\mathcal{F}_{i-1})\)). Specifically, we start with \(s_{0}>0\) and \(s_{0}\in\mathcal{F}_{0}\), and define

\[w_{i} =\frac{f(s_{i}/s_{0})x_{i}^{\mathrm{ad}}}{\sqrt{s_{0}}}\qquad \text{where}\qquad s_{i}=s_{0}+\sum_{t\leq i}(x_{t}^{\mathrm{ad}})^{2}\;\;\text{ and}\] (12a) \[f(x) =\frac{1}{\sqrt{x(\log e^{2}x)(\log\log e^{2}x)^{2}}}\qquad\text{ for }\;x>1.\] (12b)

Let us first gain some intuitions on why TABLE works. By rewriting equation (11), we have

\[\sum_{i=1}^{n}w_{i}x_{i}^{\mathrm{ad}}\!\left(\widehat{\boldsymbol{\theta}}_{ \mathrm{TALE}}-\boldsymbol{\theta}_{\mathrm{ad}}^{*}\right)=\underbrace{\sum_ {i=1}^{n}w_{i}\epsilon_{i}}_{v_{n}}+\underbrace{\sum_{i=1}^{n}w_{i}x_{i}^{ \mathrm{nad}\top}(\boldsymbol{\theta}_{\mathrm{nad}}^{*}-\widehat{ \boldsymbol{\theta}}_{\mathrm{nad}}^{\text{Tr}})}_{b_{n}}.\] (13)

Following the proof in [39], we have \(v_{n}\overset{d}{\longrightarrow}\mathcal{N}(0,\sigma^{2})\). Besides, one can show that with a proper choice of prior estimator \(\widehat{\boldsymbol{\theta}}_{\mathrm{nad}}^{\text{Tr}}\), the bias term \(b_{n}\) converges to zero in probability as \(n\) goes to infinity. It is important to note that [39] considers the linear regression model where the number of covariates is fixed, and the sample size goes to infinity. In this work, however, we are interested in a setting where the number of covariates can grow with the number of samples. Therefore, our approach, \(\mathtt{TALE-estimator}\), has distinctions with the ALEE estimator proposed in [39]. The above intuition is formalized in the following theorem. Below, we use the shorthand \(\widehat{\boldsymbol{\theta}}_{\mathrm{nad}}^{\text{OLS}}\) to denote the coordinates of the least squares estimate of \(\widehat{\boldsymbol{\theta}}_{\mathrm{nad}}^{\text{OLS}}\) corresponding to the _non-adaptive_ components.

**Theorem 3.4**: _Suppose \(1/s_{0}+s_{0}/s_{n}=o_{p}(1)\), \(n/(\log^{2}(n)\cdot d^{2})\to\infty\), and assumptions (A1)-(A4) are in force. Then, the estimate \(\widehat{\theta}_{\text{TALE}}\), obtained using weights from (12a) and \(\widehat{\boldsymbol{\theta}}_{\text{nad}}^{\text{Pr}}=\widehat{\boldsymbol{ \theta}}_{\text{nad}}^{\text{OLS}}\), satisfies_

\[\frac{1}{\widehat{\sigma}\sqrt{\sum_{1\leq i\leq n}w_{i}^{2}}}\bigg{(}\sum_{1 \leq i\leq n}w_{i}x_{i}^{\text{ad}}\bigg{)}\cdot\left(\widehat{\theta}_{\text{ TALE}}-\theta_{\text{ad}}^{*}\right)\stackrel{{ d}}{{ \longrightarrow}}\mathcal{N}(0,1),\]

_where \(\widehat{\sigma}\) is any consistent estimate of \(\sigma\). Moreover, the asymptotic variance \(\widehat{\theta}_{\text{TALE}}\) is optimal up to logarithmic-factors._

See Appendix A.6 for a proof of this theorem. The assumption \(1/s_{0}+s_{0}/s_{n}=o_{p}(1)\) in the theorem essentially requires \(s_{0}\) grows to infinity in a rate slower than \(s_{n}\). Therefore, in order to construct valid confidence intervals for \(\widehat{\theta}_{\text{TALE}}\), one has to grasp some prior knowledge about the lower bound of \(s_{n}\). In our experiments in Section 4 we set \(s_{0}=\log\log(n)\). Finally, it is also worth mentioning that one can apply martingale concentration inequalities (e.g. [1]) to control the terms \(b_{n}\) and \(v_{n}\) in equation (13), which in turn yields the finite sample bounds for \(\widehat{\theta}_{\text{TALE}}\) estimator. Finally, a consistent estimator of \(\sigma\) can be found using (24, Lemma 3).

## 4 Numerical experiments

In this section, we investigate the performance of TALE empirically, and compare it with the ordinary least squares (OLS) estimator, W-decorrelation proposed by Deshpande et al. [13], and the nonasymptotic confidence intervals derived from Theorem 8 in Lattimore et al. [26]. Our simulation set up entails the motivating Example 2.1 of treatment assignment. In our experiments, at stage \(i\), the treatments \(A_{i}\in\{0,1\}\) are assigned on the sign of \(\widehat{\theta}_{1}^{(i)}\), where \(\widehat{\boldsymbol{\theta}}^{(i)}=(\widehat{\theta}_{1}^{(i)},\widehat{ \theta}_{2}^{(i)},\ldots,\widehat{\theta}_{d}^{(i)})\) is the least square estimate based on all data up to the time point \(i-1\); here, the first coordinate of \(\widehat{\theta}_{1}^{(i)}\) is associated with treatment assignment. The detailed data generation mechanism can be found in Appendix. From Figure 2 (top) we see that both TALE and W-decorrelation have valid empirical coverage (i.e., they are close to or above the baseline), while the nonasymptotic confidence intervals are overall conservative and the OLS is downwardly biased. In addition, TALE has confidence intervals that are shorter than those of W-decorrelation, which indicates a better estimation performance. Similar observations occur in the high-dimensional model in Figure 2 (bottom), where we find that both the OLS estimator and W-decorrelation are downwardly biased while TALE has valid coverage.

Figure 2: Empirical coverage probability and the width of confidence intervals versus target coverage probability \(1-\alpha\) for TALE, the OLS estimator, non-asymptotic concentration inequalities, and W-decorrelation. We select the noise level \(\sigma=0.3\). Top: \(n=1000,d=10\). Bottom: \(n=500,d=50\). At bottom right we do not display the result for concentration since the CIs are too wide. We run the simulation \(1000\) times and display the \(\pm 1\) standard deviation.

## 5 Discussion

In this paper, we investigate the statistical limits of estimating a low-dimensional component in a high dimensional adaptive linear model. We start by recalling a recent lower bound [20], which states that we need to pay for the underlying dimension \(d\) even if we want to estimate a low (one)-dimensional component. Our main result is to show that in order to estimate a low-dimensional component, we need to pay only for the degree of adaptivity \(k\), which can potentially be much smaller than the underlying dimension \(d\). Additionally, we propose a two-stage estimator for the one-dimensional target component, which is asymptotically normal. Finally, we demonstrate the effectiveness of this two-stage estimator via numerical simulations. For the future work, there are several avenues for further exploration that can contribute to a more comprehensive understanding of adaptive regression models. First of all, it would be interesting to generalize the \((k,d)-\)adaptivity for the case when the number of adaptive components may vary between samples. It is also interesting to investigate if the current assumptions can be relaxed or not. For statistical inference part, it would be interesting to extend the TALE estimator to the case when the number of adaptive components is great than one.

Figure 3: Histograms of the scaled errors for TALE and OLS. Left: \(n=1000,d=10\). Right: \(n=500,d=50\). We choose the noise level \(\sigma=0.3\) and repeat the simulation \(1000\) times. Observe that the distribution of the OLS estimator is much more different than standard normal, and it exhibits a downwards bias [31], while TALE is in good accordance with a standard normal distribution.

## Acknowledgments

This work was partially supported by the National Science Foundation Grants DMS-2311304, CCF-1934924, DMS-2052949 and DMS-2210850.

## References

* Abbasi-yadkori et al. [2011] Yasin Abbasi-yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011.
* Auer [2002] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3(Nov):397-422, 2002.
* Bai and Silverstein [2010] Zhidong Bai and Jack W Silverstein. _Spectral analysis of large dimensional random matrices_, volume 20. Springer, 2010.
* Belloni et al. [2016] Alexandre Belloni, Victor Chernozhukov, and Ying Wei. Post-selection inference for generalized linear models with many controls. _Journal of Business & Economic Statistics_, 34(4):606-619, 2016.
* Bickel [1982] Peter J Bickel. On adaptive estimation. _The Annals of Statistics_, pages 647-671, 1982.
* Bickel et al. [1993] Peter J Bickel, Chris AJ Klaassen, Peter J Bickel, Ya'acov Ritov, J Klaassen, Jon A Wellner, and Ritov. _Efficient and adaptive estimation for semiparametric models_, volume 4. Springer, 1993.
* Box et al. [2015] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. _Time series analysis: forecasting and control_. John Wiley & Sons, 2015.
* Chernozhukov et al. [2018] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. _The Econometrics Journal_, 21(1):C1-C68, 2018.
* Chu et al. [2011] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214. JMLR Workshop and Conference Proceedings, 2011.
* Dani et al. [2008] Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback. In _Annual Conference Computational Learning Theory_, 2008.
* 1933, 2004.
* de la Pena et al. [2009] Victor H de la Pena, Michael J Klass, and Tze Leung Lai. Theory and applications of multivariate self-normalized processes. _Stochastic Processes and their Applications_, 119(12):4210-4227, 2009.
* Deshpande et al. [2018] Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Taddy. Accurate inference for adaptive linear models. In _International Conference on Machine Learning_, pages 1194-1203. PMLR, 2018.
* Dickey and Fuller [1979] David A. Dickey and Wayne A. Fuller. Distribution of the estimators for autoregressive time series with a unit root. _Journal of the American Statistical Association_, 74(366):427-431, 1979.
* Hadad et al. [2021] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. _Proceedings of the National Academy of Sciences_, 118(15):e2014602118, 2021.
* Hahn [1998] Jinyong Hahn. On the role of the propensity score in efficient semiparametric estimation of average treatment effects. _Econometrica_, 66(2):315-331, 1998.

* [17] Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. _Advances in Neural Information Processing Systems_, 33:10753-10763, 2020.
* [18] Roger A Horn and Charles R Johnson. _Matrix analysis_. Cambridge university press, 2012.
* [19] Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis testing for high-dimensional regression. _The Journal of Machine Learning Research_, 15(1):2869-2909, 2014.
* [20] Koulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J Wainwright. Near-optimal inference in adaptive linear regression. _arXiv preprint arXiv:2107.02266_, 2021.
* [21] T_L_L_ Lai and Herbert Robbins. Strong consistency of least-squares estimates in regression models. _Proceedings of the National Academy of Sciences_, 74(7):2667-2669, 1977.
* [22] T L Lai, Herbert Robbins, and C Zi Wei. Strong consistency of least squares estimates in multiple regression ii. _Journal of multivariate analysis_, 9(3):343-361, 1979.
* [23] Tze Leung Lai. Asymptotic properties of nonlinear least squares estimates in stochastic regression models. _The Annals of Statistics_, pages 1917-1930, 1994.
* [24] Tze Leung Lai and Ching Zong Wei. Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems. _The Annals of Statistics_, 10(1):154-166, 1982.
* [25] Tor Lattimore. A lower bound for linear and kernel regression with adaptive covariates. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2095-2113. PMLR, 2023.
* [26] Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.
* [27] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [28] Lihua Lei, Peter J Bickel, and Noureddine El Karoui. Asymptotics for high dimensional regression m-estimates: fixed design results. _Probability Theory and Related Fields_, 172:983-1079, 2018.
* [29] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In _International Conference on Machine Learning_, pages 2071-2080. PMLR, 2017.
* [30] Licong Lin, Koulik Khamaru, and Martin J Wainwright. Semi-parametric inference based on adaptively collected data. _arXiv preprint arXiv:2303.02534_, 2023.
* [31] Xinkun Nie, Xiaoying Tian, Jonathan Taylor, and James Zou. Why adaptively collected data have negative bias and how to correct for it. _Advances in Neural Information Processing Systems_, 84:1261-1269, 2018.
* [32] James M Robins. Correcting for non-compliance in randomized trials using structural nested mean models. _Communications in Statistics-Theory and methods_, 23(8):2379-2412, 1994.
* [33] Peter M Robinson. Root-n-consistent semiparametric regression. _Econometrica: Journal of the Econometric Society_, pages 931-954, 1988.
* [34] Jaehyeok Shin, Aaditya Ramdas, and Alessandro Rinaldo. On the bias, risk, and consistency of sample means in multi-armed bandits. _SIAM Journal on Mathematics of Data Science_, 3(4):1278-1300, 2021.
* [35] Anastasios A Tsiatis. Semiparametric theory and missing data. 2006.
* [36] Mark J Van der Laan, Sherri Rose, et al. _Targeted learning: causal inference for observational and experimental data_, volume 10. Springer, 2011.
* [37] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.

* [38] John S. White. The limiting distribution of the serial correlation coefficient in the explosive case ii. _The Annals of Mathematical Statistics_, 30(3):831-834, 1959.
* [39] Mufang Ying, Koulik Khamaru, and Cun-Hui Zhang. Adaptive linear estimating equations. _arXiv preprint arXiv:2307.07320_, 2023.
* [40] Ruohan Zhan, Vitor Hadad, David A Hirshberg, and Susan Athey. Off-policy evaluation via adaptive weighting with data from contextual bandits. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2125-2135, 2021.
* [41] Cun-Hui Zhang and Stephanie S Zhang. Confidence intervals for low dimensional parameters in high dimensional linear models. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 76(1):217-242, 2014.
* [42] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. _Advances in Neural Information Processing Systems_, 33:9818-9829, 2020.
* [43] Kelly Zhang, Lucas Janson, and Susan Murphy. Statistical inference with m-estimators on adaptively collected data. _Advances in Neural Information Processing Systems_, 34:7460-7471, 2021.

## Appendix A Proofs

Throughout the proof, we use \(c,c^{\prime},c^{\prime\prime}>0\) to denote constants universal constants. We use \(C,C^{\prime},C^{\prime\prime}>0\) to denote constants that may depend on the problem specific parameters. Concretely, we use them to denote constants that only depends polynomially on \((1,\sigma_{\max},1/\sigma_{\min},v,\nu,\mathrm{U_{x}})\). We allows the values of the constants to vary from place to place.

### Auxiliary lemmas

Before stating our main theorems, we list some useful lemmas, which can be of independent interest. All the proofs of lemmas can be found in Appendix A.

**Lemma A.1**: _Given \(n\geq d\geq 1\). Let \(\{(\mathbf{a}_{i},b_{i})\}_{i=1}^{n}\) be a sequence of pairs such that \(\mathbf{a}_{i}\in\mathbb{R}^{d}\) are \(\mathcal{F}_{i-1}\)-measurable and \(b_{i}\in\mathbb{R}\) are \(\mathcal{F}_{i}\)-measurable w.r.t. some filtration \(\{\mathcal{F}_{i}\}_{i=1}^{n}\). Assume in addition that \(b_{i}\) are zero-mean sub-Gaussian random variables with parameter \(\sigma\) conditioned on \(\mathcal{F}_{i-1}\), i.e.,_

\[\mathbb{E}[b_{i}|\mathcal{F}_{i-1}]=0,\text{ and }\mathbb{E}[e^{\lambda b_{i}}| \mathcal{F}_{i-1}]\leq e^{\sigma^{2}\lambda^{2}/2},\text{ for all }\lambda\in \mathbb{R}.\]

_Let \(\mathbf{A}=[\mathbf{a}_{1},\ldots,\mathbf{a}_{n}]^{\top}\) and \(\mathbf{b}=[b_{1},\ldots,b_{n}]^{\top}\). Suppose that \(1\leq\sigma_{\min}(\mathbf{A}^{\top}\mathbf{A})\leq\sigma_{\max}(\mathbf{A}^ {\top}\mathbf{A})\leq nB\) for some constant \(B>0\)._

1. _(A simplified version of Theorem_ 1 _in Abbasi et al._ _[_1_]__.) With probability over_ \(1-\delta\)__ \[\|\mathbf{P}_{\mathbf{A}}\mathbf{b}\|_{2}^{2}=\mathbf{b}^{\top}\mathbf{P}_{ \mathbf{A}}\mathbf{b}\leq c\sigma^{2}\log(\det(\mathbf{A}^{\top}\mathbf{A})/ \delta)\leq c\sigma^{2}d\log(nB/\delta)\] _for some universal constant_ \(c>0\)_._
2. _Let_ \(\widetilde{\mathbf{A}}:=\mathbf{A}-\mathbf{P}_{\mathbf{1}_{n}}\mathbf{A}\) _be the centered matrix, then with probability over_ \(1-\delta\)__ \[\|\mathbf{P}_{\widetilde{\mathbf{A}}}\mathbf{b}\|_{2}^{2}=\mathbf{b}^{\top} \mathbf{P}_{\widetilde{\mathbf{A}}}\mathbf{b}\leq c\sigma^{2}\log(n\det( \widetilde{\mathbf{A}}^{\top}\widetilde{\mathbf{A}})/\delta)\leq c\sigma^{2} d\log(nB/\delta)\] _for some universal constant_ \(c>0\)_._

In the proofs we choose \((\mathbf{A},\mathbf{b})=(\mathbf{X}_{\mathrm{nad}},\boldsymbol{\varepsilon}), (\mathbf{X}_{\mathrm{ad}},\boldsymbol{\varepsilon}),(\mathbf{X}_{\mathrm{ad}},\mathbf{x}_{j})\) for \(j\in[k+1,d]\). It is readily verified that conditions in Lemma A.1 are satisfied under assumptions in Theorem 3.1 and 3.2. See the proof of this lemma in Section A.7.

**Lemma A.2**: _Suppose the data set \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) is \((k,d)\)-adaptive and satisfies Assumption (A1)-(A4). Suppose the sample size condition (6) is in force. Adopt the notations in Section 3.1.2. Define \(\overline{\mathbf{X}}_{\mathrm{nad}}=\mathbf{X}_{\mathrm{nad}}-\mathbb{E}[ \mathbf{X}_{\mathrm{nad}}]\) and recall that \(\widetilde{\mathbf{X}}_{\mathrm{nad}}=(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{1}_ {n}})\mathbf{X}_{\mathrm{nad}}\). For the matrices \(\widetilde{\mathbf{X}}_{\mathrm{nad}},\widetilde{\mathbf{X}}_{\mathrm{nad}}\), we have the following with probability over \(1-\delta\)_

\[\frac{1}{\sigma_{\mathrm{min}}(\overline{\mathbf{X}}_{\mathrm{ nad}})^{2}} =\left|\kern-1.075pt\left|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top} \overline{\mathbf{X}}_{\mathrm{nad}})^{-1}\right|\kern-1.075pt\right|_{op} \leq\frac{2}{n\sigma_{\mathrm{min}}}\] (14a) \[\left|\kern-1.075pt\left|\kern-1.075pt\left|\overline{ \mathbf{X}}_{\mathrm{nad}}\right|\kern-1.075pt\right|_{op} \leq\sqrt{2n\sigma_{\mathrm{max}}}\] (14b) \[\left|\kern-1.075pt\left|\kern-1.075pt\left|\overline{ \mathbf{X}}_{\mathrm{nad}}-\widetilde{\mathbf{X}}_{\mathrm{nad}}\right|\kern-1.075pt \right|_{op} \leq C\sqrt{\log(n/\delta)}\sqrt{d-k}\leq\frac{1}{2}\sigma_{ \mathrm{min}}(\overline{\mathbf{X}}_{\mathrm{nad}}),\] (14c)

\[\left|\kern-1.075pt\left|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top} \overline{\mathbf{X}}_{\mathrm{nad}})^{-1}-(\widetilde{\mathbf{X}}_{\mathrm{ nad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}\right|\kern-1.075pt\right|_{op} \leq\frac{C\sqrt{\log(n/\delta)}\sqrt{d-k}}{n^{3/2}}.\] (14d) \[\left|\kern-1.075pt\left|\kern-1.075pt\left|\mathbf{P}_{ \widetilde{\mathbf{X}}_{\mathrm{nad}}}-\mathbf{P}_{\widetilde{\mathbf{X}}_{ \mathrm{nad}}}\right|\kern-1.075pt\right|_{op} \leq C\sqrt{\log(n/\delta)}\sqrt{\frac{d-k}{n}}\leq\frac{1}{4}\] (14e) \[\left|\kern-1.075pt\left|(\mathbf{P}_{\widetilde{\mathbf{X}}_{ \mathrm{nad}}}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\bm{\varepsilon }\right|\kern-1.075pt\right|_{2} \leq C\] (14f)

_for some parameter-dependent constants \(C>0\). Moreover, equation (14a), (14b) also hold when replacing \(\widetilde{\mathbf{X}}_{\mathrm{nad}}\) with the zero-mean matrix \(\mathbf{X}_{\mathrm{nad}}\) defined in Section 3.1.1._

See the proof in Section A.8.

**Lemma A.3**: _Under assumptions in Theorem 3.1, with probability over \(1-\delta\)_

\[\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{ X}_{\mathrm{ad}}\preceq\frac{C(d-k)k\log(n/\delta)}{n}\mathbf{X}_{\mathrm{ad}}^{ \top}\mathbf{X}_{\mathrm{ad}}\]

_for some parameter-dependent constant \(C>0\)._

See the proof in Section A.9.

**Lemma A.4**: _Under assumptions in Theorem 3.2, with probability over \(1-\delta\)_

\[\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\mathbf{P}_{\widetilde{\mathbf{X}}_ {\mathrm{nad}}}\widetilde{\mathbf{X}}_{\mathrm{ad}}\preceq\frac{C(d-k)k\log( n/\delta)}{n}\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}\widetilde{\mathbf{X}}_{ \mathrm{ad}}\]

_for some parameter-dependent constant \(C>0\)._

See the proof in Section A.10.

### Proof of Proposition 2.2

Part (a).Proposition 2.2(a) is a standard result on the minimax optimality of the OLS estimator in linear models. A proof of this result using a Bayes argument can be found in the proof of Theorem 2(a) in Khamaru et al. [20].

By properties of the OLS estimator, we have \((\widehat{\bm{\theta}}_{\mathrm{ols}}-\bm{\theta}^{*})\ \sim\mathcal{N}(0,\sigma^{2}( \mathbf{S}_{n})^{-1})\) conditioned on \(\mathbf{X}\). Therefore \((\widehat{\bm{\theta}}_{\mathrm{ols},\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^{*} )\ \sim\mathcal{N}(0,\sigma^{2}(\mathbf{S}_{n}^{-1})_{\mathcal{I}\mathcal{I}})\) conditioned on \(\mathbf{X}\), where \((\mathbf{S}_{n}^{-1})_{\mathcal{I}\mathcal{I}}\) denotes the submatrix of \(\mathbf{S}_{n}^{-1}\) that consists of the coordinates in \(\mathcal{I}\). It follows immediately that

\[\mathbb{E}\Big{\|}\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^ {*}\Big{\|}_{[(\mathbf{S}_{n}^{-1})_{\mathcal{I}\mathcal{I}}]^{-1}}^{2}=\sigma^ {2}\operatorname{tr}(\mathbf{I}_{|\mathcal{I}|})=\sigma^{2}|\mathcal{I}|\]

for any \(\bm{\theta}^{*}\in\mathbb{R}^{d}\). As a result, the OLS estimator attains the minimax lower bound when the data are i.i.d.

Part (b).When \(|\mathcal{I}|=1\), part (b) follows immediately from Theorem 2(b) of Khamaru et al. [20] with \(\mathbf{v}\) chosen to be the one-hot vector supported on \(\mathcal{I}\). When \(|\mathcal{I}|>1\), w.l.o.g. assume \(\mathcal{I}\) consists of the first \(|\mathcal{I}|\) coordinates of \([d]\). It suffices to show the scaled-MSE for \(\mathcal{I}\) is always no less than the scaled-MSE for the first coordinate.

This follows from properties of Schur complement and the projection operator,

\[\left\|\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^ {*}\right\|_{[(\mathbf{S}_{n}^{-1})_{\mathcal{I}\mathcal{I}}]^{-1}}^{2}\] \[=(\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^ {*})^{\top}\mathbf{X}_{\mathcal{I}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{ X}_{\mathcal{I}^{c}}})\mathbf{X}_{\mathcal{I}}(\widehat{\bm{\theta}}_{ \mathcal{I}}-\bm{\theta}_{\mathcal{I}}^{*})\] \[=(\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^ {*})^{\top}\begin{pmatrix}1&\star\\ 0&\mathbf{I}_{k-1}\end{pmatrix}\begin{pmatrix}d_{1}&\mathbf{0}^{\top}\\ \mathbf{0}&\mathbf{D}_{2}\end{pmatrix}\begin{pmatrix}1&0\\ \star&\mathbf{I}_{k-1}\end{pmatrix}(\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{ \theta}_{\mathcal{I}}^{*})\] \[=\begin{pmatrix}\widehat{\theta}_{1}-\theta_{1}^{*}&\star\\ \end{pmatrix}\begin{pmatrix}d_{1}&\mathbf{0}^{\top}\\ \mathbf{0}&\mathbf{D}_{2}\end{pmatrix}\begin{pmatrix}\widehat{\theta}_{1}- \theta_{1}^{*}\\ \star\end{pmatrix}\] \[\geq d_{1}(\widehat{\theta}_{1}-\theta_{1}^{*})^{2},\]

where \(\mathbf{P}_{\mathbf{M}}^{\perp}\) denote the projection onto the orthogonal space of the column space of \(\mathbf{M}\) and

\[d_{1}=\mathbf{x}_{1}^{\top}\mathbf{P}_{\mathbf{X}_{\mathcal{I}^ {c}}}^{\perp}\mathbf{x}_{1}-\mathbf{x}_{1}^{\top}\mathbf{P}_{\mathbf{X}_{ \mathcal{I}^{c}}}^{\perp}\mathbf{X}_{\mathcal{I},-1}(\mathbf{X}_{\mathcal{I}, -1}^{\top}\mathbf{P}_{\mathbf{X}_{\mathcal{I}^{c}}}^{\perp}\mathbf{X}_{ \mathcal{I},-1})^{-1}\mathbf{X}_{\mathcal{I},-1}^{\top}\mathbf{P}_{\mathbf{X}_ {\mathcal{I}^{c}}}^{\perp}\mathbf{x}_{1},\] \[\mathbf{D}_{2}=\mathbf{X}_{\mathcal{I},-1}^{\top}\mathbf{P}_{ \mathbf{X}_{\mathcal{I}^{c}}}^{\perp}\mathbf{X}_{\mathcal{I},-1}.\]

Using the properties of projection operator and linear space decomposition, it can be verified that

\[d_{1}=\mathbf{x}_{1}^{\top}\mathbf{P}_{\mathbf{X}_{-1}}^{\perp}\mathbf{x}_{1 }=\mathbf{x}_{1}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\mathbf{X}_{-1}})\mathbf{ x}_{1}.\]

Therefore \(\left\|\widehat{\bm{\theta}}_{\mathcal{I}}-\bm{\theta}_{\mathcal{I}}^{*} \right\|_{[(\mathbf{S}_{n}^{-1})_{\mathcal{I}\mathcal{I}}]^{-1}}^{2}\geq\left\| \widehat{\theta}_{1}-\theta_{1}^{*}\right\|_{[(\mathbf{S}_{n}^{-1})_{11}]^{-1}}^ {2}\). This completes the proof.

### Proof of Theorem 3.1

By the definition of the OLS estimator, we have

\[\widehat{\bm{\theta}}-\bm{\theta}^{*}=\left(\begin{array}{cc} \mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}}&\mathbf{X}_{\mathrm{ ad}}^{\top}\mathbf{X}_{\mathrm{nad}}\\ \mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{ad}}&\mathbf{X}_{\mathrm{ nad}}^{\top}\mathbf{X}_{\mathrm{nad}}\end{array}\right)^{-1}\cdot\left( \begin{array}{c}\mathbf{X}_{\mathrm{ad}}^{\top}\bm{\varepsilon}\\ \mathbf{X}_{\mathrm{nad}}^{\top}\bm{\varepsilon}\end{array}\right).\]

Applying the block matrix inverse formula, we obtain

\[\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{*}=(\mathbf{X} _{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}}-\mathbf{X}_{\mathrm{ad}}^{\top} \mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{X}_{\mathrm{ad}})^{-1}(\mathbf{ X}_{\mathrm{ad}}^{\top}\bm{\varepsilon}-\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{P}_{ \mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}).\]

To simplify notation, we define

\[\overline{\bm{R}}_{1}:=\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}} -\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{X}_ {\mathrm{ad}},\hskip 28.452756pt\overline{\bm{R}}_{2}:=\mathbf{X}_{\mathrm{ad}}^{ \top}\bm{\varepsilon}-\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{P}_{\mathbf{X}_{ \mathrm{nad}}}\bm{\varepsilon},\]

and let

\[\bm{R}_{1}:=\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}},\hskip 28.452756pt \bm{R}_{2}:=\mathbf{X}_{\mathrm{ad}}^{\top}\bm{\varepsilon}.\]

Therefore

\[(\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{*})^{\top} \overline{\bm{R}}_{1}(\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ ad}}^{*})=\overline{\bm{R}}_{2}^{\top}\overline{\bm{R}}_{1}^{-1}\overline{\bm{R}}_{2}.\]

We claim the following results which we prove later. With probability over \(1-\delta\)

\[\mathbf{0}\preceq\frac{1}{2}\bm{R}_{1}\preceq\overline{\bm{R}}_ {1}\preceq\bm{R}_{1},\] (15a) \[|\overline{\bm{R}}_{2}^{\top}\bm{R}_{1}^{-1}\overline{\bm{R}}_{2} -\bm{R}_{2}^{\top}\bm{R}_{1}^{-1}\bm{R}_{2}| \leq C\log(n\det(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ ad}})/\delta),\] (15b) \[\bm{R}_{2}^{\top}\bm{R}_{1}^{-1}\bm{R}_{2} \leq C\log(\det(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ ad}})/\delta),\] (15c)

Taking these claims as given, we establish

\[(\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{*})^{ \top}\overline{\bm{R}}_{1}(\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{ \mathrm{ad}}^{*})=\overline{\bm{R}}_{2}^{\top}\overline{\bm{R}}_{1}^{-1} \overline{\bm{R}}_{2}\leq 2\overline{\bm{R}}_{2}^{\top}\bm{R}_{1}^{-1}\overline{\bm{R}}_{2},\] (16) \[\overline{\bm{R}}_{2}^{\top}\bm{R}_{1}^{-1}\overline{\bm{R}}_{2} \leq|\overline{\bm{R}}_{2}^{\top}\bm{R}_{1}^{-1}\overline{\bm{R}}_{2}-\bm{R}_{ 2}^{\top}\bm{R}_{1}^{-1}\bm{R}_{2}|+\bm{R}_{2}^{\top}\bm{R}_{1}^{-1}\bm{R}_{2} \leq C\log(n\det(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})/\delta)\] (17)

where equation (16) uses claim (15a) and equation (17) uses claim (15b), (15c). Putting the last two displays together completes the proof.

Proof of claim (15a)The first and third inequality follows from the definition of \(\bm{R}_{1}\) and \(\bm{R}_{1}\). The second inequality follows from Lemma A.3 and noting that \(\frac{C(d-k)k\log(n/\delta)}{n}<1/2\) under the sample size assumption (6) with \(C\) in (6) chosen sufficiently large.

Proof of claim (15b)Define \(\widetilde{\mathbf{P}}_{\mathbf{A}}:=(\mathbf{A}^{\top}\mathbf{A})^{-1/2} \mathbf{A}^{\top}\) for any \(\mathbf{A}\in\mathbb{R}^{n\times d}\). Then we have \(\|\widetilde{\mathbf{P}}_{\mathbf{A}}\mathbf{b}\|_{2}=\|\mathbf{P}_{\mathbf{A }}\mathbf{b}\|_{2}\) for any \(\mathbf{b}\in\mathbb{R}^{n}\). Note that

\[|\widetilde{\bm{R}}_{2}^{\top}\bm{R}_{1}^{-1}\widetilde{\bm{R}}_ {2}-\bm{R}_{2}^{\top}\bm{R}_{1}^{-1}\bm{R}_{2}|\] \[\leq|(\widetilde{\bm{R}}_{2}-\bm{R}_{2})^{\top}\bm{R}_{1}^{-1}( \widetilde{\bm{R}}_{2}-\bm{R}_{2})|+2|(\widetilde{\bm{R}}_{2}-\bm{R}_{2})^{ \top}\bm{R}_{1}^{-1}\bm{R}_{2}|\] \[=\bm{e}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{X}_{ \mathrm{ad}}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}+2|\bm{ \varepsilon}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{P}_{\mathbf{X }_{\mathrm{ad}}}\bm{\varepsilon}|\] \[=\bm{e}^{\top}\widetilde{\mathbf{P}}_{\mathbf{X}_{\mathrm{nad}}} (\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1/2}(\mathbf{X}_ {\mathrm{nad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\mathbf{X}_{\mathrm{ nad}})(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1/2} \widetilde{\mathbf{P}}_{\mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}\] \[\quad+2|\bm{\varepsilon}^{\top}\widetilde{\mathbf{P}}_{\mathbf{X }_{\mathrm{nad}}}(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{ -1/2}\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{ad}}} \bm{\varepsilon}|\] \[\leq\frac{\|\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{P}_{\mathbf{X }_{\mathrm{ad}}}\mathbf{X}_{\mathrm{nad}}\|_{\mathrm{op}}}{\sigma_{\min}( \mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})}\|\widetilde{\mathbf{ P}}_{\mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}\|_{2}^{2}+2\frac{\|\mathbf{X}_{ \mathrm{nad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\bm{\varepsilon}\|_{ 2}}{\sigma_{\min}(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^ {1/2}}\|\widetilde{\mathbf{P}}_{\mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}\|_{ 2}.\]

Applying equation (14a) and Lemma A.1 in the last display, we continue

\[|\widetilde{\bm{R}}_{2}^{\top}\bm{R}_{1}^{-1}\widetilde{\bm{R}}_ {2}-\bm{R}_{2}^{\top}\bm{R}_{1}^{-1}\bm{R}_{2}|\] \[\leq\frac{C}{n}\operatorname{tr}(\mathbf{X}_{\mathrm{nad}}^{\top} \mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\mathbf{X}_{\mathrm{nad}})\|\widetilde{ \mathbf{P}}_{\mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}\|_{2}^{2}+\frac{C\| \mathbf{X}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{P}}_{\mathbf{X}_{\mathrm{ ad}}}\|_{\mathrm{F}}\|\widetilde{\mathbf{P}}_{\mathbf{X}_{\mathrm{ad}}}\bm{ \varepsilon}\|_{2}}{\sqrt{n}}\|\widetilde{\mathbf{P}}_{\mathbf{X}_{\mathrm{nad}}} \bm{\varepsilon}\|_{2}\] \[\leq\frac{C(d-k)}{n}\max_{k+1\leq j\leq d}(\mathbf{x}_{j}^{\top} \mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\mathbf{x}_{j})\|\mathbf{P}_{\mathbf{X}_{ \mathrm{nad}}}\bm{\varepsilon}\|_{2}^{2}\] \[\quad+\frac{C\sqrt{k}\max_{k+1\leq j\leq d}(\sqrt{\mathbf{x}_{j}^ {\top}\mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\mathbf{x}_{j}})\|\mathbf{P}_{ \mathbf{X}_{\mathrm{ad}}}\bm{\varepsilon}\|_{2}}{\sqrt{n}}\|\mathbf{P}_{ \mathbf{X}_{\mathrm{nad}}}\bm{\varepsilon}\|_{2}\] \[\leq\frac{C(d-k)^{2}}{n}\log(n\det(\mathbf{X}_{\mathrm{ad}}^{\top} \mathbf{X}_{\mathrm{ad}})/\delta)\log(n/\delta)+\frac{C\sqrt{k(d-k)}}{\sqrt{n}} \log(n\det(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})/\delta) \log^{1/2}(n/\delta)\] \[\leq\log(n\det(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{ \mathrm{ad}})/\delta),\]

where the fourth line uses Lemma A.1 and equation (14a), the last line follows from the sample size assumption (6).

Proof of claim (15c)This is a direct consequence of Lemma A.1 since \(\varepsilon_{i}\) are conditionally zero-mean sub-Gaussian by Assumption (A4).

### Proof of Theorem 3.2

Let \(\bm{\mu}^{*}=(\bm{\mu}_{\mathrm{ad}}^{*\top},\bm{\mu}_{\mathrm{nad}}^{*\top})^ {\top}\) denote the mean vector of \(\mathbb{E}[\bm{x}_{i}]\) and define

\[\widetilde{\mathbf{X}}_{\mathrm{nad}}:=\mathbf{X}_{\mathrm{nad}}-\mathbf{1}_{n} \bm{\mu}_{\mathrm{nad}}^{*\top}.\]

We write \(\widetilde{\mathbf{X}}_{\mathrm{nad}}=[\widetilde{\mathbf{x}}_{k+1}\quad\dots \quad\widetilde{\mathbf{x}}_{d}]\). The proof of this theorem follows the same basic steps as the proof of Theorem 3.1.

Recall that \(\widehat{\bm{\theta}}_{\mathrm{ad}}\in\mathbb{R}^{k}\) denotes the non-adaptive component of the centered OLS estimator. By definition and the matrix inverse formula, we have

\[\widehat{\bm{\theta}}_{\mathrm{ad}}-\bm{\theta}_{\mathrm{ad}}^{*}=(\widetilde{ \mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{ \mathbf{X}}_{\mathrm{nad}}})\widetilde{\mathbf{X}}_{\mathrm{ad}})^{-1}\cdot \widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{ \mathbf{X}}_{\mathrm{nad}}})\bm{\varepsilon}.\]

To simplify notation, we introduce

\[\widetilde{\bm{R}}_{3}:=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}- \mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\widetilde{\mathbf{X}}_{ \mathrm{ad}},\qquad\widetilde{\bm{R}}_{4}:=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{ \top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\bm{\varepsilon},\]

and

\[\widetilde{\bm{R}}_{3}:=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}- \mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\widetilde{\mathbf{X}}_{ \mathrm{ad}},\qquad\widetilde{\bm{R}}_{4}:=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{ \top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\bm{\varepsilon},\]

\[\bm{R}_{3}:=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top}\widetilde{\mathbf{X}}_{ \mathrm{ad}},\qquad\qquad\qquad\bm{R}_{4}:=\widetilde{\mathbf{X}}_{\mathrm{ad}}^{ \top}\bm{\varepsilon},\]Consequently,

\[(\widehat{\bm{\theta}}_{\rm ad}-\bm{\theta}_{\rm ad}^{*})^{\top} \widetilde{\bm{R}}_{3}(\widehat{\bm{\theta}}_{\rm ad}-\bm{\theta}_{\rm ad}^{*})= \widetilde{\bm{R}}_{4}^{\top}\widetilde{\bm{R}}_{3}^{-1}\widetilde{\bm{R}}_{4}.\]

Again, we claim the following results which we prove later. With probability over \(1-\delta\)

\[\bm{0}\preceq\frac{1}{2}\bm{R}_{3}\preceq\widetilde{\bm{R}}_{3} \preceq\bm{R}_{3},\] (18a) \[|\widetilde{\bm{R}}_{4}^{\top}\bm{R}_{3}^{-1}\widetilde{\bm{R}}_ {4}-\bm{R}_{4}^{\top}\bm{R}_{3}^{-1}\bm{R}_{4}| \leq C\log(n\det(\widetilde{\mathbf{X}}_{\rm ad}^{\top}\widetilde{ \mathbf{X}}_{\rm ad})/\delta),\] (18b) \[\bm{R}_{4}^{\top}\bm{R}_{3}^{-1}\bm{R}_{4} \leq C\log(n\det(\widetilde{\mathbf{X}}_{\rm ad}^{\top}\widetilde {\mathbf{X}}_{\rm ad})/\delta).\] (18c)

With the claims at hand, we obtain

\[(\widehat{\bm{\theta}}_{\rm ad}-\bm{\theta}_{\rm ad}^{*})^{\top} \widetilde{\bm{R}}_{3}(\widehat{\bm{\theta}}_{\rm ad}-\bm{\theta}_{\rm ad}^{* })=\widetilde{\bm{R}}_{4}^{\top}\widetilde{\bm{R}}_{3}^{-1}\widetilde{\bm{R}}_ {4}\leq 2\widetilde{\bm{R}}_{4}^{\top}\bm{R}_{3}^{-1}\widetilde{\bm{R}}_{4},\] (19) \[\widetilde{\bm{R}}_{4}^{\top}\bm{R}_{3}^{-1}\widetilde{\bm{R}}_ {4}\leq|\widetilde{\bm{R}}_{4}^{\top}\bm{R}_{3}^{-1}\widetilde{\bm{R}}_{4}-\bm {R}_{4}^{\top}\bm{R}_{3}^{-1}\bm{R}_{4}|+\bm{R}_{4}^{\top}\bm{R}_{3}^{-1}\bm{ R}_{4}\leq C\log(n\det(\widetilde{\mathbf{X}}_{\rm ad}^{\top}\widetilde{\mathbf{X}}_{ \rm ad})/\delta)\] (20)

where equation (19) uses claim (18a) and equation (20) uses claim (18b), (18c). Combining the last two displays concludes the proof.

Proof of claim (18a)The first and third inequality follows from the definition of \(\widetilde{\bm{R}}_{3}\) and \(\bm{R}_{3}\). For the second inequality, we have

\[\bm{R}_{3}-\widetilde{\bm{R}}_{3} =(\bm{R}_{3}-\overline{\bm{R}}_{3})+(\overline{\bm{R}}_{3}- \widetilde{\bm{R}}_{3})\] \[=\widetilde{\mathbf{X}}_{\rm ad}^{\top}\mathbf{P}_{\widetilde{\bm{ \chi}}_{\rm ad}}\widetilde{\mathbf{X}}_{\rm ad}+\widetilde{\mathbf{X}}_{\rm ad }^{\top}(\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm ad}}-\mathbf{P}_{\widetilde{ \mathbf{X}}_{\rm ad}})\widetilde{\mathbf{X}}_{\rm ad}\] \[\preceq\frac{1}{4}\widetilde{\mathbf{X}}_{\rm ad}^{\top}\widetilde {\mathbf{X}}_{\rm ad}+\frac{1}{4}\widetilde{\mathbf{X}}_{\rm ad}^{\top} \widetilde{\mathbf{X}}_{\rm ad}=\frac{1}{2}\bm{R}_{3},\]

where the last line uses Lemma A.2, A.4 and noting that \(\frac{C(d-k)k\log(n/\delta)}{n}<1/4\) under our sample size assumption (6) with \(C\) in (6) chosen sufficiently large.

Proof of claim (18b)Recall that we define \(\widetilde{\mathbf{P}}_{\mathbf{A}}:=(\mathbf{A}^{\top}\mathbf{A})^{-1/2} \mathbf{A}^{\top}\) for any \(\mathbf{A}\in\mathbb{R}^{n\times d}\). Note that

\[|\widetilde{\bm{R}}_{4}^{\top}\bm{R}_{3}^{-1}\widetilde{\bm{R}}_ {4}-\bm{R}_{4}^{\top}\bm{R}_{3}^{-1}\bm{R}_{4}|\] \[\leq|(\widetilde{\bm{R}}_{4}-\bm{R}_{4})^{\top}\bm{R}_{3}^{-1}( \widetilde{\bm{R}}_{4}-\bm{R}_{4})|+2|(\widetilde{\bm{R}}_{4}-\bm{R}_{4})^{ \top}\bm{R}_{3}^{-1}\bm{R}_{4}|\] \[\leq 2[(\widetilde{\bm{R}}_{4}-\overline{\bm{R}}_{4})^{\top}\bm{R} _{3}^{-1}(\widetilde{\bm{R}}_{4}-\overline{\bm{R}}_{4})+(\overline{\bm{R}}_{4 }-\bm{R}_{4})^{\top}\bm{R}_{3}^{-1}(\overline{\bm{R}}_{4}-\bm{R}_{4})\] \[\quad+|(\widetilde{\bm{R}}_{4}-\overline{\bm{R}}_{4})^{\top}\bm{R} _{3}^{-1}\bm{R}_{4}|+|(\overline{\bm{R}}_{4}-\bm{R}_{4})^{\top}\bm{R}_{3}^{-1} \bm{R}_{4}|]\] \[=:2[W_{1}+W_{2}+W_{3}+W_{4}],\]

where

\[W_{1}:=\bm{\varepsilon}^{\top}(\mathbf{P}_{\widetilde{\mathbf{X} }_{\rm had}}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm had}})\mathbf{P}_{ \widetilde{\mathbf{X}}_{\rm ad}}(\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm had}}- \mathbf{P}_{\widetilde{\mathbf{X}}_{\rm had}})\bm{\varepsilon}\] \[W_{2}:=|\bm{\varepsilon}^{\top}(\mathbf{P}_{\widetilde{\mathbf{X} }_{\rm had}}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm had}})\mathbf{P}_{ \widetilde{\mathbf{X}}_{\rm ad}}\bm{\varepsilon}|\] \[W_{3}:=\bm{\varepsilon}^{\top}\mathbf{P}_{\widetilde{\mathbf{X} }_{\rm ad}}\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm ad}}\bm{\varepsilon}.\] \[W_{4}:=|\bm{\varepsilon}^{\top}\mathbf{P}_{\widetilde{\mathbf{X} }_{\rm had}}\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm ad}}\bm{\varepsilon}|.\]

We next bound \(W_{i}(i=1,2,3,4)\) respectively.

For \(W_{1}\) and \(W_{2}\), we have from Lemma A.1 and equation (14f) that

\[W_{1} \leq\|(\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm had}}-\mathbf{P}_{ \widetilde{\mathbf{X}}_{\rm had}})\bm{\varepsilon}\|_{2}^{2}\leq C\] \[W_{2} \leq\|\bm{\varepsilon}^{\top}(\mathbf{P}_{\widetilde{\mathbf{X}}_{ \rm had}}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\rm had}})\|_{2}\|\mathbf{P}_{ \widetilde{\mathbf{X}}_{\rm ad}}\bm{\varepsilon}\|_{2}\leq C\sqrt{\log(n\det( \widetilde{\mathbf{X}}_{\rm ad}^{\top}\widetilde{\mathbf{X}}_{\rm ad})/\delta)}.\]For \(W_{3}\), similar to the proof of claim (15b), we have

\[W_{3} =\epsilon^{\top}\widetilde{\mathbf{P}}_{\widetilde{\mathbf{X}}_{ \mathrm{nad}}}(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}} _{\mathrm{nad}})^{-1/2}\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\mathbf{P}_{ \widetilde{\mathbf{X}}_{\mathrm{nad}}}\overline{\mathbf{X}}_{\mathrm{nad}}( \overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}}) ^{-1/2}\widetilde{\mathbf{P}}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}\boldsymbol {\varepsilon}\] \[\leq\frac{(d-k)\max_{k+1\leq j\leq d}\overline{\mathbf{x}}_{j}^{ \top}\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{ad}}}\overline{\mathbf{x}}_{j }}{\sigma_{\min}(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{ \mathbf{X}}_{\mathrm{nad}})}\|\widetilde{\mathbf{P}}_{\widetilde{\mathbf{X}}_{ \mathrm{nad}}}\boldsymbol{\varepsilon}\|_{2}^{2}\] \[\leq\frac{C(d-k)^{2}}{n}\log(n\det(\widetilde{\mathbf{X}}_{ \mathrm{ad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{ad}})/\delta)\log(n/\delta)\] \[\leq C\log(n\det(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top} \widetilde{\mathbf{X}}_{\mathrm{ad}})/\delta),\]

where third line follows from Lemma A.1 and the last line follows from the sample size assumption (6). Likewise,

\[W_{4} =|\boldsymbol{\varepsilon}^{\top}\mathbf{P}_{\widetilde{\mathbf{X }}_{\mathrm{nad}}}\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{ad}}}\boldsymbol {\varepsilon}|\] \[\leq\|\widetilde{\mathbf{P}}_{\widetilde{\mathbf{X}}_{\mathrm{nad }}}\boldsymbol{\varepsilon}\|_{2}\|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{ \top}\overline{\mathbf{X}}_{\mathrm{nad}})^{-1/2}\|_{\mathbf{0}\mathbf{p}}\| \overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\mathbf{P}_{\widetilde{\mathbf{X}}_ {\mathrm{nad}}}\boldsymbol{\varepsilon}\|_{2}\] \[\leq\frac{c\sqrt{d-k}}{\sigma_{\min}(\widetilde{\mathbf{X}}_{ \mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}})^{1/2}}\max_{k+1\leq j \leq d}\|\widetilde{\mathbf{x}}_{j}^{\top}\widetilde{\mathbf{P}}_{\widetilde{ \mathbf{X}}_{\mathrm{nad}}}\|_{2}\|\widetilde{\mathbf{P}}_{\widetilde{\mathbf{X }}_{\mathrm{nad}}}\boldsymbol{\varepsilon}\|_{2}\|\widetilde{\mathbf{P}}_{ \widetilde{\mathbf{X}}_{\mathrm{nad}}}\boldsymbol{\varepsilon}\|_{2}\] \[\leq\frac{C(d-k)}{\sqrt{n}}\log(n\det(\widetilde{\mathbf{X}}_{ \mathrm{ad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{ad}})/\delta)\sqrt{\log(n/\delta)}\] \[\leq C\log(n\det(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top} \widetilde{\mathbf{X}}_{\mathrm{ad}})/\delta).\]

where the third inequality follows from Lemma A.1 again. Putting pieces together yields the desired result.

Proof of claim (18c)This is a direct consequence of Lemma A.1.

### Proof of Corollary 3.3

W.l.o.g. assume \(\ell=1\). By Schur decomposition, we have

\[(\widetilde{\boldsymbol{\theta}}_{\mathrm{ad}}-\boldsymbol{\theta }_{\mathrm{ad}}^{*})^{\top}\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top} \widetilde{\mathbf{X}}_{\mathrm{ad}}(\widetilde{\boldsymbol{\theta}}_{\mathrm{ ad}}-\boldsymbol{\theta}_{\mathrm{ad}}^{*})\] \[=(\widetilde{\boldsymbol{\theta}}_{\mathrm{ad}}-\boldsymbol{ \theta}_{\mathrm{ad}}^{*})^{\top}\begin{pmatrix}1&\widetilde{\mathbf{x}}_{1}^{ \top}\widetilde{\mathbf{X}}_{\mathrm{ad},-1}\left(\widetilde{\mathbf{X}}_{ \mathrm{ad},-1}^{\top}\widetilde{\mathbf{X}}_{\mathrm{ad},-1}\right)^{-1}\\ 0&\mathbf{I}_{k-1}\end{pmatrix}\begin{pmatrix}\widetilde{\mathbf{x}}_{1}^{ \top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{ad},-1}}) \widetilde{\mathbf{x}}_{1}&\mathbf{0}^{\top}\\ \mathbf{0}&\widetilde{\mathbf{X}}_{\mathrm{ad},-1}^{\top}\widetilde{\mathbf{ X}}_{\mathrm{ad},-1}\end{pmatrix}\] \[\quad\cdot\begin{pmatrix}1&0\\ \left(\widetilde{\mathbf{X}}_{\mathrm{ad},-1}^{\top}\widetilde{\mathbf{X}}_{ \mathrm{ad},-1}\right)^{-1}\widetilde{\mathbf{X}}_{\mathrm{ad},-1}^{\top} \widetilde{\mathbf{x}}_{\mathrm{ad},-1}\end{pmatrix}(\widetilde{\boldsymbol {\theta}}_{\mathrm{ad}}-\boldsymbol{\theta}_{\mathrm{ad}}^{*})\] \[=\begin{pmatrix}\widetilde{\theta}_{\mathrm{ad},1}-\theta_{ \mathrm{ad},1}^{*}&\star\end{pmatrix}\begin{pmatrix}\widetilde{\mathbf{x}}_{1}^{ \top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{ad},-1}}) \widetilde{\mathbf{x}}_{1}&\mathbf{0}^{\top}\\ \mathbf{0}&\widetilde{\mathbf{X}}_{\mathrm{ad},-1}^{\top}\widetilde{\mathbf{X}}_ {\mathrm{ad},-1}\end{pmatrix}\begin{pmatrix}\widetilde{\theta}_{\mathrm{ad},1}- \theta_{\mathrm{ad},1}^{*}\\ \star\end{pmatrix}\] \[\geq(\widetilde{\theta}_{\mathrm{ad},1}-\theta_{\mathrm{ad},1}^{*}) ^{2}(\widetilde{\mathbf{x}}_{1}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{ \mathbf{X}}_{\mathrm{ad},-1}})\widetilde{\mathbf{x}}_{1}).\]

Therefore by Lemma A.4, the sample size assumption (6), and Theorem 3.2, we establish

\[(\widetilde{\theta}_{\mathrm{ad},1}-\theta_{\mathrm{ad},1}^{*})^{ 2}(\widetilde{\mathbf{x}}_{1}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{ \mathbf{X}}_{\mathrm{ad},-1}})\widetilde{\mathbf{x}}_{1})\] \[\leq(\widetilde{\boldsymbol{\theta}}_{\mathrm{ad}}-\boldsymbol{ \theta}_{\mathrm{ad}}^{*})^{\top}\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top} \widetilde{\mathbf{X}}_{\mathrm{ad}}(\widetilde{\boldsymbol{\theta}}_{\mathrm{ ad}}-\boldsymbol{\theta}_{\mathrm{ad}}^{*})\leq C(\widetilde{\boldsymbol{ \theta}}_{\mathrm{ad}}-\boldsymbol{\theta}_{\mathrm{ad}}^{*})^{\top}\widetilde{ \mathbf{X}}_{\mathrm{ad}}^{\top}(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_ {\mathrm{nad}}})\widetilde{\mathbf{X}}_{\mathrm{ad}}(\widetilde{\boldsymbol{ \theta}}_{\mathrm{ad}}-\boldsymbol{\theta}_{\mathrm{ad}}^{*})\] \[\leq C\log(n\det(\widetilde{\mathbf{X}}_{\mathrm{ad}}^{\top} \widetilde{\mathbf{X}}_{\mathrm{ad}})/\delta)\leq Ck\log(n/\delta).\]

Corollary 3.3 follows immediately from the fact that \(\mathbf{I}_{n}-\mathbf{P}_{\widetilde{\mathbf{X}}_{-1}}\preceq\mathbf{I}_{n}- \mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{ad},-1}}\) since \(\widetilde{\mathbf{X}}_{\mathrm{ad},-1}\) consists of some columns of \(\widetilde{\mathbf{X}}_{-1}\).

### Proof of Theorem 3.4

We start with the following decomposition

\[\sum_{i=1}^{n}w_{i}x_{i}^{\mathrm{ad}}\bigg{(}\widehat{\theta}_{\mathrm{TALE}}- \theta_{\mathrm{ad}}^{*}\bigg{)}=\underbrace{\sum_{i=1}^{n}w_{i}\epsilon_{i}}_{ v_{n}}+\underbrace{\sum_{i=1}^{n}w_{i}\bm{x}_{i}^{\mathrm{nad}\top}(\bm{\theta}_{ \mathrm{nad}}^{*}-\widehat{\bm{\theta}}_{\mathrm{nad}}^{\mathrm{Pr}})}_{b_{n}}.\]

To prove the theorem, it suffices to show

\[v_{n}\xrightarrow{d}\mathcal{N}(0,\alpha\cdot\sigma^{2})\qquad\text{and} \qquad b_{n}\xrightarrow{p}0,\]

where \(\xrightarrow{d}\) stands for convergence in distribution and \(\alpha\) is a constant that is specified in equation (21).

Proof of \(v_{n}\xrightarrow{d}\mathcal{N}(0,\alpha\cdot\sigma^{2})\):The proof of this part directly follows from [39, Theorem 3.1]. For completeness, we provide a proof here. Note that function \(f\) is a positive decreasing function and satisfies properties

\[\int_{1}^{\infty}f(x)dx=\infty\qquad\text{and}\qquad\int_{1}^{\infty}f^{2}(x) dx=\alpha.\] (21)

Furthermore, it can be shown that

\[\max_{1\leq i\leq n}f^{2}(\frac{s_{i}}{s_{0}})\frac{(x_{i}^{\mathrm{ad}})^{2} }{s_{0}}=o_{p}(1)\quad\text{and}\quad\max_{1\leq i\leq n}\bigg{(}1-\frac{f(s_{ i}/s_{0})}{f(s_{i-1}/s_{0})}\bigg{)}=o_{p}(1).\] (22)

Next we compute

\[\sum_{i=1}^{n}w_{i}^{2} =\sum_{i=1}^{n}f^{2}(s_{i}/s_{0})\frac{(x_{i}^{\mathrm{ad}})^{2}} {s_{0}}=\int_{1}^{s_{n}/s_{0}}f^{2}(x)dx\cdot\frac{\sum_{t\leq n}f^{2}(s_{i}/s _{0})(x_{i}^{\mathrm{ad}})^{2}/s_{0}}{\int_{1}^{s_{n}/s_{0}}f^{2}(x)dx}\] \[\stackrel{{(i)}}{{=}}\int_{1}^{s_{n}/s_{0}}f^{2}(x) dx\cdot(1+\frac{\sum_{i\leq n}(\frac{f^{2}(s_{i}/s_{0})}{f^{2}(\xi_{i}/s_{0})}-1)f^{2}( \xi_{i}/s_{0})(x_{i}^{\mathrm{ad}})^{2}/s_{0}}{\sum_{i\leq n}f^{2}(\xi_{i}/s_{0 })(x_{i}^{\mathrm{ad}})^{2}/s_{0}}).\]

In equation \((i)\), we consider the mean value theorem where \(\int_{s_{i-1}/s_{0}}^{s_{i}/s_{0}}f^{2}(x)dx=f^{2}(\xi_{i}/s_{0})(x_{i}^{ \mathrm{ad}})^{2}/s_{0}\). Consequently, we have

\[\frac{\sum_{i\leq n}|\frac{f^{2}(s_{i}/s_{0})}{f^{2}(\xi_{i}/s_{ 0})}-1|f^{2}(\xi_{i}/s_{0})(x_{i}^{\mathrm{ad}})^{2}/s_{0}}{\sum_{i\leq n}f( \xi_{i}/s_{0})(x_{i}^{\mathrm{ad}})^{2}/s_{0}} \leq\frac{\sum_{i\leq n}|\frac{f^{2}(s_{i}/s_{0})}{f^{2}(s_{i-1}/ s_{0})}-1|f^{2}(\xi_{i}/s_{0})(x_{i}^{\mathrm{ad}})^{2}/s_{0}}{\sum_{i\leq n}f^{2}( \xi_{i}/s_{0})(x_{i}^{\mathrm{ad}})^{2}/s_{0}}\] \[\leq\max_{i\leq n}\bigg{(}1-\frac{f^{2}(s_{i}/s_{0})}{f^{2}(s_{i -1}/s_{0})}\bigg{)}=o_{p}(1).\]

We conclude that

\[\sum_{i=1}^{n}w_{i}^{2}=(1+o_{p}(1))\int_{1}^{s_{n}/s_{0}}f^{2}(x)dx=\alpha+o_{ p}(1)\] (23)

By noticing \(\max_{1\leq i\leq n}w_{i}^{2}=\max_{1\leq i\leq n}f^{2}(s_{i}/s_{0})(x_{i}^{ \mathrm{ad}})^{2}/s_{0}^{2}=o_{p}(1)\), we conclude from martingale central limit theorem that

\[\sum_{i=1}^{n}w_{i}\varepsilon_{i}\xrightarrow{d}\mathcal{N}(0,\alpha\cdot \sigma^{2}).\]

Moreover, applying Slutsky's theorem yields

\[\frac{1}{\widehat{\sigma}(\sum_{1\leq i\leq n}w_{i}^{2})^{1/2}}\bigg{(}\sum_{i= 1}^{n}w_{i}\varepsilon_{i}\bigg{)}\xrightarrow{d}\mathcal{N}(0,1).\] (24)Proof of \(b_{n}\stackrel{{ p}}{{\rightarrow}}0\):To simplify notations, let \(\bm{w}=(w_{1},\ldots,w_{n})^{\top}\). Without loss of generality, we consider the first column of the design matrix is collected adaptively. By the definition of \(b_{n}\), we observe that

\[\begin{split}\left|\sum_{i=1}^{n}w_{i}\bm{x}_{i}^{\mathrm{nad} \top}(\widehat{\bm{\theta}}_{\mathrm{nad}}^{\text{Pr}}-\bm{\theta}_{\mathrm{ nad}}^{*})\right|&\leq\|\sum_{i=1}^{n}w_{i}\bm{x}_{i}^{\mathrm{nad}}\|_{2} \cdot\|\widehat{\bm{\theta}}_{\mathrm{nad}}^{\text{Pr}}-\bm{\theta}_{\mathrm{ nad}}^{*}\|_{2}\\ &=\underbrace{\sqrt{\sum_{i=2}^{d}(\bm{w}^{\top}\mathbf{x}_{i}^{ \mathrm{nad}})^{2}}}_{\triangleq b_{n,1}}\cdot\underbrace{\|\widehat{\bm{ \theta}}_{\mathrm{nad}}^{\text{Pr}}-\bm{\theta}_{\mathrm{nad}}^{*}\|_{2}}_{ \triangleq b_{n,2}}\end{split}\] (25)

Analysis of \(b_{n,1}\):By the construction of the weights \(\{w_{i}\}_{1\leq i\leq n}\), we have

\[\|\bm{w}\|_{2}^{2}\leq\int_{1}^{\infty}f^{2}(x)dx=\alpha.\] (26)

Applying Lemma A.1 with \(\bm{\mathrm{A}}=\bm{w}\) and \(b=\mathbf{x}_{i}^{\mathrm{nad}}\), we conclude that with probability at least \(1-\delta\),

\[(\bm{w}^{\top}\mathbf{x}_{i}^{\mathrm{nad}})^{2}\leq c\nu^{2}\|\bm{w}\|_{2}^{2 }\log(\|\bm{w}\|_{2}^{2}/\delta)\leq c\nu^{2}\alpha\log(\alpha/\delta),\] (27)

where \(c\) is a universal constant. Therefore, with probability at least \(1-\delta\),

\[b_{n,1}\leq\sqrt{d}\cdot\sqrt{c\nu^{2}\alpha\log(d\alpha/\delta)}.\] (28)

Analysis of \(b_{n,2}\):note \(\widehat{\bm{\theta}}_{\mathrm{nad}}^{\text{Pr}}=\widehat{\bm{\theta}}_{ \mathrm{nad}}^{\mathrm{ols}}\) is the OLS estimate. Therefore, we can use block-wise matrix inverse formula to get its expression. Precisely, we have

\[\begin{split}\widehat{\bm{\theta}}_{\mathrm{nad}}^{\text{Pr}}- \bm{\theta}_{\mathrm{nad}}^{*}&=-\frac{(\mathbf{X}_{\mathrm{nad}}^ {\top}\mathbf{X}_{\mathrm{nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{ x}_{1}\mathbf{x}_{1}^{\top}\bm{\varepsilon}}{\|\mathbf{x}_{1}-\mathbf{P}_{\mathbf{X}_{ \mathrm{nad}}}\|_{2}^{2}}+(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{ nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{\top}\bm{\varepsilon}\\ &\quad\quad\quad\quad\quad\quad\quad+\frac{(\mathbf{X}_{\mathrm{ nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{\top} \mathbf{x}_{1}\mathbf{x}_{1}^{\top}\mathbf{X}_{\mathrm{nad}}(\mathbf{X}_{ \mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{ \top}\bm{\varepsilon}}{\|\mathbf{x}_{1}-\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}} \mathbf{x}_{1}\|_{2}^{2}}.\end{split}\] (29)

Therefore, we can upper bound \(b_{n,2}\) by

\[\begin{split} b_{n,2}&\leq\underbrace{\frac{1}{\| \mathbf{x}_{1}-\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{x}_{1}\|_{2}^{2}} \|\|(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}\|_{ \text{op}}\cdot\|\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{x}_{1}\|_{2}\cdot\| \mathbf{x}_{1}^{\top}\bm{\varepsilon}\|_{2}}_{\doteq b_{n,2}^{(1)}}\\ &\quad\quad\quad+\underbrace{\|(\mathbf{X}_{\mathrm{nad}}^{\top} \mathbf{X}_{\mathrm{nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{\top}\bm{\varepsilon} \|_{2}}_{\doteq b_{n,2}^{(2)}}\\ &\quad\quad\quad+\underbrace{\frac{1}{\|\mathbf{x}_{1}-\mathbf{P}_ {\mathbf{X}_{\mathrm{nad}}}\mathbf{x}_{1}\|_{2}^{2}}\|(\mathbf{X}_{\mathrm{ nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}\|_{\text{op}}\cdot\|\mathbf{X}_{ \mathrm{nad}}^{\top}\mathbf{x}_{1}\|_{2}^{2}\cdot\|(\mathbf{X}_{\mathrm{nad}}^{ \top}\mathbf{X}_{\mathrm{nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{\top}\bm{ \varepsilon}\|_{2}}_{\doteq b_{n,2}^{(3)}}.\end{split}\] (30)To analyze terms \(b_{n,2}^{(1)},b_{n,2}^{(2)},\) and \(b_{n,2}^{(3)},\) we make use of the results from Lemma A.1, Lemma A.2, and Lemma A.3. Specifically, we have with probability \(1-\delta\), the following statements hold

\[b_{n,2}^{(1)} =\frac{1}{\|\mathbf{x}_{1}-\mathbf{P}_{\mathbf{X}_{\mathrm{nad}} }\mathbf{x}_{1}\|_{2}^{2}}\cdot\|\!(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X} _{\mathrm{nad}})^{-1}\|\!|_{\mathsf{op}}\cdot\|\mathbf{X}_{\mathrm{nad}}^{\top} \mathbf{x}_{1}\|_{2}\cdot\|\mathbf{x}_{1}^{\top}\mathbf{\varepsilon}\|_{2}\] \[\stackrel{{(i)}}{{\leq}}c^{\prime}\left(\frac{1}{(1- Cd\log(n/\delta)/n)\cdot\|\mathbf{x}_{1}\|_{2}^{2}}\right)\cdot\frac{1}{n}\cdot\| \mathbf{x}_{1}\|_{2}\sqrt{d}\sqrt{\log(d\|\mathbf{x}_{1}\|_{2}^{2}/\delta)}\] \[\qquad\cdot\|\mathbf{x}_{1}\|_{2}\sqrt{\log(\|\mathbf{x}_{1}\|_{ 2}^{2}/\delta)}\] \[\leq\frac{c^{\prime}\sqrt{d}\log(n^{2}d/\delta)}{(1-Cd\log(n/ \delta)/n)n}\] \[b_{n,2}^{(2)} =\|\!(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}}) ^{-1}\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{\varepsilon}\|_{2}\leq\|\!|( \mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}|\!|\!|_{ \mathsf{op}}\cdot\|\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{\varepsilon}\|_{2}\] (31) \[\stackrel{{(ii)}}{{\leq}}c^{\prime\prime}\sqrt{\frac{ d\log(d/\delta)}{n}}\] \[b_{n,2}^{(3)} =\frac{1}{\|\mathbf{x}_{1}-\mathbf{P}_{\mathbf{X}_{\mathrm{nad}} }\mathbf{x}_{1}\|_{2}^{2}}\|\!|(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{ \mathrm{nad}})^{-1}|\!|\!|_{\mathsf{op}}\cdot\|\mathbf{X}_{\mathrm{nad}}^{\top }\mathbf{x}_{1}\|_{2}^{2}\cdot\|(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{ \mathrm{nad}})^{-1}\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{\varepsilon}\|_{2}\] \[\stackrel{{(iii)}}{{\leq}}\frac{1}{(1-Cd\log(n/ \delta)/n)\cdot\|\mathbf{x}_{1}\|_{2}^{2}}\cdot\frac{c^{\prime\prime\prime}}{n }\cdot d\|\mathbf{x}_{1}\|_{2}^{2}\log(d\|\mathbf{x}_{1}\|_{2}^{2}/\delta) \cdot\sqrt{\frac{d\log(d/\delta)}{n}}\] \[\leq c^{\prime\prime\prime}\frac{1}{(1-Cd\log(n/\delta)/n)}\frac{ d^{3/2}\{\log(dn^{2}/\delta)\}^{3/2}}{n^{3/2}},\]

where \(c^{\prime}\), \(c^{\prime\prime}\) and \(c^{\prime\prime\prime}\) are universal constants that are independent of \(n\) and \(d\). In inequatiity \((i)\), we use Lemma A.3 to obtain a lower bound for \(\|\mathbf{x}_{1}-\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{x}_{1}\|_{2}^{2}\) and apply Lemma A.1 and Lemma A.2 to control the other three terms separately. Inequality \((ii)\) makes use of the fact that \(\bm{x}_{ij}\varepsilon_{i}\) is sub-exponential with parameter \((c\nu v,c\nu v)\) conditioned on \(\mathcal{F}_{i-1}\) for \(j=2,\ldots,d\). Therefore, by Azuma-Bernstein inequality and the sample size assumption, we obtain for \(2\leq j\leq d\),

\[|\mathbf{x}_{j}^{\top}\mathbf{\varepsilon}|\leq\nu v\sqrt{\log(d/\delta)} \Big{(}\sqrt{n}\vee\sqrt{\log(d/\delta)}\Big{)}=\nu v\sqrt{n\log(d/\delta)}\]

with probability over \(1-\delta/d\) for any \(2\leq j\leq d\). Applying a union bound to \(j=2,\ldots,d\), we have

\[\|\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{\varepsilon}\|_{2}\leq C^{\prime} nd\log(d/\delta),\] (32)

for some constant \(C^{\prime}\). Inequality \((iii)\) makes use of the bound for \(b_{n,2}^{(2)}\) and Lemmas A.1 and A.2. Therefore, when \(d^{2}\log^{2}(n)/n\to 0\), we conclude

\[b_{n,1}\cdot b_{n,2}=o_{p}(1).\] (33)

With \(b_{n}\stackrel{{ p}}{{\longrightarrow}}0\) at hand, a direct application of Slutsky's theorem yields

\[\frac{1}{\widehat{\sigma}\sqrt{\sum_{1\leq i\leq n}w_{i}^{2}}} \bigg{(}\sum_{i=1}^{n}w_{i}\mathbf{x}_{i}^{\mathrm{nad}\top}\bigg{)}\cdot( \widehat{\bm{\theta}}_{\mathrm{nad}}^{\mathbf{p}_{\mathrm{r}}}-\bm{\theta}_{ \mathrm{nad}}^{\ast})\stackrel{{ p}}{{\longrightarrow}}0.\] (34)

Putting things together, we conclude that

\[\frac{1}{\widehat{\sigma}\sqrt{\sum_{1\leq i\leq n}w_{i}^{2}}} \bigg{(}\sum_{i=1}^{n}w_{i}x_{i}^{\mathrm{ad}}\bigg{)}\cdot(\widehat{\theta}_{ \mathrm{TALE}}-\theta_{1}^{\ast})\stackrel{{ d}}{{\longrightarrow}} \mathcal{N}(0,1).\] (35)

### Proof of Lemma a.1

Proof of Part (a)The proof follows immediately from choosing \(V=\mathbf{I}_{d}\) in Theorem 1 of Abbasi et al. [1].

[MISSING_PAGE_FAIL:23]

By equation (14a), (14b), (14c) and a standard triangular inequality, we find

\[\|\!\|\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{ \mathrm{nad}}-\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_ {\mathrm{nad}}\|\!\|_{\text{op}}\leq C\sqrt{\log(n/\delta)}\sqrt{(d-k)n}\leq \frac{n\sigma_{\min}}{4}\leq\frac{1}{2}\sigma_{\min}(\overline{\mathbf{X}}_{ \mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}}),\]

where the second inequality follows from the sample size assumption (6). Therefore, we have

\[\|\!\|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{ \mathrm{nad}})^{-1}\|\!\|_{\text{op}}\leq\frac{2}{\sigma_{\min}(\overline{ \mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}})}\]

and hence

\[\|\!\|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{ \mathbf{X}}_{\mathrm{nad}})^{-1}-(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top }\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}\|\!\|_{\text{op}} \leq\frac{C}{\sigma_{\min}(\overline{\mathbf{X}}_{\mathrm{nad}}^{ \top}\overline{\mathbf{X}}_{\mathrm{nad}})^{2}}\|\!\|\mathbf{X}_{\mathrm{nad}}^ {\top}\overline{\mathbf{X}}_{\mathrm{nad}}-\widetilde{\mathbf{X}}_{\mathrm{ nad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}}\|\!\|_{\text{op}}\] \[\leq\frac{C\sqrt{\log(n/\delta)}\sqrt{d-k}}{n^{3/2}}.\] (37)

This gives equation (14d). Moreover, note that

\[\|\!\|\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}-\mathbf{ P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}\|\!\|_{\text{op}}\] \[\leq\|\!\|\widetilde{\mathbf{X}}_{\mathrm{nad}}[(\overline{ \mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}})^{-1}-( \widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{ nad}})^{-1}]\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\|\!\|_{\text{op}}\] \[\quad+\|\!\|(\overline{\mathbf{X}}_{\mathrm{nad}}-\widetilde{ \mathbf{X}}_{\mathrm{nad}})(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top} \overline{\mathbf{X}}_{\mathrm{nad}})^{-1}(\overline{\mathbf{X}}_{\mathrm{nad} }-\widetilde{\mathbf{X}}_{\mathrm{nad}})^{\top}\|\!\|_{\text{op}}\] \[\quad+2\|(\overline{\mathbf{X}}_{\mathrm{nad}}-\widetilde{ \mathbf{X}}_{\mathrm{nad}})^{\top}(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top} \overline{\mathbf{X}}_{\mathrm{nad}})^{-1}\overline{\mathbf{X}}_{\mathrm{nad}} ^{\top}\|\!\|_{\text{op}}\] \[\leq\|\!\|\widetilde{\mathbf{X}}_{\mathrm{nad}}\|\!\|_{\text{op}}^{ 2}\|\!\|(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{ \mathrm{nad}})^{-1}-(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{ \mathbf{X}}_{\mathrm{nad}})^{-1}\|\!\|_{\text{op}}\] \[\quad+\|\!\|\overline{\mathbf{X}}_{\mathrm{nad}}-\widetilde{ \mathbf{X}}_{\mathrm{nad}}\|\!\|_{\text{op}}(\|\!\|\overline{\mathbf{X}}_{ \mathrm{nad}}-\widetilde{\mathbf{X}}_{\mathrm{nad}}\|\!\|_{\text{op}}+2\| \overline{\mathbf{X}}_{\mathrm{nad}}\|\!\|_{\text{op}})\|(\overline{\mathbf{X} }_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}})^{-1}\|\!\|_{ \text{op}}.\]

It follows immediately from equation (14a), (14b), (14c), (14d) and (37) that with probability over \(1-\delta\)

\[\|\!\|\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}-\mathbf{ P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}\|\!\|_{\text{op}}\leq\frac{C\sqrt{\log(n/ \delta)}\sqrt{d-k}}{\sqrt{n}}.\]

This yields equation (14e).

Proof of equation (14f).Define \(\boldsymbol{\Delta}:=\boldsymbol{\hat{\mu}}-\boldsymbol{\mu}^{\star}\). We have

\[\|(\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}-\mathbf{ P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}})\boldsymbol{\varepsilon}\|_{2}\] \[\leq\|\!|\widetilde{\mathbf{X}}_{\mathrm{nad}}(\widetilde{\mathbf{ X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}\boldsymbol{ \Delta}\boldsymbol{1}_{n}^{\top}\boldsymbol{\varepsilon}\|_{2}+\|(\widetilde{ \mathbf{X}}_{\mathrm{nad}}(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{ \mathbf{X}}_{\mathrm{nad}})^{-1}-\overline{\mathbf{X}}_{\mathrm{nad}}( \overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}})^ {-1})\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\boldsymbol{\varepsilon}\|_{2}\] \[\leq\|\!|\widetilde{\mathbf{X}}_{\mathrm{nad}}(\widetilde{\mathbf{ X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}\|\!\|_{\text{op}}\| \boldsymbol{\Delta}\|_{2}|\boldsymbol{1}_{n}^{\top}\boldsymbol{\varepsilon}\| +\|\!|\widetilde{\mathbf{X}}_{\mathrm{nad}}(\widetilde{\mathbf{X}}_{\mathrm{nad }}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}\] \[\quad-\overline{\mathbf{X}}_{\mathrm{nad}}(\overline{\mathbf{X}}_{ \mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}})^{-1}\|\!\|_{\text{op}}\| \overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{\mathrm{nad}}^{ \top}\overline{\mathbf{X}}_{\mathrm{nad}}\|_{\text{op}}^{1/2}\|\widetilde{ \mathbf{P}}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}\boldsymbol{\varepsilon}\|_{2}.\]

Since \(|\boldsymbol{1}_{n}^{\top}\boldsymbol{\varepsilon}|\leq cv\sqrt{n\log(1/\delta)}\) with probability over \(1-\delta\) by Assumption (A4) and concentration of sub-Gaussian variables, and

\[\|\!|\!|\widetilde{\mathbf{X}}_{\mathrm{nad}}(\widetilde{\mathbf{ X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}-\overline{\mathbf{X}}_{ \mathrm{nad}}(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{\mathbf{X}}_{ \mathrm{nad}})^{-1}|\!|\!|_{\text{op}}\] \[\leq \|\!|\widetilde{\mathbf{X}}_{\mathrm{nad}}-\overline{\mathbf{X}}_{ \mathrm{nad}}\|\!|_{\text{op}}\|\!|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{ \top}\widetilde{\mathbf{X}}_{\mathrm{nad}})^{-1}\|\!|\!|_{\text{op}}\] \[\quad+|\!|\!|\overline{\mathbf{X}}_{\mathrm{nad}}|\!|_{\text{op}} \|\!|(\widetilde{\mathbf{X}}_{\mathrm{nad}}^{\top}\widetilde{\mathbf{X}}_{ \mathrm{nad}})^{-1}-(\overline{\mathbf{X}}_{\mathrm{nad}}^{\top}\overline{ \mathbf{X}}_{\mathrm{nad}})^{-1}|\!|\!|_{\text{op}}\]

by the triangular inequality, it follows immediately from combining equation (14a), (14b), (14c), (14d), (36), (37) that

\[\|(\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{nad}}}-\mathbf{P}_{\widetilde{ \mathbf{X}}_{\mathrm{nad}}})\boldsymbol{\varepsilon}\|_{2}\leq C\Big{[}\sqrt{ \frac{(d-k)\log(n/\delta)\log(1/\delta)}{n}}+\sqrt{\frac{(d-k)^{2}\log^{2}(n/ \delta)}{n}}\Big{]}\leq C\]

with probability over \(1-\delta\).

### Proof of Lemma a.3

Denote \((\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})^{-1/2}\mathbf{X}_{ \mathrm{ad}}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\mathbf{X}_{\mathrm{ad} }(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})^{-1/2}\) by \(\mathbf{B}\). By our construction and noting that \(\|\!\|\mathbf{B}\|\!\|_{\text{op}}\leq\operatorname{tr}(\mathbf{B})\), it suffices to show

\[\operatorname{tr}(\mathbf{B})\leq\frac{C(d-k)k\log(n/\delta)}{n}\]

with probability over \(1-\delta\) for some \(C>0\). Plugging in the definition of \(\mathbf{P}_{\mathbf{X}_{\mathrm{nad}}}\), we obtain

\[\operatorname{tr}(\mathbf{B}) =\operatorname{tr}((\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{ \mathrm{ad}})^{-1/2}\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{nad}}( \mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}\mathbf{X}_{ \mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{ad}}(\mathbf{X}_{\mathrm{ad}}^{\top} \mathbf{X}_{\mathrm{ad}})^{-1/2})\] \[=\operatorname{tr}(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{ \mathrm{ad}}(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})^{-1} \mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{nad}}(\mathbf{X}_{\mathrm{ nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1})\] \[\leq\operatorname{tr}(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_ {\mathrm{ad}}(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})^{-1} \mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{nad}})\cdot\|(\mathbf{X}_{ \mathrm{nad}}^{\top}\mathbf{X}_{\mathrm{nad}})^{-1}\|\!\|_{\text{op}}\] \[\leq\frac{c}{n\sigma_{\min}}\operatorname{tr}(\mathbf{X}_{\mathrm{ nad}}^{\top}\mathbf{X}_{\mathrm{ad}}(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{ \mathrm{ad}})^{-1}\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{nad}})\] (38)

with probability over \(1-\delta\), where the third line follows from von Neumann's trace inequality (see e.g., Theorem A.15 in Bai et al. [3]), and the last line uses equation (14a). Write \(\mathbf{X}_{\mathrm{ad}}=[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{k} ]\in\mathbb{R}^{n\times k}\) and \(\mathbf{X}_{\mathrm{nad}}=[\mathbf{x}_{k+1},\mathbf{x}_{k+2},\ldots,\mathbf{x }_{d}]\in\mathbb{R}^{n\times(d-k)}\). Following the calculation, we further have

\[\operatorname{tr}(\mathbf{X}_{\mathrm{nad}}^{\top}\mathbf{X}_{ \mathrm{ad}}(\mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{ad}})^{-1} \mathbf{X}_{\mathrm{ad}}^{\top}\mathbf{X}_{\mathrm{nad}})=\sum_{j=k+1}^{d} \mathbf{x}_{j}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\mathbf{x}_{j}.\] (39)

It follows from Lemma A.1 that

\[\mathbf{x}_{j}^{\top}\mathbf{P}_{\mathbf{X}_{\mathrm{ad}}}\mathbf{x}_{j}\leq Ck \log(n/\delta)\] (40)

for all \(k+1\leq j\leq d\) with probability over \(1-\delta\). The desired result follows immediately from combining equation (40) with (38) and (39).

### Proof of Lemma a.4

Following the same arguments as in the proof of Lemma A.3 with \(\mathbf{X}_{\mathrm{ad}}\) replaced by \(\widetilde{\mathbf{X}}_{\mathrm{ad}}:=\mathbf{X}_{\mathrm{ad}}-\mathbf{P}_{ \mathbf{1}_{n}}\mathbf{X}_{\mathrm{ad}}\), it suffices to show

\[\mathbf{x}_{j}^{\top}\mathbf{P}_{\widetilde{\mathbf{X}}_{\mathrm{ad}}}\mathbf{ x}_{j}\leq Ck\log(n/\delta).\] (41)

for all \(k+1\leq j\leq d\) with probability over \(1-\delta\). This follows immediately from Lemma A.1 (b).

## Appendix B Simulation set up

We provide implementation details of our simulations in this section. The code is available at https://github.com/licong-lin/low-dim-debias.

### Single coordinate estimation

In this section, we detail the simulation set up that was used to generate the Figure 1. The goal of this simulation is to display the effect of degree of adaptivity \(k\) on the estimation of a single coordinate, and provide empirical validation to the theory developed in the paper (c.f. Corollary 3.3).

We want to design an adaptive data collection mechanism, which can capture the estimation lower bound. Therefore, we adopt a similar data collection procedure as provided in Khamaru et al. [20]. We also refer readers to Lattimore [25] for related information.

**Simulation set-up:**

* Sample size \(n=1000\), \(d=300\).
* The degree of adaptivity \(k\) varies from 2 to 200 with step size equal to \(3\).
* Replication number \(20\) for each level of adaptivity \((k,d)\).
* \(\theta_{1}^{\mathrm{ad}}=1\) and other coefficients are generated independently from \(\mathcal{N}(0,1)\).
* \(\bm{x}_{i}^{\mathrm{nad}}\) is generated independently from uniform distribution on the sphere \(\mathcal{S}^{d-k-1}\). If \(\bm{x}_{i}^{\mathrm{nad}}\) has mean not equal to zero, then we consider \(\bm{x}_{i}^{\mathrm{nad}}\) plus \(\mathbb{E}\bm{x}_{i}^{\mathrm{nad}}\), where \(\mathbb{E}\bm{x}_{i}^{\mathrm{nad}}\) is generated from \(\mathcal{N}(\mathbf{1},\mathbf{I}_{d-k})\).

Data collection method:Here we modified the data collection algorithm from Section \(5.2.2\) in Khamaru et al. [20]. The only difference between our data collection algorithm and the one in Khamaru et al. [20] is that we replace \(m_{u,v}\coloneqq\sum_{w=1}^{v}b_{w}\left(y_{u,w}-a_{u,w}\right)\) by

\[m_{u,v}\coloneqq\sum_{w=1}^{v}b_{w}\left(y_{u,w}-a_{u,w}-\bm{\theta}^{\mathrm{ nad}\top}\bm{x}_{u+(w-1)(d-1)}^{\mathrm{nad}}\right).\] (42)

Figure 1 shows that empirical relation between the MSE of the centered OLS estimate of the first coordinate and the degree of dependence \(k\).

### Single coordinate inference

In this section, we detail the simulation set up that is used to generate the Figure 2 and 3.

We generate a dataset \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) that satisfies the assumptions in Theorem 3.4. On this simulated dataset, we compare our method with the ordinary least squares (OLS) estimator, W-decorrelation proposed by Deshpande [13], and the non-asymptotic confidence intervals derived from Theorem 8 in Lattimore et al. [26].

We begin by describing our data generating mechanism. We assume the data \(\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\in\mathbb{R}^{d}\times\mathbb{R}\) are generated from a linear model \(y_{i}=\bm{x}_{i}^{\top}\bm{\theta}^{*}+\varepsilon_{i},\) where \(\varepsilon_{i}\overset{iid}{\sim}\mathcal{N}(0,\sigma^{2}).\) We generate the covariates \(\{\bm{x}_{i}\}_{i=1}^{n}\) in the following way

1. We assume the non-adaptive component \(\bm{x}_{i,2:d}\) are i.i.d \(\mathcal{N}(0,\mathbf{I}_{d-1})\) across \(i\in[n]\).
2. For the adaptive coordinate, we choose \(x_{1,1}=1\) and assume \(x_{i,1}\in\{0,1\}\) for all \(i\in[n]\).
3. At each stage \(i\geq 2\), denote by \(\widehat{\theta}_{1}^{(i)}\) the OLS estimator for the first coordinate \(\theta_{1}^{*}\) obtained using the first \(i-1\) samples \((\mathbf{X}_{1:i-1},\bm{y}_{1:i-1})\). With probability \(p\) we choose \(x_{i,1}=1\) if \(\widehat{\theta}_{1}^{(i)}>0\) and \(x_{i,1}=0\) if otherwise; with probability \(1-p\) we simply choose \(x_{i,1}=1\) to encourage exploration.

Recalling Example 2.1 on treatment assignment, in the simulated data, we use the OLS estimator to obtain an prior estimate of the treatment effect \(\theta_{1}^{*}\) and assign the treatment to the \(i\)-th patient if the prior estimation suggests that the treatment has a positive effect \((i.e.,\widehat{\theta}_{1}^{(i)}>0)\). Moreover, to encourage exploration, we assign the treatment (i.e., \(x_{i,1}=1\)) with some small probability \(1-p\), regardless of the prior estimation.

Throughout the simulation we choose \(\bm{\theta}_{2:d}^{*}=\mathbf{1}_{d-1}/\sqrt{d-1}\) and \(\theta_{1}^{*}=0\), which corresponds to the case where no treatment effect is presented. We choose the noise level \(\sigma=0.3\) and the probability \(p=0.8\). In the simulations we assume the noise level is known for simplicity. We run our simulations on both a low-dimensional model \((n=1000,d=10)\) and a high-dimensional model \((n=500,d=50)\).

Comparison with W-decorrelation by Deshpande et al. [13]In our implementation of W-decorrelation, we follow Algorithm 1 in [13], with the parameter \(\lambda\cdot\log(n)\) be the \(1/n\)-quantile of \(\sigma_{\min}(\mathbf{X}^{\top}\mathbf{X})\). To estimate the quantile, we use the sample estimate from \(1000\) i.i.d. data matrices \(\mathbf{X}\)'s to estimate the quantile.