# Peri-midFormer: Periodic Pyramid Transformer for Time Series Analysis

Qiang Wu Gechang Yao Zhixi Feng Shuyuan Yang

Key Laboratory of Intelligent Perception and Image Understanding of

Ministry of Education, School of Artificial Intelligence, Xidian University, China

{wu_qiang, yao_gechang}@stu.xidian.edu.cn, {zxfeng, syyang}@xidian.edu.cn

###### Abstract

Time series analysis finds wide applications in fields such as weather forecasting, anomaly detection, and behavior recognition. Previous methods attempted to model temporal variations directly using 1D time series. However, this has been quite challenging due to the discrete nature of data points in time series and the complexity of periodic variation. In terms of periodicity, taking weather and traffic data as an example, there are multi-periodic variations such as yearly, monthly, weekly, and daily, etc. In order to break through the limitations of the previous methods, we decouple the implied complex periodic variations into inclusion and overlap relationships among different level periodic components based on the observation of the multi-periodicity therein and its inclusion relationships. This explicitly represents the naturally occurring pyramid-like properties in time series, where the top level is the original time series and lower levels consist of periodic components with gradually shorter periods, which we call the periodic pyramid. To further extract complex temporal variations, we introduce self-attention mechanism into the periodic pyramid, capturing complex periodic relationships by computing attention between periodic components based on their inclusion, overlap, and adjacency relationships. Our proposed Peri-midFormer demonstrates outstanding performance in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. The code is available at [https://github.com/WuQiangXDU/Peri-midFormer](https://github.com/WuQiangXDU/Peri-midFormer).

## 1 Introduction

Time series analysis stands as a foundational challenge pivotal across diverse real-world scenarios [1], such as weather forecasting [2], imputation of missing data within offshore wind speed time series [3], anomaly detection for industrial maintenance [4], and classification [5]. Due to its substantial practical utility, time series analysis has garnered considerable interest, leading to the development of a large number of deep learning-based methods for it.

Different from other forms of sequential data, like language or video, time series data is continuously recorded, capturing scalar values at each time point. Furthermore, real-world time series variations often entail complex temporal patterns, where multiple fluctuations (e.g., ascents, descents, fluctuations, etc.) intermingle and intertwine, particularly salient is the presence of various overlapping periodic components in it, rendering the modeling of temporal variations exceptionally challenging.

Deep learning models, known for their powerful non-linear capabilities, capture intricate temporal variations in real-world time series. Recurrent neural networks (RNNs) leverage sequential data, allowing past information to influence future predictions [6; 7]. However, they face challenges with long-term dependencies and computational inefficiency due to their sequential nature. Temporal convolutional neural networks (TCNs) [8; 9] extract variation information but struggle with capturing long-term dependencies. Transformers with attention mechanisms have gained popularity for sequential modeling [10; 11], capturing pairwise temporal dependencies among time points. Yet, discerning reliable dependencies directly from scattered time points remains challenging [12]. Timesnet [13] innovatively transforms 1D time series into 2D tensors, unifying intra- and inter-period variations in 2D space. However, it overlooks inclusion relationships between periods of different scales and is constrained by limited feature extraction capability of CNNs, hindering its ability to explore complex relationships within time series.

We analyze time series by examining the inclusion and overlap (hereinafter collectively referred to as inclusion) relationships between various periodic components to address complex temporal variations. Real-world time series often show multiple periodicities, like yearly and daily weather variations or weekly and daily traffic fluctuations. And these periods exhibit clear inclusion relationships, for instance, yearly weather variations encompass multiple daily weather variations. Besides variations between different period levels, it also occur within periods of the same level. For example, daily weather variations differ based on conditions like sunny or cloudy. Due to these inclusion and adjacency relationships, different periods show similarities, with short and long periods being consistent in overlapping portions, and periods of the same level being similar. Additionally, a long period can be decomposed into multiple short ones, forming a hierarchical pyramid structure. In our study, time series without explicit periodicity are treated as having infinitely long periods.

Based on the above analysis, we decompose the time series into multiple periodic components, forming a pyramid structure where longer components encompass shorter ones, termed the Periodic Pyramid as shown in Figure 1, which illustrates the intricate periodic inclusion relationships within the time series. Each level consists of components with the same period, exhibiting the same phase, while different levels contain components with inclusion relationships. This transformation converts the original 1D time series into a 2D representation, explicitly showing the implicit multi-period relationships. Within the Periodic Pyramid, a shorter period may belong to two longer periods simultaneously, reflecting the complexity of the time series. There is a clear similarity between components within the same level and those in adjacent levels where inclusion relationships exist. Thus inspired by Pyraformer [10], we propose the **Periodic Pyramid Transformer** (**Peri-midFormer**), which computes self-attention among periodic components to capture complex temporal variations in time series. Furthermore, we consider each branch in the Periodic Pyramid as a Periodic Feature Flow, and aggregating features from multiple flows to provide rich periodic information for downstream tasks. In experiments, Peri-midFormer achieves state-of-the-art performance in various analytic tasks, including forecasting, imputation, anomaly detection, and classification.

1. Based on the inclusion relationships of multiple periods in time series, this paper proposes a top-down constructed Periodic Pyramid structure, which expands 1D time series variations into 2D, explicitly representing the implicit multi-period relationships within the time series.
2. We propose Peri-midFormer, which uses the Periodic Pyramid Attention Mechanism to automatically capture dependencies between different and same-level periodic components, extracting diverse temporal variations in time series. Additionally, to further harness the potential of Peri-midFormer, we introduce Periodic Feature Flows to provide rich periodic information for downstream tasks.
3. We conduct extensive experiments on five mainstream time series analysis tasks, and Peri-midFormer achieves state-of-the-art across all of them, demonstrating its superior capability in time series analysis.

The remainder of this paper is structured as follows. Section 2 briefly summarizes the related work. Section 3 details the proposed model structure. Section 4 extensively evaluates our method's performance across five main time series analysis tasks. Section 5 presents ablations analysis, Section 6 presents complexity analysis, and Section 7 discusses our results and future directions.

Figure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid.

Related Work

Temporal variation modeling, a crucial aspect of time series analysis, has been extensively investigated. In recent years, numerous deep models have emerged for this purpose, including MLP [14; 15], TCN [8], and RNN [6; 7]-based architectures. Furthermore, Transformers have shown remarkable performance in time series forecasting [16; 12; 17; 18]. They utilize attention mechanisms to uncover temporal dependencies among time points. For instance, Wu et al. [12] introduce Autoformer with an Auto-Correlation mechanism, adept at capturing series-wise temporal dependencies derived from learned periods. Moreover, to address complex temporal patterns, they adopted a deep decomposition architecture to extract seasonal and trend parts from input series. Subsequently, FEDformer [17] enhances seasonal-trend decomposition through a mixture-of-expert design and introduces sparse attention within the frequency domain. Pyraformer [10] constructs a down-top pyramid structure through multiple convolution operations on time series to address the issue of long information propagation paths in Transformers, significantly reducing both time and space complexity. PatchTST [19] partitions individual data points into patches and uses them as tokens for the Transformer, thereby enhancing its understanding of local information in time series. Additionally, PatchTST innovatively processes each channel separately, making it particularly suitable for forecasting tasks.

Additionally, there are some recent innovative works. Timesnet [13] unravels intricate temporal patterns by exploring the multi-periodicity of time series and captures temporal 2D-variations using computer vision CNN backbones. GPT4TS [20] ingeniously utilizes the large language model GPT2 as a pretrained model, fine-tuning some of its structures with time series, achieving state-of-the-art results. FITS [21] proposes a time series analysis model based on frequency domain operations, requiring very low parameter count and memory consumption. And recent works have considered multi-scale information in time series. PDF [22] captures both short-term and long-term variations by transforming 1D time series into 2D tensors using a multi-periodic decoupling block. It achieves superior forecasting performance by modeling these decoupled variations and integrating them for accurate predictions. SCNN [23] decomposes multivariate time series into long-term, seasonal, short-term, co-evolving, and residual components, enhancing interpretability, adaptability to distribution shifts, and scalability by modeling each component separately. TimeMixer [24] uses a novel multiscale mixing approach, decomposing time series into fine and coarse scales to capture both detailed and macroscopic variations. It employs Past-Decomposable-Mixing to extract historical information and Future-Multipredictor-Mixing to leverage multiscale forecasting capabilities, achieving great performance in forecasting task.

## 3 Methodology

### Model Structure

The overall flowchart of the proposed approach is shown in Figure 2, it begins with time embedding of the original time series at the top. Then, we use the FFT to decompose it into multiple periodic components of varying lengths across different levels, with lines indicating the inclusion relationships between them. Moving down, padding and projection are then applied to ensure uniform dimensions, forming the Periodic Pyramid. Each component is treated as an independent token and receives positional embedding. Next, the Periodic Pyramid is fed into Peri-midFormer, which consists of multiple layers for computing Periodic Pyramid Attention. Finally, depending on the task, two strategies are employed: for classification, components are directly concatenated and projected into the category space; for other reconstruction tasks (since forecasting, imputation, and anomaly detection all necessitate the model to reconstruct the channel dimensions or input lengths, we collectively refer to such tasks as reconstruction tasks), features from different pyramid branches are integrated through Periodic Feature Flows Aggregation to generate the final output. Please note that we referred to [11] for de-normalization and [12] for time series decomposition to maximize the effectiveness of our method, but we omitted these details from the figure to maintain simplicity. See Appendix A for a complete flowchart. Further details are provided below.

### Periodic Pyramid Construction

Multiple periods in the time series exhibit clear inclusion relationships, however, the 1D structure limits the representation of variations between them. Hence, it's crucial to separate periodic components with inclusion relationships to explicitly represent implicit periodic relationships. Firstly, as Peri-midFormer is designed to focus on periodic components, we first normalize the original time series \(\mathbf{X}\in\mathbb{R}^{L\times C}\) that with length \(L\) and \(C\) channels, then decompose it to obtain the seasonal part \(\mathbf{X}_{s}\in\mathbb{R}^{L\times C}\), thus removing the interference of the trend part. For a detailed description of normalization and decomposition, please refer to the Appendix A. Then we partition \(\mathbf{X}_{s}\) into periodic components, following the approach used in Timesnet [13]. It's important to note that, inspired by PatchTST [19], we retain the channel dimension \(C\), as it is advantageous for Peri-midFormer in capturing periodic features within each channel (note that we adopt a channel independent strategy and the Figure 2 shows the processing of only one of the channels). The periodic components are extracted in the frequency domain, accomplished specifically through FFT:

\[\mathbf{A}=Avg\left(Amp\left(FFT\left(\mathbf{X}_{s}\right)\right)\right), \left\{f_{1},\cdots,f_{k}\right\}=\operatorname*{arg\,Topk}_{f_{*}\in\left\{1,\cdots,\left\lceil\frac{L}{2}\right\rceil\right\}}\left(\mathbf{A}\right),p_{ i}=\left\lceil\frac{L}{f_{i}}\right\rceil,i\in\left\{1,\cdots,k\right\}, \tag{1}\]

where \(FFT(\cdot)\) and \(Amp(\cdot)\) denote Fourier Transform and amplitude calculation, respectively. \(\mathbf{A}\in\mathbb{R}^{L}\) represents the amplitude of each frequency, averaged across \(C\) channels using Avg(\(\cdot\)). Note that the \(j\)-th value \(\mathbf{A}j\) denotes the intensity of the \(j\)-th frequency periodic basis function, associated with period length \(\left\lceil\frac{L}{j}\right\rceil\). To handle frequency domain sparsity and minimize noise from irrelevant high frequencies [17], the top-\(k\) amplitude values \(\{\mathbf{A}_{f_{1}},\cdots,\mathbf{A}_{f_{k}}\}\) corresponding to the most significant frequencies \(\left\{f_{1},\cdots,f_{k}\right\}\) are selected, where \(k\) is a hyper-parameter, beginning from 2 to ensure the fundamental pyramid structure. Additionally, to ensure the top level of the pyramid corresponds to the original time series, we define \(f_{1}=1\), with other frequencies arranged in ascending order. These selected frequencies correspond to \(k\) period lengths \(\left\{p_{1},\cdots,p_{k}\right\}\), arranged in descending order. Due to the frequency domain's conjugacy, only frequencies within \(\left\{1,\cdots,\left\lceil\frac{L}{2}\right\rceil\right\}\) are considered. Based on the selected frequencies \(\left\{f_{1},\cdots,f_{k}\right\}\) and their associated period lengths \(\left\{p_{1},\cdots,p_{k}\right\}\), we partition the original time series into periodic components for each pyramid level, denoted as \(\mathbf{C}_{\ell}\):

\[\mathbf{C}_{\ell}=\{\mathbf{C}_{\ell}^{1},\mathbf{C}_{\ell}^{2},\cdots, \mathbf{C}_{\ell}^{n}\},\ell\in\{1,\cdots,k\},n\in\{1,\cdots,f_{k}\}, \tag{2}\]

where \(\mathbf{C}_{\ell}^{n}\) denotes the \(n\)-th periodic component in the \(\ell\)-th pyramid level. Here, \(\ell\) is the pyramid level index, starting from the top and increasing, with a maximum value of \(k\), indicating the number of levels determined by the selected periods. Similarly, \(n\) represents the component index within a level, increasing from left to right, with a maximum value of \(f_{k}\), indicating the number of components per level is determined by the frequency corresponding to that period in the original series. The Periodic Pyramid can thus be represented as:

\[\mathbf{P}=Stack\left(\mathbf{C}_{\ell}\right),\ell\in\{1,\cdots,k\}, \tag{3}\]

Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.

here \(Stack(\cdot)\) denotes a stacking operation. The constructed Periodic Pyramid, depicted in the upper right of Figure 2, displays evident inclusion relationship among different levels, shown by connections between levels. Let \(R\) denote the relationship between pairs of periodic components from the upper and lower levels, determined by the presence or absence of overlap as follows:

\[R_{\ell-1,\ell}^{n_{\ell-1},n_{\ell}}=\left\{\begin{array}{l}1,\;Index(\mathbf{ C}_{\ell-1}^{n_{\ell-1}})\bigcap Index(\mathbf{C}_{\ell}^{n_{\ell}})>0\;\;\;,\; \ell\in\{2,\cdots,k\},\,n_{\ell-1},n_{\ell}\in\{1,\cdots,f_{k}\},\end{array}\right. \tag{4}\]

when \(R=1\), it signifies an inclusion relationship, while \(R=0\) indicates no overlap. This is illustrated in the left half of Figure 3. \(n_{\ell}\) denotes the index of the \(n\)-th component in the \(\ell\)-th level. \(Index(\cdot)\) denotes the positional index of each data point within the periodic component at that level. Indices for points in the first level are contained in \(\{0,\ldots,L-1\}\). For subsequent levels, most indices match those of the first level. However, due to varying component lengths, there may be slight differences in indices for the last portion. Nonetheless, this doesn't impact relationship determination between components across levels. In practice, the relationship between the components is realized by masking the corresponding elements in the attention matrix.

Thanks to the inclusion relationships between periodic components across different levels in the Periodic Pyramid, complex periodic relationships inherent in 1D time series are explicitly represented. Next, due to the varying lengths of the components, it's necessary to map \(\mathbf{C}^{\ell}\) to the same scale for subsequent Periodic Pyramid Attention Mechanism, with the equation provided as follows:

\[\mathbf{P}^{\prime}=Projection\left(Padding\left(\mathbf{C}_{\ell}^{n}\right) \right),\ell\in\{1,\cdots,k\},n\in\{1,\cdots,f_{k}\}, \tag{5}\]

where \(Padding(\cdot)\) denotes zero-padding the periodic components across the time dimension to match the length of the original data, while \(Projection(\cdot)\) represents a single linear mapping layer.

### Periodic Pyramid Transformer (Peri-midFormer)

Once we have the Periodic Pyramid, it can be inputted into the Peri-midFormer, as depicted in Figure 2. The Peri-midFormer introduces a specialized attention mechanism tailored for the Periodic Pyramid, called **P**eriodic **P**yramid **A**ttention **M**echanism (**PPAM**), shown in the right half of Figure 3. Here, original connections are replaced with bidirectional arrows, and also added within the same level. These bidirectional arrows signify attention between periodic components. In PPAM, inter-level attention focuses on period dependencies across levels, while intra-level attention focuses on dependencies within the same level. Note that attention occurs among all components within the same level, not just between adjacent ones. However, for clarity, not all attention connections within the same level are depicted.

In Periodic Pyramid, a periodic component \(\mathbf{C}_{\ell}^{n}\) generally has three types of interconnected relationships (denoted as \(\mathbb{I}\)) with other components: the parent node in the level above (denoted as \(\mathbb{P}\)), all nodes within the same level including itself (denoted as \(\mathbb{A}\)), and the child nodes in its next level (denoted as \(\mathbb{C}\)). Therefore, the relationships of \(\mathbf{C}_{\ell}^{n}\) can be expressed by the following equation:

\[\left\{\begin{array}{l}\mathbb{I}_{\ell}^{(n)}=\mathbb{P}_{\ell}^{(n)} \bigcup\mathbb{A}_{\ell}^{(n)}\bigcup\mathbb{C}_{\ell}^{(n)}\\ \mathbb{P}_{\ell}^{(n)}=\left\{\mathbf{C}_{\ell-1}^{j}:j=\{n_{\ell-1}:R_{\ell-1,\ell}^{n_{\ell-1},n_{\ell}}=1\}\right\},\quad\text{if }\ell\geq 2\text{ else } \varnothing\\ \mathbb{A}_{\ell}^{(n)}=\left\{\mathbf{C}_{\ell}^{j}:1\leq j\leq f_{k}\right\} \\ \mathbb{C}_{\ell}^{(n)}=\left\{\mathbf{C}_{\ell+1}^{j}:j=\{n_{\ell+1}:R_{\ell, \ell+1}^{n_{\ell},n_{\ell+1}}=1\}\right\},\quad\text{if }\ell\leq k-1\text{ else } \varnothing\end{array}\right.. \tag{6}\]

Figure 3: Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism (right).

The equation shows that a component at the topmost level lacks a parent node, while one at the bottommost level lacks a child node. Based on the interconnected relationships \(\mathbb{I}\), the attention of the component \(\mathbf{C}_{\ell}^{n}\) can be expressed as:

\[\mathbf{a}_{i}=\sum_{m\in\mathbb{I}_{\ell}^{\binom{n}{n}}}\frac{\exp\left( \mathbf{q}_{i}\mathbf{k}_{m}^{\top}/\sqrt{d_{K}}\right)\mathbf{v}_{m}}{\sum_{m \in\mathbb{I}_{\ell}^{\binom{n}{n}}}\exp\left(\mathbf{q}_{i}\mathbf{k}_{m}^{ \top}/\sqrt{d_{K}}\right)}, \tag{7}\]

where \(\mathbf{q}\), \(\mathbf{k}\), and \(\mathbf{v}\) denote query, key, and value vectors, respectively, as in the classical self-attention mechanism. \(m\) used for selecting components that have interconnected relationships with \(\mathbf{C}_{\ell}^{n}\). \(\mathbf{k}_{m}^{\top}\) represents the transpose of row \(m\) in \(K\). \(d_{K}\) refers to the dimension of key vectors, ensuring stable attention scores through scaling.

We apply this attention mechanism to each component across all levels of the Periodic Pyramid, enabling the automatic detection of dependencies among all components in the Periodic Pyramid and capturing the intricate temporal variations in the time series. For a detailed theoretical proof of the PPAM see the Appendix F.

### Periodic Feature Flows Aggregation

Here we explain the Periodic Feature Flows Aggregation used for reconstruction tasks. The output of the Peri-midFormer retains the original pyramid structure. To leverage the diverse periodic components across different levels, we treat a single branch from the top to the bottom of the pyramid as a periodic feature flow, highlighted by the red line in Figure 4. Since a periodic feature flow passes through periodic components at different levels, it contains periodic features of different scales from the time series. Additionally, due to variations among periodic components within each level, each feature flow carries distinct information. Therefore, we aggregate multiple feature flows through Periodic Feature Flow Aggregation. This involves linearly mapping each feature flow to match the length of the target time series and then averaging it across multiple feature flows to obtain the aggregated result \(\mathbf{Y}_{s}\), as expressed in the following equation:

\[\mathbf{Y}_{s}=MeanPolling\left(Projection\left(\left\{\hat{\mathbf{C}}_{1}^{n_ {1}},\hat{\mathbf{C}}_{2}^{n_{2}},\cdots,\hat{\mathbf{C}}_{\ell}^{n_{k}}\right\} \right)\right),\ell\in\{2,\cdots,k\},n_{k}\in\{1,\cdots,f_{k}\}, \tag{8}\]

where \(\hat{\mathbf{C}}_{\ell}^{n_{k}}\) represents a specific periodic component in the Peri-midFormer's output, and \(\{\hat{\mathbf{C}}_{1}^{n_{1}},\hat{\mathbf{C}}_{2}^{n_{2}},\cdots,\hat{ \mathbf{C}}_{\ell}^{n_{k}}\}\) forms a feature flow, as shown in Figure 4. \(Projection(\cdot)\) maps each feature flow to match the target output length. \(Meanpooling(\cdot)\) averages the feature flows. \(\mathbf{Y}_{s}\) indicates that this is the output from the seasonal part. Since we retained the channel dimension of the original time series, the result obtained after aggregating the periodic feature flows here becomes the shape of the final output. Finally, adding the trend part and de-normalization to obtain the ultimate output.

## 4 Experiments

We extensively test Peri-midFormer on five mainstream analysis tasks: short- and long-term forecasting, imputation, classification, and anomaly detection. We adopted the same benchmarks as Timesnet [13], see Appendix C for details. Due to space limits, we provide only a summary of the results here, more details about the datasets, experiment implementation, model configuration, and full results can be found in Appendix.

Figure 4: Periodic Feature Flows Aggregation.

**Baselines** The baselines include CNN-based models: TimesNet [13]; MLP-based models: LightTS [15], DLinear [14] and FITS [21]; Transformer-based models: GPT4TS [20], Time-LLM [25], iTransformer [26], TSLANet [27], Reformer [28], Pyraformer [10], Informer [16], Autoormer [12], FEDformer [17], Non-stationary Transformer [11], ETSformer [29], PatchTST [19]. Besides, N-HiTS [30] and N-BEATS [31] are used for short-term forecasting. Anomaly Transformer [32] is used for anomaly detection. Rocket [33], LSTNet [34], TCN [8] and Flowformer [35] are used for classification.

### Main Results

Figure 5 displays the comprehensive comparison results between Per-midFormer and other methods, it consistently excels across all five tasks.

### Long-term Forecasting

**Setups** Referring to [13], we adopt eight real-world benchmark datasets for long-term forecasting, including Weather [36], Traffic [37], Electricity [38], Exchange [34], and four EIT [16] datasets (ETTh1, ETH2, ETTm1, ETTm2). Forecast lengths are set to 96, 192, 336, and 720. For the fairness of the comparison, we set the look-back window for all the methods to 512 (64 on Exchange), the results for other look-back windows can be found in the Appendix H.3

**Results** From Table 1, it is evident that Peri-midFormer performs exceptionally well, even completely outperforms GPT4TS and closely approaching the state-of-the-art method Time-LLM. While Time-LLM demonstrates remarkable capabilities in long-term forecasting, our Peri-midFormer shows clear advantages on the ETTh2, Electricity, and Exchange datasets. Although Time-LLM achieves the best results, it relies on a very large model, leading to significant computational overhead that is unavoidable. The same issue exists for GPT4TS. In contrast, our Peri-midFormer achieves performance close to that of Time-LLM without requiring excessive computational resources, making it more suitable for practical applications. Further analysis of model complexity is provided in Section 6. In addition, our Peri-midFormer exhibits better performance with longer look-back window, as further detailed in the Appendix E.6.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c|}{**Per-mid**} & \multicolumn{2}{c|}{**GPT4TS**} & \multicolumn{2}{c|}{**TSLANet**} & \multicolumn{2}{c|}{**Time-LLM**} & \multicolumn{2}{c|}{**FITS**} & \multicolumn{2}{c|}{**DLinear**} & \multicolumn{2}{c}{**PatchTST**} & \multicolumn{2}{c}{**TimesNet**} & \multicolumn{2}{c}{**Pyraformer**} \\  & \multicolumn{2}{c|}{**Former**} & \multicolumn{2}{c|}{[20]} & \multicolumn{2}{c|}{[27]} & \multicolumn{2}{c|}{[25]} & \multicolumn{2}{c|}{[21]} & \multicolumn{2}{c|}{[14]} & \multicolumn{2}{c|}{[19]} & \multicolumn{2}{c|}{[13]} & \multicolumn{2}{c|}{[10]} \\ \hline Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline Weather & 0.233 & 0.271 & 0.237 & 0.270 & 0.276 & 0.291 & **0.225** & **0.257** & 0.244 & 0.281 & 0.241 & 0.294 & 0.226 & 0.266 & 0.249 & 0.286 & 0.281 & 0.349 \\ \hline ETTh1 & 0.409 & 0.430 & 0.427 & 0.426 & 0.422 & 0.440 & 0.408 & 0.423 & **0.407** & **0.422** & 0.418 & 0.438 & 0.430 & 0.444 & 0.492 & 0.490 & 0.913 & 0.748 \\ \hline ETTh2 & **0.317** & **0.377** & 0.354 & 0.394 & 0.328 & 0.385 & 0.334 & 0.383 & 0.333 & 0.328 & 0.504 & 0.482 & 0.388 & 0.414 & 0.408 & 0.440 & 1.374 & 0.554 \\ \hline ETTm1 & 0.354 & 0.385 & 0.352 & 0.383 & 0.348 & 0.383 & **0.329** & **0.372** & 0.358 & 0.376 & 0.379 & 0.363 & 0.391 & 0.398 & 0.418 & 0.724 & 0.609 \\ \hline EFTm2 & 0.258 & 0.320 & 0.266 & 0.326 & 0.263 & 0.263 & 0.325 & **0.251** & **0.313** & 0.254 & **0.313** & 0.275 & 0.342 & 0.273 & 0.329 & 0.287 & 0.343 & 1.356 & 0.797 \\ \hline Electricity & **0.152** & **0.202** & 0.167 & 0.263 & 0.195 & **0.224** & 0.158 & 0.252 & 0.168 & 0.263 & 0.167 & 0.268 & 0.166 & 0.257 & 0.217 & 0.314 & 0.299 & 0.391 \\ \hline Traffic & 1.392 & 0.270 & 0.414 & 0.294 & 0.397 & 0.272 & **0.388** & **0.264** & 0.420 & 0.287 & 0.433 & 0.305 & 0.392 & 0.220 & 0.622 & 0.332 & 0.705 & 0.401 \\ \hline Exchange & **0.346** & **0.393** & 0.373 & 0.410 & 0.365 & 0.410 & 0.371 & 0.409 & 0.393 & 0.429 & 0.495 & 0.493 & 0.418 & 0.433 & 0.701 & 0.593 & 1.157 & 0.844 \\ \hline Average & **0.308** & 0.332 & 0.324 & 0.346 & 0.320 & 0.341 & **0.308** & **0.334** & 0.322 & 0.344 & 0.361 & 0.375 & 0.331 & 0.350 & 0.350 & 0.422 & 0.402 & 11.147 & 0.711 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Long-term forecasting task. The results are averaged from four different series length \(\{96,192,336,720\}\). See Table 13 and 14 for full results. **Red**: best, Blue: second best.

Figure 5: Model performance comparison.

### Short-term Forecasting

**Setups** For short-term analysis, we adopt the M4 [39], which contains the yearly, quarterly and monthly collected univariate marketing data. We measure forecast performance using the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), which are calculated as detailed in the Appendix D.1.

**Results** Table 2 shows that Peri-midFormer outperforms Time-LLM, GPT4TS, TimesNet, and N-BEATS, highlighting its exceptional performance in short-term forecasting. In the M4 dataset, some data lacks clear periodicity, such as the Yearly data, which mainly exhibits a strong trend. A similar situation is observed in the Exchange dataset for long-term forecasting task. Peri-midFormer performs well on these datasets due to its time series decomposition strategy. For a detailed analysis, please refer to the Appendix E.7.

### Time Series Classification

**Setups** We assessed Peri-midFormer's capacity for high-level representation learning via classification task. Mimicking settings akin to TimesNet [13], we tested it on 10 multivariate UEA classification datasets from [42], covering tasks like gesture recognition, action recognition, audio recognition, medical diagnoses, and other real-world applications.

**Results** As shown in Figure 6, Peri-midFormer achieves an average accuracy of 76.6%, surpassing all baselines including TSLANet (76.0%), GPT4TS (74.0%), TimesNet (73.6%), and all other Transformer-based methods. This suggests that Peri-midFormer has excellent time series representation capabilities. See Appendix H.1 for full results.

### Imputation

**Setups** To validate Peri-midFormer's imputation capabilities, we conduct experiment on six real-world datasets, including four ETT datasets [16] (ETTh1, ETTh2, ETTm1, ETTm2), Electricity [38], and Weather [36]. We evaluate different random mask ratios (12.5%, 25%, 37.5%, 50%) for varying levels of missing data. Notably, due to the large number of missing values, the time series do not reflect their original periodicity. Therefore, before imputation, we simply interpolate the original missing data through a linear interpolation strategy in order to use Peri-midFormer efficiently, which we call pre-interpolation. For a description of pre-interpolation and its impact on other methods, please refer to the Appendix E.5.

**Results** Table 3 demonstrates Peri-midFormer's outstanding performance on specific datasets (Electricity and Weather), surpassing other methods significantly and securing the highest average scores. However, its performance on other datasets was ordinary, possibly due to the lack of obvious periodic characteristics in them.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Models}} & \multicolumn{2}{c}{**Peri-mid**} & Time-LLM & GPT4TS & TimesNet & Net-BEATS & N-BEATS & ETS- & Lights & Lights/TS & Dilation & FLOPS & Station & Auto & Pyra & Ine & Re- \\  & **Former** & [25] & [20] & [13] & [19] & [40] & [31] & [29] & [41] & [14] & [17] & [11] & [12] & [10] & [16] & [28] \\ \hline SMAPE & 11.833 & 11.993 & 11.991 & **11.829** & 12.059 & 11.927 & 11.851 & 14.718 & 3.525 & 13.639 & 12.840 & 12.780 & 12.909 & 16.987 & 14.086 & 18.200 \\ MASE & **1.584** & 1.595 & 1.600 & 1.585 & 1.623 & 1.613 & 1.599 & 2.408 & 1.111 & 2.095 & 1.701 & 1.756 & 1.771 & 3.265 & 2.718 & 4.223 \\ OWA & **0.850** & 0.859 & 0.861 & 0.851 & 0.869 & 0.861 & 0.855 & 1.172 & 1.051 & 1.051 & 0.918 & 0.930 & 0.939 & 1.480 & 1.230 & 1.775 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Short-term forecasting task on M4. The prediction lengths are in \(\{6,48\}\) and results are weighted averaged from several datasets under different sample intervals. (\(*\) means former, Station means the Non-stationary Transformer.) See Table 12 for full results. **Red**: best, **Blue**: second best.

Figure 6: Model comparison in classification. The results are averaged from 10 subsets of UEA.

[MISSING_PAGE_FAIL:9]

considering periodic components. "w/o PPAM" divides the time series into periodic components but without Period Pyramid Attention Mechanism, using periodic full attention instead, wherein attention is computed among all periodic components. "w/o Feature Flows Aggregation" employs PPAM but without Periodic Feature Flows Aggregation. "Peri-midFormer" indicates our final approach.

**Results** Table 5 illustrates the incremental performance enhancement achieved with the integration of each additional module, validating the effectiveness of Peri-midFormer. Notably, good results are achieved even without PPAM. This can be attributed to the model's ability to extract periodic characteristics inherent in the original time series data by delineating the periodic components. However, without highlighting inclusion relationships through PPAM, periodic full attention's ability to capture temporal changes is limited, emphasizing the significance of PPAM.

## 6 Complexity Analysis

We conducted experiments on the model complexity of Peri-midFormer using the Heartbeat dataset for the classification task and the ETTh2 dataset for the long-term forecasting task. We considered the number of training parameters, FLOPs, and accuracy (or MSE). The results are depicted in Figure 7. In the classification task, Peri-midFormer not only achieves a significant advantage in accuracy but also requires relatively fewer training parameters and FLOPs, much less than many methods such as TimesNet, GPT4TS, Crossformer, and PatchTST. In the long-term forecasting task, Peri-midFormer achieves the lowest MSE without requiring the enormous FLOPs that Time-LLM does. This shows that although Time-LLM has strong long-term forecasting capabilities on most datasets, its computational demands are unacceptable. See Appendix E.4 for more analysis.

Further Model Analysis is provided in the Appendix E.

## 7 Conclusions

In this paper, we introduced a method for general time series analysis called Peri-midFormer. It leverages the multi-periodicity of time series and the inclusion relationships between different periods. By segmenting the original time series into different levels of periodic components, Peri-midFormer constructs a Periodic Pyramid along with its corresponding attention mechanism. Through extensive experiments covering forecasting, classification, imputation, and anomaly detection tasks, we validated the capabilities of Peri-midFormer in time series analysis, achieving outstanding results across all tasks. However, Peri-midFormer exhibits limitations, particularly in scenarios where the periodic characteristics are less apparent. We aim to address this limitation in future research to broaden its applicability.

Figure 7: Number of training parameters and FLOPs for Peri-midFormer versus baseline in terms of classification accuracy for the UEA Heartbeat dataset (left) and long-term forecasting MSE for the ETTh2 dataset (right). In the left graph, the closer to the top left, the better, while in the right graph, the closer to the bottom left, the better. Note that in the long-term forecasting, we did not fully depict the corresponding sizes due to the oversized FLOPs of Time-LLM, but instead illustrated it with text.

## Acknowledgement

This work was supported by the National Natural Science Foundation of China under Grant Nos. 62276205, U22B2018, and Graduate Student Innovation Fund under Grant Nos. YJSJ24012.

## References

* [1]Q. Wen, L. Yang, T. Zhou, and L. Sun (2022) Robust time series analysis and applications: an industrial perspective. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4836-4837. Cited by: SS1.
* [2]K. Bi, L. Xie, H. Zhang, X. Chen, X. Gu, and Q. Tian (2023) Accurate medium-range global weather forecasting with 3d neural networks. Nature619 (7970), pp. 533-538. Cited by: SS1.
* [3]Y. Chen, C. Cai, L. Cao, D. Zhang, L. Kuang, Y. Peng, H. Pu, C. Wu, D. Zhou, and Y. Cao (2024) Windfix: harnessing the power of self-supervised learning for versatile imputation of offshore wind speed time series. Energy287, pp. 128995. Cited by: SS1.
* [4]H. Si, C. Pei, H. Cui, J. Yang, Y. Sun, S. Zhang, J. Li, H. Zhang, J. Han, D. Pei, et al. (2024) Timeseriesbench: an industrial-grade benchmark for time series anomaly detection models. arXiv preprint arXiv:2402.10802. Cited by: SS1.
* [5]Z. Liu, W. Pei, D. Lan, and Q. Ma (2024) Diffusion language-shapelets for semi-supervised time-series classification. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38, pp. 14079-14087. Cited by: SS1.
* [6]H. Li, S. Yu, and J. Principe (2023) Causal recurrent variational autoencoder for medical time series generation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, pp. 8562-8570. Cited by: SS1.
* [7]M. Lu and X. Xu (2024) Trnn: an efficient time-series recurrent neural network for stock price prediction. Information Sciences657, pp. 119951. Cited by: SS1.
* [8]J. Franceschi, A. Dieuleveut, and M. Jaggi (2019) Unsupervised scalable representation learning for multivariate time series. Advances in Neural Information Processing Systems32. Cited by: SS1.
* [9]Y. He and J. Zhao (2019) Temporal convolutional networks for anomaly detection in time series. J. Phys. Conf. Ser. Cited by: SS1.
* [10]S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar (2021) Pyraformer: low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, Cited by: SS1.
* [11]Y. Liu, H. Wu, J. Wang, and M. Long (2022) Non-stationary transformers: exploring the stationarity in time series forecasting. In Advances in Neural Information Processing Systems, Cited by: SS1.
* [12]H. Wu, J. Xu, J. Wang, and M. Long (2021) Autoformer: decomposition transformers with auto-correlation for long-term series forecasting. In Advances in Neural Information Processing Systems, pp. 101-112. Cited by: SS1.
* [13]H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long (2023) Timesnet: temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* [14]A. Zeng, M. Chen, L. Zhang, and Q. Xu (2023) Are transformers effective for time series forecasting?. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37, pp. 11121-11128. Cited by: SS1.
* [15]T. Zhang, Y. Zhang, W. Cao, J. Bian, X. Yi, S. Zheng, and J. Li (2022) Less is more: fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186. Cited by: SS1.

* [16] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _35th AAAI Conference on Artificial Intelligence_, pages 11106-11115, 2021.
* [17] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _Proc. 39th International Conference on Machine Learning_, 2022.
* [18] Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, and Weiyao Lin. Basisformer: Attention-based time series forecasting with learnable and interpretable basis. _Advances in Neural Information Processing Systems_, 36, 2024.
* [19] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _The Eleventh International Conference on Learning Representations_, 2022.
* [20] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. _Advances in Neural Information Processing Systems_, 36, 2024.
* [21] Zhijian Xu, Ailing Zeng, and Qiang Xu. Fits: Modeling time series with \(10k\) parameters. In _The Twelfth International Conference on Learning Representations_, 2023.
* [22] Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong Jiang, and Shu-Tao Xia. Periodicity decoupling framework for long-term series forecasting. In _The Twelfth International Conference on Learning Representations_, 2024.
* [23] Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor W Tsang. Disentangling structured components: Towards adaptive, interpretable and scalable time series forecasting. _IEEE Transactions on Knowledge and Data Engineering_, 2024.
* [24] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2024.
* [25] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [26] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2023.
* [27] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, and Xiaoli Li. Tslanet: Rethinking transformers for time series representation learning. _arXiv preprint arXiv:2404.08472_, 2024.
* [28] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _International Conference on Learning Representations_, 2020.
* [29] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. _arXiv preprint arXiv:2202.01381_, 2022.
* [30] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 6989-6997, 2023.
* [31] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In _International Conference on Learning Representations_, 2019.

* Xu et al. [2021] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. In _International Conference on Learning Representations_, 2021.
* Dempster et al. [2020] Angus Dempster, Francois Petitjean, and Geoffrey I Webb. ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. _Data Mining and Knowledge Discovery_, 34(5):1454-1495, 2020.
* Lai et al. [2018] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 95-104, 2018.
* Wu et al. [2022] Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation flows. In _International Conference on Machine Learning_, pages 24226-24242. PMLR, 2022.
* [36] Wetterstation. Weather. [https://www.bgc-jena.mpg.de/wetter/](https://www.bgc-jena.mpg.de/wetter/).
* [37] PeMS. Traffic. [http://pems.dot.ca.gov/](http://pems.dot.ca.gov/).
* [38] UCI. Electricity. [https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014).
* Makridakis et al. [2018] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: Results, findings, conclusion and way forward. _International Journal of Forecasting_, 34(4):802-808, 2018.
* Challu et al. [2022] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. _arXiv preprint arXiv:2201.12886_, 2022.
* Zhang et al. [2022] T. Zhang, Yizhuo Zhang, Wei Cao, J. Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. _arXiv preprint arXiv:2207.01186_, 2022.
* Bagnall et al. [2018] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. _arXiv preprint arXiv:1811.00075_, 2018.
* Su et al. [2019] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2828-2837, 2019.
* Hundman et al. [2018] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 387-395, 2018.
* Mathur and Tippenhauer [2016] Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In _2016 international workshop on cyber-physical systems for smart water networks (CySWater)_, pages 31-36. IEEE, 2016.
* Abdulaal et al. [2021] Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous multivariate time series anomaly detection and localization. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pages 2485-2494, 2021.
* Li et al. [2019] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In _NeurIPS_, 2019.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.

## Appendix A Method Details

Here, we provide further elaboration on the details of Peri-midFormer. Its full flowchart is depicted in Figure 8, depicts two strategies for input. The strategy indicated by the red line is for classification task, where de-normalization and time series decomposition are not used. This is because, in classification task, there is no need to reconstruct the original data; therefore, the trend part does not need to be extracted and added back. Additionally, it is important to note that the trend part is a significant discriminative feature for classification data, so it cannot be separated from the original data before feature extraction. In the strategy indicated by the blue line, employed for reconstruction tasks, Peri-midFormer needs to focus on the periodicity in the time series. Therefore, we utilize de-normalization and time series decomposition to eliminate other influencing factors. To achieve this, we first refer to [11] to address instability factors. We normalize the input \(\mathbf{X}=[x_{1},x_{2},...,x_{L}]\in\mathbb{R}^{L\times C}\) to obtain \(\mathbf{X}_{norm}=[x^{\prime}_{1},x^{\prime}_{2},...,x^{\prime}_{L}]\in\mathbb{ R}^{L\times C}\):

\[\mu_{\mathbf{x}}=\frac{1}{L}\sum_{i=1}^{L}x_{i},\ \sigma_{\mathbf{x}}^{2}= \frac{1}{L}\sum_{i=1}^{L}(x_{i}-\mu_{\mathbf{x}})^{2},\ x^{\prime}_{i}=\frac{ 1}{\sigma_{\mathbf{x}}}\odot(x_{i}-\mu_{\mathbf{x}}), \tag{9}\]

where \(\mu_{\mathbf{x}},\sigma_{\mathbf{x}}\in\mathbb{R}^{C\times 1}\) are the mean and variance, respectively, \(\frac{1}{\sigma_{\mathbf{x}}}\) means the element-wise division and \(\odot\) is the element-wise product. Normalization reduces the disparity in distribution among individual input time series, thereby stabilizing the model input distribution.

Then, to remove the trend part from the time series and only preserve the seasonal part for Peri-midFormer, we refer to [12] for time series decomposition, as shown in the following equation:

\[\mathbf{X}_{t} =AvgPool(Padding(\mathbf{X}_{norm})), \tag{10}\] \[\mathbf{X}_{s} =\mathbf{X}_{norm}-\mathbf{X}_{t},\]

where \(\mathbf{X}_{s},\mathbf{X}_{t}\in\mathbb{R}^{L\times C}\) denote the seasonal and the trend part respectively. We adopt the \(AvgPool(\cdot)\) for moving average with the padding operation to keep the series length unchanged.

The seasonal part \(\mathbf{X}_{s}\) obtained after decomposition can be directly input into Peri-midFormer. After the output from Peri-midFormer, we add the trend part back, then de-normalize it to obtain the final output \(\mathbf{\hat{Y}}=[\hat{y_{1}},\hat{y_{2}},...,\hat{y_{T}}]\):

\[\mathbf{Y}_{s}=\mathcal{H}(\mathbf{X}_{s}),\mathbf{Y}=\mathbf{Y}_{s}+Projection( \mathbf{X}_{t}),\hat{y}_{i}=\sigma_{\mathbf{x}}\odot y_{i}+\mu_{\mathbf{x}}, \tag{11}\]

where \(\mathcal{H}\) represents the Peri-midFormer model, \(\mathbf{Y}_{s}\) represents the output of Peri-midFormer, \(Projection(\cdot)\) represents mapping the trend part to the target output length, and \(\hat{y}_{i}=\sigma_{\mathbf{x}}\odot y_{i}+\mu_{\mathbf{x}}\) denotes de-normalization.

## Appendix B Visualization

To provide a clearer demonstration of Peri-midFormer's representational capabilities, Figure 9, 10 and 11 visualize some of the results for imputation, long-term forecasting and short-term forecasting, respectively. It illustrates that Peri-midFormer outperforms other methods in capturing periodic variations in time series.

Figure 8: Full flowchart.

Figure 9: Visualization of imputation.

Figure 10: Visualization of long-term forecasting.

Figure 11: Visualization of short-term forecasting.

Dataset Details

A detailed description of the dataset is given in Table 6.

## Appendix D Experimental Details

All the deep learning networks are implemented in PyTorch and trained on NVIDIA 4090 24GB GPU. We repeated each experiment three times to eliminate randomness. The detailed experiment configuration is shown in Table 7.

### Metrics

We utilize various metrics to evaluate different tasks. For long-term forecasting and imputations, we employ the mean square error (MSE) and mean absolute error (MAE). In anomaly detection, we utilize the F1-score, which combines precision and recall. For short-term forecasting, we utilize the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), with OWA being a unique metric used in the M4 competition.

\begin{table}
\begin{tabular}{c|l|c|c|c|c} \hline Tasks & Dataset & Dim & Series Length & Dataset Size & Information (Frequency) \\ \hline \multirow{6}{*}{Forecasting (Long-term)} & ETTm1, ETTm2 & 7 & \{96, 192, 336, 720\} & \{34465, 11521, 11521\} & Electricity (15 mins) \\ \cline{2-5}  & ETTh1, ETTh2 & 7 & \{96, 192, 336, 720\} & \{8545, 2881, 2881\} & Electricity (15 mins) \\ \cline{2-5}  & Electricity & 321 & \{96, 192, 336, 720\} & \{18317, 2633, 5261\} & Electricity (Hourly) \\ \cline{2-5}  & Traffic & 862 & \{96, 192, 336, 720\} & \{12185, 1757, 3509\} & Transportation (Hourly) \\ \cline{2-5}  & Weather & 21 & \{96, 192, 336, 720\} & \{36792, 5271, 10540\} & Weather (10 mins) \\ \cline{2-5}  & Exchange & 8 & \{96, 192, 336, 720\} & \{5120, 665, 1422\} & Exchange rate (Daily) \\ \hline \multirow{6}{*}{Forecasting (short-term)} & M4-Yearly & 1 & 6 & \((23000,0,23000)\) & Demographic \\ \cline{2-5}  & M4-Quarterly & 1 & 8 & \((24000,0,24000)\) & Finance \\ \cline{2-5}  & M4-Monthly & 1 & 18 & \((48000,0,48000)\) & Industry \\ \cline{2-5}  & M4-Weakly & 1 & 13 & \((359,0,359)\) & Macro \\ \cline{2-5}  & M4-Daily & 1 & 14 & \((4227,0,4227)\) & Micro \\ \cline{2-5}  & M4-Hourly & 1 & 48 & \((414,0,414)\) & Other \\ \hline \multirow{6}{*}{Imputation} & ETTm1, ETTm2 & 7 & 96 & \{34465, 11521, 11521\} & Electricity (15 mins) \\ \cline{2-5}  & ETTh1, ETTh2 & 7 & 96 & \{8545, 2881, 2881\} & Electricity (15 mins) \\ \cline{2-5}  & Electricity & 321 & 96 & \{18317, 2633, 5261\} & Electricity (15 mins) \\ \cline{2-5}  & Weather & 21 & 96 & \{36792, 5271, 10540\} & Weather (10 mins) \\ \hline \multirow{6}{*}{Classification (UEA)} & EthanolConcentration & 3 & 1751 & \{261, 0, 263\} & Alcohol Industry \\ \cline{2-5}  & FaceDetection & 144 & 62 & \{5890, 0, 3524\} & Face (250Hz) \\ \cline{2-5}  & Handwriting & 3 & 152 & \{150, 0, 850\} & Handwriting \\ \cline{2-5}  & Heartbeat & 61 & 405 & \{204, 0, 205\} & Heart Beat \\ \cline{2-5}  & JapaneseVowels & 12 & 29 & \{270, 0, 370\} & Voice \\ \cline{2-5}  & PEMS-SF & 963 & 144 & \{267, 0, 173\} & Transportation (Daily) \\ \cline{2-5}  & SelfRegulationSCP1 & 6 & 896 & \{268, 0, 293\} & Health (256Hz) \\ \cline{2-5}  & SelfRegulationSCP2 & 7 & 1152 & \{200, 0, 180\} & Health (256Hz) \\ \cline{2-5}  & SpokenArabicDigits & 13 & 93 & \{6599, 0, 2199\} & Voice (11025Hz) \\ \cline{2-5}  & UWaveGestur.Library & 3 & 315 & \{120, 0, 320\} & Gesture \\ \hline \multirow{6}{*}{Anomaly} & SMD & 38 & 100 & \{56724, 141681, 708420\} & Server Machine \\ \cline{2-5}  & MSL & 55 & 100 & \{44653, 11664, 73729\} & Spaeccraft \\ \cline{1-1} \cline{2-5}  & SMAP & 25 & 100 & \{108146, 27037, 427617\} & Spaeccraft \\ \cline{1-1} \cline{2-5}  & SWaT & 51 & 100 & \{39600, 9900, 449919\} & Infrastructure \\ \cline{1-1} \cline{2-5}  & PSM & 25 & 100 & \{105984, 26497, 87841\} & Server Machine \\ \hline \end{tabular}

* Since our datasets setup is the same as Timesnet [13], an excerpt from it describes the datasets.

\end{table}
Table 6: Dataset descriptions. The dataset size is organized in (Train, Validation, Test).

These metrics are computed as follows:

\[\text{SMAPE} =\frac{200}{T}\sum_{i=1}^{T}\frac{|\mathbf{X}_{i}-\mathbf{\hat{Y}}_{i }|}{|\mathbf{X}_{i}|+|\mathbf{\hat{Y}}_{i}|}, \tag{12}\] \[\text{MAPE} =\frac{100}{T}\sum_{i=1}^{T}\frac{|\mathbf{X}_{i}-\mathbf{\hat{Y} }_{i}|}{|\mathbf{X}_{i}|},\] \[\text{MASE} =\frac{1}{T}\sum_{i=1}^{T}\frac{|\mathbf{X}_{i}-\mathbf{\hat{Y}}_{ i}|}{\frac{1}{T-q}\sum_{j=q+1}^{T}|\mathbf{X}_{j}-\mathbf{X}_{j-q}|},\] \[\text{OWA} =\frac{1}{2}\left[\frac{\text{SMAPE}}{\text{SMAPE}_{\text{Naw2}}} +\frac{\text{MASE}}{\text{MASE}_{\text{Naw2}}}\right],\]

where \(q\) is the periodicity of the data. \(\mathbf{X}\), \(\mathbf{\hat{Y}}\in\mathbb{R}^{T\times C}\) are the ground truth and prediction result of the future with \(T\) time pints and \(C\) dimensions. \(\mathbf{X}_{i}\) means the \(i\)-th future time point.

## Appendix E Model Analysis

### Hyper Parameter Analysis and Model Limitations

In Equation (1), we introduced a hyperparameter \(k\) to select the most important frequency, which also determines the number of levels in the Periodic Pyramid. We conducted sensitivity analysis on it, as shown in Figure 12. It's evident that our proposed Peri-midFormer exhibits relatively stable performance across different choices of \(k\) for all four tasks. However, there are still some fluctuation in results among different \(k\) values depending on the task and dataset, which are determined by the periodic characteristics in the dataset. To illustrate this, we visualize individual data for long-term forecasting and classification tasks, as shown in Figure 13. It can be observed that in the long-term forecasting task, the Etth1 dataset exhibits clear periodicity. Therefore, with larger \(k\), Peri-midFormer can capture more periodic information, resulting in better performance. In contrast, the Etth2 dataset has less obvious periodicity, so it achieves better results with smaller \(k\), as larger \(k\) introduce unnecessary noise, affecting model performance. In the classification task, the EthanolConcentration dataset is difficult to classify, whereas a larger \(k\) allows for the construction of Periodic Pyramid with more levels, thus extracting more representative features and achieving higher accuracy. In addition, the SelfRegulationSCP1 dataset contains a lot of high-frequency noise, so larger \(k\) would focus on irrelevant information, leading to decreased accuracy. Therefore, we adjusted \(k\) values differently for different tasks and datasets, as shown in the range in Table 7.

The above analysis reveals that the limitation of Peri-midFormer lies in its inability to fully leverage its advantages on datasets with poor periodicity characteristics. We plan to address this issue in our future work.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{Tasks / Configurations} & \multicolumn{3}{c|}{Model Hyper-parameter} & \multicolumn{4}{c}{Training Process} \\ \cline{2-7}  & \(k\) & Layers & \(d_{\text{model}}\) & LR\({}^{*}\) & Loss & Batch Size & Epochs \\ \hline Long-term Forecasting & 2-5 & 1-3 & 128-768 & \(10^{-4}\) - \(5\times 10^{-4}\) & MSE & 2-32 & 15 \\ \hline Short-term Forecasting & 2-3 & 1-2 & 256 & \(10^{-4}\) & SMAPE & 2 & 15 \\ \hline Imputation & 2-5 & 1-2 & 64-768 & \(10^{-4}\) - \(5\times 10^{-4}\) & MSE & 2-4 & 15 \\ \hline Classification & 2-8 & 1-4 & 16-128 & \(10^{-4}\) - \(5\times 10^{-3}\) & Cross Entropy & 2-64 & 20 \\ \hline Anomaly Detection & 2-5 & 1-4 & 8-32 & \(10^{-4}\) - \(5\times 10^{-3}\) & MSE & 2-32 & 3 \\ \hline \end{tabular}

* LR means the initial learning rate.

\end{table}
Table 7: Experiment configuration of Peri-midFormer. All the experiments use the ADAM [49] optimizer with the default hyperparameter configuration for \((\beta_{1},\beta_{2})\) as (0.9, 0.999).

### Periodic Pyramid Attention Mechanism Analysis

To illustrate the PPAM more clearly, we visualize the original time series and attention scores within the periodic pyramid for the SelfRegulationSCP2 dataset in the classification task, as shown in Figure 14. It is evident that the attention scores among components are distributed based on inclusion and adjacency relationships, meaning that components in different levels with inclusion relationships or those in the same level have higher attention scores. This demonstrates the rationality of the Periodic Pyramid structure. Additionally, when the hyperparameter \(k\) in Equation (1) is set to 2, the PPAM degenerates into periodic full attention, wherein attention is computed among all periodic

Figure 12: Sensitivity analysis of hyper-parameters \(k\) in each task.

Figure 13: Visualization of the Eth1 and Eth2 datasets for the Long-term forecasting task (1), and the EthanolConcentration and SelfRegulationSCP1 datasets for the classification task (2).

components. To illustrate this, we visualize the Electricity dataset in the long-term forecasting task and its corresponding attention distribution, as shown in Figure 15. The figure shows that the Periodic Attention Mechanism can capture the dependencies among the periodic components and identify which components belong to a longer component (note that \(k=2\) corresponds to 21 periodic components with larger amplitudes). This is attributed to the separation of the periodic components, which explicitly expresses the hidden periodic inclusion relationships in the time series. The above analysis demonstrates the effectiveness of the PPAM.

### Periodic Feature Flows Analysis

To intuitively understand the Periodic Feature Flows, we visualize it as shown in Figure 16. On the left side is the pyramid representation of the Periodic Feature Flows, with the horizontal axis representing the number of feature flows and the vertical axis representing the length of each feature flow. The feature flows are divided into multiple levels, each containing multiple periodic components. The red box encloses one feature flow, with its position from top to bottom corresponding to the pyramid from top to bottom. It can be observed that some adjacent feature flows are the same at the corresponding positions, this is because they pass through the same periodic component. On the right side, the waveform of each feature flow is displayed in different colors, with the position from left to right corresponding to the top to the bottom of the pyramid. It can be observed that each feature flow differs, which is why it is necessary to aggregate different feature flows. The aim is to fully utilize the information from each periodic component to better reconstruct the target sample.

### Training/Inferencing Cost

To further validate the computational complexity and scalability of the proposed method, we conducted detailed experiments on Electricity and ETTh1 datasets for the long-term forecasting task. These experiments focused on the complexity and actual time of training and inference, as well as memory usage, with results presented in Tables 8 and 9. It can be seen that our proposed Peri

Figure 14: Visualization of the original time series and attention scores within the periodic pyramid for the SelfRegulationSCP2 dataset in the classification task. The left side shows the original data, the middle displays the corresponding pyramid structure, and the right side depicts the attention scores within the pyramid. In this example, \(k=3\), \(f_{1}=1,f_{2}=2,f_{3}=4\).

Figure 15: Original time series of Electricity dataset (left) and Periodic Pyramid Attention score (right).

midFormer demonstrates a significant advantage in computational complexity on the Electricity dataset and achieves the lowest MSE. Similarly, on the ETTh1 dataset, Peri-midFormer's computational cost and inference time are not disadvantages. Instead, it achieves a lower MSE, second only to Time-LLM, while having much lower computational cost and inference time compared to it. These results highlight the advantages of our method in terms of computational complexity and scalability.

### Pre-interpolation

Since Peri-midFormer is designed to focus on the periodic components of time series, directly handling data with missing values in imputation tasks may prevent it from correctly capturing the periodic characteristics of the original data without missing values, thus affecting its imputation effectiveness. Therefore, to adapt Peri-midFormer for imputation tasks, we first apply a linear interpolation strategy (Equation (13)) to the data with missing values to partially restore the periodic characteristics of the original data before inputting it into Peri-midFormer for further imputation. The visualization of original data, data with 50% missing values, and pre-interpolated data of Electricity dataset are illustrated in Figure 17. It is evident that missing values significantly disrupt the periodic

Figure 16: Visualization of the pyramid form of Periodic Feature Flows (left) and the waveform of Periodic Feature Flows (right).

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{\begin{tabular}{c} Train \\ FLOPs \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Test \\ FLOPs \\ \end{tabular} } & GPU & \multirow{2}{*}{\begin{tabular}{c} Train CPU \\ memory \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Test CPU \\ memory \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} Train time \\ memory \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} Test time \\ (s) \\ \end{tabular} } & \multirow{2}{*}{MSE} \\ \cline{1-1} \cline{5-10} \cline{7-1

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

## Appendix F Proof

To demonstrate the essence of attention computation among multi-level periodic components, we need to analyze how the interactions between periodic components at different levels affect the final feature extraction. In time series analysis, different periodic components correspond to different time scales. This means that through decomposition, we can capture components of various frequencies within the time series. The essence of the periodic pyramid is to capture these different frequency components through its hierarchical structure.

Using single-channel data as an example, and given that we adopt an independent channel strategy, this can be easily extended to all channels. Assume the time series \(x(t)\) can be decomposed into multiple periodic components \(x_{n}(t)\) :

\[x(t)=\sum_{n=1}^{N}x_{n}(t). \tag{14}\]

Taking two different periodic components as examples:

\[x_{i}(t)=A_{i}\sin\left(\frac{2\pi t}{T_{i}}+\phi_{i}\right),x_{j}(t)=A_{j} \cos\left(\frac{2\pi t}{T_{j}}+\phi_{j}\right), \tag{15}\]

where \(A\) is amplitude, \(T\) is period, and \(\phi\) is phase. Due to the overlap and inclusion relationships between different periodic components, we employ an attention mechanism in the periodic pyramid to capture the similarities between different periodic components, focusing on important periodic features. When applying the attention mechanism, we have:

\[Q_{i}=W_{Q}x_{i}(t),\quad K_{j}=W_{K}x_{j}(t),\quad V_{j}=W_{V}x_{j}(t), \tag{16}\]

where \(W_{Q}\), \(W_{K}\) and \(W_{V}\) are learnable weight matrices. From Equations (15) and (16):

\[Q_{i}=W_{Q}A_{i}\sin\left(\frac{2\pi t}{T_{i}}+\phi_{i}\right),\quad K_{j}=W_{ K}A_{j}\cos\left(\frac{2\pi t}{T_{j}}+\phi_{j}\right). \tag{17}\]

Further, the dot-product attention can be expressed as:

\[Q_{i}K_{j}^{T}=A_{i}A_{j}\left(W_{Q}\sin\left(\frac{2\pi t}{T_{i}}+\phi_{i} \right)\right)\left(W_{K}\cos\left(\frac{2\pi t}{T_{j}}+\phi_{j}\right)\right) ^{T}. \tag{18}\]

Using the trigonometric identity \(\sin(a)\cos(b)=\frac{1}{2}[\sin(a+b)+\sin(a-b)]\), the dot-product \(Q_{i}K_{j}^{T}\) can be further expressed as:

\[Q_{i}K_{j}^{T}=\frac{1}{2}A_{i}A_{j}\left\{W_{Q}\left[\sin\left(\frac{2\pi t}{ T_{i}}+\phi_{i}+\frac{2\pi t}{T_{j}}+\phi_{j}\right)+\sin\left(\frac{2\pi t}{T_{i}} +\phi_{i}-\frac{2\pi t}{T_{j}}-\phi_{j}\right)\right]\right\}(W_{K})^{T}. \tag{19}\]

Figure 19: Visualization of Exchange and Yearly dataset.

Based on this, considering the periodicity and symmetry of \(\sin(a+b)\) and \(\sin(a-b)\), when the periods of two time series components are close / same (intra-layer attention in the pyramid, see the right side of Figure 3 in the original paper) or have overlapping / inclusive parts (inter-layer attention in the pyramid, see the right side of Figure 3 in the original paper), the values of these two sine functions will be highly correlated, resulting in a large \(Q_{i}K_{j}^{T}\) value. This indicates that the periodic pyramid model can effectively capture similar periodic patterns across different time scales.

Next, incorporating this into the calculation of the attention score:

\[s_{ij}=\frac{\exp\left(\frac{Q_{i}K_{j}^{T}}{\sqrt{d_{k}}}\right)}{\sum\limits _{j^{\prime}}\exp\left(\frac{Q_{i}K_{j^{\prime}}^{T}}{\sqrt{d_{k}}}\right)}. \tag{20}\]

It can be seen that the attention scores between highly correlated periodic components will be higher, which we have already validated in Figures 13 and 14 of the original paper.

Further, the attention vector \(\mathbf{a}_{i}\) of \(x_{i}(t)\) can be obtained as:

\[\mathbf{a}_{i}=\sum\limits_{j}s_{ij}V_{j}\, \tag{21}\]

where \(V_{j}=W_{V}x_{j}(t)=W_{V}A_{j}\cos\left(\frac{2\pi t}{7_{j}}+\phi_{j}\right)\).

From the above derivation, it can be seen that the attention mechanism can measure the similarity between different periodic components. This similarity reflects the alignment between different periodic components in the time series, allowing the model to capture important periodic patterns. By capturing these periodic patterns, the periodic pyramid can extract key features of the time series, resulting in a comprehensive and accurate time series representation. This representation not only includes information across different time scales but also enhances the representation of important periodic patterns.

## Appendix G Broader Impacts

Our research has implications for time-series-based analyses such as weather forecasting and anomaly detection for industrial maintenance, as discussed in the Introduction 1. Our work carries no negative social impact.

## Appendix H Full Results

### Full Results of Classification (Table 11)

### Full Results of Short-term Forecasting (Table 12)

### Full Results of Long-term Forecasting (Table 13 and 14)

### Full Results of Imputation (Table 15)

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Models} & \multirow{2}{*}{\begin{tabular}{c} **Peri-mid** T-LLM \\ **Forner** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **GPT** \\ **25** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **T-Net** \\ **20** \\ \end{tabular} } & Patch & \multirow{2}{*}{\begin{tabular}{c} **HTS** \\ **13** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **BEATS** \\ **19** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **ETS** \\ **29** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Flow-DLinear** \\ **11** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Light** \\ **17** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **T-Net** \\ **29** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **GPT** \\ **29** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **ILS** \\ **14** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **HTS** \\ **15** \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} **Real** \\ **16** \\ \end{tabular} } \\ \cline{1-1} \cline{8-14}  & & & & & & & & & & & & & & & & & & & & \\ \hline \hline EthanolConcentration & 45.2 & 29.6 & 28.9 & 32.7 & 31.9 & 30.8 & 31.6 & 32.7 & 31.2 & 28.1 & 33.8 & 32.6 & 29.7 & 35.7 & 34.2 & 30.4 & **50.6** \\ FaceDetection & 64.7 & 67.8 & 52.8 & 67.3 & 68.6 & 65.7 & 68.4 & 68.0 & 66.0 & 66.3 & 67.6 & 68.0 & 67.5 & 68.6 & **69.2** & 66.7 & 68.7 \\ Handwriting & **58.8** & 23.3 & 32.0 & 27.4 & 29.4 & 36.7 & 31.6 & 28.0 & 32.5 & 33.8 & 27.0 & 26.1 & 32.1 & 32.7 & 57.8 & 31.5 \\ Heartbeat & 75.6 & 75.7 & 75.6 & 76.1 & 77.1 & 75.6 & 74.6 & 73.7 & 73.7 & 71.2 & 77.6 & 75.1 & 75.8 & 70.7 & 77.5 & **95.1** \\ JapaneseVowels & 96.2 & 94.0 & 98.9 & 98.7 & 97.8 & 98.4 & 96.2 & **99.2** & 98.4 & 95.9 & 98.9 & 96.2 & 96.2 & 98.4 & 98.6 & 99.2 & 97.3 \\ PEMS-SF & 75.1 & 80.9 & 68.8 & 82.1 & 82.7 & 83.2 & 82.7 & 87.3 & 80.9 & 86.0 & 83.8 & 75.1 & 88.4 & **89.6** & 87.9 & 83.8 & 88.2 \\ SelfRegulationSCP1 & 90.8 & 82.2 & 84.6 & 92.2 & 90.4 & 88.1 & 84.0 & 89.4 & 88.7 & 89.6 & 92.5 & 87.3 & 89.8 & 91.8 & **93.2** & 91.8 & 92.1 \\ SelfRegulationSCP2 & 53.3 & 53.6 & 55.6 & 53.9 & 56.7 & 53.3 & 50.6 & 57.2 & 54.4 & 55.0 & 56.1 & 5

[MISSING_PAGE_EMPTY:28]

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \multicolumn{12}{l}{Look-back} & \multicolumn{12}{l}{**96**} & \multicolumn{12}{l}{**96**} & \multicolumn{12}{l}{**96**} & \multicolumn{12}{l}{**96**} & \multicolumn{12}{l}{**96**} & \multicolumn{12}{l}{**96**} & \multicolumn{12}{l}{**Methods**} & \multicolumn{12}{l}{**Peri-mid**} & \multicolumn{1}{l}{iTrans-*} & \multicolumn{1}{l}{**FITS**} & \multicolumn{1}{l}{**Dlinear**} & \multicolumn{1}{l}{**PatchTST TimesNet**} & \multicolumn{1}{l}{**Pyra-*} & \multicolumn{1}{l}{**FED-*} & \multicolumn{1}{l}{**Auto-*} & \multicolumn{1}{l}{**Station**} & \multicolumn{1}{l}{**ETS+**} & \multicolumn{1}{l}{**LightTS**} & \multicolumn{1}{l}{**In-*} & \multicolumn{1}{l}{**Re-***} \\  & **Former** & **[**26**]** & **[**21**]** & **[**14**]** & **[**19**]** & **[**13**]** & **[**10**]** & **[**17**]** & **[**12**]** & **[**17**]** & **[**12**]** & **[**11**]** & **[**11**]** & **[**29**]** & **[**15**]** & **[**16**]** & **[**28**]** \\ \hline \multicolumn{12}{l}{Metrics} & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE \\ \hline \multicolumn{12}{l}{**96**} & 96.15 & 0.200.104 & 0.714.219 & 0.2370 & 0.176 & 0.2370 & 0.149 & 0.198 & 0.172 & 0.220 & 0.622 & 0.560 & 0.217 & 0.296 & 0.266 & 0.336 & 0.173 & 0.223 & 0.197 & 0.281 & 0.182 & 0.242 & 0.300 & 0.384 & 0.68

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_FAIL:31]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist",**
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We made our main claims in the abstract and introduction. Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We illustrate the limitations of the model in conjunction with the hyperparametric analysis in the Appendix E.1. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide a full theoretical description of the proposed methodology in the main paper and appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ** Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the proposed methodology in detail in Section 3 and provide further explanation in Appendix A. Experimental details are outlined in Appendix D. Additionally, all the code related to the proposed method is included in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is available at [https://github.com/WuQiangXDU/Peri-midFormer](https://github.com/WuQiangXDU/Peri-midFormer).

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We offer a detailed description of the dataset in Appendix C and outline the experimental setup in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In Appendix D, we demonstrate that the experiments were repeated multiple times to eliminate randomness. However, due to the large number of experiments, detailed standard deviations are not shown. Instead, we uniformly present them in the headings of each table in Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We describe the compute workers required for the experiments in Appendix D, and the amount of computation in Section 6 and Appendix E.4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and conducted it in the paper conform in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We illustrate broader impacts of our research in Appendix G. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: We cite all the datasets and models involved in the paper. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is available at [https://github.com/WuQiangXDU/Peri-midFormer](https://github.com/WuQiangXDU/Peri-midFormer). Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.