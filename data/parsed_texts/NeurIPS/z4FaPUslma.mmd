# Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame

 Evan Markou

Australian National University

evan.markou@anu.edu.au &Thalaiyasingam Ajanthan

Australian National University & Amazon

thalaiyasingam.ajanthan@anu.edu.au &Stephen Gould

Australian National University

stephen.gould@anu.edu.au

###### Abstract

Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer when trained until zero training loss. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class. By duality, the penultimate layer feature means also converge to the same simplex ETF. Since this simple symmetric structure is optimal, our idea is to utilise this property to improve convergence speed. Specifically, we introduce the notion of _nearest simplex ETF geometry_ for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation. Then, at each iteration, the classifier weights are implicitly set to the nearest simplex ETF by solving this inner-optimisation, which is encapsulated within a declarative node to allow backpropagation. Our experiments on synthetic and real-world architectures for classification tasks demonstrate that our approach accelerates convergence and enhances training stability1.

Footnote 1: Code available at https://github.com/evanmarkou/Guiding-Neural-Collapse.git.

## 1 Introduction

While modern deep neural networks (DNNs) have demonstrated remarkable success in solving diverse machine learning problems [22, 34, 38], the fundamental mechanisms underlying their training process remain elusive. In recent years, considerable research efforts have focused on delineating the optimisation trajectory and characterising the solution space resulting from the optimisation process in training neural networks [72, 17, 49, 41]. One such finding is that gradient descent algorithms, when combined with certain loss functions, introduce an implicit bias that often favours max-margin solutions, influencing the learned representations and decision boundaries. [44, 57, 33, 27, 21, 54, 70, 31, 49].

In this vein, Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer in both balanced [50, 76, 74, 28, 48, 63, 43] and imbalanced dataset settings [19, 59, 5]. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class, and by duality, the penultimate layer feature means converge to the classifier weights, _i.e_., to the simplex ETF (formal definitions are provided in Appendix A). This simple, symmetric structure is shown to be the only set of optimal solutions for a variety of loss functions when the features are also assumed to be free parameters, _i.e_., Unconstrained FeatureModels (UFMs) [32; 19; 74; 76; 28; 75]. Nevertheless, even in realistic large-scale deep networks, this phenomenon is observed when trained to convergence, even after attaining zero training error.

Since we can characterise the optimal solution space for the classifier layer, a natural extension is to _leverage the simplex ETF structure of the classifier weights to improve training_. To this end, researchers have tried fixing the classifier weights to a canonical simplex ETF, effectively reducing the number of trainable parameters [76]. However, in practice, this approach does not improve the convergence speed as the backbone network still needs to do the heavy lifting of matching feature means to the chosen fixed simplex ETF.

In this work, we introduce a mechanism for finding the nearest simplex ETF to the features at any given training iteration. Specifically, the nearest simplex ETF is determined by solving a Riemannian optimisation problem. Therefore, our classifier weights are dynamically updated based on the penultimate layer feature means at each iteration, _i.e._, implicitly defined rather than trained using gradient descent. Additionally, by constructing this inner-optimisation problem as a deep declarative node [23], we allow gradients to propagate through the Riemannian optimisation facilitating end-to-end learning. Our whole framework significantly speeds up convergence to a NC solution compared to the fixed simplex ETF and conventional learnable classifier approaches. We demonstrate the effectiveness of our approach on synthetic UFMs and standard image classification experiments.

Our main contributions are as follows:

1. We introduce the notion of the nearest simplex ETF geometry given the penultimate layer features. Instead of selecting a predetermined simplex ETF (canonical or random), we implicitly fix the classifier as the solution to a Riemannian optimisation problem.
2. To establish end-to-end learning, we encapsulate the Riemannian optimisation problem of determining the nearest simplex ETF geometry within a declarative node. This allows for efficient backpropagation throughout the network.
3. We demonstrate that our method achieves an optimal neural collapse solution more rapidly compared to fixed simplex ETF methods or conventional training approaches, where a learned linear classifier is employed. Additionally, our method ensures training stability by markedly reducing variance in network performance.

## 2 Related Work

Neural Collapse and Simplex ETFs.Zhu et al. [76] proposed fixing classifier weights to a simplex ETF, reducing parameters while maintaining performance. Simplex ETFs effectively tackle imbalanced learning, as demonstrated by Yang et al. [67], where they fix the target classifier to an arbitrary simplex ETF, relying on the network's over-parameterisation to adapt. Similarly, Yang et al. [68] addressed class incremental learning by fixing the target classifier to a simplex ETF. They advocate adjusting prototype means towards the simplex ETF using a convex combination, smoothly guiding backbone features into the targeted simplex ETF. However, these methods did not yield any benefits regarding convergence speed. The work most relevant to ours is that of Peifeng et al. [51], who argued about the significance of feature directions, particularly in long-tailed learning scenarios. They compared their method against a fixed simplex ETF target, formulating their problem to enable the network to learn feature direction through a rotation matrix. Additionally, they efficiently addressed their optimisation using trivialisation techniques [39; 40]. However, they did not demonstrate any improvements in convergence speed over the fixed simplex ETF, achieving only a minimal increase in test accuracy. Fixing a classifier is not a recent concept, as it has been proposed prior to the emergence of neural collapse [52; 58; 30]. Most notably, Pernici et al. [52] demonstrated improved convergence speed by fixing the classifier to a simplex structure only on ImageNet while maintaining comparable performance on smaller-scale datasets. In contrast, our method shows superior convergence speed compared to both a fixed simplex ETF and a learned classifier across both small and large-scale datasets.

Optimisation on Smooth Manifolds.Our optimisation problem involves orthogonality constraints, characterised by the Stiefel manifold [2; 11]. Due to the nonlinearity of these constraints, efficiently solving such problems requires leveraging Riemannian geometry [18]. A multitude of works are dedicated to solving such problems by either transforming existing classical optimisation techniques into Riemannian equivalent algorithms [1; 73; 20; 64; 55] or by carefully designing penalty functionsto address equivalent unconstrained problems [66; 65]. In our approach, we opt for a retraction-based Riemannian optimisation algorithm [1] to optimally handle orthogonality constraints.

Implicit Differentiable Optimisation.In a neural network setting, end-to-end architectures are commonplace. To backpropagate solutions to optimisation problems, we rely on machinery from implicit differentiation. Pioneering works [4; 3] demonstrated efficient gradient backpropagation when dealing with solutions of convex optimisation problems. This concept was independently introduced as a generalised version by Gould et al. [23; 24] to encompass any twice-differentiable optimisation problem. A key advantage of Deep Declarative Networks (DDNs) lies in their ability to efficiently solve problems at any scale by leveraging the problem's underlying structure [25]. Our setting involves utilising an equality-constrained declarative node to efficiently backpropagate through the network.

## 3 Optimising Towards the Nearest Simplex ETF

In this section, we introduce our method to determine the nearest simplex ETF geometry and detail how we can dynamically steer the training algorithm to converge towards this particular solution.

### Problem Setup

Let us first introduce key notation that will be useful when formulating our optimisation problem.

Simplex ETF.Mathematically, a general simplex ETF is a collection of points in \(\mathbb{R}^{C}\) specified by the columns of a matrix

\[\bm{M}=\alpha\sqrt{\frac{C}{C-1}}\bm{U}\left(\bm{I}_{C}-\frac{1}{C}\bm{1}_{C} \bm{1}_{C}^{\top}\right)\.\] (1)

Here, \(\alpha\in\mathbb{R}_{+}\) denotes an arbitrary scale factor, \(\bm{1}_{C}\) is the \(C\)-dimensional vector of ones, and \(\bm{U}\in\mathbb{R}^{d\times C}\) (with \(d\geq C\)) represents a semi-orthogonal matrix (\(\bm{U}^{\top}\bm{U}=\bm{I}_{C}\)). Note that there are many simplex ETFs in \(\mathbb{R}^{C}\) as the rotation \(\bm{U}\) varies, and \(\bm{M}\) is rank-deficient. Additionally, the standard simplex ETF with unit Frobenius norm is defined as: \(\tilde{\bm{M}}=\frac{1}{\sqrt{C-1}}\left(\bm{I}_{C}-\frac{1}{C}\bm{1}_{C}\bm{ 1}_{C}^{\top}\right)\).

Mean of Features.Consider a classification dataset \(\mathcal{D}=\{(\bm{x}_{i},y_{i})\mid i=1,\dots,N\}\) where the data \(\bm{x}_{i}\in\mathcal{X}\) and labels \(y_{i}\in\mathcal{Y}=\{1,\dots,C\}\). Suppose, \(n_{c}\) is the number of samples correspond to label \(c\), then \(\sum_{c=1}^{C}n_{c}=N\). Let us consider a scenario where we have a collection of features defined as,

\[\bm{H}\triangleq[\bm{h}_{c,i}:1\leq c\leq C,\,1\leq i\leq n_{c}]\in\mathbb{R }^{d\times N}\.\] (2)

Here, each feature may originate from a nonlinear compound mapping of input data through a neural network, denoted as, \(\bm{h}_{y_{i},i}=\phi_{\bm{\theta}}(\bm{x}_{i})\) for the data sample \((\bm{x}_{i},y_{i})\). Now, for the final layer, our decision variables (weights and biases) are represented as \(\bm{W}\triangleq[\bm{w}_{1},\dots,\bm{w}_{C}]^{\top}\in\mathbb{R}^{C\times d}\), and \(\bm{b}\in\mathbb{R}^{C}\), and the logits for the \(i\)-th sample is computed as,

\[\psi_{\bm{\Theta}}(\bm{x}_{i})=\bm{W}\bm{h}_{y_{i},i}+\bm{b}\,\qquad\text{ where}\quad\bm{h}_{y_{i},i}=\phi_{\bm{\theta}}(\bm{x}_{i})\.\] (3)

In UFMs, the features are assumed to be free variables, which serves as a rough approximation for neural networks and helps derive theoretical guarantees. Additionally, we define the global mean and per-class mean of the features \(\{\bm{h}_{c,i}\}\) as:

\[\bm{h}_{G}\triangleq\frac{1}{N}\sum_{c=1}^{C}\sum_{i=1}^{n_{c}}\bm{h}_{c,i}\,\quad \bar{\bm{h}}_{c}\triangleq\frac{1}{n_{c}}\sum_{i=1}^{n_{c}}\bm{h}_{c,i}\,\quad(1\leq c\leq C)\,\] (4)

and the globally centred feature mean matrix as,

\[\bar{\bm{H}}\triangleq[\bar{\bm{h}}_{1}-\bm{h}_{G},\dots,\bar{\bm{h}}_{C}- \bm{h}_{G}]\in\mathbb{R}^{d\times C}\.\] (5)

Finally, we scale the feature mean matrix to have unit Frobenius norm, _i.e._, \(\bar{\bm{H}}=\bar{\bm{H}}/\|\bar{\bm{H}}\|_{F}\) which will be used in formulation below.

### Nearest Simplex ETF through Riemannian Optimisation

Once we obtain the feature means, our objective is to calculate the nearest simplex ETF based on these means and subsequently adjust the classifier weights \(\bm{W}\) to align with this particular simplex ETF. The rationale is to identify and establish a simplex ETF that closely corresponds to the feature means at any given iteration. This approach aims to expedite convergence during the training process by providing the algorithm with a starting point that is closer to an optimal solution rather than requiring it to learn a simplex ETF direction or converge towards an arbitrary one.

To find the nearest simplex ETF geometry, we solve the following Riemannian optimisation problem

\[\underset{\bm{U}\in St_{C}^{d}}{\text{minimize}}\ \Big{\|}\tilde{\bm{H}}-\bm{U} \tilde{\bm{M}}\Big{\|}_{F}^{2}\] (6)

where \(St_{C}^{d}=\{\bm{X}\in\mathbb{R}^{d\times C}:\bm{X}^{\top}\bm{X}=\bm{I}_{C}\}\). Here, \(\tilde{\bm{M}}\) is the standard simplex ETF with unit Frobenius norm, and the set of the orthogonality constraints \(St_{C}^{d}\) forms a compact Stiefel manifold [2; 11] embedded in a Euclidean space.

### Proximal Problem

The solution to the Riemannian optimisation problem, denoted as \(\bm{U}^{\star}\), is not unique since a component of \(\bm{U}^{\star}\) lies in the null space of \(\tilde{\bm{M}}\). As simplex ETFs reside in \((C-1)\)-dimensional space, the matrix \(\tilde{\bm{M}}\) is rank-one deficient. Consequently, we are faced with a family of solutions, leading to challenges in training stability, as we may oscillate between multiple simplex ETF directions. We address this issue by introducing a proximal term to the problem's objective function. This guarantees the uniqueness of the solution and stabilises the training process, ensuring that our problem converges to a solution closer to the previous one.

So, the original problem in Equation 6 is transformed into:

\[\underset{\bm{U}\in St_{C}^{d}}{\text{minimize}}\ \Big{\|}\tilde{\bm{H}}-\bm{U} \tilde{\bm{M}}\Big{\|}_{F}^{2}+\frac{\delta}{2}\Big{\|}\bm{U}-\bm{U}_{\text{ prox}}\Big{\|}_{F}^{2}\.\] (7)

Here, \(\bm{U}_{\text{ prox}}\) represents the proximal target simplex ETF direction, and \(\delta>0\) serves as the proximal coefficient, handling the trade-off between achieving the optimal solution's proximity to the feature means and its proximity to a given simplex ETF direction. In fact, one can perceive our problem formulation in Equation 7 as a generalisation to a predetermined fixed simplex ETF solution. This is evident when considering that if we significantly increase \(\delta\), the optimal direction \(\bm{U}^{\star}\) would converge towards the fixed proximal direction \(\bm{U}_{\text{ prox}}\).

### General Learning Setting

Our problem formulation, following the general deep neural network architecture in Equation 3, can be seen as a bilevel optimisation problem as follows:

\[\underset{\bm{\Theta}}{\text{minimize}}\ \mathcal{L}(\mathcal{D};\bm{ \Theta},\bm{U}^{\star})\triangleq-\frac{1}{N}\sum_{i=1}^{N}\log\left(\frac{ \exp\left(\psi_{\bm{\Theta}}(\bm{x}_{i},\bm{U}^{\star})_{y_{i}}\right)}{\sum_{ j=1}^{C}\exp\left(\psi_{\bm{\Theta}}(\bm{x}_{i},\bm{U}^{\star})_{j}\right)} \right)\,\] (8) \[\text{subject to}\ \bm{U}^{\star}\in\underset{\bm{U}\in St_{C}^{d}}{ \text{arg}\min}\ \Big{\|}\tilde{\bm{H}}-\bm{U}\tilde{\bm{M}}\Big{\|}_{F}^{2}+\frac{\delta}{2} \Big{\|}\bm{U}-\bm{U}_{\text{ prox}}\Big{\|}_{F}^{2}\,\]

where \(\psi_{\bm{\Theta}}(\bm{x}_{i},\bm{U}^{\star})=\tau\bm{M}\bm{U}^{\star}(\bm{h}_{ i}-\bm{h}_{G})\) with \(\bm{h}_{i}=\phi_{\bm{\theta}}(\bm{x}_{i})\). Here, \(\psi\) denotes the logits, where the classifier weights are set as \(\bm{W}=\bm{M}\bm{U}^{\star}\), and the bias is set to \(\bm{b}=-\bm{M}\bm{U}^{\star}\bm{h}_{G}\) to account for feature centring. Furthermore, \(\bm{M}\) is the standard simplex ETF, \(\tilde{\bm{M}}\) is its normalised version, and \(\tilde{\bm{H}}\) is the normalised centred feature matrix. The temperature parameter \(\tau>0\) controls the lower bound of the cross-entropy loss when dealing with normalised features, as defined in [69; Theorem 1].

### Handling Stochastic Updates

In practice, we use stochastic gradient descent updates, and, as such, adjustments to our computations are necessary. With each gradient update now based on a mini-batch, we implement two keychanges. First, rather than directly optimising the problem of finding the nearest simplex ETF geometry concerning the feature means of the mini-batch, we introduce an exponential moving average operation during the computation of the feature means. This operation accumulates statistics and enhances training stability throughout iterations. Formally, at time step \(t\), we have the following equation, where \(\alpha\in\mathbb{R}\) represents the smoothing factor:

\[\tilde{\bm{H}}_{t}=\alpha\tilde{\bm{H}}_{\text{batch}}+(1-\alpha)\tilde{\bm{H} }_{t-1}\;.\] (9)

Second, we employ stratified batch sampling to guarantee that all class labels are represented in the mini-batch. This ensures that we avoid degenerate solutions when finding the nearest simplex ETF geometry, as our optimisation problem requires input feature means for all \(C\) classes. In cases where the number of classes exceeds the chosen batch size, we compute the per-class feature mean for the class labels present in the given batch. For the remaining class labels, we set their feature mean as the global mean of the batch. We repeat this process for each training iteration until we have sampled examples belonging to the missing class labels. At that point, we update the feature mean of those missing class labels with the new feature statistics. We reserve this method only for cases where the batch size is smaller than the number of labels since it can introduce instability during early iterations.

### Deep Declarative Layer

We can backpropagate through the Riemannian optimisation problem to update the feature means using a declarative node [23]. Then, the features are updated from both the loss and the feature means through auto-differentiation. The motivation for developing the DDN layer lies in recognising that, despite the presence of a proximal term, abrupt and sudden changes to the classifier may occur as the features are updated. These changes can pose challenges for backpropagation, potentially disrupting the stability and convergence of the training process. Incorporating an additional stream of gradients through the feature means to account for such changes, as depicted in Figure 1, assists in stabilising the feature updates during backpropagation.

To efficiently backpropagate through the optimisation problem, we employ techniques described in Gould et al. [23] utilising the implicit function theorem to compute the gradients. In our case, we have a scalar objective function \(f:\mathbb{R}^{d\times C}\to\mathbb{R}\), and a matrix constraint function \(J:\mathbb{R}^{d\times C}\to\mathbb{R}^{C\times C}\). Since we have matrix variables, we use vectorisation techniques [46] to avoid numerically dealing with tensor gradients. More specifically, we have the following:

**Proposition 1** (Following directly from Proposition 4.5 in Gould et al. [23]).: _Consider the optimisation problem in Equation 7. Assume that the solution exists and that the objective function \(f\) and the constraint function \(J\) are twice differentiable in the neighbourhood of the solution. If the \(\operatorname{rank}(\bm{A})=\frac{C(C+1)}{2}\) and \(\bm{G}\) is non-singular then:_

\[Dy(\bm{U})=\bm{G}^{-1}\bm{A}^{\top}(\bm{A}\bm{G}^{-1}\bm{A}^{\top})^{-1}(\bm {A}\bm{G}^{-1}\bm{B})-\bm{G}^{-1}\bm{B}\;,\]

_where,_

\[\bm{A} =\operatorname{rvech}(D_{\bm{U}}J(\tilde{\bm{H}},\bm{U}))\in \mathbb{R}^{\frac{C(C+1)}{2}\times dC}\;,\] \[\bm{B} =\operatorname{rvec}(D^{2}_{\bm{H}\bm{U}}\,f(\tilde{\bm{H}},\bm{ U}))\in\mathbb{R}^{dC\times dC}\;,\] \[\bm{G} =\operatorname{rvec}(D^{2}_{\bm{U}\bm{U}}\,f(\tilde{\bm{H}},\bm{ U}))-\bm{\Lambda}:\operatorname{rvech}(D^{2}_{\bm{U}\bm{U}}\,J(\tilde{\bm{H}}, \bm{U}))\in\mathbb{R}^{dC\times dC}\;.\]

Figure 1: Schematic of our proposed architecture for optimising towards the nearest simplex ETF. The classifier weights \(\bm{W}=\bm{U}^{\star}\bm{M}\) are an implicit function of the CNN features \(\bm{H}\). Note that the parameters of the CNN are updated via two gradient paths from the loss function \(\mathcal{L}\), a direct path (top) and an indirect path through \(\bm{U}^{\star}\) (bottom).

_Here, the double dot product symbol \((\cdot)\) denotes a tensor contraction on appropriate indices between the Lagrange multiplier matrix \(\bm{\Lambda}\) and a fourth-order tensor Hessian. Also, \(\operatorname{rvec}(\cdot)\) and \(\operatorname{rvech}(\cdot)\) refer to the row-major vectorisation and half-vectorisation operations, respectively. To find the Lagrange multiplier matrix \(\bm{\Lambda}\in\mathbb{R}^{C\times\frac{c+1}{2}}\), we solve the following equation where we have vectorised the matrix as \(\bm{\lambda}\in\mathbb{R}^{\frac{C(C+1)}{2}}\),_

\[\bm{\lambda}^{\top}\bm{A}=D_{\bm{U}}\ f(\tilde{\bm{H}},\bm{U})\.\]

_Alternatively, for a more efficient computation of the identity \(\bm{G}\), we can utilise the embedded gradient field method as defined in Birtea et al. [9, 10]. Therefore, we obtain:_

\[\bm{G}=\operatorname{rvec}(D_{\bm{U}}^{2}\ f(\tilde{\bm{H}},\bm{U}))-\bm{I}_{d }\otimes\Sigma(\bm{U})\in\mathbb{R}^{dC\times dC}\,\]

_where \(\Sigma(\bm{U})=\frac{1}{2}\Big{(}D_{\bm{U}}\ f(\tilde{\bm{H}},\bm{U})^{\top} \bm{U}+\bm{U}^{\top}D_{\bm{U}}\ f(\tilde{\bm{H}},\bm{U})\Big{)}\), and \(\otimes\) here denotes Kronecker product._

A detailed derivation of each identity in the proposition can be found in Appendix B.

## 4 Experiments

In our experiments, we perform feature normalisation onto a hypersphere, a common practice in training neural networks, which improves representation and enhances model performance [71, 26, 53, 42, 61, 62, 12, 16, 35]. We find that combining classifier weight normalisation with feature normalisation accelerates convergence [69]. Given that simplex ETFs are inherently normalised, we include classifier weight normalisation in our standard training procedure to ensure fair method comparisons.

Experimental Setup.In this study, we conduct experiments on three model variants. First, the standard method involves training a model with learnable classifier weights, following conventional practice. Second, in the fixed ETF method, we set the classifier to a predefined simplex ETF. In all experiments, we choose the simplex ETF with canonical direction. In Appendix C, we also include additional experiments for fixed simplex ETFs with random directions generated from a Haar measure [47]. Last, our implicit ETF method, where we set the classifier weights on-the-fly as the simplex ETF closest to the current feature means.

We repeat experiments on each method five times with distinct random seeds and report the median values alongside their respective ranges. For reproducibility and to streamline hyperparameter tuning, we employed Automatic Gradient Descent (AGD) [6]. Following the authors' recommendation, we set the gain/momentum parameter to 10 to expedite convergence, aligning it with other widely used optimisers like Adam [36] and SGD. Our experiments on real datasets run for 200 epochs with batch size 256; for the UFM analysis, we run 2000 iterations.

Our method underwent rigorous evaluation across various UFM sizes and real model architectures trained on actual datasets, including CIFAR10 [37], CIFAR100 [37], STL10 [14], and ImageNet-1000 [15], implemented on ResNet [29] and VGG [56] architectures. More specifically, we trained CIFAR10 on ResNet18 and VGG13, CIFAR100 and STL10 on ResNet50 and VGG13, and ImageNet-1000 on ResNet50. The input images were preprocessed pixel-wise by subtracting the mean and dividing by the standard deviation. Additionally, standard data augmentation techniques were applied, including random horizontal flips, rotations, and crops. All experiments were conducted using Nvidia RTX3090 and A100 GPUs.

Hyperparameter Selection and Riemannian Initialisation Schemes.We solve the Riemannian optimisation problem defined in Equation 7 using a Riemannian Trust-Region method [1] from pyManopt [60]. We maintain a proximal coefficient \(\delta\) set to \(10^{-3}\) consistently across all experiments. It is worth mentioning that algorithm convergence is robust to the precise value of \(\delta\). In our problem, determining values for \(\bm{U}_{\text{init}}\) and \(\bm{U}_{\text{prox}}\) is crucial. We explored several methods to initialise these parameters. One approach involved setting both towards the canonical simplex ETF direction. This means initialising them as a partial orthogonal matrix where the first \(C\) rows and columns form an identity matrix while the remaining \(d-C\) rows are filled with zeros. Another approach is to initialise both of them as random orthogonal matrices from classical compact groups, selected according to a Haar measure [47]. In the end, the approach that yielded the most stable results at initialisation was to employ either of the aforementioned initialisation methods to solve the original problem without the proximal term in Equation 6. We then used the obtained \(\bm{U}^{\star}\) to initialise both \(\bm{U}_{\text{init}}\) and \(\bm{U}_{\text{prox}}\) for the problem in Equation 7. This process was carried out only for the first gradient update of the first epoch. In subsequent iterations, we update these parameters to the \(\bm{U}^{\star}\) obtained from the previous time step. Importantly, the proximal term is held fixed during each Riemannian optimisation.

Regarding the calculation of the exponential moving average of the feature means, we have found that employing a decay policy on the smoothing factor \(\alpha\) yields optimal results. Specifically, we set \(\alpha=2/(T+1)\), where \(T\) represents the number of iterations. Additionally, we include a thresholding value of \(10^{-4}\), such that if \(\alpha\) falls below this threshold, we fix \(\alpha\) to be equal to the threshold. This precaution ensures that \(\alpha\) does not diminish throughout the iterations, thereby guaranteeing that the newly calculated feature means contribute sufficient statistics to the exponential moving average.

Finally, in our experiments, we set the temperature parameter \(\tau\) to five. This choice aligns with the findings discussed by Yaras et al. [69], highlighting the influence of the temperature parameter value on the extent of neural collapse statistics with normalised features.

Unconstrained Feature Models (UFMs).Our experiments on UFMs, which provide a controlled setting for evaluating the effectiveness of our method, are done using the following configurations:

* UFM-10: a 10-class UFM containing 1000 features with a dimension of 512.
* UFM-100: a 100-class UFM containing 5000 features with a dimension of 1024.
* UFM-200: a 200-class UFM containing 5000 features, with a dimension of 1024.
* UFM-1000: a 1000-class UFM containing 10000 features, with a dimension of 1024.

Results.We present the results for the synthetic UFM-10 case in Figure 2. The CE loss plot demonstrates that fixing the classifier weights to a simplex ETF achieves the theoretical lower bound of Yaras et al. [69, Thm. 1], indicating the attainment of a globally optimal solution. We also visualise the average cosine margin per epoch and the cosine margin distributions of each example at the end of training, defined in Zhou et al. [74]. The neural collapse metrics, \(NC1\) and \(NC3\), which measure the features' within-class variability, and the self-duality alignment between the feature means and the classifier weights [76], are also plotted. Last, we depict the absolute difference of the classifier and feature means norms to illustrate their convergence towards equinorms, as described in Papyan et al. [50]. A comprehensive description of the metrics can be found in Appendix A. Collectively, the plots indicate the superior performance of our method in achieving a neural collapse (NC) solution

Figure 2: UFM-10 results. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

faster than other approaches. In Figure 3, we demonstrate under the UFM setting that as we increase the number of classes, our method maintains constant performance and converges at the same rate, while the fixed ETF and the standard approach require more time to reach the interpolation threshold.

Numerical results for the top-1 train and test accuracy are reported in Tables 1 and 2, respectively. The results are provided for snapshots taken at epoch 50 and epoch 200. It is evident that our method achieves a faster convergence speed compared to the competitive methods while ultimately converging to the same performance level. Additionally, it is noteworthy that our method exhibits the smallest degree of variability across different runs, as indicated by the range values provided. Finally, in Figure 4, we present qualitative results that confirm our solution's ability to converge much faster and reach peak performance earlier than the standard and fixed ETF methods on ImageNet. It's important to note that the standard method with AGD is reported to converge to the same testing accuracy (\(65.5\%\)) at epoch 350, as shown in Bernstein et al. (2016, Figure 4). At epoch 200, the authors exhibit a testing accuracy of approximately \(51\%\). Since we have increased the gain parameter on AGD compared to the results reported in the original paper, we report a final \(60.67\%\) testing accuracy for the standard method, whereas our method reaches peak convergence at approximately epoch 80. We note that the ImageNet results reported in Tables 1 and 2, as well as Figure 4, are generated solely by solving the Riemannian optimisation problem without considering its gradient stream on the feature updates, due to computational constraints. We discuss the computational requirements of our method in Section 5. We also present qualitative results for all the other datasets and architectures in Appendix C.

## 5 Discussion: Limitations and Future Directions

Our method involves two gradient streams updating the features, as depicted in Figure 1. Interestingly, empirical observations on small-scale datasets (see Figure 15) indicate that even without the backpropagation through the DDN layer, the performance remains comparable, rendering the gradient calculation of the DDN layer optional. In Figure 14(c), we observe a strong impact of the DDN layer gradient on the atomic feature level, with more features reaching the theoretical simplex ETF margin by the end of training. To reach a consensus on the exact effect of the DDN gradient on the learning

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline  & & \multicolumn{4}{c}{Train accuracy at epoch 50} & \multicolumn{4}{c}{Train accuracy at epoch 200} \\ \cline{3-10} Dataset & Network & Standard & Fixed ETF & Implicit ETF & Standard & Fixed ETF & Implicit ETF \\ \hline \multirow{3}{*}{CIFAR10} & ResNet18 & \(87.42\begin{smallmatrix}89.7\\ 86.1\end{smallmatrix}\) & \(86.89\begin{smallmatrix}88.6\\ 84.7\end{smallmatrix}\) & \(\mathbf{88.71}\begin{smallmatrix}89.6\\ 88.5\end{smallmatrix}\) & \(96.69\begin{smallmatrix}98.6\\ 96.5\end{smallmatrix}\) & \(97.18\begin{smallmatrix}97.9\\ 95.6\end{smallmatrix}\) & \(\mathbf{98.09}\begin{smallmatrix}98.6\\ 97.7\end{smallmatrix}\) \\  & VGG13 & \(93.59\begin{smallmatrix}97.0\\ 90.7\end{smallmatrix}\) & \(76.66\begin{smallmatrix}53.5\\ 53.9\end{smallmatrix}\) & \(\mathbf{95.69}\begin{smallmatrix}96.2\\ 95.2\end{smallmatrix}\) & \(99.15\begin{smallmatrix}98.7\\ 98.7\end{smallmatrix}\) & \(97.93\begin{smallmatrix}96.91\\ 95.96\end{smallmatrix}\) & \(\mathbf{99.56}\begin{smallmatrix}99.7\\ 99.4\end{smallmatrix}\) \\  & ResNet50 & \(58.47\begin{smallmatrix}59.6\\ 53.93\end{smallmatrix}\) & \(63.93\begin{smallmatrix}65.2\\ 61.1\end{smallmatrix}\) & \(\mathbf{72.15}\begin{smallmatrix}74.1\\ 70.1\end{smallmatrix}\) & \(95.87\begin{smallmatrix}98.6\\ 94.7\end{smallmatrix}\) & \(97.13\begin{smallmatrix}94.21\\ 90.4\end{smallmatrix}\) & \(\mathbf{96.96}\begin{smallmatrix}97.3\\ 96.2\end{smallmatrix}\) \\  & VGG13 & \(82.00\begin{smallmatrix}84.0\\ 80.5\end{smallmatrix}\) & \(81.14\begin{smallmatrix}81.9\\ 76.0\end{smallmatrix}\) & \(\mathbf{83.9}\begin{smallmatrix}89.4\\ 88.9\end{smallmatrix}\) & \(\mathbf{99.34}\begin{smallmatrix}99.2\\ 99.2\end{smallmatrix}\) & \(94.55\begin{smallmatrix}95.3\\ 92.5\end{smallmatrix}\) & \(98.92\begin{smallmatrix}90.0\\ 98.8\end{smallmatrix}\) \\  & ResNet50 & \(83.86\begin{smallmatrix}90.7\\ 84.5\end{smallmatrix}\) & \(86.76\begin{smallmatrix}68.8\\ 77.78\end{smallmatrix}\) & \(\mathbf{93.54}\begin{smallmatrix}95.3\\ 91.3\end{smallmatrix}\) & \(99.42\begin{smallmatrix}99.9\\ 99.9\end{smallmatrix}\) & \(98.38\begin{smallmatrix}93.3\\ 98.1\end{smallmatrix}\) & \(\mathbf{99.72}\begin{smallmatrix}99.9\\ 99.8\end{smallmatrix}\) \\  & VGG13 & \(82.66\begin{smallmatrix}90.7\\ 93.7\end{smallmatrix}\) & \(83.60\begin{smallmatrix}85.1\\ 65.6\end{smallmatrix}\) & \(\mathbf{90.14}\begin{smallmatrix}93.5\\ 69.2\end{smallmatrix}\) & \(100.0\begin{smallmatrix}00\\ 1000\\ 99.2\end{smallmatrix}\) & \(99.98\begin{smallmatrix}93.1\\ 99.8\end{smallmatrix}\) & \(\mathbf{100.0}\begin{smallmatrix}100\\ 1000\\ 1000\\ 1000\end{smallmatrix}\) \\ ImageNet & ResNet50 & \(58.35\begin{smallmatrix}91.1\\ 88.0\end{smallmatrix}\) & \(70.44\begin{smallmatrix}70.7\\ 69.5\end{smallmatrix}\) & \(\mathbf{74.09}\begin{smallmatrix}74.5\\ 73.8\end{smallmatrix}\) & \(77.20\begin{smallmatrix}73.5\\ 76.3\end{smallmatrix}\) & \(83.09\begin{smallmatrix}83.6\\ 83.1\end{smallmatrix}\) & \(\mathbf{88.01}\begin{smallmatrix}85.5\\ 87.5\end{smallmatrix}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Train top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.

Figure 3: The evolution of convergence measured in top-1 accuracy of the UFM as we increase the number of classes, plotted for the first 800 epochs. We omit the rest of the epochs as all methods have converged and have identical results.

process, further experiments on large-scale datasets are needed. However, on large-scale datasets with large \(d\) and \(C\), such as ImageNet, computing the backward pass of the Riemannian optimisation is challenging due to the memory inefficiency of the current implementation of DDN gradients. This limitation is an area we aim to address in future work. Note that in all other experiments, we use the full gradient computations, including both direct and indirect components, through the DDN layer. We summarise the GPU memory requirements for each method across various datasets in Table 3.

Our discussion so far has focused on convergence speed in terms of the number of epochs required for the network to converge. However, it is also important to consider the time required per epoch. In our case, as training progresses, the time taken by the Riemannian optimization quickly becomes almost negligible compared to the network's total forward pass time, while it approaches the standard and fixed ETF training forward times, as shown in Figure 4(a). However, DDN gradient computation increases considerably when the feature dimension \(d\) and the number of classes \(C\) increase and starts to dominate the runtime for large datasets such as ImageNet. Nevertheless, for ImageNet, we do not compute the DDN gradients and still outperform other methods. We plan to explore ways to expedite the DDN forward and backward pass in future work.

## 6 Conclusion

In this paper, we introduced a novel method for determining the nearest simplex ETF to the penultimate features of a neural network and utilising it as our target classifier at each iteration. This contrasts with previous approaches, which either fix to a specific simplex ETF or allow the network

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline  & & \multicolumn{3}{c}{Test accuracy at epoch 50} & \multicolumn{3}{c}{Test accuracy at epoch 200} \\ \cline{3-8} Dataset & Network & Standard & Fixed ETF & Implicit ETF & Standard & Fixed ETF & Implicit ETF \\ \hline \multirow{3}{*}{CIFAR10} & ResNet18 & \(80.47\,^{82.6}_{79.4}\) & \(80.63\,^{81.8}_{79.4}\) & \(\mathbf{81.76}\,^{82.0}_{81.4}\) & \(83.97\,^{84.8}_{83.2}\) & \(84.53\,^{84.9}_{83.7}\) & \(\mathbf{84.78}\,^{85.1}_{84.3}\) \\  & VGG13 & \(86.70\,^{89.4}_{83.7}\) & \(70.99\,^{80.7}_{50.7}\) & \(\mathbf{88.30}\,^{88.7}_{87.4}\) & \(90.34\,^{91.5}_{89.1}\) & \(72.48\,^{90.5}_{56.9}\) & \(\mathbf{90.98}\,^{91.5}_{90.6}\) \\  & ResNet50 & \(45.91\,^{46.3}_{42.1}\) & \(45.37\,^{45.6}_{43.2}\) & \(\mathbf{48.38}\,^{49.3}_{83.0}\) & \(\mathbf{51.29}\,^{51.9}_{50.4}\) & \(48.03\,^{49.3}_{47.7}\) & \(50.52\,^{51.2}_{50.2}\) \\  & VGG13 & \(60.82\,^{61.2}_{59.3}\) & \(60.10\,^{61.1}_{57.1}\) & \(\mathbf{62.39}\,^{63.6}_{61.8}\) & \(\mathbf{67.54}\,^{68.1}_{66.4}\) & \(62.78\,^{63.4}_{61.6}\) & \(67.14\,^{67.6}_{66.8}\) \\ \multirow{3}{*}{STL10} & ResNet50 & \(55.41\,^{58.3}_{54.8}\) & \(\mathbf{58.11}\,^{60.0}_{57.1}\) & \(57.56\,^{59.2}_{56.4}\) & \(63.60\,^{65.2}_{61.8}\) & \(\mathbf{64.33}\,^{66.5}_{63.9}\) & \(62.75\,^{63.0}_{60.7}\) \\  & VGG13 & \(66.09\,^{72.3}_{60.0}\) & \(66.15\,^{67.7}_{52.2}\) & \(\mathbf{68.78}\,^{71.3}_{56.2}\) & \(\mathbf{81.61}\,^{81.9}_{81.3}\) & \(79.53\,^{80.3}_{78.6}\) & \(79.94\,^{80.9}_{79.5}\) \\ \multirow{3}{*}{ImageNet} & ResNet50 & \(52.64\,^{53.3}_{52.3}\) & \(58.85\,^{59.1}_{58.3}\) & \(\mathbf{63.40}\,^{63.8}_{63.2}\) & \(60.20\,^{60.7}_{59.8}\) & \(61.47\,^{62.0}_{61.4}\) & \(\mathbf{65.36}\,^{65.7}_{65.0}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test top-1 accuracy results presented as a median with indices indicating the range of values from five random seeds. Best results are bolded.

Figure 4: ImageNet results on ResNet-50. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

to learn it through gradient descent. Our method involves solving a Riemannian optimisation problem facilitated by a deep declarative node, enabling backpropagation through this process.

We demonstrated that our approach enhances convergence speed across various datasets and architectures while also reducing variability stemming from different random initialisations. By defining the optimal structure of the classifier and efficiently leveraging its rotation invariance property to find the one closest to the backbone features, we anticipate that our method will facilitate the creation of new architectures and the utilisation of new datasets without necessitating specific learning or tuning of the classifier's structure.

## References

* Absil et al. [2007] P.-A. Absil, C. G. Baker, and K. A. Gallivan. Trust-region methods on Riemannian manifolds. _Foundations of Computational Mathematics_, 7(3):303-330, 2007. doi: 10.1007/s10208-005-0179-9.
* Absil et al. [2008] P.-A. Absil, R. Mahony, and R. Sepulchre. _Optimization Algorithms on Matrix Manifolds_. Princeton University Press, 2008. ISBN 9780691132983. URL http://www.jstor.org/stable/j.ctt7smmk.
* Agrawal et al. [2019] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable convex optimization layers. _Advances in neural information processing systems_, 32, 2019.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Standard & Fixed ETF & \multicolumn{2}{c}{Implicit ETF} \\ \cline{3-5} Model & & & w/o DDN Bwd & w/ DDN Bwd \\ \hline UFM-10 & \(1.5\) & \(1.5\) & \(1.5\) & \(1.6\) \\ UFM-100 & \(1.7\) & \(1.7\) & \(1.7\) & \(10.7\) \\ UFM-200 & \(1.7\) & \(1.7\) & \(1.8\) & \(70.3\) \\ UFM-1000 & \(1.9\) & \(1.9\) & \(2.9\) & N/A \\ ResNet18 - CIFAR10 & \(2.2\) & \(2.2\) & \(2.2\) & \(2.3\) \\ ResNet50 - CIFAR10 & \(2.6\) & \(2.6\) & \(2.7\) & \(2.8\) \\ ResNet50 - CIFAR100 & \(2.6\) & \(2.6\) & \(2.7\) & \(18.9\) \\ ResNet50 - ImageNet & \(27.5\) & \(27.2\) & \(27.8\) & N/A \\ \hline \hline \end{tabular}
\end{table}
Table 3: GPU memory (in Gigabytes) during training.

Figure 5: CIFAR100 computational cost results on ResNet-50. In (a), we plot the forward pass time for each method. For the implicit ETF method, which has dynamic computation times, we also include the mean and median time values. In (b), we plot the computational cost for each forward and backward pass across methods. For the implicit ETF forward pass, we have taken its median time. The notation is as follows: S/F = Standard Forward Pass, S/B = Standard Backward Pass, F/F = Fixed ETF Forward Pass, F/B = Fixed ETF Backward Pass, I/F = Implicit ETF Forward Pass, and I/B = Implicit ETF Backward Pass.

* Amos and Kolter [2017] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In _International Conference on Machine Learning_, pp. 136-145. PMLR, 2017.
* Behnia et al. [2023] Tina Behnia, Ganesh Ramachandra Kini, Vala Vakilian, and Christos Thrampoulidis. On the implicit geometry of cross-entropy parameterizations for label-imbalanced data. In _International Conference on Artificial Intelligence and Statistics_, pp. 10815-10838. PMLR, 2023.
* Bernstein et al. [2023] Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue. Automatic Gradient Descent: Deep Learning without Hyperparameters. _arXiv:2304.05187_, 2023.
* Birtea and Comanescu [2012] Petre Birtea and Dan Comanescu. Geometrical dissipation for dynamical systems. _Communications in Mathematical Physics_, 316:375-394, 2012.
* Birtea and Comanescu [2015] Petre Birtea and Dan Comanescu. Hessian operators on constraint manifolds. _Journal of Nonlinear Science_, 25:1285-1305, 2015.
* Birtea et al. [2019] Petre Birtea, Ioan Casu, and Dan Comanescu. First order optimality conditions and steepest descent algorithm on orthogonal stiefel manifolds. _Optim. Lett._, 13(8):1773-1791, November 2019.
* Birtea et al. [2020] Petre Birtea, Ioan Casu, and Dan Comanescu. Second order optimality on orthogonal stiefel manifolds. _Bulletin des Sciences Mathematiques_, 161:102868, 2020. ISSN 0007-4497. doi: https://doi.org/10.1016/j.bulsci.2020.102868. URL https://www.sciencedirect.com/science/article/pii/S0007449720300385.
* Boumal [2023] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023.
* Chan et al. [2020] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. Deep networks from the principle of rate reduction. _arXiv preprint arXiv:2010.14765_, 2020.
* Xu and Lin [2020] Lingling He Changqing Xu and Zerong Lin. Commutation matrices and commutation tensors. _Linear and Multilinear Algebra_, 68(9):1721-1742, 2020. doi: 10.1080/03081087.2018.1556242. URL https://doi.org/10.1080/03081087.2018.1556242.
* Coates et al. [2011] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dudik (eds.), _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15 of _Proceedings of Machine Learning Research_, pp. 215-223, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/coates1ia.html.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on_, pp. 248-255. IEEE, 2009. URL https://ieeexplore.ieee.org/abstract/document/5206848/.
* Deng et al. [2019] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 4690-4699, 2019.
* Du et al. [2019] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pp. 1675-1685. PMLR, 2019.
* Edelman et al. [1998] Alan Edelman, Tomas A Arias, and Steven T Smith. The geometry of algorithms with orthogonality constraints. _SIAM journal on Matrix Analysis and Applications_, 20(2):303-353, 1998.
* Fang et al. [2021] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118. URL https://www.pnas.org/doi/abs/10.1073/pnas.2103091118.

* Gao et al. [2018] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework for optimization problems with orthogonality constraints. _SIAM Journal on Optimization_, 28(1):302-332, 2018.
* Gidel et al. [2019] Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/f39ae9ff3a81f499230c4126e01f421b-Paper.pdf.
* Goodfellow et al. [2016] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.
* Gould et al. [2022] S. Gould, R. Hartley, and D. Campbell. Deep declarative networks. _IEEE Transactions on Pattern Analysis & Machine Intelligence_, 44(08):3988-4004, aug 2022. ISSN 1939-3539. doi: 10.1109/TPAMI.2021.3059462.
* Gould et al. [2016] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. _arXiv preprint arXiv:1607.05447_, 2016.
* Gould et al. [2022] Stephen Gould, Dylan Campbell, Itzik Ben-Shabat, Chamin Hewa Koneputugodage, and Zhiwei Xu. Exploiting problem structure in deep declarative networks: Two case studies. _arXiv preprint arXiv:2202.12404_, 2022.
* Graf et al. [2021] Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised contrastive learning. In _International Conference on Machine Learning_, pp. 3821-3830. PMLR, 2021.
* Gunasekar et al. [2018] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. _Advances in neural information processing systems_, 31, 2018.
* Han et al. [2022] X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=w1UbdvWH_R3.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In _Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition_, CVPR '16, pp. 770-778. IEEE, June 2016. doi: 10.1109/CVPR.2016.90. URL http://ieeexplore.ieee.org/document/7780459.
* Hoffer et al. [2018] Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the last weight layer. _arXiv preprint arXiv:1801.04540_, 2018.
* Jagadeesan et al. [2022] Meena Jagadeesan, Ilya Razenshteyn, and Suriya Gunasekar. Inductive bias of multi-channel linear convolutional networks with bounded weight norm. In _Conference on Learning Theory_, pp. 2276-2325. PMLR, 2022.
* Ji et al. [2021] Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie J Su. An unconstrained layer-peeled perspective on neural collapse. _arXiv preprint arXiv:2110.02796_, 2021.
* Ji et al. [2020] Ziwei Ji, Miroslav Dudik, Robert E. Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. In Jacob Abernethy and Shivani Agarwal (eds.), _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pp. 2109-2136. PMLR, 09-12 Jul 2020. URL https://proceedings.mlr.press/v125/j120a.html.
* Jumper et al. [2020] John M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, TamasBerghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold. _Nature_, 596:583 - 589, 2021. URL https://api.semanticscholar.org/CorpusID:235959867.
* Khosla et al. [2020] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Advances in neural information processing systems_, 33:18661-18673, 2020.
* Kingma and Ba [2015] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, San Diego, CA, USA, 2015.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* LeCun et al. [2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. _nature_, 521(7553):436-444, 2015.
* Casado [2019] Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. _Advances in Neural Information Processing Systems_, 32, 2019.
* Lezcano-Casado and Martinez-Rubio [2019] Mario Lezcano-Casado and David Martinez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In _International Conference on Machine Learning_, pp. 3794-3803. PMLR, 2019.
* Li et al. [2018] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. _Advances in neural information processing systems_, 31, 2018.
* Liu et al. [2017] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 212-220, 2017.
* Lu and Steinerberger [2022] Jianfeng Lu and Stefan Steinerberger. Neural collapse under cross-entropy loss. _Applied and Computational Harmonic Analysis_, 59:224-241, 2022.
* Lyu and Li [2020] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJeLlgBKPS.
* Magnus and Neudecker [1980] Jan R. Magnus and H. Neudecker. The elimination matrix: Some lemmas and applications. _SIAM Journal on Algebraic Discrete Methods_, 1(4):422-449, 1980. doi: 10.1137/0601049. URL https://doi.org/10.1137/0601049.
* Magnus and Neudecker [2019] Jan R. Magnus and Heinz Neudecker. _Matrix differential calculus with applications in statistics and econometrics / Jan Rudolph Magnus and Heinz Neudecker_. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ, third edition. edition, 2019. ISBN 1-119-54121-2.
* Mezzadri [2006] Francesco Mezzadri. How to generate random matrices from the classical compact groups. _arXiv preprint math-ph/0609050_, 2006.
* Mixon et al. [2020] Dustin G. Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. _CoRR_, abs/2011.11619, 2020. URL https://arxiv.org/abs/2011.11619.
* Neyshabur et al. [2014] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* Papyan et al. [2020] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Science_, 117(40):24652-24663, October 2020. doi: 10.1073/pnas.2015509117.
* Peifeng et al. [2023] Gao Peifeng, Qianqian Xu, Peisong Wen, Zhiyong Yang, Huiyang Shao, and Qingming Huang. Feature directions matter: Long-tailed learning via rotated balanced representation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 27542-27563. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/peifeng23a.html.

* Pernici et al. [2021] Federico Pernici, Matteo Bruni, Claudio Baecchi, and Alberto Del Bimbo. Regular polytope networks. _IEEE Transactions on Neural Networks and Learning Systems_, 33(9):4373-4387, 2021.
* Ranjan et al. [2017] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained softmax loss for discriminative face verification. _arXiv preprint arXiv:1703.09507_, 2017.
* Shamir [2021] Ohad Shamir. Gradient methods never overfit on separable data. _Journal of Machine Learning Research_, 22(85):1-20, 2021.
* Siegel [2020] Jonathan W. Siegel. Accelerated optimization with orthogonality constraints. _Journal of Computational Mathematics_, 39(2):207-226, 2020. ISSN 1991-7139. doi: https://doi.org/10.4208/jcm.1911-m2018-0242. URL http://global-sci.org/intro/article_detail/jcm/18372.html.
* Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _CoRR_, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.
* Soudry et al. [2018] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=r1q7n9gAb.
* Tanwisuth et al. [2021] Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. _Advances in Neural Information Processing Systems_, 34:17194-17208, 2021.
* Thrampoulidis et al. [2022] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. _Advances in Neural Information Processing Systems_, 35:27225-27238, 2022.
* Townsend et al. [2016] J. Townsend, N. Koep, and S. Weichwald. PyManopt: a Python toolbox for optimization on manifolds using automatic differentiation. _Journal of Machine Learning Research_, 17(137):1-5, 2016. URL https://www.pymanopt.org.
* Wang et al. [2018] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 5265-5274, 2018.
* Wang and Isola [2020] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _International conference on machine learning_, pp. 9929-9939. PMLR, 2020.
* Weinan and Wojtowytsch [2022] E Weinan and Stephan Wojtowytsch. On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. In _Mathematical and Scientific Machine Learning_, pp. 270-290. PMLR, 2022.
* Wen and Yin [2013] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. _Mathematical Programming_, 142(1):397-434, 2013.
* Xiao and Liu [2021] Nachuan Xiao and Xin Liu. Solving optimization problems over the stiefel manifold by smooth exact penalty function. _arXiv preprint arXiv:2110.08986_, 2021.
* Xiao et al. [2024] Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemann optimization. _Mathematics of Operations Research_, 49(1):366-397, 2024.
* Yang et al. [2022] Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=A6EmxI3_Xc.
* Yang et al. [2023] Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin, Philip H.S. Torr, Bernard Ghanem, and Dacheng Tao. Neural collapse terminus: A unified solution for class incremental learning and its variants. _arXiv pre-print_, 2023.

* Yaras et al. [2022] Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), _Advances in Neural Information Processing Systems_, volume 35, pp. 11547-11560. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/4b3cc0d1c897ebcf71aca92a4a26ac83-Paper-Conference.pdf.
* You et al. [2020] Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant learning rates for double over-parameterization. _Advances in Neural Information Processing Systems_, 33:17733-17744, 2020.
* Yu et al. [2020] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discriminative representations via the principle of maximal coding rate reduction. _Advances in Neural Information Processing Systems_, 33:9422-9434, 2020.
* Zhang et al. [2017] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
* Zhang and Sra [2016] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In _Conference on learning theory_, pp. 1617-1638. PMLR, 2016.
* Zhou et al. [2022] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features. In _International Conference on Machine Learning_, pp. 27179-27202. PMLR, 2022.
* Zhou et al. [2022] Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses created equal: A neural collapse perspective. _Advances in Neural Information Processing Systems_, 35:31697-31710, 2022.
* Zhu et al. [2021] Zhihui Zhu, Tianyu DING, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=KHODJaa6pzE.

## Appendix A Background

Following the definitions of the global mean and class mean of the penultimate-layer features \(\{\bm{h}_{c,i}\}\) in Equation 4, here we introduce the within-class and between-class covariances,

\[\begin{split}\bm{\Sigma}_{\bm{W}}&\triangleq\frac{1} {N}\sum_{c=1}^{C}\sum_{i=1}^{n_{c}}\left(\bm{h}_{c,i}-\bar{\bm{h}}_{c}\right) \left(\bm{h}_{c,i}-\bar{\bm{h}}_{c}\right)^{\top}\,\\ \bm{\Sigma}_{\bm{B}}&\triangleq\frac{1}{C}\sum_{c=1 }^{C}\left(\bar{\bm{h}}_{c}-\bm{h}_{G}\right)\left(\bar{\bm{h}}_{c}-\bm{h}_{G} \right)^{\top}\.\end{split}\] (10)

We proceed by expanding on the four key properties of the last-layer activations and classifiers, as empirically observed by Papyan et al. [50] at the terminal phase of training (TPT), where we have achieved zero classification error and continue towards zero loss.

1. Variability Collapse: Throughout training, feature activation variability diminishes as they converge towards their respective class means. \[NC1\triangleq\frac{1}{C}\operatorname{Tr}\left(\bm{\Sigma}_{\bm{W}}\bm{ \Sigma}_{\bm{B}}^{\dagger}\right)\,\] (11) where \(\dagger\) denotes the Moore-Penrose inverse.
2. Convergence to Simplex ETF: The class-mean activation vectors, centred around their global mean, converge to uniform norms while simultaneously maintaining equal-sized and maximally separable angles between them2. Footnote 2: Mathematically, we have defined NC2 as the collapse of the linear classifiers to a simplex ETF. \[NC2\triangleq\left\|\frac{\bm{W}\bm{W}^{\top}}{\|\bm{W}\bm{W}^{\top}\|_{F}}- \frac{1}{\sqrt{C-1}}\left(\bm{I}_{C}-\frac{1}{C}\bm{1}_{C}\bm{1}_{C}^{\top} \right)\right\|_{F}\,.\] (12)
3. Convergence to Self-duality: The feature class-means and linear classifiers eventually align in a dual vector space up to some scaling. \[NC3\triangleq\left\|\frac{\bm{W}\bar{\bm{H}}}{\|\bm{W}\bar{\bm{H}}\|_{F}}- \frac{1}{\sqrt{C-1}}\left(\bm{I}_{C}-\frac{1}{C}\bm{1}_{C}\bm{1}_{C}^{\top} \right)\right\|_{F}\,.\] (13)
4. Simplification to Nearest Class-Center (NCC): The network classifier tends to select the class whose mean is closest (in Euclidean distance) to a given deepnet activation. \[NC4\triangleq\operatorname*{arg\,max}_{c^{\prime}}\langle\bm{w}_{c^{\prime}},\bm{h}\rangle+b_{c^{\prime}}\rightarrow\operatorname*{arg\,min}_{c^{\prime} }\|\bm{h}-\bar{\bm{h}}_{c^{\prime}}\|_{2}\.\] (14)

Since the NC2 and NC3 metrics involve normalised matrices, it is not immediately evident whether the linear classifier and the class-mean activations are equinorm. Consequently, we introduce the following definition:

\[\bm{W}\bar{\bm{H}}\text{-Equinorm}=|W_{\text{equinorm}}-\bar{H}_{\text{ equinorm}}|\,\] (15)

where \(\bar{H}_{\text{equinorm}}=\frac{\operatorname*{std}_{c}(\|\bar{\bm{h}}_{c}- \bar{H}_{G}\|_{2})}{\operatorname*{avg}_{c}(\|\bm{h}_{c}-\bar{H}_{G}\|_{2})}\) and \(W_{\text{equinorm}}=\frac{\operatorname*{std}_{c}(\|\bm{w}_{c}\|_{2})}{ \operatorname*{avg}_{c}(\|\bm{w}_{c}\|_{2})}\).

Finally, to measure the extent of the variability collapse of each feature separately, we define the cosine margin metric as follows:

\[CM_{c,i}=\cos\theta_{c,i;j}-\max_{j\neq c}\cos\theta_{c,i;j},\quad\text{where }\cos\theta_{c,i;j}=\frac{\langle\bm{w}_{j}-\bm{w}_{G},\bm{h}_{c,i}-\bm{h}_{G }\rangle}{\|\bm{w}_{j}-\bm{w}_{G}\|_{2}\|\bm{h}_{c,i}-\bm{h}_{G}\|_{2}}\.\] (16)

Note that the maximal angle for a simplex ETF vector collection \(\{v_{c}\}_{c=1}^{C}\) are defined as such:

\[\frac{\langle v_{c},v_{c^{\prime}}\rangle}{\|v_{c}\|_{2}\|v_{c^{\prime}}\|_{2}} =\begin{cases}1,&\text{for}\,c=c^{\prime}\\ -\frac{1}{C-1},&\text{for}\,c\neq c^{\prime}\end{cases}\.\] (17)

Therefore, we can calculate the theoretical simplex ETF cosine margin as \(\frac{C}{C-1}\).

DDN Gradients

The original problem we are trying to solve, as defined in Section 3.2, is the following:

\[\underset{\bm{U}\in St_{C}^{\text{t}}}{\text{minimize}}\ \left\|\tilde{\bm{H}}-\bm{U} \tilde{\bm{M}}\right\|_{F}^{2}\,\] (18)

or equivalently expanded as such:

\[\begin{split} y(\bm{U})\in&\underset{\bm{U}\in \mathbb{R}^{d\times C}}{\text{arg}\min}\qquad\tilde{\bm{H}}:\tilde{\bm{H}}- \frac{2}{\sqrt{C-1}}\tilde{\bm{H}}:\bm{U}\tilde{\bm{M}}+\frac{1}{C-1}\bm{U}: \bm{U}\tilde{\bm{M}}\,\\ &\text{subject to}\qquad\bm{U}^{\top}\bm{U}=\bm{I}_{C}\.\end{split}\] (19)

Here, we denote the double-dot operator : as the Frobenius inner product, _i.e._, \(\bm{A}:\bm{B}=\sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij}B_{ij}=\mathrm{Tr}(\bm{A}^{ \top}\bm{B})\).

To compute the first and second-order gradients of the objective function and constraints, respectively, we need to consider matrices as variables. Utilising matrix differentials and vectorised derivatives, as defined in [46], simplifies the computation of second-order objective derivatives and all constraint derivatives. For the objective function, we have:

\[\begin{split} d\,f(\tilde{\bm{H}},\bm{U})&=-\frac{ 2}{\sqrt{C-1}}\tilde{\bm{H}}:d\,\bm{U}\tilde{\bm{M}}+\frac{1}{C-1}\bigg{(}d\, \bm{U}:\bm{U}\tilde{\bm{M}}+\bm{U}:d\,\bm{U}\tilde{\bm{M}}\bigg{)}\\ &=\frac{2}{\sqrt{C-1}}\tilde{\bm{H}}\tilde{\bm{M}}:d\,\bm{U}+ \frac{2}{C-1}\bm{U}\tilde{\bm{M}}:d\,\bm{U}\\ &=\bigg{(}\frac{2}{\sqrt{C-1}}\tilde{\bm{H}}\tilde{\bm{M}}+\frac {2}{C-1}\bm{U}\tilde{\bm{M}}\bigg{)}:d\,\bm{U}\\ \implies\,D_{\bm{U}}f(\tilde{\bm{H}},\bm{U})&= \frac{2}{\sqrt{C-1}}\tilde{\bm{H}}\tilde{\bm{M}}+\frac{2}{C-1}\bm{U}\tilde{\bm {M}}\in\mathbb{R}^{d\times C}\.\end{split}\] (20)

\[\begin{split} d\,D_{\bm{U}}(\tilde{\bm{H}},\bm{U})& =\frac{2}{C-1}d\,\bm{U}\tilde{\bm{M}}\\ \implies\,d\,\mathrm{vec}\,D_{\bm{U}}(\tilde{\bm{H}},\bm{U})& =\frac{2}{C-1}\mathrm{vec}(d\,\bm{U}\tilde{\bm{M}})=\frac{2}{C-1 }\bigg{(}\bm{I}_{d}\otimes\tilde{\bm{M}}\bigg{)}d\,\mathrm{vec}\,\bm{U}\\ \implies\,\mathrm{vec}(D_{\bm{U}}^{2}f(\tilde{\bm{H}},\bm{U}))& =\frac{2}{C-1}\bigg{(}\bm{I}_{d}\otimes\tilde{\bm{M}}\bigg{)}\in \mathbb{R}^{dC\times dC}\,\end{split}\] (21)

where we have defined here the row-major vectorisation method, _i.e._, \(\mathrm{vec}(\bm{A})=\mathrm{vec}(\bm{A}^{\top})\), and we have used the property:

\[\mathrm{vec}(\bm{A}\bm{B}\bm{C})=(\bm{A}\otimes\bm{C}^{\top})\mathrm{vec}(\bm {B})\.\] (22)

A similar proof follows for the second-order partial gradient of the objective. We omit the proof here and just declare the result:

\[\bm{B}\triangleq\mathrm{vec}(D_{\bm{U}}^{2}f(\tilde{\bm{H}},\bm{U}))=-\frac{ 2}{\sqrt{C-1}}\bigg{(}\bm{I}_{d}\otimes\tilde{\bm{M}}\bigg{)}\in\mathbb{R}^{ dC\times dC}\.\] (23)

Regarding the gradients of the constraint function, we have:

[MISSING_PAGE_FAIL:18]

and hence, we get:

\[\mathrm{rvec}\;(D^{2}_{\bm{UU}}J(\tilde{\bm{H}},\bm{U})_{ij})=\bm{I}_{d}\otimes( \bm{e}_{i}\bm{e}_{j}^{\top})+\bm{I}_{d}\otimes(\bm{e}_{j}\bm{e}_{i}^{\top})\in \mathbb{R}^{dC\times dC}\;.\] (31)

Then, we repeat the process with the elimination matrix to eliminate the redundant constraints. However, while this is one way to compute the derivative, a more efficient approach exists, namely the _embedded gradient vector field_ method, which we outline in the following subsection. For a complete overview of the method, we recommend readers to follow through the works of Birtea et al. [9, 10], Birtea & Comanescu [8, 7].

Finally, the gradients for the proximal problem in Equation 7 are straightforward to compute, with the only change in gradients being the added proximal terms in:

\[\begin{split} D_{\bm{U}}f(\tilde{\bm{H}},\bm{U})& =\frac{2}{K-1}\bm{U}\tilde{\bm{M}}-\frac{2}{\sqrt{K-1}}\tilde{\bm {H}}\tilde{\bm{M}}+\delta(\bm{U}-\bm{U}_{\text{prox}})\in\mathbb{R}^{d\times C }\;,\\ \mathrm{rvec}(D^{2}_{\bm{UU}}f(\tilde{\bm{H}},\bm{U}))& =\frac{2}{K-1}\bigg{(}\bm{I}_{d}\otimes\tilde{\bm{M}}\bigg{)}+ \delta\bm{I}_{dC}\in\mathbb{R}^{dC\times dC}\;.\end{split}\] (32)

### Implicit Formulation of Lagrange Multipliers on Differentiable Manifolds

An issue arises in Proposition 1 with the expression for \(\bm{G}\),

\[\bm{G}=\mathrm{rvec}(D^{2}_{\bm{UU}}\,f(\tilde{\bm{H}},\bm{U}))-\bm{\Lambda} :\mathrm{rvech}(D^{2}_{\bm{UU}}\,J(\tilde{\bm{H}},\bm{U}))\in\mathbb{R}^{dC \times dC}\;,\] (33)

where both the calculation of the Lagrange multiplier matrix \(\bm{\Lambda}\) (solved via a linear system) and the construction of the fourth-order tensor representing the second-order derivatives of the constraint function are complex and challenging. However, by recognising the manifold structure of the problem, we can reformulate Equation 33 in a simpler and more computationally efficient way. The embedded gradient vector field method offers such a solution [8].

For a general Riemannian manifold \((\mathcal{M},g)\), we can define the Gram matrix for the smooth functions \(f_{1},\dots,f_{s},h_{1},\dots,h_{r}:(\mathcal{M},g)\rightarrow\mathbb{R}\) as follows,

\[\mathrm{Gram}^{(f_{1},\dots,f_{s})}_{(h_{1},\dots,h_{r})}\triangleq\left[ \begin{array}{ccc}\langle\nabla h_{1},\nabla f_{1}\rangle&\dots&\langle \nabla h_{r},\nabla f_{1}\rangle\\ \vdots&\ddots&\vdots\\ \langle\nabla h_{1},\nabla f_{s}\rangle&\dots&\langle\nabla h_{r},\nabla f_{ s}\rangle\end{array}\right]\in\mathbb{R}^{s\times r}\;.\] (34)

In our problem, we are working with a compact Stiefel manifold, which is an embedded submanifold of \(\mathbb{R}^{d\times C}\), and we identify the isomorphism (via vec) between \(\mathbb{R}^{d\times C}\) and \(\mathbb{R}^{dC}\). A Stiefel manifold \(St^{d}_{C}=\{\bm{U}\in\mathbb{R}^{d\times C}\mid\bm{U}^{\top}\bm{U}=\bm{I}_{C}\}\) can be characterised by a set of constraint functions, \(j_{s},j_{pq}:\mathbb{R}^{dC}\rightarrow\mathbb{R}\), as follows:

\[\begin{split} j_{s}(\bm{u})&=\frac{1}{2}\|\bm{u}_{s}\|^{2}, \qquad 1\leq s\leq C\;,\\ j_{pq}(\bm{u})&=\langle\bm{u}_{p},\bm{u}_{q}\rangle, \qquad 1\leq p<q\leq C\;.\end{split}\] (35)

We consider a smooth cost function \(\tilde{f}:St^{d}_{C}\rightarrow\mathbb{R}\) and define \(f:\mathbb{R}^{d\times C}\rightarrow\mathbb{R}\) as a smooth extension (or prolongation) of \(\tilde{f}\). The embedded gradient vector field (after applying the isomorphism) is defined on the open set \(\mathbb{R}^{dC}_{reg}\subset\mathbb{R}^{dC}\) formed with the regular leaves of the constrained function \(\bm{j}:\mathbb{R}^{dC}\rightarrow\mathbb{R}^{\frac{C(C+1)}{2}}\) and is tangent to the foliation of this function. The vector field takes the form [9]:

\[\partial f(\bm{u})=\nabla f(\bm{u})-\sum_{1\leq s\leq C}\sigma_{s}(\bm{u}) \nabla j_{s}(\bm{u})-\sum_{1\leq p<q\leq C}\sigma_{pq}(\bm{u})\nabla j_{pq}( \bm{u})\;,\] (36)

where \(\sigma_{s},\sigma_{pq}\) are the Lagrange multiplier functions defined as such [7]:\[\sigma_{s}(\bm{u}) =\frac{\det\left(\operatorname{Gram}_{(j_{1},\ldots,j_{s-1},j_{s},j_{s+ 1},\ldots,j_{c},j_{12},\ldots,j_{c-1,C})}^{(j_{1},\ldots,j_{s-1},j_{s+1},\ldots,j_{c},j_{12},\ldots,j_{c-1,C})}(\bm{u})\right)}{\det\left(\operatorname{Gram} _{(j_{1},\ldots,j_{c},j_{12},\ldots,j_{c-1,C})}^{(j_{1},\ldots,j_{c},j_{12}, \ldots,j_{c-1,C})}(\bm{u})\right)}\triangleq\frac{\det\left(\operatorname{ Gram}_{s}(\bm{u})\right)}{\det\left(\operatorname{Gram}(\bm{u})\right)}\;,\] (37) \[\sigma_{pq}(\bm{u}) =\frac{\det\left(\operatorname{Gram}_{(j_{1},\ldots,j_{c},j_{12}, \ldots,j_{pq},j_{pq+1},\ldots,j_{c-1,C})}^{(j_{1},\ldots,j_{c},j_{12},\ldots,j _{pq-1},j_{pq+1},\ldots,j_{c-1,C})}(\bm{u})\right)}{\det\left(\operatorname{ Gram}_{(j_{1},\ldots,j_{c},j_{12},\ldots,j_{c-1,C})}^{(j_{1},\ldots,j_{c},j_{12}, \ldots,j_{c-1,C})}(\bm{u})\right)}\triangleq\frac{\det\left(\operatorname{ Gram}_{pq}(\bm{u})\right)}{\det\left(\operatorname{Gram}(\bm{u})\right)}\;.\]

The Gram matrix in the denominator of the Lagrange multiplier functions can be defined as a block matrix as follows,

\[\operatorname{Gram}(\bm{u})=\left[\begin{array}{cc}\bm{A}&\bm{C}^{\top}\\ \bm{C}&\bm{B}\end{array}\right]\;,\] (38)

where

\[\bm{A} =\left[\begin{array}{ccc}\langle\nabla j_{1},\nabla j_{1}\rangle &\ldots&\langle\nabla j_{C},\nabla j_{1}\rangle\\ \vdots&\ddots&\vdots\\ \langle\nabla j_{1},\nabla j_{C}\rangle&\ldots&\langle\nabla j_{C},\nabla j_{ C}\rangle\end{array}\right],\] \[\bm{B} =\left[\begin{array}{ccc}\langle\nabla j_{12},\nabla j_{12} \rangle&\ldots&\langle\nabla j_{C-1,C},\nabla j_{12}\rangle\\ \vdots&\ddots&\vdots\\ \langle\nabla j_{12},\nabla j_{C-1,C}\rangle&\ldots&\langle\nabla j_{C-1,C}, \nabla j_{C-1,C}\rangle\end{array}\right],\] \[\bm{C} =\left[\begin{array}{ccc}\langle\nabla j_{1},\nabla j_{12} \rangle&\ldots&\langle\nabla j_{C},\nabla j_{12}\rangle\\ \vdots&\ddots&\vdots\\ \langle\nabla j_{1},\nabla j_{C-1,C}\rangle&\ldots&\langle\nabla j_{C},\nabla j _{C-1,C}\rangle\end{array}\right].\]

For \(\operatorname{Gram}_{s}(\bm{u})\) and \(\operatorname{Gram}_{pq}(\bm{u})\), we can similarly define block decompositions by replacing the appropriate column of the \(\operatorname{Gram}(\bm{u})\) based on the index. For instance, to define \(\operatorname{Gram}_{s}(\bm{u})\), we replace the column \(s\) with \([\langle\nabla f(\bm{u}),\nabla j_{i}(\bm{u})\rangle]_{i=1}^{C-1,C}\).

It has been proved [9] that if \(\bm{U}\in St_{C}^{d}\) is a critical point of the function \(\tilde{f}\), _i.e._, \(\partial f(\bm{u})=0\), then \(\sigma_{s}(\bm{u}),\sigma_{pq}(\bm{u})\) become the classical Lagrange multipliers.

**Proposition 2** (Lagrange multiplier functions for Stiefel Manifolds).: _The Lagrange multiplier functions are described as a function of the constraint functions in Equation 35 in the following way:_

\[\sigma_{s}(\bm{u}) =\langle\nabla f(\bm{u}),\nabla j_{s}(\bm{u})\rangle=\left\langle \frac{\partial f}{\partial\bm{u}_{s}}(\bm{u}),\bm{u}_{s}\right\rangle\;,\] (39) \[\sigma_{pq}(\bm{u}) =\frac{1}{2}\langle\nabla f(\bm{u}),\nabla j_{pq}(\bm{u})\rangle =\frac{1}{2}\left(\left\langle\frac{\partial f}{\partial\bm{u}_{q}}(\bm{u}), \bm{u}_{p}\right\rangle\;+\left\langle\frac{\partial f}{\partial\bm{u}_{p}}(\bm {u}),\bm{u}_{q}\right\rangle\right)\;.\]

Proof.: We take the definition of the Lagrange multiplier functions in Equation 37. Starting with the denominator, we observe that the Gram matrix is of the form:

\[\operatorname{Gram}(\bm{u})=\left[\begin{array}{cc}\bm{I}_{C}&\bm{0}\\ \bm{0}&2\bm{I}_{\frac{C(C-1)}{2}}\end{array}\right]\;,\] (40)

since for each block matrix (\(\bm{A},\bm{B},\bm{C}\)) of the Gramian, we get for each of their elements:

* \(\forall A_{s,r}\colon\langle\nabla j_{s}(\bm{u}),\nabla j_{r}(\bm{u})\rangle= \delta_{sr}\)
* \(\forall B_{\gamma\tau,\alpha\beta}\colon\langle\nabla j_{\gamma\tau}(\bm{u}), \nabla j_{\alpha\beta}(\bm{u})\rangle=2\delta_{\gamma\alpha}\delta_{\tau\beta}+2 \delta_{\tau\alpha}\delta_{\gamma\beta}\)* \(\forall C_{s,\alpha\beta}\): \(\langle\nabla j_{s}(\bm{u}),\nabla j_{\alpha\beta}(\bm{u})\rangle=2\delta_{s \alpha}\delta_{s\beta}=0\)

where \(\delta_{ij}\) is defined as the Kronecker delta symbol, _i.e._, \(\delta_{ij}=[i=j]\).

Thus, the determinant of this Gram matrix is:

\[\det(\mathrm{Gram}(\bm{u}))=\det\left[\begin{array}{cc}\bm{I}_{C}&\bm{0}\\ \bm{0}&2\bm{I}_{\frac{C(C-1)}{2}}\end{array}\right]=\det(\bm{I}_{C})\det\left( 2\bm{I}_{\frac{C(C-1)}{2}}\right)=2^{\frac{C(C-1)}{2}}\;.\] (41)

Now, let us turn our focus to the numerator of the Lagrange multiplier function \(\sigma_{s}(\bm{u})\).

\[\det(\mathrm{Gram}_{s}(\bm{u}))=\det\left[\begin{array}{cccc|c}1&\dots& \langle\nabla f(\bm{u}),\nabla j_{1}(\bm{u})\rangle&\dots&0&0&\dots&0\\ \vdots&\ddots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\ 0&\dots&\langle\nabla f(\bm{u}),\nabla j_{s}(\bm{u})\rangle&\dots&0&0&\dots&0 \\ \vdots&\ddots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\ 0&\dots&\langle\nabla f(\bm{u}),\nabla j_{C}(\bm{u})\rangle&\dots&1&0&\dots&0 \\ \hline 0&\dots&\langle\nabla f(\bm{u}),\nabla j_{12}(\bm{u})\rangle&\dots&0&2& \dots&0\\ \vdots&\ddots&\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\ 0&\dots&\langle\nabla f(\bm{u}),\nabla j_{C-1,C}(\bm{u})\rangle&\dots&0&0& \dots&2\end{array}\right]\] (42)

\[=\langle\nabla f(\bm{u}),\nabla j_{s}(\bm{u})\rangle\cdot\det\left[\begin{array} []{cc}\bm{I}_{C-1}&\bm{0}\\ \bm{0}&2\bm{I}_{\frac{C(C-1)}{2}}\end{array}\right]\]

\[=2^{\frac{C(C-1)}{2}}\langle\nabla f(\bm{u}),\nabla j_{s}(\bm{u})\rangle\;.\]

Similarly, for \(\sigma_{pq}(\bm{u})\), we get:

\[\det(\mathrm{Gram}_{pq}(\bm{u})) =\langle\nabla f(\bm{u}),\nabla j_{pq}(\bm{u})\rangle\cdot\det \left[\begin{array}{cc}\bm{I}_{C}&\bm{0}\\ \bm{0}&2\bm{I}_{\frac{C(C-1)}{2}-1}\end{array}\right]\] (43) \[=2^{\frac{C(C-1)}{2}-1}\langle\nabla f(\bm{u}),\nabla j_{pq}(\bm{ u})\rangle\;.\]

Finally, we get the result by combining the determinants' results for each Lagrange multiplier function and computing the gradients of the constraint functions.

Similarly to the gradient vector field, we can show that the Hessian for a constraint manifold (_e.g._, Stiefel manifold, \(\mathrm{Hess}\tilde{f}(\bm{u}):T_{\bm{u}}St_{C}^{d}\times T_{\bm{u}}St_{C}^{d} \rightarrow\mathbb{R}\)) can be given as such [10]:

\[\begin{split}\mathrm{Hess}\tilde{f}(\bm{u})=& \left(\mathrm{Hess}f(\bm{u})-\sum_{1\leq s\leq C}\sigma_{s}(\bm{u})\mathrm{ Hess}j_{s}(\bm{u})\right.\\ &\left.-\sum_{1\leq p<q\leq C}\sigma_{pq}(\bm{u})\mathrm{Hess}j_{ pq}(\bm{u})\right)_{|T_{\bm{u}}St_{C}^{d}\times T_{\bm{u}}St_{C}^{d}}\;.\end{split}\] (44)

We can transform the results into a matrix form in the following way. First, we denote,

\[\nabla f(\bm{U})\triangleq\mathrm{vec}^{-1}(\nabla f(\bm{u}))\in\mathbb{R}^{d \times C};\quad\partial f(\bm{U})\triangleq\mathrm{vec}^{-1}(\partial f(\bm{u }))\in\mathbb{R}^{d\times C}\;.\] (45)

We then introduce a symmetric matrix \(\Sigma(\bm{U})\triangleq[\sigma_{pq}(\bm{u})]\in\mathbb{R}^{C\times C}\), where we also include the Lagrange multiplier function's symmetrical component. For the Stiefel manifold, the matrix form of the embedded gradient vector field (after some simple computation) is given by,

\[\partial f(\bm{U})=\nabla f(\bm{U})-\bm{U}\Sigma(\bm{U})\;,\] (46)where \(\Sigma(\bm{U})=\frac{1}{2}\left(\nabla f(\bm{U})^{\top}\bm{U}+\bm{U}^{\top}\nabla f (\bm{U})\right)\).

Regarding the Hessian in Equation 44, to write it in matrix form, we first need to compute the Hessian matrices of the constraint functions as follows3:

Footnote 3: Here the resulting Kronecker product is expressed in a row-major way.

\[[\mathrm{Hess}j_{s}(\bm{U})]=\left[\begin{array}{ccccc}\bm{0}_{d}&\dots&\bm {0}_{d}&\dots&\bm{0}_{d}\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ \bm{0}_{d}&\dots&\bm{I}_{d}&\dots&\bm{0}_{d}\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ \bm{0}_{d}&\dots&\bm{0}_{d}&\dots&\bm{0}_{d}\end{array}\right]=\bm{I}_{d} \otimes\left(\bm{e}_{s}\otimes\bm{e}_{s}^{\top}\right);\]

\[[\mathrm{Hess}j_{pq}(\bm{U})]=\left[\begin{array}{cccccc}\bm{0}_{d}&\dots& \bm{0}_{d}&\dots&\bm{0}_{d}&\dots&\bm{0}_{d}\\ \vdots&\ddots&\vdots&\ddots&\vdots&\ddots&\vdots\\ \bm{0}_{d}&\dots&\bm{0}_{d}&\dots&\bm{I}_{d}&\dots&\bm{0}_{d}\\ \vdots&\ddots&\vdots&\ddots&\vdots&\ddots&\vdots\\ \bm{0}_{d}&\dots&\bm{0}_{d}&\dots&\bm{0}_{d}&\dots&\bm{0}_{d}\end{array} \right]=\bm{I}_{d}\otimes\left(\bm{e}_{p}\otimes\bm{e}_{q}^{\top}+\bm{e}_{q} \otimes\bm{e}_{p}^{\top}\right).\] (47)

Finally, the matrix form of the Hessian of the cost function \(\tilde{f}:St_{C}^{d}\rightarrow\mathbb{R}\) is given by,

\[\mathrm{Hess}\tilde{f}(\bm{U})=(\mathrm{Hess}f(\bm{U})-\bm{I}_{d}\otimes\Sigma (\bm{U}))_{|T_{\bm{U}}St_{C}^{d}\times T_{\bm{U}}St_{C}^{d}}\,\] (48)

where from there, we can re-define the expression \(\bm{G}\) in Equation 33 as follows,

\[\bm{G}=\mathrm{rvec}(D^{2}_{\bm{U}\bm{U}}\,f(\tilde{\bm{H}},\bm{U}))-\bm{I}_{ d}\otimes\Sigma(\bm{U})\in\mathbb{R}^{dC\times dC}\.\] (49)

## Appendix C Additional Experimental Results

Figure 6: CIFAR10 results on ResNet-18. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 8: CIFAR100 results on ResNet-50. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 7: CIFAR10 results on VGG-13. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 10: STL10 results on ResNet-50. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 9: CIFAR100 results on VGG-13. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 11: STL10 results on VGG-13. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 12: UFM-100 results. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 14: UFM-1000 results. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 13: UFM-200 results. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 16: CIFAR10 computational cost results on ResNet-18. In (a), we plot the forward pass time for each method. For the implicit ETF method, which has dynamic computation times, we also include the mean and median time values. In (b), we plot the computational cost for each forward and backward pass across methods. For the implicit ETF forward pass, we have taken its median time. The notation is as follows: S/F = Standard Forward Pass, S/B = Standard Backward Pass, F/F = Fixed ETF Forward Pass, F/B = Fixed ETF Backward Pass, I/F = Implicit ETF Forward Pass, and I/B = Implicit ETF Backward Pass.

Figure 15: CIFAR10 results on VGG-13, comparing the implicit ETF method in two scenarios: one where the DDN gradient is computed and included in the SGD update, and another where the DDN gradient computation is omitted from the update. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 17: CIFAR10 results on ResNet-18, where in standard no-norm we do not perform feature and weight normalisation. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

Figure 18: CIFAR10 results on ResNet-18, where in fixed random ETF we choose a random orthogonal direction instead of the canonical one. In all plots, the x-axis represents the number of epochs, except for plot (c), where the x-axis denotes the number of training examples.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We introduced a novel method leveraging the simplex ETF structure and the neural collapse phenomenon to enhance training convergence and stability in neural networks, and we provided experimental results supporting our claims.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitations of our approach in Section 5 of the paper.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not propose new theoretical results. We proposed a new method and its mathematical formulation as well as the appropriate mathematical background.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the necessary details required for reproducing the results are presented in the paper. Also, the code will be made publicly available upon acceptance.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Due to anonymity reasons, the code is not included in the submission. However, it will be made available in a GitHub repository upon acceptance.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the specific implementation details can be found in Section 4 of the paper and the appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have tested our results using different random seeds, and we present the median values along with the range.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: We have specified the GPUs that were used for our experiments.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms with the NeurIPS code of ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed in this paper.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not pose such risks.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, every model architecture and dataset used were cited.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.