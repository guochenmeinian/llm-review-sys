# On the Identifiability of Sparse ICA without

Assuming Non-Gaussianity

 Ignavier Ng\({}^{*1}\), Yujia Zheng\({}^{*1}\), Xinshuai Dong\({}^{1}\), Kun Zhang\({}^{1,2}\)

\({}^{1}\) Carnegie Mellon University

\({}^{2}\) Mohamed bin Zayed University of Artificial Intelligence

{ignavierng, yujiazh, dongxinshuai, kunz1}@cmu.edu

Equal contribution.

###### Abstract

Independent component analysis (ICA) is a fundamental statistical tool used to reveal hidden generative processes from observed data. However, traditional ICA approaches struggle with the rotational invariance inherent in Gaussian distributions, often necessitating the assumption of non-Gaussianity in the underlying sources. This may limit their applicability in broader contexts. To accommodate Gaussian sources, we develop an identifiability theory that relies on second-order statistics without imposing further preconditions on the distribution of sources, by introducing novel assumptions on the connective structure from sources to observed variables. Different from recent work that focuses on potentially restrictive connective structures, our proposed assumption of structural variability is both considerably less restrictive and provably necessary. Furthermore, we propose two estimation methods based on second-order statistics and sparsity constraint. Experimental results are provided to validate our identifiability theory and estimation methods.

## 1 Introduction

Independent component analysis (ICA) [12] has emerged as an essential statistical tool in the scientific community, with application in various disciplines including neuroscience [27], biology [41, 9], and Earth science [32]. It aims to uncover the hidden generative processes that govern the observed data and separate mixed signals into independent sources. However, it is known that the traditional approaches of ICA struggle with Gaussian sources, which may limit their applicability in a wide variety of contexts. For instance, several biological traits and measurements such as height, blood pressure, and measurement errors in genetics, as well as the thermal noises in electronic circuits, may often be normally distributed [29, 34]. Classical identifiability results in ICA rely on higher-order statistics (e.g., Kurtois) [24], and cannot provide desired theoretical guarantees when there is more than one Gaussian source. Identifiability based on second-order statistic is thus essential in these scenarios.

The primary hurdle in applying ICA to Gaussian sources lies in the rotational invariance of Gaussian distribution [25]. To address this issue, earlier studies [30, 5, 35] incorporated additional information by assuming that the sources are nonstationary. However, this extra information may not always be readily available, limiting the generalizability of these approaches. Thus, recent research [49, 1] has started to delve into the connective structure between the sources and the observed variables, as opposed to solely focusing on distributional assumptions (e.g., non-Gaussianity and nonstationarity). This shift of focus is motivated by a key observation: despite the rotational invariance in the distribution of Gaussian sources, the sparsity of mixing matrix undergoes noticeable changes, i.e., it may be denser after rotation [49]. Building on this insight, Zheng et al. [49] introduced two assumptions on the support of the mixing matrix to achieve identifiability of Gaussian sources, leading to a novel perspective for tackling this long-standing challenge in the field of ICA. On the other hand, Abrahamsen and Rigollet [1] assumed that the mixing matrix is generated from a sparse Bernoulli-Gaussian ensemble.

Although the rotational invariance of Gaussian distribution can be resolved by the structural assumptions on the mixing matrix proposed by Zheng et al. [49], they may be deemed overly restrictive. For instance, both of their structural assumptions cannot deal with the case where the set of observed variables influenced by one source is a subset of those affected by another source, which may not be uncommon in practice. This may limit the applicability of ICA in complex real-world scenarios, thus underscoring the need for a weaker and more flexible structural assumption that is capable of addressing the rotational invariance in a more universally applicable manner.

To enhance the applicability with Gaussian sources, we develop an identifiability theory of ICA from second-order statistics under more flexible structural constraints. We introduce a novel assumption, namely _structural variability_, that is considerably weaker than existing ones. Notably, this assumption is proved to be among the necessary conditions for identifying Gaussian sources by focusing on the connective structure (i.e., the support of the mixing matrix). Moreover, we propose two estimation methods grounded in sparsity regularization and continuous constrained optimization. The efficacy of our proposed methods has been validated through experiments, which also reaffirm the validity of our theoretical result. Lastly, as a matter of independent interest, we establish the connection between our identifiability result of ICA with causal discovery from second-order statistics; our finding further bridges the gap between these two fields and provides insights into the interpretation of our result.

## 2 Problem Setting

We consider the ICA setup given by \(\mathbf{x}=\tilde{\mathbf{A}}\mathbf{s}\), where \(\mathbf{x}\in\mathbb{R}^{n}\) denotes the observed random vector, \(\mathbf{s}\in\mathbb{R}^{n}\) is the latent random vector representing the independent components, also called sources, and \(\tilde{\mathbf{A}}=[\tilde{\mathbf{a}}_{1}|\cdots|\tilde{\mathbf{a}}_{n}]\in \mathbb{R}^{n\times n}\) denotes the unknown mixing matrix. We assume here that \(\tilde{\mathbf{A}}\) has full rank and all sources are standardized. For the estimated mixing matrix \(\hat{\mathbf{A}}\) and the ground-truth one \(\tilde{\mathbf{A}}\), we write \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\) if they differ only in column permutations and sign changes of columns, and \(\hat{\mathbf{A}}\not\succ\tilde{\mathbf{A}}\) vice versa. The goal is then to estimate \(\hat{\mathbf{A}}\) such that \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\); in this case, one could identify a one-on-one mapping between the ground truth sources and the estimated ones, i.e., the unknown mixing process has been demixed during estimation. Since the support of a mixing matrix \(\mathbf{A}\) essentially represents the connective structure between sources and observed variables, we have the following definition for the ease of reference.

**Definition 1** (Connective Structure).: _Given a mixing matrix \(\mathbf{A}\), we define its connective structure as a directed bipartite graph \(\mathcal{G}_{\mathbf{A}}=(\mathcal{V}_{\mathbf{A}},\mathcal{E}_{\mathbf{A}})\) from sources \(\mathbf{s}\) to observed variables \(\mathbf{x}\), where the nodes and edges are defined as \(\mathcal{V}_{\mathbf{A}}\coloneqq\{s_{i}\}_{i=1}^{n}\cup\{x_{i}\}_{i=1}^{n}\) and \(\mathcal{E}_{\mathbf{A}}\coloneqq\{(s_{j},x_{i}):a_{ij}\neq 0\}\), respectively._

Notations.We use bold capital letters (e.g., \(\mathbf{A}\)), bold lowercase letters (e.g., \(\mathbf{a}\)), and italic letters (e.g., \(a\)) to denote matrices, vectors, and scalar quantities, respectively. For any matrix \(\mathbf{A}\), we denote its \(j\)-th column by \(\mathbf{a}_{j}\), \(i\)-th row by \(\mathbf{a}_{i,:}\), and \((j,j)\)-th entry by \(a_{i,j}\). We also denote by \(\mathbf{A}_{\mathcal{J}}\) the submatrix of \(\mathbf{A}\) by obtaining the columns indexed by set \(\mathcal{J}\). For any vector \(\mathbf{a}\), we denote its \(i\)-th entry by \(a_{i}\). We define the support set of matrix \(\mathbf{A}\) as \(\operatorname{supp}(\mathbf{A})\coloneqq\{(i,j):a_{i,j}\neq 0\}\), and its support matrix as \(\boldsymbol{\xi}_{\mathbf{A}}\) which is of the same size as \(\mathbf{A}\), where \((\boldsymbol{\xi}_{\mathbf{A}})_{i,j}=\times\) if \(a_{i,j}\neq 0\) and \((\boldsymbol{\xi}_{\mathbf{A}})_{i,j}=0\) otherwise. The notations of support set and support matrix are similarly defined for vector \(\mathbf{a}\). We denote by \(\|\mathbf{A}\|_{0}\) the number of nonzero entries in \(\mathbf{A}\), and we have \(\|\mathbf{A}\|_{0}=|\operatorname{supp}(\mathbf{A})|=\|\boldsymbol{\xi}_{ \mathbf{A}}\|_{0}\). Furthermore, we denote the \(n\times n\) identity matrix and \(m\times n\) zero matrix by \(\mathbf{I}_{n}\) and \(\mathbf{0}_{m\times n}\), respectively; to lighten the notation, we drop their subscripts when the context is clear. We also use \([n]\) to denote \(\{1,2,\ldots,n\}\).

## 3 Identifiability Result without Assuming Non-Gaussianity

By exploiting the non-Gaussianity of the sources, such as fourth-order cumulant, existing approaches are able to estimate the true mixing matrix \(\tilde{\mathbf{A}}\) up to signed column permutation when there is at most one Gaussian source [23; 2]. However, these approaches typically fail in the presence of more than one Gaussian source, because higher-order statistics cannot be utilized for full identifiability. The primary challenge of achieving identifiability for Gaussian sources lies in the rotational invariance of the Gaussian distribution. More specifically, the second-order statistics (or, more specifically, population-level covariance matrix) \(\tilde{\mathbf{\Sigma}}=\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}\) remains unchanged if one replaces \(\tilde{\mathbf{A}}\) with \(\tilde{\mathbf{A}}\mathbf{U}\) for any orthogonal matrix \(\mathbf{U}\). Therefore, considering only second-order statistics, the true mixing matrix \(\tilde{\mathbf{A}}\) is, generally speaking, only identifiable up to right orthogonal transformation without further assumptions. In this section, we adopt a different perspective that departs from traditional distributional assumptions (i.e., non-Gaussianity and fourth-order cumulant), and instead introduces novel and precise assumptions on the connective structure, specifically the support of the mixing matrix. These assumptions enable identification of the true mixing matrix \(\tilde{\mathbf{A}}\) up to signed column permutation using second-order statistics and sparsity constraint. Roughly speaking, with the population-level covariance matrix \(\tilde{\mathbf{\Sigma}}\), we consider the following formulation:

\[\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}\|\mathbf{A}\|_{0}\quad\mathrm{ subject\ to}\quad\mathbf{A}\mathbf{A}^{\top}=\tilde{\mathbf{\Sigma}}=\tilde{\mathbf{A}} \tilde{\mathbf{A}}^{\top}.\] (1)

Note that we start with the assumption of \(\mathbf{A}\mathbf{A}^{\top}=\tilde{\mathbf{\Sigma}}\) in the formulation above and our identifiability result (e.g., in Theorem 1); such an assumption can be obtained, e.g., from the equality of Gaussian likelihoods in the large sample limit. This also inspires our sparsity-regularized likelihood-based estimation method that will be described in Section 4.2, which is more inline with, e.g., model selection approaches based on sparsity-regularized likelihood [36; 18].

In Section 3.1, we first examine various types of constraints arising from second-order statistics. We then present the main identifiability result in Section 3.2. We show that our assumptions are strictly weaker than existing sparsity assumptions in Section 3.3, and establish the connection between our identifiability theory with causal discovery in Section 3.4. All proofs are given in Appendices C and D.

### Semialgebraic Constraints Arising from Second-Order Statistics

We first discuss various notions related to constraints arising from covariance matrices of observed variables \(\mathbf{x}\), which serve as a fundamental basis for introducing our assumptions and identifiability result of ICA in Section 3.2. These notions have been studied in the field of algebraic statistics [16], factor analysis [15], graphical models [19; 40], and causality [14; 20].

We begin with the following definition on the set of covariance matrices entailed by \(\boldsymbol{\xi}\) for different values of the free parameters in \(\boldsymbol{\xi}\).

**Definition 2** (Covariance Set).: _The covariance set of support matrix \(\boldsymbol{\xi}\) is defined as_

\[\mathbf{\Sigma}(\boldsymbol{\xi})\coloneqq\{\mathbf{A}\mathbf{A}^{\top}: \mathbf{A}\in\mathbb{R}^{n\times n},\mathrm{supp}(\mathbf{A})\subseteq \mathrm{supp}(\boldsymbol{\xi}),\,\mathbf{A}\text{ is non-singular}\}.\]

The support \(\boldsymbol{\xi}\) of mixing matrix \(\mathbf{A}\) imposes certain constraints on the entries of the covariance matrix \(\mathbf{\Sigma}=\mathbf{A}\mathbf{A}^{\top}\), which, by Tarski-Seidenberg theorem (see [6]), correspond to _semialgebraic constraints_, i.e., equality and inequality constraints. The covariance set \(\mathbf{\Sigma}(\boldsymbol{\xi})\) is then said to be a _semialgebraic set_, i.e., a set that can be described with a finite number of polynomial equations and inequalities [6]. Clearly, if a covariance matrix \(\mathbf{\Sigma}\) belongs to the covariance set \(\mathbf{\Sigma}(\boldsymbol{\xi})\), then \(\mathbf{\Sigma}\) satisfies the semialgebraic constraints imposed by \(\boldsymbol{\xi}\).

For an equality constraint, the set of values satisfying the constraint has zero Lebesgue measure over the parameter space involved. Given a support matrix \(\boldsymbol{\xi}\), we denote by \(H(\boldsymbol{\xi})\) the set of equality constraints it imposes on the corresponding covariance matrices. On the other hand, the set of values satisfying an inequality constraint has nonzero Lebesgue measure. To illustrate these constraints, we provide a three-variable example below. Note that the example only serves as illustrations of the constraints; our estimation methods (in Section 4) do not require deriving them in practice.

**Example 1** (Semialgebraic Constraints).: _Consider support matrices_

\[\boldsymbol{\xi}_{1}=\begin{bmatrix}\times&0&0\\ \times&\times&0\\ \times&0&\times\end{bmatrix}\quad\text{ and }\quad\quad\boldsymbol{\xi}_{2}= \begin{bmatrix}\times&0&\times\\ \times&\times&0\\ 0&\times&\times\end{bmatrix}.\]

_The equality constraints imposed by \(\boldsymbol{\xi}_{1}\) include_

\[\Sigma_{1,1}\Sigma_{2,3}-\Sigma_{1,2}\Sigma_{1,3}=0,\]

_while the inequality constraints imposed by \(\boldsymbol{\xi}_{2}\) include_

\[(\Sigma_{1,1}\Sigma_{2,2}\Sigma_{3,3}+\Sigma_{1,1}\Sigma_{2,3}^{2}-\Sigma_{2,2 }\Sigma_{1,3}^{2}-\Sigma_{3,3}\Sigma_{1,2}^{2})^{2}-4(\Sigma_{1,1}\Sigma_{2,2}- \Sigma_{1,2}^{2})(\Sigma_{1,1}\Sigma_{3,3}\Sigma_{2,3}^{2}-\Sigma_{1,3}^{2} \Sigma_{2,3}^{2})\geq 0.\]

The detailed derivation can be found in Appendix D.1 and provides insights into how such constraints arise from the corresponding support matrices. In the example above, the covariance matrix \(\mathbf{\Sigma}\) generated by any mixing matrix \(\mathbf{A}\) with support \(\boldsymbol{\xi}_{1}\) must satisfy the corresponding equality constraint; similarly, the covariance matrix \(\mathbf{\Sigma}\) generated by any mixing matrix \(\mathbf{A}\) with support \(\boldsymbol{\xi}_{2}\) must satisfy the above inequality constraint. These constraints serve as footprints of the mixing matrix on the covariance matrix, and can be exploited for its identifiability, which we explain in the next section.

### Identifiability Result from Second-Order Statistics

In this section, we present our identifiability result of ICA from second-order statistics. The core idea is to introduce precise and mild assumptions on the connective structure from sources to observed variables. These assumptions facilitate the identification of the mixing matrix through the application of a sparsity constraint, formulated in Problem (1). To begin, we describe our primary assumption concerning the connective structure as follows.

**Assumption 1** (Structural Variability).: _Every pair of the columns in the support matrix of \(\mathbf{A}\) differ in more than one entry. That is, for every \(i,j\in[n]\) and \(i\neq j\), we have_

\[|\operatorname{supp}(\mathbf{a}_{i})\cup\operatorname{supp}(\mathbf{a}_{j})| -|\operatorname{supp}(\mathbf{a}_{i})\cap\operatorname{supp}(\mathbf{a}_{j})| >1.\]

The assumption above implies that every pair of sources should influence more than one different observed variable. Notably, in the field of nonlinear ICA with auxiliary variable, Hyvarinen and Morioka [22], Hyvarinen et al. [26] have adopted the assumption of _sufficient variability_ which requires that the auxiliary variable has a sufficiently diverse effect on the distributions of sources; specifically, the conditional distributions of the sources given the auxiliary variable must vary sufficiently. In contrast, our assumption of _structural variability_ requires that every pair of sources influence sufficiently diverse sets of observed variables, facilitating the disentanglement of each source.

We provide several examples in Appendix E.1 to illustrate the broad applicability of the assumption above. Furthermore, the following proposition justifies such an assumption because it is a necessary condition for identifiability via second-order statistics and sparsity. The intuition is that if Assumption 1 is violated, there exists a rotation that maps matrix \(\tilde{\mathbf{A}}\) to another matrix \(\tilde{\mathbf{A}}\) which has equal or smaller number of nonzero entries and is not a column permutation of \(\tilde{\mathbf{A}}\).

**Proposition 1**.: _If the true mixing matrix \(\tilde{\mathbf{A}}\) does not satisfy Assumption 1, then there exists a solution \(\hat{\mathbf{A}}\) to Problem (1) such that \(\tilde{\mathbf{A}}\not\sim\tilde{\mathbf{A}}\)._

**Remark 1** (Necessary Condition).: _Assumption 1 is a necessary condition for identifiability of ICA via second-order statistics and under sparsity constraint._

We also adopt the following assumption on the mixing matrix for the identifiability of ICA.

**Assumption 2** (Permutations to Lower Triangular Matrix).: _The matrix \(\mathbf{A}\) can be permuted by independent row and column permutations to be lower triangular. That is, there exist permutation matrices \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) such that \(\mathbf{P}_{1}^{\top}\mathbf{A}\mathbf{P}_{2}\) is lower triangular._

As we show in the proof of identifiability result in Theorem 1, Assumption 2, loosely speaking, ensures that the resulting covariance matrix does not contain "nontrivial" inequality constraints. In Example 1, support matrix \(\boldsymbol{\xi}_{1}\) satisfies Assumption 2 and leads to an equality constraint, while matrix \(\boldsymbol{\xi}_{2}\) fails to meet this assumption, resulting in an inequality constraint. The Lebesgue measure of the parameters leading to such inequality constraint is not zero, thus requiring additional assumptions to handle such cases. Therefore, we adopt Assumption 2 in this work and focus on equality constraints.

A key ingredient of our identifiability result based on sparsity is the dimension of the covariance set \(\boldsymbol{\Sigma}(\boldsymbol{\xi})\). It may be natural to expect that the dimension of \(\boldsymbol{\Sigma}(\boldsymbol{\xi})\), denoted as \(\dim(\boldsymbol{\Sigma}(\boldsymbol{\xi}))\), equals the number of parameters used to specify the mixing matrices, i.e., \(\|\boldsymbol{\xi}\|_{0}\). This is not the case for general mixing matrices, but we show that such property holds under Assumption 2.

**Proposition 2** (Dimension of Covariance Set).: _Let \(\boldsymbol{\xi}\) be a support matrix that satisfies Assumption 2. Then, its covariance set has a dimension of \(\|\boldsymbol{\xi}\|_{0}\), i.e., \(\dim(\boldsymbol{\Sigma}(\boldsymbol{\xi}))=\|\boldsymbol{\xi}\|_{0}\)._

Note that Assumption 2 allows independent row and column permutations, which thus may be rather mild especially for sparse mixing matrix. Below we provide an example of the connective structure that satisfies this assumption. We also introduce an efficient approach to verify whether a mixing matrix satisfies Assumption 2 in Appendix E.2.

**Example 2**.: _If the connective structure \(\mathcal{G}_{\mathbf{A}}\) of mixing matrix \(\mathbf{A}\) is a polytree, then matrix \(\mathbf{A}\) satisfies Assumption 2._

Finally, the following assumption is needed to ensure that the equality constraints arising from the covariance matrix are entailed by the true mixing matrix, rather than accidental parameter cancellations. This establishes a correspondence between equality constraints in the covariance matrix and those imposed by the support of the mixing matrix. Similar assumption has been employed in various tasks such as causal discovery [39, 20], as discussed in Section 3.4.

[MISSING_PAGE_FAIL:5]

### Connection with Causal Discovery

ICA has emerged as a useful tool for causal discovery over the past two decades [38]. In particular, Shimizu et al. [38] demonstrated that the identifiability of ICA based on non-Gaussianity can be leveraged to discover the complete structure of a linear non-Gaussian structural equation model (SEM). In this section, we establish the connection and provide an analogy between ICA and causal discovery from second-order statistics. This connection further bridges the gap between these two fields, and provides insights into the interpretation of our identifiability result.

Let \(\mathbb{R}_{\mathrm{off}}^{n\times n}\) be the set of matrices whose diagonal entries are zero, and \(\mathrm{diag}(\mathbb{R}_{>0}^{n})\) be the set of positive diagonal matrices. Consider the linear SEM \(\mathbf{x}=\tilde{\mathbf{B}}^{\top}\mathbf{x}+\mathbf{e}\), where \(\mathbf{x}\) denotes the random vector, \(\tilde{\mathbf{B}}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) denotes the weighted adjacency matrix representing a directed graph without self-loop, and \(\mathbf{e}\) is the independent noise vector with covariance matrix \(\tilde{\mathbf{\Omega}}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\). The graph is often assumed to be a directed acyclic graph (DAG); in this case, two DAGs are said to be _Markov equivalent_ if they share the same skeleton and v-structures [43], resulting in the same set of conditional independencies. Also, the inverse covariance matrix of \(\mathbf{x}\) is given by \(\tilde{\mathbf{\Theta}}=(\mathbf{I}-\tilde{\mathbf{B}})\tilde{\mathbf{ \Omega}}^{-1}(\mathbf{I}-\tilde{\mathbf{B}})^{\top}\). We refer readers to Spirtes et al. [39], Glymour et al. [21] for more details and a review of causal discovery.

Score-based method is a major class of causal discovery methods that optimizes a goodness-of-fit measure under a sparsity constraint [21], e.g., BIC [36]. In essence, score-based causal discovery from second-order statistics can often be formulated in the large sample limit as the following optimization problem (the commonly used acyclicity constraint is omitted here and will be clarified subsequently):

\[\min_{\begin{subarray}{c}\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}, \\ \mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\end{subarray}}\quad\| \mathbf{B}\|_{0}\quad\mathrm{subject\;to}\quad(\mathbf{I}-\mathbf{B})\mathbf{ \Omega}^{-1}(\mathbf{I}-\mathbf{B})^{\top}=\tilde{\mathbf{\Theta}}=(\mathbf{I }-\tilde{\mathbf{B}})\tilde{\mathbf{\Omega}}^{-1}(\mathbf{I}-\tilde{\mathbf{B }})^{\top}.\] (4)

By substituting \(\mathbf{A}:=(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) into the above formulation, we obtain the ICA formulation with second-order statistics and sparsity constraint introduced in Problem (1). To establish a precise connection between formulations (4) and (1), we present the following theorem which indicates that these formulations can be translated into each other.

**Theorem 4** (**Equivalent Formulations)**.: _Suppose \(\tilde{\mathbf{A}}=(\mathbf{I}-\tilde{\mathbf{B}})\tilde{\mathbf{\Omega}}^{- \frac{1}{2}}\). Then, we have:_

1. _Let_ \((\hat{\mathbf{B}},\hat{\mathbf{\Omega}})\) _be a solution to Problem (_4_). Then,_ \(\hat{\mathbf{A}}\coloneqq(\mathbf{I}-\hat{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}\) _is a solution to Problem (_1_)._
2. _Let_ \(\hat{\mathbf{A}}\) _be a solution to Problem (_1_). Then, there exist matrices_ \(\hat{\mathbf{B}}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) _and_ \(\hat{\mathbf{\Omega}}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) _such that_ \(\hat{\mathbf{A}}\sim(\mathbf{I}-\hat{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}\)_, and_ \((\hat{\mathbf{B}},\hat{\mathbf{\Omega}})\) _is a solution to Problem (_4_)._

Thus, the formulations of causal discovery and ICA via second-order statistics and sparsity constraint share inherent similarities. The key difference lies in their respective goals-the former aims to estimate the support of \(\tilde{\mathbf{B}}\) up to a Markov equivalence class [39], while the latter aims to estimate \(\tilde{\mathbf{A}}\) up to signed column permutation. The other difference is that \((\mathbf{I}-\tilde{\mathbf{B}})\tilde{\mathbf{\Omega}}^{-1}(\mathbf{I}-\tilde {\mathbf{B}})^{\top}\) represents the inverse covariance matrix \(\tilde{\mathbf{\Theta}}\) of \(\mathbf{x}\) in causal discovery, while \(\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}\) represents the covariance matrix \(\tilde{\mathbf{\Sigma}}\) of \(\mathbf{x}\) in ICA.

In addition to establishing the connection between formulations (4) and (1), we show that the assumptions we employ for identifiability of ICA, namely Assumptions 1, 2, and 3, are inherently related to causal discovery. Notably, Assumption 3 has been used in causal discovery [39, 20] to ensure that the conditional independencies in the distribution are entailed by the true directed graph. We now present a result that establishes the connection of Assumptions 1 and 2 with causal discovery.

**Theorem 5**.: _Suppose \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) for matrices \(\mathbf{A}\in\mathbb{R}^{n\times n}\), \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\), and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\). Then, \(\mathbf{A}\) satisfies Assumptions 1 and 2 if and only if \(\mathbf{B}\) represents a DAG whose Markov equivalence class is a singleton._

In causal discovery, it is rather common to assume that the true directed graph is acyclic and accordingly incorporate an acyclicity constraint to formulation (4). As indicated in Theorem 5 (and Proposition 10 in Appendix D.8), this acyclicity assumption corresponds to Assumption 2 in the context of ICA. Therefore, Theorem 4 can be straightforwardly extended to show the equivalence between formulations (2) and (4) with an additional acyclicity constraint on matrix \(\mathbf{B}\). Furthermore, it is worth noting that the mapping from mixing matrix \(\mathbf{A}\) satisfying Assumption 2 to a DAG is unique, which is straightforwardly implied by Shimizu et al. [38, Appendix A].

**Proposition 4** (Shimizu et al. [38]).: _Suppose matrix \(\mathbf{A}\) is non-singular and satisfies Assumption 2. Then, there exist unique matrices \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) such that \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\). Furthermore, matrix \(\mathbf{B}\) represents a DAG._

Moreover, as indicated in Theorem 5 (and Proposition 11 in Appendix D.8), Assumption 1 implies that the Markov equivalence class of \(\mathbf{B}\) is a singleton; in this case, the true DAG can be completely identified. In particular, the Markov equivalence class of DAG is a singleton when all edges are either part of a v-structure or required to be oriented to avoid forming new v-structures or cycles [31; 4].

## 4 Estimation Methods with Second-Order Statistics

Building upon the identifiability result provided in Section 3, we propose two estimation methods that leverage second-order statistics and sparsity. These methods involve solving a continuous constrained optimization problem, which we discuss in detail in this section. First, in Section 4.1, we introduce a novel approach to formulate the search space in Problem (2) that enables the application of continuous optimization techniques. We then describe the proposed estimation methods in Section 4.2. All proofs are provided in Appendix D.

### Characterization of Search Space

The key to achieving the identifiability result presented in Theorem (1) lies in the optimization problem (2), where the search space involves the matrices \(\mathbf{A}\) that satisfy Assumption 2. Consequently, a crucial question arises: is there an efficient approach for exploring the space of matrices \(\mathbf{A}\) that satisfy Assumption 2? Inspired by Zheng et al. [48], Wei et al. [45], Zhang et al. [47], we introduce the following function to characterize the search space:

\[g(\mathbf{A})=\mathrm{tr}\left(\sum_{k=2}^{n}(\mathrm{off}(\mathbf{A})\odot \mathrm{off}(\mathbf{A}))^{k}\right),\ \ \text{where}\ \ (\mathrm{off}(\mathbf{A}))_{i,j}=\begin{cases}0,&\text{if }i=j,\\ a_{i,j},&\text{otherwise}.\end{cases}\]

Here, symbol \(\odot\) denotes the Hadamard product. We then provide the following lemma that establishes the relationship between function \(g(\mathbf{A})\) and a specific type of permutation, namely simultaneous equal row and column permutation.

**Lemma 1**.: _For any matrix \(\mathbf{A}\), \(g(\mathbf{A})=0\) if and only if it can be permuted via simultaneous equal row and column permutations to be lower triangular._

Intuitively speaking, if we interpret matrix \(\mathbf{A}\) as a weighted adjacency matrix of a directed graph, say \(\mathcal{G}\), then \(\mathrm{tr}((\mathrm{off}(\mathbf{A})\odot\mathrm{off}(\mathbf{A}))^{k})\) counts the number of length-\(k\) weighted closed walks in \(\mathcal{G}\) excluding the self-loops. Therefore, \(g(\mathbf{A})\) counts the total number of weighted closed walks in \(\mathcal{G}\) without including self-loops. \(g(\mathbf{A})=0\) then implies that \(\mathcal{G}\) does not contain any cycle longer than one (i.e., it may contain self-loops). It is known that a directed graph is acyclic if and only if its weighted adjacency matrix can be permuted via simultaneous equal row and column permutations to be _strictly_ lower triangular. In our case, \(\mathcal{G}\) may contain self-loops, and thus can be permuted to a lower triangular form. This distinction elucidates the difference between our characterization and that introduced by Zheng et al. [48], Zhang et al. [47], i.e., their characterization focuses on matrices that are strictly lower triangular, while ours focuses on lower triangular matrices.

The following proposition sheds light on the connection between \(g(\mathbf{A})\) and Assumption 2.

**Proposition 5**.: _The matrix \(\mathbf{A}\) satisfies Assumption 2 if and only if there is a matrix \(\hat{\mathbf{A}}\) such that it is a column permutation of \(\mathbf{A}\) and that \(g(\hat{\mathbf{A}})=0\)._

The above proposition indicates that the search for matrices \(\mathbf{A}\) satisfying Assumption 2 can be effectively conducted by considering the constraint \(g(\mathbf{A})=0\). Accordingly, we establish an alternative formulation of the identifiability result presented in Section 3.2. In the following section, we will introduce efficient approaches for solving Problem (5).

**Theorem 6** (Alternative Formulation of Identifiability).: _Suppose that the true mixing matrix \(\tilde{\mathbf{A}}\) satisfies Assumptions 1, 2, and 3. Let \(\hat{\mathbf{A}}\) be a solution of the following problem:_

\[\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}\|\mathbf{A}\|_{0}\quad\mathrm{ subject}\ \mathrm{to}\quad\mathbf{A}\mathbf{A}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{ \top}\quad\mathrm{and}\quad g(\mathbf{A})=0.\] (5)

_Then, we have \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\)._```
0: initial penalty coefficient \(c_{1}>0\); multiplicative factor \(\beta>1\); maximum number of iterations \(k_{\max}>0\); tolerance \(\epsilon_{1},\epsilon_{2}>0\); initial solution \(\mathbf{A}_{0}\); empirical covariance matrix \(\tilde{\mathbf{\Sigma}}\)
1:for\(k=1,2,\ldots,k_{\max}\)do
2: Solve \(\mathbf{A}_{k}\coloneqq\arg\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}\rho( \mathbf{A})+\frac{c_{k}}{2}\|\mathbf{A}\mathbf{A}^{\top}-\tilde{\mathbf{\Sigma} }\|_{F}^{2}+\frac{c_{k}}{2}g(\mathbf{A})^{2}\) initialized at \(\mathbf{A}_{k-1}\)
3:if\(\|\mathbf{A}_{k}\mathbf{A}_{k}^{\top}-\tilde{\mathbf{\Sigma}}\|_{F}^{2}< \epsilon_{1}\) and \(g(\mathbf{A}_{k})<\epsilon_{2}\)then break
4: Update penalty coefficient \(c_{k+1}\coloneqq\beta c_{k}\)
5:Output solution \(\mathbf{A}_{k}\) ```

**Algorithm 1** Decomposition-Based Method

### Estimation Methods

Based on the identifiability results of Theorems 1 and 6, we propose two estimation methods, called _SparseICA_, to perform ICA from second-order statistics that leverage sparsity regularization and continuous constrained optimization. To proceed, we define \(\tilde{\mathbf{\Sigma}}\) as the empirical covariance matrix of observed variables \(\mathbf{x}\) and \(T\) as the sample size.

Decomposition-based method.Given the formulation in Eq. (5), we consider the following constrained optimization problem

\[\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}\rho(\mathbf{A})\quad\mathrm{ subject\ to}\quad\mathbf{A}\mathbf{A}^{\top}-\tilde{\mathbf{\Sigma}}=\mathbf{0} \quad\mathrm{and}\quad g(\mathbf{A})=0,\] (6)

where \(\rho(\mathbf{A})\) is a suitable sparsity regularizer, often expressible as \(\rho(\mathbf{A})=\sum_{i,j}\rho(a_{i,j})\). Formulation (5) indicates that one should apply the \(\ell_{0}\) regularizer \(\rho(\mathbf{A})=\|\mathbf{A}\|_{0}\). Alternatively, other possible choices include the \(\ell_{1}\) regularizer \(\rho(\mathbf{A})=\|\mathbf{A}\|_{1}\) that supports continuous optimization. Further details regarding our specific choice of sparsity regularizer will be elaborated later in this section. On the other hand, we simply use the empirical covariance matrix \(\tilde{\mathbf{\Sigma}}\) as an estimate of the true covariance matrix \(\tilde{\mathbf{\Sigma}}\), which is found to work well across different sample sizes in our experiments. One may also adopt a regularized estimator of the form \(\tilde{\mathbf{\Sigma}}+\eta\mathbf{I}\) with a proper choice of \(\eta\), which may have notable advantage in certain cases [13; 28; 37].

Likelihood-based method.In addition to the decomposition-based method above, we introduce a likelihood-based estimation method formulated by the following constrained optimization problem:

\[\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}L(\mathbf{A};\tilde{ \mathbf{\Sigma}})+\rho(\mathbf{A})\quad\mathrm{subject\ to}\quad g(\mathbf{A} )=0,\] (7) \[\text{where}\quad L(\mathbf{A};\tilde{\mathbf{\Sigma}})=\frac{T}{ 2}\operatorname{tr}((\mathbf{A}^{\top}\mathbf{A})^{-1}\tilde{\mathbf{\Sigma}})+ T\log|\det\mathbf{A}|\]

is the negative Gaussian log-likelihood function and \(\rho(\mathbf{A})\) is a sparsity regularizer. The following result establishes the theoretical guarantee of this likelihood-based method.

**Theorem 7** (Likelihood-Based Method).: _Suppose that the true mixing matrix \(\tilde{\mathbf{A}}\) satisfies Assumptions 1, 2, and 3. Let \(\hat{\mathbf{A}}\) be a solution of Problem (7) with sparsity regularizer \(\rho(\mathbf{A})=0.5\|\mathbf{A}\|_{0}\log T\). Then, we have \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\) in the large sample limit._

Implementation.Based on Theorems 6 and 7, ideally one should adopt the \(\ell_{0}\) regularizer \(\rho(\mathbf{A})=\lambda\|\mathbf{A}\|_{0}\) and develop an exact discrete search procedure over the support space of matrix \(\mathbf{A}\). However, such approach may pose computational challenges in practice. Since the functions \(L(\mathbf{A};\tilde{\mathbf{\Sigma}})\) and \(g(\mathbf{A})\) are differentiable, in this work we develop an estimation procedure that leverages efficient continuous optimization techniques. Therefore, some possible choices for \(\rho(\mathbf{A})\) are \(\ell_{1}\), smoothly clipped absolute deviation (SCAD) [17], and minimax concave penalty (MCP) [46] regularizers. The \(\ell_{1}\) regularizer has been shown to exhibit bias during estimation [17; 10], especially for large coefficients. Here, we adopt the MCP regularizer that is less susceptible to such issue, given by

\[\rho(a_{i,j})=\begin{cases}\lambda|a_{i,j}|-\frac{a_{i,j}^{2}}{2\alpha},&\text {if }|a_{i,j}|\leq\alpha\lambda,\\ \frac{\alpha\lambda^{2}}{2},&\text{otherwise},\end{cases}\]

where \(\lambda\) and \(\alpha\) are hyperparameters.

To solve Eqs. (6) and (7), standard constrained optimization methods can be used, such as quadratic penalty method, augmented Lagrangian method, and barrier method [7; 8; 33]. In this work, we adopt the quadratic penalty method that converts each constrained problem into a sequence of unconstrained optimization problems where the constraint violations are increasingly penalized. We describe the full procedure of the decomposition-based and likelihood-based methods based on quadratic penalty method in Algorithms 1 and 2, respectively. The unconstrained problem in each iteration can be solved using different continuous optimization solvers, including first-order methods such as gradient descent and steepest descent, as well as second-order methods such as quasi-Newton methods. In our experiments presented in the subsequent section, we employ L-BFGS [11], a quasi-Newton method, to solve the unconstrained optimization problem.

It is worth noting that the formulations in Eqs. (6) and (7) involve solving nonconvex optimization problems; in practice, the optimization procedure may return stationary points that correspond to suboptimal local solutions. Therefore, we run the method for a number of times and choose the final mixing matrix via model selection. Further details regarding the optimization procedure and implementation are provided in Appendix F.

## 5 Experiments

To empirically validate our proposed identifiability results, we carry out experiments under various settings. We also conduct ablation studies to verify the necessity of the proposed assumptions and include _FastICA_[23] as a representative baseline. Specifically, we consider the following methods:

* _SparseICA_: Decomposition-based (Eq. (6)) or likelihood-based (Eq. (7)) method on data where both Assumptions 1 and 2 hold;
* _Vanilla_: Decomposition-based (Eq. (6)) or likelihood-based (Eq. (7)) method without the constraint \(g(\mathbf{A})=0\), on data where neither Assumption 1 nor Assumption 2 holds;
* _FastICA-D_: FastICA on data where both Assumptions 1 and 2 hold;
* _FastICA_: FastICA on data where neither Assumption 1 nor Assumption 2 holds.

For all experiments, we simulate \(10\) sources, and generate the supports of the true mixing matrices \(\tilde{\mathbf{A}}\) according to the assumptions required by each method above. The nonzero entries of \(\tilde{\mathbf{A}}\) are sampled uniformly at random from \([-0.8,-0.2]\cup[0.2,0.8]\). We use mean correlation coefficient (MCC) and Amari distance [3] as evaluation metrics, where all results are reported for 10 random trials.

**Different sample sizes.** We first consider entirely Gaussian sources and different sample sizes. The empirical results of MCC are shown in Figure 1, while those of Amari distance are given in Figure 4 in Appendix G. By comparing _SparseICA_ with _Vanilla_ and _FastICA_, it is evident that the identification performance is much better across different sample sizes when the required assumptions on the connective structure are satisfied, as validated by Wilcoxon signed-rank test at \(5\%\) significance level. Furthermore, the unsatisfactory results of _FastICA-D_ indicate that our estimation methods are also essential for ensuring the quality of the identification, which further validates the proposed identifiability theory. Since _FastICA-D_ performs similarly to _FastICA_, it suggests that the data-generating process, while meeting our assumption, may not be inherently simpler to recover without considering specific procedure to handle Gaussian sources. In addition, as expected, the performance of SparseICA improves in terms of both MCC and Amari distance as the sample size increases.

**Different ratios of Gaussian sources.** We now conduct empirical study to investigate the performance in the presence of Gaussian and non-Gaussian sources. Here, the non-Gaussian sources follow exponential distributions. We consider different ratios of Gaussian sources, which are specifically \(0\), \(0.2\), \(0.4\), \(0.6\), \(0.8\), and \(1\). For instance, ratio of \(0.6\) indicates that there are \(6\) Gaussian sourcesand \(4\) non-Gaussian sources. The empirical results of MCC based on \(1000\) samples are depicted in Figure 1, while those of Amari distance are provided in Figure 4 in Appendix G. One observes that the identification performance _SparseICA_ is rather stable across different ratios of Gaussian sources, which may not be surprising as it leverages only second-order statistics. On the other hand, the performance of _FastICA-D_ and _FastICA_ deteriorates as the ratio of Gaussian sources increases, because it relies on non-Gaussianity of the sources. It is also observed that, in the presence of Gaussian sources, _FastICA-D_ and _FastICA_ may perform well, provided that the ratio (or number) of Gaussian sources is not large. This suggests a potential future direction to integrate our method based on second-order statistics with existing methods that rely on non-Gaussianity, which may better handle both Gaussian and non-Gaussian sources.

## 6 Conclusion

We develop an identifiability theory of ICA from second-order statistics without relying on non-Gaussianity. Specifically, we introduce novel and precise assumptions on the connective structure from sources and observed variables, and show that our proposed assumption of structural variability is strictly weaker than the previous ones. Importantly, we prove that this assumption is one of the necessary conditions for achieving identifiability in the investigated setting. We further propose two estimation methods based on second-order statistics that leverage sparsity regularization. Moreover, we establish a precise connection between our identifiability result of ICA and causal discovery from second-order statistics, which may open up avenues for exploring the interplay between ICA and causal discovery with linear Gaussian SEM. Our theoretical claims have also been empirically validated across different settings. The limitations include the lack of finite sample analysis and broader application of our theory in more real-world tasks, which are worth exploring in future work.

Figure 1: Empirical results of MCC across different sample sizes. Error bars indicate the standard errors calculated based on \(10\) random trials.

Figure 2: Empirical results of MCC across different ratios of Gaussian sources. Error bars indicate the standard errors calculated based on \(10\) random trials.

## Acknowledgments

The authors would like to thank the anonymous reviewers for helpful comments and suggestions. This project is partially supported by NSF Grant 2229881, the National Institutes of Health (NIH) under Contract R01HL159805, a grant from Apple Inc., a grant from KDDI Research Inc., and generous gifts from Salesforce Inc., Microsoft Research, and Amazon Research.

## References

* [1] N. Abrahamsen and P. Rigollet. Sparse gaussian ICA. _arXiv preprint arXiv:1804.00408_, 2018.
* [2] L. Albera, A. Ferreol, P. Chevalier, and P. Comon. ICAR: a tool for blind source separation using fourth-order statistics only. _IEEE Transactions on Signal Processing_, 53(10):3633-3643, 2005.
* [3] S.-i. Amari, A. Cichocki, and H. Yang. A new learning algorithm for blind signal separation. In _Advances in Neural Information Processing Systems_, 1995.
* [4] S. A. Andersson, D. Madigan, and M. D. Perlman. A characterization of Markov equivalence classes for acyclic digraphs. _The Annals of Statistics_, 25(2):505-541, 1997.
* [5] A. Belouchrani, K. Abed-Meraim, J.-F. Cardoso, and E. Moulines. A blind source separation technique using second-order statistics. _IEEE Transactions on signal processing_, 45(2):434-444, 1997.
* [6] R. Benedetti and J.-J. Risler. _Real algebraic and semi-algebraic sets_. Actualites mathematiques. Hermann, Paris, 1990.
* [7] D. P. Bertsekas. _Constrained Optimization and Lagrange Multiplier Methods_. Academic Press, 1982.
* [8] D. P. Bertsekas. _Nonlinear Programming_. Athena Scientific, 2nd edition, 1999.
* [9] A. Biton, I. Bernard-Pierrot, Y. Lou, C. Krucker, E. Chapeaublanc, C. Rubio-Perez, N. Lopez-Bigas, A. Kamoun, Y. Neuzillet, P. Gestraud, L. Grieco, S. Rebouissou, A. de Reynies, S. Benhamou, T. Lebret, J. Southgate, E. Barillot, Y. Allory, A. Zinovyev, and F. Radvanyi. Independent component analysis uncovers the landscape of the bladder tumor transcriptome and reveals insights into luminal and basal subtypes. _Cell Reports_, 9(4):1235-1245, Nov 2014.
* [10] P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. _The Annals of Applied Statistics_, 5(1):232-253, 2011.
* [11] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. _SIAM Journal on Scientific Computing_, 16(5):1190-1208, 1995.
* [12] P. Comon. Independent component analysis, a new concept? _Signal processing_, 36(3):287-314, 1994.
* [13] M. J. Daniels and R. E. Kass. Shrinkage estimators for covariance matrices. _Biometrics_, 57(4):1173-1184, 2001.
* [14] M. Drton. Algebraic problems in structural equation modeling. _Advanced Studies in Pure Mathematics_, 77, 2018.
* [15] M. Drton, B. Sturmfels, and S. Sullivant. Algebraic factor analysis: Tetrads, pentads and beyond. _Probability Theory and Related Fields_, 138, 09 2005.
* [16] M. Drton, B. Sturmfels, and S. Sullivant. _Lectures on Algebraic Statistics_, volume 39 of _Oberwolfach Seminars_. Springer, 2009.
* [17] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. _Journal of the American statistical Association_, 96(456):1348-1360, 2001.

* [18] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical Lasso. _Biostatistics_, 9:432-41, 2008.
* [19] D. Geiger, D. Heckerman, H. King, and C. Meek. Stratified exponential families: Graphical models and model selection. _The Annals of Statistics_, 29(2):505-529, 2001.
* [20] A. Ghassami, A. Yang, N. Kiyavash, and K. Zhang. Characterizing distribution equivalence and structure learning for cyclic and acyclic directed graphs. In _International Conference on Machine Learning_, 2020.
* [21] C. Glymour, K. Zhang, and P. Spirtes. Review of causal discovery methods based on graphical models. _Frontiers in Genetics_, 10, 2019.
* [22] A. Hyvarinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. _Advances in Neural Information Processing Systems_, 2016.
* [23] A. Hyvarinen and E. Oja. A fast fixed-point algorithm for independent component analysis. _Neural Computation_, 9(7):1483-1492, 1997.
* [24] A. Hyvarinen and E. Oja. Independent component analysis: algorithms and applications. _Neural networks_, 13(4-5):411-430, 2000.
* [25] A. Hyvarinen, J. Karhunen, and E. Oja. _Independent Component Analysis_. John Wiley & Sons, Inc, 2001.
* [26] A. Hyvarinen, H. Sasaki, and R. Turner. Nonlinear ICA using auxiliary variables and generalized contrastive learning. In _International Conference on Artificial Intelligence and Statistics_, 2019.
* [27] T.-P. Jung, S. Makeig, M. McKeown, A. Bell, T.-W. Lee, and T. Sejnowski. Imaging brain dynamics using independent component analysis. _Proceedings of the IEEE_, 89(7):1107-1122, 2001.
* [28] O. Ledoit and M. Wolf. A well-conditioned estimator for large-dimensional covariance matrices. _Journal of Multivariate Analysis_, 88(2):365-411, 2004.
* [29] M. Lynch, B. Walsh, et al. _Genetics and analysis of quantitative traits_, volume 1. Sinauer Sunderland, MA, 1998.
* [30] K. Matsuoka, M. Ohoya, and M. Kawamoto. A neural net for blind separation of nonstationary signals. _Neural Networks_, 8:411-419, 1995.
* [31] C. Meek. Causal inference and causal explanation with background knowledge. In _Conference on Uncertainty in Artificial Intelligence_, 1995.
* [32] N. Moulin, F. Gresselin, B. Dardaillon, and Z. Thomas. River temperature analysis with a new way of using independant component analysis. _Frontiers in Earth Science_, 10, 12 2022.
* [33] J. Nocedal and S. J. Wright. _Numerical optimization_. Springer series in operations research and financial engineering. Springer, 2nd edition, 2006.
* [34] H. W. Ott. _Noise Reduction Techniques in Electronic Systems_. New York : Wiley, 1988.
* [35] D.-T. Pham and J.-F. Cardoso. Blind separation of instantaneous mixtures of nonstationary sources. _IEEE Transactions on signal processing_, 49(9):1837-1848, 2001.
* [36] G. Schwarz. Estimating the dimension of a model. _The Annals of Statistics_, 6(2):461-464, 1978.
* [37] J. Schafer and K. Strimmer. A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics. _Statistical applications in genetics and molecular biology_, 4, 02 2005.
* [38] S. Shimizu, P. O. Hoyer, A. Hyvarinen, and A. Kerminen. A linear non-Gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7(Oct):2003-2030, 2006.

* [39] P. Spirtes, C. Glymour, and R. Scheines. _Causation, Prediction, and Search_. MIT press, 2nd edition, 2001.
* [40] S. Sullivant. Algebraic geometry of gaussian Bayesian networks. _Advances in Applied Mathematics_, 40(4):482-513, 2008. ISSN 0196-8858.
* [41] A. E. Teschendorff, M. Journee, P. A. Absil, R. Sepulchre, and C. Caldas. Elucidating the altered transcriptional programs in breast cancer using independent component analysis. _PLoS Comput. Biol._, 3(8):1539-1554, 2007.
* [42] L. N. Trefethen and D. Bau. _Numerical linear algebra_, volume 50. SIAM, 1997.
* [43] T. Verma and J. Pearl. Equivalence and synthesis of causal models. In _Conference on Uncertainty in Artificial Intelligence_, 1990.
* [44] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. Jarrod Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. Carey, I. Polat, Y. Feng, E. W. Moore, J. Vand erPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and S... Contributors. SciPy 1.0: Fundamental algorithms for scientific computing in Python. _Nature Methods_, 17:261-272, 2020.
* [45] D. Wei, T. Gao, and Y. Yu. DAGs with no fears: A closer look at continuous optimization for learning Bayesian networks. In _Advances in Neural Information Processing Systems_, 2020.
* [46] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. _The Annals of Statistics_, 38(2):894-942, 2010.
* [47] Z. Zhang, I. Ng, D. Gong, Y. Liu, E. M. Abbasnejad, M. Gong, K. Zhang, and J. Q. Shi. Truncated matrix power iteration for differentiable DAG learning. In _Advances in Neural Information Processing Systems_, 2022.
* [48] X. Zheng, B. Aragam, P. Ravikumar, and E. P. Xing. DAGs with NO TEARS: Continuous optimization for structure learning. In _Advances in Neural Information Processing Systems_, 2018.
* [49] Y. Zheng, I. Ng, and K. Zhang. On the identifiability of nonlinear ICA: Sparsity and beyond. In _Advances in Neural Information Processing Systems_, 2022.

## Appendix A Preliminaries on Support Rotation

Before presenting the proofs of the theoretical results, we review the definition of support rotation and its potential effects on a support matrix, given by Ghassami et al. [20]. Note that the definition and remark below are quoted from Ghassami et al. [20, Section 3.1], with only minor modifications.

**Definition 3** (Support Rotation [20]).: _The support rotation, denoted as \(R(i,j,k)\), is a transformation that modifies a support matrix \(\boldsymbol{\xi}\) by applying a Givens rotation in the \((j,k)\) plane, setting the element \(\xi_{i,j}\) to zero. The outcome of is the support matrix of \(\mathbf{Q}G\left(j,k,\tan^{-1}\left(-q_{i,j}/q_{i,k}\right)\right)\), where \(\mathbf{Q}\in\arg\max_{\mathbf{Q}^{\prime}}\left|\operatorname{supp}\left( \mathbf{Q}^{\prime}G\left(j,k,\tan^{-1}\left(-q_{i,j}^{\prime}/q_{i,k}^{ \prime}\right)\right)\right)\right|\) such that the support matrix of \(\mathbf{Q}^{\prime}\) is \(\boldsymbol{\xi}\).__Note that \(G\left(j,k,\tan^{-1}\left(-q^{\prime}_{ij}/q^{\prime}_{i,k}\right)\right)\) is the Givens rotation in the \((j,k)\) plane that makes the \(q^{\prime}_{i,j}\) entry zero._

**Remark 3** (Ghassami et al. [20]).: _The effects of applying a support rotation \(R(i,j,k)\) can be categorized into the following four cases:_

* _Reduction:_ _If_ \(\xi_{i,j}=\xi_{i,k}=\times\) _and_ \(\xi_{l,j}=\xi_{l,k}\) _for all_ \(l\in[p]\backslash\{i\}\)_, then only_ \(\xi_{i,j}\) _becomes zero._
* _Reversible acute rotation:_ _If_ \(\xi_{i,j}=\xi_{i,k}=\times\) _and there exists a row_ \(i^{\prime}\) _such that the_ \(j\)_-th and_ \(k\)_-th columns differ only in that row, then_ \(\xi_{i,j}\) _becomes zero and both_ \(\xi_{i^{\prime},j}\) _and_ \(\xi_{i^{\prime},k}\) _become_ \(\times\)_._
* _Irreversible acute rotation:_ _If_ \(\xi_{i,j}=\xi_{i,k}=\times\) _and the_ \(j\)_-th and_ \(k\)_-th columns differ in at least two rows, then_ \(\xi_{i,j}\) _becomes zero and all entries on the_ \(j\)_-th and_ \(k\)_-th columns become_ \(\times\) _on the rows on which they differed._
* _Column swap:_ _If_ \(\xi_{i,j}=\times\) _and_ \(\xi_{i,k}=0\)_, then columns_ \(j\) _and_ \(k\) _are swapped._

## Appendix B Proofs of Useful Lemmas

We provide several lemmas that will be useful for subsequent proofs.

### Proof of Lemma 2

**Lemma 2** (Nonzero Diagonal Entries).: _For any non-singular matrix \(\mathbf{A}\), there exists a permutation matrix \(\mathbf{P}\) such that the diagonal entries of \(\mathbf{AP}\) are nonzero._

Proof.: We prove it by contradiction. Suppose that there always exists a zero diagonal entry in every column permutation.

We first represent the determinant of the matrix \(\mathbf{A}\) as its Leibniz formula:

\[\det(\mathbf{A})=\sum_{\sigma\in\mathcal{S}_{n}}\left(\operatorname{sgn}( \sigma)\prod_{i=1}^{n}a_{i,\sigma(i)}\right),\]

where \(\mathcal{S}_{n}\) is the set of \(n\)-permutations. Because for every permutation, there always exists a diagonal entry with value zero, we have

\[\prod_{i=1}^{n}a_{i,\sigma(i)}=0,\quad\forall\sigma\in\mathcal{S}_{n}.\]

Thus, it follows that \(\det(\mathbf{A})=0\). This indicates that the matrix \(\mathbf{A}\) is singular, which leads to a contradiction. 

### Proof of Lemma 3

**Lemma 3** (Non-Singular Solution).: _Let \(\tilde{\mathbf{A}}\) be a non-singular matrix. Suppose \(\hat{\mathbf{A}}\) is a matrix such that \(\hat{\mathbf{A}}\tilde{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A} }^{\top}\). Then, \(\hat{\mathbf{A}}\) is non-singular._

Proof.: A matrix is non-singular (or invertible) if and only if its determinant is nonzero. Therefore, we need to show that \(\det(\tilde{\mathbf{A}})\neq 0\).

From the given, we have \(\hat{\mathbf{A}}\hat{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A} }^{\top}\). Taking determinants on both sides, we get

\[\det(\hat{\mathbf{A}}\hat{\mathbf{A}}^{\top})=\det(\tilde{\mathbf{A}}\tilde{ \mathbf{A}}^{\top}).\]

Since \(\hat{\mathbf{A}}\) and \(\tilde{\mathbf{A}}\) are square matrices, we can simplify both sides:

\[\det(\hat{\mathbf{A}})^{2}=\det(\tilde{\mathbf{A}})^{2}.\]

Since \(\tilde{\mathbf{A}}\) is non-singular, we know that \(\det(\tilde{\mathbf{A}})\neq 0\). Therefore, \(\det(\hat{\mathbf{A}})^{2}\neq 0\). This implies that \(\det(\hat{\mathbf{A}})\neq 0\). Thus, \(\hat{\mathbf{A}}\) is non-singular.

### Proof of Lemma 5

**Lemma 4**.: _Let \(\mathbf{A}\) be a matrix with all diagonal entries being nonzero. Then, there exists matrix \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) such that \(\mathbf{A}=(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\mathbf{D}\), where \(\mathbf{D}\) is a diagonal matrix with diagonal entries being \(\pm 1\)._

Proof.: Since the diagonal entries of \(\mathbf{A}\) are nonzero, we can construct a diagonal matrix \(\mathbf{D}\) such that the diagonal entries of \(\mathbf{A}\mathbf{D}\) are positive, by defining \(d_{i,i}=1\) if \(a_{i,i}>0\) and \(d_{i,i}=-1\) if \(a_{i,i}<0\). Let \(\mathbf{\Omega}\) be a diagonal matrix of the same size as matrix \(\mathbf{A}\), where \((\mathbf{\Omega})_{i,i}=1/(\mathbf{A}\mathbf{D})_{i,i}^{2}\), Also, let \(\mathbf{B}\coloneqq\mathbf{I}-\mathbf{A}\mathbf{D}\mathbf{\Omega}^{\frac{1}{2}}\). Since the diagonal entries of matrix \(\mathbf{A}\mathbf{D}\mathbf{\Omega}^{\frac{1}{2}}\) are ones, the diagonal entries of matrix \(\mathbf{B}\) are zeros. Therefore, we have \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\), \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\), and \(\mathbf{A}=(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\mathbf{D}\), where \(\mathbf{D}\) is a diagonal matrix with entries being \(\pm 1\). 

**Lemma 5**.: _Let \(\mathbf{A}\) be a non-singular matrix. Then, there exists matrix \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) such that \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\)._

Proof.: Since matrix \(\mathbf{A}\) is non-singular, by Lemma 2, there exists a permutation matrix \(\mathbf{P}\) such that the diagonal entries of \(\mathbf{A}\mathbf{P}\) are nonzero. By Lemma 4, there exists matrix \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) such that \(\mathbf{A}\mathbf{P}=(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}} \mathbf{D}\), where \(\mathbf{D}\) is a diagonal matrix with diagonal entries being \(\pm 1\). Clearly, we have \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\). 

### Proof of Lemma 6

**Lemma 6**.: _Suppose \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) for matrices \(\mathbf{A}\in\mathbb{R}^{n\times n}\), \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\), and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\). Then, we have \(\|\mathbf{A}\|_{0}=\|\mathbf{B}\|_{0}+n\)._

Proof.: First notice that the diagonal entries of \(\mathbf{B}\) are zeros, which implies

\[\|\mathbf{I}-\mathbf{B}\|_{0}=\|\mathbf{I}\|_{0}+\|\mathbf{B}\|_{0}=\| \mathbf{B}\|_{0}+n.\]

Since \(\mathbf{\Omega}^{-\frac{1}{2}}\) is a diagonal matrix, right multiplication of \(\mathbf{\Omega}^{-\frac{1}{2}}\) amounts to rescaling the columns of \(\mathbf{I}-\mathbf{B}\), and does not affect its support. Therefore, we have

\[\|(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\|_{0}=\| \mathbf{I}-\mathbf{B}\|_{0}=\|\mathbf{B}\|_{0}+n.\]

Since \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\), matrices \(\mathbf{A}\) and \((\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) differ only in signed column permutations. Therefore, their number of nonzero entries are the same, i.e.,

\[\|\mathbf{A}\|_{0}=\|\mathbf{B}\|_{0}+n.\qed\]

### Proof of Lemma 8

We first provide the following lemma that will be used to prove Lemma 8.

**Lemma 7**.: _Let \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) be two permutation matrices. Then, \(\mathbf{P}_{1}^{\top}\mathbf{P}_{2}\) is lower triangular if and only if \(\mathbf{P}_{1}=\mathbf{P}_{2}\)._

Proof.: The "if part" is clear because permutation matrices are orthogonal matrices, and thus \(\mathbf{P}_{1}^{\top}\mathbf{P}_{1}=\mathbf{I}\) is lower triangular. It remains to prove the "only if part". Since \(\mathbf{P}_{1}^{\top}\mathbf{P}_{2}\) is also a permutation matrix, this matrix being lower triangular implies that it is an identity matrix, because identity matrix is the only permutation matrix that is lower triangular. We then have \(\mathbf{P}_{1}^{\top}\mathbf{P}_{2}=\mathbf{I}\), which implies

\[\mathbf{P}_{1}=\mathbf{P}_{2}^{-\top}=\mathbf{P}_{2}.\qed\]

We now provide the proof of Lemma 8.

**Lemma 8**.: _Given matrix \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\), if \(\mathbf{I}-\mathbf{B}\) satisfies Assumption 2, then \(\mathbf{I}-\mathbf{B}\) is non-singular._

Proof.: Since \(\mathbf{I}-\mathbf{B}\) satisfies Assumption 2, there exist permutation matrices \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) such that

\[\mathbf{P}_{1}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{P}_{2}=\mathbf{P}_{1}^{ \top}\mathbf{P}_{2}-\mathbf{P}_{1}^{\top}\mathbf{B}\mathbf{P}_{2}\]

is lower triangular. For all \(i,j\in[n]\) such that \((\mathbf{P}_{1}^{\top}\mathbf{P}_{2})_{i,j}\neq 0\), we have \((\mathbf{P}_{1}^{\top}\mathbf{B}\mathbf{P}_{2})_{i,j}=0\), which implies that \(\mathbf{P}_{1}^{\triangular. By Lemma 7, we then have \(\mathbf{P}_{1}=\mathbf{P}_{2}\). This indicates that \(\mathbf{I}-\mathbf{P}_{1}^{\top}\mathbf{B}\mathbf{P}_{1}\) is lower triangular, with diagonal entries equal to one. Therefore, we have

\[\det(\mathbf{I}-\mathbf{P}_{1}^{\top}\mathbf{B}\mathbf{P}_{1})=1,\]

which implies

\[\det(\mathbf{I}-\mathbf{B})=\det(\mathbf{P}_{1}(\mathbf{I}-\mathbf{B})\mathbf{ P}_{1}^{\top})=\det(\mathbf{I}-\mathbf{P}_{1}^{\top}\mathbf{B}\mathbf{P}_{1})=1.\]

Since the determinant of \(\mathbf{I}-\mathbf{B}\) is nonzero, it is non-singular. 

## Appendix C Proof of Theorem 1

We first describe the notion of covariance equivalence that is needed for the proof of Theorem 1. Specifically, if two support matrices \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) entail the same set of covariance matrices, i.e., \(\bm{\Sigma}(\bm{\xi}_{1})=\bm{\Sigma}(\bm{\xi}_{2})\), they are said to be _covariance equivalent_. This means that for any combination of parameter values in \(\bm{\xi}_{1}\), there exists a corresponding set of parameter values in \(\bm{\xi}_{2}\) that leads to the same covariance matrix, and vice versa. Furthermore, two support matrices are covariance equivalent if and only if they lead to the same set of semialgebraic constraints on the covariance matrices.

We now give the proof of Theorem 1. The proof makes use of Proposition 2, Proposition 6, Proposition 7, and Corollary 1, which are provided in Appendices C.1, C.2, C.3, and C.4, respectively. Note that the proof is partly inspired by that of Ghassami et al. (20, Theorem 3).

**Theorem 1** (Identifiability with Sparsity).: _Suppose that the true mixing matrix \(\tilde{\mathbf{A}}\) satisfies Assumptions 1, 2, and 3. Let \(\hat{\mathbf{A}}\) be a solution of the following problem:_

\[\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}\|\mathbf{A}\|_{0}\quad\mathrm{ subject\ to}\quad\mathbf{A}\mathbf{A}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{ \top}\quad\mathrm{and}\quad\mathbf{A}\text{ satisfies Assumption \ref{Assumption:covariance}.}\] (2)

_Then, we have \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\)._

Proof.: Let \(\hat{\mathbf{A}}\) be a solution of Problem (2). This implies \(\hat{\mathbf{A}}\hat{\mathbf{A}}^{\top}=\tilde{\bm{\Sigma}}=\tilde{\mathbf{A}} \tilde{\mathbf{A}}^{\top}\) and that \(\hat{\mathbf{A}}\) satisfies Assumption 2. Since \(\tilde{\mathbf{A}}\) is non-singular, by Lemma 3, matrix \(\hat{\mathbf{A}}\) is non-singular.

Since \(\hat{\mathbf{A}}\) can entail the covariance matrix \(\tilde{\bm{\Sigma}}\), we have \(\tilde{\bm{\Sigma}}\in\bm{\Sigma}(\bm{\xi}_{\hat{\mathbf{A}}})\), which indicates that \(\tilde{\bm{\Sigma}}\) contains all semialgebraic constraints of \(\bm{\xi}_{\hat{\mathbf{A}}}\). Under Assumption 3, we have

\[H(\bm{\xi}_{\hat{\mathbf{A}}})\subseteq H(\bm{\xi}_{\tilde{\mathbf{A}}}).\] (8)

The sparsity term in the objective function implies

\[\|\hat{\mathbf{A}}\|_{0}\leq\|\tilde{\mathbf{A}}\|_{0},\] (9)

because otherwise \(\hat{\mathbf{A}}\) will never be a solution of Problem (2).

We now show by contradiction that \(H(\bm{\xi}_{\hat{\mathbf{A}}})\not\subset H(\bm{\xi}_{\hat{\mathbf{A}}})\). Suppose \(H(\bm{\xi}_{\hat{\mathbf{A}}})\subset H(\bm{\xi}_{\hat{\mathbf{A}}})\), which indicates \(\dim(\bm{\Sigma}(\bm{\xi}_{\hat{\mathbf{A}}}))>\dim(\bm{\Sigma}(\bm{\xi}_{\hat {\mathbf{A}}}))\). Since the support matrices \(\bm{\xi}_{\hat{\mathbf{A}}}\) and \(\bm{\xi}_{\hat{\mathbf{A}}}\) satisfy Assumption 2, Proposition 2 implies \(\|\bm{\xi}_{\hat{\mathbf{A}}}\|_{0}>\|\bm{\xi}_{\hat{\mathbf{A}}}\|_{0}\), which is contradictory with Inequality (9). This implies

\[H(\bm{\xi}_{\hat{\mathbf{A}}})\not\subset H(\bm{\xi}_{\hat{\mathbf{A}}}).\] (10)

By Eqs. (8) and (10), we obtain \(H(\bm{\xi}_{\hat{\mathbf{A}}})=H(\bm{\xi}_{\hat{\mathbf{A}}})\), which, by Proposition 7, implies that the support matrices \(\bm{\xi}_{\hat{\mathbf{A}}}\) and \(\bm{\xi}_{\hat{\mathbf{A}}}\) are covariance equivalent, because they satisfy Assumption 2. By Proposition 6, we conclude that the columns of \(\bm{\xi}_{\hat{\mathbf{A}}}\) are a permutation of those of \(\bm{\xi}_{\hat{\mathbf{A}}}\).

Recall that matrices \(\tilde{\mathbf{A}}\) and \(\hat{\mathbf{A}}\) are non-singular, and entail the same covariance matrix \(\tilde{\bm{\Sigma}}\). Since the columns of \(\bm{\xi}_{\hat{\mathbf{A}}}\) are a permutation of those of \(\bm{\xi}_{\hat{\mathbf{A}}}\), by Corollary 1, we have \(\hat{\mathbf{A}}\sim\hat{\mathbf{A}}\). 

### Dimension of Covariance Set

We first define the Jacobian matrix of \(\bm{\Sigma}=\mathbf{A}\mathbf{A}^{\top}\) w.r.t. the free parameters of \(\mathbf{A}\) as \(\frac{\partial\bm{\Sigma}}{\partial\mathbf{A}}\), which is a \(n^{2}\times\|\mathbf{A}\|_{0}\) matrix, where the rows are indexed by \((i,j)\in[n]\times[n]\), and columns are indexed by \((k,l)\in\operatorname{supp}(\mathbf{A})\). That is, for \((i,j)\in[n]\times[n]\) and \((k,l)\in\operatorname{supp}(\mathbf{A})\), we have

\[\left(\frac{\partial\mathbf{\Sigma}}{\partial\mathbf{A}}\right)_{(i,j),(k,l)}= \frac{\partial\Sigma_{i,j}}{\partial a_{k,l}}=\begin{cases}2a_{i,l}&\text{ if $i=j$ and $k=i$},\\ 0&\text{ if $i=j$ and $k\neq i$},\\ a_{j,l}&\text{ if $i\neq j$ and $k=i$},\\ a_{i,l}&\text{ if $i\neq j$ and $k=j$},\\ 0&\text{ if $i\neq j$ and $k\not\in\{i,j\}$}.\end{cases}.\] (11)

Given a matrix \(\mathbf{M}\), we denote by \(\operatorname{off}(\mathbf{M})\) and \(\operatorname{on}(\mathbf{M})\) the off-diagonal and diagonal entries of \(\mathbf{M}\), respectively. With a slight abuse of notation, we rewrite the above Jacobian matrix as the following form that consists of four submatrices, by permuting the corresponding columns and rows:

\[\frac{\partial\mathbf{\Sigma}}{\partial\mathbf{A}}=\begin{bmatrix}\frac{ \partial\operatorname{off}(\mathbf{\Sigma})}{\partial\operatorname{off}( \mathbf{\Lambda})}&\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{ \partial\operatorname{on}(\mathbf{\Lambda})}\\ \frac{\partial\operatorname{off}(\mathbf{\Lambda})}{\partial\operatorname{ off}(\mathbf{\Lambda})}&\frac{\partial\operatorname{on}(\mathbf{\Sigma})}{ \partial\operatorname{on}(\mathbf{\Lambda})}\end{bmatrix}.\] (12)

Each submatrix above represents the Jacobian matrix for different parts (i.e., off-diagonal and diagonal entries) of matrix \(\mathbf{\Sigma}\) taken w.r.t. different free parameters (i.e., off-diagonal and diagonal free parameters) in matrix \(\mathbf{A}\). Note that column and row permutations do not affect the rank of the matrix. Since we are primarily interested in the rank of the above Jacobian matrix and its submatrices, we use the same notation to refer to different column and/or row permutations of the corresponding Jacobian matrix, depending on the context.

**Lemma 9**.: _Let \(\mathbf{A}\) be a lower triangular matrix. Then, we have_

\[\operatorname{rank}\left(\frac{\partial\mathbf{\Sigma}}{\partial\mathbf{A}} \bigg{|}_{\mathbf{\Lambda}=\mathbf{I}_{n}}\right)=\|\mathbf{A}\|_{0}.\]

Proof.: Let \(d\) be the number of diagonal free parameters in matrix \(\mathbf{A}\). This indicates that \(\operatorname{off}(\mathbf{A})\) and \(\operatorname{on}(\mathbf{A})\) contain \(\|\mathbf{A}\|_{0}-d\) and \(d\) free parameters, respectively.

By specializing \(\mathbf{A}=\mathbf{I}_{n}\) and using Eq. (11), we have

\[\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{\partial \operatorname{on}(\mathbf{\Lambda})}\bigg{|}_{\mathbf{\Lambda}=\mathbf{I}_{n }}=\mathbf{0},\quad\frac{\partial\operatorname{on}(\mathbf{\Sigma})}{ \partial\operatorname{off}(\mathbf{\Lambda})}\bigg{|}_{\mathbf{\Lambda}= \mathbf{I}_{n}}=\mathbf{0},\quad\text{and}\quad\frac{\partial\operatorname{ on}(\mathbf{\Sigma})}{\partial\operatorname{on}(\mathbf{\Lambda})}\bigg{|}_{ \mathbf{\Lambda}=\mathbf{I}_{n}}=2\mathbf{I}_{d},\] (13)

where the last equality is obtained after row permutations of the corresponding matrix. Also, since matrix \(\mathbf{A}\) is lower triangular, each nonzero entry of \(\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{\partial\operatorname{off }(\mathbf{\Sigma})}\big{|}_{\mathbf{\Lambda}=\mathbf{I}_{n}}\) corresponds to either

\[\frac{\partial\Sigma_{k,l}}{\partial a_{k,l}}=1\quad\text{or}\quad\frac{ \partial\Sigma_{l,k}}{\partial a_{k,l}}=1\quad\text{for some }\ k>l.\]

In this case, each column of \(\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{\partial\operatorname{off }(\mathbf{\Lambda})}\big{|}_{\mathbf{\Lambda}=\mathbf{I}_{n}}\) contains precisely two nonzero entries, while each row either contains precisely one nonzero entry, or does not contain any nonzero entry. Therefore, \(\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{\partial\operatorname{off }(\mathbf{\Lambda})}\big{|}_{\mathbf{\Lambda}=\mathbf{I}_{n}}\) can be rewritten after row permutations as

\[\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{\partial \operatorname{off}(\mathbf{\Lambda})}\bigg{|}_{\mathbf{\Lambda}=\mathbf{I}_{n }}=\begin{bmatrix}\mathbf{I}_{\|\mathbf{\Lambda}\|_{0}-d}\\ \mathbf{I}_{\|\mathbf{\Lambda}\|_{0}-d}\\ \mathbf{0}\end{bmatrix},\]

which yields

\[\operatorname{rank}\left(\frac{\partial\operatorname{off}(\mathbf{\Sigma})}{ \partial\operatorname{off}(\mathbf{\Lambda})}\bigg{|}_{\mathbf{\Lambda}= \mathbf{I}_{n}}\right)=\|\mathbf{A}\|_{0}-d.\] (14)

Substituting Eq. (13) into Eq. (12), we have

\[\frac{\partial\mathbf{\Sigma}}{\partial\mathbf{\Lambda}}\bigg{|}_{ \mathbf{\Lambda}=\mathbf{I}_{n}}=\begin{bmatrix}\frac{\partial\operatorname{ off}(\mathbf{\Sigma})}{\partial\operatorname{off}(\mathbf{\Lambda})} \big{|}_{\mathbf{\Lambda}=\mathbf{I}_{n}}&\mathbf{0}\\ \mathbf{0}&2\mathbf{I}_{d}\end{bmatrix},\]

which, with Eq. (14), implies

\[\operatorname{rank}\left(\frac{\partial\mathbf{\Sigma}}{\partial \mathbf{\Lambda}}\bigg{|}_{\mathbf{\Lambda}=\mathbf{I}_{n}}\right) =\operatorname{rank}\left(\frac{\partial\operatorname{off}(\mathbf{ \Sigma})}{\partial\operatorname{off}(\mathbf{\Lambda})}\bigg{|}_{\mathbf{ \Lambda}=\mathbf{I}_{n}}\right)+\operatorname{rank}(2\mathbf{I}_{d})\] \[=\|\mathbf{A}\|_{0}-d+d\] \[=\|\mathbf{A}\|_{0}.\qed\]

**Proposition 2** (Dimension of Covariance Set).: _Let \(\bm{\xi}\) be a support matrix that satisfies Assumption 2. Then, its covariance set has a dimension of \(\|\bm{\xi}\|_{0}\), i.e., \(\dim(\bm{\Sigma}(\bm{\xi}))=\|\bm{\xi}\|_{0}\)._

Proof.: Since \(\bm{\xi}\) satisfies Assumption 2, it can be permuted by column and row permutations to be lower triangular; in this case, the resulting covariance matrix and the original covariance matrix differ in equal row and column permutations. Note that the dimension of the covariance set remains the same after simultaneous equal row and column permutations of the covariance matrices. Therefore, it suffices to consider the case where \(\bm{\xi}\) is lower triangular, and show that it has a dimension of \(\|\bm{\xi}\|_{0}\).

As indicated by Geiger et al. [19, Theorem 10], the dimension of \(\dim(\bm{\Sigma}(\bm{\xi}))\) equals the maximum rank of the corresponding Jacobian matrix. In this case, it suffices to consider the columns of the Jacobian matrix that correspond to the nonzero entries of support matrix \(\bm{\xi}\), i.e., the free parameters of matrix \(\bm{\Lambda}\), which we denote by \(\frac{\partial\bm{\Sigma}}{\partial\bm{\Lambda}}\). By Lemma 9, when \(\bm{\Lambda}=\bm{\mathrm{I}}_{n}\), the Jacobian matrix \(\frac{\partial\bm{\Sigma}}{\partial\bm{\Lambda}}\) has full column rank that is equal to \(\|\bm{\xi}\|_{0}\). Therefore, the dimension of the covariance set is \(\|\bm{\xi}\|_{0}\). 

### Covariance Equivalence and Column Permutation of Support

We now state a result that is adapted from Ghassami et al. [20, Proposition 5] to the context of ICA. In our proof of Theorem 1, only the "only if part" of the following result is used.

**Proposition 6** (Ghassami et al. [20, Proposition 5]).: _Consider two support matrices \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\). If every pair of columns of \(\bm{\xi}_{1}\) differ in more than one entry, then \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) are covariance equivalent if and only if the columns of \(\bm{\xi}_{2}\) are a permutation of columns of \(\bm{\xi}_{1}\)._

### Equality Constraints and Covariance Equivalence

In this section, we show how Assumption 2 allows one to go from two support matrices \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) having the same equality constraints to covariance equivalence.

Following Ghassami et al. [20], we first denote the covariance set of a directed graph \(\mathcal{G}\) by

\[\bm{\Theta}(\mathcal{G})\coloneqq\{(\bm{\mathrm{I}}-\bm{\mathrm{B}})\bm{ \Omega}^{-1}(\bm{\mathrm{I}}-\bm{\mathrm{B}})^{\top}:\bm{\mathrm{B}}\in \mathbb{R}_{\mathrm{off}}^{n\times n},\bm{\Omega}\in\mathrm{diag}(\mathbb{R}_{ >0}^{n}),\mathrm{supp}(\bm{\mathrm{B}})\subseteq\mathrm{supp}(\bm{\mathrm{B}} _{\mathcal{G}})\},\]

where \(\bm{\mathrm{B}}_{\mathcal{G}}\) is the adjacency matrix of \(\mathcal{G}\). With a slight abuse of notation, we denote by \(H(\mathcal{G})\) the set of the equality constraints imposed by \(\mathcal{G}\) on the resulting matrix \((\bm{\mathrm{I}}-\bm{\mathrm{B}})\bm{\Omega}^{-1}(\bm{\mathrm{I}}-\bm{\mathrm{ B}})^{\top}\).

**Lemma 10**.: _Let \(\bm{\xi}\) be a non-singular support matrix that satisfies Assumption 2.22 Then, there exists a DAG \(\mathcal{G}\) such that \(\bm{\Sigma}(\bm{\xi})=\bm{\Theta}(\mathcal{G})\)._

Footnote 22: We say that a support matrix \(\bm{\xi}\) is non-singular if there exists non-singular matrix \(\bm{\Lambda}\) such that \(\mathrm{supp}(\bm{\Lambda})\subseteq\mathrm{supp}(\bm{\xi})\).

Proof.: Since \(\bm{\xi}\) is non-singular, by Lemma 2, it can be mapped via column permutations to another support matrix \(\bm{\xi}^{\prime}\) with diagonal entries being nonzero. Since \(\bm{\xi}\) satisfies Assumption 2, Proposition 10 implies that \(\bm{\xi}^{\prime}\) represents a DAG, say \(\mathcal{G}\), where the support of its adjacency matrix is \(\mathrm{supp}(\bm{\mathrm{B}}_{\mathcal{G}})=\mathrm{supp}(\mathrm{off}(\bm{ \xi}^{\prime}))\). Since column permutations of support matrices do not affect the resulting covariance matrices, we have \(\bm{\Sigma}(\bm{\xi})=\bm{\Sigma}(\bm{\xi}^{\prime})\). Therefore, it suffices to prove \(\bm{\Sigma}(\bm{\xi}^{\prime})=\bm{\Theta}(\mathcal{G})\).

Since \(\bm{\xi}^{\prime}\) is a support matrix with diagonal entries being nonzero, we have

\[\mathrm{supp}(\bm{\xi}^{\prime})=\mathrm{supp}(\bm{\mathrm{I}})\cup\mathrm{ supp}(\mathrm{off}(\bm{\xi}^{\prime})).\] (15)

We consider both parts of the statements.

**Proof of \(\bm{\Theta}(\mathcal{G})\subseteq\bm{\Sigma}^{+}(\bm{\xi}^{\prime})\):**

Suppose \(\bm{\mathrm{M}}\in\bm{\Theta}(\mathcal{G})\). By definition of \(\bm{\Theta}(\mathcal{G})\), there exist \(\bm{\mathrm{B}}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\bm{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) such that

\[\mathrm{supp}(\bm{\mathrm{B}})\subseteq\mathrm{supp}(\mathrm{off}(\bm{\xi}^{ \prime}))\quad\text{and}\quad\bm{\mathrm{M}}=(\bm{\mathrm{I}}-\bm{\mathrm{B}}) \bm{\Omega}^{-1}(\bm{\mathrm{I}}-\bm{\mathrm{B}})^{\top}.\] (16)

Let \(\bm{\mathrm{A}}\coloneqq(\bm{\mathrm{I}}-\bm{\mathrm{B}})\bm{\Omega}^{-\frac{1 }{2}}\), which, with Eq. (16), implies \(\bm{\mathrm{A}}\bm{\mathrm{A}}^{\top}=\bm{\mathrm{M}}\). Recall that \(\mathcal{G}\) is a DAG, which indicates \(\bm{\mathrm{B}}\) also represents a DAG, Therefore, \(\bm{\mathrm{I}}-\bm{\mathrm{B}}\), and thus \(\bm{\mathrm{A}}\), are non-singular. Since right multiplication of \(\bm{\Omega}^{-\frac{1}{2}}\) does not affect the support of \(\bm{\mathrm{I}}-\bm{\mathrm{B}}\), by Eqs. (15) and (16), we have

\[\mathrm{supp}(\bm{\mathrm{A}})=\mathrm{supp}(\bm{\mathrm{I}}-\bm{\mathrm{B}})= \mathrm{supp}(\bm{\mathrm{I}})\cup\mathrm{supp}(\bm{\mathrm{B}})\subseteq \mathrm{supp}(\bm{\mathrm{I}})\cup\mathrm{supp}(\mathrm{off}(\bm{\xi}^{\prime}))= \mathrm{supp}(\bm{\xi}^{\prime}).\]

Therefore, we have \(\mathrm{supp}(\bm{\mathrm{A}})\subseteq\mathrm{supp}(\bm{\xi}^{\prime})\), which, with the non-singularity of matrix \(\bm{\mathrm{A}}\), implies \(\bm{\mathrm{M}}=\bm{\mathrm{A}}\bm{\mathrm{A}}^{\top}\in\bm{\Sigma}(\bm{\xi}^{ \prime})\).

Proof of \(\bm{\Sigma}(\bm{\xi}^{\prime})\subseteq\bm{\Theta}(\bm{\beta})\):

Suppose \(\mathbf{M}\in\bm{\Sigma}(\bm{\xi}^{\prime})\). By definition of \(\bm{\Sigma}(\bm{\xi}^{\prime})\), there exists non-singular matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) such that

\[\operatorname{supp}(\mathbf{A})\subseteq\operatorname{supp}(\bm{\xi}^{\prime} )\quad\text{and}\quad\mathbf{M}=\mathbf{A}\mathbf{A}^{\top}.\] (17)

Since matrix \(\mathbf{A}\) is non-singular, all of its diagonal entries must be nonzero, because otherwise the corresponding determinant will be zero, which contradicts its non-singularity. By Lemma 4, there exists matrix \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\bm{\Omega}\in\operatorname{diag}(\mathbb{R}_{>0}^{n})\) such that \(\mathbf{A}=(\mathbf{I}-\mathbf{B})\bm{\Omega}^{-\frac{1}{2}}\mathbf{D}\), where \(\mathbf{D}\) is a diagonal matrix with diagonal entries being \(\pm 1\). By Eq. (17), we have \((\mathbf{I}-\mathbf{B})\bm{\Omega}^{-1}(\mathbf{I}-\mathbf{B})^{\top}= \mathbf{M}\). Since right multiplication of \(\bm{\Omega}^{-\frac{1}{2}}\mathbf{D}\) does not affect the support of \(\mathbf{I}-\mathbf{B}\), by Eqs. (15) and (17), we have

\[\operatorname{supp}(\mathbf{I})\cup\operatorname{supp}(\mathbf{B})= \operatorname{supp}(\mathbf{I}-\mathbf{B})=\operatorname{supp}(\mathbf{A}) \subseteq\operatorname{supp}(\mathbf{\xi})=\operatorname{supp}(\mathbf{I}) \cup\operatorname{supp}(\operatorname{off}(\bm{\xi}^{\prime})).\]

Note that \(\operatorname{supp}(\mathbf{I})\) and \(\operatorname{supp}(\mathbf{B})\) are disjoint. Furthermore, \(\operatorname{supp}(\mathbf{I})\) and \(\operatorname{supp}(\operatorname{off}(\bm{\xi}^{\prime}))\) are disjoint. Therefore, we have \(\operatorname{supp}(\mathbf{B})\subseteq\operatorname{supp}(\operatorname{ off}(\bm{\xi}^{\prime}))\) and thus \(\mathbf{M}=(\mathbf{I}-\mathbf{B})\bm{\Omega}^{-1}(\mathbf{I}-\mathbf{B})^{ \top}\in\bm{\Theta}(\mathcal{G})\). 

**Proposition 7**.: _Let \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) be non-singular support matrices that satisfy Assumption 2. If they have the same set of equality constraints, i.e., \(H(\bm{\xi}_{2})=H(\bm{\xi}_{2})\), then they are covariance equivalent._

Proof.: Since matrices \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) are non-singular and satisfy Assumption 2, by Lemma 10, there exist DAGs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) such that

\[\bm{\Sigma}(\bm{\xi}_{1})=\bm{\Theta}(\mathcal{G}_{1})\quad\text{and}\quad\bm{ \Sigma}(\bm{\xi}_{2})=\bm{\Theta}(\mathcal{G}_{2}),\] (18)

which imply

\[H(\bm{\xi}_{1})=H(\mathcal{G}_{1})\quad\text{and}\quad H(\bm{\xi}_{2})=H( \mathcal{G}_{2}).\] (19)

We now provide a proof by contrapositive. Suppose that \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) are not covariance equivalent, i.e., \(\bm{\Sigma}(\bm{\xi}_{1})\neq\bm{\Sigma}(\bm{\xi}_{2})\). By Eq. (18), we have \(\bm{\Theta}(\mathcal{G}_{1})\neq\bm{\Theta}(\mathcal{G}_{2})\), i.e., DAGs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are not covariance equivalent. By Ghassami et al. [20, Proposition 1], DAGs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are not Markov equivalent, which indicates that they do not have the same skeleton and v-structures [43]. This implies that they lead to different sets of conditional independence constraints, which in this case correspond to different sets of polynomial equality constraints [39], i.e., \(H(\mathcal{G}_{1})\neq H(\mathcal{G}_{2})\). By Eq. (19), we have \(H(\bm{\xi}_{1})\neq H(\bm{\xi}_{2})\). 

### Identifiability of Parameters from Support

In the following, we provide a result that establish the identifiability of the parameters in mixing matrix from its support.

**Proposition 8**.: _Consider two non-singular matrices \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\) that satisfy Assumption 2 and that entail the same covariance matrix, i.e., \(\bm{\Sigma}=\mathbf{A}_{1}\mathbf{A}_{1}^{\top}=\mathbf{A}_{2}\mathbf{A}_{2}^{\top}\). If matrices \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\) have the same support, then they differ only in sign changes of columns._

Proof.: Since matrix \(\mathbf{A}_{1}\) satisfies Assumption 2, there exist permutation matrices \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) such that \(\mathbf{P}_{1}^{\top}\mathbf{A}_{1}\mathbf{P}_{2}\) is lower triangular. Clearly, \(\mathbf{P}_{1}^{\top}\mathbf{A}_{2}\mathbf{P}_{2}\) is also lower triangular, because matrices \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\) have the same support. Since matrices \(\mathbf{P}_{1}^{\top}\mathbf{A}_{1}\mathbf{P}_{2}\) and \(\mathbf{P}_{1}^{\top}\mathbf{A}_{2}\mathbf{P}_{2}\) are non-singular, all diagonal entries of these two matrices must be nonzero, because otherwise the corresponding determinant will be zero, which contradict their non-singularity. Let \(\mathbf{D}_{1}\) and \(\mathbf{D}_{2}\) be diagonal matrices with diagonal entries being \(\pm 1\) such that the diagonal entries of \(\mathbf{P}_{1}^{\top}\mathbf{A}_{1}\mathbf{D}_{1}\mathbf{P}_{2}\) and \(\mathbf{P}_{1}^{\top}\mathbf{A}_{2}\mathbf{D}_{2}\mathbf{P}_{2}\) are positive. (The procedure for constructing such diagonal matrices \(\mathbf{D}_{1}\) and \(\mathbf{D}_{2}\) is straightforward and omitted here.)

Furthermore, we have

\[(\mathbf{P}_{1}^{\top}\mathbf{A}_{1}\mathbf{D}_{1}\mathbf{P}_{2})(\mathbf{P}_{ 1}^{\top}\mathbf{A}_{1}\mathbf{D}_{1}\mathbf{P}_{2})^{\top}=(\mathbf{P}_{1}^{ \top}\mathbf{A}_{2}\mathbf{D}_{2}\mathbf{P}_{2})(\mathbf{P}_{1}^{\top}\mathbf{A }_{2}\mathbf{D}_{2}\mathbf{P}_{2})^{\top}=\mathbf{P}_{1}^{\top}\bm{\Sigma} \mathbf{P}_{1}.\] (20)

Since matrix \(\mathbf{A}_{1}\) is non-singular, matrices \(\bm{\Sigma}\), and thus \(\mathbf{P}_{1}^{\top}\bm{\Sigma}\mathbf{P}_{1}\), are symmetric positive definite. Here, Eq. (20) can be viewed as the Cholesky decomposition of \(\mathbf{P}_{1}^{\top}\bm{\Sigma}\mathbf{P}_{1}\), where \(\mathbf{P}_{1}^{\top}\mathbf{A}_{1}\mathbf{D}_{1}\mathbf{P}_{2}\) and \(\mathbf{P}_{1}^{\top}\mathbf{A}_{2}\mathbf{D}_{2}\mathbf{P}_{2}\) are the Cholesky factors. Recall that they are lower triangular matrices with all diagonal entries being positive; in this case, it is known that such Cholesky factor is unique [42]. Therefore, we have

\[\mathbf{P}_{1}^{\top}\mathbf{A}_{1}\mathbf{D}_{1}\mathbf{P}_{2}=\mathbf{P}_{1}^{ \top}\mathbf{A}_{2}\mathbf{D}_{2}\mathbf{P}_{2},\]

which implies

\[\mathbf{A}_{1}=\mathbf{A}_{2}\mathbf{D}_{2}\mathbf{D}_{1}^{-1}=\mathbf{A}_{2} \mathbf{D}_{2}\mathbf{D}_{1}.\]

Since \(\mathbf{D}_{2}\mathbf{D}_{1}\) is a diagonal matrix with diagonal entries being \(\pm 1\), we conclude that matrices \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\) differ only in sign changes of columns.

**Corollary 1** (Identifiability of Parameters from Support).: _Consider two non-singular matrices \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\) that satisfy Assumption 2 and that entail the same covariance matrix, i.e., \(\mathbf{\Sigma}=\mathbf{A}_{1}\mathbf{A}_{1}^{\top}=\mathbf{A}_{2}\mathbf{A}_{ 2}^{\top}\). If the columns of \(\boldsymbol{\xi}_{\mathbf{A}_{1}}\) are a permutation of those of \(\boldsymbol{\xi}_{\mathbf{A}_{2}}\), then we have \(\mathbf{A}_{1}\sim\mathbf{A}_{2}\)._

Proof.: Suppose by contradiction that \(\mathbf{A}_{1}\not\sim\mathbf{A}_{2}\). This implies that, for every permutation matrix \(\mathbf{P}\) such that \(\boldsymbol{\xi}_{\mathbf{A}_{j}}=\boldsymbol{\xi}_{\mathbf{A}_{2}\mathbf{P}}\), matrices \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\mathbf{P}\) differ in more than sign changes of columns. By Proposition 8, this cannot happen. 

## Appendix D Proofs of Other Results

### Proof of Example 1

**Example 1** (Semialgebraic Constraints).: _Consider support matrices_

\[\boldsymbol{\xi}_{1}=\begin{bmatrix}\times&0&0\\ \times&\times&0\\ \times&0&\times\end{bmatrix}\qquad\text{and}\qquad\boldsymbol{\xi}_{2}= \begin{bmatrix}\times&0&\times\\ \times&\times&0\\ 0&\times&\times\end{bmatrix}.\]

_The equality constraints imposed by \(\boldsymbol{\xi}_{1}\) include_

\[\Sigma_{1,1}\Sigma_{2,3}-\Sigma_{1,2}\Sigma_{1,3}=0,\]

_while the inequality constraints imposed by \(\boldsymbol{\xi}_{2}\) include_

\[(\Sigma_{1,1}\Sigma_{2,2}\Sigma_{3,3}+\Sigma_{1,1}\Sigma_{2,3}^{2}-\Sigma_{2,2 }\Sigma_{1,3}^{2}-\Sigma_{3,3}\Sigma_{1,2}^{2})^{2}-4(\Sigma_{1,1}\Sigma_{2,2} -\Sigma_{1,2}^{2})(\Sigma_{1,1}\Sigma_{3,3}\Sigma_{2,3}^{2}-\Sigma_{1,3}^{2} \Sigma_{2,3}^{2})\geq 0.\]

Proof.: We first consider matrix \(\mathbf{A}_{1}\) with the support \(\boldsymbol{\xi}_{1}\). That is, matrix \(\mathbf{A}_{1}\) is of the form

\[\mathbf{A}_{1}=\begin{bmatrix}a_{1,1}&0&0\\ a_{2,1}&a_{2,2}&0\\ a_{3,1}&0&a_{3,3}\end{bmatrix}.\]

The entries of the resulting covariance matrix \(\mathbf{\Sigma}=\mathbf{A}_{1}\mathbf{A}_{1}^{\top}\) are then given by

\[\Sigma_{1,1}=a_{1,1}^{2}, \Sigma_{1,2}=a_{1,1}a_{2,1},\] \[\Sigma_{2,2}=a_{2,1}^{2}+a_{2,2}^{2}, \Sigma_{1,3}=a_{1,1}a_{3,1},\] \[\Sigma_{3,3}=a_{3,1}^{2}+a_{3,3}^{2}, \Sigma_{2,3}=a_{2,1}a_{3,1},\]

which imply

\[\Sigma_{1,1}\Sigma_{2,3}-\Sigma_{1,2}\Sigma_{1,3}=0.\]

We now consider matrix \(\mathbf{A}_{2}\) with the support \(\boldsymbol{\xi}_{2}\). That is, matrix \(\mathbf{A}_{2}\) is of the form

\[\mathbf{A}_{2}=\begin{bmatrix}a_{1,1}&0&a_{1,3}\\ a_{2,1}&a_{2,2}&0\\ 0&a_{3,2}&a_{3,3}\end{bmatrix}.\]

The entries of the resulting covariance matrix \(\mathbf{\Sigma}=\mathbf{A}_{2}\mathbf{A}_{2}^{\top}\) are then given by

\[\Sigma_{1,1}=a_{1,1}^{2}+a_{1,3}^{2}, \Sigma_{1,2}=a_{1,1}a_{2,1},\] \[\Sigma_{2,2}=a_{2,1}^{2}+a_{2,2}^{2}, \Sigma_{1,3}=a_{1,3}a_{3,3},\] \[\Sigma_{3,3}=a_{3,2}^{2}+a_{3,3}^{2}, \Sigma_{2,3}=a_{2,2}a_{3,2}.\]

Suppose we fix the value of \(a_{3,2}\). This leads to

\[a_{3,2}^{2}=\Sigma_{3,3}-\frac{\Sigma_{1,3}^{2}}{\Sigma_{1,1}-\frac{\Sigma_{1, 2}^{2}}{\Sigma_{2,2}-\frac{\Sigma_{2,3}^{2}}{a_{3,2}^{2}}}},\]

which can be rewritten as

\[(\Sigma_{1,1}\Sigma_{2,2}-\Sigma_{1,2}^{2})a_{3,2}^{4}+(-\Sigma_{1,1}\Sigma_{ 2,2}\Sigma_{3,3}-\Sigma_{1,1}\Sigma_{2,3}^{2}+\Sigma_{2,2}\Sigma_{1,3}^{2}+ \Sigma_{3,3}\Sigma_{1,2}^{2})a_{3,2}^{2}+(\Sigma_{1,1}\Sigma_{3,3}\Sigma_{2,3} ^{2}-\Sigma_{1,3}^{2}\Sigma_{2,3}^{2})=0.\]

Since the value of \(a_{3,2}\) is a real number, we have

\[(\Sigma_{1,1}\Sigma_{2,2}\Sigma_{3,3}+\Sigma_{1,1}\Sigma_{23}^{2}-\Sigma_{2,2 }\Sigma_{1,3}^{2}-\Sigma_{3,3}\Sigma_{1,2}^{2})^{2}-4(\Sigma_{1,1}\Sigma_{2,2} -\Sigma_{1,2}^{2})(\Sigma_{1,1}\Sigma_{3,3}\Sigma_{2,3}^{2}-\Sigma_{1,3}^{2} \Sigma_{2,3}^{2})\geq 0.\]

### Proof of Proposition 1

**Proposition 1**.: _If the true mixing matrix \(\tilde{\mathbf{A}}\) does not satisfy Assumption 1, then there exists a solution \(\tilde{\mathbf{A}}\) to Problem (1) such that \(\tilde{\mathbf{A}}\not\sim\tilde{\mathbf{A}}\)._

Proof.: Since the true mixing matrix \(\tilde{\mathbf{A}}\) does not satisfy Assumption 1, there exist \(i,j\in[n]\), \(i\neq j\) such that

\[|\operatorname{supp}(\tilde{\mathbf{a}}_{i})\cup\operatorname{supp}(\tilde{ \mathbf{a}}_{j})|-|\operatorname{supp}(\tilde{\mathbf{a}}_{i})\cap\operatorname {supp}(\tilde{\mathbf{a}}_{j})|\leq 1.\]

This leads to the following two cases:

* **Case 1:**\(|\operatorname{supp}(\tilde{\mathbf{a}}_{i})\cup\operatorname{supp}(\tilde{ \mathbf{a}}_{j})|-|\operatorname{supp}(\tilde{\mathbf{a}}_{i})\cap\operatorname {supp}(\tilde{\mathbf{a}}_{j})|=1\). In this case, since the mixing matrix is of full column rank, there must exist a \(k\in[n]\) such that \(\left(\mathbf{\xi}_{\tilde{\mathbf{A}}}\right)_{k,i}=\left(\mathbf{\xi}_{ \tilde{\mathbf{A}}}\right)_{k,j}=\times\). Thus, we can always apply a reversible acute rotation (see Remark 3) to the \(i\)-th and \(j\)-th columns of matrix \(\tilde{\mathbf{A}}\). This operation leads to another matrix \(\tilde{\mathbf{A}}\) with \(\|\tilde{\mathbf{A}}\|_{0}=\|\tilde{\mathbf{A}}\|_{0}\) and \(\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A }}^{\top}\). In the reversible acute rotation, we can set either \(\left(\mathbf{\xi}_{\tilde{\mathbf{A}}}\right)_{k,i}\) or \(\left(\mathbf{\xi}_{\tilde{\mathbf{A}}}\right)_{k,j}\) to \(0\). This implies \(\|\tilde{\mathbf{a}}_{k,:}\|_{0}<\|\tilde{\mathbf{a}}_{k,:}\|_{0}\), and therefore \(\tilde{\mathbf{A}}\not\sim\tilde{\mathbf{A}}\). Now, suppose that the true mixing matrix \(\tilde{\mathbf{A}}\) is not a solution to Problem (1). Clearly, there must exist a solution \(\hat{\mathbf{A}}\) to Problem (1) such that \(\|\hat{\mathbf{A}}\|_{0}<\|\tilde{\mathbf{A}}\|_{0}\), which indicates \(\hat{\mathbf{A}}\not\sim\tilde{\mathbf{A}}\). It remains to consider the case where matrix \(\tilde{\mathbf{A}}\) is a solution to Problem (1). In this case, since \(\|\tilde{\mathbf{A}}\|_{0}=\|\tilde{\mathbf{A}}\|_{0}\), matrix \(\tilde{\mathbf{A}}\) is also a solution to Problem (1), and we have shown that \(\tilde{\mathbf{A}}\not\sim\tilde{\mathbf{A}}\).
* **Case 2:**\(|\operatorname{supp}(\tilde{\mathbf{a}}_{i})\cup\operatorname{supp}(\tilde{ \mathbf{a}}_{j})|-|\operatorname{supp}(\tilde{\mathbf{a}}_{i})\cap\operatorname {supp}(\tilde{\mathbf{a}}_{j})|=0\). In this case, we can always apply a reduction (see Remark 3) to the \(i\)-th and \(j\)-th columns of matrix \(\tilde{\mathbf{A}}\). This operation leads to another matrix \(\tilde{\mathbf{A}}\) with \(\|\tilde{\mathbf{A}}\|_{0}<\|\tilde{\mathbf{A}}\|_{0}\) and \(\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{ A}}^{\top}\). Therefore, there must exist a solution \(\hat{\mathbf{A}}\) to Problem (1) such that \(\|\hat{\mathbf{A}}\|_{0}\leq\|\tilde{\mathbf{A}}\|_{0}<\|\tilde{\mathbf{A}}\|_ {0}\), which indicates \(\hat{\mathbf{A}}\not\sim\tilde{\mathbf{A}}\).

In either case, there exists a solution to Problem (1) whose columns are not signed permutations of the columns of \(\tilde{\mathbf{A}}\). 

### Proof of Example 2

**Example 2**.: _If the connective structure \(\mathcal{G}_{\mathbf{A}}\) of mixing matrix \(\mathbf{A}\) is a polytree, then matrix \(\mathbf{A}\) satisfies Assumption 2._

Proof.: Suppose that the matrix \(\mathbf{A}\) does not satisfy Assumption 2. This means that there does not exist permutation matrices \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) such that \(\mathbf{P}_{1}^{\top}\mathbf{A}\mathbf{P}_{2}\) is lower triangular, which implies that, in the connective structure \(\mathcal{G}_{\mathbf{A}}\), there exists a path that alternates between source and observed variable nodes, i.e., a sequence of nodes \(\{s_{i_{1}},x_{j_{1}},s_{i_{2}},x_{j_{2}},...,s_{i_{k}},x_{j_{k}},s_{i_{1}}\}\) where each pair \((s_{i_{t}},x_{j_{t}})\) corresponds to a nonzero entry \(a_{j_{t},i_{t}}\) in \(\mathbf{A}\), for \(t=1,\ldots,k\). Thus, by replacing the directed edges on this path with undirected edges, we obtain a cycle. Therefore, \(\mathcal{G}_{\mathbf{A}}\) cannot be a polytree. 

### Proof of Proposition 3

The proof of the following proposition is adapted from that of Ghassami et al. (20, Proposition 8).

**Proposition 3** (Generic Property).: _Suppose that the nonzero coefficients of matrix \(\mathbf{A}\) are randomly drawn from a distribution that is absolutely continuous with respect to Lebesgue measure. Then, matrix \(\mathbf{A}\) satisfies Assumption 3 with probability one._

Proof.: Let \(\phi\) be the set of possible equality constraints of any covariance matrix with the same size as \(\mathbf{\Sigma}\), which is a finite set because the number of variables is finite. Consider a matrix \(\hat{\mathbf{A}}\) with the same support as \(\mathbf{A}\) (i.e., \(\mathbf{\xi}_{\hat{\mathbf{A}}}=\mathbf{\xi}_{\mathbf{A}}\)) that violates Assumption 3, where the corresponding covariance matrix is \(\tilde{\mathbf{\Sigma}}\). To violate Assumption 3, \(\hat{\mathbf{\Sigma}}\) has to satisfy an equality constraint \(\kappa\in\phi\setminus H(\mathbf{\xi}_{\mathbf{A}})\). Thus, the set of possible matrices with the same support as \(\mathbf{A}\) that violate Assumption 3 is a subset of

\[\bigcup_{\kappa\in\phi\setminus H(\mathbf{\xi}_{\mathbf{A}})}\left\{\hat{ \mathbf{A}}:\mathbf{\xi}_{\hat{\mathbf{A}}}=\mathbf{\xi}_{\mathbf{A}}\ \text{ and }\ \hat{\mathbf{\Sigma}}\text{ satisfies equality constraint }\kappa\right\}.\]

By the definition of equality constraint, each set in the union above has zero Lebesgue measure, and thus the finite union above also has zero Lebesgue measure. This implies that the set of possiblematrices with the same support as \(\mathbf{A}\) that violate Assumption 3 has zero Lebesgue measure. Therefore, Assumption 3 is satisfied with probability one. 

### Proof of Theorem 2

We first prove the following proposition that will be used to prove both Theorems 2 and 3.

**Proposition 9**.: _If mixing matrix \(\mathbf{A}\) satisfies Assumption 6, then it satisfies Assumption 1._

Proof.: We provide a proof by contrapositive. Suppose \(\mathbf{A}\) does not satisfy Assumption 1. This means that there exist some \(i,j\in[n]\) with \(i\neq j\) such that

\[|\operatorname{supp}(\mathbf{a}_{i})\cup\operatorname{supp}(\mathbf{a}_{j})| -|\operatorname{supp}(\mathbf{a}_{i})\cap\operatorname{supp}(\mathbf{a}_{j})| \leq 1.\]

The difference can be either \(0\) or \(1\).

* **Case 1:**\(|\operatorname{supp}(\mathbf{a}_{i})\cup\operatorname{supp}(\mathbf{a}_{j})| -|\operatorname{supp}(\mathbf{a}_{i})\cap\operatorname{supp}(\mathbf{a}_{j})|=0\). This implies \(\operatorname{supp}(\mathbf{a}_{i})=\operatorname{supp}(\mathbf{a}_{j})\), i.e., the supports of \(\mathbf{a}_{i}\) and \(\mathbf{a}_{j}\) are identical. In this case, Assumption 6 is clearly violated, as \(\operatorname{supp}(\mathbf{a}_{i})\) is a subset of \(\operatorname{supp}(\mathbf{a}_{j})\) and vice versa.
* **Case 2:**\(|\operatorname{supp}(\mathbf{a}_{i})\cup\operatorname{supp}(\mathbf{a}_{j})| -|\operatorname{supp}(\mathbf{a}_{i})\cap\operatorname{supp}(\mathbf{a}_{j})|=1\). This implies that one of the columns is a proper subset of the other, meaning either \(\operatorname{supp}(\mathbf{a}_{i})\subset\operatorname{supp}(\mathbf{a}_{j})\) or \(\operatorname{supp}(\mathbf{a}_{j})\subset\operatorname{supp}(\mathbf{a}_{i})\). Again, this violates Assumption 6.

Hence, in either case, if \(\mathbf{A}\) does not satisfy Assumption 1, then it does not satisfy Assumption 6. 

We now prove the following theorem.

**Theorem 2**.: _For mixing matrix \(\mathbf{A}\), we have the following chain of chain of implications:_

_Assumption 4_ \(\implies\) _Assumption 6_ \(\implies\) _Assumption 1_._

_Furthermore, there exists a matrix \(\mathbf{A}\) satisfying Assumption 1 that does not satisfy Assumption 4._

Proof.: We first prove Assumption 4\(\implies\) Assumption 6.

In Assumption 4, Eq. (3) is assumed to be satisfied for all \(\mathcal{I}\subseteq[n]\) where \(|\mathcal{I}|>1\). Thus, in order to prove that Assumption 4 implies Assumption 6, it is sufficient to only consider the case where \(|\mathcal{I}|=2\). That is, for every \(i,j\in[n]\) and \(i\neq j\), we have

\[|\operatorname{supp}(\mathbf{a}_{i})\cup\operatorname{supp}(\mathbf{a}_{j})| >\operatorname{rank}(\operatorname{overlap}(\mathbf{A}_{\{i,j\}}))+| \operatorname{supp}(\mathbf{a}_{i})|\,,\]

where we set \(i\) as the target index without loss of generality.

Because \(\operatorname{rank}(\operatorname{overlap}(\mathbf{A}_{\{i,j\}}))\geq 1\), we have

\[|\operatorname{supp}(\mathbf{a}_{i})\cup\operatorname{supp}(\mathbf{a}_{j})| >1+|\operatorname{supp}(\mathbf{a}_{i})|\,.\] (21)

Suppose \(|\operatorname{supp}(\mathbf{a}_{i})|\geq|\operatorname{supp}(\mathbf{a}_{j})|\). Eq. (21) implies that \(\operatorname{supp}(\mathbf{a}_{j})\) is not a subset of \(\operatorname{supp}(\mathbf{a}_{i})\). Similarly, if \(|\operatorname{supp}(\mathbf{a}_{j})|\geq|\operatorname{supp}(\mathbf{a}_{i})|\), we could also show that \(\operatorname{supp}(\mathbf{a}_{i})\) is not a subset of \(\operatorname{supp}(\mathbf{a}_{j})\). Thus, Assumption 6 is satisfied.

According to Proposition 9, we have Assumption 6\(\implies\) Assumption 1, which completes the proof of the first part, i.e., Assumption 4\(\implies\) Assumption 6\(\implies\) Assumption 1.

We now provide an example of mixing matrix satisfying the Assumption 1 that does not satisfy Assumption 4. Suppose the mixing matrix \(\tilde{\mathbf{A}}\) has a support as follows:

\[\boldsymbol{\xi}_{\tilde{\mathbf{A}}}=\begin{bmatrix}\times&0&0\\ \times&\times&0\\ \times&0&\times\end{bmatrix}\]

Clearly, Assumption 1 is satisfied since every pair of columns differ on more than one entry. However, for \(k=1\) and \(\mathcal{I}=\{1,2\}\), we have

\[\left|\bigcup_{k^{\prime}\in\mathcal{I}}\operatorname{supp}(\mathbf{a}_{k^{ \prime}})\right|-\operatorname{rank}(\operatorname{overlap}(\mathbf{A}_{ \mathcal{I}}))\leq\left|\bigcup_{k^{\prime}\in\mathcal{I}}\operatorname{supp}( \mathbf{a}_{k^{\prime}})\right|=3.\]

Since \(|\operatorname{supp}(\mathbf{a}_{k})|=|\operatorname{supp}(\mathbf{a}_{1})|=3\), it is not possible for Eq. (3) to hold for \(\tilde{\mathbf{A}}\) when \(k=1\) and \(\mathcal{I}=\{1,2\}\). Thus, Assumption 4 is violated. The proof of part (b) is finished.

### Proof of Theorem 3

**Theorem 3**.: _For mixing matrix \(\mathbf{A}\), we have the following chain of chain of implications:_

\[\text{Assumption 5}\implies\text{Assumption 6}\implies\text{Assumption 1}.\]

_Furthermore, there exists a matrix \(\mathbf{A}\) satisfying Assumption 1 that does not satisfy Assumption 5._

Proof.: We first prove the contrapositive of Assumption 5\(\implies\) Assumption 6. Suppose that Assumption 6 does not hold. This means that there exist distinct indices \(i,j\in[n]\), such that \(\operatorname{supp}(\mathbf{a}_{i})\) is a subset of \(\operatorname{supp}(\mathbf{a}_{j})\).

Now, for any set of row indices \(\mathcal{I}\subset[n]\), consider the intersection over the supports of all rows \(j\in\mathcal{I}\), denoted by \(\bigcap_{j\in\mathcal{I}}\operatorname{supp}(\mathbf{a}_{j,\cdot})\). Because \(\operatorname{supp}(\mathbf{a}_{i})\) is a subset of \(\operatorname{supp}(\mathbf{a}_{j})\), it is clear that \(i\) cannot be the only element in this intersection. Therefore, it is impossible to satisfy \(\bigcap_{j\in\mathcal{I}}\operatorname{supp}(\mathbf{a}_{j,\cdot})=\{i\}\) for any choice of \(\mathcal{I}\), which indicates that Assumption 5 is violated.

According to Proposition 9, we have Assumption 6\(\implies\) Assumption 1, which completes the proof of the first part, i.e., Assumption 5\(\implies\) Assumption 6\(\implies\) Assumption 1.

We now provide an example of mixing matrix satisfying the Assumption 1 that does not satisfy Assumption 5. Suppose the mixing matrix \(\tilde{\mathbf{A}}\) has a support as follows:

\[\boldsymbol{\xi}_{\tilde{\mathbf{A}}}=\begin{bmatrix}\times&0&0\\ \times&\times&0\\ \times&0&\times\end{bmatrix}\]

Clearly, Assumption 1 is satisfied since the supports of each pair of columns differ on more than one entry. However, Assumption 5 is violated since there does not exist any set of rows such that the intersection of their nonzero indices is \(2\) or \(3\). 

### Proof of Theorem 4

**Theorem 4** (Equivalent Formulations).: _Suppose \(\tilde{\mathbf{A}}=(\mathbf{I}-\tilde{\mathbf{B}})\tilde{\mathbf{\Omega}}^{- \frac{1}{2}}\). Then, we have:_

1. _Let_ \((\hat{\mathbf{B}},\hat{\mathbf{\Omega}})\) _be a solution to Problem (_4_). Then,_ \(\hat{\mathbf{A}}\coloneqq(\mathbf{I}-\hat{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}\) _is a solution to Problem (_1_)._
2. _Let_ \(\hat{\mathbf{A}}\) _be a solution to Problem (_1_). Then, there exist matrices_ \(\hat{\mathbf{B}}\in\mathbb{R}_{\operatorname{off}}^{n\times n}\) _and_ \(\hat{\mathbf{\Omega}}\in\operatorname{diag}(\mathbb{R}_{>0}^{n})\) _such that_ \(\hat{\mathbf{A}}\sim(\mathbf{I}-\hat{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}\)_, and_ \((\hat{\mathbf{B}},\hat{\mathbf{\Omega}})\) _is a solution to Problem (_4_)._

Proof.: We consider both parts of the statements.

**Part (a):**

We provide a proof by contrapositive. For matrices \(\hat{\mathbf{B}}\) and \(\hat{\mathbf{\Omega}}\), suppose \(\hat{\mathbf{A}}\coloneqq(\mathbf{I}-\hat{\mathbf{B}})\hat{\mathbf{\Omega}}^{ -\frac{1}{2}}\) is not a solution to Problem (1). That is, there exists matrix \(\tilde{\mathbf{A}}\) such that

\[\|\tilde{\mathbf{A}}\|_{0}<\|\hat{\mathbf{A}}\|_{0}\] (22)

and

\[\ddot{\tilde{\mathbf{A}}}\ddot{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\ddot{ \mathbf{A}}^{\top}.\] (23)

By Lemma 3, matrix \(\ddot{\tilde{\mathbf{A}}}\) is non-singular, and thus, by Lemma 5, there exist matrices \(\ddot{\mathbf{B}}\in\mathbb{R}_{\operatorname{off}}^{n\times n}\) and \(\ddot{\mathbf{\Omega}}\in\operatorname{diag}(\mathbb{R}_{>0}^{n})\) such that

\[\ddot{\tilde{\mathbf{A}}}\sim(\mathbf{I}-\ddot{\mathbf{B}})\ddot{\mathbf{ \Omega}}^{-\frac{1}{2}}.\]

Lemma 6 implies

\[\|\hat{\mathbf{A}}\|_{0}=\|\hat{\mathbf{B}}\|_{0}+n\quad\text{and}\quad\| \ddot{\mathbf{A}}\|_{0}=\|\ddot{\mathbf{B}}\|_{0}+n,\]

which, with Inequality (22), indicate \(\|\ddot{\mathbf{B}}\|_{0}<\|\hat{\mathbf{B}}\|_{0}\). Furthermore, using Eq. (23) and the assumption \(\tilde{\mathbf{A}}=(\mathbf{I}-\tilde{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}\), we have

\[(\mathbf{I}-\ddot{\mathbf{B}})\tilde{\mathbf{\Omega}}^{-1}(\mathbf{I}-\ddot{ \mathbf{B}})^{\top}=(\mathbf{I}-\tilde{\mathbf{B}})\tilde{\mathbf{\Omega}}^{ -1}(\mathbf{I}-\ddot{\mathbf{B}})^{\top}.\]

Therefore, \((\ddot{\mathbf{B}},\ddot{\mathbf{\Omega}})\) satisfies the constraint of Problem (4) and leads to a smaller zero norm for the objective function, and thus \((\dot{\mathbf{B}},\dot{\mathbf{\Omega}})\) will never be a solution of Problem (4).

#### Part (b):

Let \(\hat{\mathbf{A}}\) be a solution to Problem (1). By Lemma 3, matrix \(\hat{\mathbf{A}}\) is non-singular, and thus, by Lemma 5, there exist matrices \(\hat{\mathbf{B}}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\) and \(\hat{\mathbf{\Omega}}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\) such that

\[\hat{\mathbf{A}}\sim(\mathbf{I}-\hat{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}.\]

It then remains to prove that \((\hat{\mathbf{B}},\hat{\mathbf{\Omega}})\) is a solution to Problem (4), which we do so by contradiction. Suppose that \((\hat{\mathbf{B}},\hat{\mathbf{\Omega}})\) is not a solution to Problem (4). That is, there exists solution \((\tilde{\mathbf{B}},\tilde{\mathbf{\Omega}})\) such that

\[\|\tilde{\mathbf{B}}\|_{0}<\|\hat{\mathbf{B}}\|_{0}\] (24)

and

\[(\mathbf{I}-\tilde{\mathbf{B}})\hat{\mathbf{\Omega}}^{-1}(\mathbf{I}-\tilde{ \mathbf{B}})^{\top}=(\mathbf{I}-\tilde{\mathbf{B}})\hat{\mathbf{\Omega}}^{-1} (\mathbf{I}-\tilde{\mathbf{B}})^{\top}.\] (25)

Define \(\check{\mathbf{A}}\coloneqq(\mathbf{I}-\tilde{\mathbf{B}})\tilde{\mathbf{ \Omega}}^{-\frac{1}{2}}\). Lemma 6 implies \(\|\check{\mathbf{A}}\|_{0}=\|\check{\mathbf{B}}\|_{0}+n\) and \(\|\check{\mathbf{A}}\|_{0}=\|\check{\mathbf{B}}\|_{0}+n\), which, with Inequality (24), indicates \(\|\check{\mathbf{A}}\|_{0}<\|\check{\mathbf{A}}\|_{0}\). Furthermore, using Eq. (25) and the assumption \(\tilde{\mathbf{A}}=(\mathbf{I}-\tilde{\mathbf{B}})\hat{\mathbf{\Omega}}^{- \frac{1}{2}}\), we have

\[\check{\mathbf{A}}\check{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\check{ \mathbf{A}}^{\top}.\]

Therefore, \(\check{\mathbf{A}}\) satisfies the constraint of Problem (1) and leads to a smaller zero norm for the objective function, and thus \(\hat{\mathbf{A}}\) will never be a solution of Problem (1), which is a contradiction. 

### Proof of Theorem 5

In this section, we first provide the proofs of Propositions 10 and 11, which together straightforwardly imply Theorem 5. Before proving Proposition 10, we state the following lemma from Shimizu et al. [38] that is useful for the proof.

**Lemma 11** (Shimizu et al. [38, Lemma 1]).: _Let \(\mathbf{A}\) be a lower triangular matrix with all diagonal entries being nonzero. Let \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) be two permutation matrices. Then, a permutation of rows and columns of \(\mathbf{A}\), i.e., \(\mathbf{P}_{1}^{\top}\mathbf{A}\mathbf{P}_{2}\), has only nonzero entries in the diagonal if and only if the row and column permutations are equal, i.e., \(\mathbf{P}_{1}=\mathbf{P}_{2}\)._

We now provide the proof of Proposition 10.

**Proposition 10**.: _Suppose \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) for matrices \(\mathbf{A}\in\mathbb{R}^{n\times n}\), \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\), and \(\mathbf{\Omega}\in\mathrm{diag}(\mathbb{R}_{>0}^{n})\). Then, \(\mathbf{A}\) satisfies Assumption 2 if and only if matrix \(\mathbf{B}\) represents a DAG._

Proof.: Without loss of generality, we consider the case in which \(\mathbf{A}\) and \((\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) differ only in column permutations, instead of signed column permutations. This is because, for the latter case, there exists a diagonal matrix \(\mathbf{D}\) with diagonal entries being \(\pm 1\) such that \(\mathbf{A}\mathbf{D}\) and \((\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\) differ only in column permutations, and furthermore, \(\mathbf{A}\mathbf{D}\) satisfies Assumption 2 if and only if \(\mathbf{A}\) satisfies Assumption 2.

Therefore, suppose there exists a permutation matrix \(\mathbf{P}_{1}\) such that

\[\mathbf{A}=(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\mathbf{P}_{1}.\] (26)

We now consider both parts of the statements.

#### If part:

Suppose that matrix \(\mathbf{B}\) represents a DAG. Then, there exists permutation matrix \(\mathbf{P}_{2}\) such that \(\mathbf{P}_{2}^{\top}\mathbf{B}\mathbf{P}_{2}\) is strictly lower triangular. Therefore, \(\mathbf{P}_{2}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{P}_{2}=\mathbf{I}- \mathbf{P}_{2}^{\top}\mathbf{B}\mathbf{P}_{2}\) is lower triangular. This implies that \(\mathbf{P}_{2}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}} \mathbf{P}_{2}\) is lower triangular because \(\mathbf{\Omega}^{-\frac{1}{2}}\) is a diagonal matrix and does not affect the support. By substituting Eq. (26), \(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{1}^{-1}\mathbf{P}_{2}\) is lower triangular. Clearly, \(\mathbf{P}_{1}^{-1}\mathbf{P}_{2}\) is also a permutation matrix. Therefore, matrix \(\mathbf{A}\) can be permuted by row and column permutations to be lower triangular, and thus satisfies Assumption 2.

#### Only if part:

Suppose matrix \(\mathbf{A}\) satisfies Assumption 2. Then, there exist permutation matrices \(\mathbf{P}_{2}\) and \(\mathbf{P}_{3}\) such that \(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3}\) is lower triangular. Substituting Eq. (26), \(\mathbf{P}_{2}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}} \mathbf{P}_{1}\mathbf{P}_{3}\) is lower triangular.

Since \(\mathbf{P}_{1}\mathbf{P}_{3}\) is also permutation matrix, this indicates that \((\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}}\), and thus \(\mathbf{I}-\mathbf{B}\), satisfy Assumption 2. By Lemma 8, \(\mathbf{I}-\mathbf{B}\) is non-singular, which indicates that

\[\det(\mathbf{I}-\mathbf{B})\neq 0.\]

Note that

\[\det(\mathbf{\Omega}^{-\frac{1}{2}})>0\quad\text{and}\quad\det(\mathbf{P}_{1} )=1.\]

With Eq. (26), we have

\[\det(\mathbf{A})\neq 0,\]

and therefore

\[\det(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3})\neq 0.\]

It is known that the determinant of a lower triangular matrix is the product of its diagonal entries. Since \(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3}\) is lower triangular and \(\det(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3})\neq 0\), all diagonal entries of \(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3}\) must be nonzero. Now define

\[\mathbf{P}_{4}\coloneqq\mathbf{P}_{2}^{-1}\quad\text{and}\quad\mathbf{P}_{5} \coloneqq\mathbf{P}_{3}^{-1}\mathbf{P}_{1}^{-1},\] (27)

both of which are permutation matrices. By some algebraic manipulations of Eq. (26) and further substituting the above definitions, we have

\[\mathbf{P}_{4}^{\top}(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3})\mathbf{ P}_{5}=(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}},\]

where all diagonal entries are nonzeros. Applying Lemma 11 w.r.t. matrix \(\mathbf{P}_{2}^{\top}\mathbf{A}\mathbf{P}_{3}\), we have

\[\mathbf{P}_{4}=\mathbf{P}_{5},\]

which, by plugging into Eq. (27), implies

\[\mathbf{P}_{2}=\mathbf{P}_{1}\mathbf{P}_{3}.\]

Since we have shown that \(\mathbf{P}_{2}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}} \mathbf{P}_{1}\mathbf{P}_{3}\) is lower triangular, further substitution of \(\mathbf{P}_{2}=\mathbf{P}_{1}\mathbf{P}_{3}\) indicates that \(\mathbf{P}_{2}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{\Omega}^{-\frac{1}{2}} \mathbf{P}_{2}\) is lower triangular. Since right multiplication of \(\mathbf{\Omega}^{-\frac{1}{2}}\) does not affect the support of \(\mathbf{I}-\mathbf{B}\), we see that

\[\mathbf{P}_{2}^{\top}(\mathbf{I}-\mathbf{B})\mathbf{P}_{2}=\mathbf{I}- \mathbf{P}_{2}^{\top}\mathbf{B}\mathbf{P}_{2}\]

is also lower triangular. This indicates that \(\mathbf{P}_{2}^{\top}\mathbf{B}\mathbf{P}_{2}\) is lower triangular, which, with the assumption that the diagonal entries of \(\mathbf{B}\) are zeros, imply that \(\mathbf{P}_{2}^{\top}\mathbf{B}\mathbf{P}_{2}\) is strictly lower triangular. Therefore, matrix \(\mathbf{B}\) represents a DAG. 

After proving Proposition 10, we now consider Proposition 11. Before that, we provide a result by Ghassami et al. [20] that is useful for the proof. We first describe the notion of parent exchange by Ghassami et al. [20]. Let \(\triangle\) be the symmetric difference operator that identifies the elements present in either of the sets but not in the intersection. For DAG \(\mathcal{G}\) with weighted adjacency matrix \(\mathbf{B}\), its vertices \(x_{i}\) and \(x_{j}\) are said to be _parent exchangeable_ if \(|\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp} ((\mathbf{I}-\mathbf{B})_{j})|=1\), i.e., there exists \(k\in[n]\) such that \(\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp} ((\mathbf{I}-\mathbf{B})_{j})=\{k\}\). In such case, a support rotation can be performed on columns \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{i}\) and \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{j}\) that sets a nonzero entry on those columns, except \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{i,i}\) and \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{j,j}\), to zero. In other words, the parent of \(x_{i}\) and \(x_{j}\) that corresponds to the zeroed entry is removed. Furthermore, the entry \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{k,i}\) or \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{k,j}\) is set to \(\times\), which corresponds to adding the missing edge \(x_{k}\to x_{i}\) or \(x_{k}\to x_{j}\). Ghassami et al. [20] defined such an operation to be a _parent exchange_. We then provide the following corollary that is straightforwardly derived from Ghassami et al. [20, Corollary 2 & Proposition 1].

**Corollary 2** (Ghassami et al. [20, Corollary 2]).: _DAGs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are Markov equivalent if and only if there exists a sequence of parent exchanges that maps \(\mathcal{G}_{1}\) to \(\mathcal{G}_{2}\), and one that maps \(\mathcal{G}_{2}\) to \(\mathcal{G}_{1}\)._

We now provide the proof of Proposition 11.

**Proposition 11**.: _Let \(\mathcal{G}\) be a DAG with weighted adjacency matrix \(\mathbf{B}\). Then, matrix \(\mathbf{I}-\mathbf{B}\) satisfies Assumption 1 if and only if the Markov equivalence class of \(\mathcal{G}\) is a singleton._

Proof.: We consider both parts of the statements.

#### If part:

We provide a proof by contrapositive. Suppose that matrix \(\mathbf{I}-\mathbf{B}\) does not satisfy Assumption 2. That is, there exist \(i,j\in[n]\) and \(i\neq j\) such that

\[|\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp }((\mathbf{I}-\mathbf{B})_{j})|<1.\]

Clearly, we have \(\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\neq\operatorname{supp}(( \mathbf{I}-\mathbf{B})_{j})\), because otherwise there will be a cycle with length of two over variables \(x_{i}\) and \(x_{j}\), which contradicts the assumption that \(\mathcal{G}\) is a acyclic. This implies

\[|\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp }((\mathbf{I}-\mathbf{B})_{j})|=1.\] (28)

By definition, \(x_{i}\) and \(x_{j}\) are parent exchangeable, and therefore there exists a parent exchange that maps DAG \(\mathcal{G}\) to another directed graph \(\mathcal{G}^{\prime}\) where \(\mathcal{G}^{\prime}\neq\mathcal{G}\). We now show by contradiction that \(\mathcal{G}^{\prime}\) is a DAG. Suppose that \(\mathcal{G}^{\prime}\) is a not DAG. By Eq. (28), there exists variable \(x_{k}\) such that

\[\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp }((\mathbf{I}-\mathbf{B})_{j})=\{k\}.\]

Here, we must have \(k=i\) or \(k=j\), because otherwise we have

\[(\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{i,i}=(\boldsymbol{\xi}_{\mathbf{I }-\mathbf{B}})_{j,j}=(\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{i,j}=( \boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{j,i}=\times,\]

which leads to a cycle over variables \(x_{i}\) and \(x_{j}\). Without loss of generality, we consider the case of \(k=j\), which implies

\[\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp }((\mathbf{I}-\mathbf{B})_{j})=\{j\}\] (29)

and \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{j,i}=0\). This indicates that the entry \((\boldsymbol{\xi}_{\mathbf{I}-\mathbf{B}})_{j,i}\) is set to \(\times\) (i.e., the edge \(x_{j}\to x_{i}\) is added) after the parent exchange to DAG \(\mathcal{G}^{\prime}\), which subsequently leads to a cycle in \(\mathcal{G}^{\prime}\). In this case, there must exist a path \(x_{i}\to x_{l_{1}}\to\dots\to x_{l_{m}}\to x_{j}\) in DAG \(\mathcal{G}\), where \(m<n-2\) and \(l_{1},\dots,l_{m}\in[n]\setminus\{i,j\}\), to which adding the edge \(x_{j}\to x_{i}\) leads to a cycle in DAG \(\mathcal{G}^{\prime}\). By Eq. (29), \(x_{l_{m}}\) is also a parent of \(x_{i}\), indicating that there exists a cycle \(x_{i}\to x_{l_{1}}\to\dots\to x_{l_{m}}\to x_{i}\) in DAG \(\mathcal{G}\), which contradicts the assumption that \(\mathcal{G}\) is acyclic. Therefore, \(\mathcal{G}^{\prime}\) must be a DAG.

Since there exists a parent exchange that maps DAG \(\mathcal{G}\) to another DAG \(\mathcal{G}^{\prime}\), clearly there also exists a (reversed) parent exchange that maps DAG \(\mathcal{G}^{\prime}\) back to DAG \(\mathcal{G}\). By Corollary 2, DAGs \(\mathcal{G}\) and \(\mathcal{G}^{\prime}\) are Markov equivalent. Therefore, the Markov equivalence class of \(\mathcal{G}\) contains at least two DAGs and is not a singleton.

#### Only if part:

Suppose that matrix \(\mathbf{I}-\mathbf{B}\) satisfies Assumption 2. That is, for all \(i,j\in[n]\) and \(i\neq j\),we have

\[|\operatorname{supp}((\mathbf{I}-\mathbf{B})_{i})\triangle\operatorname{supp }((\mathbf{I}-\mathbf{B})_{j})|>1.\]

In this case, every pair of vertices are not parent exchangeable, and thus parent exchange cannot be applied for any pair of vertices in DAG \(\mathcal{G}\). Therefore, for any DAG \(\mathcal{G}^{\prime}\neq\mathcal{G}\), there exists no sequence of parent exchanges that maps \(\mathcal{G}\) to \(\mathcal{G}^{\prime}\), implying that they are not Markov equivalent. This indicates that all DAGs are not Markov equivalent to DAG \(\mathcal{G}\), except itself, and thus the Markov equivalence class of \(\mathcal{G}\) is a singleton. 

With Propositions 10 and 11 in place, we provide the proof of Theorem 5.

**Theorem 5**.: _Suppose \(\mathbf{A}\sim(\mathbf{I}-\mathbf{B})\boldsymbol{\Omega}^{-\frac{1}{2}}\) for matrices \(\mathbf{A}\in\mathbb{R}^{n\times n}\), \(\mathbf{B}\in\mathbb{R}_{\mathrm{off}}^{n\times n}\), and \(\boldsymbol{\Omega}\in\operatorname{diag}(\mathbb{R}_{>0}^{n})\). Then, \(\mathbf{A}\) satisfies Assumptions 1 and 2 if and only if \(\mathbf{B}\) represents a DAG whose Markov equivalence class is a singleton._

Proof.: We consider both parts of the statements.

#### If part:

Suppose matrix \(\mathbf{B}\) represents a DAG whose Markov equivalence class is a singleton. By Proposition 10, matrix \(\mathbf{A}\) satisfies Assumption 2. Furthermore, Proposition 11 implies that \(\mathbf{I}-\mathbf{B}\), and thus \((\mathbf{I}-\mathbf{B})\boldsymbol{\Omega}^{-\frac{1}{2}}\), satisfy Assumption 1. Since \(\mathbf{A}\) and \((\mathbf{I}-\mathbf{B})\boldsymbol{\Omega}^{-\frac{1}{2}}\) differ only in signed column permutations, and Assumption 1 involves only pairwise comparison of the support matrix, \(\mathbf{A}\) must also satisfy Assumption 1.

#### Only if part:

Suppose \(\mathbf{A}\) satisfies Assumptions 1 and 2. By Proposition 10, matrix \(\mathbf{B}\) represents a DAG. Since \(\mathbf{A}\) and \((\mathbf{I}-\mathbf{B})\boldsymbol{\Omega}^{-\frac{1}{2}}\) differ only in signed column permutations, and Assumption 1 involves only pairwise comparison of the support matrix, \(\mathbf{I}-\mathbf{B}\) also satisfies Assumption 1. By Proposition 11, the Markov equivalence class of the DAG represented by \(\mathbf{B}\) is a singleton.

### Proof of Lemma 1

Before proving Lemma 1, we first provide another result that is useful for the proof. To ease further reasoning, we define the function

\[f(\mathbf{A})=\operatorname{tr}\left(\sum_{k=1}^{n}(\mathbf{A}\odot\mathbf{A})^{ k}\right),\]

and clearly we have

\[g(\mathbf{A})\equiv f(\operatorname{off}(\mathbf{A})).\] (30)

Zheng et al. [48], Zhang et al. [47] have shown that, for any matrix \(\mathbf{A}\), \(f(\mathbf{A})=0\) if and only if \(\mathbf{A}\) represents the weighted adjacency matrix of a DAG. Also, it is known that the weighted adjacency matrix of a directed graph can be permuted via simultaneous equal row and column permutations to be strictly lower triangular if and only if the graph is a DAG. Therefore, we provide the following corollary that is straightforwardly implied by the results by Zheng et al. [48], Wei et al. [45].

**Lemma 12** (Wei et al. [45, Theorem I]).: _For any matrix \(\mathbf{A}\), \(f(\mathbf{A})=0\) if and only if it can be permuted via simultaneous equal row and column permutations to be strictly lower triangular._

We now provide the proof for Lemma 1.

**Lemma 1**.: _For any matrix \(\mathbf{A}\), \(g(\mathbf{A})=0\) if and only if it can be permuted via simultaneous equal row and column permutations to be lower triangular._

Proof.: We first define \(\mathbf{D}_{\mathbf{A}}\) as a diagonal matrix of the same size as matrix \(\mathbf{A}\), where its diagonal entries are equal to those of \(\mathbf{A}\) and its non-diagonal entries are zero. Clearly, we have \(\mathbf{A}=\operatorname{off}(\mathbf{A})+\mathbf{D}_{\mathbf{A}}\). For any matrix \(\mathbf{P}\), This implies

\[\mathbf{P}^{\top}\mathbf{A}\mathbf{P}=\mathbf{P}^{\top}\operatorname{off}( \mathbf{A})\mathbf{P}+\mathbf{P}^{\top}\mathbf{D}_{\mathbf{A}}\mathbf{P}.\] (31)

#### If part:

Suppose that there exists permutation matrix \(\mathbf{P}\) such that \(\mathbf{P}^{\top}\mathbf{A}\mathbf{P}\) is lower triangular. By Eq. (31), we have \(\mathbf{P}^{\top}\operatorname{off}(\mathbf{A})\mathbf{P}=\mathbf{P}^{\top} \mathbf{A}\mathbf{P}-\mathbf{P}^{\top}\mathbf{D}_{\mathbf{A}}\mathbf{P}\). Clearly, the diagonal entries of \(\mathbf{P}^{\top}\mathbf{A}\mathbf{P}\) are exactly the same as those of \(\mathbf{P}^{\top}\mathbf{D}_{\mathbf{A}}\mathbf{P}\), which cancel out each other. Therefore, the diagonal entries of \(\mathbf{P}^{\top}\operatorname{off}(\mathbf{A})\mathbf{P}\) are zeros, indicating that it is strictly lower triangular. By Lemma 12, this implies \(f(\operatorname{off}(\mathbf{A}))=0\), and thus \(g(\mathbf{A})=0\) by Eq. (30).

#### Only if part:

Suppose \(g(\mathbf{A})=f(\operatorname{off}(\mathbf{A}))=0\). By Lemma 12, there exists permutation matrix \(\mathbf{P}\) such that \(\mathbf{P}^{\top}\operatorname{off}(\mathbf{A})\mathbf{P}\) is strictly lower triangular. Clearly, \(\mathbf{P}^{\top}\mathbf{D}_{\mathbf{A}}\mathbf{P}\) is a diagonal matrix. By Eq. (31), \(\mathbf{P}^{\top}\mathbf{A}\mathbf{P}\) is lower triangular, i.e., \(\mathbf{A}\) can be permuted via simultaneous equal row and column permutations to be lower triangular. 

### Proof of Proposition 5

**Proposition 5**.: _The matrix \(\mathbf{A}\) satisfies Assumption 2 if and only if there is a matrix \(\hat{\mathbf{A}}\) such that it is a column permutation of \(\mathbf{A}\) and that \(g(\hat{\mathbf{A}})=0\)._

Proof.: We consider both parts of the statements.

#### If part:

Suppose that there exists matrix \(\hat{\mathbf{A}}\) such that it is a column permutation of \(\mathbf{A}\) and that \(g(\hat{\mathbf{A}})=0\). By definition, there exists permutation matrix \(\mathbf{P}_{2}\) such that \(\hat{\mathbf{A}}=\mathbf{A}\mathbf{P}_{2}\). Also, by Lemma 1, there exists permutation matrix \(\mathbf{P}_{1}\) such that \(\mathbf{P}_{1}^{\top}\hat{\mathbf{A}}\mathbf{P}_{1}\) is lower triangular, which implies that \(\mathbf{P}_{1}^{\top}\mathbf{A}\mathbf{P}_{2}\mathbf{P}_{1}\) is lower triangular. Clearly, \(\mathbf{P}_{2}\mathbf{P}_{1}\) is also a permutation matrix. Therefore, matrix \(\mathbf{A}\) can be permuted by row and column permutations to be lower triangular, indicating that it satisfies Assumption 2.

#### Only if part:

Suppose that matrix \(\mathbf{A}\) satisfies Assumption 2, i.e., there exist permutation matrices \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) such that \(\mathbf{P}_{1}^{\top}\mathbf{A}\mathbf{P}_{2}\) is lower triangular. Defining \(\mathbf{P}_{3}\coloneqq\mathbf{P}_{2}\mathbf{P}_{1}^{-1}\), which is also a permutation matrix,and substituting it into the previous statement, we see that \(\mathbf{P}_{1}^{\top}\mathbf{A}\mathbf{P}_{3}\mathbf{P}_{1}\) is lower triangular. Further substitution of \(\hat{\mathbf{A}}\coloneqq\mathbf{A}\mathbf{P}_{3}\) implies that \(\mathbf{P}_{1}^{\top}\hat{\mathbf{A}}\mathbf{P}_{1}\) is lower triangular, which, by Lemma 1, indicates \(g(\hat{\mathbf{A}})=0\). Clearly, \(\hat{\mathbf{A}}\) is a column permutation of \(\mathbf{A}\). 

### Proof of Theorem 6

**Theorem 6** (Alternative Formulation of Identifiability).: _Suppose that the true mixing matrix \(\tilde{\mathbf{A}}\) satisfies Assumptions 1, 2, and 3. Let \(\hat{\mathbf{A}}\) be a solution of the following problem:_

\[\min_{\mathbf{A}\in\mathbb{R}^{n\times n}}\|\mathbf{A}\|_{0}\quad\mathrm{ subject\ to}\quad\mathbf{A}\mathbf{A}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{ \top}\quad\mathrm{and}\quad g(\mathbf{A})=0.\] (5)

_Then, we have \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\)._

Proof.: Let \(\hat{\mathbf{A}}\) be a solution to Problem (5). Suppose by contradiction that it is not a solution to Problem (2). That is, there exists matrix \(\hat{\mathbf{A}}\) satisfying Assumption 2 such that

\[\|\tilde{\mathbf{A}}\|_{0}<\|\hat{\mathbf{A}}\|_{0}\quad\mathrm{and}\quad \tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}=\tilde{\mathbf{A}}\tilde{\mathbf{A }}^{\top}.\] (32)

By Proposition 5, there exists permutation matrix \(\mathbf{P}\) such that \(g(\tilde{\mathbf{A}}\mathbf{P})=0\). Furthermore, by Eq. (32), we have

\[\|\tilde{\mathbf{A}}\mathbf{P}\|_{0}<\|\hat{\mathbf{A}}\|_{0}\quad\mathrm{and} \quad(\tilde{\mathbf{A}}\mathbf{P})(\tilde{\mathbf{A}}\mathbf{P})^{\top}= \tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}.\]

Therefore, \(\tilde{\mathbf{A}}\mathbf{P}\) satisfies the constraint of Problem (5) and leads to a smaller zero norm for the objective function, and thus \(\hat{\mathbf{A}}\) will never be a solution of Problem (5), which is a contradiction. Therefore, \(\hat{\mathbf{A}}\) must be a solution to Problem (2). Since matrix \(\tilde{\mathbf{A}}\) also satisfies Assumptions 1, 2, and 3, applying Theorem 1 completes the proof. 

### Proof of Theorem 7

**Theorem 7** (Likelihood-Based Method).: _Suppose that the true mixing matrix \(\tilde{\mathbf{A}}\) satisfies Assumptions 1, 2, and 3. Let \(\hat{\mathbf{A}}\) be a solution of Problem (7) with sparsity regularizer \(\rho(\mathbf{A})=0.5\|\mathbf{A}\|_{0}\log T\). Then, we have \(\hat{\mathbf{A}}\sim\tilde{\mathbf{A}}\) in the large sample limit._

Proof.: First, we have \(g(\hat{\mathbf{A}})=0\) and, in the large sample limit, \(\tilde{\mathbf{\Sigma}}=\tilde{\mathbf{\Sigma}}\). Similar to BIC [36], the likelihood term dominates in the large sample limit as the weight of the likelihood function increases much faster than that of the sparsity regularizer. Therefore, in the large sample limit, we have \(\tilde{\mathbf{A}}\hat{\mathbf{A}}=\tilde{\mathbf{\Sigma}}=\tilde{\mathbf{A }}\tilde{\mathbf{A}}\). Also, for any matrix \(\mathbf{A}\) that satisfies \(\mathbf{A}\mathbf{A}=\tilde{\mathbf{A}}\tilde{\mathbf{A}}\) and \(g(\mathbf{A})=0\), the sparsity regularizer indicates \(\|\hat{\mathbf{A}}\|_{0}\leq\|\mathbf{A}\|_{0}\), because otherwise \(\hat{\mathbf{A}}\) will never be a solution of Problem (7). This implies that \(\hat{\mathbf{A}}\) is also a solution to Problem (5). Since matrix \(\tilde{\mathbf{A}}\) also satisfies Assumptions 1, 2, and 3, applying Theorem 1 completes the proof. 

## Appendix E Supplementary Discussion on Structural Assumptions

### Examples Satisfying Structural Variability Assumption

To illustrate the intuition of the proposed assumption of structural variability (i.e., Assumption 1), we provide several examples on the connective structure from sources to observed variables (which corresponds to the support of mixing matrix) satisfying that assumption, as illustrated in Figure 3.

### Efficient Approach for Verifying Assumption 2

Assumption 2 involves finding a certain combination of row and column permutations for mixing matrix \(\mathbf{A}\), which may at first appear inefficient to verify. We provide a more efficient way to do so, by leveraging the interpretation of our assumptions in the context of causal discovery (see Section 3.4 for detailed discussion). Specifically, we provide the following corollary that is a straightforward consequence of Lemma 2 and Proposition 10, whose proof is omitted.

**Corollary 3** (Verification of Assumption 2).: _Let \(\mathbf{A}\) be a non-singular matrix and \(\mathbf{P}\) be a permutation matrix such that the diagonal entries of \(\mathbf{A}\mathbf{P}\) are nonzero. Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be a directed graph where \(\mathcal{V}=\{v_{1},\ldots,v_{n}\}\) and \(\mathcal{E}=\{v_{j}\to v_{i}:(\mathbf{A}\mathbf{P})_{i,j}\neq 0,i\neq j\}\). Then, directed graph \(\mathcal{G}\) is acyclic if and only if matrix \(\mathbf{A}\) satisfies Assumption 2._Specifically, Corollary 3 implies that it suffices to find a column permutation such that the diagonal entries of the permuted matrix are nonzero. Note that such column permutation is guaranteed to exist as indicated by Lemma 2. We then construct a directed graph \(\mathcal{G}\) based on such permuted matrix and check if \(\mathcal{G}\) contains cycle, e.g., via depth-first search. Instead of searching for a certain combination of row and column permutations, this procedure may be more efficient because it involves only finding specific column permutations.

## Appendix F Supplementary Estimation Details

We provide the estimation details for the methods described in Section 4.2. In our experiments, we use the average log-likelihood \(\frac{1}{T}L(\mathbf{A};\bar{\mathbf{\Sigma}})\) as the objective (instead of \(L(\mathbf{A};\bar{\mathbf{\Sigma}})\) in Eq. (7)) for likelihood-based method. For the sparsity term \(\rho(\mathbf{A})\), we use MCP with hyperparameters \(\lambda=1,\alpha=40\) and \(\lambda=0.1,\alpha=10\) for decomposition-based and likelihood-based methods, respectively.

Furthermore, for both methods, we use the L-BFGS algorithm [11] implemented in SciPy [44] to solve each unconstrained optimization problem of quadratic penalty method. Since the formulations involve solving nonconvex optimization problems, we run L-BFGS with \(30\) random initializations, where each entry of the initial solution \(\mathbf{A}_{0}\) is sampled uniformly at random from \([-0.1,0.1]\). In this case, the final solution is chosen via model selection. For quadratic penalty method, we use \(c_{1}=10^{-5}\) and \(c_{1}=10^{-2}\) for decomposition-based and likelihood-based methods, respectively, and use \(\beta=1.5\) for both methods.

Lastly, we also use a threshold of \(0.01\) to remove small weights in the estimated mixing matrix. We run each of the experiments on \(12\) CPUs and \(8\) GBs of memory.

Computational complexity.To compute the constraint term \(g(\mathbf{A})\), a straightforward approach is to compute each matrix power in \(g(\mathbf{A})\) and then sum their traces up, which requires \(O(n)\) matrix multiplications. In our implementation, we adopt a more efficient approach with computational complexity of \(O(\log n)\) matrix multiplications, inspired by Zhang et al. (2020). The rough idea is to perform exponentiation by squaring (i.e., a procedure similar to binary search) and recursively compute the term \(g(\mathbf{A})\).

Furthermore, each L-BFGS run has a computational complexity of \(O(m^{2}n^{2}+m^{3}+mn^{2}t)\), where \(m\ll n^{2}\) is the memory size and \(t\) is the number of inner iterations of the L-BFGS run. Typically, we have \(t=250\) for each L-BFGS run, and \(125\) iterations for the quadratic penalty method.

## Appendix G Supplementary Experimental Results

In addition to the MCC reported in Section 5, we report the Amari distance to evaluate the identification performance in Figures 4 and 5, respectively.

Figure 3: Graphical representations of examples that satisfy Assumption 1.

Figure 4: Empirical results of Amari distance across different sample sizes. Error bars indicate the standard errors calculated based on \(10\) random trials.

Figure 5: Empirical results of Amari distance across different ratios of Gaussian sources. Error bars indicate the standard errors calculated based on \(10\) random trials.