# LocCa: Visual Pretraining

with Location-aware Captioners

 Bo Wan\({}_{1,3}\)1

Michael Tschannen\({}_{1}\)

Yongqin Xian\({}_{2}\)

Filip Pavetic\({}_{1}\)

Ibrahim Alabdulmohsin\({}_{1}\)

Xiao Wang\({}_{1}\)

Andre Susano Pinto\({}_{1}\)

Andreas Steiner\({}_{1}\)

Lucas Beyer\({}_{1}\)

Xiaohua Zhai\({}_{1}^{\dagger}\)

\({}^{1}\)Google DeepMind, Zurich

\({}^{2}\)Google, Zurich

\({}^{3}\)KU Leuven

Footnote 1: Work done during Google DeepMind internship. \({}^{\dagger}\) Project lead. All authors made significant technical contributions.

###### Abstract

Image captioning was recently found to be an effective pretraining method similar to contrastive pretraining. This opens up the largely-unexplored potential of using natural language as a flexible and powerful interface for handling diverse pretraining tasks. In this paper, we demonstrate this with a novel visual pretraining paradigm, LocCa, that incorporates location-aware tasks into captioners to teach models to extract rich information from images. Specifically, LocCa employs two tasks, bounding box prediction and location-dependent captioning, conditioned on the image pixel input. Thanks to the multitask capabilities of an encoder-decoder architecture, we show that an image captioner can effortlessly handle multiple tasks during pretraining. LocCa significantly outperforms standard captioners on downstream localization tasks, achieving state-of-the-art results on RefCOCO/+/g, while maintaining comparable performance on holistic tasks. Our work paves the way for further exploration of natural language interfaces in visual pretraining.

## 1 Introduction

Remarkable progress has been made in large-scale visual pretraining [1, 2, 3, 4], where vision models are pretrained on large-scale annotated datasets [5, 6, 4] with a supervised classification loss. Yet, the manual annotation required for such datasets is time-consuming and costly, posing a challenge to scalability.

In light of this, the modern contrastive pretraining methods [7, 8] extract learning signals from web-crawled image-text pairwise datasets [9, 10], circumventing the need for extensive manual annotations. The contrastively pretrained models demonstrate remarkable capability on zero-shot transfer tasks, especially on downstream applications that require fine-grained visual [11, 12] or textual understanding [13]. More recently, image captioning has been shown as an alternative visual pretraining task to learn capable vision encoders [14], where an encoder-decoder architecture is pretrained to generate text captions from the image input. Some studies, such as [15, 16], pioneered the joint pretraining of contrastive and generative methods. Typically, encoded image features are fed into two parallel branches: one employs a text encoder to produce sentence embeddings for contrastive learning, while the other utilizes a text decoder to generate image captions. Despite the effectiveness of these works, they typically focus on a holistic understanding of images, often overlooking the region-specific details of the visual content.

The recent success of image captioning [14] and the advancements in multitasking learning of decoders [17, 18, 19] opens up the largely-unexplored potential of using natural language as aflexible and powerful interface for handling diverse tasks. We demonstrate this with a novel visual pretraining paradigm, LocCa, that enhances the visual representation with location-aware context. Works including [20; 21; 22] investigate the matching of image regions with corresponding text during pretraining. The central concept involves extracting Region of Interest (RoI) features from image embedding to facilitate contrastive learning with corresponding textual features. These approaches yield encouraging outcomes in location-sensitive tasks, such as object detection [23; 24; 25; 26] and referring expression [27; 28; 29; 30]. However, they require complex model architectures (e.g. RPN [24] and FPN [25]) for RoI generation. Also, given the presence of multiple object candidates within an image, region-wise matching becomes computationally demanding.

By contrast, LocCa is a simple yet effective location-aware captioning pretraining method as shown in Figure 1, which uses an autoregressive decoder as an interface to handle additional location-aware pretraining tasks. Concretely, other than the image-to-text captioning task, LocCa also pretrains the model with two location-aware tasks: (i) _automatic referring expressions_, which amounts to predict bounding box coordinates from automatically generated captions for specific image regions, and (ii) _grounded captioning_ to jointly predict box coordinates and captions from the image. Specifically, LocCa leverages a multi-task decoder [17] for pretraining, where the model outputs are conditioned on the task prefixes for each task. Thanks to the shared vision transformer for multiple tasks, the additional localization losses are relatively cheap to compute, while the model inference speed is identical to the standard image caption pretrained models.

Our experimental results show that the LocCa performs significantly better on downstream tasks that require localization capabilities, while maintaining the same level of capabilities on holistic tasks. We summarize our contributions as follows: (i) For the first time we explore location-aware tasks as proxies for generative visual pretraining (as opposed to transfer/instruction tuning in prior works), enabling flexibly customized inference (detailed in Sec.3.3); (ii) Without bells and whistles, LocCa achieves state-of-the-art results on localization tasks, while preserving the competitive performance on holistic tasks; and (iii) When integrated in vision-language models, i.e. PaLI-3 [31], the vision encoder outperforms strong SigLIP baselines [13].

## 2 Related Works

Contrastive visual pretraining is a prominent direction in training vision and vision-language foundation models. Early works [32; 33; 34; 35] explore image-only contrastive loss by matching different views of the same image in the self-supervised learning setting. In vision-language model pretraining, CLIP [7] and ALIGN [8] show that a two-tower architecture trained with the contrastive objective on noisy image-text pairs can learn highly transferable image and text representations for various

Figure 1: **Overview of LocCa. LocCa consists of a standard vision transformer and a transformer decoder. The vision transformer takes image pixel as input, produces visual tokens as cross attention input to the transformer decoder. The transformer decoder is trained to read out rich information from the visual tokens. We adopt the following three task for pretraining: Cap, AREF and GCAP.**

downstream tasks. There have been many follow-up works [10; 36; 13; 37] that further improve the zero-shot image classification and retrieval performance. Notably, [20; 21] propose to incorporate location cues by contrastively aligning image regions and text phrases. In contrast, our work focuses on learning a location-aware vision-language model with a generative loss.

A natural alternative to contrastive pretraining is image captioning: Rather than matching image and text embeddings, one tries to predict captions from an image embedding. Early works investigate this approach at small scale [38; 39; 40; 41]. Later works augment large-scale contrastive pretraining with a captioning loss [15; 16; 42], or scale-up captioning as a stand-alone task without investigating transfer to a broad set of vision tasks [43; 44]. [14] recently showed that image captioning alone leads to visual representations competitive with contrastive pretraining.

Many recent large multimodal models are trained with a mixture of tasks [45; 18; 19; 46; 10; 47; 31; 48; 49]. Among the most popular types of tasks are those which can be formulated by mapping an image to a text string, including captioning, detection, and VQA [50; 45; 18; 19; 10; 47; 31; 48]. Another popular group of tasks are dense prediction tasks such as segmentation and depth prediction [19; 46]. While several studies have enhanced model pretraining by incorporating location information, their methodologies primarily leverage either pretrained language decoders [18; 51; 52] or pretrained cross-modal encoder-decoder [51] to integrate vision and language features for multitasking purposes, often neglecting the independent significance of visual pretraining from scratch. Furthermore, there is a trend of towards co-training on images, video, and audio [53; 54; 55], highlighting the multifaceted nature of current multi-modal research. Crucially, essentially all of these works rely on pretrained vision and language backbones, and merely fine-tune those together on the described tasks. Here, by contrast, we use multi-task pretraining to train visual encoders from scratch.

## 3 Location-aware Captioner

In this section, we introduce the location-aware image captioner LocCa for multitask pretraining. LocCa builds on an image captioner but provides a recipe for integrating location-aware information during model pretraining.

### Pretraining tasks

The pretraining phase of LocCa draws inspiration from pioneering works that have successfully integrated a unified decoder for multitasking based on pretrained models [18; 10; 19; 46; 52], utilizing a task-specific prefix for each distinct task. This enhances the model's ability to handle multiple tasks concurrently.

For conventional image captioning, the process involves taking an image \(x\) as input and generating a sequence of text tokens \(y=[y_{1},y_{2},\ldots,y_{n}]\). In the LocCa framework, a task-specific prefix, labeled as "_Cap_:", is added to the beginning of the caption sequence to designate the task at hand. Moreover, LocCa integrates two additional location-aware tasks during its pretraining phase: _automatic referring expression_ (AREF) and _grounded captioning_ (GCAP). These tasks are inspired by referring expression comprehension [27; 28; 29; 30] and dense captioning [56; 57; 58; 59; 60] respectively. The key difference is that LocCa predicts both regional captions and box coordinates sequentially with task prefixes, instead of relying on either caption or box conditional inputs (see Fig. 1).

The foundation of LocCa's pretraining is built upon dense, automatically generated region annotations. Each image \(x\) is associated with a comprehensive set of annotations \(\{\{b,c\}\}\), where \(b\in\mathcal{N}^{4}\) denotes the bounding box coordinates, and \(c\) represents the corresponding textual descriptions or labels. For every bounding box, two distinct prompts are generated to cater to the aforementioned location-aware tasks: "_ARef_: \(\{c\}:\{b\}\)" for automatic referring expression and "_GCap_: \(\{b\}:\{c\}\)" for grounded captioning, each prefixed with "_ARef_:" and "_GCap_:" respectively. These prompts are then tokenized to produce the sequence \(y\) for each task, facilitating pretraining with a text interface.

For each image, LocCa utilizes the same visual features extracted by the image encoder and performs three tasks using the same decoder in parallel. This pretraining scheme aims to make LocCa adept at linking fine-grained regional visual elements with appropriate textual descriptions.

### Model details

ArchitectureLocCa utilizes a conventional encoder-decoder framework, where the encoder comprises a Vision Transformer [2] to transform the input image \(x\) into a sequence of feature embeddings. The decoder, built on a Transformer architecture [61], processes these image features, employing cross-attention across each layer to integrate visual and textual information effectively.

Autogressive decodingIn the decoding stage, LocCa uses causal attention masks [61] to guide the prediction of each token in the sequence, ensuring that each token is generated based on the ones before it, in a step-by-step manner. This setup helps in creating coherent and context-aware captions, drawing from the visual cues encoded earlier and maintaining a logical flow in the generated text.

Parallel predictionInspired by [14], LocCa also adopts parallel prediction for a fraction of the training examples (concretely 50%) in the standard image captioning task. This technique requires the model to predict the caption tokens independently in parallel, focusing exclusively on visual information to prevent the model from relying on preceding text tokens to predict the following ones. This strategy was shown to improve the learned representation on a range of tasks [14].

ObjectiveThe optimization of LocCa's parameters \(\theta\) is achieved through the maximization of the log-likelihood: \(\sum_{i=1}^{|y|}\log P_{\theta}(y_{i}|y_{<i},x)\). It is important to emphasize that, in the learning process for the location-aware tasks, LocCa is structured to predict captions and bounding boxes sequentially, contrasting with traditional approaches that might predict a caption based on a given bounding box or vice versa [18; 51]. This is achieved by applying losses to the entire prompt excluding the task prefix. Taking the AREF task as an example, the overall loss is computed for both textual predictions \(c\) and box coordinates \(b\). The loss on \(c\) guides the model to identify a foreground region and generate its caption, while the loss on \(b\) aims at refining the model's ability to accurately regress the box location relative to the caption.

### Discussion

To the best of our knowledge, LocCa is the first end-to-end method that incorporates the location-aware tasks (i.e., AREF and GCAP) into generative VLM _pretraining_. Our key novelty lies in the formulation of the location-aware proxy tasks which allows for scalable pretraining and enhances the visual localization capabilities. Compared to [22; 18; 51; 52] that require either a pretrained visual encoder or language decoder, LocCa does not need any pretrained model for initialization. Compared to [22; 62] that employ complex architectures with multiple losses, LocCa adopts a simple encoder-decoder architecture with a single generative loss. Compared to [18; 19] that are pretrained on a large number of tasks, LocCa introduces the novel use of simple AREF and GCAP proxy tasks for visual pretraining.

The dual-faceted loss structure of AREF and GCAP achieves comparable results to directly adopting the traditional referring expression and dense captioning tasks. However, it enhances inference flexibility of LocCa, allowing for varied input configurations. For instance, users can input a single task prefix (e.g., "_ARef_:") to prompt the model to identify and describe an area of interest along with its location. Alternatively, by inputting both the task and a conditional input (e.g., "_ARef_: a black and white cat :"), LocCa can be directed to focus solely on predicting the location. This flexibility allows for customized responses to various inquiries, highlighting the model's adaptability to meet specific user needs.

## 4 Experiments

### Experimental setup

Pretraining datasetWe use a subset of the WebLI dataset [10] corresponding to English websites and apply text-based filtering [8] to obtain 1B image/alt-text pairs. The WebLI data was de-duplicated w.r.t. all the images in the evaluation data sets used in this paper. To obtain fine-grained object locations, a publicly available OWL-ViT CLIP L/14 model [63] is applied to generate detection pseudo annotations. Specifically, two groups of box categories are generated: the n-gram texts from alt-text and the object categories as used by PaLI [10], more details can be found in [64]. In LocCa pretraining, we first filter the candidate bounding boxes according to their OWL-ViT confidence scorewith a threshold of 0.3, and then randomly sample one box-caption or box-category pair for referring expression and grounded captioning separately.

**Baselines** Our main baselines are CLIP-style contrastively pretrained dual-encoder models [7; 8] on our dataset (referred to as CLIP* to differentiate from the model released by [7]), as well as the captioning-pretrained encoder-decoder models from [14]. For both types of models we follow the exact training recipe from [14]. For the captioning-based pretraining we consider standard autoregressive captioning (Cap) as well as the variant with parallel prediction (CapPa). This variant removes the decoder attention mask for a fraction of the training examples and replaces the decoder input, which corresponds to the right-shifted output sequence during autoregressive training, with a sequence of all mask tokens. Parallel prediction [14] improves the representation learning capabilities captioning-based pretraining at no extra computation cost.

**Implementation details** LocCa adopts an encoder-decoder structure where, unless otherwise noted, the encoder defaults to a standard ViT-L/14 design with 24 transformer blocks handling input image patches of size 14. The decoder, following [14], is a Transformer-L model consisting of 12 transformer decoder blocks. In total, the model comprises approximately 600M parameters. Two more LocCa setups with ViT-B/16 and ViT-G/14 (see [4] for model specifications) are adopted for ablation tests and scaling experiments respectively.

**Pretraining details** LocCa is pretrained for about 9 billion image/alt-text seen examples, which corresponds to about 9 epochs on our tailored subset of WebLI. For the optimizer, we employ the Scaling-ViT AdaFactor variant [4], combined with a cosine schedule that includes 10,000 warmup steps. The batch size is set at 8,192, while the learning rate and decay factor are adjusted to \(10^{-3}\) and \(10^{-4}\), respectively. During this process, images are uniformly resized to a resolution of 224 x 224 pixels. Alt-texts are tokenized into a vocabulary consisting of 32,000 tokens using a sentence piece model trained on the English segment of C4 [65], with a cap on the sequence length at 64 tokens. We represent bounding box coordinates using up to 500 integral numbers, which are then directly converted into strings for straightforward representation of coordinate tokens. For parallel prediction in the vanilla image captioning task, the fraction of examples is set to 50% by default. The pretraining of LocCa\({}_{L}\) takes 153 hours using 256 TPUv3 chips.

**Downstream tasks** Our goal with LocCa is to preserve or even improve the capability on various image-level understanding tasks, and get a higher performance on fine-grained location-aware tasks. To this end, we assess the capabilities of LocCa in holistic and location-aware image understanding tasks across a range of downstream tasks. In the realm of holistic image understanding, we focus on the same LiT-Decoder tasks [17] in CapPa [14] including image classification (CLS)[5; 66; 67; 68; 69], image captioning (CAP) [70; 71], optical character recognition (OCR-VQA) [72], visual question answering (VQA) [73], and graph question answering (GQA) [74]. For evaluating location-aware image understanding, we choose two widely recognized tasks like referring expression comprehension (REC) [27], referring expression segmentation (RES) [75] and object detection (OD) [24]. Additionally, paralleling the approaches of PaLI [10; 31], we integrate LocCa to a pretrained large language model to assess performance on a variety of vision-language tasks, including image captioning and visual question answering. Notably, despite LocCa not being exposed to any video content during pretraining, we adapt it to various video-related tasks, such as video captioning, QA, and classification. This adaptation aims to assess its generalization capabilities to new modalities, demonstrating its versatility compared to other image-text pretraining approaches. We adopt different strategies for transferring LocCa to downstream tasks, as summarized in Appendix A.1.

### Quantitative results

We conduct extensive experiments to evaluate LocCa. The integration of location-aware cues enables LocCa to maintain its performance on holistic image understanding tasks while achieving substantially improved outcomes on location-aware tasks. Further enhanced by an advanced pretrained large language model, LocCa exhibits exceptional performance across a range of vision-language tasks, substantially surpassing baseline models.

**Referring Expression Comprehension** In this section, we present results on the referring expression comprehension benchmarks, including RefCOCO, RefCOCO+ and RefCOCOg [28]. As shown in Table 1, LocCa establishes a new state-of-the-art across these benchmarks. This advancement is particularly noteworthy considering the inherent limitations of previous methods. For example,UNINEXT [77], which adopts a Deformable DETR architecture [84] tailored for detection-based tasks, and OFA, which requires a pretrained BART [85] for initialization, both achieve good results but are constrained by their specialized setups. Besides, some location-aware VLMs, such as Shikra [78] and Ferret [79], require an LLM for knowledge-based reasoning, which increases inference costs. In contrast, LocCa employs a standard encoder-decoder architecture with auto-regressive pretraining from scratch, significantly outperforming these methods across all benchmarks without the need for complex task-specific adaptations.

Achieving good performance on RefCOCO usually requires pretraining at high image resolutions. For instance, OFA\({}_{L}\) is pretrained at 480\({}^{2}\)px, while UNI-NEXT undergoes multi-scale pre-training. We opt for a standard 224\({}^{2}\)px pretrain resolution for simplicity. We transfer the 224\({}^{2}\)px pretrained model to RefCOCO by fine-tuning with 640\({}^{2}\)px resolution, using learning rate \(10^{-4}\) and no weight decay. We report the standard metric Acc@0.5 on the validation and test sets.

Notably, we train on the combined training sets of RefCOCO, RefCOCO+ and RefCOCOg but _removing all validation and test images_ from this combination. The splits of these three datasets are largely overlapping, meaning that methods trained on the combined training sets without de-duplication, a recently common phenomenon, have trained on about half of the test images, see Appendix A.3 for details. We provide the list of removed image IDs in the Appendix A.3. Furthermore, some models such as UNITER [80], VILLA [81], and MDETR [82] use COCO pre-trained detection components which have seen test images from the three RefCOCO versions. We have removed all training, validation, and test images from the COCO dataset from our pre-training data, as well as near-duplicates thereof. Hence, we group models reported in Table 1 into three distinct categories. We transfer LocCa on both the full and the "clean" combined training set and provide both results. We hope that, going forward, the community evaluates models on RefCOCO in this clean setup.

Moreover, as shown in Table 2, LocCa improved significantly over all the baselines of image-text pretrained models. To ensure a fair comparison with contrastive baselines (i.e. CLIP) which lack a text decoder for box prediction, we employ the LiT-Decoder [17] setup for comparison on RefCOCO benchmarks. It involves freezing the pretrained image encoder while training a text decoder from scratch with a small resolution of 224\({}^{2}\)px. This setup is necessary to properly compare with CLIP-style models without a decoder. The performance improvements attributable to location-aware pretraining are evident, demonstrating the enhanced sensitivity of visual encoder to object regions, which is crucial for excelling in referring expression tasks.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{**MODEL**} & \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{3}{c}{RefCOCOg} \\ \cline{2-10}  & val & testA & testB & val & testA & testB & val-u & test-u \\ \hline \multicolumn{10}{l}{_Models that trained on val/test images, see text for more._} \\ \hline PixelLLM[52] & 89.8 & 92.2 & 86.4 & 83.2 & 87.0 & 78.9 & 84.6 & 86.0 \\ UniTAB[76] & 88.59 & 91.06 & 83.75 & 80.97 & 85.36 & 71.55 & 84.58 & 84.70 \\ OFA\({}_{L}\)[18] & 90.05 & 92.93 & 85.26 & 85.80 & 89.87 & 79.22 & 85.89 & 86.55 \\ UNINEXT\({}_{L}\)[77] & 91.43 & 93.73 & 88.93 & 83.09 & 87.90 & 76.15 & 86.91 & 87.48 \\ LocCa\({}_{L}\) & **91.94** & **94.56** & **89.13** & **86.47** & **91.67** & **80.43** & **87.46** & **87.95** \\ \hline OFA\({}_{H}\)[18] & 92.04 & 94.03 & 88.44 & 87.86 & 91.70 & 80.71 & 88.07 & 88.78 \\ UNINEXT\({}_{H}\)[77] & 92.64 & 94.33 & **91.46** & 85.24 & 89.63 & 79.79 & 88.73 & 89.37 \\ ONE- & 92.58 & 94.18 & 89.26 & 88.77 & 92.21 & 83.23 & 89.22 & 89.27 \\ PACE\({}_{1.5B}\)[62] & & & & & & & & \\ Shikra\({}_{13B}\)[78] & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16 \\ Ferret\({}_{13B}\)[79] & 89.48 & 92.41 & 84.36 & 82.81 & 88.14 & 75.17 & 85.83 & 86.34 \\ LocCa\({}_{G}\) & **92.99** & **95.02** & 90.48 & **89.12** & **92.87** & **83.55** & **89.24** & **89.90** \\ \hline \multicolumn{10}{l}{_Models that have seen val/test images during pre-training, see text for more._} \\ \hline UNITER\({}_{L}\)[80] & 81.41 & 87.04 & 74.17 & 75.90 & 81.45 & 66.70 & 74.86 & 75.77 \\ VILLA\({}_{L}\)[81] & 82.39 & 87.48 & 74.84 & 76.17 & 81.54 & 66.84 & 76.18 & 76.71 \\ MDETR[82] & 86.75 & 89.58 & 81.41 & 79.52 & 84.09 & 70.62 & 81.64 & 80.89 \\ \hline \multicolumn{10}{l}{_Models that have never seen val/test images, see text for more._} \\ \hline RefTR[83] & 85.65 & 88.73 & 81.16 & 77.55 & 82.26 & 68.99 & 79.25 & 80.01 \\ LocCa\({}_{L}\) & 89.70 & 92.75 & 85.30 & 83.85 & 89.40 & 76.76 & 84.62 & 85.86 \\ LocCa\({}_{G}\) & **91.20** & **93.34** & **87.56** & **86.89** & **90.71** & **80.73** & **87.34** & **87.90** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Result comparison with previous SOTA methods on RefCOCO benchmarks.

Referring Expression SegmentationFor referring expression segmentation, we extend the REC task by adding a suffix "_Mask_:" to the text, followed by the indexes of segmentation tokens. The segmentation tokens specify the precise shape of the segmentation mask within the bounding box that is identified during the REC task. This process leverages a pretrained VQ-VAE [86] to convert the semantic masks to tokens, please refer to Appendix A.3 for more details.

LocCa is adapted to RES under the same settings as in REC, using the "clean" combined training sets of RefCOCO, RefCOCO+, and RefCOCOg (c.f. Appendix C). Specifically, we compare different frozen encoders and train a decoder from scratch. As shown in Table 2, LocCa's vision encoder outperforms other encoders substantially, thereby providing further validation of its location-wise sensitivity. Moreover, we employ the full encoder-decoder LocCa model and fine-tune it for RES. Notably, LocCa achieves competitive results even compared to the state-of-the-art PaLI-3 model, albeit with considerably fewer parameters (0.6B vs. 5B). See Appendix A.3 for details.

Holistic Image UnderstandingWhile being great on the referring expression comprehension tasks, we also verified that LocCa performs equally well on the holistic image understanding tasks. Interestingly, we found that LocCa outperforms the image-text pretrained baselines on the object-centric tasks like VQAv2 and GQA.

Following the similar evaluation protocol in [14], we evaluate the capability of LocCa with the "LiT decoder" [17] setup, to investigate the adaptation capability of the learned representations to interface with a text decoder. Here we report the classification accuracy on 5 classification (CLS) datasets (ImageNet-1k [5], Sun-397 [67], Food-101 [66], Resisc-45 [68], Oxford-Pet [69]), and also answer accuracy on VQAv2 and GQA and OCR-VQA datasets. Besides, we report the CIDEr score on 2 captioning (CAP) datasets (COCO [70] and Flickr [71]).

As shown in Table 3, the performance of LocCa is better than image-text pretrained baselines on image captioning, VQA and GQA, comparable on image classification, and slightly lags on OCR

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline \multirow{3}{*}{**MODEL**} & \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{3}{c}{RefCOCOg} \\ \cline{3-10}  & & val & testA & testB & val & testA & testB & val-u & test-u \\ \hline \multirow{5}{*}{**LiT**} & CLIP [7] & 65.21 & 71.28 & 58.17 & 57.53 & 66.44 & 47.77 & 59.32 & 60.24 \\  & CLIP* & 58.28 & 63.59 & 53.73 & 49.01 & 55.94 & 41.96 & 55.70 & 55.88 \\  & Cap [14] & 60.64 & 65.47 & 56.17 & 52.56 & 58.32 & 45.99 & 56.75 & 57.99 \\  & CapPa [14] & 64.17 & 69.90 & 58.25 & 56.14 & 63.68 & 48.18 & 58.90 & 59.91 \\  & LocCa & **88.34** & **91.20** & **85.10** & **79.39** & **85.13** & **72.61** & **81.69** & **82.64** \\ \hline \multirow{5}{*}{**LiT**} & CLIP [7] & 36.15 & 38.25 & 35.82 & 31.40 & 36.00 & 28.69 & 29.21 & 29.44 \\  & CLIP* & 32.78 & 34.89 & 33.03 & 27.49 & 30.14 & 25.07 & 26.99 & 26.83 \\ \cline{1-1}  & Cap [14] & 34.84 & 37.68 & 35.39 & 31.93 & 35.24 & 28.79 & 28.84 & 29.11 \\ \cline{1-1}  & CapPa [14] & 36.62 & 39.23 & 36.67 & 32.54 & 37.49 & 29.59 & 30.40 & 30.43 \\ \cline{1-1}  & LocCa & **64.98** & **65.39** & **64.09** & **57.85** & **60.92** & **52.72** & **55.84** & **56.95** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with baselines on RefCOCOs. A randomly initialized decoder is trained for REC and RES, with frozen image encoders. We report Acc@0.5 for REC and mIoU for RES. Here CLIP uses model checkpoints released by [7]; all other baselines use the same data as LocCa.

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline \multirow{2}{*}{**MODEL**} & \multicolumn{3}{c}{Classification} & \multicolumn{3}{c}{Captioning} & OCR & \multicolumn{3}{c}{VQA} \\ \cline{2-11}  & i1k & sun & food & res & pet & COCO & Flickr & VQA & VQAv2 & GQA \\ \hline CLIP [7] & 84.8 & 84.8 & 95.2 & 96.3 & 95.4 & 124.4 & 87.1 & 64.1 & 70.4 & 58.7 \\ CLIP* & 84.7 & **85.7** & 94.6 & **96.4** & 95.2 & 123.2 & 85.5 & 61.3 & 68.5 & 55.3 \\ Cap [14] & 83.8 & 84.7 & 93.4 & 95.1 & 95.0 & 125.9 & 88.3 & 64.2 & 70.9 & 58.5 \\ CapPa [14] & 84.4 & 84.9 & 93.8 & 96.0 & **95.6** & 125.8 & 89.3 & **65.6** & 70.9 & 58.3 \\ LocCa & 84.5 & 84.9 & 93.9 & 96.0 & 95.0 & **127.1** & **90.7** & 64.5 & **72.8** & **61.8** \\ \hline LocCa\({}_{G}\) & 85.8 & 85.1 & 95.4 & 96.1 & 95.8 & 130.9 & 92.6 & 67.6 & 73.9 & 61.5 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on holistic image understanding tasks. Here CLIP uses model checkpoints released by [7]; all other baselines are trained on the same data as LocCa.

VQA. Notably, both VQA and GQA necessitate fine-grained instance-level comprehension, focusing on the spatial and semantic relationships between objects to accurately provide the correct answer. For example, GQA involves complicated information about objects, attributes and relations provided by scene graphs [57]. Such an observation further reveals the advantage of LocCa on fine-grained object-level sensitivity.

It is also interesting to investigate the full potential of the LocCa model by fine-tuning it on holistic tasks with a small \(3\times 10^{-6}\) learning rate without weight decay. To this end, we choose the widely used COCO image captioning task as an example. LocCa achieves competitive results of 138.0 CIDEr score with a standard 224\({}^{2}\)px. When increasing the transfer resolution to 640\({}^{2}\)px, the LocCa\({}_{L}\) model achieves **140.3** CIDEr score without CIDEr metric optimization [87].

Vision-Language Models with LocCaIn this section, we present results on vision-language tasks with LocCa plugged into a pretrained large language model. More specifically, we use PaLI-3 [31] here to test the vision encoder quality. Importantly, we discovered that the generative LocCa vision backbone outperforms the SigLIP backbone, which is the default setup used in PaLI-3.

We select a wide range of tasks to assess the models' proficiency in understanding various visual concepts, including: image scene (COCO Caption), objects (VQAv2 and TallyQA [89]), visually-situated text (TextVQA [90], ST-VQA [91]) and general knowledge (OKVQA [92]). Remarkably, as shown in Table 4, LocCa consistently surpasses the Cap and CapPa vision encoders by a significant margin across all these tasks. Note that SigLIP\({}_{L}\) in Table 4 uses image patch size 16 while all the other models use patch size 14. SigLIP\({}_{L}\) is marked grey because it's not directly comparable. The LocCa\({}_{G}\) model consistently outperforms SigLIP\({}_{G}\) across all the tasks. With the knowledge of object and textual regions, LocCa is better positioned to understand more challenging and subtle

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**MODEL** & COCO & VQAv2 & OKVQA & TextVQA & ST-VQA & TallyQA \\ \hline SigLIP\({}_{L}\)[13] & 135.8 & 75.6 & 57.5 & 41.1 & 46.2 & 74.9/61.4 \\ Cap\({}_{L}\)[14] & 135.0 & 75.3 & 57.5 & 44.6 & 44.5 & 73.2/62.0 \\ CapPa\({}_{L}\)[14] & 135.3 & 75.5 & 57.7 & 44.0 & 45.2 & 73.4/60.9 \\ LocCa\({}_{L}\) & **138.9** & **77.6** & **58.4** & **49.2** & **50.9** & **79.3/64.1** \\ SigLIP\({}_{G}\) & 140.3 & 77.5 & 58.6 & 50.6 & 50.5 & 76.3/61.8 \\ LocCa\({}_{G}\) & **140.9** & **78.1** & **59.8** & **52.1** & **52.3** & **79.0/64.2** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on PaLI-3 [31] transfers to diverse captioning and question answering tasks. LocCa consistently outperforms other image encoders, especially on tasks requiring understanding of objects, including both natural and text (OCR) objects.

concepts like counting, visual relations, and word characters. Further fine-tuning details and the hyperparameters we used are provided in Appendix A.4.

**Object Detection** To evaluate for object detection, we use COCO detection dataset and follow [88] which models the task with an encoder-decoder model that outputs sequences of bounding boxes similar to pix2seq[50]. The model is trained in two phases, first it maximizes the log-likelihood of generating the ground truth sequences of boxes, secondly it uses reinforce to tune the model for a reward related to the mAP metric.

Both [88, 50] pretrain their encoder-decoder model in Objects365 [93] and we found this to be critical. In particular the task output format is different from the ones LocCa used during pretraining and the size of COCO is too small to learn it without overfitting. To measure the performance of the encoders, we opt to initialize the decoder from an Objects365 pretrained model. More details of adapting LocCa for object detection please refer to Appendix A.5.

We present the comparative results in Figure 2, where LocCa significantly outperforms all image-text pretrained baselines in both mAP and AR, at both stages -- before and after the application of reinforcement tuning. This performance aligns with our expectations regarding LocCa's object-centric comprehension, attributed to the integration of additional location-aware pretraining.

**Zero-shot Transfer** Following locked-image tuning [36], we freeze the pre-trained vision encoder and train a text encoder contrastively to perform zero-shot classification and zero-shot retrieval on downstream tasks. With an L/14 architecture and 3B examples LiT-tune duration, LocCa\({}_{L}\) achieves 77.1% 0-shot accuracy on ImageNet, which is competitive to the same size CapPa\({}_{L}\)[14] model's 76.4% accuracy. On COCO retrieval tasks, LocCa\({}_{L}\) achieves 46.6% and 64.6% on text-to-image and image-to-text retrieval tasks respectively. It outperforms CapPa\({}_{L}\)'s 43.9% and 60.3% COCO retrieval results by a large margin. Please refer to Appendix A.6 for more details.

**Semantic Segmentation** We investigate the dense feature learning capabilities of LocCa by evaluating it for semantic segmentation on ADE20k [95] along with Cap and CapPa. To this end, we use the Segmenter framework [94] which attaches a linear layer to every patch embedding to predict the semantic label of that patch, and obtains the high-resolution semantic map by upsampling the low-resolution map. The results in Table 5 show that LocCa outperforms Cap and CapPa by about 2 mIoU points, and the Seg ViT-L baseline which is based on an ImageNet-21k-pretrained ViT-L by about 1 point. The transfer details are provided in Appendix A.7.

**Video Understanding** We follow PaLI-3 [31] to evaluate LocCa and CapPa on video understanding tasks. Specifically, we use the image encoder to process each frame separately and concatenate all resulting tokens. A LiT-Decoder [17] is then tuned on a mixture of six video understanding tasks. Despite no video has been seen during pretraining, LocCa showcases the video understanding capabilities and outperforms CapPa on most of the datasets. In particular, for the video captioning task on VATEX [96] dataset, LocCa achieves a CIDER score of 65.0 vs 64.0 of CapPa. For the video QA task on MSVD [97] dataset, LocCa obtains an accuracy of 50.9 vs 50.0 of CapPa. More details could be found from Appendix A.8.

### Qualitative Results

In this section, we discuss the raw LocCa model's zero-shot capability to detect multiple objects using its own decoder, despite only a single object per example being observed during pretraining. To achieve this, we employ beam search when decoding the output from LocCa. This involves using the prompt of a single task prefix "_GCap_:" to instruct LocCa to predict the RoIs along with their labels. As shown in Appendix Fig. 6, we observe that depending on the setting we get lower number of bounding boxes with correct class names, or higher number of boxes with class names which start to contain noise. We provide more discussions in Appendix B. Nevertheless, pre-trained LocCa model shows powerful capability after a short finetuning on a clean downstream dataset as shown in Table 1. Finding a decoding strategy which can output high number of boxes and quality labels at the same time is an open problem.

\begin{table}
\begin{tabular}{l c} \hline \hline
**MODEL** & mIoU \\ \hline Seg ViT-L [94] & \(50.71\) \\ Cap & \(49.82^{\pm 0.6}\) \\ CapPa & \(49.92^{\pm 0.1}\) \\ \hline LocCa & \(51.81^{\pm 0.3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Fine-tuning vision backbones with a linear head [94] for semantic seg. on ADE20k.

### Ablations

**Pretraining and transfer resolutions**  We present results with different pretrain resolution and transfer resolution combinations in Figure 3 (a). To get a LocCa model with higher 384 pretrain resolution, we finetune the 224 resolution model using the target 384 resolution for 900M examples seen on the same dataset. In Figure 3 (a), we transfer both the 224 resolution model and the 384 resolution model to RefCOCOs using different transfer resolutions (224, 384, 640) as marked on the x-axis. We find that the higher pretrain model achieves slightly higher or competitive results compared to the 224 resolution models. We opt for the 224 resolution pretrain models by default in this paper for simplicity. More results with high pretrain resolution could be found in the Appendix A.10.

**Coordinate tokenization**  Previous studies [18; 51] often tokenize object coordinates by adding a location vocabulary. In contrast, LocCa simplifies this process by directly converting integral coordinates into textual strings, which are then tokenized with the same text tokenizer. This section presents an ablation study on the RefCOCO benchmarks to examine the differences between these two options. As shown in Figure 3 (b), both tokenization strategies for box coordinates yield remarkably similar results. However, our approach is notably simpler and more straightforward.

**Selection of pretraining tasks**  To evaluate the importance of the selected tasks for pretraining LocCa, an ablation study was conducted focusing on the effects of AREF and GCAP tasks. Specifically, the study explored the impact on LocCa by removing these tasks individually and collectively, with the model defaulting to the CapPa baseline when both are excluded. All these models are pretrained on the WebLI split for 900M examples with the resolution of 224\({}^{2}\)px, and subsequently evaluated on the holistic tasks using a LiT-Decoder setup and on RefCOCOs by fine-tuning the whole model. As shown in Table 6, incorporating any location-aware task into the pretraining of LocCa yields significant performance improvements, particularly evident in the RefCOCO results. Interestingly, incorporating solely the GCAP task leads to a marked improvement in RefCOCO performance compared to the CapPa baseline, despite GCAP not being directly aligned with the AREF task. This highlights the importance of introducing object concepts for enhancing regional-level understanding, which is beneficial for fine-grained visual comprehension and applicable to other object-sensitive tasks. Furthermore, the combined inclusion of both AREF and GCAP tasks yields even better results, demonstrating their complementary nature in improving LocCa's performance.

## 5 Conclusion

LocCa introduces a novel visual pretraining paradigm, using natural language interface to construct the proxy location-aware pretrain tasks. This model excels in understanding both the overall scene and specific spatial details, setting new performance standards on location-aware tasks while preserving the capability on holistic image understanding. LocCa simplifies the process of blending location information with visual data, offering significant improvements on tasks requiring detailed spatial awareness. Our contributions pave the way for advanced model capabilities in processing a broad spectrum of vision-language tasks, demonstrating LocCa's versatility and effectiveness.

LocCa already demonstrates superior zero-shot detection capability in qualitative results. However, it lacks the capability for zero-shot segmentation due to the absence of pretraining on pixel-level annotations, and we leave this for future work. Building on the robust foundation of LocCa, a promising direction for future work involves enhancing its pixel-level precision through the incorporation of segmentation tasks during the pretraining phase. This extension aims to equip LocCa with a more nuanced understanding of images, enabling it to discern and interpret intricate details and textures with unparalleled accuracy, further broadening its applicability across diverse vision-language tasks.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{AREF} & \multirow{2}{*}{GCAP} & \multirow{2}{*}{INet} & \multirow{2}{*}{COCO} & \multirow{2}{*}{VQAv2} & \multirow{2}{*}{GQA} & \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} \\ \cline{6-12}  & & & & & val & & testA & testB & val & testA & testB \\ \hline ✓ & ✓ & 80.4 & 117.7 & 68.4 & 59.0 & 88.0 & 90.8 & 84.0 & 80.2 & 84.7 & 73.8 \\ ✓ & ✗ & 79.7 & 115.9 & 67.5 & 57.9 & 86.8 & 89.6 & 83.0 & 77.7 & 83.5 & 71.2 \\ ✗ & ✓ & 79.7 & 114.8 & 67.4 & 57.7 & 83.7 & 87.2 & 80.7 & 72.3 & 77.9 & 66.6 \\ ✗ & ✗ & 78.3 & 111.0 & 66.2 & 54.3 & 75.2 & 78.8 & 69.5 & 64.7 & 71.0 & 55.9 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study on applying loss on AREF and GCAP tasks during training.

## Acknowledgements

We thank Matthias Minderer and Jeremiah Harmsen for their valuable feedback on the manuscript, and Debidatta Dwibedi and Xingyi Zhou for their feedback on dense captioning evaluators. We also thank Paul Voigtlaender for discussions on the referring expression segmentation task. Finally, we thank the Google DeepMind team for providing a supportive research environment. We used the big_vision codebase [98, 99] for all experiments in this project.

## References

* [1] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (BiT): General visual representation learning. In _ECCV_, 2020.
* [2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16\(\times\)16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [3] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, pages 11966-11976, 2022.
* [4] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. _CVPR_, 2022.
* [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* [6] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In _ICCV_, 2017.
* [7] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [8] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.
* [9] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LALON-400M: Open dataset of CLIP-filtered 400 million image-text pairs. _CoRR_, arXiv:2111.02114, 2021.
* [10] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapilyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. _CoRR_, arXiv:2209.06794, 2022.
* [11] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. In _ICLR_, 2022.
* [12] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _CoRR_, arXiv:2303.05499, 2023.
* [13] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In _ICCV_, pages 11941-11952, 2023.

* [14] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. Image captioners are scalable vision learners too. _NeurIPS_, 2023.
* [15] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _Trans. Machine Learning Research_, 2022.
* [16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _ICML_, volume 162, pages 12888-12900, 2022.
* [17] Lucas Beyer, Bo Wan, Gagan Madan, Filip Pavetic, Andreas Steiner, Alexander Kolesnikov, Andre Susano Pinto, Emanuele Bugliarello, Xiao Wang, Qihang Yu, Liang-Chieh Chen, and Xiaohua Zhai. A study of autoregressive decoders for multi-tasking in computer vision. _CoRR_, arXiv:2303.17376, 2023.
* [18] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _ICML_, volume 162, pages 23318-23340, 2022.
* [19] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. UNIFIED-IO: A unified model for vision, language, and multi-modal tasks. In _ICLR_, 2023.
* [20] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In _CVPR_, pages 10955-10965, 2022.
* [21] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. In _NeurIPS_, 2022.
* [22] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In _CVPR_, pages 16793-16803, 2022.
* [23] Ross Girshick. Fast r-cnn. In _ICCV_, pages 1440-1448, 2015.
* [24] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In _NeurIPS_, 2015.
* [25] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _CVPR_, pages 936-944, 2017.
* [26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross B. Girshick. Mask r-cnn. In _ICCV_, pages 2980-2988, 2017.
* [27] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In _EMNLP_, pages 787-798, 2014.
* [28] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. In _ECCV_, 2016.
* [29] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In _CVPR_, 2018.
* [30] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: A survey of methods and datasets. _CoRR_, arXiv:2007.09554, 2020.
* [31] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. Pali-3 vision language models: Smaller, faster, stronger. _CoRR_, arXiv:2310.09199, 2023.

* [32] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.
* A new approach to self-supervised learning. In _NeurIPS_, 2020.
* [34] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, pages 9726-9735, 2020.
* [35] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _CoRR_, arXiv:1807.03748, 2018.
* [36] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In _CVPR_, 2022.
* [37] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. _CoRR_, arXiv:2212.00794, 2022.
* [38] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In _ECCV_, pages 67-84, 2016.
* [39] Ang Li, Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Learning visual n-grams from web data. In _ICCV_, pages 4183-4192, 2017.
* [40] Karan Desai and Justin Johnson. VirTex: Learning visual representations from textual annotations. In _CVPR_, 2021.
* [41] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption annotations. In _ECCV_, pages 153-170, 2020.
* [42] Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, et al. Mammut: A simple architecture for joint learning for multimodal tasks. _CoRR_, arXiv:2303.16839, 2023.
* [43] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In _CVPR_, pages 17980-17989, 2022.
* [44] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In _ICLR_, 2022.
* [45] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language. _CoRR_, arXiv:2205.14100, 2022.
* [46] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized decoding for pixel, image, and language. In _CVPR_, pages 15116-15127, 2023.
* [47] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A. J. Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model. _CoRR_, arXiv:2305.18565, 2023.
* [48] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _CoRR_, arXiv:2304.08485, 2023.

* [49] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-language pre-training with fusion in the backbone. In _NeurIPS_, 2022.
* [50] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey E. Hinton. Pix2seq: A language modeling framework for object detection. In _ICLR_, 2022.
* [51] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. _CoRR_, arXiv:2311.06242, 2023.
* [52] Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, and Cordelia Schmid. Pixel aligned language models. _CoRR_, arXiv:2312.09237, 2023.
* [53] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: transformers for multimodal self-supervised learning from raw video, audio and text. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _NeurIPS 2021_, pages 24206-24221, 2021.
* [54] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. MERLOT RESERVE: neural script knowledge through vision and language and sound. In _CVPR_, pages 16354-16366, 2022.
* [55] Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. In _CVPR_, pages 6598-6608, 2023.
* [56] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In _CVPR_, 2016.
* [57] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _CoRR_, arXiv:1602.07332, 2016.
* [58] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. _CoRR_, arXiv:2212.00280, 2022.
* [59] Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, and Jing Shao. Context and attribute grounded dense captioning. In _CVPR_, 2016.
* [60] Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, and Yusuf Aytar. Flexcap: Generating rich, localized, and flexible captions in images. _CoRR_, arXiv:2403.12026, 2024.
* [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, pages 5998-6008, 2017.
* [62] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. ONE-PEACE: exploring one general representation model toward unlimited modalities. _CoRR_, arXiv:2305.11172, 2020.
* [63] Matthias Minderer, Alexey A. Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection. In _ECCV_, volume 13670, pages 728-755, 2022.
* [64] Matthias Minderer, Alexey A. Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. _CoRR_, arXiv:2306.09683, 2023.
* [65] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _CoRR_, arXiv:1910.10683, 2019.

- mining discriminative components with random forests. In _ECCV_, 2014.
* [67] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio Torralba, and Aude Oliva. SUN database: Exploring a large collection of scene categories. _IJCV_, 119(1):3-22, 2016.
* [68] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. _CoRR_, arXiv:1703.00121, 2017.
* [69] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In _CVPR_, 2012.
* [70] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. _CoRR_, arXiv:1504.00325, 2015.
* [71] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _TACL_, 2:67-78, 2014.
* [72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: Visual Question Answering by Reading Text in Images. In _ICDAR_, 2019.
* [73] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In _CVPR_, 2017.
* [74] Drew A. Hudson and Christopher D. Manning. GQA: a new dataset for compositional question answering over real-world images. In _CVPR_, 2019.
* [75] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In _ECCV_, 2016.
* [76] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In _ECCV_, 2022.
* [77] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In _CVPR_, 2023.
* [78] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.
* [79] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In _ICLR_, 2024.
* [80] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: universal image-text representation learning. In _ECCV_, 2020.
* [81] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. _CoRR_, arXiv:2006.06195, 2020.
* [82] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In _ICCV_, 2021.
* [83] Li Muchen and Sigal Leonid. Referring transformer: A one-step approach to multi-task visual grounding. In _NeurIPS_, 2021.
* [84] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In _ICLR_, 2021.

* [85] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _ACL_, pages 7871-7880, 2020.
* [86] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang Geng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying output space of visual tasks via soft token. In _ICCV_, 2023.
* [87] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Improved image captioning via policy gradient optimization of spider. In _ICCV_. IEEE, 2017.
* [88] Andre Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas Beyer, and Xiaohua Zhai. Tuning computer vision models with task rewards. In _ICML_, 2023.
* [89] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In _AAAI_, 2019.
* [90] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _CVPR_, pages 8317-8326, 2019.
* [91] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, C.V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Scene text visual question answering. In _ICCV_, pages 4290-4300, 2019.
* [92] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _CVPR_, 2019.
* [93] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, October 2019.
* [94] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In _CVPR_, pages 7262-7272, 2021.
* [95] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ADE20k dataset. _IJCV_, 127:302-321, 2019.
* [96] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. VaTeX: A large-scale, high-quality multilingual dataset for video-and-language research. In _ICCV_, October 2019.
* [97] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In _ACM Multimedia_, 2017.
* [98] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big vision. https://github.com/google-research/big_vision, 2022.
* [99] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet-1k. _CoRR_, arXiv:2205.01580, 2022.
* [100] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In _NAACL-HLT_, 2021.
* [101] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 2020.
* [102] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and R. Manmatha. Polyformer: Referring image segmentation as sequential polygon generation. In _CVPR_, pages 18653-18663, 2023.

* [103] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _CVPR_, 2016.
* [104] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _CVPR_, June 2016.
* [105] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video. _CoRR_, arXiv:1702.00824, 2017.
* [106] David F. Fouhey, Wei-cheng Kuo, Alexei A. Efros, and Jitendra Malik. From lifestyle vlogs to everyday interactions. In _CVPR_, June 2018.
* [107] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _CVPR_, 2015.
* [108] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. _arXiv preprint arXiv:2307.03601_, 2023.
* [109] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. In _CVPR_, 2024.

Details on transfer results and ablations

### Transfer to downstream tasks

In this study, we introduce three methodologies to evaluate the performance of the pretrained LocCa across a variety of downstream tasks. The first methodology focuses on assessing the efficacy and generalization capability of the standalone pretrained visual encoder. In this setup, the visual encoder is kept frozen, and we employ two main strategies: (i) Drawing inspiration from Lit-Decoder [17], we train a multi-task decoder from scratch for all downstream tasks. This approach is particularly suited for holistic image understanding and video-related tasks due to the broad range of tasks, allowing for a simplified evaluation process with a single decoder; and (ii) Aligning with the recent advancements [10; 47; 31; 52] that showcase the exceptional results achieved by integrating a pretrained visual encoder with a large language model [100] across a multitude of downstream tasks, we adopt the training strategy outlined in PaLI-3 [31], substituting their visual encoder with our LocCa pretrained encoder.

The second methodology entails assessing the performance of the full LocCa model by fine-tuning it on downstream tasks using a minimal learning rate and weight decay. Given the high cost associated with fine-tuning the entire model, we selectively evaluate its adaptability on just two tasks: image captioning for holistic image understanding and referring expression comprehension for location-aware understanding.

The third methodology involves combining the pretrained LocCa visual encoder with a task-specific pretrained decoder for selected tasks, such as object detection. This approach is designed to leverage the strengths of LocCa's visual encoder in particular scenarios, notably in contexts requiring fine-grained object-centric understanding.

### LiT-Decoder setup for Referring Expression Comprehension and holistic image understanding tasks

For the evaluation of different frozen vision backbones on classification, captioning, and VQA with the protocol from [17] (LiT-Decoder) we rely on the hyper-parameters from [14] (which only differ from [17] in the data mixing strategy).

We also apply this protocol to Referring Expression Comprehension to compare the encoders only (without pretrained decoders), with small modifications. Specifically, besides training on a data mix of the different RefCOCO variants, we reduce the number of decoder layers from 12 to 6, and reduce the learning rate from \(10^{-3}\) to \(3\times 10^{-4}\). Also note that transfer is done at the pretraining resolution of \(224^{2}\)px, unlike the fine-tuning transfers which are done at higher resolution.

### Referring Expression Segmentation

For referring expression segmentation, we extend the REC task by adding a suffix "_Mask_:" to the text, followed by 16 integer numbers corresponding to segmentation tokens. The segmentation tokens specify the precise shape of the segmentation task within the bounding box that is predicted as part of the REC task. We use the vector-quantized variational auto-encoder (VQ-VAE) from [86], which was trained as in [31] on OpenImages data [101]. The VQ-VAE converts a single channel \(64\times 64\) pixel mask into a sequence of 16 integer values from a discrete codebook of 128 tokens. LocCa is trained to predict the 16 segmentation tokens, and the VQ-VAE decoder is then used to convert these tokens to a \(64\times 64\) pixels segmentation mask. This mask is then resized to the predicted box coordinates (with bilinear interpolation), before computing the IoU with the ground truth mask.

In Table 7, we showcase the results of employing the full encoder-decoder LocCa model and fine-tuning it for RES task. It achieves competitive results even compared to the state-of-the-art PaLI-3 model, with considerably smaller model size.

### Vision-Language Models with LocCa

We follow the setup of PaLI-3 [31] ablations for all PaLI-3 transfer evaluations. More specifically, we pretrain PaLI-3 stage 1 (frozen image encoder, \(224^{2}\)px resolution) with batch 16k for 13k steps (i.e. 208 million examples). Then we fine-tune PaLI-3 model on each downstream task separately,keeping image encoder frozen and the same \(224^{2}\) resolution. The details of each task is listed in Table 8.

### Object Detection

To validate the effectiveness of LocCa compared with image-text pretrained baselines on object detection, we employ a ViT-B/16 encoder and pretrain all these models on our WebLI subset for 9B examples with 224\({}^{2}\)px. Subsequently, we integrate the pretrained visual encoder with a 6-layer Objects365 pretrained decoder, adapting the combined model for the COCO detection task. This adaptation processes inputs at a 640\({}^{2}\)px and is designed to predict up to 25 bounding boxes, using a batch size of 256. First we fine-tune the model for 10k steps with learning rate of \(10^{-4}\), employing a standard auto-regressive loss. This is followed by an additional fine-tuning phase focused on maximizing the mAP reward for 20k steps with learning rate of \(10^{-6}\).

### Zero-shot transfer

Following locked-image tuning [36], we freeze the pre-trained vision encoder and train a text encoder contrastively to perform zero-shot classification and zero-shot retrieval on downstream tasks. We take the L/14 vision encoder from a LocCa\({}_{L}\) model, and then train a text encoder from scratch to pair with the frozen L/14 model. Input image is resized to resolution \(224^{2}\)px. The model is trained for 3B seen examples, with a standard \(10^{-3}\) learning rate and \(10^{-4}\) weight decay. Beyond that, we also attach a randomly initiated MAP head [4] to the L/14 vision encoder and finetune the MAP head following the CapPa baseline [14].

### Semantic segmentation on ADE20k

We follow the setup of Segmenter [94] which fine-tunes the vision encoder jointly with a linear head predicting the semantic label for every patch, followed by upsampling. The input image resolution is set to \(512\times 512\), and we apply random resizing with aspect ratio in the range \([0.5,2.0]\), photometric jitter, and random horizontal flipping. We train for 160k steps with batch 16 using Adam with learning rate \(3\times 10^{-5}\) and decoupled weight decay of 0.01. To accommodate variable inference resolution for evaluation, we apply our model in sliding-window fashion at the training resolution.

### Experiments on video understanding

Even though LocCa has never seen any videos during pretraining, its training recipe results in an improvement upon CapPa in many video-related tasks, including captioning, QA, and classification. Here, we follow the setup used in PaLI-3 [31], in which we use the image encoder to process each

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{**MODEL**} & \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{3}{c}{RefCOCOg} \\ \cline{2-9}  & val & testA & testB & val & testA & testB & val-u & test-u \\ \hline PaLI-3[31] & 77.33 & – & – & 72.53 & – & – & 72.72 & – \\ PolyFormer\({}_{L}\)[102] & 76.94 & 78.49 & 74.83 & 72.15 & 75.71 & 66.73 & 71.15 & 71.17 \\ LocCa\({}_{L}\) & 76.98 & 78.25 & 72.90 & 71.25 & 76.52 & 63.67 & 69.51 & 70.44 \\ \hline \hline \end{tabular}
\end{table}
Table 7: mIoU on RefCOCO **segmentation** results when fine-tuning the full LocCa model, and comparison with models from the literature.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**PARAM** & COCO & VQAv2 & OKVQA & TextVQA & ST-VQA & TallyQA \\ \hline steps (k) & 20 & 10 & 5 & 10 & 10 & 10 \\ batch size & 256 & 256 & 128 & 128 & 128 & 128 \\ learning rate & 1e-4 & 1e-4 & 3e-5 & 1e-4 & 1e-4 & 3e-5 \\ weight decay & 1e-4 & 5e-5 & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Details of PaLI-3 [31] transfer evaluations, where the model is fine-tuned for each task separately.

frame separately and concatenate all resulting tokens. Then, we tune a LiT decoder [17] by freezing the pretrained image encoder while training a two-layer text decoder from scratch with resolution 224 on a mixture of six video tasks, each of which is given an equal weight during training; i.e. the same number of examples are seen from each dataset. We use C4 tokenizer [65]. The model is trained for 5K steps, using a maximum text length of 64 tokens and batch size 256. We use label smoothing of 0.1 [103], learning rate \(10^{-3}\) with weight decay \(10^{-4}\), and cosine learning rate schedule with 10% warm-up. Encoder ViT-B/16 is pretrained on 3B examples while ViT-L/16 is pretrained on 9B examples.

The six datasets are: (1) **MSRVTT**[104], a collection of video clips annotated with 20 captions, (2) **VATEX**[96], another collection with 10 captions each, (3) **MSRVTT-QA**[97], which contains QA pairs generated from video descriptions using an automated tool, (4) **MSVD-QA**[97], another collection of QA pairs, (5) **YTBB**[105], a video classification dataset containing 23 classes, and (6) **VLOGS**[106], a multi-label classification containing 23 classes. Of the six datasets, VLOGS is location-related, since the task involves tracking the movement of a hand. Indeed, while LocCA leads to favorable improvements overall over CapPa, especially using ViT-L/16 encoder, the gain is particularly notable in VLOGS as expected. Table 9 summarizes all the results.

**Grounded captioning**

For the grounded captioning task, we follow the standard setup that generates regional captions based on a given bounding box location [58, 108, 109]. We report the grounded captioning results on the Visual Genome dataset and compare them with state-of-the-art methods.

As shown in Table 10, LocCa (0.6B, without LLM), outperforms GPT4RoI [108] (7B, with LLM) on both METEOR and CIDEr scores. METEOR evaluates the precision, recall, and alignment of words between the generated and reference captions, while CIDEr assesses the similarity of n-grams between them. When compared with GLaMM [109] (7B, with LLM), LocCa performs better on METEOR but lags on CIDEr. This difference can be attributed to the relatively simple captions in Visual Genome, which typically consist of only a few words. LocCa uses a simpler decoder (0.3B params only) that closely matches the reference captions in terms of word choice and order, which aligns well with the dataset's simple nature. GLaMM, on the other hand, uses a more complex LLM as its decoder, which likely generates more diverse captions that include n-grams that match the reference captions more effectively.

It is important to note that the pretrained LocCa encoder complements multimodal LLMs (i.e. as a better alternative option to the CLIP encoder), and we anticipate further performance gains on downstream tasks when combining both.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **\# Param** & **mAP** & **METEOR** & **CIDEr** \\ \hline GRIT & 1B & 15.5 & 17.1 & 142 \\ GPT4RoI & 7B & - & 17.4 & 145.2 \\ GLaMM & 7B & - & 19.7 & **180.5** \\ \hline LocCa\({}_{L}\) & 0.6B & **34.5** & **20.7** & 157.0 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Grounded captioning results on the Visual Genome dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{**MODEL**} & \multicolumn{4}{c}{Captioning} & \multicolumn{4}{c}{QA} & \multicolumn{2}{c}{Classification} \\ \cline{2-9}  & \multicolumn{2}{c}{MSRVTT} & \multicolumn{2}{c}{VATEX} & \multicolumn{2}{c}{MSRVTT-QA} & \multicolumn{2}{c}{MSVD-QA} & \multicolumn{2}{c}{YTBB} & \multicolumn{2}{c}{VLOGS} \\  & C & B & C & B & A & A & A & J \\ \hline CapPa/B & \(47.4_{\pm 3}\) & \(41.8_{\pm 3}\) & \(49.7_{\pm 2}\) & \(28.9_{\pm 0}\) & \(37.7_{\pm 1}\) & \(45.6_{\pm 9}\) & \(94.6_{\pm 1}\) & \(52.7_{\pm 2}\) \\ LocCa/B & \(47.6_{\pm 4}\) & \(41.8_{\pm 0}\) & \(49.7_{\pm 2}\) & \(29.1_{\pm 0}\) & \(37.9_{\pm 1}\) & \(44.3_{\pm 4}\) & \(94.2_{\pm 2}\) & \(\mathbf{54.0_{\pm 3}}\) \\ \hline CapPa/L & \(55.6_{\pm 3}\) & \(47.1_{\pm 1}\) & \(64.0_{\pm 2}\) & \(36.2_{\pm 1}\) & \(39.9_{\pm 0}\) & \(50.0_{\pm 3}\) & \(94.8_{\pm 1}\) & \(58.0_{\pm 3}\) \\ LocCa/L & \(55.1_{\pm 2}\) & \(47.3_{\pm 4}\) & \(\mathbf{65.0_{\pm 4}}\) & \(\mathbf{36.8_{\pm 1}}\) & \(\mathbf{40.5_{\pm 2}}\) & \(\mathbf{50.9_{\pm 5}}\) & \(94.7_{\pm 0}\) & \(\mathbf{58.6_{\pm 1}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Video evaluation results with LocCa-pretrained vision encoder. We use CIDEr [107] (C) and BLEU-4 (B) for captioning, matching accuracy (A) for QA and YTBB, and the Jackard index (J) for VLOGS. LocCa improves upon CapPa, particularly when using the larger ViT-L/16 encoder.

[MISSING_PAGE_EMPTY:21]

Figure 6: Visualizations of LocCa\({}_{L}\)’s zero-shot predictions on Visual Genome[57]: from left to right we increase noise when sampling. The top rows show all the boxes without non-maximum suppression and the bottom rows show captions for boxes passing the non-maximum suppression filter. We can see that adding more noise increases the variety and amount of decoded boxes and at the same time degrades the quality of captions.

More Visualization Results

We provide visualizations of LocCa\({}_{L}\)'s zero-shot predictions on Visual Genome[57] in Fig. 6. We have two notable observations: (i) When comparing the top rows to the bottom rows, LocCa excels at identifying foreground regions, yet the box regions exhibit significant overlap without non-maximum suppression. This is because only one single object per example is observed during LocCa pretraining, leading to the selection of RoIs that are highly random. (ii) From left to right we increase noise during sampling, which increases the variety and quantity of decoded boxes, while simultaneously degrades the quality of captions. We hypothesize that box sampling demands higher noise levels to explore to explore more diverse RoIs, whereas text sampling requires lower noise levels to ensure the generated texts are highly semantic relevant.

## Appendix C Duplicate image ids for RefCOCOs

Recent studies in referring expression comprehension [52; 76; 77; 18] typically train their models using the combined training sets of RefCOCO, RefCOCO+, and RefCOCOg. However, the splits of these three datasets largely overlap, implying that methods trained on more than half of the test images. In Table 11, we present the image ratio of validation and test splits of RefCOCOs that overlap with the combined training sets. Adhering to the general principle, we provide a list of image IDs in the training set of RefCOCO benchmarks that are duplicated with validation and test images.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{3}{c}{RefCOCOg} \\ \cline{2-9}  & val & testA & testB & val & testA & testB & val-u & test-u \\ \hline Ratio & 61.2\% & 60.5\% & 65.1\% & 61.2\% & 60.5\% & 65.1\% & 48.8\% & 48.3\% \\ \hline \hline \end{tabular}
\end{table}
Table 11: The image ratio of validation and test splits of RefCOCOs that overlap with the combined training sets.

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We discussed the main contribution and scope of LocCa in abstract and the last paragraph of Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed this in Sec. 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: [NA] Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We discussed implementation details and pretraining details in Sec. 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: LocCa is pretrained on a subset of the WebLI dataset, which consists of image-text pairs and is not publicly accessible. Nonetheless, we provide details on how to prepare a pretraining dataset, applicable to any public image-text datasets such as LAION[9]. The code will be released soon. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provided detailed description of the datasets, model size, optimization, and inference details in our Experiment section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For most experiments we ran 3 seeds and report the mean but the results show marginal variability. For video understanding, we present the mean and standard deviation in Table 9. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We discussed this in Sec 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms with the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This work belongs to a large body of work on vision-language pretraining from web image/text data. The same potential positive and negative impacts as for prior work apply, and these were discussed in-depth there, see, for example [59]. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: The main focus of this work is on visual representation learning, rather than vision/language generation. Furthermore, we do not plan to release model checkpoints or data. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We rely on widely used academic data sets and referenced all of them according to established standards. Further, we carefully cited the source of our baseline CLIP and compared with it in Sec. 4.1. _Baselines_. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not rely on crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.