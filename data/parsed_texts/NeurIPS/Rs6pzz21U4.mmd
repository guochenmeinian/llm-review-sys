# A Partially Supervised Reinforcement Learning Framework for Visual Active Search

Anindya Sarkar Nathan Jacobs Yevgeniy Vorobeychik

{anindya, jacbsn, yvorobeychik}@wustl.edu,

Department of Computer Science and Engineering

Washington University in St. Louis

###### Abstract

Visual active search (VAS) has been proposed as a modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hot spots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. We develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.

## 1 Introduction

Consider a scenario where a child is abducted and law enforcement needs to scan across hundreds of potential regions from a helicopter for a particular vehicle. An important strategy in such a search and rescue portfolio is to obtain aerial imagery using drones that helps detect a target object of interest (e.g., the abductor's car) [1; 2; 3; 4; 5]. The quality of the resulting photographs, however, is generally somewhat poor, making the detection problem extremely difficult. Moreover, security officers can only inspect relatively few small regions to confirm search and rescue activity, doing so sequentially.

We can distill some key generalizable structure from this scenario: given a broad area image (often with a relatively low resolution), sequentially query small areas within it (e.g., by sending security officers to the associated regions, on the ground) to identify as many target objects as possible. The number of queries we can make is typically limited, for example, by budget or resource constraints. Moreover, query results (e.g., detected search and rescue activity in a particular region) are _highly informative about the locations of target objects in other regions_, for example, due to spatial correlation. We refer to this general modeling framework as _visual active search (VAS)_. Numerous other scenarios share this broad structure, such as identification of drug or human trafficking sites, anti-poaching enforcement activities, identifying landmarks, and many others. Sarkar et al. [6]recently proposed a visual active search (VAS) framework for geospatial exploration, as well as a deep reinforcement learning (DRL) approach for learning a search policy. However, the efficacy of this DRL approach remains limited by its inability to adapt to search tasks that differ significantly from those in the training data, with the DRL approach struggling in such settings even when combined with test-time adaptation methods.

To gain intuition about the importance of domain adaptivity in VAS, consider Figure 1. Suppose, we pre-train a policy by leveraging a fully annotated search tasks with _large vehicle_ as a target class to learn a search policy. Now the challenge in visual active search problem is how to utilize such a policy for our current task, that is to search for _small car_ given a broad aerial image. As depicted in Figure 1, an adaptive policy (right) initially makes a mistake but then quickly adapts to the current task by efficiently leveraging information obtained in response to queries to learning from its mistakes. In contrast, a non-adaptive policy (left) keeps repeating its mistakes and ultimately fails to find the region containing the target object.

Indeed, traditional active search approaches have been designed precisely with such adaptivity in mind [7; 8; 9; 10] by combining an explicit machine learning model that predicts labels over inputs with custom algorithmic approaches aiming to balance exploration (which improves prediction efficacy) and exploitation (to identify as many actual objects of interest as possible within a limited budget). However, as Sarkar et al. [6] have shown, such approaches perform poorly in VAS settings compared to DRL, despite the lack of effective domain adaptivity of the latter.

We combine the best of traditional active search and the state-of-the-art DRL-based VAS approach by developing a partially-supervised reinforcement learning framework for VAS (PSVAS). A PSVAS policy architecture is comprised of two module: (i) A _task-specific prediction module_ that learns to predict the locations of the target object based on the task aerial image and labels resulting from queries during the search process, and (ii) a _task-agnostic search module_ that learns a search policy given the predictions provided by the prediction module and prior search results. The key advantage of this decomposition is that it enables us to use supervised information _observed at decision time_ to update the parameters of the prediction module, without changing the search module. Furthermore, to learn search policies that are effective in the context of evolving predictions during the search, we propose a novel meta-learning approach to jointly learning the search module with the _initialization_ parameters of the prediction module (used at the beginning of each task). Finally, we generalize the original VAS framework to allow for multiple simultaneous queries (a common setting in practice), and develop the PSVAS framework and a meta-learning approach for such settings.

In summary, we make the following contributions:

* A novel partially supervised reinforcement learning (PSVAS) framework that enables effective adaptation of VAS search policies to out-of-distribution search tasks.
* A novel meta-learning approach (MPS-VAS) to learn initialization parameters of the prediction module jointly with a search policy that is robust to the evolving predictions during a search task.
* A generalization of the VAS problem to domains in which we can make multiple simultaneous queries, and a variant of MPS-VAS that learns how to choose a subset of queries to make in each search iteration.
* An extensive experimental evaluation on two publicly available satellite imagery datasets, xView and DOTA, in a variety of unknown target settings, demonstrating that the proposed approaches significantly outperform all baselines. Our code is publicly available at this link.

## 2 Preliminaries

We consider a generalization of the _visual active search (VAS)_ problem proposed by Sarkar et al. [6]. The basic building block of VAS is a _task_, which centers around an aerial image \(x\) divided into

Figure 1: Comparative search strategy of non-adaptive and adaptive policy with _small car_ as a target.

\(N\) grid cells, so that \(x=(x^{(1)},x^{(2)},...,x^{(N)})\), with each grid cell a subimage. Broadly, the goal is to identify as many target objects of interest through iterative exploration of these grid cells as possible, subject to a budget constraint \(\mathcal{C}\). To this end, we represent the subset of grids containing the target object by associating each grid cell \(j\) with a binary label \(y^{(j)}\in\{0,1\}\), where \(y^{(j)}=1\) if the grid cell \(j\) contains the target object, and 0 otherwise. The complete label vector associated with the task is \(y=(y^{(1)},y^{(2)},...,y^{(N)})\). When we are faced with the task at decision time, we have no direct information about \(y\), but when we query a grid cell \(j\), we obtain the ground truth label \(y^{(j)}\) for this cell. Moreover, if \(y^{(j)}=1\), we also accrue utility from exploring \(j\). In the original variant of VAS, we can make a single query \(j\) in each time step. Here, we consider a natural generalization where we have \(R\) query resources (for example, \(R<N\) patrol units identifying traps in a wildlife conservation setting), so that we can make \(R\) queries in each time step. We assume that \(R\) is constant for convenience; it is straightforward to generalize our approach below if \(R\) is time-varying.

Let \(c(j,k)\) be the cost of querying grid cell \(k\) if we start in grid cell \(j\). For the very first query, we can define a dummy initial grid cell \(d\), so that cost function \(c(d,k)\) captures the initial query cost. Let \(q_{t}^{r}\) denote the set of queries performed in step \(t\) by a query resource \(r\). Our ultimate goal is to solve the following optimization problem:

\[\begin{split}&\max_{\{q_{t}^{r}\}}U(x;\{q_{t}^{r}\})\equiv\sum_{t }y^{(q_{t}^{r})}\\ &\text{s.t.}:\sum_{t\geq 0}\sum_{r=1}^{R}c\big{(}q_{t-1}^{r},q_{t}^{ r}\big{)}\leq\mathcal{C}.\end{split}\] (1)

Finally, we assume that we possess a collection of tasks (in this case, aerial images) for which we have annotated whether each grid cell contains the target object or not. This collection, which we will refer to as \(\mathcal{D}=\{(x_{i},y_{i})\}\), is comprised of images \(x_{i}\) with corresponding grid cell labels \(y_{i}\), where each \(x_{i}\) is composed of \(N\) elements \((x_{i}^{(1)},x_{i}^{(2)},\dots,x_{i}^{(N)})\) representing the cells in the image, and each \(y_{i}\) contains \(N\) corresponding labels \((y_{i}^{(1)},y_{i}^{(2)},\dots,y_{i}^{(N)})\).

The central technical goal in VAS is to learn an effective search policy that maximizes the total number of targets discovered, on average, for a sequence of tasks \(x\) on which we have no prior direct experience, given a set of resources \(R\), exploration cost function \(c(j,k)\), and total exploration budget \(\mathcal{C}\). Sarkar et al. [6] proposed a deep reinforcement learning (DRL) approach in which they learned a search policy \(\pi(x,o,B)\) that outputs at each time step \(t\) the grid we should explore at the next step \(t+1\), given the task \(x\), remaining budget \(B\), and information produced from the sequence of previous queries encoded into an observation vector \(o\) with \(o^{(j)}=2y^{(j)}-1\) if \(j\) has been explored, and \(o^{(j)}=0\) otherwise. The reward function for this DRL approach is naturally captured by \(R(x,o,j)=y^{(j)}\).

Note that active search (including VAS) is qualitatively distinct from active learning [11; 12; 13]: in the latter, the goal is solely to learn to predict well, so that the entire query process serves the goal of improving predictions. In active search, in contrast, we aim to learn a search policy that balances exploration (improving our ability to predict where target objects are) and exploitation (actually finding such objects) within a limited budget. Indeed, it has been shown that active learning approaches are not competitive in the VAS context [6].

## 3 Partially-Supervised Reinforcement Learning Approach for VAS

The DRL approach for VAS proposed by Sarkar et al. [6] is end-to-end, producing a policy that is only partly adaptive to observations \(o\) made during exploration for each task. In particular, this end-to-end policy aims to capture the tension between exploration and exploitation fundamental in active search [7; 9], without explicitly representing the central aim of exploration, which is to improve our ability to _predict_ which grid cells in fact contain the target object. In conventional active search, in contrast, this is directly represented by learning a prediction function \(f(x^{(j)})\) that is updated each time a grid \(j\) is explored during the search process, so that exploration directly impacts our ability to predict target locations, and thereby make better query choices in the future. While the end-to-end approach implicitly learns this, it would only do so effectively so long as tasks \(x\) we face at prediction time are closely related to those used in training. However, it does not take advantage of the _supervised_ information we obtain about which grid cells actually contain the target object, either at training or test time, potentially reducing the efficacy of learning as well as ability to adapt when task distribution changes.

To address this issue, we propose a novel _partially-supervised reinforcement learning_ approach for visual active search, which we refer to as PSVAS. In PSVAS, the search policy we obtain is a composition of two modules: 1) the task-specific prediction module \(f_{\theta}(x,o)\) and 2) the task-agnostic search module \(g_{\phi}(p,o,B)\), where \(\theta\) and \(\phi\) are trainable parameters, where \(p=f_{\theta}(x,o)\) is the vector of predicted probabilities with \(p^{(j)}\) the predicted probability of a target in grid cell \(j\) (see Figure 2, and Supplement Section A for further details about the policy network architecture). Conceptually, \(f_{\theta}\) makes predictions based solely on the task \(x\) and the prediction-relevant information gathered during the search \(o\), while \(g\) relies _solely_ on the information relevant to the search itself: predicted locations of objects \(p\), observations acquired during the search \(o\), and remaining budget \(B\). The search policy is then the composition of these modules, \(\pi(x,o,B)=g_{\phi}(f_{\theta}(x,o),o,B)\).

Although in principle we can still train the policy \(\pi\) above end-to-end (effectively collapsing the composition), a key advantage of PSVAS is that it enables us to directly make use of supervised information about true labels \(y\) as they are observed either at training or decision time, using these to update \(f_{\theta}\) in both cases. In prior work that relies solely on end-to-end policy representation, there was no straightforward way to take advantage of this information. Specifically, we train the policy using the following objective function:

\[\mathcal{L}_{\text{PARVS}}=\mathcal{L}_{RL}+\lambda\mathcal{L}_{\text{{BCE}}},\]

where \(\lambda\) is a hyperparameter. We represent \(\mathcal{L}_{RL}\) loss as follows:

\[\nabla\mathcal{L}_{RL}=-\sum_{i=1}^{M}\sum_{t=1}^{T_{i}}\mathds{1}_{\Sigma_{t =0}\,c(q_{t-1},q_{t})\in\mathcal{C}}\nabla\log\pi_{(\theta,\phi)}(a_{i}^{t}|x_ {i},o_{i}^{t},B_{i}^{t})\mathbb{R}_{i}^{t}\]

Where \(M\) is the number of example search task seen during training and \(\mathbb{R}_{i}^{t}\) is the discounted cumulative reward defined as \(\mathbb{R}_{i}^{t}=\sum_{k=t}^{T}\gamma^{k-t}R_{i}^{k}\) with a discount factor \(\gamma\in[0,1]\). Note that the RL loss in this approach _also updates the parameters of the prediction module_, \(\theta\), in an end-to-end fashion. We also represents \(\mathcal{L}_{\text{{BCE}}}\) as follows: \(\mathcal{L}_{\text{{BCE}}}=\sum_{i=1}^{M}-(y_{i}\log(p_{i})+(1-y_{i})\log(1-p_ {i}))\).

The PSVAS algorithm is more formally presented in Algorithm 1. The combined RL and supervised loss yields a balance between using supervised information to improve the quality of initial predictions at the beginning of the episode and ensuring that these serve the goal of producing the best episode-level policies. However, it is still crucial to note that \(f_{\theta}\) serves solely an _instrumental_ role in the process, with learning a search policy that _effectively adapts to each task_ the primary goal. Consequently, we ultimately wish to jointly learn \(g_{\phi}\) and \(f_{\theta}\) in such a way that \(f_{\theta}\) facilitates adaptive search as it is updated during a search task at decision time. To address this, we propose a meta-learning approach, MPS-VAS, that learns \(g_{\phi}\) along with an initial parametrization \(\theta_{0}\) of \(f_{\theta}\) for each task. At the beginning of each task, then, \(\theta\) is initialized to \(\theta_{0}\), and then updated as labels are observed after each query.

At the high level, MPS-VAS trains over a series of _episodes_, where each episode corresponds to a fully labeled training task \((x_{i},y_{i})\) and budget constraint \(\mathcal{C}\) (we vary the budget constraints during training). We begin an episode \(i\) with the current prediction function parameters \(\theta_{i}=\theta\) and search policy parameters \(\phi\). We fix \(g_{\phi}\) over the length of the episode, and use it to generate a sequence of queries (since the policy is stochastic, it naturally induces exploration). After observing the label \(y^{(j)}\)

Figure 2: The PSVAS policy network architecture.

for each queried grid cell \(j\) during the episode, we update \(f_{\theta_{i}}\) using a standard supervised (binary cross-entropy) loss. At the completion of the episode (once we have exhausted the search budget \(\mathcal{C}\)), we update the policy parameters, as well as the _initialization_ prediction function parameters \(\theta\) by combining RL and supervised loss. For the search module, we use the accumulated sum of rewards \(R_{i}=\sum_{j}y^{(j)}\) over grids \(j\) explored during the episode, with the RL loss \(\mathcal{L}_{RL}\) based on the _REINFORCE_ algorithm [14]. For the prediction module, we use the collected labels \(y^{(j)}\) during the episode and the standard binary cross-entropy loss. Finally, the MPS-VAS loss also explicitly trades off the RL and supervised loss: \(\mathcal{L}_{\text{MLPARVS}}=\big{(}\mathcal{L}_{RL}+\lambda\mathcal{L}_{BCE} \big{)}\). _The proposed meta-learning approach thus explicitly trains the policy to account for the evolution of the prediction during the episode_. The full MPS-VAS is presented more formally in Algorithm 2.

```
0: A search task instance \((x_{i},y_{i})\); budget constraint \(\mathcal{C}\); Prediction function (\(f\)) with current parameters \(\theta_{i}\), i.e., \(f_{\theta_{i}}\); Search policy (\(g\)) with current parameters \(\phi_{i}\), i.e., \(g_{\phi_{i}}\);
1:Initialize\(o^{0}=[0...0]\); \(B^{0}=\mathcal{C}\); step \(t=0\)
2:while\(B^{t}>0\)do
3:\(\tilde{y}=f_{\theta_{i}}(x_{i},o^{t})\)
4:\(j\leftarrow\text{Sample}_{j\in\{\textit{Unexplored Grids}\}}[g_{\phi_{i}}( \tilde{y},o^{t},B^{t})]\)
5: Query grid cell with index \(j\) and observe true label \(y^{(j)}\).
6: Obtain reward \(R^{t}=y^{(j)}\), Update \(o^{t}\) to \(o^{t+1}\) with \(o^{(j)}=2y^{(j)}-1\), and update \(B^{t}\) to \(B^{t+1}\) with \(B^{t+1}=B^{t}-c(k,j)\) (assuming we query \(k\)th grid at \((t-1)\)).
7: Collect transition tuple (\(\tau\)) at step t, i.e., \(\tau^{t}=\) ( state = \((x_{i},o^{t},B^{t})\), action = \(j\), reward = \(R^{t}\), next state = \((x_{i},o^{t+1},B^{t+1})\) ).
8:\(t\leftarrow t+1\)
9:endwhile
10: Update the prediction and search policy parameters, i.e., \(\theta_{i+1}\) and \(\phi_{i+1}\) respectively. ```

**Algorithm 1** The PSVAS algorithm.

The search policy \(\pi\) produces a probability distribution over grid cells. However, since in our setting no advantage can be gained by querying previously queried grid cells, we simply renormalize the probability distribution induced by \(\pi\) over the remaining grid cells, both during training and at decision time. Note that during inference, in both PSVAS and MPS-VAS framework, we freeze the parameters of the search module \((\phi)\) and update the parameters of the prediction module \((\theta)\) after observing query outcomes at each step using \(\mathcal{L}_{\mathit{BCE}}\) loss between predicted label and pseudo label as shown in step \((\ref{eq:model})\) of Algorithm 2.

The discussion thus far effectively assumed that \(R=1\), that is, we make only a single query in each search time step. Next, we describe the generalization of our approach when we have multiple query resources \(R>1\). First, note that the prediction module \(f_{\theta}\) is unaffected, since the number of query resources only pertain to the actual search strategy \(g_{\phi}\). One way to handle \(R\) queries is to simply sample the search module (which is stochastic) iteratively \(R\) times without replacement during training, and to greedily choose the most probable \(R\) grid cells to query at decision time. We refer to this as MPS-VAS-topk. However, since the underlying problem is now combinatorial (the choice of \(R\) queries out of \(N\) grid cells), such a greedy policy may fail to capture important interdependencies among such search decisions.

To address this, we propose a novel policy architecture which is designed to learn how to optimize such a heuristic greedy approach for combinatorial grid cell selection in a way that is non-myopic and accounts for the interdependent effects of sequential greedy choices. Specifically, let \(\psi\) be a vector corresponding to the grid cells, with \(\psi_{j}=1\) if grid cell \(j\) has either been queried in the past (during previous search steps), or has been already chosen to be queried in the current search step, and \(\psi_{j}=0\) otherwise. Thus, \(\psi\) encodes the choices that have already been made, and enables the policy to learn the best next sequential choice to make using a greedy procedure through the same RL framework that we described above. We refer to this approach as MPS-VAS-MQ (in reference to multiple queries).

## 4 Experiments

### Experiment Setup

Since the goal of active search is to maximize the number of target objects identified, we use _average number of targets_ identified through exploration (**ANT**) as our evaluation metric.

We consider two ways of generating query costs: (i) \(c(i,j)=1\) for all \(i,j\), where \(\mathcal{C}\) is just the number of queries, and (ii) \(c(i,j)\) is based on Manhattan distance between \(i\) and \(j\). Most of the results we present in the main paper reflect the Manhattan distance based setting; we also report the results for uniform query costs in the Supplement. We use \(\lambda=0.1\) in all the experiments. We present the details of policy architecture and hyper-parameter details for each different experimental settings in the Supplement.

BaselinesWe compare the proposed PSVAS and MPS-VAS policy learning framework to the following baselines.

* _Random Search (RS)_, in which unexplored grid cells are selected uniformly at random.
* _Conventional Active Search (AS)_ proposed by Jiang et. al. [9], using a low-dimensional feature representation for each image grid from the same feature extraction network as in our approach.
* _Greedy Classification (GC)_, in which we train a classifier \(\psi_{GC}\) to determine whether a grid contains a target object. We then prioritize the search in grids with the highest probability of containing the target object until the search budget is depleted.
* _Active Learning (AL)_, in which the first grid is selected randomly for querying, and the remaining grids are chosen using a state-of-the-art active learning approach by Yoo et al. [13] until the search budget is saturated.
* _Greedy Selection (GS)_, proposed by Uzkent et al. [15], that trains a policy \(\phi_{GS}\) to assign a probability of zooming into each grid cell \(j\). We use this policy to select grids greedily until the search budget \(\mathcal{C}\) is exhausted.
* _End-to-end visual active search (E2EVAS)_, the state-of-the-art approach for VAS proposed by Sarkar et al. [6].

When dealing with a multi-query scenario, we compare the effectiveness of MPS-VAS-topk and MPS-VAS-MQ.

DatasetsWe evaluate the proposed approach using two datasets: xView [16] and DOTA [17]. xView is a satellite imagery dataset which consists of large satellite images representing 60 categories, with approximately 3000 pixels in each dimensions. We use \(67\%\) and \(33\%\) of the large satellite images to train and test the policy network respectively. DOTA is also a satellite imagery dataset. We rescale the original \(3000\times 3000px\) images to \(1200\times 1200px\).

### Single Query Setting

We begin by considering a setting with a single query resource, as in most prior work. We first evaluate the proposed method on the xView dataset with varying search budget \(\mathcal{C}\in\{25,50,75\}\) and the number of equal sized grid cells \(N=49\). We train the policy with _small car_ as target and evaluate the performance of the policy with the following target classes : _Small Car_ (SC), _Helicopter_, _Sail Boat_ (SB), _Construction Cite_ (CC), _Building_, and _Helipad_. As the dataset contains variable size images, we take random crops of \(3500\times 3500\) for \(N=49\), ensuring equal grid cell sizes. We present the results with different values of \(N\) in the Supplement. The results with \(N=49\) are presented in Table 1. We observe significant improvements in performance of the proposed PSVAS approach compared to all baselines in each different target setting, ranging from \(3\) to \(25\%\) improvement relative to the most competitive E2EVAS method. The significance of utilizing supervised information of true labels \(y\), which are observed after each query at inference time, is supported by the obtained results. This highlights the effectiveness of the PSVAS framework, which enables us to update task-specific prediction module \(f\) by leveraging such crucial information in an efficient manner. The experimental outcomes also indicates two consistent trends. In each target setting, the overall search performance improves as \(\mathcal{C}\) increases, and the relative advantage of MPS-VAS over PSVAS increases, as it is better able to exploit the observed outcomes and in turn improve the policy further for the larger search budget \(\mathcal{C}\). We also observe that the extent of improvement in performance is greater when there is a greater difference between the target class used in training and the one used during inference. For example, when the target class is a _sail boat_, the improvement in performance of MPS-VAS in comparison to PSVAS ranges between \(~{}15\%\) to \(35\%\). However, if the target class is a _small car_, the improvement in performance of MPS-VAS is only between \(~{}1\%\) to \(2\%\). Considering all different target settings, performance improvement of MPS-VAS in comparison to PSVAS, ranges from \(1\%\) to \(35\%\). The results demonstrate the efficacy of the MPS-VAS framework in learning a search policy that enables adaptive search. Figure 3 demonstrates the exploration strategies of different policies that are trained on small car as the target class and test on sail boat as the target. The figure showcases the different exploration behaviors exhibited by each policy in response to the target class, highlighting the impact of the proposed _adaptive search_ framework on the resulting exploration strategies. In Table 2, we present similar results on the DOTA dataset with \(N\) as 64. Here, we train the policy with _Large Vehicle_ as target and evaluate the policy with the following target classes: _Ship_, _Large Vehicle_ (LV), _Harbor_, _Helicopter_, _Plane_, and _Roundabout_ (RB). We observe that MPS-VAS approach significantly outperforms all other baseline methods across different target scenarios. This shows the importance of MPS-VAS framework for deploying visual active search when search tasks differ significantly from those that are used for training.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{6}{c}{Test with Helicopter as Target} & \multicolumn{6}{c}{Test with SB as Target} & \multicolumn{6}{c}{Test with Building as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline RS & 0.16 & 0.39 & 0.72 & 0.35 & 0.71 & 1.02 & 2.41 & 3.56 & 4.62 \\ GC & 0.29 & 0.55 & 0.93 & 0.52 & 0.95 & 1.21 & 3.94 & 5.14 & 6.61 \\ GS [15] & 0.41 & 0.68 & 1.08 & 0.61 & 1.03 & 1.26 & 4.51 & 5.80 & 6.82 \\ AL [13] & 0.27 & 0.54 & 0.92 & 0.52 & 0.93 & 1.18 & 3.92 & 5.12 & 6.60 \\ AS [9] & 0.25 & 0.46 & 0.83 & 0.51 & 0.95 & 1.20 & 3.79 & 5.01 & 6.34 \\ E2EVAS [6] & 0.53 & 0.83 & 1.25 & 0.67 & 1.10 & 1.30 & 5.85 & 9.26 & 11.96 \\ OnlineTTA[6] & 0.54 & 0.84 & 1.26 & 0.68 & 1.10 & 1.32 & 5.86 & 9.26 & 11.98 \\ \hline
**PSVAS** & **0.87** & **1.08** & **1.28** & **0.93** & **1.23** & **1.66** & **6.81** & **10.53** & **13.44** \\
**MPS-VAS** & **0.92** & **1.13** & **1.38** & **1.07** & **1.67** & **2.10** & **6.83** & **10.59** & **13.64** \\ \hline \hline \multicolumn{6}{c}{Test with CC as Target} & \multicolumn{6}{c}{Test with SC as Target} & \multicolumn{6}{c}{Test with Helpad as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline RS & 0.57 & 1.18 & 1.72 & 1.80 & 3.40 & 5.11 & 0.19 & 0.42 & 0.72 \\ GC & 1.05 & 1.81 & 2.14 & 2.64 & 4.88 & 7.04 & 0.41 & 0.78 & 1.04 \\ GS [15] & 1.12 & 1.97 & 2.48 & 3.35 & 5.39 & 7.52 & 0.54 & 0.93 & 1.12 \\ AL [13] & 1.04 & 1.77 & 2.12 & 2.62 & 4.88 & 7.03 & 0.40 & 0.76 & 1.03 \\ AS [9] & 1.02 & 1.61 & 2.03 & 2.43 & 4.61 & 6.95 & 0.38 & 0.75 & 0.98 \\ E2EVAS [6] & 1.43 & 2.31 & 2.98 & 4.73 & 7.43 & 9.59 & 0.81 & 1.20 & 1.46 \\ OnlineTTA[6] & 1.43 & 2.33 & 2.99 & 4.75 & 7.44 & 9.59 & 0.83 & 1.21 & 1.46 \\ \hline
**PSVAS** & **1.62** & **2.49** & **3.14** & **5.51** & **8.33** & **10.52** & **0.91** & **1.22** & **1.47** \\
**MPS-VAS** & **1.74** & **2.64** & **3.47** & **5.55** & **8.40** & **10.69** & **0.96** & **1.30** & **1.63** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **ANT** comparisons when trained with _small car_ as target on xView in single-query setting.

### Multi Query Setting

Next, we evaluate the proposed MPS-VAS-MQ approach on the xView and DOTA dataset in multi query setting. In Table 3, we present the results with varying search budget \(\mathcal{C}=\{25,50,75\}\) and the number of equal sized grid cells \(N=49\). We consider \(K=3\) in all the experiments we perform in different target settings. We observe two general consistent trends across different target settings. First, the search performance of MPS-VAS in single query setting is always better than the performance of MPS-VAS-MQ in multi query settings, due to the fact that in single query setting the task-specific prediction module is updated \(K\) times more frequently than in the multi-query setting. Second, across various target settings, the performance improvement of MPS-VAS-MQ over MPS-VAS-topk ranges from \(0.08\%\) to \(3.5\%\). In Table 4, we present similar result with the number of grid cells \(N\) = \(64\) and train the policy with _large vehicle_ as the target class. We report the results with different values of \(N\) in the Supplement. Here the improvement of MPS-VAS-MQ over MPS-VAS-topk is up to \(3.5\%\) across different target settings, suggesting that there is added value from learning to capture interdependence in greedy search decisions.

### Effect of \(\lambda\) on Search Performance

We perform experiments with different choices of \(\lambda\) and found \(\lambda\) = 0.1 to be the best choice across all different experimental setup. For comparison, here we report the results in the case when we train the policy with different values of \(\lambda\) using _small car_ as a target class and test the policy with

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{6}{c}{Test with Ship as Target} & \multicolumn{6}{c}{Test with LV as Target} & \multicolumn{6}{c}{Test with Harbor as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline Random & 1.08 & 2.11 & 3.06 & 1.48 & 2.96 & 3.91 & 1.41 & 2.72 & 3.90 \\ GC & 1.52 & 3.04 & 4.19 & 2.59 & 3.77 & 5.48 & 1.90 & 3.77 & 5.02 \\ GS[15] & 1.79 & 3.56 & 4.47 & 2.72 & 4.10 & 5.77 & 2.31 & 4.14 & 5.87 \\ AL[13] & 1.51 & 3.02 & 4.18 & 2.57 & 3.74 & 5.47 & 1.89 & 3.74 & 5.01 \\ AS[9] & 1.43 & 2.87 & 4.01 & 1.64 & 3.15 & 4.23 & 1.73 & 3.45 & 4.68 \\ VAS[6] & 2.45 & 4.37 & 5.87 & 5.33 & 8.47 & 10.51 & 3.12 & 5.04 & 6.82 \\ OnlineTTA[6] & 2.46 & 4.38 & 5.89 & 5.33 & 8.47 & 10.52 & 3.12 & 5.06 & 6.83 \\ \hline
**PSVAS** & **2.46** & **4.41** & **6.00** & **5.33** & **8.52** & **10.59** & **3.15** & **5.24** & **6.94** \\
**MPS-VAS** & **3.08** & **5.25** & **7.13** & **5.34** & **8.53** & **10.63** & **3.82** & **6.77** & **9.00** \\ \hline \hline \multicolumn{6}{c}{Test with Helicopter as Target} & \multicolumn{6}{c}{Test with Plane as Target} & \multicolumn{6}{c}{Test with Roundabout as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline Random & 0.27 & 0.46 & 0.71 & 1.37 & 2.55 & 3.62 & 1.47 & 2.53 & 3.81 \\ GC & 0.38 & 0.59 & 0.91 & 1.96 & 3.14 & 4.02 & 1.62 & 2.86 & 4.23 \\ GS[15] & 0.42 & 0.66 & 1.01 & 2.26 & 3.56 & 4.71 & 1.91 & 3.24 & 4.68 \\ AL[13] & 0.37 & 0.58 & 0.89 & 1.94 & 3.14 & 3.99 & 1.61 & 2.82 & 4.21 \\ AS[9] & 0.34 & 0.53 & 0.82 & 1.89 & 3.06 & 3.92 & 1.55 & 2.71 & 4.06 \\ E2EVAS[6] & 0.47 & 0.72 & 1.05 & 3.07 & 4.87 & 6.34 & 3.05 & 4.94 & 6.34 \\ OnlineTTA[6] & 0.48 & 0.72 & 1.06 & 3.08 & 4.89 & 6.34 & 3.05 & 4.95 & 6.36 \\ \hline
**PSVAS** & **0.50** & **0.73** & **1.10** & **3.09** & **4.96** & **6.38** & **3.09** & **4.96** & **6.39** \\
**MPS-VAS** & **0.63** & **1.03** & **1.60** & **3.46** & **5.57** & **7.71** & **3.46** & **5.57** & **7.71** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **ANT** comparisons when trained with _large vehicle_ as target on DOTA in single-query setting.

Figure 3: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row).

[MISSING_PAGE_EMPTY:9]

## 5 Related Work

**RL for Visual Navigation** RL has found broad applicability in visual navigation tasks [18; 19; 20; 21]. While these tasks share some similarities at a high level, such as requiring a sequence of visual navigation steps based on a local view of the environment, they often do not involve search budget constraints and rely on a predetermined kinematic model of motion. In contrast, our approach involves observing the full environment, albeit potentially at a lower resolution, and sequentially determining which regions to query without being constrained to a particular kinematic model. This highlights the distinctive nature of our approach compared to traditional visual navigation tasks and demonstrates the potential value of active search strategies in addressing budget-constrained settings.

**Active Search** Active Search was first introduced by Garnett et al. [7] as a means to discover members of valuable and rare classes rather than solely focusing on learning an accurate model as in Active Learning [11]. Subsequently, Jiang et al. [9; 22] proposed efficient nonmyopic active search techniques and incorporated search cost into the problem. Sarkar et al. [6] demonstrate that prior active search techniques do not scale well in high-dimensional visual space, and instead propose a DRL based visual active search framework for geo-spatial broad area search. However, the efficacy of the proposed approach by Sarkar et al. [6] can be limited when the search task varies between training and testing, which is often the case in real-world applications. In this work, we propose a novel framework that enables efficient and adaptive search in any previously unseen search task.

**Meta Learning** The concept of meta-learning, has consistently attracted attention in the field of machine learning [23; 24; 25; 26]. Finn et al. [25] present Model Agnostic Meta-Learning, a technique that utilizes SGD updates to rapidly adapt to new tasks. This approach, based on gradient-based meta-learning, can be seen as learning an effective parameter initialization, enabling the network to achieve good performance with just a few gradient updates. Wortsman et al. [27] introduces a self-adaptive visual navigation approach, which has the ability to learn and adapt to novel environments without the need for explicit supervision. Our work is significantly different than all these prior works as we observe true label at each step during search and hence the main challenge is how to leverage the supervised information in order to learn an efficient adaptive search policy.

**Foveated Processing of Large Images** Several studies have investigated the utilization of low-resolution images for guiding the selection of high-resolution regions to process [28; 29; 30; 31; 32; 33; 34; 35], with some employing reinforcement learning techniques [36; 15] to improve this process. However, our work differs significantly, as we focus on selecting a sequence of regions to query, where each query provides the true label, instead of a higher resolution image region. These labels are crucial for guiding further search and serving as an ultimate objective. As such, our approach tackles a unique challenge that differs from existing methods that rely on low-resolution imagery.

## 6 Conclusions

We present a novel approach for visual active search in which we decompose the search policy into a prediction and search modules. This decomposition enables us to combine supervised and reinforcement learning for training, and make use of supervised learning even during execution. Moreover, we propose a novel meta-learning framework to jointly learn a policy and initialization parameters for the supervised prediction module. Our findings demonstrate the significance of the proposed frameworks for conducting efficient search, particularly in real-world situations where search tasks may differ significantly from those utilized during policy training. We hope our framework will find its applicability in many practical scenarios ranging from human trafficking to animal poaching.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{1}{c}{} & \multicolumn{3}{c}{Test with Small Car as Target} & \multicolumn{3}{c}{Test with Building as Target} & \multicolumn{3}{c}{Test with Sail Boat as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline USVAS & 4.77 & 7.46 & 9.61 & 5.86 & 9.37 & 12.05 & 0.64 & 1.08 & 1.27 \\ PSVAS & 5.51 & 8.33 & 10.52 & 6.81 & 10.53 & 13.44 & 0.93 & 1.23 & 1.66 \\ MPS-VAS & **5.55** & **8.40** & **10.69** & **6.83** & **10.59** & **13.64** & **1.07** & **1.67** & **2.10** \\ \hline \hline \multicolumn{1}{c}{} & \multicolumn{3}{c}{Test with Helicopter as Target} & \multicolumn{3}{c}{Test with Cs as Target} & \multicolumn{3}{c}{Test with Heiplad as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline USVAS & 0.53 & 0.84 & 1.19 & 1.44 & 2.27 & 2.99 & 0.80 & 1.16 & 1.42 \\ PSVAS & 0.87 & 1.08 & 1.28 & 1.62 & 2.49 & 3.14 & 0.91 & 1.22 & 1.47 \\ MPS-VAS & **0.92** & **1.13** & **1.38** & **1.74** & **2.64** & **3.47** & **0.96** & **1.30** & **1.63** \\ \hline \hline \end{tabular}
\end{table}
Table 7: **ANT** comparisons when trained with _small car_ as target on xView.

## Acknowledgments

This research was partially supported by the NSF (IIS-1905558, IIS-1903207, and IIS-2214141), ARO (W911NF-18-1-0208), Amazon, NVIDIA, and the Taylor Geospatial Institute managed by Saint Louis University.

## References

* Bondi et al. [2018] Elizabeth Bondi, Debadeepta Dey, Ashish Kapoor, Jim Piavis, Shital Shah, Fei Fang, Bistra Dilkina, Robert Hannaford, Arvind Iyer, Lucas Joppa, et al. Airsim-w: A simulation environment for wildlife conservation with uavs. In _ACM SIGCAS Conference on Computing and Sustainable Societies_, pages 1-12, 2018.
* Bondi et al. [2018] Elizabeth Bondi, Fei Fang, Mark Hamilton, Debarun Kar, Donnabell Dmello, Jongmoo Choi, Robert Hannaford, Arvind Iyer, Lucas Joppa, Milind Tambe, et al. Spot poachers in action: Augmenting conservation drones with automatic detection in near real time. In _AAAI Conference on Artificial Intelligence_, 2018.
* Bondi et al. [2020] Elizabeth Bondi, Raghav Jain, Palash Aggarwal, Saket Anand, Robert Hannaford, Ashish Kapoor, Jim Piavis, Shital Shah, Lucas Joppa, Bistra Dilkina, et al. Birdsai: A dataset for detection and tracking in aerial thermal infrared videos. In _IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1747-1756, 2020.
* Fang et al. [2015] Fei Fang, Peter Stone, and Milind Tambe. When security games go green: Designing defender strategies to prevent poaching and illegal fishing. In _International Joint Conference on Artificial Intelligence_, 2015.
* Fang et al. [2016] Fei Fang, Thanh Hong Nguyen, Rob Pickles, Wai Y Lam, Gopalasamy R Clements, Bo An, Amandeep Singh, Milind Tambe, and Andrew Lemieux. Deploying paws: Field optimization of the protection assistant for wildlife security. In _AAAI Conference on Artificial Intelligence_, volume 16, pages 3966-3973, 2016.
* Sarkar et al. [2022] Anindya Sarkar, Michael Lanier, Scott Alfeld, Roman Garnett, Nathan Jacobs, and Yevgeniy Vorobeychik. A visual active search framework for geospatial exploration. _arXiv preprint arXiv:2211.15788_, 2022.
* Garnett et al. [2012] Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, and Richard Mann. Bayesian optimal active search and surveying. _arXiv preprint arXiv:1206.6406_, 2012.
* Garnett et al. [2015] Roman Garnett, Thomas Gartner, Martin Vogt, and Jurgen Bajorath. Introducing the 'active search'method for iterative virtual screening. _Journal of Computer-Aided Molecular Design_, 29(4):305-314, 2015.
* Jiang et al. [2017] Shali Jiang, Gustavo Malkomes, Geoff Converse, Alyssa Shofner, Benjamin Moseley, and Roman Garnett. Efficient nonmyopic active search. In _International Conference on Machine Learning_, pages 1714-1723. PMLR, 2017.
* Jiang et al. [2019] Shali Jiang, Roman Garnett, and Benjamin Moseley. Cost effective active search. _Advances in Neural Information Processing Systems_, 32, 2019.
* Settles [2009] Burr Settles. Active learning literature survey. 2009.
* Settles [2012] Burr Settles. Active learning. _Synthesis lectures on artificial intelligence and machine learning_, 6(1):1-114, 2012.
* Yoo and Kweon [2019] Donggeun Yoo and In So Kweon. Learning loss for active learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 93-102, 2019.
* Sutton et al. [1999] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12, 1999.

* [15] Burak Uzkent and Stefano Ermon. Learning when and where to zoom with deep reinforcement learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12345-12354, 2020.
* [16] Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, and Brendan McCord. xview: Objects in context in overhead imagery. _arXiv preprint arXiv:1802.07856_, 2018.
* [17] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3974-3983, 2018.
* [18] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. _Advances in Neural Information Processing Systems_, 33:4247-4258, 2020.
* [19] Bar Mayo, Tamir Hazan, and Ayellet Tal. Visual navigation with spatial attention. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16898-16907, June 2021.
* [20] Mahdi Kazemi Moghaddam, Ehsan Abbasnejad, Qi Wu, Javen Qinfeng Shi, and Anton Van Den Hengel. Foresi: Success-aware visual navigation agent. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 691-700, January 2022.
* [21] Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, and Roozbeh Mottaghi. What do navigation agents learn about their environment? In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10276-10285, 2022.
* [22] Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, and Roman Garnett. Efficient nonmyopic batch active search. _Advances in Neural Information Processing Systems_, 31, 2018.
* [23] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. _Learning to learn_, pages 3-17, 1998.
* [24] Jurgen Schmidhuber, Jieyu Zhao, and Marco Wiering. Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement. _Machine Learning_, 28:105-130, 1997.
* [25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [26] Harkirat Singh Behl, Aulim Gunes Baydin, and Philip HS Torr. Alpha maml: Adaptive model-agnostic meta-learning. _arXiv preprint arXiv:1905.07435_, 2019.
* [27] Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6750-6759, 2019.
* [28] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S Davis. Liteeval: A coarse-to-fine framework for resource efficient video recognition. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [29] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptive networks for efficient inference. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* [30] Yi Wang, Youlong Yang, and Xi Zhao. Object detection using clustering algorithm adaptive searching regions in aerial images. In _European Conference on Computer Vision_, pages 651-664, 2020.

* [31] Athanasios Papadopoulos, Pawel Korus, and Nasir Memon. Hard-attention for scalable image classification. In _Advances in Neural Information Processing Systems_, volume 34, pages 14694-14707, 2021.
* [32] Chitresh Thavamani, Mengtian Li, Nicolas Cebron, and Deva Ramanan. Fovea: Foveated image magnification for autonomous navigation. In _IEEE/CVF International Conference on Computer Vision_, pages 15539-15548, 2021.
* [33] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12309-12318, 2022.
* [34] Chenlin Meng, Enci Liu, Willie Neiswanger, Jiaming Song, Marshall Burke, David Lobell, and Stefano Ermon. Is-count: Large-scale object counting from satellite images with covariate-based importance sampling. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 12034-12042, 2022.
* [35] Chenhongyi Yang, Zehao Huang, and Naiyan Wang. Querydet: Cascaded sparse query for accelerating high-resolution small object detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13668-13677, June 2022.
* [36] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Efficient object detection in large images using deep reinforcement learning. In _IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, March 2020.

A Partially Supervised Reinforcement Learning Framework for Visual Active Search: Supplementary Material

## Appendix A Policy Network Architecture and Hyperparameter Details

Recall that the policy network \(\pi\) is composed of two parts : (1) a task specific prediction module, and (2) a task-agnostic search module. The task specific prediction module consists of an encoder \(e(x;\eta)\) that maps the aerial image \(x\) to a low-dimensional latent feature representation \(z\), and a grid prediction network \(p(z,o;\kappa)\) that predicts the probabilities of grids containing a target by leveraging the latent semantic feature \(z\) and the outcomes of previous search queries \(o\). Note that the task specific prediction module is represented as \(f(x,o,\theta)=p(z=e(x;\eta),o;\kappa)\), where \(\theta=(\eta,\kappa)\). Following [6], we use frozen ResNet-34, pre-trained on ImageNet, followed by a learnable \(1\times 1\) convolution layer with a ReLU activation as a feature extraction component of the task specific prediction module that we refer as encoder \(e(.)\). We then combine the latent semantic feature \(z\) with the previous query information \(o\). We apply the tiling operation in order to convert \(o\) into a representation with the same dimensions as the extracted features \(z\), enabling us to effectively apply channel-wise concatenation of latent image feature and auxiliary state feature while preserving the grid specific spatial and query related information. This combined representation is then fed to a grid prediction network comprises of a \(1\times 1\) convolution layer, flattening, and a MLP block consists of 2 fully connected layer with ReLU activations. Note that the output of grid prediction network is of dimension \(N\). We finally apply sigmoid activation to each output neuron to convert them into a probability value representing the probability of the grids containing target. The proposed policy architecture is depicted in figure 2 of the main paper.

We re-shape the output of task specific prediction module by converting it back from 1D to 2D of shape \((m\times n)=N\) before feeding it to the task agnostic search module \(g(.)\) that takes the following three inputs: (1) the reshaped 2D output of the task specific prediction module, which is the probabilities of grids containing target; (2) the remaining search budget \(B\), which is a scalar but we apply tiling to the scalar budget \(B\) to transform it to match the size of the reshaped 2D output of the task specific prediction module; (3) we also apply the tiling operation to \(o\) in a way that allows us to concatenate the features \((z,o,B)\) along the channels dimension to finally obtain the combined representation that serves as a input to task agnostic search module. The task agnostic search module is composed of a flattening, a MLP block consists of 2 fully connected layer with ReLU activations, and a final softmax layer to convert the output to a probability distribution that guides us in selecting the grid to query next.

In Table 8, we detail the architecture of task specific prediction module (\(f\)) of PSVAS policy network. In Table 9, we detail the architecture of task agnostic search module (\(g\)) of PSVAS policy network. Note that, the task specific prediction module and task agnostic search module remains unchanged in MPS-VAS framework.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Layers** & **Configuration** & **o/p Feature Map size** \\ \hline Input & RGB Image & 3 × 3500 × 3500 \\ \hline Encoder & ResNet-34 & 512 × 14 × 14 \\ \hline Conv1 & Channel:N; kernel size:\(1\times 1\) & N × 14 × 14 \\ \hline
2D MaxPool & Pooling size:\(2\times 2\) & N × 7 × 7 \\ \hline Tile1 & Grid State (\(o\)) & \(N\times 7\times 7\) \\ \hline Channelwise Concat & Conv1,Tile1 & \((2N)\times 7\times 7\) \\ \hline Conv2 & Channel:3; kernel size: \(1\times 1\) & \(3\times 7\times 7\) \\ \hline Flattened & Conv2 & 147 \\ \hline FC1+ReLU & (\(147\)– \(>2N\)) & 2N \\ \hline FC2+Sigmoid & (\(2N\)– \(>N\)) & N \\ \hline \hline \end{tabular}
\end{table}
Table 8: Task Specific Prediction Module Architecture with number of grid cell \(N=(m\times n)\)In MPS-VAS-MQ framework, the network architecture of task specific prediction module remains unaltered, but the additional dependence of task agnostic search module (\(g\)) on \(\psi\) enforce a slight modification of its architecture as detailed in Table 10.

We use a learning rate of \(10^{-4}\), batch size of 16, number of training epochs 200, and the Adam optimizer to train the policy network in all experimental settings. During Inference, in all experimental settings, we update the parameters of task specific prediction module \(f\) after each query step using a learning rate of \(10^{-4}\) and the Adam optimizer. We use 1 NVidia A100 and 3 GeForce GTX 1080Ti GPU servers for all our experiments.

## Appendix B Results with Uniform Query Cost

### Single Query Setting

Here we present the results by considering a setting with a single query resource and query costs \(c(i,j)=1\) for all \(i,j\), where \(\mathcal{C}\) is the number of queries. We evaluate PSVAS and MPS-VAS on the xView dataset with varying search budget \(\mathcal{C}\in\{12,15,18\}\) and the number of grid cells \(N=49\). We train the policy with _small car_ as the target and test the performance of the policy with the following target classes : _Small Car (SC)_, _Helicopter_, _Sail Boat (SB)_, _Construction Cite (CC)_, _Building_, and _Helipad_. The results are presented in Table 11. We observe noticeable improvement in performance of the proposed PSVAS approach compared to all baselines in each different target setting, ranging from approximately \(0.50\) to \(52.0\%\) relative to the most competitive E2EVAS baseline. In Table 12, we report the results on DOTA dataset with \(N=64\). In this setting, we train the policy with _large vehicle_ as the target and evaluate the performance with the following target classes : _Ship_, _large vehicle_ (LV), _Harbor_, _Helicopter_, _Plane_, and _Roundabout_. Here, we notice significant improvement in performance of PSVAS compared to all the baselines including E2EVAS, ranging from approximately \(3.5\) to \(25.0\%\). The effectiveness of the PSVAS framework becomes evident as it allows us to efficiently update the task-specific prediction module \(f\) by leveraging the crucial supervised information. We

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Layers** & **Configuration** & **o/p Feature Map size** \\ \hline Input 1 & 2D Reshape of Task Specific Prediction Module Output & \(1\times m\times n\) \\ \hline Input 2: Tile2 & Grid State (\(o\)) & \(1\times m\times n\) \\ \hline Input 3: Tile3 & Query Budget Left (\(B\)) & \(1\times m\times n\) \\ \hline Input 4: Tile4 & Encoded Locations of the queried Grid cells (\(\psi\)) & \(1\times m\times n\) \\ \hline Input: Channelwise Concat & Input 1, Input 2, Input 3, Input 4 & \((4)\times m\times n\) \\ \hline Flattened & Input: Channelwise Concat & \(D=(4)\times m\times n\) \\ \hline FC1+ReLU & (\(D\)\(->2N\)) & 2N \\ \hline FC2+Softmax & (\(2N\)\(->N\)) & N \\ \hline \hline \end{tabular}
\end{table}
Table 10: Task Agnostic Search Module Architecture in multi query setting with number of grid cell \(N=(m\times n)\)

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Layers** & **Configuration** & **o/p Feature Map size** \\ \hline Input 1 & 2D Reshape of Task Specific Prediction Module Output & \(1\times m\times n\) \\ \hline Input 2: Tile2 & Grid State (\(o\)) & \(1\times m\times n\) \\ \hline Input 3: Tile3 & Query Budget Left (\(B\)) & \(1\times m\times n\) \\ \hline Input: Channelwise Concat & Input 1, Input 2, Input 3 & \((3)\times m\times n\) \\ \hline Flattened & Input: Channelwise Concat & \(K=(3)\times m\times n\) \\ \hline FC1+ReLU & (\(K\)\(->2N\)) & 2N \\ \hline FC2+Softmax & (\(2N\)\(->N\)) & N \\ \hline \hline \end{tabular}
\end{table}
Table 9: Task Agnostic Search Module Architecture with number of grid cell \(N=(m\times n)\)also observe a consistent trend, i.e., the performance of MPS-VAS is significantly better than PSVAS across different target settings, ranging from approximately \(0.6\) to \(60.0\%\). The significance of the MPS-VAS framework becomes apparent when deploying visual active search in scenarios where the search tasks differ substantially from those encountered during training.

### Multi Query Setting

In Table 13, we present the results of MPS-VAS-MQ and compare its performance with MPS-VAS-TOPK with varying search budget \(\mathcal{C}\in\{12,15,18\}\) and the number of grid cell N=49. Here, we train the policy with _small car_ as the target and evaluate the performance of the policy with the following target classes : _Small Car_ (SC), _Helicopter_, _Sail Boat_ (SB), _Construction Cite_ (CC), _Building_, and _Helipad_. In table 14, we present similar results with the number of grid cell \(N=64\). In this setting, we train the policy with _Large Vehicle_ as the target and evaluate the policy with the

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Test with Helicopter as Target} & \multicolumn{5}{c}{Test with NB as Target} & \multicolumn{5}{c}{Test with Building as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline Random & 2.41 & 3.02 & 3.95 & 3.40 & 4.03 & 5.14 & 3.17 & 3.93 & 4.78 \\ GC & 2.82 & 3.44 & 4.27 & 3.87 & 4.59 & 5.55 & 3.48 & 4.25 & 4.98 \\ GS[15] & 2.96 & 3.59 & 4.48 & 3.99 & 4.77 & 5.67 & 3.62 & 4.40 & 5.07 \\ AL[13] & 2.81 & 3.42 & 4.26 & 3.85 & 4.54 & 5.51 & 3.47 & 4.25 & 4.97 \\ AS[9] & 2.57 & 3.27 & 4.03 & 3.61 & 4.12 & 5.26 & 3.35 & 4.16 & 4.92 \\ EEEVAS[6] & 3.57 & 4.42 & 5.15 & 6.30 & 7.65 & 8.90 & 4.28 & 5.21 & 6.09 \\ OnlineTA[6] & 3.57 & 4.43 & 5.15 & 6.31 & 7.67 & 8.90 & 4.30 & 5.22 & 6.10 \\ \hline
**PSVAS** & **3.60** & **4.51** & **5.23** & **6.50** & **7.86** & **9.22** & **4.61** & **5.72** & **6.87** \\
**MPS-VAS** & **3.79** & **4.75** & **5.58** & **6.51** & **7.88** & **9.24** & **4.90** & **6.23** & **7.38** \\ \hline \hline \multicolumn{1}{c}{Test with Helicopter as Target} & \multicolumn{5}{c}{Test with Plane as Target} & \multicolumn{5}{c}{Test with Roundabout as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline Random & 0.66 & 0.73 & 0.82 & 2.91 & 3.94 & 4.74 & 2.66 & 3.59 & 4.37 \\ GC & 0.71 & 0.82 & 0.89 & 3.22 & 4.35 & 5.07 & 2.93 & 3.81 & 4.59 \\ GS[15] & 0.75 & 0.87 & 0.97 & 3.47 & 4.56 & 5.25 & 2.99 & 3.96 & 4.73 \\ AL[13] & 0.70 & 0.81 & 0.88 & 3.22 & 4.34 & 5.07 & 2.93 & 3.79 & 4.59 \\ AS[9] & 0.68 & 0.78 & 0.86 & 3.16 & 4.21 & 4.97 & 2.82 & 3.74 & 4.51 \\ EEEVAS[6] & 0.78 & 0.96 & 1.18 & 4.02 & 5.07 & 5.90 & 4.00 & 5.05 & 5.88 \\ OnlineTA[6] & 0.78 & 0.97 & 1.19 & 4.02 & 5.07 & 5.91 & 4.01 & 5.06 & 5.88 \\ \hline
**PSVAS** & **0.95** & **1.21** & **1.49** & **4.33** & **5.32** & **6.44** & **4.33** & **5.36** & **6.41** \\
**MPS-VAS** & **1.10** & **1.37** & **1.67** & **4.52** & **5.58** & **6.75** & **4.51** & **5.56** & **6.73** \\ \hline \hline \end{tabular}
\end{table}
Table 11: **ANT** comparisons when trained with _small car_ as target on xView in single-query setting.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multicolumn{1}{c}{Method} & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline Random & 2.41 & 3.02 & 3.95 & 3.40 & 4.03 & 5.14 & 3.17 & 3.93 & 4.78 \\ GC & 2.82 & 3.44 & 4.27 & 3

[MISSING_PAGE_FAIL:17]

MPS-VAS framework for visual active search in real-world scenarios where the search tasks differ from the ones used during policy training.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{5}{c}{Test with Helicopter as Target} & \multicolumn{5}{c}{Test with SB as Target} & \multicolumn{5}{c}{Test with Building as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline MS-VAS-topk & 0.40 & 0.51 & 0.62 & 0.69 & 0.98 & 1.30 & 4.29 & 6.84 & 8.66 \\
**MPS-VAS-MQ** & **0.42** & **0.53** & **0.66** & **0.71** & **1.03** & **1.32** & **4.33** & **6.95** & **8.78** \\ \hline \hline \multicolumn{5}{c}{Test with CC as Target} & \multicolumn{5}{c}{Test with SC as Target} & \multicolumn{5}{c}{Test with Helipad as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline MS-VAS-topk & 0.96 & 1.53 & 2.12 & 3.19 & 5.09 & 6.47 & 0.45 & 0.59 & 0.77 \\
**MPS-VAS-MQ** & **0.98** & **1.65** & **2.17** & **3.25** & **5.12** & **6.55** & **0.47** & **0.61** & **0.82** \\ \hline \hline \end{tabular}
\end{table}
Table 16: **ANT comparisons when trained with _small car_ as target on xView in multi-query setting.**

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{5}{c}{Test with Helicopter as Target} & \multicolumn{5}{c}{Test with SB as Target} & \multicolumn{5}{c}{Test with Building as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline RS & 0.22 & 0.31 & 0.38 & 0.48 & 0.55 & 0.63 & 3.43 & 4.25 & 4.97 \\ E2EVAS [6] & 0.31 & 0.39 & 0.43 & 0.80 & 1.05 & 1.30 & 5.23 & 6.37 & 7.41 \\ OnlineTTA[6] & 0.31 & 0.40 & 0.44 & 0.80 & 1.06 & 1.31 & 5.24 & 6.38 & 7.43 \\ \hline
**PSVAS** & **0.43** & **0.48** & **0.51** & **0.83** & **1.09** & **1.33** & **5.34** & **6.41** & **7.52** \\
**MPS-VAS** & **0.47** & **0.50** & **0.54** & **0.84** & **1.11** & **1.39** & **5.44** & **6.69** & **7.75** \\ \hline \hline \multicolumn{5}{c}{Test with CC as Target} & \multicolumn{5}{c}{Test with SC as Target} & \multicolumn{5}{c}{Test with Helipad as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline RS & 0.78 & 1.02 & 1.17 & 3.12 & 3.61 & 4.45 & 0.25 & 0.33 & 0.41 \\ E2EVAS [6] & 0.98 & 1.29 & 1.47 & 4.61 & 5.64 & 6.55 & 0.44 & 0.46 & 0.56 \\ OnlineTTA[6] & 0.99 & 1.32 & 1.50 & 4.62 & 5.64 & 6.56 & 0.45 & 0.47 & 0.56 \\ \hline
**PSVAS** & **1.28** & **1.64** & **1.86** & **4.74** & **5.72** & **6.75** & **0.53** & **0.59** & **0.78** \\
**MPS-VAS** & **1.39** & **1.69** & **2.05** & **4.81** & **5.93** & **6.96** & **0.61** & **0.66** & **0.83** \\ \hline \hline \end{tabular}
\end{table}
Table 17: **ANT comparisons when trained with _small car_ as target on xView in single-query setting.**

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multicolumn{5}{c}{Test with Helicopter as Target} & \multicolumn{5}{c}{Test with SB as Target} & \multicolumn{5}{c}{Test with Building as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline MS-VAS-topk & 0.41 & 0.43 & 0.4

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Test with Ship as Target} & \multicolumn{6}{c}{Test with LV as Target} & \multicolumn{6}{c}{Test with Harbor as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline RS & 2.92 & 3.34 & 3.99 & 3.44 & 4.08 & 5.19 & 4.17 & 5.04 & 5.92 \\ E2EVAS [6] & 3.34 & 4.15 & 4.77 & 5.14 & 6.05 & 7.00 & 5.38 & 6.51 & 7.54 \\ OnlineTTA[6] & 3.36 & 4.15 & 4.79 & 5.14 & 6.06 & 7.01 & 5.40 & 6.52 & 7.55 \\ \hline
**PSVAS** & **3.48** & **4.37** & **5.15** & **5.23** & **6.08** & **7.12** & **5.57** & **6.09** & **7.78** \\
**MPS-VAS** & **3.85** & **4.69** & **5.38** & **5.25** & **6.11** & **7.14** & **5.71** & **6.95** & **8.15** \\ \hline \hline \multicolumn{1}{c}{Test with Helicopter as Target} & \multicolumn{6}{c}{Test with Plane as Target} & \multicolumn{6}{c}{Test with RB as Target} \\ \hline Method & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) & \(\mathcal{C}=12\) & \(\mathcal{C}=15\) & \(\mathcal{C}=18\) \\ \hline RS & 1.03 & 1.52 & 1.77 & 4.05 & 5.11 & 6.12 & 1.25 & 1.54 & 1.91 \\ E2EVAS [6] & 1.50 & 1.87 & 2.13 & 5.47 & 6.59 & 7.65 & 1.87 & 2.17 & 2.47 \\ OnlineTTA[6] & 1.50 & 1.88 & 2.16 & 5.47 & 6.61 & 7.68 & todo & todo \\ \hline
**PSVAS** & **1.77** & **2.23** & **2.50** & **5.54** & **6.65** & **7.66** & **2.03** & **2.32** & **2.65** \\
**MPS-VAS** & **2.10** & **2.57** & **2.77** & **5.73** & **6.87** & **7.90** & **2.12** & **2.66** & **2.99** \\ \hline \hline \end{tabular}
\end{table}
Table 22: **ANT comparisons when trained with \(LV\) as target on DOTA in multi-query setting.**

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Test with Ship as Target} & \multicolumn{6}{c}{Test with LV as Target} & \multicolumn{6}{c}{Test with Harbor as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline RS & 1.45 & 3.17 & 4.30 & 1.79 & 3.50 & 5.10 & 2.35 & 4.34

[MISSING_PAGE_FAIL:20]

with _plane_ as the target. We observe PSVAS-F yields 9 successful searches, while PSVAS yields 12 successful search out of 15 total query.

Figure 6 illustrates the contrasting exploration strategy behaviors between PSVAS and PSVAS-F in the case when both the policies are trained with _large vehicle_ as the target and test with _roundabout_ as the target. We observe PSVAS-F yields 5 successful searches, while PSVAS yields 7 successful search out of 15 total query.

### Effect on MPS-VAS Framework

Next, we examine the influence of inference time adaptation of the task-specific prediction module on the MPS-VAS framework across various target settings. For this purpose, we train a policy using our proposed MPS-VAS approach. But during inference, we freeze both the task-specific prediction module and the task-agnostic search module, which differs from the standard MPS-VAS approach. We refer the resulting policy as MPS-VAS-F. Table 26 presents a comparison of the search performance between MPS-VAS and MPS-VAS-F, considering a grid cell count of \(N=36\), across various target settings. Similarly, in Table 25, we provide corresponding results with a grid cell count of \(N=49\). Across various target settings, we observe a notable enhancement in the performance of MPS-VAS compared to MPS-VAS-F. This finding underscores the significance of adapting the task-specific prediction module during inference after each query, validating its importance on adaptive visual active search. Following Figures demonstrate the divergent exploration strategy behaviors exhibited by MPS-VAS and MPS-VAS-F.

Figure 7 illustrates the contrasting exploration strategy behaviors of MPS-VAS and MPS-VAS-F when both policies are trained with a _large vehicle_ as the target and tested with a _plane_ as the target. Among a total of 15 queries, MPS-VAS-F achieves 2 successful searches, while MPS-VAS achieves

Figure 5: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using PSVAS-F (top row), PSVAS (bottom row).

Figure 6: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using PSVAS-F (top row), PSVAS (bottom row).

4 successful searches.

In Figure 8, the distinct exploration strategy behaviors of MPS-VAS and MPS-VAS-F are depicted when both policies are trained with a _large vehicle_ as the target and tested with a _ship_ as the target. Out of a total of 15 queries, MPS-VAS-F achieves 7 successful searches,

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{3}{c}{Test with Ship as Target} & \multicolumn{3}{c}{Test with LV as Target} & \multicolumn{3}{c}{Test with Harbor as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline MPS-VAS-F & 2.69 & 4.50 & 5.88 & 4.63 & 6.79 & 8.07 & 4.22 & 6.92 & 9.06 \\
**MPS-VAS** & **3.42** & **5.19** & **6.73** & **4.80** & **7.08** & **8.23** & **5.02** & **8.04** & **9.91** \\ \hline \hline \multicolumn{3}{c}{Test with Helicopter as Target} & \multicolumn{3}{c}{Test with Plane as Target} & \multicolumn{3}{c}{Test with RB as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline MPS-VAS-F & 1.00 & 2.07 & 2.66 & 4.57 & 7.23 & 9.14 & 1.56 & 2.28 & 2.72 \\
**MPS-VAS** & **1.80** & **2.60** & **3.03** & **5.17** & **7.83** & **10.02** & **1.96** & **2.76** & **3.19** \\ \hline \hline \end{tabular}
\end{table}
Table 26: **ANT** comparisons when trained with _LV_ as target on DOTA in single-query setting.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multicolumn{3}{c}{Test with Helicopter as Target} & \multicolumn{3}{c}{Test with SB as Target} & \multicolumn{3}{c}{Test with Building as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline MPS-VAS-F & 0.54 & 0.89 & 1.22 & 0.64 & 1.14 & 1.37 & 5.97 & 9.31 & 12.04 \\
**MPS-VAS** & **0.92** & **1.13** & **1.38** & **1.07** & **1.67** & **2.10** & **6.83** & **10.59** & **13.64** \\ \hline \hline \multicolumn{3}{c}{Test with CC as Target} & \multicolumn{3}{c}{Test with SC as Target} & \multicolumn{3}{c}{Test with Heliopad as Target} \\ \hline Method & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline MPS-VAS-F & 1.37 & 2.33 & 3.05 & 4.82 & 7.46 & 9.56while MPS-VAS achieves 9 successful searches. Figure 9 showcases the contrasting exploration strategy behaviors of MPS-VAS and MPS-VAS-F when both policies are trained with a _large vehicle_ as the target and tested with a _roundabout_ as the target. Among a total of 15 queries, MPS-VAS-F achieves 6 successful searches, while MPS-VAS achieves 8 successful searches.

## Appendix E More Visualizations of Comparative Exploration Strategies of Different Approaches

The showcased visualizations (10, 11, 12, 13, 14) in all these examples demonstrate the superiority of our PSVAS and MPS-VAS framework compared to the E2EVAS baseline, especially in scenarios where search tasks vary from those employed in policy training.

## Appendix F Analyzing Search Performance Across Multiple Trials

Here, we compare the search performance of E2EVAS, PSVAS, and MPS-VAS across multiple trials. In Figure 15, we present the results when the polices are trained with small car as the target and evaluate the performance under Manhattan distance based query cost \(\mathcal{C}=25\) with the following target classes: _Small Car (SC)_, _Helicopter_, _Sail Boat (SB)_, _Construction Cite (CC)_, _Building_, and _Helpad_. In figure 16, we present similar results with Manhattan distance based query cost budget \(\mathcal{C}=50\). In figure 17, we also present similar results with Manhattan distance based query cost budget \(\mathcal{C}=75\).

In Figure 18, we present the results when the polices are trained with _large vehicle_ as the target and evaluate the performance under Manhattan distance based query cost \(\mathcal{C}=25\) with the following target

Figure 10: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row). Note that during the training phase, all these policies are trained with _large vehicle_ as the target, while evaluation is conducted using _roundabout_ as the target.

Figure 9: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using MPS-VAS-F (top row), MPS-VAS (bottom row).

classes: _Ship_, _large vehicle_ (LV), _Harbor_, _Helicopter_, _Plane_, and _Roundabout_. In figure 19, we present similar results with Manhattan distance based query cost budget \(\mathcal{C}=50\). In figure 20, we also present similar results with Manhattan distance based query cost budget \(\mathcal{C}=75\).

Figure 11: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row). Note that during the training phase, all these policies are trained with _large vehicle_ as the target, while evaluation is conducted using _ship_ as the target.

Figure 12: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row). Note that during the training phase, all these policies are trained with _large vehicle_ as the target, while evaluation is conducted using _ship_ as the target.

Figure 13: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row). Note that during the training phase, all these policies are trained with _large vehicle_ as the target, while evaluation is conducted using _plane_ as the target.

## Appendix G Search Performance Comparisons Across Datasets

Our experimental outcomes indicate that it is possible to apply our method directly across different datasets without requiring any further modifications or hyperparameter tuning. In the following table 27, we demonstrate this by presenting results of training on one dataset for one target class while evaluating on another dataset and for another target class. We use the number of equal sized grid cells \(N=64\) and varying search budgets \(\mathcal{C}=\{25,50,75\}\).

Figure 16: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=50\)).

Figure 14: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using E2EVAS (top row), PSVAS (middle row), and MPS-VAS (bottom row). Note that during the training phase, all these policies are trained with _large vehicle_ as the target, while evaluation is conducted using _plane_ as the target.

Figure 15: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=25\)).

Figure 17: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=75\)).

Figure 18: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=25\)).

Figure 20: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=75\)).

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{6}{c}{Test with Small Car as Target} & \multicolumn{6}{c}{Test with Building as Target} & \multicolumn{6}{c}{Test with Sail Boat as Target} \\ \hline \(Method\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) & \(\mathcal{C}=25\) & \(\mathcal{C}=50\) & \(\mathcal{C}=75\) \\ \hline E2EVAS [6] & 4.22 & 6.73 & 8.07 & 5.12 & 8.24 & 10.50 & 0.48 & 0.56 & 0.92 \\ Online TTA [6] & 4.23 & 6.75 & 8.10 & 5.14 & 8.27 & 10.53 & 0.49 & 0.57 & 0.95 \\ PSVAS & 4.95 & 7.74 & 9.45 & 6.10 & 9.45 & 12.31 & 0.89 & 1.05 & 1.54 \\ MPS-VAS & **5.07** & **7.92** & **9.73** & **6.18** & **9.68** & **12.83** & **1.02** & **1.39** & **1.91** \\ \hline \hline \end{tabular}
\end{table}
Table 27: **ANT** comparisons when trained with _large vehicle_ on DOTA as target and evaluated with _small car_, _building_, and _sail boat_ as target class from xView.

Figure 19: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=50\)).

Figure 18: Comparative Search Performance of E2EVAS, PSVAS, MPS-VAS under Distance Based Query Cost (\(\mathcal{C}=25\)).