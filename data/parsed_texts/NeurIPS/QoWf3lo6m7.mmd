# Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics

 Hanlin Zhu

UC Berkeley

hanlinzhu@berkeley.edu

&Baihe Huang

UC Berkeley

baihe_huang@berkeley.edu

&Shaolun Zhang

UC Berkeley

shaolun_zhang@berkeley.edu

&Michael Jordan

UC Berkeley

jordan@cs.berkeley.edu

&Jiantao Jiao

UC Berkeley

jiantao@berkeley.edu

&Yuandong Tian

Meta AI

yuandong@meta.com

&Stuart Russell

UC Berkeley

russell@cs.berkeley.edu

Equal contributions.

###### Abstract

Auto-regressive large language models (LLMs) show impressive capacities to solve many complex reasoning tasks while struggling with some simple logical reasoning tasks such as inverse search: when trained on "\(A\to B\)" (e.g., _Tom is the parent of John_), LLM fails to directly conclude "\(B\gets A\)" (e.g., _John is the child of Tom_) during inference even if the two sentences are semantically identical, which is known as the "reversal curse". In this paper, we theoretically analyze the reversal curse via the training dynamics of (stochastic) gradient descent for two auto-regressive models: (1) a bilinear model that can be viewed as a simplification of a one-layer transformer; (2) one-layer transformers under certain assumptions. Our analysis reveals that for both models, the reversal curse is a consequence of the (effective) model weights _asymmetry_, i.e., the increase of weights from a token \(A\) to token \(B\) during training does not necessarily cause the increase of the weights from \(B\) to \(A\), which is caused by the training dynamics under certain choice of loss function and the optimization space of model parameters. Moreover, our analysis can be naturally applied to other logical reasoning tasks such as chain-of-thought (COT), which provides a new perspective different from previous work that focuses on expressivity. Finally, we conduct experiments to validate our theory on multi-layer transformers under different settings. Our code is available at https://github.com/marlo-z/reversal_curse_analysis/.

## 1 Introductions

Large language models (LLMs) have shown great performance in solving complex reasoning tasks that require multiple reasoning steps through in-context learning (ICL), such as zero-shot learning [1, 2], few-shot learning [3, 4, 5], or via further fine-tuning [6, 7, 8]. However, without the above inference-time techniques or model fine-tuning (probably combined with data manipulations), an auto-regressive LLM might struggle with simple logical reasoning tasks that require multiple reasoning steps learned during training separately [9], where the reversal curse [10] serves as a well-known example.

The reversal curse refers to the phenomenon that an auto-regressive LLM that learns "\(A\to B\)" (e.g., _Tom is the parent of John_) during training fails to generalize to the reverse direction "\(B\gets A\)" (e.g., _John is the child of Tom_) even if the pair of relationship "\(\rightarrow\)" and "\(\leftarrow\)" are reverse to each other and the two sentences are semantically identical. Although some previous works propose different methods to mitigate the reversal curse, including reversing the training dataset [11, 12] and training on different objectives such as autoregressive blank infilling [13], these methods might negatively affect the model performance on other tasks since they either alter the dataset or the model architecture. Without dataset manipulation or changing the auto-regressive nature (causal structure) of the model, there are two other candidate solutions to mitigate the reversal curse.

First, one might constrain the model parameters to satisfy a higher-level regularity for specific relationships. For example, a reversal-type regularity can be viewed as a pair of relationships (\(\rightarrow\), \(\leftarrow\)) and two sets \(\mathcal{A},\mathcal{B}\) such that a model trained on "\(A\to B\)" will also increase its probability of "\(B\gets A\)" for all \(A\in\mathcal{A},B\in\mathcal{B}\), which induces a subspace of model parameters that satisfy this regularity. If one can train the model within this subspace, then training on "\(A\to B\)" can, by definition, help to learn "\(B\gets A\)". However, for a general LLM, it is extremely challenging to find the subspace and manually hard-code the constraint during optimization even for one pair of relationships, not to mention there are numerous relationships. Since it is intractable to manually hard-code the constraints to the model parameter, one can alternatively expect the model to learn the higher-level regularity by training samples under unconstrained optimization. However, this is also hard, according to our analysis, through the popular cross-entropy (CE) loss that aims to maximize the next token prediction probability for the models studied in our paper.

Second, one can use a different loss function which is "symmetric", rather than the popular CE loss. However, the "symmetric" loss might drive the model to learn meaningless sentences. For example, when trained on the sentence "John is tall", a "symmetric" loss function might drive the model to learn "tall is John", which is not what we expect. To prevent the model from the above undesired behavior, in practice, CE loss is still widely-used.

Therefore, in this paper, we analyze the reversal curse via training dynamics of the widely-used unconstrained optimization for the CE loss. We summarize our main contributions as follows:

* We theoretically analyze the reversal curse where training or test sequences have the form "\(A\to B\)" or "\(B\gets A\)" via training dynamics of (stochastic) gradient descent under two auto-regressive models: a bilinear model (Section3) and one-layer transformers under certain assumptions similar to [14] (Section4). The analysis of both models reveals that the widely-used unconstrained optimization for CE loss leads to model weights _asymmetry_, i.e., the increase of (effective) weights (after reparameterization) from the token \(A\) to token \(B\)1 during training does not necessarily cause the increase of the weights from \(B\) to \(A\), which further causes the reversal curse. Although the (effective) weights from \(A\) to \(B\) and from \(B\) to \(A\) might be related to some extent due to reparameterization, their correlation is weak and thus show asymmetry as empirically verified in Section5. Footnote 1: The weights from \(A\) to \(B\) can be viewed as the logits of token \(B\) when the input is \(A\).
* The techniques we used to analyze the reversal curse can be applied to other logical reasoning tasks. In particular, we use the above framework to analyze chain-of-thought (COT) [4], and we show that a model trained on "\(A\to B\)" and "\(B\to C\)" separately struggles to directly conclude "\(A\sim C\)" without COT even if it is logically true (Section4.2). Different from the previous work [15] that theoretically studies COT through the expressivity of transformers, our work provides a new perspective through training dynamics.
* We also empirically validate our theoretical results on multi-layer transformers (Section5).

The _asymmetry_ of auto-regressive model weights caused by widely-used unconstrained optimization for CE loss indicates that auto-regressive LLMs might not automatically deduce certain types of conclusions using separate knowledge learned during training under current popular training paradigms: to make a model predicting token \(B\) where the input token is \(A\), the model might need to see \(B\) following \(A\) in the same sequence during the training set. This also highlights the importance of ICL, data augmentation, or planning for LLMs with the current popular causal transformer-based structures to solve complex reasoning tasks.

### Related works

LLM Reasoning.The strong performance of LLMs on reasoning tasks [3; 7; 16; 2; 17; 4; 18; 19] has prompted many studies on the reasoning capabilities of LLMs. [20] argues that transformers perform implicit Bayesian inference in ICL. [21] shows that transformers implement a specific type of circuits called "induction heads" that are key to the ICL abilities of LLMs. [22] proves that causal structures are encoded in transformer layers during the training dynamics. [23] identifies a backward chaining mechanism of transformers in deductive reasoning. Apart from in-context reasoning, LLMs still demonstrate limitations in other types of reasoning tasks [24; 25; 26].

Reversal Curse.[10] identifies the phenomenon of reversal curse. This drawback of LLMs is also demonstrated in [27]. [9] studies a similar phenomenonin which LLMs face difficulty in manipulating already learned knowledge. Several paper studies eliminating the reversal curse by extending causal attention to bidirectional attention [13], training on reversed samples [12], permuting semantic units [11], or introducing reverse logic data [28]. Given all the empirical works, theoretical analysis of the reversal curse phenomenon remains scarce.

Expressivity of LLMs.There is a long line of works [29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 21; 45] studying the behavior of LLMs through the expressivity of transformers. It has been shown that transformers can implement simple functions such as sparse linear functions, two-layer neural networks, and decision trees [42], gradient descent [37; 44; 46], automata [47], Turing machines [48], variational inference [49], and bandit algorithms [50]. Different from [15] that study COT via expressivity, we analyze reversal curse and COT via training dynamics.

Training dynamics of LLMs.There are rich literatures in the optimization of attention layers [51; 52; 53; 54; 55; 56; 57]. [58; 59] study the dynamics of a single linear attention layer in in-context linear regression. [60] proves convergence of one-layer transformers in random feature regime. [61] shows the convergence of gradient descent on one-layer transformers in in-context linear regression with orthogonal data. [14] studies the convergence of one-layer transformers in a class of next-token prediction tasks. [62] studies training dynamics of multi-layer transformers. [22] studies gradient descent on a class of two-layer transformers in in-context learning tasks with latent causal structures. Our paper studies the reversal curse via training dynamics under both bilinear settings and one-layer transformers. For one-layer transformers, we use the same framework as [14] without the need for certain technical assumptions such as long input sequences, different learning rates for different parameters (except for Appendix C.3), or weak correlations that are required for [14]. Besides, we focus on the generalization ability of models for logical reasoning tasks while [14] mainly focus on optimization, and we identify the asymmetry and intransitivity properties of model weights, which are the core reasons for the failure of LLM for certain types of logical reasoning. Moreover, our analysis of the bilinear model only requires the embedding to be almost orthonormal, while [14] essentially assumed the embedding vectors to be fixed and one-hot.

## 2 Preliminaries

Basic notations.For any integer \(N>0\), we use \([N]\) to denote the set \(\{1,2,\ldots,N\}\). Let \(\mathbb{R}\), \(\mathbb{N}\) denote the set of real numbers and natural numbers, respectively. For real variables \(x_{1},\ldots,x_{n}\), we use \(\operatorname{poly}(x_{1},\ldots,x_{n})\) to denote the polynomial of \(x_{1},\ldots,x_{n}\). We use \(f(n)\lesssim g(n)\) if there exists a constant \(C>0\) s.t. \(f(n)\leq Cg(n),\forall n\); we say \(g(n)\gtrsim f(n)\) if \(f(n)\lesssim g(n)\).

We use \(\bm{e}_{i}\) to denote one-hot vectors where only the \(i\)-th entry of \(\bm{e}_{i}\) equals one and all other entries are zero. We use \(\bm{1}\) to denote all-one vectors, \(\bm{0}\) to denote zero vectors or zero matrices, and \(I\) to denote the identity matrix. We will also add subscripts when we want to explicitly show the dimension, such as \(\bm{0}_{d}\), \(I_{d}\) for \(d\)-dimensional zero vector and \(d\times d\) identity matrix. We use \(\otimes\) to denote tensor product of vectors or matrices and use \(\bm{x}^{\otimes 2}\) and \(A^{\otimes 2}\) to denote \(\bm{x}\otimes\bm{x}\) and \(A\otimes A\) for vector \(\bm{x}\) and matrix \(A\).

We use \(\mathcal{N}(\bm{\mu},\Sigma)\) (or adding subscripts such as \(\mathcal{N}_{d}(\cdot,\cdot)\) if we want to show dimensions explicitly) to denote the (multi-variate) Gaussian distribution with mean \(\bm{\mu}\) and covariance \(\Sigma\). Also, we use \(\Delta(\mathcal{X})\) to denote the set of distributions over a set \(\mathcal{X}\) and use \(\mathbb{E}[\cdot]\) to denote expectation. For any dataset \(\mathcal{D}=\{x_{1},x_{2},\ldots,x_{n}\}\) where \(x_{i}\in\mathcal{X}\) and a function \(f:\mathcal{X}\rightarrow\mathbb{R}\), we define the empirical expectation over the dataset as \(\mathbb{E}_{\mathcal{D}}[f]=\frac{1}{n}\sum_{i=1}^{n}f(x_{i})\). See additional notations in Appendix A.

Auto-regressive models.Define the vocabulary \(\mathcal{V}=[M]\) for a positive integer \(M>0\) which is the size of the vocabulary. Let \(x=(x_{1},x_{2},\ldots,x_{T})\) be a sequence of tokens of length \(T\) where each token \(x_{t}\in\mathcal{V},~{}\forall t\in[T]\). See Table 1 for notations of different tokens used in Section 4. We study auto-regressive models \(p_{\theta}(\cdot|x)\in\Delta(\mathcal{V})\) parameterized by \(\theta\) that take the sequence \(x\) as input and predict the distribution of the next token \(x_{T+1}\in\mathcal{V}\). For both models that we study in this paper, the next token probability is modeled as the softmax applied to the logits \(l_{\theta}(\cdot|x)\in\mathbb{R}^{M}\) of each token in the vocabulary, i.e., \(p_{\theta}(y|x)=\frac{\exp(l_{\theta}(y|x))}{\sum_{v\in\mathcal{V}}\exp(l_{ \theta}(v|x))},~{}~{}\forall y\in\mathcal{V}\). Also, each token \(v\in\mathcal{V}\) has a corresponding (fixed or learnable) embedding vector \(\bm{u}_{v}\in\mathbb{R}^{d}\).

## 3 Bilinear Models

We start analyzing the reversal curse under bilinear models, which can be viewed as simplified one-layer transformers with input length one and decoder layer only. Also, in this section, we assume the embeddings of each token are fixed, so we directly use the embedding vector to represent a token.

Datasets.Assume the vocabulary has size \(m\) where each token \(v_{1},v_{2},\ldots,v_{m}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N} _{d}(0_{d},\frac{1}{d}I_{d})\). Let \(\mathcal{V}=\{v_{1},\ldots,v_{m}\}\) and let \(\mathcal{X}=\{x_{1},\ldots,x_{n}\}\) and \(\mathcal{Y}=\{y_{1},\ldots,y_{n}\}\) be disjoint random subsets of \(\mathcal{V}\). Assume all training and test sequences have a length of two. For any \(2\leq i\leq n\), the training dataset contains both sequence \((x_{i},y_{i})\) and \((y_{i},x_{i})\). In addition, the training set contains \((x_{1},y_{1})\) while the test set only contain one example \((y_{1},x_{1})\). During training, the model learns both \((x_{i},y_{i})\) and \((y_{i},x_{i})\) for \(i\geq 2\) to conclude that \((x_{i},y_{i})\) is equivalent to \((y_{i},x_{i})\). For example, \(\mathcal{X}\) is a set of names and \(\mathcal{Y}\) is a set of books. The sequence \((x_{i},y_{i})\) means "\(x_{i}\) is the author of \(y_{i}\)", and the sentence \((y_{i},x_{i})\) means "\(y_{i}\) is written by \(x_{i}\)". We test whether the model is able to infer an unseen sequence \((y_{1},x_{1})\) given the training data which includes the other direction \((x_{1},y_{1})\).

Bilinear model.We consider a bilinear model parameterized by \(\Theta\in\mathbb{R}^{d\times d}\) of which the input contains single token \(x\in\mathcal{V}\). The logits of the next token \(y\in\mathcal{V}\) is defined as \(l_{\Theta}(y|x)=x^{\top}\Theta y\) which is bilinear in \(x\) and \(y\), and thus the next token probability is \(p_{\Theta}(y|x)=\frac{\exp(l_{\Theta}(y|x))}{\sum_{v\in\mathcal{V}}\exp(l_{ \Theta}(v|x))}\). The training loss for the bilinear model is the cross-entropy loss \(\mathcal{L}(\Theta)=\frac{1}{2n-1}\left(\sum_{i=1}^{n}-\log p_{\Theta}(y_{i}| x_{i})+\sum_{i=2}^{n}-\log p_{\Theta}(x_{i}|y_{i})\right)\) and the test loss (reversal loss) is \(\mathcal{L}^{\texttt{rev}}(\Theta)=-\log p_{\Theta}(x_{1}|y_{1})\). We study the training dynamics of gradient flow \(\frac{d\Theta_{t}}{dt}=-\nabla\mathcal{L}(\Theta_{t})\) with the initialization \(\Theta_{0}\) that can be either randomly sampled from \(\mathcal{N}(\Theta^{\otimes 2},\sigma^{2}I^{\otimes 2})\) or set as a pretrained parameter satisfying \(\frac{1}{2m}<p_{\Theta_{0}}(y_{i}|x_{i}),p_{\Theta_{0}}(x_{i}|y_{i})<\frac{2}{m}\) for all \(i\in[n]\). The following theorem shows a separation during training dynamics.

**Theorem 1** (Separation of training dynamics (informal statement of Theorem 5)).: _Fix any \(\delta,\epsilon\in(0,1)\). For small \(\sigma\) and \(d\geq\operatorname{poly}(n,m,1/\epsilon,\log(1/\delta))\), with probability at least \(1-\delta\), we have_

\[\mathcal{L}^{\texttt{rev}}(\Theta_{t})/\mathcal{L}^{\texttt{rev}}(\Theta_{0}) \geq(\mathcal{L}(\Theta_{t})/\mathcal{L}(\Theta_{0}))^{\epsilon},~{}\forall t \geq 0.\]

Theorem 1 shows that the reversal loss is lower bounded by the training loss. Note that for large \(d\) and small \(\epsilon\) close to 0, \((\mathcal{L}(\Theta_{t})/\mathcal{L}(\Theta_{0}))^{\epsilon}\) is close to 1 and thus \(\mathcal{L}^{\texttt{rev}}(\Theta_{t})\gtrsim\mathcal{L}^{\texttt{rev}}( \Theta_{0})\) which implies that \(p_{\Theta}(x_{1}|y_{1})\) remains small during training. We summarize the above argument in Theorem 2.

**Theorem 2** (Lower bound of reversal loss (informal statement of Theorem 6)).: _Fix arbitrary \(c>0\) and \(C\leq\log(m/2)\). Suppose \(\sigma\) is small and \(d\geq\operatorname{poly}(n,m,\log(1/\delta),\log c,1/\log C)\). With probability at least \(1-\delta\), it holds that \(\mathcal{L}^{\texttt{rev}}(\Theta_{\tau})\geq C\), where \(\tau\) denotes the first time such that \(\mathcal{L}(\Theta_{t})\leq c\)._

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Enities & Forward & Backward & Direct & Indirect & Others \\ \hline A, B, C, A\({}_{i}\), B\({}_{i}\), C\({}_{i}\) & \(\rightarrow\) & \(\leftarrow\) & \(\rightarrow\) & \(\leadsto\) & R\({}_{1}\), R\({}_{2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notations for tokens in Section 4. “\(\rightarrow\)” and “\(\leftarrow\)” denote forward and backward relationships for the reversal curse. “\(\rightarrow\)” and “\(\leadsto\)” denote direct and indirect implication for COT. R\({}_{1}\) and R\({}_{2}\) are relationship tokens in Section 4.3. A, B, C, A\({}_{i}\), B\({}_{i}\), C\({}_{i}\) denote tokens representing entities.

The proofs of Theorems1 and 2 are deferred to AppendixB. Theorem2 implies that for large \(d^{2}\), while the training loss can be trained to be arbitrarily small, the reversal loss remains large. In other words, the model fails to infer an unseen sequence \((y_{1},x_{1})\) given the training data which includes the other direction \((x_{1},y_{1})\). Furthermore, even if the model is fine-tuned on new data from a pre-trained parameter \(\Theta_{0}\) that initially grasps the concept of reversal and satisfies \(\frac{1}{2m}<p_{\Theta_{0}}(y_{i}|x_{i}),p_{\Theta_{0}}(x_{i}|y_{i})<\frac{2}{m}\) for new data, it fails to extend this understanding to new, unseen data.

A core reason that the reversal curse happens on the above bilinear model is that the parameter matrix \(\Theta_{t}\) is asymmetric. Consequently, the logits \(l_{\Theta_{t}}(y|x)=x^{\top}\Theta_{t}y\) and \(l_{\Theta_{t}}(x|y)=y^{\top}\Theta_{t}x\) generally differ. Consider a special case where each \(v_{i}\) is a one-hot vector. Then \(l_{\Theta}(y|x)=x^{\top}\Theta y=\Theta_{ij}\) and \(l_{\Theta}(x|y)=y^{\top}\Theta x=\Theta_{ji}\) for \(x=\bm{e}_{i},y=\bm{e}_{j}\). Training on \((x,y)\) can increase \(\Theta_{ij}\) but not \(\Theta_{ji}\), which means the model does not automatically learn the reversal direction \((y,x)\) from \((x,y)\). In Section4, we will show that for one-layer transformers, the reversal curse is mainly caused by the same reason, i.e., the asymmetry of the model weights.

## 4 One-Layer Transformers

In Section3, we analyzed the reversal curse under a bilinear model. In this section, we analyze the reversal curse for one-layer transformers in a similar setting to [14] via training dynamics. We also extend our analysis to chain-of-thought in Section4.2.

Basic notations.Let \(\mathcal{V}=[M]\) be the vocabulary. For any token \(x\in[M]\), we also use the corresponding one-hot vector \(\bm{x}=\bm{e}_{x}\in\mathbb{R}^{M}\) to represent it. Let \(U=[\bm{u}_{1},\bm{u}_{2},\dots,\bm{u}_{M}]^{\top}\in\mathbb{R}^{M\times d}\) be the embedding matrix, where \(\bm{u}_{x}\in\mathbb{R}^{d}\) is the embedding of token \(x\in[M]\). Note that \(U^{\top}\bm{x}=\bm{u}_{x}\). Consider the \(i\)-th training sample in the dataset, \(x[i]=(x_{1}[i],\dots,x_{T[i]}[i],x_{T[i]+1}[i])\), a sequence of tokens of length \(T[i]+1\). Here, \(x_{T[i]+1}[i]\) is the next token (or equivalently the label) to be predicted, \(x_{T[i]}[i]\) is the query token, and \((x_{1}[i],\dots,x_{T[i]-1}[i])\) are contextual tokens. For token \(x_{t}[i]\), we also use its one-hot vector \(\bm{x}_{t}[i]=\bm{e}_{x_{t}[i]}\in\mathbb{R}^{M}\) to represent it. Define the contextual token matrix \(X[i]=[\bm{x}_{1}[i],\dots,\bm{x}_{T[i]-1}[i]]^{\top}\in\mathbb{R}^{(T[i]-1) \times M}\). We omit all \(i\) in notations when the context is clear.

One-layer transformer.For a training sample \(x=(x_{1},\dots,x_{T},x_{T+1})\), its contextual token matrix \(X=[\bm{x}_{1},\dots,\bm{x}_{T-1}]^{\top}\) and thus \(XU=[\bm{u}_{x_{1}},\dots,\bm{u}_{x_{T-1}}]^{\top}\) contains the contextual token embeddings. We study one-layer transformers in the same setting as [14]. In particular, for an input token sequence \((x_{1},\dots,x_{T})\), after the one-layer self-attention, we can obtain \(\tilde{\bm{u}}_{T}=U^{\top}\text{LN}(X^{\top}\bm{b}_{T})\) where \(b_{tT}=\frac{\exp(\bm{u}_{x_{T}}^{\top}W_{Q}W_{K}^{\top}\bm{u}_{x_{t}}/\sqrt{ d})}{\sum_{r^{\prime}=1}^{r-1}\exp(\bm{u}_{x_{T}}^{\top}W_{Q}W_{K}^{\top}\bm{u}_{x_{t}} /\sqrt{d})}\), \(\bm{b}_{T}=[b_{1T},\dots,b_{T-1,T}]^{\top}\) contains attention scores (after softmax) that query token \(x_{T}\) attend to each contextual token3, \(\text{LN}(\bm{x})=\bm{x}/\|\bm{x}\|_{2}\) is the \(\ell_{2}\)-normalization operator, and \(W_{Q},W_{K}\in\mathbb{R}^{d\times d_{k}}\) are trainable query and key matrices respectively. The logit of \(x\in[M]\) is then calculated by a decoder layer, i.e., \(l_{\theta}(x|x_{1},\dots,x_{T})=\bm{u}_{x}^{\top}W_{V}\tilde{\bm{u}}_{T}\), where \(\theta\) encodes all parameters in the transformer, and \(W_{V}\in\mathbb{R}^{d\times d}\) can be viewed as a reparameterization of value and output matrices. Finally, the next token prediction probability is obtained by

Footnote 3: foot: Note that we assume the query token will not attend to itself as in [14].

\[p_{\theta}(x|x_{1},\dots,x_{T})=\frac{\exp(l_{\theta}(x|x_{1},\dots,x_{T}))}{ \sum_{x^{\prime}\in[M]}\exp(l_{\theta}(x|x_{1},\dots,x_{T}))}=\frac{\exp(\bm{u }_{x}^{\top}W_{V}\tilde{\bm{u}}_{T})}{\sum_{x^{\prime}\in[M]}\exp(\bm{u}_{x^{ \prime}}^{\top}W_{V}\tilde{\bm{u}}_{T})}.\]

We use the cross-entropy loss function to train the model over the whole training set \(\mathcal{D}_{\text{train}}\), i.e., \(\max_{U,W_{K},W_{Q},W_{V}}J\triangleq\mathbb{E}_{\mathcal{D}_{\text{train}}}[ \log p_{\theta}(x_{T+1}|x_{1},\dots,x_{T})]\).

Reparameterization.Similar to [14], we define \(Z=UW_{Q}W_{K}^{\top}U^{\top}/\sqrt{d}\in\mathbb{R}^{M\times M}\) and \(Y=UW_{V}^{\top}U^{\top}\in\mathbb{R}^{M\times M}\) and are interested in their dynamics after reparameterization. Then, the attention score (after softmax) and next token probability become

\[b_{tT}=\frac{\exp(\bm{x}_{T}^{\top}Z\bm{x}_{t})}{\sum_{t^{\prime}=1}^{T-1}\exp( \bm{x}_{T}^{\top}Z\bm{x}_{t^{\prime}})},\;\;p_{\theta}(x|x_{1},\dots,x_{T})= \frac{\exp\left(\bm{x}^{\top}Y^{\top}\text{LN}(X^{\top}\bm{b}_{T})\right)}{ \sum_{x^{\prime}}\exp\left(\bm{x}^{\prime\top}Y^{\top}\text{LN}(X^{\top}\bm{b}_ {T})\right)},\] (1)and the objective can be written as

\[\max_{Y,Z}J=\mathbb{E}_{\mathcal{D}_{\text{train}}}[\bm{x}_{T+1}^{\top}Y^{\top} \text{LN}(X^{\top}\bm{b}_{T})-\log\sum_{x^{\prime}\in[M]}\bm{x}^{\prime\top}Y^{ \top}\text{LN}(X^{\top}\bm{b}_{T})].\] (2)

Let \(\eta_{Y}\), \(\eta_{Z}\) be the learning rate of matrices \(Y\) and \(Z\) respectively. Then the gradient of \(Y\) and \(Z\) can be characterized by the following lemma:

**Lemma 1** (Gradient of \(Y\) and \(Z\) for 1-layer transformer, Lemma 1 of [14]).: _The gradient of \(Y\) and \(Z\) w.r.t. (2) of batch size 1 and learning rate \(\eta_{Y}\) and \(\eta_{Z}\) can be written as_

\[\dot{Y}=\eta_{Y}\text{LN}(X^{\top}\bm{b}_{T})(\bm{x}_{T+1}-\bm{\alpha})^{\top },\ \ \dot{Z}=\eta_{Z}\bm{x}_{T}(\bm{x}_{T+1}-\bm{\alpha})^{\top}Y^{\top}\frac{P_{X^{ \top}\bm{b}_{T}}^{\perp}}{\|X^{\top}\bm{b}_{T}\|_{2}}X^{\top}\operatorname{ diag}(\bm{b}_{T})X,\] (3)

_where \(P_{\bm{v}}^{\perp}\triangleq I-\bm{v}\bm{v}^{\top}/\|\bm{v}\|_{2}^{2}\) projects any vector to orthogonal complement of \(\bm{v}\), \(\bm{\alpha}=[\alpha_{1},\alpha_{2},\dots,\alpha_{M}]^{\top}\in\mathbb{R}^{M}\) with \(\bm{\alpha}=\exp\left(Y^{\top}\text{LN}(X^{\top}\bm{b}_{T})\right)/\bm{1}^{ \top}\exp\left(Y^{\top}\text{LN}(X^{\top}\bm{b}_{T})\right)\)._

Proof.: \(\dot{Y},\dot{Z}\) can be obtained by direct calculation. One can refer to the proof of Lemma 1 of [14]. 

### Main results for the reversal curse

In this section, we analyze the reversal curse where data points are three-token sentences "A \(\rightarrow\) B" or "B \(\leftarrow\) A". For each sentence, A and B are two distinct tokens that represent two entities, and "\(\rightarrow\)" and "\(\leftarrow\)" are two special tokens representing a pair of relationships inverse to each other.

Datasets.Let \(N_{\text{train}}>0\), \(N_{\text{test}}^{(1)}>0\) and \(N_{\text{test}}^{(2)}>0\) and denote \(N_{\text{total}}=N_{\text{train}}+N_{\text{test}}^{(1)}+N_{\text{test}}^{(2)}\). Let \(\texttt{A}_{i},\texttt{B}_{i}\in\mathcal{V},\forall i\in[N_{\text{total}}]\) be \(2N_{\text{total}}\) distinct tokens representing distinct entities. Let \(\rightarrow\), \(\leftarrow\in\mathcal{V}\) be two additional different tokens that represent two inverse relationships. Specifically, we have \(\texttt{A}_{i}\rightarrow\texttt{B}_{i}\) and \(\texttt{B}_{i}\leftarrow\texttt{A}_{i}\) for all \(i\in[N_{\text{total}}]\). For notation convenience, we define the following three index sets

\[\mathcal{I}_{\text{train}}=[N_{\text{train}}],\qquad\mathcal{I}_{\text{test}}^ {(1)}=[N_{\text{train}}+N_{\text{test}}^{(1)}]\backslash\mathcal{I}_{\text{ train}},\qquad\mathcal{I}_{\text{test}}^{(2)}=[N_{\text{total}}]\backslash( \mathcal{I}_{\text{train}}\cup\mathcal{I}_{\text{test}}^{(1)}).\]

The training set \(\mathcal{D}_{\text{train}}\) consists of all \(\texttt{A}_{i}\rightarrow\texttt{B}_{i}\) and \(\texttt{B}_{i}\leftarrow\texttt{A}_{i}\) for \(i\in\mathcal{I}_{\text{train}}\). In addition, \(\mathcal{D}_{\text{train}}\) contains \(\texttt{A}_{i}\rightarrow\texttt{B}_{i}\) for \(i\in\mathcal{I}_{\text{test}}^{(1)}\) and \(\texttt{B}_{i}\leftarrow\texttt{A}_{i}\) for \(i\in\mathcal{I}_{\text{test}}^{(2)}\). For convenience, we let \(N=|\mathcal{D}_{\text{train}}|\) to be the size of the training set. The test set \(\mathcal{D}_{\text{test}}\) consists of \(\texttt{B}_{i}\leftarrow\texttt{A}_{i}\) for \(i\in\mathcal{I}_{\text{test}}^{(1)}\) and \(\texttt{A}_{i}\rightarrow\texttt{B}_{i}\) for \(i\in\mathcal{I}_{\text{test}}^{(2)}\). Under our construction of the dataset, the LLM will learn the relationship between \(\texttt{A}_{i}\) and \(\texttt{B}_{i}\) for \(i\in\mathcal{I}_{\text{train}}\) in both directions to deduce that \(\rightarrow\) is reverse to \(\leftarrow\), and learn the relationship between \(\texttt{A}_{i}\) and \(\texttt{B}_{i}\) for \(i\in\mathcal{I}_{\text{test}}^{(1)}\cup\mathcal{I}_{\text{test}}^{(2)}\) in one direction and will be tested for the other.

We use \(p_{\theta}(\texttt{A}_{i}|\texttt{B}_{i}\leftarrow)\) and \(p_{\theta}(\texttt{B}_{i}|\texttt{A}_{i}\rightarrow)\) to more compactly represent \(p_{\theta}(x_{3}=\texttt{A}_{i}|x_{1}=\texttt{B}_{i},x_{2}=\leftarrow)\) and \(p_{\theta}(x_{3}=\texttt{B}_{i}|x_{1}=\texttt{A}_{i},x_{2}=\rightarrow)\), respectively. Our goal is to prove through the training dynamics of one-layer transformers that the test probability remains negligible during training. In particular, we are interested in \(p_{\theta}(\texttt{A}_{i}|\texttt{B}_{i}\leftarrow),\forall i\in\mathcal{I}_{ \text{test}}^{(1)}\) and \(p_{\theta}(\texttt{B}_{i}|\texttt{A}_{i}\rightarrow),\forall i\in\mathcal{I}_{ \text{test}}^{(2)}\).

For convenience, we assume zero-initialization \(Y(0)=\bm{0}\) and \(Z(0)=\bm{0}\). This is the same as [14] and is reasonable since empirically, \(Y\) and \(Z\) are usually initialized as inner products of \(d\)-dimensional vectors with i.i.d Gaussian entries, and thus are almost zero (Lemma 8 in Appendix B). The following proposition shows the initial train/test probabilities are uniform over the vocabulary \(\mathcal{V}\).

**Proposition 4.1** (Initial probability under zero initializaion).: _Assume the transformer is under zero-initialization \(\theta(0)=(Y(0),Z(0))\) with \(Y(0)=\bm{0}\) and \(Z(0)=\bm{0}\). For any \(i\in[N_{\text{total}}]\), we have_

\[p_{\theta(0)}(\texttt{B}_{i}|\texttt{A}_{i}\rightarrow)=p_{\theta(0)}(\texttt{A }_{i}|\texttt{B}_{i}\leftarrow)=1/M.\]

The proof is deferred to Appendix C.1.1. Proposition 4.1 shows that initially, the probability of predicting any B (or A, respectively) given any A \(\rightarrow\) (or B \(\leftarrow\), respectively) as input is uniform over the whole vocabulary. When \(Y(0)\) and \(Z(0)\) are not exactly \(\bm{0}\) but close to \(\bm{0}\), the initial prediction will still be close to the uniform distribution, which is similar to Lemma 6. Next we analyze the dynamics of \(p_{\theta(t)}(\texttt{B}_{i}|\texttt{A}_{i}\rightarrow)\) and \(p_{\theta(t)}(\texttt{A}_{i}|\texttt{B}_{i}\leftarrow)\).

**Proposition 4.2** (Next token probability).: _For input sequence \((x_{1},x_{2})\), the next token probability under parameters \(\theta(t)\) is \(p_{\theta(t)}(x|x_{1},x_{2})=\exp\left(Y(t)_{x_{1},x}\right)/\sum_{x^{\prime}\in[M]} \exp\left(Y(t)_{x_{1},x^{\prime}}\right)\), where \(Y(t)_{i,j}\) is the entry of the matrix \(Y(t)\) at row \(i\) and column \(j\)._The proof is also deferred to SectionC.1.1. According to Section4.2, the next token probability when the entity in the input is \(x_{1}\) is determined by the \(x_{1}\)-th row of the matrix \(Y(t)\). Another nice property indicated by Section4.2 is that we don't need to keep track of the dynamics of \(Z(t)\), which could greatly simplify the analysis. The following lemma shows the dynamics of \(Y(t)\).

**Lemma 2** (Dynamics of \(Y(t)\)).: _Assume we run SGD with batch size 14, and assume \(M\gg 100\) and \(\frac{1}{M^{0.99}}\ll\eta_{Y}<1\). Let \(t\gtrsim\frac{N\ln M}{\eta_{Y}}\) and let \(Y(t)_{i}\) denote the \(i\)-th row of \(Y(t)\) and \(Y(t)_{ij}\) denote the \((i,j)\)-th entry of \(Y(t)\). Then for training sequence \((x_{1},x_{2},x_{3})\in\mathcal{D}_{\text{train}}\) at time \(t\), we have_

Footnote 4: The lemma holds even if the batch size is larger than 1 and the analysis is essentially the same.

\[Y(t)_{x_{1},x_{3}}\gtrsim\ln\left(M\eta_{Y}t/N\right),\quad\text{and}\quad Y(t )_{x_{1},x}\lesssim-\ln\left(M\eta_{Y}t/N\right)/M,\quad\forall x\neq x_{3},\]

_and for any test sequence \((x_{1},x_{2},x_{3})\in\mathcal{D}_{\text{test}}\), we have \(Y(t)_{x_{1},x}=0,\forall x\in[M]\)._

The proof of Section2 is presented in SectionC.1.2. Lemma2 implies the asymmetry of the model weights \(Y(t)\): for two tokens \(x_{1},x_{3}\), when \(x_{1}\) appears as a contextual token and \(x_{3}\) serves as the next token in the same training sequence, the model weights \(Y(t)_{x_{1},x_{3}}\) gets increased during training while \(Y(t)_{x_{3},x_{1}}\) will not get increased. Combining Section4.2, we can obtain our main theorem for the reversal curse.

**Theorem 3** (Reversal curse).: _Assume we run SGD with batch size 1, and assume \(M\gg 100\) and \(\frac{1}{M^{0.99}}\ll\eta_{Y}<1\). Let \(t\gtrsim\frac{N\ln M}{\eta_{Y}}\) denote the time step which also satisfies \(\ln t\gtrsim\ln(NM/\eta_{Y})\). For training sequence \((x_{1},x_{2},x_{3})\in\mathcal{D}_{\text{train}}\) at time \(t\), we have_

\[p_{\theta(t)}(x_{3}|x_{1},x_{2})\geq 1-(M-1)(M\eta_{Y}t/N)^{-c}\to 1,\quad \text{as}\;\;t\to\infty\]

_for some constant \(c>0\), and for any test sequence \((x_{1},x_{2},x_{3})\in\mathcal{D}_{\text{test}}\) that is not included in the training set \(\mathcal{D}_{\text{train}}\), we have \(p_{\theta(t)}(x_{3}|x_{1},x_{2})\leq 1/M\)._

Theorem3 shows that although the direction presented in the training set can be learned nearly perfectly, the model's next token prediction of the reverse direction is almost a random guess. The proof is deferred to SectionC.1.3. We also empirically validate the above results for multi-layer transformers in Section5.

### Chain-of-thought

In this section, we extend our analysis in Section4.1 to study other logical relationships. In particular, we study chain-of-thought (COT) [4] and show its importance via training dynamics. COT encourages LLMs to output a series of intermediate reasoning steps to increase their performance. Consider the simplest example, where the model learns two facts that \(\texttt{A}\to\texttt{B}\) and \(\texttt{B}\to\texttt{C}\), and we want to test whether the model is able to directly conclude that \(\texttt{A}\leadsto\texttt{C}\). COT indicates that if an LLM is only trained on \(\texttt{A}\to\texttt{B}\) and \(\texttt{B}\to\texttt{C}\), it would be easier for the model to deduce \(\texttt{A}\leadsto\texttt{C}\) during the inference time if the model can first output the intermediate steps \(\texttt{A}\to\texttt{B}\) and \(\texttt{B}\to\texttt{C}\), instead of directly predicting the next token \(\texttt{C}\) given the input "\(\texttt{A}\leadsto\)". The failure of directly deducing \(\texttt{A}\leadsto\texttt{C}\) is also empirically observed by [9].

Theoretically, [15] shows the importance of COT for some complex reasoning tasks through the lens of the expressivity of transformers. In this section, we show the importance of COT through a different angle, i.e., training dynamics. We show that for the above simplest two-step reasoning, without COT, the model is not able to directly predict \(\texttt{C}\) given the input "\(\texttt{A}\leadsto\)" even if it learns \(\texttt{A}\to\texttt{B}\) and \(\texttt{B}\to\texttt{C}\).

**Theorem 4** (Importance of chain-of-thought, informal statement of Section7).: _Under certain assumptions as stated in Section7, for any \(\texttt{A}_{i},\texttt{B}_{i},\)\(\texttt{C}_{i}\) s.t. \(\texttt{A}_{i}\to\texttt{B}_{i}\) and \(\texttt{B}_{i}\to\texttt{C}_{i}\) are in the training set but \(\texttt{A}_{i}\leadsto\texttt{C}_{i}\) is not, we have_

\[p_{\theta(t)}(\texttt{B}_{i}|\texttt{A}_{i}\to)\to 1,\quad p_{\theta(t)}(\texttt{C}_{i}| \texttt{B}_{i}\to)\to 1,\quad p_{\theta(t)}(\texttt{C}_{i}|\texttt{A}_{i} \leadsto\leq 1/M,\quad\text{as}\;\;t\to\infty.\]

We defer the details of the dataset construction and proof to SectionC.2. Theorem4 shows that although the LLM learns \(\texttt{A}_{i}\to\texttt{B}_{i}\) and \(\texttt{B}_{i}\to\texttt{C}_{i}\) nearly perfectly, it cannot directly deduce \(\texttt{A}_{i}\leadsto\texttt{C}_{i}\). Analogous to the asymmetry of causal transformer weights as we discussed in Section4.1, our analysis of COT reveals another property, i.e., intransitivity: training the weights associated with \(\texttt{A}\) to \(\texttt{B}\) and \(\texttt{B}\) to \(\texttt{C}\) does not necessarily increase the weights associated with \(\texttt{A}\) to \(\texttt{C}\).

We also emphasize that the model fails to directly deduce \(\mathsf{A}_{i}\leadsto\mathsf{C}_{i}\) when the two intermediate steps \(\mathsf{A}_{i}\to\mathsf{B}_{i}\) and \(\mathsf{B}_{i}\to\mathsf{C}_{i}\) are trained _separately_. If the two steps are concatenated into a single training sequence, it is possible that the model learns \(\mathsf{A}_{i}\leadsto\mathsf{C}_{i}\) directly [19].

### Roles of the attention score matrix

During the analysis of Sections 4.1 and 4.2, we show that the reversal curse and the importance of COT are largely due to the asymmetry and intransitivity of causal transformer weights (in our case, the weight matrix \(Y(t)\)). However, it seems that the dynamics of the attention score matrix \(Z(t)\) do not impact the model performance. Below, we briefly discuss the role of the attention score matrix \(Z(t)\).

In (1), the attention score is used to calculate the weights \(b_{tT}\), where a contextual token \(x_{t}\) with a larger attention score attended by the query token \(x_{T}\) has a larger weight. Note that we use the same formulation as the previous work [14] where the query token will not attend to itself. Therefore, for a three-token training sequence, the weights \(b_{12}\) is always one since there is only one contextual token \(x_{1}\), no matter whether the value of the attention score is high or low.

However, consider a slightly different setting, where the relationship is represented by two tokens. In that case, \(x_{1}=\mathsf{A}_{i},x_{2}=\mathsf{R}_{1},x_{3}=\mathsf{R}_{2},x_{4}=\mathsf{ B}_{i}\), and there are two contextual tokens \(\mathsf{A}_{i}\) and \(\mathsf{R}_{1}\). The role of the attention score is then to select the important token, i.e., \(\mathsf{A}_{i}\), by putting more weights on it. Theorem 2 of [14] showed that under certain assumptions, the query token \(\mathsf{R}_{2}\) will attend more to "distinct tokens" \(\mathsf{A}_{i}\) and less to the "common token" \(\mathsf{R}_{1}\). Therefore, the query token \(\mathsf{R}_{2}\) will eventually put all weights to \(\mathsf{A}_{i}\), and the remaining analysis remains the same as in Sections 4.1 and 4.2. See Appendix C.3 for a more rigorous analysis.

## 5 Experiments

In this section, we conduct experiments to further validate our theoretical results in Section 4 on multi-layer transformers. We show experimental results of the reversal curse in this section and COT in Appendix D. Note that in Sections 3 and 4, we theoretically proved the reversal curse for both the bilinear model and one-layer transformer under certain assumptions. Now, we empirically show that the reversal curse still happens even for multi-layer transformers. In Appendix E.2.3, we also provide empirical results that the reversal curse does not happen in ICL settings.

Figure 1: Experiment results of reversal curse under default configuration (see Table 3). The curves represent the (average) negative log probability of the model predicting the next token to be \(\mathsf{B}_{i}\) when the input is “\(\mathsf{A}_{i}\to\)”, or to be \(\mathsf{A}_{i}\) when the input is “\(\mathsf{B}_{i}\leftarrow\)”. While the sentences in the training set can be learned nearly perfectly (as shown by the training curve where the next token probability converges to one), the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.

Dataset construction.Below, we describe how we generate our synthetic dataset for experiments on the reversal curse. We choose the vocabulary \(\mathcal{V}=\{0,1,\dots,N\}\) for a specified \(N>0\). We randomly sample two disjoint sets of entities \(\mathcal{A},\mathcal{B}\subset\mathcal{V}\) with \(|\mathcal{A}|=|\mathcal{B}|=|\mathcal{V}|/4\), and reserve two additional tokens for relationships \(\to\) and \(\leftarrow\), respectively. 5 Next, we specify a bijection from \(\mathcal{A}\) to \(\mathcal{B}\) uniformly at random. For each \(\mathbb{A}_{i}\in\mathcal{A}\) and its corresponding \(\mathbb{B}_{i}\in\mathcal{B}\), we can obtain a pair of sequence (\(\mathbb{A}_{i}\to\mathbb{B}_{i}\), \(\mathbb{B}_{i}\leftarrow\mathbb{A}_{i}\)). We split the set of all pairs into training pairs and validation pairs. For each training pair, both sequences will be included in the training set, while for the validation pair, we randomly select one sequence for the training set and the other for the validation set. Therefore, the model will learn both directions for the training pairs and only one direction for each validation pair while being tested in the unseen direction.

Footnote 5: By default, each entity consists of one token. See multi-token experiments in Appendices E.2 and E.3

Model architectures.We train multi-layer transformers based on GPT-2 architecture [63]. Figure 1 shows the results where the model has 24 layers, 12 attention heads per layer, uses absolute positional encoding, and we choose the vocabulary size of 800. The training set size is 340, and the validation set size is 60 (resulting from 140 training pairs and 60 validation pairs). We also conducted experiments with various model configurations and vocabulary sizes in Appendix E.2. Besides, all hyperparameters and different model configurations are presented in Appendix E.1.

Results.Figure 1 shows that during the training, the next token probability for training data increases a lot while the next token probability for validation data remains unchanged or gets even smaller. This is consistent with our theoretical results of Theorem 3.

According to our theoretical analysis, the reversal curse happens due to the asymmetry of model (reparameterized) weights (i.e., logits of a token given another token as input), and we also empirically validate the asymmetry for multi-layer transformers. Figure 2 shows the model weights from a token \(x_{1}\) to \(x_{3}\) is trained large for a training sequence \((x_{1},x_{2},x_{3})\) as represented by the diagonals of the

Figure 2: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for the reversal curse experiment. For the top-left matrix, the \(i\)-th row corresponds to an entity token \(\mathbb{A}_{i}\) for a training pair, and the \(i\)-th column corresponds to an entity token \(\mathbb{B}_{i}\) for a training pair. The \((i,j)\)-th entry represents the model weights from the token \(\mathbb{A}_{i}\) to \(\mathbb{B}_{j}\), i.e., the logits of \(\mathbb{B}_{j}\) when the input sequence consists of only \(\mathbb{A}_{i}\). Similarly, for the bottom-left matrix, the row corresponds to the input entity tokens of the seen direction (the direction included in the training set) of validation pairs, and the column corresponds to output entity tokens. The two matrices on the right are obtained by swapping row tokens and column tokens of their corresponding left matrices. Note that the diagonals of the bottom-right matrix are all close to zero, while the diagonals of other matrices all have large values. This implies that if a pair of tokens \((\mathbb{A},\mathbb{B})\) only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained.

first three matrices, while the weights from a token \(x_{1}\) to \(x_{3}\) remains nearly zero for a validation sequence \((x_{1},x_{2},x_{3})\) as represented by the diagonals of the last matrix, which is consistent with Lemma 2. This implies that if a pair of tokens \((\mathtt{A},\mathtt{B})\) only appear in the training set in one direction, then the model weights associated with the other direction will hardly get trained.

## 6 Conclusions

In this paper, we study the reversal curse theoretically via training dynamics of (1) a bilinear model, which is a simplification of the one-layer transformer; (2) one-layer transformers under certain technical assumptions similar to [14]. Our theoretical results suggest that a core reason the reversal curse happens in auto-regressive LLMs is the asymmetry of the model weights, and we apply our technique to prove the necessity of COT for one-layer transformers, which is mainly due to the intransitivity of model weights. The asymmetry and intransitivity of model weights caused by unconstrained optimization of CE loss indicate that an auto-regressive LLM might mainly focus on learning text sequences during training _separately_ instead of automatically deducing indirect conclusions under the current popular training paradigms. This highlights the importance of ICL, data augmentation, or planning for current auto-regressive LLMs to solve complex reasoning tasks.

As for future directions, it would be interesting and important to study: (1) What is a unified way to characterize and study the reversal curse, COT, and other similar logical reasoning tasks? (2) Our paper mainly focuses on three-token sequences, where each entity or relationship is represented by a single token. While we empirically explored the setting where each entity might consist of multiple tokens and distinct entities might share a few tokens, it would be interesting to analyze the multiple-token setting theoretically. (3) We theoretically analyzed the bilinear model and one-layer transformer, and it would be an important future direction to extend the analysis to multi-layer transformers.

## Acknowledgements

This work was partially supported by a gift from Open Philanthropy to the Center for Human-Compatible AI (CHAI) at UC Berkeley and by NSF Grants IIS-1901252 and CCF-2211209. The work was done when HZ was a visiting researcher at Meta.

## References

* [1] Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In _Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems_, pages 1-7, 2021.
* [2] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35:24824-24837, 2022.
* [5] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* [6] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_, 2021.
* [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.

* [8] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrap reasoning with reasoning. _Advances in Neural Information Processing Systems_, 35:15476-15488, 2022.
* [9] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation. _arXiv preprint arXiv:2309.14402_, 2023.
* [10] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on" a is b" fail to learn" b is a". _arXiv preprint arXiv:2309.12288_, 2023.
* [11] Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, and Yujiu Yang. Mitigating reversal curse via semantic-aware permutation training. _arXiv preprint arXiv:2403.00758_, 2024.
* [12] Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. Reverse training to nurse the reversal curse. _arXiv preprint arXiv:2403.13799_, 2024.
* [13] Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan. Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse. _arXiv preprint arXiv:2311.07468_, 2023.
* [14] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023.
* [15] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: a theoretical perspective. _Advances in Neural Information Processing Systems_, 36, 2024.
* [16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pages 9118-9147. PMLR, 2022.
* [17] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Miaeutic prompting: Logically consistent reasoning with recursive explanations. _arXiv preprint arXiv:2205.11822_, 2022.
* [18] Simon Jerome Han, Keith J Ransom, Andrew Perfors, and Charles Kemp. Inductive reasoning in humans and large language models. _Cognitive Systems Research_, 83:101155, 2024.
* [19] Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, and William Yang Wang. Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation. _arXiv preprint arXiv:2402.03268_, 2024.
* [20] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* [21] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* [22] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. _arXiv preprint arXiv:2402.14735_, 2024.
* [23] Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt. A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task. _arXiv preprint arXiv:2402.11917_, 2024.
* [24] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for lms on planning and reasoning about change). _arXiv preprint arXiv:2206.10498_, 2022.
* [25] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems and Technology_, 2023.
* [26] Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, and Talia Ringer. Transformer-based models are not yet perfect at learning to emulate structural recursion. _arXiv preprint arXiv:2401.12947_, 2024.
* [27] Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, and Yuanjun Laili. An investigation of llms' inefficacy in understanding converse relations. _arXiv preprint arXiv:2310.05163_, 2023.

* [28] Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, and Yujiu Yang. Chain of history: Learning and forecasting with llms for temporal knowledge graph completion. _arXiv preprint arXiv:2401.06072_, 2024.
* [29] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* [30] Satwik Bhattacharya, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. _arXiv preprint arXiv:2009.11264_, 2020.
* [31] Satwik Bhattacharya, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. _arXiv preprint arXiv:2006.09286_, 2020.
* [32] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. _arXiv preprint arXiv:1807.03819_, 2018.
* [33] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing-complete. _Journal of Machine Learning Research_, 22(75):1-35, 2021.
* [34] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* [35] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1:1, 2021.
* [36] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. _arXiv preprint arXiv:2106.03764_, 2021.
* [37] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* [38] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023.
* [39] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. _arXiv preprint arXiv:2105.11115_, 2021.
* [40] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. _arXiv preprint arXiv:2207.04901_, 2022.
* [41] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. _Advances in Neural Information Processing Systems_, 35:21750-21764, 2022.
* [42] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* [43] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. _arXiv preprint arXiv:2212.07677_, 2022.
* [44] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _arXiv preprint arXiv:2306.04637_, 2023.
* [45] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression. _arXiv preprint arXiv:2304.13276_, 2023.
* [46] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* [47] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.

* [48] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022.
* [49] Song Mei and Yuchen Wu. Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models. _arXiv preprint arXiv:2309.11420_, 2023.
* [50] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. _arXiv preprint arXiv:2310.08566_, 2023.
* [51] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In _International Conference on Learning Representations_, 2020.
* [52] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In _International Conference on Machine Learning_, pages 4376-4386. PMLR, 2020.
* [53] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. _arXiv preprint arXiv:2203.03466_, 2022.
* [54] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Transformers learn through gradual rank increase. _arXiv preprint arXiv:2306.07042_, 2023.
* [55] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. _arXiv preprint arXiv:2306.00802_, 2023.
* [56] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, 35:37822-37836, 2022.
* [57] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. _arXiv preprint arXiv:2103.07601_, 2021.
* [58] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. _arXiv preprint arXiv:2307.03576_, 2023.
* [59] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_, 2023.
* [60] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. _Advances in Neural Information Processing Systems_, 36, 2024.
* [61] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_, 2023.
* [62] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. _arXiv preprint arXiv:2310.00535_, 2023.
* [63] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emmlp-demos.6.
* [64] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. _Annals of Statistics_, pages 1302-1338, 2000.
* [65] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.

## Appendix A Additional Notations

Let \(\delta_{ij}=1\) for \(i=j\) and \(\delta_{ij}=0\) for \(i\neq j\). For a squared matrix \(A\in\mathbb{R}^{d\times d}\), its trace is \(\mathrm{Tr}(A)=\sum_{i=1}^{d}A_{ii}\). For two matrices \(A,B\in\mathbb{R}^{m\times n}\) of the same shape, their inner product is defined as \(\langle A,B\rangle=\mathrm{Tr}(AB^{\top})\). For any matrix \(A\in\mathbb{R}^{m\times n}\), its (Frobenius) norm is defined as \(\|A\|=\sqrt{\langle A,A\rangle}\). For any vector \(\bm{x}=(x_{1},\ldots,x_{d})^{\top}\in\mathbb{R}^{d}\) or a matrix \(A\in\mathbb{R}^{m\times n}\), we define the zero-norm as \(\|\bm{x}\|_{0}=\sum_{i=1}^{d}\mathbbm{1}\{x_{i}\neq 0\}\) or \(\|A\|_{0}=\sum_{i=1}^{m}\sum_{j=1}^{n}\mathbbm{1}\{A_{ij}\neq 0\}\) where \(\mathbbm{1}\{\cdot\}\) is the indicator function.

## Appendix B Missing Proofs of Section 3

**Theorem 5** (Separation of training dynamics, formal statement of Theorem 1).: _Fix arbitrary \(\delta,\epsilon\in(0,1)\). Let \(v_{1},\ldots,v_{m}\) be independently sampled from \(\mathcal{N}_{d}(\bm{0}_{d},\frac{1}{d}I_{d})\). Let \(x_{1},\ldots,x_{n}\) and \(y_{1},\ldots,y_{n}\) be sampled uniformly at random from \(\{v_{1},\ldots,v_{m}\}\) without replacement. Define_

\[\mathcal{L}(\Theta) =\frac{1}{2n-1}\left(\sum_{i=1}^{n}-\log p_{\Theta}(y_{i}|x_{i})+ \sum_{i=2}^{n}-\log p_{\Theta}(x_{i}|y_{i})\right)\] \[\mathcal{L}^{\mathsf{rev}}(\Theta) =\ -\log p_{\Theta}(x_{1}|y_{1}).\]

_Consider the gradient flow \(\Theta_{t}:t\geq 0\)_

\[\frac{d\Theta_{t}}{dt}=-\nabla\mathcal{L}(\Theta_{t}).\]

_where \(\Theta_{0}\sim\mathcal{N}(\bm{0}^{\otimes 2},\sigma^{2}\cdot I^{\otimes 2})\) or \(\Theta_{0}\) satisfies \(\frac{1}{2m}<p_{\Theta_{0}}(y_{i}|x_{i}),p_{\Theta_{0}}(x_{i}|y_{i})<\frac{2}{m}\) for all \(i\in[n]\). Suppose \(\sigma\leq\frac{1}{100\ln(64m^{2}/\delta)}\) and_

\[d\geq\frac{10^{6}n^{4}m^{2}\log^{4}(2m)\log(64m^{2}n^{2}/\delta)}{\epsilon^{2 }}.\]

_With probability at least \(1-\delta\), we have_

\[\frac{\mathcal{L}^{\mathsf{rev}}(\Theta_{t})}{\mathcal{L}^{\mathsf{rev}}( \Theta_{0})}\geq\left(\frac{\mathcal{L}(\Theta_{t})}{\mathcal{L}(\Theta_{0})} \right)^{\epsilon},\;\forall t\geq 0.\]

Proof.: Let \(v=\sqrt{\frac{400n^{2}m^{2}\log(64m^{2}n^{2}/\delta)}{d}}\). By Lemma 3 and Lemma 4, with probability at least \(1-\delta\),

\[\mathcal{L}^{\mathsf{rev}}(\Theta_{t}) \geq\mathcal{L}^{\mathsf{rev}}(\Theta_{0})\cdot\left(1+\frac{ \mathcal{L}(\Theta_{0})\cdot t}{8(2n-1)\log^{2}(2m)}\right)^{-8v(2n-1)\log^{2 }(2m)}.\] \[\geq\mathcal{L}^{\mathsf{rev}}(\Theta_{0})\cdot\left(\frac{ \mathcal{L}(\Theta_{t})}{\mathcal{L}(\Theta_{0})}\right)^{8v(2n-1)\log^{2}(2m)}\]

By definition of \(d\), we have \(8v(2n-1)\log^{2}(2m)\leq\epsilon\). Notice that \(\frac{\mathcal{L}(\Theta_{t})}{\mathcal{L}(\Theta_{0})}<1\), thus

\[\mathcal{L}^{\mathsf{rev}}(\Theta_{t})\geq\mathcal{L}^{\mathsf{rev}}(\Theta_ {0})\cdot\left(\frac{\mathcal{L}(\Theta_{t})}{\mathcal{L}(\Theta_{0})}\right)^ {\epsilon}.\]

**Theorem 6** (Lower bound of reversal loss, formal statement of Theorem 2).: _Fix arbitrary \(c>0\) and \(C\leq\log(m/2)\). Under the setting of Theorem 5, suppose \(\sigma\leq\frac{1}{100\ln(64m^{2}/\delta)}\) and_

\[d\geq 10^{6}n^{4}m^{2}\log^{4}(2m)\log(64m^{2}n^{2}/\delta)\cdot\frac{\log^{2} \frac{c}{\log(2m)}}{\log^{2}\frac{C}{\log(m/2)}}.\]

_With probability at least \(1-\delta\),_

\[\mathcal{L}^{\mathsf{rev}}(\Theta_{\tau})\geq C.\]

_where \(\tau\) denotes the first time such that \(\mathcal{L}(\Theta_{t})\leq c\)._Proof.: By continuity, \(\mathcal{L}(\Theta_{\tau})=c\). By Theorem5, when

\[d\geq\frac{10^{6}n^{4}m^{2}\log^{4}(2m)\log(64m^{2}n^{2}/\delta)}{ \epsilon^{2}}\] (4)

with probability at least \(1-\delta\),

\[\mathcal{L}^{\mathtt{rev}}(\Theta_{\tau}) \geq\mathcal{L}^{\mathtt{rev}}(\Theta_{0})\cdot\left(\frac{ \mathcal{L}(\Theta_{\tau})}{\mathcal{L}(\Theta_{0})}\right)^{\epsilon}\] \[\geq\mathcal{L}^{\mathtt{rev}}(\Theta_{0})\cdot\left(\frac{c}{ \mathcal{L}(\Theta_{0})}\right)^{\epsilon}.\]

Under this event, applying Lemma6, we can obtain

\[\mathcal{L}^{\mathtt{rev}}(\Theta_{\tau})\geq\log(m/2)\cdot \left(\frac{c}{\log(2m)}\right)^{\epsilon}.\]

To ensure that the right hand side is \(C\), we set \(\epsilon=\frac{\log\frac{C}{\log(m/2)}}{\log\frac{C}{\log(2m)}}\). One may check that the definition of \(d\) satisfies Eq.4. It follows that

\[\mathcal{L}^{\mathtt{rev}}(\Theta_{\tau})\geq C.\]

### Training dynamics

**Lemma 3** (Dynamics of the forward loss).: _Let \(x_{1},\ldots,x_{n}\), \(y_{1},\ldots,y_{n}\), \(\mathcal{L}(\Theta)\), and \(\Theta_{t}\)\((t\geq 0)\) be defined as in Theorem5. When \(\sigma\leq\frac{1}{100\ln(16n^{2}/\delta)}\) and \(d\geq 1600n^{3}m^{2}\log(8m^{2}n^{2}/\delta)\), with probability at least \(1-\delta\), we have_

\[\mathcal{L}(\Theta_{t})\leq\frac{1}{\frac{t}{8(2n-1)\log^{2}(2m )}+\frac{1}{\mathcal{L}(\Theta_{0})}},\,\forall t\geq 0.\]

_Furthermore,_

\[\inf\left\{p_{\Theta_{t}}(x_{i}|y_{i}),p_{\Theta_{t}}(y_{i}|x_{i} ):t\geq 0,i\in[n]\right\}>\frac{1}{2m}.\]

Proof.: For convenience, we assume \(x_{1},\ldots,x_{n}=v_{1},\ldots,v_{n}\) and \(y_{1},\ldots,y_{n}=v_{n+1},\ldots v_{2n}\) WLOG.

Let \(\epsilon=\sqrt{\frac{400n^{2}m^{2}\log(8m^{2}n^{2}/\delta)}{d}}\). Then \(\epsilon\leq\frac{1}{2\sqrt{n}}\). Define \(l_{i}(\Theta)=-\log p_{\Theta}(y_{i}|x_{i})\) and \(l_{i}^{\mathtt{rev}}(\Theta)=-\log p_{\Theta}(x_{i}|y_{i})\). Let \(\alpha_{i,j}^{(t)}=-p_{\Theta_{t}}(v_{j}|v_{i})+\delta_{i,j-n},\beta_{i,j}^{( t)}=-p_{\Theta_{t}}(v_{j}|v_{i})+\delta_{i-n,j}\). By Lemma5,

\[\frac{d\mathcal{L}(\Theta_{t})}{dt} =\] \[= -\left\|\frac{1}{2n-1}\left(\sum_{i=1}^{n}x_{i}(y_{i}-\mathbb{E}_ {p_{\Theta_{t}}(\cdot|x_{i})}[y])^{\top}+\sum_{i=2}^{n}y_{i}(x_{i}-\mathbb{E}_ {p_{\Theta_{t}}(\cdot|y_{i})}[x])^{\top}\right)\right\|^{2}\] \[= -\left\|\frac{1}{2n-1}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}\alpha_{i,j}^{(t)}v_{i}v_{j}^{\top}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}\beta_{i,j}^{(t)}v_{ i}v_{j}^{\top}\right)\right\|^{2}.\]Similarly, we have

\[\frac{dl_{i}(\Theta_{t})}{dt}\] (5) \[= \left\langle\nabla l_{i}(\Theta_{t}),\frac{d\Theta_{t}}{dt}\right\rangle =-\left\langle\nabla l_{i}(\Theta_{t}),\nabla\mathcal{L}(\Theta_{t})\right\rangle\] \[= -\left\langle x_{i}(y_{i}-\mathbb{E}_{p_{\Theta_{t}}(\cdot|x_{i} )}[y])^{\top},\frac{1}{2n-1}\left(\sum_{i=1}^{n}x_{i}(y_{i}-\mathbb{E}_{p_{ \Theta_{t}}(\cdot|x_{i})}[y])^{\top}+\sum_{i=2}^{n}y_{i}(x_{i}-\mathbb{E}_{p_{ \Theta_{t}}(\cdot|y_{i})}[x])^{\top}\right)\right\rangle\] \[= -\left\langle\sum_{j=1}^{m}\alpha_{i,j}v_{i}v_{j}^{\top},\frac{1} {2n-1}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}\alpha_{i,j}^{(t)}v_{i}v_{j}^{\top}+ \sum_{i=n+2}^{2n}\sum_{j=1}^{m}\beta_{i,j}^{(t)}v_{i}v_{j}^{\top}\right) \right\rangle,\]

and

\[\frac{dl_{i}^{\mathsf{rev}}(\Theta_{t})}{dt}\] (7) \[= \left\langle\nabla l_{i}^{\mathsf{rev}}(\Theta_{t}),\frac{d\Theta _{t}}{dt}\right\rangle=-\left\langle\nabla l_{i}^{\mathsf{rev}}(\Theta_{t}), \nabla\mathcal{L}(\Theta_{t})\right\rangle\] \[= -\left\langle y_{i}(x_{i}-\mathbb{E}_{p_{\Theta_{t}}(\cdot|y_{i} )}[x])^{\top},\frac{1}{2n-1}\left(\sum_{i=1}^{n}x_{i}(y_{i}-\mathbb{E}_{p_{ \Theta_{t}}(\cdot|x_{i})}[y])^{\top}+\sum_{i=2}^{n}y_{i}(x_{i}-\mathbb{E}_{p_{ \Theta_{t}}(\cdot|y_{i})}[x])^{\top}\right)\right\rangle\] \[= -\left\langle\sum_{j=1}^{m}\beta_{i+n,j}^{(t)}v_{i+n}v_{j}^{\top },\frac{1}{2n-1}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}\alpha_{i,j}^{(t)}v_{i}v_{j} ^{\top}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}\beta_{i,j}^{(t)}v_{i}v_{j}^{\top} \right)\right\rangle.\]

Applying Lemma7, with probability at least \(1-\delta/2\), for any \(t\geq 0\) we have

\[\left|\frac{d\mathcal{L}(\Theta_{t})}{dt}+\frac{1}{(2n-1)^{2}} \left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n} \sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)\right|\] (8) \[\leq \epsilon\cdot\frac{1}{(2n-1)^{2}}\left(\sum_{i=1}^{n}\sum_{j=1}^ {m}(\alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)} )^{2}\right),\]

and

\[\left|\frac{dl_{i}(\Theta_{t})}{dt}+\frac{1}{2n-1}\sum_{j=1}^{m }(\alpha_{i,j}^{(t)})^{2}\right|\] (9) \[\leq \epsilon\cdot\frac{1}{2n-1}\left(\sum_{j=1}^{m}(\alpha_{i,j}^{(t )})^{2}\right)^{1/2}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2} +\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)^{1/2},\] \[\left|\frac{d_{i}^{\mathsf{rev}}(\Theta_{t})}{dt}+\frac{1}{2n-1} \sum_{j=1}^{m}(\beta_{i+n,j}^{(t)})^{2}\right|\] \[\leq \epsilon\cdot\frac{1}{2n-1}\left(\sum_{j=1}^{m}(\beta_{i+n,j}^{ (t)})^{2}\right)^{1/2}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^ {2}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)^{1/2}.\]

Furthermore, Lemma6 implies that with probability at least \(1-\delta/2\)

\[\frac{1}{2m}<p_{\Theta_{0}}(y_{i}|x_{i}),p_{\Theta_{0}}(x_{i}|y_{i})<\frac{2}{m}.\] (10)

The following arguments are based on the event that the above inequalities hold.

We first show that

\[\inf\left\{p_{\Theta_{t}}(x_{i}|y_{i}),p_{\Theta_{t}}(y_{i}|x_{i}):t\geq 0,i\in[n] \right\}>\frac{1}{2m}.\]

We prove this by contradiction. Let

\[\tau=\inf\left\{t\geq 0:\exists i\in[n],\text{s.t.}\min\{p_{\Theta_{t}}(y_{i}|x _{i}),p_{\Theta_{t}}(x_{i}|y_{i})\}\leq\frac{1}{2m}\right\}.\]

By Eq. (7), it is obvious that \(\tau>0\). Assume without loss of generality that \(p_{\Theta_{\tau}}(y_{1}|x_{1})\leq\frac{1}{2m}\). It follows that there exists \(\delta>0\) such that \(p_{\Theta_{t}}(y_{1}|x_{1})\) is a decreasing function in \((\tau-\delta,\tau)\) and \(p_{\Theta_{t}}(y_{1}|x_{1})=\min\left\{p_{\Theta_{t}}(x_{i}|y_{i}),p_{\Theta_{ t}}(y_{i}|x_{i}):i\in[n]\right\}\) for any \(t\in(\tau-\delta,\tau)\). It follows that for \(t\in(\tau-\delta,\tau)\),

\[\frac{dl_{1}(\Theta_{t})}{dt} \leq\frac{1}{2n-1}\left(-\sum_{j=1}^{m}(\alpha_{1,j}^{(t)})^{2}+ \epsilon\cdot\left(\sum_{j=1}^{m}(\alpha_{1,j}^{(t)})^{2}\right)^{1/2}\left( \sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n}\sum_{j= 1}^{m}(\beta_{i,j}^{(t)})^{2}\right)^{1/2}\right)\] \[\leq\frac{1}{2n-1}\left(-\sum_{j=1}^{m}(\alpha_{1,j}^{(t)})^{2}+ \epsilon\cdot\left(\sum_{j=1}^{m}(\alpha_{1,j}^{(t)})^{2}\right)^{1/2}\left( 2\sum_{i=1}^{n}(\alpha_{i,i+n}^{(t)})^{2}+2\sum_{i=n+2}^{2n}(\beta_{i,i-n}^{(t )})^{2}\right)^{1/2}\right)\] \[\leq\frac{1}{2n-1}\left(\sum_{j=1}^{m}(\alpha_{1,j}^{(t)})^{2} \right)^{1/2}\cdot\left(-\left(\sum_{j=1}^{m}(\alpha_{1,j}^{(t)})^{2}\right)^ {1/2}+\epsilon\cdot\left(4n(\alpha_{1,n+1}^{(t)})^{2}\right)^{1/2}\right)\] \[\leq 0\]

where the first inequality is from Eq. (6); the second inequality is due to \(\sum_{j\neq i+n}|\alpha_{i,j}^{(t)}|=\alpha_{i,i+n}^{(t)}=1-p_{\Theta_{t}}(y_ {i}|x_{i}),\sum_{j\neq i}|\beta_{i+n,j}^{(t)}|\)= \(\beta_{i+n,i}^{(t)}=1-p_{\Theta_{t}}(x_{i}|y_{i})\) for all \(i\in[n]\); the third inequality is because \(p_{\Theta_{t}}(y_{1}|x_{1})=\min\left\{p_{\Theta_{t}}(x_{i}|y_{i}),p_{\Theta_ {t}}(y_{i}|x_{i}):i\in[n]\right\}\). However, \(p_{\Theta_{t}}(y_{1}|x_{1})\) is a decreasing function in \((\tau-\delta,\tau)\), a contradiction. Therefore, we conclude that

\[\inf\left\{p_{\Theta_{t}}(x_{i}|y_{i}),p_{\Theta_{t}}(y_{i}|x_{i}):t\geq 0,i \in[n]\right\}>\frac{1}{2m}.\]

Now we show

\[\mathcal{L}(\Theta_{t})\leq\frac{1}{\frac{t}{8(2n-1)\log^{2}(2m)}+\frac{1}{ \mathcal{L}(\Theta_{0})}},\;\forall t\geq 0.\]

By Eq. (5),

\[\frac{d\mathcal{L}(\Theta_{t})}{dt} \leq -\frac{1-\epsilon}{(2n-1)^{2}}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}( \alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)\] \[\leq -\frac{1-\epsilon}{(2n-1)^{2}}\left(\sum_{i=1}^{n}(1-p_{\Theta_{ t}}(y_{i}|x_{i}))^{2}+\sum_{i=2}^{n}(1-p_{\Theta_{t}}(x_{i}|y_{i}))^{2}\right)\] \[\leq -\frac{1-\epsilon}{(2n-1)^{3}}\left(\sum_{i=1}^{n}(1-p_{\Theta_{ t}}(y_{i}|x_{i}))+\sum_{i=2}^{n}(1-p_{\Theta_{t}}(x_{i}|y_{i}))\right)^{2}\] \[\leq -\frac{1-\epsilon}{8(2n-1)\log^{2}(2m)}\mathcal{L}(\Theta_{t})^{2}\] \[\leq -\frac{1}{8(2n-1)\log^{2}(2m)}\mathcal{L}(\Theta_{t})^{2},\]

where the second inequality is due to \(\sum_{j\neq i+n}|\alpha_{i,j}^{(t)}|=\alpha_{i,i+n}^{(t)}=1-p_{\Theta_{t}}(y_ {i}|x_{i}),\sum_{j\neq i}|\beta_{i+n,j}^{(t)}|=\beta_{i+n,i}^{(t)}=1-p_{\Theta_{ t}}(x_{i}|y_{i})\) for all \(i\in[n]\); the third inequality applies Cauchy-Schwarz inequality;the last inequality uses the fact that \(p_{\Theta_{t}}(x_{i}|y_{i}),p_{\Theta_{t}}(y_{i}|x_{i})>\frac{1}{2m}\) for any \(t\geq 0,i\in[n]\) and the inequality \(1-x\geq\frac{\log x}{2\log(1/(2m))}\) for \(x\in(\frac{1}{2m},1)\).

By Lemma10, we conclude that \(\forall t\geq 0\),

\[\mathcal{L}(\Theta_{t})\leq\frac{1}{\frac{t}{8(2n-1)\log^{2}(2m)}+\frac{1}{ \mathcal{L}(\Theta_{0})}},\]

which completes the proof. 

**Lemma 4** (Dynamics of the reversal loss).: _Let \(x_{1},\ldots,x_{n}\), \(y_{1},\ldots,y_{n}\), \(\mathcal{L}^{\mathsf{rev}}(\Theta)\), and \(\Theta_{t}\)\((t\geq 0)\) be defined as in Theorem5. When \(\sigma\leq\frac{1}{100\ln(16n^{2}/\delta)}\) and \(d\geq 400n^{2}m^{2}\log(8m^{2}n^{2}/\delta)/\epsilon^{2}\), with probability at least \(1-\delta\),_

\[\mathcal{L}^{\mathsf{rev}}(\Theta_{t})\geq\mathcal{L}^{\mathsf{rev}}(\Theta_ {0})\cdot\left(1+\frac{\mathcal{L}(\Theta_{0})\cdot t}{8(2n-1)\log^{2}(2m)} \right)^{-8\epsilon(2n-1)\log^{2}(2m)}.\]

Proof.: Similar to Lemma3, we assume \(x_{1},\ldots,x_{n}=v_{1},\ldots,v_{n}\) and \(y_{1},\ldots,y_{n}=v_{n+1},\ldots v_{2n}\) WLOG. Let \(\alpha_{i,j}^{(t)}=-p_{\Theta_{t}}(v_{j}|v_{i})+\delta_{i,j-n},\beta_{i,j}^{( t)}=-p_{\Theta_{t}}(v_{j}|v_{i})+\delta_{i-n,j}\). By Lemma5,

\[\frac{d\mathcal{L}^{\mathsf{rev}}(\Theta_{t})}{dt}\] \[= \left\langle\nabla\mathcal{L}^{\mathsf{rev}}(\Theta_{t}),\frac{d \Theta_{t}}{dt}\right\rangle\] \[= -\left\langle y_{1}(x_{1}-\mathbb{E}_{p_{\Theta_{t}}(\cdot|y_{1} )}[x])^{\top},\frac{1}{2n-1}\left(\sum_{i=1}^{n}x_{i}(y_{i}-\mathbb{E}_{p_{ \Theta_{t}}(\cdot|x_{i})}[y])^{\top}+\sum_{i=2}^{n}y_{i}(x_{i}-\mathbb{E}_{p_{ \Theta_{t}}(\cdot|y_{i})}[x])^{\top}\right)\right\rangle\] \[= -\left\langle\sum_{j=1}^{m}\beta_{n+1,j}^{(t)}v_{n+1}v_{j}^{\top },\frac{1}{2n-1}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}\alpha_{i,j}^{(t)}v_{i}v_{j} ^{\top}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}\beta_{i,j}^{(t)}v_{i}v_{j}^{\top} \right)\right\rangle.\]

Applying Lemma7, with probability at least \(1-\delta/2\), for any \(t\geq 0\) we have

\[\left|\frac{d\mathcal{L}^{\mathsf{rev}}(\Theta_{t})}{dt}\right|\leq\epsilon \cdot\frac{1}{2n-1}\left(\sum_{j=1}^{m}(\beta_{n+1,j}^{(t)})^{2}\right)^{1/2} \left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n} \sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)^{1/2},\] (8)

and

\[\left|\frac{d\mathcal{L}(\Theta_{t})}{dt}+\frac{1}{(2n-1)^{2}} \left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n} \sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)\right|\] \[\leq \epsilon\cdot\frac{1}{(2n-1)^{2}}\left(\sum_{i=1}^{n}\sum_{j=1}^ {m}(\alpha_{i,j}^{(t)})^{2}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)} )^{2}\right),\]

as well as

\[\left|\frac{d_{i}(\Theta_{t})}{dt}+\frac{1}{2n-1}\sum_{j=1}^{m}( \alpha_{i,j}^{(t)})^{2}\right|\] \[\leq \epsilon\cdot\frac{1}{2n-1}\left(\sum_{j=1}^{m}(\alpha_{i,j}^{(t )})^{2}\right)^{1/2}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2}+ \sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)^{1/2},\] \[\left|\frac{d_{i}^{\mathsf{rev}}(\Theta_{t})}{dt}+\frac{1}{2n-1} \sum_{j=1}^{m}(\beta_{i+n,j}^{(t)})^{2}\right|\] \[\leq \epsilon\cdot\frac{1}{2n-1}\left(\sum_{j=1}^{m}(\beta_{i+n,j}^{(t )})^{2}\right)^{1/2}\left(\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^{(t)})^{2} +\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}\right)^{1/2}.\]Furthermore, Lemma 6 implies that with probability at least \(1-\delta/2\),

\[\frac{1}{2m}\leq p_{\Theta_{0}}(y_{i}|x_{i}),p_{\Theta_{0}}(x_{i}|y_{i})\leq\frac {2}{m}.\] (9)

The following arguments are based on the event that the above inequalities hold.

By Eq. (8),

\[\frac{d\mathcal{L}^{\mathsf{rev}}(\Theta_{t})}{dt}\geq-\epsilon\cdot\frac{1}{ 2n-1}\left(\underbrace{\sum_{j=1}^{m}(\beta_{n+1,j}^{(t)})^{2}}_{\mathcal{A}_ {t}}\right)^{1/2}\left(\underbrace{\sum_{i=1}^{n}\sum_{j=1}^{m}(\alpha_{i,j}^ {(t)})^{2}+\sum_{i=n+2}^{2n}\sum_{j=1}^{m}(\beta_{i,j}^{(t)})^{2}}_{B_{t}} \right)^{1/2}.\]

Notice that

\[B_{t} \leq 2\sum_{i=1}^{n}(1-p_{\Theta_{t}}(y_{i}|x_{i}))^{2}+2\sum_{i= 2}^{n}(1-p_{\Theta_{t}}(x_{i}|y_{i}))^{2}\] \[\leq 2\left(\sum_{i=1}^{n}(1-p_{\Theta_{t}}(y_{i}|x_{i})+\sum_{i= 2}^{n}(1-p_{\Theta_{t}}(x_{i}|y_{i}))\right)^{2}\] \[\leq 2(2n-1)^{2}\mathcal{L}(\Theta_{t})^{2}\] \[\leq 2(2n-1)^{2}\left(\frac{\frac{t}{8(2n-1)\log^{2}(2m)}+\frac{1 }{\mathcal{L}(\Theta_{0})}}{}\right)^{2}\]

where the first inequality uses \(\sum_{j\neq i+n}\lvert\alpha_{i,j}^{(t)}\rvert=\alpha_{i,i+n}^{(t)}=1-p_{ \Theta_{t}}(y_{i}|x_{i}),\sum_{j\neq i}\lvert\beta_{i+n,j}^{(t)}\rvert=\beta_ {i+n,i}^{(t)}=1-p_{\Theta_{t}}(x_{i}|y_{i})\) for all \(i\in[n]\); the third inequality uses the fact that \(1-x\leq-\log x\); the last inequality applies Lemma 3.

Similarly,

\[A_{t}\leq 2(1-p_{\Theta_{t}}(x_{1}|y_{1}))^{2}\leq 2\mathcal{L}^{\mathsf{rev}} (\Theta_{t})^{2}.\]

Combining, we have

\[\frac{d\mathcal{L}^{\mathsf{rev}}(\Theta_{t})}{dt}\geq -\epsilon\cdot\frac{1}{2n-1}\cdot 2\mathcal{L}^{\mathsf{rev}}( \Theta_{t})\cdot(2n-1)\cdot\frac{1}{8(2n-1)\log^{2}(2m)}+\frac{1}{\mathcal{L} (\Theta_{0})}\] \[\geq -8\epsilon(2n-1)\log^{2}(2m)\cdot\mathcal{L}^{\mathsf{rev}}( \Theta_{t})\cdot\frac{1}{t+\frac{8(2n-1)\log^{2}(2m)}{\mathcal{L}(\Theta_{0} )}}.\]

By Lemma 10, we conclude that \(\forall t\geq 0\),

\[\mathcal{L}^{\mathsf{rev}}(\Theta_{t})\geq\mathcal{L}^{\mathsf{rev}}(\Theta_ {0})\cdot\left(1+\frac{\mathcal{L}(\Theta_{0})\cdot t}{8(2n-1)\log^{2}(2m)} \right)^{-8\epsilon(2n-1)\log^{2}(2m)},\]

This completes the proof. 

**Lemma 5** (Gradient of the loss function).: _Define_

\[\mathcal{L}(\Theta) =\frac{1}{2n-1}\left(\sum_{i=1}^{n}-\log p_{\Theta}(y_{i}|x_{i})+ \sum_{i=2}^{n}-\log p_{\Theta}(x_{i}|y_{i})\right),\] \[\mathcal{L}^{\mathsf{rev}}(\Theta) =\ -\log p_{\Theta}(x_{1}|y_{1}).\]

_Then we have_

\[\nabla\mathcal{L}(\Theta)= \ -\frac{1}{2n-1}\left(\sum_{i=1}^{n}x_{i}(y_{i}-\mathbb{E}_{p_{ \Theta}(\cdot|x_{i})}[y])^{\top}+\sum_{i=2}^{n}y_{i}(x_{i}-\mathbb{E}_{p_{ \Theta}(\cdot|y_{i})}[x])^{\top}\right),\] \[\nabla\mathcal{L}^{\mathsf{rev}}(\Theta)= \ -y_{1}(x_{1}-\mathbb{E}_{p_{\Theta}(\cdot|y_{1})}[x])^{\top}.\]Proof.: We have

\[\nabla(-\log p_{\Theta}(y|x))\] \[= -\frac{\nabla p_{\Theta}(y|x)}{p_{\Theta}(y|x)}\] \[= -\frac{1}{p_{\Theta}(y|x)}\cdot\frac{xy^{\top}\exp(x^{\top}\Theta y )\left(\sum_{y\in\mathcal{V}}\exp(x^{\top}\Theta y)\right)-\exp(x^{\top}\Theta y )\left(\sum_{y\in\mathcal{V}}xy^{\top}\exp(x^{\top}\Theta y)\right)}{\left( \sum_{y\in\mathcal{V}}\exp(x^{\top}\Theta y)\right)^{2}}\] \[= -\frac{1}{p_{\Theta}(y|x)}\left(p_{\Theta}(y|x)xy^{\top}-p_{ \Theta}(y|x)\sum_{y\in\mathcal{V}}p_{\Theta}(y|x)xy^{\top}\right)\] \[= -x\left(y-\sum_{y\in\mathcal{V}}p_{\Theta}(y|x)y\right)^{\top}\] \[= -x\left(y-\mathbb{E}_{y\sim p_{\Theta}(\cdot|x)}[y]\right)^{\top}.\]

The statements follow immediately. 

### Initialization

**Lemma 6** (Initial distributions are all close to uniform).: _Fix any \(\delta\in(0,1)\). Let \(x_{1},x_{2},\ldots,x_{n}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N}_{ d}(\mathbf{0}_{d},\frac{1}{d}I_{d})\). Let \(\Theta\in\mathbb{R}^{d\times d}\), where each \(\Theta_{ij}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N}(0,\sigma^{2})\) independent of \(x_{1},\ldots,x_{n}\). For any \(i,j\in[n]\), define_

\[p_{\Theta}(x_{j}|x_{i})=\frac{\exp(l_{\Theta}(x_{j}|x_{i}))}{\sum_{k=1}^{n} \exp(l_{\Theta}(x_{k}|x_{i}))},\ \text{where}\ \ l_{\Theta}(x_{j}|x_{i})=x_{i}^{\top}\Theta x_{j}.\]

_Then when \(\sigma^{2}\leq\frac{1}{100\ln(4n^{2}/\delta)}\) and \(d\geq 400\log(1/(2\delta n^{2}))/\epsilon^{2}\), with probability at least \(1-\delta\),_

\[|p_{\Theta}(x_{j}|x_{i})-1/n|\leq\frac{1}{2n},\ \forall i,j\in[n].\]

Proof.: Let \(v=0.2\). By Lemma 8, with probability at least \(1-\delta\), we have

\[|\langle x_{i},x_{j}\rangle-\delta_{ij}|\leq v,\ \forall i,j\in[n].\]

Conditioned on the above high-probability event, we can further obtain that for any \(j\in[n]\)

\[p_{\Theta}(x_{j}|x_{i})=\frac{\exp(l_{\Theta}(x_{j}|x_{i}))}{\sum_{k=1}^{n} \exp(l_{\Theta}(x_{k}|x_{i}))}\leq\frac{\exp(v)}{\sum_{k=1}^{n}\exp(-v)}=\frac {\exp(2v)}{n},\]

and

\[p_{\Theta}(x_{j}|x_{i})=\frac{\exp(l_{\Theta}(x_{j}|x_{i}))}{\sum_{k=1}^{n} \exp(l_{\Theta}(x_{k}|x_{i}))}\geq\frac{\exp(-v)}{\sum_{k=1}^{n}\exp(v)}=\frac {\exp(-2v)}{n},\]

It follows that

\[\frac{1}{2n}<p_{\Theta}(x_{j}|x_{i})<\frac{3}{2n}\implies|p_{\Theta}(x_{j}|x _{i})-1/n|<\frac{1}{2n}.\]

which completes the proof. 

### Subspace embedding

**Lemma 7** (\(\ell_{1}\)-subspace embedding of Gaussian second-order tensors).: _Let \(z_{1},\ldots,z_{n}\) be independently sampled from \(\mathcal{N}_{d}(\bm{\theta}_{d},\frac{1}{d}I_{d})\). Let \(\mathcal{I}_{1},\mathcal{I}_{2}\subseteq[n]\times[n]\) be two index sets. Let \(\mathcal{I}_{0}=\mathcal{I}_{1}\cap\mathcal{I}_{2}\). If \(d\geq 64\log(2n^{2}/\delta)/\epsilon^{2}\), then with probability at least \(1-\delta\),_

\[\left|\left\langle\sum_{(i,j)\in\mathcal{I}_{1}}\alpha_{i,j}z_{i}z_{j}^{\top},\sum_{(i,j)\in\mathcal{I}_{2}}\beta_{i,j}z_{i}z_{j}^{\top}\right\rangle-\sum_ {(i,j)\in\mathcal{I}_{0}}\alpha_{i,j}\beta_{i,j}\right|\leq\epsilon\cdot \left(\sum_{(i,j)\in\mathcal{I}_{1}}|\alpha_{i,j}|\right)\left(\sum_{(i,j)\in \mathcal{I}_{2}}|\beta_{i,j}|\right)\]_holds for any \(\alpha_{i,j},\beta_{i,j}\). Furthermore, if \(d\geq 64k^{2}\log(2n^{2}/\delta)/\epsilon^{2}\), then with probability at least \(1-\delta\),_

\[\left|\left\langle\sum_{(i,j)\in\mathcal{I}_{1}}\alpha_{i,j}z_{i}z_{j}^{\top}, \sum_{(i,j)\in\mathcal{I}_{2}}\beta_{i,j}z_{i}z_{j}^{\top}\right\rangle-\sum_{ (i,j)\in\mathcal{I}_{0}}\alpha_{i,j}\beta_{i,j}\right|\leq\epsilon\cdot\left( \sum_{(i,j)\in\mathcal{I}_{1}}\alpha_{i,j}^{2}\right)^{1/2}\left(\sum_{(i,j) \in\mathcal{I}_{2}}\beta_{i,j}^{2}\right)^{1/2}\]

_holds for any \(\alpha_{i,j},\beta_{i,j}\) such that \(\|\alpha\|_{0}\leq k\), \(\|\beta\|_{0}\leq k\)._

Proof.: Using Lemma 8 and Cauchy-Schwarz inequality, we have

\[\left|\left\langle\sum_{(i,j)\in\mathcal{I}_{1}}\alpha_{i,j}z_{i }z_{j}^{\top},\sum_{(i,j)\in\mathcal{I}_{2}}\beta_{i,j}z_{i}z_{j}^{\top} \right\rangle-\sum_{(i,j)\in\mathcal{I}_{0}}\alpha_{i,j}\beta_{i,j}\right|\] \[\leq\left|\sum_{(i,j)\in\mathcal{I}_{0}}\alpha_{i,j}\beta_{i,j}( \|z_{i}\|^{2}\|z_{j}\|^{2}-1)+\sum_{(i,j)\in\mathcal{I}_{1},(k,l)\in\mathcal{I }_{2},(i,i)\neq(k,l)}\alpha_{i,j}\beta_{k,l}z_{i}^{\top}z_{k}z_{j}^{\top}z_{l}\right|\] \[\leq\sqrt{\frac{64\log(2n^{2}/\delta)}{d}}\cdot\left(\sum_{(i,j) \in\mathcal{I}}\alpha_{i,j}\beta_{i,j}|+\sum_{(i,j)\in\mathcal{I}_{1},(k,l)\in \mathcal{I}_{2},(i,j)\neq(k,l)}|\alpha_{i,j}\beta_{k,l}|\right)\] \[=\sqrt{\frac{64\log(2n^{2}/\delta)}{d}}\cdot\left(\sum_{(i,j)\in \mathcal{I}_{1}}|\alpha_{i,j}|\right)\cdot\left(\sum_{(i,j)\in\mathcal{I}_{2}} |\beta_{i,j}|\right)\] \[\leq\sqrt{\frac{64k^{2}\log(2n^{2}/\delta)}{d}}\cdot\left(\sum_{( i,j)\in\mathcal{I}_{2}}\alpha_{i,j}^{2}\right)^{1/2}\left(\sum_{(i,j)\in\mathcal{I}_{2}} \beta_{i,j}^{2}\right)^{1/2}.\]

The statements follow directly from plugging in suitable values of \(d\). 

**Lemma 8** (Almost orthonormal).: _Let \(x_{1},x_{2},\ldots,x_{n}\)\({}^{i,j,d}\). \(\mathcal{N}_{d}(\mathbf{0}_{d},\frac{1}{d}I_{d})\). For any \(\epsilon,\delta\in(0,1)\), when \(d\geq 16\log(2n^{2}/\delta)/\epsilon^{2}\), it holds that with probability at least \(1-\delta\),_

\[|\langle x_{i},x_{j}\rangle-\delta_{ij}|\leq\epsilon,\ \forall i,j\in[n].\]

Proof.: Fix \(i\neq j\in[n]\). Notice

\[|\langle x_{i},x_{j}\rangle| =|\langle x_{i}+x_{j},x_{i}+x_{j}\rangle-\langle x_{i}-x_{j},x_{ i}-x_{j}\rangle|/4\] \[\leq(|\langle x_{i}+x_{j},x_{i}+x_{j}\rangle-1|+|\langle x_{i}-x _{j},x_{i}-x_{j}\rangle-1|)/4.\]

Using Lemma 9, we have that with probability at least \(1-\delta/n^{2}\),

\[|\langle x_{i},x_{j}\rangle|\leq\epsilon.\]

Furthermore, fix \(i\in[n]\), with probability at least \(1-\delta/n^{2}\),

\[|\langle x_{i},x_{i}\rangle-1|\leq\epsilon.\]

The statement then follows from union bound over \(i,j\in[n]\). 

**Lemma 9** (Almost normal for a fixed vector).: _For a \(d\)-dimensional random vector \(x\sim\mathcal{N}_{d}(\mathbf{0}_{d},\frac{1}{d}I_{d})\) and any \(v\in(0,1/2)\),_

\[\mathbb{P}\left(|\langle x,x\rangle-1|\geq v\right)\leq 2e^{-v^{2}d/16}.\]

_In particular, when \(d\geq 16\log(1/(2\delta))/v^{2}\), we have_

\[\mathbb{P}\left(|\langle x,x\rangle-1|\geq v\right)\leq\delta\]

Proof.: Let \(x=(x_{1},\ldots,x_{d})\). By Lemma 11 (letting \(x=v^{2}d/16\)),

\[\mathbb{P}\left(|\langle x,x\rangle-1|\geq v\right) \leq\mathbb{P}\left(\left|\sum_{i=1}^{d}(\sqrt{d}\cdot x_{i})^{2}- d\right|\geq vd\right)\] \[\leq 2e^{-v^{2}d/16}.\]

The second inequality follows from simple arithmetics.

### Useful results

**Lemma 10** (ODE bound).: _Let \(c_{1},c_{2},c_{3}>0\). Suppose the function \(f_{1},f_{2}:\mathbb{R}_{+}\to\mathbb{R}\) satisfies \(f_{1}(0)>0,f_{2}(0)>0\) and_

\[\frac{df_{1}(t)}{dt} \leq -c_{1}\cdot f_{1}(t)^{2},\] \[\frac{df_{2}(t)}{dt} \geq -c_{2}\cdot f_{2}(t)\cdot\frac{1}{t+c_{3}}.\]

_Then_

\[f_{1}(t) \leq\frac{1}{c_{1}t+\frac{1}{f_{1}(0)}},\] \[f_{2}(t) \geq f_{2}(0)\cdot\left(1+\frac{t}{c_{3}}\right)^{-c_{2}}.\]

Proof.: The conditions imply that

\[\frac{df_{1}^{-1}(t)}{dt} = -\frac{1}{f_{1}^{2}(t)}\cdot\frac{df_{1}(t)}{dt}\geq c_{1},\] \[\frac{d\log f_{2}(t)}{dt} = \frac{1}{f_{2}(t)}\cdot\frac{df_{2}(t)}{dt}\geq-c_{2}/(t+c_{3}).\]

It follows that

\[f_{1}^{-1}(t) \geq c_{1}t+f_{1}^{-1}(0),\] \[\log f_{2}(t) \geq -c_{2}\log(1+t/c_{3})+\log f_{2}(0).\]

Rearranging the above inequalities, one can obtain the desired results. 

**Lemma 11** (\(\chi^{2}\)-concentration bound, Lemma 1 of [64]).: _Let \(g_{1},\ldots,g_{t}\) be i.i.d. \(\mathcal{N}(0,1)\) random variables. Then for any \(x\geq 0\),_

\[\Pr\left[\sum_{i=1}^{t}g_{i}^{2}\geq t+2\sqrt{tx}+2x\right]\leq\exp(-x),\]

_and_

\[\Pr\left[\sum_{i=1}^{t}g_{i}^{2}\leq t-2\sqrt{tx}\right]\leq\exp(-x).\]

## Appendix C Missing Proofs of Section 4

In this section, we show missing proofs in Section 4.

### Proofs of Section 4.1

#### c.1.1 Proofs of Proposition 4.1 and Proposition 4.2

We first show the proofs of Proposition 4.1 and Proposition 4.2, respectively.

Proof of Proposition 4.1.: Actually, for any three tokens \(x_{1},x_{2},x_{3}\), it holds that

\[p_{\theta(0)}(x_{3}|x_{1},x_{2})=\frac{\exp\left(\bm{x}_{3}^{ \top}Y(0)^{\top}\text{LN}(X^{\top}\bm{b}_{2})\right)}{\sum_{x^{\prime}\in[M]} \exp\left(\bm{x^{\prime}}^{\top}Y(0)^{\top}\text{LN}(X^{\top}\bm{b}_{2}) \right)}=\frac{\exp\left(0\right)}{\sum_{x^{\prime}\in[M]}\exp\left(0\right)}= 1/M,\]

since \(Y(0)=\bm{0}\)Proof of Proposition 4.2.: Note that the input sequence length \(T=2\). By (1),

\[p_{\theta(t)}(x|x_{1},x_{2})=\frac{\exp\left(\bm{x}^{\top}Y(t)^{\top}\text{LN}(X ^{\top}\bm{b}_{2})\right)}{\sum_{x^{\prime}\in[M]}\exp\left(\bm{x}^{\prime^{ \top}}Y(t)^{\top}\text{LN}(X^{\top}\bm{b}_{2})\right)}\]

where \(\bm{b}_{2}=[b_{12}]\) and \(b_{12}=1\). Also, \(X=[\bm{x}_{1}]^{\top}\) is a one-hot row vector. Therefore, \(\text{LN}(X^{\top}\bm{b}_{2})=\text{LN}(\bm{x}_{1}b_{12})=\text{LN}(\bm{x}_{1 })=\bm{x}_{1}\), and thus

\[p_{\theta(t)}(x|x_{1},x_{2})=\frac{\exp\left(\bm{x}^{\top}Y(t)^{\top}\bm{x}_{1 }\right)}{\sum_{x^{\prime}\in[M]}\exp\left(\bm{x}^{\prime^{\top}}Y(t)^{\top} \bm{x}_{1}\right)}=\frac{\exp\left(Y(t)_{x_{1},x}\right)}{\sum_{x^{\prime}\in [M]}\exp\left(Y(t)_{x_{1},x^{\prime}}\right)}\]

where \(Y(t)_{i,j}\) is the entry of the matrix \(Y(t)\) at row \(i\) and column \(j\).

#### c.1.2 Proof of Lemma 2

Proof of Lemma 2.: We first calculate the gradient of \(Y\) when the current batch is a sequence \((x_{1},x_{2},x_{3})\). Note that the input sequence length \(T=2\) and by the proof of Proposition 4.2, we have \(\text{LN}(X^{\top}\bm{b}_{T})=\text{LN}(X^{\top}\bm{b}_{2})=\bm{x}_{1}\). By Lemma 1, we have

\[\dot{Y}=\eta_{Y}\text{LN}(X^{\top}\bm{b}_{T})(\bm{x}_{T+1}-\bm{\alpha})^{\top} =\eta_{Y}\bm{x}_{1}(\bm{x}_{3}-\bm{\alpha})^{\top}\]

where \(\bm{\alpha}=[\alpha_{1},\alpha_{2},\ldots,\alpha_{M}]^{\top}\in\mathbb{R}^{M}\) with \(\bm{\alpha}=\exp\left(Y_{x_{1}}^{\top}\right)/\bm{1}^{\top}\exp\left(Y_{x_{1} }^{\top}\right)\). Note that \(\bm{x}_{1}(\bm{x}_{3}-\bm{\alpha})^{\top}\) is a matrix with only \(x_{1}\)-th row non-zero since \(\bm{x}_{1}\) is one-hot. Therefore, the update of each row of \(Y\) are independent and only \(x_{1}\)-th row of \(Y\) gets updated at the current time step.

Now we consider any fixed \(x_{1}\in\mathcal{V}\). Let \(t_{x_{1},i}\) be the time step that the \(x_{1}\)-th row of Y gets updated (i.e., the first token of the training data is \(x_{1}\) for the current batch) for the \(i\)-th time and let \(t_{x_{1},0}=0\) for notation convenience, then

\[Y(t_{x_{1},i})_{x_{1}}=Y(t_{x_{1},i-1})_{x_{1}}+\eta_{Y}(\bm{x}_{3}-\bm{\alpha })^{\top}.\]

For convenience, we denote \(\bm{y}(i)=Y(t_{x_{1},i})_{x_{1}}^{\top}\), and thus

\[\bm{y}(i)=\bm{y}(i-1)+\eta_{Y}(\bm{x}_{3}-\bm{\alpha}(i-1))\] (10)

where \(\bm{y}(0)=\bm{0}\) and \(\bm{\alpha}(i-1)=\exp(\bm{y}(i-1))/\bm{1}^{\top}\exp(\bm{y}(i-1))\). Note that for a fixed \(x_{1}\), \(x_{3}\) is also fixed by our construction of the dataset. By Lemma 5 of [14], we can obtain that

\[\bm{y}(i)=(M-1)h^{*}(i)\bm{\xi}_{x_{3}},\]

where \(\bm{\xi}_{x_{3}}=\frac{M}{M-1}(\bm{x}_{3}-\frac{1}{M}\bm{1})\) and \(h^{*}(i)\) can be derived recursively as

\[h^{*}(i)=h^{*}(i-1)+\frac{\eta_{Y}}{(M-1)+\exp(Mh^{*}(i-1))}\]

with \(h^{*}(0)=0\). Combining Lemma 7 and 9 in [14], we have

\[h^{*}(i)\gtrsim\frac{1}{M}\ln(M\eta_{Y}i),\quad\forall i\gtrsim\frac{\ln M}{ \eta_{Y}}.\] (11)

Note that the update of each row of \(Y\) are independent, the training set has size \(N\), and the batch size is 1, we have

\[Y(t)_{x_{1}}^{\top}=(M-1)h^{*}\left(\lceil t/N\rceil\right)\bm{\xi}_{x_{3}}\]

where the training data at time step \(t\) is \((x_{1},x_{2},x_{3})\). Combining (11), we can obtain that

\[Y(t)_{x_{1},x_{3}}\gtrsim(M-1)\cdot\frac{M}{M-1}(1-\frac{1}{M})\cdot\frac{1}{M }\ln\left(M\eta_{Y}\lceil t/N\rceil\right)\geq\ln\left(\frac{M\eta_{Y}t}{N}\right)\]

and

\[Y(t)_{x_{1},x}\lesssim(M-1)\cdot\frac{M}{M-1}(-\frac{1}{M})\cdot\frac{1}{M} \ln\left(M\eta_{Y}\lceil t/N\rceil\right)\leq-\frac{1}{M}\ln\left(\frac{M\eta_{ Y}t}{N}\right),\quad\forall x\neq x_{3}.\]

On the other hand, for any sequence \((x_{1},x_{2},x_{3})\) in the test set, since the \(x_{1}\)-th row of \(Y\) has never been updated, we have

\[Y(t)_{x_{1},x}=Y(0)_{x_{1},x}=0,\quad\forall x\in[M].\]

#### c.1.3 Proof of Theorem 3

Proof.: We first consider training sequence \((x_{1},x_{2},x_{3})\) at time \(t\). By Proposition 4.2, we have

\[p_{\theta(t)}(x_{3}|x_{1},x_{2})=\frac{\exp\left(Y(t)_{x_{1},x_{3}} \right)}{\sum_{x^{\prime}\in[M]}\exp\left(Y(t)_{x_{1},x^{\prime}}\right)}\]

and by Lemma 2, we have

\[Y(t)_{x_{1},x_{3}}\geq c\ln\left(\frac{M\eta_{Y}t}{N}\right),\quad\text{and} \quad Y(t)_{x_{1},x}\leq-\frac{c}{M}\ln\left(\frac{M\eta_{Y}t}{N}\right),\quad \forall x\neq x_{3}\]

for some constant \(c>0\). Therefore,

\[p_{\theta(t)}(x_{3}|x_{1},x_{2})\geq \frac{\exp\left(c\ln\left(\frac{M\eta_{Y}t}{N}\right)\right)}{ \exp\left(c\ln\left(\frac{M\eta_{Y}t}{N}\right)\right)+(M-1)\ln\left(-\frac{c }{M}\ln\left(\frac{M\eta_{Y}t}{N}\right)\right)}\] \[\geq \frac{\left(\frac{M\eta_{Y}t}{N}\right)^{c}}{\left(\frac{M\eta_{ Y}t}{N}\right)^{c}+(M-1)}\] \[= 1-\frac{M-1}{\left(\frac{M\eta_{Y}t}{N}\right)^{c}+(M-1)}\] \[\geq 1-\frac{M-1}{2\left(\frac{M\eta_{Y}t}{N}\right)^{c}}\]

where the last inequality holds since

\[\ln t\gtrsim\ln(NM/\eta_{Y})\implies t\geq\frac{N(M-1)^{1/c}}{M \eta_{Y}}\implies\left(\frac{M\eta_{Y}t}{N}\right)^{c}\geq M-1.\]

Finally, for any sequence \((x_{1},x_{2},x_{3})\in\mathcal{D}_{\text{test}}\), since \(Y(t)_{x_{1},x}=0,\forall x\in[M]\) according to Lemma 2, we have

\[p_{\theta(t)}(x_{3}|x_{1},x_{2})=\frac{\exp\left(Y(t)_{x_{1},x_{3}} \right)}{\sum_{x^{\prime}\in[M]}\exp\left(Y(t)_{x_{1},x^{\prime}}\right)}= \frac{\exp(0)}{M\cdot\exp(0)}=1/M,\]

which completes the proof. 

### Additional details and proof of Section 4.2

Datasets.Let \(N_{\text{train}}>0\), \(N_{\text{test}}>0\) be two positive integers and let \(N_{\text{total}}=N_{\text{train}}+N_{\text{test}}\). Let \(\mathtt{A}_{i},\mathtt{B}_{i},\mathtt{C}_{i}\in\mathcal{V},\forall i\in[N_{ \text{total}}]\) be \(3N_{\text{total}}\) distinct tokens. Let \(\rightarrow,\leadsto\in\mathcal{V}=[M]\) be two additional different tokens that represent "direct implication" and "indirect implication" respectively. Specifically, we have \(\mathtt{A}_{i}\rightarrow\mathtt{B}_{i}\), \(\mathtt{B}_{i}\rightarrow\mathtt{C}_{i}\) and \(\mathtt{A}_{i}\leadsto\mathtt{C}_{i}\) for all \(i\in[N_{\text{total}}]\). For notation convenience, we define the following two index sets

\[\mathcal{I}_{\text{train}}=\{1,2,\ldots,N_{\text{train}}\},\ \ \mathcal{I}_{\text{test}}=\{N_{\text{train}}+1,\ldots,N_{\text{total}}\}.\]

The training set \(\mathcal{D}_{\text{train}}\) consists of all \(\mathtt{A}_{i}\rightarrow\mathtt{B}_{i}\), \(\mathtt{B}_{i}\rightarrow\mathtt{C}_{i}\) and \(\mathtt{A}_{i}\leadsto\mathtt{C}_{i}\) for \(i\in\mathcal{I}_{\text{train}}\). In addition, \(\mathcal{D}_{\text{train}}\) contains \(\mathtt{A}_{i}\rightarrow\mathtt{B}_{i}\) and \(\mathtt{B}_{i}\rightarrow\mathtt{C}_{i}\) for \(i\in\mathcal{I}_{\text{test}}\). For convenience, we let \(N=|\mathcal{D}_{\text{train}}|\) to be the size of the training set. The test set \(\mathcal{D}_{\text{test}}\) consists of \(\mathtt{A}_{i}\leadsto\mathtt{C}_{i}\) for \(i\in\mathcal{I}_{\text{test}}\). Under our construction of the dataset, the LLM will learn the relationship between \(\mathtt{A}_{i}\), \(\mathtt{B}_{i}\) and \(\mathtt{C}_{i}\) for \(i\in\mathcal{I}_{\text{train}}\) in both direct and indirect implication, and learn the relationship between \(\mathtt{A}_{i}\), \(\mathtt{B}_{i}\) and \(\mathtt{C}_{i}\) for \(i\in\mathcal{I}_{\text{test}}\) only in direct implication and will be tested for indirect implication.

Similar to the reversal curse in Section 4.1, we aim to prove through the training dynamics of one-layer transformers that the test probability remains negligible during training. In particular, we are interested in

\[p_{\theta}(x_{3}=\mathtt{C}_{i}|x_{1}=\mathtt{A}_{i},x_{2}=\leadsto),\quad i \in\mathcal{I}_{\text{test}}.\]

We also use \(p_{\theta}(\mathtt{B}_{i}|\mathtt{A}_{i}\rightarrow)\), \(p_{\theta}(\mathtt{C}_{i}|\mathtt{B}_{i}\rightarrow)\) and \(p_{\theta}(\mathtt{C}_{i}|\mathtt{A}_{i}\leadsto)\) to more compactly represent \(p_{\theta}(x_{3}=\mathtt{B}_{i}|x_{1}=\mathtt{A}_{i},x_{2}=\rightarrow)\), \(p_{\theta}(x_{3}=\mathtt{C}_{i}|x_{1}=\mathtt{B}_{i},x_{2}=\rightarrow)\) and \(p_{\theta}(x_{3}=\mathtt{C}_{i}|x_{1}=\mathtt{A}_{i},x_{2}=\leadsto)\), respectively.

The following theorem shows the importance of the chain-of-thought method:

**Theorem 7** (Importance of chain-of-thought, formal statement of Theorem 4).: _Assume we run SGD with batch size 1, and assume \(M\gg 100\) and \(\frac{1}{M^{0.99}}\ll\eta_{Y}<1\). Let \(t\gtrsim\frac{N_{\text{train}}M}{\eta_{Y}}\) denote the time step which also satisfies \(\ln t\gtrsim\ln(NM/\eta_{Y})\). For any test index \(i\in\mathcal{I}_{\text{test}}\), we have_

\[p_{\theta(t)}(\mathcal{B}_{i}|\text{{A}}_{i}\to)\geq 1-\frac{M-1}{2 \left(\frac{Myvt}{N}\right)^{c}},\qquad p_{\theta(t)}(\mathcal{C}_{i}|\text{{B }}_{i}\to)\geq 1-\frac{M-1}{2\left(\frac{Myvt}{N}\right)^{c}}\]

_for some constant \(c>0\) and_

\[p_{\theta(t)}(\mathcal{C}_{i}|\text{{A}}_{i}\leadsto)\leq\frac{1}{M}.\]

Proof.: Recall that by Proposition 4.2, we have

\[p_{\theta(t)}(x_{3}|x_{1},x_{2})=\frac{\exp\left(Y(t)_{x_{1},x_{3}} \right)}{\sum_{x^{\prime}\in[M]}\exp\left(Y(t)_{x_{1},x^{\prime}}\right)}\]

and by Lemma 2, we have

\[Y(t)_{\text{{A}}_{i},\text{{B}}_{i}}\geq c\ln\left(\frac{M\eta_{Y}t}{N} \right),\quad\text{and}\quad Y(t)_{\text{{A}}_{i},x}\leq-\frac{c}{M}\ln\left( \frac{M\eta_{Y}t}{N}\right),\quad\forall x\neq\text{{B}}_{i}\]

for some constant \(c>0\). Therefore, using the same proof as Theorem 3, we have

\[p_{\theta(t)}(\text{{B}}_{i}|\text{{A}}_{i}\to)\geq 1-\frac{M-1}{2 \left(\frac{Myvt}{N}\right)^{c}}.\]

Additionally, according to the proof of Lemma 2, \(Y(t)_{\text{{A}}_{i},x}\) has the same value across all \(x\neq\text{{B}}_{i}\), which implies

\[p_{\theta(t)}(\mathcal{C}_{i}|\text{{A}}_{i}\leadsto)\leq\frac{1}{M}.\]

Similarly, applying Lemma 2 to \(Y(t)_{\text{{B}}_{i}}\), we can obtain that

\[p_{\theta(t)}(\mathcal{C}_{i}|\text{{B}}_{i}\to)\geq 1-\frac{M-1}{2 \left(\frac{Myvt}{N}\right)^{c}}.\]

### Analysis of the reversal curse for the four-token sequences

In this section, we analyze the reversal curse where data points are four-token sentences "\(\text{{A}}\text{{R}}_{1}\text{{R}}_{2}\text{{B}}\)" or "\(\text{{B}}\text{{R}}_{1}\text{{R}}_{2}\text{{A}}\)". For each sentence, A and B are two distinct tokens that represent two entities, and R\({}_{1}\), R\({}_{2}\) are two special tokens jointly representing a relationship that is inverse to itself (e.g., R\({}_{1}\)R\({}_{2}\) represents "is the friend of", then \(\text{{A}}\text{{R}}_{1}\text{{R}}_{2}\text{{B}}\) means "A is the friend of B" and \(\text{{B}}\text{{R}}_{1}\text{{R}}_{2}\text{{A}}\) means "B is the friend of A" ).

Datasets.Let \(N_{\text{train}}>0\) and \(N_{\text{test}}>0\) and denote \(N_{\text{total}}=N_{\text{train}}+N_{\text{test}}\). Let \(\text{{A}}_{i},\text{{B}}_{i}\in\mathcal{V},\forall i\in[N_{\text{total}}]\) be \(K\triangleq 2N_{\text{total}}\) distinct tokens representing distinct entities. WLOG, we assume \(\text{{A}}_{i},\text{{B}}_{i}\in[K],\forall i\in[N_{\text{total}}]\). Let \(\text{{R}}_{1},\text{{R}}_{2}\in\mathcal{V}\) be two additional different tokens that jointly represent a relationship that is inverse to itself. Specifically, we have \(\text{{A}}_{i}\text{{R}}_{1}\text{{R}}_{2}\text{{B}}_{i}\) and \(\text{{B}}_{i}\text{{R}}_{1}\text{{R}}_{2}\text{{A}}_{i}\) for all \(i\in[N_{\text{total}}]\). For notation convenience, we define the following two index sets

\[\mathcal{I}_{\text{train}}=[N_{\text{train}}],\qquad\mathcal{I}_{\text{test}} =[N_{\text{total}}]\backslash\mathcal{I}_{\text{train}}.\]

The training set \(\mathcal{D}_{\text{train}}\) consists of all \(\text{{A}}_{i}\text{{R}}_{1}\text{{R}}_{2}\text{{B}}_{i}\) and \(\text{{B}}_{i}\text{{R}}_{1}\text{{R}}_{2}\text{{A}}_{i}\) for \(i\in\mathcal{I}_{\text{train}}\). In addition, \(\mathcal{D}_{\text{train}}\) contains \(\text{{A}}_{i}\text{{R}}_{1}\text{{R}}_{2}\text{{B}}_{i}\) for \(i\in\mathcal{I}_{\text{test}}\). For convenience, we let \(N=|\mathcal{D}_{\text{train}}|\) to be the size of the training set. The test set \(\mathcal{D}_{\text{test}}\) consists of \(\text{{B}}_{i}\text{{R}}_{1}\text{{R}}_{2}\text{{A}}_{i}\) for \(i\in\mathcal{I}_{\text{test}}\). Under our construction of the dataset, the LLM will learn the relationship between \(\text{{A}}_{i}\) and \(\text{{B}}_{i}\) for \(i\in\mathcal{I}_{\text{train}}\) in both directions to deduce that the relationship "\(\text{{R}}_{1}\text{{R}}_{2}\)" is reverse to itself, and learn the relationship between \(\text{{A}}_{i}\) and \(\text{{B}}_{i}\) for \(i\in\mathcal{I}_{\text{test}}\) in one direction and will be tested for the other. WLOG, we assume for each training sequence \((x_{1},\text{{R}}_{1},\text{{R}}_{2},x_{4})\in\mathcal{D}_{\text{train}}\), \(x_{4}\in[N]\).

Now we assume that the learning rate \(\eta_{Y}\gg\eta_{Z}\) and therefore can treat \(X^{\top}\mathfrak{b}_{T}\) as fixed when analyzing the dynamics of \(Y\). For any sequence \((x_{1},x_{2}=\mathtt{R}_{1},x_{3}=\mathtt{R}_{2},x_{4}=n)\) in training or test dataset with \(T=3\), we define \(\boldsymbol{f}_{n}=\text{LN}(X^{\top}\mathfrak{b}_{T})\). Then the gradient of \(Y\) in (3) becomes

\[\dot{Y}=\eta_{Y}\boldsymbol{f}_{n}(\boldsymbol{x}_{T+1}-\boldsymbol{\alpha})^ {\top}=\eta_{Y}\boldsymbol{f}_{n}(\boldsymbol{e}_{n}-\boldsymbol{\alpha})^{ \top}.\]

Note that for three-token sequences, \(\boldsymbol{f}_{n}\) is a one-hot vector, and thus \(\dot{Y}\) has only one non-zero row, and each row of \(Y\) can be analyzed independently. For the four-token sequences, we use the same reparameterization strategy as in [14], where we denote \(W=[\boldsymbol{w}_{1},\boldsymbol{w}_{2},\ldots,\boldsymbol{w}_{K}]^{\top} \triangleq F^{\top}Y\in\mathbb{R}^{K\times M}\) with \(F=[\boldsymbol{f}_{1},\ldots,\boldsymbol{f}_{K}]\in\mathbb{R}^{M\times K}\).

Note that the parameter \(W\) can be viewed as a combination of \(Y\) and \(Z\) where \(Z\) is fixed. The following lemma shows the dynamics of \(W\), assuming we are performing gradient updates on \(W\) instead of \(Y\).

**Lemma 12** (Dynamics of \(W\)).: _Assume we perform gradient updates directly on \(W\) instead of \(Y\) with learning rate \(\eta_{Y}\) and batch size 1, and assume \(M\gg 100\) and \(\frac{1}{M^{0.99}}\ll\eta_{Y}<1\). Let \(t\gtrsim\frac{N\ln M}{\eta_{Y}}\) and let \(W(t)_{i}\) denote the \(i\)-th row of \(W(t)\) and \(W(t)_{ij}\) denote the \((i,j)\)-th entry of \(W(t)\). Then for training sequence \((x_{1},\mathtt{R}_{1},\mathtt{R}_{2},x_{4})\in\mathcal{D}_{\text{train}}\) at time \(t\), we have_

\[W(t)_{x_{4},x_{4}}\gtrsim\ln\left(M\eta_{Y}t/N\right),\quad\text{and}\quad W(t )_{x_{4},x}\lesssim-\ln\left(M\eta_{Y}t/N\right)/M,\quad\forall x\neq x_{4},\]

_and for any test sequence \((x_{1},\mathtt{R}_{1},\mathtt{R}_{2},x_{4})\in\mathcal{D}_{\text{test}}\), we have \(W(t)_{x_{4},x}=0,\forall x\in[M]\)._

Proof.: According to Lemma 3 of [14], for training sequence \((x_{1},\mathtt{R}_{1},\mathtt{R}_{2},x_{4})\in\mathcal{D}_{\text{train}}\) at time \(t\), only the \(x_{4}\)-th row of \(W\) will be updated and the gradient

\[\dot{\boldsymbol{w}}_{x_{4}}(t)=\eta_{Y}(\boldsymbol{x}_{4}-\boldsymbol{ \alpha}_{x_{4}}(t))\]

where \(\boldsymbol{\alpha}(t)=\exp(\boldsymbol{w}_{x_{4}}(t))/(\boldsymbol{1}^{\top} \exp(\boldsymbol{w}_{x_{4}}(t)))\). Therefore, the dynamics of \(W\) is nearly identical to the dynamics of \(Y\) in Lemma 2, and we can use the proof of Lemma 2 to conclude the results. 

With the dynamics of \(W\), we can obtain the following result:

**Proposition C.1** (Reversal curse for the four-token sequences).: _Assume we perform gradient updates directly on \(W\) instead of \(Y\) with learning rate \(\eta_{Y}\) and batch size 1, and assume \(M\gg 100\) and \(\frac{1}{M^{0.99}}\ll\eta_{Y}<1\). Let \(t\gtrsim\frac{N\ln M}{\eta_{Y}}\) denote the time step which also satisfies \(\ln t\gtrsim\ln(NM/\eta_{Y})\). For training sequence \((x_{1},\mathtt{R}_{1},\mathtt{R}_{2},x_{4})\in\mathcal{D}_{\text{train}}\) at time \(t\), we have_

\[p_{\theta(t)}(x_{4}|x_{1},\mathtt{R}_{1},\mathtt{R}_{2})\geq 1-\frac{M-1}{(M \eta_{Y}t/N)^{c}}\to 1,\quad\text{as}\;\;t\to\infty\]

_for some constant \(c>0\), and for any test sequence \((x_{1},\mathtt{R}_{1},\mathtt{R}_{2},x_{4})\in\mathcal{D}_{\text{test}}\) that is not included in the training set \(\mathcal{D}_{\text{train}}\), we have_

\[p_{\theta(t)}(x_{4}|x_{1},\mathtt{R}_{1},\mathtt{R}_{2})\leq 1/M.\]

Proof.: For any sequence \((x_{1},x_{2}=\mathtt{R}_{1},x_{3}=\mathtt{R}_{2},x_{4})\) where \(T=3\),

\[p_{\theta(t)}(x|x_{1},\mathtt{R}_{1},\mathtt{R}_{2})= \frac{\exp\left(\boldsymbol{x}^{\top}Y(t)^{\top}\text{LN}(X^{\top} \boldsymbol{b}_{T})\right)}{\sum_{x^{\prime}\in[M]}\exp\left(\boldsymbol{x}^{ \prime\top}Y(t)^{\top}\text{LN}(X^{\top}\boldsymbol{b}_{T})\right)}\] \[= \frac{\exp\left(\boldsymbol{x}^{\top}Y(t)^{\top}\boldsymbol{f}_{x_ {4}}\right)}{\sum_{x^{\prime}\in[M]}\exp\left(\boldsymbol{x}^{\prime\top}Y(t)^{ \top}\boldsymbol{f}_{x_{4}}\right)}\] \[= \frac{\exp\left(\boldsymbol{x}^{\top}\boldsymbol{w}_{x_{4}}(t) \right)}{\sum_{x^{\prime}\in[M]}\exp\left(\boldsymbol{x}^{\prime\top} \boldsymbol{w}_{x_{4}}(t)\right)}\] \[= \frac{\exp\left(W(t)_{x_{4},x}\right)}{\sum_{x^{\prime}\in[M]}\exp \left(W(t)_{x_{4},x^{\prime}}\right)}.\]

The above next token probability formulation is almost identical to Proposition 4.2 after replacing \(Y\) with \(W\). Combining the dynamics of \(W\) as shown in Lemma 12, we can use the proof of Theorem 3 to conclude the result.

Finally, we discuss the role of \(Z\) in the four-token-sequence settings. Note that Proposition C.1 assumes gradient update on \(W\) instead of \(Y\). While we are not able to perform gradient updates on \(W\) directly, it is equivalent to modifying the gradient of \(Y\) to be

\[\dot{Y}=\eta_{Y}(\bm{f}_{n}-FE^{\prime}\bm{e}_{n})(\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\]

according to Lemma 3 of [14], where the next token \(x_{T+1}=n\), \(E^{\prime}=(I+E)^{-1}-I\), \(E=F^{\top}F-I\), and \(F=[\bm{f}_{1},\dots,\bm{f}_{N}]\in\mathbb{R}^{M\times N}\) which only contains the next token that appears in the training set. Compared to the original gradient of \(Y\)

\[\dot{Y}=\eta_{Y}\bm{f}_{n}(\bm{e}_{n}-\bm{\alpha}_{n})^{\top},\]

one can obtain that the modification of the gradient of \(Y\) is small if \(\lambda_{1}(E)\) is small.

Note that \(E_{ii}=\|\bm{f}_{i}\|_{2}^{2}-1=0\) and \(E_{ij}=\bm{f}_{i}^{\top}\bm{f}_{j}\). Also, for any sequence \((x_{1}=n^{\prime},x_{2}=\texttt{R}_{1},x_{3}=\texttt{R}_{2},x_{4}=n)\) in training set,

\[\bm{f}_{n}=\text{LN}(X^{\top}\bm{b}_{T})=\frac{b_{13}\bm{e}_{n^{\prime}}+b_{2 3}\bm{e}_{\texttt{R}_{1}}}{\sqrt{b_{13}^{2}+b_{23}^{2}}},\]

where

\[b_{13}=\exp(Z_{\texttt{R}_{2},n^{\prime}}),b_{13}=\exp(Z_{\texttt{R}_{2}, \texttt{R}_{1}}).\]

Note that \(\texttt{R}_{1}\) is a common token that appears in each training sentence, and \(n^{\prime}\) is a distinct token that only appears in one training sentence as a contextual token. By Theorem 2 of [14], under certain technical assumptions, \(\hat{Z}_{\texttt{R}_{2},n^{\prime}}>0\) and \(\hat{Z}_{\texttt{R}_{1},n}<0\) and one can expect \(Z_{\texttt{R}_{2},n^{\prime}}-Z_{\texttt{R}_{1},n}\) to be sufficiently large after sufficient time of training. Therefore,

\[\bm{f}_{n}=\tilde{b}_{13}\bm{e}_{n^{\prime}}+\tilde{b}_{23}\bm{e}_{\texttt{R }_{1}}\]

with \(\tilde{b}_{23}\) close to 0. Consider a simple case where for each \(n\in[N]\), \(\bm{f}_{n}=\sqrt{1-c^{2}}\bm{e}_{n^{\prime}}+c\bm{e}_{\texttt{R}_{1}}\) for \(c\) sufficiently small. Then

\[E_{ij}=\bm{f}_{i}^{\top}\bm{f}_{j}=(\sqrt{1-c^{2}}\bm{e}_{i^{\prime}}+c\bm{e}_ {\texttt{R}_{1}})^{\top}(\sqrt{1-c^{2}}\bm{e}_{j^{\prime}}+c\bm{e}_{\texttt{ R}_{1}})=c^{2}.\]

Therefore, one can calculate that \(\lambda_{1}(E)=c^{2}(N-1)\). When \(c\ll\frac{1}{\sqrt{N}}\), we have \(\lambda_{1}(E)\ll 1\), and thus the gradient update of \(W\) and gradient update of \(Y\) are almost the same.

## Appendix D Experiments for chain-of-thought

In this section, we conduct experiments for COT on multi-layer transformers to validate theoretical results in Section 4.2.

Dataset construction.Similar to Section 5, we randomly sample three disjoint sets of entities \(\mathcal{A},\mathcal{B},\mathcal{C}\subset\mathcal{V}\), and reverse two additional tokens for \(\rightarrow\) and \(\sim\), respectively. Next, we specify a bijection from \(\mathcal{A}\) to \(\mathcal{B}\), and a bijection from \(\mathcal{B}\) to \(\mathcal{C}\) randomly. For each \(\texttt{A}_{i}\in\mathcal{A}\) and its corresponding \(\texttt{B}_{i}\in\mathcal{B}\) and \(\texttt{C}_{i}\in\mathcal{C}\), we can obtain a triple of sequences \((\texttt{A}_{i}\rightarrow\texttt{B}_{i},\texttt{B}_{i}\rightarrow\texttt{C}_ {i},\texttt{A}_{i}\leadsto\texttt{C}_{i})\), and split the set of all triples into training triples and validation triples. All three sequences of a training triple will be added to the training set, while for a validation triple, we add \(\texttt{A}_{i}\rightarrow\texttt{B}_{i}\) and \(\texttt{B}_{i}\rightarrow\texttt{C}_{i}\) to the training set and add \(\texttt{A}_{i}\leadsto\texttt{C}_{i}\) to the validation set. Therefore, the model will learn both direct and indirect implications for the training triples and only learn the direct implications for each validation triple while being tested on the indirect implication.

Results.Figure 3 shows the experiment results for COT using the same model architecture and configurations as in Figure 1 (the training set size is 540, and the validation set size is 60 resulting from 140 training triples and 60 validation triples), which is consistent with Theorem 7. One can refer to Appendix E.3 for additional experiments with various model configurations and vocabulary sizes. We also empirically validate the intransitivity of model weights (i.e., logits) for multi-layer transformers in Figure 4, which shows that for a validation triple \((\texttt{A}_{i},\texttt{B}_{i},\texttt{C}_{i})\) of which only the direct implication "\(\texttt{A}_{i}\rightarrow\texttt{B}_{i}\)" and "\(\texttt{B}_{i}\rightarrow\texttt{C}_{i}\)" appears in the training set, although the weights from \(\texttt{A}_{i}\) to \(\texttt{B}_{i}\) and from \(\texttt{B}_{i}\) to \(\texttt{C}_{i}\) are trained large as indicated by the diagonals of the first two bottom matrices, the weights from \(\texttt{A}_{i}\) to \(\texttt{C}_{i}\) gets hardly trained as indicated by the diagonals of the last matrix. We also emphasize that another reason that COT is necessary is that all tokens \(\texttt{A}_{i}\), \(\texttt{B}_{i}\), and \(\texttt{C}_{i}\) are different tokens with randomly initialized embedding and thus irrelevant. When these tokens are relevant and show specific patterns, the validation loss can also get better. See more details in Appendix E.4.

## Appendix E Additional Experimental Results

In this section, we show additional experimental results for Section5.

### Details of model architectures and hyperparameters

For both the reversal curse and COT experiments, we used the GPT2 model architecture [63]6 and trained the model with the AdamW optimizer for 3000 epochs of batch size 64. See Table2 for a full list of hyperparameters. We also conducted experiments under various model configurations and vocabulary sizes to show that the results in Section5 and appendixD are consistent under different settings. See Table3 for a complete list of different configurations, where the default choices are boldened. For each curve in all figures, the results are averaged over 10 trials, and the error bar is calculated using standard deviation. We run each trial on an Nvidia A100 GPU and it typically takes 0.5-1.5 hours for each trial.

Footnote 6: Apache License 2.0

### Additional experimental results for the reversal curse

In this section, we show additional experimental results for the reversal curse under different configurations, including different vocabulary sizes (Figure5), different number of layers (Figure6),

\begin{table}
\begin{tabular}{|l|l|} \hline
**Parameters** & **Values** \\ \hline Learning Rate & 0.01 \\ Weight Decay \(\lambda\) & 0.9 \\ \((\beta_{1},\beta_{2})\) & (0.9, 0.999) \\ Batch Size & 64 \\ Number of Epochs & 3000 \\ \hline \end{tabular}
\end{table}
Table 2: Full list of hyperparameters for AdamW optimizer and training.

Figure 3: Experiment results of COT under default configuration (see Table3). The curves represent the (average) negative log probability of the model predicting the next token to be: (1) B\({}_{i}\) given the input “\(\bm{A}_{i}\rightarrow\)”, (2) C\({}_{i}\) given the input “\(\bm{B}_{i}\rightarrow\)”, or (3) C\({}_{i}\) given the input “\(\bm{A}_{i}\rightarrow\)”. Similar to the reversal curse experiment, while the sentences in the training set can be learned nearly perfectly, the model is not able to predict the correct next token in the validation set better than a uniformly random guess. Both curves are averaged over 10 random seeds.

different positional encoding (Figure 7), different entity lengths (Figure 8) and whether token and positional embeddings are trainable or fixed (Figure 9). Our experimental results consistently show that the reversal curse happens under different settings.

We provide additional experimental results that (1) the reversal curse still happens even if the embedding dimension is much smaller than the vocabulary size in Appendix E.2.1; (2) the embeddings of different tokens are nearly orthogonal; (3) the reversal curse does not happen under the in-context learning settings.

#### e.2.1 The reversal curse under small embedding dimensions

Although for theoretical analysis in Section 3, we assumed the embedding dimension is polynomial in the vocabulary size, in practice, the embedding dimension only needs to be the order of logarithm of the vocabulary size. Figure 10 shows that for a much smaller embedding size, the reversal curse still happens.

Figure 4: Visualization of the weights (logits) of the model with default configurations trained after 3000 epochs for COT experiment. The matrices are similar to Figure 2. The row tokens for the top matrices are \(\mathtt{A}_{i}\), \(\mathtt{B}_{i}\), \(\mathtt{A}_{i}\) and column tokens are \(\mathtt{B}_{i}\), \(\mathtt{C}_{i}\), \(\mathtt{C}_{i}\) for training triples respectively. Similarly, the bottom matrices correspond to validation triples. For validation triples \((\mathtt{A}_{i},\mathtt{B}_{i},\mathtt{C}_{i})\), the weights from \(\mathtt{A}_{i}\) to \(\mathtt{C}_{i}\) get hardly trained as indicated by the diagonals of the last matrix.

\begin{table}
\begin{tabular}{|l|l|} \hline
**Parameters** & **Values** \\ \hline Number of Layers & 12, **24**, 48 \\ Number of Heads & **12** \\ Vocabulary Size & 20, 50, 200, **800**, 2000 \\ Entity Length & **1**, 2, 3 \\ Positional Encoding Type & None, **Absolute**, Relative \\ Token, Positional Embedding & **Learnable**, Frozen \\ \hline \end{tabular}
\end{table}
Table 3: The list of different configurations for experiments in Appendices E.2 and E.3. Default choices are boldened for each row.

Figure 5: Results for reversal curse for different vocabulary sizes. All other configurations are set as default values as in Table 3. The training set sizes for the above four experiments are \(9\), \(20\), \(85\), \(850\) respectively, and the validation set sizes are \(1\), \(4\), \(15\), \(150\) respectively.

Figure 6: Results for reversal curse for different numbers of layers of the transformer. All other configurations are set as default values as in Table 3.

Figure 7: Results for reversal curse with no positional encoding or relative positional encoding. For relative positional encoding, we follow the Rotary Position Embedding (RoPE) method proposed by [65]. We use the implementation of this repo, MIT license. All other configurations are set as default values as in Table 3.

#### e.2.2 Near orthogonal embeddings

Note that in Section 3, our analysis relies on the fact that embeddings of different tokens are nearly orthogonal, and in Section 4, the embeddings are effectively one-hot. In Figure 11, We show that in practice, even if the embedding dimension is much smaller than the vocabulary size, the near orthogonality condition still holds.

#### e.2.3 The reversal curse does not happen in ICL settings

We also emphasize that the reversal curse does not happen in ICL settings, which means if "A \(\rightarrow\) B" is provided as part of the prompt, then the model is able to answer "B \(\leftarrow\) A". Figure 12 shows preliminary results of ICL. All the sentences in the dataset have the format of "A\({}_{i}\)RB\({}_{j}\)\(\Leftrightarrow\) B\({}_{j}\)R\({}^{-1}\)A\({}_{i}\)", which is a seven-token sentence where R and R\({}^{-1}\) is a pair of relationships inverse to each other, and "\(\Leftrightarrow\)" is another reserved token representing equivalence. There are ten different B\({}_{j}\) and \(n\) different A\({}_{i}\) where \(n=100\) for the left figure in Figure 12 and \(n=200\) for the right figure. For each \(A_{i}\), we construct ten sentences using different \(B_{j}\), and we randomly chose three of them to be included in the validation set and seven other sentences in the training set. The result of Figure 12 shows that the reversal curse does not happen during the ICL setting.

Figure 8: Results for reversal curse with different entity lengths. Each entity A\({}_{i}\) or B\({}_{i}\) consists of multiple tokens and different entities may have overlapped tokens. The “Train” curve represents the negative log probability of predicting the first token of the output entity, and “Train (word)” represents the negative log probability of predicting all tokens one by one of the output entity. All other configurations are set as default values as in Table 3. The training set sizes for the above two experiments are \(680\) and \(250\), respectively, and the validation set sizes are \(120\) and \(50\), respectively.

Figure 9: Results for reversal curse with fixed token embedding and fixed positional embedding. All other configurations are set as default values as in Table 3.

[MISSING_PAGE_EMPTY:32]

Figure 12: Training and validation loss under in-context learning (ICL) settings. All sentences consist of seven tokens and have the form of “A\({}_{i}\)RB\({}_{j}\)\(\Leftrightarrow\) B\({}_{j}\)R\({}^{-1}\)A\({}_{i}\)”. The loss is calculated on the last token. For the left figure, the training set size is 700, the validation set size is 300, and the vocabulary size is 400; for the right figure, the training set size is 1400, the validation set size is 600, and the vocabulary size is 800. All other configurations are set as default values as in Table 3. The result shows that the reversal curse does not happen in ICL settings, i.e., if “A\({}_{i}\)RB\({}_{j}\)” is provided as part of the prompt, then the model is able to recognize “B\({}_{j}\)R\({}^{-1}\)A\({}_{i}\)”.

Figure 13: Results for COT for different vocabulary sizes. All other configurations are set as default values as in Table 3. The training set sizes for the above four experiments are \(14\), \(32\), \(135\), \(1350\) respectively, and the validation set sizes are \(1\), \(4\), \(15\), \(150\) respectively.

Figure 16: Results for COT with different entity lengths. The setting and curves are similar to Figure 8. All other configurations are set as default values as in Table 3. The training set sizes for the above two experiments are \(1080\) and \(400\), respectively, and the validation set sizes are \(120\) and \(50\).

Figure 14: Results for COT for different number of layers of the transformer. All other configurations are set as default values as in Table 3.

Figure 15: Results for COT with no positional encoding or relative positional encoding. For relative positional encoding, we follow the Rotary Position Embedding (RoPE) method proposed by [65]. All other configurations are set as default values as in Table 3.

Figure 17: Results for COT with fixed token embedding and fixed positional embedding. All other configurations are set as default values as in Table 3.

Figure 18: Results for COT where each entity is represented by two tokens, i.e., Ai, Bi, or Ci. The validation set sizes are 50. The model is able to “deduce” unseen Ai\(\leadsto\)Ci by learning underlying patterns.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See abstract and Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Sections 1 and 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Sections3 and4 and appendicesB and C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section5 and appendicesD and E. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Our code is available at https://github.com/marlo-z/reversal_curse_analysis/. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 5 and appendices D and E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Appendix E.1. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix E.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section 1 and Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Appendix E. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our code is available at https://github.com/marlo-z/reversal_curse_analysis/. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.