# Parts of Speech-Grounded Subspaces in

Vision-Language Models

 James Oldfield\({}^{1}\) Christos Tzelepis\({}^{1}\) Yannis Panagakis\({}^{2,3}\)

Mihalis A. Nicolaou\({}^{4}\) Ioannis Patras\({}^{1}\)

\({}^{1}\)Queen Mary University of London \({}^{2}\)National and Kapodistrian University of Athens

\({}^{3}\)Archimedes/Athena RC \({}^{4}\)The Cyprus Institute

Corresponding author: j.a.oldfield@qmul.ac.uk

###### Abstract

Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased towards specific visual properties (such as _objects_ or _actions_) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP's joint vision-language space by leveraging the association between _parts of speech_ and specific visual modes of variation (e.g. nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What's more, we show the proposed model additionally facilitates learning subspaces corresponding to _specific_ visual appearances (e.g. artists' painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists' styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification.

## 1 Introduction

Many recent advances in machine learning have been driven by vision-language (VL) models' ability to learn powerful, generalisable image representations from natural language supervision [1, 2, 3]. The image features from VL models well-capture representations of many visual attributes as evidenced by the broad applicability they have found for use in downstream tasks. The image or text encoders of CLIP in particular [1] have been used for controllable image synthesis [4, 5, 6], image captioning [7, 8], and multiple other discriminative tasks [9, 10, 11]. However, modelling the many different visual modalities in a single vector representation is not without its drawbacks-recent work shows that CLIP's visual representations are often _entangled_. For example, Goh et al. [12] find that specific neurons fire in response to both images containing a visual concept and _images of text_ relating to the same concept. This leaves CLIP open to vulnerabilities in the form of 'Typographic attacks'-writing another class name as text on the image can often cause CLIP to predict this class with a higher probability than that of the original image's true category. Other recent works show that CLIP's visual representations encode task-specific attributes (such as of the object or action depicted in an image) inan unpredictable manner, and often the embedding is biased in the prominence with which it encodes different modalities [13]. We show in Figure 0(a) a motivating example of this problem identified by Menon et al. [13]-the 'goldfish' noun embedding dominates the image's representation despite there being multiple additional labels which accurately describe important information about the image's contents. As a visual example, we find that CLIP encodings of text prompts containing 'visually polysemous' [14] phrases of artists' names lead CLIP-based text-to-image models [15] to synthesize an unpredictable combination of _both_ images of the artists and of artworks in their signature styles (as shown in Figure 0(b)). Multiple visual associations of the text prompt, including both the appearance of the artist themselves and the style of their artwork, are entangled in the same CLIP embedding. For VL representations to make for useful image features, it's vital that the particular modalities of interest are indeed well-represented in the embedding. One popular means to this end is fine-tuning the representations for specific downstream tasks [9]. However, this not only requires additional computation but makes the restrictive assumption of the existence of labelled data for each task.

In this paper, we address the problem of better disentangling the modes of visual variation in CLIP's shared vision-language space. In particular, we ask the question: do there exist subspaces in CLIP's joint VL space that capture the representations of the 'content' of an image or text, that are invariant to its 'appearance'? To take the first steps towards achieving this we leverage the association between _parts of speech_ in natural language and specific modes of visual variation: our learnt noun subspace isolates representations of the 'object' of an image or text prompt (e.g. an animal in an image, or the noun described in a sentence), and the adjective space its appearance (e.g. whether an object is shiny, or a scene is snowy). This image-text semantic similarity is installed in the representations through CLIP's contrastive learning training objective, through which it is encouraged to learn image encodings that have large cosine similarity in the shared vision-language space with text encodings of captions describing its visual content. We show how by using example words in the parts of speech categories in the WordNet [16] database, one can extract representations of _both_ image and text embeddings that better isolate the individual modes of visual variation. This is in contrast to the method of VL model 'prompting' [17; 18; 13], which often steers the embeddings of either only the text or image modality. We take inspiration from the related line of work that finds so-called 'interpretable directions' in latent space [19; 20; 21; 22; 23] that capture high-level semantic attributes-however, we learn subspaces that capture variation _uniquely_ present amongst representations of words of particular parts of speech. We achieve this by formulating an appropriate objective function which we show can be manipulated into a well-known trace maximisation problem with a fast solution given in closed form. Since the CLIP representations live on the hypersphere [24], we further propose a manifold generalisation of the subspaces [25; 26] (illustrated in Figure 2) that share the property of capturing the variance of only the desired visual attributes, yet better respect the manifold on which the data lie. Concretely, we compute the proposed component analysis in the tangent space to the sphere's intrinsic mean, which can be seen as a local approximation of the manifold [27].

Figure 1: CLIP represents multiple visual modes of variation in an embedding (e.g. the ‘object’ and its ‘appearance’). The learnt PoS subspaces more reliably separate the constituent visual components.

The method's ability to disentangle visual modes of variation is measured both qualitatively and quantitatively. Using a popular CLIP-based text-to-image model we demonstrate _visually_ how the learnt PoS subspaces can better separate the content from the style associated with a text prompt. We show, for example, how simply projecting onto the orthogonal complements of the noun and adjective subspaces respectively can more reliably produce images of either the artists' work, or of the artists themselves, as shown in rows 2 and 3 of Figure 0(b). We find removing a CLIP representations' adjective space component to be remarkably effective for preventing the visual imitation of artists' styles in this new class of CLIP-based text-to-image models [15], addressing societal concerns of the technology. Further, we show our objective additionally facilitates learning subspaces corresponding to more specific visual appearances (e.g. 'gory'). Projections onto the orthogonal complements of such subspaces consequently remove entire visual themes from text-to-image models, whilst preserving existing concepts through the PoS guidance.

Finally, we validate the subspaces' ability to isolate visual modes of variation quantitatively through a measure of class invariance (comparing to two existing baselines), and by showing how the baseline zero-shot classification protocol _in the learnt subspaces_ leads to higher accuracies on the ViT-B-32 CLIP model on \(14/15\) datasets considered. Our contributions can be summarised as follows:

* We present a method for learning geometry-aware subspaces in CLIP's shared vision-language space that disentangle representations of the content in an image or text prompt from the way it looks, using parts of speech as supervision. To the best of our knowledge, we are the first to use the semantic relationship between parts of speech and specific visual modes of variation to disentangle CLIP's shared vision-language space.
* We formulate and solve an appropriate trace maximisation problem that admits a fast closed-form solution respecting the manifold on which the data lie.
* We validate our method's success at disentangling the visual modes of variation both quantitatively and qualitatively: by visually separating a text prompt's 'content' from its 'appearance' with text-to-image models for the former and through improved zero-shot classification in the submanifolds for the latter.
* We show the model's ability to further learn subspaces of more specific appearance-based variation (e.g. artists' styles), providing a way of erasing entire visual themes from CLIP-based text-to-image models.

## 2 Method

We first recall the basics of CLIP. We then motivate and introduce our objective function for learning parts of speech-specific linear subspaces in Section 2.1, and detail its closed-form solution in Section 2.1.1. Finally, in Section 2.2 we show how to learn _subspaces of the tangent space_ to the CLIP VL hypersphere's intrinsic mean, which better respects the geometry of the manifold on which the CLIP representations lie.

CLIP preliminariesPre-trained image and text encoders map images and text respectively to latent representations \(\mathbf{z}_{I},\mathbf{z}_{T}\in\mathbb{R}^{d}\) in a shared 'vision-language' (VL) space [1]. For test-time zero-shot classification, candidate text prompts are first encoded with the text encoder. Then, the cosine similarity between an encoded image representation of interest and the text candidates is computed with \(S(\mathbf{z}_{T},\mathbf{z}_{I})=\mathbf{z_{{}_{T}}}^{\top}\mathbf{z}_{I}\ /\ (\| \mathbf{z}_{T}\|\cdot\|\mathbf{z}_{I}\|)\), after which the softmax function is applied to determine the most likely text label for the image. We now address the task of extracting representations of solely the target modes of variation in the section that follows.

### Objective

We seek a lower-dimensional subspace on which to project either a text or image CLIP representation \(\mathbf{z}\in\mathbb{R}^{d}\) to predictably isolate the desired visual mode of variation. We achieve this through natural language supervision in the form of words from the different parts of speech. Let the elements of a set \(\mathcal{C}\) index into the relevant word class of interest (i.e. \(\mathcal{C}=\{N,A,V,R\}\) for nouns, adjectives, verbs, and adverbs respectively), and \(\mathbf{X}_{i}\in\mathbb{R}^{d\times n},\forall i\in\mathcal{C}\) contain in their columns the CLIP encodings of \(n\) words belonging to word class \(i\)[16]. Then, for a word class \(i\in\mathcal{C}\) of interest, we seek a \(k\)-dimensional subspace of \(\mathbb{R}^{d}\) spanned by the columns of a learnt \(\mathbf{W}_{i}\in\mathbb{R}^{d\times k}\) in which the CLIPrepresentations of the words in the class of interest \(i\) have a large norm and the remaining categories' representations in \(\mathcal{C}\setminus\{i\}\) are close to the zero vector. Intuitively, a hyperplane with this property models factors of variation that are uniquely present in representations of text that belong to a particular part of speech. We quantify this by formulating the following objective function:

\[\mathbf{W}_{i}=\operatorname*{arg\,max}_{\mathbf{W}_{i}^{\top}\mathbf{W}_{i}= \mathbf{I}_{k}}\bigg{\{}(1-\lambda)||\mathbf{W}_{i}^{\top}\mathbf{X}_{i}||_{F }^{2}-\underset{j\in\mathcal{C}\setminus\{i\}}{\sum}\lambda||\mathbf{W}_{i}^{ \top}\mathbf{X}_{j}||_{F}^{2}\bigg{\}},\] (1)

where \(\lambda\in[0,1]\) is a hyperparameter that controls the importance of killing the variation in the non-target categories relative to preserving the variation in the target class.

#### 2.1.1 Closed-form solution

Imposing column orthonormality on \(\mathbf{W}_{i}\) not only ensures its columns span a full \(k\)-dimensional subspace, but also allows Equation (1) to be solved in closed form. Concretely, we first manipulate the objective as follows

\[(1-\lambda)||\mathbf{W}_{i}^{\top}\mathbf{X}_{i}||_{F}^{2}- \underset{j\in\mathcal{C}\setminus\{i\}}{\sum}\lambda|| \mathbf{W}_{i}^{\top}\mathbf{X}_{j}||_{F}^{2}\] \[=(1-\lambda)\text{tr}\left((\mathbf{W}_{i}^{\top}\mathbf{X}_{i}) ^{\top}(\mathbf{W}_{i}^{\top}\mathbf{X}_{i})\right)-\underset{j\in\mathcal{C} \setminus\{i\}}{\sum}\lambda\text{tr}\left((\mathbf{W}_{i}^{\top}\mathbf{X}_{ j})^{\top}(\mathbf{W}_{i}^{\top}\mathbf{X}_{j})\right)\] (2) \[=\text{tr}\big{(}\mathbf{W}_{i}^{\top}((1-\lambda)\mathbf{X}_{i} \mathbf{X}_{i}^{\top}-\underset{j\in\mathcal{C}\setminus\{i\}}{\sum}\lambda \mathbf{X}_{j}\mathbf{X}_{j}^{\top})\mathbf{W}_{i}\big{)}\] (3) \[=\text{tr}\left(\mathbf{W}_{i}^{\top}\mathbf{C}_{i}\mathbf{W}_{i} \right),\] (4)

where \(\mathbf{C}_{i}=\left((1-\lambda)\mathbf{X}_{i}\mathbf{X}_{i}^{\top}-\underset {j\in\mathcal{C}\setminus\{i\}}{\sum}\lambda\mathbf{X}_{j}\mathbf{X}_{j}^{ \top}\right)\). Having now reformulated our original objective in Equation (1) as a (constrained) trace maximisation problem, its solution \(\mathbf{W}_{i}\) is given in closed form2 as the leading \(k\) eigenvectors of \(\mathbf{C}_{i}\).

Footnote 2: _Corollary 4.3.39_ of Horn and Johnson [28]. Note that all summands in \(\mathbf{C}_{i}\) are symmetric.

For ease of presentation, we have assumed the number of data points in each class to be equal. One can account for class imbalance straightforwardly whilst retaining a solution in closed form however: multiplying each Frobenius norm term in the original objective of Equation (1) by \(\frac{1}{n_{p}}\) (where \(n_{p}\) is the number of columns of \(\mathbf{X}_{p},\forall p\in\mathcal{C}\)) leads to \(\mathbf{C}_{i}=\left(\frac{1-\lambda}{n_{i}}\mathbf{X}_{i}\mathbf{X}_{i}^{\top }-\underset{j\in\mathcal{C}\setminus\{i\}}{\sum}\frac{\lambda}{n_{j}}\mathbf{ X}_{j}\mathbf{X}_{j}^{\top}\right)\) in Equation (4).

Figure 2: Our proposed method, shown on toy data, for learning ‘geodesic submanifolds’ [25] that capture the variance of visual attributes uniquely associated with specific parts of speech. After mapping to the tangent space (1), a linear subspace of the tangent space is learnt that captures the variation in only the target class \(i\) (leading eigenvectors of \(\hat{\mathbf{C}}_{i}\)). Test samples can then be projected onto this subspace (2), and mapped back (3) to the VL sphere.

Special casesTo contrast the proposed objective with related decompositions, it's instructive to consider the resulting subspaces for extreme values of \(\lambda\) (for zero-mean data). In the limit case of \(\lambda:=0\) for example, \(\mathbf{W}_{i}\) is given by the top principal components of the data points for one particular class \(i\). Conversely, a value of \(\lambda:=1\) gives the _bottom_ principal components of datapoints in all other classes \(j\in\mathcal{C}\setminus\{i\}\). Thus, \(\lambda\) can be seen as providing a trade-off between hyperplanes well-capturing the variance of the attributes in the target class and lying near-orthogonal to the points of the remaining classes. Finally, one recovers regular PCA when \(\mathbf{C}_{i}:=\sum_{j\in\mathcal{C}}\frac{1}{n_{j}}\mathbf{X}_{j}\mathbf{X}_ {j}^{\top}\).

### From subspaces to submanifolds

Through CLIP's choice of the cosine similarity as an objective function during training, the unit-norm VL representations live on the _hypersphere_\(\bm{\mathbf{z}}_{I},\bm{\mathbf{z}}_{T}\in\mathbb{S}^{d-1}\subset\mathbb{R}^{d}\)[24]. However, subspace learning in the ambient Euclidean space \(\mathbb{R}^{d}\) does not respect the underlying geometry of the manifold on which the data lie. An orthogonal projection of a vector onto the learnt Euclidean subspaces is not guaranteed to result in a vector that remains on the sphere, even if all the input data do. Motivated by this, we extend the component analysis for linear subspaces developed in Equation (1) to _geodesic submanifolds_[25]. As demonstrated in Fletcher et al. [25] for PCA, an analogous approximate projection onto the geodesic submanifolds capturing the desired variances can be made by applying the exact same component analysis of Equation (1) in the tangent space \(\mathcal{T}_{\bm{\mathbf{\mu}}}\mathbb{S}^{d-1}\subset\mathbb{R}^{d}\) to the VL sphere data's intrinsic mean \(\bm{\mathbf{\mu}}\in\mathbb{S}^{d-1}\) instead. To this end, we use the so-called _Logarithmic Map_\(\text{Log}_{\bm{\mathbf{p}}}:\mathbb{S}^{d-1}\to\mathcal{T}_{\bm{\mathbf{p}}} \mathbb{S}^{d-1}\), which maps points on the sphere to the tangent space at a reference point \(\bm{\mathbf{p}}\in\mathbb{S}^{d-1}\) and its inverse, the _Exponential Map_\(\text{Exp}_{\bm{\mathbf{p}}}:\mathcal{T}_{\bm{\mathbf{p}}}\mathbb{S}^{d-1}\to \mathbb{S}^{d-1}\) to map points back onto the hypersphere (whose well-known definitions are provided in the supplementary material).

We can then compute the subspace _of the tangent space to the intrinsic mean_ spanned by the columns of a \(\hat{\mathbf{W}}_{i}\in\mathbb{R}^{d\times k}\) for word class \(i\) as the leading \(k\) eigenvectors of \(\hat{\mathbf{C}}_{i}=\sum_{n}(1-\lambda)\text{Log}_{\bm{\mathbf{\mu}}}(\mathbf{ x}_{in})\text{Log}_{\bm{\mathbf{\mu}}}(\mathbf{x}_{in})^{\top}-\sum_{j\in \setminus\{i\}}\lambda\text{Log}_{\bm{\mathbf{\mu}}}(\mathbf{x}_{jn})\text{ Log}_{\bm{\mathbf{\mu}}}(\mathbf{x}_{jn})^{\top}\). We have again assumed an equal number of class data points purely for ease of presentation. This whole process is visualised in Figure 2.

By projecting onto the learnt subspaces of \(\mathcal{T}_{\bm{\mathbf{\mu}}}\mathbb{S}^{d-1}\), one can better _isolate_ or _remove_ the visual attributes associated with part of speech \(i\). To isolate or kill the attributes we compute the projection onto the column space (i) or orthogonal complements (ii) respectively with

(5)

Projection onto the Euclidean subspaces can be computed using the projection matrices \(\mathbf{P}_{i}=\mathbf{W}_{i}\mathbf{W}_{i}^{\top}\) and \(\mathbf{P}_{i}^{\perp}=\mathbf{I}_{d}-\mathbf{P}_{i}\). The way in which one can extract representations relating to the visual modes of variation for the parts of speech is explored in detail in Section 3.

## 3 Experiments

Here we present both qualitative (Section 3.1) and quantitative (Section 3.2) experiments to validate the model's ability to disentangle the object in a CLIP text or image representation from its appearance. We henceforth use'subspace' to refer throughout to the manifold generalisation from Section 2.2.

Implementation detailsFor all experiments, we use the following 4 parts of speech: _nouns_, _adjectives_, _verbs_, and _adverbs_. Our labelled data points (with which we compute the closed-form solution of Equation (4)) for these parts of speech are given by the WordNet [16] database. There are a total of \(112219\), \(18021\), \(7295\), and \(3910\) text-string data points from each of the categories respectively, after filtering out any word that appears in two or more parts of speech. We set \(\lambda:=1/2\) for all experiments. For all quantitative results, we use the base CLIP ViT-B-32 model. Please see the supplementary material for ablation studies on \(\lambda\) and experiments on additional CLIP architectures.

### Qualitative results

#### 3.1.1 Visual disentanglement

Recent CLIP-based text-to-image models (TTIMs) learn a mapping from the CLIP embeddings to synthetic images depicting visually a text prompt [15; 29; 30]-a process that Menon et al. [13]show is prone to also inheriting task bias. We begin in this section by following the experimental protocol of Materzynska et al. [31], using a recent popular CLIP-based TTIM [15] from LAION to demonstrate _visually_ how our PoS subspaces can succeed in predictably isolating visual variation in the CLIP representations pertaining to either the object described in a text prompt or its appearance.

As one motivating example, we first study a curious instance of 'visually polysemous' [14] natural language phrases-terms that have multiple visual associations. In particular, we find that when prompted with artists' names, TTIMs produce _both_ artworks in their style and images of the person themselves (e.g. the top row of Figure 3). That is to say, CLIP entangles in its representation these two dual meanings of the artists-their works' style and their physical appearance. We find that the PoS subspaces offer an intuitive way of reliably separating factors of visual variation in the representations for not just visually polysemous phrases, but also for natural language prompts more generally.

Concretely, projecting onto the orthogonal complement of the _noun_ subspace \(\Pi_{N}^{\perp}(\mathbf{z}_{T})\) successfully removes from the embedding visual information about the object/content described implicitly in a text prompt \(T\). For example, as visualised in row 2 of Figure 3 by reliably synthesising just artwork in an artist's style, or by leaving just the snowy or multicoloured appearance-based textures. On the other hand, projecting onto the adjective subspace's orthogonal complement \(\Pi_{A}^{\perp}(\mathbf{z}_{T})\) removes the visual appearances and styles associated with a text description (row 3 of Figure 3). For the visually polysemous artists' names, this isolates the representations of the artists themselves as humans-removing representations of their artwork styles. For the more complex prompts, this produces images of just the basic object described in the text prompt instead, such as the penguin or New York City. Many more examples of this visual disentanglement, details on the experimental setup, and ablation studies can be found in the supplementary material.

#### 3.1.2 Style-blocking subspace projections

One societal concern with free-form TTIMs is their ability to produce imitation artworks copying the style of artists. Here, we show how the learnt adjective subspace can be used as is as a step towards mitigating this. To achieve this, one simply modifies the TTIM forward pass to first project the CLIP text representations onto the orthogonal complement of the adjective subspace with \(\Pi_{A}^{\perp}(\mathbf{z}_{T})\) before feeding it into the image generator. We see from the results in Figure 4 that this modification indeed prevents the imitation of the visual styles of a range of artists (even with multiple forms of sentence structure in the prompt), whilst still enabling a diverse set of images to be generated nonetheless.

Visual theme subspacesWhilst successfully preventing the visual imitation of many famous artists, applying the adjective subspace projection to _every_ text prompt's CLIP representation can restrict the ability to use adjectives to specify visual appearance. We find a further effective strategy is to build additional subspaces for more specific visual appearances (e.g. artists' painting styles). Concretely,

Figure 3: The synthetic images from the original CLIP representations (top row) and when first projecting them onto the orthogonal complements of the noun and adjective subspaces to remove the ‘content’ and ‘appearance’ component, respectively.

we embed \(830\) artists' names and surnames in a new matrix \(\mathbf{X}_{i^{\prime}}\) and solve Equation (1) using all PoS classes in the negative summation to prevent the destruction of existing concepts. In contrast to the adjective space, projection onto the orthogonal complement of this 'artist subspace' _preserves_ adjective-based visual descriptions whilst also successfully preventing style imitation (Figure 4(a)). Crucially, we highlight that for the example shown in Figure 4(a), **the artist's name Qi Baishi is not present in the 'training' list of example artists**, suggesting the subspace has learnt a more general notion of an artist rather than simply the variation for only those artists whose names are provided as supervision.

We suggest more generally that subspaces learnt from a collection of words describing specific themes (whilst retaining variation in the PoS data through the main objective) could be useful for erasing entire visual concepts (such as NSFW imagery) from the CLIP representations. An additional example of another such custom subspace is shown in Figure 4(b) for removing only gory/bloody visual appearances. Not only does this offer a way to erase specific appearances in CLIP-based text-to-image models, but might also offer application in discriminative tasks, such as being able to block CLIP-based retrieval of images of sensitive nature. Please see the supplementary material for additional details, results, and visualisation of failure cases.

### Quantitative results

Here we show quantitatively how the PoS-grounded subspaces can isolate the variation in the CLIP representation of either an image _or_ text prompt. We validate this in two ways: through a class invariance metric in Section 3.2.1 and through zero-shot classification in Section 3.2.2.

Figure 4: Killing the CLIP representations’ component in the adjective subspace provides a way to block the imitation of artists’ styles in CLIP-based text-to-image models.

[MISSING_PAGE_FAIL:8]

classification3, given this is a very common application of CLIP in practice [1]. Importantly, these experiments serve as an additional way of measuring the subspaces' ability to isolate task-relevant modes of variation in the CLIP representations. With this in mind, we consider the baseline zero-shot setting [1] which computes the cosine similarity between all images in the training sets' and names of the class labels' CLIP representations. In our case, rather than computing the cosine similarity of the CLIP representations with \(S(\mathbf{z}_{I},\mathbf{z}_{T})\) we measure instead the similarity in the _noun_ subspace with \(S\big{(}\Pi_{N}(\mathbf{z}_{I}),\mathbf{z}_{T}\big{)}\). We hypothesise that if the noun subspace indeed better isolates the object of an image-invariant to the particular style of the image-we should expect to see improvement to the baseline image classification.

Footnote 3: For example, the colour in which a digit is drawn [33; 34] is not relevant information for the task of classifying which number it is.

We show the Top-1 accuracies for a large variety of datasets in Table 1. We find the noun submanifold projection to lead to improved zero-shot classification on \(14/15\) of the datasets considered with CLIP ViT-B-32, and include a comparison to the Euclidean subspaces in Figure 7 to further illustrate the benefit of the geometry-aware variant. This is without needing any prompt engineering or domain knowledge about each dataset separately, confirming that the subspaces serve the intended role of being able to isolate the visual modalities of interest automatically. For all results in this subsection, we project onto a relatively large \(k:=500\) dimensional subspace for PCA, PGA, and the proposed method. Please see the supplementary material for results on 2 more alternative CLIP architectures.

## 4 Related work

Vision-language representationsThere has been much interest in studying the properties of the VL representations in large-scale models such as CLIP. In particular, Goh et al. [12] show that there exist neurons that fire in response to both visual representations of a concept as well as to text relating to the concept. Viewing this as a form of _entanglement_, Materzynska et al. [31] address this by learning a linear subspace in which written text is disentangled from its visual component. Whilst we also wish to disentangle visual concepts, we do not focus on disentangling written words from visual representations, but rather visual modalities more generally. Further, our solution is instead given in closed form and has far fewer hyperparameters to tune. In addition to disentangling written/visual concepts, a number of works explore other forms of disentanglement in multimodal models. For example, to better perform image captioning [35; 36], generation [37; 38; 39; 40; 41], or discovering compositional representations or structure [37; 38; 39; 42].

Another popular approach to learning more useful VL representations is fine-tuning; Ilharco et al. [43] use fine-tuning in combination with weight interpolation to improve results on specific tasks without affecting existing performance, whilst many other works fine-tune CLIP for specific domains such as action recognition [11], video recognition [10], image captioning [7], and more [9]. Despite the success of fine-tuning, this comes with the restrictive requirement of task-specific labels. The method of 'prompting' [44; 17; 18; 13] is one alternative approach. At a high level, this technique learns a set of parameters (either in the form of 'words' in a prompt [17; 44] or as pixels appended to an input image [18; 13]) to steer the input to produce more useful representations for specific downstream tasks. In contrast, our method disentangles the modes of variation directly in the joint

Figure 7: Ablation study on the zero-shot accuracy for the subspaces vs the submanifolds with CLIP ViT-B-32; the submanifold outperforms the subspaces on almost all datasets considered.

vision-language representation space-this facilitates the ability to separate semantic information in _both_ image and text representations. Consequently, the proposed method has direct application for both discriminative tasks and CLIP text-driven generative tasks.

Component analysisLet the matrices \(\mathbf{X}_{1},\mathbf{X}_{2}\in\mathbb{R}^{d\times n}\) contain \(n\) data points of two classes in their columns. The _Principal Component Analysis_ (PCA) [32, 45] computes the low-dimensional subspace of maximum variance as the leading eigenvectors of the empirical covariance matrix of all data points. One pertinent drawback of PCA is its unsupervised nature in disregarding the labels of the data points. Whilst one can learn class-specific subspaces via PCA on the relevant subset of data (i.e. the eigenvectors of zero-mean \(\frac{1}{n}\mathbf{X}_{1}\mathbf{X}_{1}^{\top}\)), there is nothing to prevent the remaining 'nuisance' class(es) from also having large variance in this learnt subspace. One method that provides a way to jointly maximise the variance in one class' projected embeddings whilst minimizing that of another class is the _Fukunaga-Koontz transform_[46, 47] (FKT). FKT learns a class \(1\)-specific subspace spanned by the columns of \(\mathbf{W}_{1}\in\mathbb{R}^{d\times k}\) as the leading eigenvectors of \(\left(\mathbf{U}\mathbf{D}^{-\frac{1}{2}}\right)^{\top}\mathbf{X}_{1}\mathbf{ X}_{1}^{\top}\left(\mathbf{U}\mathbf{D}^{-\frac{1}{2}}\right)\), where \(\mathbf{U},\mathbf{D}\) are the eigenvectors (stacked column-wise) and eigenvalues (along the diagonal) respectively of the matrix \(\left(\mathbf{X}_{1}\mathbf{X}_{1}^{\top}+\mathbf{X}_{2}\mathbf{X}_{2}^{\top}\right)\). Whilst FKT also learns a subspace in which solely the target class' points have a large coefficient, the FKT operates on just two classes of interest and is more computationally expensive than the proposed method in requiring two eigendecompositions (rather than one). Another related supervised decomposition is the _Fisher Discriminant Analysis_ (FDA) [48] which learns a subspace in which classes can be easily discriminated. Despite FDA's objective involving a similar trace maximisation form to Equation (4), the goals of FDA and the proposed objective are very different. FDA aims to _minimise_ intra-class variation, whilst the proposed objective _maximises_ the norm of vectors of the target class in the learnt subspaces, whilst minimising those of the remaining classes. Additionally, the proposed objective does not suffer from the same restrictively small upper bound on the dimensionality of the learnt subspace as FDA does through the low-rank of the between-scatter matrix. A comparison to the subspaces learnt with FDA, FKT, and the proposed objective is shown in the supplementary material to illustrate the differences visually.

## 5 Conclusion

In this paper, we have proposed a method for learning geometry-aware subspaces in CLIP's vision-language space that disentangle the modes of visual variation in representations of both images and text. To achieve this, we used the semantic relationship between parts of speech in natural language and specific visual modes of variation. We demonstrated the disentanglement qualitatively with a text-to-image model, showcasing the model's ability to remove visual imitation of artists' styles from synthetic images. Class invariance metrics and zero-shot classification results further validated the disentanglement quantitatively.

LimitationsA common drawback of subspace learning approaches is choosing a good number of dimensions \(k\). Our method inherits this limitation, and one must choose the appropriate value for the specific task. Despite this, the closed-form eigensolution means only a single fast computation is needed, and any desired number of eigenvectors can be used at test-time. Although the recovered subspaces show wide applicability in downstream tasks, they are not able to perfectly separate the modes of variation for every possible image and text prompt. Additionally, the PoS supervision can not help address polysemy _within_ a particular part of speech (such as disambiguating between the bird/machine meanings of the word 'crane'). This further alludes to the promise of the custom subspaces in being able to target more specific visual concepts. Finally, often even minor changes to text-to-image models' text input or its representation produces very different output images [49]. As can be seen from our results, the subspace projection of the entire text representation similarly does not guarantee the preservation of the structure of the images generated from the 'original' prompts.

## 6 Acknowledgements

This research was partially supported by the EU's Horizon 2020 programme H2020-951911 AI4Media project, a grant from The Cyprus Institute on Cyclone, and by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program.

## References

* R. Bohbach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2022)High-resolution image synthesis with latent diffusion models. CVPR. Cited by: SS1.
* A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever (2021)Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., Cited by: SS1.
* C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. Le, Y. Sung, Z. Li, and T. Duerig (2021)Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904-4916. Cited by: SS1.
* L. Yuan, D. Chen, Y. Chen, N. C. F. Codella, X. Dai, J. Gao, H. Hu, X. Huang, B. Li, C. Li, C. Liu, M. Liu, Z. Liu, Y. Lu, Y. Shi, L. Wang, J. Wang, B. Xiao, Z. Xiao, J. Yang, M. Zeng, L. Zhou, and P. Zhang (2021)Florence: a new foundation model for computer vision. ArXivabs/2111.11432. Cited by: SS1.
* R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2022)High-resolution image synthesis with latent diffusion models. CVPR. Cited by: SS1.
* K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hallahan, L. Castricato, and E. Raff (2022)VQGAN-clip: open domain image generation and editing with natural language guidance. In ECCV, pp. 88-105. Cited by: SS1.
* O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021)StyleCLIP: text-driven manipulation of styleGAN imagery. In ICCV, pp. 2085-2094. Cited by: SS1.
* R. Mokady, A. Hertz, and A. H. Bermano (2021)ClipCap: clip prefix for image captioning. arXiv preprint arXiv:2111.09734. Cited by: SS1.
* Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier (2022)Language models can see: plugging visual controls in text generation. ArXivabs/2205.02655. Cited by: SS1.
* S. Shen, L. Li, H. Tan, M. Bansal, A. Rohrbach, K. Chang, Z. Yao, and K. Keutzer (2022)How much can CLIP benefit vision-and-language tasks?. In ICLR, Cited by: SS1.
* B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang, and H. Ling (2022)Expanding language-image pretrained models for general video recognition. In ECCV, pp. 1-18. Cited by: SS1.
* M. Wang, J. Xing, and Y. Liu (2021)ActionCLIP: a new paradigm for video action recognition. ArXivabs/2109.08472. Cited by: SS1.
* G. Goh, N. Cammarata, C. Voss, S. Barr, M. Petrov, L. Schubert, A. Radford, and C. Olah (2021)Multimodal neurons in artificial neural networks. Distill. Cited by: SS1.
* S. Menon, I. Preetam Chandratreya, and C. Vondrick (2022)Task bias in vision-language models. ArXivabs/2212.04412. Cited by: SS1.
* Y. Yao, J. Zhang, F. Shen, W. Yang, P. Huang, and Z. Tang (2018)Discovering and distinguishing multiple visual senses for polysemous words. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32. Cited by: SS1.
* D. Rampas, P. Pernias, E. Zhong, and M. Aubreville (2022)Fast text-conditional discrete denoising on vector-quantized latent spaces. Cited by: SS1.
* G. A. Miller (1995)WordNet: a lexical database for english. Commun. ACM38 (11), pp. 39-41. External Links: ISSN 0001-0782 Cited by: SS1.
* K. Zhou, J. Yang, C. C. Loy, and Z. Liu (2022)Conditional prompt learning for vision-language models. In CVPR, pp. 16816-16825. Cited by: SS1.
* H. Bahng, A. Jahanian, S. Sankaranarayanan, and P. Isola (2022)Exploring visual prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274. Cited by: SS1.
* A. Radford, L. Metz, and S. Chintala (2016)Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR. Cited by: SS1.
** [20] Erik Harkonen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. GANSpace: Discovering interpretable GAN controls. _NeurIPS_, 33:9841-9850, 2020.
* [21] Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in GANs. In _CVPR_, 2020.
* [22] Christos Tzelepis, Georgios Tzimiropoulos, and Ioannis Patras. WarpedGANSpace: Finding non-linear RBF paths in GAN latent space. In _ICCV_, 2021.
* [23] Yue Song, Jichao Zhang, Nicu Sebe, and Wei Wang. Householder projector for unsupervised latent semantics discovery. In _ICCV_, 2023.
* [24] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _Int. Conf. Mach. Learn._ JMLR.org, 2020.
* [25] P.T. Fletcher, Conglin Lu, S.M. Pizer, and Sarang Joshi. Principal geodesic analysis for the study of nonlinear statistics of shape. _IEEE Trans. Med. Imag._, 23(8):995-1005, 2004.
* [26] P Thomas Fletcher, Conglin Lu, and Sarang Joshi. Statistics of shape via principal geodesic analysis on lie groups. In _CVPR_, volume 1, pages I-I, 2003.
* [27] Stefan Sommer, Francois Lauze, Soren Hauberg, and Mads Nielsen. Manifold valued statistics, exact principal geodesic analysis and the effect of linear approximations. In _ECCV_, pages 43-56. Springer, 2010.
* [28] Roger A Horn and Charles R Johnson. _Matrix analysis, 2nd Ed._ Cambridge university press, 2012.
* [29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. _ArXiv_, abs/2204.06125, 2022.
* [30] Zihao Wang, Wei Liu, Qian He, Xin ru Wu, and Zili Yi. CLIP-GEN: Language-free training of a text-to-image generator with CLIP. _ArXiv_, abs/2203.00386, 2022.
* [31] Joanna Materzynska, Antonio Torralba, and David Bau. Disentangling visual and written concepts in CLIP. _CVPR_, pages 16389-16398, 2022.
* [32] Harold Hotelling. Analysis of a complex of statistical variables into principal components. _Journal of educational psychology_, 24(6):417, 1933.
* [33] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [34] Andrea Vedaldi. _Invariant representations and learning for computer vision_. PhD thesis, University of California, 2008.
* [35] Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G. Schwing, and David Alexander Forsyth. Fast, diverse and accurate image captioning guided by part-of-speech. _CVPR_, pages 10687-10696, 2018.
* [36] Shweta Mahajan and Stefan Roth. Diverse image captioning with context-object split latent spaces. _NeurIPS_, abs/2011.00966, 2020.
* [37] Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, and Lior Wolf. The hidden language of diffusion models, 2023.
* [38] Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for score-based conditional model. In _ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling_, 2023.
* [39] Matthew Trager, Pramuditha Perera, Luca Zancato, Alessandro Achille, Parminder Bhatia, and Stefano Soatto. Linear spaces of meanings: Compositional structures in vision-language models, 2023.
* [40] Jan Zuiderveld. Style-content disentanglement in language-image pretraining representations for zero-shot sketch-to-image synthesis, 2022.
* [41] Ian Huang, Panos Achlioptas, Tianyi Zhang, Sergei Tulyakov, Minhyuk Sung, and Leonidas Guibas. LADIS: Language disentanglement for 3D shape editing. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 5519-5532, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.404.

* [42] Tian Yun, Usha Bhalla, Ellie Pavlick, and Chen Sun. Do vision-language pretrained models learn composable primitive concepts? _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.
* [43] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. _NeurIPS_, 2022.
* [44] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _IJCV_, 130(9):2337-2348, 2022.
* [45] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. _The London, Edinburgh, and Dublin philosophical magazine and journal of science_, 2(11):559-572, 1901.
* [46] Keinosuke Fukunaga. _Introduction to statistical pattern recognition_. Elsevier, 2013.
* [47] Sheng Zhang and Terence Sim. Discriminant subspace analysis: A fukunaga-koontz approach. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 29, 2007.
* [48] R. A. Fisher. The use of multiple measurements in taxonomic problems. _Annals of Eugenics_, 7(2):179-188, 1936.
* [49] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022.
* [50] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022.
* [51] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _NeurIPS_, 35:27730-27744, 2022.
* [52] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. _arXiv preprint arXiv:2303.07345_, 2023.
* [53] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. _arXiv preprint arXiv:2303.13516_, 2023.

[MISSING_PAGE_EMPTY:14]

Figure 8: Additional results for visual disentanglement of text prompts using the learnt PoS subspaces.

Figure 9: Additional results for visual disentanglement of text prompts using the learnt PoS subspaces.

Visual theme removalAs shown in the main paper, the adjective subspace works remarkably well for preventing the imitation of artists' styles in the CLIP-based text-to-image model Paella. However, as stated in the main paper, the adjective subspace orthogonal projection for the task of'style blocking' is overly restrictive in also preventing the description of visual appearance with adjectives. We provide in Figure 10 some examples of this-for example, the'stormy' and'red' visual appearances are removed in Figure 10 after projection. On the other hand, the custom visual theme subspaces can target _specific_ visual appearances more precisely-two examples are shown in Figure 11 (following the experimental setup outlined in Appendix D).

Figure 11: Additional results for the two custom visual theme orthogonal complement subspace projection for ‘gore’ and artists’ styles.

Figure 10: ‘Style blocking’ with the adjective subspace is overly restrictive (here blocking ‘stormy’ and ‘red’ visual appearances).

Polysemy within a single part-of-speechAs described in the limitations, the PoS subspaces cannot disambiguate between polysemous phrases _within_ one particular PoS. We show in Figure 12 a proof-of-concept result suggesting that the custom visual subspaces can be a means of addressing this. In particular, we learn a custom "animal" subspace following the same protocol in Sect. 3.1.2 of the main paper. When projecting CLIP representations of "a photo of a crane" onto this subspace, we produce photos of the bird, rather than of machinery. Conversely, projecting the CLIP representation of "A photo of a bass" onto this subspace's orthogonal complement removes the representation of "bass" as a fish, synthesizing instead just images of bass guitars.

### Quantitative results

Visual theme subspace invarianceHere we show quantitative results for the visual theme-specific subspaces. We have evaluated this visually via text-to-image models in Figure 11, however here we wish to demonstrate that large variation in the CLIP representations directly is captured for only the themes of interest. Concretely, in Figure 13 we compute the same class invariance metric for \(200\) random WordNET words and random theme-specific words in the learnt custom theme subspaces through \(\frac{1}{200}||(\hat{\mathbf{W}}_{i}^{\top}\mathbf{Y})||_{F}^{2}\). Here, \(\mathbf{Y}\in\mathbb{R}^{d\times 200}\) contains in its columns the CLIP word embedding mapped to the tangent space, and \(i\) denotes the specific custom visual theme of interest for a particular subfigure. As can be seen from the high magnitude in the orange bars and low magnitude of the blue bars in Figure 13, these subspaces map the 200 random other words much closer to the zero vector than the theme-specific words. This indicates the variance in just the words of interest has indeed been captured.

Figure 12: Projecting onto a custom subspace (given by 2758 names of animals) and its orthogonal complement learnt using the proposed objective. Such custom visual subspaces show promise in helping disambiguate between polysemous nouns.

Additional zero-shot classifcationWe show in both Table 2 and Table 3 additional results for the baseline zero-shot classification protocol (following the exact same setup in the main paper) with the similarity metric \(S(\Pi_{N}(\mathbf{z}_{I}),\mathbf{z}_{T})\) after the noun subspace projection. As can be seen, the proposed subspace leads to improved zero-shot classification on a wide range of datasets, for multiple CLIP architectures.

## Appendix C Ablation studies

### Relationship to alternative component analyses

We first compare 1D subspaces learnt with our method, FDA [48], and FKT [46] shown in Figure 14 on toy data chosen to illustrate the qualitative differences in the properties of the subspaces. Given the very different goals of FDA in minimising intra-class variation, the resulting FDA subspace is near-orthogonal to that of FKT and the proposed method. Whilst the FKT-given subspace is very close to ours, for this particular illustrative toy data our subspace better kills the variance in the red data (as illustrated in Figure 14 b.iii). This figure also provides a visualisation of the learnt subspace's proximity to the principal component of the target class and the bottom principal component of the red class' datapoints-the proximity to the two extremes being controlled by \(\lambda\).

### Role of \(k\)

We show the impact of various choices of \(k\) (dimensionality of the subspaces) as visualised with text-to-image models in Figure 15 with the projections onto the orthogonal complements to kill the undesired variation. As can be seen, increasing \(k\) removes more and more visual information relevant to the particular visual mode. For example, we see the image feature increasingly less snow on the right, whilst increasing less of London on the left.

Figure 14: (a) A visual comparison of the leading eigenvector of \(\mathbf{C}_{i}\) to the first FDA component (centred for comparison), to the first FKT component. Shown in (b) are the points’ coordinates in the three subspaces. As can be seen, the learnt \(\mathbf{w}_{B1}\) captures large variance in the blue target class and is close to orthogonal to data points of the red class.

Figure 15: Ablation study on the value of \(k\) on the \(d=1024\) OpenCLIP VL space when projecting onto the orthogonal complements of \(k\)-dimensional adjective and noun subspaces.

### Role of \(\lambda\)

We next provide an ablation study on the value of \(\lambda\). We provide both a visual qualitative ablation and a quantitative one.

QualitativeConcretely, in each subfigure (row) of Figure 17, we take the first \(5000\) WordNET text strings from each part of speech and compute their CLIP text embeddings \(\mathbf{z}_{T}\in\mathbb{R}^{d}\). We then calculate \(\hat{\mathbf{W}}_{\uparrow}^{\top}\text{Log}_{\boldsymbol{\mu}}(\mathbf{z}_{T})\), plotting the first coordinate along the \(x\)-axis and the second along the \(y\)-axis of each subfigure in Figure 17. Ideally, the data points in the target class should be the only ones with a large norm if this hyperplane captures visual variation that is unique to a particular word class. We see that \(\lambda:=0\) preserves the most variance in the target class' embeddings but the different categories' projections are clearly entangled-the other classes' datapoints also have large norm. Conversely, \(\lambda:=1.0\) maps all points effectively to the zero vector-killing the variance in all categories. As can be seen in Figure 16(c), \(\lambda:=0.5\) offers a reasonable balance of both properties in this 4-class setting. The exact same experiments are run on the larger version of CLIP, shown in Figure 18, where similar conclusions can be drawn about the practical impact of \(\lambda\).

QuantitativeFor quantifying this in more dimensions, we compute the class invariance metric (used in the main paper) in Figure 19 and Figure 20 for various values of \(\lambda\), where we observe that \(\lambda:=0.5\) is a sensible choice for multiple CLIP architectures.

Visual subspacesFinally, we demonstrate the importance of using PoS as 'negative examples' in the summation in the main objective when learning visual theme-specific subspaces. Intuitively, whilst we want to maximise the variation for phrases of a particular theme (such as 'gory'), we also want to preserve the ability to generate other concepts with the TIIM, which is what the objective provides through the hyperparameter \(\lambda\).

In particular, we show in the second row of Figure 16 the visual results when projecting onto the orthogonal complement of a 'gory' subspace learnt when we do _not_ use the PoS as 'negative guidance' (i.e. when \(\lambda:=0\)). As can be seen in comparison to the third row of Figure 16, using the PoS is critical in this instance for retaining the ability to synthesise existing related concepts (here ensuring e.g. 'cranberry juice' can still be synthesised even though visually 'gory' appearances are removed).

Figure 16: Without using PoS as ‘negative guidance’ (i.e. when \(\lambda:=0\)), related concepts (e.g. ‘cranberry juice’) can be visually removed to a much greater extent than when using the PoS guidance (\(\lambda:=0.5\)).

Figure 17: Embeddings’ first two coordinates in the tangent space(es), with various values of \(\lambda\) in the main objective (axis limits are fixed to compare length of vectors across values of \(\lambda\)). The base CLIP model clip-vit-base-patch32 is used here.

Figure 18: Embeddings’ first two coordinates in the tangent space(es), with various values of \(\lambda\) in the main objective (axis limits are fixed to compare length of vectors across values of \(\lambda\)). The larger CLIP model clip-vit-large-patch14 is used here.

## Appendix D Experimental details

For reference in this section, we first include the dimensionalities of the shared VL spaces and public links to implementations used of the three CLIP models in this paper in Table 4.

Figure 19: Ablation on \(\lambda\) with the quantity \(\frac{1}{n_{j}}||(\hat{\mathbf{W}}_{i}^{\top}\mathbf{Y}_{j})||_{F}^{2}\) introduced in the main paper. Row-normalisation is performed to highlight the relative representation of each class’ embeddings within each subspace. The base CLIP model clip-vit-base-patch32 is used here.

Figure 20: Ablation on \(\lambda\) with the quantity \(\frac{1}{n_{j}}||(\hat{\mathbf{W}}_{i}^{\top}\mathbf{Y}_{j})||_{F}^{2}\) introduced in the main paper. Row-normalisation is performed to highlight the relative representation of each class’ embeddings within each subspace. The larger CLIP model clip-vit-large-patch14 is used here.

Visual disentanglementThe Paella TTIM [15] used in the main paper adopts the 'OpenCLIP' [50] model with a larger \(d=1024\)-dimensional VL representation. For all 'visual disentanglement' results throughout both the paper and supplementary material, we use \(k=768\) dimensional 'adjective' and 'noun' subspaces for all text prompts (apart from when removing the 'content' representations in visually polysemous phrases, where we find only \(k=32\) components are necessary).

Visual theme subspacesFor the custom visual subspaces, we produce a list of phrases related to the visual theme of interest by asking ChatGPT [51] questions of the format: Please give me a list of 250 words and phrases related to the concept of {x}, where x is the visual concept of interest (such as 'gore'). For the case of the artist subspace, we ask: Please give me a list of 250 of the most famous painters and visual artists of all time. In each scenario, we follow up twice more asking for additional responses (given the limited response length), specifying that it tries not to repeat any of the previous answers in the list. For the experiments in the 'gory' and 'artist' custom subspaces, ChatGPT gave us \(371\) and \(830\) unique phrases respectively (taking just the provided artists' surnames as additional examples for the latter), and use a \(k=128\)- and \(k=512\)-dimensional subspace respectively (given the limited number of phrases for the gore subspace provided by ChatGPT).

Concurrent workRecent preprints [52, 53] explicitly address the task of ablating particular concepts in diffusion models specifically. However, in contrast to the proposed method, these concurrent works fine-tune Stable Diffusion-specific [4] submodules, and do not focus on the final CLIP vector representations. Thus, there is no straightforward way to compare the proposed method working in CLIP's shared vision-language space directly nor the alternative Paella [15] TIIM. One methodological benefit to [52] over the proposed method however (purely in the context of text-to-image synthesis) is in the requirement of only a single text prompt describing a concept, relative to our necessary collection4. On the other hand, our subspaces are learnt in closed form-for example, the 'gory' subspace takes only 0.28 seconds to compute on a V100 GPU, given the CLIP embeddings. This is in contrast to [52]'s models which are stated to require 1000 gradient descent steps to compute, and [53] taking 5 minutes per concept.

Footnote 4: In particular, \(n\) phrases’ embeddings can span a subspace with a maximum of \(n\) dimensions

Compute time and hardwareTo run the Paella model, we use a 32GB NVIDIA Tesla V100 GPU. Learning the subspaces is particularly fast given the closed-form solution, taking just 1.1 seconds to compute all 4 (noun, adjective, verb, and adverb) PoS subspaces. Encoding all WordNet PoS examples with CLIP takes 28.91 minutes, however, this is a fixed cost and only needs to be done once at the beginning (after which any number of additional subspaces can be computed very quickly).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model name & clip-vit-base-patch32 & clip-vit-base-patch16 & clip-vit-large-patch14 & “OpenCLIP” \\ \hline Dimensionality of \(\mathbf{z_{I}},\mathbf{z_{T}}\) & 512 & 512 & 768 & 1024 \\ Public link & HuggingFace & HuggingFace & HuggingFace & Github \\ \hline \hline \end{tabular}
\end{table}
Table 4: Dimensionality of the VL space representations in the four CLIP models.