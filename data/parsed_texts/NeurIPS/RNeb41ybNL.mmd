# On the Mode-Seeking Properties of

Langevin Dynamics

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The Langevin Dynamics framework, which aims to generate samples from the score function of a probability distribution, is widely used for analyzing and interpreting score-based generative modeling. While the convergence behavior of Langevin Dynamics under unimodal distributions has been extensively studied in the literature, in practice the data distribution could consist of multiple distinct modes. In this work, we investigate Langevin Dynamics in producing samples from multimodal distributions and theoretically study its mode-seeking properties. We prove that under a variety of sub-Gaussian mixtures, Langevin Dynamics is unlikely to find all mixture components within a sub-exponential number of steps in the data dimension. To reduce the mode-seeking tendencies of Langevin Dynamics, we propose _Chained Langevin Dynamics_, which divides the data vector into patches of constant size and generates every patch sequentially conditioned on the previous patches. We perform a theoretical analysis of Chained Langevin Dynamics by reducing it to sampling from a constant-dimensional distribution. We present the results of several numerical experiments on synthetic and real image datasets, supporting our theoretical results on the iteration complexities of sample generation from mixture distributions using the chained and vanilla Langevin Dynamics.

## 1 Introduction

A central task in unsupervised learning involves learning the underlying probability distribution of training data and efficiently generating new samples from the distribution. Score-based generative modeling (SGM) (Song et al., 2020c) has achieved state-of-the-art performance in various learning tasks including image generation (Song and Ermon, 2019, 2020; Ho et al., 2020; Song et al., 2020a; Ramesh et al., 2022; Rombach et al., 2022), audio synthesis (Chen et al., 2020; Kong et al., 2020), and video generation (Ho et al., 2022; Blattmann et al., 2023). In addition to the successful empirical results, the convergence analysis of SGM has attracted significant attention in the recent literature (Lee et al., 2022, 2023; Chen et al., 2023; Li et al., 2023, 2024).

Stochastic gradient Langevin dynamics (SGLD) (Welling and Teh, 2011), as a fundamental methodology to implement and interpret SGM, can produce samples from the (Stein) score function of a probability density, i.e., the gradient of the log probability density function with respect to data. It has been widely recognized that a pitfall of SGLD is its slow mixing rate (Wooddard et al., 2009; Raginsky et al., 2017; Lee et al., 2018). Specifically, Song and Ermon (2019) shows that under a multi-modal data distribution, the samples from Langevin dynamics may have an incorrect relative density across the modes. Based on this finding, Song and Ermon (2019) proposes _anneal Langevin dynamics_, which injects different levels of Gaussian noise into the data distribution and samples with SGLD on the perturbed distribution. While outputting the correct relative density across modes can be challenging for SGLD, a natural question is whether SGLD would be able to find all the modes of a multi-modal distribution.

In this work, we study this question by analyzing the mode-seeking properties of SGLD. The notion of mode-seekingness (Bishop, 2006; Ke et al., 2021; Li and Farnia, 2023) refers to the property that a generative model captures only a subset of the modes of a multi-modal distribution. We note that a similar problem, known as metastability, has been studied in the context of Langevin diffusion, a continuous-time version of SGLD described by stochastic differential equation (SDE) (Bovier et al., 2002, 2004; Gayrard et al., 2005). Specifically, Bovier et al. (2002) gave a sharp bound on the mean hitting time of Langevin diffusion and proved that it may require exponential (in the space dimensionality \(d\)) time for transition between modes. Regarding discrete SGLD, Lee et al. (2018) constructed a probability distribution whose density is close to a mixture of two well-separated isotropic Gaussians, and proved that SGLD could not find one of the two modes within an exponential number of steps. However, further exploration of mode-seeking tendencies of SGLD and its variants such as annealed Langevin dynamics for general distributions is still lacking in the literature.

In this work, we theoretically formulate and demonstrate the potential mode-seeking tendency of SGLD. We begin by analyzing the convergence under a variety of Gaussian mixture probability distributions, under which SGLD could fail to visit all the mixture components within sub-exponential steps (in the data dimension). Subsequently, we generalize this result to mixture distributions with sub-Gaussian modes. This generalization extends our earlier result on Gaussian mixtures to a significantly larger family of mixture models, as the sub-Gaussian family includes any distribution over an \(\ell_{2}\)-norm-bounded support set. Furthermore, we extend our theoretical results to anneal Langevin dynamics with bounded noise scales.

To reduce SGLD's large iteration complexity shown under a high-dimensional input vector, we propose _Chained Langevin Dynamics (Chained-LD)_. Since SGLD could suffer from the curse of dimensionality, we decompose the sample \(\mathbf{x}\in\mathbb{R}^{d}\) into \(d/Q\) patches \(\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(d/Q)}\), each of constant size \(Q\), and sequentially generate every patch \(\mathbf{x}^{(q)}\) for all \(q\in[d/Q]\) statistically conditioned on previous patches, i.e., \(P(\mathbf{x}^{(q)}\mid\mathbf{x}^{(0)},\cdots\mathbf{x}^{(q-1)})\). The combination of all patches generated from the conditional distribution faithfully follows the probability density \(P(\mathbf{x})\), while learning each patch requires less cost due to the reduced dimension. We also provide a theoretical analysis of Chained-LD by reducing the convergence of a \(d\)-dimensional sample to the convergence of each patch.

Finally, we present the results of several numerical experiments to validate our theoretical findings. For synthetic experiments, we consider moderately high-dimensional Gaussian mixture models, where the vanilla and annealed Langevin dynamics could not find all the components within a million steps, while Chained-LD could capture all the components with correct frequencies in \(\mathcal{O}(10^{4})\) steps. For experiments on real image datasets, we consider a mixture of two modes by using the original images from MNIST/Fashion-MNIST training dataset (black background and white digits/objects) as the first mode and constructing the second mode by i.i.d. flipping the images (white background and black digits/objects) with probability 0.5. Following from Song and Ermon (2019), we trained a Noise Conditional Score Network (NCSN) to estimate the score function. Our numerical results indicate that vanilla Langevin dynamics can fail to capture the two modes, as also observed by Song and Ermon (2019). On the other hand, Chained-LD was capable of finding both modes regardless of initialization. We summarize the contributions of this work as follows:

* Theoretically studying the mode-seeking properties of vanilla and annealed Langevin dynamics,
* Proposing Chained Langevin Dynamics (Chained-LD), which decomposes the sample into patches and sequentially generates each patch conditioned on previous patches,
* Providing a theoretical analysis of the convergence behavior of Chained-LD,
* Numerically comparing the mode-seeking properties of vanilla, annealed, and chained Langevin dynamics.

**Notations:** We use \([n]\) to denote the set \(\{1,2,\cdots,n\}\). Also, in the paper, \(\|\cdot\|\) refers to the \(\ell_{2}\) norm. We use \(\mathbf{0}_{n}\) and \(\mathbf{1}_{n}\) to denote a 0-vector and 1-vector of length \(n\). We use \(\bm{I}_{n}\) to denote the identity matrix of size \(n\times n\). In the text, TV stands for the total variation distance.

## 2 Related Works

**Langevin Dynamics:** The convergence guarantees for Langevin diffusion, a continuous version of Langevin dynamics, are classical results extensively studied in the literature (Bhattacharya, 1978;Roberts and Tweedie, 1996; Bakry and Emery, 1983; Bakry et al., 2008). Langevin dynamics, also known as Langevin Monte Carlo, is a discretization of Langevin diffusion typically modeled as a Markov Chain Monte Carlo (Welling and Teh, 2011). For unimodal distributions, e.g., the probability density function that is log-concave or satisfies log-Sobolev inequality, the convergence of Langevin dynamics is provably fast (Dalalyan, 2017; Durmus and Moulines, 2017; Vempala and Wibisono, 2019). However, for multimodal distributions, the non-asymptotic convergence analysis is much more challenging (Cheng et al., 2018). Raginsky et al. (2017) gave an upper bound on the convergence time of Langevin dynamics for arbitrary non-log-concave distributions with certain regularity assumptions, which, however, could be exponentially large without imposing more restrictive assumptions. Lee et al. (2018) studied the special case of a mixture of Gaussians of equal variance and provided heuristic analysis of sampling from general non-log-concave distributions.

**Mode-Seekingness of Langevin Dynamics:** The investigation of the mode-seekingness of generative models starts with different generative adversarial network (GAN) (Goodfellow et al., 2014) model formulations and divergence measures, from both the practical (Goodfellow, 2016; Poole et al., 2016) and theoretical (Shannon et al., 2020; Li and Farnia, 2023) perspectives. In the context of Langevin dynamics, mode-seekingness is closely related to a lower bound on the transition time between two modes, e.g., two local maximums. Bovier et al. (2002, 2004); Gayrard et al. (2005) studied the mean hitting time of the continuous Langevin diffusion. Lee et al. (2018) proved the existence of a mixture of two Gaussian distributions whose covariance matrices differ by a constant factor, Langevin dynamics cannot find both modes in polynomial time.

**Score-based Generative Modeling:** Since Song et al. (2020b) proposed sliced score matching which can train deep models to learn the score functions of implicit probability distributions on high-dimensional data, score-based generative modeling (SGM) has been going through a spurt of growth. Annealed Langevin dynamics (Song and Ermon, 2019) estimates the noise score of the probability density perturbed by Gaussian noise and utilizes stochastic gradient Langevin dynamics to generate samples from a sequence of decreasing noise scales. Song and Ermon (2020) conducted a heuristic analysis of the effect of noise levels on the performance of annealed Langevin dynamics. Denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) incorporates a step-by-step introduction of random noise into data, followed by learning to reverse this diffusion process in order to generate desired data samples from the noise. Song et al. (2020c) unified anneal Langevin dynamics and DDPM via a stochastic differential equation. A recent line of work focuses on the non-asymptotic convergence guarantees for SGM with an imperfect score estimation under various assumptions on the data distribution (Block et al., 2020; De Bortoli et al., 2021; Lee et al., 2022; Chen et al., 2023; Benton et al., 2023; Li et al., 2023, 2024).

## 3 Preliminaries

### Langevin Dynamics

Generative modeling aims to produce samples such that their distribution is close to the underlying true distribution \(P\). For a continuously differentiable probability density \(P(\mathbf{x})\) on \(\mathbb{R}^{d}\), its score function is defined as the gradient of the log probability density function (PDF) \(\nabla_{\mathbf{x}}\log P(\mathbf{x})\). Langevin diffusion is a stochastic process defined by the stochastic differential equation (SDE)

\[\mathrm{d}\mathbf{x}_{t}=-\nabla_{\mathbf{x}}\log P(\mathbf{x}_{t})\,\mathrm{d }t+\sqrt{2}\,\mathrm{d}\mathbf{w}_{t},\]

where \(\mathbf{w}_{t}\) is the Wiener process on \(\mathbb{R}^{d}\). To generate samples from Langevin diffusion, Welling and Teh (2011) proposed stochastic gradient Langevin dynamics (SGLD), a discretization of the SDE for \(T\) iterations. Each iteration of SGLD is defined as

\[\mathbf{x}_{t}=\mathbf{x}_{t-1}+\frac{\delta_{t}}{2}\nabla_{\mathbf{x}}\log P( \mathbf{x}_{t-1})+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t},\] (1)

where \(\delta_{t}\) is the step size and \(\boldsymbol{\epsilon}_{t}\sim\mathcal{N}(\mathbf{0}_{d},\boldsymbol{I}_{d})\) is Gaussian noise. It has been widely recognized that Langevin diffusion could take exponential time to mix without additional assumptions on the probability density (Bovier et al., 2002, 2004; Gayrard et al., 2005; Raginsky et al., 2017; Lee et al., 2018). To combat the slow mixing, Song and Ermon (2019) proposed annealed Langevin dynamics by perturbing the probability density with Gaussian noise of variance \(\sigma^{2}\), i.e.,

\[P_{\sigma}(\mathbf{x}):=\int P(\mathbf{z})\mathcal{N}(\mathbf{x}\mid\mathbf{z },\sigma^{2}\boldsymbol{I}_{d})\,\mathrm{d}\mathbf{z},\] (2)and running SGLD on the perturbed data distribution \(P_{\sigma_{t}}(\mathbf{x})\) with gradually decreasing noise levels \(\{\sigma_{t}\}_{t\in[T]}\), i.e.,

\[\mathbf{x}_{t}=\mathbf{x}_{t-1}+\frac{\delta_{t}}{2}\nabla_{\mathbf{x}}\log P_{ \sigma_{t}}(\mathbf{x}_{t-1})+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t},\] (3)

where \(\delta_{t}\) is the step size and \(\boldsymbol{\epsilon}_{t}\sim\mathcal{N}(\mathbf{0}_{d},\boldsymbol{I}_{d})\) is Gaussian noise. When the noise level \(\sigma\) is vanishingly small, the perturbed distribution is close to the true distribution, i.e., \(P_{\sigma}(\mathbf{x})\approx P(\mathbf{x})\). Since we do not have direct access to the (perturbed) score function, Song and Ermon (2019) proposed the Noise Conditional Score Network (NCSN) \(\mathbf{s}_{\boldsymbol{\theta}}(\mathbf{x},\sigma)\) to jointly estimate the scores of all perturbed data distributions, i.e.,

\[\forall\sigma\in\left\{\sigma_{t}\right\}_{t\in[T]},\;\mathbf{s}_{\boldsymbol {\theta}}(\mathbf{x},\sigma)\approx\nabla_{\mathbf{x}}\log P_{\sigma}( \mathbf{x}).\]

To train the NCSN, Song and Ermon (2019) adopted denoising score matching, which minimizes the following loss

\[\mathcal{L}\left(\boldsymbol{\theta};\left\{\sigma_{t}\right\}_{t\in[T]} \right):=\frac{1}{2T}\sum_{t\in[T]}\sigma_{t}^{2}\mathbb{E}_{\mathbf{x}\sim P }\mathbb{E}_{\tilde{\mathbf{x}}\sim\mathcal{N}(\mathbf{x},\sigma_{t}^{2} \boldsymbol{I}_{d})}\bigg{[}\bigg{\|}\mathbf{s}_{\boldsymbol{\theta}}(\tilde{ \mathbf{x}},\sigma_{t})-\frac{\tilde{\mathbf{x}}-\mathbf{x}}{\sigma_{t}^{2}} \bigg{\|}^{2}\bigg{]}.\]

Assuming the NCSN has enough capacity, \(\mathbf{s}_{\boldsymbol{\theta}^{*}}(\mathbf{x},\sigma)\) minimizes the loss \(\mathcal{L}\left(\boldsymbol{\theta};\left\{\sigma_{t}\right\}_{t\in[T]}\right)\) if and only if \(\mathbf{s}_{\boldsymbol{\theta}^{*}}(\mathbf{x},\sigma_{t})=\nabla_{\mathbf{x }}\log P_{\sigma_{t}}(\mathbf{x})\) almost surely for all \(t\in[T]\).

### Multi-Modal Distributions

Our work focuses on multi-modal distributions. We use \(P=\sum_{i\in[k]}w_{i}P^{(i)}\) to represent a mixture of \(k\) modes, where each mode \(P^{(i)}\) is a probability density with frequency \(w_{i}\) such that \(w_{i}>0\) for all \(i\in[k]\) and \(\sum_{i\in[k]}w_{i}=1\). In our theoretical analysis, we consider Gaussian mixtures and sub-Gaussian mixtures, i.e., every component \(P^{(i)}\) is a Gaussian or sub-Gaussian distribution. A probability distribution \(p(\mathbf{z})\) of dimension \(d\) is defined as a sub-Gaussian distribution with parameter \(\nu^{2}\) if, given the mean vector \(\boldsymbol{\mu}:=\mathbb{E}_{\mathbf{z}\sim\mathcal{D}}[\mathbf{z}]\), the moment generating function (MGF) of \(p\) satisfies the following inequality for every vector \(\boldsymbol{\alpha}\in\mathbb{R}^{d}\):

\[\mathbb{E}_{\mathbf{z}\sim p}\left[\exp\big{(}\boldsymbol{\alpha}^{T}(\mathbf{ z}-\boldsymbol{\mu})\big{)}\leq\exp\Bigl{(}\frac{\nu^{2}\left\|\boldsymbol{ \alpha}\right\|_{2}^{2}}{2}\Bigr{)}.\] (4)

We remark that sub-Gaussian distributions include a wide variety of distributions such as Gaussian distributions and any distribution within a bounded \(\ell_{2}\)-norm distance from the mean \(\boldsymbol{\mu}\). From equation 2 we note that the perturbed distribution is the convolution of the original distribution and a Gaussian random variable, i.e., for random variables \(\mathbf{z}\sim p\) and \(\mathbf{t}\sim\mathcal{N}(\mathbf{0}_{d},\boldsymbol{I}_{d})\), their sum \(\mathbf{z}+\mathbf{t}\sim p_{\sigma}\) follows the perturbed distribution with noise level \(\sigma\). Therefore, a perturbed (sub)Gaussian distribution remains (sub)Gaussian. We formalize this property in Proposition 1 and defer the proof to Appendix A for completeness.

**Proposition 1**.: _Suppose the perturbed distribution of a \(d\)-dimensional probability distribution \(p\) with noise level \(\sigma\) is \(p_{\sigma}\), then the mean of the perturbed distribution is the same as the original distribution, i.e., \(\mathbb{E}_{\mathbf{z}\sim p_{\sigma}}[\mathbf{z}]=\mathbb{E}_{\mathbf{z}\sim p }[\mathbf{z}]\). If \(p=\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})\) is a Gaussian distribution, \(p_{\sigma}=\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma}+\sigma^{2} \boldsymbol{I}_{d})\) is also a Gaussian distribution. If \(p\) is a sub-Gaussian distribution with parameter \(\nu^{2}\), \(p_{\sigma}\) is a sub-Gaussian distribution with parameter \((\nu^{2}+\sigma^{2})\)._

## 4 Theoretical Analysis of the Mode-Seeking Properties of Langevin Dynamics

In this section, we theoretically investigate the mode-seeking properties of vanilla and annealed Langevin dynamics. We begin with analyzing Langevin dynamics in Gaussian mixtures.

### Langevin Dynamics in Gaussian Mixtures

**Assumption 1**.: _Consider a data distribution \(P:=\sum_{i=0}^{k}w_{i}P^{(i)}\) as a mixture of Gaussian distributions, where \(1\leq k=o(d)\) and \(w_{i}>0\) is a positive constant such that \(\sum_{i=0}^{k}w_{i}=1\). Suppose that \(P^{(i)}=\mathcal{N}(\boldsymbol{\mu}_{i},\nu_{i}^{2}\boldsymbol{I}_{d})\) is a Gaussian distribution over \(\mathbb{R}^{d}\) for all \(i\in\{0\}\cup[k]\) such that for all \(i\in[k]\), \(\nu_{i}<\nu_{0}\) and \(\left\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\right\|^{2}\leq\frac{\nu_{0}^ {2}-\nu_{1}^{2}}{2}\left(\log\left(\frac{\nu_{0}^{2}}{\nu_{0}^{2}}\right)-\frac{ \nu_{1}^{2}}{2\nu_{0}^{2}}+\frac{\nu_{2}^{2}}{2\nu_{1}^{2}}\right)d\). Denote \(\nu_{\max}:=\max_{i\in[k]}\nu_{i}\)._Regarding the first requirement \(\nu_{i}<\nu_{0}\), we first note that the probability density \(p(\mathbf{z})\) of a Gaussian distribution \(\mathcal{N}(\boldsymbol{\mu},\nu^{2}\boldsymbol{I}_{d})\) decays exponentially in terms of \(\frac{\left\|\mathbf{z}-\boldsymbol{\mu}\right\|^{2}}{\nu^{2}}\). When a state \(\mathbf{z}\) is sufficiently far from all modes (i.e., \(\left\|\mathbf{z}\right\|\gg\left\|\boldsymbol{\mu}_{i}\right\|\)), the Gaussian distribution with the largest variance (i.e., \(P^{(0)}\) in Assumption 1) dominates all other modes because \(\frac{\left\|\mathbf{z}-\boldsymbol{\mu}_{0}\right\|^{2}}{\nu_{0}^{2}}\approx \frac{\left\|\mathbf{z}\right\|^{2}}{\nu_{0}^{2}}\gg\frac{\left\|\mathbf{z} \right\|^{2}}{\nu_{t}^{2}}\approx\frac{\left\|\mathbf{z}-\boldsymbol{\mu}_{i} \right\|^{2}}{\nu_{t}^{2}}\). We call such mode \(P^{(0)}\) the _universal mode_. Therefore, if \(\mathbf{z}\) is initialized far from all modes, it can only converge to the universal mode because the gradient information of other modes is masked. Once \(\mathbf{z}\) enters the universal mode \(P^{(0)}\), if the step size \(\delta_{t}\) of Langevin dynamics is small (i.e., \(\delta_{t}\leq\nu_{0}^{2}\)), it would take exponential steps to escape the local mode \(P^{(0)}\); while if the step size is large (i.e., \(\delta_{t}>\nu_{0}^{2}\)), the state \(\mathbf{z}\) would again be far from all modes and thus the universal mode \(P^{(0)}\) dominates all other modes. Hence, \(\mathbf{z}\) can only visit the universal mode unless the stochastic noise \(\boldsymbol{\epsilon}_{t}\) miraculously leads it to the region of another mode. In addition, it can be verified that \(\log\left(\frac{\nu_{t}^{2}}{\nu_{0}^{2}}\right)-\frac{\nu_{t}^{2}}{2\nu_{0}^ {2}}+\frac{\nu_{0}^{2}}{2\nu_{t}^{2}}\) is a positive constant for \(\nu_{i}<\nu_{0}\), thus the second requirement of Assumption 1 essentially represents \(\left\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\right\|^{2}\leq\mathcal{O}(d)\). We formalize the intuition in Theorem 1 and defer the proof to Appendix A.1.

**Theorem 1**.: _Consider a data distribution \(P\) satisfying Assumption 1. We follow Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps. Suppose the sample is initialized in \(P^{(0)}\), then with probability at least \(1-T\cdot\exp(-\Omega(d))\), we have \(\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{i}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_ {\max}^{2}}{2}d\) for all \(t\in\{0\}\cup[T]\) and \(i\in[k]\)._

We note that \(\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{i}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_ {\max}^{2}}{2}d\) is a strong notion of mode-seekingness, since the probability density of mode \(P^{(i)}=\mathcal{N}(\boldsymbol{\mu}_{i},\nu_{t}^{2}\boldsymbol{I}_{d})\) concentrates around the \(\ell_{2}\)-norm ball \(\left\{\mathbf{z}:\left\|\mathbf{z}-\boldsymbol{\mu}_{i}\right\|^{2}\leq\nu_ {i}^{2}d\right\}\). This notion can also easily be translated into a lower bound in terms of other distance measures such as total variation distance and Wasserstein 2-distance. Moreover, in Theorem 2 we extend the result to annealed Langevin dynamics with bounded noise level, and the proof is deferred to Appendix A.2.

**Theorem 2**.: _Consider a data distribution \(P\) satisfying Assumption 1. We follow annealed Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps with noise levels \(c_{\sigma}\geq\sigma_{0}\geq\cdots\geq\sigma_{T}\geq 0\) for constant \(c_{\sigma}>0\). In addition, assume for all \(i\in[k]\), \(\left\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\right\|^{2}\leq\frac{\nu_{0} ^{2}-\nu_{0}^{2}}{2}\left(\log\left(\frac{\nu_{t}^{2}+c_{\sigma}^{2}}{\nu_{0}^ {2}+c_{\sigma}^{2}}\right)-\frac{\nu_{t}^{2}+c_{\sigma}^{2}}{2\nu_{0}^{2}+c _{\sigma}^{2}}+\frac{\nu_{0}^{2}+c_{\sigma}^{2}}{2\nu_{t}^{2}+c_{\sigma}^{2}} \right)d\). Suppose that the sample is initialized in \(P^{(0)}_{\sigma_{0}}\), then with probability at least \(1-T\cdot\exp(-\Omega(d))\), we have \(\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{i}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_ {\max}^{2}+2\sigma_{t}^{2}}{2}d\) for all \(t\in\{0\}\cup[T]\) and \(i\in[k]\)._

### Langevin Dynamics in Sub-Gaussian Mixtures

We further generalize our results to sub-Gaussian mixtures. We impose the following assumptions on the mixture. It is worth noting that these assumptions automatically hold for Gaussian mixtures.

**Assumption 2**.: _Consider a data distribution \(P:=\sum_{i=0}^{k}w_{i}P^{(i)}\) as a mixture of sub-Gaussian distributions, where \(1\leq k=o(d)\) and \(w_{i}>0\) is a positive constant such that \(\sum_{i=0}^{k}w_{i}=1\). Suppose that \(P^{(0)}=\mathcal{N}(\boldsymbol{\mu}_{0},\nu_{0}^{2}\boldsymbol{I}_{d})\) is Gaussian and for all \(i\in[k]\), \(P^{(i)}\) satisfies_

1. \(P^{(i)}\) _is a sub-Gaussian distribution of mean_ \(\boldsymbol{\mu}_{i}\) _with parameter_ \(\nu_{i}^{2}\)_,_
2. \(P^{(i)}\) _is differentiable and_ \(\nabla P^{(i)}(\boldsymbol{\mu}_{i})=\boldsymbol{0}_{d}\)_,_
3. _the score function of_ \(P^{(i)}\) _is_ \(L_{i}\)_-Lipschitz such that_ \(L_{i}\leq\frac{c_{L}}{\nu_{t}^{2}}\) _for some constant_ \(c_{L}>0\)_,_
4. \(\nu_{0}^{2}>\max\left\{1,\frac{4(c_{L}^{2}+c_{\sigma}c_{L})}{c_{\nu}(1-c_{\nu})} \right\}\frac{\nu_{\max}^{2}}{1-c_{\nu}}\) _for constant_ \(c_{\nu}\in(0,1)\)_, where_ \(\nu_{\max}:=\max_{i\in[k]}\nu_{i}\)_,_
5. \(\left\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\right\|^{2}\leq\frac{(1-c_{ \nu})\nu_{0}^{2}-\nu_{0}^{2}}{2(1-c_{\nu})}\left(\log\frac{c_{\nu}\nu_{t}^{2}}{( c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}-\frac{\nu_{t}^{2}}{2(1-c_{\nu})\nu_{0}^{2}}+ \frac{(1-c_{\nu})\nu_{0}^{2}}{2\nu_{t}^{2}}\right)d\)_._

We validate the feasibility of Assumption 2.v. in Lemma 9 in the Appendix. With Assumption 2, we show the mode-seeking tendency of Langevin dynamics under sub-Gaussian distributions in Theorem 3 and defer the proof to Appendix A.3.

**Theorem 3**.: _Consider a data distribution \(P\) satisfying Assumption 2. We follow Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps. Suppose the sample is initialized in \(P^{(0)}\), then with probability at least \(1-T\cdot\exp(-\mathcal{O}(d))\), we have \(\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{i}\right\|^{2}>\left(\frac{\nu_{0}^{2}} {2}+\frac{\nu_{\max}^{2}}{2(1-e_{\nu})}\right)d\) for all \(t\in\{0\}\cup[T]\) and \(i\in[k]\)._

Finally, we slightly modify Assumption 2 and extend our results to annealed Langevin dynamics under sub-Gaussian mixtures in Theorem 4. The details of Assumption 3 and the proof of Theorem 4 are deferred to Appendix A.4.

**Theorem 4**.: _Consider a data distribution \(P\) satisfying Assumption 3. We follow annealed Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps with noise levels \(c_{\sigma}\geq\sigma_{0}\geq\cdots\geq\sigma_{T}\geq 0\). Suppose the sample is initialized in \(P^{(0)}_{\sigma_{0}}\), then with probability at least \(1-T\cdot\exp(-\mathcal{O}(d))\), we have \(\left\|\mathbf{x}_{t}-\boldsymbol{\mu}_{i}\right\|^{2}>\left(\frac{\nu_{0}^{2} +\sigma_{1}^{2}}{2}+\frac{\nu_{\max}^{2}+\sigma_{2}^{2}}{2(1-e_{\nu})}\right)d\) for all \(t\in\{0\}\cup[T]\) and \(i\in[k]\)._

## 5 Chained Langevin Dynamics

To reduce the mode-seeking tendencies of vanilla and annealed Langevin dynamics, we propose Chained Langevin Dynamics (Chained-LD) in Algorithm 1. While vanilla and annealed Langevin dynamics apply gradient updates to all coordinates of the sample in every step, we decompose the sample into patches of constant size and generate each patch sequentially to alleviate the exponential dependency on the dimensionality. More precisely, we divide a sample \(\mathbf{x}\) into \(d/Q\) patches \(\mathbf{x}^{(1)},\cdots\mathbf{x}^{(d/Q)}\) of some constant size \(Q\), and apply annealed Langevin dynamics to sample each patch \(\mathbf{x}^{(q)}\) (for \(q\in[d/Q]\)) from the conditional distribution \(P(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots\mathbf{x}^{(q-1)})\).

An ideal conditional score function estimator \(\mathbf{s}_{\boldsymbol{\theta}}\) could jointly estimate the scores of all perturbed conditional patch distribution, i.e., \(\forall\sigma\in\left\{\sigma_{t}\right\}_{t\in[TQ/d]},q\in[d/Q]\),

\[\mathbf{s}_{\boldsymbol{\theta}}\left(\mathbf{x}^{(q)}\mid\sigma,\mathbf{x}^ {(1)},\cdots,\mathbf{x}^{(q-1)}\right)\approx\nabla_{\mathbf{x}^{(q)}}\log P_ {\sigma}(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots\mathbf{x}^{(q-1)}).\]

Following from Song and Ermon (2019), we use the denoising score matching to train the estimator. For a given \(\sigma\), the denoising score matching objective is

\[\ell(\boldsymbol{\theta};\sigma):=\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim P} \mathbb{E}_{\tilde{\mathbf{x}}\sim\mathcal{N}(\mathbf{x},\sigma^{2}}\boldsymbol {I}_{d})\sum_{q\in[d/Q]}\left[\left\|\mathbf{s}_{\boldsymbol{\theta}}\left( \mathbf{x}^{(q)}\mid\sigma,\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(q-1)}\right)- \frac{\tilde{\mathbf{x}}^{(q)}-\mathbf{x}^{(q)}}{\sigma^{2}}\right\|^{2} \right].\]

Then, combining the objectives gives the following loss

\[\mathcal{L}\left(\boldsymbol{\theta};\left\{\sigma_{t}\right\}_{t\in[TQ/d]} \right):=\frac{d}{TQ}\sum_{t\in[TQ/d]}\sigma_{t}^{2}\ell(\boldsymbol{\theta}; \sigma_{t}).\]

As shown in Vincent (2011), an estimator \(\mathbf{s}_{\boldsymbol{\theta}}\) with enough capacity minimizes the loss \(\mathcal{L}\) if and only if \(\mathbf{s}_{\boldsymbol{\theta}}\) outputs the scores of all perturbed conditional patch distribution almost surely. Ideally, if a samplerperfectly generates every patch, combining all patches gives a sample from the original distribution since \(P(\mathbf{x})=\prod_{q\in[d/Q]}P(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots \mathbf{x}^{(q-1)})\). In Theorem 5 we give a linear reduction from producing samples of dimension \(d\) using Chained-LD to learning the distribution of a \(Q\)-dimensional variable for constant \(Q\). The proof of Theorem 5 is deferred to Appendix A.5.

**Theorem 5**.: _Consider a sampler algorithm taking the first \(q-1\) patches \(\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(q-1)}\) as input and outputing a sample of the next patch \(\mathbf{x}^{(q)}\) with probability \(\hat{P}\left(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(q-1)}\right)\) for all \(q\in[d/Q]\). Suppose that for every \(q\in[d/Q]\) and any given previous patches \(\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(q-1)}\), the sampler algorithm can achieve_

\[\text{TV}\left(\hat{P}\left(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots, \mathbf{x}^{(q-1)}\right),P\left(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(q-1)}\right)\right)\leq\varepsilon\cdot\frac{Q}{d}\]

_in \(\tau(\varepsilon,d)\) iterations for some \(\varepsilon>0\). Then, equipped with the sampler algorithm, the Chained-LD algorithm in \(\frac{d}{Q}\cdot\tau(\varepsilon,d)\) iterations can achieve_

\[\text{TV}\left(\hat{P}(\mathbf{x}),P(\mathbf{x})\right)\leq\varepsilon.\]

## 6 Numerical Results

In this section, we empirically evaluated the mode-seeking tendencies of vanilla, annealed, and chained Langevin dynamics. We performed numerical experiments on synthetic Gaussian mixture models and real image datasets including MNIST (LeCun, 1998) and Fashion-MNIST (Xiao et al., 2017). Details on the experiment setup are deferred to Appendix B.

Figure 1: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are \(\ell_{2}\) distance from samples to the mean of the three modes. The samples are initialized in mode 0.

**Synthetic Gaussian mixture model:** We define the data distribution \(P\) as a mixture of three Gaussian components in dimension \(d=100\), where mode 0 defined as \(P^{(0)}=\mathcal{N}(\mathbf{0}_{d},3\boldsymbol{I}_{d})\) is the universal mode with the largest variance, and mode 1 and mode 2 are respectively defined as \(P^{(1)}=\mathcal{N}(\mathbf{1}_{d},\boldsymbol{I}_{d})\) and \(P^{(2)}=\mathcal{N}(-\mathbf{1}_{d},\boldsymbol{I}_{d})\). The frequencies of the three modes are 0.2, 0.4 and 0.4, i.e.,

\[P=0.2P^{(0)}+0.4P^{(1)}+0.4P^{(2)}=0.2\mathcal{N}(\mathbf{0}_{d},3\boldsymbol{ I}_{d})+0.4\mathcal{N}(\mathbf{1}_{d},\boldsymbol{I}_{d})+0.4\mathcal{N}(- \mathbf{1}_{d},\boldsymbol{I}_{d}).\]

As shown in Figure 1, vanilla and annealed Langevin dynamics cannot find mode 1 or 2 within \(10^{6}\) iterations if the sample is initialized in mode 0, while chained Langevin dynamics can find the other two modes in 1000 steps and correctly recover their frequencies as gradually increasing the number of iterations. In Appendix B.1 we present additional experiments on samples initialized in mode 1 or 2, which also verify the mode-seeking tendencies of vanilla and annealed Langevin dynamics.

**Image datasets:** We construct the distribution as a mixture of two modes by using the original images from MNIST/Fashion-MNIST training dataset (black background and white digits/objects) as the first mode and constructing the second mode by i.i.d. randomly flipping an image (white background and black digits/objects) with probability 0.5. Regarding the neural network architecture of the score function estimator, for vanilla and annealed Langevin dynamics we use U-Net (Ronneberger et al., 2015) following from Song and Ermon (2019). For chained Langevin dynamics, we proposed to use Recurrent Neural Network (RNN) architectures. We note that for a sequence of inputs, the output of RNN from the previous step is fed as input to the current step. Therefore, in the scenario of chained Langevin dynamics, the hidden state of RNN contains information about the previous patches and allows the network to estimate the conditional score function \(\nabla_{\mathbf{x}^{(q)}}\log P(\mathbf{x}^{(q)}\mid\mathbf{x}^{(1)},\cdots \mathbf{x}^{(q-1)})\). More implementation details are deferred to Appendix B.2.

Figure 2: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as original images from MNIST.

The numerical results on image datasets are shown in Figures 2 and 3. Vanilla Langevin dynamics fails to generate reasonable samples, as also observed in Song and Ermon (2019). When the sample is initialized as original images from the datasets, annealed Langevin dynamics tends to generate samples from the same mode, while chained Langevin dynamics can generate samples from both modes. Additional experiments are deferred to Appendix B.2.

## 7 Conclusion

In this work, we theoretically and numerically studied the mode-seeking properties of vanilla and annealed Langevin dynamics sampling methods under a multi-modal distribution. We characterized Gaussian and sub-Gaussian mixture models under which Langevin dynamics are unlikely to find all the components within a sub-exponential number of iterations. To reduce the mode-seeking tendency of vanilla Langevin dynamics, we proposed Chained Langevin Dynamics (Chained-LD) and analyzed its convergence behavior. Studying the connections between Chained-LD and denoising diffusion models will be an interesting topic for future exploration.

### Limitations

Our RNN-based implementation of Chained-LD is currently limited to image data generation tasks. An interesting future direction is to extend the application of Chained-LD to other domains such as audio and text data. Another future direction could be to study the convergence of Chained-LD under an imperfect score estimation which we did not address in our analysis.

Figure 3: Samples from a mixture distribution of the original and flipped images from the Fashion-MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as original images from Fashion-MNIST.

## References

* Bakry et al. (2008) Bakry, D., Barthe, F., Cattiaux, P., and Guillin, A. (2008). A simple proof of the poincare inequality for a large class of probability measures. _Electronic Communications in Probability [electronic only]_, 13:60-66.
* Bakry and Emery (1983) Bakry, D. and Emery, M. (1983). Diffusions hypercontractives. _Seminaire de Probabilites XIX_, page 177.
* Benton et al. (2023) Benton, J., De Bortoli, V., Doucet, A., and Deligiannidis, G. (2023). Linear convergence bounds for diffusion models via stochastic localization. _arXiv preprint arXiv:2308.03686_.
* Bhattacharya (1978) Bhattacharya, R. (1978). Criteria for recurrence and existence of invariant measures for multidimensional diffusions. _The Annals of Probability_, pages 541-553.
* Bishop (2006) Bishop, C. M. (2006). Pattern recognition and machine learning. _Springer google schola_, 2:645-678.
* Blattmann et al. (2023) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. (2023). Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22563-22575.
* Block et al. (2020) Block, A., Mroueh, Y., and Rakhlin, A. (2020). Generative modeling with denoising auto-encoders and langevin sampling. _arXiv preprint arXiv:2002.00107_.
* Bovier et al. (2002) Bovier, A., Eckhoff, M., Gayrard, V., and Klein, M. (2002). Metastability and low lying spectra in reversible markov chains. _Communications in mathematical physics_, 228:219-255.
* Bovier et al. (2004) Bovier, A., Eckhoff, M., Gayrard, V., and Klein, M. (2004). Metastability in reversible diffusion processes i: Sharp asymptotics for capacities and exit times. _Journal of the European Mathematical Society_, 6(4):399-424.
* Chen et al. (2020) Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. (2020). Wavegrad: Estimating gradients for waveform generation. In _International Conference on Learning Representations_.
* Chen et al. (2023) Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2023). Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _International Conference on Learning Representations_.
* Cheng et al. (2018) Cheng, X., Chatterji, N. S., Abbasi-Yadkori, Y., Bartlett, P. L., and Jordan, M. I. (2018). Sharp convergence rates for langevin dynamics in the nonconvex setting. _arXiv preprint arXiv:1805.01648_.
* Dalalyan (2017) Dalalyan, A. S. (2017). Theoretical guarantees for approximate sampling from smooth and log-concave densities. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 79(3):651-676.
* De Bortoli et al. (2021) De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709.
* Durmus and Moulines (2017) Durmus, A. and Moulines, E. (2017). Nonasymptotic convergence analysis for the unadjusted langevin algorithm. _The Annals of Applied Probability_, 27(3):1551-1587.
* Gayrard et al. (2005) Gayrard, V., Bovier, A., and Klein, M. (2005). Metastability in reversible diffusion processes ii: Precise asymptotics for small eigenvalues. _Journal of the European Mathematical Society_, 7(1):69-99.
* Goodfellow (2016) Goodfellow, I. (2016). Nips 2016 tutorial: Generative adversarial networks. _arXiv preprint arXiv:1701.00160_.
* Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. _Advances in neural information processing systems_, 27.
* Goodfellow et al. (2016)* Ho et al. (2022) Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. (2022). Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_.
* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851.
* Ke et al. (2021) Ke, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S. (2021). Imitation learning as f-divergence minimization. In _Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14_, pages 313-329. Springer.
* Kong et al. (2020) Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2020). Diffwave: A versatile diffusion model for audio synthesis. In _International Conference on Learning Representations_.
* Laurent and Massart (2000) Laurent, B. and Massart, P. (2000). Adaptive estimation of a quadratic functional by model selection. _Annals of statistics_, pages 1302-1338.
* LeCun (1998) LeCun, Y. (1998). The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_.
* Lee et al. (2022) Lee, H., Lu, J., and Tan, Y. (2022). Convergence for score-based generative modeling with polynomial complexity. _Advances in Neural Information Processing Systems_, 35:22870-22882.
* Lee et al. (2023) Lee, H., Lu, J., and Tan, Y. (2023). Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pages 946-985. PMLR.
* Lee et al. (2018) Lee, H., Risteski, A., and Ge, R. (2018). Beyond log-concavity: Provable guarantees for sampling multi-modal distributions using simulated tempering langevin monte carlo. _Advances in neural information processing systems_, 31.
* Li and Farnia (2023) Li, C. T. and Farnia, F. (2023). Mode-seeking divergences: theory and applications to gans. In _International Conference on Artificial Intelligence and Statistics_, pages 8321-8350. PMLR.
* Li et al. (2024) Li, G., Huang, Y., Efimov, T., Wei, Y., Chi, Y., and Chen, Y. (2024). Accelerating convergence of score-based diffusion models, provably. _arXiv preprint arXiv:2403.03852_.
* Li et al. (2023) Li, G., Wei, Y., Chen, Y., and Chi, Y. (2023). Towards non-asymptotic convergence for diffusion-based generative models. In _The Twelfth International Conference on Learning Representations_.
* Lin et al. (2017) Lin, G., Milan, A., Shen, C., and Reid, I. (2017). Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1925-1934.
* Poole et al. (2016) Poole, B., Alemi, A. A., Sohl-Dickstein, J., and Angelova, A. (2016). Improved generator objectives for gans. _arXiv preprint arXiv:1612.02780_.
* Raginsky et al. (2017) Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017). Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In _Conference on Learning Theory_, pages 1674-1703. PMLR.
* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3.
* Roberts and Tweedie (1996) Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of langevin distributions and their discrete approximations. _Bernoulli_, pages 341-363.
* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695.
* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In _Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18_, pages 234-241. Springer.
* Rombach et al. (2016)Sak, H., Senior, A., and Beaufays, F. (2014). Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. _arXiv preprint arXiv:1402.1128_.
* Shannon et al. (2020) Shannon, M., Poole, B., Mariooryad, S., Bagby, T., Battenberg, E., Kao, D., Stanton, D., and Skerry-Ryan, R. (2020). Non-saturating gan training as divergence minimization. _arXiv preprint arXiv:2010.08029_.
* Song et al. (2020a) Song, J., Meng, C., and Ermon, S. (2020a). Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_.
* Song and Ermon (2019) Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32.
* Song and Ermon (2020) Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448.
* Song et al. (2020b) Song, Y., Garg, S., Shi, J., and Ermon, S. (2020b). Sliced score matching: A scalable approach to density and score estimation. In _Uncertainty in Artificial Intelligence_, pages 574-584. PMLR.
* Song et al. (2020c) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2020c). Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_.
* Vempala and Wibisono (2019) Vempala, S. and Wibisono, A. (2019). Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. _Advances in neural information processing systems_, 32.
* Vincent (2011) Vincent, P. (2011). A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674.
* Welling and Teh (2011) Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688. Citeseer.
* Wooddard et al. (2009) Wooddard, D. B., Schmidler, S. C., and Huber, M. (2009). Conditions for rapid mixing of parallel and simulated tempering on multimodal distributions. _The Annals of Applied Probability_, 19(2):617-640.
* Xiao et al. (2017) Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_.

Theoretical Analysis on the Mode-Seeking Tendency of Langevin Dynamics

We begin by introducing some well-established lemmas used in our proof. We first provide the proof of Proposition 1 for completeness:

Proof of Proposition 1.: By the definition in equation 2, we have

\[p_{\sigma}(\mathbf{z})=\int p(\mathbf{t})\mathcal{N}(\mathbf{z}\mid\mathbf{t}, \sigma^{2}\bm{I}_{d})\,\mathrm{d}\mathbf{t}=\int p(\mathbf{t})\mathcal{N}( \mathbf{z}-\mathbf{t}\mid\bm{0}_{d},\sigma^{2}\bm{I}_{d})\,\mathrm{d}\mathbf{t}.\]

For random variables \(\mathbf{t}\sim p\) and \(\mathbf{y}\sim\mathcal{N}(\bm{0}_{d},\bm{I}_{d})\), their sum \(\mathbf{z}=\mathbf{t}+\mathbf{y}\sim p_{\sigma}\) follows the perturbed distribution with noise level \(\sigma\). Therefore,

\[\mathbb{E}_{\mathbf{z}\sim p_{\sigma}}[\mathbf{z}]=\mathbb{E}_{(\mathbf{t}+ \mathbf{y})\sim p_{\sigma}}[\mathbf{t}+\mathbf{y}]=\mathbb{E}_{\mathbf{t}\sim p }[\mathbf{t}]+\mathbb{E}_{\mathbf{y}\sim\mathcal{N}(\bm{0}_{d},\bm{I}_{d})}[ \mathbf{y}]=\mathbb{E}_{\mathbf{t}\sim p}[\mathbf{t}].\]

If \(\mathbf{t}\sim p=\mathcal{N}(\bm{\mu},\bm{\Sigma})\) follows a Gaussian distribution, we have \(\mathbf{z}=\mathbf{t}+\mathbf{y}\sim p_{\sigma}=\mathcal{N}(\bm{\mu},\bm{ \Sigma}+\sigma^{2}\bm{I}_{d})\). If \(p\) is a sub-Gaussian distribution with parameter \(\nu^{2}\), we have \(\mathbf{z}=\mathbf{t}+\mathbf{y}\sim p_{\sigma}\) is a sub-Gaussian distribution with parameter \((\nu^{2}+\sigma^{2})\). Hence we obtain Proposition 1. 

We use the following lemma on the tail bound for multivariate Gaussian random variables.

**Lemma 1** (Lemma 1, Laurent and Massart (2000)).: _Suppose that a random variable \(\mathbf{z}\sim\mathcal{N}(\bm{0}_{d},\bm{I}_{d})\). Then for any \(\lambda>0\),_

\[\mathbb{P}\left(\left\|\mathbf{z}\right\|^{2}\geq d+2\sqrt{d \lambda}+2\lambda\right) \leq\exp(-\lambda),\] \[\mathbb{P}\left(\left\|\mathbf{z}\right\|^{2}\leq d-2\sqrt{d \lambda}\right) \leq\exp(-\lambda).\]

We also use a tail bound for one-dimensional Gaussian random variables and provide the proof here for completeness.

**Lemma 2**.: _Suppose a random variable \(Z\sim\mathcal{N}(0,1)\). Then for any \(t>0\),_

\[\mathbb{P}(Z\geq t)=\mathbb{P}(Z\leq-t)\leq\frac{\exp(-t^{2}/2)}{ \sqrt{2\pi t}}.\]

Proof of Lemma 2.: Since \(\frac{z}{t}\geq 1\) for all \(z\in[t,\infty)\), we have

\[\mathbb{P}(Z\geq t)=\frac{1}{\sqrt{2\pi}}\int_{t}^{\infty}\exp \left(-\frac{z^{2}}{2}\right)\,\mathrm{d}z\leq\frac{1}{\sqrt{2\pi}}\int_{t}^{ \infty}\frac{z}{t}\exp\left(-\frac{z^{2}}{2}\right)\,\mathrm{d}z=\frac{\exp(-t ^{2}/2)}{\sqrt{2\pi t}}.\]

Since the Gaussian distribution is symmetric, we have \(\mathbb{P}(Z\geq t)=\mathbb{P}(Z\leq-t)\). Hence we obtain the desired bound. 

### Proof of Theorem 1: Langevin Dynamics under Gaussian Mixtures

Without loss of generality, we assume that \(\bm{\mu}_{0}=\bm{0}_{d}\) for simplicity. Let \(r\) and \(n\) respectively denote the rank and nullity of the vector space \(\left\{\bm{\mu}_{i}\right\}_{i\in[k]}\), then we have \(r+n=d\) and \(0\leq r\leq k=o(d)\). Denote \(\mathbf{R}\in\mathbb{R}^{d\times r}\) an orthonormal basis of the vector space \(\left\{\bm{\mu}_{i}\right\}_{i\in[k]}\), and denote \(\mathbf{N}\in\mathbb{R}^{d\times n}\) an orthonormal basis of the null space of \(\left\{\bm{\mu}_{i}\right\}_{i\in[k]}\). Now consider decomposing the sample \(\mathbf{x}_{t}\) by

\[\mathbf{r}_{t}:=\mathbf{R}^{T}\mathbf{x}_{t}\text{, and }\mathbf{n}_{t}:= \mathbf{N}^{T}\mathbf{x}_{t},\]

where \(\mathbf{r}_{t}\in\mathbb{R}^{r}\), \(\mathbf{n}_{t}\in\mathbb{R}^{n}\). Then we have

\[\mathbf{x}_{t}=\mathbf{R}\mathbf{r}_{t}+\mathbf{N}\mathbf{n}_{t}.\]

Similarly, we decompose the noise \(\bm{\epsilon}_{t}\) into

\[\bm{\epsilon}_{t}^{(\mathbf{r})}:=\mathbf{R}^{T}\bm{\epsilon}_{t}\text{, and }\bm{\epsilon}_{t}^{(\mathbf{n})}:=\mathbf{N}^{T}\bm{\epsilon}_{t},\]

where \(\bm{\epsilon}_{t}^{(\mathbf{r})}\in\mathbb{R}^{r}\), \(\bm{\epsilon}_{t}^{(\mathbf{n})}\in\mathbb{R}^{n}\). Then we have

\[\bm{\epsilon}_{t}=\mathbf{R}\bm{\epsilon}_{t}^{(\mathbf{r})}+\mathbf{N}\bm{ \epsilon}_{t}^{(\mathbf{n})}.\]Since a linear combination of a Gaussian random variable still follows Gaussian distribution, by \(\bm{\epsilon}_{t}\sim\mathcal{N}(\bm{0}_{d},\bm{I}_{d})\), \(\mathbf{R}^{T}\mathbf{R}=\bm{I}_{r}\), and \(\mathbf{N}^{T}\mathbf{N}=\bm{I}_{n}\) we obtain

\[\bm{\epsilon}_{t}^{(\mathbf{r})}\sim\mathcal{N}(\bm{0}_{r},\bm{I}_{r}),\text{ and }\bm{\epsilon}_{t}^{(\mathbf{n})}\sim\mathcal{N}(\bm{0}_{n},\bm{I}_{n}).\]

By the definition of Langevin dynamics in equation 1, the two components of \(\mathbf{x}_{t}\) follow from the update rule:

\[\mathbf{n}_{t} =\mathbf{n}_{t-1}+\frac{\delta_{t}}{2}\mathbf{N}^{T}\nabla_{ \mathbf{x}}\log P(\mathbf{x}_{t-1})+\sqrt{\delta_{t}}\bm{\epsilon}_{t}^{( \mathbf{n})},\] (5) \[\mathbf{r}_{t} =\mathbf{r}_{t-1}+\frac{\delta_{t}}{2}\mathbf{R}^{T}\nabla_{ \mathbf{x}}\log P(\mathbf{x}_{t-1})+\sqrt{\delta_{t}}\bm{\epsilon}_{t}^{( \mathbf{r})}.\]

It is worth noting that since \(\mathbf{N}^{T}\bm{\mu}_{i}=\bm{0}_{n}\). To show \(\left\|\mathbf{x}_{t}-\bm{\mu}_{i}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2 }}{2}d\), it suffices to prove

\[\left\|\mathbf{n}_{t}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d.\]

We start by proving that the initialization of the state \(\mathbf{x}_{0}\) has a large norm on the null space with high probability in the following proposition.

**Proposition 2**.: _Suppose that a sample \(\mathbf{x}_{0}\) is initialized in the distribution \(P^{(0)}\), i.e., \(\mathbf{x}_{0}\sim P^{(0)}\), then for any constant \(\nu_{\max}<\nu_{0}\), with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}d\)._

Proof of Proposition 2.: Since \(\mathbf{x}_{0}\sim P^{(0)}=\mathcal{N}(\bm{0}_{d},\nu_{0}^{2}\bm{I}_{d})\) and \(\mathbf{N}^{T}\mathbf{N}=\bm{I}_{n}\), we know \(\mathbf{n}_{0}=\mathbf{N}^{T}\mathbf{x}_{0}\sim\mathcal{N}(\bm{0}_{n},\nu_{0} ^{2}\bm{I}_{n})\). Therefore, by Lemma 1 we can bound

\[\mathbb{P}\left(\left\|\mathbf{n}_{0}\right\|^{2}\leq\frac{3\nu_ {0}^{2}+\nu_{\max}^{2}}{4}d\right) =\mathbb{P}\left(\frac{\left\|\mathbf{n}_{0}\right\|^{2}}{\nu_{0 }^{2}}\leq d-2\sqrt{d\cdot\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^{2} }\right)^{2}d}\right)\] \[\leq\mathbb{P}\left(\frac{\left\|\mathbf{n}_{0}\right\|^{2}}{\nu _{0}^{2}}\leq n-2\sqrt{n\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^{2} }\right)^{2}\frac{d}{2}}\right)\] \[\leq\exp\left(-\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^ {2}}\right)^{2}\frac{d}{2}\right),\]

where the second last step follows from the assumption \(d-n=r=o(d)\). Hence we complete the proof of Proposition 2. 

Then, with the assumption that the initialization satisfies \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}d\), the following proposition shows that \(\left\|\mathbf{n}_{t}\right\|\) remains large with high probability.

**Proposition 3**.: _Consider a data distribution \(P\) satisfies the constraints specified in Theorem 1. We follow the Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps. Suppose that the initial sample satisfies \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}d\), then with probability at least \(1-T\cdot\exp(-\Omega(d))\), we have that \(\left\|\mathbf{n}_{t}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d\) for all \(t\in\{0\}\cup[T]\)._

Proof of Proposition 3.: To establish a lower bound on \(\left\|\mathbf{n}_{t}\right\|\), we consider different cases of the step size \(\delta_{t}\). Intuitively, when \(\delta_{t}\) is large enough, \(\mathbf{n}_{t}\) will be too noisy due to the introduction of random noise \(\sqrt{\delta_{t}}\bm{\epsilon}_{t}^{(\mathbf{n})}\) in equation 5. While for small \(\delta_{t}\), the update of \(\mathbf{n}_{t}\) is bounded and thus we can iteratively analyze \(\mathbf{n}_{t}\). We first handle the case of large \(\delta_{t}\) in the following lemma.

**Lemma 3**.: _If \(\delta_{t}>\nu_{0}^{2}\), with probability at least \(1-\exp(-\Omega(d))\), for \(\mathbf{n}_{t}\) satisfying equation 5, we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}d\) regardless of the previous state \(\mathbf{x}_{t-1}\)._

Proof of Lemma 3.: Denote \(\mathbf{v}:=\mathbf{n}_{t-1}+\frac{\delta_{t}}{2}\mathbf{N}^{T}\nabla_{ \mathbf{x}}\log P(\mathbf{x}_{t-1})\) for simplicity. Note that \(\mathbf{v}\) is fixed for any given \(\mathbf{x}_{t-1}\). We decompose \(\bm{\epsilon}_{t}^{(\mathbf{n})}\) into a vector aligning with \(\mathbf{v}\) and another vector orthogonal to \(\mathbf{v}\). Consider an orthonormal matrix \(\mathbf{M}\in\mathbb{R}^{n\times(n-1)}\) such that \(\mathbf{M}^{T}\mathbf{v}=\mathbf{0}_{n-1}\) and \(\mathbf{M}^{T}\mathbf{M}=\boldsymbol{I}_{n-1}\). By denoting \(\mathbf{u}:=\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}-\mathbf{M}\mathbf{M}^{T} \boldsymbol{\epsilon}_{t}^{(\mathbf{n})}\) we have \(\mathbf{M}^{T}\mathbf{u}=\mathbf{0}_{n-1}\), thus we obtain

\[\left\|\mathbf{n}_{t}\right\|^{2} =\left\|\mathbf{v}+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t}^{( \mathbf{n})}\right\|^{2}\] \[=\left\|\mathbf{v}+\sqrt{\delta_{t}}\mathbf{u}+\sqrt{\delta_{t} }\mathbf{M}\mathbf{M}^{T}\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}\right\|^{2}\] \[=\left\|\mathbf{v}+\sqrt{\delta_{t}}\mathbf{u}\right\|^{2}+ \left\|\sqrt{\delta_{t}}\mathbf{M}\mathbf{M}^{T}\boldsymbol{\epsilon}_{t}^{( \mathbf{n})}\right\|^{2}\] \[\geq\left\|\sqrt{\delta_{t}}\mathbf{M}\mathbf{M}^{T}\boldsymbol{ \epsilon}_{t}^{(\mathbf{n})}\right\|^{2}\] \[\geq\nu_{0}^{2}\left\|\mathbf{M}^{T}\boldsymbol{\epsilon}_{t}^{( \mathbf{n})}\right\|^{2}.\]

Since \(\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}\sim\mathcal{N}(\mathbf{0}_{n}, \boldsymbol{I}_{n})\) and \(\mathbf{M}^{T}\mathbf{M}=\boldsymbol{I}_{n-1}\), we obtain \(\mathbf{M}^{T}\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}\sim\mathcal{N}( \mathbf{0}_{n-1},\boldsymbol{I}_{n-1})\). Therefore, by Lemma 1 we can bound

\[\mathbb{P}\left(\left\|\mathbf{n}_{t}\right\|^{2}\leq\frac{3\nu_{0 }^{2}+\nu_{\max}^{2}}{4}d\right) \leq\mathbb{P}\left(\left\|\mathbf{M}^{T}\boldsymbol{\epsilon}_{t }^{(\mathbf{n})}\right\|^{2}\leq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4\nu_{0}^{ 2}}d\right)\] \[=\mathbb{P}\left(\left\|\mathbf{M}^{T}\boldsymbol{\epsilon}_{t}^{( \mathbf{n})}\right\|^{2}\leq d-2\sqrt{d\cdot\left(\frac{\nu_{0}^{2}-\nu_{\max }^{2}}{8\nu_{0}^{2}}\right)^{2}d}\right)\] \[\leq\mathbb{P}\left(\left\|\mathbf{M}^{T}\boldsymbol{\epsilon}_{t }^{(\mathbf{n})}\right\|^{2}\leq(n-1)-2\sqrt{(n-1)\left(\frac{\nu_{0}^{2}-\nu_ {\max}^{2}}{8\nu_{0}^{2}}\right)^{2}\frac{d}{2}}\right)\] \[\leq\exp\left(-\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^{ 2}}\right)^{2}\frac{d}{2}\right),\]

where the second last step follows from the assumption \(d-n=r=o(d)\). Hence we complete the proof of Lemma 3. 

We then consider the case when \(\delta_{t}\leq\nu_{0}^{2}\). Let \(\mathbf{r}:=\mathbf{R}^{T}\mathbf{x}\) and \(\mathbf{n}:=\mathbf{N}^{T}\mathbf{x}\), then \(\mathbf{x}=\mathbf{R}\mathbf{r}+\mathbf{N}\mathbf{n}\). We first show that when \(\left\|\mathbf{n}\right\|^{2}\geq\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d\), \(P^{(i)}(\mathbf{x})\) is exponentially smaller than \(P^{(0)}(\mathbf{x})\) for all \(i\in[k]\) in the following lemma.

**Lemma 4**.: _Given that \(\left\|\mathbf{n}\right\|^{2}\geq\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d\) and \(\left\|\boldsymbol{\mu}_{i}\right\|^{2}\leq\frac{\nu_{0}^{2}-\nu_{i}^{2}}{2} \left(\log\left(\frac{\nu_{i}^{2}}{\nu_{0}^{2}}\right)-\frac{\nu_{i}^{2}}{2\nu _{0}^{2}}+\frac{\nu_{0}^{2}}{2\nu_{i}^{2}}\right)d\) for all \(i\in[k]\), we have \(\frac{P^{(i)}(\mathbf{x})}{P^{(0)}(\mathbf{x})}\leq\exp(-\Omega(d))\) for all \(i\in[k]\)._

Proof of Lemma 4.: For all \(i\in[k]\), define \(\rho_{i}(\mathbf{x}):=\frac{P^{(i)}(\mathbf{x})}{P^{(0)}(\mathbf{x})}\), then

\[\rho_{i}(\mathbf{x}) =\frac{P^{(i)}(\mathbf{x})}{P^{(0)}(\mathbf{x})}=\frac{(2\pi\nu_{i }^{2})^{-d/2}\exp\left(-\frac{1}{2\nu_{i}^{2}}\left\|\mathbf{x}-\boldsymbol{ \mu}_{i}\right\|^{2}\right)}{(2\pi\nu_{0}^{2})^{-d/2}\exp\left(-\frac{1}{2\nu_ {0}^{2}}\left\|\mathbf{x}\right\|^{2}\right)}\] \[=\left(\frac{\nu_{0}^{2}}{\nu_{i}^{2}}\right)^{d/2}\exp\left(\frac {1}{2\nu_{0}^{2}}\left\|\mathbf{x}\right\|^{2}-\frac{1}{2\nu_{i}^{2}}\left\| \mathbf{x}-\boldsymbol{\mu}_{i}\right\|^{2}\right)\] \[=\left(\frac{\nu_{0}^{2}}{\nu_{i}^{2}}\right)^{d/2}\exp\left( \left(\frac{1}{2\nu_{0}^{2}}-\frac{1}{2\nu_{i}^{2}}\right)\left\|\mathbf{N} \mathbf{n}\right\|^{2}+\left(\frac{\left\|\mathbf{R}\mathbf{r}\right\|^{2}}{2\nu _{0}^{2}}-\frac{\left\|\mathbf{R}\mathbf{r}-\boldsymbol{\mu}_{i}\right\|^{2}}{2 \nu_{i}^{2}}\right)\right)\] \[=\left(\frac{\nu_{0}^{2}}{\nu_{i}^{2}}\right)^{d/2}\exp\left(\left( \frac{1}{2\nu_{0}^{2}}-\frac{1}{2\nu_{i}^{2}}\right)\left\|\mathbf{n}\right\|^{2 }+\left(\frac{\left\|\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{\left\|\mathbf{r }-\mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\right)\right),\]

where the last step follows from the definition that \(\mathbf{R}\in\mathbb{R}^{d\times r}\) an orthonormal basis of the vector space \(\left\{\boldsymbol{\mu}_{i}\right\}_{i\in[k]}\) and \(\mathbf{N}^{T}\mathbf{N}=\boldsymbol{I}_{n}\). Since \(\nu_{0}^{2}>\nu_{i}^{2}\), the quadratic term \(\frac{\left\|\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{\left\|\mathbf{r}- \mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\) is maximized at \(\mathbf{r}=\frac{\nu_{0}^{2}\mathbf{R}^{T}\boldsymbol{\mu}_{i}}{\nu_{0}^{2}-\nu_{i} ^{2}}\). Therefore,

\[\frac{\left\|\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{\left\|\mathbf{r}- \mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\leq\frac{\nu_{0} ^{4}\left\|\mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{0}^{2}(\nu_{0 }^{2}-\nu_{i}^{2})^{2}}-\frac{1}{2\nu_{i}^{2}}\left(\frac{\nu_{0}^{2}}{\nu_{0 }^{2}-\nu_{i}^{2}}-1\right)^{2}\left\|\mathbf{R}^{T}\boldsymbol{\mu}_{i} \right\|^{2}=\frac{\left\|\boldsymbol{\mu}_{i}\right\|^{2}}{2(\nu_{0}^{2}-\nu _{i}^{2})}.\]

Hence, for \(\left\|\mathbf{n}\right\|^{2}\geq\frac{\nu_{0}^{2}+\nu_{0}^{2}}{2}d\) and \(\left\|\boldsymbol{\mu}_{i}\right\|^{2}\leq\frac{\nu_{0}^{2}-\nu_{i}^{2}}{2 \nu_{0}^{2}}\left(\log\left(\frac{\nu_{0}^{2}}{\nu_{0}^{2}}\right)-\frac{\nu_ {i}^{2}}{2\nu_{0}^{2}}+\frac{\nu_{0}^{2}}{2\nu_{i}^{2}}\right)d\), we have

\[\rho_{i}(\mathbf{x}) =\left(\frac{\nu_{0}^{2}}{\nu_{i}^{2}}\right)^{d/2}\exp\left( \left(\frac{1}{2\nu_{0}^{2}}-\frac{1}{2\nu_{i}^{2}}\right)\left\|\mathbf{n} \right\|^{2}+\left(\frac{\left\|\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{ \left\|\mathbf{r}-\mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2 }}\right)\right)\] \[\leq\left(\frac{\nu_{0}^{2}}{\nu_{i}^{2}}\right)^{d/2}\exp\left( \left(\frac{1}{2\nu_{0}^{2}}-\frac{1}{2\nu_{i}^{2}}\right)\frac{\nu_{0}^{2}+ \nu_{i}^{2}}{2}d+\frac{\left\|\boldsymbol{\mu}_{i}\right\|^{2}}{2(\nu_{0}^{2} -\nu_{i}^{2})}\right)\] \[=\exp\left(-\left(\log\left(\frac{\nu_{i}^{2}}{\nu_{0}^{2}} \right)-\frac{\nu_{i}^{2}}{2\nu_{0}^{2}}+\frac{\nu_{0}^{2}}{2\nu_{i}^{2}} \right)\frac{d}{2}+\frac{\left\|\boldsymbol{\mu}_{i}\right\|^{2}}{2(\nu_{0}^{ 2}-\nu_{i}^{2})}\right)\] \[\leq\exp\left(-\left(\log\left(\frac{\nu_{i}^{2}}{\nu_{0}^{2}} \right)-\frac{\nu_{i}^{2}}{2\nu_{0}^{2}}+\frac{\nu_{0}^{2}}{2\nu_{i}^{2}} \right)\frac{d}{4}\right).\]

Notice that for function \(f(z)=\log z-\frac{z}{2}+\frac{1}{2z}\), we have \(f(1)=0\) and \(\frac{\mathrm{d}}{\mathrm{d}z}f(z)=\frac{1}{z}-\frac{1}{2}-\frac{1}{2z^{2}}=- \frac{1}{2}\left(\frac{1}{z}-1\right)^{2}<0\) when \(z\in(0,1)\). Thus, \(\log\left(\frac{\nu_{i}^{2}}{\nu_{0}^{2}}\right)-\frac{\nu_{i}^{2}}{2\nu_{0}^ {2}}+\frac{\nu_{0}^{2}}{2\nu_{i}^{2}}\) is a positive constant for \(\nu_{i}<\nu_{0}\), i.e., \(\rho_{i}(\mathbf{x})=\exp(-\Omega(d))\). Therefore we finish the proof of Lemma 4. 

Lemma 4 implies that when \(\left\|\mathbf{n}\right\|\) is large, the Gaussian mode \(P^{(0)}\) dominates other modes \(P^{(i)}\). To bound \(\left\|\mathbf{n}_{t}\right\|\), we first consider a simpler case that \(\left\|\mathbf{n}_{t-1}\right\|\) is large. Intuitively, the following lemma proves that when the previous state \(\mathbf{n}_{t-1}\) is far from a mode, a single step of Langevin dynamics with bounded step size is not enough to find the mode.

**Lemma 5**.: _Suppose \(\delta_{t}\leq\nu_{0}^{2}\) and \(\left\|\mathbf{n}_{t-1}\right\|^{2}>36\nu_{0}^{2}d\), then for \(\mathbf{n}_{t}\) following from equation 5, we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\nu_{0}^{2}d\) with probability at least \(1-\exp(-\Omega(d))\)._

Proof of Lemma 5.: From the recursion of \(\mathbf{n}_{t}\) in equation 5 we have

\[\mathbf{n}_{t} =\mathbf{n}_{t-1}+\frac{\delta_{t}}{2}\mathbf{N}^{T}\nabla_{ \mathbf{x}}\log P(\mathbf{x}_{t-1})+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t}^ {(\mathbf{n})}\] \[=\mathbf{n}_{t-1}-\frac{\delta_{t}}{2}\sum_{i=0}^{k}\frac{P^{(i)} (\mathbf{x}_{t-1})}{P(\mathbf{x}_{t-1})}\cdot\frac{\mathbf{N}^{T}(\mathbf{x}_{t -1}-\boldsymbol{\mu}_{i})}{\nu_{i}^{2}}+\sqrt{\delta_{t}}\boldsymbol{ \epsilon}_{t}^{(\mathbf{n})}\] \[=\left(1-\frac{\delta_{t}}{2}\sum_{i=0}^{k}\frac{P^{(i)}( \mathbf{x}_{t-1})}{P(\mathbf{x}_{t-1})}\cdot\frac{1}{\nu_{i}^{2}}\right) \mathbf{n}_{t-1}+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}.\] (6)

By Lemma 4, we have \(\frac{P^{(i)}(\mathbf{x}_{t-1})}{P^{(i)}(\mathbf{x}_{t-1})}\leq\exp(-\Omega(d))\) for all \(i\in[k]\), therefore

\[1-\frac{\delta_{t}}{2}\sum_{i=0}^{k}\frac{P^{(i)}(\mathbf{x}_{t-1})}{P( \mathbf{x}_{t-1})}\cdot\frac{1}{\nu_{i}^{2}}\geq 1-\frac{\delta_{t}}{2}\cdot\frac{1}{\nu_{0}^{2}}- \frac{\delta_{t}}{2}\sum_{i\in[k]}\frac{w_{i}P^{(i)}(\mathbf{x}_{t-1})}{w_{0}P^{ (0)}(\mathbf{x}_{t-1})}\cdot\frac{1}{\nu_{i}^{2}}\geq 1-\frac{1}{2}-\exp(-\Omega(d))> \frac{1}{3}.\] (7)

On the other hand, from \(\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}\sim\mathcal{N}(\mathbf{0}_{n}, \boldsymbol{I}_{n})\) we know \(\frac{\left\langle\mathbf{n}_{t-1},\boldsymbol{\epsilon}_{t}^{(\mathbf{n})} \right\rangle}{\left\|\mathbf{n}_{t-1}\right\|}\sim\mathcal{N}(0,1)\) for any fixed \(\mathbf{n}_{t-1}\neq\mathbf{0}_{n}\), hence by Lemma 2 we have

\[\mathbb{P}\left(\frac{\left\langle\mathbf{n}_{t-1},\boldsymbol{\epsilon}_{t}^{( \mathbf{n})}\right\rangle}{\left\|\mathbf{n}_{t-1}\right\|}\geq\frac{\sqrt{d}}{4 }\right)=\mathbb{P}\left(\frac{\left\langle\mathbf{n}_{t-1},\boldsymbol{\epsilon}_{t}^ {(\mathbf{n})}\right\rangle}{\left\|\mathbf{n}_{t-1}\right\|}\leq-\frac{\sqrt{d}}{4 }\right)\leq\frac{4}{\sqrt{2\pi d}}\exp\left(-\frac{d}{32}\right)\] (8)Combining equation 6, equation 7 and equation 8 gives that

\[\left\|\mathbf{n}_{t}\right\|^{2} \geq\left(\frac{1}{3}\right)^{2}\left\|\mathbf{n}_{t-1}\right\|^{2 }-2\nu_{0}|\langle\mathbf{n}_{t-1},\boldsymbol{\epsilon}_{t}^{(\mathbf{n})}\rangle|\] \[\geq\frac{1}{9}\left\|\mathbf{n}_{t-1}\right\|^{2}-\frac{\nu_{0} \sqrt{d}}{2}\left\|\mathbf{n}_{t-1}\right\|\] \[\geq\frac{1}{9}\cdot 36\nu_{0}^{2}d-\frac{\nu_{0}\sqrt{d}}{2} \cdot 6\nu_{0}\sqrt{d}\] \[=\nu_{0}^{2}d\]

with probability at least \(1-\frac{8}{\sqrt{2\pi d}}\exp\left(-\frac{d}{32}\right)=1-\exp(-\Omega(d))\). This proves Lemma 5. 

We then proceed to bound \(\left\|\mathbf{n}_{t}\right\|\) iteratively for \(\left\|\mathbf{n}_{t-1}\right\|^{2}\leq 36\nu_{0}^{2}d\). Recall that equation 5 gives

\[\mathbf{n}_{t}=\mathbf{n}_{t-1}+\frac{\delta_{t}}{2}\mathbf{N}^{T}\nabla_{ \mathbf{x}}\log P(\mathbf{x}_{t-1})+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{ t}^{(\mathbf{n})}.\]

We notice that the difficulty of solving \(\mathbf{n}_{t}\) exhibits in the dependence of \(\log P(\mathbf{x}_{t-1})\) on \(\mathbf{r}_{t-1}\). Since \(P=\sum_{i=0}^{k}w_{i}P^{(i)}=\sum_{i=0}^{k}w_{i}\mathcal{N}(\boldsymbol{\mu} _{i},\nu_{i}^{2}\boldsymbol{I}_{d})\), we can rewrite the score function as

\[\nabla_{\mathbf{x}}\log P(\mathbf{x})=\frac{\nabla_{\mathbf{x}}P(\mathbf{x})} {P(\mathbf{x})}=-\sum_{i=0}^{k}\frac{P^{(i)}(\mathbf{x})}{P(\mathbf{x})}\cdot \frac{\mathbf{x}-\boldsymbol{\mu}_{i}}{\nu_{i}^{2}}=-\frac{\mathbf{x}}{\nu_{0 }^{2}}+\sum_{i\in[k]}\frac{P^{(i)}(\mathbf{x})}{P(\mathbf{x})}\left(\frac{ \mathbf{x}}{\nu_{0}^{2}}-\frac{\mathbf{x}-\boldsymbol{\mu}_{i}}{\nu_{i}^{2}} \right).\] (9)

Now, instead of directly working with \(\mathbf{n}_{t}\), we consider a surrogate recursion \(\hat{\mathbf{n}}_{t}\) such that \(\hat{\mathbf{n}}_{0}=\mathbf{n}_{0}\) and for all \(t\geq 1\),

\[\hat{\mathbf{n}}_{t}=\hat{\mathbf{n}}_{t-1}-\frac{\delta_{t}}{2\nu_{0}^{2}} \hat{\mathbf{n}}_{t-1}+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t}^{(\mathbf{n })}.\] (10)

The advantage of the surrogate recursion is that \(\hat{\mathbf{n}}_{t}\) is independent of \(\mathbf{r}\), thus we can obtain the closed-form solution to \(\hat{\mathbf{n}}_{t}\). Before we proceed to bound \(\hat{\mathbf{n}}_{t}\), we first show that \(\hat{\mathbf{n}}_{t}\) is sufficiently close to the original recursion \(\mathbf{n}_{t}\) in the following lemma.

**Lemma 6**.: _For any \(t\geq 1\), given that \(\delta_{j}\leq\nu_{0}^{2}\) and \(\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d\leq\left\|\mathbf{n}_{j-1}\right\|^{2} \leq 36\nu_{0}^{2}d\) for all \(j\in[t]\) and \(\left\|\boldsymbol{\mu}_{i}\right\|^{2}\leq\frac{\nu_{0}^{2}-\nu_{i}^{2}}{2 \nu_{0}^{2}}\left(\log\left(\frac{\nu_{i}^{2}}{\nu_{0}^{2}}\right)-\frac{\nu_ {i}^{2}}{2\nu_{0}^{2}}+\frac{\nu_{0}^{2}}{2\nu_{i}^{2}}\right)d\) for all \(i\in[k]\), we have \(\left\|\hat{\mathbf{n}}_{t}-\mathbf{n}_{t}\right\|\leq\frac{t}{\exp(\mathbb{ I}(d))}\sqrt{d}\)._

Proof of Lemma 6.: Upon comparing equation 5 and equation 10, by equation 9 we have that for all \(j\in[t]\),

\[\left\|\hat{\mathbf{n}}_{j}-\mathbf{n}_{j}\right\| =\left\|\hat{\mathbf{n}}_{j-1}-\frac{\delta_{j}}{2\nu_{0}^{2}} \hat{\mathbf{n}}_{j-1}-\mathbf{n}_{j-1}-\frac{\delta_{j}}{2}\mathbf{N}^{T} \nabla_{\mathbf{x}}\log P(\mathbf{x}_{j-1})\right\|\] \[=\left\|\left(1-\frac{\delta_{j}}{2\nu_{0}^{2}}\right)(\hat{ \mathbf{n}}_{j-1}-\mathbf{n}_{j-1})+\frac{\delta_{j}}{2}\sum_{i\in[k]}\frac{P ^{(i)}(\mathbf{x}_{j-1})}{P(\mathbf{x}_{j-1})}\left(\frac{1}{\nu_{i}^{2}}- \frac{1}{\nu_{0}^{2}}\right)\mathbf{n}_{j-1}\right\|\] \[\leq\left(1-\frac{\delta_{j}}{2\nu_{0}^{2}}\right)\left\|\hat{ \mathbf{n}}_{j-1}-\mathbf{n}_{j-1}\right\|+\sum_{i\in[k]}\frac{\delta_{j}}{2 }\frac{P^{(i)}(\mathbf{x}_{j-1})}{P(\mathbf{x}_{j-1})}\left(\frac{1}{\nu_{i}^{ 2}}-\frac{1}{\nu_{0}^{2}}\right)\left\|\mathbf{n}_{j-1}\right\|\] \[\leq\left\|\hat{\mathbf{n}}_{j-1}-\mathbf{n}_{j-1}\right\|+\sum_ {i\in[k]}\frac{\delta_{j}}{2}\frac{P^{(i)}(\mathbf{x}_{j-1})}{P^{(0)}( \mathbf{x}_{j-1})}\left(\frac{1}{\nu_{i}^{2}}-\frac{1}{\nu_{0}^{2}}\right)6 \nu_{0}\sqrt{d}.\]

By Lemma 4, we have \(\frac{P^{(i)}(\mathbf{x}_{j-1})}{P^{(0)}(\mathbf{x}_{j-1})}\leq\exp(-\Omega(d))\) for all \(i\in[k]\), hence we obtain a recursive bound

\[\left\|\hat{\mathbf{n}}_{j}-\mathbf{n}_{j}\right\|\leq\left\|\hat{\mathbf{n}}_{j -1}-\mathbf{n}_{j-1}\right\|+\frac{1}{\exp(\Omega(d))}\sqrt{d}.\]Finally, by \(\hat{\mathbf{n}}_{0}=\mathbf{n}_{0}\), we have

\[\left\|\hat{\mathbf{n}}_{t}-\mathbf{n}_{t}\right\|=\sum_{j\in[t]}\left(\left\| \hat{\mathbf{n}}_{j}-\mathbf{n}_{j}\right\|-\left\|\hat{\mathbf{n}}_{j-1}- \mathbf{n}_{j-1}\right\|\right)\leq\frac{t}{\exp(\Omega(d))}\sqrt{d}.\]

Hence we obtain Lemma 6. 

We then proceed to analyze \(\hat{\mathbf{n}}_{t}\), The following lemma gives us the closed-form solution of \(\hat{\mathbf{n}}_{t}\). We slightly abuse the notations here, e.g., \(\prod_{i=c_{1}}^{c_{2}}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)=1\) and \(\sum_{j=c_{1}}^{c_{2}}\delta_{j}=0\) for \(c_{1}>c_{2}\).

**Lemma 7**.: _For all \(t\geq 0\), \(\hat{\mathbf{n}}_{t}\sim\mathcal{N}\left(\prod_{i=1}^{t}\left(1-\frac{\delta_ {i}}{2\nu_{0}^{2}}\right)\mathbf{n}_{0},\;\sum_{j=1}^{t}\prod_{i=j+1}^{t} \left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)^{2}\delta_{j}\bm{I}_{n}\right)\), where the mean and covariance satisfy \(\prod_{i=1}^{t}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)^{2}+\frac{1}{\nu _{0}^{2}}\sum_{j=1}^{t}\prod_{i=j+1}^{t}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2} }\right)^{2}\delta_{j}\geq 1\)._

Proof of Lemma 7.: We prove the two properties by induction. When \(t=0\), they are trivial. Suppose they hold for \(t-1\), then for the distribution of \(\hat{\mathbf{n}}_{t}\), we have

\[\hat{\mathbf{n}}_{t} =\hat{\mathbf{n}}_{t-1}-\frac{\delta_{t}}{2\nu_{0}^{2}}\hat{ \mathbf{n}}_{t-1}+\sqrt{\delta_{t}}\bm{\epsilon}_{t}^{(\mathbf{n})}\] \[\sim\mathcal{N}\left(\left(1-\frac{\delta_{t}}{2\nu_{0}^{2}} \right)\prod_{i=1}^{t-1}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)\mathbf{ n}_{0},\;\left(1-\frac{\delta_{t}}{2\nu_{0}^{2}}\right)^{2}\sum_{j=1}^{t-1} \prod_{i=j+1}^{t-1}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)^{2}\delta_{j }\bm{I}_{n}+\delta_{t}\bm{I}_{n}\right)\] \[=\mathcal{N}\left(\prod_{i=1}^{t}\left(1-\frac{\delta_{i}}{2\nu_ {0}^{2}}\right)\mathbf{n}_{0},\;\sum_{j=1}^{t}\prod_{i=j+1}^{t}\left(1-\frac{ \delta_{i}}{2\nu_{0}^{2}}\right)^{2}\delta_{j}\bm{I}_{n}\right).\]

For the second property,

\[\prod_{i=1}^{t}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)^{2} +\frac{1}{\nu_{0}^{2}}\sum_{j=1}^{t}\prod_{i=j+1}^{t}\left(1-\frac{\delta_{i}} {2\nu_{0}^{2}}\right)^{2}\delta_{j}\] \[=\left(1-\frac{\delta_{t}}{2\nu_{0}^{2}}\right)^{2}\left(\prod_{i= 1}^{t-1}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)^{2}+\frac{1}{\nu_{0}^{ 2}}\sum_{j=1}^{t-1}\prod_{i=j+1}^{t-1}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}} \right)^{2}\delta_{j}\right)+\frac{1}{\nu_{0}^{2}}\delta_{t}\] \[\geq\left(1-\frac{\delta_{t}}{2\nu_{0}^{2}}\right)^{2}+\frac{1}{ \nu_{0}^{2}}\delta_{t}=1+\frac{\delta_{t}^{2}}{4\nu_{0}^{4}}\geq 1.\]

Hence we finish the proof of Lemma 7. 

Armed with Lemma 7, we are now ready to establish the lower bound on \(\left\|\hat{\mathbf{n}}_{t}\right\|\). For simplicity, denote \(\alpha:=\prod_{i=1}^{t}\left(1-\frac{\delta_{i}}{2\nu_{0}^{2}}\right)^{2}\) and \(\beta:=\frac{1}{\nu_{0}^{2}}\sum_{j=1}^{t}\prod_{i=j+1}^{t}\left(1-\frac{ \delta_{i}}{2\nu_{0}^{2}}\right)^{2}\delta_{j}\). By Lemma 7 we know \(\hat{\mathbf{n}}_{t}\sim\mathcal{N}(\alpha\mathbf{n}_{0},\beta\nu_{0}^{2}\bm{I}_ {n})\), so we can write \(\hat{\mathbf{n}}_{t}=\alpha\mathbf{n}_{0}+\sqrt{\beta}\nu_{0}\bm{\epsilon}\), where \(\bm{\epsilon}\sim\mathcal{N}(\mathbf{0}_{n},\bm{I}_{n})\).

**Lemma 8**.: _Given that \(\left\|\hat{\mathbf{n}}_{0}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}\), we have \(\left\|\hat{\mathbf{n}}_{t}\right\|^{2}\geq\frac{5\nu_{0}^{2}+3\nu_{\max}^{2}}{8}d\) with probability at least \(1-\exp\left(-\Omega(d)\right)\)._

Proof of Lemma 8.: By \(\hat{\mathbf{n}}_{t}=\alpha\mathbf{n}_{0}+\sqrt{\beta}\nu_{0}\bm{\epsilon}\) we have

\[\left\|\hat{\mathbf{n}}_{t}\right\|^{2}=\alpha^{2}\left\|\mathbf{n}_{0}\right\|^ {2}+\beta\nu_{0}^{2}\left\|\bm{\epsilon}\right\|^{2}+2\alpha\sqrt{\beta}\nu_{0} \langle\mathbf{n}_{0},\bm{\epsilon}\rangle\]By Lemma 1 we can bound

\[\mathbb{P}\left(\left\|\boldsymbol{\epsilon}\right\|^{2}\leq\frac{3 \nu_{0}^{2}+\nu_{\max}^{2}}{4\nu_{0}^{2}}d\right) =\mathbb{P}\left(\left\|\boldsymbol{\epsilon}\right\|^{2}\leq d-2 \sqrt{d\cdot\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^{2}}\right)^{2}d}\right)\] \[\leq\mathbb{P}\left(\left\|\boldsymbol{\epsilon}\right\|^{2}\leq( n-1)-2\sqrt{(n-1)\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^{2}}\right)^{2} \frac{d}{2}}\right)\] \[\leq\exp\left(-\left(\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{8\nu_{0}^ {2}}\right)^{2}\frac{d}{2}\right),\]

where the second last step follows from the assumption \(d-n=r=o(d)\). Since \(\boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0}_{n},\boldsymbol{I}_{n})\), we know \(\frac{\langle\mathbf{n}_{0},\boldsymbol{\epsilon}\rangle}{\left\|\mathbf{n}_{ 0}\right\|}\sim\mathcal{N}(0,1)\). Therefore by Lemma 2,

\[\mathbb{P}\left(\frac{\langle\mathbf{n}_{0},\boldsymbol{\epsilon}\rangle}{ \left\|\mathbf{n}_{0}\right\|}\leq-\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{4\nu_{0} \sqrt{3\nu_{0}^{2}+\nu_{\max}^{2}}}\sqrt{d}\right)\leq\frac{4\nu_{0}\sqrt{3 \nu_{0}^{2}+\nu_{\max}^{2}}}{\sqrt{2\pi}(\nu_{0}^{2}-\nu_{\max}^{2})\sqrt{d}} \exp\left(-\frac{(\nu_{0}^{2}-\nu_{\max}^{2})^{2}d}{32\nu_{0}^{2}(3\nu_{0}^{2 }+\nu_{\max}^{2})}\right)\]

Conditioned on \(\left\|\hat{\mathbf{n}}_{0}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{ 4}d\), \(\left\|\boldsymbol{\epsilon}\right\|^{2}>\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4 \nu_{0}^{2}}d\) and \(\frac{1}{\left\|\mathbf{n}_{0}\right\|}\langle\mathbf{n}_{0},\boldsymbol{ \epsilon}\rangle>-\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{4\nu_{0}\sqrt{3\nu_{0}^{2} +\nu_{\max}^{2}}}\sqrt{d}\), since Lemma 7 gives \(\alpha^{2}+\beta\geq 1\) we have

\[\left\|\hat{\mathbf{n}}_{t}\right\|^{2} =\alpha^{2}\left\|\mathbf{n}_{0}\right\|^{2}+\beta\nu_{0}^{2} \left\|\boldsymbol{\epsilon}\right\|^{2}+2\alpha\sqrt{\beta}\nu_{0}\langle \mathbf{n}_{0},\boldsymbol{\epsilon}\rangle\] \[\geq\alpha^{2}\left\|\mathbf{n}_{0}\right\|^{2}+\beta\nu_{0}^{2} \left\|\boldsymbol{\epsilon}\right\|^{2}-2\alpha\sqrt{\beta}\nu_{0}\left\| \mathbf{n}_{0}\right\|\frac{\nu_{0}^{2}-\nu_{\max}^{2}}{4\nu_{0}\sqrt{3\nu_{ 0}^{2}+\nu_{\max}^{2}}}\sqrt{d}\] \[\geq\alpha^{2}\left\|\mathbf{n}_{0}\right\|^{2}+\beta\nu_{0}^{2} \left\|\boldsymbol{\epsilon}\right\|^{2}-2\alpha\sqrt{\beta}\nu_{0}\left\| \mathbf{n}_{0}\right\|\left\|\boldsymbol{\epsilon}\right\|\cdot\frac{\nu_{0}^ {2}-\nu_{\max}^{2}}{6\nu_{0}^{2}+2\nu_{\max}^{2}}\] \[\geq\frac{5\nu_{0}^{2}+3\nu_{\max}^{2}}{6\nu_{0}^{2}+2\nu_{\max} ^{2}}\left(\alpha^{2}+\beta\right)\cdot\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}d\] \[\geq\frac{5\nu_{0}^{2}+3\nu_{\max}^{2}}{8}d.\]

Hence by union bound, we complete the proof of Lemma 8. 

Upon having all the above lemmas, we are now ready to establish Proposition 3 by induction. Suppose the theorem holds for all \(T\) values of \(1,\cdots,T-1\). We consider the following 3 cases:

* If there exists some \(t\in[T]\) such that \(\delta_{t}>\nu_{0}^{2}\), by Lemma 3 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}}{4}d\), thus the problem reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}\) for all \(t\in[T]\). If there exists some \(t\in[T]\) such that \(\left\|\mathbf{n}_{t-1}\right\|^{2}>36\nu_{0}^{2}d\), by Lemma 5 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\nu_{0}^{2}d>\frac{3\nu_{0}^{2}+\nu_{\max} ^{2}}{4}d\), thus the problem similarly reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}\) and \(\left\|\mathbf{n}_{t-1}\right\|^{2}\leq 36\nu_{0}^{2}d\) for all \(t\in[T]\). Conditioned on \(\left\|\mathbf{n}_{t-1}\right\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d\) for all \(t\in[T]\), by Lemma 6 we have that for \(T=\exp(\mathcal{O}(d))\), \[\left\|\hat{\mathbf{n}}_{T}-\mathbf{n}_{T}\right\|<\left(\sqrt{\frac{5\nu_{0}^{2 }+3\nu_{\max}^{2}}{8}}-\sqrt{\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}}\right)\sqrt{d}.\] By Lemma 8 we have that with probability at least \(1-\exp(-\Omega(d))\), \[\left\|\hat{\mathbf{n}}_{T}\right\|^{2}\geq\frac{5\nu_{0}^{2}+3\nu_{\max}^{2}}{8 }d.\]Combining the two inequalities implies the desired bound \[\|\mathbf{n}_{T}\|\geq\|\hat{\mathbf{n}}_{T}\|-\|\hat{\mathbf{n}}_{T}-\mathbf{n}_ {T}\|>\sqrt{\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}}d.\] Hence by induction we obtain \(\|\mathbf{n}_{t}\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}}{2}d\) for all \(t\in[T]\) with probability at least \[(1-(T-1)\exp(-\Omega(d)))\cdot(1-\exp(-\Omega(d)))\geq 1-T\exp(-\Omega(d)).\]

Therefore we complete the proof of Proposition 3. 

Finally, combining Propositions 2 and 3 finishes the proof of Theorem 1.

### Proof of Theorem 2: Annealed Langevin Dynamics under Gaussian Mixtures

To establish Theorem 2, we first note from Proposition 1 that perturbing a Gaussian distribution \(\mathcal{N}(\boldsymbol{\mu},\nu^{2}\boldsymbol{I}_{d})\) with noise level \(\sigma\) results in a Gaussian distribution \(\mathcal{N}(\boldsymbol{\mu},(\nu^{2}+\sigma^{2})\boldsymbol{I}_{d})\). Therefore, for a Gaussian mixture \(P=\sum_{i=0}^{k}w_{i}P^{(i)}=\sum_{i=0}^{k}w_{i}\mathcal{N}(\boldsymbol{\mu} _{i},\nu_{i}^{2}\boldsymbol{I}_{d})\), the perturbed distribution of noise level \(\sigma\) is

\[P_{\sigma}=\sum_{i=0}^{k}w_{i}\mathcal{N}(\boldsymbol{\mu}_{i},(\nu_{i}^{2}+ \sigma^{2})\boldsymbol{I}_{d}).\]

Similar to the proof of Theorem 1, we decompose

\[\mathbf{x}_{t}=\mathbf{R}\mathbf{r}_{t}+\mathbf{N}\mathbf{n}_{t},\text{ and }\boldsymbol{\epsilon}_{t}=\mathbf{R}\boldsymbol{\epsilon}_{t}^{(\mathbf{r})}+ \mathbf{N}\boldsymbol{\epsilon}_{t}^{(\mathbf{n})},\]

where \(\mathbf{R}\in\mathbb{R}^{d\times r}\) an orthonormal basis of the vector space \(\left\{\boldsymbol{\mu}_{i}\right\}_{i\in[k]}\) and \(\mathbf{N}\in\mathbb{R}^{d\times n}\) an orthonormal basis of the null space of \(\left\{\boldsymbol{\mu}_{i}\right\}_{i\in[k]}\). Now, we prove Theorem 2 by applying the techniques developed in Appendix A.1 via substituting \(\nu^{2}\) with \(\nu^{2}+\sigma_{t}^{2}\) at time step \(t\).

First, by Proposition 2, suppose that the sample is initialized in the distribution \(P_{\sigma_{0}}^{(0)}\), then with probability at least \(1-\exp(-\Omega(d))\), we have

\[\|\mathbf{n}_{0}\|^{2}\geq\frac{3(\nu_{0}^{2}+\sigma_{0}^{2})+(\nu_{\max}^{2} +\sigma_{0}^{2})}{4}d=\frac{3\nu_{0}^{2}+\nu_{\max}^{2}+4\sigma_{0}^{2}}{4}d.\] (11)

Then, with the assumption that the initialization satisfies \(\|\mathbf{n}_{0}\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}+4\sigma_{0}^{2}}{4}d\), the following proposition similar to Proposition 3 shows that \(\|\mathbf{n}_{t}\|\) remains large with high probability.

**Proposition 4**.: _Consider a data distribution \(P\) satisfies the constraints specified in Theorem 2. We follow annealed Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps with noise level \(c_{\sigma}\geq\sigma_{0}\geq\sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{T}\geq 0\) for some constant \(c_{\sigma}>0\). Suppose that the initial sample satisfies \(\|\mathbf{n}_{0}\|^{2}\geq\frac{3\nu_{0}^{2}+\nu_{\max}^{2}+4\sigma_{0}^{2}}{4}d\), then with probability at least \(1-T\cdot\exp(-\Omega(d))\), we have that \(\|\mathbf{n}_{t}\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}+2\sigma_{t}^{2}}{2}d\) for all \(t\in\{0\}\cup[T]\)._

Proof of Proposition 4.: We prove Proposition 4 by induction. Suppose the theorem holds for all \(T\) values of \(1,\cdots,T-1\). We consider the following 3 cases:

* If there exists some \(t\in[T]\) such that \(\delta_{t}>\nu_{0}^{2}+\sigma_{t}^{2}\), by Lemma 3 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\|\mathbf{n}_{t}\|^{2}\geq\frac{3(\nu_{0}^{2}+\sigma_{t}^{2})+(\nu_{\max}^{2} +\sigma_{t}^{2})}{4}d=\frac{3\nu_{0}^{2}+\nu_{\max}^{2}+4\sigma_{t}^{2}}{4}d\), thus the problem reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}+\sigma_{t}^{2}\) for all \(t\in[T]\). If there exists some \(t\in[T]\) such that \(\|\mathbf{n}_{t-1}\|^{2}>36(\nu_{0}^{2}+\sigma_{t-1}^{2})d\geq 36(\nu_{0}^{2}+ \sigma_{t}^{2})d\), by Lemma 5 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\|\mathbf{n}_{t}\|^{2}\geq(\nu_{0}^{2}+\sigma_{t}^{2})d>\frac{3\nu_{0}^{2}+\nu_ {\max}^{2}+4\sigma_{t}^{2}}{4}d\), thus the problem similarly reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.

* Suppose \(\delta_{t}\leq\nu_{0}^{2}+\sigma_{t}^{2}\) and \(\|\mathbf{n}_{t-1}\|^{2}\leq 36(\nu_{0}^{2}+\sigma_{t-1}^{2})d\) for all \(t\in[T]\). Consider a surrogate sequence \(\hat{\mathbf{n}}_{t}\) such that \(\hat{\mathbf{n}}_{0}=\mathbf{n}_{0}\) and for all \(t\geq 1\), \[\hat{\mathbf{n}}_{t}=\hat{\mathbf{n}}_{t-1}-\frac{\delta_{t}}{2\nu_{0}^{2}+2 \sigma_{t}^{2}}\hat{\mathbf{n}}_{t-1}+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{ t}^{(\mathbf{n})}.\] Since \(\nu_{0}>\nu_{i}\) and \(c_{\sigma}\geq\sigma_{t}\) for all \(t\in\{0\}\cup[T]\), we have \(\frac{\nu_{t}^{2}+c_{\sigma}^{2}}{\nu_{0}^{2}+c_{\sigma}^{2}}\geq\frac{\nu_{t }^{2}+\sigma_{t}^{2}}{\nu_{0}^{2}+\sigma_{t}^{2}}\). Notice that for function \(f(z)=\log z-\frac{z}{2}+\frac{1}{2z}\), we have \(\frac{\mathrm{d}}{\mathrm{d}z}f(z)=\frac{1}{z}-\frac{1}{2}-\frac{1}{2z^{2}}=- \frac{1}{2}\left(\frac{1}{z}-1\right)^{2}\leq 0\). Thus, by the assumption \[\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\|^{2}\leq\frac{\nu_{0}^{2}-\nu_{i }^{2}}{2}\left(\log\left(\frac{\nu_{t}^{2}+c_{\sigma}^{2}}{\nu_{0}^{2}+c_{ \sigma}^{2}}\right)-\frac{\nu_{i}^{2}+c_{\sigma}^{2}}{2\nu_{0}^{2}+c_{\sigma}^ {2}}+\frac{\nu_{0}^{2}+c_{\sigma}^{2}}{2\nu_{i}^{2}+c_{\sigma}^{2}}\right)d,\] we have that for all \(t\in[T]\), \[\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\|^{2}\leq\frac{\nu_{0}^{2}-\nu_{i }^{2}}{2}\left(\log\left(\frac{\nu_{i}^{2}+\sigma_{t}^{2}}{\nu_{0}^{2}+\sigma _{t}^{2}}\right)-\frac{\nu_{i}^{2}+\sigma_{t}^{2}}{2\nu_{0}^{2}+\sigma_{t}^{2 }}+\frac{\nu_{0}^{2}+\sigma_{t}^{2}}{2\nu_{i}^{2}+\sigma_{t}^{2}}\right)d.\] Conditioned on \(\|\mathbf{n}_{t-1}\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}+2\sigma_{t-1}^{2}}{ 2}d\) for all \(t\in[T]\), by Lemma 6 we have that for \(T=\exp(\mathcal{O}(d))\), \[\|\hat{\mathbf{n}}_{T}-\mathbf{n}_{T}\|<\left(\sqrt{\frac{5\nu_{0}^{2}+3\nu_{ \max}^{2}+8\sigma_{T}^{2}}{8}}-\sqrt{\frac{\nu_{0}^{2}+\nu_{\max}^{2}+2\sigma_ {T}^{2}}{2}}\right)\sqrt{d}.\] By Lemma 8 we have that with probability at least \[\|\hat{\mathbf{n}}_{T}\|^{2}\geq\frac{5\nu_{0}^{2}+3\nu_{\max}^{2}+8\sigma_{T }^{2}}{8}d.\] Combining the two inequalities implies the desired bound \[\|\mathbf{n}_{T}\|\geq\|\hat{\mathbf{n}}_{T}\|-\|\hat{\mathbf{n}}_{T}-\mathbf{ n}_{T}\|>\sqrt{\frac{\nu_{0}^{2}+\nu_{\max}^{2}+2\sigma_{T}^{2}}{2}}d.\] Hence by induction we obtain \(\|\mathbf{n}_{t}\|^{2}>\frac{\nu_{0}^{2}+\nu_{\max}^{2}+2\sigma_{t}^{2}}{2}d\) for all \(t\in\{0\}\cup[T]\) with probability at least \[(1-(T-1)\exp(-\Omega(d)))\cdot(1-\exp(-\Omega(d)))\geq 1-T\exp(-\Omega(d)).\] Therefore we complete the proof of Proposition 4. 

Finally, combining equation 11 and Proposition 4 finishes the proof of Theorem 2.

### Proof of Theorem 3: Langevin Dynamics under Sub-Gaussian Mixtures

The proof framework is similar to the proof of Theorem 1. To begin with, we validate Assumption 2.v. in the following lemma:

**Lemma 9**.: _For constants \(\nu_{0},\nu_{i},c_{\nu},c_{L}\) satisfying Assumptions 2.iii. and 2.iv., we have \(\frac{(1-c_{\nu})\nu_{0}^{2}-\nu_{i}^{2}}{2(1-c_{\nu})}>0\) and \(\log\frac{c_{\nu}\nu_{i}^{2}}{(c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}-\frac{\nu_{ i}^{2}}{2(1-c_{\nu})\nu_{0}^{2}}+\frac{(1-c_{\nu})\nu_{0}^{2}}{2\nu_{i}^{2}}>0\) are both positive constants._

Proof of Lemma 9.: From Assumption 2.iv. that \(\nu_{0}^{2}>\frac{\nu_{\max}^{2}}{1-c_{\nu}}\geq\frac{\nu_{i}^{2}}{1-c_{\nu}}\), we easily obtain \(\frac{(1-c_{\nu})\nu_{0}^{2}-\nu_{i}^{2}}{2(1-c_{\nu})}>0\) is a positive constant. For the second property, let \(f(z):=\log\frac{c_{\nu}\nu_{i}^{2}}{(c_{L}^{2}+c_{\nu}c_{L})z}-\frac{\nu_{i}^{2 }}{2(1-c_{\nu})z}+\frac{(1-c_{\nu})z}{2\nu_{i}^{2}}\). For any \(z>\frac{\nu_{i}^{2}}{1-c_{\nu}}\), the derivative of \(f(z)\) satisfies

\[\frac{\mathrm{d}}{\mathrm{d}z}f(z)=-\frac{1}{z}+\frac{\nu_{i}^{2}}{2(1-c_{\nu })z^{2}}+\frac{1-c_{\nu}}{2\nu_{i}^{2}}=\frac{\nu_{i}^{2}}{2(1-c_{\nu})}\left( \frac{1-c_{\nu}}{\nu_{i}^{2}}-\frac{1}{z}\right)^{2}>0.\]Therefore, when \(\frac{4(c_{L}^{2}+c_{\nu}c_{L})}{c_{\nu}(1-c_{\nu})}\leq 1\), we have

\[f(\nu_{0}^{2})>f\left(\frac{\nu_{i}^{2}}{1-c_{\nu}}\right)=\log\frac{c_{\nu}(1-c _{\nu})}{c_{L}^{2}+c_{\nu}c_{L}}\geq\log 4>0.\]

When \(\frac{4(c_{L}^{2}+c_{\nu}c_{L})}{c_{\nu}(1-c_{\nu})}>1\), we have

\[f(\nu_{0}^{2}) >f\left(\frac{4(c_{L}^{2}+c_{\nu}c_{L})}{c_{\nu}(1-c_{\nu})}\frac {\nu_{i}^{2}}{1-c_{\nu}}\right)=2\log\frac{c_{\nu}(1-c_{\nu})}{2(c_{L}^{2}+c_{ \nu}c_{L})}-\frac{c_{\nu}(1-c_{\nu})}{8(c_{L}^{2}+c_{\nu}c_{L})}+\frac{2(c_{L}^ {2}+c_{\nu}c_{L})}{c_{\nu}(1-c_{\nu})}\] \[\geq 2-2\log 2-\frac{2(c_{L}^{2}+c_{\nu}c_{L})}{c_{\nu}(1-c_{\nu} )}-\frac{c_{\nu}(1-c_{\nu})}{8(c_{L}^{2}+c_{\nu}c_{L})}+\frac{2(c_{L}^{2}+c_{ \nu}c_{L})}{c_{\nu}(1-c_{\nu})}>2-2\log 2-\frac{1}{2}>0.\]

Thus we obtain Lemma 9. 

Without loss of generality, we assume \(\bm{\mu}_{0}=\bm{0}_{d}\). Similar to the proof of Theorem 1, we decompose

\[\mathbf{x}_{t}=\mathbf{R}\mathbf{r}_{t}+\mathbf{N}\mathbf{n}_{t}\text{, and }\bm{\epsilon}_{t}=\mathbf{R}\bm{\epsilon}_{t}^{(\mathbf{r})}+\mathbf{N}\bm{ \epsilon}_{t}^{(\mathbf{n})},\]

where \(\mathbf{R}\in\mathbb{R}^{d\times r}\) an orthonormal basis of the vector space \(\left\{\bm{\mu}_{i}\right\}_{i\in[k]}\) and \(\mathbf{N}\in\mathbb{R}^{d\times n}\) an orthonormal basis of the null space of \(\left\{\bm{\mu}_{i}\right\}_{i\in[k]}\). To show \(\left\|\mathbf{x}_{t}-\bm{\mu}_{i}\right\|^{2}>\left(\frac{\nu_{0}^{2}}{2}+ \frac{\nu_{\max}^{2}}{2(1-c_{\nu})}\right)d\), it suffices to prove \(\left\|\mathbf{n}_{t}\right\|^{2}>\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max }^{2}}{2(1-c_{\nu})}\right)d\). By Proposition 2, if \(\mathbf{x}_{0}\) is initialized in the distribution \(P^{(0)}\), i.e., \(\mathbf{x}_{0}\sim P^{(0)}\), since \(\nu_{0}^{2}>\frac{1}{1-c_{\nu}}\nu_{\max}^{2}\), with probability at least \(1-\exp(-\Omega(d))\) we have

\[\left\|\mathbf{n}_{0}\right\|^{2}\geq\left(\frac{3\nu_{0}^{2}}{4}+\frac{\nu_{ \max}^{2}}{4(1-c_{\nu})}\right)d.\] (12)

Then, conditioned on \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\left(\frac{3\nu_{0}^{2}}{4}+\frac{\nu_{ \max}^{2}}{4(1-c_{\nu})}\right)d\), the following proposition shows that \(\left\|\mathbf{n}_{t}\right\|\) remains large with high probability.

**Proposition 5**.: _Consider a distribution \(P\) satisfying Assumption 2. We follow the Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps. Suppose that the initial sample satisfies \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\left(\frac{3\nu_{0}^{2}}{4}+\frac{\nu_{ \max}^{2}}{4(1-c_{\nu})}\right)d\), then with probability at least \(1-T\cdot\exp(-\Omega(d))\), we have that \(\left\|\mathbf{n}_{t}\right\|^{2}>\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max }^{2}}{2(1-c_{\nu})}\right)d\) for all \(t\in\{0\}\cup[T]\)._

Proof of Proposition 5.: Firstly, by Lemma 3, if \(\delta_{t}>\nu_{0}^{2}\), since \(\nu_{0}^{2}>\frac{\nu_{\max}^{2}}{1-c_{\nu}}\), we similarly have that \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\left(\frac{3\nu_{0}^{2}}{4}+\frac{\nu_{ \max}^{2}}{4(1-c_{\nu})}\right)d\) with probability at least \(1-\exp(-\Omega(d))\) regardless of the previous state \(\mathbf{x}_{t-1}\). We then consider the case when \(\delta_{t}\leq\nu_{0}^{2}\). Intuitively, we aim to prove that the score function is close to \(-\frac{\mathbf{x}}{\nu_{0}^{2}}\) when \(\left\|\mathbf{n}\right\|^{2}\geq\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max} ^{2}}{2(1-c_{\nu})}\right)d\). Towards this goal, we first show that \(P^{(0)}(\mathbf{x})\) is exponentially larger than \(P^{(i)}(\mathbf{x})\) for all \(i\in[k]\) in the following lemma:

**Lemma 10**.: _Suppose \(P\) satisfies Assumption 2. Then for any \(\left\|\mathbf{n}\right\|^{2}\geq\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max}^{ 2}}{2(1-c_{\nu})}\right)d\), we have \(\frac{P^{(i)}(\mathbf{x})}{P^{(0)}(\mathbf{x})}\leq\exp(-\Omega(d))\) and \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})\right\|}{P(\mathbf{x})} \leq\exp(-\Omega(d))\) for all \(i\in[k]\)._

Proof of Lemma 10.: We first give an upper bound on the sub-Gaussian probability density. For any vector \(\mathbf{v}\in\mathbb{R}^{d}\), by considering some vector \(\mathbf{m}\in\mathbb{R}^{d}\), from Markov's inequality and the definition in equation 4 we can bound

\[\mathbb{P}_{\mathbf{z}\sim P^{(i)}}\left(\mathbf{m}^{T}(\mathbf{z} -\bm{\mu}_{i})\geq\mathbf{m}^{T}(\mathbf{v}-\bm{\mu}_{i})\right) \leq\frac{\mathbb{E}_{\mathbf{z}\sim P^{(i)}}\left[\exp\left(\mathbf{m} ^{T}(\mathbf{z}-\bm{\mu}_{i})\right)\right]}{\exp\left(\mathbf{m}^{T}(\mathbf{ v}-\bm{\mu}_{i})\right)}\] \[\leq\exp\left(\frac{\nu_{i}^{2}\left\|\mathbf{m}\right\|^{2}}{2}- \mathbf{m}^{T}(\mathbf{v}-\bm{\mu}_{i})\right).\]Upon optimizing the last term at \(\mathbf{m}=\frac{\mathbf{v}-\boldsymbol{\mu}_{i}}{\nu_{i}^{2}}\), we obtain

\[\mathbb{P}_{\mathbf{z}\sim P^{(i)}}\left((\mathbf{v}-\boldsymbol{\mu}_{i})^{T}( \mathbf{v}-\mathbf{z})\leq 0\right)\leq\exp\left(-\frac{\left\|\mathbf{v}- \boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\right).\] (13)

Denote \(\mathbb{B}:=\left\{\mathbf{z}:(\mathbf{v}-\boldsymbol{\mu}_{i})^{T}(\mathbf{v }-\mathbf{z})\leq 0\right\}\). To bound \(\mathbb{P}_{\mathbf{z}\sim P^{(i)}}(\mathbf{z}\in\mathbb{B})\), we first note that

\[\log P^{(i)}(\mathbf{v})-\log P^{(i)}(\mathbf{z})\] \[=\int_{0}^{1}\left\langle\mathbf{v}-\mathbf{z},\nabla\log P^{(i) }(\mathbf{v}+\lambda(\mathbf{z}-\mathbf{v}))\right\rangle\mathrm{d}\lambda\] \[=\left\langle\mathbf{v}-\mathbf{z},\nabla\log P^{(i)}(\mathbf{v })\right\rangle+\int_{0}^{1}\left\langle\mathbf{v}-\mathbf{z},\nabla\log P^{( i)}(\mathbf{v}+\lambda(\mathbf{z}-\mathbf{v}))-\nabla\log P^{(i)}(\mathbf{v}) \right\rangle\mathrm{d}\lambda\] \[\leq\left\|\mathbf{v}-\mathbf{z}\right\|\left\|\nabla\log P^{(i) }(\mathbf{v})\right\|+\int_{0}^{1}\left\|\mathbf{v}-\mathbf{z}\right\|\left\| \nabla\log P^{(i)}(\mathbf{v}+\lambda(\mathbf{z}-\mathbf{v}))-\nabla\log P^{( i)}(\mathbf{v})\right\|\mathrm{d}\lambda\] \[\leq\left\|\mathbf{v}-\mathbf{z}\right\|\cdot L_{i}\left\|\mathbf{ v}-\boldsymbol{\mu}_{i}\right\|+\int_{0}^{1}\left\|\mathbf{v}-\mathbf{z} \right\|\cdot L_{i}\left\|\lambda(\mathbf{z}-\mathbf{v})\right\|\mathrm{d}\lambda\] (14)

where equation 14 follows from Assumption 2.ii. that \(\nabla\log P^{(i)}(\boldsymbol{\mu}_{i})=\mathbf{0}_{d}\) and Assumption 2.iii. that the score function \(\nabla\log P^{(i)}\) is \(L_{i}\)-Lipschitz. Therefore we obtain

\[\mathbb{P}_{\mathbf{z}\sim P^{(i)}}(\mathbf{z}\in\mathbb{B})=\int _{\mathbf{z}\in\mathbb{B}}P^{(i)}(\mathbf{z})\,\mathrm{d}\mathbf{z}\] \[\geq\int_{\mathbf{z}\in\mathbb{B}}P^{(i)}(\mathbf{v})\exp\left(- \frac{L_{i}c_{\nu}}{2c_{L}}\left\|\mathbf{v}-\boldsymbol{\mu}_{i}\right\|^{2}- \frac{c_{L}+c_{\nu}}{2c_{\nu}}L_{i}\left\|\mathbf{v}-\mathbf{z}\right\|^{2} \right)\,\mathrm{d}\mathbf{z}\] \[=P^{(i)}(\mathbf{v})\exp\left(-\frac{L_{i}c_{\nu}}{2c_{L}}\left\| \mathbf{v}-\boldsymbol{\mu}_{i}\right\|^{2}\right)\int_{\mathbf{z}\in\mathbb{ B}}\exp\left(-\frac{c_{L}+c_{\nu}}{2c_{\nu}}L_{i}\left\|\mathbf{v}-\mathbf{z} \right\|^{2}\right)\,\mathrm{d}\mathbf{z}.\] (15)

By observing that \(g:\mathbb{B}\rightarrow\left\{\mathbf{z}:(\mathbf{v}-\boldsymbol{\mu}_{i})^{T} (\mathbf{v}-\mathbf{z})\geq 0\right\}\) with \(g(\mathbf{z})=2\mathbf{v}-\mathbf{z}\) is a bijection such that \(\left\|\mathbf{v}-\mathbf{z}\right\|=\left\|\mathbf{v}-g(\mathbf{z})\right\|\) for any \(\mathbf{z}\in\mathbb{B}\), we have

\[\int_{\mathbf{z}\in\mathbb{B}}\exp\left(-\frac{c_{L}+c_{\nu}}{2c _{\nu}}L_{i}\left\|\mathbf{v}-\mathbf{z}\right\|^{2}\right)\,\mathrm{d} \mathbf{z} =\frac{1}{2}\int_{\mathbf{z}\in\mathbb{R}^{d}}\exp\left(-\frac{ c_{L}+c_{\nu}}{2c_{\nu}}L_{i}\left\|\mathbf{v}-\mathbf{z}\right\|^{2}\right)\, \mathrm{d}\mathbf{z}\] \[=\frac{1}{2}\left(\frac{2\pi c_{\nu}}{(c_{L}+c_{\nu})L_{i}} \right)^{\frac{d}{2}}.\] (16)

Hence, by combining equation 13, equation 15, and equation 16, we obtain

\[\exp\left(-\frac{\left\|\mathbf{v}-\boldsymbol{\mu}_{i}\right\|^{2 }}{2\nu_{i}^{2}}\right) \geq\mathbb{P}_{\mathbf{z}\sim P^{(i)}}\left((\mathbf{v}- \boldsymbol{\mu}_{i})^{T}(\mathbf{v}-\mathbf{z})\leq 0\right)\] \[\geq P^{(i)}(\mathbf{v})\exp\left(-\frac{L_{i}c_{\nu}}{2c_{L}} \left\|\mathbf{v}-\boldsymbol{\mu}_{i}\right\|^{2}\right)\cdot\frac{1}{2} \left(\frac{2\pi c_{\nu}}{(c_{L}+c_{\nu})L_{i}}\right)^{\frac{d}{2}}.\]

By Assumption 2.iii. that \(L_{i}\leq\frac{c_{L}}{\nu_{i}^{2}}\) we obtain the following bound on the probability density:

\[P^{(i)}(\mathbf{v})\leq 2\left(\frac{2\pi c_{\nu}\nu_{i}^{2}}{(c_{L}+c_{\nu})c_{L}} \right)^{-\frac{d}{2}}\exp\left(-\frac{1-c_{\nu}}{2\nu_{i}^{2}}\left\|\mathbf{v}- \boldsymbol{\mu}_{i}\right\|^{2}\right).\] (17)Then we can bound the ratio of \(P^{(i)}\) and \(P^{(0)}\). For all \(i\in[k]\), define \(\rho_{i}(\mathbf{x}):=\frac{P^{(i)}(\mathbf{x})}{P^{(0)}(\mathbf{x})}\), then we have

\[\rho_{i}(\mathbf{x})=\frac{P^{(i)}(\mathbf{x})}{P^{(0)}(\mathbf{x })}\leq\frac{2(2\pi c_{\nu}\nu_{i}^{2}/(c_{L}^{2}+c_{\nu}c_{L}))^{-d/2}\exp \left(-(1-c_{\nu})\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|^{2}/2\nu_{i}^ {2}\right)}{(2\pi\nu_{0}^{2})^{-d/2}\exp\left(-\left\|\mathbf{x}\right\|^{2}/2 \nu_{0}^{2}\right)}\] \[=2\left(\frac{(c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}{c_{\nu}\nu_{i} ^{2}}\right)^{\frac{d}{2}}\exp\left(\left(\frac{1}{2\nu_{0}^{2}}-\frac{1-c_{ \nu}}{2\nu_{i}^{2}}\right)\left\|\mathbf{N}\mathbf{n}\right\|^{2}+\left(\frac {\left\|\mathbf{R}\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{(1-c_{\nu}) \left\|\mathbf{R}\mathbf{r}-\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}} \right)\right)\] \[=2\left(\frac{(c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}{c_{\nu}\nu_{i} ^{2}}\right)^{\frac{d}{2}}\exp\left(\left(\frac{1}{2\nu_{0}^{2}}-\frac{1-c_{ \nu}}{2\nu_{i}^{2}}\right)\left\|\mathbf{n}\right\|^{2}+\left(\frac{\left\| \mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{(1-c_{\nu})\left\|\mathbf{r}- \boldsymbol{\mathsf{R}}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}} \right)\right),\]

where the last step follows from the definition that \(\mathbf{R}\in\mathbb{R}^{d\times r}\) an orthogonal basis of the vector space \(\left\{\boldsymbol{\mu}_{i}\right\}_{i\in[k]}\) and \(\mathbf{N}^{T}\mathbf{N}=\boldsymbol{I}_{n}\). Since \(\nu_{i}^{2}<(1-c_{\nu})\nu_{0}^{2}\), the quadratic term \(\frac{\left\|\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{(1-c_{\nu})\left\| \mathbf{r}-\mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\) is maximized at \(\mathbf{r}=\frac{(1-c_{\nu})\nu_{0}^{2}\mathbf{R}^{T}\boldsymbol{\mu}_{i}}{(1 -c_{\nu})\nu_{0}^{2}-\nu_{i}^{2}}\). Therefore, we obtain

\[\frac{\left\|\mathbf{r}\right\|^{2}}{2\nu_{0}^{2}}-\frac{(1-c_{\nu})\left\| \mathbf{r}-\mathbf{R}^{T}\boldsymbol{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\leq \frac{(1-c_{\nu})\left\|\boldsymbol{\mu}_{i}\right\|^{2}}{2((1-c_{\nu})\nu_{0} ^{2}-\nu_{i}^{2})}.\]

Hence, for \(\left\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\right\|^{2}\leq\frac{(1-c_{ \nu})\nu_{0}^{2}-\nu_{i}^{2}}{2(1-c_{\nu})}\left(\log\frac{c_{\nu}\nu_{0}^{2}} {(c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}-\frac{\nu_{i}^{2}}{2(1-c_{\nu})\nu_{0}^ {2}}+\frac{(1-c_{\nu})\nu_{0}^{2}}{2\nu_{i}^{2}}\right)d\) and \(\left\|\mathbf{n}\right\|^{2}\geq\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max}^ {2}}{2(1-c_{\nu})}\right)d\), we have

\[\rho_{i}(\mathbf{x})\leq 2\left(\frac{(c_{L}^{2}+c_{\nu}c_{L})\nu_{0} ^{2}}{c_{\nu}\nu_{i}^{2}}\right)^{\frac{d}{2}}\exp\left(\left(\frac{1}{2\nu_{0 }^{2}}-\frac{1-c_{\nu}}{2\nu_{i}^{2}}\right)\left\|\mathbf{n}\right\|^{2}+ \frac{(1-c_{\nu})\left\|\boldsymbol{\mu}_{i}\right\|^{2}}{2((1-c_{\nu})\nu_{0 }^{2}-\nu_{i}^{2})}\right)\] \[\leq 2\left(\frac{(c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}{c_{\nu} \nu_{i}^{2}}\right)^{\frac{d}{2}}\exp\left(\left(\frac{1}{2\nu_{0}^{2}}-\frac{ 1-c_{\nu}}{2\nu_{i}^{2}}\right)\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{i}^{2}}{2 (1-c_{\nu})}\right)d+\frac{(1-c_{\nu})\left\|\boldsymbol{\mu}_{i}\right\|^{2}}{2 ((1-c_{\nu})\nu_{0}^{2}-\nu_{i}^{2})}\right)\] \[=2\exp\left(-\left(\log\frac{c_{\nu}\nu_{i}^{2}}{(c_{L}^{2}+c_{ \nu}c_{L})\nu_{0}^{2}}-\frac{\nu_{i}^{2}}{2(1-c_{\nu})\nu_{0}^{2}}+\frac{(1-c_{ \nu})\nu_{0}^{2}}{2\nu_{i}^{2}}\right)\frac{d}{2}+\frac{(1-c_{\nu})\left\| \boldsymbol{\mu}_{i}\right\|^{2}}{2((1-c_{\nu})\nu_{0}^{2}-\nu_{i}^{2})}\right)\] \[\leq 2\exp\left(-\left(\log\frac{c_{\nu}\nu_{i}^{2}}{(c_{L}^{2}+c_{ \nu}c_{L})\nu_{0}^{2}}-\frac{\nu_{i}^{2}}{2(1-c_{\nu})\nu_{0}^{2}}+\frac{(1-c_{ \nu})\nu_{0}^{2}}{2\nu_{i}^{2}}\right)\frac{d}{4}\right).\]

From Lemma 9, we obtain \(\rho_{i}(\mathbf{x})\leq\exp(-\Omega(d))\).

To show \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})\right\|}{P(\mathbf{x})} \leq\exp(-\Omega(d))\), from Assumptions 2.ii. and 2.iii. we have

\[\left\|\frac{\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})}{P^{(i)}( \mathbf{x})}\right\| =\left\|\frac{\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})}{P^{(i)}( \mathbf{x})}-\frac{\nabla_{\mathbf{x}}P^{(i)}(\boldsymbol{\mu}_{i})}{P^{(i)}( \boldsymbol{\mu}_{i})}\right\|=\left\|\nabla_{\mathbf{x}}\log P^{(i)}(\mathbf{x })-\nabla_{\mathbf{x}}\log P^{(i)}(\boldsymbol{\mu}_{i})\right\|\] \[\leq L_{i}\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|\leq\frac{ c_{L}}{\nu_{i}^{2}}\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|.\]

Therefore, we can bound \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})\right\|}{P(\mathbf{x})} \leq\frac{c_{L}}{\nu_{i}^{2}}\rho_{i}(\mathbf{x})\left\|\mathbf{x}- \boldsymbol{\mu}_{i}\right\|\). When \(\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|=\exp(o(d))\) is small, by \(\rho_{i}(\mathbf{x})\leq\exp(-\Omega(d))\) we directly have \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})\right\|}{P(\mathbf{x})} \leq\exp(-\Omega(d))\). When \(\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|=\exp(\Omega(d))\) is exceedingly large, from equation 17 we have

\[\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})\right\|}{P(\mathbf{x})} \leq\frac{2c_{L}}{\nu_{i}^{2}}\left(\frac{(c_{L}^{2}+c_{\nu}c_{L})\nu_{0}^{2}}{c_{ \nu}\nu_{i}^{2}}\right)^{\frac{d}{2}}\exp\left(\frac{\left\|\mathbf{x} \Since \(\nu_{0}^{2}>\frac{\nu_{t}^{2}}{1-c_{\nu}}\), when \(\left\|\mathbf{x}-\bm{\mu}_{i}\right\|=\exp(\Omega(d))\gg\left\|\bm{\mu}_{i}\right\|\) we have

\[\exp\left(\frac{\left\|\mathbf{x}\right\|^{2}}{2\nu_{0}^{2}}-\frac{(1-c_{\nu}) \left\|\mathbf{x}-\bm{\mu}_{i}\right\|^{2}}{2\nu_{i}^{2}}\right)=\exp(-\Omega( \left\|\mathbf{x}-\bm{\mu}_{i}\right\|^{2})).\]

Therefore \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})\right\|}{P(\mathbf{x})} \leq\exp(-\Omega(d))\). Thus we complete the proof of Lemma 10. 

Similar to Lemma 5, the following lemma proves that when the previous state \(\mathbf{n}_{t-1}\) is far from a mode, a single step of Langevin dynamics with bounded step size is not enough to find the mode.

**Lemma 11**.: _Suppose \(\delta_{t}\leq\nu_{0}^{2}\) and \(\left\|\mathbf{n}_{t-1}\right\|^{2}>36\nu_{0}^{2}d\), then we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\nu_{0}^{2}d\) with probability at least \(1-\exp(-\Omega(d))\)._

Proof of Lemma 11.: For simplicity, denote \(\mathbf{v}:=\mathbf{n}_{t-1}+\frac{\delta_{t}}{2}\mathbf{N}^{T}\nabla_{ \mathbf{x}}\log P(\mathbf{x}_{t-1})\). Since \(P=\sum_{i=0}^{k}w_{i}P^{(i)}\) and \(P^{(0)}=\mathcal{N}(\bm{\mu}_{0},\nu_{0}^{2}\bm{I}_{d})\), the score function can be written as

\[\nabla_{\mathbf{x}}\log P(\mathbf{x}) =\frac{\nabla_{\mathbf{x}}P(\mathbf{x})}{P(\mathbf{x})}=\frac{ \nabla_{\mathbf{x}}w_{0}P^{(0)}(\mathbf{x})}{P(\mathbf{x})}+\sum_{i\in[k]} \frac{\nabla_{\mathbf{x}}w_{i}P^{(i)}(\mathbf{x})}{P(\mathbf{x})}\] \[=-\frac{w_{0}P^{(0)}(\mathbf{x})}{P(\mathbf{x})}\cdot\frac{ \mathbf{x}}{\nu_{0}^{2}}+\sum_{i\in[k]}\frac{w_{i}\nabla_{\mathbf{x}}P^{(i)}( \mathbf{x})}{P(\mathbf{x})}\] \[=-\frac{\mathbf{x}}{\nu_{0}^{2}}+\sum_{i\in[k]}\frac{w_{i}P^{(i)} (\mathbf{x})}{P(\mathbf{x})}\cdot\frac{\mathbf{x}}{\nu_{0}^{2}}+\sum_{i\in[k]} \frac{w_{i}\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x})}{P(\mathbf{x})}.\] (18)

For \(\left\|\mathbf{n}_{t-1}\right\|^{2}>36\nu_{0}^{2}d\) by Lemma 10 we have \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x}_{t-1})\right\|}{P(\mathbf{x }_{t-1})}\leq\exp(-\Omega(d))\). Since \(\delta_{t}\leq\nu_{0}^{2}\), we can bound the norm of \(\mathbf{v}\) by

\[\left\|\mathbf{v}\right\| =\left\|\mathbf{n}_{t-1}+\frac{\delta_{t}}{2}\mathbf{N}^{T}\nabla _{\mathbf{x}}\log P(\mathbf{x}_{t-1})\right\|\] \[=\left\|\mathbf{n}_{t-1}-\frac{\delta_{t}}{2\nu_{0}^{2}}\mathbf{ n}_{t-1}+\sum_{i\in[k]}\frac{w_{i}\delta_{t}}{2\nu_{0}^{2}}\frac{P^{(i)}( \mathbf{x}_{t-1})}{P(\mathbf{x}_{t-1})}\mathbf{n}_{t-1}+\sum_{i\in[k]}\frac{w _{i}\delta_{t}}{2}\frac{\mathbf{N}^{T}\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x}_{ t-1})}{P(\mathbf{x}_{t-1})}\right\|\] \[\geq\left\|\left(1-\frac{\delta_{t}}{2\nu_{0}^{2}}+\sum_{i\in[k]} \frac{w_{i}\delta_{t}}{2\nu_{0}^{2}}\frac{P^{(i)}(\mathbf{x}_{t-1})}{P( \mathbf{x}_{t-1})}\right)\mathbf{n}_{t-1}\right\|-\sum_{i\in[k]}\frac{w_{i} \delta_{t}}{2}\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x}_{t-1}) \right\|}{P(\mathbf{x}_{t-1})}\] \[\geq\frac{1}{2}\left\|\mathbf{n}_{t-1}\right\|-\sum_{i\in[k]} \frac{w_{i}\delta_{t}}{2}\exp(-\Omega(d))\] \[>2\nu_{0}\sqrt{d}.\]

On the other hand, from \(\bm{\epsilon}_{t}^{(\mathbf{n})}\sim\mathcal{N}(\mathbf{0}_{n},\bm{I}_{n})\) we know \(\frac{\left\langle\mathbf{v},\bm{\epsilon}_{t}^{(\mathbf{n})}\right\rangle}{ \left\|\mathbf{v}\right\|}\sim\mathcal{N}(0,1)\) for any fixed \(\mathbf{v}\neq\mathbf{0}_{n}\), hence by Lemma 2 we have

\[\mathbb{P}\left(\frac{\left\langle\mathbf{v},\bm{\epsilon}_{t}^{(\mathbf{n})} \right\rangle}{\left\|\mathbf{v}\right\|}\geq\frac{\sqrt{d}}{4}\right)= \mathbb{P}\left(\frac{\left\langle\mathbf{v},\bm{\epsilon}_{t}^{(\mathbf{n})} \right\rangle}{\left\|\mathbf{v}\right\|}\leq-\frac{\sqrt{d}}{4}\right)\leq \frac{4}{\sqrt{2\pi d}}\exp\left(-\frac{d}{32}\right)\]

Combining the above inequalities gives

\[\left\|\mathbf{n}_{t}\right\|^{2}=\left\|\mathbf{v}+\sqrt{\delta_{t}}\bm{ \epsilon}_{t}^{(\mathbf{n})}\right\|^{2}\geq\left\|\mathbf{v}\right\|^{2}-2\nu _{0}|\langle\mathbf{v},\bm{\epsilon}_{t}^{(\mathbf{n})}\rangle|\geq\left\| \mathbf{v}\right\|^{2}-\frac{\nu_{0}\sqrt{d}}{2}\left\|\mathbf{v}\right\|>\nu_{0 }^{2}d\]

with probability at least \(1-\frac{8}{\sqrt{2\pi d}}\exp\left(-\frac{d}{32}\right)=1-\exp(-\Omega(d))\). This proves Lemma 11.

When \(\left\|\mathbf{n}_{t-1}\right\|^{2}\leq 36\nu_{0}^{2}d\), similar to Theorem 1, we consider a surrogate recursion \(\hat{\mathbf{n}}_{t}\) such that \(\hat{\mathbf{n}}_{0}=\mathbf{n}_{0}\) and for all \(t\geq 1\),

\[\hat{\mathbf{n}}_{t}=\hat{\mathbf{n}}_{t-1}-\frac{\delta_{t}}{2\nu_{0}^{2}} \hat{\mathbf{n}}_{t-1}+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_{t}^{(\text{n})}.\] (19)

The following Lemma shows that \(\hat{\mathbf{n}}_{t}\) is sufficiently close to the original recursion \(\mathbf{n}_{t}\).

**Lemma 12**.: _For any \(t\geq 1\), given that for all \(j\in[t]\), \(\delta_{j}\leq\nu_{0}^{2}\) and \(\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max}^{2}}{2(1-c_{\nu})}\right)d\leq \left\|\mathbf{n}_{j-1}\right\|^{2}\leq 36\nu_{0}^{2}d\), if \(\boldsymbol{\mu}_{i}\) satisfies Assumption 2.v. for all \(i\in[k]\), we have \(\left\|\hat{\mathbf{n}}_{t}-\mathbf{n}_{t}\right\|\leq\frac{t}{\exp(\Omega(d ))}\sqrt{d}\)._

Proof of Lemma 12.: By equation 18 we have that for all \(j\in[t]\),

\[\left\|\hat{\mathbf{n}}_{j}-\mathbf{n}_{j}\right\| =\left\|\hat{\mathbf{n}}_{j-1}-\mathbf{n}_{j-1}-\frac{\delta_{j} }{2\nu_{0}^{2}}\hat{\mathbf{n}}_{j-1}-\frac{\delta_{j}}{2}\mathbf{N}^{T} \nabla_{\mathbf{x}}\log P(\mathbf{x}_{j-1})\right\|\] \[=\left\|\hat{\mathbf{n}}_{j-1}-\mathbf{n}_{j-1}-\sum_{i\in[k]} \frac{w_{i}P^{(i)}(\mathbf{x}_{j-1})}{\nu_{0}^{2}P(\mathbf{x}_{j-1})}\mathbf{ n}_{j-1}-\sum_{i\in[k]}\frac{w_{i}\mathbf{N}^{T}\nabla_{\mathbf{x}}P^{(i)}( \mathbf{x}_{j-1})}{P(\mathbf{x}_{j-1})}\right\|\] \[\leq\left\|\hat{\mathbf{n}}_{j-1}-\mathbf{n}_{j-1}\right\|+\sum_ {i\in[k]}\frac{w_{i}P^{(i)}(\mathbf{x}_{j-1})}{\nu_{0}^{2}P(\mathbf{x}_{j-1}) }\left\|\mathbf{n}_{j-1}\right\|+\sum_{i\in[k]}\frac{w_{i}\left\|\nabla_{ \mathbf{x}}P^{(i)}(\mathbf{x}_{j-1})\right\|}{P(\mathbf{x}_{j-1})}.\]

By Lemma 10, we have \(\frac{P^{(i)}(\mathbf{x}_{j-1})}{P^{(0)}(\mathbf{x}_{j-1})}\leq\exp(-\Omega(d))\) and \(\frac{\left\|\nabla_{\mathbf{x}}P^{(i)}(\mathbf{x}_{j-1})\right\|}{P(\mathbf{ x}_{j-1})}\leq\exp(-\Omega(d))\) for all \(i\in[k]\), hence from \(\left\|\mathbf{n}_{j-1}\right\|\leq 6\nu_{0}\sqrt{d}\) we obtain a recursive bound

\[\left\|\hat{\mathbf{n}}_{j}-\mathbf{n}_{j}\right\|\leq\left\|\hat{\mathbf{n}} _{j-1}-\mathbf{n}_{j-1}\right\|+\frac{1}{\exp(\Omega(d))}\sqrt{d}.\]

Finally, by \(\hat{\mathbf{n}}_{0}=\mathbf{n}_{0}\), we have

\[\left\|\hat{\mathbf{n}}_{t}-\mathbf{n}_{t}\right\|=\sum_{j\in[t]}\left(\left\| \hat{\mathbf{n}}_{j}-\mathbf{n}_{j}\right\|-\left\|\hat{\mathbf{n}}_{j-1}- \mathbf{n}_{j-1}\right\|\right)\leq\frac{t}{\exp(\Omega(d))}\sqrt{d}.\]

Hence we obtain Lemma 12. 

Armed with the above lemmas, we are now ready to establish Proposition 5 by induction. Please note that we also apply some lemmas from the proof of Theorem 1 by substituting \(\nu_{\max}^{2}\) with \(\frac{\nu_{\max}^{2}}{1-c_{\nu}}\). Suppose the theorem holds for all \(T\) values of \(1,\cdots,T-1\). We consider the following 3 cases:

* If there exists some \(t\in[T]\) such that \(\delta_{t}>\nu_{0}^{2}\), by Lemma 3 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\left(\frac{3\nu_{0}^{2}}{4}+\frac{\nu_{ \max}^{2}}{4(1-c_{\nu})}\right)d\), thus the problem reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}\) for all \(t\in[T]\). If there exists some \(t\in[T]\) such that \(\left\|\mathbf{n}_{t-1}\right\|^{2}>36\nu_{0}^{2}d\), by Lemma 11 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\nu_{0}^{2}d>\left(\frac{3\nu_{0}^{2}}{4}+ \frac{\nu_{\max}^{2}}{4(1-c_{\nu})}\right)d\), thus the problem similarly reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}\) and \(\left\|\mathbf{n}_{t-1}\right\|^{2}\leq 36\nu_{0}^{2}d\) for all \(t\in[T]\). Conditioned on \(\left\|\mathbf{n}_{t-1}\right\|^{2}>\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{ \max}^{2}}{2(1-c_{\nu})}\right)d\) for all \(t\in[T]\), by Lemma 12 we have that for \(T=\exp(\mathcal{O}(d))\), \[\left\|\hat{\mathbf{n}}_{T}-\mathbf{n}_{T}\right\|<\left(\sqrt{\frac{5\nu_{0}^{2} }{8}+\frac{3\nu_{\max}^{2}}{8(1-c_{\nu})}}-\sqrt{\frac{\nu_{0}^{2}}{2}+\frac{ \nu_{\max}^{2}}{2(1-c_{\nu})}}\right)\sqrt{d}.\] By Lemma 8 we have that with probability at least \(1-\exp(-\Omega(d))\), \[\left\|\hat{\mathbf{n}}_{T}\right\|^{2}\geq\left(\frac{5\nu_{0}^{2}}{8}+\frac{3 \nu_{\max}^{2}}{8(1-c_{\nu})}\right)d.\]Combining the two inequalities implies the desired bound

\[\|\mathbf{n}_{T}\|\geq\|\hat{\mathbf{n}}_{T}\|-\|\hat{\mathbf{n}}_{T}-\mathbf{n} _{T}\|>\sqrt{\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max}^{2}}{2(1-c_{\nu})} \right)}\,d.\]

Hence by induction we obtain \(\|\mathbf{n}_{t}\|^{2}>\left(\frac{\nu_{0}^{2}}{2}+\frac{\nu_{\max}^{2}}{2(1-c _{\nu})}\right)d\) for all \(t\in[T]\) with probability at least

\[(1-(T-1)\exp(-\Omega(d)))\cdot(1-\exp(-\Omega(d)))\geq 1-T\exp(-\Omega(d)).\]

Therefore we complete the proof of Proposition 5. 

Finally, combining equation 12 and Proposition 5 finishes the proof of Theorem 3.

### Proof of Theorem 4: Annealed Langevin Dynamics under Sub-Gaussian Mixtures

**Assumption 3**.: _Consider a data distribution \(P:=\sum_{i=0}^{k}w_{i}P^{(i)}\) as a mixture of sub-Gaussian distributions, where \(1\leq k=o(d)\) and \(w_{i}>0\) is a positive constant such that \(\sum_{i=0}^{k}w_{i}=1\). Suppose that \(P^{(0)}=\mathcal{N}(\boldsymbol{\mu}_{0},\nu_{0}^{2}\boldsymbol{I}_{d})\) is Gaussian and for all \(i\in[k]\), \(P^{(i)}\) satisfies_

1. \(P^{(i)}\) _is a sub-Gaussian distribution of mean_ \(\boldsymbol{\mu}_{i}\) _with parameter_ \(\nu_{i}^{2}\)_,_
2. \(P^{(i)}\) _is differentiable and_ \(\nabla P^{(i)}_{\sigma_{i}}(\boldsymbol{\mu}_{i})=\boldsymbol{0}_{d}\) _for all_ \(t\in\{0\}\cup[T]\)_,_
3. _for all_ \(t\in\{0\}\cup[T]\)_, the score function of_ \(P^{(i)}_{\sigma_{i}}\) _is_ \(L_{i,t}\)_-Lipschitz such that_ \(L_{i,t}\leq\frac{c_{L}}{\nu_{i}^{2}+\sigma_{t}^{2}}\) _for some constant_ \(c_{L}>0\)_,_
4. \(\nu_{0}^{2}>\max\left\{1,\frac{4(c_{L}^{2}+c_{\nu}c_{L})}{c_{\nu}(1-c_{\nu})} \right\}\frac{\nu_{\max}^{2}+c_{\sigma}^{2}}{1-c_{\nu}}-c_{\sigma}^{2}\) _for constant_ \(c_{\nu}\in(0,1)\)_, where_ \(\nu_{\max}:=\max_{i\in[k]}\nu_{i}\)_,_
5. \(\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\|^{2}\leq\frac{(1-c_{\nu})\nu_{0} ^{2}-\nu_{0}^{2}-c_{\nu}c_{\sigma}^{2}}{2(1-c_{\nu})}\left(\log\frac{c_{\nu}( \nu_{i}^{2}+c_{\sigma}^{2})}{(c_{L}^{2}+c_{\sigma}c_{L})(\nu_{0}^{2}+c_{ \sigma}^{2})}-\frac{(\nu_{1}^{2}+c_{\sigma}^{2})}{2(1-c_{\nu})(\nu_{0}^{2}+c_{ \sigma}^{2})}+\frac{(1-c_{\nu})(\nu_{0}^{2}+c_{\sigma}^{2})}{2(\nu_{i}^{2}+c_{ \sigma}^{2})}\right)d\)_._

The feasibility of Assumption 3.v. can be validated by substituting \(\nu^{2}\) in Lemma 9 with \(\nu^{2}+c_{\sigma}^{2}\). To establish Theorem 4, we first note from Proposition 1 that for a sub-Gaussian mixture \(P=\sum_{i=0}^{k}w_{i}P^{(i)}\), the perturbed distribution of noise level \(\sigma\) is \(P_{\sigma}=\sum_{i=0}^{k}w_{i}P_{\sigma}^{(i)}\), where \(P^{(0)}=\mathcal{N}(\boldsymbol{\mu}_{0},(\nu_{i}^{2}+\sigma^{2})\boldsymbol{ I}_{d})\) and \(P^{(i)}\) is a sub-Gaussian distribution with mean \(\boldsymbol{\mu}_{i}\) and sub-Gaussian parameter \((\nu_{i}^{2}+\sigma^{2})\). Similar to the proof of Theorem 1, we decompose

\[\mathbf{x}_{t}=\mathbf{R}\mathbf{r}_{t}+\mathbf{N}\mathbf{n}_{t}\text{, and }\boldsymbol{\epsilon}_{t}=\mathbf{R}\boldsymbol{\epsilon}_{t}^{(\mathbf{r})}+ \mathbf{N}\boldsymbol{\epsilon}_{t}^{(\mathbf{n})},\]

where \(\mathbf{R}\in\mathbb{R}^{d\times r}\) an orthonormal basis of the vector space \(\left\{\boldsymbol{\mu}_{i}\right\}_{i\in[k]}\) and \(\mathbf{N}\in\mathbb{R}^{d\times n}\) an orthonormal basis of the null space of \(\left\{\boldsymbol{\mu}_{i}\right\}_{i\in[k]}\). Now, we prove Theorem 4 by applying the techniques developed in Appendix A.1 and A.3 via substituting \(\nu^{2}\) and \(\frac{\nu^{2}}{1-c_{\nu}}\) with \(\frac{\nu^{2}+\sigma_{t}^{2}}{1-c_{\nu}}\) at time step \(t\). Note that for all \(t\in\{0\}\cup[T]\), Assumption 3.iv. implies \(\nu_{0}^{2}+\sigma_{t}^{2}>\max\left\{1,\frac{4(c_{L}^{2}+c_{\nu}c_{L})}{c_{ \nu}(1-c_{\nu})}\right\}\frac{\nu_{\max}^{2}+\sigma_{t}^{2}}{1-c_{\nu}}\) because \(c_{\sigma}\geq\sigma_{t}\).

First, by Proposition 2, suppose that the sample is initialized in the distribution \(P_{\sigma_{0}}^{(0)}\), then with probability at least \(1-\exp(-\Omega(d))\), we have

\[\left\|\mathbf{n}_{0}\right\|^{2}\geq\left(\frac{3(\nu_{0}^{2}+\sigma_{0}^{2})} {4}+\frac{\nu_{\max}^{2}+\sigma_{0}^{2}}{4(1-c_{\nu})}\right)d.\] (20)

Then, with the assumption that the initialization satisfies \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\left(\frac{3(\nu_{0}^{2}+\sigma_{0}^{2})} {4}+\frac{\nu_{\max}^{2}+\sigma_{0}^{2}}{4(1-c_{\nu})}\right)d\), the following proposition similar to Proposition 5 shows that \(\left\|\mathbf{n}_{t}\right\|\) remains large with high probability.

**Proposition 6**.: _Consider a distribution \(P\) satisfying Assumption 3. We follow annealed Langevin dynamics for \(T=\exp(\mathcal{O}(d))\) steps with noise level \(c_{\sigma}\geq\sigma_{0}\geq\sigma_{1}\geq\cdots\geq\sigma_{T}\geq 0\) for some constant \(c_{\sigma}>0\). Suppose that the initial sample satisfies \(\left\|\mathbf{n}_{0}\right\|^{2}\geq\left(\frac{3(\nu_{0}^{2}+\sigma_{0}^{2})} {4}+\frac{\nu_{\max}^{2}+\sigma_{0}^{2}}{4(1-c_{\nu})}\right)d\), then with probability at least \(1-T\cdot\exp(-\Omega(d))\), we have that \(\left\|\mathbf{n}_{t}\right\|^{2}>\left(\frac{\nu_{0}^{2}+\sigma_{t}^{2}}{2}+ \frac{\nu_{\max}^{2}+\sigma_{t}^{2}}{2(1-c_{\nu})}\right)d\) for all \(t\in\{0\}\cup[T]\)._Proof of Proposition 6.: We prove Proposition 6 by induction. Suppose the theorem holds for all \(T\) values of \(1,\cdots,T-1\). We consider the following 3 cases:

* If there exists some \(t\in[T]\) such that \(\delta_{t}>\nu_{0}^{2}+\sigma_{t}^{2}\), by Lemma 3 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq\left(\frac{3(\nu_{0}^{2}+\sigma_{t}^{2} )}{4}+\frac{\nu_{0\text{max}}^{2}+\sigma_{t}^{2}}{4(1-c_{\nu})}\right)d\), thus the problem reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}+\sigma_{t}^{2}\) for all \(t\in[T]\). If there exists some \(t\in[T]\) such that \(\left\|\mathbf{n}_{t-1}\right\|^{2}>36(\nu_{0}^{2}+\sigma_{t-1}^{2})d\geq 3 6(\nu_{0}^{2}+\sigma_{t}^{2})d\), by Lemma 11 we know that with probability at least \(1-\exp(-\Omega(d))\), we have \(\left\|\mathbf{n}_{t}\right\|^{2}\geq(\nu_{0}^{2}+\sigma_{t}^{2})d>\left( \frac{3(\nu_{0}^{2}+\sigma_{t}^{2})}{4}+\frac{\nu_{0\text{max}}^{2}+\sigma_{t }^{2}}{4(1-c_{\nu})}\right)d\), thus the problem similarly reduces to the two sub-arrays \(\mathbf{n}_{0},\cdots,\mathbf{n}_{t-1}\) and \(\mathbf{n}_{t},\cdots,\mathbf{n}_{T}\), which can be solved by induction.
* Suppose \(\delta_{t}\leq\nu_{0}^{2}+\sigma_{t}^{2}\) and \(\left\|\mathbf{n}_{t-1}\right\|^{2}\leq 36(\nu_{0}^{2}+\sigma_{t-1}^{2})d\) for all \(t\in[T]\). Consider a surrogate sequence \(\hat{\mathbf{n}}_{t}\) such that \(\hat{\mathbf{n}}_{0}=\mathbf{n}_{0}\) and for all \(t\geq 1\), \[\hat{\mathbf{n}}_{t}=\hat{\mathbf{n}}_{t-1}-\frac{\delta_{t}}{2\nu_{0}^{2}+2 \sigma_{t}^{2}}\hat{\mathbf{n}}_{t-1}+\sqrt{\delta_{t}}\boldsymbol{\epsilon}_ {t}^{(\mathbf{n})}.\] Since \(\nu_{0}>\nu_{i}\) and \(c_{\sigma}\geq\sigma_{t}\) for all \(t\in\{0\}\cup[T]\), we have \(\frac{\nu_{t}^{2}+c_{\sigma}^{2}}{\nu_{0}^{2}+c_{\sigma}^{2}}>\frac{\nu_{t}^{ 2}+\sigma_{t}^{2}}{\nu_{0}^{2}+\sigma_{t}^{2}}\). Notice that for function \(f(z)=\log z-\frac{z}{2}+\frac{1}{2z}\), we have \(\frac{\mathrm{d}}{\mathrm{d}z}f(z)=\frac{1}{z}-\frac{1}{2}-\frac{1}{2z^{2}}=- \frac{1}{2}\left(\frac{1}{z}-1\right)^{2}\leq 0\). Thus, by Assumption 3.v. we have that for all \(t\in[T]\), \[\left\|\boldsymbol{\mu}_{i}-\boldsymbol{\mu}_{0}\right\|^{2} \leq\frac{(1-c_{\nu})\nu_{0}^{2}-\nu_{i}^{2}-c_{\nu}c_{\sigma}^{2} }{2(1-c_{\nu})}\left(\log\frac{c_{\nu}(\nu_{i}^{2}+c_{\sigma}^{2})}{(c_{L}^{2} +c_{\nu}c_{L})(\nu_{0}^{2}+c_{\sigma}^{2})}\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\left. \frac{(\nu_{i}^{2}+c_{\sigma}^{2})}{2(1-c_{\nu})(\nu_{0}^{2}+c_{\sigma}^{2})} +\frac{(1-c_{\nu})(\nu_{0}^{2}+c_{\sigma}^{2})}{2(\nu_{i}^{2}+c_{\sigma}^{2}) }\right)d\] \[\leq\frac{(1-c_{\nu})\nu_{0}^{2}-\nu_{i}^{2}-c_{\nu}\sigma_{t}^{ 2}}{2(1-c_{\nu})}\left(\log\frac{c_{\nu}(\nu_{i}^{2}+\sigma_{t}^{2})}{(c_{L}^ {2}+c_{\nu}c_{L})(\nu_{0}^{2}+\sigma_{t}^{2})}\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad- \left.\frac{(\nu_{i}^{2}+\sigma_{t}^{2})}{2(1-c_{\nu})(\nu_{0}^{2}+\sigma_{t }^{2})}+\frac{(1-c_{\nu})(\nu_{0}^{2}+\sigma_{t}^{2})}{2(\nu_{i}^{2}+\sigma_{t }^{2})}\right)d\] Conditioned on \(\left\|\mathbf{n}_{t-1}\right\|^{2}>\left(\frac{\nu_{0}^{2}+\sigma_{t-1}^{2} }{2}+\frac{\nu_{\text{max}}^{2}+\sigma_{t-1}^{2}}{2(1-c_{\nu})}\right)d\) for all \(t\in[T]\), by Lemma 12 we have that for \(T=\exp(\mathcal{O}(d))\), \[\left\|\hat{\mathbf{n}}_{T}-\mathbf{n}_{T}\right\|<\left(\sqrt{\frac{5(\nu_{0} ^{2}+\sigma_{T}^{2})}{8}+\frac{3(\nu_{\text{max}}^{2}+\sigma_{T}^{2})}{8(1-c_{ \nu})}}-\sqrt{\frac{\nu_{0}^{2}+\sigma_{T}^{2}}{2}+\frac{\nu_{\text{max}}^{2}+ \sigma_{T}^{2}}{2(1-c_{\nu})}}\right)\sqrt{d}.\] By Lemma 8 we have that with probability at least \(1-\exp(-\Omega(d))\), \[\left\|\hat{\mathbf{n}}_{T}\right\|^{2}\geq\left(\frac{5(\nu_{0}^{2}+\sigma_{T} ^{2})}{8}+\frac{3(\nu_{\text{max}}^{2}+\sigma_{T}^{2})}{8(1-c_{\nu})}\right)d.\] Combining the two inequalities implies the desired bound \[\left\|\mathbf{n}_{T}\right\|\geq\left\|\hat{\mathbf{n}}_{T}\right\|-\left\| \hat{\mathbf{n}}_{T}-\mathbf{n}_{T}\right\|>\sqrt{\left(\frac{\nu_{0}^{2}+ \sigma_{T}^{2}}{2}+\frac{\nu_{\text{max}}^{2}+\sigma_{T}^{2}}{2(1-c_{\nu})} \right)d}.\] Hence by induction we obtain \(\left\|\mathbf{n}_{t}\right\|^{2}>\left(\frac{\nu_{0}^{2}+\sigma_{T}^{2}}{2}+ \frac{\nu_{\text{max}}^{2}+\sigma_{T}^{2}}{2(1-c_{\nu})}\right)d\) for all \(t\in[T]\) with probability at least \[(1-(T-1)\exp(-\Omega(d)))\cdot(1-\exp(-\Omega(d)))\geq 1-T\exp(-\Omega(d)).\]

Therefore we complete the proof of Proposition 6. 

Finally, combining equation 20 and Proposition 6 finishes the proof of Theorem 4.

### Proof of Theorem 5: Convergence Analysis of Chained Langevin Dynamics

For simplicity, denote \(\mathbf{x}^{[q]}=\left\{\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(q)}\right\}\). By the definition of total variation distance, for all \(q\in[d/Q]\) we have

\[\text{TV}\left(\hat{P}\left(\mathbf{x}^{[q]}\right),P\left(\mathbf{ x}^{[q]}\right)\right)\] \[=\frac{1}{2}\int\left|\hat{P}\left(\mathbf{x}^{[q]}\right)-P \left(\mathbf{x}^{[q]}\right)\right|\,\mathrm{d}\mathbf{x}^{[q]}\] \[=\frac{1}{2}\int\left|\hat{P}\left(\mathbf{x}^{(q)}\mid\mathbf{ x}^{[q-1]}\right)\hat{P}\left(\mathbf{x}^{[q-1]}\right)-P\left(\mathbf{x}^{(q)} \mid\mathbf{x}^{[q-1]}\right)P\left(\mathbf{x}^{[q-1]}\right)\right|\,\mathrm{ d}\mathbf{x}^{[q]}\] \[\leq\frac{1}{2}\int\left|\hat{P}\left(\mathbf{x}^{(q)}\mid \mathbf{x}^{[q-1]}\right)\hat{P}\left(\mathbf{x}^{[q-1]}\right)-\hat{P}\left( \mathbf{x}^{(q)}\mid\mathbf{x}^{[q-1]}\right)P\left(\mathbf{x}^{[q-1]}\right) \right|\,\mathrm{d}\mathbf{x}^{[q]}\] \[=\frac{1}{2}\int\hat{P}\left(\mathbf{x}^{(q)}\mid\mathbf{x}^{[q- 1]}\right)\,\mathrm{d}\mathbf{x}^{(q)}\int\left|\hat{P}\left(\mathbf{x}^{[q-1] }\right)-P\left(\mathbf{x}^{[q-1]}\right)\right|\,\mathrm{d}\mathbf{x}^{[q-1]}\] \[\qquad+\frac{1}{2}\int\left|\hat{P}\left(\mathbf{x}^{(q)}\mid \mathbf{x}^{[q-1]}\right)-P\left(\mathbf{x}^{(q)}\mid\mathbf{x}^{[q-1]}\right) \right|\,\mathrm{d}\mathbf{x}^{(q)}\int P\left(\mathbf{x}^{[q-1]}\right)\, \mathrm{d}\mathbf{x}^{[q-1]}\] \[=\text{TV}\left(\hat{P}\left(\mathbf{x}^{[q-1]}\right),P\left( \mathbf{x}^{[q-1]}\right)\right)+\text{TV}\left(\hat{P}\left(\mathbf{x}^{(q)} \mid\mathbf{x}^{[q-1]}\right),P\left(\mathbf{x}^{(q)}\mid\mathbf{x}^{[q-1]} \right)\right)\] \[\leq\text{TV}\left(\hat{P}\left(\mathbf{x}^{[q-1]}\right),P\left( \mathbf{x}^{[q-1]}\right)\right)+\varepsilon\cdot\frac{Q}{d}.\]

Upon summing up the above inequality for all \(q\in[d/Q]\), we obtain

\[\text{TV}\left(\hat{P}(\mathbf{x}),P(\mathbf{x})\right) =\sum_{q=1}^{d/Q}\left(\text{TV}\left(\hat{P}\left(\mathbf{x}^{[q] }\right),P\left(\mathbf{x}^{[q]}\right)\right)-\text{TV}\left(\hat{P}\left( \mathbf{x}^{[q-1]}\right),P\left(\mathbf{x}^{[q-1]}\right)\right)\right)\] \[\leq\sum_{q=1}^{d/Q}\varepsilon\cdot\frac{Q}{d}=\varepsilon\]

Thus we finish the proof of Theorem 5.

## Appendix B Additional Experiments

**Algorithm Setup:** Our choices of algorithm hyperparameters are based on Song and Ermon (2019). We consider \(L=10\) different standard deviations such that \(\left\{\lambda_{i}\right\}_{i\in[L]}\) is a geometric sequence with \(\lambda_{1}=1\) and \(\lambda_{10}=0.01\). For annealed Langevin dynamics with \(T\) iterations, we choose the noise levels \(\left\{\sigma_{t}\right\}_{t\in[T]}\) by repeating every element of \(\left\{\lambda_{i}\right\}_{i\in[L]}\) for \(T/L\) times and we set the step size as \(\delta_{t}=2\times 10^{-5}\cdot\sigma_{t}^{2}/\sigma_{T}^{2}\) for every \(t\in[T]\). For vanilla Langevin dynamics with \(T\) iterations, we use the same step size as annealed Langevin dynamics. For chained Langevin dynamics with \(T\) iterations, the patch size \(Q\) is chosen depending on different tasks. For every patch of chained Langevin dynamics, we choose the noise levels \(\left\{\sigma_{t}\right\}_{t\in[TQ/d]}\) by repeating every element of \(\left\{\lambda_{i}\right\}_{i\in[L]}\) for \(TQ/dL\) times and we set the step size as \(\delta_{t}=2\times 10^{-5}\cdot\sigma_{t}^{2}/\sigma_{TQ/d}^{2}\) for every \(t\in[TQ/d]\).

### Synthetic Gaussian Mixture Model

We choose the data distribution \(P\) as a mixture of three Gaussian components in dimension \(d=100\):

\[P=0.2P^{(0)}+0.4P^{(1)}+0.4P^{(2)}=0.2\mathcal{N}(\mathbf{0}_{d},3\bm{I}_{d})+0. 4\mathcal{N}(\mathbf{1}_{d},\bm{I}_{d})+0.4\mathcal{N}(-\mathbf{1}_{d},\bm{I} _{d}).\]

Since the distribution is given, we assume that the sampling algorithms have access to the ground-truth score function. We set the batch size as 1000 and patch size \(Q=10\) for chained Langevin dynamics. We use \(T\in\left\{10^{3},10^{4},10^{5},10^{6}\right\}\) iterations for vanilla, annealed, and chained Langevin dynamics. The initial samples are i.i.d. chosen from \(P^{(0)}\), \(P^{(1)}\), or \(P^{(2)}\), and the results are presented in Figures1, 4, and 5 respectively. The two subfigures above the dashed line illustrate the samples from the initial distribution and target distribution, and the subfigures below the dashed line are the samples generated by different algorithms. A sample \(\mathbf{x}\) is clustered in mode 1 if it satisfies \(\left\|\mathbf{x}-\boldsymbol{\mu}_{1}\right\|^{2}\leq 5d\) and \(\left\|\mathbf{x}-\boldsymbol{\mu}_{1}\right\|^{2}\leq\left\|\mathbf{x}- \boldsymbol{\mu}_{2}\right\|^{2}\); in mode 2 if \(\left\|\mathbf{x}-\boldsymbol{\mu}_{2}\right\|^{2}\leq 5d\) and \(\left\|\mathbf{x}-\boldsymbol{\mu}_{1}\right\|^{2}>\left\|\mathbf{x}- \boldsymbol{\mu}_{2}\right\|^{2}\); and in mode 0 otherwise. The experiments were run on an Intel Xeon CPU with 2.90GHz.

### Image Datasets

Our implementation and hyperparameter selection are based on Song and Ermon (2019). During training, we i.i.d. randomly flip an image with probability 0.5 to construct the two modes (i.e., original and flipped images). All models are optimized by Adam with learning rate 0.001 and batch size 128 for a total of 200000 training steps, and we use the model at the last iteration to generate the samples. We perform experiments on MNIST (LeCun, 1998) (CC BY-SA 3.0 License) and Fashion-MNIST (Xiao et al., 2017) (MIT License) datasets and we set the patch size as \(Q=14\).

For the score networks of vanilla and annealed Langevin dynamics, following from Song and Ermon (2019), we use the 4-cascaded RefineNet (Lin et al., 2017), a modern variant of U-Net (Ronneberger et al., 2015) with residual design. For the score networks of chained Langevin dynamics, we use the official PyTorch implementation of an LSTM network (Sak et al., 2014) followed by a linear layer. For MNIST and Fashion-MNIST datasets, we set the input size of the LSTM as \(Q=14\), the number of features in the hidden state as 1024, and the number of recurrent layers as 2. The inputs of LSTM include inputting tensor, hidden state, and cell state, and the outputs of LSTM include the next hidden state and cell state, which can be fed to the next input. To estimate the noisy score function, we first input the noise level \(\sigma\) (repeated for \(Q\) times to match the input size of LSTM) and all-0 hidden and

Figure 4: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are \(\ell_{2}\) distance from samples to the mean of the three modes. The samples are initialized in mode 1.

cell states to obtain an initialization of the hidden and cell states. Then, we divide a sample into \(d/Q\) patches and input the sequence of patches to the LSTM. For every output hidden state corresponding to one patch, we apply a linear layer of size \(1024\times Q\) to estimate the noisy score function of the patch.

To generate samples, we use \(T\in\{3000,10000,30000,100000\}\) iterations for vanilla, annealed, and chained Langevin dynamics. The initial samples are chosen as either original or flipped images from the dataset, and the results for MNIST and Fashion-MNIST datasets are presented in Figures 2, 6, 3, and 7 respectively. The two subfigures above the dashed line illustrate the samples from the initial distribution and target distribution, and the subfigures below the dashed line are the samples generated by different algorithms. High-quality figures generated by annealed and chained Langevin dynamics for \(T=100000\) iterations are presented in Figures 8 and 9.

All experiments were run with one RTX3090 GPU. It is worth noting that the training and inference time of chained Langevin dynamics using LSTM is considerably faster than vanilla/annealed Langevin dynamics using RefineNet. For a course of 200000 training steps on MNIST/Fashion-MNIST, due to the different network architectures, LSTM takes around 2.3 hours while RefineNet takes around 9.2 hours. Concerning image generation, chained Langevin dynamics is significantly faster than vanilla/annealed Langevin dynamics since every iteration of chained Langevin dynamics only updates a patch of constant size, while every iteration of vanilla/annealed Langevin dynamics requires computing all coordinates of the sample. One iteration of chained Langevin dynamics using LSTM takes around 1.97 ms, while one iteration of vanilla/annealed Langevin dynamics using RefineNet takes around 43.7 ms.

Figure 5: Samples from a mixture of three Gaussian modes generated by vanilla, annealed, and chained Langevin dynamics. Three axes are \(\ell_{2}\) distance from samples to the mean of the three modes. The samples are initialized in mode 2.

## Appendix C Boarder Impacts

This paper presents work whose goal is to advance the field of machine learning. No potential societal consequence of this work needs to be highlighted here.

Figure 6: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as flipped images from MNIST.

Figure 7: Samples from a mixture distribution of the original and flipped images from the Fashion-MNIST dataset generated by vanilla, annealed, and chained Langevin dynamics. The samples are initialized as flipped images from Fashion-MNIST.

## Appendix A

Figure 8: Samples from a mixture distribution of the original and flipped images from the MNIST dataset generated by annealed and chained Langevin dynamics for \(T=100000\) iterations. The samples are initialized as the original or flipped images from MNIST.

## 6 Conclusion

Figure 9: Samples from a mixture distribution of the original and flipped images from the Fashion-MNIST dataset generated by annealed and chained Langevin dynamics for \(T=100000\) iterations. The samples are initialized as the original or flipped images from Fashion-MNIST.

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We list the paper's contributions about the mode-seeking tendencies of vanilla, annealed, and chained Langevin Dynamics in the abstract and at the end of Section 1. The scope of this work is also discussed in Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of this work are discussed in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The assumptions and proof of every theorem are clearly stated. For Theorem 1, the assumptions are listed in Assumption 1 and the proof is in Appendix A.1; for Theorem 2, the assumptions are listed in Assumption 1 and the proof is in Appendix A.2; for Theorem 3, the assumptions are listed in Assumption 2 and the proof is in Appendix A.3; for Theorem 4, the assumptions are listed in Assumption 3 and the proof is in Appendix A.4.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

#### Experimental Result Reproducibility

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes] Justification: We provide all the information about our numerical experiments in Section 6 and Appendix B that enable readers to reproduce our results.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We submitted our code in supplementary materials for the readers to reproduce our numerical results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details of the numerical experiments are listed in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For the results of synthetic data, we report 1000 samples for every experiment. For the results of image datasets, we report 49 or 100 samples for every experiment. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Information on the computational resources of the numerical experiments is provided in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We read the guidelines and wrote the paper according to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: As discussed in Appendix C, this work has potential societal consequences, none of which needs to be specially highlighted. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The models used in this work do not have a high risk of misuse. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets are properly cited in our work. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Crowdsourcing or research with human subjects is not involved in this work. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Crowdsourcing or research with human subjects is not involved in our paper. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.