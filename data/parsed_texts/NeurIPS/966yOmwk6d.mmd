# Towards Data-Algorithm Dependent Generalization:

a Case Study on Overparameterized Linear Regression

 Jing Xu

IIIS, Tsinghua University

xujing21@mails.tsinghua.edu.cn &Jiaye Teng

IIIS, Tsinghua University

tjy20@mails.tsinghua.edu.cn &Yang Yuan

IIIS, Tsinghua University

Shanghai Artificial Intelligence Laboratory

Shanghai Qi Zhi Institute

yuanyang@tsinghua.edu.cn &Andrew Chi-Chih Yao

IIIS, Tsinghua University

Shanghai Artificial Intelligence Laboratory

Shanghai Qi Zhi Institute

andrewcyao@tsinghua.edu.cn

Equal Contribution

###### Abstract

One of the major open problems in machine learning is to characterize generalization in the overparameterized regime, where most traditional generalization bounds become inconsistent even for overparameterized linear regression [46]. In many scenarios, this failure can be attributed to obscuring the crucial interplay between the training algorithm and the underlying data distribution. This paper demonstrate that the generalization behavior of overparameterized model should be analyzed in a both data-relevant and algorithm-relevant manner. To make a formal characterization, We introduce a notion called data-algorithm compatibility, which considers the generalization behavior of the entire data-dependent training trajectory, instead of traditional last-iterate analysis. We validate our claim by studying the setting of solving overparameterized linear regression with gradient descent. Specifically, we perform a data-dependent trajectory analysis and derive a sufficient condition for compatibility in such a setting. Our theoretical results demonstrate that if we take early stopping iterates into consideration, generalization can hold with significantly weaker restrictions on the problem instance than the previous last-iterate analysis.

## 1 Introduction

Although deep neural networks achieve great success in practice [13, 14, 63], their remarkable generalization ability is still among the essential mysteries in the deep learning community. One of the most intriguing features of deep neural networks is overparameterization, which confers a level of tractability to the training problem, but leaves traditional generalization theories failing to work. In generalization analysis, both the training algorithm and the data distribution play essential roles [23, 28]. For instance, a line of work [46, 74] highlights the role of the algorithm by showing that the algorithm-irrelevant uniform convergence bounds can become inconsistent in deep learning regimes. Another line of work [8, 68] on benign overfitting emphasizes the role of data distribution via profound analysis of specific overparameterized models.

Despite the significant role of data and algorithm in generalization analysis, existing theories usually focus on either the data factor (_e.g._, uniform convergence [46] and last iterate analysis [8, 68]) or the algorithm factor (_e.g._, stability-based bounds [24]). Combining both data and algorithm factor intogeneralization analysis can help derive tighter generalization bounds and explain the generalization ability of overparameterized models observed in practice. In this sense, a natural question arises:

_How to incorporate both data factor and algorithm factor into generalization analysis?_

To gain insight into the interplay between data and algorithms, we provide motivating examples of a synthetic overparameterized linear regression task and a classification task on the corrupted MNIST dataset in figure 1. In both scenarios, the final iterate with less algorithmic information, which may include the algorithm type (_e.g._, GD or SGD), hyperparameters (_e.g._, learning rate, number of epochs), generalizes much worse than the early stopping solutions (see the Blue Line). In the linear regression case, the generalization error of the final iterate can be more than \(\times 100\) larger than that of the early stopping solution. In the MNIST case, the final iterate on the SGD trajectory has 19.9% test error, much higher than the 2.88% test error of the best iterate on the GD trajectory. Therefore, the almost ubiquitous strategy of early stopping is a key ingredient in generalization analysis for overparameterized models, whose benefits have been demonstrated both theoretically and empirically [1, 2, 27, 34, 41, 72]. By focusing on the entire optimization trajectory and performing data-dependent trajectory analysis, both data information and the dynamics of the training algorithm can be exploited to yield consistent generalization bounds.

When we take the algorithm into consideration and analyze the data-dependent training trajectory, generalization occurs if the minimum excess risk of the iterates on the training trajectory converges to zero, as the sample size tends to infinity. This accords with the real practice of training deep neural networks, where one can pick up the best parameter on the training trajectory, by calculating its loss on a validation dataset. We dub this notion of generalization as as _data-algorithm-compatibility_, which is formally defined in Section 3.2.

The significance of compatibility comes in three folds. Firstly, it incorporates both data and algorithm factors into generalization analysis, and is suitable for the overparameterization regime (see Definition 3.1). Secondly, it serves as a minimal condition for generalization, without which one cannot expect to find a consistent solution via standard learning procedures. Consequently, compatibility holds with only mild assumptions and applies to a wide range of problem instances (see Theorem 4.1). Thirdly, it captures the algorithmic significance of early stopping in generalization. By exploiting the algorithm information along the entire trajectory, we arrive at better generalization bounds than the last-iterate analysis (see Table 1 and 2 for examples).

To theoretically validate compatibility, we study it under overparameterized linear regression setting. Analysis of the overparameterized linear regression is a reasonable starting point to study more complex models like deep neural networks [17, 57], since many phenomena of the high dimensional non-linear model are also observed in the linear regime (_e.g._, Figure 1). Furthermore, the neural

Figure 1: **(a) The training plot for linear regression with spectrum \(\lambda_{i}=1/i^{2}\) using GD.** Note that the axes are in the log scale. **(b) The training plot of CNN on corrupted MNIST with 20% label noise using SGD.** Both models successfully learn the useful features in the initial phase of training, but it takes a long time for them to fit the noise in the dataset. The observations demonstrate the power of data-dependent trajectory analysis, since the early stopping solutions on the trajectory generalize well but the final iterate fails to generalize. See Appendix C for details.

tangent kernel (NTK) framework [4; 26] demonstrates that very wide neural networks trained using gradient descent with appropriate random initialization can be approximated by kernel regression in a reproducing kernel Hilbert space, which rigorously establishes a close relationship between overparameterized linear regression and deep neural network training.

Specifically, we investigate solving overparameterized linear regression using gradient descent with constant step size, and prove that under some mild regularity conditions, gradient descent is compatible with overparameterized linear regression if the effective dimensions of the feature covariance matrix are asymptotically bounded by the sample size. Compared with the last-iterate analysis [8], the main theorems in this paper require significantly weaker assumptions, which demonstrates the benefits of data-relevant and algorithm-relevant generalization analysis.

We summarize our contributions as follows:

* We formalize the notion of data-algorithm-compatibility, which highlights the interaction between data and algorithm and serves as a minimal condition for generalization.
* We derive a sufficient condition for compatibility in solving overparameterized linear regression with gradient descent. Our theory shows that generalization of early-stopping iterates requires much weaker restrictions in the considered setting.
* Technically, we derive time-variant generalization bounds for overparameterized linear regression via data-dependent trajectory analysis. Empirically, we conduct the various experiments to verify the the theoretical results and demonstrate the benefits of early stopping.

## 2 Related Works

**Data-Dependent Techniques** mainly focus on the data distribution condition for generalization. One of the most popular data-dependent methods is uniform convergence [7; 29; 74; 75]. However, recent works [46; 47] point out that uniform convergence may not be powerful enough to explain generalization, because it may only yield inconsistent bound in even linear regression cases. Another line of works investigates benign overfitting, which mainly studies generalization of overfitting solutions [8; 21; 35; 68; 70; 76; 77].

**Algorithm-Dependent Techniques** measure the role of the algorithmic information in generalization. A line of works derives generalization bounds via algorithm stability [9; 12; 18; 19; 24; 31; 32; 45; 67]. A parallel line of works analyzes the implicit bias of algorithmic information [11; 25; 39; 40; 59; 64], which are mainly based on analyzing a specific data distribution (_e.g._, linear separable).

**Other Generalization Techniques.** Besides the techniques above, there are many other approaches. For example, PAC-Bayes theory performs well empirically and theoretically [16; 42; 43; 48; 49; 58; 61] and can yield non-vacuous bounds in deep learning regimes [50; 54]. Furthermore, there are other promising techniques including information theory [6; 56; 71] and compression-based bounds [3].

**Early Stopping** has the potential to improve generalization for various machine learning problems [5; 30; 33; 38; 53; 62; 69; 74]. A line of works studies the rate of early stopping in linear regression and kernel regression with different algorithms, _e.g._, gradient descent [72], stochastic gradient descent [15; 37; 51; 55; 66], gradient flow [2], conjugate gradient [10] and spectral algorithms [22; 36]. Beyond linear models, early-stopping is also effective for training deep neural networks [27; 34]. Another line of research focuses on the signal for early stopping [20; 52].

## 3 Preliminaries

In this section, we formally define compatibility between the data distribution and the training algorithm, starting from the basic notations.

### Notations

**Data Distribution.** Let \(\mathcal{D}\) denote the population distribution and \(\bm{z}\sim\mathcal{D}\) denote a data point sampled from distribution \(\mathcal{D}\). Usually, \(\bm{z}\) contains a feature and its corresponding response. Besides, we denote the dataset with \(n\) samples as \(\bm{Z}\triangleq\{\bm{z}_{i}\}_{i\in[n]}\), where \(\bm{z}_{i}\sim\mathcal{D}\) are i.i.d. sampled from distribution \(\mathcal{D}\).

**Loss and Excess Risk.** Let \(\ell(\bm{\theta};\bm{z})\) denote the loss on sample \(\bm{z}\) with parameter \(\bm{\theta}\in\mathbb{R}^{p}\). The corresponding population loss is defined as \(L(\bm{\theta};\mathcal{D})\triangleq\mathbb{E}_{\bm{z}\sim\mathcal{D}}\ell(\bm{ \theta};\bm{z})\). When the context is clear, we omit the dependency on \(\mathcal{D}\) and denote the population loss by \(L(\bm{\theta})\). Our goal is to find the optimal parameter \(\bm{\theta}^{*}\) which minimizes the population loss, i.e., \(L(\bm{\theta}^{*})=\min_{\bm{\theta}}L(\bm{\theta})\). Measuring how a parameter \(\bm{\theta}\) approaches \(\bm{\theta}^{*}\) relies on a term _excess risk_\(R(\bm{\theta})\), defined as \(R(\bm{\theta})\triangleq L(\bm{\theta})-L(\bm{\theta}^{*})\).

**Algorithm.** Let \(\mathcal{A}(\cdot)\) denote a iterative algorithm that takes training data \(\bm{Z}\) as input and outputs a sequence of parameters \(\{\bm{\theta}_{n}^{(t)}\}_{t\geq 0}\), where \(t\) is the iteration number. The algorithm can be either deterministic or stochastic, _e.g._, variants of (S)GD.

### Definitions of Compatibility

Based on the above notations, we introduce the notion of compatibility between data distribution and algorithm in Definition 3.1. Informally, compatibility measures whether a consistent excess risk can be reached along the training trajectory. Note that we omit the role of the loss function in the definition, although the algorithm depends on the loss function.

**Definition 3.1** (Compatibility).: _Given a loss function \(\ell(\cdot)\) with corresponding excess risk \(R(\cdot)\), a data distribution \(\mathcal{D}\) is compatible with an algorithm \(\mathcal{A}\) if there exists nonempty subsets \(T_{n}\) of \(\mathbb{N}\), such that \(\sup_{t\in T_{n}}R(\bm{\theta}_{n}^{(t)})\) converges to zero in probability as sample size \(n\) tends to infinity, where \(\{\bm{\theta}_{n}^{(t)}\}_{t\geq 0}\) denotes the output of algorithm \(\mathcal{A}\), and the randomness comes from the sampling of training data \(\bm{Z}\) from distribution \(\mathcal{D}\) and the execution of algorithm \(\mathcal{A}\). That is to say, \((\mathcal{D},\mathcal{A})\) is compatible if there exists nonempty sets \(T_{n}\), such that_

\[\sup_{t\in T_{n}}R(\bm{\theta}_{n}^{(t)})\overset{P}{\rightarrow}0\quad\text {as}\quad n\rightarrow\infty.\] (1)

_We call \(\{T_{n}\}_{n>0}\) the compatibility region of \((\mathcal{D},\mathcal{A})\). The distribution \(\mathcal{D}\) is allowed to change with \(n\). In this case, \(\mathcal{D}\) should be understood as a sequence of distributions \(\{\mathcal{D}_{n}\}_{n\geq 1}\). We also allow the dimension of model parameter \(\bm{\theta}\) to be infinity or to grow with \(n\). We omit this dependency on \(n\) when the context is clear._

Compatibility serves as a minimal condition for generalization, since if a data distribution is incompatible with the algorithm, one cannot expect to reach a small excess risk even if we allow for _arbitrary_ early stopping. However, we remark that considering only the minimal excess risk is insufficient for a practical purpose, as one cannot exactly find the \(t\) that minimizes \(R(\bm{\theta}_{n}^{(t)})\) due to the noise in the validation set. Therefore, it is meaningful to consider a region of time \(t\) on which the excess risk is consistent as in Definition 3.1. The larger the region is, the more robust the algorithm will be to the noise in its execution.

**Comparisons with Other Notions.** Compared to classic definitions of learnability, _e.g._, PAC learning, the definition of compatibility is data-specific and algorithm-specific, and is thus a more fine-grained notion. Compared to the concept of _benign_ proposed in [8], which studies whether the excess risk at \(t=\infty\) converges to zero in probability as the sample size goes to infinity, compatibility only requires that there exists a time to derive a consistent excess risk. We will show later in Section 4.2 that in the overparameterized linear regression setting, there exist cases such that the problem instance is compatible but not benign.

## 4 Analysis of Overparameterized Linear Regression with Gradient Descent

To validate the meaningfulness of compatibility, we study it in the overparameterized linear regression regime. We first introduce the data distribution, loss, and training algorithm, and then present the main theorem, which provides a sufficient condition for compatibility in this setting.

### Preliminaries for Overparameterized Linear Regression

**Notations.** Let \(O,o,\Omega,\omega\) denote asymptotic notations, with their usual meaning. For example, the argument \(a_{n}=O(b_{n})\) means that there exists a large enough constant \(C\), such that \(a_{n}\leq Cb_{n}\). We use \(\lesssim\) with the same meaning as the asymptotic notation \(O\). Besides, let \(\|\bm{x}\|\) denote the \(\ell_{2}\) norm for vector \(\bm{x}\), and \(\|\bm{A}\|\) denote the operator norm for matrix \(\bm{A}\). We allow the vector to belong toa countably infinite-dimensional Hilbert space \(\mathcal{H}\), and with a slight abuse of notation, we use \(\mathbb{R}^{\infty}\) interchangeably with \(\mathcal{H}\). In this case, \(x^{\top}z\) denotes inner product and \(xz^{\top}\) denotes tensor product for \(x,z\in\mathcal{H}\). A random variable \(X\) is called \(\sigma\)-subgaussian if \(\mathbb{E}[e^{\lambda X}]\leq e^{\lambda^{2}\sigma^{2}/2}\) for any \(\lambda\).

**Data Distribution.** Let \((\bm{x},y)\in\mathbb{R}^{p}\times\mathbb{R}\) denote the feature vector and the response, following a joint distribution \(\mathcal{D}\). Let \(\bm{\Sigma}\triangleq\mathbb{E}[\bm{x}\bm{x}^{\top}]\) denote the feature covariance matrix, whose eigenvalue decomposition is \(\bm{\Sigma}=\bm{V}\bm{\Lambda}\bm{V}^{\top}=\sum_{i>0}\lambda_{i}\bm{v}_{i} \bm{v}_{i}^{\top}\) with decreasing eigenvalues \(\lambda_{1}\geq\lambda_{2}\geq\cdots\). We make the following assumptions on the distribution of the feature vector.

**Assumption 1** (Assumptions on feature distribution).: _We assume that_

1. \(\mathbb{E}[\bm{x}]=0\)_._
2. \(\lambda_{1}>0,\sum_{i>0}\lambda_{i}<C\) _for some absolute constant_ \(C\)_._
3. _Let_ \(\tilde{\bm{x}}=\bm{\Lambda}^{-\frac{1}{2}}\bm{V}^{\top}\bm{x}\)_. The random vector_ \(\tilde{\bm{x}}\) _has independent_ \(\sigma_{x}\)_-subgaussian entries._

**Loss and Excess Risk.** We choose square loss as the loss function \(\ell\), i.e. \(\ell(\bm{\theta},(\bm{x},y))=1/2(y-\bm{x}^{\top}\bm{\theta})^{2}\). The corresponding population loss is denoted by \(L(\bm{\theta})=\mathbb{E}\ell(\bm{\theta},(\bm{x},y))\) and the optimal parameter is denoted by \(\bm{\theta}^{*}\triangleq\operatorname*{argmin}_{\bm{\theta}\in\mathbb{R}^{p} }L(\bm{\theta})\). We assume that \(\|\bm{\theta}^{*}\|<C\) for some absolute constant \(C\). If there are multiple such minimizers, we choose an arbitrary one and fix it thereafter. We focus on the excess risk of parameter \(\bm{\theta}\), defined as

\[R(\bm{\theta})=L(\bm{\theta})-L(\bm{\theta}^{*})=\frac{1}{2}(\bm{\theta}-\bm {\theta}^{*})^{\top}\bm{\Sigma}(\bm{\theta}-\bm{\theta}^{*}).\] (2)

Let \(\varepsilon=y-\bm{x}^{\top}\bm{\theta}^{*}\) denote the noise in data point \((\bm{x},y)\). The following assumptions involve the conditional distribution of the noise.

**Assumption 2** (Assumptions on noise distribution).: _We assume that_

1. _The conditional noise_ \(\varepsilon|\bm{x}\) _has zero mean._
2. _The conditional noise_ \(\varepsilon|\bm{x}\) _is_ \(\sigma_{y}\)_-subgaussian._

Note that both Assumption 1 and Assumption 2 are commonly considered in the related literatures [8, 68, 76].

**Training Set.** Given a training set \(\{(\bm{x}_{i},y_{i})\}_{1\leq i\leq n}\) with \(n\) pairs independently sampled from the population distribution \(\mathcal{D}\), we define \(\bm{X}\triangleq(\bm{x}_{1},\cdots,\bm{x}_{n})^{\top}\in\mathbb{R}^{n\times p}\) as the feature matrix, \(\bm{Y}\triangleq(y_{1},\cdots,y_{n})^{\top}\in\mathbb{R}^{n}\) as the corresponding noise vector, and \(\bm{\varepsilon}\triangleq\bm{Y}-\bm{X}\bm{\theta}^{*}\) as the residual vector. Let the singular value decomposition (SVD) of \(\bm{X}\) be \(\bm{X}=\bm{U}\tilde{\bm{\Lambda}}^{\frac{1}{2}}\bm{W}^{\top}\), with \(\tilde{\bm{\Lambda}}=\text{diag}\{\mu_{1}\,\cdots,\mu_{n}\}\in\mathbb{R}^{n \times n}\), \(\mu_{1}\geq\cdots\geq\mu_{n}\).

We consider the overparameterized regime where the feature dimension is larger than the sample size, namely, \(p>n\). In this regime, we assume that \(\text{rank}(\bm{X})=n\) almost surely as in Bartlett et al. [8]. This assumption is equivalent to the invertibility of \(XX^{\top}\).

**Assumption 3** (Linear independent training set).: _For any \(n<p\), we assume that the features in the training set \(\{\bm{x}_{1},\bm{x}_{2},\cdots,\bm{x}_{n}\}\) is linearly independent almost surely._

**Algorithm.** Given the dataset \((\bm{X},\bm{Y})\), define the empirical loss function as \(\hat{L}(\bm{\theta})\triangleq\frac{1}{2n}\|\bm{X}\bm{\theta}-\bm{Y}\|^{2}\). We choose full-batch gradient descent on the empirical risk with a constant learning rate \(\lambda\) as the algorithm \(\mathcal{A}\) in the previous template. In this case, the update rule for the optimization trajectory \(\{\bm{\theta}_{t}\}_{t\geq 0}\) is formulated as

\[\bm{\theta}_{t+1}=\bm{\theta}_{t}-\frac{\lambda}{n}\bm{X}^{\top}(\bm{X}\bm{ \theta}_{t}-\bm{Y}).\] (3)

Without loss of generality, we consider zero initialization \(\bm{\theta}_{0}=\bm{0}\) in this paper. In this case, for a sufficiently small learning rate \(\lambda\), \(\bm{\theta}_{t}\) converges to the _min-norm interpolator_\(\hat{\bm{\theta}}=\bm{X}^{\top}(\bm{X}\bm{X}^{\top})^{-1}\bm{Y}\) as \(t\) goes to infinity, which was well studied previously [8]. This paper takes one step further and discuss the excess risk along the entire training trajectory \(\{R(\bm{\theta}_{t})\}_{t\geq 0}\).

**Effective Rank and Effective Dimensions.** We define the effective ranks of the feature matrix \(\bm{\Sigma}\) as \(r(\bm{\Sigma})\triangleq\frac{\sum_{j\geq 0}\lambda_{i}}{\lambda_{1}}\) and \(R_{k}(\bm{\Sigma})\triangleq\frac{(\sum_{j\geq k}\lambda_{i})^{2}}{\sum_{j\leq k }\lambda_{i}^{2}}\). Our results depend on two notions of effective dimension of the feature covariance \(\bm{\Sigma}\), defined as

\[k_{0} \triangleq\min\left\{l\geq 0:\lambda_{l+1}\leq\frac{c_{0}\sum_{i>l} \lambda_{i}}{n}\right\},\] (4) \[k_{1} \triangleq\min\left\{l\geq 0:\lambda_{l+1}\leq\frac{c_{1}\sum_{i>0} \lambda_{i}}{n}\right\},\] (5)

where \(c_{0},c_{1}\) are constants independent of the dimension \(p\), sample size \(n\), and time \(t\)1. We omit the dependency of \(k_{0},k_{1}\) on \(c_{0},c_{1},n,\bm{\Sigma}\) when the context is clear.

Footnote 1: Constants may depend on \(\sigma_{x}\), and we omit this dependency thereafter for clarity.

### Main Theorem for Overparameterized Linear Regression with Gradient Descent

Next, we present the main result of this section, which provides a clean condition for compatibility between gradient descent and overparameterized linear regression.

**Theorem 4.1**.: _Consider the overparameterized linear regression setting defined in section 4.1. Let Assumption 1,2 and 3 hold. Assume the learning rate satisfies \(\lambda=O\left(\frac{1}{\operatorname{Tr}(\bm{\Sigma})}\right)\)._

* _If the covariance satisfies_ \(k_{0}=o(n),R_{k_{0}}(\bm{\Sigma})=\omega(n),\;r(\bm{\Sigma})=o(n)\)_, it is compatible in the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),\infty\right)\)_._
* _If the covariance satisfies_ \(k_{0}=O(n),k_{1}=o(n),r(\bm{\Sigma})=o(n)\)_, it is compatible in the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda} \right)\right)\)_._

The proof of Theorem 4.1 is given in Appendix A and sketched in Section 5. The theorem shows that gradient descent is compatible with overparameterized linear regression under some mild regularity conditions on the learning rate, effective rank and effective dimensions. The condition on the learning rate is natural for optimizing a smooth objective. We conjecture that the condition \(k_{0}=O(n)\) can not be removed in general cases, since the effective dimension \(k_{0}\) characterizes the concentration of the singular values of the data matrix \(\bm{X}\) and plays a crucial role in the excess risk of the gradient descent dynamics.

**Comparison with Benign Overfitting.** The paper [8] studies overparameterized linear regression and gives the condition for min-norm interpolator to generalize. They prove that the feature covariance \(\bm{\Sigma}\) is benign if and only if

\[k_{0}=o(n),\;R_{k_{0}}(\bm{\Sigma})=\omega(n),\;r(\bm{\Sigma})=o(n)\] (6)

As discussed in Section 3.2, benign problem instance also satisfies compatibility, since benign overfitting requires a stronger condition on \(k_{0}\) and an additional assumption on \(R_{k_{0}}(\bm{\Sigma})\). The following example shows that this inclusion relationship is strict.

**Example 4.1**.: _Under the same assumption as in Theorem 4.1, if the spectrum of \(\bm{\Sigma}\) satisfies_

\[\lambda_{k}=\frac{1}{k^{\alpha}},\] (7)

_for some \(\alpha>1\), we derive that \(k_{0}=\Theta(n)\). Therefore, this problem instance satisfies compatibility, but does not satisfy benign overfitting._

Example 4.1 shows the existence of a case where the early stopping solution can generalize but interpolating solution fails. Therefore, Theorem 4.1 can characterize generalization for a much wider range of problem instances.

Proof Sketch and Techniques

### A Time Variant Bound

We further introduce an additional type of effective dimension besides \(k_{0},k_{1}\), which is time variant and is utilized to track the optimization dynamics.

**Definition 5.1** (Effective Dimensions).: _Given a feature covariance matrix \(\bm{\Sigma}\), define the effective dimension \(k_{2}\) as_

\[k_{2}\triangleq\min\left\{l\geq 0:\sum_{i>l}\lambda_{i}+n\lambda_{l+1}\leq c_{2}c (t,n)\sum_{i>0}\lambda_{i}\right\},\] (8)

_where \(c_{2}\) is a constant independent of the dimension \(p\), sample size \(n\), and time \(t\). The term \(c(t,n)\) is a function to be discussed later. When the context is clear, we omit its dependencies on \(c_{2},c(t,n),n,\bm{\Sigma}\) and only denote it by \(k_{2}\)._

Based on the effective rank and effective dimensions defined above, we provide a time-variant bound in Theorem 5.1 for overparameterized linear regression, which further leads to the argument in Theorem 4.1. Compared to the existing bound [8], Theorem 5.1 focuses on investigating the role of training epoch \(t\) in the excess risk, to give a refined bias-variance decomposition.

**Theorem 5.1** (Time Variant Bound).: _Suppose Assumption 1, 2 and 3 hold. Fix a function \(c(t,n)\). Given \(\delta\in(0,1)\), assume that \(k_{0}\leq\frac{n}{c}\), \(\log\frac{1}{\delta}\leq\frac{n}{c}\), \(0<\lambda\leq\frac{1}{c\sum_{i>0}\lambda_{i}}\) for a large enough constant \(c\). Then with probability at least \(1-\delta\), we have for any \(t\in\mathbb{N}\),_

\[R(\bm{\theta}_{t})\lesssim B(\bm{\theta}_{t})+V(\bm{\theta}_{t}),\] (9)

_where_

\[B(\bm{\theta}_{t}) =\|\bm{\theta}^{*}\|^{2}\left(\frac{1}{\lambda t}+\|\bm{\Sigma} \|\max\left\{\sqrt{\frac{r(\bm{\Sigma})}{n}},\frac{r(\bm{\Sigma})}{n},\sqrt{ \frac{\log(\frac{1}{\delta})}{n}}\right\}\right),\] \[V(\bm{\theta}_{t}) =\sigma_{y}^{2}\log\left(\frac{1}{\delta}\right)\left(\frac{k_{1 }}{n}+\frac{k_{2}}{c(t,n)n}+c(t,n)\left(\frac{\lambda t}{n}\sum_{i>0}\lambda_ {i}\right)^{2}\right).\]

We provide a high-level intuition behind Theorem 5.1. We decompose \(R(\bm{\theta}_{t})\) into the bias term and the variance term. The variance term is then split into the leading part and tailing part based on the sprctrum of the feature covariance \(\bm{\Sigma}\). The eigenvalues in the tailing part will cause the variance term in the excess risk of the min-norm interpolating solution to be \(\Omega(1)\) for fast decaying spectrum, as is the case in [8]. However, since the convergence in the tailing eigenspace is slower compared with the leading eigenspace, a proper early stopping strategy will prevent the overfitting in the tailing eigenspace and meanwhile avoid underfitting in the leading eigenspace.

**The \(c(t,n)\) Principle.** It is worth emphasizing that our bound holds for arbitrary positive function \(c(t,n)\). Therefore, one can fine-tune the generalization bound by choosing a proper \(c(t,n)\). In the subsequent sections, we show how to derive consistent risk bounds for different time \(t\), based on different choices of \(c(t,n)\). We present the case of choosing a constant \(c(t,n)\) in the next section. We leave the case of choosing a varying \(c(t,n)\) to Appendix 5.3.

### Varying \(t\), Constant \(c(t,n)\)

Theorem 5.1 provides an excess risk upper bound uniformly for \(t\in\mathbb{N}\). However, it is still non-trivial to derive Theorem 4.1, where the remaining question is to decide the term \(c(t,n)\). The following corollary shows the generalization bound when setting \(c(t,n)\) to a constant.

**Corollary 5.1**.: _Let Assumption 1, 2 and 3 hold. Fix a constant \(c(t,n)\). Suppose \(k_{0}=O(n)\), \(k_{1}=o(n)\), \(r(\bm{\Sigma})=o(n)\), \(\lambda=O\left(\frac{1}{\sum_{i>0}\lambda_{i}}\right)\). Then there exists a sequence of positive constants \(\{\delta_{n}\}_{n\geq 0}\) which converge to 0, such that with probability at least \(1-\delta_{n}\), the excess risk is consistent for \(t\in\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\), i.e._

\[R(\bm{\theta}_{t})=o(1).\] (10)_Furthermore, for any positive constant \(\delta\), with probability at least \(1-\delta\), the minimal excess risk on the training trajectory can be bounded as_

\[\min_{t}R(\bm{\theta}_{t})\lesssim\frac{\max\{\sqrt{r(\bm{\Sigma})},1\}}{\sqrt{n} }+\frac{\max\{k_{1},1\}}{n}.\] (11)

Lemma 5.1 below shows that \(k_{1}=o(n)\) always holds for fixed distribution. Therefore, combining Corollary 5.1 and the following Lemma 5.1 completes the proof of Theorem 4.1.

**Lemma 5.1**.: _For any fixed (i.e. independent of sample size \(n\)) feature covariance \(\bm{\Sigma}\) satisfying assumption 1, we have \(k_{1}(n)=o(n)\)._

Next we apply the bound in Corollary 5.1 to several data distributions. These distributions are widely discussed in [8, 76]. We also derive the existing excess risk bounds, which focus on the min-norm interpolator [8] and one-pass SGD iterate [76], of these distributions and compare them with our theorem. The results are summarized in Table 1, which shows that the bound in Corollary 5.1 outperforms previous results for a general class of distributions.

**Example 5.1**.: _Under the same conditions as Theorem 5.1, let \(\bm{\Sigma}\) denote the feature covariance matrix. We show the following examples:_

1. **(Inverse Polynomial).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \(\lambda_{k}=\frac{1}{k^{\alpha}}\) _for some_ \(\alpha>1\)_, we derive that_ \(k_{0}=\Theta(n)\)_,_ \(k_{1}=\Theta\left(n^{\frac{1}{\alpha}}\right)\)_. Therefore,_ \(\min_{t}V(\bm{\theta}_{t})=O\left(n^{\frac{1-\alpha}{\alpha}}\right)\) _and_ \(\min_{t}R(\bm{\theta}_{t})=O\left(n^{-\min\left\{\frac{\alpha-1}{\alpha},\frac {1}{2}\right\}}\right)\)_._
2. **(Inverse Log-Polynomial).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \(\lambda_{k}=\frac{1}{k\log^{q}(k+1)}\) _for some_ \(\beta>1\)_, we derive that_ \(k_{0}=\Theta\left(\frac{n}{\log n}\right)\)_,_ \(k_{1}=\Theta\left(\frac{n}{\log^{q}n}\right)\)_. Therefore,_ \(\min_{t}V(\bm{\theta}_{t})=O\left(\frac{1}{\log^{q}n}\right)\) _and_ \(\min_{t}R(\bm{\theta}_{t})=O\left(\frac{1}{\log^{q}n}\right)\)_._
3. **(Constant).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \(\lambda_{k}=\frac{1}{n^{1+\varepsilon}},1\leq k\leq n^{1+\varepsilon}\)_, for some_ \(\varepsilon>0\)_, we derive that_ \(k_{0}=0\)_,_ \(k_{1}=0\)_. Therefore,_ \(\min_{t}V(\bm{\theta}_{t})=O\left(\frac{1}{n}\right)\) _and_ \(\min_{t}R(\bm{\theta}_{t})=O\left(\frac{1}{\sqrt{n}}\right)\)_._
4. **(Piecewise Constant).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \(\lambda_{k}=\begin{cases}\frac{1}{s}&1\leq k\leq s,\\ \frac{1}{d-s}&s+1\leq k\leq d,\end{cases}\) _where_ \(s=n^{r},d=n^{q},0<r\leq 1,q\geq 1\)_. We derive that_ \(k_{0}=n^{r}\)_,_ \(k_{1}=n^{r}\)_. Therefore,_ \(\min_{t}V(\bm{\theta}_{t})=O(n^{r-1})\) _and_ \(\min_{t}R(\bm{\theta}_{t})=O\left(n^{-\min\left\{1-r,\frac{1}{2}\right\}} \right).\)

\begin{table}
\begin{tabular}{l c c c} \hline \hline Distributions & Ours & Bartlett et al. [8] & Zou et al. [76] \\ \hline \hline Inverse polynomial & \(O\left(n^{-\min\left\{\frac{\alpha-1}{\alpha},\frac{1}{2}\right\}}\right)\) & \(O(1)\) & \(O\left(n^{-\frac{\alpha-1}{\alpha}}\right)\) \\ \hline Inverse log polynomial & \(O\left(\frac{1}{\log^{q}n}\right)\) & \(o(1)\) & \(O\left(\frac{1}{\log^{q}n}\right)\) \\ \hline Constant & \(O\left(n^{-\frac{1}{2}}\right)\) & \(O\left(n^{-\min\left\{\varepsilon,\frac{1}{2}\right\}}\right)\) & \(O\left(n^{-\min\left\{\varepsilon,1\right\}}\right)\) \\ \hline Piecewise constant & \(O\left(n^{-\min\left\{1-r,\frac{1}{2}\right\}}\right)\) & \(O\left(n^{-\min\left\{1-r,q-1,\frac{1}{2}\right\}}\right)O\left(n^{-\min\left\{ 1-r,q-1\right\}}\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparisons of excess risk bound with Bartlett et al. [8] and Zou et al. [76].** We provide four types of feature covariance with eigenvalues \(\lambda_{k}\), including _Inverse Polynomial_ (\(\lambda_{k}=\frac{1}{k^{\alpha}}\), \(\alpha>1\)), _Inverse Log Polynomial_ (\(\lambda_{k}=\frac{1}{k\log^{2}(k+1)}\), \(\beta>1\)), _Constant_ (\(\lambda_{k}=\frac{1}{n^{1+\varepsilon}}\), \(1\leq k\leq n^{1+\varepsilon}\), \(\varepsilon>0\)), and _Piecewise Constant_ (\(\lambda_{k}=\frac{1}{s}\) if \(1\leq k\leq s\) and \(\lambda_{k}=\frac{1}{d-s}\) if \(s+1\leq k\leq d\), where \(s=n^{r},d=n^{q},0<r\leq 1,q>1\)). In light of these bounds, ours outperforms Bartlett et al. [8] in all the cases, and outperforms Zou et al. [76] in Constant / Piecewise Constant cases if \(\varepsilon<\frac{1}{2}\) and \(q<\min\{2-r,\frac{3}{2}\}\). We refer to Appendix B for more details.

### Varying \(t\), Varying \(c(t,n)\)

Although setting \(c(t,n)\) to a constant as in Corollary 5.1 suffices to prove Theorem 4.1, in this section we show that the choice of \(c(t,n)\) can be much more flexible. Specifically, we provide a concrete example and demonstrate that by setting \(c(t,n)\) to a non-constant, Theorem 5.1 can indeed produce larger compatibility regions.

**Example 5.2**.: _Under the same conditions as Theorem 5.1, let \(\bm{\Sigma}\) denote the feature covariance matrix. If the spectrum of \(\bm{\Sigma}\) satisfies \(\lambda_{k}=\frac{1}{k^{\alpha}}\) for some \(\alpha>1\), we set \(c(t,n)=\Theta\left(n^{\frac{\alpha+1-2\alpha\tau}{2\alpha+1}}\right)\) for a given \(\frac{\alpha+1}{2\alpha}\leq\tau\leq\frac{3\alpha+1}{2\alpha+2}\). Then for \(t=\Theta(n^{\tau})\), we derive that \(V(\bm{\theta}_{t})=O\left(n^{\frac{2\alpha-3\alpha+2\tau-1}{2\alpha+1}}\right)\)._

Example 5.2 shows that by choosing \(c(t,n)\) as a non-constant, we exploit the full power of Theorem 5.1, and extend the region to \(t=\Theta\left(n^{\frac{3\alpha+1}{2\alpha+2}}\right)=\omega(n)\). In this example, Theorem 5.1 outperforms all \(O\left(\frac{t}{n}\right)\)-type bounds, which become vacuous when \(t=\omega(n)\).

## 6 Experiments

In this section, we provide numerical studies of overparameterized linear regression problems. We consider overparameterized linear regression instances with input dimension \(p=1000\), sample size \(n=100\). The features are sampled from Gaussian distribution with different covariances. The empirical results **(a.) demonstrate the benefits of trajectory analysis underlying the definition of compatibility,** since the optimal excess risk along the algorithm trajectory is significantly lower than that of the min-norm interpolator **(b.) validate the statements in Corollary 5.1**, since the optimal excess risk is lower when the eigenvalues of feature covariance decay faster. We refer to Appendix C for detailed setups, additional results and discussions.

**Observation One: Early stopping solution along the training trajectory generalizes significantly better than the min-norm interpolator.** We calculate the excess risk of optimal early stopping solutions and min-norm interpolators from 1000 independent trials and list the results in Table 2. The results illustrate that the early stopping solution on the algorithm trajectory enjoys much better generalization properties. This observation corroborates the importance of data-dependent training trajectory in generalization analysis.

**Observation Two: The faster covariance spectrum decays, the lower optimal excess risk is.** Table 2 also illustrates a positive correlation between the decaying rate of \(\lambda_{i}\) and the generalization performance of the early stopping solution. This accords with Theorem 5.1, showing that the excess risk is better for a smaller effective dimension \(k_{1}\), where small \(k_{1}\) indicates a faster-decaying eigenvalue \(\lambda_{i}\). We additionally note that such a phenomenon also illustrates the difference between min-norm and early stopping solutions in linear regression, since Bartlett et al. [8] demonstrate that the min-norm solution is not consistent when the eigenvalues decay too fast. By comparison, early stopping solutions do not suffer from this restriction.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Distributions & \(k_{1}\) & Optimal Excess Risk & Min-norm Excess Risk \\ \hline \(\lambda_{i}=\frac{1}{i}\) & \(\Theta\left(n\right)\) & \(2.399\pm 0.0061\) & \(24.071\pm 0.2447\) \\ \(\lambda_{i}=\frac{1}{i^{2}}\) & \(\Theta\left(n^{\frac{1}{2}}\right)\) & \(0.214\pm 0.0050\) & \(43.472\pm 0.6463\) \\ \(\lambda_{i}=\frac{1}{i^{3}}\) & \(\Theta\left(n^{\frac{1}{3}}\right)\) & \(0.077\pm 0.0005\) & \(10.401\pm 0.2973\) \\ \(\lambda_{i}=\frac{1}{i\log(i+1)}\) & \(\Theta\left(\frac{n}{\log n}\right)\) & \(0.697\pm 0.0053\) & \(89.922\pm 0.9591\) \\ \(\lambda_{i}=\frac{1}{i\log^{2}(i+1)}\) & \(\Theta\left(\frac{n}{\log^{2}n}\right)\) & \(0.298\pm 0.0054\) & \(82.413\pm 0.9270\) \\ \(\lambda_{i}=\frac{1}{i\log^{3}(i+1)}\) & \(\Theta\left(\frac{n}{\log^{3}n}\right)\) & \(0.187\pm 0.0047\) & \(38.145\pm 0.5862\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **The effective dimension \(k_{1}\), the optimal early stopping excess risk, and the min-norm excess risk for different feature distributions, with sample size \(n=100\), \(p=1000\). The table shows that early stopping solutions generalize significantly better than min-norm interpolators, and reveals a positive correlation between the effective dimension \(k_{1}\) and excess risk of early stopping solution. We calculate the 95% confidence interval for each excess risk.**Conclusion

In this paper, we investigate how to characterize and analyze generalization in a data-dependent and algorithm-dependent manner. We formalize the notion of data-algorithm compatibility and study it under the regime of overparameterized linear regression with gradient descent. Our theoretical and empirical results demonstrate that one can ease the assumptions and broaden the scope of generalization by fully exploiting the data information and the algorithm information. Despite linear cases in this paper, compatibility can be a much more general concept. Therefore, we believe this paper will motivate more work on data-dependent trajectory analysis.

## Acknowledgement

The authors would like to acknowledge the support from the 2030 Innovation Megaprojects of China (Programme on New Generation Artificial Intelligence) under Grant No. 2021AAA0150000.

## References

* Advani et al. [2020] Madhu S. Advani, Andrew M. Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. _Neural Networks_, 132:428-446, 2020. doi: 10.1016/j.neunet.2020.08.022. URL https://doi.org/10.1016/j.neunet.2020.08.022.
* Ali et al. [2019] Alnur Ali, J. Zico Kolter, and Ryan J. Tibshirani. A continuous-time view of early stopping for least squares regression. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan_, volume 89 of _Proceedings of Machine Learning Research_, pages 1370-1378. PMLR, 2019. URL http://proceedings.mlr.press/v89/ali19a.html.
* Arora et al. [2018] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 254-263. PMLR, 2018. URL http://proceedings.mlr.press/v80/arora18b.html.
* Arora et al. [2019] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 322-332. PMLR, 2019. URL http://proceedings.mlr.press/v97/arora19a.html.
* Bai et al. [2021] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels. _CoRR_, abs/2106.15853, 2021. URL https://arxiv.org/abs/2106.15853.
* Banerjee and Montufar [2021] Pradeep Kr. Banerjee and Guido Montufar. Information complexity and generalization bounds. _CoRR_, abs/2105.01747, 2021. URL https://arxiv.org/abs/2105.01747.
* Bartlett et al. [2017] Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6240-6249, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/b22b257ad0519d4500539da3c8bcf4dd-Abstract.html.
* Bartlett et al. [2019] Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _CoRR_, abs/1906.11300, 2019. URL http://arxiv.org/abs/1906.11300.

* Bassily et al. [2020] Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient descent on nonsmooth convex losses. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Abstract.html.
* Blanchard and Kramer [2016] Gilles Blanchard and Nicole Kramer. Convergence rates of kernel conjugate gradient for random design regression. _Analysis and Applications_, 14(06):763-794, 2016.
* Bousquet and Elisseeff [2002] Olivier Bousquet and Andre Elisseeff. Stability and generalization. _J. Mach. Learn. Res._, 2:499-526, 2002. URL http://jmlr.org/papers/v2/busquet02a.html.
* Bousquet et al. [2020] Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Jacob D. Abernethy and Shivani Agarwal, editors, _Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria]_, volume 125 of _Proceedings of Machine Learning Research_, pages 610-626. PMLR, 2020. URL http://proceedings.mlr.press/v125/busquet20b.html.
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.
* Dieuleveut and Bach [2016] Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. _The Annals of Statistics_, 44(4):1363-1399, 2016.
* Dziugaite and Roy [2017] Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Gal Elidan, Kristian Kersting, and Alexander T. Ihler, editors, _Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017_. AUAI Press, 2017. URL http://auai.org/uai2017/proceedings/papers/173.pdf.
* Emami et al. [2020] Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, and Alyson K. Fletcher. Generalization error of generalized linear models in high dimensions. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 2892-2901. PMLR, 2020. URL http://proceedings.mlr.press/v119/emami20a.html.
* Feldman and Vondrak [2018] Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 9770-9780, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/05a624166c8eb8273b8464e8d9eb5bd9-Abstract.html.

* Feldman and Vondrak [2019] Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Alina Beygelzimer and Daniel Hsu, editors, _Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA_, volume 99 of _Proceedings of Machine Learning Research_, pages 1270-1279. PMLR, 2019. URL http://proceedings.mlr.press/v99/feldman19a.html.
* European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part II_, volume 12976 of _Lecture Notes in Computer Science_, pages 217-232. Springer, 2021. doi: 10.1007/978-3-030-86520-7_14. URL https://doi.org/10.1007/978-3-030-86520-7_14.
* Frei et al. [2022] Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. _CoRR_, abs/2202.05928, 2022. URL https://arxiv.org/abs/2202.05928.
* Lo Gerfo et al. [2008] L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral algorithms for supervised learning. _Neural Comput._, 20(7):1873-1897, 2008. doi: 10.1162/neco.2008.05-07-517. URL https://doi.org/10.1162/neco.2008.05-07-517.
* Goldt et al. [2019] Sebastian Goldt, Madhu Advani, Andrew M. Saxe, Florent Krzakala, and Lenka Zdeborova. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 6979-6989, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/cab070d53bd0d200746fb852a922064a-Abstract.html.
* Hardt et al. [2016] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 1225-1234. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/hardt16.html.
* Hu et al. [2020] Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/c6dfc6b7c601ac2978357b7a81e2d7ae-Abstract.html.
* Jacot et al. [2018] Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 8580-8589, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html.
* Ji et al. [2021] Ziwei Ji, Justin D. Li, and Matus Telgarsky. Early-stopped neural networks are consistent. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 1805-1817, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/0e1ebad68af7f0ae4830b7ac92bc3c6f-Abstract.html.
* Jiang et al. [2020] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020._ OpenReview.net, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH.

* [29] Vladimir Koltchinskii and Dmitriy Panchenko. Rademacher processes and bounding the risk of function learning. In _High dimensional probability II_, pages 443-457. Springer, 2000.
* [30] Ilja Kuzborskij and Csaba Szepesvari. Nonparametric regression with shallow overparameterized neural networks trained by GD with early stopping. In Mikhail Belkin and Samory Kpotufe, editors, _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 2853-2890. PMLR, 2021. URL http://proceedings.mlr.press/v134/kuzborskij21a.html.
* [31] Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic gradient descent. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 5809-5819. PMLR, 2020. URL http://proceedings.mlr.press/v119/lei20c.html.
* [32] Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods for non-convex learning. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL https://openreview.net/forum?id=SkxxtgHKPS.
* [33] Jiangyuan Li, Thanh V. Nguyen, Chinmay Hegde, and Ka Wai Wong. Implicit sparse regularization: The impact of depth and early stopping. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 28298-28309, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ee188463935a061ee6df8bf449cb882-Abstract.html.
* [34] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In Silvia Chiappa and Roberto Calandra, editors, _The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]_, volume 108 of _Proceedings of Machine Learning Research_, pages 4313-4324. PMLR, 2020. URL http://proceedings.mlr.press/v108/li20j.html.
* [35] Zhu Li, Weijie Su, and Dino Sejdinovic. Benign overfitting and noisy features. _CoRR_, abs/2008.02901, 2020. URL https://arxiv.org/abs/2008.02901.
* [36] Junhong Lin and Volkan Cevher. Optimal rates for spectral-regularized algorithms with least-squares regression over hilbert spaces. _CoRR_, abs/1801.06720, 2018. URL http://arxiv.org/abs/1801.06720.
* [37] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods. _J. Mach. Learn. Res._, 18:97:1-97:47, 2017. URL http://jmlr.org/papers/v18/17-176.html.
* [38] Chaoyue Liu, Amirhesam Abedsoltan, and Mikhail Belkin. On emergence of clean-priority learning in early stopped neural networks. _CoRR_, abs/2306.02533, 2023. doi: 10.48550/arXiv.2306.02533. URL https://doi.org/10.48550/arXiv.2306.02533.
* [39] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL https://openreview.net/forum?id=SJeLlgBKPS.
* [40] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 12978-12991, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/6c351da15b5e8a743a21ee96a86e25df-Abstract.html.

* Mallinar et al. [2022] Neil Mallinar, James B. Simon, Amirhesam Abedsolotlan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html.
* McAllester [2003] David A. McAllester. Simplified pac-bayesian margin bounds. In Bernhard Scholkopf and Manfred K. Warmuth, editors, _Computational Learning Theory and Kernel Machines, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003, Proceedings_, volume 2777 of _Lecture Notes in Computer Science_, pages 203-215. Springer, 2003. doi: 10.1007/978-3-540-45167-9_16. URL https://doi.org/10.1007/978-3-540-45167-9_16.
* McAllester [2013] David A. McAllester. A pac-bayesian tutorial with A dropout bound. _CoRR_, abs/1307.2118, 2013. URL http://arxiv.org/abs/1307.2118.
* Mohri et al. [2012] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. Adaptive computation and machine learning. MIT Press, 2012. ISBN 978-0-262-01825-8. URL http://mitpress.mit.edu/books/foundations-machine-learning-0.
* Mou et al. [2018] Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018_, volume 75 of _Proceedings of Machine Learning Research_, pages 605-638. PMLR, 2018. URL http://proceedings.mlr.press/v75/mou18a.html.
* Nagarajan and Kolter [2019] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 11611-11622, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/05e97c207235dc63ceb1db43c60db7bbb-Abstract.html.
* Negrea et al. [2020] Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 7263-7272. PMLR, 2020. URL http://proceedings.mlr.press/v119/negrea20a.html.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=Skx_WfbCZ.
* Parrado-Hernandez et al. [2012] Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. Pac-bayes bounds with data dependent priors. _J. Mach. Learn. Res._, 13:3507-3531, 2012. URL http://dl.acm.org/citation.cfm?id=2503353.
* Perez-Ortiz et al. [2021] Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates for neural networks. _J. Mach. Learn. Res._, 22:227:1-227:40, 2021. URL http://jmlr.org/papers/v22/20-879.html.
* Pillaud-Vivien et al. [2018] Loucas Pillaud-Vivien, Alessandro Rudi, and Francis R. Bach. Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 8125-8135, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/10ff0b5e85e5b85cc3095d431d8c08b4-Abstract.html.

- but when? In Gregoire Montavon, Genevieve B. Orr, and Klaus-Robert Muller, editors, _Neural Networks: Tricks of the Trade
- Second Edition_, volume 7700 of _Lecture Notes in Computer Science_, pages 53-67. Springer, 2012. doi: 10.1007/978-3-642-35289-8_5. URL https://doi.org/10.1007/978-3-642-35289-8_5.
* Raskutti et al. [2014] Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. _J. Mach. Learn. Res._, 15(1):335-366, 2014. doi: 10.5555/2627435.2627446. URL https://dl.acm.org/doi/10.5555/2627435.2627446.
* Rivasplata et al. [2020] Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, and John Shawe-Taylor. Pac-bayes analysis beyond the usual bounds. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html.
* Rosasco and Villa Villa [2015] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 1630-1638, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/1587965fb4d4b5afe8428a4a024feb0d-Abstract.html.
* Russo and Zou [2016] Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In Arthur Gretton and Christian C. Robert, editors, _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016_, volume 51 of _JMLR Workshop and Conference Proceedings_, pages 1232-1240. JMLR.org, 2016. URL http://proceedings.mlr.press/v51/russo16.html.
* Sahraee-Ardakan et al. [2022] Mojtaba Sahraee-Ardakan, Melikasadat Emami, Parthe Pandit, Sundeep Rangan, and Alyson K. Fletcher. Kernel methods and multi-layer perceptrons learn linear models in high dimensions. _CoRR_, abs/2201.08082, 2022. URL https://arxiv.org/abs/2201.08082.
* Seeger [2002] Matthias W. Seeger. Pac-bayesian generalisation error bounds for gaussian process classification. _J. Mach. Learn. Res._, 3:233-269, 2002. URL http://jmlr.org/papers/v3/seeger02a.html.
* Shah et al. [2020] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2aoef2ae1067d915-Abstract.html.
* Shankar et al. [2020] Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and Ludwig Schmidt. Evaluating machine accuracy on imagenet. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 8634-8644. PMLR, 2020. URL http://proceedings.mlr.press/v119/shankar20c.html.
* Shawe-Taylor and Williamson [1997] John Shawe-Taylor and Robert C. Williamson. A PAC analysis of a bayesian estimator. In Yoav Freund and Robert E. Schapire, editors, _Proceedings of the Tenth Annual Conference on Computational Learning Theory, COLT 1997, Nashville, Tennessee, USA, July 6-9, 1997_, pages 2-9. ACM, 1997. doi: 10.1145/267460.267466. URL https://doi.org/10.1145/267460.267466.
* Shen et al. [2022] Ruoqi Shen, Liyao Gao, and Yi-An Ma. On optimal early stopping: Over-informative versus under-informative parametrization. _CoRR_, abs/2202.09885, 2022. URL https://arxiv.org/abs/2202.09885.

* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. _Nat._, 550(7676):354-359, 2017. doi: 10.1038/nature24270. URL https://doi.org/10.1038/nature24270.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=r1q7n9gAb.
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VI_, volume 11210 of _Lecture Notes in Computer Science_, pages 504-519. Springer, 2018. doi: 10.1007/978-3-030-01231-1_31. URL https://doi.org/10.1007/978-3-030-01231-1_31.
* Tarres and Yao [2014] Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence. _IEEE Trans. Inf. Theory_, 60(9):5716-5735, 2014. doi: 10.1109/TIT.2014.2332531. URL https://doi.org/10.1109/TIT.2014.2332531.
* Teng et al. [2021] Jiaye Teng, Jianhao Ma, and Yang Yuan. Towards understanding generalization via decomposing excess risk dynamics. _CoRR_, abs/2106.06153, 2021. URL https://arxiv.org/abs/2106.06153.
* Tsigler and Bartlett [2020] Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. _arXiv preprint arXiv:2009.14286_, 2020.
* Vaskevicius et al. [2020] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. The statistical complexity of early-stopped mirror descent. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/024d2d699e6c1a82c9ba986386f4d824-Abstract.html.
* Wang and Thrampoulidis [2021] Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of gaussian mixtures. In _IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021_, pages 4030-4034. IEEE, 2021. doi: 10.1109/ICASSP39728.2021.9413946. URL https://doi.org/10.1109/ICASSP39728.2021.9413946.
* Xu and Raginsky [2017] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 2524-2533, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html.
* Yao et al. [2007] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. _Constructive Approximation_, 26(2):289-315, 2007.
* Yun et al. [2021] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and Sanghyuk Chun. Re-labeling imagenet: From single to multi-labels, from global to localized labels. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_, pages 2340-2350. Computer Vision Foundation / IEEE, 2021. URL https://openaccess.thecvf.com/content/CVPR2021/html/Yun_Re-Labeling_ImageNet_From_Single_to_Multi-Labels_From_Global_to_Localized_CVPR_2021_paper.html.

* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Commun. ACM_, 64(3):107-115, 2021. doi: 10.1145/3446776. URL https://doi.org/10.1145/3446776.
* Zhou et al. [2020] Lijia Zhou, Danica J. Sutherland, and Nati Srebro. On uniform convergence and low-norm interpolation learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/4cc5400e63624c44fadeda9f57588a6-Abstract.html.
* Zou et al. [2021] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. Benign overfitting of constant-stepsize SGD for linear regression. In Mikhail Belkin and Samory Kpotufe, editors, _Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA_, volume 134 of _Proceedings of Machine Learning Research_, pages 4633-4635. PMLR, 2021. URL http://proceedings.mlr.press/v134/zou21a.html.
* Zou et al. [2022] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M. Kakade. Risk bounds of multi-pass SGD for least squares in the interpolation regime. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/543924ff260ba990f2ef84f940f3db2-Abstract-Conference.html.

**Supplementary Materials**

## Appendix A Proofs for the Main Results

We first sketch the proof in section A.1 and give some preliminary lemmas A.2. The following sections A.3, A.4 and A.5 are devoted to the proof of Theorem 5.1. The proof of Theorem 4.1 is given in A.6.

### Proof Sketch

We start with a standard bias-variance decomposition following Bartlett et al. [8], which derives that the time-variant excess risk \(R(\bm{\theta}_{t})\) can be bounded by a bias term and a variance term. We refer to Appendix A.3 for more details.

For the bias part, we first decompose it into an optimization error and an approximation error. For the optimization error, we use the spectrum analysis to bound it with \(O\left(1/t\right)\) where \(t\) denotes the time. For the approximation error, we bound it with \(O\left(1/\sqrt{n}\right))\) where \(n\) denotes the sample size, inspired by Bartlett et al. [8]. We refer to Appendix A.4 for more details.

For the variance part, a key step is to bound the term \((\bm{I}-\frac{\lambda}{n}\bm{X}\bm{X}^{\top})^{t}\), where \(\bm{X}\) is the feature matrix. The difficulty arises from the different scales of the eigenvalues of \(\bm{X}\bm{X}^{\top}\), where the largest eigenvalue has order \(\Theta(n)\) while the smallest eigenvalue has order \(O(1)\), according to Lemma 10 in Bartlett et al. [8]. To overcome this issue, we divide the matrix \(\bm{X}\bm{X}^{\top}\) based on whether its eigenvalues is larger than \(c(t,n)\), which is a flexible term dependent on time \(t\) and sample size \(n\). Therefore, we split the variance term based on eigenvalues of covariance matrix \(\bm{\Sigma}\) (leading to the \(k_{1}\)-related term) and based on the eigenvalues of \(\bm{X}\bm{X}^{\top}\) (leading to the \(k_{2}\)-related term). We refer to Appendix A.5 for more details.

### Preliminaries

The following result comes from Bartlett et al. [8], which bounds the eigenvalues of \(\bm{X}\bm{X}^{\top}\).

**Lemma A.1**.: _(Lemma 10 in Bartlett et al. [8]) For any \(\sigma_{x}\), there exists a constant \(c\), such that for any \(0\leq k<n\), with probability at least \(1-e^{-\frac{n}{c}}\),_

\[\mu_{k+1}\leq c\left(\sum_{i>k}\lambda_{i}+\lambda_{k+1}n\right).\] (12)

This implies that as long as the step size \(\lambda\) is small than a threshold independent of sample size \(n\), gradient descent is stable.

**Corollary A.1**.: _There exists a constant \(c\), such that with probability at least \(1-e^{-\frac{n}{c}}\), for any \(0\leq\lambda\leq\frac{1}{c\sum_{i>0}\lambda_{i}}\) we have_

\[\bm{O}\preceq\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\preceq\bm{I}.\] (13)

Proof.: The right hand side of the inequality is obvious since \(\lambda>0\). For the left hand side, we have to show that the eigenvalues of \(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\) is non-negative. since \(\bm{X}^{\top}\bm{X}\) and \(\bm{X}\bm{X}^{\top}\) have the same non-zero eigenvalues, we know that with probability at least \(1-e^{-\frac{n}{c}}\), the smallest eigenvalue of \(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\) can be lower bounded by

\[1-\frac{\lambda}{n}\mu_{1}\geq 1-c\lambda\left(\frac{\sum_{i>0}\lambda_{i}}{n }+\lambda_{k+1}\right)\geq 1-2c\lambda\sum_{i>0}\lambda_{i}\geq 0.\] (14)

where the second inequality uses lemma A.1, and the last inequality holds if \(\lambda\leq\frac{1}{2c\sum_{i>0}\lambda_{i}}\).

### Proof for the Bias-Variance Decomposition

Let \(\bm{X}^{\dagger}\) denote the Moore-Penrose pseudoinverse of matrix \(\bm{X}\). The following lemma gives a closed form expression for \(\bm{\theta}_{t}\).

**Lemma A.2**.: _The dynamics of \(\{\bm{\theta}_{t}\}_{t\geq 0}\) satisfies_

\[\bm{\theta}_{t}=\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t} \left(\bm{\theta}_{0}-\bm{X}^{\dagger}\bm{Y}\right)+\bm{X}^{\dagger}\bm{Y}.\] (15)

Proof.: We prove the lemma using induction. The equality holds at \(t=0\) as both sides are \(\bm{\theta}_{0}\). Recall that \(\bm{\theta}_{t}\) is updated as

\[\bm{\theta}_{t+1}=\bm{\theta}_{t}+\frac{\lambda}{n}\bm{X}^{\top}(\bm{Y}-\bm{ X}\bm{\theta}_{t}).\] (16)

Suppose that the dynamic holds up to the \(t\)-th step. Plug the expression for \(\bm{\theta}_{t}\) into the above recursion and note that \(\bm{X}^{\top}\bm{X}\bm{X}^{\dagger}=\bm{X}^{\top}\), we get

\[\bm{\theta}_{t+1} =\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)\bm{ \theta}_{t}+\frac{\lambda}{n}\bm{X}^{\top}\bm{Y}\] \[=\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t+1} \left(\bm{\theta}_{0}-\bm{X}^{\dagger}\bm{Y}\right)+\left(\bm{I}-\frac{ \lambda}{n}\bm{X}^{\top}\bm{X}\right)\bm{X}^{\dagger}\bm{Y}+\frac{\lambda}{n} \bm{X}^{\top}\bm{Y}\] (17) \[=\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t+1} \left(\bm{\theta}_{0}-\bm{X}^{\dagger}\bm{Y}\right)+\bm{X}^{\dagger}\bm{Y}.\]

which finishes the proof. 

Next we prove two identities which will be used in further proof.

**Lemma A.3**.: _The following two identities hold for any matrix \(X\) and non-negative integer \(t\):_

\[\bm{I}-\bm{X}^{\dagger}\bm{X}+\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{ X}\right)^{t}\bm{X}^{\dagger}\bm{X}=\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top} \bm{X}\right)^{t},\] (18)

\[\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t} \right]\bm{X}^{\dagger}\bm{X}\bm{X}^{\top}=\bm{X}^{\top}\left[\bm{I}-\left( \bm{I}-\frac{\lambda}{n}\bm{X}\bm{X}^{\top}\right)^{t}\right].\] (19)

Proof.: Note that \(\bm{X}^{\top}\bm{X}\bm{X}^{\dagger}=\bm{X}^{\top}\), we can expand the left hand side of the first identity above using binomial theorem and eliminate the pseudo-inverse \(\bm{X}^{\dagger}\):

\[\bm{I}-\bm{X}^{\dagger}\bm{X}+\left(\bm{I}-\frac{\lambda}{n}\bm{X }^{\top}\bm{X}\right)^{t}\bm{X}^{\dagger}\bm{X}\] \[=\bm{I}-\bm{X}^{\dagger}\bm{X}+\sum_{k=0}^{t}\binom{t}{k}\left(- \frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{k}\bm{X}^{\dagger}\bm{X}\] \[=\bm{I}-\bm{X}^{\dagger}\bm{X}+\bm{X}^{\dagger}\bm{X}+\sum_{k=1}^ {t}\binom{t}{k}\left(-\frac{\lambda}{n}\right)^{k}\left(\bm{X}^{\top}\bm{X} \right)^{k-1}\bm{X}^{\top}\bm{X}\bm{X}^{\dagger}\bm{X}\] (20) \[=\bm{I}+\sum_{k=1}^{t}\binom{t}{k}\left(-\frac{\lambda}{n}\right) ^{k}(\bm{X}^{\top}\bm{X})^{k}\] \[=\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}.\]The second identity can be proved in a similar way:

\[\begin{split}&\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{ \top}\bm{X}\right)^{t}\right]\bm{X}^{\dagger}\bm{X}\bm{X}^{\top}\\ &=-\sum_{k=1}^{t}\binom{t}{k}\left(-\frac{\lambda}{n}\bm{X}^{\top} \bm{X}\right)^{k}\bm{X}^{\dagger}\bm{X}\bm{X}^{\top}\\ &=-\sum_{k=1}^{t}\binom{t}{k}\left(-\frac{\lambda}{n}\right)^{k} \left(\bm{X}^{\top}\bm{X}\right)^{k-1}\bm{X}^{\top}\bm{X}\bm{X}^{\dagger}\bm{X} \bm{X}^{\top}\\ &=-\sum_{k=1}^{t}\binom{t}{k}\left(-\frac{\lambda}{n}\right)^{k} \left(\bm{X}^{\top}\bm{X}\right)^{k-1}\bm{X}^{\top}\bm{X}\bm{X}^{\top}\\ &=-\sum_{k=1}^{t}\binom{t}{k}\left(-\frac{\lambda}{n}\right)^{k} \bm{X}^{\top}(\bm{X}\bm{X}^{\top})^{k}\\ &=\bm{X}^{\top}\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X} \bm{X}^{\top}\right)^{t}\right].\end{split}\] (21)

We are now ready to prove the main result of this section.

**Lemma A.4**.: _The excess risk at the \(t\)-th epoch can be upper bounded as_

\[R(\bm{\theta}_{t})\leq\bm{\theta}^{*\top}\bm{B}\bm{\theta}^{*}+\bm{\varepsilon }^{\top}\bm{C}\bm{\varepsilon},\] (22)

_where_

\[\bm{B}=\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\bm{ \Sigma}\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t},\] (23)

\[\bm{C}=\left(\bm{X}\bm{X}^{\top}\right)^{-1}\left[\bm{I}-\left(\bm{I}-\frac{ \lambda}{n}\bm{X}\bm{X}^{\top}\right)^{t}\right]\bm{X}\bm{\Sigma}\bm{X}^{\top} \left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}\bm{X}^{\top}\right)^{t} \right]\left(\bm{X}\bm{X}^{\top}\right)^{-1},\] (24)

_which characterizes bias term and variance term in the excess risk. Furthermore, there exists constant \(c\) such that with probability at least \(1-\delta\) over the randomness of \(\bm{\varepsilon}\), we have_

\[\bm{\varepsilon}^{\top}\bm{C}\bm{\varepsilon}\leq c\sigma_{y}^{2}\log\frac{1} {\delta}\operatorname{Tr}[\bm{C}].\] (25)

Proof.: First note that \(\bm{X}\bm{X}^{\top}\) is invertible by Assumption 3. Express the excess risk as follows

\[\begin{split} R(\bm{\theta}_{t})&=\frac{1}{2}\mathbb{ E}[(y-\bm{x}^{\top}\bm{\theta}_{t})^{2}-(y-\bm{x}^{\top}\bm{\theta}^{*})^{2}]\\ &=\frac{1}{2}\mathbb{E}[(y-\bm{x}^{\top}\bm{\theta}^{*}+\bm{x}^{ \top}\bm{\theta}^{*}-\bm{x}^{\top}\bm{\theta}_{t})^{2}-(y-\bm{x}^{\top}\bm{ \theta}^{*})^{2}]\\ &=\frac{1}{2}\mathbb{E}[(\bm{x}^{\top}(\bm{\theta}_{t}-\bm{ \theta}^{*}))^{2}+2(y-\bm{x}^{\top}\bm{\theta}^{*})(\bm{x}^{\top}\bm{\theta}^ {*}-\bm{x}^{\top}\bm{\theta}_{t})]\\ &=\frac{1}{2}\mathbb{E}[\bm{x}^{\top}(\bm{\theta}_{t}-\bm{ \theta}^{*})]^{2}.\end{split}\] (26)

Recall that \(\bm{\theta}_{0}=0\) and \(\bm{Y}=\bm{X}\bm{\theta}^{*}+\bm{\varepsilon}\) and we can further simplify the formula for \(\bm{\theta}_{t}\) in lemma A.2:

\[\begin{split}\bm{\theta}_{t}&=\left(\bm{I}-\frac{ \lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}(\bm{\theta}_{0}-\bm{X}^{\dagger}\bm{Y })+\bm{X}^{\dagger}\bm{Y}\\ &=\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X} \right)^{t}\right]\bm{X}^{\dagger}(\bm{X}\bm{\theta}^{*}+\bm{\varepsilon}). \end{split}\] (27)Plug it into the above expression for \(R(\bm{\theta}_{t})\), we have

\[R(\bm{\theta}_{t}) =\frac{1}{2}\mathbb{E}\left[\bm{x}^{\top}\left[\bm{I}-\left(\bm{I}- \frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\right]\bm{X}^{\dagger}(\bm{X} \bm{\theta}^{*}+\bm{\varepsilon})-\bm{x}^{\top}\bm{\theta}^{*}\right]^{2}\] \[=\frac{1}{2}\mathbb{E}\left[\bm{x}^{\top}\left(\bm{X}^{\dagger} \bm{X}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\bm{X}^{ \dagger}\bm{X}-\bm{I}\right)\bm{\theta}^{*}\right.\] (28) \[\quad\left.+\bm{x}^{\top}\left[\bm{I}-\left(\bm{I}-\frac{\lambda }{n}\bm{X}^{\top}\bm{X}\right)^{t}\right]\bm{X}^{\dagger}\bm{\varepsilon} \right]^{2}.\]

Applying lemma A.3, we obtain

\[R(\bm{\theta}_{t}) =\frac{1}{2}\mathbb{E}\left[-\bm{x}^{\top}\left(\bm{I}-\frac{ \lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\bm{\theta}^{*}+\bm{x}^{\top}\bm{X}^{ \top}\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}\bm{X}^{\top}\right)^{t }\right](\bm{X}\bm{X}^{\top})^{-1}\bm{\varepsilon}\right]^{2}\] \[\leq\mathbb{E}\left[\bm{x}^{\top}\left(\bm{I}-\frac{\lambda}{n} \bm{X}^{\top}\bm{X}\right)^{t}\bm{\theta}^{*}\right]^{2}+\mathbb{E}\left[\bm{ x}^{\top}\bm{X}^{\top}\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}\bm{X}^{ \top}\right)^{t}\right](\bm{X}\bm{X}^{\top})^{-1}\bm{\varepsilon}\right]^{2}\] \[:=\bm{\theta}^{*\top}\bm{B}\bm{\theta}^{*}+\bm{\varepsilon}^{\top }\bm{C}\bm{\varepsilon}.\] (29)

which proves the first claim in the lemma. The second part of the theorem directly follows from lemma 18 in Bartlett et al. [8]. 

### Proof for the Bias Upper Bound

The next lemma guarantees that the sample covariace matrix \(\frac{1}{n}\bm{X}^{\top}\bm{X}\) concentrates well around \(\bm{\Sigma}\).

**Lemma A.5**.: _(Lemma 35 in Bartlett et al. [8]) There exists constant \(c\) such that for any \(0<\delta<1\) with probability as least \(1-\delta\),_

\[\left\|\bm{\Sigma}-\frac{1}{n}\bm{X}^{\top}\bm{X}\right\|\leq c |\bm{\Sigma}||\max\left\{\sqrt{\frac{r(\bm{\Sigma})}{n}},\frac{r(\bm{\Sigma}) }{n},\sqrt{\frac{\log(\frac{1}{\delta})}{n}},\frac{\log(\frac{1}{\delta})}{n} \right\}.\] (30)

The following inequality will be useful in our proof to characterize the decaying rate of the bias term with \(t\).

**Lemma A.6**.: _For any positive semidefinite matrix \(\bm{P}\) which satisfies \(\|\bm{P}\|\leq 1\), we have_

\[\|\bm{P}(1-\bm{P})^{t}\|\leq\frac{1}{t}.\] (31)

Proof.: Assume without loss of generality that \(\bm{P}\) is diagonal. Then it suffices to consider seperately each eigenvalue \(\sigma\) of \(\bm{P}\), and show that \(\sigma(1-\sigma)^{t}\leq\frac{1}{t}\).

In fact, by AM-GM inequality we have

\[\sigma(1-\sigma)^{t}\leq\frac{1}{t}\left[\frac{t\sigma+(1-\sigma)t}{t+1}\right] ^{t+1}\leq\frac{1}{t},\] (32)

which completes the proof. 

Next we prove the main result of this section.

**Lemma A.7**.: _There exists constant \(c\) such that if \(0\leq\lambda\leq\frac{1}{c\sum_{i>0}\lambda_{i}}\), then for any \(0<\delta<1\), with probability at least \(1-\delta\) the following bound on the bias term holds for any \(t\)_

\[\bm{\theta}^{*\top}\bm{B}\bm{\theta}^{*}\leq c\left\|\bm{\theta}^{*}\right\|^ {2}\left(\frac{1}{\lambda t}+\|\bm{\Sigma}\|\max\left\{\sqrt{\frac{r(\bm{ \Sigma})}{n}},\frac{r(\bm{\Sigma})}{n},\sqrt{\frac{\log(\frac{1}{\delta})}{n}},\frac{\log(\frac{1}{\delta})}{n}\right\}\right).\] (33)Proof.: The bias can be decomposed into the following two terms

\[\begin{split}\boldsymbol{\theta}^{*\top}\boldsymbol{B}\boldsymbol{ \theta}^{*}&=\boldsymbol{\theta}^{*\top}\left(\boldsymbol{I}- \frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X}\right)^{t}\left( \boldsymbol{\Sigma}-\frac{1}{n}\boldsymbol{X}^{\top}\boldsymbol{X}\right) \left(\boldsymbol{I}-\frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X} \right)^{t}\boldsymbol{\theta}^{*}\\ &\quad+\boldsymbol{\theta}^{*\top}\left(\frac{1}{n}\boldsymbol{X} ^{\top}\boldsymbol{X}\right)\left(\boldsymbol{I}-\frac{\lambda}{n}\boldsymbol{ X}^{\top}\boldsymbol{X}\right)^{2t}\boldsymbol{\theta}^{*}.\end{split}\] (34)

For sufficiently small learning rate \(\lambda\) as given by corollary A.1, we know that with high probability

\[\left\|\boldsymbol{I}-\frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X} \right\|\leq 1,\] (35)

which together with lemma A.5 gives a high probability bound on the first term:

\[\begin{split}\boldsymbol{\theta}^{*\top}\left(\boldsymbol{I}- \frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X}\right)^{t}\left( \boldsymbol{\Sigma}-\frac{1}{n}\boldsymbol{X}^{\top}\boldsymbol{X}\right) \left(\boldsymbol{I}-\frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X} \right)^{t}\boldsymbol{\theta}^{*}\\ \leq c\|\boldsymbol{\Sigma}\|\,\|\boldsymbol{\theta}^{*}\|^{2} \max\left\{\sqrt{\frac{r(\boldsymbol{\Sigma})}{n}},\frac{r(\boldsymbol{ \Sigma})}{n},\sqrt{\frac{\log(\frac{1}{\delta})}{n}},\frac{\log(\frac{1}{ \delta})}{n}\right\}.\end{split}\] (36)

For the second term, invoke lemma A.6 with \(\boldsymbol{P}=\frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X}\) and we get

\[\begin{split}\boldsymbol{\theta}^{*\top}\left(\frac{1}{n} \boldsymbol{X}^{\top}\boldsymbol{X}\right)\left(\boldsymbol{I}-\frac{\lambda }{n}\boldsymbol{X}^{\top}\boldsymbol{X}\right)^{2t}\boldsymbol{\theta}^{*}& \leq\frac{1}{\lambda}\left\|\boldsymbol{\theta}^{*}\right\|^{ 2}\left\|\left(\frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X}\right) \left(\boldsymbol{I}-\frac{\lambda}{n}\boldsymbol{X}^{\top}\boldsymbol{X} \right)^{2t}\right\|\\ &\leq\frac{1}{2\lambda t}\left\|\boldsymbol{\theta}^{*}\right\|^ {2}.\end{split}\] (37)

Putting these two bounds together gives the proof for the main theorem.

### Proof for the Variance Upper Bound

Recall that \(\boldsymbol{X}=\boldsymbol{U}\tilde{\boldsymbol{\Lambda}}^{\frac{1}{2}} \boldsymbol{W}^{\top}\) is the singular value decomposition of data matrix \(\boldsymbol{X}\), where \(\boldsymbol{U}=(\boldsymbol{u}_{1},\cdots,\boldsymbol{u}_{n})\), \(\boldsymbol{W}=(\boldsymbol{w}_{1},\cdots,\boldsymbol{w}_{n})\), \(\tilde{\boldsymbol{\Lambda}}=\text{diag}\{\mu_{1},\cdots,\mu_{n}\}\) with \(\mu_{1}\geq\mu_{2}\geq\cdots\mu_{n}\).

Recall that

\[\begin{split} k_{0}&=\min\{l\geq 0:\lambda_{l+1} \leq\frac{c_{0}\sum_{i>l}\lambda_{i}}{n}\},\\ k_{1}&=\min\{l\geq 0:\lambda_{l+1}\leq\frac{c_{1} \sum_{i>0}\lambda_{i}}{n}\},\\ k_{2}&=\min\{l\geq 0:\sum_{i>l}\lambda_{i}+n\lambda_{l+1} \leq c_{2}c(t,n)\sum_{i>0}\lambda_{i}\},\end{split}\] (38)

for some constant \(c_{0},c_{1},c_{2}\) and function \(c(t,n)\).

We further define

\[k_{3}=\min\{l\geq 0:\mu_{l+1}\leq c_{3}c(t,n)\sum_{i>0}\lambda_{i}\},\] (39)

for some constant \(c_{3}\).

The next lemma shows that we can appropriately choose constants to ensure that \(k_{3}\leq k_{2}\) holds with high probability, and in some specific cases we have \(k_{2}\leq k_{1}\).

**Lemma A.8**.: _For any function \(c(t,n)\) and constant \(c_{2}\), there exists constants \(c,c_{3}\), such that \(k_{3}\leq k_{2}\) with probability at least \(1-e^{-\frac{c}{c}}\). Furthermore, if \(c(t,n)\) is a positive constant function, for any \(c_{1}\), there exists \(c_{2}\) such that \(k_{2}\leq k_{1}\)._Proof.: According to lemma A.1, there exists a constant \(c\), with probability at least \(1-e^{-\frac{n}{c}}\) we have

\[\mu_{k_{2}+1}\leq c\big{(}\sum_{i>k_{2}}\lambda_{i}+n\lambda_{k_{2}+1}\big{)} \leq cc_{2}c(t,n)\sum_{i>0}\lambda_{i}.\] (40)

Therefore, we know that \(k_{3}\leq k_{2}\) for \(c_{3}=cc_{2}\).

By the definition of \(k_{1}\), we have

\[\sum_{i>k_{1}}\lambda_{i}+n\lambda_{k_{1}+1}\leq(c_{1}+1)\sum_{i>0}\lambda_{i},\] (41)

which implies that \(k_{2}\leq k_{1}\) for \(c_{2}=\frac{c_{1}+1}{c(t,n)}\), if \(c(t,n)\) is a positive constant. 

Next we bound \(\mathrm{Tr}[\bm{C}]\), which implies an upper bound on the variance term.

**Theorem A.1**.: _There exist constants \(c,c_{0},c_{1},c_{2}\) such that if \(k_{0}\leq\frac{n}{c}\), then with probability at least \(1-e^{-\frac{n}{c}}\), the trace of the variance matrix \(C\) has the following upper bound for any \(t\):_

\[\mathrm{Tr}[\bm{C}]\leq c\left(\frac{k_{1}}{n}+\frac{k_{2}}{c(t,n)n}+c(t,n) \left(\frac{\lambda t}{n}\sum_{i>0}\lambda_{i}\right)^{2}\right).\] (42)

Proof.: We divide the eigenvalues of \(\bm{X}\bm{X}^{\top}\) into two groups based on whether they are greater than \(c_{3}c(t,n)\sum_{i>0}\lambda_{i}\). The first group consists of \(\mu_{1}\cdots\mu_{k_{3}}\), and the second group consists of \(\mu_{k_{3}+1}\cdots\mu_{n}\). For \(1\leq j\leq k_{3}\), we have

\[1-\left(1-\frac{\lambda}{n}\mu_{j}\right)^{t}\leq 1.\] (43)

Therefore we have the following upper bound on \(\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}\bm{X}^{\top}\right)^{t} \right]^{2}\):

\[\begin{split}&\left[\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X} \bm{X}^{\top}\right)^{t}\right]^{2}\\ &=\bm{U}\text{diag}\left\{\left[1-\left(1-\frac{\lambda}{n}\mu_{ 1}\right)^{t}\right]^{2}\cdots\left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^ {t}\right]^{2}\right\}\bm{U}^{\top}\\ &\preceq\bm{U}\text{diag}\left\{\underbrace{\underbrace{k_{3} \text{ times}}_{1,\cdots 1,}\underbrace{\left[1-\left(1-\frac{\lambda}{n}\mu_{k_{3}+1} \right)^{t}\right]^{2}},\cdots\left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^ {t}\right]^{2}}_{\underbrace{\left\}\bm{U}^{\top}\right.}\\ &=\bm{U}\text{diag}\left\{\underbrace{\underbrace{k_{3}\text{ times}}_{1,\cdots 1,}\underbrace{n-k_{3}\text{ times}}_{0,\cdots 0}\right\}\bm{U}^{\top}\\ &\quad+\bm{U}\text{diag}\left\{\underbrace{\underbrace{k_{3} \text{ times}}_{0,\cdots 0,}\underbrace{\left[1-\left(1-\frac{\lambda}{n}\mu_{k_{3}+1} \right)^{t}\right]^{2}},\cdots\left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^ {t}\right]^{2}}_{\underbrace{\left\}\bm{U}^{\top}.}\right.}\right.\end{split}\] (44)

For positive semidefinite matrices \(\bm{P},\bm{Q},\bm{R}\) which satisfies \(\bm{Q}\preceq\bm{R}\), it holds that \(\mathrm{Tr}[\bm{PQ}]\leq\mathrm{Tr}[\bm{PR}]\). It implies the following upperbound of \(\mathrm{Tr}[\bm{C}]\):\[\begin{split}&\operatorname{Tr}[\bm{C}]\\ =&\operatorname{Tr}\left[\left[\bm{I}-\left(\bm{I}-\frac{ \lambda}{n}\bm{X}\bm{X}^{\top}\right)^{t}\right]^{2}\left(\bm{X}\bm{X}^{\top} \right)^{-2}\bm{X}\bm{\Sigma}\bm{X}^{\top}\right]\\ \leq&\underbrace{\operatorname{Tr}\left[\bm{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \widehat{0},\cdots 0 \right\}\right\}\bm{U}^{\top}\left(\bm{X}\bm{X}^{\top}\right)^{-2}\bm{X}\bm{ \Sigma}\bm{X}^{\top}\right]\\ +&\underbrace{\operatorname{Tr}\left[\bm{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{ \widehat{0},\cdots 0},\left[\left[1-\left(1-\frac{\lambda}{n}\mu_{k_{3}+1}\right)^{t} \right]^{2},\cdots\left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^{t}\right]^ {2}\right\}\bm{U}^{\top}\left(\bm{X}\bm{X}^{\top}\right)^{-2}\bm{X}\bm{ \Sigma}\bm{X}^{\top}\right]}_{\mathfrak{L}}.\end{split}\] (45)

**Bounding 1**

Noticing \(\bm{X}=\bm{U}\tilde{\bm{\Lambda}}^{\frac{1}{2}}\bm{W}^{\top}\) and \(\bm{\Sigma}=\sum_{i\geq 1}\lambda_{i}\bm{v}_{i}\bm{v}_{i}^{\top}\), we express the first term as sums of eigenvector products,

\[\begin{split}\text{1}&=\operatorname{Tr}\left[\bm {U}\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \widehat{0},\cdots 0 \right\}\right\}\bm{U}^{\top}\left(\bm{X}\bm{X}^{\top}\right)^{-2}\bm{X}\bm{ \Sigma}\bm{X}^{\top}\right]\\ &=\operatorname{Tr}\left[\bm{U}\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \widehat{0},\cdots 0 \right\}\right\}\bm{U}^{\top}\bm{U}\tilde{\bm{\Lambda}}^{-2}\bm{U}^{\top}\bm{U} \tilde{\bm{\Lambda}}^{\frac{1}{2}}\bm{W}^{\top}\bm{\Sigma}\bm{W}\tilde{\bm{ \Lambda}}^{\frac{1}{2}}\bm{U}^{\top}\right]\\ &=\operatorname{Tr}\left[\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \widehat{0},\cdots 0 \right\}\tilde{\bm{\Lambda}}^{-1}\bm{W}^{\top}\bm{\Sigma}\bm{W}\right]\\ &=\sum_{i\geq 1}\lambda_{i}\operatorname{Tr}\left[\text{diag }\left\{\frac{k_{3}\text{ times }}{1,\cdots 1},\ \widehat{0},\cdots 0 \right\}\tilde{\bm{\Lambda}}^{-1}\bm{W}^{\top}\bm{v}_{i}\bm{v}_{i}^{\top} \bm{W}\right]\\ &=\sum_{i\geq 1}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_{j}} \left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2}.\end{split}\] (46)

Next we divide the above summation into \(1\leq i\leq k_{1}\) and \(i>k_{1}\). For the first part, notice that

\[\begin{split}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_{j}} \left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2}&\leq\sum_{1\leq j \leq n}\frac{\lambda_{i}}{\mu_{j}}\left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2} \\ &=\lambda_{i}\bm{v}_{i}^{\top}\left(\sum_{1\leq j\leq n}\frac{1}{ \mu_{j}}\bm{w}_{j}\bm{w}_{j}^{\top}\right)\bm{v}_{i}\\ &=\lambda_{i}\bm{v}_{i}^{\top}\bm{W}\tilde{\bm{\Lambda}}^{-1}\bm{W} ^{\top}\bm{v}_{i}\\ &=\lambda_{i}\bm{v}_{i}^{\top}\bm{W}\tilde{\bm{\Lambda}}^{\frac{1} {2}}\bm{U}^{\top}\bm{U}\tilde{\bm{\Lambda}}^{-2}\bm{U}^{\top}\bm{U}\tilde{ \bm{\Lambda}}^{\frac{1}{2}}\bm{W}^{\top}\bm{v}_{i}\\ &=\lambda_{i}^{2}\tilde{\bm{\varepsilon}}_{i}^{\top}(\bm{X}\bm{X} ^{\top})^{-2}\tilde{\bm{x}}_{i},\end{split}\] (47)

where \(\tilde{\bm{x}}_{i}\) is defined as \(\tilde{\bm{x}}_{i}=\frac{\bm{X}\bm{v}_{i}}{\sqrt{\lambda_{i}}}=\frac{\bm{U} \tilde{\bm{\Lambda}}^{\frac{1}{2}}\bm{W}^{\top}\bm{v}_{i}}{\sqrt{\lambda_{i}}}\).

From the proof of lemma 11 in Bartlett et al. [8], we know that for any \(\sigma_{x}\), there exists a constant \(c_{0}\) and \(c\) such that if \(k_{0}\leq\frac{n}{c}\), with probability at least \(1-e^{-\frac{n}{c}}\) the first part can be bounded as \[\sum_{1\leq i\leq k_{1}}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_{j}} \left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2}\leq\sum_{1\leq i\leq k_{1}}\lambda_ {i}^{2}\bm{\tilde{x}}_{i}(\bm{X}\bm{X}^{\top})^{-2}\tilde{\bm{x}}_{i}\leq c\frac {k_{1}}{n},\] (48)

which gives a bound for the first part.

For the second part we interchange the order of summation and have

\[\sum_{i\geq k_{1}}\sum_{1\leq j\leq k_{3}}\frac{\lambda_{i}}{\mu_ {j}}\left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2} =\sum_{1\leq j\leq k_{3}}\sum_{i\geq k_{1}}\frac{\lambda_{i}}{\mu _{j}}\left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2}\] (49) \[\leq\frac{1}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}}\sum_{1\leq j\leq k _{3}}\sum_{i\geq k_{1}}\lambda_{i}\left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^{2}\] \[=\frac{\lambda_{k_{1}+1}}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}}\sum_{ 1\leq j\leq k_{3}}\sum_{i\geq k_{1}}\left(\bm{v}_{i}^{\top}\bm{w}_{j}\right)^ {2}\] \[\leq\frac{\lambda_{k_{1}+1}}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}} \sum_{1\leq j\leq k_{3}}1\] \[=\frac{\lambda_{k_{1}+1}k_{3}}{c_{3}c(t,n)\sum_{i>0}\lambda_{i}}\] \[\leq c\frac{k_{3}}{c(t,n)n}.\]

for \(c\) large enough.

Putting 48 and 49 together, and noting that \(k_{3}\leq k_{2}\) with high probability as given in lemma A.8, we know there exists a constant \(c\) that with probability at least \(1-e^{-\frac{n}{c}}\),

\[\textcfrac{k_{1}}{n}+c\frac{k_{2}}{c(t,n)n}.\] (50)

Bounding 2

Similar to the first step in bounding 1, we note that

\[\textcfrac{2}{}=\operatorname{Tr}\left[\bm{U}\text{diag}\begin{cases} \overbrace{0,\cdots 0,}^{k_{3}\text{ times}}\overbrace{\left[1-\left(1-\frac{\lambda}{n}\mu_{k_{3}+1} \right)^{t}\right]^{2},\cdots,\left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^ {t}\right]^{2}}^{n-k_{3}\text{ times}}\\ \bm{U}\tilde{\bm{\Lambda}}^{-2}\bm{U}^{\top}\bm{U}\tilde{\bm{\Lambda}}^{\frac {1}{2}}\bm{W}^{\top}\bm{\Sigma}\bm{W}\tilde{\bm{\Lambda}}^{\frac{1}{2}}\bm{U}^ {\top}\end{cases}\right]\] (51) \[=\operatorname{Tr}\left[\text{diag}\begin{cases}\overbrace{0, \cdots 0,}^{k_{3}\text{ times}}\overbrace{\frac{1}{\mu_{k_{3}+1}}\left[1-\left(1- \frac{\lambda}{n}\mu_{k_{3}+1}\right)^{t}\right]^{2},\cdots,\frac{1}{\mu_{n}} \left[1-\left(1-\frac{\lambda}{n}\mu_{n}\right)^{t}\right]^{2}}^{2}\\ \bm{W}^{\top}\bm{\Sigma}\bm{W}\end{cases}.\right.\]

From Bernoulli's inequality and the definition of \(k_{3}\), for any \(k_{3}+1\leq j\leq n\), we have

\[\frac{1}{\mu_{k}}\left[1-\left(1-\frac{\lambda}{n}\mu_{k}\right)^{t}\right]^{ 2}\leq\frac{1}{\mu_{k}}\left(\frac{\lambda}{n}\mu_{k}t\right)^{2}=\left(\frac{ \lambda t}{n}\right)^{2}\mu_{k}\leq c_{3}\left(\frac{\lambda t}{n}\right)^{2}c (t,n)\sum_{i>0}\lambda_{i},\] (52)

Hence,\[\text{\textcircled{2}} \leq c_{3}c(t,n)\left(\frac{\lambda t}{n}\right)^{2}\sum_{i>0} \lambda_{i}\operatorname{Tr}[\boldsymbol{W}^{\top}\boldsymbol{\Sigma} \boldsymbol{W}]\] (53) \[=c_{3}c(t,n)\left(\frac{\lambda t}{n}\sum_{i>0}\lambda_{i}\right)^ {2}.\]

Putting things togetherFrom the bounds for 1 and 2 given above, we know that there exists a constant \(c\) such that with probability at least \(1-e^{-\frac{n}{c}}\), the trace of the variance matrix \(C\) has the following upper bound

\[\operatorname{Tr}[C]\leq c\left(\frac{k_{1}}{n}+\frac{k_{2}}{c(t,n)n}+c(t,n) \left(\frac{\lambda t}{n}\sum_{i>0}\lambda_{i}\right)^{2}\right).\] (54)

Proof of theorem 5.1.: Lemma A.4, A.7 and Theorem A.1 gives the complete proof. Note that the high probability events in the proof are independent of the epoch number \(t\), and this implies that the theorem holds uniformly for all \(t\in\mathbb{N}\). 

### Proof of Compatibility Results

**Corollary A.2** (Corollary 5.1 restated).: _Let Assumption 1, 2 and 3 hold. Fix a constant \(c(t,n)\). Suppose \(k_{0}=O(n)\), \(k_{1}=o(n)\), \(r(\boldsymbol{\Sigma})=o(n)\), \(\lambda=O\left(\frac{1}{\sum_{i>0}\lambda_{i}}\right)\). Then there exists a sequence of positive constants \(\{\delta_{n}\}_{n\geq 0}\) which converge to 0, such that with probability at least \(1-\delta_{n}\), the excess risk is consistent for \(t\in\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\), i.e._

\[R(\boldsymbol{\theta}_{t})=o(1).\]

_Furthermore, for any positive constant \(\delta\), with probability at least \(1-\delta\), the minimal excess risk on the training trajectory can be bounded as_

\[\min_{t}R(\boldsymbol{\theta}_{t})\lesssim\frac{\max\{\sqrt{r(\boldsymbol{ \Sigma})},1\}}{\sqrt{n}}+\frac{\max\{k_{1},1\}}{n}.\]

Proof.: According to Lemma A.7, with probability at least \(1-\frac{\delta_{n}}{2}\), the following inequality holds for all \(t\):

\[B(\boldsymbol{\theta}_{t})\lesssim\left(\frac{1}{\lambda t}+\max\left\{\sqrt{ \frac{r(\boldsymbol{\Sigma})}{n}},\frac{r(\boldsymbol{\Sigma})}{n},\sqrt{ \frac{\log(\frac{1}{\delta_{n}})}{n}},\frac{\log(\frac{1}{\delta_{n}})}{n} \right\}\right).\] (55)

If \(\delta_{n}\) is chosen such that \(\log\frac{1}{\delta_{n}}=o(n)\), we have that with probability at least \(1-\frac{\delta_{n}}{2}\), we have for all \(t=\omega\left(\frac{1}{\lambda}\right)\):

\[B(\boldsymbol{\theta}_{t})=o(1),\] (56)

in the sample size \(n\).

When \(c(t,n)\) is a constant, we have \(k_{2}\leq k_{1}\) with high probability as given in lemma A.8. Therefore, according to Lemma A.4 and Theorem A.1, we know that if \(\log\frac{1}{\delta_{n}}=O(n)\), with probability at least \(1-\frac{\delta_{n}}{2}\), the following bound holds for all \(t\):

\[V(\boldsymbol{\theta}_{t})\lesssim\log\left(\frac{1}{\delta_{n}}\right)\left( \frac{k_{1}}{n}+\frac{\lambda^{2}t^{2}}{n^{2}}\right).\] (57)

Since \(k_{1}=o(n)\), \(t=o\left(\frac{n}{\lambda}\right)\), we have \(\frac{k_{1}}{n}+\frac{\lambda^{2}t^{2}}{n^{2}}=o(1).\) Therefore, there exists a mildly decaying sequence of \(\delta_{n}\) with \(\log\left(\frac{1}{\delta_{n}}\right)\left(\frac{k_{1}}{n}+\frac{\lambda^{2}t ^{2}}{n^{2}}\right)=o(1)\), i.e.,

\[V(\boldsymbol{\theta}_{t})=o(1).\] (58)To conclude, \(\delta_{n}\) can be chosen such that

\[\log\left(\frac{1}{\delta_{n}}\right)=\omega(1),\log\left(\frac{1}{\delta_{n}} \right)=O(n),\log\left(\frac{1}{\delta_{n}}\right)=O\left(\frac{1}{\frac{k_{1} }{n}+\frac{\lambda^{2}t^{2}}{n^{2}}}\right),\] (59)

and then with probability at least \(1-\delta_{n}\), the excess risk is consistent for all \(t\in\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\):

\[R(\bm{\theta}_{t})=B(\bm{\theta}_{t})+V(\bm{\theta}_{t})=o(1).\] (60)

This completes the proof for the first claim. The second claim follows from Equation 55 and 57 by setting \(t=\Theta\left(\frac{\sqrt{n}}{\lambda}\right)\). 

**Lemma A.9** (Lemma 5.1 restated).: _For any fixed (i.e. independent of sample size \(n\)) feature covariance \(\bm{\Sigma}\) satisfying assumption 1, we have \(k_{1}(n)=o(n)\)._

Proof.: Suppose there exists constant \(c\), such that \(k_{1}(n)\geq cn\). By definition of \(k_{1}\), we know that \(\lambda_{l}\geq\frac{c_{1}\sum_{i>0}\lambda_{i}}{n}\) holds for \(1\leq l\leq k_{1}(n)\). Hence we have

\[\sum_{l=\lfloor cn^{2^{l+1}}\rfloor+1}^{\lfloor cn^{2^{l+1}}\rfloor}\lambda_{ l}\gtrsim\frac{c_{1}\sum_{i>0}\lambda_{i}}{n2^{i+1}}cn2^{i}\gtrsim\sum_{i>0} \lambda_{i}.\] (61)

summing up all \(l\) leads to a contradiction since \(\sum_{i>0}\lambda_{i}<\infty\), which finishes the proof. 

**Theorem A.2** (Theorem 4.1 restated).: _Consider the overparameterized linear regression setting defined in section 4.1. Let Assumption 1,2 and 3 hold. Assume the learning rate satisfies \(\lambda=O\left(\frac{1}{\operatorname{Tr}(\bm{\Sigma})}\right)\)._

* _If the covariance satisfies_ \(k_{0}=o(n),R_{k_{0}}(\bm{\Sigma})=\omega(n),\;r(\bm{\Sigma})=o(n)\)_, it is compatible with the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),\infty\right)\)_._
* _If the covariance satisfies_ \(k_{0}=O(n),k_{1}=o(n),r(\bm{\Sigma})=o(n)\)_, it is compatible with the region_ \(T_{n}=\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\)_._

Proof.: For the first argument, notice that (a) the bias term can still be bounded when \(t=\omega(1)\) and \(r(\Sigma)=o(n)\), according to Lemma A.4; (b) the variance term can be bounded with \(t\to\infty\) (that is, the variance loss would increase with time \(t\)). Therefore, the first argument directly follows Theorem 4 in Bartlett et al. [8].

The second argument follows Corollary 5.1, and the third argument follows Corollary 5.1 and Lemma 5.1. Specifically, for any \(\varepsilon>0\), there exists \(\{\delta_{n}\}_{n>0}\) and \(N\) such that for any sample size \(n>N\), we have

\[\Pr\left[\left|\sup_{t\in T_{n}}R(\bm{\theta}_{t})\right|>\varepsilon\right] \leq\delta_{n}.\] (62)

Let \(n\to\infty\) shows that \(\sup_{t\in T_{n}}R(\bm{\theta}_{t})\) converges to \(0\) in probability, which completes the proof for the second and the third claim. 

## Appendix B Comparisons and Discussions

In this section, we provide additional discussions and calculations for the main results, and compare it with previous works, including benign overfitting (Section B.1), stability-based bounds (Section B.2), uniform convergence (Section B.3), and early-stopping bounds (Section B.4).

### Comparisons with Benign Overfitting

We summarize the results in Bartlett et al. [8], Zou et al. [76] and our results in Table 1, and provide a detailed comparison with them below.

**Comparison to Bartlett et al. [8].** In this seminal work, the authors study the excess risk of the min-norm interpolator. As discussed before, gradient descent converges to the min-norm interpolator in the overparameterized linear regression setting. One of the main results in Bartlett et al. [8] is to provide a tight bound for the variance part in excess risk as

\[V(\boldsymbol{\hat{\theta}})=O\left(\frac{k_{0}}{n}+\frac{n}{R_{k_{0}}( \boldsymbol{\Sigma})}\right),\] (63)

where \(\boldsymbol{\hat{\theta}}=\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{X} ^{\top})^{-1}\boldsymbol{Y}\) denotes the min-norm interpolator, and \(R_{k}(\boldsymbol{\Sigma})=(\sum_{i>k}\lambda_{i})^{2}/\)\((\sum_{i>k}\lambda_{i}^{2})\) denote another type of effective rank.

By introducing the time factor, Theorem 5.1 improves over Equation (63) in at least two aspects. Firstly, Theorem 5.1 guarantees the consistency of the gradient descent dynamics for a broad range of step number \(t\), while Bartlett et al. [8] study the limiting behavior of the dynamics of \(t\to\infty\). Secondly, Theorem 5.1 implies that the excess risk of early stopping gradient descent solution can be much better than the min-norm interpolator. Compared to the bound in Equation (63), the bound in Corollary 5.1 (a.) replaces \(k_{0}\) with a much smaller quantity \(k_{1}\); and (b.) drops the second term involving \(R_{k_{0}}(\boldsymbol{\Sigma})\). Therefore, we can derive a consistent bound for an early stopping solution, even though the excess risk of limiting point (min-norm interpolator) can be \(\Omega(1)\).

**Comparison to Zou et al. [76].** Zou et al. [76] study a different setting, which focuses on the one-pass stochastic gradient descent solution of linear regression. The authors prove a bound for the excess risk as

\[R(\boldsymbol{\tilde{\theta}}_{t})=O\left(\frac{k_{1}}{n}+\frac{n\sum_{i>k_{1 }}\lambda_{i}^{2}}{(\sum_{i>0}\lambda_{i})^{2}}\right),\] (64)

where \(\boldsymbol{\tilde{\theta}}_{t}\) denotes the parameter obtained using stochastic gradient descent (SGD) with constant step size at epoch \(t\). Similar to our bound, Equation 64 also uses the effective dimension \(k_{1}\) to characterize the variance term. However, we emphasize that Zou et al. [76] derive the bound in a pretty different scenario from ours, which is one-pass SGD scenario. During the one-pass SGD training, one uses a fresh data point to perform stochastic gradient descent in each epoch, and therefore they set \(t=\Theta(n)\) by default. As a comparison, we apply the standard full-batch gradient descent, and thus the time can be more flexible. Besides, our results in Corollary 5.1 improve the bound in Equation (64) by dropping the second term. We refer to the third and fourth example in Example 5.1 for a numerical comparison of the bounds2.

Footnote 2: Due to the bias term in Theorem 5.1, the overall excess risk bound cannot surpass the order \(O(1/\sqrt{n})\), which leads to the cases that Zou et al. [76] outperforms our bound. However, we note that such differences come from the intrinsic property of GD and SGD, which may be unable to avoid in the GD regimes.

### Comparisons with Stability-Based Bounds

In this section, we show that Theorem 5.1 gives provably better upper bounds than the stability-based method. We cite a result from Teng et al. [67], which uses stability arguments to tackle overparameteried linear regression under similar assumptions.

**Theorem B.1** (modified from Theorem 1 in Teng et al. [67]).: _Under the overparameterized linear regression settings, assume that \(\|\boldsymbol{x}\|\leq 1\), \(|\varepsilon|\leq V\), \(w=\frac{\boldsymbol{\theta}^{*,\top}\boldsymbol{x}}{\sqrt{\boldsymbol{\theta}^ {*,\top}\boldsymbol{\Sigma}\boldsymbol{\theta}^{*}}}\) is \(\sigma_{w}^{2}\)-subgaussian. Let \(B_{t}=\sup_{\tau\in[t]}\|\boldsymbol{\theta}_{t}\|\). the following inequality holds with probability at least \(1-\delta\):_

\[R(\boldsymbol{\theta}_{t})=\tilde{O}\left(\max\{1,\boldsymbol{\theta}^{*,\top }\boldsymbol{\Sigma}\boldsymbol{\theta}^{*}\sigma_{w}^{2},(V+B_{t})^{2}\} \sqrt{\frac{\log(4/\delta)}{n}}+\frac{\|\boldsymbol{\theta}^{*}\|^{2}}{\lambda t }+\frac{\lambda t(V+B_{t})^{2}}{n}\right).\] (65)

Theorem B.1 applies the general stability-based results [19; 24] in the overparameterized linear regression setting, by replacing the bounded Lipschitz condition with the bounded domain condition. A fine-grained analysis [31] may remove the bounded Lipschitz condition, but it additionally requireszero noise or decaying learning rate, which is different from our setting. We omit the excess risk decomposition technique adopted in Teng et al. [67] for presentation clarity.

Theorem B.1 can not directly yield the stability argument in Theorem 4.1, since obtaining a high probability bound of \(B_{t}\) requires a delicate trajectory analysis and is a non-trivial task. Therefore, data-irrelevant methods such as stability-based bounds can not be directly applied to our setting. Even if one can replace \(B_{t}\) in Equation 65 with its expectation that is easier to handle (this modification will require adding concentration-related terms, and make the bound in Equation 65 looser), we can still demonstrate that Theorem 5.1 is tighter than the corresponding stability-based analysis by providing a lower bound on \(\mathbb{E}[B_{t}^{2}]\), which will imply a lower bound on the righthand side of Equation 65.

**Theorem B.2**.: _Let Assumption 1, 2, 3 holds. Suppose \(\lambda=O\left(\frac{1}{\sum_{i>0}\lambda_{i}}\right)\). Suppose the conditional variance of the noise \(\varepsilon|\bm{x}\) is lower bounded by \(\sigma_{\varepsilon}^{2}\). There exists constant \(c\), such that with probability at least \(1-ne^{-\frac{n}{\varepsilon}}\), we have for \(t=o(n)\),_

\[\mathbb{E}\|\bm{\theta}_{t}\|^{2}=\Omega\left(\frac{\lambda^{2}t^{2}}{n}\left( \sum_{i>k_{0}}\lambda_{i}\right)\right)\] (66)

First we prove the following lemma, bounding the number of large \(\mu_{i}\).

**Lemma B.1**.: _Suppose \(t=o(n)\). Let \(l\) denote the number of \(\mu_{i}\), such that \(\mu_{i}=\Omega\left(\frac{n}{t}\right)\). Then with probability at least \(1-ne^{-\frac{n}{\varepsilon}}\), we have \(l=O(t)\)._

Proof.: According to Lemma A.1, we know that with probability at least \(1-ne^{-\frac{n}{\varepsilon}}\), Equation 12 holds for all \(0\leq k\leq n-1\). Conditioned on this, we have

\[\frac{n}{t}l\lesssim\sum_{k=1}^{l}\mu_{i}\lesssim\sum_{k=1}^{l}(\sum_{i\geq k} \lambda_{i}+\lambda_{k}n)\lesssim(l+n)\sum_{i>0}\lambda_{i}\lesssim l+n.\] (67)

Since \(t=o(n)\), we have \(l=O(t)\) as claimed. 

We also need the result from Bartlett et al. [8], which gives a lowerbound of \(\mu_{n}\).

**Lemma B.2**.: _(Lemma 10 in Bartlett et al. [8]) For any \(\sigma_{x}\), there exists a constant \(c\), such that with probability at least \(1-e^{-\frac{n}{\varepsilon}}\) we have,_

\[\mu_{n}\geq c\left(\sum_{i>k_{0}}\lambda_{i}\right).\] (68)

We are now ready to prove Theorem B.2.

Proof.: We begin with the calculation of \(\|\bm{\theta}_{t}\|^{2}\). By Lemma A.2, the conditional unbiasedness of noise in Assumption 2 and the noise variance lower bound, we have

\[\mathbb{E}\|\bm{\theta}_{t}\|^{2} =\left\|\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^ {t}(\bm{\theta}_{0}-\bm{X}^{\dagger}\bm{Y})+\bm{X}^{\dagger}Y\right\|^{2}\] \[=\mathbb{E}\left\|\left(\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm {X}^{\top}\bm{X}\right)^{t}\right)\bm{X}^{\dagger}\left(\bm{X}\bm{\theta}^{*} +\bm{\varepsilon}\right)\right\|^{2}\] \[=\mathbb{E}\left\|\left(\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm {X}^{\top}\bm{X}\right)^{t}\right)\bm{X}^{\dagger}\bm{X}\bm{\theta}^{*}\right\| ^{2}+\mathbb{E}\left\|\left(\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top }\bm{X}\right)^{t}\right)\bm{X}^{\dagger}\bm{\varepsilon}\right\|^{2}\] \[\geq\mathbb{E}\left\|\left(\bm{I}-\left(\bm{I}-\frac{\lambda}{n} \bm{X}^{\top}\bm{X}\right)^{t}\right)\bm{X}^{\dagger}\bm{\varepsilon}\right\| ^{2}\] \[=\mathbb{E}\operatorname{Tr}\left[\left(\bm{I}-\left(\bm{I}- \frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\right)\bm{X}^{\dagger}\bm{ \varepsilon}\bm{\varepsilon}^{\top}\bm{X}^{\dagger,\top}\left(\bm{I}-\left( \bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\right)\right]\] \[\geq\sigma_{\varepsilon}^{2}\mathbb{E}\operatorname{Tr}\left[ \left(\bm{I}-\left(\bm{I}-\frac{\lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t} \right)\bm{X}^{\dagger}\bm{X}^{\dagger,\top}\left(\bm{I}-\left(\bm{I}-\frac{ \lambda}{n}\bm{X}^{\top}\bm{X}\right)^{t}\right)\right]\] \[=\sigma_{\varepsilon}^{2}\sum_{i=1}^{n}\frac{[1-(1-\frac{\lambda} {n}\mu_{i})^{t}]^{2}}{\mu_{i}}.\] (69)

When \(\mu_{i}=o\left(\frac{n}{t}\right)\), we have

\[1-(1-\frac{\lambda}{n}\mu_{i})^{t}=1-1+\frac{\lambda}{n}\mu_{i}t+O\left(\left( \frac{\lambda}{n}\mu_{i}t\right)^{2}\right)=\Theta\left(\frac{\lambda}{n}\mu_ {i}t\right).\] (70)

Plugging it into Equation 69 and then use Lemma B.1, B.2, we know that under the high probability event in Lemma B.1 and B.2,

\[\mathbb{E}\|\bm{\theta}_{t}\|^{2}=\Omega\left((n-l)\frac{\lambda^{2}}{n^{2}} \mu_{n}t^{2}\right)=\Omega\left(\frac{\lambda^{2}}{n}\mu_{n}t^{2}\right)= \Omega\left(\frac{\lambda^{2}t^{2}}{n}\left(\sum_{i>k_{0}}\lambda_{i}\right)\right)\] (71)

Therefore, the stability-based bound, i.e., the right hand side of Equation 65, can be lower bounded in expectation as \(\Omega\left(\frac{\lambda^{2}t^{3}}{n^{2}}\sum_{i>k_{0}}\lambda_{i}\right)\). This implies that the stability-based bound is vacuous when \(t=\Omega\left(\frac{n^{\frac{2}{3}\left(\sum_{i>k_{0}}\lambda_{i}\right)^{- \frac{1}{3}}}}{\lambda}\right)\). Thus, stability-based methods will provably yield smaller compatibility region than \(\left(\omega\left(\frac{1}{\lambda}\right),o\left(\frac{n}{\lambda}\right)\right)\) in Theorem 4.1 when \(\sum_{i>k_{0}}\lambda_{i}\) is not very small, as demonstrated in the examples below.

**Example B.1**.: _Let Assumption 1, 2, 3 holds. Assume without loss of generality that \(\lambda=\Theta(1)\). We have the following examples:_

1. **(Inverse Polynomial).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \[\lambda_{k}=\frac{1}{k^{\alpha}},\] _for some_ \(\alpha>1\)_, we derive that_ \(k_{0}=\Theta(n)\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=\Theta(\frac{1}{n^{\alpha-1}})\)_. Therefore, the stability bound in Theorem B.1 is vacuous when_ \[t=\Omega\left(n^{\frac{\alpha+1}{\lambda}}\right),\] _which is outperformed by the region in Theorem_ 5.1 _when_ \(\alpha<2\)2. **(Inverse Log-Polynomial).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \[\lambda_{k}=\frac{1}{k\log^{\beta}(k+1)},\] _for some_ \(\beta>1\) _, we derive that_ \(k_{0}=\Theta\left(\frac{n}{\log n}\right)\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=\tilde{\Theta}(1)\)_. Therefore, the stability bound in Theorem_ B.1 _is vacuous when_ \[t=\tilde{\Omega}\left(n^{\frac{2}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1_._
3. **(Constant).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \[\lambda_{k}=\frac{1}{n^{1+\varepsilon}},1\leq k\leq n^{1+\varepsilon},\] _for some_ \(\varepsilon>0\)_, we derive that_ \(k_{0}=0\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=1\)_. Therefore, the stability bound in Theorem_ B.1 _is vacuous when_ \[t=\Omega\left(n^{\frac{2}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1_._
4. **(Piecewise Constant).** _If the spectrum of_ \(\bm{\Sigma}\) _satisfies_ \[\lambda_{k}=\begin{cases}\frac{1}{s}&1\leq k\leq s,\\ \frac{1}{d-s}&s+1\leq k\leq d,\end{cases}\] _where_ \(s=n^{r},d=n^{q},0<r\leq 1,q\geq 1\)_, we derive that_ \(k_{0}=n^{r}\)_,_ \(\sum_{i>k_{0}}\lambda_{i}=1\)_. Therefore, the stability bound in Theorem_ B.1 _is vacuous when_ \[t=\Omega\left(n^{\frac{2}{3}}\right),\] _which is outperformed by the region in Theorem_ 5.1_._

### Comparisons with Uniform Convergence Bounds

We first state a standard bound on the Rademacher complexity of linear models.

**Theorem B.3** (Theorem in Mohri et al. [44]).: _Let \(S\subseteq\{\bm{x}:\|x\|_{2}\leq r\}\) be a sample of size \(n\) and let \(\mathcal{H}=\{x\mapsto\langle w,x\rangle:\|w\|_{2}\leq\Lambda\}\). Then, the empirical Rademacher complexity of \(\mathcal{H}\) can be bounded as follows:_

\[\hat{\mathcal{R}}_{S}(\mathcal{H})\leq\sqrt{\frac{r^{2}\Lambda^{2}}{n}}.\] (72)

Furthermore, Talagrand's Lemma (See Lemma 5.7 in Mohri et al. [44]) indicates that

\[\hat{\mathcal{R}}_{S}(l\circ\mathcal{H})\leq L\hat{\mathcal{R}}_{S}(\mathcal{ H})=\frac{\Theta(\Lambda^{2})}{\sqrt{n}},\] (73)

where \(L=\Theta(\Lambda)\) is the Lipschitz coefficient of the square loss function \(l\) in our setting. Therefore, the Rademacher generalization bound is vacuous when \(\Lambda=\Omega(n^{\frac{1}{3}})\). By Theorem B.2, we know that \(\mathbb{E}\|\bm{\theta}_{t}\|^{2}=\Omega(n^{\frac{1}{2}})\) when \(t=\Omega\left(\frac{n^{\frac{3}{4}}}{\lambda\left(\sum_{i>k_{0}}\lambda_{i} \right)^{\frac{1}{2}}}\right)\). A similar comparison as in Example B.1 can demonstrate that uniform stability arguments will provably yield smaller compatibility region than that in Theorem 5.1 for example distributions.

### Comparison with Previous Works on Early Stopping

A line of works focuses on deriving the excess risk guarantee of linear regression or kernel regression with early stopping (stochastic) gradient descent. We refer to Section 2 for details. Here we compare our results with some most relevant works, including [37, 51, 72].

**Comparison with Yao et al. [72].** Yao et al. [72] study kernel regression with early stopping gradient descent. Their approaches are different from ours in the following aspects.

Firstly, the assumptions used in the two approaches are different, due to different goals and techniques. Yao et al. [72] assume that the input feature and data noise have bounded norm (see Section 2.1 in Yao et al. [72]), while we require that the input feature is subgaussian with independent entries.

Furthermore, although Yao et al. [72] obtain a minimax bound in terms of the convergence rate, it is suboptimal in terms of compatibility region. Specifically, The results in our paper show a region like \((0,n)\) while the techniques Yao et al. [72] can only lead to a region like \((0,\sqrt{n})\). See Proof of the Main Theorem in section 2 in Yao et al. [72] for details. Such differences come from different goals of the two approaches, where Yao et al. [72] focus on providing the optimal early-stopping time while we focus on providing a larger time region in which the loss is consistent.

**Comparison with Lin and Rosasco [37].** Lin and Rosasco [37] study stochastic gradient descent with arbitrary batchsize, which is reduced to full batch gradient descent when setting the batchsize to sample size \(n\). Their results are different from ours, since they require the boundness assumption, and focus more on the optimal early stopping time rather than the largest compatibility region, in the same spirit of Yao et al. [72]. Specifically, Lin and Rosasco [37] derive a region like \((0,n^{\frac{\zeta+1}{2\alpha+1}})\), where \(\zeta\) and \(\gamma\) are problem dependent constants (See Theorem 1 in Lin and Rosasco [37] for details). The following examples demonstrate that this paper's results yield larger regions for a wide range of distribution classes.

**Example B.2**.: **(Inverse Polynomial).** _If the spectrum of \(\mathbf{\Sigma}\) satisfies_

\[\lambda_{k}=\frac{1}{k^{\alpha}},\]

_for some \(\alpha>1\). For this distribution, we have \(\zeta=\frac{1}{2}\), \(\gamma=\frac{1}{\alpha}\), and their region is \((0,n^{\frac{3\alpha}{2\alpha+1}})\), which is smaller than \((0,n^{\frac{3\alpha+1}{2\alpha+1}})\) given in Example 5.2._

**Example B.3**.: **(Inverse Log-Polynomial).** _If the spectrum of \(\mathbf{\Sigma}\) satisfies_

\[\lambda_{k}=\frac{1}{k\log^{\beta}(k+1)},\]

_for some \(\beta>1\). For this distribution, we have \(\zeta=\frac{1}{2}\), \(\gamma=1\), and their region is \((0,n^{\frac{3}{4}})\), which is smaller than \((0,n)\) given Corollary 5.1._

### Calculations in Example 5.1

We calculate the quantities \(r(\Sigma),k_{0},k_{1},k_{2}\) for the example distributions in 5.1. The results validate that \(k_{1}\) is typically a much smaller quantity than \(k_{0}\).

1. **Calculations for \(\lambda_{k}=\frac{1}{k^{\alpha}},\alpha>1\).** Define \(r_{k}(\mathbf{\Sigma})=\frac{\sum_{i>k}\lambda_{i}}{\lambda_{k+1}}\) as in Bartlett et al. [8]. Since \(\sum_{i>k}\frac{1}{i^{\alpha}}=\Theta(\frac{1}{k^{\alpha-1}})\), we have \(r_{k}(\mathbf{\Sigma})=\Theta\left(\frac{\frac{1}{k^{\alpha-1}}}{\frac{k^{ \alpha}}{k^{\alpha}}}\right)=\Theta(k)\). Hence, \(k_{0}=\Theta(n)\)3, and the conditions of theorem 5.1 is satisfied. As \(\sum_{i>0}\lambda_{i}<\infty\), By its definition we know that \(k_{1}\) is the smallest \(l\) such that \(\lambda_{l+1}=O(\frac{1}{n})\). Therefore, \(k_{1}=\Theta(n^{\frac{1}{\alpha}})\). Footnote 3: The calculations for \(k_{0},k_{1}\) and \(k_{2}\) in this section only apply when \(n\) is sufficiently large.
2. **Calculations for \(\lambda_{k}=\frac{1}{k\log^{\beta}(k+1)},\beta>1\).** \(\sum_{i>k}\frac{1}{i\log^{\beta}(i+1)}=\Theta(\int_{k}^{\infty}\frac{1}{x\log^ {\beta}x}dx)=\Theta(\frac{1}{\log^{\beta-1}k})\), which implies \(r_{k}(\mathbf{\Sigma})=k\log k\). Solving \(k_{0}\log k_{0}\geq\Theta(n)\), we have \(k_{0}=\Theta(\frac{n}{\log n})\).

By the definition of \(k_{1}\), we know that \(k_{1}\) is the smallest \(l\) such that \(l\log^{\beta}(l+1)\geq\Theta(n)\). Therefore, \(k_{1}=\Theta(\frac{n}{\log^{\beta}n})\).
3. **Calculations for \(\lambda_{i}=\frac{1}{n^{l+\varepsilon}},1\leq i\leq n^{1+\varepsilon}, \varepsilon>0\).** Since \(r_{0}(\bm{\Sigma})=n^{1+\varepsilon}\), we have \(k_{0}=0\). By the definition of \(k_{1}\), we also have \(k_{1}=0\).
4. **Calculations for \(\lambda=\left\{\frac{1}{s_{1}}\right.\quad\begin{array}{l}1\leq k\leq s\\ s+1\leq k\leq d\end{array},s=n^{r},d=n^{q},0<r\leq 1,q\geq 1\).** For \(0\leq k<s\), \(r_{k}(\bm{\Sigma})=\Theta(\frac{1}{z})=\Theta(n^{r})=o(n)\), while \(r_{s}(\bm{\Sigma})=\frac{1}{\frac{1}{d-s}}=\Theta(n^{q})=\omega(n)\). Therefore, \(k_{0}=s=n^{r}\). Similarly, noting that \(\lambda_{k}=\frac{1}{n^{r}}=\omega(n)\) for \(0\leq k<s\) and \(\lambda_{s}=\Theta(\frac{1}{n^{q}})=o(n)\), we know that \(k_{1}=s=n^{r}\).

### Calculations for \(\lambda_{k}=1/k^{\alpha},\alpha>1\) in Section 5.3

Set \(c(t,n)=\frac{1}{n^{\beta}}\), where \(\beta>0\) will be chosen later. First we calculate \(k_{2}\) under this choice of \(c(t,n)\). Note that \(\sum_{i>k}\frac{1}{k^{\alpha}}=\Theta\left(\frac{1}{k^{\alpha-1}}\right)\). Therefore, \(k_{2}\) is the smallest \(k\) such that \(\frac{1}{k^{\alpha-1}}+\frac{n}{k^{\alpha}}=O(\frac{1}{n^{\beta}})\). For the bound on \(V(\theta_{t})\) to be consistent, we need \(k_{2}=o(n)\). Hence, \(\frac{1}{k^{\alpha-1}}=O(\frac{n}{k^{\alpha}})\), which implies \(k_{2}=n^{\frac{\beta+1}{\alpha}}\).

Plugging the value of \(c(t,n)\) and \(k_{2}\) into our bound, we have

\[V(\theta_{t})=O\left(n^{(\frac{1}{\alpha}-1)+(\frac{1}{\alpha}+1)\beta}+n^{2 \tau-\beta-2}\right)\]

which attains its minimum \(\Theta(n^{\frac{2\alpha\tau-3\alpha+2\tau-1}{2\alpha+1}})\) at \(\beta=\Theta\left(\frac{2\alpha\tau-\alpha-1}{2\alpha+1}\right)\).

For \(V(\theta)=O(1)\), we need \(\tau\leq\frac{3\alpha+1}{2\alpha+2}\). For \(\beta\geq 0\), we need \(\tau\geq\frac{\alpha+1}{2\alpha}\). Putting them together gives the range of \(t\) in which the above calculation applies.

### Discussions on \(\mathcal{D}_{n}\)

In this paper, the distribution \(\mathcal{D}\) is regarded as a sequence of distribution \(\{\mathcal{D}_{n}\}\) which may dependent on sample size \(n\). The phenomenon comes from overparameterization and asymptotic requirements. In the definition of compatibility, we require \(n\rightarrow\infty\). In this case, overparameterization requires that the dimension \(p\) (if finite) cannot be independent of \(n\) since \(n\rightarrow\infty\) would break overparameterization. Therefore, the covariance \(\Sigma\) also has \(n\)-dependency, since \(\Sigma\) is closely related to \(p\).

Several points are worth mentioning: (1) Similar arguments generally appear in related works, for example, in Bartlett et al. [8] when discussing the definition of benign covariance. (2) One can avoid such issues by considering an infinite dimensional feature space. This is why we discuss the special case \(p=\infty\) in Theorem 4.1. (3) If \(p\) is a fixed finite constant that does not alter with \(n\), the problem becomes underparameterized and thus trivial to get a consistent generalization bound via standard concentration inequalities.

## Appendix C Additional Experiment Results

### Details for Linear Regression Experiments

In this section, we provide the experiment details for linear regression experiments and present additional empirical results.

The linear regression experiment in Figure 1 follows the setting described in section 6. Although the final iterate does not interpolate the training data, the results suffice to demonstrate the gap between the early-stopping and final-iterate excess risk. The training plot for different covariances are given in Figure 2.

Next, we provide the experiment results of sample sizes \(n=50\), \(n=200\) and feature dimensions \(p=500\), \(p=2000\). The settings are the same as described above, except for the sample size. The optimal excess risk and min-norm excess risk are provided in Table 3, 4, 6 and 5. The tables indicate that the two observations stated above hold for different sample size \(n\).

### Details for MNIST Experiments

In this section, we provide the experiment details and additional results in MNIST dataset.

The MNIST experiment details are described below. We create a noisy version of MNIST with label noise rate \(20\%\), i.e. randomly perturbing the label with probability \(20\%\) for each training data, to simulate the label noise which is common in real datasets, e.g ImageNet [60, 65, 73]. We do not inject noise into the test data.

We choose a standard four layer convolutional neural network as the classifier. We use a vanilla SGD optimizer without momentum or weight decay. The initial learning rate is set to \(0.5\). Learning rate is decayed by 0.98 every epoch. Each model is trained for 300 epochs. The training batch size is set to 1024, and the test batch size is set to 1000. We choose the standard cross entropy loss as the loss function.

We provide the plot for different levels of label noise in Figure 3. We present the corresponding test error of the best early stopping iterate and the final iterate in Table 7. Since the theoretical part of this paper focuses on GD, we also provide a corresponding plot of GD training in Figure 4 for completeness.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Distributions & \(k_{1}\) & Optimal Excess Risk & Min-norm excess risk \\ \hline \(\lambda_{i}=\frac{1}{i}\) & \(\Theta(n)\) & \(2.515\pm 0.0104\) & \(12.632\pm 0.1602\) \\ \(\lambda_{i}=\frac{1}{i^{2}}\) & \(\Theta(n^{\frac{1}{2}})\) & \(0.269\pm 0.0053\) & \(50.494\pm 0.9378\) \\ \(\lambda_{i}=\frac{1}{i^{3}}\) & \(\Theta(n^{\frac{1}{3}})\) & \(0.083\pm 0.0011\) & \(13.208\pm 0.4556\) \\ \(\lambda_{i}=\frac{1}{i^{3}\log(i+1)}\) & \(\Theta(\frac{n}{\log n})\) & \(0.808\pm 0.0090\) & \(46.706\pm 0.6983\) \\ \(\lambda_{i}=\frac{1}{i^{3}\log^{2}(i+1)}\) & \(\Theta(\frac{n}{\log^{2}n})\) & \(0.381\pm 0.0076\) & \(74.423\pm 1.1472\) \\ \(\lambda_{i}=\frac{1}{i^{3}\log^{3}(i+1)}\) & \(\Theta(\frac{n}{\log^{3}n})\) & \(0.233\pm 0.0052\) & \(43.045\pm 0.8347\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: **The effective dimension \(k_{1}\), the optimal early stopping excess risk and min-norm excess risk for different feature distributions, with sample size \(n=50\), \(p=1000\). We calculate the 95% confidence interval for the excess risk.**

Figure 2: **The training plot for overparameterized linear regression with different covariances using GD.**

\begin{table}
\begin{tabular}{l c c c} \hline \hline Distributions & \(k_{1}\) & Optimal Excess Risk & Min-norm excess risk \\ \hline \(\lambda_{i}=\frac{1}{i}\) & \(\Theta(n)\) & \(1.997\pm 0.0876\) & \(27.360\pm 0.3019\) \\ \(\lambda_{i}=\frac{1}{i^{2}}\) & \(\Theta(n^{\frac{1}{2}})\) & \(0.211\pm 0.0050\) & \(43.531\pm 0.7025\) \\ \(\lambda_{i}=\frac{1}{i^{3}}\) & \(\Theta(n^{\frac{1}{3}})\) & \(0.076\pm 0.0011\) & \(10.062\pm 0.3022\) \\ \(\lambda_{i}=\frac{1}{i\log(i+1)}\) & \(\Theta(\frac{n}{\log n})\) & \(0.645\pm 0.0056\) & \(96.465\pm 1.0594\) \\ \(\lambda_{i}=\frac{1}{i\log^{2}(i+1)}\) & \(\Theta(\frac{n}{\log^{2}n})\) & \(0.289\pm 0.0055\) & \(83.694\pm 0.9827\) \\ \(\lambda_{i}=\frac{1}{i\log^{3}(i+1)}\) & \(\Theta(\frac{n}{\log^{3}n})\) & \(0.181\pm 0.0045\) & \(38.090\pm 0.6378\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: **The effective dimension \(k_{1}\), the optimal early stopping excess risk and min-norm excess risk for different feature distributions, with sample size \(n=100\), \(p=2000\).** We calculate the 95% confidence interval for the excess risk.

\begin{table}
\begin{tabular}{l c c} \hline \hline \(0\%\) & \(1.07\%\) & \(1.13\%\) \\ \(10\%\) & \(1.75\%\) & \(10.16\%\) \\ \(20\%\) & \(2.88\%\) & \(19.90\%\) \\ \(30\%\) & \(2.18\%\) & \(27.94\%\) \\ \(40\%\) & \(2.57\%\) & \(35.15\%\) \\ \(50\%\) & \(2.71\%\) & \(42.95\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: **The test error of optimal stopping iterate and final iterate on MNIST dataset with different levels of label noise.** The results demonstrate that stopping iterate can have significantly better generalization performance than interpolating solutions for real datasets.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Distributions & \(k_{1}\) & Optimal Excess Risk & Min-norm excess risk \\ \hline \(\lambda_{i}=\frac{1}{i}\) & \(\Theta(n)\) & \(2.173\pm 0.0065\) & \(52.364\pm 0.4009\) \\ \(\lambda_{i}=\frac{1}{i^{2}}\) & \(\Theta(n^{\frac{1}{2}})\) & \(0.161\pm 0.0039\) & \(36.855\pm 0.4833\) \\ \(\lambda_{i}=\frac{1}{i^{3}}\) & \(\Theta(n^{\frac{1}{3}})\) & \(0.068\pm 0.0012\) & \(8.1990\pm 0.2313\) \\ \(\lambda_{i}=\frac{1}{i\log(i+1)}\) & \(\Theta(\frac{n}{\log n})\) & \(0.628\pm 0.0034\) & \(152.70\pm 1.1073\) \\ \(\lambda_{i}=\frac{1}{i\log^{2}(i+1)}\) & \(\Theta(\frac{n}{\log^{2}n})\) & \(0.241\pm 0.0036\) & \(83.550\pm 0.7596\) \\ \(\lambda_{i}=\frac{1}{i\log^{3}(i+1)}\) & \(\Theta(\frac{n}{\log^{3}n})\) & \(0.146\pm 0.0108\) & \(33.469\pm 0.4540\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **The effective dimension \(k_{1}\), the optimal early stopping excess risk and min-norm excess risk for different feature distributions, with sample size \(n=200\), \(p=1000\).** We calculate the 95% confidence interval for the excess risk.

\begin{table}
\begin{tabular}{l c c} \hline \hline Distributions & \(k_{1}\) & Optimal Excess Risk & Min-norm excess risk \\ \hline \(\lambda_{i}=\frac{1}{i}\) & \(\Theta(n)\) & \(1.997\pm 0.0876\) & \(27.360\pm 0.3019\) \\ \(\lambda_{i}=\frac{1}{i^{2}}\) & \(\Theta(n^{\frac{1}{2}})\) & \(0.211\pm 0.0050\) & \(43.531\pm 0.7025\) \\ \(\lambda_{i}=\frac{1}{i^{3}}\) & \(\Theta(n^{\frac{1}{3}})\) & \(0.076\pm 0.0011\) & \(10.062\pm 0.3022\) \\ \(\lambda_{i}=\frac{1}{i\log(i+1)}\) & \(\Theta(\frac{n}{\log n})\) & \(0.645\pm 0.0056\) & \(96.465\pm 1.0594\) \\ \(\lambda_{i}=\frac{1}{i\log^{2}(i+1)}\) & \(\Theta(\frac{n}{\log^{2}n})\) & \(0.289\pm 0.0055\) & \(83.694\pm 0.9827\) \\ \(\lambda_{i}=\frac{1}{i\log^{3}(i+1)}\) & \(\Theta(\frac{n}{\log^{3}n})\) & \(0.181\pm 0.0045\) & \(38.090\pm 0.6378\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: **The effective dimension \(k_{1}\), the optimal early stopping excess risk and min-norm excess risk for different feature distributions, with sample size \(n=100\), \(p=500\).** We calculate the 95% confidence interval for the excess risk.

Figure 4: **The training plot for corrupted MNIST with 20% label noise using GD.**

Figure 3: **The training plot for corrupted MNIST with different levels of label noise using SGD. Figure (c) is copied from Figure 1.**