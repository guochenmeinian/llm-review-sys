# First- and Second-Order Bounds for Adversarial Linear Contextual Bandits

Julia Olkhovskaya

Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlands

Jack Mayo

Korteweg-de Vries Institute for Mathematics, University of Amsterdam, Amsterdam, The Netherlands

Tim van Erven

Korteweg-de Vries Institute for Mathematics, University of Amsterdam, Amsterdam, The Netherlands

Gergely Neu

AI group, DTIC, Universitat Pompeu Fabra, Barcelona, Spain

Chen-Yu Wei

MIT Institute for Data, Systems, and Society, Massachusetts Institute of Technology, Cambridge, MA, USA

###### Abstract

We consider the adversarial linear contextual bandit setting, which allows for the loss functions associated with each of \(K\) arms to change over time without restriction. Assuming the \(d\)-dimensional contexts are drawn from a fixed known distribution, the worst-case expected regret over the course of \(T\) rounds is known to scale as \(\tilde{O}(\sqrt{KdT})\). Under the additional assumption that the density of the contexts is log-concave, we obtain a second-order bound of order \(\tilde{O}(K\sqrt{dV_{T}})\) in terms of the cumulative second moment of the learner's losses \(V_{T}\), and a closely related first-order bound of order \(\tilde{O}(K\sqrt{dL_{T}^{*}})\) in terms of the cumulative loss of the best policy \(L_{T}^{*}\). Since \(V_{T}\) or \(L_{T}^{*}\) may be significantly smaller than \(T\), these improve over the worst-case regret whenever the environment is relatively benign. Our results are obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, which we analyse by exploiting a novel connection to the linear bandit setting without contexts.

## 1 Introduction

The contextual bandit problem is a generalization of the multi-armed bandit setting in which a learner observes relevant contextual information before choosing an arm. The goal of the learner is to minimize the excess cumulative loss of the chosen arms compared to the best fixed policy for mapping contexts to arms. This framework addresses a broad range of important real-world problems like sequential treatment allocation (Tewari and Murphy, 2017), online recommendation (Beygelzimer et al., 2011) or online advertising (Li et al., 2010), and is actively used in practice (Agarwal et al., 2016). Numerous variants of the setting have been studied, which differ in the assumptions they make about the losses and the contexts. In this paper, we focus on the recently introduced setting of Neu and Olkhovskaya (2020) where the contexts are finite-dimensional i.i.d. random vectors, and the losses are time-varying linear functions of the context that may potentially be generated by an adversary. In this setting, the worst-case rate for the expected regret is known to be \(\tilde{O}(\sqrt{T})\) for time horizon \(T\)(Neu and Olkhovskaya, 2020).

Our main contribution is to replace the worst-case rate by adaptive bounds. Specifically, we obtain a bound of \(\tilde{O}(\sqrt{V_{T}})\) in terms of a quadratic measure of variance \(V_{T}\) for the losses of the algorithm, and a bound of \(\tilde{O}(\sqrt{L_{T}^{*}})\), where \(L_{T}^{*}\) is the cumulative loss incurred by the optimal policy. Such bounds in terms of \(L_{T}^{*}\) or \(V_{T}\) are generally referred to as _first-order_ and _second-order bounds_, respectively,and have been extensively studied in the bandit literature. They can lead to much stronger guarantees in the often realistic case when \(T\) is large, but the losses vary little or when there exists a policy with very low cumulative loss.

Worst-case guarantees in terms of \(T\) have first been proved for the contextual bandit problem with finite policy classes by Auer et al. (2002), with further improvements by Beygelzimer et al. (2011). These methods can deal with adversarial losses and contexts, but only work for finite policy classes and have run-time scaling linearly with the size of the class--which is generally unacceptable in practice. This latter challenge has been addressed by a line of work culminating in Agarwal et al. (2014), which only requires access to an optimization oracle over the policy class. Their results, however, remain restricted to i.i.d. contexts and losses. An alternative line of work has been initiated by Auer (2002); Chu et al. (2011); Abbasi-Yadkori et al. (2011), who studied the special case of i.i.d. _linear_ loss functions with changing decision sets. The case of i.i.d. contexts and adversarial linear losses has first been studied by Neu and Olkhovskaya (2020).

Improvements of worst-case guarantees of order \(\sqrt{T}\) to first-order bounds scaling with \(\sqrt{L_{T}^{s}}\) have been known for a variety of bandit settings since the works of Stoltz (2005); Allenberg et al. (2006), and Neu (2015). Regarding contextual bandits, the COLT 2017 open problem of Agarwal et al. (2017) asks for efficient algorithms that achieve first-order bounds for large, but finite, policy classes, either when both contexts and losses are i.i.d. or when both are fully adversarial. First to answer the open problem were Allen-Zhu et al. (2018), who obtained an optimal first-order regret guarantee for adversarial losses and contexts, but with an algorithm that is inefficient for large policy classes. Foster and Krishnamurthy (2021) provide the first efficient algorithm for the non-adversarial setting where the loss function is fixed over time and one has access to an oracle that can solve various optimization tasks over the policy class. We improve on these works in terms of the computational efficiency of our algorithm and by allowing the loss function to vary adversarially over time, although we do rely on the extra assumption that the loss functions are linear.

Another relevant framework is the adversarial linear bandit setting (without contexts), where there also exist adaptive results (Bubeck et al., 2019; Lee et al., 2020; Ito et al., 2020). While conceptually related, an important distinction is that the linear bandit setting assumes a fixed decision set, whereas reducing the linear contextual bandit problem to a linear bandit problem requires the use of decision sets that change as a function of the contexts.

Main Contributions.We consider a \(K\)-armed linear contextual bandit problem with \(d\)-dimensional contexts over \(T\) rounds. The contexts are assumed to be drawn i.i.d., but the linear loss functions mapping contexts to losses for the arms are chosen by an adaptive adversary. The aim of the learner is to minimize their regret, which is the gap between the expected cumulative loss of the learner and the expected cumulative loss of the best fixed policy \(\pi_{T}^{*}\) chosen in full knowledge of the sequence of losses. In this setting, \(\pi_{T}^{*}\) is known to be a linear classifier, i.e. it chooses the arm with smallest predicted loss, where the predictions are fixed linear functions of the context (see Section 2). The goal is therefore to compete with all linear classifiers. We first obtain the following second-order bound on the expected regret

\[R_{T}=\tilde{O}\Big{(}K\sqrt{dV_{T}}\Big{)},\] (1)

where \(V_{T}\) is defined in (5) as a measure of the cumulative second moments of the losses for the arms played by the algorithm. Following Ito et al. (2020), we allow these moments to be centered around optimistic estimates that can further improve the bound when available or can simply be set to zero when they are not. We further obtain a first order bound of the form

\[R_{T}(\pi_{T}^{*})=\tilde{O}\Big{(}K\sqrt{dL_{T}^{*}}\Big{)}.\] (2)

The second-order bound is obtained using a truncated version of the continuous exponential weights algorithm over the probability simplex, similar to the algorithm for linear non-contextual bandits of Ito et al. (2020), and the first-order bound may be obtained as a corollary. As discussed in Section 3.3, the computational complexity of this method is dominated by two steps that together require \(\tilde{O}(K^{5})+(d/\epsilon)^{O(1)}\) per round for approximation up to precision \(\epsilon>0\), which is computationally feasible for moderate \(K\) and \(\epsilon\). Both results are not strict improvements on the worst-case rate of \(\tilde{O}(\sqrt{KdT})\) by Neu and Olkhovskaya (2020): first, they have a slightly worse dependence on \(K\). We consider this a price worth paying for the first adaptive bounds in this setting. Second, they require the extra assumption that the distribution of the contexts is _log-concave_. Although log-concavity is weaker than assuming the contexts follow e.g. (truncated) Gaussian distributions, we conjecture that it may not be necessary to obtain a computationally efficient algorithm. This conjecture is based on the observation that there exists in fact an easy way to obtain at least the first-order bound (2) without the log-concavity assumption, but with an algorithm that has no hope of being efficiently implemented. As described in Section 2.2, this is possible by running the MYGA algorithm (Allen-Zhu et al., 2018) on \(O(\frac{T}{dK^{2}})^{Kd}\) experts that cover the set of linear classifiers to sufficient precision. The run-time of this approach is prohibitive, because it scales linearly with the number of experts, which is a large polynomial in \(T\).

Techniques.The LinExp3 method of Neu and Olkhovskaya (2020) is based on an adaptation of the classic Exp3 algorithm for regular multi-armed bandits (Auer et al., 2002a). A natural approach would therefore be to replace the Exp3 component in LinExp3 by a method with first-order guarantees for the multi-armed bandit setting, but, as discussed in Section D, this leads to difficulties controlling the variance. Instead of building on Exp3, we therefore follow the perhaps surprising approach of building our algorithm on _continuous exponential weights_ over the probability simplex (van der Hoeven et al., 2018). In particular, our approach is based on a combination of the recently proposed techniques of Ito et al. (2020) for linear bandits with tools designed by Neu and Olkhovskaya (2020) to deal with the contextual case.

Outline.The rest of the paper is organized as follows. After describing the setting in the next section, we state a formal version of the simple first-order bound that can be obtained using the MYGA algorithm (Theorem 2.1). This is followed by Section 3, which states our main results corresponding to the regret bounds in Equations 1 and 2. Section 4 then gives a high-level overview of the proofs, with pointers provided to the details in the appendix. Finally, Section 5 concludes with discussion.

## 2 Preliminaries

NotationLet \(\Delta^{K}=\{w\in\mathbb{R}^{K}|w_{1}\geq 0,\ldots,w_{K}\geq 0,\sum_{a=1}^{K}w_{a }=1\}\) denote the \((K-1)\)-dimensional probability simplex. For any positive semi-definite matrix \(M\in\mathbb{R}^{d\times d}\), \(\left\|v\right\|_{M}=\sqrt{v^{\top}Mv}\) denotes the corresponding Mahalanobis norm, and for any positive integer \(n\), we abbreviate \([n]=\{1,\ldots,n\}\).

### Setting

We consider the setting of (Neu and Olkhovskaya, 2020), in which there is an interaction between a learner and an unknown environment. This interaction proceeds in rounds indexed by \(t\in[T]\), such that for each \(t\):

1. The environment commits to \([K]\) parameter vectors \(\theta_{t,1},\ldots,\theta_{t,K}\in\mathbb{R}^{d}\) without revealing any to the learner.
2. A context vector \(X_{t}\in\mathbb{R}^{d}\) is drawn i.i.d. from some fixed distribution \(\mathcal{D}\) according to \(X_{t}\sim\mathcal{D}\), and revealed to the learner.
3. The learner commits to an action \(A_{t}\in[K]\), and incurs the loss \(\ell_{t}(X_{t},A_{t})\), where \(\ell_{t}(X,a)=\langle X,\theta_{t,a}\rangle\).

The environment is allowed to randomize its choices of \(\theta_{t,a}\). These must be independent from the context \(X_{t}\) in round \(t\), but they may depend on previous contexts \(X_{s}\) and actions \(A_{s}\) for \(s<t\).

We write \(\pi_{t}(a|X_{t})\) for the policy of the learner in round \(t\) conditional on observing context \(X_{t}\), so that \(A_{t}\sim\pi_{t}(X_{t})\), and we use the following notation for the expected cumulative losses of the algorithm and policy \(\pi\), respectively:

\[L_{T}=\mathbb{E}\left[\sum_{t=1}^{T}\ell_{t}(X_{t},A_{t})\right],L_{T}^{\pi}= \mathbb{E}\left[\sum_{t=1}^{T}\ell_{t}(X_{t},\pi(X_{t}))\right].\]Let \(\Pi\) be the set of all all stationary deterministic policies \(\pi:\mathbb{R}^{d}\rightarrow[K]\), we define the optimal policy \(\pi^{*}\) as \(\pi^{*}=\text{ arg}\min_{\pi\in\Pi}L_{T}^{\pi}\). Then the learner's goal is to compete with policy \(\pi^{*}\), as measured by the expected regret:

\[R_{T}=L_{T}-L_{T}^{\pi^{*}}=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle X_{t}, \theta_{t,A_{t}}-\theta_{t,\pi^{*}(X_{t})}\right\rangle\right],\]

where the expectation is taken over each \(X_{t}\sim\mathcal{D}\), and any randomness applied by the learner or environment in their respective choices. Using the linearity of the loss functions it can be shown that the optimal policy is always a linear classifier (Neu and Olkhovskaya, 2020):

\[\pi_{T}^{*}(x)=\text{ arg}\min_{a}\left\langle x,\sum_{t=1}^{T}\mathbb{E}[ \theta_{t,a}]\right\rangle.\]

We may therefore restrict attention to competing with policies of the form

\[\pi_{\beta}(x)=\text{ arg}\min_{a}\left\langle x,\beta_{a}\right\rangle\qquad( \beta\in\mathbb{R}^{K\times d}).\] (3)

For deriving our technical results, it will be useful to define the filtration \(\mathcal{F}_{t}=\sigma(\{X_{s},A_{s}:s\leq t\})\), and the notations \(\mathbb{E}_{t}\left[\cdot\right]=\mathbb{E}\left[\cdot|\,\mathcal{F}_{t-1}\right]\) and \(\mathbb{P}_{t}\left[\cdot\right]=\mathbb{P}\left[\cdot|\,\mathcal{F}_{t-1}\right]\).

AssumptionsFollowing Neu and Olkhovskaya (2020), we assume that \(\|X_{t}\|\leq\sigma\), \(\|\theta_{t,a}\|\leq R\) and \(\ell_{t}(x,a)\in[-1,1]\) almost surely. In addition, the covariance matrix \(\Sigma=\mathbb{E}[XX^{\top}]\) of the context distribution is assumed to be positive definite, with smallest eigenvalue \(\lambda_{\min}(\Sigma)>0\).

### An Inefficient Algorithm

A first order bound for our problem can be obtained by instantiating the MYGA algorithm of Allen-Zhu et al. (2018) for a set of \(\Theta(\frac{T}{K^{2}d})^{Kd}\) experts that cover the parameter space of policies of the form (3), which is guaranteed to contain the optimal policy \(\pi_{T}^{*}\):

**Theorem 2.1**.: _Suppose that \(0\leq\ell_{t}(a,X_{t})\leq 1\) almost surely for all \(a\in[K]\). Then, by instantiating MYGA with \(\Theta(\frac{T}{K^{2}d})^{Kd}\) experts, it obtains the following first-order bound for the adversarial linear contextual bandit problem:_

\[R_{T}=O\left(K\sqrt{dL_{T}^{*}\log T}+K^{2}d\log T\right).\] (4)

Although this provides a quick way to see that first-order bounds are possible, the resulting algorithm is completely impractical, because its run-time is proportional to the number of experts, which grows as a large polynomial in \(T\). The proof, including a more detailed description of the experts, can be found in Appendix A.

## 3 First- and Second-Order Bounds

In this section we present an algorithm using a novel adaptation of a method developed for the adversarial linear bandit to be suitable for use in the adversarial linear contextual bandit setting. The method proposed is based on a form of continuous exponential weights that has been shown to lead to a first-order bound in the former (Ito et al., 2020). The algorithm allows for optimistic estimates \(m_{t,a}\in\mathbb{R}^{d}\) for the environment's choices \(\theta_{t,a}\), which can always be set to \(0\) when they are not available. We show two types of guarantees. First, in Theorem 3.1, we obtain a second-order regret bound in terms of the cumulative squared error of the estimates \(m_{t,a}\):

\[V_{T}=\mathbb{E}\left[\sum_{s=1}^{T}\left\langle X_{s},\theta_{s,A_{s}}-m_{s, A_{s}}\right\rangle^{2}\right].\] (5)

Taking \(m_{t,a}=0\), this provides a second-order regret bound in terms of the squared losses. Alternatively, \(m_{t,a}\) may be estimated using an online regression algorithm, as described by Ito et al. (2020). As our second result, we show in Theorem 3.2 that a first-order bound can be derived for the same algorithm with a different choice of hyperparameters and the assumption that the losses are non-negative.

### Algorithm Description

Our full algorithm is shown in Algorithm 1. As it is an adaptation of continuous exponential weights for the contextual bandits setting, we refer to it as ContextEW. It runs a two-stage sampling procedure: after observing context \(X_{t}\), the first stage of the algorithm samples a random policy \(\widetilde{Q}_{t}\in\Delta^{K}\), and then the second stage consists of drawing an arm \(A_{t}\) randomly from \(\widetilde{Q}_{t}\). The distribution of \(\widetilde{Q}_{t}\) is constructed as follows: first we sample a different policy \(Q_{t}\) from the exponential weights distribution over the probability simplex with density proportional to

\[w_{t}(q|X_{t})=\exp(-\eta_{t}\sum_{a=1}^{K}q_{a}\langle X_{t},\sum_{s=1}^{t-1} \widehat{\theta}_{s,a}+m_{t,a}\rangle),\] (7)

where \(m_{s,a}\) is a function that is measurable with respect to \(\mathcal{F}_{s-1}\). The sum \(\sum_{a=1}^{K}q_{a}\left\langle X_{t},\sum_{s=1}^{t-1}\widehat{\theta}_{s,a}\right\rangle\) estimates the cumulative loss that the policy \(q\) would have incurred if it had been played in all previous rounds. It relies on estimates \(\widehat{\theta}_{s,a}\) of the loss vectors \(\theta_{s,a}\), which will be defined below, and a time-varying learning rate \(\eta_{t}>0\), which is hyperparameter of the algorithm. The normalized density function corresponding to the weights in (7) is:

\[p_{t}(q|X_{t})=\frac{w_{t}(q|X_{t})}{\int_{\Delta^{K}}w_{t}(q|X_{t})dq}.\] (8)

Following Ito et al. (2020), we then introduce a rejection sampling step (6) to reduce the variance, which is based on the following covariance matrices \(\Sigma_{t,a}\) corresponding to \(Q_{t}\):

\[\Sigma_{t,a}=\mathbb{E}_{t}\left[Q_{t,a}^{2}X_{t}X_{t}^{\intercal}\right],\] (9)

so that \(\widetilde{Q}_{t}\) ends up being sampled according to the following truncated exponential weights density:

\[\tilde{p}_{t}(q|X_{t})=\frac{p_{t}(q|X_{t})\mathds{1}\left\{\sum_{a=1}^{K} \|q_{a}X_{t}\|_{\Sigma_{t,a}^{-1}}^{2}\leq dK\gamma^{2}\right\}}{\mathbb{P}_{t }\left[\sum_{a=1}^{K}\|q_{a}X_{t}\|_{\Sigma_{t,a}^{-1}}^{2}\leq dK\gamma^{2}|X _{t}\right]},\] (10)

with truncation level hyperparameter \(\gamma>0\). We will show that all \(\Sigma_{t,a}\) are invertible, as are their analogues in which \(Q_{t}\) is replaced by \(\widetilde{Q}_{t}\):

\[\widetilde{\Sigma}_{t,a}=\mathbb{E}_{t}\left[\widetilde{Q}_{t,a}^{2}X_{t}X_{t }^{\intercal}\right].\] (11)

It remains to specify our estimators for \(\theta_{t,a}\), which are defined as follows:

\[\widehat{\theta}_{t,a}=m_{t,a}+\widetilde{Q}_{t,a}\widetilde{\Sigma}_{t,a}^{- 1}X_{t}\left(\langle X_{t},\theta_{t,a}\rangle-\langle X_{t},m_{t,a}\rangle \right)\mathds{1}\left\{A_{t}=a\right\}.\] (12)These estimates can be shown to be unbiased:

\[\mathbb{E}_{t}\left[\widehat{\theta}_{t,a}\right] =m_{t,a}+\widetilde{\Sigma}_{t,a}^{-1}\mathbb{E}_{t}\left[\widetilde {Q}_{t,a}X_{t}X_{t}^{\mathsf{T}}\mathds{1}\left\{A_{t}=a\right\}\right](\theta _{t,a}-m_{t,a})\] \[=m_{t,a}+\widetilde{\Sigma}_{t,a}^{-1}\mathbb{E}_{t}\left[ \widetilde{Q}_{t,a}^{2}X_{t}X_{t}^{\mathsf{T}}\right](\theta_{t,a}-m_{t,a})= \theta_{t,a}.\]

### Results

We instantiate ContextEW with adaptive learning rates \(\eta_{t}\). For our second-order result, these are defined in terms of the empirical counterpart to \(V_{t}\): \(\widehat{V}_{t}=\sum_{s=1}^{t}\left\langle X_{s},\theta_{s,A_{s}}-m_{s,A_{s}} \right\rangle^{2},\) and we abbreviate \(G_{t}=8\sqrt{\widehat{V}_{t-1}\ln(2T^{2})+144\ln^{2}T+176\ln T}.\) Then we set

\[\eta_{t}=(100dK\gamma^{2}+d(\widehat{V}_{t-1}+1+G_{t-1}))^{-1/2}.\] (13)

This leads to the following second-order bound:

**Theorem 3.1** (Second-Order).: _Suppose \(\mathcal{D}\) has a log-concave density. Then, for \(\gamma=4\log(10dKT)\), \(\eta_{t}\) as in (13) and any \(\mathcal{F}_{t-1}\)-measurable estimates \(m_{t}\), the expected regret of ContextEW is at most \(R_{T}=\widetilde{O}(K\sqrt{d\widehat{V}_{T}}).\)_

To tune \(\eta_{t}\) adaptively for our first-order bound, we define it using the algorithm's empirical cumulative loss \(\widehat{L}_{t}=\sum_{s=1}^{t}\ell_{t}(X_{s},A_{s}),\) which acts as a self-confident empirical estimate of \(L_{T}^{*}\). We further abbreviate

\[H_{t}=8\sqrt{2\widehat{L}_{t}\ln T+40\ln^{2}T}+72\ln T,\] (14)

and then set

\[\eta_{t}=(100d\gamma^{2}+dK(\widehat{L}_{t-1}+1+H_{t-1}))^{-1/2}.\] (15)

This leads to the following first-order bound:

**Theorem 3.2** (First-Order).: _Suppose that \(\mathcal{D}\) has a log-concave density and that \(0\leq\ell_{t}(a,X_{t})\leq 1\) almost surely for all \(a\in[K]\). Then, for \(\gamma=4\log(10dKT)\), \(\eta_{t}\) as in (15) and \(m_{t}=0\), the expected regret of ContextEW is at most \(R_{T}=\widetilde{O}(K\sqrt{dL_{T}^{*}}).\)_

### Computational Efficiency

The two computational bottlenecks in the algorithm are the cost of sampling from the output distribution \(p_{t}(q|X_{t})\) and computation of the covariance matrices \(\Sigma_{t,a}\) in each round.

Due to the log-linearity of our method, there exists several practical methods of sampling. As mentioned in Ito et al. (2020), one can employ the methods of Lovasz and Vempala (2007), which was shown in Lovasz and Vempala (2006) to enjoy a bound of \(O(K^{4}\log(1/\epsilon))\) (where \(\epsilon\) is a bound on the total variation distance between the output distribution and the target), but this still requires knowledge of a density dominating the target distribution on all but a set with total starting mass \(\leq\epsilon/2\). In Narayanan and Rakhlin (2017), a method is developed for general log-concave distributions which, specialized to log-linear distributions (and without additional assumptions on the initial distribution) yields an \(O(K^{3}\nu^{2}+\log(1/\epsilon))\) method when the geometry admits a \(\nu\)-self concordant barrier. Since there always exists a \(K\)-self-concordant barrier for a \(K\)-dimensional convex body, and thus the running time of this method for our problem is \(O(K^{5}+\log T)\) up to a precision \(\epsilon\sim\frac{1}{T^{\beta}}\) for some \(\beta>0\). As referred to in Ito et al. (2020), the covariance matrix \(\Sigma_{t,a}\) is computable in \(\mathcal{O}((d/\epsilon)^{O(1)})\) sampling steps drawing upon the results of Lovasz and Vempala (2007).

## 4 Analysis

In this section we provide the analysis of ContextEW from which Theorems 3.1 and 3.2 follow. Throughout the analysis, we will be extensively using the following property of log-concave distributions:

**Lemma 4.1**.: _If \(x\) follows a log-concave distribution \(p\) over \(\mathbb{R}^{d}\) and \(\mathbb{E}\left[xx^{\mathsf{T}}\right]\preccurlyeq I\), we have, for any \(\alpha\geq 0:\)_

\[\mathbb{P}\left[\left\|x\right\|_{2}^{2}\geq d\alpha^{2}\right]\leq d\exp(1- \alpha).\] (16)This result was proven in Lemma 1 in Ito et al. (2020), and also follows from Lemma 5.7 in Lovasz and Vempala (2007).

First, we need to introduce some notation which will be useful for the reduction to the linear bandit setting and for the accompanying proofs. We denote \(z_{a}(q,x)=q_{a}x\) and \(z(q,x)=(z_{1}(q,x),\ldots,z_{K}(q,x))^{\intercal}\). We also define \(\Sigma_{t}=\operatorname{diag}_{\alpha\in[K]}(\Sigma_{a,t})\) as a block diagonal arrangement of the covariance matrices per arm. Using this notation, the distribution of the sampling algorithm (10) may be rewritten as

\[\tilde{p}_{t}(q|x)=\frac{p_{t}(q|x)\mathds{1}\left\{\left\|z(q,x)\right\|_{ \Sigma_{t}^{-1}}^{2}\leq dK\gamma^{2}\right\}}{\mathbb{P}_{t}\left[\left\|z(q, x)\right\|_{\Sigma_{t}^{-1}}^{2}\leq dK\gamma^{2}\right]}.\] (17)

Let \(\widetilde{Q}_{t}(x)\sim\tilde{p}_{t}(q|x)\), \(Q_{t}(x)\sim p_{t}(q|x)\) and \(\tilde{Z}_{t}(x)=z(\widetilde{Q}_{t}(x),x)\), \(Z_{t}(x)=z(Q_{t}(x),x)\), \(Z^{*}(x)=z(\pi^{*}(x),x)\). And we denote the aggregated loss parameter \(\theta_{t}=(\theta_{1},\ldots,\theta_{K})^{\intercal}\) and its estimate \(\widehat{\theta}_{t}=(\widehat{\theta}_{1},\ldots,\widehat{\theta}_{K})^{\intercal}\). Then we can express the regret as follows:

\[R_{T}=\mathbb{E}\left[\sum_{t=1}^{T}\ell_{t}(X_{t},A_{t})-\ell_{t}(X_{t},\pi^{ *}(X_{t}))\right]=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\tilde{Z}_{t}(X_ {t})-Z^{*}(X_{t}),\theta_{t}\right\rangle\right].\] (18)

The crucial observation is that the log-concavity of the distribution of \(Z_{t}(X_{t})\) follows from that of the distribution of \(X_{t}\):

**Lemma 4.2**.: _Suppose \(z(q,x)=\sum_{a}q_{a}\varphi(x,a)\) for \(\varphi(x,a)=(\bar{0}^{\intercal},\ldots,x^{\intercal},\cdots)\) such that \(x\) is on the \(da\)'th co-ordinate and \(Q(x)\sim p_{t}(\cdot|x)\) for \(p_{t}(\cdot|x)\) defined in (8). If \(X\sim p_{X}(\cdot)\) and \(p_{X}(\cdot)\) is log-concave and \(Z(x)=z(Q_{t}(x),x)\), then \(Z(X)\) also follows a log-concave distribution._

The proof of this result is a rather straightforward computation of the density of \(Z_{t}(X_{t})\) and can be found in Appendix C. To proceed, we write regret as a sum of two terms

\[R_{t}=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\widetilde{Z}_{t}(X_{t})-Z_{t }(X_{t}),\theta_{t}\right\rangle\right]+\mathbb{E}\left[\sum_{t=1}^{T}\left \langle Z_{t}(X_{t})-Z^{*}(X_{t}),\theta_{t}\right\rangle\right].\] (19)

Having shown that \(Z_{t}(X_{t})\) is log-concave, and since the log-concavity is preserved under linear transformations, for \(y=\Sigma_{t}^{-1/2^{\intercal}}Z_{t}(X_{t})\) we can see that \(\mathbb{E}\left[yy^{\intercal}\right]=I\), and thus by Lemma 4.1 it immediately follows that the probability that (6) is not satisfied is small for a proposed choice of \(\gamma=4\log(10dKT)\):

\[\mathbb{P}_{t}\left[\left\|Z_{t}(X_{t})\right\|_{\Sigma_{t}^{-1}}^{2}>dK \gamma^{2}\right]\leq dK\exp(1-\gamma)\leq 3dK\exp(-\gamma)\leq\frac{1}{6T^{2}}.\]

Using this observation, we show that the first term of (19) is just \(\mathcal{O}(1)\), which is formally proved in Lemma C.2 in the appendix.

To control the second term of the regret decomposition (19), consider the reduction of the contextual bandit problem to a combination of auxiliary online learning problems that are defined separately for each context, as proposed in Neu and Olkhovskaya (2020), Lemma 3. More details and a full proof can be found in Appendix C.

**Lemma 4.3**.: _Let \(\pi^{*}\) be any fixed stochastic policy and let \(X_{0}\sim\mathcal{D}\) be a sample from the context distribution independent from \(\mathcal{F}_{T}\). Suppose that \(p_{t}\in\mathcal{F}_{t-1}\), such that \(p_{t}(\cdot|x)\) is a probability density with respect to Lebesgue measure with support \(\Delta^{K}\) and let \(Q_{t}(x)\sim p_{t}(\cdot|x)\). Then,_

\[\mathbb{E}_{t}\left[\left\langle Z_{t}(X_{t})-Z^{*}(X_{t}),\theta_{t}\right\rangle \right]=\mathbb{E}_{t}\left[\left\langle Z_{t}(X_{0})-Z^{*}(X_{0}),\widehat{ \theta}_{t}\right\rangle\right].\] (20)

To see why this would be useful further in the proof, we interpret the right-hand side of (20) as follows. Consider the online learning problem for a fixed \(x\) with the decision set to be \(\Delta^{K}\) and losses \(\ell_{t}(x,q)=\left\langle z(q,x),\widehat{\theta}_{t}\right\rangle\) and consider running a version of a contextual bandit problem with a fixed context \(x\), such that the probability of an action \(q\) defined as in Equation 8, so \(p_{t}(q|x)\propto\exp\left(-\eta_{t}\sum_{a=1}^{K}q_{a}\left\langle x,\sum_{s=1} ^{t-1}\widehat{\theta}_{s,a}\right\rangle\right)\). Then, the regret for the fixed \(x\) against \(\pi^{*}(x)\) can be written as:

\[\widehat{R}_{T}(x)=\sum_{t=1}^{T}\mathbb{E}_{Q_{t}(x)\sim p_{t}(\cdot|x)}\left[ \left\langle z(Q_{t}(x),x)-z(\pi^{*}(x),x),\widehat{\theta}_{t}\right\rangle \right].\]Then it is easy to see that the right-hand side of (20) is equal to \(\mathbb{E}\left[\widehat{R}_{T}(X_{0})\right]\). Thus, we first show a bound on \(\widehat{R}_{T}(x)\) that holds almost surely for any \(x\) and then take an expectation with respect to \(X_{0}\). We control the regret \(\widehat{R}_{T}(x)\) by following the general schema of the optimistic mirror descent analysis developed in (Rakhlin and Sridharan, 2013; Ito et al., 2020). With this analysis, we get the following bound for any \(x\in\mathcal{X}\) :

**Lemma 4.4**.: _Assume that \(\eta_{t+1}\leq\eta_{t}\) for all \(t\), let \(q_{0}\) be a uniform distribution over \([K]\) and \(\psi(y)=\exp(y)-y-1\). Then, the regret \(\widehat{R}_{T}(x)\) of ContextEW almost surely satisfies_

\[\widehat{R}_{T}(x) \leq\frac{1}{T}\sum_{t=1}^{T}\left\langle z(q_{0}-\pi^{*}(x),x), \ \widehat{\theta}_{t}\right\rangle+\frac{K\log T}{\eta_{T}}\] \[+\sum_{t=1}^{T}\frac{1}{\eta_{t}}\mathbb{E}_{Q_{t}(x)\sim p_{t}( \cdot|x)}\left[\psi\left(-\eta_{t}\left\langle z(Q_{t}(x),x),\widehat{\theta} _{t}-m_{t}\right\rangle\right)\right],\] (21)

_for \(\psi(y)=\exp(y)-y-1\)._

We place the derivation of the this bound in the appendix. The crucial ingredient is to show that the square of the estimated loss can be bounded by the square of the true loss. Using the definition of \(\theta_{t}\), denoting \(Var_{t}=\text{tr}\left(\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{0})Z_{t}(X_{0})^{ \intercal}\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{\intercal}\right)\), we get

\[\mathbb{E}_{t}\left[\left(-\eta_{t}\left\langle Z_{t}(X_{0}), \widehat{\theta}_{t}-m_{t}\right\rangle\right)^{2}\right]=\mathbb{E}_{t}\left[ \eta_{t}^{2}\left(\ell_{t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^ {2}Var_{t}\right],\] (22)

As additional corollary of the concentration result for log-concave random variables, we can show the following relation between matrices \(\Sigma_{t}\) and \(\widetilde{\Sigma}_{t}\):

\[\frac{3}{4}\Sigma_{t}\preceq\widetilde{\Sigma}_{t}\preceq\frac{4}{3}\Sigma_{t},\] (23)

which we prove in Lemma C.2 in the appendix. Then we can show that, almost surely:

\[\mathbb{E}_{X_{0}}\left[Var_{t}\right]=\mathbb{E}_{X_{0}}\left[ \text{tr}\left(\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{0})Z_{t}(X_{0})^{ \intercal}\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{\intercal} \right)\right]\] \[=\text{tr}\left(\widetilde{\Sigma}_{t}^{-1}\Sigma_{t}\widetilde{ \Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{\intercal}\right)\leq\frac{4}{3} \text{tr}\left(\widetilde{\Sigma}_{t}^{-1}\widetilde{\Sigma}_{t}\widetilde{ \Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{\intercal}\right)\] \[=\frac{4}{3}Z_{t}(X_{t})^{\intercal}\widetilde{\Sigma}_{t}^{-1}Z _{t}(X_{t})\leq Z_{t}(X_{t})^{\intercal}\Sigma_{t}^{-1}Z_{t}(X_{t})\leq dK \gamma^{2}.\] (24)

where the first inequality follows from (23) and the second inequality is immediate from (23) and the fact that for symmetric positive definite matrices \(A\succeq B\) follows from \(B^{-1}\succeq A^{-1}\). The last inequality follows from (6) in the ContextEW. So, from (22) and (35), we get

\[\mathbb{E}_{t}\left[\left(-\eta_{t}\left\langle Z_{t}(X_{0}), \widehat{\theta}_{t}-m_{t}\right\rangle\right)^{2}\right]\leq dK\gamma^{2} \mathbb{E}_{t}\left[\eta_{t}^{2}\left(\ell_{t}(A_{t},X_{t})-X_{t}^{\intercal}m_ {t,A_{t}}\right)^{2}\right],\]

which, as we stated above, is the key step to prove Theorem 3.1.

First-order regret boundTo prove result of Theorem 3.2, we show that the bound in the Theorem 3.1 can instantiated to obtain a first-order regret bound with a different choice of the learning rate \(\eta_{t}\). Going along the same lines with regard to the concentration of \(\widehat{L}_{t}\) as for \(\widehat{V}_{t}\), by setting \(m_{t}=0\) and noticing that then \(V_{T}\leq L_{T}\) we get

\[R_{T}\leq 2dK\gamma^{2}\mathbb{E}\left[\sum_{t=1}^{T}\eta_{t}\ell_{t}(A_{t},X_{t} )^{2}\right]+\widetilde{\mathcal{O}}(K\sqrt{dV_{T}})\leq 4\sqrt{d}K\gamma^{2} \sqrt{L_{T}}+\widetilde{\mathcal{O}}(K\sqrt{dL_{T}}).\]

Since \(R_{T}=L_{t}-L_{T}^{*}\), by solving the quadratic inequality with respect to \(L_{T}^{*}\), we get that \(L_{T}\leq L_{T}^{*}+\widetilde{\mathcal{O}}(K\sqrt{d})\), yielding the final bound.

## 5 Discussion

In conclusion, by applying the approach of Ito et al. (2020) we have constructed the first scheme achieving \(\tilde{O}\left(K\sqrt{dL_{T}^{*}}\right)\) regret with a runtime of \(\mathcal{O}\left(\left(K^{5}+\log T\right)\cdot g_{\Sigma}\right)\), where \(g_{\Sigma}\) is the time taken to construct the covariance matrix per round - a potentially large polynomial improvement over the \(\mathcal{O}\left(T^{Kd}\right)\) runtime of MYGA. The application of linear bandit algorithms to the contextual bandit problem constitutes, to the best of our knowledge, a novel approach. In doing so we've found a number of positive aspects, including efficiency, but also the direct applicability of other properties enjoyed by the algorithm such as second order bounds Ito et al. (2020).

Our approach is based on reducing the linear contextual bandit problem to a linear bandit problem, as opposed to a multi-armed bandit problem as in Neu and Olkhovskaya (2020). While the specifics of this reduction heavily relied on the joint log-concavity of the context distributions and the exponential-weights posterior over the simplex of actions, we wonder if such approaches can be successfully applied to achieve other types of improvements for linear contextual bandits. In particular, it is curious to what extent other recent advances in the linear bandit problem can be translated to the linear contextual bandit setting. Note that, while the truncation step in Algorithm 1 has an insignificant computational cost as the condition is satisfied with probability \(\mathcal{O}(1-1/T)\), it can be removed by paying a \(\log(1/\lambda_{min}(\Sigma))\) multiplicative term in the regret by implementing additional exploration with probability \(1/T\). It is natural to ask whether or not approaches based on other instantiations of online mirror descent would also yield first-order bounds, and possibly improve the dependence on \(K\). The answer is not obvious: for an example of how a naive application of an instantiation of FTRL fails to achieve a first-order bound, see Appendix D.

A relevant question pertains to whether or not such an application of algorithms for linear bandits is necessary at all, but standard approaches such as direct adaptation of Exp3, and first-order adaptations thereof such as GREEN Allenberg et al. (2006) do not seem to give the desired result.In addition, thresholding the worst performing arms inevitably biases the loss estimator due to undersampling of those arms for which the threshold has been applied, and the resulting additional bias term picked up in the regret scales with \(1/\lambda_{\min}(\Sigma_{t,a})\), which may be arbitrarily large. Another standard approach of finding an optimistic estimator yielded no fruit during the course of this study due to the lack of the existence of such an estimator without saving all previous losses explicitly.

Our algorithm achieves the regret bound \(\mathcal{O}(K\sqrt{dV_{T}})\), while the worst case guarantee of LinExp3 of Neu and Olkhovskaya (2020) is \(\mathcal{O}(\sqrt{dKT})\). This discrepancy is not surprising as the Algorithm 1 of Ito et al. (2020) scales as \(\mathcal{O}(n\sqrt{T})\) (\(n\) being the dimension of the action space for the linear bandit), which arises from the deployment of continuous exponential weights. MYGA achieves the same \(\mathcal{O}(K\sqrt{dL_{T}^{*}})\) bound due to the number of experts needed to cover the joint set of additive loss parameters. It is worth here emphasising that no known algorithm achieves a better dependence on \(K\) than \(\mathcal{O}(K\sqrt{dL_{T}^{*}})\) for the linear adversarial contextual bandit problem. Meanwhile, if the linear bandit is played on the \(n\)-simplex, an improvement to \(\sqrt{nT}\) is possible. For further discussion of this point, see Section 28.5 of Lattimore and Szepesvari (2020). It is thus still unclear whether or not the extra factor of \(\sqrt{K}\) is necessary if one aims for a first-order bound.

An additional point is that while the MYGA algorithm Allen-Zhu et al. (2018) allows for adversarially chosen contexts, the analysis of MYGA for our setting relies heavily on the assumption that contexts are drawn i.i.d. at each iteration. A natural question is then whether or not a similar result is achievable in the adversarial context case. It is known that achieving sub-linear regret is not possible even for full-information online learning of one-dimensional threshold classifiers when both contexts and losses are adversarial Ben-David et al. (2009); Syrgkanis et al. (2016), which renders sub-linear regret similarly impossible to guarantee for the even harder setting that we consider in this paper. However, we do conjecture that we could overcome the assumption that the distribution is known or that we can sample from it by employing a more elaborate algorithm to estimate the distribution from the data. Indeed, it is not obvious if the distributional assumption of a lower bound to the covariance matrix eigenvalues is entirely necessary, since the regret does not depend on this.

Lastly, it would be an interesting challenge to see if a high-probability regret bound could be obtained in the form stated in the COLT 2017 open problem Agarwal et al. (2017) for this setting, but since a high-probability \(O(\sqrt{T})\) has not yet been proved for the problem here considered, the latter may be more worthy of focus in the short term.

## 6 Acknowledgments

Tim van Erven and Jack Mayo were supported by the Netherlands Organization for Scientific Research (NWO) under grant number VI.Vidi.192.095. Gergely Neu was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant agreement No. 950180). Chen-Yu Wei would like to acknowledge the support from Simons-Berkeley Research Fellowship.

## References

* Abbasi-Yadkori et al. (2011) Abbasi-Yadkori, Y., Pal, D., and Szepesvari, Cs. (2011). Improved algorithms for linear stochastic bandits. In _Advances in Neural Information Processing Systems (NIPS)_.
* Agarwal et al. (2016) Agarwal, A., Bird, S., Cozowicz, M., Hoang, L., Langford, J., Lee, S., Li, J., Melamed, D., Oshri, G., Ribas, O., Sen, S., and Slivkins, A. (2016). Making contextual decisions with low technical debt.
* Agarwal et al. (2014) Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and Schapire, R. (2014). Taming the monster: A fast and simple algorithm for contextual bandits. In Xing, E. P. and Jebara, T., editors, _Proceedings of the 31st International Conference on Machine Learning_, volume 32 of _Proceedings of Machine Learning Research_, pages 1638-1646, Bejing, China. PMLR.
* Agarwal et al. (2017) Agarwal, A., Krishnamurthy, A., Langford, J., Luo, H., and Schapire, R. E. (2017). Open problem: First-order regret bounds for contextual bandits. In Kale, S. and Shamir, O., editors, _Proceedings of the 2017 Conference on Learning Theory_, volume 65 of _Proceedings of Machine Learning Research_, pages 4-7. PMLR.
* Allen-Zhu et al. (2018) Allen-Zhu, Z., Bubeck, S., and Li, Y. (2018). Make the minority great again: First-order regret bound for contextual bandits. In Dy, J. and Krause, A., editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 186-194. PMLR.
* Allenberg et al. (2006) Allenberg, C., Auer, P., Gyorfi, L., and Ottucsak, G. (2006). Hannan consistency in on-line learning in case of unbounded losses under partial monitoring. In Balcazar, J. L., Long, P. M., and Stephan, F., editors, _Algorithmic Learning Theory_, pages 229-243, Berlin, Heidelberg. Springer Berlin Heidelberg.
* Auer (2002) Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3:397-422.
* Auer et al. (2002a) Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002a). Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2):235-256.
* Auer et al. (2002b) Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002b). The nonstochastic multiarmed bandit problem. _SIAM J. Comput._, 32(1):48-77.
* Bartlett et al. (2008) Bartlett, P., Dani, V., Hayes, T., Kakade, S., Rakhlin, A., and Tewari, A. (2008). High-probability regret bounds for bandit online linear optimization. pages 335-342.
* Ben-David et al. (2009) Ben-David, S., Pal, D., and Shalev-Shwartz, S. (2009). Agnostic online learning.
* Beygelzimer et al. (2011) Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and Schapire, R. (2011). Contextual bandit algorithms with supervised learning guarantees. In Gordon, G., Dunson, D., and Dudik, M., editors, _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15 of _Proceedings of Machine Learning Research_, pages 19-26, Fort Lauderdale, FL, USA. PMLR.
* Bubeck et al. (2019) Bubeck, S., Li, Y., Luo, H., and Wei, C.-Y. (2019). Improved path-length regret bounds for bandits. In Beygelzimer, A. and Hsu, D., editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 508-528. PMLR.
* Chu et al. (2011) Chu, W., Li, L., Reyzin, L., and Schapire, R. (2011). Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214.

Foster, D. J. and Krishnamurthy, A. (2021). Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. volume 23, page 18907 - 18919.

* Ito et al. (2020) Ito, S., Hirahara, S., Soma, T., and Yoshida, Y. (2020). Tight first- and second-order regret bounds for adversarial linear bandits. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2028-2038. Curran Associates, Inc.
* Lattimore and Szepesvari (2020) Lattimore, T. and Szepesvari, C. (2020). _Bandit Algorithms_. Cambridge University Press.
* Lee et al. (2020) Lee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. (2020). Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, _Advances in Neural Information Processing Systems_, volume 33, pages 15522-15533. Curran Associates, Inc.
* Li et al. (2010) Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th international conference on World wide web_, pages 661-670. ACM.
* Lovasz and Vempala (2006) Lovasz, L. and Vempala, S. (2006). Hit-and-run from a corner. _SIAM Journal on Computing_, 35(4):985-1005.
* Lovasz and Vempala (2007) Lovasz, L. and Vempala, S. (2007). The geometry of logconcave functions and sampling algorithms. _Random Structures & Algorithms_, 30(3):307-358.
* Narayanan and Rakhlin (2017) Narayanan, H. and Rakhlin, A. (2017). Efficient sampling from time-varying log-concave distributions. _Journal of Machine Learning Research_, 18(112):1-29.
* Neu (2015) Neu, G. (2015). First-order regret bounds for combinatorial semi-bandits. In _Conference on Learning Theory_, pages 1360-1375. PMLR.
* Neu and Olkhovskaya (2020) Neu, G. and Olkhovskaya, J. (2020). Efficient and robust algorithms for adversarial linear contextual bandits. In _Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)_, pages 3049-3068.
* Volume 2_, NIPS'13, page 3066-3074, Red Hook, NY, USA. Curran Associates Inc.
* Stoltz (2005) Stoltz, G. (2005). _Incomplete information and internal regret in prediction of individual sequences_. PhD thesis, Universite Paris Sud-Paris XI.
* Syrgkanis et al. (2016) Syrgkanis, V., Krishnamurthy, A., and Schapire, R. (2016). Efficient algorithms for adversarial contextual learning. In Balcan, M. F. and Weinberger, K. Q., editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2159-2168, New York, New York, USA. PMLR.
* Sensors, Analytic Methods, and Applications_.
* van der Hoeven et al. (2018) van der Hoeven, D., van Erven, T., and Kotlowski, W. (2018). The many faces of exponential weights in online learning. In _Conference On Learning Theory_, pages 2067-2092.

First-order Bound by Reduction to MYGA

Proof.: The MYGA algorithm of Allen-Zhu et al. (2018) competes with a class of experts \(E\), where each expert \(e\in E\) provides a stochastic prediction \(\xi_{t}^{e}\in\Delta_{K}\) in each round \(t\). It provides the following expected regret bound with respect to the best expert:

\[R_{T}=O\left(\sqrt{K\mathrm{log}(|E|+T)L_{T}^{*}}+K\mathrm{log}(|E|+T)\right).\] (25)

Losses for the arms can be adversarial, and are assumed to take values in \([0,1]\).

We will instantiate the experts to cover the parameter space \(\{\beta\in\mathbb{R}^{K\times d}:\max_{a}\|\beta_{a}\|\leq RT\}\) of potentially optimal parameters for deterministic policies of the form (3), which we know must contain the optimal policy \(\pi_{T}^{*}\) with corresponding parameters \(\beta^{*}=\mathbb{E}[\sum_{t=1}^{T}\theta_{t}]\). The covering number for a ball of radius \(RT\) at precision \(\epsilon>0\) is between \(\left(\frac{RT}{\epsilon}\right)^{d}\) and \(\left(\frac{3RT}{\epsilon}\right)^{d}\), so by taking the Cartesian product of this covering with itself \(K\) times we can cover all \(\beta\) with \(\left(\frac{RT}{\epsilon}\right)^{Kd}\leq|E|\leq\left(\frac{3RT}{\epsilon} \right)^{Kd}\) points \(\beta^{1},\ldots,\beta^{|E|}\). Let \(\breve{\beta}\in\{\beta^{1},\ldots,\beta^{|E|}\}\) be the closest point in the covering to the optimal parameters \(\beta^{*}\). Then its expected approximation error can be upper bounded as follows:

\[\mathbb{E}\left[\sum_{t=1}^{T}\left\langle X_{t},\theta_{t,\pi_{ \beta}(X_{t})}\right\rangle-\left\langle X_{t},\theta_{t,\pi_{\beta^{*}}(X_{t} )}\right\rangle\right] =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle X_{0},\theta_{t,\pi_ {\beta}(X_{0})}\right\rangle-\left\langle X_{0},\theta_{t,\pi_{\beta^{*}}(X_{ 0})}\right\rangle\right]\] \[=\mathbb{E}\left[\left\langle X_{0},\beta^{*}_{\pi_{\beta}(X_{0} )}\right\rangle-\left\langle X_{0},\beta^{*}_{\pi_{\beta^{*}}(X_{0})}\right\rangle\right]\] \[\leq\mathbb{E}\left[\left\langle X_{0},\beta^{*}_{\pi_{\beta^{*} }(x_{0})}\right\rangle-\left\langle X_{0},\beta^{*}_{\pi_{\beta^{*}}(x_{0})} \right\rangle\right]+2\sigma\epsilon\] \[=2\sigma\epsilon.\]

Adding this to (25), instantiated with \(|E|\leq\left(\frac{3RT}{\epsilon}\right)^{Kd}\), and choosing \(\epsilon=\frac{dK^{2}}{2}\) completes the proof. 

## Appendix B Auxiliary lemmas

To ensure that step 2 in ContextEW is defined correctly, we show that the matrix \(\Sigma_{t}\) is full rank:

**Lemma B.1**.: _Let the distribution of \(X_{t}\) be such that \(\lambda_{\min}(\mathbb{E}\left[X_{t}X_{t}^{T}\right])>0\). Then, we can show_

\[\lambda_{\min}(\Sigma_{t,a})>0\] (26)

_for any \(a\in[K]\), and consequently_

\[\lambda_{\min}(\Sigma_{t})>0\] (27)

Proof.: To show that \(\Sigma_{t,a}\) is full rank, it suffices to show that there is no \(v\in\mathbb{R}^{d}\) such that \(v^{\top}\Sigma_{t,a}v=0\). Suppose, to the contrary, that such a \(v\) does exist. Then \(0=v^{\top}\mathbb{E}_{t}\left[Q_{t,a}^{2}(X_{t})X_{t}X_{t}^{\top}\right]v= \mathbb{E}_{t}\left[Q_{t,a}^{2}(v^{\top}X_{t})^{2}\right]\), which implies that \(Q_{t,a}v^{\top}X_{t}=0\) almost surely. Since \(Q_{t,a}>0\) almost surely, it follows that in fact \(v^{\top}X_{t}=0\) almost surely and therefore \(0=\mathbb{E}_{t}\left[(v^{\top}X_{t})^{2}\right]=v^{\top}\mathbb{E}_{t}\left[X _{t}X_{t}^{\top}\right]v\). But this contradicts our assumption that \(\lambda_{\min}(\mathbb{E}_{t}\left[X_{t}X_{t}^{\top}\right])>0\). 

We will use a simple corollary of Freedman's inequality Freedman (1975) that was introduced in Lemma 2 in Bartlett et al. (2008):

**Lemma B.2**.: _Let \(Y_{1},\ldots,Y_{t}\) be a martingale difference sequence with respect to a filtration \(\mathcal{F}_{1}\subset\cdots\subset\mathcal{F}_{t}\) such that \(\mathbb{E}\left[Y_{s}\left|\mathcal{F}_{s}\right.\right]=0\). Suppose that \(Y_{s}\leq b\) holds almost surely. Then with probability at least \(1-\delta\) we have \(\sum_{s=1}^{t}Y_{s}\leq 2\max\{2\sqrt{\sum_{s=1}^{t}\mathbb{E}\left[Y_{s}^{2} \left|\mathcal{F}_{s}\right.\right]},b\sqrt{\ln(1/\delta)}\}\sqrt{\ln(1/\delta)}\)._In our analysis, we will use some results from Ito et al. (2020). We will make use of the following concentration property of log-concave distributions, Lemma 1 from Ito et al. (2020):

**Lemma B.3**.: _If \(y\) follows a log-concave distribution \(p\) over \(\mathbb{R}^{d}\) and \(\mathbb{E}_{y\sim p}\left[yy^{\intercal}\right]\preccurlyeq I\), we have_

\[\mathbb{P}\left[\left\|y\right\|_{2}^{2}\geq d\alpha^{2}\right]\leq d\exp(1- \alpha),\]

_for arbitrary \(\alpha\geq 0\)._

The lemma presented below introduces a property that will be instrumental in analyzing the variance of loss estimates, presented as Lemma 6 in Ito et al. (2020):

**Lemma B.4**.: _If \(y\) follows a log-concave distribution over \(\mathbb{R}\), and if \(\mathbb{E}\left[y^{2}\right]\leq 1/100\), we have_

\[\mathbb{E}\left[\psi(y)\right]\leq\mathbb{E}\left[y^{2}\right]+30\exp\left(- \frac{1}{\sqrt{\mathbb{E}\left[y^{2}\right]}}\right)\leq 2\mathbb{E}\left[y^{2} \right],\text{ where }\psi(x)=\exp(x)-x-1.\]

## Appendix C Proof of Theorem 3.1

The proof of Theorem 3.1 proceeds in a sequence of lemmas. First, we need to show that the distribution of \(Z_{t}(X_{t})\) is log-concave for all \(t\in[T]\), and after we follow the analysis of Algorithm 1 of Ito et al. (2020), bounding both components of (19) taking into account the required alterations to incorporate contextual structure

**Lemma C.1**.: _Suppose \(z(q,x)=\sum_{a}q_{a}\varphi(x,a)\) for \(\varphi(x,a)=(0\intercal,\ldots,x\intercal,\cdots)\) such that \(x\) is on the \(da\)'th co-ordinate and \(Q(x)\sim p(\cdot|x)\) for \(p(\cdot|x)\) log-concave. If \(X\sim p_{X}(\cdot)\) and \(p_{X}(\cdot)\) is log-concave and \(Z(x)=z(Q(x),x)\), then \(Z(X)\) also follows a log-concave distribution._

Proof.: Assume that \(|x^{i}|>0\) for all \(i\in[d]\). Set \((z_{1},\ldots,z_{K-1},x)=h(q_{1}\bar{1},\ldots,q_{K-1}\bar{1},x)\), where \(h:\mathbb{R}^{dK}\rightarrow\mathbb{R}^{dK}\) and \(z_{i}=(\ldots,z_{i}^{j},\ldots)^{\intercal}\) for each \(i\in\{1,\ldots,K-1\}\). Thus \(z_{i}^{j}=h_{i}(q_{i},x^{j})=q_{i}(x^{j})\) and \(h_{K}(x)=x\). The Jacobian of \(h^{-1}(z_{1},\ldots,z_{K-1},x)\) can be expressed as the block matrix

\[J(h^{-1}(z_{1},\ldots,z_{K-1},x))=\begin{bmatrix}\Lambda_{x}&\Gamma_{z,x}\\ 0_{d\times(K-1)}&\mathrm{Id}_{d\times d}\end{bmatrix},\]

where \(\Lambda_{x}\in\mathbb{R}^{(K-1)\times(K-1)}\) is diagonal with \((\Lambda_{x})_{ii}=\frac{1}{x^{i}}\) and \(\Gamma_{z,x}\in\mathbb{R}^{(K-1)\times d}\) with \((\Gamma_{z,x})_{ij}=-\frac{z^{j}}{(x^{j})^{2}}\). Since \(J(h^{-1}(z_{1},\ldots,z_{K-1},x))\) is upper-triangular, \(\det(J(h^{-1}(z_{1},\ldots,z_{K-1},x)))=\left(\prod_{i=1}^{d}\frac{1}{x^{i}} \right)^{K-1}\). The joint distribution of \(Z_{i}\) and \(X\) can thus be written

\[p_{Z_{1},\ldots,Z_{K-1},X}(z_{1},\ldots,z_{K-1},x)=p_{Q,X}\left(h^{-1}(z_{1}, \ldots,z_{K-1},x)\right)\left(\prod_{i=1}^{d}\frac{1}{x^{i}}\right)^{K-1}\]

with the joint distribution between \(Q\) and \(X\) of the form

\[p_{Q,X}\left(h^{-1}(z_{1},\ldots,z_{K-1},x)\right)=\frac{e^{-\eta\langle\psi(z,x,\varphi),\widehat{\Theta}_{t-1}\rangle}}{\int_{q^{\prime}\in C}e^{-\eta \langle\sum_{a=1}^{K-1}q^{\prime}_{a}\varphi(x,a),\widehat{\Theta}_{t-1} \rangle}dq^{\prime}}p_{X}(x)\]

where \((\psi(z,x,\varphi))_{i}=\sum_{a=1}^{K-1}\frac{z_{i}^{i}}{x^{i}}\varphi(x,a)^{i }+\left(1-\sum_{a=1}^{K-1}\frac{z_{i}^{i}}{x^{i}}\right)\varphi(x,K)^{i}\) has been defined for readability. We can reabsorb the factor \(\left(\prod_{i=1}^{d}\frac{1}{x^{i}}\right)^{K-1}\) in the denominator to rewrite the normalization constant as a in terms of the random variable \(Z(x)\), and so

\[p_{Z_{1},\ldots,Z_{K-1},X}(z_{1},\ldots,z_{K-1},x)=\frac{e^{-\eta\langle\psi(z,x,\varphi),\widehat{\Theta}_{t-1}\rangle}}{\int_{z^{\prime}\in Z(x)}e^{- \eta\langle\psi(z^{\prime},x,\varphi),\widehat{\Theta}_{t-1}\rangle}dz^{ \prime}}p_{X}(x).\]

Define a new function \(g:\mathbb{R}^{d\times K}\rightarrow\mathbb{R}^{d\times K}\) such that \(y=g(z_{1},\ldots,z_{K-1},x)=(\ldots,g_{i}(z_{1},\ldots,z_{K-1},x),\ldots)^{\intercal}\), where for \(i\in[1,K-1]\), \(g_{i}(z_{1},\ldots,z_{K-1},x)=z_{i}\) and \(g_{K}(z_{1},\dots,z_{K-1},x)=(\dots,\left(1-\frac{1}{x^{i}}\sum_{a=1}^{K-1}z_{a}^{i }\right)x^{i},\dots)\). Then for \(i\in\{1,\dots,K-1\}\), \(g_{i}^{-1}(y)=y_{i}\) and \(g_{K}^{-1}(y)=\sum_{a=1}^{K}y_{a}\). The determinant \(\det(J(g^{-1}(y)))=1\), so

\[p_{Y}(y) =p_{Z_{1},\cdot,Z_{K-1},X}(g^{-1}(y))\] \[=\frac{e^{-\eta\left\langle y,\widehat{\Theta}_{t-1}\right\rangle }}{\int_{y^{\prime}\in Y(y)}e^{-\eta\left\langle y^{\prime},\widehat{\Theta}_ {t-1}\right\rangle}dy^{\prime}}p_{X}\left(\sum_{a=1}^{K}y_{a}\right).\]

Since both \(p_{X}\) and \(\frac{e^{-\eta\left\langle y,\widehat{\Theta}_{t-1}\right\rangle}}{\int_{y^{ \prime}\in Y(y)}e^{-\eta\left\langle y^{\prime},\widehat{\Theta}_{t-1}\right \rangle}dy^{\prime}}\) are both log-concave, the lemma follows. 

Having shown the log-concavity of \(Z(X_{t})\), we may safely proceed.

We state the analog of Lemma 4 in Ito et al. (2020) adapted to our setting, leading to a bound on the first term of (19) as well as providing a useful relation between \(\Sigma_{t}\) and \(\widetilde{\Sigma}_{t}\).

**Lemma C.2**.: \[\left|\mathbb{E}_{t}\left[\left\langle Z_{t}(X_{t})-\widetilde{Z}_{t}(X_{t}), \theta_{t}\right\rangle\right]\right|\leq\frac{1}{2T^{2}},\] (28)

_and we have_

\[\frac{3}{4}\Sigma_{t}\preceq\widetilde{\Sigma}_{t}\preceq\frac{4}{3}\Sigma_{t}.\] (29)

Proof.: From definition of \(\tilde{p}_{t}\), for any \(x\in\mathcal{X},\theta\in\Theta\), we have

\[\mathbb{E}_{t}\left[\left\langle\widetilde{Z}_{t}(X_{t}),\theta \right\rangle\right]\] \[\quad=\frac{1}{\mathbb{P}_{t}\left[\|Z_{t}(X_{t})\|^{2}_{\Sigma_{ t}^{-1}}\leq dK\gamma^{2}\right]}\int_{\Delta^{K}}\int_{\mathcal{X}}\left\langle z (q,x),\theta\right\rangle\mathds{1}\left\{\|z(q,x)\|^{2}_{\Sigma_{t}^{-1}} \leq dK\gamma^{2}\right\}p_{t}(q|x)p(x)dxdq\] \[\quad=\frac{1}{1-\delta}\int_{\Delta^{K}}\int_{\mathcal{X}} \left\langle z(q,x),\theta\right\rangle\mathds{1}\left\{\|z(q,x)\|^{2}_{\Sigma _{t}^{-1}}\leq dK\gamma^{2}\right\}p_{t}(q|x)p(x)dxdq\] \[\quad=\frac{1}{1-\delta}\left(\mathbb{E}_{t}\left[\left\langle Z _{t}(X_{t}),\theta\right\rangle\right]-\int_{\Delta^{K}}\int_{\mathcal{X}} \left\langle z(q,x),\theta\right\rangle\mathds{1}\left\{\|z(q,x)\|^{2}_{\Sigma _{t}^{-1}}>dK\gamma^{2}\right\}p_{t}(q|x)p(x)dxdq\right),\]

where \(\delta=\mathbb{P}_{t}\left[\|Z_{t}(X_{t})\|^{2}_{\Sigma_{t}^{-1}}>dK\gamma^{2}\right]\). Plugging this into the l.h.s. of (28) yields

\[\left|\mathbb{E}_{t}\left[\left\langle Z_{t}(X_{t})-\widetilde{Z}_ {t}(X_{t}),\theta_{t}\right\rangle\right]\right|\] \[\quad=\frac{1}{1-\delta}\bigg{|}\delta\mathbb{E}_{t}\left[\left\langle Z _{t}(X_{t}),\theta_{t}\right\rangle\right]+\int_{\Delta^{K}}\int_{\mathcal{X}} \left\langle z(q,x),\theta\right\rangle\mathds{1}\left\{\|z(q,x)\|^{2}_{\Sigma _{t}^{-1}}>dK\gamma^{2}\right\}p_{t}(q|x)p(x)dxdq\bigg{|}\] \[\quad\leq\frac{1}{1-\delta}\left(\delta+\int_{\Delta^{K}}\int_{ \mathcal{X}}\mathds{1}\left\{\|z(q,x)\|^{2}_{\Sigma_{t}^{-1}}>dK\gamma^{2} \right\}p_{t}(q|x)p(x)dxdq\right)=\frac{2\delta}{1-\delta}.\]

Since the distribution of \(Z_{t}(X_{t})\) is log-concave (Lemma C.1), we can apply Lemma 1 of Ito et al. (2020) to \(x=\Sigma_{t}^{-1/2}Z_{t}(X_{t})\). The assumptions of Lemma 1 of Ito et al. (2020) hold since we have \(\mathbb{E}\left[xx^{\intercal}\right]=I\) and since log-concavity is preserved under linear maps. Using Lemma 1 of Ito et al. (2020), we have

\[\delta=\mathbb{P}_{t}\left[\left\|Z_{t}(X_{t})\right\|^{2}_{\Sigma_{t}^{-1}}>dK \gamma^{2}\right]\leq dK\exp(1-\gamma)\leq 3dK\exp(-\gamma)\leq\frac{1}{6T^{2}},\]

where the last inequality follows from \(\gamma\geq 4\log(10dKT)\), which obtains (28). We proceed to showing (29). For any \(y\in\mathbb{R}^{dK}\), we have

\[y^{\intercal}\widetilde{\Sigma}_{t}y =\mathbb{E}\left[(y^{\intercal}\widetilde{Z}_{t}(X_{t}))^{2} \right]=\frac{1}{1-\delta}\mathbb{E}_{t}\left[(y^{\intercal}Z_{t}(X_{t}))^{2} \mathds{1}\left\{\|Z_{t}(X_{t})\|^{2}_{\Sigma_{t}^{-1}}\leq dK\gamma^{2} \right\}\right]\] \[\leq\frac{1}{1-\delta}\mathbb{E}_{t}\left[(y^{\intercal}Z_{t}(X_{ t}))^{2}\right]=\frac{1}{1-\delta}y^{\intercal}\Sigma_{t}y.\]Since this holds for all \(y\in\mathbb{R}^{dK}\) and \(\frac{1}{1-\delta}\leq\frac{4}{3}\), the second inequality in (29) holds. Furthermore, we have

\[y^{\gamma}\Sigma_{t}y-y^{\gamma}\widetilde{\Sigma}_{t}y =\mathbb{E}_{t}\left[(y^{\gamma}Z_{t}(X_{t}))^{2}\right]-\frac{1} {1-\delta}\mathbb{E}_{t}\left[(y^{\gamma}Z_{t}(X_{t}))^{2}\mathds{1}\left\{ \left\|Z_{t}(X_{t})\right\|_{\Sigma_{t}^{-1}}^{2}\leq dK\gamma^{2}\right\}\right]\] \[\leq\mathbb{E}_{t}\left[(y^{\gamma}Z_{t}(X_{t}))^{2}\mathds{1} \left\{\left\|Z_{t}(X_{t})\right\|_{\Sigma_{t}^{-1}}^{2}>dK\gamma^{2}\right\}\right]\] \[\leq y^{\gamma}\Sigma_{t}y\mathbb{E}_{t}\left[\left\|Z_{t}(X_{t} )\right\|_{\Sigma_{t}^{-1}}^{2}\mathds{1}\left\{\left\|Z_{t}(X_{t})\right\|_{ \Sigma_{t}^{-1}}^{2}>dK\gamma^{2}\right\}\right],\] (30)

where the last inequality follows from Cauchy-Schwarz:

\[(y^{\gamma}Z_{t}(X_{t}))^{2}=\left(\left\langle\Sigma_{t}^{1/2}y,\Sigma_{t}^{ -1/2}x\right\rangle\right)^{2}\leq\left\|\Sigma_{t}^{1/2}y\right\|_{2}^{2}, \left\|\Sigma_{t}^{-1/2}x\right\|_{2}^{2}=y^{\gamma}\Sigma_{t}y\left\|x\right\| _{\Sigma_{t}^{-1}}^{2}.\]

The right-hand side of (30) can be bounded using Lemma B.3 as follows:

\[\mathbb{E}_{t} \left[\left\|Z_{t}(X_{t})\right\|_{\Sigma_{t}^{-1}}^{2}\mathds{1} \left\{\left\|Z_{t}(X_{t})\right\|_{\Sigma_{t}^{-1}}^{2}>dK\gamma^{2}\right\}\right]\] \[\leq\sum_{n=1}^{\infty}(n+1)^{2}dK\gamma^{2}\mathbb{P}_{t}\left[n ^{2}dK\gamma^{2}\leq\left\|Z_{t}(X_{t})\right\|_{\Sigma_{t}^{-1}}^{2}\leq(n+1) ^{2}dK\gamma^{2}\right]\] \[\leq\sum_{n=1}^{\infty}(n+1)^{2}(dK)^{2}\gamma^{2}\exp(1-n\gamma)\] \[\leq(dK)^{2}\gamma^{2}\sum_{n=1}^{\infty}\exp(2+n-n\gamma)=(dK)^{ 2}\gamma^{2}\frac{\exp(3-\gamma)}{1-\exp(1-\gamma)}\leq\frac{1}{4}.\] (31)

Combining (31) and (30) we get the first inequality of (29). 

**Lemma C.3**.: _Let \(\pi^{*}\) be any fixed stochastic policy and let \(X_{0}\sim\mathcal{D}\) be a sample from the context distribution independent from \(\mathcal{F}_{T}\). Suppose that \(p_{t}\in\mathcal{F}_{t-1}\), such that \(p_{t}(\cdot|x)\) is a probability density with respect to Lebesgue measure with support \(\Delta^{K}\) and let \(Q_{t}(x)\sim p_{t}(\cdot|x)\). Then,_

\[\mathbb{E}\left[\sum_{t=1}^{T}\left\langle z(Q_{t}(X_{t}),X_{t})-z(\pi^{*}(X_{ t}),X_{t}),\theta_{t}\right\rangle\right]=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle z (Q_{t}(X_{0}),X_{0})-z(\pi^{*}(X_{0})),X_{0},\widehat{\theta}_{t}\right\rangle \right].\]

Proof.: For any \(t\), we have

\[\mathbb{E}_{t}\left[\left\langle Z_{t}(X_{0})-Z^{*}(X_{0}), \widehat{\theta}_{t}\right\rangle\right]=\mathbb{E}_{t}\left[\mathbb{E}_{t} \left[\left\langle Z_{t}(X_{0})-Z^{*}(X_{0}),\widehat{\theta}_{t}\right\rangle \right|X_{0}\right]\right]\] \[\quad=\mathbb{E}_{t}\left[\mathbb{E}_{t}\left[\left\langle Z_{t}(X _{0})-Z^{*}(X_{0}),\theta_{t}\right\rangle\right|X_{0}\right]]=\mathbb{E}_{t} \left[\left\langle Z_{t}(X_{t})-Z^{*}(X_{t}),\theta_{t}\right\rangle\right].\]

Then, we prove the almost sure regret bound for any \(x\) and then take an expectation over \(X_{0}\). We further proceed with an adaptation of the analysis of the continuous exponential weights algorithm, which was stated in Ito et al. (2020) as Lemma 16, but we include it here for the clarity. Let \(\psi(y)=\exp(y)-y-1\). For any \(x\in\mathcal{X}\), we show the following :

**Lemma C.4**.: _Assume that \(\eta_{t+1}\leq\eta_{t}\) for all \(t\), let \(q_{0}\) be a uniform distribution over \([K]\) and \(\psi(y)=\exp(y)-y-1\). Then, the regret \(\widehat{R}_{T}(x)\) for any \(x\in\mathcal{X}\) of ContextEW almost surely satisfies_

\[\widehat{R}_{T}(x)\leq\frac{1}{T}\sum_{t=1}^{T}\left\langle z(q_{0}-\pi^{*}(x), x),\,\widehat{\theta}_{t}\right\rangle+\frac{K\log T}{\eta_{T}}+\sum_{t=1}^{T} \frac{1}{\eta_{t}}\mathbb{E}_{Q_{t}(x)\sim p_{t}(\cdot|x)}\left[\psi\left(-\eta _{t}\left\langle z(Q_{t}(x),x),\widehat{\theta}_{t}-m_{t}\right\rangle\right) \right].\]

Proof.: Note that we can write \(\widehat{R}_{T}(x)\) as

\[\widehat{R}_{T}(x)=\sum_{t=1}^{T}\left(\int_{\Delta^{K}}p_{t}(q|x)\left\langle z (q,x),\widehat{\theta}_{t}\right\rangle dq-\left\langle z(\pi^{*}(x),x),\sum_{t =1}^{T}\widehat{\theta}_{t}\right\rangle\right).\]Define \(W_{t}(x)=\int_{\Delta^{K}}w_{t}(q|x)dq\), \(u_{t}(q|x)=\exp\left(-\eta_{t}\sum_{a}q_{a}\left\langle x,\sum_{s=1}^{t}\widehat{ \theta}_{s,a}\right\rangle\right)\), \(U_{t}(x)=\int_{\Delta^{K}}u_{t}(q|x)dq\) and \(v_{t}(q|x)=\exp\left(-\eta_{t+1}\sum_{a}q_{a}\left\langle x,\sum_{s=1}^{t} \widehat{\theta}_{s,a}\right\rangle\right)\), \(V_{t}(x)=\int_{\Delta^{K}}v_{t}(q|x)dq\). We have

\[U_{t}(x) =\int_{\Delta^{K}}w_{t}(q|x)\exp\left(-\eta_{t}\left\langle z(q,x),\widehat{\theta}_{t}-m_{t}\right\rangle\right)dq=W_{t}(x)\int_{\Delta^{K}}p_{ t}(q|x)\exp\left(-\eta_{t}\left\langle z(q,x),\widehat{\theta}_{t}-m_{t} \right\rangle\right)dq\] \[=W_{t}(x)\int_{\Delta^{K}}p_{t}(q|x)\left(1-\eta_{t}\left\langle z (q,x),\widehat{\theta}_{t}-m_{t}\right\rangle+\psi(-\eta_{t}\left\langle z(q, x),\widehat{\theta}_{t}-m_{t}\right\rangle)\right)dq.\]

Taking the logarithm of both sides, we get

\[\log(U_{t}(x)) =\log(W_{t}(x))+\log\left(\int_{\Delta^{K}}p_{t}(q|x)\left(1- \eta_{t}\left\langle z(q,x),\widehat{\theta}_{t}-m_{t}\right\rangle+\psi(-\eta _{t}\left\langle z(q,x),\widehat{\theta}_{t}-m_{t}\right\rangle)\right)dq\right)\] (32) \[\leq\log(W_{t}(x))+\int_{\Delta^{K}}p_{t}(q|x)\left(-\eta_{t} \left\langle z(q,x),\widehat{\theta}_{t}-m_{t}\right\rangle+\psi(-\eta_{t} \left\langle z(q,x),\widehat{\theta}_{t}-m_{t}\right\rangle)\right)dq,\]

where we used the inequality \(\log(1+x)\leq x\) for \(x>-1\).

\[V_{t-1}(x) =\int_{\Delta^{K}}w_{t}(q|x)\exp\left(\eta_{t}\sum_{a}q_{a} \left\langle x,m_{t,a}\right\rangle\right)dq=W_{t}(x)\int_{\Delta^{K}}p_{t}(q| x)\exp\left(\eta_{t}\sum_{a}q_{a}\left\langle x,m_{t,a}\right\rangle\right)dq\] \[\geq W_{t}(x)\exp\left(\eta_{t}\int_{\Delta^{K}}p_{t}(q|x)\sum_{a} q_{a}\left\langle x,m_{t,a}\right\rangle dq\right),\] (33)

using Jensen's inequality. It holds that

\[\int_{\Delta^{K}}p_{t}(q|x)\sum_{a}q_{a}\left\langle x,m_{t,a}\right\rangle dq \leq\frac{1}{\eta_{t}}\log\frac{V_{t-1}(x)}{W_{t}(x)}.\]

Then, we get

\[\sum_{t=1}^{T}\int_{\Delta^{K}}p_{t}(q|x)\left\langle z(q,x),\widehat{\theta} _{t}\right\rangle dq\leq\sum_{t=1}^{T}\frac{1}{\eta_{t}}\left(\log\frac{V_{t- 1}(x)}{U_{t}(x)}+\int_{\Delta^{K}}p_{t}(q|x)\psi(-\eta_{t}\left\langle z(q,x), \widehat{\theta}_{t}-m_{t}\right\rangle)dq\right).\]

Noting that \(V_{0}=U_{0}\), we have

\[\sum_{t=1}^{T}\frac{1}{\eta_{t}}\log\frac{V_{t-1}(x)}{U_{t}(x)} =\sum_{t=1}^{T}\frac{1}{\eta_{t}}\left(\log\frac{V_{t-1}(x)}{V_{0} }-\log\frac{U_{t}(x)}{U_{0}}\right)\] \[=\sum_{t=1}^{T-1}\left(\frac{1}{\eta_{t+1}}\log\frac{V_{t}(x)}{V_ {0}}-\frac{1}{\eta_{t}}\log\frac{U_{t}(x)}{U_{0}}\right)-\frac{1}{\eta_{T}}\log \frac{U_{T}(x)}{U_{0}}\]

To bound the first term, we use that \(\eta_{t+1}\leq\eta_{t}\) and an additional application of Jensen's inequality:

\[\frac{1}{\eta_{t+1}}\log\frac{V_{t}(x)}{V_{0}} =\frac{1}{\eta_{t+1}}\log\mathbb{E}\left[\exp\left(-\eta_{t+1} \langle\sum_{s=1}^{t}\widehat{\theta}_{s},z(Q_{t},x)\rangle\right)\right]\] \[=\frac{1}{\eta_{t+1}}\log\mathbb{E}\left[\exp\left(-\eta_{t} \langle\sum_{s=1}^{t}\widehat{\theta}_{s},z(Q_{t},x)\rangle\right)^{\frac{\eta_ {t+1}}{\eta_{t}}}\right]\] \[\leq\frac{1}{\eta_{t}}\log\mathbb{E}\left[\exp\left(-\eta_{t} \langle\sum_{s=1}^{t}\widehat{\theta}_{s},z(Q_{t},x)\rangle\right)\right]=\frac{ 1}{\eta_{t}}\log\frac{U_{t}(x)}{U_{0}},\]

Set \(Q_{\pi^{*}(x)}:=\{(1-\frac{1}{T})\pi^{*}(x)+\frac{1}{T}q|q\in\Delta^{K}\}\), and denote \(q_{0}\) as the uniform distribution over \(K\) arms. We then have

\[U_{T}(x)\geq\int_{Q_{\pi^{*}(x)}}\exp\left(-\eta_{T}\left\langle z(q,x),\sum_{ t=1}^{T}\widehat{\theta}_{t}\right\rangle\right)dq\]\[=T^{-K}\int_{\Delta^{K}}\exp\left(-\eta_{T}\left\langle z((1-\frac{1}{T} )\pi^{*}(x)+\frac{1}{T}q,x),\sum_{t=1}^{T}\widehat{\theta}_{t}\right\rangle \right)dq\] \[\geq T^{-K}U_{0}(x)\exp\left(-\eta_{T}\left\langle z((1-\frac{1}{T })\pi^{*}(x)+\frac{1}{T}q_{0},x),\sum_{t=1}^{T}\widehat{\theta}_{t}\right\rangle \right),\]

where the first inequality constitutes a change of variables and the second follows from Jensen's bound. After rearranging and taking the logarithm, we get

\[-\frac{1}{\eta_{T}}\log\frac{U_{T}(x)}{U_{0}(x)} \leq\sum_{t=1}^{T}\left\langle z((1-\frac{1}{T})\pi^{*}(x)+\frac {1}{T}q_{0},x),\widehat{\theta}_{t}\right\rangle+\frac{K\log T}{\eta_{T}}\] \[=\sum_{t=1}^{T}\left\langle z(\pi^{*}(x),x),\sum_{t=1}^{T} \widehat{\theta}_{t}\right\rangle+\frac{1}{T}\sum_{t=1}^{T}\left\langle z(q _{0}-\pi^{*}(x),x),\;\widehat{\theta}_{t}\right\rangle+\frac{K\log T}{\eta_{ T}}.\]

Combining everything together, we get

\[\leq\sum_{t=1}^{T}\frac{1}{\eta_{t}}\int_{\Delta^{K}}p_{t}(q|x) \psi(-\eta_{t}\left\langle z(q,x),\widehat{\theta}_{t}-m_{t}\right\rangle)dq\] \[+\frac{1}{T}\sum_{t=1}^{T}\left\langle z(q_{0}-\pi^{*}(x),x),\; \widehat{\theta}_{t}\right\rangle+\frac{K\log T}{\eta_{T}}.\]

From Lemma 4.3 and Lemma 4.4, we get a bound on the second term of (19):

\[\mathbb{E}\left[\sum_{t=1}^{T}\left\langle Z_{t}(X_{t})-Z^{*}(X_ {t}),\theta_{t}\right\rangle\right] =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle Z_{t}(X_{0})-Z^{*}(X_ {0}),\widehat{\theta}_{t}\right\rangle\right]\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\frac{1}{\eta_{t}}\psi\left(- \eta_{t}\left\langle Z_{t}(X_{0}),\widehat{\theta}_{t}-m_{t}\right\rangle \right)+\frac{1}{T}\sum_{t=1}^{T}\left\langle z(q_{0}-\pi^{*}(X_{0}),X_{0}),\; \widehat{\theta}_{t}\right\rangle+\frac{K\log T}{\eta_{T}}\right]\] (34)

We first find a bound on the first term using Lemma B.4. To satisfy the assumptions of Lemma B.4, we need to show that \(\mathbb{E}_{t}\left[\left(-\eta_{t}\left\langle Z_{t}(X_{0}),\widehat{\theta }_{t}-m_{t}\right\rangle\right)^{2}\right]\leq\frac{1}{100}\):

\[\mathbb{E}_{t}\left[\left(-\eta_{t}\left\langle Z_{t}(X_{0}), \widehat{\theta}_{t}-m_{t}\right\rangle\right)^{2}\right] =\mathbb{E}_{t}\left[\eta_{t}^{2}\left(\ell_{t}(A_{t},X_{t})-X_{t }^{\intercal}m_{t,A_{t}}\right)^{2}\text{tr}\left(\widetilde{\Sigma}_{t}^{-1}Z _{t}(X_{0})Z_{t}(X_{0})^{\intercal}\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t }(X_{t})^{\intercal}\right)\right]\] \[=\eta_{t}^{2}\mathbb{E}_{t}\left[\left(\ell_{t}(A_{t},X_{t})-X_{t }^{\intercal}m_{t,A_{t}}\right)^{2}\text{tr}\left(\widetilde{\Sigma}_{t}^{-1} \Sigma_{t}\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{\intercal} \right)\right]\] \[\leq\eta_{t}^{2}\frac{4}{3}\mathbb{E}_{t}\left[\left(\ell_{t}(A_{ t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}\text{tr}\left(\widetilde{\Sigma}_{t}^{-1} \widetilde{\Sigma}_{t}\widetilde{\Sigma}_{t}^{-1}Z_{t}(X_{t})Z_{t}(X_{t})^{ \intercal}\right)\right]\] \[=\eta_{t}^{2}\frac{4}{3}\mathbb{E}_{t}\left[\left(\ell_{t}(A_{t},X _{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}Z_{t}(X_{t})^{\intercal}\widetilde {\Sigma}_{t}^{-1}Z_{t}(X_{t})\right]\] \[\leq dK\eta_{t}^{2}\gamma^{2}\mathbb{E}_{t}\left[\left(\ell_{t}(A _{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}\right]\] (35) \[\leq\frac{1}{100},\] (36)

where the first inequality follows from \(\ell_{t}\leq 1\) and (29), the second is immediate from (29) and the fact that for symmetric positive definite matrices \(A\succeq B\) follows from \(B^{-1}\succeq A^{-1}\). The third inequality follows from the truncation in the algorithm and the last is immediate from plugging in the definition of \(\eta_{t}\). So, by applying Lemma B.4 and (35), we get:

\[\frac{1}{\eta}\mathbb{E}\left[\psi\left(-\eta\left\langle Z_{t}(X_{0}),\widehat{ \theta}_{t}-m_{t}\right\rangle\right)\right]\leq\frac{2}{\eta}\mathbb{E}\left[ \left(-\eta\left\langle Z_{t}(X_{0}),\widehat{\theta}_{t}-m_{t}\right\rangle \right)^{2}\right]\leq 2dK\eta\gamma^{2}\mathbb{E}_{t}\left[\left(\ell_{t}(A_{t},X_{t} )-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}\right].\] (37)

For the second term of (34), we simply get from \(-1\leq\ell_{t}\leq 1\) and \(\widehat{\theta}_{t}\) is unbiased:

\[\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}\left\langle z(q_{0}-\pi^{*}(X_{0}),X _{0}),\;\widehat{\theta}_{t}\right\rangle\right]=\mathbb{E}\left[\frac{1}{T} \sum_{t=1}^{T}\left\langle z(q_{0}-\pi^{*}(X_{0}),X_{0}),\;\theta_{t}\right\rangle \right]\leq 2.\] (38)

The expression that we use for the learning rate is the following:

\[\eta_{t}=(100dK\gamma^{2}+d(\widehat{V}_{t-1}+1+G_{t})))^{-1/2},\]

where \(G_{t}=8\sqrt{2\widehat{V}_{t-1}\ln T+144\ln^{2}T}+176\ln T\). We show that with probability at least \(1-\delta\) the following holds for all \(t\in[T]\):

\[V_{t}\leq\widehat{V}_{t}+8\sqrt{\widehat{V}_{t}\ln(2T/\delta)+72\ln(2T/\delta) ^{2}}+88\ln(T/\delta)\] (39)

Let \(Y_{s}=\mathbb{E}_{s}\left[\left(\ell_{t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A _{t}}\right)^{2}\right]-\left(\ell_{t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t }}\right)^{2}\). Then, \(Y_{s}\leq 4\) almost surely, since \(\ell_{t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\leq 2\). Similarly we bound the second moment of \(Y_{s}\), using Jensen's inequality:

\[\mathbb{E}_{s}\left[Y_{s}^{2}\right] =\mathbb{E}_{s}\left[\left(\mathbb{E}_{s}\left[(\ell_{t}(A_{t},X_ {t})-X_{t}^{\intercal}m_{t,A_{t}})^{2}\right]-(\ell_{t}(A_{t},X_{t})-X_{t}^{ \intercal}m_{t,A_{t}})^{2}\right)^{2}\right]\] \[\leq 2\mathbb{E}_{s}\left[\left(\ell_{t}(A_{t},X_{t})-X_{t}^{ \intercal}m_{t,A_{t}}\right)^{2}\right]^{2}+2\mathbb{E}_{s}\left[\left(\ell_{ t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{4}\right]\] \[\leq 16\mathbb{E}_{s}\left[\left(\ell_{t}(A_{t},X_{t})-X_{t}^{ \intercal}m_{t,A_{t}}\right)^{2}\right].\]

By Lemma B.2, the following holds for some \(\delta^{\prime}\in(0,1)\):

\[V_{t}\leq\widehat{V}_{t}+8\max\biggl{\{}2\sqrt{V_{t}},\sqrt{\ln(1/\delta^{ \prime})}\biggr{\}}\sqrt{\ln(1/\delta^{\prime})}\] (40)

Note that this inequality i can be rearranged as

\[V_{t}\leq\widehat{V}_{t}+8\sqrt{\widehat{V}_{t}\ln(1/\delta^{\prime})+72\ln( 1/\delta^{\prime})^{2}}+88\ln(1/\delta^{\prime}).\]

Then, taking a union bound over \(t\in[T]\) and taking \(\delta=\delta^{\prime}/T\), we get that (39) holds for all \(t\in[T]\). Let \(\mathcal{E}_{T}\) be an event that for all \(t\in[1,T]\), (39) holds with \(\delta=1/T\). From (37), (38), and the choice of \(\eta_{t}\), we get:

\[R_{T} \leq\mathbb{E}\left[2dK\gamma^{2}\sum_{t=1}^{T}\eta_{t}\left(\ell _{t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}+2+\frac{K\log T}{ \eta_{T}}\right]\] (41) \[=2dK\gamma^{2}\mathbb{E}\left[\sum_{t=1}^{T}\eta_{t}\left(\ell_{ t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}\mathds{1}\left\{ \mathcal{E}_{T}\right\}\right]\] \[+2dK\gamma^{2}\mathbb{E}\left[\sum_{t=1}^{T}\eta_{t}\left(\ell_{ t}(A_{t},X_{t})-X_{t}^{\intercal}m_{t,A_{t}}\right)^{2}\mathds{1}\left\{ \mathcal{E}_{T}\right\}\right]+2+\mathbb{E}\left[\frac{K\log T}{\eta_{T}}\right]\] \[\leq 2\sqrt{d}K\gamma^{2}\sum_{t=1}^{T}\frac{V_{t}-V_{t-1}}{\sqrt{V_ {t}}}+\frac{1}{T}2\sqrt{d}K\gamma^{2}T+2+\frac{K\log T}{\eta_{T}^{\prime}}\] \[=2\sqrt{d}K\gamma^{2}\sum_{t=1}^{T}\frac{(\sqrt{V_{t}}-\sqrt{V_{t-1 }})(\sqrt{V_{t}}+\sqrt{V_{t-1}})}{\sqrt{V_{t}}}+2\sqrt{d}K\gamma^{2}+2+\frac{K \log T}{\eta_{T}^{\prime}}\]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]