# Post-Calibration Techniques: Balancing Calibration and Score Distribution Alignment

Agathe Fernandes Machado

Universite du Quebec a Montreal

201 Av. du President-Kennedy, Montreal, QC H2X 3Y7, Canada

fernandes_machado.agathe@courrier.uqam.ca

Arthur Charpentier

Universite du Quebec a Montreal

201 Av. du President-Kennedy, Montreal, QC H2X 3Y7, Canada

charpentier.arthur@uqam.ca

Emmanuel Flachaire

Aix-Marseille School of Economics, Aix-Marseille Univ.

5 Bd Maurice Bourdet CS 50498, 13205 Marseille Cedex 01, France

Emmanuel.flachaire@univ-amu.fr

Ewen Gallic

Aix-Marseille School of Economics, Aix-Marseille Univ.

5 Bd Maurice Bourdet CS 50498, 13205 Marseille Cedex 01, France

ewen.gallic@gmail.com

Francois Hu

Milliman France

14 Av. de la Grande Armee, 75017 Paris, France

hu.faugon@gmail.com

###### Abstract

A binary scoring classifier can appear well-calibrated according to standard calibration metrics, even when the distribution of scores does not align with the distribution of the true events. In this paper, we investigate the impact of post-processing calibration on the score distribution (sometimes named "recalibration"). Using simulated data, where the true probability is known, followed by real-world datasets with prior knowledge on event distributions, we compare the performance of an XGBoost model before and after applying calibration techniques. The results show that while applying methods such as Platt scaling, Beta calibration, or isotonic regression can improve the model's calibration, they may also lead to an increase in the divergence between the score distribution and the underlying event probability distribution.

## 1 Introduction

When estimating a probabilistic scoring classifier, the model must not only discriminate between observations according to their class but also return scores that can be interpreted as probabilities. Thedistribution of scores produced by the classifier should align with the underlying event distribution. To assess whether classifiers return probabilistic scores, one must evaluate the model's calibration [4; 24; 8]. While some models, such as logistic regression when correctly specified, are known to be well-calibrated [22], others, including ensemble methods like Random Forests (RF) [13; 3] and XGBoost [10], are not inherently calibrated [28]. To assess a model's ability to provide probabilistic scores, the literature recommends evaluating its calibration using metrics like the Brier Score (BS, [4]) or the Integrated Calibration Index (ICI, [1]). When a model is not well-calibrated, post-processing calibration methods including Platt scaling [29], Beta calibration [17], or isotonic regression [37] are often applied to adjust the scores [21; 11; 17]. After applying calibration techniques, these metrics generally indicate an improvement in the model's calibration relative to its initial state.

Since the true underlying probability distribution of the data is typically unobserved in practice, calibration metrics are assessed solely on the classifier's output range. Fernandes Machado et al. [9] demonstrated with simulated data that methods such as RF and XGBoost, can appear well-calibrated according to standard calibration metrics and exhibit strong discrimination based on performance metrics, yet still fail to align the score distribution with the true event distribution. This discrepancy can arise when predicted scores from those algorithms lack the heterogeneity present in the underlying data distribution. They demonstrate this misalignment by comparing the selection of model hyperparameters based on Kullback-Leibler (KL) divergence with the selection based on performance or calibration metrics, knowing the true event distribution in the case of simulated data. For real data, where the true distribution is unknown, the approach involves prior information about the underlying data distribution to better align it with the predicted score distribution. Their analysis only considers model evaluation to accurately interpret predicted scores as probabilities, typically through calibration metrics. However, many practitioners employ post-calibration techniques to ensure that output scores represent probabilistic estimates. In this paper, we examine how post-calibration techniques affect the variability of score distribution in XGBoost binary classifiers, comparing it to the true underlying data distribution using KL divergence. We find that these post-processing methods often reduce score heterogeneity. Additionally, the misalignment between tree-based models optimized for KL divergence and those optimized for calibration or performance metrics persists and may even worsen after calibration, indicating that score alignment can decrease following post-calibration. Using simulated data with known true probabilities, followed by real-world datasets with prior knowledge of event distributions, we evaluate the abilities of XGBoost-predicted scores as probabilistic estimates before and after applying calibration techniques, with an emphasis on score distribution rather than solely on their calibration.

## 2 Calibration

We focus on the context of a binary scoring classifier. Let \(Y\in\mathcal{Y}=\{0,1\}\) be a binary response variable, and let \(\bm{X}\in\mathcal{X}=\mathbb{R}^{d}\) denote features. The goal is to predict \(s(\bm{X})=\mathbb{P}(Y=1|\bm{X})\), using a sample of \(n\) i.i.d. observations \((\mathbf{x}_{i},y_{i})_{i=1}^{n}\). We estimate this probability \(\hat{s}(\mathbf{x}_{i})\in[0,1]\) using an XGBoost classifier, which produces a distribution of estimated scores \(\hat{s}(\bm{X})\). If the score distribution is poorly calibrated, these scores cannot be interpreted as the "true underlying probabilities" [33; 19; 16]. A model \(\hat{s}\) is well-calibrated for a binary variable \(Y\) when [31]:

\[\mathbb{P}(Y=1\mid\hat{s}(\bm{X}))=\mathbb{E}[Y\mid\hat{s}(\bm{X})]=\hat{s}( \bm{X})\quad\text{a.s.},\] (1)

i.e., equivalently, \(\mathbb{E}[Y\mid\hat{s}(\bm{X})=p]=p,\forall p\in[0,1]\).

### Calibration Metrics

To measure calibration, the literature suggests various metrics. Here, we focus on two of them: BS and ICI. The former [12; 17; 29; 30], often used to assess a model's calibration, is a proper scoring rule that also accounts for refinement loss [18]. It writes: \(\text{BS}=n^{-1}\sum_{i=1}^{n}\big{(}\hat{s}(\mathbf{x}_{i})-y_{i}\big{)}^{2}\)[4]. More recently, Austin and Steyerberg [1] introduced the ICI, a metric that relies on the calibration curve. In the binary case, the calibration curve writes \(\mathrm{g}:[0,1]\rightarrow[0,1],\quad p\mapsto\mathrm{g}(p):=\mathbb{E}[Y \mid\hat{s}(\mathbf{X})=p]\). For a well-calibrated model, the calibration curve corresponds to the identity function, \(\mathrm{g}(p)=p\), where the predicted score \(p\) equals the true likelihood of the event. Graphically, this is represented by the calibration curve aligning with the 45-degree diagonal. While the calibration curve is usually estimated using bins [35; 20; 27], the ICI relies on a smoother version, based on splines. The empirical version writes \(\text{ICI}=n^{-1}\sum_{i=1}^{n}\big{|}\hat{s}(\mathbf{x}_{i})-\hat{\mathrm{g}} \big{(}\hat{s}(\mathbf{x}_{i})\big{)}\big{|}\), which corresponds to computing the average of the absolute difference between the estimated calibration curve and the identity function, the latter representing perfect calibration.

### Calibration Methods

When using scores generated by a model estimating the probability of a binary event, the literature advocates calibrating the model by applying the calibration curve g--which serves as a transformation function--on the scores [29; 37; 17; 20]. In this paper, we focus on three calibration methods: Platt scaling, isotonic regression, and Beta calibration.

Platt ScalingThis parametric approach consists of fitting a logistic regression to the binary response variable using predicted scores of a binary classifier as the unique feature [29]. The obtained calibrated probabilities are \(\text{g}(\hat{s}(\mathbf{x}))=\big{(}1+\exp\big{\{}-\frac{1}{s}(\hat{s}( \mathbf{x})-\mu)\big{\}}\big{)}^{-1}\), where \(\mu\) and \(s\) (\(s>0\) for a non-decreasing calibration map \(\text{g}\)) are estimated on a calibration set. It should be noted that Platt scaling is unable to learn the identity function g if the predicted scores are already calibrated [17].

Beta CalibrationThe scores returned by a binary classifier are in range \([0,1]\). Beta calibration [17] builds on this feature and assumes that the score within each class of the target variable \(y\) are distributed according to a Beta distribution. By contrast, Platt scaling assumes the scores follow a Normal distribution within each class. The calibration map writes \(\text{g}(\hat{s}(\mathbf{x}))=(1+\exp\big{\{}-a\log\hat{s}(\mathbf{x})+b\log( 1-\hat{s}(\mathbf{x}))-c\big{\}})^{-1}\), where \(a\), \(b\), and \(c\) are the three parameters that need to be estimated on a calibration set. Unlike Platt scaling, Beta calibration can learn the identity function g (with \(a=b=1,c=0\)), making it suitable for already well-calibrated models. By restricting \(a,b>0\), the calibration map is monotone.

Isotonic RegressionThis solution arises from a constrained optimization problem [37], solved using the Pool-Adjacent-Violators Algorithm, ensuring that corrected predicted scores remain monotonic: \(\min_{\beta_{1},\ldots,\beta_{n}}\sum_{i=1}^{n}(y_{(i)}-\beta_{i})^{2}\), s.t. \(\beta_{1}\leq\ldots\leq\beta_{n}\), where \(y_{(i)}\) corresponds to the value in \(\{y_{1},\cdots,y_{n}\}\) associated with the \(i\)-th largest predicted score \(\{\hat{s}(\mathbf{x}_{1}),\cdots,\hat{s}(\mathbf{x}_{n})\}\). Isotonic regression will lead to \(\text{g}(\hat{s}(\mathbf{x}_{i}))=\beta_{i}^{*}\) where \(\beta_{i}^{*}\) solve the optimization problem.

## 3 Score Heterogeneity

To accurately interpret predicted scores from a binary classifier as probabilistic estimates, since the true underlying probability \(s(\bm{X})\) is usually unobservable, calibration metrics rely solely on the predicted score range. When a binary classification model is well-calibrated, the distribution of its scores \(\hat{s}(\bm{X})\), as defined by Eq. 1, should align with the actual probability of the event in the vicinity of score values. Therefore, calibration metrics cannot fully capture discrepancies between the score distribution and the true probability distribution of the response variable \(Y\) when the predicted score variability does not accurately reflect the latter.

Kullback-Leibler divergenceFernandes Machado et al. [9] demonstrated through simulated data that scores from ensemble methods may exhibit less variability compared to the true underlying probabilities when selecting hyperparameters based on calibration (ICI) or performance (AUC) metrics. This reduced heterogeneity makes calibration metrics less reliable for interpreting output scores as probabilities of event occurrence. Instead of evaluating discrepancies solely on predicted score values, the authors emphasize evaluating the model's probabilistic estimates using KL divergence between the overall score distribution, \(\hat{s}(\bm{X})\), and the available information on the "true" distribution, \(s(\bm{X})\). Additionally, the flexibility of tree-based methods like XGBoost enables the selection of model hyperparameters based on KL divergence instead of traditional performance metrics, ensuring the availability of a model whose predicted score distribution closely aligns with prior knowledge.

Bayesian FrameworkWhen working with simulated data, the distribution \(s(\bm{X})\) is fully known, allowing for the direct computation of KL divergence with \(\hat{s}(\bm{X})\). However, with real data, the KL divergence can only be computed by relying on a prior belief about the distribution of \(s(\bm{X})\), potentially informed by expert opinion, and thus assuming a prior distribution \(\mathcal{B}\). In the following, as in Fernandes Machado et al. [9], we take \(s(\bm{X})\sim\mathcal{B}=\text{Beta}(\alpha,\beta)\) as the assumed prior distribution where each probability \(p_{i}\) of the \(i\)-th observation is a sample from \(\mathcal{B}\). We observe a sequence of \(n\) independent (as the features \(\bm{X}_{i}\) are considered \(n\) i.i.d. random variables) but non-identically distributed binary random variables \(Y_{i}\) where \(Y_{i}|s(\bm{X}_{i})=p_{i}\sim\text{Bernoulli}(p_{i})\). In this case, instead of selecting the model with hyperparameters that minimize the empirical mean of the KL divergence across individual distributions, we directly minimize the distance between the prior distribution \(\mathcal{B}\) and the overall distribution of \(\hat{s}(\bm{X})\).

Calibration techniquesWe extend the work of Fernandes Machado et al. [9] by investigating how score heterogeneity predicted by certain XGBoost algorithms is affected after applying post-calibration techniques such as Platt scaling, Beta calibration, or isotonic regression. These methods can potentially reduce score heterogeneity; for instance, isotonic regression applies a stepwise function g. Additionally, with Platt scaling, the range of calibrated predicted scores is always narrower than the range of the initial scores when the parameter \(s\geq\frac{1}{4}\) (see Appendix A.1). And, due to the concavity of the sigmoid function over \([0,+\infty]\), this post-calibration method tends to reduce the range of predicted scores more significantly when the initial scores are highly concentrated.

## 4 Numerical Experiments

### Simulated Data

We use the simulated data from Fernandes Machado et al. [9]. We consider four data-generating processes (DGPs), all of which use a logistic link function. The first three are from Ojeda et al. [26], the fourth adds interaction terms (see Appendix B.1). For each DGP, we generate data that include more or less noise variables: 0, 10, 50 or 100. We split the data into four samples: the train and validation samples used to train an XGBoost model and select the set of hyperparameters, the calibration sample to train a calibrator using the selected model, and lastly, a test sample to assess the performance of models. We select the model's hyperparameters (number of boosting iterations and maximum tree depth) to optimize either one of three different criteria on the validation set: maximizing AUC (AUC*), minimizing KL divergence (KL*), and, for illustrative purposes, producing a model that is poorly calibrated based on the ICI metric (High ICI). Once the hyperparameters are selected, a calibration technique is applied to the scores. This allows for a comparison of models on the test set, both before and after calibration, according to the chosen optimization criterion. We run the simulations on 100 replications for each configuration. The results for DGP 1 are shown in Fig. 1 (see Fig. C17 for full results and Table C1 for numerical values). The x-axis represents calibration, measured by the ICI, where lower ICI values indicate better calibration. The y-axis shows the KL divergence between the model's predicted score distribution and the true probabilities, with lower values indicating closer alignment between the two distributions. A model is preferable when it achieves better calibration and closer alignment between score distributions and true probabilities. Shapes represent models before calibration, while arrows show their performance after applying _post-hoc_ calibration. Ideally, _post-hoc_ calibration improves both metrics for uncalibrated models, resulting in arrows pointing down and to the left on the graph.

When the model is selected to optimize AUC (AUC*), calibration is generally fairly good across all DGPs, regardless of the number of noise variables. Applying a _post-hoc_ calibration technique typically reduces the ICI, further improving model calibration. However, Platt scaling (green solid arrows) often fails, as the logistic function lacks the identity mapping. The score distributions from models optimized for AUC, however, are poorly aligned with the true probability distributions. For noise-free datasets, the KL divergence is approximately 2.5 times larger compared to models optimized for KL divergence. This gap widens as the number of noise variables increases. When post-calibration techniques are applied to AUC-optimized models, KL divergence increases with Platt scaling and isotonic regression but decreases with Beta calibration. However, even with Beta calibration, the KL divergence remains higher than that of models optimized for KL divergence. For initially miscalibrated models (High ICI), post-calibration generally improves calibration, with improvements seemingly unaffected by the number of noise variables. However, the impact on KL divergence is more mixed, with no systematic improvement observed, particularly for DGPs 2 and 4.

Overall, while post-calibration improves model calibration, it does not consistently align score distributions with true probabilities and may even exacerbate misalignment, highlighting trade-offs between calibration and distribution alignment.

### Real Data

The 10 datasets from the UCI ML Repository used in Fernandes Machado et al. [9] are used here (see details in Appendix B.2). For each dataset, we apply the method outlined in Section 4.1, this time calculating the KL divergence between the predicted score distribution and the prior distribution described in Section 3.1 The results across the 10 datasets are shown in Fig. 2, with detailed metric values in Table C2. The x-axis represents calibration with ICI, and the y-axis shows KL divergence (lower values indicate closer alignment with Beta priors). Shapes denote models before calibration, and arrows indicate changes after applying post-calibration techniques from Sec. 2.2.

Footnote 1: For illustration purposes, the parameters of the prior distribution \(\mathcal{B}\) are estimated via maximum likelihood using scores from a GAMSEL model [6], where the event is regressed on the variables generating the data.

The findings are consistent with Section 4.1. When applied to already calibrated scores with low ICI (models AUC* and KL*), Platt scaling often worsens both calibration and score alignment with the Beta prior, since the calibration map cannot approximate the identity function (as seen in datasets adult, bank, default, drybean, occupancy, and spambase). In this case, Beta calibration and isotonic regression frequently outperform Platt scaling in both calibration and alignment with the Beta prior. Notably, Beta calibration surpasses isotonic regression in most datasets, particularly concerning KL divergence. For models with initially uncalibrated scores (High ICI), post-calibration techniques either show lower ICI and lower KL divergence (abalone, coupon), or result in increased KL divergence alongside improved calibration (mushroom, occupancy). In such cases, all calibration methods exhibit similar trends in KL divergence and ICI, with no single post-calibration technique consistently outperforming the others, as their effectiveness varies across datasets.

To summarize, for already calibrated scores, post-calibration techniques generally reduce score alignment with Beta priors, as indicated by KL divergence, although Beta calibration results in a smaller deterioration compared to isotonic regression and Platt scaling. For scores with high initial ICI, post-calibration improves calibration but may either reduce or increase score alignment depending on the dataset.

Figure 1: Average KL divergence and ICI before and after recalibration, for DGP 1.

Figure 2: Average KL divergence and ICI before and after recalibration.

## References

* Austin and Steyerberg [2019] Austin, P. C. and Steyerberg, E. W. (2019). The integrated calibration index (ici) and related metrics for quantifying the calibration of logistic regression models. _Statistics in Medicine_, 38(21):4051-4065.
* Becker and Kohavi [1996] Becker, B. and Kohavi, R. (1996). Adult. UCI Machine Learning Repository. doi:10.24432/C5XW20.
* Breiman [2001] Breiman, L. (2001). Random forests. _Machine Learning_, 45(1):5-32.
* Brier [1950] Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. _Monthly Weather Review_, 78(1):1-3.
* Candanedo [2016] Candanedo, L. (2016). Occupancy Detection. UCI Machine Learning Repository. doi:10.24432/C5X01N.
* Chouldechova and Hastie [2015] Chouldechova, A. and Hastie, T. (2015). Generalized additive model selection. _arXiv:1506.03850_.
* Cortez et al. [2009] Cortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J. (2009). Wine Quality. UCI Machine Learning Repository. doi:10.24432/C56S3T.
* Dawid [1982] Dawid, A. P. (1982). The well-calibrated bayesian. _Journal of the American Statistical Association_, 77(379):605-610.
* Fernandes Machado et al. [2024] Fernandes Machado, A., Charpentier, A., Flachaire, E., Gallic, E., and Hu, F. (2024). Probabilistic scores of classifiers, calibration is not enough. _arXiv:2408.03421_.

* Guo et al. [2017] Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On calibration of modern neural networks. In Precup, D. and Teh, Y. W., editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 1321-1330. PMLR.
* Gupta et al. [2021] Gupta, K., Rahimi, A., Ajanthan, T., Sminchisescu, C., Mensink, T., and Hartley, R. I. (2021). Calibration of neural networks using splines. In _International Conference on Learning Representations (ICLR)_.
* Ho [1995] Ho, T. K. (1995). Random decision forests. In _Proceedings of 3rd International Conference on Document Analysis and Recognition_, volume 1, pages 278-282 vol.1.
* Hopkins et al. [1999] Hopkins, M., Reeber, E., Forman, G., and Suermondt, J. (1999). Spambase. UCI Machine Learning Repository. doi:10.24432/C53G6X.
* Koklu and Ali Ozkan [2020] Koklu, M. and Ali Ozkan, I. (2020). Dry Bean. UCI Machine Learning Repository. doi:10.24432/C50S4B.
* Konek [2016] Konek, J. (2016). Probabilistic knowledge and cognitive ability. _Philosophical Review_, 125(4):509-587.

* Kull and Flach [2015] Kull, M. and Flach, P. (2015). Novel decompositions of proper scoring rules for classification: Score adjustment as precursor to calibration. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15_, pages 68-85. Springer.
* Kull and Flach [2014] Kull, M. and Flach, P. A. (2014). Reliability maps: a tool to enhance probability estimates and improve classification accuracy. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14_, pages 18-33. Springer.
* Kumar et al. [2019] Kumar, A., Liang, P. S., and Ma, T. (2019). Verified uncertainty calibration. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R., editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc.
* Leathart et al. [2019] Leathart, T., Frank, E., Pfahringer, B., and Holmes, G. (2019). On calibration of nested dichotomies. In _Springer-Verlag_, page 69-80.
* Mildenhall [1999] Mildenhall, S. J. (1999). A systematic relationship between minimum bias and generalized linear models. In _Journal Proceedings of the Casualty Actuarial Society_, volume 86, pages 393-487.
* Moro et al. [2012] Moro, S., Rita, P., and Cortez, P. (2012). Bank Marketing. UCI Machine Learning Repository. doi: https://doi.org/10.24432/C5K306.

* Murphy (1972) Murphy, A. H. (1972). Scalar and vector partitions of the probability score: Part i. two-state situation. _Journal of Applied Meteorology and Climatology_, 11(2):273-282.
* Nash et al. (1995) Nash, W., Sellers, T., Talbot, S., Cawthorn, A., and Ford, W. (1995). Abalone. UCI Machine Learning Repository. doi:10.24432/C55C7W.
* Ojeda et al. (2023) Ojeda, F. M., Jansen, M. L., Thiery, A., Blankenberg, S., Weimar, C., Schmid, M., and Ziegler, A. (2023). Calibrating machine learning approaches for probability estimation: A comprehensive comparison. _Statistics in Medicine_, 42(29):5451-5478.
* Pakdaman Naeini et al. (2015) Pakdaman Naeini, M., Cooper, G., and Hauskrecht, M. (2015). Obtaining well calibrated probabilities using bayesian binning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 29(1):2901-2907.
* Park and Ho (2020) Park, Y. and Ho, J. C. (2020). Califorest: Calibrated random forest for health data. _Proceedings of the ACM Conference on Health, Inference, and Learning 2020_, pages 40-50.
* Platt (1999) Platt, J. (1999). Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in large margin classifiers_, 10(3):61-74.
* Rahimi et al. (2020) Rahimi, A., Shaban, A., Cheng, C.-A., Hartley, R., and Boots, B. (2020). Intra order-preserving functions for calibration of multi-class neural networks. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, _Advances in Neural Information Processing Systems_, volume 33, pages 13456-13467. Curran Associates, Inc.
* Schervish (1989) Schervish, M. J. (1989). A General Method for Comparing Probability Assessors. _The Annals of Statistics_, 17(4):1856-1879.
* Schlimmer (1987) Schlimmer, J. (1987). Mushroom. UCI Machine Learning Repository. doi:10.24432/C5959T.
* Van Fraassen (1995) Van Fraassen, B. C. (1995). Fine-grained opinion, probability, and the logic of full belief. _Journal of Philosophical logic_, 24(4):349-377.
* Wang et al. (2020) Wang, T., Rudin, C., Doshi-Velez, F., Liu, Y., Klampfl, E., and MacNeille, P. (2020). In-Vehicle Coupon Recommendation. UCI Machine Learning Repository. doi:10.24432/C5GS4P.
* Wilks (1990) Wilks, D. S. (1990). On the combination of forecast probabilities for consecutive precipitation periods. _Weather and Forecasting_, 5(4):640-650.
* Yeh (2016) Yeh, I.-C. (2016). Default of Credit Card Clients. UCI Machine Learning Repository. doi:10.24432/C5553H.
* Zadrozny and Elkan (2002) Zadrozny, B. and Elkan, C. (2002). Transforming classifier scores into accurate multiclass probability estimates. In _Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 694-699.

## Appendix A Platt scaling

### Reduction in Score Range

Platt scaling learns parameters, \(\mu\) and \(s\) (\(s>0\) for a non-decreasing calibration map \(\mathrm{g}\)) on a calibration set. The obtained calibrated probabilities are:

\[\mathrm{g}\left(\hat{s}(\mathbf{x})\right)=\frac{1}{1+\exp\left\{-\frac{1}{s} \big{(}\hat{s}(\mathbf{x})-\mu\big{)}\right\}}.\]

With Platt scaling, the range of calibrated predicted scores is always narrower than the range of the initial scores when the parameter \(s\geq\frac{1}{4}\). Indeed, since \(\rho=\frac{1}{4}\) is the minimum value for which \(\sigma(x)=\frac{1}{1+\exp-x}\) remains \(\rho\)-Lipschitz on \(\mathbb{R}\), for \(x_{1}<x_{2}\in\mathbb{R}\), we have:

\[\left|\sigma\left(\frac{x_{2}-\mu}{s}\right)-\sigma\left(\frac{x_{1}-\mu}{s} \right)\right|\leq\frac{1}{4}\left|\frac{x_{2}-\mu}{s}-\frac{x_{1}-\mu}{s} \right|\leq\frac{1}{4s}\left|x_{2}-x_{1}\right|\text{ with }s>0.\]As a result, if \(s\geq\frac{1}{4}\), the range of \((\hat{s}(\mathbf{x}_{i}))_{i=1}^{n}\) is larger than the range of the calibrated scores with Platt scaling \((\mathrm{g}(\hat{s}(\mathbf{x}_{i})))_{i=1}^{n}\). Let \(\hat{s}_{m}\) (resp. \(\hat{s}_{M}\)) denote the minimum (resp. the maximum) value of \((\hat{s}(\mathbf{x}_{i}))_{i=1}^{n}\). If \(s\geq\frac{1}{4}\), we have:

\[\left|\frac{\mathrm{g}(\hat{s}_{M})-\mathrm{g}(\hat{s}_{m})}{\hat{s}_{M}-\hat {s}_{m}}\right|=\left|\frac{\sigma\left(\frac{\hat{s}_{M}-\mu}{s}\right)- \sigma\left(\frac{\hat{s}_{m}-\mu}{s}\right)}{\hat{s}_{M}-\hat{s}_{m}}\right| \leq 1.\]

And, due to the concavity of the sigmoid function over \([0,+\infty]\), this post-calibration method tends to reduce the range of predicted scores more significantly when the initial scores are highly concentrated.

## Appendix B Data

### Simulated Data

To simulate data, we consider the DGPs from Fernandes Machado et al. [9]. The first three are from Ojeda et al. [26]. In the fourth, an interaction term between two predictors is added. Each scenario uses a logistic model to generate the outcome. Let \(Y_{i}\) be a binary variable following a Bernoulli distribution: \(Y_{i}\sim B(p_{i})\), where \(p_{i}\) is the probability of observing \(Y_{i}=1\). The probability \(p_{i}\) is defined by:

\[p_{i}=\mathbb{P}(Y=1\mid\mathbf{x}_{i})=\big{[}1+\exp(-\eta_{i})\big{]}^{-1}.\] (B.2)

For the second DGP, to introduce non-linearities, \(p^{3}\) is used as true probabilities instead of \(p\).

For all DGPs, \(\eta_{i}=\mathbf{x}_{i}^{\top}\boldsymbol{\beta}\), where \(\mathbf{x}_{i}\) is a vector of covariates and \(\boldsymbol{\beta}\) is a vector of arbitrary scalars. The covariate vector includes two continuous predictors for DGPs 1 and 2. For DGP 3, it includes five continuous and five categorical predictors. For DGP 4, it contains three continuous variables, the square of the first variable, and an interaction term between the second and third variables. Specifically, \(\eta_{i}=\beta_{1}x_{1,i}+\beta_{2}x_{2,i}+\beta_{3}x_{3,i}+\beta_{4}x_{1,i}^ {2}+\beta_{5}x_{2,i}\times x_{3,i}\). Continuous predictors are drawn from \(\mathcal{N}(0,1)\). Categorical predictors consist of two variables with two categories, one with three categories, and one with five categories, all uniformly distributed. The values of coefficients \(\boldsymbol{\beta}\) are reported in Table B1.

For each DGP, we generate data considering four scenarios with varying numbers of noise variables: 0, 10, 50, or 100 variables drawn from \(\mathcal{N}(0,1)\).

For the fourth DGP, to achieve a similar probability distribution to DGP 1, we perform resampling using a rejection algorithm (the algorithm is detailed in [9]).

The datasets are split into four parts: a training sample, a validation sample, a calibration sample, and a test sample, each containing 10,000 observations. The empirical distribution of samples of from each DGP are shown in Fig. B1.

### Real Data

The main characteristics of the datasets are summarized in Table B2.

Most of the datasets used are associated with classification tasks. If not, they contain a binary variable suitable for classification or a variable that can be converted into a binary variable. The target variables for each dataset are as follows:* abalone: gender of abalones (1 for male, 0 for female); originally used to predict the size of abalones.
* adult: high income (1 if income \(\geq\) 50k per year).
* bank: subscription to a term deposit (1 if yes, 0 otherwise).
* default: default payment (1 if default, 0 otherwise).
* drybean: type of dry bean (1 if demason, 0 otherwise); originally a multi-class variable.
* coupon: acceptance of a recommended coupon in different driving scenarios (1 if accepted, 0 otherwise).
* mushroom: mushroom classification (1 if edible, 0 otherwise).
* occupancy: prediction of room occupancy (1 if occupied, 0 otherwise); originally aimed at predicting the age of occupancy from physical measurements.
* winequality: quality of wine (1 if quality \(\geq\) 6, 0 otherwise); originally a scale from 0 to 10, with 0 being bad quality and 10 being good quality.
* spambase: email classification (1 if spam, 0 otherwise).

## Appendix C Numerical Experiments

### Simulated Data

For each of the four DGPs (see Section B.1) and each configuration of the number of noise variables (0, 10, 50, or 100), we generate 100 sample replications. For each sample, we train an XGBoost model on 10,000 observation using the xgb.train function from the R package xgboost. The learning rate is set to 0.3. The tree depth (argument max_depth) varies according to the following values: 2, 4, 6. The number of boosting iterations (argument nrounds) ranges from 1 to 400. All variables (predictors and, if applicable, noise variables) are included in the model without transformation.

For each model configuration, we select the hyperparameters based on different criteria using the validation set results. Specifically, we make three model choices:

* AUC*: hyperparameters are selected to maximize the AUC.

* KL*: hyperparameters are chosen to minimize the Kullback-Leibler divergence between the scores on the validation set and the true probability distribution (observable here in the context of simulated data).
* High ICI: hyperparameters are selected to produce relatively poor calibration, as measured by the ICI. Specifically, we select the model with the smallest ICI among those with an ICI at least one standard deviation above the mean ICI obtained during grid search.

Once the hyperparameters are selected, we apply a recalibration method on an independent calibration set: either Platt scaling, Beta calibration, or isotonic regression.

The model performance is then evaluated on a test set, allowing for comparison based on: (i) the metric used to select the hyperparameters, and (ii) whether or not calibration techniques were applied to the scores.

Figs C1 to C16 display the empirical distribution of scores for a single replication (the first one) in each of the \(4\times 4\) configurations (4 DGPs and 4 different values for the number of noise variables introduced in the training data). In each figure, the first row shows the distribution of test set scores without applying any calibration technique to the selected model. The second row, in green, shows the score distributions after applying Platt scaling for calibration. The third row, in orange, shows the score distribution after applying Beta calibration, and, lastly, the fourth row, in purple, displays the score distributions after applying isotonic regression. The columns correspond to the criteria used to select the hyperparameters based on the validation set results: AUC, Brier score, ICI, KL, or a set chosen such that the ICI is high.

Figure C2: Distribution of estimated scores for XGB: **DGP 1, 10 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Figure C4: Distribution of estimated scores for XGB: **DGP 1**, **100 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Figure C6: Distribution of estimated scores for XGB: **DGP 2, 10 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Figure C8: Distribution of estimated scores for XGB: **DGP 2**, **100 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Figure C10: Distribution of estimated scores for XGB: **DGP 3, 10 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Figure C12: Distribution of estimated scores for XGB: **DGP 3**, **100 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Figure C14: Distribution of estimated scores for XGB: **DGP 4, 10 noise variables**, single replication. Notes: AUC*, Brier*, ICI*, and KL*: models selected based on optimizing AUC, Brier score, ICI, and Kullback-Leibler divergence, resp.

Table C1 reports the average values of metrics calculated on the test set over 100 replications for each DGP and each number of noise variables in the training data. The values are presented for models selected by optimizing, on the validation set, either AUC (AUC*) or KL divergence (KL*), as well as for a model with poor calibration (High ICI). The metrics are calculated before applying any calibration method (column "None"), after applying Platt scaling, Beta calibration and isotonic regression calibration.

Fig. C17 shows the performance of the models, measured by the KL divergence between the test set score distribution (x-axis) and the true probability distribution (y-axis), before and after applying calibration methods. The values represent the average of these two metrics over 100 replications for each DGP (rows), based on the number of noise variables in the training set (columns). The point corresponds to the model whose hyperparameters (number of boosting iterations and tree depth) are selected to maximize AUC on the validation set. The square represents the model selected by minimizing the Kullback-Leibler divergence between the score distribution on the validation set and the true probability distribution. The triangle denotes a model with poor calibration on the test set. Solid green arrows illustrate the change in metrics after applying Platt scaling calibration, dotted orange arrows show changes after applying Beta calibration, and dashed purple arrows indicate changes after applying isotonic regression calibration.

### Real Data

We train XGBoost models on the 10 datasets presented in Section B.2. Unlike Section C.1, the true probabilities underlying the binary events are not observable. Here, we assume that we have prior knowledge about the probability distribution, which can be considered as expert opinion. To simulate this expert opinion, we assume that the true probabilities follow a Beta distribution. The parameters of this distribution, specific to each dataset, are estimated via MLE using the scores from a GAMSEL model [6].

Using these prior distributions, it is possible to replicate the estimation procedure previously applied to the simulated data. Each dataset is split into two parts: 80% of the observations are used to train an XGBoost model (on a training set comprising 70% of these observations, with hyperparameters selected based on metrics calculated on the remaining 20% validation set), and the remaining 30%

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]