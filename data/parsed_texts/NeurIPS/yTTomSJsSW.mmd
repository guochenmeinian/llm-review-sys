# Aligning Large Language Models with Representation Editing: A Control Perspective

 Lingkai Kong\({}^{1}\)

Haorui Wang\({}^{1}\)

Equal contribution. Correspondence to Lingkai Kong <lingkaikong@g.harvard.edu>, Haorui Wang <hwang984@gatech.edu>, and Wenhao Mu <muwenhao@umich.edu>

Wenhao Mu\({}^{1}\)

Yuanqi Du\({}^{2}\)

Yuchen Zhuang\({}^{1}\)

Yifei Zhou\({}^{3}\)

Yue Song\({}^{4}\)

Rongzhi Zhang\({}^{1}\)

Kai Wang\({}^{1}\)

Chao Zhang\({}^{1}\)

\({}^{1}\)Georgia Tech \({}^{2}\)Cornell University \({}^{3}\)UC Berkeley \({}^{4}\)University of Trento

###### Abstract

Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods. Our code is available at https://github.com/Lingkai-Kong/RE-Control.

## 1 Introduction

Autoregressive large language models (LLMs) such as ChatGPT [1], PaLM [13], and LLama [59], which are trained on extensive datasets, have demonstrated impressive abilities across a diverse array of tasks. However, the heterogeneous nature of their training data may lead these models to inadvertently generate misinformation and harmful content [22, 17, 66]. This issue highlights the critical challenge of aligning language models with human objectives and safety considerations, a concern extensively discussed in recent research [44, 11].

Existing approaches to aligning LLMs generally fall into two categories: fine-tuning and test-time alignment. Among fine-tuning methods, Reinforcement Learning from Human Feedback (RLHF; [51, 74, 59]) is particularly powerful. RLHF involves training a Reward Model (RM) based on human preferences and then using this model to fine-tune LLMs through reinforcement learning techniques [48]. However, RL training can be difficult and unstable. Recent works [46, 70, 14] propose simpler alternatives to RLHF, but these methods still demand substantial computational resources. Additionally, the necessity of fine-tuning to adapt alignment objectives complicates the ability to swiftly customize models in response to evolving datasets and emerging needs.

On the other front, several test-time alignment techniques have been developed to tailor LLMs to specific objectives without altering their weights, such as prompt engineering and guided decoding [43, 31, 27]. However, since these methods do not modify the underlying LLM, their alignment capability remains questionable, and performance may heavily depend on the original LLM.

In this paper, we take an alternative approach to aligning LLMs using representation editing. Instead of updating model weights, representation engineering perturbs a small fraction of model representations to steer behaviors, demonstrating great potential in improving LLMs' truthfulness [35] and reducing hallucinations [75]. However, previous works typically rely on adding a fixed perturbation to the representation space during the generation process and do not take into account the autoregressive generation nature of LLMs. To address this, we propose a dynamic representation editing method from a control perspective.

The foundation of our model design is the connection between discrete-time stochastic dynamical systems and autoregressive language models. Inspired by techniques from control theory, we introduce control signals to the state space of the language dynamical system to achieve specific alignment objectives. According to Bellman equation, we directly train a value function in the representation space of LLMs. At test time, we perform gradient-based optimization to determine the control signals. Since the value function is simply a two- or three-layer neural network, the intervention is very fast and efficient. To align with the objective while preserving the generation quality of the original LLMs, we regularize the control signal to be as small as possible. This regularization is equivalent to control the step size or the number of steps during interventions at test time.

The main contributions of our work are: (1) We propose a new representation editing method to align LLMs from a control perspective. Our model, named Re-Control, does not require extensive computing resources compared to fine-tuning methods. Unlike existing test-time alignment methods such as prompt engineering and guided decoding, our approach perturbs the representation space of LLMs, offering greater flexibility. (2) We propose training a value function and computing the control signal at test time using gradient-based optimization. (3) We empirically show that Re-Control outperforms various existing test-time alignment methods and exhibits strong generalization ability.

## 2 Related Works

### Large Language Model Alignment

Alignment through Fine-tuning.RLHF has been a popular method in LLM alignment [51; 74; 59]. While effective, RLHF entails a complex process that involves training multiple models and continuously sampling from the LM policy during the learning loop. DPO [46] simplifies the RLHF framework by using a direct optimization objective derived from Proximal Policy Optimization (PPO; [48]), reducing the process to supervised training of the policy model alone. However, DPO is memory-intensive and resource-demanded as it requires managing two policies simultaneously. Contrastive Preference Optimization (CPO; [69]) mitigates these challenges by utilizing a uniform reference model, which not only reduces memory requirements but also enhances training efficiency. Alternative methods such as [71; 50] simplify model management and parameter tuning in the RLHF framework by adopting a supervised fine-tuning (SFT) approach. Additionally, RSO [39] and RAFT [18] employ rejection sampling to refine the alignment process. RSO focuses on estimating the optimal policy more accurately, while RAFT uses high-quality samples for iterative fine-tuning of the policy model.

Despite these advancements, a notable limitation of aligning LLMs through fine-tuning methods is their inflexibility in adapting quickly to emerging data and standards without extensive retraining, which poses challenges in dynamic environments where rapid adaptability is crucial.

Test time alignment.The other branch of methods to align LLMs involves adjustments at inference time. The simplest way is through prompt engineering. Existing works [4; 72; 36] have proposed the use of prompts that blend instructions with in-context examples to enhance the honesty and harmlessness of responses from LLMs. For instruction-tuned models, it has been shown that simply employing prompt engineering--without the addition of in-context examples--can enhance the safety of the models, as reported in [59].

In addition to prompting methods, guided decoding techniques have also been explored. ARGS [31], incorporate the score of a pre-trained reward model into the token probabilities. Other works [43; 25] learn a prefix scorer for the reward that is used to steer the generation from a partially decoded path. Moreover, DeAL [27] approaches the decoding process as an A* search agent, optimizing the selection of tokens

### Representation Engineering

Representation engineering [75] introduces steering vectors to the representation space of LLMs to enable controlled generation without resource-intensive fine-tuning. This concept of activation perturbation has its origins in plug-and-play controllable text generation methods [15], which utilizes a separate classifier for each attribute to perturb the model's activations, thereby producing text that aligns more closely with the classifier's target attributes. Prior research have demonstrated that both trained and manually selected steering vectors can facilitate style transfer in language models [53; 60]. Li et al. [35] have shown that steering the outputs of attention heads can enhance the truthfulness of LLMs. Liu et al. [38] suggest that standard in-context learning can be seen as a process of "shifting" the latent states of a transformer. More recently, representation fine-tuning [68; 67] has been introduced as a direct substitute for existing parameter-efficient fine-tuning methods. Remarkably, Wu et al.[68] show that the representation editing can even surpass fine-tuning based methods by intervening on hidden representations within the linear subspace defined by a low-rank projection matrix. The effectiveness of these approaches confirms that the representations of pretrained LMs are semantically rich. Liu et al. [40] also explore representation engineering for aligning LLMs. However, their approach is notably more complex, necessitating an initial fine-tuning phase to capture the representation pattern, followed by a subsequent fine-tuning of the final model based on these patterns.

### Control Theory and Large Language Models

Understanding LLMs from a dynamical system perspective is a burgeoning field. Current research leverages control theory to enhance prompt design, demonstrating that LLMs can be effectively directed by carefully chosen inputs ("prompts") given sufficient time and memory resources. The seminal work by Soatto et al. [49] investigates the controllability of LLMs, focusing on'meaningful sentences' defined as the sigma-algebra generated by text fragments on the Internet. Subsequent research [9] broadens this analysis to encompass arbitrary sentences. Additionally, Luo et al. [42] expand the scope to include multi-round interactions with LLMs and multi-agent collaboration, offering new insights into the dynamical capabilities of these models. To the best of our knowledge, our study is the first to investigate optimal control for representation editing in LLMs.

## 3 Background: Stochastic Dynamical System and Optimal Control

Optimal control theory [56; 6], when applied to discrete-time dynamical systems [47], seeks to determine a control strategy that maximizes a cumulative reward over a sequence of time steps. This framework is particularly relevant to fields such as robotics [57; 58; 34; 28], automated trading systems [41; 65; 16; 41], autonomous vehicle navigation [30; 62; 29; 33], where decisions must be made sequentially to achieve a long-term goal.

Formally, a discrete-time stochastic dynamical system can be defined as follows:

\[s_{t+1}=f(s_{t},u_{t},\omega_{t}),\]

where \(s_{t}\in\mathcal{S}\) denotes the system's state at time \(t\), and \(u_{t}\in\mathcal{U}\) represents the control input at the same time step. The stochastic term \(\omega_{t}\) is typically modeled as a random noise drawn from a known probability distribution (e.g. Brownian motion), which introduces uncertainty into the state transition process. The function \(f\) specifies the state transition dynamics influenced by the current state, control input, and the stochastic nature of the environment.

The process begins from an initial state \(s_{0}\), which serves as the starting point for all subsequent decisions and state transitions. The aim of optimal control is to determine a control policy \(\pi:\mathcal{S}\rightarrow\mathcal{U}\), mapping states to optimal control actions, that maximizes the expected cumulative reward:

\[\mathbb{E}_{\pi}[R]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{T}r(s_{t})\right],\]

where \(R\) is the cumulative reward and \(r(s_{t})\) is the intermediate reward received at each time step.

Methods such as policy iteration [7; 37] can be used to determine the optimal control policy. Each iteration involves two steps. First, we evaluate the current policy \(\pi\) by solving the Bellman equation:

\[V^{\pi}(s_{t})=\mathbb{E}_{\omega_{t}}\left[r(s_{t})+V^{\pi}\left(f(s_{t},u_{t },\omega_{t})\right)\right],\]where \(V^{\pi}(s_{t})\) represents the expected return over \(\omega_{t}\) when the system starts in state \(s_{t}\) and follows policy \(\pi\).

Next, we improve the policy:

\[\pi(s_{t})\leftarrow\operatorname*{arg\,max}_{u\in\mathcal{U}}\left(r(s_{t})+ \mathbb{E}_{\omega_{t}}\left[V^{\pi}(f(s_{t},u_{t},\omega_{t}))\right]\right).\]

These evaluation and improvement steps are repeated until convergence.

## 4 Aligning Large Language Models from a Control Perspective

In this section, we present our method, Re-Control. First, we explain how autoregressive language models can be viewed as discrete-time stochastic dynamical systems. Next, we describe how to introduce control through representation editing. Finally, we detail the process of training the value function and performing test-time alignment.

### Autoregressive LLMs are Discrete-Time Stochastic Dynamical Systems

A pre-trained autoregressive language model processes a sequence of input tokens and predicts subsequent tokens by recursively processing the sequence. we focus on the transformer-based architecture [61] prevalent in modern language models [10; 55; 1].

**Definition 4.1** (Language dynamical system): _The behavior of a language dynamical system is governed by a function \(f_{\mathrm{LM}}\), which acts as the state transition function, defined as:_

\[y_{t}\sim\text{Softmax}(Wo_{t}),\quad h_{t+1},o_{t+1}=f_{\mathrm{LM}}(h_{t},y_ {t}).\]

_Here, \(y_{t}\) is the newly generated token at each time step. \(h_{t}\) comprises key-value pairs accumulated from previous time steps, represented as \(h_{t}=[\{(K_{0}^{(l)},V_{0}^{(l)})\}_{l=1}^{L},\cdots,\{(K_{t}^{(l)},V_{t}^{( l)})\}_{l=1}^{L}]\). Each pair \((K_{t}^{(i)},V_{t}^{(i)})\) corresponds to the key-value pairs generated from the \(i\)-th layer at time \(t\). \(W\) is a linear transformation that maps the logits \(o_{t+1}\) to a probability distribution over the vocabulary space \(\mathcal{V}\). The system's evolution continues until \(y_{t}=\mathrm{EOS}\), where \(\mathrm{EOS}\) represents a special stopping token that signifies the end of the system._

In this system, the hidden state \(h_{t}\) along with the logits \(o_{t}\) corresponds to the state \(s_{t}\) in a traditional stochastic dynamical system. The newly sampled token \(y_{t}\) at each time step plays a role similar to the random variable \(\omega_{t}\), introducing stochasticity into the system. The initial state, \(s_{0}=\{h_{0},o_{0}\}\), is set by a given prompt \(\mathbf{x}\), marking the starting point of the dynamical process.

Figure 1: Overview of Re-Control: A value function is trained on the hidden space of an LLM to predict the expected reward. At test time, we optimize the hidden state of the LLM to maximize the value score. Re-Control effectively steers LLMs toward specific alignment objectives while avoiding the expensive fine-tuning process.

However, unlike typical dynamical systems, this model lacks a direct control signal, functioning as an uncontrolled system. Next, we will explore how optimal control techniques can be applied to align the behavior of pre-trained language models with specific objectives.

### Adding Control Signals to Large Language Models with Representation Editing

We introduce control signals \(u_{t}=\{u_{t}^{h},u_{t}^{o}\}\) into the state of the language dynamical system \(s_{t}=\{h_{t},o_{t}\}\) at each time step to achieve specific alignment objectives. Thus, the controlled language dynamical system is described as follows:

\[y_{t}\sim\text{Softmax}\left(W(o_{t}+u_{t}^{o})\right),\quad h_{t+1},o_{t+1}= f_{\text{LM}}(h_{t}+u_{t}^{h},y_{t}).\]

As we can see, adding control to such a language dynamical system is similar to representation editing. However, unlike existing representation editing methods [35], which add a fixed vector during the generation process, we dynamically perturb the representation space from a control perspective, offering greater flexibility. In practice, it is not necessary to add controls to the entire state space; perturbing only a subset is sufficient. For example, we can perturb only the state of the last layer.

For an alignment task, the reward function is defined as:

\[R\left([\mathbf{x},\mathbf{y}_{t}]\right):=\begin{cases}0&\text{if }y_{t}\neq \text{EOS}\\ r\left([\mathbf{x},\mathbf{y}_{t}]\right)&\text{if }y_{t}=\text{EOS},\end{cases}\]

where \([\mathbf{x},\mathbf{y}_{t}]\) denotes the concatenation of the prompt and the model's response generated up to time \(t\). A reward is given only upon completion of decoding, with no reward assigned to a partial decoding path. The reward on the final response \(r\) can come from a pre-trained reward model [51] based on human preference data or specified by heuristics, such as a concise summary in fewer than 10 words, with a reward of 1 if achieved and 0 if it fails.

Our objective is to determine the control signals at each time step that maximize the expected reward while not deviating too much from the original state:

\[\operatorname*{arg\,max}_{\{u_{t}\}_{t=1}^{T}}\mathbb{E}[R]-\lambda\sum_{t=1} ^{T}||u_{t}||_{2}^{2},\] (1)

where \(\lambda\) is a hyper-parameter for regularization. The regularization term is designed to prevent reward overoptimization and maintain the generation quality of the perturbed LLMs.

### Training of Value Function

Traditional policy iteration involves multiple iterations of policy evaluation and policy improvement. However, in our case, to avoid significant deviation from the pre-trained model's original state, we perform only one-step policy iteration. The initial policy is to not add any control signal to LLMs, i.e., \(u_{t}=0\). Therefore, we only need to estimate the value function of the original language model.

The value function of the initial zero policy satisfies the Bellman equation [54]:

\[V(s_{t})=\begin{cases}\mathbb{E}_{s_{t+1}}\left[V(s_{t+1})\right],&\text{if }y_{t}\neq \text{EOS}\\ r\left([\mathbf{x},\mathbf{y}_{t}]\right),&\text{if }y_{t}=\text{EOS}\,.\end{cases}\]

Figure 2: At test time, we perform gradient-based optimization to determine the control signals added to the language dynamical system for alignment. The color represents the value score on the state space, with darker colors indicating higher scores. Our goal is not to update the state to the global optimum but to control the state to achieve a better value score while remaining close to the original state.

To construct the training dataset for the value function, for a prompt \(\mathbf{x}^{i}\) in the given training dataset, we sample \(M\) responses \(\{\mathbf{y}^{i,m}\}_{m=1}^{M}\). We score each response using the reward function and extract the states along the trajectories \(\mathcal{D}_{V}=\{\{\mathbf{s}^{i,m},\mathbf{y}^{i,m},r^{i,m}\}_{m=1}^{M}\}_{i=1}^ {N}\). Our training objective is:

\[\mathcal{L}=\sum_{i}\sum_{m}\sum_{t}\left(V_{\phi}(s_{t}^{i,m})-\mathrm{stop \text{-}grad}(v_{t}^{i,m})\right)^{2}.\]

Here, \(s_{t}^{i,m}\) and \(v_{t}^{i,m}\) represent the state and the generated token of the LLM at generation time step \(t\). \(\mathrm{stop\text{-}grad}(\cdot)\) indicates that the gradient is not propagated through \(v_{t}^{i,m}\). The target value \(v_{t}^{i,m}\) is computed as follows:

\[v_{t}^{i,m}=\begin{cases}V_{\phi}(s_{t+1}^{i,m})&\text{if }y_{t}^{i,m}\neq \mathrm{EOS}\\ r^{i,m},&\text{if }y_{t}^{i,m}=\mathrm{EOS}\,.\end{cases}\]

### Test-time Intervention

At inference time, we can directly perform gradient ascent on the model states to maximize the expected value score, as we train the value function on the state space. Our goal is not to find the global optimum in the state space but to improve the current state while staying close to the original state. Specifically, we initialize \(u_{t}=0\) and update \(u_{t}\) through gradient ascent as:

\[u_{t}=u_{t}+\alpha\nabla_{s_{t}}V_{\phi}(s_{t}+u_{t}),\]

where \(\alpha\) is the step size. This update step can be repeated \(n\) times.

**Implicit Regularization.** Note that this update already incorporates the regularization effect. The regularization is achieved by using a small step size \(\alpha\) and a limited number of updates \(n\), ensuring that the control signal remains small. After adding the final control signals to the hidden states, we perform a forward pass in the language model to generate a new token.

**Parameterization of the Value Function.** Rigorously, policy iteration requires the input to the value function to be the full state, but it does not require the control signals to be added to the full state. This means we can train the value function on the full state and backpropagate through it with respect to partial inputs at test time. The simplest approach is to add control signals only to the logit \(o_{t}\). In this case, we find that training a two- or three-layer neural network using \(o_{t}\) as the input is already sufficient for achieving good empirical performance. If we want to further incorporate the attention key-value pairs \(h_{t}\) in the input, we need to address the input's varying size. To achieve this, we can initialize a vector and compute an attention weight by taking the dot product with the keys to aggregate all value embeddings. Then, we concatenate the aggregated value embedding with \(o_{t}\) and input it into a neural network.

## 5 Experiment

In this section, we conduct experiments to examine the effectiveness of our method. Our focus is on aligning large language models (LLMs) for helpfulness and minimizing harmfulness, which are essential qualities for an AI assistant.

### Experimental Setup

We evaluate our method on the HH-RLHF [5] and Stanford SHP (SHP) [21] datasets, which are popular for LLM alignment. These two datasets are used to improve the AI assistant's helpfulness and harmlessness. Each sample in the datasets contains a prompt and two responses with one preferred over another. For the base model, we adopt Vicuna-7B [12], Falcon-7B [3] and L1ama3-8B [19] as the instructed fine-tuned AI assistant. We evaluate these models by generating text responses based on test prompts of HH-RLHF and SHP. For reproducibility, we use publicly available reward models2[3]. We train the value network on the last layer of the hidden states \(o_{t}\), and at test time, we add control signals only to this layer. For future studies, we can also explore adding controls to the attention key-value pairs \(h_{t}\) which should further improve the performance.

Footnote 2: HH-RLHF: https://huggingface.co/argsearch/llama-7b-rm-float32

Footnote 3: SHP: https://huggingface.co/openbmb/ULtraRM-13b

Following [31], we leverage Diversity, Coherence, Average Reward, and Win Rate as our evaluation metrics. **Diversity** measures the frequency of repeated n-grams in generated text. The diversity score for a given response \(\mathbf{y}\) is represented as \(\prod_{n=2}^{4}\frac{\text{unique-grams}(\mathbf{y})}{\text{total n-grams}( \mathbf{y})}\). A higher diversity score suggests a broader vocabulary range in text generation. **Coherence** calculates the cosine similarity between the embeddings of the prompt and its continuation. We use the pre-trained SimCSE sentence embedding model, following the approach outlined in [52], to obtain these embeddings. **Average Reward** is the mean of the rewards evaluated by the reward model across all responses corresponding to the test prompts. **Win Rate** is the rate at which the model's response is rated better than the preferred response in the dataset. Following [31; 12], we use GPT-4 as the judge, having it review and score two responses to the same prompt on a scale from 1 to 10. We provide explicit instructions to assess the responses based on criteria such as helpfulness, harmlessness, relevance, accuracy, and insightfulness. The detailed prompt is provided in Appendix D. We randomly sample 300 prompts from the test set of HH-RLHF for the GPT-4 evaluation. To mitigate position bias, we randomize the order in which we present the generated responses to GPT-4, as in [73]. Additionally, we also present the **Inference Time** in hour under batch size of 32.

We randomly sample 1000 data points from the training set as a separate validation set to select the hyperparameters--the step size \(\alpha\) and the number of updates \(n\)--based on the sum of coherence, diversity, and average reward. Additional experimental details are provided in Appendix C.

### Baselines

We compare our method with several existing test-time alignment methods.

**Prompt Engineering:** In this method, we instruct the model within the prompt to provide responses that are more helpful and harmless [59]. **Controlled Decoding (CD):** During the decoding process of LLMs, this method combines token probabilities with reward scores. We consider two versions. The first version [31] directly uses a reward model trained on human preference data, requiring the tokenization strategies of both the reward model and the base model to be the same. The second version [43] trains a prefix scorer to predict the expected reward from partially generated responses. We refer to it as CD prefix. **Static Representation Editing (RE):** Following [35], we first train a linear regression layer on the hidden state of an LLM, after feeding it the prompt, to predict the

\begin{table}
\begin{tabular}{l|c|c c c c c} \hline \hline Dataset & Backbone & Model & Diversity \(\uparrow\) & Coherence \(\uparrow\) & Average Reward \(\uparrow\) & Win Rate (\%) \(\uparrow\) & Inference time (hour) \\ \hline \multirow{8}{*}{Vicuna-7B} & \multirow{8}{*}{Vicuna-7B} & Base & 0.816 & 0.568 & 5.894 & 57.6 & 0.60 \\  & & Static RE & 0.818 & 0.568 & 5.907 & 64.3 & 0.65 \\  & & CD & 0.806 & **0.608** & 5.458 & 72.3 & 47.43 \\  & & CD prefix & 0.805 & 0.576 & 6.105 & 74.6 & 32.13 \\  & & Ours. & 0.824 & 0.579 & 6.214 & **75.6** & 0.85 \\ \cline{2-7}  & \multirow{8}{*}{Ours+Prompting} & 0.817 & 0.570 & 5.913 & 66.0 & 0.69 \\  & & CD prefix + Prompting & 0.812 & 0.593 & 6.120 & 74.3 & 47.16 \\  & & Ours+Prompting & **0.830** & 0.577 & **6.267** & **80.3** & 0.93 \\ \cline{2-7} HH-RLHF & \multirow{8}{*}{Falcon-7B} & Base & 0.705 & 0.613 & 3.439 & 42.3 & 0.67 \\  & & Static RE & 0.698 & 0.610 & 3.449 & 52.6 & 0.56 \\  & & CD & N/A & N/A & N/A & N/A & N/A \\  & & CD prefix & 0.648 & 0.575 & **4.397** & 49.6 & 48.13 \\  & & Ours & 0.699 & 0.615 & 3.512 & 58.0 & 1.93 \\  & & Proempting & **0.746** & 0.620 & 4.010 & 52.3 & 0.59 \\  & & CD prefix + Prompting & 0.571 & **0.638** & 3.619 & 51.6 & 47.87 \\  & & Ours+Prompting & 0.741 & 0.619 & 4.083 & **62.6** & 2.00 \\ \hline \multirow{8}{*}{Vicuna-7B} & \multirow{8}{*}{Vicuna-7B} & Base & 0.845 & 0.657 & -5.68 & 40.3 & 0.13 \\  & & Static RE & 0.848 & 0.652 & -5.65 & 49.3 & 0.15 \\  & & CD & 0.845 & 0.655 & -5.65 & 55.6 & 22.16 \\  & & CD prefix & 0.838 & **0.660** & -5.62 & 41.0 & 14.15 \\  & & Ours & 0.849 & 0.652 & -5.38 & 58.0 & 0.21 \\  & & Prompting & 0.847 & 0.570 & -4.83 & 56.6 & 0.13 \\  & & CD prefix + Prompting & 0.842 & 0.574 & -4.88 & 56.3 & 14.32 \\  & & Ours+Prompting & **0.854** & 0.571 & **-46.63** & **63.6** & 0.23 \\ \cline{2-7}  & \multirow{8}{*}{Llama3-8B} & Base & 0.878 & 0.672 & -4.64 & 56.3 & 0.13 \\  & & Static RE & 0.875 & 0.674 & -4.49 & 57.0 & 0.15 \\ \cline{2-7}  & & CD & N/A & N/A & N/A & N/A & N/A \\ \cline{1-1}  & & CD prefix & 0.862 & **0.685** & -4.41 & 64.0 & 12.16 \\ \cline{1-1}  & & Ours & 0.883 & 0.669 & -4.39 & 71.0 & 0.21 \\ \cline{1-1}  & & Prompting & 0.891 & 0.605 & -4.45 & 59.6 & 0.13 \\ \cline{1-1}  & & CD prefix + Prompting & 0.872 & 0.603 & -4.25 & 68.0 & 12.74 \\ \cline{1-1}  & & Ours+Prompting & **0.893** & 0.605 & **-4.14** & **77.0** & 0.24 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparison between Re-Control and other test-time alignment approaches. The win rate is evaluated by GPT-4 as the rate at which the model’s response is rated better than the preferred response in the dataset. Note that CD [31] requires the base model to have the same tokenization strategy as the reward model.

expected reward. At test time, we shift the activation space along the direction of the weights of the linear layer. Unlike our method, this approach adds a fixed vector to the representation space along the generation trajectory.

We provide more implementation details of the baselines in Appendix C.

### Experimental Results

Table 1 shows the performance of all the methods. Our findings can summarized as follows: (1) Re-Control achieves the highest alignment score in terms of the win rate evaluated by GPT-4. Moreover, it maintains generation quality, as measured by diversity and coherence. (2) The strongest baseline is controlled decoding. However, controlled decoding is 20 times slower than Re-Control. This is because controlled decoding needs to evaluate multiple candidate tokens and perform forward passes through the entire reward model repeatedly, while Re-Control only requires optimization through a value function that is a two- or three-layer neural network, making it much faster. Additionally, CD [31] lacks support for batch generation, making it even slower. (3) Combining prompt engineering with Re-Control can further improve alignment performance in terms of both average reward and GPT-4 evaluation. Specifically, it outperforms the strongest baseline by {7.6%, 19.0%, 12.4%, 13.2%} in terms of the GPT-4 win rate. In contrast, controlled decoding with prompting shows only marginal improvements. This might be because Re-Control perturbs the activation space of the LLM, which is more flexible than merely changing the final token probability. (4) Re-Control significantly outperforms static representation editing by {17.6%, 10.2% 17.6%, 24.6%}. This is because Re-Control dynamically adjusts the representation during the autoregressive generation process, offering more control. In contrast, static representation editing applies a fixed shift, which is more rigid.

In Table 2, we present a qualitative example demonstrating how Re-Control can steer the base model to output more helpful and harmless responses. In this example, the user asks for suggestions on lying to an organization. The base model provides various tactics, while Re-Control refuses to give such suggestions and emphasizes that lying can damage relationships and trust within an organization.

## 6 Further Analysis

### Comparison with Training-time Alignment

In the previous section, we compared Re-Control with test-time alignment methods that do not require extensive computing resources. This feature is crucial when we need the model to quickly adapt to different requirements, as it only involves training a simple value network with just two or three layers. In this subsection, we further compare Re-Control with fine-tuning based approaches with LoRA [26]. Figure 3 shows the comparison between Re-Control, Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO) [46].

\begin{table}
\begin{tabular}{l} Example \\ \hline
**Prompt** \\ USER: What’s the easiest way to lie your way to the top of an organization? ASSISTANT: \\
**Base** \\ There is no easy way to lie your way to the top of an organization, as it ultimately depends on the specific company and industry. However, some common tactics that people may use to advance their careers through deception include: 1. Exaggering skills or accomplishments: This can be done by inflating the scope or impact of past projects, or by claiming skills or certifications that you do not have. 2. Pretending to be someone you’re not: This can involve adopting a different personality or presenting yourself as more confident or experienced than you actually are. 3. \\
**Re-Control** \\ There is no easy way to lie your way to the top of an organization, as ethical and honest behavior is typically expected of leaders. Additionally, lying and deceit can damage relationships and trust within an organization, which can ultimately harm one’s career prospects. It’s important to focus on building credibility and trust through honest and transparent communication, as well as demonstrating competence and dedication in one’s work. \\ \hline \end{tabular}
\end{table}
Table 2: Comparative examples of base model Vicuna-7B and Re-Control.

All the models use Vicuna-7B as the base model and we test them on HH-RLHF. The training details for LoRa-based PPO and DPO are provided in C. Overall, the results indicate that our approach is a competitive alternative to LoRa-based fine-tuning methods. Similar findings have also been reported in the controlled decoding literature [31]. Overall, for users prioritizing real-time inference, amortizing computation during the training process remains preferable. However, for those without resources for fine-tuning, test-time alignment is a more practical choice, as it easily adapts to different alignment objectives, albeit with increased inference time.

### Generalization to a new input distribution

An important question is how our method can generalize to a new input distribution different from the value function is trained on. To investigate this question, we further test on a out-of-distribution (OOD) dataset HarmfulQA [8] with the value function trained on HH-RLHF. The test split of HarmfulQA contains harmful questions to evaluate language model performance against red-teaming attempts. We focus on the GPT-4 evaluation since the reward model will not be accurate for the OOD data. We compare Re-Control + Prompting with other test-time alignment methods + Prompting. Figure 4 presents the results. As illustrated, Re-Control + Prompting achieves the highest performance in terms of the GPT-4 win rate on both Vicuna-7B and Falcon-7B. This is an important ability especially when we want to deploy the LLM in the open world.

### Hyperparameter Study

To better understand the characteristics of Re-Control, we vary two hyperparameters--the step size \(\alpha\) and the number of updates \(n\) for the test-time intervention--and measure key performance statistics. Figure 5 shows the diversity, coherence, and average reward of the generated responses in relation to these two parameters on 1000 randomly sampled prompts from HH-RLHF.

As we can see, increasing the step size \(\alpha\) initially improves the reward, but beyond a certain point, larger step sizes fail to compute the control signal accurately, causing the reward to decrease. The influence of the number of updates \(n\) shows a more complex pattern: the reward first improves, then decreases, and improves again, indicating a transition from escaping a local minimum to moving towards another minimum. The coherence and diversity metrics drop to nearly zero, which is evidence of reward overoptimization. Thus, regularization to prevent significant deviation from the original states is essential. In practice, we select these two hyperparameters based on the sum of all three metrics on the validation set.

Figure 4: Testing on out-of-distribution data HarmfulQA. The win rate is measured by GPT-4 as the rate at which responses are better than those of the base model, since the test set of HarmfulQA does not provide reference responses.

Figure 3: Comparison with LoRa-based fine-tuning methods using Vicuna-7B as the base model on HH-RLHF.

### Inference Time Analysis

We provide additional analysis of the inference time. Figure 6 presents the inference time across different batch sizes. As shown, increasing the batch size reduces the discrepancy between Re-Control and the base model, becoming negligible at a batch size of 32. Figure 7 illustrates the compute-performance tradeoff between Re-Control and CD. For Re-Control, we vary the number of iterations when optimizing through the value function at test time, while for CD, we adjust the number of candidate tokens. As shown, the performance of RE-Control initially increases with more computing time but eventually decreases. This decline occurs because a large number of iterations at test time can lead to reward hacking, reducing the quality of the generated sentences. As discussed in Section 6.3, this hyperparameter can be selected based on the validation set. Since CD does not support batch generation, its inference speed is significantly slower than Re-Control. Even when RE-Control does not use batch generation, it outperforms CD when using the same computing resources. For example, when the inference time is around \(155\) minutes, the win rate of Re-Control is \(75\%\), while CD is only \(62\%\).

## 7 Conclusion, Limitations and Future Work

In this paper, we propose Re-Control to align large language models (LLMs) at test-time using representation editing. We view autoregressive language models as discrete-time stochastic dynamical systems and introduce control signals to their representation space. Throughout the generation process, the representation space is dynamically perturbed to achieve higher value scores. Our method does not require fine-tuning the LLMs and offers more flexibility than existing test-time alignment methods such as prompting and guided decoding. We empirically show that Re-Control outperforms existing test-time alignment methods and exhibits strong generalization ability. Due to the space limit, we discuss limitations and future work in Appendix A.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their helpful comments. This work was supported in part by NSF IIS-2008334, IIS-2106961, IIS-2403240, and CAREER IIS-2144338, Schmidt Sciences AI2050 Fellowship and computing resources from Georgia Tech.

## References

* [1]J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. Leoni, A. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.
* [2]A. Aksell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. (2021) A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861. Cited by: SS1.
* [3]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan (2022) Training a helpful and harmless assistant with reinforcement learning from human feedback. Cited by: SS1.
* [4]L. David Berkovitz (2013) Optimal control theory. Vol. 12, Springer Science & Business Media. Cited by: SS1.
* [5]D. P. Bertsekas (2011) Approximate policy iteration: a survey and some new methods. Journal of Control Theory and Applications9 (3), pp. 310-335. Cited by: SS1.
* [6]R. Bhardwaj and S. Poria (2023) Red-teaming large language models using chain of utterances for safety-alignment. arXiv preprint arXiv:2308.09662. Cited by: SS1.
* [7]A. Bhargava, C. Witkowski, M. Shah, and M. Thomson (2023) What's the magic word? a control theory of llm prompting. arXiv preprint arXiv:2310.0444. Cited by: SS1.
* [8]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [9]S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. (2023) Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research. Cited by: SS1.
* [10]W. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al. (2023) Vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. Imsys. org (accessed 14 April 2023), pp. 6. Cited by: SS1.
* [11]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022) Palm: scaling language modeling with pathways. arxiv preprint arXiv:2204.02311. Cited by: SS1.
* [12]J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang (2024) Safe RLHF: safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, Cited by: SS1.
* [13]S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu (2020) Plug and play language models: a simple approach to controlled text generation. In International Conference on Learning Representations, Cited by: SS1.
* [14]J. D. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-10. Cited by: SS1.
* [15]S. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [16]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [17]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [18]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [19]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [20]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [21]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [22]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [23]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [24]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [25]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [26]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [27]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [28]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [29]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [30]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [31]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [32]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [33]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [34]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [35]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [36]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [37]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [38]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [39]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [40]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [41]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [42]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [43]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [44]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [45]J. Lee, S. Lee, and J. Lee (2020) A unified learning framework for reinforcement learning. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1-1. Cited by: SS1.
* [46]J. Lee, S. Lee, and J. Lee (202* [16] Michael AH Dempster and Vasco Leemans. An automated fx trading system using adaptive reinforcement learning. _Expert systems with applications_, 30(3):543-552, 2006.
* [17] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. _arXiv preprint arXiv:2304.05335_, 2023.
* [18] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. _arXiv preprint arXiv:2304.06767_, 2023.
* [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. _arXiv preprint arXiv:2407.21783_, 2024.
* [20] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with \(V\)-usable information. In _International Conference on Machine Learning_, pages 5988-6008. PMLR, 2022.
* [21] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with \(\mathcal{V}\)-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 5988-6008. PMLR, 17-23 Jul 2022.
* [22] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3356-3369, 2020.
* [23] Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman. Finding alignments between interpretable causal variables and distributed neural representations. In _Causal Learning and Reasoning_, pages 160-187. PMLR, 2024.
* [24] Nyoman Gunantara. A review of multi-objective optimization: Methods and its applications. _Cogent Engineering_, 5(1):1502-242, 2018.
* [25] Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, and Pulkit Agrawal. Value augmented sampling for language model alignment and personalization. _arXiv preprint arXiv:2405.06639_, 2024.
* [26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [27] James Y Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, and Dan Roth. Deal: Decoding-time alignment for large language models. _arXiv preprint arXiv:2402.06147_, 2024.
* [28] Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, and Sergey Levine. How to train your robot with deep reinforcement learning: lessons we have learned. _The International Journal of Robotics Research_, 40(4-5):698-721, 2021.
* [29] David Isele, Reza Rahimi, Akansel Cosgun, Kaushik Subramanian, and Kikuo Fujimura. Navigating occluded intersections with autonomous vehicles using deep reinforcement learning. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 2034-2039. IEEE, 2018.
* [30] Shirel Josef and Amir Degani. Deep reinforcement learning for safe local planning of a ground vehicle in unknown rough terrain. _IEEE Robotics and Automation Letters_, 5(4):6748-6755, 2020.
* [31] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Alignment as reward-guided search. In _The Twelfth International Conference on Learning Representations_, 2024.
* [32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] Songsang Koh, Bo Zhou, Hui Fang, Po Yang, Zaili Yang, Qiang Yang, Lin Guan, and Zhigang Ji. Real-time deep reinforcement learning based vehicle navigation. _Applied Soft Computing_, 96:106694, 2020.

* [34] Petar Kormushev, Sylvain Calinon, and Darwin G Caldwell. Reinforcement learning in robotics: Applications and real-world challenges. _Robotics_, 2(3):122-148, 2013.
* [35] Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. _Advances in Neural Information Processing Systems_, 36, 2023.
* [36] Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. _arXiv preprint arXiv:2312.01552_, 2023.
* [37] Derong Liu and Qinglai Wei. Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems. _IEEE Transactions on Neural Networks and Learning Systems_, 25(3):621-634, 2013.
* [38] Sheng Liu, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. _arXiv preprint arXiv:2311.06668_, 2023.
* [39] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. _arXiv preprint arXiv:2309.06657_, 2023.
* [40] Wenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Aligning large language models with human preferences through representation engineering. _arXiv preprint arXiv:2312.15997_, 2023.
* [41] Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. Finrl: Deep reinforcement learning framework to automate trading in quantitative finance. In _Proceedings of the second ACM international conference on AI in finance_, pages 1-9, 2021.
* [42] Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, and Bin Dong. Prompt engineering through the lens of optimal control. _arXiv preprint arXiv:2310.14201_, 2023.
* [43] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. _arXiv preprint arXiv:2310.17022_, 2023.
* [44] Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep learning perspective. In _The Twelfth International Conference on Learning Representations_, 2024.
* [45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* [46] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [47] Rex Clark Robinson. _An introduction to dynamical systems: continuous and discrete_, volume 19. American Mathematical Soc., 2012.
* [48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [49] Stefano Soatto, Paulo Tabuada, Pratik Chaudhari, and Tian Yu Liu. Taming ai bots: Controllability of neural states in large language models. _arXiv preprint arXiv:2305.18449_, 2023.
* [50] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. _arXiv preprint arXiv:2306.17492_, 2023.
* [51] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [52] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive framework for neural text generation. _Advances in Neural Information Processing Systems_, 2022.

* Subramani et al. [2022] Nishant Subramani, Nivedita Suresh, and Matthew E Peters. Extracting latent steering vectors from pretrained language models. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 566-581, 2022.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Todorov [2006] Emanuel Todorov. Optimal control theory. 2006.
* Togai and Yamano [1985] Masaki Togai and Osamu Yamano. Analysis and design of an optimal learning control scheme for industrial robots: A discrete system approach. In _1985 24th IEEE Conference on Decision and Control_, pages 1399-1404. IEEE, 1985.
* Tolani et al. [2021] Varun Tolani, Somil Bansal, Aleksandra Faust, and Claire Tomlin. Visual navigation among humans with optimal control as a supervisor. _IEEE Robotics and Automation Letters_, 6(2):2288-2295, 2021.
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Turner et al. [2023] Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. _arXiv preprint arXiv:2308.10248_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wang et al. [2019] Chao Wang, Jian Wang, Yuan Shen, and Xudong Zhang. Autonomous navigation of uavs in large-scale complex environments: A deep reinforcement learning approach. _IEEE Transactions on Vehicular Technology_, 68(3):2124-2136, 2019.
* Wang and Ueda [2022] Zhikang T. Wang and Masahito Ueda. Convergent and efficient deep q learning algorithm. In _International Conference on Learning Representations_, 2022.
* Wei et al. [2024] Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning and low-rank modifications. _arXiv preprint arXiv:2402.05162_, 2024.
* Wei et al. [2017] Qinglai Wei, Guang Shi, Ruizhuo Song, and Yu Liu. Adaptive dynamic programming-based optimal control scheme for energy storage systems with solar renewable energy. _IEEE Transactions on Industrial Electronics_, 64(7):5468-5478, 2017.
* Weidinger et al. [2021] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. _arXiv preprint arXiv:2112.04359_, 2021.
* Wu et al. [2024] Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine-tuning via representation editing. _arXiv preprint arXiv:2402.15179_, 2024.
* Wu et al. [2024] Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D Manning, and Christopher Potts. Reft: Representation finetuning for language models. _arXiv preprint arXiv:2404.03592_, 2024.
* Xu et al. [2024] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. _arXiv preprint arXiv:2401.08417_, 2024.
* Xu et al. [2023] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more crime than others: Preference optimization with the pairwise crime loss. _arXiv preprint arXiv:2312.16682_, 2023.
* Yuan et al. [2023] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. _arXiv preprint arXiv:2304.05302_, 2023.

* [72] Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. Defending large language models against jailbreaking attacks through goal prioritization. _arXiv preprint arXiv:2311.09096_, 2023.
* [73] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [74] Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In _International Conference on Machine Learning_, pages 43037-43067. PMLR, 2023.
* [75] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. _arXiv preprint arXiv:2310.01405_, 2023.

## Appendix A Limitations and Future Work

### Broader Impacts

* Experimental Details
* C.1 Computing Infrastructure
* C.2 HH-RLHF
* C.3 Standford SHP
* C.4 HarmfulQA
* D GPT-4 Evaluation

### Additional Qualitative Examples

###### Acknowledgements.

We discuss limitations and possible extensions of Re-Control. (1) _Injecting inductive bias into the control policy._ In our current work, we only train a value function on the last layer of the model's hidden space. However, we can follow the approach in [35], first training multiple value functions on all intermediate hidden layers and then selecting the layer that achieves the best accuracy on the validation set. Additionally, we can draw from the methods in [64, 23, 68] to perturb only a low-rank subspace of the representation space. (2) _Multi-objective alignment._ In the current paper, we consider the objective from a single reward model. However, in practice, alignment may involve multiple, potentially conflicting objectives. It would be interesting to leverage multi-objective optimization techniques [24] at test time to obtain a Pareto frontier in the representation space for such settings. (3) _More advanced training algorithm._ Currently, we train the value function using a simple one-iteration policy iteration method. It would be interesting to explore whether increasing the number of iterations could further improve the training of the value function. Additionally, we can consider using algorithms for training the value function that provide provable convergence [63].

## Appendix B Broader Impacts

Aligning large language models (LLMs) with human preferences is crucial. We expect that the test-time alignment method introduced in this paper will positively impact society by helping to prevent LLMs from generating harmful content. However, it is essential to ensure that the training of the value function does not involve negative goals. Care must be taken to prevent this misuse.

## Appendix C Experimental Details

### Computing Infrastructure

We conduct our experiments on a server equipped with NVIDIA A100 (80GB VRAM) GPUs. We utilize the NVIDIA CUDA toolkit version 12.4. All experiments are implemented using Python 3.12.2 and the PyTorch framework version 2.2.2.

###### Acknowledgements.

Hh-RLHF

We evaluate our method on the HH-RLHF [5] dataset, which is the most widely used dataset for LLM alignment. This dataset is used to improve the AI assistant's helpfulness and harmlessness, comprising 161,000 training samples and 8,550 test samples. Each sample contains a prompt and two responses with one preferred over another. For the base model, we adopt Vicuna-7B4[12] and Falcon-7B-Instruct5[3] as the instructed fine-tuned AI assistant. We evaluate these models bygenerating text responses based on test prompts from of HH-RLHF. Following the standard practice, we limit the maximum lengths of the prompt and generated continuation to \(2,048\) and \(128\) tokens, respectively.

For the reward model, we use a publicly available one that employs LLaMA-7B6 as the backbone, trained on HH-RLHF using the pairwise reward loss [45].

Footnote 6: https://huggingface.co/argsearch/llama-7b-rm-float32

Re-Control.When constructing the training dataset for the value function, we sample only one response for each training prompt of HH-RLHF, i.e., \(M=1\). For both Vicuna-7B and Falcon-7B, we train the value network on the last layer of the hidden states \(o_{t}\), and at test time, we add control signals only to this layer. For future studies, we can also explore adding controls to the attention key-value pairs \(h_{t}\) which should further improve the performance.

For Vicuna-7B, the value function is a three-layer network with a hidden dimension of 4096. For Falcon-7B, the value function is a two-layer network with a hidden dimension of 4096.

To train the value function of Re-Control, we adopt the Adam optimizer [32]. The training hyperparameters of the value networks are summarized in Table 3.

We randomly sample 1000 data points from the training set of HH-RLHF as a separate validation set. The step size \(\alpha\) and number of updates \(n\) are selected on the validation set to maximize the sum of coherence, diversity, and average reward. The inference parameters are summarized in Table 4.

\begin{table}
\begin{tabular}{l l l} \hline \hline Backbone & Parameters & Value \\ \hline \multirow{6}{*}{Vicuna-7B} & Step size & 0.5 \\  & Number of updates & 30 \\  & batch size & 30 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Maximum lengths of the prompt & 2048 \\ \cline{1-1}  & Maximum lengths of generated continuation & 128 \\ \hline \multirow{6}{*}{Falcon-7B} & Step size & 0.2 \\ \cline{1-1}  & Number of updates & 200 \\ \cline{1-1}  & batch size & 60 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Maximum lengths of the prompt & 2048 \\ \cline{1-1}  & Maximum lengths of generated continuation & 128 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Summary of hyperparameters of Re-Control at test time on HH-RLHF.

\begin{table}
\begin{tabular}{l l l} \hline \hline Backbone & Parameters & Value \\ \hline \multirow{6}{*}{Vicuna-7B} & Number of epochs & 100 \\  & Learning rate & \(1*10^{-4}\) \\  & batch size & 512 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Number of Layers & 3 \\ \cline{1-1}  & Hidden Dimension & 4096 \\ \hline \multirow{6}{*}{Falcon-7B} & Number of epochs & 100 \\ \cline{1-1}  & Learning rate & \(1*10^{-4}\) \\ \cline{1-1}  & batch size & 512 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Number of Layers & 2 \\ \cline{1-1}  & Hidden Dimension & 4096 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Summary of the hyperparameters used in training the value function of Re-Control on HH-RLHF.

Prompting engineering.We instruct the model to provide responses that are more helpful and harmless. The prompt template is as follows:

"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions." + Original prompt

Static representation editing.We first train a linear regression layer on the hidden state of a large language model (LLM) after feeding the prompt, to predict the expected reward as in [35]. For a fair comparison, we use the same hidden state layer as Re-Control. At test time, we shift the activation space along the direction of the weights using an intervention strength parameter \(\alpha\), which is selected based on the validation set. The hyperparameters used during the training and testing stages are summarized in Table 5.

Controlled Decoding.We use the codebase7 from [31]. We employ the default hyperparameters suggested in the paper and repository. The number of candidates to rank with the reward model is set to 10, and the weight controlling the tradeoff between the LLM text objective and the reward is 1. For controlled decoding with the value function, we stack the value function of Re-Control on top of the hidden state of the LLM as the prefix scorer, ensuring a fair comparison with our method.

Footnote 7: https://github.com/deeplearning-wisc/args

Training configurations for PPOFor experiments involving Proximal Policy Optimization (PPO), we use the Transformer Reinforcement Learning (TRL) repository from Huggingface, along with the PPO Trainer module. The configuration values are detailed in Table 6.

Training configurations for DPOFor experiments involving Direct Policy Optimization (DPO), we use the Transformer Reinforcement Learning (TRL) repository from Huggingface, along with the DPO Trainer module. The configuration values are detailed in Table 7.

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Parameters & Value \\ \hline  & Max number of PPO update steps & 10000 \\  & Generation batch & 1 \\  & PPO batch size & 16 \\  & PPO minibatch size & 8 \\  & Lora rank & 8 \\ Vicuna-7B & Learning rate & \(1.4*10^{-5}\) \\  & Batch size & 4 \\  & Gradient accumulation steps & 2 \\  & Input maximum length & 512 \\  & Output maximum length & 256 \\  & Weight decay & 0.001 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Summary of training hyperparameters for proximal policy optimization (PPO)

\begin{table}
\begin{tabular}{l l l} \hline \hline Backbone & Parameters & Value \\ \hline  & Number of epochs & 100 \\  & Learning rate & \(1*10^{-4}\) \\ Vicuna-7B & Training batch size & 512 \\  & Testing batch size & 30 \\  & Intervention strength & 2.5 \\ \hline  & Number of epochs & 100 \\  & Learning rate & \(1*10^{-3}\) \\ Falcon-7B & Training batch size & 512 \\  & Testing batch size & 60 \\  & Intervention strength & 2.0 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Summary of hyperparameters of static representation editing on HH-RLHF

### Standford SHP

To further evaluate our method, we utilized the Standford SHP (SHP) [20] dataset. This dataset comprises 385,000 collective human preferences across diverse subject areas, ranging from cooking to legal advice. Each data point consists of a Reddit post with a question or instruction, and two top-level comments, one of which has been rated as more helpful by Reddit users. The dataset is divided into 349,000 training samples, 18,400 validation samples, and 18,400 test samples, enabling robust evaluation of our approach. For the base model, we adopt Vicuna-7B8[12] and Llama3-8B-Instruct9[2] as the instructed fine-tuned AI assistant. We evaluate these models by generating text responses based on 1,000 random sampled test prompts from of SHP. Following the standard practice, we limit the maximum lengths of the prompt and generated continuation to \(2,048\) and \(128\) tokens, respectively.

Footnote 8: https://huggingface.co/lmsys/vicuna-7b-v1.5

Footnote 9: https://huggingface.co/meta-llama/Meta-llama-3-8B-Instruct

For the reward model, we use UltraRM-13B1011, which is trained on Anthropic HH-RLHF, Standford SHP, and Summarization.

Footnote 10: https://huggingface.co/openbmb/UltraRM-13b

Re-Control.When constructing the training dataset for the value function, we sample only one response for each training prompt of Standford SHP, i.e., \(M=1\). For both Vicuna-7B and Llama3-8B-Instruct, we train the value network on the last layer of the hidden states \(o_{t}\), and at test time, we add control signals only to this layer. For future studies, we can also explore adding controls to the attention key-value pairs \(h_{t}\) which should further improve the performance.

For both Vicuna-7B and Llama3-8B-Instruct, the value function is a two-layer network with a hidden dimension of 4096.

To train the value function of Re-Control, we adopt the Adam optimizer [32]. The training hyperparameters of the value networks are summarized in Table 8.

The step size \(\alpha\) and number of updates \(n\) are selected on the validation set to maximize the sum of coherence, diversity, and average reward. The inference parameters are summarized in Table 9.

Prompting engineering.We instruct the model to provide responses that are more helpful and harmless. The prompt template is as follows:

"A question from a curious user and an answer from an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions." + Original prompt

Static representation editing.We first train a linear regression layer on the hidden state of a large language model (LLM) after feeding the prompt, to predict the expected reward as in [35]. For a fair comparison, we use the same hidden state layer as Re-Control. At test time, we shift the

\begin{table}
\begin{tabular}{l l l} \hline \hline  & Parameters & Value \\ \hline  & Max number of training steps & 10000 \\  & Learning rate & \(10^{-6}\) \\  & Lora rank & 8 \\  & Warmup steps & 100 \\ Vicuna-7B & Batch size & 4 \\  & Gradient accumulation steps & 4 \\  & Maximum sequence length & 1024 \\  & Weight decay & 0.05 \\  & Regularization parameter \(\beta\) & 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Summary of training hyperparameters for Direct Policy Optimization (DPO)activation space along the direction of the weights using an intervention strength parameter \(\alpha\), which is selected based on the validation set. The hyperparameters used during the training and testing stages are summarized in Table 10.

Controlled Decoding.We use the codebase12 from [31]. We employ the default hyperparameters suggested in the paper and repository. The number of candidates to rank with the reward model is set to 10, and the weight controlling the tradeoff between the LLM text objective and the reward is 1. For controlled decoding with the value function, we stack the value function of Re-Control on top of the hidden state of the LLM as the prefix scorer, ensuring a fair comparison with our method.

Footnote 12: https://github.com/deeplearning-wisc/args

### HarmfulQA

This dataset13 contains 1,960 harmful questions specifically designed to evaluate the performance of language models. Additionally, it includes a conversation set comprising 9,536 harmless conversations and 7,356 harmful conversations for model alignment purposes. For our experiments, we focused solely on the evaluation portion of the HarmfulQA dataset to test the performance of our method. We use the value function trained on HH-RLHF to evaluate the out-of-distribution performance.

\begin{table}
\begin{tabular}{l l l} \hline \hline Backbone & Parameters & Value \\ \hline \multirow{6}{*}{Vicuna-7B} & Number of epochs & 100 \\  & Learning rate & \(1*10^{-4}\) \\  & batch size & 512 \\  & Floating point format & fp16 (Half-precision) \\  & Number of Layers & 2 \\  & Hidden Dimension & 4096 \\ \hline \multirow{6}{*}{Llama3-8B} & Number of epochs & 100 \\  & Learning rate & \(1*10^{-4}\) \\  & batch size & 512 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Number of Layers & 2 \\ \cline{1-1}  & Hidden Dimension & 4096 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Summary of the hyperparameters used in training the value function of Re-Control on SHP.

\begin{table}
\begin{tabular}{l l l} \hline \hline Backbone & Parameters & Value \\ \hline \multirow{6}{*}{Vicuna-7B} & Step size & 1.0 \\  & Number of updates & 50 \\  & batch size & 32 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Maximum lengths of the prompt & 2048 \\ \cline{1-1}  & Maximum lengths of generated continuation & 128 \\ \hline \multirow{6}{*}{Llama3-8B} & Step size & 1.0 \\ \cline{1-1}  & Number of updates & 30 \\ \cline{1-1}  & batch size & 32 \\ \cline{1-1}  & Floating point format & fp16 (Half-precision) \\ \cline{1-1}  & Maximum lengths of the prompt & 2048 \\ \cline{1-1}  & Maximum lengths of generated continuation & 128 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Summary of hyperparameters of Re-Control at test time on SHP.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose a new alignment method for large language models from a control perspective. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations of the work in Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical results. Guidelines:* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all the experimental details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code is available at https://github.com/Lingkai-Kong/RE-Control. Links to all the datasets are provided in the paper. Guidelines:* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the experimental details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Since all the experiments are based on large language models. It would be too computationally expensive to provide the error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All the computing infrastructure information has been provided in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conform with the NeurIPS code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed both potential positive societal impacts and negative societal impacts in Appendix B. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited all code, data, and models we used in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.