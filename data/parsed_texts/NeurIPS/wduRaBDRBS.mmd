# Video Token Merging

for Long-form Video Understanding

Seon-Ho Lee

Korea University

seonholee@mcl.korea.ac.kr

&Jue Wang

Amazon AGI

juewangn@amazon.com

Zhikang Zhang

Amazon AGI

zhikang@amazon.com

&David Fan

Meta FAIR

davidfan@meta.com

&Xinyu Li

Amazon AGI

xxnl@amazon.com

Work done during an internship at Amazon Prime Video.Work done while at Amazon Prime Video.

###### Abstract

As the scale of data and models for video understanding rapidly expand, handling long-form video input in transformer-based models presents a practical challenge. Rather than resorting to input sampling or token dropping, which may result in information loss, token merging shows promising results when used in collaboration with transformers. However, the application of token merging for long-form video processing is not trivial. We begin with the premise that token merging should not rely solely on the similarity of video tokens; the saliency of tokens should also be considered. To address this, we explore various video token merging strategies for long-form video classification, starting with a simple extension of image token merging, moving to region-concentrated merging, and finally proposing a learnable video token merging (VTM) algorithm that dynamically merges tokens based on their saliency. Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets. Moreover, our approach significantly reduces memory costs by **84%** and boosts throughput by approximately **6.89** times compared to baseline algorithms.

## 1 Introduction

Over the past few years, the Transformer architecture (Vaswani et al., 2017) has risen as a revolutionary paradigm within natural language processing (NLP) (Devlin et al., 2018) and has seamlessly expanded its influence into the domain of computer vision (Wang and Torresani, 2022; Bertsius et al., 2021; Wang et al., 2022; Akbari et al., 2021; Li et al., 2022; Fan et al., 2021). This expansion has been exemplified by remarkable achievements in recent multi-modality foundation models such as Sora (Brooks et al., 2024), GPT4 (Achiam et al., 2023), and Gemini (gem), showcasing its exceptional performance and versatility across diverse applications.

In contrast to the natural language processing, the visual input has much lower information density and thus tokenizing the raw RGB image as non-overlapped patches becomes the essential operation in vision transformers (Wang and Torresani, 2022; Bertsius et al., 2021; Dosovitskiy et al., 2021). However, the computational cost of transformer exponentially increases with the sequence length, which generates tremendous computation when feeding visual input into the large-scaled transformer models with billions of parameters. Due to the redundancy in video sequence, this phenomenon becomes more severe with video input, especially for long-form videos. This bottleneck impedes thefurther advancement of foundational models in handling long-form video data. Various attempts (Yin et al., 2021; Wang et al., 2021; Meng et al., 2022; Rao et al., 2021; Liang et al., 2022) have been proposed to improve the efficiency of vision transformer by introducing a token selection module. However, these methods are primarily designed for images and may require non-trivial adaptation to the long-form video scenarios due to the video-level long-term dependencies and motion dynamics. Moreover, tokens dropped by the token selection module cannot be reused in later layers, which may result in the loss of important information.

In addition to the token selection, token merging techniques (Bolya et al., 2022; Ren et al., 2023; Bolya and Hoffman, 2023; Li et al., 2024) have been proposed to increase the efficiency and effectiveness of transformer-based networks. Specifically, they reduce the sequence length by merging similar tokens, thereby decreasing the computational cost. In addition to the efficiency, token merging demonstrates huge advantages by increasing the contextual information so that the model can learn from patterns presented across multiple tokens. Previous token merging algorithms in both the image and video domains use manually designed token partitioning methods and merge tokens based on their similarity. Even though the merged tokens would still keep the original information, they may have different granularity after the merging operation. In this paper, it is argued that different regions in the visual data may have different information density. Since the discriminative information of tokens may be degenerated after merging, some visual tokens should not be merged even if they look similar to each other. Rather than relying solely on similarity, we question whether more unmerged tokens should be used to describe salient areas, while merging more tokens for the background.

In this paper, we explore various video token merging (VTM) methods in long-form video classification task and aim to find the effective token merging method for long-form videos. Previous video token merging method (Li et al., 2024) only decouples the spatial and temporal dimensions, which is unfavorable, especially for long-form videos. As the long-term dependency plays an important role in the long-form video understanding, spatiotemporal visual tokens should be considered jointly. Sequentially merging token along with one dimension after another may generate biased prior. In our work, we first naively extend the image-based token merging (Bolya et al., 2022) to the video domain and then propose region-centralized and motion-based token merging algorithms, which estimate the salient region of video sequences. Finally, we develop a learnable VTM which predicts the saliency score of each token and adaptively merges spatiotemporal visual tokens in data-driven manner. Experimental results demonstrate that the proposed algorithm improves the effectiveness and the efficiency of the transformer-based network and outperforms the conventional long-video understanding methods with better throughput and less memory usage, as also shown in Figure 1.

We summarize the contributions of this paper as following:

* We explore various video token merging methods including the naive VTM, the region-concentrated VTM, and the motion-based VTM.
* We propose the learnable video token merging algorithm, which estimates the saliency score of each token and adaptively merges visual tokens based on their scores.
* The proposed algorithm achieves the best or competitive results on various datasets including LVU, Breakfast and COIN. Moreover, we significantly reduce memory costs by 84% and improve the throughput by 6.89 times via the proposed learnable VTM.

Figure 1: Comparison of GPU memory footprint and throughput against scene prediction accuracy on the LVU dataset (Wu and Krahenbuhl, 2021).

Related Work

### Long-form Video Modeling

Transformers have demonstrated remarkable progress in capturing long-term dependencies, as evidenced in their success in natural language processing (NLP) tasks (Brown et al., 2020; Dai et al., 2019; Devlin et al., 2018). However, the intensive computational requirements stemming from dense self-attention calculations (Vaswani et al., 2017) pose a significant obstacle not only in NLP but also in the domain of computer vision, especially for the long-form videos. Many recent video transformer works (Wang and Torresani, 2022; Liu et al., 2021; Bertasius et al., 2021) focuses on improving the global attention mechanism. However, they are not designed for dealing with redundant spatial and temporal image tokens that are common in long-form video scenarios. To capture longer temporal information, LF-VILA (Sun et al., 2022) develops a hierarchical architecture to include more frames in the model. Similarly, McMViT (Wu et al., 2022) utilizes longer temporal information by emerging the previously cached "memory" from the past. A novel alternative to transformers is the Structured State-Space Sequence (S4) model proposed by Gu et al. (2021), which models the long-range dependencies by simulating a linear time invariant (LTI) system. Subsequently, ViS4mer (Islam and Bertaisu, 2022) and S5 (Wang et al., 2023) extend S4 model to the long-form video classification task. ViS4mer (Islam and Bertaisu, 2022) stacks multiple S4 layers with different scales in modeling long-form videos, and S5 (Wang et al., 2023) include an additional selective module to further improve the performance. Unlike these works that focus on the improvement of architecture and attention mechanism, this paper will start from a more basic concept in the transformer, video tokens and how to effectively merge them. Even though our proposed method can theoretically be applied on S4 model, the scope of this paper is on the well established transformer architecture. We will leave the investigation of video token merging on S4 (Gu et al., 2021) in the future work.

### Adaptive Token Selection

Adaptive token selection is widely used to improve model efficiency by leveraging a light-weight selection module to pick up the 'useful' tokens while dropping the 'unuseful' ones. In vision transformer, STTS (Wang et al., 2021) utilizes a token selection module known as the named scorer network to assign importance scores to each token, subsequently selecting the top-K frames with the highest scores. Building upon this concept, AdaViT (Meng et al., 2022) further extends the approach by developing instance-specific policies. These policies guide the activation of patches, self-attention heads, and transformer blocks, enhancing adaptability and efficiency in processing visual data. STTS, AdaVit and other similar approaches (Wang et al., 2021; Meng et al., 2022; Rao et al., 2021; Liang et al., 2022) drop a significant number of tokens in the early decision stage to save more cost, but the dropped tokens cannot be reused in the later layers, which is easier to degenerate the contextual information in the long-form videos.

### Token Merging

Visual token merging is first proposed in (Bolya et al., 2022) which aims at increasing the throughput of existing ViT models without training. Following works (Ren et al., 2023; Bolya and Hoffman, 2023; Li et al., 2024) leverage this idea to save computational cost in different downstream applications, such as diffusion model, video and language understanding, and video editing. Specifically, visual tokens are first partitioned into two sets with equal size; for each of the edge tokens in one set, find the most similar token in another set and merge them by average pooling; finally, concatenate two sets back together. Although the token merging is simple and effective, its applications have mostly remained in the image domain. There is no fundamental research work has been explored for the long-form video token merging strategies, where the spatiotemporal tokens are more redundant and embed complicated dependencies locally and globally. In this work, we argue that visual tokens from the long-form video should be carefully partitioned and merged based on the salient areas in videos. To this end, we ablate various video token merging algorithms and provide extensive experimental results and analysis.

## 3 Proposed Algorithm

### Preliminary - Token Merging

Token merging (Bolya et al., 2022) aims to reduce the redundancy by merging similar tokens at each transformer block, thereby increasing the effectiveness and efficiency of a transformer-based network. Specifically, token merging has three steps: partitioning, matching, and merging.

**Partitioning:** For given a set of \(N\) tokens \(\mathcal{X}=\{x_{1},x_{2},\ldots,x_{N}\}\), token merging first partition \(\mathcal{X}\) into a set of target tokens \(\mathcal{T}\) and a set of source tokens \(\mathcal{S}\), given by

\[\mathcal{T} = \{x_{i}:i\bmod\gamma=0\},\] (1) \[\mathcal{S} = \{x_{j}:j\bmod\gamma\neq 0\}\] (2)

where \(\gamma\) is partition factor and \(\bmod\) denotes the modulo operator. Thus, \(|\mathcal{T}|=\frac{|\mathcal{X}|}{\gamma}\). Also, \(\mathcal{X}=\mathcal{T}\cup\mathcal{S}\) and \(\mathcal{T}\cap\mathcal{S}=\varnothing\).

**Matching:** Then, for each source token in \(\mathcal{S}\), it finds the most similar target token in \(\mathcal{T}\). Here, the similarity between tokens \(x_{i}\) and \(x_{j}\) is defined as the cosine similarity of the corresponding key vectors \(k_{i}\) and \(k_{j}\), which are obtained in the most recent self-attention layer. For a source token \(x_{j}\in\mathcal{S}\), the index of its matched target token \(m_{j}\) is obtained by,

\[m_{j}=\operatorname*{argmax}_{i:\{ix_{i}\in\mathcal{T}\}}\frac{k_{i}^{t}k_{j} }{|k_{i}||k_{j}|}.\] (3)

**Mergging:** Lastly, token merging merges the tokens based on the matching results. For each target token \(x_{i}\in\mathcal{T}\), it obtains the merged token \(y_{i}\) by using average pooling,

\[y_{i}=\frac{x_{i}+\sum_{j\in\mathcal{M}_{i}}x_{j}}{1+|\mathcal{M}_{i}|}\] (4)

where \(\mathcal{M}_{i}=\{j:m_{j}=i,\forall x_{j}\in\mathcal{S}\}\) is the index set of source tokens which are matched with \(x_{i}\). In this case, the number of tokens is reduced by \(|\mathcal{S}|\) after token merging. It can also control the number of reduced tokens by \(R\) by reassigning the matching index as

\[m_{j}=-1\] (5)

for all \(x_{j}\) except for the source tokens with the \(R\) highest similarity scores.

### Problem Definition

Suppose that a video with \(L\) frames is given, where \(L\geq 60\). To perform the classification or regression on the given video, we can use a simple transformer-based network, which is shown in Figure 2 (a) and (b). It first obtains the tokens \(X_{1},X_{2},\ldots,X_{L}\in\mathbb{R}^{H\times W\times C}\) by using an encoder. Here, \(H,W,\) and \(C\) denote the height, the width, and the channel dimension of the token tensor, respectively. Then, it utilizes transformer blocks to update the tokens. As its input, \(i\)-th transformer block takes the tokens corresponding to \(L_{i}\) frames without overlapping, where \(L_{i}\leq L_{j}\leq L\) for \(i<j\). The prediction head yields the estimation results. However, it requires the prohibitively large memory and computation costs due to the quadratic complexity of the self-attention \(\mathcal{O}(L^{2}H^{2}W^{2}D^{2})\), where \(D\) is the dimension of key vectors. Hence, our goal is to increase the efficiency of this baseline network by reducing the number of tokens via token merging, while maintaining or even improving the performances of the network by removing the redundant or noisy information in the video. To this end, we explore various token merging methods for long video processing.

### Video Token Merging - Exploration

**Naive video token merging:** First, we combine the standard token merging with the baseline network as intact as possible. To this end, we substitute transformer blocks with VTM blocks, in which token merging layer is inserted after the output layer, as illustrated in Figure 2 (c). In this naive VTM, we employ the standard token merging in (1)-(4). At each \(i\)-th VTM block, naive VTM reduces the \(R_{i}\) tokens. For example, at \(\gamma=4\) and \(R=|\mathcal{S}|\), it gradually removes the \(68\%\) of tokens over the network. Hence, the computation cost of the self-attention is reduced from \(\mathcal{O}(N^{2}D^{2})\) to \(\mathcal{O}((\frac{N}{\gamma^{i-1}})^{2}D^{2})\) at \(i\)-th VTM block. As shown in Table 1, this naive VTM shows better scores than the baseline network, because the token merging reduces the information redundancy in the videos.

**Region-concentrated video token merging:** Compared to an image, a video contains redundant spatiotemporal tokens. Depending on the tasks, some tokens are more important than others. However, as shown in Figure 3 (a), naive VTM selects every \(\gamma\)-th token as the target tokens since it exploits uniform token partitioning in (1). Also, for token merging, it purely relies on the similarity between tokens regardless of the semantics, and thus the self-attention can more easily swayed by unnecessaryinformation. Therefore, for better video token merging, it is important to consider the saliency of each token before merging them.

To investigate this issue, we explore center-concentrated video token merging and boundary-concentrated video token merging. The center-concentrated token merging samples \(50\%\) of the entire target tokens from the center area with the size of \(\frac{H}{2}\times\frac{W}{2}\), which uses more unmerged tokens to describe center area and merge more token from the boundary. Specifically, we use the partition factor of \(\frac{7}{2}\) for the center area and \(\frac{3}{2}\gamma\) for the remaining area. On the other side, we implement the opposite operation for the boundary-concentrated video token merging. As shown in Table 1, center-concentrated VTM shows better performances than naive and boundary-concentrated VTM in general. Since meaningful objects and motions are typically center-concentrated, this suggests more tokens from salient regions should be unmerged while more of the rest tokens should be merged. Figure 3 (b) shows the partitioning results of center-concentrated VTM.

**Motion-based video token merging:** Even though center-concentrated VTM has shown better performances than naive VTM, the meaningful tokens are not always located in the center area. Moreover, the hand-crafted partitioning method forces the center-concentrated VTM to select the same number of target tokens from each frame, which is not flexible enough when applied at scale. Therefore, we explore motion-based video token merging which divides tokens into \(\mathcal{T}\) and \(\mathcal{S}\) based on the motion information since the moving objects carry important cues in general (Fan et al., 2023).

Let us assume that we have the magnitude of motion vector \(v_{i}\) for each token \(x_{i}\). We compute the sampling probability by using softmax

\[p_{i}=\frac{e^{v_{i}}}{\sum_{j=1}^{N}e^{v_{j}}}.\] (6)

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Content (\(\uparrow\))} & \multicolumn{3}{c}{Meta data (\(\uparrow\))} & \multicolumn{3}{c}{User (\(\downarrow\))} \\ \cline{2-10} Algorithm & Relationship & Speaking & Scene & Director & Genre & Writer & Year & Like & View \\ \hline Baseline & 57.14 & 36.68 & 69.76 & 62.61 & 56.73 & 49.40 & 39.86 & 0.28 & 4.18 \\ Naïve & 61.90 & 36.18 & 72.09 & 67.28 & 55.12 & 51.19 & 44.75 & 0.28 & **4.01** \\ Boundary & 59.52 & 37.18 & 69.76 & 61.68 & 57.21 & 50.0 & 47.55 & 0.26 & 4.16 \\ Center & 61.90 & 40.20 & 74.41 & 62.61 & 58.81 & 51.19 & 44.05 & 0.25 & 4.11 \\ Motion & **64.28** & 37.68 & 74.41 & 64.48 & 58.49 & **55.95** & 47.55 & 0.24 & 4.13 \\ Learnable & **64.28** & **42.12** & **75.58** & **70.09** & **59.77** & 53.57 & **48.55** & **0.21** & **4.01** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of different VTM methods on the LVU dataset. The best results are boldfaced and the second-best ones are underlined.

Figure 2: The architectures of (a) the baseline network, (b) the transformer block, and (c) the video token merging block.

Note that the sampling probability is proportional to the motion magnitude. Then, we construct \(\mathcal{T}\) by sampling \(\frac{N}{\gamma}\) tokens from \(\mathcal{X}\) with the sampling probability \(p_{i}\) for each token \(x_{i}\).

The goal of VTM is to increase the efficiency of transformer-based network for long video understanding. Therefore, the motion information should be obtained with negligible time and computation costs. Hence, instead of estimating the motion information with an additional module, we use the motion information which is already stored in the video files; most modern video codecs, such as MPEG-4 (Richardson, 2004), H.264 (Richardson, 2004), and HEVC (Wien, 2015), exploit the motion information for efficient compression. The motion decoding takes only 0.3 milliseconds for each frame which is negligible. Figure 3 (c) shows the token partitioning examples of motion-based VTM.

### Learnable Video Token Merging

There are some videos in which unimportant objects have large motions due to various factors such as camera movement. Motion-based VTM may not yield good results on those videos. To maximize the generalizability, we develop learnable video token merging method. Instead of depending on the motion information, learnable VTM estimates the saliency score of each token and samples the target tokens based on the estimated scores. Figure 4 shows the architecture of learnable VTM block.

Learnable VTM block contains two forward paths: a main path and an auxiliary path. Let us assume that we have a tensor of \(N\) tokens \(X\in\mathbb{R}^{N\times C}\). In the main path, we first obtain query \(Q\), key \(K\), and value \(V\) by

\[Q=XU_{q},\qquad K=XU_{k},\qquad V=XU_{v}\] (7)

using learnable projection matrices \(U_{q},U_{k},U_{v}\in\mathbb{R}^{C\times D}\). We perform the standard self-attention on \(Q,K,\) and \(V\) and yield the updated tokens \(X^{\prime}\) as

\[X^{\prime}=\frac{\mathrm{softmax}(QK^{\top})}{\sqrt{D}}V.\] (8)

Also, from \(K\), we estimate the saliency scores \(S\) of tokens by

\[S=[s_{1},s_{2},\dots,s_{N}]^{\top}=\tanh(KU_{s})\] (9)

where \(U_{s}\in\mathbb{R}^{D\times 1}\) is a learnable matrix. Also, \(s_{i}\in(-1,1)\) for \(1\leq i\leq N\). Then, we compute the sampling probability using (6) with \(s_{i}\) instead of \(v_{i}\) for each token \(x_{i}\) and sample \(T\) with the sampling probability as in motion-based VTM. After the token partitioning, we match and merge the tokens in \(X^{\prime}\) by using (3) and (4), respectively.

However, the learnable matrix \(U_{s}\) in (9) can not be trained only with the main path since the partitioning process is not differentiable. To handle this issue, we employ the auxiliary path. This path consists of a saliency guided self-attention layer and a merging operation. The auxiliary path takes a tensor of auxiliary tokens \(X_{\mathrm{aux}}\in\mathbb{R}^{N\times C}\) as its input. Similar to the main path, we obtain query \(Q_{\mathrm{aux}}\), key \(K_{\mathrm{aux}}\), and value \(V_{\mathrm{aux}}\). Then, we perform the saliency guided attention to obtain the updated auxiliary tokens \(X^{\prime}_{\mathrm{aux}}\) as

\[X^{\prime}_{\mathrm{aux}}=\frac{\mathrm{softmax}(Q_{\mathrm{aux}}K^{\top}_{ \mathrm{aux}}+\mathbf{1}S^{\top})}{\sqrt{C}}V_{\mathrm{aux}}\] (10)

Figure 3: Visualizations of target tokens of different VTM methods: (a) naive VTM, (b) center-concentrated VTM, (c) motion-based VTM, and (d) learnable VTM. In (d), learnable VTM selects the target tokens around salient objects rather than backgrounds.

where \(\mathbf{1}\) is a \(N\) dimensional vector of ones. In the saliency guided attention, the contribution of each token is controlled by its estimated saliency score; if \(s_{i}\) is positive, \(i\)-th token affects more on \(X^{\prime}_{\text{aux}}\), whereas if \(s_{i}\) is negative it has less influence on \(X^{\prime}_{\text{aux}}\). In other words, it increases the influences of the tokens with high saliency scores in the attention process. Therefore, during training, the network is encouraged to assign high saliency scores to the tokens with meaningful information and low saliency scores to the others to obtain the better predictions. At the first VTM block, the auxiliary path employs the same input with the main path. From the second VTM block, it takes the output of auxiliary path in the previous VTM block as its input.

Also, it is worth pointing out that the auxiliary path is used for the network training only. Compared to other VTM methods, learnable VTM only introduces additional computation of score estimation module, which is fast and light enough, during test. Therefore, it shows almost same inference speed with other VTM methods.

## 4 Experiments

### Datasets

**LVU (Wu & Krahenbuhl, 2021):** It contains \(\sim\)30K videos sampled from \(\sim\)3K movies on the MovieClips (mov) website. Most videos are 1 to 3 minutes long. It provides the labels for 9 long-video understanding tasks which can be grouped into three major categories:

* Content understanding:'relationship,''speaking style,' and'scene/place'
* Metadata prediction: 'director,' 'genre,' 'writer,' and'movie release year'
* User engagement: 'YouTube like ratio,' and 'YouTube popularity'

As the evaluation metrics, we adopt the top 1 classification accuracy for content understanding and metadata prediction tasks and mean-squared error (MSE) for user engagement tasks.

**Breakfast (Kuehne et al., 2014):** It provides 1,712 videos with the average length of 2.32 minutes and the total length of 77 hours. The videos contain 52 individuals and 18 different backgrounds in total. Each video belongs to one of 10 complex cooking activities.

**COIN (Tang et al., 2019):** It consists of 11,827 videos with the average length of 2.36 minutes, collected from YouTube. Each video belongs to one of 180 distinct procedural tasks.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Context (\(\uparrow\))} & \multicolumn{3}{c}{Meta data (\(\uparrow\))} & \multicolumn{3}{c}{User (\(\downarrow\))} \\ \cline{2-10} Algorithm & Relationship & Speaking & Scene & Director & Genre & Writer & Year & Like & View & GPU & Throughput \\ \hline Obj. Tanner (Wu \& Krahenbuhl, 2021) & 54.76 & 33.37 & 52.94 & 47.65 & 52.74 & 36.30 & 37.76 & 0.30 & 3.68 & - & - \\ VideoBERT (Sun et al., 2019) & 52.80 & 37.90 & 54.90 & 47.30 & 51.90 & 38.50 & 36.30 & 3.24 & 4.46 & - \\ Performance Choudias et al. (2021) & 50.00 & 38.80 & 60.46 & 58.87 & 49.45 & 48.21 & 41.25 & 0.31 & 3.93 & 5.936N & - \\ Orthoerferfer (Prieder et al., 2021) & 50.00 & 38.30 & 66.27 & 55.14 & 55.79 & 47.02 & 43.35 & 40.29 & 3.86 & 5.956N & - \\ IST (Jham \& Brussels, 2022) & 52.38 & 37.31 & 62.79 & 56.67 & 52.70 & 42.46 & 39.36 & 0.31 & 3.83 & 41.356N & - \\ Visualizer (Jham et al. Brussels, 2022) & 57.14 & 40.79 & 67.44 & 62.61 & 54.71 & 48.80 & 44.75 & 40.26 & 3.63 & 4.156N & 25.64 \\ SS (Wang et al., 2023) & 61.98 & 41.75 & 69.88 & 66.50 & 58.50 & 56.00 & 47.07 & 0.25 & **3.51** & 3.850B & 25.0 \\ SS4. SSLSCL (Wang et al., 2023) & 61.98 & 41.75 & 72.53 & 66.40 & **61.34** & 50.60 & 47.70 & 0.24 & **3.51** & 3.850B & 25.0 \\ Learnable VTM & **64.28** & **42.12** & **75.58** & **70.09** & 59.77 & **53.57** & **48.58** & **0.21** & 4.01 & 1.600B & **44.94** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of the proposed learnable VTM algorithm with conventional algorithms on the LVU dataset.

Figure 4: An overview of the learnable video token merging block. The auxiliary path is used during training only.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

**Analysis on \((L_{1},L_{2},L_{3})\):** Table 6 shows the results of the proposed learnable VTM with different \((L_{1},L_{2},L_{3})\). Note that \(i\)-th VTM block takes the tokens in \(L_{i}\) consecutive frames as its input at a time. At \((L_{1},L_{2},L_{3})=(4,20,60)\), the proposed algorithm yields the lowest scores, since it can not capture the long temporal dependency in the early stage of the network. We see that the proposed algorithm yields the best scores at \((L_{1},L_{2},L_{3})=(6,30,60)\).

**Weighted average pooling:** To merge tokens, we use average pooling as the default setting in all VTM methods. However, once tokens are merged, they may represent more than one input patch. Thus, to reflect the token size in merging process, we combine tokens by averaging weighted by their sizes. However, as shown in Table 7, this weighted average pooling decrease the performances of learnable VTM. Thus, we exploit the average pooling to merge tokens.

**Motion weighted average pooling:** In motion-based VTM, we can combine tokens by averaging weighted by their motion magnitudes. Table 7 shows the performances of motion-based VTM with the motion weighted average pooling. It yields the similar scores with the standard motion-based VTM with the average pooling.

**Complexity:** Table 8 compares the throughput and memory footprint of learnable VTM during training and inference. Since the auxiliary path is additionally employed during training, it requires more computation costs. However, even during training, learnable VTM is still faster than conventional methods including ViS4mer Islam Bertasius (2022) and S5 Wang et al. (2023) and it also requires less amounts of memory than them.

**Visualizations:** In Figure 7, we visualize the tokens merging results at the end of the network over multiple frames of video. We see that tokens with similar semantics are merged together. Also, tokens corresponding to backgrounds or unnecessary informations are merged more than tokens corresponding to salient objects. It is because learnable VTM selects tokens with high saliency scores as the target tokens. More visualization results are available in the supplemental document.

## 5 Conclusion

In this paper, we investigate the video token merging techniques for long-form video data. Unlike previous algorithms that apply uniform partitioning and merge tokens solely based on the visual similarity, we argue tokens with different saliencies should be treated unequally to avoid undesirable information loss after merging important tokens. To this end, we explore various video token merging methods and receive interesting intuitions from region-concentrated and motion-based token merging results. Lastly, we propose a learnable video token merging scheme that adaptively samples target tokens and learns discriminative representations from the long-form videos. Compared to the baseline, the proposed algorithm achieves substantial improvements in terms of the performance, memory cost and throughput.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Algorithm & Scene & Director & Like \\ \hline Weighted average & 72.09 & 68.22 & 0.24 \\ Motion & 74.41 & 64.48 & 0.24 \\ Motion weighted average & 74.41 & 66.35 & 0.23 \\ Learnable & 75.58 & 70.09 & 0.21 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of long-video understanding results of various VTM designs on the LVU dataset.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Algorithm & Phase & Throughput & Memory \\ \hline ViS4mer Islam Bertasius (2022) & Inference & 25.64 & 5.15GB \\ S5 Wang et al. (2023) & Inference & 25 & 3.85GB \\ Learnable VTM & Training & 27.84 & 2.8GB \\ Learnable VTM & Inference & 44.94 & 1.6GB \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of throughput and memory footprint of learnable VTM for training and inference.

## References

* Gemini (2018) Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. URL https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf.
* MovicClips (2023) MovieClips. URL https://www.movieclips.com/.
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. _arXiv_, 2023.
* Akbari et al. (2021) Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: Transformers for multimodal self-supervised learning from raw video, audio and text. _arXiv_, 2021.
* Bertasius et al. (2021) Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In _ICML_, 2021.
* Bolya and Hoffman (2023) Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In _CVPR Workshop_, 2023.
* Bolya et al. (2022) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT but faster. In _ICLR_, 2022.
* Brooks et al. (2024) Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _NeurIPS_, 2020.
* Carreira et al. (2018) Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. _arXiv_, 2018.
* Choromanski et al. (2021) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In _ICLR_, 2021.
* Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. _arXiv_, 2019.
* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. _arXiv_, 2018.
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* Fan et al. (2023) David Fan, Jue Wang, Shuai Liao, Yi Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, and Xinyu Li. Motion-guided masking for spatiotemporal representation learning. In _ICCV_, 2023.
* Fan et al. (2021) Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In _Int. Conf. Comput. Vis._, 2021.
* Gotmare et al. (2018) Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. In _arXiv_, 2018.
* Gu et al. (2021) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.
* Hussein et al. (2019a) Noureldien Hussein, Efstratios Gavves, and Arnold WM. Smeulders. Timeception for complex action recognition. In _CVPR_, 2019a.
* Huang et al. (2019)* Hussein et al. (2019b) Noureldien Hussein, Efstratios Gavves, and Arnold WM. Smeulders. VideoGraph: Recognizing minutes-long human activities in videos. In _ICCV Workshop_, 2019b.
* Islam and Bertasius (2022) Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In _ECCV_, 2022.
* Kay et al. (2017) Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. In _arXiv_, 2017.
* Kuehne et al. (2014) Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In _CVPR_, 2014.
* Li et al. (2024) Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. VidToMe: Video token merging for zero-shot video editing. In _CVPR_, 2024.
* Li et al. (2022) Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. MViTv2: Improved multiscale vision transformers for classification and detection. In _CVPR_, 2022.
* Liang et al. (2022) Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. _arXiv preprint arXiv:2202.07800_, 2022.
* Lin et al. (2022) Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities with distant supervision. In _CVPR_, 2022.
* Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _arXiv_, 2017.
* Meng et al. (2022) Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. AdaViT: Adaptive vision transformers for efficient image recognition. In _CVPR_, 2022.
* Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips. In _ICCV_, pp. 2630-2640, 2019.
* Patrick et al. (2021) Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Joao F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. In _NeurIPS_, 2021.
* Rao et al. (2021) Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision transformers with dynamic token sparsification. In _NeurIPS_, 2021.
* Ren et al. (2023) Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, and Lu Hou. TESTA: Temporal-spatial token aggregation for long-form video-language understanding. In _EMNLP_, 2023.
* Richardson (2004) Iain E. Richardson. _H. 264 and MPEG-4 video compression: video coding for next-generation multimedia_. 2004.
* Ridnik et al. (2021) Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. ImageNet-21K pretraining for the masses. In _NeurIPS_, 2021.
* Sun et al. (2019) Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A joint model for video and language representation learning. In _ICCV_, 2019.
* Sun et al. (2022) Yuchong Sun, Bei Liu, Hongwei Xue, Ruihua Sone, Huan Yang, and Jianlong Fu. Long-form video-language pre-training with multimodal temporal contrastive learning. In _NeurIPS_, 2022.
* Tang et al. (2019) Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. COIN: A large-scale dataset for comprehensive instructional video analysis. In _CVPR_, 2019.
* Wang et al. (2019)* Tang et al. [2020] Yansong Tang, Jiwen Lu, and Jie Zhou. Comprehensive instructional video analysis: The COIN dataset and performance evaluation. _IEEE TPAMI_, 43(9):3138-3153, 2020.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _arXiv preprint arXiv:1706.03762_, 2017.
* Wang and Torresani [2022] Jue Wang and Lorenzo Torresani. Deformable video transformer. In _CVPR_, 2022.
* Wang et al. [2022] Jue Wang, Gedas Bertasius, Du Tran, and Lorenzo Torresani. Long-short temporal contrastive learning of video transformers. In _CVPR_, 2022.
* Wang et al. [2023] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In _CVPR_, 2023.
* Wang et al. [2021] Junke Wang, Xitong Yang, Hengduo Li, Zuxuan Wu, and Yu-Gang Jiang. Efficient video transformers with spatial-temporal token selection. _arXiv preprint arXiv:2111.11591_, 2021.
* Wien [2015] Mathias Wien. High efficiency video coding. _Coding Tools and Specification_, 24, 2015.
* Wu and Krahenbuhl [2021] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In _CVPR_, 2021.
* Wu et al. [2022] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. MemViT: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In _CVPR_, 2022.
* Yin et al. [2021] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. Adavit: Adaptive tokens for efficient vision transformer. _arXiv preprint arXiv:2112.07658_, 2021.
* Zhou et al. [2021] Jiaming Zhou, Kun-Yu Lin, Haoxin Li, and Wei-Shi Zheng. Graph-based high-order relation modeling for long-term action recognition. In _CVPR_, 2021.

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_EMPTY:15]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See L65-L71 in Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See L280-287 in Section 4.3. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: NA Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 4.2 and A. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No]

Justification: NA

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: NA Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Section 4.2 and Appendix.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: NA Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: NA Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: NA Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: NA Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: NA Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.