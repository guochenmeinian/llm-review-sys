# On the Interplay between Social Welfare and Tractability of Equilibria

Ioannis Anagnostides

Carnegie Mellon University

ianagnos@cs.cmu.edu

&Tuomas Sandholm

Carnegie Mellon University

Strategic Machine, Inc.

Strategy Robot, Inc.

Optimized Markets, Inc.

sandholm@cs.cmu.edu

###### Abstract

Computational tractability and social welfare (aka. efficiency) of equilibria are two fundamental but in general orthogonal considerations in algorithmic game theory. Nevertheless, we show that when (approximate) _full efficiency_ can be guaranteed via a _smoothness_ argument a la Roughgarden, Nash equilibria are approachable under a family of no-regret learning algorithms, thereby enabling fast and decentralized computation. We leverage this connection to obtain new convergence results in _large games_--wherein the number of players \(n\gg 1\)--under the well-documented property of full efficiency via smoothness in the limit. Surprisingly, our framework unifies equilibrium computation in disparate classes of problems including games with vanishing _strategic sensitivity_ and two-player zero-sum games, illuminating en route an immediate but overlooked equivalence between smoothness and a well-studied condition in the optimization literature known as the _Minty property_. Finally, we establish that a family of no-regret dynamics attains a welfare bound that improves over the smoothness framework while at the same time guaranteeing convergence to the set of coarse correlated equilibria. We show this by employing the _clairvoyant_ mirror descent algortihm recently introduced by Piliouras _et al._

## 1 Introduction

The _Nash equilibrium (NE)_[76] formalizes the notion of a _stable_ outcome in a multiagent strategic interaction, and has arguably served as the most influential solution concept in the development of game theory. Indeed, algorithms designed to approximate Nash equilibria in two-player zero-sum games have recently resolved major challenges in AI [7, 10, 81]. Its prescriptive power, however, has been severely undermined in multi-player general-sum games by intrinsic computational barriers [30, 20, 94, 35, 28, 44], limitations which also manifest in the inability of natural learning algorithms to converge [73, 105, 60, 70]. Another well-established but orthogonal critique is the _equilibrium selection_ problem [52]: a general-sum game may have multiple Nash equilibria with widely different welfare. As a result, a modern research agenda in computational game theory has been to identify and characterize natural classes of games that circumvent those fundamental limitations.

In this paper, we uncover a new natural class of games for which the aforementioned caveats of Nash equilibria can be effectively addressed. In particular, our investigation originates from a natural question: when are _efficient_--in terms of social welfare--Nash equilibria easy to compute? The answer to this question is, at first glance, unsatisfactory: even if Nash equilibria are fully efficient, computational hardness still persists given that constant-sum multi-player games are hard. Indeed, efficiency and computational tractability are, in general, two orthogonal considerations. We show,however, an interesting twist, an unexpected interplay between efficiency--when viewed from a specific lens--and the behavior of a family of _no-regret_ learning algorithms.

### Our results

To elucidate the alluded connection that drives much of our results, we first have to recall that the canonical paradigm for establishing the efficiency of equilibria is Roughgarden's celebrated _smoothness_ framework [90] (exposed thoroughly in Section 2). In this context, we observe that _if full efficiency of equilibria can be guaranteed via a smoothness argument (with bounded parameters), then Nash equilibria are approachable under a family of no-regret learning algorithms_. In other words, full efficiency _via smoothness_ implies computational tractability of NE. This is surprising in that Roughgarden's smoothness framework was developed primarily in order to automatically extend _price of anarchy (PoA)_ bounds to more permissive equilibrium concepts, such as _coarse correlated equilibria (CCE)_; tractability of NE appears at first glance entirely unrelated. In fact, while applying the smoothness framework to (multi-player) constant-sum games appears to make little sense, given that PoA considerations are trivial in such games (all outcomes attain the same social welfare), it turns out that a certain regime of smoothness in (multi-player) constant-sum games is equivalent to a well-known condition in the optimization literature called the _Minty property_[36] (Observation 3.4).

The condition described above already captures--somewhat unexpectedly--well-studied settings, such as games that admit a minimax theorem [13], but we have found that the most fertile and novel ground to apply this theory revolves around _large games_, that is, games with a large number of players \(n\gg 1\).1 The reason we focus on large games is a well-documented economic phenomenon: equilibria in large games approach--under natural conditions--full efficiency as \(n\to+\infty\), a property often established via smoothness [39, 23]--a crucial ingredient in our framework. (One rough intuition for this is that in large games each player's influence on the outcome becomes negligible, making optimal behavior easier to characterize [39].) To state our first main result, we denote by \((\lambda,\mu)\) a pair of bounded _smoothness_ parameters of a game \(\mathcal{G}\); the ratio \(\rho(\lambda,\mu)\coloneqq\frac{\lambda}{1+\mu}\) circumscribes the (in)efficiency of equilibria of \(\mathcal{G}\), in that all equilibria will attain at least a \(\rho\) fraction of the optimal welfare.

Footnote 1: We use the terminology large games here—in accordance with much of the economics literature—to refer to games with a large number of players; it should not be confused with another common usage referring to games with a “large” action space.

**Theorem 1.1** (Informal).: _Consider a sequence of \(n\)-player \((\lambda_{n},\mu_{n})\)-smooth games \((\mathcal{G}_{n})_{n\geq 1}\) such that \(\rho_{n}\coloneqq\frac{\lambda_{n}}{1+\mu_{n}}\to 1\) with a sufficiently fast rate. Then, there are decentralized and computationally efficient no-regret dynamics approaching an \((o_{n}(1),o_{n}(1))\)-weak Nash equilibrium._

A few remarks are in order. First, in Theorems 3.1 and A.2 we give a precise non-asymptotic characterization that quantifies the number of iterations as well as the approximation error. We also recall that in an \((o_{n}(1),o_{n}(1))\)-_weak_ Nash equilibrium _almost all_ players are _almost best responding_ (Definition 2.2); this is a well-studied relaxation for which hardness results carry over in general [4, 2, 5, 94]. A limitation of a weak Nash equilibrium is that it can prescribe a strategy profile in which a large numbers of players--albeit of vanishing fraction--can significantly benefit from deviating; this limitation is inherent in a certain regime of Theorem 1.1. Depending on the rate with which \(\rho_{n}\) approaches full efficiency, Theorem 1.1 can also imply convergence to the usual notion of Nash equilibrium--wherein _all_ players are almost best responding (Corollary A.5). More generally, for a broad class of games that includes _graphical_ games with bounded degree, _polymatrix_ games, and games exhibiting small _strategic sensitivity_, the conclusion of Theorem 1.1 applies without imposing any restrictions on the rate of convergence of \(\rho_{n}\); those are the most commonly studied classes of games when \(n\gg 1\).

There are ample compelling aspects in connecting the convergence of no-regret learning algorithms with Roughgarden's smoothness framework. First, there has been a considerable interest in understanding smoothness, insights that can now be inherited to a seemingly entirely different but equally fundamental problem. For example, smoothness naturally extends to _Bayesian games_[101, 91, 53] (see also [97] for the related class of contextual games), a property that we leverage in Theorem 3.8 to expand our scope to mechanisms of incomplete information. Additional extensions based on _local smoothness_[92, 78] and the refined primal-dual framework of Nadav and Roughgarden [75] are also described in Appendices A.3 and A.4. Finally, our criterion has a clear and natural economic interpretation, and is part of an ongoing research effort to identify tractable classes of variational inequalities (VIs) beyond the Minty property [33].

As a concrete example, our framework subsumes games wherein each player's effect on the outcome vanishes as \(n\to\infty\); this captures, for example, simple voting settings [57], as well as general auction design problems where establishing that property turns out to be highly non-trivial and quite delicate [39]. More concretely, for games with vanishing _sensitivity_\(\epsilon_{n}\to 0\), in that unilateral deviations can only affect a player's utility by an additive \(\epsilon_{n}\) (Definition 3.5), we show that the conclusion of Theorem 1.1 applies as long as \(\epsilon_{n}\leq o_{n}(1/\sqrt{n})\) (Theorem 3.6). At the same time, the condition that \(\rho_{n}\to 1\) goes much deeper than games with vanishing sensitivity, and, as we explained earlier, surprisingly applies to two-player zero-sum games as well. We find it conceptually appealing that a unifying framework can establish tractability of Nash equilibria in two seemingly disparate classes of problems such as two-player zero-sum games and games with vanishing strategic sensitivity.

Remaining on smooth games, but relaxing the assumption \(\rho\approx 1\), we next study conditions under which the efficiency guaranteed by the smoothness framework can be improved, while at the same time ensuring the no-regret property, thereby implying convergence to the set of CCE. The smoothness bound is known to be applicable to _any_ outcome of no-regret dynamics, but here we are instead interested in more refined guarantees when specific learning algorithms are in place. Building on a recent result [1], we show in Theorem 4.1 that the _clairvoyant_ variant of gradient descent, introduced by Piliouras et al. [85], enjoys an improved welfare bound _and_ ensures fast convergence to the set of CCE for the average correlated distribution of play. Crucially, compared to an earlier result [1], the clairvoyant algorithm manages to satisfy an appealing notion of per-player incentive compatibility, in the form of convergence to CCE. In other words, improving the welfare predicted by smoothness is not at odds with incentive compatibility (Corollary 4.3).

## 2 Background

NotationWe let \(\mathbb{N}=\{1,2,\dots\}\) be the set of natural numbers. For \(n\in\mathbb{N}\), we denote by \(\llbracket n\rrbracket\coloneqq\{1,2,\dots,n\}\). For a vector \(\bm{x}\in\mathbb{R}^{d}\), we denote by \(\|\bm{x}\|_{2}\) its (Euclidean) \(\ell_{2}\) norm. For a convex, compact and nonempty set \(\mathcal{X}\), we let \(\mathcal{P}_{\mathcal{X}}(\cdot)\) represent the Euclidean projection operator with respect to \(\mathcal{X}\). We let \(D_{\mathcal{X}}\) be the \(\ell_{2}\)-diameter of \(\mathcal{X}\). To simplify the exposition, we often use the \(O(\cdot)\) notation in the main body to suppress the dependence on certain parameters; we also write \(O_{n}(\cdot)\) to indicate the asymptotic growth solely as a function of \(n\), so as to lighten the exposition.

Multilinear gamesWe consider \(n\)-player _multilinear_ games. In a multilinear game \(\mathcal{G}\) each player \(i\in\llbracket n\rrbracket\) has a convex, compact and nonempty set of feasible strategies \(\mathcal{X}_{i}\subseteq\mathbb{R}^{d_{i}}\), for some dimension \(d_{i}\in\mathbb{N}\). Under a joint strategy profile \(\bm{x}=(\bm{x}_{1},\dots,\bm{x}_{n})\in\prod_{i=1}^{n}\mathcal{X}_{i} \eqqcolon\mathcal{X}\), there is a continuous utility function \(u_{i}:(\bm{x}_{1},\dots,\bm{x}_{n})\mapsto\mathbb{R}\) such that \(u_{i}(\bm{x})=\langle\bm{x}_{i},\bm{u}_{i}(\bm{x}_{-i})\rangle\), for some function \(\bm{u}_{i}:\bm{x}_{-i}\mapsto\mathbb{R}^{d_{i}}\); here, we used for convenience the standard notation \(\bm{x}_{-i}\coloneqq(\bm{x}_{1},\dots,\bm{x}_{i-1},\bm{x}_{i+1},\dots,\bm{x}_ {n})\). This setup readily captures _normal-_ as well as _extensive-form_ games.

Specifically, in a normal-form game every player \(i\in\llbracket n\rrbracket\) selects as strategy a probability distribution \(\bm{x}_{i}\in\Delta(\mathcal{A}_{i})\) over a finite set of available actions \(\mathcal{A}_{i}\). There is an arbitrary utility function \(u_{i}:\mathcal{A}\coloneqq\prod_{i=1}^{n}\mathcal{A}_{i}\to[-1,1]\) that maps a joint action profile to a utility for Player \(i\); we will be making the standard assumption that the range of each player's utilities is bounded by an absolute constant, which is in particular independent of the number of players \(n\). In this setting, the mixed extension of the utility function indeed satisfies the multilinearity condition imposed above: for \(\bm{x}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\), \(u_{i}(\bm{x})\coloneqq\mathbb{E}_{\bm{a}\sim\bm{x}}[u_{i}(\bm{a})]=\langle \bm{x}_{i},\nabla_{\bm{x}_{i}}u_{i}(\bm{x})\rangle\), where we overloaded the notation \(u_{i}(\cdot)\).

Returning to the general setting of multilinear games, we let \(F_{\mathcal{G}}:\prod_{i=1}^{n}\mathcal{X}_{i}\to\prod_{i=1}^{n}\mathcal{X}_ {i}\) denote the underlying _operator_ of the game \(\mathcal{G}\), defined as \(F_{\mathcal{G}}:(\bm{x}_{1},\dots,\bm{x}_{n})\mapsto(\bm{u}_{1}(\bm{x}_{-1}), \dots,\bm{u}_{n}(\bm{x}_{-n}))\), where function \(\bm{u}_{i}\) was introduced earlier. For notational simplicity, we will often omit the subscript \(\mathcal{G}\) when it is clear from the context. We will say that \(F\) is _L-Lipschitz continuous_ (w.r.t. the \(\ell_{2}\) norm) if for any \(\bm{x},\bm{x}^{\prime}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) it holds that \(\|F(\bm{x})-F(\bm{x}^{\prime})\|_{2}\leq L\|\bm{x}-\bm{x}^{\prime}\|_{2}\).

Welfare and the price of anarchyFor a joint strategy profile \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\), we define the _social welfare_ attained under \(\bm{x}\) as \(\operatorname{SW}(\bm{x})\coloneqq\sum_{i=1}^{n}u_{i}(\bm{x})\). The maximum possible social welfare attainable in a game \(\mathcal{G}\) will be denoted by \(\text{OPT}_{\mathcal{G}}\). Without any essential loss of generality, it will be assumed that \(\text{OPT}_{\mathcal{G}}>0\). We say that the game is _constant-sum_ if \(\operatorname{SW}(\bm{x})=V\in\mathbb{R}_{>0}\) for any \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\). The _price of anarchy (PoA)_ of a game \(\mathcal{G}\) quantifies the loss in efficiency incurred on account of strategic players [61]. Formally, if \(\mathbb{NE}_{\mathcal{G}}\) is the nonempty set of _(mixed) Nash equilibria_ of \(\mathcal{G}\) (Definition 2.2) [88], we define \(\mathsf{PoA}_{\mathcal{G}}\coloneqq\sup_{\bm{x}\in\mathbb{NE}_{\mathcal{G}}} \left\{\frac{\mathsf{SW}(\bm{x})}{\mathsf{OPT}_{\mathcal{G}}}\right\}\).

Smooth gamesWe are now ready to recall the seminal notion of a _smooth game_,2 conceived in the pioneering work of Roughgarden [90] as a technique to (lower) bound the price of anarchy.

Footnote 2: Smoothness in the sense of Definition 2.1 should not be confused with the unrelated notion of smoothness in the optimization nomenclature.

**Definition 2.1** (Smooth game [90]).: An \(n\)-player game \(\mathcal{G}\) is called _\((\lambda,\mu)\)-smooth_, where \(\lambda>0\) and \(\mu>-1\), if there exists \(\bm{x}^{\star}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) with \(\mathsf{SW}(\bm{x}^{\star})=\mathsf{OPT}_{\mathcal{G}}\) such that for every \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\),

\[\sum_{i=1}^{n}u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i})\geq\lambda\mathsf{OPT}_{ \mathcal{G}}-\mu\mathsf{SW}(\bm{x}).\] (1)

(In the definition above, and throughout this paper, we slightly abuse notation by parsing \(u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i})\) as \(u_{i}(\bm{x}_{1},\ldots,\bm{x}_{i-1},\bm{x}_{i}^{\star},\bm{x}_{i+1},\ldots, \bm{x}_{n})\).) Roughgarden [90] observed that in a \((\lambda,\mu)\)-smooth game, in the sense of Definition 2.1, every Nash equilibrium attains at least a \(\rho(\lambda,\mu)\coloneqq\frac{\lambda}{1+\mu}\) fraction of the optimal social welfare \(\mathsf{OPT}_{\mathcal{G}}\). Importantly, this efficiency guarantee immediately carries over to outcomes of no-regret learning algorithms as well. The _robust price of anarchy (\(\mathsf{rPoA}_{\mathcal{G}}\))_ is the best (_i.e._, largest) price of anarchy bound provable via a smoothness argument, and can be defined as the solution to the linear program induced by the smoothness constraints given in (1) (see (24) in Appendix A.5). One delicate point here is that the value of \(\mathsf{rPoA}_{\mathcal{G}}\) could be associated with unbounded smoothness parameters, which is a pathological and rather trivial manifestation of smoothness (Remark A.10 elaborates on this point). To be clear, when we say a game \(\mathcal{G}\) attains a certain value \(\rho_{\mathcal{G}}\), we mean that there exists a finite pair of legitimate smoothness parameters \((\lambda,\mu)\) such that \(\rho_{\mathcal{G}}=\frac{\lambda}{1+\mu}\). With this convention, it might be the case that \(\rho_{\mathcal{G}}(\lambda,\mu)\neq\mathsf{rPoA}_{\mathcal{G}}\) under any finite pair of smoothness parameters \((\lambda,\mu)\) (Remark A.10). It is also easy to see that \(\mathsf{PoA}_{\mathcal{G}}\geq\mathsf{rPoA}_{\mathcal{G}}\).

Nash equilibriumWe next recall the concept of a _weak_ Nash equilibrium, a natural generalization of the standard notion which is meaningful in multi-player games.

**Definition 2.2** (Weak Nash equilibrium [5]).: Let \(\delta\in[0,1)\) and \(\epsilon\in\mathbb{R}_{\geq 0}\). A joint strategy profile \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) is an _\((\epsilon,\delta)\)-weak Nash equilibrium_ if at least a \(1-\delta\) fraction of the players are \(\epsilon\)-best responding. An \((\epsilon,0)\)-weak Nash equilibrium will be simply referred to as _\(\epsilon\)-Nash equilibrium_.

In the definition above, we clarify that a player \(i\in\llbracket n\rrbracket\) is said to be _\(\epsilon\)-best responding_ if \(\mathsf{BRGap}_{i}(\bm{x}_{1},\ldots,\bm{x}_{n})\coloneqq\max_{\bm{x}_{i}^{ \prime}\in\mathcal{X}_{i}}\langle\bm{x}_{i}^{\prime},\bm{u}_{i}(\bm{x}_{-i}) \rangle-\langle\bm{x}_{i},\bm{u}_{i}(\bm{x}_{-i})\rangle\leq\epsilon\). We also define the _Nash equilibrium gap_ as \(\mathsf{NEGap}(\bm{x})\coloneqq\max_{1\leq i\leq n}\mathsf{BRGap}_{i}(\bm{x})\). It has been shown that hardness results in multi-player general-sum games persist under the weak Nash equilibrium concept introduced above, even when \(\epsilon\) and \(\delta\) are absolute constants bounded away from \(0\)[94, 4].

RegretWe are operating in the usual online learning setting. At every time \(t\in\mathbb{N}\) a player \(i\in\llbracket n\rrbracket\) selects a strategy \(\bm{x}_{i}^{(t)}\in\mathcal{X}_{i}\), and then receives as feedback the linear utility function \(\bm{x}_{i}\mapsto\langle\bm{x}_{i},\bm{u}_{i}^{(t)}\rangle\), where we overload notation so that \(\bm{u}_{i}^{(t)}\coloneqq\bm{u}_{i}(\bm{x}_{-i}^{(t)})\). The _regret_ of player \(i\in\llbracket n\rrbracket\) under a time horizon \(T\in\mathbb{N}\) is defined as

\[\mathsf{Reg}_{i}^{(T)}\coloneqq\max_{\bm{x}_{i}^{\prime}\in\mathcal{X}_{i}} \left\{\sum_{t=1}^{T}u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i}^{(t)})\right\}-\sum_ {t=1}^{T}u_{i}(\bm{x}^{(t)}).\]

Optimistic gradient descentBy now, there are many online algorithms known to guarantee _sublinear_ regret, \(\mathsf{Reg}_{i}^{(T)}=o(T)\), even if the observed utilities are selected adversarially [18, 98]. The main no-regret learning algorithm we consider in this paper is _optimistic gradient descent_ (henceforth OGD) [86, 22], which is known to yield improved regret guarantees in the setting of learning in games [103]. For each player \(i\in\llbracket n\rrbracket\), OGD is defined through the following update rule for \(t\in\mathbb{N}\).

\[\bm{x}_{i}^{(t)} \coloneqq\mathcal{P}_{\mathcal{X}_{i}}\left(\hat{\bm{x}}_{i}^{(t)}+ \eta\bm{m}_{i}^{(t)}\right),\] (OGD) \[\hat{\bm{x}}_{i}^{(t+1)} \coloneqq\mathcal{P}_{\mathcal{X}_{i}}\left(\hat{\bm{x}}_{i}^{(t)}+ \eta\bm{u}_{i}^{(t)}\right).\]Here, \(\eta>0\) is the _learning rate_; \(\bm{m}_{i}^{(t)}\in\mathbb{R}^{d_{i}}\) is the _prediction_ vector; and \(\hat{\bm{x}}_{i}^{(1)}\in\mathcal{X}_{i}\) is the _initialization_. It is assumed that the strategy set \(\mathcal{X}_{i}\) is such that the Euclidean projection \(\mathcal{P}_{\mathcal{X}_{i}}(\cdot)\) can be implemented efficiently. We will let \(\bm{m}_{i}^{(t)}\coloneqq\bm{u}_{i}^{(t-1)}\) for any \(t\in\mathbb{N}\), where \(\bm{u}_{i}^{(0)}\coloneqq\bm{u}_{i}(\hat{\bm{x}}_{-i}^{(1)})\).

## 3 Convergence to Nash equilibria via smoothness

In this section, we study the convergence of optimistic gradient descent (OGD) in large games, that is to say, in the regime \(n\gg 1\). In our first main result, stated below as Theorem 3.1, we show that when \(\lim_{n\to+\infty}\rho_{n}=1\) with a sufficiently fast rate, not only are Nash equilibria approaching the optimal social welfare, but they can also be computed efficiently in a decentralized fashion via OGD.

In the sequel, when considering a sequence of games \((\mathcal{G}_{n})_{n\geq 1}\), with each game \(\mathcal{G}_{n}\) being parameterized by the number of players \(n\), we will use a subscript with variable \(n\) to index the \(n\)th game in the sequence; that notation will also be used to refer to the other underlying parameters of the game that depend on the number of players.

**Theorem 3.1**.: _Consider an \(n\)-player \((\lambda,\mu)\)-smooth game \(\mathcal{G}_{n}\) such that the game operator \(F_{n}\) is \(L_{n}\)-Lipschitz continuous and \(\lambda\geq(1-\epsilon_{n})(1+\mu)\), with \(\epsilon_{n}\neq 0\). Suppose further that all players follow OGD with learning rate \(\eta_{n}=1/(4L_{n})\). Then, for \(\delta\in(0,1)\) and a sufficiently large number of iterations \(T=O_{n}(nL_{n}^{2}/\gamma_{n})\), where \(\gamma_{n}\coloneqq L_{n}\textsc{OPT}_{\mathcal{G}_{n}}\epsilon_{n}\), there is time \(t^{\star}\in\llbracket T\rrbracket\) such that \(\bm{x}^{(t^{\star})}\coloneqq(\bm{x}_{1}^{(t^{\star})},\ldots,\bm{x}_{n}^{(t^ {\star})})\) is a_

\[\left(\frac{1}{\sqrt{\delta}}O_{n}\left(\sqrt{\frac{\gamma_{n}}{n}}\right), \delta\right)-\text{weak Nash equilibrium.}\] (2)

_In particular, if \(\gamma_{n}=O_{n}(n^{1-\alpha})\), for \(\alpha\in(0,1)\), OGD yields an \((O_{n}(n^{-\frac{\alpha}{3}}),O_{n}(n^{-\frac{\alpha}{3}}))\)-weak NE._

We remark that if it further holds that \(\gamma_{n}=o_{n}(1)\), the above theorem establishes convergence to the standard notion of Nash equilibrium--wherein _all_ players are (almost) best responding (Corollary A.5). Parameter \(\gamma_{n}\), which is proportional to the error term \(\epsilon_{n}\), controls both the number of iterations and the approximation guarantee in (2); we refer to Theorem A.2 (in Appendix A) for a more precise non-asymptotic characterization, which bounds the players' cumulative best response gap as a function of the growth of \(\gamma_{n}\). We also note that Theorem 3.1 can be strengthened so that (2) holds for _most_ iterates of OGD (say 99%), not just a single one (Remark A.4).

Now, to be more concrete regarding the preconditions of Theorem 3.1, we first observe that the growth of the Lipschitz constant \(L_{n}\) depends on the normalization assumptions as well as the structure of the underlying game \(\mathcal{G}_{n}\). In particular, let us assume--as is standard--that the range of the utility functions is independent of \(n\), in which case \(\textsc{OPT}_{\mathcal{G}_{n}}=O_{n}(n)\). We then show that \(L_{n}=O_{n}(1)\) in each of the following cases: graphical games with bounded degree (Lemma A.7), games with \(O_{n}(1/n)\) strategic sensitivity per Definition 3.5 (Lemma A.8), and polymatrix (general-sum) games even with unbounded neighborhoods (Lemma A.9); the first two of the aforementioned classes are the most commonly studied classes in the literature under the regime \(n\gg 1\). For all of those classes, applying Theorem 3.1 yields an \((o_{n}(1),o_{n}(1))\)-weak Nash equilibrium for any \(\epsilon_{n}\leq o_{n}(1)\). More generally, the Lipschitz constant \(L_{n}\) can grow with \(n\) (Lemma A.6), in which case \(\epsilon_{n}\) must vanish with a faster rate for the conclusion of Theorem 3.1 to kick in. One important limitation of Theorem 3.1 is that, at least in a certain regime, it prescribes a strategy in which many players--albeit a vanishing fraction--may have a profitable deviation (in accordance with Definition 2.2); this is admittedly an inherent feature of our framework.

_Remark 3.2_.: In a decentralized environment, one question that arises from Theorem 3.1 concerns the identification of a time index \(t^{\star}\in\llbracket T\rrbracket\) that satisfies (2), which can be viewed as the stopping condition of the algorithm. We suggest two possible approaches. First, if we accept that players have access to a common source of randomness, then all players can sample the same index \(t^{\star}\) uniformly at random from the set \(\llbracket T\rrbracket\). As we point out in Remark A.4, this suffices to provide a guarantee with high probability with only a small degradation in the solution quality. The second approach, which does not rest any having a common source of randomness, involves a coordinator who can communicate with the players but possesses no information whatsoever about the underlying game. The coordinator sets a target solution quality parameterized by \((\epsilon,\delta)\) (per Definition 2.2), and after each iteration \(t\) elicits from each player \(i\in\llbracket n\rrbracket\) a single bit, encoding whether \(\mathbbm{1}\{\textsc{BRGAP}_{i}(\bm{x}^{(t)})>\epsilon\}\). (We note that each player can indeed determine its best response gap with only its local information--namely,the utility feedback.) The coordinator can then evaluate whether the fraction of the players with at most an \(\epsilon\) best response gap matches the desired accuracy. While the second approach makes for a less decentralized protocol, the communication overhead described above is arguably very limited.

The key precondition of Theorem 3.1 pertains the behavior of the smoothness parameters, to be discussed next after we first sketch the proof of Theorem 3.1, which extends a recent technique [1].

Proof sketch of Theorem 3.1The proof is based on the fact that the sum of the players' regrets \(\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)}\)_cannot be too negative_, which in turn follows from the assumption that \(\rho_{n}\geq 1-\epsilon_{n}\). The argument then proceeds by bounding the players' cumulative best response gap across the \(T\) iterations \(\sum_{t=1}^{T}\sum_{i=1}^{n}\left(\mathsf{BRGap}_{i}(\bm{x}^{(t)})\right)^{2}\) as a function of \(\gamma_{n}\) and the time horizon \(T\in\mathbb{N}\), ultimately leading to the conclusion of Theorem 3.1.

Connection with the Minty propertyWhile we have stated Theorem 3.1 in the regime \(n\gg 1\), under the premise that \(\rho_{n}\) is sufficiently close to \(1\) (as a function of \(n\)), its conclusion is in fact interesting beyond that regime. Indeed, an important observation is that any two-player constant-sum game \(\mathcal{G}\coloneqq(\mathbf{A},\mathbf{B})\), with \(\langle\bm{x}_{1},\mathbf{A}\bm{x}_{2}\rangle+\langle\bm{x}_{1},\mathbf{B}\bm {x}_{2}\rangle=V\) for any \((\bm{x}_{1},\bm{x}_{2})\in\mathcal{X}_{1}\times\mathcal{X}_{2}\), satisfies \(\rho_{\mathcal{G}}=1\). This is indeed a consequence of Von Neumann's minimax theorem: \(\exists(\bm{x}_{1}^{\star},\bm{x}_{2}^{\star})\in\mathcal{X}_{1}\times \mathcal{X}_{2}\) such that \(u_{1}(\bm{x}_{1}^{\star},\bm{x}_{2})+u_{2}(\bm{x}_{2}^{\star},\bm{x}_{1})-V= \langle\bm{x}_{1}^{\star},\mathbf{A}\bm{x}_{2}\rangle-\langle\bm{x}_{1}, \mathbf{A}\bm{x}_{2}^{\star}\rangle\geq 0\) for any \((\bm{x}_{1},\bm{x}_{2})\in\mathcal{X}_{1}\times\mathcal{X}_{2}\), in turn implying that \(u_{1}(\bm{x}_{1}^{\star},\bm{x}_{2})+u_{2}(\bm{x}_{2}^{\star},\bm{x}_{1})\geq( 1+\mu)\text{OPT}_{\mathcal{G}}-\mu\text{SW}(\bm{x})=V\); that is, any two-player constant-sum game is \((1+\mu,\mu)\)-smooth, thereby making the conclusion of Theorem 3.1 readily applicable (by taking \(\rho\geq 1-\epsilon^{2}\) for any \(\epsilon>0\); see the explicit statement of Theorem A.2). This captures and unifies earlier iteration complexity bounds under OGD and the extra-gradient method [15, 48, 46].

**Proposition 3.3**.: _Any two-player constant-sum game \(\mathcal{G}\) is \((1+\mu,\mu)\)-smooth for \(\mu>-1\), implying that \(\rho_{\mathcal{G}}=1\). Thus, \(O(1/\epsilon^{2})\) iterations of OGD suffice to obtain an \(\epsilon\)-Nash equilibrium, for any \(\epsilon>0\)._

More broadly, there is a surprisingly overlooked but immediate connection between smooth games (per Definition 2.1) and the _Minty property_, a well-known condition in the literature on variational inequalities (VIs) [36, 66, 71]. More precisely, the Minty property postulates the existence of a strategy profile \(\bm{x}^{\star}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) such that \(\langle\bm{x}^{\star}-\bm{x},F(\bm{x})\rangle\geq 0\) for any \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\), where we recall that \(F\) is the operator of the game. (We caution that the last inequality is typically stated with the opposite sign since the operator \(F\) is defined oppositely.) The following connection is thus immediate from the fact that \(\langle\bm{x}^{\star},F(\bm{x})\rangle=\sum_{i=1}^{n}\langle\bm{x}_{i}^{\star},\bm{u}_{i}(\bm{x}_{-i})\rangle=\sum_{i=1}^{n}u_{i}(\bm{x}_{i}^{\star},\bm{x}_ {-i})\) and \(\langle\bm{x},F(\bm{x})\rangle=\sum_{i=1}^{n}\langle\bm{x}_{i},\bm{u}_{i}(\bm{x }_{-i})\rangle=\text{SW}(\bm{x})\) (by multilinearity).

**Observation 3.4**.: _For any (multi-player) constant-sum game \(\mathcal{G}\), the Minty property is equivalent to \(\mathcal{G}\) being \((1+\mu,\mu)\)-smooth for some \(\mu>-1\)._

Indeed, the Minty property implies that \(\sum_{i=1}^{n}u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i})\geq V=(1+\mu)\text{OPT}_{ \mathcal{G}}-\mu\text{SW}(\bm{x})\), and the converse direction is also immediate. In fact, for (multi-player) zero-sum games, the Minty property is equivalent to \(\mathcal{G}\) satisfying (1) under some pair \((\lambda,\mu)\in\mathbb{R}^{2}\). We stress that even if \(\rho_{\mathcal{G}}=1\), traditional no-regret learning algorithms such as online mirror descent do not generally enjoy iterate convergence to Nash equilibria [70], which stands in stark contrast to the behavior of OGD (Proposition 3.3). In light of Observation 3.4, Theorem 3.1 should also be viewed as part of an ongoing effort to establish sufficient conditions of tractability that are more permissive than the Minty property (_e.g._, [33, 12, 14, 84, 11]). The criterion we furnish herein, based on the smoothness framework, has the important benefit of enjoying a natural economic interpretation, as well as having being extensively studied in the literature. Indeed, we will leverage insights from prior work to obtain several interesting extensions in the remainder of this section.

Games with vanishing sensitivityReturning to the regime \(n\gg 1\), why should we expect \(\rho_{n}\to 1\)? Indeed, if anything large games are more general than games with a small number of players since one can always incorporate "dummy" players into the game. Yet, the point is that large games oftentimes exhibit a structure that leads to more efficient outcomes. For example, one immediate implication of our framework relates to games with a _vanishing (strategic) sensitivity_ (see, _e.g._, [57, 3, 72, 31, 45]). There are various ways of defining sensitivity; here, we adopt the following standard definition.

**Definition 3.5**.: The strategic _sensitivity_\(\epsilon\in\mathbb{R}_{>0}\) of an \(n\)-player game in normal form is defined as

\[\epsilon\coloneqq\max_{1\leq i\leq n}\max_{\bm{a}\in\mathcal{A}}\max_{1\leq i^{ \prime}\leq n}\max_{a_{i^{\prime}}^{\prime}\in\mathcal{A}_{i^{\prime}}}|u_{i}(a_{ i^{\prime}}^{\prime},\bm{a}_{-i})-u_{i}(\bm{a})|.\]In words, a unilateral deviation can only impact a player's utility by an additive \(\epsilon\). Now, as long as the sensitivity decays fast enough, a proof analogous to that of Theorem 3.1 implies the following.

**Theorem 3.6**.: _Consider an \(n\)-player game \(\mathcal{G}_{n}\) with sensitivity \(\epsilon_{n}\in\mathbb{R}_{>0}\). Then, \(T=O_{n}(n)\) iterations of_ OGD _suffic to obtain a \(\left(\frac{1}{\sqrt{\delta}}O_{n}\left(\epsilon_{n}\sqrt{n}\right),\delta\right)\)-weak Nash equilibrium, for \(\delta\in(0,1)\)._

In particular, Theorem 3.6 yields an \(\left(o_{n}(1),o_{n}(1)\right)\)-weak Nash equilibrium as long as \(\epsilon_{n}=o_{n}(\sqrt{n})\). Further, in the canonical regime where \(\epsilon_{n}=O_{n}(1/n)\), Theorem 3.6 circumscribes the best response gap for all but a constant number of players (by taking \(\delta=O_{n}(1/n)\)). There are many natural settings where we should expect results such as Theorem 3.6 to be applicable [57, 58, 69]. We find it conceptually compelling that our framework can provide in a unifying way equilibrium guarantees for two seemingly disparate classes of games, namely two-player zero-sum games and games with vanishing strategic sensitivity.

In a similar vein, Feldman et al. [39] showed that \(\rho_{n}\to 1\) with a rate of \(1/\sqrt{n}\) in a general auction design problem (see also [23]) under the relatively mild assumption that each player participates in the market with some constant probability (aka. probabilistic demand), thereby bypassing known barriers regarding the inefficiency of equilibria in general combinatorial domains. Their proof is based on the fact that each bidder's impact on the prices--under a simultaneous uniform-price auction format--becomes asymptotically negligible, in the spirit of Definition 3.5 introduced above. The difficulty that arises in that setting is that the natural representation of the utility functions violates our multilinearity assumption (postulated in Section 2). Instead, one would have to resort to some form of discretization before attempting to apply Theorem 3.1, which could be computationally prohibitive; the other prerequisite is that \(L_{n}=o_{n}(\sqrt{n})\), for which our approach in Lemma A.8 in conjunction with the insights of Feldman et al. [39] could be useful. Understanding whether our techniques can be applied in the combinatorial auction setting of Feldman et al. [39] is left as a challenging open question.

Efficiency of equilibria does not suffice for tractabilityA natural question arising from Theorem 3.1 is whether a similar statement applies under the assumption that \(\mathsf{PoA}\to 1\), that is, assuming that all Nash equilibria of \(\mathcal{G}\) are (approximately) fully efficient. This is clearly a weaker assumption, but it is unfortunately not sufficient to yield any non-trivial guarantees even in normal-form games:

**Proposition 3.7**.: _Even under the promise that \(\mathsf{PoA}_{\mathcal{G}}=1\), computing a \((1/\mathsf{poly}(\mathcal{G}))\)-Nash equilibrium in normal-form games in polynomial time is impossible when \(n\geq 3\), unless \(\mathsf{PPAD}\subseteq\mathsf{P}\)._

This stands in contrast to the class of \((1+\mu,\mu)\)-smooth games, where a fully polynomial-time approximation scheme (\(\mathsf{FPTAS}\)) is implied by Theorem 3.1--assuming access to a utility and a projection oracle, both of which are available in, for example, most succinct normal-form games. Proposition 3.7 is a straightforward consequence of the fact that Nash equilibria are hard to compute even in constant-sum \(3\)-player games [20]. Furthermore, in Example A.11 we identify a specific \(3\)-player game in which OGD fails (unconditionally) to converge to \(\epsilon\)-Nash equilibria for a constant value of \(\epsilon>0\), even though \(\mathsf{PoA}=1\). Our example is based on a variant of _Shapley's game_[99].

Another notable advantage of smoothness as a criterion of convergence is that, at least when the game is represented explicitly, it is easy to compute; this is in contrast to \(\mathsf{PoA}\), whose identification even in two-player games is \(\mathsf{NP}\)-hard (Proposition A.12).

ExtensionsIt turns out that smoothness per Definition 2.1 can be further sharpened using a primal-dual framework [75]. Such refined guarantees can also be translated into our setting (in the context of Theorem 3.1), as we elaborate on in Appendix A.3. The upshot is that the modification of Nadav and Roughgarden [75] necessitates analyzing the _weighted_ sum of the players' regrets \(\sum_{i=1}^{n}z_{i}\mathsf{Re}_{i}^{(T)}\) under a dual set of variables \(\{z_{i}\}_{i=1}^{n}\). Another interesting extension worth noting relies on the _local_ smoothness framework [92, 78], as we explain in Appendix A.4; the key observation is that local smoothness per Nguyen [78] can be associated with a _linearized_ notion of regret, at which point the analysis of Theorem 3.1 readily carries over. Finally, we also expand our scope to Bayesian mechanisms, as we expound in the upcoming subsection.

Before we proceed, it is important to point out that, unsurprisingly, smoothness is not merely enough to guarantee convergence of OGD; see Example A.13. It is instead crucial to additionally ensure that \(\rho\approx 1\) in order to obtain interesting guarantees for the behavior of algorithms such as OGD.

### Bayesian mechanisms

Next, we leverage the connection between smoothness and convergence to Nash equilibria to extend the scope of our framework to Bayesian mechanisms. In particular, analogously to Definition 2.1, Syrgkanis and Tardos [102] have introduced the notion of a _smooth mechanism_ (Definition A.14); detailed background on Bayesian mechanisms and smoothness in that realm is provided in Appendix A.7. In this context, we leverage a reduction of Hartline et al. [53] from an incomplete-information to a complete-information mechanism to arrive at the following theorem. (The standard notion of a _Bayes-Nash equilibrium_ is analogous to Definition 2.2, and is recalled in Definition A.15 of Appendix A.7.)

**Theorem 3.8**.: _Consider a Bayesian mechanism \(\mathcal{M}\) such that \(\rho_{\mathcal{M}}=1\). Then, for any \(\epsilon>0\), \(T=O(1/\epsilon^{2})\) iterations of \(\mathtt{OGD}\) suffice to obtain an \(\epsilon\)-Bayes-Nash equilibrium of \(\mathcal{M}\)._

In particular, \(\mathtt{OGD}\) above is executed on the so-called _agent-form_ representation of \(\mathcal{M}\) (Appendix A.7). Analogously to Theorem 3.1, the above theorem can also be extended in the large \(n\gg 1\) under the assumption that \(\rho_{n}\to 1\). It is worth noting that Theorem 3.1 already can be applied to certain games of incomplete information (such as imperfect-information extensive-form games), but Theorem 3.8 additionally makes a connection with the literature on smoothness in mechanism design, which facilitates characterizing the smoothness parameters.

## 4 Improved welfare for no-regret dynamics

Roughgarden's seminal work [90] established that no-regret learning algorithms always attain asymptotically at least \(\mathsf{rPoA}\) fraction of the optimal social welfare (on average). This guarantee is satisfactory for many classes of games where \(\mathsf{rPoA}\) is close to \(1\) (emphatically those studied earlier in Section 3), but smoothness is certainly not a universal phenomenon: there are simple games in which the smoothness framework only provides vacuous guarantees; one such example is Shapley's game, discussed in Appendix A.10. As a result, one important question arising is whether it is possible to improve the efficiency bound predicted by smoothness when specific learning algorithms are in place, while at the same time still guaranteeing convergence to the set of _coarse correlated equilibria (CCE)_. We stress that optimizing over the set of CCE is typically NP-hard in succinct games [79; 6], making this question interesting also from a complexity-theoretic standpoint.

In this section, we show that it is indeed possible to obtain improved efficiency bounds under a generic condition, while at the same time guaranteeing the no-regret property for each player. A key ingredient in our improvement is the use of _clairvoyant_ mirror descent, an algorithm recently introduced by Piliouras et al. [85]. More precisely, we will instantiate that algorithm with (squared) Euclidean regularization, which can be defined as follows. Let \(\Pi_{\boldsymbol{x}_{i}}(\boldsymbol{u}_{i})\coloneqq\arg\max_{\boldsymbol{x} _{i}^{\prime}\in\mathcal{X}_{i}}\left\{\langle\boldsymbol{x}_{i}^{\prime}, \boldsymbol{u}_{i}\rangle-\frac{1}{2}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{i}^ {\prime}\|_{2}^{2}\right\}\) be the induced _prox operator_, where \(\boldsymbol{x}_{i}\in\mathcal{X}_{i}\) and \(\boldsymbol{u}_{i}\in\mathbb{R}^{d_{i}}\). _Clairvoyant gradient descent_ (henceforth \(\mathtt{CGD}\)) at time \(t\in\mathbb{N}\) outputs \(\boldsymbol{x}^{(t)}=\Pi_{\boldsymbol{x}^{(t-1)}}(\eta F(\boldsymbol{w}^{(t) }))\coloneqq(\Pi_{\boldsymbol{x}_{1}^{(t-1)}}(\eta\boldsymbol{u}_{1}( \boldsymbol{w}_{-1}^{(t)})),\ldots,\Pi_{\boldsymbol{x}_{n}^{(t-1)}}(\eta \boldsymbol{u}_{n}(\boldsymbol{w}_{-n}^{(t)})))\), where \(\boldsymbol{w}^{(t)}\) is any \(\epsilon^{(t)}\)-approximate fixed point of the map \(\prod_{i=1}^{n}\mathcal{X}_{i}\ni\boldsymbol{w}\mapsto\Pi_{\boldsymbol{x}^{(t-1 )}}(\eta F(\boldsymbol{w}))\), and \(\boldsymbol{x}^{(0)}\in\mathcal{X}\) is an arbitrary initialization. It turns out that for \(\eta<1/L\), this map is a _contraction_[37; 85; 19], thereby making approximate fixed points easy to compute. Furthermore, there is also an uncoupled implementation of \(\mathtt{CGD}\)[85], making the algorithm compelling from a decentralized standpoint as well, but we will not dwell on this issue here. We are now ready to state the main result of this section.

**Theorem 4.1**.: _Suppose that all players are updating their strategies using \(\mathtt{CGD}\) with \(\epsilon^{(t)}\leq\frac{\min_{i}D_{\mathcal{X}_{i}}}{t^{2}}\) and learning rate \(\eta=\frac{1}{2L}\) in a \((\lambda,\mu)\)-smooth game \(\mathcal{G}\), where \(L\) is the Lipschitz-continuity parameter of \(F\). Then, for any \(\epsilon_{0}>0\) and \(T\geq\frac{64L^{2}D_{\mathcal{X}}^{4}}{\epsilon_{0}^{2}}\) iterations,_

1. _the average correlated distribution of play is a_ \(\frac{4LD_{\mathcal{X}}^{2}}{T}-\text{CCE}\)_;_
2. _there is a time_ \(t^{\star}\in[\![T]\!]\) _such that_ \[\operatorname{SW}(\boldsymbol{x}^{(t^{\star})})\geq\sup_{\epsilon\geq\epsilon _{0}}\min\left\{\rho_{\mathcal{G}}(\lambda,\mu)\cdot\mathsf{OPT}_{\mathcal{G}}+ \frac{\epsilon^{2}}{16(\mu+1)LD_{\mathcal{X}}^{2}},\mathsf{PoA}_{\mathcal{G}}^ {\epsilon}\cdot\mathsf{OPT}_{\mathcal{G}}\right\}.\] (3)

This is the first result that establishes simultaneously these properties under a computationally efficient algorithm, improving a recent work [1] (see also [42; 67] for related results) that failed toguarantee convergence to CCE. In particular, that earlier work was analyzing \(\mathtt{OGD}\), and as it turns out, under a time-invariant learning rate \(\eta\) it is not even known whether \(\mathtt{OGD}\) ensures _sublinear_ per-player regret, let alone constant (as in Corollary 4.3). The basic ingredient to this improvement is a new property of \(\mathtt{CGD}\), which we explain below. Before we sketch the proof, we note that Item 2 above can be readily strengthened so that the improvement holds for the average welfare of most of the strategies, not just a single one (Remark A.22).

Proof sketch of Theorem 4.1The key step in the proof is showing (in Corollary A.21) that \(\mathtt{CGD}\) satisfies the remarkable per-player regret bound \(\mathsf{Reg}_{i}^{(T)}\leq\alpha-\gamma\sum_{t=1}^{T}\big{(}\mathsf{BRG} \mathsf{G}\mathsf{G}\mathsf{P}_{i}(\bm{x}^{(t)})\big{)}^{2}\), where \(\alpha>0\) depends on the approximation error of the fixed points of \(\mathtt{CGD}\)--and can be made time-invariant with only an \(O(\log T)\) per-iteration overhead--and \(\gamma>0\). To do this, we crucially rely on a certain property of the Euclidean regularizer (Lemma A.20), which we use in conjunction with the analysis of Farina et al. [37] who extended the original argument of Piliouras et al. [85] beyond entropic regularization.

It is worth noting that the above per-player regret bound (Corollary A.21) implies that a player with nonnegative regret will be almost always approximately best responding, a rather singular occurrence in the context of learning in games; this has interesting implications and goes well-beyond what is currently known for \(\mathtt{OGD}\). In particular, it is an open question whether Theorem 4.1 holds under \(\mathtt{OGD}\).

Next, we shall describe a concrete implication of Theorem 4.1 under a generic condition. To do so, let us denote by \(\mathsf{PoA}_{\mathcal{G}}^{c}\) the price of anarchy in \(\mathcal{G}\) with respect to the worst-case \(\epsilon\)-Nash equilibrium (so that \(\mathsf{PoA}_{\mathcal{G}}^{0}\equiv\mathsf{PoA}_{\mathcal{G}}\)).

**Condition 4.2**.: _Consider a game \(\mathcal{G}\) and some game-dependent parameter \(C=C(\mathcal{G})>0\). There exists an \(\epsilon_{0}>0\) such that \(\mathsf{PoA}_{\mathcal{G}}^{\epsilon_{0}}>\mathsf{rPoA}_{\mathcal{G}}+ \epsilon_{0}^{2}C\)._

Naturally, it is always the case that \(\mathsf{PoA}_{\mathcal{G}}\geq\mathsf{rPoA}_{\mathcal{G}}\). Further, \(\mathsf{rPoA}_{\mathcal{G}}\) is in general strictly smaller since it measures the worst-case welfare over a larger set than \(\mathsf{PoA}_{\mathcal{G}}\) (even broader than outcomes of no-regret learning); Figure 1 in Appendix A.8 further corroborates this premise in a sequence of random normal-form games. Now assuming that \(\mathsf{PoA}_{\mathcal{G}}>\mathsf{rPoA}_{\mathcal{G}}\), Condition 4.2 is met if \(\lim_{\epsilon\to 0}\mathsf{PoA}_{\mathcal{G}}^{\epsilon}=\mathsf{PoA}_{ \mathcal{G}}\), a mild continuity condition (see, for example, the discussion by Roughgarden [89]).

**Corollary 4.3**.: _Consider a \((\lambda,\mu)\)-smooth game \(\mathcal{G}\) that satisfies Condition 4.2 under some \(\epsilon_{0}>0\). Then, \(\mathtt{CGD}\) after \(T\geq\frac{64L^{2}D_{\lambda}^{4}}{\epsilon_{0}^{2}}\) iterations and \(\eta=\frac{1}{2L}\) satisfies the following:_

1. _the average correlated distribution of play is an_ \(O\left(\frac{1}{T}\right)\)_-CCE;_
2. _there is a time_ \(t^{\star}\in[\![T]\!]\) _and_ \(C^{\prime}(\mathcal{G})>0\) _such that_ \(\mathsf{SW}(\bm{x}^{(t^{\star})})\geq(\rho_{\mathcal{G}}(\lambda,\mu)+\epsilon _{0}^{2}C^{\prime}(\mathcal{G}))\mathsf{OPT}_{\mathcal{G}}\)_._

A fundamental question that arises from Theorem 4.1 is whether there exists a computationally efficient algorithm that determines a CCE with social welfare at least a \(\mathsf{PoA}\) fraction of the optimal welfare.3 In games where \(\mathsf{PoA}=\mathsf{rPoA}\) this is clearly possible; in contrast, while Theorem 4.1 improves over the smoothness bound, it does not always guarantee welfare up to \(\mathsf{PoA}\). This is a central question in light of the intractability of Nash equilibria [30, 20], which has indeed served as a primary critique to the literature quantifying the price of anarchy of Nash equilibria [90].

Footnote 3: We clarify that Theorem 4.1 could have also been stated as follows: \(\mathtt{CGD}\) outputs an approximate CCE with social welfare attaining the right-hand side of (3); this is evident from the proof in Appendix A.9.

Another promising avenue to improving the welfare predicted by the smoothness framework revolves around eliminating certain strategy profiles by arguing that they are reached with negligible probability. For example, in Appendix A.10 we identify an example where iteratively eliminating strictly dominated actions can improve the predictive power of the smoothness framework.

## 5 Further related work

Large gamesThe study of non-cooperative games with many players (_i.e._, large games) has been a classical topic in economic theory [96, 41, 68, 95, 83, 38, 50], most recently revived in the context of _mean-field games_ (_e.g._, [64, 47, 74, 51, 16, 82, 100, 80, 29, 65]). Indeed, many traditional motivating scenarios in algorithmic game theory, including markets and Internet routing, often feature a large number of players in practice. In particular, it has emerged that, under certain conditions, equilibria in large games exhibit certain remarkable robustness and stability properties; see, for example, the recent survey of Gradwohl and Kalai [49], as well as the older treatment of Kalai [56] on the subject. Furthermore, mechanism design in large games, along with privacy guarantees, is explored in the work of Kearns et al. [58] (see also [57, 59]).

Efficiency in large gamesOf particular importance to our work, and specifically the precondition of Theorem 3.1, is the line of work uncovering the by now well-documented phenomenon in economics that large games exhibit, under certain relatively mild assumptions, fully efficient equilibria. Our framework additionally requires that the efficiency of equilibria can be derived via a smoothness argument, in the sense of Roughgarden [90]; we stress again that efficiency alone is of little use when it comes to equilibrium computation (Proposition 3.7). Fortunately, smoothness has emerged as the canonical paradigm for bounding the price of anarchy (_e.g._, see the survey of Roughgarden et al. [93]), albeit with some notable exceptions [40, 55]. In particular, Feldman et al. [39] quantify the price of anarchy in large games via the smoothness framework. They show that in a general combinatorial domain with simultaneous uniform-price auctions, it holds that \(\rho_{n}\to 1\) with a rate of \(1/\sqrt{n}\) as long as there is _probabilistic demand_, meaning that every buyer abstains from the auction with a constant probability. Several other papers have studied the price of anarchy in large games [63, 23, 24, 26, 25, 17]. In particular, we highlight the work of Cole and Tao [23] which, as Feldman et al. [39], relies on a smoothness argument to establish full efficiency in the limit with a rate of \(1/\sqrt{n}\) in a Walrasian auction, while asymptotic full efficiency is also shown for Fisher markets under the gross substitutes condition. Further, Carmona et al. [17] provide sufficient conditions under which equilibria are fully efficient in a class of mean-field games; understanding thus whether our framework has new implications in such games is an interesting direction for the future. We finally point out that many other papers have focused on learning in auctions and markets; see [21, 43, 104, 8, 9, 32], and references therein.

## 6 Conclusions and future work

In conclusion, we have furnished a new sufficient condition under which a family of no-regret learning algorithms, including optimistic gradient descent (OGD), approaches (weak) Nash equilibria. Our criterion has a natural economic interpretation, being intricately connected with Roughgarden's smoothness framework, and captures other well-studied conditions such as the Minty property. We have also shown that _clairvoyant_ gradient descent attains an improved welfare bound compared to that predicted by the smoothness framework, while ensuring at the same time fast convergence to the set of CCE.

There are many promising directions for future work. First, we have seen that under the condition \(\rho=1\) there exists an algorithm that computes an \(\epsilon\)-NE in time \(\mathsf{poly}(1/\epsilon)\), leading to a _pseudo_ polynomial-time algorithm (under natural game representations); is there an algorithm that instead runs in time \(\mathsf{poly}(\log(1/\epsilon))\)?

Convergence to Nash equilibria via computational hardness?Another promising approach for showing convergence to Nash equilibria is by harnessing computational hardness results for the underlying welfare maximization problem. To be specific, we consider the following condition.

**Condition 6.1**.: _Consider a multi-player \((\lambda,\mu)\)-smooth game \(\mathcal{G}\) with \(\rho_{\mathcal{C}}\coloneqq\frac{\lambda}{1+\mu}\) from a class of games \(\mathcal{C}\) with the polynomial expectation property [79]. For any \(\mathcal{G}\in\mathcal{C}\), computing a joint strategy profile \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) such that \(\mathsf{SW}(\bm{x})\geq\rho_{\mathcal{C}}\cdot\mathsf{OPT}_{\mathcal{G}}+1/ \mathsf{poly}(\mathcal{G})\) is \(\mathsf{NP}\)-hard, for any \(\mathsf{poly}(\mathcal{G})\)._

Indeed, smoothness often circumscribes the welfare of polynomial algorithms, such as combinatorial auctions under XOS valuations--in fact, unconditionally under polynomial communication; see [34, Theorem 1.4] and [102, Appendix A.7]. Now, the role of Condition 6.1 is that (unless \(\mathsf{P}=\mathsf{NP}\)) a polynomially-bounded algorithm such as OGD--which is efficiently implementable (for games with a polynomial number of actions) under the polynomial expectation property--will have the property that \(\frac{1}{T}\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)}\geq-1/\mathsf{poly}(\mathcal{G})\), for any \(\mathcal{G}\in\mathcal{C}\) and \(\mathsf{poly}(\mathcal{G})\), which in turn leads to the following.

**Theorem 6.2**.: _Consider a class \(\mathcal{C}\) satisfying Condition 6.1. For any \(\mathcal{G}\in\mathcal{C}\) and \(\epsilon=1/\mathsf{poly}(\mathcal{G})\), there is a polynomial-time algorithm for computing an \(\epsilon\)-Nash equilibrium, unless \(\mathsf{P}=\mathsf{NP}\)._

By virtue of Corollary 4.3, the same conclusion applies even under the weaker condition that computing a CCE with welfare improving over the smoothness bound is hard; this is related to the hardness result of Barman and Ligett [6], discussed in the full version of this paper.

## Acknowledgments and Disclosure of Funding

We are grateful to anonymous reviewers and the area chair at NeurIPS for valuable feedback. We also thank Brendan Lucier for helpful pointers to the literature. This material is based on work supported by the Vannevar Bush Faculty Fellowship ONR N00014-23-1-2876, National Science Foundation grants RI-2312342 and RI-1901403, ARO award W911NF2210266, and NIH award A240108S001.

## References

* [1] I. Anagnostides, I. Panageas, G. Farina, and T. Sandholm. On last-iterate convergence beyond zero-sum games. In _International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 536-581. PMLR, 2022.
* [2] I. Arieli and Y. Babichenko. Simple approximate equilibria in games with many players. In _ACM Conference on Economics and Computation (EC)_, pages 681-691. ACM, 2017.
* [3] Y. Azrieli and E. Shmaya. Lipschitz games. _Mathematics of Operation Research_, 38(2):350-357, 2013.
* [4] Y. Babichenko and A. Rubinstein. Communication complexity of approximate nash equilibria. _Games and Economic Behavior_, 134:376-398, 2022.
* [5] Y. Babichenko, C. H. Papadimitriou, and A. Rubinstein. Can almost everybody be almost happy? In _Conference on Innovations in Theoretical Computer Science (ITCS)_, pages 1-9. ACM, 2016.
* [6] S. Barman and K. Ligett. Finding any nontrivial coarse correlated equilibrium is hard. In _ACM Conference on Economics and Computation (EC)_, pages 815-816. ACM, 2015.
* [7] M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold'em poker is solved. _Science_, 347(6218), Jan. 2015.
* [8] S. Branzei. Exchange markets: proportional response dynamics and beyond. _SIGecom Exch._, 19(2):37-45, 2021.
* [9] S. Branzei, N. R. Devanur, and Y. Rabani. Proportional dynamics in exchange economies. In _ACM Conference on Economics and Computation (EC)_, pages 180-201. ACM, 2021.
* [10] N. Brown and T. Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. _Science_, 359(6374):418-424, 2018.
* [11] A. Bohm. Solving nonconvex-nonconcave min-max problems exhibiting weak minty solutions, 2022.
* [12] Y. Cai and W. Zheng. Accelerated single-call methods for constrained min-max optimization. _CoRR_, abs/2210.03096, 2022.
* [13] Y. Cai, O. Candogan, C. Daskalakis, and C. Papadimitriou. Zero-sum polymatrix games: A generalization of minmax. _Mathematics of Operations Research_, 41(2):648-655, 2016.
* [14] Y. Cai, A. Oikonomou, and W. Zheng. Accelerated algorithms for monotone inclusions and constrained nonconvex-nonconcave min-max optimization. _CoRR_, abs/2206.05248, 2022.
* [15] Y. Cai, A. Oikonomou, and W. Zheng. Finite-time last-iterate convergence for learning in multi-player games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [16] L. Campi and M. Fischer. Correlated equilibria and mean field games: A simple model. _Mathematics of Operations Research_, 47(3):2240-2259, 2022.
* [17] R. Carmona, C. V. Graves, and Z. Tan. Price of anarchy for mean field games. _ESAIM: Proceedings and Surveys_, 65:349-383, 2019.

* [18] N. Cesa-Bianchi and G. Lugosi. _Prediction, learning, and games_. Cambridge University Press, 2006.
* [19] V. Cevher, G. Piliouras, R. Sim, and S. Skoulakis. Min-max optimization made simple: Approximating the proximal point method via contraction maps. In _Symposium on Simplicity in Algorithms (SOSA)_, pages 192-206. SIAM, 2023.
* [20] X. Chen, X. Deng, and S.-H. Teng. Settling the complexity of computing two-player Nash equilibria. _Journal of the ACM_, 2009.
* [21] Y. K. Cheung, S. Leonardos, and G. Piliouras. Learning in markets: Greed leads to chaos but following the price is right. In _International Joint Conference on Artificial Intelligence (IJCAI)_, pages 111-117. ijcai.org, 2021.
* [22] C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In _Conference on Learning Theory (COLT)_, pages 6-1, 2012.
* [23] R. Cole and Y. Tao. Large market games with near optimal efficiency. In _ACM Conference on Economics and Computation (EC)_, pages 791-808. ACM, 2016.
* [24] R. Colini-Baldeschi, R. Cominetti, P. Mertikopoulos, and M. Scarsini. The asymptotic behavior of the price of anarchy. In _International Workshop On Internet And Network Economics (WINE)_, pages 133-145. Springer, 2017.
* [25] R. Colini-Baldeschi, R. Cominetti, and M. Scarsini. Price of anarchy for highly congested routing games in parallel networks. _Theory of Computing Systems_, 63:90-113, 2019.
* [26] R. Colini-Baldeschi, R. Cominetti, P. Mertikopoulos, and M. Scarsini. When is selfish routing bad? the price of anarchy in light and heavy traffic. _Operations Research_, 68(2):411-434, 2020.
* [27] V. Conitzer and T. Sandholm. Complexity of constructing solutions in the core based on synergies among coalitions. _Artificial Intelligence_, 170(6-7):607-619, 2006. Earlier version in IJCAI-03.
* [28] V. Conitzer and T. Sandholm. New complexity results about Nash equilibria. _Games and Economic Behavior_, 63(2):621-641, 2008. Early version in IJCAI-03.
* [29] K. Cui and H. Koeppl. Learning graphon mean field games and approximate nash equilibria. In _International Conference on Learning Representations (ICLR)_. OpenReview.net, 2022.
* [30] C. Daskalakis, P. Goldberg, and C. Papadimitriou. The complexity of computing a Nash equilibrium. In _Symposium on Theory of Computing (STOC)_, 2006.
* [31] J. Deb and E. Kalai. Stability in large bayesian games with heterogeneous players. _Journal of Economic Theory_, 157:1041-1055, 2015.
* [32] X. Deng, X. Hu, T. Lin, and W. Zheng. Nash convergence of mean-based learning algorithms in first price auctions. In _WWW '22: The ACM Web Conference_, pages 141-150. ACM, 2022.
* [33] J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 130 of _Proceedings of Machine Learning Research_, pages 2746-2754. PMLR, 2021.
* [34] S. Dobzinski, N. Nisan, and M. Schapira. Approximation algorithms for combinatorial auctions with complement-free bidders. _Mathematics of Operation Research_, 35(1):1-13, 2010.
* [35] K. Etessami and M. Yannakakis. On the complexity of Nash equilibria and other fixed points (extended abstract). In _Annual Symposium on Foundations of Computer Science (FOCS)_, pages 113-123, 2007.
* [36] F. Facchinei and J.-S. Pang. _Finite-dimensional variational inequalities and complementarity problems_. Springer, 2003.

* [37] G. Farina, C. Kroer, C. Lee, and H. Luo. Clairvoyant regret minimization: Equivalence with memirovski's conceptual prox method and extension to general convex games. _CoRR_, abs/2208.14891, 2022.
* [38] T. Feddersen and W. Pesendorfer. Voting behavior and information aggregation in elections with private information. _Econometrica_, 65(5):1029-1058, 1997.
* [39] M. Feldman, N. Immorlica, B. Lucier, T. Roughgarden, and V. Syrgkanis. The price of anarchy in large games. In _Symposium on Theory of Computing (STOC)_, pages 963-976. ACM, 2016.
* [40] M. Feldman, H. Fu, N. Gravin, and B. Lucier. Simultaneous auctions without complements are (almost) efficient. _Games and Economic Behavior_, 123:327-341, 2020.
* [41] D. Fudenberg and D. K. Levine. Open-loop and closed-loop equilibria in dynamic games with many players. _Journal of Economic Theory_, 44(1):1-18, 1988.
* Leibniz-Zentrum fur Informatik, 2023.
* [43] Y. Gao. _New Optimization Models and Methods for Classical, Infinite-Dimensional, and Online Fisher Markets_. Columbia University, 2022.
* [44] I. Gilboa and E. Zemel. Nash and correlated equilibria: Some complexity considerations. _Games and Economic Behavior_, 1:80-93, 1989.
* [45] P. W. Goldberg and M. J. Katzman. Lower bounds for the query complexity of equilibria in lipschitz games. _Theoretical Comput. Sci._, 962, 2023.
* [46] N. Golowich, S. Pattathil, and C. Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [47] D. A. Gomes and J. Saude. Mean field games models--a brief survey. _Dynamic Games and Applications_, 4:110-154, 2014.
* [48] E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: \(O(1/K)\) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 151 of _Proceedings of Machine Learning Research_, pages 366-402. PMLR, 2022.
* [49] R. Gradwohl and E. Kalai. Large games: Robustness and stability. _Annual Review of Economics_, 13:39-56, 2021.
* [50] R. Gradwohl and O. Reingold. Fault tolerance in large games. _Games and Economic Behavior_, 86:438-457, 2014.
* [51] Y. Guan, M. Afshari, and P. Tsiotras. Zero-sum games between mean-field teams: A common information and reachability based analysis. _CoRR_, abs/2303.12243, 2023.
* [52] J. C. Harsanyi, R. Selten, et al. A general theory of equilibrium selection in games. _MIT Press Books_, 1, 1988.
* [53] J. D. Hartline, V. Syrgkanis, and E. Tardos. No-regret learning in bayesian games. In _Conference on Neural Information Processing Systems (NeurIPS)_, pages 3061-3069, 2015.
* [54] W. Hoeffding. Probability inequalities for sums of bounded random variables. _Journal of the American Statistical Association_, 58(301):13-30, 1963.
* [55] Y. Jin and P. Lu. First price auction is \(1-1/e^{2}\) efficient. In _Annual Symposium on Foundations of Computer Science (FOCS)_, pages 179-187. IEEE, 2022.
* [56] E. Kalai. Large robust games. _Econometrica_, 72(6):1631-1665, 2004.

* [57] M. J. Kearns and Y. Mansour. Efficient nash computation in large population games with bounded influence. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 259-266. Morgan Kaufmann, 2002.
* [58] M. J. Kearns, M. M. Pai, A. Roth, and J. R. Ullman. Mechanism design in large games: incentives and privacy. In _Innovations in Theoretical Computer Science (ITCS)_, pages 403-410. ACM, 2014.
* [59] M. J. Kearns, M. M. Pai, R. M. Rogers, A. Roth, and J. R. Ullman. Robust mediators in large games. _CoRR_, abs/1512.02698, 2015.
* [60] D. Kim, M. Riemer, M. Liu, J. N. Foerster, M. Everett, C. Sun, G. Tesauro, and J. P. How. Influencing long-term behavior in multiagent reinforcement learning. _CoRR_, abs/2203.03535, 2022.
* [61] E. Koutsoupias and C. Papadimitriou. Worst-case equilibria. In _Symposium on Theoretical Aspects in Computer Science (STACS)_, 1999.
* [62] J. Kulkarni and V. S. Mirrokni. Robust price of anarchy bounds via LP and fenchel duality. In _Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 1030-1049. SIAM, 2015.
* [63] D. Lacker and K. Ramanan. Rare nash equilibria and the price of anarchy in large static games. _Mathematics of Operations Research_, 44(2):400-422, 2019.
* [64] J.-M. Lasry and P.-L. Lions. Mean field games. _Japanese journal of mathematics_, 2(1):229-260, 2007.
* [65] M. Lauriere, S. Perrin, S. Girgin, P. Muller, A. Jain, T. Cabannes, G. Piliouras, J. Perolat, R. Elie, O. Pietquin, and M. Geist. Scalable deep reinforcement learning algorithms for mean field games. In _International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 12078-12095. PMLR, 2022.
* [66] T. Lin, Z. Zhou, P. Mertikopoulos, and M. I. Jordan. Finite-time last-iterate convergence for multi-agent learning in games. In _International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 6161-6171. PMLR, 2020.
* [67] B. Lucier, S. Pattathil, A. Slivkins, and M. Zhang. Autobidders with budget and ROI constraints: Efficiency, regret, and pacing dynamics. _CoRR_, abs/2301.13306, 2023.
* [68] G. J. Mailath and A. Postlewaite. Asymmetric information bargaining problems with many agents. _The Review of Economic Studies_, 57(3):351-367, 1990.
* [69] F. McSherry and K. Talwar. Mechanism design via differential privacy. In _Annual Symposium on Foundations of Computer Science (FOCS)_, pages 94-103. IEEE Computer Society, 2007.
* [70] P. Mertikopoulos, C. H. Papadimitriou, and G. Piliouras. Cycles in adversarial regularized learning. In _Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2703-2717. SIAM, 2018.
* [71] P. Mertikopoulos, B. Lecouat, H. Zenati, C. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In _International Conference on Learning Representations (ICLR)_. OpenReview.net, 2019.
* [72] P. Milgrom and R. Weber. Distributional strategies for games with incomplete information. _Mathematics of Operations Research_, 10:619-632, 1985.
* [73] J. Milionis, C. H. Papadimitriou, G. Piliouras, and K. Spendlove. Nash, conley, and computation: Impossibility and incompleteness in game dynamics. _CoRR_, abs/2203.14129, 2022.
* [74] P. Muller, M. Rowland, R. Elie, G. Piliouras, J. Perolat, M. Lauriere, R. Marinier, O. Pietquin, and K. Tuyls. Learning equilibria in mean-field games: Introducing mean-field PSRO. In _Autonomous Agents and Multi-Agent Systems_, pages 926-934. International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2022.

* [75] U. Nadav and T. Roughgarden. The limits of smoothness: A primal-dual framework for price of anarchy bounds. In _International Workshop On Internet And Network Economics (WINE)_, volume 6484 of _Lecture Notes in Computer Science_, pages 319-326. Springer, 2010.
* [76] J. Nash. Equilibrium points in n-person games. _National Academy of Sciences_, 36:48-49, 1950.
* [77] A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1), 2004.
* Leibniz-Zentrum fur Informatik, 2019.
* [79] C. H. Papadimitriou and T. Roughgarden. Computing correlated equilibria in multi-player games. _Journal of the ACM_, 55(3):14:1-14:29, 2008.
* [80] J. Perolat, S. Perrin, R. Elie, M. Lauriere, G. Piliouras, M. Geist, K. Tuyls, and O. Pietquin. Scaling mean field games by online mirror descent. In _Autonomous Agents and Multi-Agent Systems_, pages 1028-1037. International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS), 2022.
* [81] J. Perolat, B. D. Vylder, D. Hennes, E. Tarassov, F. Strub, V. de Boer, P. Muller, J. T. Connor, N. Burch, T. Anthony, S. McAleer, R. Elie, S. H. Cen, Z. Wang, A. Gruslys, A. Malysheva, M. Khan, S. Ozair, F. Timbers, T. Pohlen, T. Eccles, M. Rowland, M. Lanctot, J.-B. Lespiau, B. Piot, S. Omidshatiei, E. Lockhart, L. Sifre, N. Beauguerrange, R. Munos, D. Silver, S. Singh, D. Hassabis, and K. Tuyls. Mastering the game of strategy with model-free multiagent reinforcement learning. _Science_, 378(6623):990-996, 2022.
* [82] S. Perrin, M. Lauriere, J. Perolat, R. Elie, M. Geist, and O. Pietquin. Generalization in mean field games by learning master policies. In _AAAI Conference on Artificial Intelligence (AAAI)_, pages 9413-9421. AAAI Press, 2022.
* [83] W. Pesendorfer and J. M. Swinkels. The loser's curse and information aggregation in common value auctions. _Econometrica_, 65(6):1247-1281, 1997.
* [84] T. Pethick, P. Latafat, P. Patrinos, O. Fercoq, and V. Cevher. Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems. _CoRR_, abs/2302.09831, 2023.
* [85] G. Piliouras, R. Sim, and S. Skoulakis. Beyond time-average convergence: Near-optimal uncoupled online learning via clairvoyant multiplicative weights update. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [86] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In _Conference on Learning Theory_, pages 993-1019, 2013.
* [87] S. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences. In _Advances in Neural Information Processing Systems_, pages 3066-3074, 2013.
* [88] J. B. Rosen. Existence and uniqueness of equilibrium points for concave n-person games. _Econometrica_, 33(3):520-534, 1965.
* [89] T. Roughgarden. Barriers to near-optimal equilibria. In _Annual Symposium on Foundations of Computer Science (FOCS)_, pages 71-80. IEEE Computer Society, 2014.
* [90] T. Roughgarden. Intrinsic robustness of the price of anarchy. _Journal of the ACM_, 62(5):32:1-32:42, 2015.
* [91] T. Roughgarden. The price of anarchy in games of incomplete information. _ACM Trans. Economics and Comput._, 3(1):6:1-6:20, 2015.

* [92] T. Roughgarden and F. Schoppmann. Local smoothness and the price of anarchy in splittable congestion games. _Journal of Economic Theory_, 156:317-342, 2015.
* [93] T. Roughgarden, V. Syrgkanis, and E. Tardos. The price of anarchy in auctions. _Journal of Artificial Intelligence Research_, 59:59-101, 2017.
* [94] A. Rubinstein. Inapproximability of nash equilibrium. _SIAM Journal on Computing_, 47(3):917-959, 2018.
* [95] A. Rustichini, M. Satterthwaite, and S. Williams. Convergence to efficiency in a simple market with incomplete information. _Econometrica_, 62:1041-1063, 1994.
* [96] D. Schmeidler. Equilibrium points of nonatomic games. _Journal of statistical Physics_, 7:295-300, 1973.
* [97] P. G. Sessa, I. Bogunovic, A. Krause, and M. Kamgarpour. Contextual games: Multi-agent learning with side information. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020.
* [98] S. Shalev-Shwartz. Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2), 2012. ISSN 1935-8237.
* [99] L. S. Shapley. Some topics in two-person games. In M. Drescher, L. S. Shapley, and A. W. Tucker, editors, _Advances in Game Theory_. Princeton University Press, 1964.
* [100] S. G. Subramanian, M. E. Taylor, M. Crowley, and P. Poupart. Decentralized mean field games. In _AAAI Conference on Artificial Intelligence (AAAI)_, pages 9439-9447. AAAI Press, 2022.
* [101] V. Syrgkanis. Bayesian games and the smoothness framework. _CoRR_, abs/1203.5155, 2012.
* [102] V. Syrgkanis and E. Tardos. Composable and efficient mechanisms. In _Symposium on Theory of Computing (STOC)_, pages 211-220. ACM, 2013.
* [103] V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in games. In _Advances in Neural Information Processing Systems_, pages 2989-2997, 2015.
* [104] Y. Tao. _Market Efficiency and Dynamics_. PhD thesis, New York University, 2020.
* [105] E. Vlatakis-Gkaragkounis, L. Flokas, T. Lianeas, P. Mertikopoulos, and G. Piliouras. No-regret learning and mixed nash equilibria: They do not mix. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020_, 2020.

Omitted proofs

In this section, we provide the proofs and a number of results omitted from the main body.

### Proof of Theorem 3.1

We commence with the proof of Theorem 3.1. Below, we give a more detailed version of the statement provided earlier in the main body. We first state an auxiliary lemma which will be useful for the proof, and can be extracted from earlier work [1]

**Lemma A.1**.: _Suppose that player \(i\in\llbracket n\rrbracket\) updates its strategy using \(\mathtt{OGD}\) with learning rate \(\eta>0\). Then, for any time \(t\in\mathbb{N}\),_

\[\mathtt{BRGAP}_{i}(\bm{x}^{(t)})\leq\left(\frac{D_{\mathcal{X}_{i}}}{\eta}+ \|\bm{u}_{i}^{(t)}\|_{2}\right)\left(\|\bm{x}_{i}^{(t)}-\hat{\bm{x}}_{i}^{(t)} \|_{2}+\|\bm{x}_{i}^{(t)}-\hat{\bm{x}}_{i}^{(t+1)}\|_{2}\right).\]

In the sequel, we denote by \(B_{i}\in\mathbb{R}_{>0}\) any number such that \(\|\bm{u}_{i}(\bm{x}_{-i})\|_{2}\leq B_{i}\), for any \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\). In the asymptotic notation below, we make the standard assumption that the parameters \(B_{i}\) and \(D_{\mathcal{X}_{i}}\) do not depend on the number of players \(n\).

**Theorem A.2** (Precise version of Theorem 3.1).: _Consider an \(n\)-player \((\lambda,\mu)\)-smooth game \(\mathcal{G}_{n}\) such that the game operator \(F_{n}\) is \(L_{n}\)-Lipschitz continuous and \(\lambda\geq(1-\epsilon_{n})(1+\mu)\) (\(\rho_{n}\geq 1-\epsilon_{n}\)). Suppose further that all players follow \(\mathtt{OGD}\) with learning rate \(\eta_{n}=\frac{1}{4L_{n}}\) and any initialization \((\hat{\bm{x}}_{1}^{(1)},\ldots,\hat{\bm{x}}_{n}^{(1)})\in\prod_{i=1}^{n} \mathcal{X}_{i}\). If \(\epsilon_{n}>0\), then after \(T\coloneqq\frac{D_{\mathcal{X}_{i}}^{2}}{2\eta_{n}\epsilon_{n}(1+\mu)\text{ OPT}_{\mathcal{G}_{n}}}\) iterations there is a time \(t^{\star}\in\llbracket T\rrbracket\) such that_

\[\sum_{i=1}^{n}\left(\mathtt{BRGAP}_{i}(\bm{x}^{(t^{\star})}) \right)^{2} \leq 32\left(\frac{\max_{1\leq i\leq n}D_{\mathcal{X}_{i}}^{2}}{( \eta_{n})^{2}}+\max_{1\leq i\leq n}B_{i}^{2}\right)\eta_{n}\epsilon_{n}(1+\mu )\text{OPT}_{\mathcal{G}_{n}}\] (4) \[=O_{n}\left(L_{n}\text{OPT}_{\mathcal{G}_{n}}\epsilon_{n}\right).\] (5)

_In particular, for \(\delta\in\{\frac{1}{n},\frac{2}{n},\ldots,1\}\), \(t^{\star}\) constitutes a_

\[\left(\frac{1}{\sqrt{\delta}}O_{n}\left(\sqrt{\frac{L_{n}\text{OPT}_{\mathcal{ G}_{n}}\epsilon_{n}}{n}}\right),\delta\right)-\text{weak Nash equilibrium}.\]

_On the other hand, if \(\epsilon_{n}=0\), then for any \(T\in\mathbb{N}\) there is a time \(t^{\star}\in\llbracket T\rrbracket\) such that_

\[\sum_{i=1}^{n}\left(\mathtt{BRGAP}_{i}(\bm{x}^{(t^{\star})})\right)^{2}\leq \frac{8}{T}\left(\frac{\max_{1\leq i\leq n}D_{\mathcal{X}_{i}}^{2}}{(\eta_{n}) ^{2}}+\max_{1\leq i\leq n}B_{i}^{2}\right)D_{\mathcal{X}}^{2}.\]

Before we proceed with the proof, we note that the underlying assumption \(\mu=O_{n}(1)\) in the asymptotic notation above is consistent with known smoothness bounds in large games [39]; we also refer to Remark A.10 for an important point regarding the range of the smoothness parameters.

Proof of Theorem a.2.: We will first translate the assumed property \(\rho_{n}\geq 1-\epsilon_{n}\) to a lower bound on the sum of the players' regrets. In particular, we have that the sum of the players' regrets \(\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)}\) for any \(T\in\mathbb{N}\) can be expressed as

\[\sum_{i=1}^{n}\max_{\bm{x}_{i}^{\star}\in\mathcal{X}_{i}}\left\{\sum_{t=1}^{T}u _{i}(\bm{x}_{i}^{\star},\bm{x}_{-i}^{(t)})\right\}-\sum_{t=1}^{T}\mathsf{SW}( \bm{x}^{(t)})\geq\lambda\text{OPT}_{\mathcal{G}_{n}}T-(1+\mu)\sum_{t=1}^{T} \mathsf{SW}(\bm{x}^{(t)}),\] (6)

by Definition 2.1, where \((\lambda,\mu)\) are the assumed smoothness parameter of \(\mathcal{G}_{n}\). Now, using the assumption that \(\rho_{n}\geq 1-\epsilon_{n}\), it follows from (6) that \(\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)}\) is in turn lower bounded by

\[(1-\epsilon_{n})(1+\mu)\text{OPT}_{\mathcal{G}_{n}}T-(1+\mu)\sum_{t=1}^{T} \mathsf{SW}(\bm{x}^{(t)})\geq-\epsilon_{n}(1+\mu)\text{OPT}_{\mathcal{G}_{n}}T,\] (7)

[MISSING_PAGE_FAIL:18]

_Remark A.3_.: One can also introduce a variant of weak Nash equilibria which is instead parameterized by the average over the players' best response gaps \(\frac{1}{n}\sum_{i=1}^{n}\textsc{BRGap}_{i}(\cdot)\). Inequality (4) implies that the average best response gap of \(\bm{x}^{(t^{\star})}\) can be bounded by \(O_{n}\left(\sqrt{\frac{L_{n}\textsc{OPT}_{\mathcal{G}_{n}}\epsilon_{n}}{n}}\right)\).

_Remark A.4_.: While Theorem A.2 bounds the (weak) Nash equilibrium gap of a single iterate of the dynamics, (11) implies that at least a \(1-\gamma\) fraction of the iterates of the dynamics constitutes a \(\left(\frac{1}{\sqrt{\delta\gamma}}O_{n}\left(\sqrt{\frac{L_{n}\textsc{OPT}_{ \mathcal{G}_{n}}\epsilon_{n}}{n}}\right),\delta\right)\)-weak Nash equilibrium. So, by selecting a time index \(t^{\star}\in\llbracket T\rrbracket\) uniformly at random we obtain the desired guarantee with high probability, incurring only a small degradation in the solution quality.

In particular, if \(\epsilon_{n}\) approaches to \(0\) with a sufficiently fast rate, Theorem A.2 also implies convergence to the more standard notion of Nash equilibrium (_i.e._, Definition 2.2 with \(\delta\coloneqq 0\)), as we state below. In particular, the following corollary can be derived directly from (4).

**Corollary A.5**.: _In the setting of Theorem A.2, if it additionally holds that \(L_{n}\epsilon_{n}\textsc{OPT}_{\mathcal{G}_{n}}\leq o_{n}(1)\), \(\textsc{OGD}\) yields an \(o_{n}(1)\)-Nash equilibrium after a sufficiently large number of iterations._

On the Lipschitz constantNext, we make some remarks regarding the dependence of the Lipschitz constant \(L_{n}\) on the number of players in the context of general normal-form games. We first note that the Lipschitz constant \(L_{n}\) of the underlying game operator can always be bounded as \(O_{n}(n)\).

**Lemma A.6** (Lipschitz constant in normal-form games).: _For any \(n\)-player normal-form game \(\mathcal{G}\) with utilities bounded in \([-1,1]\), the Lipschitz constant \(L_{n}\) of the game operator satisfies \(L_{n}\leq n\max_{1\leq i\leq n}|\mathcal{A}_{i}|\)._

Proof.: For any player \(i\in\llbracket n\rrbracket\) and any joint strategies \(\bm{x},\bm{x}^{\prime}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\), we have

\[\|\bm{u}_{i}(\bm{x}_{-i})-\bm{u}_{i}(\bm{x}^{\prime}_{-i})\|_{2} \leq\sqrt{|\mathcal{A}_{i}|}\|\bm{u}_{i}(\bm{x}_{-i})-\bm{u}_{i}( \bm{x}^{\prime}_{-i})\|_{\infty}\] (13) \[\leq\sqrt{|\mathcal{A}_{i}|}\left\|\sum_{\bm{a}_{-i}\in\mathcal{A }_{-i}}u_{i}(\cdot,\bm{a}_{-i})\left(\prod_{i^{\prime}\neq i}\bm{x}_{i^{\prime }}[a_{i^{\prime}}]-\prod_{i^{\prime}\neq i}\bm{x}^{\prime}_{i^{\prime}}[a_{i^{ \prime}}]\right)\right\|_{1}\] (14) \[\leq\sqrt{|\mathcal{A}_{i}|}\left\|\sum_{\bm{a}_{-i}\in\mathcal{A }_{-i}}\left(\prod_{i^{\prime}\neq i}\bm{x}_{i^{\prime}}[a_{i^{\prime}}]-\prod _{i^{\prime}\neq i}\bm{x}^{\prime}_{i^{\prime}}[a_{i^{\prime}}]\right)\right\| _{1}\] (15) \[\leq\sqrt{|\mathcal{A}_{i}|}\sum_{i^{\prime}\neq i}\|\bm{x}_{i^{ \prime}}-\bm{x}^{\prime}_{i^{\prime}}\|_{1}\leq\max_{1\leq i\leq n}|\mathcal{A }_{i}|\sum_{i^{\prime}\neq i}\|\bm{x}_{i^{\prime}}-\bm{x}^{\prime}_{i^{\prime} }\|_{2},\] (16)

where (13) uses the equivalence between the \(\ell_{2}\) and the \(\ell_{\infty}\) norm; (14) follows from the definition of the (expected) utility: \(u_{i}(a_{i},\bm{x}_{-i})\coloneqq\mathbb{E}_{\bm{a}_{-i}\sim\bm{x}_{-i}}[u_{i} (\bm{a})]=\sum_{\bm{a}_{-i}\in\mathcal{A}_{-i}}u_{i}(\bm{a})\prod_{i^{\prime} \neq i}\bm{x}_{i^{\prime}}[a_{i^{\prime}}]\), for any \(a_{i}\in\mathcal{A}_{i}\); (15) uses the triangle inequality along with the assumption that \(|u_{i}(\bm{a})|\leq 1\); and (16) follows from the well-known fact that the total variation distance between two product distributions can be upper bounded by the sum of the total variation distance of each individual component [54], as well as the equivalence between the \(\ell_{1}\) and the \(\ell_{2}\) norm. As a result, continuing from (16), we have

\[\|F(\bm{x})-F(\bm{x}^{\prime})\|_{2}^{2}=\sum_{i=1}^{n}\|\bm{u}_ {i}(\bm{x}_{-i})-\bm{u}_{i}(\bm{x}^{\prime}_{-i})\|_{2}^{2} \leq\left(\max_{1\leq i\leq n}|\mathcal{A}_{i}|\right)^{2}\sum_{i= 1}^{n}\left(\sum_{i^{\prime}\neq i}\|\bm{x}_{i^{\prime}}-\bm{x}^{\prime}_{i^{ \prime}}\|_{2}\right)^{2}\] \[\leq n^{2}\left(\max_{1\leq i\leq n}|\mathcal{A}_{i}|\right)^{2} \|\bm{x}-\bm{x}^{\prime}\|_{2}^{2},\]

where the last inequality used that, by Jensen's inequality, \(\left(\sum_{i^{\prime}\neq i}\|\bm{x}_{i^{\prime}}-\bm{x}^{\prime}_{i^{\prime} }\|_{2}\right)^{2}\leq(n-1)\sum_{i^{\prime}\neq i}\|\bm{x}_{i^{\prime}}-\bm{x} ^{\prime}_{i^{\prime}}\|_{2}^{2}\leq n\|\bm{x}-\bm{x}^{\prime}\|_{2}^{2}\). This concludes the proof. 

Graphical gamesAs a byproduct of the proof above, we next point out an important refinement of Lemma A.6 concerning _graphical games_. In particular, here we assume that the utility of each player \(i\in\llbracket n\rrbracket\) only depends on the actions of players belonging to its _neighborhood_\(\mathcal{N}_{i}\subseteq\llbracket n\rrbracket\setminus\{i\}\).

We further assume that \(|\mathcal{N}_{i}|\leq\Delta\) for any player \(i\in\llbracket n\rrbracket\), where \(\Delta\in\mathbb{N}\) will be referred to as the _degree_ of the graphical game. To conclude the definition, we also posit that each player \(i\in\llbracket n\rrbracket\) can only affect the utilities of at most \(\Delta\) other players: \(|i^{\prime}\in\llbracket n\rrbracket:i\in\mathcal{N}_{i^{\prime}}|\leq\Delta\).

**Lemma A.7** (Lipschitz constant in graphical games).: _For any \(n\)-player graphical game with degree \(\Delta\in\mathbb{N}\) and utilities bounded in \([-1,1]\), the Lipschitz constant \(L_{n}\) of the game operator satisfies \(L_{n}\leq\Delta\max_{1\leq i\leq n}|\mathcal{A}_{i}|\)._

Proof.: For any player \(i\in\llbracket n\rrbracket\) and any joint strategies \(\boldsymbol{x},\boldsymbol{x}^{\prime}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{ i})\), we have

\[\|\boldsymbol{u}_{i}(\boldsymbol{x}_{-i})-\boldsymbol{u}_{i}( \boldsymbol{x}_{-i}^{\prime})\|_{2} \leq\sqrt{|\mathcal{A}_{i}|}\|\boldsymbol{u}_{i}(\boldsymbol{x} _{-i})-\boldsymbol{u}_{i}(\boldsymbol{x}_{-i}^{\prime})\|_{\infty}\] \[\leq\sqrt{|\mathcal{A}_{i}|}\left\|\sum_{\prod_{i^{\prime}\in \mathcal{N}_{i}}\mathcal{A}_{i^{\prime}}}\left(\prod_{i^{\prime}\in\mathcal{N }_{i}}\boldsymbol{x}_{i^{\prime}}[a_{i^{\prime}}]-\prod_{i^{\prime}\in \mathcal{N}_{i}}\boldsymbol{x}_{i^{\prime}}^{\prime}[a_{i^{\prime}}]\right) \right\|_{1}\] \[\leq\sqrt{|\mathcal{A}_{i}|}\sum_{i^{\prime}\in\mathcal{N}_{i}}\| \boldsymbol{x}_{i^{\prime}}-\boldsymbol{x}_{i^{\prime}}^{\prime}\|_{1}\leq \max_{1\leq i\leq n}|\mathcal{A}_{i}|\sum_{i^{\prime}\in\mathcal{N}_{i}}\| \boldsymbol{x}_{i^{\prime}}-\boldsymbol{x}_{i^{\prime}}^{\prime}\|_{2},\]

where the derivation above is similar to that in Lemma A.6. As a result,

\[\|F(\boldsymbol{x})-F(\boldsymbol{x}^{\prime})\|_{2}^{2} =\sum_{i=1}^{n}\|\boldsymbol{u}_{i}(\boldsymbol{x}_{-i})- \boldsymbol{u}_{i}(\boldsymbol{x}_{-i}^{\prime})\|_{2}^{2}\] \[\leq\left(\max_{1\leq i\leq n}|\mathcal{A}_{i}|\right)^{2}\sum_{ i=1}^{n}\left(\sum_{i^{\prime}\in\mathcal{N}_{i}}\|\boldsymbol{x}_{i^{\prime}}- \boldsymbol{x}_{i^{\prime}}^{\prime}\|_{2}\right)^{2}\] \[\leq\Delta\left(\max_{1\leq i\leq n}|\mathcal{A}_{i}|\right)^{2} \sum_{i=1}^{n}\sum_{i^{\prime}\in\mathcal{N}_{i}}\|\boldsymbol{x}_{i^{\prime} }-\boldsymbol{x}_{i^{\prime}}^{\prime}\|_{2}^{2}\] \[=\Delta\left(\max_{1\leq i\leq n}|\mathcal{A}_{i}|\right)^{2} \sum_{i=1}^{n}\|\boldsymbol{x}_{i}-\boldsymbol{x}_{i}^{\prime}\|_{2}^{2}|i^{ \prime}\in\llbracket n\rrbracket:i\in\mathcal{N}_{i^{\prime}}|\] \[\leq\Delta^{2}\left(\max_{1\leq i\leq n}|\mathcal{A}_{i}|\right) ^{2}\|\boldsymbol{x}-\boldsymbol{x}^{\prime}\|_{2}^{2}.\]

Games with vanishing sensitivityWe next focus on a different subclass of normal-form games; namely, games with small sensitivity (per Definition 3.5). Taking a step back, in graphical games every player can only be impacted by (and have an impact to) a small number of other players. Instead, here we consider games where a player's utility can be impacted by all other players, but only by a small amount.

**Lemma A.8** (Lipschitz constant in games with vanishing sensitivity).: _For any \(n\)-player normal-form game with sensitivity \(\epsilon_{n}\in\mathbb{R}_{>0}\), the Lipschitz constant \(L_{n}\) of the game operator satisfies \(L_{n}\leq\epsilon_{n}n\max_{1\leq i\leq n}|\mathcal{A}_{i}|\)._

Proof.: Let \(i\in\llbracket n\rrbracket\). For \(\boldsymbol{x}_{1},\boldsymbol{x}_{1}^{\prime}\in\Delta(\mathcal{A}_{1})\) and \(\boldsymbol{a}_{-1}\in\mathcal{A}_{-1}\) (restricting on Player \(1\) here is without any loss, and only made for the sake of simplicity in the notation), it follows that

\[u_{i}(\boldsymbol{x}_{1},\boldsymbol{a}_{-1})-u_{i}(\boldsymbol{ x}_{1}^{\prime},\boldsymbol{a}_{-1}) =\sum_{a_{1}\in\mathcal{A}_{1}}\boldsymbol{x}_{1}[a_{1}]u_{i}(a_{1}, \boldsymbol{a}_{-1})-\sum_{a_{1}\in\mathcal{A}_{1}}\boldsymbol{x}_{1}^{\prime }[a_{1}]u_{i}(a_{1},\boldsymbol{a}_{-1})\] \[=\sum_{a_{1}\in\mathcal{A}_{1}\setminus\{a_{1}^{\prime}\}}( \boldsymbol{x}_{1}[a_{1}]-\boldsymbol{x}_{1}^{\prime}[a_{1}])u_{i}(a_{1}, \cdot)+(\boldsymbol{x}_{1}[a_{1}^{\prime}]-\boldsymbol{x}_{1}^{\prime}[a_{1}^{ \prime}])u_{i}(a_{1}^{\prime},\cdot)\] \[=\sum_{a_{1}\in\mathcal{A}_{1}\setminus\{a_{1}^{\prime}\}}( \boldsymbol{x}_{1}[a_{1}]-\boldsymbol{x}_{1}^{\prime}[a_{1}])(u_{i}(a_{1}, \boldsymbol{a}_{-1})-u_{i}(a_{1}^{\prime},\boldsymbol{a}_{-1})),\] (17)for some \(a_{1}^{\prime}\in\mathcal{A}_{1}\), where we used that \((\bm{x}_{1}[a_{1}^{\prime}]-\bm{x}_{1}^{\prime}[a_{1}^{\prime}])=\sum_{a_{1}\in \mathcal{A}_{1}\setminus\{a_{1}^{\prime}\}}(\bm{x}_{1}^{\prime}[a_{1}]-\bm{x}_ {1}[a_{1}])\) since \(\bm{x}_{1},\bm{x}_{1}^{\prime}\in\Delta(\mathcal{A}_{1})\). Continuing from (17), we have

\[|u_{i}(\bm{x}_{1},\bm{a}_{-1})-u_{i}(\bm{x}_{1}^{\prime},\bm{a}_{ -1})| \leq\sum_{a_{1}\in\mathcal{A}_{1}\setminus\{a_{1}^{\prime}\}}|\bm{ x}_{1}[a_{1}]-\bm{x}_{1}^{\prime}[a_{1}]||u_{i}(a_{1},\bm{a}_{-1})-u_{i}(a_{1}^{ \prime},\bm{a}_{-1})|\] \[\leq\epsilon_{n}\sum_{a_{1}\in\mathcal{A}_{1}\setminus\{a_{1}^{ \prime}\}}|\bm{x}_{1}[a_{1}]-\bm{x}_{1}^{\prime}[a_{1}]|\leq\epsilon_{n}\|\bm{ x}_{1}-\bm{x}_{1}^{\prime}\|_{1},\]

where \(\epsilon_{n}\) is the sensitivity of the game. Similar reasoning yields that \(|u_{i}(\bm{x})-u_{i}(\bm{x}_{1}^{\prime},\bm{x}_{-1})|\leq\epsilon_{n}\|\bm{ x}_{1}-\bm{x}_{1}^{\prime}\|_{1}\), for any \(\bm{x}_{-1}\in\prod_{i=2}^{n}\Delta(\mathcal{A}_{i})\). As a result, we have

\[\|\bm{u}_{1}(\bm{x}_{-1})-\bm{u}_{1}(\bm{x}_{-1}^{\prime})\|_{ \infty} \leq|u_{1}(\cdot,\bm{x}_{2},\bm{x}_{3},\ldots,\bm{x}_{n})-u_{1}( \cdot,\bm{x}_{2}^{\prime},\bm{x}_{3},\ldots,\bm{x}_{n})|\] \[+|u_{1}(\cdot,\bm{x}_{2}^{\prime},\bm{x}_{3},\ldots,\bm{x}_{n})- u_{1}(\cdot,\bm{x}_{2}^{\prime},\bm{x}_{3}^{\prime},\ldots,\bm{x}_{n})|\] \[+\ldots\] \[+|u_{1}(\cdot,\bm{x}_{2}^{\prime},\bm{x}_{3}^{\prime},\ldots,\bm{ x}_{n-1}^{\prime}\bm{x}_{n})-u_{1}(\cdot,\bm{x}_{2}^{\prime},\bm{x}_{3}^{\prime}, \ldots,\bm{x}_{n}^{\prime})|\] \[\leq\epsilon_{n}\sum_{i\neq 1}\|\bm{x}_{i}-\bm{x}_{i}^{\prime}\|_{ 1}.\]

By symmetry, we have shown that \(\|\bm{u}_{i}(\bm{x}_{-i})-\bm{u}_{i}(\bm{x}_{-i}^{\prime})\|_{\infty}\leq \epsilon_{n}\sum_{i^{\prime}\neq i}\|\bm{x}_{i^{\prime}}-\bm{x}_{i^{\prime}}^{ \prime}\|_{1}\), and the rest of the argument is identical to that of Lemma A.6. 

Polymatrix gamesA careful examination of the proof of Lemma A.8 reveals that its conclusion in fact applies under a more relaxed condition compared to what imposed by Definition 3.5; namely, we can define

\[\epsilon:=\max_{1\leq i\leq n}\max_{\bm{a}\in\mathcal{A}}\max_{i^{\prime}\neq i }\max_{a_{i^{\prime}}^{\prime}\in\mathcal{A}_{i^{\prime}}}|u_{i}(\bm{a})-u_{i} (a_{i^{\prime}}^{\prime},\bm{a}_{-i^{\prime}})|.\] (18)

In words, when considering the utility of a player \(i\in\llbracket n\rrbracket\), we only bound deviations by players besides \(i\). It is easy to see that Lemma A.8 in fact applies even if \(\epsilon\in\mathbb{R}_{>0}\) is defined as in (18). This observation enables capturing other interesting classes of games under the premise that \(L_{n}=O_{n}(1)\), such as _polymatrix games_. Specifically, a polymatrix game is defined with respect to an underlying directed graph \(G=(\llbracket n\rrbracket,E)\), so that each node of \(G\) is uniquely associated with the corresponding player. For every edge \((i,i^{\prime})\in E\) there is a matrix \(\mathbf{A}_{i,i^{\prime}}\in\mathbb{R}^{\mathcal{A}_{i}\times\mathcal{A}_{i^ {\prime}}}\) so that the utility of Player \(i\) is defined as

\[u_{i}(\bm{x})\coloneqq\frac{1}{n}\sum_{i^{\prime}\in\mathcal{N}_{i}}\bm{x}_{i}^ {\top}\mathbf{A}_{i,i^{\prime}}\bm{x}_{i^{\prime}},\] (19)

where \(\mathcal{N}_{i}\coloneqq\{i^{\prime}\in\llbracket n\rrbracket:(i,i^{\prime}) \in E\}\). Unlike the class of graphical games we saw earlier, here we do not restrict the size of the neighborhoods. For this reason, we have normalized each player's utility by a \(1/n\) factor in (19), for otherwise the utilities are not guaranteed to be bounded (independent of the number of players \(n\)). It is then easy to see that polymatrix games are subject to Lemma A.8 under \(\epsilon\) defined in (18), which here satisfies \(\epsilon=O(1/n)\). Below, we provide a simpler and sharper argument compared to Lemma A.8.

**Lemma A.9**.: _For any \(n\)-player polymatrix game, the Lipschitz constant \(L_{n}\) of the game operator satisfies \(L_{n}\leq\max_{(i,i^{\prime})\in E}\|\mathbf{A}_{i,i^{\prime}}\|_{2}\), where \(\|\cdot\|_{2}\) here denotes the spectral norm._Proof.: By definition of the utility functions in (19), we have that for any player \(i\in\llbracket n\rrbracket\) and \(\bm{x},\bm{x}^{\prime}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\),

\[\|\bm{u}_{i}(\bm{x}_{-i})-\bm{u}_{i}(\bm{x}_{-i}^{\prime})\|_{2} \leq\frac{1}{n}\left\|\sum_{i^{\prime}\in\mathcal{N}_{i}}\bm{A}_{i, i^{\prime}}(\bm{x}_{i^{\prime}}-\bm{x}_{i^{\prime}}^{\prime})\right\|_{2}\] \[\leq\frac{1}{n}\sum_{i^{\prime}\in\mathcal{N}_{i}}\|\bm{A}_{i,i^ {\prime}}(\bm{x}_{i^{\prime}}-\bm{x}_{i^{\prime}}^{\prime})\|_{2}\] \[\leq\frac{1}{n}\sum_{i^{\prime}\in\mathcal{N}_{i}}\|\bm{A}_{i,i^ {\prime}}\|_{2}\|\bm{x}_{i^{\prime}}-\bm{x}_{i^{\prime}}^{\prime}\|_{2}\] \[\leq\frac{\max_{(i,i^{\prime})\in E}\|\bm{A}_{i,i^{\prime}}\|_{2} }{n}\sum_{i^{\prime}\in\mathcal{N}_{i}}\|\bm{x}_{i^{\prime}}-\bm{x}_{i^{ \prime}}^{\prime}\|_{2},\]

and the claim follows. 

### Games with vanishing sensitivity

We next establish an important implication of our framework concerning the class of games with vanishing strategic sensitivity (per Definition 3.5); the statement of the theorem is recalled below.

**Theorem 3.6**.: _Consider an \(n\)-player game \(\mathcal{G}_{n}\) with sensitivity \(\epsilon_{n}\in\mathbb{R}_{>0}\). Then, \(T=O_{n}(n)\) iterations of \(\mathtt{OGD}\) suffice to obtain a \(\left(\frac{1}{\sqrt{\delta}}O_{n}\left(\epsilon_{n}\sqrt{n}\right),\delta\right)\)-weak Nash equilibrium, for \(\delta\in(0,1)\)._

Proof.: Let \(i\in\llbracket n\rrbracket\). Following the proof of Lemma A.8, the definition of sensitivity implies that \(|u_{i}(\bm{x}_{i}^{\prime},\bm{x}_{-i})-u_{i}(\bm{x})|\leq\epsilon_{n}\|\bm{x }_{i}^{\prime}-\bm{x}_{i}\|_{1}\leq 2\epsilon_{n}\), for any \(\bm{x}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\) and \(\bm{x}_{i}^{\prime}\in\Delta(\mathcal{A}_{i})\). The proof now follows that of Theorem A.2. In particular, we can lower bound the sum of the players' regrets as follows.

\[\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)} =\sum_{i=1}^{n}\left(\max_{\bm{x}_{i}^{\star}\in\Delta(\mathcal{ A}_{i})}\left\{\sum_{t=1}^{T}u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i}^{(t)}) \right\}-\sum_{t=1}^{T}u_{i}(\bm{x}^{(t)})\right)\] \[\geq-\sum_{i=1}^{n}\sum_{t=1}^{T}|u_{i}(\bm{x}_{i}^{\prime},\bm{ x}_{-i}^{(t)})-u_{i}(\bm{x}^{(t)})|\geq-2Tn\epsilon_{n}.\]

As a result, following the proof of Theorem A.2, we conclude that for learning rate \(\eta_{n}\coloneqq\frac{1}{4L_{n}}\) there exists a time \(t^{\star}\in\llbracket T\rrbracket\) such that

\[\sum_{i=1}^{n}\left(\mathsf{BRGAP}_{i}(\bm{x}^{(t^{\star})})\right)^{2}\leq 4 \left(\frac{\max_{1\leq i\leq n}D_{\mathcal{A}_{i}}^{2}}{\eta_{n}^{2}}+\max_{1 \leq i\leq n}B_{i}^{2}\right)\left(\frac{2D_{\mathcal{X}}^{2}}{T}+4\eta_{n}n \epsilon_{n}\right),\]

Thus, setting

\[T\coloneqq\frac{D_{\mathcal{X}}^{2}}{2\eta_{n}n\epsilon_{n}}=\frac{2D_{ \mathcal{X}}^{2}L_{n}}{n\epsilon_{n}}\leq 2D_{\mathcal{X}}^{2}\left(\max_{1\leq i \leq n}|\mathcal{A}_{i}|\right)=O_{n}(n),\]

by Lemma A.8, yields that

\[\sum_{i=1}^{n}\left(\mathsf{BRGAP}_{i}(\bm{x}^{(t^{\star})})\right)^{2}\leq O_ {n}(L_{n}n\epsilon_{n})=O_{n}(n^{2}\epsilon_{n}^{2}),\]

where the asymptotic notation above applies in the regime \(L_{n}\geq\Omega_{n}(1)\). The statement thus follows. 

We note that the conclusion of Theorem 3.6 also applies under the weaker notion of sensitivity defined in (18), if we additionally assume that there exists \(\bm{x}^{\star}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\) such that for any \(\bm{x}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\),

\[\sum_{i=1}^{n}u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i})-\sum_{i=1}^{n}u_{i}(\bm{x}) \geq-n\epsilon_{n}.\] (20)Condition (20) can be met even in games with large sensitivity (Observation 3.4), which is why we chose to state Theorem 3.6 under a stronger but more interpretable condition.

Next, we point out two interesting extensions of our approach based on developments following the original smoothness framework of Roughgarden [90].

### Refined smoothness

Throughout this paper we have relied on the original smoothness framework [90] to derive much of our results. Nevertheless, it is worth noting that there are certain extensions documented in the literature that can make our predictions sharper. To be more precise, focusing on normal-form games, Nadav and Roughgarden [75] noted that the original smoothness framework is in fact able to provide efficiency bounds for a set of equilibria even more permissive than CCE, which they refer to as _average_ CCE with respect to a welfare-maximizing strategy \(\bm{x}^{\star}\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\) (ACCE\({}^{*}\)). In particular, the latter relaxation only requires a guarantee for the average regret over the players, \(\frac{1}{n}\sum_{i=1}^{n}\text{Reg}_{i}^{(T)}(\bm{x}_{i}^{\star})\), while CCE instead requires a guarantee for the maximum regret \(\max_{1\leq i\leq n}\text{Reg}_{i}^{(T)}\). Based on this observation, Nadav and Roughgarden [75] developed a primal-dual framework in order to provide refined guarantees (beyond the original smoothness framework) for CCE\({}^{*}\), which boils down to solving the following linear program.

\[\begin{array}{ll}\text{maximize}&\rho\\ \text{subject to}&\sum_{i=1}^{n}z_{i}\left(u_{i}(a_{i}^{*},\bm{a}_{-i})-u_{i}( \bm{a})\right)\geq\rho\text{SW}(\bm{a}^{\star})-\text{SW}(\bm{a}),\bm{a}\in \prod_{i=1}^{n}\mathcal{A}_{i},\\ &z_{i}\geq 0.\end{array}\] (21)

Here, it is assumed that the underlying game is in normal form, with \(\prod_{i=1}^{n}\mathcal{A}_{i}\) being the set of joint action profiles, and \(\bm{a}^{\star}\in\prod_{i=1}^{n}\mathcal{A}_{i}\) a welfare-maximizing joint action. (The formulation of Nadav and Roughgarden [75] is based on cost-minimization games, but of course their LP can be cast directly for payoff-maximization games in the form of (21).) The above LP is a simple transformation of the fractional-linear program corresponding to optimizing \(\rho\coloneqq\frac{\lambda}{1+\mu}\) subject to the smoothness constraints of Definition 2.1, but with the additional flexibility of optimizing over a vector \(\bm{z}\in\mathbb{R}_{\geq 0}^{n}\). In particular, when restricting \(z_{1}=z_{2}=\cdots=z_{n}\), this exactly recovers the LP for computing the robust price of anarchy--given in (24). Nevertheless, Nadav and Roughgarden [75] pointed out that the additional flexibility of (21) can have an arbitrarily large impact on the predicted efficiency [75, Remark 1].

The sharper definition of smoothness introduced by Nadav and Roughgarden [75] can be leveraged in our framework as follows. We can define a generalized robust price of anarchy as the solution of the LP (21). If this quantity approaches to \(1\) with a sufficiently fast rate (in terms of the number of players), we can extend Theorem A.2 by analyzing the _weighted_ sum of the players' regrets \(\sum_{i=1}^{n}z_{i}\text{Reg}_{i}^{(T)}\). It is easy to see that the argument of Theorem 3.1 readily carries over under the condition that the ratio \(z_{i}/z_{i^{\prime}}\) is bounded for any \(i,i^{\prime}\in\llbracket n\rrbracket\), as well as the assumption that each \(z_{i}\) is bounded away from \(0\). In contrast, if there exists a pair \(i,i^{\prime}\in\llbracket n\rrbracket\) for which the ratio \(z_{i}/z_{i^{\prime}}\) is unbounded, our current techniques do not appear to be applicable; this is related to the well-known difficulty of deriving so-called RVU bounds for the maximum of the players' regret, instead of their sum [103]. In other words, our current techniques can sharpen Theorem 3.1 by considering solutions of (21) under the additional (linear) constraint that the variables \(\{z_{i}\}_{i=1}^{n}\) have bounded pairwise ratio. Given that the more general framework of Nadav and Roughgarden [75] leads to improved bounds, we expect that this modification should have applications in our setting as well. In particular, we point out that the \(2\)-player example of Nadav and Roughgarden [75] that separates ACCE\({}^{*}\) from CCE\({}^{*}\) indeed satisfies \(z_{1}/z_{2}\approx 2.3\)[75, Proposition 2], so the separation manifests itself even when the pairwise ratio is bounded by an absolute constant.

We finally refer to the works of Nguyen [78] and Kulkarni and Mirrokni [62] for a different primal-dual take on smoothness.

### Local smoothness

Another interesting extension of our techniques can be obtained using the framework of _local smoothness_, first introduced by Roughgarden and Schoppmann [92] and recently refined by Nguyen [78] in the context of splittable congestion games. In what follows, we follow the treatment of Nguyen [78] as it fits our framework. In this context, Nguyen [78] introduced the notion of a \((\lambda,\mu)\)-_dual-smooth_ differentiable utility function \(u:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}\) with the property that for every vectors \(\bm{z}=(z_{1},\ldots,z_{n})\in\mathbb{R}_{\geq 0}^{n}\) and \(\bm{z}^{\prime}=(z_{1}^{\prime},\ldots,z_{n}^{\prime})\in\mathbb{R}_{\geq 0}^{n}\).

\[z^{\prime}u(z)+\sum_{i=1}^{n}z_{i}(z_{i}^{\prime}-z_{i})u^{\prime}(z)\geq \lambda z^{\prime}u(z^{\prime})-\mu zu(z),\] (22)

where \(z\coloneqq\sum_{i=1}^{n}z_{i}\) and \(z^{\prime}\coloneqq\sum_{i=1}^{n}z_{i}^{\prime}\). We also note that \(u^{\prime}\) above denotes the derivative of \(u\). (Again, (22) has been translated to utility-maximization games compared to the original formulation of Nguyen [78]). Furthermore, a splittable congestion game is called \((\lambda,\mu)\)-dual-smooth [78] if for every resource \(e\in E\) the utility function \(u_{e}:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}\) is \((\lambda,\mu)\)-dual-smooth in the sense of (22). The importance of Nguyen's extension is that it can be applied to _coarse_ correlated equilibria, as opposed to the original definition of Roughgarden and Schoppmann [92] that was applicable to correlated equilibria; this distinction is incidentally important for our framework.

We will connect this concept of local smoothness with the linearization of the players' regrets. In particular, the utility of a player \(i\in\llbracket n\rrbracket\) under a joint strategy \(\bm{x}\) is defined as \(u_{i}(\bm{x})\coloneqq\sum_{e\in E}\bm{x}_{i}[e]u_{e}(\sum_{i=1}^{n}\bm{x}_{i} [e])\). Then,

\[\frac{\partial u_{i}(\bm{x})}{\partial\bm{x}_{i}[e]}=u_{e}\left(\sum_{i=1}^{n }\bm{x}_{i}[e]\right)+\bm{x}_{i}[e]u_{e}^{\prime}\left(\sum_{i=1}^{n}\bm{x}_{ i}[e]\right).\]

As a result, for any \(t\in\mathbb{N}\),

\[\sum_{i=1}^{n}\langle\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)},\nabla_ {\bm{x}_{i}}u_{i}(\bm{x}^{(t)})\rangle =\sum_{e\in E}u_{e}(\bm{x}[e])\bm{x}^{\star}[e]-\sum_{e\in E}u_{e }(\bm{x}[e])\bm{x}[e]\] \[+\sum_{e\in E}\sum_{i=1}^{n}\bm{x}_{i}[e](\bm{x}_{i}^{\star}[e]- \bm{x}_{i}[e])u_{e}^{\prime}(\bm{x}[e]),\]

with the understanding that \(\bm{x}[e]\coloneqq\sum_{i=1}^{n}\bm{x}_{i}[e]\) and \(\bm{x}^{\star}[e]\coloneqq\sum_{i=1}^{n}\bm{x}_{i}^{\star}[e]\) for any \(e\in E\). Now let us define \(\mathsf{Reg}_{\mathcal{L},i}^{(T)}(\bm{x}_{i}^{\star})\coloneqq\sum_{t=1}^{T} \langle\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)},\nabla_{\bm{x}_{t}}u_{i}(\bm{x})\rangle\). Combining the last displayed equality with local smoothness (22), we get that

\[\sum_{i=1}^{n}\mathsf{Reg}_{\mathcal{L},i}^{(T)}(\bm{x}_{i}^{\star}) \geq\lambda\sum_{e\in E}\bm{x}^{\star}[e]u_{e}(\bm{x}^{\star}[e])-( \mu+1)\sum_{e\in E}\bm{x}[e]u_{e}(\bm{x}[e])\] \[=\lambda\mathsf{SW}(\bm{x}^{\star})-(\mu+1)\mathsf{SW}(\bm{x}).\] (23)

Consequently, if we define a local smoothness bound, \(\rho_{\mathcal{L}}\coloneqq\frac{\lambda}{1+\mu}\) with \((\lambda,\mu)\) being subject to (22), Theorem 3.1 can be readily extended in the regime \(\rho_{\mathcal{L}}\to 1\) based on (23).

### Considerations based on PoA

It is natural to ask if the conclusion of Theorem 3.1 can be relaxed to \(\mathsf{PoA}\to 1\). Here, we point out that such an assumption does not suffice to obtain interesting guarantees. In particular, we next note Proposition 3.7, stated earlier in the main body, which is an immediate byproduct of the hardness of computing Nash equilibria in constant-sum games.

**Proposition 3.7**.: _Even under the promise that \(\mathsf{PoA}_{\mathcal{G}}=1\), computing a \((1/\mathsf{poly}(\mathcal{G}))\)-Nash equilibrium in normal-form games in polynomial time is impossible when \(n\geq 3\), unless \(\mathsf{PPAD}\subseteq\mathsf{P}\)._

Proof.: Chen et al. [20] showed that computing a \((1/\mathsf{poly}(\max_{i}|\mathcal{A}_{i}|))\)-Nash equilibrium in a general-sum two-player game in normal form is \(\mathsf{PPAD}\)-hard. As a result, the same applies in constant-sum \(3\)-player games by suitably incorporating an additional player who has no strategic impact on the game. Further, in any constant-sum game \(\mathcal{G}\) it clearly holds that \(\mathsf{PoA}_{\mathcal{G}}=1\), concluding the proof. 

_Remark A.10_.: The positive result established in Theorem A.2 requires that \(\rho\approx 1\) under a bounded pair of smoothness parameters \((\lambda,\mu)\). One may wonder whether similar conclusions apply even when the smoothness parameters are unbounded. In particular, the _robust price of anarchy_ (rPoA) can be defined as the solution to the following linear program.

\[\begin{array}{ll}\text{maximize}&\rho\\ \text{subject to}&z\sum_{i=1}^{n}\left(u_{i}(a_{i}^{\star},\bm{a}_{-i})-u_{i}( \bm{a})\right)\geq\rho\text{SW}(\bm{a}^{\star})-\text{SW}(\bm{a}),\bm{a}\in \prod_{i=1}^{n}\mathcal{A}_{i},\\ &z\geq\bm{0}.\end{array}\] (24)

Above \(\bm{a}^{\star}=(a_{1}^{\star},\ldots,a_{n}^{\star})\in\prod_{i=1}^{n}\mathcal{ A}_{i}\) is a welfare-maximizing action profile, which can be assumed to be unique for the purposes of our discussion here. In this context, can we extend the conclusion of Theorem A.2 under the assumption that rPoA\({}_{\mathcal{G}_{n}}\to 1\)? In general, that is not possible. Indeed, in any constant-sum game one can make the LP (24) feasible by taking \(z=0\) and \(\rho=1\); that is, rPoA\({}_{\mathcal{G}_{n}}=1\) for any constant-sum game \(\mathcal{G}\). Thus, assuming merely that rPoA\({}_{\mathcal{G}}\to 1\) is not enough to obtain interesting guarantees for computing Nash equilibria (in accordance with Proposition 3.7). In other words, our underlying assumption that the smoothness parameters are bounded is necessary. To further explain this discrepancy, we note that a constant-sum game is generally not \((1+\mu,\mu)\)-smooth (for a finite \(\mu>-1\)), for otherwise any constant-sum game would satisfy the Minty property (Proposition 3.3), which would in turn contradict Proposition 3.7. The case where \(z=0\) in any optimal solution of (24) essentially corresponds to a pathological manifestation of smoothness in which the underlying parameters are unbounded. We stress again that throughout this paper, when we say that a game is \((\lambda,\mu)\)-smooth we, of course, posit that those smoothness parameters are finite. Further, when we say that \(\rho_{\mathcal{G}}=1\) in a game \(\mathcal{G}\), we accept that there are finite smoothness parameters associated with \(\rho_{\mathcal{G}}\). With this convention, we reiterate that there are games in which rPoA\({}_{\mathcal{G}}\neq\rho_{\mathcal{G}}\).

Next, we provide a concrete example based on Shapley's game [99] in which OGD (unconditionally) fails to converge to an \(\epsilon\)-Nash equilibrium, for a constant \(\epsilon>0\).

_Example A.11_.: We consider a \(3\)-player game \(\mathcal{G}\) defined as follows. We let

\[\mathbf{A}\coloneqq\begin{bmatrix}1&1&2\\ 2&1&1\\ 1&2&1\end{bmatrix},\mathbf{B}\coloneqq\begin{bmatrix}1&2&1\\ 1&1&2\\ 2&1&1\end{bmatrix}.\] (25)

Then, we set \(u_{1}(\bm{x}_{1},\bm{x}_{2},\bm{x}_{3})\coloneqq\bm{x}_{1}^{\top}\mathbf{A} \bm{x}_{2}\), \(u_{2}(\bm{x}_{1},\bm{x}_{2},\bm{x}_{3})\coloneqq\bm{x}_{1}^{\top}\mathbf{B} \bm{x}_{2}\), and \(u_{3}(\bm{x}_{1},\bm{x}_{2},\bm{x}_{3})\coloneqq\bm{x}_{1}^{\top}\mathbf{A} \bm{x}_{2}-\bm{x}_{1}^{\top}\mathbf{B}\bm{x}_{2}\). Thus, for any joint strategy \((\bm{x}_{1},\bm{x}_{2},\bm{x}_{3})\) it holds that \(\text{SW}(\bm{x}_{1},\bm{x}_{2},\bm{x}_{3})=3\), implying that \(\mathsf{PoA}_{\mathcal{G}}=1\). Further, \(\mathcal{G}\) is in normal form. Now, through a numerical simulation we draw the following conclusion: Although \(\mathsf{PoA}_{\mathcal{G}}=1\), for \(T\gg 1\) OGD with learning rate \(\eta\coloneqq 0.01\) and initialization \((\hat{\bm{x}}_{1}^{(1)},\hat{\bm{x}}_{2}^{(1)},\cdot)\coloneqq((0.5,0.25,0.25 ),(0.25,0.5,0.25),\cdot)\) satisfies \(\text{NEGap}(\bm{x}^{(t)})\geq 0.1875\) for any \(t\in[\![T]\!]\), where \((\bm{x}^{(t)})_{t\geq 1}\) is the sequence of iterates produced by OGD. We note that here we do not consider the initialization from the uniform distribution simply because that happens to be the unique Nash equilibrium in Shapley's game; the conclusion above readily applies for any initialization by suitably modifying the underlying game. We also note that the specific value for the learning rate specified above is used for concreteness, and the conclusion is not tied to that specific value.

We next note another separation between the bound predicted by smoothness and PoA, which gives an additional reason why the smoothness framework is more suited as a criterion for determining tractability. Below, we make for simplicity the assumption that the game has a unique welfare-maximizing action profile, which can always be enforced by incorporating an arbitrarily small noise in the players' utilities.

**Proposition A.12**.: _Determining whether a game is \((\lambda,\mu)\)-smooth can be done in polynomial time in explicitly represented normal-form games. In contrast, even in two-player games, determining PoA is NP-hard._

Proof.: First of all, the welfare-maximizing action profile \(\bm{a}^{\star}\in\prod_{i=1}^{n}\mathcal{A}_{i}\) can be trivially computed in polynomial time (in the size of the input) since the game is explicitly represented. For a legitimate \((\lambda,\mu)\) pair, determining whether the game is \((\lambda,\mu)\)-smooth can be phrased as a feasibility linear program, with a number of constraints equal to the number of possible joint action profiles, each corresponding to a separate constraint in (1); namely, \(\sum_{i=1}^{n}u_{i}(a_{i}^{\star},\bm{a}_{-i})\geq\lambda\text{OPT}-\mu\sum_{i= 1}^{n}u_{i}(\bm{a})\). As a result, the number of constraints is polynomial in the description of the game (since it is assumed that the game is explicitly represented). Furthermore, one can optimize over the smoothness parameters by considering the LP (of polynomial size) given in (24), which determines the robust price of anarchy (rPoA). In accordance with Theorem A.2 (see Remark A.10), one can also incorporate the constraint \(z\geq 1/\mathsf{poly}(\mathcal{G})\). Regarding the second claim, hardness of determining PoA even in two-player games follows directly from the reduction of Conitzer and Sandholm [27, Theorem 1]. In particular, an algorithm computing PoA would enable determining the satisfiability of a SAT formula. 

One important question is whether smoothness can be identified in polynomial time even in _succinctly_ represented games [79]. Indeed, the obvious algorithm for identifying smoothness described above requires a number of constraints that scales exponentially with the number of players, which is especially problematic in the regime of large games we focus on in Section 3.

We need to clarify, however, that knowing the smoothness parameters is certainly not a prerequisite for applying our approach. In detail, one can first of all apply Theorem 3.1 in classes of games where there are available analytical bounds for \(\rho_{n}\) as a function of \(n\), obviating the need to determine whether \(\rho_{n}\) is close to \(1\). Besides this point, and more importantly, one can always execute the algorithm prescribed by Theorem 3.1--which does not require any knowledge regarding the smoothness parameters--and then efficiently evaluate the solution quality (per Definition 2.2). If the desired accuracy has been reached, then this is precisely the initial goal; otherwise, we can safely assume that the preconidtons of Theorem 3.1 are not met.

### Smoothness does not suffice for convergence

Next, we provide for completeness an example of a smooth game where OGD fails to converge to Nash equilibria. This shows that, as expected, it is not merely enough to know that \(\rho\neq 0\) to obtain interesting guarantees.

_Example A.13_.: This example is based on a bimatrix game in normal form described with the payoff matrices

\[\mathbf{A}=\begin{bmatrix}0.2&0.8&0.9&0.3\\ 0.2&0.8&0.2&0.3\\ 0.9&0.2&0.4&0.4\\ 0.6&0.9&0.3&0.1\end{bmatrix},\mathbf{B}=\begin{bmatrix}0.4&0.2&0&0.1\\ 0.5&0&0.2&0.8\\ 0.7&0.8&0&0.4\\ 0&0&0.1&0.4\end{bmatrix}.\] (26)

We first claim that this bimatrix game \(\mathcal{G}\) satisfies \(\rho_{\mathcal{G}}\geq 0.125\). Indeed, we first see that the welfare-maximizing profile of (26) reads \((\bm{x}_{1}^{\star},\bm{x}_{2}^{\star})=((0,0,1,0),(1,0,0,0))\). We also claim that for any pair of actions \(a_{1}\in\mathcal{A}_{1}\) and \(a_{2}\in\mathcal{A}_{2}\) it holds that

\[\sum_{i=1}^{2}u_{i}(\bm{x}_{i}^{\star},a_{-i})\geq 0.125\cdot\text{OPT}_{ \mathcal{G}},\]

where \(\text{OPT}_{\mathcal{G}}=1.6\). As a result, it follows that for any \(\bm{x}_{1}\in\Delta(\mathcal{A}_{1})\) and \(\bm{x}_{2}\in\Delta(\mathcal{A}_{2})\) it holds that \(\mathbb{E}_{a_{1}\sim\bm{x}_{1},a_{2}\sim\bm{x}_{2}}[\sum_{i=1}^{2}u_{i}(\bm{ x}_{i}^{\star},a_{-i})]\geq 0.125\cdot\text{OPT}_{\mathcal{G}}\), in turn implying that \(\sum_{i=1}^{2}u_{i}(\bm{x}_{i}^{\star},\bm{x}_{-i})\geq 0.125\cdot\text{OPT}_{ \mathcal{G}}\). This means that \(\mathcal{G}\) is \((0.125,0)\)-smooth. Furthermore, through a numerical simulation we draw the following conclusion: for \(T\gg 1\), OGD with learning rate \(\eta\coloneqq 0.01\) satisfies \(\text{NEGap}(\bm{x}_{1}^{(t)},\bm{x}_{2}^{(t)})\geq 0.046\) for any \(t\in\llbracket T\rrbracket\), where \((\bm{x}_{1}^{(t)},\bm{x}_{2}^{(t)})_{t\geq 1}\) is the sequence of iterates produced by OGD. Once again, it is worth noting that this conclusion is not tied to the specific choice of learning rate we chose above, which is only used for concreteness.

### Bayesian mechanisms

In this subsection, we consider the standard _independent private value_ model of Bayesian mechanisms. In particular, each player \(i\in\llbracket n\rrbracket\) has a _type_\(v_{i}\) drawn from a distribution \(\mathcal{F}_{i}\) over a finite set of types \(\mathcal{V}_{i}\); without any loss, we may assume that \(\mathcal{F}_{i}\) is the uniform distribution over \(\mathcal{V}_{i}\). It is further assumed that players' types are pairwise independent. After each player \(i\in\llbracket n\rrbracket\) draws a type \(v_{i}\sim\mathcal{F}_{i}\), \(i\) selects an action \(a_{i}(v_{i})\in\mathcal{A}_{i}\), which is a function of its type \(v_{i}\). Now consider a fixed profile of types \(\bm{v}\in\mathcal{V}\coloneqq\mathcal{V}_{1}\times\mathcal{V}_{2}\ldots \mathcal{V}_{n}\). The (expected) utility of Player \(i\) under a joint strategy profile \(\bm{x}(\bm{v})\) is denoted by \(u_{i}(\bm{x};v_{i})\coloneqq\mathbb{E}_{\bm{a}\sim\bm{x}}[u_{i}(\bm{x};v_{i})]\). There is also a _principal_ agent who does not take an action in the game, and whose utility under \(\bm{x}\) is given by \(R(\bm{x})\coloneqq\mathbb{E}_{\bm{a}\sim\bm{x}}[R(\bm{a})]\). Accordingly, the social welfare is defined as \(\text{SW}(\bm{x},\bm{v})\coloneqq\sum_{i=1}^{n}u_{i}(\bm{x};v_{i})+R(\bm{x})\), while \(\text{OPT}_{\mathcal{M}}(\bm{v})\) represents the optimal social welfare of mechanism \(\mathcal{M}\) as a function of the joint type \(\bm{v}\in\mathcal{V}\). We will make the assumption that the utility functions assign solely nonnegative values.

Smooth mechanismsAnalogously to the notion of a smooth game (Definition 2.1), Syrgkanis and Tardos [102] introduced the notion of a smooth mechanism, formally recalled below.

**Definition A.14** (Smooth mechanism [102]).: A mechanism \(\mathcal{M}\) is \((\lambda,\mu)\)-smooth, where \(\lambda,\mu\geq 0\), if there exists an strategy profile \(\bm{x}^{\star}(\bm{v})\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\), for every type profile \(\bm{v}\in\mathcal{V}\), such that for any action profile \(\bm{a}\in\mathcal{A}\) and type profile \(\bm{v}=(v_{1},\ldots,v_{n})\in\mathcal{V}\),

\[\sum_{i=1}^{n}u_{i}(\bm{x}_{i}^{\star}(\bm{v}),\bm{a}_{-i};v_{i})\geq\lambda \mathsf{OPT}_{\mathcal{M}}(\bm{v})-\mu R(\bm{a}).\] (27)

As it turns out, many important mechanisms satisfy the above definition under various parameters \((\lambda,\mu)\)[102]. For a \((\lambda,\mu)\)-smooth mechanism \(\mathcal{M}\), we define \(\rho_{\mathcal{M}}:=\frac{\lambda}{\max(1,\mu)}\).

**Definition A.15**.: Let \(\bm{x}_{i}:\mathcal{V}_{i}\to\Delta(\mathcal{A}_{i})\) be the strategy of each player \(i\in\llbracket n\rrbracket\). A joint strategy profile \(\bm{x}\) is a _Bayes-Nash equilibrium (BNE)_ if for any player \(i\in\llbracket n\rrbracket\), any type \(v_{i}\in\mathcal{V}_{i}\) and deviation \(a_{i}^{\prime}\in\mathcal{A}_{i}\),

\[\mathbb{E}_{\bm{v}_{-i}\sim\mathcal{F}_{-i}}[u_{i}(\bm{x}(\bm{v});v_{i})] \geq\mathbb{E}_{\bm{v}_{-i}\sim\mathcal{F}_{-i}}[u_{i}(a_{i}^{\prime},\bm{x}_{ -i}(\bm{v}_{-i});v_{i})].\] (28)

This definition coincides with the standard notion of Nash equilibrium we saw in Definition 2.2 when each distribution \(\mathcal{F}_{i}\) is a point mass. We also note that an \(\epsilon\)-BNE incorporates an \(\epsilon\geq 0\) additive slackness in (28).

Population interpretation of Bayesian gamesIn the _agent-form_ representation of a Bayesian game [53], it is assumed that there are \(n\) finite subpopulations of players, each corresponding to a player \(i\in\llbracket n\rrbracket\). Each player belonging to population \(i\) corresponds to a type \(v_{i}\in\mathcal{V}_{i}\), which is distinct in each population and across populations. In this induced population game, nature first draws one player from each population, and then each player \(v_{i}\) selects an action \(a_{i}(v_{i})\); the game is then played under those selected actions. In symbols, the utility of Player \(v_{i}\) from population \(i\) reads

\[u_{i,v_{i}}^{\mathsf{AG}}(\bm{a})\coloneqq\mathbb{E}_{\bm{v}\sim\mathcal{F}}[ u_{i}(\bm{a}(\bm{v});v_{i})\mathbbm{1}\{v_{i}\}],\] (29)

where by \(\{v_{i}\}\) above we denote the event that type \(v_{i}\) is selected by nature among the population corresponding to Player \(i\). The importance of the population interpretation of the Bayesian game is that it induces a mechanism of complete information, which will be denoted by \(\mathcal{M}^{\mathsf{AG}}=\mathcal{M}^{\mathsf{AG}}(\mathcal{M})\). The following characterization highlights an important connection between \(\mathcal{M}^{\mathsf{AG}}\) and \(\mathcal{M}\).

**Theorem A.16** ([53]).: _If a mechanism \(\mathcal{M}\) is \((\lambda,\mu)\)-smooth, then the complete information mechanism \(\mathcal{M}^{\mathsf{AG}}=\mathcal{M}^{\mathsf{AG}}(\mathcal{M})\) is also \((\lambda,\mu)\)-smooth._

We now proceed with the proof of Theorem 3.8. To keep the exposition self-contained, we will not make explicit use of Theorem A.16.

**Theorem 3.8**.: _Consider a Bayesian mechanism \(\mathcal{M}\) such that \(\rho_{\mathcal{M}}=1\). Then, for any \(\epsilon>0\), \(T=O(1/\epsilon^{2})\) iterations of \(\mathtt{OGD}\) suffice to obtain an \(\epsilon\)-Bayes-Nash equilibrium of \(\mathcal{M}\)._

Proof.: Under the assumption that \(\rho_{\mathcal{M}}=1\), it follows that there exists a pair \((\lambda,\mu)\in\mathbb{R}^{2}_{\geq 0}\) such that \(\mathcal{M}\) is \((\lambda,\mu)\)-smooth with \(\lambda=\max\{1,\mu\}\). Further, by definition it holds that \(\mathsf{OPT}_{\mathcal{M}}(\bm{v})\geq\sum_{i=1}^{n}u_{i}(\bm{a};v_{i})+R(\bm{a})\), for any action profile \(\bm{a}\in\mathcal{A}\). Combining with (27), we have that there exists a strategy profile \(\bm{x}^{\star}(\bm{v})\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\) such that for every type profile \(\bm{v}\in\mathcal{V}\) and action profile \(\bm{a}\in\mathcal{A}\),

\[\sum_{i=1}^{n}u_{i}(\bm{x}_{i}^{\star}(\bm{v}),\bm{a}_{-i};v_{i})\geq\lambda \sum_{i=1}^{n}u_{i}(\bm{a};v_{i})+\lambda R(\bm{a})-\mu R(\bm{a})\geq\sum_{i=1} ^{n}u_{i}(\bm{a};v_{i}),\]

since \(\lambda=\max\{1,\mu\}\) and utilities are nonnegative. As a result, for any \(\bm{x}(\bm{v})\in\prod_{i=1}^{n}\Delta(\mathcal{A}_{i})\),

\[\sum_{i=1}^{n}\mathbb{E}_{\bm{v}\sim\mathcal{F}}[u_{i}(\bm{x}_{i}^{\star}(\bm{v} ),\bm{x}_{-i}(\bm{v}_{-i});v_{i})]\geq\sum_{i=1}^{n}\mathbb{E}_{\bm{v}\sim \mathcal{F}}[u_{i}(\bm{x}(\bm{v});v_{i})].\] (30)

Further, by definition of the agent-form utilities (29) and the law of total expectation, (30) can be equivalently cast as

\[\sum_{i=1}^{n}\sum_{v_{i}\in\mathcal{V}_{i}}u_{i,v_{i}}^{\mathsf{AG}}(\bm{x}_{i,v_{i}}^{\star},\bm{x}_{-(i,v_{i})})\geq\sum_{i=1}^{n}\sum_{v_{i}\in\mathcal{ V}_{i}}u_{i,v_{i}}^{\mathsf{AG}}(\bm{x}),\]where we used the notation \(\bm{x}_{i,v_{i}}\coloneqq\bm{x}_{i}(v_{i})\). This implies that the sum of the players' regrets in the agent-form representation is nonnegative. The proof of Theorem 3.8 then follows from the correspondence between Nash equilibria in the agent-form representation of \(\mathcal{M}\) and Bayes-Nash equilibria in the original incomplete-information mechanism \(\mathcal{M}\). 

### PoA vs robust PoA

In this subsection, we provide some additional justification for the condition \(\mathsf{rPoA}\neq\mathsf{PoA}\) required in Corollary 4.3. In particular, we conduct experiments on a set of random normal-form games. Some illustrative results for \(10\) random games are demonstrated in Figure 1. Overall, we observe that not only \(\mathsf{rPoA}\neq\mathsf{PoA}\), but in fact the gap between the two quantities is typically substantial. This discrepancy is expected since, as we have already stressed, \(\mathsf{rPoA}\) quantifies the worst-case welfare over a (typically) much broader set of equilibria.

### Proof of Theorem 4.1

In this subsection, we provide the proof of Theorem 4.1. For completeness, we also include the proofs of certain results known from prior work [37, 85, 77].

To this end, we first recall that the _prox_ operator associated with the (squared) Euclidean regularizer \(\frac{1}{2}\|\cdot\|_{2}^{2}\) is defined as

\[\Pi_{\bm{x}_{i}}(\bm{u}_{i})\coloneqq\arg\max_{\bm{x}_{i}^{\prime}\in\mathcal{ X}_{i}}\left\{\langle\bm{x}_{i}^{\prime},\bm{u}_{i}\rangle-\frac{1}{2}\|\bm{x}_{i}- \bm{x}_{i}^{\prime}\|_{2}^{2}\right\},\] (31)

under some utility vector \(\bm{u}_{i}\in\mathbb{R}^{d_{i}}\). Accordingly, we let \(\Pi_{\bm{x}}(\bm{u})\coloneqq(\Pi_{\bm{x}_{1}}(\bm{u}_{1}),\ldots,\Pi_{\bm{x}_ {n}}(\bm{u}_{n}))\), where \(\bm{u}\coloneqq(\bm{u}_{1},\ldots,\bm{u}_{n})\). With this definition in mind, we note the following property of the prox operator.

**Lemma A.17** ([37, 77]).: _The prox operator \(\Pi_{\bm{x}}(\cdot)\) is \(1\)-Lipschitz continuous with respect to \(\|\cdot\|_{2}\) for any \(\bm{x}\in\prod_{i=1}^{n}\mathcal{X}_{i}\)._

Proof.: Let \(i\in\llbracket\textit{n}\rrbracket\). For any \(\bm{u}_{i},\bm{u}_{i}^{\prime}\in\mathbb{R}^{d_{i}}\) and \(\bm{x}_{i}\in\mathcal{X}_{i}\),

\[\|\Pi_{\bm{x}_{i}}(\bm{u}_{i})-\Pi_{\bm{x}_{i}}(\bm{u}_{i}^{\prime})\|_{2}=\| \mathcal{P}_{\mathcal{X}_{i}}(\bm{x}_{i}+\bm{u}_{i})-\mathcal{P}_{\mathcal{X} _{i}}(\bm{x}_{i}+\bm{u}_{i}^{\prime})\|_{2}\leq\|\bm{u}_{i}-\bm{u}_{i}^{\prime }\|_{2},\]

where we used the fact that the projection operator is non-expansive with respect to \(\|\cdot\|_{2}\), and that

\[\Pi_{\bm{x}_{i}}(\bm{u}_{i})=\arg\max_{\bm{x}_{i}^{\prime}\in \mathcal{X}_{i}}\left\{\langle\bm{x}_{i}^{\prime},\bm{u}_{i}\rangle-\frac{1}{2 }\|\bm{x}_{i}-\bm{x}_{i}^{\prime}\|_{2}^{2}\right\} =\arg\max_{\bm{x}_{i}^{\prime}\in\mathcal{X}_{i}}\left\{-\frac{1}{ 2}\|\bm{x}_{i}^{\prime}\|_{2}^{2}+\langle\bm{x}_{i}^{\prime},\bm{u}_{i}+\bm{x} _{i}\rangle\right\}\] \[=\arg\min_{\bm{x}_{i}^{\prime}\in\mathcal{X}_{i}}\left\{\|\bm{x}_ {i}^{\prime}-(\bm{u}_{i}+\bm{x}_{i})\|_{2}^{2}\right\}\] \[=\mathcal{P}_{\mathcal{X}_{i}}(\bm{x}_{i}+\bm{u}_{i}).\]

This implies that \(\|\Pi_{\bm{x}}(\bm{u})-\Pi_{\bm{x}}(\bm{u}^{\prime})\|_{2}\leq\|\bm{u}-\bm{u}^ {\prime}\|\), where \(\bm{u}=(\bm{u}_{1},\ldots,\bm{u}_{n})\) and \(\bm{u}^{\prime}=(\bm{u}_{1}^{\prime},\ldots,\bm{u}_{n}^{\prime})\). 

Figure 1: PoA versus \(\mathsf{rPoA}\) in random normal-form games.

Using this lemma, a key observation for the analysis of _clairvoyant_ mirror descent is the following contraction property [85, 37].

**Proposition A.18** ([85, 37]).: _Suppose that \(F\) is \(L\)-Lipschitz continuous. For any \(\bm{x}^{\prime}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) the function_

\[\prod_{i=1}^{n}\mathcal{X}_{i}\ni\bm{w}\mapsto\Pi_{\bm{x}^{\prime}}(\eta F( \bm{w}))\] (32)

_is \((\eta L)\)-Lipschitz continuous. As a result, function (32) is a contraction mapping as long as \(\eta<\frac{1}{L}\)._

This follows directly from Lemma A.17: \(\|\Pi_{\bm{x}^{\prime}}(\eta F(\bm{w}))-\Pi_{\bm{x}^{\prime}}(\eta F(\bm{w}^ {\prime}))\|_{2}\leq\eta\|F(\bm{w})-F(\bm{w}^{\prime})\|_{2}\leq\eta L\|\bm{w} -\bm{w}^{\prime}\|_{2}\).

Proposition A.18 reassures us that fixed points of the contraction mapping (32) not only exist, but \(\epsilon\)-approximate fixed points can also be computed in a time proportional to \(\log(1/\epsilon)\)[85]. In this context, if \(\bm{x}^{(0)}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) is an arbitrary point, we consider the update rule defined for \(t\in\mathbb{N}\) via

\[\bm{x}^{(t)}=\Pi_{\bm{x}^{(t-1)}}(\eta F(\bm{w}^{(t)})),\] (33)

where \(\bm{w}^{(t)}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) is any point such that \(\|\bm{w}^{(t)}-\Pi_{\bm{x}^{(t-1)}}(\eta F(\bm{w}^{(t)}))\|_{2}\leq\epsilon^{ (t)}.\) It is important to note that this sequence \((\bm{x}^{(t)})_{t\geq 1}\) is not uniquely defined, but with a slight abuse we refer to any sequence satisfying (33) as _clairvoyant gradient descent_ (CGD). CGD satisfies the following remarkable regret bound.

**Theorem A.19** ([85, 37]).: _For any \(T\in\mathbb{N}\), the regret of player \(i\in\llbracket n\rrbracket\) under CGD satisfies_

\[\mathsf{Reg}_{i}^{(T)}\leq\frac{D_{\mathcal{X}_{i}}^{2}}{2\eta}-\frac{1}{2\eta }\sum_{t=1}^{T}\|\bm{x}_{i}^{(t)}-\bm{x}_{i}^{(t-1)}\|_{2}^{2}+D_{\mathcal{X} _{i}}L_{i}\sum_{t=1}^{T}\epsilon^{(t)},\] (34)

_where \(L_{i}\in\mathbb{R}_{>0}\) is the Lipschitz constant of \(\bm{u}_{i}\) with respect to \(\|\cdot\|_{2}\)._

Proof.: Given that \(\bm{x}_{i}^{(t)}\coloneqq\Pi_{\bm{x}_{i}^{(t-1)}}(\eta\bm{u}_{i}(\bm{w}_{-i}^ {(t)}))\), the first-order optimality condition implies that for any \(\bm{x}_{i}^{\star}\in\mathcal{X}_{i}\) and \(t\in\llbracket T\rrbracket\),

\[\left\langle\bm{x}_{i}^{(t)}-\bm{x}_{i}^{\star},\eta\bm{u}_{i}(\bm{w}_{-i}^{( t)})-(\bm{x}_{i}^{(t)}-\bm{x}_{i}^{(t-1)})\right\rangle\geq 0.\] (35)

We further have that

\[\langle\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)},\bm{x}_{i}^{(t)}-\bm{x}_{i}^{(t-1)} \rangle=-\frac{1}{2}\|\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)}\|_{2}^{2}+\frac{1}{ 2}\|\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t-1)}\|_{2}^{2}-\frac{1}{2}\|\bm{x}_{i}^{( t)}-\bm{x}_{i}^{(t-1)}\|_{2}^{2},\]

thereby implying through a telescopic summation that

\[\sum_{t=1}^{T}\langle\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)},\bm{x}_{i}^{(t)}-\bm{ x}_{i}^{(t-1)}\rangle\leq\frac{1}{2}D_{\mathcal{X}_{i}}^{2}-\frac{1}{2}\sum_{t=1}^{ T}\|\bm{x}_{i}^{(t)}-\bm{x}_{i}^{(t-1)}\|_{2}^{2}.\]

Combining with (35),

\[\sum_{t=1}^{T}\langle\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)},\bm{u}_{i}(\bm{w}_{-i} ^{(t)})\rangle\leq\frac{1}{2\eta}D_{\mathcal{X}_{i}}^{2}-\frac{1}{2\eta}\sum_ {t=1}^{T}\|\bm{x}_{i}^{(t)}-\bm{x}_{i}^{(t-1)}\|_{2}^{2}.\]

The claim thus follows from the fact that

\[\sum_{t=1}^{T}\langle\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)},\bm{u}_ {i}(\bm{w}_{-i}^{(t)})-\bm{u}_{i}(\bm{x}_{-i}^{(t)})\rangle \geq-\sum_{t=1}^{T}\|\bm{x}_{i}^{\star}-\bm{x}_{i}^{(t)}\|_{2}\| \bm{u}_{i}(\bm{w}_{-i}^{(t)})-\bm{u}_{i}(\bm{x}_{-i}^{(t)})\|_{2}\] \[\geq-D_{\mathcal{X}_{i}}L_{i}\sum_{t=1}^{T}\epsilon^{(t)}.\]Unlike prior work, here we will make crucial use of the negative term in (34). The important lemma below will enable us to express the regret bound (34) in terms of \(i\)'s best response gap; it is analogous to Lemma A.1 we stated earlier for \(\mathtt{GGD}\).

**Lemma A.20**.: _Fix any \(t\in\mathbb{N}\) and let \(\bm{x}^{(t)}=\Pi_{\bm{x}^{(t-1)}}(\eta F(\bm{w}^{(t)}))\), where \(\bm{w}^{(t)}\in\prod_{i=1}^{n}\mathcal{X}_{i}\) is any point such that \(\|\bm{w}^{(t)}-\Pi_{\bm{x}^{(t-1)}}(\eta F(\bm{w}^{(t)}))\|_{2}\leq\epsilon^{ (t)}\). Then,_

\[\mathtt{BRGAP}_{i}(\bm{x}^{(t)})\leq D_{\mathcal{X}_{i}}\|\bm{u}_{i}(\bm{w}^{(t )}_{-i})-\bm{u}_{i}(\bm{x}^{(t)}_{-i})\|_{2}+\frac{D_{\mathcal{X}_{i}}}{\eta}\| \bm{x}^{(t)}_{i}-\bm{x}^{(t-1)}_{i}\|_{2}.\]

Proof.: By the first-order optimality condition, it follows that for any \(\bm{x}^{\star}_{i}\in\mathcal{X}_{i}\),

\[\left\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\eta\bm{u}_{i}(\bm{w}^{(t)}_{- i})-(\bm{x}^{(t)}_{i}-\bm{x}^{(t-1)}_{i})\right\rangle\geq 0,\]

in turn implying that

\[\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{w}^{(t)}_{-i})\rangle \geq-\frac{1}{\eta}\langle\bm{x}^{\star}_{i}-\bm{x}^{(t)}_{i},\bm{x}^{(t)}_{i }-\bm{x}^{(t-1)}_{i}\rangle.\] (36)

Furthermore,

\[\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{w}^{(t)} _{-i})\rangle =\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{x}^{(t )}_{-i})\rangle+\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{w}^{( t)}_{-i})-\bm{u}_{i}(\bm{x}^{(t)}_{-i})\rangle\] \[\leq\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{x}^{ (t)}_{-i})\rangle+\|\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i}\|_{2}\|\bm{u}_{i}(\bm{ w}^{(t)}_{-i})-\bm{u}_{i}(\bm{x}^{(t)}_{-i})\|_{2}\] \[\leq\langle\bm{x}^{(t)}_{i}-\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{x}^{ (t)}_{-i})\rangle+D_{\mathcal{X}_{i}}\|\bm{u}_{i}(\bm{w}^{(t)}_{-i})-\bm{u}_{i }(\bm{x}^{(t)}_{-i})\|_{2}.\] (37)

Combining (36) and (37) yields that

\[\langle\bm{x}^{(t)}_{i},\bm{u}_{i}(\bm{x}^{(t)}_{-i})\rangle-\max_{\bm{x}^{ \star}_{i}\in\mathcal{X}_{i}}\langle\bm{x}^{\star}_{i},\bm{u}_{i}(\bm{x}^{(t )}_{-i})\rangle\geq-D_{\mathcal{X}_{i}}\|\bm{u}_{i}(\bm{w}^{(t)}_{-i})-\bm{u} _{i}(\bm{x}^{(t)}_{-i})\|_{2}-\frac{D_{\mathcal{X}_{i}}}{\eta}\|\bm{x}^{(t)}_{ i}-\bm{x}^{(t-1)}_{i}\|_{2},\]

concluding the proof. 

**Corollary A.21**.: _For any \(T\in\mathbb{N}\), the regret of player \(i\in\llbracket n\rrbracket\) under \(\mathtt{CGD}\) can be bounded as_

\[\mathsf{Reg}^{(T)}_{i}\leq\frac{D_{\mathcal{X}_{i}}^{2}}{2\eta}-\frac{\eta}{4D _{\mathcal{X}_{i}}^{2}}\sum_{t=1}^{T}\Big{(}\mathtt{BRGAP}_{i}(\bm{x}^{(t)}) \Big{)}^{2}+L_{i}D_{\mathcal{X}_{i}}\sum_{t=1}^{T}\epsilon^{(t)}+\frac{1}{2} \eta\sum_{t=1}^{T}\|\bm{u}_{i}(\bm{w}^{(t)}_{-i})-\bm{u}_{i}(\bm{x}^{(t)}_{-i })\|_{2}^{2}.\]

_In particular, if \(\eta\coloneqq\frac{1}{2L}\) and \(\epsilon^{(t)}\leq\frac{D_{\mathcal{X}_{i}}}{\epsilon^{2}}\) for any \(t\in\llbracket T\rrbracket\),_

\[\mathsf{Reg}^{(T)}_{i}\leq 3LD_{\mathcal{X}_{i}}^{2}-\frac{1}{8LD_{\mathcal{X}_{i}}^{ 2}}\sum_{t=1}^{T}\Big{(}\mathtt{BRGAP}_{i}(\bm{x}^{(t)})\Big{)}^{2}+\frac{1}{2 }\eta\sum_{t=1}^{T}\|\bm{u}_{i}(\bm{w}^{(t)}_{-i})-\bm{u}_{i}(\bm{x}^{(t)}_{-i })\|_{2}^{2}.\]

Proof.: By Lemma A.20, it follows that for any \(t\in\llbracket T\rrbracket\),

\[\frac{1}{2\eta}\|\bm{x}^{(t)}_{i}-\bm{x}^{(t-1)}_{i}\|_{2}^{2}\geq\frac{\eta}{4D _{\mathcal{X}_{i}}^{2}}\Big{(}\mathtt{BRGAP}_{i}(\bm{x}^{(t)})\Big{)}^{2}- \frac{1}{2}\eta\|\bm{u}_{i}(\bm{w}^{(t)}_{-i})-\bm{u}_{i}(\bm{x}^{(t)}_{-i})\|_ {2}^{2}.\]

Summing over all \(t\in\llbracket T\rrbracket\) and combining with Theorem A.19 implies the statement since \(L_{i}D_{\mathcal{X}_{i}}\sum_{t=1}^{T}\epsilon^{(t)}\leq L_{i}D_{\mathcal{X}_ {i}}^{2}\sum_{t=1}^{T}\frac{1}{t^{2}}\leq 2LD_{\mathcal{X}_{i}}^{2}\). 

We are now ready to prove Theorem 4.1, which is recalled below.

**Theorem 4.1**.: _Suppose that all players are updating their strategies using \(\mathtt{CGD}\) with \(\epsilon^{(t)}\leq\frac{\min_{i}D_{\mathcal{X}_{i}}}{t^{2}}\) and learning rate \(\eta=\frac{1}{2L}\) in a \((\lambda,\mu)\)-smooth game \(\mathcal{G}\), where \(L\) is the Lipschitz-continuity parameter of \(F\). Then, for any \(\epsilon_{0}>0\) and \(T\geq\frac{64L^{2}D_{\mathcal{X}}^{4}}{\epsilon_{0}^{2}}\) iterations,_

1. _the average correlated distribution of play is a_ \(\frac{4LD_{\mathcal{X}}^{2}}{T}-\text{CCE}\)_;_
2. _there is a time_ \(t^{\star}\in\llbracket T\rrbracket\) _such that_ \[\operatorname{\mathrm{SW}}(\bm{x}^{(t^{\star})})\geq\sup_{\epsilon\geq\epsilon_{0 }}\min\left\{\rho_{\mathcal{G}}(\lambda,\mu)\cdot\operatorname{\mathrm{OPT}}_{ \mathcal{G}}+\frac{\epsilon^{2}}{16(\mu+1)LD_{\mathcal{X}}^{2}},\mathsf{ PoA}^{\epsilon}_{\mathcal{G}}\cdot\operatorname{\mathrm{OPT}}_{ \mathcal{G}}\right\}.\] (3)Proof.: First, by Corollary A.21,

\[\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)} \leq 3L\sum_{i=1}^{n}D_{\mathcal{X}_{i}}^{2}-\frac{1}{8LD_{ \mathcal{X}}^{2}}\sum_{i=1}^{n}\sum_{t=1}^{T}\left(\mathsf{BRGAP}_{i}(\bm{x}^{( t)})\right)^{2}+\frac{1}{2}\eta\sum_{t=1}^{T}\sum_{i=1}^{n}\|\bm{u}_{i}(\bm{w}_{-i}^{( t)})-\bm{u}_{i}(\bm{x}_{-i}^{(t)})\|_{2}^{2}\] \[=3LD_{\mathcal{X}}^{2}-\frac{1}{8LD_{\mathcal{X}}^{2}}\sum_{i=1}^ {n}\sum_{t=1}^{T}\left(\mathsf{BRGAP}_{i}(\bm{x}^{(t)})\right)^{2}+\frac{1}{2} \eta\sum_{t=1}^{T}\|F(\bm{w}^{(t)})-F(\bm{x}^{(t)})\|_{2}^{2}\] \[\leq 3LD_{\mathcal{X}}^{2}-\frac{1}{8LD_{\mathcal{X}}^{2}}\sum_{i= 1}^{n}\sum_{t=1}^{T}\left(\mathsf{BRGAP}_{i}(\bm{x}^{(t)})\right)^{2}+\frac{1} {2}\eta L^{2}\sum_{t=1}^{T}\|\bm{w}^{(t)}-\bm{x}^{(t)}\|_{2}^{2}\] \[\leq 3LD_{\mathcal{X}}^{2}-\frac{1}{8LD_{\mathcal{X}}^{2}}\sum_{i =1}^{n}\sum_{t=1}^{T}\left(\mathsf{BRGAP}_{i}(\bm{x}^{(t)})\right)^{2}+\frac{1 }{2}\eta L^{2}\sum_{t=1}^{T}(\epsilon^{(t)})^{2}\] \[\leq 4LD_{\mathcal{X}}^{2}-\frac{1}{8LD_{\mathcal{X}}^{2}}\sum_{i =1}^{n}\sum_{t=1}^{T}\left(\mathsf{BRGAP}_{i}(\bm{x}^{(t)})\right)^{2},\] (38)

where we used the fact that \(\sum_{t=1}^{T}(\epsilon^{(t)})^{2}\leq D_{\mathcal{X}}^{2}\sum_{t=1}^{T}\frac{ 1}{t^{4}}\leq 2D_{\mathcal{X}}^{2}\) and \(\eta=\frac{1}{2L}\). As a result, Item 1 follows directly from (38) by invoking the well-known fact that the CCE gap is bounded by \(\max_{1\leq i\leq n}\mathsf{Reg}_{i}^{(T)}\).

For Item 2, let us fix any \(\epsilon\geq\epsilon_{0}\). Suppose that at every time \(t\in[\![T]\!]\) it holds that \(\mathsf{BRGAP}_{i}(\bm{x}^{(t)})>\epsilon\) for some player \(i\in[\![n]\!]\). Then, by (38) we conclude that

\[\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)}\leq 4LD_{\mathcal{X}}^{2}-\frac{1}{8LD_{ \mathcal{X}}^{2}}\epsilon^{2}T\leq-\frac{1}{16LD_{\mathcal{X}}^{2}}\epsilon^{ 2}T,\]

since \(T\geq\frac{64L^{2}D_{\mathcal{X}}^{4}}{\epsilon_{0}^{2}}\geq\frac{64L^{2}D_{ \mathcal{X}}^{4}}{\epsilon^{2}}\). As a result, by \((\lambda,\mu)\)-smoothness, it follows that

\[\sum_{i=1}^{n}\mathsf{Reg}_{i}^{(T)}\geq\lambda\mathsf{OPT}_{ \mathcal{G}}T-(1+\mu)\sum_{t=1}^{T}\mathsf{SW}(\bm{x}^{(t)}),\]

in turn implying that

\[\frac{1}{T}\sum_{t=1}^{T}\mathsf{SW}(\bm{x}^{(t)})\geq\rho_{ \mathcal{G}}(\lambda,\mu)\cdot\mathsf{OPT}_{\mathcal{G}}-\frac{1}{T}\sum_{i= 1}^{n}\mathsf{Reg}_{i}^{(T)}\geq\rho_{\mathcal{G}}(\lambda,\mu)\cdot\mathsf{ OPT}_{\mathcal{G}}+\frac{\epsilon^{2}}{16(\mu+1)LD_{\mathcal{X}}^{2}}.\]

As a result, there is a time \(t^{\star}\in[\![T]\!]\) such that \(\mathsf{SW}(\bm{x}^{(t^{\star})})\geq\rho_{\mathcal{G}}(\lambda,\mu)\cdot \mathsf{OPT}_{\mathcal{G}}+\frac{\epsilon^{2}}{16(\mu+1)LD_{\mathcal{X}}^{2}}\). In the contrary case, if there is a time \(t^{\star}\in[\![T]\!]\) such that \(\mathsf{BRGAP}_{i}(\bm{x}^{(t^{\star})})\leq\epsilon\) for any player \(i\in[\![n]\!]\), it follows that \(\mathsf{SW}(\bm{x}^{(t^{\star})})\geq\mathsf{PoA}_{\mathcal{G}}^{\star}\cdot \mathsf{OPT}_{\mathcal{G}}\) (by definition). This concludes the proof. 

In particular, we note that Corollary 4.3 stated earlier in the main body is an immediate consequence of Theorem 4.1 under Condition 4.2 since \(r\mathsf{PoA}_{\mathcal{G}}\geq\rho_{\mathcal{G}}\). It is also worth noting that for the special case of normal-form games, one can state Theorem 4.1 so that the first term in the right-hand side of (3) reads \(\rho_{\mathcal{G}}(\lambda,\mu)\cdot\mathsf{OPT}_{\mathcal{G}}+\frac{\epsilon^ {2}}{32(\mu+1)L}\); thus, for a broad class of games (see Lemmas A.7 to A.9), the improvement over the smoothness bound is an absolute constant when \(\epsilon\) and \(\mu\) are also bounded by absolute constants. Relatedly, we should note that Theorem 6.2 applies in the regime where \(\mu\) is polynomially bounded (we are not aware of any application of smoothness where this is not the case).

_Remark A.22_.: It is direct to see that Item 2 of Theorem 4.1 can be refined so that there is a set \(S\subseteq[\![T]\!]\), with \(|S|\geq(1-\gamma)T\), so that

\[\frac{1}{|S|}\sum_{t\in S}\mathsf{SW}(\bm{x}^{(t)})\geq\sup_{ \epsilon\geq\epsilon_{0}}\min\left\{\rho_{\mathcal{G}}\cdot\mathsf{OPT}_{ \mathcal{G}}+\frac{\gamma\epsilon^{2}}{16(\mu+1)LD_{\mathcal{X}}^{2}}, \mathsf{PoA}_{\mathcal{G}}^{\star}\cdot\mathsf{OPT}_{\mathcal{G}}\right\}.\]

_Remark A.23_.: Theorem 4.1 can also be refined using the primal-dual framework of Nadav and Roughgarden [75] discussed in Appendix A.3, with the caveat that the variables \(\{z_{i}\}_{i=1}^{n}\) need to again have a bounded pairwise ratio. It is interesting to note, however, that using \(\mathtt{CGD}\) enables making non-trivial conclusions even when a subset of the variables \(\{z_{i}\}_{i=1}^{n}\) are zero in an optimal solution. In particular, when the quantity \(\sum_{i=1}^{n}z_{i}\mathsf{Reg}_{i}^{(T)}\) is nonnegative, we can readily draw the non-trivial conclusion that all players in the set \(\{i\in[\![n]\!]:z_{i}\neq 0\}\) are eventually best responding (by virtue of Corollary A.21). It is not at all clear if such conclusions apply to \(\mathtt{OGD}\) as well. In other words, using \(\mathtt{CGD}\) one can essentially replace \(\mathsf{PoA}_{\mathcal{G}}^{*}\) in Theorem 4.1 by the price of anarchy with respect to a solution concept in which a _particular subset_ of players are best responding. Shedding light to this type of guarantee necessitates understanding the implications of having some of the variables \(\{z_{i}\}_{i=1}^{n}\) being zero in an optimal solution of the LP (21). Relatedly, even if we constraint \(z_{1}=z_{2}=\cdots=z_{n}=z\), can we characterize the games for which \(z\approx 0\)? We explained earlier in Remark A.10 that such pathologies (typically) occur in constant-sum games, for which questions concerning social welfare are trivial.

### Beyond smoothness

In this subsection, we discuss an example in which the welfare predicted by the smoothness framework is far from the welfare obtained by learning algorithms such as \(\mathtt{OGD}\). We then suggest a natural direction that enables obtaining sharper predictions; we have already seen refined guarantees beyond the standard smoothness framework in Appendices A.3 and A.4, but here we explore a different direction.

Our example is again based on Shapley's game, a bimatrix game in normal form defined with the matrices

\[\mathbf{A}=\begin{bmatrix}0&0&1\\ 1&0&0\\ 0&1&0\end{bmatrix},\mathbf{B}=\begin{bmatrix}0&1&0\\ 0&0&1\\ 1&0&0\end{bmatrix}.\] (39)

**Claim A.24**.: _Let \(\mathcal{G}\) be the bimatrix game defined in (39). Then, \(\mathsf{rPoA}_{\mathcal{G}}=0\)._

Proof.: Let \((\bm{x}_{1}^{\star},\bm{x}_{2}^{\star})\coloneqq((1,0,0),(0,1,0))\) be a welfare-maximizing joint action. By symmetry, the following argument will apply to any welfare-maximizing joint action. Consider \((\bm{x}_{1},\bm{x}_{2})=((0,1,0),(0,1,0))\). Then, we have that \(u_{1}(\bm{x}_{1}^{\star},\bm{x}_{2})+u_{2}(\bm{x}_{2}^{\star},\bm{x}_{1})=( \bm{x}_{1}^{\star})^{\top}\mathbf{A}\bm{x}_{2}+\bm{x}_{1}^{\top}\mathbf{B}\bm {x}_{2}^{\star}=0+0\). As a result, the smoothness constraint corresponding to \((\bm{x}_{1},\bm{x}_{2})\) necessitates that \(0\geq\lambda\), which is incompatible with the constraint that \(\lambda>0\) (Definition 2.1). 

In spite of the fact that \(\mathsf{rPoA}_{\mathcal{G}}=0\), we see in Figure 2 (left) that \(\mathtt{OGD}\) approaches close to the optimal social welfare \(1\). This can be explained by the fact that certain joint actions--in particular, those in the diagonal--are played with small probability under \(\mathtt{OGD}\) (right-side of Figure 2). This motivates introducing a more refined notion of smoothness in which the smoothness constraints are only enforced on the joint actions that are visited with a non-negligible probability.

Figure 2: The behavior of \(\mathtt{OGD}\) in the bimatix game (39) with \(\eta\coloneqq 0.01\). On the left, we plot the average social welfare of the dynamics. On the right, we plot the probability of playing a joint action in the diagonal.

One class of games in which one can easily make such refinements concerns games solvable under iterated elimination of strictly dominated actions. As a concrete example, let

\[\mathbf{A}=\begin{bmatrix}0&0\\ 1&1\end{bmatrix},\mathbf{B}=\begin{bmatrix}1&0\\ 0&1\end{bmatrix}.\] (40)

**Observation A.25**.: _Let \(\mathcal{G}\) be the game defined in (40). Then, \(\mathsf{rPoA}_{\mathcal{G}}=\frac{1}{2}\). Furthermore, if \(\mathcal{G}^{\prime}\) is the game4 resulting from iterative removal of strictly dominated actions, then \(\mathsf{rPoA}_{\mathcal{G}^{\prime}}=1\)._

Footnote 4: Under elimination of strictly dominated actions, this is always uniquely defined.