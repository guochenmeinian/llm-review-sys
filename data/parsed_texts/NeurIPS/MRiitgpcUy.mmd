# On the Exploration of Local Significant Differences

For Two-Sample Test

 Zhi-Jian Zhou, Jie Ni, Jia-He Yao, Wei Gao

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{zhouzj, nij, yaojh, gaow}@lamda.nju.edu.cn

###### Abstract

Recent years have witnessed increasing attentions on two-sample test with diverse real applications, while this work takes one more step on the exploration of local significant differences for two-sample test. We propose the \(\text{ME}_{\text{MaBJD}}\), an effective test for two-sample testing, and the basic idea is to exploit local information by multiple Mahalanobis kernels and introduce bi-directional hypothesis for testing. On the exploration of local significant differences, we first partition the embedding space into several rectangle regions via a new splitting criterion, which is relevant to test power and data correlation. We then explore local significant differences based on our bi-directional masked \(p\)-value together with the \(\text{ME}_{\text{MaBJD}}\) test. Theoretically, we present the asymptotic distribution and lower bounds of test power for our \(\text{ME}_{\text{MaBJD}}\) test, and control the familywise error rate on the exploration of local significant differences. We finally conduct extensive experiments to validate the effectiveness of our proposed methods on two-sample test and the exploration of local significant differences.

## 1 Introduction

Two-sample test has attracted much attention with diverse applications such as cancer detection [1], distribution-shift detection [2], generative modeling [3; 4], etc. The basic problem is to assess whether two i.i.d. samples are drawn from the same distribution. Various kernel-based methods have been developed for two-sample test such as Maximum Mean Discrepancy (MMD) [5; 6; 7; 8] and Mean Embedding (ME) [9; 10; 11]. Another relevant approach is to construct a binary classifier and assess two samples according to classification performance [12; 13; 14; 15; 16; 17; 18; 19; 20]. For an overview of two-sample test, we refer to a survey [21, and references therein].

In many real applications, however, it is necessary to take one more step to explore and understand local significant differences, rather than only two-sample test. For example, a scientific problem in galaxy morphology is to identify some local regions of significant differences between two kinds of galaxies, which is important to discover galaxy formation and evolution history [22]. On the analysis of mass cytometry data in cell biology, researchers are always interested in finding local regions of significantly different abundance between disease and healthy samples [23].

Several attempts have been made to explore local significant differences in the past years. A feasible solution is to partition space into several regions, and identify significantly different regions according to their cardinalities of samples [24; 25; 26; 27]. Another relevant work is to identify local significant differences by estimating kernel densities [28; 29] and conditional probabilities [30]. Generally, it is not easy to deal with complex data by simply counting cardinalities of samples, regardless of data intrinsic correlations, and it is also difficult to make accurate estimation of kernel densities and conditional probabilities without sufficient data, especially for many regions with finite samples.

This work presents a new two-sample test from local and directional information, and further explore local significant differences. The main contributions can be summarized as follows:

* We propose the effective ME\({}_{\text{MaBID}}\) test for two-sample testing, and the basic idea is to exploit local information by multiple Mahalanobis kernels and introduce bi-directional hypothesis for testing. Intuitively, Mahalanobis kernels are more flexible to exploit local differences from neighborhoods and feature maps, and the bi-directional hypothesis is beneficial to improve the sensitivity of two-sample test with proper parameter adaptation.
* We partition the embedding space into several rectangle regions based on a new splitting criterion, which is relevant to test power and data correlation. We introduce the bi-directional masked \(p\)-value for each rectangle region, and finally explore local regions with significant difference based on our bi-directional masked \(p\)-value together with the ME\({}_{\text{MaBiD}}\) test.
* We present theoretical guarantees for our ME\({}_{\text{MaBID}}\) test via the asymptotic distribution, as well as the lower bounds on the test power for our test. We also present the upper bounds on familywise error rate for our exploration of local significant differences.
* We conduct extensive experiments to validate the effectiveness and efficiency of our methods. Specifically, our methods achieve better performance on most datasets for two-sample test and exploring local significant differences, along with comparable or smaller running time.

The rest of this work is organized as follows: Section 2 presents our ME\({}_{\text{MaBiD}}\) test. Section 3 explores local significant differences. Section 4 conducts extensive experiments, and Section 5 concludes with future work. All technical proofs are given in Appendix A.

## 2 Our ME\({}_{\text{MaBiD}}\) Test for Two-Sample Testing

Let \(\mathbb{P}\) and \(\mathbb{Q}\) denote two (unknown) Borel probability measures over an instance space \(\mathcal{X}\subseteq\mathbb{R}^{d}\), and \(X=\{\bm{x}_{i}\}_{i=1}^{m}\) and \(Y=\{\bm{y}_{j}\}_{j=1}^{n}\) are two i.i.d. samples from \(\mathbb{P}\) and \(\mathbb{Q}\), respectively. The goal of two-sample test is to assess whether \(X\) and \(Y\) are drawn from the same distribution; in other words, we aim to assess whether \(\mathbb{P}=\mathbb{Q}\) from two samples \(X\) and \(Y\).

We introduce some necessary notations used in this work. Write \([\tau]=\{1,2,\cdots,\tau\}\) for integer \(\tau\geq 2\), and \(|A|\) denotes the cardinality of set \(A\). Let \(\mathbf{I}_{d}\) be the identity matrix of size \(d\times d\). For a vector \(\bm{a}=[a_{1},a_{2},\cdots,a_{d}]\), denote by \(\text{sgn}(\bm{a})=[\text{sgn}(a_{1}),\text{sgn}(a_{2}),\cdots,\text{sgn}(a_{ d})]\) with \(\text{sgn}(a_{i})=a_{i}/|a_{i}|\) for \(a_{i}\neq 0\); otherwise, \(\text{sgn}(a_{i})=0\). Let \(\chi_{\ell}^{2}\) be the \(\chi^{2}\) distribution with \(\ell\) degree of freedom, as well as the \(p\)-value function \(\chi_{\ell}^{2}(\cdot)\). Denote by \(\chi_{\ell,\alpha}^{2}\) the \(\alpha\)-quantile of distribution \(\chi_{\ell}^{2}\) for \(\alpha\in(0,1)\).

### Learning multiple Mahalanobis kernels via maximizing test power in training

Following ME test [9; 10], we begin with a set of test locations \(\mathcal{V}=\{\bm{v}_{1},\bm{v}_{2},\dots,\bm{v}_{\ell}\}\subset\mathcal{X}\) to construct discriminative features. For every \(\bm{v}_{i}\in\mathcal{V}\), we introduce a Mahalanobis kernel as follows:

\[\kappa_{i}(\bm{x},\bm{v}_{i})=\exp\left(-(\bm{x}-\bm{v}_{i})^{\top}M_{i}(\bm{ x}-\bm{v}_{i})/2\gamma_{i}^{2}\right)\ \ \text{for}\ \ \gamma_{i}>0\ \text{and positive definite matrix}\ M_{i}.\] (1)

Here, we propose multiple Mahalanobis kernels for two-sample test, which is motivated from multiple kernel learning [31; 32] and Mahalanobis distance [33; 34; 35]. The advantage of multiple Mahalanobis kernels is to exploit intrinsic structures and correlations from different directions and regions, and adjust geometrical distribution of data so as to enlarge the distance between different samples [36; 37].

Figure 1: An illustration of different contours of Mahalanobis and Gaussian kernels for two-sample test.

This is different from previous Gaussian kernel \(\kappa_{i}(\bm{x},\bm{v}_{i})=\exp(-\left\|\bm{x}-\bm{v}_{i}\right\|^{2}/2\gamma^{2})\)[10, 38], which deals with every direction isotropically without difference. Figure 1 presents an illustration of different contours of Mahalanobis and Gaussian kernels for two-sample test. As we can see, Mahalanobis kernels are more flexible to exploit different directional information than Gaussian kernels. Our work is also different from previous deep kernel approaches [4, 39], which train single one deep neural network combined with Gaussian kernel for variations in distribution smoothness and shape.

We then embed each element in \(X=\{\bm{x}_{i}\}_{i=1}^{m}\) and \(Y=\{\bm{y}_{j}\}_{j=1}^{n}\) into an \(\ell\)-dimensional space as

\[\hat{\bm{x}}_{i}=(\kappa_{1}(\bm{x}_{i},\bm{v}_{1}),\cdots,\kappa_{\ell}(\bm{ x}_{i},\bm{v}_{\ell}))^{\top}\text{ and }\hat{\bm{y}}_{j}=(\kappa_{1}(\bm{y}_{j},\bm{v}_{1}),\cdots,\kappa_{\ell}(\bm{ y}_{j},\bm{v}_{\ell}))^{\top},\text{ respectively }.\] (2)

Denote by \(\hat{X}=\{\hat{\bm{x}}_{i}\}_{i=1}^{m}\) and \(\hat{Y}=\{\hat{\bm{y}}_{j}\}_{j=1}^{n}\). We define the _pooled covariance matrix_ as

\[\Sigma_{\hat{X},\hat{Y}}=\sum_{i=1}^{m}\frac{(\hat{\bm{x}}_{i}-\bm{c}_{\hat{X }})(\hat{\bm{x}}_{i}-\bm{c}_{\hat{X}})^{\top}}{m+n-2}+\sum_{i=1}^{n}\frac{( \hat{\bm{y}}_{i}-\bm{c}_{\hat{Y}})(\hat{\bm{y}}_{i}-\bm{c}_{\hat{Y}})^{\top}} {m+n-2}+\epsilon\mathbf{I}_{d}\,\] (3)

where \(\bm{c}_{\hat{X}}=\sum_{i=1}^{m}\hat{\bm{x}}_{i}/m\) and \(\bm{c}_{\hat{Y}}=\sum_{j=1}^{n}\hat{\bm{y}}_{i}/n\), and \(\epsilon\mathbf{I}_{d}\) is introduced to guarantee the positive definiteness for small constant \(\epsilon>0\). We consider the _Hotelling \(T^{2}\) statistic_, as in [40, 41, 42],

\[\mathcal{T}(\hat{X},\hat{Y})=mn(\bm{c}_{\hat{X}}-\bm{c}_{\hat{Y}})^{\top} \Sigma_{\hat{X},\hat{Y}}^{-1}(\bm{c}_{\hat{X}}-\bm{c}_{\hat{Y}})\big{/}(m+n)\.\] (4)

Test power is the probability of correctly identifying two different samples. Maximizing \(\mathcal{T}(\hat{X},\hat{Y})\) is essentially equivalent to maximizing a lower bound of test power [10, 11], and we learn test locations and Mahalanobis kernels as follows:

\[\{\mathcal{V},M_{1},\cdots,M_{\ell},\gamma_{1},\cdots,\gamma_{\ell}\}\in\arg \max\{\mathcal{T}(\hat{X},\hat{Y})\}\.\] (5)

We take gradient method [43] to solve the above optimization, as done by Jitkrittum et al. [10], and the details are presented in Appendix B.

We decompose \(\Sigma_{\hat{X},\hat{Y}}=\bm{L}\bm{L}\) via the Schur method [44] to remove feature correlations, and it follows

\[\mathcal{T}(\hat{X},\hat{Y})=\frac{mn}{m+n}(\bm{c}_{\hat{X}}-\bm{c}_{\hat{Y}} )^{\top}\Sigma_{\hat{X},\hat{Y}}^{-1}(\bm{c}_{\hat{X}}-\bm{c}_{\hat{Y}})=\frac {mn}{m+n}\left\|\bm{L}^{-1}\bm{c}_{\hat{X}}-\bm{L}^{-1}\bm{c}_{\hat{Y}}\right\| _{2}^{2}\.\]

Hence, \(\mathcal{T}(\hat{X},\hat{Y})\) essentially measures the difference between two samples via the \(L_{2}\)-norm of vector \(\bm{L}^{-1}\bm{c}_{\hat{X}}-\bm{L}^{-1}\bm{c}_{\hat{Y}}\). We further exploit their _inference direction_, defined by

\[\bm{F}=\text{sgn}\left(\bm{L}^{-1}\bm{c}_{\hat{X}}-\bm{L}^{-1}\bm{c}_{\hat{Y}} \right)\in\{-1,0,+1\}^{\ell}\.\] (6)

#### Bi-directional hypothesis for testing

Let \(\hat{\mathbb{P}}\) and \(\hat{\mathbb{Q}}\) be the corresponding embedding distributions from the original \(\mathbb{P}\) and \(\mathbb{Q}\), respectively. Denote by \(\bm{\mu}_{\hat{\mathbb{P}}}=E_{\hat{\bm{x}}^{\prime}\sim\mathbb{P}}[\hat{\bm{x }}^{\prime}]\) and \(\bm{\mu}_{\hat{\mathbb{Q}}}=E_{\hat{\bm{y}}^{\prime}\sim\mathbb{Q}}[\hat{\bm{y }}^{\prime}]\). We consider the following null hypothesis

\[H_{0}\colon\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\.\]

The null hypothesis \(H_{0}\) can be used to test whether \(\mathbb{P}=\mathbb{Q}\) by the following lemma:

**Lemma 1**.: _We have \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\) iff \(\mathbb{P}=\mathbb{Q}\), for bounded Mahalanobis kernels \(\{\kappa_{j}\}_{j=1}^{\ell}\) and for test locations \(\{\bm{v}_{j}\}_{j=1}^{\ell}\) drawn i.i.d. from a absolutely-continuous distribution w.r.t. Lebesgue measure._

Let \(\hat{X}^{\prime}=\{\hat{\bm{x}}^{\prime}_{i}\}_{i=1}^{m^{\prime}}\) and \(\hat{Y}^{\prime}=\{\hat{\bm{y}}^{\prime}_{j}\}_{j=1}^{n^{\prime}}\) denote two embedding testing samples. We make similar Schur decomposition \(\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}=\bm{L}^{\prime}\bm{L}^{\prime}\), and calculate testing statistic \(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\) according to Eqn. (4). Based on Lemma 1, we can present the asymptotic distribution of statistic \(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\) as follows:

**Theorem 2**.: _The testing statistic \(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\) is almost surely asymptotically distributed as \(\chi_{\ell}^{2}\) if \(\mathbb{P}=\mathbb{Q}\); otherwise, \(\chi_{\ell}^{2}(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime}))\to 0\) as \(m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\to\infty\)._

From Theorem 2, we propose the _bi-directional hypothesis_, by considering inference direction,

\[h(\hat{X}^{\prime},\hat{Y}^{\prime})=\begin{cases}\mathbb{I}\left[\,\chi_{\ell}^{2} \big{(}\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\big{)}\leq\beta\alpha \right]&\text{for}\quad\bm{F}^{\top}\bm{L}^{\prime-1}(\bm{c}_{\hat{X}^{\prime}} -\bm{c}_{\hat{Y}^{\prime}})\geq 0\\ \mathbb{I}\left[\,\chi_{\ell}^{2}\big{(}\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{ \prime})\big{)}\leq(2-\beta)\alpha\right]&\text{for}\quad\bm{F}^{\top}\bm{L}^{ \prime-1}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{Y}^{\prime}})<0,\end{cases}\] (7)where \(\alpha\in(0,1)\) is the significance level of hypothesis test, and \(\beta\in[1,2]\) is an adaptive parameter.

Our bi-directional hypothesis is essentially about designing a rejection region of null hypothesis [45]. How to design an efficient rejection region is an interesting problem from the early work [46], and some techniques has been developed for selecting rejection regions [47, 48, 49]. We consider the most discriminative directions \(\bm{F}\) and \(-\bm{F}\) in our bi-directional hypothesis, which could improve the sensitivity for two-sample test by selecting appropriate parameters according to different datasets.

Our bi-directional hypothesis can be viewed as a generalization of previous hypotheses, that is,

* By setting \(\beta=1\), our test has been the non-directional hypothesis [50, 9, 11] regardless of direction information, which is also referred to as two-sided/tailed hypothesis [51, 49, 52];
* By setting \(\beta=2\) and \(\bm{F}=(1,1,\cdots,1)^{\top}\), our test has become one-directional hypothesis [53, 54, 55], which is also referred to as one-sided/tailed hypothesis [52, 56].

Notice that previous non/one-directional hypotheses fix the structures of rejection region for a given significance level \(\alpha\), whereas our bi-directional hypothesis could adjust rejection region according to inference direction \(\bm{F}\) w.r.t. different datasets, which can be illustrated in Figure 2. Here, we consider an illustrative dataset, and our bi-directional hypothesis gives the rejection region adaptive to dataset, which could yield higher test power in two-sample test, as shown in Figure 5 (in Section 4).

We can also present some distribution and probability information for \(\bm{F}^{\top}\bm{L}^{\prime-1}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{Y}^{ \prime}})\geq 0\) in Eqn. (7). Denote by \(\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}=E_{\hat{X}^{\prime}\sim\hat{ \mathbb{P}}^{\prime},\hat{Y}^{\prime}\sim\hat{\mathbb{Q}}^{\prime\prime}}|\bm {L}^{\prime}|\) and \(\xi=\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{ Y}^{\prime}})\geq 0]\). We have

**Lemma 3**.: _For inference direction \(\bm{F}\) in Eqn. (6) and for embedding samples \(\hat{X}^{\prime}\) and \(\hat{Y}^{\prime}\), we have_

\[\bm{L}^{\prime-1}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{Y}^{\prime}})\sim \mathcal{N}\big{(}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{ \mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}),\omega^{-1}\bm{I}_{\hat{\mathbb{Q}} }\big{)}\ \ \text{and}\ \ \text{sgn}(\bm{F}^{\top}\bm{L}^{\prime-1}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{ \hat{Y}^{\prime}}))\sim\mathcal{T}\mathcal{P}(\xi)\]

_with \(\omega=m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\) and \(\xi=1-\Phi(-\sqrt{\omega/\ell}\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q} }}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}))\). Here, \(\Phi(\cdot)\) is the distribution function of standard Gaussian, and \(\mathcal{T}\mathcal{P}(\xi)\) denotes a distribution over \(\{-1,+1\}\) with probability \(\xi\) on the selection of \(+1\)._

The selection of parameter \(\beta\) is highly positive-relevant to the probability \(\xi\). This is because a larger \(\xi\) implies larger difference between two samples in the inference direction \(\bm{F}\), and we should select a larger \(\beta\) to enlarge the rejection region and improve the sensitivity of dataset. Figure 6 (in Section 4) shows such positive relevance between the optimal parameter \(\beta\) and probability \(\xi\) empirically.

We finally present theoretical analysis on test power and type-I error of our bi-directional hypothesis. Let \(f(x\colon n_{1},n_{2},\lambda)\) be the density function of noncentral \(F\)-distribution with \(n_{1}\) and \(n_{2}\) degrees of freedom and non-centrality parameter \(\lambda\), and denote by \(F_{n_{1},n_{2},\alpha}\) the \(\alpha\)-quantile of central \(F\)-distribution with \(n_{1}\) and \(n_{2}\) degrees of freedom for \(\alpha\in(0,1)\). We define the following probability, from the work of [57],

\[q(n_{1},n_{2},\lambda,\alpha)=\int_{F_{n_{1},n_{2},\alpha}}^{\infty}f\left(x \mid n_{1},n_{2},\lambda\right)dx\.\] (8)

**Theorem 4**.: _For our bi-directional hypothesis \(h(\hat{X}^{\prime},\hat{Y}^{\prime})\), the test power can be lower bounded by_

\[q(\ell,\nu-\ell,\lambda,\beta\alpha)\cdot\xi+\Phi\left(-(\chi_{\ell,(2-\beta) \alpha}^{2})^{1/2}-(\omega/\ell)^{1/2}\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{ \mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\right)\]

Figure 2: An illustration of different rejection regions on two samples for our bi-directional hypothesis and previous non/one-directional hypothesis. Our rejection region can be adaptive according to different datasets.

_if \(\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{P}}}>\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu }_{\hat{\mathbb{Q}}}\); and the test power can also be lower bounded by_

\[q(\ell,\nu-\ell,\lambda,(2-\beta)\alpha)(1-\xi)+1-\Phi\left((\chi_{\ell,\beta \alpha}^{2})^{1/2}-(\omega/\ell)^{1/2}\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}}, \hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}} })\right)\]

_if \(\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{P}}}<\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{ \mu}_{\hat{\mathbb{Q}}}\); and the type-I error rate is equal to \(\alpha\) if \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\). Here, \(\omega=m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\), \(\nu=m^{\prime}+n^{\prime}-1\) and \(\lambda=\omega\|\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{ \hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\|_{2}^{2}\)._

This theorem presents lower bounds on test power, and the performance of the statistical test is maintained under the general condition. Notice that the type-I error in our hypothesis test is controlled only by the significant level \(\alpha\), regardless of different \(\xi\) and \(\beta\).

We call our test as \(\textit{ME}_{\textit{MaBiD}}\) test because of multiple Mahalanobis kernels in training and our bi-directional hypothesis in testing.

## 3 Explore Local Significant Differences for Two-sample Test

On the exploration of local significant difference, most previous studies [24; 25; 26; 27; 28] partition the instance space into several regions, and then exploit the difference on each region from two samples. Motivated from polya tree method [26; 58], we first partition the embedding instance space with a new splitting criterion, which is relevant to test power and data correlations. We then exploit local regions (i.e., leaves nodes of partition tree) with significant difference.

### Partition of the embedding instance space

Our partition tree is constructed iteratively as follows: We initiate the tree root with embedding space \((0,1]^{\ell}\). In each iteration, each node is associated with a rectangle region, and all leaves constitute a partition of embedding instance space. The following procedure is repeated \(s-1\) iterations (\(s\geq 2\)):

* Randomly select a leaf node, denote by \(\mathcal{B}\), uniformly over leaf nodes of the largest \(|\hat{X}\cap\mathcal{B}|\).
* Let \(\tau_{j}=\text{median}\{\hat{\bm{x}}_{i,j}\colon\hat{\bm{x}}_{i}\in\hat{X} \cap\mathcal{B}\}\) for \(j\in[\ell]\). We select the best splitting feature \[j^{*}\in\arg\max_{j\in[\ell]}\left\{\mathcal{T}(\hat{X}_{\mathcal{B}^{j}_{l}},\hat{Y}_{\mathcal{B}^{j}_{l}})\times\mathcal{T}(\hat{X}_{\mathcal{B}^{j}_{l}},\hat{Y}_{\mathcal{B}^{j}_{l}})\right\}\,\] with \(\hat{X}_{\mathcal{B}^{j}_{l}}=\mathcal{B}^{j}_{l}\cap\hat{X}\), \(\hat{X}_{\mathcal{B}^{j}_{r}}=\mathcal{B}^{j}_{r}\cap\hat{X}\), \(\hat{Y}_{\mathcal{B}^{j}_{l}}=\mathcal{B}^{j}_{l}\cap\hat{Y}\) and \(\hat{Y}_{\mathcal{B}^{j}_{r}}=\mathcal{B}^{j}_{r}\cap\hat{Y}\). Here, \(\mathcal{B}^{j}_{l}\) and \(\mathcal{B}^{j}_{r}\) are left and right children of \(\mathcal{B}\) w.r.t. the \(j\)-th splitting feature and splitting position \(\tau_{j}\), respectively, and \(\mathcal{T}(\cdot,\cdot)\) is defined by Eqn. (4).
* Select the splitting position \(\tau_{j^{*}}=\text{median}\{\hat{\bm{x}}_{i,j^{*}}\colon\hat{\bm{x}}_{i}\in \hat{X}\cap\mathcal{B}\}\).

We finally get the partition tree with \(s\) leaf nodes, associated with \(s\) rectangle regions \(\mathcal{B}_{1},\mathcal{B}_{2},\ldots,\mathcal{B}_{s}\). Algorithm 1 presents the detailed description on tree construction and rectangle region splitting.

We take the statistic \(\mathcal{T}(\cdot,\cdot)\) as a splitting criterion, relevant to test power and data correlations, and it is helpful to exploit local significant difference directly. We also adopt the median splitting position with equal probabilities on partitioned regions, i.e., balanced examples for each partition region, and this could yield better performance than regular grids, as shown empirically in [59, 60].

Our partition tree is different from previous \(p\)-value histogram based on Chi-square test [61, 62], where the difference is measured by cardinalities of elements in two samples over a local rectangle region. Our splitting criterion is also different from that of previous decision trees [63, 64, 65, 66], which consider some information-theoretic criterions such as entropy, Gini index, information gain, etc. In comparisons, our statistic \(\mathcal{T}(\cdot,\cdot)\) is more essential to reflect the test power for two-sample test.

For each rectangle region \(\mathcal{B}_{i}\), let \(\bm{c}_{\hat{X}_{\mathcal{B}_{i}}}\) and \(\bm{c}_{\hat{Y}_{\mathcal{B}_{i}}}\) be the means of \(\hat{X}_{\mathcal{B}_{i}}=\mathcal{B}_{i}\cap\hat{X}\) and \(\hat{Y}_{\mathcal{B}_{i}}=\mathcal{B}_{i}\cap\hat{Y}\), respectively. We make similar Schur decomposition \(\bm{L}_{\mathcal{B}_{i}}\bm{L}_{\mathcal{B}_{i}}=\Sigma_{\hat{X}_{\mathcal{B }_{i}},\hat{Y}_{\mathcal{B}_{i}}}\) for covariance matrix \(\Sigma_{\hat{X}_{\mathcal{B}_{i}},\hat{Y}_{\mathcal{B}_{i}}}\), and introduce the local inference direction for each \(\mathcal{B}_{i}\) as follow:

\[\bm{F}_{\mathcal{B}_{i}}=\text{sgn}\left(\bm{L}_{\mathcal{B}_{i}}^{-1}\bm{c}_{ \hat{X}_{\mathcal{B}_{i}}}-\bm{L}_{\mathcal{B}_{i}}^{-1}\bm{c}_{\hat{Y}_{ \mathcal{B}_{i}}}\right)\in\{-1,0,+1\}^{\ell}\;.\] (9)

For different rectangle regions, we could have different or even contrary inference directions, which is helpful to exploit local differences from distributional shapes of two samples.

#### Exploration of local significant differences

For testing embedding samples \(\hat{X}^{\prime}\) and \(\hat{Y}^{\prime}\), we denote by \(\hat{X}^{\prime}_{\mathcal{B}_{i}}=\mathcal{B}_{i}\cap\hat{X}^{\prime}\) and \(\hat{Y}^{\prime}_{\mathcal{B}_{i}}=\mathcal{B}_{i}\cap\hat{Y}^{\prime}\) with their respective means \(\bm{c}_{\hat{X}^{\prime}_{\mathcal{B}_{i}}}\) and \(\bm{c}_{\hat{Y}^{\prime}_{\mathcal{B}_{i}}}\) for each rectangle region \(\mathcal{B}_{i}\), and calculate testing statistic \(\mathcal{T}_{\mathcal{B}_{i}}=\mathcal{T}(\hat{X}^{\prime}_{\mathcal{B}_{i}}, \hat{Y}^{\prime}_{\mathcal{B}_{i}})\) by Eqn. (4).

We propose the new _bi-directional masked \(p\)-value_ for each rectangle region \(\mathcal{B}_{i}\) as follows:

\[g(\hat{X}^{\prime}_{\mathcal{B}_{i}},\hat{Y}^{\prime}_{\mathcal{B}_{i}})= \begin{cases}\min\left\{\frac{\chi^{2}_{\ell}(\mathcal{T}_{\mathcal{B}_{i}})}{ \beta},\frac{p_{*}\left(1-\chi^{2}_{\ell}(\mathcal{T}_{\mathcal{B}_{i}})\right) }{1-\beta p_{*}}\right\}&\text{for}\ \ \bm{F}^{\top}_{\mathcal{B}_{i}}\bm{L}^{\prime-1}_{\mathcal{B}_{i}}(\bm{c}_{\hat{X} ^{\prime}_{\mathcal{B}_{i}}}-\bm{c}_{\hat{Y}^{\prime}_{\mathcal{B}_{i}}})\geq 0\\ \min\left\{\frac{\chi^{2}_{\ell}(\mathcal{T}_{\mathcal{B}_{i}})}{2-\beta}, \frac{p_{*}\left(1-\chi^{2}_{\ell}(\mathcal{T}_{\mathcal{B}_{i}})\right)}{1-(2 -\beta)p_{*}}\right\}&\text{for}\ \ \bm{F}^{\top}_{\mathcal{B}_{i}}\bm{L}^{\prime-1}_{\mathcal{B}_{i}}(\bm{c}_{\hat{X} ^{\prime}_{\mathcal{B}_{i}}}-\bm{c}_{\hat{Y}^{\prime}_{\mathcal{B}_{i}}})<0\;, \end{cases}\] (10)

where \(\bm{L}^{\prime-1}_{\mathcal{B}_{i}}\) is from Schur decomposition \(\Sigma_{\hat{X}^{\prime}_{\mathcal{B}_{i}},\hat{Y}^{\prime}_{\mathcal{B}_{i}}}= \bm{L}^{\prime-1}_{\mathcal{B}_{i}}\bm{L}^{\prime-1}_{\mathcal{B}_{i}}\), \(p_{*}\in(0,1)\) is a parameter on significance level, and \(\beta\) is an adaptive parameter. Here, we also consider two discriminative directions \(\bm{F}_{\mathcal{B}_{i}}\) and \(-\bm{F}_{\mathcal{B}_{i}}\) on each rectangle region \(\mathcal{B}_{i}\), which is different from previous masked \(p\)-value [67] without directional information.

Our bi-directional masked \(p\)-value directly reflects the significant level of local difference when there is a significant difference in local \(\mathcal{B}_{i}\), similarly to [67]. The smaller the bi-directional masked \(p\)-value, the more significant the local difference. On the other hand, the bi-directional masked \(p\)-value is a random number with uniform distribution over \((0,p_{*})\) when there is no significant difference in \(\mathcal{B}_{i}\), since \(\chi^{2}_{\ell}(\mathcal{T}_{\mathcal{B}_{i}})\) follows a uniform distribution in such case [68].

Based on such recognition, we resort rectangle regions as \(\mathcal{B}_{\langle 1\rangle},\mathcal{B}_{\langle 2\rangle},\ldots, \mathcal{B}_{\langle s\rangle}\) according to their bi-directional masked \(p\)-value, i.e.,

\[g(\hat{X}^{\prime}_{\mathcal{B}_{\langle 1\rangle}},\hat{Y}^{\prime}_{ \mathcal{B}_{\langle 1\rangle}})\leq g(\hat{X}^{\prime}_{\mathcal{B}_{\langle 2 \rangle}},\hat{Y}^{\prime}_{\mathcal{B}_{\langle 2\rangle}})\leq\cdots\leq g(\hat{X}^{\prime}_{ \mathcal{B}_{\langle s\rangle}},\hat{Y}^{\prime}_{\mathcal{B}_{\langle s \rangle}})\;.\]

\begin{table}
\begin{tabular}{c c c c|c c c|c c|c c c} \hline Dataset & \# Inst. & \# Feat. & Dataset & \# Inst. & \# Feat. & Dataset & \# Inst. & \# Feat. & Dataset & \# Inst. & \# Feat. \\ \hline dna & 3,186 & 180 & krypt & 27,705 & 6 & santan & 200,000 & 200 & adult & 1,000,000 & 14 \\ \hline agnos & 3,468 & 970 & diamon & 53,940 & 9 & codma & 487,867 & 8 & labor & 1,000,000 & 16 \\ \hline topo21 & 8,885 & 266 & cifar10 & 60,000 & 3072 & blob & 1,000,000 & 2 & poker & 1,025,010 & 10 \\ \hline har & 10,299 & 561 & mnist & 70,000 & 784 & sea50 & 1,000,000 & 3 & higgs & 11,000,000 & 4 \\ \hline \end{tabular}
\end{table}
Table 1: DatasetsWe then take our bi-directional hypothesis \(h(\hat{X}^{\prime}_{\bm{B}_{i}},\hat{Y}^{\prime}_{\bm{B}_{i}})\) with parameter \(\beta\) and \(\alpha=p_{*}\) as in Eqn. (7), and finally get the local regions with significant differences as

\[\left\{\mathcal{B}_{\langle i\rangle}\colon\;\;i\leq t^{*}\;\;\text{and}\;\;h( \hat{X}^{\prime}_{\bm{B}_{\langle i\rangle}},\hat{Y}^{\prime}_{\bm{B}_{\langle i \rangle}})=1\right\}\;,\]

where

\[t^{*}=\operatorname*{arg\,max}_{t\in[s]}\left\{t-\left|\{i\in[t]\colon h(\hat{ X}^{\prime}_{\bm{B}_{\langle i\rangle}},\hat{Y}^{\prime}_{\bm{B}_{\langle i \rangle}})=1\}\right|+1\leq\frac{\ln(1-\alpha_{*})}{\ln(1-p_{*})}\right\}\;.\] (11)

Here, \(p_{*}\) is selected as in Eqn. (10), and \(\alpha_{*}\in[p_{*},1)\) is a parameter to control the probability of mis-identifying at least one rectangle region without significant difference, also called _familywise error rate_[69, 70, 71, 72]. We present theoretical analysis for familywise error rate as follows:

**Theorem 5**.: _For our exploration, the familywise error rate is upper bounded by \(\alpha_{*}\), if 1) the \(p\)-values of local regions without differences are mutually independent; and 2) the \(p\)-values of local regions with differences are independent to those \(p\)-values of local regions without differences._

Our method is different from previous space partition methods of trees or clusters [24, 25, 26, 60, 27], where the splitting criterion is taken as the cardinalities of samples in each region. Duong [28] partitioned and searched local regions from the estimated density function, and Kim et al. [30] identified local regions by clustering data samples from the estimated conditional probabilities. It is not easy to make accurate estimation for density and conditional probabilities without sufficient data, particularly for multiple small regions. Other relevant studies detected local differences implicitly based on interactive rank test [73] or a learned classifier [74].

## 4 Experiments

We conduct experiments on 16 datasets1 as summarized in Table 1. Most dataset have been studied in previous two-sample test, and features have been scaled to \([0,1]\) for all datasets. All experiments are performed with Python on nodes of a computational cluster with a single CPU (Intel Core i9-10900X 3.7GHz) and a single GPU (GeForce RTX 2080 Ti), running Ubuntu with 128GB main memory.

Footnote 1: Dataset blob is downloaded from _github.com/fengliu90/DK-for-TST_,

and other datasets are downloaded from _www.openml.org_.

#### Experimental comparisons for two-sample test

We compare our ME\({}_{\text{MaBiD}}\) with the state-of-the-art approaches on two-sample test as follows:

\begin{table}
\begin{tabular}{c c c c c c c c} \hline Dataset & Our ME\({}_{\text{MaBiD}}\) & ME & MMDAgg & MMD-D & C2ST-L & C2ST-S & AutoMLST \\ \hline blob & **.985\(\pm\).009** &.823\(\pm\).000 &.935\(\pm\).012 &.963\(\pm\).010 &.972\(\pm\).078 &.946\(\pm\).037 &.980\(\pm\).029 \\ dna & **.717\(\pm\).068** &.536\(\pm\).059 &.659\(\pm\).070 &.628\(\pm\).006 &.699\(\pm\).028 &.505\(\pm\).044 &.603\(\pm\).085 \\ agnos & **.812\(\pm\).018** &.602\(\pm\).033 &.779\(\pm\).046 &.734\(\pm\).006 &.742\(\pm\).012 &.679\(\pm\).051 &.632\(\pm\).077 \\ topo21 & **.692\(\pm\).006** &.526\(\pm\).058 &.605\(\pm\).077 &.633\(\pm\).062 &.679\(\pm\).046 &.517\(\pm\).046 &.591\(\pm\).006 \\ har & **.858\(\pm\).065** &.816\(\pm\).015 &.814\(\pm\).026 &.728\(\pm\).064 &.761\(\pm\).093 &.738\(\pm\).063 &.740\(\pm\).058 \\ kropt & **.992\(\pm\).012** &.875\(\pm\).027 &.971\(\pm\).024 &.916\(\pm\).066 &.946\(\pm\).013 &.929\(\pm\).031 &.971\(\pm\).026 \\ diamond & **.837\(\pm\).066** &.697\(\pm\).068 &.676\(\pm\).047 &.755\(\pm\).056 &.747\(\pm\).086 &.727\(\pm\).076 &.831\(\pm\).062 \\ cifar & **.893\(\pm\).022** &.859\(\pm\).075 &.866\(\pm\).091 &.878\(\pm\).090 &.834\(\pm\).099 &.798\(\pm\).019 &.882\(\pm\).086 \\ mnist & **.985\(\pm\).017** &.926\(\pm\).056 &.932\(\pm\).068 &.972\(\pm\).051 &.969\(\pm\).042 &.930\(\pm\).029 &.963\(\pm\).074 \\ samtan & **1.00\(\pm\).000** &.896\(\pm\).060 & **1.00\(\pm\).000** &.887\(\pm\).021 &.911\(\pm\).084 &.850\(\pm\).021 &.954\(\pm\).012 \\ codrma & **1.00\(\pm\).000** &.946\(\pm\).085 &.926\(\pm\).037 &.914\(\pm\).076 & **1.00\(\pm\).000** & **1.00\(\pm\).000** &.876\(\pm\).067 \\ sea50 & **.993\(\pm\).018** & **.993\(\pm\).018** &.982\(\pm\).012 & **.993\(\pm\).018** & **.993\(\pm\).018** &.970\(\pm\).053 &.989\(\pm\).029 \\ adult & **.996\(\pm\).002** &.875\(\pm\).034 &.967\(\pm\).029 &.908\(\pm\).072 &.761\(\pm\).091 &.854\(\pm\).058 &.992\(\pm\).006 \\ labor &.992\(\pm\).012 &.807\(\pm\).078 &.988\(\pm\).010 &.930\(\pm\).093 &.756\(\pm\).059 &.791\(\pm\).031 & **1.00\(\pm\).000** \\ poker &.821\(\pm\).079 &.719\(\pm\).096 &.712\(\pm\).033 &.701\(\pm\).056 &.743\(\pm\).039 &.731\(\pm\).052 & **.832\(\pm\).048** \\ higgs & **.979\(\pm\).024** &.818\(\pm\).090 &.938\(\pm\).047 &.953\(\pm\).055 &.968\(\pm\).043 &.933\(\pm\).013 &.969\(\pm\).030 \\ \hline Average & **.909\(\pm\).026** &.795\(\pm\).053 &.859\(\pm\).039 &.843\(\pm\).050 &.842\(\pm\).052 &.806\(\pm\).039 &.863\(\pm\).043 \\ \hline \end{tabular}
\end{table}
Table 2: Comparisons of test powers (mean\(\pm\)std) on two-sample test. Bold denotes the highest mean in per row.

* ME: Mean Embeddings over multiple test locations and a single Gaussian kernel [9; 10];
* MMD-D: Maximum Mean Discrepancy based on a Deep kernel [39];
* MMDAgg: Maximum Mean Discrepancy with Aggregating of multiple Gaussian kernels [75];
* C2ST-S: Train a binary classification network and test its accuracy on a hold-out set [13];
* C2ST-L: Train a binary classification network with a statistic about class probabilities [14; 18];
* AutoMLTST: Train a binary classifier based on AutoML method with a statistic as C2ST-L [19].

Following [39; 76], we train on a subset of each available data, and test on 100 random subsets from the remaining dataset, and the ratio is set as \(4:1\) for training and testing. We repeat such process 10 times for each dataset. More details are given in Appendix C. For our ME\({}_{\text{MaBiD}}\), we set \(\alpha=0.05\) and take 5-fold cross validation to select \(\beta\in[1:0.2:2]\). We limit the cardinality of test locations within \(20\) for ME and ME\({}_{\text{MaBiD}}\) as in [9; 10; 11], and optimization parameters of Eqn. (5) is presented in Appendix C. We take parameter settings for other methods as in their respective inferences.

Table 2 summarizes the average of test powers and standard deviations. It is evident that our ME\({}_{\text{MaBiD}}\) takes better performance than ME and MMDAgg, because they both take Gaussian kernels with isotropic scale, and ignore the distributional differences from different directions. Our method is still better than MMD-D with a deep kernel, and a reason is that multiple Mahalanobis kernels are more flexible than a deep kernel to capture local difference from multiple neighborhoods and directions.

From Table 2, it is also observed that our ME\({}_{\text{MaBiD}}\) outperforms three classifier-based methods C2ST-S, C2ST-L and AutoML expect for datasets labor and poker, since those methods focus merely on the prediction information from outputs of classifiers, rather than local and directional information among data samples. For datasets labor and poker, AutoML generates new features automatically from the original mixture of continuous and symbolic features, and thus achieves better performance.

We further compare the average running time (in seconds) for different methods on two-sample test, as shown in Figure 3. As expected, ME takes the least running time since it considers only one Gaussian kernel, yet with the smallest average of test powers in Table 2. Our ME\({}_{\text{MaBiD}}\) method takes smaller and comparable running time in contrast to other methods since our method takes relatively smaller time on training Mahalanobis kernels without permutation test in the testing process.

#### Experiments on the exploration of local significant differences

We compare with the state-of-the-art approaches on exploring local significant difference as follows:

* FDG: Partition space by probability binning and compare cardinalities of two samples [24];
* K-PRIM: Partition space by patient rule induction and estimate kernel density differences [28];
* MRS: Partition space by polya tree and measure difference via Binomial distributions [26];
* TEAM: Partition space by data variance and measure difference via Binomial distributions [27];
* BTLDD: Estimate conditional probabilities of two samples and cluster data via difference [77];
* MMDT: Partition space into equal grids and test density difference via Welch's statistic [29].

\begin{table}
\begin{tabular}{|c c c c c c c c|} \hline Dataset & Our method & FDG & KPRIM & MRS & MMDT & BTLDD & TEAM \\ \hline blob & **.945\(\pm\).082** &.902\(\pm\).075 &.879\(\pm\).045 &.849\(\pm\).075 &.909\(\pm\).091 &.932\(\pm\).046 &.877\(\pm\).067 \\ diamon & **.974\(\pm\).054** &.852\(\pm\).010 &.895\(\pm\).089 &.876\(\pm\).066 &.867\(\pm\).104 &.951\(\pm\).070 &.947\(\pm\).022 \\ codrna & **.969\(\pm\).026** &.936\(\pm\).037 &.876\(\pm\).061 &.966\(\pm\).045 &.884\(\pm\).105 &.905\(\pm\).050 &.863\(\pm\).036 \\ sea50 &.977\(\pm\).056 &.975\(\pm\).062 &.933\(\pm\).074 &.928\(\pm\).058 & **.985\(\pm\).045** &.892\(\pm\).059 &.944\(\pm\).065 \\ adult & **.953\(\pm\).048** &.862\(\pm\).044 &.838\(\pm\).109 &.911\(\pm\).085 &.927\(\pm\).101 &.875\(\pm\).074 &.880\(\pm\).070 \\ labor & **.959\(\pm\).062** & **.894\(\pm\).080** &.905\(\pm\).016 &.900\(\pm\).037 &.911\(\pm\).093 &.922\(\pm\).048 &.932\(\pm\).067 \\ poker & **.945\(\pm\).030** &.901\(\pm\).030 &.882\(\pm\).027 &.925\(\pm\).023 &.927\(\pm\).057 &.894\(\pm\).028 &.884\(\pm\).064 \\ higgs & **.946\(\pm\).016** &.932\(\pm\).011 &.918\(\pm\).001 &.940\(\pm\).000 &.927\(\pm\).026 &.926\(\pm\).027 &.937\(\pm\).002 \\ \hline Average & **.959\(\pm\)**.047 &.907\(\pm\).044 &.891\(\pm\).053 &.912\(\pm\).049 &.917\(\pm\).078 &.912\(\pm\).050 &.908\(\pm\).049 \\ \hline \end{tabular}
\end{table}
Table 3: Comparisons of density differences (mean\(\pm\)std) on the exploration of local significant differences, and the bold denotes the highest mean in per row.

[MISSING_PAGE_FAIL:9]

Figure 5 illustrates the test power versus sample size for our bi-directional hypothesis, non-directional hypothesis and one-directional hypothesis. As can be seen, our bi-directional hypothesis achieves higher test power by considering the inference and its contrary direction and adaptive parameter selection. Figure 6 exploits the relationship between the optimal parameter \(\beta\) and the probability \(\xi=\Pr[\bm{F}^{\top}\bm{\bar{L}}^{\prime-1}(\bm{c}_{\hat{\chi}^{\prime}}-\bm{c}_ {\hat{\chi}^{\prime}})\geq 0]\) for our ME\({}_{\text{MaBiD}}\) method. We can easily find the positive relevance between \(\beta\) and \(\xi\): the larger the probability \(\xi\), the larger the optimal parameter \(\beta\).

Figure 7 indicates that the type-I error is limited about \(\alpha=0.05\) for different \(\beta\) in our experiments, as shown in Theorem 4, and thus our method could effectively control the rate of falsely reject the null hypothesis, which empirically verify the trustworthiness of our ME\({}_{\text{MaBiD}}\) test. Figure 8 empirically shows the familywise error rate is limited about \(\alpha_{*}=0.05\) for different number of local regions \(s\); therefore, our exploring method could control the rate of incorrectly exploiting the local regions with significant difference, and this is nicely in accordance with Theorem 5.

## 5 Conclusion

This work takes one more step on the exploration of local significant differences. We propose the ME\({}_{\text{MaBiD}}\) test by exploiting local information from multiple Mahalanobis kernels and introducing bi-directional hypothesis for testing. We partition embedding space via a new splitting criterion, and then identify local significant differences based on our bi-directional masked \(p\)-value and ME\({}_{\text{MaBiD}}\) test. We verify the effectiveness of our proposed methods both theoretically and empirically. An interesting work is to explore other local and directional information for local significant differences.

## Acknowledgments and Disclosure of Funding

The authors want to thank the reviewers for their helpful comments and suggestions. This research was supported by National Key R&D Program of China (2021ZD0112802), NSFC (61921006, 62376119) and CAAI-Huawei MindSpore Open Fund. W. Gao is corresponding author of this paper.

Figure 5: The comparisons of test power vs sample size for our bi-directional hypothesis and previous one/non-directional hypothesis.

Figure 8: The FWER is limited about \(\alpha_{*}=0.05\) w.r.t. different \(s\) for exploring local significant difference.

Figure 6: The type-I error is limited about \(\alpha=0.05\) w.r.t different \(\beta\) for our ME\({}_{\text{MaBiD}}\).

Figure 7: The type-I error is limited about \(\alpha=0.05\) w.r.t different \(\beta\) for our ME\({}_{\text{MaBiD}}\).

## References

* [1] S. Shell, S.-M. Park, A. R. Radjabi, R. Schickel, E. O. Kistner, D. A. Jewell, C. Feig, E. Lengyel, and M. E. Peter. Let-7 expression defines two differentiation stages of cancer. _National Academy of Sciences_, 104(27):11400-11405, 2007.
* [2] G. Boracchi, D. Carrera, C. Cervellera, and D. Maccio. Quanttree: Histograms for change detection in multivariate data streams. In _Proceedings of the 35th International Conference on Machine Learning_, pages 638-647, Stockholm, Sweden, 2018.
* [3] C.-L. Li, W.-C Chang, Y. Cheng, Y.-M. Yang, and B. Poczos. MMD GAN: Towards deeper understanding of moment matching network. In _Advances in Neural Information Processing Systems 30_, pages 2203-2213. Curran Associates, Dutches, NY, 2017.
* [4] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A.J. Smola, and A. Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In _Proceedings of the 5th International Conference on Learning Representations_, Toulon, France, 2017.
* [5] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. _Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [6] W. Zaremba, A. Gretton, and M. B. Blaschko. B-test: A non-parametric, low variance kernel two-sample test. In _Advances in Neural Information Processing Systems 26_, pages 755-763. Curran Associates, Dutches, NY, 2013.
* [7] G. Wynne and A. B. Duncan. A kernel two-sample test for functional data. _Journal of Machine Learning Research_, 23(73):1-51, 2022.
* [8] S. Shekhar, I. Kim, and A. Ramdas. A permutation-free kernel two-sample test. In _Advances in Neural Information Processing Systems 35_, pages 18168-18180. Curran Associates, Dutches, NY, 2022.
* [9] K. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast two-sample testing with analytic representations of probability measures. In _Advances in Neural Information Processing Systems 28_, pages 1981-1989. Curran Associates, Dutches, NY, 2015.
* [10] W. Jitkrittum, Z. Szabo, K. P. Chwialkowski, and A. Gretton. Interpretable distribution features with maximum testing power. In _Advances in Neural Information Processing Systems 29_, pages 181-189. Curran Associates, Dutches, NY, 2016.
* [11] M. Scetbon and G. Varoquaux. Comparing distributions: \(\ell_{1}\) geometry improves kernel two-sample testing. In _Advances in Neural Information Processing Systems 32_, pages 12306-12316. Curran Associates, Dutches, NY, 2019.
* [12] P. Golland and B. Fischl. Permutation tests for classification: towards statistical significance in image-based studies. In _Proceedings of the 18th International Conference on Information Processing in Medical Imaging_, pages 330-341, Cambria, England, 2003.
* [13] D. Lopez-Paz and M. Oquab. Revisiting classifier two-sample tests. In _Proceedings of the 5th International Conference on Learning Representations_, Toulon, France, 2017.
* [14] X.-Y. Cheng and A. Cloninger. Classification logit two-sample testing by neural networks. _CoRR/abstract_, 1909.11298, 2019.
* [15] H. Cai, B. Goggin, and Q. Jiang. Two-sample test based on classification probability. _Statistical Analysis and Data Mining: The ASA Data Science Journal_, 13(1):5-13, 2020.
* [16] I. Kim, A. Ramdas, A. Singh, and L. Wasserman. Classification accuracy as a proxy for two-sample testing. _Annals of Statistics_, 49(1):411-434, 2021.
* [17] S. Jang, S. Park, I. Lee, and O. Bastani. Sequential covariate shift detection using classifier two-sample tests. In _Proceedings of the 39th International Conference on Machine Learning_, pages 9845-9880, Baltimore, MD, 2022.

* [18] X. Cheng and A. Cloninger. Classification logit two-sample testing by neural networks for differentiating near manifold densities. _IEEE Transactions on Information Theory_, 68(10):6631-6662, 2022.
* [19] J.-M. Kubler, V. Stimper, S. Buchholz, K. Muandet, and B. Scholkopf. AutoML Two-Sample Test. In _Advances in Neural Information Processing Systems 35_, pages 15929-15941. Curran Associates, Dutches, NY, 2022.
* [20] S. Hediger, Loris L. Michel, and J. Naf. On the use of random forest for two-sample testing. _Computational Statistics & Data Analysis_, 170:107435-107468, 2022.
* [21] K. Muandet, K. Fukumizu, B. Sriperumbudur, and B. Scholkopf. Kernel mean embedding of distributions: A review and beyond. _Foundations and Trends(r) in Machine Learning_, 10(1-2):1-141, 2017.
* [22] P.E. Freeman, I. Kim, and A.B. Lee. Local two-sample testing: A new tool for analysing high-dimensional astronomical data. _Monthly Notices of the Royal Astronomical Society_, 471 (3):3273-3282, 2017.
* [23] A. T. L. Lun, A. C. Richard, and J. C. Marioni. Testing for differential abundance in mass cytometry data. _Nature Methods_, 14(7):707-709, 2017.
* [24] M. Roederer and R. R. Hardy. Frequency difference gating: A multivariate method for identifying subsets that differ between samples. _Journal of the International Society for Analytical Cytology_, 45(1):56-64, 2001.
* [25] T. Duong, I. Koch, and M. P. Wand. Highest density difference region estimation with application to flow cytometric data. _Journal of Mathematical Methods in Biosciences_, 51(3):504-521, 2009.
* [26] J. Soriano and L. Ma. Probabilistic multi-resolution scanning for two-sample differences. _Journal of the Royal Statistical Society: Series B_, 79(2):547-572, 2017.
* [27] J. A. Pura, X.-C. Li, C. Chan, and J.-C. Xie. Team: A multiple testing algorithm on the aggregation tree for flow cytometry analysis. _Annals of Applied Statistics_, 17(1):621-640, 2023.
* [28] T. Duong. Local significant differences from nonparametric two-sample tests. _Journal of Nonparametric Statistics_, 25(3):635-645, 2013.
* [29] J. D. Dworkin, K. A. Linn, A. J. Solomon, T. D. Satterthwaite, A. Raznahan, R. Bakshi, and R. T. Shinohara. A local group differences test for subject-level multivariate density neuroimaging outcomes. _Biostatistics_, 22(3):646-661, 2021.
* [30] I. Kim, Ann. B. Lee, and J. Lei. Global and local two-sample tests via regression. _Electronic Journal of Statistics_, 13(2):5253-5305, 2019.
* [31] S. Sonnenburg, G. Ratsch, and C. Schafer. A general and efficient multiple kernel learning algorithm. In _Advances in Neural Information Processing Systems 18_, pages 1273-1280. MIT Press, Cambridge, MA, 2005.
* [32] M. Gonen and E. Alpaydin. Multiple kernel learning algorithms. _Journal of Machine Learning Research_, 12:2211-2268, 2011.
* [33] R. De Maesschalck, D. Jouan-Rimbaud, and D..L. Massart. The mahalanobis distance. _Chemometrics and Intelligent Laboratory Systems_, 50(1):1-18, 2000.
* [34] Q. K. Weinberger and K. L. Saul. Distance metric learning for large margin nearest neighbor classification. _Journal of Machine Learning Research_, 10(9):207-244, 2009.
* [35] J. Wang, A. Kalousis, and A. Woznica. Parametric local metric learning for nearest neighbor classification. In _Advances in Neural Information Processing Systems 25_, page 1610-1618. Curran Associates, Dutches, NY, 2012.
* [36] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning with application to clustering with side-information. In _Advances in Neural Information Processing Systems 15_, pages 505-512. MIT Press, Cambridge, MA, 2002.

* [37] S. Xiang, F. Nie, and C. Zhang. Learning a mahalanobis distance metric for data clustering and classification. _Pattern Recognition_, 41(12):3600-3612, 2008.
* [38] A. Gretton, B. K. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Balakrishnan, and K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In _Advances in Neural Information Processing Systems 25_, pages 1214-1222. Curran Associates, Dutches, NY, 2012.
* [39] F. Liu, W.-K. Xu, J. Lu, G.-Q. Zhang, A. Gretton, and D. J. Sutherland. Learning deep kernels for non-parametric two-sample tests. In _Proceedings of the 37th International Conference on Machine Learning_, pages 6316-6326, Virtual, 2020.
* [40] B. S. Everitt. A monte carlo investigation of the robustness of hotelling's one-and two-sample t 2 tests. _Journal of the American Statistical Association_, 74(365):48-51, 1979.
* [41] H. Hotelling. The generalization of student's ratio. _Annals of Mathematical Statistics_, 2(3):360-378, 1931.
* [42] N. Deb, B. B. Bhattacharya, and B. Sen. Efficiency lower bounds for distribution-free hotelling-type two-sample tests based on optimal transport. _CoRR/abstract_, 2104.01986, 2021.
* [43] S. Boyd, S. P. Boyd, and L. Vandenberghe. _Convex Optimization_. Cambridge University Press, 2004.
* [44] E. Deadman, N. J. Higham, and R. Ralha. Blocked schur algorithms for computing the matrix square root. In _Proceedings of the 11th International Workshop on Applied Parallel Computing_, pages 171-182, Helsinki, Finland, 2012.
* [45] L. Wasserman. _All of Statistics: A Concise Course in Statistical Inference_. Springer, 2004.
* [46] J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypotheses. _Philosophical Transactions of the Royal Society of London: Series A_, 231:289-337, 1933.
* [47] J. Cornfield. Sequential trials, sequential analysis and the likelihood principle. _The American Statistician_, 20(2):18-23, 1966.
* [48] C. R. Genovese, K. Roeder, and L. Wasserman. False discovery control with p-value weighting. _Biometrika_, 93(3):509-524, 2006.
* [49] F. Zhang and J. Gou. Refined critical boundary with enhanced statistical power for non-directional two-sided tests in group sequential designs with multiple endpoints. _Statistical Papers_, 62(3):1265-1290, 2021.
* [50] H. F. Kaiser. Directional statistical decisions. _Psychological Review_, 67(3):160-167, 1960.
* [51] A. Cohen and H. B. Sackrowitz. Directional tests for one-sided alternatives in multivariate models. _Annals of Statistics_, 26(6):2321-2338, 1998.
* [52] O. Ibe. _Fundamentals of Applied Probability and Random Processes_. Academic Press, 2014.
* [53] L. Anselin. Lagrange multiplier test diagnostics for spatial dependence and spatial heterogeneity. _Geographical analysis_, 20(1):1-17, 1988.
* [54] D. Follmann. A simple multivariate test for one-sided alternatives. _Journal of the American Statistical Association_, 91(434):854-861, 1996.
* [55] M. J. McIntosh. Calculating sample size for follmann's simple multivariate test for one-sided alternatives. _American Statistician_, 76(1):16-21, 2022.
* [56] J. M. Lachin. Applications of the wei-lachin multivariate one-sided test for multiple outcomes on possibly different scales. _PloS One_, 9(10):e108784, 2014.
* [57] F. A. Graybill. _Theory and Application of the Linear Model_. Duxbury Press, 1976.
* [58] L. Ma and W.-H. Wong. Coupling optional polya trees and the two sample problem. _Journal of the American Statistical Association_, 106(496):1553-1565, 2011.

* [59] G. Boracchi, C. Cervellera, and D. Maccio. Uniform histograms for change detection in multivariate data. In _Proceedings of the 30th International Joint Conference on Neural Networks_, pages 1732-1739, Brisbane, Australia, 2017.
* [60] A.-J. Liu, J. Lu, and G.-Q. Zhang. Concept drift detection via equal intensity k-means space partitioning. _IEEE Transactions on Cybernetics_, 51(6):3198-3211, 2021.
* [61] G. V. Kass. An exploratory technique for investigating large quantities of categorical data. _Journal of the Royal Statistical Society: Series C_, 29(2):119-127, 1980.
* [62] J. R. Quinlan. Induction of decision trees. _Machine Learning_, 1(1):81-106, 1986.
* [63] J. H. Friedman. A recursive partitioning decision rule for nonparametric classification. _IEEE Transactions on Computers_, 26(4):404-408, 1977.
* [64] J. R. Quinlan. Simplifying decision trees. _International Journal of Man-machine Studies_, 27(3):221-234, 1987.
* [65] S. B. Gelfand, CS Ravishankar, and E. J. Delp. An iterative growing and pruning algorithm for classification tree design. In _Proceedings of the IEEE International Conference on Systems, Man and Cybernetics_, pages 818-823, Cambridge, MA, 1989.
* [66] L. Rokach and O. Maimon. Top-down induction of decision trees classifiers-a survey. _IEEE Transactions on Systems, Man, and Cybernetics, Part C_, 35(4):476-487, 2005.
* [67] B. Duan, A. Ramdas, and L. A. Wasserman. Familywise error rate control by interactive unmasking. In _Proceedings of the 37th International Conference on Machine Learning_, pages 2720-2729, Virtual, 2020.
* [68] G. S. Mudholkar and D. K Srivastava. A class of robust stepwise alternatives to hotelling's t 2 tests. _Journal of Applied Statistics_, 27(5):599-619, 2000.
* [69] Y. Hochberg and A. C. Tamhane. _Multiple Comparison Procedures_. John Wiley & Sons, Inc., 1987.
* [70] S. Holm. A simple sequentially rejective multiple test procedure. _Scandinavian Journal of Statistics_, 6(2):65-70, 1979.
* [71] F. Bretz, W. Maurer, W. Brannath, and M. Posch. A graphical approach to sequentially rejective multiple test procedures. _Statistics in Medicine_, 28(4):586-604, 2009.
* [72] A. C. Tamhane and J. Gou. Advances in p-value based multiple test procedures. _Journal of Biopharmaceutical Statistics_, 28(1):10-27, 2018.
* [73] B. Duan, A. Ramdas, and L. Wasserman. Interactive rank testing by betting. In _Proceedings of the 1st Conference on Causal Learning and Reasoning_, pages 201-235, Eureka, Canada, 2022.
* [74] W. Li, G. Dasarathy, K. N. Ramamurthy, and V. Berisha. A label efficient two-sample test. In _Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence_, pages 1168-1177, Eindhoven, Netherlands, 2022.
* [75] A. Schrab, I. Kim, B. Guedj, and A. Gretton. Efficient aggregated kernel tests using incomplete \(u\)-statistics. In _Advances in Neural Information Processing Systems 35_, pages 18793-18807. Curran Associates, Dutches, NY, 2022.
* [76] F. Liu, W. Xu, J. Lu, and D. J. Sutherland. Meta two-sample testing: Learning kernels for testing with limited data. In _Advances in Neural Information Processing Systems 34_, pages 5848-5860. Curran Associates, Dutches, NY, 2021.
* [77] F. Cazais and A. Lheritier. Beyond two-sample-tests: Localizing data discrepancies in high-dimensional spaces. In _Proceedings of the IEEE International Conference on Data Science and Advanced Analytics_, pages 1-10, Paris, France, 2015.

* [78] M. Sugiyama, T. Kanamori, T. Suzuki, M. Plessis, S. Liu, and I. Takeuchi. Density-difference estimation. In _Advances in Neural Information Processing Systems 25_, pages 692-700. Curran Associates, Dutches, NY, 2012.
* [79] Y. P. Mack and M. Rosenblatt. Multivariate k-nearest neighbor density estimates. _Journal of Multivariate Analysis_, 9(1):1-15, 1979.
* [80] S. Dasgupta and S. Kpotufe. Optimal rates for k-nn density and mode estimation. In _Advances in Neural Information Processing Systems 27_, pages 2555-2563. Curran Associates, Dutches, NY, 2014.
* [81] W. Jitkrittum, Z. Szabo, and A. Gretton. An adaptive test of independence with analytic kernel embeddings. In _Proceedings of the 34th International Conference on Machine Learning_, pages 1742-1751, Sydney, Australia, 2017.
* [82] R. A. Johnson and D. W. Wichermand. _Applied Multivariate Statistical Analysis_. Prentice Hall, 2002.
* [83] Papoulis A and U. Pillai. _Probability, Random Variables and Stochastic Processes_. McGraw Hill, 2001.
* [84] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate analysis. _Probability and Mathematical Statistics_, 1979.
* [85] L. Lei and W. Fithian. Adapt: An interactive procedure for multiple testing with side information. _Journal of the Royal Statistical Society: Series B_, 80(4):649-679, 2018.
* [86] S. Rabanser, S. Gunnemann, and Z. Lipton. Failing loudly: An empirical study of methods for detecting dataset shift. In _Advances in Neural Information Processing Systems 32_, pages 1396-1408. Curran Associates, Dutches, NY, 2019.
* [87] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In _Proceedings of the 4th International Conference on Learning Representations_, San Juan, PR, 2016.
* [88] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _Proceedings of the 3rd International Conference on Learning Representations_, San Diego, CA, 2015.
* [89] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Dutches, NY, 2019.

Detailed Proofs for Our Theoretical Results

### Proof of Lemma 1

We begin with some useful definitions and lemmas as follows:

**Definition 6**.: (Random Metric [9]). We say that \(\rho\) is a random metric with values in \(\mathbb{R}\), indexed with pairs from the set of probability measures \(\mathcal{M}\), i.e., \(\rho=\{\rho(\mathbb{P},\mathbb{Q})\colon\mathbb{P},\mathbb{Q}\in\mathcal{M}\}\), if it satisfies the conditions for a metric with qualification 'almost surely'. Formally, for every \(\mathbb{P},\mathbb{Q},\mathbb{U}\in\mathcal{M}\), random variables \(\rho(\mathbb{P},\mathbb{Q})\), \(\rho(\mathbb{P},\mathbb{U})\) and \(\rho(\mathbb{Q},\mathbb{U})\) satisfy

1. \(\rho(\mathbb{P},\mathbb{Q})\geq 0\) a.s.
2. if \(\mathbb{P}=\mathbb{Q}\), then \(\rho(\mathbb{P},\mathbb{Q})=0\) a.s., if \(\mathbb{P}\neq\mathbb{Q}\) then \(\rho(\mathbb{P},\mathbb{Q})\neq 0\) a.s.
3. \(\rho(\mathbb{P},\mathbb{Q})=\rho(\mathbb{Q},\mathbb{P})\) a.s.
4. \(\rho(\mathbb{P},\mathbb{Q})\leq\rho(\mathbb{P},\mathbb{U})+\rho(\mathbb{U}, \mathbb{Q})\) a.s.

**Lemma 7**.: _[_11_]_ _Let \(\mu\) be a absolutely continuous Lebesgue measure on \(\mathbb{R}^{d}\). A non-zero analytic function \(f\) can be zero at most in the set of measure \(0\) w.r.t. \(\mu\)._

**Lemma 8**.: _[_9_]_ _If \(\kappa\) is a bounded and analytic kernel on \(\mathbb{R}^{d}\times\mathbb{R}^{d}\), then it is analytic for every function in the RKHS associated with this kernel._

**Definition 9**.: For test locations \(\mathcal{V}=\{\bm{v}_{1},\bm{v}_{2},\cdots,\bm{v}_{\ell}\}\) and Mahalanobis kernels \(\kappa_{1},\kappa_{2},\cdots,\kappa_{\ell}\) given by Eqn. (1), we define

\[\rho_{\mu_{1},\mu_{2},\cdots,\mu_{\ell}}(\mathbb{P},\mathbb{Q})=\sum_{j=1}^{ \ell}\left(\left[\mu_{j}\mathbb{P}\right](\bm{v}_{j})-\left[\mu_{j}\mathbb{Q} \right](\bm{v}_{j})\right)^{2},\] (12)

where \(\left[\mu_{j}\mathbb{P}\right](\bm{v}_{j})=E_{\bm{x}\sim\mathbb{P}}[\kappa_{j }(\bm{x},\bm{v}_{j})]\) and \(\left[\mu_{j}\mathbb{Q}\right](\bm{v}_{j})=E_{\bm{y}\sim\mathbb{Q}}[\kappa_{j }(\bm{y},\bm{v}_{j})]\).

We show that the above distance metric for two probability measures is a random metric as follows:

**Lemma 10**.: _If \(\bm{v}_{1},\bm{v}_{2},\cdots,\bm{v}_{\ell}\) are drawn i.i.d. from a absolutely continuous distribution \(\mathcal{G}\), then \(\rho_{\mu_{1},\mu_{2},\cdots,\mu_{\ell}}(\cdot,\cdot)\) is a random metric for bounded kernels \(\{\kappa_{1},\kappa_{2},\cdots,\kappa_{\ell}\}\)._

Proof.: For \(j\in[\ell]\), we first introduce a function

\[\rho_{\mu_{j}}(\mathbb{P},\mathbb{Q})=\left(\left[\mu_{j}\mathbb{P}\right]( \bm{v}_{j})-\left[\mu_{j}\mathbb{Q}\right](\bm{v}_{j})\right)^{2}\,\]

and it is sufficient to prove that \(\rho_{\mu_{j}}(\mathbb{P},\mathbb{Q})\) is a random metric for each \(j\in[\ell]\) from Eqn. (12).

It is well-known that Mahalanobis kernels in Eqn. (1) are characteristic and analytic from [81], and the corresponding mapping

\[\mu_{j}:\ \mathbb{P}\to\mu_{j}\mathbb{P}\ \ \text{and}\ \ \mu_{j}:\ \mathbb{Q}\to\mu_{j}\mathbb{Q}\]

are injective for \(\kappa_{j}\), where \(\mu_{j}\mathbb{P}\) and \(\mu_{j}\mathbb{Q}\) denote the images of measures \(\mathbb{P}\) and \(\mathbb{Q}\), respectively. Hence, the image of \(\mu_{j}\) is a subset of analytic functions for analytic and bounded \(\kappa_{j}\), according to Lemma 8.

If \(\mathbb{P}=\mathbb{Q}\), then we have

\[f_{j}=\mu_{j}\mathbb{P}-\mu_{j}\mathbb{Q}=0\ \ \ \ \text{and}\ \ \ \ \rho_{\mu_{j}}(\mathbb{P},\mathbb{Q})=(\left[\mu_{j}\mathbb{P}\right](\bm{v}_{j })-\left[\mu_{j}\mathbb{Q}\right](\bm{v}_{j}))^{2}=0\.\]

We now prove that if \(\mathbb{P}\neq\mathbb{Q}\) then \(f_{j}\neq 0\) almost surely, by applying Lemma 7 to analytic function \(f_{j}=\mu_{j}\mathbb{P}-\mu_{j}\mathbb{Q}\) with distribution \(\mathcal{G}\). For injective map \(\mu_{j}\), there exists at least one point \(a\) such that \(f_{j}(a)\neq 0\), and there exists a ball around \(a\) with non-zero \(f_{j}\) from the continuity of \(f_{j}\). Hence, \(f\) is almost everywhere nonzero based on Lemma 7, and this follows that

\[\rho_{\mu_{j}}(\mathbb{P},\mathbb{Q})=(\left[\mu_{j}\mathbb{P}\right](\bm{v}_{j })-\left[\mu_{j}\mathbb{Q}\right](\bm{v}_{j}))^{2}>0\ \ \ \ \text{a.s.}\ \ \ \text{for}\ \ \ \ \mathbb{P}\neq\mathbb{Q}\.\]

Hence, \(\rho_{\mu_{j}}\) is random metric from Definition 6 from the symmetry and triangle inequality of \(\rho_{\mu_{j}}\). 

Proof of Lemma 1.: We first have

\[\bm{\mu}_{\hat{\mathbb{P}}} = E_{\hat{\bm{x}}\sim\mathbb{P}}[\hat{\bm{x}}]=\left(\left[\mu_{1 }\mathbb{P}\right](\bm{v}_{1}),\left[\mu_{2}\mathbb{P}\right](\bm{v}_{2}), \cdots,\left[\mu_{\ell}\mathbb{P}\right](\bm{v}_{\ell})\right)\,\] \[\bm{\mu}_{\hat{\mathbb{Q}}} = E_{\hat{\bm{y}}\sim\mathbb{Q}}[\hat{\bm{y}}]=\left(\left[\mu_{1 }\mathbb{Q}\right](\bm{v}_{1}),\left[\mu_{2}\mathbb{Q}\right](\bm{v}_{2}), \cdots,\left[\mu_{\ell}\mathbb{Q}\right](\bm{v}_{\ell})\right)\,\]and further rewrite Eqn. (12) as

\[\rho_{\mu_{1},\mu_{2},\cdots,\mu_{\ell}}(\mathbb{P},\mathbb{Q})=\sum_{j=1}^{\ell} \left(\left[\mu_{j}\mathbb{P}\right]\left(\bm{v}_{j}\right)-\left[\mu_{j} \mathbb{Q}\right]\left(\bm{v}_{j}\right)\right)^{2}=\sum_{j=1}^{\ell}\left( \bm{\mu}_{\hat{\mathbb{P}},j}-\bm{\mu}_{\hat{\mathbb{Q}},j}\right)^{2}=\left\| \bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}\right\|_{2}^{2}\,.\]

From Lemma 10, we can see that \(\rho_{\mu_{1},\mu_{2},\cdots,\mu_{\ell}}(\mathbb{P},\mathbb{Q})=0\) if and only if \(\mathbb{P}=\mathbb{Q}\), and this implies that \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\) if and only if \(\mathbb{P}=\mathbb{Q}\). This completes the proof of Lemma 1. 

### Proof of Theorem 2

We begin with a useful lemma from [11] as follows.

**Lemma 11**.: _For symmetric and positive definite matrix \(\Sigma\), function \(h(\Sigma)=\Sigma^{-1/2}\) is continuous and well-defined on the positive definite space._

Based on this lemma, we present the detailed proof of Theorem 2 as follows:

**Proof of Theorem 2**. For embedding testing sample \(\hat{X}^{\prime}=\{\hat{\bm{x}}^{\prime}_{1},\cdots,\hat{\bm{x}}^{\prime}_{m^ {\prime}}\}\) and \(\hat{Y}^{\prime}=\{\hat{\bm{y}}^{\prime}_{1},\cdots,\hat{\bm{y}}^{\prime}_{n^ {\prime}}\}\), recall the pooled covariance matrix in Eqn. (3) as

\[\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}=\frac{(m^{\prime}-1)\Sigma_{\hat{X }}+(n^{\prime}-1)\Sigma_{\hat{Y}}}{m^{\prime}+n^{\prime}-2}+\epsilon\bm{I}_{d }\,,\]

and we also define

\[\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}=E_{\hat{X}^{\prime}\sim\hat{\mathbb{ P}}^{m^{\prime}},\hat{Y}^{\prime}\sim\hat{\mathbb{Q}}^{n^{\prime}}}\left[\Sigma_{ \hat{X}^{\prime},\hat{Y}^{\prime}}\right]\,.\]

We first prove that testing statistic \(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\) is almost surely asymptotically distributed as \(\chi^{2}_{\ell}\) with \(\ell\) degrees of freedom under the condition \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\) for small \(\epsilon\).

For i.i.d. samples \(\hat{X}^{\prime}\) and \(\hat{Y}^{\prime}\), \(\bm{c}_{\hat{\bm{x}}}\) is independent to \(\bm{c}_{\hat{\bm{y}}}\). If \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\), then \(\bar{\bm{z}}=\bm{c}_{\hat{X}}-\bm{c}_{\hat{Y}}\) follows a multivariate normal distribution with mean \(\bm{\mu}=\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}=\bm{0}\) and covariance matrix \((m^{\prime}+n^{\prime})\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}/m^{\prime}n^ {\prime}\), from [41, 82] and the Slutsky's theorem [83], that is

\[\bar{\bm{z}}\overset{d}{\rightarrow}\mathcal{N}(\bm{0},(m^{\prime}+n^{\prime}) \Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}/m^{\prime}n^{\prime})\,\]

where \(\overset{d}{\rightarrow}\) denotes convergence in distribution.

The matrix \(\bm{L}\) is symmetric and invertible in the Schur decomposition \(\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}=\bm{L}\bm{L}\). Our statistic can be formalized as

\[\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime}) = m^{\prime}n^{\prime}\,\bar{\bm{z}}^{\top}\Sigma_{\hat{X},\hat{Y }}^{-1}\bar{\bm{z}}/(m^{\prime}+n^{\prime})\] \[= m^{\prime}n^{\prime}\,\bar{\bm{z}}^{\top}\bm{L}^{\prime-1}\bm{L}^ {\prime-1}\bar{\bm{z}}/(m^{\prime}+n^{\prime})\] \[= \left(\sqrt{m^{\prime}n^{\prime}}\left\|\bm{L}^{\prime-1}\bar{\bm{ z}}\right\|_{2}/\sqrt{m^{\prime}+n^{\prime}}\right)^{2}\,\]

and by applying the Slutsky's theorem, we have

\[\sqrt{m^{\prime}n^{\prime}}\bm{L}^{\prime-1}\bar{\bm{z}}/\sqrt{m^{\prime}+n^{ \prime}}\overset{d}{\longrightarrow}\mathcal{N}(\bm{0},\bm{I}_{\ell})\.\]

This follows that

\[\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})=\frac{m^{\prime}n^{\prime}}{m^{ \prime}+n^{\prime}}\sum_{j=1}^{\ell}(\bm{L}^{\prime-1}\bar{\bm{z}})_{j}^{2}\,\]

where \((\bm{L}^{\prime-1}\bar{\bm{z}})_{j}\) is the \(j\)-th dimension value of \(\bm{L}^{\prime-1}\bar{\bm{z}}\), and \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}(\bm{L}^{\prime-1}\bar{\bm{z }})_{j}\) follows the standard normal distribution. This proves the \(\chi^{2}_{\ell}\) distribution for \(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\) from the sum of \(\ell\) squares of standard (i.i.d.) normal random variables.

We then prove that \(\chi^{2}_{\ell}(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime}))\to 0\) as \(m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\rightarrow\infty\) if \(\bm{\mu}_{\hat{\mathbb{P}}}\neq\bm{\mu}_{\hat{\mathbb{Q}}}\), and hence our test rejects \(H_{0}\) almost surely.

From Lemma 11, we have

\[\lim_{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\to\infty}\boldsymbol{L}^{\prime -1}=\lim_{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\to\infty}\Sigma_{\hat{X}^ {\prime},\hat{Y}^{\prime}}^{-1/2}=\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{- 1/2}=\boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\;.\]

In a similar manner, \(\bar{\boldsymbol{z}}=\boldsymbol{c}_{\hat{X}}-\boldsymbol{c}_{\hat{Y}}\) converges to \(\boldsymbol{\mu}_{\hat{\mathbb{P}}}-\boldsymbol{\mu}_{\hat{\mathbb{Q}}}\) in probability. For \(\boldsymbol{\mu}_{\hat{\mathbb{P}}}\neq\boldsymbol{\mu}_{\hat{\mathbb{Q}}}\), we have

\[\left\|\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-\frac{1}{2}}(\boldsymbol{ \mu}_{\hat{\mathbb{P}}}-\boldsymbol{\mu}_{\hat{\mathbb{Q}}})\right\|_{2}^{2}>0\;.\]

Then, \(\|\boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}\|_{2}^{2}\) is a continuous function with entries \(\bar{\boldsymbol{z}}\) and \(\boldsymbol{L}^{\prime-1}\), and it is convergent to some positive constant. We have \((m^{\prime}+n^{\prime})/(m^{\prime}n^{\prime})\chi_{\ell,\alpha}^{2}\to 0\), and

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\left\|\boldsymbol {L}^{\prime-1}\bar{\boldsymbol{z}}\right\|_{2}^{2}>\chi_{\ell,\alpha}^{2} \right]=P\left(\left\|\boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}\right\|_{ 2}^{2}>\frac{m^{\prime}+n^{\prime}}{m^{\prime}n^{\prime}}\chi_{\ell,\alpha}^{ 2}\right)\to 1\;.\]

This follows that

\[\chi_{\ell}^{2}\left(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\right)= \chi_{\ell}^{2}\left(\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\left\| \boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}\right\|_{2}^{2}\right)\to 0\;,\]

and hence \(\chi_{\ell}^{2}(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime}))\to 0\) as \(m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\to\infty\) if \(\boldsymbol{\mu}_{\hat{\mathbb{P}}}\neq\boldsymbol{\mu}_{\hat{\mathbb{Q}}}\). 

### Proof of Lemma 3

Recall the pooled covariance matrix \(\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}\) and \(\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}=E_{\hat{X}^{\prime}\sim\mathbb{P}^ {m^{\prime}},\hat{Y}^{\prime}\sim\hat{\mathbb{Q}}^{n^{\prime}}}[\Sigma_{\hat{X }^{\prime},\hat{Y}^{\prime}}]\) in the proof of Theorem 2, and \(\bar{\boldsymbol{z}}=(\boldsymbol{c}_{\hat{X}^{\prime}}-\boldsymbol{c}_{\hat {Y}^{\prime}})\) follows a multivariate normal distribution with mean \(\boldsymbol{\mu}_{\hat{\mathbb{P}}}-\boldsymbol{\mu}_{\hat{\mathbb{Q}}}\) and covariance matrix \((m^{\prime}+n^{\prime})\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}/m^{\prime}n^ {\prime}\). We first observe

\[\boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}=E_{\hat{X}^{\prime}\sim \hat{\mathbb{P}}^{m^{\prime}},\hat{Y}^{\prime}\sim\hat{\mathbb{Q}}^{n^{\prime }}}[\boldsymbol{L}^{\prime}]=\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{1/2}\,,\]

and this follows that

\[\boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}\sim\mathcal{N}\left(\boldsymbol {L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\boldsymbol{\mu}_{\hat{\mathbb{ P}}}-\boldsymbol{\mu}_{\hat{\mathbb{Q}}}),\frac{m^{\prime}+n^{\prime}}{m^{ \prime}n^{\prime}}\boldsymbol{I}_{\ell}\right)\;.\]

This is because \(E[\boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}]=\boldsymbol{L}_{\hat{\mathbb{P }},\hat{\mathbb{Q}}}^{-1}(\boldsymbol{\mu}_{\hat{\mathbb{P}}}-\boldsymbol{\mu }_{\hat{\mathbb{Q}}})\) and the covariance matrix is given by

\[\text{Cov}(\boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}, \boldsymbol{L}^{\prime-1}\bar{\boldsymbol{z}}) = \boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\left(\frac{ m^{\prime}+n^{\prime}}{m^{\prime}n^{\prime}}\Sigma_{\hat{\mathbb{P}},\hat{ \mathbb{Q}}}\right)\boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\] \[= \boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\left(\frac{ m^{\prime}+n^{\prime}}{m^{\prime}n^{\prime}}\boldsymbol{L}_{\hat{\mathbb{P}},\hat{ \mathbb{Q}}}\boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}\right)\boldsymbol {L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}=\frac{m^{\prime}+n^{\prime}}{m^{ \prime}n^{\prime}}\boldsymbol{I}_{\ell}\;.\]

Write \(\boldsymbol{B}=\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\boldsymbol {L}^{\prime-1}\text{diag}(\boldsymbol{F})\bar{\boldsymbol{z}}\), and we have

\[\boldsymbol{B}^{\top}\boldsymbol{B} = \left(\sqrt{\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}} \boldsymbol{L}^{\prime-1}\text{diag}(\boldsymbol{F})\bar{\boldsymbol{z}} \right)^{\top}\left(\sqrt{\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}} \boldsymbol{L}^{\prime-1}\text{diag}(\boldsymbol{F})\bar{\boldsymbol{z}}\right)\] \[= \frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{\boldsymbol {z}}^{\top}\text{diag}(\boldsymbol{F})(\boldsymbol{L}^{\prime-1})^{\top} \boldsymbol{L}^{\prime-1}\text{diag}(\boldsymbol{F})\bar{\boldsymbol{z}}=\frac{m^{ \prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{\boldsymbol{z}}^{\top}\text{diag}( \boldsymbol{F})\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\text{diag}( \boldsymbol{F})\bar{\boldsymbol{z}}\] \[= \frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{\boldsymbol {z}}^{\top}\text{diag}(\boldsymbol{F})\text{diag}(\boldsymbol{F})\Sigma_{\hat{X} ^{\prime},\hat{Y}^{\prime}}^{-1}\bar{\boldsymbol{z}}=\frac{m^{\prime}n^{ \prime}}{m^{\prime}+n^{\prime}}\bar{\boldsymbol{z}}^{\top}\text{diag}( \boldsymbol{1})\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bar{\boldsymbol{z}}\] \[= \frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{\boldsymbol {z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bar{\boldsymbol{z}}\]

by using the symmetry of \(\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\). It is easy to get

\[E[\boldsymbol{B}]=\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})} \boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\text{diag}( \boldsymbol{F})(\boldsymbol{\mu}_{\hat{\mathbb{P}}}-\boldsymbol{\mu}_{\hat{ \mathbb{Q}}})\]

and covariance matrix is given by

\[\text{Cov}(\boldsymbol{B},\boldsymbol{B}) = \sqrt{\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}} \boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\text{diag}( \boldsymbol{F})\big{(}\frac{m^{\prime}+n^{\prime}}{m^{\prime}n^{\prime}} \Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}\big{)}\sqrt{\frac{m^{\prime}n^{ \prime}}{m^{\prime}+n^{\prime}}}\text{diag}(\boldsymbol{F})\boldsymbol{L}_{ \hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\] \[= \sqrt{\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}}\text{diag}( \boldsymbol{F})\boldsymbol{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}

This yields that

\[\bm{B}\sim\mathcal{N}\left(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{ L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\text{diag}(\bm{F})(\bm{\mu}_{\hat{\mathbb{P}}}- \bm{\mu}_{\hat{\mathbb{Q}}}),\bm{I}_{\ell}\right)\;,\]

and all random variables in \(\bm{B}\) are mutually independent. Define

\[\bar{B}=\bm{1}^{\top}\bm{B}/\ell=\sum_{i=1}^{\ell}\bm{B}_{i}/\ell\;\;\text{ and}\;\;\;S^{2}=\sum_{i=1}^{\ell}(\bm{B}_{i}-\bar{B})^{2}\;,\]

and \(\bar{B}\) is normally distributed with mean \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{F}^{\top}\bm{L}_{ \mathbb{P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})/\ell\) and variance \(1/\ell\).

It is easy to see that

\[\bm{F}^{\top}\,\bm{L}^{\prime-1}\left(\,\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{ \hat{Y}^{\prime}}\right)=\sqrt{(m^{\prime}+n^{\prime})/m^{\prime}n^{\prime}} \bm{1}^{\top}\bm{B}\;,\]

which yields that

\[\Pr\left[\text{sgn}(\bm{F}^{\top}\,\bm{L}^{\prime-1}\left(\,\bm{c}_{\hat{X}^{ \prime}}-\bm{c}_{\hat{Y}^{\prime}}\right))=1\right]=\Pr\left[\sqrt{\frac{m^{ \prime}+n^{\prime}}{m^{\prime}n^{\prime}}}\bm{1}^{\top}\bm{B}>0\right]=\Pr \left[\bar{B}>0\right]\;.\] (13)

We further have

\[\Pr\left[\text{sgn}(\bm{F}^{\top}\,\bm{L}^{\prime-1}\left(\,\bm{c}_{\hat{X}^{ \prime}}-\bm{c}_{\hat{Y}^{\prime}}\right))=1\right]=1-\Phi\left(-\sqrt{\frac{m ^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}_{\mathbb{ P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})\right)\;,\]

where \(\Phi(\cdot)\) is the cumulative distribution function of standard Gaussian distribution. For continuous normal distribution, we have

\[\Pr\left[\text{sgn}(\bm{F}^{\top}\,\bm{L}^{\prime-1}\left(\,\bm{c}_{\hat{X}^{ \prime}}-\bm{c}_{\hat{Y}^{\prime}}\right))=0\right]=\Pr\left[\bar{B}=0\right]= 0\;,\]

and this follows that

\[\Pr\left[\text{sgn}(\bm{F}^{\top}\,\bm{L}^{\prime-1}\left(\,\bm{c}_{\hat{X}^{ \prime}}-\bm{c}_{\hat{Y}^{\prime}}\right))=-1\right]=\Pr\left[\bar{B}<0\right] =\Phi\left(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm {F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P} }}-\bm{\mu}_{\hat{\mathbb{Q}}})\right)\;.\]

Hence, the \(\text{sgn}(\bm{F}^{\top}\,\bm{L}^{\prime-1}\left(\,\bm{c}_{\hat{X}^{\prime}}- \bm{c}_{\hat{Y}^{\prime}}\,\right))\) follows a two-point distribution \(\mathcal{TP}(\xi)\) with parameter

\[\xi=1-\Phi\left(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime}) \ell}}\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{ \mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\right)\;.\]

This completes the proof. 

### Proof of Theorem 4

Recall the pooled covariance matrix \(\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}\) and \(\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}=E_{\hat{X}^{\prime}\sim\hat{ \mathbb{P}}^{m^{\prime}},\hat{Y}^{\prime}\sim\hat{\mathbb{Q}}^{n^{\prime}}}[ \Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}]\) in the proof of Theorem 2, and \(\bar{\bm{z}}=(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{Y}^{\prime}})\) follows a multivariate normal distribution with mean \(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}\) and covariance matrix \((m^{\prime}+n^{\prime})\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}/m^{\prime}n^ {\prime}\). We have \(\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}\hat{\mathbb{P}}_{\hat{\mathbb{Q}}}= \Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}\).

We begin with a useful lemma and corollary as follows.

**Lemma 12**.: _If \(\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{\mathbb{P} }}>\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{Q}}}\) or null hypothesis \(H_{0}\colon\bm{\mu}_{\bar{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}^{\prime}}\) we have_

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{\bm{z}}^{\top} \Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bar{\bm{z}}\geq c\mid\bm{F}^{ \top}\bm{L}^{\prime-1}\bar{\bm{z}}\geq 0\right]\geq\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{ \prime}+n^{\prime}}\bar{\bm{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime }}^{-1}\bar{\bm{z}}\geq c\right]\;.\] (14)

Proof.: Recall \(\bm{B}=\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{L}^{\prime-1}\text{ diag}(\bm{F})\bar{\bm{z}}\) and \(\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}=\sqrt{(m^{\prime}+n^{\prime})/m^{ \prime}n^{\prime}}\bm{1}^{\top}\bm{B}\) in the proof of Lemma 3, and Eqn. (14) is equivalent to

\[\Pr\left[\bm{B}^{\top}\bm{B}\geq c\mid\bm{1}^{\top}\bm{B}\geq 0\right]\geq\Pr \left[\bm{B}^{\top}\bm{B}\geq c\right]\;.\] (15)

Recall that

\[\bar{B}=\bm{1}^{\top}\bm{B}/\ell=\sum_{i=1}^{\ell}\bm{B}_{i}/\ell\;\;\text{ and}\;\;S^{2}=\sum_{i=1}^{\ell}(\bm{B}_{i}-\bar{B})^{2}\;,\]and from Eqn. (15), we have

\[\Pr\left[\bar{B}^{2}\geq(c-S^{2})/\ell|\bar{B}\geq 0\right]\geq\Pr\left[\bar{B}^{2 }\geq(c-S^{2})/\ell\right]\;.\]

From the independence of \(S^{2}\) and \(\bar{B}\), it is sufficient to prove that, for every \(\delta\geq 0\),

\[\Pr\left[\bar{B}^{2}\geq\delta|\bar{B}\geq 0\right]\geq\Pr\left[\bar{B}^{2} \geq\delta\right]\;.\] (16)

It's easy to see that \(\bar{B}\) is normally distributed with mean \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{F}^{\top}\bm{L}_{\mathbb{ P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{Q}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})/\ell\) and variance \(1/\ell\). We define

\[a=\sqrt{\ell\delta}\;\;\;\text{and}\;\;\;b=\sqrt{\frac{m^{\prime}n^{\prime}}{ (m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q} }}^{-1}(\bm{\mu}_{\hat{\mathbb{Q}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\;,\]

and from Eqn. (16), we have

\[\frac{\Phi(-b)}{\Phi(-b-a)}\geq\frac{\Phi(b)}{\Phi(b-a)}\;,\]

where the equality holds from \(b=0\), i.e., \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\). This completes the proof. 

**Corollary 13**.: _If \(\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{\mathbb{ Q}}}<\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{Q}}}\), we have_

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{\bar{z}}^{\top }\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1},\bm{\bar{z}}\geq c\mid\bm{F}^ {\top}\bm{L}^{\prime-1}\bm{\bar{z}}<0\right]\geq\Pr\left[\frac{m^{\prime}n^{ \prime}}{m^{\prime}+n^{\prime}}\bm{\bar{z}}^{\top}\Sigma_{\hat{X}^{\prime}, \hat{Y}^{\prime}}^{-1},\bm{\bar{z}}\geq c\right]\;,\]

**Lemma 14**.: _If \(\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{\mathbb{ P}}}>\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{Q}}}\), then the test power of our bi-directional hypothesis can be bounded by_

\[q(\ell,\nu-\ell,\lambda,\beta\alpha)\cdot\xi+\Phi\left(-(\chi_{\ell,(2-\beta) \alpha}^{2})^{1/2}-(\omega/\ell)^{1/2}\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{ \mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}) \right)\;,\]

_where \(\omega=m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\), \(\nu=m^{\prime}+n^{\prime}-1\) and \(\lambda=\omega\|\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{ \mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\|_{2}^{2}\)._

Proof.: From Lemma 12, we first have, for \(\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{\mathbb{ P}}}>\bm{F}^{\top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{Q}}}\),

\[\Pr[h=1\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\geq 0] = \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}} \geq 0\right]\] (17) \[> \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\right]\;,\]

by substituting \(c=\chi_{\ell,\beta\alpha}^{2}\) into Eqn. (14). From \(\bm{\mu}_{\hat{\mathbb{P}}}\neq\bm{\mu}_{\hat{\mathbb{Q}}}\), we also have, from the work of [84],

\[\frac{m^{\prime}+n^{\prime}-\ell-1}{(m^{\prime}+n^{\prime}-2)\ell}\mathcal{T}( \hat{X}^{\prime},\hat{Y}^{\prime})=\frac{m^{\prime}+n^{\prime}-\ell-1}{(m^{ \prime}+n^{\prime}-2)\ell}\times\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{ \prime}}\bm{\bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{ \bar{z}}\sim F(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda)\;,\]

where \(\lambda=m^{\prime}n^{\prime}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})^{\top}\Sigma_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{ \hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})/(m^{\prime}+n^{\prime})\). Given the significance level \(\beta\alpha\), we have

\[\Pr\left[\frac{m^{\prime}+n^{\prime}-\ell-1}{(m^{\prime}+n^{\prime}-2)\ell} \mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})\geq F_{\ell,m^{\prime}+n^{\prime }-1-\ell,\beta\alpha}\right]=q(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,\beta \alpha)\;,\] (18)

and this follows that

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{\bar{z}}^{\top} \Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1},\bm{\bar{z}}\geq\chi_{\ell, \beta\alpha}^{2}\right]=q(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,\beta\alpha)\;.\]

Recall \(\bm{B}=\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{L}^{\prime-1} \text{diag}(\bm{F})\bm{\bar{z}}\) and \(\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}=\sqrt{(m^{\prime}+n^{\prime})/m^{ \prime}n^{\prime}}\bm{1}^{\top}\bm{B}\) in the proof of Lemma 3, and that \(\bar{B}=\bm{1}^{\top}\bm{B}/\ell\) is normally distributed with mean \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{F}^{\top}\bm{L}_{\mathbb{P}, \hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})/\ell\) and variance \(1/\ell\). We have

\[\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}>0]=\Pr[\bar{B}\geq 0]=1-\Phi \left(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{ \top}\bm{L}_{\mathbb{P},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{ \hat{\mathbb{Q}}})\right)\;,\] (19)since \(\bar{B}\) is normally distributed with mean \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{F}^{\top}\bm{L}_{\hat{\mathbb{ P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})/\ell\) and variance \(1/\ell\). Combining with Eqns. (17)-(19), we have

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}}^{-1},\bm{\bar{z}}\geq\chi_{ \ell,\beta\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\geq 0\right]\] (20) \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}}^{-1},\bm{\bar{z}}\geq\chi_{ \ell,\beta\alpha}^{2}\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\geq 0 \right]\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\geq 0]\] \[> \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1},\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\right]\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bm{ \bar{z}}\geq 0]\] \[= q(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,\beta\alpha)\left(1 -\Phi(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^ {\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{ Q}}}-\bm{\mu}_{\hat{\mathbb{Q}}}))\right)\.\]

For \(\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{P}}}>\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{ \mu}_{\hat{\mathbb{Q}}}\), we substitute \(c=\chi_{\ell,(2-\beta)\alpha}^{2}\) into Eqn. (14), and it holds that

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{ z}}<0\right]\] (21) \[= \Pr\left[\bar{B}^{2}\geq(\chi_{\ell,(2-\beta)\alpha}^{2}-S^{2})/ \ell\cap\bar{B}<0\right]\] \[= \Pr\left[\bar{B}\leq-\sqrt{(\chi_{\ell,(2-\beta)\alpha}^{2}-S^{2} )/\ell}\right]\] \[= \Phi\left[-\sqrt{\chi_{\ell,(2-\beta)\alpha}^{2}-S^{2}}-\sqrt{ \frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}_ {\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu} _{\hat{\mathbb{Q}}})\right]\] \[> \Phi\left[-\sqrt{\chi_{\ell,(2-\beta)\alpha}^{2}}-\sqrt{\frac{m^{ \prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}_{\hat{ \mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat {\mathbb{Q}}})\right]\.\]

Combining with Eqns. (20)-(21), we give a lower bound for the test power of bi-directional hypothesis

\[\Pr\left[h=1\right] = \Pr\left[h=1\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}>0 \right]+\Pr\left[h=1\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}<0\right]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1},\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}} \geq 0\right]\] \[+\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}}\geq \chi_{\ell,(2-\beta)\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}<0\right]\] \[> q(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,\beta\alpha)\left(1- \Phi(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{ \top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{ P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}))\right)\] \[+\Phi\left(-\sqrt{\chi_{\ell,(2-\beta)\alpha}^{2}}-\sqrt{\frac{m^{ \prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}_{\hat{ \mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})\right)\.\]

**Lemma 15**.: _If \(\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{P}}}<\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_ {\hat{\mathbb{Q}}}\), then the test power of our bi-directional hypothesis can be lower bounded by_

\[q(\ell,\nu-\ell,\lambda,(2-\beta)\alpha)(1-\xi)+1-\Phi\left((\chi_{\ell,\beta \alpha}^{2})^{1/2}-(\omega/\ell)^{1/2}\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}}, \hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}}) \right)\,\]

_where \(\omega=m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})\), \(\nu=m^{\prime}+n^{\prime}-1\) and \(\lambda=\omega\|\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}(\bm{\mu}_{\hat{ \mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\|_{2}^{2}\)._

Proof.: From Corollary 13, we have, for \(\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_{\hat{ \mathbb{P}}}<\bm{F}^{\top}\bm{L}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}^{-1}\bm{\mu}_ {\hat{\mathbb{Q}}}\),

\[\Pr[h=1\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\leq 0] = \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}<0\right]\] \[> \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}}\geq \chi_{\ell,(2-\beta)\alpha}^{2}\right]\,\]Recall that

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{\bm{z}}^{\top} \Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}}\geq\chi^{2}_{\ell,(2 -\beta)\alpha}\right]=q(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,(2-\beta)\alpha)\]

in the proof of Lemma 14, and this follows that

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{ \bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}}\geq \chi^{2}_{\ell,(2-\beta)\alpha}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}< 0\right]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{ \bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}}\geq \chi^{2}_{\ell,(2-\beta)\alpha}\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}< 0\right]\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}<0]\] \[> \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{ \bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}}\geq \chi^{2}_{\ell,(2-\beta)\alpha}\right]\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bar{ \bm{z}}<0]\] \[= q\left(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,(2-\beta) \alpha\right)\Phi\left(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{ \prime})\ell}}\bm{F}^{\top}\bm{L}^{-1}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}( \bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{\mathbb{Q}}})\right)\;.\] (22)

Recall \(\bm{B}=\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{L}^{\prime-1} \text{diag}(\bm{F})\bar{\bm{z}}\) and \(\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}=\sqrt{(m^{\prime}+n^{\prime})/m^{ \prime}n^{\prime}}\bm{1}^{\top}\bm{B}\) in the proof of Lemma 3, and that \(\bar{B}=\bm{1}^{\top}\bm{B}/\ell\) is normally distributed with mean \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{F}^{\top}\bm{L}^{-1}_{ \hat{\mathbb{P}},\hat{\mathbb{Q}}}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{ \hat{\mathbb{Q}}})/\ell\) and variance \(1/\ell\). We have, if \(\bm{F}^{\top}\bm{L}^{-1}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}\bm{\mu}_{\hat{ \mathbb{P}}}<\bm{F}^{\top}\bm{L}^{-1}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}\bm{ \mu}_{\hat{\mathbb{Q}}}\),

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar {\bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}} \geq\chi^{2}_{\ell,\beta\alpha}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}} \geq 0\right]\] \[= \Pr\left[\bar{B}^{2}\geq(\chi^{2}_{\ell,\beta\alpha}-S^{2})/\ell \cap\bar{B}\geq 0\right]\] \[= \Pr\left[\bar{B}\geq\sqrt{(\chi^{2}_{\ell,\beta\alpha}-S^{2})/ \ell}\right]\] \[= 1-\Phi\left(\sqrt{\chi^{2}_{\ell,\beta\alpha}-S^{2}}-\sqrt{\frac {m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}^{-1}_ {\hat{\mathbb{P}},\hat{\mathbb{Q}}}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{ \hat{\mathbb{Q}}})\right)\] \[> 1-\Phi\left(\sqrt{\chi^{2}_{\ell,\beta\alpha}}-\sqrt{\frac{m^{ \prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}^{-1}_{ \hat{\mathbb{P}},\hat{\mathbb{Q}}}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{ \hat{\mathbb{Q}}})\right)\;.\] (23)

Combining with Eqns. (22)-(23), we give a lower bound for the test power of bi-directional hypothesis

\[\Pr\left[h=1\right] = \Pr\left[h=1\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}\geq 0 \right]+\Pr\left[h=1\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}<0\right]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar{ \bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}} \geq\chi^{2}_{\ell,\beta\alpha}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}} \geq 0\right]\] \[+\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar {\bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}} \geq\chi^{2}_{\ell,(2-\beta)\alpha}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}} <0\right]\] \[> q(\ell,m^{\prime}+n^{\prime}-1-\ell,\lambda,(2-\beta)\alpha)\Phi \left(-\sqrt{\frac{m^{\prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{ \top}\bm{L}^{-1}_{\hat{\mathbb{P}},\hat{\mathbb{Q}}}(\bm{\mu}_{\hat{\mathbb{P}}}- \bm{\mu}_{\hat{\mathbb{Q}}})\right)\] \[+1-\Phi\left(\sqrt{\chi^{2}_{\ell,\beta\alpha}}-\sqrt{\frac{m^{ \prime}n^{\prime}}{(m^{\prime}+n^{\prime})\ell}}\bm{F}^{\top}\bm{L}^{-1}_{\hat{ \mathbb{P}},\hat{\mathbb{Q}}}(\bm{\mu}_{\hat{\mathbb{P}}}-\bm{\mu}_{\hat{ \mathbb{Q}}})\right)\;.\]

This completes the proof. 

**Lemma 16**.: _For our bi-directional hypothesis, the type-I error rate is equal to \(\alpha\) if \(\bm{\mu}_{\hat{\mathbb{P}}}=\bm{\mu}_{\hat{\mathbb{Q}}}\)._

Proof.: We first consider the case \(\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}>0\). By substituting \(c=\chi^{2}_{\ell,\beta\alpha}\) into Eqn. (14), we have

\[\Pr[h=1|\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}}\geq 0]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar {\bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}} \geq\chi^{2}_{\ell,\beta\alpha}\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bar{\bm{z}} \geq 0\right]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bar {\bm{z}}^{\top}\Sigma^{-1}_{\hat{X}^{\prime},\hat{Y}^{\prime}},\bar{\bm{z}} \geq\chi^{2}_{\ell,\beta\alpha}\right]\] \[= \beta\alpha\;.\]Recall \(\bm{B}=\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{L}^{-1}\text{diag}( \bm{F})\bm{\bar{z}}\) and \(\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}=\sqrt{(m^{\prime}+n^{\prime})/m^{ \prime}n^{\prime}}\bm{1}^{\top}\bm{B}\) in the proof of Lemma 3, and that \(\bar{B}=\bm{1}^{\top}\bm{B}/\ell\) is normally distributed with mean \(\sqrt{m^{\prime}n^{\prime}/(m^{\prime}+n^{\prime})}\bm{F}^{\top}\bm{L}_{\bar{p},\bar{\mathbb{Q}}}^{-1}(\bm{\mu}_{\bar{p}}-\bm{\mu}_{\bar{\mathbb{Q}}})/\ell\) and variance \(1/\ell\). We have

\[\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\geq 0]=\Pr[\bar{B}\geq 0]=1/2\;,\]

and we have

\[P\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}} \geq 0\right]\] (24) \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\mid\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}} \geq 0\right]\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}\geq 0]=\frac{\beta\alpha}{2}\;,\]

since \(\bar{B}\) is normally distributed with mean \(0\).

For the case \(\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}<0\), we similarly substitute \(c=\chi_{\ell,(2-\beta)\alpha}^{2}\) into Eqn. (14), and it follows that

\[\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar {z}}<0\right]\] (25) \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\right]\] \[-\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar {z}}\geq 0\right]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\right]\] \[-\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\right]\Pr[\bm{F}^{\top}\bm{L}^{\prime-1} \bm{\bar{z}}\geq 0]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\right]-\frac{1}{2}\Pr\left[\frac{m^{\prime}n ^{\prime}}{m^{\prime}+n^{\prime}}\bm{\bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}}\geq\chi_{\ell,(2-\beta)\alpha}^{2}\right]\] \[= \frac{1}{2}\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{ \prime}}\bm{\bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{ \bar{z}}\geq\chi_{\ell,(2-\beta)\alpha}^{2}\right]=(1-\beta/2)\alpha\;.\]

From Eqns. (24)-(25), we have

\[\Pr\left[h=1\right] = \Pr\left[h=1\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}>0 \right]+\Pr\left[h=1\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}}<0\right]\] \[= \Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,\beta\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}} \geq 0\right]\] \[+\Pr\left[\frac{m^{\prime}n^{\prime}}{m^{\prime}+n^{\prime}}\bm{ \bar{z}}^{\top}\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}^{-1}\bm{\bar{z}} \geq\chi_{\ell,(2-\beta)\alpha}^{2}\cap\bm{F}^{\top}\bm{L}^{\prime-1}\bm{\bar{z}} <0\right]\] \[= \frac{\beta\alpha}{2}+(1-\beta/2)\alpha=\alpha\;.\]

This completes the proof. 

Theorem 4 follows from Lemmas 14-16.

### Proof of Theorem 5

Recall that \(\mathcal{B}_{\langle 1\rangle},\mathcal{B}_{\langle 2\rangle},\ldots,\mathcal{B}_{ \langle s\rangle}\) are rectangle regions of a non-increasing order w.r.t. \(g(\cdot,\cdot)\). For each rectangle region \(\mathcal{B}_{\langle i\rangle}\), we could define its local null hypothesis

\[H_{0,\langle i\rangle}:\bm{\mu}_{\hat{\mathcal{B}}_{\langle i\rangle}}=\bm{\mu}_{ \hat{\mathbb{Q}}_{\mathcal{B}_{\langle i\rangle}}}\quad\text{with}\quad\bm{\mu}_{ \hat{\mathcal{B}}_{\langle i\rangle}}=E_{\hat{\bm{x}}^{\prime}\sim\mathbb{B}_{ \mathcal{B}_{\langle i\rangle}}}[\hat{\bm{x}}^{\prime}]\;\;\text{and}\;\;\bm{\mu}_{ \hat{\mathbb{Q}}_{\mathcal{B}_{\langle i\rangle}}}=E_{\hat{\bm{y}}^{\prime}\sim \mathbb{Q}_{\mathcal{B}_{\langle i\rangle}}}[\hat{\bm{y}}^{\prime}]\;.\]

From Theorem 2, the testing statistic \(\mathcal{T}(X^{\prime}_{\mathcal{B}_{\langle i\rangle}},Y^{\prime}_{\mathcal{B}_{ \langle i\rangle}})\) follows the \(\chi^{2}\) distribution with freedom of \(\ell\) degrees under the local null hypothesis \(H_{0,\langle i\rangle}\). Denote by \(\chi_{\ell}^{2}(\mathcal{T}(\hat{X}^{\prime}_{\mathcal{B}_{i}},\hat{Y}^{\prime}_{ \mathcal{B}_{i}}))\) the \(p\)-value, and we have 

**Lemma 17**.: _[_68_]_ _The \(p\)-value \(\chi_{\ell}^{2}(\mathcal{T}(\hat{X}^{\prime}_{\mathcal{B}_{\langle i\rangle}}, \hat{Y}^{\prime}_{\mathcal{B}_{\langle i\rangle}}))\) follows a uniform distribution \(\mathcal{U}[0,1]\) under the local null hypothesis \(H_{0,\langle i\rangle}:\boldsymbol{\mu}_{\mathbb{I}_{\mathbb{R}_{\langle i \rangle}}}=\boldsymbol{\mu}_{\mathbb{I}_{\mathbb{R}_{\langle i\rangle}}}\)._

We take an interactive multi-step testing procedure to identify the index set of rectangle regions of local significant differences. Define the candidate rejection set \(\mathcal{R}(t)=\{H_{0,\langle i\rangle}\}_{i=1}^{t}\) for \(1\leq t\leq s\) with \(\mathcal{R}(0)=\emptyset\), and exclude one null hypothesis \(H_{0,\langle i\rangle}\) at the \(t\)-th step. We could generate a sequence as follows:

\[\{H_{0,\langle i\rangle}\}_{i=1}^{s}=\mathcal{R}(s)\supseteq\mathcal{R}(s-1) \supseteq\mathcal{R}(s-2)\supseteq\cdots\supseteq\mathcal{R}(0)=\emptyset\;.\] (26)

and it holds that \(H_{0,\langle i\rangle}=\mathcal{R}(i)\setminus\mathcal{R}(i-1)\). Recall the local bi-directional hypothesis

\[h_{\langle i\rangle}=h(\hat{X}^{\prime}_{\mathcal{B}_{\langle i\rangle}}, \hat{Y}^{\prime}_{\mathcal{B}_{\langle i\rangle}})\quad\text{ for }\quad i\in[s]\;.\]

Denote by \(p_{*}\) the parameter of significant level for the local two-sample test and masked \(p\)-value. From Theorem 4, we have \(\Pr[h_{\langle i\rangle}=1]=p_{*}\) under the null local hypothesis \(H_{0,\langle i\rangle}\). We further present some useful lemmas as follows.

**Lemma 18**.: _[_67_]_ _We have \(E[h_{\langle i\rangle}]=p_{*}\) for \(i\in[s]\) and \(h_{\langle 1\rangle},h_{\langle 2\rangle},\cdots,h_{\langle s\rangle}\) are mutually independent, if \(\boldsymbol{\mu}_{\mathbb{I}_{\mathbb{R}_{\langle i\rangle}}}=\boldsymbol{ \mu}_{\mathbb{I}_{\mathbb{Q}_{\langle i\rangle}}}\) for every \(i\in[s]\) and the \(p\)-values are uniformly distributed._

Denote by \(\widetilde{\mathcal{B}}\) the set of rectangle regions that the local two samples \(\hat{X}^{\prime}_{\mathcal{B}_{\langle i\rangle}}\) and \(\hat{Y}^{\prime}_{\mathcal{B}_{\langle i\rangle}}\) are actually drawn from one identical distribution, and we define

\[\mathcal{H}_{0}=\left\{H_{0,\langle i\rangle}\colon\mathcal{B}_{\langle i \rangle}\in\widetilde{\mathcal{B}}\right\}\;.\]

**Lemma 19**.: _[_67_]_ _If there is some rectangle region such that \(\boldsymbol{\mu}_{\mathbb{I}_{\mathbb{R}_{\langle i\rangle}}}\neq \boldsymbol{\mu}_{\mathbb{I}_{\mathbb{R}_{\langle i\rangle}}}\), and if the \(p\)-value follows a uniform distribution under null hypothesis \(H_{0,\langle\cdot\rangle}\), then we have_

\[E\left[h_{\langle i\rangle}\mid\left\{h_{\langle k\rangle}\right\}_{k=i+1}^{s},\left\{\mathbb{I}\left(H_{0,\langle k\rangle}\in\mathcal{H}_{0}\right) \right\}_{k=i+1}^{s},H_{0,\langle i\rangle}\in\mathcal{H}_{0}\right]=p_{*} \quad\text{ for }\quad i\in[s]\;.\]

We also define the _weighted mirror-conservativeness_, motivated from [85, 67], as follows.

**Definition 20**.: We say that a density function \(f(\cdot)\) satisfies the _weighted mirror-conservativeness_ if it holds that, for some given \(p\in(0,1]\),

\[f(aw)\leq f(1-(1-wp)a/p)\text{ for every }w\in[1,2],\;a\in[0,p]\;.\]

Here, we introduce an additional parameter \(w\) to incorporate two directions for our method, which is different from the previous mirror-conservativeness [85, 67]. We could also present two sufficient conditions for weighted mirror-conservativeness from [85, 67]: i) the non-decrease of \(f\) and ii) the convexity of cumulative density function of \(p\)-value.

**Lemma 21**.: _If the density function of the \(p\)-value satisfies the weighted mirror-conservativeness under local null hypothesis \(H_{0,\langle\cdot\rangle}\), and if there is some rectangle region such that \(\boldsymbol{\mu}_{\mathbb{I}_{\mathbb{R}_{\langle i\rangle}}}\neq\boldsymbol{ \mu}_{\mathbb{Q}_{\mathbb{R}_{\langle i\rangle}}}\), then we have, for every \(i\in[s]\),_

\[E\left[h_{\langle i\rangle}\mid\{h_{\langle k\rangle}\}_{k=i+1}^{s},\left\{ \mathbb{I}(h_{\langle k\rangle}\in\mathcal{H}_{0})\right\}_{k=i+1}^{s},H_{0, \langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k\rangle}\}_{k=1}^{s}\right] \leq p_{*}\;,\] (27)

_where \(g_{\langle k\rangle}=g(\hat{X}^{\prime}_{\mathcal{B}_{\langle k\rangle}},\hat {Y}^{\prime}_{\mathcal{B}_{\langle k\rangle}})\)._

Proof.: We first prove

\[E[h_{\langle i\rangle}\mid g_{\langle i\rangle}=a]\leq p_{*}\quad\text{ for }\quad H_{0,\langle i\rangle}\in\mathcal{H}_{0}\;,\] (28)

from our bi-directional hypothesis and masked \(p\)-value with \(\beta\in[1,2]\) in Eqn. (7), and it is sufficient to consider the following two cases:* For \(\bm{F}_{\mathcal{B}_{(i)}}^{\top}\bm{L}_{\mathcal{B}_{(i)}}^{-1}(\bm{c}_{\hat{X}_{ \hat{R}_{(i)}}^{-1}}-\bm{c}_{\hat{Y}_{\hat{R}_{(i)}}^{-1}})\geq 0\), we have \[E\left[h_{\langle i\rangle}\mid g_{\langle i\rangle}=a,\bm{F}_{ \mathcal{B}_{(i)}}^{\top}\bm{L}_{\mathcal{B}_{(i)}}^{\prime-1}(\bm{c}_{\hat{X} _{\hat{R}_{(i)}}^{-1}}-\bm{c}_{\hat{Y}_{\hat{R}_{(i)}}^{-1}})>0\right]\] \[= \frac{p_{*}f(\beta a)}{p_{*}f(\beta a)+(1-p_{*})f(1-\frac{1-\beta p _{*}}{p_{*}}a)}\] \[= \frac{p_{*}}{p_{*}+(1-p_{*})f(1-\frac{1-\beta p_{*}}{p_{*}}a)/f( \beta a)}\] \[\leq p_{*}\;,\] where the last inequality holds from the weighted mirror-conservativeness with \(p\)-value's uniform distribution \(\mathcal{U}[0,1]\).
* For \(\bm{F}_{\mathcal{B}_{(i)}}^{\top}\bm{L}_{\mathcal{B}_{(i)}}^{\prime-1}(\bm{c}_ {\hat{X}_{\hat{R}_{(i)}}^{-1}}-\bm{c}_{\hat{Y}_{\hat{R}_{(i)}}^{-1}})<0\), we similarly have \[E\left[h_{\langle i\rangle}\mid g_{\langle i\rangle}=a,\bm{F}_{ \mathcal{B}_{(i)}}^{\top}\bm{L}_{\mathcal{B}_{(i)}}^{\prime-1}(\bm{c}_{\hat{ X}_{\hat{R}_{(i)}}^{\prime}}-\bm{c}_{\hat{Y}_{\hat{R}_{(i)}}^{-1}})<0\right]\] \[= \frac{p_{*}f((2-\beta)a)}{p_{*}f((2-\beta)a)+(1-p_{*})f(1-\frac{ 1-(2-\beta)p_{*}}{p_{*}}a)}\] \[= \frac{p_{*}}{p_{*}+(1-p_{*})f(1-\frac{1-(2-\beta)p_{*}}{p_{*}}a)/ f((2-\beta)a)}\] \[\leq p_{*}\;.\]

We define the information available for choosing \(H_{0,\langle i\rangle}\) as a filtration (sequence of nested \(\sigma\)-fields)

\[\mathcal{F}_{\langle i\rangle}=\sigma\left(\{\mathcal{B}_{\langle k\rangle}, g_{\langle k\rangle}\}_{k=1}^{s},\{\chi_{\ell}^{2}(\mathcal{T}(\hat{X}_{ \mathcal{B}_{(k)}}^{\prime},\hat{Y}_{\mathcal{B}_{(k)}}^{\prime}))\}_{k=i+1}^ {s}\right)\;,\]

and also define the filtration

\[\mathcal{F}_{\langle i\rangle}^{h}=\sigma\left(\{h_{\langle k\rangle}\}_{k=i +1}^{s},\left\{\mathbb{I}(H_{0,\langle k\rangle}\in\mathcal{H}_{0})\right\}_{ k=i+1}^{s}\right)\;.\]

This follows that

\[E\left[h_{\langle i\rangle}\mid\{h_{\langle k\rangle}\}_{k=i+1} ^{s},\left\{\mathbb{I}(h_{\langle k\rangle}\in\mathcal{H}_{0})\right\}_{k=i+1} ^{s},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k\rangle}\}_{k=1} ^{s}\right]\] \[= E\left[h_{\langle i\rangle}\mid\mathcal{F}_{\langle i\rangle}^ {h},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k\rangle}\}_{k=1}^{s }\right]\] \[= E\left[E\left[h_{\langle i\rangle}\mid\mathcal{F}_{\langle i \rangle},\mathcal{F}_{\langle i\rangle}^{h},H_{0,\langle i\rangle}\in \mathcal{H}_{0},\{g_{\langle k\rangle}\}_{k=1}^{s}\right]\left|\mathcal{F}_{ \langle i\rangle}^{h},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k \rangle}\}_{k=1}^{s}\right]\]

where the last equality holds from the law of total expectation. We have

\[E\left[h_{\langle i\rangle}\mid\mathcal{F}_{\langle i\rangle}, \mathcal{F}_{\langle i\rangle}^{h},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g _{\langle k\rangle}\}_{k=1}^{s}\right]\] \[= \sum_{H_{h,j}\in\mathcal{R}(i)\cap\mathcal{H}_{0}}E[h_{\langle i \rangle}\mid\mathcal{F}_{\langle i\rangle}]\Pr\left[H_{0,\langle i\rangle}=H_{0,j}\mid\mathcal{F}_{\langle i\rangle},\mathcal{F}_{\langle i\rangle}^{h},H_{0, \langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k\rangle}\}_{k=1}^{s}\right]\;,\]

where the last equation holds from the fact that \(\{\mathcal{F}_{\langle i\rangle}^{h},H_{0,\langle i\rangle}\in\mathcal{H}_{0}, \{g_{\langle k\rangle}\}_{k=1}^{s}\}\) is a subset of \(\mathcal{F}_{\langle i\rangle}\). We further have, since \(h_{\langle i\rangle}\) is independent of other information in \(\mathcal{F}_{\langle i\rangle}\),

\[E\left[h_{\langle i\rangle}\mid\mathcal{F}_{\langle i\rangle}, \mathcal{F}_{\langle i\rangle}^{h},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g_ {\langle k\rangle}\}_{k=1}^{s}\right]\] \[= \sum_{H_{h,j}\in\mathcal{R}(i)\cap\mathcal{H}_{0}}E\left[h_{ \langle i\rangle}\mid g_{\langle i\rangle}\right]\Pr\left[H_{0,\langle i \rangle}=H_{0,j}\mid\mathcal{F}_{\langle i\rangle},\mathcal{F}_{\langle i \rangle}^{h},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k\rangle}\}_{k=1}^ {s}\right]\] \[\leq p_{*}\sum_{H_{0,j}\in\mathcal{R}(i)\cap\mathcal{H}_{0}}\Pr\left[H_{0, \langle i\rangle}=H_{0,j}\mid\mathcal{F}_{\langle i\rangle},\mathcal{F}_{ \langle i\rangle}^{h},H_{0,\langle i\rangle}\in\mathcal{H}_{0},\{g_{\langle k \rangle}\}_{k=1}^{s}\right]\] \[= p_{*}\;,\]which completes the proof. 

We say that a random variable \(Z\) follows a Bernoulli distribution with parameter \(p\), denoted by \(Z\sim\mathcal{B}ern(p)\), if

\[\Pr[Z=1]=p\quad\text{ and }\quad\Pr[Z=0]=1-p\;.\]

We also say that a random variable \(Z\) follows a negative binomial distribution with parameters \(r\) and \(p\), denoted by \(Z\sim\mathcal{NB}(r,p)\), if

\[\Pr[Z=k]=\binom{k+r-1}{k}(1-p)^{r}p^{k}\;.\]

It is necessary to introduce a definition as follows:

**Definition 22**.: We say that a random variable \(Z\) is stochastically dominated by a distribution \(\mathbb{G}\), denoted by

\[Z\preceq\mathbb{G}\;,\]

if for random variable \(X\sim\mathbb{G}\), it holds that

\[\Pr[Z\geq x]\leq\Pr[X\geq x]\quad\text{ for }\quad x\in(-\infty,+\infty).\]

We further introduce some useful lemmas as follows:

**Lemma 23**.: _[_67_]_ _Let \(Z_{1},\cdots,Z_{s}\) be i.i.d random variables with \(Z_{i}\sim\mathcal{B}ern(p_{*})\) for some \(p_{*}>0\), and write \(\widetilde{N}_{t}=\sum_{j=1}^{t}Z_{j}\) and \(\widetilde{\mathcal{G}}_{t}=\sigma(\widetilde{N}_{t},\{Z_{j}\}_{j=t+1}^{s})\) for \(t\in[s]\). We have_

\[\widetilde{N}_{\widetilde{\tau}}\preceq\mathcal{NB}(v,p_{*})\;,\]

_where the stopping index \(\widetilde{\tau}\) is parameterized by some constant \(v(\geq 1)\), defined by_

\[\widetilde{\tau}=\max\left\{0<t\leq s:t-\widetilde{N}_{t}<v\;\;\text{or}\;\;t =1\right\}\;.\]

We further introduce a weighted version of Lemma 23 as follows:

**Lemma 24**.: _[_67_]_ _Let \(\{W_{j}\}_{j=1}^{s}\) be a sequence of weights, drawn from a Bernoulli distribution, s.t. \(\sum_{j=1}^{s}W_{j}=u\) for fixed constant \(u\leq s\); and \(Z_{j}\mid\sigma(\{Z_{k},W_{k}\}_{k=j+1}^{s},W_{j}=1)\;\sim\;\mathcal{B}ern(p_ {*})\). Write \(N_{t}^{w}=\sum_{j=1}^{t}W_{j}Z_{j}\), and we have_

\[N_{\tau^{w}}^{w}\preceq\mathcal{NB}(v,p_{*})\;,\]

_where the stopping index \(\tau^{w}\) is parameterized by some constant \(v(\geq 1)\), defined by_

\[\tau^{w}=\max\left\{0<t\leq s:\sum_{j=1}^{t}W_{j}-N_{t}^{w}<v\;\;\text{or}\;\; t=1\right\}\;.\]

We now introduce a different version of Lemma 24 by considering different parameter for Bernoulli distribution as follows:

**Lemma 25**.: _[_67_]_ _Let \(Z_{j}\mid\sigma\big{(}\{Z_{k},W_{k}\}_{k=j+1}^{s},W_{j}=1\big{)}\) follow a Bernoulli distribution with parameter \(p(\{Z_{k},W_{k}\}_{k=j+1}^{s})\) for \(j\in[s]\), respectively. We have_

\[N_{\tau^{w}}^{w}\preceq\mathcal{NB}\left(v,p(\{Z_{k},W_{k}\}_{k=j+1}^{s}) \right)\preceq\mathcal{NB}(v,p_{*})\;,\]

_if \(p(\{Z_{k},W_{k}\}_{k=j+1}^{s})\leq p_{*}\) for every \(j\in[s]\)._

We now present the detailed proof of Theorem 5 as follows.

**Proof of Theorem 5**. We first consider \(\boldsymbol{\mu}_{\triangleq_{\mathcal{B}_{(i)}}}\;=\;\boldsymbol{\mu}_{ \triangleq_{\mathcal{B}_{(i)}}}\) for every \(i\in[s]\). From Lemma 18, \(\{h_{(i)}\}_{i=1}^{s}\) are \(s\) i.i.d. random variables with \(h_{(i)}\sim\mathcal{B}ern(p_{*})\). Recall that the stopping rule in our testing, i.e., Eqn. (11), which is equivalent to

\[1-(1-p_{*})^{t-|\mathcal{I}(t)|+1}\leq\alpha_{*}\;,\]where \(\alpha_{\star}\) is a parameter to control familywise error rate and \(\mathcal{I}(t)=\{i\in[t]\colon h(\tilde{X}^{\prime}_{\mathcal{B}_{(i)}},\tilde{Y}^ {\prime}_{\mathcal{B}_{(i)}})=1\}\). The stopping rule can be rewritten as \(t-|\mathcal{I}(t)|<v\) with

\[v=\lfloor\ln(1-\alpha_{\star})/\ln(1-p_{\star})\rfloor\enspace.\] (29)

Let \(Z_{j}=h_{\langle j\rangle}\) and \(\widetilde{N}_{t}=\sum_{j=1}^{t}Z_{j}\). We define the stopping index \(\widetilde{\tau}\) as follows

\[\widetilde{\tau}=\max\left\{0<t\leq s:t-\widetilde{N}_{t}<v\enspace\text{or} \enspace t=1\right\}\enspace.\]

From Lemma 23, the number of rejections at the stopping index is given by

\[|\mathcal{I}(\widetilde{\tau})|=\sum_{j=1}^{\widetilde{\tau}}h_{\langle j \rangle}=\widetilde{N_{\widetilde{\tau}}}\preceq\mathcal{NB}(v,p_{\star})\enspace.\]

If \(\boldsymbol{\mu}_{\tilde{\Phi}_{\mathcal{B}_{(j)}}}=\boldsymbol{\mu}_{ \tilde{\mathbb{Q}}_{\mathcal{B}_{(j)}}}\) for every \(i\in[s]\), then the number of false rejections is

\[|\mathcal{I}(\widetilde{\tau})\cap\mathcal{H}_{0}|=|\mathcal{I}(\widetilde{ \tau})|\preceq\mathcal{NB}(v,p_{\star})\enspace,\]

and hence the familywise error rate (FWER) is upper bounded by

\[\Pr\left[|\mathcal{I}(\widetilde{\tau})\cap\mathcal{H}_{0}|\geq 1\right] \leq 1-(1-p_{\star})^{v}\leq\alpha_{\star}\enspace,\]

where the last inequality follows from Eqn. (29).

We now consider that there is some rectangle region with \(\boldsymbol{\mu}_{\tilde{\mathbb{P}}_{\mathcal{B}_{(j)}}}\neq\boldsymbol{\mu }_{\tilde{\mathbb{Q}}_{\mathcal{B}_{(j)}}}\). In such case, we provide an upper bound for familywise error rate without the information of masked \(p\)-values, and prove that the number of false rejections is stochastically dominated by \(\mathcal{NB}(v,p_{\star})\).

Let \(Z_{j}=h_{\langle j\rangle}\) and \(W_{j}=\mathbb{I}(H_{0,\langle j\rangle}\in\mathcal{H}_{0})\). We define the stopping index \(\tau^{w}\) as follows:

\[\tau^{w}=\max\left\{0<t\leq s:\sum_{j=1}^{t}\mathbb{I}[h_{\langle j\rangle}=0 \cap H_{0,\langle j\rangle}\in\mathcal{H}_{0}]=\sum_{j=1}^{t}W_{j}(1-Z_{j})<v \enspace\text{or}\enspace t=1\right\},\]

where \(v\) is given in Eqn. (29). It is easy to see that

\[Z_{j}\mid\sigma\big{(}\{Z_{k},W_{k}\}_{k=j+1}^{s},W_{j}=1\big{)}\enspace \sim\enspace\mathcal{Bern}(p_{\star})\]

from Lemma 19. Denote by \(u=|\mathcal{H}_{0}|\), and we have \(\sum_{j=1}^{s}W_{j}=u\) and \(u\leq s\). From Lemma 24, we have the number of false rejections

\[\sum_{j=1}^{\tau^{w}}\mathbb{I}[h_{\langle j\rangle}=1\cap H_{0,\langle j \rangle}\in\mathcal{H}_{0}]=\sum_{j=1}^{\tau^{w}}W_{j}Z_{j}=N^{w}_{\tau^{w}} \preceq\mathcal{NB}(v,p_{\star})\enspace.\] (30)

Recall that \(t-|\mathcal{I}(t)|<v\) is our stopping rule on the exploration of local significant differences. Denote by \(\tau^{w}_{T}\) the stopping index in our exploration, and we have

\[\sum_{j=1}^{\tau^{w}_{T}}\mathbb{I}[h_{\langle j\rangle}=0\cap H _{0,\langle j\rangle}\in\mathcal{H}_{0}]\] \[\leq \sum_{j=1}^{\tau^{w}_{T}}\mathbb{I}[h_{\langle j\rangle}=0]=\tau ^{w}_{T}-\sum_{j=1}^{\tau^{w}_{T}}\mathbb{I}[h_{\langle j\rangle}=1]=\tau^{w }_{T}-\mathcal{I}(\tau^{w}_{T})<v\enspace.\]

Since \(N^{w}_{t}\) is non-decreasing with respect to \(t\), it is easy to obtain

\[\tau^{w}_{T}\leq\tau^{w}\quad\text{ and }\quad N^{w}_{\tau^{w}_{T}}\leq N^{w}_{\tau^{w}}\enspace,\]

and we have the number of false rejections

\[|\mathcal{I}(\tau^{w}_{T})\cap\mathcal{H}_{0}|=\sum_{j=1}^{\tau^{w}_{T}} \mathbb{I}[h_{\langle j\rangle}=1\cap H_{0,\langle j\rangle}\in\mathcal{H}_{0 }]=N^{w}_{\tau^{w}_{T}}\leq N^{w}_{\tau^{w}}\preceq\mathcal{NB}(v,p_{\star})\enspace.\] (31)We upper bound the familywise error rate without considering the masked \(p\)-values as follows:

\[\Pr\left[|\mathcal{I}(\tau_{T}^{w})\cap\mathcal{H}_{0}|\geq 1\right]\leq\Pr\left[| \mathcal{I}(\tau^{w})\cap\mathcal{H}_{0}|\geq 1\right]\leq 1-(1-p_{*})^{v}\leq \alpha_{*}\;.\]

We finally take masked \(p\)-values \(\{g_{(k)}\}_{k=1}^{s}\) into consideration. From Lemma 21, it is easy to observe

\[Z_{j}\mid\sigma\big{(}\{Z_{k},W_{k}\}_{k=j+1}^{s},W_{j}=1\big{)}\;\sim\; \mathcal{Bern}(p(\{Z_{k},W_{k}\}_{k=j+1}^{s}))\;,\]

where

\[p(\{Z_{k},W_{k}\}_{k=j+1}^{s})=E\left[h_{\langle j\rangle}\mid\{h_{\langle k \rangle}\}_{k=j+1}^{s},\{\mathbb{I}(h_{\langle k\rangle}\in\mathcal{H}_{0}) \}_{k=j+1}^{s},H_{0,\langle j\rangle}\in\mathcal{H}_{0},\{g_{\langle k\rangle} \}_{k=1}^{s}\right]\leq p_{*}\;.\]

This follows that

\[\mathcal{NB}\left(v,p\left(\{Z_{k},W_{k}\}_{k=j+1}^{s}\right)\right)\preceq \mathcal{NB}(v,p_{*})\;,\]

and we further have, from Lemma 25,

\[|\mathcal{I}(\tau_{T}^{w})\cap\mathcal{H}_{0}|=\sum_{j=1}^{\tau^{w}}W_{j}Z_{j }=N_{\tau^{w}}^{w}\preceq\mathcal{NB}\left(v,p\left(\{Z_{k},W_{k}\}_{k=j+1}^{ s}\right)\right)\preceq\mathcal{NB}(v,p_{*})\;.\]

We finally upper bound the familywise error rate by considering the masked \(p\)-values \(\big{\{}g_{\langle k\rangle}\big{\}}_{k=1}^{s}\) as

\[\Pr\left[|\mathcal{I}(\tau_{T}^{w})\cap\mathcal{H}_{0}|\geq 1 \mid\{g_{\langle k\rangle}\}_{k=1}^{s}\right] \leq \mathbb{P}\left(|\mathcal{I}(\tau^{w})\cap\mathcal{H}_{0}|\geq 1 \mid\{g_{\langle k\rangle}\}_{k=1}^{s}\right)\] \[\leq 1-(1-p_{*})^{v}\leq\alpha_{*}\;.\]

This completes the proof. 

## Appendix B Optimization for Test Locations and Mahalanobis Kernels

We take gradient method [43] for the optimization of Eqn. (5) as in the work of [10]. Specifically, we calculate gradients, and update test locations and Mahalanobis kernels iteratively. In the following of this section, we present the calculation of some crucial gradients in optimization.

For test location \(\bm{v}_{j}\) with \(j\in[\ell]\), we have

\[\nabla_{\bm{v}_{j}}\mathcal{T}(\hat{X},\hat{Y})=\left(\frac{\partial\mathcal{ T}(\hat{X},\hat{Y})}{\partial\bm{v}_{j,1}},\frac{\partial\mathcal{T}(\hat{X}, \hat{Y})}{\partial\bm{v}_{j,2}},\ldots,\frac{\partial\mathcal{T}(\hat{X},\hat{ Y})}{\partial\bm{v}_{j,\ell}}\right)^{\top}\;,\] (32)

where, for \(i\in[\ell]\),

\[\frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{\partial\bm{v}_{j,i}}=\frac{ \partial\mathcal{T}(\hat{X},\hat{Y})}{\partial\bm{c}_{\hat{X}}}\frac{\partial \bm{c}_{\hat{X}}}{\partial\bm{v}_{j,i}}+\frac{\partial\mathcal{T}(\hat{X}, \hat{Y})}{\partial\bm{c}_{\hat{Y}}}\frac{\partial\bm{c}_{\hat{Y}}}{\partial\bm {v}_{j,i}}+\mbox{Tr}\left[\frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{ \partial\Sigma_{\hat{X},\hat{Y}}}\frac{\partial\Sigma_{\hat{X},\hat{Y}}}{ \partial\bm{v}_{j,i}}\right]\;,\] (33)

where \(\mbox{Tr}[\cdot]\) denotes the trace.

We further have

\[\frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{\partial\bm{c}_{\hat{ X}}} = 2mn\Sigma_{\hat{X},\hat{Y}}^{-1}(\bm{c}_{\hat{X}}-\bm{c}_{\hat{Y} })/(m+n)\;,\] \[\frac{\partial\bm{c}_{\hat{X}}}{\partial\bm{v}_{j,i}} = \frac{1}{m}\sum_{r=1}^{m}\frac{\partial\hat{\bm{x}}_{r}}{\partial \bm{v}_{j,i}}\quad\mbox{with}\quad\frac{\partial\hat{\bm{x}}_{r}}{\partial\bm{v} _{j,i}}=\left(0,\cdots,0,\frac{\partial\kappa_{j}(\hat{\bm{x}}_{r},\bm{v}_{j}) }{\partial\bm{v}_{j,i}},0,\cdots,0\right)^{\top}\;,\]

where all elements are zeros except for the \(j\)-th element. We also have

\[\frac{\partial\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{\partial\bm{v}_{j,i}} = \frac{\partial}{\partial\bm{v}_{j,i}}\left\{\exp\left(-(\bm{x}_{r} -\bm{v}_{j})^{\top}M_{j}(\bm{x}_{r}-\bm{v}_{j})/2\gamma_{j}^{2}\right)\right\}\] \[= \kappa_{j}(\hat{\bm{x}}_{r},\bm{v}_{j})(M_{j}(\hat{\bm{x}}_{r}-\bm {v}_{j}))^{\top}(0,\ldots,1,\ldots,0)^{\top}/\gamma_{j}^{2}\;,\]

where the \(i\)-th element is \(1\). We similarly calculate \(\partial\mathcal{T}(\hat{X},\hat{Y})/\partial\bm{c}_{\hat{Y}}\times\partial\bm{c }_{\hat{Y}}/\partial\bm{v}_{j,i}\).

[MISSING_PAGE_EMPTY:29]

We similarly calculate

\[\frac{\partial\Sigma_{\hat{X},\hat{Y}}}{\partial\gamma_{j}}=\frac{1}{m+n-2}\left( \sum_{r=1}^{m}\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r} -\bm{c}_{\hat{X}})^{\top}}{\partial\gamma_{j}}+\sum_{r=1}^{n}\frac{\partial( \hat{\bm{y}}_{r}-\bm{c}_{\hat{Y}})(\hat{\bm{y}}_{r}-\bm{c}_{\hat{Y}})^{\top}}{ \partial\gamma_{j}}\right)\;,\]

where

\[\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r} -\bm{c}_{\hat{X}})^{\top}}{\partial\gamma_{j}}\] \[= \begin{pmatrix}0&\ldots&\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{ \hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})^{\top}}{\partial\gamma_{j}}& \ldots&0\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ \frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{ \hat{X}})^{\top}_{j,1}}{\partial\gamma_{j}}&\ldots&\frac{\partial(\hat{\bm{x }}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})^{\top}_{j,1}}{ \partial\gamma_{j}}&\ldots&\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})( \hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})^{\top}_{j,\ell}}{\partial\gamma_{j}}\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ 0&\ldots&\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}- \bm{c}_{\hat{X}})^{\top}_{j,1}}{\partial\gamma_{j}}&\ldots&0\end{pmatrix}\;,\]

and this follows that

\[\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r} -\bm{c}_{\hat{X}})^{\top}_{j,t}}{\partial\gamma_{j}}\] \[= \begin{pmatrix}\frac{m-1}{m}\kappa_{j}(\bm{x}_{r},\bm{v}_{t})- \frac{1}{m}\sum_{s\neq r}^{m}\kappa_{j}(\bm{x}_{s},\bm{v}_{t})\end{pmatrix} \begin{pmatrix}\frac{m-1}{m}\frac{\partial\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{ \partial\gamma_{j}}-\frac{1}{m}\sum_{s\neq r}^{m}\frac{\partial\kappa_{j}(\bm{ x}_{s},\bm{v}_{j})}{\partial\gamma_{j}}\end{pmatrix}\;.\]

We similarly have

\[\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r} -\bm{c}_{\hat{X}})^{\top}_{j,j}}{\partial\gamma_{j}}\] \[=2\begin{pmatrix}\frac{m-1}{m}\kappa_{j}(\bm{x}_{r},\bm{v}_{j})- \frac{1}{m}\sum_{s\neq r}^{m}\kappa_{j}(\bm{x}_{s},\bm{v}_{j})\end{pmatrix} \begin{pmatrix}\frac{m-1}{m}\frac{\partial\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{ \partial\gamma_{j}}-\frac{1}{m}\sum_{s=r}^{m}\frac{\partial\kappa_{j}(\bm{x}_{ s},\bm{v}_{j})}{\partial\gamma_{j}}\end{pmatrix}\;.\]

For Mahalanobis matrix \(M_{j}\) with \(j\in[\ell]\), we have

\[\nabla_{M_{j}}\mathcal{T}(\hat{X},\hat{Y})=\begin{pmatrix}\frac{\partial \mathcal{T}(\hat{X},\hat{Y})}{\partial M_{j,1,1}}&\ldots&\frac{\partial \mathcal{T}(\hat{X},\hat{Y})}{\partial M_{j,1,\ell}}\\ \vdots&\ddots&\vdots\\ \frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{\partial M_{j,\ell}}&\ldots&\frac{ \partial\mathcal{T}(\hat{X},\hat{Y})}{\partial M_{j,\ell}}\end{pmatrix}\;,\] (35)

where we denote by \(M_{j,a,b}\) the element in \(a\)-th row and \(b\)-th column in \(M_{j}\).

We further have

\[\frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{\partial M_{j,a,b}}=\frac{\partial \mathcal{T}(\hat{X},\hat{Y})}{\partial\bm{c}_{\hat{X}}}\frac{\partial\bm{c}_{ \hat{X}}}{\partial M_{j,a,b}}+\frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{ \partial\bm{c}_{\hat{Y}}}\frac{\partial\bm{c}_{\hat{Y}}}{\partial M_{j,a,b}}+ \text{Tr}\begin{bmatrix}\frac{\partial\mathcal{T}(\hat{X},\hat{Y})}{\partial \Sigma_{\hat{X},\hat{Y}}}\frac{\partial\Sigma_{\hat{X},\hat{Y}}}{\partial M_{j,a,b}}\end{bmatrix}\;,\]

with

\[\frac{\partial\bm{c}_{\hat{X}}}{\partial M_{j,a,b}}=\frac{1}{m}\sum_{r=1}^{m} \frac{\partial\hat{\bm{x}}_{r}}{\partial M_{j,a,b}}\quad\text{and}\quad\frac{ \partial\hat{\bm{x}}_{r}}{\partial M_{j,a,b}}=\begin{pmatrix}0,\ldots,\frac{ \partial\kappa_{j}(\hat{\bm{x}}_{r},\bm{v}_{j})}{\partial M_{j,a,b}},\ldots,0 \end{pmatrix}^{\top}\;,\]

and

\[\frac{\partial\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{\partial M_{j,a,b}} = \frac{\partial\exp\left(-(\bm{x}_{r}-\bm{v}_{j})^{\top}M_{j}(\bm{x} _{r}-\bm{v}_{j})/2\gamma_{j}^{2}\right)}{\partial M_{j,a,b}}\] \[= -\frac{\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{2\gamma_{j}^{2}}\text{ Tr}\begin{bmatrix}\frac{\partial(\bm{x}_{r}-\bm{v}_{j})^{\top}M_{j}(\bm{x}_{r}-\bm{v}_{j})}{ \partial M_{j}}\frac{\partial M_{j}}{\partial M_{j,a,b}}\end{bmatrix}\] \[= -\frac{\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{2\gamma_{j}^{2}}\text{ Tr}\begin{bmatrix}(\bm{x}_{r}-\bm{v}_{j})(\bm{x}_{r}-\bm{v}_{j})^{\top}\bm{J}^{a,b} \end{bmatrix}\;,\]where \(\bm{J}^{a,b}\) is the single-entry matrix (1 at \((a,b)\) and zero elsewhere). We similarly calculate \(\partial\bm{c}_{\hat{Y}}/\partial M_{j,a,b}\) and have

\[\frac{\partial\Sigma_{\hat{X},\hat{Y}}}{\partial M_{j,a,b}}=\frac{1}{m+n-2} \left(\sum_{r=1}^{m}\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm {x}}_{r}-\bm{c}_{\hat{X}})^{\top}}{\partial M_{j,a,b}}+\sum_{r=1}^{n}\frac{ \partial(\hat{\bm{y}}_{r}-\bm{c}_{\hat{Y}})(\hat{\bm{y}}_{r}-\bm{c}_{\hat{Y}}) ^{\top}}{\partial M_{j,a,b}}\right)\,,\]

where

\[\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r} -\bm{c}_{\hat{X}})^{\top}}{\partial M_{j,a,b}}\] \[= \left(\begin{array}{ccccc}0&\cdots&\frac{\partial(\hat{\bm{x}}_ {r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})^{\top}_{1,j}}{ \partial M_{j,a,b}}&\cdots&0\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ \frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{ \hat{X}})^{\top}_{1,1}}{\partial M_{j,a,b}}&\cdots&\frac{\partial(\hat{\bm{x} }_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})^{\top}_{1,j}}{ \partial M_{j,a,b}}&\cdots&\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})( \hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})^{\top}_{j,\ell}}{\partial M_{j,a,b}}\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ 0&\cdots&\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}- \bm{c}_{\hat{X}})^{\top}_{\ell,j}}{\partial M_{j,a,b}}&\cdots&0\end{array} \right)\,.\]

Here, we have

\[\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r }-\bm{c}_{\hat{X}})^{\top}_{j,t}}{\partial M_{j,a,b}}\] \[= \left(\frac{m-1}{m}\kappa_{j}(\bm{x}_{r},\bm{v}_{t})-\frac{1}{m} \sum_{s\neq r}^{m}\kappa_{j}(\bm{x}_{s},\bm{v}_{t})\right)\left(\frac{m-1}{m} \frac{\partial\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{\partial M_{j,a,b}}-\frac{1}{m }\sum_{s\neq r}^{m}\frac{\partial\kappa_{j}(\bm{x}_{s},\bm{v}_{j})}{\partial M _{j,a,b}}\right)\,.\]

We similarly have \(\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}}) ^{\top}_{t,j}\,/\,\partial M_{j,a,b}\) and

\[\frac{\partial(\hat{\bm{x}}_{r}-\bm{c}_{\hat{X}})(\hat{\bm{x}}_{r }-\bm{c}_{\hat{X}})^{\top}_{j,j}}{\partial M_{j,a,b}}\] \[= 2\left(\frac{m-1}{m}\kappa_{j}(\bm{x}_{r},\bm{v}_{j})-\frac{1}{m }\sum_{s\neq r}^{m}\kappa_{j}(\bm{x}_{s},\bm{v}_{j})\right)\left(\frac{m-1}{m} \frac{\partial\kappa_{j}(\bm{x}_{r},\bm{v}_{j})}{\partial M_{j,a,b}}-\frac{1}{m }\sum_{s=r}^{m}\frac{\partial\kappa_{j}(\bm{x}_{s},\bm{v}_{j})}{\partial M_{j, a,b}}\right)\,.\]

#### Project Mahalanobis matrix onto a positive definite cone

We can not guarantee the positive-definiteness of Mahalanobis matrices during the optimization process via gradient ascend. Motivated from [43], we project Mahalanobis matrix onto a positive definite cone as follows:

* Present the spectral (eigenvalue) decomposition of a Mahalanobis matrix \(M\) as \[M=\sum_{i=1}^{d}\lambda_{i}\bm{p}_{i}\bm{p}_{i}^{T}\,.\] where \(\lambda_{1},\lambda_{2},\cdots,\lambda_{d}\) are their eigenvalues with corresponding eigenvectors \(\bm{p}_{1},\bm{p}_{2},\cdots,\bm{p}_{d}\).
* Project the Mahalanobis matrix \(M\) onto a positive definite cone \[M=\sum_{i=1}^{d}\max\left\{\lambda_{i},\delta\right\}v_{i}v_{i}^{T}\,\text{ for small positive constant }\,\delta\,.\]

## Appendix C Datasets and Parameter Setting

#### Datasets

We partition datasets into several disjoint subsets, and then randomly draw data elements from each subset based on the sample fraction, i.e., the proportion of samples to be selected from each subset. We construct two different samples by using two different sample fractions in above stratified sampling process, and construct two samples drawn from one identical distribution by adapting a same sample fraction, as done in [60].

We provide the details of constructing two samples for each dataset as follows:

* blob is constructed as the mixture of nine Gaussian modes. We first write \[\begin{array}{ll}u_{1}=[0,0],&u_{2}=[0,1],&u_{3}=[0,2],\\ u_{4}=[1,0],&u_{5}=[1,1],&u_{6}=[1,2],\\ u_{7}=[2,0],&u_{8}=[2,1],&u_{9}=[2,2],\end{array}\] and \[\Delta_{i}=\begin{cases}-0.02-0.002\times(i-1)&\text{for}&i<5\\ 0&\text{for}&i=5\\ 0.02+0.002\times(i-6)&\text{for}&i>5\;.\end{cases}\] To construct different distributions \(\mathbb{P}\) and \(\mathbb{Q}\), we adapt different covariance structures and \[\mathbb{P}=\sum_{i=1}^{9}\frac{1}{9}\mathcal{N}\left(u_{i},0.03\times\bm{I}_ {2}\right)\;\;\;\text{and}\;\;\;\mathbb{Q}=\sum_{i=1}^{9}\frac{1}{9}\mathcal{ N}\left(u_{i},\left[\begin{array}{cc}0.03&\Delta_{i}\\ \Delta_{i}&0.03\end{array}\right]\right).\] To construct identical distribution for two samples, i.e., \(\mathbb{P}=\mathbb{Q}\), we have \[\mathbb{P}=\sum_{i=1}^{9}\frac{1}{9}\mathcal{N}\left(u_{i},0.03\times\bm{I}_ {2}\right)\;\;\;\text{and}\;\;\;\mathbb{Q}=\sum_{i=1}^{9}\frac{1}{9}\mathcal{ N}\left(u_{i},0.03\times\bm{I}_{2}\right).\] We set the sample size for training to \(900\) and for testing to \(224\).
* dna is a categorical dataset with \(3\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.30,0.35,0.35]\) and for the other sample to \([0.45,0.25,0.3]\); To construct two samples with one identical distribution, we set the same sample fraction \([0.30,0.35,0.35]\) for two samples. We set the sample size for training to \(1000\) and for testing to \(250\).
* agnos (agnostic) is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.35,0.65]\) and for the other sample to \([0.65,0.35]\); To construct two samples with one identical distribution, we set the same sample fraction \([0.35,0.65]\) for two samples. We set the sample size for training to \(1000\) and for testing to \(250\).
* topo21 is a regression dataset with continuous target variables. Based on the sorted target variables, we divide the data into \(4\) equal parts. For constructing two different samples, we set the sample fraction for one sample to \([0.1,0.3,0.2,0.4]\) and for the other sample to \([0.5,0.2,0.1,0.2]\); To construct two samples with one identical distribution, we set the sample size for training to \(2200\) and for testing to \(550\).
* kror is a categorical dataset with \(18\) classes, where we only consider categories \(13\), \(14\), \(15\) and \(16\) which have the majority of the data.. For constructing two different samples, we set the sample fraction for one sample to \([0.15,0.2,0.3,0.35]\) and for the other sample to \([0.35,0.35,0.15,0.15]\); To construct two samples with one identical distribution, we set the same sample fraction \([0.25,0.25,0.25,0.25]\) for two samples. We set the sample size for training to \(2000\) and for testing to \(500\).
* diamond (diamonds) is a regression dataset with continuous target variables. Based on the sorted target variables, we divide the data into \(4\) equal parts. For constructing two different samples, we set the sample fraction for one sample to \([0.35,0.2,0.2,0.25]\) and for the other sample to \([0.2,0.3,0.3,0.2]\); To construct two samples with one identical distribution, we set the same sample fraction \([0.25,0.25,0.25,0.25]\) for two samples. We set the sample size for training to \(2000\) and for testing to \(500\).

* Original cifar10 (samples \(\mathbb{P}\)) is compared to adversarial-cifar10 (samples \(\mathbb{Q}\)) following [19], where adversarial-cifar10 is constructed by [86] for distribution shift detection. To construct two samples with one identical distribution, we select randomly between original cifar10 and adversarial-cifar10 and drawn two samples from the same dataset. The image is scaled to \(32\times 32\) and we set the sample size for training to \(200\) and for testing to \(50\).
* mnist contains \(70000\) handwritten digit images, we compare true mnist data (samples \(\mathbb{P}\)) to Fake-mnist data (samples \(\mathbb{Q}\)) following [39], where the Fake-mnist data is drawn from a pre-trained deep convolutional generative adversarial network [87] and the image is scaled to \(32\times 32\). To construct two samples with one identical distribution, we select randomly between original mnist and Fake-mnist and drawn two samples from the same dataset. We set the sample size for training to \(600\) and for testing to \(150\).
* santan (santandercustomersatisfaction) is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.8,0.2]\) and for the other sample to \([0.25,0.75]\); To construct two samples with one identical distribution, we set the same sample fraction \([0.5,0.5]\) for two samples. We set the sample size for training to \(1000\) and for testing to \(250\).
* codrna is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.7,0.3]\) and for the other sample to \([0.35,0.65]\); To construct two samples with one identical distribution, we set same sample fraction \([0.5,0.5]\) for two samples. We set the sample size for training to \(5000\) and for testing to \(1250\).
* adult is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.6,0.4]\) and for the other sample to \([0.4,0.6]\); To construct two samples with one identical distribution, we set same sample fraction \([0.5,0.5]\) for two samples. We set the sample size for training to \(5000\) and for testing to \(1250\).
* labor is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.6,0.4]\) and for the other sample to \([0.45,0.55]\); To construct two samples with one identical distribution, we set same sample fraction \([0.5,0.5]\) for two samples. We set the sample size for training to \(5000\) and for testing to \(1250\).
* poker is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.2,0.8]\) and for the other sample to \([0.8,0.2]\); To construct two samples with one identical distribution, we set same sample fraction \([0.5,0.5]\) for two samples. We set the sample size for training to \(3000\) and for testing to \(750\).

\begin{table}
\begin{tabular}{|l l l l|} \hline Dataset & \# Test Locations & Learning Rate & Optimization Epoch \\ \hline blob & 17 & 0.007 & 1000 \\ dna & 15 & 0.0004 & 1000 \\ agnos & 15 & 0.001 & 200 \\ topo21 & 15 & 0.001 & 1000 \\ har & 15 & 0.001 & 100 \\ kropt & 1 & 0.002 & 1000 \\ diamond & 2 & 0.0013 & 1000 \\ cifar10 & 2 & 0.003 & 200 \\ mnist & 2 & 0.001 & 500 \\ santan & 10 & 0.001 & 1000 \\ codrna & 2 & 0.01 & 1000 \\ sea50 & 1 & 0.01 & 1000 \\ adult & 1 & 0.001 & 1000 \\ labor & 1 & 0.001 & 1000 \\ poker & 3 & 0.002 & 200 \\ higgs & 15 & 0.001 & 1000 \\ \hline \end{tabular}
\end{table}
Table 4: Optimization parameters of our \(\text{MEM}_{\text{MBID}}\) test for different datasets.

* higgs is a categorical dataset with \(2\) classes. For constructing two different samples, we set the sample fraction for one sample to \([0.0,1.0]\) and for the other sample to \([1.0,0.0]\); To construct two samples with one identical distribution, we set same sample fraction \([1.0,0.0]\) for two samples. We set the sample size for training to \(16000\) and for testing to \(4000\).

In optimization, we adapt Adam optimization method from the pytorch library in python [88, 89]. Table 4 presents the details of the hyperparameter settings for each dataset, including the number of test locations, learning rate.

#### Experimental settings

At initialization, we usually set the Mahalanobis matrices \(\{M_{i}\}_{i=1}^{\ell}\) to identity matrices. We further provide an alternative initialization for the Mahalanobis matrices based on the correlation information of two training samples as follows:

\[M_{j}=\sum_{i=1}^{m+n}(\bm{S}_{j,i}-\bm{c}_{\bm{S}_{j}})(\bm{S}_{j,i}-\bm{c}_{ \bm{S}_{j}})^{\top}/(m+n-1)+\delta\mathbf{I}_{d}\;,\]

where \(\bm{S}_{j}=\{\bm{x}_{1}-\bm{v}_{j},...,\bm{x}_{m}-\bm{v}_{j},\bm{y}_{1}-\bm{v}_ {j},...,\bm{y}_{n}-\bm{v}_{j}\}\), \(\bm{S}_{j,i}\) is the \(i\)-th element of \(\bm{S}_{j}\), and \(\bm{c}_{\bm{S}_{j}}=\sum_{i=1}^{m+n}\bm{S}_{j,i}/(m+n)\). \(\mathbf{I}_{d}\) is an identity matrix of size \(d\times d\) to guarantee the positive definiteness with small constant \(\delta>0\). Here, the elements \(\bm{x}\) and \(\bm{y}\) are come from training samples \(X\) and \(Y\), and \(m\) and \(n\) are the numbers of elements in \(X\) and \(Y\) respectively.

For initialization of test locations, we first fit a Gaussian distribution for each sample and draw half of the number of test locations from each distribution. This could be expensive for high dimensional dataset, and we can simplify the process by directly sampling test locations from original dataset.

For initialization of the bandwidth parameters of Mahalanobis kernels, we let \(\gamma_{j}=\gamma\) for every \(j\in[\ell]\) and then linearly search for \(\gamma\) that maximize the statistic from a candidate list with fixed test locations and Mahalanobis matrices, which can be formalized as follows:

\[\gamma_{*}=\text{argmax}_{\gamma}\mathcal{T}(\hat{X},\hat{Y})\ \ \text{ for }\ \gamma\in\left\{med^{2}.2^{-4},\ med^{2}.2^{-3.8},\ med^{2}.2^{-3.6},\ \ldots\,\ med^{2}.2^{4}\right\},\] (36)

where \(med\) denotes the median of pairwise Euclidean distances of points in \(X\) and \(Y\).

#### Compared methods

We now present the details for our compared methods as follows:

* **ME2**: The mean embeddding method learns a set of test locations and a single Gaussian kernel, and then measures the difference between two mean embeddings with a statistic following \(\chi^{2}\) distribution [9, 10]; Footnote 2: The code is downloaded from _github.com/wittawat/interpretable-test_.

**Input**: Two testing samples \(X,Y\), Mahalanobis kernels' parameters \(\{M_{i}\}_{i=1}^{\ell}\) and \(\{\gamma_{i}\}_{i=1}^{\ell}\), test locations \(\{\bm{v}_{i}\}_{i=1}^{\ell}\), partition tree \(T\)

**Output**: \(t^{*}\)

```
1:Calculate the embedding testing samples \(\hat{X}^{\prime}\) and \(\hat{Y}^{\prime}\) based on Eqn. (2)
2:Calculate mean embeddings \(\bm{c}_{\hat{X}^{\prime}}=\sum_{i=1}^{m^{\prime}}\hat{\bm{x}}_{i}^{\prime}/m^{\prime}\) and \(\bm{c}_{\hat{Y}^{\prime}}=\sum_{j=1}^{n}\hat{\bm{y}}_{i}^{\prime}/n^{\prime}\)
3:Calculate the pooled covariance matrix \(\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}\) based on Eqn. (3)
4:Calculate the Schur decomposition of the pooled covariance matrix: \(\bm{L}^{\prime}\bm{L}^{\prime}=\Sigma_{\hat{X}^{\prime},\hat{Y}^{\prime}}\).
5:if\(\bm{F}^{T}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{Y}^{\prime}})\geq 0\)then
6:\(h=\mathbb{I}[\chi_{\ell}^{2}(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})) \leq\beta\alpha]\)
7:else
8:\(h=\mathbb{I}[\chi_{\ell}^{2}(\mathcal{T}(\hat{X}^{\prime},\hat{Y}^{\prime})) \leq(2-\beta)\alpha]\)
9:endif
10:return\(h\) ```

**Algorithm 4** Exploring the local significant differences

* **C2ST-S3**: A binary classification neural network is trained and the statistic is computed as the accuracy over a hold-out set of two samples [13]; Footnote 3: The code is downloaded from _github.com/lopezpaz/classifier_tests_.
* **MMDAgg4**: A solution for the fundamental kernel selection problem involves the aggregation of a large number of kernels with several bandwidths, where the incomplete \(U\)-statistics are used to measure the difference between two samples Schrab et al. [75]. Notice that we set the testing sample size for MMDAgg to 5 times that of the other methods, since it does not require training; Footnote 4: The code is downloaded from _github.com/antoninschrab/agginc-paper_.
* **MMD-D5**: A deep kernel approach for Maximum Mean Discrepancy (MMD), where the parameters of a neural network, two lengthscales of Gaussian kernels and a regularization parameter are optimized [39]; Footnote 5: The code is downloaded from _github.com/xycheng/net_logit_test_.
* **C2ST-L6**: A binary classification neural network is trained and the statistic is computed as the difference between outputs of the logit function corresponding to two samples [14, 18]; Footnote 7: The code is downloaded from _github.com/jmkuebler/autoML-TST-paper_.
* **AutoML7**: A binary classifier is trained based on Automated Machine Learning techniques and the statistic is same to C2ST-L approach [19]. Footnote 7: The code is downloaded from _github.com/lopezpaz/classifier_tests_.

We implement methods for exploring local significant differences in Python, following their respective guidelines as follows:

* **FDG**: Partition the sample space based on probability binning and then compare the cardinalities of two samples over rectangle regions, where a normalized chi-squared value is computed for each bin to measure the local difference [24];* **K-PRIM**: Partition the space based on patient rule induction method along with the information of kernel density estimation, and use a statistic following \(\chi^{2}_{1}\) distribution to identify local distributional difference [28];
* **MRS**: Partition the sample space based on polya tree method, which splits a leaf node based on the median of randomly selected feature, and then measure the difference between local two samples based on Binomial distribution [26];
* **TEAM**: Partition the sample space on the median of the feature with largest sample variance, and then measure differences between local two samples with Binomial distribution [27];
* **BTLDD**: Estimate the conditional probabilities of two samples based on a regression model and then cluster those elements with significant different conditional probabilities [77];
* **MMDT**: Partition the sample space into multiple equal grids based on quantile values of the features, and then estimate the kernel densities of two samples and measure local differences based on Welch's two-sample t-test statistic [29].

For the exploration of local significant differences, we take density differences [78] between two samples in a local region as an evaluation measure for local significant differences, and follow the works of [79, 80] based on \(k\)-NN density estimator with \(k=20\). Here, denote by \(X\) and \(Y\) the available data for density evaluation, we have estimated density functions as follows:

\[f_{X}(\bm{z}):=\frac{20}{N\cdot v_{d}\cdot r_{X}(\bm{z})^{d}}\quad\text{and} \quad f_{Y}(\bm{z}):=\frac{20}{N\cdot v_{d}\cdot r_{Y}(\bm{z})^{d}}\,\]

where \(\bm{z}\in[0,1]^{d}\), \(N\) denotes sample size for density estimation and \(v_{d}\) is the volume of a unit ball in \(\mathbb{R}^{d}\). Denote by \(r_{X}(\bm{z})^{d}\) and \(r_{Y}(\bm{z})^{d}\) the distances from \(\bm{z}\) to its \(20\)-th nearest neighbors in \(X\) and \(Y\), respectively. For density estimation, we use all data of diamond, and \(1,000,000\) data of other datasets due to the limitation of time complexity.

For our ME\({}_{\text{M}\text{B}\text{i}\text{B}\text{I}}\) test, we present the detailed training procedure in Algorithm 2 and testing procedure in Algorithm 3. Figure 9 is a pictorial illustration to present the region-splitting method and clarify that our partition tree is constructed iteratively: We initiate tree root with embedding space, and during each iteration, we select randomly one of those leaves with the largest size of training data points, and select the feature of the largest statistics value and with the median splitting position. We present the detailed description on the exploration of local significant differences in Algorithm 4.

Figure 9: An illustration for our partition tree with splitting regions.

[MISSING_PAGE_FAIL:37]

[MISSING_PAGE_FAIL:38]

additional experiments on the relationship between the optimal parameter \(\beta\) and the probability \(\xi=\Pr[\bm{F}^{\top}\bm{L}^{\prime-1}(\bm{c}_{\hat{X}^{\prime}}-\bm{c}_{\hat{Y}^ {\prime}})\geq 0]\), as shown in Figure 6.