FSEO: A Few-Shot Evolutionary Optimization Framework for Expensive Multi-Objective Optimization and Constrained Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Meta-learning has been demonstrated to be useful to improve the sampling efficiency of Bayesian optimization (BO) and surrogate-assisted evolutionary algorithms (SAEAs) when solving expensive optimization problems (EOPs). However, existing studies focuses on only single-objective optimization, leaving other expensive optimization scenarios unconsidered. We propose a generalized few-shot evolutionary optimization (FSEO) framework and focus on its performance on two common expensive optimization scenarios: multi-objective EOPs (EMOPs) and constrained EOPs (ECOPs). We develop a novel meta-learning modeling approach to train surrogates for our FSEO framework, an accuracy-based update strategy is designed to adapt surrogates during the optimization process. The surrogates in FSEO framework combines neural network with Gaussian Processes (GPs), their network parameters and some parameters of GPs represent useful experience and are meta-learned across related optimization tasks, the remaining GPs parameters are task-specific parameters that represent unique features of the target task. We demonstrate that our FSEO framework is able to improve sampling efficiency on both EMOP and ECOP. Empirical conclusions are made to guide the application of our FSEO framework.

## 1 Introduction

Expensive optimization problems (EOPs) aim to find as good as possible solutions within a budget of limited solution evaluations. Conventional Bayesian optimization (BO) and surrogate-assisted evolutionary algorithms (SAEAs) have been widely used to solve EOPs, but they train surrogate models from the scratch. To further improve the sampling efficiency and optimization performance, many efforts have been made to pre-train surrogates with the prior experience gain from related optimization tasks, resulting in experience-based optimization algorithms [1; 21; 36; 35].

This work considers solving EOPs on the context of few-shot problems [5; 41], where plenty of expensive related tasks are available and each of them can provide a small dataset for experience learning. Therefore, many experience-based optimization approaches such as multi-tasking optimization [43; 2; 47], transfer optimization [35; 17; 16] are not considered as they cannot learn experience from small related tasks (A discussion is available in Appendix A). In comparison, meta-learning [14] has been proved to be powerful in solving few-shot problems, leading to a new subcategory of experience-based optimization, namely few-shot optimization (FSO) [46].

Existing studies on FSO are mainly few-shot Bayesian optimization (FSBO) where meta-learning approaches are combined with BO to solve EOPs with only one objective. In this paper, we propose a generalized few-shot evolutionary optimization (FSEO) framework to address EOPs from the perspective of SAEAs and consider two expensive optimization scenarios which have been limited studied: multi-objective EOPs (EMOPs) and constrained EOPs (ECOPs). Major contributions are summarized as follows.

* A novel meta-learning method, namely Meta Deep Kernel Learning (MDKL), is developed to gain prior experience from related expensive tasks. Our model architecture and parameter designs make it possible to generate a regression-based surrogate on the prior experience and then continually adapt the surrogate to approximate the target task.
* We propose a FSEO framework to solve EOPs from the perspective of SAEAs. FSEO framework is applicable to regression-based SAEAs since FSEO embed our meta-learning models in these SAEAs as their surrogates. In addition, an update strategy is designed to adapt surrogates constantly during the optimization. Note that our FSEO framework is a general framework but we focus on its performance on EMOPs and ECOPs in this paper.
* Experiments are conducted on EMOPs and ECOPs to show our FSEO framework is effective. Our comprehensive ablation studies discover the influence of some factors on FSEO performance and provide empirical guidance to the application of FSEO framework.

## 2 Related Work

Experience-based optimization can be divided into several subcategories according to the techniques of learning prior experience from related tasks. A detailed classification and discussion on these subcategories is available in Appendix A. This subsection focuses on related work on FSO.

FSO studies in the literature can be classified based on their model architectures. Most studies meta-learn parameters for Gaussian Processes (GPs) [44], namely FSBO or Meta Bayesian Optimization (MBO) [32, 42, 26, 38]. In addition, [23] meta-learns with transformer neural processes and [46, 6] meta-learn parameters for the architecture of deep kernel learning (DKL) [45]. The MDKL model in our FSEO belongs to the last category as its model architecture is relevant to DKL.

Our work is different from existing studies in three points: Firstly, many studies [46] use existing meta-learning models [27] as their surrogates. No further adaptations are made to these surrogates during optimization since they are not originally designed for optimization. In comparison, we try to develop a meta-learning model, MDKL, for optimization purpose. MDKL has explicit task-specific parameters, which allows continually model adaptations during the optimization. Secondly, existing work investigated only global optimization, leaving other optimization scenarios such as EMOP and ECOP still awaiting for investigation. As our MDKL is designed for optimization and is capable of continually adaptation, we pay attention on EMOPs and ECOPs which require more effective models than global optimization. Our work widens the scope of existing FSO research and it focuses on the perspective of SAEAs instead of BO. Lastly, in-depth ablation studies are lacking in the literature, making it unclear which factors affect the performance of FSO. Our extensive ablation studies fill this gap and we conclude some empirical rules to improve the performance of FSO.

## 3 Background

This section gives preliminaries about meta-learning and DKL. The former is the method of experience learning, the latter is the underlying structure of experience representation.

### Meta-Learning in Few-Shot Problems

In the context of few-shot problems, we have plenty of related tasks, each task \(T\) contributes a couple of small datasets \(D=\{(S,Q)\}\), namely support dataset \(S\) and query dataset \(Q\), respectively. After learning from datasets of random related tasks, a support set \(S_{*}\) from new unseen task \(T_{*}\) is given and one is asked to estimate the labels or values of a query set \(Q_{*}\). The problem is called 1-shot or 5-shot when only 1 data point or 5 data points are provided in \(S_{*}\). A comprehensive definition of few-shot problems is available in [5, 41].

Meta-learning methods have been widely used to solve few-shot problems [41]. They learn domain-specific features that are shared among related tasks as experience, such experience is used to understand and interpret the data collected from new tasks encountered in the future.

### Deep Kernel Learning (DKL)

DKL aims at constructing kernels that encapsulate the expressive power of deep architectures for GPs. To create expressive and scalable closed form covariance kernels, DKL combines the non-parametric flexibility of kernel methods and the structural properties of deep neural networks. In practice, a deep kernel \(k(\mathbf{x}^{i},\mathbf{x}^{j}|\boldsymbol{\gamma})\) transforms the inputs \(\mathbf{x}\) of a base kernel \(k(\mathbf{x}^{i},\mathbf{x}^{j}|\boldsymbol{\theta})\) through a non-linear mapping given by a deep architecture \(\phi(\mathbf{x}|\mathbf{w},\mathbf{b})\):

\[k(\mathbf{x}^{i},\mathbf{x}^{j}|\boldsymbol{\gamma})=k(\phi(\mathbf{x}^{i}| \mathbf{w},\mathbf{b}),\phi(\mathbf{x}^{j}|\mathbf{w},\mathbf{b})|\boldsymbol{ \theta}),\] (1)

where \(\boldsymbol{\theta}\) and \((\mathbf{w},\mathbf{b})\) are parameter vectors of the base kernel and the deep architecture, respectively. \(\boldsymbol{\gamma}=\{\boldsymbol{\theta},\mathbf{w},\mathbf{b}\}\) is the set of all parameters in this deep kernel. Note that in DKL, all parameters \(\boldsymbol{\gamma}\) of a deep kernel \(k(\mathbf{x}^{i},\mathbf{x}^{j}|\boldsymbol{\gamma})\) are learned jointly by using the log marginal likelihood function of GPs as a loss function. Such a jointly learning strategy has been shown to make a DKL algorithm outperform a combination of a deep neural network and a GP model, where a trained GP model is applied to the output layer of a trained deep neural network [45].

### Meta-Learning on DKL

An important distinction between DKL algorithms and the applications of meta-learning to DKL is that DKL algorithms learn their deep kernels from single tasks instead of collections of related tasks. Such a difference alleviates two drawbacks of single task DKL [39]: First, the scalability of deep kernels is no longer an issue as datasets in meta-learning are small. Second, the risk of overfitting is decreased since diverse data points are sampled across tasks.

## 4 Few-Shot Evolutionary Optimization (FSEO) Framework

In this paper, \(T_{*}\) denotes the target optimization task, and plenty of small datasets \(D_{i}\) sampled from related tasks \(T_{i}\) are available for experience learning. A complete list of notations is available at the beginning of Appendix.

### Overall Working Mechanism

As illustrated in Fig. 1, all modules covering the optimization of target task \(T_{*}\) are included in a grey block. The modules beyond the grey block are associated with related tasks \(T_{i}\) and experience learning, which distinguishes our FSEO framework from conventional SAEAs and BO. The MDKL surrogate modeling method consists of two procedures: meta-learning procedure and adaptation procedure. The former learns prior experience from \(T_{i}\), the latter uses experience to adapt surrogates to fit \(T_{*}\). The framework of FSEO is depicted in Alg. 1, it consists of the following major steps.

1. **Experience learning**: Before expensive optimization starts, a meta-learning procedure is conducted to train task-independent parameters \(\boldsymbol{\gamma}^{e}\) for MDKL surrogates (line 2). \(N_{m}\) datasets \(\{D_{m1},\ldots,D_{mN_{m}}\}\) collected from \(N\) related tasks \(\{T_{1},\ldots,T_{N}\}\) are used to train \(\boldsymbol{\gamma}^{e}\). \(\boldsymbol{\gamma}^{e}\) is the experience that represents the domain-specific features of related tasks.
2. **Initialize surrogates with experience**: Optimization starts when a target optimization task \(T_{*}\) is given. An initial dataset \(S_{*}\) is sampled (line 3) to adapt task-specific parameters \(\boldsymbol{\gamma}^{*}\) on the basis of experience \(\boldsymbol{\gamma}^{e}\). After that, MDKL surrogates are updated (line 4).

Figure 1: Diagram of our FSEO framework.

3. **Reproduction**: MDKL surrogates \(h(\bm{\gamma}^{*})\) are combined with a SAEA optimizer \(Opt\) to search for optimal solution(s) \(\textbf{x}^{*}\) on \(h(\bm{\gamma}^{*})\) (line 7). This is implemented by replacing the original (regression-based) surrogates in a SAEA with \(h(\bm{\gamma}^{*})\).
4. **Update archive and surrogates**: New optimal solution(s) \(\textbf{x}^{*}\) is evaluated on target task \(T_{*}\) (line 8). The evaluated solutions will be added to dataset \(S_{*}\) (line 9) which serves as an archive. Then, surrogate adaptation is triggered, surrogates \(h(\bm{\gamma}^{*})\) are updated (line 10).
5. **Stop criterion**: Once the evaluation budget has run out, the evolutionary optimization will be terminated and the optimal solutions in dataset \(S_{*}\) will be outputted. Otherwise, the algorithm goes back to step 3.

```
1:Input:\(D_{i}\): Datasets collected from related tasks \(T_{i}\), i=\(\{1,\ldots,N\}\); \(N_{m}\): Number of subsets \(D_{m}\) for meta-learning; \(|D_{m}|\): Size of subsets \(D_{m}\). \(|D_{m}|\leq|D_{i}|\) due to \(D_{m}\subseteq D_{i}\); Batch size \(B\); Surrogate learning rates \(\alpha,\beta\); Target task \(T_{*}\); A SAEA optimizer \(Opt\); Fitness evaluation budget \(FE_{max}\).
2: Experience \(\bm{\gamma}^{e}\leftarrow\) Meta-learning(\(D_{i},N_{m},|D_{m}|,B,\alpha\)). /*Alg. \(2.^{*}/\)
3:\(S_{*}\leftarrow\) Sampling \(1d\) solutions from \(T_{*}\).
4:\(h(\bm{\gamma}^{*})\leftarrow\) Adaptation(\(\bm{\gamma}^{e},S_{*},\beta\)). /*Initialize surrogate.\({}^{*}/\)
5: Set evaluation counter \(FE=|S_{*}|\).
6:while\(FE<FE_{max}\)do
7: Candidate solution(s) \(\textbf{x}^{*}\leftarrow\) Surrogate-assisted optimization (\(Opt,h(\bm{\gamma}^{*})\)).
8:\(f(\textbf{x}^{*})\leftarrow\) Evaluate \(\textbf{x}^{*}\) on \(T_{*}\).
9:\(S_{*}\gets S_{*}\cup\{(\textbf{x}^{*},f(\textbf{x}^{*}))\}\).
10:\(h(\bm{\gamma}^{*})\leftarrow\) Update(\(\bm{\gamma}^{*},S_{*},\beta\)). /*Alg. \(4.^{*}/\)
11: Update \(FE\).
12:endwhile
13:Output: Optimal solutions in \(S_{*}\). ```

**Algorithm 1** FSEO Framework.

### Learning and Using Experience by MDKL

In MDKL, the domain-specific features of related tasks are used as experience, which are represented by the task-independent parameters \(\bm{\gamma}^{e}\) learned across related tasks. To make MDKL more capable of expressing complex domain-specific features, the base kernel \(k(\textbf{x}^{i},\textbf{x}^{j}|\ \bm{\theta})\) in GP is combined with a neural network \(\phi(\textbf{w},\textbf{b})\) to construct a deep kernel (see Eq.(1)). The modeling of a MDKL model consists of two procedures: meta-learning procedure and adaptation procedure. To make a clear illustration, we introduce frameworks of two procedures and then explain them in detail.

**Meta-learning procedure: Learning experience**

Our MDKL model uses the kernel in [18] as its base kernel:

\[k(\textbf{x}^{i},\textbf{x}^{j}|\bm{\theta},\textbf{p})=exp(-\sum_{k=1}^{d} \theta_{k}|x_{k}^{i}-x_{k}^{j}|^{p_{k}}).\] (2)

Therefore, the deep kernel will be:

\[k(\textbf{x}^{i},\textbf{x}^{j}|\bm{\gamma})=exp(-\sum_{k=1}^{d}\theta_{k}| \phi(x_{k}^{i})-\phi(x_{k}^{j})|^{p_{k}}),\] (3)

where \(\bm{\gamma}=\{\textbf{w},\textbf{b},\bm{\theta},\textbf{p}\}\) is a set of deep kernel parameters. \(\phi,\textbf{w}\) and **b** are neural network and its parameters (see Eq.(1)). Details about alternative base kernels are available in [44].

The aim of meta-learning procedure is to learn experience \(\bm{\gamma}^{e}\) from related tasks \(\{T_{1},\ldots,T_{N}\}\), including neural network parameters \(\textbf{w},\textbf{b}\), and task-independent base kernel parameters \(\bm{\theta}^{e},\textbf{p}^{e}\). The pseudo-code of meta-learning procedure is given in Alg. 2. Ideally, the experience \(\bm{\gamma}^{e}\) is learned from plenty of (\(N_{m}\)) small datasets \(D_{m}\) collected from different related tasks. However, in practice, the number of available related tasks \(N\) may be much smaller than \(N_{m}\). Hence, the meta-learning is conducted gradually over \(U\) update iterations (line 3). During each update iteration, a small batch of \(\bm{\gamma}^{*}\) is the set of all the \(L(S_{*},h(\bm{\gamma}^{*}))\) variables.

```
1:Input:\(D_{i}\): Datasets collected from related tasks \(T_{i}\), i=\(\{1,\ldots,N\}\); \(N_{m}\): Number of subsets \(D_{m}\) for meta-learning; \(|D_{m}|\): Size of subsets \(D_{m}\). \(|D_{m}|\leq|D_{i}|\) due to \(D_{m}\subseteq D_{i}\); Batch size \(B\); Learning rate for priors \(\alpha\).
2:Randomly initialize \(\mathbf{w},\mathbf{b},\bm{\theta}^{e},\mathbf{p}^{e}\).
3:Set the number of update iterations U = \(N_{m}/B\).
4:for\(j=1\) to \(U\)do
5:\(\{D_{1}^{\prime},\ldots,D_{B}^{\prime}\}\leftarrow\) Randomly select a batch of datasets from \(\{D_{1},\ldots,D_{N}\}\).
6:for all\(D_{i}^{\prime}\) in the batch do
7:\(D_{mi}\leftarrow\) A subset of size \(|D_{m}|\) from \(D_{i}^{\prime}\).
8: Initialize task-specific increment \(\Delta\bm{\theta}^{i},\Delta\mathbf{p}^{i}\).
9: Compute task-specific parameters: \(\bm{\theta}^{i}=\bm{\theta}^{e}+\Delta\bm{\theta}^{i}\),\(\mathbf{p}^{i}=\mathbf{p}^{e}+\Delta\mathbf{p}^{i}\).
10: Obtain deep kernel \(k(\mathbf{x}^{i},\mathbf{x}^{j}|\bm{\gamma})\) based GP: \(h(\bm{\gamma})\), where \(\bm{\gamma}=\{\mathbf{w},\mathbf{b},\bm{\theta}^{i},\mathbf{p}^{i}\}\) (Eq.(3)).  Compute the loss function \(L(D_{mi},h(\bm{\gamma}))\) (Eq.(4)).
11:endfor
12: Update \(\mathbf{w},\mathbf{b},\bm{\theta}^{e},\mathbf{p}^{e}\) via gradient descent: \(\alpha\bigtriangledown L(D_{mi},h(\bm{\gamma}))\) (Eq.(5)).
13:endfor
14:endfor
15:Output: Task-independent parameters: \(\bm{\gamma}^{e}\) = \(\{\mathbf{w},\mathbf{b},\bm{\theta}^{e},\mathbf{p}^{e}\}\). ```

**Algorithm 2** Meta-learning(\(D_{i},N_{m},|D_{m}|,B,\alpha\))

Figure 2: Diagram of our deep kernel implementation. The solid lines depict the training process, the dotted lines depict the inference process. \(Q_{*}\) denotes query samples to be evaluated on our surrogates.

**Surrogate prediction.** Due to the nature of a GP, when predicting the fitness of a solution \(\textbf{x}^{*}\), a MDKL surrogate produces a predictive Gaussian distribution \(\mathcal{N}(\hat{y}(\textbf{x}^{*}),\hat{s}^{2}(\textbf{x}^{*}))\), the predicted mean \(\hat{y}(\textbf{x}^{*})\) and covariance \(\hat{s}^{2}(\textbf{x}^{*})\) are specified as [18]:

\[\hat{y}(\textbf{x}^{*})=\mu+\textbf{r}^{\prime}\textbf{R}^{-1}(\textbf{y}- \textbf{1}\mu),\] (6)

\[\hat{s}^{2}(\textbf{x}^{*})=\sigma^{2}(1-\textbf{r}^{\prime}\textbf{R}^{-1} \textbf{r}),\] (7)

where **r** is a correlation vector consisting of covariances between \(\textbf{x}^{*}\) and \(S_{*}\), other variables are explained in Eq.(4).

### Surrogate Update Strategy

In this subsection, we describe the update strategy in our FSEO framework. To properly integrate experience and data from \(T_{*}\), our update strategy is designed to determine whether a MDKL surrogate should be adapted in the current iteration or not, ensuring an optimal update frequency of surrogates.

As illustrated in Alg. 4, the surrogate update starts when a new optimal solution(s) has been evaluated on expensive functions and an updated archive \(S_{*}\) is available. For a given surrogate \(h(\boldsymbol{\gamma}^{*})\), its mean squared error (MSE) on \(S_{*}\) is selected as the update criterion: If the MSE after an adaptation \(e_{1}\) (line 4) is larger than the MSE without an adaptation \(e_{0}\) (line 2), then the surrogate will roll back to the status before the adaptation. This indicates the surrogate update has been refused and \(h(\boldsymbol{\gamma}^{*})\) will not be adapted in the current iteration. Otherwise, the adapted surrogate will be chosen (line 6). Note that no matter whether surrogate adaptations are accepted or refused, the resulting surrogates will be treated as updated surrogates, which are employed to assist the SAEA optimizer in the next iteration.

## 5 Computational Studies

Our computational studies can be divided into three parts: (1). Appendix D evaluates the effectiveness of learning experience through a synthetic problem and a real-world engine modeling problem. (2). Sections 5.1 to 5.2 use EMOPs as examples to investigate the performance of our FSEO framework in depth. Empirical evidence is provided to guide the use of our FSEO framework. (3). Section 5.3 investigates the performance of our FSEO framework on a real-world ECOP. The source code is available online1 For all meta-learning methods used in our experiments, their basic setups are listed in Table 1. The neural network structure is suggested by [10; 27], and the learning rates are the default values that have been widely used in many meta-learning methods [13; 27].

Footnote 1: A link will be disclosed here once the paper is accepted.

### Performance on EMOPs

In the following subsections, we aim to demonstrate the effectiveness of our FSEO framework. The experiment in this subsection is designed to answer the question below: With the experience learned from related tasks, can our FSEO framework helps a SAEA to save \(9d\) solutions without a loss of optimization performance?

The computational study is conducted on the DTLZ test problems [8]. All the DTLZ problems have \(d=10\) decision variables and 3 objectives, as the setups that have been widely used in [25; 33]. The details of generating DTLZ variants (related tasks) are provided in Appendix C. We test our FSEO framework using an instantiation on MOEA/D-EGO, resulting MOEA/D-FS. Details of the comparison algorithms are given in Appendix E.1.

#### 5.1.1 Experimental setups

The parameter setups for this multi-objective optimization experiment are listed in Table 2. During the optimization process, an initial dataset \(S_{*}\) is sampled using Latin-Hypercube Sampling (LHS) method [24], then extra evaluations are conducted until the evaluation budget has run out. Note that we aim to use related tasks to save \(9d\) evaluations without a loss of SAEA optimization performance. Hence, the total evaluation budgets for MOEA/D-FS and comparison algorithms are different.

Since the test problems have 3 objectives, we employ inverted generational distance plus (IGD+) [15] as our performance indicator, where smaller IGD+ values indicate better optimization results. 5000 reference points are generated for computing IGD+ values, as suggested in [25]. More results in IGD [4] and HV [55] metrics are reported in Appendix E.3.

#### 5.1.2 Results and analysis

The statistical test results are reported in Fig. 3 and Appendix E.2 (Table 5). It can be seen from Fig. 3 that, although 90 fewer evaluations are used in surrogate initialization, MOEA/D-FS can still achieve competitive or even smaller IGD+ values than MOEA/D-EGO on all DTLZ problems except for DTLZ7. In addition, the IGD+ values obtained by MOEA/D-FS drop rapidly, especially during the first few evaluations, implying the experience learned from DTLZ variants are effective. Therefore, in most situations, our FSEO framework is able to assist MOEA/D-EGO in reaching competitive or even better optimization results, with the number of evaluations used for surrogate initialization reduced from \(10d\) to only \(1d\).

MOEA/D-FS is less effective on DTLZ7 than on other DTLZ problems, which might be attributed to the discontinuity of the Pareto front on DTLZ7. Note that MOEA/D-FS learns experience from small datasets such as \(D_{m}\) and \(S_{*}\). The solutions in these small datasets are sampled at random, hence, the

\begin{table}
\begin{tabular}{l l l} \hline Module & Parameter & Value \\ \hline Meta-learning & Number of meta-learning datasets \(N_{m}\) & 20000 \\  & Number of update iterations \(U\) & 2000 \\  & Batch size \(B\) & 10 \\ \hline Neural network & Number of hidden layers & 2 \\  & Number of units in each hidden layer & 40 \\  & Learning rates \(\alpha,\beta\) & 0.001, 0.001 \\  & Activation function & ReLU \\ \hline \end{tabular}
\end{table}
Table 1: Parameter setups for meta-learning methods.

Figure 3: IGD+ curves averaged over 30 runs on the DTLZ problems. Solid lines are mean values, while shadows are error regions. **Upper**: DTLZ1, DTLZ2, DTLZ3, DTLZ4. **Lower**: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. X-axis denotes the extra 50 evaluations allowed in the further optimization. Note that ‘FS(out)’ indicates the target task is excluded from the range of related tasks during the meta-learning procedure) (see Appendix F).

\begin{table}
\begin{tabular}{l l l} \hline Parameter & MOEA/D-FS & Comparisons \\ \hline Number of related tasks \(N\) & 20000 (\(N_{m}\) in Table 1) & - \\ Size of datasets from real tasks \(|D_{m}|\) & 2000 (\(20.2\)) & - \\ Size of datasets for meta-learning \(|D_{m}|\) & \(|D_{m}|\) & - \\ \hline Evaluation for initialization & 10(\(1.4\)) & 100(\(1.4\)) \\ Evaluations for further optimization & 50 & 50 \\ Total evaluations & 60 & 150 \\ \hline \end{tabular}
\end{table}
Table 2: Parameter setups for DTLZ optimization.

probability of having optimal solutions being sampled is small. However, it is difficult to learn the discontinuity of the Pareto front from the sampled non-optimal solutions. As a result, the knowledge of 'there are four discrete optimal regions' cannot be learned from such small datasets (\(|D_{m}|=20\)) collected from related tasks. The performance analysis between MOEA/D-FS and other comparison algorithms are available in Appendix E.2.

#### 5.1.3 More comparison experiments

We also compared the performance of our FSEO framework when only 10 evaluations are used for surrogate initialization for comparison algorithms. The results are reported in Table 8 in Appendix E.4. In addition, the performance of our FSEO framework in the context of extremely expensive optimization has been investigated in Appendix H (Table 11 and Fig. 7).

The question raised at the beginning of this subsection can be answered by the results discussed so far. Due to the integration of the experience learned from related tasks (DTLZ variants), although the evaluation cost of surrogates initialization has been reduced from 10\(d\) to 1\(d\), our FSEO framework is still capable of assisting regression-based SAEAs to achieve competitive or even better optimization results in most situations.

### Ablation Studies

We conduct two ablation studies to investigate the influence of task similarity and that of the dataset size used in meta-learning, results and analysis are reported in Appendixes F and G, respectively.

### Performance on Real-World ECOPs

The experiments on EMOPs have investigated the performance of our FSEO framework in depth. In this subsection, we evaluate our FSEO framework on a real-world gasoline motor engine calibration problem, which is an ECOP.

The calibration problem has 6 adjustable engine parameters, namely the throttle angle, waste gate orifice, ignition timing, valve timings, state of injection, and air-fuel-ratio. The calibration aims at minimizing the BSFC while satisfying 4 constraints in terms of temperature, pressure, CA50, and load simultaneously [53].

#### 5.3.1 Comparison algorithms

Since the comparison algorithms in the DTLZ optimization experiments are not designed for handling constrained optimization, our comparison is conducted with 3 state-of-the-art constrained optimization algorithms used in industry: A variant of EGO designed to handle constrained optimization problems (cons_EGO) [53], a GA customized for this calibration problem (adaptiveGA) [53], and a bilevel constrained SAEA (SAB-DE) [50]. The settings of the comparison algorithms are the same as suggested in the literature. In this experiment, we apply our FSEO framework to cons_EGO and investigate its optimization performance. The GP surrogates in cons_EGO are replaced by our MDKL surrogates to conduct the comparison, and the resulting algorithm is denoted as cons_FS.

#### 5.3.2 Experimental setups

The setup of related tasks (\(N,D_{i}\)) is the same as described in Appendix D. In the meta-learning procedure, both the support set and the query set contain 6 data points, thus \(|D_{m}|=12\). The total evaluation budget for all algorithms is set to 60. For adaptiveGA, all evaluations are used in the optimization process as it is not a SAEA. For cons_EGO and SAB-DE, 40 samples are used to initialize the surrogates and 20 extra evaluations are used in the optimization process. For cons_FS, only 6 samples are used to initialize MDKL surrogates, and the remaining evaluations are used for further optimization.

#### 5.3.3 Optimization results and analysis

The left side and right side of Fig. 4 plot the normalized BSFC results and the number of feasible solutions found over the number of used evaluations, respectively. Solid lines are mean lines, while shadows are error regions. From the left side of Fig. 4, it can be observed that the minimal BSFCobtained by cons_FS decreases drastically in the first few evaluations, implying that the experience learned from related tasks is effective. In comparison, the minimal BSFC obtained by adaptiveGA and cons_EGO drops in a relatively slow rate, even though cons_EGO has used 34 more samples to initialize its surrogates. The star marker denotes the point at which cons_FS has evaluated 20 samples after surrogate initialization. It is worth noting that when 20 samples have been evaluated in the optimization, cons_FS achieves a smaller BSFC value than cons_EGO. After the star marker, the decrease of BSFC becomes slow as cons_FS has reached the optimal region. Therefore, further improvement in the normalized BSFC value is not significant and thus hard to be observed. The advantages of our FSEO framework can also be observed in constraint handling. In the right side of Fig. 4, cons_FS finds more feasible solutions than the 3 comparison algorithms. These results indicate that our FSEO framework improves the performance of cons_EGO on both objective function and constraint functions. Meanwhile, only \(1d\) evaluations are used to initialize surrogates.

#### 5.3.4 Discussion on runtime

It should be noted that real engine performance evaluations on engine facilities are very costly in terms of both time and financial budget [49]. Since a single real engine performance evaluation can cost several hours [22; 49], the time cost of the meta-learning procedure is negligible as it takes only a few minutes. Savings from reduced real engine performance evaluations on engine facilities and the reduced development cycle due to our FSEO framework could amount to millions of dollars [49]. our FSEO framework is an effective and efficient method to solve this real-world calibration problem.

## 6 Conclusion and further work

**Conclusion.** In this paper, we present a FSEO framework to address EMOPs and ECOPs from the perspective of SAEAs. A novel meta-learning approach MDKL is proposed to learn prior experience from related expensive tasks. Our MDKL model is designed for optimization and has explicit task-specific parameters, which allows continually update of task-specific parameters during the optimization process. Our empirical experiments show that the FSEO framework is able to improve the sampling efficiency and thus save expensive evaluations for existing regression-based SAEAs. Ablation studies reveal the influence between optimization performance and solutions similarity as well as the size of datasets for meta-learning.

**Limitation and further work.** The limitations of this work can be summarized as the following two points: First, we do not have a mathematical definition of related tasks. Second, the proposed framework is currently for regression-based SAEAs only.

Figure 4: Results of 30 runs on the real-world engine calibration problem, all BSFC values are normalized. Solid lines are mean values, while shadows are error regions. Left figure shows how BSFC varies with the number of evaluations. The star markers highlight the results achieved when 20 evaluations are used in the optimization process. Right figure illustrates how the number of feasible solutions varies with the number of evaluations.

## References

* Bai et al. [2023] Tianyi Bai, Yang Li, Yu Shen, Xinyi Zhang, Wentao Zhang, and Bin Cui. Transfer learning for bayesian optimization: A survey. _arXiv preprint arXiv:2302.05927_, 2023.
* Bali et al. [2019] Kavitesh Kumar Bali, Yew-Soon Ong, Abhishek Gupta, and Puay Siew Tan. Multifactorial evolutionary algorithm with online transfer parameter estimation: MFEA-II. _IEEE Transactions on Evolutionary Computation_, 24(1):69-83, 2019.
* Bian et al. [2023] Hongli Bian, Jie Tian, Jialiang Yu, and Han Yu. Bayesian co-evolutionary optimization based entropy search for high-dimensional many-objective optimization. _Knowledge-Based Systems_, 274:110630, 2023.
* Bosman and Thierens [2003] Peter AN Bosman and Dirk Thierens. The balance between proximity and diversity in multiobjective evolutionary algorithms. _IEEE Transactions on Evolutionary Computation_, 7(2):174-188, 2003.
* Chen et al. [2019] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classification. In _Proceedings of the 7th International Conference on Learning Representations (ICLR'19)_, 2019.
* Chen et al. [2023] Wenlin Chen, Austin Tripp, and Jose Miguel Hernandez-Lobato. Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. In _Proceedings of the 11th International Conference on Learning Representations (ICLR'23)_, 2023.
* Chugh et al. [2016] Tinkle Chugh, Yaochu Jin, Kaisa Miettinen, Jussi Hakanen, and Karthik Sindhya. A surrogate-assisted reference vector guided evolutionary algorithm for computationally expensive many-objective optimization. _IEEE Transactions on Evolutionary Computation_, 22(1):129-142, 2016.
* Deb et al. [2005] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems for evolutionary multiobjective optimization. In _Evolutionary Multiobjective Optimization_, pages 105-145. Springer, London, U.K., 2005.
* Ding et al. [2017] Jinliang Ding, Cuie Yang, Yaochu Jin, and Tianyou Chai. Generalized multitasking for evolutionary optimization of expensive problems. _IEEE Transactions on Evolutionary Computation_, 23(1):44-58, 2017.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _Proceedings of the 34th International Conference on Machine Learning (ICML'17)_, pages 1126-1135, 2017.
* Guo et al. [2022] Zhendong Guo, Haitao Liu, Yew-Soon Ong, Xinghua Qu, Yuzhe Zhang, and Jianmin Zheng. Generative multiform Bayesian optimization. _IEEE Transactions on Cybernetics_, 53(7):4347-4360, 2022.
* Gupta et al. [2017] Abhishek Gupta, Yew-Soon Ong, and Liang Feng. Insights on transfer optimization: Because experience is the best teacher. _IEEE Transactions on Emerging Topics in Computational Intelligence_, 2(1):51-64, 2017.
* Harrison et al. [2018] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online Bayesian regression. In _Proceedings of the 13th Workshop on the Algorithmic Foundations of Robotics (WAFR'18)_, pages 318-337, 2018.
* Hospedales et al. [2021] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. Meta-learning in neural networks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* Ishibuchi et al. [2015] Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance calculation in generational distance and inverted generational distance. In _Proceedings of the 8th International Conference on Evolutionary Multi-criterion Optimization (EMO'15)_, pages 110-125, 2015.
* Jiang et al. [2020] Min Jiang, Zhenzhong Wang, Shihui Guo, Xing Gao, and Kay Chen Tan. Individual-based transfer learning for dynamic multiobjective optimization. _IEEE Transactions on Cybernetics_, 51(10):4968-4981, 2020.

* [17] Min Jiang, Zhenzhong Wang, Liming Qiu, Shihui Guo, Xing Gao, and Kay Chen Tan. A fast dynamic evolutionary multiobjective algorithm via manifold transfer learning. _IEEE Transactions on Cybernetics_, 51(7):3417-3428, 2020.
* [18] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of expensive black-box functions. _Journal of Global Optimization_, 13(4):455-492, 1998.
* [19] Joshua Knowles. ParEGO: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. _IEEE Transactions on Evolutionary Computation_, 10(1):50-66, 2006.
* [20] Rung-Tzuo Liaw and Chuan-Kang Ting. Evolutionary manytasking optimization based on symbiosis in biocoenosis. In _Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI'19)_, pages 4295-4303, 2019.
* [21] Shengcai Liu, Ke Tang, and Xin Yao. Experience-based optimization: A coevolutionary approach. _arXiv preprint arXiv:1703.09865_, 2017.
* [22] He Ma. _Control Oriented Engine Modeling and Engine Multi-objective Optimal Feedback Control_. PhD thesis, University of Birmingham, 2013.
* [23] Alexandre Maraval, Matthieu Zimmer, Antoine Grosnit, and Haitham Bou Ammar. End-to-end meta-bayesian optimisation with transformer neural processes. _arXiv preprint arXiv:2305.15930_, 2023.
* [24] Michael D. McKay, Richard J. Beckman, and William J. Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. _Technometrics_, 42(1):55-61, 2000.
* [25] Linqiang Pan, Cheng He, Ye Tian, Handing Wang, Xingyi Zhang, and Yaochu Jin. A classification-based surrogate-assisted evolutionary algorithm for expensive many-objective optimization. _IEEE Transactions on Evolutionary Computation_, 23(1):74-88, 2018.
* [26] Jiarong Pan, Stefan Falkner, Felix Berkenkamp, and Joaquin Vanschoren. MALIBO: Meta-learning for likelihood-free bayesian optimization. _arXiv preprint arXiv:2307.03565_, 2023.
* [27] Massimiliano Patacchiola, Jack Turner, Elliot J Crowley, Michael O'Boyle, and Amos Storkey. Bayesian meta-learning for the few-shot setting via deep kernels. In _Advance in Neural Information Processing Systems 33 (NeurIPS'20)_, 2020.
* [28] Shufen Qin, Chaoli Sun, Farooq Akhtar, and Gang Xie. Expensive many-objective evolutionary optimization guided by two individual infill criteria. _Memetic Computing_, pages 1-15, 2023.
* [29] Gan Ruan, Leandro L. Minku, Stefan Menzel, Bernhard Sendhoff, and Xin Yao. When and how to transfer knowledge in dynamic multi-objective optimization. In _Proceedings of the 2019 IEEE Symposium Series on Computational Intelligence (SSCI'19)_, pages 2034-2041, 2019.
* [30] Gan Ruan, Leandro L. Minku, Stefan Menzel, Bernhard Sendhoff, and Xin Yao. Computational study on effectiveness of knowledge transfer in dynamic multi-objective optimization. In _Proceedings of the 22nd IEEE Congress on Evolutionary Computation (CEC'20)_, pages 1-8, 2020.
* [31] Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and analysis of computer experiments. _Statistical Science_, 4(4):409-423, 1989.
* [32] Gresa Shala, Thomas Elsken, Frank Hutter, and Josif Grabocka. Transfer NAS with meta-learned bayesian surrogates. In _Proceedings of the 11th International Conference on Learning Representations (ICLR '23)_, 2023.
* [33] Zhenshou Song, Handling Wang, Cheng He, and Yaochu Jin. A Kriging-assisted two-archive evolutionary algorithm for expensive many-objective optimization. _IEEE Transactions on Evolutionary Computation_, 25(6):1013-1027, 2021.
* [34] Michael L. Stein. _Interpolation of Spatial Data: Some Theory for Kriging_. Springer Science & Business Media, New York, NY, 1999.

* [35] Kay Chen Tan, Liang Feng, and Min Jiang. Evolutionary transfer optimization-a new frontier in evolutionary computation research. _IEEE Computational Intelligence Magazine_, 16(1):22-33, 2021.
* [36] Ke Tang, Shengcai Liu, Peng Yang, and Xin Yao. Few-shots parallel algorithm portfolio construction via co-evolution. _IEEE Transactions on Evolutionary Computation_, 25(3):595-607, 2021.
* [37] Ye Tian, Ran Cheng, Xingyi Zhang, and Yaochu Jin. PlatEMO: A MATLAB platform for evolutionary multi-objective optimization [educational forum]. _IEEE Computational Intelligence Magazine_, 12(4):73-87, 2017.
* [38] Petru Tighineanu, Lukas Grossberger, Paul Baireuther, Kathrin Skubch, Stefan Falkner, Julia Vinogradska, and Felix Berkenkamp. Scalable meta-learning with gaussian processes. _arXiv preprint arXiv:2312.00742_, 2023.
* [39] Prudencio Tossou, Basile Dura, Francois Laviolette, Mario Marchand, and Alexandre Lacoste. Adaptive deep kernel learning. _arXiv preprint arXiv:1905.12131_, 2019.
* [40] Michael Volpp, Lukas P. Frohlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, and Christian Daniel. Meta-learning acquisition functions for transfer learning in Bayesian optimization. In _Proceedings of the 8th International Conference on Learning Representations (ICLR'20)_, 2020.
* [41] Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. Generalizing from a few examples: A survey on few-shot learning. _ACM Computing Surveys_, 53(3):1-34, 2020.
* [42] Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zachary Nado, Justin Gilmer, Jasper Snoek, and Zoubin Ghahramani. Pre-trained gaussian processes for bayesian optimization. _arXiv preprint arXiv:2109.08215_, 2021.
* [43] Tingyang Wei, Shibin Wang, Jinghui Zhong, Dong Liu, and Jun Zhang. A review on evolutionary multi-task optimization: Trends and challenges. _IEEE Transactions on Evolutionary Computation_, 26(5):941-960, 2021.
* [44] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian Processes for Machine Learning_. MIT press, Cambridge, MA, 2006.
* [45] Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. In _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS'16)_, pages 370-378, 2016.
* [46] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surrogates. In _Proceedings of the 9th International Conference on Learning Representations (ICLR'21)_, 2021.
* [47] Xiaoming Xue, Kai Zhang, Kay Chen Tan, Liang Feng, Jian Wang, Guodong Chen, Xinggang Zhao, Liming Zhang, and Jun Yao. Affine transformation-enhanced multifactorial optimization for heterogeneous problems. _IEEE Transactions on Cybernetics_, pages 1-15, 2020.
* [48] Xunzhao Yu, Xin Yao, Yan Wang, Ling Zhu, and Dimitar Filev. Domination-based ordinal regression for expensive multi-objective optimization. In _Proceedings of the 2019 IEEE Symposium Series on Computational Intelligence (SSCI'19)_, pages 2058-2065, 2019.
* [49] Xunzhao Yu, Ling Zhu, Yan Wang, Dimitar Filev, and Xin Yao. Internal combustion engine calibration using optimization algorithms. _Applied Energy_, 305:117894, 2022.
* [50] Xunzhao Yu, Yan Wang, Ling Zhu, Dimitar Filev, and Xin Yao. Engine calibration with surrogate-assisted bilevel evolutionary algorithm. _IEEE Transactions on Cybernetics (Early Access)_, 2023.
* [51] Qingfu Zhang, Wudong Liu, Edward Tsang, and Botond Virginas. Expensive multiobjective optimization by MOEA/D with gaussian process model. _IEEE Transactions on Evolutionary Computation_, 14(3):456-474, 2010.

* [52] Liangjie Zhang, Yuling Xie, Jianjun Chen, Liang Feng, Chao Chen, and Kai Liu. A study on multiform multi-objective evolutionary optimization. _Memetic Computing_, 13(3):307-318, 2021.
* [53] Ling Zhu, Yan Wang, Anuj Pal, and Guoming Zhu. Engine calibration using global optimization methods with customization. Technical Report 2020-01-0270, SAE Technical Paper, 2020.
* [54] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. _Proceedings of the IEEE_, 109(1):43-76, 2020.
* a comparative case study. In _Proceedings of the 5th International Conference on Parallel Problem Solving from Nature (PPSN V)_, pages 292-301, 1998.
Discussion on Experience-based Optimization

In the past decade, experience-based optimization has attracted much attention as it uses the experience gained from other optimization problems to improve the optimization efficiency of target problems, which mimics human capabilities of cognitive and knowledge generalization [12]. The optimization problems that provide experience or knowledge are regarded as source tasks, while the target optimization problems are regarded as target tasks. To obtain useful experience, the tasks that are related to target tasks are chosen as source tasks since they usually share domain-specific features with target tasks. Diverse experience-based optimization methods have been proposed to use the experience gained from related tasks to tackle target tasks. They can be divided into two categories based on the direction of experience transformation.

In the first category, experience is transformed mutually. Every considered optimization problem is a target task and also is one of the source tasks of other optimization problems. In other words, the roles of source task and target task are compatible. One representative tributary is EMTO that aims to solve multiple optimization tasks concurrently [9, 43, 20, 2, 47]. In EMTO, experience is learned, updated, and spontaneously shared among target tasks through multi-task learning techniques. A variant of EMTO is multiforms optimization [12, 52, 11]. In multiforms optimization, multi-task learning methods are employed to learn experience from distinct formulations of a single target task.

In the second category, experience is transformed unidirectionally. The roles of source task and target task are not compatible, an optimization problem cannot serve as a source task and a target task simultaneously. One popular tributary is transfer optimization which employs transfer learning techniques to transform experience from source tasks to target tasks [35, 17, 16, 40]. In transfer learning, experience can be transformed from a single source task, multiple source tasks, or even source tasks from a different domain [54]. However, these transfer learning techniques pay more attention to experience transformation instead of experience learning. Despite diverse and complex situations of experience transformation have been studied [29, 30], the difficult of learning experience from small (expensive) source tasks has not been well studied. Actually, a common scenario in transfer learning is that the source task(s) is/are large enough such that useful experience can be obtained easily through solving source task(s) [54]. In contrast to transfer optimization, recently, some experience-based optimization algorithms attempted to use meta-learning methods to learn experience from small source tasks, which are known as few-shot optimization (FSO)[46]. Since meta-learning only works for related tasks in the same domain, the situations of experience transformation are less complex than that of transfer learning. As a result, meta-learning pays more attention to experience learning instead of experience transformation. Domain-specific features are extracted as experience and no related task needs to be solved.

Our work belongs to the FSO in the second category discussed above since our experience is transformed unidirectionally. More importantly, our experience is learned across many related expensive tasks, rather than gained through solving more or less source tasks. Therefore, our work is different from transfer optimization.

## Appendix B Discussion on Framework Compatibility and Limitation

Our FSEO framework is applicable to regression-based SAEAs as our MDKL surrogates can be embedded in these SAEAs directly. Classification-based SAEAs are not compatible with our FSEO framework. The classification surrogates in these SAEAs are employed to learn the relation between pairs of solutions, or the relation between solutions and a set of reference solutions. The class labels used for surrogate training can be fluctuating during the optimization and thus hard to be learned over related tasks. Similarly, in ordinal-regression-based SAEAs, the ordinal relation values to be learned are not as stable as the fitness of expensive functions. So ordinal-regression-based SAEAs are also not compatible with our FSEO framework. In this paper, we focus on FSO for regression-based SAEAs, while other SAEA categories are left to be discussed in future work.

## Appendix C Generation of DTLZ variants

Our DTLZ optimization experiments generate \(m\)-objective DTLZ variants in the following ways:

**DTLZ1**:

\[f_{1}=(a_{1}+g)0.5\prod_{i=1}^{m-1}x_{i},\] (8)

\[f_{j=2:m-1}=(a_{j}+g)(0.5\prod_{i=1}^{m-j}x_{i})(1-x_{m-j+1}),\] (9)

\[f_{m}=(a_{m}+g)0.5(1-x_{1}),\] (10)

\[g=100\left(k+\sum_{i=1}^{k}\left((z_{i}-0.5)^{2}-cos\left(20\pi(z_{i}-0.5) \right)\right)\right),\] (11)

where \(\mathbf{z}\) is a vector consisting of the last \(k=d-m+1\) variables in \(\mathbf{x}\). In other words, \(\mathbf{z}=\{z_{1},\ldots,z_{k}\}=\{x_{m},\ldots,x_{d}\}\). The variants of DTLZ1 introduce only one variable \(\mathbf{a}\in[0.1,5.0]^{m}\) in Eq.(8), Eq.(9), and Eq.(10), where \(\mathbf{a}=\mathbf{1}\) in the original DTLZ1. For out-of-range test, \(\mathbf{a}\in[1.5,5.0]^{m}\).

**DTLZ2**:

\[f_{1}=(a_{1}+g)\prod_{i=1}^{m-1}cos(\frac{x_{i}\pi}{b_{1}}),\] (12)

\[f_{j=2:m-1}=(a_{j}+g)\left(\prod_{i=1}^{m-j}cos(\frac{x_{i}\pi}{b_{j}})\right) sin(\frac{x_{m-j+1}\pi}{b_{j}}),\] (13)

\[f_{m}=(a_{m}+g)sin(\frac{x_{1}\pi}{b_{m}}),\] (14)

\[g=\sum_{i=1}^{k}(z_{i}-0.5)^{2}.\] (15)

The variants of DTLZ2 introduce two variables \(\mathbf{a}\in[0.1,5.0]^{m}\) and \(\mathbf{b}\in[0.5,2.0]^{m}\) in Eq.(12), Eq.(13), and Eq.(14), where \(\mathbf{a}=\mathbf{1}\) and \(\mathbf{b}=\mathbf{2}\) in the original DTLZ2. For out-of-range test, \(\mathbf{a}\in[1.5,5.0]^{m},\mathbf{b}\in[0.5,1.5]^{m}\).

**DTLZ3**: The variants of DTLZ3 are generated using the same way as described in DTLZ2, except the equation \(g\) from Eq.(15) is replaced by the one from Eq.(11).

**DTLZ4**: The variants of DTLZ4 are generated using the same way as described in DTLZ2, except all \(x_{i}\) are replaced by \(x_{i}^{100}\).

**DTLZ5**: The variants of DTLZ5 are generated using the same way as described in DTLZ2, except all \(x_{2},\ldots,x_{m-1}\) are replaced by \(\frac{1+2gx_{i}}{2(1+g)}\).

**DTLZ6**:

\[g=\sum_{i=1}^{k}z_{i}^{0.1}.\] (16)

The variants of DTLZ6 are generated using the same way as described in DTLZ5, except the equation \(g\) from Eq.(15) is replaced by the one from Eq.(16).

**DTLZ7**:

\[f_{j=1:m-1}=x_{j}+a_{j},\] (17)\[f_{m}=(1+g)\left(m-\sum_{i=1}^{m-1}\left(\frac{f_{i}}{1+g}\left(1+sin(3\pi f_{i}) \right)\right)\right),\] (18)

\[g=a_{m}+9\sum_{i=1}^{k}\frac{z_{i}}{k}.\] (19)

The variants of DTLZ7 introduce one variable \(\mathbf{a}\in[0.1,5.0]^{m}\) in Eq.(17) and Eq.(19), where \(a_{j=1:m-1}=0\) and \(a_{m}=1\) in the original DTLZ7. For out-of-range test, \(\mathbf{a}\in[1.5,5.0]^{m}\).

## Appendix D Effectiveness of Learning Experience

Evaluating the effectiveness of learning experiences aims to demonstrate that our MDKL model can learn experience from related tasks and outperforms other meta-learning models. For this reason, the experiment is designed to answer the following questions:

* Given a small dataset \(S_{*}\) from target task \(T_{*}\), can MDKL learn experience from related tasks and then generate a model that has the smallest MSE?
* If yes, which components of MDKL contribute to the effectiveness of learning experience? Meta-learning or/and deep kernel learning? If not, why not?

To answer the two questions above, we consider two experiments to evaluate the effectiveness of learning experience: amplitude prediction for unknown periodic sinusoid functions, and fuel consumption prediction for a gasoline motor engine. The former is a few-shot regression problem that motivates many meta-learning studies [10, 13, 39, 27], while the latter is a real-world regression problem [53].

### Effectiveness of Learning Experience: Sinusoid Function Regression

In the sinusoid regression experiment, we learn experience from a series of 1-dimensional sinusoid functions:

\[y=Asin(wx+b)+\epsilon,\] (20)

where the amplitude \(A\) and phase \(w\) of sine waves are varied between functions. The target is to approximate an unknown sinusoid function with a small support dataset \(S_{*}\) and the learned experience. Clearly, by integrating experience with \(S_{*}\), we estimate parameters \((A,w,b)\) for an unknown sinusoid function. As a result, the output \(y\) of the given sinusoid function can be predicted once a query data \(x\) is inputted.

#### d.1.1 Generation of Sinusoid Function Variants

As suggested in [10, 13], we set amplitude \(A\in\) [0.1, 5.0], frequency \(w\in[0.999,1.0]\), phase \(b\in\) [0, \(\pi\)], and Gaussian noise \(\epsilon\sim(0,0.1)\). Therefore, a sinusoid function can be generated by sampling three parameters \((A,w,b)\) from their ranges uniformly. In total, \(N_{m}=N=20000\) related sinusoid functions are generated at random.

#### d.1.2 Experimental Setups

All data points \(x\) are sampled from the range \(\in\) [-5.0, 5.0]. In the meta-learning procedure, both support set and query set contain 5 data points. Hence, a dataset \(D_{i}\) is sampled from each (related) sinusoid function \(T_{i}\), and \(|D_{i}|=|D_{m}|=10\). Six experiments are conducted where \(|S_{*}|=\{2,3,5,10,20,30\}\) data points are sampled from the target function. Considering Gaussian noise \(\epsilon\) could be relatively large when amplitude \(A\) is close to 0.1, normalized mean squared error (NMSE) is chosen as a performance indicator. NMSE is measured using a dataset that contains 100 data points sampled uniformly from the \(x\) range.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

compared SAEAs when only 1\(d\) evaluations are used to initialize their surrogates. The effectiveness of our FSEO framework has been demonstrated. Note that OREA is an evolutionary algorithm assisted by ordinal-regression-based surrogates. Currently, our FSEO framework is applicable to the SAEAs working with regression-based surrogates. The meta-learning of ordinal-regression models can be a topic of further research.

## Appendix F Influence of Task Similarity

In real-world applications, it is optimistic to assume that some related tasks are very similar to the target task. A more common situation is that all related tasks have limited similarity to the target task. To investigate the relationship between task similarity and FSEO optimization performance, we also test the performance in an 'out-of-range' situation, where the original DTLZ is excluded from the range of DTLZ variants during the MDKL meta-learning procedure. As a result, only the DTLZ variants that are quite different from the original DTLZ problem can be used to learn experience. The 'out-of-range' situation eliminates the probability that MDKL surrogates benefit greatly from the

\begin{table}
\begin{tabular}{l c c c} \hline Problem & MOEA/D-FS & ParEGO & OREA \\ \hline DTLZ1 & 9.70e+1(1.87e+1) & 6.70e+1(4.75e+0)\(-\) & 1.10e+2(3.65e+1)\(\approx\) \\ DTLZ2 & 1.43e-1(2.29e-2) & 5.51e-1(5.37e-2)\(+\) & 4.28e-1(6.68e-2)\(+\) \\ DTLZ3 & 1.97e+2 (1.64e+1) & 1.84e+2(8.86e+0)\(\approx\) & 2.72e+2(6.59e+1)\(+\) \\ DTLZ4 & 4.44e-1(1.35e-1) & 6.29e-1(7.99e-2)\(+\) & 6.45e-1(1.24e-1)\(+\) \\ DTLZ5 & 1.13e-1(2.24e-2) & 4.32e-1(8.88e-2)\(+\) & 3.02e-1(7.63e-2)\(+\) \\ DTLZ6 & 1.11e+0(5.71e-1) & 1.03e+0(4.78e-1)\(\approx\) & 5.71e+0(6.73e-1)\(+\) \\ DTLZ7 & 2.47e+0(1.89e+0) & 4.38e-1(1.39e-1)\(-\) & 7.12e+0(1.77e+0)\(+\) \\ \hline \(+/\approx/-\) & -/-/- & 3/2/2 & 6/1/0 \\ \hline \end{tabular}
\end{table}
Table 8: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. MOEA/D-FS is compared with ParEGO and OREA under the same evaluation budget: 10 evaluations for surrogate initialization and 50 evaluations for the optimization process. ‘\(+\)’, ‘\(\approx\)’, and ‘\(-\)’ denote MOEA/D-FS is statistically significantly superior to, almost equivalent to, and inferior to the compared two algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results.

Figure 6: HV curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. **Upper**: DTLZ1, DTLZ2, DTLZ3, DTLZ4. **Lower**: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 100 samples, respectively. Extra 50 evaluations are allowed in the further optimization. Note that ‘FS(out)’ indicates the target task is excluded from the range of related tasks during the meta-learning procedure). X-axis denotes the number of evaluations used after the surrogate initialization.

DTLZ variants that are very similar to the original DTLZ problem. Detailed definitions of the related tasks used in the 'out-of-range' situation are given in Appendix C. Apart from the related tasks used, the remaining experimental setups are the same as the setups described in Section 5.1. For the sake of convenience, we denote the situation we tested in Section 5.1 as 'in-range' below.

The statistical test results reported in Table 9 show that the 'out-of-range' situation achieves competitive IGD+ values to the 'in-range' situation on all 7 test instances. This suggests that related tasks that are very similar to the target task have a limited impact on the optimization performance of our FSEO framework. Useful experience can be learned from related tasks that are not very similar to the target task. Crucially, when comparing the performance of the 'out-of-range' situation and that of MOEA/D-EGO, we can still observe competitive or improved optimization results on 6 DTLZ problems (see Table 9, the row titled by 'vs MOEA/D-EGO', or Fig. 3). Moreover, it can be seen from the last row of Table 9 that the 'out-of-range' situation achieves better/competitive/worse IGD+ values than all compared SAEAs on 27/7/8 test instances. In comparison, the corresponding statistical test results for the 'in-range' situation are 26/9/7. The difference between these statistical test results is not significant.

A study on the 'out-of-range' situation in the context of extremely expensive multi-objective optimization is presented in Appendix H.2. Consistent results can be observed from Table 12 and Fig. 7.

Consequently, related tasks that are very similar to the target task are not essential to the optimization performance of our FSEO framework. In the 'out-of-range' situation, our MOEA/D-FS can still achieve competitive or better optimization results than MOEA/D-EGO while using only 1\(d\) samples for surrogate initialization.

## Appendix G Influence of the Size of Datasets Used in Meta-Learning

We also investigated the performance of our FSEO framework when different sizes of datasets \(|D_{m}|\) are used in the meta-learning procedure. The experimental setups are the same as the setups of MOEA/D-FS in Section 5.1 except for \(|D_{m}|\).

It is evident from Table 10 that when each DTLZ variant provides \(|D_{m}|=60\) samples for the meta-learning of MDKL surrogates, the performance of both MOEA/D-FSs are improved on 2 or 3 DTLZ problems. Particularly, a significant improvement can be observed from the optimization results of DTLZ7. As we discussed in Section 5.1, the poor performance of our experience-based optimization on DTLZ7 is caused by the small size of \(D_{m}\). Optimal solutions have few chances to be included in a small \(D_{m}\), which makes \(D_{m}\) fails to provide the experience about the discontinuity of optimal regions. In comparison, the experience of 'optimal regions' can be learned from large datasets \(D_{m}\) and thus the optimization results are improved significantly.

In conclusion, for our FSEO framework, a large \(D_{m}\) for the meta-learning procedure indicates more useful experience can be learned from related tasks, which further improves the performance of experience-based optimization. Therefore, when applying our FSEO framework to real-world

\begin{table}
\begin{tabular}{l c c} \hline MOEA/D-FSs & In-range & Out-of-range \\ \hline DTLZ1 & 9.70e+1(1.87e+1)\(\approx\) & 9.11e+1(1.53e+1) \\ DTLZ2 & 1.43e-1(2.29e-2)\(\approx\) & 1.41e-1(1.75e-2) \\ DTLZ3 & 1.97e+2 (1.64e+1)\(\approx\) & 1.98e+1(1.51e+1) \\ DTLZ4 & 4.44e-1(1.35e-1)\(\approx\) & 4.96e-1(8.63e-2) \\ DTL25 & 1.13e-1(2.24e-2)\(\approx\) & 1.03e-1(2.39e-2) \\ DTLZ6 & 1.11e+0(5.71e-1)\(\approx\) & 1.17e+0(6.88e-1) \\ DTLZ7 & 2.47e+0(1.89e+0)\(\approx\) & 2.86e+0(1.87e+0) \\ \hline \(+/\approx/-\) & 0/7/0 & -/-/- \\ \hline vs MOEA/D-EGO & 4/2/1 & 4/2/1 \\ vs 6 Comparisons & 26/9/7 & 27/7/8 \\ \hline \end{tabular}
\end{table}
Table 9: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. Both MOEA/D-FSs initialize their surrogates with 10 samples, extra 50 evaluations are allowed in the further optimization. The last two rows count the statistical test results between MOEA/D-FSs and other compared algorithms.

optimization problems, it is preferable to collect more data from related tasks for experience learning.

## Appendix H Experiments on Extremely Expensive Multi-Objective Optimization

In this section, we investigate the performance of our FSEO framework in the context of extremely expensive optimization, where the allowed fitness evaluations on target problems are fewer than that in the experiment carried out in Sections 5.1 of the main file and Appendix F.

### Performance between Comparison Algorithms

We conduct the experiment described in Section 5.1 of the main file, but with a smaller evaluation budget than the budget listed in Table 2. The size of the initial dataset \(S_{*}\) is set to 10, 60 for our MOEA/D-FS and comparison algorithms, respectively. 30 extra evaluations for further optimization are allowed. The total evaluation budget is 40, 90 for our MOEA/D-FS and comparison algorithms, respectively.

The aim of this subsection is to answer the question below:

* Is our FSEO framework more suitable for the optimization problems in which evaluations are extremely expensive? In other words, will the advantage of our FSEO framework become more prominent if the optimization problems allow a smaller evaluation budget?

The comparison results reported in Fig. 7 and Table 11 show that MOEA/D-FS has achieved competitive or smaller IGD+ values than MOEA/D-EGO on all DTLZ problems except for DTLZ7. Meanwhile, 5\(d\) evaluations have been saved.

Consistent with the results discussed in Section 5.1 of the main file, MOEA/D-FS fails to achieve a competitive result compared to MOEA/D-EGO on DTLZ7 since experience is learned from small datasets collected from related tasks. Although we set a different evaluation budget for all SAEAs, the size of datasets for meta-learning \(|D_{m}|\) has not been modified. However, it can be observed from the statistical test results (see the last row of Tables 5 and 11) that our MOEA/D-FS outperforms the comparison algorithms on 26, 29 test instances when the total evaluation budget of comparison

\begin{table}
\begin{tabular}{l c c c} \hline \multicolumn{2}{c}{In-range} & \multicolumn{2}{c}{Out-of-range} \\  & \(|D_{m}|\)=20 & \(|D_{m}|\)=60 & \(|D_{m}|\)=20 & \(|D_{m}|\)=60 \\ \hline DTLZ1 & 9.70\(\pm\)1(1.87e+1)= & 9.77e+1(1.73e+1) & 9.11e+1(1.53e+1)\% & 9.93e+1(1.87e+1) \\ DTLZ2 & 1.43e+(2.29e-2) & 1.24e+(2.11e-2) & 1.41e+(1.75e-2)\% & 1.29e+2(1.36e-2) \\ DTLZ3 & 1.97e+2(1.64e+1)\% & 1.98e+2(2.12e+1) & 1.98e+1(1.51e+1)\% & 1.93e+2(1.19e+1) \\ DTLZ4 & 4.44e-1(1.35e-1)\% & 5.17e+1(5.68e-2) & 4.96e-1(8.63e-2)\% & 5.15e-1(5.38e-2) \\ DTLZ5 & 1.13e-(1.24e-2)\% & 9.96e-2(2.18e-2) & 9.96e-2(1.39e-2)\% & 1.05e+(2.39e-2)\% \\ DTLZ6 & 1.11e+(0.57e-1)\% & 1.04e+(0.66e-1) & 1.17e+(0.68e-1)\% & 1.22e+0(0.64e-1) \\ DTLZ7 & 2.47e+0(1.89e+0)+\% & 7.49e-1(2.61e-1) & 2.86e+0(1.87e+0)+ & 6.96e-1(2.41e-1) \\ \hline \(+/\approx/-\) & 3/40 & -/-/-/- & 2/50 & -/-/- \\ \hline \end{tabular}
\end{table}
Table 10: Mean IGD+ values and standard deviation (in parentheses) obtained from 30 runs on DTLZ problems. 10 samples are used for initialization and extra 50 evaluations are allowed in the further optimization. \(|D_{m}|\) is the size of the dataset collected from each related task.

\begin{table}
\begin{tabular}{l c c|c c c c c c c c} \hline \multicolumn{2}{c|}{Fashion} & MOEA/D-FS (ours) & FSEO & MOEA/D-EGO & FSEO & FSEO & FSEO & FSEO & FSEO & FSEO \\ \hline DTLZ1 & 1.70\(\pm\)3.75e+1(2.74e+2) & 1.70\(\pm\)3.01e+1(1.87e+1) & 7.23e+2(1.74e+2) & 1.70\(\pm\)3.01e+1(1.73e+1) & 7.23e+2(1.74e+2) & 1.70\(\pm\)2.94e+1(1.73e+2) & 1.70\(\pm\)2.94e+1(1.73e+2) & 1.70\(\pm\)2.94e+1(1.73e+2) \\ DTLZ2 & 1.70\(\pm\)3.75e+1(2.74e+2) & 1algorithms is set to 150, 90, respectively. This answers the question we raised before: The advantage of our FSEO framework is more prominent in the extremely expensive problems where a smaller evaluation budget is allowed. The comparison between the results obtained from Tables 5 and 11 has demonstrated that our FSEO framework is preferable when solving optimization problems within a very limited evaluation budget.

### Out-Of-Range Analysis on Extremely Expensive Optimization

In Section F of the main file, we carried out an experiment to study the influence of task similarity on the performance of experience-based expensive multi-objective optimization. The optimization results obtained from the 'in-range' and the 'out-of-range' situations are compared. In this subsection, we conduct an experiment to investigate the difference between the 'in-range' and the 'out-of-range' situations for extremely expensive multi-objective optimization. The experimental setups are the same as the setups described in Section F of the main file, except the allowed fitness evaluation budget is the same as described in Appendix H.1.

Table 12 gives the statistical test results, it can be seen that the 'out-of-range' situation achieves competitive IGD+ values to the 'in-range' situation on all 7 test instances. In comparison to MOEA/D-EGO, the experience gained in the 'out-of-range' situation leads to competitive or smaller IGD+ values on 6 DTLZ problems. Furthermore, similar results can be observed in the last row of Table 12, the 'out-of-range' situation achieves better/competitive/worse IGD+ values than all compared SAEAs on 28/9/5 test instances. In comparison, the 'in-range' situation achieves better/competitive/worse IGD+ values than all compared SAEAs on 29/8/5 test instances. There is only a minor difference between the optimization results obtained in two situations. These observations are consistent with the conclusions we made in Section F of the main file.

Figure 7: IGD+ curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. **Upper**: DTLZ1, DTLZ2, DTLZ3, DTLZ4. **Lower**: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. Note that ‘FS(out)’ indicates the target task is excluded from the range of related tasks during the meta-learning procedure. X-axis denotes the number of evaluations used after the surrogate initialization. In comparison to MOEA/D-EGO, both MOEA/D-FSs achieve smaller or competitive IGD+ values on all DTLZ test problems except for DTLZ7, while 50 evaluations are saved with the assistance from related tasks. Moreover, MOEA/D-FSs achieve the smallest IGD+ values on DTLZ2, DTLZ3, DTLZ4, DTLZ5 and DTLZ6.

[MISSING_PAGE_EMPTY:25]

base kernel parameters are essential to the modeling accuracy of a MDKL model. As a result, our MDKL model outperforms the compared meta-learning modeling and non-meta-learning modeling methods on both the engine fuel consumption regression task and the sinusoid function regression task.

Second, we demonstrate the main contribution of this work: In most situations, the proposed FSEO framework can assist regression-based SAEAs to reach competitive or even better optimization

Figure 8: IGD curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. **Upper**: DTLZ1, DTLZ2, DTLZ3, DTLZ4. **Lower**: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization.

Figure 9: HV curves averaged over 30 runs on 7 DTLZ problems. Solid lines are mean values, while shadows are error regions. **Upper**: DTLZ1, DTLZ2, DTLZ3, DTLZ4. **Lower**: DTLZ5, DTLZ6, DTLZ7. MOEA/D-FSs and comparison algorithms initialize their surrogates with 10, 60 samples, respectively. Extra 30 evaluations are allowed in the further optimization. Note that ‘FS(out)’ indicates the target task is excluded from the range of related tasks during the meta-learning procedure. X-axis denotes the number of evaluations used after the surrogate initialization.

results, while the cost of surrogate initialization is only 1\(d\) samples. Due to the effectiveness of saving evaluations, our FSEO framework is preferable to other SAEAs when solving problems within a very limited evaluation budget. Moreover, some empirical guidelines are concluded to help the application of our FSEO framework. For the influence of task similarity, we find that related tasks that are very similar to the target task are not necessary to the application of our approach. The influence of these similar tasks on the optimization performance is limited. Our FSEO framework can achieve competitive results without datasets from very similar related tasks. Besides, for the related tasks used for meta-learning, we have demonstrated that more useful experience can be learned if more data points are sampled from related tasks.

Third, the effectiveness of our FSEO framework is validated on a real-world engine calibration problem. Competitive or better results are achieved on the objective and constraint functions, while 1\(d\) samples are used to initialize surrogates. Therefore, our FSEO framework can also be applied to optimization scenarios such as single-objective optimization and constrained optimization.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Claims we made accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix B. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Not applicable. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experimental setups are described in detail. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: Will release our code after acceptation, or we can provide the code if any reviewers are interested in it during the review process. Anyway, the details about the code have already described in the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have described all the details about of experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have conducted statistical tests in our experiments, error bars are plotted in figures. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We did not provide information about compute workers and memory since our experiments do not have specific requirements on memory or other computation resource. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: We do not think optimization algorithm can cause any negative social impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: Code will be released after acceptation, it would be open access, no safeguards are required. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited the algorithm platform and the data we used in our paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We did not introduce any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not have any experiments or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not have any experiments or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.