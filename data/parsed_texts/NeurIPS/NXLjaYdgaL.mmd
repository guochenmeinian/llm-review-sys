# Intra-Modal Proxy Learning for

Zero-Shot Visual Categorization with CLIP

 Qi Qian\({}^{1}\)1  Yuanhong Xu\({}^{2}\)  Juhua Hu\({}^{3}\)

\({}^{1}\) Alibaba Group, Bellevue, WA 98004, USA

\({}^{2}\) Alibaba Group, Hangzhou, China

\({}^{3}\) School of Engineering and Technology, University of Washington, Tacoma, WA 98402, USA

{qi.qian, yuanhong.xuyh}@alibaba-inc.com, juhuah@uw.edu

Footnote 1: Corresponding author

###### Abstract

Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from \(77.02\%\) to \(80.21\%\) on ImageNet with ViT-L/14@336 pre-trained by CLIP.

## 1 Introduction

Vision-language pre-training in CLIP aims to align image-text pairs to facilitate multi-modal understanding for downstream tasks [26]. By optimizing the contrastive loss consisting of images and their corresponding text descriptions, CLIP demonstrates an impressive zero-shot transfer of the pre-trained model to generic tasks. Concretely, given a downstream data set, each class has a proxy obtained from the text embedding about the corresponding class name, and then images can be categorized by finding the nearest class proxy. With class names only, zero-shot accuracy on ImageNet [28] can achieve \(77.02\%\) with ViT-L/14@336 [9] pre-trained by CLIP.

After the success of CLIP, many research efforts have been devoted to improving the transfer performance on downstream tasks. Most of developed methods can be cast into two categories, that is, few-shot refinement and zero-shot enhancement. Few-shot methods require a small amount of labeled data from the target task to refine the prediction [12, 41, 42]. While these methods show a better performance than zero-shot learning, labeled examples from the target domain may not be available in real-world applications. Therefore, many methods focus on improving the zero-shot learning itself by exploring side information. On one hand, many pre-trained large language models (LLMs), e.g., GPT-3 [4], can be leveraged to obtain better text proxy for classification [20]. On the other hand, unlabeled data from the target domain can be adopted to fine-tune an adapter network or input prompts [23, 30] for alignment. In this work, we focus on understanding the behavior of CLIP by exploiting the unlabeled data without external LLMs or additional architectures for zero-shot categorization.

When considering pre-trained CLIP models and excluding any auxiliary networks for zero-shot transfer, many existing methods try to optimize input text prompts to align images and the text of class names better [30; 33; 42]. However, recent observations demonstrate that the modality gap between text (i.e., language) and vision is still significant after the optimization of CLIP [18]. Consequently, the efforts on improving the text proxy can be ineffective for the vision task.

To mitigate the problem from the gap between the text and vision space, we propose to obtain proxies of classes in the vision space directly. First, our theoretical analysis in Proposition 1 shows that the modality gap will be preserved due to a small temperature parameter in the contrastive loss of CLIP, which confirms the empirical observation in [18]. Furthermore, Proposition 2 indicates that the optimal proxy for the vision task can only be obtained from the vision space. Therefore, with the gap between the text and vision space, directly using the text proxy makes the zero-shot transfer sub-optimal. To recover the optimal vision proxy, we develop an intra-modal proxy learning (InMaP) method to obtain visual proxies with pseudo labels from text proxies. The proposed method is illustrated in Fig. 1. With fixed features extracted by CLIP encoders, the optimization problem in InMaP is convex without encoders in the loop, which can be solved efficiently with gradient descent [3]. Our main contributions can be summarized as follows.

* We theoretically show that the modality gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP. While the optimal proxy for image classification is in the vision space, the gap degenerates the zero-shot performance of the text proxy as shown in Theorem 1.
* We propose a novel algorithm to obtain the intra-modal proxy directly from the vision space. By recovering the vision proxy with help from the text proxy and an appropriate temperature suggested by Proposition 3 for calibration, effective proxies can be observed from unlabeled data.
* Our Theorem 2 indicates that the accuracy of pseudo labels from the text proxy is essential to recover the proxy in the vision space. Therefore, we further develop strategies to refine pseudo labels according to the data distribution by optimal transport (OT) [34].
* Experiments on 14 downstream vision tasks demonstrate both the effectiveness and efficiency of our proposal. Concretely, InMaP can achieve \(80.21\%\) accuracy on ImageNet with ViT-L/14@336 using a single GPU within one minute.

## 2 Related Work

Before CLIP, zero-shot learning that aims to identify examples of novel classes without any labeled training data has attracted much attention since [22]. However, most of existing works are only able to discover new classes closely related to the training classes [1; 5; 6; 11; 22; 38], where they share the similar attributes, and have to train an individual model for each task. On the contrary, pre-training on large-scale data with the contrastive loss aligns visual and language features and makes a single CLIP model applicable for diverse downstream tasks in a straightforward way. Compared with conventional zero-shot methods, the pre-training data in CLIP may be overlapped with downstream tasks, which can result in data leak for evaluation. While the issue can be addressed by eliminating the overlapping data, the influence on the performance is negligible as discussed in [26].

After pre-training with image-text pairs, the obtained CLIP models can be transferred to various downstream tasks with few-shot or zero-shot strategies. We briefly review these two related directions as follows.

Few-shot transferCLIP demonstrates that an ensemble of input text prompts for the text encoder can improve the zero-shot performance by a significant margin. Therefore, many methods try to

Figure 1: Illustration of our proposed intra-modal proxy learning (InMaP). Triangles denote the class proxy while circles denote the vision data. Different classes are denoted with different colors. By recovering the vision proxy, it can align with vision data better (i.e., from solid line as in the text space to dashed line in the vision space).

further refine the prompts. Given a small set of labeled data from the target vision tasks, CoOp [42] optimizes input prompts for the text encoder with labels. By formulating the prompts as learnable variables, it can obtain task-dependent prompts without any hand-crafted designs. On the contrary, CLIP-Adapter [12] optimizes obtained output features from both encoders. However, it adds additional bottleneck layers for training, which increases the complexity of optimization. To reduce the efforts of training with encoders, Tip-Adapter [41] demonstrates a training-free method that incorporates the prediction from a cache model consisting of the labeled image data and that from the text proxy. In this work, we also consider to minimize the training efforts by excluding encoders in optimization and the intra-modal proxy can be obtained from extracted features.

Zero-shot transferSince labeled data are not always available, many efforts are devoted to improving zero-shot transfer. Some methods explore other large pre-trained models as the supplementary for CLIP. VisDesc [20] leverages GPT-3 to generate rich context descriptions for given class names, and thus shows superior performance over the simple prompt in CLIP. SuS-X [33] includes additional images to mitigate the modality gap. Concretely, auxiliary images can be obtained either from a large data set (e.g., LAION [29]) or a text-to-image generation model (e.g., stable diffusion [27]). The additional large models will inevitably increase the inference time, hence, recent methods consider to leverage the unlabeled target data for fine-tuning. UPL [15] and TPT [30] utilize the unlabeled data to optimize learnable input text prompts. SVL-Adapter [23] trains an additional encoder from the unlabeled data with self-supervised learning for ensemble. Unlike existing methods, our proposal does not need any additional model and we learn the proxy for each class from the vision space directly.

## 3 The Proposed Method

In this section, we first analyze the modality gap in CLIP, and then elaborate the proposed method that recovers the proxy in the vision space to reduce the influence from the modality gap.

### Cross-Modal Learning in CLIP

By pre-training paired images and text, a vision-language model, e.g., CLIP [26], is capable of classifying unlabeled images in the zero-shot manner. Let \(f(\cdot)\) and \(g(\cdot)\) denote the encoder for images and text in the pre-trained CLIP model, respectively. Given a set of unlabeled image data \(\{x_{i}\}_{i=1}^{n}\) and the unique class names for the set \(\{z_{j}\}_{j=1}^{C}\), their corresponding visual and text representations can be extracted as

\[\mathbf{x}_{i}=f(x_{i});\quad\mathbf{z}_{j}=g(z_{j})\]

where \(\mathbf{x}_{i}\in\mathcal{R}^{d}\) and \(\mathbf{z}_{j}\in\mathcal{R}^{d}\) have the same dimension and a unit norm. \(d\) is the dimension of features. \(\mathbf{z}_{j}\) can be considered as the proxy for the \(j\)-th class.

With the features of the \(i\)-th image and all text proxies, the image can be classified as

\[y_{i}=\arg\max_{j}\mathbf{x}_{i}^{\top}\mathbf{z}_{j}\]

where \(y_{i}\) is the prediction from the zero-shot classification. The capacity of zero-shot prediction is from the alignment between vision and language that is optimized by the pre-training objective in CLIP. Given image-text pairs as \((x_{i},t_{i})\), two encoders are learned by minimizing the contrastive loss

\[\sum_{i}-\log\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{i}/\tau)}{\sum_{j}^{ m}\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{j}/\tau)}-\log\frac{\exp(\mathbf{t}_{i}^{ \top}\mathbf{x}_{i}/\tau)}{\sum_{j}^{m}\exp(\mathbf{t}_{i}^{\top}\mathbf{x}_{j }/\tau)}\]

where \(m\) is the mini-batch size, \(\mathbf{t}_{i}=g(t_{i})\) is the extracted text representation of \(t_{i}\) with a unit norm, and \(\tau\) is the temperature for the normalized cross entropy loss.

While CLIP demonstrates an impressive zero-shot performance, the cross entropy loss focuses on cross-modal ranking and can be sub-optimal for intra-modal vision categorization. It is because that cross entropy loss is closely related to a triplet loss [25] as

\[\forall j,\quad\mathbf{x}_{i}^{\top}\mathbf{t}_{i}-\mathbf{x}_{i}^{\top} \mathbf{t}_{j}\geq 0;\quad\mathbf{t}_{i}^{\top}\mathbf{x}_{i}-\mathbf{t}_{i}^{ \top}\mathbf{x}_{j}\geq 0\]

which mainly aims to rank the multi-modal positive pair over negative pairs rather than to mix the text and vision space for modality gap reduction. Moreover, a small temperature, e.g., \(0.01\) in CLIP, will amplify the difference between pairs and preserve the distance between modalities. The phenomenon can be stated as follows. The detailed proof can be found in the appendix.

**Proposition 1**.: _Assuming \(\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{i}/\tau)}{\sum_{j}^{C}\exp(\mathbf{x }_{i}^{\top}\mathbf{t}_{j}/\tau)}=\delta\), we have_

\[\mathbf{x}_{i}^{\top}\mathbf{t}_{k}+c_{1}\tau\leq\mathbf{x}_{i}^{\top} \mathbf{t}_{i}\leq\mathbf{x}_{i}^{\top}\mathbf{t}_{k}+c_{2}\tau\]

_where \(k\) denotes the nearest negative pair as \(k=\arg\max_{j:j\neq i}\mathbf{x}_{i}^{\top}\mathbf{t}_{j}\). \(c_{1}\) and \(c_{2}\) are constants. \(c_{1}>0\) when \(\delta>1/2\) and \(c_{2}>0\) when \(\delta>1/m\)._

RemarkProposition 1 implies that when \(\delta\) is sufficiently large, the relevant text has a higher rank than the irrelevant text, which is the key for zero-shot classification. Moreover, it shows that with the same prediction \(\delta\), the absolute distance between modalities, i.e., \(\|\mathbf{x}_{i}-\mathbf{t}_{i}\|_{2}^{2}=2-2\mathbf{x}_{i}^{\top}\mathbf{t}_ {i}\), partially depends on the value of temperature, i.e., \(\|\mathbf{x}_{i}-\mathbf{t}_{i}\|_{2}^{2}\geq 2-2\mathbf{x}_{i}^{\top} \mathbf{t}_{k}-2c_{2}\tau\). The analysis indicates that optimizing cross entropy loss with a small \(\tau\) will not pull the text and vision space together.

Some recent work empirically observed that inter-modal distribution is significantly different from the intra-modal distribution in CLIP [18; 33], which confirms our analysis. Consequently, the proxies of class names from the text space may not be sufficient to capture the distribution of vision space, which results in the degenerated zero-shot performance. In lieu of using the text proxy directly for zero-shot visual categorization, we propose to obtain the class proxy in vision space as follows.

### Intra-Modal Proxy Learning

With ground-truth labels \(\{y_{i}\}\), the standard supervised learning in vision space can be written as

\[\min_{W}\sum_{i}-\log(\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{w}_{y_{i}}/\tau _{I})}{\sum_{j}^{C}\exp(\mathbf{x}_{i}^{\top}\mathbf{w}_{j}/\tau_{I})})\] (1)

where \(\mathbf{w}_{j}\) is the learnable proxy for the \(j\)-th class and \(W=[\mathbf{w}_{1},\dots,\mathbf{w}_{C}]\in\mathcal{R}^{d\times C}\). \(\tau_{I}\) is the temperature for the optimization with vision data. With an equivalent form, it is easy to show that \(\mathbf{w}_{j}\) is in the vision space spanned by \(\{\mathbf{x}_{i}\}\).

**Proposition 2**.: _Let \(W^{*}\) be the optimal solution to_

\[\min_{W}\sum_{i}-\log(\frac{\exp(-\|\mathbf{x}_{i}-\mathbf{w}_{y_{i}}\|_{2}^{ 2}/(2\tau_{I}))}{\sum_{j}^{C}\exp(-\|\mathbf{x}_{i}-\mathbf{w}_{j}\|_{2}^{2}/ (2\tau_{I}))})\]

_which is equivalent to Eqn. 1 if \(\mathbf{w}_{j}\) as well as \(\mathbf{x}_{i}\) has a unit norm. Then, we have_

\[\forall j,\quad\mathbf{w}_{j}^{*}=\Pi_{\|\mathbf{w}\|_{2}=1}\frac{\sum_{i:y_{ i}=j}(1-p_{i,j})\mathbf{x}_{i}-\sum_{k:y_{k}\neq j}p_{k,j}\mathbf{x}_{k}}{ \sum_{i:y_{i}=j}(1-p_{i,j})-\sum_{k:y_{k}\neq j}p_{k,j}}\]

_where \(p_{i,j}=\frac{\exp(-\|\mathbf{x}_{i}-\mathbf{w}_{j}^{*}\|_{2}^{2}/(2\tau_{I}))} {\sum_{k}^{C}\exp(-\|\mathbf{x}_{i}-\mathbf{w}_{k}^{*}\|_{2}^{2}/(2\tau_{I}))}\) and \(\Pi_{\|\mathbf{w}\|_{2}=1}\) projects the vector to be with a unit norm._

Proof.: It is directly from K.K.T condition [3]. 

RemarkProposition 2 demonstrates that the optimal proxy for vision categorization should be in the vision space. While proxies from the text space can work as the substitute for vision proxies in CLIP, the distance between the text and vision space (i.e., the modality gap) as illustrated in Proposition 1 will degenerate the performance for vision tasks.

To further demonstrate the challenge from the gap between modalities and simplify the analysis, we decompose the proxy of class names as

\[\mathbf{z}_{j}=\sqrt{a}\mathbf{z}_{j}^{x}+\sqrt{1-a}\mathbf{z}_{j}^{\perp}\]

where \(\mathbf{z}_{j}^{x}\) is from the vision space spanned by \(\{\mathbf{x}_{i}\}\) and \(\mathbf{z}_{j}^{\perp}\) shows the component from the orthogonal subspace such that \(\mathbf{z}_{j}^{x\top}\mathbf{z}_{j}^{\perp}=0\). Both \(\mathbf{z}_{j}^{x}\) and \(\mathbf{z}_{j}^{\perp}\) have the unit norm. \(\mathbf{z}_{j}^{\perp}\) can be considered as encoding the specific information for the text modality. With the decomposition, we can observe that it is possible to recover the optimal prediction if the vision space is covered by the text space as follows.

**Proposition 3**.: _Let \(p_{i,j}\) and \(p_{i,j}^{\prime}\) denote the prediction probability obtained with vision proxies \(\{\mathbf{w}_{j}^{*}\}\) and text proxies \(\{\mathbf{z}_{j}\}\), respectively. \(\tau_{T}\) and \(\tau_{I}\) denote the temperature in CLIP and that in vision proxy learning, respectively. If \(\mathbf{z}_{j}^{x}=\mathbf{w}_{j}^{*}\) and \(0<a<1\), we have \(p_{i,j}^{\prime}=p_{i,j}\) when \(\tau_{I}=\tau_{T}/\sqrt{a}\)._RemarkProposition 3 shows that if the text space contains the whole vision space, the intra-modal prediction can be recovered from the cross-modal prediction using a larger temperature as \(\tau_{I}=\tau_{T}/\sqrt{a}\), where \(a\) measures the overlap between modalities.

However, recent observations indicate that the text and vision space learned by CLIP are distinct with a clear margin [18] and it is hard for the text space to cover the vision space. The difference between the text and vision proxy can be lower-bounded in the following theorem.

**Theorem 1**.: _Let \(Z\) and \(W^{*}\) denote the text proxies and optimal vision proxies, where \(Z=\sqrt{a}Z^{x}+\sqrt{1-aZ^{\perp}}\) and \(Z^{x}\) consists of a rank \(r\) approximation of \(W^{*}\) as \(Z^{x}=U_{r}A^{\top}\). \(W^{*}=U\Sigma V^{\top}=\sum_{i}^{d^{\prime}}s_{i}u_{i}v_{i}^{\top}\), where \(d^{\prime}=\min\{d,C\}\) and \(s_{1}\geq\cdots\geq s_{d^{\prime}}\geq 0\). Then, we have_

\[\|Z-W^{*}\|_{F}^{2}\geq 2C(1-\sqrt{a})+\sqrt{a}\sum_{i=r+1}^{d^{\prime}}s_{i}^{2}\]

RemarkTheorem 1 shows that the gap between text and vision proxy comes from two parts. The former term indicates the distance to the irrelevant text space as \(1-\sqrt{a}\). The latter one depicts the approximation loss from the low-rank overlapping between text space and vision space with \(r<d^{\prime}\). Due to the inherent difference between these two modalities, the distance between \(Z\) and \(W\) is hard to be minimized.

Therefore, to improve the zero-shot performance in the vision space, we consider to obtain the vision proxy in lieu of directly using the text proxy. The main challenge comes from the fact that the label \(\{y_{i}\}\) is unavailable in zero-shot classification. Fortunately, the intra-modal proxy can be learned by mimicking the proxy from the other modal. Concretely, we propose to minimize the KL divergence between distributions from the text and vision proxy as

\[\min_{W}L(P^{\prime},W)=\sum_{i}D_{\mathrm{KL}}(P^{\prime}_{i}||P_{i})\] (2)

where \(P^{\prime}_{i}\) and \(P_{i}\) indicate the distribution estimated by the text proxy \(Z\) and the learnable vision proxy \(W\), respectively.

\[p^{\prime}_{i,j}=\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{z}_{j}/\tau_{T})}{ \sum_{k}^{C}\exp(\mathbf{x}_{i}^{\top}\mathbf{z}_{k}/\tau_{T})};\quad p_{i,j} =\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{w}_{j}/\tau_{I})}{\sum_{k}^{C}\exp( \mathbf{x}_{i}^{\top}\mathbf{w}_{k}/\tau_{I})}\]

Proposition 3 suggests that \(\tau_{I}\) for \(P_{i}\) should be larger than \(\tau_{T}\) in \(P^{\prime}_{i}\) to calibrate the magnitude of the distribution and our ablation study confirms the analysis.

Unlike prompt learning methods [30], the problem in Eqn. 2 is defined with extracted features, which does not require the backward operation with large encoder networks for optimization. Moreover, it is convex and the optimal vision proxy can be obtained efficiently by the standard gradient descent [3].

By learning with the pseudo label predicted from the text proxy, we can recover the optimal proxy in the vision space as shown in the following theorem.

**Theorem 2**.: _If simplifying the loss function as \(L(P^{\prime},W)=-\sum_{i,j}P^{\prime}_{i,j}\log(P_{i,j})\) and assuming that it is \(\mu\)-strongly convex in \(W\) and letting_

\[W^{\prime*}=\arg\min_{W}L(P^{\prime},W);\quad W^{*}=\arg\min_{W}L(Y,W)\]

_where \(Y_{i}\) is the ground-truth label distribution for \(\mathbf{x}_{i}\), we have_

\[\|W^{\prime*}-W^{*}\|_{F}^{2}\leq\frac{2}{\mu}\|P^{\prime}-Y\|_{F}\|\log(P_{W ^{\prime*}})-\log(P_{W^{*}})\|_{F}\]

RemarkTheorem 2 shows that the distance between the recovered vision proxy and the optimal vision proxy is bounded by the gap between pseudo labels predicted by text proxy and the ground-truth label distribution. After pre-training CLIP models on a large scale data set, it can approximate the distribution of real data, which enables learning of the intra-modal proxy.

### Pseudo Label Refinement

According to Theorem 2, the accuracy of pseudo label is essential for recovering the appropriate vision proxy. Therefore, we develop strategies to improve the quality of pseudo labels.

First, since we have the whole test set rather than a single example for zero-shot learning, the labels obtained from the text proxy can be refined according to the geometry of the target vision data. Given the logits matrix \(M\in\mathcal{R}^{n\times C}\) with \(M_{i,j}=\mathbf{x}_{i}^{\top}\mathbf{z}_{j}\), the original pseudo label can be obtained by solving the problem

\[P^{\prime}=\arg\max_{P}(P,M)+\tau_{T}H(P)\quad s.t.\quad\forall i,\sum_{j}P_{i,j}=1/n;\quad\forall i,j,P_{i,j}\geq 0\]

where \(H(P)\) computes the entropy of \(P\) and the obtained results should be multiplied by \(n\) as the predicted labels. By incorporating a reference distribution \(q\in\mathcal{R}^{C}\) over classes, the problem can be rewritten as

\[\max_{P}\langle P,M\rangle+\tau_{T}H(P)\quad s.t.\quad\forall i,\sum_{j}P_{i,j}=1/n;\quad\forall j,\sum_{i}P_{i,j}=q_{j};\quad\forall i,j,P_{i,j}\geq 0\] (3)

which is known as Sinkhorn distance [8] and is an approximation of optimal transport distance. The refined pseudo label can be obtained by an efficient iterative method [8].

The last challenge is from observing the appropriate reference distribution. Without any prior knowledge, a balanced distribution \(q=\mathbf{1}/C\) is a popular selection in practice. By investigating the prediction from the text proxy, it implies a distribution as \(\hat{q}_{j}=\sum_{i}P^{\prime}_{i,j}/n\). Then, a reference distribution can be obtained by further smoothing \(\hat{q}\) as

\[q_{j}=\frac{\hat{q}_{j}^{\gamma}}{\sum_{k}\hat{q}_{k}^{\gamma}}\] (4)

where \(\gamma\leq 1\) is the smoothing parameter and \(\gamma=1\) implies the original distribution from the text proxy.

Moreover, inspired by the semi-supervised learning [31], soft label with high confidence can be converted into one-hot label to eliminate the influence from irrelevant classes. Let \(\alpha\) be the threshold. After obtaining the pseudo label from Eqn. 3, we have

\[\tilde{P}_{i}=\left\{\begin{array}{ll}\mathbf{e}_{k}&k=\arg\max_{j}P^{\prime }_{i,j},\quad P^{\prime}_{i,k}>\alpha\\ P^{\prime}_{i}&o.w.\end{array}\right.\] (5)

where \(\mathbf{e}_{k}\in\{0,1\}^{C}\) is the one-hot vector and the \(k\)-th element is \(1\).

The proposed intra-modal proxy learning method (InMaP) is summarized in Alg. 1. Pre-trained CLIP models are only applied to extract features as in Step 2, which is the inevitable cost of conventional zero-shot learning. After obtaining features, the computational overhead of our proposed optimizations in Steps 3-5 is mild as shown in the analysis of running time.

```
1:Input: Unlabeled image set \(\{x_{i}\}\), class names \(\{z_{j}\}\), pre-trained CLIP encoders \((f,g)\), iterations \(T_{w}\), \(T_{p}\), temperature \(\tau_{T}\), \(\tau_{I}\), threshold \(\alpha\)
2: Extract features \(\{\mathbf{x}_{i}\}\) and \(\{\mathbf{z}_{j}\}\) using pre-trained encoders \(f\) and \(g\), respectively.
3: Obtain pseudo labels by solving Eqn. 3 with \(T_{p}\) iterations.
4: Refine pseudo labels with the threshold as in Eqn. 5.
5: Obtain vision proxies \(W\) by solving Eqn. 2 with \(T_{w}\) iterations.
6:return\(y_{i}=\arg\max_{j}\mathbf{x}_{i}^{\top}\mathbf{w}_{j}\) ```

**Algorithm 1** Intra-Modal Proxy Learning (InMaP)

## 4 Experiments

To evaluate the proposed method, we follow the common practice and conduct experiments on ImageNet [28] and \(13\) diverse downstream vision tasks for zero-shot transfer. Text prompts areimportant for obtaining appropriate text proxies. We have the selected \(7\) prompts in [33] as templates for ensemble, which shows better performance than a single prompt or multiple prompts in CLIP. For the proposed method, the temperature for obtaining pseudo labels with the text proxy \(\tau_{T}\) is set to \(0.01\) as obtained by CLIP. The temperature for recovering the visual proxy \(\tau_{I}\) is set to \(0.04\) for all experiments. The intra-modal proxy is learned by standard projected gradient descent, where the initial learning rate is \(10\) and the number of iterations is \(2,000\) for sufficient training. The learning rate will be decayed by \(2\) when the norm of gradient increases. Sinkhorn distance is optimized by \(20\) iterations for refining pseudo labels. All experiments are conducted on a single V100 GPU.

### Ablation Study

First, we have the ablation experiments on ImageNet to demonstrate the efficacy of our method InMaP. Pre-trained ResNet-50 [13] is adopted as the vision encoder and the vanilla zero-shot method in CLIP is denoted as "Baseline".

Temperature \(\tau_{I}\)\(\tau_{I}\) is used to recover the proxy in the vision space as in Eqn. 2. According to our analysis in Proposition 3, \(\tau_{I}\) should be larger than \(\tau_{T}\) in CLIP to leverage the overlap between the text and vision space. Table 1 shows the result when varying \(\tau_{I}\) in \(\{0.01,0.02,0.03,0.04,0.05\}\). "Sim" computes the mean similarity between each example and its nearest proxy, depicting the gap (lower the similarly, higher the gap).

First, when \(\tau_{I}=0.01\), the gain is mainly from learning the vision proxy with refined labels and is already \(2.59\%\) better than the baseline with text proxy. By increasing \(\tau_{I}\), the similarity between examples and proxies also increases, which confirms our analysis in Proposition 1. With an appropriate \(\tau_{I}\), an additional gain of \(0.83\%\) can be observed with the similarity of \(0.39\). According to Proposition 3, a large \(\tau_{I}\) is crucial to recover the optimal vision proxy with the guidance from the text proxy. Thus, we fix \(\tau_{I}=0.04\) for the main comparison.

Pseudo label threshold \(\alpha\)Since the SoftMax operator can incur the over-smoothing issue for output logits, converting the soft label with high confidence to one-hot is an effective strategy in semi-supervised learning. We adopt and evaluate the performance with the threshold \(\alpha\) in Eqn. 5.

Table 2 summarizes the results with different thresholds. When \(\alpha=1\), the soft label after Sinkhorn distance optimization is adopted for optimization, which already surpasses the baseline by a clear margin of \(3.17\%\). When reducing \(\alpha\), more soft labels will be converted to one-hot labels and the performance can be further improved by eliminating noise from the SoftMax operator. When letting \(\alpha=0\), all pseudo labels become one-hot, which amplifies the noise from low-confidence prediction and degenerates the performance. We find that our method is insensitive to the parameter and fix it as \(0.6\) in the remaining experiments if not specified.

Pseudo label smoothing \(\gamma\)While the validation set of ImageNet is well-balanced with \(50\) examples for each class, we conduct the experiments with different \(\gamma\) in Eqn. 4 for demonstration.

Table 3 shows the accuracy on ImageNet with varying \(\gamma\). \(\gamma=1\) follows the distribution from the text prediction and degenerates to the original pseudo label without Sinkhorn distance refinement. By balancing the distribution of the class assignment with a small \(\gamma\), the accuracy can be increased with refined pseudo labels. When \(\gamma\) is sufficiently small, the constraint in Sinkhorn distance degenerates to the balanced constraint and shows the desired performance due to the consistency with the ground-truth distribution. We will have the balanced prior distribution as \(\gamma=0\) for other data sets if not specified.

Components in InMaPFinally, we show the gain from two components in InMaP in Table 4. Let "InMaP\({}^{0.25}\)" denote the proxy learning without refined labels and "InMaP\({}^{0.5}\)" be the variant optimized by solely applying the threshold \(\alpha\) to pseudo labels from the text proxy. "Sinkhorn" denotes the pseudo labels refined by Sinkhorn distance without proxy learning. We can observe that with the labels predicted from the text proxy, the recovered vision proxy of InMaP\({}^{0.25}\) can improve the baseline by \(0.51\%\) with the reduction in the modality gap. Then, simply thresholding the prediction from the text proxy can further improve the performance by \(0.12\%\) as in InMaP\({}^{0.5}\). On the other hand, refining the labels via Sinkhorn distance can substantially outperform the baseline by \(2.21\%\). Learning with better pseudo labels, InMaP shows an additional gain of \(1.21\%\) over refined labels. It confirms our analysis in Theorem 2 that the proposed intra-modal proxy learning can benefit from appropriate pseudo labels.

Number of text promptsAn ensemble of text prompts shows the superior performance over the single prompt on obtaining text proxy for zero-shot classification [26]. We compare these two strategies on our method and summarize the accuracy on ImageNet in Table 5.

First, we can observe that the ensemble improves the performance of baseline with a clear margin of \(2\%\) with different vision encoders. It shows that the vanilla zero-shot transfer strategy is sensitive to the selection of text prompts and confirms the motivation of prompt learning methods. Second, our method outperforms the baseline with the single or ensemble of text prompts for text proxy, which demonstrates the effectiveness of our proposed intra-modal vision proxy. Moreover, the gap between the single and ensemble prompts shrinks on InMaP to less than \(1\%\), which can help reduce the efforts of text prompt tuning.

Running timeInMaP consists of two optimization problems, that is, pseudo label optimization in Eqn. 3 and proxy learning in Eqn. 2. The former one can be formulated as optimizing the Sinkhorn distance that can be solved efficiently by the standard iterative method in [8]. With 20 iterations, the running time is only \(0.002\) second, which is negligible. The latter problem is convex and defined with only extracted features. Unlike prompt learning, the expensive encoders are not included in optimization. Even with \(2,000\) iterations for gradient descent, the running time is only about \(30\) seconds on a single GPU and the cost can be further reduced to \(6\) seconds with FP16. In addition, many efficient methods are available for solving convex problems as in Eqn. 2 and can be explored if needed [3].

### Comparison on ImageNet

In this subsection, we compare InMaP to state-of-the-art methods on ImageNet with different vision encoders provided by CLIP. Table 6 summarizes the results. Besides the vanilla zero-shot prediction in CLIP, TPT [30], SuS-X [33] and VisDesc [20] are included in the comparison, where TPT leverages test data as ours but for prompt learning and the other two employ external large models that are marked in grey color. Results for these methods are directly borrowed from their original papers.

First, we can observe that our method InMaP\({}^{0.5}\) outperforms the baseline and TPT with both ResNet-50 and ViT-B/16 as the vision encoder. Unlike the baseline and TPT that optimizes text prompts for images, we propose to reconstruct the proxy in vision space, which reduces the influence from the modality gap and obtains an appropriate intra-modal classifier as demonstrated in Theorem 2. In addition, the running time of proxy learning mainly depends on the dimension of extracted features and that with features from ViT-L/14@336 runs faster than ResNet-50, which guarantees the efficiency with different encoders. By further improving pseudo labels with Sinkhorn distance, InMaP shows the best performance among all methods and even surpasses those ones with additional large models, which confirms the effectiveness of our method. Moreover, InMaP has a steady improvement over InMaP\({}^{0.5}\) using different encoders. The observation indicates that label refinement

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Vision encoder & Baseline\_single & Baseline\_ensemble & InMaP\_single & InMaP\_ensemble \\ \hline ResNet-50 & 58.15 & 60.32 & 63.15 & 63.74 \\ ViT-B/16 & 66.72 & 68.75 & 71.93 & 72.55 \\ \hline \end{tabular}
\end{table}
Table 5: Comparison of accuracy(%) on ImageNet with different text prompts. “Single” denotes the application of the single prompt of “a photo of a ().”, while “ensemble” contains 7 prompts as suggested by [33].

is complementary to the intra-modal proxy learning, which is consistent with Theorem 2. Finally, with the largest backbone provided by CLIP, InMaP achieves \(80.21\%\) accuracy on ImageNet, which shows the potential of large pre-trained vision-language models.

After the optimization with the target data set, we demonstrate the performance of InMaP when the target data arrives in a streaming manner but an unlabeled set of relevant data is available. Concretely, the vision proxy is constructed from the unlabeled training set in lieu of the target validation set while the learned proxy is evaluated on the validation set. Table 7 shows the results of our method learned with unlabeled training set ("InMaP\({}_{\rm train}\)") or validation set ("InMaP\({}_{\rm val}\)") of ImageNet. For InMaP\({}_{\rm train}\), \(\tau_{I}\) and \(\alpha\) are reduced to \(0.03\) and \(0.4\), respectively, while other parameters remain unchanged.

We can observe that even without the target data for recovering the vision proxy, an appropriate unlabeled data set is sufficient to obtain effective proxy for zero-shot classification. By thresholding the original pseudo labels, InMaP\({}_{\rm train}^{0.5}\) outperforms InMaP\({}_{\rm val}^{0.5}\) over all vision encoders, where the benefit may come from the large size of the training set. Then, if refining pseudo labels with Sinkhorn distance, the performance of proxies obtained with different data becomes similar and is better than the variant without optimized pseudo labels, which confirms the efficacy of our strategy for improving labels. Finally, this experiment demonstrates that the proposed method is applicable for different scenarios in real-world applications.

### Comparison on 13 Downstream Tasks

Then, we evaluate the proposed method on 13 diverse downstream tasks, including Aircraft [19], Caltech101 [10], Stanford Cars [16], CIFAR-10 [17], CIFAR-100 [17], CUB200-2011 [35], Describable Textures Dataset (DTD) [7], EuroSAT [14], Flowers [21], Food101 [2], Oxford-IIIT Pet (Pets) [24], Sun397 [39], and UCF101 [32]. These data sets cover a large range of classification tasks such as fine-grained visual categorization, texture recognition, scene categorization, classification with low-resolution images, etc. The same parameters used for ImageNet are applied for most of them except certain parameters for refining labels. Concretely, Caltech and Flowers can benefit from the appropriate reference distribution for generating pseudo labels and we set \(\gamma\) in Sinkhorn distance as \(\gamma=0.9\) for ResNet and \(\gamma=\{1,0.5\}\) for ViT, respectively for the two data sets. Moreover, the threshold \(\alpha\) is reduced to \(0.3\) on EuroSAT. Other parameters remain unchanged.

Table 8 summarizes the comparison. First, we observe that with the same parameters for all data sets, InMaP\({}^{0.5}\) shows the better performance than the baseline consistently and obtains \(1\%\) improvement on average by ResNet-50. It demonstrates that recovering the proxy in vision space can benefit different tasks for zero-shot transfer with pre-trained CLIP models. Second, by incorporating refined labels from optimizing Sinkhorn distance, InMaP can further improve InMaP\({}^{0.5}\) by more than \(4\%\) on average. The superior performance confirms that appropriate labels can facilitate the intra-modal

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|} \hline Vision encoder & Baseline [26] & PTT [30] & InMaP\({}^{0.5}\) & InMaP & VisDesc\({}^{\dagger}\)[20] & SuS-X-LC\({}^{\dagger}\)[33] & SuS-X-SD\({}^{\dagger}\)[33] \\ \hline ResNet-50 & 60.32 & 60.74 & 60.95 & **63.74** & 59.68 & 61.89 & 61.84 \\ ViT-B/32 & 63.77 & - & 64.82 & **67.29** & 62.97 & 64.73 & 64.71 \\ ViT-B/16 & 68.75 & 68.98 & 70.17 & **72.55** & 68.03 & 70.00 & 69.88 \\ ViT-L/14 & 75.96 & - & 77.14 & **79.29** & 75.00 & **-** & **-** \\ ViT-L/14\(\oplus\)336 & 77.02 & - & 78.27 & **80.21** & 76.16 & **-** & **-** \\ \hline \end{tabular}
\end{table}
Table 6: Comparison of accuracy (%) on ImageNet with different vision encoders in CLIP. \(\dagger\) in grey indicates the application of external large models. The overall best performance is in bold, while the best performance without any additional model is underlined. “-” denotes that the result is unavailable in their original papers.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|} \hline Vision encoder & ResNet-50 & ViT-B/32 & ViT-B/16 & ViT-L/14 & ViT-L/14\(\oplus\)336 \\ \hline InMaP\({}_{\rm val}^{0.5}\) & 60.95 & 64.82 & 70.17 & 77.14 & 78.27 \\ InMaP\({}_{\rm val}^{0.5}\) & 63.74 & 67.29 & 72.55 & 79.29 & 80.21 \\ InMaP\({}_{\rm train}^{0.5}\) & 61.38 & 65.12 & 70.37 & 77.69 & 78.68 \\ InMaP\({}_{\rm train}\) & 63.98 & 67.37 & 72.59 & 79.21 & 80.13 \\ \hline \end{tabular}
\end{table}
Table 7: Comparison of accuracy (%) on ImageNet. For InMaP, “train” and “val” denote that the vision proxy is obtained from the unlabeled training set and validation set, respectively.

proxy learning to obtain better proxy reconstruction in the target space. In addition, our method is the best among all methods on 8 out of 13 data sets with ResNet-50, which shows that leveraging unlabeled target data can be more effective than external models for zero-shot learning. Moreover, the same parameters are shared by different tasks for InMaP, while fine-tuning them by a validation set for each data set can further improve the performance.

With ViT-B as the vision backbone, the accuracy of the baseline increases by \(10\%\), which shows the potential of large models. The proposed method can also benefit from the large models and achieve the accuracy of \(71.4\%\) on average, which is more than \(5\%\) better than the baseline. Compared with SuS-X with external models, InMaP outperforms it by \(1.74\%\) that confirms the effectiveness of intra-modal proxy learning. Moreover, for the data set with a high zero-shot accuracy as Caltech, the predicted distribution can approximate the ground-truth well and the label refinement with Sinkhorn distance can be skipped by letting \(\gamma=1\). Finally, both InMaP\({}^{0.5}\) and InMaP improves over the baseline on diverse tasks with ResNet and ViT, which illustrates that the proposed method is applicable for different vision encoders.

## 5 Conclusion

In this work, we focus on improving the zero-shot transfer with CLIP. First, our theoretical analysis indicates that the modality gap between the text and vision space obtained by CLIP will be preserved, which can degenerate the performance of zero-shot transfer. To mitigate the challenge, we propose to recover the proxy of each class in the vision space with the help from the text proxy. Concretely, by leveraging the unlabeled target data and refining the prediction from the text proxy, we can obtain the vision proxy via learning with unlabeled data and the corresponding pseudo label. Experiments on diverse data sets demonstrate that our method can improve the zero-shot accuracy consistently with different vision encoders. Exploring our method with other pre-trained models rather than CLIP can be our future work. Besides, recent work [33] shows that other large pre-trained models, i.e., GPT-3, can help CLIP for zero-shot prediction. Combining our method with existing large models is also an interesting future direction. Finally, our analysis for the efficacy of the text proxy is from the perspective of approximating the supervised learning for images as in Eqn. 1. Text information can be complementary to the knowledge in images [36] and investigating the potential benefit of text for vision tasks can help understand vision-language models better.

## Limitations

While our analysis is for the modality gap in contrastive pre-training with the architecture of dual encoders, it cannot be applied directly for other pre-training paradigms with different architectures. For example, BEiT-3 [37] has masked data modeling for pre-training with the shared backbone for different modalities and CoCa [40] has encoder-decoder captioning for pre-training. The analysis for the modality gap in these methods has been less investigated and can be inspired by our results as a future direction.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline  & Aircraft & Caltech & Cars & Cifar10 & Cifar100 & UCB & DTD & EuroSAT & Flowers & Food & Pets & SUN & UCF101 & Avg. \\ \hline \multicolumn{13}{|l|}{_ResNet-50:_} \\ \hline Baseline & 16.62 & 86.00 & 56.31 & 73.15 & 40.60 & 41.37 & 41.13 & 26.90 & 63.13 & 74.10 & 81.85 & **59.25** & **55.56** & **55.07** \\
**TPTP** & 17.58 & 87.02 & 58.46 & & 40.84 & 28.33 & 62.69 & 74.88 & 84.49 & 61.46 & 60.82 & - \\ InMaP\({}^{0.5}\) & 16.95 & 86.41 & 58.11 & 74.23 & 40.72 & 41.82 & 42.14 & 27.52 & 66.29 & 74.95 & 82.94 & 59.95 & 57.49 & **56.12** \\
**InMaP** & 18.96 & 86.73 & **63.30** & **78.84** & **49.26** & **49.17** & 48.46 & 38.58 & 66.68 & **78.36** & **87.39** & **63.82** & **63.36** & **60.85** \\
**VisDesi\({}^{1}\)** & 16.26 & 88.11 & 54.76 & 73.22 & 39.69 & 48.31 & 41.96 & 37.60 & 65.37 & 76.80 & 82.59 & 59.84 & 58.47 & 57.14 \\
**SS-X-LC\({}^{1}\)** & 21.09 & 89.57 & 57.17 & 74.95 & 44.48 & 48.56 & 49.23 & 44.23 & 67.07 & 77.62 & 86.59 & 63.01 & 61.49 & 60.41 \\
**S-X-SC\({}^{1}\)** & 19.47 & 89.53 & 57.27 & 74.69 & 45.43 & 49.12 & 50.59 & 45.87 & 67.72 & 77.58 & 58.34 & 62.95 & 61.54 & 60.46 \\ \hline ViT-B/16: & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline Baseline & 23.13 & 93.51 & 66.29 & 91.09 & 67.29 & 49.36 & 45.09 & 50.17 & 67.64 & 84.43 & 87.00 & 65.68 & 65.21 & **65.84** \\
**TPTP** & **24.78** & **94.16** & **66.87** & & **47.55** & **42.44** & **68.98** & **84.67** & **87.79** & **65.50** & **68.04** & **11.09** & **67.18** & **67.15** & **67.34** \\
**InMaP\({}^{0.5}\)** & **23.55** & **93.59** & **67.96** & **92.48** & **68.37** & **51.54** & **46.28** & **53.64** & **69.64** & **85.76** & **88.17** & **67.18** & **67.25** & **67.34** \\
**InMaP\({}^{0.5}\)** & **28.35** & **93.59** & **73.00** & **93.27** & **72.51** & **57.87** & **50.77** & **63.98** & **71.28** & **87.72** & **79.24** & **79.08** & **72.06** & **72.06** & **71.40** \\
**VisDesi\({}^{1}\)** & * & - & - & - & - & - & 57.75 & 45.59 & 48.82 & - & - & - & - & - \\
**S-X-LC\({}^{1}\)** & 30.81 & 93.91 & 65.90 & 90.84 & 68.66 & 56.96 & 56.832 & 58.05 & 73.03 & 86.08 & 91.58 & 67.85 & 66.02 & 69.66 \\
**S-X-SD\({}^{1}\)** & 28.68 & 93.96 & 66.13 & 89.88 & 68.47 & 57.11 & 54.55 & 57.45 & 73.81 & 68.09 & 50.57 & 67.73 & 66.59 & 69.31 \\ \hline \end{tabular}
\end{table}
Table 8: Comparisons of accuracy (%) on 13 diverse downstream tasks with ResNet-50 and ViT-B/16. \(\dagger\) in grey indicates the application of external large models. The overall best performance is in bold, while the best performance without any additional model is underlined. “-” denotes that the result is unavailable in their original papers.

## References

* [1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for image classification. _IEEE Trans. Pattern Anal. Mach. Intell._, 38(7):1425-1438, 2016.
* mining discriminative components with random forests. In _ECCV_, volume 8694 of _Lecture Notes in Computer Science_, pages 446-461. Springer, 2014.
* [3] S. P. Boyd and L. Vandenberghe. _Convex Optimization_. Cambridge University Press, 2014.
* [4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In _NeurIPS_, 2020.
* [5] S. Chen, Z. Hong, G. Xie, J. Zhao, H. Li, X. You, S. Yan, and L. Shao. Transzero++: Cross attribute-guided transformer for zero-shot learning. _CoRR_, abs/2112.08643, 2021.
* [6] S. Chen, G. Xie, Y. Liu, Q. Peng, B. Sun, H. Li, X. You, and L. Shao. HSVA: hierarchical semantic-visual adaptation for zero-shot learning. In _NeurIPS_, pages 16622-16634, 2021.
* [7] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In _CVPR_, pages 3606-3613, 2014.
* [8] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In _NeurIPS_, pages 2292-2300, 2013.
* [9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_. OpenReview.net, 2021.
* [10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _CVPR workshop_, pages 178-178. IEEE, 2004.
* [11] Z. Fu, T. Xiang, E. Kodirov, and S. Gong. Zero-shot learning on semantic class prototype graph. _IEEE Trans. Pattern Anal. Mach. Intell._, 40(8):2009-2022, 2018.
* [12] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. _CoRR_, abs/2110.04544, 2021.
* [13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778. IEEE Computer Society, 2016.
* [14] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens._, 12(7):2217-2226, 2019.
* [15] T. Huang, J. Chu, and F. Wei. Unsupervised prompt learning for vision-language models. _CoRR_, abs/2204.03649, 2022.
* [16] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In _ICCV workshops_, pages 554-561, 2013.
* [17] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [18] W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Y. Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In _NeurIPS_, 2022.
* [19] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [20] S. Menon and C. Vondrick. Visual classification via description from large language models. _CoRR_, abs/2210.07183, 2022.
* [21] M. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In _ICVGIP_, pages 722-729. IEEE Computer Society, 2008.

* [22] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell. Zero-shot learning with semantic output codes. In _NeurIPS_, pages 1410-1418. Curran Associates, Inc., 2009.
* [23] O. Pantazis, G. J. Brostow, K. E. Jones, and O. M. Aodha. Svl-adapter: Self-supervised adapter for vision-language pretrained models. In _BMVC_, page 580. BMVA Press, 2022.
* [24] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In _CVPR_, pages 3498-3505. IEEE, 2012.
* [25] Q. Qian, L. Shang, B. Sun, J. Hu, T. Tacoma, H. Li, and R. Jin. Softtriple loss: Deep metric learning without triplet sampling. In _ICCV_, pages 6449-6457. IEEE, 2019.
* [26] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.
* [27] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10674-10685. IEEE, 2022.
* [28] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge. _Int. J. Comput. Vis._, 115(3):211-252, 2015.
* [29] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In _NeurIPS_, 2022.
* [30] M. Shu, W. Nie, D. Huang, Z. Yu, T. Goldstein, A. Anandkumar, and C. Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In _NeurIPS_, 2022.
* [31] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. Raffel, E. D. Cubuk, A. Kurakin, and C. Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _NeurIPS_, 2020.
* [32] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. _CoRR_, abs/1212.0402, 2012.
* [33] V. Udandarao, A. Gupta, and S. Albanie. Sus-x: Training-free name-only transfer of vision-language models. _CoRR_, abs/2211.16198, 2022.
* [34] C. Villani. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [35] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.
* [36] J. Wang, Y. Xu, J. Hu, M. Yan, J. Sang, and Q. Qian. Improved visual fine-tuning with natural language supervision. In _ICCV_, 2023.
* [37] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, and F. Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. _CoRR_, abs/2208.10442, 2022.
* A comprehensive evaluation of the good, the bad and the ugly. _IEEE Trans. Pattern Anal. Mach. Intell._, 41(9):2251-2265, 2019.
* [39] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In _CVPR_, pages 3485-3492. IEEE Computer Society, 2010.
* [40] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. _Trans. Mach. Learn. Res._, 2022, 2022.
* [41] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Training-free adaption of CLIP for few-shot classification. In _ECCV_, volume 13695 of _Lecture Notes in Computer Science_, pages 493-510. Springer, 2022.
* [42] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. _Int. J. Comput. Vis._, 130(9):2337-2348, 2022.

Theoretical Analysis

### Proof of Proposition 1

Proof.: According to the definition, we have

\[(1-\delta)\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{i}/\tau)=\delta\sum_{j:j\neq i}^{ m}\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{j}/\tau)\]

Hence

\[\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{i}/\tau) =\frac{\delta}{1-\delta}\sum_{j:j\neq i}^{m}\exp(\mathbf{x}_{i}^ {\top}\mathbf{t}_{j}/\tau)\leq\frac{\delta(m-1)}{1-\delta}\exp(\mathbf{x}_{i}^ {\top}\mathbf{t}_{k}/\tau)\] \[\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{i}/\tau) \geq\frac{\delta}{1-\delta}\exp(\mathbf{x}_{i}^{\top}\mathbf{t}_{ k}/\tau)\]

where \(k=\arg\max_{j:j\neq i}\mathbf{x}_{i}^{\top}\mathbf{t}_{j}\). Since logarithm function is monotone, the similarity between positive pair can be bounded by

\[\mathbf{x}_{i}^{\top}\mathbf{t}_{k}+c_{1}\tau\leq\mathbf{x}_{i}^{\top} \mathbf{t}_{i}\leq\mathbf{x}_{i}^{\top}\mathbf{t}_{k}+c_{2}\tau\]

where \(c_{1}=\log(\frac{\delta}{1-\delta})\) and \(c_{2}=\log(\frac{\delta(m-1)}{1-\delta})\). 

### Proof of Proposition 3

Proof.: According to the definition, we have

\[p_{i,j}^{\prime}=\frac{\exp(\mathbf{x}_{i}^{\top}\mathbf{z}_{j}/ \tau_{T})}{\sum_{k}\exp(\mathbf{x}_{i}^{\top}\mathbf{z}_{k}/\tau_{T})}=\frac{ \exp(\sqrt{a}\mathbf{x}_{i}^{\top}\mathbf{z}_{j}^{x}/\tau_{T})}{\sum_{k}\exp( \sqrt{a}\mathbf{x}_{i}^{\top}\mathbf{z}_{k}^{x}/\tau_{T})}=\frac{\exp(\sqrt{a} \mathbf{x}_{i}^{\top}\mathbf{w}_{j}^{*}/\tau_{T})}{\sum_{k}\exp(\sqrt{a} \mathbf{x}_{i}^{\top}\mathbf{w}_{j}^{*}/\tau_{T})}\]

If \(\sqrt{a}/\tau_{T}=1/\tau_{I}\), the predictions are equivalent. 

### Proof of Theorem 1

Proof.: According to the definition, we have

\[\|Z-W^{*}\|_{F}^{2}=2C-2\sqrt{a}\langle Z^{x},W^{*}\rangle=2C(1- \sqrt{a})+\sqrt{a}\|Z^{x}-W^{*}\|_{F}^{2}\] (6)

To bound the similarity between \(Z^{x}\) and \(W^{*}\), we assume \(Z^{x}=U_{r}A^{\top}\) where \(U_{r}\in\mathcal{R}^{d\times r}\) is a subset of \(U\in\mathcal{R}^{d\times d^{\prime}}\). By solving the problem \(\min_{A}\|U_{r}A-W^{*}\|_{F}^{2}\), we have \(Z^{x}=U_{r}A=U_{r}(U_{r}^{\top}U_{r})^{-1}U_{r}^{\top}W^{*}=U_{r}U_{r}^{\top}W^ {*}\) that is projecting \(W^{*}\) to the subspace spanned by \(U_{r}\). Since \(U_{r}\) is a subset from \(U\), we have \(U_{r}U_{r}^{\top}=\sum_{i}^{d^{\prime}}r_{i}u_{i}u_{i}^{\top}\) with \(\forall i,r_{i}\in\{0,1\}\) and \(\sum_{i}r_{i}=r\). Then, we have

\[\|U_{r}U_{r}^{\top}W^{*}-W^{*}\|_{F}^{2}=\|\sum_{i}(r_{i}-1)s_{i}u_{i}v_{i}^{ \top}\|_{F}^{2}=\sum_{i}(r_{i}-1)^{2}s_{i}^{2}\]

Therefore

\[\|Z^{x}-W^{*}\|_{F}^{2}\geq\|U_{r}U_{r}^{\top}W^{*}-W^{*}\|_{F}^{2} \geq\sum_{i=r+1}^{d^{\prime}}s_{i}^{2}\]

where the last inequality is from keeping the largest singular values for minimizing the approximation loss. The target result is obtained by taking this inequality to Eqn. 6. 

### Proof of Theorem 2

Proof.: First, we note that \(L(P,W)\) is a convex function in \(W\). We assume that it is \(\mu\)-strongly convex in \(W\) such that for the arbitrary \((W_{1},W_{2})\), we have

\[L(P,W_{1})\geq L(P,W_{2})+\langle\nabla_{W_{2}}L(P,W_{2}),W_{1}-W_{2}\rangle+ \frac{\mu}{2}\|W_{1}-W_{2}\|_{F}^{2}\]According to the optimality of \(W^{\prime*}\), we have

\[L(P^{\prime},W^{\prime*})-L(Y,W^{*})\leq L(P^{\prime},W^{*})-L(Y,W^{*})=\langle P ^{\prime}-Y,-\log(P_{W^{*}})\rangle\] (7)

The lower-bound can be obtained from the strongly convexity of \(W\) as

\[L(P^{\prime},W^{\prime*})-L(Y,W^{*})=L(Y,W^{\prime*})-L(Y,W^{*})+ \langle P^{\prime}-Y,-\log(P_{W^{\prime*}})\rangle\] \[\geq\frac{\mu}{2}\|W^{\prime*}-W^{*}\|_{F}^{2}+\langle P^{\prime }-Y,-\log(P_{W^{\prime*}})\rangle\] (8)

Combining inequalities in Eqns. 7 and 8, we have

\[\|W^{\prime*}-W^{*}\|_{F}^{2}\leq\frac{2}{\mu}\langle P^{\prime}-Y,\log(P_{W^{ \prime*}})-\log(P_{W^{*}})\rangle\leq\frac{2}{\mu}\|P^{\prime}-Y\|_{F}\|\log(P _{W^{\prime*}})-\log(P_{W^{*}})\|_{F}\]