In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates

 Shicheng Liu & Minghui Zhu

School of Electrical Engineering and Computer Science

Pennsylvan State University

University Park, PA 16802, USA

{sf15539,muz16}@psu.edu

###### Abstract

Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that best fit the demonstrated trajectories of an expert. However, current IRL works cannot learn incrementally from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn. To bridge the gap, this paper considers the problem of learning a reward function and a corresponding policy while observing the initial state-action pair of an ongoing trajectory and keeping updating the learned reward and policy when new state-action pairs of the ongoing trajectory are observed. We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy. We propose a novel algorithm to solve this problem and guarantee that the algorithm achieves sub-linear local regret \(O(\sqrt{T}+\log T+\sqrt{T}\log T)\). If the reward function is linear, we prove that the proposed algorithm achieves sub-linear regret \(O(\log T)\). Experiments are used to validate the proposed algorithm.

## 1 Introduction

Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that are consistent with the demonstrated trajectories of an expert. In recent years, several IRL methods are proposed to help learn the reward and policy, including maximum margin methods [1; 2], maximum entropy methods [3; 4], maximum likelihood methods [5; 6], and Bayesian methods [7; 8].

The aforementioned IRL works learn from pre-collected demonstration sets and do not improve the learned model during deployment. Online IRL [9; 10; 11] instead can learn from sequentially arrived demonstrated trajectories and continuously improve the learned reward and policy from the newly observed complete trajectories. However, recent applications of IRL motivate the need to learn incrementally from an ongoing trajectory before it terminates. For example, inferring a moving shooter's intention from its ongoing movement in order to evacuate the hiding victims [12] before the shooter finds them. In this case, we need to quickly update the inference about the shooter's intention once a new movement of the shooter is observed, so that we can use the latest inference to plan a rescue strategy as soon as possible. We cannot wait until the shooter trajectory ends, in case the shooter has found the victims. Another example is learning a target customer's investment preference from its daily updated investment trajectory in a stock market [13] in order to recommend appropriate stocks [14; 15] before other competitors get this customer. However, current IRL works cannot learn from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn from. To bridge the gap, this paper proposes in-trajectory IRL, a new type of IRL that learns a 

[MISSING_PAGE_EMPTY:2]

Note that \(L_{t}(\theta;(S^{E}_{t},A^{E}_{t}))\) is defined using \(\pi_{\theta}\) and \(\theta\) is the parameter of the reward function \(r_{\theta}\), here the policy \(\pi_{\theta}\) is also parameterized by \(\theta\) because it is computed by solving an RL problem (in the lower level) under the reward function \(r_{\theta}\), and thus is indirectly parameterized by \(\theta\). Maximum likelihood IRL (ML-IRL) [5] has a similar bi-level formulation with (1), however, ML-IRL only solves an offline optimization problem and its analysis does not hold for non i.i.d. input data and continuous state-action space. We discuss our distinctions from ML-IRL in Appendix A.1.

The upper-level loss function \(L_{t}\) has two terms. The first term \(-\gamma^{t}\log\pi_{\theta}(A^{E}_{t}|S^{E}_{t})\) is the discounted negative log-likelihood of the state-action pair \((S^{E}_{t},A^{E}_{t})\) at time \(t\) and the second term \(\frac{\lambda\gamma^{t}}{2}||\theta-\bar{\theta}||^{2}\) is the discounted meta-regularization term [25] where \(\lambda\) is a hyper-parameter. The likelihood function is commonly used in IRL [5, 6] to learn a reward function. Basically, the upper-level loss function at time \(t\) encourages to find a reward function \(r_{\theta}\) that makes the observed state-action pair \((S^{E}_{t},A^{E}_{t})\) most likely and meanwhile, the reward parameter \(\theta\) should not be too far from the prior experience, i.e., the meta-prior \(\bar{\theta}\). Note that \(\bar{\theta}\) is a pre-trained meta-prior that embeds the information of "relevant experience". We will introduce the training of \(\bar{\theta}\) in Subsection 4.3 and Appendix C.

The lower-level problem is used to compute \(\pi_{\theta}\) using the current reward function \(r_{\theta}\). It proposes to find a policy \(\pi_{\theta}\) that maximizes the entropy-regularized cumulative reward \(J_{\theta}(\pi)+H(\pi)\). The cumulative reward of a policy \(\pi\) under the reward function \(r_{\theta}\) is \(J_{\theta}(\pi)\triangleq E^{\pi}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}r_{ \theta}(S_{t},A_{t})]\) where the initial state is drawn from \(P_{0}\). The causal entropy of a policy \(\pi\) is defined as \(H(\pi)\triangleq E^{\pi}_{S,A}[-\sum_{t=0}^{\infty}\gamma^{t}\log\pi(A_{t}|S_{ t})]\).

Since the expert demonstrates \(\{(S^{E}_{t},A^{E}_{t})\}_{t\geq 0}\) sequentially, we have a sequence of loss functions \(\{L_{t}(\theta;(S^{E}_{t},A^{E}_{t}))\}_{t\geq 0}\). We use this sequence of loss functions (1) to formulate an online learning problem. A typical online learning problem is to minimize the regret: \(\sum_{t=0}^{T-1}L_{t}(\theta_{t};(S^{E}_{t},A^{E}_{t}))-\min_{\theta}\sum_{t=0 }^{T-1}L_{t}(\theta;(S^{E}_{t},A^{E}_{t}))\). However, it is too challenging to minimize the regret in our case because the loss function \(L_{t}\) could be non-convex. Therefore, we aim to minimize the local regret which is widely adopted in online non-convex optimization [26, 27] and online IRL [11]. The local regret quantifies the general stationarity of a sequence of loss functions under the learned parameters. In specific, given a sequence of loss functions \(\{f_{t}(x)\}_{t\geq 0}\), the local regret [11, 26, 27] at time \(t\) is defined as \(||\frac{1}{t+1}\sum_{i=0}^{t}\nabla f_{i}(x_{t})||^{2}\) which quantifies the gradient norms of the average of all the previous loss functions under the current learned parameter \(x_{t}\). The total local regret is defined as the sum of the local regret at each time \(t\), i.e., \(\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla f_{i}(x_{t})||^{2}\). In our case, we replace \(\{f_{t}\}_{t\geq 0}\) with the loss function \(\{L_{t}\}_{t\geq 0}\) defined in (1) and thus formulate the local regret (2)-(3) which has a bi-level formulation. We aim to minimize the following local regret:

\[E_{\{(S^{E}_{t},A^{E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}(\cdot, \cdot)\}_{t\geq 0}}\bigg{[}\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L _{i}(\theta_{t};(S^{E}_{i},A^{E}_{i}))||^{2}\bigg{]},\] (2) \[\text{s.t.}\quad\pi_{\theta_{t}}=\operatorname*{arg\,max}_{\pi}J_ {\theta_{t}}(\pi)+H(\pi),\] (3)

where \((S^{E}_{t},A^{E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}(\cdot,\cdot)\) means that \((S^{E}_{t},A^{E}_{t})\) is drawn from the state-action distribution \(\mathbb{P}^{\pi_{E}}_{t}(\cdot,\cdot)\), and \(\mathbb{P}^{\pi}_{t}(\cdot,\cdot)\) is the state-action distribution induced by \(\pi\) at time \(t\) in the MDP.

**Difficulties of solving problem (2)-(3)**. We want to design a fast algorithm to solve problem (2)-(3) since we need to finish the update of \(\theta\) and \(\pi\) before the next state-action pair is observed and the time between two consecutive state-action pairs can be short. However, designing and analyzing such a fast algorithm is difficult due to the following challenges:

(i) First and foremost, current state-of-the-arts [26, 27] on online non-convex optimization use follow-the-leader-based algorithms which solve \(\min_{\theta}\sum_{i=0}^{t}L_{i}(\theta;(S^{E}_{i},A^{E}_{i}))\) to near stationarity at each time \(t\). This is time-consuming because they require multiple gradient descent updates of \(\theta\). One way to alleviate this problem is to use online gradient descent (OGD) which only updates \(\theta\) by one gradient descent step at each time \(t\). However, since OGD does not solve the problem to near stationarity at any time \(t\), it is extremely difficult to quantify the overall stationarity after \(T\) iterations. While OGD has been well studied in online convex optimization, it is rarely studied in online non-convex optimization. The recent work [11] uses OGD to quantify the local regret, however, its analysis can only hold when the input data is i.i.d. In contrast, the input data in our problem is not i.i.d. In specific, the input data at time \(t\) (i.e., \((S^{E}_{t},A^{E}_{t})\)) is actually affected by the input data at last step (i.e.,\((S_{t-1}^{E},A_{t-1}^{E})\)). This correlation between any two consecutive input data makes it difficult to analyze the growth rate of the local regret.

(ii) Second, it is time-consuming if we fully solve the lower-level problem (3) to get \(\pi_{\theta}\) because this requires multiple policy updates to solve an RL problem. Therefore, we use a "single-loop" method which only requires one-step policy update for a given \(r_{\theta}\). However, since the policy is only updated once, the updated policy can be far from \(\pi_{\theta}\) and thus making the analysis difficult. Single-loop methods are widely adopted to solve hierarchical problems, including bi-level optimization [28; 29], game theory [30; 31], min-max problems [32; 33], etc. Recently, single-loop methods are applied to IRL [5], however, the paper [5] only solves an offline optimization problem and its analysis does not hold for non i.i.d. input data and continuous state-action space. We include a section in Appendix A.1 to discuss our distinctions from [5].

## 4 Algorithm and Theoretical Analysis

This section has three parts. The first part presents a novel online learning algorithm that solves the problem (2)-(3) and tackles the aforementioned two difficulties. The second part proves that Algorithm 1 achieves sub-linear local regret. If the reward function is linear, we prove that Algorithm 1 achieves sub-linear regret. The third part introduces a meta-learning method to get the meta-prior \(\bar{\theta}\).

### The proposed algorithm

In practice, the expert will demonstrate a specific trajectory \(s_{0}^{E},a_{0}^{E},s_{1}^{E},a_{1}^{E},\cdots\). For distinction, we use the capital letters (e.g., \(S\)) to represent random variables and the lower-case letters (e.g., \(s\)) to represent specific values. To design a fast algorithm, we propose an online-gradient-descent-based single-loop algorithm. In specific, at each time \(t\), the algorithm updates both policy \(\pi\) and reward parameter \(\theta\) only once. The policy update is to solve the lower-level problem (3) and the reward update is to solve the upper-level problem (2). In the following, we elaborate the procedure of policy update and reward update.

```
0: Initialized policy \(\pi_{0}\), the streaming input data \(\{(s_{t}^{E},a_{t}^{E})\}_{t\geq 0}\)
0: Learned reward parameter \(\theta_{T}\) and policy \(\pi_{T}\)
1: Compute \(\bar{\theta}\) using the meta-regularization in Section 4.3 and Appendix C, and set \(\theta_{0}=\bar{\theta}\)
2:for\(t=0,1,\cdots,T-1\)do
3: Compute the soft Q-function \(Q^{\text{soft}}_{\theta_{t},\pi_{t}}\) (defined in Appendix B.1) under the current reward function \(r_{\theta_{t}}\) and policy \(\pi_{t}\)
4: Update \(\pi_{t+1}(a|s)\propto\exp(Q^{\text{soft}}_{\theta_{t},\pi_{t}}(s,a))\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\)
5: Roll out policy \(\pi_{t+1}\) twice: one starting from \(s_{0}^{E}\) to get \(s_{0}^{E},a_{0}^{\prime},s_{1}^{\prime},a_{1}^{\prime},\cdots\), and the other starting from \((s_{t}^{E},a_{t}^{E})\) to get \(s_{t+1}^{\prime\prime},a_{t+1}^{\prime\prime},\cdots\)
6: Compute \(g_{t}=\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s_{i}^{ \prime},a_{i}^{\prime})-\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_ {t}}(s_{i}^{\prime\prime},a_{i}^{\prime\prime})+\frac{\lambda(1-\gamma^{t+1}) }{1-\gamma}(\theta_{t}-\bar{\theta})\) where \(s_{0}^{\prime}=s_{0}^{E}\) and \((s_{i}^{\prime\prime},a_{i}^{\prime\prime})=(s_{i}^{E},a_{i}^{E})\) for \(0\leq i\leq t\)
7: Update \(\theta_{t+1}=\theta_{t}-\alpha_{t}g_{t}\)
8:endfor ```

**Algorithm 1** Meta-regularized In-trajectory Inverse Reinforcement Learning (MERIT-IRL)

**Policy update** (lines 3-4 of Algorithm 1). At each time \(t\), we only partially solve the lower-level problem (3) via one-step soft policy iteration [5; 34]. In specific, the soft policy iteration contains two steps: policy evaluation and policy improvement. Policy evaluation aims to compute the soft \(Q\)-function \(Q^{\text{soft}}_{\theta_{t},\pi_{t}}\) (see the expression in Appendix B.1) under the current learned reward function \(r_{\theta_{t}}\) and learned policy \(\pi_{t}\). Policy improvement aims to update policy according to \(\pi_{t+1}(s,a)\propto\exp(Q^{\text{soft}}_{\theta_{t},\pi_{t}}(s,a))\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\). In practical implementations, \(\pi_{t+1}\) can be obtained by one-step policy update in soft Q-learning [34] or one-step actor update in soft actor-critic [35].

**Reward update** (lines 5-7 of Algorithm 1). At each time \(t\), the algorithm observes \((s_{t}^{E},a_{t}^{E})\) and aims to leverage all the previously observed data to update the reward parameter. In specific, as \(L_{t}(\theta;(s_{t}^{E},a_{t}^{E}))=-\gamma^{t}\log\pi_{\theta}(a_{t}^{E}|s_{t}^ {E})+\frac{\lambda\gamma^{t}}{2}||\theta-\bar{\theta}||^{2}\) is the meta-regularized negative log-likelihoodof \((s_{t}^{E},a_{t}^{E})\), the algorithm can formulate \(\sum_{i=0}^{t}L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))\) at time \(t\) using all the previously collected data (i.e., \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0,\dots,t}\)). To update the reward parameter, the algorithm partially minimizes \(\sum_{i=0}^{t}L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))\) via one-step gradient descent.

**Lemma 1**.: _The gradient of \(\sum_{i=0}^{t}L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))\) can be calculated as follows:_

\[\nabla\sum_{i=0}^{t}L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))=E_{S,A}^{ \pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}\tau_{\theta }(S_{i},A_{i})\bigg{|}S_{0}=s_{0}^{E}\bigg{]}+\frac{\lambda(1-\gamma^{t+1})}{1- \gamma}(\theta-\bar{\theta})\] \[-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}\tau_{\theta}(s_{i}^{E}, a_{i}^{E})-E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=t+1}^{\infty}\gamma^{i}\nabla_{ \theta}\tau_{\theta}(S_{i},A_{i})\bigg{|}S_{t}=s_{t}^{E},A_{t}=a_{t}^{E}\bigg{]}.\] (4)

Note that the gradient (4) holds for continuous state-action space. Since the gradient (4) has expectation terms under the policy \(\pi_{\theta}\), we can only approximate it. In specific, we roll out the policy \(\pi_{t+1}\) twice: one starting from \(s_{0}^{E}\) to get a trajectory \(\{(s_{i}^{\prime},a_{i}^{\prime})\}_{i\geq 0}\) where \(s_{0}^{\prime}=s_{0}^{E}\), and the other one starting from \((s_{t}^{E},a_{t}^{E})\) to get a trajectory \(\{(s_{i}^{\prime\prime},a_{i}^{\prime\prime})\}_{i\geq 0}\) where \((s_{i}^{\prime\prime},a_{i}^{\prime\prime})=(s_{i}^{E},a_{i}^{E})\) for \(0\leq i\leq t\). Then we use the empirical estimate \(g_{t}=\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}\tau_{\theta_{t}}(s_{i}^{ \prime},a_{i}^{\prime})-\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}\tau_{ \theta_{t}}(s_{i}^{\prime\prime},a_{i}^{\prime\prime})+\frac{\lambda(1-\gamma^ {t+1})}{1-\gamma}(\theta_{t}-\bar{\theta})\) to approximate \(\nabla\sum_{i=0}^{t}L_{i}(\theta_{t};(s_{i}^{E},a_{i}^{E}))\). With the gradient approximation \(g_{t}\), we utilize stochastic online gradient descent \(\theta_{t+1}=\theta_{t}-\alpha_{t}g_{t}\) to update the reward parameter.

**Discussion on our special design of the reward update**. The right subfigure in Figure 1 visualizes our reward update (modulo the meta-regularization term). The green trajectory (i.e., \(\{(s_{t}^{E},a_{t}^{E})\}_{t\geq 0}\)) is the expert trajectory, and the red trajectories (i.e., \(\{(s_{t}^{\prime},a_{t}^{\prime})\}_{t\geq 0}\) and \(\{(s_{t}^{\prime\prime},a_{i}^{\prime\prime})\}_{i\geq t}\)) are the trajectories generated by the learned policy. Given the expert trajectory prefix (i.e., the incomplete trajectory \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) observed so far), our method completes the expert trajectory by rolling out the learned policy starting from \((s_{t}^{E},a_{t}^{E})\) and filling the trajectory suffix \(\{(s_{i}^{\prime\prime},a_{i}^{\prime\prime})\}_{i>t}\). The combined complete trajectory includes the expert trajectory prefix \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) and the learner-filled trajectory suffix \(\{(s_{i}^{\prime\prime},a_{i}^{\prime\prime})\}_{i\geq t}\). We update the reward function by comparing this combined trajectory to a complete trajectory \(\{(s_{t}^{\prime},a_{i}^{\prime})\}_{t\geq 0}\) generated by the learned policy starting from the expert's initial state \(s_{0}^{E}\).

A more straightforward way for the reward update is to directly compare the trajectory prefixes (visualized in the middle of Figure 1) at each time \(t\).

However, this naive method can be problematic. We explain the issue of this naive method and the advantage of our method in the following context. Figure 1 visualizes the reward update for standard IRL (left), the naive method (i.e., directly run standard IRL algorithms on the expert trajectory prefix) (middle), and our method (right). The standard IRL (left) updates the reward function by comparing the complete expert trajectory and the complete trajectory generated by the learned policy. This case is ideal, however, it is infeasible when the trajectory is ongoing and we can only observe an incomplete expert trajectory \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) at each time \(t\). The naive method also computes the complete trajectory prefix \(\{(s_{i}^{E},a_{i}^{E})\}_{i=0}^{t}\) and the complete trajectory \(\{(s_{i}^{\prime},a_{i}^{\prime})\}_{i=0}^{t}\) generated by the learned policy. The comparison between the learner prefix \(\{s_{i}^{\prime},a_{i}^{\prime}\}_{i=0}^{t}\) and expert

Figure 1: Standard IRL (left), the naive method for trajectory learning (middle), and our method (right).

prefix \(\{s_{i}^{E},a_{i}^{E}\}_{i=0}^{t}\) encourages the learned reward function to explain the expert's demonstrated behaviors so far, and the comparison between the suffixes (\(\{s_{i}^{\prime},a_{i}^{\prime}\}_{i>t}\) and \(\{s_{i}^{\prime\prime},a_{i}^{\prime\prime}\}_{i>t}\)) encourages that we are learning a reward function that is useful for predicting the future. Note that as \(t\) increases, the expert trajectory prefix weights more and more in the combined complete trajectory, and eventually we will recover the standard IRL reward update when \(t\) goes to infinity. In Theorems 1 and 2, we theoretically guarantee that the proposed reward update can achieve sub-linear (local) regret. This shows the perfect consistency between the intuition and theory.

### Theoretical analysis

To quantify the local regret of Algorithm 1, we have two challenges: (i) Since we only update \(\pi\) by one step at each time \(t\), we have \(\pi_{t+1}\) instead of the optimal solution \(\pi_{\theta_{t}}\) of the lower-level problem (3). The policy \(\pi_{t+1}\) can be far away from \(\pi_{\theta_{t}}\) and thus the empirical gradient estimate \(g_{t}\) can be a bad approximation of the gradient (4). (ii) Since we only update \(\theta\) once at each time \(t\) instead of finding a near-stationary point \(\theta^{\prime}\) such that \(||\sum_{i=0}^{t}L_{i}(\theta^{\prime};(s_{i}^{E},a_{i}^{E}))||\leq\epsilon\) as in [26; 27], the gradient norm \(||\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(s_{i}^{E},a_{i}^{E}))||\) is not stabilized under the threshold \(\epsilon\) at every time \(t\). Therefore the local regret (i.e., \(\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(s_{i}^{E },a_{i}^{E}))||^{2}\)) is hard to quantify and may not be sub-linear in \(T\). What's worse, the input data is not i.i.d. but correlated, i.e., the input data \((s_{t}^{E},a_{t}^{E})\) at time \(t\) is affected by the input data \((s_{t-1}^{E},a_{t-1}^{E})\) at last step. This correlation makes it even more difficult to quantify the local regret.

To solve the first challenge, we adopt the idea of two-timescale stochastic approximation [28] where the lower level updates in a faster timescale and the upper level updates in a slower timescale. The policy update is faster because it converges linearly under a fixed reward function [36] while the reward update is slower given that we choose \(\alpha_{t}\propto(t+1)^{-1/2}\). Intuitively, since the policy update is faster than the reward update, the reward parameter is "relatively fixed" compared to the policy. It is expected that \(\pi_{t+1}\) shall stay close to \(\pi_{\theta_{t}}\) and at last converges to \(\pi_{\theta_{t}}\) when \(t\) increases.

To solve the second challenge, we divide our analysis into two steps: (i) We quantify the difference of the gradient norms between the current correlated state-action distribution \(\mathbb{P}_{t}^{\pi_{E}}(\cdot,\cdot)\) and a stationary state-action distribution for any loss function \(L_{i}\), \(i\geq 0\). Note that \(\mathbb{P}_{t+1}^{\pi_{E}}(\cdot,\cdot)\) is affected by \(\mathbb{P}_{t}^{\pi_{E}}(\cdot,\cdot)\). (ii) We quantify the local regret under the stationary distribution. The benefit of doing so is that the input data is i.i.d. under the stationary distribution, and thus we can cast the online gradient descent method as a stochastic gradient descent method and quantify its local regret. Finally, we can quantify the local regret under the current correlated distribution \(\mathbb{P}_{t}^{\pi_{E}}(\cdot,\cdot)\) by combining (i) and (ii).

We start our analysis with the definitions of stationary state distribution and stationary state-action distribution. For a given policy \(\pi\), the corresponding stationary state distribution is \(\mu^{\pi}(s)\triangleq(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}_{t}^{ \pi}(s)\) and the stationary state-action distribution is \(\mu^{\pi}(s,a)\triangleq(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}_{t} ^{\pi}(s,a)\).

**Assumption 1**.: _The parameterized reward function \(r_{\theta}\) satisfies \(|r_{\theta_{t}}(s,a)-r_{\theta_{2}}(s,a)|\leq\tilde{C}_{r}||\theta_{1}-\theta _{2}||\) and \(||\nabla_{\theta}r_{\theta_{1}}(s,a)-\nabla_{\theta}r_{\theta_{2}}(s,a)||\leq \tilde{C}_{r}||\theta_{1}-\theta_{2}||\) for any \((\theta_{1},\theta_{2})\) and any \((s,a)\in\mathcal{S}\times\mathcal{A}\) where \(\tilde{C}_{r}\) and \(\tilde{C}_{r}\) are positive constants._

**Assumption 2** (Ergodicity).: _There exist constants \(C_{M}>0\) and \(\rho\in(0,1)\) such that for any policy \(\pi\) and any \(t\geq 0\), the following holds for the Markov chain induced by the policy \(\pi\) and the state transition function \(P\): \(\sup_{S_{0}\sim P_{0}}d_{\text{TV}}(\mathbb{P}_{t}^{\pi}(\cdot),\mu^{\pi}(\cdot) )\leq C_{M}\rho^{t}\) where \(d_{\text{TV}}(\mathbb{P}_{1}(\cdot),\mathbb{P}_{2}(\cdot))\triangleq\frac{1} {2}\int_{s\in\mathcal{S}}|\mathbb{P}_{1}(s)-\mathbb{P}_{2}(s)|ds\) is the total variation distance between the two state distributions \(\mathbb{P}_{1}\) and \(\mathbb{P}_{2}\), \(\tilde{S}_{0}\) is the initial state, and \(\mathbb{P}_{t}^{\pi}(\cdot)\) is the state distribution induced by the policy \(\pi\) at time \(t\)._

Assumptions 1-2 are common in RL [37; 38; 39; 40]. Assumption 2 holds for any time-homogeneous Markov chain with finite state space or any uniformly ergodic Markov chain with general state space.

**Proposition 1**.: _Suppose Assumptions 1-2 hold and \(\alpha_{t}\in(0,\frac{1-\gamma}{\lambda})\), we have the following relation for any \(i\geq 0\) and any \(\theta_{t},t\geq 0\):_

\[\big{|}E_{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot, \cdot)}\big{[}||\nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\big{]}-E_{( S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)}\big{[}||\nabla L_{i}( \theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\big{]}\big{|},\] \[\leq 8C_{M}\tilde{C}_{r}^{2}\big{(}\frac{2-\gamma}{1-\gamma}\big{)}^{ 2}\rho^{i}\gamma^{2i},\]

_where \((S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot,\cdot)\) means that \((S_{i}^{E},A_{i}^{E})\) is drawn from the correlated distribution \(\mathbb{P}_{i}^{\pi_{E}}(\cdot,\cdot)\) and \((S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\) means that \((S_{i}^{E},A_{i}^{E})\) is drawn from the stationary distribution \(\mu^{\pi_{E}}(\cdot,\cdot)\)._Proposition 1 quantifies the gap of gradient norms between the current correlated distribution \(\mathbb{P}_{i}^{\pi_{E}}\) and the stationary distribution \(\mu^{\pi_{E}}\). We next quantify the local regret under the stationary distribution \(\mu^{\pi_{E}}\) with the following lemma:

**Lemma 2**.: _Suppose Assumptions 1-2 hold and choose \(\alpha_{t}=\frac{(1-\gamma)(t+1)^{-1/2}}{\lambda}\), it holds that:_

\[E_{\{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{t\geq 0 }}\bigg{[}\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L_{i}(\theta_{t} ;(S_{i}^{E},A_{i}^{E}))||^{2}\bigg{]}\] \[\leq D_{1}(\log T+1)+D_{2}\sqrt{T}+D_{3}\sqrt{T}(\log T+1),\]

_where \(D_{1}\), \(D_{2}\), and \(D_{3}\) are positive constants whose expressions can be found in Appendix B.4._

Lemma 2 quantifies the local regret under the stationary distribution \(\mu^{\pi_{E}}\). With Proposition 1 and Lemma 2, we can quantify the local regret under the current correlated distribution.

**Theorem 1**.: _Suppose Assumptions 1-2 hold and choose \(\alpha_{t}=\frac{(1-\gamma)(t+1)^{-1/2}}{\lambda}\), we have that:_

\[E_{\{(S_{t}^{E},A_{t}^{E})\sim\mathbb{P}_{t}^{\pi_{E}}(\cdot, \cdot)\}_{t\geq 0}}\bigg{[}\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t} \nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E}))||^{2}\bigg{]}\] \[\leq\bigg{(}D_{1}+\frac{8C_{M}\bar{C}_{r}^{2}(2-\gamma)^{2}}{(1- \rho\gamma^{2})(1-\gamma)^{2}}\bigg{)}(\log T+1)+D_{2}\sqrt{T}+D_{3}\sqrt{T}( \log T+1).\]

Theorem 1 is based on Proposition 1 and Lemma 2. It shows that Algorithm 1 achieves sub-linear local regret. Moreover, if the reward function is linear, Algorithm 1 achieves sub-linear regret:

**Theorem 2**.: _Suppose the expert reward function \(r_{E}\) and the parameterized reward \(r_{\theta}\) are linear, and Assumptions 1-2 hold. Choose \(\alpha_{t}=\frac{1-\gamma}{\lambda(t+1)(1-\gamma^{t+1})}\) we have that:_

\[E_{\{(S_{t}^{E},A_{t}^{E})\sim\mathbb{P}_{t}^{\pi_{E}}(\cdot, \cdot)\}_{t\geq 0}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta_{t};(S_{t}^{E},A_{t}^{E})) \bigg{]}\] \[-\min_{\theta}E_{\{(S_{t}^{E},A_{t}^{E})\sim\mathbb{P}_{t}^{\pi_ {E}}(\cdot,\cdot)\}_{t\geq 0}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta;(S_{t}^{E},A_{ t}^{E}))\bigg{]}\leq D_{4}+D_{5}(\log T+1),\]

_where \(D_{4}\) and \(D_{5}\) are positive constants whose expressions are in Appendix B.6._

### Meta-Regularization

Since there is only one training trajectory and this trajectory is not complete during the learning process, we need to add a regularization term to avoid overfitting. Inspired by humans' using relevant experience to help do inference, we introduce the meta-regularization \(\frac{\lambda}{2}||\theta-\bar{\theta}||^{2}\) where \(\lambda\) is the hyper-parameter and the meta-prior \(\bar{\theta}\) is learned from "relevant experience". In specific, we introduce a set of relevant tasks \(\{\mathcal{T}_{j}\}_{j\sim P_{\mathcal{T}}}\) where each task \(\mathcal{T}_{j}\) is an IRL problem and \(P_{\mathcal{T}}\) is the implicit task distribution. The tasks \(\{\mathcal{T}_{j}\}_{j\sim P_{\mathcal{T}}}\) are relevant in the sense that they share the components \((\mathcal{S},\mathcal{A},\gamma,P_{0},P)\) of the MDP with our in-trajectory learning problem. However, the expert's reward functions of different tasks are different and are drawn from an unknown reward function distribution. For example, in the stock market case mentioned in the introduction, the experts of different tasks invest in the same stock market but may have different preferences. As standard in meta-learning [41; 42; 43], we assume that the expert's reward function \(r_{E}\) of our in-trajectory learning problem is also drawn from the same unknown reward function distribution. Note that the reward functions of the relevant tasks \(\{\mathcal{T}_{j}\}_{j\sim P_{\mathcal{T}}}\) are different from \(r_{E}\) even if they are drawn from the same unknown reward function distribution.

For each task \(\mathcal{T}_{j}\), there is a batch of trajectories and we divide this batch into two sets, i.e., \(\mathcal{D}_{j}^{\text{tr}}\) and \(\mathcal{D}_{j}^{\text{eval}}\). The training set \(\mathcal{D}_{j}^{\text{tr}}\) only has one trajectory, just as our in-trajectory learning problem, and the evaluation set \(\mathcal{D}_{j}^{\text{eval}}\) has abundant trajectories. Define the loss function on a certain data set \(\mathcal{D}\triangleq\{\zeta^{v}\}_{v=1}^{m}\) as \(L(\theta,\mathcal{D})\triangleq-\sum_{v=1}^{m}\sum_{t=0}^{\infty}\gamma^{t} \log\pi_{\theta}(a_{t}^{v}|s_{t}^{v})\). The goal of each task \(\mathcal{T}_{j}\) is to learn a task-specific adaptation \(\phi_{j}\) using the training set \(\mathcal{D}_{j}^{\text{tr}}\), such that \(\phi_{j}\) can minimize the test loss \(L(\phi_{j},\mathcal{D}_{j}^{\text{eval}})\) on the evaluation set \(\mathcal{D}_{j}^{\text{eval}}\). The goal of meta-regularization is to find a meta-prior \(\bar{\theta}\), from which such task-specific adaptations \(\phi_{j}\) can be adapted to all tasks \(\{\mathcal{T}_{j}\}_{j\sim P_{\mathcal{T}}}\). In specific, meta-regularization [25] proposes a bi-level optimization problem (5). The lower-level problem uses only one trajectory \(\mathcal{D}_{j}^{\text{tr}}\) to find the task-specific adaptation \(\phi_{j}\) such that the meta-regularized loss function \(L(\phi,\mathcal{D}_{j}^{\text{tr}})+\frac{\lambda}{2(1-\gamma)}||\phi-\bar{ \theta}||^{2}\) is minimized. The upper-level problem is to find a meta-prior \(\bar{\theta}\) such that the corresponding task-specific adaptations \(\{\phi_{j}\}_{j\sim P_{\mathcal{T}}}\) can minimize the expected loss function \(L(\phi_{j},\mathcal{D}_{j}^{\text{eval}})\) over the evaluation sets of all tasks \(\{\mathcal{T}_{i}\}_{j\sim P_{\mathcal{T}}}\).

\[\min_{\bar{\theta}}\;E_{j\sim P_{\mathcal{T}}}\big{[}L(\phi_{j},\mathcal{D}_{j }^{\text{eval}})\big{]},\quad\text{s.t. }\phi_{j}=\operatorname*{arg\,min}_{\phi}L(\phi,\mathcal{D}_{j}^{\text{tr}}) +\frac{\lambda}{2(1-\gamma)}||\phi-\bar{\theta}||^{2}.\] (5)

The lower-level loss function in (5) is the offline version of our in-trajectory loss function (1) (i.e., \(L(\phi,\mathcal{D}_{j}^{\text{tr}})+\frac{\lambda}{2(1-\gamma)}||\phi-\bar{ \theta}||^{2}=\sum_{t=0}^{\infty}L_{t}(\phi;(s_{t}^{\text{tr}},a_{t}^{\text{tr }}))\)) where \((s_{t}^{\text{tr}},a_{t}^{\text{tr}})\in\mathcal{D}_{j}^{\text{tr}}\). Our in-trajectory learning problem can also be regarded as to find a task-specific adaptation. Note that the in-trajectory learning problem is online while the lower-level problem in (5) is offline because the in-trajectory problem is ongoing where we keep observing new state-action pairs. In contrast, the lower-level problem in (5) is based on "experience" that has already happened. Due to the space limit, we include the algorithm and theoretical guarantees of solving the problem (5) in Appendix C.

## 5 Experiments

We present three experiments to show the effectiveness of MERIT-IRL. We use four baselines for comparisons. (i) **IT-IRL**: this method is MERIT-IRL without meta-regularization. (ii) **Naive MERIT-IRL**: this method has the meta-regularization term but uses the naive way (in the middle of Figure 1) to update reward. (iii) **Naive IT-IRL**: this method uses the naive way to update the learned reward and does not have the meta-regularization term. (iv) **Hindsight**: this method is meta-regularized ML-IRL [5] which can access the complete expert trajectory and uses the standard IRL (visualized in the left of Figure 1) with meta-regularization to update the learned reward. The experiment details are in Appendix D.

### MuJoCo experiment

In this subsection, we consider the target velocity problem for three MuJoCo robots: HalfCheetah, Walker, and Hopper. The target velocity problem is widely used in meta-RL [44] and meta-IRL [45]. In specific, the robots aim to maintain a target velocity in each task and the target velocity of different tasks is different. To test the performance of MERIT-IRL, we use \(10\) test tasks whose target velocity is randomly between \(1.5\) and \(2.0\). In the test tasks, there is only one expert trajectory and the state-action pairs of this trajectory are sequentially revealed (to MERIT-IRL, IT-IRL, Naive MERIT-IRL, and Naive IT-IRL) in an online fashion. The baseline Hindispit uses the complete expert trajectory to learn a reward function. The ground truth reward is designed as \(-|v-v_{\text{target}}|\) (as in [44]) where \(v\) is the current robot velocity and \(v_{\text{target}}\) is the target velocity. To learn the meta-prior \(\bar{\theta}\), we use \(50\) relevant tasks whose target velocity is randomly between \(0\) and \(3\).

Figures 1(a)-1(c) show the in-trajectory learning performance where the \(x\)-axis is the time step \(t\) of the expert trajectory and the \(y\)-axis is the cumulative reward of the learned policy \(\pi_{t}\) when only the first \(t\) steps of the expert trajectory are observed. The \(x\)-limit is \(1,000\) because the trajectory length in MuJoCo is \(1,000\). Note that the baseline "Hindsight" is not in-trajectory learning since it learns from a complete expert trajectory. For comparison, we use two horizontal lines (close to each other) to show the performance of Hindsight and the expert in the figures. Figure 1(a) shows that MERIT-IRL achieves similar performance with the expert when only \(40\%\) of the complete expert trajectory (\(t=400\)) is observed while IT-IRL can only achieve performance close to the expert after observing more than \(90\%\) of the complete expert trajectory (\(t=900\)). This shows the effectiveness of the meta-regularization. Naive MERIT-IRL and Naive IT-IRL fail to imitate the expert even if the complete expert trajectory is observed (\(t=1,000\)). This shows the effectiveness of our special design of the reward update. The discussions on Figures 1(b) and 1(c) are in Appendix D.2.

Table 1 shows the results after observing the complete expert trajectory. MERIT-IRL performs much better than IT-IRL, Naive MERIT-IRL, and Naive IT-IRL. MERIT-IRL achieves similar performance

with Hindsight and expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since Hindsight uses the complete expert trajectory to learn.

### Stock market experiment

RL to train a stock trading agent has been widely studied in AI for finance [46; 47; 48]. In this experiment, we use IRL to learn the reward function (i.e., investing preference) of the target investor in a stock market scenario. In specific, we use the real-world data of \(30\) constituent stocks in Dow Jones Industrial Average from 2021-01-01 to 2022-01-01. We use a benchmark called "FinRL" [48] to configure the real-world stock data into an MDP environment. The target investor (i.e., expert) has an initial asset of \(\$1,000\) and trades stocks on every stock market opening day. The stock market opens 252 days between 2021-01-01 and 2022-01-01, and thus the trajectory length is 252. The reward function of the target investor is defined as \(p_{1}-p_{2}\) where \(p_{1}\) is the investor's profit which is the money earned from trading stocks subtracting the transaction cost, and \(p_{2}\) models the investor's preference of whether willing to take risks. In specific, \(p_{2}\) is positive if the investor buys stocks whose turbulence indices are larger than a certain turbulence threshold, and zero otherwise. The value of \(p_{2}\) depends on the type and amount of the trading stocks. The turbulence thresholds of different investors are different. The turbulence index measures the price fluctuation of a stock. If the turbulence index is high, the corresponding stock has a high fluctuating price and thus is risky to buy [48]. Therefore, an investor unwilling to take risks has a relatively low turbulence threshold. We include experiment details in Appendix D.3. To test performance, we use \(10\) test tasks whose turbulence thresholds are randomly between \(45\) and \(50\). To learn \(\bar{\theta}\), we use \(50\) relevant tasks whose turbulence thresholds are randomly between \(30\) and \(60\).

Figure 2d shows that MERIT-IRL achieves similar cumulative reward with the expert at \(t=140\) which is less than \(60\%\) of the whole trajectory, while the three in-trajectory baselines fail to imitate the expert before the ongoing trajectory terminates. The last row in Table 1 shows that MERIT-IRL achieves similar performance with Hindsight and the expert. More discussions on the results are in Appendix D.3.

### Learning from a shooter's ongoing trajectory

This part presents the experiment of learning from an ongoing shooter trajectory. Following [12], we model the shooter's movement as a navigation problem. We build a simulator in Gazebo (Figure 2(a)) where the shooter moves from the door (lower left corner) to the red target (upper right corner). The learner observes the ongoing trajectory of the shooter and keeps updating the learned reward and policy. In our case, the complete shooter trajectory has the length of \(140\). Figures 2(b)-2(g) show our in-trajectory learning performance where the heat maps visualize the learned reward. We normalize the learned reward to \([0,1]\). We can observe that as the ongoing trajectory is expanding, the learned reward function becomes more and more precise to locate the goal area. When \(t=40\), we cannot tell

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \multicolumn{2}{|c|}{MERIT-IRL} & TRIL & Naive MERIT-IRL & \multicolumn{2}{c|}{MERIT-IRL} & Hindsight & Expert \\ \hline HalfCheath & \(-214.63\pm 53.96\) & \(-386.78\pm 152.05\) & \(-548.22\pm 0.51\) & \(-765.27\pm 104.1\) & \(-208.74\pm 37.23\) & \(-181.51\pm 28.35\) \\ \hline Walker & \(-654.77\pm 102.59\) & \(-891.79\pm 156.90\) & \(-962.42\pm 111.60\) & \(-1349.25\pm 158.88\) & \(-681.71\pm 157.92\) & \(-634.17\pm 120.57\) \\ \hline Hopper & \(-476.72\pm 32.09\) & \(-669.88\pm 53.63\) & \(-691.03\pm 93.35\) & \(-112.06\pm 74.33\) & \(-455.70\pm 74.93\) & \(-421.74\pm 84.30\) \\ \hline Stock market & \(386.70\pm 62.95\) & \(266.81\pm 68.61\) & \(192.49\pm 75.34\) & \(72.33\pm 16.73\) & \(390.30\pm 77.37\) & \(403.15\pm 61.94\) \\ \hline \end{tabular}
\end{table}
Table 1: Experiment results. The mean and standard deviation are calculated from \(10\) test tasks.

Figure 2: In-trajectory learning performance.

the goal area from the heat map (Figure 2(b)). However, as the time \(t\) grows, we can almost locate the goal area when \(t=60\) (Figure 2(c)) and precisely locate the goal area when \(t=80\) (Figure 2(d)).

Figure 2(h) shows the policy learning performance. Since there is no ground truth reward in this problem, we use "success rate" to quantify the performance of the learned policy. The success rate is the rate that the learned policy successfully reaches the goal. From 2(h), we can see that MERIT-IRL outperforms the other baselines and can achieve \(100\%\) success rate when \(t=80\) (i.e., only observing \(57\%\) of the complete trajectory). Note that we do not include Hindsight and Expert in Figure 3 since they both achieve \(100\%\) success rate.

## 6 Conclusion

This paper proposes MERIT-IRL, the first in-trajectory inverse reinforcement learning theoretical framework that learns a reward function while observing an initial portion of a trajectory and keeps updating the learned reward function when extended portions (i.e., new state-action pairs) of the trajectory are observed. Experiments show that MERIT-IRL can imitate the expert from the ongoing expert trajectory before it terminates.

## 7 Acknowledgements

This work is partially supported by the National Science Foundation through grants ECCS 1846706 and ECCS 2140175. We would like to thank the reviewers for their insightful and constructive suggestions.

Figure 3: Learning performance on the active shooting scenario.

## References

* [1] P. Abbeel and A. Y. Ng, "Apprenticeship learning via inverse reinforcement learning," in _International Conference on Machine Learning_, pp. 1-8, 2004.
* [2] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, "Maximum margin planning," in _International Conference on Machine Learning_, pp. 729-736, 2006.
* [3] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, "Maximum entropy inverse reinforcement learning," in _National Conference on Artificial intelligence_, pp. 1433-1438, 2008.
* [4] B. D. Ziebart, J. A. Bagnell, and A. K. Dey, "Modeling interaction via the principle of maximum causal entropy," in _International Conference on Machine Learning_, pp. 1255-1262, 2010.
* [5] S. Zeng, C. Li, A. Garcia, and M. Hong, "Maximum-likelihood inverse reinforcement learning with finite-time guarantees," in _Advances in Neural Information Processing Systems_, 2022.
* [6] S. Liu and M. Zhu, "Distributed inverse constrained reinforcement learning for multi-agent systems," _Advances in Neural Information Processing Systems_, vol. 35, pp. 33444-33456, 2022.
* [7] D. Ramachandran and E. Amir, "Bayesian inverse reinforcement learning.," in _International Joint Conference on Artificial Intelligence_, pp. 2586-2591, 2007.
* [8] A. J. Chan and M. van der Schaar, "Scalable bayesian inverse reinforcement learning," in _International Conference on Learning Representations_, 2021.
* [9] N. Rhinehart and K. M. Kitani, "First-person activity forecasting with online inverse reinforcement learning," in _IEEE International Conference on Computer Vision_, pp. 3696-3705, 2017.
* [10] S. Arora, P. Doshi, and B. Banerjee, "Online inverse reinforcement learning under occlusion," in _International Conference on Autonomous Agents and Multiagent Systems_, pp. 1170-1178, 2019.
* [11] S. Liu and M. Zhu, "Learning multi-agent behaviors from distributed and streaming demonstrations," _Advances in Neural Information Processing Systems_, vol. 36, 2024.
* [12] A. Aghalari, N. Morshedlou, M. Marufuzzaman, and D. Carruth, "Inverse reinforcement learning to assess safety of a workplace under an active shooter incident," _IISE Transactions_, vol. 53, no. 12, pp. 1337-1350, 2021.
* [13] Q. Sun, X. Gong, and Y.-W. Si, "Transaction-aware inverse reinforcement learning for trading in stock markets," _Applied Intelligence_, vol. 53, no. 23, pp. 28186-28206, 2023.
* [14] J. Chang and W. Tu, "A stock-movement aware approach for discovering investors' personalized preferences in stock markets," in _International Conference on Tools with Artificial Intelligence_, pp. 275-280, 2018.
* [15] X. Hu, Y. Chen, L. Ren, and Z. Xu, "Investor preference analysis: An online optimization approach with missing information," _Information Sciences_, vol. 633, pp. 27-40, 2023.
* [16] Y. Xu, W. Gao, and D. Hsu, "Receding horizon inverse reinforcement learning," _Advances in Neural Information Processing Systems_, vol. 35, pp. 27880-27892, 2022.
* [17] G. Swamy, D. Wu, S. Choudhury, D. Bagnell, and S. Wu, "Inverse reinforcement learning without reinforcement learning," in _International Conference on Machine Learning_, pp. 33299-33318, 2023.
* [18] K. Yan, A. Schwing, and Y.-X. Wang, "A simple solution for offline imitation from observations and examples with possibly incomplete trajectories," _Advances in Neural Information Processing Systems_, 2024.
* [19] M. Sun and X. Ma, "Adversarial imitation learning from incomplete demonstrations," in _International Joint Conference on Artificial Intelligence_, pp. 3513-3519, 2019.

* [20] D. Xu, F. Zhu, Q. Liu, and P. Zhao, "Arail: Learning to rank from incomplete demonstrations," _Information Sciences_, vol. 565, pp. 422-437, 2021.
* [21] J. Ho and S. Ermon, "Generative adversarial imitation learning," in _Advances in Neural Information Processing Systems_, pp. 4572-4580, 2016.
* [22] G. Qiao, G. Liu, P. Poupart, and Z. Xu, "Multi-modal inverse constrained reinforcement learning from a mixture of demonstrations," _Advances in Neural Information Processing Systems_, vol. 36, 2024.
* [23] S. Liu and M. Zhu, "Meta inverse constrained reinforcement learning: Convergence guarantee and generalization analysis," in _International Conference on Learning Representations_, 2023.
* [24] G. Qiao, G. Quan, R. Qu, and G. Liu, "Modelling competitive behaviors in autonomous driving under generative world model," in _European Conference on Computer Vision_, 2024.
* [25] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine, "Meta-learning with implicit gradients," in _Advances in Neural Information Processing Systems_, pp. 113-124, 2019.
* [26] E. Hazan, K. Singh, and C. Zhang, "Efficient regret minimization in non-convex games," in _International Conference on Machine Learning_, pp. 1433-1441, 2017.
* [27] N. Hallak, P. Mertikopoulos, and V. Cevher, "Regret minimization in stochastic non-convex learning via a proximal-gradient approach," in _International Conference on Machine Learning_, pp. 4008-4017, 2021.
* [28] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang, "A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic," _arXiv preprint arXiv:2007.05170_, 2020.
* [29] T. Chen, Y. Sun, Q. Xiao, and W. Yin, "A single-timescale method for stochastic bilevel optimization," in _International Conference on Artificial Intelligence and Statistics_, pp. 2466-2488, 2022.
* [30] F. Schafer and A. Anandkumar, "Competitive gradient descent," _Advances in Neural Information Processing Systems_, pp. 7625-7635, 2019.
* [31] V. S. Varma, J. Veetaseveera, R. Postoyan, and I.-C. Morarescu, "Distributed gradient methods to reach a nash equilibrium in potential games," in _IEEE Conference on Decision and Control_, pp. 3098-3103, 2021.
* [32] S. Boyd and L. Vandenberghe, _Convex Optimization_. Cambridge university press, 2004.
* [33] J. Zhang, P. Xiao, R. Sun, and Z. Luo, "A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems," _Advances in Neural Information Processing Systems_, pp. 7377-7389, 2020.
* [34] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, "Reinforcement learning with deep energy-based policies," in _International Conference on Machine Learning_, pp. 1352-1361, 2017.
* [35] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor," in _International Conference on Machine Learning_, pp. 1861-1870, 2018.
* [36] S. Cen, C. Cheng, Y. Chen, Y. Wei, and Y. Chi, "Fast global convergence of natural policy gradient methods with entropy regularization," _Operations Research_, vol. 70, no. 4, pp. 2563-2578, 2022.
* [37] Z. Zheng, F. Gao, L. Xue, and J. Yang, "Federated Q-learning: Linear regret speedup with low communication cost," _arXiv preprint arXiv:2312.15023_, 2023.
* [38] H. Gong and M. Wang, "A duality approach for regret minimization in average-award ergodic markov decision processes," in _Conference on Learning for Dynamics and Control_, vol. 120, pp. 862-883, 2020.

* [39] Z. Zheng, H. Zhang, and L. Xue, "Federated Q-learning with reference-advantage decomposition: Almost optimal regret and logarithmic communication cost," _arXiv preprint arXiv:2405.18795_, 2024.
* [40] Z. Zheng, H. Zhang, and L. Xue, "Gap-dependent bounds for Q-learning using reference-advantage decomposition," _arXiv preprint arXiv:2410.07574_, 2024.
* [41] S. Xu and M. Zhu, "Efficient gradient approximation method for constrained bilevel optimization," in _AAAI Conference on Artificial Intelligence_, vol. 37, pp. 12509-12517, 2023.
* [42] S. Xu and M. Zhu, "Meta value learning for fast policy-centric optimal motion planning," in _Robotics science and systems_, 2022.
* [43] S. Xu and M. Zhu, "Online constrained meta-learning: provable guarantees for generalization," _Advances in Neural Information Processing Systems_, vol. 36, 2024.
* [44] C. Finn, P. Abbeel, and S. Levine, "Model-agnostic meta-learning for fast adaptation of deep networks," in _International Conference on Machine Learning_, pp. 1126-1135, 2017.
* [45] S. K. Seyed Ghasemipour, S. S. Gu, and R. Zemel, "Smile: Scalable meta inverse reinforcement learning through context-conditional policies," in _Advances in Neural Information Processing Systems_, pp. 7881-7891, 2019.
* [46] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, "Deep direct reinforcement learning for financial signal representation and trading," _IEEE Transactions on Neural Networks and Learning Systems_, vol. 28, no. 3, pp. 653-664, 2016.
* [47] Z. Zhang, S. Zohren, and S. Roberts, "Deep reinforcement learning for trading," _The Journal of Financial Data Science_, vol. 2, no. 2, pp. 25-40, 2020.
* [48] X.-Y. Liu, H. Yang, J. Gao, and C. D. Wang, "Finrl: Deep reinforcement learning framework to automate trading in quantitative finance," in _ACM International Conference on AI in Finance_, pp. 1-9, 2021.
* [49] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, "Human behavior modeling with maximum entropy inverse optimal control.," in _AAAI Spring Symposium: Human Behavior Modeling_, vol. 92, 2009.
* [50] M. Monfort, A. Liu, and B. D. Ziebart, "Intent prediction and trajectory forecasting via predictive inverse linear-quadratic regulation," in _AAAI Conference on Artificial Intelligence_, pp. 3672-3678, 2015.
* [51] S. Gaurav and B. Ziebart, "Discriminatively learning inverse optimal control models for predicting human intentions," in _International Conference on Autonomous Agents and MultiAgent Systems_, pp. 1368-1376, 2019.
* [52] W. Krichene, M. Balandat, C. Tomlin, and A. Bayen, "The hedge algorithm on a continuum," in _International Conference on Machine Learning_, pp. 824-832, 2015.
* [53] N. Agarwal, A. Gonen, and E. Hazan, "Learning in non-convex games with an optimization oracle," in _Conference on Learning Theory_, pp. 18-29, 2019.
* [54] D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon, "Iq-learn: Inverse soft-q learning for imitation," _Advances in Neural Information Processing Systems_, vol. 34, pp. 4028-4039, 2021.
* [55] M.-F. Balcan, M. Khodak, and A. Talwalkar, "Provable guarantees for gradient-based meta-learning," in _International Conference on Machine Learning_, pp. 424-433, 2019.
* [56] K. Xu, E. Ratner, A. Dragan, S. Levine, and C. Finn, "Learning a prior over intent via meta-inverse reinforcement learning," in _International Conference on Machine Learning_, pp. 6952-6962, 2019.
* [57] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, "Meta-learning in neural networks: A survey," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 44, no. 9, pp. 5149-5169, 2021.

* [58] X.-Y. Liu, Z. Xia, J. Rui, J. Gao, H. Yang, M. Zhu, C. Wang, Z. Wang, and J. Guo, "FinRL-Meta: Market environments and benchmarks for data-driven financial reinforcement learning," _Advances in Neural Information Processing Systems_, vol. 35, pp. 1835-1849, 2022.

This appendix consists of four parts: related works, proof, meta-regularization algorithm and convergence guarantee, and Experiment details.

## Appendix A Related works

**Applying IRL to predict from ongoing trajectories**. Papers [49, 50, 51] use standard IRL to predict goals of incomplete (or ongoing) trajectories. In specific, they first learn the reward function corresponding to each potential goal candidate from complete trajectories in the training phase and then use Bayesian methods to pick the most likely goal candidate of incomplete trajectories in the testing phase. However, these works are not in-trajectory learning since they do not learn a reward function from an incomplete ongoing trajectory.

**Online non-convex optimization**. This paper casts the in-trajectory learning problem as an online non-convex bi-level optimization problem where at each online iteration, a new state-action pair is input. Current literature on online non-convex optimization has two major categories. The first one is to use the regret originally defined in online convex optimization [52, 53]. However, it assumes to find a global optimal solution of a non-convex optimization problem at each online iteration. Therefore, the second category studies "local regret" [26, 27] and uses follow-the-leader-based methods to minimize the local regret. However, the follow-the-leader-based methods need to solve a non-convex optimization problem to obtain a near-stationary point at each online iteration, which can be computationally expensive and time-consuming. If the streaming data arrives at a fast speed, the computation at each online iteration may not be finished before the next data arrives. The computational burden of each online iteration can be mitigated by online gradient descent (OGD) methods where we only partially solve the non-convex optimization problem by one-step gradient descent at each online iteration. While OGD is widely studied in online convex optimization, it is rarely studied in online non-convex optimization. [11] uses OGD to quantify the local regret, however, its analysis only holds when the input data is identically independent distributed (i.i.d.). In contrast, the input data in our problem is not i.i.d. In specific, the input data at time \(t\) (i.e., \((S_{t}^{E},A_{t}^{E})\)) is affected by the input data at last step (i.e., \((S_{t-1}^{E},A_{t-1}^{E})\)). This temporal correlation between any two consecutive input data makes it difficult to analyze the growth rate of the local regret.

**Regularization and meta-learning in IRL**. Moreover, the data of in-trajectory learning is extremely lacking since there is only one demonstrated trajectory and this trajectory is not complete during the learning process. The lack of data can easily lead to overfitting and a common way to alleviate this problem is to use regularizers [21, 54]. Inspired by humans' using relevant experience to help the inference, we introduce a novel regularization method called meta-regularization [25, 55]. Compared to the regularizers commonly used in IRL [21, 54], the meta-regularizer provides human-experience-like prior information which helps recover the reward function from few data. Similar to the meta-initialization method [55, 44] commonly used in IRL [56], meta-regularization provides an initialization that the algorithm starts at. However, more importantly, meta-regularization also provides a regularization term to avoid overfitting.

### Distinction from Maximum-likelihood inverse reinforcement learning (ML-IRL) [5]

We discuss our distinctions from ML-IRL from the following three aspects: problem setting, algorithm design, and theoretical analysis.

**Distinction in problem setting**. We study in-trajectory IRL and formulate an online optimization problem, while ML-IRL studies standard IRL and formulates an offline optimization problem.

**Distinctions in algorithm design**. ML-IRL and our algorithm both update policy and reward in a single loop. However, we propose a novel reward update mechanism specially designed for the in-trajectory learning case. This special design requires to use the current learned policy to complete the expert trajectory, which gives the algorithm the ability to consider for the future. This special design of reward update is novel compared to ML-IRL.

**Distinctions in theoretical analysis**. The analysis in our paper is substantially different from that in ML-IRL due to three facts: (1) The input data in our paper is not i.i.d., while the input data in ML-IRL is i.i.d. (2) We solve an online optimization problem, while ML-IRL solves an offline optimization problem. (3) Our analysis holds for continuous state-action space, while the analysis of ML-IRL is limited to finite state-action space. We now discuss the distinctions in theoretical analysis caused by the three facts in detail.

In our case, the input data is not i.i.d. but temporally correlated, i.e., the input data \((s_{t},a_{t})\) is affected by the input data \((s_{t-1},a_{t-1})\) at last time step. In contrast, the input data in ML-IRL is i.i.d. sampled from a pre-collected data set. To solve this non i.i.d. issue of the input data, we propose a novel theoretical technique that has three steps (detailed in Subsection 4.2). Step 1: We propose the stationary distribution \(\mu^{\pi_{E}}\) and quantify the gradient norm difference between the real distribution \(\mathbb{P}_{t}^{\pi_{E}}\) and this stationary distribution \(\mu^{\pi_{E}}\) in Proposition 1. Step 2: We quantify the local regret over the stationary distribution in Lemma 2. The benefit of doing this is that the input data can be regarded as i.i.d. sampled from this stationary distribution. Step 3: We combine step 1 and step 2, and quantify the (local) regret over the real distribution, where the data is not i.i.d., in Theorem 1 and Theorem 2. We can see that step 1 and step 3 are to solve the non i.i.d. issue of the input data, so that the corresponding theorem statements (Proposition 1, Theorem 1, and Theorem 2) are novel compared to ML-IRL because ML-IRL does not have this non i.i.d. issue. The only theorem statement relevant to ML-IRL is Lemma 2 in step 2 where we both analyze the algorithm over a stationary distribution, and the data is i.i.d. sampled from the stationary distribution.

However, Lemma 2 in step 2 still has significant distinctions from ML-IRL because Lemma 2 quantifies the local regret in the context of online optimization, while ML-IRL quantifies convergence in the context of offline optimization. First, the objective function is dynamically changing in the online setting because the learner observes a new state-action pair at each online iteration, while the objective function is fixed in ML-IRL. Second, the local regret contains the term \(L_{t}(\theta_{t};(s_{t}^{E},a_{t}^{E}))\), however, \(\theta_{t}\) is computed before the learner knows \((s_{t}^{E},a_{t}^{E})\). This makes it more difficult to quantify the local regret because the learner does not know \(L_{t}\) when it computes \(\theta_{t}\). These two difficulties do not appear in the offline optimization in ML-IRL. To solve these two issues, we need to additionally construct a new time-invariant function \(\bar{L}\) in Appendix B.4 and quantify the convergence of the new function \(\bar{L}\). Then, in order to quantify the local regret of \(\{L_{t}\}_{t\geq 0}\), we need to quantify the difference between the real loss function \(\{L_{t}\}_{t\geq 0}\) and the constructed loss function \(\bar{L}\).

Moreover, our theoretical analysis holds for continuous state-action space while the theoretical analysis in ML-IRL is limited to finite state-action space. The extension to continuous state-action space brings new difficulties and requires significant novel analysis. In general, the difficulties stem from two aspects: (1) The constants in ML-IRL, e.g., the smoothness constant of the loss function \(L\) and the coefficient of convergence rate, include the term \(|\mathcal{S}|\times|\mathcal{A}|\). When the state-action space is continuous, those constants are not finite because \(|\mathcal{S}|\times|\mathcal{A}|\) is now infinite. To address this issue, we propose new methods to bound those constants. For example, in order to show that the loss function \(L\) is smooth, rather than using \(||\nabla L(\theta_{1})-\nabla L(\theta_{2})||\) to find the smoothness constant as in ML-IRL, we aim to show that \(||\nabla^{2}L(\theta)||\) is upper bounded by a constant \(C_{L}\) in Lemma A.2 and this constant \(C_{L}\) does not rely on \(|\mathcal{S}|\times|A|\). Given that \(||\nabla^{2}L(\theta)||\leq C_{L}\), the loss function \(L\) is \(C_{L}\)-smooth. (2) Since the action space \(\mathcal{A}\) is finite in ML-IRL, their proved properties of the \(Q\)-function \(Q^{\text{soft}}\) (e.g., Lipschitz continuity, contraction property, monotonic improvement, and smoothness) can be easily extended to the value function \(V^{\text{soft}}\) by summing over different actions \(a\in\mathcal{A}\). When the action space becomes continuous, summing over infinitely many different actions does not preserve those properties. Thus we have to propose new methods to prove those properties of the value function \(V^{\text{soft}}\). In specific, we prove the Lipschitz continuity, contraction property, monotonic improvement, and smoothness of the value function \(V^{\text{soft}}\) in Claims 3-5 in Appendix B.4.

## Appendix B Proof

This section provides the proof of all the proposition, lemmas, and theorems in the paper. To start with, we first introduce the expression of soft \(Q\)-function and soft Bellman policy.

### Notions

The soft \(Q\)-function and soft value function are:

\[Q^{\text{soft}}_{\theta,\pi}(s,a)\triangleq r_{\theta}(s,a)+\gamma\int_{s^{ \prime}\in\mathcal{S}}P(s^{\prime}|s,a)V^{\text{soft}}_{\theta,\pi}(s^{\prime} )ds^{\prime},\]\[V^{\text{soft}}_{\theta,\pi}(s)\triangleq E^{\pi}_{S,A}\bigg{[}\sum_{t=0}^{\infty} \gamma^{t}\big{(}r_{\theta}(S_{t},A_{t})-\log\pi(A_{t}|S_{t})\big{)}\bigg{|}S_{0} =s_{0}^{E}\bigg{]}.\]

The soft Bellman policy is as follows:

\[\pi_{\theta}(a|s) =\frac{\exp(Q^{\text{soft}}_{\theta}(s,a))}{\exp(V^{\text{soft}}_{ \theta}(s))},\] \[Q^{\text{soft}}_{\theta}(s,a) =r_{\theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime }|s,a)V^{\text{soft}}_{\theta}(s^{\prime})ds^{\prime},\] \[V^{\text{soft}}_{\theta}(s) =\log\biggl{(}\int_{a\in\mathcal{A}}\exp(Q^{\text{soft}}_{\theta} (s,a))da\biggr{)}.\]

It has been proved [34] that the soft Bellman policy \(\pi_{\theta}\) is the optimal solution of the lower-level problem (4). We define \(J_{\theta}(s)\triangleq E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}r _{\theta}(S_{t},A_{t})|S_{0}=s]\) as the expected cumulative reward of policy \(\pi_{\theta}\) starting from state \(s\) and \(J_{\theta}(s,a)\triangleq E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t }r_{\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a]\).

**Lemma 3**.: _We have the gradient \(\nabla_{\theta}\log\pi_{\theta}(a|s)=E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty }\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a]-E^{\pi_{ \theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A _{t})|S_{0}=s]\)._

Proof.: Define \(Z_{\theta}(s,a)\triangleq\exp(Q^{\text{soft}}_{\theta}(s,a))\) and \(Z_{\theta}(s)\triangleq\exp(V^{\text{soft}}_{\theta}(s))\), therefore \(Z_{\theta}\) is smooth in \(\theta\) given that it is a composition of logarithmic, exponential, and linear functions of \(r_{\theta}\) and \(r_{\theta}\) is smooth in \(\theta\) (Assumption 1).

\[\nabla_{\theta}\log Z_{\theta}(s) =\frac{\int_{a\in\mathcal{A}}\nabla_{\theta}Z_{\theta}(s,a)da}{Z _{\theta}(s)},\] \[=\int_{a\in\mathcal{A}}\frac{Z_{\theta}(s,a)}{Z_{\theta}(s)} \nabla_{\theta}\log Z_{\theta}(s,a)da,\] \[=\int_{a\in\mathcal{A}}\pi_{\theta}(a|s)\bigg{[}\nabla_{\theta}r_ {\theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)\nabla_{ \theta}\log Z_{\theta}(s^{\prime})ds^{\prime}\bigg{]}da,\] \[=\int_{a\in\mathcal{A}}\pi_{\theta}(a|s)\bigg{[}\nabla_{\theta}r _{\theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)\int_{a^ {\prime}\in\mathcal{A}}\bigg{(}\nabla_{\theta}r_{\theta}(s^{\prime},a^{\prime})\] \[+\gamma\int_{s^{\prime\prime}\in\mathcal{S}}P(s^{\prime\prime}|s^ {\prime},a^{\prime})\nabla_{\theta}\log Z_{\theta}(s^{\prime\prime})ds^{ \prime\prime}\bigg{)}da^{\prime}ds^{\prime}\bigg{]}da.\]

Keep the expansion, we can get \(\nabla_{\theta}\log Z_{\theta}(s)=E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty} \gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s]\) and similarly we can get \(\nabla_{\theta}\log Z_{\theta}(s,a)=E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty} \gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a]\). Thus we have the gradient \(\nabla_{\theta}\log\pi_{\theta}(a|s)=\nabla_{\theta}\log Z_{\theta}(s,a)- \nabla_{\theta}\log Z_{\theta}(s)\). 

### Proof of Lemma 1

Recall that \(\sum_{i=0}^{t}L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))=-\sum_{i=0}^{t}\gamma^{i} \log\pi_{\theta}(a_{i}^{E}|s_{i}^{E})\). When the dynamics \(P\) is deterministic, we have that

\[\nabla\sum_{i=0}^{t}L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))=-\sum_{i=0} ^{t}\gamma^{i}\nabla_{\theta}\log\pi_{\theta}(a_{i}^{E}|s_{i}^{E}),\] \[=-\sum_{i=0}^{t}\gamma^{i}\bigg{[}\nabla_{\theta}Q^{\text{soft}}_{ \theta}(s_{i}^{E},a_{i}^{E})-\nabla_{\theta}V^{\text{soft}}_{\theta}(s_{i}^{E}) \bigg{]},\] \[=-\sum_{i=0}^{t}\gamma^{i}\bigg{[}\nabla_{\theta}r_{\theta}(s_{i}^ {E},a_{i}^{E})+\gamma\nabla_{\theta}V^{\text{soft}}_{\theta}(s_{i+1}^{E})-\nabla_ {\theta}V^{\text{soft}}_{\theta}(s_{i}^{E})\bigg{]},\] \[=-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{\theta}(s_{i}^{E},a_{ i}^{E})-\sum_{i=1}^{t+1}\gamma^{i}\nabla_{\theta}V^{\text{soft}}_{\theta}(s_{i}^{E})+ \sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}V^{\text{soft}}_{\theta}(s_{i}^{E}),\]\[=-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{\theta}(s_{i}^{E},a_{i}^{E})- \gamma^{t+1}\nabla_{\theta}V_{\theta}^{\text{soft}}(s_{t+1}^{E})+\nabla_{\theta} V_{\theta}^{\text{soft}}(s_{0}^{E}),\] \[\stackrel{{(a)}}{{=}}-\sum_{i=0}^{t}\gamma^{i}\nabla_ {\theta}r_{\theta}(s_{i}^{E},a_{i}^{E})-E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i= t+1}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta}(S_{i},A_{i})|S_{i}=s_{t+1}^{E} \bigg{]}\] \[+E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i} \nabla_{\theta}r_{\theta}(S_{i},A_{i})|S_{0}=s_{0}^{E}\bigg{]},\] \[=-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{\theta}(s_{i}^{E},a_{ i}^{E})-E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=t+1}^{\infty}\gamma^{i}\nabla_{ \theta}r_{\theta}(S_{i},A_{i})|S_{t}=s_{t}^{E},A_{t}=a_{t}^{E}\bigg{]}\] \[+E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i} \nabla_{\theta}r_{\theta}(S_{i},A_{i})|S_{0}=s_{0}^{E}\bigg{]},\]

where equality \((a)\) follows from the proof of Lemma 3.

When the dynamics \(P\) is stochastic, we can prove that the above gradient is an unbiased estimate of

\[\nabla E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}( \cdot,\cdot)\}_{i\geq 0}}\bigg{[}\sum_{i=0}^{t}L_{i}(\theta;(S_{i}^{E},A_{i}^{E})) \bigg{]};\] \[=\nabla E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}( \cdot,\cdot)\}_{i\geq 0}}\bigg{[}-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta} \log\pi_{\theta}(A_{i}^{E}|S_{i}^{E})\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot, \cdot)\}_{i\geq 0}}\bigg{[}-\sum_{i=0}^{t}\gamma^{i}[\nabla_{\theta}r_{ \theta}(S_{i}^{E},A_{i}^{E})+\gamma E_{S_{i+1}\sim P(\cdot|S_{i}^{E},A_{i}^{E} )}[\nabla_{\theta}V_{\theta}^{\text{soft}}(S_{i+1})]\] \[-\nabla_{\theta}V_{\theta}^{\text{soft}}(S_{i}^{E})\bigg{]}\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot, \cdot)\}_{i\geq 0}}\bigg{[}-\sum_{i=0}^{t}\gamma^{i}[\nabla_{\theta}r_{ \theta}(S_{i}^{E},A_{i}^{E})+\gamma\nabla_{\theta}V_{\theta}^{\text{soft}}(S_{ i+1}^{E})-\nabla_{\theta}V_{\theta}^{\text{soft}}(S_{i}^{E})]\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot, \cdot)\}_{i\geq 0}}\bigg{[}-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{ \theta}(S_{i}^{E},A_{i}^{E})-\gamma^{t+1}\nabla_{\theta}V_{\theta}^{\text{ soft}}(S_{t+1}^{E})\] \[+E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\nabla _{\theta}r_{\theta}(S_{i},A_{i})|S_{0}=S_{0}^{E}\bigg{]}\bigg{]},\] \[\stackrel{{(b)}}{{=}}E_{\{(S_{i}^{E},A_{i}^{E})\sim \mathbb{P}_{i}^{\pi_{E}}(\cdot,\cdot),\cdot)\}_{i\geq 0}}\bigg{[}-\sum_{i=0}^{t}\gamma^{i} \nabla_{\theta}r_{\theta}(S_{i}^{E},A_{i}^{E})\] \[-E_{S,A}^{\pi_{\theta}}[\sum_{i=t+1}^{\infty}\gamma^{i}\nabla_{ \theta}r_{\theta}(S_{i},A_{i})|S_{t}=S_{t}^{E},A_{t}=A_{t}^{E}]+E_{S,A}^{\pi_ {\theta}}[\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta}(S_{i},A_{i})| S_{0}=S_{0}^{E}]\bigg{]},\]

where equality \((b)\) follows from the fact that

\[E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot, \cdot)(\cdot,\cdot)\}_{i\geq 0}}\bigg{[}E_{S,A}^{\pi_{\theta}}[\sum_{i=t+1}^{\infty} \gamma^{i}\nabla_{\theta}r_{\theta}(S_{i},A_{i})|S_{t}=S_{t}^{E},A_{t}=A_{t}^{E }]\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mathbb{P}_{i}^{\pi_{E}}(\cdot, \cdot)(\cdot,\cdot)\}_{i\geq 0}}\bigg{[}E_{S,A}^{\pi_{\theta}}[\sum_{i=t+1}^{\infty} \gamma^{i}\nabla_{\theta}r_{\theta}(S_{i},A_{i})|S_{t+1}=S_{t+1}^{E}]\bigg{]},\]

because \(\mathbb{P}_{t+1}^{\pi_{E}}(\cdot)=\mathbb{P}_{t}^{\pi_{E}}(S_{t}^{E},A_{t}^{E})P( \cdot|S_{t}^{E},A_{t}^{E})\) and \(S_{t+1}^{E}\sim P(\cdot|S_{t}^{E},A_{t}^{E})\).

Since we quantify the local regret in expectation in Theorem 1, this unbiased estimate can be used when the dynamics is stochastic.

### Proof of Proposition 1

From Assumption 2, we know that \(d_{\text{TV}}(\mathbb{P}_{t}^{\pi_{E}}(\cdot),\mu^{\pi_{E}}(\cdot))=\frac{1}{2} \int_{s\in\mathcal{S}}|\mathbb{P}_{t}^{\pi_{E}}(s)-\mu^{\pi_{E}}(s)|ds\leq C_{M} \rho^{t}\) where the initial state is \(s_{0}\). For any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), we know that \(\mathbb{P}_{t}^{\pi_{E}}(s,a)=\mathbb{P}_{t}^{\pi_{E}}(s)\pi_{E}(a|s)\) and \(\mu^{\pi_{E}}(s,a)=\mu^{\pi_{E}}(s)\pi_{E}(a|s)\). Therefore, we have that:

\[d_{\text{TV}}(\mathbb{P}^{\pi_{E}}(\cdot,\cdot),\mu^{\pi_{E}}( \cdot,\cdot)),\] \[=\frac{1}{2}\int_{s\in\mathcal{S}}\int_{a\in\mathcal{A}}|\mathbb{ P}_{t}^{\pi_{E}}(s)\pi_{E}(a|s)-\mu^{\pi_{E}}(s)\pi_{E}(a|s)|dsda,\] \[=\frac{1}{2}\int_{s\in\mathcal{S}}\int_{a\in\mathcal{A}}|\mathbb{ P}_{t}^{\pi_{E}}(s)-\mu^{\pi_{E}}(s)|\pi_{E}(a|s)dsda,\] \[=\frac{1}{2}\int_{s\in\mathcal{S}}|\mathbb{P}_{t}^{\pi_{E}}(s)- \mu^{\pi_{E}}(s)|ds,\] \[\leq C_{M}\rho^{t}.\]

**Claim 1**.: _The trajectory of \(\theta_{t}\) is bounded, i.e., \(||\theta_{t}-\bar{\theta}||\leq\frac{2C_{r}}{\lambda}\) for any \(t\geq 0\)._

Proof.: \[||\theta_{t+1}-\bar{\theta}||=||\theta_{t}-\alpha_{t}g_{t}-\bar{ \theta}||,\] \[=\bigg{|}\bigg{|}\theta_{t}-\bar{\theta}-\alpha_{t}\bigg{[}\sum_{ i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s_{i}^{\prime},a_{i}^{ \prime})-\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s_{i}^{ \prime\prime},a_{i}^{\prime\prime})+\frac{\lambda(1-\gamma^{t+1})}{1-\gamma}( \theta_{t}-\bar{\theta})\bigg{]}\bigg{|}\bigg{|},\] \[=\bigg{|}\bigg{|}(1-\frac{\alpha_{t}\lambda(1-\gamma^{t+1})}{1- \gamma})(\theta_{t}-\bar{\theta})-\alpha_{t}\bigg{[}\sum_{i=0}^{\infty}\gamma ^{i}\nabla_{\theta}r_{\theta_{t}}(s_{i}^{\prime},a_{i}^{\prime})-\sum_{i=0}^{ \infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s_{i}^{\prime\prime},a_{i}^{ \prime\prime})\bigg{]}\bigg{|}\bigg{|},\] \[\stackrel{{(b)}}{{\leq}}(1-\frac{\alpha_{t}\lambda(1- \gamma^{t+1})}{1-\gamma})||\theta_{t}-\bar{\theta}||+\frac{2\alpha_{t}\bar{C}_ {r}}{1-\gamma},\] \[\leq(1-\frac{\alpha_{t}\lambda}{1-\gamma})||\theta_{t}-\bar{ \theta}||+\frac{2\alpha_{t}\bar{C}_{r}}{1-\gamma},\]

where \((a)\) follows triangle inequality and \((b)\) uses the upper bound of \(\nabla_{\theta}r_{\theta}\) in Assumption 1. Therefore, we have the following relation:

\[||\theta_{t+1}-\bar{\theta}||-\frac{2\bar{C}_{r}}{\lambda}\leq(1 -\frac{\alpha_{t}\lambda}{1-\gamma})\bigg{(}||\theta_{t}-\bar{\theta}||-\frac{ 2\bar{C}_{r}}{\lambda}\bigg{)},\] \[\Rightarrow||\theta_{t}-\bar{\theta}||\leq(1-\frac{\alpha_{t} \lambda}{1-\gamma})^{t}\bigg{(}||\theta_{0}-\bar{\theta}||-\frac{2\bar{C}_{r}}{ \lambda}\bigg{)}+\frac{2\bar{C}_{r}}{\lambda},\] \[\stackrel{{(c)}}{{=}}\frac{2\bar{C}_{r}}{\lambda} \bigg{[}1-(1-\frac{\alpha_{t}\lambda}{1-\gamma})^{t}\bigg{]}\stackrel{{ (d)}}{{\leq}}\frac{2\bar{C}_{r}}{\lambda},\]

where \((c)\) follows the fact that \(\theta_{0}=\bar{\theta}\) and \((d)\) follows the fact that \(\alpha_{t}\leq\frac{1-\gamma}{\lambda}\). 

Recall that the loss function \(L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))=-\gamma^{i}\log\pi_{\theta}(a_{i}^{E}|s_{i}^ {E})+\frac{\lambda\gamma^{i}}{2}||\theta-\bar{\theta}||^{2}\) and thus \(\nabla L_{i}(\theta;(s_{i}^{E},a_{i}^{E}))=-\gamma^{i}|\nabla_{\theta}Q_{ \theta}^{\text{soft}}(s_{i}^{E},a_{i}^{E})-\nabla_{\theta}V_{\theta}^{\text{ soft}}(s_{i}^{E})]+\lambda\gamma^{i}(\theta-\bar{\theta})\). From Lemma 3, we know that \(\nabla_{\theta}V_{\theta}^{\text{soft}}(s_{i}^{E})=E_{S,A}^{\pi_{\theta}}[\sum_{ t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s_{i}^{E}]\) and \(\nabla_{\theta}Q_{\theta}^{\text{soft}}(s_{i}^{E},a_{i}^{E})=E_{S,A}^{\pi_{ \theta}}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{ 0}=s_{i}^{E},A_{0}=a_{i}^{E}]\). Then, \(||\nabla_{\theta}V_{\theta}^{\text{soft}}(s_{i}^{E})||\leq\frac{\bar{C}_{r}}{1-\gamma}\) and \(||\nabla_{\theta}Q_{\theta}^{\text{soft}}(s_{i}^{E},a_{i}^{E})||\leq\frac{\bar{C}_ {r}}{1-\gamma}\).

Now we can see that

\[||\nabla L_{i}(\theta_{t};;(s_{i}^{E},a_{i}^{E}))||=\gamma^{i}||\nabla_{\theta}Q_ {\theta_{t}}^{\text{soft}}(s_{i}^{E},a_{i}^{E})-\nabla_{\theta}V_{\theta_{t}}^{ \text{soft}}(s_{i}^{E})+\lambda(\theta_{t}-\bar{\theta})||,\]\[\leq\gamma^{i}||\nabla_{\theta}Q^{\text{soft}}_{\theta_{t}}(s^{E}_{i},a^{E}_{i}) -\nabla_{\theta}V^{\text{soft}}_{\theta_{t}}(s^{E}_{i})+\lambda(\theta_{t}- \bar{\theta})||\leq\gamma^{i}\bigg{(}\frac{2\bar{C}_{r}}{1-\gamma}+2\bar{C}_{r} \bigg{)},\]

Therefore, we have that

\[\bigg{|}E_{(S^{E}_{i},A^{E}_{i})\sim\mathbb{P}^{\pi_{E}}_{i}(\cdot, )}\bigg{[}||\nabla L_{i}(\theta_{t};(S^{E}_{i},A^{E}_{i}))||^{2}\bigg{]}-E_{(S^ {E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)}\bigg{[}||\nabla L_{i}(\theta _{t};(S^{E}_{i},A^{E}_{i}))||^{2}\bigg{]}\bigg{|},\] \[=\bigg{|}\int_{s\in\mathcal{S}}\int_{a\in\mathcal{A}}\mathbb{P}^{ \pi_{E}}_{i}(s,a)||\nabla L_{i}(\theta_{t};(s,a))||^{2}dads\] \[-\int_{s\in\mathcal{S}}\int_{a\in\mathcal{A}}\mu^{\pi_{E}}(s,a)|| \nabla L_{i}(\theta_{t};(s,a))||^{2}dads\bigg{|},\] \[\leq\int_{s\in\mathcal{S}}\int_{a\in\mathcal{A}}|\mathbb{P}^{\pi _{E}}_{i}(s,a)-\mu^{\pi_{E}}(s,a)|\cdot||\nabla L_{i}(\theta_{t};(s,a))||^{2} dads,\] \[\leq 2C_{M}\rho^{i}\cdot 4\bar{C}^{2}_{r}\gamma^{2i}\bigg{(} \frac{2-\gamma}{1-\gamma}\bigg{)}^{2}=8C_{M}\bar{C}^{2}_{r}\bigg{(}\frac{2- \gamma}{1-\gamma}\bigg{)}^{2}\rho^{i}\gamma^{2i}.\]

**Lemma 4**.: _Suppose Assumptions 1-2 hold, the we have the following for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and any \(\theta_{1},\theta_{2},t\): \(||\nabla L_{t}(\theta_{1},(s,a))-\nabla L_{t}(\theta_{2},(s,a))||\leq C_{L}|| \theta_{1}-\theta_{2}||\) and \(|Q^{\text{soft}}_{\theta_{1},\pi_{\theta_{1}}}(s,a)-Q^{\text{soft}}_{\theta_{2 },\pi_{\theta_{2}}}(s,a)|\leq C_{Q}||\theta_{1}-\theta_{2}||\), where \(C_{L}=\frac{2\bar{C}_{r}}{1-\gamma}+\frac{4\bar{C}^{3}_{r}}{(1-\gamma)^{4}}+\lambda\) and \(C_{Q}=\frac{\bar{C}_{r}}{1-\gamma}\)._

Proof.: Note that \(Q^{\text{soft}}_{\theta,\pi_{\theta}}=Q^{\text{soft}}_{\theta}\) and \(\nabla_{\theta}Q^{\text{soft}}_{\theta}(s,a)=E^{\pi_{\theta}}_{S,A}[\sum_{t=0} ^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a]\) (proof of Lemma 3). Therefore, we have that

\[||\nabla_{\theta}Q^{\text{soft}}_{\theta}(s,a)||\leq\frac{\bar{C}_{r}}{1- \gamma}\triangleq C_{Q}\]

We know from Lemma 1 that \(\nabla L_{t}(\theta;(s^{E}_{t},a^{E}_{t}))=-\gamma^{t}[\nabla_{\theta}Q^{\text {soft}}_{\theta}(s^{E}_{t},a^{E}_{t})-\nabla_{\theta}V^{\text{soft}}_{\theta}(s ^{E}_{t})]+\lambda\gamma^{t}(\theta-\bar{\theta})\). To find the smoothness constant of \(L_{t}\), we need to compute the Hessian of \(L_{t}\). First, we have that

\[\nabla^{2}_{\theta\theta}Q^{\text{soft}}_{\theta}(s,a)=\nabla_{ \theta}E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{ \theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a],\] \[=\nabla^{2}_{\theta\theta}r_{\theta}(s,a)+\gamma\int_{s^{\prime} \in\mathcal{S}}P(s^{\prime}|s,a)\nabla_{\theta}E^{\pi_{\theta}}_{S,A}[\sum_{t=0} ^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s^{\prime}]ds^{ \prime},\] \[=\nabla^{2}_{\theta\theta}r_{\theta}(s,a)\] \[+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)\nabla_{ \theta}\int_{a^{\prime}\in\mathcal{A}}\pi_{\theta}(a^{\prime}|s^{\prime})E^{ \pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t },A_{t})|S_{0}=s^{\prime},A_{0}=a^{\prime}]da^{\prime}ds^{\prime},\] \[=\nabla^{2}_{\theta\theta}r_{\theta}(s,a)+\gamma\int_{s^{\prime} \in\mathcal{S}}P(s^{\prime}|s,a)\int_{a^{\prime}\in\mathcal{A}}\bigg{[}\nabla_{ \theta}\pi_{\theta}(a^{\prime}|s^{\prime})\cdot E^{\pi_{\theta}}_{S,A}[\sum_{t=0 }^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s^{\prime},A_{ 0}=a^{\prime}]ds^{\prime},\] \[A_{0}=a^{\prime}]+\pi_{\theta}(a^{\prime}|s^{\prime})\cdot\nabla_{ \theta}E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{ \theta}(S_{t},A_{t})|S_{0}=s^{\prime},A_{0}=a^{\prime}]\bigg{]}da^{\prime}ds^{\prime}.\]

Keep the expansion, we can get

\[\nabla^{2}_{\theta\theta}Q^{\text{soft}}_{\theta}(s,a)=E^{\pi_{ \theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla^{2}_{\theta\theta}r_{\theta}(S_{t },A_{t})|S_{0}=s_{0},A_{0}=a_{0}]\] \[+E^{\pi_{\theta}}_{S,A}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\nabla_{ \theta}\pi_{\theta}(A_{i}|S_{i})\cdot E^{\pi_{\theta}}_{S^{\prime},A^{\prime}}[ \sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t}^{\prime},A^{\prime}_{ t})|S^{\prime}_{0}=S_{i},A^{\prime}_{0}=A_{i}]\bigg{|}S_{0}=s_{0},A_{0}=a_{0} \bigg{]}.\]Now we take a look at the second term in the above equality:

\[\nabla_{\theta}\pi_{\theta}(a|s)\cdot E^{\pi_{\theta}}_{S,A}[\sum_{t=0 }^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a],\] \[=\pi_{\theta}(a|s)\nabla_{\theta}\log\pi_{\theta}(a|s)\cdot E^{\pi _{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t}, A_{t})|S_{0}=s,A_{0}=a],\] \[=\pi_{\theta}(a|s)\bigg{[}\nabla_{\theta}Q^{\text{soft}}_{\theta} (s,a)-\nabla_{\theta}V^{\text{soft}}_{\theta}(s)\bigg{]}\cdot E^{\pi_{\theta}} _{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{ 0}=s,A_{0}=a],\] \[\Rightarrow\bigg{|}\bigg{|}\nabla_{\theta}\pi_{\theta}(a|s) \cdot E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{ \theta}(S_{t},A_{t})|S_{0}=s,A_{0}=a]\bigg{|}\bigg{|},\] \[\leq(\frac{\bar{C}_{r}}{1-\gamma}+\frac{\bar{C}_{r}}{1-\gamma}) \cdot\frac{\bar{C}_{r}}{1-\gamma}=\frac{2\bar{C}_{r}^{3}}{(1-\gamma)^{3}}.\]

Therefore, we have that

\[||\nabla^{2}_{\theta\theta}Q^{\text{soft}}_{\theta}(s,a)||\leq \bigg{|}\bigg{|}E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla^{2}_ {\theta\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s_{0},A_{0}=a_{0}]\bigg{|}\bigg{|}\] \[+\bigg{|}E^{\pi_{\theta}}_{S^{\prime},A^{\prime}}\bigg{[}\sum_{i =0}^{\infty}\gamma^{i}\nabla_{\theta}\pi_{\theta}(A^{\prime}_{i}|S^{\prime}_{i })\cdot E^{\pi_{\theta}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{ \theta}(S_{t},A_{t})|S_{0}=S^{\prime}_{i},A_{0}=A^{\prime}_{i}]\bigg{]}\bigg{|} \bigg{|},\] \[\leq\frac{\tilde{C}_{r}}{1-\gamma}+\sum_{t=0}^{\infty}\gamma^{t} \frac{2\bar{C}_{r}^{3}}{(1-\gamma)^{3}}=\frac{\tilde{C}_{r}}{1-\gamma}+\frac{ 2\bar{C}_{r}^{3}}{(1-\gamma)^{4}}.\]

Similarly, we can get \(||\nabla^{2}_{\theta\theta}V^{\text{soft}}_{\theta}(s)||\leq\frac{\tilde{C}_{r }}{1-\gamma}+\frac{2\bar{C}_{r}^{3}}{(1-\gamma)^{4}}\). Therefore, we have that

\[||\nabla^{2}L_{t}(\theta;(s^{E}_{t},a^{E}_{t}))||\leq\gamma^{t} \bigg{(}||\nabla^{2}_{\theta\theta}Q^{\text{soft}}_{\theta}(s^{E}_{t},a^{E}_{t })||+||\nabla^{2}_{\theta\theta}V^{\text{soft}}_{\theta}(s^{E}_{t})||+\lambda \bigg{)},\] \[\leq\gamma^{t}\bigg{(}\frac{2\tilde{C}_{r}}{1-\gamma}+\frac{4 \bar{C}_{r}^{3}}{(1-\gamma)^{4}}+\lambda\bigg{)}\leq\frac{2\tilde{C}_{r}}{1- \gamma}+\frac{4\bar{C}_{r}^{3}}{(1-\gamma)^{4}}+\lambda\triangleq C_{L}.\] (6)

### Proof of Lemma 2

This proof is based on the proof in ML-IRL [5]. The differences are: (i) their proof only holds for finite state-action space while we extend to continuous state-action space; (ii) their analysis is for offline settings while we extend to online settings to quantify the local regret. We first introduce the following claims which serve as building blocks in this subsection.

**Claim 2**.: _For any given policy \(\pi\) and state-action pair \((s,a)\), it holds that \(|Q^{\text{soft}}_{\theta_{1},\pi}(s,a)-Q^{\text{soft}}_{\theta_{2},\pi}(s,a)| \leq C_{Q}||\theta_{1}-\theta_{2}||\) and \(|V^{\text{soft}}_{\theta_{1},\pi}(s)-V^{\text{soft}}_{\theta_{2},\pi}(s)|\leq C _{Q}||\theta_{1}-\theta_{2}||\)._

Proof.: \[Q^{\text{soft}}_{\theta_{1},\pi}(s,a)-Q^{\text{soft}}_{\theta_{2},\pi}(s,a),\] \[=E^{\pi}_{S,A}\bigg{[}\sum_{t=0}^{\infty}\gamma^{t}\big{[}r_{ \theta_{1}}(S_{t},A_{t})-\log\pi(A_{t}|S_{t})\big{]}\bigg{|}S_{0}=s,A_{0}=a \bigg{]}\] \[-E^{\pi}_{S,A}\bigg{[}\sum_{t=0}^{\infty}\gamma^{t}\big{[}r_{ \theta_{2}}(S_{t},A_{t})-\log\pi(A_{t}|S_{t})\big{]}\bigg{|}S_{0}=s,A_{0}=a \bigg{]},\] \[=E^{\pi}_{S,A}\bigg{[}\sum_{t=0}^{\infty}\gamma^{t}[r_{\theta_{1} }(S_{t},A_{t})-r_{\theta_{2}}(S_{t},A_{t})]\bigg{|}S_{0}=s,A_{0}=a\bigg{]},\]\[\Rightarrow|Q_{\theta_{1},\pi}(s,a)-Q_{\theta_{2},\pi}(s,a)|\leq\sum_{t=0}^{ \infty}\gamma^{t}\bar{C}_{r}||\theta_{1}-\theta_{2}||=\frac{\bar{C}_{r}}{1- \gamma}||\theta_{1}-\theta_{2}||=C_{Q}||\theta_{1}-\theta_{2}||.\]

Similarly, we can get that

\[|V^{\text{soft}}_{\theta_{1},\pi}(s)-V^{\text{soft}}_{\theta_{2},\pi}(s)|\leq E _{S,A}^{\pi}\bigg{[}\sum_{t=0}^{\infty}\gamma^{t}[r_{\theta_{1}}(S_{t},A_{t})-r _{\theta_{2}}(S_{t},A_{t})]\bigg{|}S_{0}=s\bigg{]},\]

\[\leq\sum_{t=0}^{\infty}\gamma^{t}\bar{C}_{r}||\theta_{1}-\theta_{2}||=\frac{ \bar{C}_{r}}{1-\gamma}||\theta_{1}-\theta_{2}||=C_{Q}||\theta_{1}-\theta_{2}||.\]

**Claim 3**.: _The soft Bellman operator \(\mathcal{T}^{\text{soft}}_{\theta}\):_

\[(\mathcal{T}^{\text{soft}}_{\theta}Q)(s,a) \triangleq r_{\theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{ \prime}|s,a)\log\biggl{[}\int_{a^{\prime}\in\mathcal{A}}\exp(Q(s^{\prime},a^{ \prime}))da^{\prime}\biggr{]}ds^{\prime},\] \[(\mathcal{T}^{\text{soft}}_{\theta}V)(s) \triangleq\log\biggl{[}\int_{a\in\mathcal{A}}\exp\biggl{(}r_{ \theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)V(s^{\prime })ds^{\prime}\biggr{)}da\biggr{]},\]

_is a contraction map with constant \(\gamma\)._

Proof.: It has been proved that \(\mathcal{T}^{\text{soft}}_{\theta}Q\) is a contraction map with constant \(\gamma\) (Appendix A.2 in [34]). Here we show that \(\mathcal{T}^{\text{soft}}_{\theta}V\) is a contraction map with constant \(\gamma\). Define a norm of \(V\) as \(||V_{1}-V_{2}||=\sup_{s\in\mathcal{S}}|V_{1}(s)-V_{2}(s)|\) and suppose \(||V_{1}-V_{2}||=\epsilon\). Then we have that

\[\mathcal{T}^{\text{soft}}_{\theta}V_{1}(s)=\log\biggl{[}\int_{a \in\mathcal{A}}\exp\biggl{(}r_{\theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{ S}}P(s^{\prime}|s,a)V_{1}(s^{\prime})ds^{\prime}\biggr{)}da\biggr{]},\] \[\leq\log\biggl{[}\int_{a\in\mathcal{A}}\exp\biggl{(}r_{\theta}(s, a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)[V_{2}(s^{\prime})+ \epsilon]ds^{\prime}\biggr{)}da\biggr{]},\] \[=\log\biggl{[}\int_{a\in\mathcal{A}}\exp\biggl{(}r_{\theta}(s,a)+ \gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)V_{2}(s^{\prime})ds^{ \prime}+\gamma\epsilon\biggr{)}da\biggr{]},\] \[=\log\biggl{[}\int_{a\in\mathcal{A}}\exp(\gamma\epsilon)\exp \biggl{(}r_{\theta}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a )V_{2}(s^{\prime})ds^{\prime}\biggr{)}da\biggr{]},\] \[=\mathcal{T}^{\text{soft}}_{\theta}V_{2}(s)+\gamma\epsilon.\]

Similarly, we can get \(\mathcal{T}^{\text{soft}}_{\theta}V_{1}(s)\geq\mathcal{T}^{\text{soft}}_{\theta }V_{2}(s)-\gamma\epsilon\). Therefore, \(||\mathcal{T}^{\text{soft}}_{\theta}V_{1}-\mathcal{T}^{\text{soft}}_{\theta}V_ {2}||\leq\gamma\epsilon=\gamma||V_{1}-V_{2}||\). 

**Claim 4**.: _It holds that \(Q^{\text{soft}}_{\theta_{t},\pi_{t+1}}(s,a)\geq\mathcal{T}^{\text{soft}}_{\theta _{t}}(Q^{\text{soft}}_{\theta_{t},\pi_{t}})(s,a)\) and \(V^{\text{soft}}_{\theta_{t},\pi_{t+1}}(s)\geq\mathcal{T}^{\text{soft}}_{\theta _{t}}(V^{\text{soft}}_{\theta_{t},\pi_{t}})(s)\) for any \((s,a)\)._

Proof.: \[Q^{\text{soft}}_{\theta_{t},\pi_{t+1}}(s,a)\overset{(i)}{=}r_{ \theta_{t}}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)E_{a^{ \prime}\sim\pi_{t+1}}[Q^{\text{soft}}_{\theta_{t},\pi_{t+1}}(s^{\prime},a^{ \prime})-\log\pi_{t+1}(a^{\prime}|s^{\prime})]ds^{\prime},\] \[\overset{(ii)}{\geq}r_{\theta_{t}}(s,a)+\gamma\int_{s^{\prime} \in\mathcal{S}}P(s^{\prime}|s,a)E_{A^{\prime}\sim\pi_{t+1}(\cdot|s^{\prime})}[Q ^{\text{soft}}_{\theta_{t},\pi_{t}}(s^{\prime},A^{\prime})-\log\pi_{t+1}(A^{ \prime}|s^{\prime})]ds^{\prime},\] \[=r_{\theta_{t}}(s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{ \prime}|s,a)\log\biggl{[}\int_{a^{\prime}\in\mathcal{A}}\exp(Q^{\text{soft}}_{ \theta_{t},\pi_{t}}(s^{\prime},a^{\prime}))da^{\prime}\biggr{]}ds^{\prime},\] \[=\mathcal{T}^{\text{soft}}_{\theta}(Q^{\text{soft}}_{\theta_{t},\pi_{ t}})(s,a),\]

where \((i)\) follow equations (2)-(3) in [35] and \((ii)\) follows policy improvement theorem (Theorem 4 in [34]). Similarly, we can get that

\[V^{\text{soft}}_{\theta_{t},\pi_{t+1}}(s)=E_{A\sim\pi_{t+1}(\cdot|s)}[Q^{\text{ soft}}_{\theta_{t},\pi_{t+1}}(s,A)-\log\pi_{t+1}(A|s)],\]\[\geq E_{A\sim\pi_{t+1}(\cdot|s)}[Q_{\theta_{t},\pi_{t}}^{\text{soft}}(s,A)- \log\pi_{t+1}(A|s)],\] \[=\log\biggl{[}\int_{a\in\mathcal{A}}\exp(Q_{\theta_{t},\pi_{t}}^{ \text{soft}}(s,a))da\biggr{]},\] \[=\log\biggl{[}\int_{a\in\mathcal{A}}\exp\biggl{(}r_{\theta_{t}}( s,a)+\gamma\int_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)V_{\theta_{t},\pi_{t}}^{ \text{soft}}(s^{\prime})ds^{\prime}\biggr{)}da\biggr{]},\] \[=\mathcal{T}_{\theta_{t}}^{\text{soft}}(V_{\theta_{t},\pi_{t}}^{ \text{soft}})(s).\]

**Claim 5**.: _The following holds for any \((s,a)\in\mathcal{S}\times\mathcal{A}\) and \(\theta_{1}\), \(\theta_{2}\), \(t\):_

\[|V_{\theta_{1}}^{\text{soft}}(s,a)-V_{\theta_{2}}^{\text{soft}}(s,a)|\leq C_{Q }||\theta_{1}-\theta_{2}||.\]

Proof.: Note that \(\nabla_{\theta}V_{\theta}^{\text{soft}}(s)=E_{S,A}^{\pi_{\theta}}[\sum_{t=0}^{ \infty}\gamma^{t}\nabla_{\theta}r_{\theta}(S_{t},A_{t})|S_{0}=s]\) (proof of Lemma 3), therefore

\[||\nabla_{\theta}V_{\theta}^{\text{soft}}(s)||\leq\frac{\tilde{C}_{r}}{1- \gamma}=C_{Q}.\]

We first show the convergence of \(\pi_{t}\) in Algorithm 1. For any \((s,a)\in\mathcal{S}\times\mathcal{A}\), we know that

\[|\log\pi_{t+1}(a|s)-\log\pi_{\theta_{t}}(a|s)|\leq|Q_{\theta_{t},\pi_{t}}^{ \text{soft}}(s,a)-Q_{\theta_{t}}^{\text{soft}}(s,a)|+|V_{\theta_{t},\pi_{t}}^{ \text{soft}}(s)-V_{\theta_{t}}^{\text{soft}}(s)|.\]

Now we take a look at the term \(|Q_{\theta_{t},\pi_{t}}^{\text{soft}}(s,a)-Q_{\theta_{t}}^{\text{soft}}(s,a)|\):

\[|Q_{\theta_{t},\pi_{t}}^{\text{soft}}(s,a)-Q_{\theta_{t}}^{\text{ soft}}(s,a)|,\] \[\leq|Q_{\theta_{t},\pi_{t}}^{\text{soft}}(s,a)-Q_{\theta_{t-1}, \pi_{t}}^{\text{soft}}(s,a)|+|Q_{\theta_{t-1},\pi_{t}}^{\text{soft}}(s,a)-Q_{ \theta_{t-1}}^{\text{soft}}(s,a)|+|Q_{\theta_{t-1}}^{\text{soft}}(s,a)-Q_{ \theta_{t}}^{\text{soft}}(s,a)|,\] \[\stackrel{{(a)}}{{\leq}}C_{Q}||\theta_{t}-\theta_{t-1 }||+|Q_{\theta_{t-1},\pi_{t}}^{\text{soft}}(s,a)-Q_{\theta_{t-1}}^{\text{soft}}( s,a)|+C_{Q}||\theta_{t}-\theta_{t-1}||,\] \[=2C_{Q}||\theta_{t}-\theta_{t-1}||+|Q_{\theta_{t-1},\pi_{t}}^{ \text{soft}}(s,a)-Q_{\theta_{t-1}}^{\text{soft}}(s,a)|,\] \[\stackrel{{(b)}}{{=}}2C_{Q}||\theta_{t}-\theta_{t-1 }||+Q_{\theta_{t-1}}^{\text{soft}}(s,a)-Q_{\theta_{t-1},\pi_{t}}^{\text{soft}}( s,a),\] \[\stackrel{{(c)}}{{\leq}}2C_{Q}||\theta_{t}-\theta_{t-1 }||+Q_{\theta_{t-1}}^{\text{soft}}(s,a)-\mathcal{T}_{\theta_{t-1}}^{\text{soft}}( Q_{\theta_{t-1},\pi_{t-1}}^{\text{soft}})(s,a),\] \[\stackrel{{(d)}}{{=}}2C_{Q}||\theta_{t}-\theta_{t-1}||+ \mathcal{T}_{\theta_{t-1}}^{\text{soft}}(Q_{\theta_{t-1}}^{\text{soft}})(s,a)- \mathcal{T}_{\theta_{t-1}}^{\text{soft}}(Q_{\theta_{t-1},\pi_{t-1}}^{\text{soft} })(s,a),\] \[\stackrel{{(e)}}{{\leq}}2C_{Q}||\theta_{t}-\theta_{t-1 }||+\gamma|Q_{\theta_{t-1}}^{\text{soft}}(s,a)-Q_{\theta_{t-1},\pi_{t-1}}^{\text{ soft}}(s,a)|,\] (7)

where \((a)\) follows Lemma 2 and Claim 2, \((b)\) follows the fact that \(\pi_{\theta}\) is the optimal solution, \((c)\) follows Claim 4, \((d)\) follows the fact that \(Q_{\theta_{t-1}}^{\text{soft}}\) is a fixed point of \(\mathcal{T}_{\theta_{t-1}}^{\text{soft}}\) (Theorem 2 in [34]), and \((e)\) follows Claim 3.

Similarly we can bound the term \(|V_{\theta_{t},\pi_{t}}^{\text{soft}}(s)-V_{\theta_{t}}^{\text{soft}}(s)|\):

\[|V_{\theta_{t},\pi_{t}}^{\text{soft}}(s)-V_{\theta_{t}}^{\text{ soft}}(s)|,\] \[\leq|V_{\theta_{t},\pi_{t}}^{\text{soft}}(s)-V_{\theta_{t-1},\pi_{ t}}^{\text{soft}}(s)|+|V_{\theta_{t-1},\pi_{t}}^{\text{soft}}(s)-V_{\theta_{t-1}}^{ \text{soft}}(s)|+|V_{\theta_{t-1}}^{\text{soft}}(s)-V_{\theta_{t-1},\pi_{t-1}}^{ \text{soft}}(s)|,\] \[\stackrel{{(f)}}{{\leq}}C_{Q}||\theta_{t}-\theta_{t-1 }||+|V_{\theta_{t-1},\pi_{t}}^{\text{soft}}(s)-V_{\theta_{t-1}}^{\text{soft}}(s)|+C_{Q }||\theta_{t}-\theta_{t-1}||,\] \[\stackrel{{(g)}}{{\leq}}2C_{Q}||\theta_{t}-\theta_{t-1 }||+V_{\theta_{t-1}}^{\text{soft}}(s)-\mathcal{T}_{\theta_{t-1}}^{\text{soft}}(V_{ \theta_{t-1},\pi_{t}}^{\text{soft}})(s),\] \[\leq 2C_{Q}||\theta_{t}-\theta_{t-1}||+\gamma|V_{\theta_{t-1}}^{\text{ soft}}(s)-V_{\theta_{t-1},\pi_{t}}^{\text{soft}}(s)|,\]

where \((f)\) follows Claim 2 and claim 5 and \((e)\) follows Claim 4.

Now we take a look at the term \(||\theta_{t}-\theta_{t-1}||\):

\[||\theta_{t}-\theta_{t-1}||=\alpha_{t-1}||g_{t-1}||,\]\[\leq\alpha_{t-1}\bigg{[}\bigg{\|}\bigg{\|}\sum_{t=0}^{\infty}\gamma^{t} \nabla_{\theta}r_{\theta_{t-1}}(s^{\prime}_{t},a^{\prime}_{t})\bigg{\|}+\bigg{|} \sum_{t=0}^{\infty}\gamma^{t}\nabla_{\theta}r_{\theta_{t-1}}(s^{\prime\prime}_ {t},a^{\prime\prime}_{t})\bigg{\|}+\frac{\lambda(1-\gamma^{t-1})}{1-\gamma} \bigg{\|}\theta_{t-1}-\bar{\theta}\bigg{\|}\bigg{]},\] \[\overset{(h)}{\leq}\alpha_{t-1}\bigg{(}\frac{\bar{C}_{r}}{1- \gamma}+\frac{\bar{C}_{r}}{1-\gamma}+\frac{\lambda(1-\gamma^{t-1})}{1-\gamma} \cdot\frac{2\bar{C}_{r}}{\lambda}\bigg{)},\] \[\leq\frac{4\alpha_{t-1}\bar{C}_{r}}{1-\gamma},\] (8)

where \((h)\) follows claim 1. Recall that

\[|\log\pi_{t+1}(a|s)-\log\pi_{\theta_{t}}(a|s)|\leq|Q^{\text{soft}}_{\theta_{t},\pi_{t}}(s,a)-Q^{\text{soft}}_{\theta_{t}}(s,a)|+|V^{\text{soft}}_{\theta_{t},\pi_{t}}(s)-V^{\text{soft}}_{\theta_{t}}(s)|.\]

Summing from \(i=0\) to \(t\), we get

\[\leq\sum_{i=1}^{t}\bigg{[}4C_{Q}||\theta_{i}-\theta_{i-1}||+\gamma \bigg{(}|Q^{\text{soft}}_{\theta_{i-1}}(s,a)-Q^{\text{soft}}_{\theta_{i-1},\pi_ {i-1}}(s,a)|+|V^{\text{soft}}_{\theta_{i-1},\pi_{i-1}}(s)-V^{\text{soft}}_{ \theta_{i-1}}(s)|\bigg{)}\bigg{]},\] \[\Rightarrow(1-\gamma)\sum_{i=0}^{t-1}\bigg{[}|Q^{\text{soft}}_{ \theta_{i},\pi_{i}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|+|V^{\text{soft}}_{ \theta_{i},\pi_{i}}(s)-V^{\text{soft}}_{\theta_{i}}(s)|\bigg{]},\] \[\overset{(i)}{\leq}\frac{16C_{Q}\bar{C}_{r}}{1-\gamma}\sum_{i=1} ^{t}\alpha_{i-1}+\bigg{(}|Q^{\text{soft}}_{\theta_{0}}(s,a)-Q^{\text{soft}}_{ \theta_{0},\pi_{0}}(s,a)|+|V^{\text{soft}}_{\theta_{0},\pi_{0}}(s)-V^{\text{ soft}}_{\theta_{0}}(s)|\] \[-|Q^{\text{soft}}_{\theta_{t}}(s,a)-Q^{\text{soft}}_{\theta_{t}, \pi_{t}}(s,a)|-|V^{\text{soft}}_{\theta_{t},\pi_{t}}(s)-V^{\text{soft}}_{\theta _{t}}(s)|\bigg{)},\] \[\Rightarrow\frac{1}{t}\sum_{i=0}^{t-1}\bigg{[}|Q^{\text{soft}}_{ \theta_{i},\pi_{i}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|+|V^{\text{soft}}_{ \theta_{i},\pi_{i}}(s)-V^{\text{soft}}_{\theta_{i}}(s)|\bigg{]}\] \[\leq\frac{16C_{Q}\bar{C}_{r}}{t(1-\gamma)^{2}}\sum_{i=1}^{t} \alpha_{i-1}+\frac{1}{t(1-\gamma)}\bigg{(}|Q^{\text{soft}}_{\theta_{0}}(s,a)-Q ^{\text{soft}}_{\theta_{0},\pi_{0}}(s,a)|+|V^{\text{soft}}_{\theta_{0},\pi_{0} }(s)-V^{\text{soft}}_{\theta_{0}}(s)|\] \[-|Q^{\text{soft}}_{\theta_{t}}(s,a)-Q^{\text{soft}}_{\theta_{t}, \pi_{t}}(s,a)|-|V^{\text{soft}}_{\theta_{t},\pi_{t}}(s)-V^{\text{soft}}_{\theta _{t}}(s)|\bigg{)},\] \[=\frac{\bar{D}_{1}}{\sqrt{t}}+\frac{\bar{D}_{2}}{t},\]

where \((i)\) follows (8), \(\bar{D}_{1}=\frac{16C_{Q}\bar{C}_{r}}{(1-\gamma)^{2}}\), and \(\bar{D}_{2}=\frac{1}{1-\gamma}\bigg{(}|Q^{\text{soft}}_{\theta_{0}}(s,a)-Q^{ \text{soft}}_{\theta_{0},\pi_{0}}(s,a)|+|V^{\text{soft}}_{\theta_{0},\pi_{0}}( s)-V^{\text{soft}}_{\theta_{0}}(s)|-V^{\text{soft}}_{\theta_{0}}(s)|-V^{\text{soft}}_{ \theta_{t}}(s)|-V^{\text{soft}}_{\theta_{t}}(s)-V^{\text{soft}}_{\theta_{t}}(s)| \bigg{)}\). Therefore, we can see that

\[\frac{1}{t}\sum_{i=0}^{t-1}|\log\pi_{i+1}(a|s)-\log\pi_{\theta_{i}}(a|s)|\leq \frac{\bar{D}_{1}}{\sqrt{t}}+\frac{\bar{D}_{2}}{t}.\] (9)

Define the loss function \(\bar{L}(\theta)\triangleq\bar{E}_{(S,A)\sim\mu^{x}(\cdot,\cdot)}\big{[}-\log \pi_{\theta}(A|S)+\frac{\lambda}{2}||\theta-\bar{\theta}||^{2}\big{]}\), then we can see that \(E_{(S^{E}_{i},A^{E}_{i})\sim\mu^{x}(\cdot,\cdot)}[L_{i}(\theta;(S^{E}_{i},A^{E }_{i}))]=\gamma^{i}\bar{L}(\theta)\). Moreover, we have that

\[||\nabla L_{i}(\theta_{t};(S^{E}_{i},A^{E}_{i}))||\overset{(j)}{ \leq}\gamma^{i}\lambda||\theta_{t}-\bar{\theta}||\] \[+\gamma^{i}||E^{\pi_{\theta_{t}}}_{S,A}[\sum_{k=0}^{\infty}\gamma^{k }\nabla_{\theta}r_{\theta_{t}}(S_{k},A_{k})|S_{0}=S^{E}_{i}]-E^{\pi_{\theta_{t}}}_ {S,A}[\sum_{k=0}^{\infty}\gamma^{k}\nabla_{\theta}r_{\theta_{t}}(S_{k},A_{k})|S_{0 }=S^{E}_{i},A_{0}=A^{E}_{i}]||,\] \[\overset{(k)}{\leq}2\gamma^{i}\bar{C}_{r}+\frac{2\gamma^{i}\bar{C}_ {r}}{1-\gamma},\] (10)where \((j)\) follows Lemma 3 and \((k)\) follows Claim 1.

Therefore, we have that

\[E_{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)}[||\frac{ \nabla L_{i}(\theta_{t};(S_{t}^{E},A_{i}^{E}))}{\gamma^{i}}-\nabla\bar{L}( \theta_{t})||^{2}]\stackrel{{(l^{\prime})}}{{\leq}}\bigg{[}2\bar{ C}_{r}+\frac{2\bar{C}_{r}}{1-\gamma}\bigg{]}^{2},\] \[\Rightarrow E_{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)}[|| \frac{1}{t}\sum_{i=0}^{t-1}L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E}))-\gamma^{i} \bar{L}(\theta_{t})||^{2}],\] \[=\bigg{(}\frac{1-\gamma^{t}}{1-\gamma}\bigg{)}^{2}E_{(S_{t}^{E},A _{t}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)}[||\frac{1}{t}\sum_{i=0}^{t-1}\frac{L_ {i}(\theta_{t};(S_{i}^{E},A_{i}^{E}))}{\gamma^{i}}-\bar{L}(\theta_{t})||^{2}],\] \[\leq\frac{1}{t}\cdot\frac{4\bar{C}_{r}^{2}(2-\gamma)^{2}}{(1- \gamma)^{4}},\] (11)

where \((l^{\prime})\) follows the fact that a bounded variable \(X\in[-a,a]\) has bounded variance at most \(a^{2}\). Now we take a look at the term \(g_{t}-\frac{1-\gamma^{t+1}}{1-\gamma}\nabla\bar{L}(\theta_{t})\):

\[E_{(S,A)\sim\mu^{\pi_{E}}(\cdot,\cdot)}\bigg{[}g_{t}-\frac{1- \gamma^{t+1}}{1-\gamma}\nabla\bar{L}(\theta_{t})\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}g_{t}-\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E }))\bigg{]}\] \[+E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E }))-\frac{1-\gamma^{t+1}}{1-\gamma}\nabla\bar{L}(\theta_{t})\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}g_{t}-\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E }))\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s_{i }^{\prime},a_{i}^{\prime})-E_{S,A}^{\pi_{\theta_{t}}}[\sum_{i=0}^{\infty} \gamma^{i}\nabla_{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{0}=S_{0}^{E}]\bigg{]}\] \[+E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}\sum_{i=t+1}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s_{ i}^{\prime\prime},a_{i}^{\prime\prime})\] \[-E_{S,A}^{\pi_{\theta_{t}}}[\sum_{i=t+1}^{\infty}\gamma^{i}\nabla _{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{t}=S_{t}^{E},A_{t}=A_{t}^{E}]\bigg{]},\] \[=E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}E_{S,A}^{\pi_{t+1}}[\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_ {\theta_{t}}(S_{i},A_{i})|S_{0}=S_{0}^{E}]\] \[-E_{S,A}^{\pi_{\theta_{t}}}[\sum_{i=0}^{\infty}\gamma^{i}\nabla _{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{0}=S_{0}^{E}]+E_{S,A}^{\pi_{t+1}}[ \sum_{i=t+1}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{t }=S_{t}^{E},A_{t}=A_{t}^{E}]\] \[-E_{S,A}^{\pi_{\theta_{t}}}[\sum_{i=t+1}^{\infty}\gamma^{i}\nabla _{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{t}=S_{t}^{E},A_{t}=A_{t}^{E}]\bigg{]}.\]

From equation (64) in [5], we know that

\[\bigg{|}\bigg{|}E_{S,A}^{\pi_{t+1}}[\sum_{i=0}^{\infty}\gamma^{i} \nabla_{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{0}=s_{0}]-E_{S,A}^{\pi_{\theta_{t }}}[\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(S_{i},A_{i})|S_{0}= s_{0}]\bigg{|}\bigg{|},\] \[\leq\frac{2\bar{C}_{r}}{1-\gamma}\int_{s\in\mathcal{S}}\int_{a \in\mathcal{A}}|Q_{\theta_{t}}^{\text{soft}}(s,a)-Q_{\theta_{t},\pi_{t}}^{\text{ soft}}(s,a)|dads,\] \[\leq\frac{2\bar{C}_{r}C_{d}}{1-\gamma}\sup_{(s,a)\in\mathcal{S} \times\mathcal{A}}\{|Q_{\theta_{t}}^{\text{soft}}(s,a)-Q_{\theta_{t},\pi_{t}}^{ \text{soft}}(s,a)|\},\]where \(C_{d}\) is the product of the area of \(\mathcal{S}\) and the area of \(\mathcal{A}\).

Therefore, we can get that

\[E_{(S,A)\sim\mu^{\pi_{E}}}\bigg{[}\bigg{\|}g_{t}-\frac{1-\gamma^{t+1}}{1-\gamma} \nabla\bar{L}(\theta_{t})\bigg{\|}\bigg{]}\leq\frac{4\bar{C}_{r}C_{d}}{1- \gamma}\sup_{(s,a)\in\mathcal{S}\times\mathcal{A}}\{|Q^{\text{soft}}_{\theta_{ t}}(s,a)-Q^{\text{soft}}_{\theta_{t},\pi_{t}}(s,a)|\}.\] (12)

From (7) and (8), we know that

\[|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(s,a)-Q^{\text{soft}}_{\theta _{i}}(s,a)|\leq\gamma|Q^{\text{soft}}_{\theta_{i-1},\pi_{i-1}}(s,a)-Q^{\text{ soft}}_{\theta_{i-1}}(s,a)|+\frac{8\alpha_{i-1}C_{Q}\bar{C}_{r}}{1-\gamma},\] \[\Rightarrow\alpha_{i}|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(s,a)- Q^{\text{soft}}_{\theta_{i}}(s,a)|\leq\alpha_{i-1}\gamma|Q^{\text{soft}}_{ \theta_{i-1},\pi_{i-1}}(s,a)-Q^{\text{soft}}_{\theta_{i-1}}(s,a)|+\frac{8 \alpha_{i-1}^{2}C_{Q}\bar{C}_{r}}{1-\gamma},\] \[\Rightarrow\sum_{i=1}^{t}\alpha_{i}|Q^{\text{soft}}_{\theta_{i}, \pi_{t}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|\leq\sum_{i=0}^{t-1}\alpha_{i} \gamma|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|+\sum_{i=0}^{t-1}\frac{8\alpha_{i}^{2}C_{Q}\bar{C}_{r}}{1-\gamma},\] \[\Rightarrow(1-\gamma)\sum_{i=0}^{t-1}\alpha_{i}|Q^{\text{soft}}_{ \theta_{i},\pi_{t}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|,\] \[\leq\alpha_{0}|Q^{\text{soft}}_{\theta_{0},\pi_{0}}(s,a)-Q^{\text{ soft}}_{\theta_{0}}(s,a)|-\alpha_{t}|Q^{\text{soft}}_{\theta_{t},\pi_{t}}(s,a)-Q^{ \text{soft}}_{\theta_{t}}(s,a)|+\sum_{i=0}^{t-1}\frac{8\alpha_{i}^{2}C_{Q}\bar {C}_{r}}{1-\gamma},\] (13)

and similarly we can see that

\[\sum_{i=1}^{t}|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(s,a)-Q^{\text {soft}}_{\theta_{i}}(s,a)|\leq\sum_{i=0}^{t-1}\gamma|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|+\frac{8\alpha_{i}C_{Q}\bar{C} _{r}}{1-\gamma},\] \[\Rightarrow(1-\gamma)\sum_{i=1}^{t-1}|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(s,a)-Q^{\text{soft}}_{\theta_{i}}(s,a)|,\] \[\leq|Q^{\text{soft}}_{\theta_{0},\pi_{0}}(s,a)-Q^{\text{soft}}_{ \theta_{0}}(s,a)|-|Q^{\text{soft}}_{\theta_{t},\pi_{t}}(s,a)-Q^{\text{soft}}_{ \theta_{t}}(s,a)|+\sum_{i=0}^{t-1}\frac{8\alpha_{i}C_{Q}\bar{C}_{r}}{1-\gamma},\] (14)

Telescoping from \(i=0\) to \(t-1\), we get

\[\sum_{i=0}^{t-1}\alpha_{i}E_{(S,A)\sim\mu^{\pi_{E}}}\bigg{[}\bigg{|} \bigg{|}g_{i}-\frac{1-\gamma^{i+1}}{1-\gamma}\nabla\bar{L}(\theta_{i})\bigg{|} \bigg{]}\bigg{]},\] \[\overset{(l)}{\leq}\frac{4\bar{C}_{r}C_{d}}{1-\gamma}\sum_{i=0}^{ t-1}\alpha_{i}|Q^{\text{soft}}_{\theta_{i},\pi_{t}}(S,A)-Q^{\text{soft}}_{\theta_{i}}(S,A)|,\] \[\overset{(m)}{\leq}\frac{4\bar{C}_{r}C_{d}}{(1-\gamma)^{2}}\bigg{[} \alpha_{0}|Q^{\text{soft}}_{\theta_{0},\pi_{0}}(S,A)-Q^{\text{soft}}_{\theta_{0} }(S,A)|-\alpha_{t}|Q^{\text{soft}}_{\theta_{t},\pi_{t}}(S,A)-Q^{\text{soft}}_{ \theta_{t}}(S,A)|+\sum_{i=0}^{t-1}\frac{8\alpha_{i}^{2}C_{Q}\bar{C}_{r}}{1- \gamma}\bigg{]},\] (15)

where \((l)\) follows (12) and \((m)\) follows (13). Similarly, we can see that

\[\sum_{i=0}^{t-1}E_{(S,A)\sim\mu^{\pi_{E}}}\bigg{[}\bigg{|}\bigg{|}g_ {i}-\frac{1-\gamma^{i+1}}{1-\gamma}\nabla\bar{L}(\theta_{i})\bigg{|}\bigg{|} \bigg{]},\] \[\leq\frac{4\bar{C}_{r}C_{d}}{1-\gamma}\sum_{i=0}^{t-1}|Q^{\text{ soft}}_{\theta_{i},\pi_{t}}(S,A)-Q^{\text{soft}}_{\theta_{i}}(S,A)|,\] \[\leq\frac{4\bar{C}_{r}C_{d}}{(1-\gamma)^{2}}\bigg{[}|Q^{\text{ soft}}_{\theta_{0},\pi_{0}}(S,A)-Q^{\text{soft}}_{\theta_{0}}(S,A)|-|Q^{\text{soft}}_{ \theta_{t},\pi_{t}}(S,A)-Q^{\text{soft}}_{\theta_{t}}(S,A)|+\sum_{i=0}^{t-1} \frac{8\alpha_{i}C_{Q}\bar{C}_{r}}{1-\gamma}\bigg{]},\] (16)Now, we start to quantify the local regret:

\[\bar{L}(\theta_{i+1})\geq\bar{L}(\theta_{i})+[\nabla\bar{L}(\theta_{i })]^{\top}(\theta_{i+1}-\theta_{i})-\frac{C_{L}}{2}||\theta_{i+1}-\theta_{i}||^{ 2},\] \[\overset{(n)}{\geq}\bar{L}(\theta_{i})+\alpha_{t}[\nabla\bar{L}( \theta_{i})]^{\top}g_{i}-\frac{8\alpha_{i}^{2}\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^ {2}},\] \[=\bar{L}(\theta_{i})+\alpha_{i}\frac{1-\gamma^{i+1}}{1-\gamma}|| \nabla\bar{L}(\theta_{i})||^{2}+\alpha_{i}[\nabla\bar{L}(\theta_{i})]^{\top}(g _{i}-\frac{1-\gamma^{i+1}}{1-\gamma}\nabla\bar{L}(\theta_{i}))-\frac{8\alpha_ {i}^{2}\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^{2}},\] \[\geq\bar{L}(\theta_{i})+\alpha_{i}\frac{1-\gamma^{i+1}}{1-\gamma} ||\nabla\bar{L}(\theta_{i})||^{2}-\alpha_{i}||\nabla\bar{L}(\theta_{i})|| \cdot||g_{i}-\frac{1-\gamma^{i+1}}{1-\gamma}\nabla\bar{L}(\theta_{i})||-\frac{ 8\alpha_{i}^{2}\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^{2}},\] \[\overset{(o)}{\geq}\bar{L}(\theta_{i})+\alpha_{i}\frac{1-\gamma^ {i+1}}{1-\gamma}||\nabla\bar{L}(\theta_{i})||^{2}-\alpha_{i}\frac{2\bar{C}_{r} (2-\gamma)}{1-\gamma}\cdot||g_{i}-\frac{1-\gamma^{i+1}}{1-\gamma}\nabla\bar{L }(\theta_{i})||-\frac{8\alpha_{i}^{2}\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^{2}},\] \[\Rightarrow E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot, \cdot)\}_{i\geq 0}}\bigg{[}\sum_{i=0}^{t-1}\alpha_{i}\frac{1-\gamma^{i+1}}{1- \gamma}||\nabla\bar{L}(\theta_{i})||^{2}\bigg{]},\] \[\leq\bar{L}(\theta_{t})-\bar{L}(\theta_{0})+\frac{2\bar{C}_{r}(2- \gamma)}{1-\gamma}\sum_{i=0}^{t-1}\alpha_{i}E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^ {x_{E}}(\cdot,\cdot)\}_{i\geq 0}}\bigg{[}||g_{i}-\frac{1-\gamma^{i+1}}{1- \gamma}\nabla\bar{L}(\theta_{i})||\bigg{]}\] \[+\sum_{i=0}^{t-1}\frac{8\alpha_{i}^{2}\bar{C}_{r}^{2}C_{L}}{(1- \gamma)^{2}},\] \[\overset{(p)}{\leq}\bar{L}(\theta_{t})-\bar{L}(\theta_{0})+\frac{ 8\bar{C}_{r}^{2}C_{d}(2-\gamma)}{(1-\gamma)^{3}}E_{\{(S_{i}^{E},A_{i}^{E})\sim \mu^{x_{E}}(\cdot,\cdot)\}_{i\geq 0}}\bigg{[}\alpha_{0}|Q_{\theta_{0}, \pi_{0}}^{\text{soft}}(S_{0}^{E},A_{0}^{E})-Q_{\theta_{0}}^{\text{soft}}(S_{0} ^{E},A_{0}^{E})|\] \[-\alpha_{t}|Q_{\theta_{t},\pi_{t}}^{\text{soft}}(S_{t}^{E},A_{t}^{ E})-Q_{\theta_{t}}^{\text{soft}}(S_{t}^{E},A_{t}^{E})|\bigg{]}+\bigg{(}\frac{32C_{Q} \bar{C}_{r}^{2}C_{d}}{(1-\gamma)^{3}}+\frac{8\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^ {2}}\bigg{)}\sum_{i=0}^{t-1}\alpha_{i}^{2},\]

where \((n)\) follows (8), \((o)\) follows (10), \((p)\) follows (15). Therefore, we have that

\[E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot,\cdot)\}_{i\geq 0}} \bigg{[}\sum_{t=0}^{T-1}\alpha_{T-1}||\nabla\bar{L}(\theta_{t})||^{2}\bigg{]},\] (17) \[\leq E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}\sum_{t=0}^{T-1}\alpha_{t}\frac{1-\gamma^{t}}{1-\gamma}|| \nabla\bar{L}(\theta_{t})||^{2}\bigg{]},\] \[\leq\bar{L}(\theta_{T})-\bar{L}(\theta_{0})+\frac{8\bar{C}_{r}^{2} C_{d}(2-\gamma)}{(1-\gamma)^{3}}E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}( \cdot,\cdot)\}_{i\geq 0}}\bigg{[}\alpha_{0}|Q_{\theta_{0},\pi_{0}}^{\text{soft}}(S_{0} ^{E},A_{0}^{E})-Q_{\theta_{0}}^{\text{soft}}(S_{0}^{E},A_{0}^{E})|\] \[-\alpha_{T}|Q_{\theta_{T},\pi_{T}}^{\text{soft}}(S_{T}^{E},A_{T}^{ E})-Q_{\theta_{T}}^{\text{soft}}(S_{T}^{E},A_{T}^{E})|\bigg{]}+\bigg{(}\frac{32C_{Q} \bar{C}_{r}^{2}C_{d}}{(1-\gamma)^{3}}+\frac{8\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^ {2}}\bigg{)}\sum_{i=0}^{T-1}\alpha_{i}^{2},\] \[\Rightarrow E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot,\cdot)\}_{i \geq 0}}\bigg{[}\sum_{t=0}^{T-1}||\nabla\bar{L}(\theta_{t})||^{2}\bigg{]} \leq D_{2}\sqrt{T}+D_{3}\sqrt{T}(\log T+1),\] (18)

where \(D_{2}=\bar{L}(\theta_{T})-\bar{L}(\theta_{0})+\frac{8\bar{C}_{r}^{2}C_{d}(2- \gamma)}{(1-\gamma)^{3}}E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot,\cdot)\}_ {i\geq 0}}\bigg{[}\alpha_{0}|Q_{\theta_{0},\pi_{0}}^{\text{soft}}(S_{0}^{E},A_{0}^{ E})-Q_{\theta_{0}}^{\text{soft}}(S_{0}^{E},A_{0}^{E})|-\alpha_{T}|Q_{\theta_{T}, \pi_{T}}^{\text{soft}}(S_{T}^{E},A_{T}^{E})-Q_{\theta_{T}}^{\text{soft}}(S_{T}^{E}, A_{T}^{E})|\bigg{]}\) and \(D_{3}=\frac{2(1-\gamma)}{\lambda}\bigg{(}\frac{32C_{Q}\bar{C}_{r}^{2}C_{d}}{(1- \gamma)^{3}}+\frac{8\bar{C}_{r}^{2}C_{L}}{(1-\gamma)^{2}}\bigg{)}\). Then we can see that

\[E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot,\cdot)\}_{i\geq 0}} \bigg{[}\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(S _{i}^{E},A_{i}^{E}))||^{2}\bigg{]},\] \[\leq 2E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{x_{E}}(\cdot,\cdot)\}_{i\geq 0}} \bigg{[}\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L_{i}(\theta_{t};(S _{i}^{E},A_{i}^{E}))-\gamma^{i}\nabla\bar{L}(\theta_{t})||^{2}\bigg{]}\]\[+2E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i\geq 0}} \biggl{[}\sum_{t=0}^{T-1}||\frac{1-\gamma^{t}}{(t+1)(1-\gamma)}\nabla\bar{L}( \theta_{t})||^{2}\biggr{]},\] \[\leq 2E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\biggl{[}\sum_{t=0}^{T-1}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L_{i}( \theta_{t};(S^{E}_{i},A^{E}_{i}))-\gamma^{i}\nabla\bar{L}(\theta_{t})||^{2} \biggr{]}\] \[+2E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\biggl{[}\sum_{t=0}^{T-1}||\nabla\bar{L}(\theta_{t})||^{2}\biggr{]},\] \[\stackrel{{(g)}}{{\leq}}\sum_{t=0}^{T-1}\frac{8 \bar{C}^{2}_{r}(2-\gamma)^{2}}{(t+1)(1-\gamma)^{4}}+2E_{\{(S^{E}_{i},A^{E}_{i} )\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i\geq 0}}\biggl{[}\sum_{t=0}^{T-1}|| \nabla\bar{L}(\theta_{t})||^{2}\biggr{]},\] \[\leq D_{1}(\log T+1)+D_{2}\sqrt{T}+D_{3}\sqrt{T}(\log T+1),\]

where \(D_{1}=\frac{8C^{2}_{r}(2-\gamma)^{2}}{(1-\gamma)^{4}}\). The \((q)\) follows (11). Note that to achieve this local regret rate, we actually need to take an extra expectation over the dynamics \(P\) because we need to roll out \(\pi_{t}\) to formulate \(g_{t}\). Here, we omit the expectation over the dynamics.

### Proof of Theorem 1

We know that

\[E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\biggl{[}\sum_{i=0}^{t}||\nabla L_{i}(\theta_{t};(S^{E}_{i},A^{E}_{i})) ||^{2}\biggr{]},\] \[\leq E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_ {i\geq 0}}\biggl{[}\sum_{i=0}^{t}||\gamma^{i}\nabla\bar{L}(\theta_{t})||^{2} \biggr{]}\] \[+E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\biggl{[}\sum_{i=0}^{t}||\nabla L_{i}(\theta_{t};(S^{E}_{i},A^{E}_{i}) )-\gamma^{i}\nabla\bar{L}(\theta_{t})||^{2}\biggr{]},\] \[\leq E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\biggl{[}\sum_{i=0}^{t}||\nabla\bar{L}(\theta_{t})||^{2}\biggr{]}\] \[+E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i \geq 0}}\biggl{[}\sum_{i=0}^{t}||\nabla L_{i}(\theta_{t};(S^{E}_{i},A^{E}_{i}) )-\gamma^{i}\nabla\bar{L}(\theta_{t})||^{2}\biggr{]},\] \[\stackrel{{(a)}}{{\leq}}O(\sqrt{t+1}+\sqrt{t+1}\log (t+1))+\sum_{i=0}^{t}\gamma^{2i}\cdot 4\bar{C}^{2}_{r}\biggl{(}\frac{2-\gamma}{1- \gamma}\biggr{)}^{2},\] \[\Rightarrow E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}(\cdot, \cdot)\}_{i\geq 0}}\biggl{[}\frac{1}{t+1}\sum_{i=0}^{t}||\nabla L_{i}(\theta_{t};( S^{E}_{i},A^{E}_{i}))||^{2}\biggr{]}\leq O(\frac{1}{\sqrt{t+1}}+\frac{\log(t+1)}{ \sqrt{t+1}}+\frac{1}{t+1}),\] (19)

where \((a)\) follows (18) and (11).

Therefore,

\[\sum_{t=0}^{T-1}E_{\{(S^{E}_{i},A^{E}_{i})\sim\mathbb{P}^{\pi_{E}} _{i}(\cdot,\cdot)\}_{i\geq 0}}\biggl{[}||\frac{1}{t+1}\sum_{i=0}^{t}\nabla L_{i}( \theta_{t};(S^{E}_{i},A^{E}_{i}))||^{2}\biggr{]},\] \[\leq\sum_{t=0}^{T-1}E_{\{(S^{E}_{i},A^{E}_{i})\sim\mathbb{P}^{\pi _{E}}_{i}(\cdot,\cdot)\}_{i\geq 0}}\biggl{[}\frac{1}{t+1}\sum_{i=0}^{t}||\nabla L_{i}( \theta_{t};(S^{E}_{i},A^{E}_{i}))||^{2}\biggr{]},\] \[\leq\sum_{t=0}^{T-1}E_{\{(S^{E}_{i},A^{E}_{i})\sim\mu^{\pi_{E}}( \cdot,\cdot)\}_{i\geq 0}}\biggl{[}\frac{1}{t+1}\sum_{i=0}^{t}||\nabla L_{i}( \theta_{t};(S^{E}_{i},A^{E}_{i}))||^{2}\biggr{]}\]\[+\sum_{t=0}^{T-1}\frac{1}{t+1}\sum_{i=0}^{t}\biggl{[}E_{\{(S_{i}^{E},A_{i}^{E}) \sim p_{i}^{\pi_{E}}(\cdot,\cdot)\}_{i\geq 0}}[\|\nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E} ))\|^{2}\] \[-E_{\{(S_{i}^{E},A_{i}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)\}_{i\geq 0 }}[\|\nabla L_{i}(\theta_{t};(S_{i}^{E},A_{i}^{E}))\|^{2}]\biggr{]},\] \[\overset{(s)}{\leq}D_{1}\log T+D_{2}\sqrt{T}+D_{3}\log T\sqrt{T}+ \sum_{t=0}^{T-1}\frac{1}{t+1}\sum_{i=0}^{t}8C_{M}\bar{C}_{r}^{2}\biggl{(}\frac{ 2-\gamma}{1-\gamma}\biggr{)}^{2}\rho^{i}\gamma^{2i},\] \[\leq D_{1}\log T+D_{2}\sqrt{T}+D_{3}\log T\sqrt{T}+\sum_{t=0}^{T- 1}\frac{1}{t+1}\cdot 8C_{M}\bar{C}_{r}^{2}\biggl{(}\frac{2-\gamma}{1-\gamma} \biggr{)}^{2}\frac{1}{1-\rho\gamma^{2}},\] \[\leq\biggl{(}D_{1}+\frac{8C_{M}\bar{C}_{r}^{2}(2-\gamma)^{2}}{(1 -\rho\gamma^{2})(1-\gamma)^{2}}\biggr{)}\log T+D_{2}\sqrt{T}+D_{3}\log T\sqrt{ T},\]

where \((s)\) follows (19) and Proposition 1. Note that to achieve this local regret rate, we actually need to take an extra expectation over the dynamics \(P\) because we need to roll out \(\pi_{t}\) to formulate \(g_{t}\). Here, we omit the expectation over the dynamics.

### Proof of Theorem 2

Suppose the expert reward function \(r_{E}\) and the parameterized reward function \(r_{\theta}\) are both linear, i.e., \(r_{E}=\theta_{E}^{\top}\phi\) and \(r_{\theta}=\theta^{\top}\phi\), where \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}_{+}^{n}\) is an \(n\)-dimensional feature vector such that \(||\phi(s,a)||\leq\bar{C}_{r}\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\). The proof follows the similar idea with that of Theorem 1 where follow two-step process: step (i) quantifies the regret under the stationary distribution \(\mu^{\pi_{E}}(\cdot,\cdot)\) and step (ii) quantifies the difference between the correlated distribution \(\mathbb{P}_{t}^{\pi_{E}}\) and the stationary distribution \(\mu^{\pi_{E}}\). We start our proof with the following claim:

**Claim 6**.: _If the parameterized reward function \(r_{\theta}\) is linear, the function \(\bar{L}(\theta)\) is \(\lambda\)-strongly convex for any \(\theta\)._

Proof.: Recall that \(\bar{L}(\theta)=E_{(\bar{S},\bar{A})\sim\mu^{\pi_{E}}(\cdot,\cdot)}[-\log\pi_ {\theta}(\bar{A}|\bar{S})]+\frac{\lambda}{2}||\theta-\bar{\theta}||^{2}\). Define \(\bar{L}(\theta;(S,A))\triangleq-\log\pi_{\theta}(\bar{A}|\bar{S})+\frac{ \lambda}{2}||\theta-\bar{\theta}||^{2}\), from Lemma 3, we can see that

\[\nabla\bar{L}(\theta;(\bar{S},\bar{A}))=E_{S,A}^{\pi_{\theta}}\biggl{[}\sum_{ i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\biggl{|}S_{0}=\bar{S}\biggr{]}-E_{S,A}^{ \pi_{\theta}}\biggl{[}\sum_{i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\biggl{|} S_{0}=\bar{S},A_{0}=\bar{A}\biggr{]}+\lambda(\theta-\bar{\theta}).\]

Therefore, we have that

\[\nabla_{\theta\bar{\theta}}^{2}\bar{L}(\theta;(\bar{S},\bar{A})),\] \[=\nabla_{\theta}E_{S,A}^{\pi_{\theta}}\biggl{[}\sum_{i=0}^{\infty }\gamma^{i}\phi(S_{i},A_{i})\biggl{|}S_{0}=\bar{S}\biggr{]}-\nabla_{\theta}E_{ S,A}^{\pi_{\theta}}\biggl{[}\sum_{i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{i}) \biggl{|}S_{0}=\bar{S},A_{0}=\bar{A}\biggr{]}+\lambda.\]

Now, we take a look at

\[\nabla_{\theta}E_{S,A}^{\pi_{\theta}}\biggl{[}\sum_{i=0}^{\infty }\gamma^{i}\phi(S_{i},A_{i})\biggl{|}S_{0}=\bar{S},A_{0}=\bar{A}\biggr{]}= \nabla_{\theta}\biggl{\{}\phi(\bar{S},\bar{A})\] \[+\int_{s_{1}\in\mathcal{S}}P(s_{1}|\bar{S},\bar{A})\int_{a_{t+1} \in\mathcal{A}}\pi_{\theta}(a_{1}|s_{1})E_{S,A}^{\pi_{\theta}}\biggl{[}\sum_{ i=1}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\biggl{|}S_{1}=s_{1},A_{1}=a_{1} \biggr{]}da_{1}ds_{1}\biggr{\}},\] \[=\int_{s_{1}\in\mathcal{S}}P(s_{1}|\bar{S},\bar{A})\int_{a_{1}\in \mathcal{A}}\biggl{\{}\nabla_{\theta}\pi_{\theta}(a_{1}|s_{1})\cdot E_{S,A}^{\pi _{\theta}}\biggl{[}\sum_{i=1}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\biggl{|}S_{1}= s_{1},A_{1}=a_{1}\biggr{]}\] \[+\pi_{\theta}(a_{1}|s_{1})\cdot\nabla_{\theta}E_{S,A}^{\pi_{ \theta}}\biggl{[}\sum_{i=1}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\biggl{|}S_{1}= s_{1},A_{1}=a_{1}\biggr{]}\biggr{\}}da_{1}ds_{1}.\]

Keep the expansion, we can see that

\[\nabla_{\theta}E_{S,A}^{\pi_{\theta}}\biggl{[}\sum_{i=0}^{\infty}\gamma^{i}\phi(S _{i},A_{i})\biggl{|}S_{0}=\bar{S},A_{0}=\bar{A}\biggr{]},\]\[=E_{S,A}^{\pi_{\theta}}\bigg{\{}\sum_{i=0}^{\infty}\nabla_{\theta}\pi_{ \theta}(A_{i}^{\prime}|S_{i}^{\prime})\cdot E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{ i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\bigg{|}S_{0}=\bar{S},A_{0}=A\bigg{]}\bigg{|}S_{0}^{ \prime}=\bar{S},A_{0}^{\prime}=\bar{A}\bigg{\}},\] \[\nabla_{\theta}E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty} \gamma^{i}\phi(S_{i},A_{i})\bigg{|}S_{0}=\bar{S}\bigg{]},\] \[=E_{S,A}^{\pi_{\theta}}\bigg{\{}\sum_{i=0}^{\infty}\nabla_{ \theta}\pi_{\theta}(A_{i}^{\prime}|S_{i}^{\prime})\cdot E_{S,A}^{\pi_{\theta}} \bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\bigg{|}S_{0}=S_{0}^{ \prime},A_{0}=A_{0}^{\prime}\bigg{]}\bigg{|}S_{0}^{\prime}=\bar{S}\bigg{\}},\]

Thus we have that

\[E_{(\bar{S},\bar{A})\sim\mu^{\pi_{E}}(\cdot,\cdot)}\bigg{\{} \nabla_{\theta}E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\phi (S_{i},A_{i})\bigg{|}S_{0}=\bar{S}\bigg{]}-\nabla_{\theta}E_{S,A}^{\pi_{\theta} }\bigg{[}\sum_{i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\bigg{|}S_{0}=\bar{S},A _{0}=\bar{A}\bigg{]}\bigg{\}},\] \[=E_{(\bar{S},\bar{A})\sim\mu^{\pi_{E}}(\cdot,\cdot)}E_{S^{\prime},A^{\prime}}^{\pi_{\theta}}\bigg{\{}\nabla_{\theta}\pi_{\theta}(A_{0}^{\prime} |S_{0}^{\prime})\cdot E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^ {i}\phi(S_{i},A_{i})\bigg{|}S_{0}=S_{0}^{\prime},A_{0}=A_{0}^{\prime}\bigg{]} \bigg{|}S_{0}^{\prime}=\bar{S}_{0}\bigg{\}}\] \[+E_{(\bar{S},\bar{A})\sim\mu^{\pi_{E}}(\cdot,\cdot)}\bigg{\{}E_{ S^{\prime},A^{\prime}}^{\pi_{\theta}}\bigg{\{}\sum_{i=1}^{\infty}\nabla_{ \theta}\pi_{\theta}(A_{i}^{\prime}|S_{i}^{\prime})\cdot E_{S,A}^{\pi_{\theta}} \bigg{[}\sum_{i=1}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\bigg{|}S_{1}=S_{1}^{ \prime},A_{1}=A_{1}^{\prime}\bigg{]}\bigg{|}S_{0}^{\prime}=\bar{S}\bigg{\}}\] \[-E_{S,A}^{\pi_{\theta}}\bigg{\{}\sum_{i=1}^{\infty}\nabla_{ \theta}\pi_{\theta}(A_{i}^{\prime}|S_{i}^{\prime})\cdot E_{S,A}^{\pi_{\theta}} \bigg{[}\sum_{i=1}^{\infty}\gamma^{i}\phi(S_{i},A_{i})\bigg{|}S_{1}=S_{1}^{ \prime},A_{1}=A_{1}^{\prime}\bigg{]}\bigg{|}S_{0}=\bar{S},A_{0}=\bar{A}\bigg{\}} \bigg{\}},\] \[=E_{(\bar{S},\bar{A})\sim\mu^{\pi_{E}}(\cdot,\cdot)}E_{S^{\prime},A^{\prime}}^{\pi_{\theta}}\bigg{\{}\nabla_{\theta}\pi_{\theta}(A_{0}^{\prime }|S_{0}^{\prime})\cdot E_{S,A}^{\pi_{\theta}}\bigg{[}\sum_{i=0}^{\infty}\gamma^ {i}\phi(S_{i},A_{i})\bigg{|}S_{0}=S_{0}^{\prime},A_{0}=A_{0}^{\prime}\bigg{]} \bigg{|}S_{0}^{\prime}=\bar{S}_{0}\bigg{\}}\] \[\overset{(a)}{=}E_{(\bar{S},\bar{A})\sim\mu^{\pi_{E}}(\cdot,\cdot)} E_{S^{\prime},A^{\prime}}^{\pi_{\theta}}\bigg{\{}\pi_{\theta}(A_{0}^{\prime}|S_{0}^{ \bar{E}})[X-EX]X|S_{0}=\bar{S}\bigg{\}},\] \[=\text{Cov}(X)\succ 0,\] (20)

where \((a)\) follows Lemma 3, \(X\triangleq E_{S,A}^{\pi_{\theta}}[\sum_{i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{ i})|S_{0}=\bar{S},A_{0}=A]\) is a random vector, \(EX\triangleq E_{S,A}^{\pi_{\theta}}[\sum_{i=0}^{\infty}\gamma^{i}\phi(S_{i},A_{ i})|S_{0}=\bar{S}]\) is the expectation of \(X\) over the action distribution, and \(\text{Cov}(X)\) is the covariance matrix of the random vector \(X\) with itself, which is always positive definite because the policy \(\pi_{\theta}\) is always stochastic.

Therefore, we can see that

\[||\nabla_{\theta\theta}^{2}\bar{L}(\theta)||=||\text{Cov}(X)+\lambda||\overset{(b )}{\geq}\lambda,\]

where \((b)\) follows (20). 

**Step (i)**. We first quantify the regret under the stationary distribution \(\mu^{\pi_{E}}(\cdot,\cdot)\). Suppose \(\theta^{*}\in\arg\min\bar{L}(\theta)=\arg\min\frac{1-\gamma^{T}}{1-\gamma}\bar{L }(\theta)=\arg\min E_{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{E}}}[\sum_{t=0}^{T-1}L_ {t}(\theta;(S_{t}^{E},A_{t}^{E}))]\).

\[||\theta_{t+1}-\theta^{*}||^{2}=||\theta_{t}-\alpha_{t}g_{t}- \theta^{*}||^{2}=||\theta_{t}-\theta^{*}||^{2}+\alpha_{t}^{2}||g_{t}||^{2}-2 \alpha_{t}\langle g_{t},\theta_{t}-\theta^{*}\rangle,\] \[\Rightarrow E[||\theta_{t+1}-\theta^{*}||^{2}],\] \[\overset{(c)}{\leq}E[||\theta_{t}-\theta^{*}||^{2}]+\frac{16 \alpha_{t}^{2}\bar{C}_{r}^{2}}{(1-\gamma)^{2}}-2\alpha_{t}\langle\frac{1- \gamma^{t+1}}{1-\gamma}\nabla\bar{L}(\theta_{t}),\theta_{t}-\theta^{*}\rangle-2 \alpha_{t}\langle g_{t}-\frac{1-\gamma^{t+1}}{1-\gamma}\nabla\bar{L}(\theta_{t}), \theta_{t}-\theta^{*}\rangle,\] (21)

where \((c)\) follows (8)

Since \(\bar{L}(\theta)\) is \(\lambda\)-strongly convex, we have that

\[\bar{L}(\theta^{*})\geq\bar{L}(\theta_{t})+\langle\nabla\bar{L}( \theta_{t}),\theta^{*}-\theta_{t}\rangle+\frac{\lambda}{2}||\theta_{t}-\theta^{*} ||^{2},\] \[\Rightarrow\bar{L}(\theta_{t})-\bar{L}(\theta^{*})\leq\langle \nabla\bar{L}(\theta_{t}),\theta_{t}-\theta^{*}\rangle-\frac{\lambda}{2}|| \theta_{t}-\theta^{*}||^{2},\]\[\leq\bar{D}_{4}+\frac{32C_{Q}C_{d}\hat{C}C_{r}^{2}}{\lambda(1-\gamma)^{3}}( \log T+1),\] (23)

where \((f)\) follows (16), and \(\bar{D}_{4}=\frac{1-\gamma-\lambda\alpha_{0}}{2\alpha_{0}}E_{(S_{t}^{E},A_{t}^{ E})\sim\mu^{\pi_{E}}(\cdot,\cdot)}[||\theta_{t}-\theta^{*}||^{2}]-\frac{1- \gamma}{2\alpha_{T-1}(1-\gamma^{T})}E_{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{E}}( \cdot,\cdot)}[||\theta_{T}-\theta^{*}||^{2}]-\frac{4C_{C}C_{d}\hat{C}}{(1- \gamma)^{2}}[|Q^{\text{soft}}_{\theta_{0},\pi_{T}}(S,A)-Q^{\text{soft}}_{\theta_ {T}}(S,A)|-|Q^{\text{soft}}_{\theta_{T},\pi_{T}}(S,A)|-|Q^{\text{soft}}_{\theta_ {T},\pi_{T}}(S,A)-Q^{\text{soft}}_{\theta_{T}}(S,A)|].\)

**Step (ii)**. We now quantify the difference between the stationary distribution \(\mu^{\pi_{E}}(\cdot,\cdot)\) and the correlated distribution \(\mathbb{P}_{t}^{\pi_{E}}(\cdot,\cdot)\). Note that \(||\theta_{t}||\) is bounded (proved in Claim 1), the soft Bellman policy \(\pi_{\theta}\) is continuous in \(\theta\) and \((s,a)\), and we assume that the state-action space is bounded, there is a positive constant \(C_{\pi}\) such that \(||\log\pi_{\theta_{t}}(a|s)||\leq C_{\pi}\) for any \((s,a)\in\mathcal{S}\times\mathcal{A}\). We start with the following relation:

\[\bigg{|}E_{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{E}}(\cdot,\cdot)} \bigg{[}L_{t}(\theta_{t};(S_{t}^{E},A_{t}^{E}))\bigg{]}-E_{(S_{t}^{E},A_{t}^{ E})\sim\mathbb{P}_{t}^{\pi_{E}}(\cdot,\cdot)}\bigg{[}L_{t}(\theta_{t};(S_{t}^{E},A_{t}^ {E}))\bigg{]}\bigg{|}\bigg{|},\] \[=\gamma^{t}\bigg{|}\bigg{|}\int_{s\in\mathcal{S}}\int_{a\in \mathcal{A}}|\mathbb{P}_{t}^{\pi_{E}}(s,a)-\mu^{\pi_{E}}(s,a)|\cdot||\log\pi_{ \theta_{t}}(a|s)||dads\bigg{|}\bigg{|},\] \[\leq C_{\pi}C_{M}\gamma^{t}\rho^{t},\] \[\Rightarrow\bigg{|}\bigg{|}E_{(S_{t}^{E},A_{t}^{E})\sim\mu^{\pi_{ E}}(\cdot,\cdot)}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta_{t};(S_{t}^{E},A_{t}^{E})) \bigg{]}-E_{(S_{t}^{E},A_{t}^{E})\sim\mathbb{P}_{t}^{\pi_{E}}(\cdot,\cdot)} \bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta_{t};(S_{t}^{E},A_{t}^{E}))\bigg{]}\bigg{|} \bigg{|},\]\[\leq\frac{C_{\pi}C_{M}}{1-\gamma\rho}.\] (24)

Therefore, we have that

\[E_{(S^{E}_{t},A^{E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}}\bigg{[}\sum_{ t=0}^{T-1}L_{t}(\theta_{t};(S^{E}_{t},A^{E}_{t}))\bigg{]}-\min_{\theta}E_{(S^{E}_{t},A^ {E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta;(S^{ E}_{t},A^{E}_{t}))\bigg{]},\] \[\leq E_{(S^{E}_{t},A^{E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}}\bigg{[} \sum_{t=0}^{T-1}L_{t}(\theta_{t};(S^{E}_{t},A^{E}_{t}))\bigg{]}-E_{(S^{E}_{t},A^ {E}_{t})\sim\mu^{\pi_{E}}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta_{t};(S^{E}_{t}, A^{E}_{t}))\bigg{]}\] \[+E_{(S^{E}_{t},A^{E}_{t})\sim\mu^{\pi_{E}}}\bigg{[}\sum_{t=0}^{T-1 }L_{t}(\theta_{t};(S^{E}_{t},A^{E}_{t}))-\sum_{t=0}^{T-1}L_{t}(\theta^{*};(S^{ E}_{t},A^{E}_{t}))\bigg{]}\] \[+E_{(S^{E}_{t},A^{E}_{t})\sim\mu^{\pi_{E}}}\bigg{[}\sum_{t=0}^{T- 1}L_{t}(\theta^{*};(S^{E}_{t},A^{E}_{t}))\bigg{]}-\min_{\theta}E_{(S^{E}_{t},A ^{E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta;(S^ {E}_{t},A^{E}_{t}))\bigg{]},\] \[\leq E_{(S^{E}_{t},A^{E}_{t})\sim\mathbb{P}^{\pi_{E}}_{t}}\bigg{[} \sum_{t=0}^{T-1}L_{t}(\theta_{t};(S^{E}_{t},A^{E}_{t}))\bigg{]}-E_{(S^{E}_{t},A ^{E}_{t})\sim\mu^{\pi_{E}}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta_{t};(S^{E}_{t},A^{E}_{t}))\bigg{]}\] \[+E_{(S^{E}_{t},A^{E}_{t})\sim\mu^{\pi_{E}}}\bigg{[}\sum_{t=0}^{T- 1}L_{t}(\theta^{*};(S^{E}_{t},A^{E}_{t}))-\sum_{t=0}^{T-1}L_{t}(\theta^{*};(S^ {E}_{t},A^{E}_{t}))\bigg{]}\] \[+E_{(S^{E}_{t},A^{E}_{t})\sim\mu^{\pi_{E}}}\bigg{[}\sum_{t=0}^{T- 1}L_{t}(\theta^{*};(S^{E}_{t},A^{E}_{t}))\bigg{]}-E_{(S^{E}_{t},A^{E}_{t})\sim \mathbb{P}^{\pi_{E}}_{t}}\bigg{[}\sum_{t=0}^{T-1}L_{t}(\theta^{*};(S^{E}_{t}, A^{E}_{t}))\bigg{]},\] \[\overset{(\text{\ref{eq:E_1\(1\)1_1_2}})}{\leq}\frac{C_{\pi}C_{M}} {1-\gamma\rho}+\bar{D}_{4}+\frac{32C_{Q}C_{d}\hat{C}\bar{C}_{r}^{2}}{\lambda( 1-\gamma)^{3}}(\log T+1)+\frac{C_{\pi}C_{M}}{1-\gamma\rho},\] \[=D_{4}+D_{5}(\log T+1),\]

where \((f)\) follows (23) and (24), \(D_{4}=\frac{2C_{\pi}C_{M}}{1-\gamma\rho}+\bar{D}_{4}\), and \(D_{5}=\frac{32C_{Q}C_{d}\hat{C}\hat{C}\bar{C}_{r}^{2}}{\lambda(1-\gamma)^{3}}\).

## Appendix C Meta-Regularization Algorithm and Convergence Guarantee

To solve problem (5), we use a double-loop method, i.e., we first solve the lower-level problem for \(K\) iterations to get an approximate \(\phi_{j,K}\) of \(\phi_{j}\) and then use the approximate \(\phi_{j,K}\) to solve the upper-level problem. We do not use a single-loop method to solve (5) because we need to get the task-specific adaptation \(\phi_{j}\) for each task \(\mathcal{T}_{j}\). The single-loop method only partially solves the lower-level problem by one-step gradient descent and thus the obtained parameter can be far away from \(\phi_{j}\).

**Lemma 5**.: _The gradient of the lower-level problem in (5) is \(E_{S,A}^{\pi_{\phi}}\big{[}\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi}(S _{t},A_{t})\big{|}S_{0}=s_{0}^{\rm tr}\big{]}-\sum_{t=0}^{\infty}\gamma^{t} \nabla_{\phi}r_{\phi}(s_{t}^{\rm tr},a_{t}^{\rm tr})+\frac{\lambda}{1-\gamma} (\phi-\bar{\theta})\) where \((s_{t}^{\rm tr},a_{t}^{\rm tr})\in\mathcal{D}_{j}^{\rm tr}\)._

We roll out the policy \(\pi_{\phi}\) from \(s_{t}^{\rm tr}\) to get a trajectory \(s_{0}^{\phi},a_{0}^{\phi},\cdots\) where \(s_{0}^{\phi}=s_{t}^{\rm tr}\) and approximate the lower-level gradient by \(g_{\phi}=\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi}(s_{t}^{\phi},a_{t}^{ \phi})-\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi}(s_{t}^{\rm tr},a_{t}^ {\rm tr})+\frac{\lambda}{1-\gamma}(\phi-\bar{\theta})\). We update \(\phi_{j,k+1}=\phi_{j,k}-\beta_{k}g_{\phi_{j,k}}\) for \(K\) times to get \(\phi_{j,K}\) where \(\beta_{k}\) is the step size and \(\phi_{j,k}\) is the parameter at time \(k\), and use \(\phi_{j,K}\) to calculate the approximate of the hyper-gradient \(\frac{d}{d\theta}L(\phi_{j},\mathcal{D}_{j}^{\rm eval})\).

**Lemma 6**.: _The hyper-gradient (i.e., gradient of the upper-level problem in (5)) is \(\frac{d}{d\theta}L(\phi_{j},\mathcal{D}_{j}^{\rm eval})=\big{[}I+\frac{1-\gamma}{ \lambda}\nabla_{\phi\phi}^{2}L(\phi_{j},\mathcal{D}_{j}^{\rm tr})\big{]}^{-1} \nabla_{\phi}L(\phi_{j},\mathcal{D}_{j}^{\rm eval})\), where_

\[\nabla_{\phi}L(\phi_{j},\mathcal{D}_{j}^{\rm eval}) =|\mathcal{D}_{j}^{\rm eval}|\cdot E_{S,A}^{\pi_{\phi_{j}}}\bigg{[} \sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi_{j}}(S_{t},A_{t})\bigg{|}S_{0} \sim P_{0}\bigg{]}-\sum_{v=1}^{|\mathcal{D}_{j}^{\rm eval}|}\sum_{t=0}^{\infty} \gamma^{t}\nabla_{\phi}r_{\phi_{j}}(s_{t}^{v},a_{t}^{v}),\] \[\nabla_{\phi\phi}^{2}L(\phi_{j},\mathcal{D}_{j}^{\rm tr}) =E_{S,A}^{\pi_{\phi_{j}}}\bigg{[}\sum_{t=0}^{\infty}\gamma^{t}\nabla_ {\phi\phi}^{2}r_{\phi_{j}}(S_{t},A_{t})\bigg{|}S_{0}=s_{0}^{\rm tr}\bigg{]}- \sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}^{2}r_{\phi_{j}}(s_{t}^{\rm tr},a_{t}^ {\rm tr})+e.\]_The \(|\mathcal{D}^{\rm eval}_{j}|\) is the number of trajectories in \(\mathcal{D}^{\rm eval}_{j}\), \((s^{v}_{t},a^{v}_{t})\in\mathcal{D}^{\rm eval}_{j}\), and \(e\) is an extra term whose expression can be found in Appendix C.2._

To approximate the hyper-gradient, we first roll out \(\pi_{\phi_{j,K}}\) to get two trajectories \(s^{\phi_{j,K}}_{0},a^{\phi_{j,K}}_{0},\cdots\) and \(\bar{s}^{\phi_{j,K}}_{0},\bar{a}^{\phi_{j,K}}_{0},\cdots\) where \(s^{\phi_{j,K}}_{0}\) is drawn from \(P_{0}\) and \(\bar{s}^{\phi_{j,K}}_{0}=s^{\mathbf{tr}}_{0}\). We estimate \(\nabla_{\phi}L(\phi_{j},\mathcal{D}^{\rm eval}_{j})\) via \(\bar{\nabla}_{\phi}L(\phi_{j,K},\mathcal{D}^{\rm eval}_{j})=|\mathcal{D}^{\rm eval }_{j}|\cdot\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi_{j,K}}(s^{\phi_{ j,K}}_{t},a^{\phi_{j,K}}_{t})-\sum_{v=1}^{|\mathcal{D}^{\rm eval}_{j}|}\sum_{t=0}^{ \infty}\gamma^{t}\nabla_{\phi}r_{\phi_{j,K}}(s^{v}_{t},a^{v}_{t})\) and estimate the term \(\nabla^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\rm tr}_{j})\) via \(\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j,K},\mathcal{D}^{\rm tr}_{j})=\sum_{t=0}^ {\infty}\gamma^{t}\nabla^{2}_{\phi\phi}r_{\phi_{j,K}}(\bar{s}^{\phi_{j,K}}_{t},\bar{a}^{\phi_{j,K}}_{t})-\sum_{t=0}^{\infty}\gamma^{t}\nabla^{2}_{\phi\phi}r _{\phi_{j,K}}(s^{\mathbf{tr}}_{t},a^{\mathbf{tr}}_{t})\). Therefore, we can approximate the hyper-gradient term \(\frac{d}{d\theta}L(\phi_{j},\mathcal{D}^{\rm eval}_{j})\) via \(h_{j}=[I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j,K}, \mathcal{D}^{\rm eval}_{j})]^{-1}\bar{\nabla}_{\phi}L(\phi_{j,K},\mathcal{D}^{ \rm eval}_{j})\). We omit the extra term \(e\) in the approximate \(h_{j}\) and its impact on the convergence can be bounded (proved in Appendix C.4).

To solve the upper-level problem in (5), at each iteration \(n\), we sample a batch of \(B\) tasks, and compute the task-specific adaptation \(\phi_{j,K}\) and the hyper-gradient \(h_{j}\) for each task. The update law to solve the upper-level problem is: \(\bar{\theta}_{n+1}=\bar{\theta}_{n}-\frac{\tau_{n}}{B}\sum_{j=1}^{B}h_{j}\) where \(\tau_{n}\) is the step size. Note that the time index \(k\) is for the lower-level problem and \(n\) is for the upper-level problem.

```
0: Initialized meta-prior \(\bar{\theta}_{0}\) and task-specific adaptation \(\phi_{j,0}\)
0: Learned prior \(\bar{\theta}_{N}\)
1:for\(n=0,1,\cdots,N-1\)do
2: Sample a batch of \(B\) tasks \(\{\mathcal{T}_{j}\}_{j=1}^{B}\sim P_{\mathcal{T}}\)
3:for each task \(\mathcal{T}_{j}\)do
4:for\(k=0,1,\cdots,K-1\)do
5: Compute the soft Bellman policy \(\pi_{\phi_{j,k}}\) via soft Q-learning or soft actor-critic
6: Roll out the policy \(\pi_{\phi_{j,k}}\) to get a trajectory \(s^{\phi_{j,k}}_{0},a^{\phi_{j,k}}_{0},\cdots\)
7: Compute the gradient \(g_{\phi_{j,k}}=\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi_{j,k}}(s^{ \phi_{j,k}}_{t},a^{\phi_{j,k}}_{t})-\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r _{\phi_{j,k}}(s^{\mathbf{tr}}_{t},a^{\mathbf{tr}}_{t})\)\(+\frac{\lambda}{1-\gamma}(\phi_{j,k}-\bar{\theta}_{n})\)
8: Update \(\phi_{j,k+1}=\phi_{j,k}-\beta_{k}g_{\phi_{j,k}}\)
9:endfor
10: Compute the soft Bellman policy \(\pi_{\phi_{j,K}}\) via soft Q-learning or soft actor-critic
11: Roll out the policy \(\pi_{\phi_{j,K}}\) to get two trajectories \(s^{\phi_{j,K}}_{0},a^{\phi_{j,K}}_{0},\cdots\) and \(\bar{s}^{\phi_{j,K}}_{0},\bar{a}^{\phi_{j,K}}_{0},\cdots\)
12: Compute the hyper-gradient \(h_{j}=[I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j,K}, \mathcal{D}^{\rm tr}_{j})]^{-1}\bar{\nabla}_{\phi}L(\phi_{j,K},\mathcal{D}^{ \rm eval}_{j})\)
13:endfor
14: Update \(\bar{\theta}_{n+1}=\bar{\theta}_{n}-\frac{\tau_{n}}{B}\sum_{j=1}^{B}h_{j}\)
15:endfor ```

**Algorithm 2** Meta-regularization

**Lemma 7** (Convergence of the lower-level problem).: _Suppose Assumptions 1-2 hold and \(\lambda\geq\frac{C_{L}}{2}+\eta\) where \(\eta\in(0,\frac{C_{L}}{2})\). Let \(\beta_{k}=\frac{1-\gamma}{\eta(k+1)}\), then we have_

\[E[||\phi_{j,K}-\phi_{j}||^{2}]\leq O(\frac{1}{K}).\]

**Assumption 3**.: _The parameterized reward function \(\tau_{\theta}\) has bounded third-order gradient, i.e., \(||\nabla^{3}_{\theta\theta\theta}r_{\theta}(s,a)||\leq\hat{C}_{r}\) for any \((s,a)\) where \(\hat{C}_{r}\) is a positive constant._

**Theorem 3** (Convergence of the upper-level problem).: _Suppose Assumption 3 and the condition in Lemma 7 hold. Let \(\tau_{n}=(n+1)^{-1/2}\) and define \(F(\bar{\theta})\) as \(E_{j\sim P_{\mathcal{T}}}[L(\phi_{j},\mathcal{D}^{\rm eval}_{j})]\) under the meta-prior \(\bar{\theta}\). Then we have the following convergence:_

\[\frac{1}{N}\sum_{n=0}^{N-1}E[||\nabla F(\bar{\theta}_{n})||^{2}]\leq O(\frac{1 }{\sqrt{N}}+\frac{\log N}{\sqrt{N}}+\frac{1}{K})+C_{1},\]

_and the expression of \(C_{1}\) can be found in (27)._

### Proof of Lemma 5

The proof is similar to that of Lemma 1. We first prove the case of deterministic dynamics and the corresponding result can be an unbiased estimate in cases of stochastic dynamics (proved in Subsection B.2).

\[\nabla_{\phi}L(\phi,\mathcal{D}_{j}^{\text{tr}})=-\sum_{t=0}^{ \infty}\gamma^{t}\nabla_{\phi}\log\pi_{\phi}(a_{t}^{\text{tr}}|s_{t}^{\text{tr} }),\] \[=-\sum_{t=0}^{\infty}\gamma^{t}\bigg{[}\nabla_{\phi}Q_{\phi}^{ \text{soft}}(s_{t}^{\text{tr}},a_{t}^{\text{tr}})-\nabla_{\phi}V_{\phi}^{\text{ soft}}(s_{t}^{\text{tr}})\bigg{]},\] \[=-\sum_{t=0}^{\infty}\gamma^{t}\bigg{[}\nabla_{\phi}r_{\phi}(s_{t }^{\text{tr}},a_{t}^{\text{tr}})+\gamma\nabla_{\phi}V_{\phi}^{\text{soft}}(s_{t +1}^{\text{tr}})-\nabla_{\phi}V_{\phi}^{\text{soft}}(s_{t}^{\text{tr}})\bigg{]},\] \[=\nabla_{\phi}V_{\phi}^{\text{soft}}(s_{0}^{\text{tr}})-\sum_{t=0 }^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi}(s_{t}^{\text{tr}},a_{t}^{\text{tr}}),\] \[\overset{(a)}{=}E_{S,A}^{\pi_{\phi}}\bigg{[}\sum_{t=0}^{\infty} \gamma^{t}\nabla_{\phi}r_{\phi}(S_{t},A_{t})\bigg{|}S_{0}=s_{0}^{\text{tr}} \bigg{]}-\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi}r_{\phi}(s_{t}^{\text{tr}},a _{t}^{\text{tr}}),\]

where \((a)\) follows the proof of Lemma 3.

### Proof of Lemma 6

Since the lower-level problem of (5) is unconstrained and \(\phi_{i}\) is the optimal solution, we know that

\[\nabla_{\phi}L(\phi_{j},\mathcal{D}_{j}^{\text{tr}})+\frac{\lambda}{1-\gamma} (\phi_{j}-\bar{\theta})=0\] (25)

We further take derivative of both sides in (25) with respect to \(\bar{\theta}\) and we get that

\[\bigg{[}\nabla_{\phi\phi}^{2}L(\phi_{j},\mathcal{D}_{j}^{\text{tr}})+\frac{ \lambda}{1-\gamma}\bigg{]}\nabla_{\bar{\theta}}\phi_{j}-\frac{\lambda}{1- \gamma}=0\Rightarrow\nabla_{\bar{\theta}}\phi_{j}=\bigg{[}I+\frac{1-\gamma}{ \lambda}\nabla_{\phi\phi}^{2}L(\phi_{j},\mathcal{D}_{j}^{\text{tr}})\bigg{]}^ {-1}.\]

Therefore, we have that

\[\frac{d}{d\bar{\theta}}L(\phi_{j},\mathcal{D}_{j}^{\text{eval}})=(\nabla_{ \bar{\theta}}\phi_{j})^{\top}\nabla_{\phi}L(\phi_{j},\mathcal{D}_{j}^{\text{ eval}})=\bigg{[}I+\frac{1-\gamma}{\lambda}\nabla_{\phi\phi}^{2}L(\phi_{j}, \mathcal{D}_{j}^{\text{tr}})\bigg{]}^{-1}\nabla_{\phi}L(\phi_{j},\mathcal{D}_ {j}^{\text{eval}}).\]

Similar to Lemma 5, we can know that

\[\nabla_{\phi}L(\phi_{j},\mathcal{D}_{j}^{\text{eval}})=\sum_{v=1}^{| \mathcal{D}_{j}^{\text{eval}}|}E_{S,A}^{\pi_{\phi_{j}}}\bigg{[}\sum_{t=0}^{ \infty}\gamma^{t}\nabla_{\phi}r_{\phi_{j}}(S_{t},A_{t})\bigg{|}S_{0}=s_{0}^{v }\bigg{]}-\sum_{v=1}^{|\mathcal{D}_{j}^{\text{eval}}|}\sum_{t=0}^{\infty} \gamma^{t}\nabla_{\phi}r_{\phi_{j}}(s_{t}^{v},a_{t}^{v}),\] (26)

where \((s_{t}^{v},a_{t}^{v})\in\zeta^{v}\) and \(\zeta^{v}\in\mathcal{D}_{j}^{\text{eval}}\).

Since there is usually abundant data in \(\mathcal{D}_{j}^{\text{eval}}\) and \(S_{0}\sim P_{0}\), we can reformulate (26) as the following:

\[\nabla_{\phi}L(\phi_{j},\mathcal{D}_{j}^{\text{eval}})\approx|\mathcal{D}_{j}^ {\text{eval}}|\cdot E_{S,A}^{\pi_{\phi_{j}}}\bigg{[}\sum_{t=0}^{\infty}\gamma^ {t}\nabla_{\phi}r_{\phi_{j}}(S_{t},A_{t})\bigg{|}S_{0}\sim P_{0}\bigg{]}-\sum_{ v=1}^{|\mathcal{D}_{j}^{\text{eval}}|}\sum_{t=0}^{\infty}\gamma^{t} \nabla_{\phi}r_{\phi_{j}}(s_{t}^{v},a_{t}^{v}).\]

**Claim 7**.: _The second-order information \(\nabla_{\phi\phi}^{2}Q_{\phi}^{\text{soft}}(s,a)=\Delta(s,a)+E_{S,A}^{\pi_{ \phi}}\sum_{t=0}^{\infty}\gamma^{t}\text{Cov}(S_{t})|S_{0}=s,A_{0}=a|\) and \(\nabla_{\phi\phi}^{2}V_{\phi}^{\text{soft}}(s)=\Delta(s)+E_{S,A}^{\pi_{\phi}}[ \sum_{t=0}^{\infty}\gamma^{t}\text{Cov}(S_{t})|S_{0}=s]\) where \(\Delta(s,a)=E_{S,A}^{\pi_{\phi}}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi\phi}^{ 2}r_{\phi}(S_{t},A_{t})|S_{0}=s,A_{0}=a]\), \(\Delta(s)=E_{S,A}^{\pi_{\phi}}[\sum_{t=0}^{\infty}\gamma^{t}\nabla_{\phi\phi}^{ 2}r_{\phi}(S_{t},A_{t})|S_{0}=s]\), and \(\text{Cov}(s)\triangleq\int_{a\in\mathcal{A}}\pi_{\phi}(a|s)[\Delta(s,a)- \Delta(s)]\Delta(s)da\) is the covariance matrix of \(\Delta(s,\cdot)\) at state \(s\)._The proof of Claim 7 follows the proof of Lemma 4

Now we take a look at the term \(\nabla^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})\) and consider the case of deterministic dynamics:

\[\nabla^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})=-\sum _{t=0}^{\infty}\gamma^{t}\nabla^{2}_{\phi\phi}\log\pi_{\phi_{j}}(a^{\text{tr}}_ {t}|s^{\text{tr}}_{t}),\] \[=-\sum_{t=0}^{\infty}\gamma^{t}\bigg{[}\nabla^{2}_{\phi\phi}Q^{ \text{soft}}_{\phi_{j}}(s^{\text{tr}}_{t},a^{\text{tr}}_{t})-\nabla^{2}_{\phi \phi}V^{\text{soft}}_{\phi_{j}}(s^{\text{tr}}_{t})\bigg{]},\] \[=-\sum_{t=0}^{\infty}\gamma^{t}\bigg{[}\nabla^{2}_{\phi\phi}r_{ \phi_{j}}(s^{\text{tr}}_{t},a^{\text{tr}}_{t})+\gamma\nabla^{2}_{\phi\phi}V^{ \text{soft}}_{\phi_{j}}(s^{\text{tr}}_{t+1})-\nabla^{2}_{\phi\phi}V^{\text{ soft}}_{\phi_{j}}(s^{\text{tr}}_{t})\bigg{]},\] \[=\nabla^{2}_{\phi\phi}V^{\text{soft}}_{\phi_{j}}(s^{\text{tr}}_{0 })-\sum_{t=0}^{\infty}\gamma^{t}\nabla^{2}_{\phi\phi}r_{\phi_{j}}(s^{\text{tr}} _{t},a^{\text{tr}}_{t}),\] \[\stackrel{{(a)}}{{=}}E^{\pi_{\phi_{j}}}_{S,A}[\sum_{t =0}^{\infty}\gamma^{t}\nabla^{2}_{\phi\phi}r_{\phi_{j}}(S_{t},A_{t})|S_{0}=s^{ \text{tr}}_{0}]-\sum_{t=0}^{\infty}\gamma^{t}\nabla^{2}_{\phi\phi}r_{\phi_{j}}( s^{\text{tr}}_{t},a^{\text{tr}}_{t})\] \[+E^{\pi_{\phi_{j}}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\text{Cov}( S_{t})|S_{0}=s^{\text{tr}}_{0}],\]

where \((a)\) follows Claim 7. As proved in Subsection B.2, we can still use this expression in the case of stochastic dynamics. We define \(e\triangleq E^{\pi_{\phi_{j}}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\text{Cov}( S_{t})|S_{0}=s^{\text{tr}}_{0}]\). However, \(e\) is intractable to compute because it requires to compute \(\text{Cov}(S_{t})\) at every visited state. To approximate the covariance matrix \(\text{COV}(S_{t})\), we need to empirically roll out the policy \(\pi_{\phi_{j}}\) from \(S_{t}\) for enough times to get enough samples. We need to do these policy roll-outs at every state \(S_{t}\). For example, suppose we roll out the policy \(\pi_{\phi_{j}}\) ten times to get ten samples at each state \(S_{t}\). Empirically, we need to do these roll-outs \(10\times\bar{T}\) where \(\bar{T}\) is a very large integer that we regard as infinity because the trajectory horizon is infinite. This is intractable because we need to roll out the policy for too many times. Moreover, we cannot guarantee that we can approximate \(\text{COV}(S_{t})\) well given that we only use ten samples to approximate it.

### Proof of Lemma 7

We know that \(||\nabla^{2}_{\phi\phi}L(\phi,\mathcal{D}^{\text{tr}}_{j})+\frac{\lambda}{1- \gamma}||\leq||\nabla^{2}_{\phi\phi}L(\phi,\mathcal{D}^{\text{tr}}_{j})||+ \frac{\lambda}{1-\gamma}\leq\sum_{t=0}^{\infty}\bigg{(}||\nabla^{2}L_{t}( \phi)||+\lambda\gamma^{t}\bigg{)}\stackrel{{(a)}}{{\leq}}\frac{1 }{1-\gamma}C_{L}\) where \((a)\) follows (6). Therefore, the lower-level objective function in (8) is \(\frac{C_{L}}{1-\gamma}\)-smooth. Moreover, since \(\lambda\geq\frac{C_{L}}{2}+\eta\), then \(||\nabla^{2}_{\phi\phi}L(\phi,\mathcal{D}^{\text{tr}}_{j})||\leq\frac{C_{L}-2 \eta}{2(1-\gamma)}\). Therefore, the lower-level objective function in (8) is \(\frac{2\eta}{1-\gamma}\)-strongly convex. Following the standard result for strongly-convex and smooth stochastic optimization, we can reach the result in Lemma 7.

### Proof of Theorem 3

In this proof, we first bound the hyper-gradient approximation error (i.e., \(||\frac{d}{d\theta}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})-h_{j}||\)) and then prove the convergence. Define \(\bar{h}_{j}\triangleq[I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L( \phi_{j},\mathcal{D}^{\text{tr}}_{j})]^{-1}\nabla_{\phi}L(\phi_{j},\mathcal{D }^{\text{eval}}_{j})\) where \(\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})\triangleq E ^{\pi_{\phi_{j}}}_{S,A}[\sum_{t=0}^{\infty}\gamma^{t}\nabla^{2}_{\phi\phi}r_{ \phi_{j}}(S_{t},A_{t})|S_{0}=s^{\text{tr}}_{0}]-\sum_{t=0}^{\infty}\gamma^{t} \nabla^{2}_{\phi\phi}r_{\phi_{j}}(s^{\text{tr}}_{t},a^{\text{tr}}_{t})\). Therefore, we have that

\[||\bar{h}_{j}-\frac{d}{d\theta}L(\phi_{j},\mathcal{D}^{\text{eval }}_{j})||,\] \[\leq\left||\Big{[}I+\frac{1-\gamma}{\lambda}\nabla^{2}_{\phi\phi}L (\phi_{j},\mathcal{D}^{\text{tr}}_{j})]^{-1}-[I+\frac{1-\gamma}{\lambda}\bar{ \nabla}^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})]^{-1}\right|| \cdot||\nabla_{\phi}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})||,\] \[\leq\left[\left||\big{[}I+\frac{1-\gamma}{\lambda}\nabla^{2}_{ \phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})\big{]}^{-1}\right||+\left|| \big{[}I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j},\mathcal{ D}^{\text{tr}}_{j})\big{]}^{-1}\right||\right]\cdot||\nabla_{\phi}L(\phi_{j}, \mathcal{D}^{\text{eval}}_{j})||,\] \[\stackrel{{(a)}}{{\leq}}\left(\frac{2\lambda}{2\lambda+2 \eta-C_{L}}+\frac{\lambda}{\lambda-2\tilde{C}_{r}}\right)\cdot\frac{2|\mathcal{D }^{\text{eval}}_{j}|\bar{C}_{r}}{1-\gamma}\triangleq C_{1}>0,\] (27)where \((a)\) follows the fact that \(||I+\frac{1-\gamma}{\lambda}\nabla^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\text{ tr}}_{j})||\geq 1-||\frac{1-\gamma}{\lambda}\nabla^{2}_{\phi\phi}L(\phi_{j}, \mathcal{D}^{\text{tr}}_{j})||\geq 1-||\frac{1-\gamma}{\lambda}\nabla^{2}_{\phi\phi}L(\phi_{j}, \mathcal{D}^{\text{tr}}_{j})||\geq\frac{C_{L}-2\eta}{2\lambda}\) given that \(||\nabla^{2}_{\phi\phi}L(\phi,\mathcal{D}^{\text{tr}}_{j})||\leq\frac{C_{L}-2 \eta}{2(1-\gamma)}\) (proved in the proof of Lemma 7). Therefore \(||[I+\frac{1-\gamma}{\lambda}\nabla^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{ \text{tr}}_{j})]^{-1}||\leq\frac{2\lambda}{\lambda+2\eta-C_{L}}\). Similarly, we can bound \(||[I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^ {\text{tr}}_{j})]^{-1}||\leq\frac{\lambda}{\lambda-2C_{r}}\) given that \(||\bar{\nabla}^{2}_{\phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})||\leq \frac{2C_{r}}{1-\gamma}\). Note that \(\lambda>\frac{C_{L}}{2}\) and \(C_{L}>\tilde{C}_{r}\) (proved in (6)), therefore \(C_{1}\) is a positive constant.

Now, we bound the term \(||h_{j}-\bar{h}_{j}||\). We define \(\Delta_{\phi_{j}}=I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L(\phi_ {j},\mathcal{D}^{\text{tr}}_{j})\) and \(\Delta_{\phi_{j}}=I+\frac{1-\gamma}{\lambda}\bar{\nabla}^{2}_{\phi\phi}L(\phi_ {j,K},\mathcal{D}^{\text{tr}}_{j})\). Thus we have \(||\Delta^{-1}_{\phi_{j}}||\leq\frac{\lambda}{\lambda-2C_{r}}\) (follows (27)) and similarly \(||\Delta^{-1}_{\phi_{j,K}}||\leq\frac{\lambda}{\lambda-2C_{r}}\). Therefore,

\[E[||h_{j}-\bar{h}_{j}||]=E\bigg{[}\bigg{|}\bigg{|}\Delta^{-1}_{ \phi_{j,K}}\nabla_{\phi}L(\phi_{j,K},\mathcal{D}^{\text{eval}}_{j})-\Delta^{- 1}_{\phi_{j}}\nabla_{\phi}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})\bigg{|} \bigg{|}\bigg{]},\] \[\leq E\bigg{[}\bigg{|}\bigg{|}\Delta^{-1}_{\phi_{j,K}}\nabla_{ \phi}L(\phi_{j,K},\mathcal{D}^{\text{eval}}_{j})-\Delta^{-1}_{\phi_{j,K}} \nabla_{\phi}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})\bigg{|}\bigg{|}\bigg{|}\] \[+\bigg{|}\bigg{|}\Delta^{-1}_{\phi_{j,K}}\nabla_{\phi}L(\phi_{j}, \mathcal{D}^{\text{eval}}_{j})-\Delta^{-1}_{\phi_{j}}\nabla_{\phi}L(\phi_{j}, \mathcal{D}^{\text{eval}}_{j})\bigg{|}\bigg{|}\bigg{]},\] \[\leq E\bigg{[}||\Delta^{-1}_{\phi_{j,K}}||\cdot||\nabla_{\phi}L( \phi_{j,K},\mathcal{D}^{\text{eval}}_{j})-\nabla_{\phi}L(\phi_{j},\mathcal{D} ^{\text{eval}}_{j})||+||\Delta^{-1}_{\phi_{j,K}}-\Delta^{-1}_{\phi_{j}}||\cdot |\nabla_{\phi}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})||\bigg{]}\bigg{]},\] \[\overset{(b)}{\leq}\frac{\lambda}{\lambda-2\tilde{C}_{r}}\cdot| \mathcal{D}^{\text{eval}}_{j}|\cdot\frac{C_{L}-2\eta}{2(1-\gamma)}||\phi_{j,K} -\phi_{j}||+E[||\Delta^{-1}_{\phi_{j,K}}-\Delta^{-1}_{\phi_{j}}||]\cdot| \mathcal{D}^{\text{eval}}_{j}|\cdot\frac{2\tilde{C}_{r}}{1-\gamma},\] \[\leq O(\frac{1}{K})+E[||\Delta^{-1}_{\phi_{j,K}}||\cdot||\Delta^ {-1}_{\phi_{j}}||\cdot|\Delta^{-1}_{\phi_{j,K}}-\Delta^{-1}_{\phi_{j}}||]\cdot |\mathcal{D}^{\text{eval}}_{j}|\cdot\frac{2\tilde{C}_{r}}{1-\gamma},\] \[\leq O(\frac{1}{K})+\tilde{C}_{L}E[||\Delta^{-1}_{\phi_{j,K}}|| \cdot||\Delta^{-1}_{\phi_{j}}||]\cdot|\mathcal{D}^{\text{eval}}_{j}|\cdot \frac{2\tilde{C}_{r}}{1-\gamma}||\phi_{j,K}-\phi_{j}||\leq O(\frac{1}{K}),\] (28)

where \(||\nabla^{3}_{\phi\phi\phi}L(\phi_{j},\mathcal{D}^{\text{tr}}_{j})||\leq\tilde {C}_{L}\). The expression of \(\tilde{C}_{L}\) can be derived following the proof of Lemma 4 by notifying Assumption 3. The \((b)\) holds because (i) \(||\nabla^{2}_{\phi\phi}L(\phi,\mathcal{D}^{\text{eval}}_{j})||=\frac{| \mathcal{D}^{\text{eval}}_{j}||}{|\mathcal{D}^{\text{tr}}_{j}||}||\nabla^{2}_{ \phi\phi}L(\phi,\mathcal{D}^{\text{tr}}_{j})||\leq|\mathcal{D}^{\text{eval}}_{j}| \cdot\frac{C_{L}-2\eta}{2(1-\gamma)}\) and (ii) \(||\nabla_{\phi}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})||\leq|\mathcal{D}^{ \text{eval}}_{j}|\cdot\frac{2\tilde{C}_{r}}{1-\gamma}\) (see the expression of \(\nabla_{\phi}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})\) in Lemma 5).

\[E[||h_{j}-\frac{d}{d\bar{\theta}}L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})||] \leq E[||h_{j}-\bar{h}_{j}||+||\bar{h}_{j}-\frac{d}{d\bar{\theta}}L(\phi_{j}, \mathcal{D}^{\text{eval}}_{j})||]\overset{(c)}{\leq}C_{1}+O(\frac{1}{K}),\] (29)

where \((c)\) follows (27)-(28). Define \(F(\bar{\theta})\) as \(E_{j\sim\mathcal{P}_{r}}[L(\phi_{j},\mathcal{D}^{\text{eval}}_{j})]\) under the meta-prior \(\bar{\theta}\). Note that \(||h_{j}||\leq||\Delta^{-1}_{\phi_{j,K}}||\cdot||\nabla_{\phi}L(\phi_{j,K}, \mathcal{D}^{\text{eval}}_{j})||\leq\frac{\lambda}{\lambda-2\tilde{C}_{r}}\cdot| \mathcal{D}^{\text{eval}}_{j}|\cdot\frac{2\tilde{C}_{r}}{1-\gamma}\). Therefore,

\[F(\bar{\theta}_{n+1})\geq F(\bar{\theta}_{n})+(\nabla F(\bar{ \theta}_{n}))^{\top}(\bar{\theta}_{n+1}-\bar{\theta}_{n})-\frac{|\mathcal{D}^{ \text{eval}}|(C_{L}-2\eta)}{4(1-\gamma)}||\bar{\theta}_{n+1}-\bar{\theta}_{n}||^{2},\] \[\geq F(\bar{\theta}_{n})+\frac{\tau_{n}}{B}\sum_{j=1}^{B}(\nabla F (\bar{\theta}_{n}))^{\top}h_{j}-\frac{|\mathcal{D}^{\text{eval}}|(C_{L}-2 \eta)\tau_{n}^{2}}{4(1-\gamma)}||\frac{1}{B}\sum_{j=1}^{B}h_{j}||^{2},\] \[\geq F(\bar{\theta}_{n})+\frac{\tau_{n}}{B}\sum_{j=1}^{B}(\nabla F (\bar{\theta}_{n}))^{\top}h_{j}-\frac{|\mathcal{D}^{\text{eval}}|(C_{L}-2 \eta)\tau_{n}^{2}}{4(1-\gamma)}\cdot\frac{\lambda}{\lambda-2\tilde{C}_{r}}\cdot| \mathcal{D}^{\text{eval}}_{j}|\cdot\frac{2\bar{C}_{r}}{1-\gamma},\] \[\geq F(\bar{\theta}_{n})+\frac{\tau_{n}}{B}\sum_{j=1}^{B}(\nabla F (\bar{\theta}_{n}))^{\top}h_{j}-\frac{2\lambda\bar{C}_{r}|\mathcal{D}^{ \text{eval}}|^{2}(C_{L}-2\eta)\tau_{n}^{2}}{4(1-\gamma)^{2}(\lambda-2 \tilde{C}_{r})},\] \[\Rightarrow E \[\stackrel{{(d)}}{{\geq}}E[F(\bar{\theta}_{n})]+\tau_{n}E[|| \nabla F(\bar{\theta}_{n})||^{2}]+\tau_{n}(C_{1}+O(\frac{1}{K}))-\frac{2\lambda \bar{C}_{r}|\mathcal{P}^{\text{eval}}|^{2}(C_{L}-2\eta)\tau_{n}^{2}}{4(1- \gamma)^{2}(\lambda-2\bar{C}_{r})},\] \[\Rightarrow\frac{1}{N}\sum_{n=0}^{N-1}\tau_{N}E[||\nabla F(\bar{ \theta}_{n})||^{2}]\leq\frac{1}{N}\sum_{n=0}^{N-1}\tau_{n}E[||\nabla F(\bar{ \theta}_{n})||^{2}],\] \[\leq\frac{1}{N}[F(\bar{\theta}_{N})-F(\bar{\theta}_{0})]+\frac{1} {N}\sum_{n=0}^{N-1}\tau_{n}(C_{1}+O(\frac{1}{K}))+\frac{1}{N}\sum_{n=0}^{N-1} \frac{2\lambda\bar{C}_{r}|\mathcal{D}^{\text{eval}}|^{2}(C_{L}-2\eta)\tau_{n} ^{2}}{4(1-\gamma)^{2}(\lambda-2\bar{C}_{r})},\] \[\Rightarrow\frac{1}{N}\sum_{n=0}^{N-1}E[||\nabla F(\bar{\theta}_{ n})||^{2}]\leq O(\frac{1}{\sqrt{N}}+\frac{\log N}{\sqrt{N}}+\frac{1}{K})+C_{1},\]

where \(|\mathcal{D}^{\text{eval}}|\triangleq\sup_{i\sim P_{T}}\{|\mathcal{D}^{\text{ eval}}_{j}|\}\) and \((d)\) follows (29).

## Appendix D Experiment details

The code was running on a laptop whose processor is AMD Ryzen 7 4700U with Radeon Graphics, 2.00GHz, and the installed RAM is 20.0GB. The operating system is Ubuntu \(18.04\). We use a neural network to parameterize the learned reward function. The neural network has two hidden layers where each hidden layer has 64 neurons. The activation functions are respectively ReLU and Tanh.

### Baselines

Here we provide the update rule for each baseline. Given a learned reward function \(r_{\theta}\), the policy updates of the four baselines are the same with that of MERIT-IRL, i.e., one-step policy iteration. The difference is the reward update.

**IT-IRL**: IT-IRL is MERIT-IRL without the meta-regularization term. Therefore, the reward update of IT-IRL is \(\theta_{t+1}=\theta_{t}-\alpha_{t}g^{\prime}_{t}\) where \(g^{\prime}_{t}=\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s^ {\prime}_{i},a^{\prime}_{i})-\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r_{ \theta_{t}}(s^{\prime\prime}_{i},a^{\prime\prime}_{i})\). Recall from Subsection 4.1 that \(\{(s^{\prime}_{i},a^{\prime}_{i})\}_{i\geq 0}\) is generated by the learned policy \(\pi_{\theta_{t}}\) starting from the initial state \(s^{\prime}_{0}=s^{E}_{0}\), and \(\{(s^{\prime\prime}_{i},a^{\prime\prime}_{i})\}_{i\geq 0}\) is generated by the learned policy \(\pi_{\theta_{t}}\) starting from \((s^{\prime\prime}_{i},a^{\prime\prime}_{t})\) where \((s^{\prime\prime}_{i},a^{\prime\prime}_{i})=(s^{E}_{i},a^{E}_{i})\) for \(0\leq i\leq t\).

**Naive MERIT-IRL**: This method has the meta-regularization term, however, it uses the naive way (depicted in the middle of Figure 1) to update the reward function. In specific, it only compares the partial expert trajectory \(\{s^{E}_{i},a^{E}_{i}\}_{i=0}^{t}\) and partial learner trajectory \(\{s^{\prime}_{i},a^{\prime}_{i}\}_{i=0}^{t}\). Therefore, the reward update of Naive MERIT-IRL is \(\theta_{t+1}=\theta_{t}-\alpha_{t}g^{\prime\prime}_{t}\) where \(g^{\prime\prime}_{t}=\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{\theta_{t}}(s^ {\prime}_{i},a^{\prime}_{i})-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{\theta _{t}}(s^{E}_{i},a^{E}_{i})+\frac{\lambda(1-\gamma^{t+1})}{1-\gamma}(\theta- \bar{\theta})\).

**Naive IT-IRL**: This method does not have the meta-regularization term and uses the naive way to update the reward function. Therefore, the reward update of Naive IT-IRL is \(\theta_{t+1}=\theta_{t}-\alpha_{t}g^{\prime\prime\prime}_{t}\) where \(g^{\prime\prime\prime}_{t}=\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{\theta_{t} }(s^{\prime}_{i},a^{\prime}_{i})-\sum_{i=0}^{t}\gamma^{i}\nabla_{\theta}r_{ \theta_{t}}(s^{E}_{i},a^{E}_{i})\).

**Hindsight**: This method is a standard IRL method with the meta-regularization term where the complete expert trajectory \(\{s^{E}_{i},a^{E}_{i}\}_{i\geq 0}\) and the complete learner trajectory \(\{s^{\prime}_{i},a^{\prime}_{i}\}_{i\geq 0}\) are compared to update the reward function. Therefore, the reward update of Hindsight is \(\theta_{t+1}=\theta_{t}-\alpha_{t}g^{\prime\prime\prime}_{t}\) where \(g^{\prime\prime\prime\prime}_{t}=\sum_{i=0}^{\infty}\gamma^{i}\nabla_{\theta}r _{\theta_{t}}(s^{\prime}_{i},a^{\prime}_{i})-\sum_{i=0}^{\infty}\gamma^{i} \nabla_{\theta}r_{\theta_{t}}(s^{E}_{i},a^{E}_{i})+\frac{\lambda(1-\gamma^{t+1 })}{1-\gamma}(\theta-\bar{\theta})\).

### MuJoCo

#### d.2.1 Walker

Figure 1(b) shows that MERIT can achieve similar performance with the expert after \(t=600\) while the other three in-trajectory learning baselines fail to imitate the expert before the ongoing trajectory terminates. Note that the naive methods (i.e., Naive MERIT-IRL and Naive IT-IRL) have much smaller improvement from \(t=0\) compared to MERIT-IRL and IT-IRL. The reason is that the naive reward update method is flawed. Intuitively, the reward update mechanism of these two baselines are myopic as explained in Subsection 4.1. Theoretically, the gradients \(g^{\prime\prime}_{t}\) of Naive MERIT-IRL and

of Naive IT-IRL are biased estimate of (4) even if \(\pi_{t}\) approaches \(\pi_{\theta_{t}}\) since (4) includes the trajectory suffix (\(i>t\)) terms while \(g_{t}^{\prime\prime}\) and \(g_{t}^{\prime\prime\prime}\) only include the trajectory prefix (\(i\leq t\)) terms.

MERIT-IRL performs much better than IT-IRL. The reason is that the meta-regularization term restricts the learned reward parameter within a certain neighborhood of the meta-prior \(\bar{\theta}\) (proved in Appendix B.3). Given that \(\bar{\theta}\) is trained over a family of relevant tasks, it is expected that the actual reward function parameter of our task shall be "close" to \(\bar{\theta}\)[25, 44, 57], i.e., inside this neighborhood. Therefore, MERIT-IRL can efficiently learn the expert's reward function. On the contrary, IT-IRL does not have the meta-prior \(\bar{\theta}\) as a guidance and thus has to search over the whole parameter space, which is extremely difficult to learn the expert's reward function when the data is lacking. Note that MERIT-IRL and Naive MERIT-IRL have better initial performance than IT-IRL and Naive IT-IRL since MERIT-IRL and Naive MERIT-IRL starts at the meta-prior \(\bar{\theta}\) while IT-IRL and Naive IT-IRL initializes randomly.

#### d.2.2 Hopper

Figure 1(c) shows that MERIT can achieve similar performance with the expert after \(t=500\) while the other three in-trajectory learning baselines fail to imitate the expert before the ongoing trajectory terminates.

### Stock Market

We use the real-world data of 30 constitute stocks in Dow Jones Industrial Average from 2021-01-01 to 2022-01-01. The 30 stocks are respectively: 'AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW'.

The state of the stock market MDP is the perception of the stock market, including the open/close price of each stock, the current asset, and some technical indices [48]. The action has the same dimension as the number of stocks where each dimension represents the amount of buying/selling the corresponding stock. The detailed formulation of the MDP can be found in FinRL [48, 58].

The turbulence index is a technical index of stock market and is included as a dimension of the state [48, 58]. The function \(p_{2}\) is defined as the amount of buying the stocks whose turbulence index is larger than the turbulence threshold. Therefore, the more the target investor buys the stocks whose turbulence index is larger than the turbulence threshold, the larger \(p_{2}\) will be and thus the smaller reward the target investor will receive.

**Discussion on the experiment results**. In Figure 1(d), MERIT-IRL can achieve the similar cumulative reward with the expert when only the first \(60\%\) of the trajectory is observed while IT-IRL can achieve performance close to the expert after \(t=220\). This shows that the meta-regularization can help imitate the expert faster. In contrast, Naive MERIT-IRL and Naive IT-IRL barely improves because the naive reward update method is flawed. Intuitively, the reward update mechanism of these two baselines are myopic as explained in Subsection 4.1. Theoretically, the gradients \(g_{t}^{\prime\prime}\) of Naive MERIT-IRL and \(g_{t}^{\prime\prime\prime}\) of Naive IT-IRL are biased estimate of (4) even if \(\pi_{t}\) approaches \(\pi_{\theta_{t}}\) since (4) includes the trajectory suffix (\(i>t\)) terms while \(g_{t}^{\prime\prime}\) and \(g_{t}^{\prime\prime\prime}\) only include the trajectory prefix (\(i\leq t\)) terms.

The last row in Table 1 shows the final results of the algorithms. We can see that MERIT-IRL achieves much better performance than the other in-trajectory learning baselines (i.e., IT-IRL, Naive MERIT-IRL, and Naive IT-IRL). MERIT-IRL achieves comparable performance with Hindsight and the expert. Note that it is not expected that MERIT-IRL outperforms Hindsight since Hindsight has the complete expert trajectory to learn.

## Appendix E Potential negative societal impact

Since MERIT-IRL can infer the reward function of the expert, potential negative societal impact may occur when the learner is malicious. Take the stock market experiment as an example, private information like preferences or habits of the investors may be leaked by using MERIT-IRL. To avoid this situation, the investors needs to take additional strategies such as protecting its investment data from unsecure resources.

Limitations

From the objective (2), we can see that the goal of MERIT-IRL is to align with the expert demonstration, i.e., finding a reward function such that its corresponding policy makes the expert trajectory most likely. An ideal case is that we can also directly quantify the reward learning performance and study the reward identifiability issue. Thus, a future work is to study the reward identifiability issue in the context of in-trajectory IRL.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract summarizes our contributions, and the introduction has a "contribution statement" part which elaborates our contributions and mentions the consistency with theoretical and experiment results. The assumptions and limitations are included in "theoretical analysis" (Subsection 4.2) and Appendix F. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitation is mentioned in Appendix F. Under the assumptions, i.e., Assumptions 1 and 2, we have a justification of either the assumption is widely used in literature or the assumption can be satisfied by real scenarios. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We clearly state our assumptions in Assumptions 1 and 2. The theoretical statements are also clearly stated in Subsection 4.2 and the correct proof is included in Appendix B. All the assumptions and theoretical statements are numbered and cross-referenced. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This paper provides pseudocode and elaborates each step of the algorithm in Subsection 4.1 and Appendix 4.3. We also include the experiment details in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is submitted in the supplementary materials and we include a document in the code folder to describe how to run the code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Appendix D includes the experiment details, and we also submit the code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In the experiment results (i.e., Table 1, Table 1, Table 1, Table 1), we include both the mean and standard deviation. In Figure 2, we also plot the mean and standard deviation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We include the type of operating system, CPU, and RAM used in the first paragraph in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper follows NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[Yes] Justification: We discuss the potential negative societal impact in Appendix E. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks because we do not have data nor model to release. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use the SAC code from a Github repository, and we clearly state it at the top of that code script. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The submitted code can be considered as an asset, and we provide a file along with the code to document the code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.