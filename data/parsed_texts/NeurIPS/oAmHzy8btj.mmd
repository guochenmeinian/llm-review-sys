# Exact Random Graph Matching with Multiple Graphs

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This work studies fundamental limits for recovering the underlying correspondence among _multiple_ correlated random graphs. We identify a necessary condition for any algorithm to correctly match all nodes across all graphs, and propose two algorithms for which the same condition is also sufficient. The first algorithm employs global information to simultaneously match all the graphs, whereas the second algorithm first partially matches the graphs pairwise and then combines the partial matchings by transitivity. Both algorithms work down to the information theoretic threshold. Our analysis reveals a scenario where exact matching between two graphs alone is impossible, but leveraging more than two graphs allows exact matching among all the graphs. Along the way, we derive independent results about the \(k\)-core of Erdos-Renyi graphs.

## 1 Introduction

The information age has ushered an abundance of correlated networked data. For instance, the network structure of two social networks such as Facebook and Twitter is correlated because users are likely to connect with the same individuals in both networks. This wealth of correlated data presents both opportunities and challenges. On one hand, information from various datasets can be combined to increase the fidelity of data - translating to better performance in downstream learning tasks. On the other hand, the interconnected nature of this data also raises privacy and security concerns. Linkage attacks, for instance, exploit correlated data to identify individuals in an anonymized network by linking to other sources [20]. This poses a significant threat to user privacy.

Graph matching is the problem of recovering the underlying latent correspondence between correlated networks. The problem finds many applications in machine learning: de-anonymizing social networks [20, 21], identifying similar functional components between species by matching their protein-protein interaction networks [1, 17], object detection [22] and tracking [23] in computer vision, and textual inference for natural language processing [14]. In most applications of interest, data is available in the form of _several_ correlated networks. For instance, social media users are active each month on 6.7 social platforms on average [18]. Similarly, reconciling protein-protein interaction networks among _multiple_ species is an important problem in computational biology [24]. As a first step toward this objective, many research works have studied the problem of matching _two_ correlated graphs.

### Related Work

The theoretical study of graph matching algorithms and their performance guarantees has primarily focused on Erdos-Renyi (ER) graphs. Pedarsani and Grossglauser [16] introduced the subsampling model to generate two such correlated graphs. The model entails twice subsampling each edge independently from a parent ER graph to obtain two sibling graphs, both of which are marginally ER graphs themselves. The goal is then to match nodes between the two graphs to recover theunderlying latent correspondence. This has been the framework of choice for many works that study graph matching. For example, Cullina and Kiyavash studied the problem of _exactly matching_ two ER graphs, where the objective is to match _all_ vertices correctly [16, 17]. They identified a threshold phenomenon for this task: exact recovery is possible if the problem parameters are above a threshold, and impossible otherwise. Subsequently, threshold phenomena were also identified for _partial_ graph matching between ER graphs - where the objective is to match only a positive fraction of nodes [1, 18, 20, 21]. The case of almost-exact recovery - where the objective is to match all but a negligible fraction of nodes - was studied by Cullina and co-authors: a necessary condition for almost exact recovery was identified, and it was shown that the same condition is also sufficient for the _\(k\)-core estimator_[17]; the estimator is described formally in Section 3. This estimator proved useful to uncover the fundamental limits for graph matching in other contexts such as the stochastic block model [14] and inhomogeneous random graphs [20]. Ameen and Hajek [1] showed some robustness properties of the \(k\)-core estimator in the context of matching ER graphs under node corruption. The estimator plays an important role in the present work as well.

A sound understanding of ER graphs inspires algorithms for real-world networks. Various _efficient_ algorithms have been proposed, including algorithms based on the spectrum of the graph adjacency matrices [13], node degree and neighborhood based algorithms [1, 12] as well as algorithms based on iterative methods [14] and counting subgraphs [15, 16]. Some of these are discussed in Section 5 in relation to the present work.

Incorporating information from multiple graphs to match them has been recognized as an important research direction, for instance in the work of Gaudio and co-authors [14]. To our knowledge, the only other papers to consider matchings among multiple graphs are the works of Josephs and co-authors [10], and of Racz and Sridhar [20]. However, these works have different objectives and are not concerned with the fundamental limits for matching \(m\) graphs. In fact, both works note that it is possible to exactly match \(m\) graphs whenever it is possible to exactly match any two graphs by pairwise matching all the graphs exactly. In contrast, we show that under appropriate conditions, it is possible to exactly match \(m\) ER graphs even when no two graphs can be pairwise matched exactly.

ContributionsIn this work, we investigate the problem of combining information from _multiple_ correlated networks to boost the number of nodes that are correctly matched among them. We consider the natural generalization of the subsampling model to generate \(m\) correlated random graphs, and identify a threshold such that it is impossible for any algorithm to match all nodes correctly across all graphs when the problem parameters are below this threshold. Conversely, we show that exact recovery is possible above the threshold. This characterization generalizes known results for exact graph matching when \(m=2\). Subsequently, we show that there is a region in parameter space for which exactly matching any two graphs is impossible using only the two graphs, and yet exact graph matching is possible among \(m>2\) graphs using all the graphs.

We present two algorithms and prove their optimality for this task. The first algorithm matches all \(m\) graphs simultaneously based on global information about the graphs. In contrast, the second algorithm first _pairwise_ matches graphs, and then combines them to match all nodes across all graphs. We show that both algorithms correctly match all the graphs all the way down to the information theoretic threshold. Finally, we illustrate through simulation that our subroutine to combine information from pairwise comparisons between networks works well when paired with efficient algorithms for graph matching. Our analysis also yields some theoretical results about the \(k\)-core of ER graphs that are of independent interest.

## 2 Preliminaries and Setup

NotationIn this work, \(G\sim\mathsf{ER}(n,p)\) denotes that the graph \(G\) is sampled from the Erdos-Renyi distribution with parameters \(n\) and \(p\), i.e. \(G\) has \(n\) nodes and each edge is independently present with probability \(p\). For a graph \(G\), we denote the set of its vertices by \(V\equiv V(G)\) and its edges by \(E(G)\). The _edge status_ of each vertex pair \(\{i,j\}\) with \(i\neq j\) is denoted by \(G\{i.j\}\), so that \(G\{i,j\}=1\) if \(\{i,j\}\in E(G)\) and \(G\{i,j\}=0\) otherwise. The degree of a node \(v\) in graph \(G\) is denoted \(\delta_{G}(v)\). Let \(\pi\) denote a permutation on \(V(G)=\{1,\cdots,n\}\). For a graph \(G\), denote by \(G^{\pi}\) the graph obtained by permuting the nodes of \(G\) according to \(\pi\), so that

\[G\{i,j\}=G^{\pi}\left\{\pi(i),\pi(j)\right\}\ \forall\ i,j\in V(G)\text{ such that }i\neq j.\]

Standard asymptotic notation \((O(\cdot),o(\cdot),\cdots)\) is used throughout and it is implicit that \(n\to\infty\).

Subsampling modelConsider the subsampling model for correlated random graphs [10], which has a natural generalization to the setting of \(m\) graphs. In this model, a parent graph \(G\) is sampled from the Erdos-Renyi distribution \(\mathsf{ER}(n,p)\). The \(m\) graphs \(G_{1},G_{2}^{\prime},\cdots,G_{m-1}^{\prime},G_{m}^{\prime}\) are obtained by independently subsampling each edge from \(G\) with probability \(s\). Finally, the graphs \(G_{2},\cdots,G_{m}\) are obtained by permuting the nodes of each of the graphs \(G_{2}^{\prime},\cdots,G_{m}^{\prime}\) respectively according to independent permutations \(\pi_{12}^{*},\cdots,\pi_{1m}^{*}\) sampled uniformly at random from the set of all permutations on \([n]\), i.e.

\[G_{j}=(G_{j}^{\prime})^{\pi_{ij}^{*}}\text{ for all }\ j\in\{2,\cdots,m\}.\]

Figure 1 illustrates this process of obtaining correlated graphs using the subsampling model. In this work, we are interested in the setting where \(s\) is constant and \(p=C\log(n)/n\) for some \(C>0\).

**Objective 1**.: _Determine conditions on parameters \(C\), \(s\) and \(m\) so that given correlated graphs \(G_{1},\cdots,G_{m}\) from the subsampling model, it is possible to exactly recover the underlying correspondences \(\pi_{12}^{*},\cdots,\pi_{1m}^{*}\) with probability \(1-o(1)\)._

Stated thus, the underlying correspondences use the graph \(G_{1}\) as a reference. Thus, for ease of notation, we will use \(G_{1}\) and \(G_{1}^{\prime}\) interchangeably. Note that the underlying correspondence between all the graphs is fixed upon fixing \(\pi_{12}^{*},\cdots,\pi_{1m}^{*}\): for any two graphs \(G_{i}\) and \(G_{j}\), their underlying correspondence is given by \(\pi_{ij}^{*}:=\pi_{1j}^{*}\circ(\pi_{1i}^{-})^{-1}\).

Formally, a _matching_\((\mu_{12},\cdots,\mu_{1m})\) is a collection of injective functions with domain \(\mathsf{dom}(\mu_{1i})\subseteq V\) for each \(i\), and co-domain \(V\). An _estimator_ is simply a mechanism to map any collection of graphs \((G_{1},\cdots,G_{m})\) to a matching. We say that an estimator _completely_ matches the graphs if the output mappings \(\mu_{12},\cdots\mu_{1m}\) are all complete, i.e. they are all permutations on \(\{1,\cdots,n\}\).

## 3 Main Results and Algorithm

This section presents necessary and sufficient conditions to meet Objective 1.

**Theorem 2** (Impossibility).: _Let \(G_{1},\cdots,G_{m}\) be correlated graphs obtained from the subsampling model with parameters \(C\) and \(s\), and let \(\pi_{12}^{*},\cdots,\pi_{1m}^{*}\) denote the underlying latent correspondences between \(G_{1}\) and \(G_{2},\cdots,G_{m}\) respectively. Suppose that_

\[Cs\left(1-(1-s)^{m-1}\right)<1.\]

_The output \(\widehat{\pi}_{12},\cdots,\widehat{\pi}_{1m}\) of any estimator satisfies_

\[\mathbb{P}\left(\widehat{\pi}_{12}=\pi_{12}^{*},\,\widehat{\pi}_{13}=\pi_{13} ^{*},\cdots,\,\widehat{\pi}_{1m}=\pi_{1m}^{*}\right)=o(1).\]

Figure 1: Illustration of obtaining \(m\) correlated graphs from the subsampling model

Theorem 2 implies that the condition \(Cs(1-(1-s)^{m-1}>1\) is a necessary condition to exactly match \(m\) graphs with probability bounded away from \(0\). We show that this condition is also sufficient to exactly match \(m\) graphs with probability going to \(1\).

**Theorem 3** (Achievability).: _Let \(G_{1},\cdots,G_{m}\) be correlated graphs obtained from the subsampling model with parameters \(C\) and \(s\), and let \(\pi_{12}^{*},\cdots,\pi_{1m}^{*}\) denote the underlying latent correspondences between \(G_{1}\) and \(G_{2},\cdots,G_{m}\) respectively. Suppose that_

\[Cs\left(1-(1-s)^{m-1}\right)>1.\]

_There is an estimator whose output \(\widehat{\pi}_{12},\cdots,\widehat{\pi}_{1m}\) satisfies_

\[\mathbb{P}\left(\widehat{\pi}_{12}=\pi_{12}^{*},\;\widehat{\pi}_{13}=\pi_{13}^ {*},\cdots,\;\widehat{\pi}_{1m}=\pi_{1m}^{*}\right)=1-o(1).\]

Theorems 2 and 3 together characterize the threshold for exact recovery. A few remarks are in order.

1. For \(m=2\), the condition \(Cs(1-(1-s)^{m-1})>1\) reduces to \(Cs^{2}>1\), which is known to be necessary and sufficient for exactly matching two graphs [1, 2].
2. For any \(m>2\), there is a non-empty region in the parameter space defined by \[Cs(1-(1-s)^{m-1})>1>Cs^{2}.\] For any \(C\) and \(s\) in this region, it is impossible to exactly match any two graphs \(G_{i}\) and \(G_{j}\) without using the other \(m-2\) graphs as side information. Upon using them, however, it is possible to exactly match all nodes across the \(m\) graphs. This is illustrated in Figure 2.

### Algorithms for exact recovery

For any two graphs \(H_{1}\) and \(H_{2}\) on the same vertex set \(V\), denote by \(H_{1}\lor H_{2}\) their _union graph_ and by \(H_{1}\wedge H_{2}\) their _intersection graph_. An edge \(\{i,j\}\) is present in \(H_{1}\lor H_{2}\) if it is present in either \(H_{1}\) or \(H_{2}\). Similarly, the edge is present in \(H_{1}\wedge H_{2}\) if it is present in both \(H_{1}\) and \(H_{2}\).

A natural starting point is to study the maximum likelihood estimator (MLE) because it is optimal. To that end, we compute the log-likelihood function; the details are deferred to Appendix A.

**Theorem 4**.: _Let \(\pi_{12},\cdots,\pi_{1m}\) denote a collection of permutations on \(\{1,\cdots,n\}\). Then_

\[\log\mathbb{P}\left(G_{1},\cdots,G_{m}\mid\pi_{12}^{*}=\pi_{12},\cdots,\pi_{1 m}^{*}=\pi_{1m}\right)\propto\text{const.}-\left|E\left(G_{1}\lor G_{2}^{ \pi_{12}}\vee\cdots\lor G_{m}^{\pi_{1m}}\right)\right|,\]

_where const. depends only on \(p,s\) and \(G_{1},\cdots,G_{m}\)._

Theorem 4 reveals that the MLE for exactly matching \(m\) graphs has a neat interpretation: simply pick \(\pi_{12},\cdots,\pi_{1m}\) to minimize the number of edges in the corresponding union graph. This is presented as Algorithm 1. Despite this nice interpretation of the MLE, its analysis is quite cumbersome. We instead present and analyze a different estimator, presented as Algorithm 2.

Figure 2: Regions in parameter space. _Orange_: Exactly matching \(m\) graphs is impossible even with \(m\) graphs. _Blue_: Exactly matching \(2\) graphs is possible with \(2\) graphs. _Striped_: Impossible to match \(2\) graphs using only the \(2\) graphs, but possible using \(m\) graphs as side information.

Algorithm 2 runs in two steps: In step 1, the \(k\)-core estimator, for a suitable choice of \(k\), is used to pairwise match all the graphs. For any \(i\) and \(j\), the \(k\)-core estimator selects a permutation \(\widehat{\nu}_{ij}\) to maximize the size of the \(k\)-core1 of \(G_{i}\wedge G_{j}^{\widehat{\nu}_{ij}}\). It then outputs a matching \(\widehat{\mu}_{ij}\) by restricting the domain of \(\widehat{\nu}_{ij}\) to \(\mathsf{core}_{k}(G_{i}\wedge G_{j}^{\widehat{\nu}_{ij}})\). These matchings \(\widehat{\mu}_{ij}\) need not be complete - in fact, each of them is a partial matching with high probability whenever \(Cs^{2}<1\). In step 2, these partial matchings are _boosted_ as follows: If a node \(v\) is unmatched between two graphs \(G_{i}\) and \(G_{j}\), then search for a sequence of graphs \(G_{i},G_{k_{1}},\cdots,G_{k_{\ell}},G_{j}\) such that \(v\) is matched between any two consecutive graphs in the sequence. If such a sequence exists, then extend \(\widehat{\mu}_{i,j}\) to include \(v\) by transitively matching it from \(G_{i}\) to \(G_{j}\).

Footnote 1: The \(k\)-core of a graph \(G\) is the largest subset of vertices \(\mathsf{core}_{k}(G)\) such that the induced subgraph has minimum degree at least \(k\).

In Section 4.2, we show that Algorithm 2 correctly matches all nodes across all graphs with probability \(1-o(1)\), whenever the necessary condition \(Cs(1-(1-s)^{m-1})>1\) holds. We remark that this also implies that Algorithm 1 succeeds under the same condition, because the MLE is optimal. Note that the MLE selects all permutations \(\widehat{\pi}_{12},\cdots,\widehat{\pi}_{1m}\)_simultaneously_ based on their union graph. In contrast, Algorithm 2 only ever makes _pairwise_ comparisons between graphs. Perhaps surprisingly, it turns out that this is sufficient for exact recovery. An analysis of Algorithm 2 is presented in Section 4. Along the way, independent results of interest on the \(k\)-core of Erdos-Renyi graphs are obtained.

Proof Outlines and Key Insights

### Impossibility of exact graph matching (Theorem 2)

This result has a simple proof following a genie-aided converse argument. The idea is to reduce the problem to that of matching two graphs by providing extra information to the estimator.

Proof of Theorem 2.: If the correspondences \(\pi_{12}^{*},\cdots,\pi_{1,m-1}^{*}\) were provided as extra information to an estimator, then the estimator must still match \(G_{m}\) with the union graph \(G_{1}^{\prime}\lor G_{2}^{\prime}\vee\cdots\lor G_{m-1}^{\prime}\). This can be viewed as an instance of matching two graphs obtained by _asymmetric_ subsampling: the graph \(G_{m}\) is obtained from a parent graph \(G\sim\text{ER}(n,C\log(n)/n)\) by subsampling each edge independently with probability \(s_{1}:=s\), and the graph \(\widetilde{G}_{m-1}:=G_{1}^{\prime}\lor G_{2}^{\prime}\vee\cdots\lor G_{m-1}^ {\prime}\) is obtained from \(G\) by subsampling each edge independently with probability \(s_{2}:=1-(1-s)^{m-1}\). Cullina and Kiyavash studied this model for matching two graphs: Theorem 2 of [1] establishes that matching \(G_{m}\) and \(\widetilde{G}_{m-1}\) is impossible if \(Cs_{1}s_{2}<1\), or equivalently if \(Cs(1-(1-s)^{m-1})<1\). 

### Achievability of exact graph matching (Theorem 3)

Algorithm 2 succeeds if both step 1 and step 2 succeed, i.e.

1. Each instance of pairwise matching using the \(k\)-core estimator is correct on its domain, i.e. \[\widehat{\mu}_{ij}(v)=\pi_{ij}^{*}(v)\ \forall v\in\mathsf{dom}(\widehat{\mu}_{ ij}),\ \forall i,j.\]
2. For each node \(v\) and any two graphs \(G_{i}\) and \(G_{j}\), there is a sequence of graphs such that \(v\) can be transitively matched through those graphs between \(G_{i}\) and \(G_{j}\).

On step 1This falls back to the regime of analyzing the performance of the \(k\)-core estimator in the setting of two graphs. Cullina and co-authors [1] showed that the \(k\)-core estimator is _precise_: For any two correlated graphs \(G_{i}\) and \(G_{j}\) with \(p=C\log(n)/n\) and constant \(s\), the \(k\)-core estimator correctly matches all nodes in \(\mathsf{core}_{k}(G_{i}^{\prime}\wedge G_{j}^{\prime})\) with probability \(1-o(1)\). In fact, this is true for any \(C>0\) and for any \(k\geq 13\)[13]. Therefore, using the fact that the number of instances of pairwise matchings is constant whenever \(m\) is constant, a union bound reveals

\[\mathbb{P}(\exists\ 1\leq i<j\leq m \text{ such that }\widehat{\mu}_{ij}(v)\neq\pi_{ij}^{*}(v)\text{ for some }v\in\mathsf{core}_{k}(G_{i}^{\prime}\wedge G_{j}^{\prime}))\] \[\leq\sum_{i=1}^{m}\sum_{j=1}^{m}\mathbb{P}\left(\widehat{\mu}_{i, j}(v)\neq\pi_{i,j}^{*}(v)\text{ for some }v\in\mathsf{core}_{k}(G_{i}^{\prime}\wedge G_{j}^{\prime})\right)\] \[=o(1).\]

We have proved the following.

**Proposition 5**.: _Let \(G_{1},\cdots,G_{m}\) be correlated graphs from the subsampling model. Let \(k\geq 13\) and let \(\widehat{\mu}_{ij}\) denote the matching output by the \(k\)-core estimator on graphs \(G_{i}\) and \(G_{j}\). Then,_

\[\mathbb{P}(\exists\ 1\leq i<j\leq m,\text{ and }v\in\mathsf{core}_{k}(G_{i}^{ \prime}\wedge G_{j}^{\prime}))\text{ such that }\widehat{\mu}_{ij}(v)\neq\pi_{ij}^{*}(v))=o(1).\]

On step 2The challenging part of the proof is to show that boosting through transitive closure matches all the nodes with probability \(1-o(1)\) if \(Cs(1-(1-s)^{m-1})>1\). It is instructive to visualize this using _transitivity graphs_.

**Definition 6** (Transitivity graph, \(\mathcal{H}(v)\)).: _For each node \(v\in V\), let \(\mathcal{H}(v)\) denote the graph on the vertex set \(\{g_{1},\cdots,g_{m}\}\) such that an edge \(\{g_{i},g_{j}\}\) is present in \(\mathcal{H}(v)\) if and only if \(v\in\mathsf{core}_{k}(G_{i}^{\prime}\wedge G_{j}^{\prime})\)._

On the event that each instance of pairwise matching using the \(k\)-core is correct, the edge \(\{g_{i},g_{j}\}\) is present in \(\mathcal{H}(v)\) if and only if \(v\) is correctly matched using the \(k\)-core estimator between \(G_{i}\) and \(G_{j}\), i.e. \(\pi_{1i}^{*}(v)\) is matched to \(\pi_{1j}^{*}(v)\). Thus, in order for Step 2 to succeed (i.e. to exactly match all vertices across all graphs), it suffices that the graph \(\mathcal{H}(v)\) is connected for each node \(v\in V\). However, studying the connectivity of the transitivity graphs is challenging because in any graph \(\mathcal{H}(v)\), no two edges are independent. This is because the \(k\)-cores of any two intersection graphs \(G_{a}^{\prime}\wedge G_{b}^{\prime}\) and \(G_{c}^{\prime}\wedge G_{d}^{\prime}\) are correlated, because all the graphs \(G_{a},G_{b},G_{c}\) and \(G_{d}\) are themselves correlated. To overcome this, we introduce another graph \(\widetilde{\mathcal{H}}(v)\) that relates to \(\mathcal{H}(v)\) and is amenable to analysis.

**Definition 7**.: _For each node \(v\in V\), let \(\widetilde{\mathcal{H}}(v)\) denote a complete weighted graph on the vertex set \(\{g_{1},\cdots,g_{m}\}\) such that the weight on any edge \(\{g_{i},g_{j}\}\) is \(\widetilde{c}_{v}\left(i,j\right):=\delta_{G^{\prime}_{i}\wedge G^{\prime}_{j} }(v)\)._

The relationship between the graphs \(\mathcal{H}(v)\) and \(\widetilde{\mathcal{H}}(v)\) stems from a useful relationship between the degree of node \(v\) in \(G^{\prime}_{i}\wedge G^{\prime}_{j}\) and the inclusion of \(v\) in \(\mathsf{core}_{k}(G^{\prime}_{i}\wedge G^{\prime}_{j})\) for each \(i\) and \(j\). Since this result is of independent interest in the study of random graphs, we state it below for general Erdos-Renyi graphs.

**Lemma 8**.: _Let \(n\) and \(k\) be positive integers and let \(G\sim\mathsf{ER}(n,\alpha\log(n)/n)\) for some \(\alpha>0\). Let \(v\) be a node of \(G\) and let \(\delta_{G}(v)\) denote the degree of \(v\) in \(G\). Then,_

\[\mathbb{P}\left(\{v\notin\mathsf{core}_{k}(G)\}\cap\{\delta_{G}(v)\geq k+1/ \alpha\}\right)=o\left(1/n\right).\] (1)

For any \(i\) and \(j\), the graph \(G^{\prime}_{i}\wedge G^{\prime}_{j}\sim\mathsf{ER}(n,Cs^{2}\log(n)/n)\). Thus, Lemma 8 implies that with probability \(1-o(1/n)\), if a pair \(\{g_{i},g_{j}\}\) has edge weight \(\widetilde{c}_{ij}\geq k+1/\alpha\) in \(\widetilde{\mathcal{H}}(v)\), then the corresponding edge \(\{g_{i},g_{j}\}\) is present in the transitivity graph \(\mathcal{H}(v)\). Equivalently, \(v\) is correctly matched between \(G_{i}\) and \(G_{j}\) in the instance of pairwise \(k\)-core matching between them.

The graph \(\mathcal{H}(v)\) is not connected only if it contains a (non-empty) vertex cut \(U\subset\{1,\cdots,m\}\) with no edge crossing between \(U\) and \(U^{c}\). Let \(c_{v}(U)\) denote the number of such crossing edges in \(\mathcal{H}(v)\). Furthermore, define the _cost_ of the cut \(U\) in \(\widetilde{\mathcal{H}}(v)\) as

\[\widetilde{c}_{v}(U):=\sum_{i\in U}\sum_{j\in U^{c}}\widetilde{c}_{v}\left(i, j\right).\]

Lemma 8 is a statement about a single graph, but we show it can be invoked to prove the following.

**Theorem 9**.: _Let \(G_{1},\cdots,G_{m}\) be correlated graphs from the subsampling model with parameters \(C\) and \(s\). Let \(v\in V\) and let \(U\) be a vertex cut of \(\{1,\cdots,m\}\) such that \(|U|\leq\lfloor m/2\rfloor\). Then,_

\[\mathbb{P}\left(\{c_{v}(U)=0\}\cap\left\{\widetilde{c}_{v}(U)>\frac{m^{2}}{4} \left(k+\frac{1}{Cs^{2}}\right)\right\}\right)=o(1/n).\] (2)

It suffices therefore to analyze the probability that the graph \(\widetilde{\mathcal{H}}(v)\) has a cut \(U\) such that its cost \(\widetilde{c}_{v}(U)\) is too small. To that end, we show that the bottleneck arises from vertex cuts of small size. Formally,

**Theorem 10**.: _Let \(G_{1},\cdots,G_{m}\) be correlated graphs from the subsampling model. Let \(v\in V\) and let \(U_{\ell}\) denote the set \(\{1,\cdots,\ell\}\) for \(\ell\) in \(\{1,\cdots,\lfloor m/2\rfloor\}\). For any vertex cut \(U\) of \(\{1,\cdots,m\}\), let \(\widetilde{c}_{v}(U)\) denote its cost in the graph \(\widetilde{\mathcal{H}}(v)\). The following stochastic ordering holds:_

\[\widetilde{c}_{v}(U_{1})\preceq\widetilde{c}_{v}(U_{2})\preceq\cdots\preceq \widetilde{c}_{v}(U_{\lfloor m/2\rfloor}).\]

Theorems 9 and 10 imply that the tightest bottleneck to the connectivity of \(\mathcal{H}(v)\) is the event that \(\widetilde{c}_{v}(U_{1})\) is below the threshold \(r:=\frac{m^{2}}{4}\left(k+\frac{1}{Cs^{2}}\right)\), i.e. the sum of degrees of \(v\) over the intersection graphs \((G_{1}\wedge G^{\prime}_{j}:j=2,\cdots,m)\) is less than \(r\). This event occurs only if the degree of \(v\) is less than \(r\) in each of the intersection graphs \((G_{1}\wedge G^{\prime}_{j}:j=2,\cdots,m)\). However, under the condition \(Cs(1-(1-s)^{m-1})>1\), it turns out that this event occurs with probability \(o(1/n)\).

**Theorem 11**.: _Let \(G_{1},\cdots,G_{m}\) be obtained from the subsampling model with parameters \(C\) and \(s\). Let \(r=\frac{m^{2}}{4}\left(k+\frac{1}{Cs^{2}}\right)\). Let \(v\in[n]\) and suppose that \(Cs(1-(1-s)^{m-1})>1\). Then,_

\[\mathbb{P}\left(\widetilde{c}_{v}(U_{1})\leq r\right)\leq\mathbb{P}\left(\left\{ \delta_{G_{1}\wedge G^{\prime}_{2}}(v)\leq r\right\}\cap\cdots\cap\left\{ \delta_{G_{1}\wedge G^{\prime}_{m}}(v)\leq r\right\}\right)=o\left(1/n\right).\]

### Piecing it all together: Proof of Theorem 3

Proof of Theorem 3.: Let \(\widehat{\pi}_{12},\cdots,\widehat{\pi}_{1m}\) denote the output of Algorithm 2 with \(k\geq 13\). Let \(E_{1}\) (resp. \(E_{2}\)) denote the event that Algorithm 1 (resp. Algorithm 2) fails to match all \(m\) graphs exactly, i.e.

\[E_{1}=\left\{\widehat{\pi}_{12}^{\text{ML}}\neq\pi_{12}^{*}\right\}\cup\cdots \cup\left\{\widehat{\pi}_{1m}^{\text{ML}}\neq\pi_{1m}^{*}\right\},\qquad E_{2}= \left\{\widehat{\pi}_{12}\ \neq\pi_{12}^{*}\right\}\cup\cdots\cup\left\{\widehat{\pi}_{1m}\neq\pi_{1m}^{ *}\right\}.\]First, we show that the output of Algorithm 2 is correct with probability \(1-o(1)\) whenever \(Cs(1-(1-s)^{m-1})>1\). If the event \(E_{2}\) occurs, then either step 1 failed, i.e. there is a \(k\)-core matching \(\widehat{\mu}_{ij}\) that is incorrect, or step 2 failed, i.e. at least one of the graphs \(\mathcal{H}(v)\) is not connected. Therefore,

\[\mathbb{P}\left(E_{2}\right)\leq\mathbb{P}\left(\bigcup_{i,j}\bigcup_{v\in \mathbf{core}_{k}(G_{i}^{\prime}\wedge G_{j}^{\prime})}\bigl{\{}\widehat{\mu} _{ij}\neq\pi_{ij}^{\ast}\bigr{\}}\right)+\mathbb{P}\left(\bigcup_{v\in V} \left\{\mathcal{H}(v)\text{ is not connected}\right\}\right)\leq o(1)+\sum_{v\in V}q _{v},\]

where the last step uses Proposition 5, and \(q_{v}\) denotes the probability that the transitivity graph \(\mathcal{H}(v)\) is not connected. For each \(\ell\) in the set \(\{1,\cdots,\lfloor m/2\rfloor\}\), let \(U_{\ell}\) denote the set \(\{1,\cdots,\ell\}\). Then,

\[q_{v} =\mathbb{P}\left(\bigcup_{\ell=1}^{\lfloor m/2\rfloor}\left\{ \exists\;U\subset\{1,\cdots,m\}:|U|=\ell\text{ and }c_{v}(U)=0\right\}\right)\] \[\leq\sum_{\ell=1}^{\lfloor m/2\rfloor}\binom{m}{\ell}\cdot \mathbb{P}\left(c_{v}(U_{\ell})=0\right)\] \[\leq\sum_{\ell=1}^{\lfloor m/2\rfloor}\binom{m}{\ell}\left[ \mathbb{P}\left(\widetilde{c}_{v}(U_{\ell})\leq\frac{m^{2}}{4}\biggl{(}k\!+ \!\frac{1}{Cs^{2}}\biggr{)}\!\right)\!+\!\mathbb{P}\left(\{c_{v}(U_{\ell})=0 \}\cap\left\{\widetilde{c}_{v}(U_{\ell})>\frac{m^{2}}{4}\biggl{(}k\!+\!\frac{1 }{Cs^{2}}\biggr{)}\right\}\right)\right)\!\] \[\overset{\text{(a)}}{\leq}\sum_{\ell=1}^{\lfloor m/2\rfloor} \binom{m}{\ell}\left[\mathbb{P}\left(\widetilde{c}_{v}(U_{\ell})\leq\frac{m^{2 }}{4}\biggl{(}k\!+\!\frac{1}{Cs^{2}}\biggr{)}\!\right)\!+\!o\left(\frac{1}{n} \right)\right]\] \[\overset{\text{(b)}}{\leq}\sum_{\ell=1}^{\lfloor m/2\rfloor} \binom{m}{\ell}\left[\mathbb{P}\left(\widetilde{c}_{v}(U_{1})\leq\frac{m^{2}}{ 4}\biggl{(}k\!+\!\frac{1}{Cs^{2}}\biggr{)}\!\right)\!+\!o\left(\frac{1}{n} \right)\right]\] \[\overset{\text{(c)}}{\leq}\sum_{\ell=1}^{\lfloor m/2\rfloor}m^{ \ell}\left[o\left(\frac{1}{n}\right)+o\left(\frac{1}{n}\right)\right]=o\left( \frac{1}{n}\right).\]

Here, (a) uses Theorem 9, and (b) uses the fact that for any \(\ell\geq 2\), the random variable \(\widetilde{c}_{v}(U_{\ell})\) stochastically dominates \(\widetilde{c}_{v}(U_{1})\) (Theorem 10). Finally, (c) uses Theorem 11 and the fact that \(Cs(1-(1-s)^{m-1})>1\). Therefore, a union bound over all the nodes yields

\[\mathbb{P}\left(E_{2}\right)\leq o(1)+\sum_{v\in V}q_{v}\leq o(1)+n\times o(1/ n)=o(1).\]

Finally, by optimality of the MLE, it follows that

\[\mathbb{P}\left(E_{1}\right)\leq\mathbb{P}\left(E_{2}\right)=o(1),\]

whenever \(Cs(1-(1-s)^{m-1})>1\). This concludes the proof. 

## 5 Discussion and Future Work

In this work, we introduced and analyzed matching through transitive closure - an approach that combines information from multiple graphs to recover the underlying correspondence between them. Despite its simplicity, it turns out that matching through transitive closure is an optimal way to combine information in the setting where the graphs are pairwise matched using the \(k\)-core estimator. A limitation of our algorithms is the runtime: Algorithm 2 does not run in polynomial time because it uses the \(k\)-core estimator for pairwise matching, which involves searching over the space of permutations. Even so, it is useful to establish the fundamental limits of exact recovery, and serve as a benchmark to compare the performance of any other algorithm.

The transitive closure subroutine (Step 2) itself is _efficient_ because it runs in polynomial time \(O(mn)\). Therefore, a natural next step is to modify Step 1 in our algorithm so that the pairwise matchings are done by an _efficient_ algorithm. However, it is not clear if transitive closure is optimal for combining information from the pairwise matchings in this setting. For example, there is a possibility that the pairwise matchings resulting from the efficient algorithm are heavily correlated, and transitive closure is unable to boost them. In Figure 3, we show experimentally that this is not the case for two algorithms of interest: GRAMPA [14] and Degree Profiles [13].

1. GRAMPA is a spectral algorithm that uses the entire spectrum of the adjacency matrices to match the two graphs. The code is available in [13].
2. Degree Profiles associates with each node a signature derived from the degrees of its neighbors, and matches nodes by signature proximity. The code is available in [12].

Evidently, both algorithms benefit substantially from using transitive closure to boost the number of matched nodes. This suggests that transitive closure can be a practical algorithm to boost matchings between networks by using other networks as side-information. Unfortunately, both GRAMPA and Degree Profiles require the graphs to be close to isomorphic in order to perform well, and so they do not perform well when the model parameters are close to the information theoretic threshold for exact recovery. Subsequently, they cannot be used to answer the question in Objective 1.

Our work presents several directions for future research.

* this precision is present in the \(k\)-core estimator and enables the transitive closure subroutine to work correctly. Can the performance guarantees of the \(k\)-core estimator be realized through polynomial time algorithms that meet this constraint?
* **Beyond Erdos-Renyi graphs.** The study of matching _two_ ER graphs provided tools and techniques that extended to the analysis of more realistic models. For instance, the \(k\)-core estimator itself played a crucial role in establishing limits to matching two correlated stochastic block models [10] and two inhomogeneous random graphs [11]. Can the techniques developed in the present work be used to identify the information theoretic limits to exact recovery in these models in the general setting of \(m\) graphs?
* **Boosting for partial recovery.** This work focused on _exact_ recovery, where the objective is to match _all_ nodes across _all_ graphs. It would be interesting to consider a regime where any instance of pairwise matching recovers at best a small fraction of nodes. Is it possible to quantify the extent to which transitive closure boosts the number of matched nodes?
* **Robustness.** Finally, how sensitive to perturbation is the transitive closure algorithm? Is it possible to quantify the extent to which an adversary may perturb edges in some of the graphs without losing the performance guarantees of the matching algorithm? Algorithms that perform well on models such as ER graphs and are further generally robust are expected to also work well with real-world networks.

Figure 3: Matching through transitive closure

## References

* [AH23] Taha Ameen and Bruce Hajek. Robust graph matching when nodes are corrupt. _arXiv preprint arXiv:2310.18543_, 2023.
* [BCL\({}^{+}\)19] Boaz Barak, Chi-Ning Chou, Zhixian Lei, Tselli Schramm, and Yueqi Sheng. (Nearly) efficient algorithms for the graph matching problem on correlated random graphs. _Advances in Neural Information Processing Systems_, 32, 2019.
* [BSI06] Sourav Bandyopadhyay, Roded Sharan, and Trey Ideker. Systematic identification of functional orthologs based on protein network comparison. _Genome research_, 16(3):428-435, 2006.
* [CK16] Daniel Cullina and Negar Kiyavash. Improved achievability and converse bounds for Erdos-Renyi graph matching. _ACM SIGMETRICS performance evaluation review_, 44(1):63-72, 2016.
* [CK17] Daniel Cullina and Negar Kiyavash. Exact alignment recovery for correlated Erdos-Renyi graphs. _arXiv preprint arXiv:1711.06783_, 2017.
* [CKMP19] Daniel Cullina, Negar Kiyavash, Prateek Mittal, and Vincent Poor. Partial recovery of Erdos-Renyi graph alignment via \(k\)-core alignment. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 3(3):1-21, 2019.
* [DCKG19] Osman Emre Dai, Daniel Cullina, Negar Kiyavash, and Matthias Grossglauser. Analysis of a canonical labeling algorithm for the alignment of correlated Erdos-Renyi graphs. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 3(2):1-25, 2019.
* [DD23] Jian Ding and Hang Du. Matching recovery threshold for correlated random graphs. _The Annals of Statistics_, 51(4):1718-1743, 2023.
* [DL23] Jian Ding and Zhangsong Li. A polynomial-time iterative algorithm for random graph matching with non-vanishing correlation. _arXiv preprint arXiv:2306.00266_, 2023.
* [DMWX20] Jian Ding, Zongming Ma, Yihong Wu, and Jiaming Xu. MATLAB code for degree profile in graph matching. _Available at: https://github.com/xjmofside/degree_profile_, 2020.
* [DMWX21] Jian Ding, Zongming Ma, Yihong Wu, and Jiaming Xu. Efficient random graph matching via degree profiles. _Probability Theory and Related Fields_, 179:29-115, 2021.
* [FMWX20] Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu. MATLAB code for GRAMPA. _Available at: https://github.com/xjmofside/grampa_, 2020.
* [FMWX22] Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu. Spectral graph matching and regularized quadratic relaxations II: Erdos-Renyi graphs and universality. _Foundations of Computational Mathematics_, pages 1-51, 2022.
* [GML21] Luca Ganassali, Laurent Massoulie, and Marc Lelarge. Impossibility of partial recovery in the graph alignment problem. In _Conference on Learning Theory_, pages 2080-2102. PMLR, 2021.
* [GRS22] Julia Gaudio, Miklos Z Racz, and Anirudh Sridhar. Exact community recovery in correlated stochastic block models. In _Conference on Learning Theory_, pages 2183-2241. PMLR, 2022.
* [HM23] Georgina Hall and Laurent Massoulie. Partial recovery in the graph alignment problem. _Operations Research_, 71(1):259-272, 2023.
* [HNM05] Aria Haghighi, Andrew Y Ng, and Christopher D Manning. Robust textual inference via graph matching. In _Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing_, pages 387-394, 2005.
* [Hoe94] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. _The collected works of Wassily Hoeffding_, pages 409-426, 1994.
* [Ind23] Global Web Index. Social behind the screens trends report. _GWI_, 2023.
* [JLK21] Nathaniel Josephs, Wenrui Li, and Eric. D. Kolaczyk. Network recovery from unlabeled noisy samples. In _2021 55th Asilomar Conference on Signals, Systems, and Computers_, pages 1268-1273, 2021.
* [KHGPM16] Ehsan Kazemi, Hamed Hassani, Matthias Grossglauser, and Hassan Pezeshgi Modarres. Proper: global protein interaction network alignment through percolation matching. _BMC bioinformatics_, 17(1):1-16, 2016.
* [Luc91] Tomasz Luczak. Size and connectivity of the \(k\)-core of a random graph. _Discrete Mathematics_, 91(1):61-68, 1991.
* [MRT23] Cheng Mao, Mark Rudelson, and Konstantin Tikhomirov. Exact matching of random graphs with constant correlation. _Probability Theory and Related Fields_, 186(1-2):327-389, 2023.
* [MU17] Michael Mitzenmacher and Eli Upfal. _Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis_. Cambridge University Press, 2017.

* [MWXY23] Cheng Mao, Yihong Wu, Jiaming Xu, and Sophie H Yu. Random graph matching at Otter's threshold via counting chandeliers. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 1345-1356, 2023.
* [NS08] Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. In _2008 IEEE Symposium on Security and Privacy (sp 2008)_, pages 111-125. IEEE, 2008.
* [NS09] Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In _2009 30th IEEE Symposium on Security and Privacy_, pages 173-187. IEEE, 2009.
* [PG11] Pedram Pedarani and Matthias Grossglauser. On the privacy of anonymized networks. In _Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1235-1243, 2011.
* [RS21] Miklos Z Racz and Anirudh Sridhar. Correlated stochastic block models: Exact graph matching with applications to recovering communities. _Advances in Neural Information Processing Systems_, 34:22259-22273, 2021.
* [RS23] Miklos Z Racz and Anirudh Sridhar. Matching correlated inhomogeneous random graphs using the \(k\)-core estimator. _arXiv preprint arXiv:2302.05407_, 2023.
* [SS05] Christian Schellewald and Christoph Schnorr. Probabilistic subgraph matching based on convex relaxation. In _International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition_, pages 171-186. Springer, 2005.
* [SXB08] Rohit Singh, Jinbo Xu, and Bonnie Berger. Global alignment of multiple protein interaction networks with application to functional orthology detection. _Proceedings of the National Academy of Sciences_, 105(35):12763-12768, 2008.
* [WXY22] Yihong Wu, Jiaming Xu, and Sophie H Yu. Settling the sharp reconstruction thresholds of random graph matching. _IEEE Transactions on Information Theory_, 68(8):5391-5417, 2022.
* [YYL\({}^{+}\)16] Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng, Hongyuan Zha, and Xiaokang Yang. A short survey of recent advances in graph matching. In _Proceedings of the 2016 ACM on international conference on multimedia retrieval_, pages 167-174, 2016.

[MISSING_PAGE_EMPTY:12]

Concentration Inequalities for Binomial Random Variables

The following bounds for the binomial distribution are used frequently in the analysis.

**Lemma 13**.: _Let \(X\sim\mathsf{Bin}(n,p)\). Then,_

1. _For any_ \(\delta>0\)_,_ \[\mathbb{P}\left(X\geq(1+\delta)np\right)\leq\left(\frac{e^{\delta}}{(1+\delta) ^{1+\delta}}\right)^{np}\leq\left(\frac{e}{1+\delta}\right)^{(1+\delta)np}.\] (5)
2. _For any_ \(\delta>5\)_,_ \[\mathbb{P}\left(X\geq(1+\delta)np\right)\leq 2^{-(1+\delta)np}.\] (6)
3. _For any_ \(\delta\in(0,1)\)_,_ \[\mathbb{P}\left(X\leq(1-\delta)np\right)\leq\left(\frac{e^{-\delta}}{(1-\delta )^{1-\delta}}\right)^{np}.\] (7)

Proof.: All proofs follow from the Chernoff bound and can be found, or easily derived, from Theorems 4.4 and 4.5 of [13]. 

## Appendix C Proof of Lemma 8

We restate Lemma 8 for convenience.

**Lemma 8**.: _Let \(n\) and \(k\) be positive integers and let \(G\sim\mathsf{ER}(n,\alpha\log(n)/n)\) for some \(\alpha>0\). Let \(v\) be a node of \(G\) and let \(\delta_{G}(v)\) denote the degree of \(v\) in \(G\). Then,_

\[\mathbb{P}\left(\left\{v\notin\mathsf{core}_{k}(G)\right\}\cap\left\{\delta_{ G}(v)\geq k+1/\alpha\right\}\right)=o\left(1/n\right).\] (1)

Before presenting the proof, we present the intuition behind it. The events \(\left\{v\notin\mathsf{core}_{k}(G)\right\}\) and \(\left\{\delta_{G}(v)\geq k+1/\alpha\right\}\) are highly negatively correlated. However, consider the subgraph \((G-v)\) of \(G\) induced on the vertex set \(V-\left\{v\right\}\), and note that the \(k\)-core of this subgraph does not depend on the degree of \(v\). Furthermore, if \(v\notin\mathsf{core}_{k}(G)\), then it must be that \(v\) has fewer than \(k\) neighbors in \(\mathsf{core}_{k}(G-v)\). Intuitively, this event has low probability if \(\mathsf{core}_{k}(G-v)\) is sufficiently large.

Notice that \((G-v)\sim\mathsf{ER}(n-1,\alpha\log(n)/n)\), and so standard results about the size of the \(k\)-core of Erdos-Renyi graphs apply. However, we require the error probability that the \(k\)-core of \(G-v\) is too small to be \(o(1/n)\) - this is crucial since we will later use a union bound over all the nodes \(v\). Unfortunately, standard results such as [11] can only be invoked directly to show that the corresponding probability is \(o(1)\), which is insufficient for our purpose. Later in this section, we refine the analysis in [11] to obtain the desired convergence rate. The refinement culminates in the following.

**Lemma 14**.: _Let \(\alpha>0\) and \(G\sim\mathsf{ER}(n,\alpha\log(n)/n)\). Let \(v\) be a node of \(G\). The size of the \(k\)-core of \(G-v\) satisfies_

\[\mathbb{P}\left(\left|\mathsf{core}_{k}(G-v)\right|<n-3n^{1-\alpha}\right)=o( 1/n).\]

The proof of Lemma 14 is deferred to Appendix C.1. It remains to study the error event that \(v\) has too few neighbors in \(\mathsf{core}_{k}(G-v)\). To count the number of neighbors of \(v\) in \(\mathsf{core}_{k}(G-v)\), we exploit the independence of \(\mathsf{core}_{k}(G-v)\) and \(v\) as follows: each neighbor of \(v\) is considered a _success_ if it belongs to \(\mathsf{core}_{k}(G-v)\) and a _failure_ otherwise. Counting the number of successes is equivalent to sampling _with_ replacement \(\delta_{G}(v)\) elements, each of which is independently a success with probability \(\left|\mathsf{core}_{k}(G-v)\right|/\left(n-1\right)\). The number of successes then follows precisely a hypergeometric distribution. This intuition is made rigorous in the proof below.

Let us recall some facts about the hypergeometric distribution because it plays an important role in the proof. Denote by \(\mathsf{HypGeom}(N,K,n)\) a random variable that counts the number of successes in a sample of \(n\) elements drawn _without replacement_ from a population of \(N\) individuals, of which \(K\) elements are considered successes. Note that if this sampling were done _with replacement_, then the number of successes would follow a \(\mathsf{Bin}\left(n,K/N\right)\) distribution. A result of Hoeffding [10] establishes that the \(\mathsf{HypGeom}(N,K,n)\) distribution is convex-order dominated by the \(\mathsf{Bin}\left(n,K/N\right)\) distribution, i.e.

\[\mathbb{E}\left[f(\mathsf{HypGeom}(N,K,n))\right]\leq\mathbb{E}\left[f( \mathsf{Bin}(n,K/N))\right]\ \text{ for all convex functions }f.\]

In particular, the function \(f(x)=e^{tx}\) is convex for any value of \(t\), and so Chernoff bounds that hold for the binomial distribution also hold for the corresponding hypergeometric distribution. This yields the following proposition.

**Proposition 15**.: _Let \(X\sim\mathsf{HypGeom}(N,K,n)\). It follows for any \(\delta>0\) that_

\[\mathbb{P}\left(X>(1+\delta)\times\frac{nK}{N}\right)\leq\left(\frac{e^{ \delta}}{(1+\delta)^{1+\delta}}\right)^{nK/N}\leq\left(\frac{e}{1+\delta} \right)^{(1+\delta)nK/N}.\]

Our final remark about the hypergeometric distribution is a symmetry property. By interchanging the success and failure states, it follows that

\[\mathbb{P}\left(\mathsf{HypGeom}(N,K,n)=k\right)=\mathbb{P}\left(\mathsf{ HypGeom}(N,N-K,n)=n-k\right).\]

The above intuition for the proof of Lemma 8 is formalized below.

Proof of Lemma 8.: Let \(V\) denote the vertex set of \(G\), and let \(G-v\) denote the induced subgraph of \(G\) on the vertex set \(V-\{v\}\). For any set \(A\subseteq V\), let \(N_{v}(A)\) denote the set of neighbors of \(v\) in the set \(A\), i.e.

\[N_{v}(A):=\left\{u\in A:\{u,v\}\in E(G)\right\}.\]

Since \(\mathsf{core}_{k}(G-v)\subseteq\mathsf{core}_{k}(G)\), it is true that

\[\left\{v\notin\mathsf{core}_{k}(G)\right\}\subseteq\left\{|N_{v}(\mathsf{core }_{k}(G))|\leq k-1\right\}\subseteq\left\{|N_{v}(\mathsf{core}_{k}(G-v))| \leq k-1\right\}.\]

It follows that

\[\mathbb{P}\left(\left\{v\notin\mathsf{core}_{k}(G)\right\}\cap\left\{\delta_ {G}(v)\geq k+1/\alpha\right\}\right)\leq p_{1}+p_{2},\]

where

\[p_{1}=\mathbb{P}\left(\left\{N_{v}(\mathsf{core}_{k}(G-v))\leq k -1\right\}\cap\left\{\delta_{G}(v)\geq k+1/\alpha\right\}\cap\left\{|\mathsf{ core}_{k}(G-v)|<n-3n^{1-\alpha}\right\}\right)\] \[p_{2}=\mathbb{P}\left(\left\{N_{v}(\mathsf{core}_{k}(G-v))\leq k -1\right\}\cap\left\{\delta_{G}(v)\geq k+1/\alpha\right\}\cap\left\{|\mathsf{ core}_{k}(G-v)|\geq n-3n^{1-\alpha}\right\}\right)\]

It suffices to show that both \(p_{1}\) and \(p_{2}\) are \(o(1/n)\). The term \(p_{1}\) deals with the probability that the \(k\)-core of \(G-v\) is too small. In fact, by Lemma 14, it follows directly that

\[p_{1}\leq\mathbb{P}\left(|\mathsf{core}_{k}(G-v)|<n-3n^{1-\alpha}\right)=o(1/ n),\]

Next, the probability \(p_{2}\) is analyzed. Enumerate arbitrarily but independently the elements of sets \(N_{v}(V)\) and \(\mathsf{core}_{k}(G-v)\), so that

\[N_{v}(V)=\left\{v_{1},\cdots,v_{\delta_{G}(v)}\right\},\quad\mathsf{core}_{ k}(G-v)=\left\{a_{1},\cdots,a_{|\mathsf{core}_{k}(G-v)|}\right\}.\]

Given that \(N_{v}(V)\) has more than \(k+1/\alpha\) nodes and \(\mathsf{core}_{k}(G-v)\) has more than \(n-3n^{1-\alpha}\) nodes, it is true that

\[N_{v}(\mathsf{core}_{k}(G-v)) =N_{v}(V)\cap\mathsf{core}_{k}(G-v)\] \[\geq\left\{v_{1},\cdots,v_{\lceil k+1/\alpha\rceil}\right\}\cap \left\{a_{1},\cdots,a_{\lceil n-3n^{1-\alpha}\rceil}\right\}=:\widetilde{N_{v} }(\widetilde{\mathsf{core}}_{k}(G-v)).\]

In words, \(\widetilde{N_{v}}(\widetilde{\mathsf{core}}_{k}(G-v))\) counts among the first \(\lceil k+1/\alpha\rceil\) neighbors of \(v\) those nodes that are also in the first \(\lceil n-3n^{1-\alpha}\rceil\) nodes of \(\mathsf{core}_{k}(G-v)\). Therefore,

\[p_{2} =\mathbb{P}\left(\left\{N_{v}(\mathsf{core}_{k}(G-v))\leq k-1 \right\}\cap\left\{\delta_{G}(v)\geq\ k+1/\alpha\right\}\cap\left\{|\mathsf{ core}_{k}(G-v)|\geq n-3n^{1-\alpha}\right\}\right)\] \[\leq\mathbb{P}\left(\left\{N_{v}(\mathsf{core}_{k}(G-v))\leq k -1\right\}\big{|}\ \delta_{G}(v)\geq\ k+1/\alpha,\ |\mathsf{core}_{k}(G-v)|\geq n-3n^{1-\alpha}\right)\] (8)

[MISSING_PAGE_EMPTY:15]

Our objective is to show that the \(k\)-core of \(G-v\) is sufficiently large with probability \(1-o(1/n)\). To that end, consider Algorithm 3 to identify a subset of the \(k\)-core, originally proposed by Luczak [11].

``` require : Graph \(G\), Set \(U\subseteq V(G)\).
1\(U_{0}\gets U\)
2for\(i=0,1,2,3,\cdots\)do
3ifthere exists \(u\in V\setminus U_{i}\) such that \(u\) has \(3\) or more neighbors in \(U_{i}\)then
4\(U_{i+1}\gets U_{i}\cup\{u\}\)
5else
6return\(U_{i}\)
7 end if
8
9 end for ```

**Algorithm 3**Luczak expansion

Note that the **for** loop eventually terminates - the set \(V\setminus U_{i}\) is empty, for example when \(i=n\) for any input set \(U\). The key is to realize that the **for** loop terminates much faster when the input \(U=Z_{k+1}\), i.e the set of vertices of the input graph \(G\) whose degree is \(k+1\) or less. Furthermore, the complement of the set output by the algorithm is contained in the \(k\)-core. Formally,

**Lemma 17**.: _Let \(U_{f}\) be the output of Algorithm 3 with input graph \(G-v\) and set \(U=Z_{k+1}\). Then,_

1. \(U_{f}^{c}\subseteq\mathsf{core}_{k}(G-v)\)_._
2. _For any_ \(\delta>1-\alpha\)_,_ \[\mathbb{P}\left(|U_{f}|>3n^{\delta}\right)=o(1/n).\]

Proof.: (a) The proof is by construction: Since \(U_{f}\) is obtained by adding exactly \(f\) nodes to \(U_{0}\), it follows that \(U_{f}^{c}\subseteq U_{0}^{c}=Z_{k+1}^{c}\), so each node in \(U_{f}^{c}\) has degree \(k+2\) or more in \(G-v\). Further, each node in \(U_{f}^{c}\) has at most \(2\) neighbors in \(U_{f}\), else the **for** loop would not have terminated. Thus, the subgraph of \(G-v\) induced on the set \(U_{f}^{c}\) has minimum degree at least \(k\), and the result follows.

(b) If \(|U_{f}|>3n^{\delta}\), then either \(|U_{0}|>3n^{\delta}\) or there is some \(M\) in \(\{0,1,\cdots,f\}\) for which \(|U_{M}|=3n^{\delta}\). Therefore,

\[\mathbb{P}\left(|U_{f}|>3n^{\delta}\right) \leq\mathbb{P}\left(|U_{0}|>3n^{\delta}\right)+\mathbb{P}\left( \exists\;M\in\{0,1,\cdots,f\}\text{ s.t. }|U_{M}|=3n^{\delta}\right)\] \[=o(1/n)+\underbrace{\mathbb{P}\left(\exists\;M\in\{0,1,\cdots,f \}\text{ s.t. }|U_{M}|=3n^{\delta}\right)}_{(\star)},\]

by Proposition 16. Note that each iteration \(i=0,1,\cdots,M-1\) of the **for** loop adds exactly \(1\) vertex and at least \(3\) edges to the subgraph of \(G-v\) induced on \(U_{M}\). Therefore, the induced subgraph \(G|_{U_{M}}\) has \(3n^{\delta}\) vertices and at least \(3\left(|U_{M}|-|U_{0}|\right)\) edges. Thus,

\[(\star) \leq\mathbb{P}\left(\exists\text{ subgraph }H=(W,F)\text{ of }G-v\text{ s.t. }|W|=3n^{\delta}\text{ and }|F|\geq 3\left(3n^{\delta}-|U_{0}|\right)\right)\] \[\leq\mathbb{P}\left(|U_{0}|>n^{\delta}\right)+\mathbb{P}\left( \exists\text{ subgraph }H=(W,F)\text{ of }G-v\text{ s.t. }|W|=3n^{\delta}\text{ and }|F|\geq 6n^{\delta}\right)\] \[\leq o(1/n)+\underbrace{\binom{n}{3n^{\delta}}\cdot\mathbb{P} \left(\mathsf{Bin}\left(\binom{3n^{\delta}}{2},\frac{\alpha\log(n)}{n} \right)>6n^{\delta}\right)}_{(\star\star)},\]where the last step uses Proposition 16 and a union bound over all possible choices of \(W\). Finally, using the relation \(\binom{n}{k}\leq\binom{ne}{k}^{k}\) and the concentration inequality (5) from Lemma 13 yields

\[(\star\star) \leq\left(\frac{n^{1-\delta}e}{3}\right)^{3n^{\delta}}\mathbb{P} \left(\mathsf{Bin}\left(\frac{9n^{2\delta}}{2},\frac{\alpha\log n}{n}\right)>6n ^{\delta}\right)\] \[\leq(n^{1-\delta})^{3n^{\delta}}\times\left(\frac{3\alpha e\log n }{4n^{1-\delta}}\right)^{6n^{\delta}}\] \[=\left(\frac{3\alpha e\log n}{4n^{(1-\delta)/2}}\right)^{6n^{ \delta}}\] \[=o(1/n),\]

whenever \(0<\delta<1\). The result follows. 

Finally, notice that Lemma 17 directly implies Lemma 14.

## Appendix D Proof of Theorem 9

**Theorem 9**.: _Let \(G_{1},\cdots,G_{m}\) be correlated graphs from the subsampling model with parameters \(C\) and \(s\). Let \(v\in V\) and let \(U\) be a vertex cut of \(\{1,\cdots,m\}\) such that \(|U|\leq\lfloor m/2\rfloor\). Then,_

\[\mathbb{P}\left(\{c_{v}(U)=0\}\cap\left\{\widetilde{c}_{v}(U)>\frac{m^{2}}{4} \left(k+\frac{1}{Cs^{2}}\right)\right\}\right)=o(1/n).\] (2)

Proof.: For any vertex cut \(U\),

\[\left\{\widetilde{c}_{v}(U)>\frac{m^{2}}{4}\left(k+\frac{1}{Cs^{2 }}\right)\right\} \stackrel{{\text{(a)}}}{{\subseteq}}\left\{ \widetilde{c}_{v}(U)>|U|\left(m-|U|\right)\left(k+\frac{1}{Cs^{2}}\right)\right\}\] \[=\left\{\sum_{i\in U}\sum_{j\in U^{c}}\delta_{G^{\prime}_{i} \wedge G^{\prime}_{j}}(v)>|U|(m-|U|)\bigg{(}k+\frac{1}{Cs^{2}}\bigg{)}\right\}\] \[\subseteq\bigcup_{i\in U}\bigcup_{j\in U^{c}}\left\{\delta_{G^{ \prime}_{i}\wedge G^{\prime}_{j}}(v)>k+\frac{1}{Cs^{2}}\right\},\]

where (a) uses the fact that the maximum of a set of a numbers is greater than or equal to the average. On the other hand

\[\left\{c_{v}(U)=0\right\}=\bigcap_{i\in U}\bigcap_{j\in U^{c}}\left\{v\notin \mathsf{core}_{k}(G^{\prime}_{i}\wedge G^{\prime}_{j})\right\}.\]

Let \(p_{1}\) denote the probability in the LHS of (2). It follows from the union bound that

\[p_{1}\leq\sum_{i\in U}\sum_{j\in U^{c}}\mathbb{P}\left(\left\{v\notin \mathsf{core}_{k}(G^{\prime}_{i}\wedge G^{\prime}_{j})\right\}\cap\left\{ \delta_{G^{\prime}_{i}\wedge G^{\prime}_{j}}(v)>k+\frac{1}{Cs^{2}}\right\} \right)=o(1/n),\]

since for any choice of \(i\) and \(j\), the graph \(G^{\prime}_{i}\wedge G^{\prime}_{j}\sim\mathsf{ER}\left(n,Cs^{2}\log(n)/n\right)\). 

## Appendix E On Stochastic Dominance: Proof of Theorem 10

The objective of this section is to build up to a proof of Theorem 10. We start by making a simple observation about products of Binomial random variables.

**Lemma 18**.: _Let \(X_{1},\cdots,X_{m}\sim\mathsf{Bern}(s)\) be i.i.d. random variables, and let \(B=X_{1}+\cdots+X_{m}\) denote their sum. For each \(\ell\) in \(\{1,2,\cdots,\lfloor m/2\rfloor\}\), define_

\[T_{\ell}=\left(X_{1}+\cdots+X_{\ell}\right)\left(X_{\ell+1}+\cdots+X_{m}\right).\]

_For any \(\ell_{1},\ell_{2}\in\{1,2,\cdots,\lfloor m/2\rfloor\}\) such that \(\ell_{1}<\ell_{2}\), and for any \(t\in\mathbb{R}\) and any \(b\in\{0,1,\cdots,m\}\),_

\[\mathbb{P}\left(T_{\ell_{1}}>t\mid B=b\right)\leq\mathbb{P}\left(T_{\ell_{2}}> t\mid B=b\right).\] (11)Proof of Lemma 18.: Consider overlapping but exhaustive cases:

_Case 1: \(t<0\)._ Since \(T_{\ell}\geq 0\) almost surely for all \(\ell\), the inequality (11) holds.

_Case 2: \(t\geq b-1\)._ Note that conditioned on \(B=b\), it follows that \(T_{1}\in\{0,b-1\}\). Therefore, the left hand side of (11) equals zero, and the inequality holds.

_Case 3: \(b=0\) or \(b=1\)._ In this case, \(T_{\ell}\) is identically zero for all \(\ell\), so (11) holds.

_Case 4: \(b\geq 2\) and \(0\leq t<b-1\)._ For any \(\ell\in\{1,2,\cdots,\lfloor m/2\rfloor\}\),

\[\mathbb{P}\left(T_{\ell}>t\mid B=b\right) =\frac{\mathbb{P}\left(\{\left(X_{1}+\cdots+X_{\ell}\right)\left( X_{\ell+1}+\cdots+X_{m}\right)>t\}\cap\{X_{1}+\cdots+X_{m}=b\}\right)}{ \mathbb{P}\left(X_{1}+\cdots+X_{m}=b\right)}\] \[=\frac{\sum_{i:i(b-i)>t}\mathbb{P}\left(\{X_{1}+\cdots+X_{\ell}=i \}\cap\{X_{\ell+1}+\cdots+X_{m}=b-i\}\right)}{\mathbb{P}\left(X_{1}+\cdots+X_ {m}=b\right)}\] \[\overset{\text{(a)}}{=}\frac{\sum_{i=1}^{b-1}\mathbb{P}\left(X_{ 1}+\cdots+X_{\ell}=i\right)\mathbb{P}\left(X_{\ell+1}+\cdots+X_{m}=b-i\right) }{\mathbb{P}\left(X_{1}+\cdots+X_{m}=b\right)}\] \[\overset{\text{(b)}}{=}\frac{\sum_{i=1}^{b-1}\ell\choose i}{m- \ell\choose b-i}\] \[=\frac{\sum_{i=0}^{b}{\ell\choose i}{m-\ell\choose b-i}-{m- \ell\choose b}-{\ell\choose b}}{m\choose b},\] (12)

where (a) used the fact that for any \(t\) such that \(0\leq t<b-1\), it is true that

\[\left\{i:i(b-i)>t\right\}=\left\{1,2,\cdots,b-1\right\}.\]

Here, the notation for binomial coefficients in (b) involves setting \({n\choose k}=0\) whenever \(k<0\) or \(k>n\). Let \(f_{m,b}(\ell)\) denote the numerator of (12), i.e.

\[f_{m,b}(\ell):={m\choose b}-{m-\ell\choose b}-{\ell\choose b}\]

It suffices to show that \(f_{m,b}(\ell)-f_{m,b}(\ell-1)\geq 0\) for all \(\ell\in\{2,\cdots,\lfloor m/2\rfloor\}\). Indeed,

\[f_{m,b}(\ell)-f_{m,b}(\ell-1) ={m-\ell+1\choose b}-{m-\ell\choose b}-\left({\ell\choose b}-{ \ell-1\choose b}\right)\] \[\overset{\text{(c)}}{=}{m-\ell\choose b-1}-{\ell-1\choose b-1} \geq 0,\]

whenever \(m-\ell\geq\ell-1\), i.e. \(\ell\leq\lfloor m/2\rfloor\). Here, (c) uses the identity \({n\choose k}={n-1\choose k-1}+{n-1\choose k}\), and the fact that \({n_{1}\choose k}\geq{n_{2}\choose k}\) whenever \(n_{1}\geq n_{2}\). This concludes the proof. 

**Corollary 19**.: _Let \(F\) be a collection of edges in the parent graph \(G\). For any edge \(e_{r}\in F\), let \(X_{i}^{r}\) denote the indicator random variable \(G_{i}^{r}(e_{r})\sim\mathsf{Bern}(ps)\). For each \(\ell\) in \(\{1,\cdots,\lfloor m/2\rfloor\}\), define_

\[T_{\ell}^{r}=(X_{1}^{r}+\cdots+X_{\ell}^{r})(X_{\ell+1}^{r}+\cdots+X_{m}^{r}).\]

_Then, for any \(\ell_{1},\ell_{2}\in\{1,\cdots,\lfloor m/2\rfloor\}\) such that \(\ell_{1}<\ell_{2}\), the following stochastic ordering holds_

\[\sum_{r=1}^{\left|F\right|}T_{\ell_{1}}^{r}\preceq\sum_{r=1}^{\left|F\right|} T_{\ell_{2}}^{r}.\]

Proof.: It suffices to show that \(T_{\ell_{1}}^{r}\preceq T_{\ell_{2}}^{r}\) for each \(r\), since the edges are independent. Indeed, we have for any \(t\) that

\[\mathbb{P}\left(T_{\ell_{1}}^{r}>t\right)=\sum_{b=0}^{m}\mathbb{P}\left(B=b \right)\mathbb{P}\left(T_{\ell_{1}}^{r}>t|B=b\right)\leq\sum_{b=0}^{m} \mathbb{P}\left(B=b\right)\mathbb{P}\left(T_{\ell_{2}}^{r}>t|B=b\right)= \mathbb{P}\left(T_{\ell_{2}}^{r}>t\right),\]

which concludes the proof.

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims made in the abstract and introduction are formally stated in Section 3 and proved in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 5 explicitly mentions and discusses limitations relating to the runtime of our algorithms. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All proof outlines and intuition is presented in the main body of the paper. Some formal proofs are deferred to the Supplementary Material in view of space constraints. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

## 4 Experimental Result Reproducibility

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes] Justification: The pseudocode presented in Algorithm 2 is implementable using basic Python libraries, and existing subroutines for pairwise matching. URLs to these subroutines are referenced in the main body of the paper.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The simulations do not involve any data, since all simulations are done for random (Erdos-Renyi) graphs. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: No experiments involving any training were done in this work. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All plots include error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The simulations are classical Monte-Carlo simulations that do not require extensive runtime or hardware. Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The Code of Ethics was strictly adhered to during all stages of this research. Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The potential for graph matching through transitive closure is motivated through its application to social network de-anonymization. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

Answer: [NA]

Justification: The paper poses no such risks.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

Answer: [Yes]

Justification: The subroutines for GRAMPA and Degree Profiles have been cited.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.