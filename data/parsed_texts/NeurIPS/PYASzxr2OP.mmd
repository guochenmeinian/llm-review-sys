Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback

 Han Shao

TTIC

han@ttic.edu

&Lee Cohen

TTIC

leecohencs@gmail.com

&Avrim Blum

TTIC

avrim@ttic.edu

&Yishay Mansour

Tel Aviv University and Google Research

mansour.yishay@gmail.com &Aadirupa Saha

TTIC

aadirupa@ttic.edu &Matthew R. Walter

TTIC

mwalter@ttic.edu

###### Abstract

In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a known Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a user is provided with two policies and returns their preferred policy as feedback. We then move to a different user feedback model, where a user is instead provided with two small weighted sets of representative trajectories and selects the preferred one. In both cases, we suggest an algorithm that finds a nearly optimal policy for the user using a number of comparison queries that scales quasilinearly in the number of objectives.

## 1 Introduction

Many real-world decision making problems involve optimizing over multiple objectives. For example, when designing an investment portfolio, one's investment strategy requires trading off maximizing expected gain with minimizing risk. When using Google Maps for navigation, people are concerned about various factors such as the worst and average estimated arrival time, traffic conditions, road surface conditions (e.g., whether or not it is paved), and the scenery along the way. As the advancement of technology gives rise to personalized machine learning (McAuley, 2022), in this paper, we design efficient algorithms for personalized multi-objective decision making.

While prior works have concentrated on approximating the Pareto-optimal solution set1 (see Hayes et al. (2022) and Roijers et al. (2013) for surveys), we aim to find the optimal personalized policy for a user that reflects their unknown preferences over \(k\) objectives. Since the preferences are unknown, we need to elicit users' preferences by requesting feedback on selected policies. The problem of eliciting preferences has been studied in Wang et al. (2022) using a strong query model that provides stochastic feedback on the quality of a single policy. In contrast, our work focuses on a more natural and intuitive query model, comparison queries, which query the user's preference over two selected policies, e.g., 'do you prefer a policy which minimizes average estimated arrival time or policy which minimizes the number of turns?'. Our goal is to find the optimal personalized policy using as few queries as possible.

To the best of our knowledge, we are the first to provide algorithms with theoretical guarantees for specific personalized multi-objective decision-making via policy comparisons.

Similar to prior works on multi-objective decision making, we model the problem using a finite-horizon Markov decision process (MDP) with a \(k\)-dimensional reward vector, where each entry is a non-negative scalar reward representative of one of the \(k\) objectives. To account for user preferences, we assume that a user is characterized by a (hidden) \(k\)-dimensional _preference vector_ with non-negative entries, and that the _personalized reward_ of the user for each state-action is the inner product between this preference vector and the reward vector (for this state-action pair). We also distinguish between the \(k\)-dimensional value of a policy, which is the expected cumulative reward when selecting actions according to the policy, and the _personalized value_ of a policy, which is the scalar expected cumulative personalized reward when selecting actions according to this policy. The MDP is known to the agent and the goal is to learn an optimal policy for the personalized reward function (henceforth, the _optimal personalized policy_) of a user via policy comparative feedback.

**Comparative feedback.** If people could clearly define their preferences over objectives (e.g., "my preference vector has \(3\) for the scenery objective, \(2\) for the traffic objective, and \(1\) for the road surface objective"), the problem would be easy--one would simply use the personalized reward function as a scalar reward function and solve for the corresponding policy. In particular, a similar problem with noisy feedback regarding the value of a single multi-objective policy (as mapping from states to actions) has been studied in Wang et al. (2022). As this type of fine-grained preference feedback is difficult for users to define, especially in environments where sequential decisions are made, we restrict the agent to rely solely on comparative feedback. Comparative feedback is widely used in practice. For example, ChatGPT asks users to compare two responses to improve its performance. This approach is more intuitive for users compared to asking for numerical scores of ChatGPT responses.

Indeed, as knowledge of the user's preference vector is sufficient to solve for their optimal personalized policy, the challenge is to learn a user's preference vector using a minimal number of easily interpretable queries. We therefore concentrate on comparison queries. The question is then what exactly to compare? Comparing state-action pairs might not be a good option for the aforementioned tasks--what is the meaning of two single steps in a route? Comparing single trajectories (e.g., routes in Google Maps) would not be ideal either. Consider for example two policies: one randomly generates either (personalized) GOOD or (personalized) BAD trajectories while the other consistently generates (personalized) MEDIOCRE trajectories. By solely comparing single trajectories without considering sets of trajectories, we cannot discern the user's preference regarding the two policies.

**Interpretable policy representation.** Since we are interested in learning preferences via policy comparison queries, we also suggest an alternative, more interpretable representation of a policy. Namely, we design an algorithm that given an explicit policy representation (as a mapping from states to distributions over actions), returns a weighted set of trajectories of size at most \(k+1\), such that its expected return is identical to the value of the policy.2 It immediately follows from our formalization that for any user, the personalized return of the weighted trajectory set and the personalized value of the policy are also identical.

Footnote 2: A return of a trajectory is the cumulative reward obtained in the trajectory. The expectation in the expected return of a weighted trajectory set is over the weights.

In this work, we focus on answering two questions:

_(1) How to find the optimal personalized policy by querying as few policy comparisons as possible?_

_(2) How can we find a more interpretable representation of policies efficiently?_

**Contributions.** In Section 2, we formalize the problem of eliciting user preferences and finding the optimal personalized policy via comparative feedback. As an alternative to an explicit policy representation, we propose a _weighted trajectory set_ as a more interpretable representation. In Section 3, we provide an _efficient_ algorithm for finding an approximate optimal personalized policy, where the policies are given by their formal representations, thus answering (1). In Section 4, we design two efficient algorithms that find the weighted trajectory set representation of a policy. Combined with the algorithm in Section 3, we have an algorithm for finding an approximate optimal personalized policy when policies are represented by weighted trajectory sets, thus answering (2).

**Related Work.** Multi-objective decision making has gained significant attention in recent years (see Roijers et al. (2013); Hayes et al. (2022) for surveys). Prior research has explored various approaches,such as assuming linear preferences or Bayesian Settings, or finding an approximated Pareto frontier. However, incorporating comparison feedback (as was done for Multi-arm bandits or active learning, e.g., Benggs et al. (2021) and Kane et al. (2017)) allows us a more comprehensive and systematic approach to handling different types of user preferences and provides (nearly) optimal personalized decision-making outcomes. We refer the reader to Appendix A for additional related work.

## 2 Problem Setup

**Sequential decision model.** We consider a Markov decision process (MDP) _known3_ to the agent represented by a tuple \(\left\langle\mathcal{S},\mathcal{A},s_{0},P,R,H\right\rangle\), with finite state and action sets, \(\mathcal{S}\) and \(\mathcal{A}\), respectively, an initial state \(s_{0}\in\mathcal{S}\), and finite horizon \(H\in\mathbb{N}\). For example, in the Google Maps example a state is an intersection and actions are turns. The transition function \(P:\mathcal{S}\times\mathcal{A}\mapsto\mathrm{Simplex}^{\mathcal{S}}\) maps state-action pairs into a state probability distribution. To model multiple objectives, the reward function \(R:\mathcal{S}\times\mathcal{A}\mapsto[0,1]^{k}\) maps every state-action pair to a \(k\)-dimensional reward vector, where each component corresponds to one of the \(k\) objectives (e.g., road surface condition, worst and average estimated arrival time). The _return_ of a trajectory \(\tau=(s_{0},a_{0},\ldots,s_{H-1},a_{H-1},s_{H})\) is given by \(\Phi(\tau)=\sum_{t=0}^{H-1}R(s_{t},a_{t})\).

Footnote 3: If the MDP is unknown, one can approximate the MDP and apply the results of this work in the approximated MDP. More discussion is included in Appendix 5.

A _policy_\(\pi\) is a mapping from states to a distribution over actions. We denote the set of policies by \(\Pi\). The _value_ of a policy \(\pi\), denoted by \(V^{\pi}\), is the expected cumulative reward obtained by executing the policy \(\pi\) starting from the initial state, \(s_{0}\). Put differently, the value of \(\pi\) is \(V^{\pi}=V^{\pi}(s_{0})=\mathbb{E}_{S_{0}=s_{0}}\left[\sum_{t=0}^{H-1}R(S_{t}, \pi(S_{t}))\right]\in[0,H]^{k}\), where \(S_{t}\) is the random state at time step \(t\) when executing \(\pi\), and the expectation is over the randomness of \(P\) and \(\pi\). Note that \(V^{\pi}=\mathbb{E}_{\tau}\left[\Phi(\tau)\right]\), where \(\tau=(s_{0},\pi(s_{0}),S_{1},\ldots,\pi(S_{H-1}),S_{H})\) is a random trajectory generated by executing \(\pi\).

We assume the existence of a "do nothing" action \(a_{0}\in\mathcal{A}\), available only from the initial state \(s_{0}\), that has zero reward for each objective \(R(s_{0},a_{0})=\mathbf{0}\) and keeps the system in the initial state, i.e., \(P(s_{0}\mid s_{0},a_{0})=1\) (e.g., this action corresponds to not commuting or refusing to play in a chess game.)4. We also define the (deterministic) "do nothing" policy \(\pi_{0}\) that always selects action \(a_{0}\) and has a value of \(V^{\pi_{0}}=\mathbf{0}\). From a mathematical perspective, the assumption of "do nothing" ensures that \(\mathbf{0}\) belongs to the value vector space \(\{V^{\pi}|\pi\in\Pi\}\), which is precisely what we need.

Footnote 4: We remark that the “do nothing” action require domain-specific knowledge, for example in the Google maps example the “do nothing” will be to stay put and in the ChatGPT example the “do nothing” is to answer nothing.

Since the rewards are bounded between \([0,1]\), we have that \(1\leq\left\|V^{\pi}\right\|_{2}\leq\sqrt{k}H\) for every policy \(\pi\). For convenience, we denote \(C_{V}=\sqrt{k}H\). We denote by \(d\leq k\) the rank of the space spanned by all the value vectors obtained by \(\Pi\), i.e., \(d:=\mathrm{rank}(\mathrm{span}(\{V^{\pi}|\pi\in\Pi\}))\).

**Linear preferences.** To incorporate personalized preferences over objectives, we assume each user is characterized by an unknown \(k\)-dimensional _preference vector_\(w^{*}\in\mathbb{R}_{+}^{k}\) with a bounded norm \(1\leq\left\|w^{*}\right\|_{2}\leq C_{w}\) for some (unknown) \(C_{w}\geq 1\). We avoid assuming that \(C_{w}=1\) to accommodate for general linear rewards. Note that the magnitude of \(w^{*}\) does not change the personalized optimal policy but affects the "indistinguishability" in the feedback model. By not normalizing \(\left\|w^{*}\right\|_{2}=1\), we allow users to have varying levels of discernment. This preference vector encodes preferences over the multiple objectives and as a result, determines the user preferences over policies.

Formally, for a user characterized by \(w^{*}\), the _personalized value_ of policy \(\pi\) is \(\left\langle w^{*},V^{\pi}\right\rangle\in\mathbb{R}_{+}\). We denote by \(\pi^{*}:=\arg\max_{\pi\in\Pi}\left\langle w^{*},V^{\pi}\right\rangle\) and \(v^{*}:=\left\langle w^{*},V^{\pi^{*}}\right\rangle\) the optimal _personalized policy_ and its corresponding optimal personalized value for a user who is characterized by \(w^{*}\). We remark that the "do nothing" policy \(\pi_{0}\) (that always selects action \(a_{0}\)) has a value of \(V^{\pi_{0}}=\mathbf{0}\), which implies a personalized value of \(\left\langle w^{*},V^{\pi_{0}}\right\rangle=0\) for every \(w^{*}\). For any two policies \(\pi_{1}\) and \(\pi_{2}\), the user characterized by \(w^{*}\) prefers \(\pi_{1}\) over \(\pi_{2}\) if \(\left\langle w^{*},V^{\pi_{1}}-V^{\pi_{2}}\right\rangle>0\). Our goal is to find the optimal personalized policy for a given user using as few interactions with them as possible.

**Comparative feedback.** Given two policies \(\pi_{1},\pi_{2}\), the user returns \(\pi_{1}\succ\pi_{2}\) whenever \(\left\langle w^{*},V^{\pi_{1}}-V^{\pi_{2}}\right\rangle>\epsilon\); otherwise, the user returns "indistinguishable" (i.e., whenever \(\left|\left\langle w^{*},V^{\pi_{1}}-V^{\pi_{2}}\right\rangle\right|\leq\epsilon\)). Here \(\epsilon>0\) measures the precision of the comparative feedback and is small usually. The agent can query the user about their policy preferences using two different types of policy representations:

1. _Explicit policy representation of \(\pi\)_: An explicit representation of policy as mapping, \(\pi:\mathcal{S}\to\text{Simplex}^{\mathcal{A}}\).
2. _Weighted trajectory set representation of \(\pi\)_: A \(\kappa\)-sized set of trajectory-weight pairs \(\{\langle p_{i},\tau_{i}\rangle\}_{i=1}^{\kappa}\) for some \(\kappa\leq k+1\) such that (i) the weights \(p_{1},\ldots,p_{\kappa}\) are non-negative and sum to \(1\); (ii) every trajectory in the set is in the support5 of the policy \(\pi\); and (iii) the expected return of these trajectories according to the weights is identical to the value of \(\pi\), i.e., \(V^{\pi}=\sum_{i=1}^{\kappa}p_{i}\Phi(\tau_{i})\). Such comparison could be practical for humans. E.g., in the context of Google Maps, when the goal is to get from home to the airport, taking a specific route takes \(40\) minutes \(90\%\) of the time, but it can take \(3\) hours in case of an accident (which happens w.p. \(10\%\)) vs. taking the subway which always has a duration of \(1\) hour. Footnote 5: We require the trajectories in this set to be in the support of the policy, to avoid trajectories that do not make sense, such as trajectories that “teleport” between different unconnected states (e.g., commuting at \(3\) mph in Manhattan in one state and then at \(40\) mph in New Orleans for the subsequent state).

In both cases, the feedback is identical and depends on the hidden precision parameter \(\epsilon\). As a result, the value of \(\epsilon\) will affect the number of queries and how close the value of the resulting personalized policy is to the optimal personalized value. Alternatively, we can let the agent decide in advance on a maximal number of queries, which will affect the optimality of the returned policy.

**Technical Challenges.** In Section 3, we find an approximate optimal policy by \(\mathcal{O}(\log\frac{1}{\epsilon})\) queries. To achieve this, we approach the problem in two steps. Firstly, we identify a set of linearly independent policy values, and then we estimate the preference vector \(w^{*}\) using a linear program that incorporates comparative feedback. The estimation error of \(w^{*}\) usually depends on the condition number of the linear program. Therefore, the main challenge we face is how to search for linear independent policy values that lead to a small condition number and providing a guarantee for this estimate.

In Section 4, we focus on how to design _efficient_ algorithms to find the weighted trajectory set representation of a policy. Initially, we employ the well-known Caratheodory's theorem, which yields an inefficient algorithm with a potentially exponential running time of \(\left|S\right|^{H}\). Our main challenge lies in developing an efficient algorithm with a running time of \(\mathcal{O}(\operatorname{poly}(H\left|S\right|\left|\mathcal{A}\right|))\). The approach based on Caratheodory's theorem treats the return of trajectories as independent \(k\)-dimensional vectors, neglecting the fact that they are all generated from the same MDP. To overcome this challenge, we leverage the inherent structure of MDPs.

## 3 Learning from Explicit Policies

In this section, we consider the case where the interaction with a user is based on explicit policy comparison queries. We design an algorithm that outputs a policy being nearly optimal for this user. For multiple different users, we only need to run part of the algorithm again and again. For brevity, we relegate all proofs to the appendix.

If the user's preference vector \(w^{*}\) (up to a positive scaling) is given, then one can compute the optimal policy and its personalized value efficiently, e.g., using the Finite Horizon Value Iteration algorithm. In our work, \(w^{*}\) is unknown and we interact with the user to learn \(w^{*}\) through comparative feedback. Due to the structure model that is limited to meaningful feedback only when the compared policy values differ at least \(\epsilon\), the exact value of \(w^{*}\) cannot be recovered. We proceed by providing a high-level description of our ideas of how to estimate \(w^{*}\).

(1) _Basis policies_: We find policies \(\pi_{1},\ldots,\pi_{d}\), and their respective values, \(V^{\pi_{1}},\ldots,V^{\pi_{d}}\in[0,H]^{k}\), such that their values are linearly independent and that together they span the entire space of value vectors.6 These policies will not necessarily be personalized optimal for the current user, and instead serve only as building blocks to estimate the preference vector, \(w^{*}\). In Section 3.1 we describe an algorithm that finds a set of basis policies for any given MDP.

Footnote 6: Recall that \(d\leq k\) is the rank of the space spanned by all the value vectors obtained by all policies.

(2) _Basis ratios_: For the basis policies, denote by \(\alpha_{i}>0\) the ratio between the personalized value of a _benchmark policy_, \(\pi_{1}\), to the personalized value of \(\pi_{i+1}\), i.e.,

\[\forall i\in[d-1]:\alpha_{i}\left\langle w^{*},V^{\pi_{1}}\right\rangle=\left \langle w^{*},V^{\pi_{i+1}}\right\rangle\.\] (1)We will estimate \(\widehat{\alpha}_{i}\) of \(\alpha_{i}\) for all \(i\in[d-1]\) using comparison queries. A detailed algorithm for estimating these ratios appears in Section 3.2. For intuition, if we obtain exact ratios \(\widehat{\alpha}_{i}=\alpha_{i}\) for every \(i\in[d-1]\), then we can compute the vector \(\frac{w^{*}}{\left\|w^{*}\right\|_{1}}\) as follows. Consider the \(d-1\) equations and \(d-1\) variables in Eq (1). Since \(d\) is the maximum number of value vectors that are linearly independent, and \(V^{\pi_{1}},\ldots V^{\pi_{d}}\) form a basis, adding the equation \(\left\|w\right\|_{1}=1\) yields \(d\) independent equations with \(d\) variables, which allows us to solve for \(w^{*}\). The details of computing an estimate of \(w^{*}\) are described in Section 3.3.

### Finding a Basis of Policies

In order to efficiently find \(d\) policies with \(d\) linearly independent value vectors that span the space of value vectors, one might think that selecting the \(k\) policies that each optimizes one of the \(k\) objectives will suffice. However, this might fail--in Appendix N, we show an instance in which these \(k\) value vectors are linearly dependent even though there exist \(k\) policies whose values span a space of rank \(k\).

Moreover, our goal is to find not just any basis of policies, but a basis of policies such that (1) the personalized value of the benchmark policy \(\left\langle w^{*},V^{\pi_{1}}\right\rangle\) will be large (and hence the estimation error of ratio \(\alpha_{i}\), \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\), will be small), and (2) that the linear program generated by this basis of policies and the basis ratios will produce a good estimate of \(w^{*}\).

**Choice of \(\pi_{1}\).** Besides linear independence of values, another challenge is to find a basis of policies to contain a benchmark policy, \(\pi_{1}\) (where the index \(1\) is wlog) with a relatively large personalized value, \(\left\langle w^{*},V^{\pi_{1}}\right\rangle\), so that \(\widehat{\alpha}_{i}\)'s error is small (e.g., in the extreme case where \(\left\langle w^{*},V^{\pi_{1}}\right\rangle=0\), we will not be able to estimate \(\alpha_{i}\)).

For any \(w\in\mathbb{R}^{k}\), we use \(\pi^{w}\) denote a policy that maximizes the scalar reward \(\left\langle w,R\right\rangle\), i.e.,

\[\pi^{w}=\operatorname*{arg\,max}_{\pi\in\Pi}\left\langle w,V^{\pi}\right\rangle\,,\] (2)

and by \(v^{w}=\left\langle w,V^{\pi^{w}}\right\rangle\) to denote the corresponding personalized value. Let \(\mathbf{e}_{1},\ldots,\mathbf{e}_{k}\) denote the standard basis. To find \(\pi_{1}\) with large personalized value \(\left\langle w^{*},V^{\pi_{1}}\right\rangle\), we find policies \(\pi^{\mathbf{e}_{j}}\) that maximize the \(j\)-th objective for every \(j=1,\ldots,k\) and then query the user to compare them until we find a \(\pi^{\mathbf{e}_{j}}\) with (approximately) a maximal personalized value among them. This policy will be our benchmark policy, \(\pi_{1}\).

```
1:initialize \(\pi^{\mathbf{e}^{*}}\leftarrow\pi^{\mathbf{e}_{1}}\)
2:for\(j=2,\ldots,k\)do
3: compare \(\pi^{\mathbf{e}^{*}}\) and \(\pi^{\mathbf{e}_{j}}\)
4:if\(\pi^{\mathbf{e}_{j}}\succ\pi^{\mathbf{e}^{*}}\)then\(\pi^{\mathbf{e}^{*}}\leftarrow\pi^{\mathbf{e}_{j}}\)
5:endfor
6:\(\pi_{1}\leftarrow\pi^{\mathbf{e}^{*}}\) and \(u_{1}\leftarrow\frac{V^{\pi^{\mathbf{e}^{*}}}}{\left\|V^{\pi^{\mathbf{e}^{*}}} \right\|_{2}}\)
7:for\(i=2,\ldots,k\)do
8: arbitrarily pick an orthonormal basis, \(\rho_{1},\ldots,\rho_{k+1-i}\), for \(\operatorname*{span}(V^{\pi_{1}},\ldots,V^{\pi_{i-1}})^{\perp}\).
9:\(j_{\max}\leftarrow\arg\max_{j\in[k+1-i]}\max(\left|v^{\rho_{j}}\right|,\left|v^ {-\rho_{j}}\right|)\).
10:if\(\max(\left|v^{\rho_{j_{\max}}}\right|,\left|v^{-\rho_{j_{\max}}}\right|)>0\)then
11:\(\pi_{i}\leftarrow\pi^{\rho_{j_{\max}}}\)if\(\left|v^{\rho_{j_{\max}}}\right|>\left|v^{-\rho_{j_{\max}}}\right|\); otherwise\(\pi_{i}\leftarrow\pi^{-\rho_{j_{\max}}}\). \(u_{i}\leftarrow\rho_{j_{\max}}\)
12:else
13:return\((\pi_{1},\pi_{2},\ldots)\), \((u_{1},u_{2},\ldots)\)
14:endif
15:endfor ```

**Algorithm 1** Identification of Basis Policies

The details are described in lines 1-6 of Algorithm 1.

**Choice of \(\pi_{2},\ldots,\pi_{d}\).** After finding \(\pi_{1}\), we next search the remaining \(d-1\) polices \(\pi_{2},\ldots,\pi_{d}\) sequentially (lines 8-13 of Algorithm 1). For \(i=2,\ldots,d\), we find a direction \(u_{i}\) such that (i) the vector \(u_{i}\) is orthogonal to the space of current value vectors \(\operatorname*{span}(V^{\pi_{1}},\ldots,V^{\pi_{i-1}})\), and (ii) there exists a policy \(\pi_{i}\) such that \(V^{\pi_{i}}\) has a significant component in the direction of \(u_{i}\). Condition (i) is used to guarantee that the policy \(\pi_{i}\) has a value vector linearly independent of \(\operatorname*{span}(V^{\pi_{1}},\ldots,V^{\pi_{i-1}})\). Condition (ii) is used to cope with the error caused by inaccurate approximation of the ratios \(\widehat{\alpha}_{i}\). Intuitively, when \(\left\|\alpha_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\right\|_{2}\ll\epsilon\), the angle between \(\widehat{\alpha}_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\) and \(\alpha_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\) could be very large, which results in an inaccurate estimate of \(w^{*}\) in the direction of \(\alpha_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\). For example, if \(V^{\pi_{1}}=\mathbf{e}_{1}\) and \(V^{\pi_{i}}=\mathbf{e}_{1}+\frac{1}{w_{i}^{*}}\mathbf{e}_{i}\) for \(i=2,\ldots,k\), then \(\pi_{1},\pi_{i}\) are "indistinguishable" and the estimate ratio \(\widehat{\alpha}_{i-1}\) can be \(1\). Then the estimate of \(w^{*}\) by solving linear equations in Eq (1) is \((1,0,\ldots,0)\), which could be far from the true \(w^{*}\). Finding \(u_{i}\)'s in which policy values have a large component can help with this problem.

Algorithm 1 provides a more detailed description of this procedure. Note that if there are \(n\) different users, we will run Algorithm 1 at most \(k\) times instead of \(n\) times. The reason is that Algorithm1 only utilizes preference comparisons while searching for \(\pi_{1}\) (lines 1-6), and not for \(\pi_{2},\ldots,\pi_{k}\) (which contributes to the \(k^{2}\) factor in computational complexity). As there are at most \(k\) candidates for \(\pi_{1}\), namely \(\pi^{e_{1}},\ldots,\pi^{e_{k}}\), we execute lines 1-6 of Algorithm 1 for \(n\) rounds and lines 8-13 for only \(k\) rounds.

### Computation of Basis Ratios

As we have mentioned before, comparing basis policies alone does not allow for the exact computation of the \(\alpha_{i}\) ratios as comparing \(\pi_{1},\pi_{i}\) can only reveal which is better but not how much. To this end, we will use the "do nothing" policy to approximate every ratio \(\alpha_{i}\) up to some additive error \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\) using binary search over the parameter \(\widehat{\alpha}_{i}\in[0,C_{\alpha}]\) for some \(C_{\alpha}\geq 1\) (to be determined later) and comparison queries of policy \(\pi_{i+1}\) with policy \(\widehat{\alpha}_{i}\pi_{1}+(1-\widehat{\alpha}_{i})\pi_{0}\) if \(\widehat{\alpha}_{i}\leq 1\) (or comparing \(\pi_{1}\) and \(\frac{1}{\widehat{\alpha}_{i}}\pi_{i+1}+(1-\frac{1}{\widehat{\alpha}_{i}}) \pi_{0}\) instead if \(\widehat{\alpha}_{i}>1\)).7 Notice that the personalized value of \(\widehat{\alpha}_{i}\pi_{1}+(1-\widehat{\alpha}_{i})\pi_{0}\) is identical to the personalized value of \(\pi_{1}\) multiplied by \(\widehat{\alpha}_{i}\). We stop once \(\widehat{\alpha}_{i}\) is such that the user returns "indistinguishable". Once we stop, the two policies have roughly the same personalized value,

Footnote 7: We write \(\widehat{\alpha}_{i}\pi_{1}+(1-\widehat{\alpha}_{i})\pi_{0}\) to indicate that \(\pi_{1}\) is used with probability \(\widehat{\alpha}_{i}\), and that \(\pi_{0}\) is used with probability \(1-\widehat{\alpha}_{i}\).

\[\text{if }\widehat{\alpha}_{i}\leq 1,\left|\widehat{\alpha}_{i}\left\langle w^{ *},V^{\pi_{1}}\right\rangle-\left\langle w^{*},V^{\pi_{i+1}}\right\rangle\right| \leq\epsilon\,;\text{ if }\widehat{\alpha}_{i}>1,\left|\left\langle w^{*},V^{\pi_{1}} \right\rangle-\frac{1}{\widehat{\alpha}_{i}}\left\langle w^{*},V^{\pi_{i+1}} \right\rangle\right|\leq\epsilon.\] (3)

Eq (1) combined with the above inequality implies that \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\left\langle w^{*},V^{\pi_{1}} \right\rangle\leq C_{\alpha}\epsilon\). Thus, the approximation error of each ratio is bounded by \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\frac{C_{\alpha}\epsilon}{ \left\langle w^{*},V^{\pi_{1}}\right\rangle}\). To make sure the procedure will terminate, we need to set \(C_{\alpha}\geq\frac{v^{*}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}\) since \(\alpha_{i}\)'s must lie in the interval \([0,\frac{v^{*}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}]\). Upon stopping binary search once Eq (3) holds, it takes at most \(\mathcal{O}(d\log(C_{\alpha}\left\langle w^{*},V^{\pi_{1}}\right\rangle/ \epsilon))\) comparison queries to estimate all the \(\alpha_{i}\)'s.

Due to the carefully picked \(\pi_{1}\) in Algorithm 1, we can upper bound \(\frac{v^{*}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}\) by \(2k\) and derive an upper bound for \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\) by selecting \(C_{\alpha}=2k\).

**Lemma 1**.: _When \(\epsilon\leq\frac{v^{*}}{2k}\), we have \(\frac{v^{*}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}\leq 2k\), and \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\frac{4k^{2}\epsilon}{v^{*}}\) for every \(i\in[d]\)._

In what follows we set \(C_{\alpha}=2k\). The pseudo code of the above process of estimating \(\alpha_{i}\)'s is deferred to Algorithm 3 in Appendix C.

### Preference Approximation and Personalized Policy

We move on to present an algorithm that estimates \(w^{*}\) and calculates a nearly optimal personalized policy. Given the \(\pi_{i}\)'s returned by Algorithm 1 and the \(\widehat{\alpha}_{i}\)'s returned by Algorithm 3, consider a matrix \(\widehat{A}\in\mathbb{R}^{d\times k}\) with 1st row \(V^{\pi_{1}}{}^{\top}\) and the \(i\)th row \((\widehat{\alpha}_{i-1}V^{\pi_{1}}-V^{\pi_{i}})^{\top}\) for every \(i=2,\ldots,d\). Let \(\widehat{w}\) be a solution to \(\widehat{A}x=\mathbf{e}_{1}\). We will show that \(\widehat{w}\) is a good estimate of \(w^{\prime}:=\frac{w^{\prime}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}\) and that \(\pi^{\widehat{w}}\) is a nearly optimal personalized policy. In particular, when \(\epsilon\) is small, we have \(\left|\left\langle\widehat{w},V^{\pi}\right\rangle-\left\langle w^{\prime},V^ {\pi}\right\rangle\right|=\mathcal{O}(\epsilon^{\frac{1}{3}})\) for every policy \(\pi\). Putting this together, we derive the following theorem.

**Theorem 1**.: _Consider the algorithm of computing \(\widehat{A}\) and any solution \(\widehat{w}\) to \(\widehat{A}x=\mathbf{e}_{1}\) and outputting the policy \(\pi^{\widehat{w}}=\arg\max_{\pi\in\Pi}\left\langle\widehat{w},V^{\pi}\right\rangle\), which is the optimal personalized policy for preference vector \(\widehat{w}\). Then the output policy \(\pi^{\widehat{w}}\) satisfying that \(v^{*}-\left\langle w^{*},V^{\pi^{\widehat{w}}}\right\rangle\leq\mathcal{O} \left(\left(\sqrt{k}+1\right)^{d+\frac{14}{3}}\epsilon^{\frac{1}{3}}\right)\) using \(\mathcal{O}(k\log(k/\epsilon))\) comparison queries._

**Computation Complexity** We remark that Algorithm 1 solves Eq (2) for the optimal policy in scalar reward MDP at most \(\mathcal{O}(k^{2})\) times. Using, e.g., Finite Horizon Value iteration to solve for the optimal policy takes \(\mathcal{O}(H|\mathcal{S}|^{2}|\mathcal{A}|)\) steps. However, while the time complexity it takes to return the optimal policy for a single user is \(\mathcal{O}(k^{2}H|\mathcal{S}|^{2}|\mathcal{A}|+k\log(\frac{k}{\epsilon}))\), considering \(n\) different users rather than one results in overall time complexity of \(\mathcal{O}((k^{3}+n)H|\mathcal{S}|^{2}|\mathcal{A}|+nk\log(\frac{k}{\epsilon}))\).

**Proof Technique.** The standard technique typically starts by deriving an upper bound on \(\left\|\widehat{w}-w^{*}\right\|_{2}\) and then uses this bound to upper bound \(\sup_{\pi}\left|\left\langle\widehat{w},V^{\pi}\right\rangle-\left\langle w^{*},V^{\pi}\right\rangle\right|\) as \(C_{V}\left\|\widehat{w}-w^{*}\right\|_{2}\). However,this method fails to achieve a non-vacuous bound in case there are two basis policies that are close to each other. For instance, consider the returned basis policy values: \(V^{\pi_{1}}=(1,0,0)\), \(V^{\pi_{2}}=(1,1,0)\), and \(V^{\pi_{3}}=(1,0,\eta)\) for some \(\eta>0\). When \(\eta\) is extremely small, the estimate \(\widehat{w}\) becomes highly inaccurate in the direction of \((0,0,1)\), leading to a large \(\left\|\widehat{w}-w^{*}\right\|_{2}\). Even in such cases, we can still obtain a non-vacuous guarantee since the selection of \(V^{\pi_{3}}\) (line 11 of Algorithm 1) implies that no policy in \(\Pi\) exhibits a larger component in the direction of \((0,0,1)\) than \(\pi_{3}\).

**Proof Sketch.** The analysis of Theorem 1 has two parts. First, as mentioned in Sec 3.1, when \(\left\|\alpha_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\right\|_{2}\ll\epsilon\), the error of \(\widehat{\alpha}_{i+1}\) can lead to inaccurate estimate of \(w^{*}\) in direction \(\alpha_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\). Thus, we consider another estimate of \(w^{*}\) based only on some \(\pi_{i+1}\)'s with a relatively large \(\left\|\alpha_{i}V^{\pi_{1}}-V^{\pi_{i+1}}\right\|_{2}\). In particular, for any \(\delta>0\), let \(d_{\delta}:=\min_{i\geq 2:\max\left(\left|v^{\pi_{i}}\right|,\left|v^{-\pi_{i}} \right|\right)\leq\delta}i-1\). That is to say, for \(i=2,\ldots,d_{\delta}\), the policy \(\pi_{i}\) satisfies \(\left\langle u_{i},V^{\pi_{i}}\right\rangle>\delta\) and for any policy \(\pi\), we have \(\left\langle u_{d_{\delta}+1},V^{\pi}\right\rangle\leq\delta\). Then, for any policy \(\pi\) and any unit vector \(\xi\in\mathrm{span}(V^{\pi_{1}},\ldots,V^{\pi_{d_{\delta}}})^{\perp}\), we have \(\left\langle\xi,V^{\pi}\right\rangle\leq\sqrt{k}\delta\). This is because at round \(d_{\delta}+1\), we pick an orthonormal basis \(\rho_{1},\ldots,\rho_{k-d_{\delta}}\) of \(\mathrm{span}(V^{\pi_{1}},\ldots,V^{\pi_{d_{\delta}}})^{\perp}\) (line 8 in Algorithm 1) and pick \(u_{d_{\delta}+1}\) to be the one in which there exists a policy with the largest component as described in line 9. Hence, \(\left|\left\langle\rho_{j},V^{\pi}\right\rangle\right|\leq\delta\) for all \(j\in[k-d_{\delta}]\). Then, we have \(\left\langle\xi,V^{\pi}\right\rangle=\sum_{j=1}^{k-d_{\delta}}\left\langle\xi,\rho_{j}\right\rangle\left\langle\rho_{j},V^{\pi}\right\rangle\leq\sqrt{k}\delta\) by Cauchy-Schwarz inequality. Let \(\widehat{A}^{(\delta)}\in\mathbb{R}^{d_{\delta}\times k}\) be the sub-matrix comprised of the first \(d_{\delta}\) rows of \(\widehat{A}\). Then we consider an alternative estimate \(\widehat{w}^{(\delta)}=\arg\min_{x:\widehat{A}^{(\delta)}x=\mathbf{e}_{1}} \left\|x\right\|_{2}\), the minimum norm solution of \(x\) to \(\widehat{A}^{(\delta)}x=\mathbf{e}_{1}\). We upper bound in Lemma 2 and \(\sup_{\pi}\left|\left\langle\widehat{w},V^{\pi}\right\rangle-\left\langle \widehat{w}^{(\delta)},V^{\pi}\right\rangle\right|\) in Lemma 3. Then we are done with the proof of Theorem 1.

**Lemma 2**.: _If \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every \(\delta\geq 4C_{\alpha}^{\frac{2}{4}}C_{V}d^{\frac{1}{3}}\epsilon_{\alpha}^{ \frac{1}{3}}\), we have \(\left|\left\langle\widehat{w}^{(\delta)},V^{\pi}\right\rangle-\left\langle w^{ \prime},V^{\pi}\right\rangle\right|\leq\mathcal{O}(\frac{C_{\alpha}C_{\alpha }^{4}d_{\delta}^{2}}{\left\|w^{\prime}\right\|_{2}^{2}\epsilon_{\alpha}}+\sqrt{ k}\delta\left\|w^{\prime}\right\|_{2})\) for all \(\pi\), where \(w^{\prime}=\frac{w^{*}}{\left\langle w^{\prime},V^{\pi_{1}}\right\rangle}\)._

Since we only remove the rows in \(\widehat{A}\) corresponding to \(u_{i}\)'s in the subspace where no policy's value has a large component, \(\widehat{w}\) and \(\widehat{w}^{(\delta)}\) are close in terms of \(\sup_{\pi}\left|\left\langle\widehat{w},V^{\pi}\right\rangle-\left\langle \widehat{w}^{(\delta)},V^{\pi}\right\rangle\right|\).

**Lemma 3**.: _If \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every policy \(\pi\) and every \(\delta\geq 4C_{\alpha}^{\frac{2}{4}}C_{V}d^{\frac{1}{3}}\epsilon_{\alpha}^{ \frac{1}{3}}\), we have \(\left|\widehat{w}\cdot V^{\pi}-\widehat{w}^{(\delta)}\cdot V^{\pi}\right| \leq\mathcal{O}((\sqrt{k}+1)^{d-d_{\delta}}C_{\alpha}\epsilon^{(\delta)})\,,\) where \(\epsilon^{(\delta)}=\frac{C_{\alpha}C_{\alpha}^{4}d_{\delta}^{2}\left\|w^{ \prime}\right\|_{2}^{2}\epsilon_{\alpha}}{\delta^{2}}+\sqrt{k}\delta\left\|w^{ \prime}\right\|_{2}\) is the upper bound in Lemma 2._

Note that the result in Theorem 1 has a factor of \(k^{\frac{\delta}{2}}\), which is exponential in \(d\). Usually, we consider the case where \(k=\mathcal{O}(1)\) is small and thus \(k^{d}=\mathcal{O}(1)\) is small. We get rid of the exponential dependence on \(d\) by applying \(\widehat{w}^{(\delta)}\) to estimate \(w^{*}\) directly, which requires us to set the value of \(\delta\) beforehand. The following theorem follows directly by assigning the optimal value for \(\delta\) in Lemma 2.

**Theorem 2**.: _Consider the algorithm of computing \(\widehat{A}\) and any solution \(\widehat{w}^{(\delta)}\) to \(\widehat{A}^{(\delta)}x=\mathbf{e}_{1}\) for \(\delta=k^{\frac{5}{6}}\epsilon^{\frac{1}{3}}\) and outputting the policy \(\pi^{\widehat{w}^{(\delta)}}=\arg\max_{\pi\in\Pi}\left\langle\widehat{w}^{(\delta) },V^{\pi}\right\rangle\). Then the policy \(\pi^{\widehat{w}^{(\delta)}}\) satisfies that \(v^{*}-\left\langle w^{*},V^{\pi^{\widehat{w}^{(\delta)}}}\right\rangle\leq \mathcal{O}\left(k^{\frac{13}{6}}\epsilon^{\frac{1}{3}}\right)\,.\)_

Notice that the algorithm in Theorem 2 needs to set the hyperparameter \(\delta\) beforehand while we don't have to set any hyperparameter in Theorem 1. The improper value of \(\delta\) could degrade the performance of the algorithm. But we can approximately estimate \(\epsilon\) by binary searching \(\eta\in[0,1]\) and comparing \(\pi_{1}\) against the scaled version of itself \((1-\eta)\pi_{1}\) until we find an \(\eta\) such that the user cannot distinguish between \(\pi_{1}\) and \((1-\eta)\pi_{1}\). Then we can obtain an estimate of \(\epsilon\) and use the estimate to set the hyperparameter.

We remark that though we think of \(k\) as a small number, it is unclear whether the dependency on \(\epsilon\) in Theorems 1 and 2 is optimal. The tight dependency on \(\epsilon\) is left as an open problem. We briefly discuss a potential direction to improve this bound in Appendix I.

## 4 Learning from Weighted Trajectory Set Representation

In the last section, we represented policies using their explicit form as state-action mappings. However, such a representation could be challenging for users to interpret. For example, how safe is a car described by a list of \(\left|\mathcal{S}\right|\) states and actions such as "turning left"? In this section, we design algorithms that return a more interpretable policy representation--a weighted trajectory set.

Recall the definition in Section 2, a weighted trajectory set is a small set of trajectories from the support of the policy and corresponding weights, with the property that the expected return of the trajectories in the set (according to the weights) is **exactly** the value of the policy (henceforth, the _exact value property_).8As these sets preserve all the information regarding the multi-objective values of policies, they can be used as policy representations in policy comparison queries of Algorithm 3 without compromising on feedback quality. Thus, using these representations obtain the same optimality guarantees regarding the returned policy in Section 3 (but would require extra computation time to calculate the sets).

Footnote 8: Without asking for the exact value property, one could simply return a sample of \(\mathcal{O}(\log k/(\epsilon^{\prime})^{2})\) trajectories from the policy and uniform weights. With high probability, the expected return of every objective is \(\epsilon^{\prime}\)-far from its expected value. The problem is that this set does not necessarily capture rare events. For example, if the probability of a crash for any car is between \((\epsilon^{\prime})^{4}\) and \((\epsilon^{\prime})^{2}\), depending on the policy, users that only care about safety (i.e., no crashes) are unlikely to observe any “unsafe” trajectories at all, in which case we would miss valuable feedback.

There are two key observations on which the algorithms in this section are based:

(1) Each policy \(\pi\) induces a distribution over trajectories. Let \(q^{\pi}(\tau)\) denote the probability that a trajectory \(\tau\) is sampled when selecting actions according to \(\pi\). The expected return of all trajectories under \(q^{\pi}\) is identical to the value of the policy, i.e., \(V^{\pi}=\sum_{\tau}q^{\pi}(\tau)\Phi(\tau)\). In particular, the value of a policy is a convex combination of the returns of the trajectories in its support. However, we avoid using this convex combination to represent a policy since the number of trajectories in the support of a policy could be exponential in the number of states and actions.

(2) The existence of a small weighted trajectory set is implied by Caratheodory's theorem. Namely, since the value of a policy is in particular a convex combination of the returns of the trajectories in its support, Caratheodory's theorem implies that there exist \(k+1\) trajectories in the support of the policy and weights for them such that a convex combination of their returns is the value of the policy. Such a \((k+1)\)-sized set will be the output of our algorithms.

We can apply the idea behind Caratheodory's theorem proof to compress trajectories as follows. For any \((k+2)\)-sized set of \(k\)-dimensional vectors \(\{\mu_{1},\ldots,\mu_{k+2}\}\), for any convex combination of them \(\mu=\sum_{i=1}^{k+2}p_{i}\mu_{i}\), we can always find a \((k+1)\)-sized subset such that \(\mu\) can be represented as the convex combination of the subset by solving a linear equation. Given an input of a probability distribution \(p\) over a set of \(k\)-dimensional vectors, \(M\), we pick \(k+2\) vectors from \(M\), reduce at least one of them through the above procedure. We repeat this step until we are left with at most \(k+1\) vectors. We refer to this algorithm as C4 (Compress Convex Combination using Caratheodory's theorem). The pseudocode is described in Algorithm 4, which is deferred to Appendix J due to space limit. The construction of the algorithm implies the following lemma immediately.

**Lemma 4**.: _Given a set of \(k\)-dimensional vectors \(M\subset\mathbb{R}^{k}\) and a distribution \(p\) over \(M\), \(\text{C4}(M,p)\) outputs \(M^{\prime}\subset M\) with \(\left|M^{\prime}\right|\leq k+1\) and a distribution \(q\in\operatorname{Simplex}^{M^{\prime}}\) satisfying that \(\mathbb{E}_{\mu\sim q}\left[\mu\right]=\mathbb{E}_{\mu\sim p}\left[\mu\right]\) in time \(\mathcal{O}(\left|M\right|k^{3})\)._

So now we know how to compress a set of trajectories to the desired size. The main challenge is how to do it **efficiently** (in time \(\mathcal{O}(\operatorname{poly}(H\left|S\right|\left|\mathcal{A}\right|))\)). Namely, since the set of all trajectory returns from the support of the policy could be of size \(\Omega(\left|\mathcal{S}\right|^{H})\), using it as input to C4 Algorithm is inefficient. Instead, we will only use C4 as a subroutine when the number of trajectories is small.

We propose two efficient approaches for finding weighted trajectory representations. Both approaches take advantage of the property that all trajectories are generated from the same policy on the same MDP. First, we start with a small set of trajectories of length of \(1\), expand them, compress them, and repeat until we get the set of trajectory lengths of \(H\). The other is based on the construction of a layer graph where a policy corresponds to a flow in this graph and we show that finding representative trajectories is equivalent to flow decomposition.

In the next subsection, we will describe the expanding and compressing approach and defer the flow decomposition based approach to Appendix K due to space considerations. We remark that the flow decomposition approach has a running time of \(\mathcal{O}(H^{2}\left|\mathcal{S}\right|^{2}+k^{3}H\left|\mathcal{S}\right| ^{2})\) (see appendix for details), which underperforms the expanding and compressing approach (see Theorem 3) whenever \(|\mathcal{S}|H+|\mathcal{S}|k^{3}=\omega(k^{4}+k|\mathcal{S}|)\). For presentation purposes, in the following, we only consider the deterministic policies. Our techniques can be easily extended to random policies.9

Footnote 9: In Section 3, we consider a special type of random policy that is a mixed strategy of a deterministic policy (the output from an algorithm that solves for the optimal policy for an MDP with scalar reward) with the “do nothing” policy. For this specific random policy, we can find weighted trajectory representations for both policies and then apply Algorithm 4 to compress the representation.

**Expanding and Compressing Approach.**

The basic idea is to find \(k+1\) trajectories of length \(1\) to represent \(V^{\pi}\) first and then increase the length of the trajectories without increasing the number of trajectories. For policy \(\pi\), let \(V^{\pi}(s,h)=\mathbb{E}_{S_{0}=s}\left[\sum_{t=0}^{h-1}R(S_{t},\pi(S_{t}))\right]\) be the value of \(\pi\) with initial state \(S_{0}=s\) and time horizon \(h\). Since we study the representation for a fixed policy \(\pi\) in this section, we slightly abuse the notation and represent a trajectory by \(\tau^{\pi}=(s,s_{1},\ldots,s_{H})\). We denote the state of trajectory \(\tau\) at time \(t\) as \(s_{t}^{\tau}=s_{t}\). For a trajectory prefix \(\tau=(s,s_{1},\ldots,s_{h})\) of \(\pi\) with initial state \(s\) and \(h\leq H\) subsequent states, the return of \(\tau\) is \(\Phi(\tau)=R(s,\pi(s))+\sum_{t=1}^{h-1}R(s_{t},\pi(s_{t}))\). Let \(J(\tau)\) be the expected return of trajectories (of length \(H\)) with the prefix being \(\tau\), i.e.,

\[J(\tau):=\Phi(\tau)+V(s_{h}^{\tau},H-h)\,.\]

For any \(s\in\mathcal{S}\), let \(\tau\circ s\) denote the trajectory of appending \(s\) to \(\tau\). We can solve \(V^{\pi}(s,h)\) for all \(s\in\mathcal{S},h\in[H]\) by dynamic programming in time \(\mathcal{O}(kH\left|\mathcal{S}\right|^{2})\). Specifically, according to definition, we have \(V^{\pi}(s,1)=R(s,\pi(s))\) and

\[V^{\pi}(s,h+1)=R(s,\pi(s))+\sum_{s^{\prime}\in\mathcal{S}}P(s^{ \prime}|s,\pi(s))V^{\pi}(s^{\prime},h)\,.\] (4)

Thus, we can represent \(V^{\pi}\) by

\[V^{\pi}= R(s_{0},\pi(s_{0}))+\sum_{s\in\mathcal{S}}P(s|s_{0},\pi(s_{0}))V^{ \pi}(s,H-1)=\sum_{s\in\mathcal{S}}P(s|s_{0},\pi(s_{0}))J(s_{0},s)\,.\]

By applying C4, we can find a set of representative trajectories of length \(1\), \(F^{(1)}\subset\{(s_{0},s)|s\in\mathcal{S}\}\), with \(\left|F^{(1)}\right|\leq k+1\) and weights \(\beta^{(1)}\in\mathrm{Simplex}^{F^{(1)}}\) such that

\[V^{\pi}=\sum_{\tau\in F^{(1)}}\beta^{(1)}(\tau)J(\tau)\,.\] (5)

Supposing that we are given a set of trajectories \(F^{(t)}\) of length \(t\) with weights \(\beta^{(t)}\) such that \(V^{\pi}=\sum_{\tau\in F^{(t)}}\beta^{(t)}(\tau)J(\tau)\), we can first increase the length of trajectories by \(1\) through Eq (4) and obtain a subset of \(\{\tau\circ s|\tau\in F^{(t)},s\in\mathcal{S}\}\), in which the trajectories are of length \(t+1\). Specifically, we have

\[V^{\pi}=\sum_{\tau\in F^{(t)},s\in\mathcal{S}}\beta^{(t)}(\tau)P(s|s_{t}^{\tau },\pi(s_{t}^{\tau}))J(\tau\circ s)\,.\] (6)

Then we would like to compress the above convex combination through C4 as we want to keep track of at most \(k+1\) trajectories of length \(t+1\) due to the computing time. More formally, let \(J_{F^{(t)}}:=\{J(\tau\circ s)|\tau\in F^{(t)},s\in\mathcal{S}\}\) be the set of expected returns and \(p_{F^{(t)},\beta^{(t)}}\in\mathrm{Simplex}^{F^{(t)}\times\mathcal{S}}\) with \(p_{F^{(t)},\beta^{(t)}}(\tau\circ s)=\beta^{(t)}(\tau)P(s|s_{t}^{\tau},\pi(s_{t }^{\tau}))\) be the weights appearing in Eq (6). Here \(p_{F^{(t)},\beta^{(t)}}\) defines a distribution over \(J_{F^{(t)}}\) with the probability of drawing \(J(\tau\circ s)\) being \(p_{F^{(t)},\beta^{(t)}}(\tau\circ s)\). Then we can apply C4 over \((J_{F^{(t)}},p_{F^{(t)},\beta^{(t)}})\) and compress the representative trajectories \(\{\tau\circ s|\tau\in F^{(t)},s\in\mathcal{S}\}\). We start with trajectories of length \(1\) and repeat the process of expanding and compressing until we get trajectories of length \(H\). The details are described in Algorithm 2.

```
1: compute \(V^{\pi}(s,h)\) for all \(s\in\mathcal{S},h\in[H]\) by dynamic programming according to Eq (4)
2:\(F^{(0)}=\{(s_{0})\}\) and \(\beta^{(0)}(s_{0})=1\)
3:for\(t=0,\ldots,H-1\)do
4:\(J_{F^{(t)}}\leftarrow\{J(\tau\circ s)|\tau\in F^{(t)},s\in\mathcal{S}\}\) and \(p_{F^{(t)},\beta^{(t)}}(\tau\circ s)\leftarrow\beta^{(t)}(\tau)P(s|s_{t}^{ \tau},\pi(s_{t}^{\tau}))\) for \(\tau\in F^{(t)},s\in\mathcal{S}\) // expanding step
5:\((J^{(t+1)},\beta^{(t+1)})\leftarrow\text{C}\mathcal{A}(J_{F^{(t)}},p_{F^{(t)}, \beta^{(t)}})\) and \(F^{(t+1)}\leftarrow\{\tau|J(\tau)\in J^{(t+1)}\}\) // compressing step
6:endfor
7: output \(F^{(H)}\) and \(\beta^{(H)}\) ```

**Algorithm 2** Expanding and compressing trajectories

**Theorem 3**.: _Algorithm 2 outputs \(F^{(H)}\) and \(\beta^{(H)}\) satisfying that \(\left|F^{(H)}\right|\leq k+1\) and \(\sum_{\tau\in F^{(H)}}\beta^{(H)}(\tau)\Phi(\tau)=V^{\pi}\) in time \(\mathcal{O}(k^{4}H\left|\mathcal{S}\right|+kH\left|\mathcal{S}\right|^{2})\)._

The proof of Theorem 3 follows immediately from the construction of the algorithm. According to Eq (5), we have \(V^{\pi}=\sum_{\tau\in F^{(t)}}\beta^{(1)}(\tau)J(\tau)\). Then we can show that the output of Algorithm 2 is a valid weighted trajectory set by induction on the length of representative trajectories. C4 guarantees that \(\left|F^{(t)}\right|\leq k+1\) for all \(t=1,\ldots,H\), and thus, we only keep track of at most \(k+1\) trajectories at each step and achieve the computation guarantee in the theorem. Combined with Theorem 1, we derive the following Corollary.

**Corollary 1**.: _Running the algorithm in Theorem 1 with weighted trajectory set representation returned by Algorithm 2 gives us the same guarantee as that of Theorem 1 in time \(\mathcal{O}(k^{2}H|\mathcal{S}|^{2}|\mathcal{A}|+(k^{5}H\left|\mathcal{S} \right|+k^{2}H\left|\mathcal{S}\right|^{2})\log(\frac{k}{\epsilon}))\)._

## 5 Discussion

In this paper, we designed efficient algorithms for learning users' preferences over multiple objectives from comparative feedback. The efficiency is expressed in both the running time and number of queries (both polynomial in \(H,\left|\mathcal{S}\right|,\left|\mathcal{A}\right|,k\) and logarithmic in \(1/\epsilon\)). The learned preferences of a user can then be used to reduce the problem of finding a personalized optimal policy for this user to a (finite horizon) single scalar reward MDP, a problem with a known efficient solution. As we have focused on minimizing the policy comparison queries, our algorithms are based on polynomial time pre-processing calculations that save valuable comparison time for users.

The results in Section 3 are of independent interest and can be applied to a more general learning setting, where for some unknown linear parameter \(w^{*}\), given a set of points \(X\) and access to comparison queries of any two points, the goal is to learn \(\arg\max_{x\in X}\left\langle w^{*},x\right\rangle\). E.g., in personalized recommendations for coffee beans in terms of the coffee profile described by the coffee suppliers (body, aroma, crema, roast level,...), while users could fail to describe their optimal coffee beans profile, adopting the methodology in Section 3 can retrieve the ideal coffee beans for a user using comparisons (where the mixing with "do nothing" is done by diluting the coffee with water and the optimal coffee for a given profile is the one closest to it).

When moving from the explicit representation of policies as mappings from states to actions to a more natural policy representation as a weighted trajectory set, we then obtained the same optimality guarantees in terms of the number of queries. While there could be other forms of policy representations (e.g., a small subset of common states), one advantage of our weighted trajectory set representation is that it captures the essence of the policy multi-objective value in a clear manner via \(\mathcal{O}(k)\) trajectories and weights. The algorithms provided in Section 4 are standalone and could also be of independent interest for explainable RL (Alharin et al., 2020). For example, to exemplify the multi-objective performance of generic robotic vacuum cleaners (this is beneficial if we only have e.g., \(3\) of them--we can apply the algorithms in Section 4 to generate weighted trajectory set representations and compare them directly without going through the algorithm in Section 3.).

An interesting direction for future work is to relax the assumption that the MDP is known in advance. One direct way is to first learn the model (in model-based RL), then apply our algorithms in the learned MDP. The sub-optimality of the returned policy will then depend on both the estimation error of the model and the error introduced by our algorithms (which depends on the parameters in the learned model).

## Acknowledgements

This work was supported in part by the National Science Foundation under grants CCF-2212968 and ECCS-2216899 and by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Approved for public release; distribution is unlimited.

This project was supported in part by funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement number 882396), by the Israel Science Foundation (grant number 993/17), Tel Aviv University Center for AI and Data Science (TAD), the Eric and Wendy Schmidt Fund, and the Yandex Initiative for Machine Learning at Tel Aviv University.

We would like to thank all anonymous reviewers, especially Reviewer Gnoo, for their constructive comments.

## References

* Ailon (2012) Ailon, N. (2012). An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity. _Journal of Machine Learning Research_, 13(1):137-164.
* Ailon et al. (2014) Ailon, N., Karin, Z. S., and Joachims, T. (2014). Reducing dueling bandits to cardinal bandits. In _ICML_, volume 32, pages 856-864.
* Akrour et al. (2012) Akrour, R., Schoenauer, M., and Sebag, M. (2012). APRIL: Active preference learning-based reinforcement learning. In _Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 116-131.
* Alharin et al. (2020) Alharin, A., Doan, T.-N., and Sartipi, M. (2020). Reinforcement learning interpretation methods: A survey. _IEEE Access_, 8:171058-171077.
* Balcan et al. (2015) Balcan, M., Blum, A., and Vempala, S. S. (2015). Efficient representations for lifelong learning and autoencoding. In _Proceedings of the Annual Conference on Learning Theory (COLT)_, pages 191-210.
* Balcan et al. (2016) Balcan, M.-F., Vitercik, E., and White, C. (2016). Learning combinatorial functions from pairwise comparisons. In _Proceedings of the Annual Conference on Learning Theory (COLT)_.
* Barrett and Narayanan (2008) Barrett, L. and Narayanan, S. (2008). Learning all optimal policies with multiple criteria. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 41-47.
* Benggs et al. (2021) Benggs, V., Busa-Fekete, R., El Mesaoudi-Paul, A., and Hullermeier, E. (2021). Preference-based online learning with dueling bandits: A survey. _J. Mach. Learn. Res._
* Bhatia et al. (2020) Bhatia, K., Pananjady, A., Bartlett, P., Dragan, A., and Wainwright, M. J. (2020). Preference learning along multiple criteria: A game-theoretic perspective. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 7413-7424.
* Biyik and Sadigh (2018) Biyik, E. and Sadigh, D. (2018). Batch active preference-based learning of reward functions. In _Proceedings of the Conference on Robot Learning (CoRL)_, pages 519-528.
* Blum and Shao (2020) Blum, A. and Shao, H. (2020). Online learning with primary and secondary losses. _Advances in Neural Information Processing Systems_, 33:20427-20436.
* Chatterjee (2007) Chatterjee, K. (2007). Markov decision processes with multiple long-run average objectives. In _Proceedings of the International Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS)_, pages 473-484.
* Chatterjee et al. (2006) Chatterjee, K., Majumdar, R., and Henzinger, T. A. (2006). Markov decision processes with multiple objectives. In _Proceedings of the Annual Symposium on Theoretical Aspects of Computer Science (STACS)_, pages 325-336.
* Chatterjee et al. (2016)Chen, X., Ghadirzadeh, A., Bjorkman, M., and Jensfelt, P. (2019). Meta-learning for multi-objective reinforcement learning. In _Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 977-983.
* Cheng et al. (2011) Cheng, W., Furnkranz, J., Hullermeier, E., and Park, S.-H. (2011). Preference-based policy iteration: Leveraging preference learning for reinforcement learning. In _Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)_, pages 312-327.
* Cheung (2019) Cheung, W. C. (2019). Regret minimization for reinforcement learning with vectorial feedback and complex objectives. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Christiano et al. (2017) Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Doumpos and Zopounidis (2007) Doumpos, M. and Zopounidis, C. (2007). Regularized estimation for preference disaggregation in multiple criteria decision making. _Computational Optimization and Applications_, 38(1):61-80.
* Furnkranz et al. (2012) Furnkranz, J., Hullermeier, E., Cheng, W., and Park, S.-H. (2012). Preference-based reinforcement learning: A formal framework and a policy iteration algorithm. _Machine Learning_, 89(1):123-156.
* Hayes et al. (2022) Hayes, C. F., Radulescu, R., Bargiacchi, E., Kallstrom, J., Macfarlane, M., Reymond, M., Verstraeten, T., Zintgraf, L. M., Dazeley, R., Heintz, F., Howley, E., Irissappane, A. A., Mannion, P., Nowe, A., de Oliveira Ramos, G., Restelli, M., Vamplew, P., and Roijers, D. M. (2022). A practical guide to multi-objective reinforcement learning and planning. _Autonomous Agents and Multi-Agent Systems_.
* Ibarz et al. (2018) Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. (2018). Reward learning from human preferences and demonstrations in Atari. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Jain et al. (2015) Jain, A., Sharma, S., Joachims, T., and Saxena, A. (2015). Learning preferences for manipulation tasks from online coactive feedback. _International Journal of Robotics Research_, 34(10):1296-1313.
* Jamieson and Nowak (2011) Jamieson, K. G. and Nowak, R. (2011). Active ranking using pairwise comparisons. _Advances in neural information processing systems_, 24.
* Kane et al. (2017) Kane, D. M., Lovett, S., Moran, S., and Zhang, J. (2017). Active classification with comparison queries. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 355-366. IEEE.
* Knox et al. (2022) Knox, W. B., Hatgis-Kessell, S., Booth, S., Niekum, S., Stone, P., and Allievi, A. (2022). Models of human preference for learning reward functions. _arXiv preprint arXiv:2206.02231_.
* Lee et al. (2021) Lee, K., Smith, L., and Abbeel, P. (2021). PEBBLE: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. _arXiv preprint arXiv:2106.05091_.
* Mannor et al. (2014) Mannor, S., Perchet, V., and Stoltz, G. (2014). Approachability in unknown games: Online learning meets multi-objective optimization. In _Conference on Learning Theory_, pages 339-355. PMLR.
* Mannor and Shimkin (2001) Mannor, S. and Shimkin, N. (2001). The steering approach for multi-criteria reinforcement learning. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Mannor and Shimkin (2004) Mannor, S. and Shimkin, N. (2004). A geometric approach to multi-criterion reinforcement learning. _Journal of Machine Learning Research_, 5:325-360.
* Marinescu et al. (2017) Marinescu, R., Razak, A., and Wilson, N. (2017). Multi-objective influence diagrams with possibly optimal policies. _Proceedings of the AAAI Conference on Artificial Intelligence_.
* McAuley (2022) McAuley, J. (2022). _Personalized Machine Learning_. Cambridge University Press. in press.
* Pacchiano et al. (2022) Pacchiano, A., Saha, A., and Lee, J. (2022). Dueling RL: reinforcement learning with trajectory preferences.
* Riedmuller et al. (2017)Ren, W., Liu, J., and Shroff, N. B. (2018). PAC ranking from pairwise and listwise queries: Lower bounds and upper bounds. _arXiv preprint arXiv:1806.02970_.
* Roijers et al. (2013) Roijers, D. M., Vamplew, P., Whiteson, S., and Dazeley, R. (2013). A survey of multi-objective sequential decision-making. _Journal of Artificial Intelligence Research_, 48:67-113.
* Rothkopf and Dimitrakakis (2011) Rothkopf, C. A. and Dimitrakakis, C. (2011). Preference elicitation and inverse reinforcement learning. In _Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 34-48.
* Sadigh et al. (2017) Sadigh, D., Dragan, A. D., Sastry, S., and Seshia, S. A. (2017). Active preference-based learning of reward functions. In _Proceedings of Robotics: Science and Systems (RSS)_.
* Saha and Gopalan (2018) Saha, A. and Gopalan, A. (2018). Battle of bandits. In _Uncertainty in Artificial Intelligence_.
* Saha and Gopalan (2019) Saha, A. and Gopalan, A. (2019). PAC Battling Bandits in the Plackett-Luce Model. In _Algorithmic Learning Theory_, pages 700-737.
* Saha et al. (2021) Saha, A., Koren, T., and Mansour, Y. (2021). Dueling convex optimization. In _International Conference on Machine Learning_, pages 9245-9254. PMLR.
* Sui et al. (2017) Sui, Y., Zhuang, V., Burdick, J., and Yue, Y. (2017). Multi-dueling bandits with dependent arms. In _Conference on Uncertainty in Artificial Intelligence_, UAI'17.
* Sui et al. (2018) Sui, Y., Zoghi, M., Hofmann, K., and Yue, Y. (2018). Advancements in dueling bandits. In _IJCAI_, pages 5502-5510.
* Wang et al. (2022) Wang, N., Wang, H., Karimzadehan, M., Kveton, B., and Boutilier, C. (2022). Imo^ 3: Interactive multi-objective off-policy optimization. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_.
* Wilson et al. (2012) Wilson, A., Fern, A., and Tadepalli, P. (2012). A Bayesian approach for policy learning from trajectory preference queries. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Wirth et al. (2017a) Wirth, C., Akrour, R., Neumann, G., and Furnkranz, J. (2017a). A survey of preference-based reinforcement learning methods. _J. Mach. Learn. Res._
* Wirth et al. (2017b) Wirth, C., Akrour, R., Neumann, G., and Furnkranz, J. (2017b). A survey of preference-based reinforcement learning methods. _Journal of Machine Learning Research_, 18(136):1-46.
* Wirth et al. (2016) Wirth, C., Furnkranz, J., and Neumann, G. (2016). Model-free preference-based reinforcement learning. In _Proceedings of the National Conference on Artificial Intelligence (AAAI)_.
* Yona et al. (2022) Yona, G., Moran, S., Elidan, G., and Globerson, A. (2022). Active learning with label comparisons. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)_.
* Zoghi et al. (2014) Zoghi, M., Whiteson, S., Munos, R., Rijke, M. d., et al. (2014). Relative upper confidence bound for the \(k\)-armed dueling bandit problem. In _JMLR Workshop and Conference Proceedings_, number 32, pages 10-18. JMLR.

Related Work

**Multi-objective sequential decision making** There is a long history of work on multi-objective sequential decision making (Roijers et al., 2013), with one key focus being the realization of efficient algorithms for approximating the Pareto front (Chatterjee et al., 2006; Chatterjee, 2007; Marinescu et al., 2017). Instead of finding a possibly optimal policy, we concentrate on specific user preferences and find a policy that is optimal for that specific user. Just like the ideal car for one person could be a Chevrolet Spark (small) and for another, it is a Ford Ranger (a truck).

In the context of multi-objective RL (Hayes et al., 2022), the goal can be formulated as one of learning a policy for which the average return vector belongs to a target set (hence the term "multi-criteria" RL), which existing work has treated as a stochastic game (Mannor and Shimkin, 2001, 2004). Other works seek to maximize (in expectation) a scalar version of the reward that may correspond to a weighted sum of the multiple objectives (Barrett and Narayanan, 2008; Chen et al., 2019) as we consider here, or a nonlinear function of the objectives (Cheung, 2019). Multi-objective online learning has also been studied, see (Mannor et al., 2014; Blum and Shao, 2020) for example.

The parameters that define this scalarization function (e.g., the relative objective weights) are often unknown and vary with the task setting or user. In this case, preference learning (Wirth et al., 2017a) is commonly used to elicit the value of these parameters. Doumpos and Zopounidis (2007) describe an approach to eliciting a user's relative weighting in the context of multi-objective decision-making. Bhatia et al. (2020) learn preferences over multiple objectives from pairwise queries using a game-theoretic approach to identify optimal randomized policies. In the context of RL involving both scalar and vector-valued objectives, user preference queries provide an alternative to learning from demonstrations, which may be difficult for people to provide (e.g., in the case of robots with high degrees-of-freedom), or explicit reward specifications (Cheng et al., 2011; Rothkopf and Dimitrakakis, 2011; Akrour et al., 2012; Wilson et al., 2012; Furnkranz et al., 2012; Jain et al., 2015; Wirth et al., 2016; Christiano et al., 2017; Wirth et al., 2017b; Ibarz et al., 2018; Lee et al., 2021). These works typically assume that noisy human preferences over a pair of trajectories are correlated with the difference in their utilities (i.e., the reward acts as a latent term predictive of preference). Many contemporary methods estimate the latent reward by minimizing the cross-entropy loss between the reward-based predictions and the human-provided preferences (i.e., finding the reward that maximizes the likelihood of the observed preferences) (Christiano et al., 2017; Ibarz et al., 2018; Lee et al., 2021; Knox et al., 2022; Pacchiano et al., 2022).

Wilson et al. (2012) describe a Bayesian approach to policy learning whereby they query a user for their preference between a pair of trajectories and use these preferences to maintain a posterior distribution over the latent policy parameters. The task of choosing the most informative queries is challenging due to the continuous space of trajectories, and is generally NP-hard (Ailon, 2012). Instead, they assume access to a distribution over trajectories that accounts for their feasibility and relevance to the target policy, and they describe two heuristic approaches to selecting trajectory queries based on this distribution. Finally, Sadigh et al. (2017) describe an approach to active preference-based learning in continuous state and action spaces. Integral to their work is the ability to synthesize dynamically feasible trajectory queries. Biyik and Sadigh (2018) extend this approach to the batch query setting.

**Comparative feedback in other problems** Comparative feedback has been studied in other problems in learning theory, e.g., combinatorial functions (Balcan et al., 2016). One closely related problem is active ranking/learning using pairwise comparisons (Jamieson and Nowak, 2011; Kane et al., 2017; Saha et al., 2021; Yona et al., 2022). These works usually consider a given finite sample of points. Kane et al. (2017) implies a lower bound of the number of comparisons is linear in the cardinality of the set even if the points satisfy the linear structural constraint as we assume in this work. In our work, the points are value vectors generated by running different policies under the same MDP and thus have a specific structure. Besides, we allow comparison of policies not in the policy set. Thus, we are able to obtain the query complexity sublinear in the number of policies. Another related problem using comparative/preference feedback is dueling bandits that aim to learn through pairwise feedback (Ailon et al., 2014; Zoghi et al., 2014) (see also Benggs et al. (2021); Sui et al. (2018) for surveys), or more generally any subsetwise feedback (Sui et al., 2017; Saha and Gopalan, 2018, 2019; Ren et al., 2018). However, unlike dueling bandits, we consider noiseless comparative feedback.

Proof of Lemma 1

**Lemma 1**.: _When \(\epsilon\leq\frac{v^{*}}{2k}\), we have \(\frac{v^{*}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}\leq 2k\), and \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\frac{4k^{2}\epsilon}{v^{*}}\) for every \(i\in[d]\)._

Proof.: We first show that, according to our algorithm (lines 1-6 of Algorithm 1), the returned \(\pi_{1}\) satisfies that

\[\left\langle w^{*},V^{\pi_{1}}\right\rangle\geq\max_{i\in[k]} \left\langle w^{*},V^{\pi^{\mathbf{a}_{i}}}\right\rangle-\epsilon\,.\]

This can be proved by induction over \(k\). In the base case of \(k=2\), it's easy to see that the returned \(\pi_{1}\) satisfies the above inequality. Suppose the above inequality holds for any \(k\leq n-1\) and we prove that it will also hold for \(k=n\). After running the algorithm over \(j=2,\ldots,k-1\) (line \(2\)), the returned policy \(\pi_{e^{*}}\) satisfies that

\[\left\langle w^{*},V^{\pi^{\mathbf{e}^{*}}}\right\rangle\geq \max_{i\in[k-1]}\left\langle w^{*},V^{\pi^{\mathbf{a}_{i}}}\right\rangle- \epsilon\,.\]

Then there are two cases,

* If \(\left\langle w^{*},V^{\pi^{\mathbf{e}^{*}}}\right\rangle<\left\langle w^{*},V^ {\pi^{\mathbf{a}_{k}}}\right\rangle-\epsilon\), we will return \(\pi_{1}=\pi^{\mathbf{e}_{k}}\) and also, \(\left\langle w^{*},V^{\pi^{\mathbf{e}_{k}}}\right\rangle\geq\left\langle w^{*},V^{\pi^{\mathbf{a}_{i}}}\right\rangle-\epsilon\) for all \(i\in[k]\).
* If \(\left\langle w^{*},V^{\pi^{\mathbf{e}^{*}}}\right\rangle\geq\left\langle w^{*},V^{\pi^{\mathbf{a}_{k}}}\right\rangle-\epsilon\), then we will return \(\pi^{e^{*}}\) and it satisfies \(\left\langle w^{*},V^{\pi^{*^{*}}}\right\rangle\geq\max_{i\in[k]}\left\langle w ^{*},V^{\pi^{\mathbf{a}_{i}}}\right\rangle-\epsilon\,.\)

As \(\pi^{\mathbf{e}_{i}}\) is the optimal personalized policy when the user's preference vector is \(\mathbf{e}_{i}\), we have that

\[v^{*}=\left\langle w^{*},V^{\pi^{*}}\right\rangle=\sum_{i=1}^{k }w_{i}^{*}\left\langle V^{\pi^{*}},\mathbf{e}_{i}\right\rangle\leq\sum_{i=1}^ {k}w_{i}^{*}\left\langle V^{\pi^{\mathbf{e}_{i}}},\mathbf{e}_{i}\right\rangle \leq\left\langle w^{*},\sum_{i=1}^{k}V^{\pi^{\mathbf{e}_{i}}}\right\rangle\,,\]

where the last inequality holds because the entries of \(V^{\pi^{\mathbf{e}_{i}}}\) and \(w^{*}\) are non-negative. Therefore, there exists \(i\in[k]\) such that \(\left\langle w^{*},V^{\pi^{\mathbf{e}_{i}}}\right\rangle\geq\frac{1}{k}\left \langle w^{*},V^{\pi^{\mathbf{e}}}\right\rangle=\frac{1}{k}v^{*}\).

Then we have

\[\left\langle w^{*},V^{\pi_{1}}\right\rangle\geq\max_{i\in[k]} \left\langle w^{*},V^{\pi^{\mathbf{e}_{i}}}\right\rangle-\epsilon\geq\frac{1} {k}v^{*}-\epsilon\geq\frac{1}{2k}v^{*}\,,\]

when \(\epsilon\leq\frac{v^{*}}{2k}\). By rearranging terms, we have \(\frac{v^{*}}{\left\langle w^{*},V^{\pi_{1}}\right\rangle}\leq 2k\).

By setting \(C_{\alpha}=2k\), we have \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\left\langle w^{*},V^{\pi_{1}} \right\rangle\leq C_{\alpha}\epsilon=2k\epsilon\) and thus, \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\frac{4k^{2}\epsilon}{v^{*}}\). 

## Appendix C Pseudo Code of Computation of the Basis Ratios

The pseudo code of searching \(\widehat{\alpha}_{i}\)'s is described in Algorithm 3.

## Appendix D Proof of Theorem 1

**Theorem 1**.: _Consider the algorithm of computing \(\widehat{A}\) and any solution \(\widehat{w}\) to \(\widehat{A}x=\mathbf{e}_{1}\) and outputting the policy \(\pi^{\widehat{w}}=\arg\max_{\pi\in\Pi}\left\langle\widehat{w},V^{\pi}\right\rangle\), which is the optimal personalized policy for preference vector \(\widehat{w}\). Then the output policy \(\pi^{\widehat{w}}\) satisfying that \(v^{*}-\left\langle w^{*},V^{\pi^{\widehat{w}}}\right\rangle\leq\mathcal{O} \left(\left(\sqrt{k}+1\right)^{d+\frac{14}{3}}\epsilon^{\frac{1}{3}}\right)\) using \(\mathcal{O}(k\log(k/\epsilon))\) comparison queries._

Proof.: Theorem 1 follows by setting \(\epsilon_{\alpha}=\frac{4k^{2}\epsilon}{v^{*}}\) and \(C_{\alpha}=2k\) as shown in Lemma 1 and combining the results of Lemma 2 and 3 with the triangle inequality.

Specifically, for any policy \(\pi\), we have

\[\begin{split}|\langle\widehat{w},V^{\pi}\rangle-\langle w^{\prime },V^{\pi}\rangle|\leq&\Big{|}\langle\widehat{w},V^{\pi}\rangle- \left\langle\widehat{w}^{(\delta)},V^{\pi}\right\rangle\Big{|}+\Big{|}\Big{ }\langle\widehat{w}^{(\delta)},V^{\pi}\Big{\rangle}-\langle w^{\prime},V^{ \pi}\rangle\Big{|}\\ \leq&\mathcal{O}((\sqrt{k}+1)^{d-d_{\delta}}C_{ \alpha}(\frac{C_{\alpha}C_{V}^{4}d_{\delta}^{\frac{1}{2}}\left\|w^{\prime} \right\|_{2}^{2}\epsilon_{\alpha}}{\delta^{2}}+\sqrt{k}\delta\left\|w^{\prime }\right\|_{2}))\\ \leq&\mathcal{O}((\sqrt{k}+1)^{d-d_{\delta}+3}\left\| w^{\prime}\right\|_{2}(\frac{C_{V}^{4}k^{4}\left\|w^{\prime}\right\|_{2} \epsilon}{v^{*}\delta^{2}}+\delta))\,.\end{split}\] (7)

Since \(\left\|w^{\prime}\right\|=\frac{\left\|w^{*}\right\|}{\langle w^{*},V^{\pi_{1}} \rangle}\) and \(\langle w^{*},V^{\pi_{1}}\rangle\geq\frac{v^{*}}{2k}\) from Lemma 1, we derive

\[\begin{split}& v^{*}-\left\langle w^{*},V^{\pi^{\widehat{w}}} \right\rangle=\langle w^{*},V^{\pi_{1}}\rangle\left(\left\langle w^{\prime},V ^{\pi^{\widehat{w}}}\right\rangle-\left\langle w^{\prime},V^{\pi^{\widehat{w} }}\right\rangle\right)\\ \leq&\langle w^{*},V^{\pi_{1}}\rangle\left(\left\langle \widehat{w},V^{\pi^{*}}\right\rangle-\left\langle\widehat{w},V^{\pi^{\widehat{ w}}}\right\rangle+\mathcal{O}((\sqrt{k}+1)^{d-d_{\delta}+3}\left\|w^{\prime} \right\|_{2}(\frac{C_{V}^{4}k^{4}\left\|w^{\prime}\right\|_{2}\epsilon}{v^{*} \delta^{2}}+\delta))\right)\\ \leq&\mathcal{O}\left(\langle w^{*},V^{\pi_{1}} \rangle\left(\sqrt{k}+1\right)^{d-d_{\delta}+3}\left\|w^{\prime}\right\|_{2}( \frac{C_{V}^{4}k^{4}\left\|w^{\prime}\right\|_{2}\epsilon}{v^{*}\delta^{2}}+ \delta)\right)\\ =&\mathcal{O}\left((\sqrt{k}+1)^{d-d_{\delta}+3} \left\|w^{*}\right\|_{2}(\frac{C_{V}^{4}k^{5}\left\|w^{*}\right\|_{2}\epsilon }{v^{*2}\delta^{2}}+\delta)\right)\\ =&\mathcal{O}\left((\frac{C_{V}^{2}\left\|w^{*} \right\|_{2}^{2}}{v^{*}})^{\frac{2}{3}}(\sqrt{k}+1)^{d+\frac{19}{3}}\epsilon^{ \frac{1}{3}}\right)\,.\end{split}\]

The first inequality follows from

\[\begin{split}\left\langle w^{\prime},V^{\pi^{*}}\right\rangle- \left\langle w^{\prime},V^{\pi^{\widehat{w}}}\right\rangle=&\left \langle w^{\prime},V^{\pi^{*}}\right\rangle-\left\langle w^{\prime},V^{\pi^{ \widehat{w}}}\right\rangle\end{split}\]\[+\left(\left\langle\widehat{w},V^{\pi^{*}}\right\rangle-\left\langle \widehat{w},V^{\pi^{*}}\right\rangle\right)+\left(\left\langle\widehat{w},V^{\pi ^{\widehat{\omega}}}\right\rangle-\left\langle\widehat{w},V^{\pi^{\widehat{ \omega}}}\right\rangle\right),\]

and applying (7) twice- once for \(\pi^{*}\) and once for \(\pi^{\widehat{\omega}}\). The last inequality follows by setting \(\delta=\left(\frac{C_{V}^{\pm}k^{5}\left\|w^{*}\right\|_{2}\epsilon}{v^{*2}} \right)^{\frac{1}{3}}\). 

## Appendix E Proof of Lemma 2

**Lemma 2**.: _If \(|\widehat{\alpha}_{i}-\alpha_{i}|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every \(\delta\geq 4C_{\alpha}^{\frac{2}{3}}C_{V}d^{\frac{1}{3}}\epsilon_{\alpha}^{ \frac{1}{3}}\), we have \(\left|\left\langle\widehat{w}^{(\delta)},V^{\pi}\right\rangle-\left\langle w ^{\prime},V^{\pi}\right\rangle\right|\leq\mathcal{O}(\frac{C_{\alpha}C_{V}^{ \frac{3}{3}}d_{\delta}^{\frac{3}{3}}\left\|w^{\prime}\right\|_{2}^{2}\epsilon _{\alpha}}{\delta^{2}}+\sqrt{k}\delta\left\|w^{\prime}\right\|_{2})\) for all \(\pi\), where \(w^{\prime}=\frac{w^{*}}{\left\langle w^{\prime},V^{\pi_{1}}\right\rangle}\)._

To prove Lemma 2, we will first define a matrix, \(A^{\text{(full)}}\).

Given the output \((V^{\pi_{1}},\ldots,V^{\pi_{d}})\) of Algorithm 1, we have \(\operatorname{rank}(\operatorname{span}(\{V^{\pi_{1}},\ldots,V^{\pi_{d_{ \delta}}}\}))=d_{\delta}\).

Let \(b_{1},\ldots,b_{d-d_{\delta}}\) be a set of orthonormal vectors that are orthogonal to \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{d_{\delta}}})\) and together with \(V^{\pi_{1}},\ldots,V^{\pi_{d_{\delta}}}\) form a basis for \(\operatorname{span}(\{V^{\pi}|\pi\in\Pi\})\).

We define \(A^{\text{(full)}}\in\mathbb{R}^{d\times k}\) as the matrix of replacing the last \(d-d_{\delta}\) rows of \(A\) with \(b_{1},\ldots,b_{d-d_{\delta}}\), i.e.,

\[A^{\text{(full)}}=\left(\begin{array}{c}V^{\pi_{1}\top}\\ (\alpha_{1}V^{\pi_{1}}-V^{\pi_{2}})^{\top}\\ \vdots\\ (\alpha_{d_{\delta}-1}V^{\pi_{1}}-V^{\pi_{d_{\delta}}})^{\top}\\ b_{1}^{\top}\\ \vdots\\ b_{d-d_{\delta}}^{\top}\end{array}\right).\]

**Observation 1**.: _We have that \(\operatorname{span}(A^{\text{(full)}})=\operatorname{span}(\{V^{\pi}|\pi\in \Pi\})\) and \(\operatorname{rank}(A^{\text{(full)}})=d\)._

**Lemma 5**.: _For all \(w\in\mathbb{R}^{k}\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\), we have \(\left|w\cdot V^{\pi}-w^{\prime}\cdot V^{\pi}\right|\leq\sqrt{k}\delta\left\|w^ {\prime}\right\|_{2}\) for all \(\pi\)._

We then show that there exists a \(w\in\mathbb{R}^{k}\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\) such that \(\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right|\) is small for all \(\pi\in\Pi\).

**Lemma 6**.: _If \(|\widehat{\alpha}_{i}-\alpha_{i}|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every \(\delta\geq 4C_{\alpha}^{\frac{2}{3}}C_{V}d^{\frac{1}{3}}\epsilon_{\alpha}^{ \frac{1}{3}}\) there exists a \(w\in\mathbb{R}^{k}\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\) s.t. \(\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right|\leq\mathcal{O} (\frac{C_{\alpha}C_{V}^{4}d_{\delta}^{\frac{3}{3}}\left\|w^{\prime}\right\|_{2 }^{2}\epsilon_{\alpha}}{\delta^{2}})\) for all \(\pi\)._

We now derive Lemma 2 using the above two lemmas.

Proof of Lemma 2.: Let \(w\) be defined in Lemma 6. Then for any policy \(\pi\), we have

\[\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w^{\prime}\cdot V^{\pi}\right| \leq\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi} \right|+\left|w\cdot V^{\pi}-w^{\prime}\cdot V^{\pi}\right|\] \[\leq \mathcal{O}(\frac{C_{\alpha}C_{V}^{4}d_{\delta}^{\frac{3}{3}} \left\|w^{\prime}\right\|_{2}^{2}\epsilon_{\alpha}}{\delta^{2}}+\sqrt{k}\delta \left\|w^{\prime}\right\|_{2})\,,\]

by applying Lemma 5 and 6. 

## Appendix F Proofs of Lemma 5 and Lemma 6

**Lemma 5**.: _For all \(w\in\mathbb{R}^{k}\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\), we have \(\left|w\cdot V^{\pi}-w^{\prime}\cdot V^{\pi}\right|\leq\sqrt{k}\delta\left\|w^ {\prime}\right\|_{2}\) for all \(\pi\)._Proof of Lemma 5.: Since \(\operatorname{span}(A^{\text{(full)}})=\operatorname{span}(\{V^{\pi}|\pi\in\Pi\})\), for every policy \(\pi\), the value vector can be represented as a linear combination of row vectors of \(A^{\text{(full)}}\), i.e., there exists \(a=(a_{1},\dots,a_{d})\in\mathbb{R}^{d}\) s.t.

\[V^{\pi}=\sum_{i=1}^{d}a_{i}A_{i}^{\text{(full)}}=A^{\text{(full)}\top}a\,.\] (8)

Now, for any unit vector \(\xi\in\operatorname{span}(b_{1},\dots,b_{d-d_{\delta}})\), we have \(\left\langle V^{\pi},\xi\right\rangle\leq\sqrt{k}\delta\).

The reason is that at each round \(d_{\delta}+1\), we pick an orthonormal basis \(\rho_{1},\dots,\rho_{k-d_{\delta}}\) of \(\operatorname{span}(V^{\pi_{1}},\dots,V^{\pi_{d_{\delta}}})^{\perp}\) (line 8 in Algorithm 1) and pick \(u_{d_{\delta}+1}\) to be the one in which there exists a policy with the largest component as described in line 9. Hence, \(|\left\langle\rho_{j},V^{\pi}\right\rangle|\leq\delta\) for all \(j\in[k-d_{\delta}]\).

It follows from Cauchy-Schwarz inequality that \(\left\langle\xi,V^{\pi}\right\rangle=\sum_{j=1}^{k-d_{\delta}}\left\langle\xi,\rho_{j}\right\rangle\left\langle\rho_{j},V^{\pi}\right\rangle\leq\sqrt{k}\delta\).

Combining with the observation that \(b_{1},\dots,b_{d-d_{\delta}}\) are pairwise orthogonal and that each of them is orthogonal to \(\operatorname{span}(V^{\pi_{1}},\dots,V^{\pi_{d_{\delta}}})\) we have

\[\sum_{i=d_{\delta}+1}^{d}a_{i}^{2}=\left|\left\langle V^{\pi},\sum_{i=d_{ \delta}+1}^{d}a_{i}b_{i-d_{\delta}}\right\rangle\right|\leq\sqrt{\sum_{i=d_{ \delta}+1}^{d}a_{i}^{2}\sqrt{k}\delta}\,,\]

which implies that

\[\sqrt{\sum_{i=d_{\delta}+1}^{d}a_{i}^{2}}\leq\sqrt{k}\delta\,.\] (9)

Since \(w^{\prime}\) satisfies \(Aw^{\prime}=\mathbf{e}_{1}\), we have

\[A^{\text{(full)}}w^{\prime}=\left(1,0,\dots,0,\left\langle b_{1},w^{\prime} \right\rangle,\dots,\left\langle b_{d-d_{\delta}},w^{\prime}\right\rangle \right).\]

For any \(w\in\mathbb{R}^{k}\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\), consider \(\widetilde{w}=w+\sum_{i=1}^{d-d_{\delta}}\left\langle b_{i},w^{\prime} \right\rangle b_{i}\). Then we have \(A^{\text{(full)}}\widetilde{w}=A^{\text{(full)}}w^{\prime}\).

Thus, applying (8) twice, we get

\[\widetilde{w}\cdot V^{\pi}=\widetilde{w}^{\top}A^{\text{(full)}\top}a=w^{ \prime\top}A^{\text{(full)}\top}a=w^{\prime}\cdot V^{\pi}\,.\]

Hence,

\[\left|w\cdot V^{\pi}-w^{\prime}\cdot V^{\pi}\right|=\left|w\cdot V ^{\pi}-\widetilde{w}\cdot V^{\pi}\right|\overset{(a)}{=}\left|\sum_{i=1}^{d}a _{i}(w-\widetilde{w})\cdot A_{i}^{\text{(full)}}\right|\] \[= \left|\sum_{i=d_{\delta}+1}^{d}a_{i}\left\langle b_{i-d_{\delta}},w^{\prime}\right\rangle\right|\overset{(b)}{\leq}\sqrt{\sum_{i=d_{\delta}+1}^ {d}a_{i}^{2}}\left\|w^{\prime}\right\|_{2}\overset{(c)}{\leq}\sqrt{k}\delta \left\|w^{\prime}\right\|_{2}\,,\]

where Eq (a) follows from (8), inequality (b) from Cauchy-Schwarz, and inequality (c) from applying (9).

**Lemma 6**.: _If \(|\widehat{\alpha}_{i}-\alpha_{i}|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every \(\delta\geq 4C_{\alpha}^{\frac{2}{3}}C_{V}d_{\delta}^{\frac{1}{3}}\epsilon_{ \alpha}^{\frac{1}{3}}\) there exists a \(w\in\mathbb{R}^{k}\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\) s.t. \(\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right|\leq\mathcal{ O}(\frac{C_{\alpha}C_{V}^{\delta}d_{\delta}^{\frac{1}{3}}\left\|w^{\prime} \right\|_{2}^{2}\epsilon_{\alpha}}{\delta^{2}})\) for all \(\pi\)._

Before proving Lemma 6, we introduce some notations and a claim.

* For any \(x,y\in\mathbb{R}^{k}\), let \(\theta(x,y)\) denotes the angle between \(x\) and \(y\).
* For any subspace \(U\subset\mathbb{R}^{k}\), let \(\theta(x,U):=\min_{y\in U}\theta(x,y)\).
* For any two subspaces \(U,U^{\prime}\subset\mathbb{R}^{k}\), we define \(\theta(U,U^{\prime})\) as \(\theta(U,U^{\prime})=\max_{x\in U}\min_{y\in U^{\prime}}\theta(x,y)\).

* For any matrix \(M\), let \(M_{i}\) denote the \(i\)-th row vector of \(M\), \(M_{i:j}\) denote the submatrix of \(M\) composed of rows \(i,i+1,\dots,j\), and \(M_{i:}\) denote the submatrix composed of all rows \(j\geq i\).
* Let \(\mathrm{span}(M)\) denote the span of the rows of \(M\).

Recall that \(\widehat{A}\in\mathbb{R}^{d\times k}\) is defined as

\[\widehat{A}=\begin{pmatrix}V^{\pi_{1}\top}\\ (\widehat{\alpha}_{1}V^{\pi_{1}}-V^{\pi_{2}})^{\top}\\ \vdots\\ (\widehat{\alpha}_{d-1}V^{\pi_{1}}-V^{\pi_{d}})^{\top}\end{pmatrix}\,,\]

which is the approximation of matrix \(A\in\mathbb{R}^{d\times k}\) defined by true values of \(\alpha_{i}\), i.e.,

\[A=\begin{pmatrix}V^{\pi_{1}\top}\\ (\alpha_{1}V^{\pi_{1}}-V^{\pi_{2}})^{\top}\\ \vdots\\ (\alpha_{d-1}V^{\pi_{1}}-V^{\pi_{d}})^{\top}\end{pmatrix}\,.\]

We denote by \(\widehat{A}^{(\delta)}=\widehat{A}_{1:d_{\delta}},A^{(\delta)}=A_{1:d_{\delta }}\in\mathbb{R}^{d_{\delta}\times k}\) the sub-matrices comprised of the first \(d_{\delta}\) rows of \(\widehat{A}\) and \(A\) respectively.

**Claim 1**.: _If \(|\widehat{\alpha}_{i}-\alpha_{i}|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every \(\delta\geq 4C_{\alpha}^{\frac{2}{\delta}}C_{V}d^{\frac{1}{3}}\epsilon_{\alpha} ^{\frac{1}{3}}\), we have_

\[\theta(\mathrm{span}(A_{2:}^{(\delta)}),\mathrm{span}(\widehat{A}_{2:}^{( \delta)}))\leq\eta_{\epsilon_{\alpha},\delta}\,,\] (10)

_and_

\[\theta(\mathrm{span}(\widehat{A}_{2:}^{(\delta)}),\mathrm{span}(A_{2:}^{( \delta)}))\leq\eta_{\epsilon_{\alpha},\delta}\,,\] (11)

_where \(\eta_{\epsilon_{\alpha},\delta}=\frac{4C_{\alpha}C_{\alpha}^{2}d_{\delta} \epsilon_{\alpha}}{\delta^{2}}\)._

To prove the above claim, we use the following lemma by Balcan et al. (2015).

**Lemma 7** (Lemma 3 of Balcan et al. (2015)).: _Let \(U_{l}=\mathrm{span}(\xi_{1},\dots,\xi_{l})\) and \(\widehat{U}_{l}=\mathrm{span}(\widehat{\xi}_{1},\dots,\widehat{\xi}_{l})\). Let \(\epsilon_{acc},\gamma_{\text{new}}\geq 0\) and \(\epsilon_{acc}\leq\gamma_{\text{new}}^{2}/(10l)\), and assume that \(\theta(\widehat{\xi}_{i},\widehat{U}_{i-1})\geq\gamma_{\text{new}}\) for \(i=2,\dots,l\), and that \(\theta(\xi_{i},\widehat{\xi}_{i})\leq\epsilon_{acc}\) for \(i=1,\dots,l\)._

_Then,_

\[\theta(U_{l},\widehat{U}_{l})\leq 2l\frac{\epsilon_{acc}}{\gamma_{\text{new}}}\,.\]

Proof of Claim 1.: For all \(2\leq i\leq d_{\delta}\), we have that

\[\theta(\widehat{A}_{i}^{(\delta)},\mathrm{span}(\widehat{A}_{2:i -1}^{(\delta)}))\geq \theta(\widehat{A}_{i}^{(\delta)},\mathrm{span}(\widehat{A}_{1:i-1 }^{(\delta)}))\geq\sin(\theta(\widehat{A}_{i}^{(\delta)},\mathrm{span}( \widehat{A}_{1:i-1}^{(\delta)})))\] \[\overset{(a)}{\geq} \frac{\left\|\widehat{A}_{i}^{(\delta)}\cdot u_{i}\right\|}{ \left\|\widehat{A}_{i}^{(\delta)}\right\|}\overset{(b)}{\geq}\frac{\delta}{ \left\|\widehat{A}_{i}^{(\delta)}\right\|_{2}}=\frac{\delta}{\left\|\widehat{ \alpha}_{i-1}V^{\pi_{1}}-V^{\pi_{i}}\right\|_{2}}\geq\frac{\delta}{(C_{\alpha} +1)C_{V}}\,,\]

where Ineq (a) holds as \(u_{i}\) is orthogonal to \(\mathrm{span}(\widehat{A}_{1:i-1}^{(\delta)})\) according to line 8 of Algorithm 1 and Ineq (b) holds due to \(\left|\widehat{A}_{i}^{(\delta)}\cdot u_{i}\right|=\left|V^{\pi_{i}}\cdot u_{i }\right|\geq\delta\). The last inequality holds due to \(\left\|\widehat{\alpha}_{i-1}V^{\pi_{1}}-V^{\pi_{i}}\right\|_{2}\leq\widehat{ \alpha}_{i-1}\left\|V^{\pi_{1}}\right\|_{2}+\left\|V_{i}\right\|_{2}\leq(C_{ \alpha}+1)C_{V}\).

Similarly, we also have

\[\theta(A_{i}^{(\delta)},\mathrm{span}(A_{2:i-1}^{(\delta)}))\geq\frac{\delta}{ (C_{\alpha}+1)C_{V}}\,.\]

We continue by decomposing \(V^{\pi_{i}}\) in the direction of \(V^{\pi_{1}}\) and the direction perpendicular to \(V^{\pi_{1}}\).

For convince, we denote \(v_{i}^{\parallel}:=V^{\pi_{i}}\cdot\frac{V^{\pi_{1}}}{\left\|V^{\pi_{1}} \right\|_{2}},V_{i}^{\parallel}:=v_{i}^{\parallel}\frac{V^{\pi_{1}}}{\left\|V^ {\pi_{1}}\right\|_{2}},V_{i}^{\perp}:=V^{\pi_{i}}-V_{i}^{\parallel}\) and \(v_{i}^{\perp}:=\left\|V_{i}^{\perp}\right\|_{2}\).

Then we have

\[\theta(A_{i}^{(\delta)},\widehat{A}_{i}^{(\delta)})=\theta(\alpha_{i-1}V^{\pi_{1}} -V^{\pi_{i}},\widehat{\alpha}_{i-1}V^{\pi_{1}}-V^{\pi_{i}})=\theta(\alpha_{i-1}V ^{\pi_{1}}-V_{i}^{\parallel}-V_{i}^{\perp},\widehat{\alpha}_{i-1}V^{\pi_{1}}-V _{i}^{\perp})\,.\]

If \((\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{\parallel})\cdot(\alpha_{i-1}V^{\pi _{1}}-V_{i}^{\parallel})\geq 0\), i.e., \(\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{\parallel}\) and \(\alpha_{i-1}V^{\pi_{1}}-V_{i}^{\parallel}\) are in the same direction, then

\[\theta(A_{i}^{(\delta)},\widehat{A}_{i}^{(\delta)}) =\left|\arctan\frac{\left\|\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i }^{\parallel}\right\|_{2}}{v_{i}^{\perp}}-\arctan\frac{\left\|\alpha_{i-1}V^{ \pi_{1}}-V_{i}^{\parallel}\right\|_{2}}{v_{i}^{\perp}}\right|\] \[\leq\left|\frac{\left\|\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{ \parallel}\right\|_{2}}{v_{i}^{\perp}}-\frac{\left\|\alpha_{i-1}V^{\pi_{1}}-V _{i}^{\parallel}\right\|_{2}}{v_{i}^{\perp}}\right|\] (12) \[=\frac{\left|\widehat{\alpha}_{i-1}-\alpha_{i-1}\right|\left\|V^ {\pi_{1}}\right\|_{2}}{v_{i}^{\perp}}\] \[\leq\frac{\epsilon_{\alpha}C_{V}}{\delta}\,,\] (13)

where Ineq (12) follows from the fact that the derivative of \(\arctan\) is at most \(1\), i.e., \(\frac{\partial\arctan x}{\partial x}=\lim_{a\to x}\frac{\arctan a- \arctan x}{a-x}=\frac{1}{1+x^{2}}\leq 1\).

Inequality (13) holds since \(v_{i}^{\perp}\geq\left|\left\langle V^{\pi_{i}},u_{i}\right\rangle\right|\geq\delta\).

If \((\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{\parallel})\cdot(\alpha_{i-1}V^{\pi _{1}}-V_{i}^{\parallel})<0\), i.e., \(\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{\parallel}\) and \(\alpha_{i-1}V^{\pi_{1}}-V_{i}^{\parallel}\) are in the opposite directions, then we have \(\left\|\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{\parallel}\right\|_{2}+\left\| \alpha_{i-1}V^{\pi_{1}}-V_{i}^{\parallel}\right\|_{2}=\left\|(\widehat{\alpha }_{i-1}-\alpha_{i-1})V^{\pi_{1}}\right\|_{2}\leq\epsilon_{\alpha}\left\|V^{\pi _{1}}\right\|_{2}\).

Similarly, we have

\[\theta(\widehat{A}_{i}^{(\delta)},A_{i}^{(\delta)}) =\left|\arctan\frac{\left\|\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i }^{\parallel}\right\|_{2}}{v_{i}^{\perp}}+\arctan\frac{\left\|\alpha_{i-1}V^{ \pi_{1}}-V_{i}^{\parallel}\right\|_{2}}{v_{i}^{\perp}}\right|\] \[\leq\frac{\left\|\widehat{\alpha}_{i-1}V^{\pi_{1}}-V_{i}^{ \parallel}\right\|_{2}}{v_{i}^{\perp}}+\frac{\left\|\alpha_{i-1}V^{\pi_{1}}-V _{i}^{\parallel}\right\|_{2}}{v_{i}^{\perp}}\Bigg{|}\] \[\leq\frac{\epsilon_{\alpha}\left\|V^{\pi_{1}}\right\|_{2}}{\left| v_{i}^{\perp}\right|_{2}}\] \[\leq\frac{\epsilon_{\alpha}C_{V}}{\delta}\,.\]

By applying Lemma 7 with \(\epsilon_{\text{acc}}=\frac{\epsilon_{\alpha}C_{V}}{\delta}\), \(\gamma_{\text{new}}=\frac{\delta}{(C_{\alpha}+1)C_{V}}\), \((\xi_{i},\widehat{\xi}_{i})=(A_{i+1},\widehat{A}_{i+1})\) (and \((\xi_{i},\widehat{\xi}_{i})=(\widehat{A}_{i+1},A_{i+1})\)), we have that when \(\delta\geq 10^{\frac{1}{3}}(C_{\alpha}+1)^{\frac{2}{3}}C_{V}d_{\delta}^{\frac {1}{3}}\epsilon_{\alpha}^{\frac{1}{3}}\),

\[\theta(\operatorname{span}(A_{2:}^{(\delta)}),\operatorname{span}(\widehat{A}_{ 2:}^{(\delta)}))\leq\frac{2d_{\delta}(C_{\alpha}+1)C_{V}^{2}\epsilon_{\alpha}}{ \delta^{2}}=\eta_{\epsilon_{\alpha},\delta}\,,\]

and

\[\theta(\operatorname{span}(\widehat{A}_{2:}^{(\delta)}),\operatorname{span}(A_{ 2:}^{(\delta)}))\leq\eta_{\epsilon_{\alpha},\delta}\,.\]

This completes the proof of Claim 1 since \(C_{\alpha}\geq 1\). 

Proof of Lemma 6.: Recall that \(\widehat{w}^{(\delta)}=\arg\min_{\widehat{A}^{(\delta)}x=\mathbf{e}_{1}}\left\| x\right\|_{2}\) is the minimum norm solution to \(\widehat{A}^{(\delta)}x=\mathbf{e}_{1}\).

Thus, \(\left\langle\widehat{w}^{(\delta)},b_{i}\right\rangle=0\) for all \(i\in[d-d_{\delta}]\).

Let \(\lambda_{1},\ldots,\lambda_{d_{\delta}-1}\) be any orthonormal basis of \(\operatorname{span}(A_{2:}^{(\delta)})\).

We construct a vector \(w\) satisfying \(A^{\text{(full)}}w=\mathbf{e}_{1}\) by removing \(\widehat{w}^{(\delta)}\)'s component in \(\operatorname{span}(A^{(\delta)}_{2:})\) and rescaling.

Formally,

\[w:=\frac{\widehat{w}^{(\delta)}-\sum_{i=1}^{d_{\delta}-1}\left\langle\widehat{w }^{(\delta)},\lambda_{i}\right\rangle\lambda_{i}}{1-V^{\pi_{1}}\cdot\left(\sum _{i=1}^{d_{\delta}-1}\left\langle\widehat{w}^{(\delta)},\lambda_{i}\right\rangle \lambda_{i}\right)}\,.\] (14)

It is direct to verify that \(A_{1}\cdot w=V^{\pi_{1}}\cdot w=1\) and \(A_{i}\cdot w=0\) for \(i=2,\ldots,d_{\delta}\). As a result, \(A^{(\delta)}w=\mathbf{e}_{1}\).

Combining with the fact that \(\widehat{w}^{(\delta)}\) has zero component in \(b_{i}\) for all \(i\in[d-d_{\delta}]\), we have \(A^{\text{(full)}}w=\mathbf{e}_{1}\).

According to Claim 1, we have

\[\theta(\operatorname{span}(A^{(\delta)}_{2:}),\operatorname{span}(\widehat{A }^{(\delta)}_{2:}))\leq\eta_{\epsilon_{\alpha},\delta}.\]

Thus, there exist unit vectors \(\widetilde{\lambda}_{1},\ldots,\widetilde{\lambda}_{d_{\delta}-1}\in \operatorname{span}(\widehat{A}^{(\delta)}_{2:})\) such that \(\theta(\lambda_{i},\widetilde{\lambda}_{i})\leq\eta_{\epsilon_{\alpha},\delta}\).

Since \(\widehat{A}^{(\delta)}\widehat{w}^{(\delta)}=\mathbf{e}_{1}\), we have \(\widehat{w}^{(\delta)}\cdot\widetilde{\lambda}_{i}=0\) for all \(i=1,\ldots,d_{\delta}-1\), and therefore,

\[\left|\widehat{w}^{(\delta)}\cdot\lambda_{i}\right|=\left|\widehat{w}^{( \delta)}\cdot(\lambda_{i}-\widetilde{\lambda}_{i})\right|\leq\left\|\widehat {w}^{(\delta)}\right\|_{2}\eta_{\epsilon_{\alpha},\delta}\,.\]

This implies that for any policy \(\pi\),

\[\left|V^{\pi}\cdot\sum_{i=1}^{d_{\delta}-1}(\widehat{w}^{(\delta)}\cdot \lambda_{i})\lambda_{i}\right|\leq\left\|V^{\pi}\right\|_{2}\sqrt{d_{\delta}} \left\|\widehat{w}^{(\delta)}\right\|_{2}\eta_{\epsilon_{\alpha},\delta}\leq C _{V}\sqrt{d_{\delta}}\left\|\widehat{w}^{(\delta)}\right\|_{2}\eta_{\epsilon_ {\alpha},\delta}\,.\]

Denote by \(\gamma=V^{\pi_{1}}\cdot\left(\sum_{i=1}^{d_{\delta}-1}\left\langle\widehat{w }^{(\delta)},\lambda_{i}\right\rangle\lambda_{i}\right)\), which is no greater than \(C_{V}\sqrt{d_{\delta}}\left\|\widehat{w}^{(\delta)}\right\|_{2}\eta_{\epsilon _{\alpha},\delta}\).

We have that

\[\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right| \leq\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-\frac{1}{1-\gamma} \widehat{w}^{(\delta)}\cdot V^{\pi}\right|+\left|\frac{1}{1-\gamma}\widehat{w }^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right|\] \[\leq\frac{\gamma\left\|\widehat{w}^{(\delta)}\right\|_{2}C_{V}}{ 1-\gamma}+\frac{1}{1-\gamma}\left|\sum_{i=1}^{d_{\delta}-1}(\widehat{w}^{( \delta)}\cdot\lambda_{i})\lambda_{i}\cdot V^{\pi}\right|\] \[\leq\frac{\gamma\left\|\widehat{w}^{(\delta)}\right\|_{2}C_{V}}{ 1-\gamma}+\frac{C_{V}\sqrt{d_{\delta}}\left\|\widehat{w}^{(\delta)}\right\|_{2 }\eta_{\epsilon_{\alpha},\delta}}{1-\gamma}\] \[=2(C_{V}\left\|\widehat{w}^{(\delta)}\right\|_{2}+1)C_{V}\sqrt{d_ {\delta}}\left\|\widehat{w}^{(\delta)}\right\|_{2}\eta_{\epsilon_{\alpha}, \delta},\] (15)

where the last equality holds when \(C_{V}\sqrt{d_{\delta}}\left\|\widehat{w}^{(\delta)}\right\|_{2}\eta_{\epsilon_{ \alpha},\delta}\leq\frac{1}{2}\).

Now we show that \(\left\|\widehat{w}^{(\delta)}\right\|_{2}\leq C\left\|w^{\prime}\right\|_{2}\) for some constant \(C\).

Since \(\widehat{w}^{(\delta)}\) is the minimum norm solution to \(\widehat{A}^{(\delta)}x=\mathbf{e}_{1}\), we will construct another solution to \(\widehat{A}^{(\delta)}x=\mathbf{e}_{1}\), denoted by \(\widehat{w}_{0}\) in a simillar manner to the construction in Eq (14), and show that \(\left\|\widehat{w}_{0}\right\|_{2}\leq C\left\|w^{\prime}\right\|\).

Let \(\xi_{1},\ldots,\xi_{d_{\delta}-1}\) be any orthonormal basis of \(\operatorname{span}(\widehat{A}^{(\delta)}_{2:})\).

We construct a \(\widehat{w}_{0}\) s.t. \(\widehat{A}^{(\delta)}\widehat{w}_{0}=\mathbf{e}_{1}\) by removing the component of \(w^{\prime}\) in \(\operatorname{span}(\widehat{A}^{(\delta)}_{2:})\) and rescaling. Specifically, let

\[\widehat{w}_{0}=\frac{w^{\prime}-\sum_{i=1}^{d_{\delta}-1}\left\langle w^{ \prime},\xi_{i}\right\rangle\xi_{i}}{1-\left\langle V^{\pi_{1}},\left(\sum_{i=1 }^{d_{\delta}-1}\left\langle w^{\prime},\xi_{i}\right\rangle\xi_{i}\right) \right\rangle}\,.\] (16)

Since \(A^{(\delta)}w^{\prime}=\mathbf{e}_{1}\), it directly follows that \(\left\langle\widehat{A}_{1},\widehat{w}_{0}\right\rangle=\left\langle V^{\pi_{1}},\widehat{w}_{0}\right\rangle=1\) and that \(\left\langle\widehat{A}_{i},\widehat{w}_{0}\right\rangle=0\) for \(i=2,\ldots,d_{\delta}\), i.e., \(\widehat{A}^{(\delta)}\widehat{w}_{0}=\mathbf{e}_{1}\).

Since Claim 1 implies that \(\theta(\operatorname{span}(\widehat{A}_{2:}^{(\delta)}),\operatorname{span}(A_{2:}^ {(\delta)}))\leq\eta_{\epsilon_{\alpha},\delta}\), there exist unit vectors \(\widetilde{\xi}_{1},\dots,\widetilde{\xi}_{d_{\delta}-1}\in\operatorname{ span}(A_{2:}^{(\delta)})\) such that \(\theta(\xi_{i},\widetilde{\xi}_{i})\leq\eta_{\epsilon_{\alpha},\delta}\).

As \(w^{\prime}\) has zero component in \(\operatorname{span}(A_{2:}^{(\delta)})\), \(w^{\prime}\) should have a small component in \(\operatorname{span}(\widehat{A}_{2:}^{(\delta)})\).

In particular,

\[\left|\left\langle w^{\prime},\xi_{i}\right\rangle\right|=\left|\left\langle w ^{\prime},\xi_{i}-\widetilde{\xi}_{i}\right\rangle\right|\leq\left\|w^{\prime }\right\|_{2}\eta_{\epsilon_{\alpha},\delta}\,,\]

which implies that

\[\left\|\sum_{i=1}^{d_{\delta}-1}\left\langle w^{\prime},\xi_{i}\right\rangle \xi_{i}\right\|_{2}\leq\sqrt{d_{\delta}}\left\|w^{\prime}\right\|_{2}\eta_{ \epsilon_{\alpha},\delta}\,.\]

Hence

\[\left|\left\langle V^{\pi_{1}},(\sum_{i=1}^{d_{\delta}-1}\left\langle w^{ \prime},\xi_{i}\right\rangle\xi_{i})\right\rangle\right|\leq C_{V}\sqrt{d_{ \delta}}\left\|w^{\prime}\right\|_{2}\eta_{\epsilon_{\alpha},\delta}\,.\]

As a result, \(\left\|\widehat{w}_{0}\right\|_{2}\leq\frac{3}{2}\left\|w^{\prime}\right\|_{2}\) when \(C_{V}\sqrt{d_{\delta}}\left\|w^{\prime}\right\|_{2}\eta_{\epsilon_{\alpha}, \delta}\leq\frac{1}{3}\), which is true when \(\epsilon_{\alpha}\) is small enough.

According to Lemma 1, \(\epsilon_{\alpha}\leq\frac{4k^{2}\epsilon}{v^{*}}\), \(\epsilon_{\alpha}\to 0\) as \(\epsilon\to 0\).

Thus, we have \(\left\|\widehat{w}^{(\delta)}\right\|_{2}\leq\left\|\widehat{w}_{0}\right\|_ {2}\leq\frac{3}{2}\left\|w^{\prime}\right\|_{2}\) and \(C_{V}\sqrt{d_{\delta}}\left\|\widehat{w}^{(\delta)}\right\|_{2}\eta_{\epsilon _{\alpha},\delta}\leq\frac{1}{2}\).

Combined with Eq (15), we get

\[\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right|=\mathcal{O} \left(\left(C_{V}\left\|w^{\prime}\right\|_{2}+1\right)C_{V}\sqrt{d_{\delta}} \left\|w^{\prime}\right\|_{2}\eta_{\epsilon_{\alpha},\delta}\right)\,.\]

Since \(C_{V}\left\|w^{\prime}\right\|_{2}\geq\left|\left\langle V^{\pi_{1}},w^{\prime }\right\rangle\right|=1\), by taking \(\eta_{\epsilon_{\alpha},\delta}=\frac{4C_{\alpha}C_{V}^{4}d_{\delta}\epsilon_{ \alpha}}{\delta^{2}}\) into the above equation, we have

\[\left|\widehat{w}^{(\delta)}\cdot V^{\pi}-w\cdot V^{\pi}\right|=\mathcal{O} \left(\frac{C_{\alpha}C_{V}^{4}d_{\delta}^{\frac{3}{2}}\left\|w^{\prime}\right\| _{2}^{2}\epsilon_{\alpha}}{\delta^{2}}\right)\,,\]

which completes the proof. 

## Appendix G Proof of Lemma 3

**Lemma 3**.: _If \(\left|\widehat{\alpha}_{i}-\alpha_{i}\right|\leq\epsilon_{\alpha}\) and \(\alpha_{i}\leq C_{\alpha}\) for all \(i\in[d-1]\), for every policy \(\pi\) and every \(\delta\geq 4C_{\alpha}^{2}C_{V}d^{\frac{3}{2}}\frac{1}{\epsilon_{\alpha}^{3}}\), we have \(\left|\widehat{w}\cdot V^{\pi}-\widehat{w}^{(\delta)}\cdot V^{\pi}\right|\leq \mathcal{O}(\left(\sqrt{k}+1\right)^{d-d_{\delta}}C_{\alpha}\epsilon^{(\delta) })\,,\) where \(\epsilon^{(\delta)}=\frac{C_{\alpha}C_{V}^{4}d_{\delta}^{\frac{3}{2}}\left\|w^{ \prime}\right\|_{2}^{2}\epsilon_{\alpha}}{\delta^{2}}+\sqrt{k}\delta\left\|w^ {\prime}\right\|_{2}\) is the upper bound in Lemma 2._

Proof.: Given the output \((V^{\pi_{1}},\dots,V^{\pi_{d}})\) of Algorithm 1, we have \(\operatorname{rank}(\operatorname{span}(\{V^{\pi_{1}},\dots,V^{\pi_{d_{\delta}}} \}))=d_{\delta}\).

For \(i=d_{\delta}+1,\dots,d\), let \(\psi_{i}\) be the normalized vector of \(V^{\pi_{i}}\)'s projection into \(\operatorname{span}(V^{\pi_{1}},\dots,V^{\pi_{i-1}})^{\perp}\) with \(\left\|\psi_{i}\right\|_{2}=1\).

Then we have that \(\operatorname{span}(V^{\pi_{1}},\dots,V^{\pi_{i-1}},\psi_{i})=\operatorname{ span}(V^{\pi_{1}},\dots,V^{\pi_{i}})\) and that \(\{\psi_{i}|i=d_{\delta}+1,\dots,d\}\) are orthonormal.

For every policy \(\pi\), the value vector can be represented as a linear combination of \(\widehat{A}_{1},\dots,\widehat{A}_{d_{\delta}},\psi_{d_{\delta}+1},\dots,\psi_{d}\), i.e., there exists a unique \(a=(a_{1},\dots,a_{d})\in\mathbb{R}^{d}\) s.t. \(V^{\pi}=\sum_{i=1}^{d_{\delta}}a_{i}\widehat{A}_{i}+\sum_{i=d_{\delta}+1}^{d}a_{ i}\psi_{i}\).

Since \(\psi_{i}\) is orthogonal to \(\psi_{j}\) for all \(j\neq i\) and \(\psi_{i}\) is are orthogonal to \(\operatorname{span}(\widehat{A}_{1},\dots,\widehat{A}_{d_{\delta}})\), we have \(a_{i}=\langle V^{\pi},\psi_{i}\rangle\) for \(i\geq d_{\delta}+1\).

This implies that

\[\left|\left\langle\widehat{w},V^{\pi}\right\rangle-\left\langle \widehat{w}^{(\delta)},V^{\pi}\right\rangle\right|\leq \underbrace{\left|\sum_{i=1}^{d_{\delta}}a_{i}\left(\left\langle \widehat{w},\widehat{A}_{i}\right\rangle-\left\langle w^{(\delta)},\widehat{A}_ {i}\right\rangle\right)\right|}_{(a)}+\underbrace{\left|\sum_{i=d_{\delta}+1}^ {d}a_{i}\left\langle\widehat{w},\psi_{i}\right\rangle\right|}_{(b)}\] \[+\underbrace{\left|\sum_{i=d_{\delta}+1}^{d}a_{i}\left\langle \widehat{w}^{(\delta)},\psi_{i}\right\rangle\right|}_{(c)}\,.\]

Since \(\widehat{A}^{(\delta)}\widehat{w}=\widehat{A}^{(\delta)}\widehat{w}^{(\delta) }=\mathbf{e}_{1}\), we have term \((a)=0\).

We move on to bound term (c).

Note that the vectors \(\{\psi_{i}|i=d_{\delta}+1,\ldots,d\}\) are orthogonal to \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{d_{\delta}}})\) and that together with \(V^{\pi_{1}},\ldots,V^{\pi_{d_{\delta}}}\) they form a basis for \(\operatorname{span}(\{V^{\pi}|\pi\in\Pi\})\).

Thus, we can let \(b_{i}\) in the proof of Lemma 2 be \(\psi_{i+d_{\delta}}\).

In addition, all the properties of \(\{b_{i}|i\in[d-d_{\delta}]\}\) also apply to \(\{\psi_{i}|i=d_{\delta}+1,\ldots,d\}\) as well.

Hence, similarly to Eq (9),

\[\sqrt{\sum_{i=d_{\delta}+1}^{d}a_{i}^{2}}\leq\sqrt{k}\delta\,.\]

Consequentially, we can bound term \((c)\) is by

\[(c)\leq\sqrt{k}\delta\left\|\widehat{w}^{(\delta)}\right\|_{2}=\frac{3}{2} \sqrt{k}\delta\left\|w^{\prime}\right\|_{2}\]

since \(\left\|\widehat{w}^{(\delta)}\right\|_{2}\leq\frac{3}{2}\left\|w^{\prime} \right\|_{2}\) when \(C_{V}\sqrt{d_{\delta}}\left\|w^{\prime}\right\|_{2}\eta_{\epsilon_{\alpha}, \delta}\leq\frac{1}{3}\) as discussed in the proof of Lemma 6.

Now all is left is to bound term (b).

We cannot bound term (b) in the same way as that of term (c) because \(\left\|\widehat{w}\right\|_{2}\) is not guaranteed to be bounded by \(\left\|w^{\prime}\right\|_{2}\).

For \(i=d_{\delta}+1,\ldots,d\), we define

\[\epsilon_{i}:=\left|\left\langle\psi_{i},\widehat{A}_{i}\right\rangle\right|\,.\]

For any \(i,j=d_{\delta}+1,\ldots,d\), \(\psi_{i}\) is perpendicular to \(V^{\pi_{1}}\), thus \(\left|\left\langle\psi_{i},\widehat{A}_{j}\right\rangle\right|=\left|\left\langle \psi_{i},\widehat{d}_{j-1}V^{\pi_{1}}-V^{\pi_{j}}\right\rangle\right|=\left| \left\langle\psi_{i},V^{\pi_{j}}\right\rangle\right|\). Especially, we have

\[\epsilon_{i}=\left|\left\langle\psi_{i},\widehat{A}_{i}\right\rangle\right|= \left|\left\langle\psi_{i},V^{\pi_{i}}\right\rangle\right|\,.\]

Let \(\widehat{A}_{i}^{\parallel}:=\widehat{A}_{i}-\sum_{j=d_{\delta}+1}^{d}\left \langle\widehat{A}_{i},\psi_{j}\right\rangle\psi_{j}\) denote \(\widehat{A}_{i}\)'s projection into \(\operatorname{span}(\widehat{A}_{1},\ldots,\widehat{A}_{d_{\delta}})\).

Since \(\widehat{A}_{i}\) has zero component in direction \(\psi_{j}\) for \(j>i\), we have \(\widehat{A}_{i}^{\parallel}=\widehat{A}_{i}-\sum_{j=d_{\delta}+1}^{i}\left \langle\widehat{A}_{i},\psi_{j}\right\rangle\psi_{j}\). Then, we have

\[0=\left\langle\widehat{w},\widehat{A}_{i}\right\rangle=\widehat{w}\cdot \widehat{A}_{i}^{\parallel}+\widehat{w}\cdot\sum_{j=d_{\delta}+1}^{i}\left \langle\widehat{A}_{i},\psi_{j}\right\rangle\psi_{j}=\widehat{w}\cdot\widehat{A }_{i}^{\parallel}-\sum_{j=d_{\delta}+1}^{i}\left\langle V^{\pi_{i}},\psi_{j} \right\rangle\left\langle\widehat{w},\psi_{j}\right\rangle\,,\]

where the first equation holds due to \(\widehat{A}\widehat{w}=\mathbf{e}_{1}\).

By rearranging terms, we have

\[\left\langle V^{\pi_{i}},\psi_{i}\right\rangle\left\langle\widehat{w},\psi_{i} \right\rangle=\widehat{w}\cdot\widehat{A}_{i}^{\parallel}-\sum_{j=d_{\delta}+1 }^{i-1}\left\langle V^{\pi_{i}},\psi_{j}\right\rangle\left\langle\widehat{w}, \psi_{j}\right\rangle\,.\] (17)Recall that at iteration \(j\) of Algorithm 1, in line 8 we pick an orthonormal basis \(\rho_{1},\ldots,\rho_{k+1-j}\) of \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{j-1}})^{\perp}\). Since \(\psi_{j}\) is in \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{j-1}})^{\perp}\) according to the definition of \(\psi_{j}\), \(|\langle V^{\pi_{i}},\psi_{j}\rangle|\) is no greater then the norm of \(V^{\pi_{i}}\)'s projection into \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{j-1}})^{\perp}\).

Therefore, we have

\[|\langle V^{\pi_{i}},\psi_{j}\rangle|\leq\sqrt{k}\max_{l\in[k+1-j ]}|\langle V^{\pi_{i}},\rho_{l}\rangle|\stackrel{{(d)}}{{\leq}} \sqrt{k}\max_{l\in[k+1-j]}\max(\left|\left\langle V^{\pi^{\rho_{l}}},\rho_{l} \right\rangle\right|,\left|\left\langle V^{\pi^{-\rho_{l}}},-\rho_{l}\right\rangle \right|)\] \[\stackrel{{(e)}}{{=}} \sqrt{k}\left|\langle V^{\pi_{j}},u_{j}\rangle\right|\stackrel{{ (f)}}{{\leq}}\sqrt{k}\left|\langle V^{\pi_{j}},\psi_{j}\rangle \right|=\sqrt{k}\epsilon_{j}\,,\] (18)

where inequality (d) holds because \(\pi^{\rho_{l}}\) is the optimal personalized policy with respect to the preference vector \(\rho_{l}\), and Equation (e) holds due to the definition of \(u_{j}\) (line 9 of Algorithm 1). Inequality (f) holds since \(\langle V^{\pi_{j}},\psi_{j}\rangle\) is the norm of \(V^{\pi_{j}}\)'s projection in \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{j-1}})^{\perp}\) and \(u_{j}\) belongs to \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{j-1}})^{\perp}\).

By taking absolute value on both sides of Eq (17), we have

\[\epsilon_{i}\left|\langle\widehat{w},\psi_{i}\rangle\right|=\left| \widehat{w}\cdot\widehat{A}_{i}^{\parallel}-\sum_{j=d_{s}+1}^{i-1}\left\langle V ^{\pi_{i}},\psi_{j}\right\rangle\left\langle\widehat{w},\psi_{j}\right\rangle \right|\leq\left|\widehat{w}\cdot\widehat{A}_{i}^{\parallel}\right|+\sqrt{k} \sum_{j=d_{s}+1}^{i-1}\epsilon_{j}\left|\langle\widehat{w},\psi_{j}\rangle \right|\,.\] (19)

We can now bound \(\left|\widehat{w}\cdot\widehat{A}_{i}^{\parallel}\right|\) as follows.

\[\left|\widehat{w}\cdot\widehat{A}_{i}^{\parallel}\right| =\left|\widehat{w}^{(\delta)}\cdot\widehat{A}_{i}^{\parallel}\right|\] (20) \[=\left|\widehat{w}^{(\delta)}\cdot(\widehat{A}_{i}-\sum_{j=d_{s}+ 1}^{d}\left\langle\widehat{A}_{i},\psi_{j}\right\rangle\psi_{j})\right|= \left|\widehat{w}^{(\delta)}\cdot\widehat{A}_{i}\right|\] (21) \[\leq\left|\widehat{w}^{(\delta)}\cdot A_{i}\right|+\left|\widehat {w}^{(\delta)}\cdot(\widehat{A}_{i}-A_{i})\right|\] \[\leq|w^{\prime}\cdot A_{i}|+\left|\left(\widehat{w}^{(\delta)}-w^ {\prime}\right)\cdot A_{i}\right|+\left|\widehat{w}^{(\delta)}\cdot(\widehat{ A}_{i}-A_{i})\right|\] \[\leq 0+(C_{\alpha}+1)\sup_{\pi}\left|(\widehat{w}^{(\delta)}-w^ {\prime})\cdot V^{\pi}\right|+C_{V}\left\|\widehat{w}^{(\delta)}\right\|_{2} \epsilon_{\alpha}\] \[\leq C^{\prime}C_{\alpha}\epsilon^{(\delta)}\,,\]

for some constant \(C^{\prime}>0\).

Eq (20) holds because \(\widehat{A}^{(\delta)}\widehat{w}=\widehat{A}^{(\delta)}\widehat{w}^{(\delta)} =\mathbf{e}_{1}\) and \(\widehat{A}_{i}^{\parallel}\) belongs to \(\operatorname{span}(\widehat{A}^{(\delta)})\). Eq (21) holds because \(\widehat{w}^{(\delta)}\) is the minimum norm solution to \(\widehat{A}^{(\delta)}x=\mathbf{e}_{1}\), which implies that \(\widehat{w}^{(\delta)}\cdot\psi_{i}=0\). The last inequality follows by applying Lemma 2.

We will bound \(\epsilon_{i}\left|\langle\widehat{w},\psi_{i}\rangle\right|\) by induction on \(i=d_{\delta}+1,\ldots,d\).

In the base case of \(i=d_{\delta}+1\),

\[\epsilon_{d_{\delta}+1}\left|\langle\widehat{w},\psi_{d_{\delta}+1}\rangle \right|\leq\left|\widehat{w}\cdot\widehat{A}_{d_{\delta}+1}^{\parallel} \right|\leq C^{\prime}C_{\alpha}\epsilon^{(\delta)}\,.\]

Then, by induction through Eq (19), we have for \(i=d_{\delta}+2,\ldots,d\),

\[\epsilon_{i}\left|\langle\widehat{w},\psi_{i}\rangle\right|\leq(\sqrt{k}+1)^{ i-d_{\delta}-1}C^{\prime}C_{\alpha}\epsilon^{(\delta)}\,.\]

Similar to the deviation of Eq (18), we pick an orthonormal basis \(\rho_{1},\ldots,\rho_{k+1-i}\) of \(\operatorname{span}(V^{\pi_{1}},\ldots,V^{\pi_{i-1}})^{\perp}\) at line 8 of Algorithm 1, then we have that, for any policy \(\pi\),

\[|\langle V^{\pi},\psi_{i}\rangle|\leq\sqrt{k}\max_{l\in[k+1-i]}|\langle V^{ \pi},\rho_{l}\rangle|\leq\sqrt{k}\left|\langle V^{\pi_{i}},u_{i}\rangle\right| \leq\sqrt{k}\left|\langle V^{\pi_{i}},\psi_{i}\rangle\right|=\sqrt{k}\epsilon _{i}\,.\]

Then we have that term (b) is bounded by

\[(b)= \left|\sum_{i=d_{\delta}+1}^{d}\left\langle V^{\pi},\psi_{i} \right\rangle\left\langle\widehat{w},\psi_{i}\right\rangle\right|\leq\sum_{i=d_ {\delta}+1}^{d}|\langle V^{\pi},\psi_{i}\rangle|\cdot|\langle\widehat{w},\psi_{ i}\rangle|\]\[\leq \sqrt{k}\sum_{i=d_{d}+1}^{d}\epsilon_{i}\left|\langle\widehat{w}, \psi_{i}\rangle\right|\leq(\sqrt{k}+1)^{d-d_{d}}C^{\prime}C_{\alpha}\epsilon^{( \delta)}\,.\]

Hence we have that for any policy \(\pi\),

\[\left|\langle\widehat{w},V^{\pi}\rangle-\left\langle\widehat{w}^{(\delta)},V^ {\pi}\right\rangle\right|\leq(\sqrt{k}+1)^{d-d_{\delta}}C^{\prime}C_{\alpha} \epsilon^{(\delta)}+\frac{3}{2}\sqrt{k}\delta\left\|w^{\prime}\right\|_{2}\,.\]

## Appendix H Proof of Theorem 2

See 2

Proof of Theorem 2.: As shown in Lemma 1, we set \(C_{\alpha}=2k\) and have \(\epsilon_{\alpha}=\frac{4k^{2}\epsilon}{v^{\epsilon}}\). We have \(\left\|w^{\prime}\right\|=\frac{\left\|w^{\prime}\right\|}{\langle w^{\ast}, V^{\pi_{1}}\rangle}\) and showed that \(\langle w^{\ast},V^{\pi_{1}}\rangle\geq\frac{v^{\ast}}{2k}\) in the proof of Lemma 1.

By applying Lemma 2 and setting \(\delta=\left(\frac{C_{V}^{4}k^{5}\left\|w^{\ast}\right\|_{2}\epsilon}{v^{\ast 2 }}\right)^{\frac{1}{3}}\), we have

\[v^{\ast}-\left\langle w^{\ast},V^{\pi^{\delta(\delta)}}\right\rangle= \langle w^{\ast},V^{\pi_{1}}\rangle\left(\left\langle w^{\prime},V^{\pi^{ }}\right\rangle-\left\langle w^{\prime},V^{\pi^{\phi(\delta)}}\right\rangle\right)\] \[\leq \langle w^{\ast},V^{\pi_{1}}\rangle\left(\left\langle\widehat{w} ^{(\delta)},V^{\pi^{\ast}}\right\rangle-\left\langle\widehat{w}^{(\delta)},V^ {\pi^{\phi(\delta)}}\right\rangle+\mathcal{O}(\sqrt{k}\left\|w^{\prime}\right\| _{2}(\frac{C_{V}^{4}k^{5}\left\|w^{\ast}\right\|_{2}\epsilon}{v^{\ast 2}})^{ \frac{1}{3}})\right)\] \[= \mathcal{O}(\sqrt{k}\left\|w^{\ast}\right\|_{2}(\frac{C_{V}^{4}k ^{5}\left\|w^{\ast}\right\|_{2}\epsilon}{v^{\ast 2}})^{\frac{1}{3}})\,.\]

## Appendix I Dependency on \(\epsilon\)

In this section, we would like to discuss a potential way of improving the dependency on \(\epsilon\) in Theorems 1 and 2.

Consider a toy example where the returned three basis policies are \(\pi_{1}\) with \(V^{\pi_{1}}=(1,0,0)\), \(\pi_{2}\) with \(V^{\pi_{2}}=(1,1,1)\) and \(\pi_{3}\) with \(V^{\pi_{3}}=(1,\eta,-\eta)\) for some \(\eta>0\) and \(w^{\ast}=(1,w_{2},w_{3})\) for some \(w_{2},w_{3}\).

The estimated ratio \(\widehat{\alpha}_{1}\) lies in \([1+w_{2}+w_{3}-\epsilon,1+w_{2}+w_{3}+\epsilon]\), and \(\widehat{\alpha}_{2}\) lies in \([1+\eta w_{2}-\eta w_{3}-\epsilon,1+\eta w_{2}-\eta w_{3}+\epsilon]\). Suppose that \(\widehat{\alpha}_{1}=1+w_{2}+w_{3}+\epsilon\) and \(\widehat{\alpha}_{2}=1+\eta w_{2}-\eta w_{3}+\epsilon\).

By solving

\[\begin{pmatrix}1&0&0\\ w_{2}+w_{3}+\epsilon&-1&-1\\ \eta w_{2}-\eta w_{3}+\epsilon&-\eta&\eta\end{pmatrix}\widehat{w}=\begin{pmatrix} 1\\ 0\\ 0\end{pmatrix}\]

we can derive \(\widehat{w}_{2}=w_{2}+\frac{\epsilon}{2}(1+\frac{1}{\eta})\) and \(\widehat{w}_{3}=w_{3}+\frac{\epsilon}{2}(1-\frac{1}{\eta})\).

The quantity measuring sub-optimality we care about is \(\sup_{\pi}\left|\langle\widehat{w},V^{\pi}\rangle-\langle w^{\ast},V^{\pi}\rangle\right|\), which is upper bounded by \(C_{V}\left\|\widehat{w}-w^{\ast}\right\|_{2}\). But the \(\ell_{2}\) distance between \(\widehat{w}\) and \(w^{\ast}\) depends on the condition number of \(\widehat{A}\), which is large when \(\eta\) is small. To obtain a non-vacuous upper bound in Section 3, we introduce another estimate \(\widehat{w}^{(\delta)}\) based on the truncated version of \(\widehat{A}\) and then upper bound \(\left\|\widehat{w}^{(\delta)}-w^{\ast}\right\|_{2}\) and \(\sup_{\pi}\left|\langle\widehat{w},V^{\pi}\rangle-\left\langle\widehat{w}^{( \delta)},V^{\pi}\rangle\right|\) separately.

However, it is unclear if \(\sup_{\pi}\left|\langle\widehat{w},V^{\pi}\rangle-\langle w^{\ast},V^{\pi} \rangle\right|\) depends on the condition number of \(\widehat{A}\). Due to the construction of Algorithm 1, we can obtain some extra information about the set of all policy values.

First, since we find \(\pi_{2}\) before \(\pi_{3}\), \(\eta\) must be no greater than \(1\). According to the algorithm, \(V^{\pi_{2}}\) is the optimal policy when the preference vector is \(u_{2}\) (see line 11 of Algorithm 1 for the definition of \(u_{2}\)) and \(V^{\pi_{3}}\) is the optimal policy when the preference vector is \(u_{3}=(0,1,-1)\). Note that the angle between \(u_{2}\) and \(V^{\pi_{2}}\) is no greater than 45 degrees according to the definition of \(u_{2}\). Then the values of all policies can only lie in the small box \(B=\{x\in\mathbb{R}^{3}|\left|u_{2}^{\top}x\right|\leq|\langle u_{2},V^{\pi_{2} }\rangle|,\left|u_{3}^{\top}x\right|\leq|\langle u_{3},V^{\pi_{3}}\rangle|\}\). It is direct to check that for any \(x\in B\), \(|\langle\widehat{w},x\rangle-\langle w^{*},x\rangle|<(1+\sqrt{2})\epsilon\). This example illustrates that even when the condition number of \(\widehat{A}\) is large, \(\sup_{\pi}|\langle\widehat{w},V^{\pi}\rangle-\langle w^{*},V^{\pi}\rangle|\) can be small. It is unclear if this holds in general. Applying this additional information to upper bound \(\sup_{\pi}|\langle\widehat{w},V^{\pi}\rangle-\langle w^{*},V^{\pi}\rangle|\) directly instead of through bounding \(C_{V}\left\|\widehat{w}-w^{*}\right\|_{2}\) is a possible way of improving the term \(\epsilon^{\frac{1}{3}}\).

## Appendix J Description of C4 and Proof of Lemma 4

```
1:input a set of \(k\)-dimensional vectors \(M\subset\mathbb{R}^{k}\) and a distribution \(p\in\mathrm{Simplex}^{M}\)
2:while\(|M|>k+1\)do
3: arbitrarily pick \(k+2\) vectors \(\mu_{1},\ldots,\mu_{k+2}\) from \(M\)
4: solve for \(x\in\mathbb{R}^{k+2}\) s.t. \(\sum_{i=1}^{k+2}x_{i}(\mu_{i}\circ 1)=\mathbf{0}\), where \(\mu\circ 1\) denote the vector of appending \(1\) to \(\mu\)
5:\(i_{0}\leftarrow\arg\max_{i\in[k+2]}\frac{|x_{i}|}{p(\mu_{i})}\)
6:if\(x_{i_{0}}<0\)then\(x\leftarrow-x\)
7:\(\gamma\leftarrow\frac{p(\mu_{i_{0}})}{x_{i_{0}}}\) and \(\forall i\in[k+2]\), \(p(\mu_{i})\gets p(\mu_{i})-\gamma x_{i}\)
8: remove \(\mu_{i}\) with \(p(\mu_{i})=0\) from \(M\)
9:endwhile
10:output\(M\) and \(p\) ```

**Algorithm 4** C4: Compress Convex Combination using Caratheodory's theorem

**Lemma 4**.: _Given a set of \(k\)-dimensional vectors \(M\subset\mathbb{R}^{k}\) and a distribution \(p\) over \(M\), \(\mathcal{C}\mathcal{A}(M,p)\) outputs \(M^{\prime}\subset M\) with \(|M^{\prime}|\leq k+1\) and a distribution \(q\in\mathrm{Simplex}^{M^{\prime}}\) satisfying that \(\mathbb{E}_{\mu\sim q}\left[\mu\right]=\mathbb{E}_{\mu\sim p}\left[\mu\right]\) in time \(\mathcal{O}(|M|\,k^{3})\)._

Proof.: The proof is similar to the proof of Caratheodory's theorem. Given the vectors \(\mu_{1},\ldots,\mu_{k+2}\) picked in line 3 of Algorithm 4 and their probability masses \(p(\mu_{i})\), we solve \(x\in\mathbb{R}^{k+2}\) s.t. \(\sum_{i=1}^{k+2}x_{i}(\mu_{i}\circ 1)=\mathbf{0}\) in the algorithm.

Note that there exists a non-zero solution of \(x\) because \(\{\mu_{i}\circ 1|i\in[k+2]\}\) are linearly dependent. Besides, \(x\) satisfies \(\sum_{i=1}^{d+2}x_{i}=0\).

Therefore,

\[\sum_{i=1}^{d+2}(p(\mu_{i})-\gamma x_{i})=\sum_{i=1}^{d+2}p(\mu_{i}).\]

For all \(i\), if \(x_{i}<0\), \(p(\mu_{i})-\gamma x_{i}\geq 0\) as \(\gamma>0\); if \(x_{i}>0\), then \(\frac{x_{i}}{p(\mu_{i})}\leq\frac{x_{i_{0}}}{p(\mu_{i_{0}})}=\frac{1}{\gamma}\) and thus \(p(\mu_{i})-\gamma x_{i}\geq 0\).

Hence, after one iteration, the updated \(p\) is still a probability over \(M\) (i.e., \(p(\mu)\geq 0\) for all \(\mu\in M\) and \(\sum_{\mu\in M}p(M)=1\)). Besides, \(\sum_{i=1}^{d+2}(p(\mu_{i})-\gamma x_{i})\mu_{i}=\sum_{i=1}^{d+2}p(\mu_{i})\mu_ {i}-\gamma\sum_{i=1}^{d+2}x_{i}\mu_{i}=\sum_{i=1}^{d+2}p(\mu_{i})\mu_{i}\).

Therefore, after one iteration, the expected value \(\mathbb{E}_{\mu\sim p}\left[\mu\right]\) is unchanged.

When we finally output \((M^{\prime},q)\), we have that \(q\) is a distribution over \(M\) and that \(\mathbb{E}_{\mu\sim q}\left[\mu\right]=\mathbb{E}_{\mu\sim p}\left[\mu\right]\).

Due to line 6 of the algorithm, we know that \(x_{i_{0}}>0\). Hence \(p(\mu_{i_{0}})-\gamma x_{i_{0}}=p(\mu_{i_{0}})-\frac{p(\mu_{i_{0}})}{x_{i_{0}}} x_{i_{0}}=0\). We remove at least one vector \(\mu_{i_{0}}\) from \(M\) and we will run for at most \(|M|\) iterations.

Finally, solving \(x\) takes \(\mathcal{O}(k^{3})\) time and thus, Algorithm 4 takes \(\mathcal{O}(|M|\,k^{3})\) time in total. 

## Appendix K Flow Decomposition Based Approach

We first introduce an algorithm based on the idea of flow decomposition.

For that, we construct a layer graph \(G=((L^{(0)}\cup\ldots\cup L^{(H+1)}),E)\) with \(H+2\) pairwise disjoint layers \(L^{(0)},\ldots,L^{(H+1)}\), where every layer \(t\leq H\) contains a set of vertices labeled by the (possibly duplicated) states reachable at the corresponding time step \(t\), i.e., \(\{s\in\mathcal{S}|\Pr(S_{t}=s|\hat{S}_{0}=s_{0})>0\}\).

Let us denote by \(x_{s}^{(t)}\) the vertex in \(L^{(t)}\) labeled by state \(s\).

Layer \(L^{(H+1)}=\{x_{*}^{(H+1)}\}\) contains only an artificial vertex, \(x_{*}^{(H+1)}\), labeled by an artificial state \(*\).

For \(t=0,\ldots,H-1\), for every \(x_{s}^{(t)}\in L^{(t)}\), \(x_{s^{\prime}}^{(t+1)}\in L^{(t+1)}\), we connect \(x_{s}^{(t)}\) and \(x_{s^{\prime}}^{(t+1)}\) by an edge labeled by \((s,s^{\prime})\) if \(P(s^{\prime}|s,\pi(s))>0\). Every vertex \(x_{s}^{(H)}\) in layer \(H\) is connected to \(x_{*}^{(H+1)}\) by one edge, which is labeled by \((s,*)\).

We denote by \(E^{(t)}\) the edges between \(L^{(t)}\) and \(L^{(t+1)}\). Note that every trajectory \(\tau=(s_{0},s_{1},\ldots,s_{H})\) corresponds to a single path \((x_{s_{0}}^{(0)},x_{s_{1}}^{(1)},\ldots,x_{s_{H}}^{(H)},x_{*}^{(H+1)})\) of length \(H+2\) from \(x_{s_{0}}^{(0)}\) to \(x_{*}^{(H+1)}\).

This is a one-to-one mapping and in the following, we use path and trajectory interchangeably.

The policy corresponds to a \((x_{s_{0}}^{(0)},x_{*}^{(H+1)})\)-flow with flow value \(1\) in the graph \(G\). In particular, the flow is defined as follows.

When the layer \(t\) is clear from the context, we actually refer to vertex \(x_{s}^{(t)}\) by saying vertex \(s\).

For \(t=0,\ldots,H-1\), for any edge \((s,s^{\prime})\in E^{(t)}\), let \(f:E\rightarrow\mathbb{R}^{+}\) be defined as

\[f(s,s^{\prime})=\sum_{\tau:(s_{t}^{\tau},s_{t+1}^{\tau})=(s,s^{\prime})}q^{ \pi}(\tau)\,,\] (22)

where \(q^{\pi}(\tau)\) is the probability of \(\tau\) being sampled.

For any edge \((s,*)\in E^{(H)}\), let \(f(s,*)=\sum_{(s^{\prime},s)\in E^{(H-1)}}f(s^{\prime},s)\). It is direct to check that the function \(f\) is a well-defined flow. We can therefore compute \(f\) by dynamic programming.

For all \((s_{0},s)\in E^{(0)}\), we have \(f(s_{0},s)=P(s|s_{0},\pi(s_{0}))\) and for \((s,s^{\prime})\in E^{(t)}\),

\[f(s,s^{\prime})=P(s^{\prime}|s,\pi(s))\,\sum_{s^{\prime\prime}:(s^{\prime \prime},s)\in E^{(t-1)}}f(s^{\prime\prime},s)\,.\] (23)

Now we are ready to present our algorithm by decomposing \(f\) in Algorithm 5.

Each iteration in Algorithm 5 will zero out at least one edge and thus, the algorithm will stop within \(|E|\) rounds.

```
1:initialize \(Q\leftarrow\emptyset\).
2:calculate \(f(e)\) for all edge \(e\in E\) by dynamic programming according to Eq (23)
3:while\(\exists e\in E\) s.t. \(f(e)>0\)do
4: pick a path \(\tau=(s_{0},s_{1},\ldots,s_{H},*)\in L^{(0)}\times L^{(1)}\times\ldots\times L^ {(H+1)}\) s.t. \(f(s_{i},s_{i+1})>0\)\(\forall i\geq 0\)
5:\(f_{\tau}\leftarrow\min_{e\,\mathrm{in}\,\tau}f(e)\)
6:\(Q\gets Q\cup\{(\tau,f_{\tau})\}\), \(f(e)\gets f(e)-f_{\tau}\) for \(e\) in \(\tau\)
7:endwhile
8:output \(Q\) ```

**Algorithm 5** Flow decomposition based approach

**Theorem 4**.: _Algorithm 5 outputs \(Q\) satisfying that \(\sum_{(\tau,f_{\tau})\in Q}f_{\tau}\Phi(\tau)=V^{\pi}\) in time \(\mathcal{O}(H^{2}\left|\mathcal{S}\right|^{2})\)._The core idea of the proof is that for any edge \((s,s^{\prime})\in E^{(t)}\), the flow on \((s,s^{\prime})\) captures the probability of \(S_{t}=s\wedge S_{t+1}=s^{\prime}\) and thus, the value of the policy \(V^{\pi}\) is linear in \(\{f(e)|e\in E\}\).

The output \(Q\) has at most \(|E|\) number of weighted paths (trajectories). We can further compress the representation through C4, which takes \(O(|Q|\,k^{3})\) time.

**Corollary 2**.: _Executing Algorithm 5 with the output \(Q\) first and then running C4 over \(\{(\Phi(\tau),f_{\tau})|(\tau,f_{\tau})\in Q\}\) returns a \((k+1)\)-sized weighted trajectory representation in time \(\mathcal{O}(H^{2}\left|\mathcal{S}\right|^{2}+k^{3}H\left|\mathcal{S}\right|^ {2})\)._

We remark that the running time of this flow decomposition approach underperforms that of the expanding and compressing approach (see Theorem 3) whenever \(|\mathcal{S}|H+|\mathcal{S}|k^{3}=\omega(k^{4}+k|\mathcal{S}|)\).

## Appendix L Proof of Theorem 4

**Theorem 4**.: _Algorithm 5 outputs \(Q\) satisfying that \(\sum_{(\tau,f_{\tau})\in Q}f_{\tau}\Phi(\tau)=V^{\pi}\) in time \(\mathcal{O}(H^{2}\left|\mathcal{S}\right|^{2})\)._

Proof.: **Correctness:** The function \(f\) defined by Eq (22) is a well-defined flow since for all \(t=1,\ldots,H\), for all \(s\in L^{(t)}\), we have that

\[\sum_{s^{\prime}\in L^{(t+1)}:(s,s^{\prime})\in E^{(t)}}f(s,s^{ \prime})=\sum_{s^{\prime}\in L^{(t+1)}:(s,s^{\prime})\in E^{(t)}}\sum_{\tau:( s^{\prime}_{t},s^{\prime}_{t+1})=(s,s^{\prime})}q^{\pi}(\tau)=\sum_{\tau:s^{ \prime}_{t}=s}q^{\pi}(\tau)\] \[= \sum_{s^{\prime\prime}\in L^{(t-1)}:(s^{\prime\prime},s)\in E^{( t-1)}}f(s^{\prime\prime},s)\,.\]

In the following, we first show that Algorithm 5 will terminate with \(f(e)=0\) for all \(e\in E\).

First, after each iteration, \(f\) is still a feasible \((x^{(0)},x^{(H+1)})\)-flow feasible flow with the total flow out-of \(x^{(0)}\) reduced by \(f_{\tau}\). Besides, for edge \(e\) with \(f(e)>0\) at the beginning, we have \(f(e)\geq 0\) throughout the algorithm because we never reduce \(f(e)\) by an amount greater than \(f(e)\).

Then, since \(f\) is a \((x^{(0)},x^{(H+1)})\)-flow and \(f(e)\geq 0\) for all \(e\in E\), we can always find a path \(\tau\) in line 4 of Algorithm 5.

Otherwise, the set of vertices reachable from \(x^{(0)}\) through edges with positive flow does not contain \(x^{(H+1)}\) and the flow out of this set equals the total flow out-of \(x^{(0)}\). But since other vertices are not reachable, there is no flow out of this set, which is a contradiction.

In line 6, there exists at least one edge \(e\) such that \(f(e)>0\) is reduced to \(0\). Hence, the algorithm will run for at most \(|E|\) iterations and terminate with \(f(e)=0\) for all \(e\in E\).

Thus we have that for any \((s,s^{\prime})\in E^{(t)}\), \(f(s,s^{\prime})=\sum_{(\tau,f_{\tau})\in Q:(s^{\tau}_{t},s^{\tau}_{t+1})=(s,s^ {\prime})}f_{\tau}\).

Then we have

\[V^{\pi} =\sum_{\tau}q^{\pi}(\tau)\Phi(\tau)=\sum_{\tau}q^{\pi}(\tau) \left(\sum_{t=0}^{H-1}R(s^{\tau}_{t},\pi(s^{\tau}_{t}))\right)\] \[=\sum_{t=0}^{H-1}\sum_{\tau}q^{\pi}(\tau)R(s^{\tau}_{t},\pi(s^{ \tau}_{t}))\] \[=\sum_{t=0}^{H-1}\sum_{(s,s^{\prime})\in E^{(t)}}R(s,\pi(s))\sum_{ \tau:(s^{\tau}_{t},s^{\tau}_{t+1})=(s,s^{\prime})}q^{\pi}(\tau)\] \[=\sum_{t=0}^{H-1}\sum_{(s,s^{\prime})\in E^{(t)}}R(s,\pi(s))f(s,s^ {\prime})\] \[=\sum_{t=0}^{H-1}\sum_{(s,s^{\prime})\in E^{(t)}}R(s,\pi(s))\sum_{ (\tau,f_{\tau})\in Q:(s^{\tau}_{t},s^{\tau}_{t+1})=(s,s^{\prime})}f_{\tau}\]\[=\sum_{(\tau,f_{\tau})\in Q}f_{\tau}(\sum_{t=0}^{H-1}R(s_{t}^{\tau}, \pi(s_{t}^{\tau})))\] \[=\sum_{(\tau,f_{\tau})\in Q}f_{\tau}\Phi(\tau)\,.\]

Computational complexity:Solving \(f\) takes \(\mathcal{O}(\left|E\right|)\) time. The algorithm will run for \(\mathcal{O}(\left|E\right|)\) iterations and each iteration takes \(\mathcal{O}(H)\) time. Since \(\left|E\right|=\mathcal{O}(\left|\mathcal{S}\right|^{2}H)\), the total running time of Algorithm 5 is \(\mathcal{O}(\left|\mathcal{S}\right|^{2}H^{2})\). C4 will take \(\mathcal{O}(k^{3}\left|E\right|)\) time. 

## Appendix M Proof of Theorem 3

**Theorem 3**.: _Algorithm 2 outputs \(F^{(H)}\) and \(\beta^{(H)}\) satisfying that \(\left|F^{(H)}\right|\leq k+1\) and \(\sum_{\tau\in F^{(H)}}\beta^{(H)}(\tau)\Phi(\tau)=V^{\pi}\) in time \(\mathcal{O}(k^{4}H\left|\mathcal{S}\right|+kH\left|\mathcal{S}\right|^{2})\)._

Proof.: **Correctness:** C4 guarantees that \(\left|F^{(H)}\right|\leq k+1\).

We will prove \(\sum_{\tau\in F^{(H)}}\beta^{(H)}_{\tau}\Phi(\tau)=V^{\pi}\) by induction on \(t=1,\ldots,H\).

Recall that for any trajectory \(\tau\) of length \(h\), \(J(\tau)=\Phi(\tau)+V(s_{h}^{\tau},H-h)\) was defined as the expected return of trajectories (of length \(H\)) with the prefix being \(\tau\).

In addition, recall that \(J_{F^{(t)}}=\{J(\tau\circ s)|\tau\in F^{(t)},s\in\mathcal{S}\}\) and \(p_{F^{(t)},\beta^{(t)}}\) was defined by letting \(p_{F^{(t)},\beta^{(t)}}(\tau\circ s)=\beta^{(t)}(\tau)P(s|s_{t}^{\tau},\pi(s _{t}^{\tau}))\).

For the base case, we have that at \(t=1\)

\[V^{\pi} =R(s_{0},\pi(s_{0}))+\sum_{s\in\mathcal{S}}P(s|s_{0},\pi(s_{0}))V ^{\pi}(s,H-1)=\sum_{s\in\mathcal{S}}P(s|s_{0},\pi(s_{0}))J((s_{0})\circ s)\] \[=\sum_{s\in\mathcal{S}}p_{F^{(0)},\beta^{(0)}}((s_{0})\circ s)J(( s_{0})\circ s)=\sum_{\tau\in F^{(1)}}\beta^{(1)}(\tau)J(\tau)\,.\]

Suppose that \(V^{\pi}=\sum_{\tau^{\prime}\in F^{(t)}}\beta^{(t)}(\tau^{\prime})J(\tau^{ \prime})\) holds at time \(t\), then we prove the statement holds at time \(t+1\).

\[V^{\pi} =\sum_{\tau^{\prime}\in F^{(t)}}\beta^{(t)}(\tau^{\prime})J(\tau^ {\prime})=\sum_{\tau^{\prime}\in F^{(t)}}\beta^{(t)}(\tau^{\prime})(\Phi(\tau ^{\prime})+V^{\pi}(s_{t}^{\tau^{\prime}},H-t))\] \[=\sum_{\tau^{\prime}\in F^{(t)}}\beta^{(t)}(\tau^{\prime})\left( \Phi(\tau^{\prime})+\sum_{s\in\mathcal{S}}P(s|s_{t}^{\tau^{\prime}},\pi(s_{t}^ {\tau^{\prime}}))\left(R(s_{t}^{\tau^{\prime}},\pi(s_{t}^{\tau^{\prime}}))+V^{ \pi}(s,H-t)\right)\right)\] \[=\sum_{\tau^{\prime}\in F^{(t)}}\sum_{s\in\mathcal{S}}\beta^{(t)}( \tau^{\prime})P(s|s_{t}^{\tau^{\prime}},\pi(s_{t}^{\tau^{\prime}}))\left(\Phi( \tau^{\prime})+R(s_{t}^{\tau^{\prime}},\pi(s_{t}^{\tau^{\prime}}))+V^{\pi}(s,H -t)\right)\] \[=\sum_{\tau^{\prime}\in F^{(t)}}\sum_{s\in\mathcal{S}}\beta^{(t)}( \tau^{\prime})P(s|s_{t}^{\tau^{\prime}},\pi(s_{t}^{\tau^{\prime}}))\left(\Phi( \tau^{\prime}\circ s)+V^{\pi}(s,H-t)\right)\] \[=\sum_{\tau^{\prime}\in F^{(t)}}\sum_{s\in\mathcal{S}}p_{F^{(t)}, \beta^{(t)}}(\tau^{\prime}\circ s)J(\tau^{\prime}\circ s)\] \[=\sum_{\tau\in F^{(t+1)}}\beta^{(t+1)}(\tau)J(\tau)\,.\]

By induction, the statement holds at \(t=H\) by induction, i.e., \(V^{\pi}=\sum_{\tau\in F^{(H)}}\beta^{(H)}_{\tau}J(\tau)=\sum_{\tau\in F^{(H)}} \beta^{(H)}_{\tau}\Phi(\tau)\).

Computational complexity:Solving \(V^{\pi}(s,h)\) for all \(s\in\mathcal{S},h\in[H]\) takes time \(\mathcal{O}(kH\left|\mathcal{S}\right|^{2})\). In each round, we need to call C4 for \(\leq(k+1)\left|S\right|\) vectors, which takes \(\mathcal{O}(k^{4}\left|\mathcal{S}\right|)\) time. Thus, we need \(\mathcal{O}(k^{4}H\left|\mathcal{S}\right|+kH\left|\mathcal{S}\right|^{2})\) time in total.

Example of maximizing individual objective

**Observation 2**.: _Assume there exist \(k>2\) policies that together assemble \(k\) linear independent value vectors. Consider the \(k\) different policies \(\pi_{1}^{*},\dots,\pi_{k}^{*}\) that each \(\pi_{i}^{*}\) maximizes the objective \(i\in[k]\). Then, their respective value vectors \(V_{1}^{*},\dots,V_{k}^{*}\) are not necessarily linearly independent. Moreover, if \(V_{1}^{*},\dots,V_{k}^{*}\) are linearly depended it does not mean that \(k\) linearly independent value vectors do not exists._

Proof.: For simplicity, we show an example with a horizon of \(H=1\) but the results could be extended to any \(H\geq 1\). We will show an example where there are \(4\) different value vectors, where \(3\) of them are obtained by the \(k=3\) policies that maximize the \(3\) objectives and have linear dependence.

Consider an MDP with a single state (also known as Multi-arm Bandit) with \(4\) actions with deterministic reward vectors (which are also the expected values of the \(4\) possible policies in this case):

\[r(1)=\begin{pmatrix}8\\ 4\\ 2\end{pmatrix},\quad r(2)=\begin{pmatrix}1\\ 2\\ 3\end{pmatrix},\quad r(3)=\begin{pmatrix}85/12\\ 25/6\\ 35/12\end{pmatrix}\approx\begin{pmatrix}7.083\\ 4.167\\ 2.9167\end{pmatrix},\quad r(4)=\begin{pmatrix}1\\ 3\\ 2\end{pmatrix}.\]

Denote \(\pi^{a}\) as the fixed policy that always selects action \(a\). Clearly, policy \(\pi^{1}\) maximizes the first objective, policy \(\pi^{2}\) the third, and policy \(\pi^{3}\) the second (\(\pi^{4}\) do not maximize any objective). However,

* \(r(3)\) linearly depends on \(r(1)\) and \(r(2)\) as \[\frac{5}{6}r(1)+\frac{5}{12}r(2)=r(3).\]
* In addition, \(r(4)\) is linearly independent in \(r(1),r(2)\): Assume not. Then, there exists \(\beta_{1},\beta_{2}\in\mathbb{R}\) s.t.: \[\beta_{1}\cdot r(1)+\beta_{2}\cdot r(2)=\begin{pmatrix}8\beta_{1}+\beta_{2}\\ 4\beta_{1}+2\beta_{2}\\ 2\beta_{1}+3\beta_{2}\end{pmatrix}=\begin{pmatrix}1\\ 3\\ 2\end{pmatrix}=r(4).\] Hence, the first equations imply \(\beta_{2}=1-8\beta_{1}\), and \(4\beta_{1}+2-16\beta_{1}=3\), hence \(\beta_{1}=-\frac{1}{12}\) and \(\beta_{2}=\frac{5}{3}\). Assigning in the third equation yields \(-\frac{1}{6}+5=2\) which is a contradiction.