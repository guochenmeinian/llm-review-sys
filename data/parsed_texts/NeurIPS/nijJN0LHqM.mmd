# Practical Sharpness-Aware Minimization

Cannot Converge All the Way to Optima

 Dongkuk Si  Chulhee Yun

Korea Advanced Institute of Science and Technology (KAIST)

{dongkuksi, chulhee.yun}@kaist.ac.kr

###### Abstract

Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation \(\bm{y}_{t}=\bm{x}_{t}+\rho\frac{\nabla f(\bm{x}_{t})}{\|\nabla f(\bm{x}_{t})\|}\) of the current point \(\bm{x}_{t}\). Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size \(\rho\) and/or no gradient normalization in \(\bm{y}_{t}\), which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant \(\rho\) and gradient normalization in \(\bm{y}_{t}\)) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has _limited_ capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of \(\tilde{\Theta}(\frac{1}{T^{2}})\), the convergence bound of stochastic SAM suffers an _inevitable_ additive term \(\mathcal{O}(\rho^{2})\), indicating convergence only up to _neighborhoods_ of optima. In fact, such \(\mathcal{O}(\rho^{2})\) factors arise for stochastic SAM in all the settings we consider, and also for deterministic SAM in nonconvex cases; importantly, we prove by examples that such terms are _unavoidable_. Our results highlight vastly different characteristics of SAM with vs. without decaying perturbation size or gradient normalization, and suggest that the intuitions gained from one version may not apply to the other.

## 1 Introduction

Modern neural networks are armed with a large number of layers and parameters, having a risk to overfit to training data. In order to make accurate predictions on unseen data, generalization performance has been considered as the most important factor in deep learning models. Based on the widely accepted belief that geometric properties of loss landscape are correlated with generalization performance, studies have proved theoretical and empirical results regarding the relation between _sharpness_ measures and generalization [14; 17; 19; 21; 29]. Here, _sharpness_ of a loss at a point generally refers to the degree to which the loss varies in the small neighborhood of the point.

Motivated by prior studies that show flat minima have better generalization performance [19; 21], Foret et al. [16] propose an optimization method referred to as _Sharpness-Aware Minimization (SAM)_. A single iteration of SAM consists of one ascent step (perturbation) and one descent step. Starting from the current iterate \(\bm{x}_{t}\), SAM first takes an ascent step \(\bm{y}_{t}=\bm{x}_{t}+\rho\frac{\nabla f(\bm{x}_{t})}{\|\nabla f(\bm{x}_{t})\|}\) to (approximately) maximize the loss value in the \(\rho\)-neighborhood of \(\bm{x}_{t}\), and then uses the gradient at \(\bm{y}_{t}\) to update \(\bm{x}_{t}\) to \(\bm{x}_{t+1}\). This two-step procedure gives SAM a special characteristic: the tendency to search for a flat minimum, i.e., a minimum whose neighboring points also return low loss value. Empirical results [3; 9; 16; 20; 25] show that SAM demonstrates an exceptional ability to perform well on different models and tasks with high generalization performance. Following the success of SAM,numerous extensions of SAM have been proposed in the literature [6; 15; 18; 23; 24; 26; 27; 28; 30; 31; 32; 34; 35].

On the theoretical side, various studies have demonstrated different characteristics of SAM [1; 2; 4; 5; 12; 13; 22; 33]. However, comprehending the global convergence properties of practical SAM on a theoretical level still remains elusive. In fact, several recent results [2; 18; 27; 31; 35] attempt to prove the convergence guarantees for SAM and its variants. While these results provide convergence guarantees of SAM on smooth functions, they are somewhat detached to the practical implementations of SAM and its variants. They either (1) assume _decaying_ or _sufficiently small_ perturbation size \(\rho\)[18; 27; 31; 35], whereas \(\rho\) is set to constant in practice; or (2) assume ascent steps _without gradient normalization_[2], whereas practical implementations of SAM use normalization when calculating the ascent step \(\bm{y}_{t}\).

### Summary of Our Contributions

To address the aforementioned limitations of existing results, we investigate convergence properties of SAM using _gradient normalization_ in ascent steps and arbitrary _constant perturbation size_\(\rho\). We note that to the best of our knowledge, the convergence analysis of SAM have not been carried out under the two practical implementation choices. Our analyses mainly focus on smooth functions, with different levels of convexity assumptions ranging from strong convexity to nonconvexity. We summarize our contributions below; a summary of our convergence results (upper and lower bounds) can also be found in Table 1.

* For deterministic SAM, we prove convergence to global minima of smooth strongly convex functions, and show the tightness of convergence rate in terms of \(T\). Furthermore, we establish the convergence of SAM to stationary points of smooth convex functions. For smooth nonconvex functions, we prove that SAM guarantees convergence to stationary points up to an additive factor \(\mathcal{O}(\rho^{2})\). We provide a worst-case example that always suffers a matching squared gradient norm \(\Omega(\rho^{2})\), showing that the additive factor is unavoidable and tight in terms of \(\rho\).
* For stochastic settings, we analyze two versions of stochastic SAM (\(n\)-SAM and \(m\)-SAM) on smooth strongly convex, smooth convex, and smooth (Lipschitz) nonconvex functions. We provide convergence guarantees to global minima or stationary points up to additive factors \(\mathcal{O}(\rho^{2})\). In case of \(m\)-SAM, we demonstrate that these factors are inevitable and tight in terms of \(\rho\), by providing worst-case examples where SAM fails to converge properly.

### Related Works: Convergence Analysis of SAM

Recent results [18; 27; 31; 35] prove convergence of stochastic SAM and its variants to stationary points for smooth nonconvex functions. However, these convergence analyses are limited to only _smooth nonconvex_ functions and do not address convex functions. Also, as already pointed out, all the proofs require one crucial assumption detached from practice: _decaying_ (or _sufficiently small_) perturbation size \(\rho\). If \(\rho\) becomes sufficiently small, then the difference between \(f^{\mathrm{SAM}}(\bm{x})=\max_{\|\bm{\varepsilon}\|\leq\rho}f(\bm{x}+\bm{ \epsilon})\) and \(f(\bm{x})\) becomes negligible, which means that they undergo approximately the same updates. Due to this reason, especially in later iterates, SAM with negligible perturbation size

\begin{table}
\begin{tabular}{c c c c} \hline \hline Optimizer & Function Class & Convergence Upper/Lower Bounds & Reference \\ \hline Deterministic SAM & **C1.C2** & \(\min_{\bm{\varepsilon}\in\{0,...,T\}}f(\bm{x}_{t})-f^{*}=\mathcal{O}\left( \exp(-T)+\frac{1}{T^{2}}\right)\) & Theorem 3.1 \\ Deterministic SAM & **C1.C2** & \(\min_{\bm{\varepsilon}\in\{0,...,T\}}f(\bm{x}_{t})-f^{*}=\Omega\left(\frac{1}{T ^{2}}\right)\) & Theorem 3.2 \\ Deterministic SAM & **C1.C3** & \(\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}=\mathcal{O}\left(\frac{ 1}{T}+\frac{1}{\sqrt{T}}\right)\) & Theorem 3.3 \\ Deterministic SAM & **C1** & \(\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\mathcal{O}\left( \frac{1}{T}\right)+\beta^{2}\rho^{2}\) & Theorem 3.4 \\ Stochastic SAM & **C1.C2.C5** & \(\mathbb{E}(\bm{x}_{T})-f^{*}\lesssim\mathcal{O}\left(\exp\left(-T\right)+\frac{ |\sigma^{2}-\beta^{2}\rho^{2}|_{+}}{T}\right)+\frac{2\beta^{2}\rho^{2}}{\mu}\) & Theorem 4.1 \\ Stochastic SAM & **C1.C3.C5** & \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}\leq\mathcal{O} \left(\frac{1}{T}+\frac{1}{\sqrt{T}}\right)+\beta^{2}\rho^{2}\) & Theorem 4.3 \\ Stochastic \(n\)-SAM & **C1.C4.C5** & \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|(\|\nabla f(\bm{x}_{t})\|-\beta\rho)^{2} \leq\mathcal{O}\left(\frac{1}{T}+\frac{1}{\sqrt{T}}\right)+\beta^{2}\rho^{2}\) & Theorem 4.5 \\ Stochastic \(m\)-SAM & **C1.C4.C5** & \(\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|(\|\nabla f(\bm{x}_{t})\|-\beta\rho)^{2} \leq\mathcal{O}\left(\frac{1}{T}+\frac{1}{\sqrt{T}}\right)+5\beta^{2}\rho^{2}\) & Theorem 4.6 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Convergence of SAM with constant perturbation size \(\rho\) after \(T\) steps. **C1**, **C2**, **C3**, and **C4** indicate \(\beta\)-smoothness, \(\mu\)-strong convexity, convexity, and Lipschitzness, respectively. **C5** indicates the bounded variance of the gradient oracle. See Section 2.3 for definitions of **C1â€“C5**.

would become almost identical to gradient descent (GD), resulting in a significant difference in the convergence behavior compared to constant \(\rho\).

As for existing proofs that do not require decaying \(\rho\), Andriushchenko and Flammarion [2] prove convergence guarantees for _deterministic_ SAM with _constant_\(\rho\) on smooth nonconvex, smooth Polyak-Lojasiewicz, smooth convex, and smooth strongly convex functions. Moreover, the authors prove convergence guarantees of _stochastic_ SAM, this time with _decaying_\(\rho\), on smooth nonconvex and smooth Polyak-Lojasiewicz functions. However, their analyses are also crucially different from the practical implementations of SAM because their proofs are for a variant of SAM _without gradient normalization_ in ascent steps, with updates in the form of \(\bm{x}_{t+1}=\bm{x}_{t}-\eta\nabla f(\bm{x}_{t}+\rho\nabla f(\bm{x}_{t}))\). This form can be considered as vanilla SAM with an "effective" perturbation size \(\rho\|\nabla f(\bm{x}_{t})\|\), which indicates that even with constant \(\rho\), the effective perturbation size will become smaller and smaller as the algorithm converges towards a stationary point. As in the case of decaying \(\rho\) discussed above, this can make a huge difference in the convergence behavior.

Figure 1 illustrates a comparison between SAM with and without gradient normalization, highlighting the disparity between their convergence points. As we predicted above, it is evident that they indeed reach entirely distinct global minima, exhibiting different convergence characteristics.

Discussions on the related works on the relationship between sharpness and generalization, as well as other theoretical properties of SAM, are provided in Appendix A.

### Notation

The euclidean norm of a vector \(\bm{v}\) is denoted as \(\|\bm{v}\|\). We let \(\mathbb{R}^{*}_{+}\triangleq\{x\in\mathbb{R}\mid x>0\}\), and use \([\cdot]_{+}\) to define \(\max\{\cdot,0\}\). We use \(\mathcal{O}(\cdot)\) and \(\Omega(\cdot)\) to hide numerical constants in our upper and lower bounds, respectively. We use \(\tilde{\mathcal{O}}(\cdot)\) to additionally hide logarithmic factors. When discussing rates, we sometimes use these symbols to hide everything except \(T\), the number of SAM iterations. When discussing additive factors that arise in our convergence bounds, we use these symbols to hide all other variables except the perturbation size \(\rho\). For an objective function \(f\) and initialization \(\bm{x}_{0}\) of SAM, we define \(f^{*}\triangleq\inf_{\bm{x}}f(\bm{x})\) and \(\Delta\triangleq f(\bm{x}_{0})-f^{*}\).

## 2 Sharpness-Aware Minimization: Preliminaries and Intuitions

Before presenting the main results, we first introduce deterministic and stochastic SAM, and present definitions of function classes considered in this paper. We next introduce _virtual gradient map_ and _virtual loss_ that shed light on our intuitive understanding of SAM's convergence behavior.

### Sharpness-Aware Minimization (SAM)

We focus on minimizing a function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\), where the optimization variable is represented by \(\bm{x}\in\mathbb{R}^{d}\), using the Sharpness-Aware Minimization (SAM) optimizer. Instead of minimizing the vanilla objective function \(f(\bm{x})\), SAM [16] aims to minimize the _SAM objective_\(f^{\mathrm{SAM}}(\bm{x})\), where

\[\min_{\bm{x}}f^{\mathrm{SAM}}(\bm{x})=\min_{\bm{x}}\max_{\|\bm{v}\|\leq\rho}f( \bm{x}+\bm{\epsilon}).\] (1)

For \(\rho\in\mathbb{R}^{*}_{+}\), the SAM loss \(f^{\mathrm{SAM}}(\bm{x})\) outputs the worst function value in a \(\rho\)-ball \(\{\bm{w}\in\mathbb{R}^{d}\mid\|\bm{x}-\bm{w}\|\leq\rho\}\) around \(\bm{x}\). Assuming "sufficiently small" \(\rho\), the inner maximization of (1) can be solved by Taylor-approximating the objective:

\[\arg\max_{\|\bm{v}\|\leq\rho}f(\bm{x}+\bm{\epsilon})\approx\arg\max_{\|\bm{v} \|\leq\rho}f(\bm{x})+\langle\bm{\epsilon},\nabla f(\bm{x})\rangle=\rho\tfrac{ \nabla f(\bm{x})}{\|\nabla f(\bm{x})\|}\triangleq\hat{\bm{\epsilon}}(\bm{x}).\]

Figure 1: Trajectory plot for a function \(f(x,y)=(xy-1)^{2}\). The green line indicates global minima of \(f\). SAM and USAM indicates SAM with and without gradient normalization, respectively, and GD indicates gradient descent. When USAM approaches the green line, it converges to a nearby global minimum, which is similar to GD. In contrast, SAM travels along the green line, towards the flattest minimum \((1,1)\).

Using this approximate solution \(\hat{\bm{\epsilon}}(\bm{x})\), we can define the _approximate SAM objective_\(\hat{f}^{\mathrm{SAM}}\) as \(\hat{f}^{\mathrm{SAM}}(\bm{x})\triangleq f(\bm{x}+\hat{\bm{\epsilon}}(\bm{x}))\). In order to run gradient descent (GD) on \(\hat{f}^{\mathrm{SAM}}(\bm{x})\), one needs to calculate its gradient; however, from the definition of \(\hat{\bm{\epsilon}}(\bm{x})\), we can realize that \(\nabla\hat{f}^{\mathrm{SAM}}(\bm{x})\) has terms that involve the Hessian of \(f\). Here, SAM makes another approximation, by ignoring the Hessian term:

\[\nabla\hat{f}^{\mathrm{SAM}}(\bm{x})\approx\nabla f(\bm{x})|_{\bm{x}+\hat{ \bm{\epsilon}}(\bm{x})}.\]

From these approximations, one iteration of SAM is defined as a set of two-step update equations:

\[\begin{cases}\bm{y}_{t}=\bm{x}_{t}+\rho\frac{\nabla f(\bm{x}_{t})}{\|\nabla f (\bm{x}_{t})\|},\\ \bm{x}_{t+1}=\bm{x}_{t}-\eta\nabla f(\bm{y}_{t}).\end{cases}\] (2)

As seen in (2), we use \(\bm{x}_{t}\) to denote the iterate of SAM at the \(t\)-th step. We use \(T\) to denote the number of SAM iterations. We refer to the hyperparameter \(\rho\in\mathbb{R}^{*}_{+}\) as the _perturbation size_ and \(\eta\in\mathbb{R}^{*}_{+}\) as the _step size_. Note that in (2), the perturbation size \(\rho\) is a time-invariant constant; in practice, it is common to fix \(\rho\) as a constant throughout training [3; 9; 16].

According to the SAM update in (2), the update cannot be defined when \(\|\nabla f(\bm{x})\|=0\). In practice, we add a small numerical constant (e.g., \(10^{-12}\)) to the denominator in order to prevent numerical instability. In this paper, we ignore this constant and treat \(\frac{\nabla f(\bm{x}_{t})}{\|\nabla f(\bm{x}_{t})\|}\) as \(0\) whenever \(\|\nabla f(\bm{x}_{t})\|=0\).

### SAM under Stochastic Settings

To analyze stochastic SAM, we suppose that the objective is given as \(f(\bm{x})=\mathbb{E}_{\xi}[l(\bm{x};\xi)]\), where \(\xi\) is a stochastic parameter (e.g., data sample) and \(l(\bm{x};\xi)\) indicates the loss at point \(\bm{x}\) with a random sample \(\xi\). Based on the SAM update in (2), we can define stochastic SAM under this setting:

\[\begin{cases}\bm{y}_{t}=\bm{x}_{t}+\rho\frac{g(\bm{x}_{t})}{\|g(\bm{x}_{t})\|},\\ \bm{x}_{t+1}=\bm{x}_{t}-\eta\tilde{g}(\bm{y}_{t}).\end{cases}\] (3)

We define \(g(\bm{x})=\nabla_{\bm{x}}l(\bm{x};\xi)\) and \(\tilde{g}(\bm{x})=\nabla_{\bm{x}}l(\bm{x};\tilde{\xi})\). Here, \(\xi\) and \(\tilde{\xi}\) are stochastic parameters, queried from any distribution(s) which satisfies \(\mathbb{E}_{\xi}l(\bm{x};\tilde{\xi})=\mathbb{E}_{\xi}l(\bm{x};\xi)=f(\bm{x})\).

There are two popular variants of stochastic SAM, introduced in Andriushchenko and Flammarion [2]. Stochastic \(n\)-SAM algorithm refers to the update equation (3) when \(\xi\) and \(\tilde{\xi}\) are independent. In contrast, practical SAM algorithm in Foret et al. [16] employs stochastic \(m\)-SAM algorithm, which follows the update equation (3) where \(\xi\) is equal to \(\tilde{\xi}\). We will consider both versions in our theorems.

### Function Classes

We state definitions and assumptions of function classes of interest, which are fairly standard.

**Definition 2.1** (Convexity).: A function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is convex if, for any \(\bm{x},\bm{y}\in\mathbb{R}^{d}\) and \(\lambda\in[0,1]\), it satisfies \(f(\lambda\bm{x}+(1-\lambda)\bm{y})\leq\lambda f(\bm{x})+(1-\lambda)f(\bm{y})\).

**Definition 2.2** (\(\mu\)-Strong Convexity).: A differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is \(\mu\)-strongly convex if there exists \(\mu>0\) such that \(f(\bm{x})\!\geq\!f(\bm{y})\!+\!\langle\nabla f(\bm{y}),\bm{x}-\bm{y}\rangle+ \frac{\mu^{d}}{2}\|\bm{x}-\bm{y}\|^{2}\), for all \(\bm{x},\bm{y}\in\mathbb{R}^{d}\).

**Definition 2.3** (\(L\)-Lipschitz Continuity).: A function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is \(L\)-Lipschitz continuous if there exists \(L\geq 0\) such that \(\|f(\bm{x})-f(\bm{y})\|\leq L\|\bm{x}-\bm{y}\|\), for all \(\bm{x},\bm{y}\in\mathbb{R}^{d}\).

**Definition 2.4** (\(\beta\)-Smoothness).: A differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is \(\beta\)-smooth if there exists \(\beta\geq 0\) such that \(\|\nabla f(\bm{x})-\nabla f(\bm{y})\|\leq\beta\|\bm{x}-\bm{y}\|\), for all \(\bm{x},\bm{y}\in\mathbb{R}^{d}\).

Next we define an assumption considered in the analysis of stochastic SAM (3).

**Assumption 2.5** (Bounded Variance).: The gradient oracle of a differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\) has bounded variance if there exists \(\sigma\geq 0\) such that

\[\mathbb{E}_{\xi}\|\nabla f(\bm{x})-\nabla l(\bm{x};\xi)\|^{2}\leq\sigma^{2}, \quad\mathbb{E}_{\xi}\|\nabla f(\bm{x})-\nabla l(\bm{x};\tilde{\xi})\|^{2}\leq \sigma^{2},\quad\forall\bm{x}\in\mathbb{R}^{d}.\]

### SAM as GD on Virtual Loss

Convergence analysis of SAM is challenging due to the presence of normalized ascent steps. In order to provide intuitive explanations of SAM's convergence properties, we develop tools referred to as _virtual gradient map_ and _virtual loss_ in this section. These tools can provide useful intuitions when we discuss our main results.

In order to define the new tools, we first need to define Clarke subdifferential [10], whose definition below is from Theorem 6.2.5 of Borwein and Lewis [7].

**Definition 2.6** (Clarke Subdifferential).: Suppose that a function \(f:\mathbb{R}^{d}\to\mathbb{R}\) is locally Lipschitz around a point \(\bm{x}\) and differentiable on \(\mathbb{R}^{d}\setminus W\) where \(W\) is a set of measure zero. Then, the Clarke subdifferential of \(f\) at \(\bm{x}\) is \(\partial f(\bm{x})\triangleq\mathrm{cvxhull}\left\{\lim_{t\to\infty}\nabla f (\bm{x}_{t})\mid\bm{x}_{t}\to\bm{x},\bm{x}_{t}\notin W\right\}\), where \(\mathrm{cvxhull}\) denotes the convex hull of a set.

Clarke subdifferential \(\partial f(\bm{x})\) is a convex hull of all possible limits of \(\nabla f(\bm{x}_{t})\) as \(\bm{x}_{t}\) approaches \(\bm{x}\). It can be thought of as an extension of gradient to a nonsmooth function. It is also known that for a convex function \(f\), the Clarke subdifferential of \(f\) is equal to the subgradient of \(f\).

Using the definition of Clarke differential, we now define virtual gradient map and virtual loss of \(f\), which can be employed for understanding the convergence of SAM.

**Definition 2.7** (Virtual Gradient Map/Loss).: For a differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\), we define the _virtual gradient map_\(G_{f}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) of \(f\) to be \(G_{f}(\bm{x})\triangleq\nabla f\big{(}\bm{x}+\rho\frac{\nabla f(\bm{x})}{ \lVert\nabla f(\bm{x})\rVert})\). Additionally, if there exists a function \(J_{f}:\mathbb{R}^{d}\to\mathbb{R}\) whose Clarke subdifferential is well-defined and \(\partial J_{f}(\bm{x})\ni G_{f}(\bm{x})\) for all \(\bm{x}\), then we call \(J_{f}\) a _virtual loss_ of \(f\).

If a virtual loss \(J_{f}\) is well-defined for \(f\), the update of SAM (2) on \(f\) is equivalent to a (sub)gradient descent update on the virtual loss \(J_{f}(\bm{x})\), which means, \(\bm{x}_{t+1}=\bm{x}_{t}-\eta G_{f}(\bm{x}_{t})\). The reason why we have to use Clarke subdifferential to define the virtual loss is because even for a differentiable and smooth function \(f\), there are cases where the virtual gradient map \(G_{f}\) is discontinuous and the virtual loss \(J_{f}\) (if exists) is nonsmooth; see the discussion below Theorem 3.1.

Note that if the differentiable function \(f\) is one-dimensional (\(d=1\)), the virtual loss \(J_{f}(x)\) is always _well-defined_ because it can be obtained by simply (Lebesgue) integrating \(G_{f}(x)\). However, in case of multi-dimensional functions (\(d>1\)), there is no such guarantee, although \(G_{f}\) is always well-defined. We emphasize that the virtual gradient map \(G_{f}\) and virtual loss \(J_{f}\) are mainly used for a better intuitive understanding of our (non-)convergence results. In formal proofs, we use them for analysis only if \(J_{f}\) is well-defined.

Lastly, we note that Bartlett et al. [4] also employ a similar idea of virtual loss. In case of convex quadratic objective function \(f(\bm{x})\), the authors define \(\bm{u}_{t}=(-1)^{t}\bm{x}_{t}\) and formulate a (different) virtual loss \(\tilde{J}_{f}(\bm{u})\) such that a SAM update on \(f(\bm{x}_{t})\) is equivalent to a GD update on \(\tilde{J}_{f}(\bm{u}_{t})\).

## 3 Convergence Analysis Under Deterministic Settings

In this section, we present the main results on the (non-)convergence of deterministic SAM with _constant_ perturbation size \(\rho\) and gradient normalization. We study four function classes: smooth strongly convex, smooth convex, smooth nonconvex, and nonsmooth Lipschitz convex functions.

### Smooth and Strongly Convex Functions

For smooth strongly convex functions, we prove a global convergence guarantee for the best iterate.

**Theorem 3.1**.: _Consider a \(\beta\)-smooth and \(\mu\)-strongly convex function \(f\). If we run deterministic SAM starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{1}{\mu T}\max\left\{1,\log\left(\frac{\mu^{2}\Delta T^{ 2}}{\beta^{3}\rho^{2}}\right)\right\},\frac{1}{\beta}\right\}\) to minimize \(f\), we have_

\[\min_{t\in\{0,\dots,T\}}f(\bm{x}_{t})-f^{*}=\tilde{\mathcal{O}}\left(\exp\left(- \frac{\mu T}{2\beta}\right)\Delta+\frac{\beta^{6}\rho^{2}}{\mu^{5}T^{2}}\right).\]

The proof of Theorem 3.1 can be found in Appendix B.2. As for relevent existing studies, Theorem 1 of Dai et al. [13] can be adapted to establish the convergence guarantee \(f(\bm{x}_{T})-f^{*}=\tilde{\mathcal{O}}\left(\frac{1}{T}\right)\), by selecting the step size \(\eta=\min\left\{\frac{1}{\mu T}\max\left\{1,\log\left(\frac{\mu^{2}\Delta T}{\beta^{3}\rho^{2}} \right)\right\},\frac{1}{\beta}\right\}\) and employing a similar proof technique as ours. We highlight that Theorem 3.1 achieves a faster convergence rate compared to the concurrent bound by Dai et al. [13]. Moreover, Theorem 11 of Andriushchenko and Flammarion [2] proves the convergence guarantee for this function class: \(\|\bm{x}_{T}-\bm{x}^{*}\|^{2}=\mathcal{O}\left(\exp(-T)\right)\), but assuming SAM _without gradient normalization_, and the boundedness of \(\rho\). Here, we get a slower convergence rate of \(\tilde{\mathcal{O}}\left(\exp(-T)+\frac{1}{T^{2}}\right)\), but with any \(\rho>0\) and with normalization.

By viewing SAM as GD on virtual loss, we can get an intuition why SAM cannot achieve exponential convergence. Consider a smooth and strongly convex function: \(f(x)=\frac{1}{2}x^{2}\). One possible virtual loss of \(f\) is \(J_{f}(x)=\frac{1}{2}(x+\rho\mathrm{sign}(x))^{2}\), which is a _nonsmooth_ and strongly convex function, for which the exponential convergence of GD is impossible [8].

Indeed, we can show that the sublinear convergence rate in Theorem 3.1 is not an artifact of our analysis. Interestingly, the \(\tilde{\mathcal{O}}\left(\exp(-T)+\frac{1}{T^{2}}\right)\) rate given in Theorem 3.1 is _tight_ in terms of \(T\), up to logarithmic factors. Our next theorem provides a lower bound for smooth strongly convex functions.

**Theorem 3.2**.: _Suppose \(\frac{\beta}{\mu}\geq 2\). For any choice of perturbation size \(\rho\), step size \(\eta\), and initialization \(\bm{x}_{0}\), there exists a differentiable, \(\beta\)-smooth, and \(\mu\)-strongly convex function \(f\) such that_

\[\min_{t\in\{0,\dots,T\}}f(\bm{x}_{t})-f^{*}=\Omega\left(\frac{\beta^{3}\rho^{ 2}}{\mu^{2}T^{2}}\right)\]

_holds for deterministic SAM iterates._

For the proof of Theorem 3.2, refer to Appendix B.3. By comparing the rates in Theorems 3.1 and 3.2, we can see that the two bounds are tight in terms of \(T\) and \(\rho\). The bounds are a bit loose in terms of \(\beta\) and \(\mu\), but we believe that this may be partly due to our construction; in proving lower bounds, we used one-dimensional quadratic functions as worst-case examples. A more sophisticated construction may improve the tightness of the lower bound, which is left for future work.

### Smooth and Convex Functions

For smooth and convex functions, proving convergence of the function value to global optimum becomes more challenging, due to the absence of strong convexity. In fact, we can instead prove that the gradient norm converges to zero.

**Theorem 3.3**.: _Consider a \(\beta\)-smooth and convex function \(f\). If we run deterministic SAM starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{\sqrt{2\Delta}}{\sqrt{\beta^{3}\rho^{2}}T},\frac{1}{2 \beta}\right\}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}=\mathcal{O} \bigg{(}\frac{\beta\Delta}{T}+\frac{\sqrt{\Delta\beta^{3}\rho^{2}}}{\sqrt{T}} \bigg{)}.\]

The proof of Theorem 3.3 is given in Appendix B.4. As for relevant existing studies, Theorem 11 of Andriushchenko and Flammarion [2] proves the convergence guarantee for this function class: \(f(\bar{\bm{x}})-f^{*}=\mathcal{O}\left(\frac{1}{T}\right)\), where \(\bar{\bm{x}}\) indicates the averaged \(\bm{x}\) over \(T\) iterates, while assuming SAM _without gradient normalization_ and bounded \(\rho\). Here, we prove a weaker result: a convergence rate of \(\mathcal{O}(\frac{1}{\sqrt{T}})\) to _stationary points_, albeit for any \(\rho>0\) and with normalization.

One might expect that a reasonably good optimizer should converge to global minima of smooth convex functions. However, it turns out that both showing convergence to global minima and finding a non-convergence example are quite challenging in this function class.

Indeed, we later show non-convergence examples for other relevant settings. For stochastic SAM, we provide a non-convergence example (Theorem 4.4) in the same smooth and convex function class, showing that the suboptimality gap in terms of function values cannot have an upper bound, and therefore rendering convergence to global minima impossible. For deterministic SAM in nonsmooth Lipschitz convex functions, we show an example (Theorem 3.6) where convergence to the global minimum is possible only up to a certain distance proportional to \(\rho\).

Given these examples, we suspect that there may also exist a non-convergence example for the determinstic SAM in this smooth convex setting. We leave settling this puzzle to future work.

### Smooth and Nonconvex Functions

Existing studies prove that SAM (and its variants) with decaying or sufficiently small perturbation size \(\rho\) converges to stationary points for smooth nonconvex functions [2, 18, 27, 31, 35]. Unfortunately, with constant perturbation size \(\rho\), SAM exhibits a different convergence behavior: it does not converge all the way to stationary points.

**Theorem 3.4**.: _Consider a \(\beta\)-smooth function \(f\) satisfying \(f^{*}=\inf_{\bm{x}}f(\bm{x})>-\infty\). If we run deterministic SAM starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\frac{1}{\beta}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\mathcal{O }\left(\frac{\beta\Delta}{T}\right)+\beta^{2}\rho^{2}.\]

Refer to the Appendix B.5 for the proof of Theorem 3.4. For a comparison, Theorem 9 of Andriushchenko and Hammarion [2] proves the convergence for this function class: \(\frac{1}{T}\sum_{t=0}^{T}\|\nabla f(\bm{x}_{t})\|^{2}=\mathcal{O}\left(\frac{1 }{T}\right)\), but again assuming SAM _without gradient normalization_, and boundedness of \(\rho\). Our Theorem 3.4 guarantees \(\mathcal{O}(\frac{1}{T})\) convergence up to an additive factor \(\beta^{2}\rho^{2}\).

One might speculate that the undesirable additive factor \(\beta^{2}\rho^{2}\) is an artifact of our analysis. The next theorem presents an example which proves that this extra term is in fact unavoidable.

**Theorem 3.5**.: _For any \(\rho>0\) and \(\eta\leq\frac{1}{\beta}\), there exists a \(\beta\)-smooth and \(\Theta(\beta\rho)\)-Lipschitz continuous function such that, if deterministic SAM is initialized at a point \(\bm{x}_{0}\) sampled from a continuous probability distribution, then deterministic SAM converges to a nonstationary point, located at a distance of \(\Omega(\rho)\) from a stationary point, with probability \(1\)._

Here we present a brief outline for the proof of Theorem 3.5. For a given \(\rho\), consider a one-dimensional function \(f(x)=\frac{9\beta\rho^{2}}{25\pi^{2}}\sin\left(\frac{5\pi}{3\rho}x\right)\). Figure 2(a) demonstrates the virtual loss for this example. By examining the virtual loss \(J_{f}\) and its stationary points, we observe that SAM iterates \(x_{t}\) converge to a non-stationary point, located at a distance of \(\Omega(\rho)\) from the stationary point. This means that the limit point of SAM will have gradient norm of order \(\Omega(\beta\rho)\), thereby proving that the additive factor in Theorem 3.4 is tight in terms of \(\rho\). A detailed analysis of Theorem 3.5 is provided in Appendix B.6.

Remark: why do we ignore \(\eta=\Omega(\frac{1}{\beta})\)?As the reader may have noticed, Theorem 3.5 only considers the case \(\eta=\mathcal{O}(\frac{1}{\beta})\), and hence does not show that the additive factor is inevitable for _any_ choice of \(\eta>0\). However, one can notice that if \(\eta=\Omega(\frac{1}{\beta})\), then we can consider a one-dimensional \(\Omega(\beta)\)-strongly convex quadratic function and show that the SAM iterates blow up to infinity. For the same reason, in our other non-convergence results (Theorems 4.2 and 4.4), we only focus on \(\eta=\mathcal{O}(\frac{1}{\beta})\).

### Nonsmooth Lipschitz Convex Functions

Previous theorems study convergence of SAM assuming smoothness. The next theorem shows an example where SAM on a nonsmooth convex function converges only up to \(\Omega(\rho)\) distance from the global minimum \(\bm{x}^{*}\). This means that for constant perturbation size \(\rho\), there exist _convex_ functions that prevent SAM from converging to global minima. In Appendix B.7, we prove the following:

**Theorem 3.6**.: _For any \(\rho>0\) and \(\eta<\frac{7\rho}{4}\), there exists a nonsmooth Lipschitz convex function \(f\) such that for some initialization, deterministic SAM converges to suboptimal points located at a distance of \(\Omega(\rho)\) from the global minimum._

## 4 Convergence Analysis Under Stochastic Settings

In this section, we present the main results on the convergence analysis of stochastic SAM, again with _time-invariant (constant)_ perturbation size \(\rho\) and _gradient normalization_. We consider both types of stochastic SAM: \(n\)-SAM and \(m\)-SAM, defined in Section 2.2. We study four types of function classes: smooth strongly convex, smooth convex, smooth nonconvex, and smooth Lipschitz nonconvex functions, under the assumption that the gradient oracle has bounded variance of \(\sigma^{2}\) (Assumption 2.5). Our results in this section reveal that stochastic SAM exhibits different convergence properties compared to deterministic SAM.

### Smooth and Strongly Convex Functions

Theorem 3.1 shows the convergence of deterministic SAM to global optima, for smooth strongly convex functions. Unlike this result, we find that under stochasticity and constant perturbation size \(\rho\), both \(n\)-SAM and \(m\)-SAM ensure convergence _only up to an additive factor \(\mathcal{O}(\rho^{2})\)_.

**Theorem 4.1**.: _Consider a \(\beta\)-smooth, \(\mu\)-strongly convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(x_{0}\) with any perturbation size \(\rho>0\) and step size\[\min\left\{\frac{1}{\mu T}\cdot\max\left\{1,\log\left(\frac{\mu^{2} \Delta T}{\beta[\sigma^{2}-\beta^{2}\rho^{2}]_{+}}\right)\right\},\frac{1}{2 \beta}\right\}\text{ to minimize }f\text{, we have}\] \[\mathbb{E}f(\bm{x}_{T})-f^{*}\leq\tilde{\mathcal{O}}\left(\exp \left(-\frac{\mu T}{2\beta}\right)\Delta+\frac{\beta[\sigma^{2}-\beta^{2}\rho^ {2}]_{+}}{\mu^{2}T}\right)+\frac{2\beta^{2}\rho^{2}}{\mu}.\]

_Under \(m\)-SAM, additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), the inequality continues to hold._

For the proof, please refer to Appendix C.2. Theorem 4.1 provides a convergence rate of \(\tilde{\mathcal{O}}(\frac{1}{T})\) to global minima, but only up to suboptimality gap \(\frac{2\beta^{2}\rho^{2}}{\mu}\). If \(\sigma\leq\beta\rho\), then Theorem 4.1 becomes:

\[\mathbb{E}f(\bm{x}_{T})-f^{*}\leq\tilde{\mathcal{O}}\left(\exp\left(-\frac{\mu T }{2\beta}\right)\Delta\right)+\frac{2\beta^{2}\rho^{2}}{\mu},\]

thereby showing a convergence rate of \(\mathcal{O}\left(\exp(-T)\right)\) modulo the additive factor.

For relevant existing studies, Theorem 2 of Andriushchenko and Flammation [2] proves the convergence guarantee for smooth Polyak-Lojasiewicz functions: \(\mathbb{E}f(\bm{x}_{T})-f^{*}=\mathcal{O}\left(\frac{1}{T}\right)\), but assuming stochastic SAM _without gradient normalization_, and perturbation size \(\rho\) decaying with \(t\). In contrast, our analysis shows that the convergence property can be different with normalization and constant \(\rho\).

The reader might be curious if the additional \(\mathcal{O}(\rho^{2})\) term can be removed. Our next theorem proves that in the case of high gradient noise (\(\sigma>\beta\rho\)), \(m\)-SAM with constant perturbation size \(\rho\) cannot converge to global optima beyond a suboptimality gap \(\Omega(\rho^{2})\), when the component function \(l(\bm{x};\xi)\) is smooth for any \(\xi\). Hence, the additional term in Theorem 4.1 is _unavoidable_, at least for the more practical version \(m\)-SAM.

**Theorem 4.2**.: _For any \(\rho>0,\beta>0,\sigma>\beta\rho\) and \(\eta\leq\frac{3}{10\beta}\), there exists a \(\beta\)-smooth and \(\frac{\beta}{5}\)-strongly convex function \(f\) satisfying the following. (1) The function \(f\) satisfies Assumption 2.5. (2) The component functions \(l(\cdot;\xi)\) of \(f\) are \(\beta\)-smooth for any \(\xi\). (3) If we run \(m\)-SAM on \(f\) initialized inside

Figure 2: Examples of virtual loss plot for deterministic and stochastic SAM. The graph drawn in green indicates \(f\), and the graph drawn in blue indicates \(J_{f}\) (or \(\mathbb{E}J_{f}\)). Red arrows indicate the (expected) directions of SAM update. The (expected) updates are directed to red stars. (a) \(f\) and \(J_{f}\) in Theorem 3.5. (b) \(f\) and its component functions \(f^{(1)}\), \(f^{(2)}\) in Theorem 4.2. (c) \(\mathbb{E}J_{f}\) and its component functions \(J_{f^{(1)}}\), \(J_{f^{(2)}}\) in Theorem 4.2. (d) \(f\) and its component functions \(f^{(1)}\), \(f^{(2)}\) in Theorem 4.4. (e) \(\mathbb{E}J_{f}\) and its component functions \(J_{f^{(1)}}\), \(J_{f^{(2)}}\) in Theorem 4.4. For the simulation results of SAM trajectories on these functions, refer to Appendix D.

a certain interval, then any arbitrary weighted average \(\bar{\bm{x}}\) of the iterates \(\bm{x}_{0},\bm{x}_{1},\ldots\) must satisfy \(\mathbb{E}[f(\bar{\bm{x}})-f^{*}]\geq\Omega(\rho^{2})\)._

Here we provide a brief outline for Theorem 4.2. The in-depth analysis is provided in Appendix C.3. Given \(\rho>0\), \(\beta>0\), \(\sigma>\beta\rho\), we consider a one-dimensional quadratic function \(f(x)\) whose component function \(l(x;\xi)\) is carefully chosen to satisfy the following for \(x\in\left[\frac{\rho}{6},\frac{13\rho}{6}\right]\):

\[f(x)=\mathbb{E}[l(x;\xi)]=\frac{\beta}{10}x^{2},\quad l(x;\xi)=\begin{cases}- \frac{\beta}{10}\left(x-\frac{7\rho}{6}\right)^{2},&\text{with probability }\frac{2}{3}\\ \frac{3\beta}{10}x^{2}+\frac{\beta}{5}\left(x-\frac{7\rho}{6}\right)^{2},& \text{otherwise.}\end{cases}\]

For values of \(x\) outside this interval \(\left[\frac{\rho}{6},\frac{13\rho}{6}\right]\), each component function \(l(x;\xi)\) takes the form of a strongly convex quadratic function. Figures 2(b) and 2(c) illustrate the original and virtual loss function plots of \(l(x;\xi)\). The _local concavity_ of component function plays a crucial role in making an attracting basin in the virtual loss, and this leads to an interval \(\left[\frac{\rho}{6},\frac{13\rho}{6}\right]\) bounded away from the global minimum \(0\), from which \(m\)-SAM iterates cannot escape.

For this scenario, we obtain \(f(x_{t})-f^{*}=\Omega(\rho^{2})\) and \(\|\nabla f(x_{t})\|^{2}=\Omega(\rho^{2})\) for all iterates. From this, we can realize that the additive factor in Theorem 4.1 for \(m\)-SAM is unavoidable in the \(\sigma>\beta\rho\) regime, and tight in terms of \(\rho\). Moreover, the proof of Theorem 4.2 in Appendix C.3 reveals that even in the \(\sigma\leq\beta\rho\) case, an analogous example gives \(\|x_{t}-x^{*}\|=\Omega(\rho)\) for all iterates; hence, \(m\)-SAM fails to converge all the way to the global minimum in the small \(\sigma\) regime as well.

### Smooth and Convex Functions

We now move on to smooth convex functions and investigate the convergence guarantees of stochastic SAM for this function class. As can be guessed from Theorem 3.3, our convergence analysis in this section focuses on finding stationary points. Below, we provide a bound that ensures convergence to stationary points up to an additive factor.

**Theorem 4.3**.: _Consider a \(\beta\)-smooth, convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(x_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{\sqrt{\Delta}}{\sqrt{\beta[\sigma^{2}-\beta^{2}\rho^{2 }]_{+}T}},\frac{1}{2\beta}\right\}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}= \mathcal{O}\bigg{(}\frac{\beta\Delta}{T}+\frac{\sqrt{\beta[\sigma^{2}-\beta^{ 2}\rho^{2}]_{+}\Delta}}{\sqrt{T}}\bigg{)}+4\beta^{2}\rho^{2}.\]

_Under \(m\)-SAM, additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), the inequality continues to hold._

The proof of Theorem 4.3 can be found in Appendix C.4. Theorem 4.3 obtains a bound of \(\mathcal{O}(\frac{1}{\sqrt{T}})\) modulo an additive factor \(4\beta^{2}\rho^{2}\). Similar to Theorem 4.1, if \(\sigma\leq\beta\rho\), then Theorem 4.3 reads

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}= \mathcal{O}\bigg{(}\frac{\beta\Delta}{T}\bigg{)}+4\beta^{2}\rho^{2},\]

hence showing a convergence rate of \(\mathcal{O}(\frac{1}{T})\) modulo the additive factor. Since the non-convergence example in Theorem 4.2 provides a scenario that \(\mathbb{E}\|\nabla f(x_{t})\|^{2}=\Omega(\rho^{2})\) for all \(t\), we can see that the extra term is inevitable and also tight in terms of \(\rho\).

Theorem 4.3 sounds quite weak, as it only proves convergence to a stationary point only up to an extra term. One could anticipate that stochastic SAM may actually converge to global minima of smooth convex functions modulo the unavoidable additive factor. However, as briefly mentioned in Section 3.2, the next theorem presents a counterexample illustrating that ensuring convergence to global minima, even up to an additive factor, is impossible for \(m\)-SAM.

**Theorem 4.4**.: _For any \(\rho>0\), \(\beta>0\), \(\sigma>0\), and \(\eta\leq\frac{1}{\beta}\), there exists a \(\beta\)-smooth and convex function \(f\) satisfying the following. (1) The function \(f\) satisfies Assumption 2.5. (2) The component functions \(l(\cdot;\xi)\) of \(f\) are \(\beta\)-smooth for any \(\xi\). (3) If we run \(m\)-SAM on \(f\) initialized inside a certain interval, then any arbitrary weighted average \(\bar{\bm{x}}\) of the iterates \(\bm{x}_{0},\bm{x}_{1},\ldots\) must satisfy \(\mathbb{E}[f(\bar{\bm{x}})-f^{*}]\geq C\), and the suboptimality gap \(C\) can be made arbitrarily large and independent of the parameter \(\rho\)._

Here, we present the intuitions of the proof for Theorem 4.4. As demonstrated in Figures 2(d) and 2(e), the _local concavity_ of the component function significantly influences the formation of attractingbasins in its virtual loss, thereby creating a region from which the \(m\)-SAM updates get stuck inside the basin forever.

Also note that we can construct the function to form the basin at any point with arbitrary large function value (and hence large suboptimality gap). Therefore, establishing an upper bound on the convergence of function value becomes impossible in smooth convex functions. A detailed analysis for the non-convergence example is presented in Appendix C.5.

### Smooth and Nonconvex Functions

We now study smooth nonconvex functions. Extending Theorem 3.4, we can show the following bound for stochastic \(n\)-SAM.

**Theorem 4.5**.: _Consider a \(\beta\)-smooth function \(f\) satisfying \(f^{*}=\inf_{x}f(\bm{x})>-\infty\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{1}{2\beta},\frac{\sqrt{\Delta}}{\sqrt{\beta\sigma^{2}T}}\right\}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2} \leq\mathcal{O}\left(\frac{\beta\Delta}{T}+\frac{\sqrt{\beta\sigma^{2}\Delta} }{\sqrt{T}}\right)+\beta^{2}\rho^{2}.\]

The proof of Theorem 4.5 is provided in Appendix C.6. Notice that the non-convergence example presented in Theorem 3.5 (already) illustrates a scenario where \(\mathbb{E}\|\nabla f(x_{t})\|^{2}=\bar{\Omega}(\rho^{2})\), thereby confirming the tightness of additive factor in terms of \(\rho\).

The scope of applicability for Theorem 4.5 is limited to \(n\)-SAM. Compared to \(n\)-SAM, \(m\)-SAM employs a stronger assumption where \(\xi=\tilde{\xi}\). By imposing an additional Lipschitzness condition on the function \(f\), \(m\)-SAM leads to a similar but different convergence result.

**Theorem 4.6**.: _Consider a \(\beta\)-smooth, \(L\)-Lipschitz continuous function \(f\) satisfying \(f^{*}=\inf_{x}f(\bm{x})>-\infty\), and assume Assumption 2.5. Additionally assume \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\). Under \(m\)-SAM, starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\frac{\sqrt{\Delta}}{\sqrt{\beta(\sigma^{2}+L^{2})T}}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\left[(\|\nabla f(\bm{x}_{t})\| -\beta\rho)^{2}\right]\leq\mathcal{O}\left(\frac{\sqrt{\beta\Delta(\sigma^{2 }+L^{2})}}{\sqrt{T}}\right)+5\beta^{2}\rho^{2}.\]

**Corollary 4.7**.: _Under the setting of Theorem 4.6, we get_

\[\min_{t\in\{0,\dots,T\}}\{\mathbb{E}\|\nabla f(\bm{x}_{t})\|\}\leq\mathcal{O} \left(\frac{\left(\beta\Delta(\sigma^{2}+L^{2})\right)^{1/4}}{T^{1/4}}\right)+ \left(1+\sqrt{5}\right)\beta\rho.\]

The proofs for Theorem 4.6 and Corollary 4.7 are given in Appendix C.7. Since Theorem 3.5 presents an example where \(\mathbb{E}\|\nabla f(\bm{x}_{t})\|=\Omega(\rho)\) and \(\mathbb{E}(\|\nabla f(\bm{x}_{t})\|-\beta\rho)^{2}=\Omega(\rho^{2})\), we can verify that the additive factors in Theorem 4.6 and Corollary 4.7 are tight in terms of \(\rho\).

As for previous studies, Theorems 2, 12, and 18 of Andriushchenko and Flammarion [2] prove the convergence of \(n\),\(m\)-SAM for smooth nonconvex functions: \(\frac{1}{T}\sum_{t=0}^{T}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}=\mathcal{O}( \frac{1}{\sqrt{T}})\), but assuming SAM _without gradient normalization_, and _sufficiently small_\(\rho\). For \(m\)-SAM on smooth Lipschitz nonconvex functions, Theorem 1 of Mi et al. [27] proves \(\frac{1}{T}\sum_{t=0}^{T}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}=\tilde{ \mathcal{O}}(\frac{1}{\sqrt{T}})\), while assuming _decaying_\(\rho\). From our results, we demonstrate that such full convergence results are impossible for practical versions of stochastic SAM.

## 5 Conclusions

This paper studies the convergence properties of SAM, under constant \(\rho\) and with gradient normalization. We establish convergence guarantees of deterministic SAM for smooth and (strongly) convex functions. To our surprise, we discover scenarios in which deterministic SAM (for smooth nonconvex functions) and stochastic \(m\)-SAM (for all function class considered) converge only up to _unavoidable_ additive factors proportional to \(\rho^{2}\). Our findings emphasize the drastically different characteristics of SAM with vs. without decaying perturbation size. Establishing tighter bounds in terms of \(\beta\) and \(\mu\), or searching for a non-convergence example that applies to \(n\)-SAM might be interesting future research directions.

#### Acknowledgments

This paper was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)) funded by the Korea government (MSIT), two National Research Foundation of Korea (NRF) grants (No. NRF-2019R1A5A1028324, RS-2023-00211352) funded by the Korea government (MSIT), and a grant funded by Samsung Electronics Co., Ltd.

## References

* [1]A. Agarwala and Y. N. Dauphin (2023) Sam operates far from home: eigenvalue regularization as a dynamical phenomenon. arXiv preprint arXiv:2302.08692. Cited by: SS1.
* [2]M. Andriushchenko and N. Flammarion (2022) Towards understanding sharpness-aware minimization. In International Conference on Machine Learning, pp. 639-668. Cited by: SS1.
* [3]D. Bahri, H. Mobahi, and Y. Tay (2021) Sharpness-aware minimization improves language model generalization. arXiv preprint arXiv:2110.08529. Cited by: SS1.
* [4]P. L. Bartlett, P. M. Long, and O. Bousquet (2022) The dynamics of sharpness-aware minimization: bouncing across ravines and drifting towards wide minima. arXiv preprint arXiv:2210.01513. Cited by: SS1.
* [5]K. Behdin, Q. Song, A. Gupta, A. Acharya, D. Durfee, B. Ocejo, S. Keerthi, and R. Mazumder (2023) ImSA: micro-batch-averaged sharpness-aware minimization. arXiv preprint arXiv:2302.09693. Cited by: SS1.
* [6]K. Behdin and R. Mazumder (2023) Sharpness-aware minimization: an implicit regularization perspective. arXiv preprint arXiv:2302.11836. Cited by: SS1.
* [7]J. M. Borwein and A. S. Lewis (2010) Convex analysis and nonlinear optimization: theory and examples. Cited by: SS1.
* [8]S. Bubeck et al. (2015) Convex optimization: algorithms and complexity. Foundations and Trends(r) in Machine Learning8 (3-4), pp. 231-357. Cited by: SS1.
* [9]X. Chen, C. Hsieh, and B. Gong (2021) When vision transformers outperform resnets without pre-training or strong data augmentations. arXiv preprint arXiv:2106.01548. Cited by: SS1.
* [10]F. H. Clarke, Y. S. Ledyaev, R. J. Stern, and P. R. Wolenski (2008) Nonsmooth analysis and control theory. Vol. 178, Springer Science & Business Media. Cited by: SS1.
* [11]J. M. Cohen, S. Kaur, Y. Li, J. Z. Kolter, and A. Talwalkar (2021) Gradient descent on neural networks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065. Cited by: SS1.
* [12]E. M. Compagnoni, A. Orvieto, L. Biggio, H. Kersting, F. N. Proske, and A. Lucchi (2023) An sde for modeling sam: theory and insights. arXiv preprint arXiv:2301.08203. Cited by: SS1.
* [13]Y. Dai, K. Ahn, and S. Sra (2023) The crucial role of normalization in sharpness-aware minimization. arXiv preprint arXiv:2305.15287. Cited by: SS1.
* [14]L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio (2017) Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pp. 1019-1028. Cited by: SS1.
* [15]J. Du, H. Yan, J. Feng, J. T. Zhou, L. Zhen, R. S. M. Goh, and V. Y. Tan (2021) Efficient sharpness-aware minimization for improved training of neural networks. arXiv preprint arXiv:2110.03141. Cited by: SS1.
* [16]P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur (2020) Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412. Cited by: SS1.

[MISSING_PAGE_POST]

* [18] W. Jiang, H. Yang, Y. Zhang, and J. Kwok. An adaptive policy to employ sharpness-aware minimization. In _The Eleventh International Conference on Learning Representations_, 2022.
* [19] Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them. _arXiv preprint arXiv:1912.02178_, 2019.
* [20] J. Kaddour, L. Liu, R. Silva, and M. Kusner. When do flat minima optimizers work? In _Advances in Neural Information Processing Systems_, 2022.
* [21] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep learning: Generalization gap and sharp minima. _arXiv preprint arXiv:1609.04836_, 2016.
* [22] H. Kim, J. Park, Y. Choi, and J. Lee. Stability analysis of sharpness-aware minimization, 2023. URL https://arxiv.org/abs/2301.06308.
* [23] H. Kim, J. Park, Y. Choi, W. Lee, and J. Lee. Exploring the effect of multi-step ascent in sharpness-aware minimization. _arXiv preprint arXiv:2302.10181_, 2023.
* [24] J. Kwon, J. Kim, H. Park, and I. K. Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* [25] H. Lee, H. Cho, H. Kim, D. Gwak, J. Kim, J. Choo, S.-Y. Yun, and C. Yun. Enhancing generalization and plasticity for sample efficient reinforcement learning. _arXiv preprint arXiv:2306.10711_, 2023.
* [26] Y. Liu, S. Mai, X. Chen, C.-J. Hsieh, and Y. You. Towards efficient and scalable sharpness-aware minimization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12360-12370, 2022.
* [27] P. Mi, L. Shen, T. Ren, Y. Zhou, X. Sun, R. Ji, and D. Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. _arXiv preprint arXiv:2210.05177_, 2022.
* [28] T. Mollenhoff and M. E. Khan. Sam as an optimal relaxation of bayes. _arXiv preprint arXiv:2210.01620_, 2022.
* [29] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. _Advances in neural information processing systems_, 30, 2017.
* [30] Y. Shi, Y. Liu, K. Wei, L. Shen, X. Wang, and D. Tao. Make landscape flatter in differentially private federated learning. _arXiv preprint arXiv:2303.11242_, 2023.
* [31] H. Sun, L. Shen, Q. Zhong, L. Ding, S. Chen, J. Sun, J. Li, G. Sun, and D. Tao. Adasam: Boosting sharpness-aware minimization with adaptive learning rate and momentum for training deep neural networks. _arXiv preprint arXiv:2303.00565_, 2023.
* [32] P. Wang, Z. Zhang, Z. Lei, and L. Zhang. Sharpness-aware gradient matching for domain generalization. _arXiv preprint arXiv:2303.10353_, 2023.
* [33] K. Wen, T. Ma, and Z. Li. How does sharpness-aware minimization minimize sharpness? _arXiv preprint arXiv:2211.05729_, 2022.
* [34] X. Zhang, R. Xu, H. Yu, H. Zou, and P. Cui. Gradient norm aware minimization seeks first-order flatness and improves generalization. _arXiv preprint arXiv:2303.03108_, 2023.
* [35] J. Zhuang, B. Gong, L. Yuan, Y. Cui, H. Adam, N. Dvornek, S. Tatikonda, J. Duncan, and T. Liu. Surrogate gap minimization improves sharpness-aware training. _arXiv preprint arXiv:2203.08065_, 2022.

###### Contents

* 1 Introduction
	* 1.1 Summary of Our Contributions
	* 1.2 Related Works: Convergence Analysis of SAM
	* 1.3 Notation
* 2 Sharpness-Aware Minimization: Preliminaries and Intuitions
	* 2.1 Sharpness-Aware Minimization (SAM)
	* 2.2 SAM under Stochastic Settings
	* 2.3 Function Classes
	* 2.4 SAM as GD on Virtual Loss
* 3 Convergence Analysis Under Deterministic Settings
	* 3.1 Smooth and Strongly Convex Functions
	* 3.2 Smooth and Convex Functions
	* 3.3 Smooth and Nonconvex Functions
	* 3.4 Nonsmooth Lipschitz Convex Functions
* 4 Convergence Analysis Under Stochastic Settings
	* 4.1 Smooth and Strongly Convex Functions
	* 4.2 Smooth and Convex Functions
	* 4.3 Smooth and Nonconvex Functions
* 5 Conclusions
* A More Details on the Related Works of SAM
* B Proofs for (Non-)Convergence of full-batch SAM
* B.1 Additional Function Class and Important Lemmas
* B.2 Convergence Proof for Smooth and Strongly Convex Functions (Proof of Theorem 3.1)
* B.3 Lower Bound Proof for Smooth and Strongly Convex Functions (Proof of Theorem 3.2)
* B.4 Convergence Proof for Smooth and Convex Functions (Proof of Theorem 3.3)
* B.5 Convergence Proof for Smooth and Nonconvex Functions (Proof of Theorem 3.4)
* B.6 Non-convergence for a Smooth and Nonconvex Function (Proof of Theorem 3.5)
* B.7 Non-convergence for a Nonsmooth and Convex Functions (Proof of Theorem 3.6)
* C Proofs for (Non-)Convergence of Stochastic SAM
* C.1 Important Lemmas Regarding Stochastic SAM
* C.2 Convergence Proof for Smooth and Strongly Convex Functions (Proof of Theorem 4.1)
* C.3 Non-Convergence of \(m\)-SAM for a Smooth and Strongly Convex Function (Proof of Theorem 4.2)
* C.4 Convergence Proof for Smooth and Convex Functions (Proof of Theorem 4.3)

* 5 Non-Convergence of \(m\)-SAM for a Smooth and Convex Function (Proof of Theorem 4.4)
* 6 Convergence Proof for Smooth and Nonconvex Function under \(n\)-SAM (Proof of Theorem 4.5)
* 7 Convergence Proof for Smooth Lipschitz Nonconvex Function under \(m\)-SAM (Proof of Theorem 4.6 and Corollary 4.7)
* D Simulation Results of SAM on Example Functions
More Details on the Related Works of SAM

Sharpness and GeneralizationHochreiter and Schmidhuber [17] propose an algorithm for finding low-complexity models with high generalization performance by finding flat minima. Keskar et al. [21] show that the poor generalization performance of large-batch training is due to the fact that large-batch training tends to converge to sharp minima. Jiang et al. [19] empirically show correlation between sharpness and generalization performance.

Motivated by prior studies, Foret et al. [16] propose Sharpness-Aware Minimization (SAM) that focuses on finding flat minima by aiming to (approximately) minimize \(f^{\mathrm{SAM}}(\bm{x})=\max_{\|\bm{\epsilon}\|\leq\rho}f(\bm{x}+\bm{ \epsilon})\) instead of \(f\). Empirical results [3, 9, 16, 20, 25] show outstanding generalization performance of SAM on various tasks and models, including recent state-of-the-art models such as ViTs, MLP-Mixers, and T5.

Other Theoretical Properties of SAM. Bartlett et al. [4] prove that for quadratic functions, SAM dynamics oscillate and align to the eigenvector corresponding to the largest eigenvalue of Hessian, and show interpretation of SAM dynamics as GD on a "virtual loss". Wen et al. [33] prove that for sufficiently small perturbation size \(\rho\) of SAM, its trajectory follows a sharpness reduction flow which minimizes the maximum eigenvalue of Hessian. Dai et al. [13] examine the properties of SAM with and without normalization. They specifically focus on the stabilization and "drift-along-minima" effects observed in SAM with normalization, which are not present in SAM without normalization. Compagnoni et al. [12] derive a continuous time SDE model for SAM with and without normalization, and employ the model to explain why SAM has a preference for flat minima. Furthermore, they demonstrate that SAM with and without normalization exhibit distinct implicit regularization properties, especially for \(\rho=\mathcal{O}(\sqrt{\eta})\).

Agarwala and Dauphin [1] prove that SAM produces an Edge Of Stability [11] stabilization effect, at a lower eigenvalue than gradient descent, under quadratic function settings. Kim et al. [22] show that saddle point acts as an attractor in SAM dynamics, and stochastic SAM takes more time to escape saddle point than stochastic gradient descent. Behdin and Mazumder [5] study the implicit regularization perspective of SAM, and provide a theoretical explanation of high generalization performance of SAM.

While Agarwala and Dauphin [1], Behdin and Mazumder [5], Kim et al. [22] provide useful insights on theoretical aspects of SAM, we point out that all proofs are restricted to SAM _without gradient normalization_ in ascent steps. Considering the vastly different behaviors of SAM with and without normalization (Figure 1), it is not immediately clear whether these insights carry over to the practical version.

Proofs for (Non-)Convergence of full-batch SAM

In this section, we show detailed proofs and explanations regarding convergence of SAM with constant \(\rho\). The SAM algorithm we consider is defined as a two-step method:

\[\begin{cases}\bm{y}_{t}=\bm{x}_{t}+\rho\frac{\nabla f(\bm{x}_{t})}{\|\nabla f(\bm{ x}_{t})\|},\\ \bm{x}_{t+1}=\bm{x}_{t}-\eta\nabla f(\bm{y}_{t}).\end{cases}\]

### Additional Function Class and Important Lemmas

In this section, we define an additional function class to use in convergence proofs. We also state and prove a few lemmas that are used in our theorem proofs.

**Definition B.1** (Polyak-Lojasiewicz).: A function \(f:\mathbb{R}^{d}\to\mathbb{R}\) satisfies \(\mu\)-PL condition if there exists \(\mu>0\) such that,

\[\frac{1}{2}\|\nabla f(\bm{x})\|^{2}\geq\mu(f(\bm{x})-f^{*})\] (4)

for all \(\bm{x}\in\mathbb{R}^{d}\).

It is well-known that \(\mu\)-strongly convex functions are \(\mu\)-PL, but not vice versa.

**Lemma B.2**.: _For a differentiable and \(\mu\)-strongly convex function \(f\), we have_

\[\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_{t})-\nabla f(\bm{x}_{t})\rangle \geq\mu\rho\|\nabla f(\bm{x}_{t})\|.\]

_For a differentiable and convex function \(f\), the inequality continues to hold with \(\mu=0\)._

Proof.: If a differentiable function \(f\) is \(\mu\)-strongly convex, then its gradient map \(\bm{x}\mapsto\nabla f(\bm{x})\) is \(\mu\)-strongly monotone, i.e.,

\[\langle\bm{y}-\bm{x},\nabla f(\bm{y})-\nabla f(\bm{x})\rangle\geq\mu\|\bm{y}- \bm{x}\|^{2},\quad\forall\bm{x},\bm{y}\in\mathbb{R}^{d}.\]

Using this fact, we have

\[\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_{t})-\nabla f(\bm{x}_ {t})\rangle =\frac{\|\nabla f(\bm{x}_{t})\|}{\rho}\left\langle\rho\frac{\nabla f (\bm{x}_{t})}{\|\nabla f(\bm{x}_{t})\|},\nabla f(\bm{y}_{t})-\nabla f(\bm{x}_{ t})\right\rangle\] \[=\frac{\|\nabla f(\bm{x}_{t})\|}{\rho}\left\langle\bm{y}_{t}-\bm{ x}_{t},\nabla f(\bm{y}_{t})-\nabla f(\bm{x}_{t})\right\rangle\] \[\geq\frac{\mu\|\nabla f(\bm{x}_{t})\|}{\rho}\|\bm{y}_{t}-\bm{x}_{ t}\|^{2}=\mu\rho\|\nabla f(\bm{x}_{t})\|,\]

and this finishes the proof. 

**Lemma B.3**.: _For a \(\beta\)-smooth and \(\mu\)-strongly convex function, with step size \(\eta\leq\frac{1}{2\beta}\), we have_

\[f(\bm{x}_{t+1})\leq f(\bm{x}_{t})-\frac{\eta}{2}\|\nabla f(\bm{x}_{t})\|^{2}- \frac{\eta\mu\rho}{2}\|\nabla f(\bm{x}_{t})\|+\frac{\eta^{2}\beta^{3}\rho^{2}} {2}.\]

_For a \(\beta\)-smooth and convex function, the inequality continues to hold with \(\mu=0\)._

Proof.: Starting from the definition of \(\beta\)-smoothness, we have

\[f(\bm{x}_{t+1}) \leq f(\bm{x}_{t})+\langle\nabla f(\bm{x}_{t}),\bm{x}_{t+1}-\bm{ x}_{t}\rangle+\frac{\beta}{2}\|\bm{x}_{t+1}-\bm{x}_{t}\|^{2}\] \[=f(\bm{x}_{t})-\eta\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_{t })\rangle+\frac{\eta^{2}\beta}{2}\|\nabla f(\bm{y}_{t})\|^{2}\] \[=f(\bm{x}_{t})-\eta\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_{t })-\nabla f(\bm{x}_{t})+\nabla f(\bm{x}_{t})\rangle\] \[\quad+\frac{\eta^{2}\beta}{2}\|\nabla f(\bm{y}_{t})-\nabla f(\bm{ x}_{t})+\nabla f(\bm{x}_{t})\|^{2}\] \[=f(\bm{x}_{t})-\eta\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_{t })-\nabla f(\bm{x}_{t})\rangle-\eta\|\nabla f(\bm{x}_{t})\|^{2}+\frac{\eta^{2 }\beta}{2}\|\nabla f(\bm{y}_{t})-\nabla f(\bm{x}_{t})\|^{2}\]\[f(\bm{x}_{t+1})-f^{*}\leq(1-\eta\mu)(f(\bm{x}_{t})-f^{*})-\frac{\eta\mu\rho}{2}\| \nabla f(\bm{x}_{t})\|+\frac{\eta^{2}\beta^{3}\rho^{2}}{2}.\] (5)

Case 1: when \(\|\nabla f(\bm{x}_{t})\|\) remains large.Let us now consider the first case, where \(\|\nabla f(\bm{x}_{t})\|\geq\frac{\eta\beta^{3}\rho}{\mu}\) holds for all \(t=0,\dots,T-1\). In this case, (5) becomes

\[f(\bm{x}_{t+1})-f^{*}\leq(1-\eta\mu)(f(\bm{x}_{t})-f^{*}),\]

which holds for all \(t=0,\dots,T-1\). Unrolling the inequality, we obtain

\[f(\bm{x}_{T})-f^{*}\leq(1-\eta\mu)^{T}(f(\bm{x}_{0})-f^{*}).\]

Case 2: when some \(\|\nabla f(\bm{x}_{t})\|\) is small.In the other case where there exists \(t\) such that \(\|\nabla f(\bm{x}_{t})\|\leq\frac{\eta\beta^{3}\rho}{\mu}\), notice from \(\mu\)-PL inequality (4) that

\[f(\bm{x}_{t})-f^{*}\leq\frac{1}{2\mu}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{ \eta^{2}\beta^{6}\rho^{2}}{2\mu^{3}}.\]Therefore, combining the two cases, it is guaranteed that

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*}\leq(1-\eta\mu)^{T}\Delta+\frac{\eta ^{2}\beta^{6}\rho^{2}}{2\mu^{3}}.\] (6)

We now elaborate how the choice of step size \(\eta=\min\left\{\frac{1}{\mu T}\max\left\{1,\log\left(\frac{\mu^{5}\Delta T^{2} }{\beta^{6}\rho^{2}}\right)\right\},\frac{1}{2\beta}\right\}\) results in the convergence rate in the theorem statement. Naturally, there are four cases, depending on the outcomes of the \(\min\) and \(\max\) operations.

Case A:\(\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right)\geq 1\)and \(\frac{1}{\mu T}\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right) \leq\frac{1}{2\beta}\).Putting \(\eta=\frac{1}{\mu T}\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right)\) into (6),

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*}\leq\frac{\beta^{6}\rho^{2}}{\mu^{ 5}T^{2}}+\frac{\beta^{6}\rho^{2}}{2\mu^{5}T^{2}}\log^{2}\left(\frac{\mu^{5} \Delta T^{2}}{\beta^{6}\rho^{2}}\right).\]

Case B:\(\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right)\geq 1\)and \(\frac{1}{\mu T}\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right) \geq\frac{1}{2\beta}\).Substituting \(\eta=\frac{1}{2\beta}\leq\frac{1}{\mu T}\log\left(\frac{\mu^{5}\Delta T^{2}}{ \beta^{6}\rho^{2}}\right)\) to (6),

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*} \leq\left(1-\frac{\mu}{2\beta}\right)^{T}\Delta+\frac{\beta^{6} \rho^{2}}{2\mu^{3}}\cdot\left(\frac{1}{2\beta}\right)^{2}\] \[\leq\exp\left(-\frac{\mu T}{2\beta}\right)\Delta+\frac{\beta^{6} \rho^{2}}{2\mu^{5}T^{2}}\log^{2}\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6} \rho^{2}}\right).\]

Case C:\(\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right)\leq 1\)and \(\frac{1}{\mu T}\leq\frac{1}{2\beta}\).Putting \(\eta=\frac{1}{\mu T}\geq\frac{1}{\mu T}\log\left(\frac{\mu^{5}\Delta T^{2}}{ \beta^{6}\rho^{2}}\right)\) into (6),

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*} \leq\left(1-\frac{1}{T}\right)^{T}\Delta+\frac{\beta^{6}\rho^{2}} {2\mu^{5}T^{2}}\] \[\leq\left(1-\frac{1}{T}\log\left(\frac{\mu^{5}\Delta T^{2}}{ \beta^{6}\rho^{2}}\right)\right)^{T}\Delta+\frac{\beta^{6}\rho^{2}}{2\mu^{5} T^{2}}\leq\frac{\beta^{6}\rho^{2}}{\mu^{5}T^{2}}+\frac{\beta^{6}\rho^{2}}{2\mu^{5} T^{2}}.\]

Case D:\(\log\left(\frac{\mu^{5}\Delta T^{2}}{\beta^{6}\rho^{2}}\right)\leq 1\)and \(\frac{1}{\mu T}\geq\frac{1}{2\beta}\).By substituting \(\eta=\frac{1}{2\beta}\leq\frac{1}{\mu T}\) to (6) we obtain

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*} \leq\left(1-\frac{\mu}{2\beta}\right)^{T}\Delta+\frac{\beta^{6} \rho^{2}}{2\mu^{3}}\cdot\left(\frac{1}{2\beta}\right)^{2}\] \[\leq\exp\left(-\frac{\mu T}{2\beta}\right)\Delta+\frac{\beta^{6} \rho^{2}}{2\mu^{5}T^{2}}.\]

Combining the four cases, we conclude

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*}=\tilde{\mathcal{O}}\left(\exp\left( -\frac{\mu T}{2\beta}\right)\Delta+\frac{\beta^{6}\rho^{2}}{\mu^{5}T^{2}}\right),\]

thereby completing the proof. 

### Lower Bound Proof for Smooth and Strongly Convex Functions (Proof of Theorem 3.2)

In this section, we prove Theorem 3.2, restated below for the sake of convenience.

**Theorem 3.2**.: _Suppose \(\frac{\beta}{\mu}\geq 2\). For any choice of perturbation size \(\rho\), step size \(\eta\), and initialization \(\bm{x}_{0}\), there exists a differentiable, \(\beta\)-smooth, and \(\mu\)-strongly convex function \(f\) such that_

\[\min_{t\in\{0,\ldots,T\}}f(\bm{x}_{t})-f^{*}=\Omega\left(\frac{\beta^{3}\rho^{2 }}{\mu^{2}T^{2}}\right)\]

_holds for deterministic SAM iterates._The proof is divided into three cases, depending on the step size. For each case, we will define and analyze a one-dimensional function to show the lower bound. In doing so, without loss of generality we will fix an initialization \(x_{0}\) because we can appropriately shift the function for different choices of \(x_{0}\).

1. For \(\eta\leq\frac{1}{2\mu T}\), we show that there exists a function \(f\) such that \[\min_{t\in\{0,\dots,T\}}f(x_{t})-f^{*}=\Omega\left(\frac{\beta^{4}\rho^{2}}{ \mu^{3}}\right).\]
2. For \(\frac{1}{2\mu T}\leq\eta\leq\frac{2}{\beta}\), we show existence of a function \(f\) that satisfies \[\min_{t\in\{0,\dots,T\}}f(x_{t})-f^{*}=\Omega\left(\frac{\beta^{3}\rho^{2}}{ \mu^{2}T^{2}}\right).\]
3. For \(\eta\geq\frac{2}{\beta}\), we show that there exists a function \(f\) such that \[\min_{t\in\{0,\dots,T\}}f(x_{t})-f^{*}=\Omega\left(\frac{\beta^{3}\rho^{2}}{ \mu^{2}}\right).\]

Combining the three results, we can see that the suboptimality gap of the best iterate is at least \(\Omega\left(\frac{\beta^{3}\rho^{2}}{\mu^{2}T^{2}}\right)\), hence proving the theorem. Below, we prove the statements for the three intervals.

Case 1: \(\eta\leq\frac{1}{2\mu T}\).Consider a function

\[f(x)=\frac{1}{2}\mu x^{2}-\mu\rho x,\quad\nabla f(x)=\mu(x-\rho).\]

Suppose we start at initialization \(x_{0}=\frac{2\beta^{2}\rho}{\mu^{2}}\geq 8\rho\). Note from the definition of \(\nabla f\) that for any \(x_{t}\geq\rho\), we have \(y_{t}=x_{t}+\rho\frac{\nabla f(x_{t})}{\left|\nabla f(x_{t})\right|}=x_{t}+\rho\) and \(\nabla f(y_{t})=\mu(x_{t}+\rho-\rho)=\mu x_{t}\). Therefore, the first SAM update can be written as

\[x_{1}=x_{0}-\eta\nabla f(y_{0})=\left(1-\eta\mu\right)x_{0}.\]

Since \(1-\eta\mu\geq 1-\frac{1}{2T}\geq\frac{1}{2}\), the inequality \(x_{1}\geq\rho\) still holds and the same argument can be repeated for the second update, yielding

\[x_{2}=x_{1}-\eta\nabla f(y_{1})=\left(1-\eta\mu\right)x_{1}=\left(1-\eta\mu \right)^{2}x_{0}.\]

In fact, \((1-\eta\mu)^{t}\geq(1-\frac{1}{2T})^{T}\geq\frac{1}{2}\) for all \(t\leq T\), so all the iterates up to \(x_{T}\) stay above \(\rho\). Thus, for any \(t=0,\dots,T\), the iterate \(x_{t}\) can be written and bounded from below as

\[x_{t}=\left(1-\eta\mu\right)^{t}x_{0}\geq\frac{x_{0}}{2}=\frac{\beta^{2}\rho} {\mu^{2}}.\]

Therefore, in this case, the suboptimality gap of the best iterate is at least

\[\min_{t\in\{0,\dots,T\}}f(x_{t})-f^{*}\geq f\left(\frac{\beta^{2}\rho}{\mu^{2 }}\right)-f(\rho)=\frac{\beta^{4}\rho^{2}}{2\mu^{3}}-\frac{\beta^{2}\rho^{2}} {\mu}+\frac{\mu\rho^{2}}{2}=\Omega\left(\frac{\beta^{4}\rho^{2}}{\mu^{3}} \right).\]

Case 2: \(\frac{1}{2\mu T}\leq\eta\leq\frac{2}{\beta}\).Consider a function

\[f(x)=\frac{1}{4}\beta x^{2},\quad\nabla f(x)=\frac{1}{2}\beta x.\]

Suppose we start at initialization \(x_{0}=\frac{\eta\beta\rho}{4-\eta\beta}\). We are going to show that the SAM iterates oscillate between \(\pm\frac{\eta\beta\rho}{4-\eta\beta}\). Indeed, if \(x_{t}=\frac{\eta\beta\rho}{4-\eta\beta}\), then \(\nabla f(x_{t})>0\) and

\[y_{t}=\frac{\eta\beta\rho}{4-\eta\beta}+\rho=\frac{4\rho}{4-\eta\beta},\quad \text{and}\quad\nabla f(y_{t})=\frac{2\beta\rho}{4-\eta\beta}.\]Then, after SAM update, we get

\[x_{t+1}=x_{t}-\eta\nabla f(y_{t})=\frac{\eta\beta\rho}{4-\eta\beta}-\eta\frac{2 \beta\rho}{4-\eta\beta}=-\frac{\eta\beta\rho}{4-\eta\beta}.\]

The same argument can be repeated to show that \(x_{t+2}=-x_{t+1}=x_{t}\). As a result, the iterates oscillate between \(\pm\frac{\eta\beta\rho}{4-\eta\beta}\) forever. In this case, the suboptimality gap is bounded from below by

\[\min_{t\in\{0,\ldots,T\}}f(x_{t})-f^{*}\geq\frac{\beta}{4}\left(\frac{\eta \beta\rho}{4-\eta\beta}\right)^{2}\geq\frac{\eta^{2}\beta^{3}\rho^{2}}{64}.\]

Applying \(\eta\geq\frac{1}{2\mu T}\) yields

\[\min_{t\in\{0,\ldots,T\}}f(x_{t})-f^{*}=\Omega\left(\frac{\beta^{3}\rho^{2}}{ \mu^{2}T^{2}}\right).\]

Case 3: \(\eta\geq\frac{2}{\beta}\).Consider a function

\[f(x)=\frac{1}{2}\beta x^{2},\quad\nabla f(x)=\beta x.\]

Let the initialization be \(x_{0}=\frac{\beta\rho}{\mu}\). For any \(x_{t}\geq 0\), we have \(y_{t}=x_{t}+\rho\) and \(\nabla f(y_{t})=\beta(x_{t}+\rho)\). Then, the resulting SAM update becomes

\[x_{t+1}=x_{t}-\eta\nabla f(y_{t})=(1-\eta\beta)x_{t}-\eta\rho\leq(1-\eta\beta )x_{t}.\]

Since we have \(\eta\geq\frac{2}{\beta}\), we have \(1-\eta\beta\leq-1\) and \(x_{t+1}\) has the opposite sign as \(x_{t}\) and its absolute value is at least as large as \(|x_{t}|\). We can similarly check that any further SAM update changes the sign and does not decrease the absolute value. Therefore, for all \(t=0,\ldots,T\), we have \(|x_{t}|\geq|x_{0}|\).

Consequently, the suboptimality gap of the best iterate is at least

\[\min_{t\in\{0,\ldots,T\}}f(x_{t})-f^{*}\geq f(x_{0})-f(0)=\Omega\left(\frac{ \beta^{3}\rho^{2}}{\mu^{2}}\right).\]

Remarks on validity of lower bound.Lastly, we comment on why we choose different functions and initialization for different choices of \(\eta\) and why it suffices to provide a matching lower bound for Theorem 3.1. In convergence upper bounds in the form of Theorem 3.1, we aim to prove an upper bound on the following _minimax risk_:

\[\inf_{A\in\mathcal{A}}\sup_{f\in\mathcal{F}}\zeta(A(f)),\] (7)

where \(\mathcal{A}\) denotes the class of _algorithms_, \(\mathcal{F}\) denotes the class of _functions_, and \(\zeta(A(f))\) is the _suboptimality measure_ for algorithm's output \(A(f)\) for function \(f\). In the context of Theorem 3.1, our algorithm class \(\mathcal{A}\) corresponds to the choices of the "hyperparameters" \(\eta\), \(\rho\), and \(\bm{x}_{0}\) of SAM, and the function class \(\mathcal{F}\) here is the class analyzed by the theorem: the collection of \(\beta\)-smooth and \(\mu\)-strongly convex functions. From this viewpoint, Theorem 3.1 can be thought of as an upper bound on (7), with the choice of \(\zeta(A(f))\triangleq\min_{t\in\{0,\ldots,T\}}f(x_{t})-f^{*}\).

Hence, showing a matching lower bound for Theorem 3.1 amounts to showing a matching lower bound for the minimax risk (7). For this purpose, it suffices to show that _for each_\(A\in\mathcal{A}\), there _exists_ a choice of \(f\in\mathcal{F}\) such that a certain lower bound holds. Therefore, we are allowed to choose different choices of \(f\) for each different choice of hyperparameters \(\eta\), \(\rho\), and \(\bm{x}_{0}\). In fact, in our proof, we choose different choices of \(f\) and \(\bm{x}_{0}\) for each different choice of \(\eta\); however, this is without loss of generality once we notice here that starting an algorithm at \(\bm{x}_{0}\) to minimize \(f(\bm{x})\) is equivalent to starting at \(\bm{0}\) to minimize \(f(\bm{x}-\bm{x}_{0})\). Hence, even though we choose different \(f\)'s and \(\bm{x}_{0}\)'s for different choices of \(\eta\) in our proof of Theorem 3.2, this is sufficient for providing a matching upper bound for Theorem 3.1.

Admittedly, some authors show stronger versions than what we show, where a single function takes care of all possible hyperparameters. Indeed, such results provide lower bounds for \(\sup_{f\in\mathcal{F}}\inf_{A\in\mathcal{A}}\zeta(A(f))\). Recalling that \(\sup\inf\leq\inf\sup\), one can notice that these \(\sup\inf\) lower bounds are in fact much stronger than what suffices.

### Convergence Proof for Smooth and Convex Functions (Proof of Theorem 3.3)

For smooth and convex function, we prove the convergence of gradient norm to zero. The theorem is restated for convenience.

**Theorem 3.3**.: _Consider a \(\beta\)-smooth and convex function \(f\). If we run deterministic SAM starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{\sqrt{2\Delta}}{\sqrt{\beta^{3}\rho^{2}T}},\frac{1}{2 \beta}\right\}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}=\mathcal{O} \bigg{(}\frac{\beta\Delta}{T}+\frac{\sqrt{\Delta\beta^{3}\rho^{2}}}{\sqrt{T}} \bigg{)}.\]

Proof.: Using Lemma B.3, for smooth and convex function \(f\), we have

\[f(\bm{x}_{t+1})\leq f(\bm{x}_{t})-\frac{\eta}{2}\|\nabla f(\bm{x}_{t})\|^{2}+ \frac{\eta^{2}\beta^{3}\rho^{2}}{2},\]

which can be rewritten as

\[\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2}{\eta}(f(\bm{x}_{t})-f(\bm{x}_{t+1})) +\eta\beta^{3}\rho^{2}.\]

Adding up the inequality for \(t=0,\ldots,T-1\), and the dividing both sides by \(T\), we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2}{\eta T}(f (\bm{x}_{0})-f(\bm{x}_{T}))+\eta\beta^{3}\rho^{2}\leq\frac{2}{\eta T}\Delta+ \eta\beta^{3}\rho^{2}.\] (8)

We now spell out how the choice of step size \(\eta=\min\left\{\sqrt{\frac{2\Delta}{\beta^{3}\rho^{2}T}},\frac{1}{2\beta}\right\}\) results in the convergence rate in the theorem statement. There are two cases to be considered, depending on the outcome of the \(\min\) operation.

Case A: \(\sqrt{\frac{2\Delta}{\beta^{3}\rho^{2}T}}\leq\frac{1}{2\beta}\).Putting \(\eta=\sqrt{\frac{2\Delta}{\beta^{3}\rho^{2}T}}\) into (8), we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{\sqrt{2 \Delta\beta^{3}\rho^{2}}}{\sqrt{T}}+\frac{\sqrt{2\Delta\beta^{3}\rho^{2}}}{ \sqrt{T}}=\frac{2\sqrt{2\Delta\beta^{3}\rho^{2}}}{\sqrt{T}}.\]

Case B: \(\sqrt{\frac{2\Delta}{\beta^{3}\rho^{2}T}}\geq\frac{1}{2\beta}\).Substituting \(\eta=\frac{1}{2\beta}\leq\sqrt{\frac{2\Delta}{\beta^{3}\rho^{2}T}}\) to (8) yields

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{4\beta\Delta} {T}+\beta^{3}\rho^{2}\cdot\frac{1}{2\beta}\leq\frac{4\beta\Delta}{T}+\frac{ \sqrt{2\Delta\beta^{3}\rho^{2}}}{\sqrt{T}}.\]

Merging the two cases, we conclude

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}=\mathcal{O}\left( \frac{\beta\Delta}{T}+\frac{\sqrt{\Delta\beta^{3}\rho^{2}}}{\sqrt{T}}\right),\]

completing the proof. 

### Convergence Proof for Smooth and Nonconvex Functions (Proof of Theorem 3.4)

In this section, we prove the convergence up to an additive factor \(\beta^{2}\rho^{2}\) for smooth and nonconvex functions. The theorem is restated for convenience.

**Theorem 3.4**.: _Consider a \(\beta\)-smooth function \(f\) satisfying \(f^{*}=\inf_{\bm{x}}f(\bm{x})>-\infty\). If we run deterministic SAM starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\frac{1}{\beta}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\mathcal{O }\left(\frac{\beta\Delta}{T}\right)+\beta^{2}\rho^{2}.\]Proof.: Starting from the definition of \(\beta\)-smoothness, we have

\[f(\bm{x}_{t+1}) \leq f(\bm{x}_{t})-\eta\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_ {t})\rangle+\frac{\eta^{2}\beta}{2}\|\nabla f(\bm{y}_{t})\|^{2}\] \[=f(\bm{x}_{t})-\frac{\eta}{2}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{ \eta}{2}\|\nabla f(\bm{y}_{t})\|^{2}+\frac{\eta}{2}\|\nabla f(\bm{x}_{t})- \nabla f(\bm{y}_{t})\|^{2}+\frac{\eta^{2}\beta}{2}\|\nabla f(\bm{y}_{t})\|^{2}\] \[\leq f(\bm{x}_{t})-\frac{\eta}{2}\|\nabla f(\bm{x}_{t})\|^{2}+ \frac{\eta\beta^{2}}{2}\|\bm{x}_{t}-\bm{y}_{t}\|^{2}\] \[=f(\bm{x}_{t})-\frac{\eta}{2}\|\nabla f(\bm{x}_{t})\|^{2}+\frac{ \eta\beta^{2}\rho^{2}}{2}.\]

The inequality can be rearranged as

\[\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2}{\eta}\left(f(\bm{x}_{t})-f(\bm{x}_{t +1})\right)+\beta^{2}\rho^{2}.\]

Adding up the inequality for \(t=0,\ldots,T-1\), and the dividing both sides by \(T\), we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2}{\eta T} \left(f(\bm{x}_{0})-f(\bm{x}_{T})\right)+\beta^{2}\rho^{2}\leq\frac{2\Delta}{ \eta T}+\beta^{2}\rho^{2}.\] (9)

Substituting \(\eta=\frac{1}{\beta}\) to (9) yields

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2\beta\Delta }{T}+\beta^{2}\rho^{2}.\]

### Non-convergence for a Smooth and Nonconvex Function (Proof of Theorem 3.5)

In this section, we spell out the proof of our counterexample that SAM with constant perturbation size \(\rho\) provably fails to converge to a stationary point. Here we restate the theorem.

**Theorem 3.5**.: _For any \(\rho>0\) and \(\eta\leq\frac{1}{\beta}\), there exists a \(\beta\)-smooth and \(\Theta(\beta\rho)\)-Lipschitz continuous function such that, if deterministic SAM is initialized at a point \(\bm{x}_{0}\) sampled from a continuous probability distribution, then deterministic SAM converges to a nonstationary point, located at a distance of \(\Omega(\rho)\) from a stationary point, with probability \(1\)._

Proof.: For \(x\in\mathbb{R}\), consider the following one-dimensional function: given \(\rho\) and \(\beta\),

\[f(x)=\frac{9\beta\rho^{2}}{25\pi^{2}}\sin\left(\frac{5\pi}{3\rho}x\right).\]

It is easy to check that this function is \(\beta\)-smooth and \(\frac{3\beta\rho}{5\pi}\)-Lipschitz continuous. Also, let

\[\mathbb{X}=\{x\mid x=(0.3+0.6k)\rho,k\in\mathbb{Z}\},\]

which is the set of points \(x\) where \(\nabla f(x)=0\). For SAM with perturbation size \(\rho\), given the current iterate \(x_{t}\), the corresponding \(y_{t}=x_{t}+\rho\frac{\nabla f(x_{t})}{\|\nabla f(x_{t})\|}\) is given by

\[y_{t}=x_{t}+\begin{cases}\rho&\text{if }(-0.3+1.2k)\rho<x_{t}<(0.3+1.2k)\rho \text{ for some }k\in\mathbb{Z},\\ -\rho&\text{if }(0.3+1.2k)\rho<x_{t}<(0.9+1.2k)\rho\text{ for some }k\in\mathbb{Z},\\ 0&\text{if }x_{t}\in\mathbb{X}.\end{cases}\]

This leads to the following a virtual gradient map \(G_{f}\):

\[G_{f}(x_{t})=\nabla f(y_{t})=\begin{cases}\frac{3\beta\rho}{5\pi}\cos\left( \frac{5\pi}{3\rho}x_{t}+\frac{5\pi}{3}\right)&\text{if }(-0.3+1.2k)\rho<x_{t}<(0.3+1.2k)\rho \text{ for some }k\in\mathbb{Z},\\ \frac{3\beta\rho}{5\pi}\cos\left(\frac{5\pi}{3\rho}x_{t}-\frac{5\pi}{3}\right)& \text{if }(0.3+1.2k)\rho<x_{t}<(0.9+1.2k)\rho\text{ for some }k\in\mathbb{Z},\\ 0&\text{if }x_{t}\in\mathbb{X}.\end{cases}\]From this virtual gradient map, we can define

\[\mathbb{Y}=\{x\mid x=(0.7+1.2k)\rho,k\in\mathbb{Z}\}\cup\{x\mid x=(-0.1+1.2k)\rho, k\in\mathbb{Z}\}\]

and \(\mathbb{Y}\) is the set of all points \(x\) where \(G_{f}(x)=0\) and \(\nabla f(x)\neq 0\).

Since \(f\) is a one-dimensional function, a virtual loss \(J_{f}\) can be obtained by integrating \(G_{f}\). One possible example is

\[J_{f}(x)=\begin{cases}\frac{9\beta\rho^{2}}{25\pi^{2}}\sin\left(\frac{5\pi}{ 3\rho}x+\frac{5\pi}{3}\right)&\text{if }(-0.3+1.2k)\rho\leq x\leq(0.3+1.2k)\rho\text{ for some }k\in\mathbb{Z},\\ \frac{9\beta\rho^{2}}{25\pi^{2}}\sin\left(\frac{5\pi}{3\rho}x-\frac{5\pi}{3} \right)&\text{if }(0.3+1.2k)\rho\leq x\leq(0.9+1.2k)\rho\text{ for some }k\in\mathbb{Z},\end{cases}\]

which is a piecewise \(\beta\)-smooth function that is minimized at points in \(\mathbb{Y}\) and locally maximized at (non-differentiable) points in \(\mathbb{X}\). Since \(J_{f}\) is well-defined, we can view SAM as GD on \(J_{f}\).

For sufficiently small \(\eta\leq\frac{1}{\beta}\) and any initialization \(x_{0}\in\mathbb{R}\setminus\mathbb{X}\), the initialization belongs to one of the intervals \(((-0.3+0.6k)\rho,(0.3+0.6k)\rho)\). For such small enough \(\eta\), we can guarantee that the SAM iterates \(x_{t}\) will stay in the interval \(((-0.3+0.6k)\rho,(0.3+0.6k)\rho)\) for all \(t\geq 0\). For example, suppose that \(x_{t}\in(-0.3\rho,0.3\rho)\). Then, the next iterate will stay in the same interval if and only if the image of the interval \((-0.3\rho,0.3\rho)\) under a map \(x\mapsto x-\frac{3\eta\beta\rho}{5\pi}\cos\left(\frac{5\pi}{3\rho}x+\frac{5\pi }{3}\right)\) is a subset of \((-0.3\rho,0.3\rho)\).

\[\left\{x-\frac{3\eta\beta\rho}{5\pi}\cos\left(\frac{5\pi}{3\rho} x+\frac{5\pi}{3}\right)\mid x\in(-0.3\rho,0.3\rho)\right\}\subset(-0.3\rho,0.3\rho)\] \[\Longleftrightarrow\left\{z-\eta\beta\cos\left(z+\frac{5\pi}{3} \right)\mid z\in\left(-\frac{\pi}{2},\frac{\pi}{2}\right)\right\}\subset\left( -\frac{\pi}{2},\frac{\pi}{2}\right),\]

and the containment is true if \(\eta\beta\leq 1\). By symmetry, the same argument can be applied to any intervals of the form \(((-0.3+0.6k)\rho,(0.3+0.6k)\rho)\).

Thus, for any initialization \(x_{0}\in\mathbb{R}\setminus\mathbb{X}\), all SAM iterates stay inside the interval which \(x_{0}\) belongs to. Inside the interval, by \(\beta\)-smoothness of \(J_{f}\), the following descent lemma always holds:

\[J_{f}(x_{t+1}) \leq J_{f}(x_{t})+\langle\nabla J_{f}(x_{t}),x_{t+1}-x_{t}\rangle +\frac{\beta}{2}\|x_{t+1}-x_{t}\|^{2}\] \[=J_{f}(x_{t})-\eta\left(1-\frac{\eta\beta}{2}\right)\|G_{f}(x_{t} )\|^{2}\] \[\leq J_{f}(x_{t})-\frac{\eta}{2}\|G_{f}(x_{t})\|^{2}.\]

Therefore, if we add the inequalities up for \(t=0,\ldots,T-1\), we get

\[\sum_{t=0}^{T-1}\|G_{f}(x_{t})\|^{2}\leq\frac{2}{\eta}(J_{f}(x_{0})-J_{f}(x_{ T}))\leq\frac{2}{\eta}(J_{f}(x_{0})-J_{f}^{*})<\infty,\]

where \(J_{f}^{*}\triangleq\inf_{x}J_{f}(x)>-\infty\). Since the inequality holds for all \(T\), the series is summable, which in turn implies that \(\|G_{f}(x_{t})\|\to 0\) as \(t\to\infty\). However, the only point in the interval \(((-0.3+0.6k)\rho,(0.3+0.6k)\rho)\) satisfying \(\|G_{f}(x)\|=0\) is the one in \(\mathbb{Y}\cap((-0.3+0.6k)\rho,(0.3+0.6k)\rho)\). Hence, if \(x_{0}\in\mathbb{R}\setminus\mathbb{X}\) and \(\eta\leq\frac{1}{\beta}\), SAM must converge to a point in \(\mathbb{Y}\). Since \(x_{0}\) is drawn from a continuous probability distribution, \(x_{0}\in\mathbb{R}\setminus\mathbb{X}\) holds almost surely. This finishes the proof. 

### Non-convergence for a Nonsmooth and Convex Functions (Proof of Theorem 3.6)

For nonsmooth convex function, depending on the initialization, SAM can converge to a suboptimal point with distance \(\Omega(\rho)\) from the global minimum. The theorem is restated for convenience.

**Theorem 3.6**.: _For any \(\rho>0\) and \(\eta<\frac{7\rho}{4}\), there exists a nonsmooth Lipschitz convex function \(f\) such that for some initialization, deterministic SAM converges to suboptimal points located at a distance of \(\Omega(\rho)\) from the global minimum._Proof.: For \(\bm{x}=(x^{(1)},x^{(2)})\in\mathbb{R}^{2}\), consider a 2-dimensional function,

\[f(\bm{x})=\max\{|x^{(1)}|,|2x^{(1)}+x^{(2)}|\}.\]

A set \(\{\bm{v}_{1},\bm{v}_{2}\}\) can be established as a basis for \(\mathbb{R}^{2}\), with \(\bm{v}_{1}=\bm{e}_{1}\), and \(\bm{v}_{2}=\frac{2}{\sqrt{5}}\bm{e}_{1}+\frac{1}{\sqrt{5}}\bm{e}_{2}\). Any vector \(\bm{x}\in\mathbb{R}^{2}\) can be uniquely expressed by \(\bm{v}_{1}\) and \(\bm{v}_{2}\). Consider the region: \(\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid b^{(1)}<0\}\).

This area can be partitioned into four separate regions.

\[\begin{cases}&\mathbb{A}:\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid -\frac{7\rho}{2}<b^{(1)}<0\right\},\\ &\mathbb{B}:\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid b^{(1)}<- \frac{7\rho}{2}\right\}\cap\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2} \mid b^{(1)}+\sqrt{5}b^{(2)}>0\right\},\\ &\mathbb{C}:\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid b^{(1)}+ \sqrt{5}b^{(2)}<0\right\}\cap\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2} \mid-b^{(1)}+\sqrt{5}b^{(2)}>0\right\}\\ &\cap\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid-2b^{(1)}+\sqrt{5}b^ {(2)}>\frac{3}{2}\rho\right\},\\ &\mathbb{D}:\left\{\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid b^{(1)}<0 \right\}-\left(\mathbb{A}\cup\mathbb{B}\cup\mathbb{C}\right).\end{cases}\]

Figure 3(a) demonstrates the regions \(\mathbb{A},\mathbb{B},\mathbb{C},\mathbb{D}\), as well as the vectors \(\bm{v}_{1}\) and \(\bm{v}_{2}\). When \(\bm{x}\) belongs to the set \(\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\mid b^{(1)}<0\), we can examine four different scenarios.

Case A:\(\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\in\mathbb{A}_{\bm{\cdot}}\quad\bm{x }\in\mathbb{A}\), so \(\bm{y}=\bm{x}+\rho\frac{\nabla f(\bm{x})}{\|\nabla f(\bm{x})\|}\) and \(-\nabla f(\bm{y})\) are as follows.

\[\bm{y}=\begin{cases}\bm{x}+\rho\bm{v}_{2},&-b^{(1)}<\sqrt{5}b^{(2)}\\ \bm{x}-\rho\bm{v}_{1},&b^{(1)}<\sqrt{5}b^{(2)}<-b^{(1)}\\ \bm{x}-\rho\bm{v}_{2},&\sqrt{5}b^{(2)}<b^{(1)}.\end{cases}\]

\[-\nabla f(\bm{y})=\begin{cases}-\sqrt{5}\bm{v}_{2},&-b^{(1)}<\sqrt{5}b^{(2)}\\ \sqrt{5}\bm{v}_{2},&b^{(1)}<\sqrt{5}b^{(2)}<-b^{(1)}\\ \sqrt{5}\bm{v}_{2},&\sqrt{5}b^{(2)}<b^{(1)}.\end{cases}\]

As a result, the SAM updates \(\nabla f(\bm{y})\) only affect \(\bm{v}_{2}\), and does not affect on \(\bm{v}_{1}\). Thus, if \(\bm{x}_{t}\in\mathbb{A}\), the next SAM iterate \(\bm{x}_{t+1}\) remains in \(\mathbb{A}\).

Case B:\(\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\in\mathbb{B}\).We can verify that \(\bm{y}=\bm{x}+\rho\frac{\nabla f(\bm{x})}{\|\nabla f(\bm{x})\|}=\bm{x}+\rho \bm{v}_{2}\). Therefore, \(-\nabla f(\bm{y})=-\sqrt{5}\bm{v}_{2}\). Therefore, the SAM update shifts in the direction of \(-\bm{v}_{2}\). Hence, if \(\bm{x}_{t}\in\mathbb{B}\), the next SAM iterate will be \(\bm{x}_{t+1}\in(\mathbb{B}\cup\mathbb{C}\cup\mathbb{D})\).

Figure 3: (a) The demonstration of four seperate regions, and basis vectors of example function. (b) The trajectory of SAM for example function. As SAM enters region \(\mathbb{A}\), the update cannot get any closer to the global minima.

**Case C: \(\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\in\mathbb{C}\).**\(\bm{y}=\bm{x}+\rho\frac{\nabla f(\bm{x})}{\|\nabla f(\bm{x})\|}=\bm{x}- \rho\bm{v}_{1}\), so

\[-\nabla f(\bm{y}_{t})=\begin{cases}\sqrt{5}\bm{v}_{2},&b^{(1)}<\sqrt{5}b^{(2)}< b^{(1)}+\rho\\ \bm{v}_{1},&b^{(1)}+\rho<\sqrt{5}b^{(2)}<-b^{(1)}.\end{cases}\]

For \(\bm{x}_{t}=b^{(1)}_{t}\bm{v}_{1}+b^{(2)}_{t}\bm{v}_{2}\in\mathbb{C}\), if \(b^{(1)}_{t}<\sqrt{5}b^{(2)}_{t}<b^{(1)}_{t}+\rho\), then SAM update shifts in the direction of \(\bm{v}_{2}\), resulting in \(\bm{x}_{t+1}\in(\mathbb{B}\cup\mathbb{C}\cup\mathbb{D})\). Otherwise, the next SAM iterate \(\bm{x}_{t+1}\) shifts in the direction of \(+\bm{v}_{1}\). Consequently, \(\bm{x}_{t+1}\) can either remain in \(\mathbb{B}\cup\mathbb{C}\cup\mathbb{D}\), or move to \(\mathbb{A}\).

Assume that \(\bm{x}_{t+1}\) moves to \(\mathbb{A}\). Given that \(b^{(1)}_{t}<-\frac{7\rho}{2}\) for \(\bm{x}_{t}\in\mathbb{C}\), we can verify that \(b^{(1)}_{t+1}<-\frac{7\rho}{2}+\eta\).Based on our previous observation in **Case A** where SAM updates only affect \(\bm{v}_{2}\) when \(\bm{x}\in\mathbb{A}\), we can conclude that for all subsequent iterates \(\bm{x}_{i}\) with \(i>t+1\), the condition \(b^{(1)}_{i}<-\frac{7\rho}{2}+\eta\) continues to hold.

**Case D: \(\bm{x}=b^{(1)}\bm{v}_{1}+b^{(2)}\bm{v}_{2}\in\mathbb{D}\).**\(\bm{y}=\bm{x}+\rho\frac{\nabla f(\bm{x})}{\|\nabla f(\bm{x})\|}\) becomes

\[\bm{y}=\begin{cases}\bm{x}-\rho\bm{v}_{2},&\sqrt{5}b^{(2)}<b^{(1)}\\ \bm{x}-\rho\bm{v}_{1},&\text{otherwise}.\end{cases}\]

We can check that \(-\nabla f(\bm{y})=\sqrt{5}\bm{v}_{2}\) for all cases. So the SAM update shifts in the direction of \(+\bm{v}_{2}\). As a result, if \(\bm{x}_{t}\in\mathbb{D}\), the next SAM iterate \(\bm{x}_{t+1}\) will fall into \(\bm{x}_{t+1}\in(\mathbb{B}\cup\mathbb{C}\cup\mathbb{D})\).

Furthermore, we can verify that for \(\bm{x}\in(\mathbb{B}\cup\mathbb{C}\cup\mathbb{D})\), \(b^{(1)}<-\frac{7\rho}{2}\) holds. Therefore, summing up all the cases, we can conclude that if the initial iterate \(\bm{x}_{0}\) is chosen from \(\bm{x}_{0}\in(\mathbb{B}\cup\mathbb{C}\cup\mathbb{D})\), then all subsequent SAM iterates \(\bm{x}_{t}\) will satisfy \(b^{(1)}_{t}<-\frac{7\rho}{2}+\eta\). Since the global minimum is located at \(\bm{x}^{*}=(0,0)\), it follows that \(\|\bm{x}_{t}-\bm{x}^{*}\|>\frac{|7\rho/2-\eta|}{\sqrt{5}}\) for every \(t>0\). Thus, for \(\eta<\frac{7\rho}{4}\), we can ascertain that in this particular function, \(\|\bm{x}_{t}-\bm{x}^{*}\|>\frac{7\rho}{4\sqrt{5}}\) for every \(t>0\), thereby proving that the distance from the global minimum is at least \(\Omega(\rho)\). The trajectory plot of SAM on this example function is illustrated in Figure 3(b).

Proofs for (Non-)Convergence of Stochastic SAM

In this section, we provide in-depth demonstrations and proofs regarding convergence of stochastic SAM with constant perturbation size \(\rho\). The objective is defined as \(f(\bm{x})=\mathbb{E}_{\xi}[l(\bm{x};\xi)]\), where \(\xi\) represents a stochastic parameter (e.g., data sample) and \(l(\bm{x};\xi)\) represents the loss at point \(\bm{x}\) with a random sample \(\xi\). The update iteration for stochastic SAM is specified as follows:

\[\begin{cases}\bm{y}_{t}=\bm{x}_{t}+\rho\frac{g(\bm{x}_{t})}{\|g(\bm{x}_{t})\|}, \\ \bm{x}_{t+1}=\bm{x}_{t}-\eta\tilde{g}(\bm{y}_{t}).\end{cases}\]

We define \(g(\bm{x})=\nabla_{\bm{x}}l(\bm{x};\xi)\) and \(\tilde{g}(\bm{x})=\nabla_{\bm{x}}l(\bm{x};\tilde{\xi})\), where \(\xi\) and \(\tilde{\xi}\) are stochastic parameters. Stochastic SAM comes in two variations: \(n\)-SAM, where \(\xi\) and \(\tilde{\xi}\) are independent, and \(m\)-SAM, where \(\xi\) is equal to \(\tilde{\xi}\).

### Important Lemmas Regarding Stochastic SAM

In this section, we present a number of lemmas that are utilized in our theorem proofs regarding stochastic SAM. In order to do this, we also introduce extra notation, \(\hat{\bm{y}}_{t}=\bm{x}_{t}+\rho\frac{\nabla f(\bm{x}_{t})}{\|\nabla f(\bm{x}_ {t})\|}\), as a deterministically ascended parameter.

**Lemma C.1**.: _For a differentiable and \(\mu\)-strongly convex function \(f\), we have_

\[\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \nabla f(\bm{x}_{t})\rangle \geq\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \tilde{g}(\hat{\bm{y}}_{t})\rangle+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|.\]

_For a differentiable and convex function \(f\), the inequality continues to hold with \(\mu=0\)._

Proof.: \[\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \nabla f(\bm{x}_{t})\rangle =\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \tilde{g}(\hat{\bm{y}}_{t})\rangle+\mathbb{E}\langle\nabla f(\bm{x}_{t}), \tilde{g}(\hat{\bm{y}}_{t})-\nabla f(\bm{x}_{t})\rangle\] \[=\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \tilde{g}(\hat{\bm{y}}_{t})\rangle+\mathbb{E}\langle\nabla f(\bm{x}_{t}), \nabla f(\hat{\bm{y}}_{t})-\nabla f(\bm{x}_{t})\rangle\] \[\geq\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \tilde{g}(\hat{\bm{y}}_{t})\rangle+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|,\]

where we use Lemma B.2 in the last inequality, thereby completing the proof. 

**Lemma C.2**.: _Under \(n\)-SAM, for a \(\beta\)-smooth and \(\mu\)-strongly convex function \(f\), we have_

\[\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \nabla f(\bm{x}_{t})\rangle \geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}+\mu\rho \mathbb{E}\|\nabla f(\bm{x}_{t})\|-2\beta^{2}\rho^{2}.\]

_Under \(m\)-SAM, additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), the inequality continues to hold._

Proof.: First we consider \(n\)-SAM. Starting from Lemma C.1, we have

\[\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \nabla f(\bm{x}_{t})\rangle \geq\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \tilde{g}(\hat{\bm{y}}_{t})\rangle+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|\] \[=\mathbb{E}\langle\nabla f(\bm{x}_{t}),\nabla f(\bm{y}_{t})- \nabla f(\hat{\bm{y}}_{t})\rangle+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|\] \[\geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{1}{ 2}\mathbb{E}\|\nabla f(\bm{y}_{t})-\nabla f(\hat{\bm{y}}_{t})\|^{2}+\mu\rho \mathbb{E}\|\nabla f(\bm{x}_{t})\|\] \[\geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{\beta ^{2}}{2}\mathbb{E}\|\bm{y}_{t}-\hat{\bm{y}}_{t}\|^{2}+\mu\rho\mathbb{E}\| \nabla f(\bm{x}_{t})\|\] \[=-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{\beta^{2} }{2}\mathbb{E}\left\|\rho\frac{g(\bm{x}_{t})}{\|g(\bm{x}_{t})\|}-\rho\frac{ \nabla f(\bm{x}_{t})}{\|\nabla f(\bm{x}_{t})\|}\right\|^{2}\] \[\quad+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|\] \[\geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}+\mu\rho \mathbb{E}\|\nabla f(\bm{x}_{t})\|-2\beta^{2}\rho^{2}.\]

Next we consider \(m\)-SAM. additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), starting from Lemma C.1, we have

\[\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \nabla f(\bm{x}_{t})\rangle \geq\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})- \tilde{g}(\hat{\bm{y}}_{t})\rangle+\mu\rho\|\nabla f(\bm{x}_{t})\|\]\[\geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{1}{2} \mathbb{E}\|\tilde{g}(\bm{y}_{t})-\tilde{g}(\hat{\bm{y}}_{t})\|^{2}+\mu\rho \mathbb{E}\|\nabla f(\bm{x}_{t})\|\] \[\geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{\beta ^{2}}{2}\mathbb{E}\|\bm{y}_{t}-\hat{\bm{y}}_{t}\|^{2}+\mu\rho\mathbb{E}\| \nabla f(\bm{x}_{t})\|\] \[=-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{\beta^{ 2}}{2}\mathbb{E}\left\|\rho\frac{g(\bm{x}_{t})}{\|g(\bm{x}_{t})\|}-\rho\frac{ \nabla f(\bm{x}_{t})}{\|\nabla f(\bm{x}_{t})\|}\right\|^{2}\] \[\quad+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|\] \[\geq-\frac{1}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}+\mu\rho \mathbb{E}\|\nabla f(\bm{x}_{t})\|-2\beta^{2}\rho^{2},\]

completing the proof. 

**Lemma C.3**.: _Consider a \(\beta\)-smooth, \(\mu\)-strongly convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, with step size \(\eta\leq\frac{1}{2\beta}\), we have_

\[\mathbb{E}f(\bm{x}_{t+1})\leq\mathbb{E}f(\bm{x}_{t})-\frac{\eta} {2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}-\frac{\eta\mu\rho}{2}\mathbb{E}\| \nabla f(\bm{x}_{t})\|+2\eta\beta^{2}\rho^{2}-\eta^{2}\beta(\beta^{2}\rho^{2} -\sigma^{2}).\]

_Under \(m\)-SAM, additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), the inequality continues to hold._

Proof.: Starting from the definition of \(\beta\)-smoothness, we have

\[\mathbb{E}f(\bm{x}_{t+1}) \leq\mathbb{E}f(\bm{x}_{t})-\eta\mathbb{E}\langle\nabla f(\bm{x} _{t}),\tilde{g}(\bm{y}_{t})\rangle+\frac{\eta^{2}\beta}{2}\mathbb{E}\|\tilde{g }(\bm{y}_{t})\|^{2}\] \[=\mathbb{E}f(\bm{x}_{t})-\eta\mathbb{E}\langle\nabla f(\bm{x}_{t }),\tilde{g}(\bm{y}_{t})\rangle+\frac{\eta^{2}\beta}{2}\mathbb{E}\|\tilde{g}( \bm{y}_{t})-\nabla f(\bm{x}_{t})\|^{2}+\frac{\eta^{2}\beta}{2}\mathbb{E}\| \nabla f(\bm{x}_{t})\|^{2}\] \[\quad+\eta^{2}\beta\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{ g}(\bm{y}_{t})-\nabla f(\bm{x}_{t})\rangle\] \[=\mathbb{E}f(\bm{x}_{t})-\eta\mathbb{E}\langle\nabla f(\bm{x}_{t }),\tilde{g}(\bm{y}_{t})-\nabla f(\bm{x}_{t})\rangle-\eta\mathbb{E}\|\nabla f (\bm{x}_{t})\|^{2}\] \[\quad+\frac{\eta^{2}\beta}{2}\mathbb{E}\|\tilde{g}(\bm{y}_{t})- \nabla f(\bm{x}_{t})\|^{2}+\frac{\eta^{2}\beta}{2}\mathbb{E}\|\nabla f(\bm{x} _{t})\|^{2}\] \[\quad+\eta^{2}\beta\mathbb{E}\langle\nabla f(\bm{x}_{t}),\tilde{ g}(\bm{y}_{t})-\nabla f(\bm{x}_{t})\rangle\] \[\leq\mathbb{E}f(\bm{x}_{t})-\eta(1-\eta\beta)\mathbb{E}\langle \nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})-\nabla f(\bm{x}_{t})\rangle-\eta \left(1-\frac{\eta\beta}{2}\right)\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}\] \[\quad+\eta^{2}\beta\mathbb{E}\|\tilde{g}(\bm{y}_{t})-\nabla f(\bm {y}_{t})\|^{2}+\eta^{2}\beta\mathbb{E}\|\nabla f(\bm{y}_{t})-\nabla f(\bm{x}_{ t})\|^{2}\] \[\leq\mathbb{E}f(\bm{x}_{t})-\eta(1-\eta\beta)\mathbb{E}\langle \nabla f(\bm{x}_{t}),\tilde{g}(\bm{y}_{t})-\nabla f(\bm{x}_{t})\rangle-\eta \left(1-\frac{\eta\beta}{2}\right)\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}\] \[\quad+\eta^{2}\beta(\sigma^{2}+\beta^{2}\rho^{2}).\]

Since we assumed \(\eta\leq\frac{1}{2\beta}\), \(\eta(1-\eta\beta)\geq\frac{\eta}{2}\) hold. Applying Lemma C.2, we get

\[\mathbb{E}f(\bm{x}_{t+1}) \leq\mathbb{E}f(\bm{x}_{t})-\eta(1-\eta\beta)\left(-\frac{1}{2} \mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}+\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t}) \|-2\beta^{2}\rho^{2}\right)\] \[\quad-\eta\left(1-\frac{\eta\beta}{2}\right)\mathbb{E}\|\nabla f( \bm{x}_{t})\|^{2}+\eta^{2}\beta(\sigma^{2}+\beta^{2}\rho^{2})\] \[=\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E}\|\nabla f(\bm{x} _{t})\|^{2}-\eta(1-\eta\beta)\mu\rho\mathbb{E}\|\nabla f(\bm{x}_{t})\|+2\eta \beta^{2}\rho^{2}-\eta^{2}\beta(\beta^{2}\rho^{2}-\sigma^{2})\] \[\leq\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E}\|\nabla f(\bm{x} _{t})\|^{2}-\frac{\eta\mu\rho}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|+2\eta \beta^{2}\rho^{2}-\eta^{2}\beta(\beta^{2}\rho^{2}-\sigma^{2}),\]

completing the proof. 

### Convergence Proof for Smooth and Strongly Convex Functions (Proof of Theorem 4.1)

In this section, we demonstrate the convergence result of stochastic SAM for smooth and strongly convex functions. For convenience, we restate the theorem here.

**Theorem 4.1**.: _Consider a \(\beta\)-smooth, \(\mu\)-strongly convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(x_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{1}{\mu T}\cdot\max\left\{1,\log\left(\frac{\mu^{2}\Delta T }{\beta[\sigma^{2}-\beta^{2}\rho^{2}]_{+}}\right)\right\},\frac{1}{2\beta}\right\}\) to minimize \(f\), we have_

\[\mathbb{E}f(\bm{x}_{T})-f^{*}\leq\tilde{\mathcal{O}}\left(\exp\left(-\frac{\mu T }{2\beta}\right)\Delta+\frac{\beta[\sigma^{2}-\beta^{2}\rho^{2}]_{+}}{\mu^{2} T}\right)+\frac{2\beta^{2}\rho^{2}}{\mu}.\]

_Under \(m\)-SAM, additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), the inequality continues to hold._

Proof.: We start the proof from Lemma C.3; in order to apply the lemma, additionally assuming \(\beta\)-smoothness for component functions \(l(\cdot,\xi)\) is necessary for \(m\)-SAM.

\[\mathbb{E}f(\bm{x}_{t+1})\leq\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E} \|\nabla f(\bm{x}_{t})\|^{2}-\frac{\eta\mu\rho}{2}\mathbb{E}\|\nabla f(\bm{x}_ {t})\|+2\eta\beta^{2}\rho^{2}-\eta^{2}\beta(\beta^{2}\rho^{2}-\sigma^{2}).\]

Since \(\mu\)-strongly convex functions satisfy \(\mu\)-PL inequality (B.1), we get

\[\mathbb{E}f(\bm{x}_{t+1})-f^{*}\leq(1-\eta\mu)(\mathbb{E}f(\bm{x}_{t})-f^{*})- \frac{\eta\mu\rho}{2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|+2\eta\beta^{2}\rho^{2 }-\eta^{2}\beta(\beta^{2}\rho^{2}-\sigma^{2}).\]

Depending on the value of \(\sigma\), there are two cases in which the convergence rate varies.

Case A:\(\sigma\leq\beta\rho\).In this case, we have

\[\mathbb{E}f(\bm{x}_{t+1})-f^{*} \leq(1-\eta\mu)(\mathbb{E}f(\bm{x}_{t})-f^{*})-\frac{\eta\mu\rho} {2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|+2\eta\beta^{2}\rho^{2}\] \[\leq(1-\eta\mu)(\mathbb{E}f(\bm{x}_{t})-f^{*})+2\eta\beta^{2}\rho ^{2},\]

and our choice of \(\eta\) must be \(\frac{1}{2\beta}\). Unrolling the inequality and substituting \(\eta=\frac{1}{2\beta}\) draws out

\[\mathbb{E}f(\bm{x}_{T})-f^{*} \leq(1-\eta\mu)^{T}(\mathbb{E}f(\bm{x}_{0})-f^{*})+\frac{2\beta ^{2}\rho^{2}}{\mu}\] \[=\left(1-\frac{\mu}{2\beta}\right)^{T}\Delta+\frac{2\beta^{2}\rho ^{2}}{\mu}\] \[\leq\exp\left(-\frac{\mu T}{2\beta}\right)\Delta+\frac{2\beta^{2} \rho^{2}}{\mu}.\]

Case B:\(\sigma>\beta\rho\).In this case, we have

\[\mathbb{E}f(\bm{x}_{t+1})-f^{*} \leq(1-\eta\mu)(\mathbb{E}f(\bm{x}_{t})-f^{*})-\frac{\eta\mu\rho} {2}\mathbb{E}\|\nabla f(\bm{x}_{t})\|+2\eta\beta^{2}\rho^{2}+\eta^{2}\beta( \sigma^{2}-\beta^{2}\rho^{2})\] \[\leq(1-\eta\mu)(\mathbb{E}f(\bm{x}_{t})-f^{*})+2\eta\beta^{2} \rho^{2}+\eta^{2}\beta(\sigma^{2}-\beta^{2}\rho^{2}).\]

Again, unrolling the inequality draws out

\[\mathbb{E}f(\bm{x}_{T})-f^{*}\leq(1-\eta\mu)^{T}\Delta+\frac{2\beta^{2}\rho^{2 }}{\mu}+\frac{\eta\beta(\sigma^{2}-\beta^{2}\rho^{2})}{\mu}.\] (10)

Similar to Section B.2, substituting \(\eta=\min\left\{\frac{1}{\mu T}\cdot\max\left\{1,\log\left(\frac{\mu^{2} \Delta T}{\beta[\sigma^{2}-\beta^{2}\rho^{2}]}\right)\right\},\frac{1}{2\beta}\right\}\) can result in four cases.

Case B-1:\(\log\left(\frac{\mu^{2}\Delta T}{\beta[\sigma^{2}-\beta^{2}\rho^{2}]}\right)\geq 1\), and \(\frac{1}{2\beta}\geq\frac{1}{\mu T}\log\left(\frac{\mu^{2}\Delta T}{\beta[ \sigma^{2}-\beta^{2}\rho^{2}]}\right)\).Setting \(\eta=\frac{1}{\mu T}\log\left(\frac{\mu^{2}\Delta T}{\beta[\sigma^{2}-\beta^ {2}\rho^{2}]}\right)\),

\[\mathbb{E}f(\bm{x}_{T})-f^{*}\leq\frac{\beta(\sigma^{2}-\beta^{2}\rho^{2})}{\mu^ {2}T}+\frac{\beta(\sigma^{2}-\beta^{2}\rho^{2})}{\mu^{2}T}\cdot\log\left(\frac{ \mu^{2}\Delta T}{\beta[\sigma^{2}-\beta^{2}\rho^{2}]}\right)+\frac{2\beta^{2} \rho^{2}}{\mu}.\]

Case B-2:\(\log\left(\frac{\mu^{2}\Delta T}{\beta[\sigma^{2}-\beta^{2}\rho^{2}]}\right)\geq 1\), and \(\frac{1}{2\beta}\leq\frac{1}{\mu T}\log\left(\frac{\mu^{2}\Delta T}{\beta[ \sigma^{2}-\beta^{2}\rho^{2}]}\right)\).Setting \(\eta=\frac

**Case B-3: \(\log\left(\frac{\mu^{2}\Delta T}{\beta(\sigma^{2}-\beta^{2}\rho^{2})}\right)\leq 1\), and \(\frac{1}{2\beta}\geq\frac{1}{\mu T}\).** Setting \(\eta=\frac{1}{\mu T}\),

\[\mathbb{E}f(\bm{x}_{T})-f^{*} \leq\left(1-\frac{1}{T}\right)^{T}\Delta+\frac{\beta(\sigma^{2}- \beta^{2}\rho^{2})}{\mu^{2}T}+\frac{2\beta^{2}\rho^{2}}{\mu}\] \[\leq\left(1-\frac{1}{T}\log\left(\frac{\mu^{2}\Delta T}{\beta( \sigma^{2}-\beta^{2}\rho^{2})}\right)\right)^{T}\Delta+\frac{\beta(\sigma^{2}- \beta^{2}\rho^{2})}{\mu^{2}T}+\frac{2\beta^{2}\rho^{2}}{\mu}\] \[\leq\frac{\beta(\sigma^{2}-\beta^{2}\rho^{2})}{\mu^{2}T}+\frac{ \beta(\sigma^{2}-\beta^{2}\rho^{2})}{\mu^{2}T}+\frac{2\beta^{2}\rho^{2}}{\mu}.\]

**Case B-4: \(\log\left(\frac{\mu^{2}\Delta T}{\beta(\sigma^{2}-\beta^{2}\rho^{2})}\right)\leq 1\), and \(\frac{1}{2\beta}\leq\frac{1}{\mu T}\).** Setting \(\eta=\frac{1}{2\beta}\),

\[\mathbb{E}f(\bm{x}_{T})-f^{*} \leq\exp\left(-\frac{\mu T}{2\beta}\right)\Delta+\frac{\beta( \sigma^{2}-\beta^{2}\rho^{2})}{\mu}\cdot\frac{1}{2\beta}+\frac{2\beta^{2} \rho^{2}}{\mu}\] \[\leq\exp\left(-\frac{\mu T}{2\beta}\right)\Delta+\frac{\beta( \sigma^{2}-\beta^{2}\rho^{2})}{\mu^{2}T}+\frac{2\beta^{2}\rho^{2}}{\mu}.\]

Merging all four cases, we get

\[\mathbb{E}f(\bm{x}_{T})-f^{*}=\tilde{\mathcal{O}}\left(\exp\left(-\frac{\mu T }{2\beta}\right)\Delta+\frac{\beta(\sigma^{2}-\beta^{2}\rho^{2})}{\mu^{2}T} \right)+\frac{2\beta^{2}\rho^{2}}{\mu},\]

thereby finishing the proof. 

### Non-Convergence of \(m\)-SAM for a Smooth and Strongly Convex Function (Proof of Theorem 4.2)

In this section, we present a formal analysis of our counterexample, which shows that stochastic \(m\)-SAM with constant perturbation size \(\rho\) may fail to fully converge to a global minimum, and can get close to the global minimum by only \(\Omega(\rho)\). This counterexample indicates that the \(\mathcal{O}(\rho^{2})\) term in Theorem 4.1 is unavoidable. For readers' convenience, we restate the theorem.

**Theorem 4.2**.: _For any \(\rho>0,\beta>0,\sigma>\beta\rho\) and \(\eta\leq\frac{3}{10\beta}\), there exists a \(\beta\)-smooth and \(\frac{\beta}{5}\)-strongly convex function \(f\) satisfying the following. (1) The function \(f\) satisfies Assumption 2.5. (2) The component functions \(l(\cdot;\xi)\) of \(f\) are \(\beta\)-smooth for any \(\xi\). (3) If we run \(m\)-SAM on \(f\) initialized inside a certain interval, then any arbitrary weighted average \(\bar{\bm{x}}\) of the iterates \(\bm{x}_{0},\bm{x}_{1},\ldots\) must satisfy \(\mathbb{E}[f(\bar{\bm{x}})-f^{*}]\geq\Omega(\rho^{2})\)._

Proof.: Given \(\rho>0\), \(\beta>0\), \(\sigma\geq 0\), and \(\eta\leq\frac{3}{10\beta}\), we choose \(p\triangleq\frac{2}{3}\), \(c\triangleq\left(1+\frac{p}{4}\right)\rho\). For \(x\in\mathbb{R}\), and a constant \(a>0\) which will be chosen later. Using these \(p\), \(c\), and \(a\), we construct the counterexample. We consider a one-dimensional smooth strongly convex function

\[f(x)=\frac{a}{2}x^{2},\]

and the stochastic function \(l(x;\xi)\) can be given as

\[l(x;\xi)=\begin{cases}f^{(1)}(x),&\text{with probability }p\\ f^{(2)}(x),&\text{otherwise},\end{cases}\]

where each component functions are as described below:

\[f^{(1)}(x)=\begin{cases}\frac{a}{2}(x-c+2\rho)^{2}-a\rho^{2},&x\leq c-\rho\\ -\frac{a}{2}(x-c)^{2},&c-\rho\leq x\leq c+\rho\\ \frac{a}{2}(x-c-2\rho)^{2}-a\rho^{2},&c+\rho\leq x.\end{cases}\]

\[f^{(2)}(x)=\begin{cases}\frac{1}{1-p}\left(\frac{a}{2}x^{2}-\frac{pa}{2}(x-c+2 \rho)^{2}+pa\rho^{2}\right),&x\leq c-\rho\\ \frac{1}{1-p}\left(\frac{a}{2}x^{2}+\frac{pa}{2}(x-c)^{2}\right),&c-\rho\leq x \leq c+\rho\\ \frac{1}{1-p}\left(\frac{a}{2}x^{2}-\frac{pa}{2}(x-c-2\rho)^{2}+pa\rho^{2} \right),&c+\rho\leq x.\end{cases}\]First, it is easy to verify that \(\mathbb{E}l(x;\xi)=f(x)\). We can also confirm that \(f\), \(f^{(1)}\) are \(a\)-smooth, and \(f^{(2)}\) is \(\left(\frac{1+p}{1-p}a\right)\)-smooth. Furthermore, \(\|\nabla f(x)-\nabla f^{(1)}(x)\|^{2}\leq a^{2}(2\rho+c)^{2}\) holds, along with \(\|\nabla f(x)-\nabla f^{(2)}(x)\|^{2}\leq\left(\frac{pq}{1-p}\right)^{2}(2 \rho+c)^{2}\). Consequently, we have \(\mathbb{E}\|\nabla f(x)-\nabla l(x;\xi)\|^{2}\leq\frac{pa^{2}}{1-p}(2\rho+c)^{ 2}=2a^{2}\cdot\left(\frac{19\rho}{6}\right)^{2}\), where we used \(p=\frac{2}{3}\) and \(c=(1+\frac{p}{4})\rho\).

Now choose \(a=\min\left\{\frac{\beta}{5},\frac{\sigma}{5\rho}\right\}\leq\frac{\beta}{5}\). This choice ensures that \(f\), \(f^{(1)}\), and \(f^{(2)}\) are all \(\beta\)-smooth, as desired. Also notice that \(2a^{2}\cdot\left(\frac{19\rho}{6}\right)^{2}\leq 2\cdot\left(\frac{19}{5 \cdot 6}\right)^{2}\sigma^{2}<\sigma^{2}\), so the function satisfies Assumption 2.5. For the remaining of the proof, we investigate the virtual gradient maps \(G_{f^{(1)}}\) and \(G_{f^{(2)}}\) within the specified region of interest: \(c-\rho\leq x\leq c+\rho\). We will then demonstrate that if we initialize \(m\)-SAM in this interval, all subsequent iterations of \(m\)-SAM will remain inside \([c-\rho,c+\rho]\), as required above.

Within this interval, the perturbed iterate \(y=\rho\frac{\nabla f^{(1)}(x)}{\|\nabla f^{(1)}(x)\|}\) and the virtual gradient map \(G_{f^{(1)}}\) of \(f^{(1)}\) can be described as follows.

\[y=\begin{cases}x+\rho,&x<c\\ x,&x=c\\ x-\rho,&x>c,\end{cases}\quad G_{f^{(1)}}(x)=\begin{cases}-a(x+\rho-c),&x<c\\ 0,&x=c\\ -a(x-\rho-c),&x>c.\end{cases}\]

Additionally defining \(c^{\prime}\triangleq\frac{p}{1+p}c\), we now calculate \(y=\rho\frac{\nabla f^{(2)}(x)}{\|\nabla f^{(2)}(x)\|}\) and the virtual gradient map \(G_{f^{(2)}}\) of \(f^{(2)}\).

\[y=\begin{cases}x-\rho,&x<c^{\prime},\\ x,&x=c^{\prime},\\ x+\rho,&x>c^{\prime}\end{cases}\quad G_{f^{(2)}}(x)=\begin{cases}\frac{a}{1-p} \left(x-\rho+p(x-\rho-c)\right),&x<c^{\prime}\\ 0,&x=c^{\prime}\\ \frac{a}{1-p}\left(x+\rho+p(x+\rho-c)\right),&x>c^{\prime}\end{cases}\]

Here, given \(c=\left(1+\frac{p}{4}\right)\rho\), we can verify that

\[0<c-\rho\leq c^{\prime}\leq c\leq c+\rho,\]

where \(0\), \(c^{\prime}\), and \(c\) are the global minimum (or maximum) of \(f\), \(f^{(2)}\), and \(f^{(1)}\), respectively.

Recall that \(a\leq\frac{\beta}{5}\). Since \(\eta\leq\frac{3}{10\beta}\), we have \(\eta a\leq\frac{3}{50}\), which will be useful for the rest of the proof. Now, we analyze \(x_{t+1}\), the next iterate of \(m\)-SAM. For \(c-\rho\leq x_{t}\leq c+\rho\), the next iterate \(x_{t+1}\) of \(m\)-SAM using \(G_{f^{(1)}}(x_{t})\) is

\[x_{t+1}=\begin{cases}x_{t}+\eta a(x_{t}+\rho-c),&x_{t}<c\\ x_{t},&x_{t}=c\\ x_{t}+\eta a(x_{t}-\rho-c),&x_{t}>c.\end{cases}\]

We will show that \(x_{t+1}\) must stay within the interval \([c-\rho,c+\rho]\). For the case \(x_{t}=c\), the inclusion is trivial. To analyze the rest, we divide into two cases.

Case A-1: \(c-\rho\leq x_{t}<c\).Since \(\eta a\leq\frac{3}{50}\), we have

\[c-\rho\leq x_{t}\leq x_{t+1} =x_{t}+\eta a(x_{t}+\rho-c)\leq c+\eta a(c+\rho-c)\] \[=c+\eta a\rho\leq c+\frac{3\rho}{50}.\]

Case A-2: \(c<x_{t}\leq c+\rho\).Since \(\eta a\leq\frac{3}{50}\), we have

\[c+\rho\geq x_{t}\geq x_{t+1} =x_{t}+\eta a(x_{t}-\rho-c)\geq c+\eta a(c-\rho-c)\] \[=c-\eta a\rho\geq c-\frac{3\rho}{50}.\]

Cases A-1 and A-2 show that \(x_{t+1}\) also remains in \([c-\rho,c+\rho]\), when we update \(m\)-SAM using \(G_{f^{(1)}}(x_{t})\).

We next examine the case involving \(G_{f^{(2)}}\). The next iterate \(x_{t+1}\) of \(m\)-SAM using \(G_{f^{(2)}}(x_{t})\) is

\[x_{t+1}=\begin{cases}x_{t}-\eta\frac{a}{1-p}\left(x_{t}-\rho+p(x_{t}-\rho-c) \right),&x_{t}<c^{\prime}\\ x_{t},&x_{t}=c^{\prime}\\ x_{t}-\eta\frac{a}{1-p}\left(x_{t}+\rho+p(x_{t}+\rho-c)\right),&x_{t}>c^{\prime}.\end{cases}\]

Again, we divide it into two cases, since the \(x_{t}=c^{\prime}\) case is obvious.

Case B-1: \(c-\rho\leq x_{t}<c^{\prime}\).Since \((1+p)x_{t}-(1+p)\rho-pc<0\) for \(x_{t}\in[c-\rho,c^{\prime})\), we have

\[c-\rho\leq x_{t}\leq x_{t+1} =x_{t}-\eta\frac{a}{1-p}\left((1+p)x_{t}-(1+p)\rho-pc\right)\] \[=\left(1-\eta\frac{a(1+p)}{1-p}\right)x_{t}+\eta\frac{a(1+p)}{1-p }\rho+\eta\frac{ap}{1-p}c.\]

Since \(\eta a\leq\frac{3}{50}\), we have \(1-\eta\frac{a(1+p)}{1-p}=1-5\eta a\geq 0\), so

\[\left(1-\eta\frac{a(1+p)}{1-p}\right)x_{t}+\eta\frac{a(1+p)}{1-p }\rho+\eta\frac{ap}{1-p}c\] \[\leq\left(1-\eta\frac{a(1+p)}{1-p}\right)c^{\prime}+\eta\frac{a(1 +p)}{1-p}\rho+\eta\frac{ap}{1-p}c\] \[= c^{\prime}+\eta\frac{a(1+p)}{1-p}\rho=c^{\prime}+5\eta a\rho \leq c^{\prime}+\rho\leq c+\rho.\]

Case B-2: \(c^{\prime}<x_{t}\leq c+\rho\).Since \((1+p)x_{t}+(1+p)\rho-pc>0\) for \(x_{t}\in(c^{\prime},c+\rho]\), we have

\[c+\rho\geq x_{t}\geq x_{t+1} =x_{t}-\eta\frac{a}{1-p}\left((1+p)x_{t}+(1+p)\rho-pc\right)\] \[\geq c^{\prime}-\eta\frac{a}{1-p}\left((1+p)c^{\prime}+(1+p)\rho -pc\right)\] \[=\frac{p}{1+p}c-\eta\frac{a(1+p)}{1-p}\rho=c-\frac{1}{1+p}c-5\eta a\rho\] \[=c-\frac{1}{1+p}\cdot\left(1+\frac{p}{4}\right)\rho-5\eta a\rho\] \[\geq c-\frac{3}{5}\cdot\left(\frac{7}{6}\right)\rho-\frac{3}{10} \rho=c-\rho,\]

Figure 4: The original and virtual loss plot for the example function in Theorem 4.2. The graph drawn in purple and red are the original/virtual loss of component functions. The graph drawn in green indicates \(f\), and the graph drawn in blue indicates \(\mathbb{E}J_{f}\). (a) \(f\) and its component functions \(f^{(1)}\), \(f^{(2)}\). (b)\(\mathbb{E}J_{f}\) and its component functions \(J_{f^{(1)}}\), \(J_{f^{(2)}}\).

where the last inequality used \(\eta a\leq\frac{3}{50}\).

Cases **B-1** and **B-2** demonstrate that \(x_{t+1}\) also remains within the interval \([c-\rho,c+\rho]\) when we update \(m\)-SAM using \(G_{f^{(2)}}(x_{t})\).

To gain a better intuitive understanding, we provide Figures 4(a), 4(b), demonstrating the original and virtual loss functions of \(f^{(1)}\) and \(f^{(2)}\). In Figure 4(b), we can examine that \(m\)-SAM updates generate an attraction basin in \(J_{f^{(1)}}\), thereby making a region that \(m\)-SAM cannot escape.

The aforementioned case analyses indicate that when the initial point \(x_{0}\) falls within the interval \([c-\rho,c+\rho]\), all subsequent iterations of \(m\)-SAM will also remain within this interval, regardless of the selected component function for updating. Given \(c=\left(1+\frac{p}{4}\right)\rho=\frac{7}{6}\rho\), we can conclude that all subsequent iterations of \(m\)-SAM are points located at a distance of at least \(\frac{1}{6}\rho\) from the global optimum, for any \(\beta>0\), \(\rho>0\), and \(\sigma>0\).

Moreover, in case of \(\sigma>\beta\rho\), it follows that \(f(x)=\frac{a}{2}x^{2}=\frac{\beta}{10}x^{2}\). Consequently, the suboptimality gap at any timestep is at least

\[f(x_{t})-f^{*}=f(x_{t})\geq f\left(\frac{\rho}{6}\right)=\Omega(\beta\rho^{2 }),\]

thereby finishing the proof. 

### Convergence Proof for Smooth and Convex Functions (Proof of Theorem 4.3)

In this section, we establish the convergence of stochastic SAM for smooth and convex functions to near-stationary points. For ease of understanding, we provide the theorem statement here.

**Theorem 4.3**.: _Consider a \(\beta\)-smooth, convex function \(f\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(x_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{\sqrt{\Delta}}{\sqrt{\beta[\sigma^{2}-\beta^{2}\rho^{2} ]_{+}T}},\frac{1}{2\beta}\right\}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}= \mathcal{O}\bigg{(}\frac{\beta\Delta}{T}+\frac{\sqrt{\beta[\sigma^{2}-\beta^ {2}\rho^{2}]_{+}\Delta}}{\sqrt{T}}\bigg{)}+4\beta^{2}\rho^{2}.\]

_Under \(m\)-SAM, additionally assuming \(l(\cdot,\xi)\) is \(\beta\)-smooth for any \(\xi\), the inequality continues to hold._

Proof.: We start from Lemma C.3 with \(\mu=0\). In order to do this, additionally assuming \(\beta\)-smoothness of component functions \(l(\cdot,\xi)\) is necessary for \(m\)-SAM.

\[\mathbb{E}f(\bm{x}_{t+1})\leq\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E} \|\nabla f(\bm{x}_{t})\|^{2}+2\beta^{2}\rho^{2}\eta+\beta\eta^{2}(\sigma^{2}- \beta^{2}\rho^{2}),\]

which can be rewritten as

\[\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2}{\eta}(\mathbb{E}f(\bm{x}_{ t})-\mathbb{E}f(\bm{x}_{t+1}))+4\beta^{2}\rho^{2}+2\beta\eta(\sigma^{2}- \beta^{2}\rho^{2}).\]

Adding up the inequality for \(t=0,\cdots,T-1\), and dividing both sides by \(T\), we get

\[\frac{1}{T}\sum\limits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t })\|^{2} \leq\frac{2}{\eta T}(\mathbb{E}f(\bm{x}_{0})-\mathbb{E}f(\bm{x}_{ T}))+4\beta^{2}\rho^{2}+2\beta\eta(\sigma^{2}-\beta^{2}\rho^{2})\] \[\leq\frac{2}{\eta T}\Delta+4\beta^{2}\rho^{2}+2\eta\beta(\sigma^{ 2}-\beta^{2}\rho^{2}).\] (11)

The convergence rate varies depending on two different cases determined by the value of \(\sigma\).

Case A:\(\sigma\leq\beta\rho\).In this case, it must hold that \(\eta=\frac{1}{2\beta}\). By this choice of \(\eta\), we have

\[\frac{1}{T}\sum\limits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\| ^{2} \leq\frac{2\Delta}{\eta T}+4\beta^{2}\rho^{2}\] \[=\frac{4\beta\Delta}{T}+4\beta^{2}\rho^{2}.\]Case B: \(\sigma>\beta\rho\).Setting \(\eta=\min\left\{\sqrt{\frac{\Delta}{\beta(\sigma^{2}-\beta^{2}\rho^{2})T}},\frac{1 }{2\beta}\right\}\), we consider two cases.

Case B-1: \(\sqrt{\frac{\Delta}{\beta(\sigma^{2}-\beta^{2}\rho^{2})T}}\leq\frac{1}{2\beta}\).Putting \(\eta=\sqrt{\frac{\Delta}{\beta(\sigma^{2}-\beta^{2}\rho^{2})T}}\) into (11), we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2} \leq 4\sqrt{\frac{\beta(\sigma^{2}-\beta^{2}\rho^{2})\Delta}{T}}+4\beta^{2} \rho^{2}.\]

Case B-2: \(\sqrt{\frac{\Delta}{\beta(\sigma^{2}-\beta^{2}\rho^{2})T}}\geq\frac{1}{2\beta}\).Placing \(\eta=\frac{1}{2\beta}\) into (11), we get

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2} \leq\frac{4\beta\Delta}{T}+4\beta^{2}\rho^{2}+2\cdot\frac{1}{2 \beta}\cdot\beta(\sigma^{2}-\beta^{2}\rho^{2})\] \[\leq\frac{4\beta\Delta}{T}+4\beta^{2}\rho^{2}+2\cdot\sqrt{\frac{ \Delta}{\beta(\sigma^{2}-\beta^{2}\rho^{2})T}}\cdot\beta(\sigma^{2}-\beta^{2} \rho^{2})\] \[=\frac{4\beta\Delta}{T}+2\sqrt{\frac{\beta(\sigma^{2}-\beta^{2} \rho^{2})\Delta}{T}}+4\beta^{2}\rho^{2}.\]

Merging the two cases, we conclude

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2} =\mathcal{O}\left(\frac{\beta\Delta}{T}+\frac{\sqrt{\beta(\sigma^{2}-\beta^{2 }\rho^{2})\Delta}}{\sqrt{T}}\right)+4\beta^{2}\rho^{2},\]

thereby completing the proof. 

### Non-Convergence of \(m\)-SAM for a Smooth and Convex Function (Proof of Theorem 4.4)

In this section, we provide an in-depth analysis of our counterexample, which demonstrates that stochastic \(m\)-SAM with constant perturbation size \(\rho\) can provably converge to a point that is not a global minimum. Furthermore, the suboptimality gap in terms of the function value can be made arbitrarily large, hence indicating that proving convergence of \(m\)-SAM to global minima (modulo some additive factors \(\mathcal{O}(\rho^{2})\)) is impossible. The theorem is restated for convenience.

**Theorem 4.4**.: _For any \(\rho>0\), \(\beta>0\), \(\sigma>0\), and \(\eta\leq\frac{1}{\beta}\), there exists a \(\beta\)-smooth and convex function \(f\) satisfying the following. (1) The function \(f\) satisfies Assumption 2.5. (2) The component functions \(l(\cdot;\xi)\) of \(f\) are \(\beta\)-smooth for any \(\xi\). (3) If we run \(m\)-SAM on \(f\) initialized inside a certain interval, then any arbitrary weighted average \(\bar{\bm{x}}\) of the iterates \(\bm{x}_{0},\bm{x}_{1},\dots\) must satisfy \(\mathbb{E}[f(\bar{\bm{x}})-f^{*}]\geq C\), and the suboptimality gap \(C\) can be made arbitrarily large and independent of the parameter \(\rho\)._

Proof.: Given \(\rho>0\), \(\beta>0\), \(\sigma>0\), we set an arbitrary constant \(c>\frac{5\rho}{4}\), and a parameter \(a>0\) which will be chosen later. For \(x\in\mathbb{R}\), consider a one-dimensional smooth convex function,

\[f(x)=\begin{cases}ax+\frac{\beta}{2}x^{2},&x\leq 0\\ ax,&x\geq 0,\end{cases}\]

and \(l(x;\xi)\) can be given as

\[l(x;\xi)=\begin{cases}f^{(1)}(x),&\text{with probability }p\\ f^{(2)}(x),&\text{otherwise},\end{cases}\]

where the functions \(f^{(1)}\) and \(f^{(2)}\) are given by the following definitions.

\[f^{(1)}(x)=\begin{cases}2a\left(x-c+\frac{\rho}{8}\right)+\frac{\beta}{2}x^{2},&x\leq 0\\ 2a\left(x-c+\frac{\rho}{8}\right),&0\leq x\leq c-\frac{\rho}{4}\\ -\frac{4a}{\rho}\left(x-c\right)^{2},&c-\frac{\rho}{4}\leq x\leq c+\frac{\rho}{ 4}\\ -2a\left(x-c-\frac{\rho}{8}\right),&c+\frac{\rho}{4}\leq x,\end{cases}\]\[f^{(2)}(x)=\begin{cases}\frac{1}{1-p}\left(ax-2pa\left(x-c+\frac{\rho}{8}\right) \right)+\frac{\beta}{2}x^{2},&x\leq 0\\ \frac{1}{1-p}\left(ax-2pa\left(x-c+\frac{\rho}{8}\right)\right),&0\leq x\leq c- \frac{\rho}{4}\\ \frac{1}{1-p}\left(ax+\frac{4pa}{\rho}(x-c)^{2}\right),&c-\frac{\rho}{4}\leq x \leq c+\frac{\rho}{4}\\ \frac{1}{1-p}\left(ax+2pa\left(x-c-\frac{\rho}{8}\right)\right),&c+\frac{\rho} {4}\leq x.\end{cases}\]

It can be verified that \(f^{(1)}\) is \(\left(\max\left\{\frac{8a}{\rho},\beta\right\}\right)\)-smooth, and \(f^{(2)}\) is \(\left(\max\left\{\frac{8a}{\rho},\frac{p}{1-p},\beta\right\}\right)\)-smooth. Moreover, \(\|\nabla f(x)-\nabla f^{(1)}(x)\|^{2}\leq 9a^{2}\) holds, as well as \(\|\nabla f(x)-\nabla f^{(2)}(x)\|^{2}\leq 9a^{2}\cdot\frac{p^{2}}{(1-p)^{2}}\). Consequently, \(\mathbb{E}\|\nabla f(x)-g(x)\|^{2}\leq 9a^{2}\cdot\frac{p}{1-p}\).

By selecting \(p>\frac{1}{2}\), and setting \(a=\min\left\{\frac{\beta\rho(1-p)}{8p},\frac{\sigma\sqrt{1-p}}{3\sqrt{p}}\right\}\), we can check that \(\mathbb{E}l(x;\xi)=f(x)\), and the component functions are \(\beta\)-smooth. Additionally, \(f\) satisfies Assumption 2.5.

In the following analysis, we examine the virtual gradient maps \(G_{f^{(1)}}\) and \(G_{f^{(2)}}\) in the specified region of interest: \(c-\rho\leq x\leq c+\rho\). For this specific interval, we are going to show that if we start \(m\)-SAM inside this interval, then all the subsequent iterates of \(m\)-SAM must stay inside the same interval \([c-\rho,c+\rho]\).

In this region, \(y=x+\rho\frac{\nabla f^{(1)}(x)}{\|\nabla f^{(1)}(x)\|}\) and \(G_{f^{(1)}}\) are as follows.

\[y=\begin{cases}x+\rho,&c-\rho\leq x<c\\ x,&x=c\\ x-\rho,&c<x\leq c+\rho.\end{cases}\]

\[G_{f^{(1)}}(x)=\begin{cases}-\frac{8a}{\rho}(x+\rho-c),&c-\rho\leq x\leq c- \frac{3\rho}{4}\\ -2a,&c-\frac{3\rho}{4}\leq x<c\\ 0,&x=c\\ 2a,&c<x\leq c+\frac{3\rho}{4}\\ -\frac{8a}{\rho}(x-\rho-c),&c+\frac{3\rho}{4}\leq x\leq c+\rho.\end{cases}\]

Additionally defining \(c^{\prime}\triangleq c-\frac{\rho}{8p}\), we can compute \(y=x+\rho\frac{\nabla f^{(2)}(x)}{\|\nabla f^{(2)}(x)\|}\) and \(G_{f^{(2)}}\) as follows.

\[y=\begin{cases}x-\rho,&c-\rho\leq x<c^{\prime}\\ x,&x=c^{\prime}\\ x+\rho,&c^{\prime}<x\leq c+\rho.\end{cases}\]

\[G_{f^{(2)}}(x)=\begin{cases}\frac{a-2ap}{1-p},&c-\rho\leq x<c^{\prime}\\ 0,&x=c^{\prime}\\ \frac{a+2ap}{1-p},&c^{\prime}<x\leq c+\rho.\end{cases}\]

Now recall that \(\eta\leq\frac{1}{\beta}\). Given that \(a\leq\frac{\beta\rho(1-p)}{8p}\), we can derive \(\eta\leq\frac{1}{\beta}\leq\frac{\rho}{a}\cdot\frac{1-p}{8p}\). Furthermore, the next iterate \(x_{t+1}\) of \(m\)-SAM using \(G_{f^{(1)}}(x_{t})\) for \(c-\rho\leq x_{t}\leq c+\rho\) is

\[x_{t+1}=x_{t}-\eta G_{f^{(1)}}(x_{t})=\begin{cases}x_{t}+\eta\cdot\frac{8a}{ \rho}(x+\rho-c),&c-\rho\leq x_{t}\leq c-\frac{3\rho}{4}\\ x_{t}+2\eta a,&c-\frac{3\rho}{4}\leq x_{t}<c\\ x_{t},&x_{t}=c\\ x_{t}-2\eta a,&c<x_{t}\leq c+\frac{3\rho}{4}\\ x_{t}+\eta\cdot\frac{8a}{\rho}(x-\rho-c),&c+\frac{3\rho}{4}\leq x_{t}\leq c+ \rho.\end{cases}\]

We will now show that \(x_{t+1}\) must remain in the interval \([c-\rho,c+\rho]\). For the case \(x_{t}=c\), it is obvious. Dividing the rest into three cases,

Case A-1: \(c-\rho\leq x_{t}\leq c-\frac{3\rho}{4}\).Since \(0\leq\eta\cdot\frac{8a}{\rho}(x_{t}+\rho-c)\leq 2\eta a\leq\rho\cdot\frac{1-p}{4p}\leq \frac{\rho}{4}\), we have

\[c-\rho\leq x_{t}\leq x_{t+1}\leq x_{t}+\frac{\rho}{4}\leq c+\rho,\]

which proves that \(x_{t+1}\) also remains in \([c-\rho,c+\rho]\).

**Case A-2: \(c-\frac{3\rho}{4}\leq x_{t}<c\), \(c<x_{t}\leq c+\frac{3\rho}{4}\).** Since \(0\leq 2\eta a\leq\rho\cdot\frac{1-p}{4p}\leq\frac{\rho}{4}\), we have

\[c-\rho\leq x_{t}-\frac{\rho}{4}\leq x_{t+1}\leq x_{t}+\frac{\rho}{4}\leq c+\rho,\]

thereby proving that \(x_{t+1}\) also remains in \([c-\rho,c+\rho]\).

**Case A-3: \(c+\frac{3\rho}{4}\leq x_{t}\leq c+\rho\).** Since \(0\geq\eta\cdot\frac{8a}{\rho}(x_{t}-\rho-c)\geq-2\eta a\geq-\rho\cdot\frac{1- p}{4p}\geq-\frac{\rho}{4}\), we have

\[c-\rho\leq x_{t}-\frac{\rho}{4}\leq x_{t+1}\leq x_{t}\leq c+\rho,\]

which shows that \(x_{t+1}\) also remains in \([c-\rho,c+\rho]\).

We next consider the case of \(G_{f^{(2)}}\). The next iterate \(x_{t+1}\) of \(m\)-SAM using \(G_{f^{(2)}}(x_{t})\) for \(c-\rho\leq x_{t}\leq c+\rho\) is given by

\[x_{t+1}=x_{t}-\eta G_{f^{(2)}}(x_{t})=\begin{cases}x_{t}-\eta a\cdot\frac{1-2 p}{1-p},&c-\rho\leq x_{t}<c^{\prime}\\ x_{t},&x_{t}=c^{\prime}\\ x_{t}-\eta a\cdot\frac{1+2p}{1-p},&c^{\prime}<x_{t}\leq c+\rho.\end{cases}\]

Again, the \(x_{t}=c^{\prime}\) case is trivial. Considering the remaining two cases,

**Case B-1: \(c-\rho\leq x_{t}<c^{\prime}\).** Since \(0\geq\eta a\cdot\frac{1-2p}{1-p}\geq\rho\cdot\frac{1-2p}{8p}\geq-(c+\rho-c^{ \prime})\), we get

\[c-\rho\leq x_{t}\leq x_{t+1}\leq x_{t}+(c+\rho-c^{\prime})\leq c+\rho,\]

which indicates that \(x_{t+1}\) also remains in \([c-\rho,c+\rho]\).

**Case B-2: \(c^{\prime}<x_{t}\leq c+\rho\).** Since \(0\leq\eta a\cdot\frac{1+2p}{1-p}\leq\rho\cdot\frac{1+2p}{8p}\leq c^{\prime}-( c-\rho)\), we have

\[c-\rho\leq x_{t}-c^{\prime}+(c-\rho)\leq x_{t+1}\leq x_{t}\leq c+\rho,\]

which shows that \(x_{t+1}\) also remains in \([c-\rho,c+\rho]\).

The case analyses above indicate that if \(x_{0}\) is initialized in \([c-\rho,c+\rho]\), the subsequent iterates of \(m\)-SAM also remain in \([c-\rho,c+\rho]\), regardless of the component function chosen by \(m\)-SAM for update. Figures 5(a) and 5(b) demonstrate the original loss function and virtual loss function of component functions \(f^{(1)}\) and \(f^{(2)}\). Due to the concavity of \(f^{(1)}\), a basin of attraction is formed in \(J_{f^{(1)}}\), thereby creating a region where the iterations of \(m\)-SAM cannot escape.

Figure 5: The original and virtual loss plot for the example function in Theorem 4.4. The graph drawn in purple and red are the original/virtual loss of component functions. The graph drawn in green indicates \(f\), and the graph drawn in blue indicates \(\mathbb{E}J_{f}\). (a) \(f\) and its component functions \(f^{(1)}\), \(f^{(2)}\). (b)\(\mathbb{E}J_{f}\) and its component functions \(J_{f^{(1)}}\), \(J_{f^{(2)}}\).

Therefore, the suboptimality gap at any timestep is at least

\[f(x_{t})-f^{*}=f(x_{t})-f\left(-\frac{a}{\beta}\right)\geq f(c-\rho)-f\left(- \frac{a}{\beta}\right)=a(c-\rho)+\frac{a^{2}}{2\beta}.\]

Therefore, regardless of iterate averaging scheme, the suboptimality gap in terms of function value will stay above \(a(c-\rho)+\frac{a^{2}}{2\beta}\).

Moreover, this suboptimality gap can be made arbitrarily large if we choose larger values of \(c\); notice that \(c>\frac{5\rho}{4}\) is the only requirement on \(c\). Consequently, it is impossible to guarantee convergence to global minima, even up to an additive factor, for \(m\)-SAM. This finishes the proof. 

### Convergence Proof for Smooth and Nonconvex Function under \(n\)-SAM (Proof of Theorem 4.5)

In this section, we establish the convergence result of stochastic \(n\)-SAM for smooth and nonconvex functions. For convenience, we restate the theorem.

**Theorem 4.5**.: _Consider a \(\beta\)-smooth function \(f\) satisfying \(f^{*}=\inf_{x}f(\bm{x})>-\infty\), and assume Assumption 2.5. Under \(n\)-SAM, starting at \(\bm{x}_{0}\) with any perturbation size \(\rho>0\) and step size \(\eta=\min\left\{\frac{1}{2\beta},\frac{\sqrt{\Delta}}{\sqrt{\beta\sigma^{2}T }}\right\}\) to minimize \(f\), we have_

\[\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2} \leq\mathcal{O}\left(\frac{\beta\Delta}{T}+\frac{\sqrt{\beta\sigma^{2}\Delta }}{\sqrt{T}}\right)+\beta^{2}\rho^{2}.\]

Proof.: Starting from the definition of \(\beta\)-smoothness, we have

\[\mathbb{E}f(\bm{x}_{t+1}) \leq\mathbb{E}f(\bm{x}_{t})-\eta\mathbb{E}\left\langle\nabla f( \bm{x}_{t}),\tilde{g}(\bm{y}_{t})\right\rangle+\frac{\beta\eta^{2}}{2}\mathbb{ E}\|\tilde{g}(\bm{y}_{t})\|^{2}\] \[\leq\mathbb{E}f(\bm{x}_{t})-\eta\mathbb{E}\left\langle\nabla f( \bm{x}_{t}),\nabla f(\bm{y}_{t})\right\rangle+\beta\eta^{2}\left(\mathbb{E}\| \nabla f(\bm{y}_{t})\|^{2}+\mathbb{E}\|\tilde{g}(\bm{y}_{t})-\nabla f(\bm{y}_ {t})\|^{2}\right)\] \[\leq\mathbb{E}f(\bm{x}_{t})-\eta\mathbb{E}\left\langle\nabla f( \bm{x}_{t}),\nabla f(\bm{y}_{t})\right\rangle+\beta\eta^{2}(\mathbb{E}\| \nabla f(\bm{y}_{t})\|^{2}+\sigma^{2})\] \[=\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E}\|\nabla f(\bm{ x}_{t})\|^{2}-\frac{\eta}{2}\mathbb{E}\|\nabla f(\bm{y}_{t})\|^{2}+\frac{\eta}{2} \mathbb{E}\|\nabla f(\bm{x}_{t})-\nabla f(\bm{y}_{t})\|^{2}\] \[\quad+\beta\eta^{2}(\mathbb{E}\|\nabla f(\bm{y}_{t})\|^{2}+ \sigma^{2})\] \[\leq\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E}\|\nabla f(\bm {x}_{t})\|^{2}+\frac{\beta^{2}\eta}{2}\mathbb{E}\|\bm{x}_{t}-\bm{y}_{t}\|^{2} +\beta\sigma^{2}\eta^{2}\] \[=\mathbb{E}f(\bm{x}_{t})-\frac{\eta}{2}\mathbb{E}\|\nabla f(\bm{ x}_{t})\|^{2}+\frac{\beta^{2}\rho^{2}\eta}{2}+\beta\sigma^{2}\eta^{2}.\]

Rearranging the inequality, we get

\[\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}\leq\frac{2}{\eta}(\mathbb{E}f(\bm{x}_{ t})-\mathbb{E}f(\bm{x}_{t+1}))+\beta^{2}\rho^{2}+2\beta\sigma^{2}\eta.\]

Adding up the inequality for \(t=0,\cdots,T-1\), and dividing both sides by \(T\), we get

\[\frac{1}{T}\sum\limits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t} )\|^{2} \leq\frac{2}{\eta T}\left(\mathbb{E}f(\bm{x}_{0})-\mathbb{E}f(\bm{x}_{ T})\right)+\beta^{2}\rho^{2}+2\beta\sigma^{2}\eta\] \[\leq\frac{2\Delta}{\eta T}+\beta^{2}\rho^{2}+2\beta\sigma^{2}\eta.\] (12)

Substituting \(\eta=\min\left\{\frac{1}{2\beta},\frac{\sqrt{\Delta}}{\sqrt{T\beta\sigma^{2}}}\right\}\) to (12), we get two cases.

**Case A: \(\frac{1}{2\beta}\leq\frac{\sqrt{\Delta}}{\sqrt{T\beta\sigma^{2}}}\).** \(\eta=\frac{1}{2\beta}\), so we have

\[\frac{1}{T}\sum\limits_{t=0}^{T-1}\mathbb{E}\|\nabla f(\bm{x}_{t})\|^{2}\leq \frac{2\Delta}{\eta T}+\beta^{2}\rho^{2}+2\beta\sigma^{2}\eta\]

[MISSING_PAGE_EMPTY:37]

\[\leq\frac{1}{T}\sum\nolimits_{t=0}^{T-1}\mathbb{E}\left[\left(\|\nabla f( \boldsymbol{x}_{t})\|-\beta\rho\right)^{2}\right]\] \[\leq\mathcal{O}\left(\frac{\sqrt{\beta\Delta(\sigma^{2}+L^{2})}}{ \sqrt{T}}\right)+5\beta^{2}\rho^{2}.\]

Taking the square root on both sides,

\[\min_{t\in\{0,\ldots,T\}}\left\{\|\mathbb{E}\|\nabla f(\boldsymbol{x}_{t})\|- \beta\rho\big{|}\right\}\leq\mathcal{O}\left(\frac{\left(\beta\Delta(\sigma^{2} +L^{2})\right)^{1/4}}{T^{1/4}}\right)+\sqrt{5}\beta\rho.\]

Rearranging the inequality, we obtain

\[\min_{t\in\{0,\ldots,T\}}\left\{\mathbb{E}\|\nabla f(\boldsymbol{x}_{t})\| \right\}\leq\mathcal{O}\left(\frac{\left(\beta\Delta(\sigma^{2}+L^{2})\right) ^{1/4}}{T^{1/4}}\right)+\left(1+\sqrt{5}\right)\beta\rho,\]

thereby completing the proof.

## Appendix D Simulation Results of SAM on Example Functions

Figure 6 shows the results of SAM simulations on the example functions considered in Sections 3 and 4. It demonstrates that SAM indeed does not converge in our worst-case example constructions.

Figure 6: The results of the SAM simulations on the example functions. The yellow line indicates the trajectory of SAM iterates. (a) and (d) display deterministic SAM iterates (with initialization \(x_{0}=0.4\)) and the plot of x-coordinate values over epochs, for a smooth nonconvex function as shown in Figure 2(a) under settings in Theorem 3.5. (b) and (e) show \(m\)-SAM iterates (with initialization \(x_{0}=4\)) and the plot of x-coordinate values over epochs, for the smooth strongly convex function as shown in Figure 2(b) under settings in Theorem 4.2. (c) and (f) demonstrate \(m\)-SAM iterates (with initialization \(x_{0}=9\)) and the plot of x-coordinate values over epochs, for the smooth convex function as shown in Figure 2(d) under settings in Theorem 4.4. All plots empirically verify that practical SAM cannot converge all the way to optima. Instead, the iterates get trapped in certain regions.