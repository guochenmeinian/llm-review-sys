# Towards Self-Interpretable

Graph-Level Anomaly Detection

 Yixin Liu\({}^{1}\), Kaize Ding\({}^{2}\), Qinghua Lu\({}^{3}\), Fuyi Li\({}^{4,5}\), Leo Yu Zhang\({}^{6}\), Shirui Pan\({}^{6}\)

\({}^{1}\)Monash University, \({}^{2}\)Northwestern University, \({}^{3}\)Data61, CSIRO,

\({}^{4}\)Northwest A&F University, \({}^{5}\)The University of Adelaide, \({}^{6}\)Griffith University

yixin.liu@monash.edu, Kaize.ding@northwestern.edu, qinghua.lu@data61.csiro.au,

fuyi.li@nwsuaf.edu.cn, leo.zhang@griffith.edu.au, s.pan@griffth.edu.au

Corresponding Author.

###### Abstract

Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, _explainable GLAD_, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.

## 1 Introduction

Graphs are ubiquitous data structures in numerous domains, including chemistry, traffic, and social networks [1, 2, 3]. Among machine learning tasks for graph data, graph-level anomaly detection (GLAD) is a challenge that aims to identify the graphs that exhibit substantial dissimilarity from the majority of graphs in a collection [4]. GLAD presents great potential for various real-world scenarios, such as toxic molecule recognition [5] and pathogenic brain mechanism discovery [6]. Recently, GLAD has drawn increasing research attention, with advanced techniques being applied to this task, e.g., knowledge distillation [4] and one-class classification [7].

Despite their promising performance, existing works [4, 7, 8, 9] mainly aim to answer **how** to predict abnormal graphs by designing various GLAD architectures; however, they fail to provide explanations for the prediction, i.e., illustrating **why** these graphs are recognized as anomalies. In real-world applications, it is of great significance to make anomaly detection models explainable [10]. From the perspective of models, valid explainability makes GLAD models trustworthy to meet safety and security requirements [11]. For example, an explainable fraud detection model can pinpoint specific fraudulent behaviors when identifying defrauders, which enhances the reliability of predictions. From the perspective of data, an anomaly detection model with explainability canal equations of the dataset, which further supports human experts in data understanding [12]. For instance, an explainable GLAD model for molecules can summarize the functional groups that cause abnormality, enabling researchers to deeply investigate the properties of compounds. Hence, the broad applications of interpreting anomaly detection results motivate us to investigate the problem of **Explainable GLAD** where the GLAD model is expected to measure the abnormality of each graph sample as well as provide meaningful explanations of the predictions during the inference time. As an example shown in Fig. 1, the GLAD model also extracts a graph rationale [13, 14] corresponding to the predicted anomaly score. Although there are a few studies [10, 15, 16] proposed to explain anomaly detection results for visual or tabular data, explainable GLAD remains underexplored and it is non-trivial to apply those methods to our problem due to the discrete nature of irregular graph-structured data [17].

Towards the goal of designing an explainable GLAD model, two essential challenges need to be solved with careful design: _Challenge 1 -- how to make the GLAD model self-interpretable2?_ Even though we can leverage existing post-hoc explainers [19, 20] for GNNs to explain the predictions of the GLAD model, such post-hoc explainers are not synergistically learned with the detection models, resulting in the risk of wrong, biased, and sub-optimal explanations [21, 22]. Hence, developing a self-interpretable GLAD model which detects graph-level anomalies with explanations inherently is more desirable and requires urgent research efforts. _Challenge 2 -- how to learn meaningful graph explanations without using supervision signals?_ For the problem of GLAD, ground-truth anomalies are usually unavailable during training, raising significant challenges to both detecting anomalies and providing meaningful explanations. Since most of the existing self-interpretable GNNs [13, 21, 22] merely focus on the (semi-)supervised setting, in particular the node/graph classification tasks, how to design a self-interpretable model for the explainable GLAD problem where ground-truth labels are inaccessible remains a challenging task.

Footnote 2: In this paper, we distinguish the terms “explainability” and “interpretability” following a recent survey paper [18]: “explainable artificial intelligence” is a widespread and high-level concept, hence we define the research problem as “explainable GLAD”; for the model that can provide interpretations of the predictions of itself, we consider it as “interpretable” or “self-interpretable”. Detailed definitions are provided in Appendix A.

To solve the above challenges, in this paper, we develop a novel Self-Interpretable Graph aNomaly dETection model (SIGNET for short). Based on the information bottleneck (IB) principle, we first propose a multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD model. Under the MSIB framework, the instantiated GLAD model is able to predict the abnormality of each graph as well as generate corresponding explanations without relying on ground-truth anomalies simultaneously. To learn the self-interpretable GLAD model without ground-truth anomalies, we introduce the dual hypergraph as a supplemental view of the original graph and employ a unified bottleneck subgraph extractor to extract corresponding graph rationales. By further conducting multi-view learning among the extracted graph rationales, SIGNET is able to learn the feature patterns from both node and edge perspectives in a purely self-supervised manner. During the test phase, we can directly measure the abnormality of each graph sample based on its inter-view agreement (i.e., cross-view mutual information) and derive the corresponding graph rationales for the purpose of explaining the prediction. To sum up, our contribution is three-fold:

* **Problem.** We propose to investigate the explainable GLAD problem that has broad application prospects. To the best of our knowledge, this is the _first_ attempt to study the explainability problem for graph-level anomaly detection.
* **Algorithm.** We propose a novel self-interpretable GLAD model termed SIGNET, which infers graph-level anomaly scores and subgraph-level explanations simultaneously with the multi-view subgraph information bottleneck framework.

Figure 1: A toy example to illustrate (a) GLAD problem and (b) explainable GLAD problem.

* **Evaluation.** We perform extensive experiments to corroborate the anomaly detection performance and self-interpretation ability of SIGNET via thorough comparisons with state-of-the-art methods on 16 benchmark datasets.

## 2 Preliminaries and Related Work

In this section, we introduce the preliminaries and briefly review the related works. A more comprehensive literature review can be found in Appendix B.

**Notations.** Let \(G=(\mathcal{V},\mathcal{E},\mathbf{X})\) be a simple graph with \(n\) nodes and \(m\) edges, where \(\mathcal{V}\) is the set of nodes and \(\mathcal{E}\) is the set of edges. The node features are included by feature matrix \(\mathbf{X}\in\mathbb{R}^{n\times d_{f}}\), and the connectivity among the nodes is represented by adjacency matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\). Unlike simple graphs where each edge only connects two nodes, "hypergraph" is a generalization of a traditional graph structure in which hyperedges connect more than two nodes. We define a hypergraph with \(n^{*}\) nodes and \(m^{*}\) hyperedges as \(G^{*}=(\mathcal{V}^{*},\mathcal{E}^{*},\mathbf{X}^{*})\), where \(\mathcal{V}^{*}\), \(\mathcal{E}^{*}\), and \(\mathbf{X}^{*}\in\mathbb{R}^{n^{*}\times d_{f}^{*}}\) are the node set, hyperedge set, and node feature matrix respectively. To indicate the higher-order relations among arbitrary numbers of nodes within a hypergraph, we use an incidence matrix \(\mathbf{M}^{*}\in\mathbb{R}^{n^{*}\times m^{*}}\) to represent the interaction between \(n^{*}\) nodes and \(m^{*}\) hyperedges. Alternatively, a simple graph and a hypergraph and be represented by \(G=(\mathbf{A},\mathbf{X})\) and \(G^{*}=(\mathbf{M}^{*},\mathbf{X}^{*})\), respectively. We denote the Shannon mutual information (MI) of two random variables \(A\) and \(B\) as \(I(A;B)\).

**Graph Neural Networks (GNNs).** GNNs are the extension of deep neural networks onto graph data, which have been applied to various graph learning tasks [1; 2; 23; 24; 25; 26; 27; 28]. Mainstream GNNs usually follow the paradigm of message passing [2; 23; 24; 26]. Some studies termed hypergraph neural networks (HGNNs) also apply GNNs to hypergraphs [29; 30; 31]. The formulations of GNN and HGNN are in Appendix C. To make the predictions understandable, some efforts try to uncover the explanation for GNNs [18; 32]. A branch of methods, termed post-hoc GNN explainers, use specialized models to explain the behavior of a trained GNN [19; 20; 33]. Meanwhile, some self-interpretable GNNs can intrinsically provide explanations for predictions using interpretable designs in GNN architectures [13; 21; 22]. While these methods mainly aim at supervised classification scenarios, how to interpret unsupervised anomaly detection models still remains open.

**Information Bottleneck (IB).** IB is an information theory-based approach for representation learning that trains the encoder by preserving the information that is relevant to label prediction while minimizing the amount of superfluous information [34; 35; 36]. Formally, given the data \(X\) and the label \(Y\), IB principle aims to find the representation \(Z\) by maximizing the following objective: \(\max_{Z}I(Z;Y)-\beta I(X;Z)\), where \(\beta\) is a hyper-parameter to trade off informativeness and compression. To extend IB onto unsupervised learning scenarios, Multi-view Information Bottleneck (MIB) [37] provides an optimizable target for unsupervised multi-view learning, which alleviates the reliance on label \(Y\). Given two different and distinguishable views \(V_{1}\) and \(V_{2}\) of the same data \(X\), the objective of MIB is to learn sufficient and compact representations \(Z_{1}\) and \(Z_{2}\) for two views respectively. Taking view \(V_{1}\) as an example, by factorizing the MI between \(V_{1}\) and \(Z_{1}\), we can identify two components: \(I(V_{1};Z_{1})=I(V_{1};Z_{1}|V_{2})+I(V_{2};Z_{1})\), where the first term is the superfluous information that is expected to be minimized, and the second term is the predictive information that should be maximized. Then, \(Z_{1}\) can be learned using a relaxed Lagrangian objective:

\[\max_{Z_{1}}I(V_{2};Z_{1})-\beta_{1}I(V_{1};Z_{1}|V_{2}),\] (1)

where \(\beta_{1}\) is a trade-off parameter. By optimizing Eq. (1) and its counterpart in view \(V_{2}\), we can learn informative and compact \(Z_{1}\) and \(Z_{2}\) by extracting the information from each other.

IB principle is also proven to be effective in graph learning tasks, such as graph contrastive learning [38; 39], subgraph recognition [17; 40], graph-based recommendation [41], and robust graph representation learning [22; 42; 43]. Nevertheless, how to leverage the idea of IB on graph anomaly detection tasks is still an open problem.

**Graph-level Anomaly Detection (GLAD).** GLAD aims to recognize anomalous graphs from a set of graphs by learning an anomaly score for each graph sample to indicate its degree of abnormality [4; 7; 8; 9]. Recent studies try to address the GLAD problem with various advanced techniques, such as knowledge distillation [4], one-class classification [7], transformation learning [8],and deep graph kernel [44]. However, these methods can only learn the anomaly score but fail to provide explanations, i.e., the graph rationale causing the abnormality, for their predictions.

**Problem Formulation.** Based on this mainstream unsupervised GLAD paradigm [4; 7; 8], in this paper, we present a novel research problem termed _explainable GLAD_, where the GLAD model is expected to provide the anomaly score as well as the explanations of such a prediction for each testing graph sample. Formally, the proposed research problem can be formulated by:

**Definition 2.1** (Explainable graph-level anomaly detection).: Given the training set \(\mathcal{G}_{tr}\) that contains a number of normal graphs, we aim at learning an explainable GLAD model \(f:\mathbb{G}\rightarrow(\mathbb{R},\mathbb{G})\) that is able to predict the abnormality of a graph and provide corresponding explanations. In specific, given a graph \(G_{i}\) from the test set \(\mathcal{G}_{te}\) with normal and abnormal graphs, the model can generate an output pair \(f(G_{i})=(s_{i},G_{i}^{(es)})\), where \(s_{i}\) is the anomaly score that indicates the abnormality degree of \(G_{i}\), and \(G_{i}^{(es)}\) is the subgraph of \(G_{i}\) that explains why \(G_{i}\) is identified as a normal/abnormal sample.

## 3 Methodology

This section details the proposed model SIGNET for explainable GLAD. Firstly, we derive a multi-view subgraph information bottleneck (MSIB) framework (Sec. 3.1) that allows us to identify anomalies with causal interpretations provided. Then, we provide the instantiation of the components in MSIB framework, including view construction (Sec. 3.2), bottle subgraph extraction (Sec. 3.3), and cross-view mutual information (MI) maximization (Sec. 3.4), which compose the SIGNET model. Finally, we introduce the self-interpretable GLAD inference (Sec. 3.5) of SIGNET. The overall learning pipeline of SIGNET is demonstrated in Fig. 2(a).

### MSIB Framework Overview

To achieve the goal of self-interpretable GLAD, an unsupervised learning approach that can jointly predict the abnormality of graphs and yield corresponding explanations is required. Inspired by the concept of information bottleneck (IB) and graph multi-view learning [36; 37; 45], we propose multi-view subgraph information bottleneck (MSIB), a self-interpretable and self-supervised learning framework for GLAD. The learning objective of MSIB is to optimize the "bottleneck subgraphs", the vital substructure on two distinct views of a graph, by maximizing the predictive structural information shared by both graph views while minimizing the superfluous information that is irrelevant to the cross-view agreement. Such an objective can be optimized in a self-supervised manner, without the guidance of ground-truth labels. Due to the observation that latent anomaly patterns of graphs can be effectively captured by multi-view learning [8; 9], we can directly use the cross-view agreement, i.e., the MI between two views, to evaluate the abnormality of a graph sample. Simultaneously, the extracted bottleneck subgraphs provide us with graph rationales to explain the anomaly detection predictions, since they contain the most compact substructure sourced from the original data and the most discriminative knowledge for the predicted abnormality, i.e., the estimated cross-view MI.

Formally, in the proposed MSIB framework, we assume each graph sample \(G\) has two different and distinguishable views \(G^{1}\) and \(G^{2}\). Then, taking view \(G^{1}\) as an example, the target of MSIB is to learn a bottleneck subgraph \(G^{1(s)}\) for \(G^{1}\) by optimizing the following objective:

\[\max_{G^{1(s)}}I(G^{2};G^{1(s)})-\beta_{1}I(G^{1};G^{1(s)}|G^{2}).\] (2)

Similar to MIB (Eq. (1), the optimization for \(G^{2(s)}\), the bottleneck subgraph of \(G^{2}\), can be written in the same format. Then, by parameterizing bottleneck subgraph extraction and unifying the domain of bottleneck subgraphs, the objective can be transferred to minimize a tractable loss function:

\[\mathcal{L}_{MSIB}=-I(G^{1(s)};G^{2(s)})+\beta D_{SKL}\left(p_{\theta}(G^{1(s)} |G^{1})\|p_{\psi}(G^{2(s)}|G^{2})\right),\] (3)

where \(p_{\theta}(G^{1(s)}|G^{1})\) and \(p_{\psi}(G^{2(s)}|G^{2})\) are the bottleneck subgraph extractors (parameterized by \(\theta\) and \(\psi\)) for \(G^{1}\) and \(G^{2}\) respectively, \(D_{SKL}(\cdot)\) is the symmetrized Kullback-Leibler (SKL) divergence, and \(\beta\) is a trade-off hyper-parameter. Detailed deductions from Eq. (2) to Eq. (3) are in Appendix D.

MSIB framework can guide us to build a self-interpretable GLAD model. The first term in Eq. (3) tries to maximize the MI between the bottleneck subgraphs from two views, which not only prompts the model to capture vital subgraphs but also helps capture the cross-view matching patterns for anomaly detection. The second term in Eq. (3) is a regularization term to align the extractors, which ensures the compactness of bottleneck subgraphs. During inference, \(-I(G^{1(s)};G^{2(s)})\) can be regarded as a measure of abnormality. Meanwhile, the bottleneck subgraphs extracted by \(p_{\theta}\) and \(p_{\psi}\) can serve as explanations. In the following subsections, we take SIGNET as a practical implementation of MSIB framework. We illustrate the instantiations of view construction (\(G^{1}\) and \(G^{2}\)), subgraph extractors (\(p_{\theta}\) and \(p_{\psi}\)), MI estimation (\(I(G^{1(s)};G^{2(s)})\)), and explainable GLAD inference, respectively.

### Dual Hypergraph-based View Construction

To implement MSIB framework for GLAD, the first step is to construct two different and distinguishable views \(G^{1}\) and \(G^{2}\) for each sample \(G\). In multi-view learning approaches [37; 46; 47; 48], a general strategy is using stochastic perturbation-based data augmentation (e.g., edge modification [46] and feature perturbation [47]) to create multiple views. Despite their success in graph representation learning [46; 47; 48], we claim that perturbation-based view constructions are not appropriate in SIGNET for the following reasons. 1) Low sensitivity to anomalies. Due to the similarity of normal and anomalous graphs in real-world data, perturbations may create anomaly-like data from normal data as the augmented view [49]. In this case, maximizing the cross-view MI would result in reduced sensitivity of the model towards distinguishing between normal and abnormal data, hindering the performance of anomaly detection [50]. 2) Less differentiation. In the principle of MIB, two views should be distinguishable and mutually redundant [37]. However, the views created by graph perturbation from the same sample can be similar to each other, which violates the assumption of our basic framework. 3) Harmful instability. The MI \(I(G^{1(s)};G^{2(s)})\) for abnormality measurement is highly related to the contents of two views. Nevertheless, the view contents generated by stochastic perturbation can be quite unstable due to the randomness, leading to inaccurate estimation of abnormality.

Considering the above limitations, a perturbation-free, distinct, and stable strategy is required for view construction. To this end, we utilize _dual hypergraph transformation_ (DHT) [51] to construct the opposite view of the original graph. Concretely, for a graph sample \(G\), we define the first view as itself (i.e., \(G^{1}=G\)) and the second view as its dual hypergraph \(G^{*}\) (i.e., \(G^{2}=G^{*}\)). Based on the hypergraph duality [52; 53], dual hypergraph can be acquired from the original simple graph with DHT: each edge of the original graph is transformed into a node of the dual hypergraph, and each node of the original graph is transformed into a hyperedge of the dual hypergraph [51]. As the example shown in Fig. 2(b), the structural roles of nodes and edges are interchanged by DHT, and the incidence matrix \(\mathbf{M}^{*}\) of the dual hypergraph is the transpose of the incidence matrix of the original graph. To initialize the node features \(\mathbf{X}^{*}\in\mathbb{R}^{m\times d_{f}^{*}}\) of \(G^{*}\), we can either use the original edge features (if available), or construct edge-level features from the original node features or according to edge-level geometric property.

The DHT-based view construction brings several advantages. Firstly, the dual hypergraph has significantly distinct contents from the original view, which caters to the needs for differentiation in

Figure 2: (a) The overall pipeline of the proposed model SIGNET, consisting of (i) view construction, (ii) bottleneck subgraph extraction, and (iii) cross-view MI maximization. (b) An illustration of dual hypergraph transformation (DHT), where the nodes () and edges (—) in the original graph correspond to hyperedges (\(\cdots\)) and nodes () in its dual hypergraph, respectively.

MIB. Secondly, the dual hypergraph pays more attention to the edge-level information, encouraging the model to capture not only node-level but also edge-level anomaly patterns. Thirdly, DHT is a bijective mapping between two views, avoiding confusion between normal and abnormal samples. Fourthly, DHT is randomness-free, ensuring the stable estimation of MI.

### Bottleneck Subgraph Extraction

In MSIB framework, bottleneck subgraph extraction is a key component that learns to refine the core rationale for abnormality explanations. Following the procedure of MSIB, we need to establish two bottleneck subgraph extractors for the original view \(G\) and dual hypergraph view \(G^{*}\) respectively. To model the discrete subgraph extraction process in a differentiable manner with neural networks, following previous methods [14; 17; 22], we introduce continuous relaxation into the subgraph extractors. Specifically, for the original view \(G\), we model the subgraph extractor with \(p_{\theta}(G^{(s)}|G)=\prod_{v\in\mathcal{V}}p_{\theta}(v\in\mathcal{V}^{(s)}|G)\). In practice, the GNN-based extractor takes \(G\) as input and outputs a node probability vector \(\mathbf{p}\in\mathbb{R}^{n\times 1}\), where each entry indicates the probability that the corresponding node belongs to \(G^{(s)}\). Similarly, for the dual hypergraph view \(G^{*}\), an edge-centric HGNN serves as the subgraph extractor \(p_{\psi}\). It takes \(G^{*}\) as input and outputs an edge probability vector \(\mathbf{p}^{*}\in\mathbb{R}^{m\times 1}\) that indicates if the dual nodes (corresponding to the edges in the original graphs) belong to \(G^{*(s)}\). Once the probability vectors are calculated, the subgraph extraction can be executed by:

\[G^{(s)}=(\mathbf{A},\mathbf{X}^{(s)})=(\mathbf{A},\mathbf{X}\odot\mathbf{p}), \quad G^{*(s)}=(\mathbf{M}^{*},\mathbf{X}^{*(s)})=(\mathbf{M}^{*},\mathbf{X}^ {*}\odot\mathbf{p}^{*}),\] (4)

where \(\odot\) is the row-wise production. Then, to implement the second term in Eq. (3), we can lift the node probabilities \(\mathbf{p}\) to edge probabilities by \(\mathbf{p}^{\prime}\) by \(\mathbf{p}^{\prime}_{i(e_{ij})}=\mathbf{p}_{i}\mathbf{p}_{j}\), where \(\mathbb{I}(e_{ij})\) is the index of edge connecting node \(v_{i}\) and \(v_{j}\). After re-probabilizing \(\mathbf{p}^{\prime}\), the SKL divergence between \(\mathbf{p}^{\prime}\) and \(\mathbf{p}^{*}\) can be computed as the regularization term in MSIB framework.

Although the above "two-extractor" design correlates to the theoretical framework of MSIB, in practice, it is non-trivial to ensure the consistency of two generated subgraphs only with an SKL divergence loss. The main reason is that the input and architectures of two extractors are quite different, leading to the difficulty in output alignment. However, the consistency of two bottleneck subgraphs not only guarantees the informativeness of cross-view MI for abnormality measurement, but also affects the quality of explanations. Considering the significance of preserving consistency, we use a single extractor to generate bottleneck subgraphs for two views. In specific, the bottleneck subgraph extractor first takes the original graph \(G\) as its input and outputs the node probability vector \(\mathbf{p}\) for the bottleneck subgraph extraction of \(G^{(s)}\). Then, leveraging the node-edge correspondence in DHT, we can directly lift the node probabilities to edge probability vector \(\mathbf{p}^{*}\) via \(\mathbf{p}^{*}_{\mathbf{I}(e_{ij})}=\mathbf{p}_{i}\mathbf{p}_{j}\) and re-probabilization operation. \(\mathbf{p}^{*}\) can be used to extract subgraph for the dual hypergraph view. In this way, the generated bottleneck subgraphs in two views can be highly correlated, enhancing the quality of GLAD prediction (MI) and its explanations. Meanwhile, such a "single-extractor" design further simplifies the model architecture by removing extra extractor and loss function (i.e., the \(D_{SKL}\) term in Eq. (3)), reducing the model complexity. Empirical comparison in Sec. 4.4 also validates the effectiveness of this design.

### Cross-view MI Maximization

After bottleneck subgraph extraction, the next step is to maximize the MI \(I(G^{(s)};G^{*(s)})\) between the bottleneck subgraphs from two views. The estimated MI, in the testing phase, can be used to evaluate the graph-level abnormality. Owing to the discrete and complex nature of graph-structured data, it is difficult to directly estimate the MI between two subgraphs. Alternatively, a feasible solution is to obtain compact representations for two subgraphs, and then, calculate the representation-level MI as a substitute. In SIGNET, we use message passing-based GNN and HGNN with pooling layer (formulated in Appendix C) to learn the subgraph representations \(\mathbf{h}_{G^{(s)}}\) and \(\mathbf{h}_{G^{*(s)}}\) for \(G^{(s)}\) and \(G^{*(s)}\), respectively. In this case, \(I(G^{(s)};G^{*(s)})\) can be transferred into a tractable term, i.e., the MI between subgraph representations \(I(\mathbf{h}_{G^{(s)}};\mathbf{h}_{G^{*(s)}})\).

After that, the MI term \(I(\mathbf{h}_{G^{(s)}};\mathbf{h}_{G^{*(s)}})\) can be maximized by using sample-based differentiable MI lower bounds [37], such as Jensen-Shannon (JS) estimator [54], Donsker-Varadhan (DV) estimator [55], and Info-NCE estimator [56]. Due to its strong robustness and generalization ability [37; 57], we employ Info-NCE for MI estimation in SIGNET. Specifically, given a batch of graph samples \(\mathcal{B}=\{G_{1},\cdots,G_{B}\}\), the training loss of SIGNET can be written by:

\[\begin{split}\mathcal{L}=-\frac{1}{2|\mathcal{B}|}\sum_{G_{i} \in\mathcal{B}}I(\mathbf{h}_{G_{i}^{(s)}};\mathbf{h}_{G_{i}^{(s)}})& =-\frac{1}{2|\mathcal{B}|}\sum_{G_{i}\in\mathcal{B}}\left(\ell( \mathbf{h}_{G_{i}^{(s)}},\mathbf{h}_{G_{i}^{(s)}})+\ell(\mathbf{h}_{G_{i}^{(s )}},\mathbf{h}_{G_{i}^{(s)}})\right),\\ \ell(\mathbf{h}_{G_{i}^{(s)}},\mathbf{h}_{G_{i}^{(s)}})& =\log\frac{exp\Big{(}f_{k}(\mathbf{h}_{G_{i}^{(s)}},\mathbf{h}_{G_{i}^{(s)}})/ \tau\Big{)}}{\sum_{G_{j}\in\mathcal{B}\setminus G_{i}}exp\Big{(}f_{k}( \mathbf{h}_{G_{i}^{(s)}},\mathbf{h}_{G_{j}^{(s)}})/\tau\Big{)}},\end{split}\] (5)

where \(f_{k}(\cdot,\cdot)\) is the cosine similarity function, \(\tau\) is the temperature hyper-parameter, and \(\ell(\mathbf{h}_{G_{i}^{(s)}},\mathbf{h}_{G_{i}^{(s)}})\) is calculated following \(\ell(\mathbf{h}_{G_{i}^{(s)}},\mathbf{h}_{G_{i}^{(s)}})\).

### Self-Interpretable GLAD Inference

In this subsection, we introduce the self-interpretable GLAD inference protocol with SIGNET (marked in red in Fig. 2(a)) that is composed of two parts: anomaly scoring and explanation.

**Anomaly scoring.** By minimizing Eq. (5) on training data, the cross-view matching patterns of normal samples are well captured, leading to a higher MI for normal data; on the contrary, the anomalies with anomalous attributal and structural characteristics tend to violate the matching patterns, resulting in their lower cross-view MI in our model. Leveraging this property, during inference, the negative of MI can indicate the abnormality of testing data. For a testing sample \(G_{i}\), its anomaly score \(s_{i}\) can be calculated by \(s_{i}=-I(\mathbf{h}_{G_{i}^{(s)}};\mathbf{h}_{G_{i}^{(s)}})\), where the MI is estimated by Info-NCE.

**Explanation.** In SIGNET, the bottleneck subgraph extractor is able to pinpoint the key substructure of the input graph under the guidance of MSIB framework. The learned bottleneck subgraphs are the most discriminative components of graph samples and are highly related to the anomaly scores. Therefore, we can directly regard the bottleneck subgraphs as the explanations of anomaly detection results. In specific, the node probabilities \(\mathbf{p}\) and edge probabilities \(\mathbf{p}^{*}\) can indicate the significance of nodes and edges, respectively. In practical inference, we can pick the nodes/edges with top-k probabilities or use a threshold-based strategy to acquire a fixed-size explanation subgraph \(G^{(es)}\).

More discussion about methodology, including the pseudo-code algorithm of SIGNET, the comparison between SIGNET and existing method, and the complexity analysis of SIGNET, is illustrated in Appendix E.

## 4 Experiments

In this section, extensive experiments are conducted to answer three research questions:

* **RQ1:** Can SIGNET provide informative explanations for the detection results?
* **RQ2:** How effective is SIGNET on identifying anomalous graph samples?
* **RQ3:** What are the contributions of the core designs in SIGNET model?

### Experimental Setup

**Datasets.** For the explainable GLAD task, we introduce 6 datasets with ground-truth explanations, including three synthetic datasets and three real-world datasets. Details are demonstrated below. We also verify the anomaly detection performance of SIGNET on 10 TU datasets [58], following the setting in [4]. Detailed statistics and visualization of datasets are demonstrated in Appendix F.1.

* **BM-MT, BM-MN, and BM-MS** are three synthetic dataset created by following [13; 19]. Each graph is composed of one base (Tree, Ladder, or Wheel) and one or more motifs that decide the abnormality of the graph. For BM-MT (motif type), each normal graph has a house motif and each anomaly has a 5-cycle motif. For BM-MN (motif number), each normal graph has 1 or 2 house motifs and each anomaly has 3 or 4 house motifs. For BM-MS (motif size), each normal graph has a cycle motif with 3-5 nodes and each anomaly has a cycle motif with 6-9 nodes. The ground-truth explanations are defined as the nodes/edges within motifs.

* **MNIST-0 and MNIST-1** are two GLAD datasets derived from MNIST-75sp superpixel dataset [59]. Following [60], we consider a specific class (i.e., digit 0 or 1) as the normal class, and regard the samples belonging to other classes as anomalies. The ground-truth explanations are the nodes/edges with nonzero pixel values.
* **MUTAG** is a molecular property prediction dataset [61]. We set nonmutagenic molecules as normal samples and mutagenic molecules as anomalies. Following [20], -NO2 and -NH2 in mutagenic molecules are viewed as ground-truth explanations.

**Baselines**. Considering their competitive performance, we consider three state-of-the-art deep GLAD methods, i.e., OCGIN [7], GLocalKD [4], and OCGTL [8], as baselines. To provide explanations for them, we integrate two mainstream post-hoc GNN explainers, i.e., GNNExplainer [19] (GE for short) and PGExplainer [20] (PG for short) into the deep GLAD methods. For GLAD tasks, we further introduce the baselines composed of a graph kernel (i.e., Weisfeiler-Lehman kernel (WL) [62] or Propagation kernel (PK) [63]) and a detector (i.e., iForest (iF) [64] or one-class SVM (OCSVM) [65]).

**Metrics and Implementation.** For interpretation evaluation, we report explanation ROC-AUCs at node level (NX-AUC) and edge level (EX-AUC) respectively, similar to [19; 20]. For GLAD performance, we report the ROC-AUC w.r.t. anomaly scores and labels (AD-AUC) [7]. We repeat 5 times for all experiments and record the average performance. In SIGNET, we use GIN [2] and Hyper-Conv [30] as the GNN and HGNN encoders. The bottleneck subgraph extractor is selected from GIN [2] and MLP. We perform grid search to pick the key hyper-parameters in SIGNET and baselines. More details of implementation and infrastructures are in Appendix F. Our code is available at https://github.com/yixinliu233/SIGNET.

### Explainability Results (RQ1)

**Quantitative evaluation.** In Table 1, we report the node-level and edge-level explanation AUC [22] on 6 datasets. Note that PGExplainer [20] can only provide edge-level explanations natively. We have the following observations: 1) _SIGNET achieves SOTA performance in almost all scenarios._ Compared to the best baselines, the average performance gains of SIGNET are \(27.89\%\) in NX-AUC and \(8.99\%\) in EX-AUC. The superior performance verifies the significance of learning to interpret and detect with a unified model. 2) _The post-hoc explainers are not compatible with all GLAD models_. For instance, PGExplainer works relevantly well with GLocalKD but cannot provide informative explanations for OCGIN. The GNNExplainer, unfortunately, exhibits poor performance in most scenarios. 3) _SIGNET has larger performance gains on real-world datasets_, which illustrates the potential of SIGNET in explaining real-world GLAD tasks. 4) Despite its superior performance, _the stability of SIGNET is relevantly average_. Especially on the synthetic datasets, we can find that the standard deviations of

\begin{table}
\begin{tabular}{l l|c c c|c c|c} \hline \hline
**Dataset** & **Metric** & **OCGIN-FG** & **GLocalKD-GE** & **OCGIL-EG** & **OCGIN-PG** & **GLocalKD-PG** & **OCGTL-PG** & **SIGNET** \\ \hline \multirow{2}{*}{BM-MT} & _NX-AUC_ & 48.26\(\pm\)1.38 & 49.67\(\pm\)0.58 & 45.75\(\pm\)2.53 & - & - & 78.41\(\pm\)6.88 \\  & _EX-AUC_ & 520.43\(\pm\)3.28 & 49.11\(\pm\)2.77 & 49.80\(\pm\)2.88 & 64.08\(\pm\)12.23 & 74.59\(\pm\)57.66 & 72.72\(\pm\)10.19 & 77.69\(\pm\)13.14 \\ \hline \multirow{2}{*}{BM-MN} & _NX-AUC_ & 46.25\(\pm\)4.60 & 49.10\(\pm\)0.71 & 40.53\(\pm\)3.18 & - & - & 76.57\(\pm\)6.62 \\  & _EX-AUC_ & 60.20\(\pm\)2.90 & 50.71\(\pm\)3.14 & 56.34\(\pm\)3.10 & 50.18\(\pm\)7.88 & 76.85\(\pm\)3.53 & 74.36\(\pm\)12.78 & 83.45\(\pm\)3.93 \\ \hline \multirow{2}{*}{BM-MS} & _NX-AUC_ & 52.43\(\pm\)1.79 & 50.43\(\pm\)0.62 & 54.31\(\pm\)1.55 & - & - & - & 76.45\(\pm\)4.81 \\  & _EX-AUC_ & 54.31\(\pm\)9.61 & 49.10\(\pm\)2.99 & 68.87\(\pm\)1.44 & 43.67\(\pm\)12.60 & 82.53\(\pm\)18.56 & 77.45\(\pm\)30.70 & 70.45\(\pm\)5.07 \\ \hline \multirow{2}{*}{MNIST-0} & _NX-AUC_ & 49.48\(\pm\)0.58 & 50.11\(\pm\)0.50 & 38.87\(\pm\)3.21 & - & - & 70.83\(\pm\)5.64 \\  & _EX-AUC_ & 50.55\(\pm\)4.77 & 49.55\(\pm\)0.45 & 41.42\(\pm\)2.49 & 39.55\(\pm\)15.31 & 54.69\(\pm\)17.99 & 59.25\(\pm\)4.60 & 72.78\(\pm\)7.25 \\ \hline \multirow{2}{*}{MNIST-1} & _NX-AUC_ & 48.21\(\pm\)2.01 & 49.50\(\pm\)0.50 & 47.04\(\pm\)1.66 & - & - & - & 68.44\(\pm\)4.37 \\  & _EX-AUC_ & 48.60\(\pm\)3.28 & 49.78\(\pm\)0.26 & 45.24\(\pm\)1.11 & 47.98\(\pm\)2.44 & 49.24\(\pm\)1.56 & 57.93\(\pm\)5.51 & 74.83\(\pm\)5.24 \\ \hline \multirow{2}{*}{Muttag} & _NX-AUC_ & 48.99\(\pm\)1.50 & 49.70\(\pm\)1.19 & 49.31\(\pm\)0.44 & - & - & - & 75.60\(\pm\)5.94 \\  & _EX-AUC_ & 51.29\(\pm\)0.65 & 47.65\(\pm\)1.19 & 45.80\(\pm\)2.81 & 46.22\(\pm\)7.90 & 70.47\(\pm\)15.26 & 65.03\(\pm\)16.90 & 78.05\(\pm\)9.19 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Explanation performance in terms of _NX-AUC_ and _EX-AUC_ (in percent, mean \(\pm\) std). The best and runner-up results are highlighted with bold and underline, respectively.

Figure 3: Visualization of explanation results w.r.t. node and edge probabilities.

[MISSING_PAGE_FAIL:9]

the original SIGNET with one extractor. Meanwhile, we can witness that compared to JS [54] and DV [55] MI estimators, Info-NCE estimator can lead to superior performance, especially for anomaly detection.

## 5 Conclusion

This paper presents a novel and practical research problem, explainable graph-level anomaly detection (GLAD). Based on the information bottleneck principle, we deduce the framework multi-view subgraph information bottleneck (MSIB) to address the explainable GLAD problem. We develop a new method termed SIGNET by instantiating MSIB framework with advanced neural modules. Extensive experiments verify the effectiveness of SIGNET in identifying anomalies and providing explanations. A limitation of our paper is that we mainly focus on purely unsupervised GLAD scenarios where ground-truth labels are entirely unavailable. As a result, for few-shot or semi-supervised GLAD scenarios [44] where a few labels are accessible, SIGNET cannot directly leverage them for model training and self-interpretation. We leave the exploration of supervised/semi-supervised self-interpretable GLAD problems in future works.

## Acknowledgment

S. Pan was partially supported by an Australian Research Council (ARC) Future Fellowship (FT210100097). F. Li was supported by the National Natural Scientific Foundation of China (No. 62202388) and the National Key Research and Development Program of China (No. 2022YFF1000100).

## References

* [1] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_, 32(1):4-24, 2020.
* [2] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* [3] Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-Chung Liew, and Shirui Pan. Towards data-centric graph machine learning: Review and outlook. _arXiv preprint arXiv:2309.10979_, 2023.
* [4] Rongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level anomaly detection by glocal knowledge distillation. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, pages 704-714, 2022.
* [5] Charu C Aggarwal and Haixun Wang. Graph data management and mining: A survey of algorithms and applications. _Managing and mining graph data_, pages 13-68, 2010.
* [6] Tommaso Lanciano, Francesco Bonchi, and Aristides Gionis. Explainable classification of brain networks via contrast subgraphs. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3308-3318, 2020.
* [7] Lingxiao Zhao and Leman Akoglu. On using classification datasets to evaluate graph outlier detection: Peculiar observations and new insights. _Big Data_, 2021.
* [8] Chen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. Raising the bar in graph-level anomaly detection. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, pages 2196-2203, 7 2022.
* [9] Xuexiong Luo, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Chuan Zhou, Hongyang Chen, Zhao Li, and Quan Z Sheng. Deep graph level anomaly detection with contrastive learning. _Scientific Reports_, 12(1):19867, 2022.
* [10] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus Robert Muller. Explainable deep one-class classification. In _International Conference on Learning Representations_, 2021.
* [11] Suseela T Sarasamma, Qiuming A Zhu, and Julie Huff. Hierarchical kohonenen net for anomaly detection in network security. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 35(2):302-312, 2005.

* [12] Gregoire Montavon, Wojciech Samek, and Klaus-Robert Muller. Methods for interpreting and understanding deep neural networks. _Digital signal processing_, 73:1-15, 2018.
* [13] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In _International Conference on Learning Representations_, 2022.
* [14] Sihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In _International Conference on Machine Learning_, pages 13052-13065. PMLR, 2022.
* [15] Hongzuo Xu, Yijie Wang, Songlei Jian, Zhenyu Huang, Yongjun Wang, Ning Liu, and Fei Li. Beyond outlier detection: Outlier interpretation by attention-guided triplet deviation network. In _Proceedings of the Web Conference 2021_, pages 1328-1339, 2021.
* [16] Wonwoo Cho, Jeonghoon Park, and Jaegul Choo. Training auxiliary prototypical classifiers for explainable anomaly detection in medical image segmentation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 2624-2633, 2023.
* [17] Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. Graph information bottleneck for subgraph recognition. In _International Conference on Learning Representations_, 2021.
* [18] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [19] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnonexplainer: Generating explanations for graph neural networks. _Advances in neural information processing systems_, 32, 2019.
* [20] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. _Advances in neural information processing systems_, 33:19620-19631, 2020.
* [21] Enyan Dai and Suhang Wang. Towards self-explainable graph neural network. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 302-311, 2021.
* [22] Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In _International Conference on Machine Learning_, pages 15524-15543. PMLR, 2022.
* [23] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017.
* [24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Advances in neural information processing systems_, volume 30, 2017.
* [25] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed networks. In _Proceedings of the 2019 SIAM International Conference on Data Mining_, pages 594-602. SIAM, 2019.
* [26] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018.
* [27] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. _ACM SIGKDD Explorations Newsletter_, 24(2):61-77, 2022.
* [28] Yixin Liu, Kaize Ding, Jianling Wang, Vincent Lee, Huan Liu, and Shirui Pan. Learning strong graph neural networks with weak information. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)_, 2023.
* [29] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 3558-3565, 2019.
* [30] Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraph convolution and hypergraph attention. _Pattern Recognition_, 110:107637, 2021.
* [31] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar. Hypergcn: A new method for training graph convolutional networks on hypergraphs. _Advances in neural information processing systems_, 32, 2019.
* [32] He Zhang, Bang Wu, Xingliang Yuan, Shirui Pan, Hanghang Tong, and Jian Pei. Trustworthy graph neural networks: Aspects, methods and trends. _arXiv preprint arXiv:2205.07424_, 2022.

* [33] Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. _Advances in neural information processing systems_, 33:12225-12235, 2020.
* [34] N TISHBY. The information bottleneck method. In _Proceedings of the 37-thAnnual Allerton Conference on Communication, 2000_, 2000.
* [35] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In _2015 ieee information theory workshop (itw)_, pages 1-5. IEEE, 2015.
* [36] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottleneck. In _International Conference on Learning Representations_, 2017.
* [37] Marco Federici, Anjan Dutta, Patrick Forre, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. In _International Conference on Learning Representations_, 2020.
* [38] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. _Advances in Neural Information Processing Systems_, 34:15920-15933, 2021.
* [39] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl: Information-aware graph contrastive learning. _Advances in Neural Information Processing Systems_, 34:30414-30425, 2021.
* [40] Junchi Yu, Jie Cao, and Ran He. Improving subgraph recognition with variational graph information bottleneck. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19396-19405, 2022.
* [41] Chunyu Wei, Jian Liang, Di Liu, and Fei Wang. Contrastive graph structure learning via information bottleneck for recommendation. _Advances in Neural Information Processing Systems_, 35:20407-20420, 2022.
* [42] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. _Advances in Neural Information Processing Systems_, 33:20437-20448, 2020.
* [43] Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng Ji, and S Yu Philip. Graph structure learning with variational information bottleneck. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 4165-4174, 2022.
* [44] Ge Zhang, Zhenyu Yang, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Jianlin Su, Chuan Zhou, Quan Z Sheng, Leman Akoglu, et al. Dual-discriminative graph neural network for imbalanced graph-level anomaly detection. _Advances in Neural Information Processing Systems_, 35:24144-24157, 2022.
* [45] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In _International Conference on Machine Learning_, pages 4116-4126. PMLR, 2020.
* [46] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. In _Advances in Neural Information Processing Systems_, volume 33, pages 5812-5823, 2020.
* [47] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In _Proceedings of the Web Conference 2021_, pages 2069-2080, 2021.
* [48] Yixin Liu, Yizhen Zheng, Daokun Zhang, Vincent CS Lee, and Shirui Pan. Beyond smoothing: Unsupervised graph representation learning with edge heterophily discriminating. In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 4516-4524, 2023.
* [49] Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. _Advances in neural information processing systems_, 31, 2018.
* [50] Yixin Liu, Kaize Ding, Huan Liu, and Shirui Pan. Good-d: On unsupervised graph out-of-distribution detection. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 339-347, 2023.
* [51] Jaehyeong Jo, Jinheon Baek, Seul Lee, Dongki Kim, Minki Kang, and Sung Ju Hwang. Edge representation learning with hypergraphs. _Advances in Neural Information Processing Systems_, 34:7534-7546, 2021.
* [52] Claude Berge. _Graphs and hypergraphs_. North-Holland Pub. Co., 1973.
* [53] Edward R Scheinerman and Daniel H Ullman. _Fractional graph theory: a rational approach to the theory of graphs_. Courier Corporation, 2011.

* Velickovic et al. [2019] Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In _International Conference on Learning Representations_, 2019.
* Donsker and Varadhan [1975] Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time, i. _Communications on Pure and Applied Mathematics_, 28(1):1-47, 1975.
* van den Oord et al. [2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* Morris et al. [2020] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In _ICML Workshop_, 2020.
* Knyazev et al. [2019] Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and generalization in graph neural networks. _Advances in neural information processing systems_, 32, 2019.
* Ruff et al. [2018] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In _International conference on machine learning_, pages 4393-4402. PMLR, 2018.
* Debnath et al. [1991] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal chemistry_, 34(2):786-797, 1991.
* Shervashidze et al. [2011] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* Neumann et al. [2016] Marion Neumann, Roman Garnett, Christian Bauckhage, and Kristian Kersting. Propagation kernels: efficient graph kernels from propagated information. _Machine Learning_, 102:209-245, 2016.
* Liu et al. [2008] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _2008 eighth ieee international conference on data mining_, pages 413-422. IEEE, 2008.
* Manevitz and Yousef [2001] Larry M Manevitz and Malik Yousef. One-class svms for document classification. _Journal of machine Learning research_, 2(Dec):139-154, 2001.