# Is Heterogeneity Notorious? Taming Heterogeneity to Handle Test-Time Shift in Federated Learning

 Yue Tan

University of Technology Sydney

yue.tan@student.uts.edu.au

&Chen Chen

Sony AI

chena.chen@sony.com

&Weiming Zhuang

Sony AI

weiming.zhuang@sony.com

&Xin Dong

Sony AI

xindong@sony.com

&Lingjuan Lyu

Sony AI

lingjuan.lv@sony.com

&Guodong Long

University of Technology Sydney

guodong.long@uts.edu.au

Work done during Yue Tan's internship at Sony AI.Corresponding Author.

###### Abstract

Federated learning (FL) is an effective machine learning paradigm where multiple clients can train models based on heterogeneous data in a decentralized manner without accessing their private data. However, existing FL systems undergo performance deterioration due to feature-level test-time shifts, which are well investigated in centralized settings but rarely studied in FL. The common non-IID issue in FL usually refers to inter-client heterogeneity during training phase, while the test-time shift refers to the intra-client heterogeneity during test phase. Although the former is always deemed to be notorious for FL, there is still a wealth of useful information delivered by heterogeneous data sources, which may potentially help alleviate the latter issue. To explore the possibility of using inter-client heterogeneity in handling intra-client heterogeneity, we firstly propose a contrastive learning-based FL framework, namely FedICON, to capture invariant knowledge among heterogeneous clients and consistently tune the model to adapt to test data. In FedICON, each client performs sample-wise supervised contrastive learning during the local training phase, which enhances sample-wise invariance encoding ability. Through global aggregation, the invariance extraction ability can be mutually boosted among inter-client heterogeneity. During the test phase, our test-time adaptation procedure leverages unsupervised contrastive learning to guide the model to smoothly generalize to test data under intra-client heterogeneity. Extensive experiments validate the effectiveness of the proposed FedICON in taming heterogeneity to handle test-time shift problems.

## 1 Introduction

Federated learning (FL) is a promising machine learning paradigm that enables multiple clients or devices to collaboratively train models without sacrificing their data privacy [1, 2]. Practical FL frameworks usually suffer from notorious non-IID (independent and identically distributed) local data distributions, which may lead to severe performance degradation during the test phase [3, 4].

To improve the performance of the trained model on local data distribution, there are a variety of personalized federated learning (PFL) methods based on different techniques, _i.e._, regularization [5; 6], meta-learning [7; 8], knowledge distillation [9; 10], to help the model adapt to local datasets [11].

In existing PFL systems, it is commonly assumed that the training and test datasets within a client share the same or similar distribution. However, this assumption often does not hold in real-world applications where test samples may encounter natural variations or corruptions. Although test-time distribution shifts have received increasing attention in centralized machine learning paradigms [12; 13; 14; 15], the negative effects of these shifts in PFL systems are largely underexplored. Take federated chest X-ray classification [16; 17] among multiple hospitals as an example, while most existing studies acknowledge that different hospitals can have varying data distributions due to diverse patient populations (e.g., races, ages, etc.), they overlook the possibility of variation within a single hospital. For instance, there may be multiple X-ray machines from different vendors, each with its own unique internal parameters (e.g., exposure time, spectrum, etc.). Similar issues can be found in various other domains, such as autonomous vehicles and video surveillance. Consequently, an efficient and effective solution to handle test-time shifts is necessary to achieve satisfactory performance.

The extensively studied heterogeneous distribution in FL, also known as non-IID issue, mainly refers to the _inter-client heterogeneity_ where clients own heterogeneous training data during their federated training phase [2; 3; 18]. Differently, the aforementioned test-time distribution shift during the inference stage of PFL refers to the heterogeneity within a client, which can be defined as _intra-client heterogeneity_. As illustrated in Fig. 1(a), the test data of a client can differ from its training data in various aspects. This discrepancy results in unexpected features in the model optimized on the training data, ultimately leading to suboptimal performance. In this case, a natural question that arises is: _what is the difference between the test-time shift in centralized settings and that in FL?_ Interestingly, we find that the answer lies in the unique inherent inter-client heterogeneity in FL compared with centralized settings. While _inter-client heterogeneity_ is often considered to be a challenge for FL, in this paper, we pinpoint that heterogeneous sources can also convey a wealth of useful information that potentially helps alleviate test-time shift problems in FL systems. A benefit of this heterogeneity is that it enables the global model to capture robust features that are less sensitive to both inter-client and intra-client shifts. We firstly discover that this information has the potential to help alleviate test-time shift problems in FL systems, as it enables the global model to capture robust features that are less sensitive to both inter-client and intra-client shifts. Through our motivated experiments (Fig. 1(b)), we observe that FL under _inter-client heterogeneity_ can improve the ability of clients to handle test-time shift problems compared to training on homogeneous data. In this way, we turn the problem of inter-client heterogeneity as a blessing in solving test-time shift.

Motivated by the above findings, we introduce **F**ederated Learning via **I**nvariant**Ce** Extracti**On** and **S**hari**N**g (FedICON), a contrastive learning-based FL approach that enables clients to learn the robust features on heterogeneous data. Our theme is to fully take advantage of the benefit brought by _inter-client heterogeneity_ during training time, and then, at the test phase, tune the local model of each client on its private test data to conquer the _intra-client heterogeneity_. During the training phase, clients first carry out local representation learning under the guidance of label information

Figure 1: (a) Illustration of our setting where both _inter-client heterogeneity_ and _intra-client heterogeneity_ exist in an FL framework. (b) Compared with training on data from a single domain, the improved robustness to test-time shift is achieved when training on heterogeneous data sources, indicating that the inherent _inter-client heterogeneity_ in FL has a positive effect in handling test-time shift problems. Detailed settings can be found in Appendix A.1.

so that invariant information underlying each training sample, as well as the class it belongs to, is extracted. At the global aggregation stage of federated training, the parameters of feature encoders in all clients are aggregated to further boost the ability of clients to learn invariant information among inter-client heterogeneity. Then, during the test phase, we introduce self-supervision signals to the unlabeled test samples and smoothly tune the feature encoder based on the acquired knowledge in identifying invariant properties between diverse training data and shifted test data. Notably, we leverage contrastive learning paradigms for both federated training and local test-time adaptation, enabling the trained model to well adapt to test data distribution.

Our contributions are summarized as follows:

* We are the first to comprehensively investigate feature-level test-time shifts in FL. We formulate the problem from the perspective of _inter-client heterogeneity_ and _intra-client heterogeneity_ and propose corresponding experimental settings tailored for this problem.
* We propose a novel representation learning-based federated learning algorithm, termed FedICON, that addresses the test-time shift problem by taming inherent heterogeneity in FL for invariance extraction and sharing.
* We conduct extensive experiments on benchmark datasets to measure the performance of FedICON in handling feature-level test-time shift problems. The results demonstrate that FedICON achieves significant improvement in terms of test-time adaptation ability.

## 2 Related Work

Personalized Federated Learning.Personalized federated learning (PFL) aims to enable each client to learn a personalized model that performs well on its local dataset [19; 11]. Existing PFL methods are developed on the basis of various techniques. A branch of work utilizes model decoupling to separate the local model into the globally shared part and personally trained part to realize different levels of personalization [20; 21; 22; 23; 24]. [25; 26; 27; 5; 28] augment the original objective function with additional regularization terms to induce personalization via representation alignment, knowledge transfer, bi-level optimization, etc. [8; 7] incorporate meta-learning to generate the generalized model which can be further optimized as a personalized model. Existing personalized federated learning methods can only personalize the model to local training data, lacking the ability to generalize to the test dataset [29].

Federated Learning with Feature-Level Distribution Shifts.Non-IID issue is the core challenge in both FL and PFL [30]. A common type of non-IID data in real-world scenarios is known as feature-level distribution shifts, including attribute shift, covariate shift, and domain shift [31; 32]. For instance, in FL, the training and test images can be collected in different time period, differnt locations, and even from different photographic equipment. [33] leverages mutual information theory and diversity augmentation to mitigate the negative effects brought by attribute shift and domain shift. [18] proposes to use local batch normalization to alleviate the feature-level distribution shift problems. Enlightened by the idea of domain generalization methods, [34; 35] design different knowledge transfer strategies to learn high-quality domain consensus knowledge across shifted clients. Based on feature-level distribution shift setting, [36; 37] aim to alleviate the inter-domain knowledge forgetting problem and performance degradation problem on unseen clients/domains, respectively.

Test-Time Adaptation.Test-time adaptation methods aim to improve the model performance on out-of-distribution data by adapting the learned model to test data [38; 39]. Several methods propose to adjust the model by minimizing the pre-defined test-time entropy in various forms [40; 15]. [41] presents a classifier-only adjustment module for test domain adaptation, and [12] further shows that selectively fine-tuning part of the model can address different types of distribution shifts. [13; 42] tune the model on unlabelled test data via self-supervised learning objectives. [43] further proposes to use more informative self-supervised learning to solve the test-time performance degradation in the presence of severe distribution shifts. The performance of existing centralized test-time adaptation methods can be hindered due to the inherent heterogeneity in FL during the training phase, which further affects the test adaptation process.

Federated Representation Learning.Representation learning focuses on extracting higher-level representations from raw data using deep neural networks [44]. Combining it with the federatedlearning framework enables distributed clients to learn powerful representation and thus benefit their downstream tasks [45; 46; 47; 48]. [49] introduces a generalized federated self-supervised learning framework and provides in-depth empirical studies on the effect of key components in federated representation learning. [50] incorporates model decoupling and representation learning for better PFL. [51; 52] aim to learn a common representation model without supervision under the FL framework. Both of them utilize Siamese networks as their local representation module.

## 3 Notations and Preliminaries

### Notations

Considering \(m\) clients in an FL system, \(P_{i}\) and \(Q_{i}\) are training and test data distributions of the \(i\)-th client (\(i\in\{1,\cdots,m\}\)) respectively and \(\mathcal{D}_{i}^{P}\) and \(\mathcal{D}_{i}^{Q}\) are the corresponding training dataset and test dataset. A sample-label pair is denoted as \((\mathbf{x},y)\) where \(\mathbf{x}\) is the raw input data sample and \(y\) is its corresponding label.

### Inter-Client Heterogeneity

For arbitrary client \(i\) and client \(j\) in an FL system, the _inter-client heterogeneity_ refers to varying \(P_{i}\) and \(P_{j}\) with respect to their raw data sample \(\mathbf{x}\) and/or label \(y\). For example, in the cases where \(P_{i}(y)\neq P_{j}(y)\) while \(P_{i}(\mathbf{x}|y)=P_{j}(\mathbf{x}|y)\), there exists an inter-client label distribution shift. The case where \(P_{i}(\mathbf{x})\neq P_{j}(\mathbf{x})\) while \(P_{i}(y|\mathbf{x})=P_{j}(y|\mathbf{x})\) can lead to inter-client feature distribution shift, which is common in real-world FL systems [18; 33; 36].

### Intra-Client Heterogeneity

Most existing works on FL consider the heterogeneity of training datasets \(\mathcal{D}_{i}^{P}\) across clients while overlooking the intra-client heterogeneity \(P_{i}\neq Q_{i}\). [31] examines test distribution in robust FL benchmarks, combining label-level and feature-level shifts. To better understand their distinct impacts, we propose to study these shifts separately. Here, we focus on varying \(P_{i}\) and \(Q_{i}\) with respect to \(\mathbf{x}\) rather than \(y\), i.e., addressing the feature-level test-time shift problem in FL. Incorporating real-world scenarios, we thoroughly examine inter-client feature distribution shifts, including covariate shift, attribute shift, and domain shift. We assume that all clients have access to their own _labelled_ training data when participating in the federated training process and test the performance of their personally tuned model on their own test set.

Challenges.It is a challenging task to deal with both inter-client and intra-client heterogeneity in an FL system. Although there is useful information underlying heterogeneous clients, how to effectively extract it and how to further leverage it to benefit the test procedure remain unexplored. Hence, there is a need for feasible solutions to tame inherent inter-heterogeneity to handle the test-time intra-client heterogeneity in FL.

Figure 2: An overview of the proposed FedICON. (a) Federated training phase under inter-client heterogeneity. Each client conducts representation-wise local invariance extraction during local training and performs parameter-wise global invariance sharing during global aggregation. (b) Test phase on a single client. Clients carry out independent test-time adaptation to generalize to their local test sets.

Federated Learning via Invariance Extraction and Sharing

Both inter-client heterogeneity and intra-client heterogeneity arise from shifts in data distribution. Our core idea is to take advantage of diverse data distributions across clients so that robust features that remain invariant can be learned to handle intra-client shifts. Motivated by this, we propose **Fed**erated Learning via **I**nvariant**Ce** Extract**On** and ShariNg (FedICON) as shown in Figure 2. During the federated training phase, to extract invariant information of each class within the local dataset, we design a _local invariance extraction_ scheme by employing contrastive learning in a supervised manner (Sec. 4.1.1). Then, we perform _global invariance sharing_ under inter-client heterogeneity to further boost the invariance extraction ability for each client (Sec. 4.1.2). During the test phase, each client tunes the personalized model to adapt to its local test set (Sec. 4.2).

Optimization Objective.Since each client has varying training and test data distribution, the global objective of the personalized FL system across \(m\) clients can be formulated as a two-stage optimization process,

\[(\theta_{1}^{\prime},\theta_{2}^{\prime},\cdots,\theta_{m}^{\prime})=\min_{( \theta_{1},\theta_{2},\cdots,\theta_{m})}\frac{1}{m}\sum_{i=1}^{m}\frac{| \mathcal{D}_{i}^{P}|}{N^{P}}L_{i,\sup}\left(\theta_{i};\mathcal{D}_{i}^{P} \right),\] (1)

\[\theta_{i}^{*}=\min_{\theta_{i}}L_{i,\text{us}}\left(\theta_{i};\mathcal{D}_{i }^{Q}\right),\] (2)

where Eq. 1 and Eq. 2 are the optimization objective during the federated training phase and test phase, respectively. In Eq. 1, \(L_{i,\text{sup}}\) and \(\theta_{i}\) are the local supervised loss function and learnable model parameters for client \(i\), respectively. \(N^{P}\) is the total number of training samples among all clients. In Eq. 2, \(\theta_{i}\) is initialized as \(\theta_{i}^{\prime}\) and further optimized on the test data of client \(i\) via unsupervised learning loss function \(L_{i,\text{us}}\). The eventual purpose of the framework is to efficiently learn invariant knowledge under inter-client heterogeneity during the training phase so that the intra-client heterogeneity can be alleviated via further optimization during the test phase.

### Federated Training under Inter-Client Heterogeneity

Compared with the test-time shift problem in centralized settings, there are inherent heterogeneous source data in the federated learning framework, which is of great potential to utilize to deal with the shifted test data. Although the inter-client heterogeneous data are not directly accessible due to privacy concerns [53], their underlying invariant information can be captured by each client during local invariance extraction procedure and then globally shared across all clients to further enhance the invariance extraction ability.

#### 4.1.1 Local Invariance Extraction.

To capture invariant information from samples within the same class, we design a local invariance extraction procedure based on supervised contrastive learning. As shown in Fig. 2(a), samples are first fed into a feature encoder to generate the representation for each sample. Then, a contrastive learning scheme is applied to the representations under the guidance of their corresponding labels so that the class-level invariant information can be extracted during the local training phase.

To make the representation ability of the feature encoder more robust, we add an augmentation module during the data pre-processing stage that incorporates some perturbations to raw samples. In this way, there are various views of the same sample fed into the feature encoder to strengthen its sample-based discrimination ability. Concretely, for the \(i\)-th client, given a batch of input samples, we generate two random augmented views for each sample. The total size of the augmented local dataset is \(2|\mathcal{D}_{i}|\). For a specific sample \(\mathbf{x}\), we use \(A(\mathbf{x})\) to denote the local dataset without \(\mathbf{x}\).

Taking advantage of the powerful representation ability brought by contrastive learning frameworks, we force the feature encoder to discriminate the augmented samples from different classes. Given an input sample \(\mathbf{x}\), the output of the feature encoder is

\[z(\mathbf{x})=h\left(\mathbf{x};\theta_{i}\right).\] (3)

where \(\theta_{i}\) is the learnable parameter of the feature encoder. We denote \(z(\mathbf{x})\) as \(z_{\mathbf{x}}\) for short. Samples from the same class are forced to be closer, and samples from different classes are forced to be farther.

We perform local training with the loss function as below,

\[L_{i,\mathrm{sup}}\left(\theta_{i};\mathcal{D}_{i}^{P}\right)=\sum_{(\mathbf{x},y )\in\mathcal{D}_{i}^{P}}\frac{-1}{|P(\mathbf{x})|}\sum_{\mathbf{p}\in P(\mathbf{ x})}\log\frac{\exp\left(z_{\mathbf{x}}\cdot z_{\mathbf{p}}/\tau\right)}{\sum_{ \mathbf{a}\in A(\mathbf{x})}\exp\left(z_{\mathbf{x}}\cdot z_{\mathbf{a}}/\tau \right)},\] (4)

where \(z_{\mathbf{a}}\) is the representation of sample \(\mathbf{a}\in A(\mathbf{x})\), and \(\tau\) is the temperature adjusting the tolerance for representation difference. \(P(\mathbf{x})\) is the set of samples that have the same labels with \(\mathbf{x}\) and can be represented as

\[P(\mathbf{x})=\{\mathbf{p}\in A(\mathbf{x}):y_{\mathbf{p}}=y_{\mathbf{x}}\}.\] (5)

By training the local feature encoder in a contrastive manner, the local invariances can be directly learned from the augmented data and be encoded to the feature encoder. Even if there is inter-client heterogeneity among different clients, their invariance encoding abilities over heterogeneous data are universal, which makes it possible for clients to mutually boost their performance on invariance extraction in a federated framework.

#### 4.1.2 Global Invariance Sharing.

To tame the inter-client heterogeneity during the federated training process, there should be a uniform information channel that is globally shared across clients and continuously provides meaningful knowledge to boost local invariance extraction ability. Considering the fact that the feature encoder introduced above is trained to capture and encode the invariance, we can employ the parameters in the feature encoder as the information carrier and perform global invariance sharing at the server side. These invariant robust properties captured by the feature encoder are shared within different augmented views of a specific sample and different samples from the same class.

Similar to the standard federated aggregation process, for each client \(i\in\{1,2,\cdots,m\}\), the learnable parameter \(\theta_{i}\) of its feature encoder is sent to the server. After receiving parameters from all the clients, the server aggregates them as

\[\bar{\theta}=\frac{1}{m}\sum_{i=1}^{m}\frac{|\mathcal{D}_{i}^{P}|}{N^{P}} \theta_{i},\] (6)

and sends \(\bar{\theta}\) back to each client as their initialized parameter in a new round. Since the feature encoder of each client is trained to extract invariance efficiently rather than classify the samples correctly, the parameter aggregation is less likely to be affected by inter-client heterogeneity. Global invariance sharing with respect to the feature encoder incorporates the invariance encoding abilities learned upon heterogeneous source data, which reversely provides sufficient homogeneous information for each client and increases the robustness of local representation learning. To employ the model trained alternately via local invariance extraction and global invariance sharing, we further train a linear classifier on top of the fixed feature encoder [54, 55]. The linear classifier can be trained in a pure local manner or trained in a federated learning manner. In this paper, we conduct multiple federated training steps at first and then personalize the classifier for each client.

### Test-Time Adaptation to Handle Intra-Client Heterogeneity

In Section 4.1, we have developed components that leverage the inherent inter-client heterogeneity in FL in order to handle the test-time intra-client heterogeneity. To keep the losses of the training and test phase unified and make the whole process consistent, we adopt a self-supervised contrastive learning scheme to carry out test-time adaptation, which is also a natural and efficient way to generalize the trained model to the unlabeled and shifted test set.

Concretely, for client \(i\), the trained feature encoder is further tuned by optimizing the following unsupervised loss function on its local test set \(\mathcal{D}_{i}^{Q}\).

\[L_{i,\mathrm{us}}\left(\theta_{i};\mathcal{D}_{i}^{Q}\right)=\sum_{\mathbf{x} \in\mathcal{D}_{i}^{Q}}-\log\frac{\exp\left(z_{\mathbf{x}}\cdot z_{\mathbf{x}}^ {\prime}/\tau\right)}{\sum_{a\in A(\mathbf{x})}\exp\left(z_{\mathbf{x}}\cdot z _{\mathbf{a}}/\tau\right)}.\] (7)

where \(z_{\mathbf{x}}^{\prime}\) is the representation of the augmented sample of \(\mathbf{x}\). Similar to the training phase, \(A(\mathbf{x})\) here also denotes the local test dataset without \(\mathbf{x}\). The main difference between Eq. 4 and 7 is whether the label information is accessible or not. Apart from this, Eq. 4 and 7 are consistently formulated and can share the same optimization strategy. During the training phase, the contrastive learning process can be conducted under the guidance of label information, which encourages the feature encoder to output well-aligned representations for samples within the same class. Then, based on the robust representations learned among inter-client heterogeneity, the feature encoder can further improve the generalization ability on the test set in an unsupervised manner, which preserves the contrastive learning mode in the training phase.

The test scenario can be easily extended to the online manner where the test samples arrive one by one. First, a set of training samples is selected from the training set, replacing the \(A(\mathbf{x})\) in Eq. 7. Then, after performing the optimization for each test sample, we replace one randomly selected training sample in \(A(\mathbf{x})\) with this test sample for future test samples. This transfers the whole pipeline into an online manner and allows the model to gradually adapt to the distribution of test data.

We present our proposed FedICON in detail in Algorithm 1 in Appendix C.

## 5 Experiments

### Experimental Setup

Datasets and test-time shift settings.We evaluate our proposed method on the following four benchmark datasets: Digit-5 [56], Office-10 [57], DomainNet dataset [58], and CIFAR10 [59]. Digit-5, Office-10, and DomainNet dataset are three benchmark dataset consisting of multiple sub-datasets from different domains. We use them to simulate the _covariate shift_ and _domain shift_ during the test phase. We use CIFAR10 and its variant CIFAR10.1 [60; 61] to simulate the _attribute shift_. Details about the test-time shift settings can be found in Appendix A.2.

* Covariate shift. We follow the setting in [31] by leveraging 15 common corruptions in CIFAR10-C [62], _e.g._, Gaussian Noise, Brightness, etc.
* Attribute shift. The training data of all clients are from CIFAR10, while the test data of all clients are from CIFAR10.1 shifted from CIFAR10, _e.g._, the most common object in the airplane class can change from airliner to fighter.
* Domain shift. We follow the training data setting in [18], where clients own data from different domains with heterogeneous appearances. We follow the leave-one-out principle to select one domain as the test domain and the other domains as the training domains. The training data of each client is from a single domain.

Baselines and implementation.We first compare our proposed method with two kinds of baselines: 1) Local, where each client only performs local training without any communication; FL/PFL methods including 2) FedAvg [63], 3) FedAvg-FT that fine-tunes the global model trained by FedAvg on local dataset, 4) FedProx [64], 5) FedRep [22], 6) FedBN [18], 7) FedTHE [31]. We also compare with three classical centralized test-time adaptation (TTA) methods, including 1) Tent [38], 2) EATA [40], and 3) T3A [41]. In practice, we implement these TTA methods with the FedAvg framework. Details about the baseline methods and their parameter settings can be found in Appendix A.3.

For Digit-5, Office-10, and CIFAR10, we use simple CNN architecture as the backbone model, while for DomainNet dataset, we use ResNet18 as the backbone model. Details about the model of each dataset can be found in Appendix A.4. We set the local training batch size as 32, and an SGD optimizer with weight decay rate 5e-4 and learning rate 0.01 for cross-entropy loss and 0.005 for contrastive loss. The temperature in Eq. 4 and Eq. 7 is 0.1 for Digit-5, Office-10, CIFAR10, 10 for DomainNet dataset. The size of representation used for contrastive learning is 64 for Digit-5, Office-10, and CIFAR10, and 512 for DomainNet dataset. The default number of local epochs is \(E=5\) and the number of communication rounds is 100. We implement all the methods using PyTorch and conduct all experiments on NVIDIA Tesla V100 GPU. Detailed settings can be found in Appendix A.

### Performance Comparison

Performance on various test-time shifts.Table 1 and Table 2 report the test accuracy of FedICON and baselines on Digit-5 dataset under _covariate_ test-time shift and Office-10 dataset under _domain_ test-time shift, respectively. The results are reported in mean(standard error) format over three independent runs.

[MISSING_PAGE_FAIL:8]

FedAvg keeps dropping as the number of local epochs increases, while FedICON partially enjoys the benefit of more local training, thus more communication-efficient than FedAvg.

Extending to unsupervised scenarios.The main framework of our proposed method is based on representation learning, which makes it flexible to migrate the whole learning paradigm into the unsupervised representation learning manner. During the local invariance extraction and global invariance sharing stages, we remove the label information. The feature encoder is purely trained in a self-supervised manner and the label information is accessible only when training the classifier. As is shown in Table 4, the extended version of our method FedICON, termed as FedICON-_us_, achieves the runner-up performance on Digit-5 and Office-10 under covariate test-time shift.

### Ablation Studies

We carry out ablation studies to see the effect of each key technical component in FedICON. We alternatively remove the local invariance extraction, global invariance sharing and test-time adaptation procedure to see how these components affect the performance in handling test-time shift issues in FL. It is worth noting that when the local invariance extraction component is removed, the local training objective changes from the supervised contrastive loss to the conventional classification loss (_i.e._, cross-entropy), and the global invariance sharing component is in charge of sharing the

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Method** & **Digit-5** & **Office-10** \\ \hline Local & 34.00(3.12) & 58.07(2.84) \\ FedAvg & 55.04(1.00) & 57.55(1.46) \\ FedAvg-FT & 54.55(0.83) & 60.45(1.40) \\ FedICON & **62.23**(0.88) & **64.96**(1.43) \\ \hline FedICON-_ts_ & 61.60(1.45) & 63.79(1.17) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Test accuracy on Digit-5 and Office-10 dataset under _covariate_ test-time shift. The best and runner-up results are highlighted with **bold** and underline, respectively.

Figure 3: Test accuracy on Digit-5 under varying numbers of clients (_left_) and varying numbers of local epochs (_right_). Both are conducted in the covariate test-time shift cases. In (a), the number of clients (\(m\)) ranges from \(5\) to \(80\). The total number of samples is kept constant, so the number of samples in each client is fewer as \(m\) goes larger. In (b), the number of local training epochs ranges from \(1\) to \(9\).

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c|}{**Non-IID Dir(0.1)**} & \multicolumn{3}{c}{**Non-IID Dir(1)**} \\ \cline{2-7}  & Original & **Attribute Shift** & \(\Delta\) & Original & **Attribute Shift** & \(\Delta\) \\ \hline Local & 89.15(1.13) & 83.54(0.89) & -5.61 & 67.60(0.65) & 61.61(1.99) & **-5.99** \\ \hline FedAvg & 64.25(1.42) & 50.09(1.36) & -14.16 & 73.23(0.38) & 57.17(0.34) & -16.06 \\ FedAvg-FT & 87.82(0.59) & 80.95(2.55) & -6.87 & 78.21(0.68) & 65.59(1.46) & -12.62 \\ FedProx & 59.79(5.91) & 52.25(2.53) & -7.54 & 72.02(0.37) & 61.72(1.29) & -10.3 \\ FedRep & 88.73(0.26) & 81.72(0.63) & -7.01 & 79.25(0.59) & 66.69(1.22) & -12.56 \\ FedBN & 83.09(4.56) & 78.14(0.57) & -4.95 & 77.67(1.07) & 66.90(1.31) & -10.77 \\ FedTHE & 90.55(0.41) & 81.91(0.54) & -8.64 & **80.49**(0.89) & 66.25(0.96) & -14.24 \\ \hline Tent & **91.39**(0.77) & 85.72(1.24) & -5.67 & 78.61(0.63) & 71.50(1.29) & -7.11 \\ EATA & 91.37(0.78) & 85.60(1.16) & -5.77 & 79.72(0.62) & 70.44(1.67) & -7.28 \\ T3A & 90.79(0.73) & 85.91(1.68) & -4.88 & 78.80(0.95) & 70.93(0.68) & -7.87 \\ \hline FedICON & 90.24(0.52) & **86.47**(1.27) & **-3.77** & 79.43(0.78) & **72.41**(1.29) & -7.02 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Test accuracy on CIFAR10 dataset under _attribute_ test-time shift. All results are reported in mean(std error) format over three independent runs. \(\Delta\) represents the difference between the accuracy on _original_ test set and that on _attribute_ test-time shift case.

learnable parameters of the local model. In Table 5, we can observe that both global invariance sharing and test-time adaptation play an important role in improving the ability to deal with test-time shift problems. The effect of global invariance sharing is more notable because it is the core step to explore the inherent inter-client heterogeneity and broadcast the invariant encoding knowledge to benefit their local learning. The test-time adaptation brings about 1% to 5% improvement compared with the variants without this component, verifying the effectiveness of consistent representation alignment via a self-supervised learning scheme during the test phase.

## 6 Conclusion

In this paper, we aim to address the challenging but unexplored feature-level test-time shift problems in FL. We take the first initiative to thoroughly investigate the inherent _inter-client heterogeneity_ in FL and propose to use it to deal with the test-time _intra-client heterogeneity_. We propose a novel algorithm, namely FedICON, where invariant information underlying heterogeneous data is extracted in a contrastive learning manner during the federated training phase and further used to deal with test-time shifts in a client. On our proposed benchmarks tailored for this problem, extensive experiments are conducted to show the superiority of FedICON compared with baseline methods.

This research has great potential in solving test-time shift problems in real-world FL systems. Providing theoretical guarantees for our work and discussion on privacy and security can be further explored in future studies. Considering the evident motivation, the novel setting and the effectiveness of our method, we believe this work is intriguing for future studies.

## References

* [1] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. _ACM TIST_, 2019.
* [2] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [3] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid data. _arXiv:1806.00582_, 2018.
* [4] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In _ICLR_, 2019.
* [5] Canh T Dinh, Nguyen Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau envelopes. In _NerutPS_, 2020.
* [6] Filip Hanzely and Peter Richtarik. Federated learning of a mixture of global and local models. _arXiv:2002.05516_, 2020.
* [7] Yihan Jiang, Jakub Konecny, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. _arXiv preprint arXiv:1909.12488_, 2019.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline
**Design Component** & \multicolumn{8}{c}{**Variants**} \\ \hline Local Invariance Extraction & & ✓ & & ✓ & & ✓ & ✓ \\ Global Invariance Sharing & & & ✓ & ✓ & ✓ & & ✓ \\ Test-Time Adaptation & & & & ✓ & ✓ & ✓ & ✓ \\ \hline Test Accuracy & 34.00 & 48.60 & 55.04 & 39.92 & 59.39 & 59.54 & 49.67 & 62.23 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison between the FedICON and its variants. Experiments are conducted on Digit-5 dataset under _covariate_ test-time shift. The number of clients is 5. Each of them owns data samples from a specific domain. Corruptions are applied to their local test set to establish the _covariate_ test-time shift setting. The number of communication rounds is 100.

* Fallah et al. [2020] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-learning approach. In _NeruIPS_, 2020.
* Lin et al. [2020] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model fusion in federated learning. In _NeurIPS_, 2020.
* Shen et al. [2022] Yiqing Shen, Yuyin Zhou, and Lequan Yu. Cd2-pfed: Cyclic distillation-guided channel decoupling for model personalization in federated learning. In _CVPR_, 2022.
* Tan et al. [2022] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards personalized federated learning. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* Lee et al. [2023] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In _International Conference on Learning Representations_, 2023.
* Sun et al. [2020] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _International conference on machine learning_, pages 9229-9248. PMLR, 2020.
* Liu et al. [2023] Yixin Liu, Kaize Ding, Jianling Wang, Vincent Lee, Huan Liu, and Shirui Pan. Learning strong graph neural networks with weak information. In _ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)_, 2023.
* Zhang et al. [2022] Marvin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmentation. _Advances in Neural Information Processing Systems_, 35:38629-38642, 2022.
* Nguyen et al. [2022] Dinh C Nguyen, Quoc-Viet Pham, Pubudu N Pathirana, Ming Ding, Aruna Seneviratne, Zihuai Lin, Octavia Dobre, and Won-Joo Hwang. Federated learning for smart healthcare: A survey. _ACM Computing Surveys (CSUR)_, 55(3):1-37, 2022.
* Long et al. [2021] Guodong Long, Tao Shen, Yue Tan, Leah Gerrard, Allison Clarke, and Jing Jiang. Federated learning for privacy-preserving open innovation future on digital health. In _Humanity Driven AI: Productivity, Well-being, Sustainability and Partnership_, pages 113-133. Springer, 2021.
* Li et al. [2020] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning on non-iid features via local batch normalization. In _ICLR_, 2020.
* Zhang et al. [2021] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. A survey on federated learning. _Knowledge-Based Systems_, 216:106775, 2021.
* Chen and Chao [2022] Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In _ICLR_, 2022.
* Dong et al. [2022] Xin Dong, Sai Qian Zhang, Ang Li, and HT Kung. Spherefed: Hyperspherical federated learning. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVI_, pages 165-184. Springer, 2022.
* Collins et al. [2021] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In _ICML_, 2021.
* Arivazhagan et al. [2019] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. _arXiv:1912.00818_, 2019.
* Liang et al. [2020] Paul Pu Liang, Terrance Liu, Liu Ziyin, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with local and global representations. _NeurIPS_, 2020.
* Zhang et al. [2021] Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu. Parameterized knowledge transfer for personalized federated learning. _NeruIPS_, 34, 2021.
* Tan et al. [2022] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. FedProto: Federated prototype learning across heterogeneous clients. In _AAAI_, 2022.

* Li et al. [2021] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In _ICML_, pages 6357-6368. PMLR, 2021.
* Hanzely et al. [2020] Filip Hanzely, Slavomir Hanzely, Samuel Horvath, and Peter Richtarik. Lower bounds and optimal algorithms for personalized federated learning. _NeruIPS_, 33:2304-2315, 2020.
* Zhuang et al. [2023] Weiming Zhuang, Chen Chen, and Lingjuan Lyu. When foundation model meets federated learning: Motivations, challenges, and future directions. _arXiv preprint arXiv:2306.15546_, 2023.
* Chen et al. [2022] Chen Chen, Yuchen Liu, Xingjun Ma, and Lingjuan Lyu. CalFAT: Calibrated federated adversarial training with label skewness. In _NeurIPS_, 2022.
* Jiang and Lin [2023] Liangze Jiang and Tao Lin. Test-time robust personalization for federated learning. In _International Conference on Learning Representations_, 2023.
* Zhuang et al. [2023] Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, and Shuai Zhang. MAS: Towards resource-efficient federated multiple-task learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23414-23424, 2023.
* Luo et al. [2022] Zhengquan Luo, Yunlong Wang, Zilei Wang, Zhenan Sun, and Tieniu Tan. Disentangled federated learning for tackling attributes skew via invariant aggregation and diversity transferring. In _ICML_, 2022.
* Feng et al. [2021] Haozhe Feng, Zhaoyang You, Minghao Chen, Tianye Zhang, Minfeng Zhu, Fei Wu, Chao Wu, and Wei Chen. Kd3a: Unsupervised multi-source decentralized domain adaptation via knowledge distillation. In _ICML_, 2021.
* Chen et al. [2023] Junming Chen, Meirui Jiang, Qi Dou, and Qifeng Chen. Federated domain generalization for image recognition via cross-client style transfer. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 361-370, 2023.
* Huang et al. [2022] Wenke Huang, Mang Ye, and Bo Du. Learn from others and be yourself in heterogeneous federated learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10143-10153, 2022.
* Yuan et al. [2022] Honglin Yuan, Warren Richard Morningstar, Lin Ning, and Karan Singhal. What do we mean by generalization in federated learning? In _International Conference on Learning Representations_, 2022.
* Wang et al. [2021] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In _International Conference on Learning Representations_, 2021.
* Liu et al. [2023] Yixin Liu, Kaize Ding, Huan Liu, and Shirui Pan. Good-d: On unsupervised graph out-of-distribution detection. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 339-347, 2023.
* Niu et al. [2022] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In _International conference on machine learning_, pages 16888-16905. PMLR, 2022.
* Iwasawa and Matsuo [2021] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. _Advances in Neural Information Processing Systems_, 34:2427-2440, 2021.
* Bartler et al. [2022] Alexander Bartler, Andre Buhler, Felix Wiewel, Mario Dobler, and Bin Yang. Mt3: Meta test-time training for self-supervised test-time adaption. In _International Conference on Artificial Intelligence and Statistics_, pages 3080-3090. PMLR, 2022.
* Liu et al. [2021] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: When does self-supervised test-time training fail or thrive? _Advances in Neural Information Processing Systems_, 34:21808-21820, 2021.

* [44] Bram van Berlo, Aaqib Saeed, and Tanir Ozcelebi. Towards federated unsupervised representation learning. In _Proceedings of the third ACM international workshop on edge systems, analytics and networking_, pages 31-36, 2020.
* [45] Duc Bui, Kshitiz Malik, Jack Goetz, Honglei Liu, Seungwhan Moon, Anuj Kumar, and Kang G Shin. Federated user representation learning. _arXiv preprint arXiv:1909.12535_, 2019.
* [46] Yue Tan, Guodong Long, Jie Ma, Lu Liu, Tianyi Zhou, and Jing Jiang. Federated learning from pre-trained models: A contrastive learning approach. _Advances in Neural Information Processing Systems_, 35:19332-19344, 2022.
* [47] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on non-iid graphs via structural knowledge sharing. In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 9953-9961, 2023.
* [48] Jingtao Li, Lingjuan Lyu, Daisuke Iso, Chaitali Chakrabarti, and Michael Spranger. MocoSFL: enabling cross-client collaborative self-supervised learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [49] Weiming Zhuang, Yonggang Wen, and Shuai Zhang. Divergence-aware federated self-supervised learning. In _International Conference on Learning Representations_, 2020.
* [50] Jaehee Jang, Heoneok Ha, Dahuin Jung, and Sungroh Yoon. Fedclassavg: Local representation learning for personalized federated learning on heterogeneous neural networks. In _Proceedings of the 51st International Conference on Parallel Processing_, pages 1-10, 2022.
* [51] Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, and Shuai Yi. Collaborative unsupervised visual representation learning from decentralized data. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4912-4921, 2021.
* [52] Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yueting Zhuang, and Xiaolin Li. Federated unsupervised representation learning. _arXiv preprint arXiv:2010.08982_, 2020.
* [53] Chen Chen, Lingjuan Lyu, Han Yu, and Gang Chen. Practical attribute reconstruction attack against federated learning. _IEEE Transactions on Big Data_, 2022.
* [54] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _NerulPS_, 33, 2020.
* [55] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, pages 1597-1607. PMLR, 2020.
* [56] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization. In _ECCV_, 2020.
* [57] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised domain adaptation. In _CVPR_, 2012.
* [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1406-1415, 2019.
* [59] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [60] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10? 2018. https://arxiv.org/abs/1806.00451.
* [61] Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 30(11):1958-1970, 2008.
* [62] Dan Hendrycks and Thomas G Dietterich. Benchmarking neural network robustness to common corruptions and surface variations. In _ICLR_, 2019.

* [63] H Brendan McMahan, Eider Moore, Daniel Ramage, et al. Communication-efficient learning of deep networks from decentralized data. In _AISTATS_, 2017.
* [64] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _MLSys_, 2020.
* [65] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _ICLR_, 2019.