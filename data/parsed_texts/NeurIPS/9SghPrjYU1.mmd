Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning

 Zhishuai Liu

Duke University

zhishuai.liu@duke.edu

&Pan Xu

Duke University

pan.xu@duke.edu

###### Abstract

Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depend on a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest.

## 1 Introduction

Offline reinforcement learning (RL) [17; 18], which aims to learn an optimal policy achieving maximum expected cumulative reward from a pre-collected dataset, plays an important role in critical domains where online exploration is infeasible due to high cost or ethical issues, such as precision medicine [49; 11; 22; 21] and autonomous driving [32; 43]. The foundational assumption of offline RL [18; 15; 53] is that the offline dataset is collected from the same environment where learned policies are intended to be deployed. However, this assumption can be violated in practice due to temporal changes in dynamics. In such cases, standard offline RL could face catastrophic failures [10; 31; 64]. To address this issue, the robust offline RL [28; 30] focuses on robust policy training against the environment perturbation, which serves as a promising solution. Existing empirical successes of robust offline RL rely heavily on expressive function approximations [37; 36; 25; 45; 63; 16], as the omnipresence of applications featuring large state and action spaces necessitates powerful function representations to enhance generalization capability of decision-making in RL.

To theoretically understand robust offline RL with function approximation, the distributionally robust Markov decision process (DRMDP) [39; 30; 13] provides an established framework. In stark contrast to the standard MDP, DRMDP specifically tackles the _model uncertainty_ by forming an uncertainty set around the nominal model, and takes a max-min formulation aiming to maximize the value function corresponding to a policy, uniformly across all perturbed models in the uncertainty set [55; 52; 57; 35; 41; 59; 40]. The core of DRMDPs lies in achieving an amenable combination of uncertainty set design and corresponding techniques to solve the inner optimization over the uncertainty set. However, this consideration of model uncertainty introduces fundamental challenges to function approximation in terms of computational and statistical efficiency, particularly given the need to maximally exploit essential information in the offline dataset. For instance, in cases where the state and action spaces are large, the commonly used \((s,a)\)-rectangular uncertainty set can make the inner optimization computationally intractable for function approximation [66]. Additionally, the distribution shifts, arising from the mismatch between the behavior policy and the target policy, as well as the mismatch between the nominal model and perturbed models, complicate the statistical analysis [41, 4]. Several recent works attempt to conquer these challenges. Panaganti et al. [35] studied the \((s,a)\)-rectangularity, and their algorithm may suffer from the above mentioned computational issue. Additionally, the \((s,a)\)-rectangular uncertainty set may contain transitions that would never happen in reality, and thus leads to conservative policies; Blanchet et al. [4] proposed a novel double pessimism principle, while their algorithm requires strong oracles, which is not practically implementable. Meanwhile, a line of works study function approximation in the online setting [44, 38, 51, 3, 20] or with a simulator [66], which are not applicable to offline RL. Thus, the following question arises:

_Is it possible to design a computationally efficient and minimax optimal algorithm for robust offline RL with function approximation?_

To answer the above question, we focus on a basic setting of _\(d\)-rectangular linear DRMDP_, where the nominal model is a standard linear MDP, and all perturbed models are parameterized in a linearly structured uncertainty set. We provide the first _instance-dependent_ suboptimality analysis in the DRMDP literature with function approximation, which offers insights into the problem's intrinsic characteristics and challenges. Concretely, our contributions are summarized as follows.

* We propose a computationally efficient algorithm, Distributionally Robust Pessimistic Value Iteration (DRPVI), based on the pessimism principle [15, 53, 41] with a new _function approximation_ mechanism explicitly devised for \(d\)-rectangular linear DRMDPs. We show that DRPVI achieves the following instance-dependent upper bound on the suboptimality gap: \[\beta_{1}\cdot\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H} \mathbb{E}^{\pi^{*},P}\big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{ i}\|_{\mathbf{\Lambda}_{h}^{-1}}|s_{1}=s\big{]}^{1},\] This bound resembles those established in offline RL within standard linear MDPs [15, 62, 54]. However, there are two significant differences in our results. First, our bound depends on the _supremum_ over the uncertainty set of transition kernels instead of one single transition kernel. Second, our result relies on a _diagonal-based normalization_, instead of the Mahalanobis norm of the feature vector, \(\|\boldsymbol{\phi}(s_{h},a_{h})\|_{\mathbf{\Lambda}_{h}^{-1}}\). See Table 1 for a clearer comparison. These two distinctions are unique to DRMDPs with function approximation, which we discuss in more details in Section 4. Moreover, our analysis provides a novel pipeline for studying instance-dependent upper bounds of computationally efficient algorithms under \(d\)-rectangular linear DRMDPs.
* We improve DRPVI by incorporating _variance information_ into the new function approximation mechanism, resulting in the VA-DRPVI algorithm, which achieves a smaller upper bound: \[\beta_{2}\cdot\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H} \mathbb{E}^{\pi^{*},P}\big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_ {i}\|_{\mathbf{\Sigma}_{h}^{-1}}|s_{1}=s\big{]}^{2}.\] This improves the result of DRPVI due to the fact that \(\mathbf{\Sigma}_{h}^{*-1}\preceq H^{2}\mathbf{\Lambda}_{h}^{-1}\) by definition [60, 54]. Furthermore, when the uncertainty level \(\rho=O(1)\), we show that the robust value function attains a _Range Shrinkage_ property, leading to an improvement in the upper bound by an order of \(H\). This explicit improvement is new in variance-aware algorithms, and is unique to DRMDPs.
* We further establish an _information-theoretic lower bound_. We prove that the upper bound of VA-DRPVI matches the information-theoretic lower bound up to \(\beta_{2}\), which implies that VA-DRPVI is near-optimal in the sense of information theory. Importantly, both DRPVI and VA-DRPVI are computationally efficient and do not suffer from the high computational burden, as discussed above in settings with the \((s,a)\)-rectangular uncertainty set, due to a decoupling property of the \(d\)-rectangular uncertainty set (see Remark 4.1 for more details). Thus, we confirm that, for robust offline RL with function approximation, both the _computational efficiency_ and _minimax optimality_ are achievable under the setting of \(d\)-rectangular linear DRMDPs.

Our algorithm design and theoretical analysis draw inspiration from two crucial ideas proposed in standard linear MDPs: the reference-advantage decomposition [54] and the variance-weighted ridge regression [65]. However, the unique challenges in DRMDPs necessitate novel treatments that go far beyond a combination of existing techniques. Specifically, existing analysis of standard linear MDPs highly relies on the linear dependency of the Bellman equation on the (nominal) transition kernel. This linear dependency is disrupted by the consideration of model uncertainty, which induces essential _nonlinearity_ that significantly complicates the statistical analysis of estimation error. To obtain our instance-dependent upper bounds, we establish a new theoretical analysis _pipeline_. This pipeline starts with a nontrivial decomposition of the suboptimality, and employs a new uncertainty decomposition that transforms the estimation uncertainty over all perturbed models to estimation uncertainty under the nominal model.

The information-theoretic lower bound in our paper is the first of its kind in the linear DRMDP setting, which could be of independent interest to the community. Previous lower bounds, which are based on the commonly used _Assouad's method_ and established under the standard linear MDP, do not consider model uncertainty. In particular, one prerequisite for applying Assouad's method is switching the initial minimax objective to a minimax risk in terms of Hamming distance. The intertwining of this prerequisite with the nonlinearity induced by the model uncertainty makes the analysis significantly more challenging. To this end, we construct a novel family of _hard instances_, carefully designed to (1) mitigate the nonlinearity caused by the model uncertainty, (2) fulfil the prerequisite for Assouad's method, and (3) be concise enough to admit matrix analysis.

Notations:We denote \(\Delta(\mathcal{S})\) as the set of probability measures over some set \(\mathcal{S}\). For any number \(H\in\mathbb{Z}_{+}\), we denote \([H]\) as the set of \(\{1,2,\cdots,H\}\). For any function \(V:\mathcal{S}\rightarrow\mathbb{R}\), we denote \([\mathbb{P}_{h}V](s,a)=\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}[V(s^{ \prime})]\) as the expectation of \(V\) with respect to the transition kernel \(P_{h}\), \([\text{Var}_{h}V](s,a)=[\mathbb{P}_{h}V^{2}](s,a)-([\mathbb{P}_{h}V](s,a))^{2}\) as the variance of \(V\), \([\mathbb{V}_{h}V](s,a)=\max\{1,[\text{Var}_{h}V](s,a)\}\) as the truncated variance of \(V\), and \([V(s)]_{\alpha}=\min\{V(s),\alpha\}\), given a scalar \(\alpha>0\), as the truncated value of \(V\). For a vector \(x\), we denote \(x_{j}\) as its \(j\)-th entry. And we denote \([x_{i}]_{i\in[d]}\) as a vector with the \(i\)-th entry being \(x_{i}\). For a matrix \(A\), denote \(\lambda_{i}(A)\) as the \(i\)-th eigenvalue of \(A\). For two matrices \(A\) and \(B\), we denote \(A\preceq B\) as the fact that \(B-A\) is a positive semidefinite matrix. For any function \(f:\mathcal{S}\rightarrow\mathbb{R}\), we denote \(\|f\|_{\infty}=\sup_{s\in\mathcal{S}}f(s)\). Given \(P,Q\in\Delta(\mathcal{S})\), the total variation divergence of \(P\) and \(Q\) is defined as \(D(P\|Q)=1/2\int_{\mathcal{S}}|P(s)-Q(s)|ds\).

## 2 Most Related Work

DRMDPs.The DRMDP framework has been extensively studied under different settings. The works of [55; 52; 61; 26; 12] assumed precise knowledge of the environment and formulated the DRMDP as classic planning problems. The works of [67; 57; 33; 56; 42; 58] assumed access to a generative model and studied the sample complexities of DRMDPs. The works of [35; 41; 4] studied the offline setting assuming access to only an offline dataset, and established sample complexities under data coverage or concentrability assumptions. The works of [51; 3; 8; 19; 20] studied the online setting where the agent can actively interact with the nominal environment to learn the robust policy.

\begin{table}
\begin{tabular}{c c c} \hline \hline Algorithm & Setting & Instance-dependent upper bound on the suboptimality gap \\ \hline PEVI [15] & MDP & \(dH\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\big{[}\|\boldsymbol{\phi}(s_{h},a _{h})\|_{\boldsymbol{\Lambda}_{h}^{-1}}|s_{1}=s\big{]}\) \\ LinPEVI-ADV [54] & MDP & \(\sqrt{d}H\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\big{[}\|\boldsymbol{\phi}(s_ {h},a_{h})\|_{\boldsymbol{\Lambda}_{h}^{-1}}|s_{1}=s\big{]}\) \\ LinPEVI-ADV+ [54] & MDP & \(\sqrt{d}\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\big{[}\|\boldsymbol{\phi}(s_ {h},a_{h})\|_{\boldsymbol{\Sigma}_{h}^{-1}}|s_{1}=s\big{]}\) \\ \hline DRPVI (ours) & DRMDP & \(\sqrt{d}H\cdot\sup_{P\in\mathcal{U}^{P}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{* },P}\big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{\boldsymbol{ \Lambda}_{h}^{-1}}|s_{1}=s\big{]}\) \\ VA-DRPVI (ours) & DRMDP & \(\sqrt{d}\cdot\sup_{P\in\mathcal{U}^{P}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{* },P}\big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{\boldsymbol{ \Sigma}_{h}^{-1}}|s_{1}=s\big{]}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of instance-dependent results in offline RL with linear function approximation. \(\boldsymbol{\Lambda}_{h}\) and \(\boldsymbol{\Sigma}_{h}^{*}\) are the empirical covariance matrix defined in (4.3) and (5.5) respectively. Note that \(\pi^{\star}\) means the optimal policy in standard MDPs and the optimal robust policy in DRMDPs. The definition of \(\boldsymbol{\Sigma}_{h}^{*}\) also depends on the corresponding definition of \(\pi^{\star}\).

DRMDPs with linear function approximation.Tamar et al. [44], Badrinath and Kalathil [3] proposed to use linear function approximation to solve DRMDPs with large state and action spaces and established asymptotic convergence guarantees. Zhou et al. [66] studied the natural Actor-Critic with function approximation, assuming access to a simulator. Their function approximation mechanisms depend on two novel uncertainty sets, one based on double sampling and the other on an integral probability metric. Ma et al. [24] first combined the linear MDP with the \(d\)-rectangular uncertainty set [12], and proposed the setting dubbed as the \(d\)-rectangular linear DRMDP, which naturally admits linear representations of the robust Q-functions3. Panaganti et al. [34] leverages the \(d\)-rectangular linear DRMDP framework to address the distribution shift problem in offline linear MDPs. Blanchet et al. [4] studied the offline \(d\)-rectangular linear DRMDP setting, for which the provable efficiency is established under a double pessimism principle. Liu and Xu [20] then studied the online \(d\)-rectangular linear DRMDP setting and pointed out that the intrinsic nonlinearity of DRMDPs might pose additional challenges for linear function approximation. After the release of our work, a concurrent study [48] emerged, which independently investigated offline DRMDPs with linear function approximation. Their algorithms attained the same instance-dependent suboptimalities as our proposed algorithms DRPVI and VA-DRPVI. Their algorithm DROP also achieved the same order of worst-case suboptimality, \(\tilde{O}(dH^{2}/\sqrt{K})\), as our DRPVI. However, we further demonstrated that our algorithm VA-DRPVI can strictly improve this result to \(\tilde{O}(dH\min\{1/\rho,H\}/\sqrt{K})\). Moreover, we introduced a novel hard instance and established the first information-theoretic lower bound for offline DRMDPs with linear function approximation. We also note that there is a line of works [4, 35] studied general function approximation under DRMDPs with the commonly studied \((s,a)\)-rectangularity uncertainty sets, where no further structure is applied except the rectangularity.

Footnote 3: Ma et al. [24] study the offline \(d\)-rectangular linear DRMDPs with Kullback-Leibler (KL) uncertainty sets. We remark that 1) the proofs of their main lemmas (Lemma D.1 and Lemma D.2) related to suboptimality decomposition and the proof of theorems have technique flaws; 2) The formulation of their assumption 4.4 on the dual variable of the dual formulation of the KL-divergence is ambiguous and may be too strong to be realistic. Thus, the fundamental challenges of \(d\)-rectangular linear DRMDPs remain unresolved.

## 3 Problem Formulation

In this section, we provide the preliminary of \(d\)-rectangular linear DRMDPs, and describe the dataset as well as the learning goal in offline reinforcement learning.

Standard MDPs.We start with the standard MDP, which constitutes the basic of DRMDPs. A finite horizon Markov decision process is denoted by MDP\((\mathcal{S},\mathcal{A},H,P,r)\), where \(\mathcal{S}\) and \(\mathcal{A}\) are the state and action spaces, \(H\in\mathbb{Z}_{+}\) is the horizon length, \(P=\{P_{h}\}_{h=1}^{H}\) denotes the set of probability transition kernels, \(r=\{r_{h}\}_{h=1}^{H}\) denotes the reward functions. More specifically, for any \((h,s,a)\in[H]\times\mathcal{S}\times\mathcal{A}\), the transition kernel \(P_{h}(\cdot|s,a)\) is a probability function over the state space \(\mathcal{S}\), and the reward function \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is assumed to be deterministic for simplicity. A sequence of deterministic policies is denoted as \(\pi=\{\pi_{h}\}_{h=1}^{H}\), where \(\pi_{h}:\mathcal{S}\rightarrow\mathcal{A}\) is the policy for step \(h\in[H]\). Given any policy \(\pi\) and transition \(P\), for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), the corresponding value function \(V_{h}^{\pi,P}(s):=\mathbb{E}^{\pi,P}\big{[}\sum_{t=h}^{H}r_{t}(s_{t},a_{t}) \big{|}s_{h}=s\big{]}\) and Q-function \(Q_{h}^{\pi,P}(s,a):=\mathbb{E}^{\pi,P}\big{[}\sum_{t=h}^{H}r_{t}(s_{t},a_{t}) \big{|}s_{h}=s,a_{h}=a\big{]}\) characterize the expected cumulative rewards starting from step \(h\), and both of them are bounded in \([0,H]\).

Distributionally robust MDPs.A finite horizon distributionally robust Markov decision process is denoted by DRMDP\((\mathcal{S},\mathcal{A},H,\mathcal{U}^{\rho}(P^{0}),r)\), where \(P^{0}=\{P_{h}^{0}\}_{h=1}^{H}\) is the set of nominal transition kernels, and \(\mathcal{U}^{\rho}(P^{0})=\bigotimes_{h\in[H]}\mathcal{U}_{h}^{\rho}(P_{h}^{0})\) is the uncertainty set of transitions, where each \(\mathcal{U}_{h}^{\rho}(P_{h}^{0})\) is usually defined as a ball centered at \(P^{0}\) with radius/uncertainty level \(\rho\geq 0\) based on some probability divergence measures [57, 13, 56]. To account for the model uncertainty, the robust value function \(V_{h}^{\pi,\rho}(s):=\inf_{P\in\mathcal{U}^{\rho}(P^{0})}V_{h}^{\pi,P}(s),\)\(\forall(h,s)\in[H]\times\mathcal{S}\) is defined as the value function under the worst possible transition kernel within the uncertainty set \(\mathcal{U}^{\rho}(P^{0})\). Similarly, the robust Q-function is defined as \(Q_{h}^{\pi,\rho}(s,a)=\inf_{P\in\mathcal{U}^{\rho}(P^{0})}Q_{h}^{\pi,P}(s,a)\), for any \((h,s,a)\in[H]\times\mathcal{S}\times\mathcal{A}\). Further, we define the optimal robust value function and the optimal robust Q-function as

\[V_{h}^{\ast,\rho}(s)=\sup_{\pi\in\Pi}V_{h}^{\pi,\rho}(s),\quad Q_{h}^{\star, \rho}(s,a)=\sup_{\pi\in\Pi}Q_{h}^{\pi,\rho}(s,a),\quad\forall(h,s,a)\in[H] \times\mathcal{S}\times\mathcal{A}.\]where \(\Pi\) is the set of all policies. The optimal robust policy \(\pi^{\star}=\{\pi^{\star}_{h}\}_{h=1}^{H}\) is defined as the policy that achieves the optimal robust value function: \(\pi^{\star}_{h}(s)=\arg\sup_{\pi\in\Pi}V^{\pm,\rho}_{h}(s)\), \(\forall(h,s)\in[H]\times\mathcal{S}\).

\(d\)-rectangular linear DRMDPs.A \(d\)-rectangular linear DRMDP is a DRMDP where the nominal environment is a special case of linear MDP with a simplex feature space [14, Example 2.2] and the uncertainty set \(\mathcal{U}^{\rho}_{h}(P^{0}_{h})\) is defined based on the linear structure of the nominal transition kernel \(P^{0}_{h}\). In particular, we make the following assumption about the nominal environment.

**Assumption 3.1**.: Let \(\bm{\phi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\) be a state-action feature mapping such that \(\sum_{i=1}^{d}\phi_{i}(s,a)=1\), \(\phi_{i}(s,a)\geq 0\), for any \((i,s,a)\in[d]\times\mathcal{S}\times\mathcal{A}\). For any \((h,s,a)\in[H]\times\mathcal{S}\times\mathcal{A}\), the reward function and the nominal transition kernels have a linear representation: \(r_{h}(s,a)=\langle\bm{\phi}(s,a),\bm{\theta}_{h}\rangle,\) and \(P^{0}_{h}(\cdot|s,a)=\langle\bm{\phi}(s,a),\bm{\mu}^{0}_{h}(\cdot)\rangle\), where \(\|\bm{\theta}_{h}\|_{2}\leq\sqrt{d}\), and \(\bm{\mu}^{0}_{h}=(\mu^{0}_{h,1},\ldots,\mu^{0}_{h,d})^{\top}\) are unknown probability measures over \(\mathcal{S}\).

With notations in Assumption 3.1, we define the factor uncertainty sets as \(\mathcal{U}^{\rho}_{h,i}(\mu^{0}_{h,i})=\big{\{}\mu:\mu\in\Delta(\mathcal{S}), D(\mu||\mu^{0}_{h,i})\leq\rho\big{\}},\forall(h,i)\in[H]\times[d]\), where \(D(\cdot||\cdot)\) is specified as the total variation (TV) divergence in this work. The uncertainty set is defined as \(\mathcal{U}^{\rho}_{h}(P^{0}_{h})=\bigotimes_{(s,a)\in\mathcal{S}\times \mathcal{A}}\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h})\), where \(\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h})=\sum_{i=1}^{d}\phi_{i}(s,a)\mu_{h,i}(\cdot):\mu_{h,i}(\cdot)\in\mathcal{U}^{\rho}_{h,i}(\mu^{0}_{h,i}),\forall i \in[d]\big{\}}\). A notable feature of this design is that the factor uncertainty sets \(\{\mathcal{U}^{\rho}_{h,i}(\mu^{0}_{h,i})\}_{h,i=1}^{H,d}\) are decoupled from the state-action pair \((s,a)\) and also independent with each other. As demonstrated later, this decoupling property results in a computationally efficient regime for function approximation.

Robust Bellman equation.Under the setting of \(d\)-rectangular linear DRMDPs, it is proved that the robust value function and the robust Q-function satisfy the robust Bellman equations [20]:

\[Q^{\pi,\rho}_{h}(s,a) =r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a; \bm{\mu}^{0}_{h})}[\mathbb{P}_{h}V^{\pi,\rho}_{h+1}](s,a),\] (3.1a) \[V^{\pi,\rho}_{h}(s) =\mathbb{E}_{a\sim\pi_{h}(\cdot|s)}\big{[}Q^{\pi,\rho}_{h}(s,a) \big{]},\] (3.1b)

and the optimal robust policy \(\pi^{\star}\) is deterministic. Thus, we can restrict the policy class \(\Pi\) to the deterministic one. This leads to the robust Bellman optimality equations:

\[Q^{\star,\rho}_{h}(s,a) =r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a; \bm{\mu}^{0}_{h})}[\mathbb{P}_{h}V^{\star,\rho}_{h+1}](s,a),\] (3.2a) \[V^{\star,\rho}_{h}(s) =\max_{a\in\mathcal{A}}Q^{\star}_{h}(s,a).\] (3.2b)

Offline Dataset and the Learning Goal.Let \(\mathcal{D}\) denote an offline dataset consisting of \(K\) i.i.d trajectories generated from the nominal environment MDP\((\mathcal{S},\mathcal{A},H,P^{0},r)\) by a behavior policy \(\pi^{b}=\{\pi^{b}_{h}\}_{h=1}^{H}\). In concrete, for each \(\tau\in[K]\), the trajectory \(\{(s^{\tau}_{h},a^{\tau}_{h},r^{\tau}_{h})\}_{h=1}^{H}\) satisfies that \(a^{\tau}_{h}\sim\pi^{b}_{h}(\cdot|s^{\tau}_{h})\), \(r^{\tau}_{h}=r_{h}(s^{\tau}_{h},a^{\tau}_{h})\), and \(s^{\tau}_{h+1}\sim P^{0}_{h}(\cdot|s^{\tau}_{h},a^{\tau}_{h})\) for any \(h\in[H]\). The goal of the robust offline RL is to learn the optimal robust policy \(\pi^{\star}\) using the offline dataset \(\mathcal{D}\). We define the suboptimality gap between any policy \(\hat{\pi}\) and the optimal robust policy \(\pi^{\star}\) as

\[\text{SubOpt}(\hat{\pi},s_{1},\rho):=V^{\star,\rho}_{1}(s_{1})-V^{\hat{\pi}, \rho}_{1}(s_{1}).\] (3.3)

Then the goal of an algorithm in distributionally robust offline reinforcement learning is to learn a robust policy \(\hat{\pi}\) that minimizes the suboptimality gap SubOpt\((\hat{\pi},s,\rho)\), for any \(s\in\mathcal{S}\).

## 4 Warmup: Robust Pessimistic Value Iteration

In this section, we first propose a simple algorithm in Algorithm 1 as a warm start, and provide an instance-dependent upper bound on its suboptimality gap in Theorem 4.4.

The optimal robust Bellman equation (3.2) implies that the optimal robust policy \(\pi^{\star}\) is greedy with respect to the optimal robust Q-function. Therefore, it suffices to estimate \(Q^{\star,\rho}_{h}\) to approximate \(\pi^{\star}\). To this end, we estimate the optimal robust Q-function by iteratively performing an empirical version of the optimal robust Bellman equation similar to (3.2). In concrete, given the estimators at step \(h+1\), denoted by \(\widehat{Q}_{h+1}(s,a)\) and \(\widehat{V}_{h+1}(s)=\max_{a\in\mathcal{A}}\widehat{Q}_{h+1}(s,a)\), Liu and Xu [20] show that applying one step backward induction similar to (3.2) leads to

\[Q_{h}(s,a)=r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{ \mu}^{0}_{h})}\big{[}\mathbb{P}_{h}\widehat{V}_{h+1}\big{]}(s,a)=\big{\langle} \bm{\phi}(s,a),\bm{\theta}_{h}+\bm{\nu}^{\rho}_{h}\big{\rangle},\] (4.1)

[MISSING_PAGE_FAIL:6]

Before presenting the theoretical guarantee of DRPVI, we make the following data coverage assumption, which is standard for offline linear MDPs [50; 9; 60; 54].

**Assumption 4.3**.: We assume \(\kappa:=\min_{h\in[H]}\lambda_{\min}(\mathbb{E}^{\pi^{b},P^{0}}[\bm{\phi}(s_{h},a_{h})\bm{\phi}(s_{h},a_{h})^{\top}])>0\) for the behavior policy \(\pi^{b}\) and the nominal transition kernel \(P^{0}\).

Assumption 4.3 requires the behavior policy to sufficiently explore the state-action space under the nominal environment. Indeed, it implicitly assumes that the nominal and perturbed environments share the same state-action space, and that the full information of this space is accessible through the nominal environment and the behavior policy \(\pi^{b}\). Assumption 4.3 rules out cases where new states emerge in perturbed environments that can never be queried under the nominal environment as a result of the distribution shift. Now we present the theoretical guarantee for Algorithm 1.

**Theorem 4.4**.: Under Assumptions 3.1 and 4.3, \(\forall K>\max\{512\log(2dH^{2}/\delta)/\kappa^{2},20449d^{2}H^{2}/\kappa\}\) and \(\delta\in(0,1)\), if we set \(\lambda=1\) and \(\beta_{1}=\tilde{O}(\sqrt{d}H)\) in Algorithm 1, then with probability at least \(1-\delta\), \(\forall s\in\mathcal{S}\), the suboptimality of DRPVI satisfies

\[\text{SubOpt}(\hat{\pi},s,\rho)\leq\beta_{1}\cdot\sup_{P\in\mathcal{U}^{\rho }(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\bigg{[}\sum_{i=1}^{d}\|\phi_{i}(s _{h},a_{h})\mathbf{1}_{i}\|_{\bm{\Lambda}_{h}^{-1}}|s_{1}=s\bigg{]},\] (4.6)

where \(\bm{\Lambda}_{h}\) is the empirical covariance matrix defined in (4.3).

The result in Theorem 4.4 resembles existing instance-dependent bounds for standard linear MDPs [15; 54] (see Table 1 for a detailed comparison). However, there are two major distinctions between these results. First, our result depends on the weighted sum of diagonal elements \(\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{\bm{\Lambda}_{h}^{-1}}\), dubbed as the \(d\)-_rectangular robust estimation error_, instead of the Mahalanobis norm of the feature vector \(\|\bm{\phi}(s_{h},a_{h})\|_{\bm{\Lambda}_{h}^{-1}}\). As discussed in Remark 4.1, this term primarily arises due to the necessity to solve \(d\) distinct ridge regressions in each step, which presents a unique challenge in our analysis. Second, we consider the _supremum expectation_ of \(d\)-rectangular robust estimation error with respect to all transition kernels in the uncertainty set, which measures the _worst case coverage_ of the covariance matrix \(\bm{\Lambda}_{h}\) under the optimal robust policy \(\pi^{*}\).

To connect with existing literature [4], we further show that under Assumption 4.3, the instance-dependent suboptimality bound can be simplified as follows.

**Corollary 4.5**.: Under the same assumptions and settings as Theorem 4.4, with probability at least \(1-\delta\), for all \(s\in\mathcal{S}\), the suboptimality of DRPVI satisfies \(\text{SubOpt}(\hat{\pi},s,\rho)=\tilde{O}(\sqrt{d}H^{2}/(\sqrt{\kappa\cdot K}))\).

**Remark 4.6**.: Since \(\|\bm{\phi}(\cdot,\cdot)\|_{2}\leq 1\) by Assumption 3.1, the coverage parameter \(\kappa\) is trivially upper bounded by \(1/d\). Assuming that \(\kappa=c^{\dagger}/d\) for a constant \(0<c^{\dagger}<1\), then we have \(\text{SubOpt}(\hat{\pi},s,\rho)=\tilde{O}(dH^{2}/(c^{\dagger}\cdot\sqrt{K}))\). This bound improves the state-of-the-art, (4, Theorem 6.3), by \(O(d)\).

## 5 Distributionally Robust Variance-Aware Pessimistic Value Iteration

The instance-dependent bound in Theorem 4.4 has an explicit dependency on \(H\), which arises from the fact that \(Q_{h}^{\rho}(s,a)\in[0,H]\) for any \((h,\rho)\in[H]\times(0,1]\) and the Hoeffding-type self-normalized concentration inequality used in our analysis. We will show in this section that the range of any robust value function could be much smaller under a refined analysis. Consequently, we can leverage variance information to improve Algorithm 1 and achieve a strengthened upper bound.

IntuitionIn the robust Bellman equation (3.1), the worst-case transition kernel would put as much mass as possible on the minimizer of \(V_{h+1}^{\pi,\rho}(s)\), denoted by \(s_{\min}\). Based on this observation, we conjecture that the robust Bellman equation (3.1) recursively reduces the maximal value of robust value functions, and thus shrinks its range. To see this, we define \(\hat{\mu}_{h,i}=(1-\rho)\mu_{h,i}^{0}+\rho\delta_{s_{\min}}\), where \(\delta_{s_{\min}}\) is the Dirac measure at \(s_{\min}\), and we assume \(V_{h+1}^{\pi,\rho}(s_{\min})=0\) for any \((\pi,h)\in\Pi\times[H]\) just for illustration. It is easy to verify that \(\hat{\mu}_{h,i}\in\mathcal{U}_{h,i}^{\rho}(\mu_{h,i}^{0})\) and is indeed the worst-case factor kernel. Then by (3.1) we have \(V_{h}^{\pi,\rho}(s)=\mathbb{E}_{a\sim\pi}[r_{h}(s,a)+(1-\rho)[\mathbb{P}_{h}^{0} V_{h+1}^{\pi,\rho}](s,a)]\), which immediately implies \(\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)\leq 1+(1-\rho)\max_{s^{\prime}\in \mathcal{S}}V_{h+1}^{\pi,\rho}(s^{\prime})\). This justifies our conjecture that the range of the robust value functions shrinks over stage. We dub this phenomenon as _Range Shrinkage_ and summarize it in the following lemma, with a more formal proof postponed to Appendix G.5.

**Lemma 5.1** (Range Shrinkage).: For any \((\rho,\pi,h)\in(0,1]\times\Pi\times[H]\), we have

\[\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)-\min_{s\in\mathcal{S}}V_{h}^{\pi,\rho} (s)\leq\frac{1-(1-\rho)^{H-h+1}}{\rho}.\] (5.1)

This phenomenon only appears in DRMDPs since the range of value function is generally \([0,H]\) in standard MDPs. A similar phenomenon is first observed in infinite horizon tabular DRMDPs [42, Lemma 7]. One important implication of Lemma 5.1 is that the conditional variance of any value function shrinks accordingly. In particular, when \(\rho=O(1)\), the range of any robust value function would shrink to constant order, which leads to constant order conditional variances. This motivates us to leverage the variance information in both algorithm design and theoretical analysis. Inspired by the variance-weighted ridge regression in standard linear MDPs [65, 27, 60, 54], we propose to improve the vanilla ridge regression in (4.2) by incorporating variance weights. To this end, we first propose an appropriate variance estimator, whose form is specifically motivated by our theoretical analysis framework, to quantify the variance information.

Variance estimationWe first run Algorithm 1 using an offline dataset \(\mathcal{D}^{\prime}\) that is independent of \(\mathcal{D}\) to obtain estimators of the optimal robust value functions \(\{\widehat{V}_{h}^{{}^{\prime}\rho}\}_{h\in[H]}\). By Assumption 3.1, the variance of \([\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}\) under the nominal environment is \([\text{Var}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}](s,a)=[\mathbb{ P}_{h}^{0}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}](s,a)-([\mathbb{P}_{h}^{0}[ \widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}](s,a))^{2}=\langle\bm{\phi}(s,a),\bm{z}_{h,2}\rangle-(\langle\bm{\phi}(s,a),\bm{z}_{h,1}\rangle)^{2}\). We estimate \(\bm{z}_{h,1}\) and \(\bm{z}_{h,2}\) via ridge regression similarly as in (4.2):

\[\tilde{\bm{z}}_{h,2}(\alpha) =\operatorname*{argmin}_{\bm{z}\in\mathbb{R}^{d}}\sum_{\tau=1}^{ K}\big{(}[\widehat{V}_{h+1}^{{}^{\prime}\rho}(s_{h+1}^{\tau})]_{\alpha}^{2}- \bm{\phi}_{h}^{\tau\top}\bm{z}\big{)}^{2}+\lambda\|\bm{z}\|_{2}^{2},\] (5.2a) \[\tilde{\bm{z}}_{h,1}(\alpha) =\operatorname*{argmin}_{\bm{z}\in\mathbb{R}^{d}}\sum_{\tau=1}^{ K}\big{(}[\widehat{V}_{h+1}^{{}^{\prime}\rho}(s_{h+1}^{\tau})]_{\alpha}-\bm{\phi}_{h} ^{\tau\top}\bm{z}\big{)}^{2}+\lambda\|\bm{z}\|_{2}^{2}.\] (5.2b)

We then construct the following truncated variance estimator

\[\widehat{\sigma}_{h}^{2}(s,a;\alpha):=\max\bigg{\{}1,\big{[}\bm{\phi}(s,a)^{ \top}\tilde{\bm{z}}_{h,2}(\alpha)\big{]}_{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{ \top}\tilde{\bm{z}}_{h,1}(\alpha)\big{]}_{[0,H]}^{2}-\tilde{O}\bigg{(}\frac{ dH^{3}}{\sqrt{K\kappa}}\bigg{)}\bigg{\}},\] (5.3)

where the last term is a penalty to achieve pessimistic estimations of conditional variances.

Variance-Aware Function Approximation MechanismSimilar to the two-step estimation procedure of Algorithm 1, we first estimate \(\bm{z}_{h}(\alpha)\) by the following variance-weighted ridge regression under the nominal environment:

\[\hat{\bm{z}}_{h}(\alpha) =\operatorname*{argmin}_{\bm{z}\in\mathbb{R}^{d}}\sum_{\tau=1}^{ K}\frac{\big{(}[\widehat{V}_{h+1}^{\rho}(s_{h+1}^{\tau})]_{\alpha}-\bm{\phi}_{h} ^{\top\bm{z}}\bm{z}\big{)}^{2}}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{ \tau};\alpha)}+\lambda\|\bm{z}\|_{2}^{2}\] \[=\bm{\Sigma}_{h}^{-1}(\alpha)\bigg{[}\sum_{\tau=1}^{K}\frac{\bm{ \phi}_{h}^{\tau}[\widehat{V}_{h+1}^{\rho}(s_{h+1}^{\tau})]_{\alpha}}{ \widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha)}\bigg{]},\] (5.4)where \(\bm{\Sigma}_{h}(\alpha)=\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\bm{\phi}_{h}^{\tau \top}/\tilde{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha)+\lambda\bm{\mathrm{ I}}\) is the empirical variance-weighted covariance matrix, which can be deemed as an estimator of the following variance-weighted covariance matrix

\[\bm{\Sigma}_{h}^{\star}=\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\bm{\phi}_{h}^{\tau \top}/[\mathbb{V}_{h}V_{h+1}^{\star,\rho}](s_{h}^{\tau},a_{h}^{\tau})+\lambda \bm{\mathrm{I}}.\] (5.5)

In the second step, we estimate \(\nu_{h,i}^{\rho},\forall i\in[d]\) in the same way as (4.4). We then add a pessimism penalty based on \(\bm{\Sigma}_{h}(\alpha)\). We present the full algorithm details in Algorithm 2.

**Theorem 5.2**.: Under Assumptions 3.1 and 4.3, for \(K>\max\{\tilde{O}(d^{2}H^{6}/\kappa),\tilde{O}(H^{4}/\kappa^{2})\}\) and \(\delta\in(0,1)\), if we set \(\lambda=1/H^{2}\) and \(\beta_{2}=\tilde{O}(\sqrt{d})\) in Algorithm 2, then with probability at least \(1-\delta\), the suboptimality of VA-DRPVI satisfies

\[\text{SubOpt}(\hat{\pi},s,\rho)\leq\beta_{2}\cdot\sup_{P\in\mathcal{U}^{\rho }(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{\star},P}\bigg{[}\sum_{i=1}^{d}\|\phi_{ i}(s_{h},a_{h})\bm{\mathrm{1}}_{i}\|_{\bm{\Sigma}_{h}^{\star-1}}\big{|}s_{1}=s \bigg{]},\] (5.6)

where \(\bm{\Sigma}_{h}^{\star}\) is the population variance-weighted covariance matrix defined as in (5.5).

Note that the bound in Theorem 5.2 does not explicitly depend on \(H\) anymore compared with that in Theorem 4.4. A naive observation is that \([\mathbb{V}_{h}V_{h+1}^{\star,\rho}](s,a)\in[1,H^{2}]\). By comparing the definitions in (4.3) and (5.5), we have \(\bm{\Sigma}_{h}^{\star-1}\preceq H^{2}\bm{\Lambda}_{h}^{-1}\). Thus the upper bound of Algorithm 2 is never worse than that of Algorithm 1. This improvement brought by variance information is similar to that in standard linear MDPs [54, Theorem 2]. However, thanks to the range shrinkage phenomenon, we can further show that VA-DRPVI is strictly better than DRPVI when the uncertainty level is of constant order.

**Corollary 5.3**.: Under the same assumptions and settings as Theorem 5.2, given the uncertainty level \(\rho\), we have with probability at least \(1-\delta\), for all \(s\in\mathcal{S}\), the suboptimality of VA-DRPVI satisfies

\[\text{SubOpt}(\hat{\pi},s,\rho)\leq\beta_{2}\cdot\frac{(1-(1-\rho)^{H})}{\rho }\cdot\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{\star },P}\bigg{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\bm{\mathrm{1}}_{i}\|_{\bm{ \Lambda}_{h}^{-1}}|s_{1}=s\bigg{]}.\]

**Remark 5.4**.: Note that \((1-(1-\rho)^{H})/\rho=\bm{\Theta}(\min\{1/\rho,H\})\). When \(\rho=O(1)\), the suboptimality of Algorithm 2 is strictly smaller than that of Algorithm 1 by \(H\). With a similar argument as in Remark 4.6, if we assume there exist a constant \(0<c^{\dagger}<1\), such that \(\kappa=c^{\dagger}/d\) in Assumption 4.3, then the instance-dependent upper bound can be simplified to \(\tilde{O}(dH\min\{1/\rho,H\}/(c^{\dagger}\cdot\sqrt{K}))\), which improves the state-of-the-art [4, Theorem 6.3] by \(O(dH)\) when \(\rho=O(1)\).

## 6 Information-Theoretic Lower Bound

For a matrix \(\bm{\mathrm{A}}\in\mathbb{R}^{d\times d}\) and a state \(s\in\mathcal{S}\), we define function \(\Phi(\cdot,\cdot):\mathbb{R}^{d\times d}\times\mathcal{S}\to\mathbb{R}\) as follows.

\[\Phi(\bm{\mathrm{A}},s)=\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H} \mathbb{E}^{\pi^{\star},P}\bigg{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\bm{ \mathrm{1}}_{i}\|_{\bm{\mathrm{A}}}\big{|}s_{1}=s\bigg{]}.\] (6.1)

It can be seen our upper bounds in previous sections primarily depend on quantities such as \(\Phi(\bm{\Lambda}_{h}^{-1},s)\) and \(\Phi(\bm{\Sigma}_{h}^{\star-1},s)\). Roughly speaking, these quantities characterize the discrepancy between the (weighted) covariance matrix of the offline dataset and the state action pairs generated from the transition probability in the uncertainty set. Hence we call \(\Phi(\cdot,\cdot)\) the uncertainty function.

We now establish an information-theoretic lower bound to show that the uncertainty function is unavoidable for \(d\)-rectangular linear DRMDPs. Let \(\mathcal{M}\) be a class of DRMDPs and we define \(\text{SubOpt}(M,\hat{\pi},s,\rho)\) as the suboptimality gap specific to one DRMDP instance \(M\in\mathcal{M}\).

**Theorem 6.1**.: Given uncertainty level \(\rho\in(0,3/4]\), dimension \(d\), horizon length \(H\) and sample size \(K>\tilde{O}(d^{6})\), there exists a class of \(d\)-rectangular linear DRMDPs \(\mathcal{M}\) and an offline dataset \(\mathcal{D}\) of size \(K\) such that for all \(s\in\mathcal{S}\), with probability at least \(1-\delta\), \(\inf_{\hat{\pi}}\sup_{M\in\mathcal{M}}\text{SubOpt}(M,\hat{\pi},s,\rho)\geq c \cdot\Phi(\bm{\Sigma}_{h}^{\star-1},s)\), where \(c\) is a universal constant.

Theorem 6.1 shows that the uncertainty function \(\Phi(\bm{\Sigma}_{h}^{\star-1},s)\) is intrinsic to the information-theoretic lower bound, and thus is inevitable. It is noteworthy that the lower bound in Theorem 6.1 aligns with the upper bound in Theorem 5.2 up to a factor of \(\beta_{2}\), which implies that VA-DRPVI is minimaxoptimal in the sense of information theory, but with a small gap of \(\tilde{O}(\sqrt{d})\). Consequently, we affirm that, in the context of robust offline reinforcement learning with function approximation, both the computational efficiency and minimax optimality are achievable under the setting of \(d\)-rectangular linear DRMDPs with TV uncertainty sets. Moreover, Theorem 6.1 also suggests that achieving a good robust policy necessitates the worst case coverage of the offline dataset over the entire uncertainty set of transition models, which is significantly different from standard linear MDPs where a good coverage under the nominal model is enough [15; 60; 54]. Such a distinction indicates that learning in linear DRMDPs may be more challenging in comparison to standard linear MDPs.

Further, we highlight that the hard instances we constructed also satisfy Assumption 4.3. It remains an interesting direction to explore what would happen if the nominal and perturbed environments don't share exactly the same state-action space. We conjecture that since there could be absolutely new states emerging in perturbed environments that can never be explored in the nominal environment, the policy learned merely using data collected from the nominal environment could be arbitrarily bad.

Challenges and novelties in construction of hard instancesExisting tight lower bound analysis in standard linear MDPs [62; 60; 54] generally depends on the Assouad's method and a family of hard instances indexed by \(\bm{\xi}\in\{-1,1\}^{d\tilde{H}}\). However, they do not consider model uncertainty, which largely hinders the derivation of explicit formulas for the robust value functions. Further, one prerequisite of the Assouad's method is switching the initial minimax suboptimality \(\inf_{\#}\max_{M\in\mathcal{M}}\text{SubOpt}(\hat{\pi},s,\rho)\) to a risk of the form \(\inf_{\bm{\xi}^{\prime}}\max_{\bm{\xi}}D_{H}(\bm{\xi},\bm{\xi}^{\prime})\), where \(D_{H}(\cdot,\cdot)\) is the Hamming distance. The model uncertainty significantly complicates this procedure, as the nonlinearity involved disrupts the linear dependency between the value function and the index \(\bm{\xi}\). At the core of Theorem 6.1 is a novel class of hard instances \(\mathcal{M}\). At a high-level, the hard instances should (1) fulfill the \(d\)-rectangular linear DRMDP conditions, (2) mitigate the nonlinearity caused by model uncertainty, (3) achieve the prerequisite for Assouad's method, and (4) be concise enough to admit matrix analysis. We postpone details on the construction of hard instances and the proof of Theorem 6.1 to Appendix F.

As a side product of Theorem 6.1, we show in the following corollary an information-theoretic lower bound in terms of the instance-dependent uncertainty function \(\Phi(\bm{\Lambda}_{h}^{-1},s)\) in Theorem 4.4.

**Corollary 6.2**.: Under the same setting in Theorem 6.1, the class of hard instances \(\mathcal{M}\) and offline dataset \(\mathcal{D}\) in Theorem 6.1 also suggests that, with probability at least \(1-\delta\), \(\inf_{\hat{\pi}}\sup_{M\in\mathcal{M}}\text{SubOpt}(\hat{\pi},s,\rho)\geq c \cdot\Phi(\bm{\Lambda}_{h}^{-1},s)\), where \(c\) is a universal constant.

This implies that the uncertainty function \(\Phi(\bm{\Lambda}_{h}^{-1},s)\) in Theorem 4.4 also arises from the information-theoretic lower bound. We note the lower bound in Corollary 6.2 matches the upper bound in Theorem 4.4 up to \(\beta_{1}\), thus DRPVI is also minimax optimal in the sense of information theory, but with a larger gap of \(\tilde{O}(\sqrt{d}H)\). Moreover, the only difference between Theorem 6.1 and Corollary 6.2 is the covariance matrix. Due to the fact that \(\bm{\Lambda}_{h}^{-1}\preceq\bm{\Sigma}_{h}^{\star,-1}\), the information-theoretic lower bound in Theorem 6.1 is indeed tighter than that in Corollary 6.2.

## 7 Conclusions

We studied robust offline RL with function approximation under the setting of \(d\)-rectangular linear DRMDPs with TV uncertainty sets. We first proposed the DRPVI algorithm and built up a theoretical analysis pipeline to establish the first instance-dependent upper bound on the suboptimality gap in the context of robust offline RL. We then showed an interesting range shrinkage phenomenon specific to DRMDPs, and we proposed the VA-DRPVI algorithm, which leverages the conditional variance information of the optimal robust value function. Based on the analysis pipeline built above, we show that the upper bound of VA-DRPVI achieves sharp dependence on the horizon length \(H\). In addition, we found that an uncertainty function consisting of two crucial quantities-a supremum over uncertainty set and a diagonal-based normalization-appears in all upper bounds. We further established an information-theoretic lower bound to prove that the uncertainty function is unavoidable for robust offline RL under the setting of \(d\)-rectangular linear DRMDPs.

It remains an interesting future research question whether the computational and provable efficiency can be achieved in other settings for robust offline RL with function approximation. Another interesting future direction is to explore the unique challenges of applying general function approximation techniques in standard offline RL [6] to DRMDPs.

## Acknowledgments

We would like to thank the anonymous reviewers for their helpful comments. ZL and PX was supported in part by the National Science Foundation (DMS-2323112) and the Whitehead Scholars Program at the Duke University School of Medicine. The views and conclusions in this paper are those of the authors and should not be interpreted as representing any funding agency.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011. (p. 46.)
* [2] Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Provable benefits of representational transfer in reinforcement learning. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2114-2187. PMLR, 2023. (p. 16.)
* [3] Kishan Panaganti Badrinath and Dileep Kalathil. Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In _International Conference on Machine Learning_, pages 511-520. PMLR, 2021. (pp. 2, 3, and 4.)
* [4] Jose Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. _arXiv preprint arXiv:2305.09659_, 2023. (pp. 2, 3, 4, 7, and 9.)
* [5] Avinandan Bose, Simon Shaolei Du, and Maryam Fazel. Offline multi-task transfer rl with representational penalization. _arXiv preprint arXiv:2402.12570_, 2024. (p. 16.)
* [6] Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051. PMLR, 2019. (p. 10.)
* [7] Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang. Provable benefit of multitask representation learning in reinforcement learning. _Advances in Neural Information Processing Systems_, 35:31741-31754, 2022. (p. 16.)
* [8] Jing Dong, Jingwei Li, Baoxiang Wang, and Jingzhao Zhang. Online policy optimization for robust mdp. _arXiv preprint arXiv:2209.13841_, 2022. (p. 3.)
* [9] Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approximation. In _International Conference on Machine Learning_, pages 2701-2709. PMLR, 2020. (p. 7.)
* [10] Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in dqn. _arXiv preprint arXiv:1810.00123_, 2018. (p. 1.)
* [11] Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. _Nature medicine_, 25(1):16-18, 2019. (p. 1.)
* [12] Vineet Goyal and Julien Grand-Clement. Robust markov decision processes: Beyond rectangularity. _Mathematics of Operations Research_, 48(1):203-226, 2023. (pp. 3 and 4.)
* [13] Garud N Iyengar. Robust dynamic programming. _Mathematics of Operations Research_, 30(2):257-280, 2005. (pp. 1 and 4.)
* [14] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020. (pp. 5 and 43.)
* [15] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021. (pp. 1, 2, 3, 6, 7, 10, 16, 17, 27, and 28.)* [16] Yufei Kuang, Miao Lu, Jie Wang, Qi Zhou, Bin Li, and Houqiang Li. Learning robust policy against disturbance in transition dynamics via state-conservative policy optimization. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7247-7254, 2022. (p. 1.)
* [17] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In _Reinforcement learning: State-of-the-art_, pages 45-73. Springer, 2012. (p. 1.)
* [18] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020. (p. 1.)
* [19] Zhipeng Liang, Xiaoteng Ma, Jose Blanchet, Jun Yang, Jiheng Zhang, and Zhengyuan Zhou. Single-trajectory distributionally robust reinforcement learning. In _Forty-first International Conference on Machine Learning_. (p. 3.)
* [20] Zhishuai Liu and Pan Xu. Distributionally robust off-dynamics reinforcement learning: Provable efficiency with linear function approximation. In _International Conference on Artificial Intelligence and Statistics_, pages 2719-2727. PMLR, 2024. (pp. 2, 3, 4, 5, 6, 17, and 43.)
* [21] Zhishuai Liu, Zishu Zhan, Cunjie Lin, and Baqun Zhang. Estimation in optimal treatment regimes based on mean residual lifetimes with right-censored data. _Biometrical Journal_, 65(8):2200340, 2023. (p. 1.)
* [22] Zhishuai Liu, Zishu Zhan, Jian Liu, Danhui Yi, Cunjie Lin, and Yufei Yang. On estimation of optimal dynamic treatment regimes with multiple treatments for survival data-with application to colorectal cancer study. _arXiv preprint arXiv:2310.05049_, 2023. (p. 1.)
* [23] Rui Lu, Andrew Zhao, Simon S Du, and Gao Huang. Provable general function class representation learning in multitask bandits and mdp. _Advances in Neural Information Processing Systems_, 35:11507-11519, 2022. (p. 16.)
* [24] Xiaoteng Ma, Zhipeng Liang, Li Xia, Jiheng Zhang, Jose Blanchet, Mingwen Liu, Qianchuan Zhao, and Zhengyuan Zhou. Distributionally robust offline reinforcement learning with linear function approximation. _arXiv preprint arXiv:2209.06620_, 2022. (pp. 4 and 6.)
* [25] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust policy learning: Active construction of physically-plausible perturbations. In _2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3932-3939. IEEE, 2017. (p. 1.)
* [26] Shie Mannor, Ofir Mebel, and Huan Xu. Robust mdps with k-rectangular uncertainty. _Mathematics of Operations Research_, 41(4):1484-1509, 2016. (p. 3.)
* [27] Yifei Min, Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Variance-aware off-policy evaluation with linear function approximation. _Advances in neural information processing systems_, 34:7598-7610, 2021. (pp. 8 and 46.)
* [28] Jun Morimoto and Kenji Doya. Robust reinforcement learning. _Neural computation_, 17(2):335-359, 2005. (p. 1.)
* [29] John A Nelder and Roger Mead. A simplex method for function minimization. _The computer journal_, 7(4):308-313, 1965. (p. 6.)
* [30] Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition matrices. _Operations Research_, 53(5):780-798, 2005. (p. 1.)
* [31] Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krahenbuhl, Vladlen Koltun, and Dawn Song. Assessing generalization in deep reinforcement learning. _arXiv preprint arXiv:1810.12282_, 2018. (p. 1.)
* [32] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntak Lee, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Agile autonomous driving using end-to-end deep imitation learning. In _Robotics: science and systems_, 2018. (p. 1.)* Panaganti and Kalathil [2022] Kishan Panaganti and Dileep Kalathil. Sample complexity of robust reinforcement learning with a generative model. In _International Conference on Artificial Intelligence and Statistics_, pages 9582-9602. PMLR, 2022. (p. 3.)
* Panaganti et al. [2022] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Bridging distributionally robust learning and offline rl: An approach to mitigate distribution shift and partial data coverage. In _ICML 2024 Workshop: Foundations of Reinforcement Learning and Control-Connections and Perspectives_. (p. 4.)
* Panaganti et al. [2022] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust reinforcement learning using offline data. _Advances in neural information processing systems_, 35:32211-32224, 2022. (p. 1, 2, 3, and 4.)
* Pattanaik et al. [2018] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks. In _Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems_, pages 2040-2042, 2018. (p. 1.)
* Pinto et al. [2017] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In _International Conference on Machine Learning_, pages 2817-2826. PMLR, 2017. (p. 1.)
* Roy et al. [2017] Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch. _Advances in neural information processing systems_, 30, 2017. (p. 2.)
* Satia and Lave Jr [1973] Jay K Satia and Roy E Lave Jr. Markovian decision processes with uncertain transition probabilities. _Operations Research_, 21(3):728-740, 1973. (p. 1.)
* Shen et al. [2024] Yi Shen, Pan Xu, and Michael Zavlanos. Wasserstein distributionally robust policy evaluation and learning for contextual bandits. _Transactions on Machine Learning Research_, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=NmpjDHWlvg. Featured Certification. (p. 1.)
* Shi and Chi [2024] Laixi Shi and Yuejie Chi. Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. _Journal of Machine Learning Research_, 25(200):1-91, 2024. (p. 1, 2, 3, and 6.)
* Shi et al. [2024] Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, and Yuejie Chi. The curious price of distributional robustness in reinforcement learning with a generative model. _Advances in Neural Information Processing Systems_, 36, 2024. (p. 3, 8, and 40.)
* Sun et al. [2020] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2446-2454, 2020. (p. 1.)
* Tamar et al. [2014] Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation. In _International conference on machine learning_, pages 181-189. PMLR, 2014. (pp. 2 and 4.)
* Tessler et al. [2019] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In _International Conference on Machine Learning_, pages 6215-6224. PMLR, 2019. (p. 1.)
* Tsybakov [2009] Alexandre B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer, New York, 2009. (p. 37.)
* Vershynin [2018] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018. (p. 28.)
* Wang et al. [2024] He Wang, Laixi Shi, and Yuejie Chi. Sample complexity of offline distributionally robust linear markov decision processes. _arXiv preprint arXiv:2403.12946_, 2024. (p. 4.)* [49] Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2447-2456, 2018.
* [50] Ruosong Wang, Dean Foster, and Sham M Kakade. What are the statistical limits of offline rl with linear function approximation? In _International Conference on Learning Representations_, 2021.
* [51] Yue Wang and Shaofeng Zou. Online robust reinforcement learning with model uncertainty. _Advances in Neural Information Processing Systems_, 34:7193-7206, 2021.
* [52] Wolfram Wiesemann, Daniel Kuhn, and Berc Rustem. Robust markov decision processes. _Mathematics of Operations Research_, 38(1):153-183, 2013.
* [53] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694, 2021.
* [54] Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang. Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game. In _International Conference on Learning Representations (ICLR)_, 2023.
* [55] Huan Xu and Shie Mannor. The robustness-performance tradeoff in markov decision processes. _Advances in Neural Information Processing Systems_, 19, 2006.
* [56] Zaiyan Xu, Kishan Panaganti, and Dileep Kalathil. Improved sample complexity bounds for distributionally robust reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 9728-9754. PMLR, 2023.
* [57] Wenhao Yang, Liangyu Zhang, and Zhihua Zhang. Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics. _The Annals of Statistics_, 50(6):3223-3248, 2022.
* [58] Wenhao Yang, Han Wang, Tadashi Kozuno, Scott M Jordan, and Zhihua Zhang. Robust markov decision processes without model estimation. _arXiv preprint arXiv:2302.01248_, 2023.
* [59] Zhouhao Yang, Yihong Guo, Pan Xu, Anqi Liu, and Animashree Anandkumar. Distributionally robust policy gradient for offline contextual bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 6443-6462. PMLR, 2023.
* [60] Ming Yin, Yaqi Duan, Mengdi Wang, and Yu-Xiang Wang. Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. _arXiv preprint arXiv:2203.05804_, 2022.
* [61] Pengqian Yu and Huan Xu. Distributionally robust counterpart in markov decision processes. _IEEE Transactions on Automatic Control_, 61(9):2538-2543, 2015.
* [62] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_, 34:13626-13640, 2021.
* [63] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. _Advances in Neural Information Processing Systems_, 33:21024-21037, 2020.
* [64] Wenshuai Zhao, Jorge Pena Queralta, and Tomi Westerlund. Sim-to-real transfer in deep reinforcement learning for robotics: a survey. In _2020 IEEE symposium series on computational intelligence (SSCI)_, pages 737-744. IEEE, 2020.
* [65] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.
** [66] Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, PR Kumar, and Chao Tian. Natural actor-critic for robust reinforcement learning with function approximation. _Advances in neural information processing systems_, 36, 2024. (pp. 2 and 4.)
* [67] Zhengqing Zhou, Zhengyuan Zhou, Qinxun Bai, Linhai Qiu, Jose Blanchet, and Peter Glynn. Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3331-3339. PMLR, 2021. (p. 3.)Additional Related Work

Offline Linear MDPs.Our work focuses on the offline linear MDP setting where the nominal transition kernel, from which the offline dataset is collected, admits the linear MDP structure. Numerous works have studied the provable efficiency and statistical limits of algorithms under this setting [15, 62, 53, 60, 54]. The most relevant study to ours is the recent work of [54], which established the minimax optimality of offline linear MDPs. At the core of their analysis is an advantage-reference technique designed for offline RL under linear function approximation, together with a variance aware pessimism-based algorithm. However, the offline linear MDP setting still remains understudied in the context of DRMDPs.

Transfer-Learning in Low Rank MDPs.Besides the distributionally robust perspective to solve the planning problem in a nearly unknown target environment, another line of work focuses on transfer learning in low-rank MDPs [7, 23, 2, 5]. Specifically, the problem setup assumes that the agent has access to information of several source tasks. The agent learns a common representation from the source domains and then leverages the learned representation to learn a policy performing well in the target tasks with limited information. This setting is in stark contrast to DRMDPs, where the agent only has access to the information of a single source domain, without any available information of the target domain, assuming the same task is being performed. This motivates the pessimistic principle of the distributionally robust perspective. Among the aforementioned works, Bose et al. [5] studied the offline multi-task RL, which is the most closely related to our setting. In particular, they investigate the representation transfer error in their Theorem 1, stating that the learned representation can lead to a transition kernel that is close to the target kernel in terms of the TV divergence. Note that the uncertainty is induced by the representation estimation error, which is different from our setting assuming that the uncertainty comes from perturbations on underlying factor distributions. Nevertheless, this work provides evidence that TV divergence is a reasonable measure to quantify the uncertainty in transition kernels and motivates a future research direction in learning robust policies that are robust to the uncertainty induced by the representation estimation error.

## Appendix B A More Computationally Efficient Variant of VA-DRPVI

In this section, we propose a modified version of Algorithm 2, which reduces the computation cost in the ridge regressions for variance estimation and achieves the same theoretical guarantees.

Variance Estimator.In Section 5, we estimate the variance of the truncated robust value function \([\hat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}\). Thus, for different \(\alpha\), we need to establish different variance estimators, which significantly increases the computational burden. The theoretical analysis of Algorithm 2 suggests that it suffices to estimate the the variance of \(\hat{V}_{h+1}^{{}^{\prime}\rho}\), instead of the truncated one. In particular, we know \([\operatorname{Var}_{h}\hat{V}_{h+1}^{{}^{\prime}\rho}](s,a)=[\mathbb{P}_{h}^ {0}(\hat{V}_{h+1}^{{}^{\prime}\rho})^{2}](s,a)-([\mathbb{P}_{h}^{0}\hat{V}_{h +1}^{{}^{\prime}\rho}](s,a))^{2}=\langle\bm{\phi}(s,a),\bm{z}_{h,2}\rangle-( \langle\bm{\phi}(s,a),\bm{z}_{h,1}\rangle)^{2}\). Then we estimate \(\bm{z}_{h,1}\) and \(\bm{z}_{h,2}\) via ridge regression:

\[\tilde{\bm{z}}_{h,2} =\operatorname*{argmin}_{\bm{z}\in\mathbb{R}^{d}}\sum_{\tau=1}^{ K}\big{(}\big{(}\hat{V}_{h+1}^{{}^{\prime}\rho}(s_{h+1}^{\tau})\big{)}^{2}- \bm{\phi}_{h}^{\tau\tau}\bm{z}\big{)}^{2}+\lambda\|\bm{z}\|_{2}^{2},\] (B.1a) \[\tilde{\bm{z}}_{h,1} =\operatorname*{argmin}_{\bm{z}\in\mathbb{R}^{d}}\sum_{\tau=1}^{ K}\big{(}\hat{V}_{h+1}^{{}^{\prime}\rho}(s_{h+1}^{\tau})-\bm{\phi}_{h}^{\tau \tau}\bm{z}\big{)}^{2}+\lambda\|\bm{z}\|_{2}^{2}.\] (B.1b)

We construct the following truncated variance estimator:

\[\widehat{\sigma}_{h}^{2}(s,a):=\max\Big{\{}1,\big{[}\bm{\phi}(s,a)^{\top} \tilde{\bm{z}}_{h,2}\big{]}_{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\tilde{\bm {z}}_{h,1}\big{]}_{[0,H]}^{2}-\tilde{O}\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}} \Big{)}\Big{\}}.\] (B.2)

The modified variance-aware algorithm is presented in Algorithm 3 and the theoretical guarantee is presented in Theorem B.1.

**Theorem B.1**.: Under Assumptions 3.1 and 4.3, for \(K>\max\{\tilde{O}(d^{2}H^{6}/\kappa),\tilde{O}(H^{4}/\kappa^{2})\}\) and \(\delta\in(0,1)\), if we set \(\lambda=1/H^{2}\) and \(\beta_{2}=\tilde{O}(\sqrt{d})\) in Algorithm 3, then with probability at least \(1-\delta\), for all \(s\in\mathcal{S}\), the suboptimality of VA-DRPVI satisfies

\[\text{SubOpt}(\hat{\pi},s,\rho)\leq\beta_{2}\cdot\sup_{P\in\mathcal{U}^{\rho} (P^{\alpha})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\Big{[}\sum_{i=1}^{d}\|\phi_{i} (s_{h},a_{h})\mathbf{1}_{i}\|_{\mathbf{\Sigma}_{h}^{*-1}}|s_{1}=s\Big{]},\] (B.3)

where \(\mathbf{\Sigma}_{h}^{*}=\sum_{\tau=1}^{K}\boldsymbol{\phi}_{h}^{\tau} \boldsymbol{\phi}_{h}^{\tau\top}/[\mathbb{V}_{h}V_{h+1}^{\star}](s_{h}^{\tau}, a_{h}^{\tau})+\lambda\mathbf{I}\).

**Remark B.2**.: The computation cost of Algorithm 3 is much smaller than Algorithm 2, as the variance estimators are not related to \(\alpha\) anymore. Notably, Algorithm 3 shares the same upper bound as Algorithm 2. According to Theorem 6.1, we know the modified algorithm is also minimax optimal.

## Appendix C Experiments

We conduct numerical experiments to illustrate the performances of our proposed algorithms, DRPVI and VA-DRPVI, and compare it with the their non-robust counterpart, PEVI [15]. All numerical experiments were conducted on a MacBook Pro with a 2.6 GHz 6-Core Intel CPU. The implementation of our DRPVI algorithm is available at https://github.com/pankulab/Offline-Linear-DRMP.

Construction of the simulated linear MDPWe leverage the simulated linear MDP setting proposed by Liu and Xu [20] and modify it as an offline RL problem. In particular, the source and target linear MDP environment are shown in Figure 1(a) and Figure 1(b). The state space is set to be \(\mathcal{S}=\{x_{1},\cdots,x_{5}\}\) and the action space is to be \(\mathcal{A}=\{-1,1\}^{4}\subset\mathbb{R}^{4}\). At each episode, the state always starts with \(x_{1}\), and then transits to \(x_{2},x_{4},x_{5}\) with probability defined in the figures. \(x_{2}\) is an intermediate state, and it can transit to \(x_{3},x_{4},x_{5}\) with probability defined on the lines. Moreover, Both \(x_{4}\) and \(x_{5}\) are absorbing states. \(x_{4}\) (\(x_{5}\)) is the fail state (goal state), and the reward starting from which is always 0 (1). The reward functions and transition probabilities are designed to depend on the hyperparameter \(\boldsymbol{\xi}\in\mathbb{R}^{4}\) as shown in the figure. The target environment is constructed by only perturbing the transition probability at \(x_{1}\) of the source environment, and the extend of perturbation is controlled by the hyperparameter \(q\in(0,1)\). We refer more details on the construction of the simulated linear DRMDP to the Supplementary A.1 of [20].

ImplementationWe simply use the random policy that chooses actions uniformly at random at any \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) to collect offline dataset. The offline dataset containing 100 trajectories collected by the behavior policy from the source environment. We conduct ablation study by setting the hyperparameter \(\boldsymbol{\xi}=(1/\|\boldsymbol{\xi}\|_{1},1/\|\boldsymbol{\xi}\|_{1},1/\| \boldsymbol{\xi}\|_{1},1/\|\boldsymbol{\xi}\|_{1})^{\top}\) and consider different choices of \(\|\boldsymbol{\xi}\|_{1}\in\{0.1,0.2,0.3\}\). Following [20], we use heterogeneous uncertainty level for our two algorithms. Specifically, we set \(\rho_{1,4}=0.5\) and \(\rho_{h,i}=0\) for all other cases. The experiment results are shown in Figure 2.

Figure 2 shows the performances of the learned policies of three algorithms. We conclude that both of our proposed algorithms are robust to environmental perturbation compared to the non-robust PEVI.

Furthermore, VA-DRPVIslightly outperforms DRPVI in most settings. These numerical results are consistent with our theoretical findings.

## Appendix D Proof of Theorem 4.4

Our analysis mainly deals with the challenges induced by the model uncertainty, \(\inf_{P\in\mathcal{U}^{\rho}(P^{0})}\), and the need to maximally exploit the information in the offline dataset. More specifically, the proof of Theorem 4.4 mainly constitutes of two steps.

Step 1: suboptimality decomposition.We first decompose the suboptimality gap in the following lemma to connect it with the estimation error, the full proof of which can be found in Appendix G.1.

**Lemma D.1** (Suboptimality Decomposition for DRMDP).: If the following holds

\[\Big{|}\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})} [\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\bm{\phi}(s,a)\hat{\bm{\nu}}^{ \rho}_{h}\Big{|}\leq\Gamma_{h}(s,a),\forall(s,a,h)\in\mathcal{S}\times\mathcal{ A}\times[H],\] (D.1)

then we have \(\text{SubOpt}(\hat{\pi},s,\rho)\leq 2\sup_{P\in\mathcal{U}^{\rho}(P^{0})} \sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\big{[}\Gamma_{h}(s_{h},a_{h})|s_{1}=s \big{]}\).

The main challenge in deriving Lemma D.1 lies in the dependency of the robust Bellman equation (3.1) on the nominal kernel \(P^{0}\), which is not linear and does not even have an explicit form. It should be noted that the term \(\big{|}\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i}) }[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\bm{\phi}(s,a)^{\top}\hat{\bm{ \nu}}^{\rho}_{h}\big{|}\) in condition (D.1) stands for the estimation error of the estimated robust Q-function in (4.5), which we refer to as the robust estimation uncertainty. Lemma D.1 shows that under the condition that the robust estimation uncertainty is bounded by \(\Gamma_{h}(s,a)\), the suboptimality gap can be upper bounded in terms of \(\Gamma_{h}(s,a)\). To conclude the proof, it remains to derive \(\Gamma_{h}(s,a)\) and then substitute it back into the result in Lemma D.1.

Step 2: bounding the robust estimation uncertainty.We now bound the robust estimation uncertainty in Lemma D.1 by the following result, the full proof of which can be found in Appendix G.2.

Figure 1: The source and the target linear MDP environments. The value on each arrow represents the transition probability. For the source MDP, there are five states and three steps, with the initial state being \(x_{1}\), the fail state being \(x_{4}\), and \(x_{5}\) being an absorbing state with reward 1. The target MDP on the right is obtained by perturbing the transition probability at the first step of the source MDP, with others remaining the same.

**Lemma D.2** (Robust Estimation Uncertainty Bound).: For any sufficiently large sample size \(K\) satisfying \(K>\max\{512\log(2dH^{2}/\delta)/\kappa^{2},20449d^{2}H^{2}/\kappa\}\), and any fixed \(\delta\in(0,1)\), if we set \(\lambda=1\) in Algorithm 1, then with probability at least \(1-\delta\), for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), we have

\[\Big{|}\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h,i}^{0} )}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)-\bm{\phi}(s,a)^{\top}\hat{\bm{ \rho}}_{h}^{\rho}\Big{|}\leq\Gamma_{h}(s,a),\] (D.2)

where \(\Gamma_{h}(s,a)=4\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_ {i}\|_{\bm{\Lambda}_{h}^{-1}}\) and \(\iota=\log(2dH^{2}K/\delta)\).

\(\Gamma_{h}(s,a)\) provides an explicit bound for the robust estimation uncertainty, which also serves as the penalty term in Line 6 of Algorithm 1. The main challenge of deriving Lemma D.2 lies in inferring the worst-case behavior using information merely from the nominal environment. Our idea is to first transform the robust estimation uncertainty to the estimation uncertainty of ridge regressions (4.2) on the nominal model \(P^{0}\), where the samples are collected and statistical control is available. We then adopt a reference-advantage decomposition technique, which is new in the linear DRMDP literature, to further decompose the estimation uncertainty on the nominal model into the reference uncertainty and the advantage uncertainty. The remaining proof is to bound the reference uncertainty and advantage uncertainty respectively using concentration and union bound arguments under an induction framework to address the temporal dependency. We highlight that all these arguments are specifically designed for the unique problem of DRMDP, which is novel and nontrivial.

Figure 2: Simulation results under different source domains. The \(x\)-axis represents the perturbation level corresponding to different target environments. \(\rho_{1,4}\) is the input uncertainty level for our VA-DRPVI algorithm. \(\|\xi\|_{1}\) is the hyperparameter of the linear DRMDP environment.

Proof of the Suboptimality Upper Bounds

In this section, we prove the main results in Corollary 4.5, Remark 4.6, Theorem 5.2, and Corollary 5.3, which give out the instance-dependent upper bounds of the proposed algorithms. Before the proof, we introduce some useful notations. For any function \(f:\mathcal{S}\to[0,H-1]\), define

\[\widetilde{\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h,i}^{0 })}}[\mathbb{P}_{h}f](s,a):=\bm{\phi}(s,a)^{\top}\hat{\bm{\nu}}_{h}^{\rho}(f),\] (E.1)

where for each \(i\in[d]\), we have

\[\hat{\bm{\nu}}_{h,i}^{\rho}(f) =\max_{\alpha\in[0,H]}\Big{\{}\hat{\bm{\Sigma}}^{\mu_{h,i}^{0}}[f( s)]_{\alpha}-\rho(\alpha-\min_{s^{\prime}\in\mathcal{S}}[f(s^{\prime})]_{ \alpha})\Big{\}},\] \[\hat{\bm{\Sigma}}^{\mu_{h,i}^{0}}[f(s)]_{\alpha} =\Big{[}\bm{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau }[f(s_{h+1}^{\tau})]_{\alpha}\Big{]}_{i}.\]

### Proof of Corollary 4.5

The proof of Corollary 4.5 is straightforward given our result in Theorem 4.4.

Proof.: Define \(\tilde{\bm{\Lambda}}_{h}=\mathbb{E}^{\pi^{b},P^{0}}[\bm{\phi}(s_{h},a_{h})\bm {\phi}(s_{h},a_{h})^{\top}],\forall h\in[H]\). By Assumption 4.3, we have \(\tilde{\bm{\Lambda}}_{h}\succeq\kappa\cdot\mathbf{I}\). We further bound (6.1) as follows,

\[\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^ {*},P}\bigg{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\bm{1}_{i}\|_{\tilde{\bm{ \Lambda}}_{h}^{-1}}\Big{|}s_{1}=s\bigg{]}\] \[\leq\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\frac{2}{\sqrt{K}} \mathbb{E}^{\pi^{*},P}\bigg{[}\sum_{h=1}^{H}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h })\bm{1}_{i}\|_{\tilde{\bm{\Lambda}}_{h}^{-1}}\Big{|}s_{1}=s\bigg{]}\] (E.2) \[=\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\frac{2}{\sqrt{K}}\mathbb{E} ^{\pi^{*},P}\bigg{[}\sum_{h=1}^{H}\sum_{i=1}^{d}\phi_{i}(s,a)\sqrt{\bm{1}_{i} ^{\top}\tilde{\bm{\Lambda}}_{h}^{-1}}\bm{1}_{i}\Big{|}s_{1}=s\bigg{]}\] \[\leq\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\frac{2}{\sqrt{K}} \mathbb{E}^{\pi^{*},P}\bigg{[}\sum_{h=1}^{H}\sum_{i=1}^{d}\phi_{i}(s,a)\sqrt{ \lambda_{\max}(\tilde{\bm{\Lambda}}_{h}^{-1})}\Big{|}s_{1}=s\bigg{]}\] (E.3) \[=\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\frac{2}{\sqrt{K}}\mathbb{E} ^{\pi^{*},P}\bigg{[}\sum_{h=1}^{H}\sum_{i=1}^{d}\phi_{i}(s,a)\sqrt{\frac{1}{ \lambda_{\min}(\tilde{\bm{\Lambda}}_{h})}}\Big{|}s_{1}=s\bigg{]}\] \[\leq\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\frac{2}{\sqrt{K}} \mathbb{E}^{\pi^{*},P}\bigg{[}\sum_{h=1}^{H}\sqrt{\frac{1}{\kappa}}\bigg{]}\] (E.4) \[=\frac{2H}{\sqrt{K\cdot\kappa}},\]

where (E.2) is due to Lemma I.3, (E.3) is due to the fact that for any matrix \(\bm{A}\), \(\lambda_{\min}\leq\bm{A}_{ii}\leq\lambda_{\max}\), where \(\bm{A}_{ii}\) is the \(i\)-th diagonal element of \(\bm{A}\). (E.3) holds due to Assumption 4.3 and the fact that \(\sum_{i=1}^{d}\phi_{i}(s,a)=1\). We conclude the proof by invoking Theorem 4.4. 

### Proof of Theorem 5.2

The proof idea is similar to that of Theorem 4.4, except that we additionally analyze the variance estimation and apply the Bernstein-type self-normalized concentration inequality to bound the reference uncertainty, which is the dominant term. We start from analyzing the estimation error of conditional variances in the following lemma.

**Lemma E.1**.: Under Assumptions 3.1 and 4.3, when \(K\geq\tilde{O}(H^{4}/\kappa^{2})\), then with probability at least \(1-\delta\), for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) and any fixed \(\alpha\), we have

\[\big{[}\mathbb{V}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}\big{]}(s,a)-\tilde{O} \Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)}\leq\widehat{\sigma}_{h}^{2}(s,a; \alpha)\leq\big{[}\mathbb{V}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}\big{]}(s,a).\]The following lemma bounds the estimation error by reference-advantage decomposition.

**Lemma E.2** (Variance-Aware Reference-Advantage Decomposition).: There exist \(\{\alpha_{i}\}_{i\in[d]}\), where \(\alpha_{i}\in[0,H],\forall i\in[d]\), such that

\[\Big{|}\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\mu_{h, i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)-\widehat{\inf_{P_{h}( \cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\mu_{h,i}^{0})}}[\mathbb{P}_{h}\widehat {V}_{h+1}^{\rho}](s,a)\Big{|}\] \[\leq\lambda\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}\|\mathbb{E}^{\bm{\mu}_{h}^{0}}[V_{h+1} ^{\star,\rho}(s)]_{\alpha_{i}}\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}}_{\text{i}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \bm{\Sigma}_{h}^{-1}(\alpha_{i})}\|\sum_{\tau=1}^{K}\frac{\bm{\phi}_{h}^{ \tau}\eta_{t}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})}{\widehat{\sigma}_{ h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\|_{\bm{\Sigma}_{h}^{-1}( \alpha_{i})}}_{\text{ii}}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{ i}\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}\|\mathbb{E}^{\bm{\mu}_{h}^{0}}[[\widehat {V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}]\|_{ \bm{\Sigma}_{h}^{-1}(\alpha_{i})}}_{\text{iii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \bm{\Sigma}_{h}^{-1}(\alpha_{i})}\|\sum_{\tau=1}^{K}\frac{\bm{\phi}_{h}^{\tau }\eta_{t}^{\tau}([\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star, \rho}(s)]_{\alpha_{i}})}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau}; \alpha_{i})}\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}}_{\text{iv}},\]

where \(\eta_{h}^{\tau}([f]_{\alpha_{i}})=\big{(}\big{[}\mathbb{P}_{h}^{0}[f]_{\alpha_ {i}}\big{]}(s_{h}^{\tau},a_{h}^{\tau})-[f(s_{h+1}^{\tau})]_{\alpha_{i}}\big{)}\), for any function \(f:\mathcal{S}\to[0,H-1]\).

Now we are ready to prove Theorem5.2

Proof of Theorem5.2.: To prove this theorem, we bound the estimation error by \(\Gamma_{h}(s,a)\), then invoke LemmaD.1 to get the result. First, we bound terms i-iv in LemmaE.2 to deduce \(\Gamma_{h}(s,a)\) at each step \(h\in[H]\), respectively.

Bound i and iii:We set \(\lambda=1/H^{2}\) to ensure that for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), we have

\[\text{i}+\text{iii}\leq\sqrt{\lambda}\sqrt{d}H\sum_{i=1}^{d}\|\phi_{i}(s,a) \mathbf{1}_{i}\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}=\sqrt{d}\sum_{i=1}^{d}\| \phi_{i}(s,a)\mathbf{1}_{i}\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}.\] (E.5)

Bound ii:For all \((s,a,\alpha)\in\mathcal{S}\times\mathcal{A}\times[0,H]\), by definition we have \(\widehat{\sigma}_{h}(s,a;\alpha)\geq 1\). Thus, for all \((h,\tau,i)\in[H]\times[K]\times[d]\), we have \(|\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})/\widehat{\sigma}_{h}(s_{ h}^{\tau},a_{h}^{\tau},\alpha_{i})|\leq H\). Note that \(V_{H+1}^{\star,\rho}\) is independent of \(\mathcal{D}\), we can directly apply Bernstein-type self-normalized concentration inequality LemmaI.2 and a union bound to obtain the upper bound. In concrete, we define the filtration \(\mathcal{F}_{\tau-1,h}=\sigma(\{(s_{h}^{j},a_{h}^{j})\}_{j=1}^{\tau}\cup\{s_{h +1}^{j}\}_{j=1}^{\tau-1})\). Since \(V_{h+1}^{\star,\rho}\) and \(\widehat{\sigma}_{h}(s,a;\alpha)\) are independent of \(\mathcal{D}\), thus \(\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})/\widehat{\sigma}_{h}(s_{h} ^{\tau},a_{h}^{\tau},\alpha_{i})\) is mean-zero conditioned on the filtration \(\mathcal{F}_{\tau-1,h}\). Further, we have

\[\mathbb{E}\Big{[}\Big{(}\frac{\eta_{h}^{\tau}([V_{h+1}^{\star,\rho }]_{\alpha_{i}})}{\widehat{\sigma}_{h}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})} \Big{)}^{2}\Big{|}\mathcal{F}_{\tau-1,h}\Big{]} =\frac{[\text{Var}[V_{h+1}^{\star,\rho}]_{\alpha_{i}}](s_{h}^{ \tau},a_{h}^{\tau})}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_ {i})}\] (E.6) \[\leq\frac{[\mathbb{V}[V_{h+1}^{\star,\rho}]_{\alpha_{i}}](s_{h}^{ \tau},a_{h}^{\tau})}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_ {i})}\] \[=\frac{[\mathbb{V}[V_{h+1}^{\star,\rho}]_{\alpha_{i}}](s_{h}^{ \tau},a_{h}^{\tau})-\tilde{O}(dH^{3}/\sqrt{K\kappa})}{\widehat{\sigma}_{h}^{2}(s _{h}^{\tau},a_{h}^{\tau};\alpha_{i})}+\frac{\tilde{O}(dH^{3}/\sqrt{K\kappa})}{ \widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\] \[\leq 1+\frac{\tilde{O}(dH^{3}/\sqrt{K\kappa})}{\widehat{\sigma}_{h}^{ 2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})-\tilde{O}(dH^{3}/\sqrt{K\kappa})}\] (E.7) \[\leq 1+2\tilde{O}\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)},\] (E.8)where (E.6) holds by the fact that \(\widehat{\sigma}_{h}^{2}(\cdot,\cdot;\cdot)\) is independent of \(\mathcal{D}\) and \((s_{h}^{\star},a_{h}^{\star})\) is \(\mathcal{F}_{\tau-1,h}\) measurable. (E.7) holds by Lemma E.1, and (E.8) holds by setting \(K\geq\tilde{\Omega}(d^{2}H^{6}/\kappa)\) such that \(\widehat{\sigma}_{h}^{2}(s_{h}^{\star},a_{h}^{\star};\alpha_{i})-\tilde{O}(dH^ {3}/\sqrt{K\kappa})\geq 1-\tilde{O}(dH^{3}/\sqrt{K\kappa})\geq 1/2\). Further, by (E.8), our choice of \(K\) also ensures that \(\mathbb{E}\big{[}\big{(}\eta_{h}^{\star}(|V_{h+1}^{\star,\rho}|_{\alpha_{i}}) \big{)}^{2}|\mathcal{F}_{\tau-1,h}\big{]}=O(1)\). Then by Lemma I.2, we have

\[\left\|\sum_{\tau=1}^{K}\frac{\phi_{h}^{\star}\eta_{h}^{\star}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})}{\widehat{\sigma}_{h}^{2}(s_{h}^{\star},a_{h}^{\star}; \alpha_{i})}\right\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\leq\tilde{O}( \sqrt{d}).\]

This implies

\[\text{ii}\leq\tilde{O}(\sqrt{d})\sum_{i=1}^{d}\|\phi_{i}(s,a) \mathbf{1}_{i}\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}.\] (E.9)

Bound iv:Following the same induction analysis procedure, we have \(\|[\widehat{V}_{h+1}^{\rho}]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}]_{\alpha_{i}} \|\leq\tilde{O}(\sqrt{d}H^{2}/\sqrt{K\kappa})\). Then, using standard \(\epsilon\)-covering number argument and Lemma I.1, we have

\[\text{iv}\leq\tilde{O}\Big{(}\frac{d^{3/2}H^{2}}{\sqrt{K\kappa} }\Big{)}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\mathbf{\Sigma}_{h}^{ -1}(\alpha_{i})}.\] (E.10)

To make it non-dominant, we require \(K\geq\tilde{\Omega}(d^{2}H^{4}/\kappa)\). By Lemma E.1, for any \(\alpha\in[0,H]\), we have

\[\widehat{\sigma}_{h}^{2}(s_{h}^{\star},a_{h}^{\star};\alpha)\leq[ \forall_{h}[V_{h+1}^{\star,\rho}]_{\alpha}](s_{h}^{\star},a_{h}^{\star})\leq[ \forall_{h}V_{h+1}^{\star,\rho}](s_{h}^{\star},a_{h}^{\star}),\]

this implies that

\[\bigg{(}\sum_{\tau=1}^{K}\frac{\phi_{h}^{\star}\phi_{h}^{\star\top} }{\widehat{\sigma}_{h}^{2}(s_{h}^{\star},a_{h}^{\star};\alpha_{i})}+\lambda \mathbf{I}\bigg{)}^{-1}\preceq\bigg{(}\sum_{\tau=1}^{K}\frac{\phi_{h}^{\star} \phi_{h}^{\star\top}}{[\forall_{h}V_{h+1}^{\star,\rho}(s_{h}^{\star,\rho})}+ \lambda\mathbf{I}\bigg{)}^{-1}:=\mathbf{\Sigma}_{h}^{\star-1}.\]

Combining (E.5), (E.9) and (E.10), we have

\[\Big{|} \inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\star}(s,a;\boldsymbol{ \mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)-\widetilde{ \inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\star}(s,a;\boldsymbol{\mu}_{h,i}^{0 })}}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)\Big{|}\] \[\leq\tilde{O}(\sqrt{d})\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{ i}\|_{\mathbf{\Sigma}_{h}^{\star-1}}.\]

Define \(\Gamma_{h}(s,a)=\tilde{O}(\sqrt{d})\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{ i}\|_{\mathbf{\Sigma}_{h}^{\star-1}}\), we concludes the proof by invoking Lemma D.1. 

### Proof of Corollary 5.3

In this section, we prove Corollary 5.3. We start with an interesting phenomenon, we call 'range shrinkage', stated in the following lemma.

**Lemma E.3** (Range Shrinkage).: For any \((\rho,\pi,h)\in(0,1]\times\Pi\times[H]\), we have

\[\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)-\min_{s\in\mathcal{S}} V_{h}^{\pi,\rho}(s)\leq\frac{1-(1-\rho)^{H-h+1}}{\rho}.\] (E.11)

Proof of Corollary 5.3.: By the fact that the variance of a random variable can be upper bounded by the square of its range and Lemma E.3, for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), we have

\[[\forall V_{h+1}^{\star}](s,a)\leq\Big{(}\frac{1-(1-\rho)^{H-h+1} }{\rho}\Big{)}^{2}\leq\Big{(}\frac{1-(1-\rho)^{H}}{\rho}\Big{)}^{2}.\]

Then we have

\[\sum_{\tau=1}^{K}\frac{\phi_{h}^{\star}\phi_{h}^{\tau\top}}{[ \forall_{h}V_{h+1}^{\star}](s_{h}^{\star},a_{h}^{\star})}+\frac{1}{H^{2}} \mathbf{I}\succeq\sum_{\tau=1}^{K}\frac{\phi_{h}^{\star}\phi_{h}^{\tau\top}}{( \frac{1-(1-\rho)^{H}}{\rho})^{2}}+\frac{1}{H^{2}}\mathbf{I}.\]Thus we have

\[\bm{\Sigma}_{h}^{\star-1}=\Big{(}\sum_{\tau=1}^{K}\frac{\bm{\phi}_{h}^{\top}\bm{ \phi}_{h}^{\top}}{[\mathbb{V}_{h}V_{h+1}^{\star}](s_{h}^{\tau},a_{h}^{\tau})}+ \frac{1}{H^{2}}\mathbf{I}\Big{)}^{-1}\preceq\Big{(}\frac{1-(1-\rho)^{H}}{\rho} \Big{)}^{2}\Big{(}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\bm{\phi}_{h}^{\tau\top} +\frac{1}{H^{2}}\mathbf{I}\Big{)}^{-1}.\]

By Theorem 5.2, we have

\[\text{SubOpt}(\hat{\pi},s,\rho) \leq\tilde{O}(\sqrt{d})\cdot\sup_{P\in\mathcal{U}^{\rho}(P^{0})} \sum_{h=1}^{H}\mathbb{E}^{\pi^{\star},P}\Big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a _{h})\mathbf{1}_{i}\|_{\bm{\Sigma}_{h}^{-1}}\big{|}s_{1}=s\Big{]}\] \[\leq\tilde{O}(\sqrt{d})\cdot\frac{1-(1-\rho)^{H}}{\rho}\sup_{P\in \mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{\star},P}\Big{[}\sum _{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{\bm{\Lambda}_{h}^{-1}} \big{|}s_{1}=s\Big{]}.\]

This concludes the proof. 

## Appendix F Proof of the Information-Theoretic Lower Bound

In this section, we prove the information-theoretic lower bound. We first introduce the construction of hard instances in Appendix F.1, then we prove Theorem 6.1 in Appendix F.2, and prove Corollary 6.2 in Appendix F.3.

### Construction of Hard Instances

We design a family of \(d\)-rectangular linear DRMDPs parameterized by a Boolean vector \(\bm{\xi}=\{\bm{\xi}_{h}\}_{h\in[H]}\), where \(\bm{\xi}_{h}\in\{-1,1\}^{d}\). For a given \(\bm{\xi}\) and uncertainty level \(\rho\in(0,3/4]\), the corresponding \(d\)-rectangular linear DRMDP \(M_{\bm{\xi}}^{\star}\) has the following structure. The state space \(\mathcal{S}=\{x_{1},x_{2}\}\) and the action space \(\mathcal{A}=\{0,1\}^{d}\). The initial state distribution \(\mu_{0}\) is defined as

\[\mu_{0}(x_{1})=\frac{d+1}{d+2}\quad\text{and}\quad\mu_{0}(x_{2})=\frac{1}{d+2}.\]

The feature mapping \(\bm{\phi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d+2}\) is defined as

\[\bm{\phi}(x_{1},a)^{\top} =\Big{(}\frac{a_{1}}{d},\frac{a_{2}}{d},\cdots,\frac{a_{d}}{d},1 -\sum_{i=1}^{d}\frac{a_{i}}{d},0\Big{)}\] \[\bm{\phi}(x_{2},a)^{\top} =\big{(}0,0,\cdots,0,0,1\big{)},\]

which satisfies \(\phi_{i}(s,a)\geq 0\) and \(\sum_{i=1}^{d}\phi_{i}(s,a)=1\). The factor distributions \(\{\bm{\mu}_{h}\}_{h\in[H]}\) are defined as

\[\bm{\mu}_{h}^{\top}=\big{(}\underbrace{\delta_{x_{1}},\delta_{x_{1}},\cdots, \delta_{x_{1}},\delta_{x_{1}}}_{d+1},\delta_{x_{2}}\big{)},\forall h\in[H],\]

so the transition is homogeneous and does not depend on action but only on state. The reward parameters \(\{\bm{\theta}_{h}\}_{h\in[H]}\) are defined as

\[\bm{\theta}_{h}^{\top}=\delta\cdot\Big{(}\frac{\xi_{h1}+1}{2},\frac{\xi_{h2}+1 }{2},\cdots,\frac{\xi_{hd}+1}{2},\frac{1}{2},0\Big{)},\forall h\in[H],\]

where \(\delta\) is a parameter to control the differences among instances, which is to be determined later. The reward \(r_{h}\) is generated from the normal distribution \(r_{h}\sim\mathcal{N}(r_{h}(s_{h},a_{h}),1)\), where \(r_{h}(s,a)=\bm{\phi}(s,a)^{\top}\bm{\theta}_{h}\). Note that

\[r_{h}(x_{1},a)=\bm{\phi}(x_{1},a)^{\top}\bm{\theta}_{h}=\frac{\delta}{2d} \big{(}\langle\bm{\xi}_{h},a\rangle+d\big{)}\geq 0\quad\text{and}\quad r_{h}(x_{2},a)=\bm{ \phi}(x_{2},a)^{\top}\bm{\theta}_{h}=0,\;\forall a\in\mathcal{A},\]

which means that \(x_{2}\) is a worst state in terms of the mean reward. Thus, the worst case transition kernel should have the highest possible transition probability to \(x_{2}\). This construction is pivotal in achieving a concise expression of robust value function. Further, we only consider model uncertainty in the first step. By the fact that \(x_{2}\) is the worse state, we know the worst case factor distribution for the first step is

\[\hat{\bm{\mu}}_{1}^{\top}=\big{(}(1-\rho)\delta_{x_{1}}+\rho\delta_{x_{2}},(1- \rho)\delta_{x_{1}}+\rho\delta_{x_{2}},\cdots,(1-\rho)\delta_{x_{1}}+\rho\delta_ {x_{2}},(1-\rho)\delta_{x_{1}}+\rho\delta_{x_{2}},\delta_{x_{2}}\big{)}.\]

We illustrate the designed \(d\)-rectangular linear DRMDP \(M_{\bm{\xi}}^{\rho}\) in Figure 3(a) and Figure 3(b).

Finally, we design the procedure for collecting the offline dataset. We assume the \(K\) trajectories are collected by a behavior policy \(\pi^{b}=\{\pi_{h}^{b}\}_{h\in[H]}\) defined as

\[\pi_{h}^{b}\sim\text{Unif}\big{(}\{\bm{e}_{1},\cdots,\bm{e}_{d},\bm{0}\}\big{)},\forall h\in[H],\]

where \(\{\bm{e}_{i}\}_{i\in[d]}\) are the canonical basis vectors in \(\mathbb{R}^{d}\). The initial state is generated according to \(\mu_{0}\). It is straightforward to check that the constructed hard instances satisfy Assumption 4.3. We denote the offline dataset as \(\mathcal{D}\).

### Proof of Theorem 6.1

With this family of hard instances, we are ready to prove the information-theoretic lower bound. First, we define some notations. For any \(\bm{\xi}\in\{-1,1\}^{dH}\), let \(\mathbb{Q}_{\bm{\xi}}\) denote the distribution of dataset \(\mathcal{D}\) collected from the MDP \(M_{\bm{\xi}}\). Denote the family of parameters as \(\Omega=\{-1,1\}^{dH}\) and the family of hard instances as \(\mathcal{M}=\{M_{\bm{\xi}}:\bm{\xi}\in\Omega\}\).

Proof of Theorem 6.1.: The proof constitutes three steps. In the first step, we lower bound the minimax suboptimality gap by testing error in the following Lemma F.1, the full proof of which can be found in Appendix G.6.

**Lemma F.1** (Reduction to testing).: For the given family of \(d\)-rectangular linear DRMDPs, we have

\[\inf_{\hat{\pi}}\sup_{M\in\mathcal{M}}\text{SubOpt}(\hat{\pi},x_{1},\rho)\geq (1-\rho)\cdot\frac{\delta dH}{8d}\cdot\min_{\begin{subarray}{c}\bm{\xi},\bm{ \xi}^{\prime}\in\Omega\\ D_{H}(\bm{\xi},\bm{\xi}^{\prime})=1\end{subarray}}\inf_{\psi}\Big{[}\mathbb{Q }_{\bm{\xi}}(\psi(\mathcal{D})\neq\bm{\xi})+\mathbb{Q}_{\bm{\xi}^{\prime}}( \psi(\mathcal{D})\neq\bm{\xi}^{\prime})\Big{]},\] (F.1)

where for fixed indices \(\bm{\xi}\) and \(\bm{\xi}^{\prime}\), \(\psi\) is any test function taking value in \(\{\bm{\xi},\bm{\xi}^{\prime}\}\).

In the second step, we lower bound the testing error on the right hand side of (F.1) in the following Lemma F.2, the full proof of which can be found in Appendix G.7.

**Lemma F.2** (Lower bound on testing error).: For the given family of \(d\)-rectangular linear DRMDPs, let \(\delta=d^{3/2}/\sqrt{2K}\), then we have

\[\min_{\begin{subarray}{c}\bm{\xi},\bm{\xi}^{\prime}\\ D_{H}(\bm{\xi},\bm{\xi}^{\prime})=1\end{subarray}}\inf_{\psi}\Big{[}\mathbb{Q }_{\bm{\xi}}(\psi(\mathcal{D})\neq\bm{\xi})+\mathbb{Q}_{\bm{\xi}^{\prime}}( \psi(\mathcal{D})\neq\bm{\xi}^{\prime})\Big{]}\geq\frac{1}{2}.\]

By Lemma F.1 and Lemma F.2, we have

\[\inf_{\hat{\pi}}\sup_{M\in\mathcal{M}}\text{SubOpt}(\hat{\pi},x_{1},\rho) \geq\frac{d^{3/2}H}{128\sqrt{K}}.\] (F.2)

In the last step, we upper bound the uncertainty function \(\Phi(\bm{\Sigma}_{h}^{*},s)\) in the following Lemma F.3, the full proof of which can be found in Appendix G.8.

Figure 3: The nominal environment and the worst case environment. The value on each arrow represents the transition probability. The MDP has two states and \(H\) steps. For the nominal environment, both \(x_{1}\) and \(x_{2}\) are absorbing states, which means that the state will always stay at the initial state in the nominal environment. The worst case environment on the right is obtained by perturbing the transition probability at the first step of the nominal environment, with others remain the same.

**Lemma F.3**.: For all \(M_{\bm{\xi}}\in\mathcal{M}\), when \(K\geq\bar{O}(d^{4})\), then with probability at least \(1-\delta\), we have

\[\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P}\Big{[} \sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{\bm{\Sigma}_{h}^{*-1}} \big{|}s_{1}=x_{1}\Big{]}\leq\frac{4d^{3/2}H}{\sqrt{K}}.\]

By Lemma F.3 and (F.2), we know that with probability at least \(1-\delta\), there exist a universal constant \(c\), such that

\[\inf_{\hat{\pi}}\sup_{M\in\mathcal{M}}\text{SubOpt}(\hat{\pi},x_{1},\rho)\geq c \cdot\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},P} \Big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{\bm{\Sigma}_{h}^ {*-1}}|s_{1}=x_{1}\Big{]}.\]

This concludes the proof. 

### Proof of Corollary 6.2

Proof.: The result in Corollary 6.2 directly follows from the fact shown in (G.38): for the constructed hard instances, we have \(\bm{\Sigma}_{h}^{*}=\bm{\Lambda}_{h}\). Thus, we complete the proof by directly substituting \(\bm{\Sigma}_{h}^{*}\) in the result of Theorem 6.1 by \(\bm{\Lambda}_{h}\). 

## Appendix G Proof of Technical Lemmas

### Proof of Lemma D.1

Proof.: First, we decompose \(\text{SubOpt}(\hat{\pi},s,\rho)\) as follows

\[\text{SubOpt}(\hat{\pi},s,\rho)=\underbrace{V_{1}^{\pi^{*},\rho}(s)-\widehat {V}_{1}^{\rho}(s)}_{\mathbf{1}}+\underbrace{\widehat{V}_{1}^{\rho}(s)-V_{1}^ {\pi,\rho}(s)}_{\mathbf{1}\mathbf{1}},\]

then we bound term \(\mathbf{I}\) and term \(\mathbf{II}\), respectively.

Bounding term \(\mathbf{I}\):Note that

\[V_{h}^{\pi^{*},\rho}(s)-\widehat{V}_{h}^{\rho}(s) =Q_{h}^{\pi^{*},\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho}(s, \hat{\pi}_{h}(s))\] \[=Q_{h}^{\pi^{*},\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho}(s, \pi_{h}^{*}(s))+\widehat{Q}_{h}^{\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho }(s,\hat{\pi}_{h}(s))\] \[\leq Q_{h}^{\pi^{*},\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho }(s,\pi_{h}^{*}(s)).\] (G.1)

Here (G.1) holds by the fact that \(\hat{\pi}_{h}(s)\) is the greedy policy corresponding to \(\widehat{Q}_{h}^{\rho}(s,a)\), which leads to \(\widehat{Q}_{h}^{\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho}(s,\hat{\pi}_{h }(s))\leq 0\). Further, by the robust Bellman equation (3.1), we have

\[Q_{h}^{\pi^{*},\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho}(s, \pi_{h}^{*}(s))\] \[=r_{h}(s,\pi_{h}^{*}(s))+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h} ^{\rho}(s,a;\bm{\mu}_{h,i}^{0})}[\mathbb{P}_{h}V_{h+1}^{\pi^{*},\rho}](s,\pi_{h }^{*}(s))-\widehat{Q}_{h}^{\rho}(s,\pi_{h}^{*}(s))\] \[=r_{h}(s,\pi_{h}^{*}(s))+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h} ^{\rho}(s,a;\bm{\mu}_{h,i}^{0})}[\mathbb{P}_{h}V_{h+1}^{\pi^{*},\rho}](s,\pi_{h }^{*}(s))-r_{h}(s,\pi_{h}^{*}(s))\] \[\quad-\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu }_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,\pi_{h}^{*}(s))+r_{h}(s,\pi_{h}^{*}(s))+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_ {h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,\pi_{h}^{*}(s))\] \[\quad-\widehat{Q}_{h}^{\rho}(s,\pi_{h}^{*}(s)).\]

To proceed, we define the robust Bellman update error as follows

\[\zeta_{h}^{\rho}(s,a)=r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{ \rho}(s,a;\bm{\mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)- \widehat{Q}_{h}^{\rho}(s,a),\]

and denote the worst case transition kernel with respect to the estimated robust value function as \(\widehat{P}=\{\widehat{P}_{h}\}_{h\in[H]}\), where \(\widehat{P}_{h}(\cdot|s,a)=\arg\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho} (s,a;\bm{\mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a),\forall (s,a)\in\mathcal{S}\times\mathcal{A}\). Then we have

\[Q_{h}^{\pi^{*},\rho}(s,\pi_{h}^{*}(s))-\widehat{Q}_{h}^{\rho}(s,\pi_{h}^{*}(s))\]\[=\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}[ \mathbb{P}_{h}V^{\pi^{+},\rho}_{h+1}](s,\pi^{\star}_{h}(s))-\inf_{P_{h}(\cdot|s,a )\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{ \rho}_{h+1}](s,\pi^{\star}_{h}(s))+\zeta^{\rho}_{h}(s,\pi^{\star}_{h}(s))\] \[\leq\big{[}\widehat{\mathbb{P}}_{h}(V^{\pi^{\star},\rho}_{h+1}- \widehat{V}^{\rho}_{h+1})\big{]}(s,\pi^{\star}_{h}(s))+\zeta^{\rho}_{h}(s,\pi^{ \star}_{h}(s)).\] (G.2)

Combining (G.1) and (G.2), we have for any \(h\in[H]\),

\[V^{\pi^{\star},\rho}_{h}(s)-\widehat{V}^{\rho}_{h}(s)\leq\big{[}\widehat{ \mathbb{P}}_{h}(V^{\pi^{\star},\rho}_{h+1}-\widehat{V}^{\rho}_{h+1})\big{]}(s, \pi^{\star}_{h}(s))+\zeta^{\rho}_{h}(s,\pi^{\star}_{h}(s)).\] (G.3)

Recursively applying (G.3), we have

\[V^{\pi^{\star},\rho}_{1}(s)-\widehat{V}^{\rho}_{1}(s)\leq\sum_{h=1}^{H} \mathbb{E}^{\pi^{\star},\widehat{P}}\big{[}\zeta^{\rho}_{h}(s_{h},a_{h})|s_{1 }=s\big{]}.\] (G.4)

Bounding term II:Note that \(\widehat{V}^{\rho}_{h}(s)-V^{\hat{\pi},\rho}_{h}(s)=\widehat{Q}^{\rho}_{h}(s, \hat{\pi}_{h}(s))-Q^{\hat{\pi},\rho}_{h}(s,\hat{\pi}_{h}(s))\), by the robust Bellman equation (3.1), we have

\[\widehat{V}^{\rho}_{h}(s)-V^{\hat{\pi},\rho}_{h}(s)\] \[=\widehat{Q}^{\rho}_{h}(s,\hat{\pi}_{h}(s))-r_{h}(s,\hat{\pi}_{h} (s))-\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}[ \mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,\hat{\pi}_{h}(s))\] \[\quad+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu }^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,\hat{\pi}_{h}(s))-\inf _{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{ P}_{h}V^{\hat{\pi},\rho}_{h+1}](s,\hat{\pi}_{h}(s))\] \[=-\zeta^{\rho}_{h}(s,\hat{\pi}_{h}(s))+\inf_{P_{h}(\cdot|s,a)\in \mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{ \rho}_{h+1}](s,\hat{\pi}_{h}(s))-\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h }(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}V^{\hat{\pi},\rho}_{h+1}](s,\hat{\pi }_{h}(s)).\]

To proceed, we denote the worst case transition kernel with respect to the robust value function of \(\hat{\pi}\) as \(P^{\hat{\pi}}=\{P^{\hat{\pi}}_{h}\}_{h\in[H]}\), where \(P^{\hat{\pi}}_{h}(\cdot|s,a)=\arg\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h }(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}V^{\hat{\pi},\rho}_{h+1}](s,a)\), then we have

\[\widehat{V}^{\rho}_{h}(s)-V^{\hat{\pi},\rho}_{h}(s)\leq-\zeta^{\rho}_{h}(s, \hat{\pi}_{h}(s))+\big{[}\mathbb{P}^{\hat{\pi}}_{h}(\widehat{V}^{\rho}_{h+1}- \widehat{V}^{\hat{\pi},\rho}_{h+1})\big{]}(s,\hat{\pi}_{h}(s)).\] (G.5)

Applying (G.5) recursively, we have

\[\widehat{V}^{\rho}_{1}(s)-V^{\hat{\pi},\rho}_{1}(s)\leq\sum_{h=1}^{H}\mathbb{E }^{\hat{\pi},P^{\hat{\pi}}}\big{[}-\zeta^{\rho}_{h}(s_{h},a_{h})|s_{1}=s\big{]}.\] (G.6)

Now it remains to bound the robust Bellman error \(\zeta^{\rho}_{h}(\cdot,\cdot)\). In particular, we aim to show that for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\),

\[0\leq\zeta^{\rho}_{h}(s,a)\leq 2\Gamma_{h}(s,a).\]

Note that \(\zeta^{\rho}_{h}(s,a)=r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h }(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)- \widehat{Q}^{\rho}_{h}(s,a)\). Recall the definition of \(\widehat{Q}^{\rho}_{h}(s,a)\) in Algorithm 1 and the notation in (E.1), and we have

\[\zeta^{\rho}_{h}(s,a) =r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a; \bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)\] \[\qquad\qquad-\max\Big{\{}r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in \mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{ \rho}_{h+1}](s,a)-\Gamma_{h}(s,a),0\Big{\}}.\]

If \(r_{h}(s,a)+\widehat{\inf}_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{ \mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\Gamma_{h}(s,a)\leq 0\), then \(\zeta^{\rho}_{h}(s,a)=r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a; \bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)\geq 0\). If \(r_{h}(s,a)+\widehat{\inf}_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0 }_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\Gamma_{h}(s,a)>0\), then we have \(\zeta^{\rho}_{h}(s,a)=r_{h}(s,a)+\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a; \bm{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-r_{h}(s,a)- \widehat{\inf}_{P_{h}(\cdotThus, for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), we have

\[0\leq\zeta_{h}^{\rho}(s,a)\leq 2\Gamma_{h}(s,a).\] (G.7)

Combining (G.4), (G.6) and (G.7), we have

\[\text{SubOpt}(\hat{\pi},s,\rho) \leq\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},\widehat{P}}\big{[}\zeta_{h }(s_{h},a_{h})|s_{1}=s\big{]}+\sum_{h=1}^{H}\mathbb{E}^{\hat{\pi},P^{\mu}}\big{[} -\zeta_{h}^{\rho}(s_{h},a_{h})|s_{1}=s\big{]}\] \[\leq\sum_{h=1}^{H}\mathbb{E}^{\pi^{*},\widehat{P}}\big{[}\zeta_{h }^{\rho}(s_{h},a_{h})|s_{1}=s\big{]}\] \[\leq\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^ {\pi^{*},P}\big{[}\zeta_{h}^{\rho}(s_{h},a_{h})|s_{1}=s\big{]}\] \[\leq 2\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E} ^{\pi^{*},P}\big{[}\Gamma_{h}(s_{h},a_{h})|s_{1}=s\big{]}.\]

This concludes the proof. 

### Proof of Lemma D.2

In this section, we prove Lemma D.2. Before the proof, we first present several auxiliary lemmas.

**Lemma G.1** (Reference-Advantage Decomposition).: There exist real values \(\{\alpha_{i}\}_{i\in[d]}\), where \(\alpha_{i}\in[0,H],\forall i\in[d]\), such that

\[\Big{|}\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a; \boldsymbol{\mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)- \operatorname*{\overline{\inf}}_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a ;\boldsymbol{\mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a) \Big{|}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{1 }_{i}\|_{\Lambda_{h}^{-1}}\|\mathbb{E}^{\boldsymbol{\mu}_{h}^{0}}[V_{h+1}^{*,\rho}(s)]_{\alpha_{i}}\|_{\Lambda_{h}^{-1}}}_{\text{i}}+\underbrace{\sum_{i=1} ^{d}\|\phi_{i}(s,a)\boldsymbol{1}_{i}\|_{\Lambda_{h}^{-1}}}_{\text{i}}\Big{\|} \sum_{\tau=1}^{K}\boldsymbol{\phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{*,\rho} ]_{\alpha_{i}})\Big{\|}_{\Lambda_{h}^{-1}}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{ 1}_{i}\|_{\Lambda_{h}^{-1}}\Big{\|}\mathbb{E}^{\boldsymbol{\mu}_{h}^{0}}\big{[} [\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{*,\rho}(s)]_{\alpha_{i}} \big{]}\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{1}_{i }\|_{\Lambda_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K}\boldsymbol{\phi}_{h}^{\tau} \eta_{h}^{\tau}([\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{*,\rho}( s)]_{\alpha_{i}})\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iv}},\]

where \(\eta_{h}^{\tau}([f]_{\alpha_{i}})=\big{(}\big{[}\mathbb{P}_{h}^{0}[f]_{\alpha_ {i}}\big{]}(s_{h}^{\tau},a_{h}^{\tau})-[f(s_{h+1}^{\tau})]_{\alpha_{i}}\big{)}\), for any function \(f:\mathcal{S}\to[0,H-1]\).

**Lemma G.2** (Bound of Weights).: For any \(h\in[H]\), denote the weight \(\boldsymbol{w}_{h}^{\rho}=\boldsymbol{\theta}_{h}+\bar{\boldsymbol{\nu}}_{h}^{\rho}\) in Algorithm 1, then \(\boldsymbol{w}_{h}^{\rho}\) satisfies

\[\|\boldsymbol{w}_{h}^{\rho}\|_{2}\leq 2H\sqrt{dK/\lambda}.\]

**Lemma G.3**.: [15, Lemma B.2] Let \(f:\mathcal{S}\to[0,R-1]\) be any fixed function. For any \(\delta\in(0,1)\), we have

\[\mathbb{P}\Big{(}\Big{\|}\sum_{\tau=1}^{K}\phi_{h}^{\tau}\cdot\eta_{h}^{\tau}( f)\Big{\|}_{\Lambda_{h}^{-1}}^{2}\geq R^{2}\Big{(}2\log\Big{(}\frac{1}{ \delta}\Big{)}+d\log\Big{(}1+\frac{K}{\lambda}\Big{)}\Big{)}\Big{)}\leq\delta.\]

**Lemma G.4** (Covering number of function class \(\mathcal{V}_{h}\)).: For any \(h\in[H]\), let \(\mathcal{V}_{h}\) denote a class of functions mapping from \(\mathcal{S}\) to \(\mathbb{R}\) with the following parametric form

\[\mathcal{V}_{h}(s)=\max_{a\in\mathcal{A}}\Big{\{}\boldsymbol{\phi}(s,a)^{\top} \boldsymbol{\theta}-\beta\sum_{i=1}^{d}\sqrt{\phi_{i}(s,a)\boldsymbol{1}_{i}^{ \top}\boldsymbol{\Sigma}_{h}^{-1}\phi_{i}(s,a)\boldsymbol{1}_{i}}\Big{\}}_{[0,H -h+1]},\]where the parameters \((\bm{\theta},\beta,\bm{\Sigma}_{h})\) satisfy \(\|\bm{\theta}\|\leq L\), \(\beta\in[0,B]\), \(\lambda_{\min}(\bm{\Sigma}_{h})\geq\lambda\). Assume \(\|\bm{\phi}(s,a)\|\leq 1\) for all (s,a) pairs, and let \(\mathcal{N}_{h}(\epsilon)\) be the \(\epsilon\)-covering number of \(\mathcal{V}\) with respect to the distance \(\operatorname{dist}(V_{1},V_{2})=\sup_{x}|V_{1}(x)-V_{2}(x)|\). Then

\[\operatorname{log}\mathcal{N}_{h}(\epsilon)\leq d\log(1+4L/\epsilon)+d^{2} \log\big{[}1+8d^{1/2}B^{2}/(\lambda\epsilon^{2})\big{]}.\]

**Lemma G.5**.: [47, Covering number of an interval] Denote the \(\epsilon\)-covering number of the closed interval \([a,b]\) for some real number \(b>a\) with respect to the distance metric \(d(\alpha_{1},\alpha_{2})=|\alpha_{1}-\alpha_{2}|\) as \(\mathcal{N}_{\epsilon}([a,b])\). Then we have \(\mathcal{N}_{\epsilon}([a,b])\leq 3(b-a)/\epsilon\).

Proof of Lemma d.2.: To prove this lemma, we bound terms i-iv in Lemma G.1 at each step \(h\in[H]\), respectively. To deal with the temporal dependency, we follow the induction procedure proposed in [54] and make essential adjustments to adapt to the robust setting.

The base case.We start from the last step \(H\). By the fact that any robust value function is upper bounded by \(H\), then with \(\lambda=1\), for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), we have

\[\text{i}+\text{iii}\leq 2H\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \bm{\Lambda}_{h}^{-1}}.\] (G.8)

Next, we bound term ii. Note that \(V_{H+1}^{\star,\rho}\) is independent of \(\mathcal{D}\), we can directly apply Hoeffding-type self-normalized concentration inequality Lemma I.1 and a union bound to obtain the upper bound. In concrete, we define the filtration \(\mathcal{F}_{\tau-1,h}=\sigma(\{(s_{h}^{j},a_{h}^{j})\}_{\tau=1}^{\tau}\cup\{s_ {h+1}^{j}\}_{j=1}^{\tau-1})\). Since \(V_{H+1}^{\star,\rho}\) is independent of \(\mathcal{D}\) and is upper bounded by \(H\), thus we have \(\eta_{H}^{\tau}([V_{H+1}^{\star,\rho}]_{\alpha_{i}})|\mathcal{F}_{\tau-1,H}\) is mean zero, i.e., \(\mathbb{E}[\eta_{H}^{\tau}([V_{H+1}^{\star,\rho}]_{\alpha_{i}})|\mathcal{F}_{ \tau-1,H}]=0\) and \(H\)-subGaussian. By Lemma I.1, for any fixed index \(i\in[d]\), with probability at least \(1-\delta/2dH^{2}\), we have

\[\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{H}^{\tau}\eta_{H}^{\tau}([V_{H+1}^{\star, \rho}]_{\alpha_{i}})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\leq 2H^{2}\log\Big{(} \frac{2dH^{2}\det(\bm{\Lambda}_{h})^{1/2}}{\delta\det(\lambda\mathbf{I})^{1/2} }\Big{)}.\]

By the proof of Lemma B.2 in [15], we know \(\det(\bm{\Lambda}_{h})\leq(\lambda+K)^{d}\). Thus, we have

\[\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{H}^{\tau}\eta_{H}^{\tau}([V_{H+1}^{\star, \rho}]_{\alpha_{i}})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\leq 2H^{2}\Big{(}\frac{d}{2} \log\frac{\lambda+K}{\lambda}+\log\frac{2dH^{2}}{\delta}\Big{)}\leq dH^{2}\log \frac{2dH^{2}K}{\delta}.\]

Then by a union bound over \(i\in[d]\), with probability at least \(1-\delta/2H^{2}\), we have

\[\text{ii}\leq\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i} \|_{\bm{\Lambda}_{h}^{-1}},\] (G.9)

where \(\iota=\log(2dH^{2}K/\delta)\geq 1\). As for the term iv, by construction we have \(V_{H+1}^{\star,\rho}=\widehat{V}_{H+1}^{\rho}=0\) with probability 1. Thus, we trivially have

\[\text{iv}\leq\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i} \|_{\bm{\Lambda}_{h}^{-1}}.\] (G.10)

Combining (G.8), (G.9) and (G.10), for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), with probability at least \(1-\delta/2H^{2}\), we have

\[\Big{|} \inf_{P_{H}(\cdot|s,a)\in\mathcal{U}_{H}^{\rho}(s,a;\bm{\mu}_{H, i}^{0})}[\mathbb{P}_{H}\widehat{V}_{H+1}^{\rho}](s,a)-\widehat{\inf_{P_{H}( \cdot|s,a)\in\mathcal{U}_{H}^{\rho}(s,a;\bm{\mu}_{H,i}^{0})}}[\mathbb{P}_{H} \widehat{V}_{H+1}^{\rho}](s,a)\Big{|}\] \[\leq 4\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{ i}\|_{\bm{\Lambda}_{h}^{-1}}.\] (G.11)

Thus, we define \(\Gamma_{H}(s,a):=4\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_ {i}\|_{\bm{\Lambda}_{h}^{-1}}\). By the definition of \(\widehat{Q}_{H}^{\rho}(s,a)\) in Algorithm 1, we have

\[\widehat{Q}_{H}^{\rho}(s,a)=\big{\{}r_{H}(s,a)-\Gamma_{H}(s,a)\big{\}}_{[0,1]} \leq r_{H}(s,a)=Q_{H}^{\star,\rho}(s,a),\]which implies that a pessimistic estimation is achieved at step \(H\), i.e., \(V^{\star,\rho}_{H}(s)\geq\widehat{V}^{\rho}_{H}(s),\forall s\in\mathcal{S}\). Next, we study \(V^{\star,\rho}_{H}(s)-\widehat{V}^{\rho}_{H}(s)\). The intuition is that given the estimation error bound in (G.11), with sufficient data, the difference between \(V^{\star,\rho}_{H}(s)\) and \(\widehat{V}^{\rho}_{H}(s)\) should be small. Specifically, we have

\[V^{\star,\rho}_{H}(s)-\widehat{V}^{\rho}_{H}(s) =Q^{\star,\rho}_{H}(s,\pi^{\star}_{H}(s))-\widehat{Q}^{\rho}_{H}( s,\pi^{\star}_{H}(s))+\widehat{Q}^{\rho}_{H}(s,\pi^{\star}_{H}(s))-\widehat{Q}^{ \rho}_{H}(s,\hat{\pi}(s))\] \[\leq r_{H}(s,\pi^{\star}_{H}(s))+\inf_{P_{H}(\cdot|s,a)\in \mathcal{U}^{\prime}_{H}(s,a;\mu^{0}_{H,i})}[\mathbb{P}_{H}\widehat{V}^{\rho} _{H+1}](s,a)-\] \[\quad r_{H}(s,\pi^{\star}_{H}(s))-\widetilde{\lim_{P_{H}(\cdot|s, a)\in\mathcal{U}^{\prime}_{H}(s,a;\mu^{0}_{H,i})}}[\mathbb{P}_{H}\widehat{V}^{ \rho}_{H+1}](s,a)+\Gamma_{H}(s,\pi^{\star}_{H}(s))\] \[\leq 2\Gamma_{H}(s,\pi^{\star}_{H}(s)),\] (G.12)

where (G.12) holds by the robust Bellman equation (3.1) and the fact that \(\widehat{Q}^{\rho}_{H}(s,\pi^{\star}_{H}(s))-\widehat{Q}^{\rho}_{H}(s,\hat{ \pi}(s))\leq 0\). Then we bound the pessimism term \(\Gamma_{H}(s,a)\) in terms of the sample size \(K\). By Lemma I.3, when \(K\geq\max\{512\log(2dH^{2}/\delta)/\kappa^{2},4/\kappa\}\), with probability at least \(1-\delta/2H^{2}\), we have

\[2\Gamma_{H}(s,a)=8\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1 }_{i}\|_{\mathbf{A}_{h}^{-1}}\leq\frac{16\sqrt{d}H\sqrt{\iota}}{\sqrt{K}}\sum _{i=1}^{d}\phi_{i}(s,a)\big{(}\tilde{\mathbf{\Lambda}}_{H}^{-1}\big{)}_{ii}^{ 1/2},\]

where \(\tilde{\mathbf{\Lambda}}_{H}=\mathbb{E}^{\pi^{b},P^{0}}[\boldsymbol{\phi}(s_{ H},a_{H})\boldsymbol{\phi}(s_{H},a_{H})^{\top}]\). Note that for any positive definite matrix \(A\), we know \(\lambda_{\min}(A)\leq A_{ii}\leq\lambda_{\max}(A)\). Thus, by Assumption 4.3, we have

\[2\Gamma_{H}(s,a)\leq\frac{16\sqrt{d}H\cdot 1\sqrt{\iota}}{\sqrt{K\kappa}}:=R_{H}.\] (G.13)

To summarize, we define the event

\[\mathcal{E}_{H}=\big{\{}0\leq V^{\star,\rho}_{H}(s)-\widehat{V}^{\rho}_{H}(s) \leq R_{H},\forall s\in\mathcal{S}\big{\}}.\]

Then by a union bound over (G.11) and (G.13), we know \(\mathcal{E}_{H}\) holds with probability at least \(1-\delta_{H}=1-\delta/H^{2}\). This concludes the proof of the base case.

Inductive Hypothesis.Suppose with probability at least \(1-\delta_{h+1}\), we have

\[\Big{|}\inf_{P_{h+1}(\cdot|s,a)\in\mathcal{U}^{\prime}_{h+1}(s,a; \boldsymbol{\mu}^{0}_{h+1,i})}[\mathbb{P}_{h+1}\widehat{V}^{\rho}_{h+2}](s,a)- \widetilde{\inf_{P_{h+1}(\cdot|s,a)\in\mathcal{U}^{\prime}_{h+1}(s,a;\mu^{0}_ {h+1,i})}}[\mathbb{P}_{h+1}\widehat{V}^{\rho}_{h+2}](s,a)\Big{|}\] \[\leq 4\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1 }_{i}\|_{\Lambda^{-1}_{h+1}}:=\Gamma_{h+1}(s,a),\] (G.14)

and

\[\mathcal{E}_{h+1}=\big{\{}0\leq V^{\star}_{h+1}(s)-\widehat{V}_{h+1}(s)\leq R_ {h+1}:=\frac{16\sqrt{d}H(H-h)\sqrt{\iota}}{\sqrt{K\kappa}},\forall s\in \mathcal{S}\big{\}}.\] (G.15)

Inductive Step.Next, we establish the result for step \(h\). First, terms i, ii and iii at step \(h\) can be similarly bounded as in the base case, i.e., we have

\[\text{i}+\text{ii}+\text{iii}\leq 3\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i} (s,a)\mathbf{1}_{i}\|_{\mathbf{\Lambda}_{h}^{-1}},\] (G.16)

with probability at least \(1-\delta/3H^{2}\). It remains to bound the term iv and ensure it is non-dominating. Here, we need to deal with the temporal dependency, as \([\widehat{V}^{\rho}_{h+1}(s)]_{\alpha}-[V^{\star,\rho}_{h+1}(s)]_{\alpha}\) is correlated to \(\{(s^{\star}_{h},a^{\star}_{h},s^{\star}_{h+1})\}_{\tau=1}^{K}\), thus we need a uniform concentration argument. Consider the function class

\[\mathcal{V}_{h}(D,B,\lambda)=\{V_{h}(s;\theta,\boldsymbol{\Sigma}):\mathcal{S} \rightarrow[0,H]\text{ with }\|\theta\|\leq D,\beta\in[0,B],\boldsymbol{\Sigma} \succeq\lambda I\},\]where \(V_{h}(s;\theta,\beta,\bm{\Sigma})=\max_{a\in A}\{\bm{\phi}(s,a)^{\top}\theta- \beta\sum_{d=1}^{d}\sqrt{\phi_{i}(s,a)\bm{1}_{i}^{\top}\bm{\Sigma}^{-1}\phi_{i} (s,a)\bm{1}_{i}\}_{[0,H-h+1]}\). For simplicity, we denote \(f_{\alpha_{i}}(s):=[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{*,\rho} (s)]_{\alpha_{i}}\), then \(f_{\alpha_{i}}\in\mathcal{F}_{h+1}(\alpha_{i})\), where

\[\mathcal{F}_{h+1}(\alpha):=\big{\{}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha}-[V_ {h+1}^{*,\rho}(s)]_{\alpha}:\widehat{V}_{h+1}^{\rho}(s)\in\mathcal{V}_{h+1}(D_ {0},B_{0},\lambda)\big{\}}.\]

Note that for any fixed \(\alpha\), the covering number of \(\mathcal{F}_{h+1}(\alpha)\) is the same as that of \(\mathcal{V}_{h}(D_{0},B_{0},\lambda)\). By Lemma G.2, we have \(D_{0}=H\sqrt{Kd/\lambda}\). By the induction assumption (G.14), we have \(B_{0}=4\sqrt{d}H\sqrt{\iota}\). Denote the \(\epsilon\)-covering of the interval \([0,H]\) with respect to the distance \(\mathrm{dist}(\alpha_{1},\alpha_{2})=|\alpha_{1}-\alpha_{2}|\) as \(\mathcal{N}_{[0,H]}(\epsilon)\), and its \(\epsilon\)-covering number as \(|\mathcal{N}_{[0,H]}(\epsilon)|\). For each \(\alpha\in[0,H]\), we can find \(\alpha_{\epsilon}\in\mathcal{N}_{[0,H]}(\epsilon)\) such that \(|\alpha-\alpha_{\epsilon}|\leq\epsilon\). For any fixed \(\alpha\in[0,H]\), we denote the \(\epsilon\)-covering of \(\mathcal{F}_{h+1}(\alpha)\) with respect to the distance \(\mathrm{dist}(f_{1},f_{2})=\sup_{\tau}|f_{1}(x)-f_{2}(x)|\) as \(\mathcal{N}_{h+1}(\epsilon)\) (short for \(\mathcal{N}_{h+1}(\epsilon;D,B,\lambda)\)) and its \(\epsilon\)-covering number as \(|\mathcal{N}_{h+1}(\epsilon)|\). For each \(f_{\alpha}\in\mathcal{F}_{h+1}(\alpha)\), we can find \(f_{\alpha}^{\epsilon}\in\mathcal{N}_{h+1}(\epsilon)\) such that \(\sup_{s}|f_{\alpha}(s)-f_{\alpha}^{\epsilon}(s)|\leq\epsilon\). It follows that

\[\Big{\|}\sum_{k=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}(f_{ \alpha_{i}})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\cdot\mathds{1}\left\{\|f_{ \alpha_{i}}\|_{\infty}\leq R_{h+1}\right\}\] \[\leq 2\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau} (f_{\alpha_{i}\epsilon})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\cdot\mathds{1} \left\{\|f_{\alpha_{i}}\|_{\infty}\leq R_{h+1}+\epsilon\right\}+2\Big{\|}\sum_ {k=1}^{K}\bm{\phi}_{h}^{\tau}\big{(}\eta_{h}^{\tau}(f_{\alpha_{i}})-\eta_{h}^{ \tau}(f_{\alpha_{i}\epsilon})\big{)}\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}.\]

Note that

\[2\Big{\|}\sum_{k=1}^{K}\bm{\phi}_{h}^{\tau}\big{(}\eta_{h}^{\tau}(f_{\alpha_{i }})-\eta_{h}^{\tau}(f_{\alpha_{i}\epsilon})\big{)}\Big{\|}_{\bm{\Lambda}_{h}^{ -1}}^{2}\leq 2\epsilon^{2}\sum_{\tau,\tau^{\prime}=1}^{K}\left|\bm{\phi}_{h}^{ \tau}\bm{\Lambda}_{h}^{-1}\bm{\phi}_{h}^{\tau^{\prime}}\right|\leq 2\epsilon^{2}K ^{2}/\lambda.\]

Then we have

\[\Big{\|}\sum_{k=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}(f_{\alpha _{i}})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\cdot\mathds{1}\left\{\|f_{\alpha_{i }}\|_{\infty}\leq R_{h+1}\right\}\] \[\leq 4\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau} (f_{\alpha_{i}\epsilon}^{\epsilon})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\cdot \mathds{1}\left\{\|f_{\alpha_{i}\epsilon}^{\epsilon}\|_{\infty}\leq R_{h+1}+2 \epsilon\right\}\] \[\quad+4\Big{\|}\sum_{k=1}^{K}\bm{\phi}_{h}^{\tau}\big{(}\eta_{h}^{ \tau}(f_{\alpha_{i}\epsilon})-\eta_{h}^{\tau}(f_{\alpha_{i}\epsilon}^{\epsilon}) \big{)}\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}+\frac{2\epsilon^{2}K^{2}}{\lambda}\] \[\leq 4\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau} (f_{\alpha_{i}\epsilon}^{\epsilon})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\cdot \mathds{1}\left\{\|f_{\alpha_{i}\epsilon}^{\epsilon}\|_{\infty}\leq R_{h+1}+2 \epsilon\right\}+\frac{6\epsilon^{2}K^{2}}{\lambda},\]

where the last inequality holds by the fact that

\[4\Big{\|}\sum_{k=1}^{K}\bm{\phi}_{h}^{\tau}\big{(}\eta_{h}^{\tau}(f_{\alpha_{i} \epsilon})-\eta_{h}^{\tau}(f_{\alpha_{i}\epsilon}^{\epsilon})\big{)}\Big{\|}_{ \bm{\Lambda}_{h}^{-1}}^{2}\leq 4\epsilon^{2}\sum_{\tau,\tau^{\prime}=1}^{K}\left|\bm{\phi}_{h}^{ \tau}\bm{\Lambda}_{h}^{-1}\bm{\phi}_{h}^{\tau^{\prime}}\right|\leq 4\epsilon^{2}K^{2}/\lambda.\]

With a union bound over \(\mathcal{N}_{h+1}(\epsilon)\) and \(\mathcal{N}_{[0,H]}(\epsilon)\) and by Lemma G.3, we have

\[\mathbb{P}\Bigg{\{}\sup_{\begin{subarray}{c}\alpha_{i}\in\mathcal{N}_{[0,H]} (\epsilon)\\ f_{\alpha_{i}\epsilon}^{\epsilon}\in\mathcal{N}_{h+1}(\epsilon)\end{subarray}} \Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}(f_{\alpha_{i} \epsilon}^{\epsilon})\Big{\|}_{\bm{\Lambda}_{h}^{-1}}^{2}\cdot\mathds{1}\left\{ \|f_{\alpha_{i}\epsilon}^{\epsilon}\|_{\infty}\leq R_{h+1}+2\epsilon\right\}\] \[>(R_{h+1}+2\epsilon)^{2}\Big{(}2\log\frac{3dH^{2}|\mathcal{N}_{h+1} (\epsilon)||\mathcal{N}_{[0,H]}(\epsilon)|}{\delta}+d\log\Big{(}1+\frac{K}{ \lambda}\Big{)}\Big{)}\Bigg{\}}\leq\frac{\delta}{3dH^{2}}.\]

Then with probability at least \(1-\delta/3dH^{2}\), for all \(f_{\alpha_{i}}\in\mathcal{F}_{h+1}(\alpha_{i})\), we have

\[\Big{\|}\sum_{k=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}(f_{\alpha_{i}})\Big{\|}_ {\bm{\Lambda}_{h}^{-1}}^{2}\cdot\mathds{1}\left\{\|f_{\alpha_{i}}\|_{\infty}\leq R _{h+1}\right\}\]\[\leq 4\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i} \|_{\boldsymbol{\Lambda}_{h}^{-1}}:=\Gamma_{h}(s,a).\] (G.18)

Further, when \(K>\max\{512\log(3H^{2}/\delta)/\kappa^{2},4/\kappa\}\), by Lemma I.3, with probability at least \(1-\delta/3H^{2}\), we have

\[\Gamma_{h}(s,a)\leq 4\sqrt{d}H\sqrt{\iota}\sum_{i=1}^{d}\|\phi_{i}(s,a) \mathbf{1}_{i}\|_{\boldsymbol{\Lambda}_{h}^{-1}}\leq\frac{8\sqrt{d}H\sqrt{ \iota}}{\sqrt{K\kappa}}.\] (G.19)

Then by a union bound over (G.18) and (G.19), under the event \(\mathcal{E}_{h+1}\), with probability at least \(1-\delta/H^{2}-\delta_{h+1}\), we have

\[V_{h}^{\star,\rho}(s)-\widehat{V}_{h}^{\rho}(s)\] \[=Q_{h}^{\star,\rho}(s,\pi^{*}(s))-\widehat{Q}_{h}^{\rho}(s,\pi^ {*}(s))+\widehat{Q}_{h}^{\rho}(s,\pi^{*}(s))-\widehat{Q}_{h}^{\rho}(s,\hat{ \pi}(s))\] \[\leq\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a; \boldsymbol{\mu}_{h,i}^{0})}[\mathbb{P}_{h}V_{h+1}^{\star,\rho}](s,\pi^{*}(s) )-\widehat{\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\boldsymbol{ \mu}_{h,i}^{0})}}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)+\Gamma_{h}(s, \pi^{*}(s))\] \[=\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a; \boldsymbol{\mu}_{h,i}^{0})}[\mathbb{P}_{h}V_{h+1}^{\star,\rho}](s,\pi^{*}(s) )-\widehat{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\boldsymbol{\mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)+\Gamma_{h}(s,\pi^{*}(s))\] \[\leq R_{h+1}+2\Gamma_{h}(s,\pi^{*}(s))\] (G.20)\[\leq\Big{|}\big{[}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,2}(\alpha) \big{]}_{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,1}(\alpha)\big{]} _{[0,H]}^{2}-\big{[}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,1}(\alpha)\big{]}_{[0, H]}^{2}-\tilde{O}\Big{(}\frac{\sqrt{d}H^{3}}{\sqrt{K\kappa}}\Big{)}\Big{\}}.\]

Note that the conditional variance estimation does not involve any element of model uncertainty, and thus the proof follows from Lemma 5 of [54]. Recall that we estimate \([\mathbb{V}_{h}[V_{h+1}^{\rho}]_{\alpha}](s,a)\) based on \(\mathcal{D}^{\prime}\) as

\[\widehat{\sigma}_{h}^{2}(s,a;\alpha)=\max\Big{\{}1,\big{[}\bm{ \phi}(s,a)^{\top}\tilde{\beta}_{h,2}(\alpha)\big{]}_{[0,H^{2}]}-\big{[}\bm{ \phi}(s,a)^{\top}\tilde{\beta}_{h,1}(\alpha)\big{]}_{[0,H]}^{2}-\tilde{O}\Big{(} \frac{\sqrt{d}H^{3}}{\sqrt{K\kappa}}\Big{)}\Big{\}}.\]

Note that

\[\leq\Big{|}\big{[}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,2}(\alpha )\big{]}_{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,1}(\alpha) \big{]}_{[0,H]}^{2}-\big{[}\bm{\mathbb{P}}_{h}[\widehat{V}_{h+1}^{{}^{\prime} \rho}]_{\alpha}^{2}\big{]}(s,a)-\big{(}[\bm{\mathbb{P}}_{h}[\widehat{V}_{h+1}^ {{}^{\prime}\rho}]_{\alpha}^{2}\big{)}\big{]}\] \[\leq\underbrace{\bigg{|}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,2}( \alpha)-\big{[}\bm{\mathbb{P}}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{ \alpha}^{2}\big{]}(s,a)\bigg{|}}_{\mathrm{i}}+2H\underbrace{\big{|}\bm{\phi}(s,a)^{\top}\tilde{\beta}_{h,1}(\alpha)-\big{[}\bm{\mathbb{P}}_{h}[\widehat{V}_ {h+1}^{{}^{\prime}\rho}]_{\alpha}\big{]}(s,a)\bigg{|}}_{\mathrm{ii}}.\]

Note that the estimation error \(\mathrm{i}\) and \(\mathrm{ii}\) both come from regular ridge regressions with targets \([\widehat{V}_{h+1}^{{}^{\prime}\rho}(s)]_{\alpha}^{2}\) and \([\widehat{V}_{h+1}^{{}^{\prime}\rho}(s)]_{\alpha}\), respectively. Thus, the analysis is standard and for simplicity we omit the details here and focus on the results: with probability at least \(1-\delta/2\), we have

\[\leq\tilde{O}\Big{(}\frac{dH^{2}}{\sqrt{K\kappa}}\Big{)}.\] (G.22)Then by Theorem 4.4 and Lemma I.3, for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), with probability at least \(1-\delta/2\), we have

\[\big{|}[\mathrm{Var}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{ \alpha}](s,a)-[\mathrm{Var}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}](s,a)\big{|}\] \[\leq\big{|}[\mathbb{P}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{ \alpha}^{2}](s,a)-[\mathbb{P}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}^{*,\rho}]_{ \alpha}(s,a)\big{|}+\big{|}\big{(}[\mathbb{P}_{h}[\widehat{V}_{h+1}^{{}^{\prime }\rho}]_{\alpha}](s,a)\big{)}^{2}-\big{(}[\mathbb{P}_{h}[V_{h+1}^{\star,\rho}] _{\alpha}](s,a)\big{)}^{2}\big{|}\] \[\leq 2H\big{|}\big{[}\mathbb{P}_{h}([\widehat{V}_{h+1}^{\rho}]_{ \alpha}-[V_{h+1}^{\star,\rho}]_{\alpha})\big{]}(s,a)\big{|}+2H\big{|}\big{[} \mathbb{P}_{h}\big{(}[V_{h+1}^{\star,\rho}]_{\alpha}-[V_{h+1}^{\star,\rho}]_{ \alpha}\big{)}\big{]}(s,a)\big{|}\] \[\leq\tilde{O}\Big{(}\frac{\sqrt{d}H^{3}}{\sqrt{K\kappa}}\Big{)}.\] (G.23)

By (G.22) and (G.23) and a union bound, we know that with probability at least \(1-\delta\), we have

\[\Big{|}\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,2}(\alpha)\big{]} _{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,1}(\alpha)\big{]}_{[0,H]}^{2}-[\mathrm{Var}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}](s,a)\Big{|}\] \[\leq\Big{|}\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,2}(\alpha) \big{]}_{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,1}(\alpha) \big{]}_{[0,H]}^{2}-[\mathrm{Var}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{ \alpha}](s,a)\Big{|}\] \[\quad+\big{|}[\mathrm{Var}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho} ]_{\alpha}](s,a)-[\mathrm{Var}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}](s,a)\big{|}\] \[\leq\tilde{O}\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)},\]

which implies that

\[\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,2}(\alpha)\big{]}_{[0,H ^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,1}(\alpha)\big{]}_{[0,H]}^{ 2}-\tilde{O}\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)}\leq[\mathrm{Var}_{h}[V _{h+1}^{\star,\rho}]_{\alpha}](s,a).\]

By the fact that the operator \(\min\{1,\cdot\}\) is order preserving, thus we have

\[\widehat{\sigma}_{h}^{2}(s,a;\alpha)\leq[\mathbb{V}_{h}[V_{h+1}^{ \star,\rho}]_{\alpha}](s,a).\]

Further, by the fact that the operator \(\min\{1,\cdot\}\) is a contraction map, (G.22) and (G.23), we have

\[\big{|}\widehat{\sigma}_{h}^{2}(s,a;\alpha)-\big{[}\mathbb{V}_{h}[ V_{h+1}^{\star,\rho}]_{\alpha}\big{]}(s,a)\big{|}\] \[\leq\Big{|}\widehat{\sigma}_{h}^{2}(s,a;\alpha)-\big{[}\mathbb{V} _{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}\big{]}(s,a)+\big{|}\big{[} \mathbb{V}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}\big{]}(s,a)- \big{[}\mathbb{V}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}\big{]}(s,a)\big{|}\] \[\leq\Big{|}\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,2}(\alpha) \big{]}_{[0,H^{2}]}-\big{[}\bm{\phi}(s,a)^{\top}\bar{\beta}_{h,1}(\alpha) \big{]}_{[0,H]}^{2}-\tilde{O}\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)}-[ \mathrm{Var}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho}]_{\alpha}](s,a)\Big{|}\] \[\quad+\big{|}[\mathrm{Var}_{h}[\widehat{V}_{h+1}^{{}^{\prime}\rho} ]_{\alpha}](s,a)-[\mathrm{Var}_{h}[V_{h+1}^{\star,\rho}]_{\alpha}](s,a)\big{|}\] \[\leq\tilde{O}\Big{(}\frac{dH^{2}}{\sqrt{K\kappa}}\Big{)}+\tilde{O }\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)}+\tilde{O}\Big{(}\frac{\sqrt{d}H^{ 3}}{\sqrt{K\kappa}}\Big{)}\] \[=\tilde{O}\Big{(}\frac{dH^{3}}{\sqrt{K\kappa}}\Big{)}.\]

This concludes the proof. 

### Proof of Lemma e.2

Proof.: Note that the reference-advantage decomposition is exactly the same as that in the proof of Lemma G.1, thus we have

\[\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\prime}(s,a;\bm{\mu}_{h,i}^{0})}[\mathbb{P}_{h}\widehat{V}_{h+1}^{\rho}](s,a)-\widetilde{\inf_{P_{h}( \cdot|s,a)\in\mathcal{U}_{h}^{\prime}(s,a;\bm{\mu}_{h,i}^{0})}}[\mathbb{P}_{h} \widehat{V}_{h+1}^{\rho}](s,a)\] \[\leq\underbrace{\sum_{i=1}^{d}\phi_{i}(s,a)\bm{1}_{i}^{\top}\big{(} \mathbb{E}^{\bm{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\widehat{ \mathbb{E}}^{\bm{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{)}}_{ \text{reference uncertainty}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\phi_{i}(s,a)\bm{1}_{i}^{\top} \big{(}\mathbb{E}^{\bm{\mu}_{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{ \alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{]}-\widehat{\mathbb{E}} ^{\bm{\mu}_{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^ {\star,\rho}(s)]_{\alpha_{i}}\big{]}\big{)}}_{\text{advantage uncertainty}}.\]

Next, we further decompose the reference uncertainty and the advantage uncertainty, respectively.

The Reference Uncertainty.Specifically, we have

\[\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\big{(}\mathbb{E}^{ \boldsymbol{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\widehat{ \mathbb{E}}^{\boldsymbol{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}} \big{)}\] \[=\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\Big{(}\mathbb{E} ^{\boldsymbol{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\mathbf{ \Sigma}_{h}^{-1}(\alpha_{i})\sum_{\tau=1}^{K}\frac{\boldsymbol{\phi}_{h}^{\tau }\big{[}\mathbb{P}_{h}^{0}[V_{h+1}^{\star,\rho}]_{\alpha_{i}}\big{]}(s_{h}^{ \star},a_{h}^{\tau})}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau}; \alpha_{i})}\] \[\quad+\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})\sum_{\tau=1}^{K}\frac{ \boldsymbol{\phi}_{h}^{\tau}\big{[}\mathbb{P}_{h}^{0}[V_{h+1}^{\star,\rho}]_{ \alpha_{i}}\big{]}(s_{h}^{\tau},a_{h}^{\tau})}{\widehat{\sigma}_{h}^{2}(s_{h} ^{\tau},a_{h}^{\tau};\alpha_{i})}-\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})\sum_{ \tau=1}^{K}\frac{\boldsymbol{\phi}_{h}^{\tau}[V_{h+1}^{\star,\rho}(s_{h+1})]_{ \alpha_{i}}}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})} \Big{)}\] \[=\lambda\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\mathbf{ \Sigma}_{h}^{-1}(\alpha_{i})\mathbb{E}^{\boldsymbol{\mu}_{h}^{0}}[V_{h+1}^{ \star,\rho}(s)]_{\alpha_{i}}+\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top} \mathbf{\Sigma}_{h}^{-1}(\alpha_{i})\sum_{\tau=1}^{K}\frac{\boldsymbol{\phi}_ {h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})}{\widehat{ \sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\|\mathbb{E}^{\boldsymbol{\mu}_{h}^{ 0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{ i})}}_{\mathrm{i}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\sum_{\tau=1}^{K}\frac{\boldsymbol {\phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})}{ \widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\Big{\|}_{ \mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}}_{\mathrm{ii}}.\]

The Advantage Uncertainty.Similar to the argument in decomposing the reference uncertainty, we have

\[\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\big{(}\mathbb{E }^{\boldsymbol{\mu}_{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}} -[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{]}-\widehat{\mathbb{E}}^{ \boldsymbol{\mu}_{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_ {h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{]}\big{)}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\mathbb{E}^{\boldsymbol{\mu} _{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho }(s)]_{\alpha_{i}}\big{]}\Big{\|}_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}}_{ \mathrm{iii}}\] \[+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\sum_{\tau=1}^{K}\frac{ \boldsymbol{\phi}_{h}^{\tau}\eta_{h}^{\tau}([\widehat{V}_{h+1}^{\rho}(s)]_{ \alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}})}{\widehat{\sigma}_{h}^{2}(s _{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\Big{\|}_{\mathbf{\Sigma}_{h}^{-1}( \alpha_{i})}}_{\mathrm{iv}}.\]

Put terms i-iv together, we have

\[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\|\mathbb{E}^{\boldsymbol{\mu}_{h}^{ 0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{ i})}}_{\mathrm{i}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\sum_{\tau=1}^{K}\frac{\boldsymbol{ \phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})}{\widehat{ \sigma}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\Big{\|}_{\mathbf{ \Sigma}_{h}^{-1}(\alpha_{i})}}_{\mathrm{ii}}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\mathbb{E}^{\boldsymbol{\mu} _{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}( s)]_{\alpha_{i}}\big{]}\Big{\|}_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}}_{ \mathrm{iii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\sum_{\tau=1}^{K}\frac{\boldsymbol{ \phi}_{h}^{\tau}\eta_{h}^{\tau}([\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+ 1}^{\star,\rho}(s)]_{\alpha_{i}})}{\widehat{\sigma}_{h}^{2}(s_{h}^{\tau},a_{h }^{\tau};\alpha_{i})}\Big{\|}_{\mathbf{\Sigma}_{h}^{-1}(\alpha_{i})}}_{ \mathrm{iv}}.\]By similar argument as Lemma G.1, we know there exist \(\{\tilde{\alpha}_{i}\}_{i\in[d]}\) such that

\[\Big{|}_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h, i}^{0})}[\mathbb{P}_{h}\hat{V}_{h+1}^{\rho}](s,a)-\widehat{\underset{P_{h}(\cdot|s,a) \in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h}^{0})}{\inf}}[\mathbb{P}_{h}\hat{V}_ {h+1}^{\rho}](s,a)\Big{|}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\bm{\Sigma}_{h}^{-1}(\tilde{\alpha}_{i})}\|\mathbb{E}^{\bm{\mu}_{h}^{0}}[V _{h+1}^{\ast,\rho}(s)]_{\alpha_{i}}\|_{\bm{\Sigma}_{h}^{-1}(\tilde{\alpha}_{i })}}_{\text{i}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \bm{\Sigma}_{h}^{-1}(\tilde{\alpha}_{i})}\Big{\|}\sum_{\tau=1}^{K}\frac{\bm{ \phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\ast,\rho}]_{\alpha_{i}})}{\widehat {\partial}_{h}^{2}(s_{h}^{\tau},a_{h}^{\tau};\alpha_{i})}\Big{\|}_{\bm{\Sigma}_ {h}^{-1}(\tilde{\alpha}_{i})}}_{\text{ii}}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\bm{\Sigma}_{h}^{-1}(\alpha_{i})}\Big{\|}\mathbb{E}^{\bm{\mu}_{h}^{0}}[[ \hat{V}_{h+1}^{\rho}(s)]_{\tilde{\alpha}_{i}}-[V_{h+1}^{\ast,\rho}(s)]_{\alpha _{i}}]\Big{\|}_{\bm{\Sigma}_{h}^{-1}(\tilde{\alpha}_{i})}}_{\text{iii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \bm{\Sigma}_{h}^{-1}(\tilde{\alpha}_{i})}\Big{\|}\sum_{\tau=1}^{K}\frac{\bm{ \phi}_{h}^{\tau}\eta_{h}^{\tau}([\widehat{V}_{h+1}^{\rho}(s)]_{\tilde{\alpha} _{i}}-[V_{h+1}^{\ast,\rho}(s)]_{\alpha_{i}})}{\widehat{\partial}_{h}^{2}(s_{h }^{\tau},a_{h}^{\tau};\alpha_{i})}\Big{\|}_{\bm{\Sigma}_{h}^{-1}(\tilde{\alpha }_{i})}}.\]

This concludes the proof. 

### Proof of Lemma e.3

Proof.: By the robust bellman equation (3.1), we know

\[V_{h}^{\pi,\rho}(s)=\mathbb{E}_{a\sim\pi(\cdot|s)}\Big{[}r(s,a)+ \inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h}^{0})}[\mathbb{ P}_{h}V_{h+1}^{\pi,\rho}](s,a)\Big{]}.\] (G.24)

Then, we can trivially bound \(\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)\) as

\[\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)\leq\max_{s,a}\Big{(}1+ \inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h}^{0})}[\mathbb{ P}_{h}V_{h+1}^{\pi,\rho}](s,a)\Big{)}.\] (G.25)

Further, by the definition of the \(d\)-rectangular uncertainty set, we have

\[\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h}^{0})}[\mathbb{ P}_{h}V_{h+1}^{\pi,\rho}](s,a)=\sum_{i=1}^{d}\phi_{i}(s,a)\inf_{\mu_{h,i}: \mathcal{U}_{h,i}^{\rho}(\mu_{h,i}^{0})}\mathbb{E}_{s\sim\mu_{h,i}}[V_{h+1}^{ \pi,\rho}(s)].\] (G.26)

Denoting \(s_{\max}=\operatorname*{argmax}_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)\) and \(s_{\min}=\operatorname*{argmin}_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)\), and for all \(i\in[d]\), we construct a distribution \(\hat{\mu}_{h,i}=(1-\rho)\mu_{h,i}+\rho\delta_{s_{\min}}\), where \(\delta_{x}\) is the Dirac Delta distribution with mass on \(x\). Note that \(\hat{\mu}_{h,i}\in\mathcal{U}_{h,i}^{\rho}(\mu_{h,i}^{0})\), thus we have

\[\inf_{\mu_{h,i}\in\mathcal{U}_{h,i}^{\rho}(\mu_{h,i}^{0})}\mathbb{ E}_{s\sim\mu_{h,i}}[V_{h+1}^{\pi,\rho}(s)]\leq\mathbb{E}_{s\sim\hat{\mu}_{h,i}}[V_{h+1}^{ \pi,\rho}(s)]\leq(1-\rho)\max_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)+\rho\min_{s }V_{h+1}^{\pi,\rho}(s).\] (G.27)

Combining (G.25), (G.26) and (G.27), we have

\[\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)\leq(1-\rho)\max_{s\in \mathcal{S}}V_{h+1}^{\pi,\rho}(s)+\rho\min_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)+1.\] (G.28)

On the other hand, by (G.24), we can trivially bound \(\min_{s}V_{h}^{\pi,\rho}(s)\) as

\[\min_{s}V_{h}^{\pi,\rho}(s)\geq\min_{s,a}\inf_{P_{h}(\cdot|s,a) \in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h}^{0})}[\mathbb{P}_{h}V_{h+1}^{\pi, \rho}](s,a).\] (G.29)

By the fact that

\[\inf_{\mu_{h,i}\in\mathcal{U}_{h,i}^{\rho}(\mu_{h,i}^{0})}\mathbb{ E}_{s\sim\mu_{h,i}}[V_{h+1}^{\pi,\rho}(s)]\geq\min_{s\in\mathcal{S}}V_{h+1}^{\pi, \rho}(s),\] (G.30)combining (G.26), (G.29) and (G.30), we have

\[\min_{s}V_{h}^{\pi,\rho}(s)\geq\min_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s).\] (G.31)

For any \(h\in[H]\), by (G.28) and (G.31), we have

\[\max_{s\in\mathcal{S}}V_{h}^{\pi,\rho}(s)-\min_{s\in\mathcal{S}}V _{h}^{\pi,\rho}(s)\] \[\leq 1+(1-\rho)\max_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)-\min_{s \in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)+\rho\min_{s\in\mathcal{S}}V_{h+1}^{\pi, \rho}(s)\] \[=1+(1-\rho)\big{[}\max_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)- \min_{s\in\mathcal{S}}V_{h+1}^{\pi,\rho}(s)\big{]}.\] (G.32)

For step \(H\), by the definition of the value function, we have \(0\leq V_{H}^{\pi,\rho}(s)\leq 1,\forall s\in\mathcal{S}\). Applying (G.32) with \(h=H-1\) leads to \(\max_{s\in\mathcal{S}}V_{H-1}^{\pi,\rho}(s)-\min_{s\in\mathcal{S}}V_{H-1}^{\pi,\rho}(s)\leq 1+(1-\rho)\cdot 1\). We finish the proof by recursively applying (G.32). 

### Proof of Lemma F.1

Proof.: The proof of Lemma F.1 consists of the following two steps:

Step 1: lower bound the suboptimality by Hamming distance.For any \(\boldsymbol{\xi}\in\{-1,1\}^{dH}\), denote \(V_{\boldsymbol{\xi}}^{\star,\rho}(s)\) as the optimal robust value function for the MDP instance \(M_{\boldsymbol{\xi}}\). For any function \(\pi\), denote \(V_{\boldsymbol{\xi}}^{\pi,\rho}\) as the robust value function corresponding to a policy \(\pi\). Then by definition, we have

\[V_{\boldsymbol{\xi}}^{\star,\rho}(x_{1}) =\max_{\pi}\inf_{P\in\mathcal{U}^{\rho}(P^{0})}\mathbb{E}^{\pi,P }\big{[}r_{1}(s_{1},a_{1})+\dots+r_{H}(s_{H},a_{H})|s_{1}=x_{1}\big{]},\] \[V_{\boldsymbol{\xi}}^{\pi,\rho}(x_{1}) =\inf_{P\in\mathcal{U}^{\rho}(P^{0})}\mathbb{E}^{\pi,P}\big{[}r_{ 1}(s_{1},a_{1})+\dots+r_{H}(s_{H},a_{H})|s_{1}=x_{1}\big{]}.\]

For any given \(\boldsymbol{\xi}\), the optimal action at step \(h\) is

\[a_{h}^{\star}=((1+\xi_{h1})/2,\dots,(1+\xi_{hd})/2).\]

The worst case transition at the first step is known as

\[\mathbb{P}_{1}(x_{1}|x_{1},a)=(1-\rho),\;\mathbb{P}_{1}(x_{2}|x_{1},a)=\rho, \;\mathbb{P}_{1}(x_{2}|x_{2},a)=1,\;\forall a\in\mathcal{A},\]

and from the second step on, the state always stays at \(s_{2}\). With these facts in mind, we have

\[V_{\boldsymbol{\xi}}^{\star,\rho}(x_{1})\] \[=\frac{\delta}{2d}\Big{\{}\Big{[}d+\sum_{i=1}^{d}\frac{1+\xi_{1i }}{2}\Big{]}+(1-\rho)\Big{[}d+\sum_{i=1}^{d}\frac{1+\xi_{2i}}{2}\Big{]}+\dots+ (1-\rho)\Big{[}d+\sum_{i=1}^{d}\frac{1+\xi_{Hi}}{2}\Big{]}\Big{\}}.\]

and

\[V_{\boldsymbol{\xi}}^{\pi,\rho}(x_{1})\] \[=\frac{\delta}{2d}\mathbb{E}^{\pi}\Big{\{}\Big{[}d+\sum_{i=1}^{d} \xi_{1i}a_{1i}\Big{]}+(1-\rho)\Big{[}d+\sum_{i=1}^{d}\xi_{2i}a_{2i}\Big{]} \dots+(1-\rho)\Big{[}d+\sum_{i=1}^{d}\xi_{Hi}a_{Hi}\Big{]}\Big{\}}.\]

Then we have

\[V_{\boldsymbol{\xi}}^{\star,\rho}(x_{1})-V_{\boldsymbol{\xi}}^{ \star,\rho}(x_{1})\] \[=\frac{\delta}{2d}\Big{\{}\Big{[}\sum_{i=1}^{d}\frac{1+\xi_{1i}}{ 2}-\xi_{1i}\mathbb{E}^{\pi}a_{1i}\Big{]}+(1-\rho)\sum_{h=2}^{H}\sum_{i=1}^{d} \Big{(}\frac{1+\xi_{hi}}{2}-\xi_{hi}\mathbb{E}^{\pi}a_{hi}\Big{)}\Big{\}}\] \[\geq\frac{\delta}{2d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}\Big{(} \frac{1+\xi_{hi}}{2}-\xi_{hi}\mathbb{E}^{\pi}a_{hi}\Big{)}\]\[=\frac{\delta}{2d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}\Big{(}\frac{1}{ 2}+\xi_{hi}\mathbb{E}^{\pi}\Big{(}\frac{1}{2}-a_{hi}\Big{)}\Big{)}\] \[=\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}(1-\xi_{hi} \mathbb{E}^{\pi}(2a_{hi}-1)).\] (G.33)

Note that for any \((h,i)\in[H]\times[d]\), by design we have \(1=\xi_{hi}^{2}\), thus

\[\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}(1-\xi_{hi} \mathbb{E}^{\pi}(2a_{hi}-1)) =\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}(\xi_{hi}- \mathbb{E}^{\pi}(2a_{hi}-1))\xi_{hi}\] \[=\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}|\xi_{hi}- \mathbb{E}^{\pi}(2a_{hi}-1)|,\] (G.34)

where (G.34) holds due to the fact that \(\mathbb{E}^{\pi}(2a_{hi}-1)\in[-1,1]\). To continue, we have

\[\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}|\xi_{hi}- \mathbb{E}^{\pi}(2a_{hi}-1)|\] \[\geq\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}|\xi_{hi} -\mathbb{E}^{\pi}(2a_{hi}-1)|\,\mathds{1}\{\xi_{hi}\neq\operatorname{sign}( \mathbb{E}^{\pi}(2a_{h,i}-1))\}\] \[\geq\frac{\delta}{4d}(1-\rho)\sum_{h=1}^{H}\sum_{i=1}^{d}\mathds{1 }\{\xi_{hi}\neq\operatorname{sign}(\mathbb{E}^{\pi}(2a_{h,i}-1))\}\] \[\geq\frac{\delta}{4d}(1-\rho)D_{H}(\bm{\xi},\bm{\xi}^{\pi}),\] (G.35)

where \(D_{H}(\cdot,\cdot)\) is the Hamming distance, \(\bm{\xi}^{\pi}=\{\bm{\xi}_{h}^{\pi}\}_{h\in[H]}\), and \(\xi_{hi}^{\pi}:=\operatorname{sign}(\mathbb{E}^{\pi}(2a_{hi}-1)),\forall i \in[d]\). Combining (G.33), (G.34), (G.35) and the definition of the suboptimality gap, we have

\[\text{SupOpt}(M_{\bm{\xi}},x_{1},\pi,\rho)\geq\frac{\delta}{4d}(1-\rho)D_{H}( \bm{\xi},\bm{\xi}^{\pi}).\] (G.36)

Step 2: lower bound the hamming distance by testing error.Applying Assouad's method [46, Lemma 2.12], we have

\[\inf_{\pi}\sup_{\bm{\xi}\in\Omega}\mathbb{E}_{\bm{\xi}}\big{[}D_{H}(\bm{\xi}, \bm{\xi}^{\prime})\big{]}\geq\frac{dH}{2}\min_{\begin{subarray}{c}\bm{\xi}, \bm{\xi}^{\prime}\in\Omega\\ D_{H}(\bm{\xi},\bm{\xi}^{\prime})=1\end{subarray}}\inf_{\psi}\Big{[}\mathbb{Q} _{\bm{\xi}}(\psi(\mathcal{D})\neq\bm{\xi})+\mathbb{Q}_{\bm{\xi}^{\prime}}(\psi (\mathcal{D})\neq\bm{\xi}^{\prime})\Big{]},\] (G.37)

where \(\inf_{\psi}\) denotes the infimum over all test functions taking values in \(\{\bm{\xi},\bm{\xi}^{\prime}\}\). We conclude the proof by combining (G.36) and (G.37). 

### Proof of Lemma F.2

Proof.: By the Theorem 2.12 in [46], we lower bound the testing error as follows

\[\min_{\bm{\xi},\bm{\xi}^{\prime}:D_{H}(\bm{\xi},\bm{\xi}^{\prime })=1}\inf_{\psi}\Big{[}\mathbb{Q}_{\bm{\xi}}(\psi(\mathcal{D})\neq\bm{\xi})+ \mathbb{Q}_{\bm{\xi}^{\prime}}(\psi(\mathcal{D})\neq\bm{\xi}^{\prime})\Big{]}\] \[\geq 1-\Big{(}\frac{1}{2}\max_{\bm{\xi},\bm{\xi}^{\prime}:D_{H}(\bm {\xi},\bm{\xi}^{\prime})=1}D_{\text{KL}}\big{(}\mathbb{Q}_{\bm{\xi}}|| \mathbb{Q}_{\bm{\xi}^{\prime}}\big{)}\Big{)}^{1/2},\]

where \(D_{\text{KL}}(\cdot||\cdot)\) is the Kullback-Leibler divergence. Then it remains to bound \(D_{\text{KL}}\big{(}\mathbb{Q}_{\bm{\xi}}||\mathbb{Q}_{\bm{\xi}}\big{)}\). According to the definition of \(\mathbb{Q}_{\bm{\xi}}(\mathcal{D})\), we have

\[\mathbb{Q}_{\bm{\xi}}(\mathcal{D})=\prod_{k=1}^{K}\prod_{h=1}^{H}\pi_{h}^{b}(a_{ h}^{k}|s_{h}^{k})P_{h}(s_{h+1}^{k}|s_{h}^{k},a_{h}^{k})R(s_{h}^{k},a_{h}^{k};r_{h}^{k}),\]where \(R(s_{h}^{k},a_{h}^{k};r_{h}^{k})\) is the density function of \(\mathcal{N}(r_{h}(s_{h}^{k},a_{h}^{k}),1)\) at \(r_{h}^{k}\). Note that the difference between the two distribution \(\mathbb{Q}_{\bm{\xi}}(\mathcal{D})\) and \(\mathbb{Q}_{\bm{\xi}^{\prime}}(\mathcal{D})\) lies only in the reward distribution corresponding to the index where \(\bm{\xi}\) and \(\bm{\xi}^{\prime}\) differ. Then, by the chain rule of Kullback-Leibler divergence, we have

\[D_{\text{KL}}\big{(}\mathbb{Q}_{\bm{\xi}}(\mathcal{D})||\mathbb{Q}_{\bm{\xi}^{ \prime}}(\mathcal{D})\big{)}=\sum_{k=1}^{\frac{\mathbb{K}^{2}}{\mathbb{K}^{2} _{2}}}D_{\text{KL}}\Big{(}\mathcal{N}\Big{(}\frac{d+1}{2d}\delta,1\Big{)} \Big{|}\Big{|}\mathcal{N}\Big{(}\frac{d-1}{2d}\delta,1\Big{)}\Big{)}=\frac{K}{ d+2}\frac{\delta^{2}}{d^{2}}.\]

Then by our choice of \(\delta\), we have

\[\min_{\bm{\xi},\bm{\xi}^{\prime}:D_{H}(\bm{\xi},\bm{\xi}^{\prime })=1}\inf\Big{[}\mathbb{Q}_{\bm{\xi}}(\psi(\mathcal{D})\neq\bm{\xi})+\mathbb{ Q}_{\bm{\xi}^{\prime}}(\psi(\mathcal{D})\neq\bm{\xi}^{\prime})\Big{]} \geq 1-\Big{(}\frac{K\delta^{2}}{2(d+2)d^{2}}\Big{)}^{1/2}\] \[\geq 1-\Big{(}\frac{K\delta^{2}}{2d^{3}}\Big{)}^{1/2}\] \[=\frac{1}{2}.\]

This completes the proof. 

### Proof of Lemma f.3

Proof.: Recall that

\[\bm{\Sigma}_{h}^{*-1}=\sum_{k=1}^{K}\frac{\bm{\phi}_{h}^{\tau}\bm{\phi}_{h}^{ \tau\top}}{[\mathbb{V}_{h}V_{h}^{*,\rho}](s_{h}^{k},a_{h}^{\tau})}+\lambda I.\]

We first show that with sufficiently large \(K\), the cliped conditional variances of the optimal robust value functions are always 1. Note that \(V_{h}^{*,\rho}(x_{2})=0,\forall h\in[H]\), and

\[V_{H}^{*,\rho}(x_{1}) =\frac{\delta}{2d}\Big{(}\sum_{i=1}^{d}\frac{1+\xi_{Hi}}{2}+d \Big{)}\leq\delta,\] \[V_{H-1}^{*,\rho}(x_{1}) =\frac{\delta}{2d}\Big{(}\sum_{i=1}^{d}\frac{1+\xi_{H-1i}}{2}+d \Big{)}+V_{H}^{*,\rho}(x_{1})\leq 2\delta,\] \[\cdots\] \[V_{2}^{*,\rho}(x_{1}) =\frac{\delta}{2d}\Big{(}\sum_{i=1}^{d}\frac{1+\xi_{2i}}{2}+d \Big{)}+V_{3}^{*,\rho}(x_{1})\leq(H-1)\cdot\delta.\]

Then, when \(K\geq\Omega(H^{2}d^{3})\), we have

\[\big{[}\operatorname{Var}_{1}V_{2}^{*,\rho}\big{]}(x_{1},a)=\big{[}\mathbb{P} _{1}^{0}(V_{2}^{*,\rho})^{2}\big{]}(x_{1},a)-\big{(}\big{[}\mathbb{P}_{1}^{0 }(V_{2}^{*,\rho})^{2}\big{]}(x_{1},a)\big{)}^{2}\leq(1-\rho)\rho H^{2}\delta^{2 }\leq 1,\]

and by design we have,

\[[\operatorname{Var}_{1}V_{2}^{*,\rho}](x_{2},a)=0\text{ and }[\operatorname{Var}_{h}V_{h+1} ^{*,\rho}](s,a)=0,\forall(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]/\{1\}.\]

Thus, we have \([\mathbb{V}_{h}V_{h}^{*,\rho}](s_{h}^{\tau},a_{h}^{\tau})=1\), which implies

\[\bm{\Sigma}_{h}^{*}=\bm{\Lambda}_{h}.\] (G.38)

Define

\[\tilde{\bm{\Lambda}}_{h}=\mathbb{E}^{\pi^{k},P^{0}}[\bm{\phi}(s_{h},a_{h})\bm{ \phi}(s_{h},a_{h})^{\top}],\]

then by definition we have

\[\tilde{\bm{\Lambda}}_{h}=\frac{1}{d+2}\begin{bmatrix}\frac{1}{d^{2}}&0&\cdots &0&\frac{1}{d}(1-\frac{1}{d})&0\\ 0&0&\cdots&0&0&0\\ \vdots&\vdots&&\vdots&\vdots&\vdots\\ 0&0&\cdots&0&0&0\\ \frac{1}{d}(1-\frac{1}{d})&0&\cdots&0&(1-\frac{1}{d})^{2}&0\\ 0&0&\cdots&0&0&0\end{bmatrix}+\frac{1}{d+2}\begin{bmatrix}0&0&\cdots&0&0&0\\ 0&\frac{1}{d^{2}}&\cdots&0&\frac{1}{d}(1-\frac{1}{d})&0\\ \vdots&\vdots&\vdots&\vdots&\vdots\\ 0&0&\cdots&0&0&0\\ 0&\frac{1}{d}(1-\frac{1}{d})&\cdots&0&(1-\frac{1}{d})^{2}&0\\ 0&0&\cdots&0&0&0\end{bmatrix}\]\[+\cdots+\frac{1}{d+2}\begin{bmatrix}0&0&\cdots&0&0&0\\ 0&0&\cdots&0&0&0\\ \vdots&\vdots&&\vdots&\vdots&\vdots\\ 0&0&\cdots&\frac{1}{d^{2}}&\frac{1}{d}(1-\frac{1}{d})&0\\ 0&0&\cdots&\frac{1}{d}(1-\frac{1}{d})&(1-\frac{1}{d})^{2}&0\\ 0&0&\cdots&0&0&0\end{bmatrix}+\frac{1}{d+2}\begin{bmatrix}0&0&\cdots&0&0&0\\ 0&0&\cdots&0&0&0\\ \vdots&\vdots&&\vdots&\vdots&\vdots\\ 0&0&\cdots&0&0&0\\ 0&0&\cdots&0&1&0\\ 0&0&\cdots&0&0&0\end{bmatrix}\] \[+\frac{1}{d+2}\begin{bmatrix}0&0&\cdots&0&0&0\\ 0&0&\cdots&0&0&0\\ \vdots&\vdots&&\vdots&\vdots&\vdots\\ 0&0&\cdots&0&0&0\\ 0&0&\cdots&0&0&1\end{bmatrix}\] \[=\frac{d}{d+2}\begin{bmatrix}\frac{1}{d^{3}}&0&\cdots&0&\frac{ 1}{d^{2}}(1-\frac{1}{d})&0\\ 0&\frac{1}{d^{3}}&\cdots&0&\frac{1}{d^{2}}(1-\frac{1}{d})&0\\ \vdots&\vdots&&\vdots&\vdots&\vdots\\ 0&0&\cdots&\frac{1}{d^{3}}&\frac{1}{d^{2}}(1-\frac{1}{d})&0\\ \frac{1}{d^{2}}(1-\frac{1}{d})&\frac{1}{d^{2}}(1-\frac{1}{d})&\cdots&\frac{1} {d^{2}}(1-\frac{1}{d})&(1-\frac{1}{d})^{2}+\frac{1}{d}&0\\ 0&0&\cdots&0&0&\frac{1}{d}\end{bmatrix}.\]

Denote

\[D=\begin{bmatrix}\frac{1}{d^{3}}&0&\cdots&0&\frac{1}{d^{2}}(1- \frac{1}{d})\\ 0&\frac{1}{d^{3}}&\cdots&0&\frac{1}{d^{2}}(1-\frac{1}{d})\\ \vdots&\vdots&&\vdots&\vdots\\ 0&0&\cdots&\frac{1}{d^{3}}&\frac{1}{d^{2}}(1-\frac{1}{d})\\ \frac{1}{d^{2}}(1-\frac{1}{d})&\frac{1}{d^{2}}(1-\frac{1}{d})&\cdots&\frac{1} {d^{2}}(1-\frac{1}{d})&(1-\frac{1}{d})^{2}+\frac{1}{d}\end{bmatrix},\]

then by Gaussian elimination, we have

\[D^{-1}=\begin{bmatrix}2d^{3}-2d^{2}+d&d^{3}-2d^{2}+d&\cdots&d^{3}- 2d^{2}+d&d-d^{2}\\ d^{3}-2d^{2}+d&2d^{3}-2d^{2}+d&\cdots&d^{3}-2d^{2}+d&d-d^{2}\\ \vdots&\vdots&&\vdots&\vdots\\ d^{3}-2d^{2}+d&d^{3}-2d^{2}+d&\cdots&2d^{3}-2d^{2}+d&d-d^{2}\\ d-d^{2}&d-d^{2}&\cdots&d-d^{2}&d\end{bmatrix}.\]

Note that

\[\tilde{\Lambda}_{h}=\frac{d}{d+2}\begin{bmatrix}D&0\\ 0&\frac{1}{d}\end{bmatrix},\]

then we have

\[\tilde{\mathbf{\Lambda}}_{h}^{-1}=\frac{d+2}{d}\begin{bmatrix}D^{-1}&0\\ 0&d\end{bmatrix}.\]

Note that \(\lambda_{\min}(D)=O(1/d^{3})\), thus \(\|\tilde{\mathbf{\Lambda}}_{h}^{-1}\|=O(d^{3})\). Then when \(K>\tilde{O}(d^{6})\), for any \((s,a,i,h)\in\mathcal{S}\times\mathcal{A}\times[d]\times[H]\), with probability at least \(1-\delta\), we have

\[\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\mathbf{\Lambda}_{h}^{-1}}\leq \frac{2}{\sqrt{K}}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\tilde{\mathbf{\Lambda}}_{h} ^{-1}}.\] (G.39)

With this in mind, we have

\[\sup_{P\in\mathcal{U}^{\prime}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{ \pi^{*},P}\Big{[}\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\mathbf{\Sigma }_{h}^{-1}}|s_{1}=x_{1}\Big{]}\] \[=\sup_{P\in\mathcal{U}^{\prime}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{ \pi^{*},P}\Big{[}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\mathbf{1}_{i}\|_{ \mathbf{\Lambda}_{h}^{-1}}|s_{1}=x_{1}\Big{]}\]\[\leq\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi^ {*},P}\Big{[}\frac{2}{\sqrt{K}}\sum_{i=1}^{d}\|\phi_{i}(s_{h},a_{h})\bm{1}_{i}\|_{ \tilde{\bm{\Lambda}}_{h}^{-1}}|s_{1}=x_{1}\Big{]}\] (G.40) \[=\sup_{P\in\mathcal{U}^{\rho}(P^{0})}\sum_{h=1}^{H}\mathbb{E}^{\pi ^{*},P}\Big{[}\frac{2}{\sqrt{K}}\sum_{i=1}^{d}\phi_{i}(s_{h},a_{h})\big{(} \tilde{\bm{\Lambda}}_{h}^{-1}\big{)}_{ii}^{1/2}|s_{1}=x_{1}\Big{]}\] \[\leq\frac{4Hd^{3/2}}{\sqrt{K}},\]

where (G.40) is due to (G.39). This concludes the proof. 

## Appendix H Proof of Supporting Lemmas

### Proof of Lemma g.1

To prove Lemma G.1, we need the following proposition on the dual formulation under the TV uncertainty set.

**Proposition H.1**.: (Strong duality for TV [42, Lemma 4]). Given any probability measure \(\mu^{0}\) over \(\mathcal{S}\), a fixed uncertainty level \(\rho\), the uncertainty set \(\mathcal{U}^{\rho}(\mu^{0})=\{\mu:\mu\in\Delta(\mathcal{S}),D_{TV}(\mu||\mu^{ 0})\leq\rho\}\), and any function \(V:\mathcal{S}\to[0,H]\), we obtain

\[\inf_{\mu\in\mathcal{U}^{\rho}(\mu^{0})}\mathbb{E}_{s\sim\mu}V(s)=\max_{ \alpha\in[V_{\min},V_{\max}]}\big{\{}\mathbb{E}_{s\sim\mu^{0}}[V(s)]_{\alpha}- \rho\big{(}\alpha-\min_{s^{\prime}}[V(s^{\prime})]_{\alpha}\big{)}\big{\}},\] (H.1)

where \([V(s)]_{\alpha}=\min\{V(s),\alpha\}\), \(V_{\min}=\min_{s}V(s)\) and \(V_{\max}=\max_{s}V(s)\). Notably, the range of \(\alpha\) can be relaxed to \([0,H]\) without impacting the optimization.

Proof of Lemma g.1.: By Assumption 3.1 and Proposition H.1, we have

\[\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_ {h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\widehat{\inf_{P_{h}( \cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}}[\mathbb{P}_{h} \widehat{V}^{\rho}_{h+1}](s,a)\] \[=\sum_{i=1}^{d}\phi_{i}(s,a)\Big{[}\max_{\alpha\in[0,H]}\{ \mathbb{E}^{\mu^{0}_{h,i}}[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha}-\rho(\alpha- \min_{s^{\prime}}[\widehat{V}^{\rho}_{h+1}(s^{\prime})]_{\alpha})\}\] \[\quad-\max_{\alpha\in[0,H]}\{\widehat{\mathbb{E}}^{\mu^{0}_{h,i}} [\widehat{V}^{\rho}_{h+1}(s)]_{\alpha}-\rho(\alpha-\min_{s^{\prime}}[\widehat{ V}^{\rho}_{h+1}(s^{\prime})]_{\alpha})\}\Big{]}.\]

Denote \(\alpha_{i}=\operatorname{argmax}_{\alpha\in[0,H]}\{\mathbb{E}^{\mu^{0}_{h,i}} [\widehat{V}^{\rho}_{h+1}(s)]_{\alpha}-\rho(\alpha-\min_{s^{\prime}}[\widehat {V}^{\rho}_{h+1}(s^{\prime})]_{\alpha})\}\), then we have

\[\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_ {h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\widehat{\inf_{P_{h}(\cdot |s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}}[\mathbb{P}_{h} \widehat{V}^{\rho}_{h+1}](s,a)\] \[\leq\sum_{i=1}^{d}\phi_{i}(s,a)\big{[}\mathbb{E}^{\mu^{0}_{h,i}} [\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}-\widehat{\mathbb{E}}^{\mu^{0}_{h,i} }[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}\big{]}\] \[=\sum_{i=1}^{d}\phi_{i}(s,a)\big{[}\bm{1}_{i}^{\top}\mathbb{E}^{ \mu^{0}_{h}}[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}-\bm{1}_{i}^{\top} \widehat{\mathbb{E}}^{\mu^{0}_{h}}[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}} \big{]}.\]

Here we do reference-advantage decomposition by using the optimal robust value function as the reference function. Specifically, we have

\[\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{ h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)-\widehat{\inf_{P_{h}(\cdot|s,a) \in\mathcal{U}^{\rho}_{h}(s,a;\bm{\mu}^{0}_{h,i})}}[\mathbb{P}_{h}\widehat{V}^{ \rho}_{h+1}](s,a)\] \[\leq\sum_{i=1}^{d}\phi_{i}(s,a)\big{[}\bm{1}_{i}^{\top}\big{(} \mathbb{E}^{\mu^{0}_{h}}[[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}-[V^{*,\rho}_{ h+1}(s)]_{\alpha_{i}}+[V^{*,\rho}_{h+1}(s)]_{\alpha_{i}}\big{]}\big{)}\] \[\quad-\bm{1}_{i}^{\top}\big{(}\widehat{\mathbb{E}}^{\mu^{0}_{h}} \big{[}[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}-[V^{*,\rho}_{h+1}(s)]_{\alpha_ {i}}+[V^{*,\rho}_{h+1}(s)]_{\alpha_{i}}\big{]}\big{)}\big{]}\]\[=\underbrace{\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\big{(} \mathbb{E}^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\widehat{\mathbb{E }}^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{)}}_{\text{reference uncertainty}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top }\big{(}\mathbb{E}^{\mu_{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_ {i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{]}-\widehat{\mathbb{E}}^{\mu_{ h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star, \rho}(s)]_{\alpha_{i}}\big{]}\big{)}}_{\text{advantage uncertainty}}.\] (H.2)

The Reference Uncertainty.First, we bound the reference uncertainty. Specifically, we have

\[\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\big{(}\mathbb{E }^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\widehat{\mathbb{E}}^{ \mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{)}\] \[=\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\Big{(}\mathbb{E }^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\mathbf{\Lambda}_{h}^{- 1}\sum_{\tau=1}^{K}\phi_{h}^{\tau}\big{[}\mathbb{P}_{h}^{0}[V_{h+1}^{\star,\rho }]_{\alpha_{i}}\big{]}(s_{h}^{\tau},a_{h}^{\tau})\] \[\quad+\mathbf{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\phi_{h}^{\tau} \big{[}\mathbb{P}_{h}^{0}[V_{h+1}^{\star,\rho}]_{\alpha_{i}}\big{]}(s_{h}^{ \tau},a_{h}^{\tau})-\mathbf{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\phi_{h}^{\tau}[ V_{h+1}^{\star,\rho}(s_{h+1}^{\tau})]_{\alpha_{i}}\Big{)}\] \[=\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\Big{(}\mathbb{E }^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\mathbf{\Lambda}_{h}^{- 1}\sum_{\tau=1}^{K}\phi_{h}^{\tau}\phi_{h}^{\tau\top}\mathbb{E}^{\mu_{h}^{0}}[ V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\] \[\quad+\mathbf{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\phi_{h}^{\tau} \big{(}\big{[}\mathbb{P}_{h}^{0}[V_{h+1}^{\star,\rho}]_{\alpha_{i}}\big{]}(s_{ h}^{\tau},a_{h}^{\tau})-[V_{h+1}^{\star,\rho}(s_{h+1}^{\tau})]_{\alpha_{i}} \big{)}\Big{)}.\]

For any function \(f:\mathcal{S}\to[0,H-1]\), we define \(\eta_{h}^{\tau}([f]_{\alpha_{i}})=\big{(}\big{[}\mathbb{P}_{h}^{0}[f]_{\alpha_ {i}}\big{]}(s_{h}^{\tau},a_{h}^{\tau})-[f(s_{h+1}^{\tau})]_{\alpha_{i}}\big{)}\). Then, we have

\[\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\big{(}\mathbb{E }^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}-\widehat{\mathbb{E}}^{ \mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{)}\] \[=\lambda\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\mathbf{ \Lambda}_{h}^{-1}\mathbb{E}^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}} +\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\mathbf{\Lambda}_{h}^{-1} \sum_{\tau=1}^{K}\phi_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_ {i}})\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{ i}\|_{\mathbf{\Lambda}_{h}^{-1}}\|\mathbb{E}^{\mu_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_{ \alpha_{i}}\|_{\mathbf{\Lambda}_{h}^{-1}}}_{\text{i}}+\underbrace{\sum_{i=1}^{ d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\mathbf{\Lambda}_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K} \phi_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})\Big{\|}_{ \mathbf{\Lambda}_{h}^{-1}}}_{\text{ii}}.\] (H.3)

The Advantage Uncertainty.Next, we bound the advantage uncertainty. By similar argument in bounding the reference uncertainty, we have

\[\sum_{i=1}^{d}\phi_{i}(s,a)\mathbf{1}_{i}^{\top}\big{(}\mathbb{ E}^{\mu_{h}^{0}}\big{[}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{ \star,\rho}(s)]_{\alpha_{i}}\big{]}-\widehat{\mathbb{E}}^{\mu_{h}^{0}}\big{[}[ \widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}} \big{]}\big{)}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{ i}\|_{\mathbf{\Lambda}_{h}^{-1}}\Big{\|}\mathbb{E}^{\mu_{h}^{0}}\big{[}[[\widehat{V}_{h+1}^{ \rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}}\big{]}\Big{\|}_{ \mathbf{\Lambda}_{h}^{-1}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \mathbf{\Lambda}_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K}\phi_{h}^{\tau}\eta_{h}^{\tau}( [\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i} })\Big{\|}_{\mathbf{\Lambda}_{h}^{-1}}}_{\text{ii}}.\] (H.4)Combining (H.2), (H.3) and (H.4), we have

\[\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h,i}^{ \rho})}[\mathbb{P}_{\widehat{h}}\widehat{V}_{h+1}^{\rho}](s,a)-\widehat{\inf_{P_ {h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h,i}^{\rho})}}[\mathbb{P}_ {h}\widehat{V}_{h+1}^{\rho}](s,a)\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\Lambda_{h}^{-1}}\|\mathbb{E}^{\bm{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_ {\alpha_{i}}\|_{\Lambda_{h}^{-1}}}_{\text{i}}+\underbrace{\sum_{i=1}^{d}\| \phi_{i}(s,a)\mathbf{1}_{i}\|_{\Lambda_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K}\bm{ \phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}})\Big{\|}_{ \Lambda_{h}^{-1}}}_{\text{ii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \Lambda_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau} ([\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha _{i}})\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iv}}.\]

On the other hand, we can similarly deduce

\[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\Lambda_{h}^{-1}}\|\mathbb{E}^{\bm{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)]_ {\alpha_{i}^{\prime}}\|_{\Lambda_{h}^{-1}}}_{\text{ii}}+\underbrace{\sum_{i=1} ^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\Lambda_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K} \bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{\alpha_{i}^{\prime }})\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{ii}}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\Lambda_{h}^{-1}}\Big{\|}\mathbb{E}^{\bm{\mu}_{h}^{0}}[[\widehat{V}_{h+1 }^{\rho}(s)]_{\alpha_{i}^{\prime}}-[V_{h+1}^{\star,\rho}(s)]_{\alpha_{i}^{ \prime}}]\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \Lambda_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau} ([\widehat{V}_{h+1}^{\rho}(s)]_{\alpha_{i}^{\prime}}-[V_{h+1}^{\star,\rho}(s)] _{\alpha_{i}^{\prime}})\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iv}},\]

where \(\alpha_{i}^{\prime}=\operatorname*{argmax}_{\alpha\in[0,H]}\{\widehat{\mathbb{ E}}^{\mu_{h,i}^{0}}[\widehat{V}_{h+1}^{\rho}(s)]_{\alpha}-\rho(\alpha-\min_{s^{ \prime}}[\widehat{V}_{h+1}^{\rho}(s^{\prime})]_{\alpha})\}\). Then for all \(i\in[d]\), there exist \(\tilde{\alpha}_{i}\in\{\alpha_{i},\alpha_{i}^{\prime}\}\), such that

\[\Big{|}\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{ \mu}_{h,i}^{0})}[\mathbb{P}_{\widehat{h}}\widehat{V}_{h+1}^{\rho}](s,a)- \widehat{\inf_{P_{h}(\cdot|s,a)\in\mathcal{U}_{h}^{\rho}(s,a;\bm{\mu}_{h,i}^{0 })}}[\mathbb{P}_{\widehat{h}}\widehat{V}_{h+1}^{\rho}](s,a)\Big{|}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\Lambda_{h}^{-1}}\|\mathbb{E}^{\bm{\mu}_{h}^{0}}[V_{h+1}^{\star,\rho}(s)] _{\tilde{\alpha}_{i}}\|_{\Lambda_{h}^{-1}}}_{\text{iv}}+\underbrace{\sum_{i=1} ^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{\Lambda_{h}^{-1}}}_{\text{ii}}\Big{\|} \sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}([V_{h+1}^{\star,\rho}]_{ \tilde{\alpha}_{i}})\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{ii}}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i }\|_{\Lambda_{h}^{-1}}\Big{\|}\mathbb{E}^{\bm{\mu}_{h}^{0}}\big{[}[\widehat{V} _{h+1}^{\rho}(s)]_{\tilde{\alpha}_{i}}-[V_{h+1}^{\star,\rho}(s)]_{\tilde{ \alpha}_{i}}\big{]}\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\mathbf{1}_{i}\|_{ \Lambda_{h}^{-1}}\Big{\|}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}\eta_{h}^{\tau}([ \widehat{V}_{h+1}^{\rho}(s)]_{\tilde{\alpha}_{i}}-[V_{h+1}^{\star,\rho}(s)]_{ \tilde{\alpha}_{i}})\Big{\|}_{\Lambda_{h}^{-1}}}_{\text{iv}},\]

This concludes the proof. 

### Proof of Lemma G.2

The proof of Lemma G.2 will use the following fact.

**Lemma H.2**.: [14, Lemma D.1] Let \(\bm{\Lambda}_{t}=\lambda\bm{\mathrm{I}}+\sum_{i=1}^{t}\bm{\phi}_{i}\bm{\phi}_{i}^ {\top}\), where \(\bm{\phi}_{i}\in\mathbb{R}^{d}\) and \(\lambda>0\). Then:

\[\sum_{i=1}^{t}\bm{\phi}_{i}^{\top}(\bm{\Lambda}_{t})^{-1}\bm{\phi}_{i}\leq d.\]

Proof of Lemma G.2.: The proof of Lemma G.2 is similar to that of Lemma E.1 in [20]. Denote \(\alpha_{i}=\operatorname*{argmax}_{\alpha\in[0,H]}\{\hat{z}_{h,i}(\alpha)- \rho(\alpha-\min_{s^{\prime}}[\widehat{V}_{h+1}^{\rho}(s^{\prime})]_{\alpha}) \},i\in[d]\). For any vector \(\bm{v}\in\mathbb{R}^{d}\), we have

\[\big{|}\bm{v}^{\top}\bm{w}_{h}^{\rho}\big{|} =\Big{|}\bm{v}^{\top}\bm{\theta}_{h}+\bm{v}^{\top}\Big{[}\max_{ \alpha\in[0,H]}\{\hat{z}_{h,i}(\alpha)-\rho(\alpha-\min_{s^{\prime}}[\widehat {V}_{h+1}^{\rho}(s^{\prime})]_{\alpha})\}\Big{]}_{i\in[d]}\Big{|}\] \[\leq\sqrt{d}\|\bm{v}\|_{2}+H\|\bm{v}\|_{1}+\bigg{|}\bm{v}^{\top} \bigg{[}\bm{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}[\max_{a} \widehat{Q}_{h+1}^{\rho}(s_{h+1}^{\tau},a)]_{\alpha_{i}}\bigg{)}\bigg{]}_{i\in [d]}\bigg{|}\] (H.5) \[\leq\sqrt{d}\|\bm{v}\|_{2}+H\sqrt{d}\|\bm{v}\|_{2}+\sqrt{\bigg{[} \sum_{\tau=1}^{K}\bm{v}^{\top}\bm{\Lambda}_{h}^{-1}\bm{v}\bigg{]}\bigg{[}\sum _{\tau=1}^{K}(\bm{\phi}_{h}^{\tau})^{\top}\bm{\Lambda}_{h}^{-1}\bm{\phi}_{h}^{ \tau}\bigg{]}}\cdot H\] (H.6) \[\leq 2H\|\bm{v}\|_{2}\sqrt{dK/\lambda}.\] (H.7)

We note that the term \([(\bm{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}[\max_{a}\widehat {Q}_{h+1}^{\rho}(s_{h+1}^{\tau},a)]_{\alpha_{i}})_{i}]_{i\in[d]}\) in (H.5) is constructed by first taking out the \(i\)-th coordinate of the ridge solution vector, \(\bm{\Lambda}_{h}^{-1}\sum_{\tau=1}^{K}\bm{\phi}_{h}^{\tau}[\max_{a}\widehat{Q }_{h+1}^{\rho}(s_{h+1}^{\tau},a)]_{\alpha_{i}}\in\mathbb{R}^{d},\ \forall i\in[d]\), and then concatenating all \(d\) values into a vector. Inequality (H.5) is due to the fact that \(\rho\leq 1\), (H.6) is due to the fact that \(\widehat{Q}_{h+1}^{\rho}\leq H\), and (H.7) is due to Lemma H.2 with \(t=K\) and the fact that the minimum eigenvalue of \(\bm{\Lambda}_{h}\) is lower bounded by \(\lambda\). The remainder of the proof follows from the fact that \(\|\bm{w}_{h}^{\rho}\|_{2}=\max_{\bm{v}:\|\bm{v}\|_{2}=1}|\bm{v}^{\top}\bm{w}_{h }^{\rho}|\).

### Proof of Lemma G.4

The proof of Lemma G.4 will use the following fact.

**Lemma H.3**.: [14, Covering Number of Euclidean Ball] For any \(\epsilon>0\), the \(\epsilon\)-covering number of the Euclidean ball in \(\mathbb{R}^{d}\) with radius \(R>0\) is upper bounded by \((1+2R/\epsilon)^{d}\).

Proof of Lemma G.4.: The proof is similar to the proof of Lemma E.3 in [20]. Denote \(\bm{A}=\beta^{2}\bm{\Sigma}_{h}^{-1}\), so we have

\[\mathcal{V}_{h}(\cdot)=\max_{a\in\mathcal{A}}\Big{\{}\bm{\phi}(s,a)^{\top}\bm {\theta}-\sum_{i=1}^{d}\sqrt{\phi_{i}(s,a)\bm{1}_{i}^{\top}\bm{A}\phi_{i}(s,a) \bm{1}_{i}}\Big{\}}_{[0,H-h+1]},\] (H.8)

for \(\|\bm{\theta}\|\leq L\), \(\|\bm{A}\|\leq B^{2}\lambda^{-1}\). For any two functions \(V_{1},V_{2}\in\mathcal{V}\), let them take the form in (H.8) with parameters \((\bm{\theta}_{1},\bm{A}_{1})\) and \((\bm{\theta}_{2},\bm{A}_{2})\), respectively. Then since both \(\{\cdot\}_{[0,H-h+1]}\) and \(\max_{a}\) are contraction maps, we have

\[\operatorname*{dist}(V_{1},V_{2}) \leq\sup_{x,a}\left|\left[\bm{\theta}_{1}^{\top}\bm{\phi}(x,a)- \sum_{i=1}^{d}\sqrt{\phi_{i}(x,a)\bm{1}_{i}^{\top}\bm{A}_{1}\phi_{i}(x,a)\bm{1} _{i}}\right]\right.\] \[\qquad\left.-\left[\bm{\theta}_{2}^{\top}\bm{\phi}(x,a)-\sum_{i=1 }^{d}\sqrt{\phi_{i}(x,a)\bm{1}_{i}^{\top}\bm{A}_{2}\phi_{i}(x,a)\bm{1}_{i}} \right]\right|\] \[\leq\sup_{\bm{\phi}:\|\bm{\phi}\|\leq 1}\left|\left[\bm{\theta}_{1}^{ \top}\bm{\phi}-\sum_{i=1}^{d}\sqrt{\phi_{i}\bm{1}_{i}^{\top}\bm{A}_{1}\phi_{i} \bm{1}_{i}}\right]-\left[\bm{\theta}_{2}^{\top}\bm{\phi}-\sum_{i=1}^{d}\sqrt{ \phi_{i}\bm{1}_{i}^{\top}\bm{A}_{2}\phi_{i}\bm{1}_{i}}\right]\right|\] \[\leq\sup_{\bm{\phi}:\|\bm{\phi}\|\leq 1}\left|(\bm{\theta}_{1}-\bm{ \theta}_{2})^{\top}\bm{\phi}\right|+\sup_{\bm{\phi}:\|\bm{\phi}\|\leq 1}\sum_{i=1}^{d}\sqrt{\phi_{i}\bm{1}_{i}^{ \top}(\bm{A}_{1}-\bm{A}_{2})\phi_{i}\bm{1}_{i}}\] (H.9)\[\leq\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|+\sqrt{\| \boldsymbol{A}_{1}-\boldsymbol{A}_{2}\|}\sup_{\boldsymbol{\phi}:\|\boldsymbol{ \phi}\|\leq 1}\sum_{i=1}^{d}\|\phi_{i}\boldsymbol{1}_{i}\|\] \[\leq\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|+\sqrt{\| \boldsymbol{A}_{1}-\boldsymbol{A}_{2}\|}_{F},\] (H.10)

where (H.9) follows from triangular inequality and the fact that \(|\sqrt{x}-\sqrt{y}|\leq\sqrt{|x-y|},\ \forall x,y\geq 0\). For matrices, \(\|\cdot\|\) and \(\|\cdot\|_{F}\) denote the matrix operator norm and Frobenius norm respectively.

Let \(\mathcal{C}_{\boldsymbol{\theta}}\) be an \(\epsilon/2\)-cover of \(\{\boldsymbol{\theta}\in\mathbb{R}^{d}\|\|\boldsymbol{\theta}\|_{2}\leq L\}\) with respect to the 2-norm, and \(\mathcal{C}_{A}\) be an \(\epsilon^{2}/4\)-cover of \(\{A\in\mathbb{R}^{d\times d}|\|A\|_{F}\leq d^{1/2}B^{2}\lambda^{-1}\}\) with respect to the Frobenius norm. By Lemma H.3, we know:

\[\big{|}\mathcal{C}_{\boldsymbol{\theta}}\big{|}\leq\big{(}1+4L/\epsilon\big{)} ^{d},\quad\big{|}\mathcal{C}_{A}\big{|}\leq\big{[}1+8d^{1/2}B^{2}/(\lambda \epsilon^{2})\big{]}^{d^{2}}.\]

By (H.10), for any \(V_{1}\in\mathcal{V}\), there exists \(\boldsymbol{\theta}_{2}\in\mathcal{C}_{\boldsymbol{\theta}}\) and \(A_{2}\in\mathcal{C}_{A}\) such that \(V_{2}\) parametrized by \((\boldsymbol{\theta}_{2},A_{2})\) satisfies \(\mathrm{dist}(V_{1},V_{2})\leq\epsilon\). Hence, it holds that \(\mathcal{N}_{\epsilon}\leq|\mathcal{C}_{\boldsymbol{\theta}}|\cdot|\mathcal{ C}_{A}|\), which leads to

\[\log\mathcal{N}_{\epsilon}\leq\log|\mathcal{C}_{\mathbf{w}}|+\log|\mathcal{C}_{A}|\leq d\log(1+4L/\epsilon)+d^{2}\log\big{[}1+8d^{1/ 2}B^{2}/(\lambda\epsilon^{2})\big{]}.\]

This concludes the proof. 

### Proof of Theorem b.1

In this section, we give the proof of Theorem B.1, which largely follows the proof of Theorem 5.2, only with minor modifications of the argument of the variance estimation.

The following lemma bounds the estimation error by reference-advantage decomposition.

**Lemma H.4** (Modified Variance-Aware Reference-Advantage Decomposition).: There exist \(\{\alpha_{i}\}_{i\in[d]}\), where \(\alpha_{i}\in[0,H],\forall i\in[d]\), such that

\[\Big{|}\inf_{P_{h}(\cdot\,|s,a)\in\mathcal{U}^{\rho}_{h}(s,a; \boldsymbol{\mu}^{0}_{h,i})}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)- \widetilde{\inf_{P_{h}(\cdot\,|s,a)\in\mathcal{U}^{\rho}_{h}(s,a;\boldsymbol {\mu}^{0}_{h,i})}}[\mathbb{P}_{h}\widehat{V}^{\rho}_{h+1}](s,a)\Big{|}\] \[\leq\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{1} _{i}\|_{\boldsymbol{\Sigma}^{-1}_{h}}\|\mathbb{E}^{\boldsymbol{\mu}^{0}_{h}}[ V^{\star,\rho}_{h+1}(s)]_{\alpha_{i}}\|_{\boldsymbol{\Sigma}^{-1}_{h}}}_{i}+ \underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{1}_{i}\|_{\boldsymbol{ \Sigma}^{-1}_{h}}\Big{\|}\sum_{\tau=1}^{K}\frac{\boldsymbol{\phi}^{\tau}_{h} \eta^{\tau}_{h}([V^{\star,\rho}_{h+1}]_{\alpha_{i}})}{\widehat{\sigma}^{2}_{h} (s^{\tau}_{h},a^{\tau}_{h})}\Big{\|}_{\boldsymbol{\Sigma}^{-1}_{h}}}_{i}\] \[\quad+\underbrace{\lambda\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{ 1}_{i}\|_{\boldsymbol{\Sigma}^{-1}_{h}}\Big{\|}\mathbb{E}^{\boldsymbol{\mu}^{0}_ {h}}\big{[}[\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}-[V^{\star,\rho}_{h+1}( s)]_{\alpha_{i}}\big{]}\Big{\|}_{\boldsymbol{\Sigma}^{-1}_{h}}}_{i\text{ii}}\] \[\quad+\underbrace{\sum_{i=1}^{d}\|\phi_{i}(s,a)\boldsymbol{1}_{i} \|_{\boldsymbol{\Sigma}^{-1}_{h}}\Big{\|}\sum_{\tau=1}^{K}\frac{\boldsymbol{ \phi}^{\tau}_{h}\eta^{\tau}_{h}([\widehat{V}^{\rho}_{h+1}(s)]_{\alpha_{i}}-[V ^{\star,\rho}_{h+1}(s)]_{\alpha_{i}})}{\widehat{\sigma}^{2}_{h}(s^{\tau}_{h},a ^{\tau}_{h})}\Big{\|}_{\boldsymbol{\Sigma}^{-1}_{h}}}_{\text{ii}},\]

where \(\eta^{\tau}_{h}([f]_{\alpha_{i}})=\big{(}\big{[}\mathbb{P}^{0}_{h}[f]_{\alpha_ {i}}\big{]}(s^{\tau}_{h},a^{\tau}_{h})-[f(s^{\tau}_{h+1})]_{\alpha_{i}}\big{)}\), for any function \(f:\mathcal{S}\to[0,H-1]\).

Proof of Theorem b.1.: To prove this theorem, we bound the estimation error by \(\Gamma_{h}(s,a)\), then invoke Lemma D.1 to get the results. First, we bound terms i-iv in Lemma H.4 at each step \(h\in[H]\) respectively to deduce \(\Gamma_{h}(s,a)\).

Bound i and iii:We set \(\lambda=1/H^{2}\) to ensure that for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), we have

\[\text{i}+\text{iii}\leq\sqrt{\lambda}\sqrt{d}H\sum_{i=1}^{d}\|\phi_{i}(s,a) \boldsymbol{1}_{i}\|_{\boldsymbol{\Sigma}^{-1}_{h}}=\sqrt{d}\sum_{i=1}^{d}\| \phi_{i}(s,a)\boldsymbol{1}_{i}\|_{\boldsymbol{\Sigma}^{-1}_{h}}.\] (H.11)

Bound ii:For all \((s,a,\alpha)\in\mathcal{S}\times\mathcal{A}\times[0,H]\), by definition we have \(\widehat{\sigma}_{h}(s,a)\geq 1\). Thus, for all \((h,\tau,i)\in[H]\times[K]\times[d]\), we have \(\eta^{\tau}_{h}([V^{\star,\rho}_{h+1}]_{\alpha_{i}})/\widehat{\sigma}_{h}(s^{ \tau}_{h},a^{\tau}_{h})\leq H\). Note that \(V^{\star,\rho}_{H+1}\) is independent of \(\mathcal{D}\), we can directly apply Bernstein-type self-normalized concentration inequality 

[MISSING_PAGE_FAIL:45]

Auxiliary Lemmas

**Lemma I.1** (Concentration of Self-Normalized Processes).: [1, Theorem 1] Let \(\{\epsilon_{t}\}_{t=1}^{\infty}\) be a real-valued stochastic process with corresponding filtration \(\{\mathcal{F}_{t}\}_{t=0}^{\infty}\). Let \(\epsilon_{t}|\mathcal{F}_{t-1}\) be mean-zero and \(\sigma\)-subGaussian; i.e. \(\mathbb{E}[\epsilon_{t}|\mathcal{F}_{t-1}]=0\), and

\[\forall\lambda\in\mathbb{R},\quad\mathbb{E}[e^{\lambda\epsilon_{t}}|\mathcal{ F}_{t-1}]\leq e^{\lambda^{2}\sigma^{2}/2}.\]

Let \(\{\bm{\phi}_{t}\}_{t=1}^{\infty}\) be an \(\mathbb{R}^{d}\)-valued stochastic process where \(\bm{\phi}_{t}\) is \(\mathcal{F}_{t-1}\) measurable. Assume \(\mathbf{\Lambda}_{0}\) is a \(d\times d\) positive definite matrix, and let \(\mathbf{\Lambda}_{t}=\mathbf{\Lambda}_{0}+\sum_{s=1}^{t}\bm{\phi}_{s}\bm{\phi }_{s}^{\top}\). Then for any \(\delta>0\), with probability at least \(1-\delta\), we have for all \(t\geq 0\):

\[\bigg{\|}\sum_{s=1}^{t}\bm{\phi}_{s}\epsilon_{s}\bigg{\|}_{\mathbf{\Lambda}_{ t}^{-1}}^{2}\leq 2\sigma^{2}\log\bigg{[}\frac{\det(\mathbf{\Lambda}_{t})^{1/2} \det(\mathbf{\Lambda}_{0})^{-1/2}}{\delta}\bigg{]}.\]

**Lemma I.2** (Bernstein inequality for self-normalized martingales).: [65, Theorem 2] Let \(\{\eta_{t}\}_{t=1}^{\infty}\) be a real-valued stochastic process. Let \(\{\mathcal{F}_{t}\}_{t=0}^{\infty}\) be a filtration, such that \(\eta_{t}\) is \(\mathcal{F}_{t}\)-measurable. Assume \(\eta_{t}\) also satisfies

\[|\eta_{t}|\leq R,\mathbb{E}[\eta_{t}|\mathcal{F}_{t-1}]=0,\mathbb{E}[\eta_{t} ^{2}|\mathcal{F}_{t-1}]\leq\sigma^{2}.\]

Let \(\{\bm{x}_{t}\}_{t=1}^{\infty}\) be an \(\mathbb{R}^{d}\)-valued stochastic process where \(\bm{x}_{t}\) is \(\mathcal{F}_{t-1}\) measurable and \(\|\bm{x}_{t}\|\leq L\). Let \(\mathbf{\Lambda}_{t}=\lambda\mathbf{I}_{d}+\sum_{s=1}^{t}\bm{x}_{s}\bm{x}_{s} ^{\top}\). Then for any \(\delta>0\), with probability at least \(1-\delta\), for all \(t>0\),

\[\bigg{\|}\sum_{s=1}^{t}\bm{x}_{s}\eta_{s}\bigg{\|}_{\mathbf{\Lambda}_{t}^{-1} }\leq 8\sigma\sqrt{d\log\left(1+\frac{tL^{2}}{\lambda d}\right)\cdot\log \left(\frac{4t^{2}}{\delta}\right)}+4R\log\left(\frac{4t^{2}}{\delta}\right).\]

**Lemma I.3**.: [27, Lemma H.5] Let \(\bm{\phi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{d}\) satisfying \(\|\bm{\phi}(x,a)\|\leq C\) for all \((x,a)\in\mathcal{S}\times\mathcal{A}\). For any \(K>0\) and \(\lambda>0\), define \(\overline{\mathbb{G}}_{K}=\sum_{k=1}^{K}\bm{\phi}(x_{k},a_{k})\bm{\phi}(x_{k},a _{k})^{\top}+\lambda\mathbf{I}_{d}\) where \((x_{k},a_{k})\)'s are i.i.d. samples from some distribution \(\nu\) over \(\mathcal{S}\times\mathcal{A}\). Let \(\mathbb{G}=\mathbb{E}_{v}[\bm{\phi}(x,a)\bm{\phi}(x,a)^{\top}]\). Then, for any \(\delta\in(0,1)\), if \(K\) satisfies that

\[K\geq\max\Big{\{}512C^{4}\big{\|}\mathbb{G}^{-1}\big{\|}^{2}\log\Big{(}\frac{2 d}{\delta}\Big{)},4\lambda\big{\|}\mathbb{G}^{-1}\big{\|}\Big{\}},\]

then with probability at least \(1-\delta\), it holds simultaneously for all \(u\in\mathbb{R}^{d}\) that

\[\|u\|_{\overline{G}_{K}^{-1}}\leq\frac{2}{\sqrt{K}}\|u\|_{\mathbb{G}^{-1}}.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We accurately summarize the paper's contributions and scope in the abstract and introduction (Section 1). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 6, we claim that there are small gaps between the upper bounds in Theorem 4.4 and Theorem 5.2 and lower bound in Theorem 6.1. The computation tractability is discussed in Remark 4.2 in Section 4. The assumptions are formally stated in Assumption 3.1 and Assumption 4.3. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions are formally stated in Assumption 3.1 and Assumption 4.3. Complete proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The code of our implementation is available at https://github.com/panxulab/Offline-Linear-DRMDP. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code of our implementation is available at https://github.com/panxulab/Offline-Linear-DRMDP. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The code of our implementation is available at https://github.com/panxulab/Offline-Linear-DRMDP. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Not applicable to our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All information is provided in the experiment section and the code of our implementation is available at https://github.com/panxulab/Offline-Linear-DRMDP. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: the authors had reviewed the NeurIPS Code of Ethics and confirm that the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: this work focuses on the theoretical side of robust RL, and methods in this paper do not lead to a direct path to any negative applications. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: the paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.