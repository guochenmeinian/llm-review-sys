# Efficiently Parameterized Neural Metriplectic Systems

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Metriplectic systems are learned from data in a way that scales quadratically in both the size of the state and the rank of the metriplectic data. Besides being provably energy conserving and entropy stable, the proposed approach comes with approximation results demonstrating its ability to accurately learn metriplectic dynamics from data as well as an error estimate indicating its potential for generalization to unseen timescales when approximation error is low. Examples are provided which illustrate performance in the presence of both full state information as well as when entropic variables are unknown, confirming that the proposed approach exhibits superior accuracy and scalability without compromising on model expressivity.

## 1 Introduction

The theory of metriplectic, also called GENERIC, systems [1; 2] provides a principled formalism for encoding dissipative dynamics in terms of complete thermodynamical systems that conserve energy and produce entropy. By formally expressing the reversible and irreversible parts of state evolution with separate algebraic brackets, the metriplectic formalism provides a general mechanism for maintaining essential conservation laws while simultaneously respecting dissipative effects. Thermodynamic completeness implies that any dissipation is caught within a metriplectic system through the generation of entropy, allowing for a holistic treatment which has already found use in modeling fluids [3; 4], plasmas [5; 6], and kinetic theories [7; 8].

From a machine learning point of view, the formal separation of conservative and dissipative effects makes metriplectic systems highly appealing for the development of phenomenological models. Given data which is physics-constrained or exhibits some believed properties, a metriplectic system can be learned to exhibit the same properties with clearly identifiable conservative and dissipative parts. This allows for a more nuanced understanding of the governing dynamics via an evolution equation which reduces to an idealized Hamiltonian system as the dissipation is taken to zero. Moreover, elements in the kernel of the learned conservative part are immediately understood as Casimir invariants, which are special conservation laws inherent to the phase space of solutions, and are often useful for understanding and exerting control on low-dimensional structure in the system. On the other hand, the same benefit of metriplectic structure as a "direct sum" of reversible and irreversible parts makes it challenging to parameterize in an efficient way, since delicate degeneracy conditions must be enforced in the system for all time. In fact, there are no methods at present for learning general metriplectic systems which scale optimally with both dimension and the rank of metriplectic data--an issue which this work directly addresses.

Precisely, metriplectic dynamics on the finite or infinite dimensional phase space \(\mathcal{P}\) are generated by a free energy function(al) \(F:\mathcal{P}\rightarrow\mathbb{R}\), \(F=E+S\) defined in terms of a pair \(E,S:\mathcal{P}\rightarrow\mathbb{R}\) representing energy and entropy, respectively, along with two algebraic brackets \(\{\cdot,\cdot\},[\cdot,\cdot]:C^{\infty}(\mathcal{P})\times C^{\infty}(\mathcal{ P})\rightarrow{C^{\infty}(\mathcal{P})}\) which are bilinear derivations on \(C^{\infty}(\mathcal{P})\) with prescribed symmetries and degeneracies \(\{S,\cdot\}=[E,\cdot]=0\). Here \(\{\cdot,\cdot\}\) is an antisymmetric Poisson bracket, which is a Lie algebra realization on functions, and \([\cdot,\cdot]\) is a degenerate metric bracket which is symmetric and positivesemi-definite. When \(\mathcal{P}\subset\mathbb{R}^{n}\) for some \(n>0\), these brackets can be identified with symmetric matrix fields \(\mathbf{L}:\mathcal{P}\rightarrow\mathrm{Skew}_{n}(\mathbb{R}),\mathbf{M}:\mathcal{P} \rightarrow\mathrm{Sym}_{n}(\mathbb{R})\) satisfying \(\{F,G\}=\nabla F\cdot\mathbf{L}\nabla G\) and \([F,G]=\nabla F\cdot M\nabla G\) for all functions \(F,G\in C^{\infty}(\mathcal{P})\) and all states \(\mathbf{x}\in\mathcal{P}\). Using the degeneracy conditions along with \(\nabla\mathbf{x}=\mathbf{I}\) and abusing notation slightly then leads the standard equations for metriplectic dynamics,

\[\dot{\mathbf{x}}=\{\mathbf{x},F\}+[\mathbf{x},F]=\{\mathbf{x},E\}+[\mathbf{x},S]=\mathbf{L}\nabla E+\bm {M}\nabla S,\]

which are provably energy conserving and entropy producing. To see why this is the case, recall that \(\mathbf{L}^{\intercal}=-\mathbf{L}\). It follows that the infinitesimal change in energy satisfies

\[\dot{E}=\dot{\mathbf{x}}\cdot\nabla E=\mathbf{L}\nabla E\cdot\nabla E+\mathbf{M}\nabla S \cdot\nabla E=-\mathbf{L}\nabla E\cdot\nabla E+\nabla S\cdot\mathbf{M}\nabla E=0,\]

and therefore energy is conserved along the trajectory of \(\mathbf{x}\). Similarly, the fact that \(\mathbf{M}^{\intercal}=\mathbf{M}\) is positive semi-definite implies that

\[\dot{S}=\dot{\mathbf{x}}\cdot\nabla S=L\nabla E\cdot\nabla S+\mathbf{M}\nabla S\cdot \nabla S=-\nabla E\cdot\mathbf{L}\nabla S+\mathbf{M}\nabla S\cdot\nabla S=\left|\nabla S \right|_{\mathbf{M}}^{2}\geq 0,\]

so that entropy is nondecreasing along \(\mathbf{x}\) as well. Geometrically, this means that the motion of a trajectory \(\mathbf{x}\) is everywhere tangent to the level sets of energy and transverse to those of entropy, reflecting the fact that metriplectic dynamics are a combination of noncanonical Hamiltonian (\(\mathbf{M}=\mathbf{0}\)) and generalized gradient (\(\mathbf{L}=\mathbf{0}\)) motions. Note that these considerations also imply the Lyapunov stability of metriplectic trajectories, as can be seen by taking \(E\) as a Lyapunov function. Importantly, this also implies that metriplectic trajectories which start in the (often compact) set \(K=\{\mathbf{x}\,|\,E(\mathbf{x})\leq E(\mathbf{x}_{0})\}\) remain there for all time.

In phenomenological modeling, the entropy \(S\) is typically chosen from Casimirs of the Poisson bracket generated by \(\mathbf{L}\), i.e. those quantities \(C\in C^{\infty}(\mathcal{P})\) such that \(\mathbf{L}\nabla C=\mathbf{0}\). On the other hand, the method which will be presented here, termed neural metriplectic systems (NMS), allows for all of the metriplectic data \(\mathbf{L},\mathbf{M},E,S\) to be approximated simultaneously, removing the need for Casimir invariants to be known or assumed ahead of time. The only restriction inherent to NMS is that the metriplectic system being approximated is nondegenerate (c.f. Definition 3.1), a mild condition meaning that the gradients of energy and entropy cannot vanish at any point \(\mathbf{x}\in\mathcal{P}\) in the phase space. It will be shown that NMS alleviates known issues with previous methods for metriplectic learning, leading to easier training, superior parametric efficiency, and better generalization performance.

Contributions.The proposed NMS method for learning metriplectic models offers the following advantages over previous state-of-the-art: **(1)** It approximates arbitrary nondegenerate metriplectic dynamics with optimal quadratic scaling in both the problem dimension \(n\) and the rank \(r\) of the irreversible dynamics. **(2)** It produces realistic, thermodynamically consistent entropic dynamics from unobserved entropy data. **(3)** It admits universal approximation and error accumulation results given in Proposition 3.7 and Theorem 3.9. **(4)** It yields exact energy conservation and entropy stability by construction, allowing for effective generalization to unseen timescales.

## 2 Previous and Related Work

Previous attempts to learn metriplectic systems from data separate into "hard" and "soft" constrained methods. Hard constrained methods enforce metriplectic structure by construction, so that the defining properties of metriplecticity cannot be violated. Conversely, methods with soft constraints relax some aspects of metriplectic structure in order to produce a wider model class which is easier to parameterize. While hard constraints are the only way to truly guarantee appropriate generalization in the learned surrogate, the hope of soft constrained methods is that the resulting model is "close enough" to metriplectic that it will exhibit some of the favorable characteristics of metriplectic systems, such as energy and entropy stability. Some properties of the methods compared in this work are summarized in Table 1.

Soft constrained methods.Attempts to learn metriplectic systems using soft constraints rely on relaxing the degeneracy conditions \(\mathbf{L}\nabla S=\mathbf{M}\nabla E=\mathbf{0}\). This is the approach taken in [9], termed SPNN, which learns an almost-metriplectic model parameterized with generic neural networks through a simple \(L^{2}\) penalty term in the training loss, \(\mathcal{L}_{\mathrm{pen}}=|\mathbf{L}\nabla E|^{2}+|\mathbf{M}\nabla S|^{2}\). This widens the space of allowable network parameterizations for the approximate operators \(\mathbf{L},\mathbf{M}\). Whilethe resulting model violates the first and second laws of thermodynamics, the authors show that reasonable trajectories are still obtained, at least when applied within the range of timescales used for training. A similar approach is taken in [10], which targets larger problems and develops an almost-metriplectic model reduction strategy based on the same core idea.

Hard constrained methods.Perhaps the first example of learning metriplectic systems from data was given in [11] in the context of system identification. Here, training data is assumed to come from a finite element simulation, so that the discrete gradients of energy and entropy can be approximated as \(\nabla E(\mathbf{x})=\mathbf{A}\mathbf{x},\nabla S(\mathbf{x})=\mathbf{B}\mathbf{x}\). Assuming a fixed form for \(\mathbf{L}\) produces a constrained learning problem for the constant matrices \(\mathbf{M},\mathbf{A},\mathbf{B}\) which is solved to yield a provably metriplectic surrogate model. Similarly, the work [12] learns \(\mathbf{M},E\) given \(\mathbf{L},S\) by considering a fixed block-wise decoupled form which trivializes the degeneracy conditions, i.e. \(\mathbf{L}=[\star\mathbf{0};\mathbf{0}\;\mathbf{0}]\) and \(\mathbf{M}=[\mathbf{0}\;\mathbf{0};\mathbf{0}\;\star]\). This line of thought is continued in [13] and [14], both of which learn metriplectic systems with neural network parameterizations by assuming this decoupled block structure. A somewhat broader class of metriplectic systems are considered in [15] using tools from exterior calculus, with the goal of learning metriplectic dynamics on graph data. This leads to a structure-preserving network surrogate which scales linearly in the size of the graph domain, but also cannot express arbitrary metriplectic dynamics due to the specific choices of model form for \(\mathbf{L},\mathbf{M}\).

A particularly inspirational method for learning general metriplectic systems was given in [16] and termed GNODE, building on parameterizations of metriplectic operators developed in [17]. GNODE parameterizes learnable reversible and irreversible bracket generating matrices \(\mathbf{L},\mathbf{M}\) in terms of state-independent tensors \(\mathbf{\xi}\in(\mathbb{R}^{n})^{\otimes 3}\) and \(\mathbf{\zeta}\in(\mathbb{R}^{n})^{\otimes 4}\): for \(1\leq\alpha,\beta,\gamma,\mu,\nu\leq n\), the authors choose \(L_{\alpha\beta}(\mathbf{x})=\sum_{\gamma}\xi_{\alpha\beta\gamma}\partial^{\gamma}S\) and \(M_{\alpha\beta}(\mathbf{x})=\sum_{\mu,\nu}\zeta_{\alpha\mu,\beta\nu}\partial^{\mu} E\partial^{\nu}E,\) where \(\partial^{\alpha}F=\partial F/\partial x_{\alpha}\), \(\mathbf{\xi}\) is totally antisymmetric, and \(\mathbf{\zeta}\) is symmetric between the pairs \((\alpha,\mu)\) and \((\beta,\nu)\) but antisymmetric within each of these pairs. The key idea here is to exchange the problem of enforcing degeneracy conditions \(\mathbf{L}\nabla E=\mathbf{M}\nabla S=\mathbf{0}\) in matrix fields \(\mathbf{L},\mathbf{M}\) with the problem of enforcing symmetry conditions in tensor fields \(\mathbf{\xi},\mathbf{\zeta}\), which is comparatively easier but comes at the expense of underdetermining the problem. In GNODE, enforcement of these symmetries is handled by a generic learnable 3-tensor \(\tilde{\mathbf{\xi}}\in(\mathbb{R}^{n})^{\otimes 3}\) along with learnable matrices \(\mathbf{D}\in\mathrm{Sym}_{r}(\mathbb{R})\) and \(\mathbf{\Lambda}^{s}\in\mathrm{Skew}_{n}(\mathbb{R})\) for \(1\leq s\leq r\leq n\), leading to the final parameterizations \(\xi_{\alpha\beta\gamma}=\frac{1}{3!}\left(\tilde{\xi}_{\alpha\beta\gamma}- \tilde{\xi}_{\alpha\gamma\beta}+\tilde{\xi}_{\beta\gamma\alpha}-\tilde{\xi}_{ \beta\alpha\gamma}+\tilde{\xi}_{\gamma\alpha\beta}-\tilde{\xi}_{\gamma\beta \alpha}\right)\) and \(\zeta_{\alpha\mu,\beta\nu}=\sum_{s,t}\Lambda^{s}_{\alpha\mu}D_{st}\Lambda^{t}_ {\beta\nu}\). Along with learnable energy and entropy functions \(E,S\) parameterized by multi-layer perceptrons (MLPs), the data \(\mathbf{L},\mathbf{M}\) learned by GNODE guarantees metriplectic structure in the surrogate model and leads to successful learning of metriplectic systems in some simple cases of interest. However, note that this is a highly redundant parameterization requiring \(\binom{n}{3}+r\binom{n}{2}+\binom{r+1}{2}+2\) learnable scalar functions, which exhibits \(\mathcal{O}(n^{3}+rn^{2})\) scaling in the problem size because of the necessity to compute and store \(\binom{n}{3}\) entries of \(\mathbf{\xi}\) and \(r\binom{n}{2}\) entries of \(\mathbf{\Lambda}\). Additionally, the assumption of state-independence in the bracket generating tensors \(\mathbf{\xi},\mathbf{\zeta}\) is somewhat restrictive, limiting the class of problems to which GNODE can be applied.

A related approach to learning metriplectic dynamics with hard constraints was seen in [18], which proposed a series of GFINN architectures depending on how much of the information \(\mathbf{L},\mathbf{M},E,S\) is assumed to be known. In the case that \(\mathbf{L},\mathbf{M}\) are known, the GFINN energy and entropy are parameterized with scalar-valued functions \(f\circ\mathbf{P}_{\mathrm{ker}\mathbf{A}}\) where \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}\) (\(E\) or \(S\)) is learnable and \(\mathbf{P}_{\mathrm{ker}\mathbf{A}}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) is orthogonal projection onto the kernel of the (known) operator \(\mathbf{A}\) (\(\mathbf{L}\) or \(\mathbf{M}\)). It follows that the gradient \(\nabla(f\circ\mathbf{P}_{\mathrm{ker}\mathbf{A}})=\mathbf{P}_{\mathrm{ker}\mathbf{A}}\nabla f( \mathbf{P}_{\mathrm{ker}\mathbf{A}})\) lies in the kernel of \(\mathbf{A}\), so that the degeneracy conditions are guaranteed at the expense of constraining the model class of potential energies/entropies. Alternatively, in the case that all of \(\mathbf{L},\mathbf{M},E,S\) are unknown, GFINNs use learnable scalar functions \(f\) for \(E,S\) parameterized by MLPs as well as two matrix fields \(\mathbf{Q}^{E},\mathbf{Q}^{S}\in\mathbb{R}^{r\times n}\) with rows given by \(\mathbf{q}^{f}_{s}=\left(\mathbf{S}^{f}_{s}\nabla f\right)^{\intercal}\) for learnable skew-symmetric matrices \(\mathbf{S}^{f}_{s}\), \(1\leq s\leq r\), \(f=E,S\). Along with two triangular \((r\times r)\) matrix fields \(\mathbf{T}_{\mathbf{L}},\mathbf{T}_{\mathbf{M}}\) this yields the parameterizations \(\mathbf{L}(\mathbf{x})=\mathbf{Q}^{S}(\mathbf{x})^{\intercal}(\mathbf{T}_{\mathbf{L}}(\mathbf{x})^{\intercal }-\mathbf{T}_{\mathbf{L}}(\mathbf{x}))\mathbf{Q}^{S}(\mathbf{x})\) and \(\mathbf{M}(\mathbf{x})=\mathbf{Q}^{E}(\mathbf{x})^{\intercal}(\mathbf{T}_{\mathbf{M}}(\mathbf{x})^{\intercal }\mathbf{T}_{\mathbf{M}}(\mathbf{x}))\mathbf{Q}^{E}(\mathbf{x})\), which necessarily satisfy the symmetries and degeneracy conditions required for metriplectic structure. GFINNs are shown to both increase expressivity over the GNODE method as well as decrease redundancy, since the need for an explicit order-3 tensor field is removed and the reversible and irreversible brackets are allowed to depend explicitly on the state \(\mathbf{x}\). However, GFINNs still exhibit cubic scaling through the requirement of \(rn(n-1)+r^{2}+2=\mathcal{O}\big{(}rn^{2}\big{)}\) learnable functions, which is well above the theoretical minimum required to express a general metriplectic system and leads to difficulties in training the resulting models.

Model reduction.Finally, it is worth mentioning the closely related line of work involving model reduction for metriplectic systems, which began with [19]. As remarked there, preserving metriplecticity in reduced-order models (ROMs) exhibits many challenges due to its delicate requirements on the kernels of the involved operators. There are also hard and soft constrained approaches: the already mentioned [10] aims to learn a close-to-metriplectic data-driven ROM by enforcing degeneracies by penalty, while [20] directly enforces metriplectic structure in projection-based ROMs using exterior algebraic factorizations. The parameterizations of metriplectic data presented here are related to those presented in [20], although NMS does not require access to nonzero components of \(\nabla E,\nabla S\).

## 3 Formulation and Analysis

The proposed formulation of the metriplectic bracket-generating operators \(\mathbf{L},\mathbf{M}\) used by NMS is based on the idea of exploiting structure in the tensor fields \(\mathbf{\xi},\mathbf{\zeta}\) to reduce the necessary number of degrees of freedom. In particular, it will be shown that the degeneracy conditions \(\mathbf{L}\nabla S=\mathbf{M}\nabla E=\mathbf{0}\) imply more than just symmetry constraints on these fields, and that taking these additional constraints into account allows for a more compact representation of metriplectic data. Following this, results are presented which show that the proposed formulation is universally approximating on nondegenerate systems (c.f. Definition 3.1) and admits a generalization error bound in time.

### Exterior algebra

Developing these metriplectic expressions will require some basic facts from exterior algebra, of which more details can be found in, e.g., [21, Chapter 19]. The basic objects in the exterior algebra \(\bigwedge V\) over the vector space \(V\) are multivectors, which are formal linear combinations of totally antisymmetric tensors on \(V\). More precisely, if \(I(V)\) denotes the two-sided ideal of the free tensor algebra \(T(V)\) generated by elements of the form \(\mathbf{v}\otimes\mathbf{v}\) (\(\mathbf{v}\in V\)), then the exterior algebra is the quotient space \(\bigwedge V\simeq T(V)/I(V)\) equipped with the antisymmetric wedge product operation \(\wedge\). This graded algebra is equipped with natural projection operators \(P^{k}:\bigwedge V\rightarrow\bigwedge^{k}V\) which map between the full exterior algebra and the \(k^{\mathrm{th}}\) exterior power of \(V\), denoted \(\bigwedge^{k}V\), whose elements are homogeneous \(k\)-vectors. More generally, given an \(n\)-manifold \(M\) with tangent bundle \(TM\), the exterior algebra \(\bigwedge(TM)\) is the algebra of multivector fields whose fiber over \(x\in M\) is given by \(\bigwedge T_{x}M\).

For the present purposes, it will be useful to develop a correspondence between bivectors \(\mathsf{B}\in\bigwedge^{2}(\mathbb{R}^{n})\) and skew-symmetric matrices \(\mathbf{B}\in\mathrm{Skew}_{n}(\mathbb{R})\), which follows directly from Riesz representation in terms of the usual Euclidean dot product. More precisely, supposing that \(\mathbf{e}_{1},...,\mathbf{e}_{n}\) are the standard basis vectors for \(\mathbb{R}^{n}\), any bivector \(\mathsf{B}\in\bigwedge^{2}T\mathbb{R}^{n}\) can be represented as \(\mathsf{B}=\sum_{i<j}B^{ij}\mathbf{e}_{i}\wedge\mathbf{e}_{j}\) where \(B^{ij}\in\mathbb{R}\) denote the components of \(\mathsf{B}\). Define a grade-lowering action of bivectors on vectors through right contraction (see e.g. Section 3.4 of [22]), expressed for any vector \(\mathbf{v}\) and basis bivector \(\mathbf{e}_{i}\wedge\mathbf{e}_{j}\) as \((\mathbf{e}_{i}\wedge\mathbf{e}_{j})\cdot\mathbf{v}=(\mathbf{e}_{j}\cdot\mathbf{v})\mathbf{e}_{i}-( \mathbf{e}_{i}\cdot\mathbf{v})\mathbf{e}_{j}\). It follows that the action of \(\mathsf{B}\) is equivalent to

\[\mathsf{B}\cdot\mathbf{v}=\sum_{i<j}B^{ij}((\mathbf{e}_{j}\cdot\mathbf{v})\mathbf{e}_{i}-(\mathbf{ e}_{i}\cdot\mathbf{v})\mathbf{e}_{j})=\sum_{i<j}B^{ij}v_{j}\mathbf{e}_{i}-\sum_{j<i}B^{ji}v_{j} \mathbf{e}_{i}=\sum_{i,j}B^{ij}v_{j}\mathbf{e}_{i}=\mathbf{Bv},\]

where \(\mathbf{B}^{\intercal}=-\mathbf{B}\in\mathbb{R}^{n\times n}\) is a skew-symmetric matrix representing \(\mathsf{B}\), and we have re-indexed under the second sum and applied that \(B^{ij}=-B^{ji}\) for all \(i,j\). Since the kernel of this action is the zero bivector, it is straightforward to check that this string of equalities defines an isomorphism \(\mathcal{M}:\bigwedge^{2}\mathbb{R}^{n}\rightarrow\mathrm{Skew}_{n}(\mathbb{R})\) from the \(2^{\mathrm{nd}}\) exterior power of \(\mathbb{R}^{n}\) to the space of skew-symmetric \((n\times n)\)-matrices over \(\mathbb{R}\): in what follows, we will write \(\mathbf{B}\simeq\mathsf{B}\) rather than \(\mathbf{B}=\mathcal{M}(\mathsf{B})\) for notational convenience. Note that a correspondence in the more general case of bivector/matrix fields follows in the usual way via the fiber-wise extension of \(\mathcal{M}\).

### Learnable metriplectic operators

It is now possible to explain the proposed NMS formulation. First, note the following key definition which prevents the consideration of unphysical examples.

**Definition 3.1**.: A metriplectic system on \(K\subset\mathbb{R}^{n}\) generated by the data \(\mathbf{L},\mathbf{M},E,S\) will be called _nondegenerate_ provided \(\nabla E,\nabla S\neq\mathbf{0}\) for all \(\mathbf{x}\in K\).

With this, the NMS parameterizations for metriplectic operators are predicated on an algebraic result proven in Appendix A.

**Lemma 3.2**.: _Let \(K\subset\mathbb{R}^{n}\). For all \(\mathbf{x}\in K\), the operator \(\mathbf{L}:K\to\mathbb{R}^{n\times n}\) satisfies \(\mathbf{L}^{\intercal}=-\mathbf{L}\) and \(L\nabla S=\mathbf{0}\) for some \(S:K\to\mathbb{R}\), \(\nabla S\neq\mathbf{0}\), provided there exists a non-unique bivector field \(\mathsf{A}:U\to\bigwedge^{2}\mathbb{R}^{n}\) and equivalent matrix field \(\mathbf{A}\simeq\mathsf{A}\) such that_

\[\mathbf{L}\simeq\left(\mathsf{A}\wedge\frac{\nabla S}{\left|\nabla S\right|^{2}} \right)\cdot\nabla S=\mathsf{A}-\frac{1}{\left|\nabla S\right|^{2}}\mathbf{A}\nabla S \wedge\nabla S.\]

_Similarly, for all \(\mathbf{x}\in K\) a positive semi-definite operator \(\mathbf{M}:K\to\mathbb{R}^{n\times n}\) satisfies \(\mathbf{M}^{\intercal}=\mathbf{M}\) and \(\mathbf{M}\nabla E=\mathbf{0}\) for some \(E:K\to\mathbb{R}\), \(\nabla E\neq\mathbf{0}\), provided there exists a non-unique matrix-valued \(\mathbf{B}:K\to\mathbb{R}^{n\times r}\) and symmetric matrix-valued \(\mathbf{D}:K\to\mathbb{R}^{r\times r}\) such that \(r\leq n\) and_

\[\mathbf{M} =\sum_{s,t}D_{st}\Bigg{(}\mathbf{b}^{s}\wedge\frac{\nabla E}{\left| \nabla E\right|^{2}}\Bigg{)}\cdot\nabla E\,\otimes\,\Bigg{(}\mathbf{b}^{t}\wedge \frac{\nabla E}{\left|\nabla E\right|^{2}}\Bigg{)}\cdot\nabla E\] \[=\sum_{s,t}D_{st}\Bigg{(}\mathbf{b}^{s}-\frac{\mathbf{b}^{s}\cdot\nabla E} {\left|\nabla E\right|^{2}}\nabla E\Bigg{)}\Bigg{(}\mathbf{b}^{t}-\frac{\mathbf{b}^{t} \cdot\nabla E}{\left|\nabla E\right|^{2}}\nabla E\Bigg{)}^{\intercal},\]

_where \(\mathbf{b}^{s}\) denotes the \(s^{\mathrm{th}}\) column of \(\mathbf{B}\). Moreover, using \(\mathbf{P}_{f}^{\perp}=\left(\mathbf{I}-\frac{\nabla f\,\nabla f\,\tau}{\left|\nabla f \right|^{2}}\right)\) to denote the orthogonal projector onto \(\mathrm{Span}(\nabla f)^{\perp}\), these parameterizations of \(\mathbf{L},\mathbf{M}\) are equivalent to the matricized expressions \(\mathbf{L}=\mathbf{P}_{S}^{\perp}\mathbf{A}\mathbf{P}_{S}^{\perp}\) and \(\mathbf{M}=\mathbf{P}_{E}^{\perp}\mathbf{B}\mathbf{D}\mathbf{B}^{\intercal}\mathbf{P}_{E}^{\perp}\)._

_Remark 3.3_.: Observe that the projections appearing in these expressions are the minimum necessary for guaranteeing the symmetries and degeneracy conditions necessary for metriplectic structure. In particular, conjugation by \(\mathbf{P}_{f}^{\perp}\) respects symmetry and ensures that both the input and output to the conjugated matrix field lie in \(\mathrm{Span}(\nabla f)^{\perp}\).

Lemma 3.2 demonstrates specific parameterizations for \(\mathbf{L},\mathbf{M}\) that hold for any nondegenerate metriplectic data and are core to the NMS method for learning metriplectic dynamics. While generally underdetermined, these expressions are in a sense maximally specific given no additional information, since there is nothing available in the general metriplectic formalism to determine the matrix fields \(\mathbf{A},\mathbf{B}\mathbf{D}\mathbf{B}^{\intercal}\) on \(\mathrm{Span}(\nabla S),\mathrm{Span}(\nabla E)\), respectively. The following Theorem, also proven in Appendix A, provides a rigorous correspondence between metriplectic systems and these particular parameterizations.

**Theorem 3.4**.: _The data \(\mathbf{L},\mathbf{M},E,S\) form a nondegenerate metriplectic system in the state variable \(\mathbf{x}\in K\) if and only if there exist a skew-symmetric \(\mathbf{A}:K\to\mathrm{Skew}_{n}(\mathbb{R})\), symmetric postive semidefinite \(\mathbf{D}:K\to\mathrm{Sym}_{r}(\mathbb{R})\), and generic \(\mathbf{B}:K\to\mathbb{R}^{n\times r}\) such that_

\[\dot{\mathbf{x}}=\mathbf{L}\nabla E+\mathbf{M}\nabla S=\mathbf{P}_{S}^{\perp}\mathbf{A}\mathbf{P}_{S} ^{\perp}\nabla E+\mathbf{P}_{E}^{\perp}\mathbf{B}\mathbf{D}\mathbf{B}^{\intercal}\mathbf{P}_{E}^ {\perp}\nabla S.\]

_Remark 3.5_.: Note that the proposed parameterizations for \(\mathbf{L},\mathbf{M}\) are not one-to-one but properly contain the set of valid nondegenerate metriplectic systems in \(E,S\), since the Jacobi identity on \(\mathbf{L}\) necessary for a true Poisson manifold structure is not strictly enforced. For \(1\leq i,j,k\leq n\), the Jacobi identity is given in components as \(\sum_{\ell}L_{i\ell}\partial^{L}L_{jk}+L_{j\ell}\partial^{L}L_{ki}+L_{k\ell} \partial^{L}L_{ij}=0\). However, this requirement is not often enforced in algorithms for learning general metriplectic (or even symplectic) systems, since it is considered subordinate to energy conservation and it is well known that both qualities cannot hold simultaneously in general [23].

### Specific parameterizations

Now that Theorem 3.4 has provided a model class which is rich enough to express any desired metriplectic system, it remains to discuss what NMS actually learns. First, note that it is unlikely to be the case that \(E,S\) are known _a priori_, so it is beneficial to allow these functions to be learnable alongside the governing operators \(\mathbf{L},\mathbf{M}\). For simplicity, energy and entropy \(E,S\) are parameterized as scalar-valued MLPs with \(\tanh\) activation, although any desired architecture could be chosen for this task. The skew-symmetric matrix field \(\mathbf{A}=\mathbf{A}_{\mathrm{tri}}-\mathbf{A}_{\mathrm{tri}}^{\intercal}\) used to build \(\mathbf{L}\) is parameterized through its strictly lower-triangular part \(\mathbf{A}_{\mathrm{tri}}\) using a vector-valued MLP with output dimension \(\binom{n}{2}\)which guarantees that the mapping \(\mathbf{A}_{\mathrm{tri}}\mapsto\mathbf{A}\) above is bijective. Similarly, the symmetric matrix field \(\mathbf{D}=\mathbf{K}_{\mathrm{chol}}\mathbf{K}_{\mathrm{chol}}^{\intercal}\) is parameterized through its lower-triangular Cholesky factor \(\mathbf{K}_{\mathrm{chol}}\), which is a vector-valued MLP with output dimension \(\binom{r+1}{2}\). While this choice does not yield a bijective mapping \(\mathbf{K}_{\mathrm{chol}}\mapsto\mathbf{D}\) unless, e.g., \(\mathbf{D}\) is assumed to be positive definite with diagonal entries of fixed sign, this does not hinder the method in practice. In fact, \(\mathbf{D}\) should not be positive definite in general, as this is equivalent to claiming that \(\mathbf{M}\) is positive definite on vectors tangent to the level sets of \(E\). Finally, the generic matrix field \(\mathbf{B}\) is parameterized as a vector-valued MLP with output dimension \(nr\). Remarkably, the exterior algebraic expressions in Lemma 3.2 require less redundant operations than the corresponding matricized expressions from Theorem 3.4, and therefore the expressions from Lemma 3.2 are used when implementing NMS. Figure 1 summarizes this information.

_Remark 3.6_.: With these choices, the NMS parameterization of metriplectic systems requires only \((1/2)\big{(}(n+r)^{2}-(n-r)\big{)}+2\) learnable scalar functions, in contrast to \(\binom{n}{3}+r\binom{n}{2}+\binom{r+1}{2}+2\) for the GNODE approach in [16] and \(rn(n-1)+r^{2}+2\) for the GFINN approach in [18]. In particular, NMS is quadratic in both \(n,r\) with no decrease in model expressivity, in contrast to the cubic scaling of previous methods.

### Approximation and error

Besides offering a compact parameterization of metriplectic dynamics, the expressions used in NMS also exhibit desirable approximation properties which guarantee a reasonable bound on state error over time. To state this precisely, first note the following universal approximation result proven in Appendix A.

**Proposition 3.7**.: _Let \(K\subset\mathbb{R}^{n}\) be compact and \(E,S:K\to\mathbb{R}\) be continuous such that \(\mathbf{L}\nabla S=\mathbf{M}\nabla E=\mathbf{0}\) and \(\nabla E,\nabla S\neq\mathbf{0}\) for all \(\mathbf{x}\in K\). For any \(\varepsilon>0\), there exist two-layer neural network functions \(\tilde{E},\tilde{S}:K\to\mathbb{R},\tilde{\mathbf{L}}:K\to\operatorname{Skew}_{n}( \mathbb{R})\) and \(\tilde{\mathbf{M}}:K\to\operatorname{Sym}_{n}(\mathbb{R})\) such that \(\nabla\tilde{E},\nabla\tilde{S}\neq\mathbf{0}\) on \(K\), \(\tilde{\mathbf{M}}\) is positive semi-definite, \(\tilde{\mathbf{L}}\nabla\tilde{S}=\tilde{\mathbf{M}}\nabla\tilde{E}=\mathbf{0}\) for all \(\mathbf{x}\in K\), and each approximate function is \(\varepsilon\)-close to its given counterpart on \(K\). Moreover, if \(\mathbf{L},\mathbf{M}\) have \(k\geq 0\) continuous derivatives on \(K\) then so do \(\tilde{\mathbf{L}},\tilde{\mathbf{M}}\)._

_Remark 3.8_.: The assumption \(\mathbf{x}\in K\) of the state remaining in a compact set \(V\) is not restrictive when at least one of \(E,-S:\mathbb{R}^{n}\to\mathbb{R}\), say \(E\), has bounded sublevel sets. In this case, letting \(\mathbf{x}_{0}=\mathbf{x}(0)\) it follows from \(\tilde{E}\leq 0\) that \(E(\mathbf{x}(t))=E(\mathbf{x}_{0})+\int_{0}^{t}\tilde{E}(\mathbf{x}(\tau))\,d\tau\leq E( \mathbf{x}_{0}),\) so that the entire trajectory \(\mathbf{x}(t)\) lies in the (closed and bounded) compact set \(K=\{\mathbf{x}\,|\,E(\mathbf{x})\leq E(\mathbf{x}_{0})\}\subset\mathbb{R}^{n}\).

Leaning on Proposition 3.7 and classical universal approximation results in [24], it is further possible to establish the following error estimate also proven in Appendix A which gives an idea of the error accumulation rate that can be expected from this method.

**Theorem 3.9**.: _Suppose \(\mathbf{L},\mathbf{M},E,S\) are nondegenerate metriplectic data such that \(\mathbf{L},\mathbf{M}\) have at least one continuous derivative, \(E,S\) have Lipschitz continuous gradients, and at least one of \(E,-S\) have bounded sublevel sets. For any \(\varepsilon>0\), there exist nondegenerate metriplectic data \(\tilde{\mathbf{L}},\tilde{\mathbf{M}},\tilde{E},\tilde{S}\) defined by two-layer neural networks such that, for all \(T>0\),_

\[\left(\int_{0}^{T}\lvert\mathbf{x}-\tilde{\mathbf{x}}\rvert^{2}\,dt\right)^{\frac{1}{2 }}\leq\varepsilon\bigg{\lvert}\frac{b}{a}\bigg{\rvert}\sqrt{e^{2aT}-2e^{aT}+ T+1},\]

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \multicolumn{2}{c}{Name} & \multicolumn{1}{c}{Physics} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{Restrictive} & \multicolumn{1}{c}{Scale} \\ \hline NODE & None & No & Linear \\ SPNN & Soft & No & Quadratic \\ GNODE & Hard & Yes & Cubic \\ GFINN & Hard & No & Cubic \\ NMS & Hard & No & Quadratic \\ \hline \hline \end{tabular}
\end{table}
Table 1: Properties of the metriplectic architectures compared.

Figure 1: A visual depiction of the NMS architecture.

_where \(a,b\in\mathbb{R}\) are constants depending on both sets of metriplectic data and \(\hat{\vec{x}}=\hat{\vec{L}}(\vec{x})\nabla\tilde{E}(\vec{x})+\tilde{\vec{M}}( \hat{\vec{x}})\nabla\tilde{S}(\vec{x})\)._

_Remark 3.10_.: Theorem 3.9 provides a bound on state error over time under the assumption that the approximation error in the metriplectic networks can be controlled. On the other hand, notice that Theorem 3.9 can also be understood as a generic error bound on any trained metriplectic networks \(\tilde{\vec{L}},\tilde{\vec{M}},\tilde{E},\tilde{S}\) provided universal approximation results are not invoked in the estimation leading to \(\varepsilon b\).

This result confirms that the error in the state \(\vec{x}\) for a fixed final time \(T\) tends to zero with the approximation error in the networks \(\tilde{\vec{L}},\tilde{\vec{M}},\tilde{E},\tilde{S}\), as one would hope based on the approximation capabilities of neural networks. More importantly, Theorem 3.9 also bounds the generalization error of any trained metriplectic network for an appropriate (and possibly large) \(\varepsilon\) equal to the maximum approximation error on \(K\), where the learned metriplectic trajectories are confined for all time. With this theoretical guidance, the remaining goal of this work is to demonstrate that NMS is also practically effective at learning metriplectic systems from data and exhibits reasonable generalization to unseen timescales.

## 4 Algorithm

Similar to previous approaches in [16] and [18], the learnable parameters in NMS are calibrated using data along solution trajectories to a given dynamical system. This brings up an important question regarding how much information about the system in question is realistically present in the training data. While many systems have a known metriplectic form, it is not always the case that one will know metriplectic governing equations for a given set of training data. Therefore, two approaches are considered in the experiments below corresponding to whether full or partial state information is assumed available during NMS training. More precisely, the state \(\vec{x}=(\vec{x}^{o},\vec{x}^{u})\) will be partitioned into "observable" and "unobservable" variables, where \(\vec{x}^{u}\) may be empty in the case that full state information is available. In a partially observable system \(\vec{x}^{o}\) typically contains positions and momenta while \(\vec{x}^{u}\) contains entropy or other configuration variables which are more difficult to observe during physical experiments. In both cases, NMS will learn a metriplectic system in \(\vec{x}\) according to the procedure described in Algorithm 1.

```
1:Input: snapshot data \(\vec{X}\in\mathbb{R}^{n\times n_{s}}\), each column \(\vec{x}_{s}=\vec{x}(t_{s},\vec{\mu}_{s})\), target rank \(r\geq 1\), batch size \(n_{b}\geq 1\).
2:Initialize networks \(\vec{A}_{\mathrm{tri}},\vec{B},\vec{K}_{\mathrm{chol}},E,S,\) and loss \(L=0\)
3:for step in \(N_{\mathrm{steps}}\)do
4: Randomly draw batch \(P=\{(t_{s_{k}},\vec{x}_{s_{k}})\}_{k=1}^{n_{b}}\)
5:for\((t,\vec{x})\) in \(P\)do
6: Evaluate \(\vec{A}_{\mathrm{tri}}(\vec{x}),\vec{B}(\vec{x}),\vec{K}_{\mathrm{chol}}( \vec{x}),E(\vec{x}),S(\vec{x})\)
7: Automatically differentiate \(E,S\) to obtain \(\nabla E(\vec{x}),\nabla S(\vec{x})\)
8: Form \(\vec{A}(\vec{x})=\vec{A}_{\mathrm{tri}}(\vec{x})-\vec{A}_{\mathrm{tri}}(\vec{ x})^{\intercal}\) and \(\vec{D}(\vec{x})=\vec{K}_{\mathrm{chol}}(\vec{x})\vec{K}_{\mathrm{chol}}(\vec{x })^{\intercal}\)
9: Build \(\vec{L}(\vec{x}),\vec{M}(\vec{x})\) according to Lemma 3.2
10: Evaluate \(\dot{\vec{x}}=\vec{L}(\vec{x})\nabla E(\vec{x})+\vec{M}(\vec{x})\nabla S(\vec{ x})\)
11: Randomly draw \(n_{1},...,n_{l}\) and form \(t_{j}=t+n_{j}\Delta t\) for all \(j\)
12:\(\vec{x}_{1},...,\vec{x}_{l}=\mathrm{ODEsolve}(\vec{x},t_{1},...,t_{l})\)
13:\(L+=l^{-1}\sum_{j}\mathrm{Loss}(\vec{x}_{j},\vec{x}_{j})\)
14:endfor
15: Rescale \(L=|P|^{-1}L\)
16: Update \(\vec{A}_{\mathrm{tri}},\vec{B},\vec{K}_{\mathrm{chol}},E,S\) through gradient descent on \(L\).
17:endfor
```

**Algorithm 1** Training neural metriplectic systems

Note that the batch-wise training strategy in Algorithm 1 requires initial conditions for \(\vec{x}^{u}\) in the partially observed case. There are several options for this, and two specific strategies will be considered here. Suppose the data are drawn from the training interval \([0,T]\) with initial state \(\vec{x}_{0}\) and final state \(\vec{x}_{T}\). The first strategy sets \(\vec{x}^{u}_{0}=\vec{0}\), \(\vec{x}^{u}_{T}=\vec{1}\) (where \(\vec{1}\) is the all ones vector), and \(\vec{x}^{u}_{s}=\vec{1}/T\), \(0<s<T\), so that the unobserved states are initially assumed to lie on a straight line. The second strategy is more sophisticated, and involves training a diffusion model to predict the distribution of \(\vec{x}^{u}\) given \(\vec{x}^{o}\). Specific details of this procedure are given in Appendix E.

Examples

The goal of the following experiments is to show that NMS is effective even when entropic information cannot be observed during training, yielding superior performance when compared to previous methods including GNODE, GFINN, and SPNN discussed in Section 2. The metrics considered for this purpose will be mean absolute error (MAE) and mean squared error (MSE) defined in the usual way as the average \(\ell^{1}\) (resp. squared \(\ell^{2}\)) error between the discrete states \(\mathbf{x},\hat{\mathbf{x}}\in\mathbb{R}^{n\times n_{s}}\). For brevity, many implementation details have been omitted here and can be found in Appendix B. An additional experiment showing the effectiveness of NMS in the presence of both full and partial state information can be found in Appendix C.

_Remark 5.1_.: To facilitate a more equal parameter count between the compared metriplectic methods, the results of the experiments below were generated using the alternative parameterization \(\mathbf{D}=\mathbf{K}\mathbf{K}^{\intercal}\) where \(\mathbf{K}:K\rightarrow\mathbb{R}^{r\times r^{\prime}}\) is full and \(r^{\prime}\geq r\). Of course, this change does not affect metriplecticity since \(\mathbf{D}\) is still positive semi-definite for each \(\mathbf{x}\in K\).

### Two gas containers

The first test of NMS involves two ideal gas containers separated by movable wall which is removed at time \(t_{0}\), allowing for the exchange of heat and volume. In this example, the motion of the state \(\mathbf{x}=(q\quad p\quad S_{1}\quad S_{2})^{\intercal}\) is governed by the metriplectic equations:

\[\dot{q} =\frac{p}{m}, \dot{p} =\frac{2}{3}\bigg{(}\frac{E_{1}(\mathbf{x})}{q}-\frac{E_{2}(\mathbf{x})} {2L-q}\bigg{)},\] \[\dot{S}_{1} =\frac{9N^{2}k_{B}^{2}\alpha}{4E_{1}(\mathbf{x})}\bigg{(}\frac{1}{E_ {1}(\mathbf{x})}-\frac{1}{E_{2}(\mathbf{x})}\bigg{)}, \dot{S}_{2} =-\frac{9N^{2}k_{B}^{2}\alpha}{4E_{1}(\mathbf{x})}\bigg{(}\frac{1}{E_ {1}(\mathbf{x})}-\frac{1}{E_{2}(\mathbf{x})}\bigg{)},\]

where \((q,p)\) are the position and momentum of the separating wall, \(S_{1},S_{2}\) are the entropies of the two subsystems, and the internal energies \(E_{1},E_{2}\) are determined from the Sackur-Tetrode equation for ideal gases, \(S_{i}/Nk_{B}=\ln\Bigl{(}\hat{c}V_{i}E_{i}^{3/2}\Bigr{)},1\leq i\leq 2\). Here, \(m\) denotes the mass of the wall, \(2L\) is the total length of the system, and \(V_{i}\) is the volume of the \(i^{\text{th}}\) container. As in [16; 25]\(Nk_{B}=1\) and \(\alpha=0.5\) fix the characteristic macroscopic unit of entropy while \(\hat{c}=102.25\) ensures the argument of the logarithm defining \(E_{i}\) is dimensionless. This leads to the total entropy \(S(\mathbf{x})=S_{1}+S_{2}\) and the total energy \(E(\mathbf{x})=(1/2m)p^{2}+E_{1}(\mathbf{x})+E_{2}(\mathbf{x})\), which are guaranteed to be nondecreasing and constant, respectively.

The primary goal here is to verify that NMS can accurately and stably predict gas container dynamics without the need to observe the entropic variables \(S_{1},S_{2}\). To that end, NMS has been compared to GNODE, SPNN, and GFINN on the task of predicting the trajectories of this metriplectic system over time, with results displayed in Table 2. More precisely, given an intial condition \(\mathbf{x}_{0}\) and an interval \(0<t_{\text{train}}<t_{\text{valid}}<t_{\text{test}}\), each method is trained on partial state information (in the case of NMS) or full state information (in the case of the others) from the interval \([0,t_{\text{train}}]\) and validated on \((t_{\text{train}},t_{\text{valid}}]\) before state errors in \(q,p\) only are calculated on the whole interval \([0,t_{\text{test}}]\). As can be seen from Table 2 and Figure 2, NMS is remarkably accurate over unseen timescales even in this unfair comparison, avoiding the unphysical behavior which often hinders soft-constrained methods like SPNN. The energy and instantaneous entropy plots in Figure 2 further confirm that the strong enforcement of metriplectic structure guaranteed by NMS leads to correct energetic and entropic dynamics for all time.

### Thermoelastic double pendulum

Next, consider the thermoelastic double pendulum from [26] with 10-dimensional state variable \(\mathbf{x}=(\mathbf{q}_{1}\quad\mathbf{q}_{2}\quad\mathbf{p}_{1}\quad\mathbf{p}_{2}\quad S_{1}\quad S _{2})^{\intercal}\), which represents a highly challenging benchmark for metriplectic methods. The equations of motion in this case are given for \(1\leq i\leq 2\) as

\[\dot{\mathbf{q}}_{i}=\frac{\mathbf{p}_{i}}{m_{i}},\quad\dot{\mathbf{p}}_{i}=-\partial_{\bm {q}_{i}}(E_{1}(\mathbf{x})+E_{2}(\mathbf{x})),\quad\dot{S}_{1}=\kappa\bigl{(}T_{1}^{-1 }T_{2}-1\bigr{)},\quad\dot{S}_{2}=\kappa\bigl{(}T_{1}T_{2}^{-1}-1\bigr{)},\]

where \(\kappa>0\) is a thermal conductivity constant (set to 1), \(m_{i}\) is the mass of the \(i^{\text{th}}\) spring (also set to 1) and \(T_{i}=\partial_{S_{i}}E_{i}\) is its absolute temperature. In this case, \(\mathbf{q}_{i},\mathbf{p}_{i}\in\mathbb{R}^{2}\) represent the position and momentum of the \(i^{\rm th}\) mass, while \(S_{i}\) represents the entropy of the \(i^{\rm th}\) pendulum. As before, the total entropy \(S(\mathbf{x})=S_{1}+S_{2}\) is the sum of the entropies of the two springs, while defining the internal energies \(E_{i}(\mathbf{x})=(1/2)(\ln\lambda_{i})^{2}+\ln\lambda_{i}+e^{S_{i}-\ln\lambda_{i}}- 1,\lambda_{1}=|\mathbf{q}_{i}|,\lambda_{2}=|\mathbf{q}_{2}-\mathbf{q}_{1}|\), leads to the total energy \(E(\mathbf{x})=(1/2m_{1}){|\mathbf{p}_{1}|}^{2}+(1/2m_{2}){|\mathbf{p}_{2}|}^{2}+E_{1}(\mathbf{ x})+E_{2}(\mathbf{x})\).

The task in this case is prediction across initial conditions. As in [18], 100 trajectories are drawn from the ranges in Appendix B and integrated over the interval \([0,40]\) with \(\Delta t=0.1\), with an 80/10/10 split for training/validation/testing. Here all compared models are trained using full state information. As seen in Table 2, NMS is again the most performant, although all models struggle to approximate the dynamics over the entire training interval. It is also notable that the training time of NMS is greatly decreased relative to GNODE and GFINN due to its improved quadratic scaling; a representative study to this effect is given in Appendix D.

## 6 Conclusion

Neural metriplectic systems (NMS) have been considered for learning finite-dimensional metriplectic dynamics from data. Making use of novel non-redundant parameterizations for metriplectic operators, NMS provably approximates arbitrary nondegenerate metriplectic systems with generalization error bounded in terms of the operator approximation quality. Benchmark examples have shown that NMS is both more scalable and more accurate than previous methods, including when only partial state information is observed. Future work will consider extensions of NMS to infinite-dimensional metriplectic systems with the aim of addressing its main limitation: the difficulty of scaling NMS (among all present methods for metriplectic learning) to realistic, 3-D problems of the size that would be considered in practice. A promising direction is to consider the use of NMS in model reduction, where sparse, large-scale systems are converted to small, dense systems through a clever choice of encoding/decoding.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline  & NODE & SPNN & GNODE & GFINN & NMS \\ \hline MSE &.12 \(\pm\).04 &.13 \(\pm\).10 &.16 \(\pm\).10 &.07 \(\pm\).03 & **.01 \(\pm\).02** \\ MAE &.25 \(\pm\).10 &.26 \(\pm\).14 &.25 \(\pm\).13 &.13 \(\pm\).03 & **.08 \(\pm\).06** \\ \hline \hline \end{tabular} 
\begin{tabular}{l r r r r r r} \hline \hline  & NODE & SPNN & GNODE & GFINN & NMS \\ \hline MSE &.41 \(\pm\).01 &.42 \(\pm\).01 &.42 \(\pm\).01 &.40 \(\pm\).03 & **.38 \(\pm\).03** \\ MAE &.48 \(\pm\).04 &.47 \(\pm\).03 &.46 \(\pm\).04 &.43 \(\pm\).07 & **.42 \(\pm\).07** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Prediction errors for \(\mathbf{x}^{o}\) measured in MSE and MAE on the interval \([0,t_{\rm test}]\) in the two gas containers example (left) and on the test set in the thermoelastic double pendulum example (right).

Figure 2: The ground-truth and predicted position, momentum, instantaneous entropy, and energies for the two gas containers example in the training (white), validation (yellow), and testing (red) regimes.

## References

* [1] Philip J. Morrison. A paradigm for joined hamiltonian and dissipative systems. _Physica D: Nonlinear Phenomena_, 18(1):410-419, 1986.
* [2] Miroslav Grmela and Hans Christian Ottinger. Dynamics and thermodynamics of complex fluids. i. development of a general formalism. _Phys. Rev. E_, 56:6620-6632, Dec 1997.
* [3] P. J. Morrison. Some observations regarding brackets and dissipation. Technical Report PAM-228, Center for Pure and Applied Mathematics, University of California, Berkeley, 1984.
* [4] PJ Morrison. Thoughts on brackets and dissipation: old and new. In _Journal of Physics: Conference Series_, volume 169, page 012006. IOP Publishing, 2009.
* [5] Allan N. Kaufman and Philip J. Morrison. Algebraic structure of the plasma quasilinear equations. _Physics Letters A_, 88(8):405-406, 1982.
* [6] Emmanuele Materassi, M.; Tassi. Metriplectic framework for dissipative magneto-hydrodynamics. _Physica D: Nonlinear Phenomena_, 2012.
* [7] Allan N. Kaufman. Dissipative hamiltonian systems: A unifying principle. _Physics Letters A_, 100(8):419-422, 1984.
* [8] Darryl D Holm, Vakhtang Putkaradze, and Cesare Tronci. Kinetic models of oriented self-assembly. _Journal of Physics A: Mathematical and Theoretical_, 41(34):344010, aug 2008.
* [9] Quercus Hernandez, Alberto Badias, David Gonzalez, Francisco Chinesta, and Elias Cueto. Structure-preserving neural networks. _Journal of Computational Physics_, 426:109950, 2021.
* [10] Quercus Hernandez, Alberto Badias, David Gonzalez, Francisco Chinesta, and Elias Cueto. Deep learning of thermodynamics-aware reduced-order models from data. _Computer Methods in Applied Mechanics and Engineering_, 379:113763, 2021.
* [11] David Gonzalez, Francisco Chinesta, and Elias Cueto. Thermodynamically consistent data-driven computational mechanics. _Continuum Mechanics and Thermodynamics_, 31(1):239-253, 2019.
* [12] D. Ruiz, D. Portillo, and I. Romero. A data-driven method for dissipative thermomechanics. _IFAC-PapersOnLine_, 54(19):315-320, 2021.
* [13] Baige Xu, Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Learning generic systems using neural symplectic forms. In _International Symposium on Nonlinear Theory and Its Applications_, number A2L-D-03 in IEICE Proceeding Series, pages 29-32. The Institute of Electronics, Information, and Communication Engineers (IEICE), 2022.
* [14] Baige Xu, Yuhan Chen, Takashi Matsubara, and Takaharu Yaguchi. Equivalence class learning for GENERIC systems. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.
* [15] Anthony Gruber, Kookjin Lee, and Nathaniel Trask. Reversible and irreversible bracket-based dynamics for deep graph neural networks. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [16] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Machine learning structure preserving brackets for forecasting irreversible processes. _Advances in Neural Information Processing Systems_, 34:5696-5707, 2021.
* [17] Hans Christian Ottinger. Irreversible dynamics, onsager-casimir symmetry, and an application to turbulence. _Phys. Rev. E_, 90:042121, Oct 2014.
* [18] Zhen Zhang, Yeonjong Shin, and George Em Karniadakis. Gfinns: Generic formalism informed neural networks for deterministic and stochastic dynamical systems. _Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 380(2229):20210207, 2022.
* [19] Hans Christian Ottinger. Preservation of thermodynamic structure in model reduction. _Phys. Rev. E_, 91:032147, Mar 2015.
* [20] Anthony Gruber, Max Gunzburger, Lili Ju, and Zhu Wang. Energetically consistent model reduction for metriplectic systems. _Computer Methods in Applied Mechanics and Engineering_, 404:115709, 2023.

* [21] Loring W. Tu. _Differential Geometry: Connections, Curvature, and Characteristic Classes_. Springer International Publishing, 2017.
* [22] Leo Dorst, Daniel Fontijne, and Stephen Mann. _Geometric Algebra for Computer Science: An Object-oriented Approach to Geometry_. Morgan Kaufmann, Amsterdam, 2007.
* [23] Ge Zhong and Jerrold E. Marsden. Lie-poisson hamilton-jacobi theory and lie-poisson integrators. _Physics Letters A_, 133(3):134-139, 1988.
* [24] Xin Li. Simultaneous approximations of multivariate functions and their derivatives by neural networks with one hidden layer. _Neurocomputing_, 12(4):327-343, 1996.
* [25] Kookjin Lee, Nathaniel Trask, and Panos Stinis. Structure-preserving sparse identification of nonlinear dynamics for data-driven modeling. In _Mathematical and Scientific Machine Learning_, pages 65-80. PMLR, 2022.
* [26] Ignacio Romero. Thermodynamically consistent time-stepping algorithms for non-linear thermomechanical systems. _International Journal for Numerical Methods in Engineering_, 79(6):706-732, 2023/05/14 2009.
* [27] John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. _Journal of computational and applied mathematics_, 6(1):19-26, 1980.
* [28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [29] Xiaocheng Shang and Hans Christian Ottinger. Structure-preserving integrators for dissipative systems based on reversible-irreversible splitting. _Proceedings of the Royal Society A_, 476(2234):20190446, 2020.
* [30] Haksoo Lim, Minjung Kim, Sewon Park, and Noseong Park. Regular time-series generation using sgm. _arXiv preprint arXiv:2301.08518_, 2023.
* [31] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23(7):1661-1674, 2011.
* [32] Simo Sarkka and Arno Solin, editors. _Applied stochastic differential equations_, volume 10. Cambridge University Press, 2019.
* [33] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _CoRR_, abs/2011.13456, 2020.

Proof of Theoretical Results

This Appendix provides proof of the analytical results in Section 3 of the body. First, the parameterizations of \(\mathbf{L},\mathbf{M}\) in terms of exterior algebra are established.

Proof of Lemma 3.2.: First, it is necessary to check that the operators \(\mathbf{L},\mathbf{M}\) parameterized this way satisfy the symmetries and degeneracy conditions claimed in the statement. To that end, recall that \(\mathbf{a}\wedge\mathbf{b}\simeq\mathbf{a}\mathbf{b}^{\intercal}-\mathbf{b}\mathbf{a}^{\intercal}\), meaning that \((\mathbf{a}\mathbf{b}^{\intercal}-\mathbf{b}\mathbf{a}^{\intercal})^{\intercal}\simeq\mathbf{b} \wedge\mathbf{a}=-\mathbf{a}\wedge\mathbf{b}\). It follows that \(\mathbf{A}^{\intercal}\simeq\tilde{\mathbf{A}}=-\mathbf{A}\) where \(\tilde{\mathbf{A}}\) denotes the reversion of \(\mathbf{A}\), i.e., \(\tilde{\mathbf{A}}=\sum_{i<j}A^{ij}\mathbf{e}_{j}\wedge\mathbf{e}_{i}\). Therefore, we may write

\[\mathbf{L}^{\intercal}\simeq\tilde{\mathbf{A}}-\frac{1}{\left|\nabla S\right|^{2}} \overline{\mathbf{A}\nabla S\wedge\nabla S}=-\mathbf{A}+\frac{1}{\left|\nabla S\right| ^{2}}\mathbf{A}\nabla S\wedge\nabla S\simeq-\mathbf{L},\]

showing that \(\mathbf{L}^{\intercal}=-\mathbf{L}\). Moreover, using that

\[(\mathbf{b}\wedge\mathbf{c})\cdot\mathbf{a}=-\mathbf{a}\cdot(\mathbf{b}\wedge\mathbf{c})=(\mathbf{a} \cdot\mathbf{c})\mathbf{b}-(\mathbf{a}\cdot\mathbf{b})\mathbf{c},\]

it follows that

\[\mathbf{L}\nabla S=\mathbf{A}\cdot\nabla S-\frac{1}{\left|\nabla S\right|^{2}}(\mathbf{A} \nabla S\wedge\nabla S)\cdot\nabla S=\mathbf{A}\nabla S-\mathbf{A}\nabla S=\mathbf{0},\]

since \(\nabla S\cdot\mathbf{A}\nabla S=-\nabla S\cdot\mathbf{A}\nabla S=0\). Moving to the case of \(\mathbf{M}\), notice that \(\mathbf{M}=D_{st}\mathbf{v}^{s}\otimes\mathbf{v}^{t}\) for a particular choice of \(\mathbf{v}\), meaning that

\[\mathbf{M}^{\intercal}=\sum_{s,t}D_{st}\big{(}\mathbf{v}^{s}\otimes\mathbf{v}^{t}\big{)}^{ \intercal}=\sum_{s,t}D_{st}\mathbf{v}^{t}\otimes\mathbf{v}^{s}=\sum_{t,s}D_{ts}\mathbf{v}^ {s}\otimes\mathbf{v}^{t}=\sum_{s,t}D_{st}\mathbf{v}^{s}\otimes\mathbf{v}^{t}=\mathbf{M},\]

since \(\mathbf{D}\) is a symmetric matrix. Additionally, it is straightforward to check that, for any \(1\leq s\leq r\),

\[\mathbf{v}^{s}\cdot\nabla E=\left(\mathbf{b}^{s}-\frac{\mathbf{b}^{s}\cdot\nabla E}{\left| \nabla E\right|^{2}}\nabla E\right)\cdot\nabla E=\mathbf{b}^{s}\cdot\nabla E-\bm {b}^{s}\cdot\nabla E=0.\]

So, it follows immediately that

\[\mathbf{M}\nabla E=\sum_{s,t}D_{st}\big{(}\mathbf{v}^{s}\otimes\mathbf{v}^{t}\big{)}\cdot \nabla E=\sum_{s,t}D_{st}\big{(}\mathbf{v}^{t}\cdot\nabla E\big{)}\mathbf{v}^{s}=\mathbf{0}.\]

Now, observe that

\[\mathbf{L} =\mathbf{A}-\frac{1}{\left|\nabla S\right|^{2}}(\mathbf{A}\nabla S(\nabla S )^{\intercal}-\nabla S(\mathbf{A}\nabla S)^{\intercal})\] \[=\mathbf{A}-\frac{1}{\left|\nabla S\right|^{2}}(\mathbf{A}\nabla S(\nabla S ^{\intercal})+\nabla S(\nabla S)^{\intercal}\mathbf{A})\] \[=\Bigg{(}\mathbf{I}-\frac{\nabla S(\nabla S)^{\intercal}}{\left| \nabla S\right|^{2}}\Bigg{)}\mathbf{A}\Bigg{(}\mathbf{I}-\frac{\nabla S(\nabla S)^{ \intercal}}{\left|\nabla S\right|^{2}}\Bigg{)}=\mathbf{P}_{S}^{\perp}\mathbf{A}\mathbf{P}_ {S}^{\perp},\]

since \(\mathbf{A}^{\intercal}=-\mathbf{A}\) and hence \(\mathbf{v}^{\intercal}\mathbf{A}\mathbf{v}=0\) for all \(\mathbf{v}\in\mathbb{R}^{n}\). Similarly, it follows that for every \(1\leq s\leq r\),

\[\mathbf{P}_{E}^{\perp}\mathbf{b}^{s}=\mathbf{b}^{s}-\frac{\mathbf{b}^{s}\cdot\nabla E}{\left| \nabla E\right|^{2}}\nabla E,\]

and therefore \(\mathbf{M}\) is expressible as

\[\mathbf{M}=\sum_{s,t}D_{st}\big{(}\mathbf{P}_{E}^{\perp}\mathbf{b}^{s}\big{)}\big{(}\mathbf{P}_ {E}^{\perp}\mathbf{b}^{t}\big{)}^{\intercal}=\mathbf{P}_{E}^{\perp}\mathbf{B}\mathbf{D}\mathbf{B} ^{\intercal}\mathbf{P}_{E}^{\perp}.\qed\]

With Lemma 3.2 established, the proof of Theorem 3.4 is straightforward.

Proof of Theorem 3.4.: The "if" direction follows immediately from Lemma 3.2. Now, suppose that \(\mathbf{L}\) and \(\mathbf{M}\) define a metriplectic system, meaning that the mentioned symmetries and degeneracy conditions hold. Then, it follows from \(\mathbf{L}\nabla S=\mathbf{0}\) that the projection \(\mathbf{P}_{S}^{\perp}\mathbf{L}\mathbf{P}_{S}^{\perp}=\mathbf{L}\) leaves \(\mathbf{L}\) invariant, so that choosing \(\mathbf{A}=\mathbf{L}\) yields \(\mathbf{P}_{S}^{\perp}\mathbf{A}\mathbf{P}_{S}^{\perp}=\mathbf{L}\). Similarly, from positive semi-definiteness and \(\mathbf{M}\nabla E=\mathbf{0}\) it follows that \(\mathbf{M}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\intercal}=\mathbf{P}_{E}^{\perp}\mathbf{U}\mathbf{\Lambda} \mathbf{U}^{\intercal}\mathbf{P}_{E}^{\perp}\) for some column-orthonormal \(\mathbf{U}\in\mathbb{R}^{N\times r}\) and positive diagonal \(\mathbf{\Lambda}\in\mathbb{R}^{r\times r}\). Therefore, choosing \(\mathbf{B}=\mathbf{U}\) and \(\mathbf{D}=\mathbf{\Lambda}\) yields \(\mathbf{M}=\mathbf{P}_{E}^{\perp}\mathbf{B}\mathbf{D}\mathbf{B}^{\intercal}\mathbf{P}^{\perp}\), as desired.

Looking toward the proof of Proposition 3.7, we also need to establish the following Lemmata which give control over the orthogonal projectors \(\mathbf{P}_{\bar{E}}^{\perp},\mathbf{P}_{\bar{S}}^{\perp}\). First, we recall how control over the \(L^{\infty}\) norm \(\left|\cdot\right|_{\infty}\) of a matrix field gives control over its spectral norm \(\left|\cdot\right|\).

**Lemma A.1**.: _Let \(\mathbf{A}:K\rightarrow\mathbb{R}^{n\times n}\) be a matrix field defined on the compact set \(K\subset\mathbb{R}^{n}\) with \(m\) continuous derivatives. Then, for any \(\varepsilon>0\) there exists a two-layer neural network \(\tilde{\mathbf{A}}:K\rightarrow\mathbb{R}^{n\times n}\) such that \(\sup_{\mathbf{x}\in K}\left|\mathbf{A}-\tilde{\mathbf{A}}\right|<\varepsilon\) and \(\sup_{\mathbf{x}\in K}\left|\nabla^{k}\mathbf{A}-\nabla^{k}\tilde{\mathbf{A}}\right|_{\infty }<\varepsilon\) for \(1\leq k\leq m\) where \(\nabla^{k}\) is the (total) derivative operator of order \(k\)._

Proof.: This will be a direct consequence of Corollary 2.2 in [24] provided we show that \(\left|\mathbf{A}\right|\leq c\left|\mathbf{A}\right|_{\infty}\) for some \(c>0\). To that end, if \(\sigma_{1}\geq...\geq\sigma_{r}>0\) (\(r\leq n\)) denote the nonzero singular values of \(\mathbf{A}-\tilde{\mathbf{A}}\), it follows that for each \(\mathbf{x}\in K\),

\[\left|\mathbf{A}-\tilde{\mathbf{A}}\right|=\sigma_{1}\leq\sqrt{\sigma_{1}^{2}+...+ \sigma_{r}^{2}}=\sqrt{\sum_{i,j}\left|A_{ij}-\tilde{A}_{ij}\right|^{2}}=\left| \mathbf{A}-\tilde{\mathbf{A}}\right|_{F}.\]

On the other hand, it also follows that

\[\left|\mathbf{A}-\tilde{\mathbf{A}}\right|_{F}=\sqrt{\sum_{i,j}\left|A_{ij}-\tilde{A}_ {ij}\right|^{2}}\leq\sqrt{\sum_{i,j}\max_{i,j}\left|A_{ij}-\tilde{A}_{ij}\right| }=n\sqrt{\max_{i,j}\left|A_{ij}-\tilde{A}_{ij}\right|}=n\left|\mathbf{A}-\tilde{ \mathbf{A}}\right|_{\infty},\]

and therefore the desired inequality holds with \(c=n\). Now, for any \(\varepsilon>0\) it follows from [24] that there exists a two layer network \(\tilde{\mathbf{A}}\) with \(m\) continuous derivatives such that \(\sup_{\mathbf{x}\in K}\left|\mathbf{A}-\tilde{\mathbf{A}}\right|_{\infty}<\varepsilon/n\) and \(\sup_{\mathbf{x}\in K}\left|\nabla^{k}\mathbf{A}-\nabla^{k}\tilde{\mathbf{A}}\right|_{ \infty}<\varepsilon/n<\varepsilon\) for all \(1\leq k\leq m\). Therefore, it follows that

\[\sup_{\mathbf{x}\in K}\left|\mathbf{A}-\tilde{\mathbf{A}}\right|\leq n\sup_{\mathbf{x}\in K} \left|\mathbf{A}-\tilde{\mathbf{A}}\right|_{\infty}<n\frac{\varepsilon}{n}=\varepsilon,\]

completing the argument. 

Next, we bound the deviation in the orthogonal projectors \(\mathbf{P}_{\bar{E}}^{\perp},\mathbf{P}_{\bar{S}}^{\perp}\).

**Lemma A.2**.: _Let \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}\) be such that \(\nabla f\neq\mathbf{0}\) on the compact set \(K\subset\mathbb{R}^{n}\). For any \(\varepsilon>0\), there exists a two-layer neural network \(\tilde{f}:K\rightarrow\mathbb{R}\) such that \(\nabla\tilde{f}\neq\mathbf{0}\) on \(K\), \(\sup_{\mathbf{x}\in K}\left|f-\tilde{f}\right|<\varepsilon,\sup_{\mathbf{x}\in K} \left|\nabla f-\nabla\tilde{f}\right|<\varepsilon\), and \(\sup_{\mathbf{x}\in K}\left|\mathbf{P}_{\bar{f}}^{\perp}-\mathbf{P}_{\bar{f}}^{\perp} \right|<\varepsilon\)._

Proof.: Denote \(\nabla f=\mathbf{v}\) and consider any \(\tilde{\mathbf{v}}:K\rightarrow\mathbb{R}\). Since \(\left|\mathbf{v}\right|\leq\left|\tilde{\mathbf{v}}\right|+\left|\mathbf{v}-\tilde{\mathbf{v}}\right|\), it follows for all \(\mathbf{x}\in K\) that whenever \(\left|\mathbf{v}-\tilde{\mathbf{v}}\right|<(1/2)\inf_{\mathbf{x}\in K}\left|\mathbf{v}\right|\),

\[\left|\tilde{\mathbf{v}}\right|\geq\left|\mathbf{v}\right|-\left|\mathbf{v}-\tilde{\mathbf{v} }\right|>\left|\mathbf{v}\right|-\frac{1}{2}\inf_{\mathbf{x}\in K}\left|\mathbf{v}\right|>0,\]

so that \(\tilde{\mathbf{v}}\neq 0\) in \(K\), and since the square function is monotonic,

\[\inf_{\mathbf{x}\in K}\left|\tilde{\mathbf{v}}\right|^{2}\geq\inf_{\mathbf{x}\in K}\left( \left|\mathbf{v}\right|-\frac{1}{2}\inf_{\mathbf{x}\in K}\left|\mathbf{v}\right|\right)^{2 }=\frac{1}{4}\inf_{\mathbf{x}\in K}\left|\mathbf{v}\right|^{2}.\]

On the other hand, we also have \(\left|\tilde{\mathbf{v}}\right|\leq\left|\mathbf{v}\right|+\left|\tilde{\mathbf{v}}-\mathbf{v} \right|<\left|\mathbf{v}\right|+(1/2)\inf_{\mathbf{x}\in K}\left|\mathbf{v}\right|\), so that, adding and subtracting \(\tilde{\mathbf{v}}\mathbf{v}^{\intercal}\) and applying Cauchy-Schwarz, it follows that for all \(\mathbf{x}\in K\),

\[\left|\mathbf{v}\mathbf{v}^{\intercal}-\tilde{\mathbf{v}}\tilde{\mathbf{v}}^{\intercal}\right| \leq\left|\mathbf{v}-\tilde{\mathbf{v}}\right||\mathbf{v}|+\left|\tilde{\mathbf{v}}\right||\bm {v}-\tilde{\mathbf{v}}||\leq 2\max\{\left|\mathbf{v}\right|,\left|\tilde{\mathbf{v}}\right|\}\left|\mathbf{v} -\tilde{\mathbf{v}}\right|<\left(2|\mathbf{v}|+\inf_{\mathbf{x}\in K}\left|\mathbf{v}\right| \right)\left|\mathbf{v}-\tilde{\mathbf{v}}\right|.\]

Now, by Corollary 2.2 in [24], for any \(\varepsilon>0\) there exists a two-layer neural network \(\tilde{f}:K\rightarrow\mathbb{R}\) such that

\[\sup_{\mathbf{x}\in K}\left|\mathbf{v}-\nabla\tilde{f}\right|<\min\left\{\frac{1}{2} \inf_{\mathbf{x}\in K^{\prime}}\left|\mathbf{v}\right|,\frac{\inf_{\mathbf{x}\in K}\left| \mathbf{v}\right|^{2}}{2\sup_{\mathbf{x}\in K}\left|\mathbf{v}\right|+\inf_{\mathbf{x}\in K} \left|\mathbf{v}\right|}\frac{\varepsilon}{4},\varepsilon\right\}\leq\varepsilon,\]and also \(\sup_{\mathbf{x}\in K}\abs{f-\tilde{f}}<\varepsilon\). Letting \(\tilde{\mathbf{v}}=\nabla\tilde{f}\), it follows that for all \(\mathbf{x}\in K\),

\[\abs{\mathbf{P}_{f}^{\perp}-\mathbf{P}_{f}^{\perp}}=\abs{\frac{\mathbf{v}\mathbf{v}^{\intercal} }{\abs{\mathbf{v}}^{2}}-\frac{\tilde{\mathbf{v}}\tilde{\mathbf{v}}^{\intercal}}{\abs{\tilde {\mathbf{v}}}^{2}}}\leq\frac{\abs{\mathbf{v}\mathbf{v}^{\intercal}-\tilde{\mathbf{v}}\tilde{\bm {v}}^{\intercal}}}{\min\paren{\abs{\mathbf{v}}^{2},\abs{\tilde{\mathbf{v}}}^{2}}}\leq \frac{2\abs{\mathbf{v}}+\inf_{\mathbf{x}\in K}\abs{\mathbf{v}}}{\min\paren{\abs{\mathbf{v}}^{2 },\abs{\tilde{\mathbf{v}}}^{2}}}\abs{\mathbf{v}-\tilde{\mathbf{v}}},\]

and therefore, taking the supremum of both sides and applying the previous work yields the desired estimate,

\[\sup_{\mathbf{x}\in K}\abs{\mathbf{P}_{f}^{\perp}-\mathbf{P}_{f}^{\perp}}\leq 4\frac{2 \sup_{\mathbf{x}\in K}\abs{\mathbf{v}}+\inf_{\mathbf{x}\in K}\abs{\mathbf{v}}}{\inf_{\mathbf{x} \in K}\abs{\mathbf{v}}^{2}}\sup_{\mathbf{x}\in K}\abs{\mathbf{v}-\tilde{\mathbf{v}}}<\varepsilon.\qed\]

With these intermediate results established, the proof of the approximation result Proposition 3.7 proceeds as follows.

Proof of Proposition 3.7.: Recall from Theorem 3.4 that we can write \(\mathbf{L}=\mathbf{P}_{S}^{\perp}(\mathbf{A}_{\mathrm{tri}}-\mathbf{A}_{\mathrm{tri}}^{\intercal })\mathbf{P}_{S}^{\perp}\) and similarly for \(\tilde{\mathbf{L}}\). Notice that, by adding and subtracting \(\mathbf{P}_{S}^{\perp}\mathbf{A}_{\mathrm{tri}}\mathbf{P}_{S}^{\perp}\) and \(\mathbf{P}_{S}^{\perp}\tilde{\mathbf{A}}_{\mathrm{tri}}\mathbf{P}_{S}^{\perp}\), it follows that for all \(\mathbf{x}\in K\),

\[\abs{\mathbf{P}_{S}^{\perp}\mathbf{A}_{\mathrm{tri}}\mathbf{P}_{S}^{\perp}- \mathbf{P}_{S}^{\perp}\tilde{\mathbf{A}}_{\mathrm{tri}}\mathbf{P}_{S}^{\perp}}\] \[\qquad=\abs{\paren{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}}\mathbf{A}_ {\mathrm{tri}}\mathbf{P}_{S}^{\perp}+\mathbf{P}_{S}^{\perp}\paren{\mathbf{A}_{\mathrm{tri}} -\tilde{\mathbf{A}}_{\mathrm{tri}}}\mathbf{P}_{S}^{\perp}+\mathbf{P}_{S}^{\perp}\tilde{ \mathbf{A}}_{\mathrm{tri}}\paren{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}}}\] \[\qquad\leq\abs{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}}\abs{\mathbf{A}_ {\mathrm{tri}}}+\abs{\mathbf{A}_{\mathrm{tri}}-\tilde{\mathbf{A}}_{\mathrm{tri}}}+ \abs{\tilde{\mathbf{A}}_{\mathrm{tri}}}\abs{\abs{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{ \perp}}}\] \[\qquad\leq 2\max\paren{\abs{\mathbf{A}_{\mathrm{tri}}},\abs{\tilde{ \mathbf{A}}_{\mathrm{tri}}}}\abs{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}}+\abs{\mathbf{A }_{\mathrm{tri}}-\tilde{\mathbf{A}}_{\mathrm{tri}}}}\]

where we have used that \(\mathbf{P}_{S}^{\perp},\mathbf{P}_{S}^{\perp}\) have unit spectral norm. By Lemma A.1, for any \(\varepsilon>0\) there exists a two layer neural network \(\tilde{\mathbf{A}}_{\mathrm{tri}}\) such that \(\sup_{\mathbf{x}\in K}\abs{\mathbf{A}_{\mathrm{tri}}-\tilde{\mathbf{A}}_{\mathrm{tri}}}< \frac{\varepsilon}{4}\), and by Lemma A.2 there exists a two-layer network \(\tilde{S}\) with \(\nabla\tilde{S}\neq\mathbf{0}\) on \(K\) such that

\[\sup_{\mathbf{x}\in K}\abs{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}}<\min\Bigg{\{} \varepsilon,\max\paren{\sup_{\mathbf{x}\in K}\abs{\mathbf{A}_{\mathrm{tri}}},\sup_{\bm {x}\in K}\abs{\tilde{\mathbf{A}}_{\mathrm{tri}}}}}^{-1}\frac{\varepsilon}{8} \Bigg{\}}.\]

It follows that \(\tilde{S},\nabla\tilde{S}\) are \(\varepsilon\)-close to \(S,\nabla S\) on \(K\) and

\[\sup_{\mathbf{x}\in K}\paren{2\max\paren{\mathbf{A}_{\mathrm{tri}}},\abs{\tilde{\mathbf{A} }_{\mathrm{tri}}}}\abs{\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}}}<\frac{ \varepsilon}{4}.\]

Therefore, the estimate

\[\sup_{\mathbf{x}\in K}\abs{\mathbf{L}-\tilde{\mathbf{L}}}\leq 2\sup_{\mathbf{x}\in K}\abs{\mathbf{P}_{S}^{ \perp}\mathbf{A}_{\mathrm{tri}}\mathbf{P}_{S}^{\perp}-\mathbf{P}_{S}^{\perp}\tilde{\mathbf{A} }_{\mathrm{tri}}\mathbf{P}_{S}^{\perp}}<2\paren{\frac{\varepsilon}{4}+\frac{ \varepsilon}{4}}=\varepsilon,\]

implies that \(\tilde{\mathbf{L}}\) is \(\varepsilon\)-close to \(\mathbf{L}\) on \(K\) as well.

Moving to the case of \(\mathbf{M}\), we see that for all \(\mathbf{x}\in K\), by writing \(\mathbf{M}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\intercal}=\mathbf{K}_{\mathrm{chol}}\mathbf{K}_{ \mathrm{chol}}^{\intercal}\) for \(\mathbf{K}_{\mathrm{chol}}=\mathbf{U}\mathbf{\Lambda}^{1/2}\) and repeating the first calculation with \(\mathbf{K}_{\mathrm{chol}}\) in place of \(\mathbf{A}_{\mathrm{tri}}\) and \(\mathbf{P}_{E}^{\perp}\) in place of \(\mathbf{P}_{S}^{\perp}\),

\[\abs{\mathbf{P}_{E}^{\perp}\mathbf{K}_{\mathrm{chol}}\mathbf{K}_{\mathrm{chol} }^{\intercal}\mathbf{P}_{E}^{\perp}-\mathbf{P}_{E}^{\perp}\tilde{\mathbf{K}}_{\mathrm{chol} }\tilde{\mathbf{K}}_{\mathrm{chol}}^{\intercal}\mathbf{P}_{E}^{\perp}}\] \[\qquad\leq 2\max\paren{\mathbf{K}_{\mathrm{chol}}},\abs{\tilde{\mathbf{K} }_{\mathrm{chol}}}\Big{\}}\abs{\mathbf{P}_{E}^{\perp}-\mathbf{P}_{E}^{\perp}}+\abs{\mathbf{K} _{\mathrm{chol}}\mathbf{K}_{\mathrm{chol}}^{\intercal}-\tilde{\mathbf{K}}_{\mathrm{ chol}}\tilde{\mathbf{K}}_{\mathrm{chol}}^{\intercal}}\Big{\rvert}.\]

Moreover, if \(\abs{\mathbf{K}_{\mathrm{chol}}-\tilde{\mathbf{K}}_{\mathrm{chol}}}<(1/2)\inf_{\mathbf{x} \in K}\abs{\mathbf{K}_{\mathrm{chol}}}\) for all \(\mathbf{x}\in K\) then similar arguments as used in the proof of Lemma A.2 yield the following estimate for all \(\mathbf{x}\in K\),

\[\abs{\mathbf{K}_{\mathrm{chol}}\mathbf{K}_{\mathrm{chol}}^{\intercal}- \tilde{\mathbf{K}}_{\mathrm{chol}}\tilde{\mathbf{K}}_{\mathrm{chol}}^{\intercal}} \leq 2\max\paren{\mathbf{K}_{\mathrm{chol}}},\abs{\tilde{\mathbf{K}}_{ \mathrm{chol}}}\Big{\}}\Big{\rvert}\abs{\mathbf{K}_{\mathrm{chol}}-\tilde{\mathbf{K}}_{ \mathrm{chol}}}\] \[\leq\paren{2\abs{\mathbf{K}_{\mathrm{chol}}}+\inf_{\mathbf{x}\in K}\abs{ \mathbf{K}_{\mathrm{chol}}}}\Big{\}}\abs{\mathbf{K}_{\mathrm{chol}}-\tilde{\mathbf{K}}_{ \mathrm{chol}}}.\]

[MISSING_PAGE_FAIL:15]

By Proposition 3.7, there exists a two-layer network \(\tilde{\mathbf{M}}\) with one continuous derivative such that \(\sup_{\mathbf{x}\in K}\Bigl{|}\mathbf{M}-\tilde{\mathbf{M}}\Bigr{|}<\varepsilon\), with \(\tilde{\mathbf{M}}\) Lipschitz continuous for the same reason as before. It follows from this and \(\sup_{\mathbf{x}\in K}\Bigl{|}\nabla S-\nabla\tilde{S}\Bigr{|}<\varepsilon\) that

\[|\dot{\mathbf{y}}_{S}|\leq\biggl{(}L_{\nabla S}\sup_{\mathbf{x}\in K}\lvert\mathbf{M} \rvert+L_{\tilde{\mathbf{M}}}\sup_{\mathbf{x}\in K}\lvert\nabla S\rvert\biggr{)}\lvert \mathbf{y}\rvert+\varepsilon\biggl{(}\sup_{\mathbf{x}\in K}\Bigl{|}\tilde{\mathbf{M}} \Bigr{|}+\sup_{\mathbf{x}\in K}\lvert\nabla S\rvert\biggr{)}=:a_{S}\lvert\mathbf{y} \rvert+\varepsilon\,b_{S}.\]

Now, recall that \(\partial_{t}\lvert\mathbf{y}\rvert=\lvert\mathbf{y}\rvert^{-1}(\dot{\mathbf{y}}\cdot\mathbf{y}) \leq\lvert\dot{\mathbf{y}}\rvert\) by Cauchy-Schwarz, and therefore the time derivative of \(\lvert\mathbf{y}\rvert\) is bounded by

\[\partial_{t}\lvert\mathbf{y}\rvert\leq\lvert\dot{\mathbf{y}}_{E}\rvert+ \lvert\dot{\mathbf{y}}_{S}\rvert=(a_{E}+a_{S})\lvert\mathbf{y}\rvert+\varepsilon(b_{E }+b_{S})=:a\lvert\mathbf{y}\rvert+b.\]

This implies that \(\partial_{t}\lvert\mathbf{y}\rvert-a\lvert\mathbf{y}\rvert\leq b\), so multiplying by the integrating factor \(e^{-at}\) and integrating in time yields

\[\lvert\mathbf{y}(t)\rvert\leq\varepsilon b\int_{0}^{t}e^{a(t-\tau)}\,d\tau= \varepsilon\frac{b}{a}\bigl{(}e^{at}-1\bigr{)},\]

where we used that \(\mathbf{y}(0)=\mathbf{0}\) since the initial condition of the trajectories is shared. Therefore, the \(L^{2}\) error in time can be approximated by

\[\lVert\mathbf{y}\rVert^{2}=\int_{0}^{T}\lvert\mathbf{y}\rvert^{2}\,dt\leq \varepsilon^{2}\frac{b^{2}}{a^{2}}\bigl{(}e^{2aT}-2e^{aT}+T+1\bigr{)},\]

establishing the conclusion. 

## Appendix B Experimental and Implementation Details

This Appendix records additional details related to the numerical experiments in Section 5. For each benchmark problem, a set of trajectories is manufactured given initial conditions by simulating ODEs with known metriplectic structure. For the experiments in Table 2, only the observable variables are used to construct datasets, since entropic information is assumed to be unknown. Algorithm 2 summarizes the training of the dynamics models used for comparison with NMS.

```
1:Input: snapshot data \(\mathbf{X}\in\mathbb{R}^{n\times n_{s}}\), each column \(\mathbf{x}_{s}=\mathbf{x}(t_{s},\mathbf{\mu}_{s})\), target rank \(r\geq 1\)
2:Initialize loss \(L=0\) and networks with parameters \(\Theta\)
3:for step in \(N_{\mathrm{steps}}\)do
4: Randomly draw an initial condition \((t_{0_{k}},\mathbf{x}_{0_{k}})\) where \(k\in n_{s}\)
5:\(\tilde{\mathbf{x}}_{1},...,\tilde{\mathbf{x}}_{l}=\mathrm{ODEsolve}(\mathbf{x}_{0_{k}}, \dot{\mathbf{x}}_{l},1,...,t_{l})\)
6: Compute the loss \(L((\mathbf{x}_{1}^{o},\dots,\mathbf{x}_{l}^{o}),(\tilde{\mathbf{x}}_{0}^{o},\dots,\tilde{ \mathbf{x}}_{l}^{o}))\)
7: Update the model parameters \(\Theta\) via SGD
8:endfor
```

**Algorithm 2** Training dynamics models

For each compared method, integrating the ODEs is done via the Dormand-Prince method (dopri5) [27] with relative tolerance \(10^{-7}\) and absolute tolerance \(10^{-9}\). The loss is evaluated by measuring the discrepancy between the ground truth observable states \(\mathbf{x}^{o}\) and the approximate observable states \(\tilde{\mathbf{x}}^{o}\) in the mean absolute error (MAE) metric. The model parameters \(\Theta\) (i.e., the weights and biases) are updated by using Adamax [28] with an initial learning rate of 0.01. The number of training steps is set as 30,000, and the model parameters resulting in the best performance for the validation set are chosen for testing. Specific information related to the experiments in Section 5 is given in the subsections below.

For generating the results reported in Table 2, we implemented the proposed algorithm in Python 3.9.12 and PyTorch 2.0.0. Other required information is provided with the accompanying code. All experiments are conducted on Apple M2 Max chips with 96 GB memory. To provide the mean and the standard deviation, experiments are repeated three times with varying random seeds for all considered methods.

### Two gas containers

As mentioned in the body, the two gas container (TGC) problem tests models' predictive capability (i.e., extrapolation in time). To this end, one simulated trajectory is obtained by solving an IVP with a known TGC system and an initial condition, and the trajectory of the observable variables is split into three subsequences, [0, \(t_{\text{train}}\)], (\(t_{\text{train}},t_{\text{val}}\)], and (\(t_{\text{val}},t_{\text{test}}\)] for training, validation, and test with \(0<t_{\text{train}}<t_{\text{val}}<t_{\text{test}}\).

In the experiment, a sequence of 100,000 timesteps is generated using the Runge-Kutta 4th-order (RK4) time integrator with a step size 0.001. The initial condition is given as \(\mathbf{x}=(1,2,103.2874,103.2874)\) following [29]. The training/validation/test split is defined by \(t_{\text{train}}=20\), \(t_{\text{val}}=30\), and \(t_{\text{test}}=100\). For a fair comparison, all considered models are set to have a similar number of model parameters, \(\sim\)2,000. The specifications of the network architectures are:

* NMS: The total number of model parameters is 1959. The functions \(\mathbf{A}_{\text{tri}},\mathbf{B},\mathbf{K}_{\text{chol}},E,S\) are parameterized as MLPs with the Tanh nonlinear activation function. The MLPs parameterizing \(\mathbf{A}_{\text{tri}},\mathbf{B},\mathbf{K}_{\text{chol}},E\) are specified as 1 hidden layer with 10 neurons, and the on parameterizing \(S\) is specified as 3 hidden layers with 25 neurons.
* NODE: The total number of model parameters is 2179. The black-box NODE is parameterized as an MLP with the Tanh nonlinear activation function, 4 hidden layers and 25 neurons.
* SPNN: The total number of model parameters is 1954. The functions \(E\) and \(S\) are parameterized as MLPs with the Tanh nonlinear activation function; each MLP is specified as 3 hidden layers and 20 neurons. The two 2-tensors defining \(\mathbf{L}\) and \(\mathbf{M}\) are defined as learnable \(3\times 3\) matrices.
* GNODE: The total number of model parameters is 2343. The functions \(E\) and \(S\) are parameterized as MLPs with the Tanh nonlinear activaton function; each MLP is specified as 2 hidden layers and 30 neurons. The matrices and 3-tensors required to learn \(\mathbf{L}\) and \(\mathbf{M}\) are defined as learnable \(3\times 3\) matrices and \(3\times 3\times 3\) tensor.
* GFINN: The total number of model parameters is 2065. The functions \(E\) and \(S\) are parameterized as MLPs with Tanh nonlinear activation function; each MLP is specified as 2 hidden layers and 20 neurons. The matrices to required to learn \(\mathbf{L}\) and \(\mathbf{M}\) are defined as \(K\) learnable \(3\times 3\) matrices, where \(K\) is set to 2.

### Thermoelastic double pendulum

The equations of motion in this case are given for \(1\leq i\leq 2\) as

\[\dot{\mathbf{q}}_{i}=\frac{\mathbf{p}_{i}}{m_{i}},\quad\dot{\mathbf{p}}_{i}=-\partial_{ \mathbf{q}_{i}}(E_{1}(\mathbf{x})+E_{2}(\mathbf{x})),\quad\dot{S}_{1}=\kappa\big{(}T_{1}^ {-1}T_{2}-1\big{)},\quad\dot{S}_{2}=\kappa\big{(}T_{1}T_{2}^{-1}-1\big{)},\]

where \(\kappa>0\) is a thermal conductivity constant (set to 1), \(m_{i}\) is the mass of the \(i^{\text{th}}\) spring (also set to 1) and \(T_{i}=\partial_{S_{i}}E_{i}\) is its absolute temperature. In this case, \(\mathbf{q}_{i},\mathbf{p}_{i}\in\mathbb{R}^{2}\) represent the position and momentum of the \(i^{\text{th}}\) mass, while \(S_{i}\) represents the entropy of the \(i^{\text{th}}\) pendulum. As before, the total entropy \(S(\mathbf{x})=S_{1}+S_{2}\) is the sum of the entropies of the two springs, while defining the internal energies

\[E_{i}(\mathbf{x})=\frac{1}{2}(\ln\lambda_{i})^{2}+\ln\lambda_{i}+e^{S_{i}-\ln \lambda_{i}}-1,\quad\lambda_{1}=|\mathbf{q}_{i}|,\quad\lambda_{2}=|\mathbf{q}_{2}-\bm {q}_{1}|,\]

leads to the total energy \(E(\mathbf{x})=(1/2m_{1})|\mathbf{p}_{1}|^{2}+(1/2m_{2})|\mathbf{p}_{2}|^{2}+E_{1}(\mathbf{x})+E _{2}(\mathbf{x})\).

The thermoelastic double pendulum experiment tests model prediction across initial conditions. In this case, 100 trajectories are generated by varying initial conditions that are randomly sampled from [0.1,1.1] \(\times\) [-0.1,0.1] \(\times\) [2.1, 2.3] \(\times\) [-0.1,0.1] \(\times\) [-1.9,2.1] \(\times\) [0.9,1.1] \(\times\) [-0.1, 0.1] \(\times\) [0.9,1.1] \(\times\) [0.9,1.1] \(\times\) [0.1,0.3] \(\subset\mathbb{R}^{10}\). Each trajectory is obtained from the numerical integration of the ODEs using an RK4 time integrator with step size 0.02 and the final time \(T=40\), resulting in the trajectories of length 2,000. The resulting 100 trajectories are split into 80/10/10 for training/validation/test sets. For a fair comparison, all considered models are again set to have similar number of model parameters, \(\sim\)2,000. The specifications of the network architectures are:* NMS: The total number of model parameters is 2201. The functions \(\mathbf{A},\mathbf{B},\mathbf{K},E,S\) are parameterized as MLPs with the Tanh nonlinear activation function. The MLPs parameterizing are specified as 1 hidden layer with 15 neurons.
* NODE: The total number of model parameters is 2005. The black-box NODE is parameterized as an MLP with the Tanh nonlinear activation function, 2 hidden layers and 35 neurons.
* SPNN: The total number of model parameters is 2362. The functions \(E\) and \(S\) are parameterized as MLPs with the Tanh nonlinear activation function; each MLP is specified as 3 hidden layers and 20 neurons. The two 2-tensors defining \(\mathbf{L}\) and \(\mathbf{M}\) are defined as learnable \(3\times 3\) matrices.
* GNODE: The total number of model parameters is 2151. The functions \(E\) and \(S\) are parameterized as MLPs with the Tanh nonlinear activaton function; each MLP is specified as 2 hidden layers and 15 neurons. The matrices and 3-tensors required to learn \(\mathbf{L}\) and \(\mathbf{M}\) are defined as learnable \(3\times 3\) matrices and \(3\times 3\times 3\) tensor.
* GFINN: The total number of model parameters is 2180. The functions \(E\) and \(S\) are parameterized as MLPs with Tanh nonlinear activation function; each MLP is specified as 2 hidden layers and 15 neurons. The matrices to required to learn \(\mathbf{L}\) and \(\mathbf{M}\) are defined as \(K\) learnable \(3\times 3\) matrices, where \(K\) is set to 2.

## Appendix C Additional experiment: Damped nonlinear oscillator

Consider a damped nonlinear oscillator of variable dimension with state \(\mathbf{x}=(\mathbf{q}\quad\mathbf{p}\quad S)^{\intercal}\), whose motion is governed by the triaplectic system

\[\dot{\mathbf{q}}=\frac{\mathbf{p}}{m},\quad\dot{\mathbf{p}}=k\sin\mathbf{q}-\gamma\mathbf{p},\quad \dot{S}=\frac{\gamma\abs{\mathbf{q}}^{2}}{mT}.\]

Here \(\mathbf{q},\mathbf{p}\in\mathbb{R}^{n}\) denote the position and momentum of the oscillator, \(S\) is the entropy of a surrounding thermal bath, and the constant parameters \(m,\gamma,T\) are the mass, damping rate, and (constant) temperature. This leads to the total energy \(E(\mathbf{x})=(1/2m)\abs{\mathbf{p}}^{2}-k\cos\mathbf{q}+TS,\) which is readily seen to be constant along solutions \(\mathbf{x}(t)\).

It is now verified that NMS can accurately and stably predict the dynamics of a nonlinear oscillator \(\mathbf{x}=(\mathbf{q}\quad\mathbf{p}\quad S)^{\intercal}\) in the case that \(n=1,2\), both when the entropy \(S\) is observable as well as when it is not. As before, the task considered is prediction in time, although all compared methods NODE, GNODE, and NMSknown are now trained on full state information from the training interval, and test errors are computed over the full state \(\mathbf{x}\) on the extrapolation interval \((t_{\rm valid},t_{\rm test}]\), which is \(150\%\) longer than the training interval. In addition, another NMS model, NMSdiff, was trained using only the partial state information \(\mathbf{x}^{o}=(\mathbf{q},\mathbf{p})^{\intercal}\)and tested under the same conditions, with the initial guess for \(\mathbf{x}^{u}\) generated as in Appendix E. As can be seen in Table 3, NMS is more accurate than GNODE or NODE in both the 1-D and 2-D nonlinear oscillator experiments, improving on previous results by up to two orders of magnitude. Remarkably, NMS produces more accurate entropic dynamics even in the case where the entropic variable \(S\) is unobserved during NMS training and observed during the training of other methods. This illustrates another advantage of the NMS approach: because of the reasonable initial data for \(S\) produced by the diffusion model, the learned metriplectic system produced by NMS remains performant even when metriplectic governing equations are unknown and only partial state information is observed.

To describe the experimental setup precisely, data is collected from a single trajectory with initial condition as \(\mathbf{x}=(\mathbf{2},\mathbf{0},0)\) following [16]. The path is calculated at 180,000 steps with a time interval of 0.001, and is then split into training/validation/test sets as before using \(t_{\rm train}=60\), \(t_{\rm val}=90\) and \(t_{\rm test}=180\). Specifications of the networks used for the experiments in Table 3 are:

* NMS: The total number of parameters is 154. The number of layers for \(\mathbf{A}_{\rm tri},\mathbf{B},\mathbf{K}_{\rm chol},E,S\) is selected from {1,2,3} and the number of neurons per layer from {5,10,15}. The best hyperparameters are 1 hidden layer with 5 neurons for each network function.
* GNODE: The total number of model parameters is 203. The number of layers and number of neurons for each network is chosen from the same ranges as for NMS. The best hyperparameters are 1 layer with 10 neurons for each network function.

* NODE: The total number of model paramters is 3003. The NODE architecture is formed by stacking MLPs with Tanh activation functions. The number of blocks is chosen from {3,4,5} and the number of neurons of each MLP from {30,40,50}. The best hyperparameters are 4 and 30 for the number of blocks and number of neurons, respectively.

## Appendix D Scaling study

To compare the scalability of the proposed NMS architecture design with existing architectures, different realizations of GNODE, GFINN, and NMS are generated by varying the dimension of the state variables, \(n=\{1,5,10,15,20,30,50\}\). The specifications of these models (i.e., hyperparameters) are set so that the number of model parameters is kept similar between each method for smaller values of \(n\). For example, for \(n=1,5\) the number of model parameters is \(\sim\)20,000 for each architecture. The results in Figure 3(a) confirm that GNODE scales cubically in \(n\) while both GFINN and NMS scale quadratically. Note that only a constant scaling advantage of NMS over GFINN can be seen from this plot, since \(r\) is fixed during this study.

It is also worthwhile to investigate the computational timings of these three models. Considering the same realizations of the models listed above, i.e., the model instances for varying \(n=\{1,5,10,15,20,30,50\}\), 1,000 random samples of states \(\{\mathbf{x}^{(i)}\}_{i=1}^{1,000}\) are generated. These samples are then fed to the dynamics function \(\mathbf{L}(\mathbf{x}^{(i)})\nabla E(\mathbf{x}^{(i)})+\mathbf{M}(\mathbf{x}^{(i)})\nabla S(\mathbf{x} ^{(i)})\) for \(i=1,\ldots,1000\), and the computational wall time of the function evaluation via PyTorch's profiler API is measured. The results of this procedure are displayed in Figure 3(b). Again, it is seen that the proposed NMSs require less computational resources than GNODEs and GFINNs.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{2}{c}{1-D D.N.O.} & \multicolumn{2}{c}{T.G.C.} & \multicolumn{2}{c}{2-D D.N.O.} \\  & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \(\mathbf{NMS}_{\mathrm{diff}}\) &.0170 &.1132 &.0045 &.0548 &.0275 &.1456 \\ \(\mathbf{NMS}_{\mathrm{known}}\) &.0239 &.1011 &.0012 &.0276 &.0018 &.0357 \\ \hline NODE &.0631 &.2236 &.0860 &.2551 &.0661 &.2096 \\ GNODE &.0607 &.1976 &.0071 &.0732 &.2272 &.4267 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Experimental results for the benchmark problems with respect to MSE and MAE. The best scores are in boldface.

Figure 3: A study of the scaling behavior of GNODE, GFINN, and NMS.

Diffusion model for unobserved variables

Recent work in [30] suggests the benefits of performing time-series generation using a diffusion model. This Appendix describes how this technology is used to generate initial conditions for the unobserved NMS variables in the experiments corresponding to Table 3. More precisely, we describe how to train a conditional diffusion model which generates values for unobserved variables \(\mathbf{x}^{u}\) given values for the observed variables \(\mathbf{x}^{o}\).

Training and sampling:Recall that diffusion models add noise with the following stochastic differential equation (SDE):

\[d\textbf{x}(t)=\textbf{f}(t,\textbf{x}(t))dt+g(t)d\textbf{w},\quad t\in[0,1],\]

where \(\textbf{w}\in\mathbb{R}^{\dim(\textbf{x})}\) is a multi-dimensional Brownian motion, \(\textbf{f}(t,\cdot):\mathbb{R}^{\dim(\textbf{x})}\rightarrow\mathbb{R}^{\dim( \textbf{x})}\) is a vector-valued drift term, and \(g:[0,1]\rightarrow\mathbb{R}\) is a scalar-valued diffusion function.

For the forward SDE, there exists a corresponding reverse SDE:

\[d\textbf{x}(t)=[\textbf{f}(t,\textbf{x}(t))-g^{2}(t)\nabla_{\textbf{x}(t)}\log p (\textbf{x}(t))]dt+g(t)d\bar{\textbf{w}},\]

which produces samples from the initial distribution at \(t=0\). This formula suggests that if the score function, \(\nabla_{\textbf{x}(t)}\log p(\textbf{x}(t))\), is known, then real samples from the prior distribution \(p(\textbf{x})\sim\mathcal{N}(\mu,\sigma^{2})\) can be recovered, where \(\mu,\sigma\) vary depending on the forward SDE type.

In order for a model \(M_{\theta}\) to learn the score function, it has to optimize the following loss:

\[L(\theta)=\mathbb{E}_{t}\{\lambda(t)\mathbb{E}_{\textbf{x}(t)}\big{[}\big{\|}M _{\theta}(t,\textbf{x}(t))-\nabla_{\textbf{x}(t)}\log p(\textbf{x}(t))\big{\|} _{2}^{2}\big{]}\},\]

where \(t\) is uniformly sampled over \([0,1]\) with an appropriate weight function \(\lambda(t):[0,1]\rightarrow\mathbb{R}\). However, using the above formula is computationally prohibitive. Thanks to [31], this loss can be substituted with the following denoising score matching loss:

\[L^{*}(\theta)=\mathbb{E}_{t}\{\lambda(t)\mathbb{E}_{\textbf{x}(0)}\mathbb{E}_{ \textbf{x}(t)(\textbf{x}(0)}\big{\|}\big{\|}M_{\theta}(t,\textbf{x}(t))-\nabla_ {\textbf{x}(t)}\log p(\textbf{x}(t)|\textbf{x}(0))\big{\|}_{2}^{2}\}\}.\]

Since score-based generative models use an affine drift term, the transition kernel \(p(\textbf{x}(t)|\textbf{x}(0))\) follows a certain Gaussian distribution [32], and therefore the gradient term \(\nabla_{\textbf{x}(t)}\log p(\textbf{x}(t)|\textbf{x}(0))\) can be analytically calculated.

Experimental detailsOn the other hand, the present goal is to generate unobserved variables \(\mathbf{x}^{u}\) given values for the observed variables \(\mathbf{x}^{o}=(\mathbf{q},\mathbf{p})\), i.e., conditional generation. Therefore, our model has to learn the conditional score function, \(\nabla_{\mathbf{x}^{u}(t)}\log p(\mathbf{x}^{u}(t)|\mathbf{x}^{o})\). For example, in the damped nonlinear oscillator case, \(S(t)\) is initialized as a perturbed \(t\in[0,1]\), from which the model takes the concatenation of \(\mathbf{q},\mathbf{p},S(t)\) as inputs and learns conditional the score function \(\nabla_{S(t)}\log(S(t)|\mathbf{q},\mathbf{p})\).

For the experiments in Table 3, diffusion models are trained to generate \(\mathbf{x}^{u}\) variables on three benchmark problems: the damped nonlinear oscillator, two gas containers, and thermolastic double pendulum. On each problem, representative parameters such as mass or thermal conductivity are varied, with the total number of cases denoted by \(N\). Full trajectory data of length \(T\) is then generated using a standard numerical integrator (e.g., dopri5), before it is evenly cut into \([T/L]\) pieces of length \(L\). Let \(V,U\) denote the total number of variables and the number of unobserved variables, respectively. It follows that the goal is to generate \(U\) unobserved variables given \(V-U\) observed ones, i.e., the objective is to generate data of shape \((NT/L,L,U)\) conditioned on data of shape \((NT/L,L,V-U)\). After the diffusion model has been trained for this task, the output data is reshaped into size \((N,T,U)\), which is used to initialize the NMS model. Note that the NODE and GNODE methods compared to NMS in Table 3 use full state information for their training, i.e., \(\mathbf{x}^{u}=\varnothing\) in these cases, making it comparatively easier for these methods to learn system dynamics.

As in other diffusion models e.g. [33], a U-net architecture is used, modifying 2-D convolutions to 1-D ones and following the detailed hyperparameters described in [33]. Note the following _probability flow_ ODE seen in [33]:

\[d\textbf{x}(t)=\bigg{[}\textbf{f}(t,\textbf{x}(t))-\frac{1}{2}g^{2}(t)\nabla_{ \textbf{x}(t)}\log p(\textbf{x}(t))\bigg{]}dt,\]

Although models trained to mimic the probability flow ODE do not match the perofrmance of the forward SDE's result in the image domain, the authors of [30] observe that the probability flow ODE outperforms the forward SDE in the time-series domain. Therefore, the probability flow ODE is used with the default hyperparameters of [33].

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

### The checklist answers are an integral part of your paper submission.

They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

#### IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and contributions paragraph at the end of the introduction are justified in detail throughout the rest of the paper. Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Limitations are discussed in the Conclusion section.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**

Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

Answer: [Yes]

Justification: All theoretical results are clearly stated along with the necessary assumptions. All formal arguments are complete and contained in the Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes]

Justification: All information necessary to implement the proposed architecture is included in the body of the manuscript. In addition, all relevant experimental details are included in the Appendix.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code for running the proposed algorithm is included in the supplemental material and will be released publicly upon publication. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ** At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All relevant experimental details are presented in the Appendix at an appropriate level of detail. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments in the body contain means and standard deviations as the initialization is varied. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All necessary information is included in the Appendix. Guidelines: ** The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This is explained in the "broader impacts" section. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This paper investigates a novel machine learning method tailored to physics-based simulations and fundamental science. While subsequent applications of this work may have societal impact, the research presented here is strictly foundational and only serves to improve the production of physically realistic dynamics from data. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

Answer: [NA]

Justification: N/A

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

Answer: [NA]

Justification: N/A

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer: [NA]

Justification: N/A

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

[MISSING_PAGE_EMPTY:27]