# Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization

 Quanqi Hu

Department of Computer Science

Texas A&M University

College Station, TX 77843

quanqi-hu@tamu.edu &Dixian Zhu

Department of Genetics

Stanford University

Stanford, CA 94305

dixian-zhu@stanford.edu &Tianbao Yang

Department of Computer Science

Texas A&M University

College Station, TX 77843

tianbao-yang@tamu.edu

###### Abstract

This paper investigates new families of compositional optimization problems, called non-smooth weakly-convex finite-sum coupled compositional optimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an \(\epsilon\)-stationary point of the Moreau envelop of the objective function. Additionally, we also extend the algorithm to solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, which feature a nested arrangement of three functions. Lastly, we explore the applications of our algorithms in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization, using empirical studies to showcase the effectiveness of the proposed algorithms.

## 1 Introduction

In this paper, we consider two classes of non-convex compositional optimization problems. The first class is formulated as following:

\[\min_{\mathbf{w}\in\mathbb{R}^{d}}F(\mathbf{w}):=\frac{1}{n}\sum\nolimits_{i \in\mathcal{S}}f_{i}(\mathbb{E}_{\xi\sim\mathcal{D}_{i}}[g_{i}(\mathbf{w}; \xi)]),\] (1)

where \(\mathcal{S}\) denotes a finite set of \(n\) items and \(\mathcal{D}_{i}\) denotes a distribution that could depend on \(i\). The second class is given by:

\[\min_{\mathbf{w}\in\mathbb{R}^{d}}F(\mathbf{w}):=\frac{1}{n_{1}}\sum\nolimits_ {i\in\mathcal{S}_{1}}f_{i}\left(\frac{1}{n_{2}}\sum\nolimits_{j\in\mathcal{S} _{2}}g_{i}(\mathbb{E}_{\xi\sim\mathcal{D}_{i,j}}[h_{i,j}(\mathbf{w};\xi)]) \right),\] (2)

where \(\mathcal{S}_{1}\) denotes a finite set of \(n_{1}\) items and \(\mathcal{S}_{2}\) denotes a finite set of \(n_{2}\) items and \(\mathcal{D}_{ij}\) denotes a distribution that could depend on \((i,j)\). For simplicity of discussion, we denote by\(\mathbb{E}_{\xi\sim\mathcal{D}_{i}}[g_{i}(\mathbf{w};\xi)]:\mathbb{R}^{d}\to \mathbb{R}^{d_{i}}\) and by \(h_{i,j}(\mathbf{w})=\mathbb{E}_{\xi\sim\mathcal{D}_{i,j}}[h_{i,j}(\mathbf{w}; \xi)]:\mathbb{R}^{d}\to\mathbb{R}^{d_{2}}\). For both classes of problems, we focus our attention on **non-convex \(F\) with non-smooth non-convex functions \(f_{i}\) and \(g_{i}\)**, which, to the best of our knowledge, has not been studied in any prior works.

The first problem (1) with smooth functions \(f_{i}\) and \(g_{i}\) has been explored in previous works [26, 15, 21, 33], which is known as finite-sum coupled compositional optimization (FCCO). It is subtly different from standard stochastic compositional optimization (SCO) [27] and conditional stochastic optimization (CSO) [13]. FCCO has been successfully applied to optimizing a wide range of X-risks [33] with convergence guarantee, including smooth surrogate losses of areas under the curves [20] and ranking measures [21], listwise losses [21], and contrastive losses [36]. The second problem (2) is a novel class and is referred to as tri-level finite-sum coupled compositional optimization (TCCO). Both problems differ from traditional two-level or multi-level compositional optimization due to the coupling of variables \(i,\xi\) in (1) or the coupling of variables \(i,j,\xi\) in (2) at the inner most level.

One limitation of prior works about non-convex FCCO is that their convergence analysis heavily rely on the smoothness conditions of \(f_{i}\) and \(g_{i}\)[26, 15]. This raises a concern about whether existing techniques can be leveraged for solving _non-smooth non-convex FCCO problems_ with non-asymptotic convergence guarantee. Non-smooth non-convex FCCO and TCCO problems have important applications in ML and AI, e.g., group distributionally robust optimization [4] and two-way partial AUC maximization for deep learning [44]. We defer discussions and formulations of these problems to Section 5. The difficulty for solving smooth FCCO lies at high costs of computing a stochastic gradient \(\nabla g_{i}(\mathbf{w})\nabla f_{i}(g_{i}(\mathbf{w}))\) for a randomly sampled \(i\) and the overall gradient \(\nabla F(\mathbf{w})\). To approximate the stochastic gradient, a variance-reduced estimator of \(g_{i}(\mathbf{w}_{t})\) denoted by \(u_{i,t}\) is usually maintained and updated for sampled data in the mini-batch \(i\in\mathcal{B}_{t}\). As a result, the stochastic gradient can be approximated by \(\nabla g_{i}(\mathbf{w}_{t};\xi_{t})\nabla f_{i}(u_{i,t})\), where \(\xi_{t}\sim\mathcal{D}_{i}\) is a random sample. The overall gradient can be estimated by averaging the stochastic gradient estimator over the mini-batch or using variance-reduction techniques. A key insight of the convergence analysis for smooth FCCO is to bound the following error using the \(L\)-smoothness of \(f_{i}\), which reduces to bounding the error of \(u_{i,t}\) for estimating \(g_{i}(\mathbf{w}_{t})\):

\[\|\nabla g_{i}(\mathbf{w}_{t};\xi_{t})\nabla f_{i}(u_{i,t})-\nabla g_{i}( \mathbf{w}_{t};\xi_{t})\nabla f_{i}(g_{i}(\mathbf{w}_{t}))\|^{2}\leq\|\nabla g _{i}(\mathbf{w}_{t};\xi_{t})\|^{2}L\|u_{i,t}-g_{i}(\mathbf{w}_{t})\|^{2}.\]

A central question to be addressed in this paper is _"Can these gradient estimators be used in stochastic optimization for solving non-smooth non-convex FCCO with provable convergence guarantee"?_ To address this question we focus our attention on a specific class of FCCO/TCCO called **non-smooth weakly-convex (NSWC) FCCO/TCCO**. This approach aligns with many established works on NSWC optimization [6, 7, 8, 9]. Nevertheless, NSWC FCCO/TCCO is more complex than a standard weakly-convex optimization problem because an unbiased stochastic subgradient is not readily accessible. In addition, the convergence measure in terms of the gradient norm of smooth non-convex objectives is not applicable to weakly convex optimization, which will complicate the analysis involving the biased stochastic gradient estimator \(\partial g_{i}(\mathbf{w}_{t};\xi_{t})\partial f_{i}(u_{i}^{t})\)1.

Footnote 1: We use \(\nabla\) to denote gradient of a differentiable function and \(\partial\) to denote a subgradient of a non-smooth function.

**Contributions.** A major contribution of this paper is to present _novel convergence analysis_ of single-loop stochastic algorithms for solving NSWC FCCO/TCCO problems, respectively. In particular,

* For non-smooth FCCO, we analyze the following single-loop updates: \[\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta\frac{1}{B}\sum\nolimits_{i\in\mathcal{B }_{t}}\partial g_{i}(\mathbf{w}_{t};\xi_{t})\partial f_{i}(u_{i,t}),\] (3) where \(\mathcal{B}_{t}\) is a random mini-batch of \(B\) items, and \(u_{i,t}\) is an appropriate variance-reduced estimator of \(g_{i}(\mathbf{w}_{t})\) that is updated only for \(i\in\mathcal{B}_{t}\) at the \(t\)-th iteration. To overcome the non-smoothness, we adopt the tool of Moreau envelop of the objective as in previous works [6, 7]. The key difference of our convergence analysis from previous ones for smooth FCCO is that we bound the inner product \(\langle\mathbb{E}_{i}\partial g_{i}(\mathbf{w})\partial f_{i}(u_{i,t}),\widehat {\mathbf{w}}_{t}-\mathbf{w}_{t}\rangle\), where \(\widehat{\mathbf{w}}_{t}\) is the solution of the proximal mapping of the objective at \(\mathbf{w}_{t}\). To this end, specific conditions of \(f_{i},g_{i}\) are imposed, i.e., \(f_{i}\) is weakly convex and non-decreasing and \(g_{i}(\mathbf{w})\) is weakly convex, under which we establish an iteration complexity of \(T=\mathcal{O}(\epsilon^{-6})\) for finding an \(\epsilon\)-stationary point of the Moreau envelope of \(F(\cdot)\).
* For non-smooth TCCO, we analyze the following single-loop updates: \[\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta\frac{1}{B_{1}}\sum\nolimits_{i\in\mathcal{ B}_{1}^{*}}\left[\frac{1}{B_{2}}\sum\nolimits_{j\in\mathcal{B}_{2}^{*}} \partial h_{i,j}(\mathbf{w}_{t};\xi_{t})\partial g_{i}(v_{i,j,t})\right]\partial f _{i}(u_{i,t}),\] (4)* We extend the above algorithms to solving (multi-instance) two-way partial AUC maximization for deep learning, and conduct extensive experiments to verify the effectiveness of the both algorithms.

## 2 Related work

**Smooth SCO.** There are many studies about two-level smooth SCO [27; 38; 10; 19; 3; 28] and multi-level smooth SCO [32; 32; 1; 39]. The complexities of finding an \(\epsilon\)-stationary point for two-level smooth SCO have been improved from \(O(\epsilon^{-5})\)[27] to \(O(\epsilon^{-3})\)[19], and that for multi-level smooth SCO have been improved from a level-dependent complexity of \(O(\epsilon^{-(7+K)/2})\)[32] to a level-independent complexity of \(O(\epsilon^{-3})\)[32], where \(K\) is the number of levels. The improvements mostly come from using advanced variance reduction techniques for estimating each level function or its Jacobian and for estimating the overall gradient. Two stochastic algorithms have been developed in [13] for CSO but suffer a limitation of requiring large batch sizes.

**Smooth FCCO.** FCCO was first introduced in [20] for optimizing average precision. Its algorithm and convergence analysis was improved in [26] and [15]. The former work [26] proposed an algorithm named SOX by using moving average (MA) to estimate the inner function values and the overall gradient. In the smooth non-convex setting, SOX is proved to achieve an iteration complexity of \(\mathcal{O}(\epsilon^{-4})\). The latter work [15] proposed a novel multi-block-single-probe variance reduced (MSVR) estimator for estimating the inner function values, which helps achieve a lower iteration complexity \(\mathcal{O}(\epsilon^{-3})\). Recently, [11] proposed an extrapolation based estimator for the inner function, which yields a method with a complexity that matches MSVR when \(n\leq\epsilon^{2/3}\). These techniques have been employed for optimizing various X-risks, including contrastive losses [36], ranking measures and listwise losses [21], and other objectives [26; 15]. However, all of these prior works assume the smoothness of \(f_{i}\) and \(g_{i}\). Hence, their analysis is not applicable to NSWC FCCO problems. Our novel analysis of a simple algorithm for NSWC FCCO problems yields an iteration complexity of \(O(\epsilon^{-6})\) for using the MSVR estimators of the inner functions. The comparison with [26; 15] is shown in Table 1.

**Non-smooth Weakly Convex Optimization.** Analysis of weakly convex optimization with unbiased stochastic subgradients was pioneered by [6; 7]. Optimization of compositional functions that are weakly convex have been tackled in earlier works [8; 9], where the inner function is deterministic or does not involve coupling between two random variables. A closely related work to our NSWC FCCO is weakly-convex concave minimax optimization [22]. Assuming \(f_{i}\) is convex, (1) can be written as: \(\min_{\mathbf{w}}\max_{\pi\in\mathbb{R}^{n}}\frac{1}{n}\sum_{i\in\mathcal{S}} \langle\pi_{i},g_{i}(\mathbf{w})\rangle-f_{i}^{*}(\pi_{i})\), where \(f_{i}^{*}(\cdot)\) is the convex conjugate of \(f_{i}\). It can be solved using existing methods [22; 31; 41; 43; 17] but with several limitations: (i) the algorithms in [22; 31; 41; 43] have a comparable complexity of \(O(1/\epsilon^{6})\) but have unnecessary double loops which require setting the number of iterations for the inner loop; (ii) the algorithm in [17] is single loop but has a worse complexity of \(\mathcal{O}(1/\epsilon^{8})\); (iii) these existing algorithms and analysis does not account for complexity of updating all coordinates of \(\pi\), which could be prohibitive in

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Objective & Smoothness & Weak Convexity & Monotonicity & Complexity \\ \hline SOX [26] & (1) & \(f_{i},g_{i}\) & none & none & \(\mathcal{O}(\epsilon^{-4})\) \\ MSVR [15] & (1) & \(f_{i},g_{i}\) & none & none & \(\mathcal{O}(\epsilon^{-3})\) \\ \hline SONX (Ours) & (1) & none & \(f_{i},g_{i}\) & \(f_{i}\uparrow\) & \(\mathcal{O}(\epsilon^{-6})\) \\ SONT (Ours) & (2) & none & \(f_{i},g_{i},h_{i,j}\) & \(f_{i}\uparrow,g_{i}\uparrow\) & \(\mathcal{O}(\epsilon^{-6})\) \\ SONT (Ours) & (2) & \(h_{i,j}\) & \(f_{i},g_{i}\) & \(f_{i}\uparrow,g_{i}\) & \(\mathcal{O}(\epsilon^{-6})\) \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{where \(\mathcal{B}_{t}^{1}\) and \(\mathcal{B}_{t}^{2}\) are random mini-batches of \(B_{1}\) and \(B_{2}\) items, respectively, and \(u_{i,t}\) is an appropriate variance-reduced estimator of \(\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(h_{ij}(\mathbf{w}_{t}))\) that is updated only for \(i\in\mathcal{B}_{t}^{1}\), and \(v_{i,j,t}\) is an appropriate variance-reduced estimator of \(h_{i,j}(\mathbf{w}_{t})\) that is updated only for \(i\in\mathcal{B}_{t}^{1}\), \(\mathcal{B}_{t}^{1},j\in\mathcal{B}_{t}^{2}\). To prove the convergence, we impose conditions of \(f_{i},g_{i},h_{i,j}\), i.e., \(f_{i}\) is weakly convex and non-decreasing and \(g_{i}(\cdot)\) is weakly convex and non-decreasing (or monotonic), \(h_{ij}\) is weakly convex (or smooth), and establish an iteration complexity of \(T=\mathcal{O}\left(\epsilon^{-6}\right)\) for finding an \(\epsilon\)-stationary point of the Moreau envelope of \(F(\cdot)\).

\end{table}
Table 1: Comparison with prior works for solving (1) and (2). In the monotonicity column, notation \(\uparrow\) means the given function is required to be non-decreasing. If not specified, the given function is only required to be monotone.

many applications; iv) these approaches are not applicable to NSWC FCCO/TCCO with weakly convex \(f_{i}\). In fact, the double loop algorithm has been leveraged and extended to solving the two-way partial AUC maximization problem, a special case of NSWC FCCO [44], by sampling and updating a batch of coordinates of \(\pi\) at each iteration. However, it is less practical thus not implemented and its analysis did not explicitly show the convergence rate dependency on \(n_{+},n_{-}\) and the block batch size.

A special case of NSWC SCO problem was considered in [46], which is given by

\[\min_{x\in\mathcal{X}}f(x,g(x)),\text{ with }f(x,u)=\mathbb{E}_{\zeta}[u+ \varkappa\max(0,g(x;\zeta)-u)],\quad g(x)=\mathbb{E}_{\xi}[g(x;\xi)].\]

They proposed two methods, SCS for smooth \(g(x)\) and SCS with SPIDER for non-smooth \(g(x)\). For both proposed methods, they proved a sample complexity of \(\mathcal{O}(1/\epsilon^{6})\) for achieving an \(\epsilon\)-stationary point of the objective's Moreau envelope 2. We would like to remark that the above problem with a non-smooth \(g(x)\) is a special case of NSWC FCCO with only a convex outer function, one block and no coupled structure. Nevertheless, their algorithm for non-smooth \(g(\cdot)\) suffers a limitation of requiring a large batch size in the order of \(O(1/\epsilon^{2})\) for achieving the same convergence.

Footnote 2: It is notable that we use a slightly different definition of \(\epsilon\)-stationary point with \(\|\nabla F_{\rho}(\mathbf{w})\|^{2}\leq\epsilon^{2}\).

Finally, we would like to mention that non-smooth convex or strongly convex SCO problems have been considered in [27; 42; 26], which, however, are out of scope of the present work.

## 3 Preliminaries

Let \(\|\cdot\|\) be the Euclidean norm of a vector and spectral norm of a matrix. We use \(\Pi_{C}[\cdot]\) to denote the Euclidean projection onto \(\{v\in\mathbb{R}^{m}:\|v\|\leq C\}\). For vectors, inequality notations including \(\leq,\geq,>,<\) are used to denote element-wise inequality. For an expectation function \(f(\cdot)=\mathbb{E}_{\xi}[f(\cdot;\xi)]\), let \(f(\cdot;\mathcal{B})=\frac{1}{|\mathcal{B}|}\sum_{\xi\in\mathcal{B}}f(\cdot;\xi)\) be its stochastic unbiased estimator evaluated on a sample batch \(\mathcal{B}\). A stochastic unbiased estimator is said to have bounded variance \(\sigma^{2}\) if \(\mathbb{E}_{\xi}[\|f(\cdot)-f(\cdot;\xi)\|^{2}]\leq\sigma^{2}\). The Jacobian matrix of function \(f:\mathbb{R}^{m_{1}}\to\mathbb{R}^{m_{2}}\) is in dimension \(\mathbb{R}^{m_{1}\times m_{2}}\). We recall the definition of general subgradient and subdifferential following [6; 24].

**Definition 3.1** (subgradient and subdifferential).: Consider a function \(f:\mathbb{R}^{n}\to\mathbb{R}\cup\{\infty\}\) and a point with \(f(x)\) finite. A vector \(v\in\mathbb{R}^{n}\) is a general subgradient of \(f\) at \(x\), if

\[f(y)\geq f(x)+\langle v,y-x\rangle+o(\|y-x\|),\quad\text{as }y\to x.\]

The subdifferential \(\partial f(x)\) is the set of subgradients of \(f\) at point \(x\).

For simplicity, we abuse the notation and also use \(\partial f(x)\) to denote one subgradient from the corresponding subgradient set when no confusion could be caused. We use \(\partial f(x;\mathcal{B})\) to represent a stochastic unbiased estimator of the subgradient \(\partial f(x)\) that is evaluated on a sample batch \(\mathcal{B}\). A function is called \(C^{1}\)-smooth if it is continuously differentiable. A function \(f:\mathbb{R}^{n}\to\mathbb{R}\cup\{\infty\}\) and a point with respect to each element of the input. Note that if a Lipschitz continuous function \(f:O\to\mathbb{R}^{m_{2}}\) is assumed to be non-increasing (resp. non-decreasing), where the domain \(O\subset\mathbb{R}^{m_{1}}\) is open, then all subgradients of \(f\) are element-wise non-positive (resp. non-negative). We refer the details to Appendix D.1.

A function \(f\) is _\(C\)-Lipschitz continuous_ if \(\|f(x)-f(y)\|\leq C\|x-y\|\). A differentiable function \(f\) is _\(L\)-smooth_ if \(\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|\). A function \(f:\mathbb{R}^{d}\to\mathbb{R}\cup\{\infty\}\) is _\(\rho\)-weakly-convex_ if the function \(f(\cdot)+\frac{\rho}{2}\|\cdot\|^{2}\) is convex. A vector-valued function \(f:\mathbb{R}^{d}\to\{\mathbb{R}\cup\{\infty\}\}^{m}\) is called \(\rho\)-weakly-convex if it is \(\rho\)-weakly-convex for each output. It is difficult sometimes impossible to find an \(\epsilon\)-stationary point of a non-smooth weakly-convex function \(F\), i.e., \(\text{dist}(0,\partial F(\mathbf{w}))\leq\epsilon\). For example, an \(\epsilon\)-stationary point of function \(f(x)=|x|\) does not exist for \(0\leq\epsilon<1\) unless it is the optimal solution. To tackle this issue, [6] proposed to use the stationarity of the problem's Moreau envelope as the convergence metric, which has become a standard metric for solving weakly-convex problems [7; 22; 31; 41; 43; 17]. Given a weakly-convex function \(\varphi:\mathbb{R}^{m}\to\mathbb{R}\), its Moreau envelope and proximal map with \(\lambda>0\) are constructed as

\[\varphi_{\lambda}(x):=\min_{y}\{\varphi(y)+\frac{1}{2\lambda}\|y-x\|^{2}\}, \quad\text{prox}_{\lambda\varphi}(x):=\operatorname*{arg\,min}_{y}\{\varphi(y) +\frac{1}{2\lambda}\|y-x\|^{2}\}.\]

The Moreau envelope is an implicit smoothing of the original problem. Thus it attains a continuous differentiation. As a formal statement, the following lemma follows from standard results [6; 18].

**Lemma 3.2**.: _Given a \(\rho\)-weakly-convex function \(\varphi\) and \(\lambda<\rho^{-1}\), the envelope \(\varphi_{\lambda}\) is \(C^{1}\)-smooth with gradient given by \(\nabla\varphi_{\lambda}(x)=\lambda^{-1}(x-\text{prox}_{\lambda\varphi}(x))\)._```
1:Initialization: \(\mathbf{w}_{0}\), \(\{u_{i,0}:i\in\mathcal{S}\}\).
2:for\(t=0,\ldots,T-1\)do
3: Draw sample batches \(\mathcal{B}_{1}^{t}\sim\mathcal{S}\), and \(\mathcal{B}_{2,i}^{t}\sim\mathcal{D}_{i}\) for each \(i\in\mathcal{B}_{1}^{t}\).
4:\(u_{i,t+1}=\begin{cases}(1-\tau)u_{i,t}+\tau g_{i}(\mathbf{w}_{t};\mathcal{B}_{2,i}^{t})+\gamma(g_{i}(\mathbf{w}_{t};\mathcal{B}_{2,i}^{t})-g_{i}(\mathbf{w}_{t -1};\mathcal{B}_{2,i}^{t})),\quad i\in\mathcal{B}_{1}^{t}\\ u_{i,t},\quad i\not\in\mathcal{B}_{1}^{t}\end{cases}\)
5: Compute \(G_{t}=\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{1}^{t}}\partial g_{i}(\mathbf{w}_{t };\mathcal{B}_{2,i}^{t})\partial f_{i}(u_{i,t})\)
6: Update \(\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta G_{t}\)
7:endfor ```

**Algorithm 1** Stochastic Optimization algorithm for Non-smooth FCCO (SONX)

Moreover, for any point \(x\in\mathbb{R}^{m}\), the proximal point \(\hat{x}:=\text{prox}_{\lambda\varphi}(x)\) satisfies [6]

\[\|\hat{x}-x\|=\lambda\|\nabla\varphi_{\lambda}(x)\|,\quad\varphi(\hat{x})\leq \varphi(x),\quad\text{dist}(0,\partial\varphi(\hat{x}))\leq\|\nabla\varphi_{ \lambda}(x)\|.\]

Thus if \(\|\nabla\varphi_{\lambda}(x)\|\leq\epsilon\), we can say \(x\) is close to a point \(\hat{x}\) that is \(\epsilon\)-stationary, which is called nearly \(\epsilon\)-stationary solution of \(\varphi(x)\).

## 4 Algorithms and Convergence

### Non-Smooth Weakly-Convex FCCO

In this section, we assume the following conditions hold for the FCCO problem (1).

**Assumption 4.1**.: For all \(i\in\mathcal{S}\), we assume that

* \(f_{i}\) is \(\rho_{f}\)-weakly-convex, \(C_{f}\)-Lipschitz continuous and non-decreasing;
* \(g_{i}(\cdot)\) is \(\rho_{g}\)-weakly-convex and \(g_{i}(\cdot;\xi)\) is \(C_{g}\)-Lipschitz continuous;
* Stochastic gradient estimators \(g_{i}(\mathbf{w};\xi)\) and \(\partial g_{i}(\mathbf{w};\xi)\) have bounded variance \(\sigma^{2}\).

**Proposition 4.2**.: _Under Assumption 4.1, \(F(\mathbf{w})\) in (1) is \(\rho_{F}\) weakly convex with \(\rho_{F}=\sqrt{d_{1}}\rho_{g}C_{f}+\rho_{f}C_{g}^{2}\)._

One challenge in solving FCCO is the lack of access to unbiased estimation of the subgradients \(\frac{1}{n}\sum_{i\in\mathcal{S}}\partial g_{i}(\mathbf{w})\partial f_{i}(g_ {i}(\mathbf{w}))\) due to the expectation form of \(g_{i}(\mathbf{w})\) inside a non-linear function \(f_{i}\). A common solution in existing works for solving smooth FCCO is to maintain function value estimators \(\{u_{i}:i\in\mathcal{S}\}\) for \(\{g_{i}(\mathbf{w}):i\in\mathcal{S}\}\), and approximate the true gradient by a stochastic version \(\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{1}}\partial g_{i}(\mathbf{w};\mathcal{B }_{2})\partial f_{i}(u_{i})\)[26; 15], where \(\mathcal{B}_{1}\), \(\mathcal{B}_{2}\) are sampled mini-batches. Simply using a mini-batch estimator of \(g_{i}\) inside \(f_{i}\) does not ensure convergence if mini-batch size is small.

Inspired by existing algorithms of smooth FCCO, a simple method for solving non-smooth FCCO is presented in Algorithm 1 referred to as SONX. A key step is the step 4, which uses the multi-block-single-probe variance reduced (MSVR) estimator proposed in [15] to update \(\{u_{i}:i\in\mathcal{S}\}\) in a block-wise manner. It is an advanced variance reduced update strategy for multi-block variable inspired by STORM [5]. In the update of MSVR estimator, for each sampled \(i\in\mathcal{B}_{1}^{t}\), \(u_{i,t}\) is updated following a STORM-like rule with a specialized parameter \(\gamma=\frac{n-B_{1}}{B_{1}(1-\tau)}+(1-\tau)\) for the error correction term. For the unsampled \(i\not\in\mathcal{B}_{1}^{t}\), no update for \(u_{i,t}\) is needed. When \(\gamma=0\), the estimator becomes the moving average estimator analyzed in [26] for smooth FCCO, which is also analyzed in the Appendix. With the function values of \(\{g_{i}(\mathbf{w}_{t}):i\in\mathcal{S}\}\) well-estimated, the gradient can be approximated by \(G_{t}\) in step 5. Next, we directly update \(\mathbf{w}_{t}\) by subgradient descent using the stochastic gradient estimator \(G_{t}\). Note that unlike existing works on smooth FCCO that often maintain a moving average estimator [26] or a STORM estimator [15] for the overall gradient to attain better rates, this is not possible in the non-smooth case as those variance reduction techniques for the overall gradient critically rely on the Lipschitz continuity of \(\nabla F\), i.e., the smoothness of \(F\).

### Non-Smooth Weakly-Convex TCCO

In this section, we consider non-smooth TCCO problem and aim to extend Algorithm 1 to solve it. First of all, for convergence analysis and to ensure the weak convexity of \(F(\mathbf{w})\) in (2), we make the following assumptions.

**Assumption 4.3**.: For all \((i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\), we assume that ```
1: Initialization: \(\mathbf{w}_{0}\), \(\{u_{i,0}:i\in\mathcal{S}_{1}\}\), \(v_{i,j,0}=h_{i,j}(\mathbf{w}_{0};\mathcal{B}^{0}_{3,i,j})\) for all \((i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\).
2:for\(t=0,\dots,T-1\)do
3: Sample batches \(\mathcal{B}^{t}_{1}\subset\mathcal{S}_{1}\), \(\mathcal{B}^{t}_{2}\subset\mathcal{S}_{2}\), and \(\mathcal{B}^{t}_{3,i,j}\subset\mathcal{D}_{i,j}\) for \(i\in\mathcal{B}^{t}_{1}\) and \(j\in\mathcal{B}^{t}_{2}\).
4:\(v_{i,j,t+1}=\begin{cases}\Pi_{\tilde{C}_{h}}[(1-\tau_{1})v_{i,j,t}+\tau_{1}h_{ i,j}(\mathbf{w}_{t};\mathcal{B}^{t}_{3,i,j})+\gamma_{1}(h_{i,j}(\mathbf{w}_{t}; \mathcal{B}^{t}_{3,i,j})-h_{i,j}(\mathbf{w}_{t-1};\mathcal{B}^{t}_{3,i,j}))],\\ v_{i,j,t},\quad(i,j)\not\in\mathcal{B}^{t}_{1}\times\mathcal{B}^{t}_{2}\end{cases} \quad(i,j)\in\mathcal{B}^{t}_{1}\times\mathcal{B}^{t}_{2}\]
5:\(u_{i,t+1}=\begin{cases}(1-\tau_{2})u_{i,t}+\frac{1}{B_{2}}\sum_{j\in\mathcal{B} ^{t}_{2}}[\tau_{2}g_{i}(v_{i,j,t})+\gamma_{2}(g_{i}(v_{i,j,t})-g_{i}(v_{i,j,t-1 })],\quad i\in\mathcal{B}^{t}_{1}\\ u_{i,t},\quad i\not\in\mathcal{B}^{t}_{1}\end{cases}\)
6:\(G_{t}=\frac{1}{B_{1}}\sum_{i\in\mathcal{B}^{t}_{1}}\left[\left(\frac{1}{B_{2}} \sum_{i\in\mathcal{B}^{t}_{2}}\nabla h_{i,j}(\mathbf{w}_{t};\mathcal{B}^{t}_{3,i,j})\partial g_{i}(v_{i,j,t})\right)\partial f_{i}(u_{i,t})\right]\)
7: Update \(\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta G_{t}\)
8:endfor ```

**Algorithm 2** Stochastic Optimization algorithm for Non-smooth TCCO (SONT)

```
1: Initialization: \(\mathbf{w}_{0}\), \(\{u_{i,0}:i\in\mathcal{S}_{1}\}\), \(v_{i,j,0}=h_{i,j}(\mathbf{w}_{0};\mathcal{B}^{0}_{3,i,j})\) for all \((i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\).
2:for\(t=0,\dots,T-1\)do
3: Sample batches \(\mathcal{B}^{t}_{1}\subset\mathcal{S}_{1}\), \(\mathcal{B}^{t}_{2}\subset\mathcal{S}_{2}\), and \(\mathcal{B}^{t}_{3,i,j}\subset\mathcal{D}_{i,j}\) for \(i\in\mathcal{B}^{t}_{1}\) and \(j\in\mathcal{B}^{t}_{2}\).
4:\(v_{i,j,t+1}=\begin{cases}\Pi_{\tilde{C}_{h}}[(1-\tau_{1})v_{i,j,t}+\tau_{1}h_ {i,j}(\mathbf{w}_{t};\mathcal{B}^{t}_{3,i,j})+\gamma_{1}(h_{i,j}(\mathbf{w}_{ t};\mathcal{B}^{t}_{3,i,j})-h_{i,j}(\mathbf{w}_{t-1};\mathcal{B}^{t}_{3,i,j}))],\\ v_{i,j,t},\quad(i,j)\not\in\mathcal{B}^{t}_{1}\times\mathcal{B}^{t}_{2}\end{cases}\)
5:\(u_{i,t+1}=\begin{cases}(1-\tau_{2})u_{i,t}+\frac{1}{B_{2}}\sum_{j\in\mathcal{B }^{t}_{2}}[\tau_{2}g_{i}(v_{i,j,t})+\gamma_{2}(g_{i}(v_{i,j,t})-g_{i}(v_{i,j,t- 1})],\quad i\in\mathcal{B}^{t}_{1}\\ u_{i,t},\quad i\not\in\mathcal{B}^{t}_{1}\end{cases}\)
6:\(G_{t}=\frac{1}{B_{1}}\sum_{i\in\mathcal{B}^{t}_{1}}\left[\left(\frac{1}{B_{2}} \sum_{i\in\mathcal{B}^{t}_{2}}\nabla h_{i,j}(\mathbf{w}_{t};\mathcal{B}^{t}_{3,i,j})\partial g_{i}(v_{i,j,t})\right)\partial f_{i}(u_{i,t})\right]\)
7: Update \(\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta G_{t}\)
8:endfor ```

**Algorithm 3** Stochastic Optimization algorithm for Non-smooth TCCO (SONT)

The weak convexity of \(F(\mathbf{w})\) in (2) is guaranteed by the following Proposition.

**Proposition 4.4**.: _Under Assumption 4.3, \(F(\mathbf{w})\) in (2) is \(\rho_{F}\)-weakly-convex with \(\rho_{F}=\sqrt{d_{1}}(\sqrt{d_{2}}L_{h}C_{g}+\rho_{g}C_{h}^{2})C_{f}+\rho_{f}C_ {g}^{2}C_{h}^{2}\)._

We extend SONX to Algorithm 2 for (2), which is referred to as SONT. For dealing with the extra layer of compositional problem, we maintain another multi-block variable to track the extra layer of function value estimation. To understand this, we first write down the true subgradient:

\[\partial F(\mathbf{w})=\frac{1}{n_{1}}\sum\nolimits_{i\in\mathcal{S}_{1}}\left[ \left(\frac{1}{n_{2}}\sum\nolimits_{j\in\mathcal{S}_{2}}\nabla h_{i,j}( \mathbf{w})\partial g_{i}(h_{i,j}(\mathbf{w}))\right)\partial f_{i}\left( \frac{1}{n_{2}}\sum\nolimits_{j\in\mathcal{S}_{2}}g_{i}(h_{i,j}(\mathbf{w})) \right)\right].\]

To approximate this subgradient, we need the estimations of \(\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(h_{i,j}(\mathbf{w}))\) and \(h_{i,j}(\mathbf{w})\), which can be tracked by using MSVR estimators denoted by \(\{u_{i,t}:i\in\mathcal{S}_{1}\}\) and \(\{v_{i,j,t}:(i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\}\), respectively. As a result, a stochastic estimation of \(\partial F(\mathbf{w}_{t})\) is computed in step 6 of Algorithm 2, and the model parameter is updated similarly as before.

### Convergence Analysis

In this section, we present the proof sketch of the convergence guarantee for Algorithm 1. The analysis for Algorithm 2 follows in a similar manner. The detailed proofs can be found in Appendix A (please refer to the supplement). Before starting the proof, we define a constant \(M^{2}\geq C_{f}^{2}C_{g}^{2}\) so that under Assumption 4.1 we have \(\mathbb{E}_{t}[\|G_{t}\|^{2}]\leq M^{2}\). Then we start by giving the error bound of the MSVR estimator in Algorithm 1. The following norm bound of the estimation error follows from the squared-norm error bound in Lemma 1 from [15], whose proof is given in Appendix D.3.

**Lemma 4.5**.: _Consider the update for \(\{u_{i,t}:i\in\mathcal{S}\}\) in Algorithm 1. Assume \(g_{i}\) is \(C_{g}\)-Lipshitz for all \(i\in\mathcal{S}\). With \(\gamma=\frac{n-B_{1}}{B_{1}(1-\tau)}+(1-\tau)\), \(\tau\leq\frac{1}{2}\), we have_

\[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t+1}-g_{i}(\mathbf{ w}_{t+1})\|\bigg{]}\leq(1-\frac{B_{1}\tau}{2n})^{t+1}\frac{1}{n}\sum_{i\in \mathcal{S}}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|+\frac{2\tau^{1/2}\sigma}{B_{2} ^{1/2}}+\frac{4nC_{g}M\eta}{B_{1}\tau^{1/2}}.\]

For simplicity, denote by \(\hat{\mathbf{w}}_{t}:=\text{prox}_{F/\bar{\rho}}(\mathbf{w}_{t})\). Then using the definition of Moreau envelope and the update rule of \(\mathbf{w}_{t}\), we can obtain a bound for the change in the Moreau envelope,

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]\leq F_{1/\bar{\rho}}(\mathbf{ w}_{t})+\bar{\rho}\eta(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},\mathbb{E}_{t}[G_{t}])+\frac{ \eta^{2}\bar{\rho}M^{2}}{2}.\] (5)

where \(\mathbb{E}_{t}[G_{t}]=\frac{1}{n}\sum_{i\in\mathcal{S}_{1}}\partial g_{i}( \mathbf{w}_{t})\partial f_{i}(u_{i,t})\) is the subgradient approximation based on the MSVR estimator \(u_{i,t}\) of the inner function value. This is a standard result in weakly-convex optimization To bound the inner product \(\langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},\mathbb{E}_{t}[G_{t}]\rangle\) on the right-hand-side of (5), we apply the assumptions that \(f_{i}\) is weakly-convex, Lipschitz continuous and non-decreasing, and \(g_{i}\) is weakly-convex. Its upper bound is given as follows.

\[(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})^{\top}\mathbb{E}_{t}[G_{t}]\leq F(\hat{ \mathbf{w}}_{t})-F(\mathbf{w}_{t})+\frac{1}{n}\sum_{i\in\mathcal{S}}[f_{i}(g_{ i}(\mathbf{w}_{t}))-f(u_{i,t})-\partial f(u_{i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{ i,t})\] \[\qquad\qquad\qquad\qquad+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t} \|^{2}+(\frac{\rho_{g}C_{f}}{2}+\rho_{f}C_{g}^{2})\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\|^{2}].\] (6)

Due to the \(\rho_{f}\)-weak convexity of \(F(\mathbf{w})\), we have \((\bar{\rho}-\rho_{F})\)-strong convexity of \(\mathbf{w}\mapsto F(\mathbf{w})+\frac{\bar{\rho}}{2}\|\mathbf{w}_{t}-\mathbf{ w}\|^{2}\). Then it follows \(F(\hat{\mathbf{w}}_{t})-F(\mathbf{w}_{t})\leq(\frac{\rho_{F}}{2}-\bar{\rho})\| \mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}\). Combining this with inequalities (5), (6), and setting \(\bar{\rho}\) sufficiently large we have

\[\begin{split}&\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})] \leq F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{2}}{2}+\frac {\bar{\rho}\eta}{n}\sum\nolimits_{i\in\mathcal{S}}[-\frac{\bar{\rho}}{2}\| \mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}\\ &\quad+f_{i}(g_{i}(\mathbf{w}_{t}))-f(u_{i,t})-\partial f_{i}(u_{ i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{i,t})+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{ i,t}\|^{2}].\end{split}\] (7)

Recall Lemma 3.2, we have \(\|\mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}=\frac{1}{\bar{\rho}^{2}}\|\nabla F _{1/\bar{\rho}}(\mathbf{w}_{t})\|^{2}\). Moreover, the last three terms on the R.H.S of inequality (7) can be bounded using the Lipschitz continuity of \(f_{i}\) and the error bound given in Lemma 4.5. Then we can conclude the complexity of SONX with the following theorem.

**Theorem 4.6**.: _Under Assumption 4.1 with \(\gamma=\frac{n-B_{1}}{B_{1}(1-\tau)}+(1-\tau)\), \(\tau=\mathcal{O}(B_{2}\epsilon^{4})\leq\frac{1}{2}\), \(\eta=\mathcal{O}(\frac{B_{1}B_{2}^{1/2}\epsilon^{4}}{n})\), and \(\bar{\rho}=\rho_{F}+\rho_{g}C_{f}+2\rho_{f}C_{g}^{2}\), Algorithm 1 converges to an \(\epsilon\)-stationary point of the Moreau envelope \(F_{1/\bar{\rho}}\) in \(T=\mathcal{O}(\frac{n}{B_{1}B_{2}^{1/2}}\epsilon^{-6})\) iterations._

**Remark.** Similar to the complexity for smooth FCCO problems [26, 15], Theorem 4.6 guarantees that SONX for NSWC FCCO has a parallel speed-up in terms of the batch size \(B_{1}\) and linear dependency on \(n\). The dependency of the complexity on the batch size \(B_{2}\) is due to the use of MSVR estimator, which matches the results in [15]. If the MSVR estimator in SONX is replaced by moving average estimator, the complexity becomes \(\mathcal{O}(\frac{n}{B_{1}B_{2}}\epsilon^{-8})\) (cf. Appendix B).

Following a similar proof strategy, the convergence guarantee of Algorithm 2 is given below.

**Theorem 4.7**.: _(Informal) Under Assumption 4.3, with appropriate values of \(\gamma_{1},\gamma_{2},\tau_{1},\tau_{2},\eta\) and a proper constant \(\bar{\rho}\), Algorithm 2 converges to an \(\epsilon\)-stationary point of the Moreau envelope \(F_{1/\bar{\rho}}\) in \(T=\mathcal{O}\left(\max\left\{\frac{1}{B_{3}^{1/2}},\frac{n_{1}^{1/4}}{B_{1}^ {1/4}n_{2}^{1/4}},\frac{n_{1}^{1/2}}{B_{1}^{1/2}n_{2}^{1/2}}\right\}\frac{n_{ 1}n_{2}}{B_{1}B_{2}}\epsilon^{-6}\right)\) iterations._

**Remark.** In the worst case, the complexity has a worse dependency on \(n_{1}/B_{1}\), i.e., \(\mathcal{O}(n_{1}^{3/2}/B_{1}^{3/2})\). This is caused by the two layers of block-sampling update for \(\{u_{i,t},i\in\mathcal{S}_{1}\}\) and \(\{v_{i,j,t}:(i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\}\). When \(n_{1}=B_{1}=1\) and \(B_{3}\leq\sqrt{n_{2}}\), the complexity of SONT becomes similar as SONX, which is understandable as the inner two levels in TCCO is the same as FCCO.

## 5 Applications

NSWC FCCO finds important applications in group distributionally robust optimization (group DRO) and two-way partial AUC (TPAUC) maximization.

Consider \(N\) groups with different distributions. Each group \(k\) has an averaged loss \(L_{k}(w)=\frac{1}{n_{k}}\sum_{i=1}^{n_{k}}\ell(f_{w}(x_{i}^{k}),y_{i}^{k})\), where \(w\) is the the model parameter and \((x_{i}^{k},y_{i}^{k})\) is a data point. It has been shown in previous study [23] that the group DRO problem can be formulated into

\[\min_{w}\min_{s}F(w,s)=\frac{1}{K}\sum_{k=1}^{N}[L_{k}(w)-s]_{+}+s.\]

This formulation can be mapped into non-smooth weakly-convex FCCO under certain assumptions. Due to space limitation, we defer the comprehensive discussion of group DRO to Appendix E. The rest of this section focuses on TPAUC maximization.

Let \(X\) denote an input example and \(h_{\mathbf{w}}(X)\) denote a prediction of a parameterized deep net on data \(X\). Denote by \(\mathcal{S}_{+}\) the set of \(n_{+}\) positive examples and by \(\mathcal{S}_{-}\) the set of \(n_{-}\) negative examples. TPAUC measures the area under ROC curve where the true positive rate (TPR) is higher than \(\alpha\) and the false positive rate (FPR) is lower than an upper bound \(\beta\). A surrogate loss for optimizing TPAUCwith FPR\(\leq\beta\), TPR\(\geq\alpha\) is given by [34]:

\[\min_{\mathbf{w}}\frac{1}{n_{+}}\frac{1}{n_{-}}\sum\nolimits_{X_{i}\in\mathcal{S }_{+}^{\downarrow}[1,k_{1}]}\sum\nolimits_{X_{j}\in\mathcal{S}_{-}^{\downarrow }[1,k_{2}]}\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i})),\] (8)

where \(\ell(\cdot)\) is a convex, monotonically non-decreasing surrogate loss of the indicator function \(\mathbb{I}(h_{\mathbf{w}}(X_{j})\geq h_{\mathbf{w}}(X_{i}))\), \(\mathcal{S}_{+}^{\downarrow}[1,k_{1}]\) is the set of positive examples with \(k_{1}=\lfloor n_{+}\alpha\rfloor\) smallest scores, and \(\mathcal{S}_{-}^{\downarrow}[1,k_{2}]\) is the set of negative examples with \(k_{2}=\lfloor n_{-}\beta\rfloor\) largest scores. To tackle the challenge of selecting examples from \(\mathcal{S}_{+}^{\uparrow}[1,k_{1}]\) and \(\mathcal{S}_{-}^{\downarrow}[1,k_{2}]\), the above problem is cast into the following [44]:

\[\min_{\mathbf{w},\mathbf{s}^{\prime},\mathbf{s}}\frac{1}{n_{+}}\sum\nolimits_ {X_{i}\in\mathcal{S}_{+}}f_{i}(\psi_{i}(\mathbf{w},s_{i}),s^{\prime}),\] (9)

where \(f_{i}(g,s^{\prime})=s^{\prime}+\frac{(g-s^{\prime})_{+}}{\alpha}\), \(\psi_{i}(\mathbf{w},s_{i})=\frac{1}{n_{-}}\sum\nolimits_{X_{j}\in\mathcal{S} _{-}}s_{i}+\frac{(\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))-s_{i})_{+} }{\beta}\),

where \(\mathbf{s}=(s_{1},\ldots,s_{n_{+}})\). We will consider two scenarios, namely regular learning scenario where \(X_{i}\in\mathbb{R}^{d_{0}}\) is an instance, and multi-instance learning (MIL) scenario where \(X_{i}=\{\mathbf{x}_{1}^{1},\ldots,\mathbf{x}_{n}^{m_{i}}\in\mathbb{R}^{d_{0}}\}\) contains multiple instances (e.g., one patient has hundreds of high-resolution CT images). A challenge in MIL is that the number of instances \(m_{i}\) for each data might be large such that it is difficult to load all instances into the memory for mini-batch training. It becomes more nuanced especially because MIL involves a pooling operation that aggregates the predicted information of individual instances into a single prediction, which can be usually written as a compositional function with the inner function being an average over instances from \(X\). For simplicity of exposition, below we consider the mean pooling \(h_{\mathbf{w}}(X)=\frac{1}{|X|}\sum\nolimits_{\mathbf{x}\in X}e(\mathbf{w}_{e };\mathbf{x})^{\top}\mathbf{w}_{c}\), where \(e(\mathbf{w}_{e},\mathbf{x})\) is the encoded feature representation of instance \(\mathbf{x}\) with a parameter \(\mathbf{w}_{c}\), and \(\mathbf{w}_{c}\) is the parameter of the classifier. We will map the regular learning problem as NSWC FCCO and the MIL problem as NSWC TCCO.

The problem (9) is slightly more complicated than (1) or (2) due to the presence of \(s^{\prime},\mathbf{s}\). In order to understand the applicability of our analysis and results to (9), we ignore \(s^{\prime},\mathbf{s}\) for a moment. In the regular learning setting when \(h_{\mathbf{w}}(X)=e(\mathbf{w}_{e},X)^{\top}\mathbf{w}_{c}\) can be directly computed, we can map the problem into NSWC FCCO, where \(f_{i}(g,s^{\prime})\) is non-smooth, convex, and non-decreasing in terms of \(g\), and \(g_{i}(\mathbf{w},s_{i})=\psi_{i}(\mathbf{w},s_{i})\) is non-smooth, and is proved to be weakly when \(\ell(\cdot)\) is convex and \(h_{\mathbf{w}}(X)\) is smooth in terms of \(\mathbf{w}\). In the MIL setting with mean pooling, we can map the problem into NSWC TCCO by defining \(h_{i}(\mathbf{w})=\frac{1}{|X_{i}|}\sum\nolimits_{\mathbf{x}\in X_{i}}e( \mathbf{w}_{e};\mathbf{x})^{\top}\mathbf{w}_{c}\), \(h_{ij}(\mathbf{w})=h_{j}(\mathbf{w})-h_{i}(\mathbf{w})\) and \(g_{i}(h_{i,j}(\mathbf{w}),s_{i})=s_{i}+\frac{(\ell(h_{i,j}(\mathbf{w}))-s_{i})_ {+}}{\beta}\), and \(f_{i}(g_{i},s^{\prime})=s^{\prime}+\frac{(g_{i}-s^{\prime})_{+}}{\alpha}\), where \(f_{i}\) is non-smooth, convex, and non-decreasing in terms of \(g_{i}\), and \(g_{i}(h_{ij}(\mathbf{w}),s_{i})\) is non-smooth, convex, monotonic in terms of \(h_{ij}(\mathbf{w})\) when \(\ell(\cdot)\) is convex and monotonically non-decreasing, and \(g_{i}(h_{ij}(\mathbf{w}),s_{i})\) is weakly convex in terms of \(\mathbf{w}\) when \(h_{ij}(\mathbf{w})\) is smooth and Lipchitz continuous in terms of \(\mathbf{w}\). Hence, the problem (9) satisfies the conditions in Assumption 4.1 for the regular learning setting and that in Assumption 4.3 for the MIL with mean pooling under mild regularity conditions of the neural network. We present full details in Appendix C.1 for interested readers.

To compute the gradient estimator w.r.t \(\mathbf{w}\), \(u_{i,t}\) will be maintained for tracking \(g_{i}(\mathbf{w},s_{i})\) in the regular setting or \(\frac{1}{n_{-}}\sum\nolimits_{X_{j}\in\mathcal{S}_{-}}g_{i}(h_{i,j}(\mathbf{w}),s_{i})\) in the MIL setting, \(v_{i,t}\) will be maintained for tracking \(h_{i}(\mathbf{w})\) in the MIL setting, which are updated similar to that in SONX and SONT. One difference from SONT is that \(v_{i,j,t}\) is decoupled into \(v_{i,t}\) and \(v_{j,t}\) due to that \(h_{i,j}\) can be decoupled. In terms of the extra variable \(s^{\prime},\mathbf{s}\), the objective function is convex w.r.t both \(s^{\prime}\) and \(\mathbf{s}\), which allows us to simply update \(s^{\prime}\) by SGD using the stochastic gradient estimator \(\frac{1}{B_{1}}\sum\nolimits_{i\in\mathcal{B}_{1}^{d}}\partial_{s^{\prime}}f_{i} (u_{i,t},s^{\prime}_{t})\) and we update \(s_{i}\) by SGD using the stochastic gradient estimator \(\left[\frac{1}{B_{2}}\sum\nolimits_{j\in\mathcal{B}_{1}^{d}}\partial_{s_{i}}g_{i }(v_{j,t}-v_{i,t},s_{i,t})\right]\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\). Detailed updates are presented in Algorithm 5 and Algorithm 6 in Appendix C.2. We can extend the convergence analysis of SONX and SONT to the two learning settings of TPAUC maximization, which is included in Appendix C.4. Finally, it is worth mentioning that we can also extend the results to other pooling operations, including smoothed max pooling and attention-based pooling [45]. Due to limit of space, we include discussions in Appendix C.3 as well.

## 6 Experimental Results

We justify the effectiveness of the proposed SONX and SONT algorithms for TPAUC Maximization in the regular learning setting and MIL setting [14, 45].

**Baselines.** For _regular TPAUC maximization_, we compare SONN with the following competitive methods: 1) Cross Entropy (CE) loss minimization; 2) AUC maximization with squared hinge loss (AUC-SH); 3) AUC maximization with min-max margin loss (AUC-M) [37]; 4) Mini-Batch based heuristic loss (MB) [16]; 5) Adhoc-Weighting based method with polynomial function (AW-poly) [35]; 5) a single-loop algorithm (SOTAs) for optimizing a smooth surrogate for TPAUC [44]. For _MIL TPAUC maximization_, we consider the following baselines: 1) AUC-M with attention-based pooling (AUC-M [att]); 2) SOTAs with attention-based pooling, which is a natural combination between advanced TPAUC optimization and MIL pooling technique; 3) the recently proposed provable multi-instance deep AUC maximization methods with stochastic smoothed-max pooling and attention-based pooling (MIDAM [smx] and MIDAM [att]) [45]. The first two baselines use naive mini-batch pooling for computing the loss function in AUC-M and SOTAs. We implement SONT for MIL TPAUC maximization with attention-based pooling, which is referred to as SONT (att).

**Datasets.** For regular TPAUC maximization, we use three molecule datasets as in [44], namely moltox21 (the No.0 target), molmuv (the No.1 target) and molpcba (the No.0 target) [29]. For MIL TPAUC maximization, we use four MIL datasets, including two tabular datasets MUSK2 and Fox, and two medical image datasets Colon and Lung. MUSK2 and Fox are two tabular datasets that have been widely adopted for MIL benchmark study [14]. Colon and Lung are two histopathology (medical image) datasets that have large image size (512\(\times\)512) but local interests for classification [2]. For Colon dataset, the adenocarcinoma is regarded as positive label and benign is negative; for Lung dataset, we treat adenocarcinoma as positive and squamous cell carcinoma as negative 3. For both of the histopathology datasets, we uniformly randomly sample 100 positive and 1000 negative data for experiments. For all MIL datasets, we uniformly randomly split 10% as the testing and the remaining as the training and validation. The statistics for all used datasets are summarized in Table 3and Table 4 in Appendix F.

Footnote 3: Data available: https://www.kaggle.com/datasets/biplobdey/lung-and-colon-cancer

**Experiment Settings.** For regular TPAUC maximization, we use the same setting as in [44]. The adopted backbone Graph Nueral Network (GNN) model is Graph Isomorphism Network (GIN), which has 5 mean-pooling layers with 64 number of hidden units and dropout rate 0.5 [30]. We utilize the sigmoid function for the final output layer to generate the prediction score, and set the surrogate loss \(\ell(\cdot)\) as squared hinge loss with a margin parameter. We follow the setups for model training and tuning exactly the same as the prior work [44]. Essentially, the model is trained by 60 epochs and the learning rate is decreased by 10-fold after every 20 epochs. The model is initialized as a pretrained model from CE loss on the training datasets. We fix the learning rate of SONX as 1e-2 and moving average parameter \(\tau\) as 0.9; tune the parameter \(\gamma\) in {0, 1e-1,1e-2,1e-3}, the parameter \(\alpha,\beta\) in {0.1,0.3,0.5} and fix the margin parameter of the surrogate loss \(\ell\) as 1.0, which cost the same

\begin{table}
\begin{tabular}{l|c c c|c c|c c} \hline \hline  & \multicolumn{3}{c}{moltox21 (t0)} & \multicolumn{3}{c}{molmuv (t1)} & \multicolumn{3}{c}{molpcba (t0)} \\ \hline Method & (0.6, 0.4) & (0.5, 0.5) & (0.6, 0.4) & (0.5, 0.5) & (0.6, 0.4) & (0.5, 0.5) \\ \hline CE & 0.067 (0.001) & 0.208 (0.001) & 0.161 (0.034) & 0.469 (0.018) & 0.095 (0.001) & 0.264 (0.001) \\ AUC-SH & 0.064 (0.008) & 0.217 (0.014) & 0.260 (0.130) & 0.444 (0.128) & 0.140 (0.003) & 0.312 (0.003) \\ AUC-M & 0.066 (0.009) & 0.209 (0.01) & 0.114 (0.079) & 0.433 (0.053) & 0.142 (0.009) & 0.313 (0.003) \\ MB & 0.067 (0.015) & 0.215 (0.023) & 0.173 (0.153) & 0.426 (0.118) & 0.095 (0.002) & 0.262 (0.003) \\ AW-poly & 0.064 (0.01) & 0.206 (0.025) & 0.172 (0.144) & 0.393 (0.123) & 0.110 (0.001) & 0.281 (0.002) \\ SOTA-s & 0.068 (0.018) & 0.23 (0.021) & 0.327 (0.164) & 0.526 (0.122) & 0.143 (0.001) & 0.314 (0.002) \\ SONX & **0.07 (0.035)** & **0.252 (0.025)** & **0.347 (0.175)** & **0.575 (0.122)** & **0.158 (0.006)** & **0.335 (0.006)** \\ \hline \hline  & \multicolumn{3}{c}{MUSK2} & \multicolumn{3}{c}{Fox} \\ \hline Method & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) \\ \hline AUC-M (att) & 0.675 (0.1) & 0.783 (0.067) & **0.86 (0.036)** & 0.032 (0.03) & 0.253 (0.098) & 0.444 (0.118) \\ MIDAM (smx) & 0.525 (0.2) & 0.667 (0.149) & 0.8 (0.097) & 0.048 (0.059) & 0.265 (0.119) & 0.449 (0.113) \\ MIDAM (att) & 0.6 (0.215) & 0.717 (0.135) & 0.819 (0.092) & 0.016 (0.032) & 0.249 (0.125) & 0.509 (0.065) \\ SOTAs (att) & 0.6 (0.267) & 0.683 (0.178) & 0.819 (0.097) & 0.024 (0.032) & 0.278 (0.059) & 0.477 (0.046) \\ SONT (att) & **0.7 (0.1)** & **0.8 (0.067)** & **0.867 (0.036)** & **0.12 (0.131)** & **0.343 (0.176)** & **0.578 (0.119)** \\ \hline \hline  & \multicolumn{3}{c}{Colon} & \multicolumn{3}{c}{Lung} \\ \hline Method & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) & (0.5, 0.5) & (0.3, 0.7) & (0.1, 0.9) \\ \hline AUC-M (att) & 0.576 (0.10) & 0.739 (0.061) & 0.803 (0.038) & 0.32 (0.181) & 0.609 (0.113) & 0.744 (0.082) \\ MIDAM (smx) & 0.646 (0.083) & 0.787 (0.04) & 0.863 (0.026) & 0.43 (0.195) & 0.618 (0.128) & 0.824 (0.055) \\ MIDAM (att) & 0.548 (0.253) & 0.738 (0.149) & 0.826 (0.102) & 0.544 (0.261) & 0.716 (0.189) & 0.815 (0.129) \\ SOTAs (att) & 0.772 (0.124) & 0.862 (0.073) & 0.911 (0.045) & 0.539 (0.153) & 0.745 (0.077) & 0.841 (0.049) \\ SONT (att) & **0.8 (0.166)** & **0.875 (0.099)** & **0.916 (0.065)** & **0.639 (0.137)** & **0.779 (0.041)** & **0.865 (0.28)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Testing TPAUC on molecule datasets (top) and on MIL datasets (bottom). The two numbers in parentheses of the second line refers to the lower bound of TPR and the upper bound of FPR for evaluating TPAUC. The two numbers of each method refers to the mean TPAUC and its std.

tuning effort as the other baselines. The weight decay is set as the same value (2e-4) with the other baselines. For baselines, we directly use the results reported in [44] since we use the same setting.

For MIL TPAUC maximization, we train a simple Feed Forward Neural Network (FFNN) with one hidden layer (the number of neurons equals to data dimension) for the two tabular datasets and ResNet20 for the two medical image datasets. Sigmoid transformation is adopted for the output layer to generate prediction score. The training epoch number is fixed as 100 epochs for all methods; the bag batch size is fixed as 16 (resp. 8) and the number of sampled instances per bag is fixed as 4 (resp. 128) for tabular (resp. medical image) datasets; the learning rate is tuned in {1e-2, 1e-3, 1e-4} and decreased by 10 folds at the end of 50-th and 75-th epoch for all baselines. For SONT (att), we set moving average parameter \(\tau_{1}=\tau_{2}\) as 0.9; tune the parameter \(\gamma_{1}=\gamma_{2}=\gamma\) in {0, 1e-1,1e-2,1e-3} and fix the margin parameter of the surrogate loss \(\ell\) as 0.5, and the parameter \(\alpha,\beta\) in {0.1,0.5,0.9}. Similar parameters in baselines are set the same or tuned similarly. For all experiments, we utilize 5-fold-cross-validation to evaluate the testing performance based on the best validation performance with possible early stopping choice.

**Results.** The testing results for the regular and MIL TPUAC maximization with different TPAUC measures are summarized in the Table 2. From Table 2, we observe that our method SONX achieves the best performance for regular TPAUC maximization. It is better than the state-of-the-art method SOTAs for TPAUC maximization. We attribute the better performance of SONX to the fact that the objective of SONX is an exact estimator of TPAUC while the smoothed objective of SOTAs is an inexact estimator of TPAUC. We also observe that SONT (att) achieves the best performance in all cases, which is not surprising since it is the only one that directly optimizes the TPAUC surrogate. In contrast, other baselines either optimizes a different objective (MIDAM) or does not ensure convergence due to the use of mini-batch pooling (AUC-M, SOTAs).

**Ablation Study.** We conduct ablation studies to demonstrate the effect of the error correction term on the training convergence by varying the \(\gamma\) value for SONX and SONT, where \(\gamma_{1}=\gamma_{2}=\gamma\) is set as the same value in SONT. The training convergence results are presented in Figure 1. We can see that an appropriate value of \(\gamma>0\) can yield a faster convergence than \(\gamma=0\), which verifies the faster convergence of using MSVR estimators than using moving average estimators. However, we do observe a gap between theory and practice, as setting a large value of \(\gamma>1\) as in the theory might not yield convergence. This phenomenon is also observed in [12]. We conjecture that the gap could be fixed by considering convex objectives [40], which is left as future work.

## 7 Conclusions

In this paper, we have considered non-smooth weakly-convex two-level and tri-level finite-sum coupled compositional optimization problems. We presented novel convergence analysis of two stochastic algorithms and established their complexity. Applications in deep learning for two-way partial AUC maximization was considered and great performance of proposed algorithms were demonstrated through experiments on multiple datasets. A future work is to prove the convergence of both algorithms for convex objectives.

## Acknowledgements

We thank anonymous reviewers for constructive comments. Q. Hu, D. Zhu and T. Yang were partially supported by NSF Career Award 2246753, NSF Grant 2246757, 2246756 and 2306572.

Figure 1: Training Curves of SONX (left two) and SONT (right two) for TPAUC maximization with different \(\gamma\). The y-axis is the TPAUC (0.5, 0.5).

## References

* [1] Krishnakumar Balasubramanian, Saeed Ghadimi, and Anthony Nguyen. Stochastic multilevel composition optimization algorithms with level-independent convergence rates. _SIAM Journal on Optimization_, 32(2):519-544, 2022.
* [2] Andrew A Borkowski, Marilyn M Bui, L Brannon Thomas, Catherine P Wilson, Lauren A DeLand, and Stephen M Mastorides. Lung and colon cancer histopathological image dataset (lc25000). _arXiv preprint arXiv:1912.12142_, 2019.
* [3] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. _IEEE Transactions on Signal Processing_, 69:4937-4948, 2021.
* [4] Sebastian Curi, Kfir Y. Levy, Stefanie Jegelka, and Andreas Krause. Adaptive sampling for stochastic risk-averse learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1036-1047. Curran Associates, Inc., 2020.
* [5] Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [6] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions, 2018.
* [7] Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. _SIAM Journal on Optimization_, 29(3):1908-1930, 2019.
* [8] Dmitriy Drusvyatskiy and Courtney Paquette. Efficiency of minimizing compositions of convex functions and smooth maps. _Math. Program._, 178(1-2):503-558, 2019.
* [9] John C. Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization problems. _SIAM Journal on Optimization_, 28(4):3229-3259, 2018.
* [10] S. Ghadimi, Andrzej Ruszczy'nski, and Mengdi Wang. A single timescale stochastic approximation method for nested stochastic optimization. _SIAM J. Optim._, 30:960-979, 2020.
* [11] Lie He and Shiva Prasad Kasiviswanathan. Debiasing conditional stochastic optimization, 2023.
* [12] Quanqi Hu, Zi-Hao Qiu, Zhishuai Guo, Lijun Zhang, and Tianbao Yang. Blockwise stochastic variance-reduced methods with parallel speedup for multi-block bilevel optimization. In _Proceedings of the 39th International Conference on Machine Learning_, 2023.
* [13] Yifan Hu, Siqi Zhang, Xin Chen, and Niao He. Biased stochastic first-order methods for conditional stochastic optimization and applications in meta learning. _Advances in Neural Information Processing Systems_, 33, 2020.
* [14] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 2127-2136. PMLR, 10-15 Jul 2018.
* [15] Wei Jiang, Gang Li, Yibo Wang, Lijun Zhang, and Tianbao Yang. Multi-block-single-probe variance reduced estimator for coupled compositional optimization, 2022.
* Volume 1_, NIPS'14, page 694-702, Cambridge, MA, USA, 2014. MIT Press.
* [17] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6083-6093. PMLR, 13-18 Jul 2020.
* [18] J.J. Moreau. Proximite et dualite dans un espace hilbertien. _Bulletin de la Societe Mathematique de France_, 93:273-299, 1965.
* [19] Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. An online method for a class of distributionally robust optimization with non-convex objectives. _Advances in Neural Information Processing Systems_, 34, 2021.
* [20] Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang. Stochastic optimization of area under precision-recall curve for deep learning with provable convergence. In _Advances in neural information processing systems_, volume abs/2104.08736, 2021.
* [21] Zi-Hao Qiu, Quanqi Hu, Yongjian Zhong, Lijun Zhang, and Tianbao Yang. Large-scale stochastic optimization of NDCG surrogates for deep learning with provable convergence. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 18122-18152. PMLR, 17-23 Jul 2022.
* [22] Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex-concave min-max optimization: provable algorithms and applications in machine learning. _Optimization Methods and Software_, 37(3):1087-1121, 2022.
* [23] R. T. Rockafellar and S. Uryasev. Optimization of conditional valueat-risk. _Journal of Risk_, 2(3), 1999.
* [24] R.T. Rockafellar, M. Wets, and R.J.B. Wets. _Variational Analysis_. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2009.
* [25] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization, 2020.
* [26] Bokun Wang and Tianbao Yang. Finite-sum coupled compositional stochastic optimization: Theory and applications. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 23292-23317. PMLR, 17-23 Jul 2022.
* [27] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. _Mathematical Programming_, 161(1-2):419-449, 2017.
* [28] Mengdi Wang, Ji Liu, and Ethan X. Fang. Accelerating stochastic composition optimization, 2016.
* [29] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. MoleculeNet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [30] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _7th International Conference on Learning Representations_, 2019.
* [31] Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, 2020.
* [32] Shuoguang Yang, Mengdi Wang, and Ethan X. Fang. Multi-level stochastic gradient methods for nested composition optimization, 2018.

* [33] Tianbao Yang. Algorithmic foundation of deep x-risk optimization. _CoRR_, abs/2206.00439, 2022.
* [34] Tianbao Yang and Yiming Ying. AUC maximization in the era of big data and AI: A survey. _ACM Comput. Surv._, 55(8):172:1-172:37, 2023.
* [35] Zhiyong Yang, Qianqian Xu, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang. When all we need is a piece of the pie: A generic framework for optimizing two-way partial auc. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 11820-11829. PMLR, 18-24 Jul 2021.
* [36] Zhuoning Yuan, Yuexin Wu, Zi-Hao Qiu, Xianzhi Du, Lijun Zhang, Denny Zhou, and Tianbao Yang. Provable stochastic optimization for global contrastive learning: Small batch does not harm performance. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 25760-25782. PMLR, 17-23 Jul 2022.
* [37] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification, 2021.
* [38] Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance reduction. In _Advances in Neural Information Processing Systems_, pages 9075-9085, 2019.
* [39] Junyu Zhang and Lin Xiao. Multilevel composite stochastic optimization via nested variance reduction. _SIAM J. Optim._, 31(2):1131-1157, 2021.
* [40] Xuan Zhang, Necdet Serhat Aybat, and Mert Gurbuzbalaban. Robust accelerated primal-dual methods for computing saddle points. 2021.
* [41] Xuan Zhang, Necdet Serhat Aybat, and Mert Gurbuzbalaban. Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems, 2023.
* [42] Zhe Zhang and Guanghui Lan. Optimal algorithms for convex nested stochastic composite optimization, 2022.
* [43] Renbo Zhao. A primal-dual smoothing framework for max-structured non-convex optimization, 2022.
* [44] Dixian Zhu, Gang Li, Bokun Wang, Xiaodong Wu, and Tianbao Yang. When AUC meets DRO: Optimizing partial AUC for deep learning with non-convex convergence guarantee. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 27548-27573. PMLR, 17-23 Jul 2022.
* [45] Dixian Zhu, Bokun Wang, Zhi Chen, Yaxing Wang, Milan Sonka, Xiaodong Wu, and Tianbao Yang. Provable multi-instance deep auc maximization with stochastic pooling. In _International Conference on Machine Learning_. PMLR, 2023.
* [46] Landi Zhu, Mert Gurbuzbalaban, and Andrzej Ruszczynski. Distributionally robust learning with weakly convex losses: Convergence rates and finite-sample guarantees, 2023.

Proofs of Theorem 4.6 and Theorem 4.7

In this section, we provide the detailed proofs for Theorem 4.6 and Theorem 4.7. We first give a basic property for weakly-convex functions.

**Proposition A.1** (Proposition 2.1 in [7]).: _Suppose function \(g:\mathbb{R}^{d}\to\mathbb{R}\cup\{\infty\}\) is lower-semicontinuous. Then \(g\) is \(\rho\)-weakly-convex if and only if_

\[g(y)\geq g(x)+\langle v,y-x\rangle-\frac{\rho}{2}\|y-x\|^{2}\] (10)

_holds for all vectors \(v\in\partial g(x)\) and \(x,y\in\mathbb{R}^{d}\)._

### Proof of Theorem 4.6

Note that the proof of Lemma 4.5 also implies the following squared-norm error bound,

\[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t+1}-g_{i}(\mathbf{ w}_{t+1})\|^{2}\bigg{]}\leq(1-\frac{B_{1}\tau}{2n})^{t+1}\frac{1}{n}\sum_{i\in \mathcal{S}}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|^{2}+\frac{4\tau\sigma^{2}}{B_{2 }}+\frac{16n^{2}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau}.\]

Proof of Theorem 4.6.: Define \(\hat{\mathbf{w}}_{t}:=\text{prox}_{F/\bar{\rho}}(\mathbf{w}_{t})\). For a given \(i\in\mathcal{S}\), we have

\[f_{i}(g_{i}(\hat{\mathbf{w}}_{t}))-f_{i}(u_{i,t})\] \[\overset{(a)}{\geq}\partial f_{i}(u_{i,t})^{\top}(g_{i}(\hat{ \mathbf{w}}_{t})-u_{i,t})-\frac{\rho_{f}}{2}\|g_{i}(\hat{\mathbf{w}}_{t})-u_{i, t}\|^{2}\] \[\geq\partial f_{i}(u_{i,t})^{\top}(g_{i}(\hat{\mathbf{w}}_{t})-u_ {i,t})-\rho_{f}\|g_{i}(\hat{\mathbf{w}}_{t})-g_{i}(\mathbf{w}_{t})\|^{2}-\rho _{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\] \[\geq\partial f_{i}(u_{i,t})^{\top}(g_{i}(\hat{\mathbf{w}}_{t})-u_ {i,t})-\rho_{f}C_{g}^{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}-\rho_{f} \|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\] \[\overset{(b)}{\geq}\partial f_{i}(u_{i,t})^{\top}\bigg{[}g_{i}( \mathbf{w}_{t})-u_{i,t}+\partial g_{i}(\mathbf{w}_{t})^{\top}(\hat{\mathbf{w}} _{t}-\mathbf{w}_{t})-\frac{\rho_{g}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t} \|^{2}\bigg{]}\] \[\quad-\rho_{f}C_{g}^{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2 }-\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\] \[\overset{(c)}{\geq}\partial f_{i}(u_{i,t})^{\top}(g_{i}(\mathbf{ w}_{t})-u_{i,t})+\partial f_{i}(u_{i,t})^{\top}\partial g_{i}(\mathbf{w}_{t})^{ \top}(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})-(\frac{\rho_{g}C_{f}}{2}+\rho_{f}C_ {g}^{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\] \[\quad-\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\]

where (a) follows from the \(\rho_{f}\)-weak-convexity of \(f_{i}\), (b) follows from that \(f_{i}(\cdot)\) is non-decreasing and the weak convexity of \(g_{i}\), (c) is due to \(0\leq\partial f_{i}(u_{i,t})\leq C_{f}\). Then it follows

\[\frac{1}{n}\sum_{i\in\mathcal{S}}\partial f_{i}(u_{i,t})^{\top} \partial g_{i}(\mathbf{w}_{t})^{\top}(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})\] \[\leq\frac{1}{n}\sum_{i\in\mathcal{S}}\bigg{[}f_{i}(g_{i}(\hat{ \mathbf{w}}_{t}))-f_{i}(u_{i,t})-\partial f_{i}(u_{i,t})^{\top}(g_{i}(\mathbf{ w}_{t})-u_{i,t})+(\frac{\rho_{g}C_{f}}{2}+\rho_{f}C_{g}^{2})\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\|^{2}\] \[\quad+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\bigg{]}\] (11)

Now we consider the change in the Moreau envelope:

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})] =\mathbb{E}_{t}\left[\min_{\hat{\mathbf{w}}}F(\hat{\mathbf{w}})+ \frac{\bar{\rho}}{2}\|\hat{\mathbf{w}}-\mathbf{w}_{t+1}\|^{2}\right]\] \[\leq\mathbb{E}_{t}\left[F(\hat{\mathbf{w}}_{t})+\frac{\bar{\rho} }{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t+1}\|^{2}\right]\] \[=F(\hat{\mathbf{w}}_{t})+\mathbb{E}_{t}\bigg{[}\frac{\bar{\rho} }{2}\|\hat{\mathbf{w}}_{t}-(\mathbf{w}_{t}-\eta G_{t})\|^{2}\bigg{]}\] (12) \[\leq F(\hat{\mathbf{w}}_{t})+\frac{\bar{\rho}}{2}\|\hat{\mathbf{w }}_{t}-\mathbf{w}_{t}\|^{2}+\bar{\rho}\mathbb{E}_{t}[\eta\langle\hat{\mathbf{w} }_{t}-\mathbf{w}_{t},G_{t}\rangle]+\frac{\eta^{2}\bar{\rho}M^{2}}{2}\] \[=F_{1/\bar{\rho}}(\mathbf{w}_{t})+\bar{\rho}\eta\langle\hat{ \mathbf{w}}_{t}-\mathbf{w}_{t},\mathbb{E}_{t}[G_{t}]\rangle+\frac{\eta^{2}\bar{ \rho}M^{2}}{2}\]where

\[\mathbb{E}_{t}[G_{t}]=\frac{1}{n}\sum_{i\in\mathcal{S}}\partial g_{i}( \mathbf{w}_{t})\partial f_{i}(u_{i,t}),\]

and the second inequality uses the bound of \(\mathbb{E}[\|G_{t}\|^{2}]\), which follows from the Lipschitz continuity and bounded variance assumptions and is denoted by \(M\).

Combining inequality 29 and 30 yields

\[\begin{split}&\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]\\ &\leq F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^ {2}}{2}+\frac{\bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\left[f_{i}(g_{i}(\hat{ \mathbf{w}}_{t}))-f_{i}(u_{i,t})\right.\\ &\quad-\partial f_{i}(u_{i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{i, t})+(\frac{\rho_{g}C_{f}}{2}+\rho_{f}C_{g}^{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{ t}\|^{2}+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\right]\\ &=F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{2} }{2}+\frac{\bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\left[F_{i}(\hat{\mathbf{w }}_{t})-F_{i}(\mathbf{w}_{t})+f_{i}(g_{i}(\mathbf{w}_{t}))-f_{i}(u_{i,t}) \right.\\ &\quad-\partial f_{i}(u_{i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{i, t})+(\frac{\rho_{g}C_{f}}{2}+\rho_{f}C_{g}^{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{ t}\|^{2}+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\right]\end{split}\] (13)

Due to the \(\rho_{F}\)-weak convexity of \(F_{i}(\mathbf{w})\), we have (\(\bar{\rho}-\rho_{F}\))-strong convexity of \(\mathbf{w}\mapsto F_{i}(\mathbf{w})+\frac{\bar{\rho}}{2}\|\mathbf{w}_{t}- \mathbf{w}\|^{2}\). Then it follows

\[\begin{split} F_{i}(\hat{\mathbf{w}}_{t})-F_{i}(\mathbf{w}_{t})& =\left[F_{i}(\hat{\mathbf{w}}_{t})+\frac{\bar{\rho}}{2}\|\mathbf{ w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}\right]-\left[F_{i}(\mathbf{w}_{t})+\frac{\bar{ \rho}}{2}\|\mathbf{w}_{t}-\mathbf{w}_{t}\|^{2}\right]-\frac{\bar{\rho}}{2}\| \mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}\\ &\leq(\frac{\rho_{F}}{2}-\bar{\rho})\|\mathbf{w}_{t}-\hat{ \mathbf{w}}_{t}\|^{2}\end{split}\] (14)

Plugging inequality 32 into inequality 31 yields

\[\begin{split}\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]& \leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t})]+\frac{\eta^{2} \bar{\rho}M^{2}}{2}+\frac{\bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\left[( \frac{\rho_{F}}{2}-\bar{\rho})\|\mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2} \right.\\ &\quad+f_{i}(g_{i}(\mathbf{w}_{t}))-f_{i}(u_{i,t})-\partial f_{i} (u_{i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{i,t})\\ &\quad+(\frac{\rho_{g}C_{f}}{2}+\rho_{f}C_{g}^{2})\|\hat{\mathbf{ w}}_{t}-\mathbf{w}_{t}\|^{2}+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2} \right]\end{split}\] (15)

Set \(\bar{\rho}=\rho_{F}+\rho_{g}C_{f}+2\rho_{f}C_{g}^{2}\). We have

\[\begin{split}\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]& \leq F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{2}}{2}+ \frac{\bar{\rho}\eta}{n_{+}}\sum_{i\in\mathcal{S}}\left[-\frac{\bar{\rho}}{2}\| \mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}+f_{i}(g_{i}(\mathbf{w}_{t}))-f_{i}( u_{i,t})\right.\\ &\quad-\partial f_{i}(u_{i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{i, t})+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\right]\\ &\overset{(a)}{\leq}F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{ 2}\bar{\rho}M^{2}}{2}-\frac{\eta}{2}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t})\|^ {2}+\frac{\bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\left[f_{i}(g_{i}(\mathbf{w}_{t }))-f_{i}(u_{i,t})\right.\\ &\quad-\partial f_{i}(u_{i,t})^{\top}(g_{i}(\mathbf{w}_{t})-u_{i, t})+\rho_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|^{2}\right]\end{split}\]

where inequality (a) follows from Lemma 3.2.

Using the Lipschitz continuity of \(f_{i}\), we have

\[\begin{split}\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]& \leq F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{2}}{2}- \frac{\eta}{2}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t})\|^{2}+\frac{\bar{\rho} \eta}{n}\sum_{i\in\mathcal{S}}2C_{f}\|g_{i}(\mathbf{w}_{t})-u_{i,t}\|\\ &\quad+\frac{\bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\rho_{f}\|g_{i} (\mathbf{w}_{t})-u_{i,t}\|^{2}\end{split}\]By Lemma 4.5, the error bound of the MSVR update gives

\[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|\bigg{]}\leq(1-\mu)^{t}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i, 0}-g_{i}(\mathbf{w}_{0})\|+R_{1},\] \[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t}-g_{i} (\mathbf{w}_{t})\|^{2}\bigg{]}\leq(1-\mu)^{t}\frac{1}{n}\sum_{i\in\mathcal{S}} \|u_{i,0}-g_{i}(\mathbf{w}_{0})\|^{2}+R_{2},\]

where

\[\mu=\frac{B_{1}\tau}{2n},\quad R_{1}=\frac{2\tau^{1/2}\sigma}{B_{2}^{1/2}}+ \frac{4nC_{g}M\eta}{B_{1}\tau^{1/2}},\quad R_{2}=\frac{4\tau\sigma^{2}}{B_{2}} +\frac{16n^{2}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau}\]

Then

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})] \leq F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^ {2}}{2}-\frac{\eta}{2}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t})\|^ {2}]\] (16) \[\quad+2C_{f}\bar{\rho}\eta\left((1-\mu)^{t}\frac{1}{n}\sum_{i\in \mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|+R_{1}\right)\] \[\quad+C\rho_{f}\bar{\rho}\eta\left((1-\mu)^{t}\frac{1}{n}\sum_{i \in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|^{2}+R_{2}\right)\]

Taking summation from \(t=0\) to \(T-1\) yields

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{T})]\] (17) \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{0})+\frac{\eta^{2}\bar{\rho}M^ {2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]\] \[\quad+2C_{f}\bar{\rho}\eta\left(\sum_{t=0}^{T-1}(1-\mu)^{t}\frac{ 1}{n}\sum_{i\in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|+R_{1}T\right)\] \[\quad+C\rho_{f}\bar{\rho}\eta\left((1-\mu)^{t}\frac{1}{n}\sum_{i \in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|^{2}+R_{2}T\right)\] \[\overset{(a)}{\leq}F_{1/\bar{\rho}}(\mathbf{w}_{0})+\frac{\eta^{ 2}\bar{\rho}M^{2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/ \bar{\rho}}(\mathbf{w}_{t})\|^{2}]\] \[\quad+\frac{2C_{f}\bar{\rho}\eta}{n\mu}\sum_{i\in\mathcal{S}}\|g _{i}(\mathbf{w}_{0})-u_{i,0}\|+2C_{f}\bar{\rho}\eta R_{1}T+\frac{\rho_{f}\bar {\rho}\eta}{n\mu}\sum_{i\in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|^{2}+ 2\rho_{f}\bar{\rho}\eta R_{2}T,\]

where (a) uses \(\sum_{t=0}^{T-1}(1-\mu)^{t}\leq\frac{1}{\mu}\).

Lower bounding the left-hand-side by \(\min_{\mathbf{w}}F(\mathbf{w})\), we obtain

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]\] (18) \[\leq\frac{2}{\eta T}\bigg{[}F_{1/\bar{\rho}}(\mathbf{w}_{0})- \min_{\mathbf{w}}F(\mathbf{w})+\frac{\eta^{2}\bar{\rho}M^{2}T}{2}+\frac{2C_{f} \bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|+2C_ {f}\bar{\rho}\eta R_{1}T\] \[\quad+\frac{\rho_{f}\bar{\rho}\eta}{n}\sum_{i\in\mathcal{S}}\|g_{ i}(\mathbf{w}_{0})-u_{i,0}\|^{2}+\rho_{f}\bar{\rho}\eta R_{2}T\bigg{]}\] \[\leq\frac{2\Delta}{\eta T}+\eta\bar{\rho}M^{2}+\frac{4C_{f}\bar{ \rho}}{\mu Tn}\sum_{i\in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|+4C_{f} \bar{\rho}R_{1}+\frac{2\rho_{f}\bar{\rho}}{\mu Tn}\sum_{i\in\mathcal{S}}\|g_{i }(\mathbf{w}_{0})-u_{i,0}\|^{2}+2\rho_{f}\bar{\rho}R_{2}\] \[\leq\frac{C}{T}(\frac{1}{\eta}+\frac{1}{\mu})+C(\eta+R_{1}+R_{2})\]

where we assume \(F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{0})-\min_{\mathbf{ w},\mathbf{s},s^{\prime}}F(\mathbf{w},\mathbf{s},s^{\prime})\leq\Delta\) and

\[C=\max\{8\Delta,12\bar{\rho}M^{2},\frac{16C_{f}\bar{\rho}}{n}\sum_{i\in \mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|,\frac{8\rho_{f}\bar{\rho}}{n} \sum_{i\in\mathcal{S}}\|g_{i}(\mathbf{w}_{0})-u_{i,0}\|^{2},16C_{f}\bar{\rho},8 \rho_{f}\bar{\rho}\}.\]Thus

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}(\mathbf{ w}_{t})\|^{2}]\] \[\leq\frac{C}{T}(\frac{1}{\eta}+\frac{2n}{B_{1}\tau})+C(\eta+\frac{ 2\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4nC_{g}M\eta}{B_{1}\tau^{1/2}}+\frac{4 \tau\sigma^{2}}{B_{2}}+\frac{16n^{2}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau})\] \[=\mathcal{O}\left(\frac{1}{T}(\frac{1}{\eta}+\frac{n}{B_{1}\tau} )+(\eta+\frac{\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{n\eta}{B_{1}\tau^{1/2}}+ \frac{\tau\sigma^{2}}{B_{2}}+\frac{n^{2}\eta^{2}}{B_{1}^{2}\tau})\right)\]

Setting

\[\tau=\mathcal{O}(B_{2}\epsilon^{4}),\quad\eta=\mathcal{O}\left( \frac{B_{1}B_{2}^{1/2}\epsilon^{4}}{n}\right)\]

To reach an \(\epsilon\)-stationary point, we need

\[T=\mathcal{O}\left(\frac{n}{B_{1}B_{2}^{1/2}\epsilon^{6}}\right)\]

### Proof of Theorem 4.7

A formal statement in given below.

**Theorem A.2**.: _Under Assumption 4.3, with \(\gamma_{1}=\frac{n_{1}n_{2}-B_{1}B_{2}}{B_{1}B_{2}(1-\tau_{1})}+(1-\tau_{1})\), \(\gamma_{2}=\frac{n_{1}-B_{1}}{B_{1}(1-\tau_{2})}+(1-\tau_{2})\), \(\tau_{1}=\mathcal{O}\left(\min\{B_{3},\frac{B_{1}^{1/2}n_{2}^{1/2}}{n_{1}^{1/2} }\}\epsilon^{4}\right)\leq\frac{1}{2}\), \(\tau_{2}=\mathcal{O}(B_{2}\epsilon^{4})\leq\frac{1}{2}\), \(\eta=\mathcal{O}\left(\min\left\{B_{3}^{1/2},\frac{B_{1}^{1/4}n_{2}^{1/4}}{n_{1 }^{1/4}},\frac{B_{1}^{1/2}n_{2}^{1/2}}{n_{1}^{1/2}}\right\}\frac{B_{1}B_{2}}{ n_{1}n_{2}}\epsilon^{4}\right)\), and \(\bar{\rho}=\rho_{F}+4\rho_{f}C_{g}^{2}+2\rho_{g}C_{f}C_{h}^{2}+C_{f}C_{g}L_{h}\), Algorithm 2 converges to an \(\epsilon\)-stationary point of the Moreau envelope \(F_{1/\bar{\rho}}\) in \(T=\mathcal{O}\left(\max\left\{\frac{1}{B_{3}^{1/2}},\frac{n_{1}^{1/4}}{B_{1}^ {1/4}n_{2}^{1/4}},\frac{n_{1}^{1/2}}{B_{1}^{1/2}n_{2}^{1/2}}\right\}\frac{n_ {1}n_{2}}{B_{1}B_{2}}\epsilon^{-6}\right)\) iterations._

We first define constant \(M^{2}\geq\max\{\frac{3C_{f}^{2}C_{g}^{2}\sigma^{2}}{B_{3}}+\frac{3C_{f}^{2}C_{ g}^{2}C_{h}^{2}}{B_{2}}+\frac{3C_{f}^{2}C_{g}^{2}C_{h}^{2}}{B_{1}},\tilde{C}_{h}^{2}+ \sigma^{2}\}\) so that \(\mathbb{E}_{t}[\|G_{t}\|^{2}]\leq M^{2}\) and \(\|v_{i,j,t}\|^{2}\leq M^{2}\) for all \(i\in\mathcal{S}_{1},j\in\mathcal{S}_{2}\) and \(t\). Then to prove Theorem A.2, we need the following Lemmas.

**Lemma A.3**.: _Consider MSVR update for \(v\). Assume \(h_{i,j}(\mathbf{w};\xi)\) is \(C_{h}\)-Lipshitz for all \((i,j)\in S_{1}\times S_{2}\), and \(\mathbb{E}[\|G_{t}\|^{2}]\leq M^{2}\). With \(\gamma_{1}=\frac{n_{1}n_{2}-B_{1}B_{2}}{B_{1}B_{2}(1-\tau_{1})}+(1-\tau_{1})\), and \(\tau_{1}\leq\frac{1}{2}\), we have_

\[\mathbb{E}\bigg{[}\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}} \sum_{j\in S_{2}}\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|\bigg{]}\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{t+1}\frac{1}{n_{1 }}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}\|v_{i,j,0}-h_{i,j}(\mathbf{ w}_{0})\|+\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{4n_{1}n_{2}C_{h}M \eta}{B_{1}B_{2}\tau_{1}^{1/2}}\] \[\mathbb{E}\bigg{[}\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}} \sum_{j\in S_{2}}\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|^{2}\bigg{]}\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{2(t+1)}\frac{1}{n_ {1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}\|v_{i,j,0}-h_{i,j}(\mathbf{ w}_{0})\|^{2}+\frac{4\tau_{1}\sigma^{2}}{B_{3}}+\frac{16n_{1}^{2}n_{2}^{2}C_{h}^{2}M^{2} \eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}}\]

**Lemma A.4**.: _Consider MSVR update for \(u\). Assume \(g_{i}(\cdot)\) is \(C_{g}\)-Lipshitz for all \(i\in S_{1}\). With \(\gamma_{2}=\frac{n_{t}-B_{1}}{B_{1}(1-\tau_{2})}+(1-\tau_{2})\) and \(\tau_{2}\leq\frac{1}{2}\), we have_

\[\mathbb{E}\left[\frac{1}{n_{1}}\sum_{i\in S_{1}}\|u_{i,t+1}-\frac{ 1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t+1})\|\right]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{t+1}\frac{1}{n_{1}}\sum_{i \in S_{1}}\|u_{i,0}-\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,0})\|+\frac{2 \tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+\frac{C_{2}n_{1}^{1/2}B_{2}^{1/2}\tau_{1}} {B_{1}^{1/2}n_{2}^{1/2}\tau_{2}^{1/2}}+\frac{C_{2}n_{1}^{3/2}n_{2}^{1/2}\eta}{ B_{1}^{3/2}B_{2}^{1/2}\tau_{2}^{1/2}}\]

_where \(C_{2}\) is a constant defined in the proof._

Proof of Theorem a.2.: Consider the change in the Moreau envelope:

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})] =\mathbb{E}_{t}\left[\min_{\hat{\mathbf{w}}}F(\tilde{\mathbf{w}}) +\frac{\bar{\rho}}{2}\|\hat{\mathbf{w}}-\mathbf{w}_{t+1}\|^{2}\right]\] (18) \[\leq\mathbb{E}_{t}\left[F(\hat{\mathbf{w}}_{t})+\frac{\bar{\rho} }{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t+1}\|^{2}\right]\] \[=F(\hat{\mathbf{w}}_{t})+\mathbb{E}_{t}\bigg{[}\frac{\bar{\rho} }{2}\|\hat{\mathbf{w}}_{t}-(\mathbf{w}_{t}-\eta G_{t})\|^{2}\bigg{]}\] \[\leq F(\hat{\mathbf{w}}_{t})+\frac{\bar{\rho}}{2}\left(\|\hat{ \mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\right)+\bar{\rho}\mathbb{E}_{t}[\eta \langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},G_{t}\rangle]+\frac{\eta^{2}\bar{ \rho}M^{2}}{2}\] \[=F_{1/\bar{\rho}}(\mathbf{w}_{t})+\bar{\rho}\mathbb{E}_{t}[\eta \langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},G_{t}\rangle]+\frac{\eta^{2}\bar{ \rho}M^{2}}{2}\]

Note that

\[\mathbb{E}_{t}[G_{t}]=\frac{1}{n_{1}}\sum_{i=1}^{n_{1}}\bigg{[} \frac{1}{n_{2}}\sum_{j=1}^{n_{2}}\nabla h_{i,j}(\mathbf{w}_{t})\partial g_{i}( v_{i,j,t})\bigg{]}\partial f_{i}\left(u_{i,t}\right),\]

and the second inequality uses the bound of \(\mathbb{E}[\|G_{t}\|^{2}]\), which follows from the Lipschitz continuity and bounded variance assumptions and is denoted by \(M\).

Define \(\hat{\mathbf{w}}_{t}:=\text{prox}_{F/\hat{\mathbf{w}}}(\mathbf{v}_{t})\). For a given \(i\in\{1,\dots,m\}\), we have

\[\frac{1}{n_{1}}\sum_{i\in S_{1}}f_{i}(\frac{1}{n_{2}}\sum_{j\in S_{ 2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t})))-\frac{1}{n_{1}}\sum_{i\in S_{1}}f_{i}( u_{i,t})\] (19) \[\overset{(a)}{\geq}\frac{1}{n_{1}}\sum_{i\in S_{1}}\partial f_{i }(u_{i,t})^{\top}(\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w }}_{t}))-u_{i,t})-\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{\rho_{f}}{2}\|\frac{1}{ n_{2}}\sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t}))-u_{i,t}\|^{2}\] \[\geq\frac{1}{n_{1}}\sum_{i\in S_{1}}\partial f_{i}(u_{i,t})^{\top }(\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t}))-u_{i,t})\] \[\quad-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\|\frac{1}{n_{2}} \sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t}))-\frac{1}{n_{2}}\sum_{j \in S_{2}}g_{i}(v_{i,j,t})\|^{2}-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\| \frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\|^{2}\] \[\geq\frac{1}{n_{1}}\sum_{i\in S_{1}}\partial f_{i}(u_{i,t})^{\top }(\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t}))-u_{i,t} )-\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}\rho_{f}C_{g} ^{2}\|h_{i,j}(\hat{\mathbf{w}}_{t})-v_{i,j,t}\|^{2}\] \[\quad-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\|\frac{1}{n_{2}} \sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\|^{2}\] \[\overset{(b)}{\geq}\frac{1}{n_{1}}\sum_{i\in S_{1}}\partial f_{i }(u_{i,t})^{\top}\bigg{[}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t }+\frac{1}{n_{2}}\sum_{j\in S_{2}}\partial g_{i}(v_{i,j,t})^{\top}(h_{i,j}( \hat{\mathbf{w}}_{t})-v_{i,j,t})\] \[\quad-\frac{1}{n_{2}}\sum_{j\in S_{2}}\frac{\rho_{g}}{2}\|h_{i,j} (\hat{\mathbf{w}}_{t})-v_{i,j,t}\|^{2}\bigg{]}-\frac{1}{n_{1}}\sum_{i\in S_{1}} \frac{1}{n_{2}}\sum_{j\in S_{2}}2\rho_{f}C_{g}^{2}\|h_{i,j}(\mathbf{w}_{t})-v_ {i,j,t}\|^{2}\] \[\quad-2\rho_{f}C_{g}^{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^ {2}-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\|\frac{1}{n_{2}}\sum_{j\in S_{2}} g_{i}(v_{i,j,t})-u_{i,t}\|^{2}\] \[\quad-2\rho_{f}C_{g}^{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^ {2}-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\|\frac{1}{n_{2}}\sum_{j\in S_{2}} g_{i}(v_{i,j,t})-u_{i,t}\|^{2}\] \[\quad\geq\frac{1}{n_{1}}\sum_{i\in S_{1}}\partial f_{i}(u_{i,t})^ {\top}\bigg{[}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{]}\] \[\quad+\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}\langle\partial f_{i}(u_{i,t})^{\top}\partial g_{i}(v_{i,j,t})^{\top}(h_{ i,j}(\hat{\mathbf{w}}_{t})-v_{i,j,t})\rangle_{A_{1}}\] \[\quad-\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f})\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad-(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2})\|\hat{\mathbf{w }}_{t}-\mathbf{w}_{t}\|^{2}-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\|\frac{1}{ n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\|^{2}\]

where (a) follows from the convexity of \(f_{i}\), (b) uses the assumption that \(f_{i}(\cdot)\) is non-decreasing and \(g_{i}\) is weak convex, (c) is due to \(0\leq\partial f_{i}(u_{i,t})\leq C_{f}\).

The \(L_{h}\)-smoothness assumption of \(h_{i,j}(\mathbf{w})\) (or weakly-convexity of \(h_{i,j}(\mathbf{w})\), then only the second inequality holds) for all \(i,\mathbf{w}\) implies

\[\begin{split}& h_{i,j}(\hat{\mathbf{w}}_{t})\leq h_{i,j}(\mathbf{ w}_{t})+\nabla h_{i,j}(\mathbf{w}_{t})^{\top}(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})+ \frac{L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2},\\ & h_{i,j}(\hat{\mathbf{w}}_{t})\geq h_{i,j}(\mathbf{w}_{t})+ \nabla h_{i,j}(\mathbf{w}_{t})^{\top}(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})- \frac{L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}.\end{split}\] (20)

We first assume that \(g_{i}(\cdot)\) is non-increasing. Since \(\partial f_{i}(u_{i,t})\geq 0\) and \(\partial g_{i}(v_{i,j,t})\leq 0\), we bound \(A_{1}\) as following

\[\begin{split}& A_{1}=\partial f_{i}(u_{i,t})\partial^{\top}g_{i} (v_{i,j,t})^{\top}(h_{i,j}(\hat{\mathbf{w}}_{t})-v_{i,j,t})\\ &\stackrel{{(a)}}{{\geq}}\langle\partial f_{i}(u_{i, t})^{\top}\partial g_{i}(v_{i,j,t})^{\top}(h_{i,j}(\mathbf{w}_{t})-v_{i,j,t})+ \partial f_{i}(u_{i,t})^{\top}\partial g_{i}(v_{i,j,t})^{\top}\nabla h_{i,j}( \mathbf{w}_{t})^{\top}(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})\\ &\quad+\partial f_{i}(u_{i,t})^{\top}\partial g_{i}(v_{i,j,t})^{ \top}\frac{L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2})\\ &\stackrel{{(b)}}{{\geq}}-C_{f}C_{g}\|h_{i,j}( \mathbf{w}_{t})-v_{i,j,t}\|+\partial f_{i}(u_{i,t})^{\top}\partial g_{i}(v_{i, j,t})^{\top}\nabla h_{i,j}(\mathbf{w}_{t})^{\top}(\hat{\mathbf{w}}_{t}- \mathbf{w}_{t})\\ &\quad-\frac{C_{f}C_{g}L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w }_{t}\|^{2}\end{split}\] (21)

where inequality (a) follows from the first inequality in (20), (b) follows from the Lipschitz continuity and monotone assumptions on \(f_{i},g_{i},h_{i,j}\). On the other hand, if we assume \(g_{i}(\cdot)\) is non-decreasing, we may use the second inequality in (20) and obtain the same result as (21). Now plugging the new formulation of \(A_{1}\) back to inequality 19 yields

\[\begin{split}&\frac{1}{n_{1}}\sum_{i\in S_{1}}f_{i}(\frac{1}{n_{ 2}}\sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t})))-\frac{1}{n_{1}}\sum _{i\in S_{1}}f_{i}(u_{i,t})\\ &\geq\frac{1}{n_{1}}\sum_{i\in S_{1}}\partial f_{i}(u_{i,t})^{ \top}\bigg{[}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{]} +\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}-C_{f}C_{g} \|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|\\ &\quad+\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}\partial f_{i}(u_{i,t})^{\top}\partial g_{i}(v_{i,j,t})^{\top}\nabla h_{i,j}(\mathbf{w}_{t})^{\top}(\hat{\mathbf{w}}_{t}-\mathbf{w}_{t})-\frac{C_{f}C _{g}L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\\ &\quad-\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f})\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\| ^{2}\\ &\quad-(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2})\|\hat{\mathbf{ w}}_{t}-\mathbf{w}_{t}\|^{2}-\frac{1}{n_{1}}\sum_{i\in S_{1}}\rho_{f}\bigg{\|} \frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\\ &\geq\frac{1}{n_{1}}\sum_{i\in S_{1}}-C_{f}\bigg{\|}\frac{1}{n_{ 2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}+\frac{1}{n_{1}}\sum_{i \in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}-C_{f}C_{g}\|h_{i,j}(\mathbf{w}_{t}) -v_{i,j,t}\|\\ &\quad+\langle\mathbb{E}_{t}[G_{t}],\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\rangle-\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j \in S_{2}}(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f})\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t }\|^{2}\\ &\quad-(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2}+\frac{C_{f}C_{g} L_{h}}{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}-\frac{1}{n_{1}}\sum_{i\in S_{1}} \rho_{f}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t} \bigg{\|}^{2}\end{split}\]

It follows

\[\begin{split}&\langle\mathbb{E}_{t}[G_{t}],\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\rangle\\ &\leq\frac{1}{n_{1}}\sum_{i\in S_{1}}f_{i}(\frac{1}{n_{2}}\sum_{j \in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t})))-\frac{1}{n_{1}}\sum_{i\in S_{1}}f_{ i}(u_{i,t})+\frac{1}{n_{1}}\sum_{i\in S_{1}}C_{f}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{ i,j,t})-u_{i,t}\bigg{\|}\\ &\quad+\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}C_{f}C_{g}\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|+\frac{1}{n_{1}}\sum_{i\in S _{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f})\|h_{i,j}( \mathbf{w}_{t})-v_{i,j,t}\|^{2}\\ &\quad+(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2}+\frac{C_{f}C_{g}L_{h }}{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\frac{1}{n_{1}}\sum_{i\in S_{1}} \rho_{f}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t} \bigg{\|}^{2}\end{split}\] (22)Combining inequality 22 and 18 yields

\[\mathbb{E}_{t}[F_{1/\hat{\rho}}(\mathbf{w}_{t+1})]\] \[\leq F_{1/\hat{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{2}} {2}+\bar{\rho}\eta\bigg{\{}\frac{1}{n_{1}}\sum_{i\in S_{1}}\bigg{[}f_{i}(\frac{ 1}{n_{2}}\sum_{j\in S_{2}}g_{i}(h_{i,j}(\hat{\mathbf{w}}_{t})))-f_{i}(u_{i,t})\] \[\quad+C_{f}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t })-u_{i,t}\bigg{\|}+\frac{1}{n_{2}}\sum_{j\in S_{2}}C_{f}C_{g}\|h_{i,j}(\mathbf{ w}_{t})-v_{i,j,t}\|\] \[\quad+\frac{1}{n_{2}}\sum_{j\in S_{2}}(2\rho_{f}C_{g}^{2}+\rho_{g }C_{f})\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad+(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2}+\frac{C_{f}C_{g} L_{h}}{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\rho_{f}\bigg{\|}\frac{1}{n_{2 }}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\bigg{\}}\] \[\leq F_{1/\hat{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{ 2}}{2}+\bar{\rho}\eta\bigg{\{}\frac{1}{n_{1}}\sum_{i\in S_{1}}\bigg{[}F_{i}( \hat{\mathbf{w}}_{t})-F_{i}(\mathbf{w}_{t})+F_{i}(\mathbf{w}_{t})-f_{i}(\frac{ 1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t}))\] \[\quad+f_{i}(\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t}))-f_{i }(u_{i,t})+C_{f}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\] \[\quad+\frac{1}{n_{2}}\sum_{j\in S_{2}}C_{f}C_{g}\|h_{i,j}(\mathbf{ w}_{t})-v_{i,j,t}\|+\frac{1}{n_{2}}\sum_{j\in S_{2}}(2\rho_{f}C_{g}^{2}+\rho_{g }C_{f})\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad+(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2}+\frac{C_{f}C_{g} L_{h}}{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\rho_{f}\bigg{\|}\frac{1}{n_{ 2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\bigg{\}}\] \[\overset{(a)}{\leq}F_{1/\hat{\rho}}(\mathbf{w}_{t})+\frac{\eta^ {2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta\bigg{\{}\frac{1}{n_{1}}\sum_{i\in S_{1}} \bigg{[}F_{i}(\hat{\mathbf{w}}_{t})-F_{i}(\mathbf{w}_{t})+2C_{f}\bigg{\|}\frac {1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\] \[\quad+\frac{1}{n_{2}}\sum_{j\in S_{2}}2C_{f}C_{g}\|h_{i,j}(\mathbf{ w}_{t})-v_{i,j,t}\|+\frac{1}{n_{2}}\sum_{j\in S_{2}}(2\rho_{f}C_{g}^{2}+\rho_{g }C_{f})\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad+(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2}+\frac{C_{f}C_{g} L_{h}}{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\rho_{f}\bigg{\|}\frac{1}{n_{2}} \sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\bigg{\}}\]

where (a) follows from the Lipschitz continuity of \(f_{i},g_{i},h_{i,j}\).

Due to the \(\rho_{F}\)-weak convexity of \(F_{i}(\mathbf{w})\), we have \((\bar{\rho}-\rho_{F})\)-strong convexity of \(\mathbf{w}\mapsto F_{i}(\mathbf{w})+\frac{\bar{\rho}}{2}\|\mathbf{w}_{t}- \mathbf{w}\|^{2}\). Then it follows

\[F_{i}(\hat{\mathbf{w}}_{t})-F_{i}(\mathbf{w}_{t}) =\bigg{[}F_{i}(\hat{\mathbf{w}}_{t})+\frac{\bar{\rho}}{2}\| \mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}\bigg{]}-\bigg{[}F_{i}(\mathbf{w}_{t})+ \frac{\bar{\rho}}{2}\|\mathbf{w}_{t}-\mathbf{w}_{t}\|^{2}\bigg{]}-\frac{\bar{ \rho}}{2}\|\mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}\] \[\leq(\frac{\rho_{F}}{2}-\bar{\rho})\|\mathbf{w}_{t}-\hat{ \mathbf{w}}_{t}\|^{2}\] (23)Plugging inequality 23 back into A.2, we obtain

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]\] \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2}\bar{\rho}M^{2}} {2}+\bar{\rho}\eta\bigg{\{}\frac{1}{n_{1}}\sum_{i\in S_{1}}\bigg{[}(\frac{\rho_ {F}}{2}-\bar{\rho})\|\mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}+2C_{f}\bigg{\|} \frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\] \[\quad+\frac{1}{n_{2}}\sum_{j\in S_{2}}2C_{f}C_{g}\|h_{i,j}(\mathbf{ w}_{t})-v_{i,j,t}\|+\frac{1}{n_{2}}\sum_{j\in S_{2}}(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}) \|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad+(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}C_{h}^{2}+\frac{C_{f}C_{g} L_{h}}{2})\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\rho_{f}\bigg{\|}\frac{1}{n_{2}} \sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\bigg{\}}\] \[\overset{(a)}{\leq}F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^ {2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta\bigg{\{}\frac{1}{n_{1}}\sum_{i\in S_{1}} \bigg{[}-\frac{\bar{\rho}}{2}\|\mathbf{w}_{t}-\hat{\mathbf{w}}_{t}\|^{2}+C_{1} \bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\] \[\quad+\frac{1}{n_{2}}\sum_{j\in S_{2}}C_{1}\|h_{i,j}(\mathbf{w}_{ t})-v_{i,j,t}\|+\frac{1}{n_{2}}\sum_{j\in S_{2}}C_{1}\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad+C_{1}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j, t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\bigg{\}}\] \[\overset{(b)}{=}F_{1/\bar{\rho}}(\mathbf{w}_{t})+\frac{\eta^{2} \bar{\rho}M^{2}}{2}-\frac{\eta}{2}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t})\|^ {2}+C_{1}\bar{\rho}\eta\frac{1}{n_{1}}\sum_{i\in S_{1}}\bigg{\|}\frac{1}{n_{2} }\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\] \[\quad+C_{1}\bar{\rho}\eta\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1} {n_{2}}\sum_{j\in S_{2}}\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|+C_{1}\bar{\rho} \eta\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}\|h_{i,j}( \mathbf{w}_{t})-v_{i,j,t}\|^{2}\] \[\quad+C_{1}\bar{\rho}\eta\frac{1}{n_{1}}\sum_{i\in S_{1}}\bigg{\|} \frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\]

where in inequality (a) we use \(\bar{\rho}=\rho_{F}+4\rho_{f}C_{g}^{2}+2\rho_{g}C_{f}C_{h}^{2}+C_{f}C_{g}L_{h}\) and \(C_{1}=\max\{2C_{f}C_{g},2C_{f},(2\rho_{f}C_{g}^{2}+\rho_{g}C_{f}),\rho_{f}\}\), and equality (b) uses Lemma 3.2.

With general error bounds

\[\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}} \mathbb{E}[\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|]\leq(1-\mu_{1})^{t}\frac{1}{ n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}\|h_{i,j}(\mathbf{w}_{0})-v_{i,j,0} \|+R_{1},\] \[\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}} \mathbb{E}[\|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}]\leq(1-\mu_{1})^{t}\frac{1 }{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S_{2}}\|h_{i,j}(\mathbf{w}_{ 0})-v_{i,j,0}\|^{2}+R_{2},\] \[\frac{1}{n_{1}}\sum_{i\in S_{1}}\mathbb{E}\bigg{[}\bigg{\|}\frac{1 }{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\bigg{]}\leq(1-\mu_{ 2})^{t}\frac{1}{n_{+}}\sum_{i\in S_{+}}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g _{i}(v_{i,j,0})-u_{i,0}\bigg{\|}+R_{3},\] \[\frac{1}{n_{1}}\sum_{i\in S_{1}}\mathbb{E}\bigg{[}\bigg{\|}\frac{1 }{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\leq(1- \mu_{2})^{t}\frac{1}{n_{+}}\sum_{i\in S_{+}}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S _{2}}g_{i}(v_{i,j,0})-u_{i,0}\bigg{\|}^{2}+R_{4},\]

we have

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1})]\] \[\leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t})]+\frac{\eta^{2} \bar{\rho}M^{2}}{2}-\frac{\eta}{2}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]+C_{1}\bar{\rho}\eta(1-\mu_{min})^{t}\bigg{[}\frac{1}{n_{1 }}\sum_{i\in S_{1}}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,0})-u_{i,0 }\bigg{\|}\] \[\quad+\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}\|h_{i,j}(\mathbf{w}_{0})-v_{i,j,0}\|^{2}\bigg{]}+C_{1}\bar{\rho}\eta(R_{1}+R _{2}+R_{3}+R_{4}),\]

where \(\mu_{min}=\min\{\mu_{1},\mu_{2}\}\).

Taking summation from \(t=0\) to \(T-1\) yields

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{T})]\] \[\leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{0})]+\frac{\eta^{2} \bar{\rho}M^{2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/ \bar{\rho}}(\mathbf{w}_{t})\|^{2}]+C_{1}\bar{\rho}\eta\sum_{t=0}^{T-1}(1-\mu_{ min})^{t}\Delta_{0}\] \[\quad+TC_{1}\bar{\rho}\eta(R_{1}+R_{2}+R_{3}+R_{4})\] \[\leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{0})]+\frac{\eta^{2} \bar{\rho}M^{2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/ \bar{\rho}}(\mathbf{w}_{t})\|^{2}]+\frac{C_{1}\bar{\rho}\eta\Delta_{0}}{\mu_ {min}}+TC_{1}\bar{\rho}\eta(R_{1}+R_{2}+R_{3}+R_{4})\]

where we use \(\sum_{t=0}^{T-1}(1-\mu_{min})^{t}\leq\frac{1}{\mu_{min}}\) and define constant \(\Delta_{0}\) such that

\[\bigg{[}\frac{1}{n_{1}}\sum_{i\in S_{1}}\bigg{\|}\frac{1}{n_{2}} \sum_{j\in S_{2}}g_{i}(v_{i,j,0})-u_{i,0}\bigg{\|}+\frac{1}{n_{1}}\sum_{i\in S _{1}}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,0})-u_{i,0}\bigg{\|} ^{2}\] \[\quad+\frac{1}{n_{1}}\sum_{i\in S_{1}}\frac{1}{n_{2}}\sum_{j\in S _{2}}\|h_{i,j}(\mathbf{w}_{0})-v_{i,j,0}\|+\frac{1}{n_{1}}\sum_{i\in S_{1}} \frac{1}{n_{2}}\sum_{j\in S_{2}}\|h_{i,j}(\mathbf{w}_{0})-v_{i,j,0}\|^{2}\bigg{]} \leq\Delta_{0}.\]

Then it follows

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]\] \[\leq\frac{2}{\eta T}\bigg{[}F_{1/\bar{\rho}}(\mathbf{w}_{0})- \mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{T})]+\frac{\eta^{2}\bar{\rho}M^{2}T}{ 2}+\frac{C_{1}\bar{\rho}\eta\Delta_{0}}{\mu_{min}}+TC_{1}\bar{\rho}\eta(R_{1}+ R_{2}+R_{3}+R_{4})\bigg{]}\] \[\leq\frac{2\Delta}{\eta T}+\eta\bar{\rho}M^{2}+\frac{2C_{1}\bar{ \rho}\Delta_{0}}{\mu_{min}T}+2C_{1}\bar{\rho}(R_{1}+R_{2}+R_{3})\] \[=\mathcal{O}(\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{min}})+\eta +R_{1}+R_{2}+R_{3}+R_{4})\]

where we define constant \(\Delta\) such that \(F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{0})-\mathbb{E}[F_{ 1/\bar{\rho}}(\mathbf{w}_{T},\mathbf{s}_{T},s^{\prime}_{T})]\leq\Delta\).

With MSVR updates for \(v_{i,j,t}\) and \(u_{i,t}\), following from Lemma A.3 and Lemma A.4, we have

\[\mu_{1}=\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}},\quad\mu_{2}=\frac {B_{1}\tau_{2}}{2n_{1}},\quad R_{1}=\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+ \frac{4n_{1}n_{2}\sqrt{C_{h}}M\eta}{B_{1}B_{2}\tau_{1}^{1/2}}\] \[R_{2}=\frac{4\tau_{1}\sigma^{2}}{B_{3}}+\frac{16n_{1}^{2}n_{2}^{ 2}C_{h}M^{2}\eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}},\quad R_{3}=\frac{2\tau_{2}^ {1/2}\sigma}{B_{2}^{1/2}}+\frac{C_{2}n_{1}^{1/2}B_{2}^{1/2}\tau_{1}}{B_{1}^{1/2 }n_{2}^{1/2}\tau_{1}^{1/2}}+\frac{C_{2}n_{1}^{3/2}n_{2}^{1/2}\eta}{B_{1}^{3/2}B _{2}^{1/2}\tau_{2}^{1/2}},\] \[R_{4}=\frac{4\tau_{2}\sigma^{2}}{B_{2}}+\frac{C_{2}^{2}n_{1}B_{2 }\tau_{1}^{2}}{B_{1}n_{2}\tau_{2}}+\frac{C_{2}^{2}n_{1}^{3}n_{2}\eta^{2}}{B_{1 }^{3}B_{2}\tau_{2}}.\]

Then

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]\] \[\leq\mathcal{O}\bigg{(}\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{ min}})+\eta+\frac{\tau_{1}^{1/2}}{B_{3}^{1/2}}+\frac{\tau_{1}}{B_{3}}+\frac{\tau_{2}^ {1/2}}{B_{2}^{1/2}}+\frac{\tau_{2}}{B_{2}}\] \[\quad+\frac{n_{1}n_{2}\eta}{B_{1}B_{2}\tau_{1}^{1/2}}+\frac{n_{1}^ {2}n_{2}^{2}\eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}}+\frac{n_{1}^{1/2}B_{2}^{1/2} \tau_{1}}{B_{1}^{1/2}n_{2}^{1/2}\tau_{2}^{1/2}}+\frac{n_{1}^{3/2}n_{2}^{1/2} \eta}{B_{1}^{3/2}B_{2}^{1/2}\tau_{2}^{1/2}}+\frac{n_{1}B_{2}\tau_{1}^{2}}{B_{1} n_{2}\tau_{2}}+\frac{n_{3}^{3}n_{2}\eta^{2}}{B_{1}^{3}B_{2}\tau_{2}}\bigg{)}\] \[\leq\mathcal{O}\bigg{(}\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{ min}})+\frac{\tau_{1}^{1/2}}{B_{3}^{1/2}}+\frac{\tau_{2}^{1/2}}{B_{2}^{1/2}}+ \frac{n_{1}n_{2}\eta}{B_{1}B_{2}\tau_{1}^{1/2}}+\frac{n_{1}^{1/2}B_{2}^{1/2} \tau_{1}}{B_{1}^{1/2}n_{2}^{1/2}\tau_{2}^{1/2}}+\frac{n_{1}^{3/2}n_{2}^{1/2} \eta}{B_{1}^{3/2}B_{2}^{1/2}\tau_{2}^{1/2}}\bigg{)}.\]Setting

\[\tau_{1}=\mathcal{O}\left(\min\{B_{3},\frac{B_{1}^{1/2}n_{2}^{1/2}}{n_ {1}^{1/2}}\}\epsilon^{4}\right),\quad\tau_{2}=\mathcal{O}(B_{2}\epsilon^{4}),\] \[\eta=\mathcal{O}\left(\min\left\{\frac{B_{1}B_{2}}{n_{1}n_{2}} \tau_{1}^{1/2}\epsilon^{2},\frac{B_{1}^{3/2}B_{2}^{1/2}}{n_{1}^{3/2}n_{2}^{1/2} }\tau_{2}^{1/2}\right\}\right)\] \[\quad=\mathcal{O}\left(\min\left\{B_{3}^{1/2},\frac{B_{1}^{1/4}n_ {2}^{1/4}}{n_{1}^{1/4}},\frac{B_{1}^{1/2}n_{2}^{1/2}}{n_{1}^{1/2}}\right\} \frac{B_{1}B_{2}}{n_{1}n_{2}}\epsilon^{4}\right),\]

then with

\[T=\mathcal{O}\left(\max\left\{\frac{1}{B_{3}^{1/2}},\frac{n_{1}^{1/4}}{B_{1}^{ 1/4}n_{2}^{1/4}},\frac{n_{1}^{1/2}}{B_{1}^{1/2}n_{2}^{1/2}}\right\}\frac{n_{1 }n_{2}}{B_{1}B_{2}}\epsilon^{-6}\right),\]

we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]\leq\epsilon^{2}\]

## Appendix B Solving Non-smooth FCCO and TCCO with Coordinate Moving Average

In this section we consider solving non-smooth weakly-convex FCCO and TCCO without variance reduction method. To be specific, we use coordinate moving average updates for function values estimations instead of MSVR. This allows us to weaken the assumption on the Lipschitz continuity, i.e. the Lipschitz continuity of the stochastic function value estimation is not required, and can be replaced by the Lipschitz continuity of the function value. Moreover, compared with MSVR, coordinate moving average update does not need the stochastic evaluation from the previous iteration, and thus has a simpler implementation. However, as a result of not using variance reduction technique, the algorithms suffer from worse convergence rates in terms of \(\epsilon\).

### Solving Non-smooth FCCO with Coordinate Moving Average

We first assume the followings assumptions hold.

**Assumption B.1**.: For all \(i\in\mathcal{S}\), we assume that

* \(f_{i}(\cdot)\) is \(\rho_{f}\)-weakly-convex, \(C_{f}\)-Lipschitz continuous and non-decreasing;
* \(g_{i}(\cdot)\) is \(\rho_{g}\)-weakly-convex and \(C_{g}\)-Lipschitz continuous;
* Stochastic gradient estimators \(g_{i}(\mathbf{w};\xi)\) and \(\partial g_{i}(\mathbf{w};\xi)\) have bounded variance \(\sigma^{2}\).

With coordinate moving average update, we present the following lemma of error bound.

**Lemma B.2**.: _Consider the coordinate moving average update for \(\{u_{i,t}:i\in\mathcal{S}_{1}\}\) in Algorithm 3, assume \(g_{i}(\mathbf{w})\) is \(C_{g}\)-Lipschitz continuous for all \(i\in\mathcal{S}_{1}\) and \(\tau\leq 1\), then we have_

\[\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|]\leq(1-\frac{B_{1}\tau}{4n_{1 }})^{t+1}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|+\frac{2\sqrt{2}\tau^{1/2}\sigma}{B_ {2}^{1/2}}+\frac{4\sqrt{2}n_{1}C_{g}M\eta}{B_{1}\tau},\]

\[\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\leq(1-\frac{B_{1}\tau}{4 n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|^{2}+\frac{8\tau\sigma^{2}}{B_ {2}}+\frac{32n_{1}^{2}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau^{2}}.\]

Then we have a convergence analysis similar to Theorem 4.6.

**Theorem B.3**.: _Consider non-smooth weakly-convex FCCO problem, under Assumption B.1, setting \(\tau=\mathcal{O}(B_{2}\epsilon^{4})\leq 1\), \(\eta=\mathcal{O}(\frac{B_{1}B_{2}}{n_{1}}\epsilon^{6})\), Algorithm 3 converges to an \(\epsilon\)-stationary point of the Moreau envelope \(F_{1/\bar{\rho}}\) in \(T=\mathcal{O}(\frac{n_{1}}{B_{1}B_{2}}\epsilon^{-8})\) iterations._

Proof of Theorem B.3.: Since the only difference between SONX and Algorithm 3 is the update for \(\{u_{i,t}:i\in\mathcal{S}_{1}\}\), the proof of Theorem 4.6 still holds with the error bound replaced by Lemma B.2, i.e.,

\[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t+1}-g_{ i}(\mathbf{w}_{t+1})\|\bigg{]}\leq(1-\mu)^{t+1}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|+R_{1},\] \[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t+1}-g_ {i}(\mathbf{w}_{t+1})\|^{2}\bigg{]}\leq(1-\mu)^{t+1}\frac{1}{n}\sum_{i\in \mathcal{S}}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|^{2}+R_{2},\] \[\mu=\frac{B_{1}\tau}{4n_{1}},\quad R_{1}=\frac{2\sqrt{2}\tau^{1/ 2}\sigma}{B_{2}^{1/2}}+\frac{4\sqrt{2}n_{1}C_{g}M\eta}{B_{1}\tau},\quad R_{2} =\frac{8\tau\sigma^{2}}{B_{2}}+\frac{32n_{1}^{2}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^ {2}\tau^{2}}.\]

Then proof proceeds to

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}] \leq\mathcal{O}\left(\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu})+ \eta+R_{1}+R_{2}\right)\] \[=\mathcal{O}\left(\frac{1}{T}(\frac{1}{\eta}+\frac{n_{1}}{B_{1} \tau})+\eta+\frac{\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{n_{1}\eta}{B_{1}\tau}+ \frac{\tau\sigma^{2}}{B_{2}}+\frac{n_{1}^{2}\eta^{2}}{B_{1}^{2}\tau^{2}}\right).\]

Setting

\[\tau=\mathcal{O}(B_{2}\epsilon^{4}),\quad\eta=\mathcal{O}(\frac{B_{1}B_{2}}{n _{1}}\epsilon^{6}),\]

then to reach a nearly \(\epsilon\)-stationary point, Algorithm 3 needs

\[T=\mathcal{O}(\frac{n_{1}}{B_{1}B_{2}}\epsilon^{-8})\]

iterations. 

### Solving Non-smooth TCCO with Coordinate Moving Average

We first assume the following assumptions hold.

**Assumption B.4**.: For all \((i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\), we assume that

* \(f_{i}(\cdot)\) is \(\rho_{f}\)-weakly-convex, \(C_{f}\)-Lipschitz continuous and non-decreasing;
* \(g_{i}(\cdot)\) is \(\rho_{g}\)-weakly-convex and \(C_{g}\)-Lipschitz continuous. \(h_{i,j}(\cdot)\) is differentiable and \(C_{h}\)-Lipschitz continuous.
* Either \(g_{i}\) is monotone and \(h_{i,j}(\cdot)\) is \(L_{h}\)-smooth, or \(g_{i}\) is non-decreasing and \(h_{i,j}(\cdot)\) is \(L_{h}\)-weakly-convex.
* Stochastic estimators \(h_{i,j}(\mathbf{w},\xi)\), \(\partial h_{i,j}(\mathbf{w},\xi)\) and \(g_{i}(v_{i,j})\) have bounded variance \(\sigma^{2}\), and \(\|h_{i,j}(\mathbf{w})\|\leq\tilde{C}_{h}\).

With coordinate moving average update, we present the following lemmas of error bounds.

**Lemma B.5**.: _Consider the coordinate moving average update for \(\{v_{i,j,t}:(i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\}\) in Algorithm 4, assume \(h_{i,j}(\mathbf{w})\) is \(C_{h}\)-Lipschitz continuous for all \((i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\) and \(\tau_{1}\leq 1\), then we have_

\[\mathbb{E}\left[\frac{1}{n_{1}n_{2}}\sum_{i\in\mathcal{S}_{1}}\sum _{j\in\mathcal{S}_{2}}\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|\right]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{4n_{1}n_{2}})^{t+1}\frac{1}{n_{1 }n_{2}}\sum_{i\in\mathcal{S}_{1}}\sum_{j\in\mathcal{S}_{2}}\|v_{i,j,0}-h_{i,j} (\mathbf{w}_{0})\|+\frac{2\sqrt{2}\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{4 \sqrt{2}n_{1}n_{2}C_{h}M\eta}{B_{1}B_{2}\tau_{1}},\] \[\mathbb{E}\left[\frac{1}{n_{1}n_{2}}\sum_{i\in\mathcal{S}_{1}} \sum_{j\in\mathcal{S}_{2}}\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|^{2}\right]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{4n_{1}n_{2}})^{2(t+1)}\frac{1}{ n_{1}n_{2}}\sum_{i\in\mathcal{S}_{1}}\sum_{j\in\mathcal{S}_{2}}\|v_{i,j,0}-h_{i,j} (\mathbf{w}_{0})\|^{2}+\frac{8\tau_{1}\sigma^{2}}{B_{3}}+\frac{32n_{1}^{2}n_{ 2}^{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}^{2}}.\]

**Lemma B.6**.: _Consider the coordinate moving average update for \(\{u_{i,t}:i\in\mathcal{S}_{1}\}\) in Algorithm 4, assume \(g_{i}(\cdot)\) is \(C_{g}\)-Lipschitz continuous for all \(i\in\mathcal{S}_{1}\) and \(\tau_{2}\leq 1\), then we have_

\[\mathbb{E}\left[\frac{1}{n_{1}}\sum_{i\in\mathcal{S}_{1}}\|u_{i,t +1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t+1})\|\right]\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{t+1}\frac{1}{n_{1}}\sum_{i \in\mathcal{S}_{1}}\|u_{i,0}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v _{i,j,0})\|+\frac{2\sqrt{2}\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4\sqrt{2}C _{g}Mn_{1}^{1/2}B_{2}^{1/2}\tau_{1}}{B_{1}^{1/2}n_{2}^{1/2}\tau_{2}},\] \[\mathbb{E}\left[\frac{1}{n_{1}}\sum_{i\in\mathcal{S}_{1}}\|u_{i,t +1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t+1})\|^{2}\right]\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2(t+1)}\frac{1}{n_{1}}\sum_ {i\in\mathcal{S}_{1}}\|u_{i,0}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}( v_{i,j,0})\|^{2}+\frac{8\tau_{2}\sigma^{2}}{B_{2}}+\frac{32C_{g}^{2}M^{2}n_{1}B_{2} \tau_{1}^{2}}{B_{1}n_{2}\tau_{2}^{2}}.\]

Then we have a convergence analysis similar to Theorem A.2.

**Theorem B.7**.: _Consider non-smooth weakly-convex TCCO problem, under Assumption B.4, setting \(\tau_{1}=\mathcal{O}\left(\min\left\{B_{3}\epsilon^{4},\frac{B_{1}^{1/2}n_{2} ^{1/2}}{n_{1}^{1/2}B_{2}^{1/2}}B_{2}\epsilon^{6}\right\}\right)\leq 1\), \(\tau_{2}=\mathcal{O}(B_{2}\epsilon^{4})\leq 1\), \(\eta=\mathcal{O}\left(\min\left\{B_{3}\epsilon^{4},\frac{B_{1}^{1/2}n_{2}^{1/2} }{n_{1}^{1/2}B_{2}^{1/2}}B_{2}\epsilon^{6}\right\}\frac{B_{1}B_{2}}{n_{1}n_{2} }\epsilon^{2}\right)\), Algorithm 4 converges to an \(\epsilon\)-stationary point of the Moreau envelope \(F_{1/\bar{\rho}}\) in \(T=\mathcal{O}\left(\max\left\{\frac{1}{B_{3}},\frac{n_{1}^{1/2}}{B_{1}^{1/2}B_{ 2}^{1/2}n_{2}^{1/2}}\epsilon^{-2}\right\}\frac{n_{1}n_{2}}{B_{1}B_{2}} \epsilon^{-8}\right)\) iterations._Proof of Theorem b.7.: Since the only difference between SONT and Algorithm 4 is the update for \(\{u_{i,t}:i\in\mathcal{S}_{1}\}\) and \(\{v_{i,j,t}:(i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\}\), the proof of Theorem A.2 still holds with the error bound replaced by Lemma B.5 and Lemma B.6, i.e.,

\[\frac{1}{n_{1}n_{2}}\sum_{i\in S_{1}}\sum_{j\in S_{2}}\mathbb{E}[ \|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|]\leq(1-\mu_{1})^{t}\frac{1}{n_{1}n_{2}} \sum_{i\in S_{1}}\sum_{j\in S_{2}}\|h_{i,j}(\mathbf{w}_{0})-v_{i,j,0}\|+R_{1},\] \[\frac{1}{n_{1}n_{2}}\sum_{i\in S_{1}}\sum_{j\in S_{2}}\mathbb{E}[ \|h_{i,j}(\mathbf{w}_{t})-v_{i,j,t}\|^{2}]\leq(1-\mu_{1})^{t}\frac{1}{n_{1}n_{ 2}}\sum_{i\in S_{1}}\sum_{j\in S_{2}}\|h_{i,j}(\mathbf{w}_{0})-v_{i,j,0}\|^{2} +R_{2},\] \[\frac{1}{n_{1}}\sum_{i\in S_{1}}\mathbb{E}\bigg{[}\bigg{\|}\frac{ 1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}\bigg{]}\leq(1-\mu_ {2})^{t}\frac{1}{n_{+}}\sum_{i\in S_{+}}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S_{ 2}}g_{i}(v_{i,j,0})-u_{i,0}\bigg{\|}+R_{3},\] \[\frac{1}{n_{1}}\sum_{i\in S_{1}}\mathbb{E}\bigg{[}\bigg{\|}\frac{ 1}{n_{2}}\sum_{j\in S_{2}}g_{i}(v_{i,j,t})-u_{i,t}\bigg{\|}^{2}\bigg{]}\leq(1- \mu_{2})^{t}\frac{1}{n_{+}}\sum_{i\in S_{+}}\bigg{\|}\frac{1}{n_{2}}\sum_{j\in S _{2}}g_{i}(v_{i,j,0})-u_{i,0}\bigg{\|}^{2}+R_{4},\]

with

\[\mu_{1}=\frac{B_{1}B_{2}\tau_{1}}{4n_{1}n_{2}},\quad\mu_{2}=\frac {B_{1}\tau_{2}}{4n_{1}},\quad R_{1}=\frac{2\sqrt{2}\tau_{1}^{1/2}\sigma}{B_{3} ^{1/2}}+\frac{4\sqrt{2}n_{1}n_{2}C_{h}M\eta}{B_{1}B_{2}\tau_{1}}\] \[R_{2}=\frac{8\tau_{1}\sigma^{2}}{B_{3}}+\frac{32n_{1}^{2}n_{2}^{ 2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}^{2}},\quad R_{3}=\frac{2 \sqrt{2}\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4\sqrt{2}C_{g}Mn_{1}^{1/2}B_{ 2}^{1/2}\tau_{1}}{B_{1}^{1/2}n_{2}^{1/2}\tau_{2}},\] \[R_{4}=\frac{8\tau_{2}\sigma^{2}}{B_{2}}+\frac{32C_{g}^{2}M^{2}n_ {1}B_{2}\tau_{1}^{2}}{B_{1}n_{2}\tau_{2}^{2}}\]

Then the proof proceeds to

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t})\|^{2}]\] \[\leq\mathcal{O}\left(\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{ min}})+\eta+R_{1}+R_{2}+R_{3}+R_{4}\right)\] \[\leq\mathcal{O}\bigg{(}\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{ min}})+\eta+\frac{\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{\tau_{1}\sigma^{2}}{B_{ 3}}+\frac{\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+\frac{\tau_{2}\sigma^{2}}{B_{2}} +\frac{n_{1}n_{2}\eta}{B_{1}B_{2}\tau_{1}}+\frac{n_{1}^{2}n_{2}^{2}\eta^{2}}{B_ {1}^{2}B_{2}^{2}\tau_{1}^{2}}\] \[\quad+\frac{n_{1}^{1/2}B_{2}^{1/2}\tau_{1}}{B_{1}^{1/2}n_{2}^{1/2} \tau_{2}}+\frac{n_{1}B_{2}\tau_{1}^{2}}{B_{1}n_{2}\tau_{2}^{2}}\bigg{)}\] \[\leq\mathcal{O}\bigg{(}\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{ min}})+\frac{\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+ \frac{n_{1}n_{2}\eta}{B_{1}B_{2}\tau_{1}}+\frac{n_{1}^{1/2}B_{2}^{1/2}\tau_{1}}{B _{1}^{1/2}n_{2}^{1/2}\tau_{2}}\bigg{)}.\]

Setting

\[\tau_{1}=\mathcal{O}\left(\min\left\{B_{3}\epsilon^{4},\frac{B_{1} ^{1/2}n_{2}^{1/2}}{n_{1}^{1/2}B_{2}^{1/2}}B_{2}\epsilon^{6}\right\}\right), \quad\tau_{2}=\mathcal{O}(B_{2}\epsilon^{4}),\] \[\eta=\mathcal{O}\left(\min\left\{B_{3}\epsilon^{4},\frac{B_{1} ^{1/2}n_{2}^{1/2}}{n_{1}^{1/2}B_{2}^{1/2}}B_{2}\epsilon^{6}\right\}\frac{B_{1}B_{ 2}}{n_{1}n_{2}}\epsilon^{2}\right),\]

then to reach a nearly \(\epsilon\)-stationary point, Algorithm 4 need

\[T=\mathcal{O}\left(\max\left\{\frac{1}{B_{3}},\frac{n_{1}^{1/2}}{B_{1}^{1/2}B_{2 }^{1/2}n_{2}^{1/2}}\epsilon^{-2}\right\}\frac{n_{1}n_{2}}{B_{1}B_{2}}\epsilon^{ -10}\right)\]

iterations.

Details for TPAUC Maximization

### Assumption Verification

We first present two lemmas about the weak convexity of the objective in the regular learning setting and in the multi-instance learning setting with mean pooling.

**Lemma C.1**.: _Consider the formulation in problem (9) in the regular learning setting and assume that function \(\ell(\cdot)\) is non-decreasing, \(C_{\ell}\) Lipschitz continuous and \(\rho_{\ell}\)-weakly-convex, and function \(h_{\mathbf{w}}(X_{i})\) is \(C_{h}\) Lipschitz continuous and \(\rho_{h}\)-weakly-convex. then the following statements are true:_

* \(f_{i}(g,s^{\prime})\) _is convex and_ \(C_{f}\)_-Lipschitz continuous w.r.t._ \((g,s^{\prime})\)_, and non-decreasing w.r.t._ \(g\)_._
* \(\psi_{i}(\mathbf{w},s_{i})\) _is_ \(\rho_{\psi}\)_-weakly-convex w.r.t._ \((\mathbf{w},s_{i})\)_, and the stochastic estimator of the finite sum function value_ \(\psi_{i}(\mathbf{w},s_{i})\) _is_ \(C_{\psi}\)_-Lipschitz continuous w.r.t._ \((\mathbf{w},s_{i})\)_._
* \(\frac{1}{n_{+}}\sum_{i\in\mathcal{S}_{+}}f_{i}(\psi_{i}(\mathbf{w},s_{i}),s^{ \prime})\) _is_ \(\rho_{F}\)_-weakly-convex w.r.t._ \((\mathbf{w},\mathbf{s},s^{\prime})\)_._

**Lemma C.2**.: _Consider the formulation in problem (9) in the multi-instance learning setting with mean pooling, and assume that function \(h_{i}(\mathbf{w})=\frac{1}{|X_{i}|}\sum_{\mathbf{x}\in X_{i}}e(\mathbf{w}_{e}; \mathbf{x})^{\top}\mathbf{w}_{c}\) is \(\tilde{L}_{h}\)-smooth and is bounded by \(\tilde{C}_{h}\), and \(h_{i}(\mathbf{w};\xi)=e(\mathbf{w}_{e};\xi)^{\top}\mathbf{w}_{c}\) is \(C_{h}\)-Lipschitz continuous and has bounded variance \(\sigma^{2}\), \(\ell\) is non-decreasing and \(L_{\ell}\)-weakly-convex, then the followings are true:_

* \(f_{i}(g,s^{\prime})\) _is convex and_ \(C_{f}\)_-Lipschitz-continuous w.r.t._ \((g,s^{\prime})\)_, and non-decreasing w.r.t._ \(g\)_;_
* \(g_{i}(v,s_{i})=s_{i}+\frac{(\ell(v)-s_{i})_{+}}{\beta}\) _is_ \(\rho_{g}\)_-weakly convex and non-decreasing w.r.t._ \(v\)_, convex w.r.t._ \(s_{i}\)_, and_ \(C_{g}\)_-Lipschitz continuous w.r.t._ \((v,s_{i})\)_;_
* \(h_{i,j}(\mathbf{w})=h_{j}(\mathbf{w})-h_{i}(\mathbf{w})\) _is_ \(L_{h}\)_-weakly-convex, and_ \(h_{i,j}(\mathbf{w};\xi,\zeta)\) _is_ \(C_{h}\)_-Lipschitz continuous;_
* \(\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}}f_{i}(g_{i}(h_{i,j}(\mathbf{w}), s_{i}),s^{\prime})\) _is_ \(\rho_{F}\)_-weakly-convex w.r.t._ \((\mathbf{w},\mathbf{s},s^{\prime})\)_._

#### c.1.1 Proof of Lemma c.1

Proof of Lemma c.1.: The convexity of \(f_{i}(g,s^{\prime})\) with respect to \((g,s^{\prime})\) follows from the convexity definition. With subgradients \(\partial_{s^{\prime}}f_{i}(g,s^{\prime})\in[1-\frac{1}{\alpha},1]\), \(\partial_{g}f_{i}(g,s^{\prime})\in[0,\frac{1}{\alpha}]\), we can see that \(f_{i}(g,s^{\prime})\) is \(\frac{1}{\alpha}\)-Lipschitz continuous w.r.t. \((g,s^{\prime})\), and non-decreasing w.r.t. \(u\).

We first show that \(\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))\) is weakly-convex w.r.t. \(\mathbf{w}\).

\[\ell(h_{\tilde{\mathbf{w}}}(X_{j})-h_{\mathbf{w}}(X_{i}))\] \[\geq\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))+\langle \partial\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i})),(h_{\tilde{\mathbf{ w}}}(X_{j})-h_{\tilde{\mathbf{w}}}(X_{i}))-(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))\rangle\] \[\quad+\frac{\rho_{\ell}}{2}\|(h_{\tilde{\mathbf{w}}}(X_{j})-h_{ \tilde{\mathbf{w}}}(X_{i}))-(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))\|^{2}\] \[\stackrel{{(a)}}{{\geq}}\ell(h_{\mathbf{w}}(X_{j})-h_{ \mathbf{w}}(X_{i}))+\langle\partial\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X _{i})),\langle\nabla h_{\mathbf{w}}(X_{j})-\nabla h_{\mathbf{w}}(X_{i}), \tilde{\mathbf{w}}-\mathbf{w}\rangle\rangle\] \[\quad+2\rho_{\ell}C_{h}^{2}\|\tilde{\mathbf{w}}-\mathbf{w}\|^{2}\]

where (a) uses the weak-convexity of \(h_{\mathbf{w}}(X_{i})\) and \(h_{\mathbf{w}}(X_{j})\),

\[h_{\tilde{\mathbf{w}}}(X_{j})-h_{\mathbf{w}}(X_{j})\geq\langle\nabla h_{ \mathbf{w}}(X_{j}),\tilde{\mathbf{w}}-\mathbf{w}\rangle-\frac{\rho_{h}}{2}\| \tilde{\mathbf{w}}-\mathbf{w}\|^{2},\]

\[-h_{\tilde{\mathbf{w}}}(X_{i})+h_{\mathbf{w}}(X_{i})\geq-\langle\nabla h_{ \mathbf{w}}(X_{i}),\tilde{\mathbf{w}}-\mathbf{w}\rangle+\frac{\rho_{h}}{2}\| \tilde{\mathbf{w}}-\mathbf{w}\|^{2}.\]

Thus \(\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))\) is \(4\rho_{\ell}C_{h}^{2}\)-weakly-convex w.r.t. \(\mathbf{w}\).

By convexity of \((\ell,s_{i})\mapsto s_{i}+\frac{(\ell-s_{i})_{+}}{\beta}\), we have

\[\psi_{i}(\tilde{\mathbf{w}},\tilde{s}_{i})\] \[\geq\psi_{i}(\mathbf{w},s_{i})+\langle\partial_{\ell}\psi_{i}( \mathbf{w},s_{i}),\ell(h_{\tilde{\mathbf{w}}}(X_{j})-h_{\tilde{\mathbf{w}}}(X_{ i}))-\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))\rangle+\langle\partial_{s_{i}} \psi_{i}(\mathbf{w},s_{i}),\tilde{s}_{i}-s_{i}\rangle\] \[\stackrel{{(a)}}{{\geq}}\psi_{i}(\mathbf{w},s_{i})+ \partial_{\ell}\psi_{i}(\mathbf{w},s_{i})\bigg{[}\partial\ell(h_{\mathbf{w}}(X_ {j})-h_{\mathbf{w}}(X_{i}))\langle\nabla h_{\mathbf{w}}(X_{j})-\nabla h_{ \mathbf{w}}(X_{i}),\tilde{\mathbf{w}}-\mathbf{w}\rangle\big{)}-2\rho_{\ell}C _{h}^{2}\|\tilde{\mathbf{w}}-\mathbf{w}\|^{2}\bigg{]}\] \[\quad+\langle\partial_{s_{i}}\psi_{i}(\mathbf{w},s_{i}),\tilde{s }_{i}-s_{i}\rangle\] \[\stackrel{{(b)}}{{\geq}}\psi_{i}(\mathbf{w},s_{i})+ \partial_{\ell}\psi_{i}(\mathbf{w},s_{i})\partial\ell(h_{\mathbf{w}}(X_{j})-h_ {\mathbf{w}}(X_{i}))\langle\nabla h_{\mathbf{w}}(X_{j})-\nabla h_{\mathbf{w}}( X_{i}),\tilde{\mathbf{w}}-\mathbf{w}\rangle\big{)}\] \[\quad+\langle\partial_{s_{i}}\psi_{i}(\mathbf{w},s_{i}),\tilde{s }_{i}-s_{i}\rangle-\frac{2\rho_{\ell}C_{h}^{2}}{\beta}\|\tilde{\mathbf{w}}- \mathbf{w}\|^{2}\]

where (a) follows from the monotonicity of \(\psi_{i}\) w.r.t. \(\ell\) and weak-convexity of \(\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))\), and (b) is due to the Lipschitz continuity of \((\ell,s_{i})\mapsto s_{i}+\frac{(\ell-s_{i})_{+}}{\beta}\) w.r.t. \(\ell\). Thus \(\psi_{i}\) is \(\frac{4\rho_{\ell}C_{h}^{2}}{\beta}\)-weakly-convex w.r.t. \((\mathbf{w},s_{i})\).

With a similar argument using the convexity and Lipschitz continuity of \(f_{i}(g,s^{\prime})\) w.r.t. \((g,s^{\prime})\) and the weak-convexity of \(\psi_{i}(\mathbf{w},s_{i})\), we can show that \(f_{i}(\psi_{i}(\mathbf{w},s_{i}),s^{\prime})\) is \(\frac{4\rho_{\ell}C_{h}^{2}}{\beta}\)-weakly-convex w.r.t. \((\mathbf{w},s_{i},s^{\prime})\). Thus, \(F(\mathbf{w},s_{i},s^{\prime})\) is \(\rho_{F}=\frac{4\rho_{\ell}C_{h}^{2}}{\beta}\)-weakly-convex w.r.t. \((\mathbf{w},\mathbf{s},s^{\prime})\).

Now we show the Lipschitz continuity of \(\psi_{i}(\mathbf{w},s_{i};X_{j})\), i.e. an unbiased stochastic estimator of \(\psi_{i}(\mathbf{w},s_{i})\). We have

\[\|\psi_{i}(\mathbf{w},s_{i};X_{j})-\psi_{i}(\tilde{\mathbf{w}}, \tilde{s}_{i};X_{j})\|^{2}\] \[=\bigg{\|}(s_{i}+\frac{(\ell(h_{\mathbf{w}}(X_{j})-h_{\mathbf{w}} (X_{i}))-s_{i})_{+}}{\beta})-(\tilde{s}_{i}+\frac{(\ell(h_{\tilde{\mathbf{w}}}( X_{j})-h_{\tilde{\mathbf{w}}}(X_{i}))-\tilde{s}_{i})_{+}}{\beta})\bigg{\|}^{2}\] \[\leq 2\|s_{i}-\tilde{s}_{i}\|^{2}+2\bigg{\|}\frac{(\ell(h_{ \mathbf{w}}(X_{j})-h_{\mathbf{w}}(X_{i}))-s_{i})_{+}}{\beta}-\frac{(\ell(h_{ \tilde{\mathbf{w}}}(X_{j})-h_{\tilde{\mathbf{w}}}(X_{i}))-\tilde{s}_{i})_{+}}{ \beta}\bigg{\|}^{2}\] \[\leq 2\|s_{i}-\tilde{s}_{i}\|^{2}+\frac{2}{\beta^{2}}(8C_{\ell}^{ 2}\ell_{h}^{2}\|\tilde{\mathbf{w}}-\mathbf{w}\|^{2}+2\|\tilde{s}_{i}-s_{i}\|^ {2})\] \[\leq(2+\frac{4+16C_{\ell}^{2}C_{h}^{2}}{\beta^{2}})(\|\tilde{ \mathbf{w}}-\mathbf{w}\|^{2}+\|\tilde{s}_{i}-s_{i}\|^{2}).\]

Thus \(\psi_{i}(\mathbf{w},s_{i};X_{j})\) is \((2+\frac{4+16C_{\ell}^{2}C_{h}^{2}}{\beta^{2}})^{1/2}\)-Lipschitz continuous w.r.t. \((\mathbf{w},s_{i})\). 

#### c.1.2 Proof of Lemma c.2

Proof of Lemma c.2.: First of all, the convexity of \(f_{i}(u,s^{\prime})\) w.r.t. \((u,s^{\prime})\) and the convexity of \(g_{i}(v_{ij},s_{i})\) w.r.t. \((\ell,s_{i})\) directly follows from the convexity definition. Moreover, one can see from the formulation that \(\partial_{s^{\prime}}f_{i}(g,s^{\prime})\in[1-\frac{1}{\alpha},1]\), \(\partial_{u}f_{i}(g,s^{\prime})\in[0,\frac{1}{\alpha}]\), \(\partial_{t}g_{i}(v_{ij},s_{i})\in[1-\frac{1}{\beta},1]\), \(\partial_{s_{i}}g_{i}(v_{ij},s_{i})\in[0,\frac{1}{\beta}]\). Thus \(f_{i}\) is \(C_{f}=\frac{1}{\alpha}\)-Lipschitz continuous w.r.t. \((u,s^{\prime})\) and non-decreasing w.r.t. \(u\), \(g_{i}\) is \(\frac{1}{\beta}\)-Lipschitz continuous w.r.t. \((\ell,s_{i})\) and non-decreasing w.r.t. \(\ell\). Since \(\ell(\cdot)\) is non-decreasing, \(g_{i}(v_{ij},s_{i})\) is non-decreasing w.r.t. \(v_{ij}\). As a result of Proposition 4.2, \(g_{i}(v_{ij},s_{i})\) is \(\rho_{g}=\frac{1}{\beta}L_{\ell}\)-weakly-convex w.r.t. \(v_{ij}\). Due to the composition structure and the Lipschitz continuity of \(g_{i}\) and \(\ell\), one can see that \(g_{i}(v_{ij},s_{i})\) is \(C_{g}=\frac{1}{\beta}C_{\ell}\)-Lipschitz continuous w.r.t. \((v_{ij},s_{i})\).

The \(L_{h}=2\tilde{L}_{h}\)-weakly-convexity of \(h_{i,j}(\mathbf{w})\) and \(C_{h}=2\tilde{C}_{h}\)-Lipschitz continuity of \(h_{i,j}(\mathbf{w};\xi,\zeta)\) directly follows from the \(\tilde{L}_{h}\)-smoothness of \(h_{i}(\mathbf{w})\) and \(\tilde{C}_{h}\)-Lipschitz continuity of \(h_{i}(\mathbf{w};\xi)\). Finally,we show the weakly-convexity of \(f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s^{\prime})\):

\[f_{i}(g_{i}(h_{i,j}(\tilde{\mathbf{w}}),\tilde{s}_{i})\tilde{s}^{ \prime})\] \[\stackrel{{(a)}}{{\geq}}f_{i}(g_{i}(h_{i,j}(\mathbf{ w}),s_{i}),s^{\prime})+\langle\beta_{s^{\prime}}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s^{ \prime}),\tilde{s}^{\prime}-s^{\prime}\rangle\] \[\quad+\langle\partial_{u}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s ^{\prime}),g_{i}(h_{i,j}(\tilde{\mathbf{w}}),\tilde{s}_{i})-g_{i}(h_{i,j}( \mathbf{w}),s_{i})\rangle\] \[\stackrel{{(b)}}{{\geq}}f_{i}(g_{i}(h_{i,j}(\mathbf{ w}),s_{i}),s^{\prime})+\langle\beta_{s^{\prime}}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s^{ \prime}),\tilde{s}^{\prime}-s^{\prime}\rangle\] \[\quad+\langle\partial_{u}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s ^{\prime}),\langle\partial_{\ell}g_{i}(h_{i,j}(\mathbf{w}),s_{i}),\ell(h_{i,j} (\tilde{\mathbf{w}}))-\ell(h_{i,j}(\mathbf{w}))\rangle\rangle\] \[\quad+\langle\partial_{u}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i})s ^{\prime}),\langle\partial_{s_{i}}g_{i}(h_{i,j}(\mathbf{w}),s_{i}),\tilde{s} _{i}-s_{i}\rangle\rangle\] \[\stackrel{{(c)}}{{\geq}}f_{i}(g_{i}(h_{i,j}(\mathbf{ w}),s_{i}),s^{\prime})+\langle\beta_{s^{\prime}}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s^{ \prime}),\tilde{s}^{\prime}-s^{\prime}\rangle\] \[\quad+\langle\partial_{u}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s ^{\prime})\partial_{\ell}g_{i}(h_{i,j}(\mathbf{w}),s_{i})\partial\ell(h_{i,j} (\mathbf{w})),h_{i,j}(\tilde{\mathbf{w}})-h_{i,j}(\mathbf{w})\rangle\] \[\quad-\frac{C_{f}C_{g}L_{\ell}}{2}\|h_{i,j}(\tilde{\mathbf{w}})- h_{i,j}(\mathbf{w})\|^{2}+\langle\partial_{u}f_{i}(s^{\prime},g_{i}(h_{i,j}( \mathbf{w}),s_{i}))\partial_{s_{i}}g_{i}(h_{i,j}(\mathbf{w}),s_{i}),\tilde{s} _{i}-s_{i}\rangle\] \[\stackrel{{(d)}}{{\geq}}f_{i}(g_{i}(h_{i,j}(\mathbf{ w}),s_{i}),s^{\prime})+\langle\partial_{s^{\prime}}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s^{ \prime}),\tilde{s}^{\prime}-s^{\prime}\rangle\] \[\quad+\langle\partial_{u}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s ^{\prime})\partial_{\ell}g_{i}(h_{i,j}(\mathbf{w}),s_{i})\partial\ell(h_{i,j} (\mathbf{w}))\nabla h_{i,j}(\mathbf{w}),\tilde{\mathbf{w}}-\mathbf{w}\rangle\] \[\quad+\langle\partial_{u}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s ^{\prime})\partial_{s_{i}}g_{i}(h_{i,j}(\mathbf{w}),s_{i}),\tilde{s}_{i}-s_{i} \rangle-(\frac{C_{f}C_{g}C_{h}^{2}L_{\ell}}{2}+\frac{C_{f}C_{g}L_{h}}{2})\| \tilde{\mathbf{w}}-\mathbf{w}\|^{2}\]

where (a) uses the convexity of \(f_{i}\), (b) uses the monotonicity of \(f_{i}\) w.r.t. \(u\) and convexity of \(g_{i}(\ell,s_{i})\) w.r.t. \((\ell,s_{i})\), (c) uses monotonicity of \(f_{i}\) w.r.t. \(u\), monotonicity of \(g_{i}\) w.r.t. \(\ell\) and \(L_{\ell}\)-weak-convexity of \(\ell\), (d) uses the smoothness of \(h_{i,j}\). Thus \(f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s^{\prime})\) is \(\rho_{F}=(C_{f}C_{g}C_{h}^{2}L_{\ell}+C_{f}C_{g}L_{h})\)-weakly-convex w.r.t. \((\mathbf{w},s_{i},s^{\prime})\). Therefore, \(\frac{1}{n_{+}}\sum_{i\in\mathcal{S}+}f_{i}(g_{i}(h_{i,j}(\mathbf{w}),s_{i}),s ^{\prime})\) is \(\rho_{F}\)-weakly-convex w.r.t. \((\mathbf{w},\mathbf{s},s^{\prime})\). 

### Algorithms for TPAUC and Multi-instance TPAUC Maximization

```
1:Initialization: \(\mathbf{w}_{0},\{u_{i,0}:i\in\mathcal{S}_{+}\},\{s_{i,0}:i\in\mathcal{S}_{+}\},s_{0}^ {\prime}\)
2:for\(t=0,\dots,T-1\)do
3: Sample batches \(\mathcal{B}_{1}^{t}\subset S_{+}\) and \(\mathcal{B}_{2}^{t}\subset S_{-}\).
4:\(u_{i,t+1}=\begin{cases}(1-\tau)u_{i,t}+\tau\psi_{i}(\mathbf{w}_{t},s_{i,t}; \mathcal{B}_{2}^{t})+\gamma(\psi_{i}(\mathbf{w}_{t},s_{i,t};\mathcal{B}_{2}^{t} )-\psi_{i}(\mathbf{w}_{t-1},s_{i,t-1};\mathcal{B}_{2}^{t})),\,i\in\mathcal{B}_{1 }^{t}\\ u_{i,t},\quad i\not\in\mathcal{B}_{1}^{t}\end{cases}\)
5:\(s_{i,t+1}=\begin{cases}s_{i,t}-\eta\frac{1}{B_{1}}\partial_{s}\psi_{i}(\mathbf{ w}_{t},s_{i,t};\mathcal{B}_{2}^{t})\partial_{u}f(u_{i,t},s_{t}^{\prime}),\quad i\in \mathcal{B}_{1}^{t}\\ s_{i,t},\quad i\not\in\mathcal{B}_{1}^{t}\end{cases}\)
6:\(s_{t+1}^{\prime}=s_{t}^{\prime}-\eta\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{1}^{t}} \partial_{s^{\prime}}f(u_{i,t},s_{t}^{\prime})\)
7: Compute \(G_{t}=\frac{1}{B_{1}}\sum_{i\in\mathcal{B}_{1}^{t}}\partial_{w}\psi_{i}(\mathbf{ w}_{t},s_{i,t};\mathcal{B}_{2}^{t})\partial_{u}f(u_{i,t},s_{t}^{\prime})\)
8: Update \(\mathbf{w}_{t+1}=\mathbf{w}_{t}-\eta G_{t}\)
9:endfor
10:return\(\mathbf{w}_{\bar{t}}\) with \(\bar{t}\) uniformly sampled from \(\{0,\dots,T-1\}\). ```

**Algorithm 5** SONX for TPAUC

### TPAUC in MIL with smoothed-max pooling and attention-based pooling

We can extend our results to smoothed-max pooling and attention-based pooling.

**Smoothed-max Pooling.** The smoothed-max pooling can be written as [45]:

\[h_{\mathbf{w}}(X)=\tau\log\left(\frac{1}{|X|}\sum_{\mathbf{x}\in X}\exp(\phi( \mathbf{w};\mathbf{x})/\tau)\right),\] (24)

where \(\tau>0\) is a hyperparameter and \(\phi(\mathbf{w};\mathbf{x})=e(\mathbf{w}_{e},\mathbf{x})^{\top}\mathbf{w}_{c}\) is the prediction score for instance \(\mathbf{x}\). We can see that \(h_{\mathbf{w}}(X)\) itself is a compositional function. To map the problem into TCCO, we define \(h_{i}(\mathbf{w})=\frac{1}{|X_{i}|}\sum_{\mathbf{x}\in X_{i}}\exp(\phi(\mathbf{ w};\mathbf{x})/\tau)+C\), where \(C>0\) is a constant. Then the objective function becomes

\[\begin{split}&\min_{\mathbf{w},s^{\prime},\mathbf{s}}\frac{1}{n_{+}} \sum_{X_{i}\in\mathcal{S}_{+}}f_{i}(\psi_{i}(\mathbf{w},s_{i}),s^{\prime}),\\ &\text{where }f_{i}(g,s^{\prime})=s^{\prime}+\frac{(g-s^{\prime})_{+} }{\alpha},\\ &\psi_{i}(\mathbf{w},s_{i})=\frac{1}{n_{-}}\sum_{X_{j}\in \mathcal{S}_{-}}s_{i}+\frac{(\ell(\tau\log h_{j}(\mathbf{w})-\tau\log h_{i}( \mathbf{w}))-s_{i})_{+}}{\beta},\end{split}\] (25)

In this case we define \(g_{i}(\ell(\mathbf{v}),s_{i})=s_{i}+\frac{(\ell(\tau\log v_{1}-\tau\log v_{2}) -s_{i})_{+}}{\beta}\) and \(h_{i,j}(\mathbf{w})=[h_{i}(\mathbf{w}),h_{j}(\mathbf{w})]\). We can still prove that \(g_{i}(\ell(\mathbf{v}),s_{i})\) is monotone w.r.t to each component of \(\mathbf{v}\). It is not difficult to prove that \(\ell(\tau\log v_{1}-\tau\log v_{2})\) is weakly convex w.r.t \(\mathbf{v}\) because \(\tau\log v_{1}-\tau\log v_{2}\) is a smooth mapping of \(\mathbf{v}\) due to \(\mathbf{v}\geq C\) and \(\ell\) is a convex function [8]. As a result, since \(g_{i}(\ell,s_{i})\) is non-decreasing and convex w.r.t to \(\ell\), it is easy to prove that \(g_{i}(\ell(\mathbf{v}),s_{i})\) is weakly convex w.r.t \(\mathbf{v}\) and is monotone (either non-decreasing or non-increasing) w.r.t to each component of \(\mathbf{v}\). Hence, assuming \(h_{i}(\mathbf{w})\) is a smooth and Lipchitz continuous function, we can prove that \(g_{i}(h_{i,j}(\mathbf{w}),s_{i})\) is weakly convex w.r.t. to \(\mathbf{w}\).

**Attention-based Pooling.** Attention-based pooling was recently introduced for deep MIL [14], which aggregates the feature representations using attention, i.e.,

\[E(\mathbf{w};X)=\sum_{\mathbf{x}\in X}\frac{\exp(g(\mathbf{w};\mathbf{x}))}{ \sum_{\mathbf{x}^{\prime}\in X}\exp(g(\mathbf{w};\mathbf{x}^{\prime}))}e( \mathbf{w}_{e};\mathbf{x})\] (26)where \(g(\mathbf{w};\mathbf{x})\) is a parametric function, e.g., \(g(\mathbf{w};\mathbf{x})=\mathbf{w}_{a}^{\top}\text{tanh}(Ve(\mathbf{w}_{e}; \mathbf{x}))+C\), where \(V\in\mathbb{R}^{m\times d_{o}}\) and \(\mathbf{w}_{a}\in\mathbb{R}^{m}\). Based on the aggregated feature representation, the bag level prediction can be computed by

\[h_{\mathbf{w}}(\mathbf{w},X) =(\mathbf{w}_{c}^{\top}E(\mathbf{w};X))\] (27) \[=\left(\sum_{\mathbf{x}\in X}\frac{\exp(g(\mathbf{w};\mathbf{x}) )\delta(\mathbf{w};\mathbf{x})}{\sum_{\mathbf{x}^{\prime}\in X}\exp(g(\mathbf{ w};\mathbf{x}^{\prime}))}\right),\]

where \(\delta(\mathbf{w};\mathbf{x})=\mathbf{w}_{c}^{\top}e(\mathbf{w}_{e};\mathbf{x})\).

We can see that \(h_{\mathbf{w}}(X)\) itself is a compositional function. To map the problem into TCCO, we define \(h_{i}^{1}(\mathbf{w})=\frac{1}{|X_{i}|}\sum_{\mathbf{x}\in X_{i}}\exp(g( \mathbf{w};\mathbf{x}))\delta(\mathbf{w};\mathbf{x})\), and \(h_{i}^{2}(\mathbf{w})=\frac{1}{|X_{i}|}\sum_{\mathbf{x}^{\prime}\in X_{i}}\exp (g(\mathbf{w};\mathbf{x}^{\prime}))\). Assume \(|\mathbf{w}_{a}^{\top}\text{tanh}(Ve(\mathbf{w}_{e};\mathbf{x}))|\leq C_{b}\) then \(h_{i}^{2}(\mathbf{w})\geq\exp(C-C_{b})\). Then the objective function becomes

\[\min_{\mathbf{w},\mathbf{s}^{\prime},\mathbf{s}}\frac{1}{n_{+}} \sum_{X_{i}\in\mathcal{S}_{+}}f_{i}(\psi_{i}(\mathbf{w},s_{i}),s^{\prime}),\] (28) \[\text{where }f_{i}(g,s^{\prime})=s^{\prime}+\frac{(g-s^{\prime})_{+ }}{\alpha},\quad\psi_{i}(\mathbf{w},s_{i})=\frac{1}{n_{-}}\sum_{X_{j}\in \mathcal{S}_{-}}s_{i}+\frac{(\ell(\frac{h_{i}^{1}(\mathbf{w})}{h_{j}^{2}( \mathbf{w})}-\frac{h_{i}^{1}(\mathbf{w})}{h_{j}^{2}(\mathbf{w})})-s_{i})_{+}}{ \beta},\]

In this case we define \(g_{i}(\ell(\mathbf{v}),s_{i})=s_{i}+\frac{(\ell(\frac{v_{3}}{v_{4}}-\frac{v_{1} }{v_{2}})-s_{i})_{+}}{\beta}\) and \(h_{i,j}(\mathbf{w})=[h_{i}^{1}(\mathbf{w}),h_{i}^{2}(\mathbf{w}),h_{j}^{1}( \mathbf{w}),h_{j}^{2}(\mathbf{w})]\). We can still prove that \(g_{i}(\ell(\mathbf{v}),s_{i})\) is monotone w.r.t to each component of \(\mathbf{v}\). It is not difficult to prove that \(\ell(\frac{v_{3}}{v_{4}}-\frac{v_{1}}{v_{2}})\) is weakly convex w.r.t \(\mathbf{v}\) because \(\frac{v_{3}}{v_{4}}-\frac{v_{1}}{v_{2}}\) is a smooth mapping of \(\mathbf{v}\) when \(v_{2},v_{4}\) are lower bounded and \(\ell\) is a convex function [8]. As a result, since \(g_{i}(\ell,s_{i})\) is non-decreasing and convex w.r.t to \(\ell\), it is easy to prove that \(g_{i}(\ell(\mathbf{v}),s_{i})\) is weakly convex w.r.t \(\mathbf{v}\) and is monotone (either non-decreasing or non-increasing) w.r.t to each component of \(\mathbf{v}\). Hence, assuming \(h_{i}^{1}(\mathbf{w}),h_{i}^{2}(\mathbf{w})\) are smooth and Lipchitz continuous, we can prove that \(g_{i}(h_{i,j}(\mathbf{w}),s_{i})\) is weakly convex w.r.t. to \(\mathbf{w}\).

### Convergence Analysis of TPAUC Maximization

#### c.4.1 Convergence analysis for Algorithm 5

We first consider TPAUC maximization in the regular learning setting. Define \(F(\mathbf{w},\mathbf{s},s^{\prime}):=\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_ {+}}f_{i}(\psi_{i}(\mathbf{w},s_{i}),s^{\prime})\). Due to the weak-convexity of \(F(\mathbf{w},\mathbf{s},s^{\prime})\) w.r.t. \((\mathbf{w},\mathbf{s},s^{\prime})\), we consider the following Moreau envelope and proximal map defined as

\[F_{\lambda}(\mathbf{w},\mathbf{s},s^{\prime})=\min_{\tilde{ \mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime}}F(\tilde{\mathbf{w}},\tilde{ \mathbf{s}},\tilde{s}^{\prime})+\frac{1}{2\lambda}\left(\|\tilde{\mathbf{w}}- \mathbf{w}\|^{2}+\|\tilde{\mathbf{s}}-\mathbf{s}\|^{2}+\|\tilde{s}^{\prime}-s^ {\prime}\|^{2}\right),\] \[\text{prox}_{\lambda F}(\mathbf{w},\mathbf{s},s^{\prime})=\operatorname* {arg\,min}_{\tilde{\mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime}}F(\tilde{ \mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime})+\frac{1}{2\lambda}\left(\| \tilde{\mathbf{w}}-\mathbf{w}\|^{2}+\|\tilde{\mathbf{s}}-\mathbf{s}\|^{2}+\| \tilde{s}^{\prime}-s^{\prime}\|^{2}\right).\]

Following the same proof of Lemma 4.5, we have the following error bound

**Lemma C.3**.: _Consider the update for \(\{u_{i,t}:X_{i}\in\mathcal{S}_{+}\}\) in Algorithm 5. Assume \(\psi_{i}(\mathbf{w},s_{i})\) is \(C_{\psi}\)-Lipshitz continuous for all \(X_{i}\in\mathcal{S}_{+}\). Assume \(\mathbb{E}_{t}[\|G_{t}\|^{2}]\leq M^{2}\) and \(\mathbb{E}_{t}[\|\frac{1}{B_{1}}\sum_{X_{i}\in\mathcal{B}_{t}^{1}}\partial_{s} \psi_{i}(\mathbf{w}_{t},s_{i,t};\mathcal{B}_{t}^{t})\partial_{u}f(u_{i,t},s_{i}^ {\prime})e_{i}\|^{2}]\leq M^{2}\), where \(e_{i}\) is the \(n_{+}\)-dimensional vector with \(1\) at the \(i\)-th entry and \(0\) everywhere else. With \(\gamma=\frac{n_{+}-B_{1}}{B_{1}(1-\tau)}+(1-\tau)\) and \(\tau\leq\frac{1}{2}\), we have_

\[\mathbb{E}\bigg{[}\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}}\|u _{i,t+1}-\psi_{i}(\mathbf{w}_{t+1},s_{i,t+1})\|\bigg{]}\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{t+1}\frac{1}{n}\sum_{X_{i}\in \mathcal{S}_{+}}\|u_{i,0}-\psi_{i}(\mathbf{w}_{0},s_{i,0})\|+\frac{2\tau^{1/2} \sigma}{B_{2}^{1/2}}+\frac{8n_{+}C_{\psi}M\eta}{B_{1}\tau^{1/2}}.\]

Then we have following convergence guarantee.

**Theorem C.4**.: _Under the assumptions given in Lemma C.1, with \(\gamma=\frac{n_{+}-B_{1}}{B_{1}(1-\tau)}+(1-\tau)\), \(\tau=\mathcal{O}(B_{2}\epsilon^{4})\leq\frac{1}{2}\), \(\eta=\mathcal{O}(\frac{B_{1}B_{2}^{1/2}\epsilon^{4}}{n_{+}})\), and \(\bar{\rho}=\rho_{F}+\rho_{\psi}C_{f}\), Algorithm 5 converges to an \(\epsilon\)-stationary point of the Moreau envelope \(F_{1/\bar{\rho}}\) in \(T=\mathcal{O}(\frac{n_{+}}{B_{1}B_{2}^{1/2}}\epsilon^{-6})\) iterations._

Proof of Theorem C.4.: Define \((\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}^{\prime}_{t}):=\text{prox}_{ F/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\). For a given \(X_{i}\in\mathcal{S}_{+}\), we have

\[\begin{split}& f_{i}(\psi_{i}(\hat{\mathbf{w}}_{t},\hat{s}_{t}, \hat{s}^{\prime}_{t})-f_{i}(u_{i,t},s^{\prime}_{t})\\ &\stackrel{{(a)}}{{\geq}}\partial_{s^{\prime}}f_{i}(u _{i,t},s^{\prime}_{t})(\hat{s}^{\prime}_{t}-s^{\prime}_{t})+\partial_{u}f_{i}( u_{i,t},s^{\prime}_{t})(\psi_{i}(\hat{\mathbf{w}}_{t},\hat{s}_{i,t})-u_{i,t})\\ &\stackrel{{(b)}}{{\geq}}\partial_{s^{\prime}}f_{i}( u_{i,t},s^{\prime}_{t})(\hat{s}^{\prime}_{t}-s^{\prime}_{t})+\partial_{u}f_{i}( u_{i,t},s^{\prime}_{t})\bigg{[}\psi_{i}(\mathbf{w}_{t},s_{i,t})-u_{i,t}+ \langle\partial_{w}\psi_{i}(\mathbf{w}_{t},s_{i,t}),\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\rangle\\ &\quad-\frac{\rho_{\psi}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t} \|^{2}+\langle\partial_{s_{i}}\psi_{i}(\mathbf{w}_{t},s_{i,t}),\hat{s}_{i,t}-s _{i,t}\rangle-\frac{\rho_{\psi}}{2}\|\hat{s}_{i,t}-s_{i,t}\|^{2}\bigg{]}\\ &\stackrel{{(c)}}{{\geq}}\partial_{s^{\prime}}f_{i}( u_{i,t},s^{\prime}_{t})(\hat{s}^{\prime}_{t}-s^{\prime}_{t})+\partial_{u}f_{i}( u_{i,t},s^{\prime}_{t})\big{[}\psi_{i}(\mathbf{w}_{t},s_{i,t})-u_{i,t} \big{]}+\langle\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\partial_{w}\psi_{i}( \mathbf{w}_{t},s_{i,t}),\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\rangle\\ &\quad+\langle\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\partial_{ s_{i}}\psi_{i}(\mathbf{w}_{t},s_{i,t}),\hat{s}_{i,t}-s_{i,t}\rangle-\frac{\rho_{ \psi}C_{f}}{2}\left(\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\|\hat{s}_{i,t }-s_{i,t}\|^{2}\right)\end{split}\]

where (a) follows from the convexity of \(f_{i}\), (b) follows from the monotonicity of \(f_{i}(\cdot,s^{\prime})\) and weak convexity of \(\psi_{i}\), (c) is due to \(0\leq\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\leq C_{f}\). Then it follows

\[\begin{split}&\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}}\bigg{[} \partial_{s^{\prime}}f_{i}(u_{i,t},s^{\prime}_{t})(\hat{s}^{\prime}_{t}-s^{ \prime}_{t})+\langle\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\partial_{w}\psi_ {i}(\mathbf{w}_{t},s_{i,t}),\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\rangle\\ &\quad+\langle\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\partial_{ s_{i}}\psi_{i}(\mathbf{w}_{t},s_{i,t}),\hat{s}_{i,t}-s_{i,t}\rangle\bigg{]}\\ &\leq\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}}\bigg{[}f_{i}( \psi_{i}(\hat{\mathbf{w}}_{t},\hat{s}_{i,t}),\hat{s}^{\prime}_{t})-f_{i}(u_{i, t},s^{\prime}_{t})-\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\big{[}\psi_{i}( \mathbf{w}_{t},s_{i,t})-u_{i,t}\big{]}\\ &\quad+\frac{\rho_{\psi}C_{f}}{2}\left(\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\|^{2}+\|\hat{s}_{i,t}-s_{i,t}\|^{2}\right)\bigg{]}\end{split}\] (29)

Now we consider the change in the Moreau envelope:

\[\begin{split}&\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1}, \mathbf{s}_{t+1},s^{\prime}_{t+1})]\\ &=\mathbb{E}_{t}\left[\min_{\hat{\mathbf{w}},\hat{\mathbf{s}}, \hat{s}^{\prime}}F(\tilde{\mathbf{w}},\tilde{\mathbf{s}}_{t},\hat{s}^{\prime}_{ t})+\frac{\bar{\rho}}{2}\left(\|\hat{\mathbf{w}}-\mathbf{w}_{t+1}\|^{2}+\| \tilde{\mathbf{s}}-\mathbf{s}_{t+1}\|^{2}+\|\tilde{s}^{\prime}-\mathbf{s}^{ \prime}_{t+1}\|^{2}\right)\right]\\ &\leq\mathbb{E}_{t}\left[F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{ t},\hat{s}^{\prime}_{t})+\frac{\bar{\rho}}{2}\left(\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t+1}\|^{2}+\|\hat{\mathbf{s}}_{t}-\mathbf{s}_{t+1}\|^{2}+\|\hat{s}^{ \prime}_{t}-s^{\prime}_{t+1}\|^{2}\right)\right]\\ &=F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}^{\prime}_{t})+ \mathbb{E}_{t}\bigg{[}\frac{\bar{\rho}}{2}\big{(}\|\hat{\mathbf{w}}_{t}-(\mathbf{ w}_{t}-\eta G_{t})\|^{2}+\|\hat{\mathbf{s}}_{t}-(\mathbf{s}_{t}-\eta G_{t}^{1})\|^{2}\\ &\quad+\|\hat{s}^{\prime}_{t}-(s^{\prime}_{t}-\eta G_{t}^{2})\|^{2} \big{)}\bigg{]}\\ &\leq F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}^{\prime}_{t})+ \frac{\bar{\rho}}{2}\left(\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\|\hat{ \mathbf{s}}_{t}-\mathbf{s}_{t}\|^{2}+\|\hat{s}^{\prime}_{t}-s^{\prime}_{t}\|^{2} \right)\\ &\quad+\bar{\rho}\mathbb{E}_{t}[\eta\langle\hat{\mathbf{w}}_{t}- \mathbf{w}_{t},G_{t}\rangle+\eta\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},G_{t}^{1} \rangle+\eta\langle\hat{s}^{\prime}_{t}-s^{\prime}_{t},G_{t}^{2}\rangle]+ \frac{3\eta^{2}\bar{\rho}M^{2}}{2}\\ &=F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})+ \bar{\rho}\mathbb{E}_{t}[\eta\langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},G_{t} \rangle+\eta\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},G_{t}^{1}\assumptions and are denoted by \(M\). Moreover, we have

\[\begin{split}&\mathbb{E}_{t}[\eta\langle\hat{\mathbf{w}}_{t}-\mathbf{w} _{t},G_{t}\rangle+\eta\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},G_{t}^{1} \rangle+\eta\langle\hat{s}_{t}^{\prime}-s_{t}^{\prime},G_{t}^{2}\rangle]\\ &=\eta\langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},\mathbb{E}_{t}[G_ {t}]\rangle+\eta\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},\mathbb{E}_{t}[G_{t }^{1}]\rangle+\eta\langle\hat{s}_{t}^{\prime}-s_{t}^{\prime},\mathbb{E}_{t}[G_{t }^{2}]\rangle,\end{split}\]

and

\[\begin{split}&\mathbb{E}_{t}[G_{t}]=\frac{1}{n_{+}}\sum_{X_{i}\in \mathcal{S}_{+}}\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{w}\psi_{i}( \mathbf{w}_{t},s_{i,t})\\ &\mathbb{E}_{t}[G_{t}^{1}]=\frac{1}{n_{+}}\sum_{X_{i}\in \mathcal{S}_{+}}\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{\mathbf{s} }\psi_{i}(\mathbf{w}_{t},s_{i,t})\\ &\mathbb{E}_{t}[G_{t}^{2}]=\frac{1}{n_{+}}\sum_{X_{i}\in \mathcal{S}_{+}}\partial_{s^{\prime}}f_{i}(u_{i,t},s_{t}^{\prime}).\end{split}\]

Combining inequality 29 and 30 yields

\[\begin{split}&\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1}, \mathbf{s}_{t+1},s_{t+1}^{\prime})]\\ &\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{ \prime})+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\frac{\bar{\rho}\eta}{n_{+}}\sum_{ X_{i}\in\mathcal{S}_{+}}\bigg{[}f_{i}(\psi_{i}(\hat{\mathbf{w}}_{t},\hat{s}_{i,t}), \hat{s}_{t}^{\prime})-f_{i}(u_{i,t},s_{t}^{\prime})\\ &\quad-\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\big{[}\psi_{i}( \mathbf{w}_{t},s_{i,t})-u_{i,t}\big{]}+\frac{\rho_{\psi}C_{f}}{2}\left(\|\hat{ \mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+\|\hat{s}_{i,t}-s_{i,t}\|^{2}\right) \bigg{]}\\ &\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{ \prime})+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta(F(\hat{\mathbf{w}}_ {t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})-F(\mathbf{w}_{t},\mathbf{s}_{t },s_{t}^{\prime}))\\ &\quad+\frac{\bar{\rho}\eta}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+} }\bigg{[}f_{i}(\psi_{i}(\mathbf{w}_{t},s_{i,t}),s_{t}^{\prime})-f_{i}(u_{i,t}, s_{t}^{\prime})-\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\big{[}\psi_{i}( \mathbf{w}_{t},s_{i,t})-u_{i,t}\big{]}\\ &\quad+\frac{\rho_{\psi}C_{f}}{2}\left(\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\|^{2}+\|\hat{s}_{i,t}-s_{i,t}\|^{2}\right)\bigg{]}\end{split}\] (31)

Due to the \(\rho_{F}\)-weak convexity of \(F(\mathbf{w},\mathbf{s},s^{\prime})\), we have \((\bar{\rho}-\rho_{F})\)-strong convexity of \((\mathbf{w},\mathbf{s},s^{\prime})\mapsto F(\mathbf{w},\mathbf{s},s^{\prime} )+\frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-( \mathbf{w},\mathbf{s},s^{\prime})\|^{2}\). Then it follows

\[\begin{split} F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{ s}_{t}^{\prime})-F(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})&= \bigg{[}F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})+ \frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-(\hat{ \mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})\|^{2}\bigg{]}\\ &\quad-\bigg{[}F(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})+ \frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-(\mathbf{ w}_{t},\mathbf{s}_{t},s_{t}^{\prime})\|^{2}\bigg{]}\\ &\quad-\frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^ {\prime})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})\|^{2} \\ &\leq(\frac{\rho_{F}}{2}-\bar{\rho})\|(\mathbf{w}_{t},\mathbf{s}_{ t},s_{t}^{\prime})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{ \prime})\|^{2}\end{split}\] (32)

Plugging inequality 32 into inequality 31 yields

\[\begin{split}&\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1}, \mathbf{s}_{t+1},s_{t+1}^{\prime})]\\ &\leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s_{ t}^{\prime})]+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta(\frac{\rho_{F}}{2}-\bar{ \rho})\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-(\hat{\mathbf{w}}_{t}, \hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})\|^{2}\\ &\quad+\frac{\bar{\rho}\eta}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}} \bigg{[}f_{i}(\psi_{i}(\mathbf{w}_{t},s_{i,t}),s_{t}^{\prime})-f_{i}(u_{i,t},s_{t }^{\prime})-\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\big{[}\psi_{i}(\mathbf{w}_ {t},s_{i,t})-u_{i,t}\big{]}\\ &\quad+\frac{\rho_{\psi}C_{f}}{2}\left(\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\|^{2}+\|\hat{s}_{i,t}-s_{i,t}\|^{2}\right)\bigg{]}\end{split}\] (33)

Set \(\bar{\rho}=\rho_{F}+\rho_{\psi}C_{f}\). We have \[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1},\mathbf{s}_{t+1},s^{ \prime}_{t+1})]\] (35) \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t}) +\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{\bar{\rho}^{2}\eta}{2}\|(\mathbf{w}_ {t},\mathbf{s}_{t},s^{\prime}_{t})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t}, \hat{s}^{\prime}_{t})\|^{2}\] \[\quad+\frac{\bar{\rho}\eta}{n_{+}}\sum_{X_{i}\in S_{+}}\left[f_{i }(\psi_{i}(\mathbf{w}_{t},s_{i,t}),s^{\prime}_{t})-f_{i}(u_{i,t},s^{\prime}_{t} )-\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\big{[}\psi_{i}(\mathbf{w}_{t},s_{ i,t})-u_{i,t}\big{]}\right]\] \[\stackrel{{(a)}}{{\leq}}F_{1/\bar{\rho}}(\mathbf{w} _{t},\mathbf{s}_{t},s^{\prime}_{t})+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{ \eta}{2}\|\nabla\varphi_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{ \prime}_{t})\|^{2}\] \[\quad+\frac{\bar{\rho}\eta}{n_{+}}\sum_{X_{i}\in S_{+}}\left[f_{i }(\psi_{i}(\mathbf{w}_{t},s_{i,t}),s^{\prime}_{t})-f_{i}(u_{i,t},s^{\prime}_{t })-\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\big{[}\psi_{i}(\mathbf{w}_{t},s_{ i,t})-u_{i,t}\big{]}\right]\]

where inequality (a) follows from Lemma 3.2.

Using the Lipschitz continuity of \(f\), we have

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1},\mathbf{s}_{t+1},s^{\prime}_{t+1})]\] (34) \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{ t})+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{\eta}{2}\|\nabla F_{1/\bar{ \rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}\] \[\quad+\frac{\bar{\rho}\eta}{n_{+}}\sum_{X_{i}\in S_{+}}2C_{f}\| \psi_{i}(\mathbf{w}_{t},s_{i,t})-u_{i,t}\|\]

With the error bound from Lemma C.3, we have

\[\mathbb{E}\left[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|\psi_{i}(\mathbf{w}_{t},s_{i,t})-u_{i,t}\|\right]\leq(1-\mu)^{t}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}} \|\psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{i,0}\|+R\]

with \(\mu=\frac{B_{1}\tau}{2n_{+}}\), \(R=\frac{2\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4n_{+}C_{\circ}M\eta}{B_{1}\tau^ {1/2}}+\frac{4n_{+}^{1/2}C_{\circ}M\eta}{B_{1}\tau^{1/2}}\). Then

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1},\mathbf{s}_{t+1},s^ {\prime}_{t+1})]\] (35) \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{ t})+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{\eta}{2}\mathbb{E}[\|\nabla F_{1/ \bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\quad+2C_{f}\bar{\rho}\eta\left((1-\mu)^{t}\frac{1}{n_{+}}\sum_{X _{i}\in S_{+}}\|\psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{i,0}\|+R\right)\]

Taking summation from \(t=0\) to \(T-1\) yields

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{T},\mathbf{s}_{T},s^{ \prime}_{T})]\] (36) \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{ 0})+\frac{3\eta^{2}\bar{\rho}M^{2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\mathbb{E }[\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\quad+2C_{f}\bar{\rho}\eta\left(\sum_{t=0}^{T-1}(1-\mu)^{t}\frac{1 }{n_{+}}\sum_{X_{i}\in S_{+}}\|\psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{i,0}\|+RT\right)\] \[\stackrel{{(a)}}{{\leq}}F_{1/\bar{\rho}}(\mathbf{w}_ {0},\mathbf{s}_{0},s^{\prime}_{0})+\frac{3\eta^{2}\bar{\rho}M^{2}T}{2}-\frac{ \eta}{2}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t}, \mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\quad+\frac{4C_{f}\bar{\rho}\eta}{\mu}\sum_{X_{i}\in S_{+}}\frac {1}{n_{+}}\|\psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{i,0}\|+2C_{f}\bar{\rho}\eta RT\]

where (a) uses \(\sum_{t=0}^{T-1}(1-\mu)^{t}\leq\frac{1}{\mu}\).

Lower bounding the left-hand-side by \(\min_{\mathbf{w},\mathbf{s},s^{\prime}}F_{1/\bar{\rho}}(\mathbf{w},\mathbf{s},s^{ \prime})\), we obtain

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\leq\frac{2}{\eta T}\bigg{[}F_{1/\bar{\rho}}(\mathbf{w}_{0}, \mathbf{s}_{0},s^{\prime}_{0})-\min_{\mathbf{w},\mathbf{s},s^{\prime}}F_{1/ \bar{\rho}}(\mathbf{w},\mathbf{s},s^{\prime})+\frac{3\eta^{2}\bar{\rho}M^{2}T} {2}\] \[\quad+\frac{4C_{f}\bar{\rho}\eta}{n_{+}}\sum_{X_{i}\in S_{+}}\| \psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{i,0}\|+2C_{f}\bar{\rho}\eta RT\bigg{]}\] \[\leq\frac{2\Delta}{\eta T}+3\eta\bar{\rho}M^{2}+\frac{8C_{f}\bar {\rho}}{\mu Tn_{+}}\sum_{X_{i}\in S_{+}}\|\psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{ i,0}\|+4C_{f}\bar{\rho}R\] \[\leq\frac{C}{T}(\frac{1}{\eta}+\frac{1}{\mu})+C(\eta+R)\]

where we assume \(F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{0})-\min_{\mathbf{ w},\mathbf{s},s^{\prime}}F_{1/\bar{\rho}}(\mathbf{w},\mathbf{s},s^{\prime})\leq\Delta\) and

\[C=\max\{8\Delta,12\bar{\rho}M^{2},32C_{f}\bar{\rho}\sum_{X_{i}\in S_{+}}\| \psi_{i}(\mathbf{w}_{0},s_{i,0})-u_{i,0}\|,16C_{f}\bar{\rho}\}.\]

Plugging the expression of \(\mu\) and \(R\) yields

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\leq\mathcal{O}\left(\frac{1}{T}(\frac{1}{\eta}+\frac{n_{+}}{B_{ 1}\tau})+(\frac{\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{n_{+}\eta}{B_{1}\tau^{1/2 }})\right)\]

Setting \(\tau=\mathcal{O}(B_{2}\epsilon^{4})\) and \(\eta=\mathcal{O}(\frac{B_{1}B_{2}^{1/2}}{n_{+}}\epsilon^{4})\), with \(T=\mathcal{O}(\frac{n_{+}}{B_{1}B_{2}^{1/2}}\epsilon^{-6})\) iterations, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\leq\epsilon^{2}.\]

#### c.4.2 Convergence analysis for Algorithm 6

We now consider MIL TPAUC maximization with mean pooling. Define \(F(\mathbf{w},\mathbf{s},s^{\prime}):=\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}f_{i} (g_{i}(h_{j}(\mathbf{w})-h_{i}(\mathbf{w}),s_{i}),s^{\prime})\). Due to the weak-convexity of \(F(\mathbf{w},\mathbf{s},s^{\prime})\) w.r.t. \((\mathbf{w},\mathbf{s},s^{\prime})\), we consider the following Moreau envelope and proximal map defined as

\[F_{\lambda}(\mathbf{w},\mathbf{s},s^{\prime})=\min_{\tilde{ \mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime}}F(\tilde{\mathbf{w}},\tilde {\mathbf{s}},\tilde{s}^{\prime})+\frac{1}{2\lambda}\left(\|\tilde{\mathbf{w}}- \mathbf{w}\|^{2}+\|\tilde{\mathbf{s}}-\mathbf{s}\|^{2}+\|\tilde{s}^{\prime}-s ^{\prime}\|^{2}\right),\] \[\text{prox}_{\lambda F}(\mathbf{w},\mathbf{s},s^{\prime})=\operatorname* {arg\,min}_{\tilde{\mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime}}F(\tilde{ \mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime})+\frac{1}{2\lambda}\left(\| \tilde{\mathbf{w}}-\mathbf{w}\|^{2}+\|\tilde{\mathbf{s}}-\mathbf{s}\|^{2}+\| \tilde{s}^{\prime}-s^{\prime}\|^{2}\right).\]

Following the same proofs of Lemma A.3 and Lemma A.4, we have the following error bounds

**Lemma C.5**.: _Consider the update for \(\{v_{i,t}:X_{i}\in S_{+}\cup S_{-}\}\) in Algorithm 6. Assume \(h_{i}(\mathbf{w};\xi)\) is \(C_{h}\)-Lipshitz for all \(X_{i}\in S_{+}\cup S_{-}\), and \(\mathbb{E}[\|G_{t}\|^{2}]\leq M^{2}\). With \(\gamma_{1}=\frac{n_{+}-B_{1}}{B_{1}(1-\tau_{1})}+(1-\tau_{1})\)\(\gamma_{2}=\frac{n_{-}-B_{2}}{B_{2}(1-\tau_{1})}+(1-\tau_{1})\) and \(\tau_{1}\leq\frac{1}{2}\), we have_

\[\mathbb{E}\left[\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}}\|v_{i,t+1}-h_{i}(\mathbf{w}_{t+1})\|\right] \leq(1-\frac{B_{1}\tau_{1}}{2n_{+}})^{t+1}\sum_{X_{i}\in\mathcal{S }_{+}}\|v_{i,0}-h_{i}(\mathbf{w}_{t})\|+2\tau_{1}^{1/2}\sigma+\frac{4n_{+}C_{h} M\eta}{B_{1}\tau_{1}^{1/2}}\] \[\mathbb{E}\left[\frac{1}{n_{-}}\sum_{X_{j}\in\mathcal{S}_{-}}\|v_ {j,t+1}-h_{j}(\mathbf{w}_{t+1})\|\right] \leq(1-\frac{B_{1}\tau_{1}}{2n_{-}})^{t+1}\frac{1}{n_{-}}\sum_{X _{j}\in\mathcal{S}_{-}}\|v_{j,0}-h_{j}(\mathbf{w}_{t})\|+2\tau_{1}^{1/2}\sigma +\frac{4n_{-}C_{h}M\eta}{B_{1}\tau_{1}^{1/2}}\] \[\mathbb{E}\left[\frac{1}{n_{+}}\sum_{X_{i}\in\mathcal{S}_{+}}\|v_ {i,t+1}-h_{i}(\mathbf{w}_{t+1})\|^{2}\right] \leq(1-\frac{B_{1}\tau_{1}}{2n_{+}})^{2(t+1)}\frac{1}{n_{+}}\sum_{X _{i}\in\mathcal{S}_{+}}\|v_{i,0}-h_{i}(\mathbf{w}_{t})\|^{2}+4\tau_{1}\sigma^ {2}+\frac{16n_{+}^{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau_{1}}\] \[\mathbb{E}\left[\frac{1}{n_{-}}\sum_{X_{j}\in\mathcal{S}_{-}}\|v_ {j,t+1}-h_{j}(\mathbf{w}_{t+1})\|^{2}\right] \leq(1-\frac{B_{1}\tau_{1}}{2n_{-}})^{2(t+1)}\frac{1}{n_{-}}\sum_{X _{j}\in\mathcal{S}_{-}}\|v_{j,0}-h_{j}(\mathbf{w}_{t})\|^{2}+4\tau_{1}\sigma^{ 2}+\frac{16n_{-}^{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau_{1}}\]

**Lemma C.6**.: _Consider update for \(\{u_{i,t}:X_{i}\in\mathcal{S}_{+}\}\) in Algorithm 6. Assume \(g_{i}(v_{ij},s_{i})\) is \(C_{g}\)-Lipshitz w.r.t. \((v_{ij},s_{i})\) for all \(X_{i}\in S_{+}\) and \(X_{j}\in S_{-}\). With \(\gamma_{3}=\frac{n_{+}-B_{1}}{B_{1}(1-\tau_{2})}+(1-\tau_{2})\) and \(\tau_{2}\leq\frac{1}{2}\), we have_

\[\mathbb{E}\left[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|u_{i,t+1}- \frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t+1}-v_{i,t+1},s_{i,t+1})\|\right]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{t+1}\frac{1}{n_{+}}\sum_{X_{ i}\in S_{+}}\|u_{i,0}-\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,0}-v_{i,0},s_{i,0}) \|+2\tau_{2}^{1/2}\sigma\] \[\quad+C_{2}\frac{n_{+}}{B_{1}}(\frac{B_{1}^{1/2}}{n_{+}^{1/2}}+ \frac{B_{2}^{1/2}}{n_{-}^{1/2}})\frac{\tau_{1}}{\tau_{2}^{1/2}}+C_{2}\frac{n_{ +}}{B_{1}}(\frac{n_{+}^{1/2}}{B_{1}^{1/2}}+\frac{n_{-}^{1/2}}{B_{2}^{1/2}}) \frac{\eta}{\tau_{2}^{1/2}}+C_{2}\frac{n_{+}^{1/2}\eta}{B_{1}\tau_{2}^{1/2}}\]

_where \(C_{2}\) is a constant defined in the proof._

Then we have the following convergence guarantee.

**Theorem C.7**.: _Under assumptions given in Lemma C.2, with \(\gamma_{1}=\frac{n_{1}-B_{1}}{B_{1}(1-\tau_{1})}+(1-\tau_{1})\), \(\gamma_{2}=\frac{n_{2}-B_{2}}{B_{2}(1-\tau_{1})}+(1-\tau_{1})\), \(\gamma_{3}=\frac{n_{1}-B_{1}}{B_{1}(1-\tau_{2})}+(1-\tau_{2})\), \(\tau_{1}=\mathcal{O}\left(\min\left\{B_{3},\frac{B_{1}}{n_{+}}\min\{\frac{n_{1} ^{1/2}}{B_{1}^{1/2}},\frac{n_{1}^{1/2}}{B_{1}^{1/2}}\}B_{2}^{1/2}\right\}\epsilon ^{4}\right)\leq 1/2\), \(\tau_{2}=\mathcal{O}(B_{2}\epsilon^{4})\leq 1/2\), \(\eta=\mathcal{O}\left(\min\left\{\min\{\frac{B_{1}}{n_{+}},\frac{B_{2}}{n_{-}}\} \min\{B_{3}^{1/2},\frac{B_{1}^{1/2}}{n_{+}^{1/2}}\min\{\frac{n_{1}^{1/4}}{B_{1} ^{1/4}},\frac{n_{1}^{1/4}}{B_{2}^{1/4}}\}B_{2}^{1/4}\}B_{2}^{1/4}\right\}\), \(\frac{B_{1}}{n_{+}}\min\{\frac{B_{1}^{1/2}}{n_{+}^{1/2}},\frac{B_{2}^{1/2}}{n_{- }^{1/2}}\}B_{3}^{1/2}\right)\epsilon^{4}\right)\), then after_

\[T\geq\mathcal{O}\left(\max\left\{\max\{\max\{\frac{n_{+}}{B_{1}},\frac{n_{-}}{B_{2 }}\}\max\{\frac{1}{B_{3}^{1/2}},\frac{n_{+}^{1/2}}{B_{1}^{1/2}}\max\{\frac{B_{1} ^{1/4}}{n_{+}^{1/4}},\frac{B_{2}^{1/4}}{n_{-}^{1/4}}\}\frac{1}{B_{2}^{1/4}}\} \frac{n_{+}}{B_{1}}\max\{\frac{n_{+}^{1/2}}{B_{1}^{1/2}},\frac{n_{-}^{1/2}}{B_{2} ^{1/2}}\}\frac{1}{B_{2}^{1/2}}\right\}\epsilon^{-6}\right)\]

_iterations, Algorithm 6 gives \(\epsilon\)-stationary point to the Moreau envelope, i.e.,_

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}\leq\epsilon^{2}.\]

_where \(\bar{\rho}=\rho_{F}+\rho_{g}C_{f}+8\rho_{g}C_{f}C_{h}+C_{f}C_{g}L_{h}\)._Proof of Theorem c.7.: Consider the change in the Moreau envelope:

\[\begin{split}&\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1}, \mathbf{s}_{t+1},s^{\prime}_{t+1})]\\ &=\mathbb{E}_{t}\left[\min_{\tilde{\mathbf{w}},\tilde{\mathbf{s}},\tilde{s}^{\prime}}F(\tilde{\mathbf{w}},\tilde{\mathbf{s}}_{t},\tilde{s}^{ \prime}_{t})+\frac{\bar{\rho}}{2}\left(\|\tilde{\mathbf{w}}-\mathbf{w}_{t+1} \|^{2}+\|\tilde{\mathbf{s}}-\mathbf{s}_{t+1}\|^{2}+\|\tilde{s}^{\prime}- \mathbf{s}^{\prime}_{t+1}\|^{2}\right)\right]\\ &\leq\mathbb{E}_{t}\left[F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}} _{t},\hat{s}^{\prime}_{t})+\frac{\bar{\rho}}{2}\left(\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t+1}\|^{2}+\|\hat{\mathbf{s}}_{t}-\mathbf{s}_{t+1}\|^{2}+\|\hat{s }^{\prime}_{t}-s^{\prime}_{t+1}\|^{2}\right)\right]\\ &=F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}^{\prime}_{t })+\mathbb{E}_{t}\bigg{[}\frac{\bar{\rho}}{2}\big{(}\|\hat{\mathbf{w}}_{t}-( \mathbf{w}_{t}-\eta G_{t})\|^{2}+\|\hat{\mathbf{s}}_{t}-(\mathbf{s}_{t}-\eta G ^{1}_{t})\|^{2}\\ &\quad+\|\hat{s}^{\prime}_{t}-(s^{\prime}_{t}-\eta G^{2}_{t})\|^ {2}\big{)}\bigg{]}\\ &\leq F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s_{t}}},\hat{s}^{\prime }_{t})+\frac{\bar{\rho}}{2}\left(\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}+ \|\hat{\mathbf{s}}_{t}-\mathbf{s}_{t}\|^{2}+\|\hat{s}^{\prime}_{t}-s^{\prime}_ {t}\|^{2}\right)\\ &\quad+\bar{\rho}\mathbb{E}_{t}[\eta\langle\hat{\mathbf{w}}_{t}- \mathbf{w}_{t},G_{t}\rangle+\eta\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},G^ {1}_{t}\rangle+\eta\langle\hat{s}^{\prime}_{t}-s^{\prime}_{t},G^{2}_{t})]+ \frac{3\eta^{2}\bar{\rho}M^{2}}{2}\\ &=F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t}) +\bar{\rho}\mathbb{E}_{t}[\eta\langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},G_{t }\rangle+\eta\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},G^{1}_{t}\rangle+\eta \langle\hat{s}^{\prime}_{t}-s^{\prime}_{t},G^{2}_{t}\rangle]\\ &\quad+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}\end{split}\] (37)

where for simplicity we denote \(G^{2}_{t}=\frac{1}{B_{1}}\sum_{i\in\mathcal{B}^{1}_{1}}\partial_{s^{\prime}}f _{i}(u_{i,t},s^{\prime}_{t})\), and \(G^{1}_{t}\) is a \(n_{+}\)-dimensional vector whose \(i\)-th coordinate is defined as

\[\begin{cases}\frac{1}{B_{1}}\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\left[ \frac{1}{B_{2}}\sum_{X_{j}\in\mathcal{B}^{2}_{2}}\partial_{s_{i}}g_{i}(v_{j,t}- v_{i,t},s_{i,t})\right],&X_{i}\in\mathcal{B}^{1}_{1}\\ 0,&X_{i}\not\in\mathcal{B}^{1}_{1}\end{cases}.\]

The second inequality in the above derivation uses the bounds of \(\mathbb{E}[\|G_{t}\|^{2}],\mathbb{E}[\|G^{1}_{t}\|^{2}]\) and \(\mathbb{E}[\|G^{2}_{t}\|^{2}]\), which follow from the Lipschitz continuity and bounded variance assumptions and are denoted by \(M\).

Note that

\[\begin{split}&\mathbb{E}_{t}[G_{t}]\\ &=\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\partial_{u}f_{i}(u_{i,t},s^{ \prime}_{t})\left[\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\partial_{v}g_{i}(v_{j,t }-v_{i,t},s_{i,t})\left(\nabla h_{i}(\mathbf{w})-\nabla h_{j}(\mathbf{w}) \right)\right]\\ &\mathbb{E}_{t}[G^{1}_{t}]=\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}} \partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\left[\frac{1}{n_{-}}\sum_{X_{j}\in S _{-}}\partial_{\mathbf{s}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})\right]\\ &\mathbb{E}_{t}[G^{2}_{t}]=\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}} \partial_{s^{\prime}}f_{i}(u_{i,t},s^{\prime}_{t})\end{split}\]Define \((\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime}):=\text{prox}_{F/ \tilde{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})\). For a given \(i\in\{1,\ldots,m\}\), we have

\[\begin{split}& f_{i}(\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(h_{j}( \hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}),\hat{s}_{i,t}),\hat{s}_{t}^{ \prime})-f_{i}(u_{i,t},s_{t}^{\prime})\\ &\stackrel{{(a)}}{{\geq}}\partial_{s^{\prime}}f_{i}( u_{i,t},s_{t}^{\prime})(\hat{s}_{t}^{\prime}-s_{t}^{\prime})+\partial_{u}f_{i}(u_{i,t},s_{t} ^{\prime})(\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(h_{j}(\hat{\mathbf{w}}_{t })-h_{i}(\hat{\mathbf{w}}_{t}),\hat{s}_{i,t})-u_{i,t})\\ &\stackrel{{(b)}}{{\geq}}\partial_{s^{\prime}}f_{i}( u_{i,t},s_{t}^{\prime})(\hat{s}_{t}^{\prime}-s_{t}^{\prime})+\partial_{u}f_{i}( u_{i,t},s_{t}^{\prime})\bigg{[}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s _{i,t})-u_{i,t}\\ &\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\langle\partial_{v}g_ {i}(v_{j,t}-v_{i,t},s_{i,t}),(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{ w}}_{t}))-(v_{j,t}-v_{i,t})\rangle\rangle\\ &\quad-\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\frac{\rho_{g}}{2}\|( h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}))-(v_{j,t}-v_{i,t})\|^{2}\\ &\quad+\langle\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\partial_{s_{i }}g_{i}(v_{j,t}-v_{i,t}),s_{i,t},\hat{s}_{i,t}-s_{i,t}\rangle-\frac{\rho_{g}}{2 }\|\hat{s}_{i,t}-s_{i,t}\|^{2}\bigg{]}\\ &\stackrel{{(c)}}{{\geq}}\partial_{s^{\prime}}f_{i}( u_{i,t},s_{t}^{\prime})(\hat{s}_{t}^{\prime}-s_{t}^{\prime})+\partial_{u}f_{i}( u_{i,t},s_{t}^{\prime})\bigg{[}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s _{i,t})-u_{i,t}\bigg{]}\\ &\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\underbrace{\langle \partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{v}g_{i}(v_{j,t}-v_{i,t},s_{ i,t}),(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}))-(v_{j,t}-v_{i,t}) \rangle\rangle}_{A_{1}}\\ &\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\langle\partial_{u}f_{ i}(u_{i,t},s_{t}^{\prime})\partial_{s_{i}}g_{i}(v_{j,t}-v_{i,t},s_{i,t}),\hat{s}_{i,t} -s_{i,t}\rangle\\ &\quad-\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\frac{\rho_{g}C_{f}}{2 }\|(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}))-(v_{j,t}-v_{i,t}) \|^{2}-\frac{\rho_{g}C_{f}}{2}\|\hat{s}_{i,t}-s_{i,t}\|^{2}\end{split}\] (38)

where (a) follows from the convexity of \(f_{i}\), (b) follows from the monotonicity of \(f_{i}(\cdot,s^{\prime})\) and weak convexity of \(g_{i}\), (c) is due to \(0\leq\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\leq C_{f}\).

The \(L_{h}\)-smoothness assumption of \(h_{i}(\mathbf{w})-h_{j}(\mathbf{w})\) for all \(i,\mathbf{w}\) implies

\[\begin{split}& h_{i}(\hat{\mathbf{w}}_{t})-h_{j}(\hat{\mathbf{w}} _{t})\\ &\quad\geq h_{i}(\mathbf{w}_{t})-h_{j}(\mathbf{w}_{t})+\langle( \nabla h_{i}(\mathbf{w}_{t})-\nabla h_{j}(\mathbf{w}_{t})),\hat{\mathbf{w}}_{t }-\mathbf{w}_{t}\rangle-\frac{L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t} \|^{2}\end{split}\] (39)

Since \(\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{v}g_{i}(v_{j,t}-v_{i,t},s_{ i,t})\geq 0\), we bound \(A_{1}\) as following

\[\begin{split}& A_{1}=\langle\partial_{u}f_{i}(u_{i,t},s_{t}^{ \prime})\partial_{v}g_{i}(v_{j,t}-v_{i,t},s_{i,t}),(h_{j}(\hat{\mathbf{w}}_{t})- h_{i}(\hat{\mathbf{w}}_{t}))-(v_{j,t}-v_{i,t})\rangle\\ &\stackrel{{(a)}}{{\geq}}\langle\partial_{u}f_{i}(u_{i,t },s_{t}^{\prime})\partial_{v}g_{i}(v_{j,t}-v_{i,t},s_{i,t}),(h_{i}(\mathbf{w}_{t})- h_{j}(\mathbf{w}_{t}))-(v_{j,t}-v_{i,t})\rangle\rangle\\ &\quad-\langle\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{v}g _{i}(v_{j,t}-v_{i,t},s_{i,t}),\frac{L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t} \|^{2}\rangle\\ &\quad+\langle\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{v}g _{i}(v_{j,t}-v_{i,t},s_{i,t})(\nabla h_{i}(\mathbf{w}_{t})-\nabla h_{j}(\mathbf{w}_{t} )),\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\rangle\\ &\stackrel{{(b)}}{{\geq}}-C_{f}C_{g}[\|h_{i}(\mathbf{ w}_{t})-v_{i,t}\|+\|h_{j}(\mathbf{w}_{t})-v_{j,t}\|]-\frac{C_{f}C_{g}L_{h}}{2}\|\hat{ \mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\\ &\quad+\langle\partial_{u}f_{i}(u_{i,t},s_{t}^{\prime})\partial_{t}g _{i}(v_{j,t}-v_{i,t},s_{i,t})(\nabla h_{i}(\mathbf{w}_{t})-\nabla h_{j}(\mathbf{w}_{t} )),\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\rangle\end{split}\]

where inequality (a) follows from inequality 39, (b) follows from the Lipschitz continuity and monotone assumptions on \(f_{i},g_{i},h_{i},h_{j}\). Then plugging the new formulation of \(A_{1}\) back to inequality 38yields

\[f_{i}(\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(h_{j}(\hat{\mathbf{w }}_{t})-h_{i}(\hat{\mathbf{w}}_{t}),\hat{s}_{i,t}),\hat{s}^{\prime}_{t})-f_{i}(u_ {i,t},s^{\prime}_{t})\] \[\geq\partial_{s^{\prime}}f_{i}(u_{i,t},s^{\prime}_{t})(\hat{s}^{ \prime}_{t}-s^{\prime}_{t})+\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\bigg{[} \frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})-u_{i,t} \bigg{]}\] \[\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\left[-C_{f}C_{g}[\|h_{ i}(\mathbf{w}_{t})-v_{i,t}\|+\|h_{j}(\mathbf{w}_{t})-v_{j,t}\|\|]-\frac{C_{f}C_{g}L_{ h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\right.\] \[\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\langle\partial_{u}f_{ i}(u_{i,t},s^{\prime}_{t})\partial_{v}g_{i}(v_{j,t}-v_{i,t},s_{i,t})(\nabla h _{i}(\mathbf{w}_{t})-\nabla h_{j}(\mathbf{w}_{t})),\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\rangle\] \[\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\langle\partial_{u}f_{ i}(u_{i,t},s^{\prime}_{t})\partial_{s_{i}}g_{i}(v_{j,t}-v_{i,t},s_{i,t}),\hat{s}_{i,t} -s_{i,t}\rangle\] \[\quad-\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\frac{\rho_{g}C_{f}}{2 }\|(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}))-(v_{j,t}-v_{i,t}) \|^{2}-\frac{\rho_{g}C_{f}}{2}\|\hat{s}_{i,t}-s_{i,t}\|^{2}\]

Taking average over \(i\in S_{+}\) gives

\[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}f_{i}(\frac{1}{n_{-}}\sum_{X _{j}\in S_{-}}g_{i}(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}), \hat{s}_{i,t}),\hat{s}^{\prime}_{t})-f_{i}(u_{i,t},s^{\prime}_{t})\] \[\geq\langle\mathbb{E}_{t}[G_{t}^{2}],\hat{s}^{\prime}_{t}-s^{ \prime}_{t}\rangle+\langle\mathbb{E}_{t}[G_{t}],\hat{\mathbf{w}}_{t}-\mathbf{w }_{t}\rangle+\langle\mathbb{E}_{t}[G_{t}^{1}],\hat{\mathbf{s}}_{t}-\mathbf{s}_ {t}\rangle\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\partial_{u}f_{i}(u_{i,t },s^{\prime}_{t})\bigg{[}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{ i,t},s_{i,t})-u_{i,t}\bigg{]}\] \[\quad-C_{f}C_{g}\left[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i} (\mathbf{w}_{t})-v_{i,t}\|+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{ w}_{t})-v_{j,t}\|\right]-\frac{C_{f}C_{g}L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t} \|^{2}\] \[\quad-\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\frac{1}{n_{-}}\sum_{X_{ j}\in S_{-}}\frac{\rho_{g}C_{f}}{2}\|(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t})) -(v_{j,t}-v_{i,t})\|^{2}-\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\frac{\rho_{g}C_{f}} {2}\|\hat{s}_{i,t}-s_{i,t}\|^{2}\]

It follows

\[\langle\mathbb{E}_{t}[G_{t}^{2}],\hat{s}^{\prime}_{t}-s^{\prime}_{ t}\rangle+\langle\mathbb{E}_{t}[G_{t}],\hat{\mathbf{w}}_{t}-\mathbf{w}_{t} \rangle+\langle\mathbb{E}_{t}[G_{t}^{1}],\hat{\mathbf{s}}_{t}-\mathbf{s}_{t}\rangle\] \[\leq\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\bigg{[}f_{i}(\frac{1}{n_{ -}}\sum_{X_{j}\in S_{-}}g_{i}(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w} }_{t}),\hat{s}_{i,t}),\hat{s}^{\prime}_{t})-f_{i}(u_{i,t},s^{\prime}_{t})\] \[\quad-\partial_{u}f_{i}(u_{i,t},s^{\prime}_{t})\bigg{[}\frac{1}{n _{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{]}\] \[\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\frac{\rho_{g}C_{f}}{2} \|(h_{j}(\hat{\mathbf{w}}_{t})-h_{i}(\hat{\mathbf{w}}_{t}))-(v_{j,t}-v_{i,t})\|^{ 2}+\frac{\rho_{g}C_{f}}{2}\|\hat{s}_{i,t}-s_{i,t}\|^{2}\bigg{]}\] \[\quad+C_{f}C_{g}\bigg{[}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i }(\mathbf{w}_{t})-v_{i,t}\|+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{ w}_{t})-v_{j,t}\|\bigg{]}+\frac{C_{f}C_{g}L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\] (40)Combining inequality 37 and 40 yields

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1},\mathbf{s}_{t+1},s_{ t+1}^{\prime})]\] (41) \[=F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})+ \bar{\rho}\eta\left[\langle\hat{\mathbf{w}}_{t}-\mathbf{w}_{t},\mathbb{E}_{t}[G_ {t}]\rangle+\langle\hat{\mathbf{s}}_{t}-\mathbf{s}_{t},\mathbb{E}_{t}[G_{t}^{ 1}]\rangle+\langle\hat{s}_{t}^{\prime}-s_{t}^{\prime},\mathbb{E}_{t}[G_{t}^{2}] \rangle\right]\] \[\quad+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}\] \[\stackrel{{(a)}}{{\leq}}F_{1/\bar{\rho}}(\mathbf{w }_{t},\mathbf{s}_{t},s_{t}^{\prime})+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\bar{ \rho}\eta\Bigg{\{}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\bigg{[}F_{i}(\hat{s}_{ t}^{\prime},\hat{\mathbf{w}}_{t},\hat{s}_{i,t})-F_{i}(s_{t}^{\prime}, \mathbf{w}_{t},s_{i,t})\] \[\quad+C_{f}C_{g}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\big{[}\|h_{ i}(\mathbf{w}_{t})-v_{i,t}\|+\|h_{j}(\mathbf{w}_{t})\big{)}-v_{j,t}\|\big{]}\] \[\quad+C_{f}\bigg{\|}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{ j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{\|}+C_{f}\bigg{\|}\frac{1}{n_{-}}\sum_{X_{j} \in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{\|}\] \[\quad+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\rho_{g}C_{f}\big{[}\|( h_{i}(\hat{\mathbf{w}}_{t})-v_{i,t}\|^{2}+\|h_{j}(\hat{\mathbf{w}}_{t}))-v_{j,t}\|^{2} \big{]}+\frac{\rho_{g}C_{f}}{2}\|\hat{s}_{i,t}-s_{i,t}\|^{2}\bigg{]}\] \[\quad+C_{f}C_{g}\bigg{[}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_ {i}(\mathbf{w}_{t})-v_{i,t}\|+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}( \mathbf{w}_{t})-v_{j,t}\|\bigg{]}+\frac{C_{f}C_{g}L_{h}}{2}\|\hat{\mathbf{w}}_{ t}-\mathbf{w}_{t}\|^{2}\Bigg{\}}\] \[=F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})+ \frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta(F(\hat{\mathbf{w}}_{t},\hat {\mathbf{s}}_{t},\hat{s}_{t}^{\prime})-F(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^ {\prime}))\] \[\quad+\bar{\rho}\eta\Bigg{\{}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}} \bigg{[}\frac{2C_{f}C_{g}}{n_{-}}\sum_{X_{j}\in S_{-}}\big{[}\|h_{i}(\mathbf{w }_{t})-v_{i,t}\|+\|h_{j}(\mathbf{w}_{t}))-v_{j,t}\|\big{]}\] \[\quad+\frac{2\rho_{g}C_{f}}{n_{-}}\sum_{X_{j}\in S_{-}}\big{[}\|h _{i}(\mathbf{w}_{t})-v_{i,t}\|^{2}+\|h_{j}(\mathbf{w}_{t}))-v_{j,t}\|^{2} \big{]}+4\rho_{g}C_{f}C_{h}\|\hat{\mathbf{w}}_{t}-\mathbf{w}_{t}\|^{2}\] \[\quad+2C_{f}\bigg{\|}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_ {j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{\|}+\frac{\rho_{g}C_{f}}{2}\|\hat{s}_{i,t}-s _{i,t}\|^{2}\bigg{]}+\frac{C_{f}C_{g}L_{h}}{2}\|\hat{\mathbf{w}}_{t}-\mathbf{w} _{t}\|^{2}\Bigg{\}}\]

where (a) follows from the Lipschitz continuity of \(f_{i},g_{i},h_{i},h_{j}\) and inequality 40.

Due to the \(\rho_{F}\)-weak convexity of \(F(\mathbf{w},\mathbf{s}_{i},s^{\prime})\), we have (\(\bar{\rho}-\rho_{F}\))-strong convexity of \((\mathbf{w},s_{i},s^{\prime})\mapsto F(\mathbf{w},\mathbf{s},s^{\prime})+ \frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-(\mathbf{ w},\mathbf{s},s^{\prime})\|^{2}\). Then it follows

\[F(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})- F_{i}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime}) =\bigg{[}F_{i}(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{ t}^{\prime})+\frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-( \hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})\|^{2}\bigg{]}\] (42) \[\quad-\bigg{[}F_{i}(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})+ \frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{\prime})-(\mathbf{ w}_{t},\mathbf{s}_{t},s_{t}^{\prime})\|^{2}\bigg{]}\] \[\quad-\frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s}_{t},s_{t}^{ \prime})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{\prime})\|^{2}\] \[\leq(\frac{\rho_{F}}{2}-\bar{\rho})\|(\mathbf{w}_{t},\mathbf{s}_{t },s_{t}^{\prime})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}_{t}^{ \prime})\|^{2}\]Plugging inequality 42 back into 41, we obtain

\[\mathbb{E}_{t}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1},\mathbf{s}_{t+1},s ^{\prime}_{t+1})]\] \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t} )+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta\Bigg{\{}\frac{1}{n_{+}} \sum_{X_{i}\in S_{+}}\bigg{[}(\frac{\rho_{F}}{2}-\bar{\rho})\|(\mathbf{w}_{t}, \mathbf{s}_{t},s^{\prime}_{t})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat {s}^{\prime}_{t})\|^{2}\] \[\quad+\frac{2C_{f}C_{g}}{n_{-}}\sum_{X_{j}\in S_{-}}\big{[}\|h_{i }(\mathbf{w}_{t})-v_{i,t}\|+\|h_{j}(\mathbf{w}_{t})-v_{j,t}\|\big{]}\] \[\quad+\frac{2\rho_{g}C_{f}}{n_{-}}\sum_{X_{j}\in S_{-}}\big{[}\|h _{i}(\mathbf{w}_{t})-v_{i,t}\|^{2}+\|h_{j}(\mathbf{w}_{t})-v_{j,t}\|^{2}\big{]}\] \[\quad+2C_{f}\bigg{\|}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_ {j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{\|}+\frac{\rho_{g}C_{f}}{2}\|\hat{s}_{i,t}- s_{i,t}\|^{2}\bigg{]}+(4\rho_{g}C_{f}C_{h}+\frac{C_{f}C_{g}L_{h}}{2})\|\hat{\mathbf{w}}_{t}- \mathbf{w}_{t}\|^{2}\Bigg{\}}\] \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t} )+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}+\bar{\rho}\eta\Bigg{\{}\frac{1}{n_{+}} \sum_{X_{i}\in S_{+}}\bigg{[}-\frac{\bar{\rho}}{2}\|(\mathbf{w}_{t},\mathbf{s} _{t},s^{\prime}_{t})-(\hat{\mathbf{w}}_{t},\hat{\mathbf{s}}_{t},\hat{s}^{ \prime}_{t})\|^{2}\] \[\quad+\frac{C_{1}}{n_{-}}\sum_{X_{j}\in S_{-}}\big{[}\|h_{i}( \mathbf{w}_{t})-v_{i,t}\|+\|h_{j}(\mathbf{w}_{t})-v_{j,t}\|+\|h_{i}(\mathbf{w }_{t})-v_{i,t}\|^{2}+\|h_{j}(\mathbf{w}_{t})-v_{j,t}\|^{2}\big{]}\] \[\quad+C_{1}\bigg{\|}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_ {j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{\|}\bigg{\}}\] \[\overset{(b)}{\leq} F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t} )+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{\eta}{2}\|\nabla F_{1/\bar{\rho}}( \mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}\] \[\quad+\frac{\bar{\rho}\eta C_{1}}{n_{+}n_{-}}\sum_{X_{i}\in S_{+} }\sum_{X_{j}\in S_{-}}\big{[}\|h_{i}(\mathbf{w}_{t})-v_{i,t}\|+\|h_{j}(\mathbf{ w}_{t})-v_{j,t}\|+\|h_{i}(\mathbf{w}_{t})-v_{i,t}\|^{2}+\|h_{j}(\mathbf{w}_{t})-v_{j,t} \|^{2}\big{]}\] \[\quad+\frac{\bar{\rho}\eta C_{1}}{n_{+}}\sum_{X_{i}\in S_{+}} \bigg{\|}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})-u_{ i,t}\bigg{\|}\]

where in inequality (a) we use \(\bar{\rho}=\rho_{F}+\rho_{g}C_{f}+8\rho_{g}C_{f}C_{h}+C_{f}C_{g}L_{h}\) and \(C_{1}=\max\{2C_{f}C_{g},2\rho_{g}C_{f},2C_{f}\}\), and inequality (b) uses Lemma 3.2.

With general error bounds

\[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\mathbb{E}[\|h_{i}(\mathbf{w }_{t})-v_{i,t}\|]\leq(1-\mu_{1})^{t}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i }(\mathbf{w}_{0})-v_{i,0}\|+R_{1},\] \[\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\mathbb{E}[\|h_{j}(\mathbf{w }_{t})-v_{j,t}\|]\leq(1-\mu_{2})^{t}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j }(\mathbf{w}_{0})-v_{j,0}\|+R_{2},\] \[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\mathbb{E}[\|h_{i}(\mathbf{w }_{t})-v_{i,t}\|^{2}]\leq(1-\mu_{1})^{t}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{ i}(\mathbf{w}_{0})-v_{i,0}\|^{2}+R_{3},\] \[\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\mathbb{E}[\|h_{j}(\mathbf{w }_{t})-v_{j,t}\|^{2}]\leq(1-\mu_{2})^{t}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{ j}(\mathbf{w}_{0})-v_{j,0}\|^{2}+R_{4},\] \[\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\mathbb{E}\bigg{[}\bigg{\|}\frac {1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})-u_{i,t}\bigg{\|} \bigg{]}\] \[\quad\leq(1-\mu_{3})^{t}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}} \bigg{\|}\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})-u_{ i,0}\bigg{\|}+R_{5},\]we have

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t+1},\mathbf{s}_{t+1},s^{ \prime}_{t+1})]\] \[\leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{ \prime}_{t})]+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{\eta}{2}\mathbb{E}[\| \nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\quad+\bar{\rho}\eta C_{1}\bigg{[}(1-\mu_{1})^{t}\frac{1}{n_{+}} \sum_{X_{i}\in S_{+}}\|h_{i}(\mathbf{w}_{0})-v_{i,0}\|+(1-\mu_{2})^{t}\frac{1} {n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{w}_{0})-v_{j,0}\|\] \[\quad+(1-\mu_{1})^{t}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i}( \mathbf{w}_{0})-v_{i,0}\|^{2}+(1-\mu_{2})^{t}\frac{1}{n_{-}}\sum_{X_{j}\in S_ {-}}\|h_{j}(\mathbf{w}_{0})-v_{j,0}\|^{2}\] \[\quad+(1-\mu_{3})^{t}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\bigg{\|} \frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})-u_{i,0}\bigg{\|} +R_{1}+R_{2}+R_{3}+R_{4}+R_{5}\bigg{]}\] \[\leq\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^ {\prime}_{t})]+\frac{3\eta^{2}\bar{\rho}M^{2}}{2}-\frac{\eta}{2}\mathbb{E}[\| \nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\quad+\bar{\rho}\eta C_{1}\bigg{[}(1-\mu_{min})^{t}\bigg{(}\frac{1 }{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i}(\mathbf{w}_{0})-v_{i,0}\|+\frac{1}{n_{-}} \sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{w}_{0})-v_{j,0}\|\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i}(\mathbf{w}_{0} )-v_{i,0}\|^{2}+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{w}_{0})-v_ {j,0}\|^{2}\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\bigg{\|}\frac{1}{n_{-} }\sum_{X_{j}\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})-u_{i,0}\bigg{\|}\bigg{)}+R _{1}+R_{2}+R_{3}+R_{4}+R_{5}\bigg{]}\]

where \(\mu_{min}=\min\{\mu_{1},\mu_{2},\mu_{3}\}\).

Taking summation from \(t=0\) to \(T-1\) yields

\[\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{T},\mathbf{s}_{T},s^{ \prime}_{T})]\] \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{ 0})+\frac{3\eta^{2}\bar{\rho}M^{2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\mathbb{E} [\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}]\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i}(\mathbf{w}_{0} )-v_{i,0}\|^{2}+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{w}_{0})-v_ {j,0}\|^{2}\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\bigg{\|}\frac{1}{n_{-} }\sum_{X_{j}\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})-u_{i,0}\bigg{\|}\bigg{)}+T( R_{1}+R_{2}+R_{3}+R_{4}+R_{5})\bigg{]}\] \[\leq F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{ 0})+\frac{3\eta^{2}\bar{\rho}M^{2}T}{2}-\frac{\eta}{2}\sum_{t=0}^{T-1}\|\nabla F _{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}\] \[\quad+\bar{\rho}\eta C_{1}\bigg{[}\frac{\Delta_{0}}{\mu_{min}}+T( R_{1}+R_{2}+R_{3}+R_{4}+R_{5})\bigg{]}\]

where we use \(\sum_{t=0}^{T-1}(1-\mu_{min})^{t}\leq\frac{1}{\mu_{min}}\) and define constant \(\Delta_{0}\) such that

\[\bigg{(}\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i}(\mathbf{w}_{0} )-v_{i,0}\|+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{w}_{0})-v_{j,0}\|\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\|h_{i}(\mathbf{w}_{0} )-v_{i,0}\|^{2}+\frac{1}{n_{-}}\sum_{X_{j}\in S_{-}}\|h_{j}(\mathbf{w}_{0})-v_ {j,0}\|^{2}\] \[\quad+\frac{1}{n_{+}}\sum_{X_{i}\in S_{+}}\bigg{\|}\frac{1}{n_{-} }\sum_{X_{j}\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})-u_{i,0}\bigg{\|}\bigg{)} \leq\Delta_{0}.\]Then it follows

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t}, \mathbf{s}_{t},s^{\prime}_{t})\|^{2}\] \[\leq\frac{2}{\eta T}\Bigg{[}F_{1/\bar{\rho}}(\mathbf{w}_{0}, \mathbf{s}_{0},s^{\prime}_{0})-\mathbb{E}[F_{1/\bar{\rho}}(\mathbf{w}_{T}, \mathbf{s}_{T},s^{\prime}_{T})]+\frac{3\eta^{2}\bar{\rho}M^{2}T}{2}\] \[\quad+\bar{\rho}\eta C_{1}\bigg{[}\frac{\Delta_{0}}{\mu_{min}}+T( R_{1}+R_{2}+R_{3}+R_{4}+R_{5})\bigg{]}\Bigg{]}\] \[\leq\frac{2\Delta}{\eta T}+(2+\frac{n_{+}}{B_{1}})\eta\bar{\rho }M^{2}+\frac{2\bar{\rho}C_{1}\Delta_{0}}{\mu_{min}T}+2\bar{\rho}C_{1}(R_{1}+R _{2}+R_{3}+R_{4}+R_{5})\] \[=\mathcal{O}\left(\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_{min}} )+\eta+R_{1}+R_{2}+R_{3}+R_{4}+R_{5}\right)\]

where we define constant \(\Delta\) such that \(F_{1/\bar{\rho}}(\mathbf{w}_{0},\mathbf{s}_{0},s^{\prime}_{0})-\mathbb{E}[F_{ 1/\bar{\rho}}(\mathbf{w}_{T},\mathbf{s}_{T},s^{\prime}_{T})]\leq\Delta\).

With MSVR updates for \(v_{i,t}\) and \(u_{i,t}\), following from Lemma C.5 and Lemma C.6, we have

\[\mu_{1}=\frac{B_{1}\tau_{1}}{2n_{+}},\quad\mu_{2}=\frac{B_{1} \tau_{1}}{2n_{-}},\quad\mu_{3}=\frac{B_{1}\tau_{2}}{2n_{+}}\] \[R_{1}=\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{4n_{+}C_{h }M\eta}{B_{1}\tau_{1}^{1/2}},\quad R_{2}=\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{ 1/2}}+\frac{4n_{-}C_{h}M\eta}{B_{2}\tau_{1}^{1/2}}\] \[R_{3}=\frac{4\tau_{1}\sigma^{2}}{B_{3}}+\frac{16n_{+}^{2}C_{h}^{ 2}M^{2}\eta^{2}}{B_{1}^{2}\tau_{1}},\quad R_{4}=\frac{4\tau_{1}\sigma^{2}}{B_ {3}}+\frac{16n_{-}^{2}C_{h}^{2}M^{2}\eta^{2}}{B_{2}^{2}\tau_{1}}\] \[R_{5}=\frac{2\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+C_{2}\frac{n_{+} }{B_{1}}(\frac{B_{1}^{1/2}}{n_{+}^{1/2}}+\frac{B_{2}^{1/2}}{n_{-}^{1/2}})\frac {\tau_{1}}{\tau_{2}^{1/2}}+C_{2}\frac{n_{+}}{B_{1}}(\frac{n_{+}^{1/2}}{B_{1}^{ 1/2}}+\frac{n_{-}^{1/2}}{B_{2}^{1/2}})\frac{\eta}{\tau_{2}^{1/2}}+C_{2}\frac{ n_{+}^{1/2}\eta}{B_{1}\tau_{2}^{1/2}}\]

Then we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_ {t},\mathbf{s}_{t},s^{\prime}_{t})\|^{2}\] \[\leq\mathcal{O}\Bigg{(}\frac{1}{T}(\frac{1}{\eta}+\frac{1}{\mu_ {min}})+(\frac{\tau_{1}^{1/2}}{B_{3}^{1/2}}+\frac{\tau_{2}^{1/2}}{B_{2}^{1/2}})\sigma\] \[\quad+\frac{n_{+}\eta}{B_{1}\tau_{1}^{1/2}}+\frac{n_{-}\eta}{B_{2 }\tau_{1}^{1/2}}+\frac{n_{+}}{B_{1}}\max\{\frac{B_{1}^{1/2}}{n_{+}^{1/2}}, \frac{B_{2}^{1/2}}{n_{-}^{1/2}}\}\frac{\tau_{1}}{\tau_{2}^{1/2}}+\frac{n_{+}}{ B_{1}}\max\{\frac{n_{+}^{1/2}}{B_{1}^{1/2}},\frac{n_{-}^{1/2}}{B_{2}^{1/2}}\} \frac{\eta}{\tau_{2}^{1/2}}\Bigg{)}\]

Setting

\[\tau_{1}=\mathcal{O}\left(\min\left\{B_{3},\frac{B_{1}}{n_{+}}\min\{\frac{n_{ +}^{1/2}}{B_{1}^{1/2}},\frac{n_{-}^{1/2}}{B_{2}^{1/2}}\}B_{2}^{1/2}\right\} \epsilon^{4}\right),\quad\tau_{2}=\mathcal{O}(B_{2}\epsilon^{4}),\]

\[\eta=\mathcal{O}\left(\min\left\{\min\{\frac{B_{1}}{n_{+}},\frac{B_{2}}{n_{-}} \}\min\{B_{3}^{1/2},\frac{B_{1}^{1/2}}{n_{+}^{1/2}}\min\{\frac{n_{+}^{1/4}}{B_{1 }^{1/4}},\frac{n_{-}^{1/4}}{B_{2}^{1/4}}\}B_{2}^{1/4}\},\frac{B_{1}}{n_{+}} \min\{\frac{B_{1}^{1/2}}{n_{+}^{1/2}},\frac{B_{2}^{1/2}}{n_{-}^{1/2}}\}B_{3}^ {1/2}\right\}\epsilon^{4}\right),\]

Then with

\[T\geq\mathcal{O}\left(\max\left\{\max\{\max\{\max\{\max\{\frac{n_{+}}{B_{1}}, \frac{n_{-}}{B_{2}}\}\max\{\frac{1}{B_{3}^{1/2}},\frac{n_{+}^{1/2}}{B_{1}^{1/2}} \max\{\frac{B_{1}^{1/4}}{n_{+}^{1/4}},\frac{B_{2}^{1/4}}{n_{-}^{1/4}}\}\frac{1}{ B_{2}^{1/4}}\}\},\frac{n_{+}}{B_{1}}\max\{\frac{n_{+}^{1/2}}{B_{1}^{1/2}},\frac{n_{-}^{1/2}}{B_{2}^{1/ 2}}\}\frac{1}{B_{2}^{1/2}}\right\}\epsilon^{-6}\right)\]

iterations, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla F_{1/\bar{\rho}}(\mathbf{w}_{t},\mathbf{s}_ {t},s^{\prime}_{t})\|^{2}\leq\epsilon^{2}.\]Proofs of Lemmas and Propositions

### Additional Proposition

**Proposition D.1**.: _Consider a Lipschitz continuous function \(f:O\to\mathbb{R}\) where \(O\subset\mathbb{R}^{d}\) is an open set. Assume \(f\) to be non-increasing (resp. non-decreasing) with respect to each element in the input, then all subgradients of \(f\) are element-wise non-positive (resp. non-negative)._

Proof of Proposition D.1.: Let \(D\) be the subset of \(O\) where \(f\) is differentiable. By Theorem 9.60 in [24], a Lipschitz continuous function \(f:O\to\mathbb{R}\), where \(O\subset\mathbb{R}^{d}\) is an open set, is differentiable almost everywhere, i.e., \(D\) is dense in \(O\). Then by Theorem 9.61 in [24], the subdifferential of \(f\) at \(x\) is defined as

\[\partial f(x)=\text{con}\{v|\exists x_{k}\to x\text{ with }x_{k}\in D,\nabla f (x_{k})\to v\},\]

where con denotes the convex hull. If we assume that \(f\) is non-increasing with respect to each element in the input, then \(\nabla f(x)\leq 0\) (element-wise) for all differentiable points \(x\in D\). It implies that the all vectors in \(\{v|\exists x_{k}\to x\text{ with }x_{k}\in D,\nabla f(x_{k})\to v\}\) are element-wise non-positive. Therefore, all subgradients of \(f\) are element-wise non-positive. On the other hand, if we assume that \(f\) is non-decreasing, one may follow the same argument and conclude that all subgradients of \(f\) are element-wise non-negative. 

For functions \(f:O\to\mathbb{R}^{m}\) where \(O\subset\mathbb{R}^{d}\) is an open set, one may write \(f=(f_{1},\ldots,f_{m})\) and apply the above proposition for each \(f_{k},k=1,\ldots,m\).

### Proofs of Proposition 4.2 and Proposition 4.4

To prove Proposition 4.2 and Proposition 4.4, we first present the following proposition on the weak-convexity of composition functions.

**Proposition D.2**.: _Assume \(f:\mathbb{R}^{d}\to\mathbb{R}\) is \(\rho_{1}\)-weakly-convex and \(C_{1}\)-Lipschitz continuous, \(g:\mathbb{R}^{\bar{d}}\to\mathbb{R}^{d}\) is \(C_{2}\)-Lipschitz continuous, and either of the followings holds:_

1. \(f(\cdot)\) _is monotone and_ \(g(\cdot)\) _is_ \(L_{2}\)_-smooth;_
2. \(f(\cdot)\) _is non-decreasing and_ \(g(\cdot)\) _is_ \(L_{2}\)_-weakly-convex,_

_then \(f\circ g\) is \(\tilde{\rho}\)-weakly-convex with \(\tilde{\rho}=\sqrt{d}L_{2}C_{1}+\rho_{1}C_{2}^{2}\)._

Proof of Proposition D.2.: The weak convexity of \(f\) implies

\[f(g(y)) \geq f(g(x))+v^{\top}(g(y)-g(x))-\frac{\rho_{1}}{2}\|g(y)-g(x)\|^ {2}\] \[\geq f(g(x))+v^{\top}(g(y)-g(x))-\frac{\rho_{1}C_{2}^{2}}{2}\|x-y \|^{2}\]

where \(v\in\partial f(g(x))\). Moreover, due to the smoothness of \(g(\cdot)\) (or weakly-convexity of \(g(\cdot)\), then only the second inequality holds), we have

\[g(y)-g(x) \leq\nabla g(x)^{\top}(y-x)+\mathbf{v}\left(\frac{L_{2}}{2}\|x-y \|^{2}\right),\] (43) \[g(y)-g(x) \geq\nabla g(x)^{\top}(y-x)-\mathbf{v}\left(\frac{L_{2}}{2}\|x- y\|^{2}\right).\]

where \(\mathbf{v}(e)\) denotes a \(d\)-dimensional vector with value \(e\) on each dimensions. We first assume that \(f\) is non-increasing, then we may use the first inequality in (43) and the Lipschitz continuity of \(g\) to get

\[f(g(y)) \geq f(g(x))+v^{\top}\left[\nabla g(x)^{\top}(y-x)+\mathbf{v} \left(\frac{L_{2}}{2}\|x-y\|^{2}\right)\right]-\frac{\rho_{1}C_{2}^{2}}{2}\|x- y\|^{2}\] \[\geq f(g(x))+v^{\top}\nabla g(x)^{\top}(y-x)+v^{\top}\mathbf{v} \left(\frac{L_{2}}{2}\|x-y\|^{2}\right)-\frac{\rho_{1}C_{2}^{2}}{2}\|x-y\|^{2}\] \[\geq f(g(x))+\langle v^{\top}\nabla g(x)^{\top}(y-x)-\frac{\sqrt{ d}L_{2}C_{1}+\rho_{1}C_{2}^{2}}{2}\|x-y\|^{2}.\]On the other hand, if we assume \(f\) is non-decreasing, the same result follows from the second inequality in (43). Thus \(f\circ g\) is \(\tilde{\rho}\)-weakly-convex with \(\tilde{\rho}_{g}=\sqrt{d}L_{2}C_{1}+\rho_{1}C_{2}^{2}\). 

Proof of Proposition 4.2.: Under Assumption 4.1, Proposition D.2 directly implies the \(\rho_{F}\)-weak-convexity of \(F(\mathbf{w})\) with \(\rho_{F}=\sqrt{d_{1}}\rho_{g}C_{f}+\rho_{f}C_{g}^{2}\). 

Proof of Proposition 4.4.: Under Assumption 4.3, we first apply Proposition D.2 to the composite function \(g_{i}(h_{i,j}(\cdot))\) and obtain its \(\rho_{\tilde{g}}=\sqrt{d_{2}}L_{h}C_{g}+\rho_{g}C_{h}^{2}\)-weak-convexity. To show it Lipschitz continuity, we use the Lipschitz continuity of \(g_{i}\) and \(h_{i,j}\) to obtain

\[\|g_{i}(h_{i,j}(\mathbf{w}))-g_{i}(h_{i,j}(\tilde{\mathbf{w}}))\|^{2}\leq C_{ g}^{2}C_{h}^{2}\|\mathbf{w}-\tilde{\mathbf{w}}\|^{2}.\]

Thus \(g_{i}(h_{i,j}(\mathbf{w}))\) is \(C_{\tilde{g}}=C_{g}C_{h}\)-Lipschitz-continuous

Since we assume \(f_{i}(\cdot)\) is non-decreasing, \(\rho_{f}\)-weakly-convex and \(C_{f}\)-Lipschitz continuous, and \(g_{i}(h_{i,j}(\cdot))\) is \(\rho_{\tilde{g}}\)-weakly-convex and \(C_{\tilde{g}}\)-Lipschitz-continuous, we apply Proposition D.2 again to conclude that \(F(\cdot)\) is \(\rho_{F}=\sqrt{d_{1}}\rho_{\tilde{g}}C_{f}+\rho_{f}C_{\tilde{g}}^{2}\)-weakly-convex. 

### Proof of Lemma 4.5

Proof of Lemma 4.5.: With \(\gamma=\frac{n_{1}-B_{1}}{B_{1}(1-\tau)}+(1-\tau)\), \(\tau\leq\frac{1}{2}\), MSVR update gives recursive error bound [15]

\[\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{n_{1}})\mathbb{E}[\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|^{2}]+\frac{2\tau^{2}B_{1}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{ 1}C_{g}^{2}}{B_{1}}\mathbb{E}[\|\mathbf{w}_{t}-\mathbf{w}_{t+1}\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{n_{1}})\mathbb{E}[\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|^{2}]+\frac{2\tau^{2}B_{1}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{ 1}C_{g}^{2}}{B_{1}}\eta^{2}\mathbb{E}[\|G_{t}\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{2}\mathbb{E}[\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|^{2}]+\frac{2\tau^{2}B_{1}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{ 1}C_{g}^{2}M^{2}\eta^{2}}{B_{1}}\]

Applying this inequality recursively, we obtain

\[\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{ w}_{0})\|^{2}+\sum_{j=0}^{t}(1-\frac{B_{1}\tau}{2n_{1}})^{2(t-j)}\bigg{(}\frac{2 \tau^{2}B_{1}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{+}C_{g}^{2}M^{2}\eta^{2}}{B_{1}} \bigg{)}\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{ w}_{0})\|^{2}+\frac{4\tau\sigma^{2}}{B_{2}}+\frac{16n_{1}^{2}C_{g}^{2}M^{2} \eta^{2}}{B_{1}^{2}\tau}\]

where we use \(\sum_{j=0}^{t}(1-\frac{B_{1}\tau}{2n_{1}})^{2(t-j)}\leq\frac{2n_{1}}{B_{1}\tau}\).

It follows

\[\mathbb{E}\left[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|\right]^{2}\] \[\leq\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{ w}_{0})\|^{2}+\frac{4\tau\sigma^{2}}{B_{2}}+\frac{16n_{1}^{2}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^{ 2}\tau}\] \[\leq\bigg{[}(1-\frac{B_{1}\tau}{2n_{1}})^{t+1}\|u_{i,0}-g_{i}( \mathbf{w}_{0})\|+\frac{2\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4n_{1}C_{g}M\eta} {B_{1}\tau^{1/2}}\bigg{]}^{2}\]

Thus

\[\mathbb{E}\bigg{[}\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|\bigg{]}\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{t+1}\|u_{i,0}-g_{i}(\mathbf{w} _{0})\|+\frac{2\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4n_{1}C_{g}M\eta}{B_{1}\tau^ {1/2}}\]Taking summation over \(i\in\mathcal{S}\), we obtain the desired result

\[\mathbb{E}\bigg{[}\frac{1}{n}\sum_{i\in\mathcal{S}}\|u_{i,t+1}-g_{i}( \mathbf{w}_{t+1})\|\bigg{]}\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{t+1}\frac{1}{n}\sum_{i\in \mathcal{S}}\|u_{i,0}-g_{i}(\mathbf{w}_{0})\|+\frac{2\tau^{1/2}\sigma}{B_{2}^{ 1/2}}+\frac{4nC_{g}M\eta}{B_{1}\tau^{1/2}}\]

### Proof of Lemma c.6

Proof of Lemma c.6.: With \(\gamma_{3}=\frac{n_{+}-B_{1}}{B_{1}(1-\tau_{2})}+(1-\tau_{2})\) and \(\tau_{2}\leq\frac{1}{2}\), MSVR update gives the following recursive error bound [15]

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j, t+1}-v_{i,t+1},s_{i,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{n_{+}})\mathbb{E}[\|u_{i,t}-\frac{1}{ n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{+}B_{2}}\] \[\quad+\frac{8n_{+}C_{g}^{2}}{B_{1}}\mathbb{E}[\|(v_{j,t}-v_{i,t}, s_{i,t})-(v_{j,t+1}-v_{i,t+1},s_{i,t+1})\|^{2}]\] (44) \[\leq(1-\frac{B_{1}\tau_{2}}{n_{+}})\mathbb{E}[\|u_{i,t}-\frac{1}{ n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{+}B_{2}}\] \[\quad+\frac{16n_{+}C_{g}^{2}}{B_{1}}\mathbb{E}[\|v_{i,t}-v_{i,t+1} \|^{2}+\|v_{j,t}-v_{j,t+1}\|^{2}]+\frac{8C_{g}^{2}M^{2}\eta^{2}}{B_{1}}\]

It remains to bound \(\mathbb{E}[\|v_{i,t}-v_{i,t+1}\|^{2}]\) and \(\mathbb{E}[\|v_{j,t}-v_{j,t+1}\|^{2}]\). We bound the former, and the latter's bound naturally follows. Consider the update of \(v_{i,t+1}\) and we have

\[\mathbb{E}[\|v_{i,t}-v_{i,t+1}\|^{2}]\] \[\leq\mathbb{E}\left[\frac{B_{1}}{n_{+}}\|\tau_{1}v_{i,t}-\tau_{1} h^{(i)}(\mathbf{w}_{t};\mathcal{B}_{3,i}^{t})-\gamma_{1}(h^{(i)}(\mathbf{w}_{t}; \mathcal{B}_{3,i}^{t})-h^{(i)}(\mathbf{w}_{t-1};\mathcal{B}_{3,i}^{t}))\|^{2}\right]\] \[\leq\mathbb{E}\left[\frac{2B_{1}\tau_{1}^{2}}{n_{+}}\|v_{i,t}-h^{ (i)}(\mathbf{w}_{t};\mathcal{B}_{3,i}^{t})\|^{2}+\frac{2B_{1}\gamma_{1}^{2}}{n_ {+}}\|h^{(i)}(\mathbf{w}_{t};\mathcal{B}_{3,i}^{t})-h^{(i)}(\mathbf{w}_{t-1}; \mathcal{B}_{3,i}^{t})\|^{2}\right]\] \[\leq\mathbb{E}\left[\frac{2B_{1}\tau_{1}^{2}}{n_{+}}\|v_{i,t}-h^{ (i)}(\mathbf{w}_{t};\mathcal{B}_{3,i}^{t})\|^{2}+\frac{2B_{1}\gamma_{1}^{2}C_{ h}}{n_{+}}\|\mathbf{w}_{t}-\mathbf{w}_{t-1}\|^{2}\right]\] \[\overset{(a)}{\leq}\frac{8B_{1}\tau_{1}^{2}M^{2}}{n_{+}}+\frac{8 n_{+}C_{h}^{2}\eta^{2}M^{2}}{B_{1}}\]

where inequality (a) uses \(\tau_{1}\leq 1/2\) and \(\gamma_{1}=\frac{n_{+}-B_{1}}{B_{1}(1-\tau_{1})}+(1-\tau_{1})\leq\frac{2n_{+}}{ B_{1}}\). Plugging the above inequality back into inequality 44 gives

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j, t+1}-v_{i,t+1},s_{i,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{n_{+}})\mathbb{E}[\|u_{i,t}-\frac{1}{ n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{+}B_{2}}\] \[\quad+\frac{16n_{+}C_{g}^{2}}{B_{1}}\left(8\tau_{1}^{2}M^{2}( \frac{B_{1}}{n_{+}}+\frac{B_{2}}{n_{-}})+8C_{h}^{2}\eta^{2}M^{2}(\frac{n_{+}}{ B_{1}}+\frac{n_{-}}{B_{2}})\right)+\frac{8C_{g}^{2}M^{2}\eta^{2}}{B_{1}}\] \[\leq(1-\frac{B_{1}\tau_{2}}{n_{+}})\mathbb{E}[\|u_{i,t}-\frac{1}{ n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j,t}-v_{i,t},s_{i,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{+}B_{2}}\] \[\quad+128C_{g}^{2}M^{2}\frac{n_{+}}{B_{1}}(\frac{B_{1}}{n_{+}}+ \frac{B_{2}}{n_{-}})\tau_{1}^{2}+128C_{g}^{2}C_{h}^{2}M^{2}\frac{n_{+}}{B_{1}}( \frac{n_{+}}{B_{1}}+\frac{n_{-}}{B_{2}})\eta^{2}+\frac{8C_{g}^{2}M^{2}\eta^{2}}{B _{1}}\]Applying this inequality recursively, we obtain

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j,t+1 }-v_{i,t+1},s_{i,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{2(t+1)}\|u_{i,0}-\frac{1}{n_ {-}}\sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|^{2}+\sum_{j=0}^{t}(1- \frac{B_{1}\tau_{2}}{2n_{+}})^{2(t-j)}\bigg{(}\frac{2\tau_{2}^{2}B_{1}\sigma^{2 }}{n_{+}B_{2}}\] \[\quad+128C_{g}^{2}M^{2}\frac{n_{+}}{B_{1}}(\frac{B_{1}}{n_{+}}+ \frac{B_{2}}{n_{-}})\tau_{1}^{2}+128C_{g}^{2}C_{h}^{2}M^{2}\frac{n_{+}}{B_{1}} (\frac{n_{+}}{B_{1}}+\frac{n_{-}}{B_{2}})\eta^{2}+\frac{8C_{g}^{2}M^{2}\eta^{2 }}{B_{1}}\bigg{)}\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{2(t+1)}\|u_{i,0}-\frac{1}{n_ {-}}\sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|^{2}+\frac{4\tau_{2}\sigma ^{2}}{B_{2}}\] \[\quad+256C_{g}^{2}M^{2}\frac{n_{+}^{2}}{B_{1}^{2}}(\frac{B_{1}}{n _{+}}+\frac{B_{2}}{n_{-}})\frac{\tau_{1}^{2}}{\tau_{2}}+256C_{g}^{2}C_{h}^{2} M^{2}\frac{n_{+}^{2}}{B_{1}^{2}}(\frac{n_{+}}{B_{1}}+\frac{n_{-}}{B_{2}})\frac{ \eta^{2}}{\tau_{2}}+\frac{16n_{+}C_{g}^{2}M^{2}\eta^{2}}{B_{1}^{2}\tau_{2}}\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{2(t+1)}\|u_{i,0}-\frac{1}{n_ {-}}\sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|^{2}+\frac{4\tau_{2}\sigma ^{2}}{B_{2}}+C_{2}^{2}\frac{n_{+}^{2}}{B_{1}^{2}}(\frac{B_{1}}{n_{+}}+\frac{B_ {2}}{n_{-}})\frac{\tau_{1}^{2}}{\tau_{2}}\] \[\quad+C_{2}^{2}\frac{n_{+}^{2}}{B_{1}^{2}}(\frac{n_{+}}{B_{1}}+ \frac{n_{-}}{B_{2}})\frac{\eta^{2}}{\tau_{2}}+C_{2}^{2}\frac{n_{+}\eta^{2}}{B_ {1}^{2}\tau_{2}}\]

where we use \(\sum_{j=0}^{t}(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{2(t-j)}\leq\frac{2n_{+}}{B_{1} \tau_{1}}\) and denotes \(C_{2}^{2}=2\max\{256C_{g}^{2}M^{2},256C_{g}^{2}C_{h}^{2}M^{2},16C_{g}^{2}M^{2}\}\). Taking average over \(i\in S_{+}\) gives the squared-norm error bound.

To derive the norm error bound, we derive

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j, t+1}-v_{i,t+1},s_{i,t+1})\|^{2}\] \[\leq\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}( v_{j,t+1}-v_{i,t+1},s_{i,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{2(t+1)}\|u_{i,0}-\frac{1}{ n_{-}}\sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|^{2}+\frac{4\tau_{2}\sigma ^{2}}{B_{2}}+C_{2}^{2}\frac{n_{+}^{2}}{B_{1}^{2}}(\frac{B_{1}}{n_{+}}+\frac{B _{2}}{n_{-}})\frac{\tau_{1}^{2}}{\tau_{2}}\] \[\quad+C_{2}^{2}\frac{n_{+}^{2}}{B_{1}^{2}}(\frac{n_{+}}{B_{1}}+ \frac{n_{-}}{B_{2}})\frac{\eta^{2}}{\tau_{2}}+C_{2}^{2}\frac{n_{+}\eta^{2}}{B _{1}^{2}\tau_{2}}\] \[\leq\bigg{[}(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{t+1}\|u_{i,0}- \frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|+\frac{2\tau_{ 2}^{1/2}\sigma}{B_{2}^{1/2}}+C_{2}\frac{n_{+}}{B_{1}}(\frac{B_{1}^{1/2}}{n_{+} ^{1/2}}+\frac{B_{2}^{1/2}}{n_{-}^{1/2}})\frac{\tau_{1}}{\tau_{2}^{1/2}}\] \[\quad+C_{2}\frac{n_{+}}{B_{1}}(\frac{n_{+}^{1/2}}{B_{1}^{1/2}}+ \frac{n_{-}^{1/2}}{B_{2}^{1/2}})\frac{\eta}{\tau_{2}^{1/2}}+C_{2}\frac{n_{+}^{ 1/2}\eta}{B_{1}\tau_{2}^{1/2}}\bigg{]}^{2}\]

Thus

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j, t+1}-v_{i,t+1},s_{i,t+1})\|]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{t+1}\|u_{i,0}-\frac{1}{n_{-}} \sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|+\frac{2\tau_{2}^{1/2}\sigma}{B _{2}^{1/2}}+C_{2}\frac{n_{+}}{B_{1}}(\frac{B_{1}^{1/2}}{n_{+}^{1/2}}+\frac{B_ {2}^{1/2}}{n_{-}^{1/2}})\frac{\tau_{1}}{\tau_{2}^{1/2}}\] \[\quad+C_{2}\frac{n_{+}}{B_{1}}(\frac{n_{+}^{1/2}}{B_{1}^{1/2}}+ \frac{n_{-}^{1/2}}{B_{2}^{1/2}})\frac{\eta}{\tau_{2}^{1/2}}+C_{2}\frac{n_{+}^{1/2} \eta}{B_{1}\tau_{2}^{1/2}}\]Taking average over \(i\in S_{+}\), we obtain the norm error bound

\[\mathbb{E}\left[\frac{1}{n_{+}}\sum_{i\in S_{+}}\|u_{i,t+1}-\frac{1} {n_{-}}\sum_{j\in S_{-}}g_{i}(v_{j,t+1}-v_{i,t+1},s_{i,t+1})\|\right]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{+}})^{t+1}\frac{1}{n_{+}}\sum_{i \in S_{+}}\|u_{i,0}-\frac{1}{n_{-}}\sum_{j\in S_{-}}g_{i}(v_{i,0}-v_{j,0},s_{i,0})\|+\frac{2\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}\] \[\quad+C_{2}\frac{n_{+}}{B_{1}}(\frac{B_{1}^{1/2}}{n_{+}^{1/2}}+ \frac{B_{2}^{1/2}}{n_{-}^{1/2}})\frac{\tau_{1}}{\tau_{2}^{1/2}}+C_{2}\frac{n_ {+}}{B_{1}}(\frac{n_{+}^{1/2}}{B_{1}^{1/2}}+\frac{n_{-}^{1/2}}{B_{2}^{1/2}}) \frac{\eta}{\tau_{2}^{1/2}}+C_{2}\frac{n_{+}^{1/2}\eta}{B_{1}\tau_{2}^{1/2}}\]

### Proof of Lemma a.3

Proof of Lemma a.3.: With \(\gamma_{1}=\frac{n_{1}n_{2}-B_{1}B_{2}}{B_{1}B_{2}(1-\tau_{1})}+(1-\tau_{1})\) and \(\tau_{1}\leq\frac{1}{2}\), MSVR update has the following recursive error bound [15][15]

\[\mathbb{E}[\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{n_{1}n_{2}})\mathbb{E}[\|v_{i,j, t}-h_{i,j}(\mathbf{w}_{t})\|^{2}]+\frac{2\tau_{1}^{2}B_{1}B_{2}\sigma^{2}}{n_{1}n _{2}B_{3}}+\frac{8n_{1}n_{2}C_{h}^{2}}{B_{1}B_{2}}\mathbb{E}[\|\mathbf{w}_{t}- \mathbf{w}_{t+1}\|^{2}]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{2}\mathbb{E}[\|v_ {i,j,t}-h_{i,j}(\mathbf{w}_{t})\|^{2}]+\frac{2\tau_{1}^{2}B_{1}B_{2}\sigma^{2} }{n_{1}n_{2}B_{3}}+\frac{8n_{1}n_{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}B_{2}}\]

Applying this inequality recursively, we obtain

\[\mathbb{E}[\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{2(t+1)}\|v_{i,j,0} -h_{i,j}(\mathbf{w}_{0})\|^{2}+\sum_{j=0}^{t}(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{ 1}n_{2}})^{2(t-j)}(\frac{2\tau_{1}^{2}B_{1}B_{2}\sigma^{2}}{n_{1}n_{2}B_{3}}+ \frac{8n_{1}n_{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}B_{2}})\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{2(t+1)}\|v_{i,j,0} -h_{i,j}(\mathbf{w}_{0})\|^{2}+\frac{4\tau_{1}\sigma^{2}}{B_{3}}+\frac{16n_{1} ^{2}n_{2}^{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}}\]

where we use \(\sum_{j=0}^{t}(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{2(t-j)}\leq\frac{2n_ {1}n_{2}}{B_{1}B_{2}\tau_{1}}\). Taking average over \((i,j)\in S_{1}\times S_{2}\) gives the squared-norm error bound.

To derive the norm error bound, we derive

\[\mathbb{E}[\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|^{2}\] \[\leq\mathbb{E}[\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{2(t+1)}\|v_{i,j,0} -h_{i,j}(\mathbf{w}_{0})\|^{2}+\frac{4\tau_{1}\sigma^{2}}{B_{3}}+\frac{16n_{1} ^{2}n_{2}^{2}C_{h}^{2}M^{2}\eta^{2}}{B_{1}^{2}B_{2}^{2}\tau_{1}}\] \[\leq\left[(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{t+1}\|v_{i, j,0}-h_{i,j}(\mathbf{w}_{0})\|+\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+ \frac{4n_{1}n_{2}C_{h}M\eta}{B_{1}B_{2}\tau_{1}^{1/2}}\right]^{2}\]

Thus

\[\mathbb{E}[\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\|]\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{t+1}\|v_{i,j,0}-h_ {i,j}(\mathbf{w}_{0})\|+\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{4n_{1} n_{2}C_{h}M\eta}{B_{1}B_{2}\tau_{1}^{1/2}}\]

Taking average over \((i,j)\in\mathcal{S}_{1}\times\mathcal{S}_{2}\), we obtain the norm error bound

\[\mathbb{E}\bigg{[}\frac{1}{n_{1}}\sum_{i\in\mathcal{S}_{1}}\frac{1 }{n_{2}}\sum_{j\in\mathcal{S}_{2}}\|v_{i,j,t+1}-h_{i,j}(\mathbf{w}_{t+1})\| \bigg{]}\] \[\leq(1-\frac{B_{1}B_{2}\tau_{1}}{2n_{1}n_{2}})^{t+1}\frac{1}{n_{1} }\sum_{i\in\mathcal{S}_{1}}\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}\|v_{i,j,0} -h_{i,j}(\mathbf{w}_{0})\|+\frac{2\tau_{1}^{1/2}\sigma}{B_{3}^{1/2}}+\frac{4n_ {1}n_{2}C_{h}M\eta}{B_{1}B_{2}\tau_{1}^{1/2}}.\]

### Proof of Lemma a.4

Proof of Lemma a.4.: With \(\gamma_{2}=\frac{n_{1}-B_{1}}{B_{1}(1-\tau_{2})}+(1-\tau_{2})\) and \(\tau_{2}\leq\frac{1}{2}\), MSVR update has the following recursive error bound [15]

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_ {i}(v_{i,j,t+1})\|^{2}]\] (45) \[\leq(1-\frac{B_{1}\tau_{2}}{n_{1}})\mathbb{E}[\|u_{i,t}-\frac{1}{n_ {2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{1}B_{2}}+\frac{8n_{1}C_{g}^{2}}{B_{1}}\mathbb{E}[\|v_{i,j,t+1}-v_{ i,j,t}\|^{2}]\]It remains to bound \(\mathbb{E}[\|v_{i,j,t+1}-v_{i,j,t}\|^{2}]\), which is done as following

\[\mathbb{E}[\|v_{i,j,t+1}-v_{i,j,t}\|^{2}]\] \[\leq\mathbb{E}\left[\frac{B_{1}B_{2}}{n_{1}n_{2}}\|\tau_{1}v_{i,j, t}-\tau_{1}h_{i,j}(\mathbf{w_{t}};\mathcal{B}^{t}_{3,i,j})-\gamma_{1}(h_{i,j}( \mathbf{w_{t}};\mathcal{B}^{t}_{3,i,j})-h_{i,j}(\mathbf{w_{t-1}};\mathcal{B}^{ t}_{3,i,j}))\|^{2}\right]\] \[\leq\mathbb{E}\left[\frac{2B_{1}B_{2}\tau_{1}^{2}}{n_{1}n_{2}}\|v _{i,j,t}-h_{i,j}(\mathbf{w_{t}};\mathcal{B}^{t}_{3,i,j})\|^{2}+\frac{2B_{1}B_{2 }\tau_{1}^{2}}{n_{1}n_{2}}\|h_{i,j}(\mathbf{w_{t}};\mathcal{B}^{t}_{3,i,j})-h_ {i,j}(\mathbf{w_{t-1}};\mathcal{B}^{t}_{3,i,j})\|^{2}\right]\] \[\leq\mathbb{E}\left[\frac{2B_{1}B_{2}\tau_{1}^{2}}{n_{1}n_{2}}\|v _{i,j,t}-h_{i,j}(\mathbf{w_{t}};\mathcal{B}^{t}_{3,i,j})\|^{2}+\frac{2B_{1}B_{ 2}\gamma_{1}^{2}C_{h}}{n_{1}n_{2}}\|\mathbf{w_{t}}-\mathbf{w_{t-1}}\|^{2}\right]\] \[\overset{(a)}{\leq}\frac{8B_{1}B_{2}\tau_{1}^{2}M^{2}}{n_{1}n_{2} }+\frac{8n_{1}n_{2}C_{h}^{2}\eta^{2}M^{2}}{B_{1}B_{2}}\]

where inequality (a) uses \(\tau_{1}\leq 1/2\) and \(\gamma_{1}=\frac{n_{1}n_{2}-B_{1}B_{2}}{B_{1}B_{2}(1-\tau_{1})}+(1-\tau_{1}) \leq\frac{2n_{1}n_{2}}{B_{1}B_{2}}\). Plugging the above inequality back into inequality 45 gives

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g _{i}(v_{i,j,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{n_{1}})\mathbb{E}[\|u_{i,t}-\frac{1}{ n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{1}B_{2}}+\frac{8n_{1}C_{g}^{2}}{B_{1}}\left(\frac{8B_{1}B_{2} \tau_{1}^{2}M^{2}}{n_{1}n_{2}}+\frac{8n_{1}n_{2}C_{h}^{2}\eta^{2}M^{2}}{B_{1}B _{2}}\right)\] \[\leq(1-\frac{B_{1}\tau_{2}}{n_{1}})\mathbb{E}[\|u_{i,t}-\frac{1}{ n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]+\frac{2\tau_{2}^{2}B_{1} \sigma^{2}}{n_{1}B_{2}}+\frac{64B_{2}\tau_{1}^{2}M^{2}C_{g}^{2}}{n_{2}}+ \frac{64n_{1}^{2}n_{2}C_{h}^{2}\eta^{2}M^{2}C_{g}^{2}}{B_{1}^{2}B_{2}}\]

Applying this inequality recursively, we obtain

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g _{i}(v_{i,j,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{2(t+1)}\|u_{i,0}-\frac{1}{ n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|^{2}+\sum_{j=0}^{t}(1-\frac{B_{1} \tau_{2}}{2n_{1}})^{t-j}\bigg{(}\frac{2\tau_{2}^{2}B_{1}\sigma^{2}}{n_{1}B_{2} }+\frac{64B_{2}\tau_{1}^{2}M^{2}C_{g}^{2}}{n_{2}}\] \[\quad+\frac{64n_{1}^{2}n_{2}C_{h}^{2}\eta^{2}M^{2}C_{g}^{2}}{B_{1 }^{2}B_{2}}\bigg{)}\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{2(t+1)}\|u_{i,0}-\frac{1}{ n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|^{2}+\frac{4\tau_{2}\sigma^{2}}{B_{2} }+\frac{128n_{1}B_{2}\tau_{1}^{2}M^{2}C_{g}^{2}}{B_{1}n_{2}\tau_{2}}+\frac{128 n_{1}^{3}n_{2}C_{h}^{2}\eta^{2}M^{2}C_{g}^{2}}{B_{1}^{3}B_{2}\tau_{2}}\]

where we use \(\sum_{j=0}^{t}(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{2(t-j)}\leq\frac{2n_{1}}{B_{1} \tau_{1}}\). Taking average over \(i\in\mathcal{S}_{1}\) gives the squared-norm error bound.

To derive the norm error bound, we derive

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g _{i}(v_{i,j,t+1})\|]^{2}\] \[\leq\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2 }}g_{i}(v_{i,j,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{2(t+1)}\|u_{i,0}-\frac{1}{ n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|^{2}+\frac{4\tau_{2}\sigma^{2}}{B_{2} }+\frac{128n_{1}B_{2}\tau_{1}^{2}M^{2}C_{g}^{2}}{B_{1}n_{2}\tau_{2}}+\frac{128n_ {1}^{3}n_{2}C_{h}^{2}\eta^{2}M^{2}C_{g}^{2}}{B_{1}^{3}B_{2}\tau_{2}}\] \[\leq\bigg{[}(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{t+1}\|u_{i,0}-\frac{1 }{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|+\frac{2\tau_{2}^{1/2}\sigma}{B_ {2}^{1/2}}+\frac{8\sqrt{2}n_{1}^{1/2}B_{2}^{1/2}\tau_{1}MC_{g}}{B_{1}^{1/2}n_{2 }^{1/2}\tau_{2}^{1/2}}+\frac{8\sqrt{2}n_{1}^{3/2}n_{1}^{1/2}C_{h}\eta MC_{g}}{B_ {1}^{3/2}B_{2}^{1/2}\tau_{2}^{1/2}}\bigg{]}^{2}\]Taking squared root on both sides and taking average over \(i\in S_{1}\), we obtain the norm error bound

\[\mathbb{E}\left[\frac{1}{n_{1}}\sum_{i\in\mathcal{S}_{1}}\|u_{i,t+1} -\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t+1})\|\right]\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{t+1}\frac{1}{n_{1}}\sum_{i \in\mathcal{S}_{1}}\|u_{i,0}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_ {i,j,0})\|+\frac{2\tau_{2}^{1/2}\sigma}{B_{2}^{1/2}}+\frac{C_{2}n_{1}^{1/2}B_{2 }^{1/2}\tau_{1}}{B_{1}^{1/2}n_{2}^{1/2}\tau_{2}^{1/2}}+\frac{C_{2}n_{1}^{3/2}n _{2}^{1/2}\eta}{B_{1}^{3/2}B_{2}^{1/2}\tau_{2}^{1/2}}\]

where \(C_{2}=\max\{8\sqrt{2}MC_{g},8\sqrt{2}C_{h}MC_{g}\}\). 

### Proof of Lemma b.2

Proof of Lemma b.2.: Define

\[\tilde{u}_{i,t}=(1-\tau)u_{i,t}+\tau g_{i}(\mathbf{w}_{t};\mathcal{B}_{2,i}^{ t})\]

Then we have

\[\mathbb{E}_{\mathcal{B}_{2,i}^{t}}[\|\tilde{u}_{i,t}-g_{i}(\mathbf{ w}_{t})\|^{2}]\] \[=\mathbb{E}_{\mathcal{B}_{2,i}^{t}}[\|(1-\tau)(u_{i,t}-g_{i}( \mathbf{w}_{t}))+\tau(g_{i}(\mathbf{w}_{t};\mathcal{B}_{2,i}^{t})-g_{i}( \mathbf{w}_{t}))\|^{2}]\] \[=\mathbb{E}_{\mathcal{B}_{2,i}^{t}}[(1-\tau)^{2}\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|^{2}+\tau^{2}\|g_{i}(\mathbf{w}_{t};\mathcal{B}_{2,i}^{t})-g_ {i}(\mathbf{w}_{t})\|^{2}\] \[\quad+2(1-\tau)\tau\langle u_{i,t}-g_{i}(\mathbf{w}_{t}),g_{i}( \mathbf{w}_{t};\mathcal{B}_{2,i}^{t})-g_{i}(\mathbf{w}_{t})\rangle]\] \[\leq(1-\tau)^{2}\|u_{i,t}-g_{i}(\mathbf{w}_{t})\|^{2}+\frac{\tau ^{2}\sigma^{2}}{B_{2}}\]

It follows

\[\mathbb{E}_{\mathcal{B}_{2,i}^{t}}\mathbb{E}_{\mathcal{B}_{1}^{ t}}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t})\|^{2}]\] \[=\frac{B_{1}}{n_{1}}\mathbb{E}_{\mathcal{B}_{2,i}^{t}}[\|\tilde{u }_{i,t}-g_{i}(\mathbf{w}_{t})\|^{2}]+(1-\frac{B_{1}}{n_{1}})\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|^{2}\] \[\leq\frac{B_{1}}{n_{1}}(1-\tau)^{2}\|u_{i,t}-g_{i}(\mathbf{w}_{t} )\|^{2}+\frac{B_{1}\tau^{2}\sigma^{2}}{n_{1}B_{2}}+(1-\frac{B_{1}}{n_{1}})\|u_ {i,t}-g_{i}(\mathbf{w}_{t})\|^{2}\] \[\leq(1-\frac{B_{1}\tau}{2n_{1}})^{2}\|u_{i,t}-g_{i}(\mathbf{w}_{t })\|^{2}+\frac{B_{1}\tau^{2}\sigma^{2}}{n_{1}B_{2}}\]

where we use

\[\frac{B_{1}}{n_{1}}(1-\tau)^{2}+(1-\frac{B_{1}}{n_{1}}) =\frac{B_{1}}{n_{1}}(1-2\tau+\tau^{2})+1-\frac{B_{1}}{n_{1}}\] \[=1-2\tau\frac{B_{1}}{n_{1}}+\tau^{2}\frac{B_{1}}{n_{1}}\] \[\leq 1-\tau\frac{B_{1}}{n_{1}}\] \[\leq 1-\tau\frac{B_{1}}{n_{1}}+(\frac{\tau B_{1}}{2n_{1}})^{2}=(1- \frac{\tau B_{1}}{2n_{1}})^{2}\]

Then

\[\mathbb{E}_{t}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq\mathbb{E}_{t}\left[(1+\frac{B_{1}\tau}{4n_{1}})\|u_{i,t+1}-g_ {i}(\mathbf{w}_{t})\|^{2}+(1+\frac{4n_{1}}{B_{1}\tau})\|g_{i}(\mathbf{w}_{t}) -g_{i}(\mathbf{w}_{t+1})\|^{2}\right]\] \[\leq(1+\frac{B_{1}\tau}{4n_{1}})(1-\frac{B_{1}\tau}{2n_{1}})^{2}\| u_{i,t}-g_{i}(\mathbf{w}_{t})\|^{2}+(1+\frac{B_{1}\tau}{4n_{1}})\frac{B_{1}\tau^{2} \sigma^{2}}{n_{1}B_{2}}\] \[\quad+(1+\frac{4n_{1}}{B_{1}\tau})C_{g}^{2}\mathbb{E}_{t}\| \mathbf{w}_{t}-\mathbf{w}_{t+1}\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{4n_{1}})^{2}\|u_{i,t}-g_{i}(\mathbf{w}_{t} )\|^{2}+\frac{2B_{1}\tau^{2}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{1}C_{g}^{2}M^{2} \eta^{2}}{B_{1}\tau}\]where we use \(\frac{B_{1}\tau}{4n_{1}}\leq 1\). Applying this inequality recursively, we obtain

\[\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{4n_{1}})^{2}\mathbb{E}[\|u_{i,t}-g_{i}( \mathbf{w}_{t})\|^{2}]+\frac{2B_{1}\tau^{2}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{1} C_{g}^{2}M^{2}\eta^{2}}{B_{1}\tau}\] \[\leq(1-\frac{B_{1}\tau}{4n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{ w}_{0})\|^{2}+\sum_{j=0}^{t}(1-\frac{B_{1}\tau}{4n_{1}})^{2(t-j)}\bigg{[}\frac{2B _{1}\tau^{2}\sigma^{2}}{n_{1}B_{2}}+\frac{8n_{1}C_{g}^{2}M^{2}\eta^{2}}{B_{1} \tau}\bigg{]}\] \[\leq(1-\frac{B_{1}\tau}{4n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{ w}_{0})\|^{2}+\frac{8\tau\sigma^{2}}{B_{2}}+\frac{32n_{1}^{2}C_{g}^{2}M^{2} \eta^{2}}{B_{1}^{2}\tau^{2}}\]

where we use \(\sum_{j=0}^{t}(1-\frac{B_{1}\tau}{4n_{1}})^{2(t-j)}\leq\frac{4n_{1}}{B_{1}\tau}\).

To obtain the absolute bound, we derive

\[\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|]^{2} \leq\mathbb{E}[\|u_{i,t+1}-g_{i}(\mathbf{w}_{t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau}{4n_{1}})^{2(t+1)}\|u_{i,0}-g_{i}(\mathbf{ w}_{0})\|^{2}+\frac{8\tau\sigma^{2}}{B_{2}}+\frac{32n_{1}^{2}C_{g}^{2}M^{2} \eta^{2}}{B_{1}^{2}\tau^{2}}\] \[\leq\bigg{[}(1-\frac{B_{1}\tau}{4n_{1}})^{t+1}\|u_{i,0}-g_{i}( \mathbf{w}_{0})\|+\frac{2\sqrt{2}\tau^{1/2}\sigma}{B_{2}^{1/2}}+\frac{4\sqrt{ 2}n_{1}C_{g}M\eta}{B_{1}\tau}\bigg{]}^{2}\]

The desired result follows by taking squared root on both sides. 

### Proof of Lemma b.5

Proof of Lemma b.5.: The proof of Lemma b.5 is the same as Lemma B.2. 

### Proof of Lemma b.6

Proof of Lemma b.6.: Define

\[\tilde{u}_{i,t}=(1-\tau_{2})u_{i,t}+\tau_{2}\frac{1}{B_{2}}\sum_{j\in\mathcal{ B}_{2,i}^{\mathsf{B}}}g_{i}(v_{i,j,t})\]

Then we have

\[\mathbb{E}_{\mathcal{B}_{2}^{\mathsf{B}}}[\|\tilde{u}_{i,t}-\frac {1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]\] \[=\mathbb{E}_{\mathcal{B}_{2}^{\mathsf{B}}}[\|(1-\tau_{2})(u_{i,t}- \frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t}))+\tau_{2}(\frac{1}{B _{2}}\sum_{j\in\mathcal{B}_{2}^{\mathsf{B}}}g_{i}(v_{i,j,t})-\frac{1}{n_{2}} \sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t}))\|^{2}]\] \[=\mathbb{E}_{\mathcal{B}_{2}^{\mathsf{B}}}[(1-\tau_{2})^{2}\|u_{i,t}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}+\tau_{2}^{ 2}\|\frac{1}{B_{2}}\sum_{j\in\mathcal{B}_{2}^{\mathsf{B}}}g_{i}(v_{i,j,t})- \frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}\] \[\quad+2(1-\tau_{2})\tau_{2}\langle u_{i,t}-\frac{1}{n_{2}}\sum_{j \in\mathcal{S}_{2}}g_{i}(v_{i,j,t}),\frac{1}{B_{2}}\sum_{j\in\mathcal{B}_{2}^ {\mathsf{B}}}g_{i}(v_{i,j,t})-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v _{i,j,t})\rangle]\] \[\leq(1-\tau_{2})^{2}\|u_{i,t}-\frac{1}{n_{2}}\sum_{j\in\mathcal{ S}_{2}}g_{i}(v_{i,j,t})\|^{2}+\frac{\tau_{2}^{2}\sigma^{2}}{B_{2}}\]It follows

\[\mathbb{E}_{\mathcal{B}_{2,i}^{t}}\mathbb{E}_{\mathcal{B}_{1}^{t}}[ \|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]\] \[=\frac{B_{1}}{n_{1}}\mathbb{E}_{\mathcal{B}_{2}^{t}}[\|\tilde{u}_ {i,t}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]+(1- \frac{B_{1}}{n_{1}})\|u_{i,t}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v _{i,j,t})\|^{2}\] \[\leq\frac{B_{1}}{n_{1}}(1-\tau_{2})^{2}\|u_{i,t}-\frac{1}{n_{2}} \sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}+\frac{B_{1}\tau_{2}^{2}\sigma ^{2}}{n_{1}B_{2}}+(1-\frac{B_{1}}{n_{1}})\|u_{i,t}-\frac{1}{n_{2}}\sum_{j\in \mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}\] \[\leq(1-\frac{B_{1}\tau_{2}}{2n_{1}})^{2}\|u_{i,t}-\frac{1}{n_{2}} \sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}+\frac{B_{1}\tau^{2}\sigma^{2} }{n_{1}B_{2}}\]

where we use

\[\frac{B_{1}}{n_{1}}(1-\tau_{2})^{2}+(1-\frac{B_{1}}{n_{1}})\leq(1-\frac{\tau_{ 2}B_{1}}{2n_{1}})^{2}\]

Then

\[\mathbb{E}_{t}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2 }}g_{i}(v_{i,j,t+1})\|^{2}]\] \[\leq\mathbb{E}_{t}\left[(1+\frac{B_{1}\tau_{2}}{4n_{1}})\|u_{i,t+ 1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}+(1+\frac{4n _{1}}{B_{1}\tau_{2}})\|\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j, t})-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t+1})\|^{2}\right]\] \[\leq(1+\frac{B_{1}\tau_{2}}{4n_{1}})(1-\frac{B_{1}\tau_{2}}{2n_{ 1}})^{2}\|u_{i,t}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{ 2}+(1+\frac{B_{1}\tau_{2}}{4n_{1}})\frac{B_{1}\tau_{2}^{2}\sigma^{2}}{n_{1}B_ {2}}\] \[\quad+(1+\frac{4n_{1}}{B_{1}\tau_{2}})C_{g}^{2}\mathbb{E}_{t}[ \frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}\|v_{i,j,t}-v_{i,j,t+1}\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2}\|u_{i,t}-\frac{1}{n_{2}} \sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}+\frac{2B_{1}\tau_{2}^{2}\sigma ^{2}}{n_{1}B_{2}}+\frac{8C_{g}^{2}M^{2}B_{2}\tau_{1}^{2}}{n_{2}\tau_{2}}\]

where we use \(\frac{B_{1}\tau_{2}}{4n_{1}}\leq 1\), and

\[\mathbb{E}_{t}[\|v_{i,j,t}-v_{i,j,t+1}\|^{2}]=\frac{B_{1}B_{2}}{n_{1}n_{2}} \mathbb{E}_{\mathcal{B}_{3,i,j}^{t}}[|\tau_{1}v_{i,j,t}-\tau_{1}h_{i,j}(\mathbf{ w}_{t};\mathcal{B}_{3,i,j}^{t})\|^{2}\leq\frac{B_{1}B_{2}\tau_{1}^{2}M^{2}}{n_{1}n_{2}}.\]

Applying this inequality recursively, we obtain

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_ {i}(v_{i,j,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2}\mathbb{E}[\|u_{i,t}- \frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,t})\|^{2}]+\frac{2B_{1} \tau_{2}^{2}\sigma^{2}}{n_{1}B_{2}}+\frac{8C_{g}^{2}M^{2}B_{2}\tau_{1}^{2}}{n_{ 2}\tau_{2}}\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2(t+1)}\|u_{i,0}-\frac{1}{n_ {2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|^{2}+\sum_{j=0}^{t}(1-\frac{B_{ 1}\tau_{2}}{4n_{1}})^{2(t-j)}\bigg{[}\frac{2B_{1}\tau_{2}^{2}\sigma^{2}}{n_{1}B _{2}}+\frac{8C_{g}^{2}M^{2}B_{2}\tau_{1}^{2}}{n_{2}\tau_{2}}\bigg{]}\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2(t+1)}\|u_{i,0}-\frac{1}{n_ {2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|^{2}+\frac{87_{2}\sigma^{2}}{B _{2}}+\frac{32C_{g}^{2}M^{2}n_{1}B_{2}\tau_{1}^{2}}{B_{1}n_{2}\tau_{2}^{2}}\]

where we use \(\sum_{j=0}^{t}(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2(t-j)}\leq\frac{4n_{1}}{B_{1} \tau_{2}}\).

To obtain the absolute bound, we derive

\[\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v _{i,j,t+1})\|]^{2}\] \[\leq\mathbb{E}[\|u_{i,t+1}-\frac{1}{n_{2}}\sum_{j\in\mathcal{S}_{2} }g_{i}(v_{i,j,t+1})\|^{2}]\] \[\leq(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{2(t+1)}\|u_{i,0}-\frac{1}{n _{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|^{2}+\frac{8\tau_{2}\sigma^{2} }{B_{2}}+\frac{32C_{g}^{2}M^{2}n_{1}B_{2}\tau_{1}^{2}}{B_{1}n_{2}\tau_{2}^{2}}\] \[\leq\left[(1-\frac{B_{1}\tau_{2}}{4n_{1}})^{t+1}\|u_{i,0}-\frac{1} {n_{2}}\sum_{j\in\mathcal{S}_{2}}g_{i}(v_{i,j,0})\|+\frac{2\sqrt{2}\tau_{2}^{1 /2}\sigma}{B_{2}^{1/2}}+\frac{4\sqrt{2}C_{g}Mn_{1}^{1/2}B_{2}^{1/2}\tau_{1}}{ B_{1}^{1/2}n_{2}^{1/2}\tau_{2}}\right]^{2}\]

The desired result follows by taking squared root on both sides. 

## Appendix E Group Distributionally Robust Optimization

NSWC FCCO finds an important application in group distributionally robust optimization (group DRO), particularly valuable in addressing distributional shift [25]. Consider \(N\) groups with different distributions. Each group \(k\) has an averaged loss \(L_{k}(w)=\frac{1}{n_{k}}\sum_{i=1}^{n_{k}}\ell(f_{w}(x_{i}^{k}),y_{i}^{k})\), where \(w\) is the the model parameter and \((x_{i}^{k},y_{i}^{k})\) is a data point. For robust optimization, we assign different weights to different groups and form the following robust loss minimization problem:

\[\min_{w}\max_{p\in\Omega}\sum_{k=1}^{N}p_{k}L_{k}(w),\]

where \(\Omega\subset\Delta\) and \(\Delta\) denotes a simplex. A common choice for \(\Omega\) is \(\Omega=\{\mathbf{p}\in\Delta,p_{i}\leq 1/K\}\) where \(K\) is an integer, resulting in the so-called CVaR losses, i.e., average of top-K group losses. Consequently, the above problem can be equivalently reformulated as [23]:

\[\min_{w}\min_{s}F(w,s)=\frac{1}{K}\sum_{k=1}^{N}[L_{k}(w)-s]_{+}+s.\]

This formulation can be mapped into non-smooth weakly-convex FCCO when the loss function \(\ell(\cdot,\cdot)\) is weakly convex in terms of \(w\). In comparison to directly solving the min-max problem, solving the above FCCO problem avoids the need of dealing with the projection onto the constraint \(\Omega\) and expensive sampling as in existing works [4].

## Appendix F More Information for Experiments

### Illustration for Histopathology Dataset on MIL Task

Figure 2: Illustration for Histopathology Dataset on MIL Task. Ade. is abbreviated for adenocarcinoma and SCC is short for squamous cell carcinoma. In this work, each RGB image is separated by 32\(\times\)32 non-overlapped patches, which constitute the bag.