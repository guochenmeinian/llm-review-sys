# Selective Mixup Helps with Distribution Shifts,

But Not (Only) because of Mixup

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

**Context.** Mixup is a highly successful technique to improve generalization of neural networks by augmenting the training data with combinations of random pairs. Selective mixup is a family of methods that apply mixup to specific pairs, e.g. only combining examples across classes or domains. These methods have claimed remarkable improvements on benchmarks with distribution shifts, but their mechanisms and limitations remain poorly understood.

**Findings.** We examine an overlooked aspect of selective mixup that explains its success in a completely new light. We find that the non-random selection of pairs affects the training distribution and improve generalization by means completely unrelated to the mixing. For example in binary classification, mixup across classes implicitly resamples the data for a uniform class distribution -- a classical solution to label shift. We show empirically that this implicit resampling explains much of the improvements in prior work. Theoretically, these results rely on a "regression toward the mean", an accidental property that we identify in several datasets.

**Takeaways.** We have found a new equivalence between two successful methods: selective mixup and resampling. We identify limits of the former, confirm the effectiveness of the latter, and find better combinations of their respective benefits.

## 1 Introduction

Mixup and its variants are some of the few methods that improve generalization across tasks and modalities with no domain-specific information [36]. Standard mixup replaces training data with linear combinations of random pairs of examples, proving successful e.g. for image classification [35], semantic segmentation [9], natural language processing [30], and speech processing [21].

This paper focuses on scenarios of distribution shift and on variants of mixup that improve out-of-distribution (OOD) generalization. We examine the family of methods that apply mixup on selected pairs of examples, which we refer to as _selective mixup_[7, 15, 19, 22, 28, 31, 33]. Each of these method uses a predefined criterion.1 For example, some methods combine examples across classes [33] (Figure 1) or across domains [31, 15, 19]. These simple heuristics have claimed remarkable improvements on benchmarks such as DomainBed [5], WILDS [12], and Wild-Time [32].

Footnote 1: We focus on the basic implementation of selective mixup as described by Yao et al. [33], i.e. without additional regularizers or modifications of the learning objective described in various other papers.

Despite impressive empirical performance, the theoretical mechanisms of selective mixup remain obscure. For example, the selection criteria proposed in [33] include the selection of pairs of the same class / different domains, but also the exact opposite.

This raises questions:

1. What mechanisms are responsible for the improvements of selective mixup?
2. What makes each selection criterion suitable to any specific dataset?

This paper presents surprising answers by highlighting an overlooked side effect of selective mixup. **The non-random selection of pairs implicitly biases the training distribution and improve generalization by means completely unrelated to the mixing**. We observe empirically that simply concatenating - rather than mixing - the selected pairs along the mini-batch dimension often produces the same improvements as mixing them. This critical ablation was absent from prior studies.

We also analyze theoretically the resampling induced by different selection criteria. We find that conditioning on a "different attribute" (e.g. combining examples across classes or domains) brings the training distribution of this attribute closer to a uniform one. Consequently, the imbalances in the data often "regress toward the mean" with selective mixup. We verify empirically that several datasets do indeed shift toward a uniform class distribution in their test split (see Figure 10). We also find remarkable correlation between improvements in performance and the reduction in divergence of training/test distributions due to selective mixup. This also predicts an unknown failure mode of selective mixup when the above property does not hold.

**Our contributions are summarized as follows.**

* We point out an overlooked resampling effect when applying selective mixup (Section 3).
* We show theoretically that certain selection criteria induce a bias in the distribution of features and/or classes equivalent to a "regression toward the mean" (Theorem 3.1). In binary classification for example, selecting pairs across classes is equivalent to sampling uniformly over classes, the standard approach to address label shift and imbalanced data.
* We verify empirically that multiple datasets indeed contain a regression toward a uniform class distribution across training and test splits (Section 4.6). We also find that improvements from selective mixup correlate with reductions in divergence of training/test distributions over labels and/or covariates. This strongly suggests that resampling is the main driver for these improvements.
* We compare many selection criteria and resampling baselines on five datasets. In all cases, improvements with selective mixup are partly or fully explained by resampling effects (Section 4).

**The implications for future research are summarized as follows.**

* We connect two areas of the literature by showing that selective mixup is sometimes equivalent to resampling, a classical strategy for distribution shifts [3, 8]. This hints at possible benefits from advanced methods for label shift and domain adaptation on benchmarks with distribution shifts.
* The resampling explains why different criteria in selective mixup benefit different datasets: they affect the distribution of features and/or labels and therefore address covariate and/or label shift.
* There is a risk of overfitting to the benchmarks: we show that much of the observed improvements rely on the accidental property of a "regression toward the mean" in the datasets examined.

Figure 1: Selective mixup is a family of methods that replace the training data with combined pairs of examples fulfilling a predefined criterion, e.g. pairs of different classes. As an overlooked side effect, this modifies the training distribution: in this case, sampling classes more uniformly. This effect is responsible for much of the resulting improvements in OOD generalization.

Background: mixup and selective mixup

**Notations.** We consider a classification model \(f_{\bm{\theta}}:\mathbb{R}^{d}\rightarrow[0,1]^{C}\) of learned parameters \(\bm{\theta}\). It maps an input vector \(\bm{x}\in\mathbb{R}^{d}\) to a vector \(\bm{y}\) of scores over \(C\) classes. The training data for such a model is typically a set of labeled examples \(\mathcal{D}=\{(\bm{x}_{i},\bm{y}_{i},d_{i})\}_{i=1}^{n}\) where \(\bm{y}_{i}\) are one-hot vectors encoding ground-truth labels, and \(d_{i}\in\mathbb{N}\) are optional discrete domain indices. Domain labels are sometimes available e.g. in datasets with different image styles [14] or collected over different time periods [12].

**Training with ERM.** Standard empirical risk minimization (ERM) optimizes the model's parameters for \(\min_{\bm{\theta}}\ \mathcal{R}(f_{\bm{\theta}},\mathcal{D})\) where the expected training risk, for a chosen loss function \(\mathcal{L}\), is defined as:

\[\mathcal{R}(f_{\bm{\theta}},\mathcal{D})\ =\ \mathbb{E}_{(\bm{x},\bm{y})\in \mathcal{D}}\ \mathcal{L}\big{(}f_{\bm{\theta}}(\bm{x}),\bm{y}\big{)}.\] (1)

An empirical estimate is obtained with an arithmetic mean over instances of the dataset \(\mathcal{D}\).

**Training with mixup.** Standard mixup essentially replaces training examples with linear combinations of random pairs in both input and label space. We formalize it by redefining the training risk.

\[\mathcal{R}_{\text{mixup}}(f_{\bm{\theta}},\mathcal{D})\ =\ \mathbb{E}_{(\bm{x},\bm{y})\in \mathcal{D}}\ \mathcal{L}\big{(}f(c\,\bm{x}\!+\!(1\!-\!c)\widetilde{\bm{x}},\ c\,\bm{y}\!+\!( 1\!-\!c)\,\widetilde{\bm{y}})\big{)}\] (2)

with mixing coefficients \(c\sim\mathcal{B}(2,2)\) and paired examples \((\widetilde{\bm{x}},\widetilde{\bm{y}})\sim\mathcal{D}\). (3)

The expectation is approximated by sampling different coefficients and pairs at every training iteration.

**Selective mixup.** While standard mixup combines random pairs, selective mixup only combines pairs that fulfill a predefined criterion. To select these pairs, the method starts with the original data \(\mathcal{D}\), then for every \((\bm{x},\bm{y},d)\in\mathcal{D}\) it selects a \((\widetilde{\bm{x}},\widetilde{\bm{y}},\widetilde{d})\in\mathcal{D}\) such that they fulfill the criterion represented by the predicate \(\text{Paired}\big{(}\cdot,\cdot\big{)}\). For example, the criterion _same class/different domain_ a.k.a. "intra-label LISA" in [33] is implemented as follows:

\[\text{Paired}\big{(}(\bm{x}_{i},\bm{y}_{i},d_{i}),(\widetilde{\bm{x}}_{i}, \widetilde{\bm{y}}_{i},\widetilde{d}_{i})\big{)}\!=\!\textit{true}\ \ \text{iff}\ \ (\widetilde{\bm{y}}\!=\!\bm{y})\wedge(\widetilde{d}\!\neq\!d)\ \textit{ (same class/diff. domain)}\] (4a) Other examples: \[\text{Paired}\big{(}(\bm{x}_{i},\bm{y}_{i},d_{i}),(\widetilde{\bm{x}}_{i}, \widetilde{\bm{y}}_{i},\widetilde{d}_{i})\big{)}\!=\!\textit{true}\ \ \text{iff}\ \ ( \widetilde{\bm{y}}\!\neq\!\bm{y})\] (4b) \[\text{Paired}\big{(}(\bm{x}_{i},\bm{y}_{i},d_{i}),(\widetilde{\bm {x}}_{i},\widetilde{\bm{y}}_{i},\widetilde{d}_{i})\big{)}\!=\!\textit{true}\ \ \text{iff}\ \ ( \widetilde{d}\!=\!d)\] (4c)

## 3 Selective mixup modifies the training distribution

The new claims of this paper comprise two parts.

1. Estimating the training risk with selective mixup (Eq. 2) uses a different sampling of examples from \(\mathcal{D}\) than ERM (Eq. 1). We demonstrate it theoretically in this section.
2. We hypothesize that the biased sampling of training examples influences the generalization properties of the learned model, regardless of the mixing operation. We verify this empirically in Section 4 using ablations of selective mixup that omit the mixing operation -- a critical baseline absent from previous studies.

**Training distribution.** This distribution refers to the examples sampled from \(\mathcal{D}\) to estimate the training risk (Eq. 1 or 2) -- whether these are then mixed or not. The following discussion focuses on distributions over classes (\(\bm{y}\)) but analogous arguments apply to covariates (\(\bm{x}\)) and domains (\(d\)).

**With ERM**, the training distribution equals the dataset distribution because the expectation in Eq. (1) is over uniform samples of \(\mathcal{D}\). We obtain an empirical estimate by averaging all one-hot labels, giving the vector of discrete probabilities \(\mathbf{p}_{\text{Y}}(\mathcal{D})\ =\ \oplus_{(\bm{x},\bm{y})\in\mathcal{D}}\ \bm{y} /\left|\mathcal{D}\right|\) where \(\oplus\) is the element-wise sum.

**With selective mixup**, evaluating the risk (Eq. 2) requires pairs of samples. The first element of a pair is sampled uniformly, yielding the same \(\mathbf{p}_{\text{Y}}(\mathcal{D})\) as ERM. The second element is selected as described above, using the first element and one chosen predicate \(\text{Paired}(\cdot,\cdot)\) e.g. from (4a-4c). For our analysis, we denote these "second elements" of the pairs as the virtual data:

\[\widetilde{\mathcal{D}}\ =\ \big{\{}(\widetilde{\bm{x}}_{i},\widetilde{\bm{y}}_{i}, \widetilde{d}_{i})\sim\mathcal{D}:\ \ \text{Paired}\big{(}(\bm{x}_{i},\bm{y}_{i},d_{i}),(\widetilde{\bm{x}}_{i}, \widetilde{\bm{y}}_{i},\widetilde{d}_{i})\big{)}\!=\!true,\ \ \forall\,i=1,\ldots,\left| \mathcal{D}\right|\big{\}}.\] (5)

We can now analyze the overall training distribution of selective mixup. An empirical estimate is obtained by combining the distributions resulting from the two elements of the pairs, which gives the vector \(\mathbf{p}_{\text{Y}}(\mathcal{D}\cup\widetilde{\mathcal{D}})\ =\ \left(\mathbf{p}_{\text{Y}}( \mathcal{D})\ \oplus\ \mathbf{p}_{\text{Y}}(\widetilde{\mathcal{D}})\right)/\,2\).

**Regression toward the mean.** With the criterion _same class_, it is obvious that \(\mathbf{p}_{\mathbf{y}}(\widetilde{\mathcal{D}})=\mathbf{p}_{\mathbf{y}}( \mathcal{D})\). Therefore these variants of selective mixup are not concerned with resampling effects.2 In contrast, the criteria _different class_ or _different domain_ do bias the sampling. In the case of binary classification, we have \(\mathbf{p}_{\mathbf{y}}(\widetilde{\mathcal{D}})\!=\!1\!-\!\mathbf{p}_{\mathbf{ y}}(\mathcal{D})\) and therefore \(\mathbf{p}_{\mathbf{y}}(\mathcal{D}\cup\widetilde{\mathcal{D}})\) is uniform. This means that selective mixup with the _different class_ criterion has the side effect of balancing the training distribution of classes, a classical mitigation of class imbalance [10; 13]. For multiple classes, we have a more general result.

Footnote 2: The absence of resampling effects holds for _same class_ and _same domain_ alone, but not in conjunction with other criteria. See e.g. the differences between _same domain_/_diff. class_ and _any domain_/_diff. class_ in Figure 3.

**Theorem 3.1**.: _Given a dataset \(\mathcal{D}\!=\!\{(\bm{x}_{i},\bm{y}_{i})\}_{i}\) and paired data \(\widetilde{\mathcal{D}}\) sampled according to the "different class" criterion, i.e. \(\widetilde{\mathcal{D}}=\{(\widetilde{\bm{x}}_{i},\widetilde{\bm{y}}_{i}) \sim\mathcal{D}\;\;\text{s.t.}\;\;\widetilde{\bm{y}}_{i}\!\neq\!\bm{y}_{i}\}\), then the distribution of classes in \(\mathcal{D}\cup\widetilde{\mathcal{D}}\) is more uniform than in \(\mathcal{D}\). Formally, the entropy \(\mathbb{H}\big{(}\mathbf{p}_{\mathbf{y}}(\mathcal{D})\big{)}\leq\mathbb{H} \big{(}\mathbf{p}_{\mathbf{y}}(\mathcal{D}\cup\widetilde{\mathcal{D}})\big{)}\)._

Proof.: see Appendix C.

Theorem 3.1 readily extends in two ways. First, the same effect also results from the _different domain_ criterion: if each domain contains a different class distribution, the resampling from this criterion averages them out, yielding a more uniform aggregated training distribution. Second, this averaging applies not only to class labels (\(\bm{y}\)) but also covariates (\(\bm{x}\)). An analysis using distributions is ill-suited but the mechanism similarly affects the sampling of covariates when training with selective mixup.

**When does one benefit from the resampling (regardless of mixup)?** The above results mean that selective mixup can implicitly reduce imbalances (a.k.a. biases) in the training data. When these are not spurious and also exist in the test data, the effect on predictive performance could be detrimental.

We expect benefits (which we verify in Section 4) on datasets with distribution shifts, whose training/test splits contain different imbalances by definition. Softening imbalances in the training data is then likely to bring the training and test distributions closer to one another, in particular with extreme shifts such as the complete reversal of a spurious correlation (e.g. _waterbirds_[24], Section 4.1).

We also expect a benefit on worst-group metrics (e.g. with _civilComments_[12] in Section 4.4). The challenge in these datasets comes from the imbalance of class/domain combinations in the training data, and previous work has indeed shown that balancing is a competitive baseline [8; 24].

## 4 Experiments

We performed a large number of experiments to understand the contribution of the different effects of selective mixup and other resampling baselines (see Appendix B for complete results).

**Datasets.** We focus on five datasets that previously showed improvements with selective mixup. We selected them to cover a range of modalities (vision, NLP, tabular), settings (binary, multiclass), and types of distribution shifts (covariate, label, and subpopulation shifts).

* **Waterbirds**[24] is a popular artificial dataset used to study distribution shifts. The task is to classify images of birds into two types. The image backgrounds are also of two types, and the correlation between birds and backgrounds is reversed across the training and test splits. The type of background in each image serves as its domain label.
* **CivilComments**[12] is a widely-used dataset of online text comments to be classified as toxic or not. Each example is labeled with a topical attribute (e.g. Christian, male, LGBT, etc.) that is spuriously associated with ground truth labels in the training data. These attributes serve as domain labels. The target metric is the worst-group accuracy where the groups correspond to all toxicity/attribute combinations.
* **Wild-Time Yearbook**[32] contains yearbook portraits to be classified as male or female. It is part of the Wild-Time benchmark, which is a collection of real-world datasets captured over time. Each example belongs to a discrete time period that serves as its domain label. Distinct time periods are assigned to the training and OOD test splits (see Figure 10).
* **Wild-Time arXiv**[32] contains titles of arXiv preprints. The task is to predict each paper's primary category among 172 classes. Time periods serve as domain labels.
* **Wild-Time MIMIC-Readmission**[32] contains hospital records (sequences of codes representing diagnoses and treatments) to be classified into two classes. The positive class indicates the readmission of the patient at the hospital within 15 days. Time periods serve as domain labels.

**Methods.** We train standard architectures suited to each dataset with the methods below (details in Appendix A). We perform early stopping i.e. recording metrics for each run at the epoch of highest ID or worst-group validation performance (for _Wild-Time_ and _waterbirdscivilComments_ datasets respectively). We plot average metrics in bar charts over 9 different seeds with error bars representing \(\pm\) one standard deviation. **ERM** and **vanilla mixup** are the standard baselines. Baseline **resampling** uses training examples with equal probability from each class, domain, or combinations thereof as in [8; 24]. **Selective mixup** (\(\blacksquare\)) includes all possible selection criteria based on classes and domains. We avoid ambiguous terminology from earlier works because of inconsistent usage (e.g. "intra-label LISA" means "different domain" in [12] but not in [32]). **Selective sampling** (\(\blacksquare\)) is a novel ablation of selective mixup where the selected pairs are not mixed, but concatenated along the mini-batch dimension. Half of the pairs are dropped at random to keep the size of mini-batches constant. Therefore any difference between selective sampling and ERM is attributable only to resampling effects. We also include **novel combinations** (\(\blacksquare\)) of sampling and mixup. Code to reproduce our experiments and figures: https://github.com/<anonymized>/<anonymized>.

### Results on the _waterbirds_ dataset

The target metric for this dataset is the worst-group accuracy, with groups defined as the four class/domain combinations. The two difficulties are (1) a class imbalance (77 / 23%) and (2) a correlation shift (spurious class/domain association reversed at test time). See discussion in Figure 2.

We first observe that vanilla mixup is detrimental compared to ERM. Resampling with uniform class/domain combinations is hugely beneficial, for the reasons explained in Figure 3. The ranking of various criteria for selective sampling is similar whether with or without mixup. Most interestingly, the best criterion performs similarly, but no better than the best resampling.

### Results on the _yearbook_ dataset

The difficulty of this dataset comes from a slight class imbalance and the presence of covariate/label shift (see Figure 10). The test split contains several domains (time periods). The target metric is the worst-domain accuracy. Figure 4 shows that vanilla mixup is slightly detrimental compared to ERM. Resampling for uniform classes gives a clear improvement because of the class imbalance. With selective sampling (no mixup), the only criteria that improve over ERM contain "different class". This is expected because this criterion implicitly resamples for a uniform class distribution.

Figure 3: The sampling ratios of each class/domain clearly explain the performance of the best methods (_waterbirds_).

Figure 2: Main results on _waterbirds_.

To investigate whether some of the improvements are due to resampling, we measure the divergence between training and test distributions of classes and covariates (details in Appendix A). Figure 5) shows first that there is a clear variation among different criteria (\(\bullet\) blue dots) i.e. some bring the training/test distributions closer to one another. Second, there is a remarkable correlation between the test accuracy and the divergence, on both classes and covariates.3 This means that resampling effects do occur and also play a part in the best variants of selective mixup.

Footnote 3: As expected, the correlation is reversed for the first two test domains in Figure 5 since they are even further from a uniform class distribution than the average of the training data, as seen in Figure 10.

Finally, the improvements from simple resampling and the best variant of selective mixup suggest a new combination. We train a model with uniform class sampling and selective mixup using the "same class" criterion, and obtain performance superior to all existing results (last row in Figure 5). This confirms the **complementarity of the effects of resampling and within-class selective mixup**.

### Results on the _arXiv_ dataset

This dataset has difficulties similar to _yearbook_ and also many more classes (172). Simple resampling for uniform classes is very bad (literally off the chart in Figure 6) because it overcorrects the imbalance (the test distribution being closer to the training than to a uniform one). Uniform _domains_ is much better since its effect is similar but milder.

All variants of selective mixup (\(\blacksquare\)) perform very well, but they improve over ERM even without mixup (\(\blacksquare\)). And the selection criteria rank similarly with or without mixup, suggesting that parts of the improvements of selective mixup is due to the resampling. Given that vanilla mixup also clearly improves over ERM, the performance of **selective mixup is explained by cumulative effects of vanilla mixup and resampling effects**. This also suggests new combinations of methods (\(\blacksquare\)) among which we find one version marginally better than the best variant of selective mixup (last row).

Figure 4: Main results on _yearbook_.

Figure 5: Different selection criteria (\(\bullet\)) modify the distribution of both covariates and labels (upper and lower rows). The resulting reductions in divergence between training and test distributions correlate remarkably well with test performance.3 This confirms the contribution of resampling to the overall performance of selective mixup.

### Results on the _civilComments_ dataset

This dataset mimics a subpopulation shift because the worst-group metric requires high accuracy on classes and domains under-represented in the training data. It also contains an implicit correlation shift because any class/domain association (e.g. "Christian" comments labeled as toxic more often than not) becomes spurious when evaluating individual class/domain combinations.

For the above reasons, it makes sense that resampling for uniform classes or combinations greatly improves performance, as shown in prior work [8].

With selective mixup (), some criterion (same domain/diff. class) performs clearly above all others. But it works **even better without mixup**! () Among many other variations, **none surpasses the uniform-combinations baseline**.

Figure 8: Main results on _civilComments_. For the above reasons, it makes sense that resampling for uniform classes or combinations greatly improves performance, as shown in prior work [8].

Figure 6: Main results on _arXiv_.

Figure 7: Divergence of tr./test class distributions vs. test accuracy.

### Results on the _Mimic-Readmission_ dataset

This dataset contains a class imbalance (about 78/22% in training data), label shift (the distribution being more balanced in the test split), and possibly covariate shift. It is unclear whether the task is causal or anticausal (labels causing the features) because the inputs contain both diagnoses and treatments. The target metric is the area under the ROC curve (AUROC) which gives equal importance to both classes. We report the worst-domain AUROC, i.e. the lowest value across test time periods.

Vanilla mixup performs a bit better than ERM. Because of the class imbalance, resampling for uniform classes also improves ERM. As expected, this is perfectly equivalent to the selective sampling criterion "diffClass" and they perform therefore equally well. Adding mixup is yet a bit better, which suggests again that **the performance of selective mixup is merely the result of the independent effects of vanilla mixup and resampling**. We further verify this explanation with the novel combination of simple resampling and vanilla mixup, and observe almost no difference whether the mixing operation is performed or not (last two rows in Figure 9).

To further support the claim that these methods mostly address label shift, we report in Table 1 the proportion of the majority class in the training and test data. We observe that the distribution sampled by the best training methods brings it much closer to that of the test data.

### Evidence of a "regression toward the mean" in the data

We hypothesized in Section 3 that the resampling benefits are due to a "regression toward the mean" across training and test splits. We now check for this property and find indeed a shift toward uniform class distributions in all datasets studied. For the Wild-Time datasets, we plot in Figure 10 the ratio of the minority class (binary tasks: yearbook, MIMIC) and class distribution entropy (multiclass task: arxiv). Finding this property in all three datasets agrees with the proposed explanation and the fact that we selected them because they previously showed improvements with selective mixup in [32].

In the _waterbirds_ and _civilComments_ datasets, the shift toward uniformity" also holds, but artificially. The training data contains imbalanced groups (class/domain combinations) whereas the evaluation with worst-group accuracy implicitly gives uniform importance to all groups.

## 5 Related work

**Mixup and variants.** Mixup was originally introduced in [36] and numerous variants followed [2]. Many propose modality-specific mixing operations: CutMix [34] replaces linear combinations with collages of image patches, Fmix [6] combines image regions based on frequency contents, AlignMixup [29] combines images after spatial alignment. Manifold-mixup [30] replaces the mixing in input space with the mixing of learned representations, making it applicable to text embeddings.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Proportion of majority class** & **(\%)** \\ \hline In the dataset (training) & 78.2 \\ In the dataset (validation) & 77.8 \\ In the dataset (OOD test) & 66.5 \\ \hline
**Sampled by different training methods** & \\ Resampling (uniform classes) & 50.0 \\ Diff. domain + diff. class & 50.0 \\ Diff. class & 50.1 \\ Same domain + Diff. class & 49.9 \\ Resampling (uniform cl.) + concatenated pairs & [64.3] \\ Resampling (uniform cl.) + vanilla mixup & [64.3] \\ \hline \hline \end{tabular}
\end{table}
Table 1: The performance of the various methods on _MIMIC-Readmission_ is explained by their correction of a class imbalance. The best training methods (boxed numbers) sample the majority class in a proportion much closer to that of the test data.

Figure 9: Main results on _MIMIC-Readmission_.

**Mixup for OOD generalization.** Mixup has been integrated into existing techniques for domain adaptation (DomainMix [31]), domain generalization (FIXED [20]), and with meta learning (RegMixup [23]). This paper focuses on variants we call "_selective mixup_" that use non-uniform sampling of the pairs of mixed examples. LISA [33] proposes two heuristics, same-class/different-domain and vice versa, used in proportions tuned by cross-validation on each dataset. Palakkadavath et al. [22] use same-class pairs and an additional objective to encourage invariance of the representations to the mixing. CIFAR [28] uses same-class pairs with a contrastive objective to improve algorithmic fairness. SelecMix [7] proposes a selection heuristic to handle biased training data: same class/different biased attribute, or vice versa. DomainMix [31] uses different-domain pairs for domain adaptation. DRE [15] uses same-class/different-domain pairs and regularize their Grad-CAM explanations to improve OOD generalization. SDMix [19] applies mixup on examples from different domains with other improvements to improve cross-domain generalization for activity recognition.

**Explaining the benefits of mixup** has invoked regularization [37] and augmentation [11] effects, the introduction of label noise [18], and the learning of rare features [38]. These works focus on the mixing and in-domain generalization, whereas we focus on the selection and OOD generalization.

**Training on resampled data.** We find that selective mixup is sometimes equivalent to training on resampled or reweighted data. Both are standard tools [10; 13] to handle distribution shifts in a domain adaptation setting, also known as importance-weighted empirical risk minimization (IW-ERM) [25; 4]. For covariate shift, IW-ERM trains a model with a weight or sampling probability on each training point \(\bm{x}\) as its likelihood ratio \(p_{\text{target}}(\bm{x})/p_{\text{source}}(\bm{x})\). Likewise with labels \(\bm{y}\) and \(p_{\text{target}}(\bm{y})/p_{\text{source}}(\bm{y})\) for label shift [1; 17], Recently, [8; 24] showed that reweighting and resampling are competitive with the state of the art on multiple OOD and label-shift benchmarks [3].

## 6 Conclusions and open questions

**Conclusions.** This paper helps understand selective mixup, which is one of the most successful and general methods for distribution shifts. We showed unambiguously that much of the improvements were actually unrelated to the mixing operation and could be obtained with much simpler, well-known resampling methods. On datasets where mixup does bring benefits, we could then obtain even better results by combining the independent effects of the best mixup and resampling variants.

**Limitations.** We focused on the simplest version selective mixup as described by Yao et al. [33]. Many papers combine the principle with modifications to the learning objective [7; 15; 19; 22; 28; 31]. Resampling likely plays a role in these methods too but this claim requires further investigation.

We evaluated "only" five datasets. Since we introduced simple ablations that can single out the effects of resampling, we hope to see future re-evaluations of other datasets.

Because we selected datasets that had previously shown benefits with selective mixup, we could not verify the predicted failure mode when there is no "regression toward the mean" in the data.

Finally, this work is **not** about designing new algorithms to surpass the state of the art. Our focus is on improving the scientific understanding of existing mixup strategies and their limitations.

**Open questions.** Our results leave open the question of the applicability of selective mixup to real situations. The "regression toward the mean" explanation indicates that much of the observed improvements are accidental since they rely on an artefact of some datasets. In real deployments, distribution shifts cannot be foreseen in nature nor magnitude. This is a reminder of the relevance of Goodhart's law to machine learning [26] and of the risk of overfitting to popular benchmarks [16].

Figure 10: The class distribution shifts toward uniformity in these Wild-Time datasets, which agrees with the explanation that resampling benefits are due to a “regression toward the mean”.

## References

* [1]K. Azizzadenesheli, A. Liu, F. Yang, and A. Anandkumar (2019) Regularized learning for domain adaptation under label shifts. arXiv preprint arXiv:1903.09734. Cited by: SS1.
* [2]C. Cao, F. Zhou, Y. Dai, and J. Wang (2022) A survey of mix-based data augmentation: taxonomy, methods, applications, and explainability. arXiv preprint arXiv:2212.10888. Cited by: SS1.
* [3]S. Garg, N. Erickson, J. Sharpnack, A. Smola, S. Balakrishnan, and Z. C. Lipton (2023) Rl'sbench: domain adaptation under relaxed label shift. arXiv preprint arXiv:2302.03020. Cited by: SS1.
* [4]A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Scholkopf (2009) Covariate shift by kernel mean matching. Dataset shift in machine learning. Cited by: SS1.
* [5]I. Gulrajani and D. Lopez-Paz (2020) In search of lost domain generalization. arXiv preprint arXiv:2007.01434. Cited by: SS1.
* [6]E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prugel-Bennett, and J. Hare (2020) Fmix: enhancing mixed sample data augmentation. arXiv preprint arXiv:2002.12047. Cited by: SS1.
* [7]I. Hwang, S. Lee, Y. Kwak, S. J. Oh, D. Teney, J. Kim, and B. Zhang (2022) Selecmix: debiased learning by contradicting-pair sampling. arXiv preprint arXiv:2211.02291. Cited by: SS1.
* [8]B. Y. Idrissi, M. Arjovsky, M. Pezeshki, and D. Lopez-Paz (2022) Simple data balancing achieves competitive worst-group-accuracy. In Conference on Causal Learning and Reasoning, Cited by: SS1.
* [9]M. Islam, M. Kowal, K. G. Derpanis, and N. D. Bruce (2023) Segmix: co-occurrence driven mixup for semantic segmentation and adversarial robustness. International Journal of Computer Vision131 (3), pp. 701-716. Cited by: SS1.
* [10]N. Japkowicz (2000) The class imbalance problem: significance and strategies. In Proc. of the Int'l Conf. on artificial intelligence, Vol. 56, pp. 111-117. Cited by: SS1.
* [11]M. Kimura. Why mixup improves the model performance. In International Conference on Artificial Neural Networks (ICANN), Cited by: SS1.
* [12]P. Wei Koh, S. Sagawa, H. Marklund, S. Michael Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al. (2021) Wilds: a benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, Cited by: SS1.
* [13]M. Kubat, S. Matwin, et al. (1997) Addressing the curse of imbalanced training sets: one-sided selection. In Icml, Vol. 97, pp. 179. Cited by: SS1.
* [14]D. Li, Y. Yang, Y. Song, and T. M. Hospedales (2017) Deeper, broader and artier domain generalization. In IEEE International Conference on Computer Vision, pp. 5542-5550. Cited by: SS1.
* [15]T. Li, F. Qiao, M. Ma, and X. Peng (2023) Are data-driven explanations robust against out-of-distribution data?. arXiv preprint arXiv:2303.16390. Cited by: SS1.
* [16]T. Liao, R. Taori, I. Deborah Raji, and L. Schmidt (2021) Are we learning yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), Cited by: SS1.
* [17]Z. Lipton, Y. Wang, and A. Smola (2018) Detecting and correcting for label shift with black box predictors. In International conference on machine learning, Cited by: SS1.

* Liu et al. [2023] Zixuan Liu, Ziqiao Wang, Hongyu Guo, and Yongyi Mao. Over-training with mixup may hurt generalization. _arXiv preprint arXiv:2303.01475_, 2023.
* Lu et al. [2022] Wang Lu, Jindong Wang, Yiqiang Chen, Sinno Jialin Pan, Chunyu Hu, and Xin Qin. Semantic-discriminative mixup for generalizable sensor-based cross-domain activity recognition. _Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies_, 2022.
* Lu et al. [2022] Wang Lu, Jindong Wang, Han Yu, Lei Huang, Xiang Zhang, Yiqiang Chen, and Xing Xie. Fixed: Frustratingly easy domain generalization with mixup. _arXiv preprint arXiv:2211.05228_, 2022.
* Meng et al. [2021] Linghui Meng, Jin Xu, Xu Tan, Jindong Wang, Tao Qin, and Bo Xu. Mixspeech: Data augmentation for low-resource automatic speech recognition. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7008-7012. IEEE, 2021.
* Palakkadavath et al. [2022] Raga Palakkadavath, Thanh Nguyen-Tang, Sunil Gupta, and Svetha Venkatesh. Improving domain generalization with interpolation robustness. In _NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications_, 2022.
* Pinto et al. [2022] Francesco Pinto, Harry Yang, Ser-Nam Lim, Philip HS Torr, and Puneet K Dokania. Regmixup: Mixup as a regularizer can surprisingly improve accuracy and out distribution robustness. _arXiv preprint arXiv:2206.14502_, 2022.
* Sagawa et al. [2019] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* Shimodaira [2000] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. _Journal of statistical planning and inference_, 2000.
* Teney et al. [2020] Damien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher Kanan, and Anton Van Den Hengel. On the value of out-of-distribution testing: An example of goodhart's law. _Advances in Neural Information Processing Systems_, 33:407-417, 2020.
* Teney et al. [2022] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are sometimes inversely correlated on real-world datasets. _arXiv preprint arXiv:2209.00613_, 2022.
* Tian et al. [2023] Huan Tian, Bo Liu, Tianqing Zhu, Wanlei Zhou, and S Yu Philip. Cifair: Constructing continuous domains of invariant features for image fair classifications. _Knowledge-Based Systems_, 2023.
* Venkataramanan et al. [2022] Shashanka Venkataramanan, Ewa Kijak, Laurent Amsaleg, and Yannis Avrithis. Alignmixup: Improving representations by interpolating aligned features. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19174-19183, 2022.
* Verma et al. [2019] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In _International Conference on Machine Learning_, 2019.
* Xu et al. [2020] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In _AAAI Conference on Artificial Intelligence_, 2020.
* Yao et al. [2022] Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. _Proc. Advances in Neural Inf. Process. Syst._, 2022.
* Yao et al. [2022] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In _International Conference on Machine Learning_, 2022.
* Yun et al. [2019] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _IEEE/CVF International Conference on Computer Vision_, pages 6023-6032, 2019.

* [35] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* [36] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [37] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? _arXiv preprint arXiv:2010.04819_, 2020.
* [38] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning. _arXiv preprint arXiv:2303.08433_, 2023.