# Training Energy-Based Normalizing Flow with Score-Matching Objectives

Chen-Hao Chao\({}^{1}\), Wei-Fang Sun\({}^{1,2}\), Yen-Chang Hsu\({}^{3}\), Zsolt Kira\({}^{4}\), and Chun-Yi Lee\({}^{1}\)

\({}^{1}\) Elsa Lab, National Tsing Hua University, Hsinchu City, Taiwan

\({}^{2}\) NVIDIA AI Technology Center, NVIDIA Corporation, Santa Clara, CA, USA

\({}^{3}\) Samsung Research America, Mountain View, CA, USA

\({}^{4}\) Georgia Institute of Technology, Atlanta, GA, USA

Corresponding author. Email: cylee@cs.nthu.edu.tw

###### Abstract

In this paper, we establish a connection between the parameterization of flow-based and energy-based generative models, and present a new flow-based modeling approach called energy-based normalizing flow (EBFlow). We demonstrate that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed. This feature enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity of each training iteration from \(\mathcal{O}(D^{2}L)\) to \(\mathcal{O}(D^{3}L)\) for an \(L\)-layered model that accepts \(D\)-dimensional inputs. This makes the training of EBFlow more efficient than the commonly-adopted maximum likelihood training method. In addition to the reduction in runtime, we enhance the training stability and empirical performance of EBFlow through a number of techniques developed based on our analysis of the score-matching methods. The experimental results demonstrate that our approach achieves a significant speedup compared to maximum likelihood estimation while outperforming prior methods with a noticeable margin in terms of negative log-likelihood (NLL).

## 1 Introduction

Parameter estimation for probability density functions (pdf) has been a major interest in the research fields of machine learning and statistics. Given a \(D\)-dimensional random data vector \(\mathbf{x}\in\mathbb{R}^{D}\), the goal of such a task is to estimate the true pdf \(p_{\mathbf{x}}(\cdot)\) of \(\mathbf{x}\) with a function \(p(\cdot\,;\theta)\) parameterized by \(\theta\). In the studies of unsupervised learning, flow-based modeling methods (e.g., [1; 2; 3; 4]) are commonly-adopted for estimating \(p_{\mathbf{x}}\) due to their expressiveness and broad applicability in generative tasks.

Flow-based models represent \(p(\cdot\,;\theta)\) using a sequence of invertible transformations based on the change of variable theorem, through which the intermediate unnormalized densities are re-normalized by multiplying the Jacobian determinant associated with each transformation. In maximum likelihood estimation, however, the explicit computation of the normalizing term may pose computational challenges for model architectures that use linear transformations, such as convolutions [4; 5] and fully-connected layers [6; 7]. To address this issue, several methods have been proposed in the recent literature, which includes constructing linear transformations with special structures [8; 9; 10; 11; 12] and exploiting special optimization processes [7]. Despite their success in reducing the training complexity, these methods either require additional constraints on the linear transformations or biased estimation on the gradients of the objective.

Motivated by the limitations of the previous studies, this paper introduces an approach that reinterprets flow-based models as energy-based models [13], and leverages score-matching methods [14; 15; 16; 17] tooptimize \(p(\cdot\,;\theta)\) according to the Fisher divergence [14; 18] between \(p_{\mathbf{x}}(\cdot)\) and \(p(\cdot\,;\theta)\). The proposed method avoids the computation of the Jacobian determinants of linear layers during training, and reduces the asymptotic computational complexity of each training iteration from \(\mathcal{O}(D^{3}L)\) to \(\mathcal{O}(D^{2}L)\) for an \(L\)-layered model. Our experimental results demonstrate that this approach significantly improves the training efficiency as compared to maximum likelihood training. In addition, we investigate a theoretical property of Fisher divergence with respect to latent variables, and propose a Match-after-Preprocessing (MaP) technique to enhance the training stability of score-matching methods. Finally, our comparison on the MNIST dataset [19] reveals that the proposed method exhibit significant improvements in comparison to our baseline methods presented in [17] and [7] in terms of negative log likelihood (NLL).

## 2 Background

In this section, we discuss the parameterization of probability density functions in flow-based and energy-based modeling methods, and offer a number of commonly-used training methods for them.

### Flow-based Models

Flow-based models describe \(p_{\mathbf{x}}(\cdot)\) using a prior distribution \(p_{\mathbf{u}}(\cdot)\) of a latent variable \(\mathbf{u}\in\mathbb{R}^{D}\) and an invertible function \(g=g_{L}\circ\cdots\circ g_{1}\), where \(g_{i}(\cdot\,;\theta):\mathbb{R}^{D}\rightarrow\mathbb{R}^{D},\,\forall i\in \{1,\cdots,L\}\) and is usually modeled as a neural network with \(L\) layers. Based on the change of variable theorem and the distributive property of the determinant operation \(\det\left(\cdot\right)\), \(p(\cdot\,;\theta)\) can be described as follows:

\[p(\bm{x};\theta)=p_{\mathbf{u}}\left(g(\bm{x};\theta)\right)\left|\det(\mathbf{ J}_{g}(\bm{x};\theta))\right|=p_{\mathbf{u}}\left(g(\bm{x};\theta)\right) \prod_{i=1}^{L}\left|\det(\mathbf{J}_{g_{i}}(\bm{x}_{i-1};\theta))\right|,\] (1)

where \(\bm{x}_{i}=g_{i}\circ\cdots\circ g_{1}(\bm{x};\theta)\), \(\bm{x}_{0}=\bm{x}\), \(\mathbf{J}_{g}(\bm{x};\theta)=\frac{\partial}{\partial\bm{x}}g(\bm{x};\theta)\) represents the Jacobian of \(g\) with respect to \(\bm{x}\), and \(\mathbf{J}_{g_{i}}(\bm{x}_{i-1};\theta)=\frac{\partial}{\partial\bm{x}_{i-1}} g_{i}(\bm{x}_{i-1};\theta)\) represents the Jacobian of the \(i\)-th layer of \(g\) with respect to \(\bm{x}_{i-1}\). This work concentrates on model architectures employing _linear flows_[20] to design the function \(g\). These model architectures primarily utilize linear transformations to extract crucial feature representations, while also accommodating non-linear transformations that enable efficient Jacobian determinant computation. Specifically, let \(\mathcal{S}_{l}\) be the set of linear transformations in \(g\), and \(\mathcal{S}_{n}=\{g_{i}\,|\,i\in\{1,\cdots,L\}\}\setminus\mathcal{S}_{l}\) be the set of non-linear transformations. The general assumption of these model architectures is that \(\prod_{i=1}^{L}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|\) in Eq. (1) can be decomposed as \(\prod_{g_{i}\in\mathcal{S}_{n}}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right| \prod_{g_{i}\in\mathcal{S}_{l}}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|\), where \(\prod_{g_{i}\in\mathcal{S}_{n}}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|\) and \(\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|\) can be calculated within the complexity of \(\mathcal{O}(D^{2}L)\) and \(\mathcal{O}(D^{3}L)\), respectively. Previous implementations of such model architectures include Generative Flows (Glow) [4], Neural Spline Flows (NSF) [5], and the independent component analysis (ICA) models presented in [6; 7].

Given the parameterization of \(p(\cdot\,;\theta)\), a commonly used approach for optimizing \(\theta\) is maximum likelihood (ML) estimation, which involves minimizing the Kullback-Leibler (KL) divergence \(\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}}(\bm{x})\|p(\bm{x};\theta)\right]= \mathbb{E}_{p_{\mathbf{x}}(\bm{x})}\left[\log\frac{p_{\mathbf{x}}(\bm{x})}{p( \bm{x};\theta)}\right]\) between the true density \(p_{\mathbf{x}}(\bm{x})\) and the parameterized density \(p(\bm{x};\theta)\). The ML objective \(\mathcal{L}_{\mathrm{ML}}(\theta)\) is derived by removing the constant term \(\mathbb{E}_{p_{\mathbf{x}}(\bm{x})}\left[\log p_{\mathbf{x}}(\bm{x})\right]\) with respect to \(\theta\) from \(\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}}(\bm{x})\|p(\bm{x};\theta)\right]\), and can be expressed as follows:

\[\mathcal{L}_{\mathrm{ML}}(\theta)=\mathbb{E}_{p_{\mathbf{x}}(\bm{x})}\left[- \log p(\bm{x};\theta)\right].\] (2)

The ML objective explicitly evaluates \(p(\bm{x};\theta)\), which involves the calculation of the Jacobian determinant of the layers in \(\mathcal{S}_{l}\). This indicates that certain model architectures containing convolutional [4; 5] or fully-connected layers [6; 7] may encounter training inefficiency due to the \(\mathcal{O}(D^{3}L)\) cost of evaluating \(\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|\). Although a number of alternative methods discussed in Section 3 can be adopted to reduce their computational cost, they either require additional constraints on the linear transformation or biased estimation on the gradients of the ML objective.

### Energy-based Models

Energy-based models are formulated based on a Boltzmann distribution, which is expressed as the ratio of an unnormalized density function to an input-independent normalizing constant. Specifically, given a scalar-valued energy function \(E(\cdot\,;\theta):\mathbb{R}^{D}\rightarrow\mathbb{R}\), the unnormalized density function is defined as \(\exp\left(-E(\bm{x};\theta)\right)\), and the normalizing constant \(Z(\theta)\) is defined as the integration \(\int_{\bm{x}\in\mathbb{R}^{D}}\exp\left(-E(\bm{x};\theta)\right)d\bm{x}\). The parameterization of \(p(\cdot\,;\theta)\) is presented in the following equation:

\[p(\bm{x};\theta)=\exp\left(-E(\bm{x};\theta)\right)Z^{-1}(\theta).\] (3)

Optimizing \(p(\cdot\,;\theta)\) in Eq. (3) through directly evaluating \(\mathcal{L}_{\mathrm{ML}}\) in Eq. (2) is computationally infeasible, since the computation requires explicitly calculating the intractable normalizing constant \(Z(\theta)\). To address this issue, a widely-used technique [13] is to reformulate \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{ML}}(\theta)\) as its sampling-based variant \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{SML}}(\theta)\), which is expressed as follows:

\[\mathcal{L}_{\mathrm{SML}}(\theta)=\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})}\left[ E(\bm{x};\theta)\right]-\mathbb{E}_{\mathrm{sg}(p(\bm{x};\theta))}\left[E(\bm{x}; \theta)\right],\] (4)

where \(\mathrm{sg}(\cdot)\) indicates the stop-gradient operator. Despite the fact that Eq. (4) prevents the calculation of \(Z(\theta)\), sampling from \(p(\cdot\,;\theta)\) typically requires running a Markov Chain Monte Carlo (MCMC) process (e.g., [21, 22]) until convergence, which can still be computationally expensive as it involves evaluating the gradients of the energy function numerous times. Although several approaches [23, 24] were proposed to mitigate the high computational costs involved in performing an MCMC process, these approaches make use of approximations, which often cause training instabilities in high-dimensional contexts [25].

Another line of researches proposed to optimize \(p(\cdot\,;\theta)\) through minimizing the Fisher divergence \(\mathbb{D}_{\mathrm{F}}\left[p_{\bm{\kappa}}(\bm{x})||p(\bm{x};\theta)\right] =\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})}\left[\frac{1}{2}\|\frac{ \partial}{\partial\bm{x}}\log\left(\frac{p_{\bm{\kappa}}(\bm{x})}{p(\bm{x}; \theta)}\right)\|^{2}\right]\) between \(p_{\bm{\kappa}}(\bm{x})\) and \(p(\bm{x};\theta)\) using the score-matching (SM) objective \(\mathcal{L}_{\mathrm{SM}}(\theta)=\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})}\left[ \frac{1}{2}\|\frac{\partial}{\partial\bm{x}}E(\bm{x};\theta)\|^{2}-\mathrm{Tr }(\frac{\partial^{2}}{\partial\bm{x}^{2}}E(\bm{x};\theta))\right]\)[14] to avoid the explicit calculation of \(Z(\theta)\) as well as the sampling process required in Eq. (4). Several computationally efficient variants of \(\mathcal{L}_{\mathrm{SM}}\), including sliced score matching (SSM) [16], finite difference sliced score matching (FDSSM) [17], and denoising score matching (DSM) [15], have been proposed.

SSM is derived directly based on \(\mathcal{L}_{\mathrm{SM}}\) with an unbiased Hutchinson's trace estimator [26]. Given a random projection vector \(\mathbf{v}\in\mathbb{R}^{D}\) drawn from \(p_{\mathbf{v}}\) and satisfying \(\mathbb{E}_{p_{\mathbf{v}}(\mathbf{v})}[\bm{v}^{T}\mathbf{v}]=\bm{I}\), the objective function denoted as \(\mathcal{L}_{\mathrm{SSM}}\), is defined as follows:

\[\mathcal{L}_{\mathrm{SSM}}(\theta)=\frac{1}{2}\mathbb{E}_{p_{\bm{\kappa}}(\bm {x})}\left[\left\|\frac{\partial E(\bm{x};\theta)}{\partial\bm{x}}\right\|^{ 2}\right]-\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})p_{\bm{\nu}}(\mathbf{v})}\left[ \bm{v}^{T}\frac{\partial^{2}E(\bm{x};\theta)}{\partial\bm{x}^{2}}\bm{v}\right].\] (5)

FDSSM is a parallelizable variant of \(\mathcal{L}_{\mathrm{SSM}}\) that adopts the finite difference method [27] to approximate the gradient operations in the objective. Given a uniformly distributed random vector \(\bm{\varepsilon}\), it accelerates the calculation by simultaneously forward passing \(E(\bm{x};\theta)\), \(E(\bm{x}+\bm{\varepsilon};\theta)\), and \(E(\bm{x}-\bm{\varepsilon};\theta)\) as follows:

\[\mathcal{L}_{\mathrm{FDSSM}}(\theta) =2\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})}\left[E(\bm{x};\theta) \right]-\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})p_{\bm{\varepsilon}}(\bm{ \varepsilon})}\left[E(\bm{x}+\bm{\varepsilon};\theta)+E(\bm{x}-\bm{\varepsilon };\theta)\right]\] (6) \[+\frac{1}{8}\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})p_{\bm{\varepsilon }}(\bm{\varepsilon})}\left[\left(E(\bm{x}+\bm{\varepsilon};\theta)-E(\bm{x}- \bm{\varepsilon};\theta)\right)^{2}\right],\]

where \(p_{\bm{\varepsilon}}(\bm{\varepsilon})=\mathcal{U}(\bm{\varepsilon}\in\mathbb{ R}^{D}|\,\|\bm{\varepsilon}\|=\xi)\), and \(\xi\) is a hyper-parameter that usually assumes a small value. DSM approximates the true pdf through a surrogate that is constructed using the Parzen density estimator \(p_{\sigma}(\tilde{\bm{x}})\)[28]. The approximated target \(p_{\sigma}(\tilde{\bm{x}})=\int_{\bm{x}\in\mathbb{R}^{D}}p_{\sigma}(\tilde{ \bm{x}}|\bm{x})p_{\mathbf{x}}(\bm{x})d\bm{x}\) is defined based on an isotropic Gaussian kernel \(p_{\sigma}(\tilde{\bm{x}}|\bm{x})=\mathcal{N}(\tilde{\bm{x}}|\bm{x},\sigma^{ 2}\bm{I})\) with a variance \(\sigma^{2}\). The objective \(\mathcal{L}_{\mathrm{DSM}}\), which excludes the Hessian term in \(\mathcal{L}_{\mathrm{SSM}}\), is written as follows:

\[\mathcal{L}_{\mathrm{DSM}}(\theta)=\mathbb{E}_{p_{\bm{\kappa}}(\bm{x})p_{\sigma }(\tilde{\bm{x}}|\bm{x})}\left[\frac{1}{2}\left\|\frac{\partial E(\tilde{ \bm{x}};\theta)}{\partial\tilde{\bm{x}}}+\frac{\bm{x}-\tilde{\bm{x}}}{\sigma^{2} }\right\|^{2}\right].\] (7)

To conclude, \(\mathcal{L}_{\mathrm{SSM}}\) is an unbiased objective that satisfies \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{SSM}}(\theta)=\frac{ \partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\bm{\kappa}}(\bm{x})\|p( \bm{x};\theta)\right]\)[16], while \(\mathcal{L}_{\mathrm{FDSSM}}\) and \(\mathcal{L}_{\mathrm{DSM}}\) require careful selection of hyper-parameters \(\xi\) and \(\sigma\), since \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{FDSSM}}(\theta)=\frac{ \partial}{\partial\theta}(\mathbb{D}_{\mathrm{F}}\left[p_{\bm{\kappa}}(\bm{x})\|p( \bm{x};\theta)\right]+o\left(\xi\right))\)[17] contains an approximation error \(o\left(\xi\right)\), and \(p_{\sigma}\) in \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{DSM}}(\theta)=\frac{ \partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\sigma}(\tilde{\bm{x}}) \|p(\tilde{\bm{x}};\theta)\right]\) may bear resemblance to \(p_{\bm{\kappa}}\) only for small \(\sigma\)[15].

Related Works

### Accelerating Maximum Likelihood Training of Flow-based Models

A key focus in the field of flow-based modeling is to reduce the computational expense associated with evaluating the ML objective [7; 8; 9; 10; 11; 29]. These acceleration methods can be classified into two categories based on their underlying mechanisms.

**Specially Designed Linear Transformations.** A majority of the existing works [8; 9; 10; 11; 29] have attempted to accelerate the computation of Jacobian determinants in the ML objective by exploiting linear transformations with special structures. For example, the authors in [8] proposed to constrain the weights in linear layers as lower triangular matrices to speed up training. The authors in [9; 10] proposed to adopt convolutional layers with masked kernels to accelerate the computation of Jacobian determinants. The authors in [29] leveraged orthogonal transformations to bypass the direct computation of Jacobian determinants. More recently, the authors in [12] proposed to utilize linear operations with special _butterfly_ structures [30] to reduce the cost of calculating the determinants. Although these techniques avoid the \(\mathcal{O}(D^{3}L)\) computation, they impose restrictions on the learnable transformations, which potentially limits their capacity to capture complex feature representations, as discussed in [7; 31; 32]. Our experimental findings presented in Appendix A.5 support this concept, demonstrating that flow-based models with unconstrained linear layers outperform those with linear layers restricted by lower / upper triangular weight matrices [8] or those using lower-upper (LU) decomposition [4].

**Specially Designed Optimization Process.** To address the aforementioned restrictions, a recent study [7] proposed the relative gradient method for optimizing flow-based models with arbitrary linear transformations. In this method, the gradients of the ML objective are converted into their relative gradients by multiplying themselves with \(\bm{W}^{T}\bm{W}\), where \(\bm{W}\in\mathbb{R}^{D\times D}\) represents the weight matrix in a linear transformation. Since \(\frac{\partial}{\partial\bm{W}}\log\left|\det\left(\bm{W}\right)\right|\bm{W} ^{T}\bm{W}=\bm{W}\), evaluating relative gradients is more computationally efficient than calculating the standard gradients according to \(\frac{\partial}{\partial\bm{W}}\log\left|\det\left(\bm{W}\right)\right|=(\bm {W}^{T})^{-1}\). While this method reduces the training time complexity from \(\mathcal{O}(D^{3}L)\) to \(\mathcal{O}(D^{2}L)\), a significant downside to this approach is that it introduces approximation errors with a magnitude of \(o(\bm{W})\), which can escalate relative to the weight matrix values.

### Training Flow-based Models with Score-Matching Objectives

The pioneering study [14] is the earliest attempt to train flow-based models by minimizing the SM objective. Their results demonstrate that models trained using the SM loss are able to achieve comparable or even better performance to those trained with the ML objective in a low-dimensional experimental setup. More recently, the authors in [16] and [17] proposed two efficient variants of the SM loss, i.e., the SSM and FDSSM objectives, respectively. They demonstrated that these loss functions can be used to train a non-linear independent component estimation (NICE) [1] model on high-dimensional tasks. While the training approaches of these works bear resemblance to ours, our proposed method places greater emphasis on training efficiency. Specifically, they directly implemented the energy function \(E(\bm{x};\theta)\) in the score-matching objectives as \(-\log p(\bm{x};\theta)\), resulting in a significantly higher computational cost compared to our method introduced in Section 4. In Section 5, we further demonstrate that the models trained with the methods in [16; 17] yield less satisfactory results in comparison to our approach.

## 4 Methodology

In this section, we introduce a new framework for reducing the training cost of flow-based models with linear transformations, and discuss a number of training techniques for enhancing its performance.

### Energy-Based Normalizing Flow

Instead of applying architectural constraints to reduce computational time complexity, we achieve the same goal through adopting the training objectives of energy-based models. We name this approach as Energy-Based Normalizing Flow (EBFlow). A key observation is that the parametric density function of a flow-based model can be reinterpreted as that of an energy-based model through identifyingthe input-independent multipliers in \(p(\cdot\,;\theta)\). Specifically, \(p(\cdot\,;\theta)\) can be explicitly factorized into an unnormalized density and a corresponding normalizing term as follows:

\[\begin{split} p(\bm{x};\theta)&=p_{\mathbf{u}}\left(g( \bm{x};\theta)\right)\prod_{i=1}^{L}|\det\left(\mathbf{J}_{g_{i}}(\bm{x}_{i-1}; \theta)\right)|\\ &=\underbrace{p_{\mathbf{u}}\left(g(\bm{x};\theta)\right)\prod_{g _{i}\in\mathcal{S}_{\mathbf{n}}}|\det\left(\mathbf{J}_{g_{i}}(\bm{x}_{i-1}; \theta)\right)|}_{\text{Unnormalized Density}}\underbrace{\prod_{g_{i}\in\mathcal{S} _{l}}|\det(\mathbf{J}_{g_{i}}(\theta))|}_{\text{Norm. Const.}}\triangleq \underbrace{\exp\left(-E(\bm{x};\theta)\right)}_{\text{Unnormalized Density}}\underbrace{Z^{-1}( \theta)}_{\text{Norm. Const.}}\end{split}\] (8)

where the energy function \(E(\cdot\,;\theta)\) and the normalizing constant \(Z^{-1}(\theta)\) are selected as follows:

\[E(\bm{x};\theta)\triangleq-\log\left(p_{\mathbf{u}}\left(g(\bm{x};\theta) \right)\prod_{g_{i}\in\mathcal{S}_{\mathbf{n}}}|\det(\mathbf{J}_{g_{i}}(\bm{x }_{i-1};\theta))|\,\right),Z^{-1}(\theta)=\prod_{g_{i}\in\mathcal{S}_{l}}|\det (\mathbf{J}_{g_{i}}(\theta))|\,.\] (9)

The detailed derivations of Eqs. (8) and (9) are elaborated in Lemma A.11 of Section A.1.2. By isolating the computationally expensive term in \(p(\,;\theta)\) as the normalizing constant \(Z(\theta)\), the parametric pdf defined in Eqs. (8) and (9) becomes suitable for the training methods of energy-based models. In the subsequent paragraphs, we discuss the training, inference, and convergence property of EBFlow.

**Training Cost.** Based on the definition in Eqs. (8) and (9), the score-matching objectives specified in Eqs. (5)-(7) can be adopted to prevent the Jacobian determinant calculation for the elements in \(\mathcal{S}_{l}\). As a result, the training complexity can be significantly reduced to \(\mathcal{O}(D^{2}L)\), as the \(\mathcal{O}(D^{3}L)\) calculation of \(Z(\theta)\) is completely avoided. Such a design allows the use of arbitrary linear transformations in the construction of a flow-based model without posing computational challenge during the training process. This feature is crucial to the architectural flexibility of a flow-based model. For example, fully-connected layers and convolutional layers with arbitrary padding and striding strategies can be employed in EBFlow without increasing the training complexity. EBFlow thus exhibits an enhanced flexibility in comparison to the related works that exploit specially designed linear transformations.

**Inference Cost.** Although the computational cost of evaluating the exact Jacobian determinants of the elements in \(\mathcal{S}_{l}\) still requires \(\mathcal{O}(D^{3}L)\) time, these operations can be computed only once after training and reused for subsequent inferences, since \(Z(\theta)\) is a constant as long as \(\theta\) is fixed. In cases where \(D\) is extremely large and \(Z(\theta)\) cannot be explicitly calculated, stochastic estimators such as the importance sampling techniques (e.g., [33; 34]) can be used as an alternative to approximate \(Z(\theta)\). We provide a brief discussion of such a scenario in Appendix A.3.

**Asymptotic Convergence Property.** Similar to maximum likelihood training, score-matching methods that minimize Fisher divergence have theoretical guarantees on their _consistency_[14; 16]. This property is essential in ensuring the convergence accuracy of the parameters. Let \(N\) be the number of independent and identically distributed (i.i.d.) samples drawn from \(p_{\mathbf{x}}\) to approximate the expectation in the SM objective. In addition, assume that there exists a set of optimal parameters \(\theta^{*}\) such that \(p(\bm{x};\theta^{*})=p_{\mathbf{x}}(\bm{x})\). Under the regularity conditions (i.e., Assumptions A.1-A.7 shown in Appendix A.1.1), _consistency_ guarantees that the parameters \(\theta_{N}\) minimizing the SM loss converges (in probability) to its optimal value \(\theta^{*}\) when \(N\to\infty\), i.e., \(\theta_{N}\xrightarrow{\rho}\theta^{*}\) as \(N\to\infty\). In Appendix A.1.1, we provide a formal description of this property based on [16] and derive the sufficient condition for \(g\) and \(p_{\mathbf{u}}\) to satisfy the regularity conditions (i.e., Proposition A.10).

### Techniques for Enhancing the Training of EBFlow

As revealed in the recent studies [16; 17], training flow-based models with score-matching objectives is challenging as the training process is numerically unstable and usually exhibits significant variances. To address these issues, we propose to adopt two techniques: match after preprocessing (MaP) and exponential moving average (EMA), which are particularly effective in dealing with the above issues according to our ablation analysis in Section 5.3.

**MaP.** Score-matching methods rely on the score function \(-\frac{\partial}{\partial\bm{x}}E(\bm{x};\theta)\) to match \(\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{x}}(\bm{x})\), which requires backward propagation through each layer in \(g\). This indicates that the training process could be numerically sensitive to the derivatives of \(g\). For instance, logit pre-processing layers commonly used in flow-based models (e.g., [1; 4; 5; 7; 8; 35]) exhibit extremely large derivatives near 0 and 1, which might exacerbate the above issue. To address this problem, we propose to exclude the numerically sensitive layer(s) from the model and match the pdf of the pre-processed variable during training. Specifically, let \(\mathbf{x}_{k}\triangleq g_{k}\circ\cdots\circ g_{1}(\mathbf{x})\) be the pre-processed variable, where \(k\) represents the index of the numerically sensitive layer. This method aims to optimize a parameterized pdf \(p_{k}(\cdot\,;\theta)\triangleq p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{k+1}( \cdot\,;\theta))\prod_{i=k+1}^{L}|\det\left(\mathbf{J}_{g_{i}}\right)|\) that excludes \((g_{k},\cdots,g_{1})\) through minimizing the Fisher divergence between the pdf \(p_{\mathbf{x}_{k}}(\cdot)\) of \(\mathbf{x}_{k}\) and \(p_{k}(\cdot\,;\theta)\) by considering the (local) behavior of \(\mathbb{D}_{\mathrm{F}}\), as presented in Proposition 4.1.

**Proposition 4.1**.: _Let \(p_{\mathbf{x}_{j}}\) be the pdf of the latent variable of \(\mathbf{x}_{j}\triangleq g_{j}\circ\cdots\circ g_{1}(\mathbf{x})\) indexed by \(j\). In addition, let \(p_{j}(\cdot)\) be a pdf modeled as \(p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{j+1}(\cdot))\prod_{i=j+1}^{L}|\det \left(\mathbf{J}_{g_{i}}\right)|\), where \(j\in\{0,\cdots,L-1\}\). It follows that:_

\[\mathbb{D}_{\mathrm{F}}\left[p_{\mathbf{x}_{j}}\|p_{j}\right]=0\Leftrightarrow \mathbb{D}_{\mathrm{F}}\left[p_{\mathbf{x}}\|p_{0}\right]=0,\forall j\in\{1, \cdots,L-1\}.\] (10)

The derivation is presented in Appendix A.1.3. In Section 5.3, we validate the effectiveness of the MaP technique on the score-matching methods formulated in Eqs. (5)-(7) through an ablation analysis. Please note that MaP does not affect maximum likelihood training, since it always satisfies \(\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}}\|p_{j}\right]=\mathbb{D}_{\mathrm{ KL}}\left[p_{\mathbf{x}}\|p_{0}\right]\), \(\forall j\in\{1,\cdots,L-1\}\) as revealed in Lemma A.12.

**EMA.** In addition to the MaP technique, we have also found that the exponential moving average (EMA) technique introduced in [36] is effective in improving the training stability. EMA enhances the stability through smoothly updating the parameters based on \(\tilde{\theta}\gets m\tilde{\theta}+(1-m)\theta_{i}\) at each training iteration, where \(\tilde{\theta}\) is a set of shadow parameters [36], \(\theta_{i}\) is the model's parameters at iteration \(i\), and \(m\) is the momentum parameter. In our experiments presented in Section 5, we adopt \(m=0.999\) for both EBFlow and the baselines.

## 5 Experiments

In the following experiments, we first compare the training efficiency of the baselines trained with \(\mathcal{L}_{\mathrm{ML}}\) and EBFlow trained with \(\mathcal{L}_{\mathrm{SML}}\), \(\mathcal{L}_{\mathrm{SSM}}\), \(\mathcal{L}_{\mathrm{FDSSM}}\), and \(\mathcal{L}_{\mathrm{DSM}}\) to validate the effectiveness of the proposed method in Sections 5.1 and 5.2. Then, in Section 5.3, we provide an ablation analysis of the techniques introduced in Section 4.2, and a performance comparison between EBFlow and a number of related studies [7; 16; 17]. Finally, in Section 5.4, we discuss how EBFlow can be applied to generation tasks. Please note that the performance comparison with [8; 9; 10; 11; 12; 29] is omitted, since their methods only support specialized linear layers and are not applicable to the employed model architecture [7] that involves fully-connected layers. The differences between EBFlow, the baseline, and the related studies are summarized in Table A4 in the appendix. The sampling process involved in the calculation of \(\mathcal{L}_{\mathrm{SML}}\) is implemented by \(g^{-1}(\mathbf{u};\theta)\), where \(\mathbf{u}\sim p_{\mathbf{u}}\). The transformation \(g(\cdot\,;\theta)\) for each task is designed such that \(\mathcal{S}_{l}\neq\phi\) and \(\mathcal{S}_{n}\neq\phi\). For more details about the experimental setups, please refer to Appendix A.2.

### Density Estimation on Two-Dimensional Synthetic Examples

In this experiment, we examine the performance of EBFlow and its baseline on three two-dimensional synthetic datasets. These data distributions are formed using Gaussian smoothing kernels to ensure \(p_{\mathbf{x}}(\bm{x})\) is continuous and the true score function \(\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{x}}(\bm{x})\) is well defined. The model \(g(\cdot\,;\theta)\) is constructed using the Glow model architecture [4], which consists of actnorm layers, affine coupling layers, and fully-connected layers. The performance are evaluated in terms of the KL divergence and the Fisher divergence between \(p_{\mathbf{x}}(\bm{x})\) and \(p(\bm{x};\theta)\) using independent and identically distributed (i.i.d.) testing sample points.

Table 1 and Fig. 1 demonstrate the results of the above setting. The results show that the performance of EBFlow trained with \(\mathcal{L}_{\mathrm{SSM}}\), \(\mathcal{L}_{\mathrm{FDSSM}}\), and \(\mathcal{L}_{\mathrm{DSM}}\) in terms of KL divergence is on par with those trained using \(\mathcal{L}_{\mathrm{SML}}\) as well as the baselines trained using \(\mathcal{L}_{\mathrm{ML}}\). These results validate the efficacy of training EBFlow with score matching.

Figure 1: The visualized density functions on the Sine, Swirl, and Checkerboard datasets. The column ‘True’ illustrates the visualization of the true density functions.

### Efficiency Evaluation on the MNIST and CIFAR-10 Datasets

In this section, we inspect the influence of data dimension \(D\) on the training efficiency of flow-based models. To provide a thorough comparison, we employ two types of model architectures and train them on two datasets with different data dimensions: the MNIST [19] (\(D=1\times 28\times 28\)) and CIFAR-10 [37] (\(D=3\times 32\times 32\)) datasets.

The first model architecture is exactly the same as that adopted by [7]. It is an architecture consisting of two fully-connected layers and a smoothed leaky ReLU non-linear layer in between. The second model is a parametrically efficient variant of the first model. It replaces the fully-connected layers with convolutional layers and increases the depth of the model to six convolutional blocks. Between every two convolutional blocks, a squeeze operation [2] is inserted to enlarge the receptive field. In the following paragraphs, we refer to these models as 'FC-based' and 'CNN-based' models, respectively.

The performance of the FC-based and CNN-based models is measured using the negative log likelihood (NLL) metric (i.e., \(\mathbb{E}_{p_{\mathbf{x}}(\bm{x})}[-\log p(\bm{x};\theta)]\)), which differs from the intractable KL divergence by a constant. In addition, its normalized variant, the Bits/Dim metric [38], is also measured and reported. The algorithms are implemented using PyTorch[39] with automatic differentiation [40], and the runtime is measured on NVIDIA Tesla V100 GPUs. In the subsequent paragraphs, we assess the models through scalability analysis, performance evaluation, and training efficiency examination.

**Scalability.** To demonstrate the scalability of KL-divergence-based (i.e., \(\mathcal{L}_{\mathrm{ML}}\) and \(\mathcal{L}_{\mathrm{SML}}\)) and Fisher-divergence-based (i.e., \(\mathcal{L}_{\mathrm{SSM}}\), \(\mathcal{L}_{\mathrm{DSM}}\), and \(\mathcal{L}_{\mathrm{FDSSM}}\)) objectives used in EBFlow and the baseline method, we first present a runtime comparison for different choices of the input data size \(D\). The results presented in Fig. 2 (a) reveal that Fisher-divergence-based objectives can be computed more efficiently than KL-divergence-based objectives. Moreover, the sampling-based objective \(\mathcal{L}_{\mathrm{SML}}\) used in EBFlow, which excludes the calculation of \(Z(\theta)\) in the computational graph, can be computed slightly faster than \(\mathcal{L}_{\mathrm{ML}}\) adopted by the baseline.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Dataset & Metric & Baseline (ML) & EBFlow (SSM) & EBFlow (SSM) & EBFlow (DSM) & EBFlow (FDSSM) \\ \hline \multirow{2}{*}{Sine} & Fisher Divergence (\(\downarrow\)) & 6.86 \(\pm\) 0.73 e-1 & 6.65 \(\pm\) 1.05 e-1 & **6.25 \(\pm\) 0.84 e-1** & 6.66 \(\pm\) 0.44 e-1 & 6.66 \(\pm\) 1.33 e-1 \\  & KL Divergence (\(\downarrow\)) & **4.56 \(\pm\) 0.00 e+0** & **4.56 \(\pm\) 0.00 e+0** & **4.56 \(\pm\) 0.01 e+0** & 4.57 \(\pm\) 0.02 e+0 & 4.57 \(\pm\) 0.01 e+0 \\ \hline \multirow{2}{*}{Swirl} & Fisher Divergence (\(\downarrow\)) & 1.42 \(\pm\) 0.48 e-10 & 1.42 \(\pm\) 0.53 e-10 & 1.35 \(\pm\) 0.10 e-10 & **1.34 \(\pm\) 0.06 e-1** & 1.37 \(\pm\) 0.07 e-0 \\  & KL Divergence (\(\downarrow\)) & **4.21 \(\pm\) 0.00 e+0** & **4.21 \(\pm\) 0.01 e+0** & 4.25 \(\pm\) 0.04 e-0 & 4.22 \(\pm\) 0.02 e+0 & 4.25 \(\pm\) 0.06 e+0 \\ \hline \multirow{2}{*}{Checkerboard} & Fisher Divergence (\(\downarrow\)) & 7.24 \(\pm\) 11.50 e-1 & 1.23 \(\pm\) 0.75 e-0 & 7.07 \(\pm\) 1.93 e-1 & **7.03 \(\pm\) 0.51** & 7.08 \(\pm\) 1.62 e-1 \\  & KL Divergence (\(\downarrow\)) & **4.80 \(\pm\) 0.02 e+0** & 4.81 \(\pm\) 0.02 e+0 & 4.85 \(\pm\) 0.05 e+0 & 4.82 \(\pm\) 0.05 e+0 & 4.83 \(\pm\) 0.03 e+0 \\ \hline \hline \end{tabular}
\end{table}
Table 1: The evaluation results in terms of KL-divergence and Fisher-divergence of the flow-based models trained with \(\mathcal{L}_{\mathrm{ML}}\), \(\mathcal{L}_{\mathrm{SML}}\), \(\mathcal{L}_{\mathrm{SSM}}\), \(\mathcal{L}_{\mathrm{DSM}}\), and \(\mathcal{L}_{\mathrm{FDSSM}}\) on the Sine, Swirl, and Checkerboard datasets. The results are reported as the mean and 95% confidence interval of three independent runs.

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{8}{c|}{NIST (\(D\to 54\))} \\ \cline{2-10}  & \multicolumn{8}{c|}{C-based} & \multicolumn{8}{c}{CNN-based} \\ \hline Num. Perm. & \multicolumn{8}{c}{1,230M} & \multicolumn{8}{c}{0.047 M} \\ \hline Method & Baseline (ML) & EBFlow (SSM) & EBFlow (SSM) & EBFlow (FDSSM) & Baseline (ML) & EBFlow (SM) & EBFlow (SSM) & EBFlow (OSM) & EBFlow (FDSSM) \\ \hline NLL (\(\downarrow\)) & 1002.4 \(\pm\) 0.1 & **1002.3 \(\pm\) 0.6** & 1002.8 \(\pm\) 0.3 & 1002.9 \(\pm\) 0.2 & 1104.1 \(\pm\) 0.5 & 1101.3 \(\pm\) 1.3 & **1003.3 \(\pm\) 6.6** & 11017.5 \(\pm\) 1.4 & 1109.5 \(\pm\) 2.4 & 1122.1 \(\pm\) 3.1 \\ Bin(\(\bm{x}\)) & **2.00 \(\pm\) 0.00** & **2.61 \(\pm\) 0.00** & **2.61 \(\pm\) 0.00** & 2.02 \(\pm\) 0.00 & 2.03 \(\pm\) 0.00 & **2.01 \(\pm\) 0.00** & 2.03 \(\pm\) 0.00 & 2.04 \(\pm\) 0.00 & 2.06 \(\pm\) 0.01 \\ \hline \multirow{2}{*}{Baseline (\(\uparrow\))} & \multirow{2}{*}{8.00} & \multirow{2}{*}{12.27} & 3.11 & 66.67 & **102.1** & 0.21 & 0.29 & 7.09 & 18.32 & **30.76** \\ \hline \multirow{2}{*}{Num. Perm.} & \multicolumn{8}{c}{C-based} & \multicolumn{8}{c}{CNN-based} \\ \cline{2-10}  & Num. Perm. & \multicolumn{8}{c}{18.81 M} \\ \hline Method & Baseline (ML) & EBFlow (SSM) & EBFlow (SSM) & EBFlow (Poisson) & Baseline (ML) & EBFlow (SSM) & EBFlow (OSM) & EBFlow (Poisson) \\ \hline NLL (\(\downarrow\)) & **10192.5 \(\pm\) 10.8** & 11915.6 \(\pm\) 5.6 & 11917.2 \(\pm\) 15.5 & 11910.0 \(\pm\) 0.6 & 12347.8 \(\pm\) 6.8 & **11408.7 \(\pm\) 2.67** & 11553.64 \(\pm\) 18.7 & 11465.5 \(\pm\) 12.6 & 11463.3 \(\pm\) 7.9 & 11766.0 \(\pm\) 36.8 \\ Bin(\(\bm{x}\)) & **5.59 \(\pm\) 0.00** & **5.00 \(\pm\) 0.00** & **5.00 \(\pm\) 0.01** & **5.00 \(\pm\) 0.00** & **5.00 \(\pm\) 0.00** & **5.00 \(\pm\) 0.01** & 5.41 \(\pm\) 0.07 & 5.37 \(\pm\) 0.00 & 5.38 \(\pm\) 0.00 & 5.54 \(\pm\)

**Performance.** Table 2 demonstrates the performance of the FC-based and CNN-based models in terms of NLL on the MNIST and CIFAR-10 datasets. The results show that the models trained with Fisher-divergence-based objectives are able to achieve similar performance as those trained with KL-divergence-based objectives. Among the Fisher-divergence-based objectives, the models trained using \(\mathcal{L}_{\mathrm{SSM}}\) and \(\mathcal{L}_{\mathrm{DSM}}\) are able to achieve better performance in comparison to those trained using \(\mathcal{L}_{\mathrm{FDSSM}}\). The runtime and performance comparisons above suggest that \(\mathcal{L}_{\mathrm{SSM}}\) and \(\mathcal{L}_{\mathrm{DSM}}\) can deliver better training efficiency than \(\mathcal{L}_{\mathrm{ML}}\) and \(\mathcal{L}_{\mathrm{SML}}\), since the objectives can be calculated faster while maintaining the models' performance on the NLL metric.

**Training Efficiency.** Fig. 2 (b) presents the trends of NLL versus training wall time when \(\mathcal{L}_{\mathrm{ML}}\), \(\mathcal{L}_{\mathrm{SML}}\), \(\mathcal{L}_{\mathrm{SSM}}\), \(\mathcal{L}_{\mathrm{DSM}}\), and \(\mathcal{L}_{\mathrm{FDSSM}}\) are adopted as the objectives. It is observed that EBFlow trained with SSM and DSM consistently attain better NLL in the early stages of the training. The improvement is especially notable when both \(D\) and \(L\) are large, as revealed for the scenario of training CNN-based models on the CIFAR-10 dataset. These experimental results provide evidence to support the use of score-matching methods for optimizing EBFlow.

### Analyses and Comparisons

**Ablation Study.** Table 3 presents the ablation results that demonstrate the effectiveness of the EMA and MaP techniques. It is observed that EMA is effective in reducing the variances. In addition, MaP significantly improves the overall performance. To further illustrate the influence of the proposed MaP technique on the score-matching methods, we compare the optimization pro

Figure 3: The norm of \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{SSM}}(\theta)\) of an FC-based model trained on the MNIST dataset. The curves and shaded area depict the mean and 95% confidence interval of three independent runs.

\begin{table}
\begin{tabular}{c c|c c c} \hline \hline  & \multicolumn{4}{c}{FC-based} \\ \hline EMA & MaP & EBFlow(SSM) & EBFlow(DSM) & EBFlow(FDSSM) \\ \hline  & & 1757.5 \(\pm\) 28.0 & 4660.3 \(\pm\) 19.8 & 32670 \(\pm\) 99.2 \\ ✓ & & 1720.5 \(\pm\) 0.8 & 44550 \(\pm\) 1.6 & 3166.3 \(\pm\) 17.3 \\ ✓ & ✓ & **1092.8 \(\pm\) 0.3** & **1099.2 \(\pm\) 0.2** & **1104.1 \(\pm\) 0.5** \\ \hline \multicolumn{4}{c}{CNN-based} \\ \hline EMA & MaP & EBFlow(SSM) & EBFlow(DSM) & EBFlow(FDSSM) \\ \hline  & & 35180 \(\pm\) 33.9 & 31700 \(\pm\) 7.2 & 3593.3 \(\pm\) 125. \\ ✓ & & 3504.5 \(\pm\) 2.4 & 3180.0 \(\pm\) 2.9 & 3560.3 \(\pm\) 1.7 \\ ✓ & ✓ & **1107.5 \(\pm\) 14** & **1109.5 \(\pm\) 26** & **1122.1 \(\pm\) 3.1** \\ \hline \hline \end{tabular}
\end{table}
Table 3: The results in terms of NLL of the FC-based and CNN-based models trained using SSM, DSM, and FDSSM losses on MNIST. The performance is reported in terms of the means and 95% confidence intervals of three independent runs.

Figure 2: (a) A runtime comparison of calculating the gradients of different objectives for different input sizes (\(D\)). The input sizes are \((1,n,n)\) and \((3,n,n)\), with the x-axis in the figures representing \(n\). In the format \((c,h,w)\), the first value indicates the number of channels, while the remaining values correspond to the height and width of the input data. The curves depict the evaluation results in terms of the mean of three independent runs. (b) A comparison of the training efficiency of the FC-based and CNN-based models evaluated on the validation set of MNIST and CIFAR-10. Each curve and the corresponding shaded area depict the mean and confidence interval of three independent runs.

cesses with \(\frac{\partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}_{k}}\|p_{k}\right]\) and \(\frac{\partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}}\|p_{0} \right]=\frac{\partial}{\partial\theta}\mathbb{E}_{p_{\bm{x}_{k}}(\bm{x}_{k}) }[\frac{1}{2}\|(\frac{\partial}{\partial\bm{x}_{k}}\log(\frac{p_{\bm{x}_{k}}( \bm{x}_{k})}{p_{k}(\bm{x}_{k})}))\prod_{i=1}^{k}\mathbf{J}_{g_{i}}\|^{2}]\) (i.e., Lemma A.13) by depicting the norm of their unbiased estimators \(\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{SSM}}(\theta)\) calculated with and without applying the MaP technique in Fig. 3. It is observed that the magnitude of \(\left\|\frac{\partial}{\partial\theta}\mathcal{L}_{\mathrm{SSM}}(\theta)\right\|\) significantly decreases when MaP is incorporated into the training process. This could be attributed to the fact that the calculation of \(\frac{\partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}_{k}}\|p _{k}\right]\) excludes the calculation of \(\prod_{i=1}^{k}\mathbf{J}_{g_{i}}\) in \(\frac{\partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}}\|p_{0}\right]\), which involves computing the derivatives of the numerically sensitive logit pre-processing layer.

**Comparison with Related Works.** Table 4 compares the performance of our method with a number of related works on the MNIST dataset. Our models trained with score-matching objectives using the same model architecture exhibit improved performance in comparison to the relative gradient method [7]. In addition, when compared to the results in [16] and [17], our models deliver significantly improved performance over them. Please note that the results of [7; 16; 17] presented in Table 4 are obtained from their original papers.

### Application to Generation Tasks

The sampling process of EBFlow can be accomplished through the inverse function or an MCMC process. The former is a typical generation method adopted by flow-based models, while the latter is a more flexible sampling process that allows conditional generation without re-training the model. In the following paragraphs, we provide detailed explanations and visualized results of these tasks.

**Inverse Generation.** One benefit of flow-based models is that \(g^{-1}\) can be directly adopted as a generator. While inverting the weight matrices in linear transformations typically demands time complexity of \(\mathcal{O}(D^{3}L)\), these inverse matrices are only required to be computed once \(\theta\) has converged, and can then be reused for subsequent inferences In this experiment, we adopt the Glow [4] model architecture and train it using our method with \(\mathcal{L}_{\mathrm{SSM}}\) on the MNIST dataset. We compare our visualized results with the current best flow-based model trained using the score matching objective [17]. The results of [17] are generated using their officially released code with their best setup (i.e., FDSSM). As presented in Fig. 4, the results generated using our model demonstrate significantly better visual quality than those of [17].

**MCMC Generation.** In comparison to the inverse generation method, the MCMC sampling process is more suitable for conditional generation tasks such as data imputation due to its flexibility [41]. For the imputation task, a data vector \(\bm{x}\) is separated as an observable part \(\bm{x}_{O}\) and a masked part \(\bm{x}_{M}\). The goal of imputation is to generate the masked part \(\bm{x}_{M}\) based on the observable part \(\bm{x}_{O}\). To achieve this goal, one can perform a Langevin MCMC process to update \(\bm{x}_{M}\) according to the gradient of the energy function \(\frac{\partial}{\partial\bm{x}}E(\bm{x};\theta)\). Given a noise vector \(\bm{z}\) sampled from \(\mathcal{N}(\bm{0},\bm{I})\) and a

\begin{table}
\begin{tabular}{c|c c|c} \hline \hline  & Method & Complexity & NLL (\(\downarrow\)) \\ \hline \multirow{3}{*}{\(\mathbb{D}_{\mathrm{KL}}\)-Based} & Baseline (ML) & \(\mathcal{O}(D^{3}L)\) & \(1092.4\pm 0.1\) \\  & EBFlow (SML) & \(\mathcal{O}(D^{3}L)\) & \(1092.3\pm 0.6\) \\  & Relative Grad. [7] & \(\mathcal{O}(D^{2}L)\) & \(1375.2\pm 1.4\) \\ \hline \multirow{3}{*}{\(\mathbb{D}_{\mathrm{F}}\)-Based} & EBFlow (SSM) & \(\mathcal{O}(D^{2}L)\) & \(1092.8\pm 0.3\) \\  & EBFlow (DSM) & \(\mathcal{O}(D^{2}L)\) & \(1099.2\pm 0.2\) \\ \cline{1-1}  & EBFlow (FDSSM) & \(\mathcal{O}(D^{2}L)\) & \(1104.1\pm 0.5\) \\ \cline{1-1}  & SSM [16] & - & \(3355\) \\ \cline{1-1}  & DSM [17] & - & \(3398\pm 1343\) \\ \cline{1-1}  & FDSSM [17] & - & \(1647\pm 306\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: A comparison of performance and training complexity between EBFlow and a number of related works [16; 7; 17] on the MNIST dataset.

small step size \(\alpha\), the process iteratively updates \(\bm{x}_{M}\) based on the following equation:

\[\bm{x}_{M}^{(t+1)}=\bm{x}_{M}^{(t)}-\alpha\frac{\partial}{\partial\bm{x}_{M}^{(t )}}E(\bm{x}_{O},\bm{x}_{M}^{(t)};\theta)+\sqrt{2\alpha}\bm{z},\] (11)

where \(\bm{x}_{M}^{(t)}\) represents \(\bm{x}_{M}\) at iteration \(t\in\{1,\cdots,T\}\), and \(T\) is the total number of iterations. MCMC generation requires an overall cost of \(\mathcal{O}(TD^{2}L)\), potentially more economical than the \(\mathcal{O}(D^{3}L)\) computation of the inverse generation method. Fig. 5 depicts the imputation results of the FC-based model trained using \(\mathcal{L}_{\mathrm{DSM}}\) on the CelebA [42] dataset (\(D=3\times 64\times 64\)). In this example, we implement the masking part \(\bm{x}_{M}\) using the data from the KMNIST [43] and MNIST [19] datasets.

## 6 Conclusion

In this paper, we presented EBFlow, a new flow-based modeling approach that associates the parameterization of flow-based and energy-based models. We showed that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be bypassed, resulting in an improved training time complexity. In addition, we demonstrated that the training stability and performance can be effectively enhanced through the MaP and EMA techniques. Based on the improvements in both theoretical time complexity and empirical performance, our method exhibits superior training efficiency compared to maximum likelihood training.

## Acknowledgement

The authors gratefully acknowledge the support from the National Science and Technology Council (NSTC) in Taiwan under grant number MOST 111-2223-E-007-004-MY3, as well as the financial support from MediaTek Inc., Taiwan. The authors would also like to express their appreciation for the donation of the GPUs from NVIDIA Corporation and NVIDIA AI Technology Center (NVAITC) used in this work. Furthermore, the authors extend their gratitude to the National Center for High-Performance Computing (NCHC) for providing the necessary computational and storage resources.

## References

* [1] L. Dinh, D. Krueger, and Y. Bengio. NICE: Non-linear Independent Components Estimation, 2015.
* [2] L. Dinh, J. N. Sohl-Dickstein, and S. Bengio. Density estimation using Real NVP. In _Proc. Int. Conf. on Learning Representations (ICLR)_, 2016.
* [3] G. Papamakarios, I. Murray, and T. Pavlakou. Masked Autoregressive Flow for Density Estimation. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2017.
* [4] D. P. Kingma and P. Dhariwal. Glow: Generative Flow with Invertible 1x1 Convolutions. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2018.
* [5] C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. Neural Spline Flows. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2019.
* [6] A. Hyvarinen and E. Oja. Independent Component Analysis: Algorithms and Applications. _Neural Networks: the Official Journal of the International Neural Network Society_, 13 4-5:411-30, 2000.
* [7] L. Gresele, G. Fissore, A. Javaloy, B. Scholkopf, and A. Hyvarinen. Relative Gradient Optimization of the Jacobian Term in Unsupervised Deep Learning. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2020.
* [8] Y. Song, C. Meng, and S. Ermon. MintNet: Building Invertible Neural Networks with Masked Convolutions. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2019.
* [9] E. Hoogeboom, R. v. d. Berg, and M. Welling. Emerging Convolutions for Generative Normalizing Flows. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2019.

* [10] X. Ma and E. H. Hovy. MaCow: Masked Convolutional Generative Flow. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2019.
* [11] Y. Lu and B. Huang. Woodbury Transformations for Deep Generative Flows. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2020.
* [12] C. Meng, L. Zhou, K. Choi, T. Dao, and S. Ermon. ButterflyFlow: Building Invertible Layers with Butterfly Matrices. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2022.
* [13] Y. LeCun, S. Chopra, R. Hadsell, A. Ranzato, and F. J. Huang. A Tutorial on Energy-Based Learning. 2006.
* [14] A. Hyvarinen. Estimation of Non-Normalized Statistical Models by Score Matching. _Journal of Machine Learning Research (JMLR)_, 6(24):695-709, 2005.
* [15] P. Vincent. A Connection between Score Matching and Denoising Autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [16] Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced Score Matching: A Scalable Approach to Density and Score Estimation. In _Proc. Conf. on Uncertainty in Artificial Intelligence (UAI)_, 2019.
* [17] T. Pang, K. Xu, C. Li, Y. Song, S. Ermon, and J. Zhu. Efficient Learning of Generative Models via Finite-Difference Score Matching. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2020.
* [18] S. Lyu. Interpretation and Generalization of Score Matching. In _Proc. Conf. on Uncertainty in Artificial Intelligence (UAI)_, 2009.
* [19] L. Deng. The MNIST Database of Handwritten Digit Images for Machine Learning Research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [20] G. Papamakarios, E. T. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing Flows for Probabilistic Modeling and Inference. _Journal of Machine Learning Research (JMLR)_, 22:57:1-57:64, 2019.
* 363, 1996.
* [22] G. O Roberts and J. S Rosenthal. Optimal Scaling of Discrete Approximations to Langevin Diffusions. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 60(1):255-268, 1998.
* [23] G. E. Hinton. Training Products of Experts by Minimizing Contrastive Divergence. _Neural Computation_, 14:1771-1800, 2002.
* [24] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2008.
* [25] Y. Du, S. Li, B. J. Tenenbaum, and I. Mordatch. Improved Contrastive Divergence Training of Energy Based Models. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2021.
* [26] M. F Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. _Communications in Statistics-Simulation and Computation_, 18(3):1059-1076, 1989.
* [27] A. Neumaier. _Introduction to Numerical Analysis_. Cambridge University Press, 2001.
* [28] E. Parzen. On Estimation of a Probability Density Function and Mode. _Annals of Mathematical Statistics_, 33:1065-1076, 1962.
* [29] J. M. Tomczak and M. Welling. Improving Variational Auto-Encoders using Householder Flow. _ArXiv_, abs/1611.09630, 2016.
* [30] T. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. Re. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2019.

* [31] J. Behrmann, D. K. Duvenaud, and J.-H. Jacobsen. Invertible Residual Networks. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2018.
* [32] R. T. Q. Chen, J. Behrmann, D. K. Duvenaud, and J.-H. Jacobsen. Residual Flows for Invertible Generative Modeling. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2019.
* [33] R. M. Neal. Annealed Importance Sampling. _Statistics and Computing_, 11:125-139, 1998.
* [34] Y. Burda, R. B. Grosse, and R. Salakhutdinov. Accurate and Conservative Estimates of MRF Log-Likelihood using Reverse Annealing. volume abs/1412.8566, 2015.
* [35] W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, and D. K. Duvenaud. FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. In _Int. Conf. on Learning Representations (ICLR)_, 2018.
* [36] Y. Song and S. Ermon. Improved Techniques for Training Score-Based Generative Models. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2020.
* [37] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.
* [38] A. V. D. Oord, N. Kalchbrenner, L. Espeholt, K. Kavukcuoglu, O. Vinyals, and A. Graves. Conditional Image Generation with PixelCNN Decoders. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2016.
* [39] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2019.
* Principles and Techniques of Algorithmic Differentiation, Second Edition. In _Frontiers in applied mathematics_, 2000.
* [41] A. M Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, and J. Yosinski. Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space. In _Proc. Int. Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [42] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep Learning Face Attributes in the Wild. In _Proc. Int. Conf. on Computer Vision (ICCV)_, December 2015.
* [43] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep Learning for Classical Japanese Literature. In _Proc. Conf. on Neural Information Processing Systems (NeurIPS)_, 2018.
* [44] T. Anderson Keller, Jorn W. T. Peters, Priyank Jaini, Emiel Hoogeboom, Patrick Forr'e, and Max Welling. Self Normalizing Flows. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2020.
* [45] C.-H. Chao, W.-F. Sun, B.-W. Cheng, Y.-C. Lo, C.-C. Chang, Y.-L. Liu, Y.-L. Chang, C.-P. Chen, and C.-Y. Lee. Denoising Likelihood Score Matching for Conditional Score-based Data Generation. In _Proc. Int. Conf. on Learning Representations (ICLR)_, 2022.
* [46] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. _CoRR_, abs/1412.6980, 2014.
* [47] I. Loshchilov and F. Hutter. Decoupled Weight Decay Regularization. In _Proc. Int. Conf. on Learning Representations (ICLR)_, 2017.
* [48] M. Ning, E. Sangineto, A. Porrello, S. Calderara, and R. Cucchiara. Input Perturbation Reduces Exposure Bias in Diffusion Models. In _Proc. Int. Conf. on Machine Learning (ICML)_, 2023.

* [49] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015. Software available from tensorflow.org.
* [50] J. Sohl-Dickstein. Two Equalities Expressing the Determinant of a Matrix in terms of Expectations over Matrix-Vector Products, 2020.
* [51] K. P. Murphy. _Probabilistic Machine Learning: Advanced Topics_, page 811. MIT Press, 2023.
* [52] M. Zhang, O. Key, P. Hayes, D. Barber, B. Paige, and F.-X. Briol. Towards Healing the Blindness of Score Matching. _Workshop on Score-Based Methods at Conf. on Neural Information Processing Systems (NeurIPS)_, 2022.

Appendix

### Derivations

In the following subsections, we provide theoretical derivations. In Section A.1.1, we discuss the asymptotic convergence properties as well as the assumptions of score-matching methods. In Section A.1.2, we elaborate on the formulation of EBFlow (i.e., Eqs. (8) and (9)), and provide a explanation of their interpretation. Finally, in Section A.1.3, we present a theoretical analysis of KL divergence and Fisher divergence, and discuss the underlying mechanism behind the proposed MaP technique.

#### a.1.1 Asymptotic Convergence Property of Score Matching

In this subsection, we provide a formal description of the _consistency_ property of score matching. The description follows [16] and the notations are replaced with those used in this paper. The regularity conditions for \(p(\cdot\,;\theta)\) are defined in Assumptions A.1\(\sim\)A.7. In the following paragraph, the parameter space is defined as \(\Theta\). In addition, \(s(\bm{x};\theta)\triangleq\frac{\partial}{\partial\bm{x}}\log p(\bm{x};\theta )=-\frac{\partial}{\partial\bm{x}}E(\bm{x};\theta)\) represents the score function. \(\hat{\mathcal{L}}_{\mathrm{SM}}(\theta)\triangleq\frac{1}{N}\sum_{k=1}^{N}f (\bm{x}_{k};\theta)\) denotes an unbiased estimator of \(\mathcal{L}_{\mathrm{SM}}(\theta)\), where \(f(\bm{x};\theta)\triangleq\frac{1}{2}\left\|\frac{\partial}{\partial\bm{x}}E (\bm{x};\theta)\right\|^{2}-\mathrm{Tr}\left(\frac{\partial^{2}}{\partial\bm{x }^{2}}E(\bm{x};\theta)\right)=\frac{1}{2}\left\|s(\bm{x};\theta)\right\|^{2}+ \mathrm{Tr}\left(\frac{\partial}{\partial\bm{x}}s(\bm{x};\theta)\right)\) and \(\{\bm{x}_{1},\cdots,\bm{x}_{N}\}\) represents a collection of i.i.d. samples drawn from \(p_{\bm{\mathrm{x}}}\). For notational simplicity, we denote \(\partial h(\bm{x};\theta)\triangleq\frac{\partial}{\partial\bm{x}}h(\bm{x}; \theta)\) and \(\partial_{i}h_{j}(\bm{x};\theta)\triangleq\frac{\partial}{\partial\bm{x}_{i}}h _{j}(\bm{x};\theta)\), where \(h_{j}(\bm{x};\theta)\) denotes the \(j\)-th element of \(h\).

**Assumption A.1**.: (Positiveness) \(p(\bm{x};\theta)>0\) and \(p_{\bm{\mathrm{x}}}(\bm{x})>0\), \(\forall\theta\in\Theta\), \(\forall\bm{x}\in\mathbb{R}^{D}\).

**Assumption A.2**.: (Regularity of the score functions) The parameterized score function \(s(\bm{x};\theta)\) and the true score function \(\frac{\partial}{\partial\bm{x}}\log p_{\bm{\mathrm{x}}}(\bm{x})\) are both continuous and differentiable. In addition, their expectations \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[s(\bm{x};\theta)\right]\) and \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[\frac{\partial}{\partial\bm{x}} \log p_{\bm{\mathrm{x}}}(\bm{x})\right]\) are finite. (i.e., \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[s(\bm{x};\theta)\right]<\infty\) and \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[\frac{\partial}{\partial\bm{x}} \log p_{\bm{\mathrm{x}}}(\bm{x})\right]<\infty\))

**Assumption A.3**.: (Boundary condition) \(\lim_{\left\|\bm{x}\right\|\to\infty}p_{\bm{\mathrm{x}}}(\bm{x})s(\bm{x}; \theta)=0\), \(\forall\theta\in\Theta\).

**Assumption A.4**.: (Compactness) The parameter space \(\Theta\) is compact.

**Assumption A.5**.: (Identifiability) There exists a set of parameters \(\theta^{*}\) such that \(p_{\bm{\mathrm{x}}}(\bm{x})=p(\bm{x};\theta^{*})\), where \(\theta^{*}\in\Theta\), \(\forall\bm{x}\in\mathbb{R}^{D}\).

**Assumption A.6**.: (Uniqueness) \(\theta\neq\theta^{*}\Leftrightarrow p(\bm{x};\theta)\neq p(\bm{x};\theta^{*})\), where \(\theta,\theta^{*}\in\Theta\), \(\bm{x}\in\mathbb{R}^{D}\).

**Assumption A.7**.: (Lipschitzness of \(f\)) The function \(f\) is Lipschitz continuous w.r.t. \(\theta\), i.e., \(\left|f(\bm{x};\theta_{1})-f(\bm{x};\theta_{2})\right|\leq L(\bm{x})\left\| \theta_{1}-\theta_{2}\right\|_{2}\), \(\forall\theta_{1},\theta_{2}\in\Theta\), where \(L(\bm{x})\) represents a Lipschitz constant satisfying \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[L(\bm{x})\right]<\infty\).

**Theorem A.8**.: _(Consistency of a score-matching estimator [16]) The score-matching estimator \(\theta_{N}\triangleq\operatorname*{argmin}_{\theta\in\Theta}\hat{\mathcal{ L}}_{\mathrm{SM}}\) is consistent, i.e.,_

\[\theta_{N}\xrightarrow{p}\theta^{*},\text{ as }N\to\infty.\]

Assumptions A.1\(\sim\)A.3 are the conditions that ensure \(\frac{\partial}{\partial\theta}\mathbb{D}_{\mathrm{F}}\left[p_{\bm{\mathrm{x}} }(\bm{x})\|p(\bm{x};\theta)\right]=\frac{\partial}{\partial\theta}\mathcal{L }_{\mathrm{SM}}(\theta)\). Assumptions A.4\(\sim\)A.7 lead to the uniform convergence property [16] of a score-matching estimator, which gives rise to the _consistency_ property. The detailed derivation can be found in Corollary 1 in [16]. In the following Lemma A.9 and Proposition A.10, we examine the sufficient condition for \(g\) and \(p_{\bm{\mathrm{u}}}\) to satisfy Assumption A.7.

**Lemma A.9**.: _(Sufficient condition for the Lipschitzness of \(f\)) The function \(f(\bm{x};\theta)=\frac{1}{2}\left\|s(\bm{x};\theta)\right\|^{2}+\mathrm{Tr} \left(\frac{\partial}{\partial\bm{x}}s(\bm{x};\theta)\right)\) is Lipschitz continuous if the score function \(s(\bm{x};\theta)\) satisfies the following conditions: \(\forall\theta,\theta_{1},\theta_{2}\in\Theta\), \(\forall i\in\{1,\cdots,D\}\),_

\[\left\|s(\bm{x};\theta)\right\|_{2}\leq L_{1}(\bm{x}),\] \[\left\|s(\bm{x};\theta_{1})-s(\bm{x};\theta_{2})\right\|_{2}\leq L _{2}(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2},\] \[\left\|\partial_{i}s(\bm{x};\theta_{1})-\partial_{i}s(\bm{x}; \theta_{2})\right\|_{2}\leq L_{3}(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2},\]

_where \(L_{1}\), \(L_{2}\), and \(L_{3}\) are Lipschitz constants satisfying \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[L_{1}(\bm{x})\right]<\infty\), \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[L_{2}(\bm{x})\right]<\infty\), and \(\mathbb{E}_{p_{\bm{\mathrm{x}}}(\bm{x})}\left[L_{3}(\bm{x})\right]<\infty\)._Proof.: The Lipschitzness of \(f\) can be guaranteed by ensuring the Lipschitzness of \(\left\|s(\bm{x};\theta)\right\|_{2}^{2}\) and \(\operatorname{Tr}\left(\partial s(\bm{x};\theta)\right)\).

**Step 1.** (Lipschitzness of \(\left\|s(\bm{x};\theta)\right\|_{2}^{2}\))

\[\left|\left\|s(\bm{x};\theta_{1})\right\|_{2}^{2}-\left\|s(\bm{x} ;\theta_{2})\right\|_{2}^{2}\right|\] \[=\left|s(\bm{x};\theta_{1})^{T}s(\bm{x};\theta_{1})-s(\bm{x}; \theta_{2})^{T}s(\bm{x};\theta_{2})\right|\] \[=\left|\left(s(\bm{x};\theta_{1})^{T}s(\bm{x};\theta_{1})-s(\bm{ x};\theta_{1})^{T}s(\bm{x};\theta_{2})\right)+\left(s(\bm{x};\theta_{1})^{T}s( \bm{x};\theta_{2})-s(\bm{x};\theta_{2})^{T}s(\bm{x};\theta_{2})\right)\right|\] \[=\left|s(\bm{x};\theta_{1})^{T}\left(s(\bm{x};\theta_{1})-s(\bm{ x};\theta_{2})\right)+s(\bm{x};\theta_{2})^{T}\left(s(\bm{x};\theta_{1})-s(\bm{x}; \theta_{2})\right)\right|\] \[\overset{(i)}{\leq}\left|s(\bm{x};\theta_{1})^{T}\left(s(\bm{x} ;\theta_{1})-s(\bm{x};\theta_{2})\right)\right|+\left|s(\bm{x};\theta_{2})^{T }\left(s(\bm{x};\theta_{1})-s(\bm{x};\theta_{2})\right)\right|\] \[\overset{(ii)}{\leq}\left\|s(\bm{x};\theta_{1})\right\|_{2}\left\| s(\bm{x};\theta_{1})-s(\bm{x};\theta_{2})\right\|_{2}+\left\|s(\bm{x};\theta_{2}) \right\|_{2}\left\|s(\bm{x};\theta_{1})-s(\bm{x};\theta_{2})\right\|_{2}\] \[\overset{(iii)}{\leq}L_{1}(\bm{x})\left\|s(\bm{x};\theta_{1})-s( \bm{x};\theta_{2})\right\|_{2}+L_{1}(\bm{x})\left\|s(\bm{x};\theta_{1})-s(\bm {x};\theta_{2})\right\|_{2}\] \[\overset{(iii)}{\leq}2L_{1}(\bm{x})L_{2}(\bm{x})\left\|\theta_{1} -\theta_{2}\right\|_{2},\]

where \((i)\) is based on triangle inequality, \((ii)\) is due to Cauchy-Schwarz inequality, and \((iii)\) follows from the listed assumptions.

**Step 2.** (Lipschitzness of \(\operatorname{Tr}\left(\partial s(\bm{x};\theta)\right)\))

\[\left|\operatorname{Tr}\left(\partial s(\bm{x};\theta_{1})\right) -\operatorname{Tr}\left(\partial s(\bm{x};\theta_{2})\right)\right| =\left|\operatorname{Tr}\left(\partial s(\bm{x};\theta_{1})- \partial s(\bm{x};\theta_{2})\right)\right|\] \[\overset{(i)}{\leq}D\left\|\partial s(\bm{x};\theta_{1})-\partial s (\bm{x};\theta_{2})\right\|_{2}\] \[\overset{(ii)}{\leq}D\sqrt{\sum_{i}\left\|\partial_{i}s(\bm{x} ;\theta_{1})-\partial_{i}s(\theta_{2})\right\|_{2}^{2}}\] \[\overset{(iii)}{\leq}D\sqrt{DL_{3}^{2}(\bm{x})\left\|\theta_{1} -\theta_{2}\right\|_{2}^{2}}\] \[=D\sqrt{D}L_{3}(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2}\]

where \((i)\) holds by Von Neumann's trace inequality. \((ii)\) is due to the property \(\left\|A\right\|_{2}\leq\sqrt{\sum_{i}\left\|\bm{a}_{i}\right\|_{2}^{2}}\), where \(\bm{a}_{i}\) is the column vector of \(A\). \((iii)\) holds by the listed assumptions.

Based on Steps 1 and 2, the Lipschitzness of \(f\) is guaranteed, since

\[\left|f(\bm{x};\theta_{1})-f(\bm{x};\theta_{2})\right| =\left|\frac{1}{2}\left\|s(\bm{x};\theta_{1})\right\|^{2}+ \operatorname{Tr}\left(\frac{\partial}{\partial\bm{x}}s(\bm{x};\theta_{1}) \right)-\frac{1}{2}\left\|s(\bm{x};\theta_{2})\right\|^{2}-\operatorname{Tr} \left(\frac{\partial}{\partial\bm{x}}s(\bm{x};\theta_{2})\right)\right|\] \[=\left|\frac{1}{2}\left\|s(\bm{x};\theta_{1})\right\|^{2}-\frac{1 }{2}\left\|s(\bm{x};\theta_{2})\right\|^{2}+\operatorname{Tr}\left(\frac{ \partial}{\partial\bm{x}}s(\bm{x};\theta_{1})\right)-\operatorname{Tr}\left( \frac{\partial}{\partial\bm{x}}s(\bm{x};\theta_{2})\right)\right|\] \[\leq\frac{1}{2}\left\|s(\bm{x};\theta_{1})\right\|^{2}-\left\|s( \bm{x};\theta_{2})\right\|^{2}\right|+\left|\operatorname{Tr}\left(\frac{ \partial}{\partial\bm{x}}s(\bm{x};\theta_{1})\right)-\operatorname{Tr}\left( \frac{\partial}{\partial\bm{x}}s(\bm{x};\theta_{2})\right)\right|\] \[\leq L_{1}(\bm{x})L_{2}(\bm{x})\left\|\theta_{1}-\theta_{2} \right\|_{2}+D\sqrt{D}L_{3}(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2}\] \[=\left(L_{1}(\bm{x})L_{2}(\bm{x})+D\sqrt{D}L_{3}(\bm{x})\right) \left\|\theta_{1}-\theta_{2}\right\|_{2}.\]

**Proposition A.10**.: _(Sufficient condition for the Lipschitzness of \(f\)) The function \(f\) is Lipschitz continuous if \(g(\bm{x};\theta)\) has bounded first, second, and third-order derivatives, i.e., \(\forall i,j\in\{1,\cdots,D\}\), \(\forall\theta\in\Theta\)._

\[\left\|\mathbf{J}_{g}(\bm{x};\theta)\right\|_{2}\leq l_{1}(\bm{x}),\left\| \partial_{i}\mathbf{J}_{g}(\bm{x};\theta)\right\|_{2}\leq l_{2}(\bm{x}),\left\| \partial_{i}\partial_{j}\mathbf{J}_{g}(\bm{x};\theta)\right\|_{2}\leq l_{3}( \bm{x}),\]

_and smooth enough on \(\Theta\), i.e., \(\theta_{1},\theta_{2}\in\Theta\):_

\[\left\|g(\bm{x};\theta_{1})-g(\bm{x};\theta_{2})\right\|_{2}\leq r_{0}(\bm{x}) \left\|\theta_{1}-\theta_{2}\right\|_{2},\]\[\left\|\mathbf{J}_{g}(\bm{x};\theta_{1})-\mathbf{J}_{g}(\bm{x};\theta_{2}) \right\|_{2}\leq r_{1}(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2},\] \[\left\|\partial_{i}\mathbf{J}_{g}(\bm{x};\theta_{1})-\partial_{i} \mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2}\leq r_{2}(\bm{x})\left\|\theta_ {1}-\theta_{2}\right\|_{2}.\] \[\left\|\partial_{i}\partial_{j}\mathbf{J}_{g}(\bm{x};\theta_{1})- \partial_{i}\partial_{j}\mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2}\leq r_{3 }(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2}.\]

_In addition, it satisfies the following conditions:_

\[\left\|\mathbf{J}_{g}^{-1}(\bm{x};\theta)\right\|_{2}\leq l_{1}^{{}^{\prime}} (\bm{x}),\left\|\partial_{i}\mathbf{J}_{g}^{-1}(\bm{x};\theta)\right\|_{2} \leq l_{2}^{{}^{\prime}}(\bm{x}),\] \[\left\|\mathbf{J}_{g}^{-1}(\bm{x};\theta_{1})-\mathbf{J}_{g}^{-1} (\bm{x};\theta_{2})\right\|_{2}\leq r_{1}^{{}^{\prime}}(\bm{x})\left\|\theta _{1}-\theta_{2}\right\|_{2},\] \[\left\|\partial_{i}\mathbf{J}_{g}^{-1}(\bm{x};\theta_{1})- \partial_{i}\mathbf{J}_{g}^{-1}(\bm{x};\theta_{2})\right\|_{2}\leq r_{2}^{{}^ {\prime}}(\bm{x})\left\|\theta_{1}-\theta_{2}\right\|_{2},\]

_where \(\mathbf{J}_{g}^{-1}\) represents the inverse matrix of \(\mathbf{J}_{g}\). Furthermore, the prior distribution \(p_{\mathbf{u}}\) satisfies:_

\[\left\|s_{\mathbf{u}}(\bm{u})\right\|\leq t_{1},\left\|\partial_{i}s_{ \mathbf{u}}(\bm{u})\right\|\leq t_{2}\] \[\left\|\delta_{i}s_{\mathbf{u}}(\bm{u}_{1})-\partial_{i}s_{ \mathbf{u}}(\bm{u}_{2})\right\|_{2}\leq t_{4}\left\|\bm{u}_{1}-\bm{u}_{2} \right\|_{2},\]

_where \(s_{\mathbf{u}}(\bm{u})\triangleq\frac{\partial}{\partial\bm{u}}\log p_{ \mathbf{u}}(\bm{u})\) is the score function of \(p_{\mathbf{u}}\). The Lipschitz constants listed above (i.e., \(l_{1}\sim l_{3}\), \(r_{0}\sim r_{3}\), \(l_{1}^{{}^{\prime}}\sim l_{2}^{{}^{\prime}}\), and \(r_{1}^{{}^{\prime}}\sim r_{2}^{{}^{\prime}}\)) have finite expectations._

Proof.: We show that the sufficient conditions stated in Lemma A.9 can be satisfied using the conditions listed above.

**Step 1.** (Sufficient condition of \(\left\|s(\bm{x};\theta)\right\|_{2}\leq L_{1}(\bm{x})\))

Since \(\left\|s(\bm{x};\theta)\right\|_{2}=\left\|\frac{\partial}{\partial\bm{x}} \log p_{\mathbf{u}}(g(\bm{x};\theta))+\frac{\partial}{\partial\bm{x}}\log \left|\det\mathbf{J}_{g}(\bm{x};\theta)\right|\right\|_{2}\leq\left\|\frac{ \partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x};\theta))\right\|_{2}+ \left\|\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x}; \theta)\right|\right\|_{2}\), we first demonstrate that \(\left\|\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x};\theta)) \right\|_{2}\) and \(\left\|\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x}; \theta)\right|\right\|_{2}\) are both bounded.

(1.1) \[\left\|\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x};\theta)) \right\|_{2}=\left\|\left(s_{\mathbf{u}}(g(\bm{x};\theta))\right)^{T}\mathbf{ J}_{g}(\bm{x};\theta)\right\|_{2}\leq\left\|s_{\mathbf{u}}(g(\bm{x};\theta)) \right\|_{2}\left\|\mathbf{J}_{g}(\bm{x};\theta)\right\|_{2}\leq t_{1}l_{1}( \bm{x}).\]

(1.2) \(\left\|\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x}; \theta)\right|\right\|\) is bounded:

\[\left\|\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x}; \theta)\right|\right\| =\left\|\left|\det\mathbf{J}_{g}(\bm{x};\theta)\right|^{-1}\frac{ \partial}{\partial\bm{x}}\left|\det\mathbf{J}_{g}(\bm{x};\theta)\right|\right\|\] \[=\left\|\left(\det\mathbf{J}_{g}(\bm{x};\theta)\right)^{-1}\frac{ \partial}{\partial\bm{x}}\det\mathbf{J}_{g}(\bm{x};\theta)\right\|\] \[\stackrel{{(i)}}{{=}}\left\|\left(\det\mathbf{J}_{g}( \bm{x};\theta)\right)^{-1}\det\mathbf{J}_{g}(\bm{x};\theta)\bm{v}(\bm{x}; \theta)\right\|\] \[=\left\|\bm{v}(\bm{x};\theta)\right\|,\]

where \((i)\) is derived using Jacobi's formula, and \(\bm{v}_{i}(\bm{x};\theta)=\operatorname{Tr}\left(\mathbf{J}_{g}^{-1}(\bm{x}; \theta)\partial_{i}\mathbf{J}_{g}(\bm{x};\theta)\right)\).

\[\left\|\bm{v}(\bm{x};\theta)\right\| =\sqrt{\sum_{i}\left(\operatorname{Tr}\left(\mathbf{J}_{g}^{-1} (\bm{x};\theta)\partial_{i}\mathbf{J}_{g}(\bm{x};\theta)\right)\right)^{2}}\] \[\stackrel{{(i)}}{{\leq}}\sqrt{\sum_{i}D^{2}\left\| \mathbf{J}_{g}^{-1}(\bm{x};\theta)\partial_{i}\mathbf{J}_{g}(\bm{x};\theta) \right\|_{2}^{2}}\] \[\stackrel{{(ii)}}{{\leq}}\sqrt{\sum_{i}D^{2}\left\| \mathbf{J}_{g}^{-1}(\bm{x};\theta)\right\|_{2}^{2}\left\|\partial_{i}\mathbf{J}_{g}( \bm{x};\theta)\right\|_{2}^{2}}\] \[\stackrel{{(iii)}}{{\leq}}\sqrt{\sum_{i}D^{2}l_{1}^{{}^{ \prime}2}(\bm{x})l_{2}^{2}(\bm{x})}\] \[=\sqrt{D^{3}}l_{1}^{{}^{\prime}}(\bm{x})l_{2}(\bm{x}),\]where \((i)\) holds by Von Neumann's trace inequality, \((ii)\) is due to the property of matrix norm, and \((iii)\) is follows from the listed assumptions.

**Step 2.** (Sufficient condition of the Lipschitzness of \(s(\bm{x};\theta)\))

Since \(s(\bm{x};\theta)=\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x}; \theta))+\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x}; \theta)\right|\), we demonstrate that \(\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x};\theta))\) and \(\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x};\theta)\right|\) are both Lipschitz continuous on \(\Theta\).

(2.1) Lipschitzness of \(\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x};\theta))\):

\[\left\|\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x };\theta_{1}))-\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{u}}(g(\bm{x}; \theta_{2}))\right\|_{2}\] \[=\left\|(s_{\mathbf{u}}(g(\bm{x};\theta_{1})))^{T}\,\mathbf{J}_ {g}(\bm{x};\theta_{1})-\left(s_{\mathbf{u}}(g(\bm{x};\theta_{2}))\right)^{T} \mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2}\] \[\overset{(i)}{\leq}\left\|s_{\mathbf{u}}(g(\bm{x};\theta_{1})) \right\|_{2}\left\|\mathbf{J}_{g}(\bm{x};\theta_{1})-\mathbf{J}_{g}(\bm{x}; \theta_{2})\right\|_{2}+\left\|s_{\mathbf{u}}(g(\bm{x};\theta_{1}))-s_{\mathbf{ u}}(g(\bm{x};\theta_{2}))\right\|_{2}\left\|\mathbf{J}_{g}(\bm{x};\theta_{2}) \right\|_{2}\] \[\overset{(ii)}{\leq}\left.t_{1}r_{1}(\bm{x})\left\|\theta_{1}- \theta_{2}\right\|_{2}+t_{2}l_{1}(\bm{x})\left\|g(\bm{x};\theta_{1})-g(\bm{x} ;\theta_{2})\right\|_{2}\] \[\overset{(ii)}{\leq}\left.t_{1}r_{1}(\bm{x})\left\|\theta_{1}- \theta_{2}\right\|_{2}+t_{2}l_{1}(\bm{x})r_{0}(\bm{x})\left\|\theta_{1}- \theta_{2}\right\|_{2}\] \[=(t_{1}r_{1}(\bm{x})+t_{2}l_{1}(\bm{x})r_{0}(\bm{x}))\left\| \theta_{1}-\theta_{2}\right\|_{2},\]

where \((i)\) is obtained using a similar derivation to Step 1 in Lemma A.9, while \((ii)\) follows from the listed assumptions.

(2.2) Lipschitzness of \(\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x};\theta)\right|\):

Let \(\mathbf{M}(i,\bm{x};\theta)\triangleq\mathbf{J}_{g}^{-1}(\bm{x};\theta_{1}) \partial_{i}\mathbf{J}_{g}(\bm{x};\theta)\). We first demonstrate that \(\mathbf{M}\) is Lipschitz continuous:

\[\left\|\mathbf{M}(i,\bm{x};\theta_{1})-\mathbf{M}(i,\bm{x}; \theta_{2})\right\|_{2}\] \[=\left\|\mathbf{J}_{g}^{-1}(\bm{x};\theta_{1})\partial_{i} \mathbf{J}_{g}(\bm{x};\theta_{1})-\mathbf{J}_{g}^{-1}(\bm{x};\theta_{2}) \partial_{i}\mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2}\] \[\overset{(i)}{\leq}\left\|\mathbf{J}_{g}^{-1}(\bm{x};\theta_{1}) \right\|_{2}\left\|(\partial_{i}\mathbf{J}_{g}(\bm{x};\theta_{1})-\partial_{i} \mathbf{J}_{g}(\bm{x};\theta_{2}))\right\|_{2}+\left\|\mathbf{J}_{g}^{-1}( \bm{x};\theta_{1})-\mathbf{J}_{g}^{-1}(\bm{x};\theta_{2})\right\|_{2}\left\| \partial_{i}\mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2}\] \[\overset{(ii)}{\leq}l_{1}^{{}^{\prime}}(\bm{x})r_{2}(\bm{x}) \left\|\theta_{1}-\theta_{2}\right\|_{2}+l_{2}(\bm{x})r_{1}^{{}^{\prime}}(\bm{x })\left\|\theta_{1}-\theta_{2}\right\|_{2}\] \[=\left(l_{1}^{{}^{\prime}}(\bm{x})r_{2}(\bm{x})+l_{2}(\bm{x})r_{1} ^{{}^{\prime}}(\bm{x})\right)\left\|\theta_{1}-\theta_{2}\right\|_{2},\]

where \((i)\) is obtained by an analogous derivation of the step 1 in Lemma A.9, and \((ii)\) holds by the listed assumption.

The Lipschitzness of \(\mathbf{M}\) leads to the Lipschitzness of \(\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g}(\bm{x};\theta)\right|\), since:

\[\left\|\frac{\partial}{\partial\bm{x}}\log\left|\det\mathbf{J}_{g }(\bm{x};\theta_{1})\right|-\frac{\partial}{\partial\bm{x}}\log\left|\det \mathbf{J}_{g}(\bm{x};\theta_{2})\right|\right\|_{2}\] \[=\left\|\bm{v}(\bm{x};\theta_{1})-\bm{v}(\bm{x};\theta_{2}) \right\|_{2}\] \[=\sqrt{\sum_{i}\left(\operatorname{Tr}\left(\mathbf{M}(i,\bm{x}; \theta_{1})\right)-\operatorname{Tr}\left(\mathbf{M}(i,\bm{x};\theta_{2}) \right)\right)^{2}}\] \[=\sqrt{\sum_{i}\left(\operatorname{Tr}\left(\mathbf{M}(i,\bm{x}; \theta_{1})-\mathbf{M}(i,\bm{x};\theta_{2})\right)\right)^{2}}\] \[\overset{(i)}{\leq}\sqrt{\sum_{i}D^{2}\left\|\mathbf{M}(i,\bm{x}; \theta_{1})-\mathbf{M}(i,\bm{x};\theta_{2})\right\|_{2}^{2}}\] \[\overset{(ii)}{\leq}\sqrt{\sum_{i}D^{2}\left(l_{1}^{{}^{\prime}}( \bm{x})r_{2}(\bm{x})+l_{2}(\bm{x})r_{1}^{{}^{\prime}}(\bm{x})\right)^{2} \left\|\theta_{1}-\theta_{2}\right\|_{2}^{2}}\] \[=\sqrt{D^{3}}\left(l_{1}^{{}^{\prime}}(\bm{x})r_{2}(\bm{x})+l_{2}( \bm{x})r_{1}^{{}^{\prime}}(\bm{x})\right)\left\|\theta_{1}-\theta_{2}\right\|_{2},\]where \((i)\) holds by Von Neumann's trace inequality, \((ii)\) is due to the Lipschitzness of \(\mathbf{M}\).

**Step 3.** (Sufficient condition of the Lipschitzness of \(\partial_{i}s(\bm{x};\theta)\))

\(\partial_{i}s(\bm{x};\theta)\) can be decomposed as \(\left(\partial_{i}s_{\mathbf{u}}(g(\bm{x};\theta))\right)^{T}\mathbf{J}_{g}( \bm{x};\theta)\), \(\left(s_{\mathbf{u}}(g(\bm{x};\theta))\right)^{T}\partial_{i}\mathbf{J}_{g}( \bm{x};\theta)\), and \(\partial_{i}\left[\bm{v}(\bm{x};\theta)\right]\) as follows:

\[\partial_{i}s(\bm{x};\theta) =\partial_{i}\left[\left(s_{\mathbf{u}}(g(\bm{x};\theta))\right)^ {T}\mathbf{J}_{g}(\bm{x};\theta)\right]+\partial_{i}\left[\bm{v}(\bm{x}; \theta)\right]\] \[=\left[\left(\partial_{i}s_{\mathbf{u}}(g(\bm{x};\theta))\right) ^{T}\mathbf{J}_{g}(\bm{x};\theta)\right]+\left[\left(s_{\mathbf{u}}(g(\bm{x}; \theta))\right)^{T}\partial_{i}\mathbf{J}_{g}(\bm{x};\theta)\right]+\partial_ {i}\left[\bm{v}(\bm{x};\theta)\right].\]

(3.1) The Lipschitzness of \(\left(\partial_{i}s_{\mathbf{u}}(g(\bm{x};\theta))\right)^{T}\mathbf{J}_{g}( \bm{x};\theta)\) and \(\left(s_{\mathbf{u}}(g(\bm{x};\theta))\right)^{T}\partial_{i}\mathbf{J}_{g}( \bm{x};\theta)\) can be derived using proofs similar to that in Step 2.1:

\[\left\|\left(\partial_{i}s_{\mathbf{u}}(g(\bm{x};\theta_{1})) \right)^{T}\mathbf{J}_{g}(\bm{x};\theta_{1})-\left(\partial_{i}s_{\mathbf{u}}( g(\bm{x};\theta_{2}))\right)^{T}\mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2} \leq\left(t_{2}r_{1}(\bm{x})+t_{4}r_{0}(\bm{x})l_{1}(\bm{x})\right)\left\| \theta_{1}-\theta_{2}\right\|_{2},\] \[\left\|\left(s_{\mathbf{u}}(g(\bm{x};\theta_{1}))\right)^{T} \partial_{i}\mathbf{J}_{g}(\bm{x};\theta_{1})-\left(s_{\mathbf{u}}(g(\bm{x}; \theta_{2}))\right)^{T}\partial_{i}\mathbf{J}_{g}(\bm{x};\theta_{2})\right\|_{2 }\leq\left(t_{1}r_{2}(\bm{x})+t_{3}r_{0}(\bm{x})l_{2}(\bm{x})\right)\left\| \theta_{1}-\theta_{2}\right\|_{2}.\]

(3.2) Lipschitzness of \(\partial_{i}\left[\bm{v}(\bm{x};\theta)\right]\):

Let \(\partial_{i}\left[\bm{v}_{j}(\bm{x};\theta)\right]\triangleq\partial_{i} \mathrm{Tr}\left(\mathbf{M}(j,\bm{x};\theta)\right)=\mathrm{Tr}\left(\partial _{i}\mathbf{M}(j,\bm{x};\theta)\right)\). We first show that \(\partial_{i}\mathbf{M}(j,\bm{x};\theta)\) can be decomposed as:

\[\partial_{i}\mathbf{M}(j,\bm{x};\theta)=\partial_{i}\left(\mathbf{J}_{g}^{-1} (\bm{x};\theta)\partial_{j}\mathbf{J}_{g}(\bm{x};\theta)\right)=\left( \partial_{i}\mathbf{J}_{g}^{-1}(\bm{x};\theta)\partial_{j}\mathbf{J}_{g}(\bm {x};\theta)\right)+\left(\mathbf{J}_{g}^{-1}(\bm{x};\theta)\partial_{i} \partial_{j}\mathbf{J}_{g}(\bm{x};\theta)\right)\]

The Lipschitz constant of \(\partial_{i}\mathbf{M}\) equals to \(\left(\dot{l_{2}^{{}^{\prime}}}(\bm{x})r_{2}(\bm{x})+l_{2}(\bm{x})r_{2}^{{}^ {\prime}}(\bm{x})\right)+\left(\dot{l_{1}^{{}^{\prime}}}(\bm{x})r_{3}(\bm{x}) +l_{3}(\bm{x})r_{1}^{{}^{\prime}}(\bm{x})\right)\) based on a similar derivation as in Step 3.1. The Lipschitzness of \(\partial_{i}\mathbf{M}(j,\bm{x};\theta)\) leads to the Lipschitzness of \(\partial_{i}\left[\bm{v}(\bm{x};\theta)\right]\):

\[\left\|\partial_{i}\left[\bm{v}(\bm{x};\theta_{1})\right]-\partial _{i}\left[\bm{v}(\bm{x};\theta_{2})\right]\right\|_{2}\] \[=\sqrt{\sum_{j}\left(\mathrm{Tr}\left(\partial_{i}\mathbf{M}(j, \bm{x};\theta_{1})\right)-\mathrm{Tr}\left(\partial_{i}\mathbf{M}(j,\bm{x}; \theta_{2})\right)\right)^{2}}\] \[=\sqrt{\sum_{j}\mathrm{Tr}\left(\partial_{i}\mathbf{M}(j,\bm{x}; \theta_{1})-\partial_{i}\mathbf{M}(j,\bm{x};\theta_{2})\right)^{2}}\] \[\overset{(i)}{\leq}\sqrt{\sum_{j}D^{2}\left\|\partial_{i} \mathbf{M}(j,\bm{x};\theta_{1})-\partial_{i}\mathbf{M}(j,\bm{x};\theta_{2}) \right\|_{2}^{2}}\] \[\overset{(ii)}{\leq}\sqrt{\sum_{j}D^{2}\left(\dot{l_{2}^{{}^{ \prime}}}(\bm{x})r_{2}(\bm{x})+l_{2}(\bm{x})r_{2}^{{}^{\prime}}(\bm{x})+l_{1} ^{{}^{\prime}}(\bm{x})r_{3}(\bm{x})+l_{3}(\bm{x})r_{1}^{{}^{\prime}}(\bm{x}) \right)^{2}\left\|\theta_{1}-\theta_{2}\right)\right\|_{2}^{2}}\] \[=\sqrt{D^{3}}\left(\dot{l_{2}^{{}^{\prime}}}(\bm{x})r_{2}(\bm{x} )+l_{2}(\bm{x})r_{2}^{{}^{\prime}}(\bm{x})+l_{1}^{{}^{\prime}}(\bm{x})r_{3}( \bm{x})+l_{3}(\bm{x})r_{1}^{{}^{\prime}}(\bm{x})\right)\left\|\theta_{1}- \theta_{2}\right)\right\|_{2}\]

where \((i)\) holds by Von Neumann's trace inequality, \((ii)\) is due to the Lipschitzness of \(\partial_{i}\mathbf{M}\). 

#### a.1.2 Derivation of Eqs. (8) and (9)

Energy-based models are formulated based on the observation that any continuous pdf \(p(\bm{x};\theta)\) can be expressed as a Boltzmann distribution \(\exp\left(-E(\bm{x};\theta)\right)Z^{-1}(\theta)\)[13], where the energy function \(E(\cdot\,;\theta)\) can be modeled as any scalar-valued continuous function. In EBFlow, the energy function \(E(\bm{x};\theta)\) is selected as \(-\log(p_{\mathbf{u}}\left(g(\bm{x};\theta)\right)\prod_{g_{i}\in\mathcal{S}_{ n}}\left|\det(\mathbf{J}_{g_{i}}(\bm{x}_{i-1}\,;\theta))\right|)\) according to Eq. (9). This suggests that the normalizing constant \(Z(\theta)=\int\exp\left(-E(\bm{x};\theta)\right)d\bm{x}\) is equal to \(\left(\prod_{g_{i}\in\mathcal{S}_{i}}\left|\det(\mathbf{J}_{g_{i}}(\theta)) \right|\right)^{-1}\) according to Lemma A.11.

**Lemma A.11**.: \[\left(\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det(\mathbf{J}_{g_{i}}(\theta)) \right|\,\right)^{-1}=\int_{\bm{x}\in\mathbb{R}^{D}}p_{\mathbf{u}}\left(g(\bm {x};\theta)\right)\prod_{g_{i}\in\mathcal{S}_{n}}\left|\det(\mathbf{J}_{g_{i} }(\bm{x}_{i-1};\theta))\right|d\bm{x}.\] (A1)Proof.: \[1 =\int_{\bm{x}\in\mathbb{R}^{D}}p(\bm{x};\theta)d\bm{x}\] \[=\int_{\bm{x}\in\mathbb{R}^{D}}p_{\mathbf{u}}\left(g(\bm{x};\theta) \right)\prod_{g_{i}\in\mathcal{S}_{n}}\left|\det(\mathbf{J}_{g_{j}}(\bm{x}_{i-1 };\theta))\right|\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det(\mathbf{J}_{g_{i}}( \theta))\right|d\bm{x}\] \[=\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det(\mathbf{J}_{g_{i}}( \theta))\right|\int_{\bm{x}\in\mathbb{R}^{D}}p_{\mathbf{u}}\left(g(\bm{x}; \theta)\right)\prod_{g_{i}\in\mathcal{S}_{n}}\left|\det(\mathbf{J}_{g_{i}}(\bm {x}_{i-1};\theta))\right|d\bm{x}\]

By multiplying \(\left(\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det(\mathbf{J}_{g_{i}}(\theta)) \right|\right)^{-1}\) to both sides of the equation, we arrive at the conclusion:

\[\left(\prod_{g_{i}\in\mathcal{S}_{l}}\left|\det(\mathbf{J}_{g_{i}}(\theta)) \right|\,\right)^{-1}=\int_{\bm{x}\in\mathbb{R}^{D}}p_{\mathbf{u}}\left(g(\bm{ x};\theta)\right)\prod_{g_{i}\in\mathcal{S}_{n}}\left|\det(\mathbf{J}_{g_{i}}( \bm{x}_{i-1};\theta))\right|d\bm{x}.\]

#### a.1.3 Theoretical Analyses of KL Divergence and Fisher Divergence

In this section, we provide formal derivations for Proposition 4.1, Lemma A.12, and Lemma A.13. To ensure a clear presentation, we provide a visualization of the relationship between the variables used in the subsequent derivations in Fig. A1.

**Lemma A.12**.: _Let \(p_{\mathbf{x}_{j}}\) be the pdf of the latent variable of \(\mathbf{x}_{j}\triangleq g_{j}\circ\cdots\circ g_{1}(\mathbf{x})\) indexed by \(j\). In addition, let \(p_{j}(\cdot)\) be a pdf modeled as \(p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{j+1}(\cdot))\prod_{i=j+1}^{L}\left| \det\left(\mathbf{J}_{g_{i}}\right)\right|\), where \(j\in\{0,\cdots,L-1\}\). It follows that:_

\[\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}_{j}}\|p_{j}\right]= \mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}}\|p_{0}\right],\forall j\in\{1, \cdots,L-1\}.\] (A2)Proof.: The equivalence \(\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}}\|p_{0}\right]=\mathbb{D}_{\mathrm{KL}} \left[p_{\mathbf{x}_{j}}\|p_{j}\right]\) holds for any \(j\in\{1,\cdots,L-1\}\) since:

\[\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}}\|p_{0}\right]\] \[=\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}\left[\log\left(\frac{p_{ \mathbf{x}_{j}}(\mathbf{x})}{p_{0}(\mathbf{x})}\right)\right]\] \[=\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}\left[\log\left(\frac{p_{ \mathbf{x}_{j}}(g_{j}\circ\cdots\circ g_{1}(\mathbf{x}))\prod_{i=1}^{j}| \det\left(\mathbf{J}_{g_{i}}\right)|}{p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{ 1}(\mathbf{x}))\prod_{i=1}^{L}|\det\left(\mathbf{J}_{g_{i}}\right)|}\right)\right]\] \[=\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}\left[\log\left(\frac{p_ {\mathbf{x}_{j}}(g_{j}\circ\cdots\circ g_{1}(\mathbf{x}))}{p_{\mathbf{u}}(g_ {L}\circ\cdots\circ g_{1}(\mathbf{x}))\prod_{i=j+1}^{L}|\det\left(\mathbf{J}_ {g_{i}}\right)|}\right)\right]\] \[\stackrel{{(i)}}{{=}}\mathbb{E}_{p_{\mathbf{x}_{j}} (\mathbf{x}_{j})}\left[\log\left(\frac{p_{\mathbf{x}_{j}}(\mathbf{x}_{j})}{p_{ \mathbf{u}}(g_{L}\circ\cdots\circ g_{j+1}(\mathbf{x}_{j}))\prod_{i=j+1}^{L}| \det\left(\mathbf{J}_{g_{i}}\right)|}\right)\right]\] \[=\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}_{j}}\|p_{j}\right],\]

where \((i)\) is due to the property that \(\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}[f\circ g_{j}\circ\cdots\circ g_{1}( \mathbf{x})]=\mathbb{E}_{p_{\mathbf{x}_{j}}(\mathbf{x}_{j})}[f(\mathbf{x}_{j})]\) for a given function \(f\). Therefore, \(\mathbb{D}_{\mathrm{KL}}\left[p_{\mathbf{x}_{j}}\|p_{j}\right]=\mathbb{D}_{ \mathrm{KL}}\left[p_{\mathbf{x}}\|p_{0}\right]\), \(\forall j\in\{1,\cdots,L-1\}\). 

**Lemma A.13**.: _Let \(p_{\mathbf{x}_{j}}\) be the pdf of the latent variable of \(\mathbf{x}_{j}\triangleq g_{j}\circ\cdots\circ g_{1}(\mathbf{x})\) indexed by \(j\). In addition, let \(p_{j}(\cdot)\) be a pdf modeled as \(p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{j+1}(\cdot))\prod_{i=j+1}^{L}|\det \left(\mathbf{J}_{g_{i}}\right)|\), where \(j\in\{0,\cdots,L-1\}\). It follows that:_

\[\mathbb{D}_{\mathrm{F}}\left[p_{\mathbf{x}}\|p_{0}\right]=\mathbb{E}_{p_{ \mathbf{x}_{j}}(\mathbf{x}_{j})}\left[\frac{1}{2}\left\|\left(\frac{\partial}{ \partial\mathbf{x}_{j}}\log\left(\frac{p_{\mathbf{x}_{j}}(\mathbf{x}_{j})}{p_ {j}(\mathbf{x}_{j})}\right)\right)\prod_{i=1}^{j}\mathbf{J}_{g_{i}}\right\|^{2 }\right],\forall j\in\{1,\cdots,L-1\}.\] (A3)

Proof.: Based on the definition, the Fisher divergence between \(p_{\mathbf{x}}\) and \(p_{0}\) is written as:

\[\mathbb{D}_{\mathrm{F}}\left[p_{\mathbf{x}}\|p_{0}\right]\] \[=\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}\left[\frac{1}{2}\left\| \frac{\partial}{\partial\mathbf{x}}\log\left(\frac{p_{\mathbf{x}_{j}}(g_{j} \circ\cdots\circ g_{1}(\mathbf{x}))\prod_{i=1}^{j}|\det\left(\mathbf{J}_{g_{i }}\right)|}{p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{1}(\mathbf{x}))\prod_{i=1}^ {L}|\det\left(\mathbf{J}_{g_{i}}\right)|}\right)\right\|^{2}\right]\] \[=\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}\left[\frac{1}{2}\left\| \frac{\partial}{\partial\mathbf{x}}\log\left(\frac{p_{\mathbf{x}_{j}}(g_{j} \circ\cdots\circ g_{1}(\mathbf{x}))}{p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{ 1}(\mathbf{x}))\prod_{i=j+1}^{L}|\det\left(\mathbf{J}_{g_{i}}\right)|}\right) \right\|^{2}\right]\] \[=\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}\left[\frac{1}{2}\left\| \left(\frac{\partial}{\partial g_{j}\circ\cdots\circ g_{1}(\mathbf{x})}\log \left(\frac{p_{\mathbf{x}_{j}}(g_{j}\circ\cdots\circ g_{1}(\mathbf{x}))}{p_{ \mathbf{u}}(g_{L}\circ\cdots\circ g_{1}(\mathbf{x}))\prod_{i=j+1}^{L}|\det \left(\mathbf{J}_{g_{i}}\right)|}\right)\right)\right)\frac{\partial g_{j} \circ\cdots\circ g_{1}(\mathbf{x})}{\partial\mathbf{x}}\right\|^{2}\] \[\stackrel{{(i)}}{{=}}\mathbb{E}_{p_{\mathbf{x}}( \mathbf{x})}\left[\frac{1}{2}\left\|\left(\frac{\partial}{\partial g_{j} \circ\cdots\circ g_{1}(\mathbf{x})}\log\left(\frac{p_{\mathbf{x}_{j}}(g_{j} \circ\cdots\circ g_{1}(\mathbf{x}))}{p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{ 1}(\mathbf{x}))\prod_{i=j+1}^{L}|\det\left(\mathbf{J}_{g_{i}}\right)|}\right) \right)\right)\prod_{i=1}^{j}\mathbf{J}_{g_{i}}\right\|^{2}\right]\] \[\stackrel{{(ii)}}{{=}}\mathbb{E}_{p_{\mathbf{x}_{j}}( \mathbf{x}_{j})}\left[\frac{1}{2}\left\|\left(\frac{\partial}{\partial\mathbf{x}_ {j}}\log\left(\frac{p_{\mathbf{x}_{j}}(\mathbf{x}_{j})}{p_{\mathbf{u}}(g_{L} \circ\cdots\circ g_{j+1}(\mathbf{x}_{j}))\prod_{i=j+1}^{L}|\det\left(\mathbf{J} _{g_{i}}\right)|}\right)\right)\prod_{i=1}^{j}\mathbf{J}_{g_{i}}\right\|^{2} \right],\] \[=\mathbb{E}_{p_{\mathbf{x}_{j}}(\mathbf{x}_{j})}\left[\frac{1}{2} \left\|\left(\frac{\partial}{\partial\mathbf{x}_{j}}\log\left(\frac{p_{ \mathbf{x}_{j}}(\mathbf{x}_{j})}{p_{j}(\mathbf{x}_{j})}\right)\right)\prod_{i=1}^ {j}\mathbf{J}_{g_{i}}\right\|^{2}\right],\]

where \((i)\) is due to the chain rule, and \((ii)\) is because \(\mathbb{E}_{p_{\mathbf{x}}(\mathbf{x})}[f\circ g_{j}\circ\cdots\circ g_{1}( \mathbf{x})]=\mathbb{E}_{p_{\mathbf{x}_{j}}(\mathbf{x}_{j})}[f(\mathbf{x}_{j})]\) for a given function \(f\).

_Remark A.14_.: Lemma A.13 implies that \(\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}_{j}}\|p_{j}\right]\neq\mathbb{D}_{\mathrm{F }}\left[p_{\bm{x}}\|p_{0}\right]\) in general, as the latter contains an additional multiplier \(\prod_{i=1}^{j}\mathbf{J}_{g_{i}}\) as shown below:

\[\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}}\|p_{0}\right] =\mathbb{E}_{p_{\bm{x}_{j}}(\bm{x}_{j})}\left[\frac{1}{2}\left\| \left(\frac{\partial}{\partial\bm{x}_{j}}\log\left(\frac{p_{\bm{x}_{j}}(\bm{x} _{j})}{p_{j}(\bm{x}_{j})}\right)\right)\right)\prod_{i=1}^{j}\mathbf{J}_{g_{i}} \right\|^{2}\right],\] \[\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}_{j}}\|p_{j}\right] =\mathbb{E}_{p_{\bm{x}_{j}}(\bm{x}_{j})}\left[\frac{1}{2}\left\| \left(\frac{\partial}{\partial\bm{x}_{j}}\log\left(\frac{p_{\bm{x}_{j}}(\bm{x} _{j})}{p_{j}(\bm{x}_{j})}\right)\right)\right\|^{2}\right].\]

**Proposition 4.1**.: _Let \(p_{\bm{x}_{j}}\) be the pdf of the latent variable of \(\bm{x}_{j}\triangleq g_{j}\circ\cdots\circ g_{1}(\mathbf{x})\) indexed by \(j\). In addition, let \(p_{j}(\cdot)\) be a pdf modeled as \(p_{\mathbf{u}}(g_{L}\circ\cdots\circ g_{j+1}(\cdot))\prod_{i=j+1}^{L}|\det \left(\mathbf{J}_{g_{i}}\right)|\), where \(j\in\{0,\cdots,L-1\}\). It follows that:_

\[\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}_{j}}\|p_{j}\right]=0\Leftrightarrow \mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}}\|p_{0}\right]=0,\forall j\in\{1, \cdots,L-1\}.\] (A4)

Proof.: Based on Remark A.14, the following holds:

\[\mathbb{D}_{\mathrm{F}}\left[p_{\bm{x}_{j}}\|p_{j}\right]=\mathbb{ E}_{p_{\bm{x}_{j}}(\bm{x}_{j})}\left[\frac{1}{2}\left\|\frac{\partial}{ \partial\bm{x}_{j}}\log\left(\frac{p_{\bm{x}_{j}}(\bm{x}_{j})}{p_{j}(\bm{x}_{ j})}\right)\right\|^{2}\right]=0\] \[\overset{(i)}{\Leftrightarrow}\left\|\frac{\partial}{\partial\bm {x}_{j}}\log\left(\frac{p_{\bm{x}_{j}}(\bm{x}_{j})}{p_{j}(\bm{x}_{j})}\right) \right\|^{2}=0\] \[\overset{(ii)}{\Leftrightarrow}\left\|\frac{\partial}{\partial\bm {x}_{j}}\log\left(\frac{p_{\bm{x}_{j}}(\bm{x}_{j})}{p_{j}(\bm{x}_{j})}\right) \right\|_{i=1}^{j}\mathbf{J}_{g_{i}}\right\|^{2}=0\] \[\overset{(i)}{\Leftrightarrow}\mathbb{D}_{\mathrm{F}}\left[p_{ \bm{x}}\|p_{0}\right]=\mathbb{E}_{p_{\bm{x}_{j}}(\bm{x}_{j})}\left[\frac{1}{2} \left\|\left(\frac{\partial}{\partial\bm{x}_{j}}\log\left(\frac{p_{\bm{x}_{j}} (\bm{x}_{j})}{p_{j}(\bm{x}_{j})}\right)\right)\right)\prod_{i=1}^{j}\mathbf{J} _{g_{i}}\right\|^{2}\right]=0,\]

where \((i)\) and \((ii)\) both result from the positiveness condition presented in Assumption A.1. Specifically, for \((i)\), \(p_{\bm{x}_{j}}(\bm{x}_{j})=p_{\mathbf{x}}(g_{1}^{-1}\circ\cdots\circ g_{j}^{-1 }(\bm{x}_{j}))\prod_{i=1}^{j}\left|\det\left(\mathbf{J}_{g_{i}^{-1}}\right) \right|>0\), since \(p_{\mathbf{x}}>0\) and \(\prod_{i=1}^{j}\left|\det\left(\mathbf{J}_{g_{i}^{-1}}\right)\right|=\prod_{i=1 }^{j}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|^{-1}>0\). Meanwhile \((ii)\) holds since \(\prod_{i=1}^{j}\left|\det\left(\mathbf{J}_{g_{i}}\right)\right|>0\) and thus all of the singular values of \(\prod_{i=1}^{j}\mathbf{J}_{g_{i}}\) are non-zero. 

### Experimental Setups

In this section, we elaborate on the experimental setups and provide the detailed configurations for the experiments presented in Section 5 of the main manuscript. The code implementation for the experiments is provided in the following repository: https://github.com/chen-hao-chao/ebflow. Our code implementation is developed based on [7, 17, 44].

#### a.2.1 Experimental Setups for the Two-Dimensional Synthetic Datasets

**Datasets.** In Section 5.1, we present the experimental results on three two-dimensional synthetic datasets: Sine, Swirl, and Checkerboard. The Sine dataset is generated by sampling data points from the set \(\{(4w-2,\sin(12w-6))\,|\,w\in[0,1]\}\). The Swirl dataset is generated by sampling data points from the set \(\{(-\pi\sqrt{w}\cos(\pi\sqrt{w}),\pi\sqrt{w}\sin(\pi\sqrt{w}))\,|\,w\in[0,1]\}\). The Checkerboard dataset is generated by sampling data points from the set \(\{(4w-2,t-2s+\lfloor 4w-2\rfloor\,\mathrm{mod}\,2)\,|\,w\in[0,1],\,t\in[0,1],\,s\in\{0,1\}\}\), where \(\lfloor\,\cdot\rfloor\) is a floor function, and \(\mathrm{mod}\) represents the modulo operation.

To establish \(p_{\mathbf{x}}\) for all three datasets, we smooth a Dirac function using a Gaussian kernel. Specifically, we define the Dirac function as \(\hat{p}(\hat{\bm{x}})\triangleq\frac{1}{M}\sum_{i=1}^{M}\delta(\left\|\hat{\bm{x }}-\hat{\bm{x}}^{(i)}\right\|)\), where \(\{\hat{\bm{x}}^{(i)}\}_{i=1}^{M}\) are \(M\) uniformly-sampled data points. The data distribution is defined as \(p_{\mathbf{x}}(\bm{x})\triangleq\int\hat{p}(\hat{\bm{x}})\mathcal{N}(\bm{x}| \hat{\bm{x}},\hat{\sigma}^{2}\bm{I})d\hat{\bm{x}}=\frac{1}{M}\sum_{i=1}^{M} \mathcal{N}(\bm{x}|\hat{\bm{x}}^{(i)},\hat{\sigma}^{2}\bm{I})\). The closed-form expressions for \(p_{\mathbf{x}}(\bm{x})\) and \(\frac{\partial}{\partial\bm{x}}\log p_{\mathbf{x}}(\bm{x})\) can be obtained using the derivation in [45]. In the experiments, \(M\) is set as \(50,000\), and \(\hat{\sigma}\) is fixed at \(0.375\) for all three datasets.

**Implementation Details.** The model architecture of \(g(\cdot\,;\theta)\) consists of ten Glow blocks [4]. Each block comprises an actnorm [4] layer, a fully-connected layer, and an affine coupling layer. Table A2 provides the formal definitions of these operations. \(p_{\mathbf{u}}(\cdot)\) is implemented as an isotropic Gaussian with zero mean and unit variance. To determine the best hyperparameters, we perform a grid search over the following optimizers, learning rates, and gradient clipping values based on the evaluation results in terms of the KL divergence. The optimizers include Adam [46], AdamW [47], and RMSProp. The learning rate and gradient clipping values are selected from (5e-3, 1e-3, 5e-4, 1e-4) and (None, 2.5, 10.0), respectively. Table A1 summarizes the selected hyperparameters. The optimization processes of Sine and Swirl datasets require 50,000 training iterations for convergence, while that of the Checkerboard dataset requires 100,000 iterations. The batch size is fixed at 5,000 for all setups.

#### a.2.2 Experimental Setups for the Real-world Datasets

**Datasets.** The experiments presented in Section 5.2 are performed on the MNIST [19] and CIFAR-10 [37] datasets. The training and test sets of MNIST and CIFAR-10 contain 50,000 and 10,000 images, respectively. The data are smoothed using the uniform dequantization method presented in [1]. The observable parts (i.e., \(\bm{x}_{O}\)) of the images in Fig. 5 are produced using the pre-trained model in [48].

**Implementation Details.** In Sections 5.2 and 5.4, we adopt three types of model architectures: FC-based [7], CNN-based, and Glow [4] models. The FC-based model contains two fully-connected layers and a smoothed leaky ReLU non-linearity [7] in between, which is identical to [7]. The CNN-based model consists of three convolutional blocks and two squeezing operations [2] between every convolutional block. Each convolutional block contains two convolutional layers and a smoothed leaky ReLU in between. The Glow model adopted in Section 5.4 is composed of 16 Glow blocks. Each of the Glow block consists of an actnorm [4] layer, a convolutional layer, and an affine coupling layer. The squeezing operation is inserted between every eight blocks. The operations used in these models are summarized in Table A2. The smoothness factor \(\alpha\) of Smooth Leaky ReLU is set to 0.3 and 0.6 for models trained on MNIST and CIFAR-10, respectively. The scaling and transition functions \(s(\cdot\,;\theta)\) and \(t(\cdot\,;\theta)\) of the affine coupling layers are convolutional blocks with ReLU activation functions. The prior distribution \(p_{\mathbf{u}}(\cdot)\) is implemented as an isotropic Gaussian with zero mean and unit variance. The FC-based and CNN-based models are trained with RMSProp using a learning rate initialized at 1e-4 and a batch size of 100. The Glow model is trained with an Adam optimizer using a learning rate initialized at 1e-4 and a batch size of 100. The gradient clipping value is set to 500 during the training for the Glow model. The learning rate scheduler MultiStepLR in PyTorch is used for gradually decreasing the learning rates. The hyper-parameters \(\{\sigma,\xi\}\) used in DSM and FDSSM are selected based on a grid search over \(\{0.05,0.1,0.5,1.0\}\). The selected \(\{\sigma,\xi\}\) are \(\{1.0,1.0\}\) and \(\{0.1,0.1\}\) for the MNIST and CIFAR-10 datasets, respectively. The parameter \(m\) in EMA is set to 0.999. The algorithms are implemented using PyTorch[39]. The gradients w.r.t. \(\bm{x}\) and \(\theta\) are both calculated using automatic differential tools [40] provided by PyTorch[39]. The runtime is evaluated on Tesla V100 NVIDIA GPUs. In the experiments performed on CIFAR-10 and CelebA using score-matching methods, the energy function (i.e., \(\mathbb{E}_{p_{\mathbf{x}}(\bm{x})}\left[E(\bm{x};\theta)\right]\)) is added as a regularization loss with a balancing factor fixed at \(0.001\) during the optimization processes. The results in Fig. 2 (b) are smoothed with the exponential moving average function used in Tensorboard[49], i.e., \(w\times d_{i-1}+(1-w)\times d_{i}\), where \(w\) is set to 0.45 and \(d_{i}\) represents the evaluation result at the \(i\)-th iteration.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset & & ML & SML & SSM & DSM & FDSSM \\ \hline \multirow{3}{*}{Sine} & Optimizer & Adam & AdamW & Adam & Adam & Adam \\  & Learning Rate & 5e-4 & 5e-4 & 1e-4 & 1e-4 & 1e-4 \\  & Gradient Clip & 1.0 & None & 1.0 & 1.0 & 1.0 \\ \hline \multirow{3}{*}{Swirl} & Optimizer & Adam & Adam & Adam & Adam & Adam \\  & Learning Rate & 5e-3 & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\  & Gradient Clip & None & 10.0 & 10.0 & 10.0 & 2.5 \\ \hline \multirow{3}{*}{Checkerboard} & Optimizer & AdamW & AdamW & AdamW & AdamW & Adam \\  & Learning Rate & 1e-4 & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\ \cline{1-1}  & Gradient Clip & 10.0 & 10.0 & 10.0 & 10.0 & 10.0 \\ \hline \hline \end{tabular}
\end{table}
Table A1: The hyper-parameters used in the two-dimensional synthetic example in Section 5.1.

**Results of the Related Works.** The results of the relative gradient [7], SSM [16], and FDSSM [17] methods are directly obtained from their original paper. On the other hand, the results of the DSM method is obtained from [17]. Please note that the reported results of [16] and [17] differ from each other given that they both adopt the NICE [1] model. Specifically, the SSM method achieves NLL\(=3,355\) and NLL\(=6,234\) in [16] and [17], respectively. Moreover, the DSM method achieves NLL\(=4,363\) and NLL\(=3,398\) in [16] and [17], respectively. In Table 4, we report the results with lower NLL.

### Estimating the Jacobian Determinants using Importance Sampling

Importance sampling is a technique used to estimate integrals, which can be employed to approximate the normalizing constant \(Z(\theta)\) in an energy-based model. In this method, a pdf \(q\) with a simple closed form that can be easily sampled from is selected. The normalizing constant can then be expressed as the following formula:

\[Z(\theta)=\int_{\bm{x}\in\mathbb{R}^{D}}\exp\left(-E(\bm{x}; \theta)\right)d\bm{x} =\int_{\bm{x}\in\mathbb{R}^{D}}q(\bm{x})\frac{\exp\left(-E(\bm{x };\theta)\right)}{q(\bm{x})}d\bm{x}\] (A5) \[=\mathbb{E}_{q(\bm{x})}\left[\frac{\exp\left(-E(\bm{x};\theta) \right)}{q(\bm{x})}\right]\approx\frac{1}{M}\sum_{j=1}^{M}\frac{\exp\left(-E( \hat{\bm{x}}^{(j)};\theta)\right)}{q(\hat{\bm{x}}^{(j)})},\]

where \(\{\hat{\bm{x}}^{(j)}\}_{j=1}^{M}\) represents \(M\) i.i.d. samples drawn from \(q\). According to Lemma A.11, the Jacobian determinants of the layers in \(\mathcal{S}_{l}\) can be approximated using Eq. (A5) as follows:

\[\left(\prod_{\bm{y}_{i}\in\mathcal{S}_{l}}|\mathrm{det}(\mathbf{J}_{\bm{g}_{i }}(\theta))|\right)^{-1}\approx\frac{1}{M}\sum_{j=1}^{M}\frac{p_{\mathbf{u}} \left(g(\hat{\bm{x}}^{(j)};\theta)\right)\prod_{\bm{g}_{i}\in\mathcal{S}_{n}} \left|\mathrm{det}(\mathbf{J}_{\bm{g}_{i}}(\hat{\bm{x}}^{(j)}_{i-1};\theta)) \right|}{q(\hat{\bm{x}}^{(j)})}.\] (A6)To validate this idea, we provide a simple simulation with \(p_{\mathbf{u}}=\mathcal{N}(\mathbf{0},\bm{I})\), \(q=\mathcal{N}(\mathbf{0},\bm{I})\), \(g(\bm{x};\bm{W})=\bm{W}\bm{x}\), \(M=\{50,100,200\}\), and \(D=\{50,100,200\}\) in Table A3. The results show that larger values of \(M\) lead to more accurate estimation of the Jacobian determinants. Typically, the choice of \(q\) is crucial to the accuracy of importance sampling. To obtain an accurate approximation, one can adopt the technique of annealed importance sampling (AIS) [33] or Reverse AIS Estimator (RAISE) [34], which are commonly-adopted algorithms for effectively estimating \(Z(\theta)\).

Eq. (A6) can be interpreted as a generalization of the stochastic estimator presented in [50], where the distributions \(p_{\mathbf{u}}\) and \(q\) are modeled as isotropic Gaussian distributions, and \(g\) is restricted as a linear transformation. For the further analysis of this concept, particularly in the context of determinant estimation for matrices, we refer readers to Section I of [50], where a more sophisticated approximation approach and the corresponding experimental findings are provided.

### A Comparison among the Methods Discussed in this Paper

In Sections 2, 3, and 4, we discuss various methods for efficiently training flow-based models. To provide a comprehensive comparison of these methods, we summarize their complexity and characteristics in Table A4.

### The Impacts of the Constraint of Linear Transformations on the Performance of a Flow-based Model

In this section, we examine the impact of the constraints of linear transformations on the performance of a flow-based model. A key distinction between constrained and unconstrained linear layers liesin how they model the correlation between each element in a data vector. Constrained linear transformations, such as those used in the previous works [8; 9; 10; 11; 29; 12], impose predetermined correlations that are not learnable during the optimization process. For instance, masked linear layers [8; 9; 10] are constructed by masking either the upper or lower triangular weight matrix in a linear layer. In contrast, unconstrained linear layers have weight matrices that are fully learnable, making them more flexible than their constrained counterparts.

To demonstrate the influences of the constraint on the expressiveness of a model, we provide a performance comparison between flow-based models constructed using different types of linear layers. Specifically, we compare the performance of the models constructed using linear layers with full matrices, lower triangular matrices, upper triangular matrices, and matrices that are the multiplication of both lower and upper triangular matrices. These four types of linear layers are hereafter denoted as F, L, U, and LU, respectively, and the differences between them are depicted in Fig. A2. Furthermore, to highlight the performance discrepancy between these models, we construct the target distribution \(p_{\mathbf{x}}\) based on an autoregressive relationship of data vector \(\mathbf{x}\). Let \(\mathbf{x}_{[i]}\) denote the \(i\)-th element of \(\mathbf{x}\), and \(p_{\mathbf{x}_{[i]}}\) represent its associated pdf. \(\mathbf{x}_{[i]}\) is constructed based on the following equation:

\[\mathbf{x}_{[i]}=\begin{cases}\mathbf{u}_{[0]}&\text{if }i=1,\\ \tanh(\mathbf{u}_{[i]}\times s)\times(\mathbf{x}_{[i-1]}+d\times 2^{i}),& \text{if }i\in\{2,\ldots,D\},\end{cases}\] (A7)

where \(\mathbf{u}\) is sampled from an isotropic Gaussian, and \(s\) and \(d\) are coefficients controlling the shape and distance between each mode, respectively. In Eq. (A7), the function \(\tanh(\cdot)\) can be intuitively viewed as a smoothed variant of the function \(2H(\cdot)-1\), where \(H(\cdot)\) represents the Heaviside step function. In this context, the values of \((\mathbf{x}_{[i-1]}+d\times 2^{i})\) are multiplied by a value close to either \(-1\) or \(1\), effectively transforming a positive number to a negative one. Fig. A3 depicts a number of examples of \(p_{\mathbf{x}_{[i]}}\) constructed using this method. By employing this approach to design \(p_{\mathbf{x}}\), where capturing \(p_{\mathbf{x}_{[i]}}\) is presumed to be more challenging than modeling \(p_{\mathbf{x}_{[j]}}\) for any \(j<i\), we can inspect how the applied constraints impact performance. Inappropriately masking the linear layers, like the U-type layer, is anticipated to result in degraded performance, similar to the _anti-casual_ effect explained in [51].

In this experiment, we constructed flow-based models using the smoothed leakyReLU activation and different types of linear layers (i.e., F, L, U, and LU) with a dimensionality of \(D=10\). The models are optimized according to Eq. (2). The performance of these models is evaluated in terms of NLL, and its trends are depicted in Fig. A4. It is observed that the flow-based model built with the F-type layers achieved the lowest NLL, indicating the advantage of using unconstrained weight matrices in linear layers. In addition, there is a noticeable performance discrepancy between models with the L-type and U-type layers, indicating that imposing inappropriate constraints on linear layers may negatively affect the modeling abilities of flow-based models. Furthermore, even when both L-type and U-type layers were adopted, as shown in the red curve in Fig. A4, the performance remains inferior to those using the F-type layers. This experimental evidence suggests that linear layers constructed based on matrix decomposition (e.g., [4; 9]) may not possess the same expressiveness as unconstrained linear layers.

### Limitations and Discussions

We noticed that score-matching methods sometimes exhibit difficulty in differentiating the weights between individual modes within a multi-modal distribution. This deficiency is illustrated in Fig. A5 (a), where EBFlow fails to accurately capture the density of the Checkerboard dataset. This phenomenon bears resemblance to the _blindness_ problem discussed in [52]. While the solution proposed in [52] has the potential to address this issue, their approach is not directly applicable to the flow-based architectures employed in this paper.

In addition, we observed that the sampling quality of EBFlow occasionally experiences a significant reduction during the training iterations. This phenomenon is illustrated in Fig. A5 (b) and (c), where the Glow model trained using our approach demonstrates a decline in performance with extended training periods. The underlying cause of this phenomenon remains unclear, and we consider it a potential avenue for future investigation.