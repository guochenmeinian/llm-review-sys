# Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials

 Shengchao Liu1,2, Weitao Du3, Yanjing Li4, Zhuoxinran Li5, Zhiling Zheng6, Chernu Duan7,

Zhiming Ma3, Omar Yaghi6, Anima Anandkumar8, Christian Borgs6,

Jennifer Chayes6, Hongyu Guo9, Jian Tang1,10,11

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

6California Institute of Technology

6California Institute of Technology 9National Research Council Canada

10HEC Montreal 11CIFAR AI Chair

1Mila - Quebec Artificial Intelligence Institute 2Universite de Montreal

3University of Chinese Academy of Sciences 4Carnegie Mellon University

5University of Toronto 6University of California, Berkeley 7Massachusetts Institute of Technology

[MISSING_PAGE_POST]

Footnoterepresentation, they facilitate a more robust representation of small molecules, proteins, and crystalline materials. Nevertheless, pursuing geometric learning research is still challenging due to its evolving nature and the knowledge gap between science (_e.g._, physics) and machine learning communities. These factors contribute to a substantial barrier for machine learning researchers to investigate scientific problems and hinder efforts to reproduce results consistently. To overcome this, we introduce Geom3D, a benchmarking of the geometric representation with four advantages, as follows. 1

Footnote 1: In what follows, we may use “molecule” to refer to “small molecule” for brevity.

**(1) A unified and novel aspect in understanding symmetry-informed geometric models.** The molecule geometry needs to satisfy certain physical constraints regarding the 3D Euclidean space. For instance, the molecules' force needs to be equivariant to translation and rotation (see SE(3)-equivariance in Fig. 1). In this work, we classify the geometric methods into three categories: _invariant_ model, SE(3)-equivariant model with _spherical frame basis_ and _vector frame basis_. The invariant models only consider features that are constant w.r.t. the SE(3) group, while the two families of equivariant models can be further unified using the _frame basis_ to capture equivariant symmetry. An illustration of three categories is in Fig. 2. Building equivariant models on the _frame basis_ provides a novel and unified view of understanding geometric models and paves the way for intriguing more ML researchers to explore scientific problems.

**(2) A unified platform for various scientific domains.** There exist multiple platforms and tools for molecule discovery, but they are (1) mainly focusing on molecule's 2D graph representation [77, 102, 145]; (2) using 3D geometry with customized data structures or APIs [3, 105]; or (3) covering only a few geometric models [76]. Thus, it is necessary to have a platform benchmarking the geometric models, especially for researchers interested in solving scientific problems. In this work, we propose Geom3D, a geometric modeling framework based on PyTorch Geometric (PyG) [31], one of the most widely-used platforms for graph representation learning. Geom3D benchmarks 16 geometric models on solving 52 scientific tasks, and these tasks include the three most fundamental molecule types: small molecules, proteins, and crystalline materials. Each of them requires distinct domain-specific preprocessing steps, _e.g._, crystalline materials molecules possess periodic structures and thus need a particular periodic data augmentation. By leveraging such a unified framework, Geom3D serves as a comprehensive benchmarking tool, facilitating effective and consistent analysis components to interpret the existing geometric representation functions in a fair and convenient comparison setting.

**(3) A framework for a wider range of ML tasks.** The geometric models in Geom3D can serve as a building block for exploring extensive ML tasks, including but not limited to studying the molecule dynamic simulation and scrutinizing the transfer learning effect on molecule geometry. For example, pre

Figure 1: Pipeline for Geom3D, including dataset preprocessing, feature extraction, geometric pretraining and representation, and target tasks. We additionally demonstrate the SE(3)-equivariant force prediction task.

Figure 2: Three categories of geometric modules. (a) Invariant models only consider type-0 features. Equivariant models use either (b) spherical harmonics frames or (c) vector frames by projecting the coordinate vectors.

training is an important strategy to quickly transfer knowledge to target tasks, and recent works explore geometric pretraining on 3D conformations (including supervised and self-supervised) [59; 80; 136] and multi-modality pretraining on 2D topology and 3D geometry [30; 79; 86]. Other transfer learning venues include multi-task learning [82; 84] and out-of-distribution or domain adaptation [133; 134; 58], yet no geometry information has been utilized. All of these directions are promising for future exploration, and Geom3D serves as an auxiliary tool to accomplish them. For example, as will be shown in Sec. 4, we leverage Geom3D to effectively evaluate 14 pretraining methods with benchmarks.

**(4) A framework for exploring data preprocessing and optimization tricks.** When comparing different symmetry-informed geometric models, we find that in addition to the model architecture, there are two important factors affecting the performance: the data preprocessing (_e.g._, energy and force rescaling and shift) and optimization methods (_e.g._, learning rate, learning rate schedule, number of epochs, random seeds). In this work, we explore the effect of four preprocessing tricks and around 2-10 optimization hyperparameters for each model and task. In general, we observe that each model may benefit differently in different tasks regarding the preprocessing and optimization tricks. However, data normalization is found to help improve performance hugely in most cases. We believe that Geom3D is an effective tool for exploring and understanding various engineering tricks.

## 2 Data Structures for Geometric Data

**Small molecule 3D conformation.** Molecules are sets of points in the 3D Euclidean space, and they move in a dynamic motion, as known as the potential energy surface (PES). The region with the lowest energy corresponds to the most stable state for molecules, and molecules at these positions are called **conformations**, as illustrated in Fig. 3(b). For notation, we mark each 3D molecular graph as \(\bm{g}=(\bm{X},\bm{R})\), where \(\bm{X}\) and \(\bm{R}\) are for the atom types and positions, respectively.

**Crystalline material with periodic structure.** The crystalline materials or extended chemical structures possess a characteristic known as periodicity: their atomic or molecular arrangement repeats in a predictable and consistent pattern across all three spatial dimensions. This is the key aspect that differentiates them from small molecules. In Fig. 3(d), we show an original unit cell (marked in green) that can repeatedly compose the crystal structure along the lattice. To model such a periodic structure, we adopt the data augmentation from CGCNN [129]: for each original unit cell, we shift it along the lattice in three dimensions and connect edges within a cutoff value (hyperparameter). For more details on the two augmentation variants, please check Appendix A.

**Protein with backbone structure.** Protein structures can be classified into four primary levels, and the primary structure represents the linear arrangement of _amino acids_, and each amino acid is a molecule consisting of atoms. Geometric methods mainly focus on the tertiary structure, _i.e._, the 3D geometry of each atom, encompassing the complete organization of a single protein. However,

Figure 3: Fig. 3(a) illustrates 2D topology and 3D conformation for molecule Glycine. Fig. 3(c) displays the 3D structure of protein. Fig. 3(d) shows a simple cubic crystal of the element Po. Fig. 3(b) is a demo of PES.

atom-level modeling for proteins is consuming due to the large volume of atoms and the GPU memory limit. One solution is modeling each amino acid's _backbone structure_. The backbone structure of each amino acid is \(N-C_{\alpha}-C\), and the \(C_{\alpha}\) is bonded to the side chain. 20 common types of side chains corresponding to 20 amino acids, as illustrated in Fig. 3(c). Thus, modeling the backbone structure can balance the computational efficiency and the key geometric information.

## 3 Symmetry-Informed Geometric Representation

### Group Symmetry and Equivariance

Symmetry means the object remains invariant after certain transformations [127], and it is everywhere on Earth, such as in animals, plants, and molecules. Formally, the set of all symmetric transformations satisfies the axioms of a group. Therefore, the group theory and its representation theory are common tools to depict such physical symmetry. **Group** is a set \(G\) equipped with a group product \(\times\) satisfying:

\[(1)\ \exists\bm{e}\in G,\ \bm{a}\times\bm{e}=\bm{e}\times\bm{a},\forall\bm{a} \in G;\quad(2)\ \bm{a}\times\bm{a}^{-1}=\bm{a}^{-1}\times\bm{a}=\bm{e};\quad(3)\ \bm{a}\times(\bm{b}\times\bm{c})=\bm{a}\times\bm{b}\times\bm{c}.\] (1)

**Group representation** is a mapping from the group \(G\) to the group of linear transformations of a vector space \(X\) with dimension \(d\) (see [138] for more rigorous definition):

\[\rho_{X}(\cdot):G\rightarrow\mathbb{R}^{d\times d}\qquad\text{s.t.}\quad\rho( \bm{e})=1\ \land\ \rho_{X}(\bm{a})\rho_{X}(\bm{b})=\rho_{X}(\bm{a}\times\bm{b}),\ \forall\bm{a},\bm{b}\in G.\] (2)

During modeling, the \(X\) space can be the input 3D Euclidean space, the equivariant vector space in the intermediate layers, or the output force space. This enables the definition of equivariance as below.

**Equivariance** is the property for the geometric modeling function \(f:X\to Y\) as:

\[f(\rho_{X}(\bm{a})\bm{x})=\rho_{Y}(\bm{a})f(\bm{x}),\ \forall\bm{a}\in G,\bm{x} \in X.\] (3)

As displayed in Fig. 1, for molecule geometric modeling, the property should be rotation-equivariant and translation-equivariant (_i.e._, SE(3)-equivariant). More concretely, \(\rho_{X}(\bm{a})\) and \(\rho_{Y}(\bm{a})\) are the SE(3) group representations on the input (_e.g._, atom coordinates) and output space (_e.g._, force space), respectively. SE(3)-equivariant modeling in Eq. (3) is essentially saying that the designed deep learning model \(f\) is modeling the whole transformation trajectory on the molecule conformations, and the output is the transformed \(\hat{y}\) accordingly. Further, we want to highlight that, in addition to the network architecture or representation function, the input features can also be represented as an equivariant feature mapping from the 3D mesh to \(\mathbb{R}^{\tilde{d}}\)[11], where \(\tilde{d}\) depends on input data, _e.g._, \(\tilde{d}=1\) (for atom type dimension) + 3 (for atom coordinate dimension) on small molecules. Such features are called steerable features in [5; 11] when only considering the subgroup SO(3)-equivariance.

**Invariance** is a special type of equivariance, defined as:

\[f(\rho_{X}(\bm{a})\bm{x})=f(\bm{x}),\ \ \forall\bm{a}\in G,\bm{x}\in X,\] (4)

with \(\rho_{Y}(\bm{a})\) as the identity \(\forall\bm{a}\in G\). The group representation helps define the equivariance condition for \(f\) to follow. Then, the question boils down to how to design such an equivariant \(f\). In the following, we will discuss geometric modelings from a novel and unified perspective using the frame. In the next sections, we will provide a novel and unified aspect of understanding the advanced geometric representation and pretraining methods using the frame basis (details in Appendix H).

Figure 4: Pipelines for seven single-modal geometric pretraining methods. (a-c) conduct self-prediction. (d) maximizes the MI between nodes and graphs. (e-g) are GeoSSL, maximizing the MI between views \(\bm{g}_{1}\) and \(\bm{g}_{2}\).

### Invariant Geometric Representation Learning

One simple way of achieving SE(3) group symmetry is invariant modeling. It means the geometric model only considers the type-0 features [112], _i.e._, features that are invariant with respect to rotation and translation. Existing works have been adopting the invariant features for modeling, including pairwise distance (SchNet [109]), bond angles (DimeNet [68]), and torsion angles (SphereNet [89] and GemNet [67]). Note that the torsion angles are angles between two planes defined by pairwise bonds.

### Equivariant Geometric Representation Learning

Invariant modeling only captures the type-0 features. However, equivariant modeling of higher-order particles may bring in extra expressiveness. For example, the elementary particles in high energy physics [98] inherit higher order symmetries in the sense of SO(3) representation theory, which makes the equivariant modeling necessary. Such higher-order particles include type-1 features like coordinates and forces in molecular conformation. There are many approaches to design such SE(3)-equivariant model satisfying Eq. (3). There are two main venues, as will be discussed below.

**Spherical Frame Basis.** This research line utilizes the irreducible representations [37] for building SO(3)-equivariant representations, and the first work is IFN [112]. Its main idea is to project the 3D Euclidean coordinates into the spherical harmonics space, which transforms equivariantly according to the irreducible representations of SO(3), and the translation-equivariant can be trivially guaranteed using the relative coordinates. Following this, there have been variants combining it with the attention module (Equiformer [73]) or with more expressive network architectures (SEGNN [4], Allegro [95]).

**Vector Frame Basis.** An alternative philosophy of equivariant modeling utilizes the vector (in physics) frame basis. It constructs three vectors bases, serving as a reference frame to help locate the vectors in each corresponding local environment. Works along this line for molecule discovery include DeePMD [140] for dynamics simulation, 3D-EMGP [59] and MoleculeSDE [79] for geometric pretraining, and ClofNet [20] for conformation generation. For macromolecules like protein, the equivariant vector frame has been used for protein design (StructTrans [53]) and protein folding (AlphaFold2 [64]). We also want to highlight that, from a mathematical perspective, equivariance and invariance can be transformed to each other by the scalarization technique. Please check [49] for details.

The spherical frame basis can be easily extended to higher-order particles, yet it may suffer from the high computational cost. On the other hand, the vector frame basis is specifically designed for the 3D point clouds; thus, it is more efficient but cannot generalize to higher-order particles. Meanwhile, we would like to acknowledge other equivariant modeling paradigms, including using orbital features [99] and elevating 3D Euclidean space to SE(3) group [32; 52]. Please check Appendix F for details.

### Geometric Pretraining

Recent studies have started to explore **single-modal of geometric pretraining** on molecules. The GeoSSL paper [80] covers a wide range of geometric pretraining algorithms. The type prediction, distance prediction, and angle prediction predict the masked atom type, pairwise distance, and bond angle, respectively. The 3D InfoGraph predicts whether the node- and graph-level 3D representation are for the same molecule. GeoSSL is a novel geometric pretraining paradigm that maximizes the mutual information (MI) between the original conformation \(\bm{g}_{1}\) and augmented conformation \(\bm{g}_{2}\), where \(\bm{g}_{2}\) is obtained by adding small perturbations to \(\bm{g}_{1}\). RR, InfoNCE, and EBM-NCE optimize the objective in the latent representation space, either generative or contrastive. GeoSSL-DDM [80; 136] optimizes the same objective function using denoising score matching. 3D-EMGP [60] has the same strategy and utilizes an equivariant module to denoise the 3D noise directly. Another research line is the **multi-modal of topological and geometric pretraining**. GraphMVP [86] first proposes one contrastive objective (EBM-NCE) and one generative objective (VRR) to optimize the MI between the 2D topologies and 3D geometries in the representation space. 3D InfoMax [114] is a special case of GraphMVP, with the contrastive part only. MoleculeSDE [79] extends GraphMVP by introducing two SDE models for solving the 2D and 3D reconstruction. We illustrate these algorithms in Figs. 4 and 8.

### Discussion: Reflection-antisymmetric in Geometric Learning

Till now, we have discussed the SE(3)-equivariance, _i.e._, the translation and rotation equivariance. As highlighted in the recent work [61; 79], the molecules needlessly satisfy the reflection-equivariant,but instead, they should be reflection-antisymmetric [79]. One classic example is that the energy of small molecules is reflection-antisymmetric in a binding system. Each of the two equivariant tackges discussed in Sec. 3.3 can solve this problem easily. The spherical frame basis can achieve this by adding the reflection into the Wigner-D matrix [4], and the vector frame basis can accomplish this using the cross-product during frame construction [79].

## 4 Geometric Datasets and Benchmarks

In Sec. 3, we introduce a novel aspect for understanding symmetry-informed geometric models. In this section, we discuss utilizing Geom3D framework for benchmarking 16 geometric models over 52 tasks. For the detailed dataset acquisitions and task specifications (_e.g._, _dataset size_, _splitting_, and _task unit_), please check Appendix B. Geom3D also covers 7 1D models and 10 2D graph neural networks (GNNs) and benchmarks the 14 pretraining algorithms to learn a robust geometric representation. Additionally, we want to highlight Geom3D enables exploration of important data preprocessing and optimization tricks for performance improvement, as will be introduced next.

### Small Molecules: QM9

QM9 [100] is a dataset consisting of 134K molecules, each with up to 9 heavy atoms. It includes 12 tasks that are related to the quantum properties. For example, U0 and U298 are the internal energies at temperatures of 0K and 298.15K, respectively. On the QM9 dataset, we can easily get the 1D descriptors (Fingerprints/FPs [106], SMILES [126], SELFIES [70]), 2D topology, and 3D conformation. This enables us to build models on each of them respectively: (1) We benchmark 7 models on 1D descriptors, including multi-layer perception (MLP), random forest (RF), XGBoost (SGB), convolution neural networks (CNN), and BERT [18]. (2) We benchmark 10 2D GNN models on the molecular topology, including GCN [23; 66], ENN-S2S [38], GraphSAGE [43], GAT [119], GIN [130], D-MPNN [132], PNA [13], Graphormer [135], AWARE [17], GraphGPS [101]. (3) We benchmark 11 3D geometric models on the molecular conformation, including SchNet [109], DimeNet++ [68], SE(3)-Trans [35], EGNN [108], PaiNN [110], GemNet-T [67], SphereNet [89], SEGNN [4], Allegro [95], NequIP [3], Equformer [73]. The evaluation metric is the mean absolute error.

The results of these 28 models are in Table 1, and two important insights are observed: (1) There is no one universally best geometric model, yet DimeNet++, PaiNN, GemNet, and Equiformer perform well in most tasks. However, PaiNN takes less than 20 GPU hours, and the other three models take up to 5 GPU days per task. (2) The geometric conformation is important for quantum property prediction. The performance of 3D models is better than all the 1D and 2D models _by orders of magnitudes_.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \multirow{2}{*}{Fertarization} & \multirow{2}{*}{Model} & \multirow{2}{*}{\(\alpha\downarrow\)} & \(\nabla E\downarrow\) & _Sigmoid_ & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) & \(\hat{\varepsilon}_{\text{M}}\downarrow\) \\  & & \(\alpha_{0}^{\text{i}}\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) & \(meV\) \\ \hline \multirow{2}{*}{1D PPs} & MLP & 2.231 & 196.72 & 131.27 & 164.94 & 0.526 & 0.919 & 2158.64 & 2358.23 & 68.621 & 2340.61 & 2314.77 & 1559.21 \\  & RF & 3.801 & 207.02 & 165.72 & 183.04 & 0.534 & 1.485 & 2391.79 & 279.294 & 944.512 & 2305.76 & 3678.25 & 2531.32 \\  & XGB & 2.748 & 1971.91 & 138.98 & 165.43 & 0.516 & 1.062 & 2653.93 & 2042.87 & 28.599 & 2786.28 & 2769.29 & 1809.89 \\ \cline{2-11}  & CNN & 0.384 & 1652.52 & 124.65 & 114.81 & 0.566 & 0.173 & 1566.76 & 170.59 & 20.403 & 166.18 & 168.69 & 160.070 \\ \cline{2-11}  & BERT & 0.313 & 117.50 & 84.93 & 98.88 & 0.446 & 0.176 & 170.01 & 853.43 & 180.02 & 183.84 & 188.60 & 13.410 \\ \hline \multirow{2}{*}{1D SELFIES} & C & 0.345 & 157.04 & 115.55 & 113.00 & 0.490 & 0.168 & 136.32 & 1465.56 & 20.080 & 143.00 & 140.40 & 101.049 \\  & BERT & 0.348 & 123.11 & 91.15 & 90.80 & 0.461 & 0.203 & 168.20 & 187.50 & 19.125 & 204.93 & 195.98 & 17.328 \\ \hline \multirow{2}{*}{2D Graph} & GCN & 1.338 & 145.82 & 96.21 & 106.66 & 0.434 & 0.526 & 1198.12 & 1291.57 & 37.585 & 1281.03 & 1303.38 & 158.03 \\  & ENN-S2S & 1.401 & 205.99 & 128.13 & 324.87 & 0.577 & 1060.47 & 4372.11 & 595.524 & 34.609 & 1800.79 & 1521.32 & 51.266 \\  & GraphSAGE & 1.601 & 131.45 & 88.78 & 93.21 & 0.402 & 0.544 & 1473.42 & 1617.73 & 38.112 & 1553.01 & 1566.55 & 95.344 \\  & GAT & 1.1325 & 139.50 & 94.70 & 98.52 & 0.406 & 0.291 & 911.82 & 993.11 & 165.03 & 1129.52 & 952.67 & 55.061 \\  & GIN & 1.165 & 175.82 & 90.66 & 110.74 & 0.539 & 0.691 & 848.20 & 1093.56 & 35.110 & 1498.32 & 3614.18 & 108.331 \\  & D-MPNN & 0.568 & 118.42 & 85.01 & 86.20 & 0.441 & 0.423 & 243.43 & 458.49 & 2416.70 & 401.459 & 28.291 \\  & PNA & 0.681 & 148.48 & 88.72 & 97.31 & 0.361 & 0.409 & 646.49 & 679.24 & 23.855 & 616.04 & 694.92 & 57.217 \\  & Graphmer & 2.836 & 70.52 & 54.24 & 52.42 & 0.343 & 0.308 & 0.206 & 286.26 & 3540.16 & 115.285 & 228.52 & 144.545 \\  & AWARE & 0.297 & 144.19 & 133.89 & 98.86 & 0.602 & 0.129 & 86.62 & 94.47 & 22.180 & 93.35 & 95.73 & 5.275 \\  & GraphGPS & 0.209 & 75.98 & 54.75 & 54.53 & 0.288 & 0.089 & 528.50 & 693.19 & 12.488 & 296.60 & 411.16 & 49.888 \\ \hline \multirow{2}{*}{3D Graph} & SchNet & 0.060 & 44.13 & 27.64 & 22.55 & 0.028 & 0.031 & 14.19 & 14.05 & 0.133 & 13.93 & 13.27 & 1.749 \\  & DimeNet & 0.044 & 362.22 & 20.01 & 16.66 & 0.028 & **0.022** & **7.45** & **6.14** & 0.323 & **6.33** & 7.18 & **1.118** \\  & SE(3)-Trans & 0.137 & 56.52 & 34.36 & 34.41 & 0.500 & 0.63 & 65.28 & 70.00 & 1.

[MISSING_PAGE_FAIL:7]

### Small Molecules & Proteins Binding: LBA & LEP

The binding affinity measures the strength of the binding interaction between a small molecule (ligand) to the target protein. In Geom3D, we consider modeling both the ligands and proteins with their 3D structures. During binding, a cavity in a protein can potentially possess suitable properties for binding a small molecule, and it is called a pocket [113]. Due to the large volume of protein, Geom3D follows existing works [118] by only taking the binding pocket instead of the whole protein structure. Specifically, Geom3D models up to 600 atoms for each ligand and protein pair. For the benchmarking, we consider two binding affinity tasks. (1) The first task is ligand binding affinity (LBA) [123]. It is gathered from [124], and the task is to predict the binding affinity strength between a ligand and a protein pocket. (2) The second task is ligand efficacy prediction (LEP) [34]. The input is a ligand and both the active and inactive conformers of a protein, and the goal is to classify whether or not the ligand can activate the protein's function. The results on two binding tasks are in Table 4, and we can observe that PaiNN, SEGNN, and Equiformer are generally outstanding on the two tasks.

### Proteins: ECSingle, ECMultiple, Fold, GO, MSP, and PSR

**ECSingle** is a classification task [45] that classifies 37K proteins into 384 four-level Enzyme Commission (EC) types. This task aims to recognize the fundamental role of proteins as bio-catalysts or enzymes, which are essential in facilitating biological reactions. The EC numbering system [63] serves as a comprehensive numerical classification scheme, systematically organizing the varied functionalities of enzymes and providing a structured approach to understanding their biological roles.

**ECMultiple** is a multi-label classification task proposed by Gligorijevic et al. [39], where 19K proteins are associated with 538 distinct EC categories, including both three-level and four-level types and a single protein can be concurrently labeled with several three-level or four-level EC numbers.

**Fold** is a task classifying 16K proteins into 1,195 fold patterns [47; 74]. It is an important biological task in predicting the 3D structures from 1D amino acid sequences. We further consider three testsets (Fold, Superfamily, and Family) based on the sequence and structure similarity [94].

**GO** (Gene Ontology) is a dataset [39] with 36K proteins for GO term classification, where the GO term provides a consistent description of gene product attributes across species and databases [12]. Concretely, each protein contains up to three types of GO terms, corresponding to three types of classification tasks: (1) Molecular Function (MF) has 489 classes; (2) Biological Process (BP) has 1,943

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{3}{c}{LBA} & \multicolumn{3}{c}{LEP} \\ \cline{2-9}  & RMSD \(\downarrow\) & \(R_{P}\uparrow\) & \(R_{C}\uparrow\) & ROC \(\uparrow\) & PR \(\uparrow\) \\ \hline SchNet & 1.521 \(\pm\) 0.02 & 0.474 \(\pm\) 0.01 & 0.452 \(\pm\) 0.01 & 0.450 \(\pm\) 0.03 & 0.379 \(\pm\) 0.03 \\ DimeNet++ & 1.672 \(\pm\) 0.09 & 0.550 \(\pm\) 0.01 & 0.556 \(\pm\) 0.01 & 0.509 \(\pm\) 0.06 & 0.496 \(\pm\) 0.05 \\ EGNN & 1.494 \(\pm\) 0.04 & 0.503 \(\pm\) 0.04 & 0.483 \(\pm\) 0.05 & **0.657 \(\pm\) 0.05** & **0.559 \(\pm\) 0.05** \\ PaiNN & **1.434 \(\pm\) 0.02** & 0.583 \(\pm\) 0.02 & **0.580 \(\pm\) 0.02** & 0.585 \(\pm\) 0.02 & 0.432 \(\pm\) 0.03 \\ GemNet-T & – & – & – & – & 0.659 \(\pm\) 0.05 & 0.506 \(\pm\) 0.05 \\ SphreNet & 1.581 \(\pm\) 0.02 & 0.538 \(\pm\) 0.01 & 0.529 \(\pm\) 0.01 & 0.523 \(\pm\) 0.04 & 0.432 \(\pm\) 0.05 \\ SEGNN & 1.416 \(\pm\) 0.03 & 0.566 \(\pm\) 0.02 & 0.550 \(\pm\) 0.02 & 0.574 \(\pm\) 0.03 & 0.485 \(\pm\) 0.03 \\ Nequip & 1.606 \(\pm\) 0.02 & 0.537 \(\pm\) 0.01 & 0.520 \(\pm\) 0.01 & 0.538 \(\pm\) 0.12 & 0.481 \(\pm\) 0.07 \\ Allegro & 1.567 \(\pm\) 0.02 & 0.547 \(\pm\) 0.00 & 0.534 \(\pm\) 0.00 & 0.627 \(\pm\) 0.04 & 0.525 \(\pm\) 0.03 \\ Equiformer & 1.392 \(\pm\) 0.03 & **0.598 \(\pm\) 0.02** & 0.578 \(\pm\) 0.02 & 0.618 \(\pm\) 0.06 & 0.510 \(\pm\) 0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on 2 binding affinity prediction tasks. We select three evaluation metrics for LBA: the root mean squared error (RMSD), the Pearson correlation (\(R_{p}\)) and the Spearman correlation (\(R_{S}\)). LEP is a binary classification task, and we use the area under the curve for receiver operating characteristics (ROC) and precision-recall (PR) for evaluation. We run cross-validation with 5 seeds, and the mean and std are reported.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{ECSingle} & \multirow{2}{*}{ECMultiple} & \multicolumn{3}{c}{Fold} & \multicolumn{3}{c}{GO} & \multirow{2}{*}{MSP} & \multirow{2}{*}{PSR} \\ \cline{2-2} \cline{6-11}  & & Fold & Sup. & Fam. & MF & BP & CC & & \\ \cline{2-2} \cline{6-11}  & ACC \(\uparrow\) & \(F_{max}\uparrow\) & ACC \(\uparrow\) & ACC \(\uparrow\) & \(F_{max}\uparrow\) & \(F_{max}\uparrow\) & \(F_{max}\uparrow\) & ROC \(\uparrow\) & Global \(\rho\uparrow\) & Mean \(\rho\uparrow\) \\ \hline IEConv & – & – & 45.0 & 69.7 & 98.9 & – & – & – & – & – & – \\ GVP-GNN & 65.5 & 0.712 & 34.8 & 52.7 & 95.0 & 0.476 & 0.312 & 0.389 & 0.574 & 0.744 & 0.302 \\ GearNet & 78.8 & 0.799 & 29.1 & 43.1 & 95.9 & 0.477 & 0.283 & 0.373 & – & – & – \\ ProNet & 86.4 & 0.823 & 52.7 & 70.3 & 99.3 & 0.559 & 0.367 & 0.414 & 0.634 & **0.818** & 0.462 \\ CDConv & **86.9** & **0.862** & **60.0** & **79.9** & **99.5** & **0.649** & **0.435** & **0.450** & **0.717** & 0.817 & **0.500** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on 10 protein tasks from six datasets: ECSingle, ECMultiple, Fold (Fold, Sup., Fam.), GO (MF, BP, CC), MSP, and PSR. The evaluation metrics are Accuracy (ACC, %), \(F_{max}\) (definition in Appendix B), ACC, \(F_{max}\), receiver operating characteristics (ROC), and Spearman’s \(\rho\), respectively.

classes; and (3) Cellular Component (CC) has 320 classes. Notice that each protein can be associated with multiple GO terms in each GO term type, thus all three tasks are multi-label classifications.

**MSP & PSR** are two protein tasks from a collection of benchmark datasets for machine learning in structural biology [118]. MSP (Mutation Stability Prediction) aims to predict whether the stability of a protein increases after mutation. The dataset is a mutation dataset containing 4K proteins. It is constructed by incorporating single-point mutations given in the SKEMPI database [56]. PSR (Protein Structure Ranking) is a regression task based on the Critical Assessment of Structure Prediction (CASP) [71]. In CASP, a protein structure is predicted and a quality score, the global distance test (GDT_TS), is calculated between the predicted structure and experimentally determined structure. This task aims to predict this score for 44K proteins.

The results of 5 models are in Table 5. CDConv [29] outperforms other models by a large margin on almost all 10 tasks, while ProNet [122] performs second well in general, and reaches the best result on the PSR task with global \(\rho\) metric. Notice that certain entries in the table are temporarily left blank due to memory constraints encountered. More detailed dataset specifications are in Appendix B.

### Crystalline Materials: MatBench and QMOF

**MatBench**[21] is explicitly created to evaluate the performance of machine learning models in predicting properties of inorganic bulk materials covering mechanical, electronic, and thermodynamic material properties [21]. Here we consider 8 regression tasks with crystal structures, including predicting the formation energy (Perovskites, \(E_{\text{form}}\)), exfoliation energies (\(E_{\text{exfo}}\)), band gap, shear and bulk modulus (\(log_{10}G\) and \(log_{10}K\)), etc. Please check Appendix B for more details.

**Quantum MOF (QMOF)**[107] is a dataset of over 20K metal-organic frameworks (MOFs) and coordination polymers derived from DFT. The task is to predict the band gap, the energy gap between the valence band and the conduction band. The results of 8 geometric models on 8 MatBench tasks and 1 QMOF task are in Table 6, and we can observe that the performance of all the models is very close, while DimeNet++, PaiNN, GemNet-T, and Equiformer are slightly better.

We also conduct **ablation study on periodic data augmentation** on crystal materials. We note that there are two data augmentation (DA) methods: gathered and expanded. Gathered DA means that we shift the original unit cell along three dimensions, and the translated unit cells will have the _same_ node indices as the original unit cell, _i.e._, a multi-edge graph. However, expanded DA will assume the translated unit cells have different node indices from the original unit cell. (A visual demonstration is in Appendix A). We conduct an ablation study on the effect of these two DAs, and we plot MAE(expanded DA) - MAE(gathered DA) on six tasks in Fig. 6. It reveals that for most of the models (except EGNN), using gathered DA can lead to consistently better performance, and thus it is preferred. For more qualitative analysis, please check Appendix J.

\begin{table}
\begin{tabular}{l r r r r r r r r r} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{MatBench} & \multicolumn{6}{c}{QMOF} \\ \cline{2-10}  & Per. \(E_{\text{com}}\downarrow\) & Dielectric \(\downarrow\) & \(log_{10}G\downarrow\) & \(log_{10}K\downarrow\) & \(E_{\text{exfo}}\downarrow\) & Phonoms \(\downarrow\) & Band Gap \(\downarrow\) & \(E_{\text{com}}\downarrow\) & Band Gap \(\downarrow\) \\  & 18.928 & 4,764 & 10,987 & 10,987 & 636 & 1,265 & 106,113 & 132,752 & 20.425 \\ \hline SchNet & 0.040 & 0.334 & 0.081 & 0.060 & 65.201 & 42.586 & 0.327 & 0.026 & 0.236 \\ DimeNet++ & **0.037** & 0.357 & 0.081 & 0.058 & 68.685 & 38.339 & **0.208** & 0.025 & 0.234 \\ EGNN & 0.038 & 0.331 & 0.087 & 0.064 & 78.015 & 74.846 & 0.211 & 0.026 & 0.256 \\ PaiNN & 0.038 & 0.317 & **0.080** & **0.053** & 67.752 & 44.602 & **0.022** & 0.190 & 0.207 \\ GenNet-T & 0.042 & 0.325 & 0.088 & 0.061 & 68.425 & 48.986 & 0.186 & 0.026 & **0.207** \\ SphereNet & 0.043 & 0.388 & 0.087 & 0.061 & 72.987 & 36.300 & 0.217 & 0.029 & 0.251 \\ SEGNN & 0.046 & 0.360 & 0.087 & 0.059 & 65.052 & 43.638 & 0.330 & 0.047 & 0.330 \\ Equiformer & 0.046 & **0.280** & 0.087 & 0.057 & **62.977** & **37.381** & 0.202 & 0.027 & 0.234 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results on the 8 tasks from MatBench and 1 task from QMOF (with optimal DA). The data split and task unit are in Appendix B, and the metric is the mean absolute error (MAE).

Figure 6: Ablation study on the performance gap with data augmentation (DA): MAE(expanded DA) - MAE(gathered DA).

[MISSING_PAGE_FAIL:10]

## Acknowledgement

The authors would like to thank Zichao Rong, Chengpeng Wang, Jiarui Lu, Farzaneh Heidari, Zuobai Zhang, Limei Wang, and Hanchen Wang for their helpful discussions. This project is supported by the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund, and a National Research Council of Canada (NRC) Collaborative R&D Project. This project was also partially funded by IVADO Fundamental Research Project grant PRF-2019-3583139727.

## References

* [1] Kenneth Atz, Francesca Grisoni, and Gisbert Schneider. Geometric deep learning on molecular representations. _Nature Machine Intelligence_, 3(12):1023-1032, 2021.
* [2] Ilyes Batatia, Simon Batzner, David Peter Kovacs, Albert Musaelian, Gregor N. C. Simm, Ralf Drautz, Christoph Ortner, Boris Kozinsky, and Gabor Csanyi. The design space of e(3)-equivariant atom-centered interatomic potentials, 2022.
* [3] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. _Nature communications_, 13(1):1-11, 2022.
* [4] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik Bekkers, and Max Welling. Geometric and physical quantities improve e(3) equivariant message passing. _arXiv preprint arXiv:2110.02905_, 2021.
* [5] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* [6] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. _Journal of chemical information and modeling_, 59(3):1096-1108, 2019.
* [7] Lowik Chanussot*, Abhishek Das*, Siddharth Goyal*, Thibaut Lavril*, Muhammed Shuaibi*, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C Lawrence Zitnick, and Zachary Uissi. Open catalyst 2020 (oc20) dataset and community challenges. _ACS Catalysis_, 2021.
* [8] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schutt, and Klaus-Robert Muller. Machine learning of accurate energy-conserving molecular force fields. _Science advances_, 3(5):e1603015, 2017.
* [9] Anders Christensen and O. Anatole von Lilienfeld. Revised md17 dataset. _Materials Cloud Archive_, 2020.
* [10] Anders S Christensen and O Anatole von Lilienfeld. On the role of gradients for machine learning of molecular energies and forces. _arXiv.org_, 2020.
* [11] Taco S Cohen and Max Welling. Steerable cnns. _arXiv preprint arXiv:1612.08498_, 2016.
* [12] Gene Ontology Consortium. The gene ontology (go) database and informatics resource. _Nucleic acids research_, 32(suppl_1):D258-D261, 2004.
* [13] Gabriele Corso, Luca Cavalleri, Dominique Beainti, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. _Advances in Neural Information Processing Systems_, 33:13260-13271, 2020.
* [14] Jose M Dana, Aleksandras Gutmanas, Nidhi Tyagi, Guoying Qi, Claire O'Donovan, Maria Martin, and Sameer Velankar. Sifts: updated structure integration with function, taxonomy and sequences resource allows 40-fold increase in coverage of structure-based annotations for proteins. _Nucleic acids research_, 47(D1):D482-D489, 2019.
* [15] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning-based protein sequence design using proteinmpnn. _Science_, 378(6615):49-56, 2022.
** [16] Pierre-Paul De Breuck, Matthew L Evans, and Gian-Marco Rignanese. Robust model benchmarking and bias-imbalance in data-driven materials science: a case study on modnet. _Journal of Physics: Condensed Matter_, 33(40):404002, 2021.
* [17] Mehmet F Demirel, Shengchao Liu, Siddhant Garg, Zhenmei Shi, and Yingyu Liang. Attentive walk-aggregating graph neural networks. _TMLR_, 2022.
* [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [19] Weitao Du, Yuanqi Du, Limei Wang, Diego Feng, Guifeng Wang, Shuiwang Ji, Carla Gomes, and Zhi-Ming Ma. A new perspective on building efficient and expressive 3d equivariant graph neural networks, 2023.
* [20] Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Se (3) equivariant graph neural networks with complete local frames. In _International Conference on Machine Learning_, pages 5583-5608. PMLR, 2022.
* [21] Alexander Dunn, Qi Wang, Alex Ganose, Daniel Dopp, and Anubhav Jain. Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. _arXiv.org_, 6, 2020.
* [22] Alexander Dunn, Qi Wang, Alex Ganose, Daniel Dopp, and Anubhav Jain. Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. _npj Computational Materials_, 6(1):138, 2020.
* [23] David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gomez-Bombarelli, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. _arXiv preprint arXiv:1509.09292_, 2015.
* [24] Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. _arXiv preprint arXiv:2010.02449_, 2020.
* [25] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji. Translation between molecules and natural language. _arXiv preprint arXiv:2204.11817_, 2022.
* [26] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 595-607, 2021.
* [27] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrumas: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-1, 2021.
* [28] Thomas Engel and Johann Gasteiger. _Applied chemoinformatics: achievements and future opportunities_. John Wiley & Sons, 2018.
* [29] Hehe Fan, Zhangyang Wang, Yi Yang, and Mohan Kankanhalli. Continuous-discrete convolution for geometry-sequence modeling in proteins. In _The Eleventh International Conference on Learning Representations_, 2023.
* [30] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Chemrl-gem: Geometry enhanced molecular representation learning for property prediction. _arXiv preprint arXiv:2106.06130_, 2021.
* [31] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* [32] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In _International Conference on Machine Learning_, pages 3165-3176. PMLR, 2020.
* [33] Daniel Flam-Shepherd and Alan Aspuru-Guzik. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. _arXiv preprint arXiv:2305.05708_, 2023.

* [34] Richard A Friesner, Jay L Banks, Robert B Murphy, Thomas A Halgren, Jasna J Klicic, Daniel T Mainz, Matthew P Regnaky, Eric H Knoll, Mee Shelley, Jason K Perry, et al. Glide: a new approach for rapid, accurate docking and scoring. 1. method and assessment of docking accuracy. _Journal of medicinal chemistry_, 47(7):1739-1749, 2004.
* [35] Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translation equivariant attention networks. _arXiv preprint arXiv:2006.10503_, 2020.
* [36] Johannes Gasteiger, Shankari Giri, Johannes T Margraf, and Stephan Gunnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. 2020.
* [37] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. _arXiv preprint arXiv:2207.09453_, 2022.
* [38] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* [39] Vladimir Gligorijevic, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based protein function prediction using graph convolutional networks. _Nature communications_, 12(1):3168, 2021.
* [40] Sai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Yashaswi Pathak, Haoran Wei, Shengchao Liu, Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, et al. Learning to navigate the synthetically accessible chemical space using reinforcement learning. In _International Conference on Machine Learning_, pages 3668-3679. PMLR, 2020.
* [41] Nate Gruver, Samuel Stanton, Nathan C Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson. Protein design with guided discrete diffusion. _arXiv preprint arXiv:2305.20009_, 2023.
* [42] Thomas A Halgren. Merck molecular force field. i. basis, form, scope, parameterization, and performance of mmff94. _Journal of computational chemistry_, 17(5-6):490-519, 1996.
* [43] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [45] Pedro Hermosilla, Marco Schafer, Matej Lang, Gloria Fackelmann, Pere Pau Vazquez, Barbora Kozlikova, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. 2020.
* [46] Brian L Hie, Varun R Shanker, Duo Xu, Theodora UJ Bruun, Payton A Weidenbacher, Shaogeng Tang, Wesley Wu, John E Pak, and Peter S Kim. Efficient evolution of human antibodies from general protein language models. _Nature Biotechnology_, 2023.
* [47] Jie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for mapping protein sequences to folds. _Bioinformatics_, 34, 2018.
* [48] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. _bioRxiv_, 2022.
* [49] Elton P Hsu. _Stochastic analysis on manifolds_. Number 38. American Mathematical Soc., 2002.
* [50] Qian-Nan Hu, Hui Zhu, Xiaobing Li, Manman Zhang, Zhe Deng, Xiaoyan Yang, and Zixin Deng. Assignment of ec numbers to enzymatic reactions with reaction difference fingerprints. _PloS one_, 7(12):e52901-e52901, 2012.
* [51] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _arXiv preprint arXiv:2005.00687_, 2020.
* [52] Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. Lietransformer: Equivariant self-attention for lie groups. In _International Conference on Machine Learning_, pages 4533-4543. PMLR, 2021.

* [53] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-based protein design. _Advances in neural information processing systems_, 32, 2019.
* [54] Clemens Isert, Kenneth Atz, and Gisbert Schneider. Structure-based drug design with geometric deep learning. _Current Opinion in Structural Biology_, 79:102548, 2023.
* [55] Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The materials project: A materials genome approach to accelerating materials innovation. _APL materials_, 1(1):011002, 2013.
* [56] Justina Jankauskaite, Brian Jimenez-Garcia, Justas Dapkunas, Juan Fernandez-Recio, and Iain H Moal. Skempi 2.0: an updated benchmark of changes in protein-protein binding energy, kinetics and thermodynamics upon mutation. _Bioinformatics_, 35(3):462-469, 2019.
* [57] Jan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. _Chemical science_, 10(12):3567-3572, 2019.
* [58] Yuanfeng Ji, Lu Zhang, Jiaxiang Wu, Bingzhe Wu, Long-Kai Huang, Tingyang Xu, Yu Rong, Lanqing Li, Jie Ren, Ding Xue, et al. Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery-a focus on affinity prediction problems with noise annotations. _arXiv preprint arXiv:2201.09637_, 2022.
* [59] Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. 3d equivariant molecular graph pretraining. _arXiv preprint arXiv:2207.08824_, 2022.
* [60] Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pretraining for 3d molecular graphs. _arXiv preprint arXiv:2207.08824_, 2022.
* [61] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. _arXiv preprint arXiv:2206.01729_, 2022.
* [62] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael J. L Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. 2020.
* [63] J.M. Enzyme nomenclature: prepared by edwin c. webb, academic press, 1992. f34.00 (xiii + 862 pages) isbn 0 12 227165 3. _Trends in biochemical sciences (Amsterdam. Regular ed.)_, 18, 1993.
* [64] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [65] Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks, 2019.
* [66] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [67] Johannes Klicpera, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* [68] Johannes Klicpera, Shankari Giri, Johannes T Margraf, and Stephan Gunnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. _arXiv preprint arXiv:2011.14115_, 2020.
* [69] Johannes Klicpera, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. _arXiv preprint arXiv:2003.03123_, 2020.
* [70] Mario Krenn, Florian H"ase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. _Machine Learning: Science and Technology_, 1(4):045024, 2020.
* [71] Andriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, and John Moult. Critical assessment of methods of protein structure prediction (casp)--round xiii. _Proteins: Structure, Function, and Bioinformatics_, 87(12):1011-1020, 2019.
* [72] Greg Landrum et al. RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling, 2013.
** [73] Yi-Lun Liao and Tess Smidt. Equformer: Equivariant graph attention transformer for 3d atomistic graphs. _arXiv preprint arXiv:2206.11990_, 2022.
* [74] Chen Lin, Ying Zou, Ji Qin, Xiangrong Liu, Yi Jiang, Caihuan Ke, and Quan Zou. Hierarchical classification of protein folds using a novel ensemble classifier. _PloS one_, 8(2):e56499, 2013.
* [75] Chen Lin, Ying Zou, Ji Qin, Xiangrong Liu, Yi Jiang, Caihuan Ke, and Quan Zou. Hierarchical classification of protein folds using a novel ensemble classifier. _PloS one_, 8(2):e56499-e56499, 2013.
* [76] Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, et al. Dig: A turnkey library for diving into graph deep learning research. _The Journal of Machine Learning Research_, 22(1):10873-10881, 2021.
* [77] Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. _Journal of Machine Learning Research_, 22(240):1-9, 2021.
* [78] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. _NeurIPS_, 32, 2019.
* [79] Shengchao Liu, Weitao Du, Zhiming Ma, Hongyu Guo, and Jian Tang. A group symmetric stochastic differential equation model for molecule multi-modal pretraining. In _International Conference on Machine Learning_, 2023.
* [80] Shengchao Liu, Hongyu Guo, and Jian Tang. Molecular geometry pretraining with se (3)-invariant denoising distance matching. _arXiv preprint arXiv:2206.13602_, 2022.
* [81] Shengchao Liu, Hongyu Guo, and Jian Tang. Molecular geometry pretraining with SE(3)-invariant denoising distance matching. In _ICLR_, 2023.
* [82] Shengchao Liu, Yingyu Liang, and Anthony Gitter. Loss-balanced task weighting to reduce negative transfer in multi-task learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 9977-9978, 2019.
* [83] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. _arXiv preprint arXiv:2212.10789_, 2022.
* [84] Shengchao Liu, Meng Qu, Zuobai Zhang, Huiyu Cai, and Jian Tang. Structured multi-task learning for molecular property prediction. In _International Conference on Artificial Intelligence and Statistics_, pages 8906-8920. PMLR, 2022.
* [85] Shengchao Liu, Chengpeng Wang, Weili Nie, Hanchen Wang, Jiarui Lu, Bolei Zhou, and Jian Tang. Graphcg: Unsupervised discovery of steerable factors in graphs, 2023.
* [86] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training molecular graph representation with 3d geometry. _arXiv preprint arXiv:2110.07728_, 2021.
* [87] Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Chatgpt-powered conversational drug editing using retrieval and domain feedback. _arXiv preprint arXiv:2305.18090_, 2023.
* [88] Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar. A text-guided protein design framework. _arXiv preprint arXiv:2302.04611_, 2023.
* [89] Yi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d graph networks. _arXiv preprint arXiv:2102.05013_, 2021.
* [90] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* [91] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation. _arXiv preprint arXiv:2004.03497_, 2020.

* [92] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. _bioRxiv_, 2021.
* [93] Jan Mostowski and Joanna Pietraszewicz. Quantum versus classical angular momentum. 2019.
* [94] Alexey G. Murzin, Steven E. Brenner, Tim Hubbard, and Cyrus Chothia. Scop: A structural classification of proteins database for the investigation of sequences and structures. _Journal of molecular biology_, 247(4):536-540, 1995.
* [95] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. _arXiv preprint arXiv:2204.05249_, 2022.
* [96] Emmy Noether and M. A. Tavel. Invariant variation problems. 2005.
* [97] Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L Chevrier, Kristin A Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. _Computational Materials Science_, 68:314-319, 2013.
* [98] Donald H Perkins. _Introduction to high energy physics_. CAMBRIDGE university press, 2000.
* [99] Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R Manby, and Thomas F Miller. Orbnet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features. _The Journal of chemical physics_, 153(12), 2020.
* [100] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific data_, 1(1):1-7, 2014.
* [101] Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. _Advances in Neural Information Processing Systems_, 35, 2022.
* [102] Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin Wu. _Deep Learning for the Life Sciences_. O'Reilly Media, 2019. https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.
* [103] Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Masa transformer. _bioRxiv_, 2021.
* [104] Roshan M Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer protein language models are unsupervised structure learners. _bioRxiv_, 2020.
* [105] Patrick Reiser, Andre Eberhard, and Pascal Friederich. Graph neural networks in tensorflow-keras with raggedtensor representation (kgcnn). _Software Impacts_, page 100095, 2021.
* [106] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. _Journal of chemical information and modeling_, 50(5):742-754, 2010.
* [107] Andrew S Rosen, Shaelyn M Iyer, Debmalya Ray, Zhenpeng Yao, Alan Aspuru-Guzik, Laura Gagliardi, Justin M Notstein, and Randall Q Snurr. Machine learning the quantum-chemical properties of metal-organic frameworks for accelerated materials discovery. _Matter_, 4(5):1578-1597, 2021.
* [108] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. _arXiv preprint arXiv:2102.09844_, 2021.
* [109] Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* [110] Kristof T Schutt, Oliver T Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. _arXiv preprint arXiv:2102.03150_, 2021.
* [111] Chence Shi, Minkai Xu, Hongyu Guo, Ming Zhang, and Jian Tang. A graph to graphs framework for retrosynthesis prediction. In _International Conference on Machine Learning_, pages 8818-8827. PMLR, 2020.

* [112] Tess Smidt, Nathaniel Thomas, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. _arXiv preprint arXiv:1802.08219_, 2018.
* [113] Antonia Stank, Daria B Kokh, Jonathan C Fuller, and Rebecca C Wade. Protein binding pocket dynamics. _Accounts of chemical research_, 49(5):809-815, 2016.
* [114] Hannes Stark, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio. 3d infomax improves gnns for molecular property prediction. In _International Conference on Machine Learning_, pages 20479-20502. PMLR, 2022.
* [115] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. A molecular multimodal foundation model associating molecule graphs with natural language. _arXiv preprint arXiv:2209.05481_, 2022.
* [116] Ruoxi Sun, Hanjun Dai, Li Li, Steven Kearnes, and Bo Dai. Energy-based view of retrosynthesis. _arXiv preprint arXiv:2007.13437_, 2020.
* [117] Ruoxi Sun, Hanjun Dai, and Adams Wei Yu. Rethinking of graph pretraining on molecular representation. 2022.
* [118] Raphael JL Townshend, Martin Vogele, Patricia Suriana, Alexander Derry, Alexander Powers, Yianni Laloudakis, Sidhika Balachandar, Brandon Anderson, Stephan Eismann, Risi Kondor, et al. Atom3d: Tasks on molecules in three dimensions. _arXiv preprint arXiv:2012.04035_, 2020.
* [119] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [120] Soledad Villar, David W. Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-Smith. Scalars are universal: Equivariant machine learning, structured like classical physics. 2021.
* [121] Hanchen Wang, Jean Kaddour, Shengchao Liu, Jian Tang, Matt Kusner, Joan Lasenby, and Qi Liu. Evaluating self-supervised learning for molecular graph embeddings. _arXiv preprint arXiv:2206.08005_, 2022.
* [122] Limei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, and Shuiwang Ji. Learning hierarchical protein representations via complete 3d graph networks. 2022.
* [123] Renxiao Wang, Xueliang Fang, Yipin Lu, and Shaomeng Wang. The pdbbind database: Collection of binding affinities for protein- ligand complexes with known three-dimensional structures. _Journal of medicinal chemistry_, 47(12):2977-2980, 2004.
* [124] Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, and Shaomeng Wang. The pdbbind database: methodologies and updates. _Journal of medicinal chemistry_, 48(12):4111-4119, 2005.
* [125] Shiyu Wang, Xiaojie Guo, and Liang Zhao. Deep generative model for periodic graphs. _arXiv preprint arXiv:2201.11932_, 2022.
* [126] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [127] Hermann Weyl. _Symmetry_, volume 47. Princeton University Press, 2015.
* [128] Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. _arXiv preprint arXiv:2110.06197_, 2021.
* [129] Tian Xie and Jeffrey C Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. _Physical review letters_, 120(14):145301, 2018.
* [130] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [131] Zhao Xu, Youzhi Luo, Xuan Zhang, Xinyi Xu, Yaochen Xie, Meng Liu, Kaleb Andrew Dickerson, Cheng Deng, Maho Nakata, and Shuiwang Ji. Molecule3d: A benchmark for predicting 3d geometries from molecular graphs, 2021.
* [132] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. _Journal of chemical information and modeling_, 59(8):3370-3388, 2019.

* [133] Huaxiu Yao, Ying Wei, Long-Kai Huang, Ding Xue, Junzhou Huang, and Zhenhui Jessie Li. Functionally regionalized knowledge transfer for low-resource drug discovery. _Advances in Neural Information Processing Systems_, 34:8256-8268, 2021.
* [134] Huaxiu Yao, Xinyu Yang, Xinyi Pan, Shengchao Liu, Pang Wei Koh, and Chelsea Finn. Leveraging domain relations for domain generalization. _arXiv preprint arXiv:2302.02609_, 2023.
* [135] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* [136] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. _arXiv preprint arXiv:2206.00133_, 2022.
* [137] Chengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 617-626, 2020.
* [138] Anthony Zee. _Group theory in a nutshell for physicists_, volume 17. Princeton University Press, 2016.
* [139] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. _Nature communications_, 13(1):862, 2022.
* [140] Linfeng Zhang, Jiequ Han, Han Wang, Roberto Car, and EJPRL Weinan. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. _Physical review letters_, 120(14):143001, 2018.
* [141] Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding. _arXiv preprint arXiv:2201.11147_, 2022.
* [142] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. 2022.
* [143] Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. _bioRxiv_, pages 2023-05, 2023.
* [144] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. 2023.
* [145] Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, et al. Torchdrug: A powerful and flexible machine learning platform for drug discovery. _arXiv preprint arXiv:2202.08320_, 2022.