# Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration

 Zhihan Liu\({}^{1}\)   Miao Lu\({}^{2}\)   Wei Xiong\({}^{3}\)   Han Zhong\({}^{4}\)   Hao Hu\({}^{5}\)

**Shenao Zhang\({}^{1}\)   Sirui Zheng\({}^{1}\)   Zhuoran Yang\({}^{6}\)   Zhaoran Wang\({}^{1}\)**

\({}^{1}\)Northwestern University  \({}^{2}\)Stanford University  \({}^{3}\)University of Illinois Urbana-Champaign

\({}^{4}\)Peking University  \({}^{5}\)Tsinghua University  \({}^{6}\)Yale University

{zhihanliu2027,shenaozhang2028,siruizheng2025}@u.northwestern.edu

miaolu@stanford.edu, wx13@illinois.edu, hanzhong@stu.pku.edu.cn

huh22@mails.tsinghua.edu.cn, zhuoran.yang@yale.edu, zhaoranwang@gmail.com

Equal contribution.

###### Abstract

In reinforcement learning (RL), balancing exploration and exploitation is crucial for achieving an optimal policy in a sample-efficient way. To this end, existing sample-efficient algorithms typically consist of three components: estimation, planning, and exploration. However, to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as data-dependent level-set constraints or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called _Maximize to Explore_ (MEX), which only needs to optimize _unconstrainedly_ a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that the MEX achieves a sublinear regret with general function approximators and is extendable to the zero-sum Markov game setting. Meanwhile, we adapt deep RL baselines to design practical versions of MEX in both the model-based and model-free settings, which outperform baselines in various MuJoCo environments with sparse reward by a stable margin. Compared with existing sample-efficient algorithms with general function approximators, MEX achieves similar sample efficiency while also enjoying a lower computational cost and is more compatible with modern deep RL methods. Our codes are available at https://github.com/agentification/MEX.

## 1 Introduction

The crux of online reinforcement learning (online RL) lies in maintaining a balance between exploiting the current knowledge of the agent about the environment and exploring unfamiliar areas [69]. To fulfill this, agents in existing sample-efficient RL algorithms predominantly undertake three tasks: i) _estimate_ a hypothesis using historical data to encapsulate their understanding of the environment; ii) perform _planning_ based on the estimated hypothesis to exploit their current knowledge; iii) further _explore_ the unknown environment via carefully designed exploration strategies.

There exists a long line of research on integrating the aforementioned three components harmoniously, to find optimal policies in a sample-efficient manner. From theoretical perspectives, existing theories aim to minimize the notion of _online external regret_ which measures the cumulative suboptimality gap of the policies learned during online learning. It is well studied that one can design both statistically and computationally efficient algorithms (e.g., upper confidence bound (UCB), [6, 39, 12, 91]) with sublinear online regret for tabular and linear Markov decision processes (MDPs). But when itcomes to MDPs with general function approximations, most of them involve impractical algorithmic components to incentivize exploration. Usually, to cope with general function approximations, agents need to solve constrained optimization problems within data-dependent level-sets [37, 20], or sample from complicated posterior distributions over the space of hypotheses [19, 2, 89], both of which pose considerable challenges for implementation. From a practical perspective, a prevalent approach in deep RL for balancing exploration and exploitation is to use an ensemble of neural networks [75, 59, 14, 53, 44, 18, 45], which serves as an empirical approximation of the UCB method. However, such an ensemble method suffers from high computational cost and lacks theoretical guarantee when the underly MDP is neither linear nor tabular. As for other deep RL algorithms for exploration [29, 3, 11, 9, 17, 65], such as curiosity-driven method [60], it also remains unknown in theory whether they are provably sample-efficient in the context of general function approximations.

Hence, in this paper, we are aimed at tackling these issues and answering the following question:

_Under general function approximation, can we design a sample-efficient and easy-to-implement RL framework to trade off between exploration and exploitation?_

To this end, we propose an easy-to-implement RL framework, _Maximize to Explore_ (MEX), as an affirmative answer to the question. To strike a balance between exploration and exploitation, MEX propose to maximize a weighted sum of two objectives: (i) the optimal expected total return associated with a given hypothesis and (ii) the negative estimation error of that hypothesis. Consequently, MEX naturally combines planning and estimation components in just a single objective. By choosing the hypothesis that maximizes the weighted sum and executing the optimal policy with respect to the chosen hypothesis, MEX automatically balances between exploration and exploitation.

We highlight that the objective of MEX is _not_ obtained by taking the Lagrange dual of the constrained optimization objective within data-dependent level-sets [37, 20, 15].This is because the coefficient of the weighted sum, which remains fixed, is data-independent and predetermined for all episodes. Contrary to Lagrangian methods, MEX does not necessitate an inner loop of optimization for dual variables, thereby circumventing the complications associated with minimax optimization. As a maximization-only framework, MEX is friendly to implementations with neural networks and does not rely on sampling or ensemble.

In the theory part, we prove that MEX achieves a sublinear regret \(\widetilde{\mathcal{O}}(\text{Poly}(H)\cdot d_{\text{GEC}}^{1/2}(1/\sqrt{HK}) \cdot K^{1/2})\) under mild assumptions and is thus sample-efficient, where \(K\) is the number of episodes and \(H\) is the horizon length. Here \(d_{\text{GEC}}(\cdot)\) is the Generalized Eluder Coefficient (GEC) [89] that characterizes the complexity of learning the underlying MDP under general function approximations. Because the class of low-GEC MDPs includes almost all known theoretically tractable MDP instances, our proved result can be tailored to a multitude of specific settings with either a model-free or a model-based hypothesis, such as MDPs with low Bellman eluder dimension [37], MDPs of bilinear class [20], and MDPs with low witness rank [67]. Besides, thanks to the flexibility of the MEX framework, we further extend it to online RL in two-player zero-sum Markov games (MGs), for which we further extend the definition of GEC to two-player zero-sum MGs and establish the sample efficiency with general function approximations. Moving beyond theory and into practice, we adapt famous RL baselines TD3[27] and MBP0[34] to design practical versions of MEX in model-free and model-based fashion, respectively. On various MuJoCo environments [71] with sparse rewards, experimental results show that MEX outperforms baselines steadily and significantly. Compared with other deep RL algorithms, MEX has low computational overhead and straightforward implementation while maintaining a theoretical guarantee.

**Contributions.** We conclude our contributions from three perspectives.

1. We propose an easy-to-implement RL algorithm framework MEX that _unconstrainedly_ maximizes a single objective to fuse estimation and planning, automatically trading off between exploration and exploitation. Under mild structural assumptions, we prove that MEX achieves a sublinear regret \(\widetilde{\mathcal{O}}(\text{Poly}(H)\cdot d_{\text{GEC}}^{1/2}(1/\sqrt{HK}) K^{1/2})\) with general function approximators, and thus is sample-efficient. Here \(d_{\text{GEC}}(\cdot)\) is the generalized Eluder Coefficient (GEC) of the underlying MDP.
2. We instantiate the generic MEX framework to several model-based and model-free examples and establish corresponding theoretical results. Further, we extend the MEX framework to two-player zero-sum MGs and also prove the sample efficiency with an extended definition of GEC.
3. We design practical implementations of MEX for MDPs in both model-based and model-free styles. Experiments on various MuJoCo environments with sparse reward demonstrate the effectiveness of our proposed MEX framework.

### Related work

**Sample-efficient RL with function approximation.** The success of DRL methods has motivated a line of work focused on function approximation scenarios. This work originated in the linear case [74; 81; 12; 39; 84; 5; 82; 57; 91; 90] and is later extended to general function approximation. Wang et al. [73] first study the general function approximation using the notion of eluder dimension [63], which takes the linear MDP [39] as a special case but with inferior results. Zanette et al. [85] consider a different type of framework based on Bellman completeness, which assumes that the class used for approximating the optimal Q-functions is closed in terms of the Bellman operator and improves the results for linear MDP. After this, Jin et al. [37] consider the eluder dimension of the class of Bellman residual associated with the RL problems, which captures more solvable problems. Another line of works focuses on the low-rank structures of the problems, where Jiang et al. [35] propose the Bellman rank for model-free RL and Sun et al. [67] propose the witness rank for model-based RL. Following these two works, Du et al. [20] propose the bilinear class, which contains more MDP models with low-rank structures [6; 67; 39; 57; 12; 91] by allowing a flexible choice of discrepancy function class. However, it is known that neither BE nor bilinear class captures each other. Dann et al. [19] first consider eluder-coefficient-type complexity measure on the Q-type model-free RL. It was later extended by Zhong et al. [89] to cover all the above-known solvable problems in both model-free and model-based manners. Foster et al. [25; 23] study another notion of complexity measure, the decision-estimation coefficient (DEC), which also unifies the Bellman eluder dimension and bilinear class. The DEC framework is appealing due to the matching lower bound in some decision-making problems, where all other complexity measures do not have. However, due to the presence of a minimax subroutine in its definition, they require a much more complicated minimax optimization oracle and cannot apply to the classical optimism-based or sampling-based methods. Chen et al. [13], Foster et al. [24] extend the vanilla DEC (to the model-free case) by incorporating an optimistic modification, which was originally referred to as the feel-good modification in Zhang [87]. Chen et al. [15] study Admissible Bellman Characterization (ABC) class to generalize BE. They also extend the GOLF algorithm and Bellman completeness in model-free RL to the model-based case by considering more general (vector-form) discrepancy loss functions to construct sharper in-sample error estimators and obtain sharper bounds compared to Sun et al. [67]. Xie et al. [79] connect the online RL with the coverage condition in the offline RL, and also study the GOLF algorithm proposed in Jin et al. [37].

**Algorithmic design in sample-efficient RL with function approximation.** The most prominent approach in this area is based on the principle of "Optimism in the Face of Uncertainty" (OFU), which dates back to Auer et al. [4]. For instance, for linear function approximation, Jin et al. [39] propose an optimistic variant of Least-Squares Value Iteration (LSVI), which achieves optimism by adding a bonus at each step. For the general case, Jiang et al. [36] first propose an elimination-based algorithm with optimism in model-free RL and is extended to model-based RL by [67]. After these, Du et al. [20], Jin et al. [37] propose two OFU-based algorithms, which are more similar to the lin-UCB algorithm [1] studied in the linear contextual bandit literature. The model-based counterpart (Optimistic Maximum Likelihood Estimation) is studied in Liu et al. [46], Chen et al. [13]. Specifically, these algorithms explicitly maintain a confidence set that contains the ground truth with high probability and conducts a constraint optimization step at each iteration to select the most optimistic hypothesis in the confidence set. The other line of work studies another powerful algorithmic framework based on posterior sampling. For instance, Zanette et al. [84] study randomized least-squares value iteration (RLSVI), which can be interpreted as a sampling-based algorithm and achieves an order-optimal result for linear MDP. For general function approximation, the works mainly follow the idea of the "feel-good" modification proposed in Zhang [87]. These algorithms start from some prior distribution over the hypothesis space and update the posterior distribution according to the collected samples but with certain optimistic modifications in either the prior or the loglikelihood function. Then the hypothesis for each iteration is sampled from the posterior and guides data collection. In particular, Dann et al. [19] studies the model-free Q-type problem, and Agarwal and Zhang [2] studies the model-based problems, but under different notions of complexity measures. Zhong et al. [89] further utilize the idea in Zhang [87] and extend the posterior sampling algorithm in Dann et al. [19] to be a unified sampling-based framework to solve both model-free and model-based RL problems, which is also shown to apply to the more challenging partially observable setting. In addition to the OFU-based algorithm and the sampling-based framework, Foster et al. [25] propose the Estimation-to-Decisions (E2D) algorithm, which can solve problems with low DEC but requires solving a complicated minimax subroutine to fit in the framework of DEC.

Due to the limitation of the page, we defer the remaining discussions of **Relationship with reward-biased maximum likelihood estimation**, **Exploration of DRL**, and **Two-player Zero-Sum Markov Game** to Appendix B.

## 2 Preliminaries

### Episodic Markov Decision Processes and Online Reinforcement Learning

We consider an episodic MDP defined by a tuple \((\mathcal{S},\mathcal{A},H,\mathbb{P},r)\), where \(\mathcal{S}\) and \(\mathcal{A}\) are the state and action spaces, \(H\) is a finite horizon, \(\mathbb{P}=\{\mathbb{P}_{h}\}_{h=1}^{H}\) with \(\mathbb{P}_{h}:\mathcal{S}\times\mathcal{A}\mapsto\Delta(\mathcal{S})\) the transition kernel at the \(h\)-th timestep, and \(r=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) the reward function at the \(h\)-th timestep. Without loss of generality, we assume that the reward function \(r_{h}\) is both deterministic and known.

In the episodic MDP, the agent interacts with the environment by the following _online_ protocol. At the beginning of the \(k\)-th episode, the agent selects a policy \(\pi^{k}=\{\pi_{h}^{k}:\mathcal{S}\mapsto\Delta(\mathcal{A})\}_{h=1}^{H}\). It takes an action \(a_{h}^{k}\sim\pi_{h}^{k}(\cdot\mid x_{h}^{k})\) at timestep \(h\) and state \(x_{h}^{k}\). After receiving the reward \(r_{h}^{k}=r_{h}(x_{h}^{k},a_{h}^{k})\) from the environment, it transits to the next state \(x_{h+1}^{k}\sim\mathbb{P}_{h}(\cdot\mid x_{h}^{k},a_{h}^{k})\). When the agent reaches the state \(x_{H+1}^{k}\), it ends the \(k\)-th episode. Without loss of generality, we assume that the initial state \(x_{1}=\underline{x}\) is fixed. Our analysis can be generalized to the setting where \(x_{1}\) is sampled from a distribution on \(\mathcal{S}\).

**Policy and value functions**. For a policy \(\pi=\{\pi_{h}:\mathcal{S}\mapsto\Delta(\mathcal{A})\}_{h=1}^{H}\), we denote by \(V_{h}^{\pi}:\mathcal{S}\mapsto\mathbb{R}\) and \(Q_{h}^{\pi}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) its value function and state-action value function at the \(h\)-th timestep, which characterizes the expected total rewards received by the agent under policy \(\pi\) afterward, starting from some \(x_{h}=x\in\mathcal{S}\) (or \(x_{h}=x\in\mathcal{S},a_{h}=a\in\mathcal{A}\), resp.), till the end of the episode. Specifically,

\[V_{h}^{\pi}(x) :=\mathbb{E}_{\pi}\left[\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(x_{ h^{\prime}},a_{h^{\prime}})\right|x_{h}=x\right],\quad\forall x\in\mathcal{S},\] (2.1) \[Q_{h}^{\pi}(x,a) :=\mathbb{E}_{\pi}\left[\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(x_{ h^{\prime}},a_{h^{\prime}})\right|x_{h}=x,a_{h}=a\right],\quad\forall(x,a)\in \mathcal{S}\times\mathcal{A}.\] (2.2)

We know there exists an optimal policy \(\pi^{*}\) which has the optimal value function for all initial states [61], that is, \(V_{h}^{\pi^{*}}(s)=\sup_{\pi}V_{h}^{\pi}(x)\) for all \(h\in[H]\) and \(x\in\mathcal{S}\). For simplicity, we abbreviate \(V^{\pi^{*}}\) as \(V^{*}\) and the optimal state-action value function \(Q^{\pi^{*}}\) as \(Q^{*}\). Moreover, the optimal value functions \(Q^{*}\) and \(V^{*}\) satisfy the following Bellman optimality equation [61], given by

\[V_{h}^{*}(x) =\max_{a\in\mathcal{A}}Q_{h}^{*}(x,a),\qquad V_{H+1}^{*}(x)=0,\] (2.3) \[Q_{h}^{*}(x,a) =\left(\mathcal{T}_{h}Q_{h+1}^{*}\right)(x,a):=r_{h}(x,a)+\mathbb{ E}_{x^{\prime}\sim\mathbb{P}_{h}(\cdot\mid x,a)}[V_{h+1}^{*}\left(x^{\prime} \right)],\] (2.4)

for all \((x,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\). We call \(\mathcal{T}_{h}\) the Bellman optimality operator at timestep \(h\). Also, for any two functions \(Q_{h}\) and \(Q_{h+1}\) on \(\mathcal{S}\times\mathcal{A}\), we define

\[\mathcal{E}_{h}(Q_{h},Q_{h+1};x,a):=Q_{h}(x,a)-\mathcal{T}_{h}Q_{h+1}(x,a), \quad\forall(x,a)\in\mathcal{S}\times\mathcal{A},\] (2.5)

as the Bellman residual at timestep \(h\) of \((Q_{h},Q_{h+1})\).

**Performance metric.** We measure the online performance of an agent after \(K\) episodes by _regret_. We assume that the learner predicts the optimal policy \(\pi^{*}\) via \(\pi^{k}\) in the \(k\)-th episode for each \(k\in[K]\). Then the regret of \(K\) episodes is defined as the cumulative suboptimality gap of \(\{\pi^{k}\}_{k\in[K]}\)2,

Footnote 2: We allow the agent to predict the optimal policy via \(\pi^{k}\) while executing some other exploration policy \(\pi^{k}_{\exp}\) to interact with the environment and collect data, as is considered in the related literature [67; 20; 89]

\[\text{Regret}(K)=\sum_{k=1}^{K}V_{1}^{*}(x_{1})-V_{1}^{\pi^{k}}(x_{1}).\] (2.6)

The target of sample-efficient online RL is to achieve sublinear regret (2.6) with respect to \(K\).

### Function Approximation: Model-Free and Model-Based Hypothesis

To handle MDPs with large or even infinite state space, we introduce a family of function approximators. In specific, we consider a hypothesis class \(\mathcal{H}=\mathcal{H}_{1}\times\cdots\times\mathcal{H}_{H}\), which can be specified to model-based and model-free settings respectively. Also, we denote by \(\Pi=\Pi_{1}\times\cdots\times\Pi_{H}\) as the space of Markovian policies. We now specify \(\mathcal{H}\) for model-free and model-based settings.

**Example 2.1** (Model-free hypothesis).: _For model-free hypothesis class, \(\mathcal{H}\) contains state-action value functions of the MDP, i.e., \(\mathcal{H}_{h}\subseteq\{f_{h}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\}\). Specifically, for any \(f=(f_{1},\cdots,f_{H})\in\mathcal{H}\), we denote \(Q_{f}=\{Q_{h,f}\}_{h\in[H]}\) with \(Q_{h,f}=f_{h}\). Also, we denote the corresponding optimal state-value function \(V_{f}=\{V_{h,f}\}_{h\in[H]}\) with \(V_{h,f}(\cdot)=\max_{a\in\mathcal{A}}Q_{h,f}(\cdot,a)\) and denote the corresponding optimal policy by \(\pi_{f}=\{\pi_{h,f}\}_{h\in[H]}\) with \(\pi_{h,f}(\cdot)=\arg\max_{a\in\mathcal{A}}Q_{h,f}(\cdot,a)\). Finally, we denote the optimal state-action value function under the true model, i.e., \(Q^{*}\), by \(f^{*}\)._

**Example 2.2** (Model-based hypothesis).: _For model-based hypothesis class, \(\mathcal{H}\) contains models of the MDP, i.e., the transition kernel. Specifically, we denote \(f=\mathbb{P}_{f}=(\mathbb{P}_{1,f},\cdots,\mathbb{P}_{H,f})\in\mathcal{H}\). For any \((f,\pi)\in\mathcal{H}\times\Pi\), we define \(V_{f}^{\pi}=\{V_{h,f}^{n}\}_{h\in[H]}\) as the state-value function induced by model \(\mathbb{P}_{f}\) and policy \(\pi\). We use \(V_{f}=\{V_{h,f}\}_{h\in[H]}\) to denote the corresponding optimal state-value function, i.e., \(V_{h,f}=\sup_{\pi\in\Pi}V_{\pi,f}^{n}\). The corresponding optimal policy is denoted by \(\pi_{f}=\{\pi_{h,f}\}_{h\in[H]}\), where \(\pi_{h,f}=\arg\sup_{\pi\in\Pi}V_{h,f}^{n}\). Finally, we denote the true model \(\mathbb{P}\) of the MDP as \(f^{*}\)._

We remark that the main difference between the model-based hypothesis (Example 2.2) and the model-free hypothesis (Example 2.1) is that model-based RL directly learns the transition kernel of the underlying MDP, while model-free RL learns the optimal state-action value function. Since we do not add any specific structural form to the hypothesis class, e.g., linear function or kernel function, we are in the context of _general function approximations_[67; 37; 20; 89; 15].

## 3 Algorithm: Maximize to Explore for Online RL

In this section, we propose **Maximize to Explore** (MEX, Algorithm 1) for online RL in MDPs with general function approximations. With a novel single objective, **MEX** automatically balances the goal of exploration and exploitation. We first give a generic algorithm framework and then instantiate it to model-free (Example 2.1) and model-based (Example 2.2) hypotheses respectively.

**Generic algorithm design.** In each episode \(k\in[K]\), the agent first estimates a hypothesis \(f^{k}\in\mathcal{H}\) using historical data \(\{\mathcal{D}^{s}\}_{s=1}^{k-1}\) by maximizing a composite objective (3.1). Specifically, in order to achieve exploiting history knowledge while encouraging exploration, the agent considers a single objective that sums: **(a)** the negative loss \(-L_{h}^{k-1}(f)\) induced by the hypothesis \(f\), which represents the exploitation of the agent's current knowledge; **(b)** the expected total return of the optimal policy associated with this hypothesis, i.e., \(V_{1,f}\), which represents exploration for a higher return. With a tuning parameter \(\eta>0\), the agent balances the weight put on the tasks of exploitation and exploration. The agent then predicts \(\pi^{*}\) via the optimal policy with respect to the hypothesis that maximizes the composite exploration-exploitation objective function, i.e., \(\pi_{f^{k}}\). Also, the agent executes certain exploration policy \(\pi_{\exp}(f^{k})\) to collect data \(\mathcal{D}^{k}=\{(x_{h}^{k},a_{h}^{k},r_{h}^{k},x_{h+1}^{k})\}_{h=1}^{H}\) and updates the loss function \(L_{h}^{k}(f)\). The choice of \(\pi_{\exp}(f^{k})\) depends on the specific MDP structure, and we refer to examples in Section 5 for detailed discussions.

We highlight that **MEX** is **not** a Lagrangian duality of constrained optimization objectives within data-dependent level-sets [37; 20; 15]. In fact, **MEX** only needs to fix the parameter \(\eta\) across each episode. Thus \(\eta\) is independent of data and predetermined, which contrasts Lagrangian methods that involve an inner loop of optimization for the dual variables. We also remark that we can rewrite (3.1) as a joint optimization \((f,\pi)=\operatorname*{argsup}_{f\in\mathcal{H},\pi\in\Pi}V_{1,f}^{\pi}(x_{1}) -\eta\sum_{h=1}^{H}L_{h}^{k-1}(f).\) When \(\eta\) tends to infinity, **MEX** can be reduced to vanilla Actor-Critic framework [41], where critic \(f\) minimizes estimation error and actor \(\pi\) conducts greedy policy following the critic \(f\). In the following two parts,we instantiate Algorithm 1 to model-based and mode-free hypotheses by specifying the loss function \(L^{k}_{h}(f)\).

**Model-free algorithm.** For model-free hypothesis (Example 2.1), (3.1) becomes

\[f^{k}=\operatorname*{argsup}_{f\in\mathcal{H}}\left\{\max_{a_{1}\in\mathcal{A} }Q_{1,f}(x_{1},a_{1})-\eta\cdot\sum_{h=1}^{H}L^{k-1}_{h}(f)\right\}.\] (3.2)

Regarding the choice of the loss function, for seek of theoretical analysis, to deal with MDPs with low Bellman eluder dimension [37] and MDPs of bilinear class [20], we assume the existence of certain function \(l\) which generalizes the notion of Bellman residual.

**Assumption 3.1**.: _Suppose the function \(l:\mathcal{H}\times\mathcal{H}_{h}\times\mathcal{H}_{h+1}\times(\mathcal{S} \times\mathcal{A}\times\mathbb{R}\times\mathcal{S})\mapsto\mathbb{R}\) satisfies: **i)** (**Generalized Bellman completeness) [89, 15]** there exists an operator \(\mathcal{P}_{h}:\mathcal{H}_{h+1}\mapsto\mathcal{H}_{h}\) such that for any \((f^{\prime},f_{h},f_{h+1})\in\mathcal{H}\times\mathcal{H}_{h}\times\mathcal{ H}_{h+1}\) and \(\mathcal{D}_{h}=(x_{h},a_{h},r_{h},x_{h+1})\in\mathcal{S}\times\mathcal{A} \times\mathbb{R}\times\mathcal{S}\), it holds \(l_{f^{\prime}}\big{(}(f_{h},f_{h+1}),\mathcal{D}_{h}\big{)}-l_{f^{\prime}} \big{(}(\mathcal{P}_{h}f_{h+1},f_{h+1}),\mathcal{D}_{h}\big{)}=\mathbb{E}_{x_{ h+1}\sim\mathbb{P}_{h}(\cdot|x_{h},a_{h})}\big{[}l_{f^{\prime}}\big{(}(f_{h},f_{h+1}), \mathcal{D}_{h}\big{)}\big{]},\)_

_where we require that \(\mathcal{P}_{h}f^{*}_{h+1}=f^{*}_{h}\) and that \(\mathcal{P}_{h}f_{h+1}\in\mathcal{H}_{h}\) for any \(f_{h+1}\in\mathcal{H}_{h+1}\) and step \(h\in[H]\); **ii)** (**Boundedness)** it holds that \(\sup_{f^{\prime}\in\mathcal{H}}\|l_{f^{\prime}}((f_{h},f_{h+1}),\mathcal{D}_{ h})\|_{\infty}\leq B_{l}\) for some constant \(B_{l}>0\)._

We then set the loss function \(L^{k}_{h}\) as an empirical estimation of the generalized squared Bellman error \(|\mathbb{E}_{x_{h+1}\sim\mathbb{P}_{h}(\cdot|x_{h},a_{h})}[l_{f^{*}}((f_{h},f_ {h+1}),\mathcal{D}^{*}_{h})]|^{2}\), given by

\[L^{k}_{h}(f)=\sum_{s=1}^{k}l_{f^{s}}\big{(}(f_{h},f_{h+1}),\mathcal{D}^{*}_{h} \big{)}^{2}-\inf_{f^{\prime}_{h}\in\mathcal{H}_{h}}\sum_{s=1}^{k}l_{f^{s}} \big{(}(f^{\prime}_{h},f_{h+1}),\mathcal{D}^{*}_{h}\big{)}^{2}.\] (3.3)

We remark that the subtracted infimum term in (3.3) is to handle the variance terms in the estimation to achieve a fast theoretical rate. Similar essential ideas are also adopted by [37, 78, 19, 38, 52, 2, 89, 15].

**Model-based algorithm.** For model-based hypothesis (Example 2.2), (3.1) becomes

\[f^{k}=\operatorname*{argsup}_{f\in\mathcal{H}}\left\{\sup_{\pi\in\Pi}V^{\pi}_ {1,\mathbb{P}_{f}}(x_{1})-\eta\sum_{h=1}^{H}L^{k-1}_{h}(f)\right\},\] (3.4)

which is a joint optimization over the model \(\mathbb{P}_{f}\) and the policy \(\pi\). In the model-based algorithm, we choose the loss function \(L^{k}_{h}\) as the negative log-likelihood loss, defined as

\[L^{k}_{h}(f)=-\sum_{s=1}^{k}\log\mathbb{P}_{h,f}(x^{s}_{h+1}|x^{s}_{h},a^{s}_{h }).\] (3.5)

## 4 Regret Analysis for MEX Framework

In this section, we establish a regret analysis for the MEX framework (Algorithm 1). We give a generic theoretical guarantee which holds for both model-free and model-based settings. We first present three key assumptions needed for sample-efficient learning with MEX. In Section 5, we specify the generic theory to specific examples of MDPs and hypothesis classes satisfying these assumptions.

Firstly, we assume that the hypothesis class \(\mathcal{H}\) is well-specified, containing the true hypothesis \(f^{*}\).

**Assumption 4.1** (Realizablity).: _We assume that the true hypothesis \(f^{*}\in\mathcal{H}\)._

Then we need to make a structural assumption on the MDP to ensure sample-efficient online learning. Inspired by Zhong et al. [89], we require the MDP to have low _Generalized Eluder Coefficient_ (GEC). A low GEC indicates that the agent can effectively mitigate out-of-sample prediction errors by minimizing in-sample errors derived from historical data. To introduce, we define a discrepancy function \(\ell_{f^{\prime}}(f;\xi_{h}):\mathcal{H}\times\mathcal{H}\times(\mathcal{S} \times\mathcal{A}\times\mathbb{R}\times\mathcal{S})\mapsto\mathbb{R}\) which characterizes the error of a hypothesis \(f\in\mathcal{H}\) on data \(\xi_{h}=(x_{h},a_{h},r_{h},x_{h+1})\). Specific choices of \(\ell\) are given in Section 5 for concrete model-free and model-based examples.

**Assumption 4.2** (Low Generalized Eluder Coefficient [89]).: _We assume that given an \(\epsilon>0\), there exists \(d(\epsilon)\in\mathbb{R}_{+}\), such that for any sequence of \(\{f^{k}\}_{k\in[K]}\subset\mathcal{H}\), \(\{\pi_{\exp}(f^{k})\}_{k\in[K]}\subset\Pi\),_

\[\sum_{k=1}^{K}V_{1,f^{k}}-V^{\pi_{f^{k}}}_{1}\leq\inf_{\mu>0}\left\{\frac{\mu}{2 }\sum_{h=1}^{H}\sum_{k=1}^{K}\sum_{s=1}^{k-1}\mathbb{E}_{\xi_{h}\sim\pi_{\exp}(f ^{*})}[\ell_{f^{s}}(f^{k};\xi_{h})]+\frac{d(\epsilon)}{2\mu}+\sqrt{d( \epsilon)HK}+\epsilon HK\right\}.\]

_We denote the smallest number \(d(\epsilon)\in\mathbb{R}_{+}\) satisfying this condition as \(d_{\mathrm{GEC}}(\epsilon)\)._As is shown by Zhong et al. [89], the low-GEC MDP class covers almost all known theoretically tractable MDP instances, such as linear MDP [81; 39], linear mixture MDP [5; 57; 12], MDPs of low witness rank [67], MDPs of low Bellman eluder dimension [37], and MDPs of bilinear class [20].

Finally, we make a concentration-style assumption which characterizes how the loss function \(L_{h}^{k}\) is related to the expectation of the discrepancy function \(\mathbb{E}[\ell]\) appearing in the definition of GEC. For ease of presentation, we assume that \(\mathcal{H}\) is finite, i.e., \(|\mathcal{H}|<\infty\), but our result can be directly extended to an infinite \(\mathcal{H}\) using covering number arguments [72; 37; 49; 38].

**Assumption 4.3** (Generalization).: _We assume that \(\mathcal{H}\) is finite, i.e., \(|\mathcal{H}|<+\infty\), and that with probability at least \(1-\delta\), for any episode \(k\in[K]\) and hypothesis \(f\in\mathcal{H}\), it holds that_

\[\sum_{h=1}^{H}L_{h}^{k-1}(f^{*})-L_{h}^{k-1}(f)\lesssim-\sum_{h=1}^{H}\sum_{s =1}^{k-1}\mathbb{E}_{\xi_{h}\sim\pi_{\mathrm{exp}}(f^{s})}[\ell_{f^{s}}(f;\xi_ {h})]+B\big{(}H\log(HK/\delta)+\log(|\mathcal{H}|)\big{)},\]

_where \(B=B_{l}^{2}\) for model-free hypothesis (Assumption 3.1) and \(B=1\) for model-based hypothesis._

Such a concentration style inequality is well known in the literature of online RL with general function approximation and similar analysis is also adopted by [37; 15]. With Assumptions 4.1, 4.2, and 4.3, we can present our main result (see Appendix D.1 for a proof).

**Theorem 4.4** (Online regret of **Mex** (Algorithm 1)).: _Under Assumptions 4.1, 4.2, and 4.3, by setting_

\[\eta=\sqrt{\frac{d_{\mathrm{GEC}}(1/\sqrt{HK})}{(H\log(HK/\delta)+\log(| \mathcal{H}|))\cdot B\cdot K}},\]

_then the regret of Algorithm 1 after \(K\) episodes is upper bounded by_

\[\mathrm{Regret}(K)\lesssim\sqrt{d_{\mathrm{GEC}}(1/\sqrt{HK})\cdot(H\log(HK/ \delta)+\log(|\mathcal{H}|))\cdot B\cdot K},\]

_with probability at least \(1-\delta\). Here \(d_{\mathrm{GEC}}(\cdot)\) is defined in Assumption 4.2._

Theorem 4.4 shows that the regret of Algorithm 1 scales with the square root of the number of episodes \(K\) and the polynomials of horizon \(H\), \(\mathrm{GEC}\)\(d_{\mathrm{GEC}}(1/\sqrt{K})\), and log covering number \(\log\mathcal{N}(\mathcal{H},1/K)\). When the number of episodes \(K\) tends to infinity, the average regret vanishes, meaning that the output policy of Algorithm 1 achieves global optimality. Since the regret of Algorithm 1 is sublinear with respect to the number of episodes \(K\), Algorithm 1 is proved to be sample-efficient. In Appendix C, we extend the algorithm framework and the analysis to the two-player zero-sum Markov game (MG) setting, for which we also extend the definition of GEC to two-player zero-sum MGs.

Besides, as we can see from Theorem 4.4 and its specifications in Section 5, \(\mathtt{MEX}\) matches existing theoretical results in the literature of online RL with general function approximations [89; 67; 20; 37; 19; 2]. But in the meanwhile, \(\mathtt{MEX}\) does not require explicitly solving a constrained optimization problem within data-dependent level-sets or performing a complex sampling procedure. This advantage makes \(\mathtt{MEX}\) a principled approach with easier practical implementation. We conduct deep RL experiments for \(\mathtt{MEX}\) in Section 6 to demonstrate its power in complicated online problems.

## 5 Examples of **Mex** Framework

In this section, we specify Algorithm 1 to model-based and model-free hypothesis classes for various examples of MDPs of low GEC (Assumption 4.2), including MDPs with low witness rank [67], MDPs with low Bellman eluder dimension [37], and MDPs of bilinear class [20]. For ease of presentation, we assume that \(|\mathcal{H}|<\infty\), but our result can be directly extended to infinite \(\mathcal{H}\) using covering number arguments [72; 37; 49]. All the proofs of the propositions in this section are in Appendix E.

We note that another important step in specifying Theorem 4.4 to concrete hypothesis classes is to check Assumption 4.3 (supervised learning guarantee). It is worth highlighting that, in our analysis, for both model-free and model-based hypotheses, we provide supervised learning guarantees in a neat and unified manner, independent of specific MDP structures.

### Model-free online RL in Markov Decision Processes

In this subsection, we specify Algorithm 1 for model-free hypothesis class \(\mathcal{H}\) (Example 2.1). For a model-free hypothesis class, we choose the discrepancy function \(\ell\) as, given \(\mathcal{D}_{h}=(x_{h},a_{h},r_{h},x_{h+1})\),

\[\ell_{f^{\prime}}(f;\mathcal{D}_{h})=\left(\mathbb{E}_{x_{h+1}\sim\mathbb{P}_{ h}(\cdot|x_{h},a_{h})}[l_{f^{\prime}}((f_{h},f_{h+1}),\mathcal{D}_{h})] \right)^{2}.\] (5.1)where the function \(l:\mathcal{H}\times\mathcal{H}_{h}\times\mathcal{H}_{h+1}\times(\mathcal{S}\times \mathcal{A}\times\mathbb{R}\times\mathcal{S})\mapsto\mathbb{R}\) satisfies Assumption 3.1. We specify the choice of \(l\) in concrete examples of MDPs later. In the following, we check and specify Assumptions 4.2 and 4.3 in Section 4 for model-free hypothesis classes.

**Proposition 5.1** (Generalization: model-free RL).: _We assume that \(\mathcal{H}\) is finite, i.e., \(|\mathcal{H}|<+\infty\). Then under Assumption 3.1, with probability at least \(1-\delta\), for any \(k\in[K]\) and \(f\in\mathcal{H}\), it holds that_

\[\sum_{h=1}^{H}L_{h}^{k-1}(f^{*})-L_{h}^{k-1}(f)\lesssim-\sum_{h=1}^{H}\sum_{s= 1}^{k-1}\mathbb{E}_{\xi_{h}\sim\pi_{\mathrm{exp}}(f^{*})}[\ell_{f^{s}}(f;\xi_ {h})]+B_{l}^{2}\big{(}H\log(HK/\delta)+\log(|\mathcal{H}|)\big{)},\]

_where \(L\) and \(\ell\) are defined in (3.3) and (5.1) respectively. Here \(B_{l}\) is specified in Assumption 3.1._

Proposition 5.1 specifies Assumption 4.3 for model-free hypothesis classes. For Assumption 4.2, we need structural assumptions on the MDP. Given an MDP with generalized eluder dimension \(d_{\mathrm{GEC}}\), we have the following corollary of our main theoretical result (Theorem 4.4).

**Corollary 5.2** (Online regret of MEX: model-free hypothesis).: _Given an MDP with generalized eluder coefficient \(d_{\mathrm{GEC}}(\cdot)\) and a finite model-free hypothesis class \(\mathcal{H}\) with \(f^{*}\in\mathcal{H}\), under Assumption 3.1, setting_

\[\eta=\sqrt{\frac{d_{\mathrm{GEC}}(1/\sqrt{HK})}{(H\log(HK/\delta)+\log(| \mathcal{H}|))\cdot B_{l}^{2}\cdot K}},\] (5.2)

_then the regret of Algorithm 1 after \(K\) episodes is upper bounded by_

\[\mathrm{Regret}(T)\lesssim B_{l}\cdot\sqrt{d_{\mathrm{GEC}}(1/\sqrt{HK}) \cdot(H\log(HK/\delta)+\log(|\mathcal{H}|))\cdot K},\] (5.3)

_with probability at least \(1-\delta\). Here \(B_{l}\) is specified in Assumption 3.1._

Corollary 5.2 can be directly specified to MDPs with low GEC, including MDPs with low Bellman eluder dimension [37] and MDPs of bilinear class [20]. See Appendix E.1 for a detailed discussion.

### Model-based online RL in Markov Decision Processes

In this subsection, we specify Algorithm 1 for model-based hypothesis class \(\mathcal{H}\) (Example 2.2). For model-based hypothesis class, we choose the discrepancy function \(\ell\) in Assumption 4.2 and 4.3 as the hellinger distance. Given data \(\mathcal{D}_{h}=(x_{h},a_{h},r_{h},x_{h+1})\), we let

\[\ell_{f^{\prime}}(f;\mathcal{D}_{h})=D_{\mathrm{H}}(\mathbb{P}_{h,f}(\cdot|x_ {h},a_{h})\|\mathbb{P}_{h,f^{*}}(\cdot|x_{h},a_{h})),\] (5.4)

where \(D_{\mathrm{H}}(\cdot\|\cdot)\) denotes the Hellinger distance. We note that by (5.4), the discrepancy function \(\ell\) does not depend on the input \(f^{\prime}\in\mathcal{H}\). In the following, we check and specify Assumption 4.2 and 4.3.

**Proposition 5.3** (Generalization: model-based RL).: _We assume that \(\mathcal{H}\) is finite, i.e., \(|\mathcal{H}|<+\infty\). Then with probability at least \(1-\delta\), for any \(k\in[K]\), \(f\in\mathcal{H}\), it holds that_

\[\sum_{h=1}^{H}L_{h}^{k-1}(f^{*})-L_{h}^{k-1}(f)\lesssim-\sum_{h=1}^{H}\sum_{s =1}^{k-1}\mathbb{E}_{\xi_{h}\sim\pi_{\mathrm{exp}}(f^{*})}[\ell_{f^{*}}(f;\xi_ {h})]+H\log(H/\delta)+\log(|\mathcal{H}|),\]

_where \(L\) and \(\ell\) are defined in (3.5) and (5.4) respectively._

Proposition 5.3 specifies Assumption 4.3 for model-based hypothesis classes. For Assumption 4.2, we also need structural assumptions on the MDP. Given an MDP with generalized eluder dimension \(d_{\mathrm{GEC}}\), we have the following corollary of our main theoretical result (Theorem 4.4).

**Corollary 5.4** (Online regret of MEX: model-based hypothesis).: _Given an MDP with generalized eluder coefficient \(d_{\mathrm{GEC}}(\cdot)\) and a finite model-based hypothesis class \(\mathcal{H}\) with \(f^{*}\in\mathcal{H}\), by setting_

\[\eta=\sqrt{\frac{d_{\mathrm{GEC}}(1/\sqrt{HK})}{(H\log(H/\delta)+\log(| \mathcal{H}|))\cdot K}},\]

_then the regret of Algorithm 1 after \(K\) episodes is upper bounded by, with probability at least \(1-\delta\),_

\[\mathrm{Regret}(K)\lesssim\sqrt{d_{\mathrm{GEC}}(1/\sqrt{HK})\cdot(H\log(H/ \delta)+\log(|\mathcal{H}|))\cdot K},\] (5.5)

Corollary 5.4 can be directly specified to MDPs with low GEC, including MDPs with low witness rank [67]. We refer to Appendix E.2 for a detailed discussion.

Experiments

In this section, we aim to answer the following two questions: **(a)** What are the practical approaches to implementing \(\mathtt{MEX}\) in both model-based (\(\mathtt{MEX}\)-\(\mathtt{MB}\)) and model-free (\(\mathtt{MEX}\)-\(\mathtt{MF}\)) settings? **(b)** Can \(\mathtt{MEX}\) handle challenging exploration tasks, especially in sparse reward scenarios?

**Experimental setups.** We evaluate the effectiveness of \(\mathtt{MEX}\) by assessing its performance in both standard \(\mathtt{gym}\) locomotion tasks and sparse reward locomotion and navigation tasks within the MuJoCo [71] environment. For sparse reward tasks, we select \(\mathtt{cheetah-vel}\), \(\mathtt{walker-vel}\), \(\mathtt{hopper-vel}\), \(\mathtt{ant-vel}\), and \(\mathtt{ant-goal}\) adapted from Yu et al. [83], where the agent receives a reward only when it successfully attains the desired velocity or goal. To adapt to deep RL settings, we consider infinite-horizon \(\gamma\)-discounted MDPs and \(\mathtt{MEX}\) variants. We report the results averaged over five random seeds. The full experimental settings are in Appendix H.

**Implementation details.** For the model-based variant \(\mathtt{MEX}\)-\(\mathtt{MB}\), we use the following objective:

\[\max_{\phi}\max_{\pi}\ \mathbb{E}_{(x,a,r,x^{\prime})\sim\beta}\left[\log \mathbb{P}_{\phi}(x^{\prime},r\,|\,x,a)\right]+\eta^{\prime}\cdot\mathbb{E}_{ x\sim\sigma}\big{[}V^{\pi}_{\phi}(x)\big{]},\] (6.1)

where we denote by \(\sigma\) the initial state distribution, \(\beta\) the replay buffer, and \(\eta^{\prime}\) corresponds to \(1/\eta\) in the previous theory sections. We leverage the _score function_ to obtain the model value gradient \(\nabla_{\phi}V^{\pi}_{\mathbb{P}_{\phi}}\) in a similar way to likelihood ratio policy gradient [70], with the gradient of action log-likelihood replaced by the gradient of state and reward log-likelihood in the model. Specifically,

\[\nabla_{\phi}\,\mathbb{E}_{x\sim\sigma}\big{[}V^{\pi}_{\mathbb{P}_{\phi}}(x) \big{]}=\mathbb{E}_{\tau^{\pi}_{\phi}}\big{[}\big{(}r+\gamma V^{\pi}_{\mathbb{ P}_{\phi}}(x^{\prime})-Q^{\pi}_{\mathbb{P}_{\phi}}(x,a)\big{)}\cdot\nabla_{ \phi}\log\mathbb{P}_{\phi}(x^{\prime},r\,|\,x,a)\big{]},\] (6.2)

where \(\tau^{\pi}_{\phi}\) is the trajectory under policy \(\pi\) and transition \(\mathbb{P}_{\phi}\), starting from \(\sigma\). We refer the readers to previous works [62; 76] for a derivation of (6.2). The model \(\phi\) and policy \(\pi\) in (6.1) are updated iteratively in a Dyna [68] style, where model-free policy updates are performed on model-generated data. Particularly, we adopt \(\mathtt{SAC}\)[30] to update the policy \(\pi\) and estimate the value \(Q^{\pi}_{\mathbb{P}_{\phi}}\) by performing temporal difference on the model data generated by \(\mathbb{P}_{\phi}\). We also follow [62] to update the model using mini-batches from \(\beta\) and normalize the advantage \(r_{h}+\gamma V^{\pi}_{\mathbb{P}_{\phi}}-Q^{\pi}_{\mathbb{P}_{\phi}}\) within each mini-batch.

For the model-free variant \(\mathtt{MEX}\)-\(\mathtt{MF}\), we observe from (3.2) that adding a maximization bias term to the standard TD error is sufficient for exploration. However, this may lead to instabilities as the bias term only involves the state-action value function of the current policy, and thus the policy may be ever-changing. To address this issue, we adopt a similar treatment as in \(\mathtt{GCL}\)[42] by subtracting a baseline state-action value from random policy \(\mu=\text{Unif}(\mathcal{A})\) and obtain the following objective:

\[\max_{\theta}\max_{\pi}\ \mathbb{E}_{\beta}\left[\big{(}r+\gamma Q_{\theta}(x^{ \prime},a^{\prime})-Q_{\theta}(x,a)\big{)}^{2}\right]+\eta^{\prime}\cdot \mathbb{E}_{\beta}\big{[}\mathbb{E}_{a\sim\pi}Q_{\theta}(x,a)-\mathbb{E}_{a \sim\mu}Q_{\theta}(x,a)\big{]}.\] (6.3)

We update \(\theta\) and \(\pi\) in (6.3) iteratively in an actor-critic fashion. Due to space limits, we refer the readers to Appendix H for more implementation details of \(\mathtt{MEX}\)-\(\mathtt{MF}\).

**Results.** We report the performance of \(\mathtt{MEX}\)-\(\mathtt{MB}\) and \(\mathtt{MEX}\)-\(\mathtt{MF}\) in Figures 1 and 2, respectively. We compare \(\mathtt{MEX}\)-\(\mathtt{MB}\) with \(\mathtt{MBPO}\)[34], where our method differs from \(\mathtt{MBPO}\)_only_ in the inclusion of the value gradient in (6.2) during model updates. We find that \(\mathtt{MEX}\)-\(\mathtt{MB}\) offers an easy implementation with minimal computational overhead and yet remains highly effective across sparse and standard MuJoCo tasks. Notably, in the sparse reward settings, \(\mathtt{MEX}\)-\(\mathtt{MB}\) excels at achieving the goal velocity and outperforms \(\mathtt{MBPO}\) by a stable margin. In standard \(\mathtt{gym}\) tasks, \(\mathtt{MEX}\)-\(\mathtt{MB}\) showcases greater sample efficiency in challenging high-dimensional tasks with higher asymptotic returns.

We then compare \(\mathtt{MEX}\)-\(\mathtt{MF}\) with the model-free baseline \(\mathtt{TD3}\)[27]. We observe that \(\mathtt{TD3}\) fails in many sparse reward tasks, while \(\mathtt{MEX}\)-\(\mathtt{MF}\) significantly boosts the performance. In standard MuJoCo gym tasks, \(\mathtt{MEX}\)-\(\mathtt{MF}\) also steadily outperforms \(\mathtt{TD3}\) with faster convergence and higher final returns.

## 7 Conclusions

In this paper, we introduce a novel RL algorithm framework--_Maximize to Explore_ (\(\mathtt{MEX}\))--aimed at striking a balance between exploration and exploitation in online learning scenarios. \(\mathtt{MEX}\) is provably sample-efficient under general function approximations and is easy to implement. Theoretically, we prove that under mild structural assumptions (low generalized eluder coefficient (GEC)), \(\mathtt{MEX}\) achieves\(\widetilde{\mathcal{O}}(\sqrt{K})\)-online regret for MDPs. We further extend the definition of GEC and MEX framework to two-player zero-sum Markov games (see Appendix C) and also prove the \(\widetilde{\mathcal{O}}(\sqrt{K})\)-online regret. In practice, we adapt MEX to deep RL methods in both model-based and model-free styles and apply them to sparse-reward MuJoCo environments, outperforming baselines significantly. We hope our work can shed light on future research of designing both statistically efficient and practically effective RL algorithms with powerful function approximation.

## Acknowledgement

Zhaoran Wang acknowledges National Science Foundation (Awards 2048075, 2008827, 2015568, 1934931), Simons Institute (Theory of Reinforcement Learning), Amazon, J.P. Morgan, and Two Sigma for their supports.

## References

* Abbasi-Yadkori et al. (2011)Abbasi-Yadkori, Y., Pal, D. and Szepesvari, C. (2011). Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_**24**.
* Agarwal and Zhang (2022)Agarwal, A. and Zhang, T. (2022). Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity. _arXiv preprint arXiv:2206.07659_.
* Aubret et al. (2019)Aubret, A., Matignon, L. and Hassas, S. (2019). A survey on intrinsic motivation in reinforcement learning.
* Auer et al. (2002)Auer, P., Cesa-Bianchi, N. and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. _Machine learning_**47** 235-256.

Figure 1: Model-based MEX-MB in sparse and standard MuJoCo locomotion tasks.

Figure 2: Model-free MEX-MF in sparse and standard MuJoCo locomotion tasks.

* Ayoub et al. [2020]Ayoub, A., Jia, Z., Szepesvari, C., Wang, M. and Yang, L. (2020). Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_. PMLR.
* Azar et al. [2017]Azar, M. G., Osband, I. and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Bai and Jin [2020]Bai, Y. and Jin, C. (2020). Provable self-play algorithms for competitive reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Bai et al. [2020]Bai, Y., Jin, C. and Yu, T. (2020). Near-optimal reinforcement learning with self-play. _arXiv preprint arXiv:2006.12007_.
* Bellemare et al. [2016]Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D. and Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. _Advances in neural information processing systems_**29**.
* Blanchet et al. [2023]Blanchet, J., Lu, M., Zhang, T. and Zhong, H. (2023). Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. _arXiv preprint arXiv:2305.09659_.
* Burda et al. [2018]Burda, Y., Edwards, H., Storkey, A. and Klimov, O. (2018). Exploration by random network distillation. _arXiv preprint arXiv:1810.12894_.
* Cai et al. [2020]Cai, Q., Yang, Z., Jin, C. and Wang, Z. (2020). Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_. PMLR.
* Chen et al. [2022]Chen, F., Mei, S. and Bai, Y. (2022). Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning. _arXiv preprint arXiv:2209.11745_.
* Chen et al. [2017]Chen, R. Y., Sidor, S., Abbeel, P. and Schulman, J. (2017). Ucb exploration via q-ensembles. _arXiv preprint arXiv:1706.01502_.
* Chen et al. [2022]Chen, Z., Li, C. J., Yuan, A., Gu, Q. and Jordan, M. I. (2022). A general framework for sample-efficient function approximation in reinforcement learning. _arXiv preprint arXiv:2209.15634_.
* Chen et al. [2021]Chen, Z., Zhou, D. and Gu, Q. (2021). Almost optimal algorithms for two-player Markov games with linear function approximation. _arXiv preprint arXiv:2102.07404_.
* Choi et al. [2018]Choi, J., Guo, Y., Moczulski, M., Oh, J., Wu, N., Norouzi, M. and Lee, H. (2018). Contingency-aware exploration in reinforcement learning. _arXiv preprint arXiv:1811.01483_.
* Chua et al. [2018]Chua, K., Calandra, R., McAllister, R. and Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. _Advances in neural information processing systems_**31**.
* Dann et al. [2021]Dann, C., Mohri, M., Zhang, T. and Zimmert, J. (2021). A provably efficient model-free posterior sampling method for episodic reinforcement learning. _Advances in Neural Information Processing Systems_**34** 12040-12051.
* Du et al. [2021]Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W. and Wang, R. (2021). Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_. PMLR.
* Eysenbach et al. [2022]Eysenbach, B., Khazatsky, A., Levine, S. and Salakhutdinov, R. R. (2022). Mis-matched no more: Joint model-policy optimization for model-based rl. _Advances in Neural Information Processing Systems_**35** 23230-23243.
* Filar and Vrieze [2012]Filar, J. and Vrieze, K. (2012). _Competitive Markov decision processes_. Springer Science & Business Media.
* Foster et al. [2023]Foster, D. J., Golowich, N. and Han, Y. (2023). Tight guarantees for interactive decision making with the decision-estimation coefficient. _arXiv preprint arXiv:2301.08215_.

* Foster et al. [2022]Foster, D. J., Golowich, N., Qian, J., Rakhlin, A. and Sekhari, A. (2022). A note on model-free reinforcement learning with the decision-estimation coefficient. _arXiv preprint arXiv:2211.14250_.
* Foster et al. [2021]Foster, D. J., Kakade, S. M., Qian, J. and Rakhlin, A. (2021). The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_.
* Freedman [1975]Freedman, D. A. (1975). On tail probabilities for martingales. _the Annals of Probability_ 100-118.
* Fujimoto et al. [2018]Fujimoto, S., Hoof, H. and Meger, D. (2018). Addressing function approximation error in actor-critic methods. In _International conference on machine learning_. PMLR.
* Guo et al. [2022]Guo, H., Cai, Q., Zhang, Y., Yang, Z. and Wang, Z. (2022). Provably efficient offline reinforcement learning for partially observable markov decision processes. In _International Conference on Machine Learning_. PMLR.
* Haarnoja et al. [2018]Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (2018). Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _Proceedings of the 35th International Conference on machine learning (ICML-18)_.
* Haarnoja et al. [2018]Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_. PMLR.
* Houthooft et al. [2016]Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F. and Abbeel, P. (2016). Vime: Variational information maximizing exploration. _Advances in neural information processing systems_**29**.
* Huang et al. [2021]Huang, B., Lee, J. D., Wang, Z. and Yang, Z. (2021). Towards general function approximation in zero-sum markov games. _arXiv preprint arXiv:2107.14702_.
* Hung et al. [2021]Hung, Y.-H., Hsieh, P.-C., Liu, X. and Kumar, P. (2021). Reward-biased maximum likelihood estimation for linear stochastic bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 35.
* Janner et al. [2019]Janner, M., Fu, J., Zhang, M. and Levine, S. (2019). When to trust your model: Model-based policy optimization. _Advances in neural information processing systems_**32**.
* Jiang et al. [2017]Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E. (2017). Contextual decision processes with low Bellman rank are PAC-learnable. In _Proceedings of the 34th International Conference on Machine Learning_, vol. 70 of _Proceedings of Machine Learning Research_. PMLR.
* Jiang et al. [2017]Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_. PMLR.
* Jin et al. [2021]Jin, C., Liu, Q. and Miryoosefi, S. (2021). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in Neural Information Processing Systems_**34**.
* Jin et al. [2022]Jin, C., Liu, Q. and Yu, T. (2022). The power of exploiter: Provable multi-agent rl in large state spaces. In _International Conference on Machine Learning_. PMLR.
* Jin et al. [2020]Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_. PMLR.
* Jin et al. [2021]Jin, Y., Yang, Z. and Wang, Z. (2021). Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_. PMLR.
* Konda and Tsitsiklis [1999]Konda, V. and Tsitsiklis, J. (1999). Actor-critic algorithms. In _Advances in Neural Information Processing Systems_ (S. Solla, T. Leen and K. Muller, eds.), vol. 12. MIT Press.

* Kumar et al. [2020]Kumar, A., Zhou, A., Tucker, G. and Levine, S. (2020). Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_**33** 1179-1191.
* Kumar and Becker [1982]Kumar, P. and Becker, A. (1982). A new family of optimal adaptive controllers for markov chains. _IEEE Transactions on Automatic Control_**27** 137-146.
* Kurutach et al. [2018]Kurutach, T., Clavera, I., Duan, Y., Tamar, A. and Abbeel, P. (2018). Model-ensemble trust-region policy optimization. _arXiv preprint arXiv:1802.10592_.
* Lee et al. [2021]Lee, K., Laskin, M., Srinivas, A. and Abbeel, P. (2021). Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Liu et al. [2022]Liu, Q., Chung, A., Szepesvari, C. and Jin, C. (2022). When is partially observable reinforcement learning not scary? _arXiv preprint arXiv:2204.08967_.
* Liu et al. [2020]Liu, Q., Yu, T., Bai, Y. and Jin, C. (2020). A sharp analysis of model-based reinforcement learning with self-play. _arXiv preprint arXiv:2010.01604_.
* Liu et al. [2020]Liu, X., Hsieh, P.-C., Hung, Y. H., Bhattacharya, A. and Kumar, P. (2020). Exploration through reward biasing: Reward-biased maximum likelihood estimation for stochastic multi-armed bandits. In _International Conference on Machine Learning_. PMLR.
* Liu et al. [2022]Liu, Z., Lu, M., Wang, Z., Jordan, M. and Yang, Z. (2022). Welfare maximization in competitive equilibrium: Reinforcement learning for markov exchange economy. In _International Conference on Machine Learning_. PMLR.
* Liu et al. [2022]Liu, Z., Zhang, Y., Fu, Z., Yang, Z. and Wang, Z. (2022). Learning from demonstration: Provably efficient adversarial policy imitation with linear function approximation. In _International Conference on Machine Learning_. PMLR.
* Lopes et al. [2012]Lopes, M., Lang, T., Toussaint, M. and Oudeyer, P.-Y. (2012). Exploration in model-based reinforcement learning by empirically estimating learning progress. _Advances in neural information processing systems_**25**.
* Lu et al. [2022]Lu, M., Min, Y., Wang, Z. and Yang, Z. (2022). Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes. _arXiv preprint arXiv:2205.13589_.
* Lu and Van Roy [2017]Lu, X. and Van Roy, B. (2017). Ensemble sampling. _Advances in neural information processing systems_**30**.
* Mete et al. [2022]Mete, A., Singh, R. and Kumar, P. (2022). Augmented rbmle-ucb approach for adaptive control of linear quadratic systems. _Advances in Neural Information Processing Systems_**35** 9302-9314.
* Mete et al. [2022]Mete, A., Singh, R. and Kumar, P. (2022). The rbmle method for reinforcement learning. In _2022 56th Annual Conference on Information Sciences and Systems (CISS)_. IEEE.
* Mete et al. [2021]Mete, A., Singh, R., Liu, X. and Kumar, P. (2021). Reward biased maximum likelihood estimation for reinforcement learning. In _Learning for Dynamics and Control_. PMLR.
* Modi et al. [2020]Modi, A., Jiang, N., Tewari, A. and Singh, S. (2020). Sample complexity of reinforcement learning using linearly combined model ensembles. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Mohamed and Jimenez Rezende [2015]Mohamed, S. and Jimenez Rezende, D. (2015). Variational information maximisation for intrinsically motivated reinforcement learning. _Advances in neural information processing systems_**28**.
* Osband et al. [2016]Osband, I., Blundell, C., Pritzel, A. and Van Roy, B. (2016). Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_**29**.

* Pathak et al. [2017]Pathak, D., Agrawal, P., Efros, A. A. and Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. In _International conference on machine learning_. PMLR.
* Puterman [2014]Puterman, M. L. (2014). _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons.
* Rigter et al. [2022]Rigter, M., Lacerda, B. and Hawes, N. (2022). Rambo-rl: Robust adversarial model-based offline reinforcement learning. _arXiv preprint arXiv:2204.12581_.
* Russo and Van Roy [2013]Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_**26**.
* Stadie et al. [2015]Stadie, B. C., Levine, S. and Abbeel, P. (2015). Incentivizing exploration in reinforcement learning with deep predictive models. _arXiv preprint arXiv:1507.00814_.
* Sun et al. [2022]Sun, H., Han, L., Yang, R., Ma, X., Guo, J. and Zhou, B. (2022). Exploit reward shifting in value-based deep-rl: Optimistic curiosity-based exploration and conservative exploitation via linear reward shaping. _Advances in Neural Information Processing Systems_**35** 37719-37734.
* Sun et al. [2022]Sun, H., Xu, Z., Fang, M. and Zhou, B. (2022). Supervised q-learning can be a strong baseline for continuous control. In _NeurIPS 2022 Foundation Models for Decision Making Workshop_.
* Sun et al. [2019]Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A. and Langford, J. (2019). Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. In _Conference on learning theory_. PMLR.
* Sutton [1990]Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Machine learning proceedings 1990_. Elsevier, 216-224.
* Sutton and Barto [2018]Sutton, R. S. and Barto, A. G. (2018). _Reinforcement learning: An introduction_.
* Sutton et al. [1999]Sutton, R. S., McAllester, D., Singh, S. and Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_**12**.
* Todorov et al. [2012]Todorov, E., Erez, T. and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_. IEEE.
* Wainwright [2019]Wainwright, M. J. (2019). _High-dimensional statistics: A non-asymptotic viewpoint_, vol. 48. Cambridge university press.
* Wang et al. [2020]Wang, R., Salakhutdinov, R. R. and Yang, L. (2020). Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_**33** 6123-6135.
* Wang et al. [2019]Wang, Y., Wang, R., Du, S. S. and Krishnamurthy, A. (2019). Optimism in reinforcement learning with generalized linear function approximation. _arXiv preprint arXiv:1912.04136_.
* Wiering and Van Hasselt [2008]Wiering, M. A. and Van Hasselt, H. (2008). Ensemble algorithms in reinforcement learning. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_**38** 930-936.
* Wu et al. [2022]Wu, C., Li, T., Zhang, Z. and Yu, Y. (2022). Bayesian optimistic optimization: Optimistic exploration for model-based reinforcement learning. _Advances in Neural Information Processing Systems_**35** 14210-14223.
* Xie et al. [2020]Xie, Q., Chen, Y., Wang, Z. and Yang, Z. (2020). Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In _Conference on learning theory_. PMLR.

* [78]Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P. and Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_**34** 6683-6694.
* [79]Xie, T., Foster, D. J., Bai, Y., Jiang, N. and Kakade, S. M. (2022). The role of coverage in online reinforcement learning. _arXiv preprint arXiv:2210.04157_.
* [80]Xiong, W., Zhong, H., Shi, C., Shen, C. and Zhang, T. (2022). A self-play posterior sampling algorithm for zero-sum Markov games. In _Proceedings of the 39th International Conference on Machine Learning_, vol. 162 of _Proceedings of Machine Learning Research_. PMLR.
* [81]Yang, L. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_. PMLR.
* [82]Yang, Z., Jin, C., Wang, Z., Wang, M. and Jordan, M. (2020). Provably efficient reinforcement learning with kernel and neural function approximations. _Advances in Neural Information Processing Systems_**33** 13903-13916.
* [83]Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C. and Levine, S. (2020). Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on robot learning_. PMLR.
* [84]Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M. and Lazaric, A. (2020). Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* [85]Zanette, A., Lazaric, A., Kochenderfer, M. and Brunskill, E. (2020). Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_. PMLR.
* [86]Zha, D., Ma, W., Yuan, L., Hu, X. and Liu, J. (2021). Rank the episodes: A simple approach for exploration in procedurally-generated environments. _arXiv preprint arXiv:2101.08152_.
* [87]Zhang, T. (2022). Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_**4** 834-857.
* [88]Zhang, T. (2022). Mathematical analysis of machine learning algorithms.
* [89]Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z. and Zhang, T. (2022). A posterior sampling framework for interactive decision making. _arXiv preprint arXiv:2211.01962_.
* [90]Zhong, H. and Zhang, T. (2023). A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes. _arXiv preprint arXiv:2305.08841_.
* [91]Zhou, D., Gu, Q. and Szepesvari, C. (2021). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_. PMLR.