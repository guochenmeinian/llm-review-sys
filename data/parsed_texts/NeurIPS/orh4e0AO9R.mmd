# Bypassing the Simulator:

Near-Optimal Adversarial Linear Contextual Bandits

 Haolin Liu

University of Virginia

srs8rh@virginia.edu

The authors are listed in alphabetical order.

Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

Julian Zimmert

Google Research

zimmert@google.com

The authors are listed in alphabetical order.This work was done when Chen-Yu Wei was at MIT Institute for Data, Systems, and Society.

###### Abstract

We consider the adversarial linear contextual bandit problem, where the loss vectors are selected fully adversarially and the per-round action set (i.e. the context) is drawn from a fixed distribution. Existing methods for this problem either require access to a simulator to generate free i.i.d. contexts, achieve a sub-optimal regret no better than \(\widetilde{\mathcal{O}}(T^{5/6})\), or are computationally inefficient. We greatly improve these results by achieving a regret of \(\widetilde{\mathcal{O}}(\sqrt{T})\) without a simulator, while maintaining computational efficiency when the action set in each round is small. In the special case of sleeping bandits with adversarial loss and stochastic arm availability, our result answers affirmatively the open question by Saha et al. (2020) on whether there exists a polynomial-time algorithm with \(\mathrm{poly}(d)\sqrt{T}\) regret. Our approach naturally handles the case where the loss is linear up to an additive misspecification error, and our regret shows near-optimal dependence on the magnitude of the error.

## 1 Introduction

Contextual bandit is a widely used model for sequential decision making. The interaction between the learner and the environment proceeds in rounds: in each round, the environment provides a context; based on it, the learner chooses an action and receive a reward. The goal is to maximize the total reward across multiple rounds. This model has found extensive applications in fields such as medical treatment (Tewari and Murphy, 2017), personalized recommendations (Beygelzimer et al., 2011), and online advertising (Chu et al., 2011).

Algorithms for contextual bandits with provable guarantees have been developed under various assumptions. In the linear regime, the most extensively studied model is the _stochastic linear contextual bandit_, in which the context can be arbitrarily distributed in each round, while the reward is determined by a fixed linear function of the context-action pair. Near-optimal algorithms for this setting have been established in, e.g., (Chu et al., 2011; Abbasi-Yadkori et al., 2011; Li et al., 2019; Foster et al., 2020). Another model, which is the focus of this paper, is the _adversarial linear contextual bandit_, in which the context is drawn from a fixed distribution, while the reward is determined by a time-varying linear function of the context-action pair. 3 A computationally efficient algorithm for this setting is first proposed by Neu and Olkhovskaya (2020). However, existing research for this setting still faces challenges in achieving near-optimal regret and sample complexity when the context distribution is unknown.

The algorithm by Neu and Olkhovskaya (2020) requires the learner to have _full knowledge_ on the context distribution, and access to an _exploratory policy_ that induces a feature covariance matrix with a smallest eigenvalue at least \(\lambda\). Under these assumptions, their algorithm provides a regret guarantee of \(\widetilde{\mathcal{O}}(\sqrt{d\log(|\mathcal{A}|)T/\lambda})^{4}\), where \(d\) is the feature dimension, \(|\mathcal{A}|\) is the maximum size of the action set, and \(T\) is the number of rounds. These assumptions are relaxed in the work of Luo et al. (2021), who studied a more general linear MDP setting. When specialized to linear contextual bandits, Luo et al. (2021) only requires access to a _simulator_ from which the learner can draw free i.i.d. contexts. Their algorithm achieves a \(\widetilde{\mathcal{O}}((d\log(|\mathcal{A}|)T^{2})^{\nicefrac{{1}}{{3}}}))\) regret. The regret is further improved to the near-optimal one \(\widetilde{\mathcal{O}}(\sqrt{d\log(|\mathcal{A}|)T})\) by Dai et al. (2023) through refined loss estimator construction.

All results that attain \(\widetilde{\mathcal{O}}(T^{\nicefrac{{2}}{{3}}})\) or \(\widetilde{\mathcal{O}}(\sqrt{T})\) regret bound discussed above rely on access to the simulator. In their algorithms, the number of calls to the simulator significantly exceeds the number of interactions between the environment and the learner, but this is concealed from the regret bound. Therefore, their regret bounds do not accurately reflect the sample complexity of their algorithms. Another set of results for linear MDPs (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023; Kong et al., 2023) also consider the simulator-free scenario, essentially using interactions with the environment to fulfill the original purpose of the simulator. When applying their techniques to linear contextual bandits, their algorithms only achieve a regret bound of \(\widetilde{\mathcal{O}}(T^{\nicefrac{{5}}{{6}}})\) at best (see detailed analysis and comparison in Appendix G).

Our result significantly improves the previous ones: without simulators, we develop an algorithm that ensures a regret bound of order \(\widetilde{\mathcal{O}}(d^{2}\sqrt{T})\), and it is computationally efficient as long as the size of the action set is small in each round (similar to all previous work). Unlike previous algorithms which always collect new contexts (through simulators or interactions with the environment) to estimate the feature covariance matrix, we leverage the context samples the learner received in the past to do this. Although natural, establishing a near-tight regret requires highly efficient use of context samples, necessitating a novel way to construct the estimator of feature covariance matrix and a tighter concentration bound for it. Additionally, to address the potentially large magnitude and the bias of the loss estimator, we turn to the use of log-determinant (logdet) barrier in the follow-the-regularized-leader (FTRL) framework. Logdet accommodates larger loss estimators and induces a larger bonus term to cancel the bias of the loss estimator, both of which are crucial for our result.

Our setting subsumes sleeping bandits with stochastic arm availability (Kanade et al., 2009; Saha et al., 2020) and combinatorial semi-bandits with stochastic action sets (Neu and Valko, 2014). Our result answers affirmatively the main open question left by Saha et al. (2020) on whether there exists a polynomial-time algorithm with \(\mathrm{poly}(d)\sqrt{T}\) regret for sleeping bandits with adversarial loss and stochastic availability.

As a side result, we give a computationally inefficient algorithm that achieves an improved \(\widetilde{\mathcal{O}}(d\sqrt{T})\) regret without a simulator. While this is a direct extension from the EXP4 algorithm (Auer et al., 2002), such a result has not been established to our knowledge, so we include it for completeness.

### Related work

We review the literature of various contextual bandit problems, classifying them based on the nature of the context and the reward function, specifically whether they are stochastic/fixed or adversarial.

Contextual bandits with i.i.d. contexts and fixed reward functions (S-S)Significant progress has been made in contextual bandits with i.i.d. contexts and fixed reward functions, under general reward function classes or policy classes (Langford and Zhang, 2007; Dudik et al., 2011; Agarwal et al., 2012, 2014; Simchi-Levi and Xu, 2022; Xu and Zeevi, 2020). In the work by Dudik et al. (2011); Agarwal et al. (2012, 2014), the algorithms also use previously collected contexts to estimate the inverse probability of selecting actions under the current policy. However, these results only obtain regret bounds that polynomially depend on the number of actions. Furthermore, these results rely on having a fixed reward function, making their techniques not directly applicable to our case.

Contextual bandits with adversarial contexts and fixed reward functions (A-S)In this category, the most well-known results are in the linear setting (Chu et al., 2011; Abbasi-Yadkori et al., 2011; Zhao et al., 2023). Besides the linear case, previous work has investigated specific reward function classes (Russo and Van Roy, 2013; Li et al., 2022; Foster et al., 2018). Recently, Foster and Rakhlin (2020) introduced a general approach to deal with general function classes with a finite number of actions, which has since been improved or extended by Foster and Krishnamurthy (2021); Foster et al. (2021); Zhang (2022). This category of problems is not directly comparable to the setting studied in this paper, but both capture a certain degree of non-stationarity of the environment.

Contextual bandits with i.i.d. contexts and adversarial reward functions (S-A)This is the category which our work falls into. Several oracle efficient algorithms that require simulators have been proposed for general policy classes (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016). The oracle they use (i.e., the empirical risk minimization, or ERM oracle), however, is not generally implementable in an efficient manner. For the linear case, the first computationally efficient algorithm is by Neu and Olkhovskaya (2020), under the assumption that the context distribution is known. This is followed by Olkhovskaya et al. (2023) to obtain refined data-dependent bounds. A series of works (Neu and Olkhovskaya, 2021; Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023) apply similar techniques to linear MDPs, but when specialized to linear contextual bandits, they all assume known context distribution, or access to a simulator, or only achieves a regret no better than \(\widetilde{\mathcal{O}}(T^{\nicefrac{{5}}{{6}}})\). The work of Kong et al. (2023) also studies linear MDPs; when specialized to contextual bandits, they obtain a regret bound of \(\widetilde{\mathcal{O}}(T^{\nicefrac{{4}}{{5}}}+\mathrm{poly}(\frac{1}{ \lambda}))\) without a simulator but with a computationally inefficient algorithm and an undesired inverse dependence on the smallest eigenvalue of the covariance matrix. Related but simpler settings have also been studied. The sleeping bandit problem with stochastic arm availability and adversarial reward (Kleinberg et al., 2010; Kanade et al., 2009; Saha et al., 2020) is a special case of our problem where the context is always a subset of standard unit vectors. Another special case is the combinatorial semi-bandit problem with stochastic action sets and adversarial reward (Neu and Valko, 2014). While these are special cases, the regret bounds in these works are all worse than \(\widetilde{\mathcal{O}}(\mathrm{poly}(d)\sqrt{T})\). Therefore, our result also improves upon theirs. 5

Footnote 5: For combinatorial semi-bandit problems, our algorithm is not as computationally efficient as Neu and Valko (2014), which can handle exponentially large action sets.

Contextual bandits with adversarial contexts and adversarial reward functions (A-A)When both contexts and reward functions are adversarial, there are computational (Kanade and Steinke, 2014) and oracle-call (Hazan and Koren, 2016) lower bounds showing that no sublinear regret is achievable unless the computational cost scales polynomially with the size of the policy set. Even for the linear case, Neu and Olkhovskaya (2020) argued that the problem is at least as hard as online learning a one-dimensional threshold function, for which sublinear regret is impossible. For this

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline Target Setting & Algorithm & Regret & Simulator & Computation & Assumption \\ \hline General CB & Syrgkanis et al. (2016) & \((\log|\Pi|)^{\nicefrac{{1}}{{5}}}(|\mathcal{A}|T)^{2/3}\) & ✓ & \(\mathrm{poly}(|\mathcal{A}|,\log|\Pi|,T)\) & **ERM Oracle** \\ \hline \multirow{4}{*}{Linear MDP} & Dai et al. (2023) & \(\sqrt{dT\log|\mathcal{A}|}\) & ✓ & \(\mathrm{poly}(|\mathcal{A}|,d,T)\) & \\ \cline{2-6}  & Diá et al. (2023) & \(d(\log|\mathcal{A}|)^{\nicefrac{{1}}{{5}}}T^{\nicefrac{{5}}{{6}}}\) & & \(\mathrm{poly}(|\mathcal{A}|,d,T)\) & \\ \cline{2-6}  & Sherman et al. (2023) & \((d^{T}T^{4})^{\nicefrac{{1}}{{5}}}+\mathrm{poly}\left(\frac{1}{\lambda}\right)\) & & \(T^{d}\) & \(\exists\pi,\Sigma_{\pi}\succeq\lambda I\) \\ \cline{2-6}  & \multirow{2}{*}{Algorithm 1} & \(d^{2}\sqrt{T}\) & & \(\mathrm{poly}(|\mathcal{A}|,d,T)\) & \\ \cline{2-6}  & & Algorithm 2 & \(d\sqrt{T}\) & & \(T^{d}\) & \\ \hline Contextual SB & Neu and Valko (2014) & \((dT)^{\nicefrac{{2}}{{3}}}\) & & \(\mathrm{poly}(d,T)\) & \\ \hline Sleeping Bandit & Saha et al. (2020) & \(\sqrt{2^{d}T}\) & & \(\mathrm{poly}(d,T)\) (\(|\mathcal{A}|\leq d\)) & \\ \hline \end{tabular}
\end{table}
Table 1: Related works in the “S-A” category. CB stands for contextual bandits and SB stands for semi-bandits. The relations among settings are as follows: Sleeping Bandit \(\subset\) Contextual SB \(\subset\) Linear CB, Linear CB \(\subset\) Linear MDP, and Linear CB \(\subset\) General CB. The table compares our results with the Pareto frontier of the literature. For algorithms dealing more general settings, we have carefully translated their techniques to Linear CB and reported the resulting bounds. \(\Sigma_{\pi}\) denotes the feature covariance matrix induced by policy \(\pi\). \(|\mathcal{A}|\) and \(|\Pi|\) are sizes of the action set and the policy set.

challenging category, besides using the inefficient EXP4 algorithm, previous work makes stronger assumptions on the contexts (Syrgkanis et al., 2016) or resorts to alternative benchmarks such as dynamic regret (Luo et al., 2018; Chen et al., 2019) and approximate regret (Emamjomeh-Zadeh et al., 2021).

Lifting and exploration bonus for high-probability adversarial linear banditsOur technique is related to those obtaining high-probability bounds for linear bandits. Early development in this line of research only achieves computational efficiency when the action set size is small (Bartlett et al., 2008) or only applies to special action sets such as two-norm balls (Abernethy and Rakhlin, 2009). Recently, near-optimal high-probability bounds for general convex action sets have been obtained by lifting the problem to a higher dimensional one, which allows for a computationally efficient way to impose bonuses (Lee et al., 2020; Zimmert and Lattimore, 2022). The lifting and the bonus ideas we use are inspired by them, though for different purposes. However, due to the extra difficulty arising in the contextual case, currently we only obtain a computationally efficient algorithm when the action set size is small.

### Computational Complexity

Our main algorithm is based on log-determinant barrier optimization similar to Foster et al. (2020); Zimmert and Lattimore (2022). Computing its action distribution is closely related to computing the D-optimal experimental design (Khachiyan and Todd, 1990). Per step, this is shown to require \(\widetilde{\mathcal{O}}(|\mathcal{A}_{t}|\operatorname{poly}(d))\) computational and \(\widetilde{\mathcal{O}}(\log(|\mathcal{A}_{t}|)\operatorname{poly}(d))\) memory complexity (Foster et al., 2020, Proposition 1), where \(|\mathcal{A}_{t}|\) is the action set size at round \(t\). The computational bottleneck comes from (approximately) maximizing a quadratic function over the action set. It is an open question whether linear optimization oracles or other type of oracles can lead to efficient implementation of our algorithm for continuous action sets.

In the literature, there are few linear context bandit algorithms that provably avoid \(|\mathcal{A}|\) computation per round. The LinUCB algorithm (Chu et al., 2011; Abbasi-Yadkori et al., 2011) suffers from the same quadratic function maximization issue, and therefore is computationally comparable to our algorithm. The SquareCB.Lin algorithm by Foster et al. (2020) is based on the same log-determinant barrier optimization. Another recent algorithm by Zhang (2022) only admits an efficient implementation for continuous action sets in the Bayesian setting but not in the frequentist setting (though they provided an efficient heuristic implementation in their experiments). The Thompson sampling algorithm by Agrawal and Goyal (2013), which has efficient implementation, also relies on well-specified Gaussian prior. The only work that we know can avoids \(|\mathcal{A}|\) computation in the frequentist setting is Zhu et al. (2022), but their technique is only known to handle the A-S setting.

## 2 Preliminaries

We study the adversarial linear contextual bandit problem where the loss vectors are selected fully adversarially and the per-round action set (i.e. the context) is drawn from a fixed distribution. The learner and the environment interact in the following way. Let \(\mathbb{B}_{2}^{d}\) be the L2-norm unit ball in \(\mathbb{R}^{d}\).

For \(t=1,\cdots,T\),

1. The environment decides an adversarial loss vector \(y_{t}\in\mathbb{B}_{2}^{d}\), and generates a random action set (i.e., context) \(\mathcal{A}_{t}\subset\mathbb{B}_{2}^{d}\) from a fixed distribution \(D\) independent from anything else.
2. The learner observes \(\mathcal{A}_{t}\), and (randomly) chooses an action \(a_{t}\in\mathcal{A}_{t}\).
3. The learner receives the loss \(\ell_{t}\in[-1,1]\) with \(\mathbb{E}[\ell_{t}]=\langle a_{t},y_{t}\rangle\).

A policy \(\pi\) is a mapping which, given any action set \(\mathcal{A}\subset\mathbb{R}^{d}\), maps it to an element in the convex hull of \(\mathcal{A}\). We use \(\pi(\mathcal{A})\) to refer to the element that it maps \(\mathcal{A}\) to. The learner's _regret with respect to policy \(\pi\)_ is defined as the expected performance difference between the learner and policy \(\pi\):

\[\text{Reg}(\pi)=\mathbb{E}\left[\sum_{t=1}^{T}\langle a_{t},y_{t}\rangle-\sum_ {t=1}^{T}\langle\pi(\mathcal{A}_{t}),y_{t}\rangle\right]\]where the expectation is taken over all randomness from the environment (\(y_{t}\) and \(\mathcal{A}_{t}\)) and from the learner (\(a_{t}\)). The _pseudo-regret_ (or just _regret_) is defined as \(\text{Reg}=\max_{\pi}\text{Reg}(\pi)\), where the maximization is taken over all possible policies.

**Notations** For any matrix \(A\), we use \(\lambda_{\max}(A)\) and \(\lambda_{\min}(A)\) to denote the maximum and minimum eigenvalues of \(A\), respectively. We use \(\operatorname{Tr}(A)\) to denote the trace of matrix \(A\). For any action set \(\mathcal{A}\), let \(\Delta(\mathcal{A})\) be the space of probability measures on \(\mathcal{A}\). Let \(\mathcal{F}_{t}=\sigma(\mathcal{A}_{s},a_{s},\forall s\leq t)\) be the \(\sigma\)-algebra at round \(t\). Define \(\mathbb{E}_{t}[\cdot]=\mathbb{E}[\cdot|\mathcal{F}_{t-1}]\). Given a differentiable convex function \(F:\mathbb{R}^{d}\to\mathbb{R}\cup\{\infty\}\), the Bregman divergence with respect to \(F\) is defined as \(D_{F}(x,y)=F(x)-F(y)-\langle\nabla F(y),x-y\rangle\). Given a positive semi-definite (PSD) matrix \(A\), for any vector \(x\), define the norm generated by \(A\) as \(\|x\|_{A}=\sqrt{x^{\top}Ax}\). For any context \(\mathcal{A}\subset\mathbb{R}^{d}\) and \(p\in\Delta(\mathcal{A})\), define \(\mu(p)=\mathbb{E}_{a\sim p}[a]\) and \(\operatorname{Cov}(p)=\mathbb{E}_{a\sim p}[(a-\mu(p))(a-\mu(p))^{\top}]\). For any \(a\), define the lifted action \(\boldsymbol{a}=(a,1)^{\top}\) and the lifted covariance matrix \(\widehat{\operatorname{Cov}}(p)=\mathbb{E}_{a\sim p}[\boldsymbol{a}\boldsymbol {a}^{\top}]=\mathbb{E}_{a\sim p}\begin{bmatrix}aa^{\top}&a\\ a^{\top}&1\end{bmatrix}=\begin{bmatrix}\operatorname{Cov}(p)+\mu(p)\mu(p)^{ \top}&\mu(p)\\ \mu(p)^{\top}&1\end{bmatrix}\). We use **bold** matrices to denote matrices in the lifted space (e.g., in Algorithm 1 and Definition 1).

## 3 Follow-the-Regularized-Leader with the Log-Determinant Barrier

In this section, we present our main algorithm, Algorithm 1. This algorithm can be viewed as instantiating an individual Follow-The-Regularized-Leader (FTRL) algorithm on each action set (Line 2), with all FTRLs sharing the same loss vectors. This perspective has been taken by previous works Neu and Olkhovskaya (2020); Olkhovskaya et al. (2023) and simplifies the understanding of the problem. The rationale comes from the following calculation due to Neu and Olkhovskaya (2020): for any policy \(\pi\) that may depend on \(\mathcal{F}_{t-1}\),

\[\mathbb{E}_{t}\left[\langle\pi(\mathcal{A}_{t}),y_{t}\rangle\right]=\mathbb{E }_{\mathcal{A}_{t}}\left[\mathbb{E}_{y_{t}}\left[\langle\pi(\mathcal{A}_{t}),y _{t}\rangle\mid\mathcal{F}_{t-1}\right]\right]=\mathbb{E}_{\mathcal{A}_{0}} \left[\mathbb{E}_{y_{t}}\left[\langle\pi(\mathcal{A}_{0}),y_{t}\rangle\mid \mathcal{F}_{t-1}\right]\right]=\mathbb{E}_{t}\left[\langle\pi(\mathcal{A}_{0} ),y_{t}\rangle\right]\]

where \(\mathcal{A}_{0}\) is a sample drawn from \(D\) independent of all interaction history. This allows us to calculate the regret as

\[\mathbb{E}\left[\sum_{t=1}^{T}\langle\pi_{t}(\mathcal{A}_{t})-\pi(\mathcal{A} _{t}),y_{t}\rangle\right]=\mathbb{E}\left[\sum_{t=1}^{T}\langle\pi_{t}( \mathcal{A}_{0})-\pi(\mathcal{A}_{0}),y_{t}\rangle\right]\] (1)

where \(\pi_{t}\) is the policy used by the learner at time \(t\). Note that this view does not require the learner to simultaneously "run" an algorithm on every action set since the learner only needs to calculate the policy on \(\mathcal{A}\) whenever \(\mathcal{A}_{t}=\mathcal{A}\). In the regret analysis, in view of Eq. (1), it suffices to consider a single fixed action set \(\mathcal{A}_{0}\) drawn from \(D\) and bound the regret on it, even though the learner may never execute the policy on it. This \(\mathcal{A}_{0}\) is called a "ghost sample" in Neu and Olkhovskaya (2020).

### The lifting idea and the execution of Algorithm 1

Our algorithm is built on the logdet-FTRL algorithm developed by Zimmert and Lattimore (2022) for high-probability adversarial linear bandits, which lifts the original \(d\)-dimensional problem over the feature space to a \((d+1)\times(d+1)\) one over the covariance matrix space, with the regularizer being the negative log-determinant function. In our case, we instantiate an individual logdet-FTRL on each action set. The motivation behind Zimmert and Lattimore (2022) to lift the problem to the space of covariance matrix is that it casts the problem to one in the positive orthant, which allows for an easier way to construct the _bonus_ term that is crucial to compensate the variance of the losses, enabling a high-probability bound in their case. In our case, we use the same technique to introduce the bonus term, but the goal is to compensate the _bias_ resulting from the estimation error in the covariance matrix (see Section 3.4). This bias only appears in our contextual case but not in the linear bandit problem originally considered in Zimmert and Lattimore (2022).

As argued previously, we can focus on the learning problem over a fixed action set \(\mathcal{A}\), and our algorithm operates in the lifted space of covariance matrices \(\mathcal{H}^{\mathcal{A}}=\{\widehat{\operatorname{Cov}}(p):p\in\Delta( \mathcal{A})\}\subset\mathbb{R}^{(d+1)\times(d+1)}\). For this space, we define the lifted loss \(\gamma_{t}=\begin{bmatrix}0&\frac{1}{2}y_{t}\\ \frac{1}{2}y_{t}^{\top}&0\end{bmatrix}\in\mathbb{R}^{(d+1)\times(d+1)}\) so that \(\langle\widehat{\operatorname{Cov}}(p),\gamma_{t}\rangle=\mathbb{E}_{a\sim p }[a^{\top}y_{t}]=\langle\mu(p),y_{t}\rangle\) and thus the loss value in the lifted space (i.e., \(\langle\widehat{\operatorname{Cov}}(p),\gamma_{t}\rangle\)) is the same as that in the original space (i.e., \(\langle\mu(p),y_{t}\rangle\)).

In each round \(t\), the FTRL on \(\mathcal{A}\) outputs a lifted covariance matrix \(\bm{H}_{t}^{\mathcal{A}}\in\mathcal{H}^{\mathcal{A}}\) that corresponds to a probability distribution \(p_{t}^{\mathcal{A}}\in\Delta(\mathcal{A})\) such that \(\widehat{\mathrm{Cov}}(p_{t}^{\mathcal{A}})=\bm{H}_{t}^{\mathcal{A}}\) (Line 2 and Line 3). Upon receiving \(\mathcal{A}_{t}\), the learner samples an action from \(p_{t}^{\mathcal{A}_{t}}\) and the agent constructs the loss estimator \(\hat{y}_{t}\) (Line 5). Similarly to the construction of \(\gamma_{t}\), we define the lifted loss estimator \(\hat{\gamma}_{t}=\begin{bmatrix}0&\frac{1}{2}\hat{y}_{t}\\ \frac{1}{2}\hat{y}_{t}^{\top}&0\end{bmatrix}\) which makes \(\langle\widehat{\mathrm{Cov}}(p),\hat{\gamma}_{t}\rangle=\mathbb{E}_{\alpha \sim p}[a^{\top}\hat{y}_{t}]=\langle\mu(p),\hat{y}_{t}\rangle\). The lifted loss estimator, along with the _bonus_ term \(-\alpha_{t}\bm{\hat{\Sigma}_{t}}^{-1}\), is then fed to the FTRL on all \(\mathcal{A}\)'s. The purpose of the bonus term will be clear in Section 3.4.

In the rest of this section, we use the following notation in addition to those defined in Algorithm 1.

**Definition 1**.: _Define \(x_{t}^{\mathcal{A}}=\mathbb{E}_{a\sim p_{t}^{\mathcal{A}}}[a],\;x_{t}= \mathbb{E}_{\mathcal{A}\sim D}[x_{t}^{\mathcal{A}}],\;\;H_{t}^{\mathcal{A}}= \mathbb{E}_{a\sim p_{t}^{\mathcal{A}}}[(a-\hat{x}_{t})(a-\hat{x}_{t})^{\top}],\;H_{t}=\mathbb{E}_{\mathcal{A}\sim D}[H_{t}^{\mathcal{A}}]\). Let \(p_{t}^{\mathcal{A}}\in\Delta(\mathcal{A})\) be the action distribution used by the benchmark policy on \(\mathcal{A}\), and define \(u^{\mathcal{A}}=\mathbb{E}_{a\sim p_{t}^{\mathcal{A}}}[a],\;u=\mathbb{E}_{ \mathcal{A}\sim D}[u^{\mathcal{A}}],\;\bm{U}^{\mathcal{A}}=\mathbb{E}_{a\sim p _{t}^{\mathcal{A}}}[\bm{aa}^{\top}],\;\bm{U}=\mathbb{E}_{\mathcal{A}\sim D}[\bm {U}^{\mathcal{A}}]\). Notice that the \(x_{t}^{\mathcal{A}}\) and \(u^{\mathcal{A}}\) defined here is equivalent to the \(\pi_{t}(\mathcal{A})\) and \(\pi(\mathcal{A})\) in Eq.1, respectively._

### The construction of loss estimators and feature covariance matrix estimators

Our goal is to make \(\hat{y}_{t}\) in Line 5 an estimator of \(y_{t}\) with controllable bias and variance. If the context distribution is known (as in Neu and Olkhovskaya (2020)), then a standard unbiased estimator of \(y_{t}\) is

\[\hat{y}_{t}=\Sigma_{t}^{-1}a_{t}\ell_{t},\qquad\text{where}\quad\Sigma_{t}= \mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p_{t}^{\mathcal{A}}}\left[aa^{ \top}\right].\] (2)

To see its unbiasedness, notice that \(\mathbb{E}[a_{t}\ell_{t}]=\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p_{ t}^{\mathcal{A}}}[aa^{\top}y_{t}]\) and thus \(\mathbb{E}[\hat{y}_{t}]=y_{t}\). This \(\hat{y}_{t}\), however, can have a variance that is inversely related to the smallest eigenvalue of the covariance matrix \(\hat{\Sigma}_{t}\), which can be unbounded in the worst case. This is the main reason why Neu and Olkhovskaya (2020) does not achieve the optimal bound, and requires the bias-variance-tradeoff techniques in Dai et al. (2023) to close the gap. When the context distribution is unknown but the learner has access to a simulator (Liu et al., 2021; Dai et al., 2023; Sherman et al., 2023; Kong et al., 2023), the learner can draw free contexts to estimate the covariance matrix \(\hat{\Sigma}_{t}\) up to a very high accuracy without interacting with the environment, making the problem close to the case of known context distribution.

Challenges arise when the learner has no knowledge about the context distribution and there is no simulator. In this case, there are two natural ways to estimate the covariance matrix under the current policy. One is to draw new samples from the environment, treating the environment like a simulator. This approach is essentially taken by all previous work studying linear models in the "S-A" category. However, this is very expensive, and it causes the simulator-equipped bound \(\sqrt{T}\) in Dai et al. (2023)to deteriorate to the simulator-free bound \(T^{\nicefrac{{5}}{{\varepsilon}}}\) at best (see Appendix G for details). The other is to use the contexts received in time \(1\) to \(t\) to estimate the covariance matrix under the policy at time \(t\). This demands a very high efficiency in reusing the contexts samples, and existing ways of constructing the covariance matrix and the accompanied analysis by Dai et al. (2023); Sherman et al. (2023) are insufficient to achieve the near-optimal bound even with context reuse. This necessitates our tighter construction of the covariance matrix estimator and tighter concentration bounds for it.

Our construction of the loss estimator (Line 5) is

\[\hat{y}_{t}=\hat{\Sigma}_{t}^{-1}(a_{t}-\hat{x}_{t})\ell_{t}\qquad\text{where} \quad\hat{\Sigma}_{t}=\mathbb{E}_{\mathcal{A}\sim\hat{D}_{t}}\mathbb{E}_{a \sim p_{t}^{\mathcal{A}}}\left[(a-\hat{x}_{t})(a-\hat{x}_{t})^{\top}\right]+ \beta_{t}I\] (3)

where \(\hat{D}_{t}=\text{Uniform}\{\mathcal{A}_{1},\mathcal{A}_{2},\dots,\mathcal{A} _{t-1}\}\), \(\hat{x}_{t}=\mathbb{E}_{\mathcal{A}\sim\hat{D}_{t}},\mathbb{E}_{a\sim p_{t}^{ \mathcal{A}}}[a]\), and \(\beta_{t}=\widetilde{\mathcal{O}}(d^{3}/t)\). Comparing Eq. (3) with Eq. (2), we see that besides using the empirical context distribution \(\hat{D}_{t}\) in place of the ground truth \(D\) and adding a small term \(\beta_{t}I\) to control the smallest eigenvalue of the covariance matrix, we also centralize the features by \(\hat{x}_{t}\), an estimation of the mean features under the current policy. The centralization is important in making the bias \(y_{t}-\hat{y}_{t}\) appear in a nice form that can be compensated by a bonus term. The estimator might seem problematic on first sight, because \(p_{t}^{\mathcal{A}}\) is strongly dependent on \(\hat{D}_{t}\), which rules out canonical concentration bounds. We circumvent this issue by leveraging the special structure of \(p_{t}\) in Algorithm 1, which allows for a union bound over a sufficient covering of all potential policies (Appendix C.3). The analysis on the bias of this loss estimator is also non-standard, which is the key to achieve the near-optimal bound. In the next two subsections, we explain how to bound the _bias_ of this loss estimator (Section 3.3), and how the _bonus_ term can be used to compensate the bias (Section 3.4).

### The bias of the loss estimator

Since the true loss vector is \(y_{t}\) and we use the loss estimator \(\hat{y}_{t}\) in the update, there is a bias term emerging in the regret bound at time \(t\):

\[\mathbb{E}_{t}\left[\langle x_{t}^{\mathcal{A}_{0}}-u^{\mathcal{A}_{0}},y_{t}- \hat{y}_{t}\rangle\right]=\mathbb{E}_{t}\left[\langle x_{t}-u,y_{t}-\hat{y}_{t }\rangle\right]=\mathbb{E}_{t}\left[(x_{t}-u)^{\top}\left(I-\hat{\Sigma}_{t}^ {-1}(a_{t}-\hat{x}_{t})a_{t}^{\top}\right)y_{t}\right]\]

where definitions of \(x_{t}^{\mathcal{A}},u^{\mathcal{A}},x_{t},u\) can be found in Definition 1, and we use the definition of \(\hat{y}_{t}\) in Eq. (3) in the last equality. Now taking expectation over \(\mathcal{A}_{t}\) and \(a_{t}\) conditioned on \(\mathcal{F}_{t-1}\), we can further bound the expectation in the last expression by

\[(x_{t}-u)^{\top}\left(I-\hat{\Sigma}_{t}^{-1}H_{t}\right)y_{t}-(x _{t}-u)^{\top}\hat{\Sigma}_{t}^{-1}\left(x_{t}-\hat{x}_{t}\right)\hat{x}_{t}^{ \top}y_{t}\] \[\leq\|x_{t}-u\|_{\Sigma_{t}^{-1}}\|(\hat{\Sigma}_{t}-H_{t})y_{t} \|_{\hat{\Sigma}_{t}^{-1}}+\|x_{t}-u\|_{\Sigma_{t}^{-1}}\|x_{t}-\hat{x}_{t}\|_{ \hat{\Sigma}_{t}^{-1}}\] (4)

(see Definition 1 for the definition of \(H_{t}\)). The two terms \(\|(\hat{\Sigma}_{t}-H_{t})y_{t}\|_{\Sigma_{t}^{-1}}\) and \(\|x_{t}-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}\) in Eq. (4) are related to the error between the empirical context distribution \(\hat{D}_{t}=\text{Uniform}\{\mathcal{A}_{1},\dots,\mathcal{A}_{t-1}\}\) and the true distribution \(D\). We handle them through novel analysis and bound both of them by \(\widetilde{\mathcal{O}}\big{(}\sqrt{d^{3}/t}\big{)}\). See Lemma 13, Lemma 14, Lemma 18, and Lemma 19 for details. The techniques we use in these lemmas surpass those in Dai et al. (2023); Sherman et al. (2023). As a comparison, a similar term as \(\|(\hat{\Sigma}_{t}-H_{t})y_{t}\|_{\hat{\Sigma}_{t}^{-1}}\) is also presented in Eq. (16) of Dai et al. (2023) and Lemma B.5 of Sherman et al. (2023) when bounding the bias. While their analysis uses off-the-shelf matrix concentration inequalities, our analysis expands this expression by its definition, and applies concentration inequalities for _scalars_ on individual entries. Overall, our analysis is more tailored for this specific expression. Previous works ensure that this term can be bounded by \(\mathcal{O}(\sqrt{\beta})\) after collecting \(\mathcal{O}(\beta^{-2})\) new samples (Lemma 5.1 of Dai et al. (2023) and Lemma B.1 of Sherman et al. (2023)), we are able to bound it by \(\mathcal{O}(1/\sqrt{t})\) only using \(t\) samples that the learner received up to time \(t\). This essentially improves their \(\mathcal{O}(\beta^{-2})\) sample complexity bound to \(\mathcal{O}(\beta^{-1})\). See Appendix G for detailed comparison with Dai et al. (2023) and Sherman et al. (2023).

Now we have bounded the regret due to bias of \(\hat{y}_{t}\) by the order of \(\sqrt{d^{3}/t}\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}\). The next problem is how to mitigate this term. This is also a problem in previous work (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023), and it has become clear that this can be handled by incorporating _bonus_ in the algorithm.

### The bonus term

To handle a bias term in the form of \(\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}\), we resort to the idea of _bonus_. To illustrate this, suppose that instead of feeding \(\hat{y}_{t}\) to the FTRLs, we feed \(\hat{y}_{t}-b_{t}\) for some \(b_{t}\). Then this would give us a regret bound of the following form:

\[\text{Reg} =\mathbb{E}\left[\sum_{t=1}^{T}\langle x_{t}-u,\hat{y}_{t}-b_{t} \rangle\right]+\mathbb{E}\left[\sum_{t=1}^{T}\langle x_{t}-u,y_{t}-\hat{y}_{t} \rangle\right]+\mathbb{E}\left[\sum_{t=1}^{T}\langle x_{t}-u,b_{t}\rangle\right]\] \[\lesssim\widetilde{\mathcal{O}}(d^{2}\sqrt{T})+\mathbb{E}\left[ \sum_{t=1}^{T}\sqrt{\frac{d^{3}}{t}}\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}\right] +\mathbb{E}\left[\sum_{t=1}^{T}\langle x_{t}-u,b_{t}\rangle\right]\] (5)

where we assume that FTRL can give us \(\widetilde{\mathcal{O}}(d^{2}\sqrt{T})\) bound for the loss sequence \(\hat{y}_{t}-b_{t}\). Our hope here is to design a \(b_{t}\) such that \(\langle x_{t}-u,b_{t}\rangle\) provides a negative term that can be used to cancel the bias term \(\sqrt{d^{3}/t}\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}\) in the following manner:

\[\text{bias}+\text{bonus}=\sum_{t=1}^{T}\left(\sqrt{\frac{d^{3}}{t}}\|x_{t}-u \|_{\hat{\Sigma}_{t}^{-1}}+\langle x_{t}-u,b_{t}\rangle\right)\lesssim \widetilde{\mathcal{O}}(d^{2}\sqrt{T}).\] (6)

which gives us a \(\widetilde{\mathcal{O}}(d^{2}\sqrt{T})\) overall regret by 5. This approach relies on two conditions to be satisfied. First, we have to find a \(b_{t}\) that makes 6 hold. Second, we have to ensure that the FTRL algorithm achieves a \(\widetilde{\mathcal{O}}(d^{2}\sqrt{T})\) bound under the loss sequence \(\hat{y}_{t}-b_{t}\).

To meet the first condition, we take inspiration from Zimmert and Lattimore (2022) and lift the problem to the space of covariance matrix in \(\mathbb{R}^{(d+1)\times(d+1)}\). Considering the bonus term \(\alpha_{t}\hat{\bm{\Sigma}}_{t}^{-1}\) in the lifted space, we have

\[\langle\bm{H}_{t}-\bm{U},\alpha_{t}\hat{\bm{\Sigma}}_{t}^{-1}\rangle=\alpha_{ t}\operatorname{Tr}(\bm{H}_{t}\hat{\bm{\Sigma}}_{t}^{-1})-\alpha_{t} \operatorname{Tr}(\bm{U}\hat{\bm{\Sigma}}_{t}^{-1})\] (7)

Using 17 and Corollary 22, we can upper bound 7 by \(\mathcal{O}\left(d\alpha_{t}\right)-\frac{\alpha_{t}}{4}\|u-\hat{x}_{t}\|_{ \hat{\Sigma}_{t}^{-1}}^{2}\). This gives

\[\text{bias}+\text{bonus} \leq\sum_{t=1}^{T}\left(\sqrt{\frac{d^{3}}{t}}\|x_{t}-u\|_{\hat{ \Sigma}_{t}^{-1}}+d\alpha_{t}-\frac{\alpha_{t}}{4}\|\hat{x}_{t}-u\|_{\hat{ \Sigma}_{t}^{-1}}^{2}\right)\] \[\leq\widetilde{\mathcal{O}}(d^{2}\sqrt{T})+\sum_{t=1}^{T}\sqrt{ \frac{d^{3}}{t}}\|x_{t}-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}+\sum_{t=1}^{T} \left(\sqrt{\frac{d^{3}}{t}}\|\hat{x}_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}-\frac{ \alpha_{t}}{4}\|\hat{x}_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}^{2}\right).\]

Using 18 to bound the second term above by \(\widetilde{\mathcal{O}}(\sum_{t}d^{3}/t)=\widetilde{\mathcal{O}}(d^{3})\), and AM-GM to bound the third term by \(\widetilde{\mathcal{O}}(\sum_{t}d^{3}/(t\alpha_{t}))=\widetilde{\mathcal{O}}(d ^{2}\sqrt{T})\), we get 6, through the help of lifting.

To meet the second condition, we have to analyze the regret of FTRL under the loss \(\hat{y}_{t}-b_{t}\). The key is to show that the bonus \(\alpha_{t}\hat{\bm{\Sigma}}_{t}^{-1}\) introduces small _stability term_ overhead. Thanks to the use of the logdet regularizer and its self-concordance property, the extra stability term introduced by the bonus can indeed be controlled by the order \(\sqrt{T}\). The key analysis is in 27.

Previous works rely on exponential weights (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023) rather than logdet-FTRL, which comes with the following drawbacks. 1) In Luo et al. (2021), Sherman et al. (2023) where exponential weights is combined with standard loss estimators, the bonus introduces large stability term overhead. Therefore, their bound can only be \(T^{\nicefrac{{2}}{{3}}}\) at best even with simulators. 2) In Dai et al. (2023) where exponential weights is combined with magnitude-reduced loss estimators, the loss estimator for action \(a\) can no longer be represented as a simple linear function \(a^{\top}\hat{y}_{t}\). Instead, it becomes a complex non-linear function. This restricts the algorithm's potential to leverage linear optimization oracle over the action set and achieve computational efficiency.

### Overall regret analysis

With all the algorithmic elements discussed above, now we give a formal statement for our regret guarantee and perform a complete regret analysis. Our main theorem is the following.

**Theorem 2**.: _Algorithm 1 ensures \(\text{\rm Reg}\leq\mathcal{O}(d^{2}\sqrt{T}\log T)\)._

Proof sketch.: Let \(\mathcal{A}_{0}\) be drawn from \(D\) independently from all the interaction history between the learner and the environment. Recalling the definitions in Definition1, we have

\[\text{\rm Reg}=\mathbb{E}\left[\sum_{t=1}^{T}\langle a_{t}-u^{A_{ t}},y_{t}\rangle\right]=\mathbb{E}\left[\sum_{t=1}^{T}\langle\bm{H}_{t}^{ \mathcal{A}_{t}}-\bm{U}^{\mathcal{A}_{t}},\gamma_{t}\rangle\right]=\mathbb{E} \left[\sum_{t=1}^{T}\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0 }},\gamma_{t}\rangle\right]\] \[\leq\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\langle\bm{H}_{t}^ {\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t}\rangle \right]}_{\text{\bf Bias}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\langle \bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\alpha_{t}\bm{\hat{ \Sigma}}_{t}^{-1}\rangle\right]}_{\text{\bf Bonus}}+\underbrace{\mathbb{E} \left[\sum_{t=1}^{T}\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0 }},\hat{\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\rangle\right]}_{ \text{\bf FTRL-Reg}}\]

Each term can be bounded as follows:

* \(\text{\bf Bias}\leq\mathcal{O}(d^{2}\sqrt{T}\log T)+\frac{1}{4}\sum_{t=1}^{T} \alpha_{t}\|u-x_{t}\|_{\Sigma_{t}^{-1}}^{2}\) (discussed in Section3.3).
* \(\text{\bf Bonus}\leq\mathcal{O}(d^{2}\sqrt{T}\log T)-\frac{1}{4}\sum_{t=1}^{T} \alpha_{t}\|u-x_{t}\|_{\Sigma_{t}^{-1}}^{2}\) (discussed in Section3.4).
* \(\text{\bf FTRL-Reg}\leq\mathcal{O}(d^{2}\sqrt{T}\log T)\).

Combining all terms gives the desired bound. The complete proof is provided in AppendixD. 

### Handling Misspecification

In this subsection, we show how our approach naturally handles the case when the expectation of the loss cannot be exactly realized by a linear function but with a misspecification error. In this case, we assume that the expectation of the loss is given by \(\mathbb{E}[\ell_{t}|a_{t}=a]=f_{t}(a)\) for some \(f_{t}:\mathbb{R}^{d}\rightarrow[-1,1]\), and the realized loss \(\ell_{t}\) still lies in \([-1,1]\). We define the following notion of misspecification (slightly more refined than that in Neu and Olkhovskaya (2020)):

**Assumption 1** (misspecification).: \(\sqrt{\frac{1}{T}\sum_{t=1}^{T}\inf_{y\in\mathbb{B}_{2}^{d}}\sup_{\mathcal{A }\in\operatorname{supp}(D)}\sup_{a\in\mathcal{A}}(f_{t}(a)-\langle a,y\rangle)^ {2}}\leq\varepsilon\)_._

Based on previous discussions, the design idea of Algorithm1 is to 1) identify the bias of the loss estimator, and 2) add necessary bonus to compensate the bias. When there is misspecification, this design idea still applies. The difference is that now the loss estimator \(\hat{y}_{t}\) potentially has more bias due to misspecification. Therefore, the bias becomes larger by an amount related to \(\varepsilon\). Consequently, we need to enlarge bonus (raising \(\alpha_{t}\)) to compensate it. Due to the larger bonus, we further need to tune down the learning rate \(\eta_{t}\) to make the algorithm stable. Overall, to handle misspecification, when \(\varepsilon\) is known, it boils down to using the same algorithm (Algorithm1) with adjusted \(\alpha_{t}\) and \(\eta_{t}\). The case of unknown \(\varepsilon\) can be handled by the standard meta-learning technique _Corral_(Agarwal et al., 2017; Foster et al., 2020; Luo et al., 2022). We defer all details to AppendixE and only state the final bound here.

**Theorem 3**.: _Under misspecification, there is an algorithm ensuring \(\text{\rm Reg}\leq\widetilde{\mathcal{O}}(d^{2}\sqrt{T}+\sqrt{d}\varepsilon T)\), without knowing \(\varepsilon\) in advance._

## 4 Linear EXP4

To tighten the \(d\)-dependence in the regret bound, we can use the computationally inefficient algorithm EXP4(Auer et al., 2002). The original regret bound for EXP4 has a polynomial dependence on the number of actions, but here we take the advantage of the linear structure to show a bound that only depends on the feature dimension \(d\). The algorithm is presented in Algorithm2.

To run Algorithm 2, we restrict ourselves to a finite policy class. The policy class we use in the algorithm is the set of linear policies defined as

\[\Pi=\left\{\pi_{\theta}:\ \theta\in\Theta,\ \ \pi_{\theta}(\mathcal{A})= \operatorname*{argmin}_{a\in\mathcal{A}}a^{\top}\theta\right\}\] (8)

where \(\Theta\) is an \(1\)-net of \([-T,T]^{d}\). The next theorem shows that this suffices to give us near-optimal bounds for our problem. The proof is given in Appendix F.

**Theorem 4**.: _With \(\gamma=2d\sqrt{(\log T)/T}\) and \(\eta=\sqrt{(\log T)/T}\), Algorithm 2 with the policy class defined in Eq. (8) guarantees \(\operatorname{Reg}=\mathcal{O}\left(d\sqrt{T\log T}\right)\)._

Note that this result technically also holds in the "A-A" category with respect to the policy class defined in Eq. (8). However, this policy class is _not_ necessarily a sufficient cover of all policies of interest when the contexts and losses are adversarial.

## 5 Conclusions

We derived the first algorithm that obtains \(\sqrt{T}\) regret in contextual linear bandits with stochastic action sets in the absence of a simulator or prior knowledge on the distribution. As a side result, we obtained the first computationally efficient \(\operatorname{poly}(d)\sqrt{T}\) algorithm for adversarial sleeping bandits with general stochastic arm availabilities. We believe the techniques in this paper will be useful for improving results for simulator-free linear MDPs as well.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24:2312-2320, 2011.
* Abernethy and Rakhlin (2009) Jacob Abernethy and Alexander Rakhlin. Beating the adaptive bandit with high probability. In _2009 Information Theory and Applications Workshop_, pages 280-289. IEEE, 2009.
* Abernethy et al. (2009) Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In _Conference on Learning Theory_, 2009.
* Agarwal et al. (2012) Alekh Agarwal, Miroslav Dudik, Satyen Kale, John Langford, and Robert Schapire. Contextual bandit learning with predictable rewards. In _Artificial Intelligence and Statistics_, pages 19-26. PMLR, 2012.
* Agarwal et al. (2014) Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_, pages 1638-1646. PMLR, 2014.
* Agarwal et al. (2017) Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algorithms. In _Conference on Learning Theory_, pages 12-38. PMLR, 2017.
* Agarwal et al. (2018)* Agrawal and Goyal (2013) Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In _International conference on machine learning_, pages 127-135. PMLR, 2013.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* Bartlett et al. (2008) Peter L Bartlett, Varsha Dani, Thomas P Hayes, Sham M Kakade, Alexander Rakhlin, and Ambuj Tewari. High-probability regret bounds for bandit online linear optimization. In _COLT_, pages 335-342, 2008.
* Beygelzimer et al. (2011) Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 19-26. JMLR Workshop and Conference Proceedings, 2011.
* Chen et al. (2019) Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal and parameter-free. In _Conference on Learning Theory_, pages 696-726. PMLR, 2019.
* Chu et al. (2011) Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214. JMLR Workshop and Conference Proceedings, 2011.
* Dai et al. (2023) Yan Dai, Haipeng Luo, Chen-Yu Wei, and Julian Zimmert. Refined regret for adversarial mdps with linear function approximation. In _International Conference on Machine Learning_, 2023.
* Dann et al. (2023a) Christoph Dann, Chen-Yu Wei, and Julian Zimmert. Best of both worlds policy optimization. In _International Conference on Machine Learning_, 2023a.
* Dann et al. (2023b) Christoph Dann, Chen-Yu Wei, and Julian Zimmert. A blackbox approach to best of both worlds in bandits and beyond. In _Conference on Learning Theory_, 2023b.
* Dudik et al. (2011) M Dudik, D Hsu, S Kale, N Karampatziakis, J Langford, L Reyzin, and T Zhang. Efficient optimal learning for contextual bandits. In _Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, UAI 2011_, page 169, 2011.
* Emamjomeh-Zadeh et al. (2021) Ehsan Emamjomeh-Zadeh, Chen-Yu Wei, Haipeng Luo, and David Kempe. Adversarial online learning with changing action sets: Efficient algorithms with approximate regret bounds. In _Algorithmic Learning Theory_, pages 599-618. PMLR, 2021.
* Foster and Rakhlin (2020) Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 3199-3210. PMLR, 2020.
* Foster et al. (2018) Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 1539-1548. PMLR, 2018.
* Foster et al. (2021) Dylan Foster, Alexander Rakhlin, David Simchi-Levi, and Yunzong Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. In _Conference on Learning Theory_, pages 2059-2059. PMLR, 2021.
* Foster and Krishnamurthy (2021) Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. In _Advances in Neural Information Processing Systems_, 2021.
* Foster et al. (2020) Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. _Advances in Neural Information Processing Systems_, 33:11478-11489, 2020.
* Hazan and Koren (2016) Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_, pages 128-141, 2016.
* Hazan et al. (2017)Tiancheng Jin, Junyan Liu, Chloe Rouyer, William Chang, Chen-Yu Wei, and Haipeng Luo. No regret online reinforcement learning with adversarial losses and transitions. _arXiv preprint arXiv:2305.17380_, 2023.
* Kanade and Steinke (2014) Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. _ACM Transactions on Computation Theory (TOCT)_, 6(3):1-16, 2014.
* Kanade et al. (2009) Varun Kanade, H Brendan McMahan, and Brent Bryan. Sleeping experts and bandits with stochastic action availability and adversarial rewards. In _Artificial Intelligence and Statistics_, pages 272-279. PMLR, 2009.
* Khachiyan and Todd (1990) Leonid G Khachiyan and Michael J Todd. On the complexity of approximating the maximal inscribed ellipsoid for a polytope. Technical report, Cornell University Operations Research and Industrial Engineering, 1990.
* Kleinberg et al. (2010) Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping experts and bandits. _Machine learning_, 80(2-3):245-272, 2010.
* Kong et al. (2023) Fang Kong, Xiangcheng Zhang, Baoxiang Wang, and Shuai Li. Improved regret bounds for linear adversarial mdps via linear optimization. _arXiv preprint arXiv:2302.06834_, 2023.
* Langford and Zhang (2007) John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. _Advances in neural information processing systems_, 20(1):96-1, 2007.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Lee et al. (2020) Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang. Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. In _Advances in Neural Information Processing Systems_, 2020.
* Li et al. (2022) Gene Li, Pritish Kamath, Dylan J Foster, and Nati Srebro. Understanding the eluder dimension. _Advances in Neural Information Processing Systems_, 35:23737-23750, 2022.
* Li et al. (2019) Yingkai Li, Yining Wang, and Yuan Zhou. Nearly minimax-optimal regret for linearly parameterized bandits. In _Conference on Learning Theory_, pages 2173-2174. PMLR, 2019.
* Lu and Shiou (2002) Tzon-Tzer Lu and Sheng-Hua Shiou. Inverses of 2\(\times\) 2 block matrices. _Computers & Mathematics with Applications_, 43(1-2):119-129, 2002.
* Luo et al. (2018) Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in non-stationary worlds. In _Conference On Learning Theory_, pages 1739-1776. PMLR, 2018.
* Luo et al. (2021) Haipeng Luo, Chen-Yu Wei, and Chung-Wei Lee. Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. _Advances in Neural Information Processing Systems_, 34:22931-22942, 2021.
* Luo et al. (2022) Haipeng Luo, Mengxiao Zhang, Peng Zhao, and Zhi-Hua Zhou. Corralling a larger band of bandits: A case study on switching regret for linear bandits. In _Conference on Learning Theory_, pages 3635-3684. PMLR, 2022.
* Nemirovski (2004) Arkadi Nemirovski. Interior point polynomial time methods in convex programming. _Lecture notes_, 42(16):3215-3224, 2004.
* Neu and Olkhovskaya (2020) Gergely Neu and Julia Olkhovskaya. Efficient and robust algorithms for adversarial linear contextual bandits. In _Conference on Learning Theory_, pages 3049-3068. PMLR, 2020.
* Neu and Olkhovskaya (2021) Gergely Neu and Julia Olkhovskaya. Online learning in mdps with linear function approximation and bandit feedback. _Advances in Neural Information Processing Systems_, 34:10407-10417, 2021.
* Neu and Valko (2014) Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision sets and adversarial losses. _Advances in Neural Information Processing Systems_, 27, 2014.
* Olkhovskaya et al. (2023) Julia Olkhovskaya, Jack Mayo, Tim van Erven, Gergely Neu, and Chen-Yu Wei. First-and second-order bounds for adversarial linear contextual bandits. _arXiv preprint arXiv:2305.00832_, 2023.
* O'Hagan et al. (2018)Alexander Rakhlin and Karthik Sridharan. Bistro: An efficient relaxation-based method for contextual bandits. In _International Conference on Machine Learning_, pages 1977-1985. PMLR, 2016.
* Russo and Van Roy (2013) Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* Saha et al. (2020) Aadirupa Saha, Pierre Gaillard, and Michal Valko. Improved sleeping bandits with stochastic action sets and adversarial rewards. In _International Conference on Machine Learning_, pages 8357-8366. PMLR, 2020.
* Sherman et al. (2023) Uri Sherman, Tomer Koren, and Yishay Mansour. Improved regret for efficient online reinforcement learning with linear function approximation. In _International Conference on Machine Learning_, 2023.
* Simchi-Levi and Xu (2022) David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. _Mathematics of Operations Research_, 47(3):1904-1931, 2022.
* Syrgkanis et al. (2016a) Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. Efficient algorithms for adversarial contextual learning. In _International Conference on Machine Learning_, pages 2159-2168. PMLR, 2016a.
* Syrgkanis et al. (2016b) Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, and Robert E Schapire. Improved regret bounds for oracle-based adversarial contextual bandits. _Advances in Neural Information Processing Systems_, 29, 2016b.
* Tewari and Murphy (2017) Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. _Mobile Health: Sensors, Analytic Methods, and Applications_, pages 495-517, 2017.
* Wei and Luo (2018) Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In _Conference on Learning Theory_, pages 1263-1291. PMLR, 2018.
* Wei et al. (2022) Chen-Yu Wei, Christoph Dann, and Julian Zimmert. A model selection approach for corruption robust reinforcement learning. In _International Conference on Algorithmic Learning Theory_, pages 1043-1096. PMLR, 2022.
* Xu and Zeevi (2020) Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for contextual bandits. _arXiv preprint arXiv:2007.07876_, 2020.
* Zhang (2022) Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022.
* Zhao et al. (2023) Heyang Zhao, Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency. In _Conference on Learning Theory_, 2023.
* Zhu et al. (2022) Yinglun Zhu, Dylan J Foster, John Langford, and Paul Mineiro. Contextual bandits with large action spaces: Made practical. In _International Conference on Machine Learning_, pages 27428-27453. PMLR, 2022.
* Zimmert and Lattimore (2022) Julian Zimmert and Tor Lattimore. Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits. In _Conference on Learning Theory_, pages 3285-3312. PMLR, 2022.
* Zimmert et al. (2022) Julian Zimmert, Naman Agarwal, and Satyen Kale. Pushing the efficiency-regret pareto frontier for online learning of portfolios and quantum states. In _Conference on Learning Theory_, pages 182-226. PMLR, 2022.

## Appendix A Summary of Notation

* Auxiliary Lemmas
* Concentration Inequalities
* C.1 General Concentration Inequalities
* C.2 Concentration Inequalities under a Fixed Policy \(p\)
* C.3 Union Bound over Policies
* Regret Analysis
* D.1 Bounding the Bias term
* D.2 Bounding the Bonus term
* D.3 Bounding the Penalty term
* D.4 Bounding the Stability-1 term
* D.5 Bounding the Stability-2 term
* D.6 Bounding the Error term
* D.7 Finishing up
* Handling Misspecification
* E.1 Known misspecification
* E.2 Unknown misspecification
* Analysis for Linear EXP4
* Comparison with Dai et al. (2023) and Sherman et al. (2023)
* G.1 Regret Analysis Sketch
Summary of Notation

We summarize the notations that have been defined in Algorithm 1 and Definition 1.

\[\beta_{t} =\Theta\left(\frac{(d+1)^{3}\log(T/\delta)}{t-1}\right)\] \[\hat{x}_{t} =\frac{1}{t-1}\sum_{\tau=1}^{t-1}\mathbb{E}_{a\sim p_{t}^{A_{\tau} }}[a]\] \[\hat{H}_{t} =\frac{1}{t-1}\sum_{\tau=1}^{t-1}\mathbb{E}_{a\sim p_{t}^{A_{\tau} }}\left[(a-\hat{x}_{t})(a-\hat{x}_{t})^{\top}\right]\] \[\boldsymbol{\hat{H}}_{t} =\frac{1}{t-1}\sum_{\tau=1}^{t-1}\mathbb{E}_{a\sim p_{t}^{A_{\tau} }}\begin{bmatrix}aa^{\top}&a\\ a^{\top}&1\end{bmatrix}=\begin{bmatrix}\hat{H}_{t}+\hat{x}_{t}\hat{x}_{t}^{\top }&\hat{x}_{t}\\ \hat{x}_{t}^{\top}&1\end{bmatrix}\] \[\hat{\Sigma}_{t} =\hat{H}_{t}+\beta_{t}I\] \[\boldsymbol{\hat{\Sigma}}_{t} =\boldsymbol{\hat{H}}_{t}+\beta_{t}\boldsymbol{I}=\begin{bmatrix} \hat{\Sigma}_{t}+\hat{x}_{t}\hat{x}_{t}^{\top}&\hat{x}_{t}\\ \hat{x}_{t}^{\top}&1+\beta_{t}\end{bmatrix}\] \[x_{t} =\mathbb{E}_{\mathcal{A}\sim\mathcal{D}}\mathbb{E}_{a\sim p_{t}^{ A}}[a]\] \[H_{t} =\mathbb{E}_{\mathcal{A}\sim\mathcal{D}}\mathbb{E}_{a\sim p_{t}^{ A}}\left[(a-\hat{x}_{t})(a-\hat{x}_{t})^{\top}\right]\] \[\boldsymbol{H}_{t} =\mathbb{E}_{\mathcal{A}\sim\mathcal{D}}\mathbb{E}_{a\sim p_{t}^ {A}}\begin{bmatrix}aa^{\top}&a\\ a^{\top}&1\end{bmatrix}\]

## Appendix B Auxiliary Lemmas

**Lemma 5** (FTRL regret bound, Lemma 18 of Dann et al. (2023a)).: _Let \(\Omega\subset\mathbb{R}^{d}\) be a convex set, \(g_{1},\ldots,g_{T}\in\mathbb{R}^{d}\), and \(\eta_{1},\ldots,\eta_{T}>0\). Then the FTRL update_

\[w_{t}=\operatorname*{argmin}_{w\in\Omega}\left\{\left\langle w,\sum_{\tau=1}^ {t-1}g_{\tau}\right\rangle+\frac{1}{\eta_{t}}\psi(w)\right\}\]

_ensures for any \(u\in\Omega\) and \(\eta_{0}>0\),_

\[\sum_{t=1}^{T} \langle w_{t}-u,g_{t}\rangle\] \[\leq\underbrace{\frac{\psi(u)-\min_{w\in\Omega}\psi(w)}{\eta_{0}} +\sum_{t=1}^{T}(\psi(u)-\psi(w_{t}))\left(\frac{1}{\eta_{t}}-\frac{1}{\eta_{t -1}}\right)}_{\text{Penalty}}+\underbrace{\sum_{t=1}^{T}\left(\max_{w\in \Omega}\langle w_{t}-w,g_{t}\rangle-\frac{D_{\psi}(w,w_{t})}{\eta_{t}}\right)} _{\text{Stability}}.\]

_When \(\eta_{0},\eta_{1},\ldots,\eta_{T}\) is non-increasing, the penalty term can further be upper bounded by_

\[\text{Penalty}\leq\frac{\psi(u)-\min_{w\in\Omega}\psi(w)}{\eta_{T}}.\]

**Lemma 6** (Bernstein's inequality).: _Let \(X_{1},\cdots,X_{n}\) be iid random variables; let \(\mathbb{E}[X]\) be the expectation and \(\operatorname{Var}(X)\) be the variance of these random variables. If for any \(i\), \(|X_{i}-\mathbb{E}[X_{i}]|\leq R\), then with probability of at least \(1-\delta\),_

\[\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mathbb{E}[X]\right|\leq\sqrt{\frac{4 \operatorname{Var}(X)\log\frac{2}{\delta}}{n}}+\frac{4R\log\frac{2}{\delta}}{3 n}.\]

**Lemma 7** (Hoeffding's inequality).: _Let \(X_{1},\cdots,X_{n}\) be iid random variables; let \(a\leq X_{i}\leq b\) and let \(\mathbb{E}[X]\) be the expectation. Then with probability of at least \(1-\delta\),_

\[\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mathbb{E}[X]\right|\leq(b-a)\sqrt{\frac{1 }{2n}\log(\frac{2}{\delta})}\]

Given \(F(X)=-\log\det(X)\), \(D^{2}F(X)=X^{-1}\otimes X^{-1}\) where \(\otimes\) is the Kronecker product. For any matrix \(A=\begin{bmatrix}a_{1}&a_{2}&\cdots&a_{n}\end{bmatrix}\), let \(\text{vec}(A)=\begin{bmatrix}a_{1}\\ \vdots\\ a_{n}\end{bmatrix}\) which vectorizes matrix \(A\) to a column vector by stacking the columns \(A\). The second order directional derivative for \(F\) is \(D^{2}F(X)[A,A]=\text{vec}(A)^{T}\left(X^{-1}\otimes X^{-1}\right)\text{vec}(A )=\operatorname{Tr}(A^{\top}X^{-1}AX^{-1})\). We define \(\|A\|_{\nabla^{2}F(X)}=\sqrt{\operatorname{Tr}(A^{\top}X^{-1}AX^{-1})}\) and \(\|A\|_{\nabla^{-2}F(X)}=\sqrt{\operatorname{Tr}(A^{\top}XAX)}\). It is a pseudo-norm and more discussion can be found in Appendix D of Zimmert et al. (2022). In the following analysis, we will only use one property of this pseudo-norm shown below which is similar to the Holder inequality.

**Lemma 8**.: _For any two symmetric matrices \(A,B\) and positive definite matrix \(X\),_

\[\langle A,B\rangle\leq\|A\|_{\nabla^{2}F(X)}\|B\|_{\nabla^{-2}F(X)}\]

Proof.: Since \((X\otimes X)^{-1}=X^{-1}\otimes X^{-1}\), from Holder inequality, we have

\[\langle A,B\rangle=\langle\text{vec}(A),\text{vec}(B)\rangle\leq\|\text{vec}( A)\|_{X^{-1}\otimes X^{-1}}\|\text{vec}(B)\|_{(X^{-1}\otimes X^{-1})^{-1}}=\|A\|_{ \nabla^{2}F(X)}\|B\|_{\nabla^{-2}F(X)}\]

## Appendix C Concentration Inequalities

The goal of this section is to show Lemma 18 and Lemma 19, which are key to bound the bias term. We first introduce a useful lemma from Dai et al. (2023), which will be used later to prove our concentration bounds.

### General Concentration Inequalities

**Lemma 9** (Lemma A.4 in Dai et al. (2023)).: _Let \(H_{1},H_{2},\ldots,H_{n}\) be i.i.d. PSD matrices such that \(\mathbb{E}[H_{i}]=H\), \(H_{i}\preceq I\) almost surely and \(H\succeq\frac{1}{dn}\log\frac{d}{\delta}I\). Then with probability \(1-\delta\),_

\[\frac{1}{n}\sum_{i=1}^{n}H_{i}-H\succeq-\sqrt{\frac{d}{n}\log\frac{d}{\delta} }H^{1/2}\]

**Corollary 10**.: _Let \(H_{1},H_{2},\ldots,H_{n}\) be i.i.d. PSD matrices such that \(\mathbb{E}[H_{i}]=H\) and \(H_{i}\preceq cI\) almost surely for some positive constant \(c\). Let \(\hat{H}=\frac{1}{n}\sum_{i=1}^{n}H_{i}\), then with probability \(1-\delta\),_

\[\hat{H}+\frac{3c}{2}\cdot\frac{d}{n}\log\left(\frac{d}{\delta}\right)I \succeq\frac{1}{2}H\] (9)

Proof.: A simple corollary of Lemma 9 under the condition of Lemma 9 is that

\[\frac{1}{n}\sum_{i=1}^{n}H_{i}-H\succeq-\sqrt{\frac{d}{n}\log \frac{d}{\delta}}H^{1/2}\succeq-\frac{1}{2}H-\frac{d}{2n}\log\left(\frac{d}{ \delta}\right)I\] \[\Rightarrow \frac{1}{n}\sum_{i=1}^{n}H_{i}+\frac{d}{2n}\log\left(\frac{d}{ \delta}\right)I\succeq\frac{1}{2}H,\] (10)

where we use that \(H^{\frac{1}{2}}\preceq\frac{k}{2}H+\frac{1}{2k}\) for any \(k>0\).

Now consider the condition of this corollary. We first consider the case where \(\frac{d}{n}\log(\frac{d}{\delta})\leq 1\). In this case, we apply Eq.10 with \(H_{i}^{\prime}=\frac{1}{2c}H_{i}+\frac{d}{2n}\log(\frac{d}{\delta})I\), which satisfies the condition for Eq.10 to hold. This gives

\[\frac{1}{n}\sum_{i=1}^{n}\left(\frac{1}{2c}H_{i}+\frac{d}{2n}\log \left(\frac{d}{\delta}\right)I\right)+\frac{d}{2n}\log\left(\frac{d}{\delta} \right)I\succeq\frac{1}{2}\left(\frac{1}{2c}H+\frac{d}{2n}\log\left(\frac{d} {\delta}\right)I\right)\] \[\Rightarrow\ \hat{H}+\frac{3c}{2}\cdot\frac{d}{n}\log\left(\frac{d}{ \delta}\right)I\succeq\frac{1}{2}H\]

with probability at least \(1-\delta\). When \(\frac{d}{n}\log(\frac{d}{\delta})>1\). Eq.9 is trivial because \(\frac{1}{2}H\preceq\frac{c}{2}I\preceq\frac{c}{2}\cdot\frac{d}{n}\log(\frac{d} {\delta})I\).

### Concentration Inequalities under a Fixed Policy \(p\)

In this subsection, we establish concentration bounds for a _fixed_ policy \(p\) (with \(p^{\mathcal{A}}\in\Delta(\mathcal{A})\) denoting the action distribution it uses over \(\mathcal{A}\)) over i.i.d. contexts. The results in this subsection are preparation for AppendixC.3 where we take union bounds over policies.

The setting and notation to be used in this subsection are defined in Definition11.

**Definition 11**.: _Let \(\{\mathcal{A}_{1},\ldots,\mathcal{A}_{n}\}\) be i.i.d. context samples drawn from \(D\). Let \(\hat{D}\) be the uniform distribution over \(\{\mathcal{A}_{1},\ldots,\mathcal{A}_{n}\}\). Over this set of context samples, define for any policy \(p\),_

\[x(p) =\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{\mathcal{A}}} [a],\] \[\hat{x}(p) =\mathbb{E}_{\mathcal{A}\sim\hat{D}}\mathbb{E}_{a\sim p^{\mathcal{ A}}}[a],\] \[H(p) =\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{\mathcal{A}}} \left[(a-\hat{x}(p))(a-\hat{x}(p))^{\top}\right],\] \[\hat{H}(p) =\mathbb{E}_{\mathcal{A}\sim\hat{D}}\mathbb{E}_{a\sim p^{ \mathcal{A}}}\left[(a-\hat{x}(p))(a-\hat{x}(p))^{\top}\right],\] \[\boldsymbol{H}(p) =\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{\mathcal{A}} }\left[\boldsymbol{aa}^{\top}\right],\] \[\boldsymbol{\hat{H}}(p) =\mathbb{E}_{\mathcal{A}\sim\hat{D}}\mathbb{E}_{a\sim p^{ \mathcal{A}}}\left[\boldsymbol{aa}^{\top}\right],\] \[\hat{\Sigma}(p) =\hat{H}(p)+\beta I,\] \[\boldsymbol{\hat{\Sigma}}(p) =\boldsymbol{\hat{H}}(p)+\beta\boldsymbol{I},\]

_where \(\beta=\frac{5d\log(6d/\delta)}{n}\)._

**Lemma 12**.: _Under the setting of Definition11, for any fixed \(p\), with probability at least \(1-\delta\),_

\[\hat{H}(p)+\frac{4d\log(6d/\delta)}{n}I \succeq\frac{1}{2}H(p),\] \[\boldsymbol{\hat{H}}(p)+\frac{3d\log(d/\delta)}{n}\boldsymbol{I} \succeq\frac{1}{2}\boldsymbol{H}(p).\]

Proof.: In this proof, we use \(\hat{x},x,\hat{H},H,\boldsymbol{\hat{H}},\boldsymbol{H}\) to denote \(\hat{x}(p),x(p),\hat{H}(p),H(p),\boldsymbol{\hat{H}}(p),\boldsymbol{H}(p)\) since \(p\) is fixed throughout the proof.

Since \(\|a\|\leq 1\), \(\boldsymbol{H}\preceq 2I\) and \(\boldsymbol{\hat{H}}\preceq 2I\). Thus, we can directly apply Corollary10 with \(c=2\) to get with probability \(1-\frac{\delta}{3}\)

\[\boldsymbol{\hat{H}}+\frac{3d\log(3d/\delta)}{n}\boldsymbol{I} \succeq\frac{1}{2}\boldsymbol{H}.\]

[MISSING_PAGE_EMPTY:18]

with some \(\rho=\Theta\left(\frac{d\log(d/\delta)}{n}\right)\), where the second inequality is by Eq. (14). Thus,

\[\|x-\hat{x}\|_{\Sigma^{-1}}^{2} =(x-\hat{x})^{\top}\hat{\Sigma}^{-1}(x-\hat{x})\] \[\leq(x-\hat{x})^{\top}\left(\frac{1}{2}V\Lambda V^{\top}+\rho I \right)^{-1}(x-\hat{x})\] \[=(\hat{x}-x)^{\top}V\left(\frac{1}{2}\Lambda+\rho I\right)^{-1}V^ {\top}(\hat{x}-x).\]

Define

\[\Delta_{k}=\mathrm{e}_{k}^{\top}V^{\top}(\hat{x}-x) =\frac{1}{n}\sum_{i=1}^{n}\underbrace{\mathrm{e}_{k}^{\top}V^{ \top}\mathbb{E}_{a\sim p^{d}\mathcal{A}_{i}}[a]}_{\text{Define as }Z_{k}^{(i)}}- \underbrace{\mathrm{e}_{k}^{\top}V^{\top}\mathbb{E}_{\mathcal{A}\sim D} \mathbb{E}_{a\sim p^{d}}[a]}_{\text{Define as }Z_{k}}\]

Since \(\mathbb{E}_{\mathcal{A}_{i}\sim D}\left[Z_{k}^{(i)}\right]=Z_{k}\), by Bernstein's inequality, with probability at least \(1-\delta\), we have

\[|\Delta_{k}|\leq\mathcal{O}\left(\sqrt{\frac{\mathrm{Var}(Z_{k}^{(i)})\log(d/ \delta)}{n}}+\frac{\log(d/\delta)}{n}\right)\] (15)

for all \(k\), where

\[\mathrm{Var}(Z_{k}^{(i)})=\mathbb{E}_{\mathcal{A}\sim D}\left[\mathrm{e}_{k}^ {\top}V^{\top}\mathbb{E}_{a\sim p^{d}}[a]-\mathrm{e}_{k}^{\top}V^{\top}x)^{2} \right].\]

On the other hand,

\[\Lambda_{kk} =\mathrm{e}_{k}^{\top}\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a \sim p^{d}}[V^{\top}(a-x)(a-x)^{\top}V]\mathrm{e}_{k}\] \[=\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{d}}\left[ \left(\mathrm{e}_{k}^{\top}V^{\top}a-\mathrm{e}_{k}^{\top}V^{\top}x\right)^{2 }\right].\]

From Jensen's inequality,

\[\Lambda_{kk}=\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{d}}\left[ \left(\mathrm{e}_{k}^{\top}V^{\top}a-\mathrm{e}_{k}^{\top}V^{\top}x\right)^{2 }\right]\geq\mathbb{E}_{\mathcal{A}\sim D}\left[\left(\mathrm{e}_{k}^{\top}V^{ \top}\mathbb{E}_{a\sim p^{d}}[a]-\mathrm{e}_{k}^{\top}V^{\top}x\right)^{2} \right]=\mathrm{Var}(Z_{k}^{(i)})\]

Thus,

\[\|x-\hat{x}\|_{\hat{\Sigma}^{-1}}^{2} \leq(\hat{x}-x)^{\top}V\left(\frac{1}{2}\Lambda+\rho I\right)^{-1 }V^{\top}(\hat{x}-x)\] \[=\sum_{k=1}^{d}\frac{(\Delta_{k})^{2}}{\frac{1}{2}\Lambda_{kk}+\rho}\] \[\leq\mathcal{O}\left(\frac{\log(d/\delta)}{n}\sum_{k=1}^{d}\frac{ \mathrm{Var}(Z_{k}^{(i)})+\frac{\log(d/\delta)}{n}}{\Lambda_{kk}+\rho}\right)\] (by Eq. ( 15 ) \[\leq\mathcal{O}\left(\frac{d\log(d/\delta)}{n}\right).\] ( \[\Lambda_{kk}\geq\mathrm{Var}(Z_{k}^{(i)})\]  and \[\rho=\Theta(\tfrac{d\log(d/\delta)}{n})\] )

**Lemma 14**.: _Under the setting of Definition 11, for any fixed policy \(p\), with probability at least \(1-\mathcal{O}(\delta)\),_

\[\|(\hat{\Sigma}(p)-H(p))y\|_{\hat{\Sigma}(p)^{-1}}^{2}\leq\mathcal{O}\left( \frac{d\log(d/\delta)}{n}\right)\]

_for any \(y\in\mathbb{B}_{2}^{d}\)._

Proof.: In this proof, we use \(\hat{x},x,\hat{H},H,\hat{\bm{H}},\hat{\Sigma},\hat{\bm{\Sigma}}\) to denote \(\hat{x}(p)\), \(x(p)\), \(\hat{H}(p)\), \(H(p)\), \(\hat{\bm{H}}(p)\), \(\hat{\bm{H}}(p)\), \(\hat{\Sigma}(p)\), \(\hat{\bm{\Sigma}}(p)\) since \(p\) is fixed throughout the proof.

[MISSING_PAGE_EMPTY:20]

By Bernstein's inequality, with probability at least \(1-\delta\), we have

\[|\Delta_{kh}|=\left|\frac{1}{n}\sum_{i=1}^{n}(\lambda_{ikh}-\Lambda_{kh})\right| \leq\mathcal{O}\left(\sqrt{\frac{\operatorname{Var}(\lambda_{ikh})\log(d/ \delta)}{n}}+\frac{\log(d/\delta)}{n}\right).\] (20)

With the manipulations and notations above, we continue to bound Eq.18 by

\[\|\Gamma y\|_{\Sigma^{-1}}^{2} =y^{\top}\Delta(V^{\top}\hat{\Sigma}V)^{-1}\Delta y^{\prime}\] (let \[y^{\prime}=V^{\top}y\] ) \[\leq 2y^{\prime\top}\Delta\left(\Lambda+\rho I\right)^{-1}\Delta y ^{\prime}\] (by Eq.19) \[\leq 2\operatorname{Tr}\left(\Delta\left(\Lambda+\rho I\right)^{- 1}\Delta\right)\]

By direct expansion and the fact that \(\Lambda\) is diagonal,

\[\operatorname{Tr}\left(\Delta\left(\Lambda+\rho I\right)^{-1} \Delta\right) =\sum_{k=1}^{d}\left(\Delta\left(\Lambda+\rho I\right)^{-1}\Delta \right)_{kk}\] \[=\sum_{k=1}^{d}\sum_{h=1}^{d}\frac{\Delta_{kh}\Delta_{hk}}{ \Lambda_{hh}+\rho}\] \[\leq\mathcal{O}\left(\sum_{k=1}^{d}\sum_{h=1}^{d}\frac{1}{\Lambda _{hh}+\rho}\left(\frac{\operatorname{Var}(\lambda_{ikh})\log(d/\delta)}{n}+ \frac{\log^{2}(d/\delta)}{n^{2}}\right)\right)\] (by Eq.20) \[\leq\mathcal{O}\left(\sum_{k=1}^{d}\sum_{h=1}^{d}\frac{1}{\Lambda _{hh}+\rho}\frac{\mathbb{E}(\lambda_{ikh}^{2})\log(d/\delta)}{n}+\frac{d^{2} \log^{2}(d/\delta)}{\rho n^{2}}\right)\] (21)

By definition,

\[\lambda_{ikh}=\mathbb{E}_{a\sim p^{\mathcal{A}_{i}}}\left[\mathrm{e}_{k}V^{ \top}(a-x)(a-x)^{\top}V\mathrm{e}_{h}\right]\]

and thus

\[\sum_{k=1}^{d}\lambda_{ikh}^{2} \leq\mathbb{E}_{a\sim p^{\mathcal{A}_{i}}}\left[\sum_{k=1}^{d} \left(\mathrm{e}_{k}V^{\top}(a-x)(a-x)^{\top}V\mathrm{e}_{h}\right)^{2}\right]\] \[=\mathbb{E}_{a\sim p^{\mathcal{A}_{i}}}\left[\mathrm{e}_{h}^{\top }V^{\top}(a-x)(a-x)^{\top}V\mathrm{e}_{k}\mathrm{e}_{k}^{\top}V^{\top}(a-x)(a -x)^{\top}V\mathrm{e}_{h}\right]\] \[=\mathbb{E}_{a\sim p^{\mathcal{A}_{i}}}\left[\mathrm{e}_{h}^{\top }V^{\top}(a-x)(a-x)^{\top}(a-x)(a-x)^{\top}V\mathrm{e}_{h}\right]\] \[\leq\mathbb{E}_{a\sim p^{\mathcal{A}_{i}}}\left[\mathrm{e}_{h}^{ \top}V^{\top}(a-x)(a-x)^{\top}V\mathrm{e}_{h}\right]\] \[=\lambda_{ihh}\]

and \(\sum_{k=1}^{d}\mathbb{E}[\lambda_{ikh}^{2}]\leq\mathbb{E}[\lambda_{ihh}]= \Lambda_{hh}\). Continuing from Eq.21 and using that \(\rho=\Theta\left(\frac{d\log(d/\delta)}{n}\right)\),

\[\operatorname{Tr}\left(\Delta(\Lambda+\rho I)^{-1}\Delta\right)\leq\mathcal{O }\left(\sum_{h=1}^{d}\frac{\Lambda_{hh}\log(d/\delta)}{(\Lambda_{hh}+\rho)n}+ \frac{d^{2}\log^{2}(d/\delta)}{n^{2}}\right)\leq\mathcal{O}\left(\frac{d\log( d/\delta)}{n}\right).\]

This gives a bound on \(\|\Gamma y\|_{\Sigma^{-1}}^{2}\) and finishes the proof after combining Eq.17.

### Union Bound over Policies

In Lemma12, Lemma13, and Lemma14, we have obtained the desired concentration inequalities _under a fixed policy \(p\)_. In this subsection, we proceed to take union bound over _all policies_ that are possibly used by Algorithm1.

The set of policies that could be generated by Algorithm 1 is the following:

\[\mathbf{P}=\left\{p:\;\widehat{\mathrm{Cov}}(p^{\mathcal{A}})=\operatorname*{ argmin}_{\boldsymbol{H}\in\mathcal{H}^{\mathcal{A}}}\left\{\langle\boldsymbol{H}, \boldsymbol{Z}\rangle+F(\boldsymbol{H})\right\},\text{for}\;\boldsymbol{Z}\in \mathcal{Z}\right\}\]

where \(\mathcal{Z}=[-T^{2},T^{2}]^{(d+1)\times(d+1)}\cap\mathbb{S}\) with \(\mathbb{S}\) denoting the set of symmetric matrices. To see this, notice that Algorithm 1 at round \(t\) corresponds to the policy defined above with \(\boldsymbol{Z}=\eta_{t}\sum_{s=1}^{t-1}(\hat{\gamma}_{s}-\alpha_{s}\hat{ \boldsymbol{\Sigma}}_{s}^{-1})\).

Our goal is to construct a \(\epsilon\)-cover \(\mathbf{P}^{\prime}\) so that every policy \(p\in\mathbf{P}\) can find a policy \(p^{\prime}\in\mathbf{P}^{\prime}\) making \(-\epsilon I\preceq\widehat{\mathrm{Cov}}(p^{\mathcal{A}})-\widehat{\mathrm{ Cov}}(p^{\prime\mathcal{A}})\preceq\epsilon I\) on _every_ action set \(\mathcal{A}\). The size of such a cover is bounded in the Proposition below.

**Lemma 15**.: _There exists an \(\epsilon\)-cover \(\mathbf{P}^{\prime}\) of \(\mathbf{P}\) with size \(\log|\mathbf{P}^{\prime}|=\mathcal{O}\left(d^{2}\log\frac{d}{\epsilon}\right)\) such that for any \(p\in\mathbf{P}\), there exists an \(p^{\prime}\in\mathbf{P}^{\prime}\) satisfying_

\[\left\|\widehat{\mathrm{Cov}}(p^{\mathcal{A}})-\widehat{\mathrm{Cov}}(p^{ \prime\mathcal{A}})\right\|_{F}\leq\epsilon\]

_for all \(\mathcal{A}\)._

Proof.: It is straightforward to construct an \(\frac{\epsilon}{4}\)-cover \(\mathcal{C}\) for \(\mathcal{Z}=[-T^{2},T^{2}]^{(d+1)\times(d+1)}\cap\mathbb{S}\) in Frobenius norm with size \(|\mathcal{C}|=(\frac{24(d+1)^{2}}{\epsilon})^{(d+1)^{2}}\) (Exercise 27.6 of Lattimore and Szepesvari (2020)). Now define \(\mathbf{P}^{\prime}\) as

\[\mathbf{P}^{\prime}=\left\{p:\;\widehat{\mathrm{Cov}}(p^{\mathcal{A}})= \operatorname*{argmin}_{\boldsymbol{H}\in\mathcal{H}^{\mathcal{A}}}\left\{ \langle\boldsymbol{H},\boldsymbol{Z}\rangle+F(\boldsymbol{H})\right\},\text{ for}\;\boldsymbol{Z}\in\mathcal{C}\right\}\] (22)

Below, we show that this is a \(\epsilon\)-cover for \(\mathbf{P}\).

Consider two policies \(p_{1}\) and \(p_{2}\) defined as the following:

\[\widehat{\mathrm{Cov}}(p_{1}^{\mathcal{A}}) =\operatorname*{argmin}_{\boldsymbol{H}\in\mathcal{H}^{\mathcal{A}} }\left\{\langle\boldsymbol{H},\boldsymbol{Z}_{1}\rangle+F(\boldsymbol{H})\right\}\] \[\widehat{\mathrm{Cov}}(p_{2}^{\mathcal{A}}) =\operatorname*{argmin}_{\boldsymbol{H}\in\mathcal{H}^{\mathcal{A}} }\left\{\langle\boldsymbol{H},\boldsymbol{Z}_{2}\rangle+F(\boldsymbol{H})\right\}\]

with \(\|\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}\|_{F}\leq\frac{\epsilon}{4}\). Consider an arbitrary \(\mathcal{A}\) and define \(\boldsymbol{H}_{1}=\widehat{\mathrm{Cov}}(p_{1}^{\mathcal{A}})\), \(\boldsymbol{H}_{2}=\widehat{\mathrm{Cov}}(p_{2}^{\mathcal{A}})\). Below we show \(\|\boldsymbol{H}_{1}-\boldsymbol{H}_{2}\|_{F}\leq\epsilon\).

Since \(F(\boldsymbol{H})\) is convex for \(\boldsymbol{H}\), from the first-order optimality condition for convex function, we have

\[\langle\boldsymbol{H}_{1},\boldsymbol{Z}_{1}\rangle+F(\boldsymbol {H}_{1}) \leq\langle\boldsymbol{H}_{2},\boldsymbol{Z}_{1}\rangle+F( \boldsymbol{H}_{2})-D_{F}(\boldsymbol{H}_{2},\boldsymbol{H}_{1})\] \[=\langle\boldsymbol{H}_{2},\boldsymbol{Z}_{2}\rangle+\langle \boldsymbol{H}_{2},\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}\rangle+F( \boldsymbol{H}_{2})-D_{F}(\boldsymbol{H}_{2},\boldsymbol{H}_{1})\] \[\langle\boldsymbol{H}_{2},\boldsymbol{Z}_{2}\rangle+F( \boldsymbol{H}_{2}) \leq\langle\boldsymbol{H}_{1},\boldsymbol{Z}_{2}\rangle+F( \boldsymbol{H}_{1})-D_{F}(\boldsymbol{H}_{1},\boldsymbol{H}_{2})\] \[=\langle\boldsymbol{H}_{1},\boldsymbol{Z}_{1}\rangle+\langle \boldsymbol{H}_{1},\boldsymbol{Z}_{2}-\boldsymbol{Z}_{1}\rangle+F( \boldsymbol{H}_{1})-D_{F}(\boldsymbol{H}_{1},\boldsymbol{H}_{2})\]

Adding up these the two inequalities, we get

\[2\min\{D_{F}(\boldsymbol{H}_{1},\boldsymbol{H}_{2}),D_{F}(\boldsymbol{H}_{2}, \boldsymbol{H}_{1})\}\leq D_{F}(\boldsymbol{H}_{1},\boldsymbol{H}_{2})+D_{F}( \boldsymbol{H}_{2},\boldsymbol{H}_{1})\leq\langle\boldsymbol{Z}_{1}- \boldsymbol{Z}_{2},\boldsymbol{H}_{2}-\boldsymbol{H}_{1}\rangle\]

Since the second order directional derivative for \(F\) is \(D^{2}F(\boldsymbol{H})[\boldsymbol{X},\boldsymbol{X}]=\operatorname{Tr}( \boldsymbol{X}\boldsymbol{H}^{-1}\boldsymbol{X}\boldsymbol{H}^{-1})\) for any symmetric matrix \(\boldsymbol{X}\), from the Taylor series, there exists \(\boldsymbol{H}^{\prime}\) that is a line segment between \(\boldsymbol{H}_{1}\) and \(\boldsymbol{\dot{H}}_{2}\) such that

\[\|\boldsymbol{H}_{1}-\boldsymbol{H}_{2}\|_{\nabla^{2}F(\boldsymbol {H}^{\prime})}^{2} =2\min\{D_{F}(\boldsymbol{H}_{1},\boldsymbol{H}_{2}),D_{F}( \boldsymbol{H}_{2},\boldsymbol{H}_{1})\}\leq\langle\boldsymbol{Z}_{1}- \boldsymbol{Z}_{2},\boldsymbol{H}_{2}-\boldsymbol{H}_{1}\rangle\] \[\leq\|\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}\|_{\nabla^{-2}F( \boldsymbol{H}^{\prime})}\|\boldsymbol{H}_{1}-\boldsymbol{H}_{2}\|_{\nabla^{2}F( \boldsymbol{H}^{\prime})}\] (Lemma 8)

Thus we have \(\|\boldsymbol{H}_{1}-\boldsymbol{H}_{2}\|_{\nabla^{2}F(\boldsymbol{H}^{\prime})} \leq\|\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}\|_{\nabla^{-2}F(\boldsymbol{H}^{ \prime})}\). Since \(\|a\|_{2}\leq 1\), \(\boldsymbol{H}^{\prime}\preceq 2\boldsymbol{I}\). The left-hand side and right-hand side can be bounded as follows,

\[\|\boldsymbol{H}_{1}-\boldsymbol{H}_{2}\|_{\nabla^{2}F(\boldsymbol {H}^{\prime})}=\sqrt{\operatorname{Tr}\left((\boldsymbol{H}_{1}-\boldsymbol{H}_{2})( \boldsymbol{H}^{\prime})^{-1}(\boldsymbol{H}_{1}-\boldsymbol{H}_{2})( \boldsymbol{H}^{\prime})^{-1}\right)}\geq\frac{1}{2}\|\boldsymbol{H}_{1}- \boldsymbol{H}_{2}\|_{F}\] \[\|\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}\|_{\nabla^{-2}F(\boldsymbol {H}^{\prime})}=\sqrt{\operatorname{Tr}\left((\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}) \boldsymbol{H}^{\prime}(\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2})\boldsymbol{H}^{ \prime}\right)}\leq 2\|\boldsymbol{Z}_{1}-\boldsymbol{Z}_{2}\|_{F}\leq\frac{\epsilon}{2}\]Combining the three inequalities above, we conclude that

\[\|\bm{H}_{1}-\bm{H}_{2}\|_{F}\leq 2\|\bm{H}_{1}-\bm{H}_{2}\|_{ \nabla^{2}F(\bm{H}^{\prime})}\leq 2\|\bm{Z}_{1}-\bm{Z}_{2}\|_{\nabla^{-2}F(\bm{H}^{ \prime})}\leq 4\|\bm{Z}_{1}-\bm{Z}_{2}\|_{F}\leq\epsilon.\] \[-\epsilon\bm{I}\preceq\bm{H}_{1}-\bm{H}_{2}\preceq\epsilon\bm{I}.\]

**Lemma 16**.: _Suppose that \(p,p^{\prime}\) are two policies such that for all action set \(\mathcal{A}\),_

\[\left\|\widehat{\mathrm{Cov}}(p^{\mathcal{A}})-\widehat{\mathrm{Cov}}(p^{ \prime\mathcal{A}})\right\|_{F}\leq\epsilon\] (23)

_Then all quantities defined in Definition 11 under \(p\) and \(p^{\prime}\) are close. That is,_

\[\|x(p)-x(p^{\prime})\|\leq\epsilon\] (24) \[\|\hat{x}(p)-\hat{x}(p^{\prime})\|\leq\epsilon\] (25) \[\|H(p)-H(p^{\prime})\|_{F}\leq 7\epsilon\] (26) \[\|\hat{H}(p)-\hat{H}(p^{\prime})\|_{F}\leq 7\epsilon\] (27) \[\|\bm{H}(p)-\bm{H}(p^{\prime})\|_{F}\leq\epsilon\] (28) \[\|\bm{\hat{H}}(p)-\bm{\hat{H}}(p^{\prime})\|_{F}\leq\epsilon\] (29) \[\|\hat{\Sigma}(p)-\hat{\Sigma}(p^{\prime})\|_{F}\leq 7\epsilon\] (30) \[\|\bm{\hat{\Sigma}}(p)-\bm{\hat{\Sigma}}(p^{\prime})\|_{F}\leq\epsilon\] (31)

Proof.: Eq. (28) and Eq. (29) are direct consequences of Eq. (23) since \(\bm{H}(p)\) and \(\bm{\hat{H}}(p)\) are expectations of \(\widehat{\mathrm{Cov}}(p^{\mathcal{A}})\) over distributions over \(\mathcal{A}\). Eq. (31) is directly implied by Eq. (29) because \(\bm{\hat{\Sigma}}(p)=\bm{\hat{H}}(p)+\beta\bm{I}\).

To show Eq. (24) and Eq. (25), observe that by the definition of \(x(p)\) and \(\bm{H}(p)\),

\[\bm{H}(p)=\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{ \mathcal{A}}}\begin{bmatrix}aa^{\top}&a\\ a^{\top}&1\end{bmatrix} =\begin{bmatrix}\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p ^{\mathcal{A}}}[aa^{\top}]&\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^ {\mathcal{A}}}[a]\\ \mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{\mathcal{A}}}[a^{\top}]&1 \end{bmatrix}\] \[=\begin{bmatrix}\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p ^{\mathcal{A}}}[aa^{\top}]&x(p)\\ x(p)^{\top}&1\end{bmatrix}\]

Therefore, \(\|x(p)-x(p^{\prime})\|\leq\|\bm{H}(p)-\bm{H}(p^{\prime})\|_{F}\leq\epsilon\). Similarly, \(\|\hat{x}(p)-\hat{x}(p^{\prime})\|\leq\|\bm{\hat{H}}(p)-\bm{\hat{H}}(p^{\prime })\|_{F}\leq\epsilon\).

If remains to show Eq. (26), Eq. (27) and Eq. (30). Next, we show Eq. (26):

\[H(p)-H(p^{\prime})\] \[=\mathbb{E}_{\mathcal{A}\sim D}\left[\mathbb{E}_{a\sim p^{ \mathcal{A}}}[(a-\hat{x}(p))(a-\hat{x}(p))^{\top}]-\mathbb{E}_{a\sim p^{ \prime}\mathcal{A}}[(a-\hat{x}(p^{\prime}))(a-\hat{x}(p^{\prime}))^{\top}]\right]\] \[=\mathbb{E}_{\mathcal{A}\sim D}\Big{[}\mathbb{E}_{a\sim p^{ \mathcal{A}}}[aa^{\top}]-\mathbb{E}_{a\sim p^{\prime}\mathcal{A}}[aa^{\top}] \Big{]}\] \[\qquad\quad-x(p)\hat{x}(p)^{\top}-\hat{x}(p)x(p)^{\top}+x(p^{ \prime})\hat{x}(p^{\prime})^{\top}+\hat{x}(p^{\prime})x(p^{\prime})^{\top} \quad\text{(using $\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p^{ \mathcal{A}}}[a]=x(p)$)}\] \[\qquad\quad+\hat{x}(p)\hat{x}(p)^{\top}-\hat{x}(p^{\prime})\hat {x}(p^{\prime})^{\top}\] (32)

Using the property

\[\|ab^{\top}-cd^{\top}\|_{F}\leq\|ab^{\top}-cb^{\top}\|_{F}+\|cb^{\top}-cd^{\top }\|_{F}\leq\|a-c\|\|b\|+\|c\|b-d\|\]

we continue from Eq. (32) and bound

\[\|H(p)-H(p^{\prime})\|_{F}\] \[\leq\|\bm{H}(p)-\bm{H}(p^{\prime})\|_{F}+2(\|\hat{x}(p)-\hat{x}(p ^{\prime})\|+\|x(p)-x(p^{\prime})\|)+\|\hat{x}(p)-\hat{x}(p^{\prime})\|+\| \hat{x}(p)-\hat{x}(p^{\prime})\|\] \[\leq 7\epsilon.\]

Eq. (27) can be shown in the same manner, which further implies Eq. (30) by the definition of \(\hat{\Sigma}(p)\).

**Lemma 17**.: _With probability \(1-\delta\), for all \(t=1,\cdots,T\),_

\[\hat{H}_{t}+\frac{50(d+1)^{3}\log(3T/\delta)}{t-1}I \succeq\frac{1}{2}H_{t},\] \[\boldsymbol{\hat{H}}_{t}+\frac{50(d+1)^{3}\log(3T/\delta)}{t-1} \boldsymbol{I} \succeq\frac{1}{2}\boldsymbol{H}_{t}.\]

Proof.: Notice that \(\hat{H}_{t},\boldsymbol{\hat{H}}_{t},H_{t},\boldsymbol{H}_{t}\) corresponds to \(\hat{H}(p_{t}),\boldsymbol{\hat{H}}(p_{t}),H(p_{t}),\boldsymbol{H}(p_{t})\) defined in Definition11 with \(n=t-1\). To show the lemma, our strategy is to argue the following two facts: 1) the two desired inequalities hold for all policies in the cover \(\mathbf{P}^{\prime}\) (defined in Eq.22) with high probability. This is simply by applying Lemma12 with an union bound over policies in \(\mathbf{P}^{\prime}\). 2) \(p_{t}\) is sufficiently close to the nearest element in \(\mathbf{P}^{\prime}\) so the desired inequalities still approximately hold.

By Lemma15, we can find \(p^{\prime}\in\mathbf{P}^{\prime}\) such that for all \(\mathcal{A}\),

\[\left\|\widehat{\mathrm{Cov}}(p_{t}^{\mathcal{A}})-\widehat{\mathrm{Cov}}(p^ {\prime\mathcal{A}})\right\|_{F}\leq\epsilon.\]

By Lemma16, it holds that

\[\|H(p_{t})-H(p^{\prime})\|_{F} \leq 7\epsilon,\quad\|\hat{H}(p_{t})-\hat{H}(p^{\prime})\|_{F} \leq 7\epsilon\] (33) \[\|\boldsymbol{H}(p_{t})-\boldsymbol{H}(p^{\prime})\|_{F} \leq\epsilon,\quad\|\boldsymbol{\hat{H}}(p_{t})-\boldsymbol{\hat{H}}(p ^{\prime})\|_{F}\leq\epsilon\] (34)

On the other hand, using Lemma12 and union bound, with probability \(1-\delta\), we have

\[\hat{H}(p^{\prime})+\frac{4d\log(6d|\mathbf{P}^{\prime}|/\delta) }{n}I \succeq\frac{1}{2}H(p^{\prime}),\] (35) \[\boldsymbol{\hat{H}}(p^{\prime})+\frac{3d\log(d|\mathbf{P}^{ \prime}|/\delta)}{n}\boldsymbol{I} \succeq\frac{1}{2}\boldsymbol{H}(p^{\prime}).\] (36)

Combining Eq.35 and Eq.33, we get

\[\hat{H}(p_{t})+7\epsilon I+\frac{4d\log(6d|\mathbf{P}^{\prime}|/\delta)}{n}I \succeq\hat{H}(p^{\prime})+\frac{4d\log(6d|\mathbf{P}^{\prime}|/\delta)}{n}I \succeq\frac{1}{2}H(p^{\prime})\succeq\frac{1}{2}H(p_{t})-\frac{7}{2}\epsilon I\]

which implies the first inequality in the lemma by plugging in the choice of \(\epsilon=\frac{1}{T^{3}}\) and the upper bound of \(\log|\mathbf{P}^{\prime}|\) in Lemma16. The second inequality in the lemma can be obtained similarly by combining Eq.34 and Eq.36.

**Lemma 18**.: _With probability of at least \(1-\delta\), for all \(t=1,\cdots,T\),_

\[\|x_{t}-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}\leq\mathcal{O}\left(\frac{d ^{3}\log\left(dT/\delta\right)}{t}\right)\]

Proof.: Notice that \(x_{t},\hat{x}_{t},\hat{\Sigma}_{t}\) corresponds to \(x(p_{t}),\hat{x}(p_{t}),\hat{\Sigma}(p_{t})\) defined in Definition11 with \(n=t-1\). To show the lemma, our strategy is to argue the following two facts: 1) the two desired inequalities hold for all policies in the cover \(\mathbf{P}^{\prime}\) with high probability. This is simply by applying Lemma13 with an union bound over policies in \(\mathbf{P}^{\prime}\). 2) \(p_{t}\) is sufficiently close to the nearest element in \(\mathbf{P}^{\prime}\) so the desired inequalities still approximately hold.

By Lemma15, we can find \(p^{\prime}\in\mathbf{P}^{\prime}\) such that for all \(\mathcal{A}\),

\[\left\|\widehat{\mathrm{Cov}}(p_{t}^{\mathcal{A}})-\widehat{\mathrm{Cov}}(p^{ \prime\mathcal{A}})\right\|_{F}\leq\epsilon.\]

By Lemma16, we have

\[\|x(p^{\prime})-x(p_{t})\|\leq\epsilon,\quad\|\hat{x}(p^{\prime})-\hat{x}(p_ {t})\|\leq\epsilon,\quad\|\hat{\Sigma}(p^{\prime})-\hat{\Sigma}(p_{t})\|_{F} \leq 7\epsilon\] (37)Thus,

\[\|x(p_{t})-\hat{x}(p_{t})\|^{2}_{\hat{\Sigma}(p_{t})^{-1}}\] \[=\left(\|x(p_{t})-\hat{x}(p_{t})\|^{2}_{\hat{\Sigma}(p_{t})^{-1}}- \|x(p^{\prime})-\hat{x}(p^{\prime})\|^{2}_{\hat{\Sigma}(p^{\prime})^{-1}}\right)+ \|x(p^{\prime})-\hat{x}(p^{\prime})\|^{2}_{\hat{\Sigma}(p^{\prime})^{-1}}\] \[\leq\left(\|x(p_{t})-\hat{x}(p_{t})\|^{2}_{\hat{\Sigma}(p_{t})^{- 1}}-\|x(p^{\prime})-\hat{x}(p^{\prime})\|^{2}_{\hat{\Sigma}(p^{\prime})^{-1}} \right)+\mathcal{O}\left(\frac{d\log(d|\mathbf{P}^{\prime}|/\delta)}{t-1}\right)\] (by Lemma 13 with an union bound over \[\mathbf{P}^{\prime}\] ) \[=\theta_{t}^{\top}\hat{\Sigma}(p_{t})^{-1}\theta_{t}-\theta^{ \prime\top}\hat{\Sigma}(p^{\prime})^{-1}\theta^{\prime}+\mathcal{O}\left( \frac{d\log(d|\mathbf{P}^{\prime}|/\delta)}{t-1}\right)\] (define \[\theta_{t}=x(p_{t})-\hat{x}(p_{t})\] and \[\theta^{\prime}=x(p^{\prime})-\hat{x}(p^{\prime})\] ) \[=(\theta_{t}-\theta^{\prime})^{\top}\hat{\Sigma}(p_{t})^{-1} \theta_{t}+\theta^{\prime\top}\Big{(}\hat{\Sigma}(p_{t})^{-1}-\hat{\Sigma}(p^ {\prime})^{-1}\Big{)}\theta_{t}+\theta^{\prime\top}\hat{\Sigma}(p^{\prime})^{- 1}(\theta_{t}-\theta^{\prime})+\mathcal{O}\left(\frac{d\log(d|\mathbf{P}^{ \prime}|/\delta)}{t-1}\right)\] \[\leq(\theta_{t}-\theta^{\prime})^{\top}\Big{(}\hat{\Sigma}(p_{t} )^{-1}\theta_{t}+\hat{\Sigma}(p^{\prime})^{-1}\theta^{\prime}\Big{)}+\theta^{ \prime\top}\hat{\Sigma}(p^{\prime})^{-1}\Big{(}\hat{\Sigma}(p^{\prime})-\hat {\Sigma}(p_{t})\Big{)}\hat{\Sigma}(p_{t})^{-1}\theta_{t}+\mathcal{O}\left( \frac{d\log(d|\mathbf{P}^{\prime}|/\delta)}{t-1}\right)\]

The first two terms above can be bounded by the order of \(\mathcal{O}(\epsilon t^{2})\) by Eq. (37). Using the choice \(\epsilon=\frac{1}{T^{3}}\) and recalling that \(\log|\mathbf{P}^{\prime}|=\mathcal{O}(d^{2}\log(d/\epsilon))\) finishes the proof.

**Lemma 19**.: _With probability of at least \(1-\delta\), for all \(t=1,2,\ldots,T\),_

\[\|(\hat{\Sigma}_{t}-H_{t})y_{t}\|^{2}_{\hat{\Sigma}_{t}^{-1}}\leq\mathcal{O} \left(\frac{d^{3}\log\left(dT/\delta\right)}{t}\right)\]

Proof.: Notice that \(x_{t},\hat{x}_{t},\hat{\Sigma}_{t}\) corresponds to \(x(p_{t}),\hat{x}(p_{t}),\hat{\Sigma}(p_{t})\) defined in Definition 11 with \(n=t-1\). To show the lemma, our strategy is to argue the following two facts: 1) the two desired inequalities hold for all policies in the cover \(\mathbf{P}^{\prime}\) with high probability. This is simply by applying Lemma 13 with an union bound over policies in \(\mathbf{P}^{\prime}\). 2) \(p_{t}\) is sufficiently close to the nearest element in \(\mathbf{P}^{\prime}\) so the desired inequalities still approximately hold.

By Lemma 15, we can find \(p^{\prime}\in\mathbf{P}^{\prime}\) such that for all \(\mathcal{A}\),

\[\left\|\widehat{\mathrm{Cov}}(p_{t}^{\mathcal{A}})-\widehat{\mathrm{Cov}}(p^{ \prime\mathcal{A}})\right\|_{F}\leq\epsilon.\]

By Lemma 16, we have

\[\|x(p^{\prime})-x(p_{t})\|\leq\epsilon,\quad\|\hat{x}(p^{\prime})-\hat{x}(p_{ t})\|\leq\epsilon,\quad\|\hat{\Sigma}(p^{\prime})-\hat{\Sigma}(p_{t})\|_{F} \leq 7\epsilon\] (38)

Thus, for any \(\|y_{t}\|_{2}\leq 1\),

\[\|(\hat{\Sigma}(p_{t})-H(p_{t}))y_{t}\|^{2}_{\hat{\Sigma}(p_{t})^{- 1}}\] \[=\Big{(}\|(\hat{\Sigma}(p_{t})-H(p_{t}))y_{t}\|^{2}_{\hat{\Sigma}( p_{t})^{-1}}-\|(\hat{\Sigma}(p^{\prime})-H(p^{\prime}))y_{t}\|^{2}_{\hat{\Sigma}(p^{ \prime})^{-1}}\Big{)}+\|(\hat{\Sigma}(p^{\prime})-H(p^{\prime}))y_{t}\|^{2}_{ \hat{\Sigma}(p^{\prime})^{-1}}\] \[\leq\Big{(}\|(\hat{\Sigma}(p_{t})-H(p_{t}))y_{t}\|^{2}_{\hat{ \Sigma}(p_{t})^{-1}}-\|(\hat{\Sigma}(p^{\prime})-H(p^{\prime}))y_{t}\|^{2}_{ \hat{\Sigma}(p^{\prime})^{-1}}\Big{)}+\mathcal{O}\left(\frac{d\log(d|\mathbf{P}^ {\prime}|/\delta)}{t-1}\right)\] (by Lemma 14 with an union bound over \[\mathbf{P}^{\prime}\] ) \[=\theta_{t}^{\top}\hat{\Sigma}(p_{t})^{-1}\theta_{t}-\theta^{ \prime\top}\hat{\Sigma}(p^{\prime})^{-1}\theta^{\prime}+\mathcal{O}\left(\frac {d\log(d|\mathbf{P}^{\prime}|/\delta)}{t-1}\right)\] (define \[\theta_{t}=(\hat{\Sigma}(p_{t})-H(p_{t}))y_{t}\] and \[\theta^{\prime}=(\hat{\Sigma}(p^{\prime})-H(p^{\prime}))y_{t}\] \[=(\theta_{t}-\theta^{\prime})^{\top}\hat{\Sigma}(p_{t})^{-1}\theta_{ t}+\theta^{\prime\top}\Big{(}\hat{\Sigma}(p_{t})^{-1}-\hat{\Sigma}(p^{\prime})^{-1} \Big{)}\theta_{t}+\theta^{\prime\top}\hat{\Sigma}(p^{\prime})^{-1}(\theta_{t}- \theta^{\prime})+\mathcal{O}\left(\frac{d\log(d|\mathbf{P}^{\prime}|/\delta)}{t-1}\right)\] \[\leq(\theta_{t}-\theta^{\prime})^{\top}\Big{(}\hat{\Sigma}(p_{t})^{- 1}\theta_{t}+\hat{\Sigma}(p^{\prime})^{-1}\theta^{\prime}\Big{)}+\theta^{ \prime\top}\hat{\Sigma}(p^{\prime})^{-1}\Big{(}\hat{\Sigma}(p^{\prime})-\hat{ \Sigma}(p_{t})\Big{)}\hat{\Sigma}(p_{t})^{-1}\theta_{t}+\mathcal{O}\left(\frac{d \log(d|\mathbf{P}^{\prime}|/\delta)}{t-1}\right)\]

The first two terms above can be bounded by the order of \(\mathcal{O}(\epsilon t^{2})\) by Eq. (38). Plugging in the choice of \(\epsilon=\frac{1}{T^{3}}\) and recalling that \(\log|\mathbf{P}^{\prime}|=\mathcal{O}(d^{2}\log(d/\epsilon))\) finishes the proof.

The proof of Lemma 16 is similar to the proof of Lemma 16. The proof of Lemma 16 is similar to the proof of Lemma 16.

## Appendix D Regret Analysis

Consider the regret decomposition in Section 3.5.

\[\text{Reg}(u)=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{ \mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{t}},\gamma_{t}\right\rangle\right]= \mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U} ^{\mathcal{A}_{0}},\gamma_{t}\right\rangle\right]=\mathbb{E}\left[\sum_{t=1}^ {T}\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_ {t}\right\rangle\right]\] \[\leq\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H} _{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t} \right\rangle\right]}_{\text{\bf Bias}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^ {T}\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\alpha_{ t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]}_{\text{\bf Bonus}}+\underbrace{ \mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^ {\mathcal{A}_{0}},\hat{\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right \rangle\right]}_{\text{\bf FTRL-Reg}}\]

where \(\mathcal{A}_{0}\) is drawn from \(D\) and is independent from the interaction between the learning and the environment. Recall that our algorithm is FTRL:

\[\bm{H}_{t}^{\mathcal{A}_{0}}=\operatorname*{argmin}_{\bm{H}\in\mathcal{H}^{ \mathcal{A}_{0}}}\left\{\sum_{s=1}^{t-1}\left\langle\bm{H},\hat{\gamma}_{s}- \alpha_{s}\bm{\hat{\Sigma}}_{s}^{-1}\right\rangle+\frac{F(\bm{H})}{\eta_{t}} \right\}.\]

The **FTRL-Reg** term can be handled by the standard FTRL analysis (Lemma 5). In order to deal with the issue that \(F\) can be unbounded on the boundary of \(\mathcal{H}^{\mathcal{A}_{0}}\), we apply Lemma 5 with the regret comparator \(\bm{\overline{U}}^{\mathcal{A}_{0}}\) defined as

\[\bm{\overline{U}}^{\mathcal{A}_{0}}=\left(1-\frac{1}{T^{2}}\right)\bm{U}^{ \mathcal{A}_{0}}+\frac{1}{T^{2}}\bm{H}_{*}^{\mathcal{A}_{0}}\]

where \(\bm{H}_{*}^{\mathcal{A}_{0}}\triangleq\operatorname*{argmin}_{\bm{H}\in \mathcal{H}^{\mathcal{A}_{0}}}F(\bm{H})\). Thus,

**FTRL-Reg** \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{ \mathcal{A}_{0}}-\bm{\overline{U}}^{\mathcal{A}_{0}},\hat{\gamma}_{t}-\alpha_{ t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]+\mathbb{E}\left[\sum_{t=1}^{T} \left\langle\bm{\overline{U}}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\hat {\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]\] \[\leq\underbrace{\mathbb{E}\left[\frac{F(\bm{\overline{U}}^{ \mathcal{A}_{0}})-\min_{\bm{H}\in\mathcal{H}^{\mathcal{A}_{0}}}F(\bm{H})}{\eta _{T}}\right]}_{\text{\bf Penalty}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T} \max_{\bm{H}\in\mathcal{H}^{\mathcal{A}_{0}}}\left\langle\bm{H}_{t}^{\mathcal{ A}_{0}}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{ \mathcal{A}_{0}})}{2\eta_{t}}\right]}_{\text{\bf Stability-1}}\] \[\quad\quad+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\max_{\bm{ H}\in\mathcal{H}^{\mathcal{A}_{0}}}\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{H},- \alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{ \mathcal{A}_{0}})}{2\eta_{t}}\right]}_{\text{\bf Stability-2}}+\underbrace{ \mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{\overline{U}}^{\mathcal{A}_{0}}- \bm{U}^{\mathcal{A}_{0}},\hat{\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1 }\right\rangle\right]}_{\text{\bf Error}}\] (39)

In the rest of this section, we bound the following terms individually: **Bias**, **Bonus**, **Penalty**, **Stability-1**, **Stability-2**, **Error**.

For any \(t=2,\cdots,T\), let \(\mathcal{E}_{t-1}\) be the event that the high-probability event in Lemma 17, Lemma 18, and Lemma 19 happens for all \(1,\cdots,t-1\) and \(\overline{\mathcal{E}_{t-1}}\) be the opposite event of \(\mathcal{E}_{t-1}\)(i.e. any of these three lemmas fails for any \(1,\cdots,t-1\)). We have \(\mathcal{P}[\mathcal{E}_{t-1}]=1-\mathcal{O}(\delta)\) and \(\mathcal{P}[\overline{\mathcal{E}_{t-1}}]=\mathcal{O}(\delta)\). Let \(\mathbb{E}\left[\cdot\mid\mathcal{E}_{t-1}\right]\) be the conditional expectation that event \(\mathcal{E}_{t-1}\) happens and let \(\mathbb{E}_{t}^{\mathcal{E}}=\mathbb{E}[\cdot\mid\mathcal{F}_{t-1},\mathcal{E} _{t-1}]\)

### Bounding the Bias term

**Lemma 20**.: \[\text{\bf Bias}=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{ \mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t}\right\rangle \right]\leq\frac{1}{4}\sum_{t=1}^{T}\alpha_{t}\|x_{t}-u\|_{\Sigma_{t}^{-1}}^{2} +\mathcal{O}\left(\delta T^{2}+\sum_{t=1}^{T}\frac{d^{3}\log(T/\delta)}{\alpha_{ t}t}\right)\]Proof.: For any \(t\), we have

\[\mathbb{E}_{t}^{\mathcal{E}}\left[\left\langle\boldsymbol{H}_{t}^{ \mathcal{A}_{0}}-\boldsymbol{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t} \right\rangle\right]\] \[=\mathbb{E}_{t}^{\mathcal{E}}\left[(\boldsymbol{H}_{t}- \boldsymbol{U},\gamma_{t}-\hat{\gamma}_{t})\right]\] (taking expectation over \[\mathcal{A}_{0}\]) \[=\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u,y_{t}-\hat{y}_{t})\right]\] (by the definition of lifting) \[=\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\left(y_{t}- \hat{\Sigma}_{t}^{-1}(a_{t}-\hat{x}_{t})a_{t}^{\top}y_{t}\right)\right]\] (by the definition of \[\hat{y}_{t}\]) \[=\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\left(y_{t}- \hat{\Sigma}_{t}^{-1}(a_{t}-\hat{x}_{t})(a_{t}-\hat{x}_{t})^{\top}y_{t}\right) \right]-\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\hat{\Sigma}_{t}^{-1 }(a_{t}-\hat{x}_{t})\hat{x}_{t}^{\top}y_{t}\right]\] \[=\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\left(I-\hat{ \Sigma}_{t}^{-1}\mathbb{E}_{\mathcal{A}\sim\mathcal{D}}\mathbb{E}_{a_{t}\sim p _{t}^{\mathcal{A}}}\left[(a_{t}-\hat{x}_{t})(a_{t}-\hat{x}_{t})^{\top}\right] \right)y_{t}\right]\] \[\qquad-\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\hat{ \Sigma}_{t}^{-1}\left(\mathbb{E}_{\mathcal{A}\sim\mathcal{D}}\mathbb{E}_{a_{t} \sim p_{t}^{\mathcal{A}}}[a_{t}]-\hat{x}_{t}\right)\hat{x}_{t}^{\top}y_{t}\right]\] (taking expectation over \[\mathcal{A}_{t}\] and \[a_{t}\]) \[=\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\hat{\Sigma}_{ t}^{-1}\left(\hat{\Sigma}_{t}-H_{t}\right)y_{t}\right]-\mathbb{E}_{t}^{\mathcal{E}} \left[(x_{t}-u)^{\top}\hat{\Sigma}_{t}^{-1}\left(x_{t}-\hat{x}_{t}\right)\hat{ x}_{t}^{\top}y_{t}\right]\] (by the definition of \[H_{t}\] and \[x_{t}\]) \[\leq\mathbb{E}_{t}^{\mathcal{E}}\left[(x_{t}-u)^{\top}\hat{ \Sigma}_{t}^{-1}\left(\hat{\Sigma}_{t}-H_{t}\right)y_{t}\right]+\mathbb{E}_{t}^ {\mathcal{E}}\left[\left|(x_{t}-u)^{\top}\hat{\Sigma}_{t}^{-1}\left(x_{t}-\hat {x}_{t}\right)\right|\right]\] ( \[|\hat{x}_{t}^{\top}y_{t}|\leq 1\] ) \[\leq\mathbb{E}_{t}^{\mathcal{E}}\left[\|x_{t}-u\|_{\hat{\Sigma}_{ t}^{-1}}\left(\|(\hat{\Sigma}_{t}-H_{t})y_{t}\|_{\hat{\Sigma}_{t}^{-1}}+\|x_{t}-\hat{x}_{t} \|_{\hat{\Sigma}_{t}^{-1}}\right)\right]\] (Cauchy-Schwarz) \[\leq\mathcal{O}\left(\sqrt{\frac{d^{3}\log(T/\delta)}{t}}\|x_{t}-u \|_{\hat{\Sigma}_{t}^{-1}}\right)\] (Lemma 19 and Lemma 18 given \[\mathcal{E}_{t-1}\] ) \[\leq\frac{\alpha_{t}}{4}\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}^{2}+ \mathcal{O}\left(\frac{d^{3}\log(T/\delta)}{\alpha_{t}t}\right)\] (AM-GM inequality) On the other hand, since \[\hat{\Sigma}_{t}\succeq\frac{1}{t}I\succeq\frac{1}{T}I\], for any \[t=1,\cdots,T\], \[\|\hat{y}_{t}\|_{2}=\|\Sigma_{t}^{-1}(a_{t}-\hat{x}_{t})a_{t}^{ \top}y_{t}\|_{2}\leq\|\Sigma_{t}^{-1}(a_{t}-\hat{x}_{t})\|_{2}\leq\mathcal{O}(T)\]

Thus, we have trivial bound

\[\mathbb{E}_{t}\left[\left\langle\boldsymbol{H}_{t}^{\mathcal{A}_{0}}- \boldsymbol{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t}\right\rangle\ \left|\ \overline{\mathcal{E}_{t-1}}\right]=\mathbb{E}_{t}\left[\left\langle \boldsymbol{H}_{t}-\boldsymbol{U},\gamma_{t}-\hat{\gamma}_{t}\right\rangle\ \left|\ \overline{\mathcal{E}_{t-1}}\right]=\mathbb{E}_{t}\left[(x_{t}-u,y_{t}-\hat{y}_{t })\ \left|\ \overline{\mathcal{E}_{t-1}}\right]\leq\mathcal{O}(T)\]

Therefore, we have

\[\textbf{Bias} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\boldsymbol{H}_{t}^{ \mathcal{A}_{0}}-\boldsymbol{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t} \right\rangle\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle \boldsymbol{H}_{t}^{\mathcal{A}_{0}}-\boldsymbol{U}^{\mathcal{A}_{0}},\gamma_{t}- \hat{\gamma}_{t}\right\rangle\right]\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle \boldsymbol{H}_{t}^{\mathcal{A}_{0}}-\boldsymbol{U}^{\mathcal{A}_{0}},\gamma_{t}- \hat{\gamma}_{t}\right\rangle\ \left|\ \mathcal{E}_{t-1}\right]\mathbb{I}\{\mathcal{E}_{t-1}\}\right]+\mathbb{E} \left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle\boldsymbol{H}_{t}^{ \mathcal{A}_{0}}-\boldsymbol{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t} \right\rangle\ \left|\ \overline{\mathcal{E}_{t-1}}\right]\mathbb{I}\{\mathcal{E}_{t-1}\}\right]\] \[\leq\frac{1}{4}\sum_{t=1}^{T}\alpha_{t}\|x_{t}-u\|_{\hat{ \Sigma}_{t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1}^{T}\frac{d^{3}\log(T/\delta)}{ \alpha_{t}t}+\delta T^{2}\right)\]

### Bounding the Bonus term

We first prove the following useful technique lemma to bound the inner product of lifted matrices.

**Lemma 21**.: _Let \(\boldsymbol{G}=\begin{bmatrix}G+gg^{\top}&g\\ g^{\top}&1\end{bmatrix}\), \(\boldsymbol{H}=\begin{bmatrix}H+hh^{\top}&h\\ h^{\top}&1\end{bmatrix}\) where \(G\) and \(H\) are positive semi-definite, and \(\boldsymbol{H}^{\prime}=\boldsymbol{H}+vv^{\top}\) where \(v=\begin{bmatrix}0\\ \sqrt{\beta}\end{bmatrix}\in\mathbb{R}^{d+1}\). Then we have_1. \(\operatorname{Tr}\left(\bm{H}^{-1}\bm{G}\right)=\operatorname{Tr}(H^{-1}G)+\|g-h\|_ {H^{-1}}^{2}+1\)__
2. \(\operatorname{Tr}\left((\bm{H}^{\prime})^{-1}\bm{G}\right)\geq\frac{1}{2\left( 1+\frac{\beta}{1+\beta}\|h\|_{H^{-1}}^{2}\right)}\|g-h\|_{H^{-1}}^{2}-\frac{ \beta^{2}}{(1+\beta)^{2}}\|h\|_{H^{-1}}^{2}\)__

Proof.: From Theorem 2.1 of Lu and Shiou [2002], for any block matrix \(R=\begin{bmatrix}A&B\\ C&D\end{bmatrix}\) if \(A\) is invertible and its Schur complement \(S_{A}=D-CA^{-1}B\) is invertible, then

\[R^{-1}=\begin{bmatrix}A^{-1}+A^{-1}BS_{A}^{-1}CA^{-1}&-A^{-1}BS_{A}^{-1}\\ -S_{A}^{-1}CA&S_{A}^{-1}\end{bmatrix}\]

Using above equation, for the first equation, Since \((H+hh^{\top})^{-1}=H^{-1}-\frac{H^{-1}hh^{\top}H^{-1}}{1+h^{\top}H^{-1}h}\). The inverse Schur complement of \(H+hh^{\top}\) is \(1+h^{\top}H^{-1}h\). Thus

\[\bm{H}^{-1}=\begin{bmatrix}(I+H^{-1}hh^{\top})(H+hh^{\top})^{-1}&-H^{-1}h\\ -h^{\top}H^{-1}&1+h^{\top}H^{-1}h\end{bmatrix}=\begin{bmatrix}H^{-1}&-H^{-1}h \\ -h^{\top}H^{-1}&1+h^{\top}H^{-1}h\end{bmatrix}\]

and

\[\operatorname{Tr}(\bm{H}^{-1}\bm{G}) =\operatorname{Tr}\left(H^{-1}G+H^{-1}gg^{\top}-H^{-1}hg^{\top} \right)-h^{\top}H^{-1}g+1+h^{\top}H^{-1}h\] \[=\operatorname{Tr}\left(H^{-1}G\right)+g^{\top}H^{-1}g-2g^{\top }H^{-1}h+h^{\top}H^{-1}h+1\] \[=\operatorname{Tr}(H^{-1}G)+\|g-h\|_{H^{-1}}^{2}+1.\]

For the second equation, observe that

\[\bm{H}^{\prime}=\begin{bmatrix}H+hh^{\top}&h\\ h^{\top}&1+\beta\end{bmatrix}=(1+\beta)\begin{bmatrix}\frac{1}{1+\beta}(H+hh^ {\top})&\frac{1}{1+\beta}h\\ \frac{1}{1+\beta}h^{\top}&1\end{bmatrix}=(1+\beta)\begin{bmatrix}H^{\prime}+ h^{\prime}h^{\prime\top}&h^{\prime}\\ h^{\prime\top}&1\end{bmatrix}\]

where \(h^{\prime}=\frac{1}{1+\beta}h\) and \(H^{\prime}=\frac{1}{1+\beta}H+(\frac{1}{1+\beta}-\frac{1}{(1+\beta)^{2}})hh^ {\top}=\frac{1}{1+\beta}H+\frac{\beta}{(1+\beta)^{2}}hh^{\top}\succeq 0\).

Applying the first equality, we have

\[\operatorname{Tr}((\bm{H}^{\prime})^{-1}\bm{G})=\frac{1}{1+\beta}\left( \operatorname{Tr}((H^{\prime})^{-1}G)+\|g-h^{\prime}\|_{H^{\prime-1}}^{2}+1 \right)\geq\frac{1}{1+\beta}\|g-h^{\prime}\|_{H^{\prime-1}}^{2}.\]

Below, we continue to lower bound this term. By the same formula above, we have

\[H^{\prime-1}=\left(\frac{1}{1+\beta}H+\frac{\beta}{(1+\beta)^{2}}hh^{\top} \right)^{-1}=(1+\beta)H^{-1}-\frac{\beta H^{-1}hh^{\top}H^{-1}}{1+\frac{\beta }{1+\beta}h^{\top}H^{-1}h}.\]

Thus

\[\frac{1}{1+\beta}\|g-h^{\prime}\|_{H^{\prime-1}}^{2}\] \[\geq\frac{1}{2(1+\beta)}\|g-h\|_{H^{\prime-1}}^{2}-\frac{1}{1+ \beta}\|h-h^{\prime}\|_{H^{\prime-1}}^{2}\] (using

\[\|a+b\|^{2}\leq 2\|a\|^{2}+2\|b\|^{2}\] \[=\frac{1}{2}(g-h)^{\top}\left(H^{-1}-\frac{\frac{\beta}{1+\beta}H ^{-1}hh^{\top}H^{-1}}{1+\frac{\beta}{1+\beta}h^{\top}H^{-1}h}\right)(g-h)-(h-h ^{\prime})^{\top}\left(H^{-1}-\frac{\frac{\beta}{1+\beta}H^{-1}hh^{\top}H^{-1 }}{1+\frac{\beta}{1+\beta}h^{\top}H^{-1}h}\right)(h-h^{\prime})\] \[\geq\frac{1}{2}\|g-h\|_{H^{-1}}^{2}-\frac{\frac{\beta}{1+\beta}(( g-h)^{\top}H^{-1}h)^{2}}{2\left(1+\frac{\beta}{1+\beta}\|h\|_{H^{-1}}^{2}\right)}- \frac{\beta^{2}}{(1+\beta)^{2}}\|h\|_{H^{-1}}^{2}\] (using

\[h-h^{\prime}=\frac{\beta}{1+\beta}h\]

\[\geq\frac{1}{2}\|g-h\|_{H^{-1}}^{2}-\frac{\frac{\beta}{1+\beta}\|h\|_{H^{-1}}^{ 2}}{2\left(1+\frac{\beta}{1+\beta}\|h\|_{H^{-1}}^{2}\right)}\|g-h\|_{H^{-1}}^{2} -\frac{\beta^{2}}{(1+\beta)^{2}}\|h\|_{H^{-1}}^{2}\] (Cauchy-Schwarz) \[=\frac{1}{2\left(1+\frac{\beta}{1+\beta}\|h\|_{H^{-1}}^{2}\right) }\|g-h\|_{H^{-1}}^{2}-\frac{\beta^{2}}{(1+\beta)^{2}}\|h\|_{H^{-1}}^{2}.\]Using Lemma 21, we are able to show Corollary 22 which bound part of the second term.

**Corollary 22**.: \(\operatorname{Tr}(\bm{U}\bm{\hat{\Sigma}}_{t}^{-1})\geq\frac{1}{4}\|u-\hat{x}_{t} \|_{\hat{\Sigma}_{t}^{-1}}^{2}-\frac{1}{4}\)_._

Proof.: From Lemma 21, we have

\[\operatorname{Tr}(\bm{U}\bm{\hat{\Sigma}}_{t}^{-1})\geq\frac{1}{2\left(1+ \frac{\beta_{t}}{1+\beta_{t}}\|\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}\right) }\|u-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}-\frac{\beta_{t}^{2}}{(1+\beta_{t })^{2}}\|\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}.\]

Since \(\hat{\Sigma}_{t}\succeq\beta_{t}I\), \(\hat{\Sigma}_{t}^{-1}\preceq\frac{1}{\beta_{t}}I\). Since \(\|\hat{x}_{t}\|_{2}\leq 1\), we have \(\|\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}\leq\frac{1}{\beta_{t}}\). Then

\[\operatorname{Tr}(\bm{U}\bm{\hat{\Sigma}}_{t}^{-1}) \geq\frac{1}{2\left(1+\frac{1}{1+\beta_{t}}\right)}\|u-\hat{x}_{t }\|_{\hat{\Sigma}_{t}^{-1}}^{2}-\frac{\beta_{t}}{(1+\beta_{t})^{2}}\] \[\geq\frac{1}{4}\|u-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}- \frac{\beta_{t}}{(2\sqrt{\beta_{t}})^{2}}\] ( \[\beta_{t}\geq 0\] ) \[=\frac{1}{4}\|u-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}-\frac{1 }{4}.\]

**Lemma 23**.: \[\mathbf{Bonus} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{\mathcal{A} _{0}}-\bm{U}^{\mathcal{A}_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]\] \[\leq 2(d+2)\sum_{t=1}^{T}\alpha_{t}-\frac{1}{4}\sum_{t=1}^{T} \alpha_{t}\|u-x_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1} ^{T}\frac{d^{3}\alpha_{t}\log\left(T/\delta\right)}{t}+\delta T\sum_{t=1}^{T} \alpha_{t}\right).\]

Proof.: For any \(t\), we have

\[\mathbb{E}_{t}^{\mathcal{E}}\left[\left\langle\bm{H}_{t}^{\mathcal{ A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]\] \[=\mathbb{E}_{t}^{\mathcal{E}}\left[\operatorname{Tr}\left(\alpha _{t}\left(\bm{H}_{t}-\bm{U}\right)\bm{\hat{\Sigma}}_{t}^{-1}\right)\right]\] (taking expectation over \[\mathcal{A}_{0}\] ) \[=\mathbb{E}_{t}^{\mathcal{E}}\left[\alpha_{t}\operatorname{Tr} \left(\bm{H}_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right)-\alpha_{t}\operatorname{Tr }\left(\bm{U}\bm{\hat{\Sigma}}_{t}^{-1}\right)\right]\] \[\leq\alpha_{t}\operatorname{Tr}\left(\mathbb{E}_{t}^{\mathcal{E} }\left[\bm{H}_{t}\right]\bm{\hat{\Sigma}}_{t}^{-1}\right)-\mathbb{E}_{t}^{ \mathcal{E}}\left[\frac{\alpha_{t}}{4}\|u-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1} }^{2}\right]+\frac{1}{4}\alpha_{t}\] (Corollary 22) \[\leq 2\alpha_{t}(d+2)-\mathbb{E}_{t}^{\mathcal{E}}\left[\frac{ \alpha_{t}}{4}\|u-\hat{x}_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}\right]\] \[\leq 2\alpha_{t}(d+2)-\mathbb{E}_{t}^{\mathcal{E}}\left[\frac{ \alpha_{t}}{4}\|u-x_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}-\frac{\alpha_{t}}{4}\| \hat{x}_{t}-x_{t}\|_{\hat{\Sigma}_{t}^{-1}}^{2}\right]\] \[\leq 2\alpha_{t}(d+2)-\frac{\alpha_{t}}{4}\|u-x_{t}\|_{\hat{ \Sigma}_{t}^{-1}}^{2}+\mathcal{O}\left(\frac{d^{3}\alpha_{t}\log\left(T/ \delta\right)}{t}\right)\] (Lemma 18)

On the other hand, since \(\bm{\hat{\Sigma}}_{t}\succeq\frac{1}{t}\bm{I}\succeq\frac{1}{T}\bm{I}\), we have trivial bound

\[\mathbb{E}_{t}\left[\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{ A}_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\ \Big{|}\ \overline{\mathcal{E}_{t-1}}\right]\leq\mathcal{O}(\alpha_{t}T)\]Therefore, we have

\[\text{{Bonus}} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{A_{0}}-\bm{U}^ {A_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle\bm {H}_{t}^{A_{0}}-\bm{U}^{A_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle \right]\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle\bm {H}_{t}^{A_{0}}-\bm{U}^{A_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle \ \Big{|}\ \mathcal{E}_{t-1}\right]\mathbb{I}\{\mathcal{E}_{t-1}\}\right]+ \mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle\bm{H}_{t}^{A_{0} }-\bm{U}^{A_{0}},\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\ \Big{|}\ \overline{ \mathcal{E}_{t-1}}\right]\mathbb{I}\{\overline{\mathcal{E}_{t-1}}\}\] \[\leq 2(d+2)\sum_{t=1}^{T}\alpha_{t}-\frac{(1-\delta)}{4}\sum_{t =1}^{T}\alpha_{t}\|u-x_{t}\|_{\Sigma_{t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1} ^{T}\frac{d^{3}\alpha_{t}\log(T/\delta)}{t}+\delta T\sum_{t=1}^{T}\alpha_{t}\right)\] \[\leq 2(d+2)\sum_{t=1}^{T}\alpha_{t}-\frac{1}{4}\sum_{t=1}^{T} \alpha_{t}\|u-x_{t}\|_{\Sigma_{t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1}^{T} \frac{d^{3}\alpha_{t}\log(T/\delta)}{t}+\delta T\sum_{t=1}^{T}\alpha_{t}\right)\]

### Bounding the Penalty term

**Lemma 24**.: \(\bm{\bar{U}}^{A_{0}}\)_, we have_

\[\frac{F(\bm{\bar{U}}^{A_{0}})-\min_{\bm{H}\in\mathcal{H}^{A_{0}}}F(\bm{H})}{ \eta_{T}}\leq\frac{2d\log(T)}{\eta_{T}}\]

Proof.: Since \(\bm{\bar{U}}^{A_{0}}=\left(1-\frac{1}{T^{2}}\right)\bm{U}^{A_{0}}+\frac{1}{T^ {2}}\bm{H}_{*}^{A_{0}}\), we have \(\bm{\bar{U}}^{A_{0}}\succeq\frac{1}{T^{2}}\bm{H}_{*}^{A_{0}}\). Then

\[\frac{F(\bm{\bar{U}}^{A_{0}})-\min_{\bm{H}\in\mathcal{H}^{A_{0}}}F(\bm{H})}{ \eta_{T}}=\frac{1}{\eta_{T}}\log\frac{\det(\bm{H}_{*}^{A_{0}})}{\det(\bm{\bar {U}}^{A_{0}})}\leq\frac{2d\log(T)}{\eta_{T}}.\]

### Bounding the Stability-1 term

Zimmert and Lattimore (2022) gave a useful identity to bound the Bregman divergence. We restate it in Lemma 25 for completeness.

**Lemma 25**.: _Let \(\bm{G}=\begin{bmatrix}G+gg^{\top}&g\\ g^{\top}&1\end{bmatrix}\) and \(\bm{H}=\begin{bmatrix}H+hh^{\top}&h\\ h^{\top}&1\end{bmatrix}\), we have_

\[D(\bm{G},\bm{H})=D(G,H)+\|g-h\|_{H^{-1}}^{2}\geq\|g-h\|_{H^{-1}}^{2}\]

Proof.: \[D(\bm{G},\bm{H}) =F(\bm{G})-F(\bm{H})-\left\langle\nabla F(\bm{H}),\bm{G}-\bm{H}\right\rangle\] \[=\log\left(\frac{\det(\bm{H})}{\det(\bm{G})}\right)+\text{Tr}(\bm {H}^{-1}(\bm{G}-\bm{H}))\] \[=\log\left(\frac{\det(\bm{H})}{\det(\bm{G})}\right)+\text{Tr}(\bm {H}^{-1}\bm{G})-d-1\] \[=\log\left(\frac{\det(\bm{H})}{\det(\bm{G})}\right)+\text{Tr}(\bm {H}^{-1}\bm{G})-d-1\] \[=\log\left(\frac{\det(H)}{\det(G)}\right)+\text{Tr}(H^{-1}G)+\|g -h\|_{H^{-1}}^{2}-d\] (Lemma 21) \[=D(G,H)+\|g-h\|_{H^{-1}}^{2}\] \[\geq\|g-h\|_{H^{-1}}^{2}\]

**Lemma 26**.: _For any \(\bm{H}\in\mathcal{H}^{\mathcal{A}_{0}}\), we have_

\[\textbf{Stability-1}=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}^{\mathcal{A}_ {0}}_{t}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}^{\mathcal{ A}_{0}}_{t})}{2\eta_{t}}\right]\leq 2d\sum_{t=1}^{T}\eta_{t}+\mathcal{O}(\delta T^{2})\]

Proof.: Recall that \(\bm{H}^{\mathcal{A}_{0}}_{t}=\widehat{\mathrm{Cov}}(p^{\mathcal{A}_{0}}_{t})\) and \(\widehat{\mathrm{Cov}}(p)=\begin{bmatrix}\mathrm{Cov}(p)+\mu(p)\mu(p)^{\top}& \mu(p)\\ \mu(p)^{\top}&1\end{bmatrix}\), we have

\[\left\langle\bm{H}^{\mathcal{A}_{0}}_{t}-\bm{H},\hat{\gamma}_{t} \right\rangle-\frac{D(\bm{H},\bm{H}^{\mathcal{A}_{0}}_{t})}{2\eta_{t}} \leq\left\langle x^{\mathcal{A}_{0}}_{t}-\mu(p),\hat{y}_{t}\right\rangle- \frac{\|\mu(p)-x^{\mathcal{A}_{0}}_{t}\|^{2}_{\mathrm{Cov}(p^{\mathcal{A}_{0} }_{t})^{-1}}}{2\eta_{t}}\] (Lemma 25) \[\leq\|x^{\mathcal{A}_{0}}_{t}-\mu(p)\|_{\mathrm{Cov}(p^{\mathcal{ A}_{0}}_{t})^{-1}}\|\hat{y}_{t}\|_{\mathrm{Cov}(p^{\mathcal{A}_{0}}_{t})}-\frac{\|\mu(p)-x ^{\mathcal{A}_{0}}_{t}\|^{2}_{\mathrm{Cov}(p^{\mathcal{A}_{0}}_{t})^{-1}}}{2 \eta_{t}}\] \[\leq\frac{\eta_{t}}{2}\|\hat{y}_{t}\|^{2}_{\mathrm{Cov}(p^{ \mathcal{A}_{0}}_{t})}\] (AM-GM inequality) \[=\frac{\eta_{t}}{2}\|\hat{\Sigma}^{-1}_{t}(a_{t}-\hat{x}_{t})\ell _{t}\|^{2}_{\mathrm{Cov}(p^{\mathcal{A}_{0}}_{t})}\] \[\leq\frac{\eta_{t}}{2}(a_{t}-\hat{x}_{t})^{\top}\hat{\Sigma}^{-1 }_{t}\,\mathrm{Cov}(p^{\mathcal{A}_{0}}_{t})\hat{\Sigma}^{-1}_{t}(a_{t}-\hat{x }_{t})\] ( \[|\ell_{t}|\leq 1\] ) \[=\frac{\eta_{t}}{2}\operatorname{Tr}\left((a_{t}-\hat{x}_{t})(a_ {t}-\hat{x}_{t})^{\top}\hat{\Sigma}^{-1}_{t}\,\mathrm{Cov}(p^{\mathcal{A}_{0}} _{t})\hat{\Sigma}^{-1}_{t}\right)\]

Since \(\mathbb{E}_{\mathcal{A}\sim\mathcal{D}}\mathbb{E}_{a\sim p^{\mathcal{A}}} \left[(a-\hat{x}_{t})(a-\hat{x}_{t})^{\top}\right]=H_{t}\), taking expectations over \(\mathcal{A}_{t}\), \(a_{t}\) and \(\mathcal{A}_{0}\) conditioned on \(\mathcal{E}_{t-1}\), we have

\[\mathbb{E}^{\mathcal{E}}_{t}\left[\left\langle\bm{H}^{\mathcal{A} _{0}}_{t}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}^{\mathcal{ A}_{0}}_{t})}{2\eta_{t}}\right] \leq\mathbb{E}^{\mathcal{E}}_{t}\left[\frac{\eta_{t}}{2} \operatorname{Tr}\left((a_{t}-\hat{x}_{t})(a_{t}-\hat{x}_{t})^{\top}\hat{ \Sigma}^{-1}_{t}\,\mathrm{Cov}(p^{\mathcal{A}_{0}}_{t})\hat{\Sigma}^{-1}_{t} \right)\right]\] \[=\mathbb{E}^{\mathcal{E}}_{t}\left[\frac{\eta_{t}}{2}\operatorname {Tr}\left(H_{t}\hat{\Sigma}^{-1}_{t}\mathbb{E}_{\mathcal{A}_{0}\sim D}\left[ \mathrm{Cov}(p^{\mathcal{A}_{0}}_{t})\right]\hat{\Sigma}^{-1}_{t}\right)\right].\]

Notice that given \(\mathcal{E}_{t-1}\),

\[\hat{\Sigma}_{t}\geq\frac{1}{2}H_{t}=\frac{1}{2}\mathbb{E}_{\mathcal{A}\sim D} [\mathrm{Cov}(p^{\mathcal{A}}_{t})]+\frac{1}{2}(\hat{x}_{t}-x_{t})(\hat{x}_{t} -x_{t})^{\top}\succeq\frac{1}{2}\mathbb{E}_{\mathcal{A}\sim D}[\mathrm{Cov}(p^ {\mathcal{A}}_{t})]\]

Hence we continue to upper bound the last expression by

\[\mathbb{E}^{\mathcal{E}}_{t}\left[\eta_{t}\operatorname{Tr}\left(H_{t}\hat{ \Sigma}^{-1}_{t}\hat{\Sigma}_{t}\hat{\Sigma}^{-1}_{t}\right)\right]\leq\mathbb{ E}^{\mathcal{E}}_{t}\left[\eta_{t}\operatorname{Tr}\left(H_{t}\hat{\Sigma}^{-1}_{t} \right)\right]\leq 2\eta_{t}d.\]

On the other hand, since \(\hat{\Sigma}_{t}\succeq\frac{1}{t}I\succeq\frac{1}{T}I\), we have trivial bound

\[\mathbb{E}_{t}\left[\left\langle\bm{H}^{\mathcal{A}_{0}}_{t}-\bm{H},\hat{ \gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}^{\mathcal{A}_{0}}_{t})}{2\eta_{ t}}\right|\left.\overline{\mathcal{E}_{t-1}}\right]\leq\mathcal{O}(T)\]

Combining everything, we get

\[\textbf{Stability-1} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}^{\mathcal{A}_{0 }}_{t}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}^{\mathcal{A}_{0 }}_{t})}{2\eta_{t}}\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle\bm{ H}^{\mathcal{A}_{0}}_{t}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}^{ \mathcal{A}_{0}}_{t})}{2\eta_{t}}\right]\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle\bm{ H}^{\mathcal{A}_{0}}_{t}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H},\bm{H}^{ \mathcal{A}_{0}}_{t})}{2\eta_{t}}\right.\bigg{|}\left.\mathcal{E}_{t-1}\right] \mathbb{I}\{\mathcal{E}_{t-1}\}\right]\] \[\qquad+\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\left\langle \bm{H}^{\mathcal{A}_{0}}_{t}-\bm{H},\hat{\gamma}_{t}\right\rangle-\frac{D(\bm{H}, \bm{H}^{\mathcal{A}_{0}}_{t})}{2\eta_{t}}\right.\bigg{|}\left.\overline{ \mathcal{E}_{t-1}}\right]\mathbb{I}\{\overline{\mathcal{E}_{t-1}}\}\right]\] \[\leq 2d\sum_{t=1}^{T}\eta_{t}+\mathcal{O}(\delta T^{2}).\]

### Bounding the Stability-2 term

Note that Lemma 8 does not require matrix \(A,B\) to be positive semi-definite. We will use it to prove the following lemma based on Lemma 34 in Dann et al. (2023b).

**Lemma 27**.: _If \(\eta_{t}\alpha_{t}\leq\frac{1}{64t}\), then_

\[\textbf{Stability-2}=\mathbb{E}\left[\sum_{t=1}^{T}\max_{\bm{H}\in\mathcal{H}^{ \mathcal{A}_{0}}}\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{H},-\alpha_{t} \bm{\hat{\Sigma}}_{t}^{-1}\right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{\mathcal{A} _{0}})}{2\eta_{t}}\right]\leq d\sum_{t=1}^{T}\alpha_{t}+\mathcal{O}\left(\delta T ^{2}\right)\]

Proof.: We first show that \(\max_{\bm{H}\in\mathcal{H}^{\mathcal{A}_{0}}}\left\langle\bm{H}_{t}^{\mathcal{ A}_{0}}-\bm{H},-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle-\frac{D(\bm{H}, \bm{H}_{t}^{\mathcal{A}_{0}})}{2\eta_{t}}\leq\frac{\alpha_{t}}{2}\|\bm{\hat{ \Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H}_{t}^{\mathcal{A}_{0}})}\).

Define

\[G(\bm{H})=\left\langle\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{H},-\alpha_{t}\bm{\hat{ \Sigma}}_{t}^{-1}\right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{\mathcal{A}_{0}})}{ 2\eta_{t}}\]

and \(\lambda=\|\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\). Since \(\bm{\hat{\Sigma}}_{t}\geq\frac{1}{t}I,\bm{H}_{t}^{\mathcal{A}_{0}}\preceq 2I\), \(\eta_{t}\alpha_{t}\leq\frac{1}{64t}\), we have

\[\eta_{t}\lambda=\eta_{t}\|\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2} F(\bm{H}_{t}^{\mathcal{A}_{0}})}=\eta_{t}\alpha_{t}\sqrt{\operatorname{Tr}(\bm{H}_{t}^ {\mathcal{A}_{0}}\bm{\hat{\Sigma}}_{t}^{-1}\bm{H}_{t}^{\mathcal{A}_{0}}\bm{ \hat{\Sigma}}_{t}^{-1})}\leq 2\eta_{t}\alpha_{t}t\leq\frac{1}{32}.\]

Let \(\bm{H}^{\prime}\) be the maximizer of \(G\). Since \(G(\bm{H}_{t}^{\mathcal{A}_{0}})=0\), we have \(G(\bm{H}^{\prime})\geq 0\). It suffices to show \(\|\bm{H}^{\prime}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\leq 16\eta_{t}\lambda\) because from Lemma 8, it leads to

\[G(\bm{H}^{\prime})\leq\|\bm{H}_{t}^{\mathcal{A}_{0}}-\bm{H}^{\prime}\|_{\nabla^ {2}F(\bm{H}_{t}^{\mathcal{A}_{0}})}\|\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\|_{ \nabla^{-2}F(\bm{H}_{t}^{\mathcal{A}_{0}})}\leq 16\eta_{t}\lambda\alpha_{t}\|\bm{ \hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H}_{t}^{\mathcal{A}_{0}})}=\frac{ \alpha_{t}}{2}\|\bm{\hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\]

To show \(\|\bm{H}^{\prime}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\leq 16\eta_{t}\lambda\), it suffices to show that for all \(\bm{U}\) such that \(\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{\mathcal{A}_ {0}})}=16\eta_{t}\lambda\), \(G(\bm{U})\leq 0\). This is because given this condition, if \(\|\bm{H}^{\prime}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}>16\eta_{t}\lambda\), then there is a \(\bm{U}\) in the line segment between \(\bm{H}_{t}^{\mathcal{A}_{0}}\) and \(\bm{H}^{\prime}\) such that \(\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{\mathcal{A}_ {0}})}=16\eta_{t}\lambda\). From the condition, \(G(\bm{U})\leq 0\leq\min\{G(\bm{H}_{t}^{\mathcal{A}_{0}}),G(\bm{H}^{\prime})\}\) which contradicts to the strictly concave of \(G\).

Now consider any \(\bm{U}\) such that \(\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{\mathcal{A}_ {0}})}=16\eta_{t}\lambda\). By Taylor expansion, there exists \(\bm{U}^{\prime}\) in the line segment between \(\bm{U}\) and \(\bm{H}_{t}^{\mathcal{A}_{0}}\) such that

\[G(\bm{U})\leq\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\|\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H }_{t}^{\mathcal{A}_{0}})}-\frac{1}{4\eta_{t}}\|\bm{U}-\bm{H}_{t}^{\mathcal{A} _{0}}\|_{\nabla^{2}F(\bm{U}^{\prime})}^{2}\]

We have \(\|\bm{U}^{\prime}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\leq\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{ H}_{t}^{\mathcal{A}_{0}})}=16\eta_{t}\lambda\leq\frac{1}{2}.\) From the Equation 2.2 in page 23 of Nemirovski (2004) (also appear in Eq.(5) of Abernethy et al. (2009)) and \(\log\det\) is a self-concordant function, we have \(\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{U}^{\prime})}^{2}\geq \frac{1}{4}\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}^{2}\). Thus, we have

\[G(\bm{U})\leq\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0}}\|_{\nabla^{2}F(\bm{H}_{t}^{ \mathcal{A}_{0}})}\|\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H}_{t }^{\mathcal{A}_{0}})}-\frac{1}{16\eta_{t}}\|\bm{U}-\bm{H}_{t}^{\mathcal{A}_{0} }\|_{(\bm{H}_{t}^{\mathcal{A}_{0}})^{-1}}^{2}=16\eta_{t}\lambda^{2}-\frac{(16 \eta_{t}\lambda)^{2}}{16\eta_{t}}=0\]

We have \(\|\bm{\hat{\Sigma}}_{t}^{-1}\|_{\nabla^{-2}F(\bm{H}_{t}^{\mathcal{A}_{0}})}= \sqrt{\operatorname{Tr}(\bm{H}_{t}^{\mathcal{A}_{0}}\bm{\hat{\Sigma}}_{t}^{-1} \bm{H}_{t}^{\mathcal{A}_{0}}\bm{\hat{\Sigma}}_{t}^{-1})}=\sqrt{\operatorname{Tr} ((\bm{H}_{t}^{\mathcal{A}_{0}}\bm{\hat{\Sigma}}_{t}^{-1})^{2})}\). Observe the following two facts: 1) all eigenvalues of \(\bm{H}_{t}^{\mathcal{A}_{0}}\bm{\hat{\Sigma}}_{t}^{-1}\) are non-negative since \(\bm{H}_{t}^{\mathcal{A}_{0}}\) and \(\bm{\hat{\Sigma}}_{t}^{-1}\) are both positive semi-definite, 2) for a square matrix \(A\) with all non-negative eigenvalues, \(\operatorname{Tr}(A^{2})\leq\operatorname{Tr}(A)^{2}\) because \(\operatorname{Tr}(A^{2})=\sum_{i}\lambda_{i}(A^{2})=\sum_{i}\lambda_{i}(A)^{2} \leq(This allows us to conclude

\[\mathbb{E}_{t}^{\mathcal{E}}\left[\frac{\alpha_{t}}{2}\|\bm{\hat{\Sigma}}_{t}^{-1} \|_{\nabla^{-2}F(\bm{H}_{t}^{A_{0}})}\right]\leq\frac{\alpha_{t}}{2}\mathbb{E}_ {t}^{\mathcal{E}}\left[\operatorname{Tr}(\bm{H}_{t}^{A_{0}}\bm{\hat{\Sigma}}_{t }^{-1})\right]\leq\alpha_{t}d\]

where we use that \(\bm{\hat{\Sigma}}_{t}\succeq\frac{1}{2}\mathbb{E}_{A_{0}\sim D}[\bm{H}_{t}^{A_ {0}}]\) given \(\mathcal{E}_{t-1}\).

On the other hand, since \(\bm{\hat{\Sigma}}_{t}\succeq\frac{1}{t}\bm{I}\succeq\frac{1}{T}\bm{I}\), for any \(t=1,\cdots,T\), we have trivial bound

\[\mathbb{E}_{t}\left[\max_{\bm{H}\in\mathcal{H}^{A_{0}}}\left\langle\bm{H}_{t}^ {A_{0}}-\bm{H},-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1}\right\rangle-\frac{D(\bm {H},\bm{H}_{t}^{A_{0}})}{2\eta_{t}}\ \middle|\ \overline{\mathcal{E}_{t-1}}\right]\leq \mathcal{O}(T)\]

Overall,

\[\textbf{Stability-2} =\mathbb{E}\left[\sum_{t=1}^{T}\max_{\bm{H}\in\mathcal{H}^{A_{0}} }\left\langle\bm{H}_{t}^{A_{0}}-\bm{H},-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1} \right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{A_{0}})}{2\eta_{t}}\right]\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\max_{\bm{H} \in\mathcal{H}^{A_{0}}}\left\langle\bm{H}_{t}^{A_{0}}-\bm{H},-\alpha_{t}\bm{ \hat{\Sigma}}_{t}^{-1}\right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{A_{0}})}{2\eta_ {t}}\right]\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\max_{\bm{H} \in\mathcal{H}^{A_{0}}}\left\langle\bm{H}_{t}^{A_{0}}-\bm{H},\hat{\gamma}_{t} \right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{A_{0}})}{2\eta_{t}}\ \middle|\ \mathcal{E}_{t-1}\right] \mathbb{I}\{\xi_{t-1}\}\right]\] \[\qquad+\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{E}_{t}\left[\max_{ \bm{H}\in\mathcal{H}^{A_{0}}}\left\langle\bm{H}_{t}^{A_{0}}-\bm{H},\hat{\gamma }_{t}\right\rangle-\frac{D(\bm{H},\bm{H}_{t}^{A_{0}})}{2\eta_{t}}\ \middle|\ \overline{\mathcal{E}_{t-1}} \right]\mathbb{I}\{\overline{\xi_{t-1}}\}\right]\] \[\leq d\sum_{t=1}^{T}\alpha_{t}+\mathcal{O}\left(\delta T^{2} \right).\]

### Bounding the Error term

**Lemma 28**.: \[\textbf{Error}=\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\overline{\bm{U}}^{A _{0}}-\bm{U}^{A_{0}},\hat{\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1} \right\rangle\right]\leq\mathcal{O}(1).\]

Proof.: Since \(\overline{\bm{U}}^{A_{0}}=\left(1-\frac{1}{T^{2}}\right)\bm{U}^{A_{0}}+\frac{1 }{T^{2}}\bm{H}_{*}^{A_{0}}\), and \(\hat{\Sigma}_{t}\succeq\frac{1}{T}\bm{I},\bm{\hat{\Sigma}}_{t}\succeq\frac{1} {T}\bm{I}\) we have

\[\textbf{Error} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\overline{\bm{U}}^{A_{ 0}}-\bm{U}^{A_{0}},\hat{\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{-1} \right\rangle\right]\] \[=\mathbb{E}\left[\frac{1}{T^{2}}\sum_{t=1}^{T}\left\langle-\bm{U} ^{A_{0}}+\bm{H}_{*}^{A_{0}},\hat{\gamma}_{t}-\alpha_{t}\bm{\hat{\Sigma}}_{t}^{- 1}\right\rangle\right]\] \[\leq\mathcal{O}(1).\]

### Finishing up

Recall the regret decomposition at the beginning of Appendix D. From Lemma 24, Lemma 26, Lemma 27, and Lemma 28, we have

\[\textbf{FTRL-Reg} =\textbf{Penalty}+\textbf{Stability-1}+\textbf{Stability-2}+\textbf{Error}\] \[\leq\mathcal{O}\left(\frac{d\log(T)}{\eta_{T}}+d\sum_{t=1}^{T} \eta_{t}+d\sum_{t=1}^{T}\alpha_{t}+\delta T^{2}\right)\]From Lemma 20 and Lemma 23, we can cancel out the additional regret induced by bias through the well-designed bonus term. Namely,

\[\mathbf{Bias}+\mathbf{Bonus} =\frac{1}{4}\sum_{t=1}^{T}\alpha_{t}\|x_{t}-u\|_{\Sigma_{t}^{-1}}^ {2}+\mathcal{O}\left(\sum_{t=1}^{T}\frac{d^{3}\log(T/\delta)}{\alpha_{t}t}+ \delta T^{2}\right)\] \[\qquad+2(d+2)\sum_{t=1}^{T}\alpha_{t}-\frac{1}{4}\sum_{t=1}^{T} \alpha_{t}\|u-x_{t}\|_{\Sigma_{t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1}^{T} \frac{d^{3}\alpha_{t}\log\frac{T}{\delta}}{t}+\delta\sum_{t=1}^{T}\alpha_{t}T\right)\] \[=\mathcal{O}\left(d\sum_{t=1}^{T}\alpha_{t}+\sum_{t=1}^{T}\frac{d ^{3}\log(T/\delta)}{\alpha_{t}t}+\sum_{t=1}^{T}\frac{d^{3}\alpha_{t}\log\left( T/\delta\right)}{t}+\delta T^{2}\right)\]

Thus, we have

\[\text{Reg} =\mathbf{Bias}+\mathbf{Bonus}+\mathbf{FTRL}-\text{Reg}\] \[=\mathcal{O}\left(\frac{d\log(T)}{\eta_{T}}+d\sum_{t=1}^{T}\eta_ {t}+d\sum_{t=1}^{T}\alpha_{t}+\sum_{t=1}^{T}\frac{d^{3}\log(T/\delta)}{\alpha_ {t}t}+\sum_{t=1}^{T}\frac{d^{3}\alpha_{t}\log\left(T/\delta\right)}{t}+\delta T ^{2}\right)\]

Recall that we have an additional condition in Lemma 27 such that for any \(t\), \(\eta_{t}\alpha_{t}\leq\frac{1}{64t}\). Picking \(\alpha_{t}=\frac{d}{\sqrt{t}},\eta_{t}=\frac{1}{64d\sqrt{t}}\) and \(\delta=\frac{1}{T^{2}}\), we get

\[\text{Reg}=\mathcal{O}\left(d^{2}\sqrt{T}\log(T)+d^{4}\log(T)\right)=\mathcal{ O}(d^{2}\sqrt{T}\log(T))\]

where we assume \(d^{2}\leq\sqrt{T}\) without loss of generality (otherwise the bound is vacuous).

## Appendix E Handling Misspecification

In this section, we discuss how to handle misspecification as defined in Section 3.6. In Appendix E.1, we study the case where the amount of misspecification \(\varepsilon\) is known by the learner. In Appendix E.2, we use a blackbox approach to turn it into an algorithm that achieves almost the same regret bound (up to \(\log T\) factors) without knowing \(\varepsilon\).

### Known misspecification

As discussed in Section 3.6, when the amount of misspecification \(\varepsilon\) is known, we still use Algorithm 1, but with different \(\alpha_{t}\) and \(\eta_{t}\). Throughout this subsection, we let \(\alpha_{t}=\frac{d}{\sqrt{t}}+\frac{\varepsilon}{\sqrt{d}}\) and \(\eta_{t}=\frac{1}{64\left(d\sqrt{t}+\frac{\varepsilon}{\sqrt{d}}t\right)}\), and point out the modifications of the analysis from Appendix D.

We start with the regret decomposition similar to that in Appendix D, but here we define

\[y_{t} =\operatorname*{argmin}_{y\in\mathbb{B}_{2}^{d}}\;\max_{\mathcal{ A}\in\operatorname*{supp}(D)}\max_{a\in\mathcal{A}}|f_{t}(a)-\langle a,y \rangle|,\] \[\varepsilon_{t} =\max_{\mathcal{A}\in\operatorname*{supp}(D)}\max_{a\in\mathcal{ A}}|f_{t}(a)-\langle a,y_{t}\rangle|,\] \[c_{t}(a) =f_{t}(a)-\langle a,y_{t}\rangle.\]The regret decomposition goes as follows:

\[\text{Reg}(u) =\mathbb{E}\left[\sum_{t=1}^{T}\left(f_{t}(a_{t})-f_{t}(u^{\mathcal{ A}_{t}})\right)\right]\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\left\langle a_{t}-u^{\mathcal{ A}_{t}},y_{t}\right\rangle\right]+\sum_{t=1}^{T}\varepsilon_{t}\] \[\leq\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H} _{t}^{\mathcal{A}_{t}}-\bm{U}^{\mathcal{A}_{t}},\gamma_{t}\right\rangle\right] }_{\text{\bf Bias}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{ H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_{t}\right\rangle \right]}_{\text{\bf Bonus}}\] \[\qquad+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{ H}_{t}^{\mathcal{A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\hat{\gamma}_{t}-\alpha_{t} \bm{\hat{\Sigma}}_{t}^{-1}\right\rangle\right]}_{\text{\bf FTRL-Reg}}+ \varepsilon T.\]

Now \(\hat{y}_{t}=\hat{\Sigma}_{t}^{-1}(a_{t}-\hat{x}_{t})\ell_{t}\) with \(\mathbb{E}[\ell_{t}]=a_{t}^{\top}y_{t}+c_{t}(a_{t})\).

For the **Bias** term, the proof is almost the same as Lemma 20. The only difference is that from the fourth line, we have

\[\mathbb{E}_{t}\left[(x_{t}-u)^{\top}\left(y_{t}-\hat{\Sigma}_{t}^{-1}(a_{t}- \hat{x}_{t})\left(a_{t}^{\top}y_{t}+c_{t}(a_{t})\right)\right)\right]\]

for some \(c_{t}(a_{t})\) such that \(|c_{t}(a_{t})|\leq\varepsilon_{t}\). This leads to an additional term of

\[\mathbb{E}_{t}^{\mathcal{E}}\left[-(x_{t}-u)^{\top}\hat{\Sigma}_{ t}^{-1}(a_{t}-\hat{x}_{t})c_{t}(a_{t})\right]\] \[\leq\mathbb{E}_{t}^{\mathcal{E}}\left[\sqrt{(x_{t}-u)^{\top}\hat {\Sigma}_{t}^{-1}c_{t}(a_{t})^{2}(a_{t}-\hat{x}_{t})(a_{t}-\hat{x}_{t})^{\top }\hat{\Sigma}_{t}^{-1}(x_{t}-u)}\right]\] \[\leq\mathbb{E}_{t}^{\mathcal{E}}\left[\sqrt{(x_{t}-u)^{\top}\hat {\Sigma}_{t}^{-1}\mathbb{E}_{\mathcal{A}_{t},a_{t}}\left[c_{t}(a_{t})^{2}(a_{t }-\hat{x}_{t})(a_{t}-\hat{x}_{t})^{\top}\right]\hat{\Sigma}_{t}^{-1}(x_{t}-u)}\right]\] \[\leq\mathbb{E}_{t}^{\mathcal{E}}\left[\varepsilon_{t}\sqrt{(x_{t }-u)^{\top}\hat{\Sigma}_{t}^{-1}\left(\mathbb{E}_{\mathcal{A}_{t},a_{t}}\left[ (a_{t}-\hat{x}_{t})(a_{t}-\hat{x}_{t})^{\top}\right]\right)\hat{\Sigma}_{t}^{- 1}(x_{t}-u)}\right]\] \[\leq\mathbb{E}_{t}^{\mathcal{E}}\left[\varepsilon_{t}\sqrt{(x_{ t}-u)^{\top}\hat{\Sigma}_{t}^{-1}H_{t}\hat{\Sigma}_{t}^{-1}(x_{t}-u)}\right]\] \[\leq\varepsilon_{t}\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}\]

Plugging it into the proof of Lemma 20, we have

\[\mathbb{E}_{t}^{\mathcal{E}}\left[\left\langle\bm{H}_{t}^{\mathcal{ A}_{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t}\right\rangle\right] \leq\mathcal{O}\left(\sqrt{\frac{d^{3}\log(T/\delta)}{t}}+ \varepsilon_{t}\right)\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}\] \[\leq\frac{\alpha_{t}}{4}\|x_{t}-u\|_{\hat{\Sigma}_{t}^{-1}}^{2}+ \mathcal{O}\left(\frac{d^{3}\log(T/\delta)}{\alpha_{t}t}+\frac{\varepsilon_{t }^{2}}{\alpha_{t}}\right)\]

Other parts of the proof follow those in Lemma 20. Finally, we get

\[\text{\bf Bias} =\mathbb{E}\left[\sum_{t=1}^{T}\left\langle\bm{H}_{t}^{\mathcal{A} _{0}}-\bm{U}^{\mathcal{A}_{0}},\gamma_{t}-\hat{\gamma}_{t}\right\rangle\right]\] \[\leq\frac{1}{4}\sum_{t=1}^{T}\alpha_{t}\|x_{t}-u\|_{\hat{\Sigma}_{ t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1}^{T}\frac{d^{3}\log(T/\delta)}{\alpha_{t}t}+ \sum_{t=1}^{T}\frac{\varepsilon_{t}^{2}}{\alpha_{t}}+\delta T^{2}\right)\]The **Bonus** term will not be affected, according to Lemma 23, we have

\[\textbf{Bonus}\leq 2(d+2)\sum_{t=1}^{T}\alpha_{t}-\frac{1}{4}\sum_{t=1}^{T} \alpha_{t}\|u-x_{t}\|_{\Sigma_{t}^{-1}}^{2}+\mathcal{O}\left(\sum_{t=1}^{T} \frac{d^{3}\alpha_{t}\log\left(T/\delta\right))}{t}+\delta T^{2}\right)\]

The **Penalty** term will not be affected, according to Lemma 24, we have

\[\frac{F(\overline{\bm{U}}^{A_{0}})-\min_{\bm{H}\in\mathcal{H}^{A_{0}}}F(\bm{H })}{\eta_{T}}\leq\frac{2d\log(T)}{\eta_{T}}\]

**Stability-1** term is also unchanged, as we assume that \(\ell_{t}\) still lies in \([-1,1]\) even under misspecification. We still have

\[\textbf{Stability-1}\leq\mathcal{O}\left(d\sum_{t=1}^{T}\eta_{t}+ \delta T^{2}\right)\]

The **Stability-2** term will not be affected as long as \(\eta_{t}\alpha_{t}\leq\frac{1}{64t}\). According to Lemma 27, we have

\[\textbf{Stability-2}\leq\mathcal{O}\left(d\sum_{t=1}^{T}\alpha_{t} +\delta T^{2}\right)\]

The **Error** term is also unaffected. We still have \(\textbf{Error}=\mathcal{O}(1)\).

Adding these terms together, the regret caused by bias and the negative term induced by bonus cancel out. We have

\[\text{Reg}=\mathcal{O}\left(\frac{d\log(T)}{\eta_{T}}+d\sum_{t=1} ^{T}(\eta_{t}+\alpha_{t})+\sum_{t=1}^{T}\frac{d^{3}\log(T/\delta)}{\alpha_{t}t }+\sum_{t=1}^{T}\frac{d^{3}\alpha_{t}\log\left(T/\delta\right)}{t}+\sum_{t=1}^ {T}\frac{\varepsilon_{t}^{2}}{\alpha_{t}}+\delta T^{2}\right)\]

Recall that we pick \(\alpha_{t}=\frac{d}{\sqrt{t}}+\frac{\varepsilon}{\sqrt{d}}\). \(\eta_{t}=\frac{1}{64d\sqrt{t}+64\frac{\varepsilon}{\sqrt{d}}t}\) and \(\delta=\frac{1}{T^{2}}\). This gives

\[\text{Reg}=\mathcal{O}(d^{2}\sqrt{T}\log(T)+d^{4}\log(T)+\sqrt{d }\varepsilon T)=\mathcal{O}(d^{2}\sqrt{T}\log(T)+\sqrt{d}\varepsilon T)\]

where we assume \(d^{2}\leq\sqrt{T}\) without loss of generality.

### Unknown misspecification

In this subsection, we use a model selection technique to convert the algorithm in Appendix E.1 which requires knowledge on \(\varepsilon\) into an algorithm that achieves a similar regret bound without knowing \(\varepsilon\). Such a procedure to handle unknown misspecification/corruption has appeared in several previous works (Foster et al., 2020; Wei et al., 2022), though we adopt the technique in Jin et al. (2023) to handle the adversarial case.

The idea here is a black-box reduction which turns an algorithm that only deals with known \(\varepsilon\) to one that handles unknown \(\varepsilon\). More specifically, the reduction has two layers. The bottom layer takes as input an arbitrary misspecification-robust algorithm that operates under known \(\varepsilon\) (e.g., Algorithm 1), and outputs a _stable_ misspecification-robust algorithm (formally defined later) that still operates under known \(\varepsilon\). The top layer follows the standard Corral idea and takes as input a stable algorithm that operates under known \(\varepsilon\), and outputs an algorithm that operates under unknown \(\varepsilon\). Below, we explain these two layers of reduction in details.

Bottom Layer (from an Arbitrary Algorithm to a Stable Algorithm)The input of the bottom layer is an arbitrary misspecification-robust algorithm, formally defined as:

**Definition 29**.: _An algorithm is misspecification-robust if it takes \(\theta\) as input, and achieves the following regret for any random stopping time \(t^{\prime}\leq T\) and any policy \(u\):_

\[\mathbb{E}\left[\sum_{t=1}^{t^{\prime}}(f_{t}(a_{t})-f_{t}(u^{ \mathcal{A}_{t}}))\right]\leq\mathbb{E}\left[c_{1}\sqrt{t^{\prime}}+c_{2} \theta\right]+\Pr\left[\varepsilon_{1;t^{\prime}}>\theta\right]T\]

_for problem-dependent and \(\log(T)\) factors \(c_{1},c_{2}\geq 1\) and \(\varepsilon_{1:t^{\prime}}\triangleq\sqrt{t^{\prime}\sum_{\tau=1}^{t^{\prime}} \varepsilon_{\tau}^{2}}\)._In our case, \(c_{1}=\Theta(d^{2}\log T)\) and \(c_{2}=\Theta(\sqrt{d})\). While the regret bound in Definition29 might look cumbersome, it is in fact fairly reasonable: if the guess \(\theta\) is not smaller than the true amount of \(\varepsilon_{1:t^{\prime}}\), the regret should be of order \(d^{2}\sqrt{t^{\prime}}+\sqrt{d}\theta\); otherwise, the regret bound is vacuous since \(T\) is its largest possible value. The only extra requirement is that the algorithm needs to be _anytime_ (i.e., the regret bound holds for any stopping time \(t^{\prime}\)), but even this is known to be easily achievable by using a doubling trick over a fixed-time algorithm. It is then clear that Algorithm1 (together with a doubling trick) indeed satisfies Definition29.

As mentioned, the output of the bottom layer is a stable robust algorithm. To characterize stability, we follow Agarwal et al. (2017) and define a new learning protocol that abstracts the interaction between the output algorithm of the bottom layer and the master algorithm from the top layer:

**Protocol 1**.: In every round \(t\), before the learner makes a decision, a probability \(w_{t}\in[0,1]\) is revealed to the learner. After making a decision, the learner sees the desired feedback from the environment with probability \(w_{t}\), and sees nothing with probability \(1-w_{t}\).

One can convert any misspecification-robust algorithm (defined in Definition29) into a stable misspecification-robust algorithm (characterized in Theorem30).

This conversion is achieved by a procedure that called STABILISE (see Algorithm3 for details). The high-level idea of STABILISE is as follows. Noticing that the challenge when learning in Protocol1 is that \(w_{t}\) varies over time, we discretize the value of \(w_{t}\) and instantiate one instance of the input algorithm to deal with one possible discretized value, so that it is learning in Protocol1 but with a _fixed_\(w_{t}\), making it straightforward to bound its regret based on what it promises in Definition29.

More concretely, STABILISE instantiates \(\mathcal{O}(\log_{2}T)\) instances \(\{\mathsf{ALG}_{j}\}_{j=0}^{\lceil\log_{2}T\rceil}\) of the input algorithm that satisfies Definition29, each with a different parameter \(\theta_{j}\). Upon receiving \(w_{t}\) from the environment, it dispatches round \(t\) to the \(j\)-th instance where \(j\) is such that \(w_{t}\in(2^{-j-1},2^{-j}]\), and uses the policy generated by \(\mathsf{ALG}_{j}\) to interact with the environment (if \(w_{t}\leq\frac{1}{T}\), simply ignore this round). Based on Protocol1, the feedback for this round is received with probability \(w_{t}\). To _equalize_ the probability of \(\mathsf{ALG}_{j}\) receiving feedback as mentioned in the high-level idea, when the feedback is actually obtained, STABILISE sends it to \(\mathsf{ALG}_{j}\) only with probability \(\frac{2^{-j-1}}{w_{t}}\) (and discards it otherwise). This way, every time \(\mathsf{ALG}_{j}\) is assigned to a round, it always receives the desired feedback with probability \(w_{t}\cdot\frac{2^{-j-1}}{w_{t}}=2^{-j-1}\). This equalization step allows us to use the original guarantee of the base algorithm (Definition29) and run it as it is, without requiring it to perform extra importance weighting steps as in Agarwal et al. (2017).

The choice of \(\theta_{j}\) is crucial in making sure that STABILISE only has \(\varepsilon T\) regret overhead instead of \(\frac{\varepsilon T}{\min_{t\in[T]}w_{t}}\). Since \(\mathsf{ALG}_{j}\) only receives feedback with probability \(2^{-j-1}\), the expected total misspecification it experiences is on the order of \(2^{-j-1}\varepsilon T\). Therefore, its input parameter \(\theta_{j}\) only needs to be of this order instead of the total amount of misspecification \(\varepsilon T\).

The formal guarantee of the conversion is stated in the following Theorem30.

**Theorem 30**.: _If an algorithm is misspecification robust according to Definition29 for some constants \((c_{1},c_{2})\), then Algorithm3 ensures_

\[\text{Reg}\leq\mathcal{O}\left(\mathbb{E}\left[c_{1}^{\prime}\sqrt{T\rho_{T}} \right]+c_{2}^{\prime}\varepsilon T\right)\]

_under Protocol1, where \(\rho_{T}=\frac{1}{\min_{t\in[T]}w_{t}}\), with \(c_{1}^{\prime}=\Theta((c_{1}+c_{2})\sqrt{\log T})\)._

Proof of Theorem30.: Define indicators

\[g_{t,j} =\mathbb{I}\{w_{t}\in(2^{-j-1},2^{-j}]\}\] \[h_{t,j} =\mathbb{I}\{\mathsf{ALG}_{j}\text{ receives the feedback for episode }t\}.\]

Now we consider the regret of \(\mathsf{ALG}_{j}\). Notice that \(\mathsf{ALG}_{j}\) makes an update only when \(g_{t,j}h_{t,j}=1\). By the guarantee of the base algorithm (Definition29), we have

\[\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-f_{t}(u^{\mathcal{A} _{t}}))g_{t,j}h_{t,j}\right]\] \[\leq\mathbb{E}\left[c_{1}\sqrt{\sum_{t=1}^{T}g_{t,j}h_{t,j}}+c_{2 }\theta_{j}\max_{t\leq T}g_{t,j}\right]+\Pr\left[\sqrt{\left(\sum_{t=1}^{T}g_{ t,j}h_{t,j}\right)\left(\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}h_{t,j} \right)}>\theta_{j}\right]T.\] (40)

We first bound the last term: Notice that \(\mathbb{E}[h_{t,j}|g_{t,j}]=2^{-j-1}g_{t,j}\) by Algorithm3. Therefore,

\[\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j }] =2^{-j-1}\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}\leq 2^{-j-1}\varepsilon^{2}T\] (41) \[\sum_{t=1}^{T}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j}] =2^{-j-1}\sum_{t=1}^{T}g_{t,j}\leq 2^{-j-1}T\] (42)

By Freedman's inequality, with probability at least \(1-\frac{1}{T^{2}}\),

\[\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}h_{t,j}-\sum_{t=1}^{T} \varepsilon_{t}^{2}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j}]\] \[\leq 2\sqrt{\sum_{t=1}^{T}(\varepsilon_{t})^{4}g_{t,j}\mathbb{E}[h _{t,j}|g_{t,j}]\log(T)}+4\log(T)\] \[\leq 4\sqrt{\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}\mathbb{E}[h _{t,j}|g_{t,j}]\log(T)}+4\log(T)\] \[\leq\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}\mathbb{E}[h_{t,j}|g_ {t,j}]+8\log(T)\] (AM-GM inequality)

which gives

\[\sum_{t=1}^{T}\varepsilon_{t}^{2}g_{t,j}h_{t,j}\leq 2\sum_{t=1}^{T} \varepsilon_{t}^{2}g_{t,j}\mathbb{E}[h_{t,j}|g_{t,j}]+8\log(T)\leq 2^{-j} \varepsilon^{2}T+8\log(T)\]

with probability at least \(1-\frac{1}{T^{2}}\) using Eq.41. Similarly,

\[\sum_{t=1}^{T}g_{t,j}h_{t,j}\leq 2\sum_{t=1}^{T}g_{t,j}\mathbb{E}[h_{t,j}|g_ {t,j}]+8\log(T)\leq 2^{-j}T+8\log(T)\]with probability at least \(1-\frac{1}{T^{2}}\). Therefore, with probability at least \(1-\frac{2}{T^{2}}\),

\[\sqrt{\left(\sum_{t=1}^{T}g_{t,j}h_{t,j}\right)\left(\sum_{t=1}^{T} \varepsilon_{t}^{2}g_{t,j}h_{t,j}\right)} \leq\sqrt{2^{-2j}\varepsilon^{2}T^{2}+16\cdot 2^{-j}T\log T+64 \log^{2}T}\] \[\leq 2^{-j}\varepsilon T+4\sqrt{2^{-j}T\log T}+8\log(T)\] \[\leq\theta_{j}\]

Therefore, the last term in Eq.40 is bounded by \(\frac{2}{T^{2}}T\leq\frac{2}{T}\).

Next, we deal with other terms in Eq.40. Again, by \(\mathbb{E}[h_{t,j}|g_{t,j}]=2^{-j-1}g_{t,j}\), Eq.40 implies

\[2^{-j-1}\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-f_{t}(u^{ \mathcal{A}_{t}}))g_{t,j}\right]\leq\mathbb{E}\left[c_{1}\sqrt{2^{-j-1}\sum_{ t=1}^{T}g_{t,j}}+c_{2}\theta_{j}\max_{t\leq T}g_{t,j}\right]+\frac{2}{T}.\]

which implies after rearranging:

\[\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-f_{t}(u^{\mathcal{A}_ {t}}))g_{t,j}\right]\] \[\leq\mathbb{E}\left[c_{1}\sqrt{\frac{1}{2^{-j-1}}\sum_{t=1}^{T}g _{t,j}}+\left(\frac{c_{2}\theta_{j}}{2^{-j-1}}\right)\max_{t\leq T}g_{t,j} \right]+\frac{2}{T^{2-j-1}}\] \[\leq\mathbb{E}\left[c_{1}\sqrt{\sum_{t=1}^{T}\frac{2g_{t,j}}{w_{ t}}}+4c_{2}\left(\varepsilon T+\sqrt{\frac{T\log T}{2^{-j}}}+\log T\right) \max_{t\leq T}g_{t,j}\right]+\frac{2}{T^{2-j-1}}.\] (using that when \[g_{t,j}=1\], \[\frac{1}{2^{-j-1}}\leq\frac{2}{w_{t}}\], and the definition of \[\theta_{j}\] )

Now, summing this inequality over all \(j\in\{0,1,\ldots,\lceil\log_{2}T\rceil\}\), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-f_{t}(u^{\mathcal{A}_ {t}}))\mathbb{I}\left\{w_{t}>\frac{1}{T}\right\}\right]\] \[\leq\mathcal{O}\left(\mathbb{E}\left[c_{1}\sqrt{N\sum_{t=1}^{T} \frac{1}{w_{t}}}+Nc_{2}\varepsilon T+c_{2}\sqrt{\frac{T\log T}{\min_{t\leq T} w_{t}}}+c_{2}N\log T\right]+1\right)\] \[\leq\mathcal{O}\left(\mathbb{E}\left[(c_{1}+c_{2})\sqrt{T\log(T )\rho_{T}}\right]+c_{2}\varepsilon T\log T\right)\]

where \(N\leq\mathcal{O}(\log T)\) is the number of \(\mathsf{ALG}_{j}\)'s that has been executed at least once.

On the other hand,

\[\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-f_{t}(u^{\mathcal{A}_{t}})) \mathbb{I}\left\{w_{t}\leq\frac{1}{T}\right\}\right]<T\mathbb{E}\left[\mathbb{ I}\left\{\rho_{T}\geq T\right\}\right]\leq\mathbb{E}\left[\rho_{T}\right].\]

Combining the two parts and using the assumption \(c_{2}\geq 1\) finishes the proof. 

Top Layer (from Known \(\varepsilon\) to Unknown \(\varepsilon\))In this subsection, we use the algorithm that we construct in Theorem30 as a base algorithm, and further construct an algorithm with \(\sqrt{T}+\varepsilon\) regret under unknown \(\varepsilon\). The idea is to run multiple base algorithms, each with a different hypothesis on \(\varepsilon\); on top of them, run another multi-armed bandit algorithm to adaptively choose among them. The goal is to let the top-level bandit algorithm perform almost as well as the best base algorithm. This is the Corral idea outlined in Agarwal et al. (2017); Foster et al. (2020); Luo et al. (2022), and the algorithm is presented in Algorithm4.

**Theorem 31**.: _Using an algorithm constructed in Theorem30 as a base algorithm, Algorithm4 ensures \(\operatorname{Reg}=\mathcal{O}\left(c_{1}^{\prime}\sqrt{T\log^{3}T}+c_{2}^{ \prime}\varepsilon T\right)\) without knowing \(\varepsilon\)._The top-level bandit algorithm is an FTRL with log-barrier regularizer. We first state the standard regret bound of FTRL under log-barrier regularizer, whose proof can be found in, e.g., Theorem 7 of Wei and Luo (2018).

**Lemma 32**.: _The FTRL algorithm over a convex subset \(\Omega\) of the \((M-1)\)-dimensional simplex \(\Delta(M)\):_

\[w_{t}=\operatorname*{argmin}_{w\in\Omega}\left\{\left\langle w,\sum_{\tau=1}^ {t-1}\ell_{\tau}\right\rangle+\frac{1}{\eta}\sum_{i=1}^{M}\log\frac{1}{w_{i}}\right\}\]

_ensures for all \(u\in\Omega\),_

\[\sum_{t=1}^{T}\langle w-u,\ell_{t}\rangle\leq\frac{M\log T}{\eta}+\eta\sum_{t =1}^{T}\sum_{i=1}^{M}w_{t,i}^{2}\ell_{t,i}^{2}\]

_as long as \(\eta w_{t,i}|\ell_{t,i}|\leq\frac{1}{2}\) for all \(t,i\)._

Proof of Theorem 31.: The Corral algorithm is essential an FTRL with log-barrier regularizer. To apply Lemma 32, we first verify the condition \(\eta w_{t,i}|\ell_{t,i}|\leq\frac{1}{2}\) where \(\ell_{t,i}=\hat{z}_{t,i}-r_{t,i}\). By our choice of \(\eta\),

\[\eta w_{t,i}|\hat{z}_{t,i}| \leq\eta z_{t,i}\leq\frac{1}{4},\] (because \[c_{1}^{\prime}\geq 1\] ) \[\eta w_{t,i}r_{t,i} =\eta c_{1}^{\prime}\sqrt{T}w_{t,i}(\sqrt{\rho_{t,i}}-\sqrt{\rho _{t-1,i}}).\]

[MISSING_PAGE_FAIL:41]

Combining the two terms and using \(\eta=\Theta\left(\frac{1}{c_{i}^{\prime}\sqrt{T+c_{z}^{\prime}}}\right)\), \(M=\Theta(\log T)\), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-z_{t,i^{*}})\right] =\mathbb{E}\left[\sum_{t=1}^{T}(z_{t,i_{t}}-z_{t,i^{*}})\right]\] \[=\mathcal{O}\left(c_{1}^{\prime}\sqrt{T\log^{3}T}\right)-\mathbb{ E}\left[c_{1}^{\prime}\sqrt{\rho_{T,i^{*}}T}\right]\] (44)

On the other hand, by the guarantee of the base algorithm (Theorem 30) and that \(\varepsilon T\in[2^{i^{*}-1},2^{i^{*}}]\), we have

\[\mathbb{E}\left[\sum_{t=1}^{T}(z_{t,i^{*}}-f_{t}(u^{\mathcal{A}_{ t}})\right]\leq\mathbb{E}\left[c_{1}^{\prime}\sqrt{\rho_{T,i^{*}}T}\right]+c_{2}^ {\prime}\varepsilon T.\] (45)

Combining Eq. (44) and Eq. (45), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}(f_{t}(a_{t})-f_{t}(u^{\mathcal{A}_ {t}}))\right]\leq\mathcal{O}\left(c_{1}^{\prime}\sqrt{T\log^{3}T}\right)+c_{2} ^{\prime}\varepsilon T,\]

which finishes the proof. 

Proof of Theorem 3.: As shown in Appendix E.1, our Algorithm 1 can be adapted to satisfy Definition 29 with \(c_{1}=\Theta(d^{2}\log T)\) and \(c_{2}=\Theta(\sqrt{d})\). By a concatenation of Theorem 30 and Theorem 31, we conclude that there is an algorithm that achieves

\[\mathcal{O}\left((c_{1}+c_{2})\sqrt{T}\log^{2}T+c_{2}\varepsilon T \log T\right)=\mathcal{O}\left(d^{2}\sqrt{T}\log^{2}T+\sqrt{d}\varepsilon T \log T\right).\]

regret under unknown \(\varepsilon\). 

## Appendix F Analysis for Linear EXP4

Proof of Theorem 4.: We first show that

\[\forall\pi\in\Pi:\ \ \text{Reg}(\pi)\triangleq\mathbb{E}\left[\sum_{t=1}^{ T}a_{t}^{\top}y_{t}-\sum_{t=1}^{T}\pi(\mathcal{A}_{t})^{\top}y_{t}\right]\leq \mathcal{O}\left(\gamma T+\frac{\ln|\Pi|}{\eta}+\eta dT\right)\,.\] (46)

The magnitude of the loss is bounded by

\[|\hat{\ell}_{t,\pi}| =\left|\left\langle\pi(\mathcal{A}_{t}),\tilde{H}_{t}^{-1}a_{t} \ell_{t}\right\rangle\right|\] \[\leq\|\pi(\mathcal{A}_{t})\|_{\tilde{H}_{t}^{-1}}\|a_{t}\|_{ \tilde{H}_{t}^{-1}}\] \[\leq\frac{1}{\gamma}\left\|\pi(\mathcal{A}_{t})\right\|_{G_{t}^{- 1}}\|a_{t}\|_{G_{t}^{-1}}\leq\frac{d}{\gamma}\,.\]

If \(\gamma\geq 2d\eta\), then we have \(|\hat{\ell}_{t,\pi}|\leq\frac{1}{2}\) and we can use the standard regret bound of exponential weights:

\[\forall\pi\in\Pi:\qquad\text{Reg}(\pi)\leq\gamma T+\frac{\ln|\Pi|}{\eta}+ \eta\sum_{t=1}^{T}\mathbb{E}\left[\mathbb{E}_{a_{t}\sim p_{t}}\left[\sum_{ \pi\in\Pi}P_{t,\pi}\hat{\ell}_{t,\pi}^{2}\right]\right]\,.\]

Let \(H_{t}=\mathbb{E}_{a\sim p_{t}}[aa^{\top}]\). Then we have \(\tilde{H}_{t}^{-1}\preceq\frac{1}{1-\gamma}H_{t}^{-1}\), and thus

\[\mathbb{E}_{a_{t}\sim p_{t}}\left[\sum_{\pi\in\Pi}P_{t,\pi}\hat {\ell}_{t,\pi}^{2}\right] \leq\mathbb{E}_{a_{t}\sim p_{t}}\left[\sum_{\pi\in\Pi}P_{t,\pi} \cdot\langle\pi(\mathcal{A}_{t}),\tilde{H}_{t}^{-1}a_{t}\rangle^{2}\right]\] \[=\mathbb{E}_{a_{t}\sim p_{t}}\mathbb{E}_{a\sim p_{t}}\left[ \langle a,\tilde{H}_{t}^{-1}a_{t}\rangle^{2}\right]\qquad\qquad\text{(by the definition of $p_{t,a}$)}\] \[\leq\frac{1}{(1-\gamma)^{2}}\operatorname{Tr}\left(H_{t}H_{t}^{- 1}H_{t}H_{t}^{-1}\right)=\mathcal{O}(d)\,.\]Combining all proves Eq.46.

Next, we show that there exists \(\theta\in\Theta\) such that

\[\mathbb{E}_{\mathcal{A}\sim D}\left[\sum_{t=1}^{T}(\pi_{\theta}( \mathcal{A})-\pi^{\star}(\mathcal{A}))^{\top}y_{t}\right]\leq\mathcal{O}(1).\] (47)

Let \(\hat{\theta}\) be the closest element in \(\Theta\) to \(\sum_{t=1}^{T}y_{t}\). By the definition of \(\Theta\) and the assumption that \(\|y_{t}\|\leq 1\), we have \(\left\|\hat{\theta}-\sum_{t=1}^{T}y_{t}\right\|\leq\epsilon\). Thus, for any \(\mathcal{A}\),

\[\sum_{t=1}^{T}(\pi_{\hat{\theta}}(\mathcal{A})-\pi^{\star}( \mathcal{A}))^{\top}y_{t}\leq\sum_{a\in\mathcal{A}}(\pi_{\hat{\theta}}( \mathcal{A})-\pi^{\star}(\mathcal{A}))^{\top}\hat{\theta}+\epsilon\leq\epsilon\]

where the last inequality is by the fact that \(\pi_{\hat{\theta}}(\mathcal{A})=\operatorname*{argmin}_{a\in\mathcal{A}}a^{ \top}\hat{\theta}\). Taking expectation over \(\mathcal{A}\) gives Eq.47.

Finally, combining Eq.46 and Eq.47, choosing \(\epsilon=1\) and \(\gamma=2d\eta=2d\sqrt{\frac{\log T}{T}}\), we get

\[\text{Reg} =\mathbb{E}\left[\sum_{t=1}^{T}a_{t}^{\top}y_{t}-\sum_{t=1}^{T} \pi^{\star}(\mathcal{A}_{t})^{\top}y_{t}\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}a_{t}^{\top}y_{t}-\sum_{t=1}^{T} \pi_{\hat{\theta}}(\mathcal{A}_{t})^{\top}y_{t}\right]+\mathbb{E}_{\mathcal{ A}\sim D}\left[\sum_{t=1}^{T}(\pi_{\hat{\theta}}(\mathcal{A})-\pi^{\star}( \mathcal{A}))^{\top}y_{t}\right]\] \[=\mathcal{O}\left(\gamma T+\frac{\ln((2T)^{d})}{\eta}+\eta dT+1\right)\] \[=\mathcal{O}\left(d\sqrt{T\log T}\right),\]

finishing the proof. 

## Appendix G Comparison with Dai et al. (2023) and Sherman et al. (2023)

We state the exponential weight algorithm adopted by Luo et al. (2021); Dai et al. (2023); Sherman et al. (2023) in Algorithm5, which is an algorithm that we know to achieve the prior-art regret bound in our setting (though they studied a more general MDP setting).

Their algorithm proceeds in _epochs_ (indexed by \(k\)), where every epoch consists of \(W\) rounds. The policy on action set \(\mathcal{A}\) in the \(k\)-th epoch is defined as

\[p_{k}^{\mathcal{A}}(a)\propto\exp\left(-\eta\sum_{s=1}^{k-1}(a^{ \top}\hat{y}_{s}-b_{s}(a))\right)\]

where \(\hat{y}_{k}\) is the loss estimator for epoch \(k\), and \(b_{k}(a)\) is a (non-linear) bonus. In all \(W\) rounds in epoch \(k\), the same policy is executed. The samples obtained in these \(W\) rounds are randomly divided into two halfs. One half is used to estimate the covariance matrix \(\hat{\Sigma}_{k}\), and the other half is used to construct the loss estimator \(\hat{y}_{k}\) (see Line5 of Algorithm5).

```
1for\(k=1,2,\ldots,\frac{T}{W}\)do
2 For all \(\mathcal{A}\), define \[p_{k}^{\mathcal{A}}(a)=\frac{\exp\left(-\eta\sum_{s=1}^{k-1}(a^{\top}\hat{y}_{s}- b_{s}(a))\right)}{\sum_{a^{\prime}\in\mathcal{A}}\exp\left(-\eta\sum_{s=1}^{k-1}(a^{ \prime\top}\hat{y}_{s}-b_{s}(a^{\prime}))\right)}\qquad\text{for all }a\in\mathcal{A}.\]
3 Randomly partition \(\{(k-1)W+1,\ldots,kW\}\) into two equal parts \(\mathcal{T}_{k},\mathcal{T}_{k}^{\prime}\).
4for\(t=(k-1)W+1,\ldots,kW\)do
5 receive \(\mathcal{A}_{t}\), sample \(a_{t}\sim p_{k}^{\mathcal{A}_{t}}\), and receive \(\ell_{t}\).
6 Define \[\hat{\Sigma}_{k} =\beta I+\frac{1}{|\mathcal{T}_{k}|}\sum_{t\in\mathcal{T}_{k}}a_{ t}a_{t}^{\top}\] \[\hat{y}_{k} =\hat{\Sigma}_{k}^{-1}\left(\frac{1}{|\mathcal{T}_{k}^{\prime}|} \sum_{t\in\mathcal{T}_{k}^{\prime}}a_{t}\ell_{t}\right)\] \[b_{k}(a) =\alpha\|a\|_{\hat{\Sigma}_{k}^{-1}}.\]
7 ```

**Algorithm 5** Exponential weights with magnitude-reduced loss estimators

### Regret Analysis Sketch

The regret analysis starts with a standard decomposition that is similar to ours. We abuse the notation by defining \(y_{k}=\frac{1}{W}\sum_{t=(k-1)W}^{kW}y_{t}\). Then

\[\text{Reg} =W\mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^{\mathcal{A}_{0}}(a) \langle a-u^{\mathcal{A}_{0}},y_{k}\rangle\right]\] \[=\underbrace{W\mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^{\mathcal{A}_ {0}}(a)\Big{(}\langle a,\hat{y}_{k}\rangle-b_{k}(a)\Big{)}-\Big{(}u^{\mathcal{ A}_{0}}-b_{k}(u^{\mathcal{A}_{0}})\Big{)}\right]}_{\text{EW-Reg}}+\underbrace{W \mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^{\mathcal{A}_{0}}(a)b_{k}(a)-b_{k}(u^{ \mathcal{A}_{0}})\right]}_{\text{Bonus}}\] \[\qquad\qquad+\underbrace{W\mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^ {\mathcal{A}_{0}}(a)\langle a-u^{\mathcal{A}_{0}},y_{k}-\hat{y}_{k}\rangle \right]}_{\text{Bias}}.\]

Bounding the regret term follows the standard analysis of exponential weight:

\[\text{EW-Reg} \leq W\mathbb{E}\left[\frac{\ln|\mathcal{A}_{0}|}{\eta}+\eta \sum_{k=1}^{T/W}\sum_{a\in\mathcal{A}_{0}}p_{k}^{\mathcal{A}_{0}}(a)\langle a,\hat{y}_{k}\rangle^{2}+\eta\sum_{k=1}^{T/W}\sum_{a\in\mathcal{A}_{0}}p_{k}^{ \mathcal{A}_{0}}(a)b_{k}(a)^{2}\right]\] \[\leq W\mathbb{E}\left[\frac{\ln|\mathcal{A}_{0}|}{\eta}+\eta \sum_{k=1}^{T/W}\sum_{a\in\mathcal{A}_{0}}p_{k}^{\mathcal{A}_{0}}(a)a^{\top} \hat{\Sigma}_{k}^{-1}H_{k}\hat{\Sigma}_{k}^{-1}a+\eta\sum_{k=1}^{T/W}\frac{ \alpha^{2}}{\beta}\right]\]

where \(H_{k}=\mathbb{E}_{\mathcal{A}\sim D}\mathbb{E}_{a\sim p_{k}^{\mathcal{A}}}[aa^ {\top}]\). Then they use the following fact to bound the stability term: as long as \(W\geq\frac{d}{\beta^{2}}\), it holds with high probability that \(\hat{\Sigma}_{k}^{-1}H_{k}\hat{\Sigma}_{k}^{-1}\preceq 2\hat{\Sigma}_{k}^{-1}\). Thus **EW-Reg** can be further bounded by

\[\textbf{EW-Reg} \lesssim W\left(\frac{\ln|\mathcal{A}_{0}|}{\eta}+\eta\mathbb{E} \left[\sum_{k=1}^{T/W}\sum_{a\in\mathcal{A}_{0}}p_{k}^{\mathcal{A}_{0}}(a)\|a \|_{\hat{\Sigma}_{k}^{-1}}^{2}\right]+\eta\frac{T}{W}\frac{\alpha^{2}}{\beta}\right)\] \[\leq\frac{W\ln|\mathcal{A}_{0}|}{\eta}+\eta dT+\eta T\frac{ \alpha^{2}}{\beta}.\]

By the definition of the bonus function \(b_{t}\), it holds that

\[\textbf{Bonus}=W\mathbb{E}\left[\alpha\sum_{k=1}^{T/W}\sum_{a\in\mathcal{A}_{ 0}}p_{k}^{\mathcal{A}_{0}}(a)\|a\|_{\hat{\Sigma}_{k}^{-1}}\right]-W\mathbb{E} \left[\alpha\sum_{k=1}^{T/W}\|u^{\mathcal{A}_{0}}\|_{\hat{\Sigma}_{k}^{-1}} \right].\]

Finally, the bias term can be bounded as follows:

\[\textbf{Bias} =W\mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^{\mathcal{A}_{0}}(a)(a-u^ {\mathcal{A}_{0}})^{\top}(y_{k}-\hat{\Sigma}_{k}^{-1}H_{k}y_{k})\right]\] \[=W\mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^{\mathcal{A}_{0}}(a)(a-u^ {\mathcal{A}_{0}})^{\top}\hat{\Sigma}_{k}^{-1}(\hat{\Sigma}_{k}-H_{k})y_{k}\right]\] \[\leq W\mathbb{E}\left[\sum_{k=1}^{T/W}p_{k}^{\mathcal{A}_{0}}(a)\|a -u^{\mathcal{A}_{0}}\|_{\hat{\Sigma}_{k}^{-1}}\|(\hat{\Sigma}_{k}-H_{k})y_{k} \|_{\hat{\Sigma}_{k}^{-1}}\right].\]

The bias here has a similar form as in our case. They use the following fact to bound the bias: as long as \(W\geq\frac{d}{\beta^{2}}\), it holds that \(\|(\hat{\Sigma}_{k}-H_{k})y_{k}\|_{\hat{\Sigma}_{k}^{-1}}\leq\sqrt{\beta d}\). Therefore, the bias can further be upper bounded by

\[\textbf{Bias}\leq W\mathbb{E}\left[\sqrt{\beta d}\sum_{k=1}^{T/W}\sum_{a\in \mathcal{A}_{0}}p_{k}^{\mathcal{A}_{0}}(a)\|a\|_{\hat{\Sigma}_{k}^{-1}}+\sqrt {\beta d}\sum_{k=1}^{T/W}\|u^{\mathcal{A}_{0}}\|_{\hat{\Sigma}_{k}^{-1}} \right].\]

Combining the three parts, we get that the overall regret is of order

\[\mathbb{E}\left[\frac{W\ln|\mathcal{A}_{0}|}{\eta}+\eta dT+\eta T \frac{\alpha^{2}}{\beta}+W(\alpha+\sqrt{\beta d})\sum_{k=1}^{T/W}\sum_{a\in \mathcal{A}_{0}}p_{k}^{\mathcal{A}_{0}}(a)\|a\|_{\hat{\Sigma}_{k}^{-1}}+W( \sqrt{\beta d}-\alpha)\sum_{k=1}^{T/W}\|u^{\mathcal{A}_{0}}\|_{\hat{\Sigma}_{ k}^{-1}}\right].\]

Choosing \(\alpha\approx\sqrt{\beta d}\), we further bound it by

\[\mathbb{E}\left[\frac{W\ln|\mathcal{A}_{0}|}{\eta}+\eta dT+W\sqrt {\beta d}\sum_{k=1}^{T/W}\sum_{a\in\mathcal{A}_{0}}p_{k}^{\mathcal{A}_{0}}(a) \|a\|_{\hat{\Sigma}_{k}^{-1}}\right]\] \[\leq\mathbb{E}\left[\frac{W\ln|\mathcal{A}_{0}|}{\eta}+\eta dT+W \sqrt{\beta d}\sum_{k=1}^{T/W}\sqrt{\sum_{a\in\mathcal{A}_{0}}p_{k}^{\mathcal{ A}_{0}}(a)\|a\|_{\hat{\Sigma}_{k}^{-1}}^{2}}\right]\] \[\leq\frac{W\ln|\mathcal{A}_{0}|}{\eta}+\eta dT+\sqrt{\beta}dT.\]

Recall the constraint \(W\geq\frac{d}{\beta^{2}}\). Choosing \(W=\frac{d}{\beta^{2}}\) gives

\[\frac{d\ln|\mathcal{A}_{0}|}{\eta\beta^{2}}+\eta dT+\sqrt{\beta}dT\] (48)

which gives \(d(\ln|\mathcal{A}_{0}|)^{\frac{1}{6}}T^{\frac{5}{6}}\) with the optimally chosen \(\eta\) and \(\beta\).

RemarkDue to the restrictions on the magnitude of the loss estimator required by the exponential weight algorithm, there is actually another constraint \(\frac{\eta}{\beta}\leq 1\), which makes Eq. (48) be \(d(\ln|\mathcal{A}_{0}|)\,\ddagger\,T^{\frac{1}{\delta}}\) at best. This is exactly the bound obtained by Sherman et al. (2023). A more sophisticated way to construct \(\hat{y}_{k}\) developed by Dai et al. (2023) removes this additional requirement and allows a bound of \(d(\ln|\mathcal{A}_{0}|)\,\ddagger\,T^{\frac{1}{\delta}}\). The sub-optimal bound \(T^{\frac{1}{\delta}}\) reported in Dai et al. (2023) is due to issues related to MDPs, which are not presented in the contextual bandit case here.