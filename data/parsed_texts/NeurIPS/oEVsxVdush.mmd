# Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products

Bethia Sun

Correspondence to: bethia.sun@unsw.edu.au

Maurice Pagnucco

School of Computer Science and Engineering

UNSW Sydney

Yang Song

School of Computer Science and Engineering

UNSW Sydney

###### Abstract

Since the inception of the classicalist vs. connectionist debate, it has been argued that the ability to systematically combine symbol-like entities into compositional representations is crucial for human intelligence. In connectionist systems, the field of disentanglement has gained prominence for its ability to produce explicitly compositional representations; however, it relies on a fundamentally _symbolic, concatenative_ representation of compositional structure that clashes with the _continuous, distributed_ foundations of deep learning. To resolve this tension, we extend Smolensky's Tensor Product Representation (TPR) and introduce _Soft TPR_, a representational form that encodes compositional structure in an inherently _distributed, flexible_ manner, along with _Soft TPR Autoencoder_, a theoretically-principled architecture designed specifically to learn Soft TPRs. Comprehensive evaluations in the visual representation learning domain demonstrate that the Soft TPR framework consistently outperforms conventional disentanglement alternatives - achieving state-of-the-art disentanglement, boosting representation learner convergence, and delivering superior sample efficiency and low-sample regime performance in downstream tasks. These findings highlight the promise of a _distributed_ and _flexible_ approach to representing compositional structure by potentially enhancing alignment with the core principles of deep learning over the conventional symbolic approach.

## 1 Introduction

Compositional structure, capturing the property of being decomposable into a set of constituent parts, permeates numerous aspects of our world - from the recursive application of syntax in language, to the parsing of richly complex visual scenes into their constituent parts. Given this ubiquity, it is natural to seek deep learning representations that also embody compositional structure. Indeed, empirical evidence demonstrates a multitude of benefits conferred by explicitly compositional representations, including increased interpretability [10; 12], reduced sample complexity [30; 33], increased fairness [20; 25; 41], and improved performance in out-of-distribution generalisation [33; 48; 50].

We consider the following, intuitive notion of compositional representations. A representation of compositionally-structured data is a _compositional representation_ if its structure explicitly reflects the constituency structure of the represented data [49]. In the visual representation learning domain, data is clearly _compositionally-structured_, as images can be decomposed into a set of constituent _factors of variation_ (FoVs), e.g., \(\{\mathrm{magenta\,floor,orange\,wall,\,aqua\,object\,\,colour,\,oblong\,object\,shape}\}\) for the image in Figure 1.

A widely explored framework for learning explicitly compositional representations is that of _disentanglement_. We adopt the conventional [5; 25; 33; 43], intuitive definition of a _disentangled representation_, which states a representation, \(\psi(x)\), is disentangled if each data constituent (FoV) can be mapped onto a distinct dimension or contiguous group of dimensions in \(\psi(x)\), effectively establishing a 1-1 correspondence between FoVs and distinct representational _parts_[43]. Framed in this way, it is clear that disentangled representations explicitly encode the data's constituency structure and are thus, compositional representations by nature. The majority of state-of-the-art disentanglement approaches use a variational autoencoder backbone, and rely on weak supervision [13; 22; 31; 33; 35], or a penalisation of the aggregate posterior \(\int q(z|x)p(x)dx\)[10; 14; 17; 24; 26; 32] to promote disentanglement. More recent approaches depart from the restrictive assumptions of a variational framework, and instead use standard autoencoding [47], or energy-function based optimisation [36], with additional inductive biases to encourage disentanglement. Despite their methodological diversity, disentanglement methods share a unifying principle: by enforcing the 1-1 correspondence between FoVs and distinct "slots" in the representation, they produce compositional representations as a dimension-wise _concatenation_ of scalar-valued or vector-valued FoV _tokens_[10; 13; 14; 17; 22; 24; 26; 31; 32; 33; 35; 36; 47], as illustrated in Figure 1a. This design utilises a fundamentally localist encoding scheme, where discrete parts of the representation are exclusively dedicated to encoding _single_ FoVs, paralleling symbolic representations, in which discrete representational slots are occupied by individual symbols - e.g., the symbols \(\{\text{'c'},\text{ 'a'},\text{ 't'}\}\) in the word '\(\text{cat'}\). We theorise that this inherently _symbolic_ method, offering a _localist_ encoding of compositional structure, may be misaligned with the continuous, distributed nature of deep learning models for the following reasons (see A.3 for further details):

1. **Gradient Flow and Learning**: Symbolic compositional representations utilise a localist encoding approach, assigning each FoV to a unique, non-overlapping subset of the dimensions of the representation. This modular separation may introduce practical misalignments with gradient-based optimisation. By restricting each FoV to its own slot, gradients for one FoV are confined to a set of designated dimensions, limiting the smooth propagation of gradients globally across _all_ dimensions of the representation. Consequently, updates to a single FoV provide minimal feedback to other FoVs, hindering the model's ability to jointly refine interdependent components. Furthermore, the localist encoding - where each FoV occupies a small, disjoint subset of the total dimensions - can induce abrupt, discontinuous shifts in the representation when transitioning between FoV updates, potentially complicating convergence.
2. **Representational Expressivity**: The localist encoding inherent in symbolic compositional structures, where each FoV is assigned a distinct subset of the overall dimensions, may constrain practical expressiveness. Specifically, confining each of the \(n\) FoVs to a subset of size \(n/d\) within a \(d\)-dimensional representation causes each FoV to underutilise the full capacity of the \(d\)-dimensional space, potentially limiting the richness and flexibility of the learned representations.
3. **Robustness to Noise**: By confining each FoV to a small, non-overlapping subset of dimensions, symbolic compositional representations become highly vulnerable to dimension-wise noise. Even slight perturbations in a single dimension can significantly impair the representation of the corresponding FoV, as there is no overlapping or redundant encoding to mitigate such disruptions.

Critically, we hypothesise that this potential incompatibility between symbolic, localist representations and the distributed, continuous nature of deep learning results in suboptimal behaviour in models that learn or use these representations. To overcome these limitations, we are motivated to pursue an inherently _distributed_ approach to representing compositional structure. Instead of concatenating discrete slots (_FoV tokens_) dimension-wise to form the compositional representation, an inherently _distributed_ approach _continuously combines_ densely encoded FoVs within a unified vector space. This design allows information from each FoV to be _smoothly interwoven_ throughout all dimensions of the representation, as illustrated in Figure 1b, potentially resulting in smoother gradient flow, enhanced expressivity, and heightened robustness to noise. By reconciling the demand for representations that are explicitly compositional with the _continuous_, _distributed_ nature of deep learning, distributed compositional representations offer a compelling alternative to traditional symbolic, slot-based paradigms.

Pioneered by Smolensky, the Tensor Product Representation [3] is a specific representational form that encodes compositional structure in an inherently _distributed_ manner. At the crux of it, TPRs are formed by _continuously blending_ representational components together into the overall representation, in a manner analogous to superposing multiple waves together to produce a complex waveform, as illustrated in Figure 1b. For a representation to qualify as a TPR, it must adhere to a highly specific mathematical form, which confers upon the TPR valuable structural properties (elaborated on in Section 3.2), but also imposes two limitations (see B.1 for further details). First, as depicted by the stars in Figure 1c, only a discrete subset of points in the underlying representational space, \(\widetilde{V}\), satisfies the stringent mathematical criteria to qualify as TPRs. Consequently, to learn TPRs, representation learners must map from the data manifold onto this _discrete subset_, which constitutes a highly constrained and inherently challenging learning task. Second, the TPR specification enforces a strict, algebraic definition of compositional structure, limiting the TPR's ability to flexibly represent ambiguous, real-world data which is often _quasi_-compositional, only _approximately_ adhering to some rigid, formal definition. Historically, these limitations have confined TPR learning to formal domains characterised by explicit, algebraic structure - as evidenced by the near exclusive deployment of TPRs in language [19; 23; 28; 34; 52] - and, to contexts where strong supervision from highly structured downstream tasks is available to steer the representation learning process [23; 28; 38; 51]. To negate these drawbacks and extend distributed compositional representations to weakly supervised, non-formal domains, we propose _Soft TPR_, which can be thought of as a _continuous relaxation_ of the traditional TPR, as illustrated by the translucent circular regions in Figure 1c. At its core, the Soft TPR is designed to promote representational flexibility and ease of learning while simultaneously preserving the structural integrity of the traditional TPR. We additionally introduce _Soft TPR Autoencoder_, a theoretically-principled _weakly-supervised_ architecture for learning Soft TPRs, used to operationalise the Soft TPR framework in the visual representation learning domain.

Our main contributions are threefold: i) We propose a novel compositional representation learning framework, introducing the _distributed_, _flexible_ Soft TPR compositional form, alongside a dedicated, weakly-supervised architecture, _Soft TPR Autoencoder_, for learning this form. ii) Our framework is the first to learn _distributed compositional representations_ in the non-formal, less explicitly algebraic domain of _vision_. iii) We empirically affirm the far-reaching benefits produced by the Soft TPR framework, demonstrating that Soft TPRs achieve state-of-the-art disentanglement, accelerate representation learner convergence, and provide downstream models with enhanced sample efficiency and superior low-sample regime performance.

## 2 Related Work

**Disentanglement**: In aiming to produce explicitly compositional representations without strong supervision, our work shares the same objective as disentangled representation learning. Prior to the highly influential work of [25] which proved the impossibility of learning disentangled representations without supervision or other inductive biases, disentangled representations were learnt in a completely unsupervised fashion [8; 10; 14; 17; 24; 26; 37]. Our use of weak supervision is inspired by the work [13; 22; 31; 33; 35] relating to this highly influential impossibility result. In particular, we leverage the type of weak supervision termed'match pairing' [35], where pairs, \((x,x^{\prime})\), differing in values for a subset of known FoVs are presented to the model, to incentivise disentanglement. Our work,

Figure 1: (a) Disentangled representations can be conceptualised as a _concatenation_ of FoV tokens (coloured blocks), enforcing a _symbolic, string-like_ compositional structure, where each FoV is allocated to a discrete _slot_ in the representation. We instead, consider a _distributed_ representation of compositional structure, (b), where information from densely encoded FoV (first 6 waves) are _continuously combined_ together to form the representation, \(\psi(x)\) (in red), effectively distributing the information from _multiple_ FoVs into a _single_ dimension of \(\psi(x)\). (c) Only a subset of points (stars) in the underlying representational space (rainbow manifold) satisfy the TPR specification. The Soft TPR relaxes this, capturing larger, _continuous regions_ of the underlying representational space (the translucent circles), while approximately preserving the TPR’s key properties.

however, fundamentally diverges from all disentanglement work we are aware of, by adopting an inherently _distributed_ representation of compositional structure, which contrasts with the inherently _symbolic_, localist representations of compositional structure characterising existing work.

**TPR-based Work**: Existing TPR-based approaches generate distributed representations of compositional structure by producing an element with the explicit mathematical form of a TPR. To learn this highly specific form, these approaches rely on the algebraic characterisation of compositionality present in formal domains, such as mathematics [38], or language [19; 23; 28; 34; 52] in addition to strong supervision signals from highly structured downstream tasks, such as part-of-speech tagging [23], and answering structured language [28; 51] or mathematics questions [38]. In contrast, _Soft TPR_ eases these stringent constraints by offering a continuously relaxed specification of compositional structure. This allows our approach to extend distributed representations of compositional structure to the orthogonal and less algebraically structured domain of _visual_ data, while also reducing reliance on annotated data by instead using weak supervision to learn this relaxed representational form.

## 3 Preliminaries

### A Formal Framework for Compositional Representations

We adopt a generalised, non-generative version of the definition of _compositional representations_ from [49]. Let data \(x\in X\) be _compositionally-structured_ if there exists a decomposition function \(\beta:X\to A_{1}\times\ldots\times A_{n}\) decomposing \(x\) into constituent parts (FoVs), i.e. \(\beta(x)=\{a_{1},\ldots,a_{n}\}\), where \(a_{i}\in A_{i}\). A mapping \(\psi:X\to V_{F}\) then produces a _compositional representation_ if \(\psi(x)=C(\psi_{1}(a_{1}),\ldots,\psi_{n}(a_{n}))\), where each \(\psi_{i}:A_{i}\to V_{i}\) is a _component function_ that independently embeds a part \(a_{i}\) (i.e., a particular FoV) into a vector space, and \(C:V_{1}\times\ldots\times V_{n}\to V_{F}\) is a _compositional function_ that combines these embedded parts to form the overall representation. This construction enforces a faithful correspondence between the constituency structure of the data \(\{a_{i}\}\) and that of the representation, \(\{\psi_{i}(a_{i})\}\) (provided \(C\) is invertible).

We formalise a _symbolic_ compositional representation, \(\psi_{s}(x)\), as one in which \(C\) is given by a concatenation operation. Concretely, \(\psi_{s}(x)=\left(\psi_{1}(a_{1})^{T},\ldots,\psi_{n}(a_{n})^{T}\right)^{T}\). Indeed, the disentanglement approaches of [8; 10; 13; 14; 17; 22; 24; 26; 31; 32; 33; 35; 36; 37; 47] all fit this framework, concatenating scalar or vector-valued FoV tokens together to produce the representation. While this design may be fundamentally at odds with the _continuous_, _distributed_ nature of deep learning (as detailed in Section 1), it provides one clear benefit: the embedded FoVs, \(\{\psi_{i}(a_{i})\}\) are trivially recoverable by partitioning \(\psi_{s}(x)\).

In many scenarios, readily recovering the data constituents (i.e., FoVs), \(\{a_{i}\}\), from the compositional representation \(\psi(x)\) is essential for practical utility. Here, we assume each \(\psi_{i}\) is invertible, so direct recoverability of the data constituents, \(\{a_{i}\}\), from the representation \(\psi(x)\) is guaranteed if the embedded FoVs, \(\{\psi_{i}(a_{i})\}\), remain structurally separable in the representation, \(\psi(x)\). Motivated by this requirement, we aim to construct compositional representations that: 1) achieve an inherently _distributed_ encoding of compositional structure by _smoothly interweaving_ FoVs throughout _all_ dimensions of \(\psi(x)\), and 2) preserve the direct recoverability of the embedded FoVs.

### The TPR Framework

Smolensky's Tensor Product Representations (TPR) [3] provides one compelling realisation of an inherently _distributed_ encoding of compositional structure that, under certain conditions, preserves the _direct recoverability_ of the embedded FoVs. We briefly review key aspects here, with additional details and formal proofs deferred to Appendix A. The TPR framework conceptualises a compositionally-structured object, \(x\), as comprising a number, \(N_{R}\), of roles, where each role \(r\in R\) is bound to a corresponding filler \(f\in F\), with this binding being denoted by \(f/r\). Thus, compositionally-structured objects are decomposable into a set of role-filler bindings, \(\beta(x)=\{f_{i}/r_{i}\}_{i=1}^{N_{R}}\). In natural language, where the TPR is most commonly deployed [19; 28; 34; 52], roles may correspond to grammatical categories (e.g., \(\mathrm{noun}\)) and fillers specific words (e.g., \(\mathrm{cat}\)). Translating this role-filler formalism to the visual domain, we reinterpret roles as FoV _types_ (e.g., \(\mathrm{floor\ colour}\)), and fillers as FoV _values_ (e.g., \(\mathrm{blue}\)). Using this role-filler formalism to decompose the Shapes3D image in Figure 1, we have \(\beta(x)=\{\mathrm{magenta/floor\ colour,orange/wall\ colour,aqua/object\ colour,large/object\ size,\ oblong/object\ shape}\}\).

To construct a TPR, the roles and fillers for each binding in \(x\) are independently embedded via role and filler embedding functions, \(\xi_{R}:R\to V_{R},\xi_{F}:F\to V_{F}\) respectively. The binding \(f/r\) is then encoded by taking a tensor product (denoted by \(\otimes\)) over the embedded role, \(\xi_{R}(r)\), and embedded filler, \(\xi_{F}(f)\). Summing over the embeddings for all role-filler bindings in \(x\) yields the TPR, \(\psi_{ptr}(x)\):

\[\psi_{ptr}(x):=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i}),\] (1)

where \(m:\{1,\ldots,N_{R}\}\rightarrow\{1,\ldots,N_{F}\}\) is a matching function2 associating each of the \(N_{R}\) roles to the filler it binds to in the decomposition of \(x\) (i.e., \(\beta(x)=\{f_{m(i)}/r_{i}\}\)).

Footnote 2: We omit the dependence of \(m\) on \(x\) for notational clarity.

We now situate the TPR within the formal framework of Section 3.1, by observing that it defines each representational component, \(\psi_{i}(a_{i})\) as an embedded role-filler binding, \(\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\), and adopts _ordinary vector space addition_ as its composition function, \(C\). Unlike the _concatenation_ in symbolic, localist schemes, the TPR _additively superposes_ all embedded FoVs in the same underlying vector space, producing an inherently _distributed_ representation of compositional structure in which information from _multiple_ role-filler bindings can be smoothly interwoven into the _same_ dimensions.3

Footnote 3: See Appendix A.4 for further discussion on the distributed nature of the TPR.

A natural question arises from defining \(C\) as ordinary vector space addition: can we recover each representational component - the embedded role-filler binding, \(\xi_{i}(f_{m(i)})\otimes\xi_{R}(r_{i})\) - from the TPR, which is their sum? Remarkably, despite the non-injectivity of the summation operation, the TPR's specific algebraic structure enables faithful recoverability of all representational components from the TPR, through a process referred to as _unbinding_ (see A.2 for more details). More concretely, provided that the role embedding vectors \(\{\xi_{R}(r_{i})\}\) are linearly independent and known, the embedded filler bound to the \(i\)-th role can be _unbound_ from the representation, \(\psi_{ptr}(x)\), by taking a (tensor) inner product between \(\psi_{trr}(x)\) and the \(i\)-th _unbinding_ vector, \(u_{i}\):

\[\psi_{trr}(x)u_{i}=\left(\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\right) u_{i}=\xi_{F}(f_{m(i)}),\] (2)

where \(u_{i}\) is a vector corresponding to the \(i\)-th column of the (left) inverse of the matrix obtained by taking all (linearly independent) role embeddings as columns. Repeating this _unbinding_ procedure using each of the \(N_{R}\) unbinding vectors recovers each of the embedded fillers bound to the \(N_{R}\) roles, and thus, the entire set of embedded role filler bindings, \(\{\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\}\). Hence, provided that the linear independence of the roles holds, the TPR offers an inherently _distributed_ representation of compositional structure that nonetheless retains the chief benefit of _symbolic_ approaches - direct recoverability of each representational component.

## 4 Methods

### Soft TPR: An Extension to the TPR Framework

While Tensor Product Representations offer a robust framework for encoding compositional structure in a distributed manner, their rigid representational form imposes practical limitations. The core limitation lies in TPR's strict algebraic specification of compositional structure as a set of discrete bindings, each of which comprise a single role and a single filler. This specification precludes the representation of more ambiguous, quasi-compositional structures that cannot be neatly decomposed in this way. For instance, even in the highly compositional domain of natural language, there exist phenomena such as idiomatic expressions that resist straightforward decomposition into role-filler pairs because their meanings cannot be directly inferred from their composite parts. Such examples illustrate that real-world compositional structures may require more flexible representation than what is offered by traditional TPRs. Consequently, TPRs may struggle to model more nuanced forms of compositional structure that characterise less structured, non-formal data domains, such as vision. Additionally, TPRs present a challenging learning task. Their representational form \(\psi_{trr}(x):=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\) is only satisfied by a _discrete subset_ of points within the underlying representational space, \(V_{F}\otimes V_{R}\), as illustrated in Figure 0(c). This constraint forces representation learners to map data to a highly restricted, discrete set of points, making learning inherently arduous.

To overcome these shortcomings, we introduce _Soft TPR_, a continuous relaxation of the traditional TPR specification. Soft TPRs allow any point in the underlying representational space \(V_{F}\otimes V_{R}\) that _approximately_ conforms to the TPR specification. This removes the need for representations to perfectly adhere to the rigid structural specification of the TPR, instead permitting them to _flexibly approximate_ this rigid form. This flexibility provides two main advantages: **1. Enhanced Representational Flexibility**: Soft TPR facilitates the representation of _quasi_-compositional structures that do not perfectly adhere to the traditional TPR's structural specification. This includes scenarios where bindings are approximate rather than exact, or where multiple fillers influence a single role.

**2. Improved Ease of Learning**: The relaxed representational specification allows representation learners to map data to broader regions of the representational space (as illustrated by the translucent circular regions in Figure 0(c)). This may facilitate more efficient learning, as there are more possible mappings from the data to the required representational form.

Formally, to define the Soft TPR, we consider the vector space underlying TPRs, \(V_{F}\otimes V_{F}\), produced by an arbitrary role embedding function \(\xi_{R}:R\to V_{R}\) and an arbitrary filler embedding function \(\xi_{F}:F\to V_{F}\). A _Soft TPR_ is defined as any element \(z\in V_{F}\otimes V_{R}\) in this underlying representational space that is sufficiently close to a traditional TPR, \(\psi_{trp}\)4, as measured by a chosen distance metric (we take the Frobenius norm). Denoting the Frobenius norm by \(||\cdot||_{F}\), Soft TPR satisfies \(||z-\psi_{trp}||_{F}<\epsilon\), where \(\epsilon\) is some small, scalar-valued positive quantity. This condition ensures that the Soft TPR approximately encodes the TPR-based compositional structure of \(\psi_{trp}\), while allowing for slight deviations which capture a more flexible, relaxed notion of compositionality that cannot be reduced to a set of role-filler bindings (see B.2).

Footnote 4: We occasionally omit the dependence on \(x\) for notational clarity.

Despite this relaxation, Soft TPRs retain the critical advantage of traditional TPRs: the ability to recover constituent role-filler bindings through the unbinding operation, albeit approximately. Specifically, when unbinding a Soft TPR \(z\), the operation yields soft filler embeddings \(\hat{f}_{i}\) that closely approximate the true filler embeddings \(\xi_{F}(f_{m(i)})\). Formally, for a Soft TPR \(z\), performing the unbinding operation for the \(i\)-th role yields:

\[zu_{i} \approx\psi_{trp}u_{i}=\left(\sum_{i}\xi_{F}(f_{m(i)})\otimes \xi_{R}(r_{i})\right)u_{i}=\xi_{F}(f_{m(i)}),\] (3) \[=\xi_{F}(f_{m(i)})+\epsilon_{i}=:\tilde{f}_{i},\] (4)

where \(\epsilon_{i}=zu_{i}-\xi_{F}(f_{m(i)})\) represents the approximation error. This approximate recoverability ensures that Soft TPRs _implicitly encode_ a precise, algebraically-expressible form of compositional structure, even when their representational form _deviates_ from the exact TPR specification.

### Soft TPR Autoencoder: Learning Distributed and Flexible Compositional Representations

We define our vector spaces of interest over the reals as \(V_{F}:=\mathbb{R}^{D_{F}}\) and \(V_{R}:=\mathbb{R}^{D_{R}}\) where \(D_{F},D_{R}\) denote the dimensionality of the filler and role embedding spaces. The pivotal insight underlying our method is that a _Soft TPR_ can be effectively realised by ensuring that the encoder produces representations that are amenable to quantisation into explicit TPRs. Specifically, since a Soft TPR is any arbitrary element from the vector space \(\mathbb{R}^{D_{F}\cdot D_{R}}\)5 sufficiently close to an explicit TPR, any \((D_{F}\cdot D_{R})\)-dimensional vector produced by an encoder in a standard autoencoding framework can be treated as a Soft TPR _candidate_. This realisation suggests that a conventional autoencoding framework only requires modest modifications to generate Soft TPRs.

Footnote 5: Due to isomorphism of vector spaces \(\mathbb{R}^{D_{F}}\otimes\mathbb{R}^{D_{R}}\cong\mathbb{R}^{D_{F}\cdot D_{R}}\), we henceforth use vectors from \(\mathbb{R}^{D_{F}\cdot D_{R}}\) in place of rank-2 tensors from \(\mathbb{R}^{D_{F}}\otimes\mathbb{R}^{D_{R}}\), and the Euclidean norm instead of the Frobenius norm to align the Soft TPR framework more seamlessly with the autoencoding framework.

Our Soft TPR Autoencoder comprises three main components: a standard encoder, \(E\), the TPR decoder, and a standard decoder, \(D\). The encoder output, \(z\), serves as the Soft TPR. The overarching intuition guiding our architecture is to ensure that \(z\) can be effectively quantised or decoded into an explicit TPR, thereby enforcing the Soft TPR property, \(||z-\psi_{trp}||_{2}<\epsilon\).

**Representational Form**: To ensure that the encoder produces elements with the _form_ of a Soft TPRs, we introduce a mechanism that penalises the Euclidean distance \(||z-\psi_{trp}^{*}||_{2}\) between the encoderoutput, \(z\), and some explicit TPR, \(\psi^{*}_{tpr}\), that \(z\) best approximates. To obtain \(\psi^{*}_{tpr}\) needed to compute the loss, we derive an explicit analytical form for \(\psi^{*}_{tpr}\) and construct elements satisfying this analytical form using a role embedding matrix, \(M_{\xi_{R}}\) containing \(N_{R}\)\(D_{R}\)-dimensional role embedding vectors, \(\{\xi_{R}(r_{i})\}\), and a filler embedding matrix, \(M_{\xi_{F}}\) containing \(N_{F}\)\(D_{F}\)-dimensional filler embedding vectors, \(\{\xi_{F}(f_{i})\}\). To define an explicit analytical form for \(\psi^{*}_{tpr}\), we use a greedily optimal selection of filler embeddings based on their proximity to the soft filler embeddings extracted from \(z\):

\[\psi^{*}_{tpr}:=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i}),\text{where }m(i):=\operatorname*{arg\,min}_{j}||\tilde{f}_{k}-\xi_{F}(f_{j})||_{2},\text { and }\tilde{f}_{k}:=zu_{i}.\] (5)

That is, we define \(\psi^{*}_{tpr}\) as the TPR constructed from explicit filler embeddings \(\xi_{F}(f_{j})\) with the smallest Euclidean distance to the _soft_ filler embeddings \(\tilde{f}_{k}\) of \(z\). To construct elements satisfying (5), we use a three-step process carried out by the novel TPR decoder we introduce, visible in Figure 2: **1) Unbinding**: Utilising a randomly-initialised dense role embedding matrix \(M_{\xi_{R}}\), the unbinding module extracts soft filler embeddings \(\{\tilde{f}_{i}\}\) by performing the TPR _unbinding_ operation on \(z\). We elaborate on our theoretically-informed reason for this design, how the unbinding vectors are obtained, and for not backpropagating gradient to \(M_{\xi_{R}}\) in B.4.1. **2) Quantisation**: The quantisation module, containing a learnable filler embedding matrix \(M_{\xi_{F}}\), employs the VQ-VAE vector quantisation algorithm [11] to both 1) learn the explicit filler embeddings, and 2) quantise the soft filler embeddings \(\{\tilde{f}_{1},\ldots,\tilde{f}_{N_{R}}\}\) produced by the unbinding module into the explicit filler embeddings \(\{\xi_{F}(f_{1}),\ldots,\xi_{F}(f_{N_{R}})\}\) with the smallest Euclidean distances. **3) TPR Construction**: The quantised filler embeddings \(\{\xi_{F}(f_{m(i)})\}\) are then bound to their respective roles using the tensor product, and all embedded bindings are summed together to produce the explicit TPR \(\psi^{*}_{tpr}\) with the form of Eq 5. \(\psi^{*}_{tpr}\) is subsequently used by the decoder, \(D\), to reconstruct the input image. The overall unsupervised loss, \(\mathcal{L}_{u}\), comprises three components, where \(\mathrm{s}\) denotes the stop-gradient operator, \(\beta\) is a hyperparameter controlling the commitment loss in VQ-VAE, and \(\mathcal{L}_{r}\) is a suitable image-based reconstruction loss (we use \(L_{2}\)):

\[\mathcal{L}_{u}:=\underbrace{||z-\psi^{*}_{tpr}||_{2}^{2}}_{\text{Soft TPR form penalty}}+\underbrace{\mathcal{L}_{r}(x,D(\psi^{*}_{tpr}))}_{\text{ reconstruction loss}}+\underbrace{\sum_{i}\frac{1}{N_{R}}\left(||\mathrm{s}[\xi_{F}(f_{m(i)})]-\tilde{f}_{i} ||_{2}^{2}+\beta||\xi_{F}(f_{m(i)})-\mathrm{s}[\tilde{f}_{i}]||_{2}^{2}\right) }_{\text{VQ-VAE quantisation loss}}.\] (6)

**Representational Content**: While the unsupervised loss \(\mathcal{L}_{u}\) ensures that \(z\) maintains the Soft TPR form by being close to an explicit TPR, it does not guarantee that the content of \(z\) accurately reflects the true role-filler semantics of the data. To address this, we introduce a weakly supervised loss that aligns \(z\) with the ground-truth semantics of the image. We employ a match-pairing context similar to [13, 22, 33, 35], where image pairs \((x,x^{\prime})\) share the same role-filler bindings for all but one role, \(r_{i}\). The identity of \(r_{i}\) is known, but not any of the specific fillers or bindings. Our intuition is that, for \(z\) to accurately reflect the semantics of the image, the Euclidean distance between the quantised fillers of \(x\) and \(x^{\prime}\) bound to role \(r_{i}\) should be maximal, relative to the distances between the filler embeddings pairs for all other roles \(r_{j},j\neq i\). To encourage this, we apply the cross entropy loss corresponding to the 3rd term in Eq 7, where \(\Delta q\) denotes the \(N_{R}\)-dimensional vector with each dimension

Figure 2: Diagram illustrating the Soft TPR Autoencoder. We encourage the encoder \(E\)’s output, \(z\), to have the form of a Soft TPR by penalising its distance with the greedily defined, explicit TPR, \(\psi^{*}_{tpr}\) of Equation 5 that \(z\) best approximates. \(\psi^{*}_{tpr}\) is recovered using a 3 step process performed by our TPR decoder (center rectangle): 1) unbinding, 2) quantisation, and 3) TPR construction. The decoder, \(D\), reconstructs the input image using \(\psi^{*}_{tpr}\).

populated by the Euclidean distance between the quantised fillers of \(x\) and \(x^{\prime}\) for role \(r_{k}\), and \(l\) denotes the one-hot vector of dimension \(N_{R}\) with the index for \(r_{i}\) set to 1. Additionally, like [47], we incorporate a reconstruction loss (the 2nd term of Eq 7) to enforce consistency when swapping the quantised filler embeddings for role \(r_{i}\). Specifically, we construct new TPRs by swapping the quantised fillers for \(r_{i}\) between \(x\) and \(x^{\prime}\) to generate \(\psi^{s}_{tpr}(x)\) and \(\psi^{s}_{tpr}(x^{\prime})\), and ensure that the decoder can accurately reconstruct the corresponding swapped images from these swapped TPRs.

Combining these components, our final loss, \(\mathcal{L}\), is a weighted sum over the unsupervised and weakly supervised loss components, where \(\lambda_{1}\) and \(\lambda_{2}\) are hyperparameters:

\[\mathcal{L}:=\mathcal{L}_{u}+\lambda_{1}\left(\frac{1}{2}\mathcal{L}_{r}(x,D( \psi^{s}_{tpr}(x^{\prime})))+\frac{1}{2}\mathcal{L}_{r}(x^{\prime},D(\psi^{s }_{tpr}(x)))\right)+\lambda_{2}\mathrm{CE}(\Delta q,l).\] (7)

## 5 Results

To evaluate our Soft TPR framework, we perform evaluation along three dimensions that are standard in the compositional representation learning literature [25; 30; 33; 42]: **1) Compositional Structure / Disentanglement**: To what extent do Soft TPR representations capture _explicitly compositional structure?_**2) Representation Learner Convergence**: How _efficiently_ can representation learners acquire Soft TPR's _distributed, flexible_ compositional structure? **3) Downstream Model Performance**: Does Soft TPR's _distributed, flexible_ compositional form offer tangible benefits for _downstream models_ utilising compositional representations?

We benchmark against a suite of weakly supervised disentanglement baselines: Ada-GVAE [33], GVAE [22], ML-VAE [13], SlowVAE [39], and the GAN-based model of [35], which we henceforth refer to as 'Shu'. These models produce symbolic compositional representations corresponding to a concatenation of _scalar-valued_ FoV tokens. Similar to our approach, Ada-GVAE, GVAE, ML-VAE, SlowVAE, and Shu are trained with paired samples (\(x,x^{\prime}\)) sharing values for all but a subset of FoVs types (roles), \(I\), with ML-VAE, GVAE, SlowVAE, and Shu assuming access to \(I\), matching our model's level of supervision. To ensure a fair comparison, we modify Ada-GVAE (method detailed in Appendix C.2.2) for more direct comparability, denoting our modification by Ada-GVAE-k. Additionally, we benchmark against 2 baselines producing symbolic _vector-to-keted_ compositional representations: COMET [36], and Visual Concept Tokeniser (VCT) [47], although these are fully unsupervised and not directly comparable to our method. We train five instances of each representation learning model using five random seeds for 200,000 iterations across all datasets, and report averaged results (an unabridged suite of all results is contained in the Appendix).

### Compositional Structure / Disentanglement

To demonstrate that Soft TPRs inherently capture the algebraic compositional structure required for effective disentanglement, we first quantise each Soft TPR, \(z\), into its corresponding explicit TPR, \(\psi^{*}_{tpr}\) using the TPR decoder (note that once the model is trained, quantisation is fully deterministic). This quantisation process transforms the implicit compositional information encoded within Soft TPRs into an explicit algebraic form suitable for evaluation. We then apply standard disentanglement metrics across benchmark datasets including Cars3D [4], MPI3D [21], and Shapes3D [24]. Table 1 illustrates that Soft TPR consistently outperforms symbolic compositional baselines across all datasets, achieving state-of-the-art results with notable DCI metric improvements of \(29\%\) on Cars3D and \(74\%\) on the more challenging MPI3D dataset. To ensure these gains are attributable to Soft TPR framework rather than merely a marginal increase in model capacity (Soft TPR Autoencoder adds between 1,600-13,568 model parameters to a standard (variational) autoencoder), we conduct control experiments by equalising parameter counts in applicable baselines. Consistent with findings from prior work [40], we observe no performance improvements in relevant symbolic baselines when parameter counts are increased (see C.2.4).

### Representation Learner Convergence Rate

To explore how the Soft TPR framework influences the convergence rate of representation learners, we analyse representations produced at varying stages of representation learner training (100, 1,000, 10,000, 100,000 and 200,000 iterations), and evaluate both their **1) Disentanglement**, and **2) Downstream Utility**. We quantify downstream utility of learned representations by examining the performance of downstream models on two tasks common in the disentanglement literature [25; 33; 42; 30]: a classification-based task assessing the model's ability to perform abstract visual reasoning [30] and a regression task involving the prediction of continuous FoV values for the disentanglement datasets [42]. Our findings indicate that the Soft TPR framework generally achieves faster disentanglement convergence, particularly on Cars3D and MPI3D datasets (see Appendix C.4.1). Moreover, Soft TPR consistently accelerates the learning of useful representations for _both_ downstream tasks. The downstream performance improvements are particularly pronounced in the low iteration regime - for instance, at only 100 iterations of representation learner training, as shown in Tables 2 and 3. For fair comparison, we embed baseline representations of both higher and lower dimensionality into the same vector space as Soft TPR, which we denote by \(\dagger\), and select the best-performing variant (original or dimensionality-matched) for each baseline (details in Appendix C).

### Downstream Models

To evaluate whether the _distributed_ and _flexible_ encoding of compositional structure offered by the Soft TPR benefits downstream models, we examine both: **1) Sample Efficiency** and **2) Raw Performance in Low Sample Regimes**, using the previously mentioned abstract visual reasoning and FoV regression tasks. In line with [25], we quantify sample efficiency with a ratio-based metric comparing downstream model performance using restricted sample sizes (100, 250, 500, 1,000, and 10,000) against performance when trained using all samples. As illustrated in Table 4, Soft TPR substantially outperforms baseline models in sample efficiency, especially in the most restrictive case involving _only 100_ samples, achieving a \(93\%\) improvement. Additionally, Soft TPR produces substantial raw performance increases in the low sample regime, as evidenced by the \(138\%\) and \(168\%\) improvements in Table 4 for the low-sample regimes of 100 and 200 samples respectively, and the \(30\%\) improvement in Table 5.

### Ablation Studies

To disentangle the contributions of Soft TPR's foundational components, we conduct ablation studies focusing on two critical properties that are hypothesised to influence the Soft TPR's learning of compositional representations: **1) Relaxed Representational Constraints** and **2) Distributed Encoding**, denoted RRC and DE respectively in Table 7. For **Relaxed Representational Constraints**, we significantly increase the weighting of the form penalty \(\|z-\psi^{*}_{trpr}\|_{2}\) in the Soft TPR loss to enforce rigid representational constraints on the Soft TPR Autoencoder, effectively forcing it to

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{3}{c}{Cars3D} & \multicolumn{3}{c}{Shapes3D} & MPI3D \\ \cline{2-6}  & FactorVAE score & DCI score & FactorVAE score & DCI score & FactorVAE score & DCI score \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.902 \(\pm\) 0.035 & 0.509 \(\pm\) 0.027 & 0.950 \(\pm\) 0.032 & 0.850 \(\pm\) 0.047 & 0.455 \(\pm\) 0.083 & 0.355 \(\pm\) 0.027 \\ Ada-GVAE-k & 0.947 \(\pm\) 0.064 & _0.664 \(\pm\) 0.167 & _0.975_ \(\pm\) 0.006 & **0.963 \(\pm\) 0.077** & 0.496 \(\pm\) 0.095 & 0.343 \(\pm\) 0.040 \\  & GVAE & 0.877 \(\pm\) 0.081 & 0.262 \(\pm\) 0.095 & 0.921 \(\pm\) 0.075 & 0.842 \(\pm\) 0.040 & 0.378 \(\pm\) 0.024 & 0.245 \(\pm\) 0.074 \\ ML-VAE & 0.870 \(\pm\) 0.052 & 0.216 \(\pm\) 0.063 & 0.835 \(\pm\) 0.111 & 0.739 \(\pm\) 0.115 & 0.390 \(\pm\) 0.026 & 0.251 \(\pm\) 0.029 \\ Shu & 0.573 \(\pm\) 0.062 & 0.032 \(\pm\) 0.014 & 0.265 \(\pm\) 0.043 & 0.017 \(\pm\) 0.006 & 0.287 \(\pm\) 0.034 & 0.033 \(\pm\) 0.008 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & _0.966 \(\pm\) 0.029_ & 0.382 \(\pm\) 0.080 & 0.957 \(\pm\) 0.043 & 0.8984 \(\pm\) 0.013 & 0.689 \(\pm\) 0.035 & _0.475 \(\pm\) 0.005_ \\ COMET & 0.339 \(\pm\) 0.008 & 0.024 \(\pm\) 0.026 & 0.168 \(\pm\) 0.005 & 0.002 \(\pm\) 0.000 & 0.145 \(\pm\) 0.024 & 0.005 \(\pm\) 0.001 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.999 \(\pm\) 0.001** & **0.863 \(\pm\) 0.027** & **0.984 \(\pm\) 0.012** & _0.926 \(\pm\) 0.028_ & **0.949 \(\pm\) 0.032** & **0.828 \(\pm\) 0.015** \\ \hline \hline \end{tabular}
\end{table}
Table 1: FactorVAE and DCI scores. Additional results in Section C.3.3

\begin{table}
\begin{tabular}{c c} \hline \hline \multicolumn{3}{c}{Abstract visual reasoning dataset} \\ \hline \multicolumn{3}{c}{Symbolic scalar-tokened} \\ \hline Slow-VAE\({}^{\dagger}\) & 0.552 \(\pm\) 0.035 \\ Ada-GVAE-k\({}^{\dagger}\) & _0.631 \(\pm\) 0.037_ \\ GVAE\({}^{\dagger}\) & 0.541 \(\pm\) 0.031 \\ ML-VAE\({}^{\dagger}\) & 0.550 \(\pm\) 0.025 \\ Shu & 0.208 \(\pm\) 0.052 \\ \hline \multicolumn{3}{c}{Symbolic vector-tokened} \\ \hline VCT & 0.440 \(\pm\) 0.033 \\ COMET\({}^{\dagger}\) & 0.348 \(\pm\) 0.069 \\ \hline \multicolumn{3}{c}{Fully continuous} \\ \hline Ours & **0.804 \(\pm\) 0.016** \\ \hline \hline \end{tabular}
\end{table}
Table 2: FoV regression \(R^{2}\) scores (100 iterations of representation learner training).

produce representations that precisely match the TPR-specified compositional form. For **Distributed Encoding**, we modify the model to encode compositional structure in an inherently less distributed manner, by modifying the sparsity of \(\psi^{*}_{tpr}\) (see C.2.4). Results in Table 7 demonstrate that both the Soft TPR's distributed encoding and relaxed constraints are essential for learning representations with high compositional structure, as quantified by the disentanglement metrics. Additionally, we examine the impact of Soft TPR's flexible, _quasi-compositional_ form on downstream performance by replacing each Soft TPR, \(z\), with its quantised explicit TPR, \(\psi^{*}_{tpr}\) in downstream tasks. Table 6 and Appendix C.6.3 indicates that Soft TPRs provide downstream models with _unique_ performance improvements that explicit TPRs (which are produced under the same conditions) cannot account for. Please see Appendix C.6 for further details and full ablation results, including additional ablations.

### Key Insights

The empirical results of the Soft TPR framework reveal two primary insights. Firstly, deep learning models may more effectively learn precise compositional structures when these structures are 1) _implicitly acquired_ through _relaxed representational constraints_, allowing the model to acquire precise compositional forms (i.e., \(\psi^{*}_{tpr}\)) through quantisation of flexible Soft TPRs, \(z\), and 2) when the compositional structure is assumed to be encoded in a _distributed_ format. Secondly, the _flexible representational form_ of Soft TPR offers downstream models _unique_ advantages by enabling more efficient utilisation of _implicitly_ encoded compositional information compared to their rigid counterparts, \(\psi^{*}_{tpr}\). Together, these comprehensive model benefits empiricially demonstrate the value of the framework's 1) relaxed representational constraints, 2) distributed encoding of compositional structure, and 3) flexible representational form.

## 6 Conclusion

In this work, we tackle a challenge tracing its roots to the conception of the connectionist vs. classicalist debate: the fundamental mismatch between compositional structure and the inherently distributed nature of deep learning. To bridge this gap, we introduce the _Soft TPR_, a new, inherently _distributed_, _flexible_ compositional representational form extending Smolensky's Tensor Product Representation, together with the _Soft TPR Autocoder_, a theoretically-principled architecture designed for learning Soft TPRs. Our _flexible_, _distributed_ framework demonstrates substantial improvements in the visual representation learning domain - enhancing the representation of compositional structure, accelerating convergence in representation learners, and boosting efficiency in downstream models. Future work will extend this framework to hierarchical compositional structures, enabling bound fillers to recursively decompose into role-filler bindings for enhanced representational expressivity.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Representational form & Cars3D & Shapes3D & MPI3D \\ \hline TPR (100 samples) & 0.361 \(\pm\) 0.031 & 0.447 \(\pm\) 0.108 & 0.415 \(\pm\) 0.050 \\ Soft TPR (100 samples) & **0.638 \(\pm\) 0.010** & **0.481 \(\pm\) 0.081** & **0.531 \(\pm\) 0.069** \\ \hline TPR (250 samples) & 0.537 \(\pm\) 0.053 & 0.669 \(\pm\) 0.108 & 0.598 \(\pm\) 0.081 \\ Soft TPR (250 samples) & **0.832 \(\pm\) 0.027** & **0.728 \(\pm\) 0.021** & **0.621 \(\pm\) 0.072** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Downstream FoV \(R^{2}\) scores for TPR and Soft TPR (number of samples in brackets)

\begin{table}
\begin{tabular}{c c} \hline \hline Models & Symbolic scalar-tokensed \\ \hline Slow-VAE & 0.196 \(\pm\) 0.028 \\ Ada-GVAE-k & 0.203 \(\pm\) 0.007 \\ GVAE & 0.182 \(\pm\) 0.013 \\ ML-VAE & 0.193 \(\pm\) 0.012 \\ Shu & 0.200 \(\pm\) 0.010 \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline SVMic vector-tokensed \\ \hline VCT & _0.277 \(\pm\) 0.039_ \\ COMET & 0.259 \(\pm\) 0.016 \\ \hline \hline \end{tabular} 
\begin{tabular}{c c} \hline \hline Fully continuous \\ \hline Ours & **0.360 \(\pm\) 0.033** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Effect of model properties on disentanglement (MPI3D dataset).

\begin{table}
\begin{tabular}{c c} \hline \hline  & Samples & Samples/100 samples \\ \hline \hline \multirow{2}{*}{Models} & \multicolumn{1}{c}{\multirow{2}{*}{100 samples/all}} & \multicolumn{1}{c}{\multirow{2}{*}{250 samples 250 sample/all}} \\  & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} Symbolic scalar-tokensed compositional representations \\ \end{tabular} }} \\  & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} Symbolic scalar-tokensed compositional representations \\ \end{tabular} }} \\ \cline{2-2} \cline{4-4} \cline{6-5} Slow-VAE & 0.127 \(\pm\) 0.050 & 0.130 \(\pm\) 0.051 & 0.152 \(\pm\) 0.011 & 0.155 \(\pm\) 0.011 \\ Ada-GVAE-k & 0.206 \(\pm\) 0.031 & 0.270 \(\pm\) 0.037 & 0.213 \(\pm\) 0.023 & 0.279 \(\pm\) 0.026 \\ GVAE & 0.181 \(\pm\) 0.030 & 0.234 \(\pm\) 0.035 & 0.217 \(\pm\) 0.023 & 0.282 \(\pm\) 0.027 \\ ML-VAE & 0.182 \(\pm\) 0.013 & 0.236 \(\pm\) 0.019 & 0.222 \(\pm\) 0.024 & 0.288 \(\pm\) 0.030 \\ Shu & 0.151 \(\pm\) 0.016 & _0.343 \(\pm\) 0.024_ & 0.211 \(\pm\) 0.026 & 0.482 \(\pm\) 0.075 \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Models & Symbolic scalar-tokensed \\ \hline Slow-VAE & 0.196 \(\pm\) 0.028 \\ Ada-GVAE-k & 0.203 \(\pm\) 0.007 \\ GVAE & 0.182 \(\pm\) 0.013 \\ ML-VAE & 0.193 \(\pm\) 0.012 \\ Shu & 0.200 \(\pm\) 0.010 \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline Models & Symbolic scalar-tokensed \\ \hline Slow-VAE & 0.196 \(\pm\) 0.028 \\ Ada-GVAE-k & 0.203 \(\pm\) 0.007 \\ GVAE & 0.182 \(\pm\) 0.013 \\ ML-VAE & 0.193 \(\pm\) 0.012 \\ Shu & 0.200 \(\pm\) 0.010 \\ \hline \hline \end{tabular} \begin{tabular}{c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} Symbolic vector-tokensed \\ \hline VCT \\ COMET \\ \end{tabular} }} \\  & \multicolumn{1}{c}{\multirow{2}{*}{
\begin{tabular}{c} Fully continuous \\ \end{tabular} }} \\ \cline{2-2} \cline{4-4} \cline{6-5} \multicolumn{1}{

## Acknowledgements

This research has been supported by an UNSW University Postgraduate Award. We thank the reviewers for their valuable comments, J. Hershey for a healthy dose of skepticism which greatly improved this work, and B. Spehar for insightful discussions.

## References

* [1] Noam Chomsky. _Syntactic Structures_. The Hague: Mouton, 1957.
* [2] Jerry A. Fodor. _The Language of Thought: A Theory of Mental Representation_. Cambridge, MA: Harvard University Press, 1975.
* [3] Paul Smolensky. "Tensor product variable binding and the representation of symbolic structures in connectionist systems". In: _Artificial Intelligence_ 46.1 (1990), pp. 159-216.
* [4] Sanja Fidler, Sven Dickinson, and Raquel Urtasun. "3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model". In: _Advances in Neural Information Processing Systems_. 2012.
* [5] Yoshua Bengio. _Deep Learning of Representations: Looking Forward_. 2013. arXiv: 1305.0445 [cs.LG].
* [6] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. "Neural Module Networks". In: _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_ (2015), pp. 39-48. url: https://api.semanticscholar.org/CorpusID:5276660.
* [7] Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merrienboer, Armand Joulin, and Tomas Mikolov. _Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks_. 2015. arXiv: 1502.05698 [cs.AI]. URL: https://arxiv.org/abs/1502.05698.
* [8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets". In: _Advances in Neural Information Processing Systems_. Ed. by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett. Vol. 29. Curran Associates, Inc., 2016.
* [9] Paul Smolensky and Matthew A. Goldrick. "Gradient Symbolic Representations in Grammar: The case of French Liaison". In: 2016. url: https://api.semanticscholar.org/CorpusID:36953611.
* [10] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework". In: _International Conference on Learning Representations_. 2017.
* [11] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu koray. "Neural Discrete Representation Learning". In: _Advances in Neural Information Processing Systems_. Ed. by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017.
* [12] Tameem Adel, Zoubin Ghahramani, and Adrian Weller. "Discovering Interpretable Representations for Both Deep Generative and Discriminative Models". In: _Proceedings of the 35th International Conference on Machine Learning_. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 50-59.
* [13] Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. "Multi-Level Variational Autoencoder: Learning Disentangled Representations From Grouped Observations". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 32.1 (2018).
* [14] Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. _Understanding disentangling in \(\beta\)-VAE_. 2018. arXiv: 1804.03599 [stat.ML].
* [15] Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. "Isolating Sources of Disentangled in Variational Autoencoders". In: _Advances in Neural Information Processing Systems_. 2018.
* [16] Cian Eastwood and Christopher K. I. Williams. "A Framework for the Quantitative Evaluation of Disentangled Representations". In: _International Conference on Learning Representations_. 2018.
* May 3, 2018, Conference Track Proceedings_. 2018.
* [18] Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. "Measuring abstract reasoning in neural networks". In: _International conference on machine learning_. 2018, pp. 4477-4486.
* [19] Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, and Jianfeng Gao. "Natural- to formal-language generation using Tensor Product Representations". In: _CoRR_ abs/1910.02339 (2019). arXiv: 1910.02339. URL: http://arxiv.org/abs/1910.02339.
* [20] Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. "Flexibly Fair Representation Learning by Disentanglement". In: _Proceedings of the 36th International Conference on Machine Learning_. 2019.

* [21] Muhammad Waleed Gondal, Manuel Wuthrich, undefinedordde Miladinovic, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Scholkopf, and Stefan Bauer. "On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset". In: _Proceedings of the 33rd International Conference on Neural Information Processing Systems_. 2019.
* [22] Haruo Hosoya. "Group-based learning of disentangled representations with generalizability for novel contents". In: _Proceedings of the 28th International Joint Conference on Artificial Intelligence_. 2019, pp. 2506-2513.
* [23] Qiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and Xiaodong He. "Attentive Tensor Product Learning". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 33.01 (2019), pp. 1344-1351.
* [24] Hyunjik Kim and Andriy Mnih. _Disentangling by Factorising_. 2019. arXiv: 1802.05983 [stat.ML].
* [25] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations". In: _Proceedings of the 36th International Conference on Machine Learning_. Vol. 97. Proceedings of Machine Learning Research. PMLR, 2019, pp. 4114-4124.
* [26] Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. "Disentangling Disentanglement in Variational Autoencoders". In: _Proceedings of the 36th International Conference on Machine Learning_. 2019.
* [27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. "PyTorch: An Imperative Style, High-Performance Deep Learning Library". In: _Advances in Neural Information Processing Systems 32_. Curran Associates, Inc., 2019, pp. 8024-8035. URL: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.
* [28] Imanol Schlag and Jurgen Schmidhuber. "Learning to Reason with Third-Order Tensor Products". In: _Advances in Neural Processing Information Systems_. 2019.
* [29] Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, Jurgen Schmidhuber, and Jianfeng Gao. "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving". In: _ArXiv_ abs/1910.06611 (2019). URL: https://api.semanticscholar.org/CorpusID:204575948.
* [30] Sjoerd van Steenkiste, Francesco Locatello, Jurgen Schmidhuber, and Olivier Bachem. "Are disentangled representations helpful for abstract visual reasoning?" In: _Proceedings of the 33rd International Conference on Neural Information Processing Systems_. 2019.
* [31] Junxiang Chen and Kayhan Batmanghelich. "Weakly Supervised Disentanglement by Pairwise Similarities". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 34.04 (2020), pp. 3495-3502.
* [32] Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max Welling, and Zhuowen Tu. "Guided Variational Autoencoder for Disentanglement Learning". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2020.
* [33] F. Locatello, B. Poole, G. Ratsch, B. Scholkopf, O. Bachem, and M. Tschannen. "Weakly-Supervised Disentanglement Without Compromises". In: _Proceedings of the 37th International Conference on Machine Learning (ICML)_. Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 6348-6359.
* [34] R. Thomas McCoy, Tal Linzen, Ewan Dunbar, and Paul Smolensky. "Tensor Product Decomposition Networks: Uncovering Representations of Structure Learned by Neural Networks". In: _Proceedings of the Society for Computation in Linguistics 2020_. Association for Computational Linguistics, 2020, pp. 277-278. URL: https://aclanthology.org/2020.scil-1.34.
* [35] Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. "Weakly Supervised Disentanglement with Guarantees". In: _International Conference on Learning Representations_. 2020.
* [36] Yilun Du, Shuang Li, Yash Sharma, B. Joshua Tenenbaum, and Igor Mordatch. "Unsupervised Learning of Compositional Energy Concepts". In: _Advances in Neural Information Processing Systems_. 2021.
* [37] Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gunhee Kim. "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 35.9 (May 2021), pp. 7926-7934. DOI: 10.1609/aaai.v35i9.16967. URL: https://ojs.aaai.org/index.php/AAAI/article/view/16967.
* [38] Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, and Jianfeng Gao. "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization". In: _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_. Online: Association for Computational Linguistics, 2021, pp. 4780-4793. DOI: 10.18653/v1/2021.naacl-main.381. URL: https://aclanthology.org/2021.naacl-main.381.

* [39] David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding". In: _International Conference on Learning Representations_. 2021. URL: https://openreview.net/forum?id=EbIDjBymY8.
* [40] Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. "The role of Disentanglement in Generalisation". In: _International Conference on Learning Representations_. 2021. URL: https://openreview.net/forum?id=qBH974jkUVy.
* [41] Sungho Park, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. "Learning Disentangled Representation for Fair Facial Attribute Classification via Fairness-aware Information Alignment". In: _Proceedings of the AAAI Conference on Artificial Intelligence_ 35 (2021), pp. 2403-2411. DOI: 10.1609/aaai.v35i3.16341. URL: https://ojs.aaai.org/index.php/AAAI/article/view/16341.
* Workshop on Generalization beyond the training distribution in brains and machines_. 2021.
* [43] Frederik Trauble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bernhard Scholkopf, and Stefan Bauer. "On Disentangled Representations Learned from Correlated Data". In: _Proceedings of the 38th International Conference on Machine Learning_. Vol. 139. Proceedings of Machine Learning Research. 2021, pp. 10401-10412.
* [44] Milton Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir Ludwig, and Gaurav Malhotra. "Lost in Latent Space: Examining failures of disentangled models at combinatorial generalisation". In: _Advances in Neural Information Processing Systems_. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 10136-10149.
* [45] Zoltan Gendler Szabo. "Compositionality". In: _The Stanford Encyclopedia of Philosophy_. Ed. by Edward N. Zalta and Uri Nodelman. Fall 2022. Metaphysics Research Lab, Stanford University, 2022.
* [46] Tao Yang, Xuancchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. "Towards Building A Group-based Unsupervised Representation Disentanglement Framework". In: _International Conference on Learning Representations_. 2022. URL: https://openreview.net/forum?id=%PgQltcmyd.
* [47] Tao Yang, Yuwang Wang, Yan Lu, and Nanning Zheng. "Visual Concepts Tokenization". In: _Advances in Neural Information Processing Systems_. 2022.
* [48] H. Zhang, Y.-F. Zhang, W. Liu, A. Weller, B. Scholkopf, and E. Xing. "Towards Principled Disentanglement for Domain Generalization". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2022, pp. 8024-8034. URL: https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Towards_Principled_Disentanglement_for_Domain_Generalization_CVPR_2022_paper.pdf.
* [49] Thaddaus Wiedemer, Prasanna Mayliwalanan, Matthias Bethge, and Wieland Brendel. "Compositional Generalization from First Principles". In: _Thirty-seventh Conference on Neural Information Processing Systems_. 2023. URL: https://openreview.net/forum?id=LoQ0luJmSx.
* [50] Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and Wenwu Zhu. "Disentangled Graph Self-supervised Learning for Out-of-Distribution Generalization". In: _Forty-first International Conference on Machine Learning_. 2024. URL: https://openreview.net/forum?id=OSOSzhKpFmF.
* [51] Taewon Park, Inclul Choi, and Minho Lee. "Attention-based Iterative Decomposition for Tensor Product Representation". In: _The Twelfth International Conference on Learning Representations_. 2024. URL: https://openreview.net/forum?id=FDb2jQZsFH.
* [52] Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, and Dapeng Wu. "Tensor Product Generation Networks for Deep NLP Modeling". In: _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_. Association for Computational Linguistics.

## Appendix A TPR Framework

1. Additional Details 2. Formal Proofs 3. Shortcomings of Symbolic Compositional Representations and How TPR Helps 4.4 TPR's Distributed Nature and the Equivalence between Degenerate TPRs and Symbolic Representations
2. Soft TPR Framework 1. Shortcomings of the TPR and How Soft TPR Helps 2. Proof that Soft TPRs Do Not Necessarily Have the Explicit Form of a TPR 3. Alternative Formulations 4. Soft TPR Autoencoder 5. Model Hyperparameters and Hyperparameter Tuning
3. Results 4. Datasets 5.2 Baseline Implementations and Experimental Settings 6.3 Disentanglement 7.4 Representation Learning Convergence 8.5 Downstream Performance 9.6 Ablation Experiments
4. Limitations and Future Work 1. Extension to Linguistic Domains 2. Need for Weak Supervision 3.1 Downstream Utility 4.1 Dimensionality 4.2 Computational Cost

## Appendix A TPR Framework

In this section, we provide additional details regarding Smolensky's TPR framework [3], as well as formal proofs of presented results. We refer interested readers to [3] for a more comprehensive dive into the TPR framework.

### Additional Details

By defining the constituent components of any _compositional object_ as a set of role-filler bindings, the TPR defines the decomposition function, \(\beta\), of Section 3.1 that maps from a set of compositional objects, \(X\), to a set of parts, more explicitly as follows [3]:

\[\beta:X\to 2^{F\times R};x\rightarrow\{(f,r)|f/r\},\] (8)where \(F\) denotes a set of fillers, \(R\) denotes a set of roles, and \(f/r\) denotes the binding of filler \(f\in F\) to role \(r\in R\). Note that in contrast to the formal definition of \(\beta\) we use in Section 3.1, which assumes each \(x\in X\) is decomposable into a set of \(n\) parts, the above decomposition allows objects to be decomposed into a _variably-sized_ set of role-filler bindings, with this set corresponding to an element in the powerset of \(F\times R\). For the considered visual representation learning domain, all datasets clearly have the property of being decomposable into a _fixed_ size set of role-filler bindings, as all images in these datasets contain the same number of FoV _types_ and each FoV type is bound to a FoV _token_. Due to this property, we can take \(\beta\) as a special subcase of the generalised definition in Equation 8:

\[\beta:X\rightarrow\mathcal{A}:x\rightarrow\{(f_{m(i)},r_{i})|f_{m(i)}/r_{i}\}.\] (9)

where \(m:\{1,\ldots,N_{R}\}\rightarrow\{1,\ldots,N_{F}\}\) denotes a matching function that associates each role \(r_{i}\) with the filler it binds to in \(\beta(x)\) (we again drop the dependence of \(m\) on \(x\) for ease of notation), and \(\mathcal{A}\) denotes the set of all possible bindings produced by binding a filler to each of the \(N_{R}\) roles, with size \(N_{F}^{N_{R}}\) (we assume the same filler can bind to multiple roles).

### Formal Proofs

We now formally prove that the TPR with \(\beta\) defined in 9 has form \(\psi_{tpr}(x)=C(\psi_{1}(a_{1}),\ldots,\psi_{n}(a_{n}))\) and thus corresponds to the definition of a _compositional representation_ in Section 3.1.

Proof.: We denote the role embedding and filler embedding functions as \(\xi_{R}:R\to V_{R},\xi_{F}:F\to V_{F}\) respectively.

By definition of the TPR in Eq 1, we have that:

\[\psi_{tpr}(x):=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i}).\]

and hence,

\[\psi_{tpr}(x)=\sum_{i}\psi_{i}(f_{m(i)},r_{i}),\]

where \(a_{i}:=(f_{m(i)},r_{i})\in\beta(x)\), \(\psi_{i}:F\times R\to V_{F}\otimes V_{R};(f_{m(i)},r_{i})\rightarrow\xi_{F} (f_{m(i)})\otimes\xi_{R}(r_{i})\), and \(C\) is ordinary vector space addition. Hence, almost trivially, \(\psi_{tpr}(x)\) clearly has the required form to be a _compositional representation_. 

Now, we prove the recoverability of the embedded components \(\{\psi_{i}(f_{m(i)},r_{i})\}\) from the TPR, \(\psi_{tpr}(x)\), provided that the set of all role embedding vectors, \(\{\xi_{R}(r_{i})\}\), are linearly independent. Similar variants of this proof can be found in [3, 19].

Proof.: Assume the set of all role embedding vectors \(\{\xi_{R}(r_{i})\}\) are linearly independent. Then, the role embedding matrix, \(M_{\xi_{R}}:=(\xi_{R}(r_{1})\ldots\xi_{R}(r_{N_{R}}))\) formed by taking the role embedding vectors as columns, has a left inverse, \(U\), such that:

\[UM_{\xi_{R}}=I_{N_{R}\times N_{R}}.\]

Hence, we have that \((UM_{\xi_{R}})_{ij}=U_{i}.M_{\xi_{R}:j}=I_{ij}\).

For ease of notation, let \(u_{i}\) denote the \(i\)-th column of \(U^{T}\), and note that \(\xi_{R}(r_{j})\) clearly corresponds to \(M_{\xi_{R}:j}\). So, \(U_{i}.M_{\xi_{R}:j}=(U_{:i}^{T})^{T}M_{\xi_{R}:j}=u_{i}^{T}\xi_{R}(r_{j})=I_{ij}\).

Hence, we have that:

\[u_{i}^{T}\xi_{R}(r_{j})=\delta_{ij}=\begin{cases}1&i=j\\ 0&\text{otherwise}.\end{cases}\]Using the definition of \(\psi_{tpr}(x)\), \(\psi_{tpr}(x)=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\), we apply the (tensor) inner product of \(\psi_{tpr}(x)\) with \(u_{i}\):

\[\psi_{tpr}(x) =\left(\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\right)u_{i}\] \[=\left(\sum_{i}\xi_{F}(f_{m(i)})\xi_{R}(r_{i})^{T}\right)u_{i}\] \[=\sum_{i}\xi_{F}(f_{m(i)})\delta_{ji}\] \[=\xi_{F}(f_{m(i)}).\]

Thus, the embedding of the filler, \(f_{m(i)}\), bound to each role, \(r_{i}\), can be recovered through use of a tensor inner product with the _unbinding_ vector, \(u_{i}\), corresponding to the \(i\)-th column of \(U^{T}\). Note that the representational components of \(\psi_{tpr}(x)\), i.e., the embedded bindings, \(\psi_{i}(f_{m(i)},r_{i})\) are fully determined by the embedding of the role, \(\xi_{R}(r_{i})\), and filler, \(\xi_{F}(f_{m(i)})\), comprising the binding, as \(\xi_{F}(f_{m(i)},r_{i})\) simply corresponds to their tensor product. Thus, recovering \((\xi(f_{m(i)}),\xi_{R}(r_{i}))\) for each binding in \(\beta(x)\) corresponds to recovering the representational component, \(\psi_{i}(f_{m(i)},r_{i})\). So, provided the set of role embeddings are linearly independent, and they can be obtained (e.g. through a look-up table of role embeddings), all representational components, \(\psi_{i}(f_{m(i)},r_{i})\) can be directly recovered from the overall TPR representation, \(\psi_{tpr}(x)\). 

### Shortcomings of Symbolic Compositional Representations and How TPR Helps

Here, we provide a more detailed elaboration on the shortcomings of symbolic compositional representations as outlined in 1. We additionally illustrate how TPR-based continuous compositional representations circumvent such limitations through use of a concrete example.

We employ the formal definition of a _symbolic_ compositional representation from Section 3.1, which states a representation, \(\psi_{s}(x)\), is a _symbolic compositional representation_ if \(\psi_{s}(x)=\left(\psi_{1}(a_{1})^{T},...,\psi_{n}(a_{n})^{T}\right)^{T}\), i.e., if \(\psi_{s}(x)\) corresponds to a _concatenation_ of representational components, \(\{\psi_{i}(a_{i})\}\).

For more direct comparison between the symbolic compositional representation, \(\psi_{s}(x)\) and the TPR-based compositional representation, \(\psi_{tpr}(x)\), we rewrite the TPR-based representation as \(\psi_{tpr}(x)=\tilde{\psi}(\tilde{a}_{1})+\ldots+\tilde{\psi_{n}}(\tilde{a}_{ n})\). Thus, the TPR can be viewed as an _ordinary vector sum_ of components, \(\{\tilde{\psi}(\tilde{a}_{i})\}\), where each component, \(\tilde{\psi_{i}}\) corresponds to an embedded role-filler binding.

We consider the following example: suppose there are 2 FoV types (i.e., roles), \(\mathrm{colour}\), \(\mathrm{shape}\) and 2 FoV values (i.e., fillers), \(\mathrm{purple},\mathrm{square}\). For the _symbolic_ approach, suppose the _representational components_, \(\{\psi_{i}(a_{i})\}\) are defined as follows: \(\psi_{\mathrm{colour}}(\mathrm{purple})=\begin{bmatrix}1\\ 0\end{bmatrix}\) and \(\psi_{\mathrm{shape}}(\mathrm{square})=\begin{bmatrix}2\\ 3\end{bmatrix}\).

For the _distributed_ approach, suppose the (linearly independent) role embeddings are: \(\xi_{R}(\mathrm{colour})=\begin{bmatrix}1\\ 2\end{bmatrix}\), \(\xi_{R}(\mathrm{shape})=\begin{bmatrix}1\\ 1\end{bmatrix}\), and the filler embeddings are: \(\xi_{F}(\mathrm{purple})=\begin{bmatrix}1\\ 1\end{bmatrix}\) and \(\xi_{F}(\mathrm{square})=\begin{bmatrix}2\\ 3\end{bmatrix}\).

Then, a symbolic compositional representation is given by (denoting the concatenation operation by \(\oplus\)):

\[\psi_{s}(x) =\psi_{\mathrm{colour}}(\mathrm{purple})\oplus\psi_{\mathrm{shape} }(\mathrm{square})\] \[=\begin{bmatrix}1\\ 0\end{bmatrix}\oplus\begin{bmatrix}2\\ 3\end{bmatrix}=\begin{bmatrix}1\\ 0\\ 2\\ 3\end{bmatrix}\]

On the other hand, the distributed, TPR-based compositional representation is given by:\[\psi_{tpr}(x) =\tilde{\psi}(\text{purple/colour})+\tilde{\psi}(\text{square/shape})\] \[=\xi_{F}(\text{purple})\otimes\xi_{R}(\text{colour})+\xi_{F}(\text{ square})\otimes\xi_{R}(\text{shape})\] \[=\begin{bmatrix}1\\ 1\\ 2\\ 2\end{bmatrix}+\begin{bmatrix}2\\ 3\\ 2\\ 3\end{bmatrix}=\begin{bmatrix}3\\ 4\\ 4\end{bmatrix}\]

Note that _both_ the symbolic and distributed, TPR-based representations share the same underlying representational space, i.e., \(\mathbb{R}^{4}\).

1. **Gradient Flow and Learning6**: The _symbolic_ compositional representation allocates the FoVs to 2 distinct, independent slots of \(\psi_{d}(x)\). This creates discrete boundaries between the first two and last two dimensions of the representation, which may potentially complicate gradient-based optimisation for the following reasons: Footnote 6: Please note there is a small error in the main Author Rebuttal https://openreview.net/forum?id=oEVisxVdush&noteId=GiOPkcNVtts in L16 under section ‘1) Incompatibility between disentangled representations and deep learning’s continuous vector spaces’, where the tensor product is mistakenly taken over \(\psi_{col}(purple)\otimes\psi_{sh}(square)\) to produce the representation, instead of computing \(\xi_{F}(purple)\otimes\xi_{R}(col)+\xi_{F}(square)\otimes\xi_{R}(sh)\), but the conclusion remains exactly the same. (Note that \(\psi_{col}(purple)=\xi_{F}(purple)\otimes\xi_{R}(col)\) and \(\psi_{sh}(square)=\xi_{F}(square)\otimes\xi_{F}(sh)\) should _both_ be 4-dimensional, _not_ 2-dimensional. 1. **Discrete Boundaries and Restricted Gradient Flow**: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 6: Please note there is a small error in the main Author Rebuttal https://openreview.net/forum?id=oEVisxVdush&noteId=GiOPkcNVtts in L16 under section ‘1) Incompatibility between disentangled representations and deep learning’s continuous vector spaces’, where the tensor product is mistakenly taken over \(\psi_{col}(purple)\otimes\psi_{sh}(square)\) to produce the representation, instead of computing \(\xi_{F}(purple)\otimes\xi_{R}(col)+\xi_{F}(square)\otimes\xi_{R}(sh)\), but the conclusion remains exactly the same. (Note that \(\psi_{col}(purple)=\xi_{F}(purple)\otimes\xi_{R}(col)\) and \(\psi_{sh}(square)=\xi_{F}(square)\otimes\xi_{F}(sh)\) should _both_ be 4-dimensional, _not_ 2-dimensional. 2. **Abrupt, Discontinuous Shifts in the Representational Space**: When multiple FoVs are updated consecutively, the symbolic, slot-based representational form may induce sudden and discontinuous changes in the representational space. For instance, updating \(\text{purple/colour}\) at time \(t\) and then \(\text{shape/square}\) at \(t+1\) produces: Footnote 6: Please note there is a small error in the main Author Rebuttal https://openreview.net/forum?id=oEVisxVdush&noteId=GiOPkcNVtts in L16 under section ‘1) Incompatibility between disentangled representations and deep learning’s continuous vector spaces’, where the tensor product is mistakenly taken over \(\psi_{col}(purple)\otimes\psi_{sh}(square)\) to produce the representation, instead of computing \(\xi_{F}(purple)\otimes\xi_{R}(col)+\xi_{F}(square)\otimes\xi_{R}(sh)\), but the conclusion remains exactly the same. (Note that \(\psi_{col}(purple)=\xi_{F}(purple)\otimes\xi_{R}(col)\) and \(\psi_{sh}(square)=\xi_{F}(square)\otimes\xi_{F}(sh)\) should _both_ be 4-dimensional, _not_ 2-dimensional. \[\psi_{tpr}(x) =\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}( x)+\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}(x)+\psi_{tpr}(x)\] \[=\begin{bmatrix}1\\ 1\\ 2\\ 2\end{bmatrix}+\begin{bmatrix}2\\ 3\\ 3\\ 3\end{bmatrix}=\begin{bmatrix}3\\ 4\\ 4\\ 4\end{bmatrix}\] \[=\begin{bmatrix}1\\ 2\\ 2\end{bmatrix}+\begin{bmatrix}2\\ 3\\ 3\\ 3\end{bmatrix}=\begin{bmatrix}3\\ 4\\ 4\\ 4\end{bmatrix}\]

Note that _both_ the symbolic and distributed, TPR-based representations share the same underlying representational space, i.e., \(\mathbb{R}^{4}\).

1. **Gradient Flow and Learning7**: The _symbolic_ compositional representation allocates the FoVs to 2 distinct, independent slots of \(\psi_{d}(x)\). This creates discrete boundaries between the first two and last two dimensions of the representation, which may potentially complicate gradient-based optimisation for the following reasons: Footnote 7: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 8: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 8: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 8: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 9: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 9: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 10: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 11: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 12: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. This type of assignment means that updates to a particular FoV requires restricting gradient flow exclusively to the dimensions associated with its designated slot. For example, updating the FoV for \(\text{square/shape}\) requires modifying the last two dimensions (i.e., permitting gradient flow through these two dimensions), while keeping the first two dimensions unchanged (i.e., completely restricting gradient flow through these two dimensions). This constrains gradient flow across _all_ dimensions of representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 13: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 14: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 15: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 16: In the symbolic compositional representation, each FoV is confined to specific, non-overlapping dimensions of the representation. Mathematically, the gradient update for the above example (modifying \(\text{square/shape}\)) can be expressed in the symbolic case as: Footnote 17: In the symbolic representation, each FoV is confined to specific, non-overlapping dimensions of the representation. Mathematically, the gradient update for the In contrast, the _continuous, distributed_, TPR-based compositional representation integrates the FoVs continuously into all _dimensions_ of the representation (i.e., \(\tilde{\psi}(\mathrm{purple/colour})=(1\:1\:2\:2)^{T}\) and \(\tilde{\psi}(\mathrm{square/shape})=(2\:3\:2\:3)^{T}\) are summed together in \(\mathbb{R}^{4}\)), eliminating the necessary existence of dimension-wise slots with non-differentiable boundaries for each FoV. This alleviates the previously mentioned problems:

1. **Smoother Gradient Flow**: The inherently _distributed_ structure of the TPR allows gradients to flow _freely_ across all dimensions of the representational space, \(\mathbb{R}^{4}\). Updating the FoV for \(\mathrm{square/shape}\) involves modifying all relevant dimensions simultaneously, avoiding categorical dimension-wise restrictions and potentially allowing for more flexible and effective gradient-based optimisation as FoVs can be updated in a _coordinated_ manner (note that backpropagating through the dimensions corresponding to \(\mathrm{square/shape}\)_can_ also affect \(\mathrm{purple/colour}\), as the same dimensions are occupied): \[-\eta\nabla_{\theta_{c}}L(\theta_{c})=-\eta\begin{bmatrix}\hat{a}\\ \hat{b}\\ \hat{c}\\ \hat{d}\end{bmatrix}\:\text{, where }\hat{a},\hat{b},\hat{c},\hat{d}\in\mathbb{R}\] 2. **Smoother Learning Dynamics**: By forgoing the slot-based structure, a distributed encoding of compositional structure ensures that update transitions between FoVs do not cause sudden shifts in the representational space. For example: \[\psi_{tpr}(x)_{t} =\psi_{tpr}(x)_{t-1}-\eta\begin{bmatrix}\hat{a}_{t-1}\\ \hat{b}_{t-1}\\ \hat{c}_{t-1}\\ \hat{d}_{t-1}\end{bmatrix}\] \[\psi_{tpr}(x)_{t+1} =\psi_{tpr}(x)_{t}-\eta\begin{bmatrix}\hat{a}_{t}\\ \hat{b}_{t}\\ \hat{c}_{t}\\ \hat{d}_{t}\end{bmatrix}\:\text{,}\] potentially promoting more stable and reliable convergence during training.
2. **Representational Expressivity**: In the example above, both \(\psi_{s}(x)\) and \(\psi_{tpr}(x)\) reside in \(\mathbb{R}^{4}\). However, the symbolic, slot-based compositional representation, \(\psi_{s}(x)\) restricts each FoV to a specific, 2-dimensional subset (i.e., \(\psi_{\mathrm{colour}}(\mathrm{purple})\), \(\psi_{\mathrm{shape}}(\mathrm{square})\) each belong to \(\mathbb{R}^{2}\)). This rigid allocation limits expressivity as each FoV is confined to a subset of available dimensions, preventing the full utilisation of \(\mathbb{R}^{4}\). In contrast, an inherently distributed approach allows each FoV - such as \(\tilde{\psi}(\mathrm{purple/colour}),\tilde{\psi}(\mathrm{square/shape})\) - to exist within the same \(\mathbb{R}^{4}\) space as the overall representation, \(\psi_{tpr}(x)\). This allows the representation learner to encode FoVs as combinations of the entire set of 4-dimensional basis vectors spanning the representational space, offering enhanced expressivity.
3. **Robustness to Noise**: In symbolic compositional representations, the impact of dimension-wise noise has a localised sensitivity: since each FoV is isolated within its designated slot, noise directly degrades the representation of the corresponding FoV. For example, a small perturbation in the third dimension (assigned to \(\mathrm{square/shape}\)) has a localised, direct effect only on this FoV. In contrast, for the inherently distributed TPR-based structure, noise introduced to any dimension of the representation is _distributed_ across multiple FoVs. For example, for \(\psi_{tpr}(x)\), the same perturbation in the third dimension will have its impact distributed across both \(\mathrm{square/shape}\) and \(\mathrm{purple/colour}\), as both these FoVs contribute to that dimension.

### TPR's Distributed Nature and the Equivalence between Degenerate TPRs and Symbolic Representations

The TPR offers an inherently _distributed_ representation of compositional structure, allowing information from multiple FoVs to intermingle across the same dimensions of the representational space.

Here, we elaborate on 2 critical properties of the TPR in relation to this notion of a _distributed_ embedding of compositional structure.

1. **Equivalence of Degenerate TPRs to Concatenated Fillers**: While the TPR naturally _supports_ a distributed encoding of compositional structure, it is worth noting that when the role embedding vectors are aligned with the canonical basis vectors, the TPR reduces to concatenated filler vectors, thereby reducing to a symbolic compositional representation where the representational components \(\psi_{i}(a_{i})\) correspond to filler embeddings. In contrast to the TPR, however, it is impossible for a symbolic compositional representations to encode compositional structure in a distributed fashion.
2. **Distributed Representation through Dense Role Vectors**: In the general case where the role vectors are linearly independent (but do not correspond to the canonical basis), provided that the role vectors are _dense_, then TPRs facilitate a truly distributed representation by allowing each dimension of the representation to encapsulate a summation of information from multiple fillers. Such a representation is _not_ reducible to a dimension-wise concatenation of fillers, as in the degenerate TPR case.

We now prove both statements:

**Equivalence of Degenerate TPRs to Concatenated Fillers:**

Let \(\{e_{1},e_{2},\ldots,e_{D_{R}}\}\) denote the canonical basis vectors in \(\mathbb{R}^{D_{R}}\), where each \(e_{k}\) has a 1 in the \(k\)-th position and 0s elsewhere.

Assume that each role vector \(r_{i}=e_{i}\) for \(i=1,2,\ldots,N_{R}\).

The tensor product \(f_{m(i)}\otimes r_{i}\) results in a matrix where the \(i\)-th column is \(f_{m(i)}\) and all other columns are zero vectors:

\[f_{m(i)}\otimes r_{i}=f_{m(i)}e_{i}^{\top}=\begin{bmatrix}0&\cdots&f_{i}^{1}& \cdots&0\\ 0&\cdots&f_{i}^{2}&\cdots&0\\ \vdots&&\vdots&&\vdots\\ 0&\cdots&f_{i}^{D_{F}}&\cdots&0\end{bmatrix}\]

Summing over all \(N_{R}\) role-filler bindings in \(\beta(x)\) gives the required result:

\[\psi_{tpr}(x)=\sum_{i=1}f_{m(i)}\otimes e_{i}=\begin{bmatrix}f_{1}^{1}&f_{2}^{ 1}&\cdots&f_{D_{R}}^{1}\\ f_{1}^{2}&f_{2}^{2}&\cdots&f_{D_{R}}^{2}\\ \vdots&\vdots&\ddots&\vdots\\ f_{1}^{D_{F}}&f_{2}^{D_{F}}&\cdots&f_{D_{R}}^{D_{F}}\end{bmatrix}\]

This shows that in the case where the role vectors correspond to the canonical basis vectors, the the TPR effectively concatenates the filler vectors \(f_{i}\) along distinct, non-overlapping subsets of representational dimensions. Consequently, the TPR \(\psi_{tpr}(x)\) behaves equivalently to a symbolic, slot-based compositional structure where each filler occupies a unique representational slot without interacting with other fillers, hence validating the equivalence of degenerate TPRs to concatenated fillers.

**Distributed Representation through Dense Role Vectors:**

Assume that the role vectors \(\{r_{1},r_{2},\ldots,r_{N}\}\) in \(\mathbb{R}^{D_{R}}\) are dense and linearly independent but are not aligned with the canonical basis vectors.

Each tensor product \(f_{m(i)}\otimes r_{i}\) yields a matrix where each element \((j,k)\) is \(f_{m(i)}^{j}r_{i}^{k}\). Summing across all role-filler bindings yields:

\[\psi_{tpr}(x)=\sum_{i=1}f_{m}(i)\otimes r_{i}=\begin{bmatrix}\sum_{i=1}f_{m(i) }^{1}r_{i}^{1}&\sum_{i=1}f_{m(i)}^{1}r_{i}^{2}&\cdots&\sum_{i=1}^{N}f_{m(i)}^{1 }r_{i}^{D_{R}}\\ \sum_{i=1}^{N}f_{m(i)}^{2}r_{i}^{1}&\sum_{i=1}^{N}f_{m(i)}^{2}r_{i}^{2}&\cdots &\sum_{i=1}^{N}f_{m(i)}^{2}r_{i}^{D_{R}}\\ \vdots&\vdots&\ddots&\vdots\\ \sum_{i=1}^{N}f_{m(i)}^{D_{F}}r_{i}^{1}&\sum_{i=1}^{N}f_{m(i)}^{D_{F}}r_{i}^{ 2}&\cdots&\sum_{i=1}^{N}f_{m(i)}^{D_{F}}r_{i}^{D_{R}}\end{bmatrix}\]

Given that role vectors are dense, each element \(\psi_{tpr}^{j,k}\) incorporates contributions from multiple fillers \(f_{m(i)}^{j}\), each weighted by the respective component \(r_{i}^{k}\) of their role vectors. This intermingling ensuresthat information from different filler vectors are distributed across the same dimensions of \(\psi_{tpr}(x)\), rather than being confined to distinct subsets of \(\psi_{tpr}(x)\)'s dimensions, as in the degenerate TPR case.

## Appendix B Soft TPR Framework

In this section, we provide additional information regarding our Soft TPR framework.

### Shortcomings of the TPR and How Soft TPR Helps

In this subsection, we discuss potential practical limitations of traditional TPR due to their rigid specification (Eq 1) and demonstrate how Soft TPR addresses these issues using a concrete example.

Consider the following set of roles \(R=\{\mathrm{shape},\mathrm{colour}\}\), and fillers \(F=\{\mathrm{red},\mathrm{blue},\mathrm{square}\}\), with embedding functions \(\xi_{R}:R\rightarrow\mathbb{R}^{2}\) and \(\xi_{F}:F\rightarrow\mathbb{R}^{3}\) defined as:

\[\xi_{R}(\mathrm{shape}) =\begin{bmatrix}1\\ 0\end{bmatrix},\] \[\xi_{R}(\mathrm{colour}) =\begin{bmatrix}1\\ 1\end{bmatrix},\] \[\xi_{F}(\mathrm{red}) =\begin{bmatrix}1\\ 2\\ 3\end{bmatrix},\] \[\xi_{F}(\mathrm{blue}) =\begin{bmatrix}2\\ 2\\ 3\end{bmatrix},\] \[\xi_{F}(\mathrm{square}) =\begin{bmatrix}0\\ 0\\ 1\end{bmatrix},\]

1. **Discrete Mapping**: The set of possible TPRs \(T\) generated by these roles and fillers is limited to discrete points. For instance: \[\psi_{tpr}(x_{\text{red\_square}}) =\xi_{F}(\mathrm{red})\otimes\xi_{R}(\mathrm{colour})+\xi_{F}( \mathrm{square})\otimes\xi_{R}(\mathrm{shape})\] \[=\begin{bmatrix}1\\ 2\\ 3\end{bmatrix}\otimes\begin{bmatrix}1\\ 1\end{bmatrix}+\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}\otimes\begin{bmatrix}1\\ 0\end{bmatrix}\cong\begin{bmatrix}1\\ 2\\ 3\\ 1\\ 2\\ 3\end{bmatrix}+\begin{bmatrix}0\\ 0\\ 0\\ 0\end{bmatrix}\] \[=\begin{bmatrix}1\\ 2\\ 4\\ 1\\ 2\\ 3\end{bmatrix}\]\[\psi_{tpr}(x_{\text{blue\_square}}) =\xi_{F}(\text{blue})\otimes\xi_{R}(\text{colour})+\xi_{F}(\text{ square})\otimes\xi_{R}(\text{shape})\] \[=\begin{bmatrix}2\\ 2\\ 3\end{bmatrix}\otimes\begin{bmatrix}1\\ 1\end{bmatrix}+\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}\otimes\begin{bmatrix}1\\ 0\end{bmatrix}\cong\begin{bmatrix}2\\ 3\\ 2\\ 2\\ 3\end{bmatrix}+\begin{bmatrix}0\\ 1\\ 0\\ 0\end{bmatrix}\] \[=\begin{bmatrix}2\\ 2\\ 4\\ 2\\ 3\end{bmatrix}\] Thus, \(T=\{(2\;2\;4\;2\;2\;3)^{T},(1\;2\;4\;1\;2\;3)^{T}\}\), a discrete subset of \(\mathbb{R}^{6}\). By relaxing the TPR specification to the _Soft TPR_, we allow any \(z\in\mathbb{R}^{6}\) within an \(\epsilon\)-neighbourhood of some \(\psi_{tpr}\) in \(T\), thereby forming continuous regions defined as: \(T_{S}:=\{(2\;2\;4\;2\;2\;3)^{T}+\alpha,(1\;2\;4\;1\;2\;3)^{T}+\alpha:|\alpha|<\epsilon\}\). This relaxation significantly expands the representational space, as \(T_{S}\) has strictly more points than \(T\). Consequently, learning theoretically becomes easier because there are more functions parameterising the mapping from the observed data to \(T_{S}\) compared to \(T\). Furthermore, in contrast to \(T\), which contains discrete, singular points scattered throughout \(\mathbb{R}^{6}\), \(T_{S}\) comprises continuous spherical regions centered around each \(\psi_{tpr}\in T\). These factors collectively make the Soft TPR representation potentially easier to learn and extract information from compared to the TPR. This advantage is reflected in our empirical results in Section C.6.3, where the Soft TPR demonstrates 1) greater representation learner convergence, 2) superior sample efficiency for downstream tasks, and 3) superior raw downstream performance in the low sample regime compared to the traditional TPR. Additionally, our ablation results in Table 43 indicate that imposing stricter constraints on the encoder to produce representations in the form of explicit TPRs substantially impairs the disentanglement of the resulting representations.
2. **Quasi-Compositional Structure**: The traditional TPR enforces a strict algebraic definition of compositionality, specifically of the form \(\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\). However, even in algebraically-structured domains such as natural language, compositional structures do not always adhere to this rigid specification. For instance, French liaison consonants consist of a weighted sum of fillers bound to a single role [9], which deviates from the strict TPR formulation. The Soft TPR's continuous relaxation of this constraint permits the representation of structures that only _approximately_ satisfy the TPR's algebraic definition of compositional structure as a set of discrete role-filled bindings. Consequently, Soft TPRs can encode forms of quasi-compositionality that are not strictly algebraically precise or directly amenable to TPR-based symbolic expression (see B.2 for a proof).
3. **Duality**: Although Soft TPRs employ a relaxed structural specification to encode _quasi-compositional structures_, they inherently preserve the exact algebraic form of compositionality through their corresponding explicit TPRs, \(\psi_{tpr}^{*}\), via quantisation using the TPR Decoder. In other words, while Soft TPRs facilitate approximate and flexible compositional representations, they retain the capability to revert to the precise algebraic structures defined by traditional TPRs when necessary. This dual functionality ensures that Soft TPRs strike a balance between the flexibility required to represent complex, approximately compositional structures--which benefits learning through relaxed representational constraints--and the preservation of exact algebraic forms essential for symbolic manipulation and interpretation.
4. **Serial Construction**: Building explicit TPRs requires that the embedded fillers and roles comprising a binding (i.e. \((\xi_{F}(f_{m(i)}),\xi_{R}(r_{i}))\)) are first _tokened_ before the entire compositional representation, \(\psi_{tpr}(x)\) can be produced [19, 23, 28, 34, 38, 51, 52]. This sort of sequential approach, where constituents must be tokened before the compositional representation can be formed, is a key characteristic of the symbolic representation of compositional structure [45]. In contrast, the Soft TPR allows the Encoder to produce any _arbitrary_ element of \(V_{F}\otimes V_{R}\) (in this case \(\cong\mathbb{R}^{6}\)), provided that the sufficient closeness requirement holds. Thus, once the Encoder is trained, it is theoretically possible for the Soft TPR Autoencoderto generate approximately compositional representations by mapping directly from the data to a Soft TPR in a single step, without needing to token any representational constituents.

### Proof that Soft TPRs Do Not Necessarily Have the Explicit Form of a TPR

It is intuitive that Soft TPRs, due to their continuous relaxation, do not necessarily conform to the explicit algebraic form \(\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\) required by traditional TPRs. We provide a formal proof for completeness.

Proof.: Let \(\xi_{R}:R\to V_{R}\) and \(\xi_{F}:F\to V_{F}\) be fixed role and filler embedding functions, respectively. A traditional TPR is defined as:

\[\psi_{tpr}(x)=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i}),\]

where \(m(i)\) is a matching function assigning each role \(r_{i}\) to a filler \(f_{m(i)}\).

A Soft TPR \(z\) is any element within an \(\epsilon\)-neighborhood of some traditional TPR in \(V_{F}\otimes V_{R}\):

\[z\in\mathcal{B}_{\epsilon}(\psi_{tpr})=\left\{z\in V_{F}\otimes V_{R}\,|\, \|z-\psi_{tpr}\|<\epsilon\right\},\]

where \(\epsilon>0\).

Suppose, for contradiction, that every Soft TPR \(z\in\mathcal{B}_{\epsilon}(\psi_{tpr})\) can be expressed as another traditional TPR with a different matching function \(m^{\prime}\):

\[z=\sum_{i}\xi_{F}(f_{m^{\prime}(i)})\otimes\xi_{R}(r_{i}).\]

Given that \(\xi_{R}\) and \(\xi_{F}\) are fixed, varying \(m^{\prime}\) only changes the assignment of fillers to roles without altering the underlying embeddings.

Let \(\delta=z-\psi_{tpr}\), where \(\|\delta\|<\epsilon\). Substituting, we obtain:

\[\sum_{i}\xi_{F}(f_{m^{\prime}(i)})\otimes\xi_{R}(r_{i})=\psi_{tpr}+\delta.\]

Since \(\delta\neq 0\), this equality implies:

\[\sum_{i}\xi_{F}(f_{m^{\prime}(i)})\otimes\xi_{R}(r_{i})\neq\psi_{tpr}.\]

However, because \(\xi_{R}\) and \(\xi_{F}\) are fixed, the only way to satisfy this equality is if the matching function \(m^{\prime}\) compensates exactly for \(\delta\). This is generally impossible due to the following constraints:

1. **Fixed Embeddings**: \(m^{\prime}\) can only reassign fillers to roles without altering \(\xi_{R}(r_{i})\) or \(\xi_{F}(f_{m(i)})\).The fixed \(\xi_{F}(f_{m(i)})\) restrict the possible variations, making it infeasible to account for arbitrary \(\delta\) through mere reassignment.
2. **Dimensionality and Span**: In high-dimensional spaces \(V_{F}\otimes V_{R}\), \(\delta\) may lie outside the span of all traditional TPRs generated by varying \(m^{\prime}\), especially as \(\epsilon\) increases.

Consequently, the perturbation \(\delta\) cannot be exactly compensated by any reassignment \(m^{\prime}\), leading to a contradiction. Therefore, Soft TPRs with non-zero \(\epsilon\)-distances from their corresponding quantised TPRs, \(\psi_{tpr}^{*}\), are also **not necessarily expressible** as other TPRs in the same underlying space.

The introduction of a continuous relaxation in Soft TPRs ensures that they do not strictly adhere to the traditional TPR's explicit algebraic form. This allows Soft TPRs to represent quasi-composition structures that deviate from strict algebraic compositionality, which extends distributed compositional representations beyond the formal, explicitly algebraic domains required by traditional TPRs.

### Alternative Formulations

In contrast to the greedily optimal analytic form of \(\psi^{*}_{tpr}\) derived in Equation 5, a globally optimal TPR with respect to \(||z-\psi_{tpr}||_{F}\) can also be formulated. By fixing the sets of fillers \(F\) and roles \(R\), along with the embedding functions \(\xi_{F}:F\rightarrow\mathbb{R}^{D_{F}}\) and \(\xi_{R}:R\rightarrow\mathbb{R}^{D_{R}}\), the only degree of freedom lies in defining \(\mathcal{M}^{*}\), the set of role-filler matching functions that specify permissible role-filler binding decompositions for \(x\in X\). The globally optimal TPR that best approximates \(z\), denoted \(\psi^{opt}_{tpr}\), is given by:

\[\psi^{opt}_{tpr}:=\operatorname*{arg\,min}_{\psi_{tpr}\in\mathcal{T}}\|z-\psi_{ tpr}\|_{F},\] (10)

where \(\mathcal{T}:=\left\{\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\right\}_{m \in\mathcal{M}^{*}}\) denotes the set of all possible traditional TPRs defined by the choices of \(F\), \(R\), \(\xi_{F}\), \(\xi_{R}\), and \(\mathcal{M}^{*}\). For clarity, we omit the dependency of both \(\psi_{tpr}\) and \(m\) on \(x\).

If \(\mathcal{M}^{*}\) is a restricted subset of the entire set of matching functions \(\mathcal{M}\), meaning there are constraints on which fillers can be bound to which roles, then solving Equation 10 is NP-complete. Conversely, if \(\mathcal{M}^{*}=\mathcal{M}\), allowing any filler to be bound to any role with possible reuse, the optimisation simplifies significantly. In this unrestricted scenario, the problem reduces to independently assigning each role to the filler that minimises the local contribution to \(||z-\psi_{tpr}||_{F}\), which can be solved efficiently with a time complexity of \(O(N_{R}\times N_{F})\). Acknowledging the impracticality of exact global optimisation, and guided by the intuition that \(\psi^{*}_{tpr}\) should have an explicit dependency on \(z\), we propose the greedily optimal definition of \(\psi^{*}_{tpr}\) in Equation 5, which links the unbound _soft_ fillers of \(z\), \(\{\tilde{f}_{i}\}\), to \(\psi^{*}_{tpr}\).

### Soft TPR Autoencoder

Defining \(m(i):=\operatorname*{arg\,min}_{j}\|\tilde{f}_{k}-\xi_{F}(f_{j})\|_{2}\) in Equation 5 allows any filler \(f_{j}\) to bind to a role \(r_{i}\), which may not preserve the ground-truth semantics of the role-filler bindings in the image. To address this, the Soft TPR Autoencoder targets two objectives jointly: 1) _Representational Form_, ensuring the output resembles a Soft TPR, and 2) _Representational Content_, ensuring sensible role-filler bindings reflecting ground-truth semantics.

#### b.4.1 Representational Form

We now describe how our method satisfies the representational form property. As detailed in Section 4.2, we penalize the Euclidean distance between the encoder \(E\)'s output \(z\) and the explicit TPR \(\psi^{*}_{tpr}\) defined in Equation 5. This penalty encourages the Soft TPR Autoencoder to produce encodings that resemble a Soft TPR by enforcing \(\|z-\psi_{tpr}\|_{2}<\epsilon^{\prime}\). To obtain \(\psi^{*}_{tpr}\), we employ our TPR decoder, which consists of three modules: **1) Unbinding**, **2) Quantization**, and **3) TPR Construction**.

**1) Unbinding**: Although multiple roles can bind to the same filler (e.g., \(\operatorname{object\,colour/magenta}\) and \(\operatorname{floor\,colour/magenta}\)), each role represents an independent concept type. Therefore, we use linearly independent embedding vectors for roles to ensure the recoverability of embedded components \((\xi_{F}(f_{m(i)}),\xi_{R}(r_{i}))\) from the TPR. Typically, a randomly initialised role embedding matrix \(M_{\xi_{R}}\in\mathbb{R}^{D_{R}\times N_{R}}\) with \(D_{R}\gg N_{R}\) will have linearly independent columns. To maintain this property during training, it is possible to apply orthogonality regularisation: \(\|M^{T}_{\xi_{R}}M_{\xi_{R}}-I\|_{F}\).

However, using arbitrary linearly independent role embeddings poses a computational challenge in obtaining \(U\), the (left) inverse of \(M_{\xi_{R}}\), necessary for unbinding vectors \(\{u_{i}\}\). To address this, we utilise semi-orthogonal matrices \(A\in\mathbb{R}^{d\times n}\) where \(A^{T}A=I\). In this case, \(U^{T}=M_{\xi_{R}}\), and the unbinding vector \(u_{i}\) is simply the role embedding \(\xi_{R}(r_{i})\). Thus, unbinding a soft filler from \(z\) bound to role \(r_{i}\) involves taking the tensor inner product between \(z\) and \(\xi_{R}(r_{i})\).

To implement this, our unbinding module initialises \(M_{\xi_{R}}\) as a dense, semi-orthogonal matrix using \(\operatorname{torch.nn.utils.parameterizations.orthogonal}\) and keeps \(M_{\xi_{R}}\) fixed during training to preserve its semi-orthogonal property 8.

Footnote 8: As the authors are interested in the context of the \(\xi_{R}\)-norm, we use the \(\xi_{R}\)-norm to denote the \(\xi_{R}\)-norm.

Now, we proceed to formally prove the aforementioned semi-orthogonality properties.

We first prove that for a (left-invertible) semi-orthogonal matrix, \(A\in\mathbb{R}^{d\times n}\), \(A^{T}A=I_{n\times n}\).

Proof.: Let \(A\) denote a (left-invertible) semi-orthogonal matrix of dimension \(\mathbb{R}^{d\times n}\). Writing \(A\) out using its columns, \(\{a_{1},\ldots,a_{n}\}\), we have:

\[A^{T}A=\begin{pmatrix}a_{1}^{T}\\ \vdots\\ a_{n}^{T}\end{pmatrix}(a_{1}\quad\cdots\quad a_{n})\]

As the columns \(\{a_{1},\ldots,a_{n}\}\) are orthonormal, we have that: \(a_{i}\cdot a_{j}=\delta_{ij}\), where \(\cdot\) denotes the dot product, and

\[\delta_{ij}=\begin{cases}1&i=j\\ 0&\text{otherwise,}\end{cases}\]

from which the result clearly follows. 

For completeness, we also prove that \(u_{i}\) corresponds to the \(i\)-th role embedding vector, though this follows rather trivially from the properties of semi-orthogonal matrices, and the definition of \(u_{i}\).

Proof.: Let \(M_{\xi_{R}}\) be a (left-invertible) semi-orthogonal matrix. Then, from the properties of semi-orthogonal matrices proved earlier, we have that \(M_{\xi_{R}}^{T}M_{\xi_{R}}=I\), and so, \(U^{T}=\left(M_{\xi_{R}}^{T}\right)^{T}=M_{\xi_{R}}\). Hence, the \(i\)-th column of \(U^{T}\), which corresponds to the unbinding vector, \(u_{i}\), by definition, is simply \((M_{\xi_{R}})_{:i}=\xi_{R}(r_{i})\), the \(i\)-th role embedding vector. 

**2) Quantisation**

The quantisation module utilises the VQ-VAE algorithm [11] to learn the filler embedding matrix \(M_{\xi_{F}}\) and to quantise the soft filler embeddings \(\{\tilde{f}_{i}\}\) into explicit embeddings \(\{\xi_{F}(f_{i})\}\) based on the closest Euclidean distances. This process aligns with the matching function \(m\) in Equation 5, where each soft filler \(\tilde{f}_{i}\) is assigned to the nearest explicit filler in \(M_{\xi_{F}}\). Since quantisation involves an argmax operation and is non-differentiable, VQ-VAE employs an \(L_{2}\)_codebook loss_ to adjust the embedding vectors towards the soft fillers and a _commitment loss_ to ensure the encoder commits to specific embeddings, preventing the embedding space from expanding indefinitely.

For further details on the quantisation process, refer to [11].

**3) TPR Construction**

The TPR construction module is parameter-free and deterministically binds the quantised filler embeddings \(\{\xi_{F}(f_{m(i)})\}\) to their corresponding role embeddings \(\{\xi_{R}(r_{i})\}\) from \(M_{\xi_{R}}\), forming the explicit TPR:

\[\psi^{*}_{tpr}=\sum_{i}\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i}).\]

#### b.4.2 Representational Content

Here, we explicitly define the'swapped' TPRs, \(\psi^{s}_{tpr}(x)\) and \(\psi^{s}_{tpr}(x^{\prime})\), used in our weakly supervised reconstruction loss (second term of Equation 7). For image pairs \((x,x^{\prime})\) sharing all role-filler bindings

[MISSING_PAGE_FAIL:25]

### Model Hyperparameters and Hyperparameter Tuning

The Soft TPR Autoencoder has the following tunable hyperparameters:

**Architectural Hyperparameters**:

\[N_{R},\quad N_{F},\quad D_{R},\quad D_{F}\]

These correspond to the number of role and filler embedding vectors and the dimensionalities of their respective embedding spaces.

**Loss Function Hyperparameters**:

\[\beta\quad(\text{which weights the VQ-VAE commitment loss as per Equation 6 and})\] \[\lambda_{1},\quad\lambda_{2}\quad(\text{which weigh the unsupervised and supervised losses in Equation 7})\]

Following the VQ-VAE framework [11], we set \(\beta\), the coefficient for the commitment loss, to 0.5. To ensure fair comparisons with scalar-tokened generative baselines and COMET, which assume 10 FoV types (and VCT assumes 20), we fix \(N_{R}=10\).

We optimise the remaining hyperparameters using the Weights and Biases (WandB) hyperparameter sweep framework. The optimisation employs Bayesian search with the unsupervised MSE reconstruction loss from Equation 6 as the criterion. During this optimization, models are trained for between 50,000 and 100,000 iterations.

The final hyperparameter values for the Soft TPR Autoencoder are listed in Table 11. For ablation experiments demonstrating the model's robustness to different hyperparameter settings, refer to Section C.6.4.

Our model is implemented in Pytorch [27] and trained using the Adam optimiser on the loss corresponding to 7. We use a learning rate of \(1\mathrm{e}{-4}\), and the default setting of \((\beta_{1},\beta_{2})=(0.9,0.999)\) across all instances of model training.

\begin{table}
\begin{tabular}{c c} \hline \hline \multicolumn{2}{c}{Decoder} \\ \hline Input \(D_{F}\cdot D_{R}\) \\ ConvTranspose \(512\), \(4\times 4\), \(\text{stride}=2,\text{padding}=1\) \\ BatchNorm \(512\) \\ ReLU \(512\) \\ ConvTranspose \(256\), \(4\times 4\), \(\text{stride}=2,\text{padding}=1\) \\ BatchNorm \(256\) \\ ReLU \(256\) \\ ConvTranspose \(64\), \(4\times 4\), \(\text{stride}=2,\text{padding}=1\) \\ BatchNorm \(64\) \\ ConvTranspose \(32\), \(4\times 4\), \(\text{stride}=2,\text{padding}=1\) \\ BatchNorm \(32\) \\ ReLU \(32\) \\ ConvTranspose \(3\), \(4\times 4\), \(\text{stride}=2,\text{padding}=1\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Decoder (\(D\)) architecture.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Cars3D & Shapes3D & MPI3D \\ \cline{2-4} Hyperparameter & \multicolumn{3}{c}{Architectural hyperparameters} \\ \hline \(D_{R}\) & 12 & 16 & 12 \\ \(N_{R}\) _(fixed)_ & 10 & 10 & 10 \\ \(D_{F}\) & 128 & 32 & 32 \\ \(N_{F}\) & 106 & 57 & 50 \\ \hline \multicolumn{4}{c}{Loss function hyperparameters} \\ \hline \(\lambda_{1}\) & 0.00024 & 0.00091 & 0.0000 \\ \(\lambda_{2}\) & 0.02200 & 0.00228 & 1.16050 \\ \(\beta\) _(fixed)_ & 0.5 & 0.5 & 0.5 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Hyperparameter values.

Results

In this section, we provide additional details about our experiments, and present additional results.

### Datasets

For disentanglement, we utilise standard disentanglement datasets: Cars3D [4], Shapes3D [24], and the'real' variant of MPI3D [21], containing \(3\), \(6\), and \(7\) ground-truth FoVs respectively. These datasets are procedurally generated via taking the Cartesian product of all possible FoV values per FoV type. Metadata is provided in Tables 12, 13 and 14, with continuous-valued FoV types marked by \({}^{\ddagger}\). We treat colour FoVs as continuous variables due to their 10 linearly spaced, naturally ordered values, in line with [42].

For the downstream regression task, we use these disentanglement datasets and train downstream models to regress on each dataset's continuous-valued FoVs, as indicated by the \({}^{\ddagger}\) symbol in Tables 12, 13 and 14. For abstract visual reasoning [30], we employ the Raven's Progressive Matrix (RPM) style dataset from [30], where each sample consists of 8 context panels and 6 answer panels derived from Shapes3D images. The model must infer abstract visual relations from the context panels to select the correct answer among six options.

### Baseline Implementations and Experimental Settings

#### c.2.1 Experiment Compute Resources

For the generative weakly supervised, scalar-valued baselines (i.e., AdaGVAE-k, GVAE, MLVAE, SlowVAE, the Shu model), and our model, we perform model training on a single Nvidia RTX4090 GPU. We also perform all associated experiments for these models (i.e., downstream model training, downstream model evaluation, disentanglement evaluation, etc.) on this single Nvidia RTX4090 GPU. Our model takes approximately 1.5 hours to fully train (i.e., run 200,000 iterations) on the Cars3D and Shapes dataset, and approximately 4.0 hours to fully train on the MPI3D dataset.

For the vector-valued baselines of COMET and VCT, we perform model training, and all associated experiments on a single Nvidia V100 GPU.

#### c.2.2 Disentanglement Models

For the generative weakly supervised scalar-valued baselines (AdaGVAE-k, GVAE, MLVAE, SlowVAE, Shu model) and our model, training and all related experiments (downstream training, evaluation, disentanglement) were conducted on a single Nvidia RTX4090 GPU. Our model requires approximately 1.5 hours to train (200,000 iterations) on the Cars3D and Shapes datasets, and about 4.0 hours on the MPI3D dataset.

Vector-valued baselines COMET and VCT were trained and evaluated on a single Nvidia V100 GPU. We utilised the PyTorch-based open-source implementations from [42] for Ada-GVAE, GVAE, MLVAE, and SlowVAE, verified to reproduce reported results. Official implementations from the authors were used for COMET and VCT [36; 47]. For the GAN-based Shu model, we converted the TensorFlow implementation from [35] to PyTorch and ensured it reproduced official results. Baseline models employed recommended hyperparameters; otherwise, hyperparameter tuning was conducted using the same Weights and Biases (WandB) hyperparameter sweep as our model.

As discussed in Section 5, all weakly supervised baselines except Ada-GVAE assume access to \(I\), the differing FoV between each pair \((x,x^{\prime})\), matching our model's supervision level. To make Ada-GVAEcomparable, we modified it to receive the ground-truth \(k=|I|\), the number of changing FoVs per sample, termed Ada-GVAE-k. This adjustment resulted in superior or comparable performance to the original Ada-GVAE. All models, except SlowVAE (which assumes all FoVs change), were trained with \(k=1\), ensuring consistent training conditions across models.

For each model's selected hyperparameters, including our own, we trained five models using five random seeds and aggregated the results. All models were trained for 200,000 iterations on each dataset.

#### c.2.3 Downstream Models

For FoV regression, following [42], we use a simple, generic MLP model, with the architecture listed in Table 15. For each disentanglement dataset, the MLP regression model receives representations of images produced by a representation learner, and is trained to predict the ground-truth FoV values in a supervised fashion. Performance is measured using the \(R^{2}\) score on a held out, randomly selected test set of 1,000 samples.

For each representation learning model instantiated with a random seed, we train 2 MLPs by uniformly sampling the number of output nodes in the first, second, and fifth layers as specified in Table 15, resulting in a total of 10 MLPs per learner (we use 5 random seeds). Reported results are averaged over these models. Notably, the MLPs have no prior knowledge of the representation type (e.g., scalar-tokened symbolic, vector-tokened symbolic, or fully continuous compositional) since no inductive biases or representation-specific optimisations are applied, aside from the supervised MSE loss.

Each MLP is trained using the Adam optimiser with an MSE loss, a learning rate of \(1\times 10^{-4}\), and default \((\beta_{1},\beta_{2})=(0.9,0.999)\) settings.

For the abstract visual reasoning task, following [30], we use the Wild Relation Network (WReN) model [18] on representations produced by representation learners to predict ground-truth answers for each RPM matrix. For each of model instances produced by a random seeds, we randomly sample 2 possible configurations of the WReN model by uniformly sampling either 256 or 512 for the edge MLP, \(g\), and 128 or 256 hidden units for the graph MLP, \(f\), in line with [30]. We however, fix the number of hidden layers in \(g\) to 2, and \(f\) in 1, representing the smallest possible number of hidden layers, to constrain the capacity of the WReN model. As we use 5 random seeds, and sample 2 configurations per seed, we produce 10 WReN models for each representation learner. All WReN models are trained using Adam optimisation on the BCE loss between the predicted logits and ground-truth labels, with a learning rate of \(1\mathrm{e}{-5}\) and the default setting of \((\beta_{1},\beta_{2})=(0.9,0.999)\).

#### c.2.4 Experimental Controls

To ensure that the performance improvements of our model are attributable to the distributed, flexible nature of the Soft TPR framework, we implement controls to eliminate confounding variables.

**Disentanglement Controls**

To ensure performance gains in disentanglement metrics (Tables 16 and 17) are not attributable to our model's additional parameters (with our model's the filler embedding matrix \(M_{\xi_{F}}\), adding

\begin{table}
\begin{tabular}{c} \hline MLP \\ \hline Linear \(d_{1},d_{1}\in[256,512]\) \\ ReLU \\ Linear \(d_{2},d_{2}\in[256,512]\) \\ ReLU \\ Linear \(\dim(z)\) \\ ReLU \\ Linear \(\dim(z)\) \\ ReLU \\ Linear \(d_{3},d_{3}\in[128,256]\) \\ ReLU \\ Linear \(k\) \\ \hline \end{tabular}
\end{table}
Table 15: MLP architecture 13,568, 1,824, and 1,600 parameters for Cars3D, Shapes3D, and MPI3D respectively to a standard autoencoding framework), we adjust baseline models with fewer parameters. Specifically, for SlowVAE, Ada-GVAE-k, GVAE, ML-VAE, and the Shu model, we increase their parameters by adding filters or layers until they match or exceed our model's parameter count, denoted with \({}^{*}\) in Tables 16 and 17. VCT and COMET are substantially more parameter hungry (10+ million) than our model, so parameter controls are not applicable to these vector-valued symbolic compositional baselines. Consistent with [40], we observe that increasing parameters in these generative, scalar-tokened baselines does not enhance disentanglement performance. Therefore, we proceed with representation learning convergence experiments and downstream model tests using the original models, not the parameter-adjusted variants.

**Downstream Task Controls**

To ensure that any performance boosts on the downstream tasks of FoV regression and abstract visual reasoning are not attributable to our representation's increased dimensionality, we post-process representations produced by _all_ models (including models producing higher-dimensional representations) to match the dimensionality of our representation. Specifically, generative, weakly supervised models with scalar-tokened representations produce representations with a dimension of 10 for all datasets, whereas our Soft TPRs have dimensions of 1536 (Cars3D), 512 (Shapes3D), and 320 (MPI3D) of our Soft TPRs, highlighting the necessity of such a control.

To perform this control, we apply separate methods for the scalar-tokened and vector-tokened models. For scalar-tokened models, we multiply each dimension \(k\) of the latent vector, \(l_{k}\) with an random embedding vector \(e_{k}\in\mathbb{R}^{d}\) from an fixed, randomly initialised, semi-orthogonal embedding matrix \(E\in\mathbb{R}^{d\times 10}\), and subsequently concatenate all multiplied random embedding vectors together to form the dimensionality-controlled representation, \((l_{1}e_{k},\dots,l_{10}e_{10})\in\mathbb{R}^{10d}\). \(d\) is a dataset specific integer that ensures that the size of the dimensionality-controlled representation is at least as small as the dimensionality of the Soft TPR representation for the given dataset, i.e., \(d:=\lceil\dim(z)\rceil/10\), where \(z\) is the Soft TPR representation produced by a given dataset. Note that we choose a semi-orthogonal embedding matrix with the intuition that this ensures the maximal distinguishability of each contiguous subset of dimensions corresponding to \(l_{i}e_{i}\).

For COMET and VCT, which both produce vector-tokened representations, we consider alternative methods of performing dimensionality-control. COMET's representations have a dimensionality of 640 for all datasets, and so, the model produces lower-dimensional representations than ours for Shapes3D and Cars3D, but not MPI3D. VCT, on the other hand, produces representations with dimension 5120, and so, produces higher-dimensional representations than ours for all datasets. For any vector-tokened representation with _higher_ dimensionality than our representation, we apply PCA-based postprocessing to the model representation, reducing the dimensionality of each vector-tokened value in the representation to the required dimensionality, \(d\), where \(d:=\lceil\dim(z)\rceil/\dim(z_{baseline})\), before concatenating all PCA-reduced vectors together to produce the modified representation. For any vector-tokened representation produced by COMET with lower dimensionality than ours, we apply a simple matrix multiplication between the representation, and a randomly initialised matrix of required dimensionality to embed the representation into the same-dimensional space as ours.

We denote the results produced by all such postprocessing by \({}^{\dagger}\) in relevant tables, and indicate such postprocessing in the captions of relevant plots.

### Disentanglement

In reporting the disentanglement metric results for baseline models, we use published results where applicable, i.e., we use the results published in [47] for VCT and COMET and the published results for SlowVAE [39]. For GVAE, MLVAE, and Ada-GVAE-K, we evaluate disentanglement using the Pytorch-based implementation of disentanglement metrics, [42], which corresponds to a Pytorch-based implementation of the official disentanglement lib of [25]. We also use this implementation to evaluate the disentanglement of representations produced by all models.

#### c.3.1 Disentanglement Metrics

In line with [47], we consider the following 4 disentanglement metrics: the FactorVAE score [24], the DCI Disentanglement score [16] (we refer to DCI Disentanglement as DCI), the BetaVAE score [10], and the MIG score [15]. We provide a brief overview of all 4 disentanglement metrics, and refer interested readers to the papers these metrics are introduced in for further details, as well as the Appendix of [25] for more details on how the metrics are implemented in [42] and [25].

**FactorVAE Metric**: A randomly selected FoV of the dataset is fixed, and a mini-batch of observations is subsequently randomly sampled. The representation learner produces representations for the samples. Disentanglement is quantified using the accuracy of a majority vote classifier that predicts the index of the ground-truth fixed FoV based on the index of the representation vector with the smallest variance.

**DCI Metric**: A Gradient Boosted Tree (GBT) is trained to predict the ground-truth FoV values from representations produced by a representation learner. The predictive importance of the dimensions of a representation is obtained using the model's feature importances. For each sample, a score is computed that corresponds to one minus the entropy of the probability that a dimension of the learned representation is useful for FoV prediction, weighted by the relative entropy of the corresponding dimension. An average of these scores over the mini-batch of samples is taken to produce the final score.

**BetaVAE Metric**: This metric quantifies disentanglement by predicting the index of a fixed FoV from representations produced by a representation learner, similar to the FactorVAE metric. In contrast to the FactorVAE metric, however, the BetaVAE metric uses a linear classifier on difference vectors to predict the index of the fixed FoV. Each difference vector is produced by taking the difference between representations produced for a pair of samples \((x,x^{\prime})\) with one underlying fixed FoV.

**MIG Metric**: For each FoV, the MIG metric computes the mutual information between each dimension in the representation, and the corresponding FoV. The score is obtained by computing the average, normalised difference between the highest and second highest mutual information of each FoV with the dimensions of the representation.

#### c.3.2 Evaluating the Disentanglement of Soft TPRs

Since disentanglement metrics are typically computed under the assumption that the representation corresponds to a concatenation of _scalar-valued_ FoV tokens, we now detail how we compute the above disentanglement metrics on the Soft TPR, a _continuous_ compositional representation. Similar amendments have been made in COMET and VCT, so that the disentanglement of their representations, corresponding to a concatenation of _vector-valued_ FoV tokens, can be computed.

**FactorVAE metric**: To compute the FactorVAE score of our Soft TPR, we produce a \(N_{R}\)-dimensional vector, \(v\), for each Soft TPR, where \(N_{R}\) corresponds to the number of roles, i.e. the FoV _types_. We produce \(v\) by simply populating each dimension, \(v_{i}\), with the index of the quantised filler that role \(r_{i}\) is bound to, i.e. we set \(v_{i}\) to \(m(i)\). We use the resulting \(v\)'s to compute the variances required by the FactorVAE metric, noting that if the FoV type corresponding to role \(i\) is fixed, and this is reflected in our representation, dimension \(i\) of the \(v\) vectors produced in a mini-batch should clearly have the smallest variance, as all fillers, \(f_{m(i)}\), that role \(r_{i}\) binds to, will have the same identity across the mini-batch.

**DCI metric**: As the DCI relies on computing the ground-truth FoV values from \(N_{R}\)-dimensional representations produced by the representation learner, we again, follow a similar procedure as the above, in converting our \((D_{F}\cdot D_{R})\)-dimensional Soft TPR representation to a \(N_{R}\)-dimensional representation. That is, we simply consider a \(N_{R}\)-dimensional vector, \(v\), where each dimension, \(v_{i}\), is populated by the index of the quantised filler, \(m(i)\) that role \(r_{i}\) is bound to, and use this vector to compute the corresponding DCI result.

**BetaVAE metric**: For the BetaVAE metric, as each sample used to train the linear classifier consists of a \(N_{R}\)-dimensional difference vector obtained by computing the difference between the \(N_{R}\)-dimensional scalar-tokened compositional representations, we obtain the following difference vector, \(d\), for our Soft TPR representations. For each role \(i\in\{1,\dots,N_{R}\}\), we obtain the corresponding quantised filler embeddings for each sample \(x,x^{\prime}\) in the pair, i.e., we obtain \(\xi_{F}(f_{m(i)}),\xi_{F}(f_{m^{\prime}(i)})\). For each pair of quantised filler embeddings, we obtain a scalar distance measure corresponding to the cosine similarity between the the pair. The \(i\)-th dimension of \(d\) is populated using this value. We use difference vectors obtained in this way to compute the BetaVAE metric.

**MIG metric**: For the MIG metric, which relies on a discretisation of the values in each dimension \(i\) of the \(N_{R}\)-dimensional scalar-tokened compositional representations to compute the discrete mutual information, we apply the same postprocessing technique as in the FactorVAE, and DCI metric, to evaluate MIG on our Soft TPRs. That is, we produce the same \(N_{R}\)-dimensional vector, \(v\), noting that this choice of postprocessing also performs the discretisation required by the MIG computation.

#### c.3.3 Full Results

We now present unabridged results for all considered disentanglement metrics, denoting the learnable parameter control modification of each relevant baseline with the symbol \({}^{*}\). As can be seen in Tables 16 and 17, our Soft TPR representations are explicitly more compositional (as quantified by the disentanglement metric scores) compared to all considered baselines, with especially notable performance increases on the more challenging datasets of Cars3D and MPI3D.

### Representation Learning Convergence

We additionally examine representation learning convergence by evaluating the representations produced at 100, 1,000, 10,000, 100,000, and 200,000 iterations of model training, where the latter stage of 200,000 iterations corresponds to fully trained models. To quantify representation learning convergence, we evaluate both 1) the explicit compositionality of representations produced at these stages of training (as quantified by disentanglement metric performance), and 2) the usefulness of these representations for the downstream tasks of FoV regression and abstract visual reasoning.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c}{Cars3D} & \multicolumn{2}{c}{Shapes3D} & \multicolumn{2}{c}{MPI3D} \\ \cline{2-6}  & FactorVAE score & DCI & FactorVAE score & DCI & FactorVAE score & DCI \\ \hline \multicolumn{6}{c}{Symbolic scalar-tobeac compositional representations} \\ \hline Slow-VAE & 0.902 \(\pm\) 0.035 & 0.509 \(\pm\) 0.027 & 0.950 \(\pm\) 0.032 & 0.850 \(\pm\) 0.047 & 0.455 \(\pm\) 0.083 & 0.355 \(\pm\) 0.027 \\ Slow-VAE\({}^{*}\) & 0.872 \(\pm\) 0.038 & 0.518 \(\pm\) 0.039 & 0.961 \(\pm\) 0.028 & 0.867 \(\pm\) 0.028 & 0.421 \(\pm\) 0.091 & 0.317 \(\pm\) 0.039 \\ Ada-GVAE-k\({}^{*}\) & 0.947 \(\pm\) 0.064 & _0.664 \(\pm\) 0.167_ & _0.973 \(\pm\) 0.006_ & **0.963 \(\pm\) 0.077** & 0.496 \(\pm\) 0.095 & 0.343 \(\pm\) 0.040 \\ Ada-GVAE-k\({}^{*}\) & 0.931 \(\pm\) 0.051 & 0.641 \(\pm\) 0.179 & 0.932 \(\pm\) 0.012 & **0.932** \(\pm\) 0.108 & 0.451 \(\pm\) 0.129 & 0.319 \(\pm\) 0.056 \\ GVAE & 0.877 \(\pm\) 0.081 & 0.262 \(\pm\) 0.095 & 0.921 \(\pm\) 0.075 & 0.842 \(\pm\) 0.040 & 0.378 \(\pm\) 0.024 & 0.245 \(\pm\) 0.074 \\ GVAE\({}^{*}\) & 0.841 \(\pm\) 0.123 & 0.219 \(\pm\) 0.012 & 0.881 \(\pm\) 0.129 & 0.810 \(\pm\) 0.102 & 0.341 \(\pm\) 0.067 & 0.216 \(\pm\) 0.109 \\ ML-VAE\({}^{*}\) & 0.870 \(\pm\) 0.052 & 0.216 \(\pm\) 0.063 & 0.835 \(\pm\) 0.111 & 0.739 \(\pm\) 0.115 & 0.390 \(\pm\) 0.026 & 0.251 \(\pm\) 0.029 \\ ML-VAE\({}^{*}\) & 0.881 \(\pm\) 0.041 & 0.220 \(\pm\) 0.051 & 0.823 \(\pm\) 0.091 & 0.714 \(\pm\) 0.091 & 0.401 \(\pm\) 0.019 & 0.240 \(\pm\) 0.043 \\ Shu & 0.573 \(\pm\) 0.062 & 0.032 \(\pm\) 0.014 & 0.265 \(\pm\) 0.043 & 0.017 \(\pm\) 0.006 & 0.287 \(\pm\) 0.034 & 0.033 \(\pm\) 0.008 \\ Shu\({}^{*}\) & 0.551 \(\pm\) 0.062 & 0.019 \(\pm\) 0.021 & 0.297 \(\pm\) 0.031 & 0.020 \(\pm\) 0.006 & 0.219 \(\pm\) 0.057 & 0.029 \(\pm\) 0.010 \\ \hline \multicolumn{6}{c}{Symbolic vector-tobeac compositional representations} \\ \hline VCT & _0.966 \(\pm\) 0.029_ & 0.382 \(\pm\) 0.080 & 0.957 \(\pm\) 0.043 & 0.884 \(\pm\) 0.013 & _0.689 \(\pm\) 0.035_ & _0.475 \(\pm\) 0.005_ \\ COMET & 0.339 \(\pm\) 0.008 & 0.024 \(\pm\) 0.026 & 0.168 \(\pm\) 0.005 & 0.002 \(\pm\) 0.000 & 0.145 \(\pm\) 0.024 & 0.005 \(\pm\) 0.001 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.999 \(\pm\) 0.001** & **0.863 \(\pm\) 0.027** & **0.984 \(\pm\) 0.012** & _0.926 \(\pm\) 0.028_ & **0.949 \(\pm\) 0.032** & **0.828 \(\pm\) 0.015** \\ \hline \hline \end{tabular}
\end{table}
Table 16: FactorVAE and DCI scores

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{2}{c}{Cars3D} & \multicolumn{2}{c}{Shapes3D} & \multicolumn{2}{c}{MPI3D} \\ \cline{2-6}  & BetaVAE score & MIG & BetaVAE score & MIG & BetaVAE score & MIG \\ \hline \multicolumn{6}{c}{Symbolic scalar-tobeac compositional representations} \\ \hline Slow-VAE & **1.000 \(\pm\) 0.000** (\(=\)) & 0.104 \(\pm\) 0.018 & **1.000 \(\pm\) 0.000** (\(=\)) & _0.615 \(\pm\) 0.045_ & _0.666 \(\pm\) 0.069_ & _0.329 \(\pm\) 0.026_ \\ Slow-VAE\({}^{*}\) & **1.000 \(\pm\) 0.000** (\(=\)) & 0.071 \(\pm\) 0.013 & **1.000 \(\pm\) 0.000** (\(=\)) & **0.655 \(\pm\) 0.067** & _0.629 \(\pm\) 0.079_ & 0.287 \(\pm\) 0.045_ \\ Ada-GVAE-k & **1.000 \(\pm\) 0.000** (\(=\)) & **0.395 \(\pm\) 0.095** & **1.000 \(\pm\) 0.000** (\(=\)) & **0.556 \(\pm\) 0.064** & _0.750 \(\pm\) 0.053_ & 0.213 \(\pm\) 0.064_ \\ Ada-GVAE-k & **1.000 \(\pm\) 0.000** (\(=\)) & 0.321 \(\pm\) 0.102 & **1.000 \(\pm\) 0.000** (\(=\)) & 0.498 \(\pm\) 0.073 & _0.783 \(\pm\) 0.061_ & _0.241 \(\pm\) 0.079_ \\ GVAE & **1.000 \(\pm\) 0.000** (\(=\)) & 0.096 \(\pm\) 0.036 & 0.998 \(\pm\) 0.004 & 0.251 \(\pm\) 0.072 & _0.704 \(\pm\) 0.072_ & _0.145 \(\pm\) 0.074_ \\ GVAE\({}^{*}\) & **1.000 \(\pm\) 0.000** (\(=\)) & 0.100 \(\pm\) 0.005 & **1.000 \(\pm\) 0.000** & 0.203 \(\pm\) 0.089 & _0.681 \(\pm\) 0.061_ & _0.109 \(\pm\) 0.087_ \\ MLVAE & **1.000 \(\pm\) 0.000** (\(=\)) & 0.088 \(\pm\) 0.020 & 0.976 \(\pm\) 0.038 & 0.35In all line plots, we plot the mean, and indicate the standard deviation by the shaded regions. We use the same legend for all plots, where \(0\) (grey) denotes SlowVAE, \(1\) (orange) denotes AdaGVAE-k, \(2\) (green) denotes GVAE, \(3\) (red) denotes MLVAE, \(4\) (purple) denotes Shu, \(5\) (pink) denotes VCT, \(6\) (brown) denotes COMET, and \(7\) (blue) denotes our model, Soft TPR Autoencoder.

#### c.4.1 Disentanglement

We first present line plots of representation learner convergence for each of the four considered disentanglement metrics (i.e., the FactorVAE, DCI, BetaVAE and MIG scores) for all three disentanglement datasets (Cars3D, Shapes3D, MPI3D). A series of tables containing the values associated with these line plots is presented following the plots. As the disentanglement results produced by our learnable parameter controls for the models of Ada-GVAE-k, GVAE, ML-VAE and Shu, do not achieve superior disentanglement results compared to the original models, we only present disentanglement convergence results for the original variants of all baseline models.

Figure 4: DCI score convergence on the Cars3D dataset

Figure 3: Factor score convergence on the Cars3D dataset

Figure 5: BetaVAE score convergence on the Cars3D dataset

Figure 6: MIG score convergence on the Cars3D dataset

Figure 8: DCI score convergence on the Shapes3D dataset

Figure 7: Factor score convergence on the Shapes3D dataset

Figure 10: MIG score convergence on the Shapes3D dataset

Figure 9: BetaVAE score convergence on the Shapes3D dataset

Figure 11: Factor score convergence on the MPI3D dataset

Figure 12: DCI score convergence on the MPI3D dataset

Figure 14: MIG score convergence on the MPI3D dataset

Figure 13: BetaVAE score convergence on the MPI3D dataset

[MISSING_PAGE_FAIL:39]

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{CIC score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & _0.056 \(\pm\) 0.015_ & 0.192 \(\pm\) 0.060 & 0.353 \(\pm\) 0.111 & 0.616 \(\pm\) 0.147 & 0.749 \(\pm\) 0.118 \\ Ada-GVAE-k & 0.053 \(\pm\) 0.018 & _0.331 \(\pm\) 0.022_ & 0.662 \(\pm\) 0.042 & **0.944 \(\pm\) 0.063** & **0.964 \(\pm\) 0.067** \\ GVAE & 0.042 \(\pm\) 0.013 & 0.198 \(\pm\) 0.046 & 0.438 \(\pm\) 0.064 & 0.749 \(\pm\) 0.053 & 0.835 \(\pm\) 0.038 \\ MLVAE & 0.026 \(\pm\) 0.003 & 0.183 \(\pm\) 0.036 & 0.407 \(\pm\) 0.051 & 0.668 \(\pm\) 0.084 & 0.739 \(\pm\) 0.103 \\ Shu & 0.030 \(\pm\) 0.007 & 0.028 \(\pm\) 0.012 & 0.032 \(\pm\) 0.006 & 0.019 \(\pm\) 0.010 & 0.018 \(\pm\) 0.006 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 0.044 \(\pm\) 0.000 & **0.817 \(\pm\) 0.114** & **0.909 \(\pm\) 0.010** & _0.887 \(\pm\) 0.026_ & 0.880 \(\pm\) 0.022 \\ COMET & 0.017 \(\pm\) 0.006 & 0.031 \(\pm\) 0.005 & 0.062 \(\pm\) 0.017 & 0.173 \(\pm\) 0.064 & 0.233 \(\pm\) 0.066 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.057 \(\pm\) 0.018** & 0.183 \(\pm\) 0.077 & _0.860 \(\pm\) 0.052_ & 0.887 \(\pm\) 0.068 & _0.924 \(\pm\) 0.027_ \\ \hline \hline \end{tabular}
\end{table}
Table 23: Representation learner convergence on the Shapes3D dataset (DCI score)

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{CIC score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline Slow-VAE & 0.270 \(\pm\) 0.025 & 0.392 \(\pm\) 0.095 & 0.547 \(\pm\) 0.138 & 0.762 \(\pm\) 0.156 & 0.923 \(\pm\) 0.091 \\ Ada-GVAE-k & 0.296 \(\pm\) 0.047 & 0.577 \(\pm\) 0.023 & _0.815 \(\pm\) 0.057_ & 0.960 \(\pm\) 0.057 & _0.961 \(\pm\) 0.070_ \\ GVAE & 0.263 \(\pm\) 0.036 & 0.469 \(\pm\) 0.063 & 0.674 \(\pm\) 0.086 & 0.890 \(\pm\) 0.058 & 0.879 \(\pm\) 0.077 \\ MLVAE & 0.166 \(\pm\) 0.084 & 0.457 \(\pm\) 0.045 & 0.621 \(\pm\) 0.078 & 0.746 \(\pm\) 0.080 & 0.765 \(\pm\) 0.084 \\ Shu & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.242 \(\pm\) 0.072 & 0.194 \(\pm\) 0.021 & 0.227 \(\pm\) 0.016 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 0.263 \(\pm\) 0.000 & **0.984 \(\pm\) 0.031** & **1.000 \(\pm\) 0.000** & _0.976 \(\pm\) 0.033_ & **0.967 \(\pm\) 0.044** \\ COMET & 0.268 \(\pm\) 0.022 & 0.265 \(\pm\) 0.039 & 0.334 \(\pm\) 0.028 & 0.384 \(\pm\) 0.040 & 0.440 \(\pm\) 0.071 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.356 \(\pm\) 0.011** & 0.435 \(\pm\) 0.048 & _0.975 \(\pm\) 0.040_ & **0.995 \(\pm\) 0.005** & 0.960 \(\pm\) 0.053 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Representation learner convergence on the Shapes3D dataset (Factor score)

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{CIC score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & _0.008 \(\pm\) 0.004_ & 0.042 \(\pm\) 0.017 & 0.061 \(\pm\) 0.024 & 0.071 \(\pm\) 0.028 & 0.081 \(\pm\) 0.031 \\ Ada-GVAE-k & _0.011 \(\pm\) 0.005_ & _0.068 \(\pm\) 0.013_ & _0.092 \(\pm\) 0.035_ & _0.241 \(\pm\) 0.095_ & _0.312 \(\pm\) 0.085_ \\ GVAE & 0.014 \(\pm\) 0.006 & 0.031 \(\pm\) 0.015 & 0.056 \(\pm\) 0.008 & 0.084 \(\pm\) 0.031 & 0.095 \(\pm\) 0.032 \\ MLVAE & 0.012 \(\pm\) 0.007 & 0.037 \(\pm\) 0.024 & 0.057 \(\pm\) 0.018 & 0.085 \(\pm\) 0.021 & 0.087 \(\pm\) 0.017 \\ Shu & 0.007 \(\pm\) 0.004 & 0.014 \(\pm\) 0.005 & 0.019 \(\pm\) 0.010 & 0.002 \(\pm\) 0.001 & 0.001 \(\pm\) 0.001 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 0.001 \(\pm\) 0.001 & 0.034 \(\pm\) 0.016 & 0.033 \(\pm\) 0.021 & 0.033 \(\pm\) 0.022 & 0.032 \(\pm\) 0.015 \\ COMET & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.016 \(\pm\) 0.010** & **0.136 \(\pm\) 0.009** & **0.344 \(\pm\) 0.011** & **0.349 \(\pm\) 0.010** & **0.349 \(\pm\) 0.010** \\ \hline \hline \end{tabular}
\end{table}
Table 21: Representation learner convergence on the Cars3D dataset (MIG score)

[MISSING_PAGE_FAIL:41]

#### c.4.2 Downstream Performance

We additionally evaluate the representation learning convergence by examining the usefulness of representations produced at different stages of training (i.e., 100, 250, 500, 1,000, 10,000, 100,000

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{BetaVAE score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.359 \(\pm\) 0.004 & 0.484 \(\pm\) 0.019 & 0.629 \(\pm\) 0.038 & 0.833 \(\pm\) 0.035 & 0.851 \(\pm\) 0.045 \\ Ada-GVAE-k & _0.362 \(\pm\) 0.006_ & 0.397 \(\pm\) 0.028 & 0.587 \(\pm\) 0.040 & 0.741 \(\pm\) 0.047 & 0.748 \(\pm\) 0.041 \\ GVAE & 0.352 \(\pm\) 0.012 & 0.378 \(\pm\) 0.015 & 0.506 \(\pm\) 0.041 & 0.683 \(\pm\) 0.071 & 0.703 \(\pm\) 0.062 \\ MLVAE & 0.343 \(\pm\) 0.021 & 0.394 \(\pm\) 0.022 & 0.507 \(\pm\) 0.015 & 0.672 \(\pm\) 0.038 & 0.697 \(\pm\) 0.035 \\ Shu & 0.170 \(\pm\) 0.057 & 0.294 \(\pm\) 0.026 & 0.325 \(\pm\) 0.038 & 0.269 \(\pm\) 0.023 & 0.278 \(\pm\) 0.021 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & **0.889 \(\pm\) 0.044** & **1.000 \(\pm\) 0.001** & **1.000 \(\pm\) 0.000** & **1.000 \(\pm\) 0.000** & **1.000 \(\pm\) 0.000** \\ COMET & 0.277 \(\pm\) 0.011 & 0.277 \(\pm\) 0.011 & 0.277 \(\pm\) 0.011 & 0.270 \(\pm\) 0.007 & 0.278 \(\pm\) 0.010 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & 0.302 \(\pm\) 0.011 & _0.577 \(\pm\) 0.021_ & _0.973 \(\pm\) 0.048_ & _0.980 \(\pm\) 0.022_ & _0.976 \(\pm\) 0.024_ \\ \hline \end{tabular}
\end{table}
Table 28: Representation learner convergence on the MPI3D dataset (BetaVAE score)

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{MIG score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.014 \(\pm\) 0.004 & **0.059 \(\pm\) 0.042** & 0.084 \(\pm\) 0.061 & 0.206 \(\pm\) 0.120 & 0.214 \(\pm\) 0.125 \\ Ada-GVAE-k & _0.016 \(\pm\) 0.005_ & 0.039 \(\pm\) 0.011 & 0.113 \(\pm\) 0.051 & 0.213 \(\pm\) 0.062 & 0.214 \(\pm\) 0.057 \\ GVAE & 0.014 \(\pm\) 0.004 & 0.022 \(\pm\) 0.003 & 0.058 \(\pm\) 0.035 & 0.140 \(\pm\) 0.061 & 0.147 \(\pm\) 0.066 \\ MLVAE & 0.010 \(\pm\) 0.004 & 0.032 \(\pm\) 0.022 & 0.044 \(\pm\) 0.015 & 0.136 \(\pm\) 0.051 & 0.144 \(\pm\) 0.054 \\ Shu & 0.005 \(\pm\) 0.000 & 0.010 \(\pm\) 0.006 & 0.024 \(\pm\) 0.019 & 0.009 \(\pm\) 0.005 & 0.013 \(\pm\) 0.005 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline Slow-VAE & 0.090 \(\pm\) 0.0013 & 0.125 \(\pm\) 0.014 & 0.200 \(\pm\) 0.014 & 0.341 \(\pm\) 0.095 & 0.381 \(\pm\) 0.081 \\ Ada-GVAE-k & 0.097 \(\pm\) 0.008 & 0.117 \(\pm\) 0.009 & 0.174 \(\pm\) 0.005 & 0.319 \(\pm\) 0.038 & 0.338 \(\pm\) 0.038 \\ GVAE & 0.101 \(\pm\) 0.019 & 0.111 \(\pm\) 0.011 & 0.142 \(\pm\) 0.022 & 0.247 \(\pm\) 0.062 & 0.258 \(\pm\) 0.063 \\ MLVAE & 0.097 \(\pm\) 0.015 & 0.103 \(\pm\) 0.010 & 0.137 \(\pm\) 0.013 & 0.252 \(\pm\) 0.031 & 0.266 \(\pm\) 0.026 \\ Shu & 0.083 \(\pm\) 0.000 & 0.065 \(\pm\) 0.030 & 0.063 \(\pm\) 0.018 & 0.036 \(\pm\) 0.012 & 0.050 \(\pm\) 0.012 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.352 \(\pm\) 0.0014 & 0.484 \(\pm\) 0.019 & 0.629 \(\pm\) 0.038 & 0.833 \(\pm\) 0.035 & 0.851 \(\pm\) 0.045 \\ Ada-GVAE-k & _0.362 \(\pm\) 0.006_ & 0.397 \(\pm\) 0.028 & 0.587 \(\pm\) 0.040 & 0.741 \(\pm\) 0.047 & 0.748 \(\pm\) 0.041 \\ GVAE & 0.352 \(\pm\) 0.012 & 0.378 \(\pm\) 0.015 & 0.506 \(\pm\) 0.041 & 0.683 \(\pm\) 0.071 & 0.703 \(\pm\) 0.062 \\ MLVAE & 0.343 \(\pm\) 0.021 & 0.394 \(\pm\) 0.022 & 0.507 \(\pm\) 0.015 & 0.672 \(\pm\) 0.038 & 0.697 \(\pm\) 0.035 \\ Shu & 0.170 \(\pm\) 0.057 & 0.294 \(\pm\) 0.026 & 0.325 \(\pm\) 0.038 & 0.269 \(\pm\) 0.023 & 0.278 \(\pm\) 0.021 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & **0.889 \(\pm\) 0.044** & **1.000 \(\pm\) 0.001** & **1.000 \(\pm\) 0.000** & **1.000 \(\pm\) 0.000** & **1.000 \(\pm\) 0.000** \\ COMET & 0.277 \(\pm\) 0.011 & 0.277 \(\pm\) 0.011 & 0.277 \(\pm\) 0.011 & 0.270 \(\pm\) 0.007 & 0.278 \(\pm\) 0.010 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & 0.302 \(\pm\) 0.011 & _0.577 \(\pm\) 0.021_ & _0.973 \(\pm\) 0.048_ & _0.980 \(\pmand 200,000 iterations of training). To quantify 'usefulness', we consider performance of downstream models on the tasks of FoV regression, and abstract visual reasoning when trained using representations produced by each stage of training. For each task, we present line plots, and additionally tables with values corresponding to each of the plots. We use the same legend as in the previous section, where \(0\) (grey) denotes SlowVAE, \(1\) (orange) denotes AdaGVAE-k, \(2\) (green) denotes GVAE, \(3\) (red) denotes MLVAE, \(4\) (purple) denotes Shu, \(5\) (pink) denotes VCT, \(6\) (brown) denotes COMET, and \(7\) (blue) denotes our model, Soft TPR Autoencoder. We additionally provide all results for the dimensionality-control setting, where the dimensionality of representations produced by all representation learners is held constant following the approach detailed in C.2.4, denoting this clearly in plot captions, and by the symbol \({}^{\dagger}\) in the tables.

**FoV Regression**

As clearly visible in the tables and plots, generic regression models are able to more effectively use representations produced by our Soft TPR Autoencoder produced across almost all stages of training for all disentanglement datasets. Improvements are most notable in the low-iteration regime of \(10^{2}\) iterations, and across most stages of training for the more challenging task of FoV regression on the MPI3D dataset (Figures 19 and 20).

Figure 16: Convergence of representation learners as measured by FoV regression on the Cars3D dataset (dimensionality-controlled setting)

Figure 15: Convergence of representation learners as measured by FoV regression on the Cars3D dataset (original setting)

Figure 17: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset (original setting)

Figure 18: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset (dimensionality-controlled setting)

Figure 19: Convergence of representation learners as measured by FoV regression on the MPI3D dataset (original setting)

Figure 20: Convergence of representation learners as measured by FoV regression on the MPI3D dataset (dimensionality-controlled setting)

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{\(R^{2}\) score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.233 \(\pm\) 0.048 & 0.816 \(\pm\) 0.017 & 0.892 \(\pm\) 0.007 & 0.925 \(\pm\) 0.005 & 0.935 \(\pm\) 0.004 \\ Slow-VAE\({}^{\dagger}\) & 0.203 \(\pm\) 0.074 & 0.797 \(\pm\) 0.009 & 0.891 \(\pm\) 0.006 & 0.917 \(\pm\) 0.006 & 0.937 \(\pm\) 0.001 \\ Ada-GVAE-k & 0.307 \(\pm\) 0.084 & 0.821 \(\pm\) 0.016 & 0.896 \(\pm\) 0.009 & 0.944 \(\pm\) 0.007 & **0.957 \(\pm\) 0.005** \\ Ada-GVAE-k\({}^{\dagger}\) & 0.264 \(\pm\) 0.088 & 0.805 \(\pm\) 0.015 & 0.888 \(\pm\) 0.009 & 0.940 \(\pm\) 0.007 & _0.951 \(\pm\) 0.008_ \\ GVAE & _0.319 \(\pm\) 0.073_ & 0.831 \(\pm\) 0.012 & _0.901 \(\pm\) 0.009_ & _0.942 \(\pm\) 0.004_ & 0.947 \(\pm\) 0.005 \\ GVAE\({}^{\dagger}\) & 0.282 \(\pm\) 0.056 & 0.821 \(\pm\) 0.008 & 0.895 \(\pm\) 0.009 & 0.936 \(\pm\) 0.007 & 0.943 \(\pm\) 0.004 \\ MLVAE & 0.317 \(\pm\) 0.058 & 0.820 \(\pm\) 0.007 & 0.904 \(\pm\) 0.007 & **0.945 \(\pm\) 0.004** & 0.948 \(\pm\) 0.007 \\ MLVAE\({}^{\dagger}\) & 0.26 \(\pm\) 0.061 & 0.808 \(\pm\) 0.005 & 0.900 \(\pm\) 0.009 & 0.939 \(\pm\) 0.004 & 0.948 \(\pm\) 0.007 \\ Shu & -0.016 \(\pm\) 0.016 & 0.003 \(\pm\) 0.019 & 0.343 \(\pm\) 0.037 & 0.076 \(\pm\) 0.058 & 0.075 \(\pm\) 0.051 \\ Shu\({}^{\dagger}\) & 0.012 \(\pm\) 0.007 & -0.009 \(\pm\) 0.02 & 0.343 \(\pm\) 0.042 & 0.074 \(\pm\) 0.070 & 0.094 \(\pm\) 0.055 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 0.080 \(\pm\) 0.001 & _0.829 \(\pm\) 0.015_ & 0.877 \(\pm\) 0.007 & 0.832 \(\pm\) 0.010 & 0.813 \(\pm\) 0.014 \\ VCT\({}^{\dagger}\) & 0.038 \(\pm\) 0.014 & 0.243 \(\pm\) 0.03 & 0.655 \(\pm\) 0.011 & 0.631 \(\pm\) 0.029 & 0.623 \(\pm\) 0.036 \\ COMET & -0.029 \(\pm\) 0.011 & -0.035 \(\pm\) 0.023 & -0.035 \(\pm\) 0.023 & -0.035 \(\pm\) 0.023 & -0.035 \(\pm\) 0.023 \\ COMET\({}^{\dagger}\) & -0.484 \(\pm\) 0.477 & -0.495 \(\pm\) 0.512 & -0.495 \(\pm\) 0.512 & -0.515 \(\pm\) 0.47 & -0.715 \(\pm\) 0.295 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.531 \(\pm\) 0.054** & **0.855 \(\pm\) 0.012** & **0.920 \(\pm\) 0.009** & 0.914 \(\pm\) 0.008 & 0.910 \(\pm\) 0.010 \\ \hline \hline \end{tabular}
\end{table}
Table 31: Convergence of representation learners as measured by FoV regression on the Shapes3D dataset

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{\(R^{2}\) score} \\ \cline{2-5}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.597 \(\pm\) 0.048 & 0.927 \(\pm\) 0.007 & 0.960 \(\pm\) 0.032 & _0.999 \(\pm\) 0.000_ (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ Slow-VAE\({}^{\dagger}\) & 0.564 \(\pm\) 0.060 & 0.920 \(\pm\) 0.011 & 0.955 \(\pm\) 0.037 & _0.999 \(\pm\) 0.000_ (\(=\)) & _0.999 \(\pm\) 0.000_ (\(=\)) \\ Ada-GVAE-k & 0.684 \(\pm\) 0.068 & 0.974 \(\pm\) 0.007 & 0.997 \(\pm\) 0.000 & **1.000 \(\pm\) 0.000** (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ Ada-GVAE-k\({}^{\dagger}\) & 0.698 \(\pm\) 0.063 & 0.973 \(\pm\) 0.007 & 0.997 \(\pm\) 0.000 & **1.000 \(\pm\) 0.000** (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ GVAE & 0.565 \(\pm\) 0.034 & 0.951 \(\pm\) 0.015 & 0.997 \(\pm\) 0.000 & **1.000 \(\pm\) 0.000** (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ GVAE\({}^{\dagger}\) & 0.565 \(\pm\) 0.022 & 0.951 \(\pm\) 0.014 & _0.998 \(\pm\) 0.001_ & **1.000 \(\pm\) 0.000** (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ MLVAE & 0.551 \(\pm\) 0.059 & 0.959 \(\pm\) 0.012 & 0.997 \(\pm\) 0.000 & _0.999 \(\pm\) 0.000_ (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ MLVAE\({}^{\dagger}\) & 0.556 \(\pm\) 0.055 & 0.960 \(\pm\) 0.010 & 0.996 \(\pm\) 0.000 & _0.999 \(\pm\) 0.000_ (\(=\)) & **1.000 \(\pm\) 0.000** (\(=\)) \\ Shu & 0.327 \(\pm\) 0.13 & 0.497 \(\pm\) 0.106 & 0.631 \(\pm\) 0.087 & 0.471 \(\pm\

#### Abstract Visual Reasoning

We present now present our full suite of results for the downstream task of abstract visual reasoning. As demonstrated in Table 33 and the corresponding Figures 21 and 22, representations produced by our model at _only_\(10^{2}\) iterations of training are able to be leveraged by downstream models to achieve a 80.04\(\%\) accuracy for the challenging abstract visual reasoning task, in contrast to the value of 63.1\(\%\) obtained by the best performing baseline, representing a 26.78\(\%\) performance increase.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{\(R^{2}\) score} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & _0.557 \(\pm\) 0.012_ & _0.634 \(\pm\) 0.015_ & _0.818 \(\pm\) 0.016_ & _0.960 \(\pm\) 0.013_ & _0.980 \(\pm\) 0.005_ \\ Slow-VAE\({}^{\dagger}\) & 0.512 \(\pm\) 0.009 & 0.612 \(\pm\) 0.023 & 0.809 \(\pm\) 0.025 & **0.964 \(\pm\) 0.008** & **0.981 \(\pm\) 0.004** \\ Ada-GVAE-k & 0.519 \(\pm\) 0.023 & 0.611 \(\pm\) 0.028 & 0.694 \(\pm\) 0.007 & 0.750 \(\pm\) 0.008 & 0.762 \(\pm\) 0.014 \\ Ada-GVAE-k\({}^{\dagger}\) & 0.524 \(\pm\) 0.015 & 0.595 \(\pm\) 0.015 & 0.704 \(\pm\) 0.025 & 0.753 \(\pm\) 0.012 & 0.774 \(\pm\) 0.003 \\ GVAE & 0.511 \(\pm\) 0.037 & 0.602 \(\pm\) 0.021 & 0.724 \(\pm\) 0.012 & 0.748 \(\pm\) 0.013 & 0.771 \(\pm\) 0.018 \\ GVAE\({}^{\dagger}\) & 0.508 \(\pm\) 0.028 & 0.614 \(\pm\) 0.020 & 0.717 \(\pm\) 0.013 & 0.773 \(\pm\) 0.024 & 0.764 \(\pm\) 0.019 \\ MLVAE & 0.504 \(\pm\) 0.016 & 0.605 \(\pm\) 0.013 & 0.717 \(\pm\) 0.020 & 0.780 \(\pm\) 0.016 & 0.773 \(\pm\) 0.031 \\ MLVAE\({}^{\dagger}\) & 0.497 \(\pm\) 0.019 & 0.602 \(\pm\) 0.008 & 0.714 \(\pm\) 0.012 & 0.766 \(\pm\) 0.010 & 0.790 \(\pm\) 0.027 \\ Shu & 0.270 \(\pm\) 0.041 & 0.498 \(\pm\) 0.095 & 0.592 \(\pm\) 0.082 & 0.470 \(\pm\) 0.036 & 0.441 \(\pm\) 0.041 \\ Shu\({}^{\dagger}\) & 0.299 \(\pm\) 0.040 & 0.519 \(\pm\) 0.062 & 0.568 \(\pm\) 0.079 & 0.474 \(\pm\) 0.036 & 0.438 \(\pm\) 0.037 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 0.316 \(\pm\) 0.016 & 0.316 \(\pm\) 0.011 & 0.516 \(\pm\) 0.02 & 0.543 \(\pm\) 0.000 & 0.456 \(\pm\) 0.061 \\ VCT\({}^{\dagger}\) & 0.251 \(\pm\) 0.04 & 0.396 \(\pm\) 0.041 & 0.546 \(\pm\) 0.027 & 0.508 \(\pm\) 0.016 & 0.561 \(\pm\) 0.051 \\ COMET & 0.266 \(\pm\) 0.004 & 0.266 \(\pm\) 0.004 & 0.266 \(\pm\) 0.004 & 0.266 \(\pm\) 0.004 & 0.266 \(\pm\) 0.004 \\ COMET\({}^{\dagger}\) & 0.240 \(\pm\) 0.010 & 0.236 \(\pm\) 0.008 & 0.236 \(\pm\) 0.008 & 0.236 \(\pm\) 0.008 & 0.236 \(\pm\) 0.008 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.732 \(\pm\) 0.073** & **0.741 \(\pm\) 0.058** & **0.914 \(\pm\) 0.012** & 0.891 \(\pm\) 0.011 & 0.882 \(\pm\) 0.016 \\ \hline \hline \end{tabular}
\end{table}
Table 32: Convergence of representation learners as measured by FoV regression on the MPI3D datasetFigure 21: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset (original setting)

Figure 22: Convergence of representation learners as measured by classification performance on the abstract visual reasoning dataset (dimensionality-controlled setting)

### Downstream Performance

We present our full suite of experimental results that empirically demonstrate the utility of our Soft TPR representation from the perspective of downstream models, by considering the sample efficiency, and low-sample regime performance of downstream models on the tasks of FoV regression, and abstract visual reasoning.

#### c.5.1 Sample Efficiency Results

For sample efficiency, as mentioned in Section 5.2, and in line with [25], we compute a ratio-based metric obtained by dividing the performance of the downstream model when trained using a restricted number of 100, 250, 500, 1,000 and 10,000 samples, by its performance when trained using all samples (19,104, 480,000, 1,036,800 and 100,000 samples for the tasks of regression on the Cars3D, Shapes3D, MPI3D datasets, and the abstract visual reasoning task respectively). As this metric is dependent on the performance of downstream models when trained using all samples, we do not compute this metric for representation learners where the corresponding downstream models achieve an \(R^{2}\) score of less than \(0.5\) for regression, as this may produce sample efficiency scores with very little semantic meaning (e.g. a model that achieves a sample efficiency score of 0.9 when its final \(R^{2}\) score is 0.1). As a result, we remove Shu from Shapes3D sample efficiency calculations, and COMET and Shu from the Cars3D sample efficiency calculations.

As many models for the abstract visual reasoning task have low classification accuracies on the held-out test set following training with the maximal number of 100,000 samples, we do not compute sample efficiencies for this task, and instead refer readers to results in Section C.5.2 for the raw classification accuracies associated with each model.

Note that for all box plots, we follow standard convention, and display the median in each box with a solid line, where the box shows the quartiles of the corresponding values, and the whiskers extend to \(1.5\) times the interquartile range. We again, use the same legend, where grey denotes SlowVAE, orange denotes AdaGVAE-k, green denotes GVAE, red denotes MLVAE, purple denotes Shu, pink denotes VCT, brown denotes COMET, and blue denotes our model, Soft TPR Autoencoder.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{Classification accuracy} \\ \cline{2-6}  & \(10^{2}\) iterations & \(10^{3}\) iterations & \(10^{4}\) iterations & \(10^{5}\) iterations & \(2\times 10^{5}\) iterations \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokened compositional representations} \\ \hline Slow-VAE & 0.332 \(\pm\) 0.022 & 0.450 \(\pm\) 0.023 & 0.460 \(\pm\) 0.031 & 0.429 \(\pm\) 0.038 & 0.396 \(\pm\) 0.022 \\ Slow-VAE\({}^{\dagger}\) & 0.552 \(\pm\) 0.035 & 0.791 \(\pm\) 0.011 & 0.904 \(\pm\) 0.020 & **0.949 \(\pm\) 0.012** & _0.959 \(\pm\) 0.009_ \\ Ada-GVAE-k & 0.397 \(\pm\) 0.021 & 0.375 \(\pm\) 0.038 & 0.409 \(\pm\) 0.029 & 0.455 \(\pm\) 0.047 & 0.472 \(\pm\) 0.037 \\ Ada-GVAE-k\({}^{\dagger}\) & _0.631 \(\pm\) 0.037_ & 0.845 \(\pm\) 0.010 & 0.892 \(\pm\) 0.015 & _0.943 \(\pm\) 0.017_ & 0.954 \(\pm\) 0.009 \\ GVAE & 0.346 \(\pm\) 0.029 & 0.424 \(\pm\) 0.034 & 0.431 \(\pm\) 0.020 & 0.444 \(\pm\) 0.057 & 0.478 \(\pm\) 0.032 \\ GVAE\({}^{\dagger}\) & 0.554 \(\pm\) 0.031 & 0.815 \(\pm\) 0.008 & 0.894 \(\pm\) 0.011 & 0.894 \(\pm\) 0.055 & 0.936 \(\pm\) 0.008 \\ MLVAE & 0.336 \(\pm\) 0.016 & 0.404 \(\pm\) 0.013 & 0.431 \(\pm\) 0.038 & 0.466 \(\pm\) 0.052 & 0.432 \(\pm\) 0.023 \\ MLVAE\({}^{\dagger}\) & 0.550 \(\pm\) 0.025 & 0.824 \(\pm\) 0.021 & 0.878 \(\pm\) 0.014 & 0.919 \(\pm\) 0.034 & 0.925 \(\pm\) 0.024 \\ Shu\({}^{\dagger}\) & 0.208 \(\pm\) 0.052 & 0.273 \(\pm\) 0.012 & 0.331 \(\pm\) 0.007 & 0.317 \(\pm\) 0.025 & 0.310 \(\pm\) 0.038 \\ Shu\({}^{\dagger}\) & 0.207 \(\pm\) 0.048 & 0.399 \(\pm\) 0.072 & 0.462 \(\pm\) 0.024 & 0.433 \(\pm\) 0.023 & 0.444 \(\pm\) 0.013 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 0.440 \(\pm\) 0.033 & **0.932 \(\pm\) 0.062** & 0.769 \(\pm\) 0.087 & 0.695 \(\pm\) 0.049 & 0.641 \(\pm\) 0.039 \\ VCT\({}^{\dagger}\) & 0.432 \(\pm\) 0.043 & 0.723 \(\pm\) 0.065 & 0.731 \(\pm\) 0.017 & 0.479 \(\pm\) 0.023 & 0.458 \(\pm\) 0.040 \\ COMET & 0.369 \(\pm\) 0.077 & 0.683 \(\pm\) 0.037 & 0.848 \(\pm\) 0.049 & 0.501 \(\pm\) 0.251 & **0.984 \(\pm\) 0.006** \\ COMET\({}^{\dagger}\) & 0.348 \(\pm\) 0.069 & 0.545 \(\pm\) 0.197 & 0.829 \(\pm\) 0.051 & 0.856 \(\pm\) 0.256 & 0.532 \(\pm\) 0.348 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.804 \(\pm\) 0.016** & _0.864 \(\pm\) 0.011_ & **0.952 \(\pm\) 0.008** & 0.884 \(\pm\) 0.012 & 0.869 \(\pm\) 0.024 \\ \hline \hline \end{tabular}
\end{table}
Table 33: Convergence of representation learners as measured by classification performance on the abstract visual reasoning datasetFigure 23: Downstream regression model sample efficiency on the Cars3D dataset (original setting).

Figure 24: Downstream regression model sample efficiency on the Cars3D dataset (original setting).

Figure 26: Downstream regression model sample efficiency on the Cars3D dataset (dimensionality-controlled setting).

Figure 25: Downstream regression model sample efficiency on the Cars3D dataset (dimensionality-controlled setting).

Figure 28: Downstream regression model sample efficiency on the Shapes3D dataset (original setting).

Figure 27: Downstream regression model sample efficiency on the Shapes3D dataset (original setting).

Figure 30: Downstream regression model sample efficiency on the Shapes3D dataset (dimensionality-controlled setting).

Figure 29: Downstream regression model sample efficiency on the Shapes3D dataset (dimensionality-controlled setting).

Figure 31: Downstream regression model sample efficiency on the MPI3D dataset (original setting).

Figure 32: Downstream regression model sample efficiency on the MPI3D dataset (original setting).

Figure 34: Downstream regression model sample efficiency on the MPI3D dataset (dimensionality-controlled setting).

Figure 33: Downstream regression model sample efficiency on the MPI3D dataset (dimensionality-controlled setting).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{\(R^{2}\) ratio} \\ \cline{2-6}  & \(10^{2}\)/all samples & \(2.5\times 10^{2}\)/all samples & \(5\times 10^{3}\)/all samples & \(10^{4}\)/all samples & \(10^{5}\)/all samples \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokensed compositional representations} \\ \hline Slow-VAE & 0.273 \(\pm\) 0.052 & 0.537 \(\pm\) 0.012 & 0.833 \(\pm\) 0.025 & 0.882 \(\pm\) 0.018 & _0.994 \(\pm\) 0.004_ \\ Slow-VAE\({}^{\dagger}\) & 0.166 \(\pm\) 0.055 & 0.292 \(\pm\) 0.014 & 0.866 \(\pm\) 0.005 & 0.895 \(\pm\) 0.010 & 0.993 \(\pm\) 0.002 \\ Ada-GVAE-k & 0.610 \(\pm\) 0.131 & 0.834 \(\pm\) 0.040 & _0.881 \(\pm\) 0.026_ & _0.922 \(\pm\) 0.019_ & 0.989 \(\pm\) 0.005 \\ Ada-GVAE-k\({}^{\dagger}\) & 0.452 \(\pm\) 0.156 & 0.821 \(\pm\) 0.028 & 0.879 \(\pm\) 0.029 & 0.920 \(\pm\) 0.021 & 0.992 \(\pm\) 0.007 \\ GVAE & 0.412 \(\pm\) 0.092 & 0.791 \(\pm\) 0.013 & 0.855 \(\pm\) 0.012 & 0.900 \(\pm\) 0.008 & 0.986 \(\pm\) 0.006 \\ GVAE\({}^{\dagger}\) & 0.314 \(\pm\) 0.066 & _0.799 \(\pm\) 0.010_ & 0.849 \(\pm\) 0.014 & 0.895 \(\pm\) 0.014 & 0.987 \(\pm\) 0.004 \\ MLVAE & 0.477 \(\pm\) 0.069 & 0.698 \(\pm\) 0.027 & 0.844 \(\pm\) 0.015 & 0.893 \(\pm\) 0.016 & 0.984 \(\pm\) 0.010 \\ MLVAE\({}^{\dagger}\) & 0.305 \(\pm\) 0.071 & 0.648 \(\pm\) 0.072 & 0.854 \(\pm\) 0.013 & 0.896 \(\pm\) 0.012 & 0.989 \(\pm\) 0.008 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokensed compositional representations} \\ \hline VCT & 0.558 \(\pm\) 0.028 & 0.694 \(\pm\) 0.034 & 0.808 \(\pm\) 0.013 & 0.837 \(\pm\) 0.023 & 0.973 \(\pm\) 0.024 \\ VCT\({}^{\dagger}\) & 0.197 \(\pm\) 0.151 & 0.640 \(\pm\) 0.113 & 0.640 \(\pm\) 0.113 & 0.725 \(\pm\) 0.038 & **1.000 \(\pm\) 0.000 \((=)\)** \\ \hline Ours & **0.705 \(\pm\) 0.023** & **0.889 \(\pm\) 0.042** & **0.926 \(\pm\) 0.009** & **0.942 \(\pm\) 0.018** & **1.000 \(\pm\) 0.000 \((=)\)** \\ \hline \hline \end{tabular}
\end{table}
Table 34: Downstream regression model sample efficiency on the Cars3D dataset

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{\(R^{2}\) ratio} \\ \cline{2-6}  & \(10^{2}\)/all samples & \(2.5\times 10^{2}\)/all samples & \(5\times 10^{3}\)/all samples & \(10^{4}\)/all samples & \(10^{5}\)/all samples \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokensed compositional representations} \\ \hline Slow-VAE & 0.021 \(\pm\) 0.016 & 0.085 \(\pm\) 0.035 & 0.183 \(\pm\) 0.051 & 0.563 \(\pm\) 0.070 & 0.994 \(\pm\) 0.002 \\ Slow-VAE\({}^{\dagger}\) & 0.003 \(\pm\) 0.008 & 0.021 \(\pm\) 0.025 & 0.066 \(\pm\) 0.036 & 0.419 \(\pm\) 0.107 & 0.995 \(\pm\) 0.002 \\ Ada-GVAE-k & _0.480 \(\pm\) 0.155_ & 0.510 \(\pm\) 0.160 & 0.717 \(\pm\) 0.206 & 0.855 \(\pm\) 0.103 & 0.997 \(\pm\) 0.001 \\ Ada-GVAE-k\({}^{\dagger}\) & 0.133 \(\pm\) 0.088 & 0.199 \(\pm\) 0.058 & 0.643 \(\pm\) 0.231 & 0.809 \(\pm\) 0.164 & _0.998 \(\pm\) 0.000 \((=)\)_ \\ GVAE & 0.417 \(\pm\) 0.056 & 0.594 \(\pm\) 0.079 & 0.688 \(\pm\) 0.052 & 0.893 \(\pm\) 0.024 & 0.997 \(\pm\) 0.000 \\ GVAE\({}^{\dagger}\) & 0.13 \(\pm\) 0.047 & 0.234 \(\pm\) 0.038 & 0.479 \(\pm\) 0.076 & 0.871 \(\pm\) 0.029 & _0.998 \(\pm\) 0.000 \((=)\)_ \\ MLVAE & 0.371 \(\pm\) 0.041 & 0.515 \(\pm\) 0.093 & 0.590 \(\pm\) 0.080 & 0.855 \(\pm\) 0.059 & 0.997 \(\pm\) 0.000 \\ MLVAE\({}^{\dagger}\) & 0.123 \(\pm\) 0.048 & 0.235 \(\pm\) 0.069 & 0.413 \(\pm\) 0.035 & 0.808 \(\pm\) 0.141 & **0.999 \(\pm\) 0.000** \\ \hline \multicolumn{6}{c}{Symbolic vector-tokensed compositional representations} \\ \hline VCT & 0.020 \(\pm\) 0.025 & 0.216 \(\pm\) 0.139 & 0.377 \(\pm\) 0.100 & 0.591 \(\pm\) 0.079 & 0.884 \(\pm\) 0.008 \\ VCT\({}^{\dagger}\) & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.009 \(\pm\) 0.018 & 0.470 \(\pm\) 0.071 \\ COMET & 0.352 \(\pm\) 0.036 & 0.699 \(\pm\) 0.038 & 0.772 \(\pm\) 0.028 & 0.812 \(\pm\) 0.025 & 0.997 \(\pm\) 0.001 \\ COMET\({}^{\dagger}\) & **0.752 \(\pm\) 0.039** & **0.810 \(\pm\) 0.034** & **0.870 \(\pm\) 0.029** & **0.916 \(\pm\) 0.031** & 0.996 \(\pm\) 0.001 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & 0.464 \(\pm\) 0.071 & _0.730 \(\pm\) 0.038_ & _0.804 \(\pm\) 0.075_ & _0.889 \(\pm\) 0.035_ & 0.996 \(\pm\) 0.001 \\ \hline \hline \end{tabular}
\end{table}
Table 35: Downstream regression model sample efficiency on the Shapes3D dataset

#### c.5.2 Low Sample Regime Results

To evaluate the utility of our Soft TPR representation from the perspective of downstream models, we additionally evaluate the raw performance of downstream models as a function of the number of samples they have been trained on (again considering 100, 250, 500, 1,000, 10,000 and all samples). We find that our Soft TPR representation contributes to a substantial performance boost in the downstream model's performance in a low-sample regime where the downstream model has been trained with 100, 250, 500, and 1,000 samples. We present our full suite of experimental results, and highlight the particular performance differentials conferred by our representational form in the low-sample regime.

**FoV Regression**

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{5}{c}{\(R^{2}\) ratio} \\ \cline{2-6}  & \(10^{2}\)/all samples & \(2.5\times 10^{2}\)/all samples & \(5\times 10^{3}\)/all samples & \(10^{4}\)/all samples & \(10^{5}\)/all samples \\ \hline \multicolumn{6}{c}{Symbolic scalar-tokended compositional representations} \\ \hline Slow-VAE & 0.130 \(\pm\) 0.051 & 0.155 \(\pm\) 0.011 & _0.608 \(\pm\) 0.082_ & _0.692 \(\pm\) 0.081_ & _0.881 \(\pm\) 0.022_ \\ Slow-VAE\({}^{\dagger}\) & 0.107 \(\pm\) 0.027 & 0.095 \(\pm\) 0.011 & 0.541 \(\pm\) 0.079 & 0.668 \(\pm\) 0.117 & 0.877 \(\pm\) 0.017 \\ Ada-GVAE-k & _0.270 \(\pm\) 0.037_ & 0.279 \(\pm\) 0.026 & 0.500 \(\pm\) 0.016 & 0.541 \(\pm\) 0.020 & 0.703 \(\pm\) 0.017 \\ Ada-GVAE-k\({}^{\dagger}\) & 0.053 \(\pm\) 0.053 & 0.120 \(\pm\) 0.023 & 0.442 \(\pm\) 0.018 & 0.504 \(\pm\) 0.015 & 0.680 \(\pm\) 0.012 \\ GVAE\({}^{\dagger}\) & 0.234 \(\pm\) 0.035 & 0.282 \(\pm\) 0.027 & 0.481 \(\pm\) 0.032 & 0.524 \(\pm\) 0.035 & 0.686 \(\pm\) 0.033 \\ GVAE\({}^{\dagger}\) & 0.077 \(\pm\) 0.073 & 0.157 \(\pm\) 0.037 & 0.415 \(\pm\) 0.043 & 0.443 \(\pm\) 0.034 & 0.648 \(\pm\) 0.025 \\ MLVAE & 0.236 \(\pm\) 0.019 & 0.288 \(\pm\) 0.030 & 0.462 \(\pm\) 0.051 & 0.497 \(\pm\) 0.017 & 0.663 \(\pm\) 0.012 \\ MLVAE\({}^{\dagger}\) & 0.065 \(\pm\) 0.042 & 0.114 \(\pm\) 0.049 & 0.387 \(\pm\) 0.024 & 0.469 \(\pm\) 0.034 & 0.659 \(\pm\) 0.030 \\ Shu & 0.343 \(\pm\) 0.024 & _0.482 \(\pm\) 0.075_ & 0.549 \(\pm\) 0.091 & 0.601 \(\pm\) 0.047 & 0.750 \(\pm\) 0.058 \\ Shu\({}^{\dagger}\) & 0.143 \(\pm\) 0.103 & 0.427 \(\pm\) 0.035 & 0.493 \(\pm\) 0.073 & 0.547 \(\pm\) 0.070 & 0.714 \(\pm\) 0.067 \\ \hline \multicolumn{6}{c}{Symbolic vector-tokended compositional representations} \\ \hline VCT & 0.189 \(\pm\) 0.107 & 0.246 \(\pm\) 0.137 & 0.294 \(\pm\) 0.145 & 0.312 \(\pm\) 0.14 & 0.418 \(\pm\) 0.180 \\ VCT\({}^{\dagger}\) & 0.039 \(\pm\) 0.088 & 0.168 \(\pm\) 0.082 & 0.230 \(\pm\) 0.110 & 0.279 \(\pm\) 0.127 & 0.502 \(\pm\) 0.052 \\ COMET & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.823 \(\pm\) 0.139 \\ COMET\({}^{\dagger}\) & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.000 \(\pm\) 0.000 & 0.187 \(\pm\) 0.374 \\ \hline \multicolumn{6}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.556 \(\pm\) 0.078** & **0.665 \(\pm\) 0.067** & **0.721 \(\pm\) 0.089** & **0.787 \(\pm\) 0.078** & **0.858 \(\pm\) 0.040** \\ \hline \hline \end{tabular}
\end{table}
Table 36: Downstream regression model sample efficiency on the MPI3D datasetFigure 35: Downstream regression model \(R^{2}\) scores on the Cars3D dataset (original setting).

Figure 36: Downstream regression model \(R^{2}\) scores on the Cars3D dataset (original setting).

Figure 37: Downstream regression model \(R^{2}\) scores on the Cars3D dataset (dimensionality-controlled setting).

Figure 38: Downstream regression model \(R^{2}\) scores on the Cars3D dataset (dimensionality-controlled setting).

Figure 40: Downstream regression model \(R^{2}\) scores on the Shapes3D dataset (original setting).

Figure 39: Downstream regression model \(R^{2}\) scores on the Shapes3D dataset (original setting).

Figure 41: Downstream regression model \(R^{2}\) scores on the Shapes3D dataset (dimensionality-controlled setting).

Figure 42: Downstream regression model \(R^{2}\) scores on the Shapes3D dataset (dimensionality-controlled setting).

Figure 43: Downstream regression model \(R^{2}\) scores on the MPI3D dataset (original setting).

Figure 44: Downstream regression model \(R^{2}\) scores on the MPI3D dataset (original setting).

Figure 45: Downstream regression model \(R^{2}\) scores on the MPI3D dataset (dimensionality-controlled setting).

Figure 46: Downstream regression model \(R^{2}\) scores on the MPI3D dataset (dimensionality-controlled setting).

[MISSING_PAGE_FAIL:65]

[MISSING_PAGE_EMPTY:66]

Figure 47: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (original setting).

Figure 48: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (original setting).

Figure 49: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (dimensionality-controlled setting).

Figure 50: Downstream WReN model classification accuracy on the abstract visual reasoning dataset (dimensionality-controlled setting).

### Ablation Experiments

We perform the following ablation studies:

* **Foundational Model Components**: We evaluate the impact of _Relaxed Representational Constraints_ (RRC) and _Distributed Encoding_ (DE) on disentanglement performance, as presented in Table 43. Previous work [30, 33] has established a positive correlation between disentanglement and both downstream sample efficiency and overall performance. Therefore, we do not separately assess the effects of these foundational components on downstream tasks.
* **Auxiliary Model Components**: We investigate the influence of additional model components, as detailed in Appendix C.6.2.
* **Effect of Soft TPR on Downstream Tasks**: Separately, we examine whether Soft TPR offers advantages over its quantised, explicit counterpart, \(\psi_{tpr}^{*}\), in terms of downstream model performance.

#### c.6.1 Distributed Encoding Ablation

To perform an ablation study on distributed encoding, we set the role embedding matrix \(M_{\xi_{R}}\) to the identity matrix. This modification results in maximally sparse role-filler bindings of the form \(\xi_{F}(f_{m(i)})\otimes e_{i}\), specifically:

\[\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})=\begin{bmatrix}0&\cdots&0&\xi_{F}(f_{m(i )})&0&\cdots&0\end{bmatrix}\]

In this setup, the embedded filler \(\xi_{F}(f_{m(i)})\) occupies the \(i\)-th column of the resulting matrix. Consequently, the TPRs generated by the TPR decoder become degenerate, effectively forming a concatenation of filler embeddings:

\[\psi_{tpr}(x)\cong\left(\xi_{F}(f_{m(1)})^{\top},\ldots,\xi_{F}(f_{m(N_{R})}) ^{\top}\right)^{\top}\]

This configuration confines each filler's information to specific dimensions within the representation, thereby limiting the model's ability to leverage distributed information across multiple dimensions.

Our experiments reveal moderate reductions in disentanglement performance and increases in loss compared to the distributed encoding scenario. Considering the rotational isomorphism between an identity matrix \(M_{\xi_{R}}\) and a (semi)-orthogonal \(M_{\xi_{R}}\), it is somewhat surprising to observe these differences under identical conditions (e.g., initialisation and random seeds). While future work

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{6}{c}{Classification accuracy} \\ \cline{2-7}  & \(10^{2}\) samples & \(2.5\times 10^{2}\) samples & \(5\times 10^{2}\) samples & \(10^{3}\) samples & \(10^{4}\) samples & \(10^{5}\) samples \\ \hline \multicolumn{7}{c}{Symbolic scalar-tokensed compositional representations} \\ \hline Slow-VAE & 0.178 \(\pm\) 0.008 & 0.196 \(\pm\) 0.024 & 0.196 \(\pm\) 0.028 & 0.228 \(\pm\) 0.030 & 0.361 \(\pm\) 0.028 & 0.396 \(\pm\) 0.022 \\ Slow-VAE\({}^{\dagger}\) & 0.146 \(\pm\) 0.084 & 0.149 \(\pm\) 0.034 & 0.163 \(\pm\) 0.039 & 0.237 \(\pm\) 0.023 & 0.345 \(\pm\) 0.021 & 0.395 \(\pm\) 0.009 \\ Ada-GVAE-k\({}^{\dagger}\) & 0.188 \(\pm\) 0.023 & 0.198 \(\pm\) 0.018 & 0.203 \(\pm\) 0.007 & 0.194 \(\pm\) 0.008 & 0.295 \(\pm\) 0.028 & 0.472 \(\pm\) 0.037 \\ Ada-GVAE-k\({}^{\dagger}\) & 0.182 \(\pm\) 0.007 & _0.234 \(\pm\) 0.024_ & _0.285 \(\pm\) 0.025_ & 0.319 \(\pm\) 0.004 & _0.662 \(\pm\) 0.023_ & 0.954 \(\pm\) 0.009 \\ GVAE\({}^{\ddagger}\) & 0.171 \(\pm\) 0.009 & 0.189 \(\pm\) 0.008 & 0.182 \(\pm\) 0.013 & 0.188 \(\pm\) 0.020 & 0.319 \(\pm\) 0.038 & 0.478 \(\pm\) 0.032 \\ GVAE\({}^{\dagger}\) & 0.180 \(\pm\) 0.022 & 0.237 \(\pm\) 0.018 & 0.287 \(\pm\) 0.014 & 0.306 \(\pm\) 0.020 & **0.684 \(\pm\) 0.020** & 0.936 \(\pm\) 0.008 \\ MLVAE & 0.171 \(\pm\) 0.016 & 0.188 \(\pm\) 0.009 & 0.193 \(\pm\) 0.012 & 0.194 \(\pm\) 0.015 & 0.293 \(\pm\) 0.012 & 0.432 \(\pm\) 0.023 \\ MLVAE\({}^{\dagger}\) & 0.177 \(\pm\) 0.012 & 0.221 \(\pm\) 0.023 & 0.277 \(\pm\) 0.023 & 0.325 \(\pm\) 0.017 & 0.644 \(\pm\) 0.026 & 0.925 \(\pm\) 0.024 \\ Shu & 0.175 \(\pm\) 0.009 & 0.184 \(\pm\) 0.018 & 0.200 \(\pm\) 0.010 & 0.202 \(\pm\) 0.014 & 0.288 \(\pm\) 0.038 & 0.310 \(\pm\) 0.038 \\ Shu\({}^{\dagger}\) & 0.177 \(\pm\) 0.014 & 0.230 \(\pm\) 0.045 & 0.285 \(\pm\) 0.028 & 0.302 \(\pm\) 0.034 & 0.444 \(\pm\) 0.013 & 0.444 \(\pm\) 0.013 \\ \hline \multicolumn{7}{c}{Symbolic vector-tokensed compositional representations} \\ \hline VCT & _0.228 \(\pm\) 0.036_ & 0.229 \(\pm\) 0.049 & 0.277 \(\pm\) 0.039 & _0.326 \(\pm\) 0.042_ & 0.371 \(\pm\) 0.040 & 0.641 \(\pm\) 0.039 \\ VCT\({}^{\dagger}\) & 0.191 \(\pm\) 0.017 & 0.226 \(\pm\) 0.031 & 0.259 \(\pm\) 0.009 & 0.282 \(\pm\) 0.017 & 0.319 \(\pm\) 0.018 & 0.458 \(\pm\) 0.040 \\ COMET & 0.174 \(\pm\) 0.010 & 0.241 \(\pm\) 0.010 & 0.259 \(\pm\) 0.016 & 0.283 \(\pm\) 0.009 & 0.221 \(\pm\) 0.012 & **0.983 \(\pm\) 0.006** \\ COMET\({}^{\dagger}\) & 0.190 \(\pm\) 0.012 & 0.214 \(\pm\) 0.004 & 0.236 \(\pm\) 0.013 & 0.213 \(\pm\) 0.023 & 0.532 \(\pm\) 0.348 & 0.532 \(\pm\) 0.348 \\ \hline \multicolumn{7}{c}{Fully continuous compositional representations} \\ \hline Ours & **0.273 \(\pm\) 0.033** & **0.312 \(\pm\) 0.027** & **0.360 \(\pm\) 0.033** & **0.412 \(\pm\) 0.066** & 0.560 \(\pm\) 0.103 & 0.869 \(\pm\) 0.024 \\ \hline \hline \end{tabular}
\end{table}
Table 40: Downstream WReN model performance on the abstract visual reasoning dataset_should_ aim to learn linearly independent (but not necessarily orthogonal) \(M_{\xi_{R}}\), which would more convincingly account for such results by embedding each filler in _overlapping_ role subspaces and eliminating the rotational isomorphism with the degenerate TPR case, this behaviour warrants further analysis and investigation. One potential avenue of investigation is examining whether the sparsity of binding embeddings \(\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\) affects 'neuron' specialisation (i.e., the tendency of each row of the weight matrix to attend only to certain filler-specific information).

#### c.6.2 Additional Ablations

We additionally examine the importance of the following properties of our model in producing explicitly compositional Soft TPR representations: 1) the presence of weak supervision, by setting \(\lambda_{1}=\lambda_{2}=0\) in Equation 7, 2) the explicit dependency between the quantised filler embeddings and the decoder output, by instead using the Soft TPR to reconstruct the input image, and 3) the semi-orthogonality of the role embedding matrix, \(M_{\xi_{R}}\) by removing this constraint in the random initialisation of \(M_{\xi_{R}}\), with the results of these ablations illustrated in Table 42.

#### c.6.3 Soft TPR vs TPR

To examine the effect of the Soft TPR on downstream tasks, for each fully trained Soft TPR representation learner, we extract both the Soft TPR, \(z\), and the Soft TPR's corresponding quantised TPR, \(\psi^{*}_{tpr}\), and investigate downstream performance using both types of representation. In all plots, we use the same legend, denoting the explicit TPR as yellow (0) and the Soft TPR as blue (1). Across all considered cases: i.e., 1) convergence rate of representation learning (as measured by the downstream model's ability to effectively leverage representations produced at different stages of training), 2) sample efficiency of downstream models, and 3) raw performance of downstream models in the low sample regime, the Soft TPR confers differential performance boosts compared to the explicit TPR.

**Convergence Rate of Representation Learning**

\begin{table}
\begin{tabular}{c c} \hline \hline Property & DCI Score \\ \hline - Weak supervision & 0.225 \(\pm\) 0.034 \\ - Explicit filler dependency & 0.718 \(\pm\) 0.051 \\ - Semi orthogonality & 0.756 \(\pm\) 0.039 \\ Full & 0.828 \(\pm\) 0.015 \\ \hline \hline \end{tabular}
\end{table}
Table 42: Effect of model properties on disentanglement performance (MPI3D dataset).

\begin{table}
\begin{tabular}{c c c c} \hline \hline Property & FactorVAE score & DCI score & MIG score & BetaVAE score \\ \hline - DE & 0.901 \(\pm\) 0.102 & 0.704 \(\pm\) 0.135 & 0.530 \(\pm\) 0.087 & 0.991 \(\pm\) 0.021 \\ + DE (Full) & 0.949 \(\pm\) 0.032 & 0.828 \(\pm\) 0.015 & 0.620 \(\pm\) 0.067 & 1.000 \(\pm\) 0.000 \\ \hline \hline \end{tabular}
\end{table}
Table 41: Effect of DE on disentanglement (MPI3D dataset).

Figure 51: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the Cars3D dataset

Figure 52: Convergence of Soft TPR (0) vs TPR (1) as measured by FoV regression on the Shapes3D dataset

[MISSING_PAGE_EMPTY:72]

Figure 56: Downstream regression model sample efficiency on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 55: Downstream regression model sample efficiency on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 57: Downstream regression model sample efficiency on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 58: Downstream regression model sample efficiency on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 59: Downstream regression model sample efficiency on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 60: Downstream regression model sample efficiency on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1)

### Low Sample-Regime Performance of Downstream Models

Figure 61: Downstream WReN model sample efficiency on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 62: Downstream WReN model sample efficiency on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1)Figure 63: Downstream regression model \(R^{2}\) scores on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 64: Downstream regression model \(R^{2}\) scores on the Cars3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 65: Downstream regression model \(R^{2}\) scores on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 66: Downstream regression model \(R^{2}\) scores on the Shapes3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 67: Downstream regression model \(R^{2}\) scores on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 68: Downstream regression model \(R^{2}\) scores on the MPI3D dataset using TPR representations (0) vs Soft TPR representations (1)

#### c.6.4 Robustness to Hyperparameter Choices

We perform an additional experiment to empiricially verify that our model is robust to different hyperparameter choices. For the MPI3D dataset, which disentanglement models experience the greatest difficulty in producing disentangled representations for (see Table 1), we randomly pick another set of hyperparameters, shown in Table 43, from the top 5 models based on the MSE loss criterion. For this randomly chosen hyperparameter configuration, we evaluate the disentanglement of the representations on the MPI3D dataset produced by the resulting model.

Figure 69: Downstream WReN model classification accuracy on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1)

Figure 70: Downstream WReN model classification accuracy on the abstract visual reasoning dataset using TPR representations (0) vs Soft TPR representations (1)

## Appendix D Limitations and Future Work

### Extension to Linguistic Domains

Applying the Soft TPR to the TPR's typical domain of language is an intriguing future direction, especially as language can deviate from strict algebraic compositionality - for instance, idiomatic expressions such as'spill the beans' cannot be understood as a function of their constituents alone. Soft TPR's more flexible specification allows it to capture approximate forms of compositionality precluded from the TPR's strict algebraic definition (Eq 1), thereby potentially providing the Soft TPR the ability to better handle the nuance and complexity of language.

To adapt our framework to language, we replace our Conv/Deconv encoder/decoders with simple RNNs, retrain our TPR decoder, and remove the semi-supervised loss, using Eq 6 as the full loss. Preliminary results in Table 45 on the BaBI [7] dataset are compared with TPR baselines from AID [51]. Our Soft TPR Autoencoder does not presently surpass AID, but notable points include:

1. Our use of simpler RRN-based encoders and an MLP-based downstream network, unlike the more sophisticated architectures of [51].
2. Soft TPR retains its performance improvement above the corresponding explicit TPR it can be quantised into.
3. The smaller gap between systematic vs non-systematic dataset splits in our model compared to TPR-RNN (+AID) and FWM.
4. We train our representation learner using self-supervision (reconstruction loss) alone, only employing supervision on the downstream prediction network, while the baselines employ strong supervision and end-to-end training to produce representations.

\begin{table}
\begin{tabular}{c c c} \hline \hline Hyperparameter & MPI3D \\ \hline \multicolumn{3}{c}{Architectural hyperparameters} \\ \hline \(D_{R}\) & 16 \\ \(N_{R}\)_(fixed)_ & 10 \\ \(D_{F}\) & 64 \\ \(N_{F}\) & 50 \\ \hline \multicolumn{3}{c}{Loss function hyperparameters} \\ \hline \(\lambda_{1}\) & 0.0000 \\ \(\lambda_{2}\) & 0.25378 \\ \(\beta\)_(fixed)_ & 0.5 \\ \hline \end{tabular}
\end{table}
Table 43: Hyperparameter values of ablation setting

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Hyperparameter configuration & FactorVAE score & DCI score & BetaVAE score & MIG score \\ \hline Original & 0.949 \(\pm\) 0.032 & 0.828 \(\pm\) 0.015 & 1.000 \(\pm\) 0.000 & 0.620 \(\pm\) 0.067 \\ Ablation & 0.911 \(\pm\) 0.029 & 0.798 \(\pm\) 0.031 & 1.000 \(\pm\) 0.000 & 0.590 \(\pm\) 0.047 \\ \hline \hline \end{tabular}
\end{table}
Table 44: Disentanglement metric scores on the MPI3D dataset

\begin{table}
\begin{tabular}{c c c c} \hline \hline Model & w/o sys diff & w/ sys diff & Gap \\ \hline TPR-RNN & 0.79 \(\pm\) 0.16 & 8.74 \(\pm\) 3.74 & 7.95 \\ TPR-RNN + AID & _0.69 \(\pm\) 0.08_ & 5.61 \(\pm\) 1.78 & 4.92 \\ FWM & 0.79 \(\pm\) 0.14 & _2.85 \(\pm\) 1.61_ & 2.06 \\ FWM + AID & **0.45 \(\pm\) 0.16** & **1.21 \(\pm\) 0.66** & **0.76** \\ Ours (TPR) & 4.54 \(\pm\) 0.61 & 6.51 \(\pm\) 1.23 & 1.97 \\ Ours (Soft TPR) & 2.79 \(\pm\) 0.24 & 3.64 \(\pm\) 0.69 & _0.85_ \\ \hline \hline \end{tabular}
\end{table}
Table 45: Mean word error rate [%] on the sys-bAbI task [51]

### Need for Weak Supervision

To produce a compositional representation, \(\psi(x)=C(\psi_{1}(a_{1}),\ldots,\psi_{n}(a_{n}))\) (as in Section 3.1), each representational constituent, \(\psi_{i}(a_{i})\), must map 1-1 to a data constituent, \(a_{i}\). Without supervision (i.e., access only to observational data, \(\{x_{i}\}\)), this is challenging, because the data constituents \(\{a_{i}\}\) underlying each object, \(x\), are unknown and cannot be identified.

We can frame the above intuition in a more mathematically rigorous way, using the generative framework. Essentially, as formally proved in [25], it is impossible to identify the true distribution for the data constituents (generative factors), \(p(a)\), using observational data, \(\{x_{i}\}\), alone as there are infinitely many bijective functions \(f:\mathrm{supp}(a)\to a\) such that: 1) \(a\) and \(f(a)\) are completely entangled (i.e. non-diagonal Jacobian) _and_ 2) the marginal distributions of \(a\) and \(f(a)\) are identical (meaning the marginal distributions of the observations are also identical, i.e., \(\int p(x|a)p(a)da=\int p(x|f(a))p(f(a))da\)). Thus, without inductive biases, it is impossible to infer the data constituents \(\{a_{i}\}\) of any observation, \(x\), from observational data \(\{x_{i}\}\) alone.

To combat this non-identifiability result, in line with [13, 22, 31, 33, 35], we use weak supervision, presenting the model with data pairs \((x,x^{\prime})\) where \(x\) and \(x^{\prime}\) differ in a subset of FoVs, e.g. \(\beta(x)=\{\mathrm{shape}/\mathrm{cube},\mathrm{colour}/purple,\mathrm{size}/ \mathrm{large}\}\), \(\beta(x^{\prime})=\{\mathrm{shape}/\mathrm{cube},\mathrm{colour}/cyan, \mathrm{size}/\mathrm{large}\}\), where the FoV _types_ corresponding to the different FoV values are _known_ to the model. Note that this weak supervision is minimal, only providing the model to access to the differing FoV _types_ in an index set, \(I\), (i.e., \(I:=\{\mathrm{colour}\}\)) and not _any_ of the FoV values (i.e., \(\{\mathrm{cube},\mathrm{purple},\mathrm{cyan},\mathrm{large}\}\) are all not known by the model).

Some possible future extensions to reduce this level of weak supervision, or alternative forms of weak supervision include:

1. **Embodied Learning**: In the visual domain, some roles, e.g. \(\mathrm{object}\)\(\mathrm{position}\) correspond to affordances. Embodied agents may be able to reduce the need for explicit supervision by collecting \((x,x^{\prime})\) and \(I\) through interaction with their environment.
2. **Pretrained Filler Embeddings**: Initialising the filler embedding matrix, \(M_{\xi p}\) with embeddings learnt by a pre-trained vision model could impart knowledge of domain-agnostic fillers (e.g., colours, shapes), reducing the need to explicitly provide \((x,x^{\prime})\) and \(I\) to the model.
3. **Segmentation Masks**: Segmentation masks for each object may potentially reduce the need to explicitly provide \((x,x^{\prime})\) and \(I\) to the model.

### Downstream Utility

Our investigation of downstream utility centers on two selected tasks - FoV regression/classification and abstract visual reasoning, which aligns with the standard framework for assessing the quality and downstream utility of compositional representations [26, 30, 33, 40, 44, 46]. While existing work [30, 33] demonstrates that explicitly compositional representations enhance downstream sample efficiency compared to non-compositional representations, a result we improve upon (C.5.1), the broader utility of compositional representations remains a topic of ongoing exploration [21, 40, 42, 44].

Theoretical perspectives [1, 2] argue that explicitly compositional representations are fundamental in the production of productive, systematic, and inferentially coherent thought - 3 key properties characterising human cognition. Investigating how explicitly compositional representations can yield empirical benefits across these dimensions represents an essential avenue for future research. Although preliminary studies [40, 42, 44] do not find strong evidence that explicitly compositional representations improve compositional generalisation (a key aspect of systematicity), [44] suggests that this finding is because compositional representations are necessary, but not sufficient to induce systematicity; an explicitly compositional processing approach [6] is also required.

Future work could extend our theoretical framework with the hope of producing empirical results consistent with the theoretical arguments of [1, 2]. In particular, as our unbinding module is designed to provably and efficiently recover structured role-filler constituents from the Soft TPR, it may be possible to exploit this module to systematically reconfigure roles and fillers from existing representations to create representations of novel combinations of role-filler bindings (i.e., novelcompositional data). This type of ability could prove extremely beneficial in areas such as concept learning and compositional generalisation.

### Dimensionality

In this subsection, we denote the dimensions of the role and filler embedding spaces as \(D_{F}\) and \(D_{R}\) respectively. We also denote the number of fillers as \(N_{F}\) and the number of roles as \(N_{R}\).

The Soft TPR belongs to \(V_{F}\otimes V_{R}\), which is a \(D_{F}\times D_{R}\) dimensional space, which grows multiplicatively in \(D_{F}\), \(D_{R}\). Several factors, however, mitigate scalability concerns in light of this fact:

1. **Independence of Embedding Space Dimensionality**: We note that the dimensionality of the role and filler embedding spaces (\(D_{R},D_{F}\) respectively) are properties of the corresponding embedding functions (\(\xi_{R}:R\to V_{R}\) and \(\xi_{F}:F\to V_{F}\)) and thus, can be fixed independently of the number of roles, \(N_{R}\), the number of fillers, \(N_{F}\), or the number of total role-filled bindings (which we denote by \(n\)) within a domain. Thus, it is possible to fix the Soft TPR's dimensionality (\(D_{F}\times D_{R}\)) to be smaller than \(N_{F}\times N_{R}\) (the number of roles/FoV types multiplied by the number of fillers/FoV tokens) or \(n\) (the number of bindings), which all may be large in complex visual domains. As illustrated in Table 46, the dimensionality of the TPR is smaller than \(N_{F}\times N_{R}\) in both the Shapes3D and MPI3D domains.
2. **Relaxing Orthogonality**: While \(D_{F}\) can be set a priori with no regard to \(N_{R},N_{F}\) or \(n\), we require \(D_{R}\geq N_{R}\) for semi-orthogonality of the role-embedding matrix \(M_{\xi_{R}}\), which guarantees faithful (see proof 2 of A.2) and computationally efficient (see 'Unbinding' heading of B.4.1) recoverability of constituents. It is, however, possible to relax this constraint (i.e., to have \(D_{R}<N_{R}\)) to further reduce dimensionality. In this case, semi-orthogonality of \(M_{\xi_{R}}\) is impossible and hence the recoverability of constituents cannot be guaranteed, however, there are some less stringent guarantees on the outcome of unbinding that can still be derived (see p291 of [3] for more details).

We also more explicitly compare the dimensionality of the Soft TPR with baselines in Table 47. Scalar-tokened symbolic representations have a low dimensionality of \(10\) (\(N_{R}\)) at the expense of representational expressivity (each representational constituent \(\psi_{i}(a_{i})\) is scalar-valued). In contrast, Soft TPR has vector-valued representational constituents (i.e. \(\approx\xi_{F}(f_{m(i)})\otimes\xi_{R}(r_{i})\)), similar to VCT and COMET. When compared to these models, the Soft TPR has significantly lower dimensionality compared to VCT and is comparable with COMET.

### Computational Cost

In the Soft TPR Autoencoder, the expensive tensor product operation is employed to generate \(\psi^{*}_{tpr}\). Given the computational cost of the tensor product, we more concretely examine the computational

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{Dataset} \\ \cline{2-4}  & Cars3D & Shapes3D & MPI3D \\ \hline \(D_{R}\) & 12 & 16 & 12 \\ \(N_{R}\) & 10 & 10 & 10 \\ \(D_{F}\) & 128 & 32 & 32 \\ \(N_{F}\) & 106 & 57 & 50 \\ \(D_{F}\cdot D_{R}\) (_TPR dimension_) & 1536 & **512** & **384** \\ \(N_{F}\cdot N_{R}\) & **1060** & 570 & 500 \\ \hline \end{tabular}
\end{table}
Table 46: Comparison of multiplicative dimensionality

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Model} & Cars3D & Shapes3D & MPI3D \\ \cline{2-4}  & \multicolumn{3}{c}{Representational dimension} \\ \hline \multicolumn{4}{c}{Symbolic scalar-tokened compositional representations} \\ \hline SlowVAE & 10 & 10 & 10 \\ Ada-GVAE-k & 10 & 10 & 10 \\ GVAE & 10 & 10 & 10 \\ ML-VAE & 10 & 10 & 10 \\ Shu & 10 & 10 & 10 \\ \hline \multicolumn{4}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & 5120 & 5120 & 5120 \\ COMET & **640** & _640_ & _640_ \\ \hline \multicolumn{4}{c}{Fully continuous compositional representations} \\ \hline Ours (Soft TPR) & _1536_ & **512** & **384** \\ \hline \hline \end{tabular}
\end{table}
Table 47: Comparison of dimensionality of representationscost of training the Soft TPR Autoencoder by computing the FLOPs for a single forward pass on a batch size of 16 using the open-source implementation of fvcore https://github.com/facebookresearch/fvcore/tree/main/docs, visible in Table 48. This data demonstrates that, despite the tensor product's computational cost, the mathematically-informed derivation of our model allows it to obtain compositional representations with vector-valued representational constituents at a significantly lower cost compared to relevant, vector-tokened baselines (2 orders of magnitude less than VCT, and 4 orders of magnitude less than COMET).

Future work could explore the use of tensor contraction techniques to reduce computational expense. For instance [29] uses a Hadamard product based tensor product compression technique. This reduces computational cost from \(n^{2}\) (tensor product of 2 vectors) to \(n\) (Hadamard product), but comprises the theoretical guarantees on constituent recoverability. We believe developing tensor contraction techniques within the TPR framework is an important direction for future research, to ensure efficient TPR-based representations with provable recoverability of constituents.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multirow{2}{*}{Model} & Cars3D & Shapes3D & MPI3D \\ \cline{2-4}  & \multicolumn{4}{c}{Representational dimension} \\ \hline \multicolumn{4}{c}{Symbolic scalar-tokened compositional representations} \\ \hline SlowVAE & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) \\ Ada-GVAE-k & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) \\ GVAE & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) \\ ML-VAE & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) & \(1.47\times 10^{8}\) \\ Shu & \(1.45\times 10^{8}\) & \(1.45\times 10^{8}\) & \(1.45\times 10^{8}\) \\ \hline \multicolumn{4}{c}{Symbolic vector-tokened compositional representations} \\ \hline VCT & \(2.53\times 10^{11}\) & \(2.53\times 10^{11}\) & \(2.53\times 10^{11}\) \\ COMET & \(5.12\times 10^{13}\) & \(5.12\times 10^{13}\) & \(5.12\times 10^{13}\) \\ \hline \multicolumn{4}{c}{Fully continuous compositional representations} \\ \hline Ours (Soft TPR) & \(3.21\times 10^{9}\) & \(2.93\times 10^{9}\) & \(2.89\times 10^{9}\) \\ \hline \hline \end{tabular}
\end{table}
Table 48: Comparison of FLOPs required for a forward pass of batch size 16

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims accurately reflect the paper's contributions and scope. We provide theoretical proofs in the Appendix, as well as our complete suite of experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We explicitly state in the last paragraph of the paper that future work is to be done in developing hierarchical Soft TRP representations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate 'Limitations' section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: We include complete proofs for all theoretical results in the Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all information (specification of model architecture, computing resources, hyperparameters) to replicate experimental results in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All datasets are open-access. We provide sufficient instructions in our Appendix to reproduce experimental results. The code base is also public. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide these details in the Appendix, and additionally, in the code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We present results with standard deviations, as well as the IQR and whiskers extending to 1.5 times the IQR for all box plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer 'Yes' if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialisation, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We specify the specific GPUs we use for all experiments in the Appendix, as well as the average amount of time it takes to train the Soft TPR Autoencoder for each dataset. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: As the central aim of our work relates to the representational form of compositional representations, our research is theoretical in nature and has no immediate societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In the main paper, as well as the Appendix, we cite the papers associated with each dataset we use. We additionally make explicit references in the Appendix to all externally created code that we use in our experiments. Furthermore, if the paper is to be accepted, we will clearly provide licenses and attribution to the original authors where applicable in the code files. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our open access code is reasonably straightforward to understand, especially when supplemented by the Appendix of this paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our research does not involve crowdsourcing nor human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.