# TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration

 Yiwei Guo\({}^{1,2}\) Shaobin Zhuang\({}^{*3,4}\) Kunchang Li\({}^{*1,2,3}\) Yu Qiao\({}^{3}\) Yali Wang\({}^{\dagger 1,3}\)

\({}^{1}\)Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

\({}^{2}\)University of Chinese Academy of Sciences

\({}^{3}\)Shanghai AI Laboratory

\({}^{4}\)Shanghai Jiao Tong University

###### Abstract

Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are "isolated agents" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT which contains large domain shifts. The code will be released at https://github.com/markywg/transagent.

## 1 Introduction

Recently, Vision-Language (V-L) foundation models are mainly pre-trained by contrastive learning with massive image-text pairs from web [61, 38, 15]. As a result, they show the potential on a number of downstream visual recognition tasks, by transferring their representations with prompt learning [87, 86] and/or model adaptation [31, 80]. However, target domain data in the open world are diversified, _e.g._, EuroSAT [35] refers to satellite images that are highly different from web images in the pre-training. With this large domain shift, it is challenging to achieve good generalization only by adopting such a single model (_e.g._, CLIP), especially under low-shot regime. Alternatively, with the fast development in vision and NLP, there arises a wide range of expert models [33, 9, 43, 49, 7, 18, 64, 11, 46, 12] which contain rich knowledge by pre-training on different modalities, tasks, networks, and datasets. Hence, the natural question is, is it possible to integrate such knowledge to boost vision-language foundation models?

To answer this question, we should further analyze the form of knowledge in these models. The simplest form is the model output, which explicitly exhibits what kind of tasks these models can tackle. However, these models have heterogeneous network structures and outputs, making direct knowledge combinations infeasible. Hence, existing works often choose cascades of these models[81, 72, 47], according to their output forms. Apparently, such a design lacks flexibility in transfer learning, based on tool invocation in sequence. Moreover, the resulting pipeline is unfriendly for deployment, due to the ensemble of various models in the inference phase.

Another form is the latent representation which implicitly encodes data knowledge in these models [36, 4, 1]. Compared to the explicit output, this implicit representation has a key advantage, _i.e._, it is the feature vector that has a homogeneous form among different models. In other words, such vectorized knowledge opens the possibility for a unified integration of these heterogeneous agents. Based on this observation, we propose a general **TransAgent** framework in Figure 1. To our best knowledge, it is the _first_ unified distillation framework for generalizing vision-language foundation models with _efficient_ heterogeneous agent collaboration. Compared to the previous works mentioned above, our TransAgent contains three distinct technical contributions.

(1) **Knowledge Versatility**. In our TransAgent, we leverage 11 heterogeneous agents from vision, language and multi-modal research, which comprehensively covers diversified knowledge that is complementary with CLIP-like models, from visual recognition to dense prediction, from chatbot to text encoder, from multi-modal generation to caption.

(2) **Transfer Flexibility**. First, we provide a generic knowledge extraction method for each modality, allowing us to flexibly extend more agents if necessary in the future. Especially for multi-modal agents, we design a novel manner to extract the prediction score vector of classes as multi-modal knowledge in the target domain, via elaborate mining of vision-language alignment in these models. Second, we introduce a mixture-of-agents gating mechanism for integrating external knowledge of different agents in each modality. This allows our TransAgent to automatically select agents via soft weighting so that it can adaptively tackle few-shot settings in different domains of target datasets.

(3) **Deployment Efficiency**. We leverage multi-source distillation to transfer knowledge of these heterogeneous agents into CLIP. Since all these pre-trained models are frozen, the fine-tuning effort is neglectable with a few learnable prompts. More importantly, we can unload all the external agents after distillation, _i.e._, the inference pipeline with the enhanced CLIP is just the same as the original one, achieving deployment efficiency without a heavy model ensemble.

Finally, we conduct extensive experiments on 11 visual recognition benchmarks, where our TransAgent achieves the state-of-the-art under the same low-shot transfer setting, _e.g._, via knowledge collaboration, it outperforms the well-known CoOp [87] with around 10% on average and 20% on EuroSAT which contains large domain shifts. Our method also achieves better results than CaFo [81], which adopts a model ensemble strategy.

Figure 1: **An overview of our TransAgent. (a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains.**

Related Work

**Foundation models.** The rapid advancements in deep learning methods have brought abundant pre-trained models to the research area. We group these models into four categories and further demonstrate their ideas below. **(i) Vision models:** Vision foundation models [33; 9; 3; 59; 32; 13] pre-trained on ImageNet [21] have shown outstanding transfer capability in visual recognition by fine-tuning on downstream datasets. Moreover, various models [43; 49; 16; 17; 8] can be specialists in dense prediction tasks by pre-training on task-relevant domain data. **(ii) Large language models:** The emergence of large language models (LLMs) [7; 18; 69; 70; 27] has been raising increasing attention from the research community and the public. The astonishing comprehension ability of the LLMs is credited to the linguistic knowledge which can be further applied to solve vision tasks [51; 20; 88; 79]. **(iii) Text-to-image generative models:** Text-conditioned generation task requires high-level understanding of the given prompts. Recently, diffusion-based generative models [37; 23; 64; 11; 56; 62; 66] have become the state-of-the-art. These models can follow the text conditions faithfully and generate desired outcomes, owing to the semantic knowledge learned during the pre-training stage. **(iv) Image-to-text captioning models:** These models typically integrate visual knowledge into LLMs to obtain multi-modal understanding abilities [46; 12; 15; 2; 14; 25; 47], offering better experience in referential dialog scenario. In this work, we excavate the underlying knowledge in these heterogeneous models to empower the VL foundation models.

**Few-shot adaptation.** To efficiently transfer vision-language foundation models like CLIP [61] to downstream tasks, researchers have proposed various adaptation methods, which are primarily based on prompt learning [87; 86; 39; 40; 41; 65; 85; 53; 50] or adapter [31; 80; 68; 48; 81; 75; 89]. Lu et al. [53] explore the potential of collaborating CLIP's architectural variants and propose adaptive ensemble strategies to enhance the generalization performance. PromptKD [50] adapts a larger CLIP teacher to downstream datasets and distills the knowledge to a smaller student in an unsupervised manner, separating the need for labeled domain data during transfer. TaskRes [75] proposes to decouple the prior knowledge of the pre-trained models and the task-specific knowledge, enabling reliable old knowledge preservation and flexible new knowledge exploration. GraphAdapter [48] further utilizes the dual-modality structure knowledge for better adaptation in downstream tasks.

**Agent collaboration.** Considering the complementary knowledge of diverse pre-trained models specialized in different domains or tasks, several works have been proposed to solve vision tasks with agent collaboration [74; 82; 72; 52; 71; 28]. The most relevant to our work is CaFo [81], which transfers the external knowledge using cache models [80]. However, such an ensemble manner introduces further costs in the inference stage. On the contrary, we adopt heterogeneous agent collaboration to aggregate the knowledge, and distillation strategy to inject the knowledge into CLIP, which demonstrates better performance and guarantees deployment efficiency.

**Multi-teacher distillation.** To improve the effectiveness of knowledge distillation [36], recent works [10; 30; 26; 76; 77; 78; 83; 54; 63] attempt to integrate the knowledge from multiple teacher networks. To be noted, how to perform multi-teacher distillation is not trivial, and how to extract and collaborate knowledge from various heterogeneous teachers has not been fully explored for CLIP-like foundation models. In this work, we devise a generic knowledge extraction method and flexible knowledge collaboration mechanism to further enhance the generalization ability of VL foundation models.

## 3 Method

In this section, we introduce our TransAgent in detail. As shown in Figure 1, it consists of vision-language foundation models and agents from different modalities. In the following section, we will illustrate how to collaborate with them for downstream visual recognition tasks. To start with, we briefly review the V-L foundation models by using the well-known CLIP [61]. Specifically, CLIP consists of two branches. In the vision branch, an image is first divided into \(L\) non-overlapping equal-sized patches and then projected as input visual tokens \(\mathbf{V}_{in}\), which are fed to the vision encoder to obtain image feature. In the language branch, a text description is projected as input textual tokens \(\mathbf{T}_{in}\) which are then processed by the text encoder to generate text feature. Through contrastive learning over massive image-text pairs, CLIP achieves good alignment between the two modalities.

More interestingly, such large-scale V-L models show the potential in visual classification on the downstream tasks. By converting each class label into a text template such as "a photo of a {classname)", these models can easily achieve zero-shot inference. To further enhance their generalization ability under the few-shot settings, prompt learning methods are proposed, which introduce a number of learnable prompts while freezing the pre-trained CLIP [87, 86, 39, 40, 41, 45]. Following previous works, we add a set of learnable textual prompts \(\mathbf{P}_{T}\in\mathbb{R}^{N_{ctx}\times C}\), where \(N_{ctx}\) is the number of learnable prompts, in the language branch and concatenate them with the textual tokens, which are then processed by the text encoder to obtain the prompted textual feature \(\mathbf{T}\in\mathbb{R}^{N_{cls}\times C}\) for all \(N_{cls}\) categories. Similarly, a set of learnable visual prompts \(\mathbf{P}_{V}\) are inserted in the image branch to generate the prompted visual feature \(\mathbf{V}\in\mathbb{R}^{N\times C}\) where \(N\) denotes the number of images:

\[\mathbf{T}=\text{TextEncoder}(\mathbf{T}_{in},\mathbf{P}_{T}),\quad\quad \mathbf{V}=\text{VisionEncoder}(\mathbf{V}_{in},\mathbf{P}_{V}).\] (1)

Consequently, we compute the prediction score vectors \(\mathbf{S}=\{\mathbf{S}^{c}\}\) for the image samples, where \(\mathbf{S}^{c}\) is the cosine similarity between the visual feature \(\mathbf{V}\) and the textual feature \(\mathbf{T}^{c}\) of specific class \(c\). As a result, we minimize the cross entropy loss between the score vectors and the ground truth labels:

\[\mathcal{L}_{\text{CE}}=\text{CrossEntropy}(\text{softmax}(\mathbf{S}), \mathbf{Y}),\] (2)

to fine-tune the learnable prompts \(\mathbf{P}_{T}\) and \(\mathbf{P}_{V}\) for visual recognition. However, as mentioned in the introduction, it is difficult to achieve good generalization by adapting such a single CLIP model, especially when the domain shift of the target datasets is large. Hence, we propose to transfer diversified knowledge from heterogeneous agents in different modalities for better adaption of CLIP.

### Vision Agent Collaboration (VAC)

One important component of CLIP is its vision branch in Figure 2. Hence, we consider to enhance this branch by transferring visual knowledge from various vision agents. To achieve this goal, we have to answer three critical questions. The first question is which models should be used for collaboration. As we know, CLIP mainly establishes image-level alignment between the two modalities, neglecting visual details in the pixel space. To fill this gap, we choose vision agents from two aspects. On one aspect, we choose vision models pre-trained with self-supervision such as MAE [33] and DINO [9]. Both agents focus on detailed image modeling via image masking [33] or patch self-distillation [9]. On the other aspect, we choose vision models built on dense supervision such as ViTDet [49] and SAM [43]. Both agents work on instance-level prediction with bounding boxes and masks. Detailed information on these models can be found in the supplementary.

The second question is how to extract visual knowledge by collaborating with these agents. As mentioned in the introduction, the latent feature is a common knowledge form among these heterogeneous models. Hence, given an input image, we extract the intermediate visual features

Figure 2: **Vision Agent Collaboration and Language Agent Collaboration.** (a) VAC integrates visual knowledge via MoA gating and transfers the knowledge through layer-wise feature distillation. (b) LAC enhances the textual representations through class-specific feature distillation between the prompted textual feature and the gated textual feature.

\(\{\mathbf{V}_{A}(i)\in\mathbb{R}^{N\times C}\}\) from the vision encoders of these agents. Moreover, the contribution of different agents may vary among different domains. To fully exploit these agents in a unified manner, we introduce a Mixture-of-Agents (MoA) gating mechanism to adaptively integrate \(\{\mathbf{V}_{A}(i)\}\) as the visual knowledge. Specifically, we concatenate all the agent features along the channel dimension \(C\), and feed them into an MLP network to generate the gating weight \(\mathbf{W}_{V}\). Next, we obtain the gated visual features \(\mathbf{V}_{A}\) by computing the weighted sum over \(\{\mathbf{V}_{A}(i)\}\):

\[\mathbf{W}_{V}=\text{MLP}(\text{Concat}(\{\mathbf{V}_{A}(i)\})),\ \ \ \ \ \mathbf{V}_{A}=\sum\nolimits_{i}\mathbf{W}_{V}(i)\mathbf{V}_{A}(i).\] (3)

The final question is how to transfer the visual knowledge to enhance CLIP's vision encoder. We adopt feature distillation [36] where we compute the L1 loss between the prompted visual features \(\mathbf{V}\) in Eq. 1 and the gated visual features of the vision agents:

\[\mathcal{L}_{\text{VAC}}=|\mathbf{V}-\mathbf{V}_{A}|.\] (4)

Since the original CLIP is frozen, the above loss term allows us to fine-tune the learnable visual prompts \(\mathbf{P}_{V}\) in Eq. 1 and MLP gating network in Eq. 3, which enables us to adaptively transfer external visual knowledge to the visual prompts to empower generalization ability.

### Language Agent Collaboration (LAC)

The other important component of CLIP is its language branch in Figure 2. Similar to the vision branch, we consider three critical questions to transfer the textual knowledge from various language agents. Recent studies [60; 42] have shown that it is coarse to use a simple template (_e.g._, "a photo of a {class name}") to describe a certain category. Hence, we first interact with the popular chatbots such as GPT-3 [7] and Vicuna [18] to enrich the class descriptions using queries like "What does a {class name} look like?". After obtaining the detailed descriptions from these chatbots, we use a text encoder (_e.g._, BERT [22]) to extract the text features \(\{\mathbf{T}_{A}(j)\in\mathbb{R}^{N_{elj}\times C}\}\) of all descriptions. To adaptively integrate \(\{\mathbf{T}_{A}(j)\}\) as the textual knowledge, we also utilize the MoA gating mechanism:

\[\mathbf{W}_{T}=\text{MLP}(\text{Concat}(\{\mathbf{T}_{A}(j)\})),\ \ \ \ \ \mathbf{T}_{A}=\sum\nolimits_{j}\mathbf{W}_{T}(j)\mathbf{T}_{A}(j).\] (5)

Finally, for each category, we perform feature distillation where we compute the L1 loss between the prompted textual feature in Eq. 1 and the gated textual feature from language agents in Eq. 5:

\[\mathcal{L}_{\text{LAC}}=|\mathbf{T}-\mathbf{T}_{A}|.\] (6)

By fine-tuning the learnable textual prompts \(\mathbf{P}_{T}\) in Eq. 1 and MLP gating network in Eq. 5, we adaptively transfer the rich textual knowledge to enhance CLIP's textual representations.

### Multi-modal Agent Collaboration (MAC)

Through vision and language agent collaboration, we can enhance the learnable visual and textual prompts respectively. Considering the key success in vision-language foundation models is credited to the multi-modal alignment, we investigate how to further align the visual and textual prompts with external multi-modal agents. First, there exist two types of multi-modal agents, including Text-to-Image (T2I) generative models [64; 11] and Image-to-Text (I2T) captioning models [46; 12]. Since both types of agents involve conversion from one modality to the other, we believe they implicitly achieve vision-language alignment. Based on the above discussion, we choose our T2I agents built upon two mainstream structures including Stable Diffusion [64] in UNet style and PixArt-\(\alpha\)[11] in DiT style. Moreover, we choose our I2T agents specialized in two mainstream tasks including BLIP-2 [46] for general captioning and Shikra [12] for grounded dialogue.

Next, we consider what kind of knowledge can represent vision-language alignment. The most direct form may be the probability score vector that shows the prediction confidence over target classes. Hence, for each training image, we investigate such knowledge in these multi-modal agents.

In the T2I agents, cross attention effectively encodes relations between image and text [34; 84]. Hence, we leverage such prior to extract the score vector. Specifically, we use the template description of a class \(c\) as text. Then, we extract cross attention value \(\mathbf{M}_{k}^{c}\) between the text and the \(k\)-th token of the input image from the pre-trained T2I agents. Consequently, we sum over all the tokens to obtain the prediction score of the image _w.r.t._ class \(c\): \(\mathbf{S}_{T2I}^{c}=\log(\sum_{k}\exp(\mathbf{M}_{k}^{c}))\), where we adopt LogSumExp (LSE) pooling [5] to provide more accurate matching scores.

In the I2T agents, there exists a projection module (_e.g._, Q-Former in BLIP-2 [46] and MLP in Shikra [12]) to adapt the visual features for the large language model. Hence, we extract the visual feature of an input image via the projection and the textual features of all the classes from the LLM. By computing the cosine similarity between them, we can obtain the prediction score \(\mathbf{S}_{I2T}\) of the image over all the classes. Next, we leverage the MoA gating mechanism to adaptively summarize the prediction score vectors from all the multi-modal agents \(\mathbf{M}_{A}=\text{Concat}(\{\mathbf{S}_{T2I},\mathbf{S}_{I2T}\})\):

\[\mathbf{W}_{S}=\text{MLP}(\mathbf{M}_{A}),\hskip 14.226378pt\mathbf{S}_{A}= \sum\nolimits_{n}\mathbf{W}_{S}(n)\mathbf{M}_{A}(n).\] (7)

Finally, we explore how to transfer \(\mathbf{S}_{A}\) to enhance the learnable prompts in CLIP. Specifically, after being processed by the vision and text encoders of CLIP, the learnable visual and textual prompts \(\mathbf{P}_{V}\) and \(\mathbf{P}_{T}\) are transformed into \(\mathbf{Q}_{V}\) and \(\mathbf{Q}_{T}\). Unlike \(\mathbf{P}_{V}\) and \(\mathbf{P}_{T}\) which are universal, \(\mathbf{Q}_{V}\) is relevant to the image samples and \(\mathbf{Q}_{T}\) is relevant to the target classes. Hence, we can compute the cosine similarity between \(\mathbf{Q}_{V}\) and \(\mathbf{Q}_{T}\) to obtain the learned score vectors \(\mathbf{S}_{P}\in\mathbb{R}^{N\times N_{cls}}\) of the input images over all the classes. We perform score distillation between \(\mathbf{S}_{P}\) and \(\mathbf{S}_{A}\):

\[\mathcal{L}_{\text{MAC}}=\text{KL}(\text{softmax}(\mathbf{S}_{P})||\text{ softmax}(\mathbf{S}_{A})).\] (8)

Through computing the KL divergence, we can leverage external multi-modal knowledge \(\mathbf{S}_{A}\) as a semantic guidance to align the learnable visual and textual prompts.

### Multi-Source Knowledge Distillation

Finally, we combine all the distillation loss from multiple sources, achieving heterogeneous agent collaboration for knowledge transfer:

\[\mathcal{L}_{\text{TransAgent}}=\mathcal{L}_{\text{CE}}+\lambda_{1}\mathcal{L }_{\text{VAC}}+\lambda_{2}\mathcal{L}_{\text{LAC}}+\lambda_{3}\mathcal{L}_{ \text{MAC}},\] (9)

Figure 3: **Multi-modal Agent Collaboration.**_Top left:_ We first extract the cross attention maps from the T2I agents and then obtain the score vectors through LSE pooling. _Top right:_ We compute the score vectors from the I2T agents as the cosine similarity between the projected visual feature and the LLM’s textual feature. Finally, we perform score distillation between the learned score vectors and the gated score vectors to further align the learnable prompts.

where \(\lambda_{1}\), \(\lambda_{2}\), \(\lambda_{3}\) are hyperparameters. Since all the pre-trained models are frozen in the training phase, we only need to fine-tune the learnable vision and language prompts with negligible cost. More importantly, owing to the distillation strategy, all the agents can be unloaded and the modality-specific gates can be abandoned in the inference phase. We can simply use the enhanced CLIP just like the original one, which largely boosts deployment efficiency without a model ensemble.

## 4 Experiments

**Datasets and Metrics.** We evaluate our proposed method on 11 commonly used datasets covering a wide range of recognition tasks, including ImageNet [21], Caltech101 [29], OxfordPets [58], StanfordCars [44], Flowers102 [57], Food101 [6], FGVCAircraft [55], SUN397 [73], UCF101 [67], DTD [19] and EuroSAT [35]. We explore two typical low-shot scenarios to evaluate the performance. (i) Base-to-novel generalization: The datasets are equally split into base and novel classes. The model is trained on base classes and evaluated on the test set of both classes. We report the base and novel class accuracy and the harmonic mean (HM) of the results. (ii) Few-shot classification: We assess the accuracy trained with 1/2/4/8/16 shot(s) per class to examine the model's learning capacity.

**Implementation Details.** We adopt CLIP ViT/B-16 as our backbone and conduct all experiments using 3 different seeds to obtain an averaged result, following previous works [40; 41; 50]. Our method ensembles knowledge from heterogeneous agents, including pre-trained vision models, LLMs, T2I generative models, and I2T captioning models. Detailed information on the agents and training settings are shown in Appendix A.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**Average**} & \multicolumn{3}{c}{**ImageNet [21]**} & \multicolumn{3}{c}{**Caltech101 [29]**} & \multicolumn{3}{c}{**OxfordPets [58]**} \\ \cline{2-13}  & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\ \hline CLIP [61] & 69.34 & 74.22 & 71.70 & 72.43 & 68.14 & 70.22 & 96.84 & 94.00 & 95.40 & 91.17 & 97.26 & 94.12 \\ CoOp [87] & 82.69 & 63.22 & 71.66 & 76.47 & 67.88 & 71.92 & 98.00 & 89.81 & 93.73 & 93.67 & 95.29 & 94.47 \\ CoCoOp [86] & 80.47 & 71.69 & 75.83 & 75.98 & 70.43 & 73.10 & 97.96 & 93.81 & 95.84 & 95.20 & 97.69 & 96.43 \\ MapLe [40] & 82.28 & 75.14 & 78.55 & 75.40 & 70.32 & 72.72 & 98.27 & 93.23 & 95.68 & 95.43 & 97.83 & 96.62 \\ RPO [45] & 81.13 & 75.00 & 77.78 & 76.60 & **71.57** & 74.00 & 97.97 & 94.37 & 96.03 & 94.63 & 97.50 & 96.05 \\ PromptSRC [41] & 84.26 & 76.10 & 79.97 & 77.60 & 70.73 & 74.01 & 98.10 & 94.03 & 96.02 & 95.33 & 97.30 & 96.30 \\ \hline
**TransAgent** & **85.29** & **77.62** & **81.27** & **78.07** & 70.57 & **74.13** & **98.90** & **95.23** & **97.03** & **96.33** & **98.13** & **97.22** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**StanfordCars [44]**} & \multicolumn{3}{c}{**Flowers102**} & \multicolumn{3}{c}{**Food101 [6]**} & \multicolumn{3}{c}{**FGVCAircraft [55]**} \\ \cline{2-13}  & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\ \hline CLIP [61] & 63.37 & 74.89 & 68.65 & 72.08 & 77.80 & 74.83 & 90.10 & 91.22 & 90.66 & 27.19 & 36.29 & 31.09 \\ CoOp [87] & 78.12 & 60.40 & 68.13 & 97.60 & 59.67 & 74.06 & 88.33 & 82.26 & 85.19 & 40.44 & 22.30 & 28.75 \\ CoCoOp [86] & 70.49 & 73.59 & 72.01 & 94.87 & 71.75 & 81.71 & 90.70 & 91.29 & 90.99 & 33.41 & 23.71 & 27.74 \\ MaPLe [40] & 74.70 & 71.20 & 72.91 & 97.70 & 68.68 & 80.66 & 90.30 & 88.57 & 89.43 & 36.90 & 34.13 & 35.46 \\ RPO [45] & 73.87 & **75.53** & 74.69 & 94.13 & 76.67 & 84.50 & 90.33 & 90.83 & 90.58 & 37.33 & 34.20 & 35.70 \\ PromptSRC [41] & 78.27 & 74.97 & 76.58 & 98.07 & 76.50 & 85.95 & 90.67 & 91.53 & 91.10 & 42.73 & 37.87 & 40.15 \\ \hline
**TransAgent** & **79.53** & 74.73 & **77.06** & **98.37** & **77.13** & **86.46** & **90.87** & **92.20** & **91.53** & **43.77** & **39.00** & **41.25** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**SUN397**} & \multicolumn{3}{c}{**DTD [19]**} & \multicolumn{3}{c}{**EuroSAT [35]**} & \multicolumn{3}{c}{**UCF101 [67]**} \\ \cline{2-13}  & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM & Base & Novel & HM \\ \hline CLIP [61] & 69.36 & 75.35 & 72.23 & 53.24 & 59.90 & 56.37 & 56.48 & 64.05 & 60.03 & 70.53 & 77.50 & 73.85 \\ CoOp [87] & 80.60 & 65.89 & 72.51 & 79.44 & 41.18 & 54.24 & 92.19 & 54.74 & 68.69 & 84.69 & 56.05 & 67.46 \\ CoCoOp [86] & 79.74 & 76.86 & 78.27 & 77.01 & 56.00 & 64.85 & 87.49 & 60.04 & 71.21 & 82.33 & 73.45 & 77.64 \\ MaPLe [40] & 78.47 & 76.93 & 77.79 & 80.67 & 56.48 & 66.44 & 83.90 & 66.00 & 73.88 & 85.23 & 71.97 & 78.04 \\ RPO [45] & 80.60 & 77.80 & 79.18 & 76.70 & 62.13 & 68.61 & 86.63 & 68.97 & 76.79 & 83.67 & 75.43 & 79.34 \\ PromptSRC [41] & 82.67 & 78.47 & 80.52 & 83.37 & 62.97 & 71.75 & 92.90 & 73.90 & 82.32 & 87.10 & 78.80 & 82.74 \\ \hline
**TransAgent** & **82.90** & **79.30** & **81.06** & **84.37** & **63.67** & **72.57** & **97.43** & **83.43** & **89.89** & **87.60** & **80.47** & **83.88** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Accuracy comparison with state-of-the-art methods on base-to-novel generalization. All methods use CLIP’s ViT-B/16 as the vision encoder. Our TransAgent exhibits strong generalization ability and outperforms previous SOTA on all datasets. The best results are **bolded**.

### Comparison with State-of-the-Art

In Table 1, we compare the base-to-novel generalization performance of our proposed TransAgent with state-of-the-art prompt learning methods, including CoOp [87], CoCoOp [86], MaPLe [40], RPO [45] and PromptSRC [41]. All approaches use the same CLIP ViT-B/16 during the evaluation stage. Our method demonstrates superior performance across 11 datasets, surpassing all competing methods in terms of base and novel accuracy, as well as the harmonic mean, particularly excelling on the EuroSAT dataset. In Figure 4, we present the few-shot classification performance of TransAgent and previous methods. Our method consistently outperforms the counterparts and demonstrates growing learning capability when the training samples increase. To be mentioned, TransAgent outperforms CaFo [81] with much lower deployment costs (see Table 13). Detailed comparisons on cross-dataset evaluation and domain generalization are provided in Appendix B.

### Ablative Analysis

In this section, we present the ablative analysis of our agent collaboration designs for each modality. We adopt IVLP [40, 41] as our baseline model.

Figure 4: **Accuracy comparison in few-shot classification.** TransAgent demonstrates _state-of-the-art_ performance for all few-shot settings on different datasets, which proves promising learning capability even under extremely limited supervision.

**Effectiveness of individual agent.** In Table 2 - Table 4, we show the results of introducing an individual agent as a teacher to supervise the baseline model in the beginning rows. Nearly all vision agents contribute to the improvement in accuracy, except for DINO which is behind. However, we observe that DINO performs well on most datasets, except on EuroSAT (with a 5.60% decline). Nonetheless, we still keep DINO as one of the vision agents. While for the language and the multi-modal agents, they all boost the performance of the baseline model respectively.

**Distillation strategy.** In the middle rows of Table 2 - Table 4, we ablate the alignment strategy for agent collaboration of each modality. (i) For VAC, we choose to adopt feature distillation with either the average-pooled feature at the last layer or all features at all layers. As the results suggest, layer-wise distillation of all features performs better. (ii) For LAC, two special tokens from the output of the text encoder are considered to compute Equation 6. These tokens are inserted into the text sequence to compose a complete prompt. \([EOS]\) performs better owing to the causal attention module in Transformer blocks, so that it aggregates more information. (iii) For MAC, either the prompted logits of CLIP or the learned scores (Eq. 8) are chosen to align with the semantic logits from multi-modal agents. Since the prompted logits need to be aligned with ground-truth labels as well (Eq. 2), there might be confusion to align it with the external knowledge simultaneously. Experimental results show that using learned scores to align with semantic logits yields better results.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**VAC** & **LAC** & **MAC** & **HM** & \(\Delta\) \\ \hline ✓ & ✓ & ✗ & 80.02 & +2.51 \\ ✓ & ✗ & ✓ & 79.79 & +2.28 \\ ✗ & ✓ & ✓ & 80.40 & +2.89 \\ ✓ & ✓ & ✓ & 81.27 & +3.76 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Performance w/ module combinations.**

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Models** & **Base** & **Novel** & **HM** \\ \hline baseline & 84.21 & 71.79 & 77.51 \\ \hline GAPT-3 & 85.15 & 74.55 & 79.50 \\ Vicuna & 85.35 & 74.70 & 79.67 \\ ViTDet & 84.45 & 72.04 & 77.75 \\ \hline Last-layer & 84.47 & 75.92 & 79.97 \\ Layer-wise & 85.29 & 77.62 & 81.27 \\ \hline Average & 84.20 & 74.79 & 79.22 \\ Add & 84.40 & 75.17 & 79.52 \\ Gating & 85.29 & 77.62 & 81.27 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **VAC Design.**

Figure 5: **Averaged gating weights of each agent on different datasets.** Deeper color indicates more contributions to the gated feature(s) or score vectors.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Models** & **Base** & **Novel** & **HM** \\ \hline baseline & 84.21 & 71.79 & 77.51 \\ \hline MAE & 84.51 & 71.66 & 77.55 \\ DINO & 84.60 & 70.90 & 77.15 \\ SAM & 84.73 & 71.87 & 77.77 \\ ViTDet & 84.45 & 72.04 & 77.75 \\ \hline Last-layer & 84.47 & 75.92 & 79.97 \\ Layer-wise & 85.29 & 77.62 & 81.27 \\ \hline Average & 84.20 & 74.79 & 79.22 \\ Add & 84.40 & 75.17 & 79.52 \\ Gating & 85.29 & 77.62 & 81.27 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **LAC Design.**

**Fusion design.** In the last rows of Table 2 - Table 4, we ablate the fusion design for each modality. "Average" refers to simply calculating the average of all output features from the agents along the channel dimension; "Add" refers to calculating the distillation loss separately for each agent; "Gating" denotes our proposed MoA gating mechanism. As can be seen, gating fusion achieves the best results for all collaboration designs, since it adaptively selects the useful information from the agents.

**Effectiveness of collaboration module(s).** As presented in Table 5, the first row indicates the baseline model. Each individual module is beneficial for improving the generalization ability of the foundation models, where the last column shows the relative increase. Results in Table 6 demonstrate that the combinations of collaboration modules further boost the results.

### Visualization

We calculate the averaged gating weights of each agent with 16-shot training samples from all classes of a certain dataset and present the results with heat maps. As shown in Figure 5, different agents do not contribute equally towards certain target datasets, which also verifies the superiority of our gating fusion design. For vision agents, DINO experts in recognizing general objects while lags behind on some fine-grained datasets (_e.g._, EuroSAT), whereas the others that focus more on details perform better. Language agents provide domain-specific linguistic knowledge accordingly. I2T agents consistently provide their knowledge thanks to the grounding nature, while T2I agents demonstrate better capability in fine-grained scenarios.

## 5 Conclusion

We propose TransAgent, a unified framework to transfer vision-language foundation models through heterogeneous agent collaboration. By adaptively integrating the external knowledge of agents from different modalities viac the MoA gating mechanism, TransAgent achieves _state-of-the-art_ performance on 11 datasets under the low-shot scenarios.

**Limitations.** Although TransAgent collaborates with heterogeneous agents in a unified manner, transferring the external knowledge through distillation may harm the original CLIP's representations even using prompt learning methods because the domain knowledge from agents are diversified, and irrelevant information may also be introduced. Moreover, one of the key characteristics of these agents is large-scale pre-training, few-shot scenarios may not meet their data-hungry nature. So our future work would be integrating the knowledge with directed focus and further unleashing the potential of the agents even when the domain-specific knowledge (_e.g._, labels) is absent.

## Acknowledgments and Disclosure of Funding

This work was supported in part by the National Key R&D Program of China (NO.2022ZD0160505), the National Natural Science Foundation of China under Grant (62272450)

## References

* [1] R. Adriana, B. Nicolas, K. S. Ebrahimi, C. Antoine, G. Carlo, and B. Yoshua. Fitnets: Hints for thin deep nets. In _International conference on learning representations_, 2015.
* [2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in neural information processing systems_, 35:23716-23736, 2022.
* [3] H. Bao, L. Dong, S. Piao, and F. Wei. Beit: Bert pre-training of image transformers. _arXiv preprint arXiv:2106.08254_, 2021.
* [4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. _IEEE transactions on pattern analysis and machine intelligence_, 35(8):1798-1828, 2013.
* [5] P. Blanchard, D. J. Higham, and N. J. Higham. Accurately computing the log-sum-exp and softmax functions. _IMA journal of numerical analysis_, 41(4):2311-2330, 2021.

* [6] L. Bossard, M. Guillaumin, and L. V. Gool. Food-101-mining discriminative components with random forests. In _European conference on computer vision_, pages 446-461. Springer, 2014.
* [7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In _European conference on computer vision_, pages 213-229. Springer, 2020.
* [9] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, October 2021.
* [10] Y. Chebotar and A. Waters. Distilling knowledge from ensembles of neural networks for speech recognition. In _Interspeech_, pages 3439-3443, 2016.
* [11] J. Chen, Y. Jincheng, G. Chongjian, L. Yao, E. Xie, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li. Pixart-\(\alpha\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. In _International conference on learning representations_, 2024.
* [12] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.
* [13] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. In _Proceedings of the international conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [14] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.
* [15] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.
* [16] B. Cheng, A. Schwing, and A. Kirillov. Per-pixel classification is not all you need for semantic segmentation. _Advances in neural information processing systems_, 34:17864-17875, 2021.
* [17] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1290-1299, 2022.
* [18] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impression gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
* [19] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3606-3613, June 2014.
* [20] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [21] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 248-255. IEEE, 2009.

* [22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [23] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International conference on learning representations_, 2021.
* [25] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.
* [26] S. Du, S. You, X. Li, J. Wu, F. Wang, C. Qian, and C. Zhang. Agree to disagree: Adaptive ensemble knowledge distillation in gradient space. _Advances in neural information processing systems_, 33:12345-12355, 2020.
* [27] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. GIm: General language model pretraining with autoregressive blank infilling. _arXiv preprint arXiv:2103.10360_, 2021.
* [28] X. Fan, T. Ji, C. Jiang, S. Li, S. Jin, S. Song, J. Wang, B. Hong, L. Chen, G. Zheng, et al. Mousi: Poly-visual-expert vision-language models. _arXiv preprint arXiv:2401.17221_, 2024.
* [29] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _IEEE/CVF conference on computer vision and pattern recognition workshop_, pages 178-178. IEEE, 2004.
* [30] T. Fukuda, M. Suzuki, G. Kurata, S. Thomas, J. Cui, and B. Ramabhadran. Efficient knowledge distillation from an ensemble of teachers. In _Interspeech_, pages 3697-3701, 2017.
* [31] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. _International journal of computer vision_, 132(2):581-595, 2024.
* [32] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* [33] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009, June 2022.
* [34] X. He, W. Feng, T.-J. Fu, V. Jampani, A. Akula, P. Narayana, S. Basu, W. Y. Wang, and X. E. Wang. Discriminative diffusion models as few-shot vision and language learners. _arXiv preprint arXiv:2305.10722_, 2023.
* [35] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE journal of selected topics in applied earth observations and remote sensing_, 12(7):2217-2226, 2019.
* [36] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [37] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [38] C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _Proceedings of the international conference on machine learning_, volume 139, pages 4904-4916. PMLR, 2021.
* [39] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim. Visual prompt tuning. In _European conference on computer vision_, pages 709-727, 2022.

* [40] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan. Maple: Multi-modal prompt learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 19113-19122, June 2023.
* [41] M. U. Khattak, S. T. Wasim, M. Naseer, S. Khan, M.-H. Yang, and F. S. Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 15190-15200, October 2023.
* [42] M. U. Khattak, M. F. Naeem, M. Naseer, L. Van Gool, and F. Tombari. Learning to prompt with text only supervision for vision-language models. _arXiv preprint arXiv:2401.02418_, 2024.
* [43] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick. Segment anything. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 4015-4026, October 2023.
* [44] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE/CVF international conference on computer vision workshop_, pages 554-561, December 2013.
* [45] D. Lee, S. Song, J. Suh, J. Choi, S. Lee, and H. J. Kim. Read-only prompt optimization for vision-language few-shot learning. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1401-1411, October 2023.
* [46] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _Proceedings of the international conference on machine learning_, volume 202, pages 19730-19742. PMLR, 23-29 Jul 2023.
* [47] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao. Videochat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_, 2023.
* [48] X. Li, D. Lian, Z. Lu, J. Bai, Z. Chen, and X. Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. _Advances in neural information processing systems_, 36, 2024.
* [49] Y. Li, H. Mao, R. Girshick, and K. He. Exploring plain vision transformer backbones for object detection. In _European conference on computer vision_, pages 280-296, 2022.
* [50] Z. Li, X. Li, X. Fu, X. Zhang, W. Wang, and J. Yang. Promptkd: Unsupervised prompt distillation for vision-language models. In _Proceedings of the IEEE/CVF international conference on computer vision_, 2024.
* [51] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [52] M. Liu, S. Roy, W. Li, Z. Zhong, N. Sebe, and E. Ricci. Democratizing fine-grained visual recognition with large language models. In _International conference on learning representations_, 2023.
* [53] Z. Lu, J. Bai, X. Li, Z. Xiao, and X. Wang. Beyond sole strength: Customized ensembles for generalized vision-language models. _arXiv preprint arXiv:2311.17091_, 2023.
* [54] Z. Ma, J. Dong, S. Ji, Z. Liu, X. Zhang, Z. Wang, S. He, F. Qian, X. Zhang, and L. Yang. Let all be whitened: Multi-teacher distillation for efficient visual retrieval. In _Proceedings of the AAAI conference on artificial intelligence_, volume 38, pages 4126-4135, 2024.
* [55] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.
* [56] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [57] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In _Indian conference on computer vision, graphics & image processing_, pages 722-729. IEEE, 2008.

* [58] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3498-3505. IEEE, 2012.
* [59] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. arxiv 2022. _arXiv preprint arXiv:2208.06366_, 2022.
* [60] S. Pratt, I. Covert, R. Liu, and A. Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 15691-15701, October 2023.
* [61] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of the international conference on machine learning_, volume 139 of _PMLR_, pages 8748-8763. PMLR, 18-24 Jul 2021.
* [62] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* [63] M. Ranzinger, G. Heinrich, J. Kautz, and P. Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12490-12500, 2024.
* [64] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, June 2022.
* [65] S. Roy and A. Etemad. Consistency-guided prompt learning for vision-language models. In _International conference on learning representations_, 2024.
* [66] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.
* [67] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* [68] Y.-L. Sung, J. Cho, and M. Bansal. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5227-5237, 2022.
* [69] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [70] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [71] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. _arXiv preprint arXiv:2403.10517_, 2024.
* [72] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_, 2023.
* [73] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* [74] P. Ye, C. Huang, M. Shen, T. Chen, Y. Huang, Y. Zhang, and W. Ouyang. Merging vision transformers from different tasks and domains. _arXiv preprint arXiv:2312.16240_, 2023.

* [75] T. Yu, Z. Lu, X. Jin, Z. Chen, and X. Wang. Task residual for tuning vision-language models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10899-10909, 2023.
* [76] F. Yuan, L. Shou, J. Pei, W. Lin, M. Gong, Y. Fu, and D. Jiang. Reinforced multi-teacher selection for knowledge distillation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 14284-14291, 2021.
* [77] H. Zhang, D. Chen, and C. Wang. Confidence-aware multi-teacher knowledge distillation. In _IEEE international conference on acoustics, speech and signal processing_, pages 4498-4502. IEEE, 2022.
* [78] H. Zhang, D. Chen, and C. Wang. Adaptive multi-teacher knowledge distillation with meta-learning. In _IEEE international conference on multimedia and expo_, pages 1943-1948. IEEE, 2023.
* [79] H. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint arXiv:2306.02858_, 2023.
* [80] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In _European conference on computer vision_, pages 493-510. Springer, 2022.
* [81] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, and H. Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15211-15222, 2023.
* [82] S. Zhang, D. Huang, J. Deng, S. Tang, W. Ouyang, T. He, and Y. Zhang. Agent3d-zero: An agent for zero-shot 3d understanding. _arXiv preprint arXiv:2403.11835_, 2024.
* [83] S. Zhao, X. Wang, and X. Wei. Mitigating accuracy-robustness trade-off via balanced multi-teacher adversarial distillation. _IEEE transactions on pattern analysis and machine intelligence_, (01):1-14, 2024.
* [84] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu. Unleashing text-to-image diffusion models for visual perception. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 5729-5739, October 2023.
* [85] Z. Zheng, J. Wei, X. Hu, H. Zhu, and R. Nevatia. Large language models are good prompt learners for low-shot image classification. In _Proceedings of the IEEE/CVF international conference on computer vision_, 2024.
* [86] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Conditional prompt learning for vision-language models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16816-16825, June 2022.
* [87] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. _International journal of computer vision_, 130:2337-2348, July 2022.
* [88] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.
* [89] X. Zhu, R. Zhang, B. He, A. Zhou, D. Wang, B. Zhao, and P. Gao. Not all features matter: Enhancing few-shot clip with adaptive prior refinement. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 2605-2615, 2023.

## Appendix A Additional Implementation Details

Training Details.The number of learnable vision and language prompt tokens are both set to 4, and the prompt depth is set to 9 for base-to-novel generalization and few-shot classification, and 3 for cross-dataset and domain generalization. The learnable text prompts of the first layer are initialized with the word embeddings of "a photo of a", while the other learnable prompts are randomly initialized with a normal distribution. For vision agent collaboration, we adopt online distillation where the models are loaded during the training process. For language and multi-modal agent collaboration, due to the huge model parameters, we extract their knowledge offline before our training launches. For few-shot classification, we train the models for 50 epochs under different low-shot settings (ranging from 1 to 16). We re-implement the compared methods in Figure 4 with Vision Transformer [24] backbone for fair comparison. For the other benchmarks, we observe that PromptKD [50] adopts a transductive setting which uses all training samples in an unsupervised manner and obtains better results. However, such a setting is beyond our scope. In our work, all models are trained for 20 epochs using 16-shot samples with a fixed batch size of 4 and a learning rate of 0.0025 with SGD as the optimizer. We set \(\lambda_{1}=1\), \(\lambda_{2}=25\) and \(\lambda_{3}=1\) in Eq. 9 after extensive hyperparameter search to balance the total loss. The memory and training time needed under different settings on each dataset are provided in Table 8 using a batch size of 4. At least 48GB is required to extract knowledge from language and multi-modal agents. All experiments are conducted on a single Nvidia A6000 GPU.

Information of agents.Table 7 lists all the agents we have collaborated in our TransAgent framework. (i) Vision agents: DINO provides robust representations through image contrastive (IC) learning while MAE presents powerful capability via mask image modeling (MIM). SAM and ViT-Det are specialists in image segmentation (IS) and object detection (OD) respectively. (ii) Language agents: Both GPT-3 and Vicuna are excellent chatbots that faithfully follow human instructions and respond with convincing answers by text generation (TG). We use BERT, which is just CLIP's text encoder pre-trained using mask language modeling (MLM), to process the generated captions from chatbots before transferring these linguistic knowledge. (iii) Multi-modal agents: Stable Diffusion and Pixart-\(\alpha\) share different model architectures, but both T2I agents demonstrate astonishing image generation (IG) capability. The I2T agents are pre-trained with multiple tasks including image-text contrastive (ITC), image-text matching (ITM), and image-grounded text generation (ITG) over large-scale datasets. These agents demonstrate outstanding multi-modal understanding ability.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Parameters**} & \multirow{2}{*}{**Model Type**} & \multicolumn{2}{c}{**Pre-training**} & \multirow{2}{*}{**Knowledge**} \\ \cline{3-4} \cline{6-6}  & & & **Tasks** & & & \\ \hline DINO [9] & 86M & ViT & IC & ImageNet-1K & Vision \\ MAE [33] & 86M & ViT & MIM & ImageNet-1K & Vision \\ SAM [43] & 86M & ViT & IS & SA-1B & Vision \\ ViTDet [49] & 86M & ViT & OD & COCO & Vision \\ GPT-3 [7] & 175B & LLM & TG & - & Language \\ Vicuna [18] & 13B & LLM & TG & - & Language \\ BERT [22] & 3.8M & Transformer & MLM & - & Language \\ Stable Diffusion [64] & 0.86B & UNet & IG & LAION-2B & Multi-modal \\ Pixart-\(\alpha\)[11] & 0.6B & DiT & IG & - & Multi-modal \\ BLIP-2 [46] & 2.7B & MLLM & ITC+ITM+ITG & - & Multi-modal \\ Shikra [12] & 7B & MLLM & ITC+ITM+ITG & - & Multi-modal \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Demonstration of heterogeneous agents specialized in different domains or tasks.**

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & **Setting** & **ImageNet** & **Caltech** & **Oxford** & **Standard** & **Flowers** & **FoyC** & **CNN397** & **DTD** & **Euro** \\  & & **101** & **Pets** & **Cans** & **102** & **Food10** & **Aircraft** & **SUN397** & **DTD** & **SAT** & **UCF101** \\ \hline \multirow{2}{*}{**Memory (MB)**} & Base-to-novel & 11790 & 4708 & 4692 & 6698 & 4702 & 4708 & 5286 & 7708 & 5008 & 4034 & 4718 \\  & Few-shot (16-shot) & 19978 & 4542 & 6892 & 6892 & 5488 & 5488 & 5486 & 10110 & 4700 & 4124 & 5484 \\ \hline \multirow{2}{*}{**Time (ms/batch)**} & Base-to-novel & 400 & 190 & 195 & 226 & 211 & 209 & 219 & 273 & 191 & 190 & 193 \\  & Few-shot (16-shot) & 688 & 214 & 206 & 262 & 217 & 215 & 221 & 356 & 196 & 205 & 220 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Memory and training time required for each dataset.**

[MISSING_PAGE_FAIL:17]

**Deployment efficiency.** Table 13 presents the comparison of inference time and performance evaluated on the test set of ImageNet [21]. All models adopt Vision Transformer [24] backbone. Our method demonstrates superior deployment efficiency compared to CaFo [81] which adopts a cumbersome model ensemble. To be mentioned, the learnable prompts in the training phase are frozen during the inference stage, which carries the knowledge of heterogeneous agents to enhance CLIP's generalization ability. The inference time of our TransAgent is a little behind the zero-shot CLIP, but it still achieves a good accuracy-efficiency tradeoff.

## Appendix D Additional Visualization

To verify the robustness of TransAgent, we run few-shot classification experiments over 3 seeds and visualize the variance on 9 datasets in Figure 6 (ImageNet and SUN397 are excluded since the variance is small for both methods.). We can observe that TransAgent consistently outperforms CoOp in robustness and excels in most low-shot scenarios.

## Appendix E Examples of Generated Descriptions from Chatbots

We interact with the language agents (_i.e._, chatbots) as described in Section 3.2 to obtain rich descriptions of the categories in the target domain, following [42]. To further unleash the linguistic potential of these language agents, we gather their knowledge adaptively via the MoA gating mechanism. In fact, these language agents give descriptions in different styles with their respective unique understanding. Below we show some examples of the descriptions given by different chatbots for certain classes to demonstrate their characteristics.

**Class: Pomeranian (OxfordPets).**

Figure 6: **Variance and performance of TransAgent compared with CoOp.** TransAgent demonstrates better robustness and outperforms CoOp on most low-shot cases.

GPT-3: "A pet Pomeranian typically has a thick coat of fur that can be either straight or curly."

Vicuna: "The image shows a small dog with a fluffy white coat. The dog has a round face with big brown eyes and a black nose. Its ears are small and pointy, and its tail is curled up over its back."

**Class: Industrial Buildings (EuroSAT).**

GPT-3: "A satellite photo of industrial buildings might look like a patchwork of utilitarian structures."

Vicuna: "The image shows a large industrial building with a flat roof and several windows on the front facade. The building is made of concrete and has a gray color. The roof is also gray and appears to be made of a similar material."

**Class: Club Sandwich (Food101).**

GPT-3: "A club sandwich is a sandwich made with ham, turkey, bacon, cheese, and tomato, with lettuce in between slices of bread."

Vicuna: "The image shows a golden brown club sandwich, with a thick slice of bread on the bottom, and two thinner slices on top. The bread is toasted and has a crispy texture."

**Class: Office Building (SUN397).**

GPT-3: "An office building is usually a tall building with many floors."

Vicuna: "An office building is a structure that is designed for business-related activities. It typically has multiple floors and a variety of rooms and spaces to accommodate different functions, such as open-plan offices, meeting rooms, and private offices."

**Class: 747-200 (FGVCAircraft).**

GPT-3: "The 747-200 is a jumbo jet produced by Boeing."

Vicuna: "The Boeing 747-200 is a large, wide-body commercial airliner that was first introduced in the 1970s."

**Class: 2012 Mercedes-Benz E-Class Sedan (StandfordCars).**

GPT-3: "The Mercedes-Benz E-Class Sedan 2012 can be identified by its distinctive grille, sleek headlights, and classic Mercedes-Benz logo."

Vicuna: "The 2012 Mercedes-Benz E-Class Sedan is a luxury car that is known for its sleek and sophisticated appearance."

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA]  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes]  to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract synoptically demonstrates our proposed method, which aims to transfer VL foundation models with heterogeneous agent collaboration. The introduction section further claims the background and our contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We discuss the potential limitations of our proposed method in Section 5. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The results of the paper are mainly experimental. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The implementation details are provided in the main paper (Section 4) as well as in the supplementary (Appendix A). Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The training and evaluation codes are released at https://github.com/markywg/transagent, with detailed instructions on how to download data and reproduce the experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The training details and hyperparameters setting are listed in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Since all experiments are run over 3 different seeds, we provide the variance figures in the supplementary (Appendix D) to display the robustness of our proposed method. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As stated in Appendix A, all experiments are conducted using a Nvidia A6000 GPU. The memory and training time required on each dataset are presented in Table 8. Guidelines:* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conduct the research project following NeurIPS Code of Ethics faithfully. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work does not seem to raise any (negative) societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification: Our method does not involve data or models that might lead to intended misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have made acknowledgements to the assets used in our work. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have prepared the documentation of our codes for future reproduction, which is released alongside the codes at https://github.com/markywg/transagent. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects**Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.