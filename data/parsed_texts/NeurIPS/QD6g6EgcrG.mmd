# EnsemW2S: Can an Ensemble of LLMs be

Leveraged to Obtain a Stronger LLM?

Aakriti Agrawal\({}^{\dagger}\) Mucong Ding\({}^{\dagger}\) Zora Che\({}^{\dagger}\) Chenghao Deng\({}^{\dagger}\)

Anirudh Satheesh\({}^{\dagger}\) John Langford\({}^{*}\) Furong Huang\({}^{\dagger}\)

MicrosoftUniversity of Maryland; e-mail: {agrawal5, furongh}@umd.edu

###### Abstract

How can we harness the collective capabilities of multiple Large Language Models (LLMs) to create an even more powerful model? This question forms the foundation of our research, where we propose an innovative approach to weak-to-strong (w2s) generalization--a critical problem in AI alignment. Our work introduces an easy-to-hard (e2h) framework for studying the feasibility of w2s generalization, where weak models trained on simpler tasks collaboratively supervise stronger models on more complex tasks. This setup mirrors real-world challenges, where direct human supervision is limited. To achieve this, we develop a novel AdaBoost-inspired ensemble method, demonstrating that an ensemble of weak supervisors can enhance the performance of stronger LLMs across classification and generative tasks on difficult QA datasets. In several cases, our ensemble approach matches the performance of models trained on ground-truth data, establishing a new benchmark for w2s generalization. We observe an improvement of up to 14% over existing baselines and average improvements of 5% and 4% for binary classification and generative tasks, respectively. This research points to a promising direction for enhancing AI through collective supervision, especially in scenarios where labeled data is sparse or insufficient.

## 1 Introduction

As AI models, particularly Large Language Models (LLMs), continue to surpass human performance in various domains, a pressing challenge arises: how do we effectively supervise models that exceed our capabilities? This problem, known as super-alignment, is exacerbated by the scarcity of high-quality labeled data, which limits direct human oversight. The key question driving our work is whether weak models, trained on simpler tasks, can be leveraged to instruct and improve stronger models in complex settings--a problem known as weak-to-strong (w2s) generalization.

The concept of w2s generalization was introduced by Burns et al. (2023), where weak models are used to align stronger models in the absence of sufficient ground-truth supervision. However, while this work laid the groundwork, it left several critical challenges unresolved. **(C1) Single Weak Supervisor Limitation.** Prior studies (Burns et al., 2023; Ji et al., 2024; Charikar et al., 2024; Lang et al., 2024) tend to rely on a single weak supervisor, limiting the diversity and robustness of the supervision. A single model's perspective often falls short when attempting to instruct stronger models in more complex tasks, highlighting the need for a more diversified supervisory approach. **(C2) Lack of Focus on Weak Model Enhancement.** Another limitation is that previous research (Burns et al., 2023; Ji et al., 2024; Charikar et al., 2024; Lang et al., 2024) has focused predominantly on improving knowledge transfer from weak to strong models without addressing how to enhance the weak models themselves. This oversight leaves weak models under-optimized, thereby restricting their utility in complex problem settings. **(C3) Overlooking Task Complexity.** Furthermore, while task complexity plays a crucial role in determining how well weak models can supervise stronger ones, most prior work (Sun et al., 2024) has not adequately addressed this issue. For instance, Burns et al. (2023) briefly explored the impact of task complexity using chess data, but a more structured and systematic approach is needed to differentiate between easy and hard tasks and study their effects on supervision.

To address these challenges, we propose a novel ensemble-based method designed to improve w2s generalization. Central to our approach is an easy-to-hard (e2h) framework, which extends w2s generalization by focusing on the progression from simpler tasks (easy) to more complex tasks (hard). This mirrors practical scenarios, where human oversight is more feasible for simpler tasks, and weak models must step in to guide stronger models in tackling harder tasks. In this setting, weak models trained on easy data supervise stronger models working on more difficult problems, creating a more pragmatic approach to w2s generalization.

To further enhance the capabilities of weak models, we develop a novel AdaBoost-inspired ensemble method for generation tasks, in addition to classification tasks. By combining the supervision of multiple weak models, we create a more robust and effective supervisory system for stronger LLMs. This ensemble approach overcomes the limitations of single-supervisor systems and introduces a mechanism to refine the weak models themselves, ensuring they can provide meaningful guidance even in complex tasks. Our experiments demonstrate that this ensemble method not only improves the weak models' generalization capabilities but also enables stronger models to achieve performance on par with oracle models trained on high-quality data.

The **main contributions** of this paper are the following:

**(1) We introduce an ensemble method inspired by AdaBoost,** combining weak LLMs to provide stronger supervision for training stronger models. Our approach is validated through experiments on binary classification tasks, where we observe improvements of up to 14% over baselines and an average improvement of 7% across all model pairs, showcasing the feasibility of w2s generalization.

**(2) We extend this framework to supervised fine-tuning tasks for autoregressive LLMs,** where our novel algorithm combines weak LLMs via a voting mechanism that adjusts token probabilities. In several cases, we observe our strong model trained using weak labels to outperform the strong model trained on ground truth, thus enabling effective supervision, even on complex tasks.

**(3) We propose a practical easy-to-hard (e2h) framework for w2s generalization,** where models trained on easy data provide supervision for harder tasks. This setup emphasizes the importance of task complexity and demonstrates significant improvements when weak models guide strong LLMs. For our EnsemW2S-AdaBoost method, along with observing w2s-trained student models outperforming the strong student oracle in several e2h generalization scenarios, we also observe accuracy improvements of up to 10% over baselines and an average improvement of 3.34% and 4.4% for Quartz and ARC data respectively.

## 2 Weak-to-Strong Generalization via Easy-to-Hard Framework

Figure 1: This figure illustrates the complete pipeline of our EnsemW2S method for easy-to-hard generalization using w2s generalization. In a realistic scenario, weak experts are adept at answering easy questions but must supervise strong models to tackle hard problems. **In the leftmost portion**, we show that we train weak models on easy data, strong models on hard data, and transfer models on pseudo labels generated by the weak model on hard data. Ultimately, we aim to increase the Performance Gap Recovered (PGR). **On the right**, we depict how our EnsemW2S-AdaBoost algorithm chooses the correct answer at the token level. **At the bottom**, we provide an example of easy and hard data for the Quartz dataset for e2h generalization, highlighting the importance of distinguishing between easy and hard data for realistic w2s generation.

**The Overall Idea.** We investigate the easy-to-hard framework as a more pragmatic setting to study the (im)possibility of w2s generalization. In this framework, weak models train on simpler tasks and subsequently instruct strong models to tackle more complex challenges, closely mirroring real-world conditions with limited human oversight. Figure 1 explains our idea and pipeline for easy-to-hard generalization using w2s generalization. (Figure 6 in the Appendix provides the detailed algorithmic and data flow). In a realistic scenario, weak experts are proficient in answering easy questions but must supervise strong models to tackle hard problems. We train weak models on easy data and strong models on hard data. A transfer model is trained using pseudo labels generated by the weak model on the hard data. Ultimately, we aim to improve the Performance Gap Recovered (PGR).

### The Easy-to-Hard Framework

**Weak Model \(h_{\theta}\) as the Teacher.** A state-of-the-art LLM \(h_{\theta}\) is trained on a set of 'easy data' that we currently have access to labels, i.e., \((\bm{x}^{e},\bm{y}^{e})\). For example, this could be Go games, math problems, or common sense reasoning questions that we have solutions for. This "weak teacher is trained on the labeled easy data \((\bm{x}^{e},\bm{y}^{e})\). Although we refer to this model as a "weak teacher", it is only relatively weak compared to the strong model we aim to obtain. Moreover, the "easy data" is only relatively easy compared to the hard data for which we currently lack solutions. Thus, the easy data may not be simple but slightly easier than the hard data, which are currently unsolvable using existing models.

**Strong Model \(u_{\phi}\) as the Upper Bound.** As an important part of our thought experiment, we establish an upper bound, which is not attainable in practice. Specifically, we assume access to the ground-truth labels of the hard data \((\bm{x}^{h},\bm{y}^{h})\), which is impractical but establishes an upper bound for this thought experiment. A model \(u_{\phi}\), larger than the weak teacher \(h_{\theta}\), is trained on the labeled hard data \((\bm{x}^{h},\bm{y}^{h})\). The reason why \(u_{\phi}\) is larger than \(h_{\theta}\) is that we believe a model strong enough to solve hard questions that no existing models can solve will require high capacity.

**Weak-to-Strong Model \(f_{\phi}\) Obtained in Practice.** To test the weak-to-strong generalization, we will train a weak-to-strong transfer model \(f_{\phi}\) that has the same capacity as the strong model, i.e., the same model size as \(u_{\phi}\), but is not trained under the unrealistic assumption of oracle access to hard labels. Rather, it is trained using weak teacher's feedback. Specifically, we consider using the pseudo-labeled \((\bm{x}^{h},h_{\theta}(\bm{x}^{h})\) as training data for training the weak-to-strong transfer model \(f_{\phi}\).

### Easy and Hard Data

**Dataset and Setup.** We use the SciQ dataset (Welbl et al., 2017) for the binary classification task. It is a multiple-choice science question-answer dataset and is also used as one of the NLP classification datasets by Burns et al. (2023). We convert it into binary labels following (Burns et al., 2023). For the supervised fine-tuning (SFT) task on the Q/A dataset, we use ARC (Clark et al., 2018) and Quartz (Tafjord et al., 2019) datasets, which are also multiple-choice question-answer datasets, allowing us to generate multiple-choice pseudo labels. Ding et al. (2024) provide difficulty levels for some common mathematics and programming problems, chess puzzles, and reasoning question datasets, which can be further utilized to expand this work. For details on how we conduct **easy \((\bm{x}^{e},\bm{y}^{e})\) and hard \((\bm{x}^{h},\bm{y}^{h})\) data split**, refer Appendix Section E.

### An Ensemble of Teachers

In a practical situation, we may face a dearth of strong supervisors but have an abundance of weak supervisors. Previous works (Burns et al., 2023; Ji et al., 2024) have used only one weak supervisor. Our work aims to combine the power of multiple weak supervisors to provide stronger supervision for better weak-to-strong (w2s) generalization. However, combining multiple weak supervisors to improve w2s generalization is challenging. In the following section, we detail how to combine a collection of weak teachers with diverse skill sets to obtain a competitive w2s model that is better than the weak model and ideally reaches or even surpasses the strong model, i.e., the upper bound of performance.

## 3 W2S Generalization via AdaBoost of Diverse Teacher LLMs

In this section, we introduce our method to boost experts for two tasks: a binary classification task for an NLP dataset and a supervised fine-tuning task for multiple-choice Q/A datasets. A list of important notions is mentioned in Appendix D for reference.

### AdaBoost of Weak LLM Teachers for Classification Tasks

This simple thought experiment tests w2s generalization and is the first task tested by Burns et al. (2023). We utilize the vanilla AdaBoost algorithm (Algorithm 2 detailed in the Appendix) to generate answers to a hard question \(\bm{x}^{h}\) from each weak LLM teacher, i.e., generate \(h^{t}_{\theta}(\bm{x}^{h})\) for\(t\in\{1,\ldots,T\}\). A weighted "majority vote/aggregation" is implemented to generate a consensus as the answer \(\mathbbm{1}(\sum_{t=1}^{T}\alpha_{h}^{t}\theta_{b}^{t}(\bm{x}^{h})>0)\in\{0,1\}\), also known as the pseudo-label, to the hard question \(\bm{x}^{h}\). Here, the coefficients \(\{\alpha_{t}\mid t\in\{1,\ldots,T\}\}\) are hyperparameters learned during the AdaBoost training. (More details in Appendix Sec F.)

### Improving AdaBoost for Complex Generation Tasks

**Challenges of Applying AdaBoost.** The canonical AdaBoost algorithm assumes a sophisticated ensemble of feedback in the form of scores. However, LLMs are generative AI models known for their remarkable ability to generate coherent, free-form text. Applying the vanilla AdaBoost algorithm directly to generation tasks is challenging because (1) the output is not just a single class label but a sequence of text with no fixed length, and (2) different teachers may generate answers in various formats, making it non-trivial to combine their responses.

**EnsemW2S-AdaBoost: Our modified AdaBoost Algorithm for Multiple-Choice Q/A Task.** To address these challenges, we propose a modified multi-class AdaBoost algorithm where the number of classes corresponds to the vocabulary size. We treat each token as an independent sample, as shown in Algorithm 1, and apply multi-class AdaBoost (Hastie et al., 2009) with modifications, calling our algorithm EnsemW2S-AdaBoost.

_Token-Level Weighting._ The first modification involves generating weights for each token within a sentence sample. We define the initial token-sample weights vector \(D_{1}(i,j)\leftarrow\frac{1}{n}\) for all \(i\in[m],j\in[k_{i}]\), where \(n=\sum_{i=1}^{m}k_{i}\), \(k_{i}\) is the number of tokens in the answer part of each sample \(i\), \(m\) is the total number of training data samples and \(j\) is the \(j^{\text{th}}\) token in a particular chosen \(i^{\text{th}}\) sample. We update these weights, \(D_{t}(i,j)\), for each iteration \(t\) of EnsemW2S-AdaBoost.

_Token-Level Data Sampling._ We sample \(S^{\prime}=\{(\bm{x}_{i}^{\prime e},\bm{y}_{i}^{\prime e})\}_{i=1}^{m}\) from \(S\) using token-sample weights \(D_{t}(i,j)\). By sampling with respect to probability masses \(D_{t}(i,j)\) with repetition, we obtain a set of \(n=\sum_{i=1}^{m}k_{i}\) tokens to train on. However, treating these \(n\) sampled tokens as independent training samples is very inefficient. Instead, we "assemble" the sampled tokens back into the sentences they belong to and implement label masks to only train on the sampled tokens in each sentence. Following this method, we can train on sampled tokens with minimal overheads.

_Training and Generating New Weak Teachers._ For each iteration, \(t\), of EnsemW2S-AdaBoost we train a new weak teacher model \(h_{\theta}^{t}\) on the sampled data, \(S^{\prime}\).

_Incorporating Prior Term._ Following Hastie et al. (2009), multi-class boosting uses an additional \(\log(c-1)\) term, where \(c\) is the number of classes, in the calculation of the AdaBoost parameter \(\alpha\). This term serves two purposes: (1) It enables the generation of weak models with accuracy between 50% and random \(\frac{1}{c}\) %, which is crucial for smaller models and challenging tasks that cannot achieve 50% accuracy. (2) It ensures that \(\alpha\) remains positive. Bayesian inference is used to provide proof of the benefits of this prior term. Given the large vocabulary size in our case, we introduce a prior term \(\log(\frac{1}{1-\epsilon_{pre}}-1)\), where \(\epsilon_{pre}\) is the pre-trained model error of the chosen LLM. This term is sensible because it represents the error before fine-tuning the LLM, effectively replacing the random error baseline. Thus, the final \(\alpha\) equation is: \(\alpha_{t}\leftarrow\log(\frac{1-\epsilon_{t}}{\epsilon_{t}})+\log(\frac{1}{1- \epsilon_{pre}}-1)\).

_Weighted Error Calculation._ Our weighted error equation \(\epsilon_{t}\) also undergoes minor changes. The strict condition for each round of AdaBoost training is that the weighted model error (calculated by comparing each token of each sample) must be less than the pre-training error, i.e., \(\epsilon_{t}<\epsilon_{pre}\). The weighted model error \(\epsilon_{t}\) is defined as, \(\epsilon_{t}=\sum_{i=1}^{m}\sum_{j=1}^{k_{i}}\mathbbm{1}\{h_{\theta}^{t}(\bm{x }_{i}^{e},\bm{y}_{i}^{e,j-1})\neq\bm{y}_{i}^{e,j}\}D_{t}(i,j)<\epsilon_{pre}\). Here, \(\bm{y}_{i}^{e,j-1}\) is the \((j-1)^{\text{th}}\) ground-truth token in the answer part. The model \(h_{\theta}^{t}(\bm{x}_{i}^{e},\bm{y}_{i}^{e,j-1})\) predicts the next token and compares it with the ground-truth token \(\bm{y}_{i}^{j}\).

_Weight Update Equation._ Our weight update equation for each token is \(D_{t+1}(i,j)\leftarrow\frac{1}{2_{t}}D_{t}(i,j)e^{\alpha_{t}1\{h_{\theta}^{t} (\bm{x}_{i}^{e},\bm{y}_{i}^{e,j-1})\neq\bm{y}_{i}^{e,j}\}}\) where \(Z_{t}\) is a normalization factor calculated by taking the norm of the updated weight vector to ensure \(\sum_{i=1}^{m}\sum_{j=1}^{k_{i}}D_{t+1}(i,j)=1\).

**Combining Experts to Generate Pseudo Answers for Hard Questions:** To combine the outputs of different experts trained during the various EnsemW2S-AdaBoost rounds, we scale the probability distribution for each token generated by the model \(h_{\theta}^{t}\) in round \(t\) by its corresponding weight \(\alpha_{t}\). Specifically, we multiply \(\alpha_{t}\) by the probability distribution vector of each token. We then aggregate these weighted distributions across all rounds, normalizing the resulting vector to form a new probability distribution for each token.

```
0: An "easy" Q/A training dataset with \(m\) examples: \(S^{c}=\{(\bm{x}_{i}^{c},\bm{y}_{i}^{c})\}_{i=1}^{m}\); a pre-trained weak teacher model \(h_{\theta}^{0}\) parameterized by \(\theta\); total number of EnsemW2S-AdBoost iterations \(T\); a "hard" unlabeled (questions only) dataset with \(O\) examples: \(S^{h}=\{\bm{x}_{\theta}^{h}\}_{o=1}^{O}\)
0: Weak-to-Strong Student Model \(f_{\phi}(\cdot)\)
1: Initialize Token-Sample Weights: \(D_{1}(j,t)\leftarrow\frac{1}{n}\) for all \(i\in[m],j\in[k_{i}]\), where \(k_{i}\) is the token length in the \(i^{\text{th}}\) easy example (i.e., \(\bm{y}_{i}^{c}=(\bm{y}_{i}^{e,1},\bm{y}_{i}^{e,2}...\bm{y}_{i}^{e,k_{i}})\)) and \(n=\sum_{i=1}^{m}\,k_{i}\)
2: Calculate pre-training error of \(h_{\theta}^{0}\): \(\epsilon_{pre}\leftarrow\sum_{i=1}^{m}\sum_{j=1}^{k_{i}}1\{h_{\theta}^{0}(\bm {x}_{i}^{e},\bm{y}_{i}^{e,j-1})\neq\bm{y}_{i}^{e,j}\}D_{1}(i,j)\)
3:for\(t\gets 1\) to \(T\)do
4: Sample \(S^{\prime}=\{(\bm{x}_{i}^{\prime\prime},\bm{y}_{i}^{\prime\prime})\}_{i=1}^{m}\) from \(S\) using token-sample weights \(D_{t}(i,j)\)
5: Train a new weak teacher \(\bm{h}_{\theta}^{0}\) on \(S^{\prime}\)
6: Calculate \(\epsilon_{t}=\sum_{i=1}^{m}\sum_{j=1}^{k_{i}}1\{h_{\theta}^{0}(\bm{x}_{i}^{c},\bm{y}_{i}^{e,j-1})\neq\bm{y}_{i}^{e,j}\}D_{t}(i,j)\)
7:if\(\epsilon_{t}\geq\epsilon_{pre}\)then
8:break
9: Calculate \(\alpha_{t}\leftarrow\log\frac{1-\epsilon_{t}}{\epsilon_{t}}+\log(\frac{1}{1- \epsilon_{pre}}-1)\)
10: Update \(D_{t+1}(i,j)\leftarrow\frac{1}{Z_{t}}D_{t}(i,j)e^{\alpha_{t}1\{h_{\theta}^{0}( \bm{x}_{i}^{e},\bm{y}_{i}^{e,j-1})\neq\bm{y}_{i}^{e,j}\}}\) for all \(i\in[m],j\in[k_{i}]\), where \(Z_{t}\) is a normalization factor such that \(\sum_{i=1}^{m}\sum_{j=1}^{k_{i}}D_{t+1}(i,j)=1\)
11:for\(o\gets 1\) to \(O\)do
12:for\(j\gets 1\) to \(k_{o}\)do
13: Autoregressively generate the \(j^{\text{th}}\) token of the "pseudo-answer" \(\widehat{\bm{y}}_{o}^{h,j}\sim\Delta^{\text{vocab}}(\sum_{t=1}^{T}\alpha_{t} \cdot\mathrm{softmax}(h_{\theta}^{t}([\bm{x}_{o}^{h},\widehat{\bm{y}}_{o}^{h,1:j-1}])))\), where \(\Delta^{\text{vocab}}\) denotes the simplex on the vocabulary
14: Train weak-to-strong student model \(f_{\phi}(\cdot)\) on \(\{(\bm{x}_{o}^{h},\widehat{\bm{y}}_{o}^{h})\}_{o=1}^{O}\) ```

**Algorithm 1****Main Algorithm: EnsemW2S-AdBaBoost**

Using this aggregated distribution, we sample the final predicted token. The process is autoregressive, where the \(j^{\text{th}}\) token of the "pseudo-answer" is generated as

\[\widehat{\bm{y}}_{o}^{h,j}\sim\Delta^{\text{vocab}}\left(\sum_{t=1}^{T}\alpha_ {t}\cdot\mathrm{softmax}\left(h_{\theta}^{t}\left([\bm{x}_{o}^{h},\widehat{ \bm{y}}_{o}^{h,1:j-1}]\right)\right)\right)\] (1)

where \(\Delta^{\text{vocab}}\) represents the simplex over the vocabulary.

By combining the outputs of multiple experts, each trained in different EnsemW2S-AdBaBoost rounds, the ensemble approach leverages diverse perspectives from the weak models. Each expert contributes its learned strengths, and through weighted aggregation, we diminish the influence of models that are less confident or less effective on certain tokens. This helps reduce variance in the generation process, ensuring that errors from individual weak models are mitigated. The result is a more robust pseudo-labeling system that is better aligned with the true distribution of the hard data, often yielding a performance improvement over any single weak model.

Unlike classification, where scores are combined over a fixed set of classes, generation tasks involve predicting sequences of tokens, where each prediction affects future ones. This makes combining generation probabilities more complex, as errors in early token predictions can propagate throughout the sequence. Additionally, we are aggregating probability distributions over large vocabularies, which introduces computational overhead and potential numerical instability.

Our method addresses these challenges by using a weighted combination of expert models' token probabilities, ensuring that weaker predictions from individual rounds are minimized. By normalizing the aggregated distribution for each token, we maintain valid probability distributions across the vocabulary, effectively reducing the risk of cascading errors during autoregressive generation. This ensemble approach results in a more stable and accurate generation process, mitigating the issues inherent in sequence modeling.

**Pseudo answer generation on multiple-choice datasets:** On multiple-choice datasets, instead of using generated tokens \(\widehat{\bm{y}}^{h}\) as pseudo answers, we can select one of the choices in the MCQ datasetusing negative log-likelihood (NLL). Specifically, we calculate the NLL between the choices and \(\widehat{\bm{y}}^{h}\) and select the choice with the lowest NLL. For datasets without multiple choices, we can directly use \(\widehat{\bm{y}}^{h}\). For **ablation studies** on our method refer Appendix section G.2

**Train W2S Model:** The strong student model, \(f_{\phi}(\cdot)\), is trained using pseudo answers generated for the hard data \(\{(\bm{x}_{o}^{h},\widehat{\bm{y}}_{o}^{h})\}_{o=1}^{O}\). While it might be beneficial to include the labeled easy data in the training process, we adhere to the pipeline established by Burns et al. (2023) by focusing exclusively on the hard examples to maintain consistency.

**Evaluation Metric.** We used two metrics to evaluate this Q/A dataset. One is **(1) Token-wise comparison**, where we compare each predicted token and average the total error, and **(2) Option-wise comparison**, where we compare the negative log-likelihood (NLL) of the correct answer completion with the NLLs of the incorrect answer completions. Accuracy represents the number of entries where the correct answer completion has the lowest NLL among all choices.

## 4 Experimental Setup

We test two strategies for each task. The first, following Burns et al. (2023), randomly splits the training data into train-weak and train-strong. Train-weak is used to train the weak model. Train-strong is used to train the strong and transfer models using pseudo labels generated using the weak model. The second strategy splits the data into easy (train-weak) and hard (train-strong) subsets, with the same training pipeline, offering a more realistic w2s generalization setup, as discussed in Section 1. Both strategies aim to recover the performance gap (PGR) and maximize the strong model's capability using an ensemble of weak models. The baseline in all experiments uses a single model for w2s generalization, following the principle of Burns et al. (2023). More details in appendix sec G.3.

### Binary Classification Task

**W2S Results with Random Training Data Splits.** The baseline of this method is a replication of Burns et al. (2023). From Figure 2, by applying AdaBoost, we observe a significant improvement in the weak model accuracy, significantly improving the PGR values. In the case of the GPT-2-medium to GPT-2-large pair, we even see the PGR exceeding 100%, meaning that the transfer model has

Figure 2: **Binary Classification Task: Top figure** shows a bar plot comparing w2s generalization of our method (grey) with a baseline (blue) from Burns et al. (2023) using accuracy values(%) for different combinations of weak and strong model pairs for random data split (top bar-plot) and easy-hard split(bottom bar-plot). **Bottom figure** shows a line plot comparing the accuracy and performance gap recovered values (PGR). The left two figures are for random data split, while the right two figures are for the easy-hard split to show e2h generalization.

outperformed the strong model's performance. This is the ambitious aim of the w2s generalization problem, and our results show that w2s generalization is achievable.

**W2S Results with Easy and Hard Training Data Splits.** From Figure 2, we see that applying AdaBoost significantly improves weak model accuracy, thereby enhancing the PGR values. However, for this holistic e2h generalization problem, we are far from reaching the full capability of a strong model. For very small (GPT-2) and large model pairs (GPT-2-xl and above), we do not see improvement in w2s generalization despite the weak models' accuracy improvements. Overall, we observe an improvement of up to 14% in accuracy compared to the baseline and an average improvement of 6.52% and 3% for random and easy-hard splits, respectively.

**Scaling Law:** In Figure 2 (line plot), we see less PGR recovery for the Qwen-1.8B model even though it is similar in size to GPT-2-xl. Similarly, in the bar plot, we see a drastic difference between the oracle performance of GPT2xl and Qwen-1.8B. This is because the Qwen models series are more capable even after being the same size. Thus, model size is not a good metric, but model capability is a better metric for differentiating between weak and strong models.

**Better metric:** Figure 2 shows the accuracy and PGR plots for both random and easy-hard split. We observe that PGR is not very informative, as it can produce extremely large or even negative values. In the w2s experiments, large values occur because the ensemble of weak models becomes strong enough to match or exceed a strong model, improving w2s generalization. Negative values, seen in baseline experiments, indicate the transfer model performed worse than the weak model, often when the strong model fails to learn and its inductive bias becomes random with pseudo-label training. Similar patterns are seen in Figure 15 and 4. (Refer to Appendix Table 1 and 2 for more details.)

### Generation Task for Multiple Choice Dataset

#### 4.2.1 Comparing Weak model's performance

In Figure 3, we compare the performance of a single weak model (dark color) with combined weak models after 5 rounds of EnsemW2S-AdaBoost. Smaller models show greater improvement, which is expected since boosting works best when weak models are diverse. Using EnsemW2S-AdaBoost, smaller models can diversify through the data sampling step; however, larger models tend to learn all possible information and cannot learn something different with each round. We use token-error here since it's a more precise metric to measure improvement in weak models.

#### 4.2.2 Comparing Strong model's performance

Here, we use the multiple-choice classification accuracies to calculate the accuracy of all our plots. We show the accuracy values of token-wise metrics in the Appendix tables.

**W2S Results with Random Training Data Splits.** From Figure 4 and 15, we see that w2s training using an ensemble of experts almost consistently outperforms the baseline (single expert). Thus, ensemble learning is beneficial. We can see the trend of accuracy and performance gap recovered for the different model pairs in Figure 4 and 15 for Quartz and ARC datasets, respectively. For Quartz data, we see that our PGR percentage (Figure 4) improves as the model scales up except when the weak model is the smallest sized model (pythia-70m). This could be because the increasing capability difference between the small and large models makes it difficult for the strong model to learn anything from the weak. This trend is the same in the baseline as well as our EnsemW2S. But an important thing to note is that for some cases for both ARC and Quartz data, our method generates a large PGR percentage of >=100%, showing the ability of our w2s method to recover the performance gap.

**W2S Results with Easy-Hard Training Data Splits.** From Figure 4 and 15, we see that w2s training using an ensemble of experts almost consistently outperforms the baseline (single expert). Thus showing that ensemble learning is beneficial. Our method shows more improvement over baseline for easy-hard data split as compared to random split. This is because of two reasons. Firstly, the power of combining weak models using our modified AdaBoost is more useful when all of them are weak

Figure 3: Performance comparison of a single weak model (dark color) with the combined weak models (Lighter hue shows improvement).

but slightly different from each other. Secondly, by easy and hard splitting, the margin between weak and strong increases more, giving more room for improvement.

We also observe that PGR for e2h generalization is significantly lower, highlighting the complexity of the e2h generalization problem. We hope this work could motivate researchers to build more sophisticated methods for this more complex e2h generalization problem. Another simple observation is as the models become more capable, both the performances (baseline and ours) increase.

Note: Refer to Appendix Table 3 and 6 for detailed values of our experiments on the Quartz and ARC datasets with random data splits. Bar plots of weak and strong (oracle) model performance for these splits are shown in Appendix Figure 13 and 16. For easy-hard data split, the same details can be found in Appendix Tables 4, 7 and Figure 14 and 17.

#### 4.2.3 Performance on hard data after training on weak vs strong data

This experiment highlights the importance of e2h with w2s generalization. In Table 5, the Quartz dataset shows significant improvement for larger models when trained on hard data, indicating their better ability to understand complex data. For ARC, all models improve but with a smaller margin, suggesting less disparity between easy and hard samples in the ARC dataset.

## 5 Conclusion

This paper aims to stimulate discussion on the more holistic problem of w2s generalization by emphasizing e2h generalization. We develop a new AdaBoost-inspired algorithm and conduct a thought experiment on how to combine the "wisdom of the crowd" to improve w2s generalization. We are first to focus on the idea of making the weaks less weak using an ensemble, and test our method for complex SFT tasks. Our method in some cases recovers full strong model capability.

Figure 4: **Generation Task (Quartz Data): Top figure** shows a bar plot comparing the w2s generalization of our method (grey) with a baseline (blue) for various combinations of weak and strong model pairs for the SFT task on Q/A data for random data split (top bar-plot) and easy-hard split (bottom bar-plot). **Bottom figure** shows a line plot comparing accuracy and PGR. The left two figures are for random data split, while the right two are for the easy-hard split to show e2h generalization.

Figure 5: Accuracy (%) values for LLMs trained on easy vs hard data and evaluated on hard data.

## Acknowledgements

Agrawal, Ding, Che, Deng, Satheesh, Langford and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, National Science Foundation NSF-IIS-2147276 FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.

## References

* Bansal et al. (2024) Hrtik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q Tran, and Mehran Kazemi. Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling. _arXiv preprint arXiv:2408.16737_, 2024.
* Burns et al. (2023) Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. _arXiv preprint arXiv:2312.09390_, 2023.
* Cai et al. (2024) Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. _arXiv preprint arXiv:2401.10774_, 2024.
* Chang et al. (2023) Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, and Wen Sun. Learning to generate better than your llm. _arXiv preprint arXiv:2306.11816_, 2023.
* Charikar et al. (2024) Moses Charikar, Chirag Pabbaraju, and Kirankumar Shiragur. Quantifying the gain in weak-to-strong generalization. _arXiv preprint arXiv:2405.15116_, 2024.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Ding et al. (2024) Mucong Ding, Chenghao Deng, Jocelyn Choo, Zichu Wu, Aakriti Agrawal, Avi Schwarzschild, Tianyi Zhou, Tom Goldstein, John Langford, Anima Anandkumar, and Furong Huang. Easy2hardbench: Standardized difficulty labels for profiling llm performance and generalization, 2024. URL https://arxiv.org/abs/2409.18433.
* Freund & Schapire (1997) Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of computer and system sciences_, 55(1):119-139, 1997.
* Guo et al. (2024) Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, and Yunhe Wang. Vision superalignment: Weak-to-strong generalization for vision foundation models. _arXiv preprint arXiv:2402.03749_, 2024.
* Hase et al. (2024) Peter Hase, Mohit Bansal, Peter Clark, and Sarah Wiegreffe. The unreasonable effectiveness of easy training data for hard tasks. _arXiv preprint arXiv:2401.06751_, 2024.
* Hastie et al. (2009) Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-class adaboost. _Statistics and its Interface_, 2(3):349-360, 2009.
* Ji et al. (2024) Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang. Aligner: Achieving efficient alignment through weak-to-strong correction. _arXiv preprint arXiv:2402.02416_, 2024.
* Jin et al. (2024) Lifeng Jin, Baolin Peng, Linfeng Song, Haitao Mi, Ye Tian, and Dong Yu. Collaborative decoding of critical tokens for boosting factuality of large language models. _arXiv preprint arXiv:2402.17982_, 2024.
* Lang et al. (2024) Hunter Lang, David Sontag, and Aravindan Vijayaraghavan. Theoretical analysis of weak-to-strong generalization. _arXiv preprint arXiv:2405.16043_, 2024.
* Liu et al. (2020)* Liu and Alahi (2024) Yuejiang Liu and Alexandre Alahi. Co-supervised learning: Improving weak-to-strong generalization with hierarchical mixture of experts. _arXiv preprint arXiv:2402.15505_, 2024.
* Mudgal et al. (2023) Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. _arXiv preprint arXiv:2310.17022_, 2023.
* Rosset et al. (2024) Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. _arXiv preprint arXiv:2404.03715_, 2024.
* Sang et al. (2024) Jitao Sang, Yuhang Wang, Jing Zhang, Yanxu Zhu, Chao Kong, Junhong Ye, Shuyu Wei, and Jinlin Xiao. Improving weak-to-strong generalization with scalable oversight and ensemble learning. _arXiv preprint arXiv:2402.00667_, 2024.
* Shen et al. (2024) Shannon Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. Learning to decode collaboratively with multiple language models. _arXiv preprint arXiv:2403.03870_, 2024.
* Sun et al. (2024) Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. _arXiv preprint arXiv:2403.09472_, 2024.
* Tafjord et al. (2019) Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of qualitative relationship questions. _arXiv preprint arXiv:1909.03553_, 2019.
* Verga et al. (2024) Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhang-orodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llvm generations with a panel of diverse models. _arXiv preprint arXiv:2404.18796_, 2024.
* Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. _arXiv preprint arXiv:1707.06209_, 2017.
* Zhang et al. (2024) Edwin Zhang, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L Edelman, Milind Tambe, Sham M Kakade, and Eran Malach. Transcendence: Generative models can outperform the experts that train them. _arXiv preprint arXiv:2406.11741_, 2024.

## Appendix A Related Works

Weak-to-Strong(Burns et al., 2023) was the first to introduce the problem of weak-to-strong generalization for the super-alignment problem, where the ultimate aim is to elicit the full capabilities of the strong model using supervision only from weak models. (Charikar et al., 2024) provides a theoretical framework for the same with insights on how much w2s improvement can occur, though their work is limited to a few layer neural networks. Similarly, (Lang et al., 2024) provides bounds on expansion properties using finite data distributions for when w2s generalization will happen, but only for simple binary classification tasks. (Zhang et al., 2024) proves that transcendence (exceeding the capability of the model that generates the training data) is possible for low-temperature sampling. Although this setting is not exactly w2s, it sheds light on this direction.

Several works have attempted to solve w2s generalization in LLMs. (Sang et al., 2024) tries to improve this supervision using ensemble learning and scalable oversight for binary classification NLP tasks but cannot observe significant improvement. (Ji et al., 2024) introduces a model that enhances the alignment of LLMs with human intentions by correcting the residual differences between aligned and unaligned answers by training on a query-answer correction dataset. This method boosts w2s generalization using supervisory signal from smaller models to improve the performance of complex systems. In (Sun et al., 2024), the authors propose a scalable approach for e2h generalization which involves training reward models on easier tasks and using them to evaluate performance on harder tasks. (Liu and Alahi, 2024) introduces a method similar to the classical hierarchical mixture of experts, where multiple specialized weak supervisors are used for weak-to-strong generalization instead of a single generalist model. (Bansal et al., 2024) compares large LLM training from data generated using weak (cheap) vs strong (expensive) model in a compute matching way and finds larger data from weaker model to provide better w2s.

Guo et al. (2024) introduces an dynamic adjustable loss function for weak-to-strong supervision. Hase et al. (2024) demonstrates that current language models can achieve high performance on difficult tasks by training on simpler, cleanly labeled data, thus avoiding the high costs and noise associated with hard data labeling. None of these works focused on making the weak teachers, less weak but only focus on improving transfer learning and correction of weak labels. Thus, our method can be combined with all ideas focused on improving transfer learning.

Ensemble LearningBinary Classification Boosting (Freund and Schapire, 1997) and multi-classification boosting (Hastie et al., 2009) are common ensemble learning algorithms. In (Verga et al., 2024), they use a voting mechanism to combine multiple small LLMs instead of a single large LLM to evaluate another LLM and show it performs better than large LLMs. An extended related work section is present in Appendix A.

Multi-LLM learning:There are numerous works involving the collaboration of multiple LLMs. Chang et al. (2023) proposes Reinforcement Learning with Guided Feedback (RLGF), where a dynamic black-box guide like GPT-3 is used to fine-tune large language models. Rosset et al. (2024) introduces Direct Nash Optimization (DNO), a scalable algorithm that combines contrastive learning with general preference optimization. Cai et al. (2024) presents MEDUSA, an innovative framework designed to accelerate inference in large language models by introducing multiple decoding heads, enabling simultaneous prediction of several tokens, and enhancing efficiency through reduced decoding steps and parallel processing capabilities. Shen et al. (2024) proposes Co-LLM, a collaborative decoding framework that interleaves token-level generations from multiple models. This method optimizes the latent variable model for marginal likelihood, allowing a base model to decide when to generate tokens itself or utilize an assistant model, thereby improving performance across various specialized tasks without direct supervision. Jin et al. (2024) introduces a novel collaborative decoding framework aimed at improving the factuality of large language models by employing a critical token classifier. This approach strategically uses both pre-trained and aligned models to selectively generate critical tokens, significantly enhancing the model's ability to maintain factual accuracy without compromising the diversity of the generated content.

Additionally, Mudgal et al. (2023) introduces Controlled Decoding (CD), a method for aligning language model outputs with desired outcomes using a separate prefix scorer module. This approach allows multi-objective RL without additional training and performs well on benchmarks, bridging the gap between token-level control and sequence-level best-of sampling strategies.

## Appendix B Limitation and Future Work

(Continue from main manuscript)

_Limitation and Future Work:_ This work only explores the supervised fine-tuning phase. While SFT is an important part of the LLM learning pipeline, our future work will focus on developing weak supervision in the reward modeling phase. Another interesting future direction would be to improve the combination of tokens in the decoding phase by replacing the classical AdaBoost algorithm with more adaptive ensemble learning methods. We hope this work sparks discussion on combining multiple LLMs to improve weak-to-strong generalization.

_Computational Overhead:_ For fully generative tasks, multiple forward passes are required in an autoregressive manner. At each step, the final voted token is input to all LLMs to predict the next token. This increases generation time, which can be mitigated using efficient decoding algorithms like speculative decoding. Addressing this also forms part of our future work. _Smaller Models:_ Another limitation is of all w2s work is they attempt to mimic the weak and strong setting as an analogy to the realistic problem and cannot test on a real human with super-human model.

## Appendix C Detailed Flowchart

## Appendix D Important Notations

Easy Data: \(\{(\bm{x}_{i}^{e},\bm{y}_{i}^{e})\}_{i=1}^{m}\)

Hard Data: \(\{(\bm{x}_{o}^{h},\bm{y}_{o}^{h})\}_{o=1}^{O}\)

Total number of Easy Data points: \(m\)

Total number of Hard Data points: \(O\)

Total EnsemW2S-AdaBoost Rounds: \(T\) Weak Teachers:

Figure 6: This figure explains our pipeline for easy-to-hard generalization using w2s generalization in complete detail including the algorithm and data flow. We train weak models on easy data and strong models on hard data. A transfer model is trained using pseudo labels generated by the weak model on the hard data. Ultimately, we aim to improve the Performance Gap Recovered (PGR).

Strong Student (Oracle): \(u_{\phi}\)

Weak-to-Strong model: \(f_{\phi}\)

Total number of tokens in the answer part of each sample \(i\): \(k_{i}\)

AdaBoost voting parameter: \(\{\alpha_{t}\}_{t=1}^{T}\)

EnsemW2S-AdaBoost token-sample weights for \(i^{th}\) sample and \(j^{th}\) token: \(\{D_{t}(i,j)\}_{t=1}^{T}\)

Pre-trained Model error: \(\epsilon_{pre}\)

EnsemW2S-AdaBoost's weighted model error for round \(t\): \(\epsilon_{t}\)

## Appendix E Easy and Hard Data Split.

**Easy \((\bm{x}^{e},\bm{y}^{e})\) and Hard \((\bm{x}^{h},\bm{y}^{h})\) Data Split:** To generate difficulty ratings for our datasets, we employ the \(n\)-fold cross-validation method. We train the model on the \((n-1)\) out of \(n\) splits of the data and test on the remaining split. We repeat the process \(n\) times with different splits for testing each time and aggregate the errors. We use this error value for each sample as its difficulty rating. We split the low difficulty-rated data for weak model training and use the high difficulty-rated data to generate strong model training data and testing data randomly. We follow the same cross-validation method, with different training protocols, for generating difficulty for both binary classification and generation tasks. More details and our difficulty rating plots can be seen in Figures 7, 8, and 9 in the Appendix.

## Appendix F Binary Classification Task

AdaBoost utilizes the wisdom of the crowd to obtain a stronger learner. Inspired by its philosophy, we use an ensemble of weak LLM teachers as the "weak learners" to obtain a "stronger learner", i.e., a strong model that improves binary classification tasks, thus achieving weak-to-strong generalization.

**Results and Observations.** As shown in the weak-model performance columns in Table 1 and 2 in the Appendix, the combined weak experts (\(T=2,3,4,5\)) demonstrate higher performance than a single weak expert (baseline).

**Training Methodology.** In our method, the weak experts are trained to minimize the error on the reweighted training examples, as detailed in Line 5 of Algorithm 2. The only requirement is that they perform better than random, thus satisfying the well-known weak learning condition. These weak experts represent a practical scenario where, although individually weak, they possess complementary knowledge. Thus, when combined, they have the potential to form a stronger expert.

```
0: Training Dataset \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\sim D^{m}\) \(T=\) AdaBoost iterations \(\tilde{D}_{1}(i)\leftarrow\frac{1}{m}\forall i\in[m]\) for\(t\gets 1\) to \(T\)do \(h_{t}\) such \(\epsilon_{t}=\sum_{i=0}^{m}\mathbbm{1}\{h_{t}(x_{i})\neq y_{i}\}\tilde{D}_{t}( i)<\frac{1}{2}\) \(\alpha_{t}\leftarrow\frac{1}{2}\log\frac{1-\epsilon_{t}}{\epsilon_{t}}\) \(Z_{t}=2\sqrt{\epsilon_{t}(1-\epsilon_{t})}\) \(\tilde{D}_{t+1}\leftarrow\frac{1}{2}\tilde{D}_{t}e^{-\alpha_{t}y_{i}h_{t}(x_{ i})}\) \(g\leftarrow\sum_{t=1}^{T}\alpha_{t}h_{t}\) Return \(h(x)=\operatorname{sign}(g)\) ```

**Algorithm 2** AdaBoost Freund & Schapire (1997)

Detailed Results for Binary Classification Task with \(\alpha\) and \(Err_{t}^{Train}\) in Table 1 and Table 2

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_EMPTY:16]

### Ablation Studies for Generation Task

**Ablation Studies.** We experimented with combining the logits directly instead of probabilities but did not observe any improvement (refer to Appendix Figure 10). We conducted ablation studies where, instead of treating each token as independent, we used a sliding window of length \(L\) while calculating weights and aggregating errors (see Appendix Figure 11 and 12). Different window lengths did not cause significant changes in values, so we ultimately chose a window of \(L=1\). We also explored treating each sample as independent instead of each token as independent in the sample-answer part, finding better results with the latter. This is reasonable since the error calculated using independent-sample weights is less accurate.

g.2.1 Comparison between probability based combination with logit based combination of the tokens, during generation and evaluation of combined weak experts.

#### g.2.2 Comparison between different window lengths for "sample and token weighing".

### Experimental Details

We run AdaBoost/EnsemW2s-AdaBoost 10 times for the binary classification tasks and 5 times for the generation tasks. We pick the best w2s performing round for our plots. However, we observe that all rounds (\(n>=2\)) are better than the baseline (\(n=1\)). Additionally, we chose single model performance (\(n=1\)) for weak model performance.

Figure 11: This figure shows a comparison between different token windows for pythia 70m model.

Figure 12: This figure shows a comparison between different token windows for pythia 410m model.

Figure 10: This figure shows a comparison between probability based combination with logit based combination of the tokens.

[MISSING_PAGE_EMPTY:18]

#### g.4.1 Supervised-Fine Tuning task for ARC Question-Answer Dataset

Figure 15: **Generation Task (ARC Data): Top figure** shows a bar plot comparing the w2s generalization of our method (grey) with a baseline (blue) for various combinations of weak and strong model pairs for the SFT task on Q/A data for random data split (top bar-plot) and easy-hard split (bottom bar-plot). **Bottom figure** shows a line plot comparing accuracy and PGR. The left two figures are for random data split, while the right two are for the easy-hard split to show e2h generalization.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|} \hline Weak & Strong & Data Separation & Random & \multicolumn{3}{c|}{Data Separation: Easy-Hard} \\ Model & Model & Weak Model & Strong Model & W2S Performance & \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c|}{ImageNet} \\ Size & Size & Performance & Performance & Baseline & Ours & Performance & Performance & Performance & Baseline & Ours & \\ \hline
70M & 160M & 0.497 & 0.817 & 0.5077 & 0.5253 & 3.35 & 0.4686 & 0.4847 & 0.48 & 0.48 & 0.4989 & 20\% \\
70M & 410M & 0.4987 & 0.602 & 0.5077 & 0.5253 & 3.35 & 0.4885 & 0.4894 & 0.4631 & 0.4923 & 6\% \\ \hline
70M & 18 & 0.4987 & 0.627 & 0.5051 & 0.5253 & 4.0486 & 0.4758 & 0.4758 & 0.5026 & 5.6\% \\
70M & 1.481 & 0.4987 & 0.626 & 0.5025 & 0.5153 & 2.4286 & 0.4974 & 0.4719 & 0.5089 & 7.8\% \\ \hline
70M & 2.88 & 0.4987 & 0.7513 & 0.5 & 0.5115 & 2.39 & 0.4827 & 0.5 & 0.4923 & 0.4987 & 1.3\% \\
70M & 1.00M & 0.5013 & 0.6008 & 0.35 & 0.5214 & 5.67 & 0.4894 & 0.4923 & 0.4535 & 0.4534 & 1.6\% \\ \hline
160M & 18 & 0.5013 & 0.6403 & 0.5025 & 0.532 & 4.14 & 0.4872 & 0.5 & 0.4831 & 0.4898 & 4.6\% \\ \hline
160M & 1.48 & 0.5013 & 0.713 & 0.5077 & 0.5127 & 2.84 & 0.4686 & 0.4501 & 0.4906 & 0.5038 & 2.1\% \\
160M & 2.88 & 0.5013 & 0.717 & 0.5077 & 0.5315 & 1.35 & 0.4847 & 0.4847 & 0.4894 & 0.5125 & 3.6\% \\ \hline
410M & 18 & 0.5689 & 0.6400 & 0.551 & 0.551 & 0.6 & 0.4936 & 0.4911 & 0.4921 & 0.5179 & 5.2\% \\ \hline
410M & 1.48 & 0.5778 & 0.626 & 0.3816 & 0.5918 & 1.8 & 0.4936 & 0.4849 & 0.5268 & 0.5377 & 1.9\% \\ \hline
410M & 2.88 & 0.5561 & 0.7551 & 0.5399 & 0.611 & 9.15 & 0.5031 & 0.5115 & 0.5434 & 0.5485 & 0.3\% \\ \hline
18 & 1.48 & 0.6084 & 0.6888 & 0.5982 & 0.6288 & 5.15 & 0.5472 & 0.4923 & 0.5356 & 0.574 & 3.7\% \\ \hline
18 & 2.88 & 0.6197 & 0.7398 & 0.6288 & 0.6543 & 4.19 & 0.5497 & 0.5026 & 0.5855 & 0.5957 & 1.7\% \\ \hline
1.48 & 2.88 & 0.6099 & 0.7538 & 0.6026 & 0.713 & 2.9\% & 0.588 & 0.477 & 0.6611 & 0.6288 & 2.1\% \\ \hline \end{tabular}
\end{table}
Table 5: This table shows weak to strong generalization using random as well as easy-hard data-splits for quartz dataset. As compared to previous tables 3 and 4, here we run experiment once and note the improvement of our method with respect to the baseline.

\begin{table}
\begin{tabular}{l l l l l l l l}  & \multicolumn{2}{c}{**WorkModel**} & \multicolumn{4}{c}{**Storage Model**} \\ \cline{2-9}  & \multicolumn{1}{c}{Token-Arg Acc} & \multicolumn{1}{c}{Option Acc} & \multicolumn{1}{c}{Option Acc(on w2s)} & \multicolumn{1}{c}{\(\alpha\)} & \multicolumn{1}{c}{grade} & \multicolumn{1}{c}{Token-Arg Acc} & \multicolumn{1}{c}{Option Acc} \\  & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} \\ Baseline & \(81.7\pm 0.06\) & \(22.5\pm 0.33\) & \(27.85\pm 0.57\) & \(10.45\pm 0.02\) & \(22.3\pm 0.16\) & \(17.88\pm 0.11\) & \(22.7\pm 0.32\) \\ With Adaboot (T-G3) & \(13.3\pm 0.54\) & \(22.81\pm 0.29\) & \(27.78\pm 0.46\) & \(10.35\pm 0.02\) & \(22.3\pm 0.16\) & \(17.87\pm 0.17\) & \(22.56\pm 0.06\) \\ \multicolumn{9}{l}{} \\ Baseline & \(81.7\pm 0.06\) & \(22.5\pm 0.33\) & \(27.85\pm 0.57\) & \(10.45\pm 0.01\) & \(19.28\pm 0.15\) & \(28.92\pm 0.14\) & \(17.06\pm 0.31\) \\ With Adaboot (T-G4) & \(14.5\pm 0.72\) & \(22.93\pm 0.17\) & \(27.96\pm 0.46\) & \(10.32\pm 0.10\) & \(19.28\pm 0.15\) & \(28.84\pm 0.05\) & \(18.09\pm 0.07\) \\ Baseline & \(81.7\pm 0.06\) & \(22.5\pm 0.33\) & \(27.85\pm 0.57\) & \(10.45\pm 0.01\) & \(21.5\pm 0.42\) & \(20.55\pm 0.13\) & \(19.96\pm 0.15\) \\ With Adaboot (T-G5) & \(12.95\pm 0.88\) & \(22.58\pm 0.38\) & \(28.03\pm 0.21\) & \(10.35\pm 0.02\) & \(21.56\pm 0.24\) & \(31.84\pm 0.08\) & \(20.45\pm 0.06\) \\  & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} \\ Baseline & \(83.3\pm 0.1\) & \(22.51\pm 0.42\) & \(27.37\pm 0.42\) & \(10.45\pm 0.01\) & \(21.76\pm 0.14\) & \(32.99\pm 0.04\) & \(20.45\pm 0.24\) \\ With Adaboot (T-G4) & \(12.65\pm 0.05\) & \(23.24\pm 0.06\) & \(28.32\pm 0.76\) & \(10.33\pm 0.01\) & \(21.67\pm 0.14\) & \(32.99\pm 0.17\) & \(21.28\pm 0.02\) \\  & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} \\ Baseline & \(83.3\pm 0.1\) & \(23.24\pm 0.23\) & \(27.19\pm 0.47\) & \(10.45\pm 0.01\) & \(26.59\pm 0.13\) & \(35.98\pm 0.09\) & \(22.78\pm 0.51\) \\ With Adaboot (T-G7) & \(12.48\pm 0.15\) & \(23.26\pm 0.22\) & \(28.27\pm 0.14\) & \(10.37\pm 0.01\) & \(26.59\pm 0.13\) & \(35.86\pm 0.28\) & \(23.15\pm 0.2\) \\  & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} \\ Baseline & \(17.46\pm 0.16\) & \(21.73\pm 0.35\) & \(26.65\pm 0.1\) & \(0.61\pm 0.0\) & \(19.11\pm 0.37\) & \(28.8\pm 0.23\) & \(18.15\pm 0.15\) \\ With Adaboot (T-G4) & \(20.57\pm 0.4\) & \(22.16\pm 0.2\) & \(27.19\pm 0.5\) & \(2.22\pm 0.02\) & \(19.11\pm 0.37\) & \(28.9\pm 0.11\) & \(18.43\pm 0.04\) \\  & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} \\ Baseline & \(17.46\pm 0.16\) & \(21.73\pm 0.35\) & \(26.65\pm 0.1\) & \(0.61\pm 0.0\) & \(21.59\pm 0.07\) & \(32.60\pm 0.06\) & \(19.63\pm 0.1\) \\ With Adaboot (T-G2) & \(20.47\pm 0.09\) & \(22.77\pm 0.27\) & \(27.31\pm 0.51\) & \(0.94\pm 0.1\) & \(21.59\pm 0.07\) & \(32.07\pm 0.12\) & \(20.17\pm 0.14\) \\  & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} & \multicolumn{1}{c}{Publish-100} \\ Baseline & \(17.6\pm 0.07\) & \(22.84\pm 0.58\) & \(27.79\pm 0.64\) & \(9.61\pm 0.0\) & \(22.33\pm 0.34\) & \(33.11\pm 0.1\) & \(21.19\pm 0.15\) \\ With Adaboot (T-G3) & \(20.31\pm 0.24\) & \(22.5\pm 0.36\) & \(27.79\pm 0.42\) & \(0.27\pm 0.06\) & \(22.33\pm 0.34\) & \(33.3

## Appendix H Broader Impact

The proposed framework for weak-to-strong (w2s) generalization using ensembles of weak language models (LLMs) has significant implications across various domains. By demonstrating that multiple weak supervisors can effectively train more powerful models, our research addresses the critical challenge of superalignment, potentially transforming how advanced AI systems are developed and supervised. This approach could democratize access to powerful AI technologies by reducing reliance on scarce, high-quality labeled data and enabling more inclusive participation in AI development. Furthermore, our method encourages the creation of robust AI systems capable of tackling complex problems, which can drive advancements in fields such as healthcare, education, and scientific research. However, careful consideration must be given to ethical implications, ensuring that the deployment of these advanced models aligns with societal values and mitigates risks associated with misuse or unintended consequences.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|} \hline \multirow{2}{*}{Weak Model Size (Pythia)} & \multicolumn{3}{l|}{Strong Model} & \multicolumn{3}{l|}{Data Separation: Random} & \multicolumn{3}{l|}{Data Separation: Easy-Hard} \\  & \multicolumn{3}{l|}{Size Model} & \multicolumn{3}{l|}{Size Model} & \multicolumn{3}{l|}{Single Model} & \multicolumn{3}{l|}{W2S Performance} & \multicolumn{3}{l|}{Imppor (vs)} & \multicolumn{3}{l|}{Wide Mask: Model} & \multicolumn{3}{l|}{W3S Performance} & \multicolumn{3}{l|}{Imppor (vs)} \\  & \multicolumn{3}{l|}{Performance} & \multicolumn{3}{l|}{Performance} & \multicolumn{3}{l|}{Baseline} & \multicolumn{3}{l|}{Ours} & \multicolumn{3}{l|}{Performance} & \multicolumn{3}{l|}{Performance} & \multicolumn{3}{l|}{Baseline} & \multicolumn{3}{l|}{Ours} \\ \hline \multicolumn{1}{|c|}{TQM} & 108M & 0.251 & 0.258 & 0.2457 & 0.344 & -0.7 & 0.16 & 0.221 & 0.2201 & 0.2244 & 2 \\ \hline \multicolumn{1}{|c|}{TQM} & 4.04M & 0.2599 & 0.2847 & 0.2683 & 0.273 & 1.6 & 0.16 & 0.3894 & 0.162 & 0.1733 & 6.6 \\ \multicolumn{1}{|c|}{TQM} & 18 & 0.2581 & 0.3114 & 0.2588 & 0.2575 & 0.6 & 0.16 & 0.3977 & 0.162 & 0.2048 & 4.4 \\ \hline \multicolumn{1}{|c|}{TQM} & 1.48 & 0.2581 & 0.3166 & 0.3927 & 0.3000 & 2.6 & 0.16 & 0.2124 & 0.145 & 0.2173 & 9.7 \\ \hline \multicolumn{1}{|c|}{TQM} & 2.28 & 0.2483 & 0.3522 & 0.3166 & 0.3983 & 3.3 & 0.16 & 0.2671 & 0.2199 & 0.2321 & 7.5 \\ \hline \multicolumn{1}{|c|}{TQM} & 2.40M & 0.2423 & 0.2591 & 0.2516 & 0.258 & 1.2 & 0.17 & 0.1826 & 0.1792 & 0.1844 & 2.3 \\ \hline \multicolumn{1}{|c|}{TQM} & 18 & 0.2423 & 0.3157 & 0.2378 & 0.2588 & 2.7 & 0.17 & 0.2158 & 0.1945 & 0.2348 & 5.3 \\ \hline \multicolumn{1}{|c|}{TQM} & 1.48 & 0.2512 & 0.3524 & 0.3038 & 0.3166 & 4.2 & 0.17 & 0.2287 & 0.2082 & 0.2175 & 2.1 \\ \hline \multicolumn{1}{|c|}{TQM} & 2.28 & 0.3242 & 0.3541 & 0.3098 & 0.3538 & 5.8 & 0.17 & 0.26 & 0.2218 & 0.2238 & 5.4 \\ \hline \multicolumn{1}{|c|}{TQM} & 1.48 & 0.2799 & 0.3124 & 0.2884 & 0.2958 & 1.8 & 0.1999 & 0.2133 & 0.2055 & 0.2031 & 1.3 \\ \hline \multicolumn{1}{|c|}{TQM} & 1.48 & 0.2799 & 0.3528 & 0.3184 & 0.3232 & 2.4 & 0.1999 & 0.2279 & 0.2499 & 0.2176 & 4.1 \\ \hline \multicolumn{1}{|c|}{TQM} & 2.28 & 0.2739 & 0.3643 & 0.3187 & 0.3235 & 1.3 & 0.1999 & 0.2654 & 0.2277 & 0.2415 & 6.4 \\ \hline \multicolumn{1}{|c|}{TQM} & 1.48 & 0.3603 & 0.3577 & 0.3059 & 0.3174 & 4.3 & 0.2760 & 0.2277 & 0.2142 & 0.2167 & 1.2 \\ \hline \multicolumn{1}{|c|}{TQM} & 2.88 & 0.3012 & 0.3481 & 0.3236 & 1.5 & 0.1958 & 0.2594 & 0.2355 & 0.2372 & 0.37 \\ \hline \multicolumn{1}{|c|}{TQM} & 2.88 & 0.3119 & 0.3481 & 0.3148 & 0.3294 & 4.6 & 0.2125 & 0.2517 & 0.2360 & 0.24387 & 6.6 \\ \hline \end{tabular}
\end{table}
Table 8: This table shows weak to strong generalization using random as well as easy-hard data-splits for ARC dataset. As compared to previous tables 6 and 7, here we run experiment once and note the improvement of our method with respect to the baseline.