# Perceptual Kalman Filters: Online State Estimation

under a Perfect Perceptual-Quality Constraint

 Dror Freirich

Technion - Israel

Institute of Technology

drorfrc@gmail.com &Tomer Michaeli

Technion - Israel

Institute of Technology

tomer.m@ee.technion.ac.il &Ron Meir

Technion - Israel

Institute of Technology

rmeir@ee.technion.ac.il

###### Abstract

Many practical settings call for the reconstruction of temporal signals from corrupted or missing data. Classic examples include decoding, tracking, signal enhancement and denoising. Since the reconstructed signals are ultimately viewed by humans, it is desirable to achieve reconstructions that are pleasing to human perception. Mathematically, perfect perceptual-quality is achieved when the distribution of restored signals is the same as that of natural signals, a requirement which has been heavily researched in static estimation settings (i.e. when a whole signal is processed at once). Here, we study the problem of optimal _causal_ filtering under a perfect perceptual-quality constraint, which is a task of fundamentally different nature. Specifically, we analyze a Gaussian Markov signal observed through a linear noisy transformation. In the absence of perceptual constraints, the Kalman filter is known to be optimal in the MSE sense for this setting. Here, we show that adding the perfect perceptual quality constraint (i.e. the requirement of temporal consistency), introduces a fundamental dilemma whereby the filter may have to "knowingly" ignore new information revealed by the observations in order to conform to its past decisions. This often comes at the cost of a significant increase in the MSE (beyond that encountered in static settings). Our analysis goes beyond the classic innovation process of the Kalman filter, and introduces the novel concept of an unutilized information process. Using this tool, we present a recursive formula for perceptual filters, and demonstrate the qualitative effects of perfect perceptual-quality estimation on a video reconstruction problem.

## 1 Introduction

In many settings, it is desired to reconstruct a temporal signal from corrupted or missing data. Examples include decoding of transmitted communications, tracking targets based on noisy measurements, enhancing audio signals, and denoising videos. Traditionally, restoration quality has been assessed by distortion measures such as MSE. As a result, numerous methods targeted the minimization of such measures, including the seminal work of Kalman [11]. However, in applications involving human perception, one may favor reconstructions that cannot be told apart from valid signals. Mathematically, such _perfect perceptual quality_ can be achieved only if the distribution of restored signals is the same as that of "natural" signals.

Interestingly, it has been shown that good perceptual quality generally comes at the price of poor distortion and vice versa. This phenomenon, known as the _perception-distortion tradeoff_, was first studied in [3], and was later fully characterized in [8] for the particular setting where distortion is measured by MSE and perception is measured by the Wasserstein-\(2\) distance between the distribution of estimated signals and the distribution of real signals. However, to date, all existing works addressed the static (non-temporal) setting, in which the entire corrupted signal is available for processing all atonce. This setting is fundamentally different from situations involving temporal signals, in which the corrupted signal is processed causally over time, such that each sample is reconstructed only based on observations up to the current time.

To illustrate the inherent difficulty in causal estimation, consider video restoration tasks like denoising, super-resolution, or frame completion (see Fig. 1). Achieving high perceptual quality in those tasks requires generating restored videos whose spatio-temporal distribution matches that of natural videos. Particularly, an incorrect temporal distribution may lead to flickering artifacts [14] or to unnaturally slow dynamics [4]. To comply with this requirement, the restoration method needs to 'hallucinate' motion whenever the dynamics cannot be accurately determined from the measurements. For example, it may be impossible to determine whether a car is standing still or moving slowly from just a few noisy frames, yet the restoration method must generate _some_ (nearly) constant velocity in order to comply with the statistics of natural videos. However, as more measurements become available, the uncertainty may be reduced, and it may become evident that the hallucinated dynamics were in fact incorrect. When this happens, the method cannot suddenly change the motion in the output video, because such an abrupt change would deviate from natural video statistics. Thus, although the method "becomes aware" of its mistake, it may have to stick to its past decisions for a while. A natural question is, therefore:

_What is the precise cost of temporal consistency in online restoration?_

In this paper, we study this question in the setting where the signal to be restored, \(x_{t}\), is a discrete-time Gaussian Markov process, and the measurements \(y_{t}\) are noisy linear transformations of the signal's state. We address the problem of designing a _causal filter_ for estimating \(x_{t}\) from \(y_{t}\), where the distribution law of the filter's output, \(\hat{x}_{t}\), is constrained to be the same as that of \(x_{t}\) (perfect perceptual quality). We show that this temporal consistency constraint indeed comes at the cost of increased MSE compared to filters that only enforce the correct distribution per time step, but not joint distributions across time steps. To derive a recursive form for linear perceptual filters, we introduce the novel concept of an _unutilized information_ process, which is the portion of accumulated information that does not depend on past estimates. We provide a closed-form expression for the MSE of such filters and show how to design their coefficients to minimize different objectives. We further establish a special class of perceptual filters, based on the classic _innovation_ process, which has an explicit solution. We analyze the evolution of MSE over time for perceptual filters and for non-perceptual ones in several numerical setups. Finally, we demonstrate the qualitative effects of perfect perceptual-quality estimation on a simplified video reconstruction problem.

Related workMany works proposed practical algorithms for achieving high (spatio-temporal) perceptual quality in video restoration tasks. Bhattacharjee and Das [1] improved temporal coherence by using a loss that penalizes discontinuities between sequential frames. Perez-Pellitero et al. [14] suggested a recurrent generator architecture whose inputs include the low-resolution current frame (at time \(t\)), the high-resolution reconstruction of the previous one (at time \(t-1\)), and a low-resolution version of the previous frame, aligned to the current one. The model is trained using losses that encourage it to conserve the joint statistics between consecutive frames. Chu et al. [4] introduced a temporally coherent GAN architecture (TecoGAN). Their generator's input includes again a warped version of the previously generated frame, where each discriminator input consists of 3 consecutive frames, either generated or ground-truth. They also introduced a bi-directional loss that encourages long-term consistency by avoiding temporal accumulation of artifacts, and another loss which measures the similarity between motions. More recent progress in generating temporally coherent videos includes _Make-a-video_[16] that expands a text-to image model with spatio-temporal convolutional and attention layers, and Video-Diffusion Models [10] that use a

Figure 1: **The temporal consistency dilemma.** Estimation cannot suddenly change the motion in the output video, because such an abrupt change would deviate from natural video statistics. Thus, although the method is aware of its mistake, it may have to stick to its past decisions.

diffusion model with a network architecture adapted to video. In the context of online restoration, we mention the work of Kim et al. [12] which presented a GAN architecture for real-time video deblurring, where restoration is done sequentially. They introduce a network layer for dynamically (at test-time) blending features between consecutive frames. This mechanism enables generated features to propagate into future frames, thus improving consistency. We note, however, that our work is the first to provide a theoretical/mathematical framework, and a closed-form solution for a special case.

## 2 Preliminaries: The distortion-perception tradeoff

Let \(x,y\) be random vectors taking values in \(\mathbb{R}^{n_{x}}\) and \(\mathbb{R}^{n_{y}}\), respectively, with joint probability \(p_{xy}\). Suppose we want to estimate \(x\) based on \(y\), such that the estimator \(\hat{x}\) satisfies two requirements: (i) It has a low distortion \(\mathbb{E}[d(x,\hat{x})]\), where \(d(\cdot,\cdot)\) is some measure of discrepancy between signals; (ii) It has a good perceptual quality, _i.e._ it achieves a low value of \(d_{p}(p_{x},p_{\hat{x}})\), where \(d_{p}(\cdot,\cdot)\) is a divergence between probability measures. Blau and Michaeli [3] studied the best possible distortion that can be achieved under a given level of perceptual quality, by introducing the _distortion-perception_ function

\[D(P)=\min_{p_{\hat{x}|y}}\left\{\mathbb{E}[d(x,\hat{x})]\ :\ d_{p}(p_{x},p_{\hat{x}}) \leq P\right\}.\] (1)

Freirich et al. [8] provided a complete characterization of \(D(P)\) for the case where \(d\) is the squared-error and \(d_{p}\) is the Wasserstein-\(2\) distance. Particularly, in the Gaussian case, they developed a closed-form expression for the optimal estimator.

In this paper we discuss estimation with perfect perceptual quality, namely \(P=0\). In this case, [8, Thm. 4] implies that if \(x\) and \(y\) are zero-mean, jointly-Gaussian with covariances \(\Sigma_{x},\Sigma_{y}\succ 0\), and \(x^{*}=\mathbb{E}\left[x|y\right]\), then a MSE-optimal perfect perceptual-quality estimator is obtained by

\[\hat{x}=\mathscr{T}^{*}x^{*}+w,\quad\mathscr{T}^{*}\triangleq\Sigma_{x}^{ \frac{1}{2}}(\Sigma_{x}^{\frac{1}{2}}\Sigma_{x^{*}}\Sigma_{x}^{\frac{1}{2}}) ^{\frac{1}{2}}\Sigma_{x}^{-\frac{1}{2}}\Sigma_{x}^{\dagger},\] (2)

where \(w\) is a zero-mean Gaussian noise with covariance \(\Sigma_{w}=\Sigma_{x}-\mathscr{T}^{*}\Sigma_{x}^{*}\mathscr{T}^{*\top}\), independent of \(y\) and \(x\), and \(\Sigma_{x^{*}}^{\dagger}\) is the Moore-Penrose inverse of \(\Sigma_{x^{*}}\). For the more general case where \(\Sigma_{x}\succeq 0\), a similar result can be obtained by using Theorem F.2 in the Appendix.

## 3 Problem formulation

We consider a state \(x_{k}\in\mathbb{R}^{n_{x}}\) with linear dynamics driven by Gaussian noise, and observations \(y_{k}\in\mathbb{R}^{n_{y}}\) that are linear transformations of \(x_{k}\) perturbed by Gaussian noise,

\[x_{k} =A_{k}x_{k-1}+q_{k}, k=1,...,T,\] (3) \[y_{k} =C_{k}x_{k}+r_{k}, k=0,...,T.\] (4)

Here, the noise vectors \(q_{k}\sim\mathcal{N}(0,Q_{k})\) and \(r_{k}\sim\mathcal{N}(0,R_{k})\) are independent white Gaussian processes, and \(x_{0}\sim\mathcal{N}(0,P_{0})\) is independent of \(q_{1},r_{0}\). For convenience, we will sometimes refer to \(P_{0}\) as \(Q_{0}\). The matrices \(A_{k}\), \(C_{k}\), \(Q_{k}\), \(R_{k}\) and \(P_{0}\) are deterministic system parameters with appropriate dimensions, and assumed to be known.

Our goal is to construct an estimated sequence \(\hat{X}_{0}^{T}=(\hat{x}_{0},\ldots,\hat{x}_{T})\) based on the measurements \(Y_{0}^{T}=(y_{0},\ldots,y_{T})\), which minimizes the cost

\[\mathcal{C}(\hat{x}_{0},\ldots,\hat{x}_{T})=\sum_{k=0}^{T}\alpha_{k}\mathbb{E} \left[\|x_{k}-\hat{x}_{k}\|^{2}\right],\] (5)

for some given weights \(\alpha_{k}\geq 0\). Importantly, we want to do so under the following two constraints.

\[\mathrm{Temporal\ causality}: \hat{x}_{k}\sim p_{\hat{x}_{k}}(\cdot|y_{0},\ldots,y_{k},\hat{x} _{0},\ldots,\hat{x}_{k-1}),\] (6) \[\mathrm{Perfect\ perceptual\ quality}: p_{\hat{X}_{0}^{T}}=p_{X_{0}^{T}}.\] (7)

Condition (6) states that each prediction should depend only on past and present observations and on the past predictions. Note that Condition (7) requires not only that every estimated sample have the same distribution as the original one, but also that the _joint_ distribution of every subset of reconstructed samples be identical to that of the corresponding subset of samples in the original sequence. In the context of video processing, this means that not only does every recovered frame have to look natural, but also that motion must look natural. This perfect perceptual quality constraint is what sets our problem apart from the classical Kalman filtering problem, which considers only the causality constraint. Since we will make use of the Kalman filter, let us briefly summarize it.

The Kalman filter (no perceptual quality constraint)Let \(\hat{x}^{*}_{k|s}\triangleq\mathbb{E}\left[x_{k}|y_{0},\ldots,y_{s}\right]\) denote the estimator of \(x_{k}\) based on all observations up to time \(s\), which minimizes the MSE. The celebrated Kalman filter [11] is an efficient method for calculating the _Kalman optimal state_\(\hat{x}^{*}_{k}\equiv\hat{x}^{*}_{k|k}\) recursively without having to store all observations up to time \(k\). It is given by the recurrence

\[\hat{x}^{*}_{k}=A_{k}\hat{x}^{*}_{k-1}+K_{k}\mathcal{I}_{k},\] (8)

where \(K_{k}\) is the _optimal Kalman gain_[11], whose recursive calculation is given in Algo. 2 in the Appendix. The vector \(\mathcal{I}_{k}\) is the _innovation_ process,

\[\mathcal{I}_{k}=y_{k}-C_{k}\hat{x}^{*}_{k|k-1},\] (9)

describing the new information carried by the observation \(y_{k}\) over the optimal prediction based on the observations up to time \(k-1\), which is given by \(\hat{x}^{*}_{k|k-1}=A_{k}\hat{x}^{*}_{k-1}\). The innovation \(\mathcal{I}_{k}\) is uncorrelated with all observations up to time \(k-1\), which guaranties the MSE optimality of the estimation. The calculation of the Kalman state is also summarized in Alg. 2. Pay attention to the innovation process \(\mathcal{I}_{k}\), its covariance \(S_{k}\) and gain \(K_{k}\), which we will build upon. Our notations are summarized in Table 2 in the Appendix. Note that since the Kalman filter minimizes the MSE at each timestep, it also minimizes (5) regardless of the choice of \(\alpha_{k}\), but it generally fails to fulfill (7). As we will see later, when taking (7) into consideration, the choice of \(\alpha_{k}\) does affect the optimal filter.

Temporally-inconsistent perceptual filterA naive way to try to improve the perceptual quality of the Kalman filter would be to require that each \(\hat{x}_{k}\) be distributed like \(x_{k}\) (but without constraining the joint distribution of samples). In the context of video processing, each frame generated by such a filter would look natural, but motion would not necessarily look natural. This problem can be solved using the result (2), which gives the optimal "temporally-inconsistent" perceptual estimator

\[\hat{x}^{\text{tic}}_{k}=\mathscr{T}^{*}_{k}\hat{x}^{*}_{k}+w_{k}=\mathscr{T}^ {*}_{k}\left(A_{k}\hat{x}^{*}_{k-1}+K_{k}\mathcal{I}_{k}\right)+w_{k},\] (10)

with \(\mathscr{T}^{*}_{k}\) and \(w_{k}\) from (2). These quantities depend only on the covariances of \(x_{k},\hat{x}^{*}_{k}\), which can be computed recursively using the Kalman method. The MSE of this estimator is given by (see [8])

\[\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]=d^{*}_{k}+\operatorname{Tr} \left\{\Sigma_{x_{k}}+\Sigma_{\hat{x}^{*}_{k}}-2\left(\Sigma^{\frac{1}{2}}_{ \hat{x}_{k}}\Sigma_{\hat{x}^{*}_{k}}\Sigma^{\frac{1}{2}}_{\hat{x}_{k}}\right) ^{\frac{1}{2}}\right\},\] (11)

where \(d^{*}_{k}\) is the MSE of the Kalman filter, which can also be computed recursively.

Our setting (with the perceptual quality constraint)Going back to our setting, one may readily recognize that perceptually reconstructing the signal \(X^{T}_{0}\) from the full measurement sequence \(Y^{T}_{0}\) is also a special case of the Gaussian perceptual restoration problem discussed in Section 2, only applied to the entire sequence of states and measurements. Generally, this estimate already achieves a higher MSE than the estimate that minimizes the MSE without the perceptual constraint. However, in our setting we have the additional causality constraint (6). Requiring both constraints (7) and (6) might incur an additional cost, as illustrated by the following example, where applying each one of them does not restrict the optimal solution, but together they result in a higher MSE.

_Example 3.1_.: Let \(T=1\) and consider the process \((x_{0},x_{1})=(q_{0},q_{0})\), where \(q_{0}\sim\mathcal{N}(0,1)\), with observations \((y_{0},y_{1})=(0,x_{1})\). Assume we want to minimize the error at time \(k=1\) (namely \((\alpha_{0},\alpha_{1})=(0,1)\) in (5)). Then, considering only the causality constraint (6), the estimator \((\hat{x}_{0},\hat{x}_{1})=(y_{0},y_{1})\) is optimal. Indeed, it is causal and it achieves zero MSE. Similarly, considering only the perceptual quality constraint (7), the estimator \((\hat{x}_{0},\hat{x}_{1})=(y_{1},y_{1})\) is optimal. Indeed, it is distributed like \((x_{0},x_{1})\) and it also achieves zero MSE. However, when demanding both conditions, \(\hat{x}_{0}\) must be based on no information to obey (6) (as \(y_{0}=0\)), and it must be drawn from the prior distribution \(\mathcal{N}(0,1)\) in order to be distributed like \(x_{0}\) and obey (7). Furthermore, to satisfy (7), we must also have \(\hat{x}_{1}=\hat{x}_{0}\). Therefore, the optimal estimator in this case is \((\hat{x}_{0},\hat{x}_{1})=(\tilde{q}_{0},\tilde{q}_{0})\), where \(\tilde{q}_{0}\sim\mathcal{N}(0,1)\) is independent of \(q_{0}\). The MSE achieved by this estimator is \(2\).

## 4 Perfect perceptual-quality filters

The perceptual constraint (7) dictates that the estimator must be of the form

\[\hat{x}_{k}=A_{k}\hat{x}_{k-1}+J_{k},\quad\hat{x}_{0}=J_{0},\] (12)where \(J_{k}=\hat{x}_{k}-A_{k}\hat{x}_{k-1}\) is distributed as \(\mathcal{N}(0,Q_{k})\) and is independent of \(\hat{X}_{0}^{k-1}\). Note the similarity between (12) and the MSE-optimal state (8), in which \(J_{k}=K_{k}\mathcal{I}_{k}\). Here, however, this choice is not valid due to the constraint on the output distribution. In terms of temporal consistency, an estimator of the form (12) guarantees that previously presented features obey the natural dynamics of the domain, while newly generated estimates do not contradict the previous ones. In order to maintain causality (6), \(J_{k}\) must be of the form

\[J_{k}\sim p_{J_{k}}(\cdot|y_{0},\ldots,y_{k},\hat{x}_{0},\ldots,\hat{x}_{k-1}),\] (13)

_i.e._, \(J_{k}\) is independent of future observations \(Y_{k+1}^{T}\) given \(Y_{0}^{k}\). As a consequence, \(J_{k}\) is uncorrelated with \(\mathcal{I}_{k+n}\) for all \(n\geq 1\).

We now discuss linear estimators, where \(Y_{0}^{T}\) and \(J_{0}^{T}\) (hence \(\hat{X}_{0}^{T}\)) are jointly Gaussian. Our first result is as follows (see proof in App. B).

**Theorem 4.1**.: _Under the cost (5), there exists a linear optimal estimator of the form_

\[J_{k}=\pi_{k}\mathcal{I}_{k}+\phi_{k}\upsilon_{k}+w_{k},\] (14)

\(\pi_{k}\in\mathbb{R}^{n_{x}\times n_{y}}\) _and \(\phi_{k}\in\mathbb{R}^{n_{x}\times(kn_{y})}\) are the filter's coefficients, \(w_{k}\) is an independent Gaussian noise with covariance \(\Sigma_{w_{k}}=Q_{k}-\pi_{k}S_{k}\pi_{k}^{\top}-\phi_{k}\Sigma_{\upsilon_{k}} \phi_{k}^{\top}\succeq 0\), \(\upsilon_{k}\) is the process of unutilized information_

\[\upsilon_{k}\triangleq\mathcal{I}_{0}^{k-1}-\mathbb{E}\left[\mathcal{I}_{0}^{ k-1}\middle|\hat{X}_{0}^{k-1}\right],\,\mathcal{I}_{0}^{k-1}=\mathrm{Column} \left\{\mathcal{I}_{0},\ldots,\mathcal{I}_{k-1}\right\},\upsilon_{0}=0.\] (15)

Note that with this form for \(J_{k}\), the state \(\hat{x}_{k}\) is indeed a function of the observations \(Y_{0}^{k}\) and the previous states \(\hat{X}_{0}^{k-1}\). Intuitively, \(\upsilon_{k}\) is the part of the information in the observations, which has no correlation with the information used to construct the past estimates \(\hat{X}_{0}^{k-1}\). Thus, from the standpoint of the filter's output, this information has not yet been introduced. As opposed to the innovation \(\mathcal{I}_{k}\), the process \(\upsilon_{k}\) is not white, and it is affected by the choices of \(\pi_{t}\) and \(\phi_{t}\) up to time \(k-1\). However, \(\mathcal{I}_{k}\) is always independent of \(\upsilon_{k}\), since \(\mathcal{I}_{k}\) is independent of \(\mathcal{I}_{0}^{k-1}\) and \(J_{0}^{k-1}\), which constitute \(\upsilon_{k}\).

The filter of Theorem 4.1 is causal but not recursive. Specifically, although it is possible to obtain \(\upsilon_{k+1},\Sigma_{\upsilon_{k+1}}\) given the coefficients \(\{\pi_{t},\phi_{t}\}_{t=0}^{k}\) (see App. E), the dimension of \(\upsilon_{k}\) grows with time (it is \(kn_{y}\)), thus increasing the cost of computing \(\phi_{k}\upsilon_{k}\). Furthermore, determining the coefficients \(\{\pi_{k},\phi_{k}\}_{k=0}^{T}\) that minimize the objective (5) (which is done a-priori in an offline manner) requires solving a large optimization problem, as the total size of all coefficients is \(\mathcal{O}(n_{x}n_{y}T^{2})\). To efficiently optimize these coefficients, we next suggest two simplified versions of this form, which may generally be sub-optimal but easier to optimize.

_Remark 4.2_.: A remark is in place regarding objectives beyond the squared-error. While (14) forms an optimal filter under the cost (5), it can be considered as a representation for linear filters in general. The constraints on the coefficients (\(Q_{k}-\pi_{k}S_{k}\pi_{k}^{\top}-\phi_{k}\Sigma_{\upsilon_{k}}\phi_{k}^{\top} \succeq 0\)) are necessary and sufficient for perfect perception, regardless of the cost objective. It is therefore possible to optimize coefficients for objectives other than MSE under these constraints to obtain optimal perceptual _linear_ filters. Optimization may be performed numerically (when the cost is tractable) or in an online fashion, assuming access to ground-truth samples. Note, however, that under general distortion measures, optimal filters might be non-linear.

Figure 2: **Recursive perceptual filtering (Sec. 4.1).** The state estimator \(\hat{x}_{k}\) consists of the previous state \(\hat{x}_{k-1}\), and the innovation and unutilized information processes. The unutilized information state \(\mathcal{I}_{k+1}\) is updated using the previously unutilized information \(\mathcal{I}_{k}\) and the newly-arriving information \(\mathcal{I}_{k}\). The currently utilized information, arriving from \(J_{k}\), is then subtracted from \(\mathcal{I}_{k+1}\).

### Recursive-form filters

The optimal Kalman state \(\hat{x}_{k}^{*}\) achieves the minimal possible MSE, given by \(d_{k}^{*}=\mathbb{E}\left[\|\hat{x}_{k}^{*}-x_{k}\|^{2}\right]=\mathrm{Tr}\left\{P _{k|k}\right\}\), where \(P_{k|k}\) is the error covariance, given explicitly in Alg. 2. By the orthogonality principle (see _e.g._[8, Lemma 2]), any other estimator \(\hat{x}_{k}\) based on the observations \(Y_{0}^{k}\), satisfies

\[\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]=d_{k}^{*}+\mathbb{E}\left[\| \hat{x}_{k}-\hat{x}_{k}^{*}\|^{2}\right].\] (16)

Now, consider an estimator \(\hat{x}_{k}\) of the form (12), and let \(D_{k}\triangleq\mathbb{E}[(\hat{x}_{k}^{*}-\hat{x}_{k})(\hat{x}_{k}^{*}-\hat{x }_{k})^{\top}]\). Since we choose \(J_{k}\) to be normally distributed and independent of \(\hat{x}_{k-1}\), it is easy to see that \(D_{k}\) obeys the _Lyapunov difference equation_

\[D_{k}= A_{k}D_{k-1}A_{k}^{\top}+K_{k}S_{k}K_{k}^{\top}+Q_{k}\] \[-\mathbb{E}[J_{k}\mathcal{I}_{k}^{\top}]K_{k}^{\top}-K_{k} \mathbb{E}[\mathcal{I}_{k}J_{k}^{\top}]-A_{k}\mathbb{E}[\hat{x}_{k-1}^{*}J_{k }^{\top}]-\mathbb{E}[J_{k}\hat{x}_{k-1}^{*\top}]A_{k}^{\top}.\] (17)

As we see, the choice of \(J_{k}\) affects current (and future) errors by its correlation with the two independent components, \((A_{k}\hat{x}_{k-1}^{*},K_{k}\mathcal{I}_{k})\). Let us now consider filters of the form

\[J_{k}=\Phi_{k}A_{k}\Upsilon_{k}+\Pi_{k}K_{k}\mathcal{I}_{k}+w_{k},\quad w_{k} \sim\mathcal{N}\left(0,\Sigma_{w_{k}}\right),\] (18)

where here, by slight abuse of notation, we define the process of unutilized information as

\[\Upsilon_{k}\triangleq\hat{x}_{k-1}^{*}-\mathbb{E}\left[\hat{x}_{k-1}^{*}| \hat{x}_{0},\ldots,\hat{x}_{k-1}\right],\quad\Upsilon_{0}=0.\] (19)

The matrices \(\Pi_{k},\Phi_{k}\in\mathbb{R}^{n_{x}\times n_{x}}\) are coefficients such that

\[\Sigma_{w_{k}}=Q_{k}-\Phi_{k}A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{\top}\Phi_{k}^{ \top}-\Pi_{k}M_{k}\Pi_{k}^{\top}\succeq 0,\] (20)

where we denote the Kalman update covariance by \(M_{k}\triangleq K_{k}S_{k}K_{k}^{\top}\). This guarantees that \(J_{k}{\sim}N(0,Q_{k})\), as desired. Importantly, as opposed to \(v_{k}\), the dimension of \(\Upsilon_{k}\) is _fixed_, namely it does not grow with time \(k\). Note that since \(\hat{x}_{k-1}^{*}\) is a linear combination of \((\mathcal{I}_{0},\ldots,\mathcal{I}_{k-1})\), (18) is a special choice of \(\pi_{k}\) and \(\phi_{k}\) in (14) where coefficient size does not grow with \(k\) as well. \(\mathcal{I}_{k}\) and its covariance \(\Sigma_{\Upsilon_{k}}\) are given via a recursive form, illustrated in Fig. 2 (and derived in App. E):

\[\Upsilon_{k+1}=A_{k}\Upsilon_{k}+K_{k}\mathcal{I}_{k}-\Psi_{k}Q_{k }^{\dagger}J_{k},\,\Sigma_{\Upsilon_{k+1}}=A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{ \top}+M_{k}-\Psi_{k}Q_{k}^{\dagger}\Psi_{k}^{\top},\] (21) \[\Psi_{k}\triangleq M_{k}\Pi_{k}^{\top}+A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{ \top}\Phi_{k}^{\top}.\] (22)

Note again that unlike the innovation \(\mathcal{I}_{k}\), \(\Upsilon_{k}\) might not be a white process, but we have that \(\Upsilon_{k}\) is independent of the filter's output \(\hat{X}_{0}^{k-1}\) and \(\mathcal{I}_{k}\). Equation (17) now takes the form

\[D_{k}= A_{k}D_{k-1}A_{k}^{\top}+Q_{k}+M_{k}-\Pi_{k}M_{k}-M_{k}\Pi_{k}^{\top}-A _{k}\Sigma_{\Upsilon_{k}}A_{k}^{\top}\Phi_{k}^{\top}-\Phi_{k}A_{k}\Sigma_{ \Upsilon_{k}}A_{k}^{\top},\] (23)

where we observe that \(\Sigma_{\Upsilon_{k}}\) may depend on the choice of \(\left\{\Pi_{t},\Phi_{t}\right\}_{t=0}^{k-1}\). In order to retrieve an optimal filter, one should perform optimization of the desired objective over \(\left\{\Pi_{t},\Phi_{t}\right\}_{t=0}^{T}\), under the constraints given in (20). From (16), minimizing the cost (5) boils down to minimizing \(\sum_{k=0}^{T}\alpha_{k}\mathrm{Tr}\left\{D_{k}\right\}\) subject to the constraints in (20), which is an optimization problem over only \(\mathcal{O}(n_{x}^{2}T)\) parameters.

### An Exactly solvable reduction: Perceptual Kalman Filter

We now consider an additional reduction, which allows to obtain a closed form solution for the filter's coefficients. Specifically, a reduced-size filter can be obtained by using the form (12) and (18) with the sub-optimal choice \(\Phi_{k}\equiv 0\), namely

\[J_{k}=\Pi_{k}K_{k}\mathcal{I}_{k}+w_{k}.\] (24)

The meaning of this choice is that only newly-observed information is used for updating estimation at each stage, while non-utilized information from previous time steps is discarded. We note that such a simplification should be used with discretion; while requiring only half of the computations, rejecting the unutilized information might lead to enhanced errors in some settings (_e.g._ when an observation is missing). However, in many cases where observations are informative and different timesteps are weakly correlated, past unutilized information rapidly becomes irrelevant and can be safely ignored. We demonstrate the utility of this reduction in Sec. 5. Here, \(\Pi_{k}\) is a \(n_{x}\times n_{x}\) coefficient matrix, and\(w_{k}\sim\mathcal{N}(0,Q_{k}-\Pi_{k}M_{k}\Pi_{k}^{\top})\) is a Gaussian noise, uncorrelated with all other states, observations and noises in the system up to time \(k\). Again, note that \(\mathcal{I}_{k}\) is independent of the measurements up to time \(k-1\), hence this choice makes \(J_{k}\) independent of \(\hat{X}_{0}^{k-1}\). These innovation-based corrections resemble the mechanism exploited in (8), hence we will refer to optimal filters of the form (24) as _perceptual_ Kalman filters (PKF).

Now, by a straightforward substitution, (17) becomes

\[D_{k}=A_{k}D_{k-1}A_{k}^{\top}+Q_{k}+M_{k}-\Pi_{k}M_{k}-M_{k}\Pi_{k}^{\top}, \quad k=0,\ldots,T,\] (25)

where we consider \(Q_{0}=P_{0}\), \(M_{0}=\Sigma_{\hat{x}_{0}^{*}}\) and \(D_{-1}=0\). As before, minimizing (5) boils down to minimizing \(\sum_{k=0}^{T}\alpha_{k}\mathrm{Tr}\left\{D_{k}\right\}\), and in order for (24) to be well-defined, we should enforce the constraints \(Q_{k}-\Pi_{k}M_{k}\Pi_{k}^{\top}\succeq 0\), \(k=0,\ldots,T\). For simplicity, we consider the time-invariant case where \(A_{k}\equiv A\), so that the optimization objective becomes

\[\begin{cases}\min_{\{\Pi_{k}\}_{k=0}^{T}}&\sum_{k=0}^{T}\alpha_{k}\mathrm{Tr} \left\{D_{k}\right\}\\ \mathrm{s.t.}&D_{k}=\sum_{k=0}^{k}A^{k-t}\left[\mathcal{Q}_{t}-\Pi_{t}M_{t}-M _{t}\Pi_{t}^{\top}\right]\left(A^{\top}\right)^{k-t},\quad k=0,\ldots,T,\\ &Q_{k}-\Pi_{k}M_{k}\Pi_{k}^{\top}\succeq 0,\quad k=0,\ldots,T,\end{cases}\] (26)

where we denoted \(\mathcal{Q}_{k}=Q_{k}+M_{k}\). Substituting \(D_{k}\), we can rewrite the objective as

\[\sum_{k=0}^{T}\mathrm{Tr}\left\{\sum_{t=k}^{T}\alpha_{t}A^{t-k}\left[\mathcal{ Q}_{k}-2\Pi_{k}M_{k}\right]\left(A^{\top}\right)^{t-k}\right\}.\] (27)

As we can see, optimization over a particular coefficient \(\Pi_{k}\) does not affect other summands of the external sum. Therefore, each \(\Pi_{k}\) can be optimized separately. Minimizing the cost at the \(k\)-th step is equivalent to

\[\max_{\Pi_{k}}\mathrm{Tr}\left\{\Pi_{k}M_{k}\sum_{t=k}^{T}\alpha_{t}(A^{t-k}) ^{\top}A^{t-k}\right\}\quad\mathrm{s.t.}\quad Q_{k}-\Pi_{k}M_{k}\Pi_{k}^{\top }\succeq 0.\] (28)

Let us denote \(B_{k}\triangleq\sum_{t=k}^{T}\alpha_{t}(A^{t-k})^{\top}A^{t-k}=\alpha_{k}I+A^ {\top}B_{k+1}A\). As we now show, this optimization problem possesses a closed-form solution under a mild assumption (which is satisfied e.g. when \(Q_{k}\succ 0\)). The proof is given in Appendix F.

**Theorem 4.3**.: _Assume that \(\mathrm{Im}\left\{B_{k}M_{k}B_{k}\right\}\subseteq\mathrm{Im}\left\{Q_{k} \right\}\) for every \(k\). Let \(M_{B}\triangleq B_{k}M_{k}B_{k}\) and denote \(\Omega=\left\{\Pi_{k}:Q_{k}-\Pi_{k}M_{k}\Pi_{k}^{\top}\succeq 0\right\}\). Then the optimal value in (28) is given by_

\[\max_{\Pi_{k}\in\Omega}\mathrm{Tr}\left\{\Pi_{k}M_{k}B_{k}\right\}=\mathrm{Tr }\left\{\left(M_{B}^{1/2}Q_{k}M_{B}^{1/2}\right)^{\frac{1}{2}}\right\},\] (29)

_and is achieved by the optimal coefficient (which is generally not unique)_

\[\Pi_{k}^{*}=Q_{k}M_{B}^{1/2}\left(M_{B}^{1/2}Q_{k}M_{B}^{1/2}\right)^{1/2!}M_{ B}^{1/1/2}B_{k}.\] (30)

For a closed form solution under the alternative assumption that \(\mathrm{Im}\left\{M_{k}\right\}\subseteq\mathrm{Im}\left\{Q_{k}\right\}\), as well as a discussion of stationary filters, please see the Appendix. The Perceptual Kalman filter (PKF) obtained from Thm. 4.3 is summarized in Alg. 1.

Numerical demonstrations 1

Footnote 1: Our code is publicly available at https://github.com/ML-group-il/perceptual-kalman-filters.

We now revisit our main question: _what is the cost of temporal consistency in online restoration?_ In addition, as we have seen in Sec. 4.2, the relaxation \(\varPhi_{k}=0\), yielding the Perceptual Kalman filters, reduces the complexity of computation, possibly at the cost of higher errors. It is natural, then, to ask what is the cost of this simplification. In the following experiments, we compare the performance of several filters; \(\hat{x}^{*}_{\mathrm{kal}}\) and \(\hat{x}_{\mathrm{tic}}\) correspond to the Kalman filter and the temporally-inconsistent filter (10) (which does not possess perfect-perceptual quality). The estimate \(\hat{x}_{\mathrm{opt}}\) is a perfect-perception filter obtained by numerically optimizing the coefficients in (18), where the cost is the MSE at termination time, \(\mathcal{C}_{\mathrm{T}}=\mathbb{E}[\|\hat{x}_{T}-x_{T}\|^{2}]\). \(\hat{x}_{\mathrm{direct}}\) is a perfect-perception filter obtained by the optimization approach discussed in Appendix C, where we consider again the terminal cost. The estimates \(\hat{x}_{\mathrm{auc}},\hat{x}_{\mathrm{minT}}\) correspond to PKF outputs (Alg. 1) minimizing the _total cost_ (area under curve) \(\mathcal{C}_{\mathrm{auc}}=\sum_{k=0}^{T}\mathbb{E}[\|\hat{x}_{k}-x_{k}\|^{2}]\) and the _terminal cost_\(C_{T}\), respectively. The filters are summarized in Table 1. Full details and additional experimental results are given in App. I.

### Harmonic oscillator

We start with a simple \(2\)-D example. Specifically, we consider a harmonic oscillator, where the state \(x_{k}\in\mathbb{R}^{2}\) corresponds to position and velocity, and the observation at time \(t\) is a noisy versions of the position at time \(t-\frac{1}{2}\Delta_{t}\), where \(\Delta_{t}\) is the sampling period (see App. I for details). Figure 3 shows the MSE for \(\hat{x}_{\mathrm{opt}}\) and the sub-optimal PKF outputs \(\hat{x}_{\mathrm{auc}},\hat{x}_{\mathrm{minT}}\). The estimates \(\hat{x}^{*}_{\mathrm{kal}}\) and \(\hat{x}_{\mathrm{tic}}\) achieve lower MSE than \(\hat{x}_{\mathrm{opt}}\), however they do not possess perfect-perceptual quality. _The difference in MSE between the filters \(\hat{x}_{\mathrm{opt}}\) and \(\hat{x}_{\mathrm{tic}}\) is the cost of temporal consistency in online estimation for this setting_.

In Figure 3, we also observe that the PKF estimations are indeed not MSE optimal at time \(T=255\). However, their RMSE is only about \(30\%\) higher than that of \(\hat{x}_{\mathrm{opt}}\) and they have the advantage that they can be solved analytically and require computing only half of the coefficients (\(\Pi_{k}\)). The penalty related to this reduction may vary, depending on the exact setup. In Fig. 4 we demonstrate the relations between gaps due to temporal consistency (gap between \(\hat{x}_{\mathrm{direct}}\) and \(\hat{x}_{\mathrm{tic}}\)) and due to the reduction \(\varPhi_{k}=0\) (gap between \(\hat{x}_{\mathrm{direct}}\) and \(\hat{x}_{\mathrm{minT}}\)), in various settings based on the Harmonic Oscillator example. On the _left_ pane, dynamics are driven by \(x_{k}=\rho Ax_{k-1}+q_{k}\), where \(A\) is a fixed marginally-stable matrix, and \(\rho\) is a scalar controlling the strength of correlation between timesteps. On the _right_ pane, \(\rho=1\) and observations are given only up to time \(\tau\) (\(y_{k}\) is only noise for \(k\geq\tau\)). We observe that for systems where timesteps are strongly correlated, or in the absence of current information, discarding \(\upsilon_{k}\) leads

\begin{table}
\begin{tabular}{c c c c c}  & **description** & **def.** & \multicolumn{2}{c}{**perfect-perception**} \\  & & per-sample & temporal \\ \hline \(\hat{x}_{\mathrm{kal}}\) & Kalman filter & Alg.2 & ✗ & ✗ \\ \(\hat{x}_{\mathrm{tic}}\) & Per-sample quality & (10) & ✓ & ✗ \\ \(\hat{x}_{\mathrm{opt}}\) & Optimized filter & (18) & ✓ & ✓ \\ \(\hat{x}_{\mathrm{direct}}\) & Directly optimized & App. C & ✓ & ✓ \\ \(\hat{x}_{\mathrm{auc}}\) & \begin{tabular}{c} PKF \\ (total cost) \\ \end{tabular} & Alg. 1 & ✓ & ✓ \\ \(\hat{x}_{\mathrm{minT}}\) & 
\begin{tabular}{c} PKF \\ (terminal cost) \\ \end{tabular} & Alg.1 & ✓ & ✓ \\ \hline \end{tabular}
\end{table}
Table 1: List of demonstrated filters.

Figure 3: **MSE on Harmonic oscillator.** We observe \((i)\) the difference in distortion between the perfect-perceptual state \(\hat{x}_{\mathrm{opt}}\), optimized according to (18), and \(\hat{x}_{\mathrm{tic}}\). This additional cost is due to the perceptual constraint on the joint distribution. Also note the gap \((ii)\) between MSE of the optimized estimator and \(\hat{x}_{\mathrm{minT}}\) which is due to the sub-optimal choice of coefficients, \(\varPhi_{k}=0\).

to significantly higher errors. Whenever states are less correlated and more observations are given, unutilized information can be ignored with lower cost.

### Dynamic texture

We now illustrate the qualitative effects of perceptual estimation in a simplified video restoration setting. Specifically, we consider a video of a "dynamic texture" of waves in a lake. Such dynamic textures are accurately modeled by linear dynamics of a Gaussian latent representation [5], whose parameters we learn from a real video. Here, frames are generated from a latent \(128\)-dimensional state \(x_{k}^{FA}\) which corresponds to their _Factor-Analysis_ (FA) decomposition (see _e.g._[2, Sec. 12.2.4] for more details). Thus, \(512\times 512\times 3\) frames in the video domain are created through an affine transformation of \(x_{k}^{FA}\). Linear observations \(y_{k}\in\mathbb{R}^{32\times 32}\) are given in the frame (pixel) domain, by \(16\times\) downsampling the \(Y\)-channel of the generated ground-truth frames, and adding white Gaussian noise. All filtering is done in the latent domain, and then transformed to the pixel domain. MSE is also calculated in the FA domain. The exact settings can be found in App. I.

In the first experiment, measurements are supplied up to frame \(k=127\) and then stop (Fig. 5), letting the different filters predict the next, unobserved, frames of the sequence. We can see that until frame \(k=127\), all filters reconstruct the reference frames well. Starting from time \(k=128\), when measurements stop, the Kalman filter slowly fades into a static, blurry output which is the average frame value in this setting. This is a non- realistic' video; Neither the individual frames nor the temporal (static) behavior are natural to the domain. Our perfect-perceptual filter, \(\hat{x}_{\rm auc}\), keeps generating a 'natural' video, both spatially and temporally. This makes its MSE grow faster2.

Footnote 2: More visual details, including ground-truth clips and empirical error can be found in the Appendix. Full video clips for both experiments are supplied with the supplementary material.

We now perform a second experiment, where measurements are set to zero until frame \(k=512\). At times \(k>512\) they are given again by the noisy, downsampled frames as described above. In Fig. 6 we present the outcomes of the different filters. We first note that up to frame \(k=512\), there is no observed information, hence outputs are actually being generated according to priors. The Kalman filter outputs a static, average frame. The filter \(\hat{x}_{\rm tic}\) randomizes each frame independently, leading to unnatural random movement with flickering features. At frame \(k=513\), when observations become available, \(\hat{x}_{\rm kal}^{*}\) and \(\hat{x}_{\rm tic}\) get updated immediately, creating an inconsistent, non-smooth motion between frames \(512\) and \(513\). The PKF output \(\hat{x}_{\rm auc}\), on the other hand, maintains a smooth motion. Since the outputs of inconsistent filters rapidly becomes similar to the ground-truth, their errors drop. The perfect-perceptual filter, \(\hat{x}_{\rm auc}\), remains consistent with its previously generated frames and the natural dynamics of the model, hence its error decays more slowly.

Figure 4: **Different dynamics yield different MSE gaps. Here we use different dynamics, based on the Harmonic oscillator (matrices \(A,C,Q,R\) are given in Sec. I.1) to demonstrate behavior of gaps due to temporal consistency (gap between \(\hat{x}_{\rm direct}\) and \(\hat{x}_{\rm tic}\)) and due to the reduction \(\Phi_{k}=0\) (gap between \(\hat{x}_{\rm direct}\) and \(\hat{x}_{\rm minT}\)). We present analytical errors at time \(T=255\), normalized by the ground-truth state \(x_{T}\) variance. (Left) dynamics are driven by the stabilized matrix \(\rho A\) for different scalar values of \(\rho\). Observe that when timesteps are more correlated (higher \(\rho\)), both consistency and unutilized information play a major role, hence MSE gaps between filters grow. (Right) Here, \(\rho=1\), and observations are given only up to time \(\tau\) (\(y_{k}=\) noise, for \(k\geq\tau\)).**

ConclusionWe studied the problem of causal filtering of time sequences from corrupted or missing observations, where the the filter's output process is constrained to possess perfect (spatio-temporal) perceptual-quality in the sense of being distributed like the original signal. Our theoretical derivations focused on Gauss-Markov state-space processes. We introduced the novel concept of an unutilized information process and established a special class of perceptual filters, coined Perceptual Kalman Filters (PKF), that are based on the innovation process alone. We demonstrated the qualitative effects of perfect perceptual quality estimation on a video reconstruction problem. To the best of our knowledge, this is the first work addressing the distortion-perception tradeoff in online restoration settings. This work paves the way toward understanding perceptual online filtering, and the cost of temporal consistency in sequential estimation problems.

Figure 5: **Frame prediction on a dynamic texture domain.** In this experiment, measurements are supplied only up to frame \(k=127\) and the filter’s task is to predict the unobserved future frames. Observe that \(\hat{x}_{\text{kal}}^{*}\) fades into a blurred average frame, while the perceptual filter \(\hat{x}_{\text{auc}}\) generates a natural video, both spatially and temporally. This makes its MSE grow faster.

Figure 6: **Frame generation on a dynamic texture domain.** In the first half of the demo (\(k\leq 512\)), there are no observations, hence the reference signal is restored according to prior distribution. The filters with no perfect-perceptual quality constraint in the temporal domain generate non-realistic frames (Kalman filter output \(\hat{x}_{\text{kal}}^{*}\)) or unnatural motion (\(\hat{x}_{\text{tic}}\)). Perceptual filter \(\hat{x}_{\text{auc}}\) is constrained by previously generated frames and the natural dynamics of the domain, hence its MSE decays slower.

AcknowledgementsThe work of RM was partially supported by the Israel Science Foundation grant 1693/22 and by the Skillman chair in biomedical sciences. The work of TM was partially supported by the Israel Science Foundation grant 2318/22 and by a gift from KLA. RM and TM were supported by the Ollendorff Minerva Center, ECE Faculty, Technion.

## References

* [1]P. Bhattacharjee and S. Das (2017) Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. advances in neural information processing systems30. Cited by: SS1.
* [2]C. M. Bishop and N. M. Nasrabadi (2006) Pattern recognition and machine learning. Vol. 4, Springer. Cited by: SS1.
* [3]Y. Blau and T. Michaeli (2018) The perception-distortion tradeoff. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6228-6237. Cited by: SS1.
* [4]M. Chu, Y. Xie, J. Mayer, L. Leal-Taixe, and N. Thuerey (2020) Learning temporal coherence via self-supervision for gan-based video generation. ACM Transactions on Graphics (TOG)39 (4), pp. 75-1. Cited by: SS1.
* [5]G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto (2003) Dynamic textures. International Journal of Computer Vision51, pp. 91-109. Cited by: SS1.
* [6]D. Freirich and E. Fridman (2016) Decentralized networked control of systems with local networks: a time-delay approach. Automatica69, pp. 201-209. Cited by: SS1.
* [7]D. Freirich and E. Fridman (2018) Decentralized networked control of discrete-time systems with local networks. International Journal of Robust and Nonlinear Control28 (1), pp. 365-380. Cited by: SS1.
* [8]D. Freirich, T. Michaeli, and R. Meir (2021) A theory of the distortion-perception tradeoff in wasserstein space. Advances in Neural Information Processing Systems34, pp. 25661-25672. Cited by: SS1.
* [9]R. M. Freund (2004) Introduction to semidefinite programming (sdp). Massachusetts Institute of Technology, pp. 8-11. Cited by: SS1.
* [10]J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet (2022) Video diffusion models. arXiv preprint arXiv:2204.03458. Cited by: SS1.
* [11]R.E. Kalman (1960) A new approach to linear filtering and prediction problems. Journal of Basic Engineering82 (1) 35-45. Cited by: SS1.
* [12]T. H. Kim, M. S. M. Sajjadi, M. Hirsch, and B. Scholkopf (2018-01) Spatio-temporal transformer network for video restoration. In Proceedings of the European Conference on Computer Vision (ECCV), Cited by: SS1.
* [13]I. Olkin and F. Pukelsheim (1982) The distance between two random vectors with given dispersion matrices. Linear Algebra and its Applications48, pp. 257-263. Cited by: SS1.
* [14]E. Perez-Pellitero, M. Sajjadi, M. Hirsch, and B. Scholkopf (2018) Perceptual video super resolution with enhanced temporal consistency. arXiv preprint arXiv:1807.07930. Cited by: SS1.
* [15]A. Shapiro (1985) Extremal problems on the set of nonnegative definite matrices. Linear Algebra and its Applications67, pp. 7-18. Cited by: SS1.
* [16]U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Garni, et al. (2022) Make-a-video: text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792. Cited by: SS1.
* [17]L. Vandenberghe and S. Boyd (1996) Semidefinite programming. SIAM review38 (1), pp. 49-95. Cited by: SS1.

Perceptual Kalman Filters: Online State Estimation under a Perfect Perceptual-Quality Constraint - Supplementary Material

In App. A we provide a detailed theoretical background on the Kalman Filter, its properties and recursive calculation. In App. B we prove that under our perfect perceptual filtering setting, there exists a _linear_ optimal filter (Thm. 4.1). In App. C we discuss a direct, non-recursive method for optimizing perceptual filter coefficients. In App. D we present the derivation of the Lyapunov equation (17) for the error of perceptual filters. In App. E we derive the recursive expression for the filter given in (18). In App. F we find a closed-form solution for PKF coefficients by proving Theorem 4.3. In this appendix, we also give some brief overview on the extremal problem of finding a minimal distance between distributions. App.G contains a discussion about stationary perceptual Kalman filters in the steady-state regime. We summarize all definitions and notations in App. H. Finally, in App.I we give full details for all numerical demonstrations, and present additional empirical and visual results. More results are provided in the supplementary video.

## Appendix A The Kalman Filter algorithm

In this Section we supply a detailed reminder of the Kalman filter Algorithm. The celebrated Kalman filter [11] assumes a state \(x_{k}\in\mathbb{R}^{n_{x}}\), where dynamics are modeled as deterministic linear functions perturbed by a Gaussian noise, and observations \(y_{k}\in\mathbb{R}^{n_{y}}\) are linear functions of \(x_{k}\) with an additive noise

\[x_{k}= A_{k}x_{k-1}+q_{k}, q_{k}\sim\mathcal{N}(0,Q_{k}), k=1,...,T,\] (31) \[y_{k}= C_{k}x_{k}+r_{k}, r_{k}\sim\mathcal{N}(0,R_{k}), k=0,...,T.\] (32)

The noise terms \(q_{k}\) and \(r_{k}\) are independent white Gaussian processes with zero mean and covariances \(Q_{k},R_{k}\), respectively. \(x_{0}\) is assumed to have a zero-mean Gaussian distribution with covariance \(P_{0}\), independent of \(q_{1},r_{0}\). For convenience, we will sometimes refer to \(P_{0}\) as \(Q_{0}\). The matrices \(A_{k},C_{k},Q_{k}.R_{k}\) and \(P_{0}\) are system parameters with appropriate dimensions, and assumed to be known. Considering the MSE distortion, we denote

\[\hat{x}_{k|s}\triangleq\underset{\hat{x}}{\operatorname{argmin}} \operatorname{\mathbb{E}}\left[\|x_{k}-\hat{x}\|^{2}|y_{0},\ldots,y_{s}\right],\] (33)

namely the optimal MSE estimator of the state at time \(k\), given measurements up to time \(s\). Under the assumptions mentioned above, Kalman filters produce the mean state estimate \(\hat{x}_{k|k}\), an MSE-optimal estimator of \(x_{k}\) given the observations up to time \(k\). The _Kalman optimal state_\(\hat{x}_{k}^{*}\equiv\hat{x}_{k|k}\) is given by the recurrence

\[\hat{x}_{k}^{*}=A_{k}\hat{x}_{k-1}^{*}+K_{k}\mathcal{I}_{k},\] (34)

where \(K_{k}\) is the _optimal Kalman gain_[11], given explicitly in Algorithm 2.

The vector \(\mathcal{I}_{k}\) is the _innovation_ process,

\[\mathcal{I}_{k}=y_{k}-C_{k}\hat{x}_{k|k-1},\] (35)

describing the contribution of the new observation \(y_{k}\) over the optimal prediction based on previous observations. Since we are in the Linear-Gaussian setup, we have that the innovation state \(\mathcal{I}_{k}\) is orthogonal to the measurements \(y_{0},\ldots,y_{k-1}\), guaranteeing the MSE optimality of the estimation. The calculation of Kalman state is summarized in Algorithm 2.

``` initialize:\(\hat{x}_{0}^{\diamond}=K_{0}y_{0}=P_{0}C_{0}^{\top}\Sigma_{\mathcal{V}_{0}}^{-1}y_{0}, \quad P_{0|0}=P_{0}-P_{0}C_{0}^{\top}\Sigma_{\mathcal{V}_{0}}^{-1}C_{0}P_{0}, \mathcal{I}_{0}=y_{0}\), \(S_{0}=C_{0}P_{0}C_{0}^{\top}+R_{0}\). for\(k=1\) to \(T\)do  calculate prior: \(\hat{x}_{k|k-1}=A_{k}\hat{x}_{k-1|k-1},\quad P_{k|k-1}=A_{k}P_{k-1|k-1}A_{k}^{ \top}+Q_{k}\)  calculate Innovation: \(\mathcal{I}_{k}=y_{k}-C_{k}\hat{x}_{k|k-1},\quad S_{k}=C_{k}P_{k|k-1}C_{k}^{ \top}+R_{k}\)  Kalman gain: \(K_{k}=P_{k|k-1}C_{k}^{\top}S_{k}^{-1}\)  update (posterior): \(\hat{x}_{k}^{*}=\hat{x}_{k|k}=\hat{x}_{k|k-1}+K_{k}\mathcal{I}_{k},\quad P_{k |k}=(I-K_{k}C_{k})P_{k|k-1}\) endfor ```

**Algorithm 2** Kalman FilterOptimality of linear filters (proof of Thm. 4.1)

In this section we show that under a family of optimality criteria (5) and perfect-perceptual quality and causality constraints (6-7), linear filters of the form (14) are optimal. We start with the following.

**Theorem B.1**.: _Let \(Y_{0}^{T}=(y_{0},\ldots,y_{T})\) be the set of measurements (4), and let \((J_{0}^{T},Y_{0}^{T})\) be a joint distribution s.t. \(J_{k}\) is independent of \(y_{k+n}\) given \(Y_{0}^{k}\) for all \(k\in[0,T]\) and \(n\geq 1\). Then,_

\[\mathbb{E}\left[J_{k}\mathcal{I}_{k+n}^{\top}\right]=0.\] (36)

Proof.: Denote \(\hat{J}_{k}=\mathbb{E}\left[J_{k}|\mathcal{I}_{0}^{k}\right]\). We can write the measurements as a linear function of the innovations, \(y_{k}=\sum_{t=0}^{k}H_{k,t}\mathcal{I}_{t}\). We have

\[\hat{y}_{k+n}^{k+n-1}\triangleq\mathbb{E}\left[y_{k+n}|Y_{0}^{k+n-1}\right]= \mathbb{E}\left[y_{k+n}|\mathcal{I}_{0}^{k+n-1}\right]=\sum_{t=0}^{k+n-1}H_{k +n,t}\mathcal{I}_{t},\] (37)

and

\[\hat{y}_{k+n}^{k}\triangleq\mathbb{E}\left[y_{k+n}|Y_{0}^{k}\right]=\mathbb{ E}\left[y_{k+n}|\mathcal{I}_{0}^{k}\right]=\sum_{t=0}^{k}H_{k+n,t}\mathcal{I}_{t}= \mathbb{E}\left[\hat{y}_{k+n}^{k+n-1}|\mathcal{I}_{0}^{k}\right].\] (38)

For any \(k\) and \(n=1\), \(\mathcal{I}_{k+1}=y_{k+1}-\hat{y}_{k+1}^{k}\), and therefore

\[\mathbb{E}\left[J_{k}\mathcal{I}_{k+1}^{\top}\right]=\mathbb{E}\left[\mathbb{ E}\left[J_{k}\left[y_{k+1}-\hat{y}_{k+1}^{k}\right]^{\top}|\mathcal{I}_{0}^{k} \right]\right]=\mathbb{E}\left[\hat{J}_{k}\left[\hat{y}_{k+1}^{k}\right]^{\top }-\mathbb{E}\left[J_{k}|\mathcal{I}_{0}^{k}\right]\left[\hat{y}_{k+1}^{k} \right]^{\top}\right]=0.\] (39)

This is due to the facts that \(J_{k}\) and \(y_{k+1}\) are independent given the condition, and that \(\hat{y}_{k+1}^{k}\) is a deterministic function of \(\mathcal{I}_{0}^{k}\).

Now, assume we know that \(\mathbb{E}\left[J_{k}\mathcal{I}_{t}^{\top}\right]=0\) for \(k+1\leq t\leq k+n-1\). We can write

\[\mathbb{E}\left[J_{k}\mathcal{I}_{k+n}^{\top}\right] =\mathbb{E}\left[\mathbb{E}\left[J_{k}\left[y_{k+n}-\hat{y}_{k+n} ^{k+n-1}\right]^{\top}|\mathcal{I}_{0}^{k}\right]\right]\] \[=\mathbb{E}\left[\hat{J}_{k}\left[\hat{y}_{k+n}^{k}\right]^{\top }-\mathbb{E}\left[J_{k}\sum_{t=0}^{k}\mathcal{I}_{t}^{\top}H_{k+n,t}^{\top}| \mathcal{I}_{0}^{k}\right]-\mathbb{E}\left[J_{k}\sum_{t=k+1}^{k+n-1}\mathcal{I }_{t}^{\top}H_{k+n,t}^{\top}|\mathcal{I}_{0}^{k}\right]\right]\] \[=\mathbb{E}\left[\hat{J}_{k}\left[\hat{y}_{k+n}^{k}\right]^{\top }-\hat{J}_{k}\left[\hat{y}_{k+n}^{k}\right]^{\top}-\sum_{t=k+1}^{k+n-1}\mathbb{ E}\left[J_{k}\mathcal{I}_{t}^{\top}|\mathcal{I}_{0}^{k}\right]H_{k+n,t}^{\top}\right]\] \[=-\sum_{t=k+1}^{k+n-1}\mathbb{E}\left[J_{k}\mathcal{I}_{t}^{\top }\right]H_{k+n,t}^{\top}\] \[=0.\] (40)

We now show that for every filter which is feasible under (6) and (7), one can find a linear filter, jointly Gaussian with the measurement set, attaining the same cost objective.

**Theorem B.2**.: _Let \(Y_{0}^{T}=(y_{0},\ldots,y_{T})\) be the set of measurements (4), and let \(\mathcal{J}_{0}^{T}=(\mathcal{J}_{0},\ldots,\mathcal{J}_{T})\) be jointly distributed with \(Y_{0}^{T}\) such that:_

1. \(\mathcal{J}_{0}^{T}\sim\mathcal{N}\left(0,\operatorname{diag}\{P_{0},Q_{1}, \ldots,Q_{T}\}\right)\)_._
2. \(\mathcal{J}_{k}\) _is independent of_ \(y_{k+n}\) _given_ \(Y_{0}^{k}\) _for all_ \(k\in[0,T]\) _and_ \(n\geq 1\)_._
3. \(\sum_{k=0}^{T}\alpha_{k}\mathbb{E}\left[\|x_{k}-\chi_{k}\|^{2}\right]=\mathcal{C}\)_, where_ \(\chi_{k}\) _is the process given by_ \(\chi_{k}=A_{k}\chi_{k-1}+\mathcal{J}_{k}\) _with_ \(\chi_{0}=\mathcal{J}_{0}\)_.__Then, there exists a joint Gaussian distribution \((J_{0}^{T},Y_{0}^{T})\) in which (i) and (ii) hold, and the estimator given by_

\[\hat{x}_{k}=A_{k}\hat{x}_{k-1}+J_{k},\quad\hat{x}_{0}=J_{0}\] (41)

_achieves the same cost (iii), namely \(\sum_{k=0}^{T}\alpha_{k}\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]=C\)._

_Furthermore, we can write_

\[J_{k}=\pi_{k}\mathcal{I}_{k}+\phi_{k}\upsilon_{k}+w_{k},\] (42)

_where_

\[\upsilon_{k}=\mathcal{I}_{0}^{k-1}-\mathbb{E}\left[\mathcal{I}_{0}^{k-1}|J_{0 }^{k-1}\right]\] (43)

_and \(w_{k}\) is a white Gaussian noise, independent of \(Y_{0}^{T}\) and \(J_{0}^{k-1}\)._

Proof.: Let \((J_{0}^{T},Y_{0}^{T})\) be the Gaussian distribution defined by the moments of \((\mathcal{J}_{0}^{T},Y_{0}^{T})\) up to second order. We observe that from Theorem B.1 above, \(J_{k}\) is independent of all future innovations \(\mathcal{I}_{k+n}\), namely it is based only on measurements up to time \(k\). Using the notions of Theorem B.1's proof,

\[\mathbb{E}\left[(J_{k}-\hat{J}_{k})(y_{k+n}-\hat{y}_{k+n}^{k})^{ \top}|Y_{0}^{k}\right] =\mathbb{E}\left[(J_{k}-\hat{J}_{k})\sum_{t=k+1}^{k+n}\mathcal{I} _{t}^{\top}H_{k+n,t}^{\top}|\mathcal{I}_{0}^{k}\right]\] \[=\sum_{t=k+1}^{k+n}\left[\mathbb{E}\left[J_{k}\mathcal{I}_{t}^{ \top}|\mathcal{I}_{0}^{k}\right]-\hat{J}_{k}\mathbb{E}\left[\mathcal{I}_{t}^{ \top}|\mathcal{I}_{0}^{k}\right]\right]H_{k+n,t}^{\top}\] \[=\sum_{t=k+1}^{k+n}\mathbb{E}\left[\mathbb{E}\left[J_{k}\mathcal{ I}_{t}^{\top}|\mathcal{I}_{t},\mathcal{I}_{0}^{k}\right]|\mathcal{I}_{0}^{k} \right]H_{k+n,t}^{\top}\] \[=\sum_{t=k+1}^{k+n}\mathbb{E}\left[\mathbb{E}\left[J_{k}|\mathcal{ I}_{0}^{k}\right]\mathcal{I}_{t}^{\top}|\mathcal{I}_{0}^{k}\right]H_{k+n,t}^{\top}\] \[=\sum_{t=k+1}^{k+n}\mathbb{E}\left[J_{k}|\mathcal{I}_{0}^{k} \right]\mathbb{E}\left[\mathcal{I}_{t}^{\top}|\mathcal{I}_{0}^{k}\right]H_{k+n,t}^{\top}\] \[=0.\] (44)

This means that \(J_{k}\) and \(y_{k+n}\) are independent given \(Y_{0}^{k}\), which proves (ii).

From (17) we see that the cost functional depends only on the second order statistics of \((\mathcal{J}_{0}^{T},\mathcal{I}_{0}^{T})\) which are identical to those of \((J_{0}^{T},\mathcal{I}_{0}^{T})\), hence (iii) holds:

\[\sum_{k=0}^{T}\alpha_{k}\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]=\sum_ {k=0}^{T}\alpha_{k}\mathbb{E}\left[\|x_{k}-\chi_{k}\|^{2}\right]=\mathcal{C}.\] (45)

To prove (42), we now write

\[J_{k}=\varepsilon_{k}+w_{k},\] (46)

where \(\varepsilon_{k}=\mathbb{E}\left[J_{k}|Y_{0}^{T},J_{0}^{k-1}\right]\), and \(w_{k}=J_{k}-\mathbb{E}\left[J_{k}|Y_{0}^{T},J_{0}^{k-1}\right]\) is independent of \(Y_{0}^{T}\) and \(J_{0}^{k-1}\). Now, since both \(J_{k}\) and \(J_{0}^{k-1}\) are independent of \(\mathcal{I}_{k+1}^{T}\),

\[\varepsilon_{k}=\mathbb{E}\left[J_{k}|Y_{0}^{T},J_{0}^{k-1}\right]=\mathbb{E} \left[J_{k}|\mathcal{I}_{0}^{k},J_{0}^{k-1}\right]=\sum_{t=0}^{k}\phi_{k,t} \mathcal{I}_{t}+\sum_{t=0}^{k-1}\psi_{k,t}J_{t}.\] (47)

\(J_{k}\) is independent of \(J_{0}^{k-1}\), thus

\[\mathbb{E}\left[J_{k}|J_{0}^{k-1}\right]=\mathbb{E}\left[\mathbb{E}\left[J_{k} |\mathcal{I}_{0}^{T},J_{0}^{k-1}\right]|J_{0}^{k-1}\right]=0.\] (48)Conditioning both sides of (47) on \(J_{0}^{k-1}\) and taking expectations,

\[0=\sum_{t=0}^{k}\phi_{k,t}\mathbb{E}\left[\mathcal{I}_{t}|J_{0}^{k-1}\right]+\sum _{t=0}^{k-1}\psi_{k,t}J_{t}.\] (49)

Note that \(\mathbb{E}\left[\mathcal{I}_{k}|J_{0}^{k-1}\right]=0\), which together with (49) implies

\[\varepsilon_{k}=\phi_{k,k}\mathcal{I}_{k}+\sum_{t=0}^{k-1}\phi_{k,t}\left[ \mathcal{I}_{t}-\mathbb{E}\left[\mathcal{I}_{t}|J_{0}^{k-1}\right]\right]= \pi_{k}\mathcal{I}_{k}+\phi_{k}\upsilon_{k}.\] (50)

Now, all we have left to show is that \(w_{k}\) is a white sequence. Since \(w_{k+n}\) (\(n\geq 1\)) is independent of \(J_{0}^{k}\) and \(\mathcal{I}_{0}^{T}\) (which also constitute \(\upsilon_{k}\)), it is easy to obtain

\[\mathbb{E}\left[w_{k+n}w_{k}^{\top}\right]=\mathbb{E}\left[w_{k+n}\left[J_{k} -\pi_{k}\mathcal{I}_{k}-\phi_{k}\upsilon_{k}\right]^{\top}\right]=0.\] (51)

**Corollary B.3**.: _Given a cost objective of the form \(\mathcal{C}=\sum_{k=0}^{T}\alpha_{k}\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]\), there exists a linear filter of the form_

\[J_{k}=\pi_{k}\mathcal{I}_{k}+\phi_{k}\upsilon_{k}+w_{k},\] (52)

_such that_

\[\hat{x}_{0} =J_{0}\] (53) \[\hat{x}_{k} =A_{k}\hat{x}_{k-1}+J_{k},\,k=1,\ldots,T\] (54)

_is an optimal estimator under the perfect perceptual quality and causality constraints (6-7)._

Proof.: Under the perfect perceptual quality constraint, an estimate sequence \(\chi_{k}\) must satisfy that

\[\mathcal{J}_{k}=\chi_{k}-A_{k}\chi_{k-1}\] (55)

is a white Gaussian process with covariances \(Q_{k}\). If, in addition, \(\chi_{k}\) satisfies the causality condition (6), so does \(\mathcal{J}_{k}\). We conclude from Theorem B.2 that there exists a causal linear filter \(J_{k}\) that achieves the same expected objective \(\mathcal{C}\) as \(\chi_{k}\).

Now, note again that from (17), for perfect-perceptual quality causal filters, the objective \(\mathcal{C}\) is a continuous function of the covariance matrix

\[\mathbb{E}\left[\mathcal{J}_{0}^{T}\left(\mathcal{I}_{0}^{T}\right)^{\top} \right]=\begin{bmatrix}\mathrm{diag}\{P_{0},Q_{1},\ldots,Q_{T}\}&L\\ L^{\top}&\mathrm{diag}\{S_{0},S_{1},\ldots,S_{T}\}\end{bmatrix}\succeq 0,\] (56)

where, due to the causality demand, \(L\) is a quasi lower triangular matrix. The set of such feasible matrices is non-empty, closed (since it is the intersection of the closed cone of PSD matrices with a finite set of hyperplanes) and bounded. Hence, \(\mathcal{C}\) attains a minimal value on some joint distribution \(p_{\mathcal{J}_{0}^{T},Y_{0}^{T}}\), which can be chosen to be joint-Gaussian as we have seen.

A Direct optimization approach to perfect-perceptual quality filtering

For the sake of completeness, we now discuss a method for optimizing non-recursive perfect-perceptual quality filter coefficients. This approach leads to convex programs. However, as we will see next, it might become impractical for large configurations.

Let \(J=J_{0}^{T}\sim\mathcal{N}\left(0,Q\right)\), where \(Q=\operatorname{diag}\left\{\left\{Q_{k}\right\}_{k=0}^{T}\right\}\), be a causal function of the measurements, \(J=\Phi\mathcal{I}+W\), where \(\mathcal{I}=\mathcal{I}_{0}^{T}\) is the innovation process with covariance \(S=\operatorname{diag}\left\{S_{k}\right\}\) and \(W\) is an independent noise. Now, \(\hat{X}=\hat{X}_{0}^{T}=A_{J}J\) is the filter's output, where

\[A_{J}=\begin{bmatrix}I&0&\dots&0\\ A_{1}&I&\dots&0\\ \vdots&\vdots&\ddots&\vdots\\ \prod_{k=0}^{T-1}A_{T-k}&\prod_{k=1}^{T-1}A_{T-k}&\dots&I\end{bmatrix}.\] (57)

Recall \(X^{*}\) is the Kalman filter output given by \(X^{*}=A_{J}K\mathcal{I}\), where \(K=\operatorname{diag}\left\{K_{k}\right\}\). Let \(\mathscr{W}=\operatorname{diag}\left\{\alpha_{k}\right\}\otimes I_{n_{x}}\) be a weighting matrix. The objective (5) is now given by

\[\mathcal{C}(\hat{X}) =\mathbb{E}\left[(\hat{X}-X^{*})^{\top}\mathscr{W}(\hat{X}-X^{*})\right]\] \[=\operatorname{Tr}\left\{\mathscr{W}\mathbb{E}\left[\hat{X}\hat{X }^{T}\right]+\mathscr{W}\mathbb{E}\left[X^{*}X^{*\top}\right]-2\mathscr{W} \mathbb{E}\left[\hat{X}X^{*\top}\right]\right\}.\] (58)

Hence, we have to maximize

\[\mathcal{C}(\Phi) =2\mathrm{Tr}\left\{\mathscr{W}\mathbb{E}\left[\hat{X}X^{*\top} \right]\right\}\] \[=2\mathrm{Tr}\left\{\mathscr{W}A_{J}\Phi SK^{\top}A_{J}^{\top}\right\}\] \[=2\mathrm{Tr}\left\{\left\langle\Phi S\right\rangle K^{\top}A_{J }^{\top}\mathscr{W}A_{J}\right\}\] \[=2\mathrm{Tr}\left\{\Phi SK^{\top}B\right\},\] (59)

where \(B\triangleq A_{J}^{\top}\mathscr{W}A_{J}\). This is subject to the perfect perceptual-quality constraint

\[Q-\Phi S\Phi^{\top}\succeq 0,\text{ or equivalently }\begin{bmatrix}Q&\Phi S \\ S\Phi^{\top}&S\end{bmatrix}\succeq 0,\] (60)

where \(\Phi\) is a lower quasi-triangular matrix (causality constraint)

\[\Phi=\begin{bmatrix}\Phi_{0,0}&0&\dots&0\\ \Phi_{1,0}&\Phi_{1,1}&\dots&0\\ \vdots&\vdots&\ddots&\vdots\\ \Phi_{T,0}&\Phi_{T,1}&\dots&\Phi_{T,T}\end{bmatrix}.\] (61)

Again, under this formulation,

\[J=\Phi\mathcal{I}+W,\] (62)

where \(W\sim\mathcal{N}\left(0,Q-\Phi S\Phi^{\top}\right)\) is a Gaussian noise independent of \(\mathcal{I}\). Note that \(W_{0}^{T}\) might not be a white sequence in this case, since its covariance might not be a block-diagonal matrix. As a result, the noise sequence has to be sampled dependently. Also note that this problem possesses the same memory complexity as (14). To conclude, this method leads to convex, but large optimization programs, and is impractical for high dimensional settings or long temporal sequences.

Derivation of eq. (17)

Recall \(\hat{x}_{k}^{*}\) is the optimal Kalman state at time \(k\), achieving MSE given by

\[d_{k}^{*}=\mathbb{E}\left[\|\hat{x}_{k}^{*}-x_{k}\|^{2}\right]=\mathrm{Tr} \left\{P_{k|k}\right\}.\] (63)

\(P_{k|k}\) is the error covariance, given explicitly in Algorithm 2. By the orthogonality principle, for any estimator \(\hat{x}_{k}\) based on the measurements \(y_{0},\ldots,y_{k}\) we have

\[\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]=\mathbb{E}\left[\|x_{k}-\hat {x}_{k}^{*}\|^{2}\right]+\mathbb{E}\left[\|\hat{x}_{k}^{*}-\hat{x}_{k}\|^{2} \right]=d_{k}^{*}+\mathbb{E}\left[\|\hat{x}_{k}-\hat{x}_{k}^{*}\|^{2}\right].\] (64)

Now, consider an estimator \(\hat{x}_{k}\) of the form (12), and recall

\[D_{k}\triangleq\mathbb{E}\left[\left[\hat{x}_{k}^{*}-\hat{x}_{k}\right]\left[ \hat{x}_{k}^{*}-\hat{x}_{k}\right]^{\top}\right].\] (65)

Since we choose \(J_{k}\sim\mathcal{N}(0,Q_{k})\) to be independent of \(\hat{x}_{k-1}\) and \(\mathcal{I}_{k}\) is indepened of \(\hat{x}_{k-1}\) and \(\hat{x}_{k-1}^{*}\), we write

\[D_{k} = \mathbb{E}\left[\hat{x}_{k}^{*}-\hat{x}_{k}\right]\left[\hat{x}_{ k}^{*}-\hat{x}_{k}\right]^{\top}\] (66) \[=\] \[-\mathbb{E}\left[J_{k}\left[A_{k}\hat{x}_{k-1}^{*}+K_{k}\mathcal{ I}_{k}\right]^{\top}\right]-\mathbb{E}\left[\left[A_{k}\hat{x}_{k-1}^{*}+K_{k} \mathcal{I}_{k}\right]J_{k}^{\top}\right]\] \[= A_{k}D_{k-1}A_{k}^{\top}+K_{k}S_{k}K_{k}^{\top}+Q_{k}\] \[-\mathbb{E}\left[J_{k}\mathcal{I}_{k}^{\top}\right]K_{k}^{\top}- K_{k}\mathbb{E}\left[\mathcal{I}_{k}J_{k}^{\top}\right]-A_{k}\mathbb{E}\left[ \hat{x}_{k-1}^{*}J_{k}^{\top}\right]-\mathbb{E}\left[J_{k}\hat{x}_{k-1}^{*} \right]A_{k}^{\top}.\]Derivation of recursive perfect-perceptual quality filters

We now derive the recursive expression (21)-(22) for the filter given in (18),

\[\hat{x}_{k} =A_{k}\hat{x}_{k-1}+J_{k},\] (67) \[J_{k} =\varPhi_{k}A_{k}\Upsilon_{k}+\Pi_{k}K_{k}\mathcal{I}_{k}+w_{k}, \,w_{k}\sim\mathcal{N}\left(0,\Sigma_{w_{k}}\right),\] (68)

defined by the coefficients \(\left\{\Pi_{k},\varPhi_{t}\right\}_{t=0}^{T}\) fulfilling the constraints (20). Recall

\[\Upsilon_{k}\triangleq\hat{x}_{k-1}^{*}-\mathbb{E}\left[\hat{x}_{k-1}^{*}| \hat{x}_{0},\ldots,\hat{x}_{k-1}\right]=\hat{x}_{k-1}^{*}-\mathbb{E}\left[\hat{ x}_{k-1}^{*}|J_{0},\ldots,J_{k-1}\right]\] (69)

where \(\hat{x}_{k}^{*}\) is the Kalman state. \(J_{0}^{k-1},\Upsilon_{k},\mathcal{I}_{k},w_{k}\) are jointly-Gaussian and independent, and we have

\[\mathbb{E}\left[J_{n}J_{k}^{\top}\right] =Q_{k}\delta_{n=k},\] (70) \[\mathbb{E}\left[\mathcal{I}_{k}J_{k}^{\top}\right] =S_{k}K_{k}^{\top}\Pi_{k}^{\top},\] (71) \[\mathbb{E}\left[\mathcal{T}_{k}J_{k}^{\top}\right] =\Sigma_{\Upsilon_{k}}A_{k}^{\top}\varPhi_{k}^{\top}.\] (72)

We can write

\[\Upsilon_{k+1}-A_{k}\Upsilon_{k} =\hat{x}_{k}^{*}-A_{k}\hat{x}_{k-1}^{*}-\left[\mathbb{E}\left[ \hat{x}_{k}^{*}|J_{0}^{k}\right]-A_{k}\mathbb{E}\left[\hat{x}_{k-1}^{*}|J_{0}^ {k-1}\right]\right]\] \[=K_{k}\mathcal{I}_{k}-K_{k}\mathbb{E}\left[\mathcal{I}_{k}|J_{0} ^{k}\right]-A_{k}\left[\mathbb{E}\left[\hat{x}_{k-1}^{*}|J_{0}^{k}\right]- \mathbb{E}\left[\hat{x}_{k-1}^{*}|J_{0}^{k-1}\right]\right]\] (73)

Since \(J_{0}^{k}\) is an independent sequence, and since \(\mathcal{I}_{k}\) depends only on \(J_{k}\),

\[K_{k}\mathbb{E}\left[\mathcal{I}_{k}|J_{0}^{k}\right]=K_{k}\mathbb{E}\left[ \mathcal{I}_{k}|J_{k}\right]=K_{k}S_{k}K_{k}^{\top}\Pi_{k}^{\top}Q_{k}^{\dagger }J_{k}.\] (74)

We also have that \(\Upsilon_{k},J_{k}\) are independent of \(J_{0}^{k-1}\), implying

\[\mathbb{E}\left[\hat{x}_{k-1}^{*}|J_{0}^{k}\right]-\mathbb{E} \left[\hat{x}_{k-1}^{*}|J_{0}^{k-1}\right] =\mathbb{E}\left[\hat{x}_{k-1}^{*}-\mathbb{E}\left[\hat{x}_{k-1}^{ *}|J_{0}^{k-1}\right]|J_{0}^{k}\right]\] \[=\mathbb{E}\left[\Upsilon_{k}|J_{0}^{k}\right]=\mathbb{E}\left[ \Upsilon_{k}|J_{k}\right]\] \[=\Sigma_{\Upsilon_{k}}A_{k}^{\top}\varPhi_{k}^{\top}Q_{k}^{\dagger }J_{k}.\] (75)

Hence,

\[\Upsilon_{k+1}=A_{k}\Upsilon_{k}+K_{k}\mathcal{I}_{k}-\varPsi_{k}Q_{k}^{ \dagger}J_{k},\] (76)

where we denote

\[\varPsi_{k}\triangleq M_{k}\Pi_{k}^{\top}+A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{ \top}\varPhi_{k}^{\top}.\] (77)

The covariance is then given by the recursive form

\[\Sigma_{\Upsilon_{k+1}} =A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{\top}+M_{k}+\varPsi_{k}Q_{k}^ {\dagger}\varPsi_{k}^{\top}\] \[-A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{\top}\varPhi_{k}^{\top}Q_{k}^{ \dagger}\varPsi_{k}^{\top}-K_{k}S_{k}K_{k}^{\top}\Pi_{k}^{\top}Q_{k}^{\dagger }\varPsi_{k}^{\top}\] (78) \[-\left[A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{\top}\varPhi_{k}^{\top}Q_ {k}^{\dagger}\varPsi_{k}^{\top}\right]^{\top}-\left[K_{k}S_{k}K_{k}^{\top}\Pi _{k}^{\top}Q_{k}^{\dagger}\varPsi_{k}^{\top}\right]^{\top}\] (79) \[=A_{k}\Sigma_{\Upsilon_{k}}A_{k}^{\top}+M_{k}-\varPsi_{k}Q_{k}^ {\dagger}\varPsi_{k}^{\top}.\] (80)

At time \(k=0\) we have \(\Upsilon_{0}=0\) and \(\Sigma_{\Upsilon_{0}}=0\).

_Remark E.1_ (The non-reduced case).: For the full, non-reduced linear filter (14)- (15), we have the following similar formula

\[\upsilon_{k}=\begin{bmatrix}\mathcal{I}_{k-1}\\ \upsilon_{k-1}\end{bmatrix}-\begin{bmatrix}S_{k-1}&0\\ 0&\Sigma_{\upsilon_{k-1}}\end{bmatrix}\begin{bmatrix}\pi_{k-1}^{\top}\\ \phi_{k-1}^{\top}\end{bmatrix}Q_{k-1}^{\dagger}J_{k-1}\] (81)

and

\[\Sigma_{\upsilon_{k}}=\begin{bmatrix}S_{k-1}&0\\ 0&\Sigma_{\upsilon_{k-1}}\end{bmatrix}-\begin{bmatrix}S_{k-1}&0\\ 0&\Sigma_{\upsilon_{k-1}}\end{bmatrix}\begin{bmatrix}\pi_{k-1}^{\top}\\ \phi_{k-1}^{\top}\end{bmatrix}Q_{k-1}^{\dagger}\begin{bmatrix}\pi_{k-1}^{\top} \\ \phi_{k-1}^{\top}\end{bmatrix}^{\top}\begin{bmatrix}S_{k-1}&0\\ 0&\Sigma_{\upsilon_{k-1}}\end{bmatrix}.\] (82)

Notice, however, that the dimension of \(\upsilon_{k}\) grows with time \(k\).

A Generalized extremal problem with semidefinite constraints (proof of Thm. 4.3)

In this section we prove Theorem 4.3. We start with a brief overview of the extremal problem of finding a minimal distance between distributions, and of general semi-definite programs.

To prove the Theorem we observe that (28), is a generalization of the extremal problem, and suggest a non-trivial dual form where, under our assumptions, strong duality holds.

### Minimal distance between distributions

Consider two Gaussian distributions on \(\mathbb{R}^{n}\) with zero means and PSD covariance matrices \(\Sigma_{1},\Sigma_{2}\) respectively. We consider the problem of constructing a Gaussian vector \(\left[X,Y\right]\) minimizing \(\mathbb{E}\|X-Y\|^{2}\) while inducing the given marginal distributions, \(X\sim\mathcal{N}\left(0,\Sigma_{1}\right),Y\sim\mathcal{N}\left(0,\Sigma_{2}\right)\). This problem is equivalent to the following maximization of correlation [13]

\[\mathrm{Tr}\left\{2\Pi\right\}\rightarrow\max_{\Pi},\quad\mathrm{s.t.}\, \Sigma=\left[\begin{array}{cc}\Sigma_{1}&\Pi\\ \Pi^{\top}&\Sigma_{2}\end{array}\right]\succeq 0.\] (83)

We have the following results of Olkin and Pukelsheim [13].

**Lemma F.1**.: _[_13_, Lemma 1]__. Let \(\Sigma_{2}^{g}\) be any generalized inverse of \(\Sigma_{2}\). Then \(\Sigma\succeq 0\) iff_

\[\Sigma_{2}\Sigma_{2}^{g}\Pi^{\top}=\Pi^{\top}\,\mathrm{and}\,\Sigma_{1}-\Pi \Sigma_{2}^{g}\Pi^{\top}\succeq 0.\] (84)

**Theorem F.2**.: _[_13_, Thm. 4]__. If \(\mathrm{Im}\left\{\Sigma_{2}\right\}\subseteq\mathrm{Im}\left\{\Sigma_{1}\right\}\), then an optimal solution to (83) is given by_

\[\max_{\Pi}\mathrm{Tr}\left\{2\Pi\right\}=2\mathrm{Tr}\left\{\left(\Sigma_{2}^ {1/2}\Sigma_{1}\Sigma_{2}^{1/2}\right)^{1/2}\right\},\] (85)

_achieved by the argument_

\[\Pi^{*}=\Sigma_{1}\Sigma_{2}^{1/2}\left[\left(\Sigma_{2}^{1/2}\Sigma_{1} \Sigma_{2}^{1/2}\right)^{1/2}\right]^{g}\Sigma_{2}^{1/2}.\] (86)

_In the case where \(\mathrm{Im}\left\{\Sigma_{2}\right\}=\mathrm{Im}\left\{\Sigma_{1}\right\}\), \(\Pi^{*}\) is a unique optimal argument._

Under the setting discussed in Sec. 2, Theorem F.2 implies that in the more general case where \(\Sigma_{x}\succeq 0\), the MSE-optimal perfect perceptual-quality estimator (2) is obtained by

\[\hat{x}=\mathscr{T}^{*}x^{*}+w,\quad\mathscr{T}^{*}\triangleq\Sigma_{x} \Sigma_{x^{*}}^{\frac{1}{2}}(\Sigma_{x}^{\frac{1}{2}}\Sigma_{x}\Sigma_{x^{*}} ^{\frac{1}{2}})^{\frac{1}{2}\dagger}\Sigma_{x^{*}}^{\frac{1}{2}\dagger}.\] (87)

Here again, \(w\) is a zero-mean Gaussian noise with covariance \(\Sigma_{w}=\Sigma_{x}-\mathscr{T}^{*}\Sigma_{x^{*}}\mathscr{T}^{*\top}\), independent of \(y\) and \(x\), and \(\Sigma_{x^{*}}^{\dagger}\) is the Moore-Penrose inverse of \(\Sigma_{x^{*}}\).

### SDP Setting and duality - background

_Semi-definite programming_ (SDP) [9, 17] is an optimization problem in \(X\in\mathbb{R}^{n\times n}\) of the form

\[C\bullet X \rightarrow\max_{X}\] (88) \[\mathrm{s.t.}\ A_{i}\bullet X=b_{i}\,,i=1,\ldots,m,\] (89) \[X\succeq 0.\] (90)

Here, \(C,A_{i}\) are real symmetric matrices of appropriate dimensions, and \(A\bullet X=\mathrm{Tr}\{A^{\top}X\}\) is the Frobenius product. SDPs yield the Lagrangian

\[L(X,\lambda,\nu) =\nu^{\top}b+\left(C-\sum_{i=0}^{m}\nu_{i}A_{i}\right)\bullet X+ \lambda\rho_{min}(X)\] \[=\nu^{\top}b+\left(C-\sum_{i=0}^{m}\nu_{i}A_{i}\right)\bullet X+ \min_{Y\succeq 0,\mathrm{Tr}Y=\lambda}Y\bullet X,\] (91)

where \(\lambda\geq 0\) and \(\rho_{min}\) is the minimal eigenvalue. The Dual problem (DSP) is given by

\[\nu^{\top}b\rightarrow\min_{\nu},\quad\mathrm{s.t.}\,C-\sum_{i=0}^{m}\nu_{i}A _{i}\preceq 0.\] (92)

In this case, strong duality exists iff the SDP is strictly feasible, _i.e._ it has a feasible solution interior to the feasible set, \(X\succ 0\). This condition is sometimes referred to as the _Slater condition_.

[MISSING_PAGE_FAIL:21]

**Theorem F.5**.: _[Strong duality]. Let_

\[\Omega=\left\{\Pi\in\mathbb{R}^{n_{x}\times n_{x}}:Q-\Pi M^{\dagger} \Pi^{\top}\succeq 0,\Pi^{\top}=MM^{\dagger}\Pi^{\top}\right\},\] (101) \[\mathcal{S}=\left\{\left(S,S^{-}\right):S,S^{-}\succeq 0,SS^{-}S=S,S ^{-}SS^{-}=S^{-},\,BM=SS^{-}BM\right\},\] (102)

_and denote \(M_{B}\triangleq BMB\). Assume \(\operatorname{Im}\left\{M_{B}\right\}\subseteq\operatorname{Im}\left\{Q\right\}\). Then,_

\[\min({S},{S}^{-})\in\mathcal{S}\left\{Q\bullet S+M\bullet(BS^{-}B) \right\}=\max_{\Pi\in\Omega}\operatorname{Tr}\left\{2\Pi B\right\}\] \[=2\mathrm{Tr}\left\{\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2} \right\}.\] (103)

_The extreme value is obtained for_

\[S^{*} =M_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2\dagger}M_{ B}^{1/2},\] (104) \[S^{-*} =M_{B}^{1/2\dagger}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2}M_{ B}^{1/2\dagger},\] (105) \[\Pi^{*} =QS^{*}M_{B}^{\dagger}BM=QM_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2 }\right)^{1/2\dagger}M_{B}^{1/2\dagger}BM.\] (106)

_Optimal solution \(\Pi^{*}\) is generally not unique._

To prove strong duality, we will use the following lemmas.

**Lemma F.6**.: _Assume PSD matrices \(Q,M_{B}\) such that \(\operatorname{Im}\left\{M_{B}\right\}\subseteq\operatorname{Im}\left\{Q\right\}\), then \(\operatorname{Im}\left\{M_{B}\right\}=\operatorname{Im}\left\{M_{B}^{1/2}QM_{ B}^{1/2}\right\}\)._

Proof.: Recall \(M_{B},M_{B}^{1/2}QM_{B}^{1/2}\) are real symmetric matrices.

Let \(v\in\operatorname{Ker}\{M_{B}^{1/2}QM_{B}^{1/2}\},\) we have \(\|Q^{1/2}M_{B}^{1/2}v\|=0\) hence \(M_{B}^{1/2}v\in\operatorname{Ker}\{Q^{1/2}\}\subseteq\operatorname{Ker}\{M_{ B}^{1/2}\}\), which yields \(M_{B}v=0,\) implying \(\operatorname{Ker}\{M_{B}^{1/2}QM_{B}^{1/2}\}\subseteq\operatorname{Ker}\{M_ {B}\}\). Opposite relation is trivial.

We have

\[\operatorname{Im}\left\{M_{B}\right\}=\operatorname{Ker}\{M_{B}\}^{\perp}= \operatorname{Ker}\{M_{B}^{1/2}QM_{B}^{1/2}\}^{\perp}=\operatorname{Im}\left\{ M_{B}^{1/2}QM_{B}^{1/2}\right\}.\] (107)

**Lemma F.7**.: \(\operatorname{Im}\left\{BM\right\}\subseteq\operatorname{Im}\left\{BMB\right\}.\)__

Proof.: Let \(v\in\operatorname{Ker}\{BMB\}\), then \(\|M^{1/2}Bv\|=0\) and \(Bv\in\operatorname{Ker}\{M^{1/2}\}=Ker\{M\}\). Hence \(\operatorname{Ker}\{BMB\}\subseteq\operatorname{Ker}\{MB\}\). We have

\[\operatorname{Im}\left\{BM\right\}=\operatorname{Ker}\{MB\}^{\perp}\subseteq \operatorname{Ker}\{BMB\}^{\perp}=\operatorname{Im}\left\{BMB\right\}.\] (108)

We are now ready to prove Theorem F.5.

Proof.: [Theorem F.5]. Let \(\Pi\in\Omega\), then \(X\succeq 0\) in (95). For any \((S,S^{-})\in\mathcal{S}\) we can choose \(R=S^{{}^{1/2}},G=S^{-}R\). From the result of Lemma F.3 it follows that

\[\begin{array}{c}Q\bullet S+M\bullet(BS^{-}B)=\operatorname{Tr}\left\{QRR^ {\top}+BMBGG^{\top}\right\}\\ \geq 2\mathrm{Tr}\left\{\Pi BGR^{\top}\right\}=2\mathrm{Tr}\left\{\Pi BS^{-}S \right\}=2\mathrm{Tr}\left\{\Pi B\right\}.\end{array}\] (109)

The last equality holds since \(BM=SS^{-}BM\), and \(\operatorname{Im}\left\{\Pi^{T}\right\}\subseteq\operatorname{Im}\left\{M\right\}\).

We now prove that \(\Pi^{*}\in\Omega\).

\[Q-\Pi^{*}M^{\dagger}\Pi^{*\top}\] \[=Q-QM_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2\dagger}M_{B }^{1/2}BMM^{\dagger}MBM_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2 \dagger}M_{B}^{1/2}Q\] \[=Q-QM_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{\dagger}M_{B }^{1/2}Q\] \[=Q^{1/2}\left[I-Q^{1/2}M_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2} \right)^{\dagger}M_{B}^{1/2}Q^{1/2}\right]Q^{1/2}\] \[=Q^{1/2}\left[I-Q^{1/2}M_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2} \right)^{\dagger}M_{B}^{1/2}Q^{1/2}\right]^{2}Q^{1/2}\succeq 0.\] (110)

The last equality holds since it is easy to see that \(\left[I-Q^{1/2}M_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{\dagger}M_{B }^{1/2}Q^{1/2}\right]\) is a symmetric (orthogonal) projection.

We further prove that \(S^{*},S^{-*}\in\mathcal{S}\). It is easy to show that \(S^{*},S^{-*}\) are symmetric generalized inverses, reflexive to each other (\(S^{-*}\) is in fact the Moore-Penrose inverse of \(S^{*}\)):

\[S^{*}S^{-*} =M_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2\dagger}M_{B }^{1/2}M_{B}^{1/2\dagger}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2}M_{B}^{1/2\dagger}\] (111) \[=M_{B}^{1/2}\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2\dagger} \left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2}M_{B}^{1/2\dagger}\] (112) \[=M_{B}^{1/2}M_{B}^{1/2\dagger}=M_{B}^{1/2\dagger}M_{B}^{1/2}\] (113) \[=S^{-*}S^{*}.\] (114)

The equalities hold since by Lemma F.6,

\[\mathrm{Im}\left\{M_{B}^{1/2}\right\}=Im\{M_{B}\}=\mathrm{Im}\left\{M_{B}^{1 /2}QM_{B}^{1/2}\right\}=\mathrm{Im}\left\{\left(M_{B}^{1/2}QM_{B}^{1/2}\right) ^{1/2}\right\},\] (115)

and since for a PSD matrix \(R\), \(RR^{\dagger}=R^{\dagger}R\) is an orthogonal projection onto its image. Using Lemma F.7 we have

\[S^{*}S^{-*}BM=M_{B}^{1/2\dagger}M_{B}^{1/2}BM=BM.\] (116)

It is now easy to verify that

\[Q\bullet S^{*}+M\bullet\left(BS^{-*}B\right)=2\mathrm{Tr}\left\{\Pi^{*}B\right\} =2\mathrm{Tr}\left\{\left(M_{B}^{1/2}QM_{B}^{1/2}\right)^{1/2}\right\},\] (117)

which completes the proof. 

**Corollary F.8**.: _Under the assumption, \(\mathrm{Im}\left\{M_{B}\right\}\subseteq\mathrm{Im}\left\{Q_{k}\right\}\), the optimal gain in (28) is given by_

\[\Pi_{k}^{*}=Q_{k}M_{B}^{1/2}\left(M_{B}^{1/2}Q_{k}M_{B}^{1/2}\right)^{1/2 \dagger}M_{B}^{1/2}B_{k}.\] (118)

_Remark F.9_.: Under the alternative assumption, \(\mathrm{Im}\left\{M_{k}\right\}\subseteq\mathrm{Im}\left\{Q_{k}\right\}\), the optimal gain in (28) is given by

\[\Pi_{k}^{*}=Q_{k}\tilde{B}M_{b}^{1/2}\left(M_{b}^{1/2}Q_{b}M_{b}^{1/2}\right)^{ 1/2\dagger}M_{b}^{1/2}\tilde{B},\] (119)

where \(\tilde{B}=B_{k}^{1/2},\ Q_{b}=\tilde{B}Q_{k}\tilde{B},\ M_{b}=\tilde{B}M_{k} \tilde{B}\).

Proof.: Recall our goal in (28) is to maximize \(\mathrm{Tr}\left\{\Pi MB\right\}=\mathrm{Tr}\left\{\tilde{B}\Pi M\tilde{B}\right\}\) under the condition \(Q-\Pi M\Pi^{\top}\succeq 0\) (we omit the index \(k\)). This is equivalent to minimizing \(\mathbb{E}\left[\left\|\tilde{B}X-\tilde{B}Y\right\|^{2}\right]\)_w.r.t_\(\Pi\), where \(\left(X,Y\right)\sim\mathcal{N}\left(0,\Sigma\right)\) and \(\Sigma=\begin{bmatrix}Q&\Pi M\\ M\Pi^{\top}&M\end{bmatrix}\succeq 0\).

In this case, \((\tilde{B}X,\tilde{B}Y)\sim\mathcal{N}\left(0,\Sigma_{b}\right)\) where \(\Sigma_{b}=\begin{bmatrix}\tilde{B}Q\tilde{B}&\tilde{B}\Pi M\tilde{B}\\ \tilde{B}M\Pi^{\top}\tilde{B}&\tilde{B}M\tilde{B}\end{bmatrix}\). According to Thm. F.2, under the assumption \(\operatorname{Im}\left\{\tilde{B}M\tilde{B}\right\}\subseteq\operatorname{ Im}\left\{\tilde{B}Q\tilde{B}\right\}\), the minimal distance is achieved when

\[\tilde{B}\Pi M\tilde{B}=\tilde{B}Q\tilde{B}M_{b}^{\frac{1}{2}}\left(M_{b}^{ \frac{1}{2}}Q_{b}M_{b}^{\frac{1}{2}}\right)^{\frac{1}{2}\dagger}M_{b}^{\frac {1}{2}}.\] (120)

Note that \(\operatorname{Im}\left\{M\right\}\subseteq\operatorname{Im}\left\{Q\right\}\) implies \(\operatorname{Im}\left\{\tilde{B}M\tilde{B}\right\}\subseteq\operatorname{ Im}\left\{\tilde{B}Q\tilde{B}\right\}\), and it is straightforward to verify that \(Q-\Pi^{*}M\Pi^{*\top}\succeq 0\).

Stationary settings

A note is in place regarding the stationary perceptual Kalman filter. In the Kalman steady-state regime, where dynamics (31) -(32) are time-invariant and \(T\to\infty\), the matrices \(K\) and \(S\) in Algorithm 2 are determined by the covariance matrix \(P\),

\[K=PC^{\top}(CPC^{\top}+R)^{-1},\quad S=CPC^{\top}+R.\] (121)

Here, \(C\) stands for the time-invariant observation matrix (\(y_{k}=Cx_{k}+r_{k}\)) and \(P\) is a solution to the Discrete-Time Algebraic Riccati equation (DARE)

\[P=APA^{\top}-APC^{\top}(CPC^{\top}+R)^{-1}CPA^{\top}+Q.\] (122)

Similarly, under the steady-state regime, (26) becomes

\[\begin{cases}\operatorname{Tr}\left\{D\right\}&\to\min_{\Pi}\\ \operatorname{s.t.}&D=ADA^{\top}+Q+M-\Pi M-M\Pi^{\top},M=KSK^{\top},\,Q-\Pi M \Pi^{\top}\succeq 0\end{cases}\] (123)

where \(D\) obeys an (Algebraic) Lyapunov equation. If \(A\) is stable,

\[D(\Pi)=\sum_{k=0}^{\infty}A^{k}(Q+M-\Pi M-M\Pi^{\top})\left(A^{k}\right)^{\top}.\] (124)

Hence, stationary perceptual filter is of the form

\[\hat{x}_{k} =A\hat{x}_{k-1}+J_{k},\] (125) \[J_{k} =\Pi K\mathcal{I}_{k}+w_{k},\] (126) \[w_{k} \sim\mathcal{N}\left(0,Q-\Pi M\Pi^{\top}\right),\] (127)

and in order to find optimal gain \(\Pi\), minimizing \(\operatorname{Tr}\left\{D(\Pi)\right\}\), we have to solve

\[\max_{\Pi}\operatorname{Tr}\left\{\Pi MB\right\}\,\operatorname{s.t.}Q-\Pi M \Pi^{\top}\succeq 0,\] (128)

where we define \(B\triangleq\sum_{k=0}^{\infty}\left(A^{k}\right)^{\top}A^{k}\), and the solution (under the assumption \(\operatorname{Im}\left\{BMB\right\}\subseteq\operatorname{Im}\left\{Q\right\}\)) is given again by (30).

List of notations

We summarize our notations in the following Table.

## Appendix H List of notations

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Notation & Description & Definition & Dimensions \\ \hline \hline \(n_{x}\) & state dimension & & \\ \hline \(n_{y}\) & measurement dimension & & \\ \hline \(A_{k}\) & system dynamics & & \(n_{x}\times n_{x}\) \\ \hline \(C_{k}\) & measurement function & & \(n_{y}\times n_{x}\) \\ \hline \(Q_{k},R_{k}\) & noise covariances & & \(n_{x}\times n_{x}\),\(n_{y}\times n_{y}\) \\ \hline \(x_{k}\) & system state (ground-truth) & & \(n_{x}\) \\ \hline \(y_{k}\) & measurements & & \(n_{y}\) \\ \hline \(\hat{x}_{k}\) & state estimator & & \(n_{x}\) \\ \hline \(\hat{x}_{k}\) & optimal Kalman state & see Algorithm 2 & \(n_{x}\) \\ \hline \(\hat{x}_{k|s}\) & best MSE state esimators, up to time \(s\) & & \(n_{x}\) \\ \hline \(\hat{T}_{k}\) & innovation process & see Algorithm 2 & \(n_{y}\) \\ \hline \(\hat{S}_{k}\) & innovation covariance & see Algorithm 2 & \(n_{y}\times n_{y}\) \\ \hline \(\hat{K}_{k}\) & Kalman gain & see Algorithm 2 & \(n_{x}\times n_{y}\) \\ \hline \(\Pi_{k}\) & innovation perceptual gain & & \(n_{x}\times n_{x}\) \\ \hline \(M_{k}\) & Kalman update covariance & \(M_{k}=K_{k}S_{k}K_{k}^{{}^{\dagger}}\) & \(n_{x}\times n_{x}\) \\ \hline \(\upsilon_{k}\) & unutilized information process & see (15) & \(kn_{y}\) \\ \hline \(T_{k}\) & unutilized information process (recursive) & see (19) & \(n_{x}\) \\ \hline \(\Sigma_{\Upsilon_{k}}\) & unutilized information covariance & & \(n_{x}\times n_{x}\) \\ \hline \(\Phi_{k}\) & unutilized information perceptual gain & & \(n_{x}\times n_{x}\) \\ \hline \(B_{k}\) & weight matrix & \(B_{k}=\sum_{k=k}^{T}\alpha_{t}(A^{t-k})^{\top}A^{t-k}\) & \(n_{x}\times n_{x}\) \\ \hline \(D_{k}\) & deviation from MMSE & \(D_{k}=\mathbb{E}\left[\hat{x}_{k}^{*}-\hat{x}_{k}\right][\hat{x}_{k}^{*}-\hat{ x}_{k}]^{{}^{\dagger}}\) & \(n_{x}\times n_{x}\) \\ \hline \(T\) & Termination time (horizon) & & \\ \hline \(\mathcal{C}(\hat{X}_{0}^{T})\) & minimization objective & \(\mathcal{C}=\sum_{k=0}^{T}\alpha_{k}\mathbb{E}\left[\|x_{k}-\hat{x}_{k}\|^{2}\right]\) & \\ \hline \end{tabular}
\end{table}
Table 2: Definitions and NotationsNumerical demonstrations

In this section we provide full details for the experimental settings of Sec. 5, with additional numerical and visual results. In the following, we compare the performance of several filters; \(\hat{x}_{\mathrm{tail}}^{*}\) and \(\hat{x}_{\mathrm{tic}}\) correspond to the Kalman filter and the temporally-inconsistent filter (10) (which does not possess perfect-perceptual quality). The estimate \(\hat{x}_{\mathrm{opt}}\) is generated by a perfect-perception filter obtained by numerically optimizing the coefficients in (18), where the cost is the MSE at termination time, _i.e._ the _terminal cost_

\[\mathcal{C}_{\mathrm{T}}=\mathbb{E}\left[\|\hat{x}_{T}-x_{T}\|^{2}\right].\] (129)

The estimates \(\hat{x}_{\mathrm{auc}},\hat{x}_{\mathrm{minT}}\) correspond to PKF outputs (Alg. 1) minimizing the _total cost_ (area under curve)

\[\mathcal{C}_{\mathrm{auc}}=\sum_{k=0}^{T}\mathbb{E}\left[\|\hat{x}_{k}-x_{k}\| ^{2}\right],\] (130)

and the _terminal cost_, respectively. Finally, \(\hat{x}_{\mathrm{stat.}}\) is the stationary PKF, discussed in App. G. The filters are summarized in Table 3.

### Example: Harmonic oscillator

We start with a simple \(2\)-D example, where we demonstrate the differences in MSE distortion between the optimized perfect-perceptual quality filter \(\hat{x}_{\mathrm{opt}}\), the temporally inconsistent filter \(\hat{x}_{\mathrm{tic}}\) and the efficient sub-optimal (perceptual) PKF. Consider the harmonic oscillator, where the entries of the state \(x_{k}\in\mathbb{R}^{2}\) correspond to position and velocity, and evolve as

\[x_{k+1}=Ax_{k}+q_{k},\quad q_{k}\sim\mathcal{N}\left(0,I\right)\] (131)

with

\[A=I+\begin{bmatrix}0&1\\ -2&0\end{bmatrix}\times\Delta_{t},\] (132)

where \(\Delta_{t}=5\times 10^{-3}\) is the sampling interval. Assume we have access to noisy and delayed scalar observations of the position (corresponding to time \(t-\frac{1}{2}\Delta_{t}\)) so that \(y_{k}=\begin{bmatrix}1&-\frac{1}{2}\end{bmatrix}x_{k}+r_{k}\), where \(r_{k}\sim\mathcal{N}\left(0,1\right)\) and \(x_{0}\sim\mathcal{N}\left(0,0.8I\right)\).

We numerically optimize the coefficients \(\left\{\Pi_{k},\Phi_{k}\right\}_{k=0}^{T}\) in (18), to minimize the terminal error (129) (\(\mathrm{Tr}\left\{D_{T}\right\}\) in (23)) at time \(T=255\) under the constraints (20). Figure 7 shows the MSE distortion for the optimized perfect-perception filter \(\hat{x}_{\mathrm{opt}}\) defined by (18) and \(\left\{\Pi_{k},\Phi_{k}\right\}_{k=0}^{T}\), and the sub-optimal PKF outputs \(\hat{x}_{\mathrm{auc}},\hat{x}_{\mathrm{minT}}\), minimizing the total cost (130) and the terminal cost (129) (see Table 3). We observe that PKF estimations are indeed not MSE optimal at time \(T\), However, their RMSE at time \(T\) is only \(\sim 30\%\) higher than that of \(\hat{x}_{\mathrm{opt}}\) and they have the advantage that they can be solved analytically and require computing only half of the coefficients (\(\Pi_{k}\)).

The estimates \(\hat{x}_{\mathrm{kal}}^{*}\) and \(\hat{x}_{\mathrm{tic}}\) achieve lower MSE than \(\hat{x}_{\mathrm{opt}}\), however they do not possess perfect-perceptual quality. We can see the difference in MSE distortion between the filters \(\hat{x}_{\mathrm{opt}}\) and \(\hat{x}_{\mathrm{tic}}\), with and without perception constraint in the temporal domain. _This is the cost of temporal consistency in online estimation for this setting_.

\begin{table}
\begin{tabular}{|c|c|c|c|c|}  & **description** & **definition** & \multicolumn{2}{c|}{**perfect-perception**} \\  & & per-sample & temporal \\ \hline \(\hat{x}_{\mathrm{kal}}^{*}\) & \begin{tabular}{c} Kalman filter \\ \end{tabular} & Algorithm 2 & ✗ & ✗ \\ \hline \(\hat{x}_{\mathrm{tic}}\) & \begin{tabular}{c} Per-sample perceptual quality \\ (temporally-inconsistent) \\ \end{tabular} & (10) & ✓ & ✗ \\ \hline \(\hat{x}_{\mathrm{opt}}\) & 
\begin{tabular}{c} Optimized perfect-perceptual \\ quality filter \\ \end{tabular} & **(18)** & ✓ & ✓ \\ \(\hat{x}_{\mathrm{auc}}\) & PKF with total cost & Algorithm 1 & ✓ & ✓ \\ \hline \(\hat{x}_{\mathrm{minT}}\) & PKF with terminal cost & Algorithm 1 & ✓ & ✓ \\ \(\hat{x}_{\mathrm{stat.}}\) & Stationary PKF & (125) & ✗ & ✗ \\ \end{tabular}
\end{table}
Table 3: List of demonstrated filters.

Figure 7: **MSE distortion on Harmonic oscillator.**\(\hat{x}_{\rm opt}\) is a numerically optimized perfect-perceptual quality filter’s output (minimizing error at time \(T=255\), dashed horizontal line). \(\hat{x}_{\rm auc},\hat{x}_{\rm minT}\) are PKF outputs minimizing different objectives. Observe that PKF estimations are not MSE optimal, but require less computations. \(\hat{x}_{\rm kal}^{*}\) and \(\hat{x}_{\rm tic}\) are not perfect-perceptual quality filters. **(top)** Empirical error, over \(N=1024\) sampled trajectories. **(bottom)** Analytical error. The difference in distortion between the perfect-perceptual state \(\hat{x}_{\rm opt}\), optimized according to (18), and \(\hat{x}_{\rm tic}\) is due to the perceptual constraint on the joint distribution. This is the cost of temporal consistency in online estimation for this setting. The gap between the MSE of the optimized estimator and \(\hat{x}_{\rm minT}\) is due to the sub-optimal choice of coefficients, \(\Phi_{k}=0\).

### Example: Two coupled inverted pendulums

Next, we demonstrate the quantitative behavior of perceptual Kalman filters, by comparing the MSE between the PKF outputs when minimizing different cost functions, and between non-perceptual filters outputs. More specifically, this experiment demonstrates:

1. How minimizing different objectives in Algorithm 1 leads to different filters.
2. The cost of perfect-perceptual quality filtering, given by Algorithm 1, over optimal filters.

We consider a higher-dimensional, well-studied example of two coupled inverted pendulums, mounted on carts [7, 6]. The cart positions, pendulum deviations, and their velocities (Fig. 8), are given by the discretized stable closed-loop system with perturbation

\[x_{k+1}=Ax_{k}+q_{k},\quad q_{k}\sim\mathcal{N}\left(0,Q\right),\] (133)

where \(x_{k}\in\mathbb{R}^{8}\). The initial state is distributed as

\[x_{0}\sim\mathcal{N}\left(0,P_{0}\right).\] (134)

The system matrices are given by

\[A=I+A_{cl}\cdot\Delta_{t},\] (135)

where \(\Delta_{t}=5\times 10^{-4}\) is the sampling interval and

\[A_{cl}=\left[\begin{array}{ccc}A_{1}+BK_{1}&F\\ F&A_{2}+BK_{2}\end{array}\right],\] (136)

\[A_{1}=A_{2}=\begin{bmatrix}0&1&0&0\\ 2.9156&0&-0.0005&0\\ 0&0&0&1\\ -1.6663&0&0.0002&0\end{bmatrix},\quad B=\begin{bmatrix}0\\ -0.0042\\ 0\\ 0.0167\end{bmatrix}.\] (137)

The _coupling_ is given by

\[F=\begin{bmatrix}0&0&0&0\\ 0.0011&0&0,0.0005&0\\ 0&0&0&0\\ -0.0003&0&-0.0002&0\end{bmatrix},\] (138)

and stabilizing state-feedback controllers (each acts on a single cart) are

\[K_{1}=\left[11396.0\quad 7196.2\quad 573.96\quad 1199.0\right],\quad K_{2}= \left[29241\quad 18135\quad 2875.3\quad 3693.9\right].\] (139)

The partial measurements are given by \(y_{k}=Cx_{k}+r_{k}\), where \(r_{k}\sim\mathcal{N}\left(0,R\right)\), with coefficients

\[C=\left[\begin{array}{cc}\bar{C}_{1}&0\\ 0&\bar{C}_{2}\end{array}\right],\quad\bar{C}_{1}=\bar{C}_{2}=\begin{bmatrix} 1&0&0&0\\ 0&0&1&0\end{bmatrix}.\] (140)

Namely, we observe only position and angle for each cart/pendulum, while velocities are not being measured.

The perturbation covariances are given by

\[P_{0}=\left[\begin{array}{cc}\bar{P}_{0}&0\\ 0&\bar{P}_{0}\end{array}\right],\quad Q=\left[\begin{array}{cc}\bar{Q}&0\\ 0&\bar{Q}\end{array}\right],\quad R=\left[\begin{array}{cc}\bar{R}&\frac{ 1}{8}\bar{R}\\ \frac{1}{8}\bar{R}&\bar{R}\end{array}\right],\] (141)

where

\[\bar{P}_{0}=\begin{bmatrix}0.154&0.142&-0.143&0.093\\ 0.142&0.144&-0.124&0.058\\ -0.143&-0.124&0.167&-0.148\\ 0.093&0.058&-0.148&0.192\end{bmatrix}\cdot 5\times 10^{-4},\] (142)

\[\bar{Q}=10^{-2}\cdot\begin{bmatrix}0.642&-0.136&0.78&0.262\\ -0.136&0.894&-0.248&0.074\\ 0.78&-0.248&1.284&-0.314\\ 0.262&0.074&-0.314&1.766\end{bmatrix}\times\Delta_{t},\,\bar{R}=10^{-2}\cdot \begin{bmatrix}0.375&-0.33\\ -0.33&0.771\end{bmatrix}\times\Delta_{t}.\] (143)We simulate the system for \(2^{10}\) time steps (\(T=2^{10}-1\)), over \(N=2^{10}\) independent experiments. In Figures 9 and 10 we show the MSE distortion as a function of time, \(\mathbb{E}\left[\|\hat{x}_{k}-x_{k}\|^{2}\right]\), for the different filters of Table 3; \(\hat{x}_{\mathrm{bal}}^{*}\) is the optimal Kalman filter. \(\hat{x}_{\mathrm{tic}}\) is the perceptual filter without consistency constraints, given in (10). \(\hat{x}_{\mathrm{auc}}\) is the PKF output minimizing the total cost (130). \(\hat{x}_{\mathrm{minT}}\) (marked by '\(\star\)') is the PKF output minimizing the terminal cost (129).

We observe that filters satisfying the perfect perceptual quality constraint (\(\hat{x}_{\mathrm{auc}}\) and \(\hat{x}_{\mathrm{minT}}\)) achieve higher distortions compared to the per-sample only perceptual filter \(\hat{x}_{\mathrm{tic}}\), which in turn attains MSE distortion slightly higher than that of the MSE-optimal Kalman filter. This demonstrates again the cost of temporal consistency in online estimation. Note also that PKFs minimizing different cumulative objectives, yield different estimations; while \(\hat{x}_{minT}\) is optimal at termination time \(T\), \(\hat{x}_{\mathrm{auc}}\) achieves a lower MSE on average. As we will see next, both filters attain the same perceptual quality.

Figure 8: **Coupled inverted pendulums.**

Figure 9: **MSE distortion on Coupled inverted pendulums for perceptual and non-perceptual filters (near the time \(T\)). \(\hat{x}_{\mathrm{auc}},\hat{x}_{\mathrm{minT}}\) are PKF outputs minimizing different objectives. Observe that while both possess perfect-perceptual quality, they yield different estimations. Also, pay attention to the MSE gap between the MSE-optimal, but not perceptual, Kalman filter and the PKFs.**

In Fig. 11 we estimate the perceptual quality, given by the Wasserstein distance between the ground-truth distribution and the empirical Gaussian distributions of the different filters outputs. In Fig. 11(top) we estimate the distance between single-sample distributions, while in Fig. 11(bottom) we consider the joint distributions of \(16\) state-vectors, \(x_{t},t\in[k-15,k]\). Observe that while each sample of \(\hat{x}_{\rm{tic}}\) is distributed similarly to its reference sample, it fails to attain perfect perceptual quality where we measure the distance from the real process distribution. PKF outputs attain low perceptual index (high quality) in both scenarios. We also present the perceptual quality measured for the ground-truth signal \(x_{\rm{gt}}\) empirical distribution, as a reference.

Figure 12 shows the asymptotic behavior (empirical error for large horizon \(T\)) of \(\hat{x}_{stat.}\), the stationary PKL (125). The figure also presents the empirical errors for Kalman filter and its stationary version (multiplied by a factor of \(2\), which is an upper bound on the MSE distortion of perceptual estimators without temporal constraints, see [3]), and the theoretical steady-state error of (125), obtained by optimizing (128) (dashed horizontal line) for comparison. The error of the non-stationary perceptual filter \(\hat{x}_{\rm{auc}}\) is also shown.

### Dynamic texture

Here we illustrate the qualitative effects of perceptual (temporally consistent) estimation in a simplified video restoration setting. Please see the supplementary video for the full videos. This setup visually demonstrates how:

1. Filters with no perfect perceptual quality tend to generate non-realistic images or atypical motion (random or slow movement, flickering artifacts etc.).
2. PKF outputs are natural to the domain, both spatially and temporally.

For this extent, we introduce the 'Dynamic Texture' domain. In this domain, video frames are generated from a latent state which represents their _Factor-Analysis_ (FA) decomposition (see _e.g._ Bishop and Nasrabadi [2, Sec. 12.2.4] for more details). The dynamics in the FA domain are assumed to be linear, with a small Gaussian perturbation,

\[x_{k}^{\rm{FA}}=A^{\rm{FA}}x_{k-1}^{\rm{FA}}+q_{k},\quad x_{0}^{\rm{FA}}\sim \mathcal{N}\left(0,I\right),\quad x_{k}^{\rm{FA}}\in\mathbb{R}^{128}.\] (144)

Figure 10: **MSE distortion on Coupled inverted pendulums for perceptual and non-perceptual filters (full view).**

Figure 11: **Perceptual quality measured by estimated Wasserstein distance \(\hat{d}_{P}\) (lower is better). (top)** Distance between distributions of single samples \(p_{x_{k}}\) and \(p_{\hat{x}_{k}}\). (bottom) Distance between distributions of \(16\)-state vectors (at times \([k-15,k]\)), \(p_{X_{k-15}^{k}}\) and \(p_{\hat{X}_{k-15}^{k}}\). Observe that \(\hat{x}_{\rm tic}\) single samples are distributed similarly to the ground-truth signal, but they fail to attain the reference joint distribution between timesteps. PKF outputs \(\hat{x}_{\rm auc}\) and \(\hat{x}_{\rm minT}\) attain high measured quality in both cases.

The state vector with the given dynamics creates frames of a wavy lake in the video domain 3, through an affine transformation,

Footnote 3: Original frames are taken from ‘river-14205’ by OjasweinGuptaOIG via pixabay.com, and are free to use under the content licence.

\[x_{k}^{vid}=W_{\mathrm{FA}\to vid}\left(x_{k}^{\mathrm{FA}}+\varepsilon^{ \mathrm{FA}}\right).\] (145)

\(W_{\mathrm{FA}\to vid}\) is a linear transformation from \(\mathbb{R}^{128}\) latent states to \(\mathbb{R}^{512\times 512\times 3}\) frames, and \(\varepsilon^{\mathrm{FA}}\) is a constant vector. \(A^{FA}\) and the noise \(q_{k}\) parameters are estimated similarly to [5]. Linear observations \(y_{k}\in\mathbb{R}^{32\times 32}\) are given in the frame (pixel) domain, by

\[y_{k}=C_{k}x_{k}^{\mathrm{FA}}+r_{k}.\] (146)

At times where information is being observed,

\[C_{k}=C_{\times 16}W_{RGB\to y}W_{\mathrm{FA}\to vid},\] (147)

where \(W_{RGB\to y}\) is a projection onto the \(Y\)-channel (grayscale) and \(C_{\times 16}\) is a matrix that performs \(16\times\) downsampling in both axes. At times where there is no observed information, \(C_{k}=0\). Here, \(r_{k}\) is a Gaussian noise.

In our first experiment, measurements are supplied as in (147) up to frame \(k=127\) and then vanish (\(C_{k}=0,k\geq 128\)), letting the different filters predict the next, unobserved, frames of the sequence. We pass \(y_{k}\) as an input to the various filters (see Table 3); \(\hat{x}_{\mathrm{bal}}^{\mathrm{s}}\) is the Kalman filter output. \(\hat{x}_{\mathrm{ic}}\) is the perceptual filter in the spatial domain, given in (10). \(\hat{x}_{\mathrm{auc}}\) is our Algorithm (PKF) output reducing the total cost in the latent space, \(\mathcal{C}_{\mathrm{auc}}=\sum_{k=0}^{T}\mathbb{E}\left[\left\|x_{k}^{ \mathrm{FA}}-\hat{x}_{k}\right\|^{2}\right]\). All filtering is done in the latent domain, and then transformed to the pixel domain. MSE is also calculated in the FA domain. In (Fig. 13) we can see that until frame \(k=127\), all filters reconstruct the reference frames well. Starting at time \(k=128\), when measurements disappear, we observe that the Kalman filter slowly fades into a static, blurry output which is the average frame value in this setting. This is definitely a non-realistic' video; Neither the individual frames nor the static behavior are natural to the domain. Our perfect-perceptual filter, \(\hat{x}_{\mathrm{auc}}\), keeps generating a 'natural' video, both spatially and temporally. This makes its MSE grow faster.

We now perform a second experiment, where \(C_{k}\) is set to zero until frame \(k=512\). At times \(k\geq 513\) measurements are given again by the noisy, downsampled frames as described in (146)-(147). In Fig. 14 we present the outcomes of the different filters. We first observe that up to frame \(k=512\), there is no observed information, hence outputs are actually being generated according to priors.

Figure 12: **MSE distortion on Coupled inverted pendulums for stationary filters.**

Figure 13: **Frame prediction on a dynamic texture domain.** In this experiment, measurements are supplied only up to frame \(k=127\). The filter’s task here is to predict the unobserved future frames of the sequence. Observe that the \(\hat{x}_{\text{kal}}^{\star}\) fades into a blurred average frame, while the perceptual filter \(\hat{x}_{\text{auc}}\) generates a natural video, both spatially and temporally. This makes its MSE grow faster,

The Kalman filter outputs a static, average frame. \(\hat{x}_{\mathrm{tic}}\) randomizes each frame independently, which creates the impression of rapid, random movement with flickering features, which is unnatural to the reference domain. At frame \(k=513\), when observations become available, we can see that \(\hat{x}_{\mathrm{kal}}^{*}\) and \(\hat{x}_{\mathrm{tic}}\) are being updated immediately, creating an inconsistent, non-smooth motion between frames \(512\) and \(513\). PKF output \(\hat{x}_{\mathrm{auc}}\), on the other hand, keeps maintaining a smooth motion. Since non-consistent filters outputs rapidly becomes similar to the ground-truth, their errors drop. The perfect-perceptual filter, \(\hat{x}_{\mathrm{auc}}\), remains consistent with its previously generated frames and the natural dynamics of the model, hence its error decays more slowly.

Figure 14: **Frame generation on Dynamic texture domain.** In the first half of the demo (\(k\leq 512\)), there are no observations, hence the reference signal is restored according to prior distribution. We observe that filters with no perfect-perceptual quality constraint in the temporal domain generate non-realistic frames (Kalman filter output \(\hat{x}_{\mathrm{kal}}^{*}\)) or unnatural motion (\(\hat{x}_{\mathrm{tic}}\)). Perceptual filter \(\hat{x}_{\mathrm{auc}}\) is constrained by previously generated frames and the natural dynamics of the domain, hence its MSE decays slower.