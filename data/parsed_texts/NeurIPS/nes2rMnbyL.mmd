# _AdaSociety_: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making

 Yizhe Huang\({}^{\ast 2,1}\) Xingbo Wang\({}^{\ast}\)\({}^{1}\)\({}^{2}\) Hao Liu\({}^{\dagger}\)\({}^{3}\) Fanqi Kong\({}^{2,1}\) Aoyang Qin\({}^{4,1}\)

Min Tang\({}^{5,1}\) Song-Chun Zhu\({}^{1,2}\) Mingjie Bi\({}^{1}\) Siyuan Qi\({}^{1}\) Xue Feng\({}^{\tiny\text{\textcircled{E}}}\)\({}^{\What makes the problem even more challenging is that social connections are not predefined but adaptive, which means there's a dynamical interplay between the topology of social connections and agents' states [23]. The adaptive nature of social connections and physical surroundings requires agents to learn continuously, reason about other agents' policies, and balance between physical explorations and establishing social connections. While contemporary multi-agent decision-making environments [6; 2; 53; 66; 48] have achieved great progress in stimulating and testing capabilities of learning algorithms in fixed task sets, they fail to generate new tasks by concurrently considering expanding physical surroundings and adaptive social connections.

To bridge this gap, we propose _AdaSociety_, a multi-agent environment with massive and diverse tasks generated by adaptive social connections and expanding physical surroundings, which are influenced by agents' behavior. In particular, to the best of our knowledge, _AdaSociety_ first introduces social states (expressed as a multi-layer directed graph) to explicitly and quantitatively describe the adaptive and dynamic connections between entities, including agents and emerged organizations. This greatly enriches the diversity of tasks, supporting the establishment of stable and long-term relations between entities and the quantitative study of social intelligence, like coalition formation and the emergence of hierarchy. In such an environment, agents need to balance the exploration of physical surroundings and the alteration of social connections, leading to multiple possible victory paths and significant decision-making challenges. To stimulate algorithm design and theoretical analysis in _AdaSociety_, we provide a formulation of the multi-agent decision-making problems, named _Growing-MG_ (Sec. 3).

_AdaSociety_ serves as a platform for researchers to customize the environment for diverse research needs. Specifically, a set of fundamental elements and mechanisms can be used, and interfaces are provided to set environment attributes and hyper-parameters. Moreover, _AdaSociety_ exhibits its characteristics by offering three mini-games, where both tensor- and LLM-based methods are tested.

In summary, this paper makes three contributions. 1) We introduce a novel multi-agent general-sum environment featuring expanding physical surroundings and adaptive social connections. 2) We offer a customizable environment with three built-in mini-games, supporting both tensor- and LLM-based methods. 3) We implement RL and LLM methods in these mini-games and provide preliminary results, laying the groundwork for further research in this environment.

## 2 Environment

### Basic Components

The key components of _AdaSociety_ (Fig. 1) include the physical component, composed of resources, events, and agents' inventories, and the social component describing connections between agents and organizations. Agents can observe and act to modify both physical and social states.

Figure 1: An overview of _AdaSociety_, composed of physical component and social component. **Physical Component** consists of diverse resources and events on the map and heterogeneous agents’ inventories. **Social Component** describes the adaptive connections between agents and organizations, which shape information access and reward structure. Agents take social actions to alter their social connections. As shown in the rightmost flowchart, agents are initially independent and can establish individual connections (edges between nodes) and form groups (gray ovals).

#### 2.1.1 Physical Component

**Resource and Event.** Resources are natural or synthetic. Natural resources scatter randomly on the map. Some natural resources are visible to everyone while others can only be seen when an agent has specific resources in its inventory. For example, only the agent possessing a hammer can observe coal. When agents with specific resources in their inventories stand on an event grid and take the'synthesize' action, one unit of new resource is synthesized. Synthetic resources will be automatically placed into agents' inventories. These resources and events can be systematically described as a synthesis tree (see Fig. 4). Importantly, agents are unaware of this synthesis tree. They gradually learn the tree through interaction with the environment. Resources, event grids, and agents are initialized in random locations on the map for every episode. While there are existing 3D benchmark environments focusing on perception challenges, our research centers on the domain of multi-agent decision-making. To this end, the map is intentionally crafted in a 2D symbolic format.

**Agent's Inventory.** Every agent has an inventory with maximal capacities of every resource, implying skill diversity. For example, an agent with a \(0\) capacity for hammers cannot possess hammers and observe coal. Agents can collect resources from the map into their inventories and dump resources on the map. Agents' rewards are attached to the resources in their inventories, while they exhibit heterogeneity in resource preferences. Specifically, for agent \(i\), the reward of resource \(\rho\) is \(R_{i}(\rho)=m_{i}^{\rho}\cdot h_{i}(\rho)\cdot\overline{r}^{\rho}\), where \(m_{i}^{\rho}\) is the amount of resource \(\rho\) in \(i\)'s inventory, \(h_{i}(\rho)\in\mathbb{R}\) represents \(i\)'s preference for \(\rho\), \(\overline{r}^{\rho}\) is the objective reward of a unit of \(\rho\) (see details in Sec. A.1).

#### 2.1.2 Social Component

The social component explicitly exhibits the connections between agents or organizations. These connections drastically influence multi-agent decision-making by affecting agents' accessible information and reward structures. Centralization and its complete opposite, decentralization, can be seen as two typical connection structures, presenting very different decision-making problems. _AdaSociety_ supports adaptive connections, with corresponding interactions being modeled as general-sum games. _AdaSociety_ considers not only the connections between agents but also the subordinate connections between agents and organizations established autonomously by agents. This makes hierarchical connections possible. Agents take social actions to change social states, like connecting or disconnecting with someone. Fig. 1 illustrates evolving connection structures, from fully independent agents to sparsely connected agents with several non-overlapping small groups, and finally to a unified large group. On the other hand, as a customized environment, _AdaSociety_ also supports users to predefine and/or fix social connections for their specific research problems. The semantics of connections are diverse, which can be reward sharing, information sharing, or division of labor between involved agents. _AdaSociety_ supports that agents negotiate their connection semantics (Sec. 4).

To maintain consistency with the physical component, we refer to these connections between agents and organizations as social states, which are expressed as a multi-layer directed graph (Sec. 3). Social states explicitly and quantitatively express relations between agents or organizations. For example, the cooperation level of two agents can be measured by the frequency of connections between them. Moreover, the combination of social states with successive tasks in _AdaSociety_ supports the establishment of stable and long-term relations and the study of social intelligence, like coalition formation and the emergence of hierarchy.

#### 2.1.3 Observation and Action

**Observation.** Each agent navigates with a partially observable window, reaching \(o\) grids in the four cardinal directions of its current position. Agents can get their own inventory states of collected resources, but not those of co-players. The social states of all the agents are accessible to everyone.

**Action.** Action space consists of social actions and physical actions. Social actions aim to build and break connections with others, including other agents or organizations. Connections are directional. If agent \(i\) connects to agent \(j\), but not vice versa, \(i\) shares its information or reward with \(j\), but gets nothing from \(j\). Physical actions include movement, picking and dumping specific resources, synthesizing resources on corresponding event grids, and communicating with someone. Newly synthesized resources enrich picking and dumping actions and the action space.

### Evaluation Metrics

_AdaSociety_ provides diverse metrics to evaluate the performances of agents and organizations including **Individual reward**, **Fairness score**, **Completion rate**, and **Average degree** and **Maximum degree** of the social network. Definitions and details of the metrics are discussed in Sec. A.5.

### Environment Characteristics

There are various characteristics of _AdaSociety_ that make it novel (see Tab. 1). _AdaSociety_ is a **multi-agent** decision-making environment, which provides both mini-games for specific research problems and a **customizable** platform to researchers (see details in Sec. A.4). Agents **dynamically connect** with other agents or organizations and autonomously **communicate** to negotiate the semantics of connections, making the emergence of hierarchical social structure and diverse social intelligence possible. With these dynamic and non-deterministic connections, friends may become foes, and vice versa. Thus, the interactions between agents can be modeled as **general-sum games**, where cooperation coexists with competition. Agents navigate this playground with a **partially observable** window centered on their current position. The state and action spaces of _AdaSociety_** dynamically **expand**, adapting to agents' (physical and social) behavior. That generates **massive and diverse tasks**, supporting an evaluation of agents' abilities in multiple aspects. _AdaSociety_ is **friendly to LLM-** and **tensor-based agents**. We evaluate state-of-the-art RL methods and LLMs in Sec. 5. In addition, we want to stress that the **mutual adaptation between agents and _AdaSociety_**, which generates a variety of successive tasks and multiple possible victory paths. Achieving success in _AdaSociety_ requires a balance between the exploration of physical components and the alteration of social connections (see Fig. 5). Agents continually learn policies to efficiently explore and achieve goals in _AdaSociety_. Meanwhile, agents' (physical and social) behavior will affect the dynamics of _AdaSociety_. Synthesizing new resources will gradually expand _AdaSociety_'s physical state space and the corresponding physical action space, transition function, and reward function. Updated social states will reshape agents' observation and reward structures. Thus, tasks and task sequences are influenced by agents' behavior and social states, not sampled according to some predefined distribution of tasks. That is to say, _AdaSociety_ adapts its tasks and task sequences to agents. Mutual adaptation provides exceptionally massive and diverse complex tasks. The stochasticity and non-stability of _AdaSociety_ produce various environment dynamics. Agents need to keep learning to adapt to changing situations.

### Research Challenges

As an adaptive multi-agent environment, _AdaSociety_ provides a comprehensive platform that presents plenty of research challenges. The adaptive and dynamic characteristics of the physical and social components bring challenges mainly lying in the intricate and unpredictable interactions between agents. Through multi-dimensional **exploration**, agents learn the ability of dynamic environmental **adaptation** and engage in **communication**-enabled interactions. Meanwhile, agents may develop

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Environment & Multi- & Dynamic & Adaptive & Imperfect & Comm. & Multi- & General & Tensor & \\  & agent & Spaces & Connection & Information & task & Sum & \& LLM \\ \hline AI Economist[66] & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ & ✓ & ✗ \\ Boat Race[2] & ✓ & ✗ & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ \\ Crafter[25] & ✗ & ✓ & ✗ & ✓ & ✗ & ✓ & ✗ & ✗ \\ Diplomacy[6] & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & ✓ & ✓ \\ Melting Pot[2] & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ \\ MincDojo[18] & ✗ & ✓ & ✗ & ✓ & ✗ & ✓ & ✗ & ✓ \\ Neural MMO[53] & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ & ✗ \\ Overcooked[11] & ✓ & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ SMAC[48] & ✓ & ✗ & ✓ & ✗ & ✗ & ✓ & ✗ \\ Xland[54] & ✓ & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✗ \\ \hline _AdaSociety_ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with existing environments. _AdaSociety_ is unique for its adaptive connections between entities and expanding game spaces.

social cognition** and utilize this information to conduct **collective reasoning**, which may result in the **emergence** of various behaviors. Details of these challenges are stated in Appendix B.

## 3 Formulation

We now provide a comprehensive definition and analysis of the _Growing-MG with a social structure_, which are general enough to encompass all the research challenges mentioned above. Three concrete scenarios will be instantiated in next section.

The predominant model in multi-agent sequential decision-making is the Markov Game (MG) [37]. However, a significant limitation of MG is the assumption of constant state and action spaces and unchanged Markovian transitions or rewards, ensuring convergence to some classical solutions such as global optimality or Nash equilibrium [4; 57]. To address dynamic state and action spaces, we introduce two new structures, _Monotonic-MG-bundle_ and _Growing-MG_ as below. A Growing-MG yields a multi-agent non-stationary decision-making framework. At time step \(t\), with state \(s_{t}\) and action \(a_{t}\), the Monotonic-MG-bundle produces \(S_{t+1},A_{t+1},T_{t+1},R_{t+1}=\beta(s_{t},a_{t})\), forming one new MG instance. This framework differs from time-varying games [10; 64; 5], which only model payoff matrix dependent on past actions. On the other hand, both the transition probability and reward function in Growing-MG will evolve triggered with some certain transitions. For simplicity, we denote all possible transition and reward functions on arbitrary state and action space \(S,A\), as \(\mathcal{T}(S,A)=\{T|T:S\times A\to S\}\) and \(\mathcal{R}(S,A)=\{R|R:S\times A\to\mathbb{R}\}\) and the largest possible spaces supported by the environment as universal state space \(S_{w}\) and action space \(A_{w}\).

**Definition 1**.: A _base-MG_ is a tuple \(\mathcal{MG}_{b}=\langle\mathcal{I},S_{b},A_{b},T_{b},R_{b},\rho,\gamma\rangle\), where \(\mathcal{I}=\{1,\ldots,I\}\) is a set of agents; \(S_{b}=\{S_{b}^{1},\ldots,S_{b}^{I}\}\) and \(A_{b}=\{A_{b}^{1},\ldots,A_{I}^{I}\}\) is the state space and action space of all agents; \(T_{b}:S_{b}\times A_{b}\times S_{b}\mapsto[0,1]\) and \(R_{b}:S_{b}\times A_{b}\mapsto\mathbb{R}^{I}\) is the transition and reward function; \(\rho:S_{b}\mapsto[0,1]\) is the initial state distribution and \(\gamma\) is the temporal discount factor.

**Definition 2**.: _A Monotonic-MG-bundle upon a base-MG \(\mathcal{MG}_{b}\) within the universal state and action space \(S_{w}=\{S_{w}^{1},\ldots,S_{w}^{I}\},A_{w}=\{A_{w}^{1},\ldots,A_{w}^{I}\}\) is a map \(\beta:S_{t}\times A_{t}\to\{S_{t+1},A_{t+1},T_{t+1},R_{t+1}|S_{b}^{i}\subseteq S _{t}^{i}\subseteq S_{t+1}^{i}\subseteq S_{w}^{i},A_{b}^{i}\subseteq A_{t}^{i }\subseteq A_{t+1}^{i}\subseteq A_{w}^{i},T_{t+1}\in\mathcal{T}(S_{t+1},A_{t+ 1}),R_{t+1}\in\mathcal{R}(S_{t+1},A_{t+1})\}\)._

**Definition 3**.: _A Growing-MG upon a base-MG \(\mathcal{MG}_{b}\) within the universal state and action space \(S_{w},A_{w}\) is a tuple \(\mathcal{MG}_{g}=\langle\mathcal{MG}_{b},\beta\rangle\)._

Conceptually, each alteration in the state and action space represents a distinct stage where interrelations among agents should also change. Inspired by research in complex systems like social sciences and economics [15; 52; 14], we propose enhancing the Growing-MG framework with a multilayer graph structure [26]\(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{C})\) (see Fig. 6). \(\mathcal{C}\) is a set of layers, and \(\mathcal{V}\) is the set of nodes in all layers. \(\mathcal{E}\) is the set of edges existing between nodes in one layer or neighboring layers. We start with a non-interconnected multiplex system of networks \(\{\mathcal{G}^{1},\mathcal{G}^{2},\cdots,\mathcal{G}^{|\mathcal{C}|}\}\), where each layer \(c\) consists of a node set \(\mathcal{V}^{c}\) and an edge set \(\mathcal{E}^{c}\), represented by an adjacency matrix \(A_{ij}^{c}\) with \(i,j\in\{1,\cdots,|\mathcal{V}^{c}|\}\). Nodes in the first layer represent agents in Growing-MG, while higher layers represent groups and hierarchies of groups. To delineate relationship between nodes in neighboring layers such as agent-group membership, we introduce inter-layer connectivity using an adjacency matrix \(A_{ij}^{c,c+1}\) with \(i\in\{1,\cdots,|\mathcal{V}^{c}|\}\) and \(j\in\{1,\cdots,|\mathcal{V}^{c+1}|\}\). This representation models both static and time-varying networks, as inter-layer and intra-layer connectivity evolves with agents' behavior, distinguishing it from existing multi-agent frameworks that predetermine interactions through reward structures [46; 62]. Finally, we note that both the environmental and social states within the framework can be extended to include observational information [38; 39], thereby further enhancing the framework's generality and practical relevance.

## 4 Mini-games

To provide a comprehensive benchmark and illustrate the characteristics of _AdaSociety_, we propose a set of mini-games (Fig. 2). The three mini-games are arranged in ascending order of the complexity of decision-making. _Social structure_, prescribing agents' partners and connection semantics, evaluates agents' ability to adapt to the changeable social structure. _Contract_ predefines connection semantics, where agents need to select partners while learning coordination with various co-players.

In _Negotiation_, agents independently select partners, determine the reward distribution plan with their partners, and behave under the negotiated relationship. All of the three mini-games share the same physical component (Sec. 5.1), which contains a part of the synthesis tree. The following text provides a detailed description of the social components of _Social structure_, _Contract_, _Negotiation_. To show the full complexity of our physical components, another mini-game _Exploration_, which contains all built-in resources and events, is introduced in Sec. C.2.

_Social Structure._ The explicit representation of _social structure_ allows dynamic changes as agents interact with the environment. Pre-defined rules for structure change could be designed to compel agents to alter their social relationships while interacting with the environment. We implement structure change at certain steps: when step \(t\) reaches \(T_{1},T_{2},...\), the social structures are modified to \(\mathcal{G}_{1},\mathcal{G}_{2},...\), respectively. Different categories of social structures are stated in Sec. C.1. This forces agents to learn policies to adapt to the changing social environment.

_Contract._ The environment is divided into two stages: the contract formation stage for determining social connections and the physical interaction stage to interact with the physical component and co-players with determined social connections. The contract formation stage lasts for \(cN\) time steps, where \(c\) is a positive integer and \(N\) is the number of agents, while the physical interaction stage has a duration of \(T\). Therefore, the total duration of each episode is \(cN+T\). Before the contract formation stage \((0\leq t<cN)\), an order \((i_{1},i_{2},...,i_{N})\) is randomly sampled. At time \(t\), agent \(i_{k}\), where \(k=t\mod N\), takes social action, selecting a group node \(v_{g}\in V_{g}\) to connect. An agent can connect with only one group node. Agents within the same group are considered to have formed a contract to share rewards. In the physical interaction stage \((t\geq cN)\), all agents act synchronously within the physical component, and the rewards received are equally divided among the agents within the same group.

_Negotiation._ The game has a negotiation stage followed by a physical stage. In the beginning, agents seek cooperation by selecting an opponent and sending him a request. After mutual requests, agents bargain by exchanging proposals until agreement or breakup. In the bargaining session, agents \(i\) and \(j\) take turns to perform one of the three actions: (i) PROPOSE a new scheme \((w_{i},w_{j})\) s.t. \(w_{i}+w_{j}=1\), where \(w_{i}\) and \(w_{j}\) represent the partition of rewards obtained by \(i\) and \(j\) respectively in the physical stage. (ii) ACCEPT the proposal from one's opponent and form a new group (coalition). (iii) DECLINE the proposal and end this session without any commitment. Once a new group is formed, the cooperative relationship between \(i\) and \(j\) represented by edge \(\mathcal{E}_{ij}\) with a payoff distribution \((w_{i},w_{j})\) is established. Later, when \(i\) or \(j\) seeks to negotiate with others, it represents the group \(\{i,j\}\). For example, if \(i\) and an out-group agent \(k\) reach a new distribution plan \((w_{i}^{\text{new}},w_{k}^{\text{new}})\), then \(k\) is regarded as joining \(\{i,j\}\) to form a new group \(\{i,j,k\}\) with an updated distribution \((w_{i}\cdot w_{i}^{\text{new}},w_{j}\cdot w_{j}^{\text{new}},w_{k}^{\text{new}})\).

## 5 Experiments

### Environment Setup

We have designed two physical task settings, featuring different levels of difficulty, for _Social Structure_, _Contract_, and _Negotiation_. The parameters of these tasks are provided in Sec. C.3.

In the Easy task, the environment involves a single event HammerCraft. Within this task, agents are categorized into two types based on their inventory capacity and value preference: carpenters and miners. Carpenters have the ability to gather wood and stone, which they can then use to produce hammers through the HammerCraft event. However, their inventory is limited to holding only one hammer at a time. On the other hand, miners are unable to collect stone, making them incapable of producing hammers. However, miners possess the advantage of being able to store a considerable number of hammers in their inventory. Additionally, hammers held by miners are assigned a higher value compared to those held by carpenters.

Figure 2: Overview of three mini-games.

In the Hard task, the environment becomes more complex with the inclusion of six resources: wood, stone, hammer, coal, torch, and iron, as well as two events: HammerCraft and TorchCraft. Similar to the Easy task, agents are divided into carpenters and miners. Due to the limited capacity of certain resources, only carpenters can execute HammerCraft to produce hammers, while only miners can execute TorchCraft to produce troches. However, carpenters' inventories cannot store coal, which requires a hammer to pick up, and miners' inventories cannot store iron, which requires a torch to pick up. Consequently, in order to maximize group rewards, carpenters and miners should engage in resource exchange, providing the resources they can produce to each other. This collaborative effort ensures that the group can obtain more resources collectively.

### Baseline Methods

We use several deep reinforcement learning algorithms as baselines. _Proximal Policy Optimization (PPO)_[49] strikes a balance between sample efficiency and policy stability by constraining policy updates using a trust region approach and a clipped surrogate objective. _RecurrentPPO(RecPPO)_ uses PPO for training and add LSTM [28] to maintain memories in the network. _Rainbow_[27] is a value-based method that incorporates several key enhancements into the Deep Q-learning framework. _MAPPO_ is the multi-agent version of PPO. It learns a critic that takes the global state and other agents' actions as inputs during training. We employ a convolutional neural network for encoding grid information and a graph convolutional network [32] for encoding social state in all RL methods. The open-source library RLLib [36] is used for RL training.

Additionally, we design a _curriculum learning (CL)_ algorithm. It starts with shared rewards to enhance cooperation strategies, then gradually increases social state randomness for learning under different social structures, and finally allows agents to perform social actions to establish their own social state. RecPPO is used for RL training at each stage. We also present a _Large Language Model + rule-based controller (LLM-C)_ framework based on GPT-4 [1], which converts environmental information into prompts to query an LLM for high-level plans and then calls a rule-based controller to execute actions based on the generated plan. LLM has been shown to be effective in some single-agent environments, such as MineCraft [59; 56; 58; 67; 60]. The details of the last two algorithms are given in Appendix D.

### Results

#### 5.3.1 _Social Structure_

In the _Social Structure_ mini-game, various static and dynamic social structures are tested to evaluate baseline algorithms. Detailed results are presented in Appendix E. Here, we discuss the result of one **Dynamic** scenario, where the social structure starts with **Inequality**, then switches to **Independent (Ind.) group** at step 30, and alters to **Overlapping (Ovlp.) group** at step 60.

Fig. 2(a) presents the reward accumulation as agents take actions with three static-structure scenarios and one dynamic-structure scenario, respectively. The results verify the influence of dynamic change in social structure on agent performance since the **Dynamic** curve resembles the **Inequality** scenario initially but then it drops in later steps and approaches **Ovlp. group** scenario.

Fig. 2(b) and Fig. 2(c) illustrate the performance of various learning methods. Some traditional methods, such as PPO, RecPPO, and MAPPO, exhibit similar performance, with MAPPO performing worse due

Figure 3: **Dynamic structure: (a) Individual reward per step with different social structures using 100 samples from PPO-trained policies, (b) Individual reward per step using 100 samples from different policies (c) Learning curves using different learning methods.**

to the difficulty in learning an effective central critic for heterogeneous agents. Rainbow performs the worst, likely because of its general ineffectiveness in exploration. Curriculum learning demonstrates superior performance by leveraging prior knowledge of different structures to adapt to dynamic scenarios effectively. Additionally, figures in Fig. 3 reveal significant deviations in most tests, regardless of social structures, learning algorithms, or performance metrics. Compared to scenarios without agent groups (Fig. 9(a) and Fig. 10(a)), the results indicate that the current algorithms struggle to learn stable policies for scenarios with agent groups.

#### 5.3.2 Contract

As depicted in Tab. 2, _Contract_ presents a challenge for popular RL methods, as they are stuck in a local equilibrium of completing limited HammerCraft on both tasks (see Fig. 6(b)), while CL demonstrates notable performance on the Easy tasks and surpasses general RL methods on the Hard tasks. The first curriculum in CL equips the agent with the ability to learn effective policies in the physical realm, and the second curriculum empowers the agent to make informed judgments about different social structures while considering rational physical policies. Ultimately, this knowledge aids CL in selecting an appropriate contract. However, it appears that CL may forget the strategies acquired during the first curriculum, as the reward at the end of the second stage has dropped significantly compared to the end of the first stage (see Tab. 12 for details). This might hamper the performance of CL on the Hard task.

Sharing rewards has been recognized as an effective method for agent groups to acquire cooperative strategies, thereby supporting the feasibility of CL's approach. Fig. 6(c) and Fig. 6(d) also illustrates that. In the case of the Easy task, CL eventually establishes a stable group of three individuals who actively share rewards and form a cooperative alliance. However, it is important to note that the size of the group does not directly correlate with high returns. Rainbow, for instance, frequently forms large groups in both tasks but fails to achieve substantial returns. This outcome primarily stems from inherent limitations in the algorithm's learning capabilities.

#### 5.3.3 Negotiation

Traditional RL methods struggle to enable carpenters and miners to learn to cooperate through negotiation, dumping some tools to increase the benefit of teammates with larger capacities on the physical stage as shown in Tab. 2. This challenge arises from the complexity of coupling the negotiation and physical stages. Once negotiation fails, dumping tools in the subsequent physical stage would substantially reduce the agents' rewards. Meanwhile, the complex negotiation process exacerbates the convergence problem in multi-agent settings, and agents have the incentive to claim a larger share for themselves to exploit the co-players in bargaining, posing challenges to reaching a consensus agreement. Consequently, in both Easy and Hard tasks, the average and maximum degrees are low, with most agents opting to complete tasks independently, leading to low completion rates in HammerCraft and even a complete failure in TorchCraft (Fig. 8). In the Easy task, miners' rewards heavily rely on carpenters' cooperation, which severely compromises fairness. In contrast, by first learning the optimal strategies in physical environments under different social structures, CL can identify structures with higher cooperation degrees as more beneficial, facilitating consensus during negotiation learning and achieving higher group rewards, fairness, and successful TorchCraft. Additionally, we show the Carpenters/Miners (abbreviated as C/M) split ratio when the negotiation stage is done, which is computed by \(\sum_{i\in\{\text{Carpenters}\}}w_{i}/\sum_{i\in\{\text{Miners}\}}w_{j}\). All results exceed 1, aligning with the intuition that miners are disadvantaged in negotiations as they cannot independently produce the more rewarding hammers.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \multirow{3}{*}{_Con._} & \multicolumn{2}{c}{CL} & \multicolumn{1}{c}{PPO} & RecPPO & MAPPO & Rainbow & Random \\ \cline{2-9}  & Easy & 0.9136\(\pm\) 0.0023 & 0.2286\(\pm\) 0.0003 & 0.2276\(\pm\) 0.0015 & 0.2271\(\pm\) 0.0003 & 0.1987\(\pm\) 0.0127 & 0.0046\(\pm\) 0.0002 \\  & Hard & 0.2773\(\pm\) 0.0466 & 0.1151\(\pm\) 0.0002 & 0.1149\(\pm\) 0.0000 & 0.1137\(\pm\) 0.0005 & 0.0868\(\pm\) 0.0033 & 0.0021\(\pm\) 0.0000 \\ \multirow{2}{*}{_Nego._} & Easy & 0.3543\(\pm\) 0.0229 & 0.2276\(\pm\) 0.0006 & 0.2278\(\pm\) 0.0004 & 0.2147\(\pm\) 0.0001 & 0.1969\(\pm\) 0.0105 & 0.0040\(\pm\) 0.0001 \\  & Hard & 0.1945\(\pm\) 0.0109 & 0.1093\(\pm\) 0.0027 & 0.1107\(\pm\) 0.0019 & 0.0946\(\pm\) 0.0032 & 0.0905\(\pm\) 0.0024 & 0.0020\(\pm\) 0.0001 \\ \hline \end{tabular}
\end{table}
Table 2: Average individual reward in _Contract_ and _Negotiation_, normalized by the Oracle reward.

#### 5.3.4 Llm-C in _AdaSociety_

LLM-C runs three times for each task. Tab. 3 and Tab. 9 presents the quantitative results across various metrics. Benefiting from the embedded commonsense reasoning and social intelligence of LLMs, LLM-C exhibits outstanding performance in all three mini-games, achieving average rewards nearly surpassing all RL-based methods. After being informed of the game rules and the capability differences between carpenters and miners, LLM-C can accurately recognize the importance of cooperation and swiftly form alliances with other players through negotiation or contract. During the physical stage, manually coded controllers complement LLM's deficiencies in path planning and position judgment, precisely and efficiently realizing the high-level planning generated by the LLM based on the current social structure and physical environment. However, due to common issues with LLMs such as hallucinations, context length limitations, and randomness in outputs, LLM-C does not achieve Oracle performance, and it underperforms compared to CL in _Contract_-Easy, further validating the effectiveness of our proposed CL approach.

## 6 Related Work

**Environments.** Several craft-based environments like Malmo [31], Crafter [25], Minedojo [18] and Conan [61] create dynamic state and action spaces that expand with the agent's exploration, which, however, mainly focuses on single-agent setting. Environments including MAgent [65], XLand [54], and Miniworld [12] provide a set of different and transferable tasks that build from basic elements, and they are open for customization. Melting Pot [2] contains a set of over 50 MARL learning substrates with limited customizability. Interactive games including AI Economist [66], Overcooked [11], MPE [42], Neural MMO [53], and SMAC [48] place agents in diverse systems allowing them to compete or cooperate. Other examples, such as Diplomacy [6], focus on communication between agents. None of these environments contain both dynamic social connections and adaptive tasks like _AdaSociety_.

**Unsupervised Environment Design (UED).** In the paradigm of UED [16; 40; 30], the environment learns a policy \(\Gamma:\Pi\rightarrow\Delta(\Theta^{T})\), which is a function from agent policy \(\Pi\) to the environment's parameters \(\Theta^{\mathcal{T}}\). Such a policy will automatically produce a distribution over solvable environments and further support the continued learning of the agent's policy. _AdaSociety_ does not implement UED to produce diverse tasks. Unlike UED, _AdaSociety_ has no goals or objectives, like most ecological systems, and produces multiple tasks through adaptive social structures and expanding physical surroundings.

**Structured multi-agent systems.** In multi-agent systems, various connections may be formed between agents, and these connections may form certain structures. [17], [51] and [45] focus on finding communication topology for multi-agent coordination. Some research models the locality of interaction and learns a joint value via coordination graphs [24; 8; 35]. Networked MARL [63; 46; 47; 62; 50] learns localized policies on environments where agent interactions are contingent upon their connections within a static graph. We focus on dynamic agent connections which shape agents' rewards and observations, and these connections are modeled as a multi-layer graph.

## 7 Conclusion

We introduce _AdaSociety_, a multi-agent environment featuring expanding physical surroundings and adaptive social connections. The environment is capable of generating multiple tasks in adaptation to agents' behavior. _AdaSociety_ is friendly to tensor-based and LLM-based methods. _AdaSociety_ provides interfaces supporting superb customization and also offers a set of mini-games with diverse social connections. We test several RL and LLM-based algorithms in mini-games. Preliminary results indicate that _AdaSociety_ maintains a rational complexity level for current decision-making methods.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & _Social Structure_ & _Contract_ & _Negotiation_ \\ \hline Easy & - & 0.8433\(\pm\) 0.1312 & 0.8733\(\pm\) 0.1116 \\ Hard & 0.7894\(\pm\) 0.0444 & 0.6499\(\pm\) 0.1716 & 0.6862\(\pm\) 0.1027 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average reward of LLM-C across mini-games.

There are some limitations of _AdaSociety_ illuminating our future work. Human-machine interaction is crucial for the study of multi-agent systems, which is one of our key research objectives in _AdaSociety_. While the environment is temporarily not equipped with human interfaces, the current architecture does support the subsequent development of human-machine interfaces. In addition, our game spaces can be further expanded by introducing survival pressures (need for food, hostile creatures, and so on). These negative losses will penalize undesirable actions, complement the roles of positive rewards in reinforcing desirable behavior, and guide more diverse behavior.

## Acknowledgments and Disclosure of Funding

This work is supported by the National Science and Technology Major Project (No. 2022ZD0114904). This work is also supported by the project "Wuhan East Lake High-Tech Development Zone, National Comprehensive Experimental Base for Governance of Intelligent Society".

## References

* Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Agapiou et al. [2022] John P Agapiou, Alexander Sasha Vezhnevets, Edgar A Duenez-Guzman, Jayd Matyas, Yiran Mao, Peter Sunehag, Raphael Koster, Udari Madhushani, Kavya Kopparapu, Ramona Comanescu, et al. Melting pot 2.0. _arXiv preprint arXiv:2211.13746_, 2022.
* Albrecht and Stone [2018] Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. _Artificial Intelligence_, 258:66-95, 2018.
* Altman and Shwartz [2000] Eitan Altman and Adam Shwartz. Constrained markov games: Nash equilibria. In _Advances in dynamic games and applications_, pages 213-221. Springer, 2000.
* Anagnostides et al. [2024] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On the convergence of no-regret learning dynamics in time-varying games. _Advances in Neural Information Processing Systems_, 36, 2024.
* Bakhtin et al. [2022] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of diplomacy by combining language models with strategic reasoning. _Science_, 378(6624):1067-1074, December 2022. doi: 10.1126/science.ade9097. URL https://doi.org/10.1126/science.ade9097.
* Berner et al. [2021] Rico Berner, Simon Vock, Eckehard Scholl, and Serhiy Yanchuk. Desynchronization transitions in adaptive networks. _Physical Review Letters_, 126(2):028301, 2021.
* Bohmer et al. [2020] Wendelin Bohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In _International Conference on Machine Learning_, pages 980-991. PMLR, 2020.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Cardoso et al. [2019] Adrian Rivera Cardoso, Jacob Abernethy, He Wang, and Huan Xu. Competing against equilibria in zero-sum games with evolving payoffs. _arXiv preprint arXiv:1907.07723_, 2019.
* Carroll et al. [2019] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. _Advances in neural information processing systems_, 32, 2019.
* Chevalier-Boisvert et al. [2019] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and J Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. In _Advances in Neural Information Processing Systems_, volume 36, pages 73383-73394. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/e8916198466e8ef218a2185a491b49fa-Paper-Datasets_and_Benchmarks.pdf.
* Cobbe et al. [2019] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In _International conference on machine learning_, pages 1282-1289. PMLR, 2019.
* De Domenico [2023] Manlio De Domenico. More is different in real-world multilayer networks. _Nature Physics_, 19(9):1247-1262, 2023.
* Rossa et al. [2020] Fabio Della Rossa, Louis Pecora, Karen Blaha, Afroza Shirin, Isaac Klickstein, and Francesco Sorrentino. Symmetries and cluster synchronization in multilayer networks. _Nature communications_, 11(1):3179, 2020.
* Dennis et al. [2020] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. _Advances in neural information processing systems_, 33:13049-13061, 2020.
* Du et al. [2021] Yali Du, Bo Liu, Vincent Moens, Ziqi Liu, Zhicheng Ren, Jun Wang, Xu Chen, and Haifeng Zhang. Learning correlated communication topology in multi-agent reinforcement learning. In _Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems_, pages 456-464, 2021.
* Fan et al. [2022] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. MinedJojo: Building open-ended embodied agents with internet-scale knowledge. _Advances in Neural Information Processing Systems_, 35:18343-18362, 2022.
* Gini [1912] Corrado Gini. _Variabilita e mutabilita : contributo allo studio delle distribuzioni e delle relazioni statistiche_. January 1912. URL http://ci.nii.ac.jp/ncid/BB01068601.
* Granovetter [2018] Mark Granovetter. The impact of social structure on economic outcomes. In _The sociology of economic life_, pages 46-61. Routledge, 2018.
* Gronauer and Diepold [2022] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. _Artificial Intelligence Review_, 55(2):895-943, 2022.
* Gross and Blasius [2008] Thilo Gross and Bernd Blasius. Adaptive coevolutionary networks: a review. _Journal of the Royal Society Interface_, 5(20):259-271, 2008.
* Gross and Sayama [2009] Thilo Gross and Hiroki Sayama. _Adaptive Networks: theory, models and applications_. August 2009. URL http://pubman.mpdl.mpg.de/pubman/item/escidoc:2220530.
* Guestrin et al. [2001] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored mdps. _Advances in neural information processing systems_, 14, 2001.
* Hafner [2021] Danijar Hafner. Benchmarking the spectrum of agent capabilities. In _International Conference on Learning Representations_, 2021.
* Hammoud and Kramer [2020] Zaynab Hammoud and Frank Kramer. Multilayer networks: aspects, implementations, and application in biomedicine. _Big Data Analytics_, 5(1):2, 2020.
* Hessel et al. [2018] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.

* Huang et al. [2024] Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, and Xue Feng. Efficient adaptation in mixed-motive environments via hierarchical opponent modeling and planning. In _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 20004-20022. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/v235/huang24p.html.
* Jiang et al. [2022] Minqi Jiang, Michael Dennis, Jack Parker-Holder, Andrei Lupu, Heinrich Kuttler, Edward Grefenstette, Tim Rocktaschel, and Jakob Foerster. Grounding aleatoric uncertainty for unsupervised environment design. _Advances in Neural Information Processing Systems_, 35:32868-32881, 2022.
* Johnson et al. [2016] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In _Ijcai_, volume 16, pages 4246-4247, 2016.
* Kipf and Welling [2016] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2016.
* Kong et al. [2024] Fanqi Kong, Yizhe Huang, Song-Chun Zhu, Siyuan Qi, and Xue Feng. Learning to balance altruism and self-interest based on empathy in mixed-motive games. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.
* Lanctot et al. [2019] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien Perolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, et al. Openspiel: A framework for reinforcement learning in games. _arXiv preprint arXiv:1908.09453_, 2019.
* Li et al. [2021] Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit coordination graphs for multi-agent reinforcement learning. In _Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems_, pages 764-772, 2021.
* Liang et al. [2018] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning. In _International conference on machine learning_, pages 3053-3062. PMLR, 2018.
* Littman [1994] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In _Machine learning proceedings 1994_, pages 157-163. Elsevier, 1994.
* Liu et al. [2022] Qinghua Liu, Csaba Szepesvari, and Chi Jin. Sample-efficient reinforcement learning of partially observable markov games. _Advances in Neural Information Processing Systems_, 35:18296-18308, 2022.
* Liu and Zhang [2023] Xiangyu Liu and Kaiqing Zhang. Partially observable multi-agent rl with (quasi-) efficiency: the blessing of information sharing. In _International Conference on Machine Learning_, pages 22370-22419. PMLR, 2023.
* Mediratta et al. [2023] Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, and Tim Rocktaschel. Stabilizing unsupervised environment design with a learned adversary. In _Conference on Lifelong Learning Agents_, pages 270-291. PMLR, 2023.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* Mordatch and Abbeel [2018] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent populations. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Moreno-Mateos et al. [2020] David Moreno-Mateos, Anton Alberdi, Elly Morrien, Wim H van der Putten, Asun Rodriguez-Una, and Daniel Montoya. The long-term restoration of ecosystem complexity. _Nature Ecology & Evolution_, 4(5):676-685, 2020.
* Oroojlooy and Hajinezhad [2023] Afshin Oroojlooy and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learning. _Applied Intelligence_, 53(11):13677-13722, 2023.

* Pesce and Montana [2023] Emanuele Pesce and Giovanni Montana. Learning multi-agent coordination through connectivity-driven communication. _Machine Learning_, 112(2):483-514, 2023.
* Qu et al. [2020] Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning for networked systems with average reward. _Advances in Neural Information Processing Systems_, 33:2074-2086, 2020.
* Qu et al. [2020] Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for multi-agent networked systems. In _Learning for Dynamics and Control_, pages 256-266. PMLR, 2020.
* Samvelyan et al. [2019] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In _Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems_, pages 2186-2188, 2019.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Sha et al. [2022] Xingyu Sha, Jiaqi Zhang, Keyou You, Kaiqing Zhang, and Tamer Basar. Fully asynchronous policy evaluation in distributed reinforcement learning over networks. _Automatica_, 136:110092, 2022.
* Sheng et al. [2022] Junjie Sheng, Xiangfeng Wang, Bo Jin, Junchi Yan, Wenhao Li, Tsung-Hui Chang, Jun Wang, and Hongyuan Zha. Learning structured communication for multi-agent reinforcement learning. _Autonomous Agents and Multi-Agent Systems_, 36(2):50, 2022.
* Su et al. [2022] Qi Su, Alex McAvoy, Yoichiro Mori, and Joshua B Plotkin. Evolution of prosocial behaviours in multilayer populations. _Nature Human Behaviour_, 6(3):338-348, 2022.
* Suarez et al. [2023] Joseph Suarez, Phillip Isola, Kyoung Whan Choe, David Bloomin, Hao Xiang Li, Nikhil Pinnaraju, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de Alcantara, Herbie Bradley, Louis Castricato, Kirsty You, Yuhao Jiang, Qimai Li, Jiaxin Chen, and Xiaolong Zhu. Neural mmo 2.0: A massively multi-task addition to massively multi-agent learning, 2023.
* Team et al. [2021] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to generally capable agents. _arXiv preprint arXiv:2107.12808_, 2021.
* Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ international conference on intelligent robots and systems_, pages 5026-5033. IEEE, 2012.
* Wang et al. [2024] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _Transactions on Machine Learning Research_, 2024. ISSN 2835-8856.
* Wang and Sandholm [2002] Xiaofeng Wang and Tuomas Sandholm. Reinforcement learning to play an optimal nash equilibrium in team markov games. _Advances in neural information processing systems_, 15, 2002.
* Wang et al. [2023] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Wang et al. [2023] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. _arXiv preprint arXiv:2311.05997_, 2023.
* Wang et al. [2024] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yitao Liang. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. _arXiv preprint arXiv:2407.00114_, 2024.

* Xu et al. [2023] Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, and Yixin Zhu. Active reasoning in an open-world environment. In _Advances in Neural Information Processing Systems_, volume 36, pages 11716-11736. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/2712b17bb58ea5b2b65c45857b024744-Paper-Conference.pdf.
* Yi et al. [2022] Yuxuan Yi, Ge Li, Yaowei Wang, and Zongqing Lu. Learning to share in networked multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 35:15119-15131, 2022.
* Zhang et al. [2018] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In _International Conference on Machine Learning_, pages 5872-5881. PMLR, 2018.
* Zhang et al. [2022] Mengxiao Zhang, Peng Zhao, Haipeng Luo, and Zhi-Hua Zhou. No-regret learning in time-varying zero-sum games. In _International Conference on Machine Learning_, pages 26772-26808. PMLR, 2022.
* Zheng et al. [2018] Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, and Yong Yu. Magnet: A many-agent reinforcement learning platform for artificial collective intelligence. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Zheng et al. [2022] Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C Parkes, and Richard Socher. The ai economist: Taxation policy design via two-level deep multiagent reinforcement learning. _Science advances_, 8(18):eabb2607, 2022.
* Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. _arXiv preprint arXiv:2305.17144_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? Please see Sec. 7. 3. Did you discuss any potential negative societal impacts of your work? Please see Appendix F. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? We do not have theoretical analysis and results. 2. Did you include complete proofs of all theoretical results? We do not have theoretical analysis and results.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? Please see abstract. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? In the appendix. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? We report every error bars. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Please see Sec. D.3.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators?
5. Did you mention the license of the assets? We have cited [36], whose license is Apache-2.0 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? We do not use any human data, and we use the open source code which follows the license. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? We do not use any human data.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We do not use crowdsourcing or conducted research with human subjects. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? We do not use crowdsourcing or conducted research with human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We do not use crowdsourcing or conducted research with human subjects.

Environment Elements

In this section, we elaborate the environment elements predefined in _AdaSociety_ including resources, events, and their dependency.

### Resources

There are 15 kinds of resources in _AdaSociety_, which can be divided into _Natural Resources_ and _Synthesized Resources_ based on whether they can be produced through events. Some of the natural resources can only be discovered and gathered by agents with certain resources (denoted by _Requirements_) in their inventories. The details of resources are listed in Tab. 4.

### Events

There are 9 built-in events in _AdaSociety_ as listed in Tab. 5. Each event takes 2 to 3 kinds of resources as input and outputs 1 kind of product. Events can only be observed and executed by agents whose inventories meet the event requirements.

### Synthesis Tree

An illustration of the synthetic tree is shown in Fig. 4, which is used by all the mini-games offered by this paper. In Fig. 4, natural and synthetic resources are depicted within a green circle and blue octagon icons respectively. The solid red arrow line attached by a square event icon links low-level resources to high-level products. The eye icons indicate that some resources can help their owner discover new resources or events.

### Customization

_AdaSociety_ is a versatile multi-agent environment platform that supports extensive customization of various elements, features, and hyper-parameters. Researchers can easily create tailored environments for different objectives without needing to delve into the underlying code.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline Resource & Wood & Stone & Hammer & Coal & Torch & Iron & Steel & Shovel & Pickaxe & GemMine & Clxy & Pottery & Cutter & Gem & Totent \\ \hline Synthesized & ✗ & ✗ & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ & ✓ \\ Requirement & None & None & - & Hammer & - & Torch & - & - & - & Pickaxe & Shovel & - & - & - & - \\ Objective reward & 1 & 1 & 5 & 2 & 20 & 3 & 30 & 100 & 150 & 4 & 4 & 40 & 100 & 200 & 1000 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Resources predefined in _AdaSociety_. **Synthesized** indicates whether the resource can be crafted through events. **Requirement** is an attribute of _natural_ resources (_Synthesized_ = False) indicating that the resource is observable and collectible to agents carrying the required resources. **Objective reward** denotes the objective rewards of resources.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline ffEvent & Input & Input2 & Input3 & Output & Requirement1 & Requirement2 \\ \hline HammerCraft & 1Wood & 1Stone & - & 1Hammerer & - & - \\ TorchCraft & 1Wood & 1Cool & - & 1Torch & Coal & - \\ SteelMaking & 1Iron & 1Cool & - & 1Steel & Iron & - \\ Potting & 2Clay & 1Cool & - & 1Pottery & Clay & - \\ ShovelCraft & 2Steel & 2Wood & - & 1Shovel & Steel & - \\ PickaxeCraft & 3Steel & 2Wood & - & 1Pickaxe & Steel & - \\ CutterCraft & 2Steel & 3Stone & - & 1Cutter & Steel & - \\ GemCutting & 1GermMine & - & - & 1Germ & Cutter & GemMine \\ \hline \hline \end{tabular}
\end{table}
Table 5: Events predefined in _AdaSociety_. The ingredients of each event are covered in **Input**. Most events take 2 or 3 different kinds of input resources. The products are listed in **Output**. **Requirement** denotes the resources an agent needs to carry in its inventory to observe and execute the event.

built-in resources and events (See "Synthesis tree" of Fig. 4 and Tab. 5) are included in _AdaSociety_ for users to optionally incorporate into their own scenarios. Users are also welcome to define temporary resources and events. The customizable elements and corresponding parameters are listed in Tab. 6.

### Evaluation Metrics

Individual reward is calculated as:

\[R_{i}^{c}=\sum_{\rho\in\mathcal{E}}R_{i}(\rho),\] (1)

representing agent \(i\)'s subjective reward of all types of resources \(\varrho\).

Fairness scoreis computed based on Gini index [19] to assesses the group-wise fairness:

\[F=1-\frac{\sum_{i=1}^{N}\sum_{j=1}^{N}|R_{i}^{c}-R_{j}^{c}|}{2N\sum_{i=1}^{N}R _{i}^{c}},\] (2)

\begin{table}
\begin{tabular}{l|l l} \hline \hline Element & Parameter & Description \\ \hline \hline \multirow{2}{*}{Mapsize} & \(h,w\) & Map height and map width. \\  & \(B\) & Terrain set \(B=\{b_{1},\cdots,b_{|B|}\}\). \(b_{i}\) represents a block. \\  & & \(b_{i}^{pos}\): the position of block \(b_{i}\) on the map which can be assigned or randomly generated. \\ \multirow{2}{*}{Resource} & \(\varrho\) & Set of resources \(\varrho=\{\rho_{1},\cdots,\rho_{|\varrho|}\}\). Each resource \(\rho_{i}\) has an attribute \(\rho_{i}^{req}\). \\  & & \(\rho_{i}^{req}\): Necessary resources in agents’ inventories to observe \& collect \(\rho_{i}\). \\  & \(\rho_{temp}\) & Temporary resources (Defined by specifying \(\rho_{temp}^{req}\)) \\ \multirow{2}{*}{Event} & \(\mathcal{E}\) & Set of events \(\mathcal{E}=\{\epsilon_{1},\cdots,\epsilon_{|\mathcal{E}|}\}\). Each event \(\epsilon_{i}\) has attributes \(\epsilon_{i}^{in},\epsilon_{i}^{out},\epsilon_{i}^{req}\). \\  & & \(\epsilon_{i}^{in}\): Resources consumed by event \(\epsilon_{i}\). \\  & & \(\epsilon_{i}^{out}\): Resources produced by event \(\epsilon_{i}\). \\  & & \(\epsilon_{i}^{req}\): Necessary resources in agents’ inventories to observe \& execute \(\epsilon_{i}\). \\  & \(\mathcal{E}^{pos}\) & Event positions \(\mathcal{E}^{pos}=\{\epsilon_{1}^{pos},\cdots,\epsilon_{|\mathcal{E}|^{pos}}\}\). Each \(\epsilon_{i}^{pos}\) represents a list of positions of \(\epsilon_{i}\). \\  & \(\epsilon_{temp}\) & Temporary events (Defined by specifying \(\epsilon_{temp}^{in},\epsilon_{temp}^{out},\epsilon_{temp}^{req}\)) \\ Agent & \(P\) & Set of agents \(P=\{1,\cdots,|P|\}\) \\  & \(m_{i}(0)\) & Initial inventories. \(m_{i}^{o}(0)\) denotes the initial number of resource \(\rho\) in inventories. \\  & \(i^{cap}\) & Inventory capacity. \(i^{cap}\): \(\varrho\rightarrow\mathbb{R}\) denotes maximum quantities of resources \(i\) can carry. \\  & \(h_{i}\) & \(h_{i}\): \(\varrho\rightarrow\mathbb{R}\) denotes quantities of credits \(i\) gets by acquiring resources. \\  & & The actual reward obtained by \(i\) is \(h_{i}\) multiply by the objective reward of the resource. \\  & \(i^{pos}(0)\) & Initial positions of agents which can be predefined or generated randomly. \\ \hline \hline \end{tabular}
\end{table}
Table 6: Customizable elements and parameters.

Figure 4: Illustration of a synthesis tree.

where \(N\) is the number of agents. Intuitively, the greater the value of \(F\) a group gets, the fairer it is.

**Individual reward** is one of the most common metrics for decision-making problems. It measures agents' decision-making abilities in maximizing self-interest. However, relying solely on individual rewards can be risky. In general-sum games, agents focus on maximizing their own rewards may engage in shortsighted and exploitative behaviors that harm their own long-term rewards and the collective benefit. For example, in Prisoner's Dilemma, self-interested agents always fall into the inefficient Nash equilibrium of defection, which minimizes one's own reward and the collective benefit. To tackle this issue, we introduce the **fairness score** calculated using the Gini index, which evaluates fairness within a group. In real societies, fairness is a crucial component of social justice, significantly influencing the stability of social structures and the maintenance of long-term cooperation. This metric serves as a reference for selecting agents and algorithms that balance efficiency and fairness, rather than merely pursuing individual gains.

**Completion rate** pertains to the ratio of successful executions of an event to its maximum potential executions. It is computed separately for each event. The **completion rate** is introduced to measure agents' exploration within the synthesis tree. It is calculated as the ratio of actual executions to the optimal executions of the oracle policy (computation of the oracle policy can be found in Supplementary Material). The higher the dimension of the completion rate, the deeper the exploration. Exploration is crucial in RL. The introduction of **completion rate** will guide decision-making algorithms to avoid local optima, actively explore the environment, and find the optimal policy effectively.

**Average degree** of node type \(\Gamma\in\{\textit{agent},\textit{group}\}\) is calculated as:

\[\overline{D}_{\Gamma}=\frac{1}{|\mathcal{N}_{\Gamma}|}\sum_{n\in\mathcal{N}_{ \Gamma}}D_{n},\] (3)

where \(\mathcal{N}_{\Gamma}\) is the set of \(\Gamma\) nodes and \(D_{n}\) is the degree of node \(n\). **Maximum degree** reflects the maximum degree of a certain type of node, defined as:

\[D_{\Gamma}^{max}=\max_{n\in\mathcal{N}_{\Gamma}}D_{n}.\] (4)

In asymmetric cases (where not all edges are bidirectional), the maximum degree and the average degree mentioned above are calculated separately for in-degrees and out-degrees. Social structure is the distinctive feature of _AdaSociety_. Degree-based metrics, including **average degree** and **maximum degree**, are proposed to describe and measure the topology of social structure, which significantly influences agents' policies and performances by shaping their information streams and reward functions. Agents' degree distribution is generally correlated with their rewards. For example, an agent with a high degree can obtain more information or participate in more reward distribution, thereby gaining higher returns. Combining degree-based metrics with other metrics, like individual reward and fairness, we can recognize the effective social structure for scenarios, guiding the learning of algorithms.

### Supplementary Figures for Environment Description and Formulation

#### a.6.1 Mutual Adaption Between Agents and _AdaSociety_

Based on complex network theory, we say _AdaSociety_ is an adaptive environment. In complex network theory, a network is called an adaptive network, if there is a feedback loop between the attributes or behavior of nodes and the topology of the network [22, 43, 7]. In _AdaSociety_, agents build or break connections with others and impact social structure. Conversely, social structure influences agents' observations and reward structures and further influences their attributes and behavior. Thus, following the definition of adaptive networks, the social structure of _AdaSociety_ is adaptive. As a key component of _AdaSociety_, social structure influences the generation of new tasks. For example, independent agents collect all kinds of available resources to synthesize high-level resources. However, the team-up agents will be mostly rewarded by collecting or synthesizing some specific kind of resources, according to the division of labor in the team. Furthermore, agents initially can only observe very limited resources (wood and stone in our mini-games) and events (hammercraft). Through exploration in _AdaSociety_, agents gradually discover new resources and events. The appearance of a new kind of resource depends on agents' behavior. For instance, as shown by the synthesis tree in Fig. 4, which appears next, shovel or cutter, depends on agents' behavior. To sum up, _AdaSociety_ is an adaptive environment.

Fig. 5 describes the mutual adaption between agents and _AdaSociety_. To achieve their goals, agents learn policies to adapt their (physical and social) behavior to the environment. Meanwhile, agents' behavior will affect and even change the environment. Specifically, physical actions will expand the physical state space and the corresponding physical action space, reward, and transition functions by synthesizing new resources. Social actions will alter social connections, and then influence agents' information access and reward structures. In _AdaSociety_, there is a feedback loop between agents and the environment, making their coevolution possible and may shed light on the generation of infinite tasks.

#### a.6.2 A Multi-Layer Directed Graph Expression for Social Structure

As shown in Fig. 6, _AdaSociety_ expresses social states as a multi-layer directed graph. Each layer shows a level of social organization. _AdaSociety_ supports the description of social structures with arbitrary levels, depending on the research problems and the required granularity of social structures. The bottom 0th-level consists of individual agents, who are the fundamental units of decision-making. Nodes in each layer represent entities/agents in the corresponding level. Any agent on the kth-level (\(k\geq 1\)) is composed of its connected agents on the (k-1)th-level. Its decision-making relies on group norms, like voting, consensus decision-making and delegation. A kth-level agent will affect its (k-1)th-level neighbors' reward functions and observations, thereby influencing their decision-making and enabling their division of labour and cooperation. One agent on the (k-1)th-level may be simultaneously subordinate to any number of agents on the kth-level. For example, an individual employee is the 0th-level agent, a project team composed of several employees is the 1st-level agent, a company consisting of many teams is the 2nd-level agent, and a business group composed of many companies is the 3rd-level agent.

Figure 5: Illustration of the mutual adaptation between agents and _AdaSociety_.

Figure 6: An illustration of social states expressed as a multi-layer directed graph.

_AdaSociety_ supports the emergence of high-level social organizations. Edges inside one layer represent cooperative connections, which share information or rewards between involved entities. Edges across layers represent subordinate connections, with low-level entities complying with the policy implemented by the high-level entities. Modeling social states as a multi-layer graph will facilitate the application of existing graph theory knowledge to our research.

## Appendix B Research Challenges

**Exploration.** Agents start with a few resources and events within a simple environment initially. As the agents explore the synthesis tree, their behaviors trigger the mechanisms to depict changes in the physical environment. During this process, more resources and events are unlocked gradually, increasing the complexity of the exploration. Dependency between different resources and events evaluates the agents' abilities to make deep explorations in the environment actively.

**Adaptation.** In _AdaSociety_, agents' behaviors could trigger the environment to evolve while the changed environment affects actions that agents can take. Apart from the physical environment, the social structure of the agents could dynamically change as a consequence of either pre-defined rules or agent social behaviors. This requires agents to make decisions accordingly to adapt to and co-evolve with dynamic environments and social relationships.

**Social cognition.** Agents have beliefs in their social structures, which explicitly represent how they interact with other agents, such as exchanging information and sharing rewards. In this complex environment, several achievements require collaboration while resources are limited, forcing agents to cognitively infer others' intentions, evaluate the effectiveness of social structures, and then investigate better choices. This makes _AdaSociety_ a suitable environment for studying social cognition and behaviors, such as heterogeneous roles, labor division, ownership, and trust/betrayal.

**Communication.** Portal for communication is provided to agents for sharing information and coordinating actions. A successful agent may learn various communication protocols, context representations, and information processing for optimal objectives. Thus _AdaSociety_ could be used for studying the effectiveness of agent communication-enabled interactions, such as negotiation for resource trading, information transitivity, and semantic interoperability.

**Collective reasoning.** Agents are embedded with heterogeneous skills while they only know their own skills. The complex synthesis tree requires agents' abilities to make group decisions on collaboration, such as knowledge sharing and skill transferring for greater group benefit. Additionally, the dynamics of environments make collective reasoning harder, especially for temporal credit assignments. For instance, agents may offer tools (negative immediate reward) to collaborators to exploit unexplored resources (greater delayed reward). Therefore, _AdaSociety_ brings challenges for collective reasoning, such as adaptive cooperation and coordination, consensus, and conflict resolution.

**Emergence.** Action space in multiple perspectives, including physical actions, social actions, and communication, enables massive possibilities of agent behaviors without explicit policies. In _AdaSociety_, one could observe the emergence of coordination and cooperation, social structures and norms, and even communication protocols and language.

## Appendix C Mini-Game Details

### Social Structure

As stated in Sec. 3, agents connected by edges share observations, thereby improving their collective situational awareness. Agents connecting to the same group node share rewards based on the edge attributes, incentivizing collaborative efforts to achieve greater rewards. With the social structure, agents act synchronously in the physical environment, following the mechanism for sharing observation and reward defined by the social structure.

In this mini-game, we conducted experiments with static and dynamic social structures. In the static setting, agents are initialized with a certain social structure \(\mathcal{G}\) and keep the structure until the end of one episode. In this paper, we categorize social structures that are less than two layers into five types to examine the effects of varying structures on agent behavior and performance: 1) **Isolation:** agents are fully unconnected, i.e., \(\mathcal{C}=1;\mathcal{E}=\emptyset\); 2) **Connection:** agents are connected without forming groups, i.e., \(\mathcal{C}=1;\mathcal{E}\neq\emptyset\); **3) Independent Group:** agents are grouped while each agent joins at most one group, i.e., \(\mathcal{C}=2;\sum_{j}^{{}^{\mathcal{V}}2}A_{ij}=1\ \forall i\in\mathcal{V}^{1};\) **4) Overlapping Group:** agents are grouped and can join multiple groups, i.e., \(\mathcal{C}=2;\exists i\in\mathcal{V}^{1},\ \sum_{j}^{{}^{\mathcal{V}}2}A_{ij}>1;\) **5) Inequality:** agents are grouped with different reward weights.

In the dynamic setting, pre-defined rules for structure change could be designed to compel agents to alter their social relationships while they take actions within the physical environment. In this task, we design a **Dynamic** scenario, where the social structure starts with **Inequality**, then switches to **Ind. group** at step 30, and alters to **Ovlp. group** at step 60.

### _Exploration_

In this scenario, all built-in resources and events are included. Physical actions and social actions are available at every step. All agents share a common value preference, where 1) resources near the end of the synthesis tree are assigned high value, and 2) synthetic resources are valued higher than natural resources. Due to partial observation, time limitations, and the challenges associated with exploring new resources, agents may manipulate the social state to encourage interest binding, information sharing, and division of labor, which helps to maximize rewards.

### Parameters of Mini-Games

The parameters of the Easy task and the Hard task of _Social Structure_, _Contract_ and _Negotiation_, along with the parameters of _Exploration_ are shown in Tab. 7.

## Appendix D Baseline Details

### Curriculum Learning

We developed a curriculum learning algorithm for _AdaSociety_. The algorithm controls the social state to promote group cooperation and guides the agent to learn rational physical policies before learning social policies. Our curriculum consists of three stages. We use RecPPO for RL training in each stage.

In the first stage, all individual nodes are compelled to connect to the same group node. This arrangement ensures that all agents belong to the same group. If the reward assigned to an individual increases monotonically with respect to the group reward, which is a common setting, agents in this

\begin{table}
\begin{tabular}{c|c c c} \hline \hline Parameter & Easy & Hard & _Exploration_ \\ \hline \(h,w\) & \(7,7\) & \(15,15\) & \(20,20\) \\ \(|B|\) & 0 & 0 & 25 \\ \(y^{pos}\) & - & - & random \\ \(\varrho\) & \{wood, stone, hammer\} & \{wood, stone, hammer, & & all built-in resources \\  & & coal, torch, iron\} & & \\ \(\mathcal{E}\) & 41 HammerCraft & 98 HammerCraft, & 40 HammerCraft, 40 TorchCraft, 30 SteelMaking, \\  & & 98 TorchCraft & 30 Potting, 20 ShoveCraft, 20 PickaxeCraft, \\  & & & 20 CutterCraft, 10 GemCutting, 10 TotemMaking \\ \(\mathcal{E}^{pos}\) & random & random & random \\ \(m_{i}(0)\) & empty & empty & empty \\ \(i^{exp}\) & carpenter: \{hammer: 1\} & carpenter: \{hammer: 1, coal: 0\} & default \\  & miner: \{wood: 0, stone: 0\} & miner: \{stone: 0, torch: 1, iron: 0\} & \\ \(h_{i}\) & carpenter: default & carpenter: \{coal: 5, torch: 1.5, iron: 20\%\} & default \\  & miner: \{hammer: 2\} & miner: \{coal: 5, torch: 1.5, iron: 20\%\} & \\ \(i^{pos}(0)\) & random & random & random \\ \hline \hline \end{tabular}
\end{table}
Table 7: Parameters of mini-games. When a resource \(\rho\) is not specified, \(i^{cap}(\rho)\) defaults to \(\infty\) and \(h_{i}(\rho)\) defaults to \(1\).

[MISSING_PAGE_FAIL:22]

Figure 8: Evaluation results of _Negotiation_. Upper row: Easy; lower row: Hard.

Figure 10: _Social Structure_**-Isolation**:(a) Individual rewards per step using 100 samples from different policies (b) Learning curves using different methods. (c) Fairness of agents using different methods, (d) Completion rate of events using different methods

Figure 7: Evaluation results of _Contract_. Upper row: Easy; lower row: Hard.

Figure 9: _Social Structure_**-Dynamic**: (a) Fairness of agents using different methods, (b) Completion rate of events using different methods.

Figure 11: _Social Structure_-**Connection**: (a) Individual rewards per step using 100 samples from different policies (b) Learning curves using different methods. (c) Fairness of agents using different methods, (d) Completion rate of events using different methods

Figure 14: _Social Structure_-**Inequality**: (a) Individual rewards per step using 100 samples from different policies (b) Learning curves using different methods. (c) Fairness of agents using different methods, (d) Completion rate of events using different methods

Figure 12: _Social Structure_-**Independent Group**: (a) Individual rewards per step using 100 samples from different policies (b) Learning curves using different methods. (c) Fairness of agents using different methods, (d) Completion rate of events using different methods

Figure 13: _Social Structure_-**Overlapping Group**: (a) Individual rewards per step using 100 samples from different policies (b) Learning curves using different methods. (c) Fairness of agents using different methods, (d) Completion rate of events using different methods

\begin{table}
\begin{tabular}{c c c c c c c} \hline Time (hours) \(|\) Game Steps & CL & \multicolumn{2}{c}{PPO} & \multicolumn{2}{c}{MAPPO} & Rainbow \\ \hline Negotiation-Easy & 5.66\(\pm\) 0.16 14M & 0.12\(\pm\) 0.03 1.047M & 1.05\(\pm\) 0.041 2.4M & 21.21\(\pm\) 0.35 1.8M & 14.97\(\pm\) 0.30 142M \\ Negotiation-Hard & 12.96\(\pm\) 0.39 15M & 1.66\(\pm\) 0.03 1 2.3M & 0.30\(\pm\) 0.01 1.049M & 74.85\(\pm\) 9.75 14.4M & 40.54\(\pm\) 0.54 51 53M \\ Contract-Easy & 32.03\(\pm\) 1.38 112M & 0.19\(\pm\) 0.081 0.80M & 1.94\(\pm\) 0.031 6.0M & 9.65\(\pm\) 0.021 1.2M & 9.65\(\pm\) 1.49 125M \\ Contract-Hard & 48.58\(\pm\) 0.56 178M & 0.21\(\pm\) 0.021 0.56M & 0.74\(\pm\) 0.011 1.0M & 14.94\(\pm\) 0.01 1.094M & 19.07\(\pm\) 1.25 137M \\ Social Structure - Dynamic & 6.98\(\pm\) 0.64 14.8M & 6.90\(\pm\) 0.58 16.0M & 10.16\(\pm\) 0.12 16.0M & 46.23\(\pm\) 1.21 14.8M & 2.34\(\pm\) 3.76 12.0M \\ \hline \end{tabular}
\end{table}
Table 10: Time (in hours) and the number of game steps taken by algorithms to converge.

\begin{table}
\begin{tabular}{c c c c} \hline
4p & 8p & 20p & 100p & 1000p \\ \hline
2495.58\(\pm\) 26.33 & 1245.38\(\pm\) 3.65 & 395.33\(\pm\) 2.52 & 42.58\(\pm\) 0.26 & 2.60\(\pm\) 0.09 \\ \hline \end{tabular}
\end{table}
Table 11: Average number of game steps per second for different player counts (4, 8, 20, 100, 1000). The number of groups is the same as the number of players. The experiment is conducted in _Exploration_, where all built-in resources and events are included (see details in Sec. C.2).

\begin{table}
\begin{tabular}{c c c} \hline  & _Contract-Easy_ & _Contract-Hard_ \\ \hline Stage 1 & 0.9747\(\pm\) 0.0059 & 0.6470\(\pm\) 0.0313 \\ Stage 2 & 0.3435\(\pm\) 0.0262 & 0.2566\(\pm\) 0.0284 \\ Stage 3 & 0.9136\(\pm\) 0.0023 & 0.2773\(\pm\) 0.0466 \\ \hline \end{tabular}
\end{table}
Table 12: Group rewards in _Contract_ after each stage in Curriculum Learning.

Broader Impact

To contribute to the development of multi-agent decision-making algorithms, we propose _AdaSociety_, a customizable environment with massive and diverse tasks generated by expanding state and action spaces and adaptive social structures. Due to the complexity of tasks and the heterogeneity of agents' capacities and preferences, agents need to team up and even cooperatively establish hierarchical social structures to achieve goals. However, agents may also learn some strategies that are harmful to their co-players, as is common in multi-agent research. We have made significant efforts to mitigate such behaviors through thoughtful design within the environment. Given the heterogeneity among agents and adaptive social structures, harmful behaviors tend to be short-sighted and inferior when it comes to maximizing long-term benefits, with stable cooperation emerging as the optimal strategy. The multiple evaluation metrics introduced in _AdaSociety_, like fairness, also empower researchers to identify and exclude extreme or exploitative agents and facilitate the learning of cooperative behaviors.

Nevertheless, some harmful behaviors may still arise during training. We ask researchers utilizing our platform to meticulously observe agents' behaviors to ensure they align with human values and preferences. Should any misalignment or misrepresentation happen, we encourage contributions to the source code (including but not limited to new evaluation metrics, environmental dynamics or incentive mechanisms) to enhance the platform.