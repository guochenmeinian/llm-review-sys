# SMPLer-X: Scaling Up Expressive

Human Pose and Shape Estimation

 Zhongang Cai\({}^{*,1,2,3}\), Wanqi Yin\({}^{*,2,4}\), Ailing Zeng\({}^{5}\), Chen Wei\({}^{2}\), Qingping Sun\({}^{2}\),

**Yanjun Wang\({}^{2}\)**, Hui En Pang\({}^{1,2}\), Haiyi Mei\({}^{2}\), Mingyuan Zhang\({}^{1}\),

**Lei Zhang\({}^{5}\)**, **Chen Change Loy\({}^{1}\), Lei Yang\({}^{{\dagger},2,3}\), Ziwei Liu\({}^{{\dagger},1}\)**

\({}^{1}\) S-Lab, Nanyang Technological University, \({}^{2}\) SenseTime Research, \({}^{3}\) Shanghai AI Laboratory,

\({}^{4}\) The University of Tokyo, \({}^{5}\) International Digital Economy Academy (IDEA)

Equal contributions. \({}^{\dagger}\)Co-corresponding authors.

\({}^{1}\) S-Lab, Nanyang Technological University, \({}^{2}\) SenseTime Research, \({}^{3}\) Shanghai AI Laboratory,

\({}^{4}\) The University of Tokyo, \({}^{5}\) International Digital Economy Academy (IDEA)

###### Abstract

Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first _generalist_ foundation model (dubbed **SMPLer-X**), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. _1) For the data scaling_, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. _2) For the model scaling,_ we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into _specialist_ models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 _mm_ NMVE), UBody (57.4 _mm_ PVE), EgoBody (63.6 _mm_ PVE), and EHF (62.3 _mm_ PVE without finetuning). 2

Footnote 2: Homepage: https://caizhongang.github.io/projects/SMPLer-X/.

## 1 Introduction

The recent progress in expressive human pose and shape estimation (EHPS) from monocular images or videos offers transformative applications for the animation, gaming, and fashion industries. This task typically employs parametric human models (_e.g._, SMPL-X [49]) to adeptly represent the highly complicated human body, face, and hands. In recent years, a large number of diverse datasets have entered the field [4, 6, 6, 61, 66, 37, 3, 12, 14, 14, 62, 7], providing the community new opportunities to study various aspects such as capture environment, pose distribution, body visibility, and camera views. Yet, the state-of-the-art methods remain tethered to a limited selection of these datasets, creating a bottleneck in performance across varied scenarios and hindering the ability to generalize to unseen situations.

Our mission in this study is to explore existing data resources comprehensively, providing key insights crucial for establishing robust, universally applicable models for EHPS. Accordingly, we establish the first systematic benchmark for EHPS, utilizing 32 datasets and evaluating their performanceacross five major benchmarks. We find that there are significant inconsistencies among benchmarks, revealing the overall complicated landscape of EHPS, and calling for data scaling to combat the domain gaps between scenarios. This detailed examination emphasizes the need to reassess the utilization of available datasets for EHPS, advocating for a shift towards more competitive alternatives that offer superior generalization capabilities, and highlights the importance of harnessing a large number of datasets to capitalize on their complementary nature.

Moreover, we systematically investigate the contributing factors that determine the transferability of these datasets. Our investigation yields useful tips for future dataset collection: 1) the more is not necessarily, the merrier: datasets do not have to be very large to be useful as long as they exceed approximately 100K instances based on our observation. 2) Varying indoor scenes is a good alternative if an in-the-wild (including outdoor) collection is not viable. 3) synthetic datasets, despite having traceable domain gaps, are becoming increasingly potent to a surprising extent. 4) Pseudo-SMPL-X labels are useful when ground truth SMPL-X annotations are unavailable.

Equipped with the knowledge procured from the benchmark, we exhibit the strength of massive data with SMPLer-X, a _generalist_ foundation model that is trained using a diverse range of datasets and achieves exceptionally balanced results across various scenarios. To decouple from algorithmic research works, we design SMPLer-X with a minimalist mindset: SMPLer-X has a very simple architecture with only the most essential components for EHPS. We hope SMPLer-X could facilitate massive data and parameter scaling and serve as a baseline for future explorations in the field instead of a stringent investigation into the algorithmic aspect. Experiments with various data combinations and model sizes lead us to a well-rounded model that excels across all benchmarks that contests the community norm of limited-dataset training. Specifically, our foundation models demonstrate significant performance boost through both data scaling and model size scaling, reducing the mean primary errors on five major benchmarks (AGORA [48], UBody [37], EgoB-ody [66], 3DPW [56], and EHF [49]) from over 110 mm to below 70 mm (demonstrated in Fig. 1), and showcases impressive generalization capabilities by effectively transferring to new scenarios, such as DNA-Rendering [7] and ARCTIC [12].

Furthermore, we validate the efficacy of finetuning our _generalist_ foundation models to evolve into domain-specific _specialists_, delivering outstanding performance on all benchmarks. Specifically, we follow the same data selection strategy that empowers our specialist models to set new records on the AGORA leaderboard by being the first model to hit 107.2mm in NMVE (an 11.0% improvement) and achieving SOTA performance on EgoBody, UBody, and EHF.

Our contributions are three-fold. **1)** We build the first systematic and comprehensive benchmark on EHPS datasets, which provides critical guidance for scaling up the training data toward robust and transferable EHPS. **2)** We explore both data and model scaling in building the _generalist_ foundation model that delivers balanced results across various scenarios and extends successfully to unseen datasets. **3)** We extend the data selection strategy to finetune the foundation model into potent _specialists_, catering to various benchmark scenarios.

## 2 Related Work

**Expressive Human Pose and Shape Estimation (EHPS).** Due to the erupting 3D virtual human research applications [64; 65; 19; 18; 5] and the parametric models (e.g., SMPL [40] and SMPL-X [49]), capturing the human pose and shape (HPS) [26; 31; 28; 29; 36; 57; 58], and additionally hands and face (EHPS) [49; 59; 8; 51; 68; 13; 54; 63] from images and videos have attracted increasing

Figure 1: **Scaling up EHPS. Both data and model scaling are effective in reducing mean errors on primary metrics across key benchmarks: AGORA [48], UBody [37], EgoBody [66], 3DPW [56] and EHF [49]. OSX [37] and H4W [44] are SOTA methods. Area of the circle indicates model size, with ViT variants as the reference (top right).**

attention. Optimization-based methods (e.g., SMPLify-X [49]) detect 2D features corresponding to the whole body and fit the SMPL-X model. However, they suffer from slow speed and are ultimately limited by the quality of the 2D keypoint detectors. Hence, learning-based models are proposed. One of the key challenges of EHPS is the low resolution of hands and face compared with the body-only estimation, making the articulated hand pose estimation and high-quality expression capture difficult. Accordingly, mainstream whole-body models first detect and crop the hands and face image patches, then resize them to higher resolutions and feed them into specific hand and face networks to estimate the corresponding parameters [8; 51; 68; 13; 54; 44; 63; 33]. Due to the highly complex multi-stage pipelines, they inevitably cause inconsistent and unnatural articulation of the mesh and implausible 3D wrist rotations, especially in occluded, truncated, and blurry scenes. Recently, OSX [37] proposes the first one-stage framework based on ViT-based backbone [11] to relieve the issues in previous multi-stage pipelines. This method provides a promising and concise way to scale up the model. However, they only use confined training datasets for a fair comparison and do not explore the combination of more data toward generalizable and precise EHPS.

**Multi-dataset Training for Human-centric Vision.** Recent efforts have been using multiple datasets in pretraining a general model for a wide range of downstream human-centric tasks. For example, HumanBench [55] leverages 37 datasets, whereas UniHCP [9] utilizes 33 datasets for tasks such as ReID, pedestrian detection, and 2D pose estimation. However, these works have only evaluated the efficacy of 2D tasks. Sarandi _et al._[52] take advantage of 28 datasets in training a strong model for 3D keypoint detection, which recovers only the skeleton of subjects without estimating body shapes and meshes. Pang _et al._[47] analyze 31 datasets for human pose and shape estimation (_i.e._, SMPL estimation). However, hands and face estimation is not included, and only fewer than ten datasets are used concurrently in the most diverse training. This paper targets to scale training data and model size for EHPS, that simultaneously recovers the expressive pose and shape of the human body, hands, and face.

## 3 Benchmarking EHPS Datasets

### Preliminaries

**SMPL-X.** We study expressive human pose and shape estimation via 3D parametric human model SMPL-X [49], which models the human body, hands, and face geometries with parameters. Specifically, our goal is to estimate pose parameters \(\theta\in\mathbb{R}^{55\times 3}\) that include body, hands, eyes, and jaw poses; joint body, hands and face shape \(\beta\in\mathbb{R}^{10}\), and facial expression \(\psi\in\mathbb{R}^{10}\). The joint regressor \(\mathcal{J}\) is used to obtain 3D keypoints from parameters via \(R_{\theta}(\mathcal{J}(\beta))\) where \(R_{\theta}\) is a transformation function along the kinematic tree.

Figure 2: **Dataset attribute distributions.** a) and d) are image feature extracted by HumanBench [55] and OSX [37] pretrained ViT-L backbone. b) Global orientation (represented by rotation matrix) distribution. c) Body pose (represented by 3D skeleton joints) distribution. Both e) scenes and f) Real/Synthetic are drawn on the same distribution as d). All: all datasets. UMAP [41] dimension reduction is used with the x and y-axis as the dimensions of the embedded space (no unit).

**Evaluation Metrics.** We use standard metrics for EHPS. PVE (per-vertex error) and MPJPE (mean per-joint position error) measure the mean L2 error for vertices and regressed joints, respectively. The "PA" prefix indicates Procrutes Alignment is conducted before error computation. AGORA Leaderboard [48] introduces NMVE (normalized mean vertex error) and NMJE (normalized mean joint error) that take detection performance F1 score into consideration. Moreover, we propose MPE (mean primary error) that takes the mean of multiple primary metrics (MPJPE for 3DPW [56] test, and PVE for AGORA, UBody, EgoBody, and EHF) to gauge generalizability. All errors are reported in millimeters (mm).

### Overview of Data Sources

In this work, we study three major types of datasets. 1) motion capture datasets that leverage optical [21; 14; 12; 15; 16; 42] or vision-based [66; 17; 7; 5] multi-view motion capture systems, are typically collected in a studio environment. However, it is possible to include an outdoor setup, or utilize additional sensors such as IMUs [56]. These datasets generally provide high-quality 3D annotations but are less flexible due to physical constraints, especially those built with immobile capture systems that require accurate sensor calibrations. 2) pseudo-annotated datasets [38; 1; 34; 37; 43; 27; 67; 2; 23; 46; 62; 53] that re-annotate existing image datasets with parametric human annotations [24; 45; 37]. These datasets take advantage of the diversity of 2D datasets, and the pseudo-3D annotations, albeit typically not as high-quality, have been proven effective [47; 31; 24]. 3) synthetic datasets [4; 6; 30; 48; 61] that are produced with renderings engines (_e.g._, Unreal Engine). These datasets produce the most accurate 3D annotations and can easily scale up with high diversity. However, the synthetic-real gap is not fully addressed. Key attributes of the datasets are included in Table 1.

To evaluate the EHPS capability across diverse scenarios, we select multiple key datasets to form a comprehensive benchmark. They should possess the desirable traits such as 1) having accurate SMPL or SMPL-X annotations, 2) being representative of certain aspects of real-life scenarios, 3) being widely used, but this requirement is relaxed for the new datasets which are released within two years, and 4) has a clearly defined test set. To this end, five datasets (AGORA [48], UBody [37], EgoBody [66], 3DPW [56], and EHF [49]) representing different aspects are selected as the evaluation datasets. We briefly introduce these five datasets and the rest in the Supplementary Material. **AGORA

is the most widely-used benchmark for SMPL-X evaluation. It is a synthetic dataset featuring diverse subject appearances, poses, and environments with high-quality annotation. We evaluate on both validation and test set (leaderboard) as the latter has a monthly limit of submissions. **UBody** is the latest large-scale dataset with pseudo-SMPL-X annotations that covers fifteen real-life scenarios, such as talk shows, video conferences, and vologs, which primarily consist of the upper body in images. We follow the intra-scene protocol in training and testing, where all scenarios are seen. **EgoBody** captures human motions in social interactions in 3D scenes with pseudo-SMPL-X annotations. It comprises a first-person egocentric set (EgoSet) and a third-person multi-camera set (MVSet). We test on the EgoSet with heavy truncation and invisibility. **3DPW** is the most popular in-the-wild dataset with SMPL annotations. Since SMPL-X annotation is not available, we map SMPL-X keypoints and test on 14 LSP [22] keypoints following the conventional protocol [26; 31]. **EHF** is a classic dataset with 100 curated frames of one subject in an indoor studio setup, with diverse body poses and especially hand poses annotated in SMPL-X vertices. It has a test set but no training or validation sets. Hence, it is only used to evaluate cross-dataset performance.

Besides being popular or the latest evaluation sets for EHPS, we further analyze if these five datasets collectively provide wide coverage of existing datasets. In Fig. 3, we randomly downsample all datasets to equal length (1K examples) and employ UMAP [41] to visualize several key aspects. We use pretrained ViT-L from HumanBench [55] and OSX [37] to process patch tokens flattened as feature vectors from images cropped by bounding boxes. HumanBench is trained for various human-centric tasks (_e.g._, Re-ID, part segmentation, and 2D pose estimation), whereas OSX is an expert model on EHPS. As for global orientation, it is closely associated with camera pose as we convert all data into the camera coordinate frame; we plot its distribution by using flattened rotation matrix representations. Moreover, we follow [50; 6; 47] to represent poses as 3D keypoints regressed from the parametric model. Specifically, we flatten 21 SMPL-X body keypoints, and 15 hand keypoints from each hand, regressed with zero parameters except for the body pose and hand poses. It is shown that 1) the five benchmark datasets have varied distribution, which is expected due to their different designated purposes, and 2) collectively, the five datasets provide a wide, near-complete coverage of the entire dataset pool.

### Benchmarking on Individual Datasets

In this section, we aim to benchmark datasets and find those that do well in various scenarios. To gauge the performance of each dataset, we train a SMPLer-X model with the training set of that dataset and evaluate the model on the _val/testing_ sets of five evaluation datasets: AGORA, UBody, EgoBody, 3DPW, and EHF. Here, the benchmarking model is standardized to use ViT-S as the backbone, trained on 4 V100 GPUs for 5 epochs with a total batch size of 128 and a learning rate of \(1\times 10^{-5}\). The dataset preprocessing details are included in the Supplementary Material.

In Table 1, we report the primary metrics (Sec. 3.1) and ranking of the 32 datasets. The complete results in the Supplementary Material. We also compute the mean primary error (MPE) to facilitate easy comparison between individual datasets. Note that for AGORA, UBody, EgoBody, and 3DPW, their performances on their own test set are excluded from computing MPE. This is because in-domain evaluation results are typically much better than cross-domain ones, leading to significant error drops. In addition, note that there are datasets designed for specific purposes (_e.g._, Talkshow [62] for gesture generation, DNA-Rendering [7] for human NeRF reconstruction), being ranked lower on our benchmark, which focuses on EHPS (a perception task) does not reduce their unique values and contributions to the computer vision community.

From the benchmark, we observe models trained on a single dataset tend to perform well on the same domain but often cannot do well on other domains. For example, the model trained on AGORA is ranked \(1^{st}\) on AGORA (val), but \(6^{th}\) on UBody, \(6^{th}\) on EgoBody, \(12^{th}\) on 3DPW, and \(24^{th}\) on EHF. This observation indicates that 1) the test scenarios are diverse, showcasing the challenging landscape of EHPS, and 2) data scaling is essential for training a robust and transferable model for EHPS due to significant gaps between different domains.

### Analyses on Dataset Attributes

In this section, we study attributes that contribute to generalizability. However, it is important to acknowledge that such analyses are not a straightforward task: the attributes often exhibit coupled effects. Consequently, counter-examples are inevitable (_e.g._, we observe that InstaVariety, an in-the-wild dataset, demonstrates strong performance, whereas LSPET, another in-the-wild dataset, does not perform as well). Despite the challenges in pinpointing the exact factors that determine the success of an individual dataset, we adopt a collective perspective and aim to identify general trends with several key factors [47, 39, 6] in Fig. 3, and discussed below.

**First**, Fig. 3a) shows that the performance of a dataset (in terms of ranking) is not strongly associated with the number of training instances once the instance number exceeds approximately 100K. Although a very small amount of training data is insufficient to train a strong model, having an exceedingly large amount of data does not guarantee good performance either. For example, MSCOCO only comprises 149.8K training instances but achieves a higher ranking compared to datasets with 10\(\times\) larger scales. This may be attributed to the diverse appearance and complex scenes present in the MSCOCO dataset. Hence, it would be more cost-effective to channel resources to improve diversity and quality, when the dataset has become adequately large.

**Second**, we categorize datasets into 1) in-the-wild, which contains data from diverse environments; 2) indoor with several scenes; 3) studio, which has a fixed multi-view setup. Particularly, Fig. 3b) shows that the top 10 are mostly in-the-wild datasets, indoor datasets concentrate in the top 20 and the studio dataset tends to be ranked lower in the benchmark. Moreover, Fig. 2e) illustrates that in-the-wild datasets exhibit the most diverse distribution, covering both indoor and studio datasets. Indoor datasets display a reasonable spread, and studio datasets have the least diversity. Our findings validate previous studies that suggest an indoor-outdoor domain gap [25]. Differing from Pang _et al._[47], which does not differentiate between indoor and studio datasets, we argue that categorizing all datasets collected indoors into a single class oversimplifies the analysis. For example, consider EgoBody [66] and Human3.6M [21]. Both datasets does not have outdoor data; however, EgoBody consists of a wide variety of indoor scenes, whereas Human3.6M consists of only one scene, which may contribute to the better ranking of EgoBody compared to Human3.6M. Hence, this suggests that in-the-wild data collection is the most ideal, but diversifying indoor scenes is the best alternative.

**Third**, most of the five contemporary synthetic datasets [4, 61, 48, 30, 6] demonstrate surprising strength and are ranked highly in Fig. 3c). It is worth noting that four (UBody, EgoBody, 3DPW, and EHF) of the five evaluation benchmarks used are real datasets, indicating that knowledge learned from synthetic data is transferable to real scenarios. To explain this observation, we take a close look at Fig. 2f): although real and synthetic datasets do not have extensive overlap, synthetic data possesses two ideal characteristics. First, there is a high overlap between real and synthetic data at the rightmost cluster. Referring to Fig. 2e), which is drawn from the same distribution, we find that this cluster primarily represents in-the-wild data. Therefore, synthetic data includes a substantial number of in-the-wild images that closely resemble real in-the-wild scenarios. Second, synthetic data also have scatters of image features on other clusters, indicating that synthetic data provides coverage to some extent for various real-world scenarios.

**Fourth**, Fig. 3d) reveals that a dataset can be valuable with accurate or pseudo-SMPL-X annotations, as they constitute the most of the top 10 datasets. A prominent example is InstaVariety [27], which has only pseudo-SMPL-X annotation produced by NeuralAnnot [45], yet, is ranked third in our benchmark. However, due to the differences in parameter spaces, SMPL annotations are less effective: it is observed that datasets with SMPL annotations tend to cluster in the lower bracket of the benchmark, especially those with pseudo-SMPL annotations. This observation suggests that SMPL-X

Figure 3: **Analysis on dataset attributes. We study the impact of a) the number of training instances, b) scenes, c) real or synthetic appearance, and d) annotation type, on dataset ranking in Table 1.**

annotations are critical to EHPS; fitting pseudo labels is a useful strategy even if they could be noisy. Moreover, using SMPL labels effectively for SMPL-X estimation remains a challenge.

## 4 Scaling up EHPS

### Model Architectures

Catering to our investigation, we design a minimalistic framework (dubbed SMPLer-X) that only retains the most essential parts for two reasons. First, it must be scalable and efficient as we train with a large amount of data. Second, we aim to create a framework that is decoupled from specific algorithm designs, providing a clean foundation for future research. To this end, SMPLer-X consists of three parts: a _backbone_ extracts image features, which we employ Vision Transformer [11] for its scalability; a _neck_ that predicts bounding boxes and crop regions of interest from the feature map for hands and face; regression _heads_ that estimate parameters for each part. Note that SMPLer-X does not require third-party detectors [51], cross-part feature interaction modules [8; 13], projection of coarse SMPL-X estimations [63], or a heavy decoder [37]. As the design of SMPLer-X is not the focus of our investigation, more details are included in the Supplementary Material.

### Training the Generalist Foundation Models

The SOTA methods [37; 44] usually train with only a few (_e.g._, MSCOCO, MPII, and Human3.6M) datasets, whereas we investigate training with many more datasets. However, we highlight that the dataset benchmark in Table 1 cannot be used: selecting datasets based on their performance on the test sets of the evaluation benchmarks leaks information about the test sets. Hence, we construct another dataset benchmark in the Supplementary Material, that ranks individual datasets on the _training_ set of the major EHPS benchmarks. We use four data amounts: 5, 10, 20, and 32 datasets as the training set, with a total length of 0.75M, 1.5M, 3.0M, and 4.5M instances. We always prioritize higher-ranked

\begin{table}
\begin{tabular}{c c c c c|c c c c|c} \hline \hline \#Datasets & \#Inst. & Model & \#Param. & FPS & AGORA [48] & EgoBody [66] & UBody [37] & 3DPW [56] & EHF [49] & MPE \\ \hline
5 & 0.75M & SMPLer-X-S5 & 32M & 36.2 & 119.0 & 114.2 & 110.1 & 110.2 & 100.5 & 110.8 \\
10 & 1.58M & SMPLer-X-S10 & 32M & 36.2 & 116.0 & 88.6 & 107.7 & 97.4 & 89.9 & 99.9 \\
20 & 3.0M & SMPLer-X-S20 & 32M & 36.2 & 109.2 & 84.3 & 70.0 & 87.5 & 86.6 & 87.7 \\
32 & 4.5M & SMPLer-X-S32 & 32M & 36.2 & 105.2 & 82.5 & 68.1 & 83.2 & 74.1 & 82.6 \\ \hline
5 & 0.75M & SMPLer-X-S5 & 103M & 33.1 & 102.7 & 108.1 & 105.8 & 104.8 & 96.1 & 103.5 \\
10 & 1.58M & SMPLer-X-B10 & 103M & 33.1 & 97.8 & 76.4 & 107.3 & 89.9 & 74.7 & 89.2 \\
20 & 3.0M & SMPLer-X-B20 & 103M & 33.1 & 95.6 & 75.5 & 65.3 & 83.5 & 73.0 & 78.6 \\
32 & 4.54M & SMPLer-X-B12 & 103M & 33.1 & 88.0 & 72.7 & 63.3 & 80.3 & 67.3 & 74.3 \\ \hline
5 & 0.75M & SMPLer-X-S5 & 327M & 24.4 & 88.3 & 98.7 & 110.8 & 97.8 & 89.5 & 97.0 \\
10 & 1.58M & SMPLer-X-L10 & 327M & 24.4 & 82.6 & 69.7 & 104.0 & 82.5 & 64.0 & 80.6 \\
20 & 3.0M & SMPLer-X-L20 & 327M & 24.4 & 80.7 & 66.6 & 61.5 & 78.3 & 65.4 & 70.5 \\
32 & 4.54M & SMPLer-X-L32 & 327M & 24.4 & 74.2 & 62.2 & 57.3 & 75.2 & 62.4 & 66.2 \\ \hline
5 & 0.75M & SMPLer-X-H5 & 662M & 17.5 & 89.0 & 87.4 & 102.1 & 88.3 & 68.3 & 87.0 \\
10 & 1.5M & SMPLer-X-H10 & 662M & 17.5 & 81.4 & 65.7 & 100.7 & 78.7 & **56.6** & 76.6 \\
20 & 3.0M & SMPLer-X-H12 & 662M & 17.5 & 77.5 & 63.5 & 59.9 & **74.4** & 59.4 & 67.0 \\
32 & 4.54M & SMPLer-X-H132 & 662M & 17.5 & **69.5** & **59.5** & **54.5** & 75.0 & 56.8 & **63.1** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Foundation Models.** We study the scaling law of the amount of data and the model sizes. The metrics are MPJPE for 3DPW, and PVE for other evaluation benchmarks. Foundation models are named “SMPLer-X-MN”, where M indicates the size of ViT backbone (S, B, L, H), N is the number of datasets used in the training. FPS: inference speed (frames per second) on a V100 GPU. MPE: mean primary error. AGORA uses the validation set, and EgoBody uses the EgoSet.

Figure 4: **Architecture of SMPLer-X, which upholds the idea that ”simplicity is beauty”. SMPLer-X contains a backbone that allows for easy investigation on model scaling, a neck for hand and face feature cropping, and heads for different body parts. Note that we wish to show in this work that model and data scaling are effective, even with a straightforward architecture.**

datasets. To prevent larger datasets from shadowing smaller datasets, we adopt a balanced sampling strategy. Specifically, all selected datasets are uniformly upsampled or downsampled to the same length and add up to the designated total length. To facilitate training, we follow OSX [37] to use AGORA, UBody, MPII, 3DPW, Human3.6M in COCO-format [38], and standardize all other datasets into the HumanData [10] format. We also study four ViT backbones of different sizes (ViT-Small, Base, Large and Huge), pretrained by ViTPose [60]. The training is conducted on 16 V100 GPUs, with a total batch size of 512 (256 for ViT-Huge) for 10 epochs. More training details such as adapting SMPL or gendered SMPL-X in the training are included in the Supplementary Material.

In Table 2, we show experimental results with a various number of datasets and foundation model sizes. Foundation models are named "SMPLer-X-MN", where M can be S, B, L, H that indicates the size of the ViT backbone, and N indicates the number of datasets used in the training. For example, SMPLer-X-L10 means the foundation model takes ViT-L as the backbone, and is trained with Top 10 datasets (ranked according to the individual dataset performance on the training sets of the key evaluation benchmarks). It is observed that **1)** more training data (data scaling) leads to better performance in terms of MPE. The model performance improves gradually as the number of training datasets increases. However, besides the increment in training instances, more datasets provide a richer collection of diverse scenarios, which we argue is also a key contributor to the performance gain across evaluation benchmarks. **2)** A larger foundation model (model scaling) performs better at any given amount of data. However, the marginal benefits of scaling up decrease beyond model size L. Specifically, ViT-H has more than twice the parameters than ViT-L, but the performance gain is not prominent. **3)** The foundation model always performs better than in-domain training on a single training set. For example, SMPLer-X-B20, performs better on the validation set of AGORA, and test sets of UBody, EgoBody, and 3DPW, than models trained specifically on the corresponding training set in Table 1. This is useful for real-life applications: instead of training a model for each of the user cases, a generalist foundation model contains rich knowledge to be a one-size-fits-all alternative.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{PA-PVE\({}_{1}\) (_mm_)} & \multicolumn{2}{c}{PVE\({}_{1}\) (_mm_)} \\ \cline{2-6}  & All & Hands & Face & All & Hands & Face \\ \hline Hand4Whole [44] & 73.2 & 9.7 & 4.7 & 183.9 & 72.8 & 81.6 \\ OSX [37] & 69.4 & 11.5 & 4.8 & 168.6 & 70.6 & 77.2 \\ OSX [37] & 45.0 & **8.5** & 3.9 & 79.6 & 48.2 & 37.9 \\ SMPLer-X-B1+ & 48.9 & 8.6 & 4.0 & 86.1 & 51.5 & 41.2 \\ SMPLer-X-L20 & 48.6 & 8.9 & 4.0 & 80.7 & 51.0 & 41.3 \\ SMPLer-X-L32 & 45.1 & 8.7 & **3.8** & 72.4 & 42.8 & 38.2 \\ SMPLer-X-L20J & **39.1** & 9.3 & **3.8** & **62.5** & **42.3** & **32.8** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **AGORA Val set. \(\dagger\) and \(*\) are finetuned on the AGORA training set, and trained on the AGORA training set only, respectively.**

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{MVUE\({}_{1}\) (_mm_)} & \multicolumn{2}{c}{NMUE\({}_{1}\) (_mm_)} & \multicolumn{4}{c}{MVUE\({}_{2}\) (_mm_)} & \multicolumn{4}{c}{MPJPE\({}_{1}\) (_mm_)} \\ \cline{2-13} Method & All & Body & All & Body & All & Body & Face & LHand & RHand & All & Body & Face & LHand & RHand \\ \hline BELDAM [4] & 179.5 & 132.2 & 17.75 & 113.4 & 131.0 & 96.5 & 25.8 & 38.8 & 39.0 & 129.6 & 95.9 & 27.8 & 36.6 & 36.7 \\ Hand4Whole [44] & 144.1 & 96.0 & 141.1 & 92.7 & 135.5 & 90.2 & 41.6 & 46.3 & 48.1 & 132.6 & 87.1 & 46.1 & 44.3 & 46.2 \\ BELDAM [4] & 142.2 & 102.1 & 14.0 & 101.0 & 103.8 & 74.5 & **23.1** & **31.7** & **33.2** & 102.9 & 74.3 & **24.7** & **29.9** & **31.3** \\ PyMx-X [63] & 141.2 & 94.4 & 140.0 & 93.5 & 125.7 & 84.0 & 35.0 & 44.6 & 45.6 & 124.6 & 83.2 & 37.9 & 42.5 & 43.7 \\ OSX [37] & 130.6 & 85.3 & 127.6 & 83.3 & 122.8 & 80.2 & 36.2 & 45.4 & 46.1 & 119.9 & 78.3 & 37.9 & 43.0 & 43.9 \\ HybridX-X [33] & 120.5 & 73.7 & 115.7 & 72.3 & 112.1 & 68.5 & 37.0 & 46.7 & 47.0 & 107.6 & 67.2 & 38.5 & 41.2 & 41.4 \\ SMPLer-X-L20 & 133.1 & 88.1 & 128.9 & 84.6 & 123.8 & 81.9 & 37.4 & 43.6 & 44.8 & 119.9 & 78.7 & 39.5 & 41.4 & 44.8 \\ SMPLer-X-L32 & 122.8 & 80.3 & 191.9 & 77.6 & 114.2 & 74.7 & 35.1 & 41.3 & 42.2 & 110.8 & 72.2 & 36.7 & 39.1 & 40.1 \\ SMPLer-X-L20J & **107.2** & **65.8** & **104.1** & **66.3** & **99.7** & **63.5** & 29.9 & 39.1 & 39.5 & **96.8** & **61.7** & 31.4 & 36.7 & 37.2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **AGORA test set. \(\dagger\) denotes the methods that are finetuned on the AGORA training set. \(*\)denotes the methods that are trained on AGORA training set only.**

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multicolumn{5}{c}{PA-PVE\({}_{1}\) (_mm_)} & \multicolumn{2}{c}{PVE\({}_{1}\) (_mm_)} \\ \cline{2-5} Method & All & Hands & Face & All & Hands & Face \\ \hline Hand4Whole [44] & 50.3 & **10.8** & 5.8 & 76.8 & **39.8** & 26.1 \\ OSX [37] & 48.7 & 15.9 & 6.0 & 70.8 & 53.7 & 26.4 \\ SMPLer-X-L20 & 37.8 & 15.0 & 5.1 & 65.4 & 49.4 & 17.4 \\ SMPLer-X-L32 & **37.1** & 14.1 & **5.0** & **62.4** & 47.1 & **17.0** \\ \hline \hline \end{tabular}
\end{table}
Table 10: **3DPW. \(\ddagger\) denotes the methods that use a head for SMPL regression. \(\dagger\) and \(*\) are finetuned on the 3DPW training set and trained on 3DPW training set only, respectively. Unit: _mm_.**

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & MPJPE & PA-MPJPE \\ \hline \multicolumn{2}{c}{Body-only (SMPL) Methods} \\ \hline OSX-SMPL [37]\(\ddagger\ast\) & 74.7 & 45.1 \\ HybrIK [35] & 71.6 & **41.8** \\ CLIFF [36] & **68.0** & 43.0 \\ \hline \multicolumn{2}{c}{Whole-Body (SMPL-X) Methods} \\ \hline Hand4Whole [44] & 86.6 & 54.4 \\ ExPose [8] & 93.4 & 60.7 \\ OSX [37] & 86.2 & 60.6 \\ SMPLer-X-B1\(\ast\) & 95.6 & 67.6 \\ SMPLer-X-L20 & 78.3 & 52.1 \\ SMPLer-X-L32 & **75.2** & **50.5** \\ SMPLer-X-L20J & 76.8 & 51.5 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **EHF. As EHF does not have a training set to benchmark datasets, we do not perform finetuning. Moreover, EHF is not seen in our training and can be used to validate our foundation models’ transferability.**

[MISSING_PAGE_FAIL:9]

finetune experiments on ViT-L to match the backbone of current SOTA [37]. The results are shown in the same tables as the foundation models (Table 3, 4, 5, 6, 7, and 10), where finetuning always lead to substantial performance enhancement on the foundation models.

## 5 Conclusion

In this work, we benchmark datasets for EHPS that provide us insights for training and finetuning a foundation model. Our work is useful in three ways. First, our pretrained model (especially the backbone) can be a plug-and-play component of a larger system for EHPS and beyond. Second, our benchmark serves to gauge the performances of future generalization studies. Third, our benchmarking-finetuning paradigm can be useful for the rapid adaptation of any foundation model to specific scenarios. Specifically, users may collect a training set, evaluate pretrained models of various other datasets on it, and select the most relevant datasets to finetune a foundation model.

**Limitations.** First, although we use five comprehensive benchmark datasets to gauge the generalization capability, they may still be insufficient to represent the real-world distribution. Second, our experiments do not fully investigate the impact of various model architectures due to the prohibitive cost of training the foundation model.

**Potential negative societal impact.** As we study training strong EHPS models and release the pretrained models, they may be used for unwarranted surveillance or privacy violation.

Figure 5: **Visualization.** We compare SMPLer-X-L32 with OSX [37] and Hand4Whole [44] (trained with the MSCOCO, MPII, and Human3.6M) in various scenarios such as those with heavy truncation, hard poses, and rare camera angles.

## Acknowledgement

This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). The project is also supported by NTU NAP and Singapore MOE AcRF Tier 2 (MOET2EP20221-0012).

## References

* [1]M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and B. Schiele (2018) Posetrack: a benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5167-5176. Cited by: SS1.
* [2]M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele (2014-06) 2d human pose estimation: new benchmark and state of the art analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1.
* [3]B. Lal Bhatnagar, X. Xie, I. A. Petrov, C. Sminchisescu, C. Theobalt, and G. Pons-Moll (2022) Behave: dataset and method for tracking human object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15935-15946. Cited by: SS1.
* [4]M. J. Black, P. Patel, J. Tesch, and J. Yang (2023) Bedlam: a synthetic dataset of bodies exhibiting detailed lifelike animated motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8726-8737. Cited by: SS1.
* [5]Z. Cai, D. Ren, A. Zeng, Z. Lin, T. Yu, W. Wang, X. Fan, Y. Gao, Y. Yu, L. Pan, et al. (2022) Human: multi-modal 4d human dataset for versatile sensing and modeling. In European Conference on Computer Vision, pp. 557-577. Cited by: SS1.
* [6]Z. Cai, M. Zhang, J. Ren, C. Wei, D. Ren, Z. Lin, H. Zhao, L. Yang, C. Change Loy, and Z. Liu (2021) Playing for 3d human recovery. arXiv preprint arXiv:2110.07588. Cited by: SS1.
* [7]W. Cheng, R. Chen, S. Fan, W. Yin, K. Chen, Z. Cai, J. Wang, Y. Gao, Z. Yu, Z. Lin, et al. (2023) Dna-rendering: a diverse neural actor repository for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 19982-19993. Cited by: SS1.
* [8]V. Choutas, G. Pavlakos, T. Bolkart, D. Tzionas, and M. J. Black (2020) Monocular expressive body regression through body-driven attention. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part X 16, pp. 20-40. Cited by: SS1.
* [9]Y. Ci, Y. Wang, M. Chen, S. Tang, L. Bai, F. Zhu, R. Zhao, F. Yu, D. Qi, and W. Ouyang (2023) Unihcp: a unified model for human-centric perceptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17840-17852. Cited by: SS1.
* [10]M. Contributors (2021) Openmmlab 3d human parametric model toolbox and benchmark. Note: https://github.com/open-mmlab/mmhuman3d Cited by: SS1.
* [11]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. In ICLR, Cited by: SS1.
* [12]Z. Fan, O. Taheri, D. Tzionas, M. Kocabas, M. Kaufmann, M. J. Black, and O. Hilliges (2023) Arctic: a dataset for dexterous bimanual hand-object manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12943-12954. Cited by: SS1.

* [13] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J Black. Collaborative regression of expressive bodies using moderation. In _2021 International Conference on 3D Vision (3DV)_, pages 792-804. IEEE, 2021.
* [14] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, and Cristian Sminchisescu. Three-dimensional reconstruction of human interactions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7214-7223, 2020.
* [15] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, and Cristian Sminchisescu. Learning complex 3d human self-contact. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 1343-1351, 2021.
* [16] Mihai Fieraru, Mihai Zanfir, Silviu Cristian Pirlea, Vlad Olaru, and Cristian Sminchisescu. Aifit: Automatic 3d human-interpretable feedback models for fitness training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9919-9928, 2021.
* [17] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J Black. Resolving 3d human pose ambiguities with 3d scene constraints. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 2282-2292, 2019.
* [18] Fangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu. Garment4d: Garment reconstruction from point cloud sequences. _Advances in Neural Information Processing Systems_, 34:27940-27951, 2021.
* [19] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. _ACM Transactions on Graphics (TOG)_, 41(4):1-19, 2022.
* [20] Chun-Hao P Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael J Black. Capturing and inferring dense full-body human-scene contact. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13274-13285, 2022.
* [21] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. _IEEE transactions on pattern analysis and machine intelligence_, 36(7):1325-1339, 2013.
* [22] Sam Johnson and Mark Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In _BMVC_, pages 1-11. British Machine Vision Association, 2010.
* [23] Sam Johnson and Mark Everingham. Learning effective human pose estimation from inaccurate annotation. In _CVPR 2011_, pages 1465-1472. IEEE, 2011.
* [24] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In _2021 International Conference on 3D Vision (3DV)_, pages 42-52. IEEE, 2021.
* [25] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. _3DV_, 2022.
* [26] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 7122-7131, 2018.
* [27] Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics from video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5614-5623, 2019.
* [28] Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. Vibe: Video inference for human body pose and shape estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5253-5263, 2020.
* [29] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges, and Michael J Black. Pare: Part attention regressor for 3d human body estimation. _arXiv preprint arXiv:2104.08527_, 2021.

* [30] Muhammed Kocabas, Chun-Hao P Huang, Joachim Tesch, Lea Muller, Otmar Hilliges, and Michael J Black. Spec: Seeing people in the wild with an estimated camera. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11035-11045, 2021.
* [31] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2252-2261, 2019.
* [32] Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J Black, and Peter V Gehler. Unite the people: Closing the loop between 3d and 2d human representations. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6050-6059, 2017.
* [33] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. Hybrid-x: Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. _arXiv preprint arXiv:2304.05690_, 2023.
* [34] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10863-10872, 2019.
* [35] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. Hybrid: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In _CVPR_, pages 3383-3393. Computer Vision Foundation / IEEE, 2021.
* [36] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. Cliff: Carrying location information in full frames into human pose and shape estimation. In _European Conference on Computer Vision_, pages 590-606. Springer, 2022.
* [37] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. One-stage 3d whole-body mesh recovery with component aware transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21159-21168, 2023.
* [38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _European conference on computer vision_, pages 740-755. Springer, 2014.
* [39] Qiaho Liu, Adam Kortylewski, and Alan L Yuille. Poseexaminer: Automated testing of out-of-distribution robustness in human pose and shape estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 672-681, 2023.
* [40] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. SMPL: A skinned multi-person linear model. _ACM transactions on graphics (TOG)_, 34(6):1-16, 2015.
* [41] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. _arXiv preprint arXiv:1802.03426_, 2018.
* [42] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In _2017 international conference on 3D vision (3DV)_, pages 506-516. IEEE, 2017.
* [43] Dushyant Mehta, Oleksandr Sotnychenko, F. Mueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, and C. Theobalt. Single-shot multi-person 3d pose estimation from monocular rgb. _2018 International Conference on 3D Vision (3DV)_, pages 120-130, 2018.
* [44] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Accurate 3d hand pose estimation for whole-body 3d human mesh estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2308-2317, 2022.
* [45] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Neuralannot: Neural annotator for 3d human mesh training sets. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2299-2307, 2022.

* [46] Lea Muller, Ahmed AA Osman, Siyu Tang, Chun-Hao P Huang, and Michael J Black. On self-contact and human pose. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9990-9999, 2021.
* [47] Hui En Pang, Zhongang Cai, Lei Yang, Tianwei Zhang, and Ziwei Liu. Benchmarking and analyzing 3d human pose and shape estimation beyond algorithms. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [48] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch, David T Hoffmann, Shashank Tripathi, and Michael J Black. AGORA: Avatars in geography optimized for regression analysis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13468-13478, 2021.
* [49] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10975-10985, 2019.
* [50] Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, and Chen Change Loy. Delving deep into hybrid annotations for 3d human recovery in the wild. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5340-5348, 2019.
* [51] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1749-1759, 2021.
* [52] Istvan Sarandi, Alexander Hermans, and Bastian Leibe. Learning 3d human pose estimation from dozens of datasets using a geometry-aware autoencoder to bridge between skeleton formats. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 2956-2966, 2023.
* [53] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Synthetic training for accurate 3d human pose and shape estimation in the wild. In _British Machine Vision Conference (BMVC)_, September 2020.
* [54] Yu Sun, Tianyu Huang, Qian Bao, Wu Liu, Wenpeng Gao, and Yili Fu. Learning monocular mesh recovery of multiple body parts via synthesis. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2669-2673. IEEE, 2022.
* [55] Shixiang Tang, Cheng Chen, Qingsong Xie, Meilin Chen, Yizhou Wang, Yuanzheng Ci, Lei Bai, Feng Zhu, Haiyang Yang, Li Yi, et al. Humanbench: Towards general human-centric perception with projector assisted pretraining. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21970-21982, 2023.
* [56] Timo von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 601-617, 2018.
* [57] Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and Taku Komura. Zoly: Zoom focal length correctly for perspective-distorted human mesh reconstruction. _arXiv preprint arXiv:2303.13796_, 2023.
* [58] Yanjun Wang, Qingping Sun, Wenjia Wang, Jun Ling, Zhongang Cai, Rong Xie, and Li Song. Learning dense uv completion for human mesh recovery. _arXiv preprint arXiv:2307.11074_, 2023.
* [59] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10965-10974, 2019.
* [60] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. _Advances in Neural Information Processing Systems_, 35:38571-38584, 2022.

* [61] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, et al. Synbody: Synthetic dataset with layered human models for 3d human perception and modeling. _arXiv preprint arXiv:2303.17368_, 2023.
* [62] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael J Black. Generating holistic 3d human motion from speech. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 469-480, June 2023.
* [63] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymat-x: Towards well-aligned full-body model regression from monocular images. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [64] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. _arXiv preprint arXiv:2208.15001_, 2022.
* [65] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. _arXiv preprint arXiv:2304.01116_, 2023.
* [66] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VI_, pages 180-200. Springer, 2022.
* [67] Song-Hai Zhang, Ruitong Li, Xin Dong, Paul Rosin, Zixi Cai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min Hu. Pose2seg: Detection free human instance segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 889-898, 2019.
* [68] Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush Tewari, Christian Theobalt, and Feng Xu. Monocular real-time full body capture with inter-part correlations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4811-4822, 2021.