# On the Ability of Graph Neural Networks to

Model Interactions Between Vertices

 Noam Razin, Tom Verbin, Nadav Cohen

Tel Aviv University

{noamrazin,tomverbin}@mail.tau.ac.il, cohennadav@cs.tau.ac.il

###### Abstract

Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as _separation rank_, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, _i.e._ between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's _walk index_ -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named _Walk Index Sparsification_ (_WIS_), which preserves the ability of a GNN to model interactions when input edges are removed. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy.1 More broadly, it showcases the potential of improving GNNs by theoretically analyzing the interactions they can model.

Footnote 1: An implementation of WIS is available at https://github.com/noamrazin/gnn_interactions.

## 1 Introduction

_Graph neural networks_ (_GNNs_) are a family of deep learning architectures, designed to model complex interactions between entities represented as vertices of a graph. In recent years, GNNs have been successfully applied across a wide range of domains, including social networks, biochemistry, and recommender systems (see, _e.g._, [36, 59, 45, 49, 96, 104, 101, 18]). Consequently, significant interest in developing a mathematical theory behind GNNs has arisen.

One of the fundamental questions a theory of GNNs should address is _expressivity_, which concerns the class of functions a given architecture can realize. Existing studies of expressivity largely fall into three categories. First, and most prominent, are characterizations of ability to distinguish non-isomorphic graphs [103, 74, 72, 70, 6, 15, 10, 17, 43, 42, 80], as measured by equivalence to classical Weisfeiler-Leman graph isomorphism tests [99]. Second, are proofs for universal approximation of continuous permutation invariant or equivariant functions, possibly up to limitations in distinguishing some classes of graphs [73, 55, 25, 69, 3, 42]. Last, are works examining specific properties of GNNs such as frequency response [77, 5] or computability of certain graph attributes, _e.g._ moments, shortest paths, and substructure multiplicity [35, 9, 26, 39, 69, 23, 17, 105].

A major drawback of many existing approaches -- in particular proofs of equivalence to Weisfeiler-Leman tests and those of universality -- is that they operate in asymptotic regimes of unboundednetwork width or depth. Moreover, to the best of our knowledge, none of the existing approaches formally characterize the strength of interactions GNNs can model between vertices, and how that depends on the structure of the input graph and the architecture of the neural network.

The current paper addresses the foregoing gaps. Namely, it theoretically quantifies the ability of fixed-size GNNs to model interactions between vertices, delineating the impact of the input graph structure and the neural network architecture (width and depth). Strength of modeled interactions is formalized via _separation rank_[12] -- a commonly used measure for the interaction a function models between a subset of input variables and its complement (the rest of the input variables). Given a function and a partition of its input variables, the higher the separation rank, the more interaction the function models between the sides of the partition. Separation rank is prevalent in quantum mechanics, where it can be viewed as a measure of entanglement [62]. It was previously used for analyzing variants of convolutional, recurrent, and self-attention neural networks, yielding both theoretical insights and practical tools [30; 33; 61; 62; 64; 100; 65; 85]. We employ it for studying GNNs.

Key to our theory is a widely studied correspondence between neural networks with polynomial nonlinearity and _tensor networks_2[32; 29; 30; 34; 90; 61; 62; 7; 56; 57; 63; 64; 83; 100; 84; 85; 65]. We extend this correspondence, and use it to analyze message-passing GNNs with product aggregation. We treat both graph prediction, where a single output is produced for an entire input graph, and vertex prediction, in which the network produces an output for every vertex. For graph prediction, we prove that the separation rank of a depth \(L\) GNN with respect to a partition of vertices is primarily determined by the partition's \((L-1)\)_-walk index_ -- a graph-theoretical characteristic defined to be the number of length \(L-1\) walks originating from vertices with an edge crossing the partition. The same holds for vertex prediction, except that there walk index is defined while only considering walks ending at the target vertex. Our result, illustrated in Figure 1, implies that for a given input graph, the ability of GNNs to model interaction between a subset of vertices \(\mathcal{I}\) and its complement \(\mathcal{I}^{c}\), predominantly depends on the number of walks originating from the boundary between \(\mathcal{I}\) and \(\mathcal{I}^{c}\). We corroborate this proposition through experiments with standard GNN architectures, such as Graph Convolutional Network (GCN) [59] and Graph Isomorphism Network (GIN) [103].

Footnote 2: Tensor networks form a graphical language for expressing contractions of tensors — multi-dimensional arrays. They are widely used for constructing compact representations of quantum states in areas of physics (see, _e.g._, [97; 79]).

Our theory formalizes conventional wisdom by which GNNs can model stronger interaction between regions of the input graph that are more interconnected. More importantly, we show that it facilitates an _edge sparsification_ algorithm that preserves the expressive power of GNNs (in terms of ability to model interactions). Edge sparsification concerns removal of edges from a graph for reducing computational and/or memory costs, while attempting to maintain selected properties of the graph (_cf._[11; 93; 48; 20; 86; 98; 67; 24]). In the context of GNNs, our interest lies in maintaining prediction accuracy as the number of edges removed from the input graph increases. We propose an algorithm for removing edges, guided by our separation rank characterization. The algorithm, named _Walk Index Sparsification_ (_WIS_), is demonstrated to yield high predictive performance for GNNs (_e.g._ GCN and GIN) over standard benchmarks of various scales, even when removing a significant portion of

Figure 1: Illustration of our main theoretical contribution: quantifying the ability of GNNs to model interactions between vertices of an input graph. Consider a partition of vertices \((\mathcal{I},\mathcal{I}^{c})\), illustrated on the left, and a depth \(L\) GNN with product aggregation (Section 3). For graph prediction, as illustrated on the right, the strength of interaction the GNN can model between \(\mathcal{I}\) and \(\mathcal{I}^{c}\), measured via separation rank (Section 2.2), is primarily determined by the partition’s \((L-1)\)_-walk index_ — the number of length \(L-1\) walks emanating from \(\mathcal{C}_{\mathcal{I}}\), which is the set of vertices with an edge crossing the partition. The same holds for vertex prediction, except that there walk index is defined while only considering walks ending at the target vertex.

edges. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracies across edge sparsity levels. More broadly, WIS showcases the potential of improving GNNs by theoretically analyzing the interactions they can model, and we believe its further empirical investigation is a promising direction for future research.

The remainder of the paper is organized as follows. Section 2 introduces notation and the concept of separation rank. Section 3 presents the theoretically analyzed GNN architecture. Section 4 theoretically quantifies (via separation rank) its ability to model interactions between vertices of an input graph. Section 5 proposes and evaluates WIS -- an edge sparsification algorithm for arbitrary GNNs, born from our theory. Lastly, Section 6 concludes. Related work is discussed throughout, and for the reader's convenience, is recapitulated in Appendix B.

## 2 Preliminaries

### Notation

For \(N\in\mathbb{N}\), let \([N]:=\{1,\ldots,N\}\). We consider an undirected input graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) with vertices \(\mathcal{V}=[|\mathcal{V}|]\) and edges \(\mathcal{E}\subseteq\{\{i,j\}:i,j\in\mathcal{V}\}\). Vertices are equipped with features \(\mathbf{X}:=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in \mathbb{R}^{D_{x}\times|\mathcal{V}|}\) -- one \(D_{x}\)-dimensional feature vector per vertex (\(D_{x}\in\mathbb{N}\)). For \(i\in\mathcal{V}\), we use \(\mathcal{N}(i):=\{j\in\mathcal{V}:\{i,j\}\in\mathcal{E}\}\) to denote its set of neighbors, and, as customary in the context of GNNs, assume the existence of all self-loops, _i.e._\(i\in\mathcal{N}(i)\) for all \(i\in\mathcal{V}\) (_cf._[59; 50]). Furthermore, for \(\mathcal{I}\subseteq\mathcal{V}\) we let \(\mathcal{N}(\mathcal{I}):=\cup_{i\in\mathcal{I}}\mathcal{N}(i)\) be the neighbors of vertices in \(\mathcal{I}\), and \(\mathcal{I}^{c}:=\mathcal{V}\setminus\mathcal{I}\) be the complement of \(\mathcal{I}\). We use \(\mathcal{C}_{\mathcal{I}}\) to denote the boundary of the partition \((\mathcal{I},\mathcal{I}^{c})\), _i.e._ the set of vertices with an edge crossing the partition, defined by \(\mathcal{C}_{\mathcal{I}}:=\{i\in\mathcal{I}:\mathcal{N}(i)\cap\mathcal{I}^{c} \neq\emptyset\}\cup\{j\in\mathcal{I}^{c}:\mathcal{N}(j)\cap\mathcal{I}\neq \emptyset\}\).3 Lastly, we denote the number of length \(l\in\mathbb{N}_{\geq 0}\) walks from any vertex in \(\mathcal{I}\subseteq\mathcal{V}\) to any vertex in \(\mathcal{J}\subseteq\mathcal{V}\) by \(\rho_{l}(\mathcal{I},\mathcal{J})\).4 In particular, \(\rho_{l}(\mathcal{I},\mathcal{J})=\sum_{i\in\mathcal{I},j\in\mathcal{J}}\rho_ {l}(\{i\},\{j\})\).

Footnote 3: Due to the existence of self-loops, \(\mathcal{C}_{\mathcal{I}}\) is exactly the shared neighbors of \(\mathcal{I}\) and \(\mathcal{I}^{c}\), _i.e._\(\mathcal{C}_{\mathcal{I}}=\mathcal{N}(\mathcal{I})\cap\mathcal{N}(\mathcal{I}^{c})\).

Footnote 4: For \(l\in\mathbb{N}_{\geq 0}\), a sequence of vertices \(i_{0},\ldots,i_{l}\in\mathcal{V}\) is a length \(l\) walk if \(\{i_{l^{\prime}-1},i_{l^{\prime}}\}\in\mathcal{E}\) for all \(l^{\prime}\in[l]\).

Note that we focus on undirected graphs for simplicity of presentation. As discussed in Section 4, our results are extended to directed graphs in Appendix D.

### Separation Rank: A Measure of Modeled Interaction

A prominent measure quantifying the interaction a multivariate function models between a subset of input variables and its complement (_i.e._ all other variables) is known as _separation rank_. The separation rank was introduced in [12], and has since been employed for various applications [51; 47; 13]. It is also a common measure of _entanglement_, a profound concept in quantum physics quantifying interaction between particles [62]. In the context of deep learning, it enabled analyses of expressivity and generalization in certain convolutional, recurrent, and self-attention neural networks, resulting in theoretical insights and practical methods (guidelines for neural architecture design, pretraining schemes, and regularizers -- see [30; 33; 61; 62; 64; 100; 65; 85]).

Given a multivariate function \(f:(\mathbb{R}^{D_{x}})^{N}\to\mathbb{R}\), its separation rank with respect to a subset of input variables \(\mathcal{I}\subseteq[N]\) is the minimal number of summands required to express it, where each summand is a product of two functions -- one that operates over variables indexed by \(\mathcal{I}\), and another that operates over the remaining variables. Formally:

**Definition 1**.: The _separation rank_ of \(f:(\mathbb{R}^{D_{x}})^{N}\to\mathbb{R}\) with respect to \(\mathcal{I}\subseteq[N]\) is:

\[\begin{split}\operatorname{sep}(f;\mathcal{I}):=\min\Big{\{}R \in\mathbb{N}_{\geq 0}:&\exists\;g^{(1)},\ldots,g^{(R)}:(\mathbb{R}^{D_{x}})^{| \mathcal{I}|}\to\mathbb{R},\;\bar{g}^{(1)},\ldots,\bar{g}^{(R)}:(\mathbb{R}^{D _{x}})^{|\mathcal{I}^{c}|}\to\mathbb{R}\\ &\text{s.t. }f(\mathbf{X})=\sum\nolimits_{r=1}^{R}g^{(r)}(\mathbf{X}_{ \mathcal{I}})\cdot\bar{g}^{(r)}(\mathbf{X}_{\mathcal{I}^{c}})\Big{\}}\,,\end{split}\] (1)

where \(\mathbf{X}:=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\), \(\mathbf{X}_{\mathcal{I}}:=(\mathbf{x}^{(i)})_{i_{\in\mathcal{I}}}\), and \(\mathbf{X}_{\mathcal{I}^{c}}:=(\mathbf{x}^{(j)})_{j\in\mathcal{I}^{c}}\). By convention, if \(f\) is identically zero then \(\operatorname{sep}(f;\mathcal{I})=0\), and if the set on the right hand side of Equation (1) is empty then \(\operatorname{sep}(f;\mathcal{I})=\infty\).

InterpretationIf \(\mathrm{sep}(f;\mathcal{I})=1\), the function is separable, meaning it does not model any interaction between \(\mathbf{X}_{\mathcal{I}}\) and \(\mathbf{X}_{\mathcal{I}^{c}}\), _i.e._ between the sides of the partition \((\mathcal{I},\mathcal{I}^{c})\). Specifically, it can be represented as \(f(\mathbf{X})=g(\mathbf{X}_{\mathcal{I}})\cdot\bar{g}(\mathbf{X}_{\mathcal{I }^{c}})\) for some functions \(g\) and \(\bar{g}\). In a statistical setting, where \(f\) is a probability density function, this would mean that \(\mathbf{X}_{\mathcal{I}}\) and \(\mathbf{X}_{\mathcal{I}^{c}}\) are statistically independent. The higher \(\mathrm{sep}(f;\mathcal{I})\) is, the farther \(f\) is from separability, implying stronger modeling of interaction between \(\mathbf{X}_{\mathcal{I}}\) and \(\mathbf{X}_{\mathcal{I}^{c}}\).

## 3 Graph Neural Networks

Modern GNNs predominantly follow the message-passing paradigm [45; 50], whereby each vertex is associated with a hidden embedding that is updated according to its neighbors. The initial embedding of \(i\in\mathcal{V}\) is taken to be its input features: \(\mathbf{h}^{(0,i)}:=\mathbf{x}^{(i)}\in\mathbb{R}^{D_{x}}\). Then, in a depth \(L\) message-passing GNN, a common update scheme for the hidden embedding of \(i\in\mathcal{V}\) at layer \(l\in[L]\) is:

\[\mathbf{h}^{(l,i)}=\textsc{aggregate}\Big{(}\!\Big{\{}\!\big{\{}\!\big{\}} \mathbf{W}^{(l)}\mathbf{h}^{(l-1,j)}:j\in\mathcal{N}(i)\!\big{\}}\!\Big{)}\,,\] (2)

where \(\{\!\{.\}\!\}\) denotes a multiset, \(\mathbf{W}^{(1)}\in\mathbb{R}^{D_{h}\times D_{x}},\mathbf{W}^{(2)}\in\mathbb{R }^{D_{h}\times D_{h}},\ldots,\mathbf{W}^{(L)}\in\mathbb{R}^{D_{h}\times D_{h}}\) are learnable weight matrices, with \(D_{h}\in\mathbb{N}\) being the network's width (_i.e._ hidden dimension), and aggregate is a function combining multiple input vectors into a single vector. A notable special case is GCN [59], in which aggregate performs a weighted average followed by a non-linear activation function (_e.g._ ReLU).5 Other aggregation operators are also viable, _e.g._ element-wise sum, max, or product (_cf._[49; 53]). We note that distinguishing self-loops from other edges, and more generally, treating multiple edge types, is possible through the use of different weight matrices for different edge types [49; 88]. For conciseness, we hereinafter focus on the case of a single edge type, and treat multiple edge types in Appendix D.

Footnote 5: In GCN, aggregate also has access to the degrees of vertices, which are used for computing the averaging weights. We omit the dependence on vertex degrees in our notation for conciseness.

After \(L\) layers, the GNN generates hidden embeddings \(\mathbf{h}^{(L,1)},\ldots,\mathbf{h}^{(L,|\mathcal{V}|)}\in\mathbb{R}^{D_{h}}\). For graph prediction, where a single output is produced for the whole graph, the hidden embeddings are usually combined into a single vector through the aggregate function. A final linear layer with weights \(\mathbf{W}^{(o)}\in\mathbb{R}^{1\times D_{h}}\) is then applied to the resulting vector.6 Overall, the function realized by a depth \(L\) graph prediction GNN receives an input graph \(\mathcal{G}\) with vertex features \(\mathbf{X}:=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R }^{D_{x}\times|\mathcal{V}|}\), and returns:

Footnote 6: We treat the case of output dimension one merely for the sake of presentation. Extension of our theory (delivered in Section 4) to arbitrary output dimension is straightforward — the results hold as stated for each of the functions computing an output entry.

\[\text{(graph prediction)}\quad f^{(\theta,\mathcal{G})}(\mathbf{X}):= \mathbf{W}^{(o)}\textsc{aggregate}\big{(}\!\big{\{}\!\big{\{}\!\big{\}} \mathbf{h}^{(L,i)}:i\in\mathcal{V}\!\big{\}}\!\big{)}\,,\] (3)

with \(\theta:=(\mathbf{W}^{(1)},\ldots,\mathbf{W}^{(L)},\mathbf{W}^{(o)})\) denoting the network's learnable weights. For vertex prediction tasks, where the network produces an output for every \(t\in\mathcal{V}\), the final linear layer is applied to each \(\mathbf{h}^{(L,t)}\) separately. That is, for a target vertex \(t\in\mathcal{V}\), the function realized by a depth \(L\) vertex prediction GNN is given by:

\[\text{(vertex prediction)}\quad f^{(\theta,\mathcal{G},t)}(\mathbf{X}):= \mathbf{W}^{(o)}\mathbf{h}^{(L,t)}\,.\] (4)

Our aim is to investigate the ability of GNNs to model interactions between vertices. Prior studies of interactions modeled by different deep learning architectures have focused on neural networks with polynomial non-linearity, building on their representation as tensor networks [32; 30; 34; 90; 61; 62; 7; 56; 63; 64; 83; 100; 84; 85; 65]. Although neural networks with polynomial non-linearity are less common in practice, they have demonstrated competitive performance [28; 31; 91; 94; 27; 37; 53], and hold promise due to their compatibility with quantum computation [46; 14] and fully homomorphic encryption [44]. More importantly, their analyses brought forth numerous insights that were demonstrated empirically and led to development of practical tools for widespread deep learning models (with non-linearities such as ReLU).

Following the above, in our theoretical analysis (Section 4) we consider GNNs with (element-wise) product aggregation, which are polynomial functions of their inputs. Namely, the aggregate operator from Equations (2) and (3) is taken to be:

\[\textsc{aggregate}(\mathcal{X}):=\odot_{\mathbf{x}\in\mathcal{X}}\mathbf{x}\,,\] (5)where \(\odot\) stands for the Hadamard product and \(\mathcal{X}\) is a multiset of vectors. The resulting architecture can be viewed as a variant of the GNN proposed in [53], where it was shown to achieve competitive performance in practice. Central to our proofs are tensor network representations of GNNs with product aggregation (formally established in Appendix E), analogous to those used for analyzing other types of neural networks. We empirically demonstrate our theoretical findings on popular GNNs (Section 4.2), such as GCN and GIN with ReLU non-linearity, and use them to derive a practical edge sparsification algorithm (Section 5).

We note that some of the aforementioned analyses of neural networks with polynomial non-linearity were extended to account for additional non-linearities, including ReLU, through constructs known as _generalized tensor networks_[29]. We thus believe our theory may be similarly extended, and regard this as an interesting direction for future work.

Theoretical Analysis: The Effect of Input Graph Structure and Neural Network Architecture on Modeled Interactions

In this section, we employ separation rank (Definition 1) to theoretically quantify how the input graph structure and network architecture (width and depth) affect the ability of a GNN with product aggregation to model interactions between input vertices. We overview the main results and their implications in Section 4.1, while deferring the formal analysis to Appendix A due to lack of space. Section 4.2 provides experiments demonstrating our theory's implications on common GNNs, such as GCN and GIN with ReLU non-linearity.

### Overview and Implications

Consider a depth \(L\) GNN with width \(D_{h}\) and product aggregation (Section 3). Given a graph \(\mathcal{G}\), any assignment to the weights of the network \(\theta\) induces a multivariate function -- \(f^{(\theta,\mathcal{G})}\) for graph prediction (Equation (3)) and \(f^{(\theta,\mathcal{G},t)}\) for prediction over a given vertex \(t\in\mathcal{V}\) (Equation (4)) -- whose variables correspond to feature vectors of input vertices. The separation rank of this function with respect to \(\mathcal{I}\subseteq\mathcal{V}\) thus measures the interaction modeled across the partition \((\mathcal{I},\mathcal{I}^{c})\), _i.e._ between the vertices in \(\mathcal{I}\) and those in \(\mathcal{I}^{c}\). The higher the separation rank is, the stronger the modeled interaction.

Key to our analysis are the following notions of _walk index_, defined by the number of walks emanating from the boundary of the partition \((\mathcal{I},\mathcal{I}^{c})\), _i.e._ from vertices with an edge crossing the partition induced by \(\mathcal{I}\) (see Figure 1 for an illustration).

**Definition 2**.: Let \(\mathcal{I}\subseteq\mathcal{V}\). Denote by \(\mathcal{C}_{\mathcal{I}}\) the set of vertices with an edge crossing the partition \((\mathcal{I},\mathcal{I}^{c})\), _i.e._\(\mathcal{C}_{\mathcal{I}}:=\{i\in\mathcal{I}:\mathcal{N}(i)\cap\mathcal{I}^{c} \neq\emptyset\}\cup\{j\in\mathcal{I}^{c}:\mathcal{N}(j)\cap\mathcal{I}\neq \emptyset\}\), and recall that \(\rho_{l}(\mathcal{C}_{\mathcal{I}},\mathcal{J})\) denotes the number of length \(l\in\mathbb{N}_{\geq 0}\) walks from any vertex in \(\mathcal{C}_{\mathcal{I}}\) to any vertex in \(\mathcal{J}\subseteq\mathcal{V}\). For \(L\in\mathbb{N}\):

* (graph prediction) we define the \((L-1)\)-_walk index_ of \(\mathcal{I}\), denoted \(\mathrm{WI}_{L-1}(\mathcal{I})\), to be the number of length \(L-1\) walks originating from \(\mathcal{C}_{\mathcal{I}}\), _i.e._\(\mathrm{WI}_{L-1}(\mathcal{I}):=\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{ V})\); and
* (vertex prediction) for \(t\in\mathcal{V}\) we define the \((L-1,t)\)-_walk index_ of \(\mathcal{I}\), denoted \(\mathrm{WI}_{L-1,t}(\mathcal{I})\), to be the number of length \(L-1\) walks from \(\mathcal{C}_{\mathcal{I}}\) that end at \(t\), _i.e._\(\mathrm{WI}_{L-1,t}(\mathcal{I}):=\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})\).

As our main theoretical contribution, we prove:

**Theorem 1** (informally stated).: _For all weight assignments \(\theta\) and \(t\in\mathcal{V}\):_

\[\text{(graph prediction)}\quad\log\bigl{(}\mathrm{sep}\bigl{(}f^{( \theta,\mathcal{G})};\mathcal{I}\bigr{)}\bigr{)}=\mathcal{O}\bigl{(}\log(D_{h}) \cdot\mathrm{WI}_{L-1}(\mathcal{I})\bigr{)}\,,\] \[\text{(vertex prediction)}\quad\log\bigl{(}\mathrm{sep}\bigl{(}f^{( \theta,\mathcal{G},t)};\mathcal{I}\bigr{)}\bigr{)}=\mathcal{O}\bigl{(}\log(D_{h}) \cdot\mathrm{WI}_{L-1,t}(\mathcal{I})\bigr{)}\,.\]

_Moreover, nearly matching lower bounds hold for almost all weight assignments.7_

Footnote 7: Almost all in the sense of all weight assignments but a set of Lebesgue measure zero.

The upper and lower bounds are formally established by Theorems 2 and 3 in Appendix A, respectively, and are generalized to input graphs with directed edges and multiple edge types in Appendix D. Theorem 1 implies that, the \((L-1)\)-walk index of \(\mathcal{I}\) in graph prediction and its \((L-1,t)\)-walk index in vertex prediction control the separation rank with respect to \(\mathcal{I}\), and are thus paramount for modeling interaction between \(\mathcal{I}\) and \(\mathcal{I}^{c}\) -- see Figure 2 for an illustration. It thereby formalizes the conventional wisdom by which GNNs can model stronger interaction between areas of the input graph that are more interconnected. We support this finding empirically with common GNN architectures (_e.g._ GCN and GIN with ReLU non-linearity) in Section 4.2.

One may interpret Theorem 1 as encouraging addition of edges to an input graph. Indeed, the theorem states that such addition can enhance the GNN's ability to model interactions between input vertices. This accords with existing evidence by which increasing connectivity can improve the performance of GNNs in practice (see, _e.g._, [40; 1]). However, special care needs to be taken when adding edges: it may distort the semantic meaning of the input graph, and may lead to flights known as over-smoothing and over-squashing [68; 78; 22; 1; 8]. Rather than employing Theorem 1 for adding edges, we use it to select which edges to preserve in a setting where some must be removed. That is, we employ it for designing an edge sparsification algorithm. The algorithm, named _Walk Index Sparsification_ (_WIS_), is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy. We present and evaluate it in Section 5.

### Empirical Demonstration

Our theoretical analysis establishes that, the strength of interaction GNNs can model across a partition of input vertices is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition (see Definition 2). The analysis formally applies to GNNs with product aggregation (see Section 3), yet we empirically demonstrate that its conclusions carry over to various other message-passing GNN architectures, namely GCN [59], GAT [96], and GIN [103] (with ReLU non-linearity). Specifically, through controlled experiments, we show that such models perform better on tasks in which the partitions that require strong interaction are ones with higher walk index, given that all other aspects of the tasks are the same. A description of these experiments follows. For brevity, we defer some implementation details to Appendix H.2.

We constructed two graph prediction datasets, in which the vertex features of each input graph are patches of pixels from two randomly sampled Fashion-MNIST [102] images, and the goal is to predict whether the two images are of the same class.8 In both datasets, all input graphs have the same structure: two separate cliques with \(16\) vertices each, connected by a single edge. The datasets differ in how the image patches are distributed among the vertices: in the first dataset each clique holds all the patches of a single image, whereas in the second dataset each clique holds half of the patches from the first image and half of the patches from the second image. Figure 2 illustrates how

Figure 2: Depth \(L\) GNNs can model stronger interactions between sides of partitions that have a higher walk index (Definition 2). The partition \((\mathcal{I}_{1},\mathcal{I}_{1}^{c})\) (left) divides the vertices into two separate cliques, connected by a single edge. Only two vertices reside in \(\mathcal{C}_{\mathcal{I}_{1}}\) — the set of vertices with an edge crossing the partition. Taking for example depth \(L=3\), the \(2\)-walk index of \(\mathcal{I}_{1}\) is \(\Theta(|\mathcal{V}|^{2})\) and its \((2,t)\)-walk index is \(\Theta(|\mathcal{V}|)\), for \(t\in\mathcal{V}\). In contrast, the partition \((\mathcal{I}_{2},\mathcal{I}_{2}^{c})\) (right) equally divides the vertices in each clique to different sides. All vertices reside in \(\mathcal{C}_{\mathcal{I}_{2}}\), meaning the \(2\)-walk index of \(\mathcal{I}_{2}\) is \(\Theta(|\mathcal{V}|^{3})\) and its \((2,t)\)-walk index is \(\Theta(|\mathcal{V}|^{2})\), for \(t\in\mathcal{V}\). Hence, in both graph and vertex prediction scenarios, the walk index of \(\mathcal{I}_{1}\) is relatively low compared to that of \(\mathcal{I}_{2}\). Our analysis (Section 4.1 and Appendix A) states that a higher separation rank can be attained with respect to \(\mathcal{I}_{2}\), meaning stronger interaction can be modeled across \((\mathcal{I}_{2},\mathcal{I}_{2}^{c})\) than across \((\mathcal{I}_{1},\mathcal{I}_{1}^{c})\). We empirically confirm this prospect in Section 4.2.

image patches are distributed in the first (left hand side of the figure) and second (right hand side of the figure) datasets, with blue and red marking assignment of vertices to images.

Each dataset requires modeling strong interaction across the partition separating the two images, referred to as the _essential partition_ of the dataset. In the first dataset the essential partition separates the two cliques, thus it has low walk index. In the second dataset each side of the essential partition contains half of the vertices from the first clique and half of the vertices from the second clique, thus the partition has high walk index. For an example illustrating the gap between these walk indices see Figure 2.

Table 1 reports train and test accuracies achieved by GCN, GAT, and GIN (with ReLU non-linearity) over both datasets. In compliance with our theory, the GNNs fit the dataset whose essential partition has high walk index significantly better than they fit the dataset whose essential partition has low walk index. Furthermore, the improved train accuracy translates to improvements in test accuracy.

## 5 Practical Application: Expressivity Preserving Edge Sparsification

Section 4 theoretically characterizes the ability of a GNN to model interactions between input vertices. It reveals that this ability is controlled by a graph-theoretical property we call walk index (Definition 2). The current section derives a practical application of our theory, specifically, an _edge sparsification_ algorithm named _Walk Index Sparsification_ (_WIS_), which preserves the ability of a GNN to model interactions when input edges are removed. We present WIS, and show that it yields high predictive performance for GNNs over standard vertex prediction benchmarks of various scales, even when removing a significant portion of edges. In particular, we evaluate WIS using GCN [59], GIN [103], and ResGCN [66] over multiple datasets, including: Cora [89], which contains thousands of edges, DBLP [16], which contains tens of thousands of edges, and OGBN-ArXiv [52], which contains more than a million edges. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of prediction accuracy across edge sparsity levels. We believe its further empirical investigation is a promising direction for future research.

### Walk Index Sparsification (WIS)

Running GNNs over large-scale graphs can be prohibitively expensive in terms of runtime and memory. A natural way to tackle this problem is edge sparsification -- removing edges from an input graph while attempting to maintain prediction accuracy (_cf._[67; 24]).9\({}^{,}\)10

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & & \multicolumn{2}{c}{Essential Partition Walk Index} \\ \cline{3-4}  & & Low & High \\ \hline \multirow{2}{*}{GCN} & Train Acc. (\%) & \(70.4\pm 1.7\) & \(\mathbf{81.4\pm 2.0}\) \\  & Test Acc. (\%) & \(52.7\pm 1.9\) & \(\mathbf{66.2\pm 1.1}\) \\ \hline \multirow{2}{*}{GAT} & Train Acc. (\%) & \(82.8\pm 2.6\) & \(\mathbf{88.5\pm 1.1}\) \\  & Test Acc. (\%) & \(69.6\pm 0.6\) & \(\mathbf{72.1\pm 1.2}\) \\ \hline \multirow{2}{*}{GIN} & Train Acc. (\%) & \(83.2\pm 0.8\) & \(\mathbf{94.2\pm 0.8}\) \\  & Test Acc. (\%) & \(53.7\pm 1.8\) & \(\mathbf{64.8\pm 1.4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: In accordance with our theory (Section 4.1 and Appendix A), GNNs can better fit datasets in which the partitions (of input vertices) that require strong interaction are ones with higher walk index (Definition 2). Table reports means and standard deviations, taken over five runs, of train and test accuracies obtained by GNNs of depth \(3\) and width \(16\) on two datasets: one in which the essential partition — _i.e._ the main partition requiring strong interaction — has low walk index, and another in which it has high walk index (see Section 4.2 for a detailed description of the datasets). For all GNNs, the train accuracy attained over the second dataset is considerably higher than that attained over the first dataset. Moreover, the better train accuracy translates to better test accuracy. See Appendix H.2 for further implementation details.

``` Input:\(\mathcal{G}\) -- graph, \(N\in\mathbb{N}\) -- number of edges to remove Result: Sparsified graph obtained by removing \(N\) edges from \(\mathcal{G}\) ``` for\(n=1,\ldots,N\)do  # per edge, compute walk indices of partitions induced by \(\{t\}\), for \(t\in\mathcal{V}\), after its removal for\(e\in\mathcal{E}\) (excluding self-loops) do  initialize \(\mathbf{s}^{(e)}=(0,\ldots,0)\in\mathbb{R}^{|\mathcal{V}|}\)  remove \(e\) from \(\mathcal{G}\) (temporarily)  for every \(t\in\mathcal{V}\), set \(\mathbf{s}_{t}^{(e)}=\mathrm{WI}_{L-1,t}(\{t\})\) # = number of length \(L-1\) walks from \(\mathcal{C}_{\{t\}}\) to \(t\)  add \(e\) back to \(\mathcal{G}\) endfor  # prune edge whose removal harms walk indices the least according to an order over \(\left(\mathbf{s}^{(e)}\right)_{e\in\mathcal{E}}\)  for \(e\in\mathcal{E}\), sort the entries of \(\mathbf{s}^{(e)}\) in ascending order  let \(e^{\prime}\in\operatorname*{argmax}_{e\in\mathcal{E}}\mathbf{s}^{(e)}\) according to lexicographic order over tuples remove \(e^{\prime}\) from \(\mathcal{G}\) (permanently) endfor ```

**Algorithm 1**\((L-1)\)-Walk Index Sparsification (WIS)

``` Input:\(\mathcal{G}\) -- graph, \(N\in\mathbb{N}\) -- number of edges to remove Result: Sparsified graph obtained by removing \(N\) edges from \(\mathcal{G}\) ``` for\(n=1,\ldots,N\)do for\(\{i,j\}\in\mathcal{E}\) (excluding self-loops) do  let \(\text{deg}_{min}(i,j):=\min\{|\mathcal{N}(i)|,|\mathcal{N}(j)|\}\)  let \(\text{deg}_{max}(i,j):=\max\{|\mathcal{N}(i)|,|\mathcal{N}(j)|\}\) endfor  # prune edge \(\{i,j\}\in\mathcal{E}\) with maximal \(\text{deg}_{min}(i,j)\), breaking ties using \(\text{deg}_{max}(i,j)\)  let \(e^{\prime}\in\operatorname*{argmax}_{\{i,j\}\in\mathcal{E}}\bigl{(}\text{deg} _{min}(i,j),\text{deg}_{max}(i,j)\bigr{)}\) according to lexicographic order over pairs remove \(e^{\prime}\) from \(\mathcal{G}\) endfor ```

**Algorithm 2**\(1\)-Walk Index Sparsification (WIS) (efficient version of Algorithm 1 for \(L=2\))

Our theory (Section 4) establishes that, the strength of interaction a depth \(L\) GNN can model across a partition of input vertices is determined by the partition's walk index, a quantity defined by the number of length \(L-1\) walks originating from the partition's boundary. This brings forth a recipe for pruning edges. First, choose partitions across which the ability to model interactions is to be preserved. Then, for every input edge (excluding self-loops), compute a tuple holding what the walk indices of the chosen partitions will be if the edge is to be removed. Lastly, remove the edge whose tuple is maximal according to a preselected order over tuples (_e.g._ an order based on the sum, min, or max of a tuple's entries). This process repeats until the desired number of edges are removed. The idea behind the above-described recipe, which we call _General Walk Index Sparsification_, is that each iteration greedily prunes the edge whose removal takes the smallest toll in terms of ability to model interactions across chosen partitions -- see Algorithm 3 in Appendix F for a formal outline. Below we describe a specific instantiation of the recipe for vertex prediction tasks, which are particularly relevant with large-scale graphs, yielding our proposed algorithm -- Walk Index Sparsification (WIS). Exploration of other instantiations is regarded as a promising avenue for future work.

In vertex prediction tasks, the interaction between an input vertex and the remainder of the input graph is of central importance. Thus, it is natural to choose the partitions induced by singletons (_i.e._ the partitions \((\{t\},\mathcal{V}\setminus\{t\})\), where \(t\in\mathcal{V}\)) as those across which the ability to model interactions is to be preserved. We would like to remove edges while avoiding a significant deterioration in the ability to model interaction under any of the chosen partitions. To that end, we compare walk index tuples according to their minimal entries, breaking ties using the second smallest entries, and so forth. This is equivalent to sorting (in ascending order) the entries of each tuple separately, and then ordering the tuples lexicographically.

Algorithm 1 provides a self-contained description of the method attained by the foregoing choices. We refer to this method as \((L-1)\)-Walk Index Sparsification (WIS), where the "\((L-1)\)" indicates that only walks of length \(L-1\) take part in the walk indices. Since \((L-1)\)-walk indices can be computed by taking the \((L-1)\)'th power of the graph's adjacency matrix, \((L-1)\)-WIS runs in \(\mathcal{O}(N|\mathcal{E}||\mathcal{V}|^{3}\log(L))\) time and requires \(\mathcal{O}(|\mathcal{E}||\mathcal{V}|+|\mathcal{V}|^{2})\) memory, where \(N\) is the number of edges to be removed. For large graphs a runtime cubic in the number of vertices can be restrictive. Fortunately, \(1\)-WIS, which can be viewed as an approximation for \((L-1)\)-WIS with \(L>2\), facilitates a particularly simple and efficient implementation based solely on vertex degrees, requiring only linear time and memory -- see Algorithm 2 (whose equivalence to \(1\)-WIS is explained in Appendix G). Specifically, \(1\)-WIS runs in \(\mathcal{O}(N|\mathcal{E}|+|\mathcal{V}|)\) time and requires \(\mathcal{O}(|\mathcal{E}|+|\mathcal{V}|)\) memory.

### Empirical Evaluation

Below is an empirical evaluation of WIS. For brevity, we defer to Appendix H some implementation details, as well as experiments with additional GNN architectures (GIN and ResGCN) and datasets (Chameleon [82], Squirrel [82], and Amazon Computers [92]).

Using depth \(L=3\) GNNs (with ReLU non-linearity), we evaluate over the Cora dataset both \(2\)-WIS, which is compatible with the GNNs' depth, and \(1\)-WIS, which can be viewed as an efficient approximation. Over the DBLP and OGBN-ArXiv datasets, due to their larger scale only \(1\)-WIS is evaluated. Figure 3 (and Figure 8 in Appendix H) shows that WIS significantly outperforms the following alternative methods in terms of induced prediction accuracy: _(i)_ a baseline in which edges are removed uniformly at random; _(ii)_ a well-known spectral algorithm [93] designed to preserve the spectrum of the sparsified graph's Laplacian; and _(iii)_ an adaptation of UGS [24] -- a recent supervised approach for learning to prune edges.11 Both \(2\)-WIS and \(1\)-WIS lead to higher test accuracies, while (as opposed to UGS) avoiding the need for labels, and for training a GNN over the original (non-sparsified) graph -- a procedure which in some settings is prohibitively expensive in terms of runtime and memory. Interestingly, \(1\)-WIS performs similarly to \(2\)-WIS, indicating that the efficiency it brings does not come at a sizable cost in performance.

Footnote 11: UGS [24] jointly prunes input graph edges and GNN weights. For fair comparison, we adapt it to only remove edges.

Figure 3: Comparison of GNN accuracies following sparsification of input edges — WIS, the edge sparsification algorithm brought forth by our theory (Algorithm 1), markedly outperforms alternative methods. Plots present test accuracies achieved by a depth \(L=3\) GCN of width \(64\) over the Cora (left), DBLP (middle), and OGBN-ArXiv (right) vertex prediction datasets, with increasing percentage of removed edges (for each combination of dataset, edge sparsification algorithm, and percentage of removed edges, a separate GCN was trained and evaluated). WIS, designed to maintain the ability of a GNN to model interactions between input vertices, is compared against: _(i)_ removing edges uniformly at random, _(ii)_ a spectral sparsification method [93]; and _(iii)_ an adaptation of UGS [24]. For Cora, we run both \(2\)-WIS, which is compatible with the GNN’s depth, and \(1\)-WIS, which can be viewed as an approximation that admits a particularly efficient implementation (Algorithm 2). For DBLP and OGBN-ArXiv, due to their larger scale only \(1\)-WIS is evaluated. Markers and error bars report means and standard deviations, respectively, taken over ten runs per configuration. Note that \(1\)-WIS achieves results similar to \(2\)-WIS, suggesting that the efficiency it brings does not come at a significant cost in performance. Appendix H provides further implementation details and experiments with additional GNN architectures (GIN and ResGCN) and datasets (Chameleon, Squirrel, and Amazon Computers). Code for reproducing the experiment is available at https://github.com/noamrazin/gnn_interactions.

Conclusion

### Summary

GNNs are designed to model complex interactions between entities represented as vertices of a graph. The current paper provides the first theoretical analysis for their ability to do so. We proved that, given a partition of input vertices, the strength of interaction that can be modeled between its sides is controlled by the _walk index_ -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures, _e.g._ GCN [59] and GIN [103], corroborated this result.

Our theory formalizes conventional wisdom by which GNNs can model stronger interaction between regions of the input graph that are more interconnected. More importantly, we showed that it facilitates a novel edge sparsification algorithm which preserves the ability of a GNN to model interactions when edges are removed. Our algorithm, named _Walk Index Sparsification_ (_WIS_), is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy. More broadly, WIS showcases the potential of improving GNNs by theoretically analyzing the interactions they can model.

### Limitations and Future Work

The theoretical analysis considers GNNs with product aggregation, which are less common in practice (_cf._ Section 3). We empirically demonstrated that its conclusions apply to more popular GNNs (Section 4.2), and derived a practical edge sparsification algorithm based on the theory (Section 5). Nonetheless, extending our analysis to additional aggregation functions is a worthy avenue to explore.

Our work also raises several interesting directions concerning WIS. A naive implementation of \((L-1)\)-WIS has runtime cubic in the number of vertices (_cf._ Section 5.1). Since this can be restrictive for large-scale graphs, the evaluation in Section 5.2 mostly focused on \(1\)-WIS, which can be viewed as an efficient approximation of \((L-1)\)-WIS (its runtime and memory requirements are linear -- see Section 5.1). Future work can develop efficient exact implementations of \((L-1)\)-WIS (_e.g._ using parallelization) and investigate regimes where it outperforms \(1\)-WIS in terms of induced prediction accuracy. Additionally, \((L-1)\)-WIS is a specific instantiation of the general WIS scheme (given in Appendix F), tailored for preserving the ability to model interactions across certain partitions. Exploring other instantiations, as well as methods for automatically choosing the partitions across which the ability to model interactions is preserved, are valuable directions for further research.

## Acknowledgments and Disclosure of Funding

We thank Eshbal Hezroni for aid in preparing illustrative figures. This work was supported by a Google Research Scholar Award, a Google Research Gift, the Yandex Initiative in Machine Learning, the Israel Science Foundation (grant 1780/21), the Tel Aviv University Center for AI and Data Science, the Adelis Research Fund for Artificial Intelligence, Len Blavatnik and the Blavatnik Family Foundation, and Amnon and Anat Shashua. NR is supported by the Apple Scholars in AI/ML PhD fellowship.

## References

* [1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. _International Conference on Learning Representations (ICLR)_, 2021.
* [2] Arash Amini, Amin Karbasi, and Farokh Marvasti. Low-rank matrix approximation using point-wise operators. _IEEE transactions on information theory_, 58(1):302-310, 2011.
* [3] Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks. _International Conference on Learning Representations (ICLR)_, 2021.
* [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.

* [5] Muhammet Balcilar, Renton Guillaume, Pierre Heroux, Benoit Gauzere, Sebastien Adam, and Paul Honeine. Analyzing the expressive power of graph neural networks in a spectral perspective. _International Conference on Learning Representations (ICLR)_, 2021.
* [6] Muhammet Balcilar, Pierre Heroux, Benoit Gauzere, Pascal Vasseur, Sebastien Adam, and Paul Honeine. Breaking the limits of message passing graph neural networks. In _International Conference on Machine Learning (ICML)_, 2021.
* [7] Emilio Rafael Balda, Arash Behboodi, and Rudolf Mathar. A tensor analysis on dense connectivity via convolutional arithmetic circuits. _Preprint_, 2018.
* [8] Pradeep Kr Banerjee, Kedar Karhadkar, Yu Guang Wang, Uri Alon, and Guido Montifar. Oversquashing in gnns through the lens of information contraction and graph expansion. In _2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1-8. IEEE, 2022.
* [9] Pablo Barcelo, Egor Kostylev, Mikael Monet, Jorge Perez, Juan Reutter, and Juan-Pablo Silva. The logical expressiveness of graph neural networks. _International Conference on Learning Representations (ICLR)_, 2020.
* [10] Pablo Barcelo, Floris Geerts, Juan Reutter, and Maksimilian Ryschkov. Graph neural networks with local graph parameters. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [11] Surender Baswana and Sandeep Sen. A simple and linear time randomized algorithm for computing sparse spanners in weighted graphs. _Random Structures & Algorithms_, 30(4):532-563, 2007.
* [12] Gregory Beylkin and Martin J Mohlenkamp. Numerical operator calculus in higher dimensions. _Proceedings of the National Academy of Sciences_, 99(16):10246-10251, 2002.
* [13] Gregory Beylkin, Jochen Garcke, and Martin J Mohlenkamp. Multivariate regression and machine learning with sums of separable functions. _SIAM Journal on Scientific Computing_, 31(3):1840-1857, 2009.
* [14] Amandeep Singh Bhatia, Mandeep Kaur Saggi, Ajay Kumar, and Sushma Jain. Matrix product state-based quantum classifier. _Neural computation_, 31(7):1499-1517, 2019.
* [15] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and Michael Bronstein. Weisfeiler and lehman go cellular: Cw networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [16] Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. _International Conference on Learning Representations (ICLR)_, 2018.
* [17] Giorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou, and Michael Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [18] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* [19] Richard Caron and Tim Traynor. The zero set of a polynomial. _WSMR Report_, pages 05-02, 2005.
* [20] Alireza Chakeri, Hamidreza Farhidzadeh, and Lawrence O Hall. Spectral sparsification in spectral clustering. In _2016 23rd international conference on pattern recognition (icpr)_, pages 2301-2306. IEEE, 2016.
* [21] Michail Chatzianastasis, Johannes F Lutzeyer, George Dasoulas, and Michalis Vazirgiannis. Graph ordering attention networks. _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2023.
* [22] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-smoothing problem for graph neural networks from the topological view. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2020.
* [23] Lei Chen, Zhengdao Chen, and Joan Bruna. On graph neural networks versus graph-augmented mlps. _International Conference on Learning Representations (ICLR)_, 2021.
* [24] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery ticket hypothesis for graph neural networks. In _International Conference on Machine Learning (ICML)_, 2021.

* [25] Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph isomorphism testing and function approximation with gnns. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [26] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [27] Grigorios G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, and Stefanos Zafeiriou. P-nets: Deep polynomial neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7325-7335, 2020.
* [28] Nadav Cohen and Amnon Shashua. Simnets: A generalization of convolutional networks. _Advances in Neural Information Processing Systems (NeurIPS), Deep Learning Workshop_, 2014.
* [29] Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. _International Conference on Machine Learning (ICML)_, 2016.
* [30] Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling geometry. _International Conference on Learning Representations (ICLR)_, 2017.
* [31] Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [32] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. _Conference On Learning Theory (COLT)_, 2016.
* [33] Nadav Cohen, Or Sharir, Yoav Levine, Ronen Tamari, David Yakira, and Amnon Shashua. Analysis and design of convolutional networks via hierarchical tensor decompositions. _Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) Special Issue on Deep Learning Theory_, 2017.
* [34] Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting dilated convolutional networks with mixed tensor decompositions. _International Conference on Learning Representations (ICLR)_, 2018.
* [35] Nima Dehramy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the representation power of graph neural networks in learning graph topology. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [36] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. _Advances in neural information processing systems_, 28, 2015.
* [37] Timo Felser, Marco Trenti, Lorenzo Sestini, Alessio Gianelle, Davide Zuliani, Donatella Lucchesi, and Simone Montangero. Quantum-inspired machine learning on high-energy physics data. _npj Quantum Information_, 7(1):1-8, 2021.
* [38] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [39] Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of graph neural networks. In _International Conference on Machine Learning (ICML)_, 2020.
* [40] Johannes Gasteiger, Stefan Weissenberger, and Stephan Gunnemann. Diffusion improves graph learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [41] Floris Geerts. The expressive power of kth-order invariant graph networks. _arXiv preprint arXiv:2007.12035_, 2020.
* [42] Floris Geerts and Juan L Reutter. Expressiveness and approximation properties of graph neural networks. _International Conference on Learning Representations (ICLR)_, 2022.
* [43] Floris Geerts, Filip Mazowiecki, and Guillermo Perez. Let's agree to degree: Comparing graph convolutional networks in the message-passing framework. In _International Conference on Machine Learning (ICML)_, 2021.
* [44] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In _International Conference on Machine Learning (ICML)_, 2016.
* [45] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning (ICML)_, 2017.

* [46] Edward Grant, Marcello Benedetti, Shuxiang Cao, Andrew Hallam, Joshua Lockhart, Vid Stojevic, Andrew G Green, and Simone Severini. Hierarchical quantum classifiers. _npj Quantum Information_, 4(1):1-8, 2018.
* [47] Wolfgang Hackbusch. On the efficient evaluation of coalescence integrals in population balance models. _Computing_, 78(2):145-159, 2006.
* [48] Michael Hamann, Gerd Lindner, Henning Meyerhenke, Christian L Staudt, and Dorothea Wagner. Structure-preserving sparsification methods for social networks. _Social Network Analysis and Mining_, 6(1):1-22, 2016.
* [49] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [50] William L Hamilton. Graph representation learning. _Synthesis Lectures on Artifical Intelligence and Machine Learning_, 14(3):1-159, 2020.
* [51] Robert J Harrison, George I Fann, Takeshi Yanai, and Gregory Beylkin. Multiresolution quantum chemistry in multiwavelet bases. In _International Conference on Computational Science_, pages 103-110. Springer, 2003.
* [52] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [53] Chenqing Hua, Guillaume Rabusseau, and Jian Tang. High-order pooling for graph neural networks with tensor decomposition. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [54] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International Conference on Machine Learning (ICML)_, 2015.
* [55] Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [56] Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks. _International Conference on Learning Representations (ICLR)_, 2018.
* [57] Valentin Khrulkov, Oleksii Hrinchuk, and Ivan Oseledets. Generalized tensor models for recurrent neural networks. _International Conference on Learning Representations (ICLR)_, 2019.
* [58] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [59] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _International Conference on Learning Representations (ICLR)_, 2017.
* [60] Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In _Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 631-636, 2006.
* [61] Yoav Levine, Or Sharir, and Amnon Shashua. Benefits of depth for long-term memory of recurrent networks. _International Conference on Learning Representations (ICLR) Workshop_, 2018.
* [62] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum entanglement: Fundamental connections with implications to network design. _International Conference on Learning Representations (ICLR)_, 2018.
* [63] Yoav Levine, Or Sharir, Nadav Cohen, and Amnon Shashua. Quantum entanglement in deep learning architectures. _Physical review letters_, 122(6):065301, 2019.
* [64] Yoav Levine, Noam Wies, Or Sharir, Hoft Bata, and Amnon Shashua. Limits to depth efficiencies of self-attention. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [65] Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. The inductive bias of in-context learning: Rethinking pretraining example design. _International Conference on Learning Representations (ICLR)_, 2022.
* [66] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. _arXiv preprint arXiv:2006.07739_, 2020.

* [67] Jiayu Li, Tianyun Zhang, Hao Tian, Shengmin Jin, Makan Fardad, and Reza Zafarani. Sgcn: A graph sparsifier based on graph convolutional networks. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_, pages 275-287. Springer, 2020.
* [68] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _Thirty-Second AAAI conference on artificial intelligence (AAAI)_, 2018.
* [69] Andreas Loukas. What graph neural networks cannot learn: depth vs width. _International Conference on Learning Representations (ICLR)_, 2020.
* [70] Andreas Loukas. How hard is to distinguish graphs with graph neural networks? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [71] Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang. Learning to drop: Robust graph neural network via topological denoising. In _Proceedings of the 14th ACM international conference on web search and data mining_, pages 779-787, 2021.
* [72] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [73] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In _International Conference on Machine Learning (ICML)_, 2019.
* [74] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4602-4609, 2019.
* [75] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Krieg, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. _arXiv preprint arXiv:2112.09992_, 2021.
* [76] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying for collective classification. In _10th international workshop on mining and learning with graphs_, volume 8, page 1, 2012.
* [77] Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters. _arXiv preprint arXiv:1905.09550_, 2019.
* [78] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. _International Conference on Learning Representations (ICLR)_, 2020.
* [79] Roman Orus. A practical introduction to tensor networks: Matrix product states and projected entangled pair states. _Annals of physics_, 349:117-158, 2014.
* [80] Pal Andras Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In _International Conference on Machine Learning (ICML)_, 2022.
* [81] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In _NIPS-W_, 2017.
* [82] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. _International Conference on Learning Representations (ICLR)_, 2020.
* [83] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [84] Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in tensor factorization. _International Conference on Machine Learning (ICML)_, 2021.
* [85] Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks. _International Conference on Machine Learning (ICML)_, 2022.
* [86] Veeru Sadhanala, Yu-Xiang Wang, and Ryan Tibshirani. Graph sparsification approaches for laplacian smoothing. In _Artificial Intelligence and Statistics_, pages 1250-1259. PMLR, 2016.
* [87] Venu Satuluri, Srinivasan Parthasarathy, and Yive Ruan. Local graph sparsification for scalable clustering. In _Proceedings of the 2011 ACM SIGMOD International Conference on Management of data_, pages 721-732, 2011.

* [88] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In _European semantic web conference_, pages 593-607. Springer, 2018.
* [89] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [90] Or Sharir and Amnon Shashua. On the expressive power of overlapping architectures of deep learning. _International Conference on Learning Representations (ICLR)_, 2018.
* [91] Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tensorial mixture models. _arXiv preprint_, 2016.
* [92] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of graph neural network evaluation. _arXiv preprint arXiv:1811.05868_, 2018.
* [93] Daniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. _SIAM Journal on Computing_, 40(6):1913-1926, 2011.
* [94] E Miles Stoudenmire. Learning relevant features of data with multi-scale tensor networks. _Quantum Science and Technology_, 3(3):034003, 2018.
* [95] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. _International Conference on Learning Representations (ICLR)_, 2022.
* [96] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _International Conference on Learning Representations (ICLR)_, 2018.
* [97] Guifre Vidal. Class of quantum many-body states that can be efficiently simulated. _Physical review letters_, 101(11):110501, 2008.
* [98] Elli Voudigari, Nikos Salamanos, Theodore Papageorgiou, and Emmanuel J Yannakoudakis. Rank degree: An efficient algorithm for graph sampling. In _2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)_, pages 120-129. IEEE, 2016.
* [99] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _NTI, Series_, 2(9):12-16, 1968.
* [100] Noam Wies, Yoav Levine, Daniel Jannai, and Amnon Shashua. Which transformer architecture fits my data? a vocabulary bottleneck in self-attention. _International Conference on Machine Learning (ICML)_, 2021.
* [101] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_, 32(1):4-24, 2020.
* [102] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [103] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _International Conference on Learning Representations (ICLR)_, 2019.
* [104] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 974-983, 2018.
* [105] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. _International Conference on Learning Representations (ICLR)_, 2023.
* [106] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In _International Conference on Machine Learning (ICML)_, 2020.

Formal Analysis: Quantifying the Ability of Graph Neural Networks to Model Interactions

We begin by upper bounding the separation ranks a GNN can achieve.

**Theorem 2**.: _For an undirected graph \(\mathcal{G}\) and \(t\in\mathcal{V}\), let \(f^{(\theta,\mathcal{G})}\) and \(f^{(\theta,\mathcal{G},t)}\) be the functions realized by depth \(L\) graph and vertex prediction GNNs, respectively, with width \(D_{h}\), learnable weights \(\theta\), and product aggregation (Equations (2) to (5)). Then, for any \(\mathcal{I}\subseteq\mathcal{V}\) and assignment of weights \(\theta\) it holds that:_

\[\text{(graph prediction)}\quad\log\bigl{(}\mathrm{sep}\bigl{(}f^{( \theta,\mathcal{G})};\mathcal{I}\bigr{)}\bigr{)}\leq\log(D_{h})\cdot\bigl{(}4 \underbrace{\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})}_{\mathrm{WI}_{L -1}(\mathcal{I})}+1\bigr{)}\,,\] (6) \[\text{(vertex prediction)}\quad\log\bigl{(}\mathrm{sep}\bigl{(}f^{( \theta,\mathcal{G},t)};\mathcal{I}\bigr{)}\bigr{)}\leq\log(D_{h})\cdot 4 \underbrace{\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})}_{\mathrm{WI}_{L-1,t}( \mathcal{I})}\,.\] (7)

Proof sketch (proof in Appendix 1.2).: In Appendix E, we show that the computations performed by a GNN with product aggregation can be represented as a _tensor network_. In brief, a tensor network is a weighted graph that describes a sequence of arithmetic operations known as tensor contractions (see Appendices E.1 and E.2 for a self-contained introduction to tensor networks). The tensor network corresponding to a GNN with product aggregation adheres to a tree structure -- its leaves are associated with input vertex features and interior nodes embody the operations performed by the GNN. Importing machinery from tensor analysis literature, we prove that \(\mathrm{sep}(f^{(\theta,\mathcal{G})};\mathcal{I})\) is upper bounded by a minimal cut weight in the corresponding tensor network, among cuts separating leaves associated with input vertices in \(\mathcal{I}\) from leaves associated with input vertices in \(\mathcal{I}^{c}\). Equation (6) then follows by finding such a cut in the tensor network with sufficiently low weight. Equation (7) is established analogously. 

A natural question is whether the upper bounds in Theorem 2 are tight, _i.e._ whether separation ranks close to them can be attained. We show that nearly matching lower bounds hold for almost all assignments of weights \(\theta\). To this end, we define _admissible subsets of \(\mathcal{C}_{\mathcal{I}}\)_, based on a notion of vertex subsets with _no repeating shared neighbors_.

**Definition 3**.: We say that \(\mathcal{I},\mathcal{J}\subseteq\mathcal{V}\) have no repeating shared neighbors_ if every \(k\in\mathcal{N}(\mathcal{I})\cap\mathcal{N}(\mathcal{J})\) has only a single neighbor in each of \(\mathcal{I}\) and \(\mathcal{J}\), _i.e._\(|\mathcal{N}(k)\cap\mathcal{I}|=|\mathcal{N}(k)\cap\mathcal{J}|=1\).

**Definition 4**.: For \(\mathcal{I}\subseteq\mathcal{V}\), we refer to \(\mathcal{C}\subseteq\mathcal{C}_{\mathcal{I}}\) as an _admissible subset of \(\mathcal{C}_{\mathcal{I}}\)_ if there exist \(\mathcal{I}^{\prime}\subseteq\mathcal{I},\mathcal{J}^{\prime}\subseteq \mathcal{I}^{c}\) with no repeating shared neighbors such that \(\mathcal{C}=\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{ \prime})\). We use \(\mathcal{S}(\mathcal{I})\) to denote the set comprising all admissible subsets of \(\mathcal{C}_{\mathcal{I}}\):

\[\mathcal{S}(\mathcal{I}):=\left\{\mathcal{C}\subseteq\mathcal{C}_{\mathcal{I} }:\mathcal{C}\text{ is an admissible subset of }\mathcal{C}_{\mathcal{I}}\right\}.\]

Theorem 3 below establishes that almost all possible values for the network's weights lead the upper bounds in Theorem 2 to be tight, up to logarithmic terms and to the number of walks from \(\mathcal{C}_{\mathcal{I}}\) being replaced with the number of walks from any single \(\mathcal{C}\in\mathcal{S}(\mathcal{I})\). The extent to which \(\mathcal{C}_{\mathcal{I}}\) can be covered by an admissible subset thus determines how tight the upper bounds are. Trivially, at least the shared neighbors of any \(i\in\mathcal{I},j\in\mathcal{I}^{c}\) can be covered, since \(\mathcal{N}(i)\cap\mathcal{N}(j)\in\mathcal{S}(\mathcal{I})\). Appendix C shows that for various canonical graphs all of \(\mathcal{C}_{\mathcal{I}}\), or a large part of it, can be covered by an admissible subset.

**Theorem 3**.: _Consider the setting and notation of Theorem 2. Given \(\mathcal{I}\subseteq\mathcal{V}\), for almost all assignments of weights \(\theta\), i.e. for all but a set of Lebesgue measure zero, it holds that:_

\[\text{(graph prediction)}\quad\log\bigl{(}\mathrm{sep}\bigl{(}f^{( \theta,\mathcal{G})};\mathcal{I}\bigr{)}\bigr{)}\geq\max_{\mathcal{C}\in \mathcal{S}(\mathcal{I})}\log(\alpha_{\mathcal{C}})\cdot\rho_{L-1}(\mathcal{C}, \mathcal{V})\,,\] (8) \[\text{(vertex prediction)}\quad\log\bigl{(}\mathrm{sep}\bigl{(}f^{( \theta,\mathcal{G},t)};\mathcal{I}\bigr{)}\bigr{)}\geq\max_{\mathcal{C}\in \mathcal{S}(\mathcal{I})}\log(\alpha_{\mathcal{C},t})\cdot\rho_{L-1}(\mathcal{C}, \{t\})\,,\] (9)

_where:_

\[\alpha_{\mathcal{C}} :=\begin{cases}D^{1/\rho_{0}(\mathcal{C},\mathcal{V})}&,\text{if }L=1\\ \left(D-1\right)\cdot\rho_{L-1}(\mathcal{C},\mathcal{V})^{-1}+1&,\text{if }L\geq 2 \end{cases}\,,\] \[\alpha_{\mathcal{C},t} :=\begin{cases}D&,\text{if }L=1\\ \left(D-1\right)\cdot\rho_{L-1}(\mathcal{C},\{t\})^{-1}+1&,\text{if }L\geq 2\end{cases}\,,\]_with \(D:=\min\{D_{x},D_{h}\}\). If \(\rho_{L-1}(\mathcal{C},\mathcal{V})=0\) or \(\rho_{L-1}(\mathcal{C},\{t\})=0\), the respective lower bound (right hand side of Equation (8) or Equation (9)) is zero by convention._

Proof sketch (proof in Appendix 1.3).: Our proof follows a line similar to that used in [64, 100, 65] for lower bounding the separation rank of self-attention neural networks. The separation rank of any \(f:(\mathbb{R}^{D_{x}})^{|\mathcal{V}|}\to\mathbb{R}\) can be lower bounded by examining its outputs over a grid of inputs. Specifically, for \(M\in\mathbb{N}\)_template vectors_\(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\in\mathbb{R}^{D_{x}}\), we can create a _grid tensor_ for \(f\) by evaluating it over each point in \(\{(\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{|V|})})\}_{d_{1},\ldots,d_{|V|} =1}^{M}\) and storing the outcomes in a tensor with \(|\mathcal{V}|\) axes of dimension \(M\) each. Arranging the grid tensor as a matrix \(\mathbf{B}(f)\) where rows correspond to axes indexed by \(\mathcal{I}\) and columns correspond to the remaining axes, we show that \(\mathrm{rank}(\mathbf{B}(f))\leq\mathrm{sep}(f;\mathcal{I})\). The proof proceeds by establishing that for almost every assignment of \(\theta\), there exist template vectors with which \(\log(\mathrm{rank}(\mathbf{B}(f^{(\theta,\mathcal{G})})))\) and \(\log(\mathrm{rank}(\mathbf{B}(f^{(\theta,\mathcal{G},t)})))\) are greater than (or equal to) the right hand sides of Equations (8) and (9), respectively. 

Directed edges and multiple edge typesAppendix D generalizes Theorems 2 and 3 to the case of graphs with directed edges and an arbitrary number of edge types.

## Appendix B Related Work

Expressivity of GNNsThe expressivity of GNNs has been predominantly evaluated through ability to distinguish non-isomorphic graphs, as measured by correspondence to Weisfeiler-Leman (WL) graph isomorphism tests (see [75] for a recent survey). [103, 74] instigated this thread of research, establishing that message-passing GNNs are at most as powerful as the WL algorithm, and can match it under certain technical conditions. Subsequently, architectures surpassing WL were proposed, with expressivity measured via higher-order WL variants (see, _e.g._, [74, 72, 25, 41, 6, 15, 10, 42, 17, 80]). Another line of inquiry regards universality among continuous permutation invariant or equivariant functions [73, 55, 69, 3, 42]. [25] showed that distinguishing non-isomorphic graphs and universality are, in some sense, equivalent. Lastly, there exist analyses of expressivity focused on the frequency response of GNNs [77, 5] and their capacity to compute specific graph functions, _e.g._ moments, shortest paths, and substructure counting [35, 9, 39, 69, 26, 23, 17].

Although a primary purpose of GNNs is to model interactions between vertices, none of the past works formally characterize their ability to do so, as our theory does.12 The current work thus provides a novel perspective on the expressive power of GNNs. Furthermore, a major limitation of existing approaches -- in particular, proofs of equivalence to WL tests and universality -- is that they often operate in asymptotic regimes of unbounded network width or depth. Consequently, they fall short of addressing which type of functions can be realized by GNNs of practical size. In contrast, we characterize how the modeled interactions depend on both the input graph structure and the neural network architecture (width and depth). As shown in Section 5, this facilitates designing an efficient and effective edge sparsification algorithm.

Footnote 12: In [21], the mutual information between the embedding of a vertex and the embeddings of its neighbors was proposed as a measure of interaction. However, this measure is inherently local and allows reasoning only about the impact of neighboring nodes on each other in a GNN layer. In contrast, separation rank formulates the strength of interaction the whole GNN models across any partition of an input graph’s vertices.

Measuring modeled interactions via separation rankSeparation rank (Section 2.2) has been paramount to the study of interactions modeled by certain convolutional, recurrent, and self-attention neural networks. It enabled theoretically analyzing how different architectural parameters impact expressivity [32, 29, 30, 34, 7, 90, 62, 61, 56, 57, 64, 100, 65] and implicit regularization [83, 84, 85].13 On the practical side, insights brought forth by separation rank led to tools for improving performance, including: guidelines for architecture design [30, 62, 64, 100], pretraining schemes [65], and regularizers for countering locality in convolutional neural networks [85]. We employ separation rank for studying the interactions GNNs model between vertices, and similarly provide both theoretical insights and a practical application -- edge sparsification algorithm (Section 5).

Edge sparsificationComputations over large-scale graphs can be prohibitively expensive in terms of runtime and memory. As a result, various methods were proposed for sparsifying graphs by removing edges while attempting to maintain structural properties, such as distances between vertices [11, 48], graph Laplacian spectrum [93, 86], and vertex degree distribution [98], or outcomes of graph analysis and clustering algorithms [87, 20]. Most relevant to our work are recent edge sparsification methods aiming to preserve the prediction accuracy of GNNs as the number of removed edges increases [67, 24]. These methods require training a GNN over the original (non-sparsified) graph, hence only inference costs are reduced. Guided by our theory, in Section 5 we propose _Walk Index Sparsification_ (_WIS_) -- an edge sparsification algorithm that preserves expressive power in terms of ability to model interactions. WIS improves efficiency for both training and inference. Moreover, comparisons with the spectral algorithm of [93] and a recent method from [24] demonstrate that WIS brings about higher prediction accuracies across edge sparsity levels.

## Appendix C Tightness of Upper Bounds for Separation Rank

Theorem 2 upper bounds the separation rank with respect to \(\mathcal{I}\subseteq\mathcal{V}\) of a depth \(L\) GNN with product aggregation. According to it, under the setting of graph prediction, the separation rank is largely capped by the \((L-1)\)-walk index of \(\mathcal{I}\), _i.e._ the number of length \(L-1\) walks from \(\mathcal{C}_{\mathcal{I}}\) -- the set of vertices with an edge crossing the partition \((\mathcal{I},\mathcal{I}^{c})\). Similarly, for prediction over \(t\in\mathcal{V}\), separation rank is largely capped by the \((L-1,t)\)-walk index of \(\mathcal{I}\), which takes into account only length \(L-1\) walks from \(\mathcal{C}_{\mathcal{I}}\) ending at \(t\). Theorem 3 provides matching lower bounds, up to logarithmic terms and to the number of walks from \(\mathcal{C}_{\mathcal{I}}\) being replaced with the number of walks from any single admissible subset \(\mathcal{C}\in\mathcal{S}(\mathcal{I})\) (Definition 4). Hence, the match between the upper and lower bounds is determined by the portion of \(\mathcal{C}_{\mathcal{I}}\) that can be covered by an admissible subset.

In this appendix, to shed light on the tightness of the upper bounds, we present several concrete examples on which a significant portion of \(\mathcal{C}_{\mathcal{I}}\) can be covered by an admissible subset.

Complete graphSuppose that every two vertices are connected by an edge, _i.e._\(\mathcal{E}=\{\{i,j\}:i,j\in\mathcal{V}\}\). For any non-empty \(\mathcal{I}\subsetneq\mathcal{V}\), clearly \(\mathcal{C}_{\mathcal{I}}=\mathcal{N}(\mathcal{I})\cap\mathcal{N}(\mathcal{I}^ {c})=\mathcal{V}\). In this case, \(\mathcal{C}_{\mathcal{I}}=\mathcal{V}\in\mathcal{S}(\mathcal{I})\), meaning \(\mathcal{C}_{\mathcal{I}}\) is an admissible subset of itself. To see it is so, notice that for any \(i\in\mathcal{I},j\in\mathcal{I}^{c}\), all vertices are neighbors of both \(\mathcal{I}^{\prime}:=\{i\}\) and \(\mathcal{J}^{\prime}:=\{j\}\), which trivially have no repeating shared neighbors (Definition 3). Thus, up to a logarithmic factor, the upper and lower bounds from Theorems 2 and 3 coincide.

Chain graphSuppose that \(\mathcal{E}=\{\{i,i+1\}:i\in[|\mathcal{V}|-1]\}\cup\{\{i,i\}:i\in\mathcal{V}\}\). For any non-empty \(\mathcal{I}\subsetneq\mathcal{V}\), at least half of the vertices in \(\mathcal{C}_{\mathcal{I}}\) can be covered by an admissible subset. That is, there exists \(\mathcal{C}\in\mathcal{S}(\mathcal{I})\) satisfying \(|\mathcal{C}|\geq 2^{-1}\cdot|\mathcal{C}_{\mathcal{I}}|\). For example, such \(\mathcal{C}\) can be constructed algorithmically as follows. Let \(\mathcal{I}^{\prime},\mathcal{J}^{\prime}=\emptyset\). Starting from \(k=1\), if \(\{k,k+1\}\subseteq\mathcal{C}_{\mathcal{I}}\) and one of \(\{k,k+1\}\) is in \(\mathcal{I}\) while the other is in \(\mathcal{I}^{c}\), then assign \(\mathcal{I}^{\prime}\leftarrow\mathcal{I}^{\prime}\cup(\{k,k+1\}\cap\mathcal{ I})\), \(\mathcal{J}^{\prime}\leftarrow\mathcal{J}^{\prime}\cup(\{k,k+1\}\cap\mathcal{I}^ {c})\), and \(k\gets k+3\). That is, add each of \(\{k,k+1\}\) to either \(\mathcal{I}^{\prime}\) if it is in \(\mathcal{I}\) or \(\mathcal{J}^{\prime}\) if it is in \(\mathcal{I}^{c}\), and skip vertex \(k+2\). Otherwise, set \(k\gets k+1\). The process terminates once \(k>|\mathcal{V}|-1\). By construction, \(\mathcal{I}^{\prime}\subseteq\mathcal{I}\) and \(\mathcal{J}^{\prime}\subseteq\mathcal{I}^{c}\), implying that \(\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{\prime})\subseteq \mathcal{C}_{\mathcal{I}}\). Due to the chain graph structure, \(\mathcal{I}^{\prime}\cup\mathcal{J}^{\prime}\subseteq\mathcal{N}(\mathcal{I} ^{\prime})\cap\mathcal{N}(\mathcal{J}^{\prime})\) and \(\mathcal{I}^{\prime}\) and \(\mathcal{J}^{\prime}\) have no repeating shared neighbors (Definition 3). Furthermore, for every pair of vertices from \(\mathcal{C}_{\mathcal{I}}\) added to \(\mathcal{I}^{\prime}\) and \(\mathcal{J}^{\prime}\), we can miss at most two other vertices from \(\mathcal{C}_{\mathcal{I}}\). Thus, \(\mathcal{C}:=\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{ \prime})\) is an admissible subset of \(\mathcal{C}_{\mathcal{I}}\) satisfying \(|\mathcal{C}|\geq 2^{-1}\cdot|\mathcal{C}_{\mathcal{I}}|\).

General graphFor an arbitrary graph and non-empty \(\mathcal{I}\subsetneq\mathcal{V}\), an admissible subset of \(\mathcal{C}_{\mathcal{I}}\) can be obtained by taking any sequence of pairs \((i_{1},j_{1}),\ldots,(i_{M},j_{M})\in\mathcal{I}\times\mathcal{I}^{c}\) with no shared neighbors, in the sense that \([\mathcal{N}(i_{m})\cup\mathcal{N}(j_{m})]\cap[\mathcal{N}(i_{m^{\prime}}) \cup\mathcal{N}(j_{m^{\prime}})]=\emptyset\) for all \(m\neq m^{\prime}\in[M]\). Defining \(\mathcal{I}^{\prime}:=\{i_{1},\ldots,i_{M}\}\) and \(\mathcal{J}^{\prime}:=\{j_{1},\ldots,j_{M}\}\), by construction they do not have repeating shared neighbors (Definition 3), and so \(\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{\prime})\in \mathcal{S}(\mathcal{I})\). In particular, the shared neighbors of each pair are covered by \(\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{\prime})\), _i.e._\(\cup_{m=1}^{M}\mathcal{N}(i_{m})\cap\mathcal{N}(j_{m})\subseteq\mathcal{N}( \mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{\prime})\).

[MISSING_PAGE_FAIL:19]

Theorem 5 generalizes the lower bounds from Theorem 3 to directed graphs with multiple edge types.

**Theorem 5**.: _Consider the setting and notation of Theorem 4. Given \(I\subseteq\mathcal{V}\), for almost all assignments of weights \(\theta\), i.e. for all but a set of Lebesgue measure zero, it holds that:_

_(graph prediction)_ \[\log\bigl{(}\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G})}; \mathcal{I}\bigr{)}\bigr{)}\geq\max_{\mathcal{C}\in\mathcal{S}^{\rightarrow}( \mathcal{I})}\log(\alpha_{\mathcal{C}})\cdot\rho_{L-1}(\mathcal{C},\mathcal{V })\,,\] (13) _(vertex prediction)_ \[\log\bigl{(}\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G},t)}; \mathcal{I}\bigr{)}\bigr{)}\geq\max_{\mathcal{C}\in\mathcal{S}^{\rightarrow}( \mathcal{I})}\log(\alpha_{\mathcal{C},t})\cdot\rho_{L-1}(\mathcal{C},\{t\})\,,\] (14)

_where:_

\[\alpha_{\mathcal{C}} :=\begin{cases}D^{1/\rho_{0}(\mathcal{C},\mathcal{V})}&,\text{ if }L=1\\ (D-1)\cdot\rho_{L-1}(\mathcal{C},\mathcal{V})^{-1}+1&,\text{ if }L\geq 2\end{cases}\,,\] \[\alpha_{\mathcal{C},t} :=\begin{cases}D&,\text{ if }L=1\\ (D-1)\cdot\rho_{L-1}(\mathcal{C},\{t\})^{-1}+1&,\text{ if }L\geq 2\end{cases}\,,\]

_with \(D:=\min\{D_{x},D_{h}\}\). If \(\rho_{L-1}(\mathcal{C},\mathcal{V})=0\) or \(\rho_{L-1}(\mathcal{C},\{t\})=0\), the respective lower bound (right hand side of Equation (13) or Equation (14)) is zero by convention._

Proof sketch (proof in Appendix 1.5).: The proof follows a line identical to that of Theorem 3, only requiring adjusting definitions from undirected graphs to directed graphs with multiple edge types. 

## Appendix E Representing Graph Neural Networks With Product Aggregation as Tensor Networks

In this appendix, we prove that GNNs with product aggregation (Section 3) can be represented through tensor networks -- a graphical language for expressing tensor contractions, widely used in quantum mechanics literature for modeling quantum states (_cf._[97]). This representation facilitates upper bounding the separation ranks of a GNN with product aggregation (proofs for Theorem 2 and its extension in Appendix D), and is delivered in Appendix E.3. We note that analogous tensor network representations were shown for variants of recurrent and convolutional neural networks [61, 62]. For the convenience of the reader, we lay out basic concepts from the field of tensor analysis in Appendix E.1 and provide a self-contained introduction to tensor networks in Appendix E.2 (see [79] for a more in-depth treatment).

### Primer on Tensor Analysis

For our purposes, a _tensor_ is simply a multi-dimensional array. The _order_ of a tensor is its number of axes, which are typically called _modes_ (_e.g._ a vector is an order one tensor and a matrix is an order two tensor). The _dimension_ of a mode refers to its length, _i.e._ the number of values it can be indexed with. For an order \(N\in\mathbb{N}\) tensor \(\boldsymbol{\mathcal{A}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) with modes of dimensions \(D_{1},\ldots,D_{N}\in\mathbb{N}\), we will denote by \(\boldsymbol{\mathcal{A}}_{d_{1},\ldots,d_{N}}\) its \((d_{1},\ldots,d_{N})\)'th entry, where \((d_{1},\ldots,d_{N})\in[D_{1}]\times\cdots\times[D_{N}]\).

It is possible to rearrange tensors into matrices -- a process known as _matricization_. The matricization of \(\boldsymbol{\mathcal{A}}\) with respect to \(\mathcal{I}\subseteq[N]\), denoted \(\llbracket\boldsymbol{\mathcal{A}};\mathcal{I}\rrbracket\in\mathbb{R}^{ \prod_{i\in\mathcal{I}}D_{i}\times\prod_{j\in\mathcal{I}^{c}}D_{j}}\) is its arrangement as a matrix where rows correspond to modes indexed by \(\mathcal{I}\) and columns correspond to the remaining modes. Specifically, denoting the elements in \(\mathcal{I}\) by \(i_{1}<\cdots<i_{|\mathcal{I}|}\) and those in \(\mathcal{I}^{c}\) by \(j_{1}<\cdots<j_{|\mathcal{I}^{c}|}\), the matricization \([\boldsymbol{\mathcal{A}};\mathcal{I}]\) holds the entries of \(\boldsymbol{\mathcal{A}}\) such that \(\boldsymbol{\mathcal{A}}_{d_{1},\ldots,d_{N}}\) is placed in row index \(1+\sum_{|\mathcal{I}|}^{|\mathcal{I}|}(d_{i_{l}}-1)\prod_{l^{\prime}=l+1}^{| \mathcal{I}|}D_{i_{l^{\prime}}}\) and column index \(1+\sum_{l=1}^{|\mathcal{I}^{c}|}(d_{j_{l}}-1)\prod_{l^{\prime}=l+1}^{| \mathcal{I}|}D_{j_{l^{\prime}}}\).

Tensors with modes of the same dimension can be combined via _contraction_ -- a generalization of matrix multiplication. It will suffice to consider contractions where one of the modes being contracted is the last mode of its tensor.

**Definition 7**.: Let \(\boldsymbol{\mathcal{A}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}},\boldsymbol {\mathcal{B}}\in\mathbb{R}^{D_{1}^{\prime}\times\cdots\times D_{N^{\prime}}^{ \prime}}\) for orders \(N,N^{\prime}\in\mathbb{N}\) and mode dimensions \(D_{1},\ldots,D_{N},D_{1}^{\prime},\ldots,D_{N^{\prime}}^{\prime}\in\mathbb{N}\) satisfying \(D_{n}=D_{N^{\prime}}^{\prime}\) for some \(n\in[N]\). The _mode-\(n\) contraction_ of \(\boldsymbol{\mathcal{A}}\) with \(\boldsymbol{\mathcal{B}}\), denoted \(\boldsymbol{\mathcal{A}}*_{n}\boldsymbol{\mathcal{B}}\in\mathbb{R}^{D_{1} \times\cdots\times D_{n-1}\times D_{1}^{\prime}\times\cdots\times D_{N^{ \prime}-1}^{\prime}\times D_{n+1}\times\cdots\times D_{N}}\), is given element-wise by:

\[\left(\boldsymbol{\mathcal{A}}*_{n}\boldsymbol{\mathcal{B}}\right)_{d_{1}, \ldots,d_{n-1},d_{1}^{\prime},\ldots,d_{N^{\prime}-1}^{\prime},d_{n+1},\ldots,d_{N}}=\sum_{d_{n}=1}^{D_{n}}\boldsymbol{\mathcal{A}}_{d_{1},\ldots,d_{N}} \cdot\boldsymbol{\mathcal{B}}_{d_{1}^{\prime},\ldots,d_{N^{\prime}-1}^{\prime}, d_{n}}\,,\]for all \(d_{1}\in[D_{1}],\ldots,d_{n-1}\in[D_{n-1}],d^{\prime}_{1}\in[D^{\prime}_{1}], \ldots,d^{\prime}_{N^{\prime}-1}\in[D^{\prime}_{N^{\prime}-1}],d_{n+1}\in[D_{n+1 }],\ldots,d_{N}\in[D_{N}]\).

For example, the mode-\(2\) contraction of \(\mathbf{A}\in\mathbb{R}^{D_{1}\times D_{2}}\) with \(\mathbf{B}\in\mathbb{R}^{D^{\prime}_{1}\times D_{2}}\) boils down to multiplying \(\mathbf{A}\) with \(\mathbf{B}^{\top}\) from the right, _i.e._\(\mathbf{A}*_{2}\mathbf{B}=\mathbf{A}\mathbf{B}^{\top}\). It is oftentimes convenient to jointly contract multiple tensors. Given an order \(N\) tensor \(\boldsymbol{\mathcal{A}}\) and \(M\in\mathbb{N}_{\leq N}\) tensors \(\boldsymbol{\mathcal{B}}^{(1)},\ldots,\boldsymbol{\mathcal{B}}^{(M)}\), we use \(\boldsymbol{\mathcal{A}}*_{i\in[M]}\boldsymbol{\mathcal{B}}^{(i)}\) to denote the contraction of \(\boldsymbol{\mathcal{A}}\) with \(\boldsymbol{\mathcal{B}}^{(1)},\ldots,\boldsymbol{\mathcal{B}}^{(M)}\) in modes \(1,\ldots,M\), respectively (assuming mode dimensions are such that the contractions are well-defined).

### Tensor Networks

A _tensor network_ is an undirected weighted graph \(\mathcal{T}=(\mathcal{V}_{\mathcal{T}},\mathcal{E}_{\mathcal{T}},w_{\mathcal{ T}})\) that describes a sequence of tensor contractions (Definition 7), with vertices \(\mathcal{V}_{\mathcal{T}}\), edges \(\mathcal{E}_{\mathcal{T}}\), and a function mapping edges to natural weights \(w_{\mathcal{T}}:\mathcal{E}_{\mathcal{T}}\rightarrow\mathbb{N}\). We will only consider tensor networks that are connected. To avoid confusion with vertices and edges of a GNN's input graph, and in accordance with tensor network terminology, we refer by _nodes_ and _legs_ to the vertices and edges of a tensor network, respectively.

Every node in a tensor network is associated with a tensor, whose order is equal to the number of legs emanating from the node. Each end point of a leg is associated with a mode index, and the leg's weight determines the dimension of the corresponding tensor mode. That is, an end point of \(e\in\mathcal{E}_{\mathcal{T}}\) is a pair \((\boldsymbol{\mathcal{A}},n)\in\mathcal{V}_{\mathcal{T}}\times\mathbb{N}\), with \(n\) ranging from one to the order of \(\boldsymbol{\mathcal{A}}\), and \(w_{\mathcal{T}}(e)\) is the dimension of \(\boldsymbol{\mathcal{A}}\) in mode \(n\). A leg can either connect two nodes or be connected to a node on one end and be loose on the other end. If two nodes are connected by a leg, their associated tensors are contracted together in the modes specified by the leg. Legs with a loose end are called _open legs_. The number of open legs is exactly the order of the tensor produced by executing all contractions in the tensor network, _i.e._ by contracting the tensor network. Figure 4 presents exemplar tensor network diagrams of a vector, matrix, order \(N\in\mathbb{N}\) tensor, and vector-matrix multiplication.

### Tensor Networks Corresponding to Graph Neural Networks With Product Aggregation

Fix some undirected graph \(\mathcal{G}\) and learnable weights \(\theta=(\mathbf{W}^{(1)},\ldots,\mathbf{W}^{(L)},\mathbf{W}^{(o)})\). Let \(f^{(\theta,\mathcal{G})}\) and \(f^{(\theta,\mathcal{G},t)}\), for \(t\in\mathcal{V}\), be the functions realized by depth \(L\) graph and vertex prediction GNNs, respectively, with width \(D_{h}\) and product aggregation (Equations (2) to (5)). For \(\mathbf{X}=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R} ^{D_{x}\times|\mathcal{V}|}\), we construct tensor networks \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) whose contraction yields \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\), respectively. Both \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) adhere to a tree structure, where each leaf node is associated with a vertex feature vector, _i.e._ one of \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)}\), and each interior node is associated with a weight matrix from \(\mathbf{W}^{(1)},\ldots,\mathbf{W}^{(L)},\mathbf{W}^{(o)}\) or a _\(\delta\)-tensor_ with modes of dimension \(D_{h}\), holding ones on its hyper-diagonal and zeros elsewhere. We denote an order \(N\in\mathbb{N}\) tensor of the latter type by \(\boldsymbol{\delta}^{(N)}\in\mathbb{R}^{D_{h}\times\cdots\times D_{h}}\), _i.e._\(\boldsymbol{\delta}^{(N)}_{d_{1},\ldots,d_{N}}=1\) if \(d_{1}=\cdots=d_{N}\) and \(\boldsymbol{\delta}^{(N)}_{d_{1},\ldots,d_{N}}=0\) otherwise for all \(d_{1},\ldots,d_{N}\in[D_{h}]\).

Intuitively, \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) embody unrolled computation trees, describing the operations performed by the respective GNNs through tensor contractions. Let \(\mathbf{h}^{(l,i)}=\odot_{j\in\mathcal{N}(i)}(\mathbf{W}^{(l)}\mathbf{h}^{(l-1, j)})\) be the hidden embedding of \(i\in\mathcal{V}\) at layer \(l\in[L]\) (recall \(\mathbf{h}^{(0,j)}=\mathbf{x}^{(j)}\) for \(j\in\mathcal{V}\)), and denote \(\mathcal{N}(i)=\{j_{1},\dots,j_{M}\}\). We can describe \(\mathbf{h}^{(l,i)}\) as the outcome of contracting each \(\mathbf{h}^{(l-1,j_{1})},\dots,\mathbf{h}^{(l-1,j_{M})}\) with \(\mathbf{W}^{(l)}\), _i.e._ computing \(\mathbf{W}^{(l)}\mathbf{h}^{(l-1,j_{1})},\dots,\mathbf{W}^{(l)}\mathbf{h}^{(l- 1,j_{M})}\), followed by contracting the resulting vectors with \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}\), which induces product aggregation (see Figure 5(a)). Furthermore, in graph prediction, the output layer producing \(f^{(\theta,\mathcal{G})}(\mathbf{X})=\mathbf{W}^{(o)}(\odot_{i\in\mathcal{V} }\mathbf{h}^{(L,i)})\) amounts to contracting \(\mathbf{h}^{(L,1)},\dots,\mathbf{h}^{(L,|\mathcal{V}|)}\) with \(\boldsymbol{\delta}^{(|\mathcal{V}|+1)}\), and subsequently contracting the resulting vector with \(\mathbf{W}^{(o)}\) (see Figure 5(b)); while for vertex prediction, \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})=\mathbf{W}^{(o)}\mathbf{h}^{(L,t)}\) is a contraction of \(\mathbf{h}^{(L,t)}\) with \(\mathbf{W}^{(o)}\) (see Figure 5(c)).

Overall, every layer in a GNN with product aggregation admits a tensor network formulation given the outputs of the previous layer. Thus, we can construct a tree tensor network for the whole GNN by starting from the output layer -- Figure 5(b) for graph prediction or Figure 5(c) for vertex prediction -- and recursively expanding nodes associated with \(\mathbf{h}^{(l,i)}\) according to Figure 5(a), for \(l=L,\dots,1\) and \(i\in\mathcal{V}\). A technical subtlety is that each \(\mathbf{h}^{(l,i)}\) can appear multiple times during this procedure. In the language of tensor networks this translate to duplication of nodes. Namely, there are multiple copies of the sub-tree representing \(\mathbf{h}^{(l,i)}\) in the tensor network -- one copy per appearance when unraveling the recursion. Figure 6 displays examples for tensor network diagrams of \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\).

We note that, due to the node duplication mentioned above, the explicit definitions of \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) entail cumbersome notation. Nevertheless, we provide them in Appendix E.3.1 for the interested reader.

#### e.3.1 Explicit Tensor Network Definitions

The tree tensor network representing \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) consists of an initial input level -- the leaves of the tree -- comprising \(\rho_{L}(\{i\},\mathcal{V})\) copies of \(\mathbf{x}^{(i)}\) for each \(i\in\mathcal{V}\). We will use \(\mathbf{x}^{(i,\gamma)}\) to denote the copies of \(\mathbf{x}^{(i)}\) for \(i\in\mathcal{V}\) and \(\gamma\in[\rho_{L}(\{i\},\mathcal{V})]\). In accordance with the GNN inducing \(f^{(\theta,\mathcal{G})}\), following the initial input level are \(L+1\)_layers_. Each layer \(l\in[L]\) includes two levels: one comprising \(\rho_{L-l+1}(\mathcal{V},\mathcal{V})\) nodes standing for copies of \(\mathbf{W}^{(l)}\), and another containing \(\delta\)-tensors -- \(\rho_{L-l}(\{i\},\mathcal{V})\) copies of \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}\) per \(i\in\mathcal{V}\). We associate each node in these layers with its layer index and a vertex of the input graph \(i\in\mathcal{V}\). Specifically, we will use \(\mathbf{W}^{(l,i,\gamma)}\) to denote copies of \(\mathbf{W}^{(l)}\) and \(\boldsymbol{\delta}^{(l,i,\gamma)}\) to denote copies of \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}\), for \(l\in[L],i\in\mathcal{V}\), and \(\gamma\in\mathbb{N}\). In terms of connectivity, every leaf \(\mathbf{x}^{(i,\gamma)}\) has a leg to \(\mathbf{W}^{(1,i,\gamma)}\). The rest of the connections between nodes are such that each sub-tree whose root is \(\boldsymbol{\delta}^{(l,i,\gamma)}\) represents \(\mathbf{h}^{(l,i)}\), _i.e._ contracting the sub-tree results in the hidden Figure 6: Tensor network diagrams of \(\mathcal{T}(\mathbf{X})\) (left) and \(\mathcal{T}^{(t)}(\mathbf{X})\) (right) representing \(f^{(\vartheta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\vartheta,\mathcal{G},t)}(\mathbf{X})\), respectively, for \(t=1\in\mathcal{V}\), vertex features \(\mathbf{X}=(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)})\), and depth \(L=2\) GNNs with product aggregation (Section 3). The underlying input graph \(\mathcal{G}\), over which the GNNs operate, is depicted at the top. We draw nodes associated with \(\delta\)-tensors as rectangles to signify their special (hyper-diagonal) structure, and omit leg weights to avoid clutter (legs connected to \(\mathbf{x}^{(1)},\mathbf{x}^{(2)},\mathbf{x}^{(3)}\) have weight \(D_{x}\) while all other legs have weight \(D_{h}\)). See Appendix E.3 for further details on the construction of \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\), and Appendix E.3.1 for explicit formulations.

Figure 7: Tensor network diagrams (with explicit node duplication notation) of \(\mathcal{T}(\mathbf{X})\) (left) and \(\mathcal{T}^{(t)}(\mathbf{X})\) (right) representing \(f^{(\vartheta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\vartheta,\mathcal{G},t)}(\mathbf{X})\), respectively, for \(t=1\in\mathcal{V}\), vertex features \(\mathbf{X}=(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)})\), and depth \(L=2\) GNNs with product aggregation (Section 3). This figure is identical to Figure 6, except that it uses the explicit notation for node duplication detailed in Appendix E.3.1. Specifically, each feature vector, weight matrix, and \(\delta\)-tensor is attached with an index specifying which copy it is (rightmost index in the superscript). Additionally, weight matrices and \(\delta\)-tensors are associated with a layer index and vertex in \(\mathcal{V}\) (except for the output layer \(\delta\)-tensor in \(\mathcal{T}(\mathbf{X})\) and \(\mathbf{W}^{(o)}\)). See Equations (15) to (20) for the explicit definitions of these tensor networks.

embedding for \(i\in\mathcal{V}\) at layer \(l\in[L]\) of the GNN inducing \(f^{(\theta,\mathcal{G})}\). Last, is an output layer consisting of two connected nodes: a \(\boldsymbol{\delta}^{(|\mathcal{V}|+1)}\) node, which has a leg to every \(\delta\)-tensor from layer \(L\), and a \(\mathbf{W}^{(\circ)}\) node. See Figure 7 (left) for an example of a tensor network diagram representing \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) with this notation.

The tensor network construction for \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\) is analogous to that for \(f^{(\theta,\mathcal{G})}(\mathbf{X})\), comprising an initial input level followed by \(L+1\) layers. Its input level and first \(L\) layers are structured the same, up to differences in the number of copies for each node. Specifically, the number of copies of \(\mathbf{x}^{(i)}\) is \(\rho_{L}(\{i\},\{t\})\) instead of \(\rho_{L}(\{i\},\mathcal{V})\), the number of copies of \(\mathbf{W}^{(l)}\) is \(\rho_{L-l+1}(\mathcal{V},\{t\})\) instead of \(\rho_{L-l+1}(\mathcal{V},\mathcal{V})\), and the number of copies of \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}\) is \(\rho_{L-l}(\{i\},\{t\})\) instead of \(\rho_{L-l}(\{i\},\mathcal{V})\), for \(i\in\mathcal{V}\) and \(l\in[L]\). The output layer consists only of a \(\mathbf{W}^{(o)}\) node, which is connected to the \(\delta\)-tensor in layer \(L\) corresponding to vertex \(t\). See Figure 7 (right) for an example of a tensor network diagram representing \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\) with this notation.

Formally, the tensor network producing \(f^{(\theta,\mathcal{G})}(\mathbf{X})\), denoted \(\mathcal{T}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}(\mathbf{X})},\mathcal{E}_{ \mathcal{T}(\mathbf{X})},w_{\mathcal{T}(\mathbf{X})})\), is defined by:

\[\mathcal{V}_{\mathcal{T}(\mathbf{X})}:= \left\{\mathbf{x}^{(i,\gamma)}:i\in\mathcal{V},\gamma\in[\rho_{L} (\{i\},\mathcal{V})]\right\}\cup\] (15) \[\left\{\boldsymbol{\delta}^{(l,i,\gamma)}:l\in[L],i\in\mathcal{V},\gamma\in[\rho_{L-l+1}(\{i\},\mathcal{V})]\right\}\cup\] \[\left\{\boldsymbol{\delta}^{(|\mathcal{V}|+1)},\mathbf{W}^{(o)} \right\},\]

\[\mathcal{E}_{\mathcal{T}(\mathbf{X})}:= \left\{\left\{(\mathbf{x}^{(i,\gamma)},1),(\mathbf{W}^{(1,i, \gamma)},2)\right\}:i\in\mathcal{V},\gamma\in[\rho_{L}(\{i\},\mathcal{V})] \right\}\cup\] (16) \[\left\{\left\{(\boldsymbol{\delta}^{(l,i,\gamma)},j),(\mathbf{W}^ {(l,\mathcal{N}(i)_{j},\phi_{l,i,j}(\gamma))},1)\right\}:l\in[L],i\in\mathcal{ V},j\in[|\mathcal{N}(i)|],\gamma\in[\rho_{L-l}(\{i\},\mathcal{V})]\right\}\cup\] \[\left\{\left\{(\boldsymbol{\delta}^{(|\mathcal{V}|+1)},i),( \boldsymbol{\delta}^{(L,i,1)},|\mathcal{N}(i)|+1)\right\}:i\in\mathcal{V} \right\}\cup\left\{\left\{(\boldsymbol{\delta}^{(|\mathcal{V}|+1)},|\mathcal{ V}|+1),(\mathbf{W}^{(o)},2)\right\}\right\},\]

\[w_{\mathcal{T}(\mathbf{X})}(e):= \begin{cases}D_{x}&,\text{ if }(\mathbf{x}^{(i,\gamma)},1)\text{ is an endpoint of }e\in\mathcal{E}_{\mathcal{T}}\text{ for some }i\in\mathcal{V},\gamma\in[\rho_{L}(\{i\},\mathcal{V})]\\ D_{h}&,\text{ otherwise}\end{cases},\] (17)

where \(\phi_{l,i,j}(\gamma):=\gamma+\sum_{k<i\,\text{s.t.}\,k\in\mathcal{N}(j)}\rho_{ L-l}(\{k\},\mathcal{V})\), for \(l\in[L],i\in\mathcal{V},\) and \(\gamma\in[\rho_{L-l}(\{i\},\mathcal{V})]\), is used to map a \(\delta\)-tensor copy corresponding to \(i\) in layer \(l\) to a \(\mathbf{W}^{(l)}\) copy, and \(\mathcal{N}(i)_{j}\), for \(i\in\mathcal{V}\) and \(j\in[|\mathcal{N}(i)|]\), denotes the \(j\)'th neighbor of \(i\) according to an ascending order (recall vertices are represented by indices from \(1\) to \(|\mathcal{V}|\)).

Similarly, the tensor network \(\mathcal{T}^{(t)}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}, \mathcal{E}_{\mathcal{T}^{(t)}(\mathbf{X})},w_{\mathcal{T}^{(t)}(\mathbf{X})})\), producing \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\), is defined by:

\[\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}:= \left\{\mathbf{x}^{(i,\gamma)}:i\in\mathcal{V},\gamma\in[\rho_{L} (\{i\},\{t\})]\right\}\cup\] (18) \[\left\{\mathbf{W}^{(l,i,\gamma)}:l\in[L],i\in\mathcal{V},\gamma\in [\rho_{L-l+1}(\{i\},\{t\})]\right\}\cup\] \[\left\{\mathbf{W}^{(o)}\right\},\]\[\mathcal{E}_{\mathcal{T}^{(t)}(\mathbf{X})}:= \Big{\{}\big{\{}(\mathbf{x}^{(i,\gamma)},1),(\mathbf{W}^{(1,i, \gamma,2)},2)\big{\}}:i\in\mathcal{V},\gamma\in[\rho_{L}(\{i\},\{t\})]\Big{\}}\cup\] \[\Big{\{}\big{\{}(\bm{\delta}^{(l,i,\gamma)},j),(\mathbf{W}^{(l, \mathcal{N}(i)_{j},\phi^{(t)}_{l,i,j}(\gamma))},1)\big{\}}:l\in[L],i\in \mathcal{V},j\in[|\mathcal{N}(i)|],\gamma\in[\rho_{L-l}(\{i\},\{t\})]\Big{\}}\cup\] \[\Big{\{}\big{\{}(\bm{\delta}^{(l,i,\gamma)},|\mathcal{N}(i)|+1),( \mathbf{W}^{(l+1,i,\gamma)},2)\big{\}}:l\in[L-1],i\in\mathcal{V},\gamma\in[ \rho_{L-l}(\{i\},\{t\})]\Big{\}}\cup\] \[\Big{\{}\big{\{}(\bm{\delta}^{(L,t,1)},|\mathcal{N}(t)|+1),( \mathbf{W}^{(o)},2)\big{\}}\Big{\}}\,,\] (19)

\[w_{\mathcal{T}^{(t)}(\mathbf{X})}(e):= \begin{cases}D_{x}&,\,\text{if}\,\,(\mathbf{x}^{(i,\gamma)},1)\,\,\text{is an endpoint of}\,\,e\in\mathcal{E}_{\mathcal{T}}\,\,\text{for some}\,\,i\in\mathcal{V},\gamma\in[\rho_{L}(\{i\},\{t\})]\\ D_{h}&,\,\,\text{otherwise}\end{cases}\,,\] (20)

where \(\phi^{(t)}_{l,i,j}(\gamma):=\gamma+\sum_{k<i\,s\,L\,k\in\mathcal{N}(j)}\rho_{ L-l}(\{k\},\{t\})\), for \(l\in[L],i\in\mathcal{V}\), and \(\gamma\in[\rho_{L-l}(\{i\},\{t\})]\), is used to map a \(\delta\)-tensor copy corresponding to \(i\) in layer \(l\) to a \(\mathbf{W}^{(l)}\) copy.

Proposition 1 verifies that contracting \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) yields \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\), respectively.

**Proposition 1**.: _For an undirected graph \(\mathcal{G}\) and \(t\in\mathcal{V}\), let \(f^{(\theta,\mathcal{G})}\) and \(f^{(\theta,\mathcal{G},t)}\) be the functions realized by depth \(L\) graph and vertex prediction GNNs, respectively, with width \(D_{h}\), learnable weights \(\theta\), and product aggregation (Equations (2) to (5)). For vertex features \(\mathbf{X}=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{ R}^{D_{x}\times|\mathcal{V}|}\), let the tensor networks \(\mathcal{T}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}(\mathbf{X})},\mathcal{E}_{ \mathcal{T}(\mathbf{X})},w_{\mathcal{T}(\mathbf{X})})\) and \(\mathcal{T}^{(t)}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}, \mathcal{E}_{\mathcal{T}^{(t)}(\mathbf{X})},w_{\mathcal{T}^{(t)}(\mathbf{X})})\) be as defined in Equations (15) to (20), respectively. Then, performing the contractions described by \(\mathcal{T}(\mathbf{X})\) produces \(f^{(\theta,\mathcal{G})}(\mathbf{X})\), and performing the contractions described by \(\mathcal{T}^{(t)}(\mathbf{X})\) produces \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\)._

Proof sketch (proof in Appendix 1.6).: For both \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\), a straightforward induction over the layer \(l\in[L]\) establishes that contracting the sub-tree whose root is \(\bm{\delta}^{(l,i,\gamma)}\) results in \(\mathbf{h}^{(l,i)}\) for all \(i\in\mathcal{V}\) and \(\gamma\), where \(\mathbf{h}^{(l,i)}\) is the hidden embedding for \(i\) at layer \(l\) of the GNNs inducing \(f^{(\theta,\mathcal{G})}\) and \(f^{(\theta,\mathcal{G},t)}\), given vertex features \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)}\). The proof concludes by showing that the contractions in the output layer of \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) reproduce the operations defining \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\) in Equations (3) and (4), respectively. 

## Appendix F General Walk Index Sparsification

Our edge sparsification algorithm -- Walk Index Sparsification (WIS) -- was obtained as an instance of the General Walk Index Sparsification (GWIS) scheme described in Section 5. Algorithm 3 formally outlines this general scheme.

## Appendix G Efficient Implementation of \(1\)-Walk Index Sparsification

Algorithm 2 (Section 5) provides an efficient implementation for \(1\)-WIS, _i.e._ Algorithm 1 with \(L=2\). In this appendix, we formalize the equivalence between the two algorithms, meaning, we establish that Algorithm 2 indeed implements \(1\)-WIS.

Examining some iteration \(n\in[N]\) of \(1\)-WIS, let \(\mathbf{s}\in\mathbb{R}^{|\mathcal{V}|}\) be the tuple defined by \(\mathbf{s}_{t}=\mathrm{WI}_{1,t}(\{t\})=\rho_{1}(\mathcal{C}_{\{t\}},\{t\})\) for \(t\in\mathcal{V}\). Recall that \(\mathcal{C}_{\{t\}}\) is the set of vertices with an edge crossing the partition induced by \(\{t\}\). Thus, if \(t\) is not isolated, then \(\mathcal{C}_{\{t\}}=\mathcal{N}(t)\) and \(\mathbf{s}_{t}=\mathrm{WI}_{1,t}(\{t\})=|\mathcal{N}(t)|\). Otherwise, if \(t\) is isolated, then \(\mathcal{C}_{\{t\}}=\emptyset\) and \(\mathbf{s}_{t}=\mathrm{WI}_{1,t}(\{t\})=0\). \(1\)-WIS computes for each \(e\in\mathcal{E}\) (excluding self-loops) a tuple \(\mathbf{s}^{(e)}\in\mathbb{R}^{|\mathcal{V}|}\) holding in its \(t\)'th entry what the value of \(\mathrm{WI}_{1,t}(\{t\})\) would be if \(e\) is to be removed, for all \(t\in\mathcal{V}\). Notice that \(\mathbf{s}^{(e)}\) and \(\mathbf{s}\) agree on all entries except for \(i,j\in e\), since removing \(e\) from the graph only affects the degrees of \(i\) and \(j\). Specifically, for \(i\in e\)either \(\mathbf{s}_{i}^{(e)}=\mathbf{s}_{i}-1=|\mathcal{N}(i)|-1\) if the removal of \(e\) did not isolate \(i\), or \(\mathbf{s}_{i}^{(e)}=\mathbf{s}_{i}-2=0\) if it did (due to self-loops, if a vertex has a single edge to another then \(|\mathcal{N}(i)|=2\), so removing that edge changes \(\mathrm{WI}_{1,i}(\{i\})\) from two to zero). As a result, for any \(e=\{i,j\},e^{\prime}=\{i^{\prime},j^{\prime}\}\in\mathcal{E}\), after sorting the entries of \(\mathbf{s}^{(e)}\) and \(\mathbf{s}^{(e^{\prime})}\) in ascending order we have that \(\mathbf{s}^{(e^{\prime})}\) is greater in lexicographic order than \(\mathbf{s}^{(e)}\) if and only if the pair \((\min\{|\mathcal{N}(i^{\prime})|,|\mathcal{N}(j^{\prime})|\},\max\{|\mathcal{ N}(i^{\prime})|,|\mathcal{N}(j^{\prime})|\})\) is greater in lexicographic order than \((\min\{|\mathcal{N}(i)|,|\mathcal{N}(j)|\},\max\{|\mathcal{N}(i)|,|\mathcal{N} (j)|\})\). Therefore, at every iteration \(n\in[N]\) Algorithm 2 and \(1\)-WIS (Algorithm 1 with \(L=2\)) remove the same edge.

## Appendix H Further Experiments and Implementation Details

### Further Experiments

Figure 8 supplements Figure 3 from Section 5.2 by including experiments with additional: _(i)_ GNN architectures -- GIN and ResGCN; and _(ii)_ datasets -- Chameleon, Squirrel, and Amazon Computers. Overall, our evaluation includes six standard vertex prediction datasets in which we observed the graph structure to be crucial for accurate prediction, as measured by the difference between the test accuracy of a GCN trained and evaluated over the original graph and its test accuracy when trained and evaluated over the graph after all of the graph's edges were removed. We also considered, but excluded, the following datasets in which the accuracy difference was insignificant (less than five percentage points): Citeseer [89], PubMed [76], Coauthor CS and Physics [92], and Amazon Photo [92].

### Further Implementation Details

We provide implementation details omitted from our experimental reports (Section 4.2, Section 5, and Appendix H.1). Source code for reproducing our results and figures, based on the PyTorch [81] and PyTorch Geometric [38] frameworks, can be found at https://github.com/noamrazin/gnn_interactions. All experiments were run either on a single Nvidia RTX 2080 Ti GPU or a single Nvidia RTX A6000 GPU.

#### h.2.1 Empirical Demonstration of Theoretical Analysis (Table 1)

ModelsAll models used, _i.e._ GCN, GAT, and GIN, had three layers of width \(16\) with ReLU nonlinearity. To ease optimization, we added layer normalization [4] after each one. Mean aggregation and a linear output layer were applied over the last hidden embeddings for prediction. As in the synthetic experiments of [1], each GAT layer consisted of four attention heads. Each GIN layer had its \(\epsilon\) parameter fixed to zero and contained a two-layer feed-forward network, whose layers comprised a linear layer, batch normalization [54], and ReLU non-linearity.

DataThe datasets consisted of \(10000\) train and \(2000\) test graphs. For every graph, we drew uniformly at random a label from \(\{0,1\}\) and an image from Fashion-MNIST. Then, depending on the chosen label, another image was sampled either from the same class (for label \(1\)) or from all other classes (for label \(0\)). We extracted patches of pixels from each image by flattening it into a vector and splitting the vector to \(16\) equally sized segments.

OptimizationThe binary cross-entropy loss was minimized via the Adam optimizer [58] with default \(\beta_{1},\beta_{2}\) coefficients and full-batches (_i.e._ every batch contained the whole training set). Optimiza

Figure 8: Comparison of GNN accuracies following sparsification of input edges — WIS, the edge sparsification algorithm brought forth by our theory (Algorithm 1), markedly outperforms alternative methods. This figure supplements Figure 3 from Section 5.2 by including experiments with: _(i)_ a depth \(L=3\) GIN over the Cora, DBLP, and OGBN-ArXiv datasets; _(ii)_ a depth \(L=10\) ResGCN over the Cora, DBLP, and OGBN-ArXiv datasets; and _(iii)_ a depth \(L=3\) GCN over the Chameleon, Squirrel, and Amazon Computers datasets. Markers and error bars report means and standard deviations, respectively, taken over ten runs per configuration for GCN and GIN, and over five runs per configuration for ResGCN (we use fewer runs due to the larger size of ResGCN). For further details see caption of Figure 3 as well as Appendix H.2.

tion proceeded until the train accuracy did not improve by at least \(0.01\) over \(1000\) consecutive epochs or \(10000\) epochs elapsed. The learning rates used for GCN, GAT, and GIN were \(5\cdot 10^{-3},5\cdot 10^{-3},\) and \(10^{-2}\), respectively.

Hyperparameter tuningFor each model separately, to tune the learning rate we carried out five runs (differing in random seed) with every value in the range \(\{10^{-1},5\cdot 10^{-2},10^{-2},5\cdot 10^{-3},10^{-3}\}\) over the dataset whose essential partition has low walk index. Since our interest resides in expressivity, which manifests in ability to fit the training set, for every model we chose the learning rate that led to the highest mean train accuracy.

#### h.2.2 Edge Sparsification (Figures 3 and 8)

Adaptations to UGS [24][24] proposed UGS as a framework for jointly pruning input graph edges and weights of a GNN. At a high-level, UGS trains two differentiable masks, \(\bm{m}_{g}\) and \(\bm{m}_{\theta}\), that are multiplied with the graph adjacency matrix and the GNN's weights, respectively. Then, after a certain number of optimization steps, a predefined percentage \(p_{g}\) of graph edges are removed according to the magnitudes of entries in \(\bm{m}_{g}\), and similarly, \(p_{\theta}\) percent of the GNN's weights are fixed to zero according to the magnitudes of entries in \(\bm{m}_{\theta}\). This procedure continues in iterations, where each time the remaining GNN weights are rewinded to their initial values, until the desired sparsity levels are attained -- see Algorithms 1 and 2 in [24]. To facilitate a fair comparison of our \((L-1)\)-WIS edge sparsification algorithm with UGS, we make the following adaptations to UGS.

* We adapt UGS to only remove edges, which is equivalent to fixing the entries in the weight mask \(\bm{m}_{\theta}\) to one and setting \(p_{\theta}=0\) in Algorithm 1 of [24].
* For comparing performance across a wider range of sparsity levels, the number of edges removed at each iteration is changed from \(5\%\) of the current number of edges to \(5\%\) of the original number of edges.
* Since our evaluation focuses on undirected graphs, we enforce the adjacency matrix mask \(\bm{m}_{g}\) to be symmetric.

Spectral sparsification [93]For Cora and DBLP, we used a Python implementation of the spectral sparsification algorithm from [93], based on the PyGSP library implementation.14 To enable more efficient experimentation over the larger scale OGBN-ArXiv dataset, we used a Julia implementation based on that from the Laplacians library.15

Footnote 14: See https://github.com/epfl-lts2/pygsp/.

ModelsThe GCN and GIN models had three layers of width \(64\) with ReLU non-linearity. As in the experiments of Section 4.2, we added layer normalization [4] after each one. Every GIN layer had a trainable \(\epsilon\) parameter and contained a two-layer feed-forward network, whose layers comprised a linear layer, batch normalization [54], and ReLU non-linearity. For ResGCN, we used the implementation from [24] with ten layers of width \(64\). In all models, a linear output layer was applied over the last hidden embeddings for prediction.

DataAll datasets in our evaluation are multi-class vertex prediction tasks, each consisting of a single graph. In Cora, DBLP, and OGBN-ArXiv, vertices represent scientific publications and edges stand for citation links. In Chameleon and Squirrel, vertices represent web pages on Wikipedia and edges stand for mutual links between pages. In Amazon Computers, vertices represent products and edges indicate that two products are frequently bought together. For simplicity, we treat all graphs as undirected. Table 2 reports the number of vertices and undirected edges in each dataset. For all datasets, except OGBN-ArXiv, we randomly split the labels of vertices into train, validation, and test sets comprising \(80\%\), \(10\%\), and \(10\%\) of all labels, respectively. For OGBN-ArXiv, we used the default split from [52].

OptimizationThe cross-entropy loss was minimized via the Adam optimizer [58] with default \(\beta_{1},\beta_{2}\) coefficients and full-batches (_i.e._ every batch contained the whole training set). Optimization proceeded until the validation accuracy did not improve by at least \(0.01\) over \(1000\) consecutive epochs or \(10000\) epochs elapsed. The test accuracies reported in Figure 3 are those achieved during the epochs with highest validation accuracies. Table 3 specifies additional optimization hyperparameters.

Hyperparameter tuningFor each combination of model and dataset separately, we tuned the learning rate, weight decay coefficient, and edge mask \(\ell_{1}\) regularization coefficient for UGS, and applied the chosen values for evaluating all methods without further tuning (note that the edge mask \(\ell_{1}\) regularization coefficient is relevant only for UGS). In particular, we carried out a grid search over learning rates \(\{10^{-3},5\cdot 10^{-4},10^{-4}\}\), weight decay coefficients \(\{10^{-3},10^{-4},0\}\), and edge mask \(\ell_{1}\) regularization coefficients \(\{10^{-2},10^{-3},10^{-4}\}\). Per hyperparameter configuration, we ran ten repetitions of UGS (differing in random seed), each until all of the input graph's edges were removed. At every edge sparsity level (\(0\%,5\%,10\%,\ldots,100\%\)), in accordance with [24], we trained a new model with identical hyperparameters, but a fixed edge mask, over each of the ten graphs. We chose the hyperparameters that led to the highest mean validation accuracy, taken over the sparsity levels and ten runs.

Due to the size of the ResGCN model, tuning its hyperparameters entails significant computational costs. Thus, over the Cora and DBLP datasets, per hyperparameter configuration we ran five repetitions of UGS with ResGCN instead of ten. For the large-scale OGBN-ArXiv dataset, we adopted the same hyperparameters used for GCN.

OtherTo allow more efficient experimentation, we compute the edge removal order of \(2\)-WIS (Algorithm 1) in batches of size \(100\). Specifically, at each iteration of \(2\)-WIS, instead of removing the edge \(e^{\prime}\) with maximal walk index tuple \(\mathsf{s}^{(e^{\prime})}\), the \(100\) edges with largest walk index tuples are removed. For randomized edge sparsification algorithms -- random pruning, the spectral sparsification method of [93], and the adaptation of UGS [24] -- the evaluation runs for a given dataset and percentage of removed edges were carried over sparsified graphs obtained using different random seeds.

## Appendix I Deferred Proofs

### Additional Notation

For vectors, matrices, or tensors, parenthesized superscripts denote elements in a collection, _e.g._\((\mathbf{a}^{(i)}\in\mathbb{R}^{D})_{n=1}^{N}\), while subscripts refer to entries, _e.g._\(\mathsf{A}_{d_{1},d_{2}}\in\mathbb{R}\) is the \((d_{1},d_{2})\)'th entry of \(\mathbf{A}\in\mathbb{R}^{D_{1}\times D_{2}}\). A colon is used to indicate a range of entries, _e.g._\(\mathbf{a}_{:d}\) is the first \(d\) entries of

\begin{table}
\begin{tabular}{l l c c} \hline \hline  & \multicolumn{1}{c}{\# of Vertices} & \multicolumn{1}{c}{\# of Undirected Edges} \\ \hline Cora & \(2,\!708\) & \(5,\!278\) \\ DBLP & \(17,\!716\) & \(52,\!867\) \\ OGBN-ArXiv & \(169,\!343\) & \(1,\!157,\!799\) \\ Chameleon & \(2,\!277\) & \(31,\!396\) \\ Squirrel & \(5,\!201\) & \(198,\!423\) \\ Amazon Computers & \(13,\!381\) & \(245,\!861\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Graph size of each dataset used for comparing edge sparsification algorithms in Figures 3 and 8.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline  & & Learning Rate & Weight Decay & Edge Mask \(\ell_{1}\) & Regularization of UGS \\ \hline \multirow{4}{*}{GCN} & Cora & \(5\cdot 10^{-4}\) & \(10^{-3}\) & \(10^{-2}\) \\  & DBLP & \(10^{-3}\) & \(10^{-4}\) & \(10^{-2}\) \\  & OGBN-ArXiv & \(10^{-3}\) & \(0\) & \(10^{-2}\) \\  & Chameleon & \(10^{-3}\) & \(10^{-4}\) & \(10^{-2}\) \\  & Squirrel & \(5\cdot 10^{-4}\) & \(0\) & \(10^{-4}\) \\  & Amazon Computers & \(10^{-3}\) & \(10^{-4}\) & \(10^{-2}\) \\ \hline \multirow{3}{*}{GIN} & Cora & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) \\  & DBLP & \(10^{-3}\) & \(10^{-3}\) & \(10^{-2}\) \\  & OGBN-ArXiv & \(10^{-4}\) & \(0\) & \(10^{-2}\) \\ \hline \multirow{3}{*}{ResGCN} & Cora & \(5\cdot 10^{-4}\) & \(10^{-3}\) & \(10^{-4}\) \\  & DBLP & \(5\cdot 10^{-4}\) & \(10^{-4}\) & \(10^{-4}\) \\ \cline{1-1}  & OGBN-ArXiv & \(10^{-3}\) & \(0\) & \(10^{-2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Optimization hyperparameters used in the experiments of Figures 3 and 8 per model and dataset.

\(\mathbf{a}\in\mathbb{R}^{D}\). We use \(*\) to denote tensor contractions (Definition 7), \(\circ\) to denote the Kronecker product, and \(\odot\) to denote the Hadamard product. For \(P\in\mathbb{N}_{\geq 0}\), the \(P\)'th Hadamard power operator is denoted by \(\odot^{P}\), _i.e._\([\odot^{P}\mathbf{A}]_{d_{1},d_{2}}=\mathbf{A}_{d_{1},d_{2}}^{P}\) for \(\mathbf{A}\in\mathbb{R}^{D_{1}\times D_{2}}\). Lastly, when enumerating over sets of indices an ascending order is assumed.

### Proof of Theorem 2

We assume familiarity with the basic concepts from tensor analysis introduced in Appendix E.1, and rely on the tensor network representations established for GNNs with product aggregation in Appendix E. Specifically, we use the fact that for any \(\mathbf{X}=(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R}^ {D_{x}\times|\mathcal{V}|}\) there exist tree tensor networks \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) (described in Appendix E.3 and formally defined in Equations (15) to (20)) such that: _(i)_ their contraction yields \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\), respectively (Proposition 1); and _(ii)_ each of their leaves is associated with a vertex feature vector, _i.e._ one of \(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)}\), whereas all other aspects of the tensor networks do not depend on \(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)}\).

The proof proceeds as follows. In Appendix I.2.1, by importing machinery from tensor analysis literature (in particular, adapting Claim 7 from [62]), we show that the separation ranks of \(f^{(\theta,\mathcal{G})}\) and \(f^{(\theta,\mathcal{G},t)}\) can be upper bounded via cuts in their corresponding tensor networks. Namely, \(\operatorname{sep}(f^{(\theta,\mathcal{G})};\mathcal{I})\) is at most the minimal multiplicative cut weight in \(\mathcal{T}(\mathbf{X})\), among cuts separating leaves associated with vertices of the input graph in \(\mathcal{I}\) from leaves associated with vertices of the input graph in \(\mathcal{I}^{c}\), where multiplicative cut weight refers to the product of weights belonging to legs crossing the cut. Similarly, \(\operatorname{sep}(f^{(\theta,\mathcal{G},t)};\mathcal{I})\) is at most the minimal multiplicative cut weight in \(\mathcal{T}^{(t)}(\mathbf{X})\), among cuts of the same form. We conclude in Appendices I.2.2 and I.2.3 by applying this technique for upper bounding \(\operatorname{sep}(f^{(\theta,\mathcal{G})};\mathcal{I})\) and \(\operatorname{sep}(f^{(\theta,\mathcal{G},t)};\mathcal{I})\), respectively, _i.e._ by finding cuts in the respective tensor networks with sufficiently low multiplicative weights.

#### i.2.1 Upper Bounding Separation Rank via Multiplicative Cut Weight in Tensor Network

In a tensor network \(\mathcal{T}=(\mathcal{V}_{\mathcal{T}},\mathcal{E}_{\mathcal{T}},w_{\mathcal{ T}})\), every \(\mathcal{J}_{\mathcal{T}}\subseteq\mathcal{V}_{\mathcal{T}}\) induces a _cut_\((\mathcal{J}_{\mathcal{T}},\mathcal{J}_{\mathcal{T}}^{c})\), _i.e._ a partition of the nodes into two sets. We denote by \(\mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}}):=\{\{u,v\}\in\mathcal{E}_{ \mathcal{T}}:u\in\mathcal{J}_{\mathcal{T}},v\in\mathcal{J}_{\mathcal{T}}^{c}\}\) the set of legs crossing the cut, and define the _multiplicative cut weight_ of \(\mathcal{J}_{\mathcal{T}}\) to be the product of weights belonging to legs in \(\mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})\), _i.e._:

\[w_{\mathcal{T}}^{\Pi}(\mathcal{J}_{\mathcal{T}}):=\prod\nolimits_{e\in \mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})}w_{\mathcal{T}}(e)\,.\]

For \(\mathbf{X}=(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R}^ {D_{x}\times|\mathcal{V}|}\), let \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) be the tensor networks corresponding to \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\) (detailed in Appendix E.3), respectively. Both \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) adhere to a tree structure. Each leaf node is associated with a vertex feature vector (_i.e._ one of \(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)}\)), while interior nodes are associated with weight matrices or \(\delta\)-tensors. The latter are tensors with modes of equal dimension holding ones on their hyper-diagonal and zeros elsewhere. The restrictions imposed by \(\delta\)-tensors induce a modified notion of multiplicative cut weight, where legs incident to the same \(\delta\)-tensor only contribute once to the weight product (note that weights of legs connected to the same \(\delta\)-tensor are equal since they stand for mode dimensions).

**Definition 8**.: For a tensor network \(\mathcal{T}=(\mathcal{V}_{\mathcal{T}},\mathcal{E}_{\mathcal{T}},w_{\mathcal{ T}})\) and subset of nodes \(\mathcal{J}_{\mathcal{T}}\subseteq\mathcal{V}_{\mathcal{T}}\), let \(\mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})\) be the set of edges crossing the cut \((\mathcal{J}_{\mathcal{T}},\mathcal{J}_{\mathcal{T}}^{c})\). Denote by \(\widetilde{\mathcal{E}}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})\subseteq \mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})\) a subset of legs containing for each \(\delta\)-tensor in \(\mathcal{V}_{\mathcal{T}}\) only a single leg from \(\mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})\) incident to it, along with all legs in \(\mathcal{E}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})\) not connected to \(\delta\)-tensors. Then, the _modified multiplicative cut weight_ of \(\mathcal{J}_{\mathcal{T}}\) is:

\[\widetilde{w}_{\mathcal{T}}^{\Pi}(\mathcal{J}_{\mathcal{T}}):=\prod\nolimits _{e\in\widetilde{\mathcal{E}}_{\mathcal{T}}(\mathcal{J}_{\mathcal{T}})}w_{ \mathcal{T}}(e)\,.\]

Lemma 1 establishes that \(\operatorname{sep}(f^{(\theta,\mathcal{G})};\mathcal{I})\) and \(\operatorname{sep}(f^{(\theta,\mathcal{G},t)};\mathcal{I})\) are upper bounded by the minimal modified multiplicative cut weights in \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\), respectively, among cuts separating leaves associated with vertices in \(\mathcal{I}\) from leaves associated vertices in \(\mathcal{I}^{c}\).

**Lemma 1**.: _For any \(\mathbf{X}=(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R}^{D_{ x}\times|\mathcal{V}|}\), let \(\mathcal{T}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}(\mathbf{X})},\mathcal{E}_{\mathcal{T} (\mathbf{X})},w_{\mathcal{T}(\mathbf{X})})\) and \(\mathcal{T}^{(t)}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}, \mathcal{E}_{\mathcal{T}^{(t)}(\mathbf{X})},w_{\mathcal{T}^{(t)}(\mathbf{X})})\) be the tensor network representations of \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\) (described in Appendix E.3 and formally defined in Equations (15) to (20)), respectively,_Denote by \(\mathcal{V}_{\mathcal{T}(\mathbf{X})}[\mathcal{I}]\subseteq\mathcal{V}_{\mathcal{T}( \mathbf{X})}\) and \(\mathcal{V}_{\mathcal{T}^{(1)}(\mathbf{X})}[\mathcal{I}]\subseteq\mathcal{V}_{ \mathcal{T}^{(t)}(\mathbf{X})}\) the sets of leaf nodes in \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\), respectively, associated with vertices in \(\mathcal{I}\) from the input graph \(\mathcal{G}\). Formally:_

\[\mathcal{V}_{\mathcal{T}(\mathbf{X})}[\mathcal{I}] :=\left\{\mathbf{x}^{(i,\gamma)}\in\mathcal{V}_{\mathcal{T}( \mathbf{X})}:i\in\mathcal{I},\gamma\in[\rho_{L}(\{i\},\mathcal{V})]\right\},\] \[\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}[\mathcal{I}] :=\left\{\mathbf{x}^{(i,\gamma)}\in\mathcal{V}_{\mathcal{T}^{(t)}( \mathbf{X})}:i\in\mathcal{I},\gamma\in[\rho_{L}(\{i\},\{t\})]\right\}.\]

_Similarly, denote by \(\mathcal{V}_{\mathcal{T}(\mathbf{X})}[\mathcal{I}^{c}]\subseteq\mathcal{V}_{ \mathcal{T}(\mathbf{X})}\) and \(\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}[\mathcal{I}^{c}]\subseteq\mathcal{ V}_{\mathcal{T}^{(t)}(\mathbf{X})}\) the sets of leaf nodes in \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\), respectively, associated with vertices in \(\mathcal{I}^{c}\). Then, the following hold:_

(graph prediction) \[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G})};\mathcal{I} \bigr{)}\leq\min_{\begin{subarray}{c}\mathcal{J}_{\mathcal{T}(\mathbf{X})} \subseteq\mathcal{V}_{\mathcal{T}(\mathbf{X})}\\ \text{s.t.}\ \mathcal{V}_{\mathcal{T}(\mathbf{X})}[\mathcal{I}]\subseteq \mathcal{J}_{\mathcal{T}(\mathbf{X})}\text{ and }\mathcal{V}_{\mathcal{T}(\mathbf{X})}[\mathcal{I}^{c}] \subseteq\mathcal{J}^{c}_{\mathcal{T}(\mathbf{X})}\end{subarray}}\widetilde{w }^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}(\mathbf{X})})\,,\] (21) (vertex prediction) \[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G})};\mathcal{I} \bigr{)}\leq\min_{\begin{subarray}{c}\mathcal{J}_{\mathcal{T}^{(t)}( \mathbf{X})}\subseteq\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}\\ \text{s.t.}\ \mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}[\mathcal{I}]\subseteq \mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\text{ and }\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}[\mathcal{I}^{c}] \subseteq\mathcal{J}^{c}_{\mathcal{T}^{(t)}(\mathbf{X})}\end{subarray}},\] (22)

_where \(\widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}( \mathbf{X})})\) is the modified multiplicative cut weight of \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\) in \(\mathcal{T}(\mathbf{X})\) and \(\widetilde{w}^{\Pi}_{\mathcal{T}^{(t)}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}^ {(t)}(\mathbf{X})})\) is the modified multiplicative cut weight of \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\) in \(\mathcal{T}^{(t)}(\mathbf{X})\) (Definition 8)._

Proof.: We first prove Equation (21). Examining \(\mathcal{T}(\mathbf{X})\), notice that: _(i)_ by Proposition 1 its contraction yields \(f^{(\theta,\mathcal{G})}(\mathbf{X})\); _(ii)_ it has a tree structure; and _(iii)_ each of its leaves is associated with a vertex feature vector, _i.e._ one of \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)}\), whereas all other aspects of the tensor network do not depend on \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)}\). Specifically, for any \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\) the nodes, legs, and leg weights of \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}(\mathbf{X}^{\prime})\) are identical, up to the assignment of features in the leaf nodes. Let \(\boldsymbol{\mathcal{F}}\in\mathbb{R}^{D_{x}\times\cdots\times D_{x}}\) be the order \(\rho_{L}(\mathcal{V},\mathcal{V})\) tensor obtained by contracting all interior nodes in \(\mathcal{T}(\mathbf{X})\). The above implies that we may write \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) as a contraction of \(\boldsymbol{\mathcal{F}}\) with \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)}\). Specifically, it holds that:

\[f^{(\theta,\mathcal{G})}(\mathbf{X})=\boldsymbol{\mathcal{F}}*_{n\in[\rho_{L} (\mathcal{V},\mathcal{V})]}\mathbf{x}^{(\mu(n))}\,,\] (23)

for any \(\mathbf{X}=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R}^ {D_{x}\times|\mathcal{V}|}\), where \(\mu:[\rho_{L}(\mathcal{V},\mathcal{V})]\rightarrow\mathcal{V}\) maps a mode index of \(\boldsymbol{\mathcal{F}}\) to the appropriate vertex of \(\mathcal{G}\) according to \(\mathcal{T}(\mathbf{X})\). Let \(\mu^{-1}(\mathcal{I}):=\{n\in[\rho_{L}(\mathcal{V},\mathcal{V})]:\mu(n)\in \mathcal{I}\}\) be the mode indices of \(\boldsymbol{\mathcal{F}}\) corresponding to vertices in \(\mathcal{I}\). Invoking Lemma 2 leads to the following matricized form of Equation (23):

\[f^{(\theta,\mathcal{G})}(\mathbf{X})=\bigl{(}\circ_{n\in\mu^{-1}(\mathcal{I})} \mathbf{x}^{(\mu(n))}\bigr{)}^{\top}\bigl{[}\!\bigl{[}\boldsymbol{\mathcal{F}}; \mu^{-1}(\mathcal{I})\bigr{]}\!\bigr{]}\bigl{(}\circ_{n\in\mu^{-1}(\mathcal{I}^ {c})}\mathbf{x}^{(\mu(n))}\bigr{)}\,,\]

where \(\circ\) denotes the Kronecker product.

We claim that \(\operatorname{sep}(f^{(\theta,\mathcal{G})};\mathcal{I})\leq\operatorname{rank} \bigl{[}\!\bigl{[}\boldsymbol{\mathcal{F}};\mu^{-1}(\mathcal{I})\bigr{]}\! \bigr{]}\). To see it is so, denote \(R:=\operatorname{rank}\bigl{[}\!\bigl{[}\boldsymbol{\mathcal{F}};\mu^{-1}( \mathcal{I})\bigr{]}\!\bigr{]}\) and let \(\mathbf{u}^{(1)},\ldots,\mathbf{u}^{(R)}\in\mathbb{R}^{D_{x}^{\mu^{L}( \mathcal{I},\mathcal{V})}}\) and \(\bar{\mathbf{u}}^{(1)},\ldots,\bar{\mathbf{u}}^{(R)}\in\mathbb{R}^{D_{x}^{\mu^{L}( \mathcal{I}^{c},\mathcal{V})}}\) be such that \(\bigl{[}\!\bigl{[}\boldsymbol{\mathcal{F}};\mu^{-1}(\mathcal{I})\bigr{]}=\sum_{r=1 }^{R}\mathbf{u}^{(r)}(\bar{\mathbf{u}}^{(r)})^{\top}\!\bigr{]}\). Then, defining \(g^{(r)}:(\mathbb{R}^{D_{x}})^{|\mathcal{I}|}\rightarrow\mathbb{R}\) and \(\bar{g}^{(r)}:(\mathbb{R}^{D_{x}})^{|\mathcal{I}^{c}|}\rightarrow\mathbb{R}\), for \(r\in[R]\), as:

\[g^{(r)}(\mathbf{X}_{\mathcal{I}}):=\Bigl{\langle}\circ_{n\in\mu^{-1}(\mathcal{I})} \mathbf{x}^{(\mu(n))},\mathbf{u}^{(r)}\Bigr{\rangle}\quad,\quad\bar{g}^{(r)}( \mathbf{X}_{\mathcal{I}^{c}}):=\Bigl{\langle}\circ_{n\in\mu^{-1}(\mathcal{I}^{c})} \mathbf{x}^{(\mu(n))},\bar{\mathbf{u}}^{(r)}\Bigr{\rangle}\,,\]

where \(\mathbf{X}_{\mathcal{I}}:=\left(\mathbf{x}^{(i)}\right)_{i\in\mathcal{I}}\) and \(\mathbf{X}_{\mathcal{I}^{c}}:=\left(\mathbf{x}^{(j)}\right)_{j\in\mathcal{I}^{c}}\), we have that:

\[f^{(\theta,\mathcal{G})}(\mathbf{X}) =\bigl{(}\circ_{n\in\mu^{-1}(\mathcal{I})}\mathbf{x}^{(\mu(n))} \bigr{)}^{\top}\Bigl{(}\sum\nolimits_{r=1}^{R}\mathbf{u}^{(r)}\bigl{(}\bar{ \mathbf{u}}^{(r)}\bigr{)}^{\top}\Bigr{)}\bigl{(}\circ_{n\in\mu^{-1}(\mathcal{I}^ {c})}\mathbf{x}^{(\mu(n)tensor \(\bm{\mathcal{A}}\) produced by contracting a tree tensor network \(\mathcal{T}\). Then, for any \(\mathcal{K}\subseteq[N]\) we have that \(\operatorname{rank}[\bm{\mathcal{A}};\mathbb{K}]\) is at most the minimal modified multiplicative cut weight in \(\mathcal{T}\), among cuts separating leaves corresponding to modes \(\mathcal{K}\) from leaves corresponding to modes \(\mathcal{K}^{c}\). Thus, invoking Claim 7 from [62] establishes Equation (21):

\[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G})};\mathcal{I}\bigr{)}\leq \operatorname{rank}\bigl{[}\bm{\mathcal{F}};\mu^{-1}(\mathcal{I})\bigr{]} \leq\min_{\begin{subarray}{c}\mathcal{J}_{\mathcal{T}(\mathbf{X})}\subseteq \mathcal{V}_{\mathcal{T}(\mathbf{X})}\\ \text{s.t. }\mathcal{V}_{\mathcal{T}(\mathbf{X})}[T]\subseteq\mathcal{J}_{ \mathcal{T}(\mathbf{X})}\text{ and }\mathcal{V}_{\mathcal{T}(\mathbf{X})}[T^{c}] \subseteq\mathcal{J}_{\mathcal{T}(\mathbf{X})}^{\mathcal{F}_{\mathcal{T}( \mathbf{X})}}\end{subarray}}\widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}( \mathcal{J}_{\mathcal{T}(\mathbf{X})})\,.\]

Equation (22) readily follows by steps analogous to those used above for proving Equation (21). 

#### i.2.2 Cut in Tensor Network for Graph Prediction (Proof of Equation (6))

For \(\mathbf{X}=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R}^ {D_{x}\times|\mathcal{V}|}\), let \(\mathcal{T}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}(\mathbf{X})},\mathcal{E}_{ \mathcal{T}(\mathbf{X})},w_{\mathcal{T}(\mathbf{X})})\) be the tensor network corresponding to \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) (detailed in Appendix E.3 and formally defined in Equations (15) to (17)). By Lemma 1, to prove that

\[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G})};\mathcal{I}\bigr{)}\leq D_{ h}^{4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})+1}\,,\]

it suffices to find \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\subseteq\mathcal{V}_{\mathcal{T}( \mathbf{X})}\) satisfying: _(i)_ leaves of \(\mathcal{T}(\mathbf{X})\) associated with vertices in \(\mathcal{I}\) are in \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\), whereas leaves associated with vertices in \(\mathcal{I}^{c}\) are not in \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\); and _(ii)_\(\widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}( \mathbf{X})})\leq D_{h}^{4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})+1}\), where \(\widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}( \mathbf{X})})\) is the modified multiplicative cut weight of \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\) (Definition 8). To this end, define \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\) to hold all nodes in \(\mathcal{V}_{\mathcal{T}(\mathbf{X})}\) corresponding to vertices in \(\mathcal{I}\). Formally:

\[\mathcal{J}_{\mathcal{T}(\mathbf{X})}:=\Bigl{\{}\mathbf{x}^{(i, \gamma)}:i\in\mathcal{I},\gamma\in[\rho_{L}(\{i\},\mathcal{V})]\Bigr{\}}\cup\] \[\Bigl{\{}\mathbf{W}^{(l,i,\gamma)}:l\in[L],i\in\mathcal{I}, \gamma\in[\rho_{L-l+1}(\{i\},\mathcal{V})]\Bigr{\}}\cup\] \[\Bigl{\{}\bm{\delta}^{(l,i,\gamma)}:l\in[L],i\in\mathcal{I}, \gamma\in[\rho_{L-l}(\{i\},\mathcal{V})]\Bigr{\}}\,.\]

Clearly, \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\) upholds _(i)_.

As for _(ii)_, there are two types of legs crossing the cut induced by \(\mathcal{J}_{\mathcal{T}(\mathbf{X})}\) in \(\mathcal{T}(\mathbf{X})\). First, are those connecting a \(\delta\)-tensor with a weight matrix in the same layer, where one is associated with a vertex in \(\mathcal{I}\) and the other with a vertex in \(\mathcal{I}^{c}\). That is, legs connecting \(\bm{\delta}^{(l,i,\gamma)}\) with \(\mathbf{W}^{(l,\mathcal{N}(i),\phi_{l,i,j}(\gamma))}\), where \(i\in\mathcal{V}\) and \(\mathcal{N}(i)_{j}\in\mathcal{V}\) are on different sides of the partition \((\mathcal{I},\mathcal{I}^{c})\) in the input graph, for \(j\in[|\mathcal{N}(i)|],l\in[L],\gamma\in[\rho_{L-l}(\{i\},\mathcal{V})]\). The \(\delta\)-tensors participating in these legs are exactly those associated with some \(i\in\mathcal{I}\) (recall \(\mathcal{C}_{\mathcal{I}}\) is the set of vertices with an edge crossing the partition \((\mathcal{I},\mathcal{I}^{c})\)). So, for every \(l\in[L]\) and \(i\in\mathcal{C}_{\mathcal{I}}\) there are \(\rho_{L-l}(\{i\},\mathcal{V})\) such \(\delta\)-tensors. Second, are legs from \(\delta\)-tensors associated with \(i\in\mathcal{I}\) in the \(L\)'th layer to the \(\delta\)-tensor in the output layer of \(\mathcal{T}(\mathbf{X})\). That is, legs connecting \(\bm{\delta}^{(L,i,1)}\) with \(\delta^{(|\mathcal{V}|+1)}\), for \(i\in\mathcal{I}\). Legs incident to the same \(\delta\)-tensor only contribute once to \(\widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}( \mathbf{X})})\). Thus, since the weights of all legs connected to \(\delta\)-tensors are equal to \(D_{h}\), we have that:

\[\widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}( \mathbf{X})})\leq D_{h}^{1+\sum_{l=1}^{L}\sum_{i\in\mathcal{C}_{\mathcal{I}}} \rho_{L-l}(\{i\},\mathcal{V})}=D_{h}^{1+\sum_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{ \mathcal{I}},\mathcal{V})}\,.\]

Lastly, it remains to show that \(\sum_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\leq 4\rho_{L-1}( \mathcal{C}_{\mathcal{I}},\mathcal{V})\), since in that case Lemma 1 implies:

\[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G})};\mathcal{I}\bigr{)}\leq \widetilde{w}^{\Pi}_{\mathcal{T}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}( \mathbf{X})})\leq D_{h}^{4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})+1}\,,\]

which yields Equation (6) by taking the log of both sides.

The main idea is that, in an undirected graph with self-loops, the number of length \(l\in\mathbb{N}\) walks from vertices with at least one neighbor decays exponentially when \(l\) decreases. Observe that \(\rho_{l}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\leq\rho_{l+1}(\mathcal{C}_{ \mathcal{I}},\mathcal{V})\) for all \(l\in\mathbb{N}\). Hence:

\[\sum\nolimits_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\leq 2 \sum\nolimits_{l\in\{1,3,\ldots,L-1\}}\rho_{L-l}(\mathcal{C}_{\mathcal{I}}, \mathcal{V})\,.\] (24)

Furthermore, any length \(l\in\mathbb{N}_{\geq 0}\) walk \(i_{0},i_{1},\ldots,i_{l}\in\mathcal{V}\) from \(\mathcal{C}_{\mathcal{I}}\) induces at least two walks of length \(l+2\) from \(\mathcal{C}_{\mathcal{I}}\), distinct from those induced by other length \(l\) walks -- one which goes twice through the self-loop of \(i_{0}\) and then proceeds according to the length \(l\) walk, _i.e._\(i_{0},i_{0},i_{0},i_{1},\ldots,i_{l}\), and another that goes to a neighboring vertex (exists since \(i_{0}\in\mathcal{C}_{\mathcal{I}}\)), returns to \(i_{0}\), and then proceeds according to the length \(l\) walk. This means that \(\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\leq 2^{-1}\cdot\rho_{L-l+2}( \mathcal{C}_{\mathcal{I}},\mathcal{V})\leq\cdots\leq 2^{-\lfloor l/2\rfloor}\cdot \rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\) for all \(l\in\{3,5,\ldots,L-1\}\). Going back to Equation (24), this leads to:

\[\sum\nolimits_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}}, \mathcal{V}) \leq 2\sum\nolimits_{l\in\{1,3,\ldots,L-1\}}2^{\lfloor l/2 \rfloor}\cdot\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\] \[\leq 2\sum\nolimits_{l=0}^{\infty}2^{-l}\cdot\rho_{L-1}(\mathcal{ C}_{\mathcal{I}},\mathcal{V})\] \[=4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\mathcal{V})\,,\]

completing the proof of Equation (6).

#### 1.2.3 Cut in Tensor Network for Vertex Prediction (Proof of Equation (7))

This part of the proof follows a line similar to that of Appendix I.2.2, with differences stemming from the distinction between the operation of a GNN over graph and vertex prediction tasks.

For \(\mathbf{X}=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R }^{D_{x}\times|\mathcal{V}|}\), let \(\mathcal{T}^{(t)}(\mathbf{X})=(\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}, \mathcal{E}_{\mathcal{T}^{(t)}(\mathbf{X})},w_{\mathcal{T}^{(t)}(\mathbf{X})})\) be the tensor network corresponding to \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\) (detailed in Appendix E.3 and formally defined in Equations (18) to (20)). By Lemma 1, to prove that

\[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G},t)};\mathcal{I}\bigr{)}\leq D _{h}^{4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})}\,,\]

it suffices to find \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\subseteq\mathcal{V}_{\mathcal{T} ^{(t)}(\mathbf{X})}\) satisfying: _(i)_ leaves of \(\mathcal{T}^{(t)}(\mathbf{X})\) associated with vertices in \(\mathcal{I}\) are in \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\), whereas leaves associated with vertices in \(\mathcal{I}^{c}\) are not in \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\); and _(ii)_\(\widetilde{w}^{\Pi}_{\mathcal{T}^{(t)}(\mathbf{X})}(\mathcal{J}_{\mathcal{T} ^{(t)}(\mathbf{X})})\leq D_{h}^{4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})}\), where \(\widetilde{w}^{\Pi}_{\mathcal{T}^{(t)}(\mathbf{X})}(\mathcal{J}_{\mathcal{T} ^{(t)}(\mathbf{X})})\) is the modified multiplicative cut weight of \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\) (Definition 8). To this end, define \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\) to hold all nodes in \(\mathcal{V}_{\mathcal{T}^{(t)}(\mathbf{X})}\) corresponding to vertices in \(\mathcal{I}\). Formally:

\[\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}:= \Bigl{\{}\mathbf{x}^{(i,\gamma)}:i\in\mathcal{I},\gamma\in[\rho_ {L}(\{i\},\{t\})]\Bigr{\}}\cup\] \[\Bigl{\{}\mathbf{W}^{(l,i,\gamma)}:l\in[L],i\in\mathcal{I},\gamma \in[\rho_{L-l+1}(\{i\},\{t\})]\Bigr{\}}\cup\] \[\mathcal{W}^{(o)}\,,\]

where \(\mathcal{W}^{(o)}:=\{\mathbf{W}^{(o)}\}\) if \(t\in\mathcal{I}\) and \(\mathcal{W}^{(o)}:=\emptyset\) otherwise. Clearly, \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\) upholds _(i)_.

As for _(ii)_, the legs crossing the cut induced by \(\mathcal{J}_{\mathcal{T}^{(t)}(\mathbf{X})}\) in \(\mathcal{T}^{(t)}(\mathbf{X})\) are those connecting a \(\delta\)-tensor with a weight matrix in the same layer, where one is associated with a vertex in \(\mathcal{I}\) and the other with a vertex in \(\mathcal{I}^{c}\). That is, legs connecting \(\boldsymbol{\delta}^{(l,i,\gamma)}\) with \(\mathbf{W}^{(l,\mathcal{V}(i)_{j},\phi^{(t)}_{l,i,j}(\gamma))}\), where \(i\in\mathcal{V}\) and \(\mathcal{N}(i)_{j}\in\mathcal{V}\) are on different sides of the partition \((\mathcal{I},\mathcal{I}^{c})\) in the input graph, for \(j\in[|\mathcal{N}(i)|],l\in[L],\gamma\in[\rho_{L-l}(\{i\},\{t\})]\). The \(\delta\)-tensors participating in these legs are exactly those associated with some \(i\in\mathcal{C}_{\mathcal{I}}\) (recall \(\mathcal{C}_{\mathcal{I}}\) is the set of vertices with an edge crossing the partition \((\mathcal{I},\mathcal{I}^{c})\)). Hence, for every \(l\in[L]\) and \(i\in\mathcal{C}_{\mathcal{I}}\) there are \(\rho_{L-l}(\{i\},\{t\})\) such \(\delta\)-tensors. Legs connected to the same \(\delta\)-tensor only contribute once to \(\widetilde{w}^{\Pi}_{\mathcal{T}^{(t)}(\mathbf{X})}(\mathcal{J}_{\mathcal{T} ^{(t)}(\mathbf{X})})\). Thus, since the weights of all legs connected to \(\delta\)-tensors are equal to \(D_{h}\), we have that:

\[\widetilde{w}^{\Pi}_{\mathcal{T}^{(t)}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}^{( t)}(\mathbf{X})})=D_{h}^{\sum_{l=1}^{L}\sum_{i\in\mathcal{C}_{\mathcal{I}}}\rho_{L-l} (\{i\},\{t\})}=D_{h}^{\sum_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\{t\})}\,.\]

Lastly, it remains to show that \(\sum_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\{t\})\leq 4\rho_{L-1}( \mathcal{C}_{\mathcal{I}},\{t\})\), as in that case Lemma 1 implies:

\[\operatorname{sep}\bigl{(}f^{(\theta,\mathcal{G},t)};\mathcal{I}\bigr{)}\leq \widetilde{w}^{\Pi}_{\mathcal{T}^{(t)}(\mathbf{X})}(\mathcal{J}_{\mathcal{T}^{(t )}(\mathbf{X})})\leq D_{h}^{4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})}\,,\]

which leads to Equation (7) by taking the log of both sides.

The main idea is that, in an undirected graph with self-loops, the number of length \(l\in\mathbb{N}\) walks ending at \(t\) that originate from vertices with at least one neighbor decays exponentially when \(l\) decreases. First, clearly \(\rho_{l}(\mathcal{C}_{\mathcal{I}},\{t\})\leq\rho_{l+1}(\mathcal{C}_{\mathcal{I}},\{t\})\) for all \(l\in\mathbb{N}\). Therefore:

\[\sum\nolimits_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\{t\})\leq 2\sum \nolimits_{l\in\{1,3,\ldots,L-1\}}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\{t\})\,.\] (25)Furthermore, any length \(l\in\mathbb{N}_{\geq 0}\) walk \(i_{0},i_{1},\ldots,i_{l-1},t\in\mathcal{V}\) from \(\mathcal{C}_{\mathcal{I}}\) to \(t\) induces at least two walks of length \(l+2\) from \(\mathcal{C}_{\mathcal{I}}\) to \(t\), distinct from those induced by other length \(l\) walks -- one which goes twice through the self-loop of \(i_{0}\) and then proceeds according to the length \(l\) walk, _i.e._\(i_{0},i_{0},i_{1},\ldots,i_{l-1},t\), and another that goes to a neighboring vertex (exists since \(i_{0}\in\mathcal{C}_{\mathcal{I}}\)), returns to \(i_{0}\), and then proceeds according to the length \(l\) walk. This means that \(\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\{t\})\leq 2^{-1}\cdot\rho_{L-l+2}( \mathcal{C}_{\mathcal{I}},\{t\})\leq\cdots\leq 2^{-\lfloor l/2\rfloor}\cdot\rho_{L-1} (\mathcal{C}_{\mathcal{I}},\{t\})\) for all \(l\in\{3,5,\ldots,L-1\}\). Going back to Equation (25), we have that:

\[\sum\nolimits_{l=1}^{L}\rho_{L-l}(\mathcal{C}_{\mathcal{I}},\{t\}) \leq 2\sum\nolimits_{l\in\{1,3,\ldots,L-1\}}2^{\lfloor l/2 \rfloor}\cdot\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})\] \[\leq 2\sum\nolimits_{l=0}^{\infty}2^{-l}\cdot\rho_{L-1}( \mathcal{C}_{\mathcal{I}},\{t\})\] \[=4\rho_{L-1}(\mathcal{C}_{\mathcal{I}},\{t\})\,,\]

concluding the proof of Equation (7). 

#### i.2.4 Technical Lemma

**Lemma 2**.: _For any order \(N\in\mathbb{N}\) tensor \(\boldsymbol{\mathcal{A}}\in\mathbb{R}^{D\times\cdots\times D}\), vectors \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)}\in\mathbb{R}^{D}\), and subset of mode indices \(\mathcal{I}\subseteq[N]\), it holds that \(\boldsymbol{\mathcal{A}}\ast_{i\in[N]}\mathbf{x}^{(i)}=\big{(}\circ_{i\in \mathcal{I}}\mathbf{x}^{(i)}\big{)}^{\top}\llbracket\boldsymbol{\mathcal{A}}; \mathcal{I}\rrbracket\big{(}\circ_{j\in\mathcal{I}}\mathbf{x}^{(j)}\big{)}\in \mathbb{R}\)._

Proof.: The identity follows directly from the definitions of tensor contraction, matricization, and Kronecker product (Appendix I.1):

\[\boldsymbol{\mathcal{A}}\ast_{i\in[N]}\mathbf{x}^{(i)}=\sum\nolimits_{d_{1},\ldots,d_{N}=1}^{D}\boldsymbol{\mathcal{A}}_{d_{1},\ldots,d_{N}}\cdot\prod \nolimits_{i\in[N]}\mathbf{x}^{(i)}_{d_{i}}=\big{(}\circ_{i\in\mathcal{I}} \mathbf{x}^{(i)}\big{)}^{\top}\llbracket\boldsymbol{\mathcal{A}};\mathcal{I} \rrbracket\big{(}\circ_{j\in\mathcal{I}^{c}}\mathbf{x}^{(j)}\big{)}\,.\]

### Proof of Theorem 3

We assume familiarity with the basic concepts from tensor analysis introduced in Appendix E.1.

We begin by establishing a general technique for lower bounding the separation rank of a function through _grid tensors_, also used in [64, 100, 65, 85]. For any \(f:(\mathbb{R}^{D_{x}})^{N}\to\mathbb{R}\) and \(M\in\mathbb{N}\)_template vectors_\(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\in\mathbb{R}^{D_{x}}\), we can create a grid tensor of \(f\), which is a form of function discretization, by evaluating it over each point in \(\{(\mathbf{v}^{(\mathcal{I}_{1})},\ldots,\mathbf{v}^{(d_{N})})\}_{d_{1},\ldots, \ldots,\mathbf{v}^{(1)}=1}^{M}\) and storing the outcomes in an order \(N\) tensor with modes of dimension \(M\). That is, the grid tensor of \(f\) for templates \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\), denoted \(\boldsymbol{\mathcal{B}}(f)\in\mathbb{R}^{M\times\cdots\times M}\), is defined by \(\boldsymbol{\mathcal{B}}(f)_{d_{1},\ldots,d_{N}}=f(\mathbf{v}^{(d_{1})},\ldots, \mathbf{v}^{(d_{N})})\) for all \(d_{1},\ldots,d_{N}\in[M]\).16 Lemma 3 shows that \(\operatorname{sep}(f;\mathcal{I})\) is lower bounded by the rank of \(\boldsymbol{\mathcal{B}}(f)\)'s matricization with respect to \(\mathcal{I}\).

Footnote 16: The template vectors of a grid tensor \(\boldsymbol{\mathcal{B}}(f)\) will be clear from context, thus we omit them from the notation.

**Lemma 3**.: _For \(f:(\mathbb{R}^{D_{x}})^{N}\to\mathbb{R}\) and \(M\in\mathbb{N}\) template vectors \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\in\mathbb{R}^{D_{x}}\), let \(\boldsymbol{\mathcal{B}}(f)\in\mathbb{R}^{M\times\cdots\times M}\) be the corresponding order \(N\) grid tensor of \(f\). Then, for any \(\mathcal{I}\subseteq[N]\):_

\[\operatorname{rank}[\boldsymbol{\mathcal{B}}(f);\mathcal{I}]\leq\operatorname{ sep}(f;\mathcal{I})\,.\]

Proof.: If \(\operatorname{sep}(f;\mathcal{I})\) is \(\infty\) or zero, _i.e._\(f\) cannot be represented as a finite sum of separable functions (with respect to \(\mathcal{I}\)) or is identically zero, then the claim is trivial. Otherwise, denote \(R:=\operatorname{sep}(f;\mathcal{I})\), and let \(g^{(1)},\ldots,g^{(R)}:(\mathbb{R}^{D_{x}})^{|\mathcal{I}|}\to\mathbb{R}\) and \(\bar{g}^{(1)},\ldots,\bar{g}^{(R)}:(\mathbb{R}^{D_{x}})^{|\mathcal{I}^{ \mathcal{I}}|}\to\mathbb{R}\) such that:

\[f(\mathbf{X})=\sum\nolimits_{r=1}^{R}g^{(r)}(\mathbf{X}_{\mathcal{I}})\cdot \bar{g}^{(r)}(\mathbf{X}_{\mathcal{I}^{c}})\,,\] (26)

where \(\mathbf{X}:=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\), \(\mathbf{X}_{\mathcal{I}}:=(\mathbf{x}^{(i)})_{i\in\mathcal{I}}\), and \(\mathbf{X}_{\mathcal{I}^{c}}:=(\mathbf{x}^{(j)})_{j\in\mathcal{I}^{c}}\). For \(r\in[R]\), let \(\boldsymbol{\mathcal{B}}(g^{(r)})\) and \(\boldsymbol{\mathcal{B}}(\bar{g}^{(r)})\) be the grid tensors of \(g^{(r)}\) and \(\bar{g}^{(r)}\) over templates \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\), respectively. That is, \(\boldsymbol{\mathcal{B}}(g^{(r)})_{d_{i}:i\in\mathcal{I}}=g^{(r)}((\mathbf{v}^{( d_{i})})_{i\in\mathcal{I}})\) and \(\boldsymbol{\mathcal{B}}(\bar{g}^{(r)})_{d_{j}:j\in\mathcal{I}^{c}}=\bar{g}^{(r)}(( (\mathbf{v}^{(d_{j})})_{j\in\mathcal{I}^{c}})\) for all \(d_{1},\ldots,d_{N}\in[M]\).

By Equation (26) we have that for any \(d_{1},\ldots,d_{N}\in[M]\):

\[\boldsymbol{\mathcal{B}}(f)_{d_{1},\ldots,d_{N}} =f\big{(}\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{N})}\big{)}\] \[=\sum\nolimits_{r=1}^{R}g^{(r)}\big{(}\big{(}\mathbf{v}^{(d_{i})} \big{)}_{i\in\mathcal{I}}\big{)}\cdot\bar{g}^{(r)}\big{(}\big{(}\mathbf{v}^{(d _{j})}\big{)}_{j\in\mathcal{I}^{c}}\big{)}\] \[=\sum\nolimits_{r=1}^{R}\boldsymbol{\mathcal{B}}\big{(}g^{(r)} \big{)}_{d_{i}:i\in\mathcal{I}}\cdot\boldsymbol{\mathcal{B}}\big{(}\bar{g}^{( r)}\big{)}_{d_{j}:j\in\mathcal{I}^{c}}\,.\]

Denoting by \(\mathbf{u}^{(r)}\in\mathbb{R}^{M^{|\mathcal{I}|}}\) and \(\bar{\mathbf{u}}^{(r)}\in\mathbb{R}^{M^{|\mathcal{I}|}}\) the arrangements of \(\boldsymbol{\mathcal{B}}(g^{(r)})\) and \(\boldsymbol{\mathcal{B}}(\bar{g}^{(r)})\) as vectors, respectively for \(r\in[R]\), this implies that the matricization of \(\boldsymbol{\mathcal{B}}(f)\) with respect to \(\mathcal{I}\) can be written as:

\[\llbracket\boldsymbol{\mathcal{B}}(f);\mathcal{I}\rrbracket=\sum\nolimits_{r= 1}^{R}\mathbf{u}^{(r)}\big{(}\bar{\mathbf{u}}^{(r)}\big{)}^{\top}\,.\]

We have arrived at a representation of \(\llbracket\boldsymbol{\mathcal{B}}(f);\mathcal{I}\rrbracket\) as a sum of \(R\) outer products between two vectors. An outer product of two vectors is a matrix of rank at most one. Consequently, by sub-additivity of rank we conclude: \(\mathrm{rank}\llbracket\boldsymbol{\mathcal{B}}(f);\mathcal{I}\rrbracket\leq R =\mathrm{sep}(f;\mathcal{I})\). 

In the context of graph prediction, let \(\mathcal{C}^{*}\in\operatorname*{argmax}_{\mathcal{C}\in\mathcal{S}( \mathcal{I})}\log(\alpha_{\mathcal{C}})\cdot\rho_{L-1}(\mathcal{C},\mathcal{V})\). By Lemma 3, to prove that Equation (8) holds for weights \(\theta\), it suffices to find template vectors for which \(\log(\mathrm{rank}\llbracket\boldsymbol{\mathcal{B}}(f^{(\theta,\mathcal{G})} );\mathcal{I}\rrbracket)\geq\log(\alpha_{\mathcal{C}^{*}})\cdot\rho_{L-1}( \mathcal{C}^{*},\mathcal{V})\). Notice that, since the outputs of \(f^{(\theta,\mathcal{G})}\) vary polynomially with the weights \(\theta\), so do the entries of \(\llbracket\boldsymbol{\mathcal{B}}\big{(}f^{(\theta,\mathcal{G})}\big{)}; \mathcal{I}\rrbracket\) for any choice of template vectors. Thus, according to Lemma 9, by constructing weights \(\theta\) and template vectors satisfying \(\log(\mathrm{rank}\llbracket\boldsymbol{\mathcal{B}}(f^{(\theta,\mathcal{G})} );\mathcal{I}\rrbracket)\geq\log(\alpha_{\mathcal{C}^{*}})\cdot\rho_{L-1}( \mathcal{C}^{*},\mathcal{V})\), we may conclude that this is the case for almost all assignments of weights, meaning Equation (8) holds for almost all assignments of weights. In Appendix I.3.1 we construct such weights and template vectors.

In the context of vertex prediction, let \(\mathcal{C}^{*}_{t}\in\operatorname*{argmax}_{\mathcal{C}\in\mathcal{S}( \mathcal{I})}\log(\alpha_{\mathcal{C},t})\cdot\rho_{L-1}(\mathcal{C},\{t\})\). Due to arguments analogous to those above, to prove that Equation (9) holds for almost all assignments of weights, we need only find weights \(\theta\) and template vectors satisfying \(\log(\mathrm{rank}\llbracket\boldsymbol{\mathcal{B}}\big{(}f^{(\theta, \mathcal{G},t)}\big{)};\mathcal{I}\rrbracket)\geq\log(\alpha_{\mathcal{C}^{*},t})\cdot\rho_{L-1}(\mathcal{C}^{*}_{t},\{t\})\). In Appendix I.3.2 we do so.

Lastly, recalling that a finite union of measure zero sets has measure zero as well establishes that Equations (8) and (9) jointly hold for almost all assignments of weights. 

#### i.3.1 Weights and Template Vectors Assignment for Graph Prediction (Proof of Equation (8))

We construct weights \(\theta\) and template vectors satisfying \(\log(\mathrm{rank}\llbracket\boldsymbol{\mathcal{B}}\big{(}f^{(\theta, \mathcal{G})}\big{)};\mathcal{I}\rrbracket)\geq\log(\alpha_{\mathcal{C}^{*}})\cdot \rho_{L-1}(\mathcal{C}^{*},\mathcal{V})\), where \(\mathcal{C}^{*}\in\operatorname*{argmax}_{\mathcal{C}\in\mathcal{S}(\mathcal{I })}\log(\alpha_{\mathcal{C}})\cdot\rho_{L-1}(\mathcal{C},\mathcal{V})\).

If \(\rho_{L-1}(\mathcal{C}^{*},\mathcal{V})=0\), then the claim is trivial since there exist weights and template vectors for which \(\llbracket\boldsymbol{\mathcal{B}}\big{(}f^{(\theta,\mathcal{G})}\big{)}; \mathcal{I}\rrbracket\) is not the zero matrix (_e.g._ taking all weight matrices to be zero-padded identity matrices and choosing a single template vector holding one in its first entry and zeros elsewhere).

Now, assuming that \(\rho_{L-1}(\mathcal{C}^{*},\mathcal{V})>0\), which in particular implies that \(\mathcal{I}\neq\emptyset,\mathcal{I}\neq\mathcal{V}\), and \(\mathcal{C}^{*}\neq\emptyset\), we begin with the case of GNN depth \(L=1\), after which we treat the more general \(L\geq 2\) case.

Case of \(L=1\):Consider the weights \(\theta=(\mathbf{W}^{(1)},\mathbf{W}^{(o)})\) given by \(\mathbf{W}^{(1)}:=\mathbf{I}\in\mathbb{R}^{D_{h}\times D_{s}}\) and \(\mathbf{W}^{(o)}:=(1,\ldots,1)\in\mathbb{R}^{1\times D_{h}}\), where \(\mathbf{I}\) is a zero padded identity matrix, _i.e._ it holds ones on its diagonal and zeros elsewhere. We choose template vectors \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(D)}\in\mathbb{R}^{D_{x}}\) such that \(\mathbf{v}^{(m)}\) holds the \(m\)'th standard basis vector of \(\mathbb{R}^{D}\) in its first \(D\) coordinates and zeros in the remaining entries, for \(m\in[D]\) (recall \(D:=\min\{D_{x},D_{h}\}\)). Namely, denote by \(\mathbf{e}^{(1)},\ldots,\mathbf{e}^{(D)}\in\mathbb{R}^{D}\) the standard basis vectors of \(\mathbb{R}^{D}\), _i.e._\(\mathbf{e}^{(m)}_{d}=1\) if \(d=m\) and \(\mathbf{e}^{(m)}_{d}=0\) otherwise for all \(m,d\in[D]\). We let \(\mathbf{v}^{(m)}_{:D}:=\mathbf{e}^{(m)}\) and \(\mathbf{v}^{(m)}_{D+1}:=0\) for all \(m\in[D]\).

We prove that for this choice of weights and template vectors, for all \(d_{1},\ldots,d_{|\mathcal{V}|}\in[D]\):

\[f^{(\theta,\mathcal{G})}\big{(}\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{| \mathcal{V}|})}\big{)}=\begin{cases}1&\text{,if }d_{1}=\cdots=d_{|\mathcal{V}|}\\ 0&\text{,otherwise}\end{cases}\,.\] (27)

[MISSING_PAGE_FAIL:36]

Proof.: Consider the weights \(\theta=(\mathbf{W}^{(1)},\ldots,\mathbf{W}^{(L)},\mathbf{W}^{(o)})\) given by:

\[\mathbf{W}^{(1)} :=\mathbf{I}\in\mathbb{R}^{D_{h}\times D_{\mathrm{\tiny{E}}}}\,,\] \[\mathbf{W}^{(2)} :=\begin{pmatrix}1&1&\cdots&1\\ 0&0&\cdots&0\\ \vdots&\vdots&\cdots&\vdots\\ 0&0&\cdots&0\end{pmatrix}\in\mathbb{R}^{D_{h}\times D_{h}}\,,\] \[\forall l\in\{3,\ldots,L\}:\ \mathbf{W}^{(l)} :=\begin{pmatrix}1&0&\cdots&0\\ 0&0&\cdots&0\\ \vdots&\vdots&\cdots&\vdots\\ 0&0&\cdots&0\end{pmatrix}\in\mathbb{R}^{D_{h}\times D_{h}}\,,\] \[\mathbf{W}^{(o)} :=(1\quad 0\quad\cdots&0)\in\mathbb{R}^{1\times D_{h}}\,,\]

where \(\mathbf{I}\) is a zero padded identity matrix, _i.e._ it holds ones on its diagonal and zeros elsewhere. We define the templates \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\in\mathbb{R}^{D_{x}}\) to be the vectors holding the respective rows of \(\mathbf{Z}\) in their first \(D\) coordinates and zeros in the remaining entries (recall \(D:=\min\{D_{x},D_{h}\}\)). That is, denoting the rows of \(\mathbf{Z}\) by \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\in\mathbb{R}^{D}_{>0}\), we let \(\mathbf{v}^{(m)}_{:D}:=\mathbf{z}^{(m)}\) and \(\mathbf{v}^{(m)}_{D+1:}:=0\) for all \(m\in[M]\). We set all entries of the last template vector to one, _i.e._\(\mathbf{v}^{(M+1)}:=(1,\ldots,1)\in\mathbb{R}^{D_{x}}\).

Since \(\mathcal{C}^{*}\in\mathcal{S}(\mathcal{I})\), _i.e._ it is an admissible subset of \(\mathcal{C}_{\mathcal{I}}\) (Definition 4), there exist \(\mathcal{I}^{\prime}\subseteq\mathcal{I},\mathcal{J}^{\prime}\subseteq \mathcal{I}^{c}\) with no repeating shared neighbors (Definition 3) such that \(\mathcal{C}^{*}=\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{ \prime})\). Notice that \(\mathcal{I}^{\prime}\) and \(\mathcal{J}^{\prime}\) are non-empty as \(\mathcal{C}^{*}\neq\emptyset\) (this is implied by \(\rho_{L-1}(\mathcal{C}^{*},\mathcal{V})>0\)). We focus on the \(M\times M\) sub-matrix of \(\big{[}\boldsymbol{\mathcal{B}}(f^{(\theta,\mathcal{G})});\mathcal{I}\big{]}\) that includes only rows and columns corresponding to evaluations of \(f^{(\theta,\mathcal{G})}\) where all variables indexed by \(\mathcal{I}^{\prime}\) are assigned the same template vector from \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\), all variables indexed by \(\mathcal{J}^{\prime}\) are assigned the same template vector from \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(M)}\), and all remaining variables are assigned the all-ones template vector \(\mathbf{v}^{(M+1)}\). Denoting this sub-matrix by \(\mathbf{U}\in\mathbb{R}^{M\times M}\), it therefore upholds:

\[\mathbf{U}_{m,n}=f^{(\theta,\mathcal{G})}\Big{(}\big{(}\mathbf{x}^{(i)} \leftarrow\mathbf{v}^{(m)}\big{)}_{i\in\mathcal{I}^{\prime}},\big{(}\mathbf{ x}^{(j)}\leftarrow\mathbf{v}^{(n)}\big{)}_{j\in\mathcal{J}^{\prime}},\big{(} \mathbf{x}^{(k)}\leftarrow\mathbf{v}^{(M+1)}\big{)}_{k\in\mathcal{V}\setminus( \mathcal{I}^{\prime}\cup\mathcal{J}^{\prime})}\Big{)}\,,\]

for all \(m,n\in[M]\), where we use \((\mathbf{x}^{(i)}\leftarrow\mathbf{v}^{(m)})_{i\in\mathcal{I}^{\prime}}\) to denote that input variables indexed by \(\mathcal{I}^{\prime}\) are assigned the value \(\mathbf{v}^{(m)}\). To show that \(\mathbf{U}\) obeys the form \(\mathbf{S}(\odot^{\rho_{L-1}(\mathcal{C}^{*},\mathcal{V})}(\mathbf{Z}\mathbf{ Z}^{\top}))\mathbf{Q}\) for full-rank diagonal \(\mathbf{S},\mathbf{Q}\in\mathbb{R}^{M\times M}\), we prove there exist \(\phi,\psi:\mathbb{R}^{D_{x}}\rightarrow\mathbb{R}_{>0}\) such that \(\mathbf{U}_{m,n}=\phi(\mathbf{v}^{(m)})(\mathbf{z}^{(m)},\mathbf{z}^{(m)^{ \rho_{L-1}(\mathcal{C}^{*},\mathcal{V})}}\psi(\mathbf{v}^{(n)})\) for all \(m,n\in[M]\). Indeed, defining \(\mathbf{S}\) to hold \(\phi(\mathbf{v}^{(1)}),\ldots,\phi(\mathbf{v}^{(M)})\) on its diagonal and \(\mathbf{Q}\) to hold \(\psi(\mathbf{v}^{(1)}),\ldots,\psi(\mathbf{v}^{(M)})\) on its diagonal, we have that \(\mathbf{U}=\mathbf{S}(\odot^{\rho_{L-1}(\mathcal{C}^{*},\mathcal{V})}( \mathbf{Z}\mathbf{Z}^{\top}))\mathbf{Q}\). Since \(\mathbf{S}\) and \(\mathbf{Q}\) are clearly full-rank (diagonal matrices with non-zero entries on their diagonal), the proof concludes.

For \(m,n\in[M]\), let \(\mathbf{h}^{(l,i)}\in\mathbb{R}^{D_{h}}\) be the hidden embedding for \(i\in\mathcal{V}\) at layer \(l\in[L]\) of the GNN inducing \(f^{(\theta,\mathcal{G})}\), over the following assignment to its input variables (_i.e._ vertex features):

Invoking Lemma 10 with \(\mathbf{v}^{(m)},\mathbf{v}^{(n)},\mathcal{I}^{\prime}\), and \(\mathcal{J}^{\prime}\), for all \(i\in\mathcal{V}\) it holds that:

for some \(\phi^{(L,i)},\psi^{(L,i)}:\mathbb{R}^{D_{x}}\rightarrow\mathbb{R}_{>0}\). Since

\[\mathbf{U}_{m,n} =f^{(\theta,\mathcal{G})}\Big{(}\big{(}\mathbf{x}^{(i)}\leftarrow \mathbf{v}^{(m)}\big{)}_{i\in\mathcal{I}^{\prime}},\big{(}\mathbf{x}^{(j)} \leftarrow\mathbf{v}^{(n)}\big{)}_{j\in\mathcal{J}^{\prime}},\big{(}\mathbf{x} ^{(k)}\leftarrow\mathbf{v}^{(M+1)}\big{)}_{k\in\mathcal{V}\setminus(\mathcal{I} ^{\prime}\cup\mathcal{J}^{\prime})}\Big{)}\] \[=\mathbf{W}^{(o)}\big{(}\odot_{i\in\mathcal{V}}\mathbf{h}^{(L,i)} \big{)}\]

and \(\mathbf{W}^{(o)}=(1,0,\ldots,0)\), this implies that:

\[\mathbf{U}_{m,n} =\prod\nolimits_{i\in\mathcal{V}}\mathbf{h}^{(L,i)}_{1}\] \[=\prod\nolimits_{i\in\mathcal{V}}\phi^{(L,i)}\big{(}\mathbf{v}^{(m )}\big{)}\big{\langle}\mathbf{z}^{(m)},\mathbf{z}^{(n)}\big{\rangle}^{\rho_{L-1 }(\mathcal{C}^{*},\{i\})}\psi^{(L,i)}\big{(}\mathbf{v}^{(n)}\big{)}\,.\]Rearranging the last term leads to:

\[\mathbf{U}_{m,n}=\left(\prod\nolimits_{i\in\mathcal{V}}\phi^{(L,i)}\big{(} \mathbf{v}^{(m)}\big{)}\right)\cdot\big{\langle}\mathbf{z}^{(m)},\mathbf{z}^{(n )}\big{\rangle}^{\sum_{i\in\mathcal{V}}\rho_{L-1}(\mathcal{C}^{*},\{i\})} \cdot\left(\prod\nolimits_{i\in\mathcal{V}}\psi^{(L,i)}\big{(}\mathbf{v}^{(n) }\big{)}\right).\]

Let \(\phi:\mathbf{v}\mapsto\prod_{i\in\mathcal{V}}\phi^{(L,i)}(\mathbf{v})\) and \(\psi:\mathbf{v}\mapsto\prod_{i\in\mathcal{V}}\psi^{(L,i)}(\mathbf{v})\). Noticing that their range is indeed \(\mathbb{R}_{>0}\) and that \(\sum_{i\in\mathcal{V}}\rho_{L-1}(\mathcal{C}^{*},\{i\})=\rho_{L-1}(\mathcal{C }^{*},\mathcal{V})\) yields the sought-after expression for \(\mathbf{U}_{m,n}\):

\[\mathbf{U}_{m,n}=\phi\big{(}\mathbf{v}^{(m)}\big{)}\big{\langle}\mathbf{z}^{( m)},\mathbf{z}^{(n)}\big{\rangle}^{\rho_{L-1}(\mathcal{C}^{*},\mathcal{V})}\psi \big{(}\mathbf{v}^{(n)}\big{)}\,.\]

#### i.3.2 Weights and Template Vectors Assignment for Vertex Prediction (Proof of Equation (9))

This part of the proof follows a line similar to that of Appendix I.3.1, with differences stemming from the distinction between the operation of a GNN over graph and vertex prediction. Namely, we construct weights \(\theta\) and template vectors satisfying \(\log(\mathrm{rank}\big{[}\!\big{[}\!\big{[}\mathcal{B}\big{(}f^{(\theta, \mathcal{G},t)}\big{)};\mathcal{I}\big{]}\!\big{]})\geq\log(\alpha_{ \mathcal{C}^{*}_{t},t})\cdot\rho_{L-1}(\mathcal{C}^{*}_{t},\{t\})\), where \(\mathcal{C}^{*}_{t}\in\mathrm{argmax}_{\mathcal{C}\in\mathcal{S}(\mathcal{I}) }\log(\alpha_{\mathcal{C},t})\cdot\rho_{L-1}(\mathcal{C},\{t\})\).

If \(\rho_{L-1}(\mathcal{C}^{*}_{t},\{t\})=0\), then the claim is trivial since there exist weights and template vectors for which \(\big{[}\!\big{[}\mathcal{B}\big{(}f^{(\theta,\mathcal{G},t)}\big{)};\mathcal{ I}\big{]}\!\big{]}\) is not the zero matrix (_e.g._ taking all weight matrices to be zero-padded identity matrices and choosing a single template vector holding one in its first entry and zeros elsewhere).

Now, assuming that \(\rho_{L-1}(\mathcal{C}^{*}_{t},\{t\})>0\), which in particular implies that \(\mathcal{I}\neq\emptyset,\mathcal{I}\neq\mathcal{V},\) and \(\mathcal{C}^{*}_{t}\neq\emptyset\), we begin with the case of GNN depth \(L=1\), after which we treat the more general \(L\geq 2\) case.

Case of \(L=1\):Consider the weights \(\theta=(\mathbf{W}^{(1)},\mathbf{W}^{(o)})\) given by \(\mathbf{W}^{(1)}:=\mathbf{I}\in\mathbb{R}^{D_{h}\times D_{x}}\) and \(\mathbf{W}^{(o)}:=(1,\ldots,1)\in\mathbb{R}^{1\times D_{h}}\), where \(\mathbf{I}\) is a zero padded identity matrix, _i.e._ it holds ones on its diagonal and zeros elsewhere. We choose template vectors \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(D)}\in\mathbb{R}^{D_{x}}\) such that \(\mathbf{v}^{(m)}\) holds the \(m\)'th standard basis vector of \(\mathbb{R}^{D}\) in its first \(D\) coordinates and zeros in the remaining entries, for \(m\in[D]\) (recall \(D:=\min\{D_{x},D_{h}\}\)). Namely, denote by \(\mathbf{e}^{(1)},\ldots,\mathbf{e}^{(D)}\in\mathbb{R}^{D}\) the standard basis vectors of \(\mathbb{R}^{D}\), _i.e._\(\mathbf{e}^{(m)}_{d}=1\) if \(d=m\) and \(\mathbf{e}^{(m)}_{d}=0\) otherwise for all \(m,d\in[D]\). We let \(\mathbf{v}^{(m)}_{:D}:=\mathbf{e}^{(m)}\) and \(\mathbf{v}^{(m)}_{D+1}:=0\) for all \(m\in[D]\).

We prove that for this choice of weights and template vectors, for all \(d_{1},\ldots,d_{|\mathcal{V}|}\in[D]\):

\[f^{(\theta,\mathcal{G},t)}\big{(}\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{ |\mathcal{V}|})}\big{)}=\begin{cases}1&\text{,if }d_{j}=d_{j^{\prime}}\text{ for all }j,j^{\prime}\in \mathcal{N}(t)\\ 0&\text{,otherwise}\end{cases}\,.\] (28)

To see it is so, notice that:

\[f^{(\theta,\mathcal{G},t)}\big{(}\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{ |\mathcal{V}|})}\big{)}=\mathbf{W}^{(o)}\mathbf{h}^{(1,t)}=\sum\nolimits_{d=1}^ {D_{h}}\mathbf{h}^{(1,t)}_{d}\,,\]

with \(\mathbf{h}^{(1,t)}=\odot_{j\in\mathcal{N}(t)}(\mathbf{W}^{(1)}\mathbf{v}^{(d_ {j})})=\odot_{j\in\mathcal{N}(t)}(\mathbf{I}\mathbf{v}^{(d_{j})})\). Since \(\mathbf{v}^{(d_{j})}_{:D}=\mathbf{e}^{(d_{j})}\) for all \(j\in\mathcal{N}(t)\) and \(\mathbf{I}\) is a zero-padded \(D\times D\) identity matrix, it holds that:

\[f^{(\theta,\mathcal{G},t)}\big{(}\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{ |\mathcal{V}|})}\big{)}=\sum\nolimits_{d=1}^{D}\prod\nolimits_{j\in\mathcal{N }(t)}\mathbf{e}^{(d_{j})}_{d}\,.\]

For every \(d\in[D]\) we have that \(\prod_{j\in\mathcal{N}(t)}\mathbf{e}^{(d_{j})}_{d}=1\) if \(d_{j}=d\) for all \(j\in\mathcal{N}(t)\) and \(\prod_{j\in\mathcal{N}(t)}\mathbf{e}^{(d_{j})}_{d}=0\) otherwise. This implies that \(f^{(\theta,\mathcal{G},t)}\big{(}\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{ |\mathcal{V}|})}\big{)}=1\) if \(d_{j}=d_{j^{\prime}}\) for all \(j,j^{\prime}\in\mathcal{N}(t)\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{v}^{(d_{1})},\ldots,\mathbf{v}^{(d_{| \mathcal{V}|})})=0\) otherwise, for all \(d_{1},\ldots,d_{|\mathcal{V}|}\in[D]\).

Equation (28) implies that \(\big{[}\!\big{[}\!\big{[}\mathcal{B}\big{(}f^{(\theta,\mathcal{G},t)}\big{)}; \mathcal{I}\!\big{]}\!\big{]}\!\big{]}\) has a sub-matrix of rank \(D\). Specifically, such a sub-matrix can be obtained by examining all rows and columns of \(\big{[}\!\big{[}\!\big{[}\mathcal{B}\big{(}f^{(\theta,\mathcal{G},t)}\big{)}; \mathcal{I}\!\big{]}\!\big{]}\) corresponding to some fixed indices \((d_{i}\in[D])_{i\in\mathcal{V}\setminus\mathcal{N}(t)}\) for the vertices that are not neighbors of \(t\). Thus, \(\mathrm{rank}\big{[}\!\big{[}\mathcal{B}\big{(}f^{(\theta,\mathcal{G},t)}\big{)}; \mathcal{I}\!\big{]}\!\big{]}\geq D\). Notice that necessarily \(\rho_{0}(\mathcal{C}^{*}_{t},\{t\})=1\), as it is not zero and there can only be one length zero walk to \(t\) (the trivial walk that starts and ends at \(t\)). Recalling that \(\alpha_{\mathcal{C}^{*}_{t},t}:=D\) for \(L=1\), we therefore conclude:

\[\log\!\left(\mathrm{rank}\left[\!\big{[}\mathcal{B}\big{(}f^{(\theta,\mathcal{G},t)} \big{)};\mathcal{I}\!\big{]}\!\right)\geq\log(D)=\log(\alpha_{\mathcal{C}^{*}_{t },t})\cdot\rho_{0}(\mathcal{C}^{*}_{t},\{t\})\,.\]

[MISSING_PAGE_EMPTY:39]

[MISSING_PAGE_FAIL:40]

Proof.: We let \(M:=\left(\begin{smallmatrix}D\\ P\end{smallmatrix}\right)\) for notational convenience. Denote by \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\in\mathbb{R}^{D}\) the row vectors of \(\mathbf{Z}\in\mathbb{R}_{>0}^{M\times D}\). Observing the \((m,n)\)'th entry of \(\odot^{P}\big{(}\mathbf{Z}\mathbf{Z}^{\top}\big{)}\):

\[\big{[}\odot^{P}\big{(}\mathbf{Z}\mathbf{Z}^{\top}\big{)}\big{]}_{m,n}=\left\langle \mathbf{z}^{(m)},\mathbf{z}^{(n)}\right\rangle^{P}=\left(\sum\nolimits_{d=1}^{ D}\mathbf{z}_{d}^{(m)}\cdot\mathbf{z}_{d}^{(n)}\right)^{P},\]

by expanding the power using the multinomial identity we have that:

\[\begin{split}\big{[}\odot^{P}\big{(}\mathbf{Z}\mathbf{Z}^{\top }\big{)}\big{]}_{m,n}&=\sum_{\begin{subarray}{c}q_{1},\ldots,q_{D }\in\mathbb{N}_{\geq 0}\\ s\text{t. }\sum_{d=1}^{D}q_{d}=P\end{subarray}}\binom{P}{q_{1},\ldots,q_{D }}\prod_{d=1}^{D}\left(\mathbf{z}_{d}^{(m)}\cdot\mathbf{z}_{d}^{(n)}\right)^{ q_{d}}\\ &=\sum_{\begin{subarray}{c}q_{1},\ldots,q_{D}\in\mathbb{N}_{\geq 0 }\\ s\text{t. }\sum_{d=1}^{D}q_{d}=P\end{subarray}}\binom{P}{q_{1},\ldots,q_{D }}\left(\prod_{d=1}^{D}\left(\mathbf{z}_{d}^{(m)}\right)^{q_{d}}\right)\cdot \left(\prod_{d=1}^{D}\left(\mathbf{z}_{d}^{(n)}\right)^{q_{d}}\right),\end{split}\] (29)

where in the last equality we separated terms depending on \(m\) from those depending on \(n\).

Let \(\left(\mathbf{a}^{(q_{1},\ldots,q_{D})}\in\mathbb{R}^{M}\right)_{q_{1},\ldots,q_{D}\in\mathbb{N}_{\geq 0}\text{ st. }\sum_{d=1}^{D}q_{d}=P}\) be \(M\) vectors defined by \(\mathbf{a}_{m}^{(q_{1},\ldots,q_{D})}=\prod_{d=1}^{D}\left(\mathbf{z}_{d}^{(m) }\right)^{q_{d}}\) for all \(q_{1},\ldots,q_{D}\in\mathbb{N}_{\geq 0}\) satisfying \(\sum_{d=1}^{D}q_{d}=P\) and \(m\in[M]\). As can be seen from Equation (29), we can write:

\[\odot^{P}\big{(}\mathbf{Z}\mathbf{Z}^{\top}\big{)}=\mathbf{A}\mathbf{S} \mathbf{A}^{\top}\,,\]

where \(\mathbf{A}\in\mathbb{R}^{M\times M}\) is the matrix whose columns are \(\left(\mathbf{a}^{(q_{1},\ldots,q_{D})}\right)_{q_{1},\ldots,q_{D}\in\mathbb{N }_{\geq 0}\text{ st. }\sum_{d=1}^{D}q_{d}=P}\) and \(\mathbf{S}\in\mathbb{R}^{M\times M}\) is the diagonal matrix holding \(\binom{P}{q_{1},\ldots,q_{D}}\) for every \(q_{1},\ldots,q_{D}\in\mathbb{N}_{\geq 0}\) satisfying \(\sum_{d=1}^{D}q_{d}=P\) on its diagonal. Since all entries on the diagonal of \(\mathbf{S}\) are positive, it is of full-rank, _i.e._\(\mathrm{rank}(\mathbf{S})=M\). Thus, to prove that there exists \(\mathbf{Z}\in\mathbb{R}_{>0}^{M\times D}\) for which \(\mathrm{rank}(\odot^{P}(\mathbf{Z}\mathbf{Z}^{\top}))=M\), it suffices to show that we can choose \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\) with positive entries inducing \(\mathrm{rank}(\mathbf{A})=M\), for \(\mathbf{A}\) as defined above. Below, we complete the proof by constructing such \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\).

We associate each of \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\) with a different configuration from the set:

\[\left\{\mathbf{q}=(q_{1},\ldots,q_{D}):q_{1},\ldots,q_{D}\in\mathbb{N}_{\geq 0 }\;,\;\;\sum\nolimits_{d=1}^{D}q_{d}=P\right\},\]

where note that this set contains \(M=\left(\begin{smallmatrix}D\\ P\end{smallmatrix}\right)\) elements. For \(m\in[M]\), denote by \(\mathbf{q}^{(m)}\) the configuration associated with \(\mathbf{z}^{(m)}\). For a variable \(\gamma\in\mathbb{R}\), to be determined later on, and every \(m\in[M]\) and \(d\in[D]\), we set:

\[\mathbf{z}_{d}^{(m)}=\gamma^{\mathbf{q}_{d}^{(m)}}\,.\]

Given these \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\), the entries of \(\mathbf{A}\) have the following form:

\[\mathbf{A}_{m,n}=\prod\nolimits_{d=1}^{D}\left(\mathbf{z}_{d}^{(m)}\right)^{ \mathbf{q}_{d}^{(n)}}=\prod\nolimits_{d=1}^{D}\left(\gamma^{\mathbf{q}_{d}^{(m )}}\right)^{\mathbf{q}_{d}^{(n)}}=\gamma^{\sum_{d=1}^{D}\mathbf{q}_{d}^{(m)}} \cdot\mathbf{q}_{d}^{(n)}=\gamma^{\langle\mathbf{q}^{(m)},\mathbf{q}^{(n)} \rangle}\,,\]

for all \(m,n\in[M]\). Thus, \(\det(\mathbf{A})=\sum_{\text{permutation }\sigma:[M]\rightarrow[M]}\mathrm{sign}(\sigma)\cdot \gamma^{\sum_{m=1}^{M}\langle\mathbf{q}^{(m)},\mathbf{q}^{\langle\sigma(m) \rangle}\rangle}\) is polynomial in \(\gamma\). By Lemma 6, \(\sum_{m=1}^{M}\left\langle\mathbf{q}^{(m)},\mathbf{q}^{\langle\sigma(m) \rangle}\right\rangle<\sum_{m=1}^{M}\left\lVert\mathbf{q}^{(m)}\right\rVert^{2}\) for all \(\sigma\) which is not the identity permutation. This implies that \(\sum_{m=1}^{M}\left\lVert\mathbf{q}^{(m)}\right\rVert^{2}\) is the maximal degree of a monomial in \(\det(\mathbf{A})\), and it is attained by a single element in \(\sum_{\text{permutation }\sigma:[M]\rightarrow[M]}\mathrm{sign}(\sigma)\cdot\gamma^{\sum_{m=1}^{M} \langle\mathbf{q}^{(m)},\mathbf{q}^{\langle\sigma(m)\rangle}\rangle}\) -- that corresponding to the identity permutation. Consequently, \(\det(\mathbf{A})\) cannot be the zero polynomial with respect to \(\gamma\), and so it vanishes only on a finite set of values for \(\gamma\). In particular, there exists \(\gamma>0\) such that \(\det(\mathbf{A})\neq 0\), meaning \(\mathrm{rank}(\mathbf{A})=M\). The proof concludes by noticing that for a positive \(\gamma\) the entries of the chosen \(\mathbf{z}^{(1)},\ldots,\mathbf{z}^{(M)}\) are positive as well. 

Additionally, we make use of the following lemmas.

**Lemma 8**.: _For any \(D,P\in\mathbb{N}\), let \(\left(\!\!\left(\!\begin{array}{c}D\\ P\end{array}\!\right)\!\right):=\binom{D+P-1}{P}\) be the multiset coefficient. Then:_

\[\left(\!\!\left(\!\begin{array}{c}D\\ P\end{array}\!\right)\!\right)\geq\left(\frac{D-1}{P}+1\right)^{P}.\]

Proof.: For any \(N\geq K\in\mathbb{N}\), a known lower bound on the binomial coefficient is \(\binom{N}{K}\geq\left(\frac{N}{K}\right)^{K}\). Hence:

\[\left(\!\!\left(\!\begin{array}{c}D\\ P\end{array}\!\right)\!\right)=\binom{D+P-1}{P}\geq\left(\frac{D+P-1}{P} \right)^{P}=\left(\frac{D-1}{P}+1\right)^{P}.\]

**Lemma 9**.: _For \(D_{1},D_{2},K\in\mathbb{N}\), consider a polynomial function mapping variables \(\theta\in\mathbb{R}^{K}\) to matrices \(\mathbf{A}(\theta)\in\mathbb{R}^{D_{1}\times D_{2}}\), i.e. the entries of \(\mathbf{A}(\theta)\) are polynomial in \(\theta\). If there exists a point \(\theta^{*}\in\mathbb{R}^{K}\) such that \(\mathrm{rank}(\mathbf{A}(\theta^{*}))\geq R\), for \(R\in[\min\{D_{1},D_{2}\}]\), then the set \(\{\theta\in\mathbb{R}^{K}:\mathrm{rank}(\mathbf{A}(\theta))<R\}\) has Lebesgue measure zero._

Proof.: A matrix is of rank at least \(R\) if and only if it has a \(R\times R\) sub-matrix whose determinant is non-zero. The determinant of any sub-matrix of \(\mathbf{A}(\theta)\) is polynomial in the entries of \(\mathbf{A}(\theta)\), and so it is polynomial in \(\theta\) as well. Since the zero set of a polynomial is either the entire space or a set of Lebesgue measure zero [19], the fact that \(\mathrm{rank}(\mathbf{A}(\theta^{*}))\geq R\) implies that \(\{\theta\in\mathbb{R}^{K}:\mathrm{rank}(\mathbf{A}(\theta))<R\}\) has Lebesgue measure zero. 

**Lemma 10**.: _Let \(\mathbf{v},\mathbf{v}^{\prime}\in\mathbb{R}^{D_{x}}_{\geq 0}\) whose first \(D:=\min\{D_{x},D_{h}\}\) entries are positive, and disjoint \(\mathcal{I}^{\prime},\mathcal{J}^{\prime}\subseteq\mathcal{V}\) with no repeating shared neighbors (Definition 3). Denote by \(\mathbf{h}^{(l,i)}\in\mathbb{R}^{D_{h}}\) the hidden embedding for \(i\in\mathcal{V}\) at layer \(l\in[L]\) of a GNN with depth \(L\geq 2\) and product aggregation (Equations (2) and (5)), given the following assignment to its input variables (i.e. vertex features):_

\[\left(\mathbf{x}^{(i)}\leftarrow\mathbf{v}\right)_{i\in\mathcal{I}^{\prime}},\left(\mathbf{x}^{(j)}\leftarrow\mathbf{v}^{\prime}\right)_{j\in\mathcal{J}^ {\prime}},\left(\mathbf{x}^{(k)}\leftarrow\mathbf{1}\right)_{k\in\mathcal{V} \setminus(\mathcal{I}^{\prime}\cup\mathcal{J}^{\prime})},\]

_where \(\mathbf{1}\in\mathbb{R}^{D_{x}}\) is the vector holding one in all entries. Suppose that the weights \(\mathbf{W}^{(1)},\ldots,\mathbf{W}^{(L)}\) of the GNN are given by:_

\[\mathbf{W}^{(1)} :=\mathbf{I}\in\mathbb{R}^{D_{h}\times D_{x}}\,,\] \[\mathbf{W}^{(2)} :=\begin{pmatrix}1&1&\cdots&1\\ 0&0&\cdots&0\\ \vdots&\vdots&\ldots&\vdots\\ 0&0&\cdots&0\end{pmatrix}\in\mathbb{R}^{D_{h}\times D_{h}}\,,\] \[\forall l\in\{3,\ldots,L\}:\ \mathbf{W}^{(l)} :=\begin{pmatrix}1&0&\cdots&0\\ 0&0&\cdots&0\\ \vdots&\vdots&\ldots&\vdots\\ 0&0&\cdots&0\end{pmatrix}\in\mathbb{R}^{D_{h}\times D_{h}}\,,\]

_where \(\mathbf{I}\) is a zero padded identity matrix, i.e. it holds ones on its diagonal and zeros elsewhere. Then, for all \(l\in\{2,\ldots,L\}\) and \(i\in\mathcal{V}\), there exist \(\phi^{(l,i)},\psi^{(l,i)}:\mathbb{R}^{D_{x}}\rightarrow\mathbb{R}_{>0}\) such that:_

\[\mathbf{h}_{1}^{(l,i)}=\phi^{(l,i)}(\mathbf{v})\left\langle\mathbf{v}_{:D}, \mathbf{v}_{:D}^{\prime}\right\rangle^{\rho_{l-1}(\mathcal{C},\{i\})}\psi^{ (l,i)}(\mathbf{v}^{\prime})\quad,\quad\forall d\in\{2,\ldots,D_{h}\}:\ \mathbf{h}_{d}^{(l,i)}=0\,,\]

_where \(\mathcal{C}:=\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{ \prime})\)._

Proof.: The proof is by induction over the layer \(l\in\{2,\ldots,L\}\). For \(l=2\), fix \(i\in\mathcal{V}\). By the update rule of a GNN with product aggregation:

\[\mathbf{h}^{(2,i)}=\odot_{j\in\mathcal{N}(i)}\left(\mathbf{W}^{(2)}\mathbf{h} ^{(1,j)}\right).\]

Plugging in the value of \(\mathbf{W}^{(2)}\) we get:

\[\mathbf{h}_{1}^{(2,i)}=\prod_{j\in\mathcal{N}(i)}\left(\sum\nolimits_{d=1}^{D _{h}}\mathbf{h}_{d}^{(1,j)}\right)\quad,\quad\forall d\in\{2,\ldots,D_{h}\}:\ \mathbf{h}_{d}^{(2,i)}=0\,.\] (30)Let \(\bar{\mathbf{v}},\bar{\mathbf{v}}^{\prime}\in\mathbb{R}^{D_{h}}\) be the vectors holding \(\mathbf{v}_{:D}\) and \(\mathbf{v}^{\prime}_{:D}\) in their first \(D\) coordinates and zero in the remaining entries, respectively. Similarly, we use \(\bar{\mathbf{I}}\in\mathbb{R}^{D_{h}}\) to denote the vector whose first \(D\) entries are one and the remaining are zero. Examining \(\mathbf{h}^{(1,j)}\) for \(j\in\mathcal{N}(i)\), by the assignment of input variables and the fact that \(\mathbf{W}^{(1)}\) is a zero padded identity matrix we have that:

\[\mathbf{h}^{(1,j)} =\odot_{k\in\mathcal{N}(j)}\big{(}\mathbf{W}^{(1)}\mathbf{x}^{( k)}\big{)}=\big{(}\odot^{|\mathcal{N}(j)\cap\mathcal{I}^{\prime}|}\bar{ \mathbf{v}}\big{)}\odot\big{(}\odot^{|\mathcal{N}(j)\cap\mathcal{J}^{\prime}| }\bar{\mathbf{v}}^{\prime}\big{)}\odot\big{(}\odot^{|\mathcal{N}(j)\setminus (\mathcal{I}^{\prime}\cup\mathcal{J}^{\prime})|}\bar{\mathbf{I}}\big{)}\] \[=\big{(}\odot^{|\mathcal{N}(j)\cap\mathcal{I}^{\prime}|}\bar{ \mathbf{v}}\big{)}\odot\big{(}\odot^{|\mathcal{N}(j)\cap\mathcal{J}^{\prime}|} \bar{\mathbf{v}}^{\prime}\big{)}\,.\]

Since the first \(D\) entries of \(\bar{\mathbf{v}}\) and \(\bar{\mathbf{v}}^{\prime}\) are positive while the rest are zero, the same holds for \(\mathbf{h}^{(1,j)}\). Additionally, recall that \(\mathcal{I}^{\prime}\) and \(\mathcal{J}^{\prime}\) have no repeating shared neighbors. Thus, if \(j\in\mathcal{N}(\mathcal{I}^{\prime})\cap\mathcal{N}(\mathcal{J}^{\prime})= \mathcal{C}\), then \(j\) has a single neighbor in \(\mathcal{I}^{\prime}\) and a single neighbor in \(\mathcal{J}^{\prime}\), implying \(\mathbf{h}^{(1,j)}=\bar{\mathbf{v}}\odot\bar{\mathbf{v}}^{\prime}\). Otherwise, if \(j\notin\mathcal{C}\), then \(\mathcal{N}(j)\cap\mathcal{I}^{\prime}=\emptyset\) or \(\mathcal{N}(j)\cap\mathcal{J}^{\prime}=\emptyset\) must hold. In the former \(\mathbf{h}^{(1,j)}\) does not depend on \(\mathbf{v}\), whereas in the latter \(\mathbf{h}^{(1,j)}\) does not depend on \(\mathbf{v}^{\prime}\).

Going back to Equation (30), while noticing that \(|\mathcal{N}(i)\cap\mathcal{C}|=\rho_{1}(\mathcal{C},\{i\})\), we arrive at:

\[\mathbf{h}^{(2,i)}_{1} =\prod\nolimits_{j\in\mathcal{N}(i)\cap\mathcal{C}}\Big{(}\sum \nolimits_{d=1}^{D_{h}}\mathbf{h}^{(1,j)}_{d}\Big{)}\cdot\prod\nolimits_{j\in \mathcal{N}(i)\setminus\mathcal{C}}\Big{(}\sum\nolimits_{d=1}^{D_{h}}\mathbf{ h}^{(1,j)}_{d}\Big{)}\] \[=\prod\nolimits_{j\in\mathcal{N}(i)\cap\mathcal{C}}\Big{(}\sum \nolimits_{d=1}^{D_{h}}\big{[}\bar{\mathbf{v}}\odot\bar{\mathbf{v}}^{\prime} \big{]}_{d}\Big{)}\cdot\prod\nolimits_{j\in\mathcal{N}(i)\setminus\mathcal{C}} \Big{(}\sum\nolimits_{d=1}^{D_{h}}\mathbf{h}^{(1,j)}_{d}\Big{)}\] \[=\langle\mathbf{v}_{:D},\mathbf{v}^{\prime}_{:D}\rangle^{\rho_{1} (\mathcal{C},\{i\})}\cdot\prod\nolimits_{j\in\mathcal{N}(i)\setminus\mathcal{C }}\Big{(}\sum\nolimits_{d=1}^{D_{h}}\mathbf{h}^{(1,j)}_{d}\Big{)}\,.\]

As discussed above, for each \(j\in\mathcal{N}(i)\setminus\mathcal{C}\) the hidden embedding \(\mathbf{h}^{(1,j)}\) does not depend on \(\mathbf{v}\) or it does not depend on \(\mathbf{v}^{\prime}\). Furthermore, \(\sum\nolimits_{d=1}^{D_{h}}\mathbf{h}^{(1,j)}_{d}>0\) for all \(j\in\mathcal{N}(i)\). Hence, there exist \(\phi^{(2,i)},\psi^{(2,i)}:\mathbb{R}^{D_{x}}\to\mathbb{R}_{>0}\) such that:

\[\mathbf{h}^{(2,i)}_{1}=\phi^{(2,i)}(\mathbf{v})\,\langle\mathbf{v}_{:D}, \mathbf{v}^{\prime}_{:D}\rangle^{\rho_{1}(\mathcal{C},\{i\})}\,\psi^{(2,i)}( \mathbf{v}^{\prime})\,,\]

completing the base case.

Now, assuming that the inductive claim holds for \(l-1\geq 2\), we prove that it holds for \(l\). Let \(i\in\mathcal{V}\). By the update rule of a GNN with product aggregation \(\mathbf{h}^{(l,i)}=\odot_{j\in\mathcal{N}(i)}(\mathbf{W}^{(l)}\mathbf{h}^{(l-1,j)})\). Plugging in the value of \(\mathbf{W}^{(l)}\) we get:

\[\mathbf{h}^{(l,i)}_{1}=\prod\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}^{(l-1,j)}_ {1}\quad,\quad\forall d\in\{2,\dots,D_{h}\}:\,\mathbf{h}^{(l,i)}_{d}=0\,.\]

By the inductive assumption \(\mathbf{h}^{(l-1,j)}_{1}=\phi^{(l-1,j)}(\mathbf{v})\,\langle\mathbf{v}_{:D}, \mathbf{v}^{\prime}_{:D}\rangle^{\rho_{l-2}(\mathcal{C},\{j\})}\,\psi^{(l-1,j) }(\mathbf{v}^{\prime})\) for all \(j\in\mathcal{N}(i)\), where \(\phi^{(l-1,j)},\psi^{(l-1,j)}:\mathbb{R}^{D_{x}}\to\mathbb{R}_{>0}\). Thus:

\[\mathbf{h}^{(l,i)}_{1} =\prod\nolimits_{j\in\mathcal{N}(i)}\mathbf{h}^{(l-1,j)}_{1}\] \[=\prod\nolimits_{j\in\mathcal{N}(i)}\phi^{(l-1,j)}(\mathbf{v}) \,\langle\mathbf{v}_{:D},\mathbf{v}^{\prime}_{:D}\rangle^{\rho_{l-2}(\mathcal{C}, \{j\})}\,\psi^{(l-1,j)}(\mathbf{v}^{\prime})\] \[=\left(\prod\nolimits_{j\in\mathcal{N}(i)}\phi^{(l-1,j)}(\mathbf{v })\right)\cdot(\mathbf{v}_{:D},\mathbf{v}^{\prime}_{:D})^{\sum\nolimits_{j\in \mathcal{N}(i)}\rho_{l-2}(\mathcal{C},\{j\})}\cdot\left(\prod\nolimits_{j\in \mathcal{N}(i)}\psi^{(l-1,j)}(\mathbf{v}^{\prime})\right).\]

Define \(\phi^{(l,i)}:\mathbf{v}\mapsto\prod\nolimits_{j\in\mathcal{N}(i)}\phi^{(l-1,j)}( \mathbf{v})\) and \(\psi^{(l,i)}:\mathbf{v}^{\prime}\mapsto\prod\nolimits_{j\in\mathcal{N}(i)}\psi^{( l-1,j)}(\mathbf{v}^{\prime})\). Since the range of \(\phi^{(l-1,j)}\) and \(\psi^{(l-1,j)}\) is \(\mathbb{R}_{>0}\) for all \(j\in\mathcal{N}(i)\), so is the range of \(\phi^{(l,i)}\) and \(\psi^{(l,i)}\). The desired result thus readily follows by noticing that \(\sum\nolimits_{j\in\mathcal{N}(i)}\rho_{l-2}(\mathcal{C},\{j\})=\rho_{l-1}( \mathcal{C},\{i\})\):

\[\mathbf{h}^{(l,i)}_{1}=\phi^{(l,i)}(\mathbf{v})\,\langle\mathbf{v}_{:D}, \mathbf{v}^{\prime}_{:D}\rangle^{\rho_{l-1}(\mathcal{C},\{i\})}\,\psi^{(l,i)}( \mathbf{v}^{\prime})\,.\]

### Proof of Theorem 4

The proof follows a line identical to that of Theorem 2 (Appendix I.2), requiring only slight adjustments. We outline the necessary changes.

Extending the tensor network representations of GNNs with product aggregation to directed graphs and multiple edge types is straightforward. Nodes, legs, and leg weights are as described in Appendix E for undirected graphs with a single edge type, except that:* Legs connecting \(\delta\)-tensors with weight matrices in the same layer are adapted such that only incoming neighbors are considered. Formally, in Equations (15) to (20), \(\mathcal{N}(i)\) is replaced by \(\mathcal{N}_{in}(i)\) in the leg definitions, for \(i\in\mathcal{V}\).
* Weight matrices \(\left(\mathbf{W}^{(l,q)}\right)_{l\in[L],q\in[Q]}\) are assigned to nodes in accordance with edge types. Namely, if at layer \(l\in[L]\) a \(\delta\)-tensor associated with \(i\in\mathcal{V}\) is connected to a weight matrix associated with \(j\in\mathcal{N}_{in}(i)\), then \(\mathbf{W}^{(l,\tau(j,i))}\) is assigned to the weight matrix node, as opposed to \(\mathbf{W}^{(l)}\) in the single edge type setting. Formally, let \(\mathbf{W}^{(l,j,\gamma)}\) be a node at layer \(l\in[L]\) connected to \(\boldsymbol{\delta}^{(l,i,\gamma^{\prime})}\), for \(i\in\mathcal{V},j\in\mathcal{N}_{in}(i)\), and some \(\gamma,\gamma^{\prime}\in\mathbb{N}\). Then, \(\mathbf{W}^{(l,j,\gamma)}\) stands for a copy of \(\mathbf{W}^{(l,\tau(j,i))}\).

For \(\mathbf{X}=(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)})\in\mathbb{R}^ {D_{x}\times|\mathcal{V}|}\), let \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\) be the tensor networks corresponding to \(f^{(\theta,\mathcal{G})}(\mathbf{X})\) and \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\), respectively, whose construction is outlined above. Then, Lemma 1 (from Appendix I.2.1) and its proof apply as stated. Meaning, \(\mathrm{sep}(f^{(\theta,\mathcal{G})};\mathcal{I})\) and \(\mathrm{sep}(f^{(\theta,\mathcal{G},t)};\mathcal{I})\) are upper bounded by the minimal modified multiplicative cut weights in \(\mathcal{T}(\mathbf{X})\) and \(\mathcal{T}^{(t)}(\mathbf{X})\), respectively, among cuts separating leaves associated with vertices of the input graph in \(\mathcal{I}\) from leaves associated with vertices of the input graph in \(\mathcal{I}^{c}\). Therefore, to establish Equations (11) and (12), it suffices to find cuts in the respective tensor networks with sufficiently low modified multiplicative weights. As is the case for undirected graphs with a single edge type (see Appendices I.2.2 and I.2.3), the cuts separating nodes corresponding to vertices in \(\mathcal{I}\) from all other nodes yield the desired upper bounds. 

### Proof of Theorem 5

The proof follows a line identical to that of Theorem 3 (Appendix I.3), requiring only slight adjustments. We outline the necessary changes.

In the context of graph prediction, let \(\mathcal{C}^{*}\in\operatorname*{argmax}_{\mathcal{C}\in\mathcal{S}^{-}( \mathcal{I})}\log(\alpha_{\mathcal{C}})\cdot\rho_{L-1}(\mathcal{C},\mathcal{V})\). By Lemma 3 (from Appendix I.3), to prove that Equation (13) holds for weights \(\theta\), it suffices to find template vectors for which \(\log(\mathrm{rank}\big{[}\boldsymbol{\mathcal{B}}\big{(}f^{(\theta,\mathcal{ G})}\big{)};\mathcal{I}\big{]})\geq\log(\alpha_{\mathcal{C}^{*}})\cdot\rho_{L-1}( \mathcal{C}^{*},\mathcal{V})\). Notice that, since the outputs of \(f^{(\theta,\mathcal{G})}\) vary polynomially with \(\theta\), so do the entries of \(\big{[}\boldsymbol{\mathcal{B}}\big{(}f^{(\theta,\mathcal{G})}\big{)};\mathcal{ I}\big{]}\) for any choice of template vectors. Thus, according to Lemma 9 (from Appendix I.3.3), by constructing weights \(\theta\) and template vectors satisfying \(\log(\mathrm{rank}\big{[}\boldsymbol{\mathcal{B}}\big{(}f^{(\theta,\mathcal{ G})};\mathcal{I}\big{]})\geq\log(\alpha_{\mathcal{C}^{*}})\cdot\rho_{L-1}( \mathcal{C}^{*},\mathcal{V})\), we may conclude that this is the case for almost all assignments of weights, meaning Equation (13) holds for almost all assignments of weights. For undirected graphs with a single edge type, Appendix I.3.1 provides such weights \(\mathbf{W}^{(1)},\dots,\mathbf{W}^{(L)},\mathbf{W}^{(o)}\) and template vectors. The proof in the case of directed graphs with multiple edge types is analogous, requiring only a couple adaptations: _(i)_ weight matrices of all edge types at layer \(l\in[L]\) are set to the \(\mathbf{W}^{(l)}\) chosen in Appendix I.3.1; and _(ii)_\(\mathcal{C}_{\mathcal{I}}\) and \(\mathcal{S}(\mathcal{I})\) are replaced with their directed counterparts \(\mathcal{C}^{\to}_{\mathcal{I}}\) and \(\mathcal{S}^{\to}(\mathcal{I})\), respectively.

In the context of vertex prediction, let \(\mathcal{C}^{*}_{t}\in\operatorname*{argmax}_{\mathcal{C}\in\mathcal{S}^{-}( \mathcal{I})}\log(\alpha_{\mathcal{C},t})\cdot\rho_{L-1}(\mathcal{C},\{t\})\). Due to arguments similar to those above, to prove that Equation (14) holds for almost all assignments of weights, we need only find weights \(\theta\) and template vectors satisfying \(\log(\mathrm{rank}\big{[}\boldsymbol{\mathcal{B}}\big{(}f^{(\theta,\mathcal{ G},t)}\big{)};\mathcal{I}\big{]})\geq\log(\alpha_{\mathcal{C}^{*}_{t},t})\cdot\rho_{L-1}( \mathcal{C}^{*}_{t},\{t\})\). For undirected graphs with a single edge type, Appendix I.3.2 provides such weights and template vectors. The adaptations necessary to extend Appendix I.3.2 to directed graphs with multiple edge types are identical to those specified above for extending Appendix I.3.1 in the context of graph prediction.

Lastly, recalling that a finite union of measure zero sets has measure zero as well establishes that Equations (13) and (14) jointly hold for almost all assignments of weights. 

### Proof of Proposition 1

We first prove that the contractions described by \(\mathcal{T}(\mathbf{X})\) produce \(f^{(\theta,\mathcal{G})}(\mathbf{X})\). Through an induction over the layer \(l\in[L]\), for all \(i\in\mathcal{V}\) and \(\gamma\in[\rho_{L-l}(\{i\},\mathcal{V})]\) we show that contracting the sub-tree whose root is \(\boldsymbol{\delta}^{(l,i,\gamma)}\) yields \(\mathbf{h}^{(l,i)}\) -- the hidden embedding for \(i\) at layer \(l\) of the GNN inducing \(f^{(\theta,\mathcal{G})}\), given vertex features \(\mathbf{x}^{(1)},\dots,\mathbf{x}^{(|\mathcal{V}|)}\).

For \(l=1\), fix some \(i\in\mathcal{V}\) and \(\gamma\in[\rho_{L-1}(\{i\},\mathcal{V})]\). The sub-tree whose root is \(\boldsymbol{\delta}^{(1,i,\gamma)}\) comprises \(|\mathcal{N}(i)|\) copies of \(\mathbf{W}^{(1)}\), each associated with some \(j\in\mathcal{N}(i)\) and contracted in its second mode with a copy of \(\mathbf{x}^{(j)}\). Additionally, \(\boldsymbol{\delta}^{(1,i,\gamma)}\), which is a copy of \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}\), is contracted with the copies of \(\mathbf{W}^{(1)}\) in their first mode. Overall, the execution of all contractions in the sub-tree can be written as \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}*_{j\in|\mathcal{N}(i)|}(\mathbf{W} ^{(1)}\mathbf{x}^{(\mathcal{N}(i)_{j})})\), where \(\mathcal{N}(i)_{j}\), for \(j\in[|\mathcal{N}(i)|]\), denotes the \(j\)'th neighbor of \(i\) according to an ascending order (recall vertices are represented by indices from \(1\) to \(|\mathcal{V}|\)). The base case concludes by Lemma 11:

\[\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}*_{j\in[|\mathcal{N}(i)|]}\left( \mathbf{W}^{(1)}\mathbf{x}^{(\mathcal{N}(i)_{j})}\right)=\odot_{j\in[| \mathcal{N}(i)|]}\Big{(}\mathbf{W}^{(1)}\mathbf{x}^{(\mathcal{N}(i)_{j})} \Big{)}=\mathbf{h}^{(1,i)}\,.\]

Assuming that the inductive claim holds for \(l-1\geq 1\), we prove that it holds for \(l\). Let \(i\in\mathcal{V}\) and \(\gamma\in[\rho_{L-l}(\{i\},\mathcal{V})]\). The children of \(\boldsymbol{\delta}^{(l,i,\gamma)}\) in the tensor network are of the form \(\mathbf{W}^{(l,\mathcal{N}(i)_{j},\phi_{i,l,j}(\gamma))}\), for \(j\in[|\mathcal{N}(i)|]\), and each \(\mathbf{W}^{(l,\mathcal{N}(i)_{j},\phi_{i,l,j}(\gamma))}\) is connected in its other mode to \(\boldsymbol{\delta}^{(l-1,\mathcal{N}(i)_{j},\phi_{i,l,j}(\gamma))}\). By the inductive assumption for \(l-1\), we know that performing all contractions in the sub-tree whose root is \(\boldsymbol{\delta}^{(l-1,\mathcal{N}(i)_{j},\phi_{i,l,j}(\gamma))}\) produces \(\mathbf{h}^{(l-1,\mathcal{N}(i)_{j})}\), for all \(j\in[|\mathcal{N}(i)|]\). Since \(\boldsymbol{\delta}^{(l,i,\gamma)}\) is a copy of \(\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}\), and each \(\mathbf{W}^{(l,\mathcal{N}(i)_{j},\phi_{l,i,j}(\gamma))}\) is a copy of \(\mathbf{W}^{(l)}\), the remaining contractions in the sub-tree of \(\boldsymbol{\delta}^{(l,i,\gamma)}\) thus give:

\[\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}*_{j\in[|\mathcal{N}(i)|]}\left( \mathbf{W}^{(l)}\mathbf{h}^{(l-1,\mathcal{N}(i)_{j})}\right),\]

which according to Lemma 11 amounts to:

\[\boldsymbol{\delta}^{(|\mathcal{N}(i)|+1)}*_{j\in[|\mathcal{N}(i)|]}\left( \mathbf{W}^{(l)}\mathbf{h}^{(l-1,\mathcal{N}(i)_{j})}\right)=\odot_{j\in[| \mathcal{N}(i)|]}\Big{(}\mathbf{W}^{(l)}\mathbf{h}^{(l-1,\mathcal{N}(i)_{j})} \Big{)}=\mathbf{h}^{(l,i)}\,,\]

establishing the induction step.

With the inductive claim at hand, we show that contracting \(\mathcal{T}(\mathbf{X})\) produces \(f^{(\theta,\mathcal{G})}(\mathbf{X})\). Applying the inductive claim for \(l=L\), we have that \(\mathbf{h}^{(L,1)},\ldots,\mathbf{h}^{(L,|\mathcal{V}|)}\) are the vectors produced by executing all contractions in the sub-trees whose roots are \(\boldsymbol{\delta}^{(L,1,1)},\ldots,\boldsymbol{\delta}^{(L,|\mathcal{V}|,1)}\), respectively. Performing the remaining contractions, defined by the legs of \(\boldsymbol{\delta}^{(|\mathcal{V}|+1)}\), therefore yields \(\mathbf{W}^{(o)}\big{(}\boldsymbol{\delta}^{(|\mathcal{V}|+1)}*_{i\in[| \mathcal{V}|]}\mathbf{h}^{(L,i)}\big{)}\). By Lemma 11:

\[\boldsymbol{\delta}^{(|\mathcal{V}|+1)}*_{i\in[|\mathcal{V}|]}\mathbf{h}^{(L,i )}=\odot_{i\in[|\mathcal{V}|]}\mathbf{h}^{(L,i)}\,.\]

Hence, \(\mathbf{W}^{(o)}\big{(}\boldsymbol{\delta}^{(|\mathcal{V}|+1)}*_{i\in[| \mathcal{V}|]}\mathbf{h}^{(L,i)}\big{)}=\mathbf{W}^{(o)}(\odot_{i\in[| \mathcal{V}|]}\mathbf{h}^{(L,i)})=f^{(\theta,\mathcal{G})}(\mathbf{X})\), meaning contracting \(\mathcal{T}(\mathbf{X})\) results in \(f^{(\theta,\mathcal{G})}(\mathbf{X})\).

An analogous proof establishes that the contractions described by \(\mathcal{T}^{(t)}(\mathbf{X})\) yield \(f^{(\theta,\mathcal{G},t)}(\mathbf{X})\). Specifically, the inductive claim and its proof are the same, up to \(\gamma\) taking values in \([\rho_{L-l}(\{i\},\{t\})]\) instead of \([\rho_{L-l}(\{i\},\mathcal{V})]\), for \(l\in[L]\). This implies that \(\mathbf{h}^{(L,t)}\) is the vector produced by contracting the sub-tree whose root is \(\boldsymbol{\delta}^{(L,t,1)}\). Performing the only remaining contraction, defined by the leg connecting \(\boldsymbol{\delta}^{(L,t,1)}\) with \(\mathbf{W}^{(o)}\), thus results in \(\mathbf{W}^{(o)}\mathbf{h}^{(L,t)}=f^{(\theta,\mathcal{G},t)}(\mathbf{X})\). 

#### I.6.1 Technical Lemma

**Lemma 11**.: _Let \(\boldsymbol{\delta}^{(N+1)}\in\mathbb{R}^{D\times\cdots\times D}\) be an order \(N+1\in\mathbb{N}\) tensor that has ones on its hyper-diagonal and zeros elsewhere, i.e. \(\boldsymbol{\delta}^{(N+1)}_{d_{1},\ldots,d_{N+1}}=1\) if \(d_{1}=\cdots=d_{N+1}\) and \(\boldsymbol{\delta}^{(N+1)}_{d_{1},\ldots,d_{N+1}}=0\) otherwise, for all \(d_{1},\ldots,d_{N+1}\in[D]\). Then, for any \(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)}\in\mathbb{R}^{D}\) it holds that \(\boldsymbol{\delta}^{(N+1)}*_{i\in[N]}\mathbf{x}^{(i)}=\odot_{i\in[N]}\mathbf{ x}^{(i)}\in\mathbb{R}^{D}\)._

Proof.: By the definition of tensor contraction (Definition 7), for all \(d\in[D]\) we have that:

\[\big{(}\boldsymbol{\delta}^{(N+1)}*_{i\in[N]}\mathbf{x}^{(i)}\big{)}_{d}=\sum \nolimits^{D}_{d_{1},\ldots,d_{N}=1}\boldsymbol{\delta}^{(N+1)}_{d_{1},\ldots,d_ {N},d}\cdot\prod\nolimits_{i\in[N]}\mathbf{x}^{(i)}_{d_{i}}=\prod\nolimits_{i \in[N]}\mathbf{x}^{(i)}_{d}=\left(\odot_{i\in[N]}\mathbf{x}^{(i)}\right)_{d}.\]