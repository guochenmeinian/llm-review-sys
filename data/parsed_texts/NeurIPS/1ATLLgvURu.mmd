# Learning-Augmented Priority Queues

 Ziyad Benomar

ENSAE, Ecole Polytechnique,

FairPlay joint team

ziyad.benomar@ensae.fr &Christian Coester

Department of Computer Science

University of Oxford, UK

christian.coester@cs.ox.ac.uk

###### Abstract

Priority queues are one of the most fundamental and widely used data structures in computer science. Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element. In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance. We examine three prediction models spanning different use cases, and we show how the predictions can be leveraged to enhance the performance of priority queue operations. Moreover, we demonstrate the optimality of our solution and discuss some possible applications.

## 1 Introduction

Priority queues are an essential abstract data type in computer science [14, 15] whose objective is to enable the swift insertion of new elements and access or deletion of the highest priority element. Their applications span a wide range of problems within computer science and beyond. They play a crucial role in sorting [13, 17], in various graph algorithms such as Dijkstra's shortest path algorithm [2] or computing minimum spanning trees [1], in operating systems for scheduling and load balancing [11], in networking protocols for managing data transmission packets [12], in discrete simulations for efficient event processing based on occurrence time [13, 14], and in implementing hierarchical clustering algorithms [1, 15].

Various data structures can be used to implement priority queues, each offering distinct advantages and tradeoffs [15]. However, it is established that a priority queue with \(n\) elements cannot guarantee \(o(\log n)\) time for all the required operations [15]. This limitation can be surpassed within the learning augmented framework [16], where the algorithms can benefit from machine-learned or expert advice to improve their worst-case performance. We propose in this work learning-augmented implementations of priority queues in three different prediction models, detailed in the next section.

### Problem definition

A priority queue is a dynamic data structure where each element \(x\) is assigned a key \(u\) from a totally ordered universe \((\mathcal{U},<)\), determining its priority. The standard operations of priority queues are:

1. \(\mathrm{FindMin}()\): returns the element with the smallest key without removing it,
2. \(\mathrm{ExtractMin}()\): removes and returns the element with the smallest key,
3. \(\mathrm{Insert}(x,u)\): adds a new element \(x\) to the priority queue with key \(u\),
4. \(\mathrm{DecreaseKey}(x,v)\): decreases the key of an element \(x\) to \(v\).

The elements of the priority queue can be accessed via their keys in \(O(1)\) time using a HashMap. Hence, the focus is on establishing efficient algorithms for key storage and organization, facilitating the execution of priority queue operations. For any key \(u\in\mathcal{U}\) and subset \(\mathcal{V}\subset\mathcal{U}\), we denote by \(r(u,\mathcal{V})\) the rank of \(u\) in \(\mathcal{V}\), defined as the number of keys in \(\mathcal{V}\) that are smaller than or equal to \(u\),

\[r(u,\mathcal{V})=\#\{v\in\mathcal{V}:v\leq u\}\;.\] (1)

The difficulty lies in designing data structures offering adequate tradeoffs between the complexities of the operations listed above. This paper explores how using predictions can allow us to overcome the limitations of traditional priority queues. We examine three types of predictions.

Dirty comparisons.In the first prediction model, comparing two keys \((u,v)\in\mathcal{U}^{2}\) is slow or costly. However, the algorithm can query a prediction of the comparison \((u<v)\). This prediction serves as a rapid or inexpensive, but possibly inaccurate, method of comparing elements of \(\mathcal{U}\), termed a _dirty_ comparison and denoted by \((u\lesssim v)\). Conversely, the true outcome of \((u<v)\) is referred to as a _clean_ comparison. For all \(u\in\mathcal{U}\) and \(\mathcal{V}\subseteq\mathcal{U}\), we denote by \(\eta(u,\mathcal{V})\) the number of inaccurate dirty comparisons between \(u\) and elements of \(\mathcal{V}\),

\[\eta(u,\mathcal{V})=\#\{v\in\mathcal{V}\;:\;\mathbbm{1}(u\lesssim v)\neq \mathbbm{1}(u<v)\}\;.\] (2)

This prediction model was introduced in [10] for sorting, but it has a broader significance in comparison-based problems, such as search [1, 12, 13], ranking [21, 14], and the design of comparison-based ML algorithms [1, 13, 15, 16]. Particularly, it has great theoretical importance for priority queues, which have been extensively studied in the comparison-based framework [1, 12, 13]. Comparison-based models are often used, for example, when the preferences are determined by human subjects. Assigning numerical scores in these cases is inexact and prone to errors, while pairwise comparisons are absolute and more robust [13]. Within such a setting, dirty comparisons can be obtained by a binary classifier, and used to minimize human inference yielding clean comparisons, which are time-consuming and might incur additional costs.

Pointer predictions.In this second model, upon the addition of a new key \(u\) to the priority queue \(\mathcal{Q}\), the algorithm receives a prediction \(\operatorname{\widetilde{Pred}}(u,\mathcal{Q})\in\mathcal{Q}\) of the predecessor of \(u\), which is the largest key belonging to \(\mathcal{Q}\) and smaller than \(u\). Before \(u\) is inserted, the ranks of \(u\) and its true predecessor in \(\mathcal{Q}\) are equal, hence we define the prediction error as

\[\vec{\eta}(u,\mathcal{Q})=|r(u,\mathcal{Q})-r(\operatorname{\widetilde{Pred}} (u,\mathcal{Q}),\mathcal{Q})|\;.\] (3)

In priority queue implementations, a HashMap preserves a pointer from each inserted key to its corresponding position in the priority queue. Consequently, \(\operatorname{\widetilde{Pred}}(u,\mathcal{Q})\) provides direct access to the predicted predecessor's position. For example, this prediction model finds applications in scenarios where concurrent machines have access to the priority queue [13, 14, 15]. Each machine can estimate, within the elements it has previously inserted, which one precedes the next element it intends to insert. However, this estimation might not be accurate, as other concurrent machines may have inserted additional elements.

Rank predictions.The last setting assumes that the priority queue is used in a process where a finite number \(N\) of distinct keys will be inserted, i.e., the priority queue is not used indefinitely, but \(N\) is unknown to the algorithm. Upon the insertion of any new key \(u_{i}\), the algorithm receives a prediction \(\widehat{R}(u_{i})\) of the rank of \(u_{i}\) among all the \(N\) keys \(\{u_{j}\}_{j\in[N]}\). Denoting by \(R(u_{i})=r(u_{i},\{u_{j}\}_{j\in[N]})\) the true rank, the prediction error of \(u_{i}\) is

\[\eta^{\Delta}(u_{i})=|R(u_{i})-\widehat{R}(u_{i})|\;.\] (4)

The same prediction model was explored in [16] for the online list labeling problem. Bai and Coester [16] investigate a similar model for the sorting problem, but in an offline setting where the \(N\) elements to sort and the predictions are accessible to the algorithm from the start.

Note that \(N\) counts the distinct keys added with \(\operatorname{Insert}\) or \(\operatorname{DecreaseKey}\) operations. An arbitrarily large number of \(\operatorname{DecreaseKey}\) operations thus can make \(N\) arbitrarily large although the total number of insertions is reduced. However, with a lazy implementation of \(\mathrm{DecreaseKey}\), we can assume without loss of generality that \(N\) is at most quadratic in the total number of insertions. Indeed, it is possible to omit executing the \(\mathrm{DecreaseKey}\) operations when queried, and only store the new elements' keys to update. Then, at the first \(\mathrm{ExtractMin}\) operation, all the element's keys are updated by executing for each one only the last queried \(\mathrm{DecreaseKey}\) operation involving it. Denoting by \(k\) the total number of insertions, there are at most \(k\)\(\mathrm{ExtractMin}\) operations, and for each of them, there are at most \(k\)\(\mathrm{DecreaseKey}\) operations executed. The total number of effectively executed \(\mathrm{DecreaseKey}\) operations is therefore \(O(k^{2})\). In particular, this implies that a complexity of \(O(\log N)\) is also logarithmic in the total number of insertions.

### Our results

We first investigate augmenting binary heaps with predictions. To leverage dirty comparisons, we first design a _randomized binary search_ algorithm to find the position of an element \(u\) in a sorted list \(L\). We prove that it terminates using \(O(\log|L|)\) dirty comparisons and \(O(\log\eta(u,L))\) clean comparisons in expectation. Subsequently, we use this result to establish an insertion algorithm in binary heaps using \(O(\log\log n)\) dirty comparisons and reducing the number of clean comparisons to \(O(\log\log\eta(u,\mathcal{Q}))\). However, \(\mathrm{ExtractMin}\) still mandates \(O(\log n)\) clean comparisons. In the two other prediction models, binary heaps and other heap implementations of priority queues appear unsuitable, as the positions of the keys are not determined solely by their ranks.

Consequently, in Section 3, we shift to using skip lists. We devise randomized insertion algorithms requiring, in expectation, only \(O(\log\vec{\eta}(u,\mathcal{Q}))\) time and comparisons in the pointer prediction model, \(O(\log n)\) time and \(O(\log\eta(u,\mathcal{Q}))\) clean comparisons in the dirty comparison model, and \(O(\log\log N+\log\max_{i\in[N]}\eta^{\Delta}(u_{i}))\) time and \(O(\log\max_{i\in[N]}\eta^{\Delta}(u_{i}))\) comparisons in the rank prediction model, where we use in the latter an auxiliary van Emde Boas (vEB) tree [van Emde Boas et al., 1976]. Across the three prediction models, \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\) only necessitate \(O(1)\) time, and the complexity of \(\mathrm{DecreaseKey}\) aligns with that of insertion. Finally, we prove in Theorem 3.4 the optimality of our data structure. Table 1 summarizes the complexities of our learning-augmented priority queue (LAPQ) in the three prediction models compared to standard priority queue implementations. The complexity of \(\mathrm{FindMin}\) is \(O(1)\) for all the listed priority queues.

Our learning-augmented data structure enables additional operations beyond those of priority queues, such as the _maximum priority queue_ operations \(\mathrm{FindMax}\), \(\mathrm{ExtractMax}\), and \(\mathrm{IncreaseKey}\) with analogous complexities, and removing an arbitrary key \(u\) from the priority queue, finding its predecessor or successor in expected \(O(1)\) time.

Furthermore, we show in Section 4.1 that it can be used for sorting, yielding the same guarantees as the learning-augmented sorting algorithms presented in [Bai and Coester, 2023] for the positional predictions model with _displacement error_, and for the dirty comparison model. In the second model, our priority queue offers even stronger guarantees, as it maintains the elements sorted at any time even if the insertion order is adversarial, while the algorithm of [Bai and Coester, 2023] requires a random insertion order to achieve a sorted list by the end within the complexity guarantees. We also show how the learning-augmented priority queue can be used to accelerate Dijkstra's algorithm.

Finally, in Section 5, we compare the performance of our priority queue using predictions with binary and Fibonacci heaps when used for sorting and for Dijkstra's algorithm on both real-world city maps and synthetic graphs. The experimental results confirm our theoretical findings, showing that adequately using predictions significantly reduces the complexity of priority queue operations.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**Priority queues** & \(\mathrm{ExtractMin}\) & \(\mathrm{Insert}\) & \(\mathrm{DecreaseKey}\) \\ \hline Binary Heap & \(O(\log n)\) & \(O(\log\log n)\) & \(O(\log n)\) \\ \hline Fibonacci Heap (amortized) & \(O(\log n)\) & \(O(1)\) \\ \hline Skip List (average) & \(O(1)\) & \(O(\log n)\) \\ \hline LAPQ with dirty comparisons (average) & \(O(1)\) & \(O(\log\eta(u,\mathcal{Q}))\) \\ \hline LAPQ with pointer predictions (average) & \(O(1)\) & \(O(\log\vec{\eta}(u,\mathcal{Q}))\) \\ \hline LAPQ with rank predictions (average) & \(O(1)\) & \(O(\log\max_{i\in[n]}\eta^{\Delta}(u_{i}))\) \\ \hline \end{tabular}
\end{table}
Table 1: Number of comparisons per operation used by different priority queues.

### Related work

In this section, we briefly discuss related works on learning-augmented algorithms and priority queues. For a more extensive review of related work, please refer to Appendix A.

Learning-augmented algorithms.Learning-augmented algorithms, introduced in (Lykouris and Vassilvitskii, 2018; Purohit et al., 2018), have captured increasing interest over the last years, as they allow breaking longstanding limitations in many algorithm design problems. Assuming that the decision-maker is provided with potentially incorrect predictions regarding unknown parameters of the problem, learning-augmented algorithms must be capable of leveraging these predictions if they are accurate (consistency), while keeping the worst-case performance without advice even if the predictions are arbitrarily bad or adversarial (robustness). While many fundamental online problems were studied in this setting (see Appendix A), the design of data structures with predictions remains relatively underexplored. The seminal paper by (Kraska et al., 2018) shows how predictions can be used to optimize space usage. Another study by (Lin et al., 2022) demonstrates that the runtime of binary search trees can be optimized by incorporating predictions of item access frequency. Recent papers have extended this prediction model to other data structures, such as dictionaries (Zeynali et al., 2024) and skip lists (Fu et al., 2024). The prediction models we study in the current paper deviate from the latter, and are more related to those considered respectively in (Bai and Coester, 2023) for sorting, and (McCauley et al., 2024) for online list labeling. An overview of the growing body of work on learning-augmented algorithms (also known as algorithms with predictions) is maintained at (Lindermayr and Megow, 2022).

Priority queues implementations.Binary heaps, introduced by Williams (1964), are one of the first efficient implementations of priority queues. They allow all the operations in \(O(\log n)\) time, where \(n\) is the number of items in the queue. A first improvement was introduced with Binomial heaps (Vuillemin, 1978), reducing the amortized time of insertion to \(O(1)\). A breakthrough came later with Fibonacci heaps (Fredman and Tarjan, 1987), which allow all the operations in constant amortized time, except for \(\mathrm{ExtractMin}\), which takes \(O(\log n)\) time. However, Fibonacci heaps are known to be slow in practice (Larkin et al., 2014), and other implementations with weaker theoretical guarantees such as binary heaps are often preferred. Another possible implementation uses _skip lists_(Pugh, 1990), which are probabilistic data structures, guaranteeing in expectation a constant time for \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\), and \(O(\log n)\) time for \(\mathrm{Insert}\) and \(\mathrm{DecreaseKey}\).

## 2 Heap priority queues

A common implementation of priority queues uses binary heaps, enabling all operations in \(O(\log n)\) time. Binary heaps maintain a balanced binary tree structure, where all depth levels are fully filled, except possibly for the last one, to which we refer in all this section as the _leaf level_. Moreover, it satisfies the _heap property_, i.e., any key is smaller than all its children. To maintain these two structure properties, when a new element is added, it is first inserted in the leftmost empty position in the leaf level, and then repeatedly swapped with its parent until the heap property is restored.

### Insertion in the comparison-based model

Insertion in a binary heap can be accomplished using only \(O(\log\log n)\) comparisons, albeit \(O(\log n)\) time, by doing a binary search of the new element's position along the path from the leftmost empty position in the leaf level to the root, which is a sorted list of size \(O(\log n)\). To improve the insertion complexity with dirty comparisons, we first tackle the search problem in this setting.

Search with dirty comparisons.Consider a sorted list \(L=(v_{1},\ldots,v_{k})\) and a target \(u\), the position of \(u\) in \(L\) can be found using binary search with \(O(\log k)\) comparisons. Extending ideas from Bai and Coester (2023) and Lykouris and Vassilvitskii (2018), this complexity can be reduced using dirty comparisons. Indeed, we can obtain an estimated position \(\widehat{r}(u,L)\) through a binary search with dirty comparisons, followed by an exponential search with clean comparisons, starting from \(\widehat{r}(u,L)\) to find the exact position \(r(u,L)\). However, the positions of inaccurate dirty comparisons can be adversarially chosen to compromise the algorithm. This can be addressed by introducing randomness to the dirty search phase. We refer to the _randomized binary search_ as the algorithm that proceedssimilarly to the binary search, but whenever the search is reduced to an array \(\{v_{i},\ldots,v_{j}\}\), instead of comparing \(u\) to the pivot \(v_{m}\) with index \(m=i+\lfloor\frac{j-i}{2}\rfloor\), it compares \(u\) to a pivot with an index chosen uniformly at random in the range \(\{i+\lceil\frac{j-i}{4}\rceil,\ldots,j-\lceil\frac{j-i}{4}\rceil\}\).

**Theorem 2.1**.: _A dirty randomized binary search followed by a clean exponential search finds the target's position using \(O(\log k)\) dirty comparisons and \(O(\log\eta(u,L))\) clean comparisons in expectation._

Randomized insertion in a binary heap.In a binary heap \(\mathcal{Q}\), any new element \(u\) is always inserted along the path of size \(O(\log n)\) from the root to the leftmost empty position in the leaf level. If all the inaccurate dirty comparisons are chosen along this path, then the insertion would require \(O(\log\eta(u,\mathcal{Q}))\) clean comparisons by Theorem 2.1. This complexity can be reduced further by randomizing the choice of the root-leaf path where \(u\) is inserted, as explained in Algorithm 1.

``` Input: Binary heap \(\mathcal{Q}\) with randomly filled positions in the leaf level, new key \(u\)
1\(L\leftarrow\) keys path from a uniformly random empty position in the leaf level to the root;
2\(\widehat{r}(u,L)\leftarrow\) outcome of a dirty randomized binary search of \(u\) in \(L\);
3\(r(u,L)\leftarrow\) outcome of the clean exponential search of \(u\) in \(L\) starting from index \(\widehat{r}(u,L)\);
4Insert \(u\) in the chosen leaf empty position, then swap it with its parents until position \(r(u,L)\); ```

**Algorithm 1**Randomized insertion in binary heap

**Theorem 2.2**.: _The insertion algorithm 1 requires \(O(\log n)\) time, \(O(\log\log n)\) dirty comparisons and \(O(\log\log\eta(u,\mathcal{Q}))\) clean comparisons in expectation._

### Limitations

The previous theorem demonstrates that accurate predictions can reduce the number of clean comparisons for insertion in a binary heap. However, for the \(\mathrm{ExtractMin}\) operation, when the minimum key, which is the root of the tree, is deleted, its two children are compared and the smallest is placed in the root position, and this process repeats recursively, with each new empty position filled by comparing both of its children, requiring necessarily \(O(\log n)\) clean comparisons in total to ensure the heap priority remains intact. Improving the efficiency of \(\mathrm{ExtractMin}\) using dirty comparisons would therefore require bringing major modifications to the binary heap's structure.

Similar difficulties arise when attempting to enhance \(\mathrm{ExtractMin}\) using dirty comparisons or the other prediction models in different heap implementations, such as Binomial or Fibonacci heaps. Consequently, we explore in the next section another priority queue implementation, using skip lists, which allows for an easier and more efficient exploitation of the predictions.

## 3 Skip lists

A priority queue can be implemented naively by maintaining a dynamic sorted linked list of keys. This guarantees constant time for \(\mathrm{ExtractMin}\), but \(O(n)\) time for insertion. Skip lists (see Figure 1) offer a solution to this inefficiency, by maintaining multiple levels of linked lists, with higher levels containing fewer elements and acting as shortcuts to lower levels, facilitating faster search and insertion in expected \(O(\log n)\) time. In all subsequent discussions concerning linked lists or skip lists, it is assumed that they are doubly linked, having both predecessor and successor pointers between elements.

The first level in a skip list is an ordinary linked list containing all the elements, which we denote by \(v_{1},\ldots,v_{n}\). Every higher level is constructed by including each element from the previous level independently with probability \(p\), typically set to \(1/2\). For any key \(v_{i}\) in the skip list, we define its height \(h(v_{i})\) as the number of levels where it appears, which is an independent geometric random variable with parameter \(p\). A number \(2h(v_{i})\) of pointers are associated with \(v_{i}\), giving access to the previous and next element in each level \(\ell\in[h(v_{i})]\), denoted respectively by \(\mathrm{Prev}(v_{i},\ell)\) and \(\mathrm{Next}(v_{i},\ell)\). Using a HashMap, these pointers can be accessed in \(O(1)\) time via the key value \(v_{i}\). For convenience, we consider that the skip list contains two additional keys \(v_{0}=-\infty\) and \(v_{n+1}=\infty\), corresponding respectively to the head and the \(\mathrm{NIL}\) value. Both have a height equal to the maximum height in the queue \(h(v_{0})=h(v_{n+1})=\max_{i\in[n]}h(v_{i})\).

Since the expected height of keys in the skip list is \(1/p\), deleting any key only requires a constant time in expectation, by updating its associated pointers, along with those of its predecessors and successors in the levels where it appears. In particular, \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\) take \(O(1)\) time, and \(\mathrm{DecreaseKey}\) can be performed by deleting the element and reinserting it with the new key, yielding the same complexity as insertion. Furthermore, by the same arguments, inserting a new key \(u\) next to a given key \(v_{i}\) in the skip list can be done in expected constant time.

Therefore, implementing efficient Insert and \(\mathrm{DecreaseKey}\) operations for skip lists with predictions is reduced to designing efficient search algorithms to find the predecessor of a target key \(u\) in the skip list, i.e., the largest key \(v_{i}\) in the skip list that is smaller than \(u\). In all the following, we denote by \(\mathcal{Q}\) a skip list containing \(n\) keys \(v_{1}\leq\ldots\leq v_{n}\in\mathcal{U}\), and \(u\in\mathcal{U}\) the target key. As explained in Appendix C.1, we can assume without loss of generality that the keys \((u,v_{1},\ldots,v_{n})\) are pairwise distinct. In the following, we present separately for each model an insertion algorithm leveraging the predictions.

### Pointer prediction

Given a pointer prediction \(v_{j}=\widehat{\mathrm{Pred}}(u,\mathcal{Q})\), we describe below an algorithm for finding the true predecessor of \(u\) starting from the position of the key \(v_{j}\), then inserting \(u\). We assume in the algorithm that \(v_{j}\leq u\). If \(v_{j}>u\), then the algorithm can be easily adapted by reversing the search direction.

``` Input: Skip list \(\mathcal{Q}\), source \(v_{j}\in\mathcal{Q}\), and new key \(u\in\mathcal{U}\)
1\(w\gets v_{j}\); \(\triangleright\) Bottom-Up search
2while\(\mathrm{Next}(w,h(w))\leq u\)do
3\(w\leftarrow\mathrm{Next}(w,h(w))\); \(\ell\gets h(w)\); \(\triangleright\) Top-Down search
4while\(\ell>0\)do
5while\(\mathrm{Next}(w,\ell)\leq u\)do
6\(w\leftarrow\mathrm{Next}(w,\ell)\);
7\(\ell\leftarrow\ell-1\);
8Insert \(u\) next to \(w\); ```

**Algorithm 2**\(\mathrm{ExpSearchInsertion}(\mathcal{Q},v_{j},u)\)

Algorithm 2 is inspired by the classical exponential search in arrays. The first phase consists of a bottom-up search, expanding the size of the search interval by moving to upper levels until finding a key \(w\in\mathcal{Q}\) satisfying \(w\leq u<\mathrm{Next}(w,h(w))\). The second phase conducts a top-down search from level \(h(w)\) downward, refining the search until locating the position of \(u\). It is worth noting that the classical search algorithm in skip lists, denoted by \(\mathrm{Search}(\mathcal{Q},u)\), corresponds precisely to the top-down search, starting from the head of the skip list instead of \(w\).

**Theorem 3.1**.: _Augmented with pointer predictions, a skip list allows \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\) in expected \(O(1)\) time, and \(\mathrm{Insert}(u)\) in expected \(O(\log\vec{\eta}(u,\mathcal{Q}))\) time using Algorithm 2._

### Dirty comparisons

We devise in this section a search algorithm using dirty and clean comparisons. Algorithm 3 first estimates the position of \(u\) with a dirty top-down search starting from the head, then performs a clean exponential search starting from the estimated position to find the true position.

``` Input: Skip list \(\mathcal{Q}\), new key \(u\in\mathcal{U}\)
1\(\hat{w}\leftarrow\mathrm{Search}(\mathcal{Q},u)\) with dirty comparisons; \(\mathrm{ExpSearchInsertion}(\mathcal{Q},\hat{w},u)\) with clean comparisons; ```

**Algorithm 3**Insertion with dirty and clean comparisons

The dirty search concludes within \(O(\log n)\) steps, and Theorem 3.1 guarantees that the exponential search terminates within \(O(\log|r(\hat{w},\mathcal{Q})-r(u,\mathcal{Q})|)\) steps. Combining these results and relating the distance between \(u\) and \(\hat{w}\) in \(\mathcal{Q}\) to the prediction error \(\eta(u,\mathcal{Q})\), we derive the following theorem.

**Theorem 3.2**.: _Augmented with dirty comparisons, a skip list allows \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\) in \(O(1)\) expected time, and \(\mathrm{Insert}(u)\) with Algorithm 3 in \(O(\log n)\) expected time, using \(O(\log n)\) dirty comparisons and \(O(\log\eta(u,\mathcal{Q}))\) clean comparisons in expectation._

### Rank predictions

In the rank prediction model, each \(\mathrm{Insert}(u)\) request is accompanied by a prediction \(\widehat{R}(u)\) of the rank of \(u\) among all the distinct keys already in, or to be inserted into the priority queue. If the predictions are accurate and the total number \(N\) of distinct keys to be inserted is known, the problem reduces to designing a priority queue with integer keys in \([N]\), taking as keys the ranks \((R_{i})_{i\in[N]}\). This problem can be addressed using a van Emde Boas (vEB) tree over \([N]\)(van Emde Boas et al., 1976), which requires \(O(\log\log N)\) time for insertion, deletion, finding the minimum or maximum, and finding the predecessor or successor of any element, guaranteeing in particular \(O(\log\log N)\) time for all priority queue operations. More details on its structure can be found in Appendix C.5.

To leverage rank predictions, we use an auxiliary vEB tree \(\mathcal{T}\) along with the skip list \(\mathcal{Q}\). All the insertion and deletion operations are made simultaneously on \(\mathcal{T}\) and \(\mathcal{Q}\). However, the priorities used in \(\mathcal{T}\) are the predicted ranks \(\{\widehat{R}(u_{i})\}_{i\in[N]}\). Whenever a new key \(u_{i}\) is to be added, Algorithm 4 inserts it first in \(\mathcal{T}\) at position \(\widehat{R}(u_{i})\), gets its predecessor \(\hat{w}\) in \(\mathcal{T}\), i.e., the element in \(\mathcal{T}\) with the largest predicted rank smaller than or equal to \(\widehat{R}(u_{i})\), then uses \(\hat{w}\) as a pointer prediction to find the position of \(u_{i}\) in \(\mathcal{Q}\). If the predecessor is not unique, the algorithm chooses an arbitrary one.

``` Input: Skip list \(\mathcal{Q}\), vEB tree \(\mathcal{T}\) on \([N]\), new element \(u_{i}\in\mathcal{U}\), prediction \(\widehat{R}(u_{i})\in[N]\)
1 Insert \(u_{i}\) in \(\mathcal{T}\) with key \(\widehat{R}(u_{i})\); \(\hat{w}\leftarrow\mathrm{predecessor}\) of \(u_{i}\) in \(\mathcal{T}\); \(\mathrm{ExpSearchInsertion}(\mathcal{Q},\hat{w},u_{i})\); ```

**Algorithm 4**Insertion with rank prediction

We explain in Appendix C.5 how the data structure can be adapted when \(N\) is unknown and the rank predictions are not necessarily in \([N]\), and we prove the following theorem, giving both the runtime and comparison complexities of the priority queue operations using this data structure.

**Theorem 3.3**.: _If \(\widehat{R}(u_{i})=O(N)\) for all \(i\in[N]\), then there is a data structure allowing \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\) in \(O(1)\) amortized time, and \(\mathrm{Insert}\) in \(O(\log\log N+\log\max_{i\in[N]}\eta^{\Delta}(u_{i}))\) amortized time using \(O(\log\max_{i\in[N]}\eta^{\Delta}(u_{i}))\) comparisons in expectation._

In contrast to other prediction models, the complexity of inserting \(u_{i}\) is not impacted only by \(\eta^{\Delta}(u_{i})\), but by the maximum error over all keys \(\{u_{j}\}_{j\in[N]}\). This occurs because the exponential search conducted in Algorithm 4 starts from the key \(\hat{w}\in\mathcal{Q}\), whose error also affects insertion performance. A similar behavior is observed in the online list labeling problem (McCauley et al., 2024), where the bounds provided by the authors also depend on the maximum prediction error for insertion.

With perfect predictions, the number of comparisons for insertion becomes constant, and its runtime \(O(\log\log N)\). It is not clear if the runtime of all the priority queue operations can be reduced to \(O(1)\) with perfect predictions. Indeed, the problem in that case is reduced to a priority queue with all the keys in \([N]\). The best-known solution to this problem is a randomized priority queue, by Thorup (2007), supporting all operations in \(O(\sqrt{\log\log N})\) time. However, in our approach, we use vEB trees beyond the classical priority queue operations, as we also require fast access to the predecessor of any element. A data structure supporting all these operations solves the dynamic predecessor problem, for which vEB trees are optimal (Patrascu and Thorup, 2006). Reducing the runtime of insertion below \(O(\log\log N)\) would therefore require omitting the use of predecessor queries.

### Lower bounds

As explained earlier, \(\mathrm{ExtractMin}\) requires only \(O(1)\) expected time in skip lists. Furthermore, we presented insertion algorithms for the three prediction models and provided upper bounds on their complexities. The following theorem establishes lower bounds on the complexities of \(\mathrm{ExtractMin}\) and \(\mathrm{Insert}\) for any priority queue augmented with any of the three prediction types.

**Theorem 3.4**.: _For each of the three prediction models, the following lower bounds hold._

1. _[label=()]_
2. _Dirty comparisons: no data structure_ \(\mathcal{Q}\) _can support_ \(\mathrm{ExtractMin}\) _with_ \(O(1)\) _clean comparisons and_ \(\mathrm{Insert}(u)\) _with_ \(o(\log\eta(u,\mathcal{Q}))\) _clean comparisons in expectation._
3. _Pointer predictions: no data structure_ \(\mathcal{Q}\) _can support_ \(\mathrm{ExtractMin}\) _with_ \(O(1)\) _comparisons and_ \(\mathrm{Insert}(u)\) _with_ \(o(\log\vec{\eta}(u,\mathcal{Q}))\) _comparisons in expectation._
4. _Rank predictions: no data structure_ \(\mathcal{Q}\) _can support_ \(\mathrm{ExtractMin}\) _with_ \(O(1)\) _comparisons and_ \(\mathrm{Insert}(u_{i})\) _with_ \(o(\log\max_{i\in[N]}\eta^{\Delta}(u_{i}))\) _comparisons in expectation, for all_ \(i\in[N]\)_._

These lower bounds with Theorems 3.2 and 3.1 prove the tightness of our priority queue in the dirty comparison and the pointer prediction models. In the rank prediction model, the comparison complexities proved in Theorem 3.3 are optimal, whereas the runtimes are only optimal up to an additional \(O(\log\log N)\) term. In particular, they are optimal if the maximal error is at least \(\Omega(\log N)\).

## 4 Applications

### Sorting algorithm

Our learning-augmented priority queue can be used for sorting a sequence \(A=(a_{1},\ldots,a_{n})\), by first inserting all the elements, then repeatedly extracting the minimum until the priority queue is empty. We compare below the performance of this sorting algorithm to those of (Bai and Coester, 2023).

Dirty comparison model.Denoting by \(\eta_{i}=\eta(a_{i},A)\), Bai and Coester (2023) prove a sorting algorithm using \(O(n\log n)\) time, \(O(n\log n)\) dirty comparisons, and \(O(\sum_{i}\log(\eta_{i}+2))\) clean comparisons. Theorem 3.2 yields the same guarantees with our learning-augmented priority queue. Moreover, our learning-augmented priority queue is a skip list, maintaining elements in sorted order at any time, even if the elements are revealed online and the insertion order is adversarial, while in (Bai and Coester, 2023), it is crucial that the insertion order is chosen uniformly at random.

Positional predictions.In their second prediction model, they assume that the algorithm is given offline access to predictions \(\{\widehat{R}(a_{i})\}_{i\in[n]}\) of the relative ranks \(\{R(a_{i})\}_{i\in[n]}\) of the \(n\) elements to sort, and they study two different error measures. The rank prediction error \(\eta_{i}^{\Delta}=|R(a_{i})-\widehat{R}(a_{i})|\) matches their definition of _displacement error_, for which they prove a sorting algorithm in \(O(\sum_{i}\log(\eta_{i}^{\Delta}+2))\) time. The same bound can be deduced using our results in the pointer prediction model. Further discussion on this claim can be found in Appendix E.

Online rank predictions.If \(n\) is unknown to the algorithm, and the elements \(a_{1},\ldots,a_{n}\) along with their predicted ranks are revealed online, possibly in an adversarial order, then by Theorem 3.3, the total runtime of our priority queue for maintaining all the inserted elements sorted at any time is \(O(n\log\log n+n\log\max_{i}\eta_{i}^{\Delta})\), and the number of comparisons used is \(O(n\log\max_{i}\eta_{i}^{\Delta})\). No analogous result is demonstrated in (Bai and Coester, 2023) in this setting.

### Dijkstra's shortest path algorithm

Consider a run of Dijkstra's algorithm on a directed positively weighted graph \(G\) with \(n\) nodes and \(m\) edges. The elements inserted into the priority queue are the nodes of the graph, and the corresponding keys are their tentative distances to the source, which are updated over time. During the algorithm's execution, at most \(m+1\) distinct keys \(\{d_{i}\}_{i\in[m+1]}\) are inserted into the priority queue. Given online predictions \((\widehat{R}(d_{i}))_{i\in[m+1]}\) of their relative ranks \((R(d_{i}))_{i\in[m+1]}\), the total runtime using our priority queue augmented with rank predictions is

\[O\big{(}m\log\log n+m\log\max_{i\in[m+1]}|R(d_{i})-\widehat{R}(d_{i})|\big{)}\.\]

In contrast, the shortest path algorithm of Lattanzi et al. (2023) (which also works for negative edges) has a linear dependence on a similar error measure. Even with arbitrary error, our guarantee is never worse than the \(O(m\log n)\) runtime with binary heaps. Using Fibonacci heaps results in an \(O(n\log n+m)\) runtime, which is surpassed by our learning-augmented priority queue in the case of sparse graphs where \(m=o(\frac{n\log n}{\log\log n})\) if predictions are of high quality. However, it is known that Fibonacci heaps perform poorly in practice, even compared to binary heaps, as supported by our experiments.

## 5 Experiments

In this section, we empirically evaluate the performance of our learning-augmented priority queue (LAPQ) by comparing it with Binary and Fibonacci heaps. We use two standard benchmarks for this evaluation: sorting and Dijkstra's algorithm. For the sorting benchmark, we also compare our results with those from Bai and Coester (2023). For Dijkstra's algorithm, we assess performance on both real city maps and synthetic random graphs. In all the experiments, each data point represents the average result from 30 independent runs. Additional experiments and a detailed discussion on the prediction models and the obtained results can be found in Appendix F. The code used for conducting the experiments is available at github.com/Ziyad-Benomar/Learning-augmented-priority-queues.

Sorting.We compare sorting using our LAPQ with the algorithms of Bai and Coester (2023) under their same experimental settings. Given a sequence \(A=(a_{1},\ldots,a_{n})\), we evaluate the complexity of sorting it with predictions in the _class_ and the _decay_ setting. In the first, \(A\) is divided into \(c\) classes \(((t_{k-1},t_{k}])_{k\in[c]}\), where \(0=t_{0}\leq t_{1}\leq\ldots\leq t_{c}=n\) are uniformly random thresholds. The predicted rank of any item \(a_{i}\) with \(t_{k}\leq i<t_{k+1}\) is sampled uniformly at random within \((t_{k},t_{k+1}]\). In the decay setting, the ranking is initially accurate but degrades over time. Each time step, one item's predicted position is perturbed by 1, either left or right, uniformly at random.

In both settings, we test the LAPQ with the three prediction models. First, assuming that the rank predictions are given offline, we use pointer predictions as explained in Appendix E. In the second case, the elements to insert along with their predicted ranks are revealed online in a uniformly random order. Finally, we test the dirty comparison setting with the dirty order \((a_{i}\ \widehat{\times}\ a_{j})=(\widehat{R}(a_{i})<\widehat{R}(a_{j}))\).

Figures 2 and 3 show the obtained results respectively in the class and the decay setting for \(n\in\{10^{4},10^{5}\}\). In the class setting with offline predictions, the LAPQ slightly outperforms the _Double-Hoover_ and _Displacement_ sort algorithms of Bai and Coester (2023), which were shown to outperform classical sorting algorithms. In the decay setting, the LAPQ matches the performanceof the Displacement sort, but is slightly outperformed by the Double-Hoover sort. With online predictions, although the problem is harder, LAPQ's performance remains comparable to the previous algorithms. In both settings, the LAPQ with offline predictions, online predictions, and dirty comparisons all yield better performance than binary or Fibonacci heaps, even with predictions that are not highly accurate.

Dijkstra's algorithm.Consider a graph \(G=(V,E)\) with \(n\) nodes and \(m\) edges, and a source node \(s\in V\). In the first predictions setting, we pick a random node \(\hat{s}\) and run Dijkstra's algorithm with \(\hat{s}\) as the source, memorizing all the keys \(\hat{D}=(\hat{d}_{1},\dots,\hat{d}_{m})\) inserted into the priority queue. In subsequent runs of the algorithm with different sources, when a key \(d_{i}\) is to be inserted, we augment the insertion with the rank prediction \(\widehat{R}(d_{i})=r(d_{i},\hat{D})\). We call these _key rank predictions_. This model aims at exploiting the topology and uniformity of city maps. As computing shortest paths from any source necessitates traversing all graph edges, keys inserted into the priority queue--partial sums of edge lengths--are likely to exhibit some degree of similarity even if the algorithm is executed from different sources. Notably, this prediction model offers an explicit method for computing predictions, readily applicable in real-world scenarios.

In the second setting, we consider rank predictions of the nodes in \(G\), ordered by their distances to \(s\). As Dijkstra's algorithm explores a new node \(x\in V\), it receives a prediction \(\widehat{r}(x)\) of its rank. The node \(x\) is then inserted with a key \(d_{i}\), to which we assign the prediction \(\widehat{R}(d_{i})=\widehat{r}(x)\). Unlike the previous experimental settings, we initially have predictions of the nodes' ranks, which we extend to predictions of the keys' ranks. Similarly to the sorting experiments, we consider _class_ and _decay_ perturbations of the node ranks.

In the context of searching the shortest path, rank predictions in the class setting can be derived from subdividing the city into multiple smaller areas. Each class corresponds to a specific area, facilitating the ordering of areas from closest to furthest relative to the source. However, comparing the distances from the source to the nodes in the same class might be inaccurate. On the other hand, the decay setting simulates modifications to shortest paths, such as rural works or new route constructions, by adding or removing edges from the graph. These alterations may affect the ranks of a limited number of nodes, which corresponds to the time steps in the decay setting.

We present below the experiment results obtained with the maps of Paris and London. More experiments with additional city maps and synthetic graphs are in Appendix F. The city maps were obtained using the Python library Osmnx (Boeing, 2017). Figures 5 and 5 respectively illustrate the results in the _class_ and the _decay_ settings with _node rank predictions_. In both figures, for each city, we present the numbers of comparisons used for the same task by a binary and Fibonacci heap, and the number of comparisons used when the priority queue is augmented with _key rank predictions_.

In both settings, the performance of the LAPQ substantially improves with the quality of the predictions, and notably, _key rank predictions_ yield almost the same performance as perfect _node rank predictions_, affirming our intuition on the similarity between the keys inserted in runs of Dijkstra's algorithm starting from different sources.

## References

* Anand et al. (2020) Keerti Anand, Rong Ge, and Debmalya Panigrahi. Customizing ml predictions for online algorithms. In _International Conference on Machine Learning_, pages 303-313. PMLR, 2020.
* Anand et al. (2018)

Figure 4: Shortest path, _class_ predictions Figure 5: Shortest path, _decay_ predictions

Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching problems with machine learned advice. _Advances in Neural Information Processing Systems_, 33:7933-7944, 2020.
* Antoniadis et al. [2021] Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Learning-augmented dynamic power management with multiple states via new ski rental bounds. _Advances in Neural Information Processing Systems_, 34:16714-16726, 2021.
* Antoniadis et al. [2023a] Antonios Antoniadis, Joan Boyar, Marek Elias, Lene Monrad Favrholdt, Ruben Hoeksma, Kim S Larsen, Adam Polak, and Bertrand Simon. Paging with succinct predictions. In _International Conference on Machine Learning_, pages 952-968. PMLR, 2023a.
* Antoniadis et al. [2023b] Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online metric algorithms with untrusted predictions. _ACM Transactions on Algorithms_, 19(2):1-34, 2023b.
* Bai and Coester [2023] Xingjian Bai and Christian Coester. Sorting with predictions. _Advances in Neural Information Processing Systems_, 36, 2023.
* Bamas et al. [2020] Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. _Advances in Neural Information Processing Systems_, 33:20083-20094, 2020.
* Bansal et al. [2022] Nikhil Bansal, Christian Coester, Ravi Kumar, Manish Purohit, and Erik Vee. Learning-augmented weighted paging. In _Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 67-89. SIAM, 2022.
* Basin et al. [2017] Dmitry Basin, Edward Bortnikov, Anastasia Braginsky, Guy Golan-Gueta, Eschar Hillel, Idit Keidar, and Moshe Sulamy. Kiwi: A key-value map for scalable real-time analytics. In _Proceedings of the 22Nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming_, pages 357-369, 2017.
* Bateni et al. [2023] MohammadHossein Bateni, Prathamesh Dharangutte, Rajesh Jayaram, and Chen Wang. Metric clustering and mst with strong and weak distance oracles. _arXiv preprint arXiv:2310.15863_, 2023.
* Benomar and Perchet [2024a] Ziyad Benomar and Vianney Perchet. Advice querying under budget constraint for online algorithms. _Advances in Neural Information Processing Systems_, 36, 2024a.
* Benomar and Perchet [2024b] Ziyad Benomar and Vianney Perchet. Non-clairvoyant scheduling with partial predictions. In _Forty-first International Conference on Machine Learning_, 2024b.
* Benomar et al. [2022a] Ziyad Benomar, Chaima Ghribi, Elie Cali, Alexander Hinsen, and Benedikt Jahnel. Agent-based modeling and simulation for malware spreading in d2d networks. AAMAS '22, page 91-99, Richland, SC, 2022a. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450392136.
* Benomar et al. [2022b] Ziyad Benomar, Chaima Ghribi, Elie Cali, Alexander Hinsen, Benedikt Jahnel, and Jean-Philippe Wary. _Multi-agent simulations for virus propagation in D2D 5G+ networks_. 2022b.
* Benomar et al. [2023] Ziyad Benomar, Evgenii Chzhen, Nicolas Schreuder, and Vianney Perchet. Addressing bias in online selection with limited budget of comparisons. _arXiv preprint arXiv:2303.09205_, 2023.
* Bhaskara et al. [2021] Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, and Manish Purohit. Logarithmic regret from sublinear hints. _Advances in Neural Information Processing Systems_, 34:28222-28232, 2021.
* Boeing [2017] Geoff Boeing. Osmnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks. _Computers, environment and urban systems_, 65:126-139, 2017.
* Borgstrom and Kosaraju [1993] Ryan S Borgstrom and S Rao Kosaraju. Comparison-based search in the presence of errors. In _Proceedings of the twenty-fifth annual ACM symposium on Theory of computing_, pages 130-136, 1993.
* Brodal [1996] Gerth Stolting Brodal. Worst-case efficient priority queues. In _Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '96, page 52-58, USA, 1996. Society for Industrial and Applied Mathematics. ISBN 0898713668.
* Brodal et al. [2020]Gerth Stolting Brodal. A survey on priority queues. In _Space-Efficient Data Structures, Streams, and Algorithms: Papers in Honor of J. Ian Munro on the Occasion of His 66th Birthday_, pages 150-163. Springer, 2013.
* Brodal and Okasaki (1996) Gerth Stolting Brodal and Chris Okasaki. Optimal purely functional priority queues. _Journal of Functional Programming_, 6(6):839-857, 1996.
* Brodal et al. (2012) Gerth Stolting Brodal, George Lagogiannis, and Robert E Tarjan. Strict fibonacci heaps. In _Proceedings of the forty-fourth annual ACM symposium on Theory of computing_, pages 1177-1184, 2012.
* Chazelle (2000) Bernard Chazelle. A minimum spanning tree algorithm with inverse-ackermann type complexity. _Journal of the ACM (JACM)_, 47(6):1028-1047, 2000.
* Chen et al. (2022) Justin Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via learned predictions. In _International Conference on Machine Learning_, pages 3583-3602. PMLR, 2022.
* Chen et al. (2007) Mo Chen, Rezail Alam Chowdhury, Vijaya Ramachandran, David Lan Roche, and Lingling Tong. Priority queues and dijkstra's algorithm. 2007.
* Chlkedowski et al. (2021) Jakub Chlkedowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz.Zolna. Robust learning-augmented caching: An experimental study. In _International Conference on Machine Learning_, pages 1920-1930. PMLR, 2021.
* Christianson et al. (2023) Nicolas Christianson, Junxuan Shen, and Adam Wierman. Optimal robustness-consistency tradeoffs for learning-augmented met last systems. In _International Conference on Artificial Intelligence and Statistics_, pages 9377-9399. PMLR, 2023.
* Crane (1972) Clark Allan Crane. _Linear lists and priority queues as balanced binary trees_. Stanford University, 1972.
* David (1963) Herbert Aron David. _The method of paired comparisons_, volume 12. London, 1963.
* Day and Edelsbrunner (1984) William HE Day and Herbert Edelsbrunner. Efficient algorithms for agglomerative hierarchical clustering methods. _Journal of classification_, 1(1):7-24, 1984.
* Diakonikolas et al. (2021) Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Ali Vakilian, and Nikos Zarifis. Learning online algorithms with distributional advice. In _International Conference on Machine Learning_, pages 2687-2696. PMLR, 2021.
* Dinitz et al. (2021) Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings via learned duals. _Advances in neural information processing systems_, 34:10393-10406, 2021.
* Eberle et al. (2024) Franziska Eberle, Felix Hommelsheim, Alexander Lindermayr, Zhenwei Liu, Nicole Megow, and Jens Schluter. Accelerating matroid optimization through fast imprecise oracles. _arXiv preprint arXiv:2402.02774_, 2024.
* Edelkamp and Wegener (2000) Stefan Edelkamp and Ingo Wegener. On the performance of weak-heapsort. In _Annual Symposium on Theoretical Aspects of Computer Science_, pages 254-266. Springer, 2000.
* Eisenberg (2008) Bennett Eisenberg. On the expectation of the maximum of iid geometric random variables. _Statistics & Probability Letters_, 78(2):135-143, 2008.
* Fredman and Tarjan (1987) Michael L Fredman and Robert Endre Tarjan. Fibonacci heaps and their uses in improved network optimization algorithms. _Journal of the ACM (JACM)_, 34(3):596-615, 1987.
* Fu et al. (2024) Chunkai Fu, Jung Hoon Seo, and Samson Zhou. Learning-augmented skip lists. _arXiv preprint arXiv:2402.10457_, 2024.
* Gambin and Malinowski (1998) Anna Gambin and Adam Malinowski. Randomized meldable priority queues. In _International Conference on Current Trends in Theory and Practice of Computer Science_, pages 344-349. Springer, 1998.
* Gershman et al. (2012)* Ge and Zdonik (2008) Tingjian Ge and Stan Zdonik. A skip-list approach for efficiently processing forecasting queries. _Proceedings of the VLDB Endowment_, 1(1):984-995, 2008.
* Ghoshdastidar et al. (2019) Debarghya Ghoshdastidar, Michael Perrot, and Ulrike von Luxburg. Foundations of comparison-based hierarchical clustering. _Advances in neural information processing systems_, 32, 2019.
* Gloaguen and Cali (2018) Catherine Gloaguen and Elie Cali. Cost estimation of a fixed network deployment over an urban territory. _Annals of Telecommunications_, 73:367-380, 2018.
* Gloaguen et al. (2006) Catherine Gloaguen, Frank Fleischer, Hendrik Schmidt, and Volker Schmidt. Fitting of stochastic telecommunication network models via distance measures and monte-carlo tests. _Telecommunication Systems_, 31:353-377, 2006.
* Goh and Thng (2004) Rick Siow Mong Goh and Ian Li-Jin Thng. Dsplay: An efficient dynamic priority queue structure for discrete event simulation. In _Proceedings of the SimTecT Simulation Technology and Training Conference_. Citeseer, 2004.
* Gollapudi and Panigrahi (2019) Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In _International Conference on Machine Learning_, pages 2319-2327. PMLR, 2019.
* Gonnet and Munro (1986) Gaston H Gonnet and J Ian Munro. Heaps on heaps. _SIAM Journal on Computing_, 15(4):964-971, 1986.
* Haghiri et al. (2017) Siavash Haghiri, Debarghya Ghoshdastidar, and Ulrike von Luxburg. Comparison-based nearest neighbor search. In _Artificial Intelligence and Statistics_, pages 851-859. PMLR, 2017.
* Haghiri et al. (2018) Siavash Haghiri, Damien Garreau, and Ulrike Luxburg. Comparison-based random forests. In _International Conference on Machine Learning_, pages 1871-1880. PMLR, 2018.
* Heckel et al. (2018) Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate ranking from pairwise comparisons. In _International Conference on Artificial Intelligence and Statistics_, pages 1057-1066. PMLR, 2018.
* Hu et al. (2003) Yih-Chun Hu, Adrian Perrig, and David B Johnson. Efficient security mechanisms for routing protocols. In _Ndss_. Citeseer, 2003.
* Im et al. (2022) Sungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented caching. In _International Conference on Machine Learning_, pages 9588-9601. PMLR, 2022.
* Jaiswal (1968) Narendra Kumar Jaiswal. _Priority queues_, volume 50. Academic press New York, 1968.
* Kraska et al. (2018) Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures. In _Proceedings of the 2018 international conference on management of data_, pages 489-504, 2018.
* Larkin et al. (2014) Daniel H Larkin, Siddhartha Sen, and Robert E Tarjan. A back-to-basics empirical study of priority queues. In _2014 Proceedings of the Sixteenth Workshop on Algorithm Engineering and Experiments (ALENEX)_, pages 61-72. SIAM, 2014.
* Lassota et al. (2023) Alexandra Anna Lassota, Alexander Lindermayr, Nicole Megow, and Jens Schloter. Minimalistic predictions to schedule jobs with online precedence constraints. In _International Conference on Machine Learning_, pages 18563-18583. PMLR, 2023.
* Lattanzi et al. (2023) Silvio Lattanzi, Ola Svensson, and Sergei Vassilvitskii. Speeding up bellman ford via minimum violation permutations. In _International Conference on Machine Learning, ICML 2023_, 2023.
* Lewis (2023) Rhyd Lewis. A comparison of dijkstra's algorithm using fibonacci heaps, binary heaps, and self-balancing binary trees. _arXiv preprint arXiv:2303.10034_, 2023.
* Lin et al. (2022) Honghao Lin, Tian Luo, and David Woodruff. Learning augmented binary search trees. In _International Conference on Machine Learning_, pages 13431-13440. PMLR, 2022.
* Linden and Jonsson (2013) Jonatan Linden and Bengt Jonsson. A skiplist-based concurrent priority queue with minimal memory contention. In _Principles of Distributed Systems: 17th International Conference, OPODIS 2013, Nice, France, December 16-18, 2013. Proceedings 17_, pages 206-220. Springer, 2013.
* Liu et al. (2019)* Lindermayr and Megow (2022) Alexander Lindermayr and Nicole Megow. Algorithms with predictions. https://algorithms-with-predictions.github.io, 2022.
* Lykouris and Vassilvitskii (2018) Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. In _International Conference on Machine Learning_, pages 3296-3305. PMLR, 2018.
* Maghakian et al. (2023) Jessica Maghakian, Russell Lee, Mohammad Hajiesmaili, Jian Li, Ramesh Sitaraman, and Zhenhua Liu. Applied online algorithms with heterogeneous predictors. In _International Conference on Machine Learning_, pages 23484-23497. PMLR, 2023.
* McCauley et al. (2024) Samuel McCauley, Ben Moseley, Aidin Niaparast, and Shikha Singh. Online list labeling with predictions. _Advances in Neural Information Processing Systems_, 36, 2024.
* Meister and Nietert (2021) Michela Meister and Sloan Nietert. Learning with comparison feedback: Online estimation of sample statistics. In _Algorithmic Learning Theory_, pages 983-1001. PMLR, 2021.
* Merlis et al. (2023) Nadav Merlis, Hugo Richard, Flore Sentenac, Corentin Odic, Mathieu Molina, and Vianney Perchet. On preemption and learning in stochastic scheduling. In _International Conference on Machine Learning_, pages 24478-24516. PMLR, 2023.
* Mitzenmacher and Vassilvitskii (2022) Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. _Communications of the ACM_, 65(7):33-35, 2022.
* Moon et al. (2000) Sung-Whan Moon, Jennifer Rexford, and Kang G Shin. Scalable hardware priority queue architectures for high-speed packet switches. _IEEE Transactions on computers_, 49(11):1215-1227, 2000.
* Nowak (2009) Robert Nowak. Noisy generalized binary search. _Advances in neural information processing systems_, 22, 2009.
* Olson (1995) Clark F Olson. Parallel algorithms for hierarchical clustering. _Parallel computing_, 21(8):1313-1325, 1995.
* Patrascu and Thorup (2006) Mihai Patrascu and Mikkel Thorup. Time-space trade-offs for predecessor search. In _Proceedings of the thirty-eighth annual ACM symposium on Theory of computing_, pages 232-240, 2006.
* Perrot et al. (2020) Michael Perrot, Pascal Esser, and Debarghya Ghoshdastidar. Near-optimal comparison based clustering. _Advances in Neural Information Processing Systems_, 33:19388-19399, 2020.
* Pugh (1990) William Pugh. Skip lists: a probabilistic alternative to balanced trees. _Communications of the ACM_, 33(6):668-676, 1990.
* Purohit et al. (2018) Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. _Advances in Neural Information Processing Systems_, 31, 2018.
* Ronngren and Ayani (1997) Robert Ronngren and Rassul Ayani. A comparative study of parallel and sequential priority queue algorithms. _ACM Transactions on Modeling and Computer Simulation (TOMACS)_, 7(2):157-209, 1997.
* Sadek and Elias (2024) Karim Ahmed Abdel Sadek and Marek Elias. Algorithms for caching and MTS with reduced number of predictions. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=QuIiLSkt04.
* Shah and Wainwright (2018) Nihar B Shah and Martin J Wainwright. Simple, robust and optimal ranking from pairwise comparisons. _Journal of machine learning research_, 18(199):1-38, 2018.
* Sharma et al. (2022) Avinash Sharma, Dankan Gowda, Anil Sharma, S Kumaraswamy, MR Arun, et al. Priority queueing model-based iot middleware for load balancing. In _2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS)_, pages 425-430. IEEE, 2022.
* Shavit and Lotan (2000) Nir Shavit and Itay Lotan. Skiplist-based concurrent priority queues. In _Proceedings 14th International Parallel and Distributed Processing Symposium. IPDPS 2000_, pages 263-268. IEEE, 2000.
* Shavit et al. (2019)Yongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learning-augmented algorithms for the multi-option ski rental problem via best-possible competitive analysis. _arXiv preprint arXiv:2302.06832_, 2023.
* Silwal et al. (2023) Sandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, and Seyed Mehran Kazemi. Kwikbucks: Correlation clustering with cheap-weak and expensive-strong signals. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=p0JSSa1Av.
* Sundell and Tsigas (2005) Hakan Sundell and Philippas Tsigas. Fast and lock-free concurrent priority queues for multi-thread systems. _Journal of Parallel and Distributed Computing_, 65(5):609-627, 2005.
* Thorup (2007) Mikkel Thorup. Equivalence between priority queues and sorting. _Journal of the ACM (JACM)_, 54(6):28-es, 2007.
* Tschopp et al. (2011) Dominique Tschopp, Suhas Diggavi, Payam Delgosha, and Soheil Mohajer. Randomized algorithms for comparison-based search. _Advances in Neural Information Processing Systems_, 24, 2011.
* van Emde Boas (1977) Peter van Emde Boas. Preserving order in a forest in less than logarithmic time and linear space. _Information processing letters_, 6(3):80-82, 1977.
* van Emde Boas et al. (1976) Peter van Emde Boas, Robert Kaas, and Erik Zijlstra. Design and implementation of an efficient priority queue. _Mathematical systems theory_, 10(1):99-127, 1976.
* Vuillemin (1978) Jean Vuillemin. A data structure for manipulating priority queues. _Communications of the ACM_, 21(4):309-315, 1978.
* Wauthier et al. (2013) Fabian Wauthier, Michael Jordan, and Nebojsa Jojic. Efficient ranking from pairwise comparisons. In _International Conference on Machine Learning_, pages 109-117. PMLR, 2013.
* Williams (1964) J. W. J. Williams. Algorithm 232: Heapsort. _Communications of the ACM_, 7(6):347-348, 1964.
* Zeynali et al. (2024) Ali Zeynali, Shahin Kamali, and Mohammad Hajiesmaili. Robust learning-augmented dictionaries. _arXiv preprint arXiv:2402.09687_, 2024.
* Zhang and Dechev (2015) Deli Zhang and Damian Dechev. A lock-free priority queue design based on multi-dimensional linked lists. _IEEE Transactions on Parallel and Distributed Systems_, 27(3):613-626, 2015.

## Appendix A Extended related work

Learning-augmented algorithmsLearning-augmented algorithms, introduced in (Lykouris and Vassilvitskii, 2018; Purohit et al., 2018), have captured increasing interest over the last years, as they allow breaking longstanding limitations in many algorithm design problems. Assuming that the decision-maker is provided with potentially incorrect predictions regarding unknown parameters of the problem, learning-augmented algorithms must be capable of leveraging these predictions if they are accurate (consistency), while keeping the worst-case performance without advice even if the predictions are arbitrarily bad or adversarial (robustness). Many fundamental online problems were studied in this setting, such as ski rental (Gollapudi and Panigrahi, 2019; Anand et al., 2020; Bamas et al., 2020; Diakonikolas et al., 2021; Antoniadis et al., 2021; Maghakian et al., 2023; Shin et al., 2023), scheduling (Purohit et al., 2018; Merlis et al., 2023; Lassota et al., 2023; Benomar and Perchet, 2024), matching (Antoniadis et al., 2020; Dinitz et al., 2021; Chen et al., 2022), and caching (Lykouris and Vassilvitskii, 2018; Chlledowski et al., 2021; Bansal et al., 2022, Antoniadis et al., 2023;a, Christianson et al., 2023). Data structures can also be improved within this framework. However, this remains underexplored compared to online algorithms. The seminal paper by Kraska et al. (2018) shows how predictions can be used to optimize space usage. Another study by (Lin et al., 2022) demonstrates that the runtime of binary search trees can be enhanced by incorporating predictions of item access frequency. Recent papers have extended this prediction model to other data structures, such as dictionaries (Zeynali et al., 2024) and skip lists (Fu et al., 2024). The prediction models we study deviate from the latter, and are more related to those considered respectively in (Bai and Coester, 2023) for sorting, and (McCauley et al., 2024) for online list labeling. An overview of the growing body of work on learning-augmented algorithms (also known as algorithms with predictions) is maintained at (Lindermayr and Megow, 2022).

Dirty comparisonsThe dirty and clean comparison model is also related to the prediction querying model, gaining a growing interest in the study of learning-augmented algorithms, where the decision-maker decides when to query predictions and for which items (Im et al., 2022; Benomar and Perchet, 2024; Sadek and Elias, 2024). In particular, having free-weak and costly-strong oracles has been studied in (Silwal et al., 2023) for correlation clustering, in (Bateni et al., 2023) for clustering and computing minimum spanning trees in a metric space, and in (Eberle et al., 2024) for matroid optimization. Another related setting involves the algorithm observing partial information online, which it can then use to decide whether to query a costly hint about the current item. This has been explored in contexts such as online linear optimization (Bhaskara et al., 2021) and the multicolor secretary problem (Benomar et al., 2023).

Priority queuesBinary heaps, introduced by Williams (1964), are one of the first efficient implementations of priority queues. They allow \(\mathrm{FindMin}\) in constant time, and the other operations in \(O(\log n)\) time, where \(n\) is the number of items in the queue. Shortly after, other heap-based implementations with similar guarantees were designed, such as leftist heaps (Crane, 1972) and randomized melabide priority queues (Gambin and Malinowski, 1998). A new idea was introduced in (Vuillenim, 1978) with Binomial heaps, where instead of having a single tree storing the items, a binomial heap is a collection of \(\Theta(\log n)\) trees with exponentially growing sizes, all satisfying the heap property. They allow insertion in constant amortized time, and \(O(\log n)\) time for \(\mathrm{ExtractMin}\) and \(\mathrm{DecreaseKey}\). A breakthrough came with Fibonacci heaps (Fredman and Tarjan, 1987), which allow all the operations in constant amortized time, except for \(\mathrm{ExtractMin}\), which takes \(O(\log n)\) time. It was shown later, in works such as (Brodal, 1996; Brodal et al., 2012), that the same guarantees can be achieved in the worst-case, not only in amortized time. Although they offer very good theoretical guarantees, Fibonacci heaps are known to be slow in practice (Larkin et al., 2014; Lewis,2023], and other implementations with weaker theoretical guarantees such as binary heaps are often preferred. We refer the interested reader to the detailed survey on priority queues by Brodal [2013].

Skip listsA skip list [Pugh, 1990] is a probabilistic data structure, based on classical linked lists, having shortcut pointers allowing fast access between non-adjacent elements. Due to the simplicity of their implementation and their strong performance, skip lists have many applications [Hu et al., 2003, Ge and Zdonik, 2008, Basin et al., 2017]. In particular, they can be used to implement priority queues [Ronngren and Ayani, 1997], guaranteeing in expectation a constant time for \(\mathrm{FindMin}\) and \(\mathrm{ExtractMin}\), and \(O(\log n)\) time for \(\mathrm{Insert}\) and \(\mathrm{DecreaseKey}\). They show particularly good performance compared to other implementations in the case of concurrent priority queues, where multiple users or machines can make requests to the priority queue [Shavit and Lotan, 2000, Linden and Jonsson, 2013, Zhang and Dechev, 2015].

## Appendix B Heap priority queues

### Exponential search and randomized binary search

The insertion algorithm we propose for binary heaps combines the classical exponential search and our randomized adaptation of the binary search algorithm. We provide a brief overview of the exponential search and its runtime, followed by an analysis of the runtime upper bound for our randomized binary search algorithm.

Exponential search.The exponential search of a target \(u\) in a sorted list \(L=(v_{1},\ldots,v_{k})\) starting from index \(i\in[k]\) consists in first comparing \(u\) to \(v_{i}\), and if \(v_{i}<u\) then \(u\) is compared to \(v_{i+1},v_{i+2},v_{i+4},\ldots\) until an integer \(\ell\) is found satisfying \(v_{i+2^{\ell}}<u\leq v_{i+2^{\ell+1}}\), a binary search is then conducted between indices \(i+2^{\ell}\) and \(i+2^{\ell+1}\) to find the position of \(u\). If the first comparison shows instead that \(u<v_{i}\), then \(u\) is compared to \(v_{i-1},v_{i-2},v_{i-4},\ldots\). Denoting by \(j\) the true position of \(u\) in \(L\), the exponential search terminates in \(O(\log|i-j|)\) time.

Randomized binary search (RBS).Consider a sorted list \(L=(v_{1},\ldots,v_{k})\in\mathcal{U}^{k}\) and a target \(u\in\mathcal{U}\). When the search of \(u\) in \(L\) is reduced to the sub-list \((v_{i},\ldots,v_{j})\), RBS samples an index

\[\ell\sim\mathcal{U}\text{inform}\left(\left\{i+\lceil\tfrac{j-i}{4}\rceil, \ldots,j-\lceil\tfrac{j-i}{4}\rceil\right\}\right)\,\]

if \(u=v_{\ell}\) then algorithm terminates, if \(u<v_{\ell}\) then the search is reduced to the sub-list \((v_{i},\ldots,v_{\ell-1})\), otherwise it is reduced to \((v_{\ell+1},\ldots,v_{j})\). This process iterates until the obtained sub-list has a size of \(1\), the position of \(u\) is then immediately deduced by comparing it to the single element in the sub-list.

**Lemma B.1**.: _RBS in a list of size \(k\) terminates in \(O(\log k)\) time with probability \(1\)._

Proof.: Let us denote by \(S_{t}\) the size of the sub-list to which the search is reduced after \(t\) steps, and \(T=\min\{t\geq 1:S_{t}=1\}\). For convenience, we consider that \(S_{t}=1\) for all \(t>T\). It holds that \(S_{0}=n\), and for all \(t\geq 1\), if \(S_{t}=j-i+1\geq 2\) then

\[S_{t+1}\leq\max(j-\ell,\ell-i)\leq j-i-\lceil\tfrac{j-i}{4}\rceil\leq\tfrac{3 }{4}(j-i)\leq\tfrac{3}{4}S_{t}\.\]

On the other hand, if \(S_{t}=1\) then \(S_{t+1}=1\). We deduce that \(S_{t+1}\leq\max(1,\tfrac{3}{4}S_{t})\) with probability \(1\), hence \(S_{t}\leq\max(1,(3/4)^{t}k)\) for all \(t\geq 1\). Therefore, it holds almost surely that

\[T\leq\lceil\log_{4/3}k\rceil\.\]

### Proof of Theorem 2.1

Proof.: The algorithm described in Theorem 2.1 runs RBS with dirty comparisons to obtain an estimated position \(\widehat{r}(u,L)\) of \(u\) in \(L\), then it conducts a clean exponential search starting from \(\widehat{r}(u,L)\) to find the exact position \(r(u,L)\). Lemma B.1 guarantees that the dirty RBS uses \(O(\log k)\) comparisons, while the clean exponential search terminates in expectation after \(O(\log|r(u,L)-\widehat{r}(u,L)|)\) comparisons. Therefore, for demonstrating Theorem 2.1, it suffices to prove that \(\mathbb{E}[\log|r(u,L)-\widehat{r}(u,L)|]=O(\log\eta(u,L))\).

To lighten the expressions, we write \(\eta\) instead of \(\eta(u,L)\) in the rest of the proof. Let \((i_{0},j_{0})=(1,n)\), and \((i_{t},j_{t})\) the indices delimiting the sub-list of \(L\) to which the RBS is reduced after \(t\) steps for all \(t\geq 1\). Denoting by \(t^{*}+1\) the first step where a dirty comparison is inaccurate, it holds that \(\widehat{r}(u,L),r(u,L)\in\{i_{t^{*}},j_{t^{*}}\}\). Indeed, the first \(t^{*}\) dirty comparisons are all accurate, thus the estimated and the true positions of \(u\) in \(L\) are both in \(\{i_{t^{*}},j_{t^{*}}\}\). Therefore, \(|r(u,L)-\widehat{r}(u,L)|\leq S_{t^{*}}-1\), where \(S_{t}=j_{t}-i_{t}+1\) is the size of the sub-list delimited by indices \((i_{t},j_{t})\), which yields

\[\log|r(u,L)-\widehat{r}(u,L)|\leq\log(S_{t^{*}})\.\] (5)

We focus in the remainder on bounding \(\mathbb{E}[\log(S_{t^{*}})]\). For this, we use a proof scheme similar to that of Lemmas A.5 and A.5 in Bai and Coester (2023).

\[\mathbb{E}[\log(S_{t^{*}})] \leq\mathbb{E}[\lceil\log(S_{t^{*}})\rceil]\] \[\leq\sum_{m\geq 1}^{\infty}m\Pr(\lceil\log(S_{t^{*}})\rceil=m)\] \[\leq\log(\eta+1)+\sum_{m=\lceil\log(\eta+1)\rceil}^{\infty}m\Pr( \lceil\log(S_{t^{*}})\rceil=m)\,\] (6)

and for all \(m\geq 1\), we have that

\[\Pr(\lceil\log(S_{t^{*}})\rceil=m) =\Pr(S_{t^{*}}\in(2^{m-1},2^{m}])\] \[=\sum_{t=1}^{\infty}\Pr(S_{t}\in(2^{m-1},2^{m}],t=t^{*})\] \[=\sum_{t=1}^{\infty}\Pr(S_{t}\in(2^{m-1},2^{m}])\Pr(t=t^{*}\mid S_ {t}\in(2^{m-1},2^{m}])\.\]

In the sum above, for any \(t\geq 1\), the probability that \(t=t^{*}\) is the probability that the next sampled pivot index \(\ell_{t}\) is in the set \(\mathcal{F}(u,L)=\{v\in L:\mathbbm{1}(u\lesssim v)\neq\mathbbm{1}(u<v)\}\), which has a cardinal \(\eta\). Thus

\[\Pr(t=t^{*}\mid S_{t})=\Pr(\ell_{t}\in\mathcal{F}(u,L)\mid S_{t})=\frac{\#( \mathcal{F}\cap\{i_{t},\ldots,j_{t}\})}{S_{t}}\leq\frac{\eta}{S_{t}}\.\]

It follows that

\[\Pr(\lceil\log(S_{t^{*}})\rceil=m) \leq\frac{\eta}{2^{m-1}}\sum_{t=1}^{\infty}\Pr(S_{t}\in(2^{m-1},2 ^{m}])\] \[\leq\frac{\eta}{2^{m-1}}\mathbb{E}[\#\{t\geq 1:S_{t}\in(2^{m-1},2^{m }]\}]\] \[\leq\frac{3\eta}{2^{m-1}}\,\]

where the last inequality is an immediate consequence of the identity \(S_{t+1}\leq\frac{3}{4}S_{t+1}\) proved in Lemma B.1, which shows in particular that \(S_{t+3}<S_{t}/2\), i.e. the after at most \(3\) steps, the size of the search sub-list is divided by two, hence \((S_{t})_{t}\) falls into the interval \((2^{m-1},2^{m}]\) at most three times. Substituting into (6) gives

\[\mathbb{E}[\log S_{t^{*}}] \leq\log(\eta+1)+3\eta\cdot\sum_{m=\lceil\log(\eta+1)\rceil}^{ \infty}\frac{m}{2^{m-1}}\] \[=\log(\eta+1)+3\eta\cdot O\left(\frac{\log\eta}{\eta}\right)\] \[=O(\log\eta)\.\]

Therefore, by (5), the expected number of comparisons used during the clean exponential search is at most \(O(\mathbb{E}[\log|r(u,L)-\widehat{r}(u,L)|])=O(\log\eta)\), which concludes the proof.

### Proof of Theorem 2.2

Proof.: Consider a binary heap \(\mathcal{Q}\) containing \(n\) elements, with positions randomly filled in the leaf level. Even with this randomization, \(\mathcal{Q}\) is still balanced, and the length of any path from an empty position in the leaf level to the root has a size \(O(\log n)\). Denoting by \(L\) the random insertion path chosen in Algorithm 1, constructing \(L\) and storing it requires \(O(\log n)\) time and space. By Theorem 2.1, the randomized search in Algorithm 1 uses \(O(\log\log n)\) dirty comparisons, and the exponential search uses an expected number of \(O(\mathbb{E}[\log\eta(u,L)])\) clean comparisons. Inserting \(u\) and then swapping it up until its position does not use any comparison and requires a \(O(\log n)\) time. Therefore, to prove the theorem, it suffices to demonstrate that \(\mathbb{E}[\log\eta(u,L)]=O(\log\log\eta(u,\mathcal{Q}))\).

We assume that the key \(u\) to be inserted can be chosen by an oblivious adversary, unaware of the randomization outcome. This means that the internal state of \(\mathcal{Q}\) remains private at any time. Let \(\mathcal{F}(u,\mathcal{Q})=\{v\in\mathcal{Q}:\mathbbm{1}(u\stackrel{{ <}}{{<}}v)\neq\mathbbm{1}(u<v)\}\), which has a cardinal of \(\eta(u,\mathcal{Q})\). Enumerating the binary heap levels starting from the root, each level \(\ell\) except for the last one, denoted \(\ell_{\max}\), contains exactly \(2^{\ell-1}\) elements \(v_{1}^{\ell},\ldots,v_{2^{\ell-1}}^{\ell}\). A key \(v_{i}^{\ell}\) in level \(\ell\) has a probability \(1/2^{\ell-1}\) of belonging to \(L\). Denoting by \(\xi_{i}^{\ell}=\mathbbm{1}(v_{i}^{\ell}\in\mathcal{F}(u,\mathcal{Q}))\), the expected number of keys in \(L\) belonging to \(\mathcal{F}(u,\mathcal{Q})\) is

\[\mathbb{E}[\eta(u,L)]=\sum_{\ell=1}^{\ell_{\max}}\sum_{i=1}^{2^{\ell-1}}\frac{ \xi_{i}^{\ell}}{2^{\ell-1}}=\sum_{\ell=1}^{\ell_{\max}}\frac{1}{2^{\ell-1}} \left(\sum_{i=1}^{2^{\ell-1}}\xi_{i}^{\ell}\right)\.\]

Given that \(\sum_{\ell=1}^{\ell_{\max}}\sum_{i=1}^{2^{\ell-1}}\xi_{i}^{\ell}=\eta(u, \mathcal{Q})\leq 2^{\lceil\log(\eta(u,\mathcal{Q})+1)\rceil}-1\), the expression above is maximized under this constraint for the instance \(\bar{\xi}_{i}^{\ell}=\mathbbm{1}(\ell\leq\lceil\log(\eta(u,\mathcal{Q})+1) \rceil)\). Therefore,

\[\mathbb{E}[\eta(u,L)]\leq\sum_{\ell}\frac{1}{2^{\ell-1}}\left(\sum_{i=1}^{2^{ \ell-1}}\bar{\xi}_{i}^{\ell}\right)=\lceil\log(\eta(u,\mathcal{Q})+1)\rceil\.\]

Finally, Jensen's inequality and the concavity of \(\log\) yield

\[\mathbb{E}[\log\eta(u,L)]\leq\log\mathbb{E}[\eta(u,L)]=O(\log\log\eta(u, \mathcal{Q}))\,\]

which gives the result.

## Appendix C Skip lists

### Unique keys in the priority queue

The keys of the priority queue can be considered pairwise distinct by grouping elements with the same key together in a collection, for example, a HashSet. This collection can be accessible in \(O(1)\) time via the key using a HashMap. When a new item \(x\) with priority \(u\) is to be inserted, the algorithms first checks if the key \(u\) is already in the priority queue, if that is the case then \(x\) is simply added to the collection corresponding to \(u\) in \(O(1)\) time. Otherwise, the key \(u\) must first be inserted into its correct position.

With such implementation, when an \(\mathrm{ExtractMin}\) operation is called, if multiple elements correspond to the minimum key, then the algorithm can for example retrieve an arbitrary one of them, of the first inserted one, depending on the use case.

### Expected maximum of i.i.d. geometric random variables

Before presenting the insertion algorithms in the three prediction models, we present an upper bound from [Eisenberg, 2008] on the expected maximum of i.i.d. geometric random variables with parameter \(p\). This Lemma will be useful in the analysis of our algorithms, as the heights of elements in the skip list are i.i.d. geometric random variables. The following Lemma is an immediate consequence of

**Lemma C.1** ([Eisenberg, 2008]).: _If \(X_{1},\ldots,X_{m}\) are i.i.d. random variables following a geometric random distribution with parameter \(p\), then, denoting by \(q=1-p\), it holds that_

\[\mathbb{E}[\max_{i\in[m]}X_{i}]\leq 1+\frac{1}{\log(1/q)}\sum_{k=1}^{m}\frac{1}{k }=O(\log_{1/q}m)\.\]

### Pointer prediction model

Proof of Theorem 3.1.: The key \(w\) found at the end of the algorithm is the processor of \(u\) in \(\mathcal{Q}\). Inserting \(u\) next to \(w\) only requires expected \(O(1)\) time. Thus, we demonstrate in the following that Algorithm 2, starting from a key \(v_{j}\in\mathcal{Q}\), finds the predecessor of \(u\) in \(O(\log|r(v_{j})-r(u)|)\) expected time, where \(r(v)\) denotes the rank \(r(v,\mathcal{Q})\) of \(v\) in \(\mathcal{Q}\). In particular, for \(v_{j}=\operatorname{\overline{Pred}}(u,\mathcal{Q})\), we obtain the claim of the theorem.

We assume in the proof that \(v_{j}<u\), i.e. the exponential search goes from left to right. Let \(h^{*}(v_{j},u)\) be the maximum height of all elements in \(\mathcal{Q}\) between \(u\) and \(v_{j}\)

\[h^{*}(v_{j},u)=\max\{h(v):v\in\mathcal{Q}\text{ such that }v_{j}\leq v\leq u\}\.\]

The number of elements between \(v_{j}\) and \(u\), with \(v_{j}\) included, is \(|r(u)-r(v_{j})|+1\), and the heights of all the elements in \(\mathcal{Q}\) are independent geometric random variables with parameter \(p\), thus Lemma C.1 gives that \(\mathbb{E}[h^{*}(v_{j},u)]=O(\log|r(u)-r(v_{j})|)\).

The key \(w^{*}\) found at the end of the Bottom-Up search is the last element, going from \(v_{j}\) to \(u\), having a height of \(h^{*}(v_{j},u)\). Indeed, in the Bottom-Up search, whenever the algorithm reaches a new key, it moves to the maximum level to which the key belongs, the height of \(w^{*}\) is therefore necessarily the maximum height of all the keys between \(v_{j}\) and \(w^{*}\), i.e. \(h(w^{*})=h^{*}(v_{j},w^{*})\). Since the Bottom-Up search stops at key \(w^{*}\), then \(\operatorname{Next}(w^{*},h(w^{*}))>u\), which means that there is no key in \(\mathcal{Q}\) between \(w^{*}\) and \(u\) having a height more than \(h(w^{*})-1\).

The number of comparisons made in this phase is therefore at most the number of comparisons needed to reach level \(h^{*}(v_{j},u)+1\) starting from \(v_{j}\) using the Bottom-Up search. We consider the hypothetical setting where the skip list is infinite to the right, the expected number of comparisons to reach level \(h^{*}(v_{j},u)+1\), in this case, is an upper bound on the expected number of comparisons needed in the Bottom-Up phase of Algorithm 2, as the algorithm also terminates if the end of the skip-list is reached. Let \(T(\ell)\) be the expected number of comparisons made in the bottom-up search to reach level \(\ell\) in an infinite skip list. After each comparison made in the bottom-up search, it is possible to go at least one level up with probability \(p\), while the algorithm can only move horizontally to the right with probability \(1-p\). This induces the inequality

\[T(\ell)\leq 1+pT(\ell-1)+(1-p)T(\ell)\,\]

which yields

\[T(\ell)\leq\frac{1}{p}+T(\ell-1)\.\]

Given that \(T(1)=0\), we have for \(\ell\geq 1\) that \(T(\ell)\leq\frac{\ell-1}{p}\), and we deduce that the expected number of comparisons made by the algorithm during the Bottom-Up search is at most \(\frac{\mathbb{E}[h^{*}(v_{j},u)]}{p}=O(\log|r(v_{j})-r(u)|)\).

In the Top-Down search described in the second phase, the path traversed by the algorithm is exactly the inverse of the Bottom-Up search from the predecessor of \(u\) to \(w^{*}\). The same arguments as the analysis of the first phase give that the Top-Down search terminates after \(O(\log|r(v_{j})-r(u)|)\) comparisons in expectation, which concludes the proof. 

### Dirty comparison model

Proof of Theorem 3.2.: Let \(\mathcal{F}(u,\mathcal{Q})\) the set of keys in \(\mathcal{Q}\) whose dirty comparisons with \(u\) are inaccurate

\[\mathcal{F}(u,\mathcal{Q})=\{v\in\mathcal{Q}:\mathbbm{1}(u\ \widehat{<}\ v)\neq \mathbbm{1}(u<v)\}\.\]

The prediction error \(\eta(u,\mathcal{Q})\) defined in (2) is the cardinal of \(\mathcal{F}(u,\mathcal{Q})\). Let

\[h^{*}(\mathcal{F}(u,\mathcal{Q}))=\max\{h(v):v\in\mathcal{F}(u,\mathcal{Q})\}\]

be the maximal height of elements in \(\mathcal{F}(u,\mathcal{Q})\). The search algorithm \(\operatorname{Search}(\mathcal{Q},u)\) with dirty comparisons starts from the highest level at the head of the skip list, and then goes down the different levels until finding the predicted position \(\hat{w}\) of \(u\). This is the classical search algorithm in skip lists, and it is known to require \(O(\log n)\) comparisons to terminate. This can also be deduced from theanalysis of the exponential search described in Algorithm 2, as it corresponds to the Top-Down search starting from the head of the skip list.

Before level \(h^{*}(\mathcal{F}(u,\mathcal{Q}))\) is reached in \(\mathrm{Search}(\mathcal{Q},u)\), all the dirty comparisons are accurate. Denoting by \(v^{\prime}\) the last key in \(\mathcal{Q}\) visited in a level higher than \(h^{*}(\mathcal{F}(u,\mathcal{Q}))\) during this search, and \(v^{\prime\prime}=\mathrm{Next}(\mathcal{Q},v^{\prime},h^{*}(\mathcal{F}(u, \mathcal{Q})))\), it holds that both keys \(\hat{w}\) and \(w\) are between \(v^{\prime}\) and \(v^{\prime\prime}\), and there is no key in \(\mathcal{Q}\) between \(v^{\prime}\) and \(v^{\prime\prime}\) with height more than \(h^{*}(\mathcal{F}(u,\mathcal{Q}))-1\).

In particular, the maximal key height between \(\hat{w}\) and \(w\) is at most \(h^{*}(\mathcal{F}(u,\mathcal{Q}))-1\). We showed in the proof of Theorem 3.1 that the number of comparisons and runtime of \(\mathrm{ExpSearchInsertion}(\mathcal{Q},v_{j},u)\) is linear with the maximal height of keys in \(\mathcal{Q}\) that are between \(v_{j}\) and \(u\). Using this result with \(\hat{w}\) instead of \(v_{j}\) gives that \(\mathrm{ExpSearchInsertion}(\mathcal{Q},\hat{w},u)\) finds the position of \(u\) using \(O(h^{*}(\mathcal{F}(u,\mathcal{Q})))\) clean comparisons. Finally, since \(h^{*}(\mathcal{F}(u,\mathcal{Q}))\) is the maximum of a number \(\eta(u,\mathcal{Q})\) of i.i.d. geometric random variables with parameter \(p\), Lemma C.1 gives that

\[\mathbb{E}[h^{*}(\mathcal{F}(u,\mathcal{Q}))]=O(\log\eta(u,\mathcal{Q}))\;,\]

which proves the theorem. 

### Rank prediction model

To leverage rank predictions, as explained in Section 3.3, we use an auxiliary van Emde Boas (vEB) tree van Emde Boas (1977). We describe in the following the structure ad complexity of vEB trees on \([N]\), and we explain how they can be adapted in the case where \(N\) is unknown.

#### c.5.1 Van Emde Boas (vEB) trees

A vEB tree over an interval \(\{i+1,\ldots,i+m\}\) has a root with \(\sqrt{m}\) children, each being the root of a smaller vEB tree over a sub-interval \(\{i+k\sqrt{m}+1,\ldots,i+(k+1)\sqrt{m}\}\) for some \(k\in\{0,\ldots,\sqrt{m}-1\}\). The tree leaves are either empty or contain elements with the corresponding key, stored together in a collection, and internal nodes carry binary information indicating whether or not the subtree they root contains at least one element. Denoting by \(H(m)\) the height of a vEB tree of size \(m\), it holds that \(H(m)=1+H(\sqrt{m})\), which yields that \(H(m)=O(\log\log m)\), enabling efficient implementation of the operations listed below in \(O(\log\log m)\) time:

* \(\mathrm{Insert}(x,k)\): insert a new element \(x\) with key \(k\in[m]\) in the tree,
* \(\mathrm{Delete}(x,k)\): Delete the element/key pair \((x,k)\),
* \(\mathrm{Predecessor}(k)\): return the element in the tree with the largest key smaller than or equal to \(k\),
* \(\mathrm{Successor}(k)\): return the element in the tree with the smallest key larger than or equal to \(k\),
* \(\mathrm{ExtractMin}()\): removes and returns the element with the smallest key.

Other operations such as \(\mathrm{FindMin}\), \(\mathrm{FindMax}\), or \(\mathrm{Lookup}(k)\) are supported in \(O(1)\) time. These runtimes, however, require knowing the maximal key value \(m\) from the beginning, as it is used for constructing \(\mathcal{T}_{m}\).

#### c.5.2 Dynamic size vEB trees

If the maximal key value \(\bar{R}\) is unknown, we argue that the operations listed above can be supported in amortized \(O(\log\log\bar{R})\) time. Given a vEB tree \(\mathcal{T}_{m}\) of size \(m\), if a new key \(k\in\{m+1,\ldots,2m\}\) is to be inserted, we construct an empty vEB tree \(\mathcal{T}_{2m}\) of size \(2m\) in \(O(m)\) time, then repeatedly extract the elements with minimal key from \(\mathcal{T}_{m}\) and insert them in \(\mathcal{T}_{2m}\) with the same key. Each \(\mathrm{ExtractMin}\) operation in \(\mathcal{T}_{m}\) and insertion in \(\mathcal{T}_{2m}\) requires \(O(\log\log m)\) time. Therefore, constructing \(\mathcal{T}_{2m}\) and inserting all the elements from \(\mathcal{T}_{m}\) takes \(O(m\log\log m)\) time.

This observation can be used to define a vEB with dynamic size. First, we construct a vEB tree with an initial constant size \(R_{0}\). If at some point the size of the vEB tree is \(m\geq R_{0}\) and a new key \(k>m\) is to be inserted, then we iterate the size doubling process described before until the size of the vEB tree is at least \(k\). At any time step, denoting by \(\bar{R}\) the maximal key value inserted in the vEB tree, and letting \(i\geq 1\) such that \(2^{i-1}R_{0}\leq\bar{R}<2^{i}R_{0}\), the size of the tree has been doubled up to this step times to cover all the keys. The total time for resizing the vEB tree is at most proportional to

\[\sum_{j=0}^{i-1}2^{j}R_{0}\log\log(2^{j}R_{0}) \leq\Big{(}\sum_{j=0}^{i-1}2^{j}\Big{)}R_{0}\log\log\bar{R}\] \[\leq 2^{i}R_{0}\log\log\bar{R}\] \[\leq 2\bar{R}\log\log\bar{R}\.\]

Therefore, if \(N\) is the total number of elements inserted into the vEB tree and \(\bar{R}\) the maximum key value, we can neglect the cost of resizing by considering that each insertion requires an amortized time of \(O((1+\frac{\bar{R}}{N})\log\log\bar{R})\). The runtime of all the other operations is \(O(\log\log\bar{R})\). In particular, if \(\bar{R}=O(N)\), then all the operations run in \(O(\log\log N)\) amortized time.

#### c.5.3 Priority queue with rank predictions

Consider the setting where \(N\) is unknown and all the predicted ranks, revealed online, satisfy \(R(u_{i})=O(N)\). The priority queue we consider is a skip list \(\mathcal{Q}\) with an auxiliary dynamic vEB tree \(\mathcal{T}\). For insertions, we use Algorithm 4. For \(\mathrm{ExtractMin}\), we first extract the minimum \(u_{\min}\) from \(\mathcal{Q}\) in \(O(1)\) time, then we delete it from the corresponding position \(\widehat{R}(u_{\min})\) in \(\mathcal{T}\) in \(O(\log\log N)\) time. Deleting an arbitrary key \(u\) from \(\mathcal{Q}\) can be done in the same way, by removing it from \(\mathcal{Q}\) in expected \(O(1)\) time and then deleting it from the position \(\widehat{R}(u)\) in \(\mathcal{T}\) in \(O(\log\log N)\) time. Thus, as in the other prediction models, \(\mathrm{DecreaseKey}\) can be implemented by deleting the element and reinserting it with the new key, which requires the same complexity as insertion, with an additional \(O(\log\log N)\) term.

We will prove that the priority queue described above yields the claim of Theorem 3.3. We assume that the keys \(u_{1},\ldots,u_{N}\) are inserted in this order. For all \(t\in[N]\), we denote by \(\mathcal{Q}^{t}\), \(\mathcal{T}^{t}\) the set of keys in the skip list and the set of integer keys in the dynamic vEB tree right after the insertion of \(u_{t}\), with the keys in \(\mathcal{Q}^{t}\) or \(\mathcal{T}^{t}\). Note that, due to eventual deletions, the sizes of \(\mathcal{Q}^{t}\) and \(\mathcal{T}^{t}\) can be smaller than \(t\).

Following the definition of the rank in (1), for all \(i,t\in[N]\), we denote by \(r(u_{i},\mathcal{Q}^{t})\) the rank of \(u_{i}\) in \(\mathcal{Q}^{t}\), and \(r(\widehat{R}(u_{i}),\mathcal{T}^{t})\) the rank of \(\widehat{R}(u_{i})\) in \(\mathcal{T}^{t}\). The following lemma shows that the absolute difference between the two previous quantities for any given \(i,t\) is at most twice the maximal rank prediction error.

**Lemma C.2**.: _For any subset \(I\subset[N]\), it holds for all \(i\in I\) that_

\[|r(u_{i},\{u_{j}\}_{j\in I})-r(\widehat{R}(u_{i}),\{\widehat{R}(u_{j})\}_{j \in I})|\leq 2\max_{j\in[N]}\eta^{\Delta}(u_{j})\.\]

Proof.: Let

\[\Delta_{*}=\max_{I\subset[N]}\left(\max_{i\in I}|r(u_{i},\{u_{j}\}_{j\in I})- r(\widehat{R}(u_{i}),\{\widehat{R}(u_{j})\}_{j\in I})|\right)\,\] (7)

and let \(I\subset[N]\) for which this maximum is reached. We assume without loss of generality that \(I=[m]\). For all \(s\in[N]\), let

\[\tilde{\mathcal{Q}}^{s}=\{u_{j}\}_{j\in[s]},\quad\tilde{\mathcal{T}}^{s}=\{ \widehat{R}(u_{j})\}_{j\in[s]},\quad\Delta^{s}=\max_{i\in[s]}|r(u_{i},\tilde{ \mathcal{Q}}^{s})-r(\widehat{R}(u_{i}),\tilde{\mathcal{T}}^{s})|\.\]

To simplify the expressions, we denote by \(r_{i}^{s}=r(u_{i},\tilde{\mathcal{Q}}^{s})\) and \(\widehat{r}_{i}^{s}=r(\widehat{R}(u_{i}),\tilde{\mathcal{T}}^{s})\) for all \((s,i)\in[N]^{2}\). We will prove that \(\Delta^{s}=\Delta_{*}\) for all \(s\in\{m,\ldots,N\}\). By definition of \(\Delta_{*}\), it holds that \(\Delta_{*}\geq\Delta^{s}\) for all \(s\), it remains to prove the other inequality. It is true for \(s=m\) by definition of \(I\) and \(m\). Now let \(s\in\{m,\ldots,N-1\}\) and assume that \(\Delta^{s}=\Delta_{*}\), i.e. there exists \(i\leq s\) such that \(|r_{i}^{s}-\widehat{r}_{i}^{s}|=\Delta_{*}\). Assume for example that \(\widehat{r}_{i}^{s}=r_{i}^{s}+\Delta_{*}\).

* If \(u_{s+1}<u_{i}\), then \(r_{s+1}^{s+1}<r_{i}^{s+1}\) and \(r_{i}^{s+1}=r_{i}^{s}+1\). By definition of \(\Delta_{*}\), it holds that \[\widehat{r}_{s+1}^{s+1}\leq r_{s+1}^{s+1}+\Delta_{*}\leq r_{i}^{s+1}-1+\Delta_{ *}=r_{i}^{s}+\Delta_{*}=\widehat{r}_{i}^{s}\.\]This implies necessarily that \(\widehat{R}(u_{s+1})\leq\widehat{R}(u_{i})\), and therefore \[\widehat{r}_{i}^{s+1}=\widehat{r}_{i}^{s}+1=r_{i}^{s}+1+\Delta_{*}=r_{i}^{s+1}+ \Delta_{*}\;,\] which gives that \(\Delta^{s+1}\geq|\widehat{r}_{i}^{s+1}-r_{i}^{s+1}|=\Delta_{*}\).
* If \(u_{s+1}>u_{i}\), then \(r_{i}^{s+1}=r_{i}^{s}\) and \(\widehat{r}_{i}^{s+1}\geq\widehat{r}_{i}^{s}\), thus \(\Delta^{s+1}\geq\widehat{r}_{i}^{s+1}-r_{i}^{s+1}\geq\widehat{r}_{i}^{s}-r_{i} ^{s}=\Delta_{*}\).
* If \(u_{s+1}=u_{i}\) then \(r_{s+1}^{s+1}=r_{i}^{s+1}=r_{i}^{s}+1\). On the other hand, if \(\widehat{R}(u_{s+1})\leq\widehat{R}(u_{i})\) then \(\widehat{r}_{i}^{s+1}=\widehat{r}_{i}^{s}+1\), otherwise \(\widehat{r}_{s+1}^{s+1}\geq\widehat{r}_{i}^{s}+1\). In both cases, it holds that \[\Delta^{s+1} \geq\max(\widehat{r}_{i}^{s+1}-r_{i}^{s+1}\,,\,\widehat{r}_{s+1}^ {s+1}-r_{s+1}^{s+1})\] \[\geq(\widehat{r}_{i}^{s}+1)-(r_{i}^{s}+1)=\Delta_{*}\;.\]

The same proof can be used for the case where \(r_{i}^{s}=\widehat{r}_{i}^{s}+\Delta_{*}\). Therefore, we have for all \(s\in\{m,\ldots,N\}\) that \(\Delta_{*}=\Delta^{s}\). In particular, for \(s=N\), observing that \(r(u_{i},\tilde{\mathcal{Q}}^{N})=R(u_{i})\) for all \(i\in[N]\), we obtain

\[\Delta_{*}=\max_{i\in[N]}|R(u_{i})-r(\widehat{R}(u_{i}),\tilde{\mathcal{T}}^{ N})|\;.\] (8)

Let us denote by \(\eta_{\max}^{\Delta}=\max_{j\in[N]}\eta^{\Delta}(u_{j})\) the maximum rank prediction error. We will prove in the following that

\[\forall i\in[N]:\quad|R(u_{i})-r(\widehat{R}(u_{i}),\tilde{\mathcal{T}}^{N}) |\leq 2\eta_{\max}^{\Delta}\;.\]

With the assumption that the keys \(\{u_{i}\}_{i\in[N]}\) are pairwise distinct, the ranks \((R(u_{i}))_{i\in[N]}\) form a permutation of \([N]\).

Given that \(|R(u_{k})-\widehat{R}(u_{k})|\leq\eta_{\max}^{\Delta}\) for all \(k\in[N]\), it holds for any \(i,j\in[N]\) that

\[\widehat{R}(u_{j})\leq\widehat{R}(u_{i}) \implies R(u_{j})-\eta_{\max}^{\Delta}\leq R(u_{i})+\eta_{\max}^{\Delta}\] \[\implies R(u_{j})\leq R(u_{i})+2\eta_{\max}^{\Delta}\;,\]

hence, given that \(\mathcal{T}^{N}=\{\widehat{R}(u_{j})\}_{j\in[N]}\) and by definition (1) of the rank \(r(\widehat{R}(u_{i}),\mathcal{T}^{N})\), we have

\[r(\widehat{R}(u_{i}),\mathcal{T}^{N}) =\#\{j\in[N]:\widehat{R}(u_{j})\leq\widehat{R}(u_{i})\}\] \[\leq\#\{j\in[N]:R(u_{j})\leq R(u_{i})+2\eta_{\max}^{\Delta}\}\] \[=\#\{k\in[N]:k\leq R(u_{i})+2\eta_{\max}^{\Delta}\}\] (9) \[=\min(N\,,\,R(u_{i})+2\eta_{\max}^{\Delta})\] \[\leq R(u_{i})+2\eta_{\max}^{\Delta}\;,\] (10)

where Equation 9 holds because \((R(u_{i}))_{i\in[N]}\) is a permutation of \([N]\). Similarly, we have for all \(i,j\in[N]\) that

\[R(u_{j})\leq R(u_{i})-2\eta_{\max}^{\Delta} \implies R(u_{j})+\eta_{\max}^{\Delta}\leq R(u_{i})-\eta_{\max}^{\Delta}\] \[\implies\widehat{R}(u_{j})\leq\widehat{R}(u_{i})\;,\]

and it follows for all \(i\in[N]\) that

\[r(\widehat{R}(u_{i}),\mathcal{T}^{N}) =\#\{j\in[N]:\widehat{R}(u_{j})\leq\widehat{R}(u_{i})\}\] \[\geq\#\{j\in[N]:R(u_{j})\leq R(u_{i})-2\eta_{\max}^{\Delta}\}\] \[=\#\{k\in[N]:k\leq R(u_{i})-2\eta_{\max}^{\Delta}\}\] \[=\max(0\,,\,R(u_{i})-2\eta_{\max}^{\Delta})\] \[=R(u_{i})-2\eta_{\max}^{\Delta}\;.\] (11)

From (10) and (11), we deduce that

\[\forall i\in[N]:\quad|R(u_{i})-r(\widehat{R}(u_{i}),\mathcal{T}^{N})|\leq 2 \eta_{\max}^{\Delta}\;,\]

Combining this with (8) and (7) yields the wanted result.

Proof of Theorem 3.3

Proof.: When a new key \(u_{i}\) is to be inserted, it is first inserted in the dynamic vEB tree \(\mathcal{T}^{i}\) with integer key \(\widehat{R}(u_{i})\), and its predecessor \(\hat{w}\) in \(\mathcal{T}^{i}\) is retrieved. These first operations require \(O(\log\log N)\) time. The position of \(u_{i}\) in \(\mathcal{Q}^{i}\) is then obtained via an exponential search starting from \(\hat{w}\), which requires \(O(\log|r(u_{i},\mathcal{Q}^{i})-r(\hat{w},\mathcal{Q}^{i})|)\) expected time by Theorem 3.1. Finally, inserting \(u\) in the found position takes expected \(O(1)\) time.

For any newly inserted element, by accounting for the potential future deletion time via \(\mathrm{ExtractMin}\) at the moment of insertion, we can ensure that all \(\mathrm{ExtractMin}\) operations require constant amortized time. This approach results in only an additional \(O(\log\log N)\) time for insertions.

Therefore, to prove Theorem 3.3, we only need to show that \(\log|r(u_{i},\mathcal{Q}^{i})-r(\hat{w},\mathcal{Q}^{i})|=O(\log\max_{j\in[N]} \eta^{\Delta}(u_{i}))\). Since \(\hat{w}\) is the predecessor of \(u_{i}\) in \(\mathcal{T}^{i}\), it holds that \(r(\widehat{R}(u_{i}),\mathcal{T}^{i})\in\{r(\widehat{R}(\hat{w}),\mathcal{T} ^{i}),r(\widehat{R}(\hat{w},\mathcal{T}^{i})+1\}\), the first case occurs if \(\widehat{R}(u_{i})=\widehat{R}(\hat{w})\), and the second if \(\widehat{R}(u_{i})>\widehat{R}(\hat{w})\). Using this observation and Lemma C.2, it follows that

\[|r(u_{i},\mathcal{Q}^{i})-r(\hat{w},\mathcal{Q}^{i})|\] \[\leq|r(u_{i},\mathcal{Q}^{i})-r(\widehat{R}(u_{i}),\mathcal{T}^{ i})|+|r(\widehat{R}(u_{i}),\mathcal{T}^{i})-r(\widehat{R}(\hat{w}),\mathcal{T} ^{i})|+|r(\hat{w},\mathcal{Q}^{i})-r(\widehat{R}(\hat{w}),\mathcal{T}^{i})|\] \[\leq 4\max_{j\in[N]}\eta^{\Delta}(u_{j})+1\,\]

and it follows that \(\log|r(u_{i},\mathcal{Q}^{i})-r(\hat{w},\mathcal{Q}^{i})|=O(\log\max_{j\in[N] }\eta^{\Delta}(u_{i}))\), which concludes the proof. 

## Appendix D Lower bounds

A priority queue can be used for sorting a sequence \(A=(a_{i})_{i\in[n]}\in\mathcal{U}^{n}\), by first inserting all the elements in the priority queue, then repeatedly extracting the minimum until the priority queue is empty. In settings with dirty comparisons or _positional_ predictions, the number of comparisons required by this sorting algorithm is constrained by the impossibility result demonstrated in Theorem 1.5 of Bai and Coester (2023). We use this impossibility result to prove the lower bounds stated in Theorem 3.4.

### Impossibility result for sorting with predictions

We begin in this section by summarizing the setting and the impossibility result demonstrated in Theorem 1.5 of Bai and Coester (2023) for sorting with predictions.

Positional predictionsIn the _positional prediction_ model, the objective is to sort a sequence \(A=(a_{i})_{i\in[n]}\), given a prediction \(\widehat{R}_{i}\in[n]\) of \(R_{i}=r(a_{i},A)\) for all \(i\in[n]\). This model differs from our rank prediction model in that the sequence \(A\) and the predictions \(\widehat{R}=(\widehat{R}_{i})_{i\in[n]}\) are given offline to the algorithm. Two different error measures are considered in Bai and Coester (2023), but we restrict ourselves to the _displacement error_\(\eta_{i}^{\Delta}=|r(a_{i},A)-\widehat{R}_{i}|\), which is the same as our rank prediction error 4. In all the following, consider that \(\widehat{R}\) is a fixed permutation of \([n]\), i.e. the predicted ranks are pairwise distinct. For all \(\xi\geq 1\), consider the following set of instances

\[\text{cand}(\hat{R},n\xi)=\{A\in\mathcal{U}^{n}:\sum_{i=1}^{n}\log(\eta_{i}^{ \Delta}+2)\leq n\xi\}\.\]

The authors of Bai and Coester (2023) prove that, if \(1\leq\xi\leq O(\log n)\), then no algorithm can sort every instance from \(\text{cand}(\hat{R},n\xi)\) with \(o(n\xi)\) comparisons. However, in their proof, they demonstrate a stronger result: no algorithm can sort every instance from \(\mathcal{I}_{n}(\widehat{R},\xi)\) with \(o(n\xi)\) comparisons, where \(\mathcal{I}_{n}(\widehat{R},\xi)\) is the subset of \(\text{cand}(\hat{R},n\xi)\) defined by

\[\mathcal{I}_{n}(\widehat{R},\xi)=\{A\in\mathcal{U}^{n}:\max_{i\in[n]}\log(\eta_ {i}^{\Delta}+2)\leq\xi\}\.\] (12)

[MISSING_PAGE_EMPTY:25]

where \(A_{i-1}^{\pi}=\{a_{\pi(j)}\}_{j<i}\), and by Lemma D.1, we have that

\[\vec{\eta}(a_{\pi(i)},\mathcal{Q}^{i-1})\leq\eta_{\pi(i-1)}^{\Delta}+\eta_{\pi(i) }^{\Delta}+\widehat{R}_{\pi(i)}-\widehat{R}_{\pi(i-1)}\.\] (14)

\(\widehat{R}\) is a permutation of \([n]\), hence \(\widehat{R}_{\pi(i)}=\widehat{R}_{\pi(i-1)}+1\) by definition of \(\pi\). Moreover, \(A\in\mathcal{I}_{n}(\xi)\), thus \(\max(\log(\eta_{\pi(i-1)}^{\Delta}+2),\log(\eta_{\pi(i)}^{\Delta}+2))\leq\xi\), and it follows that

\[\log(\vec{\eta}(a_{\pi(i)},\mathcal{Q}^{i-1})+2) \leq\log(\eta_{\pi(i-1)}^{\Delta}+\eta_{\pi(i)}^{\Delta}+3)\] \[\leq\log(\eta_{\pi(i-1)}^{\Delta}+2)+\log(\eta_{\pi(i)}^{\Delta} +2)\] \[\leq 2\xi\,\]

where the second inequality is a consequence of \(\log(\alpha+\beta)\leq\log(\alpha)+\log(\beta)\) for all \(\alpha,\beta\geq 2\).

Therefore, all the insertions into \(\mathcal{Q}\) require at most \((2\xi\cdot\varepsilon(2\xi))\) comparisons, and the total number of comparisons \(T\) used to sort \(A\) is at most

\[T \leq n\left(2\xi\cdot\varepsilon(2\xi)+C\right)\] \[=n\xi\left(2\cdot\varepsilon(2\xi)+\frac{C}{\xi}\right)\] \[=o(n\xi)\.\]

This means that any instance in \(\mathcal{I}_{n}(\xi)\) can be sorted using \(o(n\xi)\) comparisons, which contradicts the lower bound on sorting algorithms augmented with positional predictions. 

#### d.2.2 Rank prediction and dirty comparisons model

In the rank prediction model, the total number of inserted keys is \(N=n\). Denote by \(\eta_{i}=\eta(a_{i},A)\) for all \(i\in[n]\). If there is a data structure \(\mathcal{Q}\) not satisfying the lower bound of Theorem 3.4 for positional predictions, then we can use it for sorting \(A\). Let \(1\leq\xi\leq O(\log n)\). Similarly to the proof for pointer predictions, if \(\widehat{R}\) is a permutation of \([n]\), using \(\mathcal{Q}\) for sorting any instance \(A\in\mathcal{I}_{n}(\xi)\) requires at most \(T=o(n\xi)\) comparisons, which contradicts the lower bound on learning-augmented sorting algorithms.

The same arguments, combined with (13), give the result also for the dirty comparison model.

## Appendix E Applications

### Sorting

In the case where the algorithm is given offline access to the sequence \(A=(a_{1},\ldots,a_{n})\) to sort and to the rank predictions \((\widehat{R}(a_{i}))_{i\in[n]}\), we argue that pointer predictions can be used to sort the sequence within a \(O(\sum_{i\in[n]}\log(\eta_{i}^{\Delta}+2))\).

With offline rank predictions, by Lemma D.1, the algorithm can construct a permutation \(\pi\) of \([n]\) satisfying \(\widehat{R}(a_{\pi(1)})\leq\ldots\leq\widehat{R}(a_{\pi(n)})\), and we showed in the proof of Theorem 3.4, in (14), that inserting the elements of \(A\) into the priority queue in the order given by \(\pi\), then taking each inserted element \(a_{\pi(i)}\) as pointer prediction for the following one \(a_{\pi(i+1)}\), yields a pointer prediction error of

\[\vec{\eta}(a_{\pi(i)},\mathcal{Q}^{i-1})\leq\eta_{\pi(i-1)}^{\Delta}+\eta_{\pi (i)}^{\Delta}+\widehat{R}_{\pi(i)}-\widehat{R}_{\pi(i-1)}\]

for all \(i\in\{2,\ldots,n\}\). By Theorem 3.1, the runtime for inserting \(a_{\pi(i)}\) using this pointer prediction is \(O(\log\vec{\eta}(a_{\pi(i)},\mathcal{Q}^{i-1}))\). The total time for inserting all the elements into the priority queue is therefore at most proportional to

\[\sum_{i=2}^{n}\log(\vec{\eta}(a_{\pi(i)},\mathcal{Q}^{i-1})+2) \leq\sum_{i=2}^{n}\log(\eta_{\pi(i-1)}^{\Delta}+\eta_{\pi(i)}^{ \Delta}+\widehat{R}_{\pi(i)}-\widehat{R}_{\pi(i-1)}+2)\] \[\leq\sum_{i=2}^{n}\log((\eta_{\pi(i-1)}^{\Delta}+2)+(\eta_{\pi(i)} ^{\Delta}+2)+(\widehat{R}_{\pi(i)}-\widehat{R}_{\pi(i-1)}+2))\] \[\leq 2\sum_{i=1}^{n}\log(\eta_{\pi(i)}^{\Delta}+2)+\sum_{i=2}^{n}( \widehat{R}_{\pi(i)}-\widehat{R}_{\pi(i-1)})+n\] \[\leq 2\sum_{i=1}^{n}\log(\eta_{\pi(i)}^{\Delta}+2)+\widehat{R}_{ \pi(2)}+n\] \[=O\left(\sum_{i\in[n]}\log(\eta_{i}^{\Delta}+2)\right)\.\]

The third inequality results from inequation \(\log(\alpha+\beta+\gamma)\leq\log\alpha+\log\beta+\log\gamma\) for all \(\alpha,\beta,\gamma\geq 2\), and the subsequent inequality follows from \(\log(\alpha+1)\leq\alpha\) for all \(\alpha\geq 0\). Finally, to complete the sorting algorithm, the minimum is repeatedly extracted from the priority queue until is it empty, and each \(\mathrm{ExtractMin}\) only takes \(O(1)\) time in the skip list.

### Dijkstra's algorithms

We give in this section more details on the complexity of Dijkstra's algorithm using our priority queue augmented with rank predictions.

Consider any priority queue implementation, having a time complexity \(T_{\mathrm{Insert}}\) for insertion, \(T_{\mathrm{ExtractMin}}\) for extracting the minimum, and \(T_{\mathrm{DecreaseKey}}\) for decreasing the key of an element. There are two possible implementations of Dijkstra's algorithm with priority queues. The first one only uses the operations \(\mathrm{Insert}\) and \(\mathrm{ExtractMin}\), and the maximum number of inserted elements is \(m+1\), which yields a complexity of \(O(mT_{\mathrm{Insert}}+mT_{\mathrm{ExtractMin}})\). The second implementation, using also the \(\mathrm{DecreaseKey}\) operation, yields a complexity of \(O(n(T_{\mathrm{Insert}}+T_{\mathrm{ExtractMin}})+mT_{\mathrm{DecreaseKey}})\).

Recalling that the number of edges is at most \(n^{2}\), using a binary heap in any of both implementations gives a total runtime of \(O(m\log n)\). On the other hand, using a Fibonacci heap in the second implementation yields a runtime of \(O(n\log n+m)\).

In the rank predictions model, we consider that the priority queue only uses \(\mathrm{Insert}\) and \(\mathrm{ExtractMin}\) operations, hence we use the first implementation. Denoting by \(\{d_{i}\}_{i\in[m]}\) the \(m\) distinct keys that are inserted, these keys are only revealed online to the priority queue. If they are accompanied by predictions \((\widehat{R}(d_{i}))_{i}\) of their ranks, then each insertion requires \(O(\log\log m+\log(\max_{i\in[m+1]}|R(d_{i})-\widehat{R}(d_{i})|+2))\) amortized time and \(O(\log\max_{i\in[m+1]}|R(d_{i})-\widehat{R}(d_{i})|)\) comparisons, while extractions only require a constant amortized time each. The total runtime is therefore

\[O\big{(}m\log\log n+m\log\max_{i\in[m+1]}|R(d_{i})-\widehat{R}(d_{i})|\big{)}\,\]

and the total number of comparisons is \(O(m\log(\max_{i\in[m+1]}|R(d_{i})-\widehat{R}(d_{i})|))\).

## Appendix F Additional experiments

### Sorting

The problem of sorting with similar prediction models have been studied in Bai and Coester (2023), hence we numerically compare sorting using our learning-augmented priority queue (LAPQ) with their sorting algorithms. To run their algorithms, we used the code provided by Bai and Coester[2023] in their paper. We give here additional experimental results for the _class_ and the _decay_ setting, for smaller values of \(n\).

### Dijkstra's algorithm

Considering the same experimental setting presented in Section 5, Figures 8 and 9 show the obtained results for the cities of Brussels, Paris, New York, and London, which have. The numbers of nodes \(n\) and edges \(m\) in each city graph are indicated in the figures.

Figure 8: Dijkstras algorithm on city maps with class predictions

Figure 6: Sorting with rank predictions in the _class_ setting, for \(n\in\{1000,10000,100000\}\).

Figure 7: Sorting with rank predictions in the _decay_ setting, for \(n\in\{1000,10000,100000\}\).

Figure 9: Dijkstras algorithm on city maps with decay predictions

Simulations on Poisson Voronoi TesselationsWe also evaluate the performance of Dijkstra's algorithm with our LAPQ on synthetic random graphs. For this, we use Poisson Voronoi Tessellations (PVT), which are a commonly used random graph model for street systems Gloaguen et al. (2006); Gloaguen and Cali (2018); Benomar et al. (2022a,b).

PVTs are random graphs created by sampling an integer \(N\) from a Poisson distribution with parameter \(n\geq 1\). Subsequently, \(N\) points, termed "seeds," are uniformly chosen at random within a two-dimensional region \(I\), typically \([0,1]^{2}\). A Voronoi diagram is then generated based on these seeds.

This process results in a planar graph where edges represent the boundaries between the cells of the Voronoi diagram, and the nodes are their intersections. For \(I=[0,1]^{2}\), the expected number of nodes in this construction is \(n\).

Figure 10 provides a visualization of a PVT with \(n=100\).

We present the results in the class and the decay settings respectively in Figures 11 and 12. Similar to previous experiments with Dijkstra on city maps, these figures illustrate how the number of comparisons decreases when the LAPQ is augmented with _node rank_ predictions or with the corresponding dirty comparator. We compare them with the number of comparisons induced by using a binary or Fibonacci heap, as well as with the number of comparisons of the LAPQ augmented with _key rank predictions_.

The same observations regarding the performance improvement can be made, as in the previous experiments with city maps. However, in PVT tessellations, the performance of the LAPQ with key rank predictions surpasses even that of perfect node rank predictions. This is due to the PVTs having a more uniform structure across space.

Figure 11: Dijkstras algorithm on Poisson Voronoi Tesselation with class predictions

Figure 12: Dijkstras algorithm on Poisson Voronoi Tesselation with decay predictions

Figure 10: PVT with \(n=100\)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract indicates the scope of the paper: learning-augmented algorithms, and describes the particular problem we consider Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the difficulties we faced for designing a learning-augmented binary heap. For learning-augmented skip-lists, we discuss the limitations and possible improvements in the rank prediction model. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: for each theoretical result, all the needed assumptions are stated. The full proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: we give full details on our data structure implementation, and all the details (instances, libraries) needed for reproducing the results are precised in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The full implementation of the learning-augmented priority queue is given in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The setting is fully described in the experiments section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: standard deviations are reported in all the figures. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Our figures show the number of comparisons used in each experiment. The computer resources are not relevant. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our paper respects the Code of Ethics or NeurIPS. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper presents foundational/theoretical work in the field of learning-augmented algorithms. We do not feel that any potential societal consequences of our work must be specifically highlighted. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer:[NA] Justification: learning-augmented algorithms, in general, pose no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We partially use existing open source code, which we clearly cite. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not include such experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not include such experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.