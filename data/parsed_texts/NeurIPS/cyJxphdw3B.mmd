# Can neural operators always be continuously discretized?

 Takashi Furuya\({}^{1,*}\)  Michael Puthawala\({}^{2,*}\)  Maarten V. de Hoop\({}^{3}\)  Matti Lassas\({}^{4}\)

\({}^{1}\)Shimane University, takashi.furuya0101@gmail.com

\({}^{2}\)South Dakota State University, Michael.Puthawala@sdstate.edu

\({}^{3}\)Rice University, mdehoop@rice.edu

\({}^{4}\)University of Helsinki, matti.lassas@helsinki.fi

* These authors contributed equally to this work

###### Abstract

We consider the problem of discretization of neural operators between Hilbert spaces in a general framework including skip connections. We focus on bijective neural operators through the lens of diffeomorphisms in infinite dimensions. Framed using category theory, we give a no-go theorem that shows that diffeomorphisms between Hilbert spaces or Hilbert manifolds may not admit any continuous approximations by diffeomorphisms on finite-dimensional spaces, even if the approximations are nonlinear. The natural way out is the introduction of strongly monotone diffeomorphisms and layerwise strongly monotone neural operators which have continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces. For these, one can guarantee discretization invariance, while ensuring that finite-dimensional approximations converge not only as sequences of functions, but that their representations converge in a suitable sense as well. Finally, we show that bilipschitz neural operators may always be written in the form of an alternating composition of strongly monotone neural operators, plus a simple isometry. Thus we realize a rigorous platform for discretization of a generalization of a neural operator. We also show that neural operators of this type may be approximated through the composition of finite-rank residual neural operators, where each block is strongly monotone, and may be inverted locally via iteration. We conclude by providing a quantitative approximation result for the discretization of general bilipschitz neural operators.

## 1 Introduction

Neural operators, first introduced in [30], have become more and more prominent in deep learning on function spaces. As opposed to traditional neural networks that learn maps between finite-dimensional Euclidean spaces, neural operators learn maps between infinite-dimensional function spaces yet may be trained and evaluated on finite-dimensional data through a rigorous notion of discretization. Neural operators are widely used in the field of scientific machine learning [6, 21, 38, 55, 56], among others, principally because of their discretization invariance. In this work, we consider the fundamental limits of this discretization. Throughout, we emphasize the importance of continuity under discretization.

A key ingredient in our analysis is the identification of properties of diffeomorphisms that may be induced by (bijective) neural operators, which are diffeomorphisms themselves. Diffeomorphisms exist in many contexts, for example, in generative models. These involve mapping one probability distribution or measure, \(\mu\), over some measurable space to another, \(X\), via a push forward, that is, \(F_{\#}\mu(U)=\mu(F^{-1}(U))\) for \(U\subset X\). If \(\mu\) admits a density \(d\mu\) then, in finite dimensions, we mayuse the change of variables formula to write \(d\rho(x)=d\mu(F^{-1}(x))|JF(x)|^{-1}\), where \(JF\) is the Jacobian of \(F\). Clearly, \(F\) must be a bijection with full-rank Jacobian. In other words, \(F\) must be a diffeomorphism onto its range. This established diffeomorphisms as natural objects of interest in finite-dimensional machine learning, and helps account for their wide use [20, 25, 31, 44, 54]. In this work, we consider the extension of these efforts from finite to infinite dimensions implemented via neural operators. Although there is no analogue of the change of variables formula in infinite dimensions, we argue that it is, nonetheless, natural to consider the role of diffeomorphisms, and how they may be approximated via diffeomorphisms on finite-dimensional spaces.

The question of when operations between Hilbert spaces may be discretized continuously may be understood through an analogy to computer vision. Consider the task of learning a map from one image space to another, for example, a style transfer problem [16], where the mapping learned does not depend much on the resolution of the images provided. It is natural to think of the map as being defined between (infinite-resolution) _continuum_ images, and then its application to images of a specific resolution. In this analogy, \(X\) is a function space (over images \(m:\ [0,1]^{2}\to\mathbb{R}\)[29]) that is approximated with a finite-dimensional space \(\mathbb{R}^{d}\) and the transformation \(F:\ X\to X\) is approximated by a map \(f:\ \mathbb{R}^{d}\to\mathbb{R}^{d}\), where each \(f\) acts on images of a particular resolution. An explicit transformation formula can be obtained when \(f\) is a diffeomorphism and has a smooth inverse.

We introduce a framework based on a generalized notion of neural operator layers including a skip connection and their restrictions to balls rather than compact sets. With bijective neural operators in mind, we give a perspective based on diffeomorphisms in infinite dimensions between Hilbert manifolds. We give a no-go theorem, framed with category theory, that shows that diffeomorphisms between Hilbert spaces may not admit any continuous approximations by diffeomorphisms on finite-dimensional spaces, _even_ if the underlying discretization is nonlinear. In this framing the discretization operation is modeled as a functor from the category of Hilbert spaces and \(C^{1}\)-diffeomorphisms on them to their finite-dimensional approximations. A natural way to mitigate the no-go theorem is described by the introduction of strongly monotone diffeomorphisms and layerwise strongly monotone neural operators. We prove that all strongly monotone neural operator layers admit continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces. We then provide various conditions under which a neural operator layer is strongly monotone. Notably, a bilipschitz (and, hence, bijective) neural operator layer can always be represented by a composition of strongly monotone neural operator layers. Hence, such an operator may be continuously discretized. More constructively, any bilipschitz neural operator layer can be approximated by residual finite-rank neural operators, each of which are strongly monotone, plus a simple isometry. Moreover, these finite-rank residual neural operators are (locally) bijective and invertible, and their inverses are limits of compositions of finite-rank neural operators. Our framework may be used "out of the box" to prove quantitative approximation results for discretization of neural operators.

### Related work

Neural operators were first introduced in [30]. Alternative designs for mappings between function spaces are the DeepONet [34, 40], and the PCA-Net [7, 12]. In spite of the multitudinous applications of neural operators, the theory of the natural class of injective or bijective neural operators is comparatively underdeveloped; see, for example [2, 14].

Our work is concerned with the of discretization of neural operators through the lens of diffeomorphisms. For recent important work on analyzing the effect of discretization error of Fourier Neural Operators (FNOs) arising from aliasing, see [35]. Our work has connections to infinite-dimensional inference, see e.g. [19], and approximation theory, see e.g. [13] while bridging the gap with the theory of neural operators.

We give a no-go theorem that uses a category theory framing. This contributes to the use of category theory as an emerging tool in the analysis and understanding of neural networks at large. In this sense, we are in league with the recent work generalizing ideas from geometric machine learning using category theory [17].

Discretization obstructions have been encountered in other contexts. Numerical methods that approximate continuous models are known to sometimes fail in surprising ways. A basic example of this is the "locking" phenomenon in the study of the Finite Elements Method (FEM). For example,linear elements used to model bending of a curved surface or beam lock in such a way that the model exhibits a non-physical stiff response to deformations [4]. Understanding this has been instrumental in developing improved numerical methods, such a high order FEM [52]. Furthermore, in discretized statistical inverse problems [27; 36; 51], the introduction of Besov priors [48; 11] has been found to be essential.

Finally, our work extends prior work (not based on deep learning) in discretization of physical or partial differential equations based forward models in inverse problems. The analogous notion of discretization invariance of solution algorithms of inverse problems was studied in [37; 48; 51] and the lack of it (in imaging methods using Bayesian inversion with \(L^{1}\) priors) in [36; 48]. By considering the neural operator as the physical model, our results state that discretization can be done locally in an appropriate way, together with constructing an inverse.

### Our contributions

The key results in this paper comprise the following:

1. We prove a general no-go theorem showing that, under general circumstances, diffeomorphisms between Hilbert spaces may not admit continuous approximation by finite-dimensional diffeomorphisms. In particular, neural operators corresponding to diffeomorphic maps, in general, cannot be approximated by finite-dimensional diffeomorphisms and their associated neural representations.
2. We show that strongly monotone neural operator layers admit continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces.
3. We show that bilipschitz neural operators can be represented in any bounded set as a composition of strongly monotone, diffeomorphic neural operator layers, plus a simple isometry. These can be approximated by finite-rank diffeomorphic neural operators, where each layer is strongly monotone. For these operators we give a quantitative approximation result.

## 2 Definitions and notation

In this section, we give the definitions and notation used throughout the paper. First, we summarize the relevant basic concepts from functional analysis. Then, we introduce generalized neural operators.

### Elements of functional analysis

In this work, all Hilbert spaces, \(X\), are endowed with their norm topology. We denote by \(B_{X}(r)=B_{X}(0,r)\) the ball in the space \(X\) having the center at zero and radius \(r>0\). We denote by \(S(X)\) the set of all finite-dimensional linear subspaces \(V\subset X\). The set \(S_{0}(V)\subset S(X)\) is a partially ordered lattice. That is, if \(V_{1},V_{2}\in S_{0}(X)\) then there is a \(V_{3}\in S_{0}(X)\) so that \(V_{1}\subset V_{3}\) and \(V_{2}\subset V_{3}\). 1 With \(Y\) standing for another Hilbert space, we denote by \(C^{n}(X;Y)\) the set of operators, \(F\colon\, X\to Y\), having \(n\) continuous (Frechet) derivatives, and \(C^{n}(X)=C^{n}(X;X)\).

Footnote 1: Each element of \(S_{0}(X)\) will come to represent a discretization of \(X\). The partially ordered lattice condition will come to represent a notion of common refinement of a discretization. This makes it possible to consider “realistic” choices of discretizations. The condition automatically follows for any discretization scheme that has a notion of “common refinement” of two discretizations. Examples include finite-difference schemes, and the finite elements Galerkin discretization that is based on a triangulation of the domain.

Next, we define what it means that a nonlinear operator or function \(F\colon\, X\to X\) on an infinite-dimensional Hilbert space, \(X\), is approximated by operators or functions on finite-dimensional subspaces \(V\subset X\). The key is that as \(V\) tends to \(X\), the complexity of \(F_{V}\) increases and one may hope that the approximation becomes better. We formalize this in the following definition.

**Definition 1** (\(\epsilon_{V}\) approximators and weak approximators).: _(i) Let \(r>0\), \(\mathcal{F}\subset C^{n}(X;X)\) be a family of functions, and \(\vec{\varepsilon}=(\varepsilon_{V})_{V\in S_{0}(X)}\) be a sequence such that \(\varepsilon_{V}\to 0\) as \(V\to X\). We say that a function_

\[\mathcal{A}_{X}\colon\,\mathcal{F}\to\bigtimes_{V\in S_{0}(X)}C(\overline{B_{V }(0,r)};V),\quad F\to(F_{V})_{V\in S_{0}(X)}\]_is an \(\vec{\varepsilon}\)-approximation operation for functions \(\mathcal{F}\) in the ball \(B_{X}(0,r)\) taking values in families \(\mathcal{F}_{V}\subset C^{1}(V;V)\) if \(\mathcal{A}_{X}\) maps a function \(F\colon\ X\to X\), where \(F\in\mathcal{F}\), to a sequence of functions \((F_{V})_{V\in S_{0}(X)}\), where \(F_{V}\in\mathcal{F}_{V}\), such that the following is valid: For all \(F\colon\ X\to X\) satisfying \(\|F\|_{C^{n}(\overline{B_{X}(0,r)};X)}\leq M\), we have_

\[\sup_{x\in\overline{B_{V}(0,r)}}\|F_{V}(x)-P_{V}(F(x))\|_{X}\leq M\varepsilon_ {V}, \tag{1}\]

_where \(P_{V}\colon\ X\to X\) is the orthogonal projection onto \(V\), that is, \(\operatorname{Ran}(P_{V})=V\)._

_(ii) We say that \(\mathcal{A}\colon C^{n}(X;X)\to\bigtimes_{V\in S_{0}(X)}C(V;V),\quad F\to(F_{V })_{V\in S_{0}(X)}\) is a weak approximation operation for the family \(\mathcal{F}\subset C^{n}(X;X)\) if for any \(F\in\mathcal{F}\) and \(r>0\) it holds that_

\[\lim_{V\to X}\sup_{x\in\overline{B_{V}(0,r)}}\|F_{V}(x)-P_{V}(F(x))\|_{X}\to 0.\]

Note that the condition (i) is stronger than the condition (ii). An example of an approximation operation for the family \(\mathcal{F}=C^{n}(X)\), that is, an \(\vec{\varepsilon}\)-approximation operation with all sequences \(\vec{\varepsilon}=(\varepsilon_{V})_{V\in S_{0}(X)}\) subject to \(\varepsilon_{V}>0\), is the linear discretization

\[\mathcal{A}_{\mathrm{lin}}(F)=(F_{V})_{V\in S_{0}(X)},\quad F_{V}=P_{V}\circ(F |_{V}):\ V\to V. \tag{2}\]

Nonlinear discretization methods that do not rely on \(P_{V}\) have been used, for example, in the numerical analysis of nonlinear partial differential equations. Here, \(X\) becomes an appropriate Sobolev space, and a Galerkin approximation is implemented through finite-dimensional subspaces, \(V\), spanned by finite element basis functions. We present an example for the nonlinear equation, \(\Delta u(t)-g(u(t))=\Delta x(t)\) where \(g\) is a smooth convex function, when \(F\colon\ x\to u\), in Appendix A.1.

Below, we will study whether a family, \(\mathcal{F}\subset\operatorname{Diff}^{1}(X)\), of \(C^{1}\) diffeomorphisms on \(X\) can be approximated by \(C^{1}\) diffeomorphisms, \(\mathcal{F}_{V}\subset\operatorname{Diff}^{1}(V)\), on finite-dimensional subspaces, \(V\). Of course, diffeomorphisms are bijective. Unless stated otherwise, from now on we will omit \(C^{1}\) and implicitly assume that diffeomorphism are \(C^{1}\) diffeomorphisms. We introduce two more notions that will play a key role in the further analysis.

**Definition 2** (Strongly Monotone).: _We say that a (nonlinear) operator \(F\colon\ X\to X\) on Hilbert space, \(X\), is strongly monotone if there exists a constant \(\alpha>0\) so that_

\[\langle F(x_{1})-F(x_{2}),x_{1}-x_{2}\rangle_{X}\geq\alpha\|x_{1}-x_{2}\|_{X} ^{2},\quad\text{for all }x_{1},x_{2}\in X. \tag{3}\]

**Definition 3** (Bilipschitz).: _We say that \(F\) if bilipschitz there exist constants \(c>0\) and \(C<\infty\) so that for all \(x_{1},x_{2}\in X\), \(c\,\|x_{1}-x_{2}\|\leq\|F(x_{1})-F(x_{2})\|\leq C\,\|x_{1}-x_{2}\|\)._

### A general framework for neural operators

In this paper, we are concerned with the modeling of diffeomorphisms between Hilbert spaces by bijective neural operators. Our working definition of neural operator, which generalizes the traditional notion, is given below. We note the presence of a skip connection, which is essential.

**Definition 4** (Generalized neural operator layer).: _For Hilbert space \(X\), a layer of a neural operator is a function \(F\colon\ X\to X\) of the form_

\[F(x)=x+T_{2}G(T_{1}x), \tag{4}\]

_where \(T_{1}\colon\ X\to X\) and \(T_{2}\colon\ X\to X\) are compact linear operators 2 and \(G\colon\ X\to X\) is a nonlinear operator in \(C^{1}(X)\). A generalized neural operator, \(H:\ X\to X\), is given by the composition_

Footnote 2: By using mapping properties of monotone operators [5], we can replace this definition by using Hilbert spaces \(Y\) and \(Z\) that are isometric to \(X\), \(T_{1}:\ X\to Y\) and \(T_{2}:\ Z\to X\) as compact linear operators, and \(G:\ Y\to Z\) as a \(C^{1}\) nonlinear operator.

\[H=A_{L}\circ\sigma\circ F_{L}\circ A_{L-1}\circ\sigma\circ F_{L-1}\circ\cdots \circ A_{1}\circ\sigma\circ F_{1}, \tag{5}\]

_where each \(F_{\ell}\), \(\ell=1,\ldots,L\) is of the form (4), the \(A_{\ell}:X\to X\) are bounded linear operators and \(\sigma\colon X\to X\) is a continuous operation (for example, a Nemytskii operator defined by a composition with a suitable activation function in function spaces)._The generalized neural operators can represent the classical neural operators [30, 33]. For an explicit construction, we refer to Appendix C.1. In the next section, we will study, under what conditions, bounded linear operators \(A_{\ell}\), Nemytskii operators \(\sigma\), and neural operator layers \(F_{\ell}\), for which the generalized neural operator consists, can be continuously discretized.

We note that because \(G\in C^{1}(X)\) in Definition 4, it follows that \(G\in L^{\infty}(X)\) and \(\text{Lip}_{X\to X}(G)<\infty\). Given a Hilbert space, \(X\), a _layer of a strongly monotone neural operator_ (respectively, a _layer of a bilipschitz neural operator,_) is a function \(F\colon\, X\to X\) that is strongly monotone (respectively, bilipschitz). Furthermore, a _strongly monotone neural operator_ (respectively, a _bilipschitz neural operator_), is a generalized neural operator with strongly monotone (respectively, bilipschitz), layers.

## 3 Category theoretic framework for discretization

"Well-behaved" operators between infinite-dimensional Hilbert spaces may have dramatically different behaviors than corresponding "well-behaved" maps between finite-dimensional Euclidean spaces. This observation applies to discretization and neural operators versus neural networks. In this section we explore this. We first present a no-go theorem, that there are _no_ procedures that continuously discretize an isotopy of diffeomorphisms. Next, we introduce strongly monotone neural operator layers, which are strongly monotone diffeomorphisms, and then prove that these allow a continuous approximation functor, that is, continuous approximations by strongly monotone diffeomorphisms on finite dimensional spaces. We finally show that bilipschitz neural operator layers admit a representation via strongly monotone operators and linear maps, allowing for their continuous approximation.

### No-go theorem for discretization of diffeomorphisms on Hilbert spaces

In this section, we present our no-go theorem. To formulate the 'impossibility' of something, we must define what is meant by discretization and approximation. Before this, we give an informal statement of the no-go theorem.

**Theorem 1** (No-go Theorem, Informal).: _Let \(\mathcal{A}\) be an approximation scheme that maps diffeomorphisms \(F\) on a Hilbert to a sequence of finite-approximations \(F_{V}\) that are themselves diffeomorphisms. If \(F_{V}\) converges to \(F\) as \(V\to X\), then \(\mathcal{A}\) is not continuous, that is, there are maps \(F^{(j)}\) that converge to \(F\) as \(j\to\infty\), but all \(F^{(j)}_{V}\) are far from \(F_{V}\)._

We want to emphasize that most practical numerical algorithms are continuous so that the output depends (in some suitable sense) continuously on the input. This shows that there are no such numerical schemes that approximate infinite-dimensional diffeomorphisms with finite-dimensional ones. In order to prove our no-go theorem in the most general setting, we phrase it in terms of category theory. Namely, we formulate \(\mathcal{A}\) (which will denote the approximation scheme) as a functor from the category of Hilbert spaces and diffeomorphisms thereon, to their finite-rank approximations.

**Definition 5** (Category of Hilbert Space Diffeomorphisms).: _We denote by \(\mathcal{D}\) the category of Hilbert diffeomorphisms with objects \(\mathcal{O}_{\mathcal{D}}\) that are pairs \((X,F)\) of a Hilbert space \(X\) and a (possibly nonlinear) \(C^{1}\)-diffeomorphism \(F\colon X\to X\) and the set of morphisms (or arrows that'map' objects to other objects) \(\mathcal{A}\) that are either_

1. _(induced isomorphisms) Maps_ \(a_{\phi}\) _that are defined for a linear isomorphism_ \(\phi:X_{1}\to X_{2}\) _of Hilbert spaces_ \(X_{1}\) _and_ \(X_{2}\) _that maps the objects_ \((X_{1},F_{1})\in\mathcal{O}_{\mathcal{D}}\) _to the object_ \((\phi(X_{1}),\phi\circ F_{1}\circ\phi^{-1})\in\mathcal{O}_{\mathcal{D}}\)_, or_
2. _(induced restrictions) Maps_ \(a_{X_{1},X_{2}}\) _that are defined for a Hilbert space_ \(X_{1}\)_, its closed subspace_ \(X_{2}\subset X_{1}\)_, and an object_ \((X_{1},F_{1})\in\mathcal{O}_{\mathcal{D}}\) _such that_ \(F_{1}(X_{2})=X_{2}\)_. Then_ \(a_{X_{1},X_{2}}\) _maps to the object_ \((X_{1},F_{1})\in\mathcal{O}_{\mathcal{D}}\) _to the object_ \((X_{2},F_{1}|_{X_{2}})\in\mathcal{O}_{\mathcal{D}}\)_._

**Definition 6** (Category of Approximation Sequences).: _We denote by \(\mathcal{B}\) the category of approximation sequences, that has objects \(\mathcal{O}_{\mathcal{B}}\) that are of the form \((X,S_{0}(X),(F_{V})_{V\in S_{0}(X)})\) where \(X\) is a Hilbert space,_

\[S_{0}(X)\subset S(X)=\{V\mid V\subset X\text{ is a finite dimensional linear subspace}\},\]

_are partially ordered lattices, \(\bigcup_{V\in S_{0}(X)}V=X\), and \(F_{V}\colon V\to V\) are \(C^{1}\)-diffeomorphisms of spaces \(V\in S_{0}(X)\).__The set of morphisms \(\mathcal{A}_{\mathcal{B}}\) consists of either_

1. _Maps_ \(A_{\phi}\) _that are defined for a linear isomorphism_ \(\phi:X_{1}\to X_{2}\) _of Hilbert spaces_ \(X_{1}\) _and_ \(X_{2}\)_, and lattices_ \(S_{0}(X_{1})\) _and_ \(S_{0}(X_{2})=\{\phi(V)\mid V\in S_{0}(X_{1})\}\)_, that maps the objects_ \((X_{1},S(X_{1}),(F_{V})_{V\in S(X_{1})})\) _to_ \((X_{2},S(X_{2}),(\phi\circ F_{\phi^{-1}(W)}\circ\phi^{-1})_{W\in S(X_{2})})\)_, or_
2. _Maps_ \(A_{X_{1},X_{2}}\) _that are defined for a Hilbert space_ \(X_{1}\)_, its closed subspace_ \(X_{2}\subset X_{1}\)_, and an object_ \((X_{1},S_{0}(X_{1}),(F_{V})_{V\in S_{0}(X_{1})})\) _such that_ \(F(X_{2})=X_{2}\) _and_ \(S_{0}(X_{2})=\{V\in S_{0}(X_{1})\mid V\subset X_{2}\}\) _is a partially ordered lattice. Then_ \(A_{X_{1},X_{2}}\) _maps the object_ \((X_{1},S_{0}(X_{1}),(F_{V})_{V\in S_{0}(X_{1})})\) _to the object_ \((X_{2},S_{0}(X_{2}),(F_{V})_{V\in S_{0}(X_{2})})\)_._

Next, we define the notion of an approximation or discretization functor. In practice, an approximation functor is an operator which maps a function \(F\) in an infinite dimensional space \(X\) to a function \(F_{V}\) that operate in finite dimensional subspaces \(V\) of \(X\) in such a way that functions \(F_{V}\) are close (in a suitable sense) to the function \(F\).

**Definition 7** (Approximation Functor).: _We define the approximation functor, denoted by \(\mathcal{A}\colon\mathcal{D}\to\mathcal{B}\), as the functor that maps each \((X,F)\in\mathcal{O}_{\mathcal{D}}\) to some \((X,S_{0}(X),(F_{V})_{V\in S_{0}(X)})\in\mathcal{O}_{\mathcal{B}}\) so that the Hilbert space \(X\) stays the same. The approximation functor maps all morphisms \(a_{\phi}\) to \(A_{\phi}\) and morphisms \(a_{X_{1},X_{2}}\) to \(A_{X_{1},X_{2}}\), and has the following the properties_

1. _For all_ \(r>0\) _and all_ \((X,F)\in\mathcal{O}_{\mathcal{D}}\)_,_ \[\lim_{V\to X}\sup_{x\in\overline{B}_{X}(0,r)\cap V}\|F_{V}(x)-F(x)\|_{X}=0.\] _In separable Hilbert spaces this means that when the finite dimensional subspaces_ \(V\subset X\) _grow to fill the whole Hilbert space_ \(X\)_, then the approximations_ \(F_{V}\) _converge uniformly in all bounded subsets to_ \(F\)_._

We recall the notation \(\lim_{V\to X}\) used above: We consider \((S_{0}(X),\supset)\) as a partially ordered set and say that real numbers \(y_{V}\) converge to the limit \(y\) as \(V\to X\), and denote

\[\lim_{V\to X}y_{V}=y,\]

if for all \(\epsilon>0\) there is \(V_{0}\in S_{0}(X)\) such that for \(V\in S_{0}(X)\) satisfying \(V\supset V_{0}\) it holds that \(|y_{V}-y|<\epsilon\).

**Definition 8**.: _We say that the approximation functor \(\mathcal{A}\) is continuous if the following holds: Let \((X,F),(X,F^{(j)})\in\mathcal{O}_{\mathcal{D}}\) be such that the Hilbert space \(X\) is the same for all these objects and let \((X,S_{0}(X),(F_{V})_{V\in S_{0}(X)})=\mathcal{A}(X,F)\) be approximating sequences of \((X,F)\) and \((X,S_{0}(X),(F_{j,V})_{V\in S_{0}(X)})=\mathcal{A}(X,F^{(j)})\) be approximating sequences of \((X,F^{(j)})\). Moreover, assume that \(r>0\) and_

\[\lim_{j\to\infty}\sup_{x\in\overline{B}_{X}(0,r)}\|F^{(j)}(x)-F(x)\|_{X}=0. \tag{6}\]

_Then, for all \(V\in S_{0}(X)\) the approximations \(F_{V}^{(j)}\) of \(F^{(j)}\) and \(F_{V}\) of \(F\) satisfy_

\[\lim_{j\to\infty}\sup_{x\in V\cap\overline{B}_{V}(0,r)}\|F_{V}^{(j)}(x)-F_{V}( x)\|_{X}=0. \tag{7}\]

The theorem below states a negative result, namely that there does not exist continuous approximating functors for diffeomorphisms.

**Theorem 2**.: _(No-go theorem for discretization of general diffeomorphisms) There exists no functor \(\mathcal{D}\to\mathcal{B}\) that satisfies the property (A) of an approximation functor and is continuous._

The proof is given in Appendix A.4.1, and is quite involved, but we give an overview of some of the steps here. A generalization of Theorem 2, in the case where the norm topology is replaced by the weak topology, is considered in Appendix D.1.

A key fact is that for finite dimensional diffeomorphisms the space of smooth embeddings consists of two connected components, one orientation preserving and the other orientation reversing. This is not the case in infinite dimensions, see e.g. [32] and [45]. For an illustration of this, see Figure 1. The proof proceeds by contradiction. First, we consider the action of the approximation functor as it operates on an isotopy (path of diffeomorphisms) that connects two diffeomorphisms. The first has only orientation-preserving discretizations, and the second only orientation-reversing discretizations. We then show that the image of the path under the approximation functor yields a disconnected path, as the discretization 'jumps' from the orientation preserving component to the orientation reversing component. This violates continuity. To encode the notions of orientation preserving and orientation reversing that allow for a description of nonlinear discretization theory, we use topological degree theory. This generalizes the familiar notion of orientation that uses the sign of the determinant of the Jacobian matrix.

### Strongly monotone diffeomorphisms and their approximation on finite-dimensional subspaces

In this section and the next two we show that, although Theorem 2 precludes continuous approximation of general diffeomorphisms, stronger constraints on the diffeomorphisms allows one to sidestep the topological obstruction. In this section, in summary, we show that the obstruction to continuous approximation vanishes when the diffeomorphisms in question are assumed to be strongly monotone. Key to our positive result is the following technical result that states that the restriction of the domain and codomain of a strongly monotone diffeomorphism always yields another strongly monotone diffeomorphism.

**Lemma 1**.: _Let \(V\subset X\) be a finite-dimensional subspace of \(X\), and let \(P_{V}:X\to X\) be orthogonal projection onto \(V\). Let \(F:X\to X\) be a strongly monotone \(C^{1}\)-diffeomorphism. Then, \(P_{V}F|_{V}:V\to V\) is strongly monotone, and a \(C^{1}\)-diffeomorphism._

The proof is given in Appendix A.5.1. Lemma 1 implies that the discretization functor \(\mathcal{A}_{\mathrm{lin}}\) defined in (2) is a well-defined functor from strongly monotone \(C^{1}\)-diffeomorphisms of \(X\) to \(C^{1}\)-diffeomorphisms of \(V\). Note that the discretization functor \(\mathcal{A}_{\mathrm{lin}}\) on strongly monotone \(C^{1}\) diffeomorphisms may not be a continuous approximation functor in the strong sense of Definitions 7 and 8, but it is obviously a continuous approximation functor in the weak sense of Definitions 11 and 12. Therefore, we obtain that :

**Proposition 1**.: _Let \(\mathcal{A}_{\mathrm{lin}}\) be the discretization functor that maps \(F\) to \(P_{V}F|_{V}\) for each finite subspace \(V\subset X\). Let \(\mathcal{D}_{sm}\) and \(\mathcal{B}_{sm}\) be categories where \(F\colon X\to X\) and \(F_{V}\colon V\to V\) are strongly monotone \(C^{1}\)-diffeomorphisms. Then, the functor \(\mathcal{A}_{\mathrm{lin}}:\mathcal{D}_{sm}\to\mathcal{B}_{sm}\) satisfies assumption (A') of a weak approximation functor in Definition 11, and is continuous in the weak sense of Definition 12._

Strongly monotone Nemytskii operators and linear bounded operators and their continuous approximation on finite-dimensional subspaces

By Proposition 1, strongly monotone maps can be continuously discretized in the weak sense. Thus, we concern under what conditions, the maps, of which generalized neural operator consists, can be

Figure 1: A figure illustrating the proof ideas for Theorem 2. It represents the disconnected components of diffeomorphisms that preserve orientation, notated by \(\mathrm{diff}^{+}\), and reverse orientation, notated, \(\mathrm{diff}^{-}\). The horizontal axis abstractly represents the two disconnected components of \(\mathrm{diff}\) for a finite-dimensional vector space \(V\). The vertical axis represents the dimension of \(V\). Observe how the two components of \(\mathrm{diff}\) connect as \(\mathrm{dim}(V)\to\infty\), and \(V\) becomes a Hilbert space \(H\).

strongly monotone. In this subsection, we focus on bounded linear operators and Nemytskii operators (layers of neural operator will be discussed in the next subsection). The following lemma is obviously given by the definition of a strongly monotone map :

**Lemma 2**.: _Let \(A:X\to X\) be a linear bounded operator and satisfy \(\langle Au,u\rangle\geq c_{0}\|u\|_{X}^{2}\) for some \(c_{0}>0\). Then, \(A:X\to X\) is strongly monotone._

Next, assuming that \(X=L^{2}(D;\mathbb{R})\), we define Nemytskii operator by

\[F^{\sigma}(u)=\sigma\circ u, \tag{8}\]

where \(\sigma:\mathbb{R}\to\mathbb{R}\) is continuous. In this case, we can show the following lemma by using [50, Corollary 3.3]:

**Proposition 2**.: _Assume that \(\sigma\) satisfies that \(|\sigma(s)|\leq C_{1}|s|+C_{2}\) and the derivative of \(s\to\sigma(s)\) is defined a.e and satisfies \(\sigma^{\prime}(s)\geq\alpha>0\). Then, \(F^{\sigma}:L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R})\) is strongly monotonous._

Strongly monotone generalized neural operators and their continuous approximation on finite-dimensional subspaces

As seen in Theorem 3 below, strongly monotone layers of neural operators do not suffer from the same topological obstruction to continuous discretization as general diffeomorphisms. We now give sufficient conditions for the layers of a neural operator to be strongly monotone, and show that these conditions imply that those are diffeomorphisms.

**Lemma 3**.: _All strongly monotone layers of neural operators (\(F\)) defined by (4) are diffeomorphisms._

Also, the following theorem is proven in Appendix A.6.3.

**Theorem 3**.: _Let \(\mathcal{A}_{\mathrm{lin}}\) be the discretization functor that maps \(F\) to \(P_{V}F|_{V}\) for each finite subspace \(V\subset X\). Let \(\mathcal{D}_{smn}\) and \(\mathcal{B}_{smn}\) be categories where \(F\colon X\to X\) and \(F_{V}\colon V\to V\) are strongly monotone \(C^{1}\)-functions of the form (4). Then, the functor \(\mathcal{A}_{\mathrm{lin}}:\mathcal{D}_{smn}\to\mathcal{B}_{smn}\) satisfies assumption (A), and it is continuous in the sense of Definition 8._

The functor defined in Theorem 3 does not suffer from the same topological obstruction as functors for general diffeomorphisms, shown in the no-go Theorem 2.This is because when \(F_{V}=P_{V}F|_{V}\) is strongly monotone, its derivative \(D|_{x}F_{V}\colon V\to V\) is a strongly monotone matrix at all points \(x\in V\). Therefore it is strictly positive definite (see [47, Prop. 12.3]) and the determinant \(\det(D|_{x}F_{V})\) is strictly positive. Due to this, the orientation of the finite-dimensional approximations never switch signs, and the key technique used in the proof of the no-go Theorem 2 does not apply.

A straightforward condition to guarantee strong monotonicity of a neural operator layer is given in

**Lemma 4**.: _Let \(F:X\to X\) be a layer of neural operator that is of the form \(F(u)=u+T_{2}G(T_{1}u)\), where \(T_{j}:X\to X\), \(j=1,2\) are compact operators and \(G:X\to X\) is a \(C^{1}\)-smooth map. Assume that Frechet derivative \(DG|_{x}\) of \(G\) at \(x\) satisfies the following for all \(x\in X\),_

\[\|DG|_{x}\|_{X\to X}\leq\tfrac{1}{2}\|T_{1}\|_{X\to X}^{-1}\|T_{2}\|_{X\to X}^{ -1}.\]

_Then, \(F\colon X\to X\) is strongly monotone._

See Appendixes A.6.1 and A.6.2 for the proofs.

### Bilipschitz neural operators are conditionally strongly monotone diffeomorphisms

Now we show an analogous result to Theorem 3, but applied to bilipschitz neural operators. Moreover, we will show that all neural operator \(F\colon X\to X\) that are bilipschitz admit approximations that can be locally inverted using iteration for each point in their range using an iteration.

**Theorem 4**.: _Let \(X\) be a Hilbert space. Then there is \(e\in X\), \(\|e\|_{X}=1\) such that the following is true: Let \(F\colon X\to X\) be a layer of a bilipschitz neural operator. Then for all \(r_{1}>0\) and \(\epsilon>0\) there are a linear invertible map \(A_{0}:X\to X\), that is either the identity map or a reflection operator3_\(x\to x-2\langle x,e\rangle_{X}e\), and strongly monotone functions \(H_{k}\) that are also layers of neural operators such that_

\[H_{k}:X\to X,\quad H_{k}(x)=x+B_{k}(x),\quad k=1,2,\ldots,J,\]

_where \(B_{k}:X\to X\) is a compact mapping and satisfies \(\text{Lip}(B_{k})<\epsilon\) and_

\[F(x)=H_{J}\circ\cdots\circ H_{2}\circ H_{1}\circ A_{0}(x),\quad\text{for all }x \in B_{X}(0,r_{1}). \tag{9}\]

_Moreover, if \(F\in C^{2}(X,X)\), then \(J=\mathcal{O}(\epsilon^{-2})\)._

The proof of Theorem 4 is in Appendix A.7.1. Theorem 4 shows that we may always decompose a bilipschitz neural operator into the composition of strongly monotone neural operator layers \(H_{j}\) and a reflection operator \(A_{0}\). Each \(H_{j}\) can be discretized using the continuous functor \(\mathcal{A}_{\text{lin}}\) from Theorem 3. If we consider the discretization (via the construction in Definition 6) using a collection of subsets \(S_{0}(X)\subset S(X)\) such that all \(V\in S_{0}(X)\) satisfy \(e\in V\), then the operator \(A_{0}\) can be discretized by \(A_{0,V}=P_{V}\circ A_{0}|_{V}\). These mean that if we write a bilipschitz neural operator as a sufficiently deep neural operator where each layer is either of the form \(Id+B_{j}\), where \(\text{Lip}(B_{j})<1\), or a reflection operator \(A_{0}\). In either case, we may use linear discretization to approximate each layer. So, we may discretize \(F\) in a ball \(B_{X}(0,R)\) by discretizing each layer \(H_{j}\) and \(A_{1}\) where \(F=H_{J}\circ\cdots\circ H_{1}\circ A_{1}\). We observe that the number of layers, \(J\), depends on \(R\).

We have observed that operators of the form identity plus a compact term are critical for continuous discretization. This insight motivates the introduction of residual networks as approximators within the framework of finite-rank neural operators. In what follows, we assume that \(X\) is a separable Hilbert space, with an orthonormal basis \(\varphi=\{\varphi_{n}\}_{n\in\mathbb{N}}\). For \(N\in\mathbb{N}\), we define \(E_{N}:X\to\mathbb{R}^{N}\) and \(D_{N}:\mathbb{R}^{N}\to X\) by \(E_{N}u:=(\langle u,\varphi_{1}\rangle_{X},...,\langle u,\varphi_{N}\rangle_{X} )\in\mathbb{R}^{N}\). \(D_{N}\alpha:=\sum_{n\leq N}\alpha_{n}\varphi_{n}\). We note that \(P_{N}=D_{N}E_{N}\), where \(P_{N}:X\to X\) is the projection onto \(V_{N}:=\operatorname{span}\{\varphi_{n}\}_{n\leq N}\). Using \(E_{N}\), \(D_{N}\), we define the class of residual networks in the separable Hilbert space, with \(T,N\in\mathbb{N}\) and activation function \(\sigma\), as

\[\mathcal{R}_{T,N,\varphi,\sigma}(X):=\Big{\{}G:X\to X:G= \bigcirc_{t=1}^{T}(Id_{X}+D_{N}\circ NN_{t}\circ E_{N}),\] \[NN_{t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\text{are neural networks with activation function }\sigma\;(t=1,...,T)\Big{\}}. \tag{10}\]

The following theorem proves a universality result for each of the layers \(G\), allowing us to obtain a general universality result for the entire network. The statement of the theorem requires the careful construction of a neural operator-representable function \(\Phi\). Giving a full description of \(\Phi\) involves introducing a lot of technical notation, and so the presentation here in the main text leaves the details of the construction of \(\Phi\) abridged. For the full statement of the theorem and definition of the notation, see Section A.7.2. Intuitively, \(\Phi\) is the 'wrapping' of a fixed-point process in a neural operator.

**Theorem 5**.: _Let \(R>0\), and let \(F\colon X\to X\) be a layer of a bilipschitz neural operator, as in Definition 3. Let \(\sigma\) be the Rectified Cubic Unit (ReLU) function defined by \(\sigma(x):=\max\{0,x\}^{3}\). Then, for any \(\epsilon\in(0,1)\), there are \(T,N\in\mathbb{N}\) and \(G\in\mathcal{R}_{T,N,\varphi,\sigma}(X)\) that has the form_

\[G=(Id_{X}+D_{N}\circ NN_{T}\circ E_{N})\circ\cdots\circ(Id_{X}+D_{N}\circ NN_{ 1}\circ E_{N}),\]

_such that each map \((Id_{X}+D_{N}\circ NN_{t}\circ E_{N})\) is strongly monotone \(C^{1}\)-diffeomorphisms on some ball and_

\[\sup_{x\in\overline{B}_{X}(0,R)}\|F(x)-G\circ A(x)\|_{X}\leq\epsilon,\]

_where \(A\colon X\to X\) is a linear invertible map that is either the identity map or a reflection operator \(x\to x-2\langle x,e\rangle_{X}e\) with some unit vector \(e\in X\). Further, \(G\circ A:B_{X}(0,R)\to G\circ A(B_{X}(0,R))\) is invertible, and there is some neural operator \(\Phi:G\circ A(B_{X}(0,R))\to A(B_{X}(0,R))\) so that \(\big{(}G\circ A|_{B_{X}(0,R)}\big{)}^{-1}=A^{-1}\circ\Phi\)._

The proof is given in Section A.7.2. Neural operator \(\Phi\) becomes a better approximation of the inverse operator when it becomes deeper. Theorem 5 means that neural operators are an operator algebra of nonlinear operators that are closed in composition and when the inverse of a neural operator exists, the inverse operator can be locally approximated by neural operators.

In the case when the separable Hilbert space \(X\) is the real-valued \(L^{2}\)-function space \(L^{2}(D;\mathbb{R})\), residual networks in the separable Hilbert space can be represented as residual neural operators defined by (179). See Lemma 11 for details. Then, we obtain the following 

**Corollary 1**.: _Let \(D\subset\mathbb{R}^{d}\) be a bounded domain, and let \(\varphi=\{\varphi_{n}\}_{n\in\mathbb{N}}\) be an orthonormal basis in \(L^{2}(D;\mathbb{R})\). Assume that the orthonormal basis \(\varphi\) include the constant function. Let \(\mathcal{RNO}_{T,N,\varphi,\sigma}(L^{2}(D;\mathbb{R}))\) be the class of residual neural operators defined in (179). Then, the statement replacing \(X\) with \(L^{2}(D;\mathbb{R})\) and \(G\in\mathcal{R}_{T,N,\varphi,ReLU}(X)\) with \(G\in\mathcal{RNO}_{T,N,\varphi,ReLU}(L^{2}(D;\mathbb{R}))\) in Theorem 5 holds._

The proof is given by a combination of Theorem 5 and Lemma 11. We note that the assumption that the orthonormal basis \(\varphi\) includes the constant function is satisfied if we choose \(\varphi\) to be a Fourier basis, which yields the Fourier neural operator, see e.g., [39; 38].

**Remark 1**.: _In this section, we have shown that residual networks in a separable Hilbert space \(X\), defined in (10), are universal approximators for layers of bilipschitz neural operators. Additionally, in the specific case where \(X=L^{2}(D;\mathbb{R})\), residual neural operators, defined in Definition 9, also provide universal approximators for layers of bilipschitz neural operators. We note that the residual network we have discussed is locally invertible but not globally. By introducing invertible residual networks on Hilbert space \(X\), defined in (166), we can similarly prove that these networks by employing sort activation functions (see [3, Section 4]) are universal approximators for strongly monotone diffeomorphisms with compact support. Specifically, when \(X=L^{2}(D;\mathbb{R})\), invertible residual neural operators, defined in Definition 9, are also universal approximators for strongly monotone diffeomorphisms with compact support. For further details, we refer to Appendix B._

## 4 Quantitative approximation

Quantitative approximation results for neural networks, see e.g. [57] or [23], can be used to derive quantitative error estimates for discretization operations. Let \(F\colon\overline{B}_{X}(0,r)\to X\) be a non-linear function satisfying \(F\in\text{Lip}(\overline{B}_{X}(0,r);X)\), in \(n=1\), or \(F\in C^{n}(\overline{B}_{X}(0,r);X)\), if \(n\geq 2\). Then, \(F\) can be discretized using neural networks in the following way: Let \(\varepsilon_{V}>0\) be numbers indexed by the linear subspaces \(V\subset X\) such that \(\varepsilon_{V}\to 0\) as \(V\to X\). When \(\breve{\varepsilon}=(\varepsilon_{V})_{V\in S(X)}\), in the sense of Definition 1, an \(\breve{\varepsilon}\)-approximation operation \(\mathcal{A}_{NN}:\ F\to(F_{V})_{V\in S(X)}\) in the ball \(B_{X}(0,r)\) can be obtained by defining \(F_{V}=J_{V}^{-1}\circ F_{V,\theta}\circ J_{V}:\ V\to V\), where \(J_{V}:\ V\to\mathbb{R}^{d}\) is an isometric isomorphism, \(d=\text{dim}(V)\), \(F_{V,\theta}\colon\ \mathbb{R}^{d}\to\mathbb{R}^{d}\) is a feed-forward neural network with ReLU-activation functions with at most \(C(d)\log_{2}((1+r)/\varepsilon_{V})\) layers and \(C(d)\varepsilon_{V}^{d}\log_{2}((1+r)/\varepsilon_{V})\) non-zero elements in the weight matrices. Details of this result are given in Proposition 5 in Appendix A.8.

## 5 Conclusion

In this work, we have studied the problem of discretizing neural operators between Hilbert spaces. Many physical models concern functions \(\mathbb{R}^{n}\to\mathbb{R}\), for example \(L^{2}(\mathbb{R}^{n})\), the computational methods based on approximations in finite dimensional spaces should become better when the dimension of the model grows and tends to infinity. We have focused on diffeomorphisms in infinite dimensions, which are crucial to understand in generative modeling. We have shown that the approximation of diffeomorphisms leads to computational difficulties. We used tools from category theory to produce a no-go theorem showing that general diffeomorphisms between Hilbert spaces may not admit any continuous approximations by diffeomorphisms on finite spaces, even if the approximations are allowed to be nonlinear. We then proceeded to give several positive results, showing that diffeomorphisms between Hilbert spaces may be continuously approximated if they are further assumed to be strongly monotone. Moreover, we showed that the difficulties can be avoided by considering a restricted but still practically rich class of diffeomorphisms. This includes bilipschitz neural operators, which may be represented in any bounded set as a composition of strongly monotone neural operators and strongly monotone diffeomorphisms. We then showed that such operators may be inverted locally via an iteration scheme. Finally we gave a simple example on how quantitative stability questions can be obtained for discretization functors, inviting other researchers to study related questions using more sophisticated methods.

## Acknowledgments

TF was supported by JSPS KAKENHI Grant Number JP24K16949, JST CREST JPMJCR24Q5, JST ASPIRE JPMJAP2329, and Grant for Basic Science Research Projects from The Sumitomo Foundation. MP was supported by CAPITAL Services of Sioux Falls, South Dakota and NSF-DMS under grant 3F5083. MVdH was supported by the Simons Foundation under the MATH + X program, the Department of Energy, BES under grant DE-SC0020345, and the corporate members of the Geo-Mathematical Imaging Group at Rice University. A significant part of the work of MVdH was carried out while he was an invited professor at the Centre Sciences des Donnees at Ecole Normale Superieure, Paris. M.L. was partially supported by a AdG project 101097198 of the European Research Council, Centre of Excellence of Research Council of Finland and the FAME flagship. Views and opinions expressed are those of the authors only and do not necessarily reflect those of the funding agencies or the EU.

## References

* [1] Ahmed Abdeljawad and Philipp Grohs. Approximations with deep neural networks in sobolev time-space. _Analysis and Applications_, 20(03):499-541, 2022.
* [2] Giovanni S. Alberti, Matteo Santacesaria, and Silvia Sciutto. Continuous generative neural networks. _arXiv:2205.14627_, 2022.
* [3] Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In _International Conference on Machine Learning_, pages 291-301. PMLR, 2019.
* [4] Ivo Babuska and Manil Suri. On locking and robustness in the finite element method. _SIAM J. Numer. Anal._, 29(5):1261-1293, 1992.
* [5] Heinz H. Bauschke and Patrick L. Combettes. _Convex analysis and monotone operator theory in Hilbert spaces_. CMS Books in Mathematics/Ouvrages de Mathematiques de la SMC. Springer, Cham, second edition, 2017. With a foreword by Hedy Attouch.
* [6] Roberto Bentivoglio, Elvin Isufi, Sebastian Nicolaas Jonkman, and Riccardo Taormina. Deep learning methods for flood mapping: a review of existing applications and future research directions. _Hydrology and earth system sciences_, 26(16):4345-4378, 2022.
* [7] Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural networks for parametric pdes. _The SMAI journal of computational mathematics_, 7:121-157, 2021.
* [8] Klaus Bohmer. _Numerical methods for nonlinear elliptic differential equations_. Numerical Mathematics and Scientific Computation. Oxford University Press, Oxford, 2010. A synopsis.
* [9] Klaus Bohmer and Robert Schaback. A nonlinear discretization theory. _J. Comput. Appl. Math._, 254:204-219, 2013.
* [10] Philippe G. Ciarlet. _Linear and Nonlinear Functional Analysis with Applications_. Society for Industrial and Applied Mathematics, USA, 2013.
* [11] Masoumeh Dashti, Stephen Harris, and Andrew Stuart. Besov priors for Bayesian inverse problems. _Inverse Probl. Imaging_, 6(2):183-200, 2012.
* [12] Maarten De Hoop, Daniel Zhengyu Huang, Elizabeth Qian, and Andrew M Stuart. The cost-accuracy trade-off in operator learning with neural networks. _arXiv preprint arXiv:2203.13181_, 2022.
* [13] Dennis Elbrachter, Dmytro Perekrestenko, Philipp Grohs, and Helmut Bolcskei. Deep neural network approximation theory. _IEEE Transactions on Information Theory_, 67(5):2581-2623, 2021.
* [14] Takashi Furuya, Michael Puthawala, Matti Lassas, and Maarten V de Hoop. Globally injective and bijective neural operators. _arXiv preprint arXiv:2306.03982_, 2023.

* [15] Jean Gallier. _Geometric methods and applications_, volume 38 of _Texts in Applied Mathematics_. Springer-Verlag, New York, 2001. For computer science and engineering.
* [16] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2414-2423, 2016.
* [17] Bruno Gavranovic, Paul Lessard, Andrew Dudzik, Tamara von Glehn, Joao GM Araujo, and Petar Velickovic. Categorical deep learning: An algebraic theory of architectures. _arXiv preprint arXiv:2402.15332_, 2024.
* [18] David Gilbarg and Neil S. Trudinger. _Elliptic partial differential equations of second order_. Classics in Mathematics. Springer-Verlag, Berlin, 2001. Reprint of the 1998 edition.
* [19] Evarist Gine and Richard Nickl. _Mathematical foundations of infinite-dimensional statistical models_. Cambridge university press, 2021.
* [20] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. _Advances in neural information processing systems_, 30, 2017.
* [21] Somdatta Goswami, Aniruddha Bora, Yue Yu, and George Em Karniadakis. Physics-informed deep neural operator networks. In _Machine Learning in Modeling and Simulation: Methods and Applications_, pages 219-254. Springer, 2023.
* [22] Ingo Guhring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep relu neural networks in w s, p norms. _Analysis and Applications_, 18(05):803-859, 2020.
* [23] Ingo Guhring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep ReLU neural networks in \(W^{s,p}\) norms. _Anal. Appl. (Singap.)_, 18(5):803-859, 2020.
* [24] E. M. Harrell and W. J. Layton. \(L^{2}\) estimates for Galerkin methods for semilinear elliptic equations. _SIAM J. Numer. Anal._, 24(1):52-58, 1987.
* [25] Isao Ishikawa, Takeshi Teshima, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama. Universal approximation property of invertible neural networks. _Journal of Machine Learning Research_, 24(287):1-68, 2023.
* [26] Thomas Jech. _Set theory_. Perspectives in Mathematical Logic. Springer-Verlag, Berlin, second edition, 1997.
* [27] Jari Kaipio and Erkki Somersalo. _Statistical and computational inverse problems_, volume 160 of _Applied Mathematical Sciences_. Springer-Verlag, New York, 2005.
* [28] R. I. Kacurovskii. Nonlinear monotone operators in Banach spaces. _Uspehi Mat. Nauk_, 23(2(140)):121-168, 1968.
* [29] Gavin Kerrigan, Justin Ley, and Padhraic Smyth. Diffusion generative models in infinite dimensions. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 9538-9563. PMLR, 25-27 Apr 2023.
* [30] Nikola B Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew M Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. _J. Mach. Learn. Res._, 24(89):1-97, 2023.
* [31] Anastasis Kratsios and Ievgen Bilokopytov. Non-euclidean universal approximation. _Advances in Neural Information Processing Systems_, 33:10635-10646, 2020.
* [32] Nicolaas H Kuiper. The homotopy type of the unitary group of hilbert space. _Topology_, 3(1):19-30, 1965.
* [33] Samuel Lanthaler, Zongyi Li, and Andrew M Stuart. The nonlocal neural operator: Universal approximation. _arXiv preprint arXiv:2304.13221_, 2023.

* [34] Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep learning framework in infinite dimensions. _Transactions of Mathematics and Its Applications_, 6(1):tmac001, 2022.
* [35] Samuel Lanthaler, Andrew M Stuart, and Margaret Trautner. Discretization error of fourier neural operators. _arXiv preprint arXiv:2405.02221_, 2024.
* [36] Matti Lassas and Samuli Siltanen. Can one use total variation prior for edge-preserving bayesian inversion? _Inverse problems_, 20(5):1537, 2004.
* [37] Markku S Lehtinen, Lassi Paivarinta, and Erkki Somersalo. Linear inverse problems for generalised random variables. _Inverse Problems_, 5(4):599, 1989.
* [38] Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for pdes on general geometries. _Journal of Machine Learning Research_, 24(388):1-26, 2023.
* [39] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020.
* [40] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* [41] Robert J. Martin and Patrizio Neff. Minimal geodesics on gl(n) for left-invariant, right-o(n)-invariant riemannian metrics. _Journal of Geometric Mechanics_, 8(3):323-357, 2016.
* [42] Peter W. Michor and David Mumford. A zoo of diffeomorphism groups on \(\mathbb{R}^{n}\). _Ann. Global Anal. Geom._, 44(4):529-540, 2013.
* [43] Donal O'Regan, Yeol Je Cho, and Yu-Qing Chen. _Topological degree theory and applications_, volume 10 of _Series in Mathematical Analysis and Applications_. Chapman & Hall/CRC, Boca Raton, FL, 2006.
* [44] Michael Puthawala, Konik Kothari, Matti Lassas, Ivan Dokmanic, and Maarten de Hoop. Globally injective relu networks. _Journal of Machine Learning Research_, 23(105):1-55, 2022.
* [45] Calvin R. Putnam and Aurel Wintner. The connectedness of the orthogonal group in Hilbert space. _Proc. Nat. Acad. Sci. U.S.A._, 37:110-112, 1951.
* [46] Calvin R. Putnam and Aurel Wintner. The orthogonal group in Hilbert space. _Amer. J. Math._, 74:52-78, 1952.
* [47] R. Tyrrell Rockafellar and Roger J.-B. Wets. _Variational analysis_, volume 317 of _Grundlehren der mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]_. Springer-Verlag, Berlin, 1998.
* [48] Matti Lassas Saksman, Samuli Siltanen, et al. Discretization-invariant bayesian inversion and besov space priors. _arXiv preprint arXiv:0901.4220_, 2009.
* [49] Martin H. Schultz. Error bounds for the Rayleigh-Ritz-Galerkin method. _J. Math. Anal. Appl._, 27:524-533, 1969.
* [50] R. E. Showalter. _Monotone operators in Banach space and nonlinear partial differential equations_, volume 49 of _Mathematical Surveys and Monographs_. American Mathematical Society, Providence, RI, 1997.
* [51] Andrew M Stuart. Inverse problems: a bayesian perspective. _Acta numerica_, 19:451-559, 2010.
* [52] Manil Suri, Ivo Babuska, and Christoph Schwab. Locking effects in the finite element approximation of plate models. _Math. Comp._, 64(210):461-482, 1995.
* [53] Michael E. Taylor. _Partial differential equations I. Basic theory_, volume 115 of _Applied Mathematical Sciences_. Springer, New York, second edition, 2011.

* [54] Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama. Coupling-based invertible neural networks are universal diffeomorphism approximators. _Advances in Neural Information Processing Systems_, 33:3362-3373, 2020.
* [55] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. _Nature_, 620(7972):47-60, 2023.
* [56] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael Steinbach, and Vipin Kumar. Integrating scientific knowledge with machine learning for engineering and environmental systems. _ACM Computing Surveys_, 55(4):1-37, 2022.
* [57] Dmitry Yarotsky. Error bounds for approximations with deep relu networks. _Neural Networks_, 94:103-114, 2017.

Proofs and additional examples

### Non-linear discretization

In this section we consider the non-linear discretization theory, see [9]. Let \(\Omega\subset\mathbb{R}^{n}\) be a bounded open set with smooth boundary. Let \(H^{1}(\Omega)=\{u\in L^{2}(\Omega)\mid\nabla u\in L^{2}(\Omega)\}\) and \(H^{1}_{0}(\Omega)=\{u\in H^{1}(\Omega)\mid u|_{\partial\Omega}=0\}\) be the Sobolev spaces with an integer order of smoothness and \(H^{s}(\Omega)\), \(s\in\mathbb{R}\) the Sobolev space with a fractional order of smoothness [53].

For simplicity, we assume in the section that \(n=1\), and that \(\Omega\subset\mathbb{R}\) is an interval and we point out that in this case the Sobolev embedding theorem implies \(H^{1}(\Omega)\subset C(\overline{\Omega})\) which significantly simplifies the constructions below.

Let \(g\in C^{m+1}(\mathbb{R};\mathbb{R})\), \(m\geq 1\), be the derivative of a convex function \(G\in C^{m+2}(\mathbb{R};\mathbb{R})\) satisfying

\[-c_{0}\leq G(r)\leq c_{1}(1+r^{p}),\ r\in\mathbb{R},\ c_{0},c_{1},p>0. \tag{11}\]

Let \(F=F^{(g)}\colon H^{1}_{0}(\Omega)\to H^{1}_{0}(\Omega)\), \(F^{(g)}\colon x\to u\) be the solution operator of the non-linear differential equation

\[\Delta u(t)-g(u(t))=x(t),\quad t\in\Omega, \tag{12}\] \[u|_{\partial\Omega}=0, \tag{13}\]

where \(\Delta\) is the Laplace operator operating in the \(t\) variable, where the function \(x(t)\) is a physical source term. Because the \(u\) that solves (12)-(13) is the minimizer of the strictly convex and lower semi-continuous function \(H:X\to\mathbb{R}\),

\[H(v)=\|\nabla v\|_{L^{2}(\Omega)^{2}}^{2}+\int_{\Omega}(G(v(t))-x(t)v(t))dt, \tag{14}\]

the Weierstrass theorem, see [50], Theorem II.8.1, implies that (12)-(13) has a unique solution. Moreover, by regularity theory for elliptic equations, see [18], Theorem 8.13, we have that the solution \(u\) is in \(H^{3}(\Omega)\). Moreover, as \(G\) is convex and satisfies (11), the function \(H\) in (14) is coercive, for any \(R>0\) there is \(\rho(R)>0\) such that when \(\|x\|_{H^{1}_{0}(\Omega)}\leq R\), then \(\|u\|_{H^{3}(\Omega)}\leq\rho(R)\).

The equation (12)-(13) can be written also as an integral equation

\[u(t)+\int_{\Omega}\Psi(t,t^{\prime})g(u(t^{\prime}))dt^{\prime}=-\int_{\Omega }\Psi(t,t^{\prime})x(t^{\prime})dt^{\prime},\quad t\in\Omega, \tag{15}\]

where \(\Psi(t,t^{\prime})\) is the Dirichlet Green's function of the negative Laplacian, that is, \(-\Delta\Psi(\cdot,t^{\prime})=\delta_{t^{\prime}}(\cdot)\), \(\Psi(\cdot,t^{\prime})|_{\partial\Omega}=0\), and \(g_{*}(u)=g(u)\). We can write (15) as

\[(Id+Q\circ g_{*})u=-Q(x),\quad Qu(t)=\int_{\Omega}\Psi(t,t^{\prime})x(t^{ \prime})dt^{\prime}, \tag{16}\]

where \(i_{H^{1}_{0}\to H^{3/4}}:H^{1}_{0}(\Omega)\to H^{3/4}(\Omega)\) and \(i_{H^{2}\to H^{1}_{0}}:H^{2}(\Omega)\cap H^{1}_{0}(\Omega)\to H^{1}_{0}(\Omega)\) are compact operators and as \(H^{3/4}(\Omega)\subset C(\overline{\Omega})\), the operators \(g_{*}:H^{3/4}(\Omega)\to L^{2}(\Omega)\) and \(Q:L^{2}(\Omega)\to H^{2}(\Omega)\cap H^{1}_{0}(\Omega)\) are continuous, we see that

\[F=Id+i_{H^{2}\to H^{1}}\circ(Q\circ g_{*})\circ i_{H^{1}\to H^{3/4}}:H^{1}_{0 }(\Omega)\to H^{1}_{0}(\Omega),\]

is a layer of the neural operator. As \(G\in C^{m+1}\), \(m\geq 2\), we have that \(F:H^{1}_{0}(\Omega)\to H^{1}_{0}(\Omega)\) is \(C^{1}\)-smooth. Moreover, as \(G\) is convex, the operator \(F:H^{1}(\Omega)\to H^{1}(\Omega)\) is strongly monotone and hence by Lemma 3, \(F\) has an inverse map \(F^{-1}:H^{1}(\Omega)\to H^{1}(\Omega)\) that is \(C^{1}\)-smooth. Thus, the solution \(u\) of (12)-(13) can be represented as

\[u=F^{-1}(Q(x)).\]

Let \(V\subset X=H^{1}_{0}(\Omega)\) be a finite dimensional space. The Galerkin methods to obtain an approximation for the solution of the boundary value problem (12)-(13) using Finite Element Method. To do this, let \(w\in V\) be the solution of the problem

\[\int_{\Omega}(\nabla w(t)\cdot\nabla\phi(t)+g(w(t))\phi(t))dt=-\int_{\Omega}x( t)\cdot\phi(t)dt,\quad\text{for all }\phi\in V. \tag{17}\]When \(\phi_{1},\ldots,\phi_{d}\in V\) are a basis of the finite dimensional vector space \(V\), and we write \(w=\sum_{k=1}^{d}w_{k}\phi_{j}\), the problem (17) is equivalent that \((w_{1},\ldots,w_{d})\in\mathbb{R}^{d}\) satisfies the system equations \(d\) equations

\[\sum_{k=1}^{d}b_{jk}w_{k}=-\int_{\Omega}\phi_{j}(t)g\bigg{(}\sum_{k=1}^{d}w_{k} \phi_{k}(t)\bigg{)}\,dt-\int_{\Omega}x(t)\cdot\phi_{j}(t)dt,\quad\text{for }j=1,2, \ldots,d, \tag{18}\]

where

\[b_{jk}=\int_{\Omega}\nabla\phi_{j}(t)\cdot\nabla\phi_{k}(t)dt.\]

When \(x\in V\), we define the map \(F^{(g)}_{V}\colon V\to V\) by setting map \(F^{(g)}_{V}(x)=w\), where \(w\in V\) is the solution of the problem (17). When

\[\mathcal{F}_{PDE}=\{F^{(g)}\colon X\to X\mid g(s)=\frac{dG}{ds}(s)\,,G\in C^{ m+2}(\mathbb{R})\text{ is convex and satisfies (\ref{eq:main_eq})}\},\]

we define the map \(\mathcal{A}_{FEM}\colon F^{(g)}\to F^{(g)}_{V}\) for \(F^{(g)}\in\mathcal{F}_{PDE}\). As the the function \(g\) in the equation (18) is non-linear, the solution \(u\) of the continuum problem (15)-(13) and the solution \(w\) of the finite dimensional problem have typically no linear relationship, even if the source \(x\) satisfies \(x\in V\).

Let \(R>0\) As the embedding \(H^{2}(\Omega)\cap H^{1}_{0}(\Omega)\to H^{1}_{0}(\Omega)\) is compact and \(Q:H^{1}_{0}(\Omega)\to H^{2}(\Omega)\cap H^{1}_{0}(\Omega)\) is continuous, we see that the image of the closed ball \(\overline{B}_{X}(0,R)\) in the map \(Q\) is a precompact set \(Q(\overline{B}_{X}(0,R))\subset X\). As \(F^{-1}:X\to X\) is continuous the set of corresponding solutions,

\[Z_{R}=\{u\in X\mid u=F^{-1}(Q(x)),\;x\in\overline{B}_{X}(0,R)\},\]

is precompact in \(X\). Hence,

\[\lim_{V\to X}\sup_{u\in\overline{Z}_{R}}\|(Id-P_{V})(u)\|_{X}=0. \tag{19}\]

As \(G\) is convex, the Frechet derivative of the map \(F=Id+Q\circ g_{*}:X\to X\) is a strictly positive linear operator at all points \(x\in X\). This, the uniform convergence (19), and the convergence results for the Galerkin method for semi-linear equations, [49, Theorem 3.2], see also [24], imply that in the space \(X=H^{1}_{0}(\Omega)\)

\[\lim_{V\to X}\sup_{\|x\|_{X}\leq R}\|F^{(g)}_{V}(x)-F^{(g)}(x)\|_{X}=0. \tag{20}\]

These imply that \(\mathcal{A}_{FEM}\colon F^{(g)}\to F^{(g)}_{V}\) is an approximation operation for the function \(F^{(g)}\in\mathcal{F}_{PDE}\).

The map \(F^{(g)}_{V}:V\to V\) is called the Galerkin approximation of the problem (12)-(13) and is an example of the non-linear approximation methods. The properties of the Galerkin approximation is studied in detail in [8], in particular sections 4.4 and 4.5.

### Negative results

On the positive results, in Appendix A.1, we have considered nonlinear discretization of the operators \(u\to-\Delta u+g(u)\). To exemplify the negative result, we consider the following example on the solution operation of differential equations and the non-existence of approximation by diffeomorphic maps: We consider the elliptic (but not not strongly elliptic) problem

\[B_{s}u:=-\frac{d}{dt}\bigg{(}(1+t)\text{sign}(t-s)\frac{d}{dt}u(t)\bigg{)}=f(t ),\quad t\in[0,1], \tag{21}\]

with the Dirichlet and Neumann boundary conditions

\[u(0)=0,\quad\frac{d}{dt}u(t)\bigg{|}_{t=1}=0. \tag{22}\]

Here, \(0\leq s\leq 1\) is a parameter of the coeffient function and \(\text{sign}(t-s)=1\) if \(t>s\) and \(\text{sign}(t-s)<0\) if \(t<s\). We consider the weak solutions of (21) in the space

\[u\in H^{1}_{D,N}(0,1):=\{v\in H^{1}(0,1):\;v(0)=0\}.\]We can write

\[B_{s}u=-D_{t}^{(2)}A_{s}D_{t}^{(1)}u,\]

where

\[A_{s}v(t)=(1+t)\mbox{sign}(t-s)v(t),\]

parametrized by \(0\leq s\leq\), are multiplication operations that are invertible operators, \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) (this invertibility makes the equation (21) elliptic). Moreover, \(D_{t}^{(1)}\) and \(D_{t}^{(2)}\) are the operators \(v\to\frac{d}{dt}v\) with the Dirichlet boundary condition \(v(0)=0\) and \(v(1)=0\), respectively. We consider the Hilbert space \(X=H^{1}_{D,N}(0,1)\); to generate an invertible operator \(G_{s}:X\to X\) related to (21), we write the source term using an auxiliary function \(g\),

\[f(t)=Qg:=-\frac{d^{2}}{dt^{2}}g(t)+g(t).\]

Then the equation,

\[B_{s}u=Qg, \tag{23}\]

defines a continuous and invertible operator,

\[G_{s}:X\to X,\quad G_{s}:g\to u.\]

In fact, \(G_{s}=B_{s}^{-1}\circ Q\) when the domains of \(B_{s}\) and \(Q\) are chosen in a suitable way. The Galerkin method (that is, the standard approximation based on the Finite Element Method) to approximate the equation (23) involves introducing a complete basis \(\chi_{j}(t)\), \(j=1,2,\dots\) of the Hilbert space \(X\), the orthogonal projection

\[P_{n}:X\to X_{n}:=\mbox{span}\{\chi_{j}:\;j=1,2,\dots,n\},\]

and approximate solutions of (23) through solving

\[P_{n}B_{s}P_{n}u_{n}=P_{n}QP_{n}g_{n},\quad u_{n}\in X_{n},\;g_{n}=P_{n}g. \tag{24}\]

This means that operator \(B_{s}^{-1}Q:g\to u\) is approximated by \((P_{n}B_{s}P_{n})^{-1}P_{n}QP_{n}:g_{n}\to u_{n}\), when \(P_{n}B_{s}P_{n}:X_{n}\to X_{n}\) is invertible.

The above corresponds to the Finite Element Method where the matrix defined by the operator \(P_{n}B_{s}P_{n}\) is \(\mathbf{b}(s)=[b_{jk}(s)]_{j,k=1}^{n}\in\mathbb{R}^{n\times n}\), where

\[b_{jk}(s)=\int_{0}^{1}(1+t)\mbox{sign}(t-s)\frac{d}{dt}\chi_{j}(t)\cdot\frac{ d}{dt}\chi_{k}(t)dt,\quad j,k=1,\dots,n.\]

Since we used the mixed Dirichlet and Neumann boundary conditions in the above boundary value problem, we see that for \(s=0\) all eigenvalues of the matrix \(\mathbf{b}(s)\) are strictly positive, and when \(s=1\) all eigenvalues are strictly negative. As the function \(s\to\mathbf{b}(s)\) is a continuous matrix-valued function, we see that there exists \(s\in(0,1)\) such that the matrix \(\mathbf{b}(s)\) has a zero eigenvalue and is no invertible. Thus, we have a situation where all operators \(B_{s}^{-1}Q:g\to u\), \(s\in[0,1]\) are invertible (and thus define diffeomorphisms \(X\to X\)) but for any basis \(\chi_{j}(t)\) and any \(n\) there exists \(s\in(0,1)\) such that the finite dimensional approximation \(\mathbf{b}(s):\mathbb{R}^{n}\to\mathbb{R}^{n}\) is not invertible. This example shows that there is no FEM-based discretization method for which the finite dimensional approximations of all operators \(B_{s}^{-1}Q\), \(s\in(0,1)\), are invertible. The above example also shows a key difference between finite and infinite dimensional spaces. The operator \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) has only continuous spectrum and not eigenvalues nor eigenfunctions whereas the finite dimensional matrices have only point spectrum (that is, eigenvalues). The continuous spectrum makes it possible to deform the positive operator \(A_{s}\) with \(s=0\) to a negative operator \(A_{s}\) with \(s=1\) in such a way that all operators \(A_{s}\), \(0\leq s\leq 1\), are invertible but this is not possible to do for finite dimensional matrices. We point out that the map \(s\to A_{s}\) is not continuous in the operator norm topology but only in the strong operator topology and the fact that \(A_{0}\) can be deformed to \(A_{1}\) in the norm topology by a path that lies in the set of invertible operators is a deeper result. However, the strong operator topology is enough to make the FEM matrix \(\mathbf{b}(s)\) to depend continuously on \(s\).

### Application of Theorem 2 to Neural Operators

In this section, we give an example of the application of Theorem 2, as applied to the problem of using a neural operator to solve a linear PDE. The guiding idea is to draw inspiration from Figure 1; the space of diffeomorphisms on \(X\) is disconnected if \(X\) is a finite-dimensional space, but connected if it is a Hilbert space.

Before constructing the path, we first construct a isotopy of diffeomorphisms in \(L^{2}(\Omega)\) in several steps

Let \(\rho\colon\mathbb{R}\to[0,1]\) be a smooth function such that \(\rho|_{(-\infty,0]}=0\), \(\rho|_{[1,\infty)}=1\). Let \(R(\theta)\in\mathbb{R}^{2\times 2}\) be the rotation matrix given by

\[R(\theta)\coloneqq\begin{pmatrix}\cos(\theta)&\sin(\theta)\\ -\sin(\theta)&\cos(\theta)\end{pmatrix}. \tag{25}\]

Then, define the function \(R_{i}\colon\mathbb{R}\to\mathbb{R}^{2\times 2}\) by

\[R_{i}(t)\coloneqq R(\rho(t-i)\pi). \tag{26}\]

Note that \(R_{i}(t)\in SO(2)\), for \(t\leq i\), \(R_{i}(t)=I\), \(t\geq i+1\), \(R_{i}(t)=-I\).

Next, we consider the 'little cell' space, \(\ell^{2}\coloneqq\big{\{}(\alpha_{1},\alpha_{2},\dots,)\colon\sum_{i=1}^{ \infty}\alpha_{i}^{2}<\infty\big{\}}\). Consider the linear operator \(\hat{\mathcal{R}}\colon\mathbb{R}\to\big{(}\ell^{2}\to\ell^{2}\big{)}\) on \(\ell^{2}\) given by a pair-wise coordinate representation given by the infinite dimensional'matrix' with block structure

\[\hat{\mathcal{R}}(t)\coloneqq\begin{pmatrix}R_{1}(t)&&&\\ &R_{2}(t)&\\ &&\ddots\end{pmatrix}. \tag{27}\]

Finally, define \(\mathcal{R}\colon[0,1]\to(\ell^{2}\to\ell^{2})\) by

\[\mathcal{R}(t)\coloneqq\begin{cases}\hat{\mathcal{R}}\left(\frac{1}{1-t} \right)&t\in[0,1)\\ -I&t\geq 1\end{cases}, \tag{28}\]

and define the related operator \(\tilde{\mathcal{R}}\colon[0,1]\to\big{(}\ell^{2}\to\ell^{2}\big{)}\) by

\[\tilde{\mathcal{R}}(t)=\begin{pmatrix}-1&&\\ &\mathcal{R}(t)\end{pmatrix}. \tag{29}\]

**Proposition 3** (\(\mathcal{R}\), \(\tilde{\mathcal{R}}\) are isotopies of smooth diffeomorphisms in \(\ell^{2}\)).: _The maps \(H_{1}\colon\ell^{2}\times[0,1]\to\ell^{2}\) and \(H_{2}\colon\ell^{2}\times[0,1]\to\ell^{2}\) defined by_

\[H_{1}(v,t)=\mathcal{R}(t)v,\quad H_{2}(v,t)=\tilde{\mathcal{R}}(t)v, \tag{30}\]

_are isotopies of smooth diffeomorphisms in \(\ell^{2}\) and agree at \(t=1\)._

Proof.: We prove that \(H_{1}\) is an isotopy of smooth diffeomorphisms on \(\ell^{2}\), as the proof of the same for \(H_{2}\) is very similar.

We first show that \(H_{1}(\cdot,t)\in\operatorname{diff}(\ell^{2})\) for each \(t\).

For \(t<1\), \(\mathcal{R}\) operates on vectors in \(\ell^{2}\) in only a finite number of indices. In those indices it corresponds to a rotation, and hence is a smooth diffeomorphism. When \(t\geq 1\), it corresponds to scalar multiplication by \(-1\), and so is too a smooth diffeomorphism. Now we show that \(\mathcal{R}(t)\) is continuous on. The only point where continuity may fail is at \(t=1\), and there it may only fail in thelimit from the left. Given a \(v\in\ell^{2}\), we compute

\[\lim_{t\to 1^{-}}\left\|\mathcal{R}(t)v-\mathcal{R}(1)v\right\|_{2}= \lim_{t\to 1^{-}}\left\|\left(\begin{array}{c}R_{1}(\frac{1}{1-t})\begin{pmatrix}v _{1}\\ v_{2}\\ R_{2}(\frac{1}{1-t})\begin{pmatrix}v_{3}\\ v_{4}\\ \end{pmatrix}\\ \vdots\\ R_{k}(\frac{1}{1-t})\begin{pmatrix}v_{2k-1}\\ v_{2k}\\ \end{pmatrix}\\ R_{k+1}(\frac{1}{1-t})\begin{pmatrix}v_{2k+1}\\ v_{2k+2}\\ \end{pmatrix}\\ \vdots\\ \end{array}\right)+\begin{pmatrix}v_{1}\\ v_{2}\\ v_{2}\\ v_{2k+1}\\ \vdots\\ \end{pmatrix}\right\|_{2}. \tag{31}\]

Let \(k^{*}(t)\coloneqq\lfloor\frac{1}{1-t}\rfloor\). Then for any integer \(i\leq k\), we have that

\[R_{i}\left(\frac{1}{1-t}\right)=R\left(\rho\left(\underbrace{\frac{1}{1-t}-i}_{ \geq 1}\right)\pi\right)=R(\pi)=-I\in\mathbb{R}^{2\times 2}, \tag{32}\]

and likewise when \(i\geq k^{*}(t)+1\), then \(R_{i}\left(\frac{1}{1-t}\right)=I\in\mathbb{R}^{2\times 2}\), and so the r.h.s. of Eqn. 31 becomes

\[\lim_{t\to 1^{-}}\left\|\left(\begin{array}{c}-v_{1}\\ -v_{2}\\ -v_{3}\\ -v_{4}\\ \vdots\\ R_{k^{*}(t)}(\frac{1}{1-t})\begin{pmatrix}v_{2k^{*}(t)-1}\\ v_{2k^{*}(t)}\\ v_{2k^{*}(t)+1}\\ v_{2k^{*}(t)+2}\\ \vdots\\ \end{pmatrix}\\ +\begin{pmatrix}v_{1}\\ v_{2}\\ v_{3}\\ v_{4}\\ \vdots\\ \end{pmatrix}\\ \vdots\\ \end{array}\right)+\begin{pmatrix}v_{1}\\ v_{2}\\ v_{3}\\ v_{4}\\ \vdots\\ \end{pmatrix}\right\|_{2} \tag{33}\] \[=\lim_{t\to 1^{-}}\left\|\left(\begin{array}{c}0\\ 0\\ 0\\ 0\\ \vdots\\ R_{k^{*}(t)}(\frac{1}{1-t})\begin{pmatrix}v_{2k^{*}(t)-1}\\ v_{2k^{*}(t)}\\ \end{pmatrix}+\begin{pmatrix}v_{2k^{*}(t)-1}\\ v_{2k^{*}(t)+2}\\ 2v_{2k^{*}(t)+1}\\ \vdots\\ \end{pmatrix}\\ \vdots\\ \end{array}\right)\right\|_{2}\] (34) \[\leq 2\sqrt{\sum_{i=k^{*}(t)}^{\infty}v_{i}^{2}}. \tag{35}\]

As \(t\to 1^{-}\), \(k^{*}(t)\to\infty\), and so, by standard estimates, \(2\sqrt{\sum_{i=k^{*}(t)}^{\infty}v_{i}^{2}}\to 0\). Hence, \(\mathcal{R}(t)\) is continuous on \(\ell^{2}\) at \(t=1\).

Finally, by definition at \(t=1\), we have that \(H_{1}(v,1)=\mathcal{R}(1)=-I=\mathcal{R}(1)=H_{2}(v,1)\). 

Finally, we define \(H\colon\mathbb{R}^{\infty}\times[0,1]\to\mathbb{R}^{\infty}\) by gluing the isotopies \(H_{1}\) and \(H_{2}\) together by

\[H(v,t)\coloneqq\left\{\begin{array}{ll}H_{1}(v,2t)&\text{if }t\leq\frac{1}{2} \\ H_{2}(v,2-2t)&\text{if }t>\frac{1}{2}\end{array}.\right. \tag{36}\]

**Proposition 4**.: _The function \(H\) given by Eqn. 36 is an isotopy of diffeomorphisms in \(\ell^{2}\)._

Proof.: Continuity of \(H\) follows from continuity of \(\mathcal{R}\), \(\tilde{\mathcal{R}}\), and that \(\mathcal{R}(1)=\tilde{\mathcal{R}}(1)\). For each \(t\in[0,1]\), \(H(\cdot,t)\in\operatorname{diff}(\ell^{2})\). 

### Proofs from Sec. 3.1

#### a.4.1 Proof of Theorem 2

Proof.: Assume that \(\mathcal{A}\colon\mathcal{D}\to\mathcal{B}\) is a functor that satisfies the property (A) of an approximation functor and is continuous. Let us consider the case when \(X=\ell^{2}\).

Let \(e\in X\) be a unit vector and \(B_{e}:X\to X\) be a linear map \(Bx=x-2\langle x,e\rangle_{X}e\). In other words, \(B\) is a diagonal matrix \(\text{diag}(-1,1,\dots)\) in some orthogonal basis where \(e\) is the first basis vector.

By [32, Theorem 2], see also and [45], for an infinite dimensional Hilbert space \(X\) the space \(GL(X)\) of linear invertible maps have only one topological component in the operator norm topology and the set \(GL(X)\) is contractible and hence path-connected. This implies that there are linear maps \(A_{t}:X\to X\), \(t\in[0,1]\) such that \(A_{0}=Id\), and \(A_{1}\in GL(X)\) is arbitrary invertible linear map and that \(t\to A_{t}\in GL(X)\) is continuous. Similarly, in an infinite dimensional real Hilbert space \(X\) the space \(O_{\mathbb{R}}(X)\) of the orthogonal operators \(A:X\to X\) is path-connected in the operator norm topology, see [46, Section 4]. We recall that an orthogonal operator \(A:X\to X\) is a bounded linear operator satisfying \(A^{*}A=AA^{*}=I\), where \(A^{*}:X\to X\) is the adjoint (i.e., the transpose) of the operator \(A:X\to X\). Observe that in a finite dimensional space \(\mathbb{R}^{n}\) the set \(O_{\mathbb{R}}(\mathbb{R}^{n})\) has two disjoint topological components, those which determinant is \(1\) and those which determinant is \(-1\), and thus the set \(O_{\mathbb{R}}(\mathbb{R}^{n})\) is not connected.

A bounded linear operator \(B:X\to X\) is called a rotation operator of the form \(B=e^{S}\) where \(S:X\to X\) is a linear, bounded, skew-symmetric operator, \(S^{*}=-S\), and \(e^{S}=I+S+\frac{1}{2!}S^{2}+\frac{1}{3!}S^{3}+\dots\). An orthogonal operator \(A\) is called a reflection if it is not a rotation.

One of the fundamental reasons for the surprising property that the space \(O_{\mathbb{R}}(X)\) of orthogonal operators in \(X\) is path-connected is that in an infinite dimensional real Hilbert space \(X\) every orthogonal operator \(A:X\to X\) can be represented as a product of two rotation operators (that is no valid in the finite dimensional spaces), that is, \(A=e^{S_{1}}e^{S_{2}}\) where \(S_{1},S_{2}:X\to X\) are skew-symmetric linear operators. Thus any operator \(A\in O_{\mathbb{R}}(X)\) can be connected to the identity operator \(Id:X\to X\) via a path \(\alpha:t\to e^{tS_{1}}e^{tS_{2}}\in O_{\mathbb{R}}(X)\), \(t\in[0,1]\), so that \(\alpha(0)=Id\) and \(\alpha(1)=A\).

As a motivating example on the fact that \(O_{\mathbb{R}}(X)\) is connected in the operator norm topology, is to consider a similar, but easier result in the strong operator topology, that is, topology generated by the evaluation maps \(A\to A(u)\), \(u\in L^{2}(0,1)\). Observe that the strong operator topology is weaker than the operator norm topology, but in finite dimensional space those are equivalent.. So, let us next show that the operators \(Id\) and \(-Id\) are in the same topological component of \(O_{\mathbb{R}}(X)\) in the strong operator topology. As all infinite dimensional separable Hilbert spaces are isometric to \(L^{2}(0,1)\), let us consider the Hilbert space \(L^{2}(\mathbb{R})\) and the orthogonal linear operators \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\), \(s\in[0,1]\) that are given by

\[A_{s}u(t)=\text{sign}(t-s)\cdot u(t), \tag{37}\]

where \(u\in L^{2}(0,1)\) and \(\text{sign}(t)\) is the sign of the real number \(t\). Then, for every \(u\in L^{2}(0,1)\) the functions

\[a_{u}:s\to A_{s}(u),\]

are continuous functions \(a_{u}:[0,1]\to L^{2}(0,1)\). As \(A_{0}=Id\) and \(A_{0}=-Id\), we see that

\[s\to A_{s}\in O_{\mathbb{R}}(L^{2}(0,1)),\quad s\in[0,1],\]

is a continuous path in the strong operator topology, that connects the operator \(Id\) to \(-Id\).

We recall that the continuity of the function \(s\to A_{s}\) in the strong operator topology, means that for all \(u\in L^{2}(0,1)\) the map

\[s\to A_{s}u\in L^{2}(0,1),\]is continuous. As for \(s>s_{0}\)

\[\|A_{s}u-A_{s_{0}}u\|^{2}_{L^{2}(0,1)}=\int_{s_{0}}^{s}|2u(x)|^{2}dx\to 0,\]

as \(s\to s_{0}\), we see that \(s\to A_{s}\) is continuous in the strong operator topology. All maps \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) are invertible and \(A_{0}=Id\), \(A_{1}=-Id\). This has implications e.g. for Finite Element method analysis of partial differential equations (See Appendix A.5.2).

Note that when \(X\) is any infinite dimensional separable Hilbert space with real scalars, there is an isometry \(J:X\to L^{2}(0,1)\) (e.g., the linear operator mapping an orthogonal basis of \(X\) to an orthogonal basis \(L^{2}(0,1)\). Then, the maps \(\tilde{A}_{s}:X\to X\) given by \(\tilde{A}_{s}=J^{-1}\circ A_{s}\circ J:X\to X\), where \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) are given above, are continuous paths in \(O_{\mathbb{R}}(X)\) from \(\tilde{A}_{0}=Id:X\to X\) to \(\tilde{A}_{1}=-Id:X\to X\). As discussed above, the deep result that \(Id\) and \(-Id\) can be connected by a continuous path in the operator norm topology of \(O_{\mathbb{R}}(X)\) is given in [32, Theorem 2], see also [45].

Let us first warm up by proving the claim in the case when an additional assumption (B) is valid:

(B) : When \(V\subset X\) is finite dimensional discretization maps \(F_{V}\) of linear invertible maps \(F\colon X\to X\) are linear invertible maps and moreover, the set \(S_{0}(X)=S(X)\) consists of all linear subspaces of \(X\).

Under assumptions (A) and (B), we consider the case when \(A_{1}=-Id\) and denote by \(F_{t}=A_{t}:X\to X\) a family of linear maps such that \(A_{0}=Id\), and that \(t\to A_{t}\in GL(X)\) is continuous path of operators connecting \(Id\) to \(A_{1}=-Id\). Let \((X,S(X),(F_{t,V})_{V\in S(X)})=\mathcal{A}(X,F_{t})\), that is, \(F_{t,V}:V\to V\) be the linear isomorphism that is the discretizations of the map \(F_{t}:X\to X\). As the functor \(\mathcal{A}\) is continuous, the map \(t\to F_{t,V}\) is a continuous map from \([0,1]\) to the space of linear operators endowed with the topology of uniform convergence on compact sets, c.f. limit (7). As \(F_{t,W}\) are linear, this implies that for all \(t^{\prime}\in[0,1]\),

\[\lim_{t\to t^{\prime}}\|F_{t,W}-F_{t^{\prime},W}\|=\lim_{t\to t^{\prime}}\sup_ {x\in V\cap\overline{B}_{X}(0,1)}\|F_{t,W}(x)-F_{t^{\prime},W}(x)\|_{X}=0. \tag{38}\]

and hence the map \(t\to F_{t,W}\) is a continuous map \([0,1]\to GL(W)\).

Let \(0<\epsilon_{0}<1/2\) and \(r>1\). Let \(t_{0}=0\) and \(t_{1}=1\). Using Property (A) for operators \(F_{t_{j}}\), \(j=0,1\), we see that there are \(W_{0},W_{1}\in S_{0}(X)\) such that for all \(V\in S_{0}(X)\) satisfying \(V\supset W_{j}\) we have

\[\sup_{x\in\overline{B}_{X}(0,r)\cap V}\|F_{t_{j},W_{j}}(x)-F_{t_{j}}(x)\|_{X}< \epsilon_{0},\quad\text{for }j\in\{0,1\}.\]

Let \(W\in S_{0}(X)\) be such a finite dimensional space which dimension is an odd integer and that \(W_{0}+W_{1}\subset W\). Then,

\[\sup_{x\in\overline{B}_{X}(0,r)\cap W}\|F_{t_{j},W}(x)-F_{t_{j}}(x)\|_{X}\leq \epsilon_{0},\quad\text{for }j\in\{0,1\}, \tag{39}\]

Clearly, \(A_{0}=Id\) satisfies \(A_{0}(W)=W\) and \(A_{0}:W\to W\) is an invertible linear map. Similarly, the map \(A_{1}=-Id\) satisfies \(A_{1}(W)=W\) and \(A_{1}:W\to W\) is an invertible linear map. These observation and assumptions (A) and (B) imply that \(F_{0,W}:W\to W\) and \(F_{1,W}:W\to W\) are linear maps and by inequality (39),

\[\|F_{0,W}-A_{0}|_{W}\|_{W\to W}<\epsilon_{0}<\frac{1}{2},\quad\|F_{1,W}-A_{1}| _{W}\|_{W\to W}<\epsilon_{0}<\frac{1}{2}, \tag{40}\]

and as the dimension of \(W\) is odd, we have

\[\text{det}(A_{0}|_{W})=\text{det}(Id|_{W})=1,\quad\text{det}(A_{1}|_{W})=\text {det}(-Id|_{W})=-1.\]

Let \(B_{0,s}=(1-s)A_{0}|_{W}+sF_{0,W}:W\to W\) and \(B_{1,s}=(1-s)A_{1}|_{W}+sF_{0,W}:W\to W\). As \(\|(A_{0}|_{W})^{-1}\|_{W\to W}=\|(A_{1}|_{W})^{-1}\|_{W\to W}=1\), we see using (40) that the maps \(B_{0,s}^{-1}:W\to W\) and \(B_{1,s}^{-1}:W\to W\) are invertible and that the functions \(s\to B_{0,s}\in GL(W)\) and \(s\to B_{1,s}\in GL(W)\) are continuous matrix-valued functions which determinants does not vanish and

\[\text{det}(B_{0,s})=\text{det}(A_{0}|_{W})>0,\quad\text{det}(B_{1,s})=\text{ det}(A_{1}|_{W})<0,\]for all \(s\in[0,1]\). These imply that

\[\text{det}(F_{0,W})=\text{det}(B_{0,1})>0,\quad\text{det}(F_{1,W})= \text{det}(B_{0,1})<0, \tag{41}\]

However, as \(t\to F_{t,W}\) is a continuous maps \([0,1]\to GL(W)\), see (38), we have that

\[t\to\text{det}(F_{t,W}), \tag{42}\]

is a continuous function \([0,1]\to\mathbb{R}\) which does not obtain value zero and thus has a constant sign. However, this is in contradiction with formula (41).

Next we return to the main part of the proof where we do not assume that assumption (B) is valid, but we only assume that the assumption (A) is valid. In this case, we use degree theory instead of the determinants.

Next, let \(F_{t}=A_{t}:X\to X\) be a family of linear maps such that \(A_{0}=Id\), but that \(A_{1}(x)=B_{e}(x)=x-2e\langle x,e\rangle_{X}\) is a reflection, where \(e\in X\) is a unit vector, and \(t\to A_{t}\in GL(X)\) is continuous. Again, let \((X,S_{0}(X),(F_{t,V})_{V\in S_{0}(X)})=\mathcal{A}(X,F_{t})\), that is, \(F_{t,V}:V\to V\) be \(C^{1}\)-diffeomorphisms that are discretizations of the map \(F\colon X\to X\). As the functor \(\mathcal{A}\) is continuous, the map \(t\to F_{t,V}|_{\overline{B}(0,R)}\in C(\overline{B}(0,R))\) is a continuous map for all \(V\in S_{0}(X)\) and \(R>0\). Moreover, we note that by our assumptions on \(\mathcal{B}\), the discretized map \(F_{t,V}:V\to V\), is \(C^{1}\)-diffeomorphism of \(V\).

As \(V\) is a finite dimensional vector space, we can use finite dimensional degree theory and consider the degree \(\text{deg}(F_{t,V},V\cap\overline{B}_{X}(0,r),p)\), that is the degree of the map \(F_{t,V}:V\cap\overline{B}_{X}(0,r)\to V\) at the point \(p\). We recall that when \(\Omega\subset\mathbb{R}^{d}\) is open and bounded, \(F\colon\overline{\Omega}\to\mathbb{R}^{d}\) is a \(C^{1}\)-smooth function and \(p\in\mathbb{R}^{d}\) are such that \(p\not\in f(\partial\Omega)\) and for all \(x\in f^{-1}(p)\) the derivative \(Df(x)\) is an invertible matrix, the degree is defined to be

\[\text{deg}(f,\Omega,p)=\sum_{x\in f^{-1}(p)}\text{sgn}(\text{det} (Df(x))),\]

where \(\text{sgn}(r)\) is sign of a real number \(r\). Also, the map \(f\to\text{deg}(h,\Omega,p)\) is defined for a continuous function \(h:\overline{\Omega}\to\mathbb{R}^{d}\) by approximating \(h\) by \(C^{1}\)-smooth function, see [43], Definition 1.2.5 on details. Let us denote \(\overline{B}_{W}(0,r)=W\cap\overline{B}_{X}(0,r)\).

Let \(r>0\). Recall that by assumption (A), for \(F_{0}=Id:X\to X\) and \(F_{1}=B_{e}:X\to X\) there are finite dimensional spaces \(V_{0}\in S_{0}(X)\) and \(V_{1}\in S_{0}(X)\) such that \(e\in V_{0}\) and \(e\in V_{1}\) and that when \(V\in S_{0}(X)\) satisfies \(V_{0}\subset V\) and \(V_{1}\subset V\), then

\[\sup_{x\in\overline{B}_{X}(0,r)\cap V}\|F_{0,V}(x)-F_{0}(x)\|_{X} <r/4, \tag{43}\]

and

\[\sup_{x\in\overline{B}_{X}(0,r)\cap V}\|F_{1,V}(x)-F_{1}(x)\|_{X} <r/4. \tag{44}\]

Moreover, let \(W\in S_{0}(X)\) be such a finite dimensional linear space that \(V_{0}\subset W\) and \(V_{1}\subset W\). Observe that as \(e\in W\), we can decompose \(W\) as

\[W=\text{span}(e)\oplus\{x\in W:\ \langle x,e\rangle_{X}=0\}. \tag{45}\]

and denote \(W^{\prime}=\{x\in W:\ \langle x,e\rangle_{X}=0\}\). We see that

\[A_{1} =B_{e}:\text{span}(e)\to\text{span}(e),\ B_{e}|_{\text{span}(e)} =-Id, \tag{46}\] \[A_{1} =B_{e}:W^{\prime}\to W^{\prime},\ B_{e}|_{W^{\prime}}=Id.\]

Next we use the facts that \(F_{0}(0)=Id(0)=0\) and \(F_{1}(0)=B_{e}(0)=0\). Let us define the maps

\[f_{0}=F_{0}|_{\overline{B}_{W}(0,r)},\quad f_{1}=F_{1}|_{\overline{B}_{W}(0,r )},\quad f_{0,W}=F_{0,W}|_{\overline{B}_{W}(0,r)},\quad f_{1,W}=F_{1,W}|_{ \overline{B}_{W}(0,r)}.\]

that are \(C^{1}\)-smooth maps \(\overline{B}_{W}(0,r)\to W\). Moreover, for \(j=0,1\), we have \(f_{0}(\partial B_{W}(0,r))=\partial B_{W}(0,r)\) and \(f_{1}(\partial B_{W}(0,r))=\partial B_{W}(0,r)\). Let \(p_{j}:=f_{j,W}(0)\). Then, by (43) and (44) we have

\[\|p_{j}\|_{W}=\|f_{j,W}(0)\|_{W}=\|f_{j,W}(0)-f_{j}(0)\|_{W}<\frac{1} {4}r, \tag{47}\]\[\text{dist}(f_{j,W}(x),p_{j})\geq\frac{1}{2}r,\quad\text{for all }x\in\partial B_{W}(0,r).\]

This implies that

\[\sup_{x\in\overline{B}_{W}(0,r)}\|f_{j,W}(x)-f_{j}(x)\|<\frac{1}{4}r<\frac{1}{2 }r\leq\text{dist}(f_{j,W}(\partial B_{W}(0,r)),p_{j}). \tag{48}\]

Then, by [43], Definition 1.2.5, the formula (48) implies that

\[\text{deg}(f_{j,W},\overline{B}_{W}(0,r),p_{j})=\text{deg}(f_{j},\overline{B}_{ W}(0,r),p_{j}).\]

Moreover, as \(F_{1}=A_{1}=B_{e}\) satisfies (46), and \(p_{j}\in\overline{B}_{W}(0,r)\), we have

\[\text{deg}(f_{0},\overline{B}_{W}(0,r),p_{0})=\text{deg}(Id,\overline{B}_{W}( 0,r),p_{0})=\text{sgn}(\text{det}(Id))=1,\]

and

\[\text{deg}(f_{1},\overline{B}_{W}(0,r),p_{1})=\text{deg}(A_{1},\overline{B}_{ W}(0,r),p_{1})=\text{sgn}(\text{det}(B_{e}|_{W}))=-1.\]

Moreover, by our assumptions on \(\mathcal{B}\), the discretized maps \(F_{j,W}:W\to W\), \(j=0,1\) are \(C^{1}\)-diffeomorphism. Hence, the inverse image \(F_{j,W}^{-1}(\{p_{j}\})\) is a set containing only one point that coincides with \(f_{j,W}^{-1}(\{p_{j}\})\) (that in fact is the set \(\{0\}\)). This and the above show that for any \(R>r\), we have

\[\text{deg}(F_{j,W},\overline{B}_{W}(0,R),p_{j}) = \text{deg}(F_{j,W},\overline{B}_{W}(0,r),p_{j}) \tag{49}\] \[= \text{deg}(f_{j,W},\overline{B}_{W}(0,r),p_{j})\] \[= \text{deg}(f_{j},\overline{B}_{W}(0,r),p_{j})\] \[= (-1)^{j}.\]

Let us now consider the maps \(F_{t,W}:W\to W\) where \(t\in[0,1]\), that are \(C^{1}\)-diffeomorphism \(f_{t,W}:W\to W\). As \(t\to F_{t}=A_{t}\) is a continuous map \([0,1]\to GL(X)\) and \(\mathcal{A}:\mathcal{D}\to\mathcal{B}\) is a continuous functor, the map \(t\to F_{t,W}\) is a continuous map \([0,1]\to C(\overline{B}_{W}(0,r);W)\).

Let us consider \(\hat{t}\in[0,1]\) and denote \(p_{t}=F_{t,W}(0)\). As \(F_{\hat{t},W}:W\to W\) is a \(C^{1}\)-diffeomorphism, the set \(F_{\hat{t},W}(B_{W}(0,r))\) is open and hence there is \(\epsilon>0\) such that

\[B_{W}(p_{\hat{t}},5\epsilon)\subset F_{\hat{t},W}(B_{W}(0,r)). \tag{51}\]

This implies that

\[\text{dist}_{W}(p_{\hat{t}},F_{\hat{t},W}(\partial B_{W}(0,r)))>5\epsilon. \tag{52}\]

As \(t\to F_{t,W}\) is a continuous map \([0,1]\to C(\overline{B}_{W}(0,r);W)\), there is \(\delta>0\) such that when \(|t-\hat{t}|<\delta\) then

\[\sup_{x\in V\cap\overline{B}_{X}(0,r)}\|F_{t,V}(x)-F_{\hat{t},V}(x)\|_{X}<\epsilon. \tag{53}\]

In particular, this implies that

\[\|F_{t,V}(0)-F_{\hat{t},V}(0)\|_{X}<\epsilon, \tag{54}\]

and thus \(p_{t}=F_{t,V}(0)\) and \(p_{\hat{t}}=F_{\hat{t},V}(0)\) are in the ball \(B_{W}(p_{\hat{t}},5\epsilon)\) and hence by (51) these points are in the same topological component of by \(W\setminus F_{\hat{t},W}(\partial B_{W}(0,r))\). Hence, it follows from [43], Theorem 1.2.6 (5), that for \(|t-\hat{t}|<\delta\), we have

\[\text{deg}(F_{\hat{t},W},\overline{B}_{W}(0,r),p_{t})=\text{deg}(F_{\hat{t},W},\overline{B}_{W}(0,r),p_{\hat{t}}). \tag{55}\]

Next, we observe that by (53), for any \(t_{1},t_{2}\in[0,1]\) satisfying \(|t_{1}-\hat{t}|<\delta\) and \(|t_{2}-\hat{t}|<\delta\),

\[\sup_{x\in V\cap\overline{B}_{X}(0,r)}\|F_{t_{1},V}(x)-F_{t_{2},V}(x)\|_{X}<2\epsilon. \tag{56}\]Inequalities (52), (54) and (56) imply that for any \(t_{1},t_{2}\in[0,1]\) satisfying \(|t_{1}-\hat{t}|<\delta\) and \(|t_{2}-\hat{t}|<\delta\) we have

\[\mbox{dist}_{W}(p_{t_{1}},F_{t_{2},W}(\partial B_{W}(0,r))) = \inf_{y\in\partial B_{W}(0,r))}\mbox{dist}_{W}(p_{t_{1}},F_{t_{2}, W}(y))\] \[> \inf_{y\in\partial B_{W}(0,r)}\mbox{dist}_{W}(p_{t_{1}},F_{\hat{t},W}(y))-\epsilon\] \[> \inf_{y\in\partial B_{W}(0,r)}\mbox{dist}_{W}(p_{\hat{t}},F_{\hat{t },W}(y))-2\epsilon-\epsilon\] \[= \mbox{dist}_{W}(p_{\hat{t}},F_{\hat{t},W}(\partial B_{W}(0,r)))-3 \epsilon-\epsilon>5\epsilon-3\epsilon=2\epsilon.\]

As \(t\to F_{t,W}\) is a continuous map \([0,1]\to C(\overline{B}_{W}(0,r);W)\) and the formula (57) is valid, it follows from [43], Theorem 1.2.6 (3) that for \(t_{1}\) and \(t_{2}\) satisfying \(|t_{1}-\hat{t}|<\delta\) and \(|t_{2}-\hat{t}|<\delta\), we have

\[\mbox{deg}(F_{t_{1},W},\overline{B}_{W}(0,r),p_{t_{2}})=\mbox{deg}(F_{\hat{t}, W},\overline{B}_{W}(0,r),p_{t_{2}}). \tag{58}\]

This and (55) imply that

\[\mbox{deg}(F_{t_{1},W},\overline{B}_{W}(0,r),p_{t_{2}}) = \mbox{deg}(F_{\hat{t},W},\overline{B}_{W}(0,r),p_{t_{2}})\] \[= \mbox{deg}(F_{\hat{t},W},\overline{B}_{W}(0,r),p_{\hat{t}}),\]

In particular, when \(t_{1}=t_{2}\) satisfy \(|t_{1}-\hat{t}|<\delta\), this implies

\[\mbox{deg}(F_{t_{1},W},\overline{B}_{W}(0,r),p_{t_{1}}) = \mbox{deg}(F_{\hat{t},W},\overline{B}_{W}(0,r),p_{\hat{t}}). \tag{60}\]

As \(\hat{t}\in[0,1]\) is above arbitrary, this implies that the function

\[g:t\rightarrow\mbox{deg}(F_{t,W},\overline{B}_{W}(0,r),F_{t,W}(0)), \tag{61}\]

is a continuous integer valued function on \([0,1]\) and thus it is constant on the interval \(t\in[0,1]\). However, by (49), \(g(0)=1\) is not equal to \(g(1)=-1\) and hence \(g(t)\) can not be constant function on the interval \(t\in[0,1]\). This contradiction shows that the required functor does not exists. 

### Proofs from Sec. 3.2

#### a.5.1 Proof of Lemma 1

Proof.: Let \(F:X\to X\) be strongly monotone and \(C^{1}\)-diffeomorphism. It is obvious that strongly monotonicity implies that strictly monotonicity.

We first show that the strongly monotonicity implies that the coercivity. Indeed, by the strongly monotonicity we have

\[\langle F(x)-F(0),x-0\rangle_{X}\geq\alpha\|x\|_{X}^{2},\]

which implies that, as \(\|x\|_{X}\rightarrow\infty\),

\[\langle F(x),\frac{x}{\|x\|}\rangle_{X}\geq\langle F(0),\frac{x}{\|x\|} \rangle_{X}+\alpha\|x\|_{X}\rightarrow\infty.\]

Therefore, \(F:X\to X\) is coercive.

Let us consider the operator

\[F_{V}:=P_{V}F|_{V}:V\to V.\]

From the definitions, \(F_{V}:V\to V\) is \(C^{1}\) and strongly monotones, and the (Frechet) derivative \(DF_{V}|_{v}\) at \(v\in V\) is given by

\[DF_{V}|_{v}=P_{V}(DF|_{v})|_{V},\]

which is linear and continuous operator from \(V\) to \(V\). Since \(F_{V}:V\to V\) is \(C^{1}\) and strongly monotone, it is hemicontinuous, strictly monotones, and coercive. By the Minty-Browder theorem [10, Theorem 9.14-1], \(F_{V}:V\to V\) is bijective, and then its inverse \(F_{V}^{-1}:V\to V\) exists.

Next, let \(v\in V\). As \(F_{V}:V\to V\) is strongly monotones, we estimate that for all \(h\in V\),

\[\langle\frac{F_{V}(v+\epsilon h)-F(x)}{\epsilon},\frac{(x+\epsilon h )-x}{\epsilon}\rangle_{X}=\frac{1}{\epsilon^{2}}\langle F(x+\epsilon h)-F(x), (x+\epsilon h)-x\rangle_{X}\] \[\geq\frac{\alpha}{\epsilon^{2}}\|(x+\epsilon h)-x\|_{X}^{2}= \alpha\|h\|_{X}^{2}.\]Taking \(\epsilon\to+0\), we have that

\[\langle DF_{V}|_{v}(h),h\rangle_{X}\geq\|h\|_{X}^{2}.\]

Therefore, \(DF_{V}|_{v}:V\to V\) is injective for all \(v\in V\). Then, it is bijective because \(V\) is now finite dimensional. By the inverse function theorem, the inverse \(F_{V}^{-1}:V\to V\) is \(C^{1}\). 

#### a.5.2 Additional examples

Let us consider the elliptic (but not stongly elliptic) problem

\[P_{s}u:=-\frac{d}{dt}\bigg{(}\text{sign}(t-s)\frac{d}{dt}u(t)\bigg{)}=f(t), \quad t\in[0,1], \tag{62}\]

with the boundary conditions

\[u(0)=0,\quad\frac{d}{dt}u(t)\bigg{|}_{t=1}=0. \tag{63}\]

The FEM (Finite Element Method) matrix corresponding to this problem is \([p_{jk}(s)]_{j,k=1}^{n}\), where

\[p_{jk}(s)=\int_{0}^{1}\text{sign}(t-s)\frac{d}{dt}\chi_{j}(t)\cdot\frac{d}{dt} \chi_{k}(t)dt,\quad j,k=1,\dots,m\]

and \(\chi_{j}(t)\), \(j=1,2,\dots\) are a complete basis of the Hilbert space \(H^{1}_{D,N}(0,1):=\{v\in H^{1}(0,1):v(0)=0\}\) that correspond to the mixed Dirichlet and Neumann boundary conditions. Note that in the FEM approximation we use only a finite subset of the complete basis of the Hilbert space. Note for \(u\) in the canonical domain of the above problem that makes the operator appearing in the above equation selfadjoint (the Friedrichs extension), that is, for

\[u\in\bigg{\{}v\in H^{1}(0,1):\ \text{sign}(t-s)\frac{d}{dt}v(t)\in H^{1}(0,1), \ v(0)=0,\ \frac{d}{dt}v(t)|_{t=1}=0\bigg{\}},\]

we have

\[P_{s}u=-D_{t}^{(2)}A_{s}D_{t}^{(1)}u,\]

where \(A_{s}\) is the multiplication with the function \(\text{sign}(t-s)\), see (37), and \(D_{t}^{(1)}\) and \(D_{t}^{(2)}\) are the derivative operators \(D_{t}^{(j)}u=\frac{d}{dt}u\) having the different domains

\[\mathcal{D}(D_{t}^{(1)}) =H^{1}_{D,N}(0,1):=\{v\in H^{1}(0,1):\ v(0)=0\}, \tag{64}\] \[\mathcal{D}(D_{t}^{(2)}) =H^{1}_{N,D}(0,1):=\{v\in H^{1}(0,1):\ v(1)=0\}. \tag{65}\]

Observe that \(D_{t}^{(1)}\) and \(D_{t}^{(2)}\) have the inverse operators

\[(D_{t}^{(1)})^{-1}v(t)=\int_{0}^{t}v(t^{\prime})dt^{\prime}, \tag{66}\]

\[(D_{t}^{(2)})^{-1}v(t)=-\int_{t}^{1}v(t^{\prime})dt^{\prime}, \tag{67}\]

that map \((D_{t}^{(j)})^{-1}:L^{2}(0,1)\to\mathcal{D}(D_{t}^{(j)})\). The eigenvalues of the matrix \(s\to[p_{jk}(s)]_{j,k=1}^{n}\) change from positive values to negative values when \(s\) moves from \(0\) to \(1\). Thus, for some value \(s\in(0,1)\) the matrix \([p_{jk}(s)]_{j,k=1}^{n}\) has a zero eigenvalue and is no invertible even though all maps \(D_{t}^{(1)}\), \(D_{t}^{(2)}\), and \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) are invertible. In particular, there are no finite FEM basis in where the Galerkin discretizations of all operators \(P_{s}\), \(s\in[0,1]\) are invertible.

Let us consider even simpler example: Similarly to the above, if we consider the Galerkin discretizations of the operator \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\), we see that the Galerkin matrix corresponding to \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) is \([a_{jk}(s)]_{j,k=1}^{n}\), where

\[a_{jk}(s)=\int_{0}^{1}\text{sign}(t-s)\psi_{j}(t)\cdot\psi_{k}(t)dt,\quad j,k =1,\dots,m,\]

and \(\psi_{j}(t)\), \(j=1,2,\dots\) are a complete basis of the Hilbert space \(L^{2}(0,1)\). Again, we see that the eigenvalues of the matrix \(s\to[a_{jk}(s)]_{j,k=1}^{n}\) change from positive values to negative values when \(s\) moves from \(0\) to \(1\). Thus, for some value \(s\in(0,1)\) the matrix \([a_{jk}(s)]_{j,k=1}^{n}\) has a zero eigenvalue and is no invertible even though all maps \(A_{s}:L^{2}(0,1)\to L^{2}(0,1)\) are invertible. Thus there are no finite basis in where the Galerkin discretizations of all operators \(A_{s}\), \(s\in[0,1]\) are invertible.

### Proofs from Sec. 3.4

#### a.6.1 Proof of Lemma 4

Proof.: From the assumption, it holds that

\[\|DF|_{x}-I\|_{X\to X}\leq\|T_{1}\|_{X\to X}\|DG|_{x}\|_{X\to X}\|T_{2}\|_{X\to X} \leq\frac{1}{2}.\]

Then, by the mean value theorem, there is \(0<t<1\) such that

\[\langle F(x_{1})-F(x_{2}),x_{1}-x_{2}\rangle_{X} = \frac{\partial}{\partial t}\langle F(x_{1}+t(x_{2}-x_{1}))-F(x_{2 }),x_{1}-x_{2}\rangle_{X}\] \[= \frac{\partial}{\partial t}\langle DF\bigg{|}_{x_{1}+t(x_{2}-x_{1 })}(x_{2}-x_{1}),x_{1}-x_{2}\rangle_{X}\geq\frac{1}{2}\|x_{1}-x_{2}\|_{X}^{2}.\]

These imply that \(F\colon X\to X\) is strongly monotone. 

#### a.6.2 Proof of Lemma 3

Proof.: Observe that strongly monotone neural operators are coercive, that is,

\[\lim_{\|u\|_{X}\to\infty}\langle F(u),\frac{u}{\|u\|_{X}}\rangle_{X}=\lim_{\| u\|_{X}\to\infty}\frac{1}{\|u\|_{X}}\langle F(u)-F(0),u-0\rangle_{X}=\infty. \tag{68}\]

and therefore \(F\colon X\to X\) is surjective by Browder-Minty theorem [28, Thm. 2.1], see also [14] considerations for neural operators. Moreover, the derivatives \(DF|_{x}\) are linear strongly monotone operators for all \(x\in X\), and therefore injective. Observe that the derivative is a linear operator of the form

\[DF|_{x}=I+T_{2}\circ DG|_{T_{1}x}\circ T_{1}:X\to X,\]

and as \(T_{1}\) and \(T_{2}\) are compact and \(DG|_{Tx}\) is bounded we see that \(DF|_{x}\) is a Fredholm operator of index zero. Observe that

\[\langle DF|_{x}v,v\rangle_{X}=\frac{1}{h}\lim_{h\to 0}\langle\frac{F(x+hv)-F(x) }{h},(x+hv)-x\rangle_{X}\geq\alpha\|v\|_{X}^{2},\]

where \(h\in\mathbb{R}\) and \(v\in X\), and hence \(DF|_{x}:X\to X\) is an injective linear operator. As \(DF|_{x}\) is a Fredholm operator of index zero, this implies that the derivative \(DF|_{x}:X\to X\) is a bijection. As the Hilbert space \(X\) can be identified with it dual space and the operator \(F\colon X\to X\) is continuous and hence hemi-continuous, it follows from [28], Theorem 3.1, that the map \(F\colon X\to X\) is a homeomorphism. As it is \(C^{1}\)-smooth and its derivative is bijective operator \(DF|_{x}:X\to X\) at all \(x\in X\), it follows that \(F\colon X\to X\) is a \(C^{1}\)-smooth diffeomorphism. 

#### a.6.3 Proof of Theorem 3

Proof.: To show the well-definedness of the discretization functor \(\mathcal{A}_{\mathrm{lin}}\), we need to show that, for each strongly monotone \(C^{1}\)-function \(F\colon X\to X\) that is of the form (4), \(F_{V}=P_{V}F|_{V}:V\to V\) is still strongly monotone \(C^{1}\)-smooth of the form (4). This is given by Lemma 1. Moreover, such functions are \(C^{1}\)-smooth diffeomorphisms.

To verify assumption (A), let \(r,\epsilon>0\), and let \(F\colon X\to X\) be a strongly monotone diffeomorphisms that is of the form \(F=Id+T_{2}\circ G\circ T_{1}:X\to X\) where \(T_{1}\colon X\to X\) and \(T_{2}\colon X\to X\) are compact linear operators, and \(G\colon X\to X\) is such that \(G\in C^{1}(X)\). Since \(T_{2}\circ G\circ T_{1}\) is a compact mapping, there is a finite-dimensional subspace \(V\subset X\), depending on \(\epsilon>0\) such that

\[\sup_{x\in\overline{B}_{X}(0,r)}\|(Id-P_{V})T_{2}G(T_{1}x)\|_{X}\leq\epsilon,\]

which implies that

\[\sup_{x\in\overline{B}_{X}(0,r)\cap V}\|F_{V}(x)-F(x)\|_{X}=\sup_{x\in \overline{B}_{X}(0,r)\cap V}\|(Id-P_{V})T_{2}G(T_{1}x)\|_{X}\leq\epsilon.\]

To prove the continuity in the sense of Definition 8 let \(r>0\) and let \(V\in S_{0}(X)\). Assume that

\[\lim_{j\to\infty}\sup_{x\in\overline{B}_{X}(0,r)}\|F^{(j)}(x)-F(x)\|_{X}=0,\]where \(F^{(j)}\) and \(F\) are strongly monotone diffeomorphisms \(F\colon X\to X\) that are of the form (4). Then, we see that, as \(j\to\infty\),

\[\sup_{x\in V\cap\overline{B}_{X}(0,r)}\|F^{(j)}_{V}(x)-F_{V}(x)\|_{ X}\leq\sup_{x\in V\cap\overline{B}_{X}(0,r)}\|P_{V}F^{(j)}(x)-P_{V}F(x)\|_{X}\] \[\leq\sup_{x\in V\cap\overline{B}_{X}(0,r)}\|P_{V}\|_{\mathrm{op}} \|F^{(j)}(x)-F(x)\|_{X}\leq\sup_{x\in\overline{B}_{X}(0,r)}\|F^{(j)}(x)-F(x)\| _{X}\to 0.\]

### Proofs from Sec. 3.5

#### a.7.1 Proof of Theorem 4

In the proof of Theorem 4, we prove first few auxiliary lemmas

**Lemma 5**.: _A layer of a neural operator \(F\colon X\to X\) is surjective. In particular, if \(F\colon X\to X\) is bilipschitz, then \(F\colon X\to X\) is a homeomorphism._

Proof.: By formula (4) a layer of neural operator \(F\colon X\to X\) is of the form \(F(u)=u+T_{2}G(T_{1}u)\) where \(T_{1}:X\to X\) and \(T_{2}:X\to X\) are compact linear operators and \(G:X\to X\) is a function in \(C^{1}(X)\). Let \(c_{0},c_{1}>0\) be such that \(\|G\|_{L^{\infty}(X)}\leq c_{0}\) and \(\text{Lip}_{X\to X}(G)\leq c_{1}\).

Let \(p\in X\) and \(R_{0}>\|T_{2}\|_{X\to X}c_{0}+\|p\|_{X}\). Then \(K_{p}:X\to X\), defined by the formula

\[K_{p}(x)=-T_{2}\circ G\circ T_{1}(x)+p,\quad x\in X,\]

is a compact non-linear operator, see [43, Definition 2.1.11].

When \(p\) satisfies \(p\not\in(Id-K_{0})(\partial B(0,R_{0}))\), let \(\text{deg}(Id-K_{0},B(0,R_{0}),p)=\text{deg}(Id-K_{p},B(0,R_{0}),0)\) be the infinite dimensional (Leray-Schauder) degree of the operator \(Id-K_{0}:X\to X\) in the set \(B(0,R_{0})\) with respect to the point \(p\), see [43, Definition 2.2.3]. Let \(K_{p;t}:X\to X\) be the non-linear compact operators that depend on the parameter \(t\in[0,1]\), obtained by multiplying \(K_{p}(x)\) by the number \(t\), that is,

\[K_{p;t}(x)=tK_{p}(x),\quad x\in X. \tag{69}\]

As \(\|K_{p;t}(x)\|_{X}\leq\|T_{2}\|_{X\to X}c_{0}+\|p\|_{X}<R_{0}\) for all \(t\in[0,1]\) and \(x\in X\), we see that

\[K_{p;t}(x)\neq x,\quad\text{for }x\in\partial B_{X}(R_{0}). \tag{70}\]

Then by the homotopy invariance of the Leray-Schauder degree, see [43, Theorem 2.2.4(3)], the function

\[d(t)=\text{deg}(Id-K_{p;t},B(0,R_{0}),0),\quad t\in[0,1], \tag{71}\]

is a constant function. Moreover, by [43, Theorem 2.2.4(1)],

\[\text{deg}(Id-K_{p},B(0,R_{0}),0)=\text{deg}(I-K_{p;1},B(0,R_{0} ),0)=d(1)\] \[= d(0)=\text{deg}(I,B(0,R_{0}),0)=1.\]

By [43, Theorem 2.2.4(2)], this implies that the equation

\[x=K_{p}(x),\quad\text{or equivalently,}\quad x+T_{2}\circ G\circ T_{1}(x)=p,\]

has a solution \(x\in B_{X}(R_{0})\). As \(p\in X\) was above arbitrary, this implies that \(F=Id+T_{2}\circ G\circ T_{1}:X\to X\) is surjection.

Finally, if \(F\colon X\to X\) is bilipschitz, it is a bijection and its inverse function is Lipschitz function, and thus \(F\colon X\to X\) is a homeomorphism. 

The next lemma shows the existence of a (finite-dimensional) orthogonal subspace so that for each compact operator, projection onto the subspace is a perturbation of the identity under either pre or post composition.

**Lemma 6**.: _Let \(T_{1},T_{2}:X\to X\) be compact operators and \(h>0\). There is a finite dimensional space \(W\subset X\) such that for the orthogonal projector \(P_{W}:X\to X\) it holds that_

\[\|T_{1}(Id-P_{W})\|_{X\to X}<h, \tag{72}\] \[\|(Id-P_{W})T_{2}\|_{X\to X}<h,\] (73) \[\|(Id-P_{W})T_{1}(Id-P_{W})\|_{X\to X}<h,\] (74) \[\|(Id-P_{W})T_{2}(Id-P_{W})\|_{X\to X}<h. \tag{75}\]

Proof.: We use the singular value decomposition of compact operators: We can write

\[T_{\ell}x=\sum_{p=1}^{\infty}\omega_{\ell,p}(x,\psi_{\ell,p})\phi_{1,p},\]

where \(\psi_{\ell,p}\) and \(\phi_{\ell,p}\) are orthogonal families in \(X\) and \(\omega_{\ell,p}\geq 0\) satisfy \(\omega_{\ell,p+1}\leq\omega_{\ell,p}\) and \(\omega_{\ell,p}\to 0\) as \(p\to\infty\). For all \(h>0\) we can choose \(P>0\) such that \(\omega_{\ell,p}<h\) when \(p\geq P\). Then

\[\|T_{\ell}-\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle\phi_ {1,p}\|_{X\to X}<h.\]

If

\[V=V_{P}=\text{span}\{\psi_{\ell,p},\phi_{\ell,p};\;\ell=1,2,\;p\leq P\}, \tag{76}\]

we see that if \(W\subset X\) is a linear subspace such that \(V\subset W\) then

\[(\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle\phi_{1,p})\circ P _{W}=\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle\phi_{1,p},\]

\[P_{W}\circ(\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle\phi_{1,p})=\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle\phi_{1,p},\]

and

\[P_{W}\circ(\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle\phi_{ 1,p})\circ P_{W}=\sum_{p=1}^{P}\omega_{\ell,p}\langle\cdot,\psi_{\ell,p}\rangle \phi_{1,p},\]

and thus

\[\|T_{\ell}-T_{\ell}\circ P_{W}\|_{X\to X}<h,\quad\|T_{\ell}-P_{W}\circ T_{\ell }\|_{X\to X}<h,\]

and moreover,

\[\|T_{\ell}-P_{W}\circ T_{\ell}\circ P_{W}\|_{X\to X}<h.\]

Let

\[F^{W}=Id+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}\;:X\to X. \tag{77}\]

Observe that for \(w\in W\) and \(v\in W^{\perp}\) we have

\[F^{W}(w+v)=F^{W}(w)+v,\quad F^{W}(w)\in W, \tag{78}\]

and

\[F^{W}(W)\subset W, \tag{79}\] \[F^{W}:W^{\perp}\to W^{\perp},\quad F^{W}|_{W^{\perp}}=Id_{W^{ \perp}}. \tag{80}\]

This means that \(F^{W}=Id_{W^{\perp}}\oplus F^{W}|_{W}\), where \(F^{W}|_{W}:W\to W\) is a function which maps the finite dimensional vector space \(W\) to itself.

The lemma below shows that given an \(F\) and \(\epsilon>0\), we may perturb it by a Lipschitz term \(B\) so it becomes the operator \(F^{W}\).

**Lemma 7**.: _For any \(\epsilon>0\), there is a finite dimensional space \(W\subset X\) such that \(\text{Lip}_{X\to X}(F-F^{W})<\epsilon\) and for any ball \(B_{X}(R)\subset X\), \(R>0\), the maps \(F\colon B_{X}(R)\to X\) and \(F^{W}:B_{X}(R)\to X\) satisfy \(\|F-F^{W}\|_{L^{\infty}(B_{X}(R))}<\frac{1}{2}(1+R)\epsilon\)._

Proof.: Let us choose a finite dimensional space \(W\subset X\) so that Lemma 6 is valid with

\[h=\frac{1}{4(1+\|G\|_{C^{1}(X)}))(1+\|T_{1}\|_{X\to X})(1+\|T_{2}\|_{X\to X})}\epsilon.\]

The right hand side of (72) is chosen so that

\[\text{Lip}_{X\to X}(F-F^{W})=\text{Lip}(P_{W}\circ T_{2}\circ G \circ T_{1}\circ P_{W}(x)-T_{2}\circ G\circ T_{1})\] \[\leq \text{Lip}(P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-P_{W} \circ T_{2}\circ G\circ T_{1}(x))\] \[+\text{Lip}(P_{W}\circ T_{2}\circ G\circ T_{1}(x)-T_{2}\circ G \circ T_{1})\] \[\leq \|T_{2}\|_{X\to X}\text{Lip}(G|_{X\to X})\|T_{1}(I-P_{W})\|_{X\to X}+ \|(I-P_{W})T_{2}\|_{X\to X}\text{Lip}(G|_{X\to X})\|T_{1}\|_{X\to X}\] \[\leq \frac{1}{2}\varepsilon,\]

and

\[\|F-F^{W}\|_{L^{\infty}(B(0,R))}=\sup_{x\in B(0,R)}\|P_{W}\circ T_ {2}\circ G\circ T_{1}\circ P_{W}(x)-T_{2}\circ G\circ T_{1}(x)\|_{X}\] \[\leq \sup_{x\in B(0,R)}\|P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W} (x)-P_{W}\circ T_{2}\circ G\circ T_{1}(x)\|_{X}\] \[+\sup_{x\in B(0,R)}\|P_{W}\circ T_{2}\circ G\circ T_{1}(x)-T_{2} \circ G\circ T_{1}(x)\|_{X}\] \[\leq \|T_{2}\|_{X\to X}\text{Lip}(G|_{B(0,\|T_{1}\|_{X\to X}R)})\|T_{1 }(I-P_{W})\|_{X\to X}\cdot R\] \[+\|(I-P_{W})T_{2}\|_{X\to X}\|G\|_{L^{\infty}(B(0,\|T_{1}\|_{X\to X }R))}\] \[\leq \frac{1}{4}(R+1)\epsilon.\]

**Lemma 8**.: _For any \(\epsilon>0\), there are finite dimensional space \(W\subset X\) and (possibly non-linear) functions \(B:X\to X\) and \(\tilde{B}:X\to X\) such that_

\[Lip(B)<\epsilon,\quad Lip(\tilde{B})<\epsilon. \tag{81}\]

_and_

\[F^{W}=(Id+B)\circ F:X\to X, \tag{82}\] \[F=(Id+\tilde{B})\circ F^{W}:X\to X. \tag{83}\]

_Moreover, \(B:X\to X\) is a compact non-linear operator of the form_

\[B=P_{W}\circ F_{1}\circ P_{W}+T_{2}\circ F_{2}\circ P_{W}+P_{W} \circ F_{3}\circ T_{1}+T_{2}\circ F_{4}\circ T_{1}, \tag{84}\]

_where \(F_{1},F_{2},F_{3},F_{4}:X\to X\) are functions in \(C^{1}(X)\) and \(\tilde{B}:X\to X\) is of the same form. In addition, the operator \(Id+B:X\to X\) and \(Id+\tilde{B}:X\to X\) are layers of neural operators._

Proof.: Below, let \(C_{0}>0\) be such that

\[\|F(x)-F(y)\|_{X}\geq C_{0}\|x-y\|_{X},\quad x,y\in X, \tag{85}\]

and let \(W\) be the space in Lemma 6 with

\[h=\frac{1}{4(1+\|G\|_{C^{1}(X)}))(1+\|T_{1}\|_{X\to X})(1+\|T_{2}\|_{X\to X})}\epsilon.\]

By Lemma 5, \(F(x)=x+T_{2}G(T_{1}(x))\) is by an invertible function \(F:X\to X\). Thus we can define a non-linear operator

\[F^{W}\circ(F)^{-1} \tag{86}\] \[= (Id+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})\circ(Id+T_{2} \circ G\circ T_{1})^{-1}\] \[= Id+(P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G \circ T_{1})\circ(Id+T_{2}\circ G\circ T_{1})^{-1}.\]Let

\[R = P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G\circ T_{1} \tag{87}\] \[= \bigg{(}P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G \circ T_{1}\circ P_{W}\bigg{)}\] \[+\bigg{(}T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G\circ T_{ 1}\bigg{)}.\]

Then for all \(x,y\in X\)

\[\|R(x)-R(y)\|_{X}\] \[\leq \|(Id-P_{W})T_{2}\|_{X\to X}\|G\|_{Lip(X,X)}\|T_{1}\|_{X\to X}\|x-y\|_{X}\] \[+\|T_{2}\|_{X\to X}\|G\|_{Lip(X,X)}\|T_{1}(Id-P_{W})\|_{X\to X}\|x-y\|_ {X}\] \[\leq \frac{1}{2C_{0}}\epsilon\|x-y\|_{X}.\]

and thus,

\[\|R\circ(Id+T_{2}\circ G\circ T_{1})^{-1}(x)-R\circ(Id+T_{2}\circ G \circ T_{1})^{-1}(y)\|_{X}\] \[\leq \frac{1}{2}\epsilon\|x-y\|_{X}.\]

This implies that \(F^{W}:X\to X\) satisfies for all \(x,y\in X\)

\[\|(F^{W}\circ(F)^{-1}-Id)(x)-(F^{W}\circ(F)^{-1}-Id)(y)\|_{X}\leq \frac{1}{2}\epsilon\|x-y\|_{X},\]

and thus

\[\|(F^{W}\circ(F)^{-1}-(F^{W}\circ(F)^{-1}(y)\|_{X}\leq(1+\frac{1}{2} \epsilon)\|x-y\|_{X}, \tag{90}\]

and

\[\mbox{Lip}(F^{W}\colon X\to X)\leq(1+\frac{1}{2}\epsilon)\cdot\mbox{ Lip}(F\colon X\to X). \tag{91}\]

Moreover, for all \(x,y\in X\)

\[\|F^{W}(x)-F^{W}(y)\|_{X}\geq(C_{0}-\epsilon)\|x-y\|_{X}\geq\frac{1}{2}C_{0}\| x-y\|_{X}. \tag{92}\]

Let \(c_{0}>0\) be such that \(\|G\|_{L^{\infty}(X)}\leq c_{0}\). Let \(R_{0}>0\). As \(\|T_{2}\circ G\|_{L^{\infty}(X)}\leq\|T_{2}\|c_{0}\), finite dimensional degree theory, see using [43, Theorem 1.2.6], as above that

\[B_{W}(0,R_{0})\subset F^{W}(B_{W}(0,R_{1})), \tag{93}\]

when \(R_{1}>R_{0}+\|T_{2}\|c_{0}\geq R_{0}+\|P_{W}\circ T_{2}\circ G\circ T_{1}\circ P _{2}\|_{L^{\infty}(X)}\). As \(R_{0}>0\) above is arbitrary, formula (93) implies that \(F^{W}:X\to X\) is surjective. Thus, we have shown that \(F^{W}:X\to X\) is a bijective bilipschitz map.

Similarly to the above, by replacing (85) by (92) and changing the roles of \(F^{W}\) and \(F\), we see that for \(x,y\in X\)

\[\|R\circ(I+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})^{-1}(x) -R\circ(I+T_{2}\circ G\circ T_{1})^{-1}(y)\|_{X}\] \[\leq \epsilon\|x-y\|_{X},\]

and

\[\|(F\circ(F^{W})^{-1}-Id)(x)-(F\circ(F^{W})^{-1}-Id)(y)\|_{X}\leq\epsilon\|x-y \|_{X}. \tag{94}\]

The above implies that

\[F^{W}=(Id+B)\circ F, \tag{95}\] \[F=(Id+\tilde{B})\circ F^{W}, \tag{96}\]where

\[B=(F^{W}\circ F^{-1}-Id):X\to X,\] \[\tilde{B}=(F\circ(F^{W})^{-1}-Id):X\to X,\]

satisfy

\[Lip(B)<\frac{1}{2}\epsilon,\quad Lip(\tilde{B})<\epsilon. \tag{97}\]

Next we use that

\[(Id+H)^{-1} = (Id+H)^{-1}\circ(Id+H-H)\] \[= Id-(Id+H)^{-1}\circ H,\]

so that

\[(Id+T_{2}\circ G\circ T_{1})^{-1} = Id-(Id+T_{2}\circ G\circ T_{1})^{-1}\circ(T_{2}\circ G\circ T_{ 1}).\]

Observe that by (86)

\[B= F^{W}\circ(F)^{-1}-Id \tag{98}\] \[= (P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G\circ T _{1})\circ(Id+T_{2}\circ G\circ T_{1})^{-1}\] \[= (P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G\circ T _{1})\] \[-(P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}-T_{2}\circ G\circ T _{1})\circ(Id+T_{2}\circ G\circ T_{1})^{-1}\circ(T_{2}\circ G\circ T_{1}),\]

and as \(P_{W}:X\to X\) is a finite rank operator and \(T_{2}:W\to W\) is a compact linear operator, we see that \(B:X\to X\) is a compact non-linear operator of the form

\[B=P_{W}\circ F_{1}\circ P_{W}+T_{2}\circ F_{2}\circ P_{W}+P_{W} \circ F_{3}\circ T_{1}+T_{2}\circ F_{4}\circ T_{1},\]

where \(F_{1},F_{2},F_{3},F_{4}:X\to X\) are functions in \(C^{1}(X)\).

Moreover, similarly to (86), we see that

\[F\circ(F^{W})^{-1} \tag{99}\] \[= (Id+T_{2}\circ G\circ T_{1})\circ(Id+P_{W}\circ T_{2}\circ G\circ T _{1}\circ P_{W})^{-1}\] \[= Id+(T_{2}\circ G\circ T_{1}-P_{W}\circ T_{2}\circ G\circ T_{1} \circ P_{W})\circ(I+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})^{-1},\]

and hence

\[\tilde{B}= F\circ(F^{W})^{-1}-Id \tag{100}\] \[= (T_{2}\circ G\circ T_{1}-P_{W}\circ T_{2}\circ G\circ T_{1}\circ P _{W})\circ(Id+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})^{-1}\] \[= (T_{2}\circ G\circ T_{1}-P_{W}\circ T_{2}\circ G\circ T_{1}\circ P _{W})\] \[-(T_{2}\circ G\circ T_{1}-P_{W}\circ T_{2}\circ G\circ T_{1}\circ P _{W})\] \[\circ(Id+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})^{-1}\circ(P _{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}),\]

and again, as \(P_{W}:X\to X\) is a finite rank operator and \(T_{2}:W\to W\) is a compact linear operator, we see that \(\tilde{B}:X\to X\) is a compact non-linear operator of the form

\[\tilde{B}=P_{W}\circ\tilde{F}_{1}\circ P_{W}+T_{2}\circ\tilde{F}_{2}\circ P_{W }+P_{W}\circ\tilde{F}_{3}\circ T_{1}+T_{2}\circ\tilde{F}_{4}\circ T_{1},\]

where \(\tilde{F}_{1},\tilde{F}_{2},\tilde{F}_{3},\tilde{F}_{4}:X\to X\) are functions in \(C^{1}(X)\). For an infinite dimensional Hilbert space \(X\) there is a linear isomorphism \(J:X\to X\times X\), as the cardinality of Hilbert basis of the space \(X\) is the same as the cardinality of the Hilbert basis of \(X\times X\), see [26, Theorem 3.5]. Then, by writing the isomorphism \(J\) as \(J(x)=(J_{1}(x),J_{2}(x))\in X\times X\), we see that

\[\tilde{B}(x) = ((\begin{array}{cc}P_{W}&T_{2}\end{array})\circ J)\circ(J^{-1} \circ\left(\begin{array}{cc}\tilde{F}_{1}&\tilde{F}_{3}\\ \tilde{F}_{2}&\tilde{F}_{4}\end{array}\right)\circ J)\circ(J^{-1}\circ\left( \begin{array}{cc}P_{W}\\ T_{1}\end{array}\right))(x),\]

that is a composition of a linear compact operator \(X\to X\), a non-linear operator \(X\to X\), and a compact operator \(X\to X\). Hence we see that \(Id+\tilde{B}\), and similarly \(Id+B\), are layers of neural operators.

Now we present our Proof of Theorem 4.

Proof.: Let \(F(x)=x+T_{2}G(T_{1}(x))\). Without loss of generality, we may assume that \(0<\epsilon<\min(\|T_{2}\|_{X\to X}C_{1}\|T_{1}\|_{X\to X},\frac{1}{2})\).

By (94)

\[\|B(x)-B(y)\|_{X}\leq\epsilon\|x-y\|_{X}, \tag{101}\]

and

\[\langle(Id+B)(x)-(Id+B)(y),x-y\rangle_{X}\geq\|x-y\|_{X}^{2}- \epsilon\|x-y\|_{X}^{2}\] \[=(1-\epsilon)\|x-y\|_{X}^{2}, \tag{102}\]

which implies that

\[Id+B:X\to X, \tag{103}\]

is a strongly monotone operator. Similarly, we see that

\[Id+\tilde{B}:X\to X, \tag{104}\]

is a strongly monotone operator.

Recall that \(F_{W}\) maps \(W^{\perp}\) to itself and it is equal to the identity map in \(W^{\perp}\), and moreover \(F^{W}\) can be decomposed according to the formula (78). Thus we can write

\[F_{W}=Id_{W^{\perp}}\oplus(P_{W}\circ F_{W}\circ P_{W}):W^{\perp} \oplus W\to W^{\perp}\oplus W.\]

Below, let us identify \(W\) with \(\mathbb{R}^{n}\) to clarify notations. We denote

\[f=F_{W}|_{W}=P_{W}\circ F_{W}\circ P_{W}:W\to W. \tag{105}\]

By (91) and (92) there are \(c_{1},c_{0}>0\) such that

\[c_{0}|x-y|\leq|f(x)-f(y)|\leq c_{1}|x-y|. \tag{106}\]

Define4 for \(0<t\leq 1\)

Footnote 4: The above used homotopy can be replace by a more explicit flow in the Lie group of diffeomorphism, see e.g. [42].

\[f_{t}(x):=\frac{1}{t}(f(tx)-f(0))+tf(0). \tag{107}\]

For \(t=0\), we define

\[f_{0}(x):=Df|_{0}x. \tag{108}\]

where \(Df|_{y}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) is the derivative of the map \(f\) at the point \(y\), that is considered as a linear map (or a matrix), and we denote its value at vector \(v\) by \(Df|_{y}(v)=Df|_{y}v\).

Then \(f_{1}(x)=f(x)\) and

\[|f_{t}(x)-f_{t}(y)|=\frac{1}{t}|f(tx)-f(ty)|, \tag{109}\]

and as

\[\frac{1}{t}c_{0}|tx-ty|\leq\frac{1}{t}|f(tx)-f(ty)|\leq\frac{1}{t}c_{1}|tx-ty|,\]

we have

\[c_{0}|x-y|\leq|f_{t}(x)-f_{t}(y)|\leq c_{1}|x-y|.\]

Below, let

\[R_{0}=|f(0)|, \tag{110}\]

and

\[R_{1}=c_{1}r_{1}+R_{0}, \tag{111}\]where \(r_{1}\) appears in the claim of Theorem 4. Observe that then \(f_{0}(B_{\mathbb{R}^{n}}(0,r_{1}))\subset B_{\mathbb{R}^{n}}(0,R_{1})\).

As \(f\in C^{2}(\mathbb{R}^{n},\mathbb{R}^{n})\), we have by Taylor's series (with the remainder written in the integral form)

\[f(tx) =f(0)+Df|_{0}(tx)+\frac{1}{2}\int_{0}^{t}(t-s)Df|_{sx}x\,ds\] \[=f(0)+tDf|_{0}x+\frac{1}{2}t^{2}\int_{0}^{1}(1-s)Df|_{tsx}x\,ds. \tag{112}\]

Thus,

\[f_{t}(x)=Df|_{0}x+\frac{1}{2}t\int_{0}^{1}(1-s)Df|_{tsx}x\,ds+tf(0). \tag{113}\]

Observe that for all \(x\in X\),

\[\|Df|_{x}\|\leq c_{1},\text{ and }\|(Df|_{x})^{-1}\|\leq c_{0}^{-1}.\]

By (113),

\[f_{t}((Df|_{0})^{-1}x)-f_{t}((Df|_{0})^{-1}y)\] \[=x-y+\frac{1}{2}t\int_{0}^{1}(1-s)\bigg{(}Df|_{ts\bar{x}}(Df|_{0} )^{-1}x-Df|_{ts\bar{y}}(Df|_{0})^{-1}y\bigg{)}\,ds\bigg{|}_{\tilde{x}=(Df|_{0} )^{-1}x,\;\tilde{y}=(Df|_{0})^{-1}y}\] \[=x-y+\frac{1}{2}t\int_{0}^{1}(1-s)\bigg{(}Df|_{ts\bar{x}}(Df|_{0} )^{-1}(x-y)\bigg{)}\,ds\bigg{|}_{\tilde{x}=(Df|_{0})^{-1}x,\;\tilde{y}=(Df|_{0} )^{-1}y}\] \[+\frac{1}{2}t\int_{0}^{1}(1-s)\bigg{(}(Df|_{ts\bar{x}}-Df|_{ts \bar{y}})(Df|_{0})^{-1}y\bigg{)}\,ds\bigg{|}_{\tilde{x}=(Df|_{0})^{-1}x,\;\tilde {y}=(Df|_{0})^{-1}y}. \tag{114}\]

Here, for \(x,y\in B_{\mathbb{R}^{n}}(0,R_{1})\),

\[\|Df|_{ts\bar{x}}-Df|_{ts\bar{y}}\|_{\mathbb{R}^{n}\to \mathbb{R}^{n}}\leq\|f\|_{C^{2}(B_{\mathbb{R}^{n}}(0,R_{1}))},\] \[\|Df|_{ts\bar{x}}\|_{\mathbb{R}^{n}\to\mathbb{R}^{n}}\leq c_{1},\] \[\|(Df|_{0})^{-1}y\|_{\mathbb{R}^{n}}\leq c_{0}^{-1}\|y\|_{\mathbb{ R}^{n}}.\]

Hence,

\[f_{t}\circ(Df|_{0})^{-1}=Id+Q_{t,0}:\mathbb{R}^{n}\to\mathbb{R}^{n}, \tag{115}\]

where

\[\text{Lip}_{B_{\mathbb{R}^{n}}(0,R_{1})}(Q_{t,0})\leq t\frac{1}{2c_{0}}(c_{1}+ \|f\|_{C^{2}(B_{\mathbb{R}^{n}}(0,R_{1}))}R_{1}). \tag{116}\]

Let us choose \(t_{1}\in(0,1)\) such that

\[t_{1}<\frac{2c_{0}}{c_{1}+\|f\|_{C^{2}(B_{X}(0,R_{1}))}R_{1}}\varepsilon, \tag{117}\]

so that

\[\text{Lip}_{B_{\mathbb{R}^{n}}(0,R_{1})}(Q_{t_{1},0})<\varepsilon. \tag{118}\]

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_EMPTY:35]

Then, when the condition \(f_{t_{k-1}}(x)\in B_{\mathbb{R}^{n}}(0,R_{2})\) it holds, we see that

\[H_{t_{k}.t_{k-1}}\circ f_{t_{k-1}}(x) = f_{t_{k-1}}(x)+\phi(f_{t_{k-1}}(x))(f_{t_{k}}(f_{t_{k-1}}^{-1}(f_{ t_{k-1}}(x)))-f_{t_{k-1}}(x))\] \[= f_{t_{k-1}}(x)+1\cdot(f_{t_{k}}(f_{t_{k-1}}^{-1}(f_{t_{k-1}}(x)) )-f_{t_{k-1}}(x))\] \[= f_{t_{k}}(f_{t_{k-1}}^{-1}(f_{t_{k-1}}(x)))\] \[= f_{t_{k}}(x)\in B_{\mathbb{R}^{n}}(0,R_{2}).\]

Observe that when \(x\in B_{\mathbb{R}^{n}}(0,R_{0})\) then \(f_{t_{0}}(x)\in B_{\mathbb{R}^{n}}(0,R_{1})\subset B_{\mathbb{R}^{n}}(0,R_{2})\). Next, we denote \(t_{0}=0\). Hence, by using induction, we see that for all \(j=1,2,\ldots,m+1\) it holds that

\[H_{t_{j},t_{j-1}}\circ H_{t_{j-1},t_{j-2}}\circ H_{t_{1},t_{0}} \circ f_{t_{0}}(x)=f_{t_{j}}(x),\mbox{ and }\] \[H_{t_{j},t_{j-1}}\circ H_{t_{j-1},t_{j-2}}\circ H_{t_{1},t_{0}} \circ f_{t_{0}}(x)=f_{t_{j}}(x)\in B_{\mathbb{R}^{n}}(0,R_{2}).\]

We recall that above we chose \(t_{1}>0\) such that

\[t_{1}<\frac{2c_{0}}{c_{1}+\|f\|_{C^{2}(B_{\mathbb{R}^{n}}(0,R_{1}))}R_{1}}\varepsilon. \tag{145}\]

We now choose \(m\) so that there are \(t_{k}\), \(k=2,3,\ldots,m+1\) satisfying \(|t_{k}-t_{k-1}|<\frac{1}{m}\) and

\[\frac{1}{m}<\varepsilon\frac{t_{1}}{\left(\frac{c_{1}}{c_{0}}(10+R_{2})+\frac{ 6}{R_{2}}(\frac{c_{1}}{c_{0}}+1)R_{0}\right)}.\]

This means that we can choose

\[m>\frac{1}{\varepsilon}\frac{2}{t_{1}}\bigg{(}\frac{c_{1}}{c_{0}}(10+R_{2})+ \frac{6}{R_{2}}(\frac{c_{1}}{c_{0}}+1)R_{0}\bigg{)}. \tag{146}\]

Then,

\[\mbox{Lip}_{\mathbb{R}^{n}}(P_{t_{k+1},t_{k}}) \leq \varepsilon. \tag{147}\]

We define the map

\[\alpha(t)=\left\{\begin{array}{ll}f_{t}|_{K},&\mbox{for}\;0<t\leq 1,\\ Df|_{0},&\mbox{for}\;t=0,\end{array}\right.\]

is a continuous map \([0,1]\to C^{1}(\mathbb{R}^{n};\mathbb{R}^{n})\).

The space \(W\) is a finite dimensional Euclidean space, and let \(GL(W)\) be the set of linear diffeomorphisms of it, that is, invertible linear operators \(A:W\to W\). We consider \(GL(W)\) with the topology given by the operator norm of linear maps. Then \(GL(W)\) is a topological space with two path connected components - those matrices which have a positive determinant and those having a negative determinant.

The above implies that \(Df|_{0}\) can be connected in \(GL(\mathbb{R}^{n})\) either to a matrix \(B\) where

\[B\mbox{ is either the identity map, or the diagonal matrix diag}(-1,1,1,\ldots,1), \tag{148}\]

with a continuous path \(\beta_{s}\in GL(W)\), \(s\in[0,1]\) with \(Df|_{0}=\beta_{1}\) and \(B=\beta_{0}\).

We need to consider the path \(\beta_{s}\) in \(GL(W)\) from \(\beta_{1}=f_{0}=Df|_{0}\) to matrix \(\beta_{0}=B\in O(n)\). There are some explicit formulas in literature with relatively complicated formulas, see [41]. However, let us next consider estimates using a non-optimal but relatively explicit path. To do this we start from the PU-decomposition of the matrix \(\beta_{1}=Df|_{0}\), that we denote \(\beta_{1}=PU\) where \(P=\mbox{diag}\;(\sigma_{1},\ldots,\sigma_{n})\) is positive matrix (given in a suitable basis) and \(U\) is an orthogonal matrix. Then, we consider the path \(t\to P_{t}U\), where \(t\in[0,1]\) and

\[P_{t}=P^{1-t}=\exp((1-t)\log(P)).\]

This is a path from the matrix \(Df|_{0}=PU\) to the matrix \(U\). Observe that

\[P_{t_{2}}U=P^{t_{1}-t_{2}}P_{t_{1}}U,\]where

\[P^{t_{1}-t_{2}}=\mbox{diag}\;(\sigma_{1}^{t_{1}-t_{2}},\ldots,\sigma_{n}^{t_{1}-t_{ 2}}),\]

and when \(|t_{1}-t_{2}|\leq 1\), by mean value theorem we have

\[\|P^{t_{1}-t_{2}}-I\| \leq \max_{j=1,\ldots,n}(\sigma_{j}^{t_{1}-t_{2}}-1)\leq\max_{j=1, \ldots,n}(\sigma_{j},\sigma_{j}^{-1})\cdot(\max_{j=1,\ldots,n}|\log\sigma_{j}| )\cdot|t_{1}-t_{2}|\] \[\leq (1+c_{1}+c_{0}^{-1})^{2}\cdot|t_{1}-t_{2}|.\]

That is, using the norm of \(Df|_{0}\) and the norm of inverse matrix \((Df|_{0})^{-1}\) we can bound the length of the path \(t\to P_{t}U\), \(t\in[0,1]\) (in the operator norm). After this, we need to consider the path from \(U\) to \(B=Id_{k}\times(-Id_{n-k})\) in \(O(W)\). For this, we could use the structure of the Lie group \(O(n)\), \(n=\dim(W)\). By [15, Thm. 11.3.3], any matrix \(U\) in \(O(n)\) can be written as a tensor product of elements in \(O(2)\) and possibly an operator \(\pm Id:\mathbb{R}\to R\), that is, in a suitable orthogonal basis a matrix \(U\in O(n)\) is a block diagonal matrix of \(2\times 2\) matrices in \(O(2)\) and one or two \(1\times 1\) matrices \(\pm Id\), that is,

\[U=\begin{bmatrix}R_{1}&&&&&&&\\ &\ddots&&&0&\\ &&R_{k}&&&\\ &&\pm 1&&\\ &0&&&\ddots&\\ &&&\pm 1\end{bmatrix},\]

where the matrices \(R_{1}=R_{1}(\vartheta_{1})\),..., \(R_{k}=R_{1}(\vartheta_{k})\) are 2-by-2 rotation matrices in \(SO(2)\),

Using this, we find a path form \(U(s)\), \(s\in[0,1]\) either to a matrix \(Id_{k}\times(-Id_{n-k})\),

\[U(s)=\begin{bmatrix}R_{1}(s\vartheta_{1})&&&&&&&\\ &\ddots&&&0&\\ &&R_{k}(s\vartheta_{k})&&&\\ &&\pm 1&&\\ &0&&&\ddots&\\ &&&\pm 1\end{bmatrix}.\]

Observe that due the block diagonal form of this matrix, the operator norm satisfies

\[\|U(s_{2})-U(s_{1})\|_{\mathbb{R}^{n}\to\mathbb{R}^{n}}\leq\max_{j=1,2,\ldots, k}\|R_{j}(s_{2}\vartheta_{j})-R_{j}(s_{1}\vartheta_{j})\|_{R^{2}\to\mathbb{R}^{2 }}\leq C_{*}|s_{2}-s_{1}|,\]

where \(C_{*}\) is an absolute constant (that does not depend on the dimension \(n\)). We obtain the path \(s\to\beta_{s}\) by concatenating the paths from \(PU\) to \(U\) in \(GL(n)\) and from \(U\) to \(B\) in \(SO(n)\). The length of the obtained path \(\beta\) in the operator norm metric can be estimated and it is bounded \(C(1+c_{1}+c_{0}^{-1})^{2}\) plus a constant. Moreover, the path \(s\to\beta_{s}\) can be decomposed to a product of \((C(1+c_{1}+c_{0}^{-1})^{2}+C)\varepsilon^{-1}\) matrices of the form \(Id+B_{j}\) where \(\|B_{j}\|\leq\varepsilon\).

Summarising the above analysis, we see that \(J\) can bounded by

\[J\leq\frac{1}{\varepsilon}\frac{2}{t_{1}}\bigg{(}\frac{c_{1}}{c_{0}}(10+R_{2}) +\frac{6}{R_{2}}(\frac{c_{1}}{c_{0}}+1)R_{0}\bigg{)}+C(1+c_{1}+c_{0}^{-1})^{2 }\varepsilon^{-1}+C\varepsilon^{-1}. \tag{149}\]

We recall that here

\[t_{1}<\frac{2c_{0}}{c_{1}+\|f\|_{C^{2}(B_{2^{n}}(0,R_{1}))}R_{1}}\varepsilon, \tag{150}\]

radii \(R_{0}\) and \(R_{1}\) are given in formulas (110) and (111) and \(c_{0}\) and \(c_{1}\) are the bi-Lipschitz constants of \(f\), see (106).

Note that \(c_{0},c_{1},C>0\) are constants independent of \(\epsilon\), while \(R_{0},R_{1},R_{2}>0\) depends on \(|f(0)|\). We see that by \(f=P_{W}F|_{W}\).

\[|f(0)|\leq\|F(0)\|_{X},\]thus, the upper bounds of \(R_{0},R_{1},R_{2}\) are independent of \(\epsilon\). By choosing \(t_{1}\) as \(t_{1}=\frac{c_{0}}{c_{1}+\|f\|_{C^{2}(B_{\mathbb{R}^{m}}(0,R_{1}))}R_{1}}\varepsilon\), we furthermore estimate that

\[J\lesssim\|f\|_{C^{2}(B_{\mathbb{R}^{m}}(0,R_{1});\mathbb{R}^{n})}\varepsilon^ {-2}. \tag{151}\]

Here, if we assume that \(F\in C^{2}(X,X)\), we see that

\[\|f\|_{C^{2}(B_{\mathbb{R}^{m}}(0,R_{1});\mathbb{R}^{n})}\leq\|F\|_{C^{2}(W;X)} \leq\|F\|_{C^{2}(X;X)}<\infty,\]

Thus, we obtain

\[J=\mathcal{O}(\varepsilon^{-2}).\]

Now we can finish the proof of Theorem 4. Denote \(\tilde{f}_{t}=f_{t}\oplus Id_{W^{\perp}}\), \(\tilde{H}_{t_{1},t_{0}}=H_{t_{1},t_{0}}\oplus Id_{W^{\perp}}\)\(\tilde{\beta}_{s}=\beta_{s}\oplus Id_{W^{\perp}}\) and \(A_{0}=B\oplus Id_{W^{\perp}}\). Using these notations, we can write \(F(x)\) as

\[F(x)=(F\circ(F^{W})^{-1})\circ\tilde{H}_{t_{m+1},t_{m}}\circ \tilde{H}_{t_{m},t_{m-1}}\circ\ldots\] \[\ldots\circ\tilde{H}_{t_{1},t_{0}}\circ(\tilde{\beta}_{1}\circ \tilde{\beta}_{s_{m}}^{-1})\circ(\tilde{\beta}_{s_{m}}\circ\tilde{\beta}_{s_{m -1}}^{-1})\circ\ldots\circ(\tilde{\beta}_{s_{1}}\circ\tilde{\beta}_{0}^{-1}) \circ A_{0}, \tag{152}\]

where \(t_{j}\) and \(s_{j}\) are chosen so that \(0=t_{0}<t_{1}<\ldots t_{m}<t_{m+1}=1\) and \(0=s_{0}<s_{1}<\ldots s_{m}<s_{m+1}=1\) and that the Lipschitz constant of the maps \(\tilde{H}_{t_{j+1},t_{j}}-Id\) and \(\tilde{\beta}_{s_{j}}\circ\tilde{\beta}_{j-1}^{-1}-Id\) are less that \(\epsilon\).

Moreover, recall that

\[F^{W}=Id+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W}\;:X\to X.\]

Observe that by writing \(x\in X\) in the form \(x=x_{0}+x_{1}\), where \(x_{0}=(I-P_{W})x\) and \(x_{1}=P_{W}x\), we see that

\[F^{W}(x_{0}+x_{1}) = (I+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})(x_{0}+x_{1}) \tag{153}\] \[= x_{0}+(I+P_{W}\circ T_{2}\circ G\circ T_{1}\circ P_{W})(x_{1})\] \[= (I-P_{W})x+P_{W}\bigg{(}(I+P_{W}\circ T_{2}\circ G\circ T_{1} \circ P_{W}))(P_{W}x)\bigg{)}\] \[= (I-P_{W})x+P_{W}(F^{W}(P_{W}x)).\]

Hence, \((F^{W})^{-1}:X\to X\) can be written as

\[(F^{W})^{-1} = I-P_{W}+P_{W}\circ(F^{W})^{-1}\circ P_{W} \tag{154}\] \[= I+P_{W}\circ(-I+(F^{W})^{-1})\circ P_{W},\]

and thus \((F^{W})^{-1}\) is a layer of a neural operator by definition. Similarly, as

\[\tilde{f}=F^{W}=Id_{X}+P_{W}\circ(F_{W}-Id_{W})\circ P_{W}:X\to X.\]

and

\[f_{t}(x) = \frac{1}{t}(f(tx)-f(0))+tf(0) \tag{155}\] \[= Id-P_{W}+P_{W}(Id+\frac{1}{t}(f(tP_{W}x)-f(0))+tf(0)),\]

for \(0<t\leq 1\), and we see as above that \(f_{t}\) and \(f_{t}^{-1}\) are neural operators. Similarly, we see that \(\tilde{\beta}_{s}^{-1}\) and \(\tilde{\beta}_{s}^{-1}\) are neural operators. Hence, all factors in the product (152) are (strictly monotone) neural operators.

#### a.7.2 Proof of Theorem 5

**Theorem 6** (Theorem 5 in the main text).: _Let \(X\) be a separable Hilbert space, and let \(\varphi=\{\varphi_{n}\}_{n\in\mathbb{N}}\) be an orthonormal basis in \(X\). Let \(\delta\in(0,1)\), and let \(R>0\), and let \(F\colon X\to X\) be a layer of a bilipschitz neural operator, as in Def. 3. Then, for any \(\epsilon\in(0,1)\), there are \(T,N\in\mathbb{N}\) and \(G\in\mathcal{R}_{T,N,\varphi,ReLU}(X)\) that has the form_

\[G=(Id_{X}+D_{N}\circ NN_{T}\circ E_{N})\circ\cdots\circ(Id_{X}+D_{N}\circ NN_{ 1}\circ E_{N}),\]

_such that_

\[\sup_{x\in\widetilde{B}_{X}(0,R)}\|F(x)-G\circ A(x)\|_{X}\leq\epsilon,\]

_where \(A:X\to X\) is a linear invertible map that is either the identity map or a reflection operator \(x\to x-2\langle x,e\rangle_{X}e\) with some unit vector \(e\in X\). Moreover, \(G\circ A:B_{X}(0,R)\to G\circ A(B_{X}(0,R))\) is invertible, and there exists \(R^{\prime}>0\) such that_

\[A(B_{X}(0,R))\subset B_{X}(0,R^{\prime}),\]

_and denoting by_

\[\Gamma_{0}:=B_{X}(0,R^{\prime}),\quad\Gamma_{1}:=B(0,R^{\prime}+\delta),\quad \cdots\quad\Gamma_{T}:=B(0,R^{\prime}+T\delta),\quad\Gamma_{T+1}:=B(0,R^{ \prime}+(T+1)\delta),\]

_and_

\[\tilde{K}_{0}:=A(B_{X}(0,R)),\quad\tilde{K}_{1}:=(Id_{X}+D_{N}\circ NN_{1}\circ E _{N})\tilde{K}_{0},\quad\tilde{K}_{2}:=(Id_{X}+D_{N}\circ NN_{2}\circ E_{N}) \tilde{K}_{1},\]

\[\cdots\quad\tilde{K}_{T}:=(Id_{X}+D_{N}\circ NN_{T}\circ E_{N})\tilde{K}_{T-1}= G\circ A(B_{X}(0,R)),\]

_we have that for each \(t=0,1,...,T\)_

\[\tilde{K}_{t}\subset\Gamma_{t}\]

_and the mapping \(G\circ A|_{B_{X}(0,R)}:B_{X}(0,R)\to G\circ A(B_{X}(0,R))\) is bijective and its inverse is given by_

\[\big{(}G\circ A|_{B_{X}(0,R)}\big{)}^{-1}=A^{-1}\circ\Phi,\]

_where \(\Phi:G\circ A(B_{X}(0,R))\to A(B_{X}(0,R))\) is defined by_

\[\Phi:=L_{1}|_{\tilde{K}_{1}}\circ\cdots\circ L_{T-1}|_{\tilde{K}_{T-1}}\circ L _{T}|_{\tilde{K}_{T}}, \tag{156}\]

_where \(L_{t}:\Gamma_{t}\to\Gamma_{t+1}\)_

_is defined by_

\[L_{t}(y):=\lim_{n\to\infty}\pi_{1}\circ(\bigodot_{h=1}^{n}\phi_{t})\circ e_{1} (y),\]

_where \(\phi_{t}:\Gamma_{t+1}\times\Gamma_{t}\to\Gamma_{t+1}\times\Gamma_{t}\) is defined by_

\[\phi_{t}(x,y)=(y+D_{N}\circ NN_{t}\circ E_{N}(x),y),\]

_where \(\pi_{1}(x,y)=x\) and \(e_{1}(y)=(0,y)\)._

**Remark 2**.: _Note that, in the proof, we have used the approximation result [22, Theorem 4.3] of Sobolev functions by neural networks with ReLU function \(x\mapsto\max\{0,x\}\), which is not differentiable at \(0\). Alternatively, we can use approximation result [1, Theorem 4.1] by neural networks with ReLU function \(x\mapsto\max\{0,x\}^{3}\), which are continuously differentiable. Then, each block \(Id_{X}+D_{N}\circ NN_{t}\circ E_{N}\) in a obtained approximator \(G\) is \(C^{1}\) and strongly monotone on ball \(\Gamma_{t}=B(0,R^{\prime}+t\delta)\), that is, it holds that there is an \(\alpha>0\) such that_

\[\langle(Id_{X}+D_{N}\circ NN_{t}\circ E_{N})x,x\rangle_{X}\geq\alpha\|x-y\|_{ X}^{2},\;x,y\in\Gamma_{t},\]

_which implies that, by the same argument as in Lemma 3,_

\[(Id_{X}+D_{N}\circ NN_{t}\circ E_{N}):\Gamma_{t}\to(Id_{X}+D_{N}\circ NN_{t} \circ E_{N})(\Gamma_{t}),\]

_is a \(C^{1}\)-diffeomorphism._

**Remark 3**.: _From the proof, we can show that, for each \(t=1,...,T\)_

\[L_{t}(\tilde{K}_{t})\subset\tilde{K}_{t-1}.\]

_Then, the mapping \(\Phi\) defined in (156) is well-defined._Proof.: Let \(\delta,\epsilon\in(0,1)\), and let \(F\colon X\to X\) be a layer of a neural operator defined in (4). Assume that \(F\) is bilipschitz, that is, it satisfies (85) and

\[\|F(x)-F(y)\|_{X}\leq C_{1}\|x-y\|_{X},\quad\text{for all }x,y\in X. \tag{157}\]

By Theorem 4, there is \(T\in\mathbb{N}\) such that \(F\) can represented in the form

\[F(x)=(Id_{X}-H_{T})\circ(Id_{X}-H_{T-1})\circ\cdots\circ(Id_{X}-H_{1})\circ A( x),\quad\text{for }x\in B_{X}(0,R),\]

where \(A:X\to X\) is a linear invertible map that is either the identity map or a reflection operator \(x\to x-2\langle x,e\rangle_{X}e\) with some unit vector \(e\in X\), and \(H_{t}:X\to X\) are global Lipschitz maps satisfying

\[\operatorname{Lip}_{X\to X}(H_{t})\leq\delta/2. \tag{158}\]

for all \(t\). Moreover, the operator \(H_{t}:X\to X\) is a compact mapping. We choose \(R^{\prime}>0\) such that

\[A(B_{X}(0,R))\subset B_{X}(0,R^{\prime}).\]

We denote by

\[K_{0}:=A(B_{X}(0,R)),\quad K_{1}:=(Id_{X}-H_{1})(K_{0}),\quad\cdots\quad K_{T }:=(Id_{X}-H_{T})(K_{T-1}).\]

\[\Gamma_{0}:=B_{X}(0,R^{\prime}),\quad\Gamma_{1}:=B(0,R^{\prime}+\delta),\quad \cdots\quad\Gamma_{T+1}:=B(0,R^{\prime}+(T+1)\delta).\]

We choose \(\tilde{R}>0\) such that for all \(t=0,...,T+1\)

\[E_{N}(K_{t}),E_{N}(\Gamma_{t})\subset B_{\mathbb{R}^{N}}(0,\tilde{R}).\]

Since \(H_{t}:X\to X\) is a compact mapping, for large enough \(N\in\mathbb{N}\), we have that for all \(t=1,...,T\),

\[\sup_{x\in B_{X}(R)}\|H_{t}(x)-P_{V_{N}}\circ H_{t}\circ P_{V_{N}}(x)\|_{X} \leq\frac{\epsilon}{2T(1+\delta)^{T}}. \tag{159}\]

Note that

\[P_{V_{N}}\circ H_{t}\circ P_{V_{N}}=D_{N}\circ\tilde{H}_{N,t}\circ E_{N},\]

where \(\tilde{H}_{N,t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\) by

\[\tilde{H}_{N,t}:=E_{N}\circ H_{t}\circ D_{N}.\]

From (158), we have

\[\tilde{H}_{N,t}\in W^{1,\infty}(B_{\mathbb{R}^{N}}(0,\tilde{R});\mathbb{R}^{N}).\]

By approximation by ReLU neural networks in Sobolev spaces (see e.g., [22, Theorem 4.3]), there is a ReLU neural network \(NN_{t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\), \(NN_{t}\in\mathcal{R}_{1,N,\varphi,ReLU}(X)\) such that

\[\|\tilde{H}_{N,t}-NN_{t}\|_{W^{1,\infty}(B_{\mathbb{R}^{N}}(0,\tilde{R}); \mathbb{R}^{N})}\leq\min\left\{\frac{\epsilon}{2T(1+\delta)^{T}},\frac{\delta }{2}\right\}. \tag{160}\]

Then, we estimate that by (159) and (160)

\[\sup_{x\in K_{t-1}}\|H_{t}(x)-D_{N}\circ NN_{t}\circ E_{N}(x)\|_{X}\leq\frac{ \epsilon}{T(1+\delta)^{T}}. \tag{161}\]

Also, we estimate that by (158) and (160)

\[\operatorname{Lip}_{B_{\mathbb{R}^{N}}(0,\tilde{R})\to\mathbb{R}^ {N}}(NN_{t}) \leq\operatorname{Lip}_{B_{\mathbb{R}^{N}}(0,\tilde{R})\to\mathbb{R}^ {N}}(\tilde{H}_{N,t})+\operatorname{Lip}_{B_{\mathbb{R}^{N}}(0,\tilde{R})\to \mathbb{R}^{N}}(\tilde{H}_{N,t}-NN_{t}) \tag{162}\] \[\leq\operatorname{Lip}_{X\to X}(H_{t})+\|\tilde{H}_{N,t}-NN_{t}\| _{W^{1,\infty}(B_{\mathbb{R}^{N}}(0,\tilde{R});\mathbb{R}^{N})}\leq\delta.\]

We denote \(G:X\to X\) by

\[G:=(I-D_{N}\circ NN_{T}\circ E_{N})\circ(I-D_{N}\circ NN_{T-1}\circ E_{N}) \circ\cdots\circ(I-D_{N}\circ NN_{1}\circ E_{N}),\]which belong to \(\mathcal{R}_{T,N,\varphi,ReLU}(X)\). Then, by (161) and (162), we estimate that for all \(x\in B_{X}(0,R)\),

\[\|F(x)-G\circ A(x)\|_{X}\] \[\leq\sum_{1\leq t\leq T}\|\left(\bigcirc_{h=t+1}^{T}(I-D_{N}\circ NN _{h}\circ E_{N})\right)\circ\left(\bigcirc_{h=1}^{t}(I-H_{h})\right)\circ A(x)\] \[\qquad\qquad\qquad\qquad-\left(\bigcirc_{h=t}^{T}(I-D_{N}\circ NN _{h}\circ E_{N})\right)\circ\left(\bigcirc_{h=1}^{t-1}(I-H_{h})\right)\circ A( x)\|_{X}\] \[\leq\sum_{1\leq t\leq T}\prod_{h=t+1}^{T}\left(1+\mathrm{Lip}_{B_ {k^{N}}(0,\bar{R})\to\mathbb{R}^{N}}(NN_{h})\right)\sup_{y\in K_{t-1}}\|(I-H_ {t})(y)-(I-D_{N}\circ NN_{t}\circ E_{N})(y)\|_{X}\] \[\leq\sum_{1\leq t\leq T}\prod_{h=t+1}^{T}(1+\delta)\sup_{y\in K_{ t-1}}\|H_{t}(y)-D_{N}\circ NN_{t}\circ E_{N}(y)\|_{X}\] \[\leq\sum_{1\leq t\leq T}(1+\delta)^{T}\frac{\epsilon}{T(1+\delta )^{T}}\leq\epsilon.\]

Next, as (160), we see that

\[(I-D_{N}\circ NN_{1}\circ E_{N})|_{\Gamma_{0}}:\Gamma_{0}\to \Gamma_{1},\quad(I-D_{N}\circ NN_{2}\circ E_{N})|_{\Gamma_{1}}:\Gamma_{1}\to \Gamma_{2},\quad\cdots\] \[(I-D_{N}\circ NN_{T}\circ E_{N})|_{\Gamma_{T-1}}:\Gamma_{T-1}\to \Gamma_{T}.\]

and

\[G|_{\Gamma_{0}}=(I-D_{N}\circ NN_{T}\circ E_{N})|_{\Gamma_{T-1}}\circ(I-D_{N} \circ NN_{T-1}\circ E_{N})|_{\Gamma_{T-2}}\circ\cdots\circ(I-D_{N}\circ NN_{1 }\circ E_{N})|_{\Gamma_{0}},\]

which means that \(G|_{\Gamma_{0}}\) maps from \(\Gamma_{0}\) to \(\Gamma_{T}\). For \(t=1,...,T\), we define \(L_{t}:\Gamma_{t}\to\Gamma_{t+1}\) by

\[L_{t}(y):=x^{*},\quad y\in\Gamma_{t},\]

where \(x^{*}\in\Gamma_{t+1}\) is a unique fixed point of \(h_{t,y}:\Gamma_{t+1}\to\Gamma_{t+1}\), where

\[h_{t,y}(x):=y+D_{N}\circ NN_{t}\circ E_{N}(x),\]

that is, \(x^{*}\in\Gamma_{t+1}\) is a unique solution of

\[h_{t,y}(x)=x\quad\iff\quad y=(I-D_{N}\circ NN_{t}\circ E_{N})(x),\quad x\in \Gamma_{t+1}.\]

Indeed, there is a unique solution because we have for \(x_{1},x_{2}\in\Gamma_{t+1}\), from (162)

\[\|h_{t,y}(x_{1})-h_{t,y}(x_{2})\|_{X}=\|D_{N}\circ NN_{t}\circ E_{N}(x_{1})-D _{N}\circ NN_{t}\circ E_{N}(x_{2})\|_{X}\leq\delta\|x_{1}-x_{2}\|_{X},\]

which implies that \(h_{t,y}:\Gamma_{t+1}\to\Gamma_{t+1}\) is a contraction map. Then, we have for \(x\in\Gamma_{t-1}\), \(t=1,...,T\),

\[L_{t}\circ(I-D_{N}\circ NN_{t}\circ E_{N})(x)=x. \tag{163}\]

Here, the solution \(x^{*}\in\Gamma_{t+1}\) is given by

\[x^{*}=\lim_{n\to\infty}x_{n},\]

where

\[x_{0}=0,\quad x_{n+1}=y+D_{N}\circ NN_{t}\circ E_{N}(x_{n}).\]

We define \(\varphi_{t}:\Gamma_{t+1}\times\Gamma_{t}\to\Gamma_{t+1}\times\Gamma_{t}\) by

\[\varphi_{t}(x,y)=(y+D_{N}\circ NN_{t}\circ E_{N}(x),y).\]

Let \(\pi_{1}(x,y)=x\) and \(e_{1}(y)=(0,y)\) where \(\pi_{1}:X\times X\to X\) and \(e_{1}:X\to X\times X\) are linear maps. Then,

\[L_{t}(y)=x^{*}=\lim_{n\to\infty}\pi_{1}\circ(\bigcirc_{h=1}^{n}\varphi_{t}) \circ e_{1}(y).\]

We define \(\Phi_{P}:\Gamma_{T}\to\Gamma_{0}\) by

\[\Phi_{P}:=P_{\Gamma_{0}}\circ L_{1}\circ P_{\Gamma_{1}}\circ L_{2}\circ\cdots \circ P_{\Gamma_{T-1}}\circ L_{T},\]

where \(P_{\Gamma_{t}}:X\to X\) is the projection onto the convex set \(\Gamma_{t}=B(0,R^{\prime}+t\delta)\). Then, by (163), we have for \(x\in B_{X}(0,R^{\prime})\)

\[\Phi_{P}\circ G(x)=x. \tag{164}\]Therefore, \(\Phi_{P}\) is the left-inverse of \(G\), that is, \(G|_{A(B_{X}(0,R))}:A(B_{X}(0,R))\to X\) is injective, and \(G|_{A(B_{X}(0,R))}:A(B_{X}(0,R))\to G\circ A(B_{X}(0,R))\) is bijective, and the inverse \((G|_{G\circ A(B_{X}(0,R))})^{-1}:G\circ A(B_{X}(0,R))\to A(B_{X}(0,R))\) is given by

\[(G|_{G\circ A(B_{X}(0,R))})^{-1}=\Phi,\]

where \(\Phi:G\circ A(B_{X}(0,R))\to A(B_{X}(0,R))\) is defined by \(\Phi:=\Phi_{P}|_{G\circ A(B_{X}(0,R))}\) and has the form

\[\Phi=L_{1}\circ L_{2}\circ\cdots\circ L_{T}|_{G\circ A(B_{X}(0,R))}.\]

Therefore, \(G\circ A:B_{X}(0,R)\to G\circ A(B_{X}(0,R))\) is also bijective, and the inverse is given by

\[(G\circ A|_{B_{X}(0,R)})^{-1}=A^{-1}\circ\Phi.\]

Note that \(G\circ A(B_{X}(0,R))\subset\Gamma_{T}\). 

### Production of Quantitative Universal Approximation Estimates

Here, we show how our framework may be used 'out of the box' to produce quantitative approximation results for discretization of neural operators.

Let \(X\) be a separable Hilbert space. We will consider a Hilbert space \(X\), endowed with its norm topology. Recall that \(S_{0}(X)\subset S(X)\) is a partially ordered lattice of finite dimensional subspaces of \(X\). Next, we consider quantified discretization of a continuous, possibly non-linear function \(F\colon X\to X\), that is, how the discretizations \(F_{V}\colon V\to V\) can be chosen (using e.g. neural networks \(\mathcal{F}_{V}\) having a given architecture for each subspace \(V\subset X\)) so that the obtained discretization operator \(\mathcal{A}_{\mathcal{F}}\) has the explicitly given error bounds \(\varepsilon_{V}\).

Using quantitative approximation results for neural networks in \(\mathbb{R}^{d}\), see e.g. [57] or [23], one obtains quantitative results for neural operators. An example of such result is given below.

**Proposition 5**.: _Let \(r>0\) and \(F\colon\overline{B}_{X}(0,r)\to X\) be a non-linear function satisfying \(F\in\text{Lip}(\overline{B}_{X}(0,r);X)\), in \(n=1\), or \(F\in C^{n}(\overline{B}_{X}(0,r);X)\), if \(n\geq 2\). Let \(\varepsilon_{V}>0\) be numbers indexed by the linear subspaces \(V\subset X\) such that \(\varepsilon_{V}\to 0\) as \(V\to X\). When \(d=\text{dim}(V)\), the space \(V\) is identified with \(\mathbb{R}^{d}\) using an isometric isomorphism \(J_{V}\colon V\to\mathbb{R}^{d}\). Then there is a feed forward neural network \(F_{V,\theta}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) with ReLU-activation functions with at most \(C(n,d)\log_{2}((1+r)^{n}/\varepsilon_{V})\) layers and \(C(n,d)\varepsilon_{V}^{-d/n}\log_{2}((1+r)^{n}/\varepsilon_{V})\) non-zero elements in the weight matrices such that \(\mathcal{A}_{NN}:F\to(F_{V})_{V\in S_{0}(X)}\), where \(F_{V}=J_{V}^{-1}\circ F_{V,\theta}\circ J_{V}\colon V\to V\), is an \(\vec{\varepsilon}\)-approximation operation in the ball \(B_{X}(0,r)\)._

Proof.: Let \(\text{Lip}(F\colon\overline{B}_{X}(0,r)\to X)\leq M\), in \(n=1\), or \(\|F\|_{C^{n}(\overline{B}_{X}(0,r);X)}\leq M\), if \(n\geq 2\). Let \(V\subset X\) be a linear subspace of dimension \(d\) and \(\varepsilon_{V}>0\). Let us use some orthogonal basis of \(V\) to identify \(V\) and \(\mathbb{R}^{d}\) and denote the identifying isomorphism by \(J:\mathbb{R}^{d}\to V\). The function \(\hat{F}\colon\mathbb{R}^{d}\to\mathbb{R}^{d}\), given by \(\hat{F}=J^{-1}\circ F_{V}\circ F\circ J\) satisfies \(\text{Lip}(\hat{F}\colon\overline{B}_{\mathbb{R}^{d}}(0,r))\to X)\leq M\) if \(n=1\), or \(\|\hat{F}\|_{C^{n}(\overline{B}_{\mathbb{R}^{d}}(0,r))}\leq M\) if \(n\geq 2\). Then, by applying [57, Theorem 1] or [23], we see that there exists a feed forward neural network \(F_{V,\theta}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) with ReLU-activation functions with at most \(C(n,d)\log_{2}((1+r)^{n}/\varepsilon_{V})\) layers and \(C(n,d)\varepsilon_{V}^{-d/n}\log_{2}((1+r)^{n}/\varepsilon_{V})\) non-zero elements in the weight matrices such that

\[\|F_{V,\theta}(x)-\hat{F}(x)\|_{\mathbb{R}^{d}}\leq M\varepsilon_{V}. \tag{165}\]

Due to Def. 1, this yields the claim. 

## Appendix B Invertible residual network on separable Hilbert spaces

In this section, we consider the approximation of diffeomorphisms by globally invertible residual networks on Hilbert spaces. From the viewpoint of no-go theorem, as the class of diffeomorphisms is too large, we focus on the class of strongly monotone \(C^{1}\)-diffeomorphisms with compact support5.

We first obverse a following similar result with Theorem 3 that strongly monotone \(C^{1}\)-diffeomorphism \(F:X\to X\) with compact support has the property that any linear discretizations \(P_{V}F|_{V}\) are still strongly monotone \(C^{1}\)-diffeomorphism with compact support. The proof can be given by the same argument in Theorem 3 because \(F\) has the form of \(F=Id+B\) where \(B:=F-Id\) is a compact mapping.

**Proposition 6**.: _Let \(\mathcal{A}_{\mathrm{lin}}\) be the discretization functor that maps \(F\) to \(P_{V}F|_{V}\) for each finite subspace \(V\subset X\). Let \(\mathcal{D}_{smc}\) and \(\mathcal{B}_{smc}\) be categories where \(F:X\to X\) and \(F_{V}:V\to V\) are strongly monotone \(C^{1}\)-diffeomorphisms with compact support. Then, the functor \(\mathcal{A}_{\mathrm{lin}}:\mathcal{D}_{smc}\to\mathcal{B}_{smc}\) satisfies assumption (A), and it is continuous in the sense of Definition 8._

Under the same setting and notations in Section 3.5, we define the class of invertible residual networks in the separable Hilbert space by, for \(T,N\in\mathbb{N}\) and \(\delta\in(0,1)\),

\[\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X):=\Big{\{}G\in \mathcal{R}_{T,N,\varphi,\delta,\sigma}(X):G=\bigcirc_{t=1}^{T}(Id_{X}+D_{N} \circ NN_{t}\circ E_{N}),\] \[\mathrm{Lip}_{X\to X}(D_{N}\circ NN_{t}\circ E_{N})\leq\delta\;(t =1,...,T)\Big{\}}, \tag{166}\]

which is a subset of \(\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\) defined in (10). The smallness of Lipschitz constants \(\mathrm{Lip}_{X\to X}(D_{N}\circ NN_{t}\circ E_{N})\) implies that \(\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\) is included in the class of homeomorphisms. The following lemma shows this fact.

**Lemma 9**.: _Let \(\delta\in(0,1)\), and let \(F\in\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\). If \(\sigma:\mathbb{R}\to\mathbb{R}\) is Lipschitz continuous, then \(F:X\to X\) is homeomorphism. Moreover, if \(\sigma:\mathbb{R}\to\mathbb{R}\) is \(C^{1}\), then, \(F:X\to X\) is \(C^{1}\)-diffeomorphism._

Proof.: Assume that \(\sigma\) is Lipschitz continuous. Let \(G\in\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\), that is,

\[G=(Id_{X}+D_{N}\circ NN_{T}\circ E_{N})\circ\cdots\circ(Id_{X}+D_{N}\circ NN_{ 1}\circ E_{N}),\]

where \(\mathrm{Lip}_{X\to X}(D_{N}\circ NN_{t}\circ E_{N})\leq\delta\). It is enough to show that for all \(t=1,...,L\)

\[Id_{X}+D_{N}\circ NN_{t}\circ E_{N}:X\to X,\]

is homeomorphism. Indeed, we have for \(u,v\in X\)

\[\langle(Id_{X}+D_{N}\circ NN_{t}\circ E_{N})(u)-(Id_{X}+D_{N} \circ NN_{t}\circ E_{N})(v),u-v\rangle_{X}\] \[=\|u-v\|_{X}^{2}+\langle D_{N}\circ NN_{t}\circ E_{N}(u)-D_{N} \circ NN_{t}\circ E_{N}(v),u-v\rangle_{X}\geq(1-\delta)\|u-v\|_{X}^{2}, \tag{167}\]

that is, \(Id_{X}+D_{N}\circ NN_{t}\circ E_{N}:X\to X\) is strongly monotone. By the same argument in Lemma 1, we can show that \((Id_{X}+D_{N}\circ NN_{t}\circ E_{N}):X\to X\) is coercive. By the Minty-Browder theorem [10, Theorem 9.14-1], \((Id_{X}+D_{N}\circ NN_{t}\circ E_{N}):X\to X\) is bijective, and then its inverse exists. Denoting by \(H_{t}:=Id_{X}+D_{N}\circ NN_{t}\circ E_{N}\), we see that by substituting \(u=H^{-1}(u)\) and \(v=H^{-1}(v)\) for (167)

\[\|H_{t}^{-1}(u)-H_{t}^{-1}(v)\|_{X}^{2}\leq\frac{1}{1-\delta} \langle H_{t}\circ H_{t}^{-1}(u)-H_{t}\circ H_{t}^{-1}(v),H_{t}^{-1}(u)-H_{t} ^{-1}(v)\rangle_{X}\] \[\leq\frac{1}{1-\delta}\|u-v\|_{X}\|H_{t}^{-1}(u)-H_{t}^{-1}(v)\| _{X},\]

which means that its inverse is continuous. Therefore, \((Id_{X}+D_{N}\circ NN_{t}\circ E_{N}):X\to X\) is homeomorphism.

For the second statement, we assume that \(\sigma\) is \(C^{1}\). By the same argument in Lemma 1, we can show that the derivative \(DH_{t}|_{u}:X\to X\) at \(u\in X\) is given by

\[DH_{t}|_{u}=I_{X}+D_{N}\circ D(NN_{t})|_{E_{N}(u)}\circ E_{N},\]

and it is injective. Since \(D_{N}\circ D(NN_{t})|_{E_{N}(u)}\circ E_{N}:X\to X\) is a finite dimensional linear operator, it is compact operator. Then by the Fredholm theorem, \(DH_{t}|_{u}:X\to X\) is bijective. By the inverse function theorem, the inverse \(H_{t}^{-1}:X\to X\) is \(C^{1}\), which implies that \(H_{t}:X\to X\) is \(C^{1}\)-diffeomorphism.

In what follow, we employ \(\sigma\) as the GroupSort activation having a group size of 2 (see [3, Section 4]). As sort activation is \(1\)-Lipschitz, from Lemma 9, \(\mathcal{R}^{inv}_{L,N,\varphi,\delta,\sigma}(X)\) is a subset of the class of homeomorphisms. We finally show that, in this case, \(\mathcal{R}^{inv}_{L,N,\varphi,\delta,\sigma}(X)\) is an universal approximator for the class of the strongly monotone diffeomorphisms with compact support.

**Theorem 7**.: _Let \(R>0\), and let \(F\colon X\to X\) be strongly monotone \(C^{1}\)-diffeomorphism with compact support, and let \(\sigma\) be the GroupSort activation having a group size of 2. Then, for any orthonormal basis \(\{\varphi_{n}\}_{n\in\mathbb{N}}\subset X\), \(\delta\in(0,1)\), and \(\epsilon\in(0,1)\), there are \(T,N\in\mathbb{N}\), and \(G\in\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\) such that_

\[\sup_{u\in\overline{B}_{X}(0,R)}\|F(u)-G(u)\|_{X}\leq\epsilon.\]

_Moreover, the inverse \(G^{-1}:X\to X\) of \(G\in\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\) is given by_

\[G^{-1}=\tilde{L}_{1}\circ\cdots\circ\tilde{L}_{T-1}\circ\tilde{L}_{T}, \tag{168}\]

_where \(\tilde{L}_{t}:X\to X\) is defined by_

\[L_{t}(y):=\lim_{n\to\infty}\pi_{1}\circ\left(\bigcirc_{h=1}^{n}\tilde{\phi}_{ t}\right)\circ e_{1}(y),\]

_where \(\tilde{\phi}_{t}:X\times X\to X\times X\) is defined by_

\[\tilde{\phi}_{t}(x,y)=(y+D_{N}\circ NN_{t}\circ E_{N}(x),y),\]

_where \(\pi_{1}(x,y)=x\) and \(e_{1}(y)=(0,y)\)._

Proof.: We denote by \(\operatorname{Diff}^{1}(X)\) the class of \(C^{1}\)-diffeomorphisms between \(X\), and \(\operatorname{Diff}^{1}_{sm}(X)\) the class of strongly monotone \(C^{1}\)-diffeomorphisms between \(X\). We also denote the support of \(F\in\operatorname{Diff}^{1}(X)\) by \(\operatorname{supp}(F):=\overline{\{x\in X:F(x)\neq x\}}\). We say that \(F\in\operatorname{Diff}^{1}_{sm,c}(X)\) if \(F\in\operatorname{Diff}^{1}_{sm}(X)\) has a compact support.

Let \(F\in\operatorname{Diff}^{1}_{c,sm}(X)\). We define \(F_{1}:X\to X\) by \(F_{1}:=Id_{X}+P_{V_{N}}(F-Id_{X})P_{V_{N}}\), and we see that

\[F_{1}=Id_{X}+P_{V_{N}}(F-Id_{X})P_{V_{N}}=P_{V_{N}^{\perp}}+P_{V_{N}}FP_{V_{N }}=P_{V_{N}^{\perp}}+D_{N}E_{N}FD_{N}E_{N}.\]

We can show that for large \(N\in\mathbb{N}\)

\[\sup_{u\in\overline{B}_{X}(0,R)}\|F(u)-F_{1}(u)\|_{X}\leq\sup_{u\in\overline{ B}_{X}(0,R)}\|P_{V_{N}}(Id_{X}-F)P_{V_{N}}(u)\|_{X}\leq\epsilon, \tag{169}\]

as \(Id_{X}-F:X\to X\) is a compact mapping. By the same argument in Lemma 1, we can show that \(F_{N}:=E_{N}FD_{N}\in\operatorname{Diff}^{1}_{sm,c}(\mathbb{R}^{N})\), and \(DF_{N}|_{0}\) is a positive definite matrix, which is connected to \(Id_{\mathbb{R}^{N}}\). By the similar argument in the proof of Theorem 4, see (105)-(152), we can construct continuous paths \(f:[0,1]\to\operatorname{Diff}^{1}_{sm,c}(\mathbb{R}^{N})\) and \(\beta:[0,1]\to GL(\mathbb{R}^{N})\) with \(f_{0}=DF_{N}|_{0}\), \(f_{1}=F_{N}\), \(\beta_{0}=Id_{\mathbb{R}^{N}}\), and \(\beta_{1}=DF_{N}|_{0}\) such that

\[F_{N} =(f_{1}\circ f_{t_{m}}^{-1})\circ(f_{t_{m}}\circ f_{t_{m-1}}^{-1}) \circ\cdots\] \[\cdots\circ(f_{t_{1}}\circ f_{0}^{-1})\circ(\beta_{1}\circ\beta_{ s_{m}}^{-1})\circ(\beta_{s_{m}}\circ\beta_{s_{m-1}}^{-1})\circ\cdots\circ(\beta_{s _{1}}\circ\beta_{0}^{-1}), \tag{170}\]

where \(t_{j}\) and \(s_{j}\) are chosen so that \(0=t_{0}<t_{1}<\ldots t_{m}<t_{m+1}=1\) and \(0=s_{0}<s_{1}\leq\ldots s_{m}<s_{m+1}=1\) and that the Lipschitz constant of the maps \(f_{t_{j}}\circ f_{t_{j-1}}^{-1}-Id_{\mathbb{R}^{N}}\) and \(\beta_{s_{j}}\circ\beta_{j-1}^{-1}-Id_{\mathbb{R}^{N}}\) are less that \(\delta\).

Then, there is \(T\in\mathbb{N}\) such that \(F_{N}\) has the form

\[F_{N}=(Id_{\mathbb{R}^{N}}-H_{T})\circ(Id_{\mathbb{R}^{N}}-H_{T-1})\circ\cdots \circ(Id_{\mathbb{R}^{N}}-H_{1}),\]

where for each \(t=1,...,T\),

\[\operatorname{Lip}_{\mathbb{R}^{N}\to\mathbb{R}^{N}}(H_{t})\leq\delta.\]

Then, remarking that \(E_{N}D_{N}=Id_{\mathbb{R}^{N}}\) and \(D_{N}E_{N}=P_{V_{N}}\), we see that

\[F_{1} =P_{V_{N}^{\perp}}+D_{N}(Id_{\mathbb{R}^{N}}-H_{L})\circ\cdots\circ (Id_{\mathbb{R}^{N}}-H_{1})E_{N}\] \[=(Id_{X}-D_{N}\circ H_{T}\circ E_{N})\circ\cdots\circ(Id_{X}-D_{N }\circ H_{1}\circ E_{N}).\]For each \(t=1,...,T\), by using [3, Theorem 3 and Observation], \(\delta\)-Lipschitz function \(H_{t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\) can be approximated by a neural network \(NN_{t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\) with GroupSort activation \(\sigma\) having a group size of 2 in \(L^{\infty}\)-norm on any compact set, and \(NN_{t}:\mathbb{R}^{N}\to\mathbb{R}^{N}\) is \(\delta\)-Lipschitz continuous.

We denoting by

\[G:=(Id_{X}-D_{N}\circ NN_{T}\circ E_{N})\circ\cdots\circ(Id_{X}-D_{N}\circ NN_{ 1}\circ E_{N})\in\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X).\]

Note that \(\operatorname{Lip}_{X\to X}(D_{N}\circ NN_{\ell}\circ E_{N})\leq\delta\). Then, we can show that, by similar way in the first half of proof of Theorem 5,

\[\sup_{u\in\overline{B}_{X}(0,R)}\|F_{1}(u)-G(u)\|_{X}\leq\epsilon. \tag{171}\]

With (169) and (171), we obtain that

\[\sup_{u\in\overline{B}_{X}(0,R)}\|F(u)-G(u)\|_{X}\leq\sup_{u\in\overline{B}_{ X}(0,R)}\|F(u)-F_{1}(u)\|_{X}+\sup_{u\in\overline{B}_{X}(0,R)}\|F_{1}(u)-G(u)\|_{X} \leq 2\epsilon.\]

The representation of inverse \(G^{-1}\) can be given by the same argument in the proof of Theorem 5. 

Similarly in Corollary 1, Theorem 7 and Lemmas 10 and 11 have the following corollary.

**Corollary 2**.: _Under the same setting and assumptions with Corollary 1, let \(\mathcal{R}\mathcal{N}\mathcal{O}^{inv}_{T,N,\varphi,\delta,\sigma}(L^{2}(D; \mathbb{R}))\) be the class of invertible residual neural operators defined in (180). Then, the statement replacing \(X\) with \(L^{2}(D;\mathbb{R})\) and \(G\in\mathcal{R}^{inv}_{T,N,\varphi,\delta,\sigma}(X)\) with \(G\in\mathcal{R}\mathcal{N}\mathcal{O}^{inv}_{T,N,\varphi,\delta,\sigma}(L^{2}( D;\mathbb{R}))\) in Theorem 7 holds._

## Appendix C Neural operators

### Examples of generalized neural operators

In the main text, we defined generalized neural operators on Hilbert spaces. Here, we give several examples to show that they are extensions of classical neural operators [33, 30].

**Example 1**.: _Let \(X\) be a Hilbert space. Let \(G_{\ell}:X\to X\) be a classical neural operator having the form_

\[G_{\ell}:=(W_{T_{\ell}}+K_{T_{\ell}})\circ\sigma(W_{T_{\ell}-1}+K_{T_{\ell}-1} )\circ\cdots\sigma(W_{1}+K_{1}),\]

_where \(W_{\ell}:X\to X\) are linear bounded operators corresponding to the local term and \(K_{\ell}:X\to X\) are compact operators corresponding to the non-local term (e.g., integral operators having a smooth kernel or smooth basis). We assume that activation function \(\sigma\) is \(C^{1}\). Then, we have \(G_{\ell}\in C^{1}(X;X)\). Let \(T_{1}=K_{lift}:X\to X\) and \(T_{2}=K_{proj}:X\to X\) be compact linear operators, which corresponds to lifting and projection, respectively. We denoting by_

\[F_{\ell}:=I+T_{1}\circ G_{\ell}\circ T_{2},\]

_which is one block of classical neural operators with skip-connection. We also denote by \(A_{\ell}=Id\) and \(\sigma=Id\). Then, the generalized neural operator \(H=F_{L}\circ\cdots\circ F_{1}\) corresponds to classical neural operators with skip-connections. In this paper, the skip-connection (ie., the structure of the identity plus some compact mapping) is so important to preserve bijectivity, discussed in Section 3._

**Example 2**.: _We show that classical neural operators, for example_

\[F_{clas}:u\mapsto(W_{2}+K_{2})\circ(\sigma(W_{1}u+K_{1}(u)),\]

_see [30, 33], can be written in the form of the generalized neural operators that we consider of the form_

\[H(x)=A_{k}\circ\sigma\circ F_{k}\circ A_{k-1}\circ\sigma\circ F_{k-1}\circ \cdots\circ A_{1}\circ\sigma\circ F_{1}\circ A_{0},\]

_where \(A_{j}:X\to X\) are linear operators which may not be bijective and \(F_{j}=Id+T_{j,1}\circ G\circ T_{j,2}:X\to X\)._

_We could start with the observations that for an infinite dimensional Hilbert space \(X\) there is a linear isomorphism \(J_{m,n}:X^{n}\to X^{m}\), where \(X^{m}=X\times\cdots\times X\). The reason for this is that the cardinality of Hilbert basis of the space \(X\) is the same as the cardinality of the Hilbert basis of \(X^{n}\), see [26, Theorem 3.5].__First we observe that we can write an operator_

\[H(x)=\tilde{A}_{k}\circ\tilde{\sigma}_{k}\circ\tilde{F}_{k}\circ\tilde{A}_{k-1} \circ\tilde{\sigma}_{k-1}\circ\tilde{F}_{k-1}\circ\cdots\circ\tilde{A}_{1}\circ \tilde{\sigma}\circ\tilde{F}_{1}\circ\tilde{A}_{0},\]

_where \(\tilde{A}_{j}:X^{n_{j}}\to X^{n_{j+1}}\) are linear operators which may not be bijective and \(\tilde{F}_{j}=Id+\tilde{T}_{j,1}\circ\tilde{G}\circ\tilde{T}_{j,2}:X^{n_{j}} \to X^{n_{j}}\) and \(\tilde{\sigma}_{k}:X^{n_{k}}\to X^{n_{k}}\) is \(\tilde{\sigma}(x_{1},x_{2},\ldots,x_{i_{k}},x_{i_{k}+1},\ldots,x_{n_{k}})=(x_ {1},x_{2},\ldots,x_{i_{k}},\sigma(x_{i_{k}+1}),\ldots,\sigma(x_{n_{k}}))\) in the form_

\[H(x)=(J_{1,n_{k+1}}\circ\tilde{A}_{k}\circ J_{n_{k},1})\circ(J_{1,n_{k}}\circ \tilde{\sigma}\circ\tilde{F}_{k}\circ J_{n_{k},1})\circ(J_{1,n_{k}}\circ\tilde {A}_{k-1}\circ J_{n_{k-1},1})\circ\cdots\circ(J_{1,n_{1}}\circ\tilde{A}_{0} \circ J_{n_{0},1}),\]

_where e.g. \((J_{1,n_{k+1}}\circ A_{k}\circ J_{n_{k},1}):X\to X\) and \((J_{1,n_{k}}\circ\tilde{\sigma}_{k}\circ\tilde{F}_{k}\circ J_{n_{k},1}):X\to X\). This means that in our formalism we can replace e.g. operators \(A_{k}\) by matrices of operators \(A_{k}\)._

_Next we go to the second step of the construction:_

_As an example, let us consider the classical neural operator_

\[F_{clas}:u\to(W_{2}+K_{2})\circ\sigma\circ(W_{1}u+K_{1}(u)),\]

_where \(W_{1}\) and \(W_{2}\) are invertible matrices and \(K_{1}\) and \(K_{2}\) are compact operators, can be written as an generalized neural operator_

\[u\to L_{4}\circ F_{3}\circ L_{2}\circ\tilde{\sigma}\circ L_{1}\circ F_{0}(u),\]

_where \(F_{0}:X\to X\) is a layer of neural operator_

\[F_{0}(v)=v+W_{1}^{-1}K_{1}(v),\]

_and \(L_{1}:X\to X\) is the invertible linear operator_

\[L_{1}(u)=W_{1}u,\]

_and \(\tilde{\sigma}(u)=\sigma(u)\) and \(L_{2}:X\to X\times X\) is a non-invertible linear operator_

\[L_{2}(w)=(w,w),\]

_and and \(F_{3}:X\times X\to X\times X\) is a layer neural operator_

\[F_{3}(w_{1},w_{2})=(w_{1},w_{2}+W_{2}^{-1}K_{2}(w_{2})),\]

_and \(L_{4}:X\times X\to X\) is the non-invertible linear operator_

\[L_{4}(u_{1},u_{2})=W_{2}(u_{2}-u_{1}).\]

_Finally, we point out that if a non-invertible activation function \(\sigma:X\to X\) satisfies \(Lip(\sigma)\leq\lambda\), it can be written as_

\[u\to L_{2}\circ\hat{\sigma}\circ L_{1}(u),\]

_where_

\[L_{1}:u\to(u,u),\]

_and \(\hat{\sigma}\) is an invertible function_

\[\hat{\sigma}:(u_{1},u_{2})\to(u_{1},2\lambda u_{2}+\sigma(u_{2})),\]

_and_

\[L_{1}:(w_{1},w_{2})\to w_{2}-2\lambda w_{1},\]

_Note that equation \(2\lambda u+\sigma(u)=w\) can be written as a fixed point equation_

\[u=g_{w}(u):=(2\lambda)^{-1}w-(2\lambda)^{-1}\sigma(u),\]

_where \(\text{Lip}(g_{w})\leq 1/2\). Hence, \(g_{w}:X\to X\) is a contraction and the equation \(u=g_{w}(u)\) has a unique solution for all \(w\) by Banach fixed point theorem. Hence, \(u\to 2\lambda u+\sigma(u)\) is invertible._

**Example 3**.: _Let \(D\subset\mathbb{R}^{d}\) be a domain, and let \(L^{2}(D;\mathbb{R})\) be the real-valued \(L^{2}\)-function space on \(D\), and let \(\varphi=\{\varphi_{n}\}_{n\in\mathbb{N}}\subset L^{2}(D;\mathbb{R})\) be an orthonormal basis in \(L^{2}(D;\mathbb{R})\). We consider the case when \(X=L^{2}(D;\mathbb{R}^{h})=L^{2}(D;\mathbb{R})^{h}\), and \(A_{j}=W^{(j)}\) where \(W^{(j)}\in\mathbb{R}^{h\times h}\) are invertible matrices, and \(F_{j}=Id+P_{V^{h}_{N}}\circ K^{(j)}_{N}\circ P_{V^{h}_{N}}W^{(j)-1})\) (corresponding to \(T_{1}=P_{V^{h}_{N}}\), \(T_{2}=P_{V^{h}_{N}}W^{(j)-1}\), \(G=K^{(j)}_{N}\))_where \(V_{N}:=\mathrm{span}\{\varphi_{n}\}_{n\leq N}\), and \(K_{N}^{(j)}:L^{2}(D;\mathbb{R})^{h}\to L^{2}(D;\mathbb{R})^{h}\) is a finite rank operator defined by_

\[K_{j}(u)(x):=\sum_{p,q\leq N}K_{p,q}^{(j)}\langle u,\varphi_{p}\rangle_{L^{2}(D; \mathbb{R})}\varphi_{q}(x),\quad x\in D,\]

_Then, \(H:L^{2}(D;\mathbb{R})^{h}\to L^{2}(D;\mathbb{R})^{h}\) can be written by_

\[H=(W^{(k)}+K^{(k)})\circ\sigma\circ(W^{(k-1)}+K^{(k-1)})\circ\sigma\circ\cdots \circ(W^{(1)}+K^{(1)})\circ\sigma\circ(W^{(0)}+K^{(0)}),\]

_which coincides with classical neural operators (assuming that all local weight matrices are invertible) used in e.g., [33]. See Definition 9 as well._

**Example 4**.: _Let us consider an example of an example of an generalized neural operator which is obtained by composition of non-linear integral operators and smooth activation functions. Let \(D\subset\mathbb{R}^{d}\) be be a bounded domain with a smooth boundary. We consider the case when \(X=H^{1}(D;\mathbb{R}^{h})=H^{1}(D;\mathbb{R})^{h}\) are Sobolev spaces, and \(A_{j}=W^{(j)}\) where \(W^{(j)}\in\mathbb{R}^{h\times h}\) are invertible matrices, and \(F_{j}=Id_{H^{1}}+iH_{{}^{2}\to H^{1}}\circ\tilde{K}^{(j)}\circ i_{H^{1}\to L^{ 2}}W^{(j)-1}\) (corresponding to \(T_{1}=iH_{{}^{1}\to L^{2}}\), \(T_{2}=iH_{{}^{2}\to H^{1}}(W^{(j)})^{-1}\), \(G=\tilde{K}^{(j)}\)) where \(\tilde{K}^{(j)}:L^{2}(D;\mathbb{R})^{h}\to H^{1}(D;\mathbb{R})^{h}\) is non-linear integral operator_

\[\tilde{K}^{(j)}(u)(x):=\int_{D}k^{(j)}(x,y,u(y))u(y)dy,\quad x\in D,\]

_where kernel satisfies \(k^{(j)}\in C^{3}(\overline{D}\times\overline{D}\times\mathbb{R}^{h};\mathbb{ R}^{h\times h})\) has uniformly bounded three derivatives. Also, let \(\sigma\in C^{1}(\mathbb{R})\) be an activation function which derivative is uniformly bounded and we denote \(\sigma_{*}f=\sigma\circ f\). Then, \(H:H^{1}(D;\mathbb{R})^{h}\to H^{1}(D;\mathbb{R})^{h}\), defined by_

\[H = (W^{(k)}+iH_{{}^{2}\to H^{1}}\circ\tilde{K}^{(j)}\circ i_{H^{1}\to L ^{2}})\circ\sigma_{*}\circ(W^{(k-1)}+iH_{{}^{2}\to H^{1}}\circ\tilde{K}^{(j)} \circ i_{H^{1}\to L^{2}})\circ\sigma_{*}\] \[\circ\cdots\circ(W^{(1)}+K^{(1)})\circ\sigma_{*}\circ(W^{(0)}+iH_{ {}^{2}\to H^{1}}\circ\tilde{K}^{(j)}\circ i_{H^{1}\to L^{2}}),\]

_is an generalized neural operator. Here, \(iH_{{}^{1}\to L^{2}}\) and \(iH_{{}^{2}\to H^{1}}\) are compact emending from \(H^{1}(D)\) to \(L^{2}(D)\) and \(H^{2}(D)\) to \(H^{1}(D)\), respectively. Non-linear integral operator in neural operator has been used in [30]._

**Example 5**.: _Let us consider an example of a typical neural operator which is a finite composition of layers of neural operators similar to those introduced in [30, 33], \(F:X\to X\) of the form_

\[F:u\rightarrow\sigma\circ(Wu+T_{2}(G(T_{1}u))), \tag{172}\]

_where \(X=H^{m}(\Omega)\), \(\Omega\subset\mathbb{R}^{d}\) is a bounded set with a smooth boundary and \(W:X\to X\) is a linear operator. In the above, \(Y=C(\overline{\Omega})\) and \(Z=C^{m+1}(\overline{\Omega})\), where \(m>d/2\). Moreover, \(G:Y\to Z\) is a nonlinear integral operator_

\[G(u)(x)=\int_{\Omega}k_{\theta}(x,y,u(y))u(y)dy,\]

_where \(k_{\theta},\partial_{u}k_{\theta}\in C^{m+1}(\overline{\Omega}\times\overline {\Omega}\times\mathbb{R})\) is a kernel given by a feed-forward neural network with sufficiently smooth activation functions. Here, we assume that \(\sigma_{j}\in C^{\ell}(\mathbb{R})\), \(\ell\geq m+2\) and that the kernel \(k_{\theta}\) is of the form_

\[k_{\theta}(x,y,t)=\sum_{j=1}^{J}c_{j}(x,y,\theta)\sigma_{j}(a_{j}(x,y,\theta)t +b_{j}(x,y,\theta)).\]

_Moreover, \(T_{1}:X\to Y\) and \(T_{2}:Z\to X\) are the identical embedding operators_

\[T_{j}(u)=u.\]

_Due to the choice of function spaces \(X,Y\) and \(Z\), the maps \(T_{1}\) and \(T_{2}\) are compact operators. Summarizing, \(F=F_{\sigma,W,k}\), where_

\[F_{\sigma,W,k}(u)(x)=\sigma((Wu)(x)+\int_{\Omega}k_{\theta}(x,y,u(y))u(y)dy). \tag{173}\]_Furthermore, by choosing \(k_{\theta}(x,y,u(y))=k_{\theta}(x-y)\) and \(\Omega=\mathbb{T}^{d}\) as the convolutional kernel and the torus, the map \(F\) takes the form of an FNO [39]._

_Here the activation functions \(\sigma\) and \(\sigma_{j}\) are in different roles as the functions \(\sigma_{j}\) appear inside the integral operator (or between the compact operators \(T_{1}\) and \(T_{2}\)). The function \(\sigma\) is useful in obtaining universal approximation results for neural operators, but as this question is somewhat technical, we postpone this discussion to the end._

_The appearance of the compact operators \(T_{1}\) and \(T_{2}\) makes the discretization of activation function \(\sigma\) and the activation functions inside \(G\) in Definition 4 different, and this is the reason why we have introduced both \(\sigma\) and \(G\). To consider invertible neural operators, we will below assume that \(\sigma\) is an invertible function, for example, the leaky Relu function. In the above operator, the nonlinear function \(G\) inside compact operators in the operation_

\[N:u\to u+T_{2}(G(T_{1}u)),\]

_and the compact operators map weakly converging sequences to norm converging sequences. This is essential in the proofs of the positive results for approximation functors as discussed in the paper. However, we do not have general results on how the operation_

\[G_{\sigma}:u\rightarrow\sigma\circ u,\]

_can be approximated by finite dimensional operators in the norm topology, but only in the weak topology in the sense of Definition 11 of the Weak Approximation Functor. However, one can overcome this difficulty in two ways. The first way is to use in the discretization a suitable finite dimensional space \(V\) that satisfies \(G_{\sigma}(V)\subset V\). For example, when \(X=L^{2}(\Omega)\) and \(\sigma\) is a leaky relu function, one can choose \(V\) to be a space that consists of piecewise constant functions. The second way is choosing a composition of layers of the form_

\[N_{j}:u\to W_{j}u+T_{1,j}(G_{j}(T_{2,j}u)),\]

_and_

\[G_{\sigma}:u\rightarrow\sigma\circ u,\]

_in different finite dimensional spaces \(V_{j}\). For example, we can consider these operations as maps_

\[N_{1}:V_{1}\to V_{1}, \tag{174}\] \[G_{\sigma}:V_{1}\to V_{2}:=G_{\sigma}(V_{1}),\] (175) \[N_{2}:V_{2}\to V_{2},\] (176) \[G_{\sigma}:V_{2}\to V_{3}:=G_{\sigma}(V_{2}). \tag{177}\]

_This makes the composition_

\[G_{\sigma}\circ N_{2}\circ G_{\sigma}\circ N_{1}:V_{1}\to V_{3},\]

_well defined. The maps, \(G_{\sigma}\), are clearly invertible and one can use Theorems 4 and 5 to analyze when \(N_{j}:V_{j}\to V_{j}\) are invertible functions._

_Finally, we return to the question whether the activation function \(\sigma\) is useful in universal approximation results. If the activation function \(\sigma\) is removed (that is, it is the identical map \(\sigma_{id}:s\to s\)), the operator \(F\) is a sum of a linear operator and a compact nonlinear integral operator,_

\[F_{W,k}(u)(x)=(Wu)(x)+\int_{\Omega}k_{\theta}(x,y,u(y))u(y)dy. \tag{178}\]

_Moreover, if we compose above operators \(F_{j}\) of the above form, the obtained operator, \(G:\ X\to X\) is also a sum of a linear operator and a compact operator,_

\[G(u)=\tilde{W}u+\tilde{K}(u).\]

_Moreover, the Frechet derivative of \(G\) at \(u_{0}\), denoted \(DG|_{u_{0}}\) is the map_

\[DG|_{u_{0}}:v\rightarrow\tilde{W}v+D\tilde{K}|_{u_{0}}v,\]

_and, due to the above assumptions on kernel \(k_{\theta}(x,y,u)\), the derivative is a linear operator_

\[D\tilde{K}|_{u_{0}}:H^{m}(\Omega)\to H^{m+1}(\Omega).\]_By the Sobolev embedding theorem it is a compact operator \(D\bar{K}|_{u_{0}}:X\to X\). This means that the Fredholm index of the derivative of \(DG|_{u_{0}}\) is constant_

\[\text{Ind}(DG|_{u_{0}})=\text{Ind}(W),\]

_that is, independent of the point \(u_{0}\) where the derivative is computed. In particular, this means that one cannot approximate an arbitrary \(C^{1}\)-function \(G:X\to X\) in compact subsets of \(X\) by neural operators which are compositions of layers (178). Indeed, for a general \(C^{1}\)-function \(G:X\to X\) the Fredholm index may be a varying function of \(u_{0}\). Thus, \(\sigma\) appears to be relevant for obtaining universal approximation theorems for neural operators._

### Residual neural operators

Let \(D\subset\mathbb{R}^{d}\) be a domain. In what follows, we consider the real-valued \(L^{2}\)-function space \(L^{2}(D;\mathbb{R})\)6. Let \(\varphi=\{\varphi_{n}\}_{n\in\mathbb{N}}\subset L^{2}(D;\mathbb{R})\) be an orthonormal basis in \(L^{2}(D;\mathbb{R})\).

Footnote 6: We will discuss the function space \(L^{2}(D;\mathbb{R})\) for easier reading, but all arguments can be replaced with real-valued function space \(\mathcal{U}(D;\mathbb{R})\) that is a separable Hilbert space.

**Definition 9** (Neural operators [33]).: _We define a neural operator \(G:L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R})\) by_

\[G:u_{0}\mapsto u_{L+1},\]

_where \(u_{L+1}\) is give by the following steps:_

\[u_{\ell+1}(x)=\sigma\left(W^{(\ell)}u_{\ell}(x)+(K_{N}^{(\ell)}u_{\ell})(x)+b^ {(\ell)}\right),\quad x\in D,\quad 0\leq\ell\leq L-1,\]

\[u_{L+1}(x)=W^{(L)}u_{L}(x)+(K_{N}^{(L)}u_{L})(x)+b^{(L)},\quad x\in D,\]

_where \(\sigma:\mathbb{R}\to\mathbb{R}\) is a non-linear activation operating element-wise, and \(W^{(\ell)}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}}\) and \(b^{(\ell)}\in\mathbb{R}^{d_{\ell+1}}\) and_

\[K_{N}^{(\ell)}(v)(x)=\sum_{p,q\leq N}K_{p,q}^{(\ell)}\langle v,\varphi_{p} \rangle_{L^{2}(D;\mathbb{R})}\varphi_{q}(x),\quad x\in D,\]

_where \(K_{p,q}^{(\ell)}\in\mathbb{R}^{d_{\ell+1}\times d_{\ell}}\) (\(\ell=0,...,L\), \(p,q=1,...,N\), \(d_{0}=d_{L+2}=1\)). Here, we use the notation for \(v=(v_{1},...,v_{d_{\ell}})\in L^{2}(D;\mathbb{R})^{d_{\ell}}\)_

\[\langle v,\varphi_{p}\rangle_{L^{2}(D;\mathbb{R})}=\left(\langle v_{1},\varphi _{p}\rangle_{L^{2}(D;\mathbb{R})},...,\langle v_{d_{\ell}},\varphi_{p}\rangle_ {L^{2}(D;\mathbb{R})}\right)\in\mathbb{R}^{d_{\ell}}.\]

_We denote by \(\mathcal{NO}_{L,N,\varphi,\sigma}(L^{2}(D;\mathbb{R}))\) the class of neural operators \(G:L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R})\) defined above, with depths \(L\), rank \(N\), orthonormal basis \(\varphi\), and activation function \(\sigma\)._

**Definition 10** (Residual Neural Operator).: _Let \(T,N\in\mathbb{N}\) and let \(\delta\in(0,1)\). We define by_

\[\mathcal{RN}\mathcal{O}_{T,N,\varphi,\sigma}(L^{2}(D;\mathbb{R}))\] \[:=\{G:L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R}):G=(Id_{L^{2}(D; \mathbb{R})}+G_{T})\circ\cdots\circ(Id_{L^{2}(D;\mathbb{R})}+G_{1}),\] \[G_{t}\in\mathcal{NO}_{L_{t},N,\varphi,\sigma}(L^{2}(D;\mathbb{R} )),\;L_{t}\in\mathbb{N},\;t=1,...,T\}, \tag{179}\]

\[\mathcal{RN}\mathcal{O}_{T,N,\varphi,\delta,\sigma}^{inv}(L^{2}(D;\mathbb{R}) ):=\{G:L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R}):\]

\[G=(Id_{L^{2}(D;\mathbb{R})}+G_{T})\circ\cdots\circ(Id_{L^{2}(D;\mathbb{R})}+G _{1}),G_{t}\in\mathcal{NO}_{L_{t},N,\varphi,\sigma}(L^{2}(D;\mathbb{R})),\]

\[\text{Lip}_{L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R})}(G_{t})\leq\delta,\;L_{ t}\in\mathbb{N},\;t=1,...,T\}, \tag{180}\]

**Lemma 10**.: _Let \(\delta\in(0,1)\), and let \(F\in\mathcal{RN}\mathcal{O}_{T,N,\varphi,\delta,\sigma}^{inv}(L^{2}(D;\mathbb{ R}))\). Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be Lipschitz continuous. If \(\sigma:\mathbb{R}\to\mathbb{R}\) is Lipschitz continuous, then \(F:X\to X\) is homeomorphism. Moreover, if \(\sigma:\mathbb{R}\to\mathbb{R}\) is \(C^{1}\), then, \(F:X\to X\) is \(C^{1}\)-diffeomorphism._

The proof is given by the same argument in Lemma 9.

**Lemma 11**.: _Assume that the orthonormal basis \(\varphi\) include the constant function. Then, we have the following inclusion:_

\[\mathcal{R}_{T,N,\varphi,\sigma}(L^{2}(D;\mathbb{R}))\subset \mathcal{RN}\mathcal{O}_{T,N,\varphi,\sigma}(L^{2}(D;\mathbb{R})), \tag{181}\] \[\mathcal{R}_{T,N,\varphi,\delta,\sigma}^{inv}(L^{2}(D;\mathbb{R} ))\subset\mathcal{RN}\mathcal{O}_{T,N,\varphi,\delta,\sigma}^{inv}(L^{2}(D; \mathbb{R})). \tag{182}\]Proof.: Let \(G\in\mathcal{R}_{T,N,\varphi,\sigma}(L^{2}(D;\mathbb{R}))\) such that

\[G=(Id_{L^{2}(D;\mathbb{R})}+D_{N}\circ NN_{T}\circ E_{N})\circ\cdots\circ(Id_{L^{ 2}(D;\mathbb{R})}+D_{N}\circ NN_{1}\circ E_{N}).\]

Since \(\varphi=\{\varphi_{n}\}_{n\in\mathbb{N}}\subset L^{2}(D;\mathbb{R})\) has the constant basis, denoting it by \(\varphi_{0}:=\frac{\mathbf{1}_{D}(x)}{\|\mathbf{1}_{D}\|_{L^{2}(D;\mathbb{R})}}\), where \(\mathbf{1}_{D}(x)=1\) for \(x\in D\), we see that for \(\alpha=(\alpha_{1},...,\alpha_{N})\in\mathbb{R}^{N}\)

\[D_{N}\alpha(x) =\sum_{n\leq N}\alpha_{n}\varphi_{n}(x)=\sum_{n\leq N}\alpha_{n} \langle\varphi_{1},\varphi_{1}\rangle_{L^{2}(D;\mathbb{R})}\varphi_{n}(x)\] \[=\sum_{n\leq N}\frac{1}{\|\mathbf{1}_{D}\|_{L^{2}(D;\mathbb{R})} }\langle\alpha_{n}\cdot\mathbf{1}_{D},\varphi_{1}\rangle_{L^{2}(D;\mathbb{R}) }\varphi_{n}(x).\]

We define \(\tilde{D}_{N}:L^{2}(D;\mathbb{R})^{N}\to L^{2}(D;\mathbb{R})\) by for \(u\in L^{2}(D;\mathbb{R})^{N}\)

\[\tilde{D}_{N}u:=\sum_{p,q\leq N}\tilde{D}_{p,q}\langle u,\varphi_{p}\rangle_{L ^{2}(D;\mathbb{R})}\varphi_{q},\]

where \(\tilde{D}_{p,q}\in\mathbb{R}^{1\times N}\) is defined by

\[\tilde{D}_{p,q}=\left\{\begin{array}{ll}\Big{(}0,...,0,\underbrace{\frac{1}{ \|\mathbf{1}_{D}\|_{L^{2}(D;\mathbb{R})}}}_{q-th},0,...,0\Big{)},&p=1\\ O,&p\neq 1.\end{array}\right.\]

Then, we have that

\[D_{N}\circ NN_{t}\circ E_{N}(u)=\tilde{D}_{N}\left(NN_{t}\circ E_{N}(u)\cdot \mathbf{1}_{D}\right). \tag{183}\]

Next, we see that

\[E_{N}(u)=\left(\langle u,\varphi_{1}\rangle_{L^{2}(D;\mathbb{R})},...,\langle u,\varphi_{N}\rangle_{L^{2}(D;\mathbb{R})}\right)=\|\mathbf{1}\|_{L^{2}(D; \mathbb{R})}\left(\langle u,\varphi_{1}\rangle_{L^{2}(D;\mathbb{R})},..., \langle u,\varphi_{N}\rangle_{L^{2}(D;\mathbb{R})}\right)\varphi_{1}(x),\]

We define \(\tilde{E}_{N}:L^{2}(D;\mathbb{R})\to L^{2}(D;\mathbb{R})^{N}\) by

\[\tilde{E}_{N}(u):=\sum_{p,q\leq N}\tilde{E}_{p,q}\langle u,\varphi_{p}\rangle_ {L^{2}(D;\mathbb{R})}\varphi_{q}(x),\]

where \(\tilde{E}_{p,q}\in\mathbb{R}^{N\times 1}\) is defined by

\[\tilde{E}_{p,q}=\left\{\begin{array}{ll}\Big{(}0,...,0,\underbrace{\langle \mathbf{1}_{D},\varphi_{n}\rangle_{L^{2}(D;\mathbb{R})}}_{p-th},0,...,0\Big{)}^ {T},&q=1\\ O,&q\neq 1.\end{array}\right.\]

Then we have that

\[\tilde{D}_{N}\left(NN_{t}\circ E_{N}(u)\cdot\mathbf{1}_{D}\right)=\tilde{D}_{ N}\circ NN_{t}\circ\tilde{E}_{N}(u). \tag{184}\]

With (183) and (184), we see that

\[D_{N}\circ NN_{t}\circ E_{N}=\tilde{D}_{N}\circ NN_{t}\circ\tilde{E}_{N}\in \mathcal{NO}_{L_{t},N,\varphi,\sigma}(L^{2}(D;\mathbb{R})),\]

where \(L_{t}\in\mathbb{N}\) is the depth of \(NN_{t}\). Therefore, \(G\) has the form

\[G=(Id_{L^{2}(D;\mathbb{R})}+\tilde{D}_{N}\circ NN_{T}\circ\tilde{E}_{N})\circ \cdots\circ(Id_{L^{2}(D;\mathbb{R})}+\tilde{D}_{N}\circ NN_{1}\circ\tilde{E}_{N })\in\mathcal{RNO}_{T,N,\varphi,\sigma}(L^{2}(D;\mathbb{R})).\]

(182) can be proved by the same argument. 

## Appendix D Generalizations

### Generalization of the no-go Theorem 2 using weak topology

We can generalize the no-go Theorem 2 for the case when approximations and continuity of approximations are considered in the weak topology of the Hilbert space \(X\). This can be done when the approximations of the identity map and the minus one times the identity operator satisfy additional assumptions and the partially ordered subset \(S_{0}(X)\) is the set of all all linear subspaces \(S(X)\) of \(X\). In the case when \(S_{0}(X)=S(X)\) and Definition 7 the condition (A) can generalized as follows:

**Definition 11** (Weak Approximation Functor).: _When \(S_{0}(X)=S(X)\) we define the weak approximation functor, that we denote by \(\mathcal{A}\colon\mathcal{D}\to\mathcal{B}\), as the functor that maps each \((X,F)\in\mathcal{O}_{\mathcal{D}}\) to some \((X,S(X),(F_{V})_{V\in S(X)})\in\mathcal{O}_{\mathcal{B}}\) so that the Hilbert space \(X\) stays the same. The approximation functor maps all morphisms \(a_{\phi}\) to \(A_{\phi}\) and morphisms \(a_{X_{1},X_{2}}\) to \(A_{X_{1},X_{2}}\), and has the following properties_

* _For all_ \(r>0\)_, all_ \((X,F)\in\mathcal{O}_{\mathcal{D}}\) _and all_ \(y\in X\)_, it holds that_ \[\lim_{V\to X}\sup_{x\in\mathcal{B}_{X}(0,r)\cap V}\langle F_{V}(x)-F(x),y\rangle _{X}=0.\] (185) _Moreover, when_ \(F:X\to X\) _is the operator_ \(Id:X\to X\) _or_ \(-Id:X\to X\)_, then_ \(F_{V}\) _is the operator_ \(Id_{V}:V\to V\) _or_ \(-Id_{V}:V\to V\)_, respectively._

Moreover, Definition 8 can generalized as follows so that it uses the weak topology.

**Definition 12**.: _We say that the approximation functor \(\mathcal{A}\) is continuous in the weak topology if the following holds: Let \((X,F),(X,F^{(j)})\in\mathcal{O}_{\mathcal{D}}\) be such that the Hilbert space \(X\) is the same for all these objects and let \((X,S(X),(F_{V})_{V\in S(X)})=\mathcal{A}(X,F)\) be approximating sequences of \((X,F)\) and \((X,S(X),(F_{j,V})_{V\in S(X)})=\mathcal{A}(X,F^{(j)})\) be approximating sequences of \((X,F^{(j)})\). Moreover, assume that for \(r>0\) and all \(y\in X\)_

\[\lim_{j\to\infty}\sup_{x\in\mathcal{B}_{X}(0,r)}|\langle F^{(j)}(x)-F(x),y \rangle_{X}|=0. \tag{186}\]

_Then, for all \(V\in S(X)\) the approximations \(F^{(j)}_{V}\) of \(F^{(j)}\) and \(F_{V}\) of \(F\) satisfy for all \(y\in X\)_

\[\lim_{j\to\infty}\sup_{x\in V\cap\mathcal{B}_{V}(0,r)}|\langle F^{(j)}_{V}(x)- F_{V}(x),y\rangle_{X}|=0. \tag{187}\]

The theorem below states a negative result, namely that there does not exist continuous approximating functors for diffeomorphisms.

**Theorem 8**.: _(No-go theorem for discretization of general diffeomorphisms) There exists no functor \(\mathcal{D}\to\mathcal{B}\) that satisfies the property (A') of a weak approximation functor and is continuous in the weak topology._

The proof of Theorem 8 is analogous to Theorem 2 by replacing \(A_{1}\) by the map \(-Id\) and considering a linear space \(V\in S(X)\) having an odd dimension, in which case \(\deg(A_{1}:V\to V)=-1\). Let \(F_{0}=Id:X\to X\) and \(F_{1}=-Id:X\to X\). Assume that \((X,S(X),(F_{t,V})_{V\in S(X)})=\mathcal{A}(X,F_{t})\) are approximations of the map \(F\colon X\to X\) in the weak topology and that \(\mathcal{A}\) is continuous in the weak topology. Recall that then \(F_{t,V}:V\to V\) are \(C^{1}\)-diffeomorphisms that are discretizations of \(F\colon X\to X\). Then, by condition (A'), \(F_{0,V}=Id_{V}\) and \(F_{1,V}=-Id_{V}\) so that \(\deg(F_{0,V})=1\) and \(\deg(F_{1,V})=-1\). Observing that when the condition (187) is applied for \(y_{1},y_{2},\ldots,y_{n}\in V\) that form a basis of the space \(V\) having the dimension \(\dim(V)=n\), we see that the condition (187) implies the condition (7). Using these observations, Theorem 8 is follows similarly to the proof of Theorem 2.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction accurately summarize and reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [NA] Justification: Our paper is theoretical. It does not have limitations in the same was as a more applied paper. All theorems and propositions are proved to be true, and so are not limited in their scope. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The statements of all theoretical statements are given in the main body, and all of their proofs are included in the attached appendix. A reference to the proof of each statement is given either immediately before, or immediately after each statement. When appropriate, a proof sketch is given. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our paper does not have any numerical experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Our paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Our paper does not include experiments Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Our paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Our paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We follow NeurIPS code of ethics Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is fundamental. Our work is not tied to a particular application. To our knowledge, there are no direct paths to any negative applications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work is theoretical, does not release data or models. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our work is theoretical and does not contain owners of outside assets, like code, data or models. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.