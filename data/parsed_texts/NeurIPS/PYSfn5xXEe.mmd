# ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning

 Mingyu Xu\({}^{1,2}\), Zheng Lian\({}^{1}\)1, Lei Feng\({}^{3}\), Bin Liu\({}^{1,2}\), Jianhua Tao\({}^{4,5}\)

\({}^{1}\)The State Key Laboratory of Multimodal Artificial Intelligence Systems,

Institute of Automation, Chinese Academy of Sciences

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)School of Computer Science and Engineering, Nanyang Technological University

\({}^{4}\)Department of Automation, Tsinghua University

\({}^{5}\)Beijing National Research Center for Information Science and Technology, Tsinghua University

{xumingyu2021, lianzheng2016}@ia.ac.cn

Equal Contribution

###### Abstract

Noisy partial label learning (noisy PLL) is an important branch of weakly supervised learning. Unlike PLL where the ground-truth label must conceal in the candidate label set, noisy PLL relaxes this constraint and allows the ground-truth label may not be in the candidate label set. To address this challenging problem, most of the existing works attempt to detect noisy samples and estimate the ground-truth label for each noisy sample. However, detection errors are unavoidable. These errors can accumulate during training and continuously affect model optimization. To this end, we propose a novel framework for noisy PLL with theoretical interpretations, called "Adjusting Label Importance Mechanism (ALIM)". It aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. ALIM is a plug-in strategy that can be integrated with existing PLL approaches. Experimental results on multiple benchmark datasets demonstrate that our method can achieve state-of-the-art performance on noisy PLL. Our code is available at: https://github.com/zeroQiaoba/ALIM.

## 1 Introduction

Partial label learning (PLL) [1; 2] (also called ambiguous label learning [3; 4] and superset label learning [5; 6]) is a typical type of weakly supervised learning. Unlike supervised learning where each sample is associated with a ground-truth label, PLL needs to identify the ground-truth label from a set of candidate labels. Due to the low annotation cost of partially labeled samples, PLL has attracted increasing attention from researchers and has been applied to many areas, such as object recognition [4], web mining [7], and ecological informatics [8].

The basic assumption of PLL is that the ground-truth label must be in the candidate label set [9]. However, this assumption may not be satisfied due to the unprofessional judgment of annotators [10]. Recently, some researchers have relaxed this assumption and focused on a more practical setup called noisy PLL [11]. In noisy PLL, the ground-truth label may not be in the candidate label set. To deal with this task, Lv et al. [11] utilized noise-tolerant loss functions to avoid overemphasizing noisy samples. However, they cannot fully exploit the useful information in noisy data. To this end, Lian et al. [12] and Wang et al. [13] proposed to detect noisy samples and estimate pseudo labels for these samples. However, detection errors are unavoidable. These errors will accumulate and continuously affect model optimization, thereby limiting their performance on noisy PLL.

To alleviate this problem, we propose a novel framework for noisy PLL called "Adjusting Label Importance Mechanism (ALIM)". Although we may make mistakes in noisy sample detection, it allows us to leverage the initial candidate set and restart correction. To reduce manual efforts in hyperparameter tuning, we propose an adaptive strategy to determine the weighting coefficient for ALIM. To further improve noise tolerance, we equip ALIM with mixup training [14], a powerful technique in noisy label learning [15]. We also perform theoretical analysis from the perspective of objective functions and EM algorithms and prove the feasibility of our method. To verify its effectiveness, we conduct experiments on multiple benchmark datasets. Experimental results demonstrate that ALIM outperforms currently advanced approaches, setting new state-of-the-art records. The main contributions of this paper can be summarized as follows:

* We propose a plug-in framework for noisy PLL with theoretical interpretations. Our method can deal with noisy samples by trading off the initial candidate set and model outputs.
* We propose an adaptive strategy to adjust the weighting coefficient for ALIM. Furthermore, we combine our method with the mixup training to improve noise robustness.
* Experimental results on multiple datasets demonstrate the effectiveness of our method. ALIM is superior to currently advanced approaches on noisy PLL.

## 2 Methodology

### Problem Definition

Let \(\mathcal{X}\) be the input space and \(\mathcal{Y}\) be the label space with \(c\) distinct categories. We consider a dataset \(\mathcal{D}\) containing \(N\) partially labeled samples \(\{(x,S(x))\}\). Here, \(S(x)\in\{0,1\}^{c}\) is the candidate set for the sample \(x\in\mathcal{X}\). We denote the \(i^{th}\) element of \(S(x)\) as \(S_{i}(x)\), which is equal to 1 if the label \(i\) is in the candidate set, and 0 otherwise. Meanwhile, we denote \(P(x)\in\mathbb{R}^{c}\), \(w(x)\in\mathbb{R}^{c}\), and \(y(x)\in\mathbb{R}^{1}\) as the prediction probabilities, the pseudo label, and the ground-truth label of the sample \(x\). The \(i^{th}\) elements of \(P(x)\) and \(w(x)\) are represented as \(P_{i}(x)\) and \(w_{i}(x)\), respectively. In this paper, all product operations between matrices are Hadamard products.

The goal of noisy PLL is to learn a classifier that can minimize the classification risk on \(\mathcal{D}\). Unlike PLL, noisy PLL allows the ground-truth label may not be in the candidate set. For a fair comparison, we adopt the same data generation procedure as previous works [12]. Specifically, to generate candidate labels, we first flip incorrect labels into false positive labels with a probability \(q\) and aggregate the flipped ones with the ground-truth label. Then, we assume that each sample has a probability \(\eta\) of being the noisy sample. For each noisy sample, we further select a label from the non-candidate set, move it into the candidate set, and move the ground-truth label out of the candidate set. We denote the probability \(q\) as the ambiguity level and the probability \(\eta\) as the noise level.

Figure 1: The core structure of ALIM. The network receives an input \(x\) and produces softmax prediction probabilities \(P(x)\). Different from traditional PLL that fully trusts the candidate set, our method can deal with noisy samples through a weighting mechanism.

### Motivation

First, let us review our own exam experience. When we are unfamiliar with a test, we believe that the correct answer must be in the candidate set. Even if every option is wrong, we still choose the most likely answer. But as we become more familiar with the test, we learn to question the correctness of the candidate set. If we believe every option is wrong, we will consider answers outside the candidate set. In noisy PLL, this strategy can also be used to handle noisy samples.

### ALIM Framework

Inspired by the above idea, we propose a simple yet effective framework called ALIM. The overall structure is shown in Figure 1. Specifically, we first predict the softmax probabilities \(P(x)\) for each sample \(x\). In traditional PLL, we fully trust the candidate set and generate the pseudo label as follows:

\[w(x)=\text{Normalize}\left(S(x)P(x)\right),\] (1)

where \(\text{Normalize}(\cdot)\) is a normalization function that ensures \(\sum_{i=1}^{c}w_{i}(x)=1\). Different from traditional PLL, ALIM introduces a coefficient \(\lambda\) to control the reliability of the candidate set:

\[\tilde{S}(x)=S(x)+\lambda\left(1-S(x)\right),\] (2)

\[w(x)=\text{Normalize}\left(\tilde{S}(x)P(x)\right).\] (3)

Here, \(\lambda=0\) means that we fully trust the given candidate set \(S(x)\); \(\lambda=1\) means that we don't believe the candidate set but trust our own judgment \(P(x)\). According to Eq. 2, the time complexity of this process is mere \(O(cN)\), where \(c\) is the number of categories and \(N\) is the number of samples.

In this paper, we discuss two choices of normalization functions: \(\text{Onehot}(\cdot)\) and \(\text{Scale}(\cdot)\). Specifically, \(\text{Onehot}(\cdot)\) sets the maximum value to 1 and others to 0; \(\text{Scale}(\cdot)\) introduces a scaling factor \(K>0\) and normalizes the probabilities as follows:

\[\text{Scale}(z)=\left\{\frac{z_{i}^{1/K}}{\sum_{j}z_{j}^{1/K}} \right\}_{i=1}^{c},\] (4)

where \(z_{i}\) is the \(i^{th}\) element of \(z\).

### Theoretical Analysis

To verify the feasibility of our method, we conduct theoretical analysis from two perspectives: (1) the manually-designed objective function; (2) the classic expectation-maximization (EM) algorithm [16].

#### 2.4.1 Interpretation from Objective Functions

Let \(P_{i}\), \(S_{i}\), and \(w_{i}\) be abbreviations for \(P_{i}(x)\), \(S_{i}(x)\), and \(w_{i}(x)\), respectively. During training, we should optimize the following objectives:

* Minimize the classification loss on \(w(x)\) and \(P(x)\).
* \(w(x)\) should be small at non-candidate labels.
* Entropy regularization on \(w(x)\) to avoid overconfidence of pseudo labels.
* \(w(x)\) should satisfy \(0\leq w_{i}\leq 1\) and \(\sum_{i=1}^{c}w_{i}=1\).

Then, the final objective function is calculated as follows:

\[\max\,\sum_{i=1}^{c}w_{i}\log P_{i}+M\left(\sum_{i=1}^{c}w_{i}S_{ i}-1\right)-K\sum_{i=1}^{c}w_{i}\log w_{i}\] \[s.t.\sum_{i}^{c}w_{i}=1,w_{i}\geq 0,\] (5)where \(M\) and \(K\) are penalty factors. By using Lagrange multipliers, we can observe that the penalty factor \(K\) is different for two normalization functions: \(K=0\) for Onehot\((\cdot)\) and \(K>0\) for Scale\((\cdot)\). The penalty factor \(M\) has a strong correlation with the weighting coefficient \(\lambda\) in Eq. 2, i.e., \(\lambda=e^{-M}\). Larger \(M\) (or smaller \(\lambda\)) means that we have tighter constraints on \((\sum_{i=1}^{c}w_{i}S_{i}-1)\), and therefore we should trust the given candidate set more. It is identical to the meaning of \(\lambda\) in our ALIM (see Section 2.3).

#### 2.4.2 Interpretation from EM Perspective

EM aims to maximize the likelihood of the corpus \(\mathcal{D}\). Following previous works [5, 17], we first make a mild assumption:

**Assumption 1**: _In noisy PLL, the ground-truth label may not be in the candidate set \(S(x)\). We assume that each candidate label \(\{i|S_{i}(x)=1\}\) has an equal probability \(\alpha(x)\) of generating \(S(x)\) and each non-candidate label \(\{i|S_{i}(x)=0\}\) has an equal probability \(\beta(x)\) of generating \(S(x)\)._

Besides the interpretation from objective functions, we further explain ALIM from the EM perspectivein Appendix B. We prove that the E-step aims to predict the ground-truth label for each sample and the M-step aims to minimize the classification loss. Meanwhile, ALIM is a simplified version of the results derived from EM. Specifically, EM uses an instance-dependent \(\lambda(x)=\beta(x)/\alpha(x)\), while ALIM uses a global \(\lambda\). In the future, we will explore the performance of the instance-dependent \(\lambda(x)\) for noisy PLL. Additionally, EM connects traditional PLL with noisy PLL. In traditional PLL, we assume that the ground-truth label must be in the candidate set, i.e., \(\beta(x)=0\). Then, our noisy PLL approach ALIM will degenerate to the classic PLL method RC [18].

### Optional Key Components

We further introduce several key components to make our method more effective, including the adaptively adjusted strategy and the mixup training.

#### 2.5.1 Adaptively Adjusted Strategy

Appropriate \(\lambda\) is important for ALIM. Too small \(\lambda\) makes us fully trust the candidate set, thus easily over-fitting on noise samples; too large \(\lambda\) makes us ignore the prior information in the candidate set, thus limiting the classification performance. Therefore, we propose a manually adjusted strategy to find a proper \(\lambda\) in a predefined parameter space.

To further reduce manual efforts, we also propose an adaptively adjusted strategy. Specifically, we first estimate the noise level of the dataset. Intuitively, we can randomly select a subset from \(\mathcal{D}\), annotate the ground-truth labels by professional annotators, and estimate the noise level of the dataset. Alternatively, we can automatically estimate noise rate via the Gaussian mixture model [19, 20] or cross-validation [21, 22]. The estimated noise level is represented as \(\eta\). Based on Appendix C, we prove that the value of Eq. 6 can be viewed as a metric, and the \(\eta\)-quantile of this value can be treated as the adaptively adjusted \(\lambda\).

\[\left\{\frac{\max_{i}S_{i}(x)P_{i}(x)}{\max_{i}(1-S_{i}(x))P_{i}(x)}\right\}_{ x\in\mathcal{D}}.\] (6)

In Section 3.2, We further present results without noise rate estimation and manually adjust \(\lambda\) as a hyper-parameter. Through experimental analysis, this approach can also achieve competitive performance. Therefore, the adaptively adjusted strategy is optional. Its main advantage is to reduce manual efforts in hyper-parameter tuning and realize a more automatic approach for noisy PLL.

#### 2.5.2 Mixup Training

Since the mixup training is powerful in noisy label learning [14, 15], we further combine this strategy with ALIM for noisy PLL. Consider a pair of samples \(x_{i}\) and \(x_{j}\) whose pseudo labels are denoted as \(w(x_{i})\) and \(w(x_{j})\), respectively. Then, we create a virtual training sample by linear interpolation:

\[x_{\text{mix}}=\alpha x_{i}+(1-\alpha)x_{j},\] (7)

\[w_{\text{mix}}=\alpha w(x_{i})+(1-\alpha)w(x_{j}).\] (8)\(\alpha\sim\text{Beta}(\zeta,\zeta)\), where \(\zeta\) is a parameter in the beta distribution. We define the mixup objective \(\mathcal{L}_{\text{mix}}\) as the cross-entropy loss on \(P(x_{\text{mix}})\) and \(w_{\text{mix}}\). During training, we combine the mixup loss \(\mathcal{L}_{\text{mix}}\) and the PLL loss \(\mathcal{L}_{\text{pll}}\) into a joint objective function \(\mathcal{L}=\mathcal{L}_{\text{pll}}+\lambda_{\text{mix}}\mathcal{L}_{\text{ mix}}\), where \(\lambda_{\text{mix}}\) controls the trade-off between two losses. The pseudo-code of ALIM is summarized in Algorithm 1.

``` Input: Dataset \(\mathcal{D}\) with the estimated noise level \(\eta\), predictive model \(f\), warm-up epoch \(e_{0}\), number of epoch \(E_{\text{max}}\), weighting coefficient \(\lambda\), trade-off between losses \(\lambda_{\text{mix}}\). Output: The optimized model \(f\).
1for\(e=1,\cdots,e_{0}\)do
2 Warm up by training \(f\) with \(\mathcal{L}_{\text{pll}}\);
3 end for
4for\(e=e_{0},\cdots,E_{\text{max}}\)do
5for\(\{x_{i},S(x_{i})\}\in\mathcal{D}\)do
6 Calculate the output of the predictive model \(f(x_{i})\);
7 Obtain the pseudo label \(w(x_{i})\) by Eq. 2\(\sim\)3;
8 Get the PLL loss \(\mathcal{L}_{\text{pll}}\) between \(f(x_{i})\) and \(w(x_{i})\);
9ifmixup trainingthen
10 Sample \(\alpha\sim\text{Beta}(\zeta,\zeta)\) and sample \(x_{j}\) from \(\mathcal{D}\);
11 Create the virtual sample \((x_{\text{mix}},w_{\text{mix}})\) by Eq. 7\(\sim\)8;
12 Calculate the mixup loss \(\mathcal{L}_{\text{mis}}\) between \(f(x_{\text{mix}})\) and \(w_{\text{mix}}\);
13 Get the final loss \(\mathcal{L}=\mathcal{L}_{\text{pll}}+\lambda_{\text{mix}}\mathcal{L}_{\text{ mix}}\);
14
15else
16 Get the final loss \(\mathcal{L}=\mathcal{L}_{\text{pll}}\);
17
18 end for
19
20 end for
210 Optimize \(f\) by minimizing \(\mathcal{L}\);
22
23ifadaptively adjusted \(\lambda\)then
24 Create an empty list \(G\);
25for\(\{x_{i},S(x_{i})\}\in\mathcal{D}\)do
26 Calculate the output of the predictive model \(f(x_{i})\);
27 Store the value in Eq. 6 to \(G\);
28
29 end for
30\(\lambda\leftarrow\eta-\)quantile of the list \(G\);
31
32 end for ```

**Algorithm 1**Pseudo-code of ALIM.

## 3 Experiments

### Experimental Setup

Corpus DescriptionIn the main experiments, we evaluate the performance on two benchmark datasets of noisy PLL, CIFAR-10 [23] and CIFAR-100 [23]. We choose the noise level \(\eta\) from \(\{0.1,0.2,0.3\}\). Since CIFAR-100 has more classes than CIFAR-10, we consider \(q\in\{0.1,0.3,0.5\}\) for CIFAR-10 and \(q\in\{0.01,0.03,0.05\}\) for CIFAR-100. In Section 3.2, we also conduct experiments on fine-grained datasets (CUB-200 [17] and CIFAR-100H [24]) and consider severe noise.

BaselinesTo verify the effectiveness of our method, we implement the following state-of-the-art methods as baselines: 1) CC [18]: a classifier-consistent PLL method under the uniform candidate label generation assumption; 2) RC [18]: a risk-consistent PLL method under the same assumption as CC; 3) LWC [25]: a PLL method that considers the trade-off between losses on candidate and non-candidate sets; 4) LWS [25]: a PLL method that combines the weighted loss with the sigmoid activation function; 5) PiCO [17]: a PLL method using contrastive learning; 6) CRDPLL [26]: a PLL method that exploits consistency regularization on the candidate set and supervised learning on the non-candidate set; 7) IRNet [12]: a noisy PLL method that progressively purifies noisy samples; 8) PiCO+ [13]: a noisy PLL method using a semi-supervised contrastive framework.

Implementation DetailsThere are mainly three user-specific parameters in ALIM: \(\lambda\), \(\lambda_{\text{mix}}\), and \(e_{0}\). Among them, \(\lambda\) controls the trade-off between the initial candidate set and model outputs. This paper proposes two selection strategies for \(\lambda\), i.e., manually and adaptively adjusted strategies. For the first one, we treat \(\lambda\) as a hyper-parameter and select it from \(\{0.1,0.2,0.3,0.4,0.5,0.7\}\). For the second one, we automatically determine \(\lambda\) using the estimated noise level. \(\lambda_{\text{mix}}\) controls the trade-off between the PLL loss and the mixup loss, and we set \(\lambda_{\text{mix}}=1.0\) as the default parameter. \(e_{0}\) is the start epoch of ALIM, and we select it from \(\{20,40,80,100,140\}\). Following the standard experimental setup in PLL [17; 25], we split a clean validation set from the training set to determine hyper-parameters. Then, we transform the validation set back to its original PLL form and incorporate it into the training set to accomplish model optimization. To optimize all trainable parameters, we choose the SGD optimizer with a momentum of 0.9 and set the weight decay to 0.001. We set the initial learning rate to 0.01 and adjust it using the cosine scheduler. To eliminate the randomness of the results, we run each experiment three times and report the average result and standard deviation on the test set. All experiments are implemented with PyTorch [27] and carried out with NVIDIA Tesla V100 GPU.

### Experimental Results and Discussion

Main ResultsFor a fair comparison, we reproduce all baselines using the same data generation strategy as previous works [12]. In Table 1, we observe that a large portion of improvement is due to mixup training rather than model innovation. To this end, we compare different approaches under the same mixup strategy. Experimental results demonstrate that our method succeeds over all baselines under varying ambiguity levels and noise levels. The main reason lies in two folds. On the one hand, existing PLL methods are mainly designed for clean samples but ignore the presence of noisy samples. Our method can deal with noisy samples by trading off the initial candidate set and model outputs. On the other hand, existing noisy PLL methods generally need to detect noisy samples, but detection errors are unavoidable. These errors can accumulate and continuously affect the training process. Differently, ALIM can deal with this problem by taking advantage of the initial candidate set and restarting correction. These results prove the effectiveness of ALIM in noisy PLL.

Compatibility of ALIMSince ALIM is a plug-in strategy, we integrate it with existing PLL methods and report results in Table 2. We observe that ALIM always brings performance improvement under noisy conditions, verifying the effectiveness and compatibility of our method. Meanwhile,

\begin{table}
\begin{tabular}{l|c c c c c c c c c} \hline \hline \multirow{2}{*}{CIFAR-10} & \multicolumn{3}{c}{\(q=0.1\)} & \multicolumn{3}{c}{\(q=0.3\)} & \multicolumn{3}{c}{\(q=0.5\)} \\ \cline{2-10}  & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) \\ \hline \(\diamond\)CC & 79.81\(\pm\)0.22 & 77.06\(\pm\)0.18 & 73.87\(\pm\)0.31 & 74.09\(\pm\)0.60 & 71.43\(\pm\)0.56 & 68.08\(\pm\)1.12 & 69.87\(\pm\)0.94 & 59.35\(\pm\)0.22 & 48.93\(\pm\)0.52 \\ \(\diamond\)RC & 80.87\(\pm\)0.30 & 73.28\(\pm\)0.23 & 75.24\(\pm\)0.17 & 79.69\(\pm\)0.40 & 77.56\(\pm\)0.92 & 73.10\(\pm\)0.54 & 72.46\(\pm\)1.51 & 59.72\(\pm\)0.42 & 49.74\(\pm\)0.70 \\ \(\diamond\)LWC & 79.30\(\pm\)0.53 & 76.15\(\pm\)0.46 & 74.76\(\pm\)0.48 & 77.47\(\pm\)0.56 & 74.02\(\pm\)0.35 & 69.10\(\pm\)0.57 & 70.59\(\pm\)1.34 & 57.42\(\pm\)1.44 & 8.94\(\pm\)0.37 \\ \(\diamond\)LWS & 82.97\(\pm\)0.24 & 79.46\(\pm\)0.09 & 74.28\(\pm\)0.79 & 89.03\(\pm\)0.28 & 76.07\(\pm\)0.38 & 69.70\(\pm\)0.72 & 70.41\(\pm\)2.68 & 58.26\(\pm\)0.28 & 39.42\(\pm\)0.39 \\ \(\diamond\)PPCO & 90.78\(\pm\)0.24 & 87.72\(\pm\)0.11 & 89.46\(\pm\)0.12 & 89.71\(\pm\)0.18 & 85.78\(\pm\)0.38 & 82.25\(\pm\)0.32 & 88.11\(\pm\)0.29 & 8.24\(\pm\)0.30 & 87.85\(\pm\)7.26 \\ \(\diamond\)CBDPLL & 93.44\(\pm\)0.17 & 93.13\(\pm\)0.39 & 86.10\(\pm\)0.48 & 92.73\(\pm\)0.10 & 89.66\(\pm\)0.21 & 83.40\(\pm\)0.11 & 91.01\(\pm\)0.00 & 82.30\(\pm\)0.46 & 73.78\(\pm\)0.55 \\ \(\diamond\)PPCO+ & 93.64\(\pm\)0.19 & 93.13\(\pm\)0.26 & 92.18\(\pm\)0.38 & 93.22\(\pm\)0.08 & 89.22\(\pm\)0.01 & 89.95\(\pm\)0.19 & 91.07\(\pm\)0.02 & 89.68\(\pm\)0.01 & 84.08\(\pm\)0.42 \\ \(\diamond\)HNR & 93.44\(\pm\)0.21 & 92.57\(\pm\)0.25 & 92.38\(\pm\)0.21 & 92.81\(\pm\)0.19 & 92.18\(\pm\)0.19 & 91.35\(\pm\)0.08 & 91.51\(\pm\)0.09 & 90.76\(\pm\)0.10 & 86.19\(\pm\)0.41 \\ \(\diamond\)ALIM-Scale & **94.15\(\pm\)0.14** & 93.41\(\pm\)0.04 & 93.28\(\pm\)0.08 & 93.40\(\pm\)0.03 & 92.69\(\pm\)0.01 & 92.24\(\pm\)0.10 & 92.52\(\pm\)0.12 & 90.92\(\pm\)0.10 & 85.51\(\pm\)0.21 \\ \(\diamond\)ALIM-Oneh & **94.15\(\pm\)0.15** & **94.04\(\pm\)0.16** & **93.77\(\pm\)0.27** & **93.44\(\pm\)0.16** & **93.25\(\pm\)0.08** & **92.42\(\pm\)0.17** & **92.67\(\pm\)0.12** & **93.83\(\pm\)0.08** & **89.08\(\pm\)0.38** \\ \hline \(\diamond\)PPCO & 94.58\(\pm\)0.29 & 94.74\(\pm\)0.13 & 94.43\(\pm\)0.19 & 94.02\(\pm\)0.03 & 94.03\(\pm\)0.01 & 92.94\(\pm\)0.24 & 93.56\(\pm\)0.08 & 92.65\(\pm\)0.26 & 82.12\(\pm\)0.37 \\ \(\diamond\)ALIM-Scale & 95.71\(\pm\)0.01 & 95.00\(\pm\)0.00 & 95.35\(\pm\)0.19 & 95.31\(\pm\)0.16 & 96.14\(\pm\)0.74 & 97.46\(\pm\)0.07 & 94.36\(\pm\)0.03 & 93.74\(\pm\)10.04 & 93.82\(\pm\)1.03 & 90.63\(\pm\)0.10 \\ \(\diamond\)ALIM-Oneh & **95.83\(\pm\)0.13** & **95.86\(\pm\)0.15** & **95.75\(\pm\)0.19** & **95.52\(\pm\)0.15** & **95.41\(\pm\)0.13** & **94.67\(\pm\)0.21** & **95.19\(\pm\)0.24** & **93.89\(\pm\)0.21** & **92.26\(\pm\)0.29** \\ \hline \hline \multirow{2}{*}{CIFAR-100} & \multicolumn{3}{c}{\(q=0.01\)} & \multicolumn{3}{c}{\(q=0.03\)} & \multicolumn{3}{c}{\(q=0.05\)} \\ \cline{2-10}  & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.2\) & \(\eta=0.3\) \\ \hline \(\diamond\)CC & 53.63\(\pm\)0.46 & 88.44\(\pm\)0integrating ALIM with PiCO generally achieves better performance under severe noise (\(\eta=0.3\)). In noisy label learning, self-training suffers from accumulated errors caused by sample-selection bias [28]. To address this issue, researchers propose co-training [29], which contains multiple branches and cross-updates these branches. Among all PLL methods, PiCO is a natural co-training network with two branches: one for classification and one for clustering [17]. The output of the classification branch guides the model to update the clustering prototypes; the output of the clustering branch affects the update of the pseudo label for classification. These results verify the effectiveness of co-training under noisy conditions. Therefore, we combine ALIM with PiCO in the following experiments.

Fine-Grained Classification PerformanceConsidering that similar categories are more likely to be added to the candidate set, we conduct experiments on more challenging fine-grained datasets, CIFAR-100H [17] and CUB-200 [24]. Different from previous works [12] that generate candidate labels from the entire label space, this section considers the case of generating candidate labels belonging to the same superclass. Experimental results in Table 3 demonstrate that ALIM outperforms existing methods on fine-grained classification tasks, verifying the effectiveness of our method.

Robustness with Severe NoiseTo demonstrate noise robustness, we conduct experiments under severe noise conditions. In this section, we compare ALIM with the most competitive baselines (RC, PiCO, CRDPLL, and PiCO+) under \(\eta\in\{0.4,0.5\}\). Experimental results in Table 4 show that ALIM succeeds over all baselines under severe noise. Taking the results on \(\eta=0.5\) as an example, our method outperforms the currently advanced approaches by 11.76%. These results demonstrate that our ALIM is more robust to noise than existing PLL and noisy PLL methods.

Discussion on Normalization FunctionsIn this section, we compare the performance of two normalization functions. In Table 1, we observe that Onehot\((\cdot)\) performs slightly better on CIFAR-10 and Scale\((\cdot)\) performs slightly better on CIFAR-100. In Table 3\(\sim\)4, Scale\((\cdot)\) generally achieves better performance in fine-grained datasets and severe noise conditions. The main difference between these normalization functions is that Onehot\((\cdot)\) compresses estimated probabilities into a specific class, while Scale\((\cdot)\) preserves prediction information for all categories. For simple datasets like CIFAR-10, the class with the highest probability is likely to be correct. Therefore, the compression operation can

\begin{table}
\begin{tabular}{l|c|c} \hline \multirow{2}{*}{Method} & CIFAR-100 & CIFAR-100 \\  & \(\{q=0.05,\eta=0.2\}\) & \(\{q=0.05,\eta=0.5\}\) \\ \hline \(\Diamond\)CC & 26.98±1.16 & 34.57±0.99 \\ \(\Diamond\)RC & 44.74±2.47 & 48.03±0.47 \\ \(\Diamond\)LWS & 18.65±2.15 & 22.18±6.12 \\ \(\Diamond\)GCE & 5.13±38.65 & 33.21±2.03 \\ \(\Diamond\)MSE & 20.92±1.20 & 35.20±1.03 \\ \(\Diamond\)CRDPLL & 44.14±0.70±2 & 53.20±21 \\ \(\Diamond\)HCO & 53.05±2.03 & 59.81±0.25 \\ \(\Diamond\)PHCO & 60.65±4.09 & 68.31±0.47 \\ \(\Diamond\)ALIM-Scale & **68.38±0.47** & **73.42±0.18** \\ \(\Diamond\)ALIM-Onehot & 63.91±0.35 & 72.36±0.20 \\ \hline \end{tabular}
\end{table}
Table 4: Performance with severe noise.

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c} \hline \multirow{2}{*}{PLL} & \multirow{2}{*}{ALIM} & \multicolumn{4}{c}{\(q=0.1\)} & \multicolumn{4}{c}{CIFAR-100} & \multicolumn{4}{c}{\(q=0.3\)} & \multicolumn{4}{c}{\(q=0.5\)} \\ \cline{3-10}  & & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) \\ \hline \(\Diamond\)RC & \(\times\) & 80.87±0.30 & 78.22±0.23 & 75.24±0.17 & 79.69±0.37 & 75.69±0.40 & 73.10±0.54 & 72.46±1.51 & 59.72±0.24 & 49.74±0.70 \\ \(\Diamond\)RC & \(\times\) & 88.81±0.17 & 87.61±0.20 & 85.53±0.05 & 86.21±0.37 & 83.64±0.07 & 78.34±0.34 & 77.40±0.31 & 53.10±0.37 & 56.75±1.59 \\ \(\Diamond\)PHCO & \(\times\) & 90.78±0.24 & 87.27±0.11 & 84.96±0.12 & 89.71±0.18 & 85.78±0.23 & 82.54±0.32 & 88.11±0.29 & 82.41±0.30 & 68.75±2.62 \\ \(\Diamond\)PHCO & \(\surd\) & 94.15±0.15 & 94.04±0.16 & 93.77±0.29 & 93.44±0.16 & 93.25±0.29 & 92.42±0.17 & 92.67±0.12 & 91.83±0.08 & 89.80±0.38 \\ \(\Diamond\)CRDPLL & \(\times\) & 93.48±0.17 & 89.80±0.39 & 86.19±0.48 & 92.73±0.19 & 86.96±0.21 & 83.40±0.14 & 91.19±0.00 & 82.30±0.46 & 73.78±0.55 \\ \(\Diamond\)CRDPLL & \(\times\) & 96.03±0.23 & 95.01±0.32 & 93.61±0.10 & 95.32±0.13 & 93.72±0.29 & 91.20±0.06 & 93.82±0.05 & 90.20±0.04 & 84.24±0.28 \\ \hline \end{tabular} 
\begin{tabular}{l|c|c c c c c c c c c} \hline \multirow{2}{*}{PLL} & \multirow{2}{*}{ALIM} & \multicolumn{4}{c}{\(q=0.01\)} & \multicolumn{4}{c}{CIFAR-100} & \multicolumn{4}{c}{\(q=0.05\)} \\ \cline{3-10}  & & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=0.2\) & \(\eta=0.3\) \\ \hline \(\Diamond\)RC & \(\times\) & 52.73±1.05 & 48.95±1.04 & 45.77±0.31 & 52.15±0.19 & 48.25±0.38 & 43.92±0.37 & 46.62±0.34 & 45.46±0.21 & 40.31±0.55 \\ \(\Diamond\)RC & \(\times\) & 61.46±0.26 & 60.10±0.23 & 55.67±0.28 & 57.43±0.20 & 52.98±0.24 & 7.84±0.37 & 56.40±0.60 & 51.91±0.12 & 46.87±0.74 \\ \(\Diamond\)PHCO & \(\times\) & 62.78±0.20 & 62.04±0.31 & 58.79±0.09 & 67.38±0.09 & 62.10±0.33 & 58.64±0.28 & 67.52±0.28 & 68.18±0.65 \\ \(\Diamond\)PHCO & \(\surd\) & 72.26±0.23 & 71.98±0.29 & 71.04±0.31 & 71.43±0.21 & 70.79±0.43 & 70.14±0.25 & 72.28±0.28 & 70.60±0.44 & 70.05±0.43 \\ \(\Diamond\)CRDPLL & \(\times\) & 68.12±0.13 & 65.32±0.34 & 62.94±0.28 & 67.53±0.07 & 64.29±0.27 & 61.79±0.11 & 67.17±0.04 & 64.11±0.42 & 61.03±0.43 \\ \(\Diamond\)CRDPLL & \(\surd\) & 69.98±0.30 & 68.58±0.16 & 66.90±0.16 & 69.60±0.20 & 67.67±0.22 & 66.15±0.12 & 68.75±0.06 & 67.07±0.29 & 64.69±0.23 \\ \hline \end{tabular}
\end{table}
Table 2: Compatibility of ALIM on different PLL methods.

\begin{table}
\begin{tabular}{l|c c} \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{CUB-200} & CIFAR-100H \\  & \(q=0.05,\eta=0.2\) & \(\{q=0.5,\eta=0.2\}\) \\ \hline \(\Diamond\)CC & 26.98±1.16 & 34.57±0.99 \\ \(\Diamond\)RC & 44.74±2.47 & 48.03±0.47 \\ \(\Diamond\)LWS & 18.65±reduce the negative impact of other categories and achieve better performance on noisy PLL. For challenging datasets, the class with the highest predicted value has a low probability of being correct. Therefore, this compression operation may lead to severe information loss. More importantly, ALIM consistently outperforms existing methods regardless of the normalization function. Therefore, the main improvement comes from our ALIM rather than normalization functions.

Rationality of Adaptively Adjusted StrategyIn this section, we further explain the rationality of our adaptively adjusted strategy. Figure 2 visualizes the distribution of the value in Eq. 6 for clean and noise subsets with increasing training iterations. We observe that this value is an effective indicator for distinguishing clean samples from noisy samples. At the same time, our adaptively adjusted \(\lambda\) serves as a suitable boundary for clean and noisy subsets.

Role of Adaptively Adjusted StrategyIt is important to select a proper \(\lambda\) in ALIM. In this paper, we propose two selection strategies: adaptively and manually adjusted strategies. Figure 3 presents the classification performance of these strategies. Experimental results show that different strategies can achieve similar performance. Therefore, the main advantage of this adaptively adjusted strategy is to reduce manual efforts in hyper-parameter tuning. The performance improvement still comes from ALIM, which exploits a coefficient to control the reliability of the candidate set.

Figure 4: Parameter sensitivity analysis on CIFAR-10 (\(q=0.3\)) and CIFAR-100 (\(q=0.05\)) with mixup training. The noise level of these datasets is fixed to \(\eta=0.3\).

Figure 3: Classification performance of manually adjusted \(\lambda\) and adaptively adjusted \(\lambda\). We conduct experiments on CIFAR-10 (\(q=0.3,\eta=0.3\)) and CIFAR-100 (\(q=0.05,\eta=0.3\)). We mark the results of the adaptively adjusted strategy with red lines.

Figure 2: Distribution of the value in Eq. 6 for clean and noise subsets with increasing training iterations. We conduct experiments on CIFAR-10 (\(q=0.3,\eta=0.3\)) with \(e_{0}=80\).

Parameter Sensitivity AnalysisIn this section, we perform parameter sensitivity analysis on two key hyper-parameters: the warm-up epoch \(e_{0}\), and the trade-off between the PLL loss and the mixup loss \(\lambda_{\text{mix}}\). In Figure 4, we observe that choosing an appropriate \(\lambda_{\text{mix}}\) is important for ALIM, and \(\lambda_{\text{mix}}=1.0\) generally achieves competitive results. Meanwhile, with the gradual increase of \(e_{0}\), the test accuracy first increases and then decreases. This phenomenon indicates that ALIM needs warm-up training. But too large \(e_{0}\) can also cause the model to overfit noise samples.

## 4 Related Work

In PLL, the ground-truth label is concealed in the candidate set. To deal with this task, the core is to disambiguate the candidate labels. In this section, we first introduce two typical disambiguation strategies, i.e., average-based methods and identification-based methods. Then, we briefly review some recent works on noisy PLL.

Average-based PLLThe most intuitive solution is the average-based approach, which assumes that each candidate label has an equal probability of being the ground-truth label. For example, Hullermeier et al. [30] utilized k-nearest neighbors for label disambiguation. For each sample, they treated all candidate labels of its neighborhood equally and predicted the ground-truth label through the majority vote. Differently, Cour et al. [31] maximized the average output of candidate labels and non-candidate labels in parametric models. However, these average-based methods can be severely affected by false positive labels [9].

Identification-based PLLTo address the shortcomings of the above methods, researchers have focused on identification-based methods. Different from average-based methods that treat all candidate labels equally, identification-based methods treat the ground-truth label as a latent variable and maximize its estimated probability by maximum margin [32] or maximum likelihood [5] criteria.

Recently, deep learning has been applied to identification-based methods and achieved promising results. For example, Lv et al. [33] proposed a self-training strategy to disambiguate candidate labels. Feng et al. [18] introduced classifier- and risk-consistent algorithms under the uniform candidate label generation assumption. Wen et al. [25] relaxed this assumption and proposed a family of loss functions for label disambiguation. To learn more discriminative representations, Wang et al. [17] exploited contrastive learning to deal with partially labeled samples. More recently, Wu et al. [26] used consistency regularization on the candidate set and supervised learning on the non-candidate set, achieving promising results under varying ambiguity levels. The above methods rely on a basic assumption that the ground-truth label must be in the candidate set. But this assumption may not be satisfied due to the unprofessional judgment of annotators.

Noisy PLLRecently, noisy PLL has attracted increasing attention from researchers due to its more practical setup. Its core challenge is how to deal with noisy samples. For example, Lv et al. [11] utilized noise-tolerant loss functions to avoid overemphasizing noisy samples during training. Lian et al. [12] proposed an iterative refinement network to purify noisy samples and reduce the noise level of the dataset. Wang et al. [13] detected clean samples through a distance-based sample selection mechanism and dealt with noisy samples via a semi-supervised contrastive framework. These noisy PLL methods generally need to detect noisy samples, but detection errors are unavoidable. These errors can accumulate and continuously affect model training. To reduce the negative impact of prediction errors, we propose a novel framework for noisy PLL. Experimental results on multiple datasets demonstrate the effectiveness of our method under noisy conditions.

## 5 Conclusion

This paper introduces a novel framework for noisy PLL called ALIM. To deal with noisy samples, it exploits the weighting mechanism to adjust the reliability of the initial candidate set and model outputs. To verify its effectiveness, we conduct experiments on multiple benchmark datasets under varying ambiguity levels and noise levels. Experimental results demonstrate that our ALIM achieves state-of-the-art classification performance on noisy PLL, especially in severe noise conditions and fine-grained datasets. Meanwhile, ALIM is a low-computation plug-in strategy that can be easily integrated with existing PLL frameworks. Furthermore, we interpret the rationality and effectiveness of the adaptively adjusted strategy. We also conduct parameter sensitivity analysis and reveal the impact of different hyper-parameters. It is worth noting that this paper leverages a global \(\lambda\) to measure the reliability of the candidate set. In the future, we will explore the instance-dependent \(\lambda(x)\) during training. At the same time, we will investigate the performance in more experimental settings, such as class-imbalanced conditions.

## 6 Acknowledge

This work is supported by the National Natural Science Foundation of China (NSFC) (No.61831022, No.62276259, No.62201572, No.U21B2010, No.62271083), Beijing Municipal Science&Technology Commission,Administrative Commission of Zhongguancun Science Park No.Z211100004821013, Open Research Projects of Zhejiang Lab (NO. 2021KHOAB06), CCF-Baidu Open Fund (No.OF2022025), the National Natural Science Foundation of China (Grant No. 62106028), Chongqing Overseas Chinese Enterpeneurship and Innovation Support Program, Chongqing Artificial Intelligence Innovation Center, CAAI-Huawei MindSporc Open Fund, and OpenI Community (https://openi.pcl.ac.cn).

## References

* [1] Lei Feng and Bo An. Leveraging latent label distributions for partial label learning. In _Proceedings of the International Joint Conference on Artificial Intelligence_, pages 2107-2113, 2018.
* [2] Yan Yan and Yuhong Guo. Partial label learning with batch label correction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 6575-6582, 2020.
* [3] Yi-Chen Chen, Vishal M Patel, Rama Chellappa, and P Jonathon Phillips. Ambiguously labeled learning using dictionaries. _IEEE Transactions on Information Forensics and Security_, 9(12):2076-2088, 2014.
* [4] Ching-Hui Chen, Vishal M Patel, and Rama Chellappa. Learning from ambiguously labeled face images. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(7):1653-1667, 2017.
* [5] Li-Ping Liu and Thomas G Dietterich. A conditional multinomial mixture model for superset label learning. In _Proceedings of the 25th International Conference on Neural Information Processing Systems_, pages 548-556, 2012.
* [6] Liping Liu and Thomas Dietterich. Learnability of the superset label learning problem. In _Proceedings of the International Conference on Machine Learning_, pages 1629-1637, 2014.
* [7] Mark J Huiskes and Michael S Lew. The mir flickr retrieval evaluation. In _Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval_, pages 39-43, 2008.
* [8] Forrest Briggs, Xiaoli Z Fern, and Raviv Raich. Rank-loss support instance machines for miml instance annotation. In _Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 534-542, 2012.
* [9] Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In _Proceedings of the 15th International Conference on Neural Information Processing Systems_, pages 921-928, 2002.
* [10] Jesus Cid-Sueiro. Proper losses for learning from partial labels. In _Proceedings of the 25th International Conference on Neural Information Processing Systems_, pages 1565-1573, 2012.
* [11] Jiaqi Lv, Lei Feng, Miao Xu, Bo An, Gang Niu, Xin Geng, and Masashi Sugiyama. On the robustness of average losses for partial-label learning. _arXiv preprint arXiv:2106.06152_, 2021.
* [12] Zheng Lian, Mingyu Xu, Lan Chen, Licai Sun, Bin Liu, and Jianhua Tao. Arnet: Automatic refinement network for noisy partial label learning. _arXiv preprint arXiv:2211.04774_, 2022.
* [13] Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico+: Contrastive label disambiguation for robust partial label learning. _arXiv preprint arXiv:2201.08984_, 2022.
* [14] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _Proceedings of the International Conference on Learning Representations_, pages 1-13, 2018.
* [15] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In _Proceedings of the International Conference on Learning Representations_, pages 1-14, 2020.
* [16] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, 1977.
* [17] Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico: Contrastive label disambiguation for partial label learning. In _Proceedings of the International Conference on Learning Representations_, pages 1-18, 2022.

* [18] Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Provably consistent partial-label learning. In _Proceedings of the Advances in Neural Information Processing Systems_, pages 10948-10960, 2020.
* [19] Eric Arazo, Diego Ortego, Paul Albert, Noel O'Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. In _Proceedings of the International Conference on Machine Learning_, pages 312-321, 2019.
* [20] Hwanjun Song, M Mineok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Robust learning by self-transition for handling noisy labels. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1490-1500, 2021.
* [21] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 38(3):447-461, 2015.
* [22] Hwanjun Song, M Mineok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust deep learning. In _Proceedings of the International Conference on Machine Learning_, pages 5907-5915, 2019.
* [23] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
* [24] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [25] Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In _Proceedings of the International Conference on Machine Learning_, pages 11091-11100, 2021.
* [26] Dong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. Revisiting consistency regularization for deep partial label learning. In _Proceedings of the International Conference on Machine Learning_, pages 24212-24225, 2022.
* [27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an imperative style, high-performance deep learning library. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pages 8026-8037, 2019.
* [28] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _Proceedings of the International Conference on Machine Learning_, pages 2304-2313, 2018.
* [29] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In _Proceedings of the 11th Annual Conference on Computational Learning Theory_, pages 92-100, 1998.
* [30] Eyke Hullermeier and Jurgen Beringer. Learning from ambiguously labeled examples. _Intelligent Data Analysis_, 10(5):419-439, 2006.
* [31] Timothee Cour, Benjamin Sapp, Chris Jordan, and Ben Taskar. Learning from ambiguously labeled images. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 919-926, 2009.
* [32] Fei Yu and Min-Ling Zhang. Maximum margin partial label learning. In _Proceedings of the Asian Conference on Machine Learning_, pages 96-111, 2016.
* [33] Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In _Proceedings of the International Conference on Machine Learning_, pages 6500-6510, 2020.

Interpretation from Objective Functions

In this section, we provide proofs of the Onehot\((\cdot)\) normalization function and the Scale\((\cdot)\) normalization function from the perspective of objective functions.

### Proof for Onehot Normalization

For \(K=0\), we choose the following objective function during training:

\[\max\,\sum_{i=1}^{c}w_{i}\log P_{i}+M\left(\sum_{i=1}^{c}w_{i}S_{i }-1\right)\] \[s.t.\,\sum_{i}^{c}w_{i}=1,w_{i}\geq 0.\] (9)

Introduce Lagrange multipliers \(\delta_{i},i\in[1,c]\) and \(\gamma\) into Eq. 9, we have:

\[\mathcal{L}=\sum_{i=1}^{c}w_{i}\log P_{i}+M\left(\sum_{i=1}^{c}w_{i}S_{i}-1 \right)+\gamma\left(1-\sum_{i=1}^{c}w_{i}\right)+\sum_{i=1}^{c}\delta_{i}w_{i}.\] (10)

Combined with the Karush-Kuhn-Tucker (KKT) conditions, the optimal point should satisfy:

\[\log P_{i}+MS_{i}-\gamma+\delta_{i}=0,\] (11)

\[\sum_{i=1}^{c}w_{i}=1,\delta_{i}\geq 0,w_{i}\geq 0,\delta_{i}w_{i}=0.\] (12)

Since \(S_{i}\in\{0,1\}\), we have \(MS_{i}=\log(e^{M}S_{i}+(1-S_{i}))\). The equivalent equation of Eq. 11 is:

\[\delta_{i}=\gamma-\log(e^{M}S_{i}+(1-S_{i}))P_{i}.\] (13)

Combined with \(\delta_{i}\geq 0\) in Eq. 12, we have:

\[\gamma\geq\max_{i}\left(\log(e^{M}S_{i}+(1-S_{i}))P_{i}\right).\] (14)

\(\delta_{i}>0\) is true if \(\gamma>\max_{i}\left(\log(e^{M}S_{i}+(1-S_{i}))P_{i}\right)\). According to \(\delta_{i}w_{i}=0\), we always have \(w_{i}=0\), which conflicts with \(\sum_{i=1}^{c}w_{i}=1\). Therefore, we get:

\[\gamma=\max_{i}\left(\log(e^{M}S_{i}+(1-S_{i}))P_{i}\right).\] (15)

We assume that only one \(i_{0}\in[1,c]\) reaches the maximum \(\gamma\), then we have \(w_{i}=0,i\in[1,c]/i_{0}\). Combined with \(\sum_{i=1}^{c}w_{i}=1\), we get \(w_{i_{0}}=1\). Therefore, \(w(x)\) should satisfy:

\[w(x)=\text{Onehot}\left(\log(e^{M}S(x)+(1-S(x)))P(x)\right).\] (16)

We mark \(\lambda=e^{-M}\) and convert Eq. 16 to its equivalent version:

\[w(x)=\text{Onehot}\left((S(x)+\lambda(1-S(x))P(x)\right).\] (17)

### Proof for Scale Normalization

For \(K>0\), \(\log w_{i}\) ensures that \(w_{i}\) must be positive. Therefore, the constraint \(w_{i}\geq 0\) can be excluded. Then, the objective function can be converted to:

\[\max\,\sum_{i=1}^{c}w_{i}\log P_{i}+M\left(\sum_{i=1}^{c}w_{i}S_{ i}-1\right)-K\sum_{i=1}^{c}w_{i}\log w_{i}\] \[s.t.\sum_{i}^{c}w_{i}=1.\] (18)Introduce the Lagrange multiplier \(\gamma\) in Eq. 18, we have:

\[\mathcal{L}=\sum_{i=1}^{c}w_{i}\log P_{i}+M\left(\sum_{i=1}^{c}w_{i}S_{i}-1 \right)-K\sum_{i=1}^{c}w_{i}\log w_{i}+\gamma\left(1-\sum_{i}^{c}w_{i}\right).\] (19)

Since the optimal point should satisfy \(\nabla_{w}\mathcal{L}=0\), we have:

\[\log P_{i}+MS_{i}-K\left(1+\log w_{i}\right)-\gamma=0.\] (20)

Since \(S_{i}\in\{0,1\}\), we have \(MS_{i}=\log(e^{M}S_{i}+(1-S_{i}))\). The equivalent equation of Eq. 20 is:

\[\log(e^{M}S_{i}+(1-S_{i}))P_{i}-(K+\gamma)-K\log w_{i}=0,\] (21)

\[w_{i}^{K}=\frac{\left(e^{M}S_{i}+(1-S_{i})\right)P_{i}}{e^{K+\gamma}}.\] (22)

We mark \(\lambda=e^{-M}\). Then, we have:

\[w_{i}=\frac{\left((S_{i}+\lambda(1-S_{i}))P_{i}\right)^{1/K}}{e^{1+(\gamma-M) /K}}.\] (23)

Since \(\sum_{i}^{c}w_{i}=1\), we have:

\[\sum_{i=1}^{c}\frac{\left((S_{i}+\lambda(1-S_{i}))P_{i}\right)^{1/K}}{e^{1+( \gamma-M)/K}}=1,\] (24)

\[e^{1+(\gamma-M)/K}=\sum_{i=1}^{c}\left((S_{i}+\lambda\left(1-S_{i}\right))P_{ i}\right)^{1/K}.\] (25)

Combine Eq. 23 and Eq. 25 and we have:

\[w_{i}=\frac{\left((S_{i}+\lambda(1-S_{i}))P_{i}\right)^{1/K}}{\sum_{i=1}^{c} \left((S_{i}+\lambda(1-S_{i}))P_{i}\right)^{1/K}}.\] (26)

Combined with the definition of \(\text{Scale}(\cdot)\) in Eq. 4, this equation can be converted to:

\[w(x)=\text{Scale}\left((S(x)+\lambda(1-S(x)))P(x)\right).\] (27)

## Appendix B EM Perspective of ALIM

EM aims to maximize the likelihood of the dataset \(\mathcal{D}\):

\[\text{max}_{\theta}\sum_{x\in\mathcal{D}}\log P\left(x,S(x);\theta\right) =\text{max}_{\theta}\sum_{x\in\mathcal{D}}\log\sum_{i=1}^{c}P\left( x,S(x),y(x)=i;\theta\right)\] \[=\text{max}_{\theta}\sum_{x\in\mathcal{D}}\log\sum_{i=1}^{c}w_{i} (x)\frac{P\left(x,S(x),y(x)=i;\theta\right)}{w_{i}(x)}\] \[\geq\text{max}_{\theta}\sum_{x\in\mathcal{D}}\sum_{i=1}^{c}w_{i} (x)\log\frac{P\left(x,S(x),y(x)=i;\theta\right)}{w_{i}(x)},\] (28)

where \(\theta\) is the trainable parameter. The last step of Eq. 28 utilizes Jensen's inequality. Since the \(\log(\cdot)\) function is strictly concave, the equal sign takes when \(P(x,S(x),y(x)=i;\theta)/w_{i}(x)\) is some constant \(C\), i.e.,

\[w_{i}(x)=\frac{1}{C}P(x,S(x),y(x)=i;\theta).\] (29)Considering that \(\sum_{i=1}^{c}w_{i}(x)=1\), we can further get:

\[C=\sum_{i=1}^{c}P(x,S(x),y(x)=i;\theta).\] (30)

Then, we have:

\[w_{i}(x)=\frac{P(x,S(x),y(x)=i;\theta)}{\sum_{i=1}^{c}P(x,S(x),y(x )=i;\theta)}=\frac{P(x,S(x),y(x)=i;\theta)}{P(x,S(x);\theta)}=P(y(x)=i|x,S(x); \theta).\] (31)

In the EM algorithm [16], the E-step aims to calculate \(w_{i}(x)\) and the M-step aims to maximize the lower bound of Eq. 28:

\[\underset{\theta}{\text{argmax}}\sum_{x\in\mathcal{D}}\sum_{i=1} ^{c}w_{i}(x)\log\frac{P(x,S(x),y(x)=i;\theta)}{w_{i}(x)}\] \[=\underset{\theta}{\text{argmax}}\sum_{x\in\mathcal{D}}\sum_{i=1 }^{c}w_{i}(x)\log P(x,S(x),y(x)=i;\theta).\] (32)

**E-Step.** In this step, we aim to predict the ground-truth label for each sample:

\[w_{i}(x)=P(y(x)=i|x,S(x);\theta) =\frac{P(S(x)|y(x)=i,x;\theta)P(y(x)=i|x;\theta)}{P(S(x)|x;\theta)}\] \[=\frac{P(S(x)|y(x)=i,x;\theta)P(y(x)=i|x;\theta)}{\sum_{i=1}^{c}P (S(x)|y(x)=i,x;\theta)P(y(x)=i|x;\theta)}.\] (33)

According to Assumption 1, we have:

\[P(S(x)|y(x),x)=\begin{cases}\alpha(x),S_{y(x)}(x)=1\\ \beta(x),S_{y(x)}(x)=0.\end{cases}\] (34)

It can be converted to:

\[P(S(x)|y(x),x)=\alpha(x)S_{y(x)}(x)+\beta(x)\left(1-S_{y(x)}(x) \right).\] (35)

Then, we get the equivalent equation of Eq. 33:

\[w_{i}(x)=\frac{(\alpha(x)S_{i}(x)+\beta(x)(1-S_{i}(x)))P(y(x)=i|x ;\theta)}{\sum_{i=1}^{c}\left(\alpha(x)S_{i}(x)+\beta(x)(1-S_{i}(x))\right) P(y(x)=i|x;\theta)}.\] (36)

We mark \(\lambda(x)=\beta(x)/\alpha(x)\) and \(P_{i}(x)=P(y(x)=i|x;\theta)\). Then, we get:

\[w_{i}(x)=\frac{(S_{i}(x)+\lambda(x)(1-S_{i}(x)))P_{i}(x)}{\sum_{i=1}^{c} \left(S_{i}(x)+\lambda(x)(1-S_{i}(x))\right)P_{i}(x)}.\] (37)

It connects traditional PLL and noisy PLL. In traditional PLL, we assume that the ground-truth label must be in the candidate set, i.e., \(\beta(x)=0\). Since \(\lambda(x)=\beta(x)/\alpha(x)=0\), Eq. 37 degenerates to:

\[w_{i}(x)=\frac{S_{i}(x)P_{i}(x)}{\sum_{i=1}^{c}S_{i}(x)P_{i}(x)},\] (38)

which is identical to the classic PLL method, RC [18].

**M-Step.** The objective function of this step is:

\[\underset{\theta}{\text{argmax}}\sum_{x\in\mathcal{D}}\sum_{i=1 }^{c}w_{i}(x)\log P(x,S(x),y(x)=i;\theta)\] \[= \underset{\theta}{\text{argmax}}\sum_{x\in\mathcal{D}}\sum_{i=1 }^{c}w_{i}(x)\log P(x;\theta)P(y(x)=i|x;\theta)P(S(x)|y(x)=i,x;\theta).\] (39)Considering that \(P(x;\theta)=P(x)\) and \(P(S(x)|y(x)=i,x;\theta)=P(S(x)|y(x)=i,x)\), the equivalent version of Eq. 39 is:

\[\underset{\theta}{\text{argmax}}\sum_{x\in\mathcal{D}}\sum_{i=1}^{c}w_{i}(x) \log P(y(x)=i|x;\theta).\] (40)

Therefore, the essence of the M-step is to minimize the classification loss.

## Appendix C Adaptively Adjusted \(\lambda\)

Since \(\eta\) controls the noise level of the dataset, we have:

\[P(S_{y(x)}(x)=0)=\eta.\] (41)

After the warm-up training, we assume that the predicted label generated by ALIM \(\hat{y}(x)=\arg\max_{1\leq i\leq c}w(x)\) is accurate, i.e., \(\hat{y}(x)=y(x)\). Then we have:

\[P(S_{\hat{y}(x)}(x)=0)=\eta.\] (42)

To estimate the value of \(\lambda\), we first study the equivalent meaning of \(S_{\hat{y}(x)}(x)=0\):

\[\max_{S_{i}(x)=0}\left(S_{i}(x)+\lambda\left(1-S_{i}(x)\right)\right)P_{i}(x) \geq\max_{S_{i}(x)=1}\left(S_{i}(x)+\lambda\left(1-S_{i}(x)\right)\right)P_{i }(x).\] (43)

We simplify the left and right sides of Eq.43 as follows:

\[\max_{S_{i}(x)=0}(S_{i}(x)+\lambda(1-S_{i}(x)))P_{i}(x)\] \[= \max_{S_{i}(x)=0}\lambda(1-S_{i}(x))P_{i}(x)\] \[= \max_{i}S_{i}(x)P_{i}(x).\] (44)

Then, we have:

\[\max_{i}\lambda(1-S_{i}(x))P_{i}(x)\geq\max_{i}S_{i}(x)P_{i}(x),\] (46)

\[\lambda\geq\frac{\max_{i}S_{i}(x)P_{i}(x)}{\max_{i}(1-S_{i}(x))P_{i}(x)}.\] (47)

Therefore, \(P(S_{\hat{y}(x)}(x)=0)=\eta\) can be converted to:

\[P\left(\lambda\geq\frac{\max_{i}\,S_{i}(x)P_{i}(x)}{\max_{i}\,(1-S_{i}(x))P_{i }(x)}\right)=\eta.\] (48)

It means that \(\lambda\) is the \(\eta\)-quantile of

\[\left\{\frac{\max_{i}S_{i}(x)P_{i}(x)}{\max_{i}(1-S_{i}(x))P_{i}(x)}\right\}_ {x\in\mathcal{D}}.\] (49)

[MISSING_PAGE_EMPTY:17]