# Local to Global: Learning Dynamics and Effect of Initialization for Transformers

 Ashok Vardhan Makkuva

EPFL

&Marco Bondaschi

EPFL

&Chanakya Ebote

EPFL

&Adway Girish

EPFL

&Alliot Nagle

UT Austin

&Hyeji Kim

UT Austin

&Michael Gastpar

EPFL

###### Abstract

In recent years, transformer-based models have revolutionized deep learning, particularly in sequence modeling. To better understand this phenomenon, there is a growing interest in using Markov input processes to study transformers. However, our current understanding in this regard remains limited with many fundamental questions about how transformers learn Markov chains still unanswered. In this paper, we address this by focusing on first-order Markov chains and single-layer transformers, providing a comprehensive characterization of the learning dynamics in this context. Specifically, we prove that transformer parameters trained on next-token prediction loss can either converge to global or local minima, contingent on the initialization and the Markovian data properties, and we characterize the precise conditions under which this occurs. To the best of our knowledge, this is the first result of its kind highlighting the role of initialization. We further demonstrate that our theoretical findings are corroborated by empirical evidence. Based on these insights, we provide guidelines for the initialization of single-layer transformers and demonstrate their effectiveness. Finally, we outline several open problems in this arena. Code is available at: https://github.com/Bond1995/Markov.

## 1 Introduction

Transformers have been at the forefront of recent successes across various fields including natural language processing [34]. To obtain insights into their impressive sequential modeling capabilities, a notable emerging theme among several recent works is to model the input data as a Markov process.

Using this Markovian perspective, works such as [24, 14, 8], among others, study the in-context learning capabilities of transformer. [23] analyzes the loss-landscape for the next-token prediction task, while [18] shows an equivalence between the attention mechanism and Markov models. Although these works reveal interesting insights about transformers and their capabilities, many fundamental questions about their learning dynamics remain unanswered. In particular, a comprehensive characterization of their training dynamics vis-a-vis the data distributional properties and the role of initialization is still missing.

To address this gap, in this paper, we focus on the canonical setting of first-order Markov chains and single-layer transformers and analyze the learning dynamics in this context. Specifically, we prove (Thms. 2, 3, and 8) that the input data properties and the parameter initialization play a significant role in the convergence of the transformer parameters to either local or global minima on the loss surface. Further, we precisely characterize (Figs. 1 and Fig. 2) the specific data characteristics and the region of initialization under which this convergence occurs. Based on these insights, we provide guidelinesfor the initialization of transformer parameters and empirically corroborate our theoretical findings. On the theoretical front, our analysis provides a novel gradient flow analysis of the transformer parameters, capitalizing on their low-rank structure during training. Our main contributions can be summarized as follows:

* _Theoretical analysis:_ We precisely characterize the loss landscape and gradient flow dynamics for single-layer transformers with first-order Markov chains (Secs. 3 and 4). We demonstrate that transformer parameters trained on next-token prediction loss can converge to global or local minima, depending on the initialization and the Markovian data properties, and determine the exact conditions under which this occurs (Thms. 2, 3, and 8). To the best of our knowledge, this is the first result of its kind.
* _Insights into initialization:_ Our theoretical analysis underscores the crucial role of initialization in transformer parameter training. Specifically, we demonstrate how the standard Gaussian initialization scheme can lead the convergence to local or global minima depending on the Markovian data properties (Thms. 2 and 8, Figs. 1 and 2).
* _Guidelines:_ Based on these insights, we provide practical guidelines for parameter initialization, corroborated by empirical evidence demonstrating their effectiveness (Sec. 5.2).

**Notation.** We denote scalars by italic lower case letters like \(x,y\) and Euclidean vectors and matrices in bold: \(\bm{x},\bm{y},\bm{M}\), etc. \(\|\cdot\|\) denotes the \(\ell_{2}\)-norm for Euclidean vectors and Frobenius norm for matrices. \([k]\triangleq\{1,\ldots,k\}\), and for a sequence \((x_{n})_{n\geq 1}\), define \(x_{k}^{m}\triangleq(x_{k},\ldots,x_{m})\) if \(k\geq 1\) and \((x_{1},\ldots,x_{m})\) otherwise. For \(z\in\mathbb{R}\), the sigmoid \(\sigma(z)\triangleq 1/(1+e^{-z})\), \(\operatorname{ReLU}(z)\triangleq\max(0,z)\) and the convex logistic loss \(\ell_{\log}(z)\triangleq\log\left(1+\exp(-z)\right)\in(0,\infty)\). For events \(A\) and \(B\), \(\mathbb{P}\left(A\right)\) denotes the probability of \(A\) whereas \(\mathbb{P}\left(A\mid B\right)\) the conditional probability. Let \((x,y)\) be a pair of discrete random variables on \([k]\times[k]\) with the probability mass function (pmf) of \(x\) being \(\bm{p}_{x}=(p_{1},\ldots,p_{k})\in[0,1]^{k}\). Then its Shannon entropy is defined as \(H(x)=H(\bm{p}_{x})\triangleq-\sum_{i\in[k]}p_{i}\log p_{i}\). The conditional entropy is defined to be \(H(y|x)\triangleq H(x,y)-H(x)\). The entropy rate of a stochastic process \((x_{n})_{n\geq 1}\) is defined as \(\lim_{n\rightarrow\infty}H(x_{n}^{n})/n\). We simply write \(x=y\) to mean \(\mathbb{P}\left(x=y\right)=1\). We also use the shorthand \(\mathbb{P}\left(y=j\mid x\right)\) for \(\mathbb{P}\left(y=j\mid x=x\right)\) as a function of the random variable \(x\). For \(p\in(0,1)\), the binary entropy function \(h(\cdot)\) is defined as \(h(p)\triangleq-p\log p-(1-p)\log(1-p)\).

## 2 Problem Setting

We formally define the problem setting for analysis of single-layer transformers with Markovian data, following [23].

**Input data.** We assume that the input word sequence \(\{x_{n}\}_{n=1}^{N}\in\{0,1\}^{N}\) is a first-order time-homogenous Markov chain with a fixed kernel \(\bm{P}=(\bm{P}_{ij})\). That is, the transition probability

Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. \((p,q)\) are Markov switching probabilities, and \((e,w)\) are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b), (d): \(\mathcal{I}_{*}\) is the basin of convergence for global minima, \(\mathcal{I}_{\min}\) for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for \(p+q\lessgtr 1\).

\(\bm{P}_{ij}\triangleq\mathbb{P}\left(x_{n+1}=j\mid x_{n}=i\right)=\mathbb{P}\left(x _{n+1}=j\mid x_{n}=i,\,x_{n}^{n-1}\right)\), for any \(x_{n}^{n-1},i,j\in\{0,1\}\). In particular, we consider \(\bm{P}=[1-p,p;\,q,1-q]\) where \(p=\bm{P}_{01}=\mathbb{P}\left(x_{n+1}=1\mid x_{n}=0\right)\) and \(q=\bm{P}_{10}=\mathbb{P}\left(x_{n+1}=0\mid x_{n}=1\right)\) denote the switching probabilities from the states \(0\) and \(1\) respectively. We call \(p+q\), the _switching factor_. We assume that the process is already mixed, i.e. \(x_{n}\sim\bm{\pi}\) for all \(n\), where \(\bm{\pi}\triangleq(\pi_{0},\pi_{1})=(q,p)/(p+q)\) is the stationary distribution satisfying \(\bm{\pi}=\bm{\pi}\bm{P}\). Succinctly, \((x_{n})_{n\geq 1}\sim(\bm{\pi},\bm{P})\). For this process, the entropy rate, \(H(x_{n+1}|x_{n})=\frac{1}{p+q}\left(q\,h(p)+p\,h(q)\right)\), and the entropy of the marginal, \(H(x_{n})=H(\bm{\pi})\), are both constant in \(n\).

**Transformer architecture.** We consider a single-layer transformer with a single-head attention and ReLU non-linearity. Given an input sequence \(\{x_{n}\}_{n=1}^{N}\), it performs the following mathematical operations at each \(n\in[N]\) to predict the next-token probability \(f_{\bm{\theta}}(x_{1}^{n})\):

\[x_{n}\in\{0,1\}\xrightarrow{\text{Embedding}}\bm{x}_{n}\xrightarrow{\text{ Attention}}\bm{y}_{n}\xrightarrow{\text{Feed-forward}}\bm{z}_{n}\xrightarrow{\text{Linear}}\text{logit}_{n} \xrightarrow{\text{Prediction}}f_{\bm{\theta}}(x_{1}^{n}),\]

where

\[\bm{x}_{n} =x_{n}\,\bm{e}+\bm{p}_{n}\in\mathbb{R}^{d},\] (Embedding) \[\bm{y}_{n} =\bm{x}_{n}+\sum_{i\in[n]}\operatorname*{\mathrm{att}}_{n,i} \cdot\bm{W}_{V}\,\bm{x}_{i}\in\mathbb{R}^{d},\] (Attention) \[\bm{z}_{n} =\bm{y}_{n}+\bm{W}_{2}\operatorname{ReLU}(\bm{W}_{1}\,\bm{y}_{n}) \in\mathbb{R}^{d},\] (Feed-forward) \[\operatorname{logit}_{n} =\langle\bm{a},\bm{z}_{n}\rangle+b \in\mathbb{R}^{\cdot}\] (Linear) \[f_{\bm{\theta}}(x_{1}^{n}) \triangleq\mathbb{P}_{\bm{\theta}}\left(x_{n+1}=1\mid x_{1}^{n} \right)=\sigma(\operatorname{logit}_{n})\in[0,1].\] (Prediction)

Here \(\bm{\theta}\triangleq(\bm{e},\{\bm{p}_{n}\}_{n=1}^{N},\ldots,\bm{W}_{1},\bm{W} _{2},b,\bm{a})\in\mathbb{R}^{D}\) denotes the full list of the transformer parameters from the embedding layer till the linear layer (SS A details them). While the underlying data \(\{x_{n}\}_{n=1}^{N}\) is Markovian, i.e. \(\mathbb{P}\left(x_{n+1}=1\mid x_{1}^{n}\right)=\mathbb{P}\left(x_{n+1}=1\mid x _{n}\right)\), the transformer is agnostic to this fact and it can potentially utilize the full past \(x_{1}^{n}\) in the Attention layer, via the attention weights \(\operatorname{att}_{n,i}\), to predict the next-symbol probability \(f_{\bm{\theta}}(x_{1}^{n})=\mathbb{P}_{\bm{\theta}}\left(x_{n+1}=1\mid x_{1}^{ n}\right)\). Note that it suffices to estimate the symbol \(1\) probability as the vocabulary is binary. We also refer to the above architecture as "full model".

**Loss and training.** The transformer parameters \(\bm{\theta}\) are usually initialized according to standard Gaussian distribution \(\mathcal{N}(0,\sigma^{2}\bm{I})\) with a small variance \(\sigma^{2}\)[26] and are trained using gradient-based methods to minimize the cross-entropy loss on the next-token prediction, i.e.

\[\min_{\bm{\theta}}L(\bm{\theta}),\quad L(\bm{\theta})\triangleq-\frac{1}{N}\sum _{n\in[N]}\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\cdot\log f_{\bm{\theta}}(x_{1}^{n })\,+(1-x_{n+1})\cdot\log(1-f_{\bm{\theta}}(x_{1}^{n}))].\] (1)

When the input sequence \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi},\bm{P})\), the minimal loss equals its entropy-rate, i.e. \(L_{\star}\triangleq\min_{\bm{\theta}}L(\bm{\theta})=H(x_{n+1}|x_{n})\)[11].

**Loss landscape.** A key surprising observation in [23] is that the loss function \(L(\cdot)\) admits both the global and local minima depending on the switching factor \(p+q\) of the Markovian data, and the weight-tying of the embedding and linear weights (\(\bm{e}=\bm{a}\)) of the transformer. In particular, they show that

1. for all \((p,q)\in(0,1)^{2}\), there exists a global minimum \(\bm{\theta}_{\star}\) for the loss \(L\) such that its prediction matches the Markov kernel, i.e. \(\mathbb{P}_{\bm{\theta}_{\star}}\left(x_{n+1}=1\mid x_{1}^{n}\right)=\mathbb{P} \left(x_{n+1}=1\mid x_{n}\right)\).
2. if \(p+q>1\) and the weights are tied (\(\bm{e}=\bm{a}\)), there exists a bad local minimum \(\bm{\theta}_{\min}\) for \(L\) whose prediction equals the marginal, i.e. \(\mathbb{P}_{\bm{\theta}_{\star}}\left(x_{n+1}=1\mid x_{1}^{n}\right)=\mathbb{P} \left(x_{n+1}=1\right)\).

In view of these results, we focus on the weight-tying scenario and hence let \(\bm{e}=\bm{a}\) to be a single parameter in \(\mathbb{R}^{d}\). Thus, \(\bm{\theta}=(\bm{e}=\bm{a},\{\bm{p}_{n}\}_{n=1}^{N},\ldots,\bm{W}_{1},\bm{W}_{2},b)\). We interchangeably refer to \(\bm{\theta}\) as both the transformer and the set of parameters.

**Our objective.** While the aforementioned results detail the static landscape of the loss, they do not characterize the learning dynamics on the loss surface and the effect of initialization, which plays a central role in training machine learning models [5]. In view of these shortcomings, the main objective of this paper is to address the following question:

1. _Can we explain how the initialization and learning dynamics affect the convergence of the transformer parameters_ \(\bm{\theta}\) _to the local or global optima?_

## 3 Canonical Low-rank Parameterization

**Motivation.** Given the complexity of the transformer architecture and the non-convex loss function, it is challenging to analyze the learning dynamics directly [24; 14]. To tackle this, we capitalize on the following empirical observation [23] which is the motivating idea behind our approach: when trained by gradient-based methods, the weight matrices \((\boldsymbol{W}_{V},\ldots,\boldsymbol{W}_{1},\boldsymbol{W}_{2})\) at the optima \(\boldsymbol{\theta}_{\star}\) and \(\boldsymbol{\theta}_{\min}\) exhibit _rank-one_ structure, whose eigenvector is the same direction in which the both the token embedding \(\boldsymbol{e}\) and the positional embeddings \(\boldsymbol{p}_{n}\) are all aligned in. Interestingly, such low-rank solutions can also be shown to be theoretically optimal (SS B details these structures). While these observations illustrate the implicit bias towards low-rank solutions at the final convergence, a natural question arises: _if we initialize with low-rank parameters, will they remain low-rank during training?_ In Sec. 5.1, we affirmatively address this based on a thorough empirical evaluation for single-layer transformers and inspired by these empirical phenomena, without loss of generality, we restrict our attention to these low-rank manifolds to characterize the learning dynamics. This is similar in spirit to [24], where they assume special attention matrix structure for learning induction heads.

**Parameterization.** More specifically, we consider a special low-rank parameterization that is empirically observed and capitalize on it to address _(Q.1)_. Interestingly, along this low-rank manifold, it suffices to consider a reduced set of parameters \(\boldsymbol{\theta}\in\mathbb{R}^{2}\) or \(\boldsymbol{\theta}\in\mathbb{R}^{3}\) given by:

\[\boldsymbol{\theta}=(e,w)\in\mathbb{R}^{2},\text{ or }\boldsymbol{\theta}=(e,w,a) \in\mathbb{R}^{3}.\] (Reparameterization)

Here \(e\) denotes the _embedding_ scalar, \(w\) the _weight_, and \(a\) the _attention_ parameter respectively. Now we describe the parameterization of the transformer vis-a-vis these scalars and refer to SS C for a more detailed descripton. Let the input \(\left\{x_{n}\right\}_{n=1}^{N}\) be a first-order Markov chain as in Sec. 2 and let \(n\in[N]\) be fixed. Then we have

\[\text{\sc Embedding}:\boldsymbol{e}=e\cdot\boldsymbol{\alpha}, \boldsymbol{p}_{n}=\left(-\frac{e}{2}\right)\cdot\boldsymbol{\alpha} \rightarrow\boldsymbol{x}_{n}=e\left(x_{n}-\frac{1}{2}\right)\boldsymbol{ \alpha},\quad e\in\mathbb{R},\boldsymbol{\alpha}\in\{\pm 1\}^{d}/\sqrt{d},\] \[\text{\sc Attention}:\boldsymbol{W}_{V}=\boldsymbol{\alpha}\, \boldsymbol{v}^{\top}\rightarrow\boldsymbol{y}_{n}=e\left(x_{n}-\frac{1}{2} \right)\boldsymbol{\alpha}+\underbrace{\langle\boldsymbol{v},\boldsymbol{ \alpha}\rangle}_{\propto a\approx 0}\left(\sum_{i\in[n]}\operatorname{att}_{n,i} \cdot e\left(x_{i}-\frac{1}{2}\right)\right)\boldsymbol{\alpha},\boldsymbol{ v}\in\mathbb{R}^{d}.\]

The scalar \(a\) is the product of \(\langle\boldsymbol{v},\boldsymbol{\alpha}\rangle\) and the scaling in the attention weights \(\operatorname{att}_{n,i}\) (Eq. (39)), which is empirically close to zero for first-order Markov chains. Hence for the ease of exposition, we first let \(a=0\) and analyze the general case when \(a\in\mathbb{R}\) in Sec. 4.1. We continue:

\[\text{\sc Feed-forward}:\boldsymbol{W}_{1}=\frac{|w|}{\sqrt{d}} \,\boldsymbol{1}\,\boldsymbol{\alpha}^{\top},\boldsymbol{W}_{2}=\frac{w}{ \sqrt{d}}\,\boldsymbol{\alpha}\,\boldsymbol{1}^{\top}\rightarrow\boldsymbol{ z}_{n}=e\left(x_{n}-\frac{1}{2}\right)\left(1+4w|w|x_{n}\right)\boldsymbol{\alpha},w\in \mathbb{R}.\]

\(\boldsymbol{1}\) is the all-one vector in \(\mathbb{R}^{r}\) with \(r=4d\) typically in practice. Substituting this \(\boldsymbol{z}_{n}\) in the linear layer with \(\boldsymbol{e}=\boldsymbol{a}\) and bias \(b\in\mathbb{R}\), the logits and the probabilities simplify to:

\[\text{\sc Linear}:\operatorname{logit}_{n}(e,w,b) =e^{2}(1+2w|w|)\,x_{n}+b-\frac{e^{2}}{2}\in\mathbb{R},\] (2) \[\text{\sc Prediction}:f_{(\boldsymbol{\theta},b)}(x_{1}^{n}) =\sigma\left(\operatorname{logit}_{n}\right)\in(0,1),\quad \boldsymbol{\theta}\triangleq(e,w).\] (3)

Finally, using the equivalence between the cross-entropy loss and the logistic loss \(\ell_{\log}(\cdot)\), the loss function in Eq. (1) can be compactly written as (Lemma 6):

\[L(\boldsymbol{\theta},b)=\frac{1}{N}\sum_{n\in[N]}\mathbb{E}[ \ell_{\log}\left((2x_{n+1}-1)\cdot\operatorname{logit}_{n}(\boldsymbol{\theta })\right)],\quad\boldsymbol{\theta}\in\mathbb{R}^{2},b\in\mathbb{R}.\] (4)

Due to convexity of \(\ell_{\log}(\cdot)\), it follows that \(L(\boldsymbol{\theta},b)\) is convex in the bias \(b\) for any fixed \(\boldsymbol{\theta}\), whose minimizer, \(b_{\star}(\boldsymbol{\theta})=\operatorname{argmin}_{b\in\mathbb{R}}L( \boldsymbol{\theta},b)\), has a closed form expression (Lemma 5). Hence, without loss of generality, we consider the loss with this optimal bias \(b_{\star}\):

\[L(\boldsymbol{\theta})\triangleq L(\boldsymbol{\theta},b_{\star})=\frac{1}{N} \sum_{n\in[N]}\mathbb{E}\left[\ell_{\log}\left((2x_{n+1}-1)\left(e^{2}(1+2w|w| )\,x_{n}+b_{\star}-\frac{e^{2}}{2}\right)\right)\right].\] (5)Empirically, this roughly translates to running the gradient-based algorithm for the bias for more steps at each \(\bm{\theta}\). In practice, one additional step is usually sufficient (see Sec. 5). Eq. (5) resembles the standard logistic regression loss [31] whose binary labels are \(2x_{n+1}-1\in\{\pm 1\}\) and the logits given by \(e^{2}(1+2w|w)\,x_{n}+b_{\star}-e^{2}/2\), for each \(n\in[N]\). The key difference here is that the logits are a non-linear function of the parameters \((e,w)\) unlike in the standard setting.

We briefly summarize our assumptions below.

**Assumption 1** (Canonical parameterization).: For our theoretical analysis, we assume that the effective transformer parameters are canonically parameterized as \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\). First we study the scenario when \(a=0\) with \(\bm{\theta}=(e,w)\) and build upon these observations to study the general setting of \(\bm{\theta}=(e,w,a)\) in Sec. 4.1.

### Loss Landscape with Canonical Parameterization

With the new set of parameters \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\), we are now ready to analyze the loss \(L(\cdot)\) in Eq. (5). First we recall the definition of a critical point [20]. A point \(\bm{\theta}_{\star}\in\mathbb{R}^{2}\) is a critical or a stationary point for \(L\) if \(\nabla L(\bm{\theta}_{\star})=0\). A critical point \(\bm{\theta}_{\star}\) is a _local minimum_ if there exists a neighborhood \(U\) around \(\bm{\theta}_{\star}\) such that \(L(\bm{\theta}_{\star})\leq L(\bm{\theta})\) for all \(\bm{\theta}\in U\), and a _local maximum_ if \(L(\bm{\theta}_{\star})\geq L(\bm{\theta})\). If the neighborhood \(U\) is whole of \(\mathbb{R}^{2}\), it is a _global minimum/maximum_. On the other hand, a critical point is a saddle point if for all neighborhoods \(U\) around \(\bm{\theta}_{\star}\), there are \(\bm{\theta}_{1},\bm{\theta}_{2}\in U\) such that \(L(\bm{\theta}_{1})\leq L(\bm{\theta}_{\star})\leq L(\bm{\theta}_{2})\).

Thm. 1 below provides a complete characterization of the loss landscape in terms of the aforementioned critical points.

**Theorem 1** (All critical points).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi},\bm{P})\), the transformer parameters \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\), and the next-token prediction loss \(L(\cdot)\) be as in Eq. (5). Then for any \((p,q)\in(0,1)^{2}\) with \(p+q\neq 1\) and \(N\in\mathbb{N}\),_

1. _the set of all global minima is given by_ \[\bm{\Theta}_{\star}(p,q)\triangleq\left\{(e,w)\in\mathbb{R}^{2}:e^{2}(1+2w|w |)=\log\frac{(1-p)(1-q)}{pq}\right\},\] (6)
2. _the set of all local minima is given by_ \[\bm{\Theta}_{\min}(p,q)\triangleq\left\{(e,w)\in\mathbb{R}^{2}:e=0,\,(p+q-1) (1+2w|w|)>0\right\},\] (7)
3. _the set of all local maxima is given by_ \[\bm{\Theta}_{\max}(p,q)\triangleq\left\{(e,w)\in\mathbb{R}^{2}:e=0,\,(p+q-1) (1+2w|w|)<0\right\},\] (8)
4. _and the set of all saddle points is_ \[\bm{\Theta}_{\mathrm{sad}}(p,q)\triangleq\left\{(0,-1/\sqrt{2})\right\}.\] (9)

_Thus the set of all critical points is_

\[\left\{\bm{\theta}\in\mathbb{R}^{2}:\nabla L(\bm{\theta})=0\right\}=\bm{ \Theta}_{\star}\cup\bm{\Theta}_{\min}\cup\bm{\Theta}_{\max}\cup\bm{\Theta}_{ \mathrm{sad}}.\] (10)

_In addition, for any \(\bm{\theta}_{\star}\in\bm{\Theta}_{\star},\bm{\theta}_{\min}\in\bm{\Theta}_{ \min}\), \(\bm{\theta}_{\max}\in\bm{\Theta}_{\max}\), and \(\bm{\theta}_{\mathrm{sad}}\in\bm{\Theta}_{\mathrm{sad}}\), the loss values satisfy_

\[H(x_{n+1}\mid x_{n})=L(\bm{\theta}_{\star})<L(\bm{\theta}_{\min})=L(\bm{\theta}_ {\max})=L(\bm{\theta}_{\mathrm{sad}})=H(x_{n+1}).\]

Proof.: We refer to SS E. 

Fig. 1 illustrates the loci of these critical points for \(p+q<1\) and \(p+q>1\). Motivated by empirical observations, while [23] characterizes local minima for \(p+q>1\), it is interesting to note that our Thm. 1 shows that local minima also exist for \(p+q<1\) (Eq. (7) and Fig. 2(a)). So why did they find the minima only when \(p+q>1\)? The answer to this, and more broadly to question _(Q.1)_ lies in the learning dynamics and initialization for \(\bm{\theta}\), which we study in the next section.

Learning Dynamics

Capitalizing on the loss landscape in terms of the critical points in Thm. 1, we now focus on the convergence of gradient-based algorithms to these points. In this regard, we analyze the dynamics of the gradient-flow (GF), which can be viewed as a continuous-time analogue of gradient-descent [6]. The gradient-flow of the parameters, \((\bm{\theta}_{t})_{t\geq 0}\), on \(L\) is governed by

\[\frac{\mathrm{d}\bm{\theta}_{t}}{\mathrm{d}t}=-\nabla L(\bm{\theta}_{t}),\quad \bm{\theta}_{t}=(e_{t},w_{t})\in\mathbb{R}^{2},\,t\geq 0,\] (GF)

where \(\bm{\theta}_{t}\triangleq\bm{\theta}(t)\) is a \(C^{1}\) (continuously differentiable) curve in \(\mathbb{R}^{2}\) starting with a randomly initalized \(\bm{\theta}_{0}\). To characterize these trajectories, we define an _energy function_\(\mathcal{E}(\cdot,\cdot)\), which plays a crucial in the GF dynamics. It is defined as

\[\mathcal{E}(e,w)\triangleq e^{2}-(w^{2}+\mathrm{sign}(w)\cdot\log|w|),\quad \forall(e,w)\in\mathbb{R}^{2}\setminus\mathrm{e-axis},\] (11)

where \(\mathrm{e-axis}\triangleq\{(e,w=0)\}\). Note that \(\mathcal{E}\) is well-defined and finite for all the points in its domain. On the other hand, \(\lim_{w\to 0^{-}}\mathcal{E}(e,w)=-\infty\) whereas \(\lim_{w\to 0^{+}}\mathcal{E}(e,w)=\infty\) for any fixed \(e\in\mathbb{R}\). Thus the \(\mathrm{e-axis}\) corresponding to \(w=0\) serves as an energy barrier for the flow. Figs. 2(a) and 2(b) illustrate this by visualizing the energy contour lines. The utility of the energy function is captured in the following lemma.

**Lemma 1** (Constant energy along the flow).: _For any \((p,q)\in(0,1)^{2}\) and initialization \(\bm{\theta}_{0}=(e_{0},w_{0})\in\mathbb{R}^{2}\), let \((\bm{\theta}_{t})_{t\geq 0}\) be the corresponding GF trajectory starting from \(\bm{\theta}_{0}\). If \(\bm{\theta}_{0}\in\mathbb{R}^{2}\setminus\mathrm{e-axis}\), the energy stays constant along the trajectory, i.e._

\[\mathcal{E}(\bm{\theta}_{t})=e_{t}^{2}-(w_{t}^{2}+\mathrm{sign}(w_{t})\cdot \log|w_{t}|)=\mathcal{E}(\bm{\theta}_{0}),\quad\forall t\geq 0.\] (12)

_On the other hand, if \(\bm{\theta}_{0}\in\mathrm{e-axis}\), we have that \(\bm{\theta}_{t}\in\mathrm{e-axis}\) for all \(t\geq 0\) with \(w_{t}=w_{0}=0\), i.e. if we initialize on the \(\mathrm{e-axis}\), the trajectory always stays there._

We are now ready to present the main results of our paper. Specifically, Thm. 2 and Thm. 8 highlight the role of the switching factor of the Markovian data, \(p+q\), and the parameter initialization, \(\bm{\theta}_{0}\), in deciding whether the GF converges to local optima or global optima. First we define the energy value \(\mathcal{E}_{\mathrm{sad}}\triangleq\mathcal{E}(e=0,w=-1/\sqrt{2})=-(1+\log 2 )/2\).

**Theorem 2** (GF dynamics for \(p+q>1\)).: _Let \((p,q)\in(0,1)^{2}\) with \(p+q>1\), the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi},\bm{P})\), and \((\bm{\theta}_{t})_{t\geq 0}\) be the corresponding GF trajectory starting from \(\bm{\theta}_{0}\). Then for all initializations \(\bm{\theta}_{0}\in\mathbb{R}^{2}\), the gradient flow converges to a critical point of the loss \(L\). That is, there exists a \(\bm{\theta}_{\mathrm{lim}}\in\mathbb{R}^{2}\) such that \(\lim_{t\to\infty}\bm{\theta}_{t}=\bm{\theta}_{\mathrm{lim}}\) and \(\nabla L(\bm{\theta}_{\mathrm{lim}})=0\). In particular, \(\bm{\theta}_{\mathrm{lim}}\) is a_

1. _a local minimum if_ \[\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{min}}\triangleq\Big{\{}(e,w):w\in(-1/\sqrt{2},0),\,e\in(-g(w),g(w)),\,g(w)=\sqrt{w^{2}-\log(-w)+ \mathcal{E}_{\mathrm{sad}}}\Big{\}}\] \[\cup\{(e,w):w\geq 0\}\,,\]
2. _a saddle point if_ \(\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{sad}}\triangleq\Big{\{}(e,w):w\in[-1/ \sqrt{2},0),\,e=\pm\sqrt{w^{2}-\log(-w)+\mathcal{E}_{\mathrm{sad}}}\Big{\}}\)_,_
3. _a local maximum if_ \(\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{max}}\triangleq\big{\{}(e,w):e=0,\,w<- 1/\sqrt{2}\big{\}}\)_,_
4. _and a global minimum if_ \(\bm{\theta}_{0}\in\mathcal{I}_{\star}\triangleq\mathbb{R}^{2}\setminus( \mathcal{I}_{\mathrm{min}}\cup\mathcal{I}_{\mathrm{sad}}\cup\mathcal{I}_{ \mathrm{max}})\)_._

_Consequently, when \(p+q>1\), if we use the standard initialization \(\bm{\theta}_{0}\sim\mathcal{N}(0,\sigma^{2}\bm{I}_{2})\) with \(\sigma^{2}\ll 1/\sqrt{2}\), \(\bm{\theta}_{\mathrm{lim}}\) will be a local minimum with high probability. If \(p+q<1\), under the same initialization scheme, \(\bm{\theta}_{\mathrm{lim}}\) will be a global minimum with high probability._

Proof sketch.: The main idea behind the proof is to show that if we do not initialize on the \(\mathrm{e-axis}\), the flows stays on the constant energy contour (Lemma 1) and hence converges to a critical point of the loss \(L\), which is at the intersection of the contour line and the set of critical points (Lemmas. 10 and 11). By determining where these intersections occur, the corresponding basins of convergence \(\mathcal{I}_{\mathrm{min}},\ldots,\mathcal{I}_{\star}\) are obtained by showing that an initialization in a specific set leads to the said critical point (Thm. 1). The proof for \(\bm{\theta}_{0}\in\mathrm{e-axis}\) is similar.

Figs. (b)b and (d)d illustrate these initialization sets corresponding to the convergence basins for \(p=q=0.9\) and \(p=q=0.1\) respectively. An analogous result about GF dynamics for \(p+q<1\) is presented in Thm. 8 (SS F.2). Here a key difference is that small Gaussian initialization around origin leads to a global minimum \(\bm{\theta}_{\mathrm{lim}}\) with high probability (Fig. (d)d).

**Key insights.** Together, Thm. 2 and Thm. 8 address our motivating question _(Q.1)_ by fully characterizing the GF dynamics in terms of initialization and input data properties. Specifically, our results explain the phenomenon in [23] wherein they observe local minima for \(p+q>1\) more often than for \(p+q<1\), owing to standard Gaussian initialization around origin (Figs. (b)b and (d)d). However, in practice, we often do not know the input switching factor, raising a natural questions: _is there a data-agnostic initialization that always converges to global minima?_ Indeed, as can be seen from Figs. (b)b and (d)d, there is a common region of initialization in the negative half-plane above the saddle-asymptotes (in yellow) that leads to the global minima convergence irrespective of the switching \(p+q\). Mathematically, this region is given by \(\mathcal{I}_{\mathrm{common}}\triangleq\{(e,w):w<0,|e|>\sqrt{w^{2}-\log(-w)+ \mathcal{E}_{\mathrm{sad}}}\}\). We empirically corroborate this fact in Sec. 5.2.

### Gradient Flow with Attention

In this section, the consider the attention scalar \(a\in\mathbb{R}\) (Sec. 3) and study the gradient flow dynamics with the parameters \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\). The parameter \(a\) captures the overall scaling from the value, key, and query components in the attention layer. Recall that the soft-max attention weights are given by \(\mathrm{att}_{n,i}\propto\exp(\langle\bm{q}_{n},\bm{k}_{i}\rangle/\sqrt{d})\), where \(\bm{q}_{n}=\bm{W}_{Q}\bm{x}_{n}\) and \(\bm{k}_{i}=\bm{W}_{K}\bm{x}_{i}\) are the query and key embeddings for any position \(i\in[n]\). Using the low-rank structure of the query and key matrices, satisfying \(\bm{W}_{Q}^{\top}\bm{W}_{K}=(q^{2}d)\,\bm{\alpha}\bm{\alpha}^{\top}\) and the value matrix \(\bm{W}_{V}=\bm{\alpha}\bm{v}^{\top}\) for some \(q\in\mathbb{R}\) and \(\bm{v}\in\mathbb{R}^{d}\) (SS G), and assuming linear attention \(\mathrm{att}_{n,i}\propto\langle\bm{q}_{n},\bm{k}_{i}\rangle/\sqrt{d}\), we define a single scalar \(a\triangleq\langle\bm{v},\bm{\alpha}\rangle q^{2}d^{5/2}/4\) that captures the essence of the attention layer (Eq. (39)). We note that linear attention weights are a standard assumption in the transformer analysis literature [3, 36]. Using this parameterization, similar to the steps in Sec. 3, we obtain the final loss function to be

\[L(\bm{\theta})=\mathbb{E}\left[\ell_{\log}\left((2Y-1)\left(e^{2}\left[\left( X-\frac{1}{2}\right)\left(1+ae^{2}\right)(1+2w|w|)+w|w(1+ae^{2})\right]\right]+b_{ \star}\right)\right)\right],\]

where \(\bm{\theta}=(e,w,a)\) and \(b_{\star}\) is the corresponding optimal bias. \(L\) recovers the loss in Eq. (5) when \(a=0\). In Thm. 10, we determine the set of all critical points of \(L\) in terms of global minima and local optima in closed-form expressions, analogous to Thm. 1. Capitalizing on this characterization, we now shift our focus to the analysis of the gradient flow in \(\mathbb{R}^{3}\). To this end, let \((\bm{\theta}_{t})_{t\geq 0}\) be a \(C^{1}\) curve in \(\mathbb{R}^{3}\) governed by

\[\frac{\mathrm{d}\bm{\theta}_{t}}{\mathrm{d}t}=-\nabla L(\bm{\theta}_{t}),\quad \bm{\theta}_{t}=(e_{t},w_{t},a_{t})\in\mathbb{R}^{3},\,t\geq 0,\] (GF-attn)

starting with a randomly initalized \(\bm{\theta}_{0}\). We define the _energy function_\(\mathcal{E}(\cdot,\cdot,\cdot)\) as

\[\mathcal{E}(e,w,a)\triangleq e^{2}-(w^{2}+\mathrm{sign}(w)\cdot\log|w|)-2a^{2 },\quad\forall(e,w,a)\in\mathbb{R}^{3}\setminus\text{ea-plane},\] (13)

Figure 2: Gradient flow dynamics for the canonical parameters \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\) with the attention scalar \(a\). Notice the contrasting behavior for Gaussian initialization around origin for \(p+q\) smaller and greater than one. For an enhanced view of the flow near the origin, please refer to Fig. 5.

where \(\mathrm{ea}\mathrm{-plane}\triangleq\{(e,w=0,a)\}\). It is similar to its counterpart in Eq. (11), except for the \(2a^{2}\) term. Fig. 2 visualizes this energy surface and the set of critical points, which reveal close resemblance to that of Fig. 1 in \(\mathbb{R}^{2}\). Capitalizing on the energy function, we now present our main result with the attention.

**Theorem 3** (GF dynamics with attention).: _For any \((p,q)\in(0,1)^{2}\) and initialization \(\bm{\theta}_{0}\in\mathbb{R}^{3}\), let \((\bm{\theta}_{t})_{t\geq 0}\) be the corresponding GF-attn trajectory starting from it. Then for all \(\bm{\theta}_{0}\in\mathbb{R}^{3}\), the gradient flow converges to a critical point of the loss \(L\). That is, there exists a \(\bm{\theta}_{\mathrm{lim}}\in\mathbb{R}^{3}\) such that \(\lim_{t\to\infty}\bm{\theta}_{t}=\bm{\theta}_{\mathrm{lim}}\) and \(\nabla L(\bm{\theta}_{\mathrm{lim}})=0\). Further,_

1. _[label=_()_]_
2. _if_ \(\bm{\theta}_{0}\in\mathbb{R}^{3}\setminus\mathrm{ea}\mathrm{-plane}\)_, we have_ \(\mathcal{E}(\bm{\theta}_{\mathrm{lim}})=\mathcal{E}(\bm{\theta}_{t})= \mathcal{E}(\bm{\theta}_{0})\) _for all_ \(t\geq 0\)_. Hence_ \(\bm{\theta}_{\mathrm{lim}}\) _is at the intersection of the energy contour line_ \(\mathcal{E}=\mathcal{E}_{0}\) _with that of the set of critical points._
3. _if_ \(\bm{\theta}_{0}\in\mathrm{ea}\mathrm{-plane}\)_, we have_ \(\bm{\theta}_{t}\in\mathrm{ea}\mathrm{-plane}\) _for all_ \(t\geq 0\) _and hence_ \(\bm{\theta}_{\mathrm{lim}}\in\mathrm{ea}\mathrm{-plane}\)_._

Proof.: We refer to SS G and SS N.4. 

Thm. 3 shows that the learning dynamics with attention closely resemble those without it (Thms. 2 and 8). While the set of all critical points of \(L\), and thus the limit points of the flow, has a closed-form expression (Thm. 10), deriving the same for the initialization sets \(\mathcal{I}_{\mathrm{min}}\) and \(\mathcal{I}_{\star}\) to determine the basin of convergence is technically challenging (see discussion in SS G). Nonetheless, empirical observations with the standard Gaussian initialization around origin reveal a similar picture as in the two-dimensional setting for both the \(p+q<1\) and \(p+q>1\) cases (Fig. 2). We believe it's an interesting direction of future research to theoretically characterize this, analogous to Thms. 2 and 8. We refer to SS G for additional details and proofs.

## 5 Empirical Results

We empirically validate our canonical parameterization \(\bm{\theta}\in\mathbb{R}^{3}\) (Sec. 3) by demonstrating full model convergence to low-rank parameters through both qualitative and quantitative evidence. Qualitatively, we visualize weight matrices across iterations; quantitatively, we plot the percentage of energy captured by the top-rank components across iterations. We then demonstrate the generalization of our theoretical findings on local optima and initialization with canonical parameters to the full model \(\bm{\theta}\in\mathbb{R}^{D}\). We conclude with a discussion on higher-order and multi-state Markov chains.

### Low-rank Parameters

**Full model converges to low-rank.** We let the input Markov sequence to be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi}(p,q),\bm{P}(p,q))\) for \(p=0.2,q=0.3,N=1024\) and consider the full model as defined in Sec. 2 with embedding dimension \(d=8\). First, we initialize the parameters \(\bm{\theta}=(\bm{e}=\bm{a},\bm{\{p_{n}\}}_{n=1}^{N},\dots,\bm{W}_{1},\bm{W}_{2 },b)\) using the standard Gaussian initialization with standard deviation \(0.001\)[26] and train them using SGD on a batch size \(B=16\) and for \(t=800\) iterations. In Fig. 6, we track the value matrix \(\bm{W}_{V}\in\mathbb{R}^{d\times d}\) and the weight matrix \(\bm{W}_{1}\in\mathbb{R}^{d4\times d}\) across iterations. We observe that at convergence both \(\bm{W}_{V}\) and \(\bm{W}_{1}\) are approximately rank-one with one of their components being same as the embedding vector (the row in \(\bm{W}_{V}\) and column in \(\bm{W}_{1}\)). Further, the embedding vector has all entries in \(\{\pm 1\}\) up to a scaling. We observe the same conclusion for other weight matrices \(\bm{W}_{K,Q},\bm{W}_{2}\) and for all values of \((p,q)\in(0,1)^{2}\). Fig. 3 also quantitatively demonstrates this.

**Full model initialized at low-rank remains low-rank during training.** Inspired by the low-rank structure obtained above, we randomly initialize the weight parameters as rank-one matrices and the embeddings on the hypercube \(\{\pm 1\}^{d}\). After the initialization, we train them without any low-rank restrictions, and track them during the course of training. Interestingly, here we observe that the parameters still stay low-rank as illustrated in Fig. 7 and Fig. 3. A similar conclusion holds for the remaining weight matrices. Together these results provide the empirical basis for our canonical parameterization analysis in Sec. 3.

### Effect of Initialization: Broader Implications

Now we investigate the findings of Sec. 3 and Sec. 4, derived for the canonical low-rank model, more broadly in the context of full model in Sec. 2. In particular, as shown in Thm. 2 and Fig. 1dfor \(p+q>1\), any small initialization around zero would lead a local minima convergence. To test this hypothesis, we compare the standard initialization where all the transformer parameters \(\bm{\theta}=(\bm{e}=\bm{a},\{\bm{p}_{n}\}_{n=1}^{N},\ldots,\bm{W}_{1},\bm{W}_{2},b)\) are randomly chosen around zero with small variance \(\sigma=0.02\), with a new initialization based on our results, where we initialize the embedding vector \(\bm{e}\) such that all ordinates are equal to \(e=0.5\), \(\bm{W}_{1}\) to be constant with the scalar \(w_{1}=1\) and \(\bm{W}_{2}\) constant with \(w_{2}=-1\) (corresponding to \(\mathcal{I}_{\star}\) in Fig. 1d). We indeed observe that the final test loss matches the unigram loss for the standard initialization, while it converges to the optimal bigram loss for our initialization (see Fig. 4). Together these results indicate that though our analysis used canonical parameterization, the corresponding insights are more general and apply more broadly to the general full model. In a similar spirit, analysis of initialization effects for deeper architectures is an interesting avenue of future research.

### Higher-Order and Multi-State Markov Chains

While the primary focus of this paper has been on binary first-order Markov chains, we believe it's possible to extend our analysis to both multi-state and higher-order settings. On the multi-state front, akin to the binary case, [23] already demonstrates the effect of switching probability and weight-tying on the final model convergence. Here, first characterizing the loss landscape and then the associated learning dynamics in line with our approach is an interesting direction. On the other hand, a recent work [28] establishes a surprising result that any \(k^{\text{th}}\)-order Markov chain can be represented by a

Figure 4: Comparison between the average loss curve for the standard gaussian initialization around \(0\) and our initialization, for \(p=0.5\) and \(q=0.8\). Starting from the standard initialization, the model converges to a local minimum corresponding to the unigram model. With our initialization, it converges to the global minimum corresponding to the bigram model.

Figure 3: Convergence to rank one parameters: percentage of energy contained in the first rank component of the weight matrices \(\bm{W}_{1}\) and \(\bm{W}_{V}\) across iterations. The percentage is computed as \(\frac{\sigma_{1}^{2}}{\sum_{i}\sigma_{i}^{2}}\), where the \(\sigma_{i}\)â€™s are the singular values of the matrices in descending order.

three layer transformer with just one head per layer, relying on induction head mechanism. Analyzing gradient flow dynamics using appropriate canonical parameterization (cf. [24]) in this scenario is also a fruitful direction of research.

## 6 Related Works

The recent success of transformer models in deep learning has sparked significant interest and active research in understanding them [38; 25; 16; 27; 15; 37; 40; 32]. In relation to our paper, they can be broadly classified into two topics: (i) **In-context learning (ICL):** ICL refers to the ability of transformers learn and reason from information present in their context [10; 13; 4; 35; 39; 7; 21; 17]. Along this thread, the works most relevant to ours are [8; 14; 24], which use Markovian input data to understand the ICL mechanism. [8; 14] heuristically show how gradient-based updates can learn an induction-head mechanism using a simplified transformer architecture with frozen encodings, query matrices and linear activations. On the other hand, we consider the canonical parameterization, capitalizing on inherent low-rank parameters, to provide a full characterization of the learning dynamics. [24] demonstrates how two-layer transformers with GD learn induction head mechanism when the input has a causal tree dependency, such as in Markov chains. In this work, we focus on the GF dynamics for single-layer transformers and show how they can also converge to local optima, further highlighting the role of initialization. (ii) **Training dynamics:** On the other hand, numerous works have investigated the training dynamics of transformers. For instance, [9] examines the gradient flow in a simplified single-layer transformer, while [33] studies the process by which self-attention integrates input tokens, assuming the decoder learns faster than the attention layer. Unlike these settings, our focus is on understanding the training dynamics of the full transformer model end-to-end. Other related works include [30], which analyzes gradient dynamics in LSTM Seq2seq models, [19], which shows how Vision Transformers learn spatial structures, and [22], which demonstrates that a single-layer transformer can learn a constrained topic model. A closely related work is [18], which shows that self-attention has a Markovian structure, but our focus is on self-attention's capability in modeling Markov chains and the associated training dynamics.

## 7 Conclusion

In this work, we present a novel characterization of gradient flow dynamics for (weight-tied) single-layer transformers with first-order Markov chains. Specifically, we highlight the significant role of the parameter initialization and inherent properties of the Markovian data in determining the parameter convergence to either global minima or local optima. Drawing upon these insights, we offer practical guidelines for parameter initialization, corroborated by empirical results demonstrating their effectiveness. While our current analysis is limited to single-layer models, uncovering similar results with gradient flow analysis for deeper architectures and higher order Markov chains is open and an interesting avenue for future research.

## Acknowledgments and Disclosure of Funding

Ashok would like to thank Aditya Vardhan Varre for many helpful discussions about the project. This work was supported in part by the Swiss National Science Foundation under Grant 200364.

## References

* Absil et al. [2005] Pierre-Antoine Absil, Robert Mahony, and Ben Andrews. Convergence of the iterates of descent methods for analytic cost functions. _SIAM Journal on Optimization_, 16(2):531-547, 2005.
* Ahmadova [2023] Arzu Ahmadova. Convergence results for gradient flow and gradient descent systems in the artificial neural network training. _arXiv preprint arXiv:2306.13086_, 2023.
* Ahn et al. [2023] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=LziniAXEI9.

* Akyurek et al. [2023] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models. In _The Eleventh International Conference on Learning Representations_, 2023.
* Arora et al. [2019] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks, 2019.
* Bach [2020] Francis Bach. Effortless optimization through gradient flows. _https://francisbach.com/gradient-flows/_, 2020.
* Bai et al. [2023] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In _Workshop on Efficient Systems for Foundation Models @ ICML2023_, 2023.
* Bietti et al. [2023] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Chandra et al. [2024] Pritam Chandra, Tanmay Kumar Sinha, Kabir Ahuja, Ankit Garg, and Navin Goyal. Towards analyzing self-attention via linear neural network, 2024. URL https://openreview.net/forum?id=4fVuBf5HE9.
* Chen et al. [2024] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality, 2024.
* Cover and Thomas [2006] Thomas M Cover and Joy A Thomas. _Elements of information theory_. John Wiley & Sons, 2nd edition, 2006.
* Danskin [1966] John M Danskin. The theory of max-min, with applications. _SIAM Journal on Applied Mathematics_, 14(4):641-664, 1966.
* Dong et al. [2023] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023. URL https://arxiv.org/abs/2301.00234.
* Edelman et al. [2024] Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains, 2024.
* Elhage et al. [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. URL https://transformer-circuits.pub/2021/framework/index.html.
* Fu et al. [2023] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? A study through the random features lens. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Garg et al. [2022] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? A case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* Ildiz et al. [2024] M Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From self-attention to markov models: Unveiling the dynamics of generative transformers. _arXiv preprint arXiv:2402.13512_, 2024.
* Jelassi et al. [2022] Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Lee et al. [2016] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In _Conference on learning theory_, pages 1246-1257. PMLR, 2016.

* Li et al. [2023] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: generalization and stability in in-context learning. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Li et al. [2023] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding, 2023.
* Makhuxa et al. [2024] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with Markov: A framework for principled analysis of transformers via Markov chains. _arXiv preprint arXiv:2402.04161_, 2024.
* Nichani et al. [2024] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. _arXiv preprint arXiv:2402.14735_, 2024.
* Oymak et al. [2023] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* Pagliardini [2023] Matteo Pagliardini. GPT-2 modular codebase implementation. _https://github.com/epfml/llm-baselines_, 2023.
* Perez et al. [2021] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is Turing-complete. _Journal of Machine Learning Research_, 22(75):1-35, 2021.
* Rajaraman et al. [2024] Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, and Ashok Vardhan Makkuva. Transformers on markov data: Constant depth suffices, 2024. URL https://arxiv.org/abs/2407.17686.
* Shapiro [1985] Alexander Shapiro. Second-order derivatives of extremal-value functions and optimality conditions for semi-infinite programs. _Mathematics of Operations Research_, 10(2):207-219, 1985.
* Snell et al. [2021] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. _CoRR_, abs/2103.07601, 2021. URL https://arxiv.org/abs/2103.07601.
* Soudry et al. [2018] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _Journal of Machine Learning Research_, 19(70):1-57, 2018.
* Tarzanagh et al. [2023] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Tian et al. [2023] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Shaolei Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. In _Conference on Parsimony and Learning (Recent Spotlight Track)_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, pages 5998-6008, 2017.
* Ooswald et al. [2023] Johannes Von Ooswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174, 2023.
* Oswald et al. [2023] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2023.
* Wei et al. [2022] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating Turing machines with transformers. In _Advances in Neural Information Processing Systems_, volume 35, pages 12071-12083, 2022.
* Weiss et al. [2021] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In _International Conference on Machine Learning_, pages 11080-11090, 2021.

* [39] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit Bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021. URL https://arxiv.org/abs/2111.02080.
* [40] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In _International Conference on Learning Representations_, 2020.

###### Contents

* 1 Introduction
* 2 Problem Setting
* 3 Canonical Low-rank Parameterization
	* 3.1 Loss Landscape with Canonical Parameterization
* 4 Learning Dynamics
	* 4.1 Gradient Flow with Attention
* 5 Empirical Results
	* 5.1 Low-rank Parameters
	* 5.2 Effect of Initialization: Broader Implications
	* 5.3 Higher-Order and Multi-State Markov Chains
* 6 Related Works
* 7 Conclusion
* A Single-layer transformer: architecture and results
* A.1 Loss landscape results
* B Low-rank structure of the optima
* C Canonical reparameterization
* D Analysis of the loss with the bias, \(L(\theta,b)\), in Eq. (4) and Eq. (23)
* D.1 Technical lemmas
* E Analysis of the loss without bias, \(L(\theta)\), and proof of Thm. 1
* E.1 Proof of Thm. 1
* F Gradient flow analysis without attention
* F.1 Proof of Thm. 2
* F.2 Gradient flow dynamics for \(p+q<1\)
* G Gradient flow analysis with attention
* G.1 Canonical parameterization with attention
* G.2 Analysis of the loss function \(L(\theta)\) from Eq. (42)
* G.3 Gradient flow analysis
* G.4 Role of standard initialization
* H Additional empirical results

H.1 Gaussian initialization converges to low-rank H.2 Low-rank initialization stays low-rank
* I Model architecture and hyper-parameters
* J Proofs of theorems in App. D
* J.1 Proof of Thm. 7
* J.2 Proof of Thm. 6
* K Proofs of technical lemmas in App. D
* K.1 Proof of Lemma 2
* K.2 Proof of Lemma 3
* K.3 Proof of Lemma 4
* L Proofs of lemmas in App. E
* L.1 Proof of Lemma 5
* L.2 Proof of Lemma 6
* L.3 Proof of Lemma 7
* L.4 Proof of Lemma 8
* M Proofs of lemmas in App. F
* M.1 Proof of Lemma 9
* M.2 Proof of Lemma 10
* M.3 Proof of Lemma 11
* M.4 Proof of Lemma 12
* N Proofs of lemmas in App. G
* N.1 Proof of Lemma 13
* N.2 Proofs of Thm. 9 and Thm. 10
* N.3 Proof of Lemma 14
* N.4 Proofs of Lemma 16, Lemma 15, and Thm. 3
* N.5 Informal proof of Thm. 11

## Appendix A Single-layer transformer: architecture and results

We first describe the transformer architecture from Sec. 2:

\[\bm{x}_{n} =x_{n}\,\bm{e}+\bm{p}_{n}\in\mathbb{R}^{d},\] (Embedding) \[\bm{y}_{n} =\bm{x}_{n}+\sum_{i\in[n]}\operatorname{att}_{n,i}\cdot\bm{W}_{V} \,\bm{x}_{i}\in\mathbb{R}^{d},\] (Attention) \[\bm{z}_{n} =\bm{y}_{n}+\bm{W}_{2}\operatorname{ReLU}(\bm{W}_{1}\,\bm{y}_{n} )\in\mathbb{R}^{d},\] (Feed-forward) \[\operatorname{logit}_{n} =\langle\bm{a},\bm{z}_{n}\rangle+b\qquad\qquad\qquad\in\mathbb{ R}^{\cdot}\] (Linear) \[f_{\bm{\theta}}(\bm{x}_{1}^{n}) \triangleq\mathbb{P}_{\bm{\theta}}\,(x_{n+1}=1\mid x_{1}^{n})= \underbrace{\sigma(\operatorname{logit}_{n})}_{\in[0,1]}.\] (Prediction)

Here \(\bm{\theta}\triangleq(\bm{e},\{\bm{p}_{n}\}_{n=1}^{N},\ldots,\bm{W}_{1},\bm{ W}_{2},b,\bm{a})\in\mathbb{R}^{D}\) denotes the full list of the transformer parameters from the embedding layer till the linear layer. In the attention layer, the weight assigned to each value, \(\operatorname{att}_{n,i}\), is computed by a compatibility function of the query vector \(\bm{q}_{n}\triangleq\bm{W}_{Q}\,\bm{x}_{n}\) and the corresponding key vectors \(\bm{k}_{i}\triangleq\bm{W}_{K}\,\bm{x}_{i}\) for all \(i\in[n]\). More precisely, \(\operatorname{att}_{n,i}\triangleq\operatorname{softmax}((\langle\bm{q}_{n}, \bm{k}_{1}\rangle,\ldots,\langle\bm{q}_{n},\bm{k}_{n}\rangle)/\sqrt{d})\). \(\bm{W}_{K,Q,V}\in\mathbb{R}^{d\times d}\) are the respective key, query, and value matrices. For multi-headed attention, the same operation is performed on multiple parallel heads, whose outputs are additively combined.

Finally, the transformer parameters \(\bm{\theta}\triangleq(\bm{e},\{\bm{p}_{n}\}_{n=1}^{N},\ldots,b,\bm{a})\) are trained via the cross-entropy loss on the next-token prediction:

\[L(\bm{\theta})\triangleq-\frac{1}{N}\sum_{n\in[N]}\mathbb{E}_{x_{1}^{n+1}}[x_ {n+1}\cdot\log f_{\bm{\theta}}(x_{1}^{n})+(1-x_{n+1})\cdot\log(1-f_{\bm{ \theta}}(x_{1}^{n}))].\] (14)

In this paper, we focus on the weight-tied scenario where \(\bm{e}=\bm{a}\). Hence we let them be a single parameter with \(\bm{\theta}=(\bm{e}=\bm{a},\{\bm{p}_{n}\}_{n=1}^{N},\ldots,b)\in\mathbb{R}^{D}\), where \(D\) is the total parameter dimensionality.

### Loss landscape results

Now we recall the theoretical results from [23] about the loss landscape of \(L\) in the form of global and local minima.

**Theorem 4** (Global minimum).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi}(p,q),\bm{P}(p,q))\) for some fixed \((p,q)\in(0,1)^{2}\). Then for all \((p,q)\), there exists a \(\bm{\theta}_{\star}\in\mathbb{R}^{D}\) with an explicit construction such that it is a global minimum for the population loss \(L(\cdot)\) in Eq. (14), i.e._

1. \(L(\bm{\theta})\geq L(\bm{\theta}_{\star})\) _for all_ \(\bm{\theta}\in\mathbb{R}^{D}\)_._

_Further, \(\bm{\theta}_{\star}\) satisfies:_

1. \(\mathbb{P}_{\bm{\theta}_{\star}}\)__\((x_{n+1}=1\mid x_{1}^{n})=\mathbb{P}\,(x_{n+1}=1\mid x_{n})\)_, the Markov kernel._
2. \(L(\bm{\theta}_{\star})=H(x_{n+1}|x_{n})\)_, the entropy rate of the Markov chain._
3. \(\nabla L(\bm{\theta}_{\star})=0\)_, i.e._ \(\bm{\theta}_{\star}\) _is a stationary point._

Let \(L_{\star}\triangleq L(\bm{\theta}_{\star})\) be the global minimal loss from Thm. 4. Now we recall the result on the bad local minimum.

**Theorem 5** (Bad local minimum).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi}(p,q),\bm{P}(p,q))\) for some fixed \((p,q)\in(0,1)^{2}\). If \(p+q>1\), there exists an explicit \(\bm{\theta}_{\min}\in\mathbb{R}^{D}\) such that it is a bad local minimum for the loss \(L(\cdot)\), i.e._

1. _there exists a neighborhood_ \(\mathcal{B}(\bm{\theta}_{\min},r)\) _with_ \(r>0\) _such that_ \(L(\bm{\theta})\geq L(\bm{\theta}_{\min})\) _for all_ \(\bm{\theta}\in\mathcal{B}(\bm{\theta}_{\min},r)\)_, with_ \(L(\bm{\theta}_{\min})>L_{\star}\)_._

_Further, \(\bm{\theta}_{\min}\) satisfies:_

1. \(\mathbb{P}_{\bm{\theta}_{\star}}\,(x_{n+1}=1\mid x_{1}^{n})=\mathbb{P}\,(x_{n +1}=1)=\pi_{1}\)_, the marginal distribution._
2. \(L(\bm{\theta}_{\min})=H(x_{n+1})=H(\bm{\pi})\)_, the entropy of the marginal._
3. \(\nabla L(\bm{\theta}_{\min})=0\)_, i.e._ \(\bm{\theta}_{\min}\) _is a stationary point._Low-rank structure of the optima

Here we recall the low-rank structure for the global minima found by SGD consistently across multiple runs when \(p+q<1\)[23, Appendix C.2]. In particular, it is observed that the token and positional encodings point in the same direction \(\bm{\alpha}\), which is a low-rank factor for the weight matrices in the attention and the feedforward layers, which in turn are all rank-one. Mathematically,

**Embedding.** The embedding vector \(\bm{e}\) obeys

\[\bm{e}=e\cdot\bm{\alpha}\]

for some \(e>0\) and \(\bm{\alpha}\in\{\pm 1\}^{d}\). Further, the positional embeddings \(\bm{p}_{n}\) are constant across positions \(n\) pointing in the same direction albeit with a negative scalar, i.e.

\[\bm{p}_{n}=-p\cdot\bm{\alpha}\]

for \(p>0\) and \(p\approx\frac{e}{2}\) such that \(e>p\). Thus from Embedding layer,

\[\bm{x}_{n}=(ex_{n}-p)\cdot\bm{\alpha},\] (15)

which ensures that the respective embeddings for the bit \(x_{n}=0\) and \(x_{n}=1\) are \(\bm{x}_{n}=-p\cdot\bm{\alpha}\) and \(\bm{x}_{n}=(e-p)\cdot\bm{\alpha}\), which are roughly anti-podal.

**Attention.** Recall from the Attention layer that the output \(\bm{y}_{n}\) is given by \(\bm{y}_{n}=\bm{x}_{n}+\bm{W}_{O}\sum_{i\in[n]}\operatorname{att}_{n,i}\cdot \bm{W}_{V}\bm{x}_{i}\), where the attention weights \(\operatorname{att}_{n,i}\) are computed according to \(\operatorname{att}_{n,i}=\exp\left(\langle\bm{q}_{n},\bm{k}_{i}\rangle/\sqrt{ d}\right)/\left(\sum_{j\in[n]}\exp\left(\langle\bm{q}_{n},\bm{k}_{j}\rangle/ \sqrt{d}\right)\right)\) with \(\bm{q}_{n}=\bm{W}_{Q}\,\bm{x}_{n}\) and \(\bm{k}_{i}=\bm{W}_{K}\,\bm{x}_{i}\). Here it is observed that the matrix products are all rank-one with \(\bm{\alpha}\) being a factor, i.e.

\[\bm{W}_{O}\bm{W}_{V} =\bm{\alpha}\cdot\bm{v}^{\top}\in\mathbb{R}^{d\times d},\quad \text{for some}\,\bm{v}\in\mathbb{R}^{d},\] \[\bm{W}_{Q}^{\top}\bm{W}_{K} =(q^{2}d)\,\bm{\alpha}\cdot\bm{\alpha}^{\top}\in\mathbb{R}^{d \times d},\quad\text{for some}\,q\in\mathbb{R}.\]

Hence,

\[\bm{W}_{V}\,\bm{x}_{i}=\langle\bm{v},\bm{\alpha}\rangle(ex_{i}-p)\,\bm{\alpha},\]

and

\[\frac{\langle\bm{q}_{n},\bm{k}_{i}\rangle}{\sqrt{d}} =\frac{1}{\sqrt{d}}\cdot\bm{x}_{n}^{\top}\bm{W}_{Q}^{\top}\bm{W} _{K}\bm{x}_{n}=\frac{q^{2}d}{\sqrt{d}}\cdot(\bm{x}_{n}^{\top}\bm{\alpha})(\bm{ x}_{i}^{\top}\bm{\alpha})\overset{(\|\bm{\alpha}\|^{2}=d)}{=}\frac{q^{2}d^{3}}{\sqrt{d}} \cdot(ex_{n}-p)(ex_{i}-p)\] \[=q^{2}d^{5/2}\cdot(ex_{n}-p)(ex_{i}-p).\]

Thus,

\[\bm{y}_{n} =\bm{x}_{n}+\sum_{i\in[n]}\operatorname{att}_{n,i}\cdot\bm{W}_{O }\bm{W}_{V}\,\bm{x}_{i}\] \[=(ex_{i}-p)\,\bm{\alpha}+\sum_{i\in[n]}\operatorname{att}_{n,i} \cdot\langle\bm{v},\bm{\alpha}\rangle(ex_{i}-p)\,\bm{\alpha}\] \[=\left((ex_{n}-p)+\langle\bm{v},\bm{\alpha}\rangle\sum_{i\in[n]} \frac{\exp\left(q^{2}d^{5/2}\,(ex_{n}-p)(ex_{i}-p)\right)}{\sum_{j\in[n]}\exp \left(q^{2}d^{5/2}\,(ex_{n}-p)(ex_{j}-p)\right)}\cdot(ex_{i}-p)\right)\bm{ \alpha}.\] (16)

It is further noticed that \(\langle\bm{v},\bm{\alpha}\rangle\approx 0\) and hence \(\bm{y}_{n}=(ex_{n}-p)\,\bm{\alpha}=\bm{x}_{n}\).

**Feed-forward.** For the Feed-forward layer, both the matrices \(\bm{W}_{1}\in\mathbb{R}^{r\times d}\) and \(\bm{W}_{2}\in\mathbb{R}^{d\times r}\) exhibit rank-one structure with \(\bm{\alpha}\) being one of the factors,

\[\bm{W}_{1} =w\cdot\bm{w}\cdot\bm{\alpha}^{\top},\quad\text{for some}\,\bm{w }\in\{\pm 1\}^{r},w>0,\] (17) \[\bm{W}_{2} =\bm{W}_{1}^{\top}.\] (18)

Thus \(\bm{W}_{1}\bm{y}_{n}=dw(ex_{n}-p)\,\bm{w}\). Since \(-p<0\) and \(e-p>0\), corresponding to \(x_{n}=0\) and \(x_{n}=1\) respectively, we obtain \(\operatorname{ReLU}(\bm{W}_{1}\bm{y}_{n})=dw\left((1-x_{n})p\cdot\operatorname{ ReLU}(-\bm{w})+x_{n}(e-p)\cdot\operatorname{ReLU}(\bm{w})\right)\). Denoting the number of ones in \(\bm{w}\) as \(\beta\), i.e. \(\beta=\sum_{i=1}^{r}\mathds{1}(w_{i}=1)\), we further simplify:

\[\bm{W}_{2}\operatorname{ReLU}(\bm{W}_{1}\bm{y}_{n})=\bm{W}_{1}^{\top} \operatorname{ReLU}(\bm{W}_{1}\bm{y}_{n})\]\[=w^{2}d\left((1-x_{n})p\cdot\left<\bm{w},\mathrm{ReLU}(-\bm{w}) \right>+x_{n}(e-p)\cdot\left<\bm{w},\mathrm{ReLU}(\bm{w})\right>\right)\bm{\alpha}\] \[=w^{2}d\left((1-x_{n})p\cdot\left(\beta-r\right)+x_{n}(e-p)\cdot \beta\right)\bm{\alpha}\] \[=w^{2}d(ex_{n}-p)\left((2\beta-r)x_{n}+r-\beta\right)\bm{\alpha}.\]

Hence

\[\bm{z}_{n}=\bm{y}_{n}+\bm{W}_{2}\mathrm{ReLU}(\bm{W}_{1}\bm{y}_{n} )=(ex_{n}-p)\left(1+w^{2}d\left((2\beta-r)x_{n}+r-\beta\right)\right)\bm{ \alpha}.\] (19)

**Linear.** Using the fact that \(\bm{e}=\bm{a}=e\cdot\bm{\alpha}\) due to weight-tying, we obtain from Linear layer that

\[\mathrm{logit}_{n}=\left<\bm{e},\bm{z}_{n}\right>+b=ed(ex_{n}-p) \left(1+w^{2}d\left((2\beta-r)x_{n}+r-\beta\right)\right)+b.\] (20)

**Prediction.** We finally obtain that the prediction probability

\[\bm{f}_{\bm{\theta}}(x_{1}^{n})=\sigma(\mathrm{logit}_{n})=x_{n} \cdot\sigma\left(ed(e-p)\left(1+\beta w^{2}d\right)+b\right)+(1-x_{n})\cdot \sigma\left(-edp\left(1+(r-\beta)w^{2}d\right)+b\right).\]

Thus we see that the prediction probability and hence the loss function \(L(\cdot)\) in Eq. (14) is influenced only by the scalars \(e,p,w,b\) and \(\beta\).

## Appendix C Canonical reparameterization

Building on the low-rank strucutre of the transformer parameters described above, we consider a special parameterization for them. A key property of this parameterization is that it covers both the global and local minima from Thm. 4 and Thm. 5 for all \((p,q)\in(0,1)^{2}\). Recall that Thm. 5 characterizes local minima only for \(p+q>1\) whereas our special parameterization allows to discover local minima even for \(p+q<1\). Our construction follows the same outline as in Eqs. (15)-(20). First we start with the embedding layer.

**Embedding.** We let \(\bm{e}=e\cdot\bm{\alpha}\) and \(\bm{p}_{n}=-p\cdot\bm{\alpha}\) for all \(n\) where \(e>0,p=\frac{e}{2}\) and \(\bm{\alpha}\in\{\pm 1\}^{d}/\sqrt{d}\). Thus the embedding \(\bm{x}_{n}\) from Eq. (15) simplifies to

\[\bm{x}_{n}=e\left(x_{n}-\frac{1}{2}\right)\bm{\alpha}\in\{\pm \frac{e}{2}\}\,\bm{\alpha}.\]

**Attention.** Substituting this \(\bm{x}_{n}\) in Eq. (16), we have

\[\bm{y}_{n}=e\left(x_{n}-\frac{1}{2}\right)\bm{\alpha}+\left<\bm{ v},\bm{\alpha}\right>\left(\sum_{i\in[n]}\mathrm{att}_{n,i}\cdot e\left(x_{i}- \frac{1}{2}\right)\right)\bm{\alpha},\] (21)

where the attention weights \(\mathrm{att}_{n,i}=\frac{\exp\left(e^{2}q^{2}d^{5/2}\left(x_{n}-\frac{1}{2} \right)(x_{i}-\frac{1}{2})\right)}{\sum_{j\in[n]}\exp\left(e^{2}q^{2}d^{5/2} \left(x_{n}-\frac{1}{2}\right)(x_{j}-\frac{1}{2})\right)}\in(0,1)\) for some \(q\in\mathbb{R}\). Since \(\left<\bm{v},\bm{\alpha}\right>\approx 0\), we let \(\bm{v}=0\) and obtain

\[\bm{y}_{n}=\bm{x}_{n}=e\left(x_{n}-\frac{1}{2}\right)\bm{\alpha}.\]

**Feed-forward**. For the feed-forward layer, we observe from Eq. (17) and Eq. (19) that for any \(\bm{w}\in\{\pm 1\}^{r}\), only the number of \(1\)'s in \(\bm{w}\), \(\beta\), matters for the final vector \(\bm{z}_{n}\) which further interacts with the weight scalar \(w\). Hence without loss of generality, we set \(\bm{w}\) to be the all-ones vector: \(\bm{w}=\bm{1}\in\mathbb{R}^{r}\) and hence \(\beta=r=4d\). While we observe from Eq. (17) that \(\bm{W}_{2}=\bm{W}_{1}^{\top}\) for \(p+q<1\), we observe from the proof of the Thm. 4 for \(p+q>1\) in [23, Appendix B.2] that we need \(\bm{W}_{2}=-\bm{W}_{1}^{\top}\) in this scenario. Hence we consider the following parameterization that covers both these scenarios:

\[\bm{W}_{1}=\frac{|w|}{\sqrt{d}}\,\bm{1}\cdot\bm{\alpha}^{\top}\in \mathbb{R}^{4d\times d},\quad\bm{W}_{2}=\frac{w}{\sqrt{d}}\,\bm{\alpha}\cdot \bm{1}^{\top}\in\mathbb{R}^{d\times 4d}.\]

Here \(w>0\) ensures \(\bm{W}_{2}=\bm{W}_{1}^{\top}\) whereas \(w<0\), \(\bm{W}_{2}=-\bm{W}_{1}^{\top}\). Using this parameterization, substituting \(\beta=r=4d\) and \(w\mapsto\frac{w}{d}\) in Eq. (19), we get

\[\bm{z}_{n}=e\left(x_{n}-\frac{1}{2}\right)\left(1+4w|w|x_{n}\right)\bm{\alpha}.\]

**Linear.** Since \(\bm{e}=\bm{a}=e\cdot\bm{\alpha}\) due to weight-tying, Eq. (20) simplifies to

\[\mathrm{logit}_{n} =\langle\bm{e},\bm{z}_{n}\rangle+b=e^{2}\left(x_{n}-\frac{1}{2} \right)(1+4w|w|x_{n})+b\] \[\overset{(x_{n}=x_{n}^{2})}{=}e^{2}\left(x_{n}+4w|w|x_{n}-\frac{1 }{2}-2w|w|x_{n}\right)+b\] \[=e^{2}(1+2w|w|)\,x_{n}+b-\frac{e^{2}}{2}.\]

**Prediction.** The next-token prediction probability is

\[f_{(\bm{\theta},b)}(x_{1}^{n})=\sigma\left(e^{2}(1+2w|w|)\,x_{n}+b-\frac{e^{2} }{2}\right),\quad\bm{\theta}\triangleq(e,w)\in\mathbb{R}^{2}.\] (22)

**Loss.** While we assumed \(e>0\) in the beginning, in view of Eq. (22) and the fact that \(\bm{\alpha}\in\{\pm 1\}^{d}/\sqrt{d}\), we see that \(e\in\mathbb{R}\) gives us the same expression for probability. Thus the final probability depends on just the three scalars \((e,w,b)\in\mathbb{R}^{3}\). Defining \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\), we recall the cross-entropy loss \(L(\cdot)\) from Eq. (4) in Sec. 2 for this canonical model:

\[L(\bm{\theta},b)=-\frac{1}{N}\sum_{n\in[N]}\mathbb{E}_{x_{1}^{n+1}}[x_{n+1} \cdot\log f_{(\bm{\theta},b)}(x_{1}^{n})\,+(1-x_{n+1})\cdot\log(1-f_{(\bm{ \theta},b)}(x_{1}^{n}))].\] (23)

It turns out that we can further remove the bias \(b\) by minimizing the loss over it which we discuss in App. E. For now in the next section, we analyze when it's present as in Eq. (23).

Analysis of the loss with the bias, \(L(\bm{\theta},b)\), in Eq. (4) and Eq. (23)

In this section, we analyze the loss function with the bias, \(L(\bm{\theta},b)\), from Eq. (4) and Eq. (23), which will later be useful for studying \(L(\bm{\theta})\). First we characterize the set of its critical points in \(\mathbb{R}^{3}\). To this end, we define the following sets of points

\[\bm{\Gamma}_{\star}(p,q) \triangleq\left\{(e,w,b)\in\mathbb{R}^{3}:e^{2}(1+2w|w|)=\log \frac{(1-p)(1-q)}{pq},\,b-\frac{e^{2}}{2}=\log\frac{p}{1-p}\right\},\] (24) \[\bm{\Gamma}_{\min}(p,q) \triangleq\left\{(e,w,b)\in\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)>0,b=\log\frac{p}{q}\right\},\] (25) \[\bm{\Gamma}_{\mathrm{sad}}(p,q) \triangleq\left\{(e,w,b)\in\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|) \leq 0,b=\log\frac{p}{q}\right\}.\] (26)

The following result establishes that these sets exhaust all the critical points.

**Theorem 6** (All critical points).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi},\bm{P})\), the transformer parameters \((\bm{\theta},b)=(e,w,b)\in\mathbb{R}^{3}\), and the next-token prediction loss \(L(\cdot)\) be as in Eq. (23). Then all the stationary points of \(L\) are either in \(\bm{\Gamma}_{\star}\), \(\bm{\Gamma}_{\min}\), or \(\bm{\Gamma}_{\mathrm{sad}}\), i.e._

\[\left\{(\bm{\theta},b)\in\mathbb{R}^{3}:\nabla L(\bm{\theta},b)=0\right\}=\bm {\Gamma}_{\star}\cup\bm{\Gamma}_{\min}\cup\bm{\Gamma}_{\mathrm{sad}}.\] (27)

Proof.: We refer to App. J.1. 

Recall the definitions of local minima & maxima, global minima, and that of all the saddle points from Sec. 3.1. We are now ready to present the main result about the loss landscape of \(L(\cdot)\).

**Theorem 7** (Loss landscape with bias).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi},\bm{P})\), the transformer parameters \((e,w,b)\in\mathbb{R}^{3}\), and the next-token prediction loss \(L(\cdot)\) be as in Eq. (23). Then for any \((p,q)\in(0,1)^{2}\) with \(p+q\neq 1\) and \(N\in\mathbb{N}\),_

1. _[label=()]_
2. _the set of all global minima of_ \(L\) _is given by_ \(\bm{\Gamma}_{\star}(p,q)\)_,_
3. _the set of all bad local minima of_ \(L\) _is given by_ \(\bm{\Gamma}_{\min}(p,q)\)_,_
4. _and the set of all saddle points of_ \(L\) _is_ \(\bm{\Gamma}_{\mathrm{sad}}(p,q)\)_._

_Furthermore, for any \(\bm{\gamma}_{\star}\in\bm{\Gamma}_{\star},\bm{\gamma}_{\min}\in\bm{\Gamma}_{ \min}\), and \(\bm{\gamma}_{\mathrm{sad}}\in\bm{\Gamma}_{\mathrm{sad}}\), the losses are ordered as_

\[H(x_{n+1}\mid x_{n})=L(\bm{\gamma}_{\star})<L(\bm{\gamma}_{\min})=L(\bm{ \gamma}_{\mathrm{sad}})=H(x_{n+1}).\]

**Remark 1**.: Note that a bad local minimum is a local minimum whose loss value is strictly less than that of the global minimum, as is the case here. Interestingly, Thm. 7 highlights that all local minima for the loss \(L\) are indeed bad local minima.

Proof.: We refer to App. J.2. 

### Technical lemmas

The proofs of both Thm. 6 and Thm. 7 rely on few key lemmas that we present below. First we start with the result that rewrites the loss \(L(\bm{\theta},b)\) from Eq. (23) in a compact manner using the logistic function \(\ell_{\log}(\cdot)\).

**Lemma 2** (Loss as a logistic function).: _The next-token prediction loss \(L(\cdot)\) in Eq. (23) can be written as_

\[L(\bm{\theta},b) =\frac{1}{N}\sum_{n\in[N]}\mathbb{E}[\ell_{\log}\left((2x_{n+1}- 1)\cdot\mathrm{logit}_{n})]\] (28) \[=\mathbb{E}_{X,Y}\left[\ell_{\log}\left((2Y-1)\left(e^{2}(1+2w|w| )X+b-\frac{e^{2}}{2}\right)\right)\right],\]

_where \((X,Y)\in\{0,1\}^{2}\) are distributed according to \((X,Y)\sim(\bm{\pi},\bm{P})\), i.e. \(X\) is a Bernoulli random variable with \(X\sim\bm{\pi}\equiv\mathrm{Bern}(p/(p+q))\) and \(Y|X\sim\bm{P}(p,q)\), the Markov kernel._The following lemma establishes the gradients of the loss function with respect to the parameters \(e,w,\) and \(b\).

**Lemma 3** (Gradient computation).: _For any \((e,w,b)\in\mathbb{R}^{3}\) and the next-token prediction loss \(L(\cdot)\) in Eq. (23), the gradients are given by_

\[\frac{\partial L}{\partial e} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})(2X(1+2w|w|)-1)\right]\cdot e,\] \[\frac{\partial L}{\partial w} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]\cdot 4e^{2}|w|,\] \[\frac{\partial L}{\partial b} =\mathbb{E}_{X}\left[f_{1}X+f_{2}\right],\]

_where \(X\in\{0,1\}\) is a Bernoulli random variable with \(X\sim\mathrm{Bern}(p/(p+q))\), \(f_{1}=\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)+q-1-\sigma\left(b-\frac {e^{2}}{2}\right)+p\), and \(f_{2}=\sigma\left(b-\frac{e^{2}}{2}\right)-p\)._

**Remark 2**.: It is interesting to note that the gradients for both \(e\) and \(w\) are product of an expectation term and an \(e\) factor. Also, except for scaling factors in terms of \((e,w,b)\), all the gradients are governed by the two expectation terms \(\mathbb{E}[(f_{1}X+f_{2})X]\) and \(\mathbb{E}[f_{1}X+f_{2}]\). This observation plays a key role in obtaining an ordinary differential equation which yields the energy function \(\mathcal{E}\), defined in Eq. (11).

Now we characterize the Hessian at both local-minima and saddle points.

**Lemma 4** (Hessian at local-minima and saddle points).: _For the canonical parameterization \(\bm{\gamma}=(b,e,w)\in\mathbb{R}^{3}\) and the next-token prediction loss \(L(\cdot)\) in Eq. (23), the Hessian at any \(\bm{\gamma}_{\min}\in\bm{\Gamma}_{\min}\) or \(\bm{\gamma}_{\mathrm{sad}}\in\bm{\Gamma}_{\mathrm{sad}}\) is given by_

\[\nabla^{2}L(\bm{\gamma})\bigg{|}_{\bm{\gamma}=\bm{\gamma}_{\min},\bm{\gamma} _{\mathrm{rad}}}=\pi_{0}\pi_{1}\begin{bmatrix}1&0&0\\ 0&2(p+q-1)(1+2w|w|)&0\\ 0&0&0\end{bmatrix},\]

_where \(\pi_{0}=\frac{q}{p+q}\) and \(\pi_{1}=\frac{p}{p+q}\)._

**Remark 3**.: We note that the Hessian is computed with the parameter ordering \((b,e,w)\).

The proofs of the lemmas are presented in App. K.

Analysis of the loss without bias, \(L(\bm{\theta})\), and proof of Thm. 1

The proof of Thm. 1, concerning the loss \(L(\bm{\theta})\) in Eq. (5), is similar to that of Thm. 7 which studies the loss \(L(\bm{\theta},b)\) with the bias present. The main idea is to establish the analogous set of lemmas, as in App. D, when the bias is substituted with its optimal choice. First we recall the loss function

\[L(\bm{\theta})\triangleq L(\bm{\theta},b_{\star}) =-\frac{1}{N}\sum_{n\in[N]}\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\cdot \log f_{(\bm{\theta},b_{\star})}(x_{1}^{n})\,+(1-x_{n+1})\cdot\log(1-f_{(\bm{ \theta},b_{\star})}(x_{1}^{n}))],\] (29) \[b_{\star} =\operatorname*{argmin}_{b\in\mathbb{R}}L(\bm{\theta},b).\]

We start with the result that establishes a closed form expression for \(b_{\star}\).

**Lemma 5** (Optimal bias).: _For \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\) and \(b\in\mathbb{R}\), let \(L(\bm{\theta},b)\) be the next-token prediction loss defined in Eq. (23). Then, for any \(\bm{\theta}\in\mathbb{R}^{2}\), \(L(\bm{\theta},b)\) is convex in \(b\) and the minimizer \(b_{\star}\triangleq\operatorname*{argmin}_{b\in\mathbb{R}}L(\bm{\theta},b)\) is given by_

\[\exp\left(b_{\star}-\frac{e^{2}}{2}\right)=\frac{1}{2A}\left[\frac{p}{q}-1+ \sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q}\cdot A}\right],\quad A \triangleq\exp(e^{2}(1+2w|w|)).\] (30)

_Consequently, if \(e^{2}(1+2w|w|)=\log\frac{(1-p)(1-q)}{pq}\), then \(b_{\star}-\frac{e^{2}}{2}=\log\frac{p}{1-p}\). If \(e=0\), then \(b_{\star}=\log\frac{p}{q}\)._

Now we rewrite the loss in terms of the logistic function.

**Lemma 6** (Loss as a logistic function).: _For any \(\bm{\theta}\in\mathbb{R}^{2}\), the next-token prediction loss \(L(\bm{\theta})\) in Eq. (29) can be written as_

\[L(\bm{\theta}) =\frac{1}{N}\sum_{n\in[N]}\mathbb{E}[\ell_{\log}\left((2x_{n+1}-1) \cdot\mathrm{logit}_{n})\right]\] (31) \[=\mathbb{E}_{X,Y}\left[\ell_{\log}\left((2Y-1)\left(e^{2}(1+2w|w| )X+b_{\star}-\frac{e^{2}}{2}\right)\right)\right].\]

_where \(b_{\star}\) follows from Eq. (30), \((X,Y)\in\{0,1\}^{2}\) are distributed according to \((X,Y)\sim(\bm{\pi},\bm{P})\), i.e. \(X\) is a Bernoulli random variable with \(X\sim\bm{\pi}\equiv\mathrm{Bern}(p/(p+q))\) and \(Y|X\sim\bm{P}(p,q)\), the Markov kernel._

The following lemma establishes the gradients of the loss.

**Lemma 7** (Gradient computation).: _For any \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\) and the next-token prediction loss \(L(\bm{\theta})\) in Eq. (29), the gradients are given by_

\[\frac{\partial L}{\partial e} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]\cdot 2(1+2w|w|)e,\] \[\frac{\partial L}{\partial w} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]\cdot 4e^{2}|w|,\]

_where \(X\in\{0,1\}\) is a Bernoulli random variable with \(X\sim\mathrm{Bern}(p/(p+q))\), \(f_{1}=\sigma\left(2e^{2}w|w|+b_{\star}+\frac{e^{2}}{2}\right)+q-1-\sigma\left( b_{\star}-\frac{e^{2}}{2}\right)+p\), and \(f_{2}=\sigma\left(b_{\star}-\frac{e^{2}}{2}\right)-p\). Further, \(\pi_{1}f_{1}+f_{2}=0\)._

**Remark 4**.: We observe above that the gradients for both \(e\) and \(w\) are proportional to each other, except for the scaling factors in terms of \(e\) and \(w\). This forms the basis for the derivation of the energy function discussed in App. F.

The following lemma characterizes the Hessian.

**Lemma 8** (Hessian computation).: _Let \(\bm{\gamma}=(b,\bm{\theta})\in\mathbb{R}^{3}\) with \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\), and \(L(\bm{\gamma})\) be the next-token prediction loss in Eq. (23) and \(L(\bm{\theta})\) be the one in Eq. (29). Let the Hessian of \(L\) at \(\bm{\gamma}\) be_

\[H(\bm{\gamma})\triangleq\nabla^{2}_{\bm{\gamma}\bm{\gamma}}L=\begin{bmatrix}H_ {b\bm{\theta}}&H_{b\bm{\theta}}\\ H_{b\bm{\theta}}^{\top}&H_{b\bm{\theta}}\end{bmatrix}=\begin{bmatrix}\nabla^{ 2}_{bb}L&\nabla^{2}_{b\bm{\theta}}L\\ (\nabla^{2}_{b\bm{\theta}}L)^{\top}&\nabla^{2}_{\bm{\theta}\bm{\theta}}L\end{bmatrix} \in\mathbb{R}^{3\times 3}.\]_Then the Hessian of \(L\) at \(\bm{\theta}\in\mathbb{R}^{2}\) is given by_

\[H(\bm{\theta})\triangleq\nabla^{2}_{\bm{\theta}\bm{\theta}}L=H_{\bm{\theta}\bm{ \theta}}-H^{\top}_{b\bm{\theta}}\cdot H^{-1}_{bb}\cdot H_{b\bm{\theta}}.\] (32)

_Consequently, for any \(\bm{\gamma}=(b,e,w)\in\bm{\Gamma}_{\min}\cup\bm{\Gamma}_{\rm sad}\), the Hessian \(H(\bm{\theta})\) at \(\bm{\theta}=(e,w)\) is given by_

\[H(\bm{\theta})=\pi_{0}\pi_{1}\begin{bmatrix}2(p+q-1)(1+2w|w|)&0\\ 0&0\end{bmatrix},\] (33)

_where \(\pi_{0}=\frac{q}{p+q}\) and \(\pi_{1}=\frac{p}{p+q}\)._

The proofs of the above lemmas are deferred to App. L. We are now ready to present the proof of Thm. 1.

### Proof of Thm. 1

Proof.: Let \(\bm{\theta}\in\mathbb{R}^{2}\) and \(\bm{\gamma}(\bm{\theta})=(\bm{\theta},b_{\star}(\bm{\theta})\in\mathbb{R}^{3}\) be its embedding in \(\mathbb{R}^{3}\) with the optimal bias \(b_{\star}(\bm{\theta})=\operatorname*{argmin}_{b\in\mathbb{R}}L(\bm{\theta},b)\) from Lemma 5. Define the following four sets of points:

\[\bm{\Theta}_{\star}(p,q) \triangleq\left\{(e,w)\in\mathbb{R}^{2}:e^{2}(1+2w|w|)=\log \frac{(1-p)(1-q)}{pq}\right\},\] \[\bm{\Theta}_{\min}(p,q) \triangleq\left\{(e,w)\in\mathbb{R}^{2}:e=0,\,(p+q-1)(1+2w|w|)>0 \right\},\] \[\bm{\Theta}_{\max}(p,q) \triangleq\left\{(e,w)\in\mathbb{R}^{2}:e=0,\,(p+q-1)(1+2w|w|)<0 \right\},\] \[\bm{\Theta}_{\rm sad}(p,q) \triangleq\left\{(e,w):e=0,\,w=-1/\sqrt{2}\right\}.\]

First we show that any critical point of \(L:\mathbb{R}^{2}\to\mathbb{R}\) has to lie in one of these sets. Then we characterize that they correspond to the set of all global minima, local minima & maxima, and saddle points respectively.

**(i) Set of all critical points:** Recall from Thm. 7 that for any critical point \(\bm{\gamma}=(\bm{\theta},b)=(e,w,b)\in\mathbb{R}^{3}\) of \(L\), \(\bm{\gamma}\in\bm{\Gamma}_{\star}\cup\bm{\Gamma}_{\min}\cup\bm{\Gamma}_{\rm sad}\). Here the main observation is that all these critical points are of the form \((\bm{\theta},b_{\star}(\bm{\theta}))\) where \(\bm{\theta}\in\bm{\Theta}_{\star}\cup\bm{\Theta}_{\min}\cup\bm{\Theta}_{\max} \cup\bm{\Theta}_{\rm sad}\). To see this, let \(\bm{\gamma}\in\bm{\Gamma}_{\star}\). Here we have \(e^{2}(1+2w|w|)=\log\frac{(1-p)(1-q)}{pq}\) from Eq. (24) and hence \(\bm{\theta}\in\bm{\Theta}_{\star}\). Further, by Lemma 5, we have that the optimal bias for this \(\bm{\theta}\) satisfies \(b_{\star}-\frac{e^{2}}{2}=\log\frac{p}{1-p}\), which is precisely the characterization of the bias \(b\) for \(\bm{\gamma}=(e,w,b)\) in Eq. (24). Likewise, if \(\bm{\gamma}\in\bm{\Gamma}_{\min}\cup\bm{\Gamma}_{\rm sad}\), we have \(e=0\) and hence \(\bm{\theta}\in\bm{\Theta}_{\min}\cup\bm{\Theta}_{\max}\cup\bm{\Theta}_{\rm sad}\). Hence by Lemma 5, \(b_{\star}=\log\frac{p}{q}\), matching that of Eq. (25) and Eq. (26). Thus the set of all critical points of \(L\) in \(\mathbb{R}^{3}\) are of the form \((\bm{\theta},b_{\star}(\bm{\theta}))\) with where \(\bm{\theta}\in\bm{\Theta}_{\star}\cup\bm{\Theta}_{\min}\cup\bm{\Theta}_{\max} \cup\bm{\Theta}_{\rm sad}\). Since \(\bm{\Gamma}_{\star}\cup\bm{\Gamma}_{\min}\cup\bm{\Gamma}_{\rm sad}\) covers the entirety of stationary points of \(L\) in \(\mathbb{R}^{3}\), it follows that the set of all stationary points in \(\mathbb{R}^{2}\) is precisely \(\bm{\Theta}_{\star}\cup\bm{\Theta}_{\min}\cup\bm{\Theta}_{\max}\cup\bm{\Theta}_ {\rm sad}\). Also, the ordering of losses directly follows from the aformentioned observation.

Now we characterize these critical points in terms of the extrema.

**(ii) Set of global and local minima:** From Eq. (24), for any global minimum \(\bm{\gamma}_{\star}=(\bm{\theta}_{\star},b_{\star}(\bm{\theta}_{\star}))\) of \(L\) in \(\mathbb{R}^{3}\), we have \(\bm{\theta}_{\star}\in\bm{\Theta}_{\star}\subseteq\mathbb{R}^{2}\). Hence by definition, \(\bm{\Theta}_{\star}\) is the set of all global minima in \(\mathbb{R}^{2}\). A similar argument holds for \(\bm{\Theta}_{\min}\), which establishes that it is a set of all local minima.

**(iii) Set of local maxima and saddle points:** From Eq. (26), for any saddle point \(\bm{\gamma}=(e,w,b_{\star}(e,w))\) of \(L\) in \(\mathbb{R}^{3}\), we have that \(e=0\) and \((p+q-1)(1+2w|w|)\leq 0\). Hence \(\bm{\theta}=(e,w)\in\bm{\Theta}_{\max}\cup\bm{\Theta}_{\rm sad}\). Suppose \(\bm{\theta}\in\bm{\Theta}_{\max}\) which implies \(e=0,(p+q-1)(1+2w|w|)<0\). By Lemma 8, the Hessian at \(\bm{\theta}\) (upto a positive scale) is a diagonal matrix with the entries \((p+q-1)(1+2w|w|)<0\) and \(0\), corresponding to the directions of \(e\) and \(w\) respectively. Though one of the eigenvalue here is zero, using a continuity argument as in the proof of Thm. 7 for local minima, we can establish that \(\bm{\theta}\) is indeed a local maximum. Thus \(\bm{\Theta}_{\max}\) is a set of local minima.

Now suppose \((e,w)\in\bm{\Theta}_{\rm sad}\). Thus \(e=0\) and \(w=-\frac{1}{\sqrt{2}}\). Since it lies at the intersection of \(\bm{\Theta}_{\min}\) and \(\bm{\Theta}_{\max}\), using a neighborhood argument, it's straightforward to see that \(\bm{\Theta}_{\rm sad}\) is indeed a set of saddle points.

Finally it follows that \(\bm{\Theta}_{\min},\bm{\Theta}_{\max},\bm{\Theta}_{\text{sad}}\) are the only set of local minima, maxima, and saddle points from the above fact about the characterization of the set of all critical points in terms of these sets and \(\bm{\Theta}_{\star}\), the ordering of the losses, and using the same argument as in the final steps of the proof of Thm. 7 with the bias. This concludes the proof.

Gradient flow analysis without attention

In this section, we analyze the learning dynamics of the transformer parameters \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\) without the attention scalar. First, we present few important lemmas regarding the same, useful for the proofs of Thm. 2 and Thm. 8 later. Recall from Sec. 4 that the trajectory \((\bm{\theta}_{t})_{t\geq 0}\) is governed by

\[\frac{\mathrm{d}\bm{\theta}_{t}}{\mathrm{d}t}=-\nabla L(\bm{\theta}_{t}),\quad \bm{\theta}_{t}=(e_{t},w_{t})\in\mathbb{R}^{2},\,t\geq 0,\] (GF)

starting with a randomly initialized \(\bm{\theta}_{0}\). The _energy function_\(\mathcal{E}(\cdot,\cdot)\) is defined as

\[\mathcal{E}(e,w)\triangleq e^{2}-(w^{2}+\mathrm{sign}(w)\cdot\log|w|),\quad \forall(e,w)\in\mathbb{R}^{2}\setminus\mathrm{e-axis},\] (34)

where \(\mathrm{e-axis}\triangleq\{(e,w=0)\}\) and \(\mathrm{w-axis}\triangleq\{(e=0,w)\}\). Note that \(\mathcal{E}_{\mathrm{sad}}=\mathcal{E}(0,-\frac{1}{\sqrt{2}})=-\frac{1+\log 2} {2}\). We re-present the Lemma 1 from Sec. 4 below for the sake of completeness.

**Lemma 9** (Constant energy along the flow).: _For any \((p,q)\in(0,1)^{2}\) and initialization \(\bm{\theta}_{0}=(e_{0},w_{0})\in\mathbb{R}^{2}\), let \((\bm{\theta}_{t})_{t\geq 0}\) be the corresponding GF trajectory starting from \(\bm{\theta}_{0}\). If \(w_{0}\neq 0\), then the energy stays constant along the trajectory, i.e._

\[\mathcal{E}(\bm{\theta}_{t})=e_{t}^{2}-(w_{t}^{2}+\mathrm{sign}(w_{t})\cdot \log|w_{t}|)=\mathcal{E}(\bm{\theta}_{0}),\quad\forall t\geq 0.\] (35)

_On the other hand, if \(w_{0}=0\), \(w_{t}=0\) for all \(t\geq 0\). Hence, if we initialize on \(\mathrm{e-axis}\) the trajectory always stays on the \(\mathrm{e-axis}\)._

Now we establish that the GF trajectories always converge.

**Lemma 10** (GF convergence).: _Let \((\bm{\theta}_{t})_{t\geq 0}\) be a continuously diferentiable GF trajectory starting from \(\bm{\theta}_{0}\). Then for all initializations \(\bm{\theta}_{0}\in\mathbb{R}^{2}\),_

1. \((\bm{\theta}_{t})_{t\geq 0}\) _is bounded,_
2. _there exists a_ \(\bm{\theta}_{\lim}\in\mathbb{R}^{2}\) _such that_ \(\lim_{t\to\infty}\bm{\theta}_{t}=\bm{\theta}_{\lim}\) _and_
3. \(\lim_{t\to\infty}\|\nabla L(\bm{\theta}_{t})\|=\|\nabla L(\bm{\theta}_{\lim}) \|=0\)_._

_Hence \(\bm{\theta}_{\lim}\) is a critical point of \(L\)._

The following result characterizes the energy of the limit point.

**Lemma 11** (Energy at the limit point).: _Consider the same setting as in Lemma 10. If \(\bm{\theta}_{0}\in\mathbb{R}^{2}\setminus\mathrm{e-axis}\), then \(\mathcal{E}(\bm{\theta}_{\lim})=\mathcal{E}(\bm{\theta}_{0})\). Hence \(\bm{\theta}_{\lim}\) lies at the intersection of the contour line \(\mathcal{E}(e,w)=\mathcal{E}_{0}\) with the set of critical points of \(L\) in \(\mathbb{R}^{2}\)._

_On the other hand, if \(\bm{\theta}_{0}\in\mathrm{e-axis}\), then \(\bm{\theta}_{\lim}\in\mathrm{e-axis}\)._

We now study the energy function on the \(\mathrm{w-axis}\) which plays a key role in the GF analysis.

**Lemma 12** (Analysis of the energy function).: _Let \(\mathcal{E}(\cdot,\cdot)\) be the energy function defined in Eq. (48) and \(f(w)\triangleq\mathcal{E}(e=0,w)=-(w^{2}+\mathrm{sign}(w)\cdot\log|w|)\) be the energy evaluated on \(\mathrm{w-axis}\) for \(w\in\mathbb{R}\setminus\{0\}\). Then_

1. \(f:(-\infty,-1/\sqrt{2}]\to(-\infty,\,\mathcal{E}_{\mathrm{sad}}]\) _is monotonically increasing with_ \(\lim_{w\to-\infty}f(w)=-\infty\) _and the maximum being_ \(f(-1/\sqrt{2})=\mathcal{E}_{\mathrm{sad}}\)_,_
2. \(f:[-1/\sqrt{2},0)\to[\mathcal{E}_{\mathrm{sad}},-\infty)\) _is monotonically decreasing with_ \(\lim_{w\to 0^{-}}f(w)=-\infty\)_,_
3. \(f^{\prime}(-\frac{1}{\sqrt{2}})=0\)_, and_
4. \(f:(0,\infty)\to(-\infty,\infty)\) _is monotonically decreasing with_ \(\lim_{w\to 0^{+}}f(w)=\infty\) _and_ \(\lim_{w\to\infty}f(w)=-\infty\)_._

We are now ready to prove Thm. 2 corresponding to \(p+q>1\).

### Proof of Thm. 2

Proof.: Let \(\bm{\theta}_{0}=(e_{0},w_{0})\in\mathbb{R}^{2}\) be the initialization for the GF trajectory \((\bm{\theta}_{t})_{t\geq 0}\). Recall that

\[\mathcal{I}_{\min}\triangleq\left\{(e,w):w\in(-1/\sqrt{2},0),\,e \in(-g(w),g(w)),\,g(w)=\sqrt{w^{2}-\log(-w)+\mathcal{E}_{\mathrm{sad}}}\right\}\] \[\qquad\qquad\cup\left\{(e,w):w>0\right\}\cup\left\{(e,w):w=0 \right\},\]\[\mathcal{I}_{\mathrm{sad}} \triangleq\left\{(e,w):w\in[-1/\sqrt{2},0),\,e=\pm\sqrt{w^{2}-\log(-w) +\mathcal{E}_{\mathrm{sad}}}\right\},\] \[\mathcal{I}_{\mathrm{max}} \triangleq\left\{(e,w):e=0,\,w<-1/\sqrt{2}\right\},\] \[\mathcal{I}_{\star} \triangleq\mathbb{R}^{2}\setminus(\mathcal{I}_{\mathrm{min}} \cup\mathcal{I}_{\mathrm{sad}}\cup\mathcal{I}_{\mathrm{max}})\,.\]

We consider the cases \(\bm{\theta}_{0}\in\mathrm{e}\)-axis and \(\bm{\theta}_{0}\in\mathbb{R}^{2}\setminus\mathrm{e}\)-axis separately. First recall from Thm. 1 and Eq. (6) that for \(p+q>1\), the loci of the global minima, \(e^{2}(1+2w|w|)=\log\frac{(1-p)(1-q)}{pq}<0\), lies entirely in the negative half-plane corresponding to \(w<-\frac{1}{\sqrt{2}}\). On the other hand, all the local minima, maxima and the saddle points span the \(\mathrm{w}\)-axis corresponding to \(e=0\).

**(i) \(\bm{\theta}_{0}\in\mathbb{R}^{2}\setminus\mathrm{e}\)-axis:** Let \(\mathcal{E}_{0}=\mathcal{E}(\bm{\theta}_{0})\in\mathbb{R}\). By Lemmas. (9), (10), and (11), we have that the trajectory \((\bm{\theta}_{t})_{t\geq 0}\) always stays on the contour line \(\mathcal{E}(e,w)=\mathcal{E}_{0}\) and converges to the limit \(\bm{\theta}_{\mathrm{lim}}\) which is an intersection of this contour line with the set of critical points of \(L\). Hence the crux of the proof is to establish where these intersections occur based on the initialization \(\bm{\theta}_{0}\) and the initial energy \(\mathcal{E}_{0}\). This gives rise to the set of initializations \(\mathcal{I}_{\mathrm{min}},\mathcal{I}_{\mathrm{max}},\mathcal{I}_{\mathrm{ sad}}\), and \(\mathcal{I}_{\star}\) that correspond to the limit being a local minimum/maximum, a saddle point, or a global minimum.

We characterize them individually below starting with \(\mathcal{I}_{\mathrm{min}}\).

**Initializations for local minima, \(\mathcal{I}_{\mathrm{min}}\).** For \(\bm{\theta}_{0}=(e_{0},w_{0})\in\mathbb{R}^{2}\setminus\mathrm{e}\)-axis, assume that \(w_{0}>0\). Since \(\mathcal{E}_{0}\in\mathbb{R}\), there exists an unique \(w_{\star}>0\) such that \(f(w_{\star})=\mathcal{E}(0,w_{\star})=\mathcal{E}_{0}\) by Lemma 12, (iv). Further using the fact that the energy contour lines do not cross each other (by definition of a contour line) and the fact they do not intersect the \(\mathrm{e}\)-axis (it's an energy barrier as discussed in Sec. 4), it follows that the contour line \(\mathcal{E}(e,w)=\mathcal{E}_{0}\) stays entirely in the positive half-plane corresponding to \(w>0\) and \(w_{\star}>0\) is the unique (and only) intersection of this line with the \(\mathrm{w}\)-axis, and hence the set of critical points. Since the \(\mathrm{w}\)-axis corresponding to \(w>0\) is a set of a local minima (Eq. (7)), it follows that any initialization \((e_{0},w_{0})\) with \(w_{0}>0\) converges to a local minimum.

Now suppose \(-\frac{1}{\sqrt{2}}<w_{0}<0\) and \(e_{0}\in(-g(w_{0}),g(w_{0}))\), where \(g(w_{0})=\sqrt{w^{2}-\log(-w)+\mathcal{E}_{\mathrm{sad}}}\). Thus \(|e_{0}|<g(w_{0})\) and hence \(e_{0}^{2}-(w_{0}^{2}-\log(-w_{0}))=\mathcal{E}(e_{0},w_{0})=\mathcal{E}_{0}< \mathcal{E}_{\mathrm{sad}}\). Hence by Lemma 12, (iii), there is a unique intersection of the contour line \(\mathcal{E}(e,w)=\mathcal{E}_{0}\) with the \(\mathrm{w}\)-axis, which lies in the region \(\left(-\frac{1}{\sqrt{2}},0\right)\). Further note that this contour line cannot intersect with the global minima loci as it lies in the half-plane \(w<-\frac{1}{\sqrt{2}}\), and hence its only intersection with the set of critical points is this segment of \(\mathrm{w}\)-axis, which is precisely the set of local minima the GF initialized on this line would converge to.

Thus we have shown that any initialization in \(\mathcal{I}_{\mathrm{min}}\setminus\cup\{(e,w):w=0\}\) converges to a local minimum, the set of which exhausts all the set of local minima \(\bm{\Theta}_{\mathrm{min}}\) except for the origin. Below we will estbalish that any initialization on \(\mathrm{e}\)-axis \(=\{(e,w):w=0\}\) converges to the origin, implying \(\mathcal{I}_{\mathrm{min}}\) is the full set of initializations for which the limit is a local minimum.

**Initializations for saddle points, \(\mathcal{I}_{\mathrm{sad}}\).** It's straightforward to see that for any \(\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{sad}}\), \(e_{0}^{2}-(w_{0}^{2}-\log(-w_{0}))=\mathcal{E}(0,-\frac{1}{\sqrt{2}})= \mathcal{E}_{\mathrm{sad}}\). Since \(-\frac{1}{\sqrt{2}}\leq w_{0}<0\), the point \((w,e)=(-\frac{1}{\sqrt{2}},0)\) is the only intersection of the contour line with the set of critical points, any initialization in \(\mathcal{I}_{\mathrm{sad}}\) converges to the saddle point. On the other hand, there also exists a contour line \(e_{0}^{2}-(w_{0}^{2}-\log(-w_{0}))=\mathcal{E}_{\mathrm{sad}}\) for \(w_{0}<-\frac{1}{\sqrt{2}}\) that passes through \((-\frac{1}{\sqrt{2}},0)\in\mathbb{R}^{2}\) and further intersecting with the global minima loci \(\bm{\Theta}_{\star}\). However, if we initialize on this line the flow escapes away from the saddle point and converges instead to a global minimum. To show this, it suffices to prove that \(\frac{\mathrm{d}e_{\star}}{\mathrm{d}t}>0\) and \(\frac{\mathrm{d}w_{\star}}{\mathrm{d}t}<0\) if \(e_{0}>0\) and \(w_{0}<-\frac{1}{\sqrt{2}}\), such that \((w_{0},e_{0})\) is close to the saddle point \((-\frac{1}{\sqrt{2}},0)\) (the case for \(e_{0}<0\) is similar as the flow is symmetric in \(e\in\mathbb{R}\)). From Lemma 7 and the definition of the GF, we have that

\[\frac{\mathrm{d}e_{t}}{\mathrm{d}t} =-\frac{\partial L}{\partial e}(e_{0},w_{0})=2\mathbb{E}_{X}\left[( f_{1}X+f_{2})X\right]\cdot(1-2w_{0}^{2}))e_{0}\] \[\frac{\mathrm{d}w_{t}}{\mathrm{d}t} =-\frac{\partial L}{\partial w}(e_{0},w_{0})=4\mathbb{E}_{X}\left[( f_{1}X+f_{2})X\right]\cdot(-e_{0}^{2}w_{0}).\]

So it suffices to show that \(\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]>0\). To establish this, we have from Lemma 5 that

\[\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]=\mathbb{E}[X](f_{1}+f_{2})=\pi_{1} \left(-\frac{f_{2}}{\pi_{1}}+f_{2}\right)=-\pi_{0}\cdot f_{2}.\]From the defintion of \(f_{2}\) and the optimal bias \(b_{\star}\) in Lemma 7 and Lemma 5 respectively, we obtain

\[f_{2} =\sigma\left(b_{\star}-\frac{e_{0}^{2}}{2}\right)-p=\left(1+\exp \left(-b_{\star}+\frac{e_{0}^{2}}{2}\right)\right)^{-1}-p\] \[=\left(1+\frac{2A}{\frac{p}{q}-1+\sqrt{\left(\frac{p}{q}-1\right) ^{2}+4\cdot\frac{p}{q}\cdot A}}\right)^{-1}-p,\quad A\triangleq\exp(e_{0}^{2}( 1-2w_{0}^{2})).\]

When \(e_{0}=0\), we have \(A=1\) and hence

\[f_{2}=\left(1+\frac{q}{p}\right)^{-1}-p=\frac{p}{p+q}-p=-\frac{p}{p+q}(p+q-1)<0,\] (36)

where we used the fact that \(p+q>1\). Hence by continuity of \(f_{2}\) in \(e_{0}\), for \(e_{0}\) sufficiently close to \(0\), \(f_{2}<0\) which proves our claim about the direction of the flow close to the saddle point. By using the continuity of the flow, it follows that GF cannot converge to saddle point when initialized on this contour line for \(w_{0}<-\frac{1}{\sqrt{2}}\). Thus \(\mathcal{I}_{\mathrm{sad}}\) is the only set of initializations for convergence to \(\bm{\Theta}_{\mathrm{sad}}\).

**Initializations for local maxima, \(\mathcal{I}_{\mathrm{max}}\)**. If \(p+q>1\), we have from Thm. 1 that \(\bm{\Theta}_{\mathrm{max}}=\left\{(e,w)\in\mathbb{R}^{2}:e=0,\,(1+2w|w|)<0 \right\}=\left\{(e,w)\in\mathbb{R}^{2}:e=0,\,w<-\frac{1}{\sqrt{2}}\right\}\). Thus for any \(\bm{\theta}_{0}\in\bm{\Theta}_{\mathrm{max}}\), \(\frac{\mathrm{d}\bm{\theta}_{t}}{\mathrm{d}t}=0\) for all \(t\geq 0\) and hence \(\bm{\theta}_{\mathrm{lim}}=\bm{\theta}_{0}\). Further if we slightly perturb away from this set, from Eq. (36) it follows that the flow diverges and hence it's an unstable set of critical points (they are local maxima indeed). Thus the only set of initializations leading to local maxima are \(\mathcal{I}_{\mathrm{max}}=\bm{\Theta}_{\mathrm{max}}\).

**Initializations for the global minima, \(\mathcal{I}_{\star}\).** Since the set of all critical points of \(L\) is \(\bm{\Theta}_{\star}\cup\bm{\Theta}_{\mathrm{min}}\cup\bm{\Theta}_{\mathrm{max}}\cup \bm{\Theta}_{\mathrm{sad}}\), and the initializations in \(\mathcal{I}_{\mathrm{min}}\), \(\mathcal{I}_{\mathrm{sad}}\), and \(\mathcal{I}_{\mathrm{max}}\) converge to \(\bm{\Theta}_{\mathrm{min}}\), \(\bm{\Theta}_{\mathrm{sad}}\), and \(\bm{\Theta}_{\mathrm{max}}\) respectively, it follows that the set of initializations for which the GF converges to global minima is \(\mathcal{I}_{\star}=\mathbb{R}^{2}\setminus(\mathcal{I}_{\mathrm{min}}\cup \mathcal{I}_{\mathrm{sad}}\cup\mathcal{I}_{\mathrm{max}})\).

In fact, since the loci of the global minima lies in the half-plane correspondint to \(w<-\frac{1}{\sqrt{2}}\) when \(p+q>1\), we can precisely determine the location of the global minimum for which the intersection occurs for any \(\bm{\theta}_{0}\in\mathcal{I}_{\star}\). Specifically, we can solve the pair of equations \(\mathcal{E}(e,w)=e^{2}-w^{2}+\log(-w)=\mathcal{E}_{0}\) and \(e^{2}(1-2w^{2})=\log\frac{(1-p)(1-q)}{pq}\) which has a unique solution for \(w<0\) (upto a sign flip in \(e\)).

**(ii) \(\bm{\theta}_{0}\in\mathrm{e-axis}\Rightarrow\bm{\theta}_{0}\in\mathcal{I}_{ \mathrm{min}}\):** If \(\bm{\theta}_{0}=(e_{0},w_{0})\in\mathrm{e-axis}\), we have that \(w_{0}=0\) and hence \(w_{t}=0\) for all \(t\geq 0\) (Lemma 9). Lemma 10-(i) also establishes that the iterates \((\bm{\theta}_{t}=(e_{t},0))_{t\geq 0}\) stay bounded on the \(\mathrm{e-axis}\) and monotonically decrease. Since the origin is the only critical point of \(L\) on the \(\mathrm{e-axis}\), and \(\lim_{t\rightarrow\infty}\bm{\theta}_{t}=\bm{\theta}_{\mathrm{lim}}\) exists, it follows that \(\bm{\theta}_{\mathrm{lim}}=(0,0)\), a local minima. Thus \(\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{min}}\).

This concludes the proof for all the initializations \(\bm{\theta}_{0}\in\mathbb{R}^{2}\).

**Gaussian initialization \(\mathcal{N}(0,\sigma^{2}\bm{I}_{2})\).** When \(\bm{\theta}_{0}\) is initialized according to the standard Gaussian distribution \(\mathcal{N}(0,\sigma^{2}\bm{I}_{2})\) with \(\sigma^{2}\ll\frac{1}{\sqrt{2}}\), we note that \(\bm{\theta}_{0}\) lands in the set \(\mathcal{I}_{\mathrm{min}}\) with high probability. In fact, this probability can be made arbitrarily close to \(1\) depending on \(\sigma^{2}\). Thus this initialization will lead to a local minimum convergence on the \(\mathrm{w-axis}\). 

### Gradient flow dynamics for \(p+q<1\)

**Theorem 8** (GF dynamics for \(p+q<1\)).: _Under the same setting as in Thm. 2 with \(p+q<1\), and any initialization \(\bm{\theta}_{0}\in\mathbb{R}^{2}\), the GF trajectory always converges to a \(\bm{\theta}_{\mathrm{lim}}\in\mathbb{R}^{2}\) which is a critical point of the loss \(L\). More specifically, \(\bm{\theta}_{\mathrm{lim}}\) is_

1. _a local minimum if_ \[\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{min}}\triangleq\left\{(e,w):w<-1/\sqrt{2 },\,e\in(-g(w),g(w)),\,g(w)=\sqrt{w^{2}-\log(-w)+\mathcal{E}_{\mathrm{sad}}} \right\},\]
2. _a saddle point if_ \[\bm{\theta}_{0}\in\mathcal{I}_{\mathrm{sad}}\triangleq\left\{(e,w):w\leq-1/ \sqrt{2},\,e=\pm\sqrt{w^{2}-\log(-w)+\mathcal{E}_{\mathrm{sad}}}\right\}\]_._
3. _a local maximum if_ \(\bm{\theta}_{0}\in\mathcal{I}_{\max}\triangleq\big{\{}(e,w):e=0,\,w>-1/\sqrt{2}\big{\}}\)_,_
4. _and a global minimum if_ \(\bm{\theta}_{0}\in\mathbb{R}^{2}\setminus(\mathcal{I}_{\min}\cup\mathcal{I}_{ \mathrm{sad}}\cup\mathcal{I}_{\max})\)_._

_Consequentely, if we use the standard initialization \(\bm{\theta}_{0}\sim\mathcal{N}(0,\sigma^{2}\bm{I}_{2})\) with \(\sigma^{2}\ll 1/\sqrt{2}\), \(\bm{\theta}_{\mathrm{lim}}\) will be a global minimum._

Proof.: The proof for the case of \(p+q<1\) essentially follows the same steps as that of \(p+q>1\). If the initialization is not on the \(\mathrm{e}\)-axis we use the energy equation to establish the convergence to the critical point at the intersection of the energy contour line with the critical set and if it starts on the \(\mathrm{e}\)-axis, the only change is that it now converges to the global minimum instead of the origin as in the earlier case. This is due to the fact that origin turns out to be a local maximum when \(p+q<1\) and hence it's an unstable critical point (which can be established as in the proof of Thm. 2 for \(\mathcal{I}_{\max}\)).

Gradient flow analysis with attention

In this section, we analyze the learning dynamics of the transformer parameters \(\bm{\theta}\in\mathbb{R}^{3}\) with the attention scalar \(a\in\mathbb{R}\), i.e. \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\). Similar to the analysis for \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\), we first introduce the canonical parameterization including \(a\in\mathbb{R}\), then analyze the corresponding loss function \(L(\cdot)\) in terms of its gradients and critical points, and capitalize on it to study the gradient flow dynamics using the energy. We first start with the parameterization.

### Canonical parameterization with attention

**Embedding.** Recall from App. C that we let \(\bm{e}=e\cdot\bm{\alpha}\) and \(\bm{p}_{n}=-p\cdot\bm{\alpha}\) for all \(n\) where \(e>0,p=\frac{e}{2}\) and \(\bm{\alpha}\in\{\pm 1\}^{d}/\sqrt{d}\). This results in the embedding

\[\bm{x}_{n}=e\left(x_{n}-\frac{1}{2}\right)\bm{\alpha}.\]

**Attention.** Similarly, we recall from Eq. (21) that the attention output \(\bm{y}_{n}\) is given by

\[\bm{y}_{n}=e\left(x_{n}-\frac{1}{2}\right)\bm{\alpha}+\langle\bm{v},\bm{ \alpha}\rangle\left(\sum_{i\in[n]}\mathrm{att}_{n,i}\cdot e\left(x_{i}-\frac{ 1}{2}\right)\right)\bm{\alpha},\] (37)

where

\[\mathrm{att}_{n,i}\triangleq\exp\left(\langle\bm{q}_{n},\bm{k}_{ i}\rangle/\sqrt{d}\right)/\left(\sum_{j\in[n]}\exp\left(\langle\bm{q}_{n},\bm{k}_{ j}\rangle/\sqrt{d}\right)\right),\,\bm{q}_{n}=\bm{W}_{Q}\,\bm{x}_{n},\,\bm{k}_{i}=\bm{W}_ {K}\,\bm{x}_{i},\] \[\bm{W}_{Q}^{\top}\bm{W}_{K}=(q^{2}d)\,\bm{\alpha}\cdot\bm{\alpha} ^{\top}\in\mathbb{R}^{d\times d},\quad\text{for some $q\in\mathbb{R}$}.\]

Instead of the softmax, now we assume that the attention weights are linear in the scaled dot product, i.e.

\[\mathrm{att}_{n,i} =\frac{\langle\bm{q}_{n},\bm{k}_{i}\rangle}{n\sqrt{d}}=\frac{1}{ \sqrt{d}}\cdot\bm{x}_{n}^{\top}\bm{W}_{Q}^{\top}\bm{W}_{K}\bm{x}_{n}=\frac{q^ {2}d}{n\sqrt{d}}\cdot(\bm{x}_{n}^{\top}\bm{\alpha})(\bm{x}_{i}^{\top}\bm{\alpha})\] (38) \[\overset{(\|\bm{\alpha}\|^{2}=d)}{=}\frac{q^{2}d^{3}}{n\sqrt{d}} \cdot(ex_{n}-p)(ex_{i}-p)\] \[=\frac{q^{2}d^{5/2}}{n}\cdot(ex_{n}-p)(ex_{i}-p)\] \[=\frac{q^{2}d^{5/2}e^{2}}{n}\cdot\left(x_{n}-\frac{1}{2}\right) \left(x_{i}-\frac{1}{2}\right).\]

Note that the \(1/n\) factor is to ensure normalization for the attention weights in Eq. (37). Now substituting Eq. (38) in Eq. (37), we obtain

\[\bm{y}_{n} =e\left(x_{n}-\frac{1}{2}\right)\bm{\alpha}+\langle\bm{v},\bm{ \alpha}\rangle\left(\sum_{i\in[n]}\mathrm{att}_{n,i}\cdot e\left(x_{i}-\frac{ 1}{2}\right)\right)\bm{\alpha}\] \[=\left[e\left(x_{n}-\frac{1}{2}\right)+\langle\bm{v},\bm{\alpha} \rangle\left(\sum_{i\in[n]}\frac{1}{n}q^{2}d^{5/2}e^{2}(x_{n}-\frac{1}{2})(x_ {i}-\frac{1}{2})\right)\cdot e\left(x_{i}-\frac{1}{2}\right)\right]\bm{\alpha}\] \[=\left[e\left(x_{n}-\frac{1}{2}\right)\left(1+\langle\bm{v},\bm {\alpha}\rangle q^{2}d^{5/2}e^{2}\left(x_{i}-\frac{1}{2}\right)^{2}\right) \right]\bm{\alpha}\] \[=e\left(x_{n}-\frac{1}{2}\right)\left(1+\langle\underbrace{ \langle\bm{v},\bm{\alpha}\rangle q^{2}d^{5/2}\frac{1}{4}}_{a}\cdot e^{2}\cdot \right)\right]\bm{\alpha}\] \[=e\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\bm{\alpha},\]where we used the fact that \((x_{i}-\frac{1}{2})^{2}=\frac{1}{4}\) since \(x_{i}\in\{0,1\}\), and

\[a\triangleq\frac{\langle\bm{v},\bm{\alpha}\rangle q^{2}d^{5/2}}{4}\] (39)

is the attention scalar. Note that this includes the scaling \(\langle\bm{v},\bm{\alpha}\rangle\) from the value matrix \(\bm{W}_{V}\) and \(q^{2}\) from the query-key dot product. Thus we succinctly have

\[\bm{y}_{n}=e\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\bm{\alpha}.\] (40)

**Feed-forward**. For the feed-forward layer, we have that \(\bm{W}_{1}=\frac{|w|}{\sqrt{d}}\,\bm{1}\cdot\bm{\alpha}^{\top}\in\mathbb{R}^{4 d\times d},\quad\bm{W}_{2}=\frac{w}{\sqrt{d}}\,\bm{\alpha}\cdot\bm{1}^{\top}\in \mathbb{R}^{d\times 4d}\). Hence Eq. (40) implies

\[\bm{W}_{1}\bm{y}_{n}=\frac{|w|}{\sqrt{d}}\,\bm{1}\cdot\bm{\alpha}^{\top}\left[ e\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\right]\bm{\alpha}=\frac{|w|}{ \sqrt{d}}\left[e\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\right]\bm {1}.\]

Thus,

\[\mathrm{ReLU}(\bm{W}_{1}\bm{y}_{n}) =\frac{|w|}{\sqrt{d}}\,\bm{1}\cdot\mathrm{ReLU}\left(\left[e \left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\right]\right)\] \[=\frac{|w|}{\sqrt{d}}\,\bm{1}\cdot e\,\mathrm{ReLU}\left(\left[ \left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\right]\right)\] \[=\frac{|w|}{\sqrt{d}}\,\bm{1}\cdot e\,\left(\frac{x_{n}}{2}\, \mathrm{ReLU}\big{(}1+ae^{2}\big{)}+\frac{1-x_{n}}{2}\,\mathrm{ReLU}\big{(}-1 -ae^{2}\big{)}\right)\] \[=\frac{|w|}{2\sqrt{d}}\,\bm{1}\cdot e\,\left(x_{n}\left[\mathrm{ ReLU}\big{(}1+ae^{2}\big{)}-\mathrm{ReLU}\big{(}-1-ae^{2}\big{)}\right]+\mathrm{ReLU} \left(-1-ae^{2}\right)\right).\]

Using \(\mathrm{ReLU}(x)-\mathrm{ReLU}(-x)=x\) above,

\[\mathrm{ReLU}(\bm{W}_{1}\bm{y}_{n})=\frac{|w|}{2\sqrt{d}}\,\bm{1}\cdot e\, \left(x_{n}\left(1+ae^{2}\right)+\mathrm{ReLU}\left(-1-ae^{2}\right)\right).\]

Hence,

\[\bm{W}_{2}\mathrm{ReLU}(\bm{W}_{1}\bm{y}_{n}) =\frac{w}{\sqrt{d}}\,\bm{\alpha}\cdot\bm{1}^{\top}\frac{|w|}{2 \sqrt{d}}\,\bm{1}\cdot e\,\left(x_{n}\left(1+ae^{2}\right)+\mathrm{ReLU} \left(-1-ae^{2}\right)\right)\] \[=2w|w|e\,\left(x_{n}\left(1+ae^{2}\right)+\mathrm{ReLU}\left(-1- ae^{2}\right)\right)\bm{\alpha}\] \[=2w|w|e\,\left(x_{n}\left(1+ae^{2}\right)+\frac{\left(-1-ae^{2} \right)}{2}+\frac{|1+ae^{2}|}{2}\right)\bm{\alpha}\] \[=2w|w|e\,\left(\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2} \right)+\frac{|1+ae^{2}|}{2}\right)\bm{\alpha}.\]

Thus the embedding \(\bm{z}_{n}\) is given by

\[\bm{z}_{n} =\bm{y}_{n}+\bm{W}_{2}\mathrm{ReLU}(\bm{W}_{1}\bm{y}_{n})\] \[=\left[e\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right) \right]\bm{\alpha}+2w|w|e\,\left(\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2} \right)+\frac{|1+ae^{2}|}{2}\right)\bm{\alpha}\] \[=e\left[\left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(1 +2w|w|\right)+w|w(1+ae^{2})|\right]\bm{\alpha}.\]

**Linear.** Since \(\bm{e}=\bm{a}=e\cdot\bm{\alpha}\) due to weight-tying, the logits are given by

\[\mathrm{logit}_{n}(e,w,a,b)=\langle\bm{a},\bm{z}_{n}\rangle+b=e^{2}\left[ \left(x_{n}-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(1+2w|w|\right)+w|w(1+ ae^{2})|\right]+b.\]

**Loss.** Denote \(\bm{\theta}\triangleq(e,w,a)\in\mathbb{R}^{3}\). Similar to the case without \(a\) (Eq. (14) and Lemma 2), the cross-entropy loss in our setting can be compactly written as

\[L(\bm{\theta},b)=\frac{1}{N}\sum_{n\in[N]}\mathbb{E}[\ell_{\log}\left((2x_{n+1} -1)\cdot\mathrm{logit}_{n}(\bm{\theta},b)\right)]=\mathbb{E}_{X,Y}\left[\ell_{ \log}\left((2Y-1)\cdot\mathrm{logit}_{X}(\bm{\theta},b)\right)\right],\] (41)

[MISSING_PAGE_FAIL:31]

3. _a set of saddle points is_ \[\bm{\Gamma}_{\rm sad}(p,q)\triangleq\left\{\bm{\gamma}_{\rm sad}=(e,w,b,a)\in \mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)\leq 0,b=\log\frac{p}{q}\right\}.\]
4. _a set of stationary points is_ \[\bm{\Gamma}_{\rm station}(p,q)\triangleq\left\{\bm{\gamma}_{\rm station}=(e,w,b,a)\in\mathbb{R}^{4}:e\neq 0,1+ae^{2}=0,1+2w|w|=0,b=\log\frac{p}{q}\right\},\]

_Thus the set of all critical points is_

\[\left\{\bm{\theta}\in\mathbb{R}^{2}:\nabla L(\bm{\theta})=0\right\}=\bm{ \Gamma}_{*}\cup\bm{\Gamma}_{\rm min}\cup\bm{\Gamma}_{\rm sad}\cup\bm{\Gamma}_{ \rm station}.\] (45)

_In addition, for any \(\bm{\theta}_{*}\in\bm{\Gamma}_{*},\bm{\theta}_{\rm min}\in\bm{\Gamma}_{\rm min}\) and \(\bm{\theta}_{\rm sad}\in\bm{\Gamma}_{\rm sad}\), the loss values satisfy_

\[H(x_{n+1}\mid x_{n})=L(\bm{\theta}_{*})<L(\bm{\theta}_{\rm min})=L(\bm{\theta} _{\rm max})=L(\bm{\theta}_{\rm sad})=H(x_{n+1}).\]

**Theorem 10** (All critical points in \(\mathbb{R}^{3}\)).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}\sim(\bm{\pi},\bm{P})\), the transformer parameters \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\), and the next-token prediction loss \(L(\cdot)\) be as in Eq. (42). Then for any \((p,q)\in(0,1)^{2}\) with \(p+q\neq 1\) and \(N\in\mathbb{N}\),_

1. _the set of all global minima is given by_ \[\bm{\Theta}_{\bullet}(p,q)\triangleq\left\{(e,w,a)\in\mathbb{R}^{3}:e^{2} \left(1+ae^{2}\right)(1+2w|w|)=\log\frac{(1-q)(1-p)}{pq}\right\}\] (46)
2. _a set of local minima is given by_ \[\bm{\Theta}_{\rm min}(p,q)\triangleq\left\{(e,w,a)\in\mathbb{R}^{3}:e=0,(p+q- 1)(1+2w|w|)>0\right\},\]
3. _a set of local maxima is given by_ \[\bm{\Theta}_{\rm min}(p,q)\triangleq\left\{(e,w,a)\in\mathbb{R}^{3}:e=0,(p+q- 1)(1+2w|w|)<0\right\},\]
4. _a set of saddle points is_ \[\bm{\Theta}_{\rm sad}(p,q)\triangleq\left\{(e,w,a)\in\mathbb{R}^{3}:\left(0, -1/\sqrt{2},a\right)\right\}.\]

_Defining a set of stationary points \(\bm{\Theta}_{\rm station}(p,q)\triangleq\left\{(e,w,a)\in\mathbb{R}^{3}:e\neq 0,1+ae^{2}=0,1+2w|w|=0\right\}\), the set of all critical points is_

\[\left\{\bm{\theta}\in\mathbb{R}^{2}:\nabla L(\bm{\theta})=0\right\}=\bm{ \Theta}_{\bullet}\cup\bm{\Theta}_{\rm min}\cup\bm{\Theta}_{\rm max}\cup\bm{ \Theta}_{\rm sad}\cup\bm{\Theta}_{\rm station}.\] (47)

_In addition, for any \(\bm{\theta}_{*}\in\bm{\Theta}_{*},\bm{\theta}_{\rm min}\in\bm{\Theta}_{\rm min}\), and \(\bm{\theta}_{\rm sad}\in\bm{\Theta}_{\rm sad}\), the loss values satisfy_

\[H(x_{n+1}\mid x_{n})=L(\bm{\theta}_{*})<L(\bm{\theta}_{\rm min})=L(\bm{\theta} _{\rm max})=L(\bm{\theta}_{\rm sad})=H(x_{n+1}).\]

**Remark 5**.: While the remaining set of stationary points could be classified to global minima, local minima, etc., it's technically unclear what category the set of critical points \(\bm{\Theta}_{\rm station}(p,q)\) belong to, as the Hessian is undefined here. We would need to rely on local perturbation analysis to characterize these class of points.

We defer the proofs of the theorems to App. N.2.

### Gradient flow analysis

Analogous to the gradient flow analysis for \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\) in App. F, we now study its counterpart toegether with the attention scalar, i.e. \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\). To this end, let \((\bm{\theta}_{t})_{t\geq 0}\) be a \(C^{1}\) curve in \(\mathbb{R}^{3}\) governed by

\[\frac{\mathrm{d}\bm{\theta}_{t}}{\mathrm{d}t}=-\nabla L(\bm{\theta}_{t}),\quad \bm{\theta}_{t}=(e_{t},w_{t},a_{t})\in\mathbb{R}^{3},\,t\geq 0,\] (GF-Attention)

starting with a randomly initialized \(\bm{\theta}_{0}\). We define the _energy function_\(\mathcal{E}(\cdot,\cdot,\cdot)\) as

\[\mathcal{E}(e,w,a)\triangleq e^{2}-(w^{2}+\mathrm{sign}(w)\cdot\log|w|)-2a^{2},\quad\forall(e,w,a)\in\mathbb{R}^{3}\setminus\mathrm{ea-plane},\] (48)

where \(\mathrm{ea-plane}\triangleq\left\{(e,w=0,a)\right\}\). The following lemma presents the crucial result that the energy is constant along the flow in GF-Attention.

**Lemma 14** (Constant energy along the flow).: _For any \((p,q)\in(0,1)^{2}\) and initialization \(\bm{\theta}_{0}=(e_{0},w_{0},a_{0})\in\mathbb{R}^{3}\), let \((\bm{\theta}_{t})_{t\geq 0}\) be the corresponding GF-Attention trajectory starting from \(\bm{\theta}_{0}\). If \(w_{0}\neq 0\), then the energy stays constant along the trajectory, i.e._

\[\mathcal{E}(\bm{\theta}_{t})=e_{t}^{2}-(w_{t}^{2}+\mathrm{sign}(w_{t})\cdot \log|w_{t}|)-2a_{t}^{2}=\mathcal{E}(\bm{\theta}_{0}),\quad\forall t\geq 0.\] (49)

_On the other hand, if \(w_{0}=0\), \(w_{t}=0\) for all \(t\geq 0\). Hence, if we initialize on \(\mathrm{ea}\mathrm{-plane}\) the trajectory always stays on the \(\mathrm{ea}\mathrm{-plane}\)._

Now we characterize the convergence of the gradient flow.

**Lemma 15** (GF convergence).: _Let \((\bm{\theta}_{t})_{t\geq 0}\) be a continuously differentiable GF-Attention trajectory starting from \(\bm{\theta}_{0}\). Then for all initializations \(\bm{\theta}_{0}\in\mathbb{R}^{3}\),_

1. \((\bm{\theta}_{t})_{t\geq 0}\) _is bounded,_
2. _there exists a_ \(\bm{\theta}_{\mathrm{lim}}\in\mathbb{R}^{3}\) _such that_ \(\lim_{t\to\infty}\bm{\theta}_{t}=\bm{\theta}_{\mathrm{lim}}\) _and_
3. \(\lim_{t\to\infty}\|\nabla L(\bm{\theta}_{t})\|=\|\nabla L(\bm{\theta}_{\mathrm{ lim}})\|=0\)_._

_Hence \(\bm{\theta}_{\mathrm{lim}}\) is a critical point of \(L\)._

The following result characterizes the energy of the limit point.

**Lemma 16** (Energy at the limit point).: _Consider the same setting as in Lemma 15. If \(\bm{\theta}_{0}\in\mathbb{R}^{3}\setminus\mathrm{ea}\mathrm{-plane}\), then \(\mathcal{E}(\bm{\theta}_{\mathrm{lim}})=\mathcal{E}(\bm{\theta}_{0})\). Hence \(\bm{\theta}_{\mathrm{lim}}\) lies at the intersection of the contour line \(\mathcal{E}(e,w)=\mathcal{E}_{0}\) with the set of critical points of \(L\) in \(\mathbb{R}^{3}\)._

_On the other hand, if \(\bm{\theta}_{0}\in\mathrm{ea}\mathrm{-plane}\), then \(\bm{\theta}_{\mathrm{lim}}\in\mathrm{ea}\mathrm{-plane}\)._

We defer the proofs of the lemmas to App. N.

### Role of standard initialization

The following picture enhances the behavior of the GF dynamics near the origin.

Based on the above theoretical results and empirical evidence, we conjecture the following theorem, whose informal proof we defer to App. N.5.

**Theorem 11** ( [Informal] Role of standard initialization for \(p+q>1\)).: _If we use the standard initialization \(\bm{\theta}_{0}\sim\mathcal{N}(0,\sigma^{2}\bm{I}_{3})\) with \(\sigma^{2}\ll 1/\sqrt{2}\) for the GF-Attention, \(\bm{\theta}_{\mathrm{lim}}\) will be a local minimum with high probability._

Figure 5: Gradient flow dynamics in \(\mathbb{R}^{3}\), near the origin, for the transformer parameters with attention scalar \(a\) (Sec. 4.1). The local minima are repellors for \(p+q<1\), while attracting for \(p+q>1\).

[MISSING_PAGE_EMPTY:34]

Figure 7: Evolution of parameters \(\bm{W}_{1}\) and \(\bm{W}_{V}\) across iterations, starting from a rank-one initialization. The parameters maintain a rank-one structure across the entire training.

[MISSING_PAGE_EMPTY:36]

Proofs of theorems in App. D

### Proof of Thm. 7

Proof.: We characterize the set of global minima, local minima, and that of the saddle points individually.

**(i) Set of all global minima.** Let \(\boldsymbol{\gamma}_{\star}\in\mathbb{R}^{3}\) be arbitrary. From [23, Lemma 1], we have that \(\boldsymbol{\gamma}_{\star}\) is a global minimum for the loss \(L(\cdot)\) in Eq. (23) if and only if its prediction probability satisfies \(f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})=\mathbb{P}\left(x_{n+1}=1\mid x_{n}\right)\), the Markov kernel. Since the input \(\{x_{n}\}_{n=1}^{N}\sim(\boldsymbol{\pi}(p,q),\boldsymbol{P}(p,q))\), we have that

\[\mathbb{P}\left(x_{n+1}=1\mid x_{n}\right)=(1-x_{n})p+x_{n}(1-q)=(1-p-q)x_{n}+p.\] (50)

On the other hand, by definition, from Eq. (3), \(f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})=\sigma\left(e^{2}(1+2w|w|)\,x_{n}+ b-\frac{e^{2}}{2}\right)\), where \(\boldsymbol{\gamma}_{\star}=(e,w,b)\). Since \(x_{n}\in\{0,1\}\), this can be further simplified to

\[\begin{split} f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})& =\sigma\left(e^{2}(1+2w|w|)\,x_{n}+b-\frac{e^{2}}{2}\right)\\ &=x_{n}\cdot\sigma\left(e^{2}(1+2w|w|)+b-\frac{e^{2}}{2}\right)+( 1-x_{n})\cdot\sigma\left(b-\frac{e^{2}}{2}\right)\\ &=x_{n}\left(\sigma\left(2e^{2}w|w|)+b+\frac{e^{2}}{2}\right)- \sigma\left(b-\frac{e^{2}}{2}\right)\right)+\sigma\left(b-\frac{e^{2}}{2} \right).\end{split}\] (51)

Since both \(f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})\) and \(\mathbb{P}\left(x_{n+1}=1\mid x_{n}\right)\) are linear functions of \(x_{n}\), equating them for all values of \(x_{n}\in\{0,1\}\) implies that the respective coeffecients in these functions in Eq. (50) and Eq. (51) are also equal, i.e.

\[\begin{split}\sigma\left(b-\frac{e^{2}}{2}\right)&= p,\\ \sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)-\sigma\left(b- \frac{e^{2}}{2}\right)&=1-p-q,\end{split}\]

and hence

\[\sigma\left(b-\frac{e^{2}}{2}\right)=p,\quad\sigma\left(2e^{2}w|w|\right)+b+ \frac{e^{2}}{2}\right)=1-q.\] (52)

Since \(\sigma(z)=y\) for \(y\in(0,1)\) implies \(z=\log\frac{y}{1-y}\), Eq. (52) can be rewritten as

\[b-\frac{e^{2}}{2}=\log\frac{p}{1-p},\quad 2e^{2}w|w|+b+\frac{e^{2}}{2}=\log \frac{1-q}{q}.\]

Using \(2e^{2}w|w|+b+\frac{e^{2}}{2}=e^{2}(1+2w|w|)+b-\frac{e^{2}}{2}=e^{2}(1+2w|w|)+ \log\frac{p}{1-p}\) in the second equality above, we obtain

\[\begin{split} b-\frac{e^{2}}{2}&=\log\frac{p}{1-p},\\ e^{2}(1+2w|w|)&=\log\frac{1-q}{q}+\log\frac{1-p}{p}= \log\frac{(1-p)(1-q)}{pq}.\end{split}\] (53)

Thus \(\boldsymbol{\gamma}_{\star}\in\mathbb{R}^{3}\) is a global minimum for \(L(\cdot)\) if and only if it satisfies Eq. (53) (note that it's already a critical point, as established in Thm. 6). Thus, the set of all global minimum \(\boldsymbol{\Gamma}_{\star}(p,q)\) is given by

\[\boldsymbol{\Gamma}_{\star}(p,q)\triangleq\left\{\boldsymbol{\gamma}_{\star} =(e,w,b)\in\mathbb{R}^{3}:e^{2}(1+2w|w|)=\log\frac{(1-p)(1-q)}{pq},\,b-\frac{e^ {2}}{2}=\log\frac{p}{1-p}\right\}.\]

Since the prediction \(f_{\boldsymbol{\gamma}_{\star}}(\cdot)\) equals the Markov kernel for any \(\boldsymbol{\gamma}_{\star}\in\boldsymbol{\Gamma}_{\star}\), it follows from Thm. 4 (or [23, Lemma 1]) that \(L(\boldsymbol{\gamma}_{\star})=H(x_{n+1}\mid x_{n})\), the entropy rate of the Markov chain.

**(ii) Set of local minima and saddle points.**

Define \(\boldsymbol{\Gamma}_{\min}(p,q)\subseteq\mathbb{R}^{3}\) and \(\boldsymbol{\Gamma}_{\rm sad}\subseteq\mathbb{R}^{3}\) as follows:

\[\boldsymbol{\Gamma}_{\min}(p,q)\triangleq\left\{\boldsymbol{\gamma}_{\min}=(e, w,b)\in\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)>0,b=\log\frac{p}{q}\right\},\]

\[\boldsymbol{\Gamma}_{\rm sad}(p,q)\triangleq\left\{\boldsymbol{\gamma}_{\rm sad }=(e,w,b)\in\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)\leq 0,b=\log\frac{p}{q}\right\}.\]

To show that \(\boldsymbol{\Gamma}_{\min}\) is the set of all bad local minima for \(L(\cdot)\), we first show that any \(\boldsymbol{\gamma}_{\min}\in\boldsymbol{\Gamma}_{\min}\) is a bad local minimum and then show that every bad local minimum should belong to \(\boldsymbol{\Gamma}_{\min}\). Similarly for \(\boldsymbol{\Gamma}_{\rm sad}\). We start with the local minima.

Let \(\boldsymbol{\gamma}_{\min}=(e,w,b)\in\boldsymbol{\Gamma}_{\min}\). Recall that \(\boldsymbol{\gamma}_{\min}\) is a stationary point (Thm. 6), i.e.

\[\nabla L(\boldsymbol{\gamma}_{\min})=0.\]

Rearranging the order of scalars and writing \(\boldsymbol{\gamma}_{\min}=(b,e,w)\), from Lemma 4, the Hessian of the loss at \(\boldsymbol{\gamma}_{\min}\) is

\[\nabla^{2}L(\boldsymbol{\gamma}_{\min})=\pi_{0}\pi_{1}\begin{bmatrix}1&0&0\\ 0&2(p+q-1)(1+2w|w|)&0\\ 0&0\end{bmatrix}.\] (54)

By definition, \(\boldsymbol{\gamma}_{\min}=(b,e,w)\) satisfies \((p+q-1)(1+2w|w|)>0\). Thus its Hessian in Eq. (54) has a block diagonal structure of the form \(\begin{bmatrix}\boldsymbol{H}_{b,e}&0\\ 0&0\end{bmatrix}\) where \(\boldsymbol{H}_{b,e}\) has both the eigen values positive, and hence positive-definite. In other words, \(\boldsymbol{\gamma}_{\min}\) is a local minimum for \(L(\cdot)\) in the \((b,e)\in\mathbb{R}^{2}\) space for any fixed \(w\) in the set. Interestingly, using the continuity argument and the fact that \(L(b=\log\frac{p}{q},e=0,w)\) is constant in \(w\in\mathbb{R}\), we can essentially follow the same steps as in proof of Theorem 2 in [23, Appendix B.3] (Thm. 5 above) to show that \(\boldsymbol{\gamma}_{\min}=(b,e,w)\) is a also local minimum for \(L(\cdot)\) in the full parameter space \(\mathbb{R}^{3}\). This establishes that \(\boldsymbol{\gamma}_{\min}\) is a local minimum for \(L(\cdot)\).

For the saddle points, let \(\boldsymbol{\gamma}_{\rm sad}=(e,w,b)\in\boldsymbol{\Gamma}_{\rm sad}\). We have that \(\boldsymbol{\gamma}_{\rm sad}\) is a stationary point (Thm. 6) and Lemma 4 implies its Hessian (after rearranging the order of scalars as above with \(\boldsymbol{\gamma}_{\rm sad}=(b,e,w)\)) is:

\[\nabla^{2}L(\boldsymbol{\gamma}_{\rm sad})=\pi_{0}\pi_{1}\begin{bmatrix}1&0&0 \\ 0&2(p+q-1)(1+2w|w|)&0\\ 0&0&0\end{bmatrix}.\] (55)

If \(w\neq-\frac{1}{\sqrt{2}}\), \((p+q-1)(1+2w|w|)<0\) for any \(\boldsymbol{\gamma}_{\rm sad}\in\boldsymbol{\Gamma}_{\rm sad}\), and hence the Hessian \(\nabla^{2}L(\boldsymbol{\gamma}_{\rm sad})\) in Eq. (55) as both positive, negative, and zero eigen values. Thus \(\boldsymbol{\gamma}_{\rm sad}\) is a saddle point for \(L(\cdot)\). Using a neighborhood argument, we can similarly argue for \(w=\frac{1}{\sqrt{2}}\) to establish that it's also a saddle point. Now we compute the loss value.

For any \(\boldsymbol{\gamma}_{\min}=(e,w,b)\in\boldsymbol{\Gamma}_{\min}\) or \(\boldsymbol{\gamma}_{\rm sad}=(e,w,b)\in\boldsymbol{\Gamma}_{\rm sad}\), we have that \(e=0\) and \(b=\log\frac{p}{q}\). Thus for \(\boldsymbol{\gamma}=\boldsymbol{\gamma}_{\min}\) or \(\boldsymbol{\Gamma}_{\rm sad}\), the prediction probability in view of Eq. (3) is

\[\mathbb{P}_{\boldsymbol{\gamma}}(x_{n+1}=1\mid x_{1}^{n})=\sigma\left(e^{2}(1+2 w|w|)\,x_{n}+b-\frac{e^{2}}{2}\right)=\sigma(b)=\frac{p}{p+q}=\mathbb{P}\left(x_{n+1 }=1\right),\]

the marginal distribution. Substituting this equality in the definition of cross-entropy loss \(L(\cdot)\) in Eq. (1) and the fact that \(\mathbb{P}\left(x_{n+1}=1\right)=\frac{p}{p+q}=\pi_{1}\), following the same steps as in [23, Appendix B.3], we obtain

\[L(\boldsymbol{\gamma})= -\frac{1}{N}\sum_{n\in[N]}\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\cdot \log f_{\boldsymbol{\gamma}}(x_{1}^{n})+(1-x_{n+1})\cdot\log(1-f_{\boldsymbol{ \gamma}}(x_{1}^{n}))]\] \[=-\frac{1}{N}\sum_{n\in[N]}\mathbb{E}_{x_{1}^{n+1}}\left[x_{n+1} \cdot\log\pi_{1}+(1-x_{n+1})\cdot\log\pi_{0}\right]\]

[MISSING_PAGE_FAIL:39]

[MISSING_PAGE_FAIL:40]

Since \(\mathrm{logit}_{n}\) is only a function of \(x_{n}\), the above expectation is over the distribution of the pairs \((x_{n},x_{n+1})\), which for all \(n\in[N]\) have the same law as a pair of random variables \((X,Y)\) with \(X\sim\bm{\pi}\equiv\mathrm{Bern}(p/(p+q))\) and \(Y|X\sim\bm{P}(p,q)\), the Markov kernel. Hence the above equality can be rewritten using the definition of \(\mathrm{logit}_{n}\) as

\[L(\bm{\gamma})=\frac{1}{N}\sum_{n\in[N]}\mathbb{E}[\ell_{\log} \left((2x_{n+1}-1)\cdot\mathrm{logit}_{n}\right)]=\mathbb{E}_{X,Y}\left[\ell_{ \log}\left((2Y-1)\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}}{2}\right)\right)\right].\]

### Proof of Lemma 3

Proof.: With \(\bm{\gamma}=(e,w,b)\) and \(\theta\) denoting either of the scalars \(e,w\), or \(b\), we have from [23, Lemma 2] that the gradient of the loss \(L(\cdot)\) is given by

\[\nabla_{\theta}L(\bm{\gamma})=-\frac{1}{N}\sum_{n\in[N]}\mathbb{E }_{x_{1}^{n+1}}\left[(x_{n+1}-f_{\bm{\theta}}(x_{1}^{n}))\cdot\nabla_{\theta} \,\mathrm{logit}_{n}\right],\] (63)

where \(f_{\bm{\gamma}}(x_{1}^{n})=\sigma(\mathrm{logit}_{n})=\sigma\left(e^{2}(1+2w| w|)\,x_{n}+b-\frac{e^{2}}{2}\right)\). Using the same argument as in the proof of Lemma 6, we can replace the expecatations in Eq. (63) with that of a pair of random variables \((X,Y)\) with \(X\sim\bm{\pi}\equiv\mathrm{Bern}(p/(p+q))\) and \(Y|X\sim\bm{P}(p,q)\), the Markov kernel. That is,

\[\nabla_{\theta}L(\bm{\gamma})=-\mathbb{E}_{X,Y}\left[\left(Y- \sigma\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}}{2}\right)\right)\cdot\nabla_{\theta }\,\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}}{2}\right)\right].\] (64)

Now we define the error term \(\mathcal{E}(X,Y)\triangleq-\left(Y-\sigma\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}} {2}\right)\right)\). Our goal is to show that \(\mathbb{E}[\mathcal{E}(X,Y)\mid X]=f_{1}X+f_{2}\), where \(f_{1}\triangleq\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)+q-1-\sigma \left(b-\frac{e^{2}}{2}\right)+p\), and \(f_{2}\triangleq\sigma\left(b-\frac{e^{2}}{2}\right)-p\), which suffices to prove the lemma. To this end, using the fact that \(X\in\{0,1\}\), we have

\[\mathcal{E}(X,Y) =-\left(Y-\sigma\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}}{2}\right)\right)\] \[=-\left(Y-X\cdot\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)-( 1-X)\cdot\sigma\left(b-\frac{e^{2}}{2}\right)\right)\] \[=-Y+X\left(\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)-\sigma \left(b-\frac{e^{2}}{2}\right)\right)+\sigma\left(b-\frac{e^{2}}{2}\right).\]

Now taking the conditional expectation with respect to \(X\) and using the fact that \(\mathbb{E}[Y|X]=\mathbb{P}\left(Y=1\mid X\right)=(1-p-q)X+p\) (since \(Y|X\sim\bm{P}(p,q)\)), we have

\[\mathbb{E}[\mathcal{E}(X,Y)\mid X] =-(1-p-q)X-p+X\left(\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2} \right)-\sigma\left(b-\frac{e^{2}}{2}\right)\right)+\sigma\left(b-\frac{e^{2} }{2}\right)\] \[=X\left(\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)+q-1- \sigma\left(b-\frac{e^{2}}{2}\right)+p\right)+\sigma\left(b-\frac{e^{2}}{2} \right)-p\] \[\stackrel{{(a)}}{{=}}f_{1}X+f_{2},\]

where \((a)\) follows from the definition of \(f_{1}\) and \(f_{2}\) above. Thus Eq. (64) simplifies to

\[\nabla_{\theta}L(\bm{\gamma})=\mathbb{E}_{X}\left[(f_{1}X+f_{2}) \cdot\nabla_{\theta}\,\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}}{2}\right)\right].\]

Letting \(\theta=e,w\), and \(b\) in the above equation, we finally obtain the individual gradients:

\[\frac{\partial L}{\partial e}=\mathbb{E}_{X}\left[(f_{1}X+f_{2})(2X(1+2w|w|-1)) \right]\cdot e,\]\[\frac{\partial L}{\partial w} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]\cdot 4e^{2}|w|,\] \[\frac{\partial L}{\partial b} =\mathbb{E}_{X}\left[f_{1}X+f_{2}\right].\]

### Proof of Lemma 4

Proof.: Slightly changing the variable order, for any \(\bm{\gamma}=(b,e,w)\in\mathbb{R}^{3}\), we define

\[\bm{H}(\bm{\gamma})\triangleq\nabla^{2}L(\bm{\gamma})=\begin{bmatrix}\frac{ \partial^{2}L}{\partial b^{2}}&\frac{\partial^{2}L}{\partial b\partial e}& \frac{\partial^{2}L}{\partial b\partial w}\\ \frac{\partial^{2}L}{\partial e\partial b}&\frac{\partial^{2}L}{\partial e^ {2}}&\frac{\partial^{2}L}{\partial e\partial w}\\ \frac{\partial^{2}L}{\partial w\partial b}&\frac{\partial^{2}L}{\partial w \partial e}&\frac{\partial^{2}L}{\partial w^{2}}\end{bmatrix}\in\mathbb{R}^{3 \times 3}.\] (65)

Recall that for any \(\bm{\gamma}_{\min}\in\bm{\Gamma}_{\min}\) and \(\bm{\gamma}_{\rm sad}\in\bm{\Gamma}_{\rm sad}\), we have \(e=0\) and \(b=\log\frac{p}{q}\). Now we compute the second derivatives of \(L\) with respect to any \(\bm{\gamma}=(b=\log\frac{p}{q},e=0,w)\). We start with the first derivatives. By Lemma 7, the gradients are

\[\frac{\partial L}{\partial b} =\mathbb{E}_{X}\left[f_{1}X+f_{2}\right],\] (66) \[\frac{\partial L}{\partial e} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})(2X(1+2w|w|)-1))\right]\cdot e,\] \[\frac{\partial L}{\partial w} =\mathbb{E}_{X}\left[(f_{1}X+f_{2})X\right]\cdot 4e^{2}|w|,\]

where

\[f_{1} \triangleq\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)+q-1- \sigma\left(b-\frac{e^{2}}{2}\right)+p,\] \[f_{2} \triangleq\sigma\left(b-\frac{e^{2}}{2}\right)-p.\]

From Eq. (66), we see that the second derivates of \(L\) depend on the first-derivatives of \(f_{1}\) and \(f_{2}\), which we now compute. Recall that the derivative of the sigmoid function obeys \(\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))=\sigma(z)\sigma(-z)\) for any \(z\in\mathbb{R}\). Now the gradients of \(f_{1}\) and \(f_{2}\) with respect to \(b,e\), and \(w\) are

\[\frac{\partial f_{1}}{\partial b} =\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)\sigma\left(-2e^ {2}w|w|-b-\frac{e^{2}}{2}\right)-\sigma\left(b-\frac{e^{2}}{2}\right)\sigma \left(-b+\frac{e^{2}}{2}\right),\] \[\frac{\partial f_{2}}{\partial b} =\sigma\left(b-\frac{e^{2}}{2}\right)\sigma\left(-b+\frac{e^{2}} {2}\right),\] \[\frac{\partial f_{1}}{\partial e} =(4ew|w|+e)\,\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right) \sigma\left(-2e^{2}w|w|-b-\frac{e^{2}}{2}\right)+e\,\sigma\left(b-\frac{e^{2} }{2}\right)\sigma\left(-b+\frac{e^{2}}{2}\right),\] \[\frac{\partial f_{2}}{\partial e} =(-e)\,\sigma\left(b-\frac{e^{2}}{2}\right)\sigma\left(-b+\frac{e ^{2}}{2}\right),\] \[\frac{\partial f_{1}}{\partial w} =(4e^{2}\cdot w{\rm sign}(w))\,\sigma\left(2e^{2}w|w|+b+\frac{e^{ 2}}{2}\right)\sigma\left(-2e^{2}w|w|-b-\frac{e^{2}}{2}\right),\] \[\frac{\partial f_{2}}{\partial w} =0.\]Using the fact that \(\sigma\left(\log\frac{p}{q}\right)=\frac{p}{p+q}=\pi_{1}\) and \(\sigma\left(-\log\frac{p}{q}\right)=\frac{q}{p+q}=\pi_{0}\), the above gradients evaluated for any \(\bm{\gamma}=(b=\log\frac{p}{q},e=0,w)\) further reduce to

\[\begin{split}\frac{\partial f_{1}}{\partial b}\bigg{|}_{\bm{ \gamma}}=0,\quad\frac{\partial f_{2}}{\partial b}\bigg{|}_{\bm{\gamma}}=\pi_{0 }\pi_{1},\\ \frac{\partial f_{1}}{\partial e}\bigg{|}_{\bm{\gamma}}=0,\quad \frac{\partial f_{2}}{\partial e}\bigg{|}_{\bm{\gamma}}=0,\\ \frac{\partial f_{1}}{\partial w}\bigg{|}_{\bm{\gamma}}=0,\quad \frac{\partial f_{2}}{\partial w}\bigg{|}_{\bm{\gamma}}=0.\end{split}\] (67)

Now substituting Eq. (67) when computing the second-derivatives of \(L\) in Eq. (66), we obtain

\[\begin{split}\frac{\partial^{2}L}{\partial b^{2}}\bigg{|}_{\bm{ \gamma}}=\mathbb{E}_{X}\left[\frac{\partial f_{1}}{\partial b}\bigg{|}_{\bm{ \gamma}}X+\frac{\partial f_{2}}{\partial b}\bigg{|}_{\bm{\gamma}}\right]=\pi_ {0}\pi_{1},\\ \frac{\partial^{2}L}{\partial b\partial e}\bigg{|}_{\bm{\gamma}}= \mathbb{E}_{X}\left[\frac{\partial f_{1}}{\partial e}\bigg{|}_{\bm{\gamma}}X+ \frac{\partial f_{2}}{\partial e}\bigg{|}_{\bm{\gamma}}\right]=0,\\ \frac{\partial^{2}L}{\partial b\partial w}\bigg{|}_{\bm{\gamma}}= \mathbb{E}_{X}\left[\frac{\partial f_{1}}{\partial w}\bigg{|}_{\bm{\gamma}}X+ \frac{\partial f_{2}}{\partial w}\bigg{|}_{\bm{\gamma}}\right]=0,\\ \frac{\partial^{2}L}{\partial e^{2}}\bigg{|}_{\bm{\gamma}}= \mathbb{E}_{X}\left[(f_{1}X+f_{2})(2X(1+2w|w|)-1))\right]\bigg{|}_{\bm{\gamma}} \\ =\mathbb{E}_{X}\left[(2f_{1}(1+2w|w|)-f_{1}+2f_{2}(1+2w|w|)X-f_{2 })\right]_{\bm{\gamma}}\\ =\mathbb{E}_{X}\left[(f_{1}(1+4w|w|)+f_{2}(2+4w|w|)X-f_{2}) \right]_{\bm{\gamma}}\\ =(f_{1}(1+4w|w|)+f_{2}(2+4w|w|))\pi_{1}-f_{2}\bigg{|}_{\bm{\gamma}} \\ \stackrel{{(a)}}{{=}}((p+q-1)(1+4w|w|)-\pi_{1}(p+q-1) (2+4w|w|))\pi_{1}+\pi_{1}(p+q-1)\\ =\pi_{1}(p+q-1)\left(1+4w|w|-\pi_{1}(2+4w|w|)+1\right)\\ \stackrel{{(b)}}{{=}}2\pi_{1}\pi_{0}(p+q-1)(1+2w|w|), \end{split}\] (68)where \((a)\) follows from the fact that \(f_{1}|_{\bm{\gamma}}=p+q-1,f_{2}|_{\bm{\gamma}}=\sigma(b)-p=\frac{p}{p+q}-p=\frac {-p}{p+q}(p+q-1)=-\pi_{1}(p+q-1)\) and \((b)\) from \(1-\pi_{1}=\pi_{0}\). Returning to the remaining second derivatives,

\[\left.\frac{\partial^{2}L}{\partial e\partial w}\right|_{\bm{ \gamma}} =\left.\frac{\partial}{\partial e}\left(\mathbb{E}[(f_{1}X+f_{2})X] \cdot 4e^{2}|w|\right)\right|_{\bm{\gamma}}\] \[=\left.\frac{\partial}{\partial e}\left(\mathbb{E}[(f_{1}+f_{2}) X]\cdot 4e^{2}|w|\right)\right|_{\bm{\gamma}}\] \[=\left.\frac{\partial}{\partial e}\left((f_{1}+f_{2})\cdot 4\pi_{1}e^{ 2}|w|\right)\right|_{\bm{\gamma}}\] \[=\left.\frac{\partial}{\partial e}\left(\left(\sigma\left(2e^{2} w|w|+b+\frac{e^{2}}{2}\right)+q-1\right)\cdot 4\pi_{1}e^{2}|w|\right)\right|_{\bm{ \gamma}}\] \[=\left(\frac{\partial}{\partial e}\sigma\left(2e^{2}w|w|+b+ \frac{e^{2}}{2}\right)\right)4\pi_{1}e^{2}|w|\bigg{|}_{\bm{\gamma}}\] (69) \[\qquad\quad+\left(\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right) +q-1\right)\cdot\frac{\partial}{\partial e}(4\pi_{1}e^{2}|w|)\bigg{|}_{\bm{ \gamma}}\] \[=0,\] \[\left.\frac{\partial^{2}L}{\partial w^{2}}\right|_{\bm{\gamma}} =\frac{\partial}{\partial w}\left(\mathbb{E}[(f_{1}X+f_{2})X] \cdot 4e^{2}|w|\right)\] \[=\left(\frac{\partial}{\partial w}\mathbb{E}[(f_{1}X+f_{2})X] \cdot 4|w|\right)e^{2}\bigg{|}_{\bm{\gamma}}\] \[=0.\]

Congregating all the second derivatives from Eq. (68) and Eq. (69) into the Hessian \(\bm{H}(\bm{\gamma})\) in Eq. (65), we finally obtain

\[\bm{H}(\bm{\gamma})=\pi_{0}\pi_{1}\begin{bmatrix}1&0&0\\ 0&2(p+q-1)(1+2w|w|)&0\\ 0&0&0\end{bmatrix}.\]Proofs of lemmas in App. E

### Proof of Lemma 5

Proof.: Recall from Lemma 2 that for any \(\bm{\theta}=(e,w)\in\mathbb{R}^{2}\) and \(b\in\mathbb{R}\), we have

\[L(\bm{\theta},b)=\mathbb{E}_{X,Y}\left[\ell_{\log}\left((2Y-1)\left(e^{2}(1+2w|w |)X+b-\frac{e^{2}}{2}\right)\right)\right].\]

Since \(\ell_{\log}(\cdot)\) is a convex function, \(Y\in\{0,1\}\) and thus \(2Y-1\in\{\pm 1\}\), the convexity of \(L\) in \(b\) follows from the following fact:

\[\frac{\partial^{2}L}{\partial b^{2}}=\mathbb{E}_{X,Y}\left[\ell_{\log}^{\prime \prime}\left((2Y-1)\left(e^{2}(1+2w|w|)X+b-\frac{e^{2}}{2}\right)\right) \right]\geq 0.\]

To find the optimal \(b_{\star}\), we set the gradient \(\frac{\partial L}{\partial b}=0\). Thus from Lemma 3, we obtain

\[\frac{\partial L}{\partial b}=\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=0,\]

\[f_{1}=\sigma\left(2e^{2}w|w|+b+\frac{e^{2}}{2}\right)+q-1-\sigma\left(b-\frac {e^{2}}{2}\right)+p,f_{2}=\sigma\left(b-\frac{e^{2}}{2}\right)-p.\]

Substituting \(E\triangleq e^{2}(1+2w|w|)\), \(B\triangleq b-\frac{e^{2}}{2}\), and \(\mathbb{E}[X]=\pi_{1}=p/(p+q)\) in the above equations,

\[\pi_{1}\left(\sigma(E+B)+q-1-\sigma(B)+p\right)+\sigma(B)-p=\pi_{1}\cdot \sigma(E+B)+\pi_{0}\cdot\sigma(B)+\frac{p(p+q-1)}{p+q}-p=0.\]

Further simplifying,

\[\pi_{0}\cdot\sigma(B)=\pi_{1}\cdot(1-\sigma(E+B))=\pi_{1}\cdot\sigma(-E-B).\]

In other words,

\[\frac{(1+\exp(-B))^{-1}}{(1+\exp(E+B))^{-1}}=\frac{\pi_{1}}{\pi_{0}}=\frac{p}{ q}\Rightarrow\frac{1+\exp(E)\exp(B)}{1+\exp(-B)}=\frac{p}{q}.\]

Defining \(x\triangleq\exp(B)\) and \(A\triangleq\exp(E)\), we thus obtain the following quadratic equation in \(x\) and its corresponding roots:

\[Ax^{2}-x\left(\frac{p}{q}-1\right)-\frac{p}{q}=0\Rightarrow x=\frac{1}{2A} \left[\frac{p}{q}-1\pm\sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q} \cdot A}\right].\]

Since \(x>0\), we take the root corresponding to the addition choice above and resubstituting \(x=\exp(b-\frac{e^{2}}{2})\) and \(A=\exp(e^{2}(1+2w|w|))\), we obtain the final expression for \(b_{\star}\). In particular, if \(e=0\), it is easy to see that \(A=1\) and hence \(x=\exp(b_{\star})=\frac{p}{q}\), implying \(b_{\star}=\log\frac{p}{q}\). Similarly, if \(A=\frac{(1-p)(1-q)}{pq}\), it's straightforward to see that \(\exp(b_{\star}-e^{2}/2)=\frac{p}{1-p}\) and hence \(b_{\star}-e^{2}/2=\log\frac{p}{1-p}\). 

### Proof of Lemma 6

Proof.: The proof directly follows from Lemma 2 by substituting \(b=b_{\star}\). 

### Proof of Lemma 7

Proof.: By Danskin's theorem [12], it follows that for \(b_{\star}=\operatorname*{argmin}_{b\in\mathbb{R}}L(\bm{\theta},b)\) and \(L(\bm{\theta})=L(\bm{\theta},b_{\star})\), we have \(\nabla_{\bm{\theta}}L(\bm{\theta})=\nabla_{\bm{\theta}}L(\bm{\theta},b_{\star})\). Using the gradient expressions of \(L(\bm{\theta},b)\) w.r.t \(\bm{\theta}\) from Lemma 3, and using the fact that \(\frac{\partial L}{\partial b}=\mathbb{E}[f_{1}X+f_{2}]\) at \(b=b_{\star}\), the claim follows. 

### Proof of Lemma 8

Proof.: Since \(L(\bm{\theta})=L(\bm{\theta},b_{\star})\) where \(b_{\star}=\operatorname*{argmin}_{b\in\mathbb{R}}L(\bm{\theta},b)\), the identity in Eq. (32) about the Hessian of the loss \(L\) with respect to \(\bm{\theta}\) follows from the classical result of [29, Lemma 2.2] about second-derivatives of extremal-value functions. Finally Eq. (33) follows from substituting the full Hessian in \(\mathbb{R}^{3\times 3}\) from Lemma 4 in this identity.

Proofs of lemmas in App. F

### Proof of Lemma 9

Proof.: Denote \((e_{t},w_{t})=(e,w)\) with the dependence on time implicitly assumed. Then by the definition of GF and the gradient expressions in Lemma 7, we have that

\[\frac{\mathrm{d}e}{\mathrm{d}t} =-\frac{\partial L}{\partial e}(\boldsymbol{\theta}_{t})=-2\mathbb{ E}[(f_{1}X+f_{2})X]\cdot(1+2w|w|)e,\] (70) \[\frac{\mathrm{d}w}{\mathrm{d}t} =-\frac{\partial L}{\partial w}(\boldsymbol{\theta}_{t})=-\mathbb{ E}[(f_{1}X+f_{2})X]\cdot 4e^{2}|w|.\] (71)

Dividing Eq. (70) by \((1+2w|w|)e\) and Eq. (71) by \(4e^{2}|w|\), we have

\[\frac{1}{1+2w|w|}\frac{\mathrm{d}e}{\mathrm{d}t} =\frac{1}{2e|w|}\frac{\mathrm{d}w}{\mathrm{d}t}\Rightarrow e \frac{\mathrm{d}e}{\mathrm{d}t}=\left(w+\frac{1}{2|w|}\right)\frac{\mathrm{d} w}{\mathrm{d}t}.\]

Noting that \(\frac{\mathrm{d}}{\mathrm{d}t}(\mathrm{sign}(w)\cdot\log|w|)=\frac{1}{|w|} \frac{\mathrm{d}w}{\mathrm{d}t}\), the above equation can be rewritten as

\[\frac{\mathrm{d}}{\mathrm{d}t}\left(e^{2}-w^{2}-\mathrm{sign}(w )\cdot\log|w|\right)=0.\]

Thus defining \(\mathcal{E}(e,w)=e^{2}-w^{2}-\mathrm{sign}(w)\cdot\log|w|\) for \(w\neq 0\), the above equation implies \(\mathcal{E}(\boldsymbol{\theta}_{t})=\mathcal{E}(\boldsymbol{\theta}_{0})\) for \(\boldsymbol{\theta}_{0}=(e_{0},w_{0})\) with \(w_{0}\neq 0\). On the other hand, it's easy to see that if \(w_{0}=0\), Eq. (71) implies \(\frac{\mathrm{d}w}{\mathrm{d}t}=0\) at \(t=0\) and hence \(w_{t}=0\) for all \(t\geq 0\). 

### Proof of Lemma 10

Proof.: To prove the convergence of the trajectory \((\boldsymbol{\theta}_{t})_{t\geq 0}\), we use the classical result due to Lojasiewicz [1, Theorem 2.2] which gurantees the convergence of gradient flow for real analytic functions, as long as the trajectory is bounded. Hence we first show the boundedness of the trajectory.

(i) \((\boldsymbol{\theta}_{t})_{t\geq 0}\) is bounded.

We consider the cases \(\boldsymbol{\theta}_{0}\in\mathrm{e}\)-axis and \(\boldsymbol{\theta}_{0}\in\mathbb{R}^{2}\setminus\mathrm{e}\)-axis separately.

Let's suppose \(\boldsymbol{\theta}_{0}=(e_{0},w_{0})\in\mathrm{e}\)-axis, i.e. \(w_{0}=0\). Thus it follows from Lemma 9 that \(w_{t}=0\) for all \(t\geq 0\). That is, the trajectory always stays on the \(\mathrm{e}\)-axis and it suffices to track \((e_{t})_{t\geq 0}\) and show that they are bounded. To this end, we show that if \(e_{0}>0\), we have \(\frac{\mathrm{d}e}{\mathrm{d}t}<0\) and if \(e_{0}<0\), we have \(\frac{\mathrm{d}e}{\mathrm{d}t}>0\) for all \(t\geq 0\), which establishes our claim. We have from the GF and Lemma 7 that

\[\frac{\mathrm{d}e_{t}}{\mathrm{d}t} =-\frac{\partial L}{\partial e}(e_{t},w_{t}=0)=-2\mathbb{E}_{X} \left[(f_{1}X+f_{2})X\right]e_{t}=-2\pi_{1}(f_{1}+f_{2})e_{t},\] (72)

and

\[f_{1}+f_{2} =-\frac{f_{2}}{\pi_{1}}+f_{2}=-\frac{\pi_{0}}{\pi_{1}}f_{2}=- \frac{\pi_{0}}{\pi_{1}}\left[\sigma\left((b_{\star})_{t}-\frac{e_{t}^{2}}{2} \right)-p\right]\] \[=-\frac{\pi_{0}}{\pi_{1}}\left[\left(1+\exp\left(-(b_{\star})_{ t}+\frac{e_{t}^{2}}{2}\right)\right)^{-1}-p\right]\] \[=-\frac{\pi_{0}}{\pi_{1}}\left[\left(1+\frac{2x_{t}}{\frac{p}{q} -1+\sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q}\cdot x_{t}}}\right) ^{-1}-p\right],\quad x_{t}\triangleq\exp(e_{t}^{2}).\] (73)

Defining

\[g(x)\triangleq\frac{2x}{\frac{p}{q}-1+\sqrt{\left(\frac{p}{q}-1 \right)^{2}+4\cdot\frac{p}{q}\cdot x}},\] (74)and substituting Eq. (74) and Eq. (73) in Eq. (72), we obtain

\[\frac{\mathrm{d}e_{t}}{\mathrm{d}t}=2\pi_{0}\left(\frac{1}{1+g(x_{t})}-p\right) \cdot e_{t}.\] (75)

Since \(x_{t}=\exp(e_{t}^{2})=\exp(-e_{t}^{2})\), in view of Eq. (75), with out loss of generality, we can assume that \(e_{0}>0\) and show that \(\frac{\mathrm{d}e_{t}}{\mathrm{d}t}<0\) for all \(t\geq 0\). That is, the RHS Eq. (75) is negative. Note that \(x_{t}\geq 1\) since \(x_{t}=\exp(e_{t}^{2})\) and \(g(x_{t})>0\) since the denominator \(\frac{p}{q}-1+\sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q}\cdot x_{ t}}>\frac{p}{q}-1+\frac{p}{q}+1=2\cdot\frac{p}{q}>0\). Further \(g(1)=\frac{q}{p}\) and \(\lim_{x\to\infty}g(x)=\infty\). If we show that \(g(x)\) is increasing in \(x\) for \(x\geq 1\), it implies \(\frac{1}{1+g(x)}-p<\frac{1}{1+g(1)}-p=\frac{1}{1+\frac{x}{p}}-p=-\frac{p}{p+q} (p+q-1)<0\). Thus the gradient in Eq. (75) remains negative starting at \(t=0\) and hence the sequence \((e_{t})_{t\geq 0}\) will be bounded. Now it remains to show \(g(\cdot)\) is increasing, i.e. \(g^{\prime}(\cdot)>0\). Defining \(C=\left(\frac{p}{q}-1\right)/\left(2\sqrt{\frac{p}{q}}\right)\) and \(D=C^{2}\), we have that \(g(x)\) upto a postive scaling is

\[g(x)=\frac{x}{C+\sqrt{x+D}}.\]

Hence

\[g^{\prime}(x)=\frac{C+\sqrt{x+D}-\frac{x}{2\sqrt{x+D}}}{(C+\sqrt{x+D})^{2}}.\]

Thus it suffices to show that \(h_{1}(x)\triangleq C+\sqrt{x+D}>h_{2}(x)\triangleq\frac{x}{2\sqrt{x+D}}\) for \(x\geq 1\). Note that \(h_{1}(1)-h_{2}(1)\) is given by

\[h_{1}(1)-h_{2}(1) =\frac{\frac{p}{q}-1}{2\sqrt{\frac{p}{q}}}+\sqrt{1+\left(\frac{ \frac{p}{q}-1}{2\sqrt{\frac{p}{q}}}\right)^{2}}-\frac{1}{2\sqrt{1+\left(\frac {\frac{p}{q}-1}{2\sqrt{\frac{p}{q}}}\right)^{2}}}\] \[=\sqrt{\frac{p}{q}}-\frac{\sqrt{\frac{p}{q}}}{1+\frac{p}{q}}\] \[>0.\]

Now we show that \(h^{\prime}(x)>h^{\prime}_{2}(x)\) for all \(x\geq 1\) which implies that \(h_{1}(x)>h_{2}(x)\) for all \(x\geq 1\), thus establishing our claim. To this end, we have that

\[h^{\prime}_{1}(x)-h^{\prime}_{2}(x) =\frac{1}{2\sqrt{x+D}}-\frac{\sqrt{x+D}-\frac{x}{2\sqrt{x+D}}}{2( x+D)}\] \[=\frac{x}{2\sqrt{x+D}(x+D)}>0.\]

This proves our claim that \(g(\cdot)\) is increasing and hence \((e_{t})_{t\geq 0}\), and consequently \((\boldsymbol{\theta}_{t})_{t\geq 0}\), is bounded when \(\boldsymbol{\theta}_{0}\in\mathrm{e}\mathrm{-axis}\).

Now let's assume that \(\boldsymbol{\theta}_{0}=(e_{0},w_{0})\in\mathbb{R}^{2}\setminus\mathrm{e} \mathrm{-axis}\). Since \(\left(\boldsymbol{\theta}_{t}\right)_{t\geq 0}\subseteq\mathbb{R}^{2}\setminus \mathrm{e}\mathrm{-axis}\), it follows that the loss \(L(\cdot)\) is analytic on the trajectory (since the logistic function is analytic), and hence by [1, Theorem 2.2], it follows that \(\lim_{t\to\infty}\|\boldsymbol{\theta}_{t}\|\) exists. Now we show that \(\lim_{t\to\infty}\|\boldsymbol{\theta}_{t}\|\neq\infty\), which implies the desired result about boundedness. To show \(\lim_{t\to\infty}\|\boldsymbol{\theta}_{t}\|\neq\infty\), we show that there exists a large \(B>0\) such that for any \(\boldsymbol{\theta}_{t}=(e,w)\in\mathbb{R}^{2}\) with \(\|(e,w)\|\geq B\), the velocity vector \(\frac{\mathrm{d}\theta}{\mathrm{d}t}\) points inwards into the ball of radius \(B\) and thus the trajectory always stays inside this ball, and hence bounded. To establish this, let's denote \((e_{t},w_{t})=(e,w)\) with the dependence on time implicitly assumed. Then by the definition of GF and the gradient expressions in Lemma 7, we have that

\[\frac{\mathrm{d}e}{\mathrm{d}t} =-\frac{\partial L}{\partial e}(\boldsymbol{\theta}_{t})=-2\mathbb{ E}[(f_{1}X+f_{2})X]\cdot(1+2w|w|)e\] (76) \[\frac{\mathrm{d}w}{\mathrm{d}t} =-\frac{\partial L}{\partial w}(\boldsymbol{\theta}_{t})=-\mathbb{ E}[(f_{1}X+f_{2})X]\cdot 4e^{2}|w|,\] (77)where \(f_{1}=\sigma\left(2e^{2}w|w|+b_{\star}+\frac{e^{2}}{2}\right)+q-1-\sigma\left(b_{ \star}-\frac{e^{2}}{2}\right)+p\), and \(f_{2}=\sigma\left(b_{\star}-\frac{e^{2}}{2}\right)-p\) with \(\pi_{1}f_{1}+f_{2}=0\). Given that only \(\frac{\mathrm{d}e}{\mathrm{d}t}\) flips in sign under the transformation \((e,w)\mapsto(-e,w)\), with out loss of generality we can assume \(e>0\). Now let's also assume \(w>0\). Thus, in view of Eq. (76) and \(\mathrm{GF}\), to show that the derivative points inwards, it suffices to show that \(\mathbb{E}[(f_{1}X+f_{2})X]>0\) for reasonably large \(B\) with \(\|(e,w)\|=B\). Similar to Eq. (73) and Eq. (74) above, using the relation \(\pi_{1}f_{1}+f_{2}=0\), we obtain

\[\mathbb{E}[(f_{1}X+f_{2})X]=\pi_{1}(f_{1}+f_{2})=-\pi_{0}\left(\frac{1}{1+g(x )}-p\right),\quad x\triangleq\exp(e^{2}(1+2w|w|)).\] (78)

Using the fact that \(g(x)\) is increasing for \(x\geq 1\) with \(\lim_{x\to\infty}g(x)=\infty\), and \(|w|=w>0\), we can chose a \(B>0\) such that for any \(\|(e,w)\|\geq B\), in Eq. (78) we have \(1/(1+g(x))<p\) and hence \(\mathbb{E}[(f_{1}X+f_{2})X]>0\). This finishes the proof of our claim. The proof for \(w<0\) is similar, where we make use of the fact that \(\lim_{x\to 0}g(x)=0\) to show \(\mathbb{E}[(f_{1}X+f_{2})X]<0\) for \(e,w\) reasonably large.

(ii) \(\lim_{t\to\infty}\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{\lim}\). Since the logistic function \(\ell_{\log}(\cdot)\) is analytic, it follows from Lemma 6 that the loss \(L(\boldsymbol{\theta})\) is analytic too whenever \(w\neq 0\). On the other hand, when \(w=0\), it's easy to see that \(L\) is an analytic function of \(e\in\mathbb{R}\). By Lemma 9, we know that if \(w_{0}\neq 0\), \(w_{t}\neq 0\) and if \(w_{0}=0\), \(w_{t}=0\) for all \(t\geq 0\). Thus the loss is analytic on the trajectory for all \(t\geq 0\). Since the trajectory is bounded, it follows from Lojasiewicz's theorem [1, Theorem 2.2] that there exists a \(\boldsymbol{\theta}_{\lim}\in\mathbb{R}^{2}\) such that \(\lim_{t\to\infty}\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{\lim}\).

(iii) \(\lim_{t\to\infty}\|\nabla L(\boldsymbol{\theta}_{t})\|=\|\nabla L(\boldsymbol{ \theta}_{\lim})\|=0\). Since the trajectory is bounded, it follows from [2, Theorem 2] that the gradient converges to zero, i.e. \(\lim_{t\to\infty}\|\nabla L(\boldsymbol{\theta}_{t})\|=0\). Since \(\nabla L(\cdot)\) is a continuous function and \(\lim_{t\to\infty}\boldsymbol{\theta}_{t}=\boldsymbol{\theta}_{\lim}\), we have \(\lim_{t\to\infty}\|\nabla L(\boldsymbol{\theta}_{t})\|=\|\nabla L(\boldsymbol {\theta}_{\lim})\|=0\).

### Proof of Lemma 11

Proof.: Since the energy function \(\mathcal{E}(\cdot,\cdot)\) in Eq. (48) is a continuous function in \(\mathbb{R}^{2}\setminus\mathrm{e}\)-axis, and any trajectory \((\boldsymbol{\theta}_{t})_{t\geq 0}\) with initialization \(\theta\in\mathbb{R}^{2}\setminus\mathrm{e}\)-axis stays in \(\mathbb{R}^{2}\setminus\mathrm{e}\)-axis for all \(t\geq 0\) (Lemma 9), it follows that \(\lim_{t\to\infty}\mathcal{E}(\boldsymbol{\theta}_{t})=\mathcal{E}(\boldsymbol {\theta}_{\lim})=\mathcal{E}(\boldsymbol{\theta}_{0})\). As \(\nabla L(\boldsymbol{\theta}_{\lim})=0\) from Lemma 10, it follows that \(\boldsymbol{\theta}_{\lim}\) lies at the intersection of the contour line \(\mathcal{E}(e,w)=\mathcal{E}_{0}\) with the set of critical points of \(L\) in \(\mathbb{R}^{2}\). On the other hand, if \(\boldsymbol{\theta}_{0}\in\mathrm{e}\)-axis, we have \(\boldsymbol{\theta}_{t}\in\mathrm{e}\)-axis from Lemma 9 for all \(t\geq 0\). Hence \(\boldsymbol{\theta}_{\lim}\in\mathrm{e}\)-axis. 

### Proof of Lemma 12

Proof.: Recall that \(f:\mathbb{R}\setminus\{0\}\to\mathbb{R}\), defined as \(f(w)\triangleq\mathcal{E}(e=0,w)=-(w^{2}+\mathrm{sign}(w)\cdot\log|w|)\). If \(w<0\), we have

\[f(w)=-(w^{2}-\log(-w)),\quad f^{\prime}(w)=-2w+\frac{1}{w}.\]

Hence \(f^{\prime}(w)\geq 0\) for \(w\in(-\infty,-1/\sqrt{2}]\) and \(f^{\prime}(w)\leq 0\) for \(w\in[-1/\sqrt{2},0)\) with \(f^{\prime}(-\frac{1}{\sqrt{2}})=0\). It's also straightforward to see that \(\lim_{w\to-\infty}f(w)=-\infty\), \(\lim_{w\to 0^{-}}f(w)=-\infty\), and \(f(-1/\sqrt{2})=\mathcal{E}_{\mathrm{sad}}\) (by the definition of \(f\)). This establishes (i), (ii), and (iii).

On the other hand, for \(w>0\), we have \(f(w)=-(w^{2}+\log w)\) and \(f^{\prime}(w)=-(2w+1/w)\). Hence \(f\) is monotonically decreasing for \(w>0\) with \(\lim_{w\to 0^{+}}f(w)=\infty\) and \(\lim_{w\to\infty}f(w)=-\infty\). Note that \(w=0\) acts as an energy barrier since \(\lim_{w\to 0^{-}}f(w)=-\infty\) whereas \(\lim_{w\to 0^{+}}f(w)=\infty\).

Proofs of lemmas in App. G

### Proof of Lemma 13

Proof.: First we recall the loss with the bias \(L(\bm{\theta},b)\) from Eq. (41):

\[L(\bm{\theta},b)=\mathbb{E}_{X,Y}\left[\ell_{\log}\left((2Y-1)\cdot\mathrm{logit}_ {X}(\bm{\theta},b)\right)\right],\]

where \(\mathrm{logit}_{X}(\bm{\theta},b)=e^{2}\left[\left(X-\frac{1}{2}\right)\left(1+ ae^{2}\right)\left(1+2w|w|)+w|w(1+ae^{2})|\right]+b\) with \(\bm{\theta}=(e,w,a)\). Using the fact that \(\ell_{\log}^{\prime}(z)=\sigma(z)-1\) and \(2Y-1\in\{\pm 1\}\), we have for any \(\theta\in\{e,w,a,b\}\) that

\[\nabla_{\theta}L=\mathbb{E}\left[\left(\sigma((2Y-1)\cdot\mathrm{logit}_{X}) -1\right)(2Y-1)\cdot\nabla_{\theta}\mathrm{logit}_{X}\right]=\mathbb{E}\left[ \left(\sigma(\mathrm{logit}_{X})-Y\right)\cdot\nabla_{\theta}\mathrm{logit}_ {X}\right].\] (79)

Now we simplify \(\sigma(\mathrm{logit}_{X})\) using the fact that \(X\in\{0,1\}\):

\[\sigma(\mathrm{logit}_{X}) =\sigma\left(e^{2}\left[\left(X-\frac{1}{2}\right)\left(1+ae^{2} \right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] \[=X\underbrace{\sigma\left(e^{2}\left[\frac{1}{2}\left(1+ae^{2} \right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)}_{\triangleq\phi_{ 1}}\] \[+(1-X)\underbrace{\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^ {2}\right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)}_{\triangleq \phi_{0}}\] \[=X\phi_{1}+(1-X)\phi_{0}\] \[=X(\phi_{1}-\phi_{0})+\phi_{0}.\]

Thus the gradients in Eq. (79) are given by

\[\nabla_{\theta}L =-\mathbb{E}\left[\left(Y-X(\phi_{1}-\phi_{0})-\phi_{0}\right) \nabla_{\theta}\mathrm{logit}_{X}\right]\] (80) \[=-\mathbb{E}_{X}\left[\left(\left(1-p-q\right)X+p-X(\phi_{1}-\phi _{0})-\phi_{0}\right)\nabla_{\theta}\mathrm{logit}_{X}\right]\] \[=-\mathbb{E}_{X}\left[\left(\left(\underbrace{1-p-q-\phi_{1}+ \phi_{0}}_{f_{1}}\right)X+\underbrace{p-\phi_{0}}_{f_{2}}\right)\nabla_{\theta }\mathrm{logit}_{X}\right]\] \[=-\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\nabla_{\theta} \mathrm{logit}_{X}\right].\]

Now we compute the individual gradients with respect to \(e,w,a\) and \(b\). Recall that

\[\mathrm{logit}_{X}=e^{2}\left[\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right) \left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b.\]

Thus,

\[\nabla_{e}\mathrm{logit}_{X} =2e\left[\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(1+2 w|w|\right)+w|w(1+ae^{2})|\right]\] \[+e^{2}\left[2ae\left(X-\frac{1}{2}\right)\left(1+2w|w|\right)+w \mathrm{sign}\left(w\left(1+ae^{2}\right)\right)\left(2ae\right)\right]\] \[=2e\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(1+2w|w| \right)+2ew|w(1+ae^{2})|\] \[+2e^{3}a\left(X-\frac{1}{2}\right)\left(1+2w|w|\right)+2e^{3}aw \mathrm{sign}\left(w\left(1+ae^{2}\right)\right)\] \[=2e\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(1+2w|w| \right)+2e^{3}a\left(X-\frac{1}{2}\right)\left(1+2w|w|\right)\] \[+2ew|w(1+ae^{2})|+2e^{3}aw\mathrm{sign}\left(w\left(1+ae^{2} \right)\right).\]Substituting the above equation in Eq. (80), we obtain

\[\nabla_{e}L =-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2} \right)\right]\right)\cdot 2e\left(1+ae^{2}\right)\left(1+2w|w|\right)\] \[-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1} {2}\right)\right]\right)\cdot 2e^{3}a\left(1+2w|w|\right)\] \[-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\right]\right) \cdot 2ew|w(1+ae^{2})|\] \[-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\right]\right) \cdot 2e^{3}aw\,\mathrm{sign}\left(w\left(1+ae^{2}\right)\right).\]

Now we compute the derivative with respect to \(w\).

\[\mathrm{logit}_{X} =e^{2}\left[\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right)\left( 1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\] \[\Rightarrow\nabla_{w}\mathrm{logit}_{X} =2e^{2}\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(|w|+ \mathrm{sign}\left(w\right)w\right)\] \[+e^{2}\left[|w(1+ae^{2})|+w\left(1+ae^{2}\right)\mathrm{sign} \left(w\left(1+ae^{2}\right)\right)\right]\] \[\Rightarrow\nabla_{w}L =-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1 }{2}\right)\right]\right)2e^{2}\left(1+ae^{2}\right)\left(|w|+\mathrm{sign} \left(w\right)w\right)\] \[-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\right]\right)e^ {2}\left[|w(1+ae^{2})|+w\left(1+ae^{2}\right)\mathrm{sign}\left(w\left(1+ae^{ 2}\right)\right)\right].\]

Similarly, for \(a\):

\[\mathrm{logit}_{X} =e^{2}\left[\left(X-\frac{1}{2}\right)\left(1+2w|w|\right)+w|w(1+ ae^{2})|\right]+b\] \[\Rightarrow\nabla_{a}\mathrm{logit}_{X} =e^{4}\left(X-\frac{1}{2}\right)\left(1+ae^{2}\right)\left(1+2w|w|\right)\] \[+e^{4}w^{2}\mathrm{sign}\left(w(1+ae^{2})\right)\] \[\Rightarrow\nabla_{a}L =-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1 }{2}\right)\right]\right)e^{4}\left(1+2w|w|\right)\] \[-\left(\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\right]\right)e^ {4}w^{2}\mathrm{sign}\left(w(1+ae^{2})\right).\]

Finally, since \(\nabla_{b}\mathrm{logit}_{X}=1\), it follows from Eq. (80) that

\[\nabla_{b}L=-\mathbb{E}\left[f_{1}X+f_{2}\right].\] (81)

For the optimal \(b_{\star}\), we have \(\nabla_{b}L=0\) and hence \(\mathbb{E}\left[f_{1}X+f_{2}\right]=0\), simplifying the expressions for the gradients of \(e,w\), and \(a\) above. In fact, there exists a closed form expression for \(b_{\star}\) in terms of \(e,w\), and \(a\). Recall from Eq. (80) that

\[-f_{1} =\sigma(z_{1})-\sigma(z_{2})+p+q-1,\] \[-f_{2} =\sigma(z_{2})-p,\] (82) \[z_{1} \triangleq e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)\left(1+2w|w |\right)+w|w(1+ae^{2})|\right]+b,\] \[z_{2} \triangleq e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right)\left(1+2w|w |\right)+w|w(1+ae^{2})|\right]+b.\]

Substituting Eq. (82) in Eq. (81) and setting the gradient to zero there, we have

\[-\mathbb{E}_{X}\left[f_{1}X+f_{2}\right] =\left(\sigma(z_{1})-\sigma(z_{2})+p+q-1\right)\mathbb{E}[X]+ \sigma(z_{2})-p\] \[=\left(\sigma(z_{1})-\sigma(z_{2})+p+q-1\right)\pi_{1}+\sigma(z_ {2})-p=0.\]Simplifying,

\[(\sigma(z_{1})-\sigma(z_{2})+p+q-1)\pi_{1} =p-\sigma(z_{2})\] \[\Rightarrow(\sigma(z_{1})-1)\,\pi_{1}-\sigma(z_{2})\pi_{1}+p =p-\sigma(z_{2})\] \[\Rightarrow(\sigma(z_{1})-1)\,\pi_{1} =\sigma(z_{2})(\pi_{1}-1)\] \[\Rightarrow\frac{\sigma(z_{2})}{1-\sigma(z_{1})} =\frac{\pi_{1}}{1-\pi_{1}}=\frac{p}{q},\]

Using the definition of the sigmoid function and rearranging,

\[\begin{split}\frac{1+\exp(z_{1})}{1+\exp(-z_{2})}& =\frac{p}{q}\\ \Rightarrow\exp(z_{1})+1&=\frac{p}{q}\left(1+\exp( -z_{2})\right)\\ \Rightarrow\exp(2z_{1})+\exp(z_{1})&=\frac{p}{q} \exp(z_{1})+\frac{p}{q}\exp(z_{1}-z_{2})\\ \Rightarrow(\exp(z_{1}))^{2}+\exp(z_{1})(1-\frac{p}{q})-\frac{p }{q}\cdot\exp(z_{1}-z_{2})&=0.\end{split}\] (83)

By definitions of \(z_{1}\) and \(z_{2}\) in Eq. (82), we have \(z_{1}-z_{2}=e^{2}(1+ae^{2})(1+2w|w|)\) and thus \(A\triangleq\exp(z_{1}-z_{2})=\exp(e^{2}(1+ae^{2})(1+2w|w|))\). Thus the quadratic equation in Eq. (83) simplifies to

\[(\exp(z_{1}))^{2}+\exp(z_{1})(1-\frac{p}{q})-\frac{p}{q}\cdot A=0.\]

On solving the quadratic equation for \(\exp(z_{1})\):

\[\begin{split}\exp(z_{1})&=\frac{1}{2}\left[\frac{p }{q}-1+\sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q}\cdot A}\right] \\ \Rightarrow z_{1}&=\log\left(\frac{1}{2}\left[\frac{p }{q}-1+\sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q}\cdot A}\right] \right)\\ \Rightarrow b_{\star}&=\log\left(\frac{1}{2}\left[ \frac{p}{q}-1+\sqrt{\left(\frac{p}{q}-1\right)^{2}+4\cdot\frac{p}{q}\cdot A} \right]\right)\\ &\qquad\quad-e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)(1+2w|w |)+w|w(1+ae^{2})|\right].\end{split}\]

Note that when \(a=0\) above, we recover the expression for the optimal bias in Lemma 5. This concludes the proof.

### Proofs of Thm. 9 and Thm. 10

We prove Thm. 9 and Thm. 10 below. Note that Thm. 10 directly follows from the former by removing the bias \(b\), since for any critical point \(\bm{\gamma}=(e,w,b,a)\in\mathbb{R}^{4}\) with \(\nabla L(\bm{\gamma})=0\), the bias \(b\) is already the optimal one corresponding to \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\), i.e. \(b=b_{\star}(\bm{\theta})=\operatorname*{argmin}_{b\in\mathbb{R}}L(\bm{\theta},b)\). This is similar to the proof of Thm. 1, which follows from Thm. 7.

Now we prove Thm. 9.

Proof.: We characterize the set of global minima first.

**Set of all global minima.** Let \(\boldsymbol{\gamma}_{\star}\in\mathbb{R}^{4}\) be arbitrary. From [23, Lemma 1], we have that \(\boldsymbol{\gamma}_{\star}\) is a global minimum for the loss \(L(\cdot)\) in Eq. (23) if and only if its prediction probability satisfies \(f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})=\mathbb{P}\left(x_{n+1}=1\mid x_{n}\right)\), the Markov kernel. Since the input \(\{x_{n}\}_{n=1}^{N}\sim(\boldsymbol{\pi}(p,q),\boldsymbol{P}(p,q))\), we have that

\[\mathbb{P}\left(x_{n+1}=1\mid x_{n}\right)=(1-x_{n})p+x_{n}(1-q)=(1-p-q)x_{n}+p.\] (84)

On the other hand, by definition, from Eq. (3), \(f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})=\sigma\left(e^{2}\left[\left(x_{n}- \frac{1}{2}\right)\left(1+ae^{2}\right)\left(1+2w|w|\right)+w|w(1+ae^{2})| \right]+b\right)\), where \(\boldsymbol{\gamma}_{\star}=(e,w,b,a)\). Since \(x_{n}\in\{0,1\}\), this can be further simplified to

\[f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n}) =\sigma\left(e^{2}\left[\left(x_{n}-\frac{1}{2}\right)\left(1+ae ^{2}\right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] (85) \[=x_{n}\sigma\left(e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right) \left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] \[\quad+(1-x_{n})\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^{2} \right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] \[=x_{n}\sigma\left(e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right) \left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] \[\quad-x_{n}\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^{2} \right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] \[\quad+\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right) \left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right).\]

Since both \(f_{\boldsymbol{\gamma}_{\star}}(x_{1}^{n})\) and \(\mathbb{P}\left(x_{n+1}=1\mid x_{n}\right)\) are linear functions of \(x_{n}\), equating them for all values of \(x_{n}\in\{0,1\}\) implies that the respective coeffceients in these functions in Eq. (84) and Eq. (85) are also equal, i.e.

\[1-p-q =\sigma\left(e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)\left(1+2 w|w|\right)+w|w(1+ae^{2})|\right]+b\right)\] (86) \[\quad-\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right) \left(1+2w|w|\right)+w|w(1+ae^{2})|\right]+b\right),\] \[p =\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right)\left(1+ 2w|w|\right)+w|w(1+ae^{2})|\right]+b\right),\]

and hence

\[\sigma\left(e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)\left(1+2 w|w|\right)+w|w(1+ae^{2})|\right]+b\right) =1-q,\] (87) \[\sigma\left(e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right)\left(1+2 w|w|\right)+w|w(1+ae^{2})|\right]+b\right) =p.\]

Since \(\sigma(z)=y\) for \(y\in(0,1)\) implies \(z=\log\frac{y}{1-y}\), Eq. (87) can be rewritten as

\[e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)\left(1+2w|w|\right)+ w|w(1+ae^{2})|\right]+b =\log\frac{1-q}{q},\] \[e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right)\left(1+2w|w|\right) +w|w(1+ae^{2})|\right]+b =\log\frac{p}{1-p}.\]Adding and subtracting the above two equations, we obtain

\[\begin{split} e^{2}w|w(1+ae^{2})|+b&=\frac{1}{2}\log \frac{p(1-q)}{q(1-p)},\\ e^{2}\left(1+ae^{2}\right)(1+2w|w|)&=\log\frac{(1-q )(1-p)}{pq}.\end{split}\] (88)

Thus \(\boldsymbol{\gamma}_{\star}\in\mathbb{R}^{4}\) is a global minimum for \(L(\cdot)\) if and only if it satisfies Eq. (88). It's easy to see that \(\boldsymbol{\gamma}_{\star}\) is already a critical point for \(L\) as Eq. (86) is equivalent to \(f_{1}=f_{2}=0\) in Lemma 13. Thus, the set of all global minimum \(\boldsymbol{\Gamma}_{\star}(p,q)\) is given by

\[\begin{split}\boldsymbol{\Gamma}_{\star}(p,q)\triangleq\{ \boldsymbol{\gamma}_{\star}=(e,w,b,a)\in\mathbb{R}^{4}:& e^{2}w|w(1+ae^{2})|+b=\frac{1}{2}\log\frac{p(1-q)}{q(1-p)}, \\ e^{2}\left(1+ae^{2}\right)(1+2w|w|)&=\log\frac{(1- q)(1-p)}{pq}\}.\end{split}\]

Since the prediction \(f_{\boldsymbol{\gamma}_{\star}}(\cdot)\) equals the Markov kernel for any \(\boldsymbol{\gamma}_{\star}\in\boldsymbol{\Gamma}_{\star}\), it follows from Thm. 4 (or [23, Lemma 1]) that \(L(\boldsymbol{\gamma}_{\star})=H(x_{n+1}\mid x_{n})\), the entropy rate of the Markov chain. Now we characterize the remaining set of stationary points.

**Non-global-min critical points.** For any critical point \(\boldsymbol{\gamma}=(e,w,a,b)\in\mathbb{R}^{4}\), we have from the gradient expressions in Lemma 13 that (denoting \(-f_{1}\) and \(-f_{2}\) from the lemma as \(f_{1}\) and \(f_{2}\)) respectively)

\[\begin{split}\frac{\partial L}{\partial b}&=\mathbb{ E}_{X}\left[f_{1}X+f_{2}\right]=0,\\ \frac{\partial L}{\partial e}&=\mathbb{E}_{X}\left[ \left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2}\right)\right]2e\left(1+ae^{2} \right)(1+2w|w|)\\ &\qquad+\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X- \frac{1}{2}\right)\right]2e^{3}a\left(1+2w|w|\right)=0,\\ \frac{\partial L}{\partial w}&=\mathbb{E}_{X}\left[ \left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2}\right)\right]2e^{2}\left(1+ae^{2} \right)(|w|+\operatorname{sign}\left(w\right)w)=0,\\ \frac{\partial L}{\partial a}&=\mathbb{E}_{X}\left[ \left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2}\right)\right]e^{4}\left(1+2w|w| \right)=0.\end{split}\] (89)

From Eq. (89), we have that \(\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=0\). If \(\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2}\right) \right]=\mathbb{E}[(f_{1}X+f_{2})X]=0\), we have that \(f_{1}=f_{2}=0\), implying \(\boldsymbol{\gamma}\) is a global minimum. Hence without loss of generality, assume that \(\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2}\right) \right]\neq 0\). Then we can partition the above set of equations into the following regions of stationarity:

1. \(\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=0,e=0,\)
2. \(\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=0,e\neq 0,1+ae^{2}=0,1+2w|w|=0\).

Slightly changing the variable order, for any \(\boldsymbol{\gamma}=(b,e,w,a)\in\mathbb{R}^{4}\), we define

\[\boldsymbol{H}(\boldsymbol{\gamma})\triangleq\nabla^{2}L(\boldsymbol{\gamma})= \begin{bmatrix}\frac{\partial^{2}L}{\partial b^{2}}&\frac{\partial^{2}L}{ \partial b\partial e}&\frac{\partial^{2}L}{\partial b\partial w}&\frac{ \partial^{2}L}{\partial b\partial a}\\ \frac{\partial^{2}L}{\partial e\partial b}&\frac{\partial^{2}L}{\partial e^{2} }&\frac{\partial^{2}L}{\partial e\partial w}&\frac{\partial^{2}L}{ \partial e\partial a}\\ \frac{\partial^{2}L}{\partial w\partial b}&\frac{\partial^{2}L}{\partial w \partial e}&\frac{\partial^{2}L}{\partial w^{2}}&\frac{\partial^{2}L}{ \partial w\partial a}\\ \frac{\partial^{2}L}{\partial a\partial b}&\frac{\partial^{2}L}{\partial a \partial e}&\frac{\partial^{2}L}{\partial a\partial w}&\frac{\partial^{2}L}{ \partial a^{2}}\end{bmatrix}\in\mathbb{R}^{4\times 4}.\] (90)Recall that

\[f_{1} =\sigma(z_{1})-\sigma(z_{2})+p+q-1,\] \[f_{2} =\sigma(z_{2})-p,\] \[z_{1} \triangleq e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)(1+2w|w|)+w|w(1 +ae^{2})|\right]+b,\] \[z_{2} \triangleq e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right)(1+2w|w|)+w|w(1 +ae^{2})|\right]+b.\]

From Eq. (89), we see that the second derivaties of \(L\) depend on the first-derivatives of \(f_{1}\) and \(f_{2}\), which we now compute. Recall that the derivative of the sigmoid function obeys \(\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))=\sigma(z)\sigma(-z)\) for any \(z\in\mathbb{R}\). Now the gradients of \(f_{1}\) and \(f_{2}\) with respect to \(b,e,w\) and \(a\) are

\[\frac{\partial f_{1}}{\partial b} =\sigma(z_{1})\sigma(-z_{1})-\sigma(z_{2})\sigma(-z_{2}),\] \[\frac{\partial f_{2}}{\partial b} =\sigma(z_{2})\sigma(-z_{2}),\] \[\frac{\partial f_{1}}{\partial e} =\sigma(z_{1})\sigma(-z_{1})\left\{2e\left[\frac{1}{2}\left(1+ ae^{2}\right)(1+2w|w|)+w|w(1+ae^{2})|\right]\right\}\] \[\quad+\sigma(z_{1})\sigma(-z_{1})\left\{2ae^{3}\left[\frac{1}{2} \left(1+2w|w|\right)+w|w|\text{sign}(1+2w|w|)\right]\right\}\] \[\quad-\sigma(z_{2})\sigma(-z_{2})\left\{2e\left[-\frac{1}{2} \left(1+ae^{2}\right)(1+2w|w|)+w|w(1+ae^{2})|\right]\right\}\] \[\quad-\sigma(z_{2})\sigma(-z_{2})\left\{2ae^{3}\left[-\frac{1}{2} \left(1+2w|w|\right)+w|w|\text{sign}(1+2w|w|)\right]\right\}\] (91) \[\frac{\partial f_{2}}{\partial e} =\sigma(z_{2})\sigma(-z_{2})\left\{2e^{3}\left[-\frac{1}{2} \left(1+ae^{2}\right)|\allowbreak(1+2w|\allowbreak(1+ae^{2})|\allowbreak \right]\right\}\] \[\quad+\sigma(z_{2})\sigma(-z_{2})\left\{2ae^{3}\left[-\frac{1}{2} \left(1+2w|w|\right)+w|w|\text{sign}(1+2w|)\right]\right\}\] \[\frac{\partial f_{1}}{\partial w} =\sigma(z_{1})\sigma(-z_{1})\left\{2e^{2}\left[\frac{1}{2}\left(1 +ae^{2}\right)|\allowbreak w|+|w|\allowbreak(1+ae^{2})\right]\right\}\] \[\quad-\sigma(z_{2})\sigma(-z_{2})\left\{2e^{2}\left[-\frac{1}{2} \left(1+ae^{2}\right)|\allowbreak w|+|\allowbreak w|\allowbreak(1+ae^{2}) \right]\right\},\] \[\frac{\partial f_{2}}{\partial w} =\left\{2e^{2}\left[-\frac{1}{2}\left(1+ae^{2}\right)|\allowbreak w |+|\allowbreak w|\allowbreak(1+ae^{2})\right]\right\},\] \[\frac{\partial f_{1}}{\partial a} =\sigma(z_{1})\sigma(-z_{1})\left\{e^{4}\left[\frac{1}{2}\left(1+ 2w|w|\right)+w|w|\text{sign}\left(1+ae^{2}\right)\right]\right\}\] \[\quad-\sigma(z_{2})\sigma(-z_{2})\left\{e^{4}\left[-\frac{1}{2} \left(1+2w|w|\right)+w|w|\text{sign}\left(1+ae^{2}\right)\right]\right\},\] \[\frac{\partial f_{2}}{\partial a} =\sigma(z_{2})\sigma(-z_{2})\left\{e^{4}\left[-\frac{1}{2}\left(1+ 2w|w|\right)+w|w|\text{sign}\left(1+ae^{2}\right)\right]\right\}.\]

Now we characterize the first set of critical points.

**(i) Stationary points with \(\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=0,e=0\)**. When \(e=0\), we have that \(z_{1}=z_{2}=b\). Hence,

\[f_{1} =\sigma(b)+p+q-1-\sigma(b)=p+q-1,\] \[f_{2} =\sigma(b)-p.\]

[MISSING_PAGE_FAIL:55]

\[\frac{\partial^{2}L}{\partial e\partial w}\bigg{|}_{\bm{\gamma}} =\frac{\partial}{\partial e}\mathbb{E}_{X}\left[\left(f_{1}X+f_{2} \right)\left(X-\frac{1}{2}\right)\right]4e^{2}\left(1+ae^{2}\right)|w|\bigg{|}_ {\bm{\gamma}}\] (94) \[=\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2 }\right)\right]2e^{4}|w|\bigg{|}_{\bm{\gamma}}\] \[\quad+\mathbb{E}_{X}\left[\left(\frac{\partial f_{1}}{\partial w }\bigg{|}_{\bm{\gamma}}X+\frac{\partial f_{2}}{\partial w}\right)\bigg{|}_{\bm {\gamma}}\right]e^{4}\left(1+2w|w|\right)\] \[=0,\] \[\frac{\partial^{2}L}{\partial a^{2}}\bigg{|}_{\bm{\gamma}} =\frac{\partial}{\partial a}\mathbb{E}_{X}\left[\left(f_{1}X+f_{2 }\right)\left(X-\frac{1}{2}\right)\right]e^{4}\left(1+2w|w|\right)\bigg{|}_{ \bm{\gamma}}\] \[=\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2 }\right)\right]2e^{4}|w|\bigg{|}_{\bm{\gamma}}\] \[\quad+\mathbb{E}_{X}\left[\left(\frac{\partial f_{1}}{\partial e }\bigg{|}_{\bm{\gamma}}X+\frac{\partial f_{2}}{\partial e}\bigg{|}_{\bm{\gamma }}\right)\left(X-\frac{1}{2}\right)\right]e^{4}\left(1+2w|w|\right)\] \[=0,\] \[\frac{\partial^{2}L}{\partial a^{2}}\bigg{|}_{\bm{\gamma}} =\frac{\partial}{\partial a}\mathbb{E}_{X}\left[\left(f_{1}X+f_{2 }\right)\left(X-\frac{1}{2}\right)\right]e^{4}\left(1+2w|w|\right)\bigg{|}_{ \bm{\gamma}}\] \[=0,\] \[=\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2 }\right)\right]2e^{4}|w|\bigg{|}_{\bm{\gamma}}\] \[=0,\] \[=0,\] \[\frac{\partial^{2}L}{\partial a^{2}}\bigg{|}_{\bm{\gamma}} =\frac{\partial}{\partial a}\mathbb{E}_{X}\left[\left(f_{1}X+f_{2 }\right)\left(X-\frac{1}{2}\right)\right]e^{4}\left(1+2w|w|\right)\bigg{|}_{ \bm{\gamma}}\] \[=\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2 }\right)\right]2e^{4}|w|\bigg{|}_{\bm{\gamma}}\] \[\quad+\mathbb{E}_{X}\left[\left(\frac{\partial f_{1}}{\partial w }\bigg{|}_{\bm{\gamma}}X+\frac{\partial f_{2}}{\partial w}\right)\bigg{|}_{\bm {\gamma}}\right]e^{4}\left(1+2w|w|\right)\] \[=0,\] \[\frac{\partial^{2}L}{\partial a^{2}}\bigg{|}_{\bm{\gamma}} =\frac{\partial}{\partial a}\mathbb{E}_{X}\left[\left(f_{1}X+f_{2 }\right)\left(X-\frac{1}{2}\right)\right]e^{4}\left(1+2w|w|\right)\bigg{|}_{ \bm{\gamma}}\] \[=\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2 }\right)\right]2e^{4}|w|\bigg{|}_{\bm{\gamma}}\] \[\quad+\mathbb{E}_{X}\left[\left(\frac{\partial f_{1}}{\partial w }\bigg{|}_{\bm{\gamma}}X+\frac{\partial f_{2}}{\partial w}\right)\bigg{|}_{ \bm{\gamma}}\right]e^{4}\left(1+2w|w|\right)\] \[=0.\]

Congregating all the second derivatives from Eq. (93) and Eq. (94) into the Hessian \(\bm{H}(\bm{\gamma})\) in Eq. (90), we finally obtain

\[\bm{H}(\bm{\gamma})=\pi_{0}\pi_{1}\begin{bmatrix}1&0&0&0\\ 0&2(p+q-1)(1+2w|w|)&0&0\\ 0&0&0&0\\ 0&0&0&0\end{bmatrix},\]which is identical to the Hessian obtained in the proof of Thm. 7 (App. J.2) for \(e=0\). Thus it follows that \(\bm{\Gamma}_{\min}(p,q)\subseteq\mathbb{R}^{4}\) and \(\bm{\Gamma}_{\mathrm{sad}}\subseteq\mathbb{R}^{4}\) defined below are a set of local minima and saddle points respectively:

\[\bm{\Gamma}_{\min}(p,q)\triangleq\left\{\bm{\gamma}_{\min}=(e,w,b,a )\in\mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)>0,b=\log\frac{p}{q}\right\},\] \[\bm{\Gamma}_{\mathrm{sad}}(p,q)\triangleq\left\{\bm{\gamma}_{ \mathrm{sad}}=(e,w,b,a)\in\mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)\leq 0,b=\log \frac{p}{q}\right\}.\]

Now we focus on the remaining set of critical points.

**(ii) Stationary points with \(\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=0,e\neq 0,1+ae^{2}=0,1+2w|w|=0\).** For this set of points, the Hessian remains undefined because \(\frac{\partial f_{1}}{\partial e},\frac{\partial f_{2}}{\partial e},\frac{ \partial f_{1}}{\partial a},\frac{\partial f_{2}}{\partial a}\) do not exist (Eq. (91)). This non-existence arises since \(\mathrm{sign}\left(1+ae^{2}\right)\) is undefined \(1+ae^{2}=0\). However, even in this scenario,when \(e\neq 0,1+ae^{2}=0,1+2w|w|=0\), we have \(z_{1}=z_{2}=b\). Hence,

\[f_{1}=\sigma(b)+p+q-1-\sigma(b)=p+q-1,\] \[f_{2}=\sigma(b)-p.\]

Thus the expectation term \(\mathbb{E}_{X}\left[f_{1}X+f_{2}\right]=\left(p+q-1\right)\mathbb{E}_{X} \left[X\right]+\sigma(b)-p=\left(p+q-1\right)\pi_{1}+\sigma(b)-p=0\). Simplifying, \(\sigma(b)=\frac{p}{p+q}\), which implies \(b=\log\frac{p}{q}\).

We could attempt to understand the characterization of the points on this manifold through local perturbation analysis. However, in this work, we classify them as stationary points and leave the comprehensive characterization for future research. This set of points \(\bm{\Gamma}_{\mathrm{station}}(p,q)\subseteq\mathbb{R}^{4}\) is defined as

\[\bm{\Gamma}_{\mathrm{station}}(p,q)\triangleq\left\{\bm{\gamma}_{\min}=(e,w,b, a)\in\mathbb{R}^{4}:e\neq 0,1+ae^{2}=0,1+2w|w|=0,b=\log\frac{p}{q}\right\}.\]

This concludes the proof.

### Proof of Lemma 14

Proof.: Recall from Lemma 13 that for \(\bm{\theta}=(e,w,a)\in\mathbb{R}^{3}\), we have

\[\frac{\partial L}{\partial e}=-\mathbb{E}\left[\left(f_{1}X+f_{2} \right)\left(X-\frac{1}{2}\right)\right]\cdot 2e\left(1+ae^{2}\right)\left(1+2w|w|\right)\] \[\qquad-\mathbb{E}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{ 2}\right)\right]\cdot 2e^{3}a\left(1+2w|w|\right),\] \[\frac{\partial L}{\partial w}=-\mathbb{E}\left[\left(f_{1}X+f_{2} \right)\left(X-\frac{1}{2}\right)\right]\cdot 2e^{2}\left(1+ae^{2}\right)\left(|w|+ \mathrm{sign}\left(w\right)w\right),\] \[\frac{\partial L}{\partial a}=-\mathbb{E}\left[\left(f_{1}X+f_{2} \right)\left(X-\frac{1}{2}\right)\right]\cdot e^{4}\left(1+2w|w|\right),\]

where \(X\in\{0,1\}\) is a Bernoulli random variable with \(X\sim\mathrm{Bern}(p/(p+q))\), and

\[f_{1}\triangleq 1-p-q-\phi_{1}+\phi_{0},\quad f_{2}\triangleq p-\phi_{0},\] \[\phi_{1}\triangleq\sigma\left(e^{2}\left(\frac{1}{2}\left(1+ae^ {2}\right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right)+b_{\star}\right),\] \[\phi_{0}\triangleq\sigma\left(e^{2}\left(\frac{-1}{2}\left(1+ae^ {2}\right)\left(1+2w|w|\right)+w|w(1+ae^{2})|\right)+b_{\star}\right),\]

[MISSING_PAGE_FAIL:58]

### Proofs of Lemma 16, Lemma 15, and Thm. 3

Proof.: We note that the proofs of Lemma 16, Lemma 15 directly follow from that of their counterparts Lemma 11 and Lemma 10 using Lojasiewicz's theorem to characterize the convergence of the gradient flow. Thm. 3 is a direct consequence of Lemma 16, Lemma 15. 

### Informal proof of Thm. 11

Proof.: [Informal] For \(\bm{\theta}=(e,w,a)\), recall from Lemma 13 that the derivative of the loss \(L\) with respect to \(e\) is

\[\frac{\partial L}{\partial e} =\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{1}{2 }\right)\right]2e\left(1+ae^{2}\right)\left(1+2w|w|\right)\] \[\quad+\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)\left(X-\frac{ 1}{2}\right)\right]2e^{3}a\left(1+2w|w|\right),\]

where

\[f_{1} =\sigma(z_{1})-\sigma(z_{2})+p+q-1,\] \[f_{2} =\sigma(z_{2})-p,\] \[z_{1} \triangleq e^{2}\left[\frac{1}{2}\left(1+ae^{2}\right)\left(1+2w |w|\right)+w|w(1+ae^{2})|\right]+b,\] \[z_{2} \triangleq e^{2}\left[\frac{-1}{2}\left(1+ae^{2}\right)\left(1+2 w|w|\right)+w|w(1+ae^{2})|\right]+b.\]

Assuming the initialization is very small, making any product of quantities in \(\bm{\theta}=(e,w,a,b)\) much smaller than the individual quantities. Therefore, we can consider these products to be approximately zero. That is, \(\forall x,y\in\bm{\theta},x\geq xy\ \ \&\ y\geq xy\ \ \&\ xy\approx 0\). Hence,

\[z_{1} =b,\] \[z_{2} =b,\] \[f_{1} =p+q-1,\] \[f_{2} =\sigma(b)-p.\]

Hence the gradient with respect to \(e\) is

\[\frac{\partial L}{\partial e}=2\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right) \left(X\right)\right]e.\] (102)

Simplifying the expectation term, \(\mathbb{E}_{X}\left[\left(f_{1}X+f_{2}\right)X\right]=(f_{1}+f_{2})\pi_{1}=f_{ 1}\pi_{1}-f_{1}\pi^{2}=(p+q-1)(\pi_{1}-\pi_{1}^{2})\), where we used the fact that \(b\) is optimal in the above equations, specifically where \(f_{1}\pi_{1}+f_{2}=0\). Thus the gradient flow for the parameter \(e\) is governed by

\[\dot{e}=-\frac{\partial L}{\partial e}=-(p+q-1)(\pi_{1}-\pi_{1}^{2})e \Rightarrow e=e_{0}\exp(-(p+q-1)(\pi_{1}-\pi_{1}^{2})t).\]

Since \((p+q-1)(\pi_{1}-\pi_{1}^{2})>0\), \(e\to 0\), which denotes it converges to the local minima.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims are in the form of theorems that are proved in Sections 3 and 4 and experimental results in Sec. 5.2, for which our code is made available for reproducibility. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: Though we do not have an explicit section for limitations, we outline the shortcomings in our approach in the conclusion and list them as future direction. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All theorems are stated with required assumptions and proofs are provided either in the appendix or the main paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all our code as open access and describe the experimental setup in App. 1.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to the code, available at https://anonymous.4open.science/r/Local-to-Global-C7OB/. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experiment details are in App. I. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experiments are repeated 5 times. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: These are unrelated to the results of the paper. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All ethical practices were followed. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: No direct societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No risk.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use code from [26] which is cited. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No assets are released, but code is made publicly available. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: No human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.