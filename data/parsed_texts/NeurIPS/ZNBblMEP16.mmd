# Depth-discriminative Metric Learning

for Monocular 3D Object Detection

 Wonhyeok Choi   Mingyu Shin   Sunghoon Im

DGIST, Daegu, Korea

{smu06117, alsrb4446, sunghoonim}@dgist.ac.kr

Equal ContributionCorresponding Author

###### Abstract

Monocular 3D object detection poses a significant challenge due to the lack of depth information in RGB images. Many existing methods strive to enhance the object depth estimation performance by allocating additional parameters for object depth estimation, utilizing extra modules or data. In contrast, we introduce a novel metric learning scheme that encourages the model to extract depth-discriminative features regardless of the visual attributes without increasing inference time and model size. Our method employs the distance-preserving function to organize the feature space manifold in relation to ground-truth object depth. The proposed \((K,B,\epsilon)\)-quasi-isometric loss leverages predetermined pairwise distance restriction as guidance for adjusting the distance among object descriptors without disrupting the non-linearity of the natural feature manifold. Moreover, we introduce an auxiliary head for object-wise depth estimation, which enhances depth quality while maintaining the inference time. The broad applicability of our method is demonstrated through experiments that show improvements in overall performance when integrated into various baselines. The results show that our method consistently improves the performance of various baselines by 25.27% and 4.54% on average across KITTI and Waymo, respectively.

## 1 Introduction

Monocular 3D object detections [20, 22, 23, 27] has gained prominence as a cost-effective and easily deployable solution, playing a critical role in autonomous driving and robotic navigation systems. Typically, the frameworks consist of a feature extractor and lightweight multi-head architectures that predict the projected centers, depths, bounding box sizes, and heading directions of multiple objects from a single RGB image [18, 20, 22, 23, 26, 27, 41]. Among these tasks, object depth estimation, inferring the distance from a monocular camera to the center of an object, is the most challenging sub-task due to depth ambiguity. Previous work [23] has highlighted this issue in ablation studies, revealing that substituting object depth predictions with ground-truth (GT) values significantly improves overall performance, while replacing other sub-tasks, such as heading direction and 3D size, does not notably enhance performance. Furthermore, several prior works [3, 10, 18, 22, 27, 28, 37, 41] have attempted to improve object depth estimation quality by adding additional modules or introducing new formulations. However, despite the subsequent increase in inference time and model size, the improvement in performance remains limited.

These observations suggest that monocular 3D object detection heavily relies on object depth quality; nevertheless, conventional methods yield unsatisfactory results due to the extraction of less-discriminative features for object depth inference. One reason is that the extracted feature involves visual attributes, such as the color, size, and heading direction of objects, resulting in the objectdepth head receiving feature with limited depth discernment. To improve object depth performance, the network should be capable of extracting purely depth-discriminative features that involve essential geometric information, irrespective of the visual attribute. A feasible method for extracting the depth-discriminative features involves using deep metric learning schemes such as contrastive learning [6; 7; 39] or representation learning [5; 34].

However, most existing deep metric learning schemes [6; 7; 16; 39] rely on aggressive two-view augmentation (_i.e._ Affine transform) to train the distance or similarity metric. This aggressive data augmentation method is hardly leveraged in current monocular 3D object detection frameworks due to the violation of geometric constraints, where horizontal flip and color distortion are the only two methods used in this field for a long time [19]. One alternative way is to learn a regression-aware representation by contrasting samples against each other based on their target distance using GT labels [39]. However, forcibly arranging the manifold of the feature space using this depth distance metric may negatively impact the performance of the other tasks, because the feature extractor for monocular 3D object detection inevitably produces complex shared representations across multiple sub-tasks. Note that the experiment of negative impact is conducted in Sec. 5.2.

To address these issues, we propose a \((K,B,\epsilon)\)-quasi-isometric loss, a new metric learning approach that encourages the network to extract depth-discriminative features using object depth labels. Inspired by several manifold learning schemes [29; 33], we locally preserve the neighborhood distances to maintain the natural non-linear manifold of feature space in order to mitigate the negative transfer effect on other sub-tasks. Our approach utilizes the quasi-isometric properties providing a relaxed condition for the distance metric between the depth and feature metric spaces. This enables the model to arrange the feature space with respect to the object depth labels while maintaining the performance of other sub-tasks. Moreover, we introduce an auxiliary head for object-wise depth estimation to further improve object depth estimation. The head component is removed after the training process, ensuring that the inference time remains unaffected and does not experience an increase. Experimental results indicate that the proposed method consistently outperforms state-of-the-art baselines across a variety of 3D object detection datasets. Our method has been shown to be compatible with various monocular 3D object detection frameworks [15; 20; 22; 23; 27], demonstrating its broad applicability without compromising inference time or increasing model size. The effectiveness of each proposed module is further underscored through comprehensive ablation studies.

Our contributions can be summarized as follows:

* We propose a simple yet effective metric learning scheme that preserves the geodesic distance of depth information to feature space.
* We present an auxiliary head for object-wise depth estimation, which enhances the depth quality without impacting the inference time, maintaining efficient performance.
* Our method significantly enhances the performance of various monocular 3D object detection methods without increasing inference time and model size.

## 2 Related work

Monocular 3D object detection.Monocular 3D object detection can be broadly categorized into two types. The first type [3; 18; 20; 22; 23; 25; 37; 41] predicts the localization of objects of interest around an ego vehicle using only RGB images, annotations, and camera calibrations. Most of them are based on CenterNet [43]. They are divided into several sub-tasks, with a primary focus on estimating accurate object depth. The works [22] and [41] propose the formulations of object depth estimation. MonoCon [20] leverages the abundant contexts in conventional 3D bounding box annotations to integrate various auxiliary tasks. The second category leverages additional data, such as pre-trained models [8; 10; 15; 27; 28; 35] or CAD models [24; 38], to compensate for the lack of 3D information in monocular images. Some prior works utilize pre-trained depth estimators to address the lack of depth information, either by converting the monocular setups to LiDAR/Stereo environments [8; 35] or by allocating more parameters to the depth estimation task [10; 15; 40]. Alternatively, certain methods employ completed depth maps from pre-trained depth completion models [27] or pre-trained LiDAR-based detectors [8; 14] to supervise their models. Notably, DID-M3D [27] divides object depth into visual and attribute depth using completed depth maps to address the ambiguity of the object depth.

Manifold geometry preservation.Deep learning networks, consisting of continuous and differentiable (precisely, almost everywhere differentiable) layers, ensure smooth mapping of networks \(f:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}\). This guarantees that neighborhoods in \(\mathbb{R}^{m}\) will be mapped into neighborhoods in the embedding space \(\mathbb{R}^{n}\) with some amount of "stretching" and vice versa. Manifold learning has been studied to preserve most or all of the essential information by minimizing these stretches. Many works [1; 2; 17; 29; 42] aim to create meaningful representations when embedding high-dimensional data into lower dimensions. The traditional linear embedding algorithms commonly use the matrix decomposition to preserve the original variance[1] or pairwise distance [17]. More recent algorithms [2; 29; 33; 42] construct the neighborhood graphs by preprocessing the dataset using \(\epsilon\)-ball or \(k\)-nearest neighbors for each point, embedding the data while preserving the neighbor samples' distance. In particular, Isomap [33] estimates the shortest path in the neighborhood graph between every pair of data points and then employs the Euclidean Multidimensional Scaling (MDS) algorithm [17] to embed the points in \(d\) dimensions with minimum distance distortion all at once. These non-linear embedding algorithms can account for the non-linear nature of the manifold by preserving the distance between neighborhood samples.

Metric learning.Metric learning is one of the machine learning techniques that aims to learn an effective distance metric between data points by using training data. To achieve this, methods such as [6; 7; 16] attempt to minimize the distance between samples from the same class while maximizing the distance between samples from different classes. These methods demonstrate improved performance for self-supervised learning by learning in an end-to-end fashion, including two-view augmentation or depending on class labels. These deep metric learning methods typically target classification tasks because the positive/negative pairs are defined as belonging to the same or different classes. More recent works [31; 36; 39] extensively apply metric learning in the context of regression. These works either directly use the target distance to contrast samples against each other [36; 39] or leverage representation learning in a semi-supervised learning scenario [31].

## 3 Method

Our final goal is to boost the object depth performance by extracting the depth-discriminative features without increasing inference time. Similar to common deep metric learning schemes [6; 7; 39], we propose a loss term that preserves a meaningful low-dimensional data structure in the feature space.

### Preliminary

Metric space.A metric space is a mathematical concept that characterizes a set of points and a function that measures the distance between any two points in the set. Formally, a metric space can be defined as a pair \((M,d)\), where \(M\) represents a set and \(d\) is a distance function on \(M\). The distance function \(d\) must satisfies the following axioms [13] for any three points \(x,y,z\in M\):

1. Non-negativity: \(d(x,y)\geq 0\) and \(d(x,y)=0\) if and only if \(x=y\).
2. Symmetry: \(d(x,y)=d(y,x)\) for all \(x,y\in M\).
3. Triangle inequality: \(d(x,y)\leq d(x,z)+d(z,y)\) for all \(x,y,z\in M\).

Quasi-isometry.A quasi-isometry is a function between two metric spaces that preserves distances up to a constant factor, even though it may locally distort angles and distances. Let \(\mathcal{Q}\) be a function from one metric space \((M_{1},d_{1})\) to another metric space \((M_{2},d_{2})\). \(\mathcal{Q}\) is considered a quasi-isometry from \((M_{1},d_{1})\) to \((M_{2},d_{2})\) if there exist constants \(K\geq 1\), \(B\geq 0\), and \(\epsilon\geq 0\) such that both of the following properties hold:

1. \(\forall x_{1},x_{2}\in M_{1}\ :\ \frac{1}{K}\cdot d_{1}(x_{1},x_{2})-B\leq d_{2 }(\mathcal{Q}(x_{1}),\mathcal{Q}(x_{2}))\leq K\cdot d_{1}(x_{1},x_{2})+B\).
2. \(\forall z\in M_{2}\ :\ \exists x\in M_{1}\ s.t.\ d_{2}(z,\mathcal{Q}(x))\leq\epsilon\).

Quasi-isometry does not necessarily require continuity [4]. This property is advantageous because other distance-preserving transformations may not possess it, and most datasets are finite. Therefore, we use these conditions as constraints to ensure that the feature space retains the geometrical information of the depth metric space.

### Problem Definition

The task of monocular 3D object detection aims to predict both the object class \(c\) and the 3D bounding box \(\mathbf{b}\) for multiple objects within an image \(\mathbf{I}\). Recently, CenterNet [43] has become the best practice for monocular 3D object detection, with subsequent works [18; 20; 22; 23; 27; 41] adopting its pipeline. These methods decompose the bounding box \(\mathbf{b}=[X,Y,Z,h,w,l,\gamma]\) estimation problem into separate estimations of the coarse projected 3D center \((u,v)\), the depth of object center \(z\), the center offset \((\delta u,\delta v)\), the 3D size dimensions \((h,w,l)\), and heading direction \(\gamma\) (_i.e._, the yaw angle). The 3D object center \([X,Y,Z]\) is computed by back-projecting the projected center with the corresponding depth, given the intrinsic matrix of the camera \(\mathbf{K}\), as follows:

\[\begin{split} u_{c}&=u+\delta u,\;v_{c}=v+\delta v,\\ [X,Y,Z]^{T}&=\mathbf{K}^{-1}[u_{c}\cdot z,v_{c}\cdot z,z]^{T}.\end{split}\] (1)

The networks consist of feature extractor \(\mathcal{F}_{\theta}\) and task-specific heads \(\mathcal{G}_{\phi^{t}}\) that produce the feature maps \(\mathbf{h}\) and the per-pixel output maps \(\tilde{\mathbf{o}}_{t}\), where \(t\in\mathbf{T}=\{t_{c},t_{u},t_{v},t_{\delta u},t_{\delta v},t_{z},t_{h},t_{w},t_{l},t_{\gamma}\}\), respectively, as follows:

\[\begin{split}\mathbf{h}=\mathcal{F}_{\theta}(\mathbf{I}),\; \mathcal{F}_{\theta}:\mathbb{R}^{3\times H\times W}\rightarrow\mathbb{R}^{C \times H^{\prime}\times W^{\prime}},\\ \tilde{\mathbf{o}}_{t}=\mathcal{G}_{\phi^{t}}(\mathbf{h}),\; \mathcal{G}_{\phi^{t}}:\mathbb{R}^{C\times H^{\prime}\times W^{\prime}} \rightarrow\mathbb{R}^{H^{\prime}\times W^{\prime}},\end{split}\] (2)

where \(H,W\) represent the spatial resolution of the image, and \(C,H^{\prime},W^{\prime}\) denote the channel and spatial resolution of the feature maps, which is downsampled from the image resolution. The final object-wise results \(\mathbf{o_{T}}\) are extracted from the non-learnable function \(\mathcal{H}\), given the intermediate per-pixel outputs \(\tilde{\mathbf{o}}_{\mathbf{T}=\{t_{c},t_{u},t_{v},t_{\delta u},t_{\delta v},t_ {z},t_{h},t_{w},t_{l},t_{\gamma}\}}\), as follows:

\[\mathbf{o_{T}}=\mathcal{H}(\tilde{\mathbf{o}}_{\mathbf{T}}),\; \mathcal{H}:\mathbb{R}^{|\mathbf{T}|\times H^{\prime}\times W^{\prime}} \rightarrow\mathbb{R}^{|\mathbf{T}|\times N},\;N\geq 0,\] (3)

where the number of detected objects \(N\) is not fixed and can be zero if no object is detected.

### Methodology

In this paper, our primary focus is on training the feature extractor \(\mathcal{F}_{\theta}\) with the objective of extracting features that enhance the discriminability of object depth. At the same time, we aim to preserve the discriminability of other sub-tasks, such as the projected 3D center, bounding box size, and heading directions. To achieve this, we propose a metric learning method that encourages the network to extract depth-discriminative features by leveraging object depth labels.

Given \(M\) training images \(\mathbf{I}^{i}\), where \(i\in I=\{1,...,M\}\), and \(N^{i}\) GT objects in the image \(\mathbf{I}^{i}\), we extract object descriptors \(\rho^{(i,j)}\), where \(j\in J=\{1,...,N^{i}\}\), from feature maps \(\mathbf{h}^{i}\) using GT coarse projected 3D center \(u^{(i,j)},v^{(i,j)}\) and obtain the corresponding GT object depths \(z^{(i,j)}\). We build a set of object descriptors \(\mathbf{P}=\bigcup_{\{i\in I,\;j\in J\}}\rho^{(i,j)}\) and a set of corresponding object depth \(\mathbf{Z}=\bigcup_{\{i\in I,\;j\in J\}}z^{(i,j)}\). We then define two metric spaces \((\mathbf{Z},d_{1})\) and \((\mathbf{P},d_{2})\), where \(d_{1}\) and \(d_{2}\) represent the Minkowski distance in Euclidean space (_i.e._, L1 distance). The finite sets \(\mathbf{Z}\) and \(\mathbf{P}\) consist of \(L\) elements, corresponding to objects in the dataset (\(|\mathbf{Z}|=|\mathbf{P}|=L=\sum_{i\in I}N^{i}\)). Consequently, we can establish a one-to-one function \(\mathcal{Q}\) between these two metric spaces as follow:

\[\mathcal{Q}(z^{l})=\rho^{l},\;\mathcal{Q}:\mathbf{Z}\rightarrow\mathbf{P},\;\;l \in\{1,2,\dots,L\}.\] (4)

Our goal is for the function \(\mathcal{Q}\) to enforce a quasi-isometric between \(\mathbf{Z}\)-space and \(\mathbf{P}\)-space by using the properties of quasi-isometry in Sec. 3.1. This can encourage the network to extract depth-discriminative features in \(\mathbf{P}\)-space by utilizing object depth labels in \(\mathbf{Z}\)-space. However, enforcing a quasi-isometric between the low-dimensional \(\mathbf{Z}\)-space and the high-dimensional \(\mathbf{P}\)-space can damage the non-linearity of the natural manifold, potentially causing the negative transfer to other sub-tasks. Therefore, we adopt the local distance-preserving condition of the non-linear embedding methods such as Isomap [33] and LLE [29]. The revised version of the quasi-isometry condition is as follows:

1. \(\mathbf{U}_{\hat{z}}=\{z\in\mathbf{Z}|z\in\mathcal{B}_{\hat{z},\epsilon}\}\),
2. \(\forall z_{1}\in\mathbf{Z},\;\text{s.t.}\;\forall z_{2}\in\mathbf{U}_{z_{1}} \;:\;\frac{1}{K}\cdot d_{1}(z_{1},z_{2})-B\leq d_{2}(\mathcal{Q}(z_{1}), \mathcal{Q}(z_{2}))\leq K\cdot d_{1}(z_{1},z_{2})+B\),
3. \(\forall\rho\in\mathbf{P}\;:\;\exists z\in\mathbf{Z},\;\text{s.t.}\;d_{2}( \rho,\mathcal{Q}(z))\leq\epsilon\),where \(\mathcal{B}_{\hat{z},\epsilon}\) is a ball with radius \(\epsilon\) centered around \(\hat{z}\). By applying the quasi-isometric properties solely to neighboring samples within the \(\epsilon\)-ball, the non-linearity of the feature manifold is preserved, and the shortest curve distance between any two arbitrary samples on the manifold (_a.k.a._ geodesic) in \(\mathbf{P}\)-space is maintained instead of the Minkowski distance. By ensuring these three conditions (i), (ii), (iii) are met for a sufficiently small \(K\geq 1\) and \(B\geq 0\), we can encourage object features \(\rho^{l}\) to be depth-discriminative using their corresponding object depth \(z^{l}\). To achieve this, function \(\mathcal{Q}\) must satisfy these conditions, which involves training the parameter \(\theta\) of the feature extractor \(\mathcal{F}\).

(K, B, \(\epsilon\))-Quasi-isometric loss.We propose a quasi-isometric loss term that enforces the quasi-isometric between the depth and feature metric spaces while preserving the distance among the neighbor data points as illustrated in Fig. 1. In particular, the proposed loss term is designed to arrange \(\mathbf{P}\)-space samples that do not satisfy these conditions. To ensure efficient training within the constraints of limited GPU memory, we use the samples within a mini-batch instead of using the entire input dataset as done in previous works [2; 33; 3]. We selectively choose the neighbor objects in a mini-batch with respect to their corresponding depth labels, where neighboring object depths are within \(\mathbf{U}_{z}\).

Suppose that objects in mini-batch images \(\mathbf{I}_{b}\) correspond to the object depths \(\mathbf{Z}_{b}\) and object features \(\mathbf{P}_{b}\) on \(\mathbf{Z}\)-space and \(\mathbf{P}\)-space, respectively. We can then identify the object feature pair sets \(\mathbf{P}_{b}^{+},\mathbf{P}_{b}^{-}\) that violate the revised property (ii) by transposing it as follow:

\[\forall(\rho_{1},\rho_{2})\in\mathbf{P}_{b}^{+}\subset\mathbf{P} _{b}\times\mathbf{P}_{b},\text{ s.t. }|z_{1},z_{2}|\leq\epsilon:d_{2}(\rho_{1},\rho_{2})\nleq K \cdot d_{1}(z_{1},z_{2})+B,\] (5a) \[\forall(\rho_{1},\rho_{2})\in\mathbf{P}_{b}^{-}\subset\mathbf{P} _{b}\times\mathbf{P}_{b},\text{ s.t. }|z_{1},z_{2}|\leq\epsilon:\frac{1}{K}\cdot d_{1}(z_{1},z_{2})-B\nleq d_{2}( \rho_{1},\rho_{2}),\] (5b)

where \((z_{1},z_{2})\) is the corresponding depth pair of the object features \((\rho_{1},\rho_{2})\). For an object feature pair with distance \(d_{2}(\rho_{1},\rho_{2})\) in \(\mathbf{P}\)-space, the distance is larger for \(\mathbf{P}_{b}^{+}\) in Eq. 5a or smaller for \(\mathbf{P}_{b}^{-}\) in Eq. 5b than the corresponding depth pair distance \(d_{1}(z_{1},z_{2})\) with a factor of \(K\) and the additive constant \(B\). To ensure these property-violating object feature pairs \(\mathbf{P}_{b}^{+},\mathbf{P}_{b}^{-}\) satisfy the property (ii), we propose the \((K,B,\epsilon)\)-quasi-isometry loss, which modifies the Normalized Temperature-Scaled Cross-Entropy loss (NT-Xent loss) from [6] as follows:

\[\begin{split}\mathcal{L}_{qi}=-\frac{1}{|\mathbf{P}_{b}^{+}|}& \sum_{(\rho_{i},\rho_{j})\in\mathbf{P}_{b}^{+}}\log\frac{\mathrm{S}^{+}( \rho_{i},\rho_{j})}{\mathrm{S}^{+}(\rho_{i},\rho_{j})+\sum_{(\rho_{k},\rho_{l} )\in\mathbf{P}_{b}^{-}}\mathrm{S}^{-}(\rho_{k},\rho_{l})},\\ &\qquad\qquad\mathrm{S}^{+}(\rho_{i},\rho_{j})=\exp(-(\|\rho_{i}, \rho_{j}\|_{p}-K|z_{i},z_{j}|-B)/\tau),\\ &\qquad\qquad\mathrm{S}^{-}(\rho_{i},\rho_{j})=\exp(-(\frac{1}{K }|z_{i},z_{j}|-B-\|\rho_{i},\rho_{j}\|_{p})/\tau),\end{split}\] (6)

Figure 1: Illustration of our quasi-isometric loss \(\mathcal{L}_{qi}\).

where \(\tau\) is the temperature term, and \(\|\cdot,\cdot\|_{p}\) is \(p\)-norm, and \(K\geq 1,B\geq 0\) denote the pre-defined hyperparameters that determine the hardness of quasi-isometric property. This loss aims to make the property-violated object features comply with the quasi-isometric property by assuming \(\mathbf{P}_{b}^{+/-}\) as positive/negative anchors, respectively. The similarity metrics \(\mathrm{S}^{+/-}\) are always positive values in \((0,1]\) that imply a negative distance gap between the distance metric on \(\mathbf{Z}\)-space and \(\mathbf{P}\)-space. These adjustments are necessary for proper alignment between the two spaces, noting that \(\mathbf{P}_{b}\) is derived from \(\mathcal{F}_{\theta}(\mathbf{I}_{b})\).

```
0:\(\mathbf{h},(u_{i},v_{i}),z_{i}\), where \(i\in\{1,2,...,n\}\), \(n\) is the number of objects in a batch.
1:\(\mathbf{h}_{avg}\leftarrow\) avg_pool_5x5(\(\mathbf{h}\))# feature map avg. pool
2:initialize \(\mathbf{P}\) =[]
3:for\(\forall\) i do
4:\(\mathbf{P}[i]=\rho_{i}=\mathbf{h}_{avg}[:,u_{i},v_{i}]\)
5:endfor
6:\(\mathbf{Z}\leftarrow\{z_{1},z_{2},\dots,z_{n}\}\) that correspond to \(\mathbf{P}\).
7:\(\mathcal{M}_{\mathbf{P}}\leftarrow\left(\begin{smallmatrix}<\rho_{1},\rho_{1 }><\rho_{1},\rho_{2}>&...&<\rho_{1},\rho_{n}>\\ \vdots&\vdots&\ddots&\vdots\\ <\rho_{n},\rho_{1}><\rho_{n},\rho_{2}>&...&<\rho_{n},\rho_{n}>\end{smallmatrix}\right)\),
8:\(\mathcal{M}_{\mathbf{Z}}\leftarrow\left(\begin{smallmatrix}|z_{1},z_{1}|&|z_{1},z_{2}|&...&|z_{1},z_{n}|\\ \vdots&\vdots&\ddots&\vdots\\ |z_{n},z_{1}|&|z_{n},z_{2}|&...&|z_{n},z_{n}|\end{smallmatrix}\right)\)
9:\(\mathcal{M}^{+}\leftarrow\mathcal{M}_{\mathbf{P}}-K\mathcal{M}_{\mathbf{Z}}-B\)# To find property-violated object features. (Eq. 5a)
10:\(\mathcal{M}^{-}\leftarrow\frac{1}{K}\mathcal{M}_{\mathbf{Z}}-\mathcal{M}_{ \mathbf{P}}-B\)# To find property-violated object features. (Eq. 5b)
11:\((\mathcal{M}^{+})_{ij}\gets 0\), where \(|z_{i},z_{j}|>\epsilon\) or \(i\geq j\) or \((\mathcal{M}^{+})_{ij}<0\).
12:\((\mathcal{M}^{-})_{ij}\gets 0\), where \(|z_{i},z_{j}|>\epsilon\) or \(i\geq j\) or \((\mathcal{M}^{-})_{ij}<0\).
13:\(\mathrm{ancs}^{+}\leftarrow\exp(-\mathcal{M}^{+}/\tau)\)#\(\tau\) is temperature term.
14:\(\mathrm{ancs}^{-}\leftarrow\sum(\exp(-\mathcal{M}^{-}/\tau))\)#\(\delta=1e-12\): div. assert
15:\(\mathcal{L}_{qi}=\text{mean}(-\log(\text{ancs}^{+}/(\text{ancs}^{+}+\text{ancs}^{-}+ \delta)))\)#\(\delta=1e-12\): div. assert ```

**Algorithm 1**\((K,B,\epsilon)\)-Quasi-isometric loss

Object-wise depth map loss.According to Eq. 1, the estimated object depth \(\hat{z}\) and coarse projected 3D center \((\hat{u},\hat{v})\) jointly determine the location of the object 3D center \((\hat{X},\hat{Y},\hat{Z})\). However, many existing methods [20, 21, 23, 27] train the depth loss on the local region exactly on the GT center \((u,v)\), which can lead to significant errors in object depth estimation even with slightly inaccurate object center \((\hat{u},\hat{v})\) during inference (_i.e._, center shifting in the image plane by a few pixels) [23].

To mitigate this issue, we train our network with an additional auxiliary head for object-wise depth estimation using the structure of [20]. Rather than solely providing depth supervision to an object center \((u,v)\), the auxiliary head is trained with depth supervision over the entire bounding box of the object. We create the foreground object-wise depth map \(\mathfrak{D}\in\mathbb{R}^{H^{\prime}\times W^{\prime}}\) following the policy of [40]. Then, we define the object-wise depth loss by adopting the Laplacian aleatoric uncertainty loss [9], the same as the object depth estimation task, only for the foreground regions:

\[\mathcal{L}_{obj}=\frac{1}{|\mathfrak{D}|}\sum_{z^{p}\in\mathfrak{D}}\frac{ \sqrt{2}}{\hat{\sigma}^{p}}\left|z^{p}-\hat{z}^{p}\right|+\log\left(\hat{ \sigma}^{p}\right),\] (7)

where \(z^{p},\hat{z}^{p}\) and \(\hat{\sigma}^{p}\) are the corresponding GT object depth, prediction, and uncertainty of each pixel, respectively. Note that the additional task head is removed after training, so the proposed method does not increase the inference time.

Total loss.The total loss \(\mathcal{L}_{total}\) combines the loss used in 3D object detection baselines \(\mathcal{L}_{baseline}\), the quasi-isometric loss \(\mathcal{L}_{qi}\), and the object-wise depth map loss \(\mathcal{L}_{obj}\) as follows:

\[\mathcal{L}_{total}=\mathcal{L}_{baseline}+\lambda_{qi}\cdot\mathcal{L}_{qi}+ \lambda_{obj}\cdot\mathcal{L}_{obj},\] (8)

where \(\lambda_{qi}\) and \(\lambda_{obj}\) represent the balancing weights for the quasi-isometric loss and the object-wise depth map loss, respectively. These values are set to 0.5 and 1. This total loss is applied to each baseline in our experiments, which we denote as "[Baseline] + Ours", as discussed further in Sec. 5.

[MISSING_PAGE_FAIL:7]

only for _Car_ but also for _Pedestrian_ and _Cyclist_. The average performance increases for these object categories are 10.3%, 12.4%, and 53.1%, respectively.

Waymo dataset.We extend the evaluations to the Waymo dataset to further demonstrate the generality of our method, as shown in Tab. 3. The results indicate that our proposed method consistently enhances performance by an average of 4.6% and 4.5% in terms of mAP and mAPH metrics across all levels, IoU thresholds, and distance ranges. Significantly, the proposed loss boosts the depth discrimination not only within specific depth ranges but across all depth ranges. This underscores the effectiveness of our method not just on the KITTI dataset, but on the Waymo dataset as well.

### Additional Experiments

Ablation studies.To demonstrate the effectiveness of our proposed method, we conduct the ablation studies of two components of our method: \((K,B,\epsilon)\)-quasi-isometric loss \(\mathcal{L}_{qi}\) and object-wise depth map loss \(\mathcal{L}_{obj}\). We employ DID-M3D [27] and MonoCon [20] as baselines of a method with and without extra data. The results in Tab. 4 indicate that any method incorporating either the quasi-isometric or object-wise depth losses sees a performance improvement. Notably, the \((K,B,\epsilon)\)-quasi-isometric loss contributes to a larger performance gain than the object-wise depth loss (+2.3% vs. +1.9% for DID-M3D and +8.7% vs. +5.0% for MonoCon). The models trained with both of the proposed losses significantly surpass the performance of the baselines.

Comparison of our method with SupCR.To demonstrate the advantage of our quasi-isometric loss over existing metric learning schemes, we compare the task performance of our method with that of SupCR (Supervised Contrastive Regression) [39]. SupCR is the first regression-aware representation learning method that effectively applies metric learning to regression tasks using GT labels. Similar to our quasi-isometric loss \(\mathcal{L}_{qi}\), SupCR selectively chooses the relative negative pair object features based on positive pair distance. In Tab. 5, we report the \(AP_{3D|R40}\) for _Car, Moderate_ and errors between the GT and prediction of four key tasks that determine the location of the 3D bounding box: \(t\in z,(h,w,l),\gamma,c\), independently. The errors include the absolute difference \(E_{z},E_{dim}\) for \(z,(h,w,l)\), respectively, the mean angular distance \(\Delta\gamma\)[11] for \(\gamma\), and the accuracy \(Acc_{c}\) for object

classification \(c\). The results illustrate that our method enhances 3D object detection performance, whereas SupCR diminishes it. Interestingly, both methods reduce depth errors \(E_{z}\) as they train the encoder to extract more depth-discriminative features. However, SupCR significantly increases the errors in bounding box size and angle estimation \(E_{dim},\Delta\gamma\) by approximately 24.0% and 18.9%, respectively. It demonstrates that SupCR, which forcibly arranges the feature manifold, can negatively impact the performance of other tasks due to the complex shared representations across multiple sub-tasks.

Scalability of our quasi-isometric loss with the anchor-based method and BEV paradigm.In Tab. 1-4, we note that our method improves performance when applied to CenterNet-based baselines (anchor-free). We further demonstrate the effectiveness of the proposed metric learning incorporated into anchor-based methods and the bird-eye-view (BEV) paradigm: MonoDTR [15] and ImVoxelNet [30]. The object-wise depth map loss is not applied to these baselines, since it already uses an auxiliary depth loss or inherent nature of the BEV paradigm.

As shown in Tab. 6, although the performance enhancement compared to anchor-free methods is not as significant, our quasi-isometric loss still shows consistent improvements across all metrics for each baseline. These results suggest that the loss has broad applicability to the tasks that extract object features on image spatial coordinates given object depth labels.

Model performance with respect to the ratio of property-violated objects.The proposed quasi-isometric loss is designed to arrange the feature space in accordance with object depth labels, aiming to meet quasi-isometric properties. To demonstrate the correlation between 3D object detection performance and the ratio of features that satisfy the quasi-isometric condition, we conduct an additional experiment. In Fig. 2, we plot the ratio of object pairs violating the properties (where the Ratio \(=(|\mathbf{P}^{\top}|+|\mathbf{P}^{\top}|)/|\mathbf{P}|\), \(|\mathbf{P}|=\binom{L}{2}\), and \(L\) is the total number of objects in the dataset.) and model performance in relation to various loss scales \(\lambda_{qi}\). For this experiment, we use the baseline model [20] trained with different loss scales \([10^{-5},10^{-3},10^{-2},10^{-1},0.5]\). We measure the \(AP_{3D}\) with IoU thresholds of 0.7 for _Car, Moderate_ on the KITTI validation dataset. The results indicate that an increase in the loss scale corresponds with a decrease in the ratio of object pairs violating the quasi-isometric properties, leading to an improvement in model performance. This

\begin{table}
\begin{tabular}{c c|c|c c c|c} \hline \hline \begin{tabular}{c} Components \\ \(\mathcal{L}_{qi}\) \\ \end{tabular} & \multirow{2}{*}{Baseline} & \multicolumn{3}{c|}{
\begin{tabular}{c} _Car_, \(AP_{3D|R40}\uparrow\) \\ \end{tabular} } & \multirow{2}{*}{**Overall**} \\ \cline{3-3} \cline{5-6} \cline{7-7} \multicolumn{1}{c|}{\(\mathcal{L}_{qi}\)} & & & Easy & & \multicolumn{1}{c|}{} & \\ \hline \multirow{2}{*}{\(\surd\)} & \multirow{2}{*}{DID-MBD [27]} & \multirow{2}{*}{23.93} & \multirow{2}{*}{16.22} & \multirow{2}{*}{13.98} & \multirow{2}{*}{0.0\%} \\  & & & & 24.61 (+2.8\%) & & 16.57 (+2.2\%) & 14.23 (+1.8\%) & \(+2.3\%\) \\  & \(\surd\) & & & 24.53 (+2.5\%) & 16.84 (+3.8\%) & 13.87 (-0.8\%) & \(+1.8\%\) \\ \(\surd\) & \(\surd\) & & & 24.77 (+3.5\%) & 17.12 (+5.5\%) & 14.30 (+2.3\%) & \(+3.8\%\) \\ \hline \multirow{2}{*}{\(\surd\)} & \multirow{2}{*}{MonoCon [20]} & \multirow{2}{*}{23.03} & \multirow{2}{*}{17.84} & \multirow{2}{*}{15.37} & \multirow{2}{*}{0.0\%} \\  & & & & 27.00 (+17.2\%) & 18.68 (+4.7\%) & 16.02(+4.2\%) & \(+8.7\%\) \\ \cline{1-1}  & \(\surd\) & & & 24.90 (+8.1\%) & 18.65 (+4.5\%) & 15.73 (+2.3\%) & \(+5.0\%\) \\ \(\surd\) & \(\surd\) & & & 27.90 (+21.1\%) & 19.43 (+8.9\%) & 16.92 (+10.3\%) & \(+13.5\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation studies on KITTI _validation_ set.

\begin{table}
\begin{tabular}{l|l l l l l} \hline \hline Baseline & \(AP_{3D|R40}\uparrow\) & \(E_{z}\) (\(m\)) \(\downarrow\) & \(E_{dim}\) (\(m\)) \(\downarrow\) & \(\Delta\gamma\) (rad) \(\downarrow\) & \(Acc_{c}\) (\(\%\)) \(\uparrow\) \\ \hline MonoCon [20] & 17.84 & 0.019 & 0.025 & \(\nicefrac{{\pi}}{{371.79}}\) & 93.15 \\ MonoCon + SupCR & 17.55 (\(-1.6\%\)) & 0.014 (\(-26.3\%\)) & 0.031 (\(+24.0\%\)) & \(\nicefrac{{\pi}}{{312.78}}\) (\(+18.9\%\)) & 91.11 (\(-2.2\%\)) \\ MonoCon + \(\mathcal{L}_{qi}\) & 18.68 (\(+4.7\%\)) & 0.011 (\(-42.1\%\)) & 0.024 (\(-4.0\%\)) & \(\nicefrac{{\pi}}{{368.11}}\) (\(+1.0\%\)) & 93.07 (\(-0.1\%\)) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of our method with SupCR on KITTI _validation_ set.

Figure 2: Loss scale (log-scaled) with respect to property-violated pairs ratio and performance.

suggests a potential relationship between the reduction in the ratio of property-violated object pairs and the enhancement of model performance.

Influence of hyperparameters \((K,B,\epsilon)\) in quasi-isometric loss.The choice of the tunable hyperparameter \(\epsilon\) is crucial. An excessively small \(\epsilon\) would sample a few objects within a mini-batch, making representation learning infeasible. On the other hand, quasi-isometric properties with too large an \(\epsilon\) would harm the non-linearity of the feature manifold in \(\mathbf{P}\)-space. To investigate the influence of hyperparameters on our quasi-isometric loss, we evaluate the performance of "Monocon + Ours" using KITTI [12] validation set for various hyperparameters \((K,B,\epsilon)\), as presented in Tab. 7. Although most hyperparameter combinations improved performance, there are notable exceptions, especially when \(K=2.0\) and \(\epsilon=\infty\). We observe a decline in performance with excessively large values of \(K\) or \(\epsilon\), which define the criteria for property violation and the non-linearity of the feature manifold.

## 6 Conclusion

In this paper, we address the challenge of monocular 3D object detection in RGB images by proposing a novel metric learning scheme. Our method, which does not rely on extra parameters, modules, or data, concentrates on extracting depth-discriminative features without increasing the inference time or model size. By employing a distance-preserving function and the \((K,B,\epsilon)\)-quasi-isometric loss, we successfully arrange the feature space manifold in accordance with ground-truth object depth, while preserving the non-linearity of the natural feature manifold. Furthermore, by introducing an auxiliary head for object-wise depth estimation, we improve object depth quality without increasing inference time. Our experimental results on the KITTI and Waymo datasets illustrate consistent performance enhancements across different baselines, highlighting the effectiveness of our proposed method. As a potential avenue for future work, our method could feasibly be extended to multi-camera 3D object detection scenarios and other regression tasks that involve multiple sub-tasks.

## Acknowledgments and Disclosure of Funding

This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00210908).

\begin{table}
\begin{tabular}{l|l l l|l} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{_Car_, \(AP_{3D|R40}\uparrow\)} & \multirow{2}{*}{**Overall**} \\  & Easy & \multicolumn{2}{c}{Moderate} & \\ \hline MonoDTR [15] & 24.57 & 18.45 & 15.37 & - \\ MonoDTR + Ours & 26.17 (+6.5\%) & 19.07 (+3.4\%) & 15.93 (+3.6\%) & +4.5\% \\ \hline ImVoxelNet [30] & 24.54 & 17.80 & 15.87 & - \\ ImVoxelNet + Ours & 26.14 (+8.0\%) & 18.20 (+7.2\%) & 15.81 (+3.8\%) & +6.3\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: Evaluation results of anchor-based and bird-eye-view paradigms on KITTI _validation_ set.

\begin{table}
\begin{tabular}{l l l|l l|l} \hline \hline \multicolumn{2}{l|}{Components} & \multicolumn{3}{c|}{_Car_, \(AP_{3D|R40}\uparrow\)} & \multirow{2}{*}{**Overall**} \\ K & B & \(\epsilon\) & Easy & \multicolumn{1}{c}{Moderate} & \\ \hline MonoCon (w/o \(\mathcal{L}_{obj}\)) & 23.03 & 17.84 & 15.37 & +0.0\% \\ \hline
1.0 & 0.5 & 10.0 & 24.67 (+7.1\%) & 18.09 (+1.4\%) & 15.34 (\(-\)0.2\%) & +2.8\% \\
1.5 & 0.5 & 10.0 & **27.00** (+17.2\%) & 18.68 (+4.7\%) & **16.02** (+4.2\%) & **+8.7\%** \\
2.0 & 0.5 & 10.0 & 23.34 (+1.3\%) & 17.44 (\(-\)2.2\%) & 14.80 (\(-\)3.7\%) & -1.5\% \\ \hline
1.5 & 0.0 & 10.0 & 24.54 (+6.6\%) & 18.01 (+1.0\%) & 15.37 (+0.0\%) & +2.5\% \\
1.5 & 1.0 & 10.0 & 25.49 (+10.7\%) & **18.70** (+4.8\%) & 15.82 (+2.9\%) & +6.1\% \\
1.5 & 5.0 & 10.0 & 23.70 (+2.9\%) & 18.00 (+0.9\%) & 15.12 (+1.6\%) & +0.9\% \\ \hline
1.5 & 0.5 & 1.0 & 24.71 (+7.3\%) & 17.84 (+0.0\%) & 15.43 (+0.4\%) & +2.6\% \\
1.5 & 0.5 & 5.0 & 24.99 (+8.5\%) & 18.23 (+2.2\%) & 15.80 (+2.8\%) & +4.5\% \\
1.5 & 0.5 & \(\infty\) & 22.85 (\(-\)0.8\%) & 16.91 (\(-\)5.2\%) & 14.14 (\(-\)8.0\%) & -4.7\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance of KITTI _validation_ with respect to \(K,B,\epsilon\).

## References

* [1]H. Abdi and L. J. Williams (2010) Principal component analysis. Wiley interdisciplinary reviews: computational statistics2 (4), pp. 433-459. Cited by: SS1.
* [2]M. Belkin and P. Niyogi (2003) Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation15 (6), pp. 1373-1396. Cited by: SS1.
* [3]G. Brazil and X. Liu (2019) M3d-rpn: monocular 3d region proposal network for object detection. In ICCV, pp. 9287-9296. Cited by: SS1.
* [4]S. G. Brick (1993) Quasi-isometries and ends of groups. Journal of pure and applied algebra86 (1), pp. 23-33. Cited by: SS1.
* [5]S. Cao, W. Lu, and Q. Xu (2016) Deep neural networks for learning graph representations. In AAAI, Vol. 30. Cited by: SS1.
* [6]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020) A simple framework for contrastive learning of visual representations. In ICML, pp. 1597-1607. Cited by: SS1.
* [7]X. Chen and K. He (2021) Exploring simple siamese representation learning. In CVPR, pp. 15750-15758. Cited by: SS1.
* [8]Y. Chen, H. Dai, and Y. Ding (2022) Pseudo-stereo for monocular 3d object detection in autonomous driving. In CVPR, pp. 887-897. Cited by: SS1.
* [9]Y. Chen, L. Tai, K. Sun, and M. Li (2020) Monopair: monocular 3d object detection using pairwise spatial relationships. In CVPR, pp. 12093-12102. Cited by: SS1.
* [10]M. Ding, Y. Huo, H. Yi, Z. Wang, J. Shi, Z. Lu, and P. Luo (2020) Learning depth-guided convolutions for monocular 3d object detection. In CVPRW, pp. 1000-1001. Cited by: SS1.
* [11]D. Eigen and R. Fergus (2015) Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV, pp. 2650-2658. Cited by: SS1.
* [12]A. Geiger, P. Lenz, and R. Urtasun (2012) Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, Cited by: SS1.
* [13]M. Gromov, M. Katz, P. Pansu, and S. Semmes (1999) Metric structures for Riemannian and non-Riemannian spaces. Vol. 152, Springer. Cited by: SS1.
* [14]Y. Hong, H. Dai, and Y. Ding (2022) Cross-modality knowledge distillation network for monocular 3d object detection. In ECCV, pp. 87-104. Cited by: SS1.
* [15]K. Huang, T. Wu, H. Su, and W. H. Hsu (2022) Monodtr: monocular 3d object detection with depth-aware transformer. In CVPR, pp. 4012-4021. Cited by: SS1.
* [16]P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan (2020) Supervised contrastive learning. NeurIPS33, pp. 18661-18673. Cited by: SS1.
* [17]J. B. Kruskal (1964) Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. Psychometrika29 (1), pp. 1-27. Cited by: SS1.
* [18]A. Kumar, G. Brazil, E. Corona, A. Parchami, and X. Liu (2022) Deviant: depth equivariant network for monocular 3d object detection. In ECCV, pp. 664-683. Cited by: SS1.

[MISSING_PAGE_POST]

* [21] Zechen Liu, Zizhang Wu, and Roland Toth. Smoke: Single-stage monocular 3d object detection via keypoint estimation. In _CVPRW_, pages 996-997, 2020.
* [22] Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie Yan, and Wanli Ouyang. Geometry uncertainty projection network for monocular 3d object detection. In _ICCV_, pages 3111-3121, 2021.
* [23] Xinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li, and Wanli Ouyang. Delving into localization errors for monocular 3d object detection. In _CVPR_, pages 4721-4730, 2021.
* [24] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape. In _CVPR_, pages 2069-2078, 2019.
* [25] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka. 3d bounding box estimation using deep learning and geometry. In _CVPR_, pages 7074-7082, 2017.
* [26] Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon. Is pseudo-lidar needed for monocular 3d object detection? In _ICCV_, pages 3142-3152, 2021.
* [27] Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, and Deng Cai. Did-m3d: Decoupling instance depth for monocular 3d object detection. In _ECCV_, pages 71-88. Springer, 2022.
* [28] Cody Reading, Ali Harakeh, Julia Chae, and Steven L Waslander. Categorical depth distribution network for monocular 3d object detection. In _CVPR_, pages 8555-8564, 2021.
* [29] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. _science_, 290(5500):2323-2326, 2000.
* [30] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Invoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection. In _WACV_, pages 2397-2406, 2022.
* [31] Adrian Spurr, Aneesh Dahiya, Xi Wang, Xucong Zhang, and Otmar Hilliges. Self-supervised 3d hand pose estimation from monocular rgb via contrastive learning. In _ICCV_, pages 11230-11239, 2021.
* [32] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In _CVPR_, pages 2446-2454, 2020.
* [33] Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. _science_, 290(5500):2319-2323, 2000.
* [34] Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In _Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 1225-1234, 2016.
* [35] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In _CVPR_, pages 8445-8453, 2019.
* [36] Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Contrastive regression for domain adaptation on gaze estimation. In _CVPR_, pages 19376-19385, 2022.
* [37] Zizhang Wu, Yunzhe Wu, Jian Pu, Xianzhi Li, and Xiaoquan Wang. Attention-based depth distillation with 3d-aware positional encoding for monocular 3d object detection. _arXiv preprint arXiv:2211.16779_, 2022.
* [38] Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Data-driven 3d voxel patterns for object category recognition. In _CVPR_, pages 1903-1911, 2015.
* [39] Kaiwen Zha, Peng Cao, Yuzhe Yang, and Dina Katabi. Supervised contrastive regression. _arXiv preprint arXiv:2210.01189_, 2022.

* [40] Renrui Zhang, Han Qiu, Tai Wang, Xuanzhuo Xu, Ziyu Guo, Yu Qiao, Peng Gao, and Hongsheng Li. Monodetr: Depth-aware transformer for monocular 3d object detection. _arXiv preprint arXiv:2203.13310_, 2022.
* [41] Yunpeng Zhang, Jiwen Lu, and Jie Zhou. Objects are different: Flexible monocular 3d object detection. In _CVPR_, pages 3289-3298, 2021.
* [42] Zhenyue Zhang and Hongyuan Zha. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. _SIAM journal on scientific computing_, 26(1):313-338, 2004.
* [43] Xingyi Zhou, Dequan Wang, and Philipp Krahenbuhl. Objects as points. _arXiv preprint arXiv:1904.07850_, 2019.