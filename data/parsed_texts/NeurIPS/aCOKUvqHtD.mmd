# Polynomial-Time Linear-Swap Regret Minimization

in Imperfect-Information Sequential Games

 Gabriele Farina

MIT

gfarina@mit.edu

&Charilaos Pipis

MIT

chpipis@mit.edu

###### Abstract

No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformations considered by the learner determines a natural notion of rationality. As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated equilibria in normal-form games and extensive-form correlated equilibria in extensive-form games. At the extreme, a _no-swap-regret_ agent is one that minimizes regret against the set of _all_ functions from the set of strategies to itself. While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained _efficiently_ in the worst case in _sequential_ (extensive-form) games is a longstanding open problem. In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to _all linear transformations_ of the mixed strategy space, a notion called _no-linear-swap regret_. This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential games--thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call _linear-deviation correlated equilibria_, that can be approached efficiently.

## 1 Introduction

The framework of regret minimization provides algorithms that players can use to gradually improve their strategies in a repeated game, enabling learning strong strategies even when facing unknown and potentially adversarial opponents. One of the appealing properties of no-regret learning algorithms is that they are _uncoupled_, meaning that each player refines their strategy based on their own payoff function, and on other players' strategies, but not on the payoff functions of other players. Nonetheless, despite their uncoupled nature and focus on _local_ optimization of each player's utility, it is one of the most celebrated results in the theory of learning in games that in many cases, when all players are learning using these algorithms, the empirical play recovers appropriate notions of _equilibrium_--a _global_ notion of game-theoretic optimality. Strategies constructed via no-regret learning algorithms (or approximations thereof) have been key components in constructing human-level and even superhuman AI agents in a variety of adversarial games, including Poker (Moravcik et al., 2017; Brown and Sandholm, 2018, 2019), Stratego (Perolat et al., 2022), and Diplomacy (Bakhtin et al., 2023).

In regret minimization, each learning agent seeks to minimize the difference between the loss (opposite of reward) they accumulated through the actions they played, and the loss they would haveaccumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformation functions considered by the learning agent determines a natural notion of rationality of the agent. Already when the agents seeks to learn strategies that cumulate low regret against _constant_ strategy transformations only--a notion of regret called _external_ regret--the average play of the agents converges to a Nash equilibrium in two-player constant-sum games, and to a coarse correlated equilibrium in general-sum multiplayer games. As the sets of transformations the each agent considers grows, more complex equilibria can be achieved, including correlated equilibria in normal-form games (Foster and Vohra (1997); Fudenberg and Levine (1995, 1999); Hart and Mas-Colell (2000, 2001); see also the monograph by Fudenberg and Levine (1998)) and extensive-form correlated equilibria in extensive-form games (Farina et al., 2022). At the extreme, a maximally hindsight-rational agent is one that minimizes regret against the set of _all_ functions from the strategy space to itself (aka. _swap_ regret). While it is known that maximum hindsight rationality can be attained efficiently in nonsequential (normal-form) games (Stoltz and Lugosi, 2007; Blum and Mansour, 2007), it is a major open problem to determine whether the same applies to sequential (_i.e._, extensive-form) games, and more generally what is the strongest notion of rationality that can be attained efficiently in the worst case in the latter setting.

In this paper, we provide a positive result in that direction, by showing that hindsight rationality can be achieved efficiently in general imperfect-information extensive-form games when one restricts to the set of _all linear transformations_ of the mixed strategy space--a notion called _linear-swap regret_, and that coincides with swap regret in normal-form games. In order to establish the result, we introduce several intermediate results related to the geometry of sequence-form strategies in extensive-form games. In particular, a crucial result is given in Theorem 3.1, which shows that the set of linear functions \(\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\) from the sequence-form strategy set \(\mathcal{Q}\) of a player in an extensive-form game to a generic convex polytope \(\mathcal{P}\) can be captured using only polynomially many linear constraints in the size of the game tree and the number of linear constraints that define \(\mathcal{P}\). Applying the result to the special case \(\mathcal{P}=\mathcal{Q}\), we are then able to conclude that the the polytope of linear transformations \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) from the sequence-form strategy set to itself can be captured by polynomially many linear constraints in the size of the game tree, and the norm of any element is polynomially bounded. The polynomial characterization and bound for \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) is used in conjunction with an idea of Gordon et al. (2008) to construct a no-linear-swap-regret minimizer for the set of strategies \(\mathcal{Q}\) starting from two primitives: i) a no-external-regret algorithm for the set of transformations \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\), and ii) an algorithm to compute a fixed point strategy for any transformation in \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\). In both cases, the polynomial representation of \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) established through Theorem 3.1 plays a fundamental role. It allows, on the one hand, to satisfy requirement ii) using linear programming. On the other hand, it enables us to construct a no-external-regret algorithm that outputs transformations in \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) with polynomial-time iterations, by leveraging the known properties of online projected gradient descent, exploiting the tractability of projecting onto polynomially-representable polytopes.

Finally, in the last section of the paper we turn our attention away from hindsight rationality to focus instead on the properties of the equilibria that our no-linear-swap-regret dynamics recover in extensive-form games. The average play of no-linear-swap-regret players converges to a set of equilibria that we coin _linear-deviation correlated equilibria (LCEs)_. LCEs form a superset of correlated equilibria and a subset of extensive-form correlated equilibria in extensive-form games. In Section 4 we show that these inclusions are in general strict, and provide additional results about the complexity of computing a welfare-maximizing LCE.

Related workAs mentioned in the introduction, the existence of uncoupled no-regret dynamics leading to correlated equilibrium (CE) in multiplayer normal-form games is a celebrated result dating back to at least the work by Foster and Vohra (1997). That work inspired researchers to seek uncoupled learning procedures in other settings as well. For example, Stoltz and Lugosi (2007) studies learning dynamics leading to CE in games with an infinite (but compact) action set, while Kakade et al. (2003) focuses on graphical games. In more recent years, a growing effort has been spent towards understanding the relationships between no-regret learning and equilibria in imperfect-information extensive-form games, the settings on which we focus. Extensive-form games pose additional challenges when compared to normal-form games, due to their sequential nature and presence of imperfect information. While efficient no-external-regret learning dynamics for extensive-form games are known (including the popular CFR algorithm (Zinkevich et al., 2008)), as of today not much is known about no-swap-regret and the complexity of learning CE in extensive-form games.

In recent work, Anagnostides et al. (2023) construct trigger-regret dynamics that converge to EFCE at a rate of \(O(\frac{\log T}{T})\), whereas our regret dynamics converge to linear-deviation correlated equilibria at a slower rate of \(O(\frac{1}{\sqrt{T}})\). While the aforementioned paper proposes a general methodology that applies to CEs in normal-form games and EFCE/EFCCE in sequential games, the authors' construction fundamentally relies on being able to express the fixed points computed by the algorithm as (linear combinations of) rational functions with positive coefficients of the deviation matrices. For EFCE and EFCCE this fundamentally follows from the fact that the fixed points can be computed inductively, solving for stationary distributions of local Markov chains at each decision point. The no-linear-swap regret algorithm proposed in our paper does not offer such a local characterization of the fixed point and thus, we cannot immediately transfer the improved regret bounds to our case.

The closest notion to CE that is known to be efficiently computable in extensive-form games is _extensive-form correlated equilibrium_ (EFCE) (von Stengel and Forges, 2008; Huang and von Stengel, 2008). The question of whether the set of EFCE could be approached via uncoupled no-regret dynamics with polynomial-time iterations in the size of the extensive-form games was recently settled in the positive (Farina et al., 2022; Celli et al., 2020). In particular, Farina et al. (2022) show that EFCE arises from the average play of no-trigger-regret algorithms, where trigger deviations are a particular subset of linear transformations of the sequence-form strategy polytope \(\mathcal{Q}\) of each player. Since this paper focuses on learning dynamics that guarantee sublinear regret with respect to _any_ linear transformation of \(\mathcal{Q}\), it follows immediately that the dynamics presented in this paper recover EFCE as a special case.

The concept of linear-swap-regret minimization has been considered before in the context of _Bayesian_ games. Mansour et al. (2022) study a setting where a no-regret _learner_ competes in a two-player Bayesian game with a rational utility _maximizer_, that is a strictly more powerful opponent than a learner. Under this setting, it can be shown that in every round the optimizer is guaranteed to obtain at least the Bayesian Stackelberg value of the game. Then they proceed to prove that minimizing _linear-swap regret_ is necessary if we want to cap the optimizer's performance at the Stackelberg value, while minimizing polytope-swap regret (a generalization of swap regret for Bayesian games, and strictly stronger than linear-swap) is sufficient to cap the optimizer's performance. Hence, these results highlight the importance of developing learning algorithms under stronger notions of _rationality_, as is our aim in this paper. Furthermore, these results provide evidence that constructing a no-linear-swap regret learner, as is our goal here, can present benefits when compared to other less rational learners. In a concurrent paper, Fujii (2023) defines the notion of _untruthful swap regret_ for Bayesian games and proves that, for Bayesian games, it is equivalent to the linear-swap regret which is of interest in our paper.

Bayesian games can be considered as a special case of extensive-form games, where a chance node initially selects one of the possible types \(\Theta\) for each player. Thus, our algorithm minimizing linear-swap regret in extensive-form games also minimizes linear-swap regret in Bayesian games. However, we remark that our regret bound depends polynomially on the number of player types \(|\Theta|\) as they are part of the game tree representation, while Fujii (2023) has devised an algorithm for Bayesian games, whose regret only depends on \(\log|\Theta|\).

Finally, we also mention that nonlinear deviations have been explored in extensive-form games, though we are not aware of notable large sets for which polynomial-time no-regret dynamics can be devised. Specifically, we point to the work by Morrill et al. (2021), which defines the notion of "behavioral deviations". These deviations are nonlinear with respect to the sequence-form representation of strategies in extensive-form games. The authors categorize several known or novel types of restricted behavioral deviations into a Deviation Landscape that highlights the relations between them. Even though both the linear deviations, we consider in this paper, and the behavioral deviations seem to constitute rich measures of rationality, none of them contains the other and thus, linear deviations do not fit into the Deviation Landscape of Morrill et al. (2021) (see also Remark E.2).

## 2 Preliminaries

We recall the standard model of extensive-form games, as well as the framework of learning in games.

### Extensive-Form Games

While normal-form games (NFGs) correspond to nonsequential interactions, such as Rock-Paper-Scissors, where players simultaneously pick one action and then receive a payoff based on what others picked, extensive-form games (EFGs) model games that are played on a game tree. They capture both sequential and simultaneous moves, as well as private information and are therefore a very general and expressive model of games, capturing chess, go, poker, sequential auctions, and many other settings as well. We now recall basic properties and notation for EFGs.

Game treeIn an \(n\)-player extensive-form game, each node in the game tree is associated with exactly one player from the set \(\{1,\ldots,n\}\cup\{c\}\), where the special player \(c\)--called the _chance_ player--is used to model random stochastic outcomes, such as rolling a die or drawing cards from a deck. Edges leaving from a node represent actions that a player can take at that node. To model private information, the game tree is supplemented with an information partition, defined as a partition of nodes into sets called information sets. Each node belongs to exactly one information set, and each information set is a nonempty set of tree nodes for the same Player \(i\). An information set for Player \(i\) denotes a collection of nodes that Player \(i\) cannot distinguish among, given what she has observed so far. (We remark that all nodes in a same information set must have the same set of available actions, or the player would distinguish the nodes). The set of all information sets of Player \(i\) is denoted \(\mathcal{J}_{i}\). In this paper, we will only consider _perfect-recall_ games, that is, games in which the information sets are arranged in accordance with the fact that no player forgets what the player knew earlier,.

Sequence-form strategiesSince nodes belonging to the same information set for a player are indistinguishable to that player, the player must play the same strategy at each of the nodes. Hence, a strategy for a player is exactly a mapping from an _information set_ to a distribution over actions. In other words, it is the information sets, and not the game tree nodes, that capture the decision points of the player. We can then represent a strategy for a generic player \(i\) as a vector indexed by each valid information set-action pair \((j,a)\). Any such valid pair is called a _sequence_ of the player; the set of all sequences is denoted as \(\Sigma_{i}:=\{(j,a):j\in\mathcal{J}_{i},a\in\mathcal{A}_{j}\}\cup\{\varnothing\}\), where the special element \(\varnothing\) is called _empty sequence_. Given an information set \(j\in\mathcal{J}_{i}\), we denote by \(p_{j}\) the parent sequence of \(j\), defined as the last pair \((j,a)\in\Sigma_{i}\) encountered on the path from the root to any node \(v\in j\); if no such pair exists we let \(p_{j}=\varnothing\). Finally, we denote by \(\mathcal{C}_{\sigma}\) the children of sequence \(\sigma\in\Sigma_{i}\), defined as the information sets \(j\in\mathcal{J}_{i}\) for which \(p_{j}=\sigma\). Sequences \(\sigma\) for which \(\mathcal{C}_{\sigma}\) is an empty set are called _terminal_; the set of all terminal sequences is denoted \(\Sigma_{i}^{\perp}\).

**Example 2.1**.: _Consider the tree-form decision process faced by Player 1 in the small game of Figure 1 (Left). The decision process has four decision nodes \(\mathcal{J}_{1}=\{\textsf{A},\textsf{B},\textsf{C},\textsf{D}\}\) and nine sequences including the empty sequence \(\varnothing\). For decision node \(\textsf{D}\), the parent sequence is \(p_{\textsf{D}}=\textsf{A}\textsf{Z}\); for \(\textsf{B}\) and \(\textsf{C}\) it is \(p_{\textsf{B}}=\textsf{A}\textsf{I}\); for \(\textsf{A}\) it is the empty sequence \(p_{\textsf{A}}=\varnothing\)._

A _reduced-normal-form plan_ for Player \(i\) represents a deterministic strategy for the player as a vector \(\boldsymbol{x}\in\{0,1\}^{\Sigma_{i}}\) where the entry corresponding to the generic sequence \(\boldsymbol{x}[ja]\) is equal to \(1\) if the player plays action \(a\) at (the nodes of) information set \(j\in\mathcal{J}_{i}\). Information sets that cannot be reached based on the strategy do not have any action select. A crucial property of the reduced-normal-form plan representation of deterministic strategies is the fact that the utility of any player is a multilinear function in the profile of reduced-normal-form plans played by the players. The set of all reduced

Figure 1: (Left) Tree-form decision process considered in the example. Black round nodes belong to Player 1; white round nodes to Player 2. Square white nodes are terminal nodes in the game tree, payoffs are omitted. Gray bags denote information sets. (Right) The constraints that define the sequence-form polytope \(\mathcal{Q}_{1}\) for Player 1 (besides nonnegativity).

normal-form plans of Player \(i\) is denoted with the symbol \(\Pi_{i}\). Typically, the cardinality of \(\Pi_{i}\) is exponential in the size of the game tree.

The convex hull of the set of reduced-normal-form plans of Player \(i\) is called the _sequence-form polytope_ of the player, and denoted with the symbol \(\mathcal{Q}_{i}\coloneqq\text{conv}(\Pi_{i})\). It represents the set of all randomized strategies in the game. An important result by Romanovskii (1962); Koller et al. (1996); von Stengel (1996) shows that \(\mathcal{Q}_{i}\) can be captured by polynomially many constraints in the size of the game tree, as we recall next.

**Definition 2.2**.: _The polytope of sequence-form strategies of Player \(i\) is equal to the convex polytope_

\[\mathcal{Q}_{i}\coloneqq\bigg{\{}\boldsymbol{x}\in\mathbb{R}_{\geq 0}^{ \Sigma}:\begin{array}{ll}(1)&\boldsymbol{x}[\varnothing]=1\\ (2)&\sum_{a\in\mathcal{A}_{j}}\boldsymbol{x}[ja]=\boldsymbol{x}[p_{j}]& \forall\,j\in\mathcal{J}\end{array}\bigg{\}}.\]

As an example, the constraints that define the sequence-form polytope for Player 1 in the game of Figure 1 (Left) are shown in Figure 1 (Right). The polytope of sequence-form strategies possesses a strong combinatorial structure that enables speeding up several common optimization procedures and will be crucial in developing efficient algorithms to converge to equilibrium.

### Hindsight Rationality and Learning in Games

Games are one of many situations in which a decision-maker has to act in an online manner. For these situations, the most widely used protocol is that of Online Learning (e.g., see Orabona (2022)). Specifically, each learner has a set of actions or behavior they can employ \(\mathcal{X}\subseteq\mathbb{R}^{d}\) (in extensive-form games, this would typically be the set of reduced-normal-form strategies). At each timestep \(t\) the learner first selects, possibly at random, an element \(\boldsymbol{x}\in\mathcal{X}\), and then receives a loss (opposite of utility) function \(\ell^{(t)}:\mathcal{X}\mapsto\mathbb{R}\). Since as we observed the above utilities in EFGs are linear in each player's reduced-normal-form plans, for the rest of the paper we focus on the case in which the loss function \(\ell^{(t)}\) is linear, that is, of the form \(\ell^{(t)}:\boldsymbol{x}\mapsto\langle\boldsymbol{\ell}^{(t)},\boldsymbol{ x}^{(t)}\rangle\).

A widely adopted objective for the learner is that of ensuring vanishing average _regret_ with high probability. Regret is defined as the difference between the loss the learner cumulated through their choice of behavior, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. In particular, let \(\Phi\) be a desired set of strategy transformations \(\phi:\mathcal{X}\rightarrow\mathcal{X}\) that the learner might want to learn not to regret. Then, the learner's \(\Phi\)-regret is defined as the quantity

\[\Phi\text{-Reg}^{(T)}\coloneqq\max_{\phi\in\Phi}\sum_{t=1}^{T}\Big{(}\langle \boldsymbol{\ell}^{(t)},\boldsymbol{x}^{(t)}\rangle-\langle\boldsymbol{\ell }^{(t)},\phi(\boldsymbol{x}^{(t)})\rangle\Big{)}\]

A _no-\(\Phi\)-regret algorithm_ (also known as a \(\Phi\)-regret minimizer) is one that, at all times \(T\), guarantees with high probability that \(\Phi\text{-Reg}^{(T)}=o(T)\) no matter what is the sequence of losses revealed by the environment. The size of the set \(\Phi\) of strategy transformations defines a natural measure of rationality (sometimes called _hindsight rationality_) for players, and several choices have been discussed in the literature. Clearly, as \(\Phi\) gets larger, the learner becomes more rational. On the flip side, guaranteeing sublinear regret with respect to all transformations in the chosen set \(\Phi\) might be intractable in general. On one end of the spectrum, perhaps the smallest meaningful choice of \(\Phi\) is the set of all _constant_ transformations \(\Phi^{\text{const}}=\{\phi_{\hat{\boldsymbol{x}}}:\boldsymbol{x}\mapsto\hat{ \boldsymbol{x}}\}_{\hat{\boldsymbol{x}}\in\mathcal{X}}\). In this case, \(\Phi^{\text{const}}\)-regret is also called _external regret_ and has been extensively studied in the field of online convex optimization. On the other end of the spectrum, _swap regret_ corresponds to the setting in which \(\Phi\) is the set of _all_ transformations \(\mathcal{X}\rightarrow\mathcal{X}\). Intermediate, and of central importance in this paper, is the notion of _linear-swap regret_, which corresponds to the case in which

\[\Phi\coloneqq\{\boldsymbol{x}\mapsto\mathbf{A}\boldsymbol{x}:\mathbf{A}\in \mathbb{R}^{d\times d},\text{ with }\mathbf{A}\boldsymbol{x}\in\mathcal{X}\quad\forall\, \boldsymbol{x}\in\mathcal{X}\}\qquad\qquad\text{(linear-swap deviations)}\]

is the set of all linear transformations from \(\mathcal{X}\) to itself.1

Footnote 1: For the purposes of this paper, the adjective _linear_ refers to the fact that each transformation can be expressed in the form \(\boldsymbol{x}\mapsto\mathbf{A}\boldsymbol{x}\) for an appropriate matrix \(\mathbf{A}\).

An important observation is that when all considered deviation functions in \(\Phi\) are linear, an algorithm guaranteeing sublinear \(\Phi\)-regret for the set \(\mathcal{X}\) can be constructed immediately from a deterministic no-\(\Phi\)-regret algorithm for \(\mathcal{X}^{\prime}=\Delta(\mathcal{X})\) by sampling \(\mathcal{X}\ni\bm{x}\) from any \(\bm{x}^{\prime}\in\mathcal{X}^{\prime}\) so as to guarantee that \(\mathbb{E}[\bm{x}]=\bm{x}^{\prime}\). Since this is exactly the setting we study in this paper, this folklore observation (see also Farina et al. (2022)) enables us to focus on the following problem: does a deterministic no-\(\Phi\)-regret algorithm for the set of sequence-form strategies \(\mathcal{X}=\mathcal{Q}_{i}\) of any player in an extensive-form game, with guaranteed sublinear \(\Phi\)-regret in the worst case, exist? In this paper we answer the question for the positive.

From regret to equilibriumThe setting of Learning in Games refers to the situation in which all players employ a learning algorithm, receiving as loss the negative of the gradient of their own utility evaluated in the strategies output by all the other players. A fascinating aspect of no-\(\Phi\)-regret learning dynamics is that if each player of a game employs a no-\(\Phi\)-regret algorithm, then the empirical frequency of play converges almost surely to the set of \(\Phi\)-equilibria, which are notions of correlated equilibria, in which the rationality of players is bounded by the size of the set \(\Phi\). Formally, for a set \(\Phi\) of strategy deviations, a \(\Phi\)-equilibrium is defined as follows.

**Definition 2.3**.: _For a \(n\)-player extensive-form game \(G\) and a set \(\Phi_{i}\) of deviations for each player, \(a\)\(\{\Phi_{i}\}\)-equilibrium is a joint distribution \(\mu\in\Delta(\Pi_{1}\times\cdots\times\Pi_{n})\) such that for each player \(i\), and every deviation \(\phi\in\Phi_{i}\) it holds that_

\[\mathbb{E}_{\bm{x}\sim\mu}[u_{i}(\bm{x})]\geq\mathbb{E}_{\bm{x}\sim\mu}[u_{i} (\phi(\bm{x}_{i}),\bm{x}_{-i})]\]

_That is, no player \(i\) has an incentive to unilaterally deviate from the recommended joint strategy \(\bm{x}\) using any transformation \(\phi\in\Phi_{i}\)._

This general framework captures several important notions of equilibrium across a variety of game theoretic models. For example, in both NFGs and EFGs, no-external regret dynamics converge to the set of Coarse Correlated Equilibria. In NFGs, no-swap regret dynamics converge to the set of Correlated Equilibria (Blum and Mansour, 2007). In EFGs, Farina et al. (2022) recently proved that a specific subset \(\Phi\) of linear transformations called _trigger deviations_ lead to the set of EFCE.

Reducing \(\Phi\)-regret to external regretAn elegant construction by Gordon et al. (2008) enables constructing no-\(\Phi\)-regret algorithms for a generic set \(\mathcal{X}\) starting from a no-external-regret algorithm for \(\Phi\). We briefly recall the result.

**Theorem 2.4** (Gordon et al. (2008)).: _Let \(\mathcal{R}\) be an external regret minimizer having the set of transformations \(\Phi\) as its action space, and achieving sublinear external regret \(\text{Reg}^{(T)}\). Additionally, assume that for all \(\phi\in\Phi\) there exists a fixed point \(\phi(\bm{x})=\bm{x}\in\mathcal{X}\). Then, \(a\)\(\Phi\)-regret minimizer \(\mathcal{R}_{\Phi}\) can be constructed as follows:_

* _To output a strategy_ \(\bm{x}^{(t)}\) _at iteration_ \(t\) _of_ \(\mathcal{R}_{\Phi}\)_, obtain an output_ \(\phi^{(t)}\in\Phi\) _of the external regret minimizer_ \(\mathcal{R}\)_, and return one of its fixed points_ \(\bm{x}^{(t)}=\phi^{(t)}(\bm{x}^{(t)})\)_._
* _For every linear loss function_ \(\ell^{(t)}\) _received by_ \(\mathcal{R}_{\Phi}\)_, construct the linear function_ \(L^{(t)}:\phi\mapsto\ell^{(t)}(\phi(\bm{x}^{(t)}))\) _and pass it as loss to_ \(\mathcal{R}\)_._

_Let \(\Phi\text{-}\mathrm{Reg}^{(T)}\) be the \(\Phi\)-regret of \(\mathcal{R}_{\Phi}\). Under the previous construction, it holds that_

\[\Phi\text{-}\mathrm{Reg}^{(T)}=\mathrm{Reg}^{(T)}\qquad\forall\,T=1,2,\ldots\]

_Thus, if \(\mathcal{R}\) is an external regret minimizer then \(\mathcal{R}_{\Phi}\) is a \(\Phi\)-regret minimizer._

## 3 A No-Linear-Swap Regret Algorithm with Polynomial-Time Iterations

In this section, we describe our no-linear-swap-regret algorithm for the set of sequence-form strategies \(\mathcal{Q}\) of a generic player in any perfect-recall imperfect-information extensive-form game. The algorithm follows the general template for constructing \(\Phi\)-regret minimizers given by Gordon et al. (2008) and recalled in Theorem 2.4. For this we need two components:

1. an efficient external regret minimizer for the set \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) of all matrices inducing linear transformations from \(\mathcal{Q}\) to \(\mathcal{Q}\),
2. an efficiently computable fixed point oracle for matrices \(\mathbf{A}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\), returning \(\bm{x}=\mathbf{A}\bm{x}\in\mathcal{Q}\).

The existence of a fixed point, required in ii), is easy to establish by Brouwer's fixed point theorem, since the polytope of sequence-form strategies is compact and convex, and the continuous function \(\bm{x}\mapsto\mathbf{A}\bm{x}\) maps \(\mathcal{Q}\) to itself by definition. Furthermore, as it will become apparent later in the section, all elements \(\mathbf{A}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) have entries in \([0,1]^{\Sigma\times\Sigma}\). Hence, requirement ii) can be satisfied directly by solving the linear feasibility program \(\{\text{find }\bm{x}:\mathbf{A}\bm{x}=\bm{x},\bm{x}\in\mathcal{Q}\}\) using any of the known polynomial-time algorithms for linear programming. Establishing requirement i) is where the heart of the matter is, and it is the focus of much of the paper. Here, we give intuition for the main insights that contribute to the algorithm. All proofs are deferred to the appendix.

### The Structure of Linear Transformations of Sequence-Form Strategy Polytopes

The crucial step in our construction is to establish a series of results shedding light on the fundamental geometry of the set \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) of _all_ linear transformations from a sequence-form polytope \(\mathcal{Q}\) to itself. In fact, our results extend beyond functions from \(\mathcal{Q}\) to \(\mathcal{Q}\) to more general functions from \(\mathcal{Q}\) to a generic compact polytope \(\mathcal{P}\coloneqq\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}=\bm{p},\bm{x} \geq\bm{0}\}\) for arbitrary \(\mathbf{P}\) and \(\bm{p}\). We establish the following characterization theorem, which shows that when the functions are expressed in matrix form, the set \(\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\) can be captured by a polynomial number of constraints. The proof is deferred to Appendix B.

**Theorem 3.1**.: _Let \(\mathcal{Q}\) be a sequence-form strategy space and let \(\mathcal{P}\) be any bounded polytope of the form \(\mathcal{P}\coloneqq\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}=\bm{p},\bm{x} \geq\bm{0}\}\subseteq[0,\gamma]^{d}\), where \(\mathbf{P}\in\mathbb{R}^{k\times d}\). Then, for any linear function \(f:\mathcal{Q}\to\mathcal{P}\), there exists a matrix \(\mathbf{A}\) in the polytope_

\[\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\coloneqq\left\{\begin{array}{rcl}(3 )&\mathbf{PA}_{(ja)}=\bm{b}_{j}&\forall\,ja\in\Sigma^{\perp}\\ (4)&\mathbf{A}_{(\sigma)}=\bm{0}&\forall\,\sigma\in\Sigma\setminus\Sigma^{ \perp}\\ (5)&\sum_{j^{\prime}\in\mathcal{C}_{\bm{a}}}\bm{b}_{j^{\prime}}=\bm{p}\\ (6)&\sum_{j^{\prime}\in\mathcal{C}_{ja}}\bm{b}_{j^{\prime}}=\bm{b}_{j}&\forall \,ja\in\Sigma\setminus\Sigma^{\perp}\\ (7)&\mathbf{A}_{(\sigma)}\in[0,\gamma]^{d}&\forall\,\sigma\in\Sigma\\ (8)&\bm{b}_{j}\in\mathbb{R}^{k}&\forall\,j\in\mathcal{J}\end{array}\right\}\]

_such that \(f(\bm{x})=\mathbf{A}\bm{x}\) for all \(\bm{x}\in\mathcal{Q}\). Conversely, any \(\mathbf{A}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\) defines a linear function \(\bm{x}\mapsto\mathbf{A}\bm{x}\) from \(\mathcal{Q}\) to \(\mathcal{P}\), that is, such that \(\mathbf{A}\bm{x}\in\mathcal{P}\) for all \(\bm{x}\in\mathcal{Q}\)._

The proof operates by induction in several steps. At its core, it exploits the combinatorial structure of sequence-form strategy polytopes, which can be decomposed into sub-problems using a series of Cartesian products and convex hulls. A high-level description of the induction is as follows:

* The _Base Case_ corresponds to the case of being at a leaf decision point. In this case, the set of deviations corresponds to all linear transformations from a probability \(n\)-simplex into a given polytope \(\mathcal{P}\). This set is equivalent to all \(d\times n\) matrices whose columns are points in \(\mathcal{P}\), which can easily be verified formally. This corresponds to constraint (3) in the characterization of \(\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\).
* For the _Inductive Step_, we are at an intermediate decision point \(j\), that is, one for which at least one action leads to further decision points.
* In Lemma B.8 we show that any terminal action \(a\) at \(j\) leads to a column in the transformation matrix that is necessarily a valid point in the polytope \(\mathcal{P}\). This is similar to the base case, and again leads to constraint (3) in the characterization.
* In Lemma B.7, we look at the other case of a non-terminal action \(a\) at \(j\). We prove that there always exists an equivalent transformation matrix whose column corresponding to sequence \(ja\) is identically \(0\) (constraint (4)). This allows for the "crux" of the transformation to happen in the subtrees below \(ja\) or equivalently, the subtrees rooted at the children decision points \(\mathcal{C}_{ja}\) of \(ja\).
* A key difficulty to conclude the proof is in using the assumption of the inductive step to characterize all such valid transformations. The set of strategies in the subtrees rooted at the children decision points \(\mathcal{C}_{ja}\) is in general the Cartesian product of the strategies in these subtrees. This explains the need for the fairly technical Proposition B.4, whose goal is to precisely characterize valid transformations of Cartesian products. This leads to constraints (5) and (6) in our final characterization.
We also remark that while the theorem calls for the polytope \(\mathcal{P}\) to be in the form \(\mathcal{P}=\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}=\bm{p},\bm{x}\geq\bm{0}\}\), with little work the result can also be extended to handle other representations such as \(\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}\leq\bm{p}\}\). We opted for the form specified in the theorem since it most directly leads to the proof, and since the constraints that define the sequence-form strategy polytope (Definition 2.2) are already in the form of the statement.

In particular, by setting \(\mathcal{P}=\mathcal{Q}\) in Theorem 3.1 (in this case, the dimensions of \(\mathbf{P}\) will be \(k=|\mathcal{J}|+1\), and \(d=|\Sigma|\)), we conclude that the set of linear functions from \(\mathcal{Q}\) to itself is a compact and convex polytope \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\subseteq[0,1]^{\Sigma\times\Sigma}\), defined by \(O(|\Sigma|^{2})\) linear constraints. As discussed, this polynomial characterization of \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) is the fundamental insight that enables polynomial-time minimization of linear-swap regret in general extensive-form games.

### Our No-Linear-Swap Regret Algorithm

From here, constructing a no-external-regret algorithm for \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) is relatively straightforward, using standard tools from the rich literature of online learning. For example, in Algorithm 1, we propose a solution employing online projected gradient descent (Gordon, 1999; Zinkevich, 2003).

``` Data:\(\mathbf{A}^{(1)}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) and fixed point \(\bm{x}^{(1)}\) of \(\mathbf{A}^{(1)}\), learning rates \(\eta^{(t)}>0\)
1for\(t=1,2,\dots\)do
2 Output \(\bm{x}^{(t)}\)
3 Receive \(\bm{\ell}^{(t)}\) and pay \(\langle\bm{\ell}^{(t)},\bm{x}^{(t)}\rangle\)
4 Set \(\mathbf{L}^{(t)}=\bm{\ell}^{(t)}(\bm{x}^{(t)})^{\top}\)
5\(\mathbf{A}^{(t+1)}=\Pi_{\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}}(\mathbf{A}^{ (t)}-\eta^{(t)}\mathbf{L}^{(t)})=\operatorname*{arg\,min}_{\mathbf{Y}\in \mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}}\|\mathbf{A}^{(t)}-\eta^{(t)}\mathbf{L }^{(t)}-\mathbf{Y}\|_{F}^{2}\)
6 Compute a fixed point \(\bm{x}^{(t+1)}=\mathbf{A}^{(t+1)}\bm{x}^{(t+1)}\in\mathcal{Q}\) of matrix \(\mathbf{A}^{(t+1)}\) ```

**Algorithm 1**\(\Phi\)-Regret minimizer for the set \(\Phi=\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\)

Combining that no-external-regret algorithm for \(\Phi\) with the construction by Gordon et al. (2008), we can then establish the following linear-swap regret and iteration complexity bounds for Algorithm 1.

**Theorem 3.2** (Informal).: _Let \(\Sigma\) denote the set of sequences of the learning player in the extensive-form game, and let \(\eta^{(t)}=1/\sqrt{t}\) for all \(t\). Then, for any sequence of loss vectors \(\bm{\ell}^{(t)}\in[0,1]^{\Sigma}\), Algorithm 1 guarantees linear-swap regret \(O(|\Sigma|^{2}\sqrt{T})\) after any number \(T\) of iterations, and runs in \(O(\operatorname{poly}(|\Sigma|)\log^{2}t)\) time for each iteration \(t\)._

The formal version of the theorem is given in Theorem D.1. It is worth noting that the polynomial-sized description of \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) is crucial in establishing the polynomial running time of the algorithm, both in the projection step (5) and in the fixed point computation step (6). We also remark that the choice of online projected gradient descent combined with the ellipsoid method for projections were arbitrary and the useful properties of \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) are retained when using it with any efficient regret minimizer.

## 4 Linear-Deviation Correlated Equilibrium

As we discussed in the preliminaries, when all players in a game employ no-\(\Phi\)-regret learning algorithms, then the empirical frequency of play converges to the set of \(\Phi\)-equilibria almost surely. Similarly, when \(\Phi=\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) the players act based on "no-linear-swap regret" dynamics and converge to a notion of \(\Phi\)-equilibrium we call _linear-deviation correlated equilibrium_ (LCE). In this section we present some notable properties of the LCE. In particular, we discuss its relation to other already established equilibria, as well as the computational tractability of optimal equilibrium selection.

### Relation to CE and EFCE

The \(\Phi\)-regret minimization framework, offers a natural way to build a hierarchy of the corresponding \(\Phi\)-equilibria based on the relationship of the \(\Phi\) sets of deviations. In particular, if for the sets \(\Phi_{1},\Phi_{2}\) it holds that \(\Phi_{1}\subseteq\Phi_{2}\), then the set of \(\Phi_{2}\)-equilibria is a subset of the set of \(\Phi_{1}\)-equilibria. Sincethe Correlated Equilibrium is defined using the set of all swap deviations, we conclude that any \(\Phi\)-equilibrium, including the LCE, is a superset of CE. What is the relationship then of LCE with the extensive-form correlated equilibrium (EFCE)? Farina et al. (2022) showed that the set \(\Phi^{\text{EFCE}}\) inducing EFCE is the set of all "trigger deviations", which can be expressed as linear transformations of extensive-form strategies. Consequently, the set \(\Phi^{\text{EFCE}}\) is a subset of all linear transformations and thus, it holds that \(\text{CE}\subseteq\text{LCE}\subseteq\text{EFCE}\). In examples E.1 and E.3 of the appendix we show that there exist specific games in which either \(\text{CE}\neq\text{LCE}\), or \(\text{LCE}\neq\text{EFCE}\). Hence, we conclude that the previous inclusions are strict and it holds \(\text{CE}\subsetneq\text{LCE}\subsetneq\text{EFCE}\).

For Example E.1 we use a signaling game from von Stengel and Forges (2008) with a known EFCE and we identify a linear transformation that is not captured by the trigger deviations of EFCE. Specifically, it is possible to perform linear transformations on sequences of a subtree based on the strategies on other subtrees of the TFSDP. For Example E.3 we have found a specific game through computational search that has a LCE which is not a normal-form correlated equilibrium. To do that we identify a particular normal-form swap that is non-linear.

Empirical evaluationTo further illustrate the separation between no-linear-swap-regret dynamics and no-trigger-regret dynamics, used for EFCE, we provide experimental evidence that minimizing linear-swap-regret also minimizes trigger-regret (Figure 2, left), while minimizing trigger-regret does _not_ minimize linear-swap regret. Specifically, in Figure 2 we compare our no-linear-swap-regret learning dynamics (given in Algorithm 1) to the no-trigger-regret algorithm introduced by Farina et al. (2022). More details about the implementation of the algorithms is available in Appendix F. In the left plot, we measure on the y-axis the average trigger regret incurred when all players use one or the other dynamics. Since trigger deviations are special cases of linear deviations, as expected, we observe that both dynamics are able to minimize trigger regret. Conversely, in the right plot of Figure 2, the y-axis measures linear-swap-regret. We observe that while our dynamics validate the sublinear regret performance proven in Theorem 3.2, the no-trigger-regret dynamics of Farina et al. (2022) exhibit an erratic behavior that is hardly compatible with a vanishing average regret. This suggests that no-linear-swap-regret is indeed a strictly stronger notion of hindsight rationality.

### Hardness of Maximizing Social Welfare

In many cases we are interested in knowing whether it is possible to select an Equilibrium with maximum Social Welfare. Let MAXPAY-LCE be the problem of finding an LCE in EFGs that maximizes the sum (or any linear combination) of all player's utilities. Below, we prove that we cannot efficiently solve MAXPAY-LCE, unless P=NP, even for 2 players if chance moves are allowed, and even for 3 players otherwise. We follow the structure of the same hardness proof for the problem MAXPAY-CE of finding an optimal CE in EFGs. Specifically, von Stengel and Forges (2008) use a reduction from SAT to prove that deciding whether MAXPAY-CE can attain the maximum value is NP-hard even for 2 players. To do that, they devise a way to map any SAT instance into a polynomially large game tree in which the root is the chance player, the second level corresponds to one player, and

Figure 2: (Left) Average trigger regret per iteration for both a linear-swap-regret minimizer and a trigger-regret minimizer. (Right) Average linear-swap regret per iteration for the same two minimizers.

the third level corresponds to the other player. The utilities for both players are exactly the same, thus the players will have to coordinate to maximize their payoff irrespective of the linear combination of utilities we aim to maximize.

**Theorem 4.1**.: _For two-player, perfect-recall extensive-form games with chance moves, the problem MAXPAY-LCE is not solvable in polynomial time, unless P=NP._

**Remark 4.2**.: _The problem retains its hardness if we remove the chance node and add a third player instead. As showed in von Stengel and Forges (2008), in that case we can always build a polynomially-sized game tree that forces the third player to act as a chance node._

## 5 Conclusions and Future Work

In this paper we have shown the existence of uncoupled no-linear-swap regret dynamics with polynomial-time iteration complexity in the game tree size in any extensive-form game. This significantly extends prior results related to extensive-form correlated equilibria, and begets learning agents that learn not to regret a significantly larger set of strategy transformations than what was known to be possible before. A crucial technical contribution we made to establish our result, and which might be of independent interest, is providing a polynomial characterization of the set of all linear transformations from a sequence-form strategy polytope to itself. Specifically, we showed that such a set of transformations can be expressed as a convex polytope with a polynomial number of linear constraints, by leveraging the rich combinatorial structure of the sequence-form strategies. Moreover, these no-linear-swap regret dynamics converge to linear-deviation correlated equilibria in extensive-form games, which are a novel type of equilibria that lies strictly between normal-form and extensive-form correlated equilibria.

These new results leave open a few interesting future research directions. Even though we know that there exist polynomial-time uncoupled dynamics converging to linear-deviation correlated equilibrium, we conjecture that it is also possible to obtain an efficient centralized algorithm similar to the Ellipsoid Against Hope for computing EFCE in extensive-form games by Huang and von Stengel (2008). Additionally, it is an intriguing question to understand whether a no-linear-swap regret algorithm exists that achieves \(O(\log T)\) regret per-player, as is the case for no-trigger regret (Anagnostides et al., 2023). Furthermore, it would be interesting to further explore problems of equilibrium selection related to LCE, possibly by devising suitable Fixed-Parameter Algorithms in the spirit of Zhang et al. (2022). Finally, the problem of understanding what is the most hindsight rational type of deviations based on which we can construct _efficient_ regret minimizers in extensive-form games remains a major open question.

## References

* Moravcik et al. (2017) Matej Moravcik, Martin Schmid, Neil Burch, Viliam Lisy, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. _Science_, 356(6337):pp. 508-513, 2017.
* Brown and Sandholm (2018) Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. _Science_, 359(6374):418-424, 2018.
* Brown and Sandholm (2019) Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. _Science_, 365(6456):885-890, 2019. doi: 10.1126/science.aay2400.
* Perolat et al. (2022) Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, and Karl Tuyls. Mastering the game of Stratego with model-free multiagent reinforcement learning. _Science_, 378(6623):990-996, 2022.
* Bakhtin et al. (2023) Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, and Noam Brown. Mastering the game of no-press Diplomacy via human-regularized reinforcement learning and planning. In _International Conference on Learning Representations (ICLR)_, 2023.
* Bakhtin et al. (2018)Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. _Games and Economic Behavior_, 21(1):40-55, 1997.
* Fudenberg and Levine (1995) Drew Fudenberg and David K Levine. Consistency and cautious fictitious play. _Journal of Economic Dynamics and Control_, 19(5-7):1065-1089, 1995.
* Fudenberg and Levine (1999) Drew Fudenberg and David K Levine. Conditional universal consistency. _Games and Economic Behavior_, 29(1-2):104-130, 1999.
* Hart and Mas-Colell (2000) Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. _Econometrica_, 68(5):1127-1150, 2000.
* Hart and Mas-Colell (2001) Sergiu Hart and Andreu Mas-Colell. A general class of adaptive strategies. _Journal of Economic Theory_, 98(1):26-54, 2001.
* Fudenberg and Levine (1998) Drew Fudenberg and David K Levine. _The theory of learning in games_, volume 2. MIT press, 1998.
* Farina et al. (2022) Gabriele Farina, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Simple uncoupled no-regret learning dynamics for extensive-form correlated equilibrium. _Journal of the ACM_, 69(6), 2022.
* Stoltz and Lugosi (2007) Gilles Stoltz and Gabor Lugosi. Learning correlated equilibria in games with compact sets of strategies. _Games and Economic Behavior_, 59(1):187-208, 2007.
* Blum and Mansour (2007) Avrim Blum and Yishay Mansour. From external to internal regret. _J. Mach. Learn. Res._, 8, 2007.
* Gordon et al. (2008) Geoffrey J Gordon, Amy Greenwald, and Casey Marks. No-regret learning in convex games. In _International Conference on Machine learning_, pages 360-367, 2008.
* Kakade et al. (2003) Sham Kakade, Michael Kearns, John Langford, and Luis Ortiz. Correlated equilibria in graphical games. In _Proceedings of the 4th ACM Conference on Electronic Commerce_, pages 42-47, 2003.
* Zinkevich et al. (2008) Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In _Advances in Neural Information Processing Systems_, pages 1729-1736, 2008.
* Anagnostides et al. (2023) Ioannis Anagnostides, Gabriele Farina, and Tuomas Sandholm. Near-optimal \(\Phi\)-regret learning in extensive-form games. In _International Conference on Machine Learning (ICML)_, 2023.
* von Stengel and Forges (2008) B. von Stengel and F. Forges. Extensive-form correlated equilibrium: Definition and computational complexity. _Mathematics of Operations Research_, 33(4):1002-1022, 2008.
* Huang and von Stengel (2008) Wan Huang and Bernhard von Stengel. Computing an extensive-form correlated equilibrium in polynomial time. In _International Workshop on Internet and Network Economics_, pages 506-513. Springer, 2008.
* Celli et al. (2020) Andrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for extensive-form correlated equilibrium. In _Advances in Neural Information Processing Systems_, volume 33, 2020.
* Mansour et al. (2022) Yishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against Learners in Bayesian games. In Po-Ling Loh and Maxim Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 5221-5252. PMLR, 02-05 Jul 2022.
* Fujii (2023) Kaito Fujii. Bayes correlated equilibria and no-regret dynamics, 2023.
* Morrill et al. (2021) Dustin Morrill, Ryan D'Orazio, Marc Lanctot, James R. Wright, Michael Bowling, and Amy R. Greenwald. Efficient deviation types and learning for hindsight rationality in extensive-form games. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 7818-7828. PMLR, 2021.
* Romanovskii (1962) I. Romanovskii. Reduction of a Game with Complete Memory to a Matrix Game. _Soviet Mathematics_, 3, 1962.
* Riedler et al. (2015)Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. Efficient computation of equilibria for extensive two-person games. _Games and Economic Behavior_, 14(2):247-259, 1996.
* von Stengel [1996] Bernhard von Stengel. Efficient computation of behavior strategies. _Games and Economic Behavior_, 14(2):220-246, 1996.
* Orabona [2022] Francesco Orabona. A Modern Introduction to Online Learning, 2022.
* 40, July 1999.
* Zinkevich [2003] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the Twentieth International Conference on International Conference on Machine Learning_, ICML'03, page 928-935. AAAI Press, 2003.
* 15, 2022_, pages 1119-1120. ACM, 2022.
* Vishnoi [2021] Nisheeth K. Vishnoi. _Algorithms for Convex Optimization_. Cambridge University Press, 2021.
* Gartner and Matousek [2014] Bernd Gartner and Jiri Matousek. _Approximation Algorithms and Semidefinite Programming_. Springer Publishing Company, Incorporated, 2014.
* Optimization [2023] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.gurobi.com.
* Kuhn [1950] H. W. Kuhn. A Simplified Two-Person Poker. In H. W. Kuhn and A. W. Tucker, editors, _Contributions to the Theory of Games_, volume 1 of _Annals of Mathematics Studies, 24_, pages 97-103. Princeton University Press, Princeton, New Jersey, 1950.

## Appendix A Additional Extensive-Form Game Notation

In the proofs, we will make use of the following symbols and notation.

Furthermore, when the subscript referring to players can be inferred or is irrelevant (that is, the quantities refer to a generic player), then we might skip it.

As hinted by some of the rows in the above table, we will sometimes find it important to consider partial strategies that only specify behavior at a decision node \(j\) and all of its descendants \(j^{\prime}\succ j\). We make this formal through the following definition.

**Definition A.1**.: _The set of sequence-form strategies for the subtree rooted at \(j\), denoted \(\mathcal{Q}_{\succeq j}\), is the set of all vectors \(\bm{x}\in\mathbb{R}_{\geq 0}^{\Sigma_{\succeq j}}\) such that probability-mass-conservation constraints hold at decision node \(j\) and all of its descendants \(j^{\prime}\succ j\), specifically_

\[\mathcal{Q}_{\succeq j}\coloneqq\Bigg{\{}\bm{x}\in\mathbb{R}_{\geq 0}^{ \Sigma_{\succeq j}}:\begin{array}{ll}(9)&\sum_{a\in\mathcal{A}_{j}}\bm{x}[ ja]=1\\ (10)&\sum_{a\in\mathcal{A}_{j^{\prime}}}\bm{x}[j^{\prime}a]=\bm{x}[p_{j^{ \prime}}]\quad\forall\,j^{\prime}\succ j\end{array}\Bigg{\}}.\]

Finally, we define the symbol \(\Sigma_{i}^{\perp}\) to be the set of all terminal sequences for Player \(i\). Thus, the set \(\Sigma_{i}\setminus\Sigma_{i}^{\perp}\) would give us all the non-terminal sequences of that player.

Access to coordinatesBy definition, sequence-form strategies are vectors indexed by sequences. To access the coordinate corresponding to sequence \(\sigma\), we will use the notation \(\bm{x}[\sigma]\). Occasionally, we will need to extract a subvector corresponding to all sequences that are successor of an information set \(j\), that is, all sequences \(\sigma\succeq j\). For that, we use the notation \(\bm{x}[\succeq j]\).

Remark on the structure of sequence-form strategiesWe further remark the following known fact about the structure of sequence-form strategies. Intuitively, it crystallizes the idea that sequence-form strategies encode product of probabilities of actions on the path from the root to any decision point. The proof follows directly from the definitions.

\begin{table}
\begin{tabular}{c l} \hline \hline
**Symbol** & **Description** \\ \hline \(\mathcal{J}_{i}\) & Set of all Player \(i\)’s infosets. \\ \(\mathcal{A}_{j}\) & Set of actions available at any node in the information set \(j\). \\ \(\Sigma_{i}\) & Set of sequences for Player \(i\), defined as \(\Sigma^{(i)}\coloneqq\{(j,a):j\in\mathcal{J},a\in A_{j}\}\cup\{\varnothing\}\), \\ \(\varnothing\) & where the special element \(\varnothing\) is called the _empty sequence_. \\ \(\Sigma_{i}^{\perp}\) & Set of terminal sequences for Player \(i\). \\ \hline \(p_{j}\) & Parent sequence of \(j\), defined as the last pair \((j,a)\in\Sigma_{i}\) encountered on the path from the root to any information set \(j\). \\ \(\mathcal{C}_{\sigma}\) & Set of all “children” of sequence \(\sigma\), defined as the information sets \(j\in\mathcal{J}\) having as parent \(p_{j}=\sigma\). \\ \(j^{\prime}\prec j\) & Information set \(j\in\mathcal{J}\) is an ancestor of \(j^{\prime}\in\mathcal{J}\), that is, there exists a path in the game tree connecting a node \(h\in j\) to some node \(h^{\prime}\in j^{\prime}\). \\ \(\sigma\prec\sigma^{\prime}\) & Sequence \(\sigma\) precedes sequence \(\sigma^{\prime}\), where \(\sigma,\sigma^{\prime}\) belong to the same player. \\ \(\sigma\succeq j\) & Sequence \(\sigma=(j^{\prime},a^{\prime})\) is such that \(j^{\prime}\succeq j\). \\ \(\Sigma_{\succeq j}\) & Sequences at \(j\in\mathcal{J}\) and all of its descendants, \(\Sigma_{\succeq j}\coloneqq\{\sigma\in\Sigma:\sigma\succeq j\}\). \\ \hline \(\mathcal{Q}_{i}\) & Sequence-form strategies of Player \(i\) (Definition 2.2). \\ \(\mathcal{Q}_{\succeq j}\) & Sequence-form strategies for the subtree rooted at \(j\in\mathcal{J}\) (Definition A.1). \\ \(\bar{\Pi}_{i}\) & Reduced-normal-form plans (a.k.a. deterministic sequence-form strategies) of Player \(i\). \\ \(\Pi_{\succeq j}\) & Reduced-normal-form plans (a.k.a. deterministic sequence-form strategies) for the subtree rooted at \(j\in\mathcal{J}\). \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of game-theoretic notation used in this paper. Note that we might skip player-specific subscripts when they can be inferred.

**Lemma A.2**.: _Let \(j\in\mathcal{J}_{i}\) be an information set for a generic player. Then, given any sequence-form strategy \(\bm{x}\in\mathcal{Q}_{\succeq j}\), action \(a\in\mathcal{A}_{j}\), and child information set \(j^{\prime}\in\mathcal{C}_{ja}\), there exists a sequence-form strategy \(\bm{x}_{\succeq j^{\prime}}\in\mathcal{Q}_{\succeq j^{\prime}}\) such that_

\[\bm{x}[\succeq j^{\prime}]=\bm{x}[ja]\bm{x}_{\succeq j^{\prime}}.\]

## Appendix B Proof of the Characterization Theorem (Theorem 3.1)

In this section we prove the central result of this paper, the characterization given in Theorem 3.1 of linear functions from the sequence-form strategy polytope \(\mathcal{Q}\) to the generic polytope \(\mathcal{P}\coloneqq\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}=\bm{p},\bm{x} \geq\bm{0}\}\), where \(\mathbf{P}\in\mathbb{R}^{k\times d}\) and \(\bm{p}\in\mathbb{R}^{k}\).

We will prove the characterization theorem by induction on the structure of the extensive-form strategy polytope. To do so, it will be useful to introduce a few additional objects and notations. We do so in the next subsection.

### Additional Objects and Notation Used in the Proof

First, we introduce a parametric version of the polytope \(\mathcal{P}\), where the right-hand side vector is made variable.

**Definition B.1**.: _Given any \(\bm{b}\in\mathbb{R}^{k}\), we will denote with \(\mathcal{P}(\bm{b})\) the polytope_

\[\mathcal{P}(\bm{b})\coloneqq\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}=\bm{b},\bm{x}\geq\bm{0}\}.\]

_In particular, \(\mathcal{P}=\mathcal{P}(\bm{p})\)._

Furthermore, we introduce the equivalence relation \(\cong_{\mathcal{D}}\) to indicate that two matrices induce the same linear function when restricted to domain \(\mathcal{D}\).

**Definition B.2**.: _Given two matrices \(\mathbf{A},\mathbf{B}\) of the same dimension, we write \(\mathbf{A}\cong_{\mathcal{D}}\mathbf{B}\) if \(\mathbf{A}\bm{x}=\mathbf{B}\bm{x}\) for all \(\bm{x}\in\mathcal{D}\). Similarly, given two sets \(\mathcal{U},\mathcal{V}\) of matrices we write \(\mathcal{U}\cong_{\mathcal{D}}\mathcal{V}\) to mean that for any \(\mathbf{A}\in\mathcal{U}\) there exists \(\mathbf{B}\in\mathcal{V}\) with \(\mathbf{A}\cong_{\mathcal{D}}\mathbf{B}\), and vice versa._

Additionally, we introduce a symbol to denote the set of all matrices that induce linear functions from a set \(\mathcal{U}\) to a set \(\mathcal{V}\).

**Definition B.3**.: _Given any sets \(\mathcal{U},\mathcal{V}\) we denote with \(\mathcal{L}_{\mathcal{U}\to\mathcal{V}}\) the set of all matrices that induce linear transformations from \(\mathcal{U}\) to \(\mathcal{V}\), that is,_

\[\mathcal{L}_{\mathcal{U}\to\mathcal{V}}\coloneqq\{\mathbf{A}:\mathbf{A}\bm{x} \in\mathcal{V}\text{ for all }\bm{x}\in\mathcal{U}\}.\]

Finally, we remark that for a matrix \(\mathbf{A}\) whose columns are indexed using sequences \(\sigma\in\Sigma\), we represent its columns as \(\mathbf{A}_{(\sigma)}\). Furthermore, for sequence-form strategies \(\bm{x}\in\mathcal{Q}\), we use \(\bm{x}[\sigma]\) to represent their entries, and \(\bm{x}[\Sigma_{\succeq j}]\) to represent a vector consisting only of the entries corresponding to sequences \(\sigma\in\Sigma_{\succeq j}\).

### A Key Tool: Linear Transformations of Cartesian Products

We are now ready to introduce the following Proposition, which will play an important role in the proof of Theorem 3.1.

**Proposition B.4**.: _Let \(\mathcal{U}_{1},\ldots,\mathcal{U}_{m}\) be sets, with \(\bm{0}\notin\operatorname{aff}\mathcal{U}_{i}\)2 for all \(i=1,\ldots,m\). Furthermore, for any \(i=1,\ldots,m\) and any \(\bm{b}_{i}\in\mathbb{R}^{k}\), let \(\mathcal{M}_{\mathcal{U}_{i}\to\mathcal{P}(\bm{b}_{i})}\) be such that \(\mathcal{M}_{\mathcal{U}_{i}\to\mathcal{P}(\bm{b}_{i})}\cong_{\mathcal{U}_{i}} \mathcal{L}_{\mathcal{U}_{i}\to\mathcal{P}(\bm{b}_{i})}\). Then, for all \(\bm{b}\in\mathbb{R}^{k}\),_

Footnote 2: Instead of the condition \(\bm{0}\notin\operatorname{aff}\mathcal{U}_{i}\), we could equivalently state that there exists \(\bm{\tau}_{i}\) such that \(\bm{\tau}_{i}^{\top}\bm{x}_{i}=1\) for all \(\bm{x}_{i}\in\mathcal{U}_{i}\) using the properties of affine sets.

\[\mathcal{L}_{(\mathcal{U}_{1}\times\cdots\times\mathcal{U}_{m})\to\mathcal{P} (\bm{b})}\cong_{(\mathcal{U}_{1}\times\cdots\times\mathcal{U}_{m})}\left\{ \begin{array}{ll}&(11)\;\;\mathbf{A}_{i}\in\mathcal{M}_{\mathcal{U}_{i}\to \mathcal{P}(\bm{b}_{i})}&\forall\,i\in\{1,\ldots,m\}\\ &(12)\;\;\bm{b}_{1}+\cdots+\bm{b}_{m}=\bm{b}&\\ &(13)\;\;\bm{b}_{i}\in\mathbb{R}^{k}&\forall\,i\in\{1,\ldots,m\}\end{array} \right\}.\] (14)

[MISSING_PAGE_FAIL:15]

### Characterization of Linear Functions of Subtrees

The following result can be understood as a version of Theorem 3.1 stated for each subtree, rooted at some decision node, of the decision space.

**Theorem B.6**.: _For any decision node \(j\in\mathcal{J}\) and vector \(\bm{b}_{j}\in\mathbb{R}^{k}\), let_

\[\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\coloneqq \left\{\underbrace{\begin{array}{cl}(15)&\mathbf{PA}_{(j^{\prime}a^{\prime}) }=\bm{b}_{j^{\prime}}&\forall\,j^{\prime}a^{\prime}\in\Sigma_{\geq j}\cap\Sigma ^{\perp}\\ \bm{\Lambda}_{(j^{\prime}a^{\prime})}=\bm{0}&\forall\,j^{\prime}a^{\prime} \in\Sigma_{\geq j}\setminus\Sigma^{\perp}\\ (16)&\sum_{j^{\prime\prime}\in\mathcal{C}_{j^{\prime}a^{\prime}}}\bm{b}_{j^{ \prime\prime}}=\bm{b}_{j^{\prime}}&\forall\,j^{\prime}a^{\prime}\in\Sigma_{ \geq j}\setminus\Sigma^{\perp}\\ (18)&\mathbf{A}_{(j^{\prime}a^{\prime})}\geq\bm{0}&\forall\,j^{\prime}a^{ \prime}\in\Sigma_{\geq j}\\ (19)&\bm{b}_{j^{\prime}}\in\mathbb{R}^{k}&\forall\,j^{\prime}\succ j\end{array} \right\}.\] (20)

_Then, \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\cong_{ \mathcal{Q}_{\geq j}}\mathcal{L}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j })}\)._

Before continuing with the proof, we remark a subtle point: unlike (7), which constraints each column to have entries in \([0,\gamma]\), (18) only specifies the lower bound at zero, but no upper bound. Hence the tilde above the symbol of this Theorem. Consequently, the matrices in the set \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\) need not have bounded entries. In that sense, Theorem B.6 is slightly different from Theorem 3.1. We will strengthen (18) to enforce a bound on each column when completing the proof of Theorem 3.1 in the next subsection.

Proof.: To aid us with the proof, we first express the definition of \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\) in a way that better captures the inductive structure we need. By direct inspection of the constraints, the set \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\) satisfies the inductive definition

\[\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}=\left\{ \begin{array}{cl}\mathbf{A}=\big{[}\,\cdots\,|\,\mathbf{A}_{(ja)}\,|\,\cdots \big{]}\in\,\mathbb{R}^{d\times\Sigma_{\geq j}},&\text{such that:}\\ (21)&\mathbf{PA}_{(ja)}=\bm{b}_{j}&\forall\,a\in\mathcal{A}_{j}:ja\in\Sigma_{ \geq j}\cap\Sigma^{\perp}\\ (22)&\mathbf{A}_{(ja)}=\bm{0}&\forall\,a\in\mathcal{A}_{j}:ja\notin\Sigma^{ \perp}\\ (23)&\sum_{j^{\prime}\in\mathcal{C}_{ja}}\bm{b}_{j^{\prime}}=\bm{b}_{j}&\forall \,a\in\mathcal{A}_{j}:ja\notin\Sigma^{\perp}\\ (24)&[\mathbf{A}_{(\sigma)}]_{\sigma\geq j^{\prime}}\in\tilde{\mathcal{M}}_{ \mathcal{Q}_{\geq j^{\prime}}\to\mathcal{P}(\bm{b}_{j^{\prime}})}&\forall\,a \in\mathcal{A}_{j},j^{\prime}\in\mathcal{C}_{ja}\\ (25)&\mathbf{A}_{(ja)}\geq\bm{0}&\forall\,a\in\mathcal{A}_{j}\\ (26)&\bm{b}_{j^{\prime}}\in\mathbb{R}^{k}&\forall\,a\in\mathcal{A}_{j},j^{ \prime}\in\mathcal{C}_{ja}\end{array}\right\}.\] (27)

We prove the result by structural induction on the tree-form decision process.

* **Base case.** We start by establishing the result for any terminal decision node \(j\in\mathcal{J}\), that is, one for which all sequences \(\{ja:a\in\mathcal{A}_{j}\}\) are terminal. In this case, the set \(\mathcal{Q}_{\geq j}\) is the probability simplex \(\Delta(\{ja:a\in\mathcal{A}_{j}\})\). Thus, for a matrix \(\mathbf{A}\) to map all \(\bm{x}\in\mathcal{Q}_{\geq j}\) to elements in the convex polytope \(\mathcal{P}(\bm{b}_{j})\) it is both necessary and sufficient that all columns of \(\mathbf{A}\) be elements of \(\mathcal{P}(\bm{b}_{j})\). It is necessary because if \(\mathbf{A}\bm{x}\in\mathcal{P}(\bm{b}_{j})\) for all \(\bm{x}\in\mathcal{Q}_{\geq j}\), then for the indicator vector \(\bm{x}\) with \(\bm{x}[ja]=1\) we get \(\mathbf{A}\bm{x}=\mathbf{A}_{(ja)}\in\mathcal{P}(\bm{b}_{j})\). And, it is sufficient because any \(\bm{x}\in\mathcal{Q}_{\geq j}\) represents a convex combination of the columns \(\mathbf{A}_{(ja)}\). The set defined by these constraints matches exactly the set \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\) defined in the statement: since all sequences \(ja\) are terminal, in this case it reduces to \[\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}=\left\{ \begin{array}{cl}\big{[}\,\cdots\,|\,\mathbf{A}_{(ja)}\,|\,\cdots\big{]}\in \mathbb{R}^{d\times\Sigma_{\geq j}}:&(28)&\mathbf{PA}_{(ja)}=\bm{b}_{j}&\forall \,a\in\mathcal{A}_{j}\\ (29)&\mathbf{A}_{(ja)}\geq\bm{0}&\forall\,a\in\mathcal{A}_{j}\end{array} \right\}\!,\] that is, the set of matrices whose columns are elements of \(\mathcal{P}(\bm{b}_{j})\). So, we have \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}=\mathcal{L}_ {\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\) with equality, which immediately implies the claim \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\cong_{ \Sigma_{\geq j}}\mathcal{L}_{\mathcal{Q}_{\geq j}\to\mathcal{P}(\bm{b}_{j})}\).

* **Inductive step.** We now look at a general decision node \(j\in\mathcal{J}\), assuming as inductive hypothesis that the claim holds for any \(j^{\prime}\succ j\). Below we prove that \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\succeq j}\rightarrow\mathcal{P}(\bm{b}_{j})} \cong_{\mathcal{Q}_{\succeq j}}\)\(\mathcal{L}_{\mathcal{Q}_{\succeq j}\rightarrow\mathcal{P}(\bm{b}_{j})}\) as well. (\(\subseteq\)) We start by showing that for any \(\bm{b}_{j}\in\mathbb{R}^{k}\), \(\bm{x}\in\Pi_{\succeq j}\) and \(\mathbf{A}\in\tilde{\mathcal{M}}_{\mathcal{Q}_{\succeq j}\rightarrow\mathcal{ P}(\bm{b}_{j})}\), we have \(\mathbf{A}\bm{x}\in\mathcal{P}(\bm{b}_{j})\). From (18) it is immediate that \(\mathbf{A}\) has nonnegative entries, and since any vector \(\bm{x}\in\Pi_{\succeq j}\) also has nonnegative entries, it follows that \(\mathbf{A}\bm{x}\geq\mathbf{0}\). Hence, it only remains to show that \(\mathbf{P}(\mathbf{A}\bm{x})=\bm{b}_{j}\). Using Lemma A.2, for any \(j^{\prime}\in\sqcup_{a\in\mathcal{A}_{j}}\mathcal{C}_{ja}\) there exists \(\bm{x}_{\succeq j^{\prime}}\in\mathcal{Q}_{\succeq j^{\prime}}\) such that \(\bm{x}[\Sigma_{\succeq j^{\prime}}]=\bm{x}[ja]\cdot\bm{x}_{\succeq j^{\prime}}\). Hence, we have \[\mathbf{P}(\mathbf{A}\bm{x}) =\sum_{a\in\mathcal{A}_{j}}\mathbf{PA}_{(ja)}\bm{x}[ja]+\sum_{ \begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\notin\Sigma^{\perp}\end{subarray}}\sum_{j^{\prime}\in\mathcal{C}_{ja}} \mathbf{PA}_{\succeq j^{\prime}}(\bm{x}[ja]\cdot\bm{x}_{\succeq j^{\prime}})\] \[=\sum_{\begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\in\Sigma^{\perp}\end{subarray}}\bm{x}[ja]\cdot\mathbf{PA}_{(ja)}+\sum_{ \begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\notin\Sigma^{\perp}\end{subarray}}\left(\bm{x}[ja]\sum_{j^{\prime}\in \mathcal{C}_{ja}}\mathbf{PA}_{\succeq j^{\prime}}\bm{x}_{\succeq j^{\prime}}\right)\] (from (22)) \[=\sum_{\begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\in\Sigma^{\perp}\end{subarray}}\bm{x}[ja]\cdot\bm{b}_{j}+\sum_{ \begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\notin\Sigma^{\perp}\end{subarray}}\left(\bm{x}[ja]\sum_{j^{\prime}\in \mathcal{C}_{ja}}\bm{b}_{j^{\prime}}\right)\] (from (21) and (24)) \[=\sum_{\begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\in\Sigma^{\perp}\end{subarray}}\bm{x}[ja]\cdot\bm{b}_{j}+\sum_{ \begin{subarray}{c}a\in\mathcal{A}_{j}\\ ja\notin\Sigma^{\perp}\end{subarray}}\bm{x}[ja]\cdot\bm{b}_{j}\] (from (23)) \[=\bm{b}_{j}\cdot\sum_{a\in\mathcal{A}_{j}}\bm{x}[ja]=\bm{b}_{j}.\] (from Definition 2.2)

(\(\supseteq\)) Conversely, consider any \(\mathbf{B}\in\mathcal{L}_{\mathcal{Q}_{\succeq j}\rightarrow\mathcal{P}(\bm{b }_{j})}\). We will show that there exists a matrix \(\mathbf{A}\in\tilde{\mathcal{M}}_{\mathcal{Q}_{\succeq j}\rightarrow\mathcal{ P}(\bm{b}_{j})}\) such that \(\mathbf{B}\cong_{\mathcal{Q}_{\succeq j}}\mathbf{A}\). First, we argue that there exists a matrix \(\mathbf{B}^{\prime}\cong_{\mathcal{Q}_{\succeq j}}\mathbf{B}\) with the property that the column \(\mathbf{B}^{\prime}_{(ja)}\) corresponding to any _nonterminal_ sequence is identically zero.

**Lemma B.7**.: _There exists \(\mathbf{B}^{\prime}\cong_{\mathcal{Q}_{\succeq j}}\mathbf{B}\) such that \(\mathbf{B}^{\prime}_{(ja)}=\mathbf{0}\) for all \(a\in\mathcal{A}_{j}\) such that \(ja\in\Sigma_{\succeq j}\setminus\Sigma^{\perp}\)._

Proof.: Fix any \(a\in\mathcal{A}_{j}\) such that \(ja\) is nonterminal. Then, by definition there exists at least one decision node \(j^{\prime}\) whose parent sequence is \(ja\). Consider now the matrix \(\mathbf{B}^{\prime\prime}\) obtained from "spreading" column \(\mathbf{B}_{(ja)}\) onto \(\mathbf{B}_{(j^{\prime}a^{\prime})}\) (\(a^{\prime}\in\mathcal{A}_{j^{\prime}}\)), that is, the matrix whose columns are defined according to the following rules: (i) \(\mathbf{B}^{\prime\prime}_{(ja)}=\mathbf{0}\), (ii) \(\mathbf{B}^{\prime\prime}_{(j^{\prime}a^{\prime})}=\mathbf{B}_{(j^{\prime}a^{ \prime})}+\mathbf{B}_{(ja)}\) for all \(a^{\prime}\in\mathcal{A}_{j^{\prime}}\), (iii) \(\mathbf{B}^{\prime\prime}_{(\sigma)}=\mathbf{B}_{(\sigma)}\) everywhere else. The column \(\mathbf{B}^{\prime\prime}_{(ja)}\) is identically zero by construction, and all other columns \(\mathbf{B}^{\prime\prime}_{(ja^{\prime})}\), \(a^{\prime}\in\mathcal{A}_{j}\setminus\{a\}\), are the same as \(\mathbf{B}\). Most importantly, since from the sequence-form constraints Definition 2.2 any sequence-form strategy \(\bm{x}\in\mathcal{Q}_{\succeq j}\) satisfies the equality \(\bm{x}[ja]=\sum_{a^{\prime}\in\mathcal{A}_{j^{\prime}}}\bm{x}[j^{\prime}a^{ \prime}]\), the matrix \(\mathbf{B}^{\prime\prime}\) satisfies \(\mathbf{B}^{\prime\prime}\bm{x}=\mathbf{B}\bm{x}\) for all \(\bm{x}\in\mathcal{Q}_{\succeq j}\), _i.e._, \(\mathbf{B}^{\prime\prime}\cong_{\mathcal{Q}_{\succeq j}}\mathbf{B}\). Iterating the argument for all actions \(a\in\mathcal{A}_{j}\) yields the statement. 

Consider now any \(a\in\mathcal{A}_{j}\) that leads to a terminal sequence \(ja\in\Sigma_{\succeq j}\cap\Sigma^{\perp}\). The vector \(\mathbf{1}_{ja}\) defined as having a \(1\) in the position corresponding to \(ja\) and \(0\) everywhere else is a valid sequence-form strategy vector, that is, \(\mathbf{1}_{ja}\in\mathcal{Q}_{\succeq j}\). Hence, since \(\mathbf{B}^{\prime}\) maps \(\mathcal{Q}_{\succeq j}\) to \(\mathcal{P}(\bm{b}_{j})\), it is necessary that \(\mathbf{B}^{\prime}_{(ja)}\in\mathcal{P}(\bm{b}_{j})\), that is, \(\mathbf{B}^{\prime}_{(ja)}\geq\mathbf{0}\) and \(\mathbf{PB}^{\prime}_{(ja)}=\bm{b}_{j}\). In other words, we have just proved the following.

**Lemma B.8**.: _For any \(a\in\mathcal{A}_{j}\) such that \(ja\in\Sigma_{\succeq j}\cap\Sigma^{\perp}\), \(\mathbf{B}^{\prime}_{(ja)}\geq\mathbf{0}\) and \(\mathbf{PB}^{\prime}_{(ja)}=\bm{b}_{j}\)._Combined, Lemmas B.7 and B.8 show that \(\mathbf{B}^{\prime}\) satisfies constraints (21), (22), and (25). Consider now any action \(a\in\mathcal{A}_{j}\) that defines a _nonterminal_ sequence \(ja\in\Sigma_{\succeq j}\setminus\Sigma^{\perp}\). For each child decision point \(j^{\prime}\in\mathcal{C}_{ja}\), let \(\bm{x}_{\succeq j^{\prime}}\in\mathcal{Q}_{\succeq j^{\prime}}\) be a choice of strategy for that decision point, and denote \(\mathbf{B}^{\prime}_{\succeq j^{\prime}}\) the submatrix of \(\mathbf{B}^{\prime}\) obtained by only considering the columns \(\mathbf{B}^{\prime}_{(\sigma)}\) corresponding to sequences \(\sigma\succeq j^{\prime}\). The vector \(\bm{x}\) defined according to \(\bm{x}[ja]=1\), \(\bm{x}[\Sigma_{\succeq j^{\prime}}]=\bm{x}_{\succeq j^{\prime}}\) for all \(j^{\prime}\in\mathcal{C}_{ja}\), and \(0\) everywhere else is a valid sequence-form strategy \(\bm{x}\in\mathcal{Q}_{\succeq j}\), and therefore \(\mathbf{B}^{\prime}\bm{x}\in\mathcal{P}(\bm{b}_{j})\) since \(\mathbf{B}^{\prime}\cong_{\mathcal{Q}_{\succeq j}}\mathbf{B}\) and \(\mathbf{B}\in\mathcal{L}_{\mathcal{Q}_{\succeq j}\to\mathcal{P}(\bm{b}_{j})}\) by hypothesis. Therefore, using the fact that \(\mathbf{B}^{\prime}_{(ja)}=\mathbf{0}\) by Lemma B.7, we conclude that

\[\mathcal{P}(\bm{b}_{j})\ni\mathbf{B}^{\prime}\bm{x}=\sum_{j^{\prime}\in \mathcal{C}_{ja}}\mathbf{B}^{\prime}_{\succeq j^{\prime}}\bm{x}_{\succeq j^{ \prime}}.\]

Because the above holds for any choice of \(\bm{x}_{\succeq j^{\prime}}\in\mathcal{Q}_{\succeq j^{\prime}}\), it follows that the matrix \([\cdots\mid\mathbf{B}^{\prime}_{\succeq j^{\prime}}\mid\cdots]\in\mathcal{L} _{\times_{j^{\prime}\in\mathcal{C}_{ja}}\mathcal{Q}_{\succeq j^{\prime}}\to \mathcal{P}(\bm{b}_{j})}\). Hence, applying Proposition B.4 (note that \(\mathbf{0}\notin\text{aff }\mathcal{Q}_{\succeq j^{\prime}}\text{ since }\sum_{a\in\mathcal{A}_{j^{\prime}}}\bm{x}[j^{\prime}a]=1\) for all \(\bm{x}\in\mathcal{Q}_{\succeq j^{\prime}}\) by Definition A.1) together with the inductive hypothesis, we conclude that for each \(j^{\prime}\in\mathcal{C}_{ja}\) there exist a vector \(\bm{b}_{j^{\prime}}\in\mathbb{R}^{k}\) and a matrix \(\mathbf{A}_{\succeq j^{\prime}}\in\mathcal{M}_{\mathcal{Q}_{\succeq j^{ \prime}}\to\mathcal{P}(\bm{b}_{j^{\prime}})}\), such that \(\sum_{j^{\prime}\in\mathcal{C}_{ja}}\bm{b}_{j^{\prime}}=\bm{b}_{j}\) and \([\cdots\mid\mathbf{B}^{\prime}_{\succeq j^{\prime}}\mid\cdots]\cong_{\times_{ j^{\prime}\in\mathcal{C}_{ja}}\mathcal{Q}_{\succeq j^{\prime}}}[\cdots\mid\mathbf{A}_{ \succeq j^{\prime}}\mid\cdots]\). We can therefore replace all columns corresponding to \(\mathbf{B}^{\prime}_{\succeq j^{\prime}}\) with those of \(\mathbf{A}_{\succeq j^{\prime}}\), obtaining a new matrix \(\cong_{\mathcal{Q}_{\succeq j}}\mathbf{B}^{\prime}\). Repeating the argument for each \(ja\in\Sigma_{\succeq j}\setminus\Sigma^{\perp}\) finally yields a new matrix that is \(\cong_{\Sigma_{\succeq j}}\mathbf{B}\) and satisfies all constraints given in (27), as we wanted to show. 

### Putting all the Pieces Together

Finally, we are ready to prove the main result of the paper.

**Theorem 3.1**.: _Let \(\mathcal{Q}\) be a sequence-form strategy space and let \(\mathcal{P}\) be any bounded polytope of the form \(\mathcal{P}\coloneqq\{\bm{x}\in\mathbb{R}^{d}:\mathbf{P}\bm{x}=\bm{p},\bm{x} \geq\mathbf{0}\}\subseteq[0,\gamma]^{d}\), where \(\mathbf{P}\in\mathbb{R}^{k\times d}\). Then, for any linear function \(f:\mathcal{Q}\to\mathcal{P}\), there exists a matrix \(\mathbf{A}\) in the polytope_

\[\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\coloneqq\left\{\begin{array}{llll}&(3 )&\mathbf{PA}_{(ja)}=\bm{b}_{j}&\forall\,ja\in\Sigma^{\perp}\\ &(4)&\mathbf{A}_{(\sigma)}=\mathbf{0}&\forall\,\sigma\in\Sigma\setminus \Sigma^{\perp}\\ &(5)&\sum_{j^{\prime}\in\mathcal{C}_{ja}}\bm{b}_{j^{\prime}}=\bm{p}&\\ &(6)&\sum_{j^{\prime}\in\mathcal{C}_{ja}}\bm{b}_{j^{\prime}}=\bm{b}_{j}&\forall \,ja\in\Sigma\setminus\Sigma^{\perp}\\ &(7)&\mathbf{A}_{(\sigma)}\in[0,\gamma]^{d}&\forall\,\sigma\in\Sigma\\ &(8)&\bm{b}_{j}\in\mathbb{R}^{k}&\forall\,j\in\mathcal{J}\end{array}\right\}\]

_such that \(f(\bm{x})=\mathbf{A}\bm{x}\) for all \(\bm{x}\in\mathcal{Q}\). Conversely, any \(\mathbf{A}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\) defines a linear function \(\bm{x}\mapsto\mathbf{A}\bm{x}\) from \(\mathcal{Q}\) to \(\mathcal{P}\), that is, such that \(\mathbf{A}\bm{x}\in\mathcal{P}\) for all \(\bm{x}\in\mathcal{Q}\)._

Proof of Theorem 3.1.: We prove the result in two steps. First, we show that

\[\mathcal{L}_{\mathcal{Q}\to\mathcal{P}}\cong_{\mathcal{Q}}\tilde{\mathcal{M}}_ {\mathcal{Q}\to\mathcal{P}}\coloneqq\left\{\begin{array}{llll}&(30)&\mathbf{ PA}_{(ja)}=\bm{b}_{j}&\forall\,ja\in\Sigma^{\perp}\\ &(31)&\mathbf{A}_{(\sigma)}=\mathbf{0}&\forall\,\sigma\in\Sigma\setminus\Sigma^{ \perp}\\ &(32)&\sum_{j^{\prime}\in\mathcal{C}_{\mathcal{G}}}\bm{b}_{j^{\prime}}=\bm{p}&\\ &(33)&\sum_{j^{\prime}\in\mathcal{C}_{ja}}\bm{b}_{j^{\prime}}=\bm{b}_{j}&\forall \,ja\in\Sigma\setminus\Sigma^{\perp}\\ &(34)&\mathbf{A}_{(\sigma)}\geq\mathbf{0}&\forall\,\sigma\in\Sigma\\ &(35)&\bm{b}_{j}\in\mathbb{R}^{k}&\forall\,j\in\mathcal{J}\end{array}\right\},\]

where the difference between \(\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{P}}\) and \(\mathcal{M}_{\mathcal{Q}\to\mathcal{P}}\) lies in constraint (34), which only sets a lower bound (at zero) for each entry of the matrix, as opposed to a bound \([0,\gamma]\) as in (7). Using the definition of \(\tilde{\mathcal{M}}_{\mathcal{Q}_{\succeq j}\to\mathcal{P}(\bm{b}_{j})}\) given in (20) (Theorem B.6), the set \(\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{P}}\) can be equivalently written as

\[\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{P}}=\left\{\begin{array}{llll}&(36)& \mathbf{A}_{(\varnothing)}=\mathbf{0}\\ &(37)&\sum_{j\in\mathcal{C}_{\mathcal{G}}}\bm{b}_{j}=\bm{p}&\\ &(38)&[\mathbf{A}_{(\sigma)}]_{\sigma\succeq j}\in\tilde{\mathcal{M}}_{\mathcal{Q }_{\succeq j}\to\mathcal{P}(\bm{b}_{j})}&\forall\,j\in\mathcal{C}_{ \mathcal{G}}\\ &(39)&\bm{b}_{j}\in\mathbb{R}^{k}&\forall\,j\in\mathcal{C}_{\mathcal{G}}\end{array} \right\}.\]

[MISSING_PAGE_EMPTY:19]

**Corollary C.2** (Alternative polytope representations).: _Let \(Q\) be a sequence-form strategy polytope, and \(\mathcal{C}\coloneqq\{\bm{y}\in\mathbb{R}^{n}:\mathbf{C}\bm{y}\leq\bm{c}\}\subseteq[ -\gamma,\gamma]^{n}\) be a bounded polytope, where \(\mathbf{C}\in\mathbb{R}^{m\times n}\). Let \(k\coloneqq\max\{\|\mathbf{C}\|_{\infty},\|\bm{c}\|_{\infty}\}\) and introduce the polytope_

\[\tilde{\mathcal{P}}\coloneqq\left\{(\tilde{\bm{y}},\bm{s})\in\mathbb{R}^{n} \times\mathbb{R}^{m}:\left[\begin{array}{c}\mathbf{C}\end{array}\right| kn\mathbf{I}\end{array}\right]\begin{bmatrix}\tilde{\bm{y}}\\ \bm{s}\end{bmatrix}=\bm{c}+\gamma\mathbf{C}\mathbf{1},\quad\begin{bmatrix} \tilde{\bm{y}}\\ \bm{s}\end{bmatrix}\geq\bm{0}\right\}\subseteq[0,2\gamma]^{n+m},\]

_which is of the form handled by Theorem 3.1. For any affine function \(g:\mathcal{Q}\to\mathcal{C}\), there exists a matrix \(\mathbf{A}\) in the polytope_

\[\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{C}}\coloneqq\left\{\begin{bmatrix} \tilde{\mathbf{M}}_{(\varnothing)}-\gamma\mathbf{1}\mid\cdots\mid\tilde{ \mathbf{M}}_{(\sigma)}\mid\cdots\end{bmatrix}\in\mathbb{R}^{n\times\Sigma}: \begin{bmatrix}\tilde{\mathbf{M}}\\ \tilde{\mathbf{Z}}\end{bmatrix}\in\mathcal{M}_{\mathcal{Q}\to\tilde{\mathcal{P }}},\ \tilde{\mathbf{M}}\in\mathbb{R}^{n\times\Sigma},\ \ \tilde{\mathbf{Z}}\in\mathbb{R}^{m \times\Sigma}\right\}\]

_such that \(g(\bm{x})=\mathbf{A}\bm{x}\) for all \(\bm{x}\in\mathcal{Q}\). Conversely, any \(\mathbf{A}\in\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{C}}\) induces an affine function from \(\mathcal{Q}\) to \(\mathcal{C}\)._

Proof.: We begin by proving that \(\mathcal{C}\subseteq[-\gamma,\gamma]^{n}\) implies \(\tilde{\mathcal{P}}\subseteq[0,2\gamma]^{n+m}\). Consider any \((\tilde{\bm{y}},\bm{s})\in\tilde{\mathcal{P}}\) and set \(\bm{y}=\tilde{\bm{y}}-\gamma\mathbf{1}\). Then, this is a valid \(\bm{y}\in\mathcal{C}\subseteq[-\gamma,\gamma]^{n}\) and, consequently, \(\tilde{\bm{y}}\in[0,2\gamma]^{n}\). For the slack variables \(\bm{s}\) it holds that \(kns=\bm{c}-\mathbf{C}\bm{y}\), where \(\bm{y}=\tilde{\bm{y}}-\gamma\mathbf{1}\) from before. By definition of \(k\) and by \(\bm{y}\geq-\gamma\mathbf{1}\), we conclude that \(\bm{c}-\mathbf{C}\bm{y}\leq(k+n\gamma k)\mathbf{1}\implies\bm{s}\in[0,2\gamma] ^{m}\).

Now let \(g:\mathcal{Q}\to\mathcal{C}\) be any affine function from \(\mathcal{Q}\) to \(\mathcal{C}\). Then we can define an affine function \(f:\mathcal{Q}\to\tilde{\mathcal{P}}\) such that

\[f(\bm{x})=\begin{bmatrix}g(\bm{x})+\gamma\mathbf{1}\\ \bm{s}\end{bmatrix}\]

for all \(\bm{x}\in\mathcal{Q}\). By Corollary C.1 we know that \(\mathcal{M}_{\mathcal{Q}\to\tilde{\mathcal{P}}}\) characterizes all affine functions from \(\mathcal{Q}\) to \(\tilde{\mathcal{P}}\), including the previous function \(f\). Thus, there exists an \(\tilde{\mathbf{M}}\in\mathbb{R}^{n\times\Sigma}\) such that \(g(\bm{x})+\gamma\mathbf{1}=\tilde{\mathbf{M}}\bm{x}\) for all \(\bm{x}\in\mathcal{Q}\). Since \(\bm{x}[\varnothing]=1\) for all \(\bm{x}\in\mathcal{Q}\), we conclude that there exists \(\mathbf{A}\in\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{C}}\) such that \(g(\bm{x})=\mathbf{A}\bm{x}=\tilde{\mathbf{M}}\bm{x}-\gamma\mathbf{1}\) for all \(\bm{x}\in\mathcal{Q}\).

Conversely, consider any \(\mathbf{A}\in\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{C}}\) and define \(g(\bm{x})=\mathbf{A}\bm{x}\). That is, there exist suitable \(\tilde{\mathbf{M}}\in\mathbb{R}^{n\times\Sigma}\), \(\tilde{\mathbf{Z}}\in\mathbb{R}^{m\times\Sigma}\) that satisfy the constraints of polytope \(\tilde{\mathcal{M}}_{\mathcal{Q}\to\mathcal{C}}\). Then for all \(\bm{x}\in\mathcal{Q}\) it holds \(g(\bm{x})=\tilde{\mathbf{M}}\bm{x}-\gamma\mathbf{1}\), and \((\tilde{\bm{y}},\bm{s})\in\tilde{\mathcal{P}}\) where \(\tilde{\bm{y}}=\tilde{\mathbf{M}}\bm{x}\) and \(\bm{s}=\tilde{\mathbf{Z}}\bm{x}\). Thus, by construction of \(\tilde{\mathcal{P}}\), as we also argued in the beginning of the proof, we conclude that \(g(\bm{x})=\tilde{\bm{y}}-\gamma\mathbf{1}=\bm{y}\in\mathcal{C}\). 

## Appendix D Additional proofs

**Theorem D.1**.: _Let \(\Sigma\) denote the set of sequences of the learning player in the extensive-form game, and let \(\eta^{(t)}=1/\sqrt{t}\) for all \(t\). Then, for any sequence of loss vectors \(\bm{\ell}^{(t)}\in[0,1]^{\Sigma}\), Algorithm 1 guarantees linear-swap regret \(O(|\Sigma|^{2}\sqrt{T})\) after any number \(T\) of iterations, and runs in \(O(|\Sigma|^{10}\log(|\Sigma|)\log^{2}t)\) time for each iteration \(t\)._

Proof of Theorem 3.2.: First we focus on the linear-swap regret bound. Based on Gordon et al. (2008) the \(\Phi\)-regret equals external regret over the set \(\Phi\) of transformations. In our case \(\Phi\) is the set \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) of all valid linear transformations and the losses for the external regret minimizer are functions \(\mathbf{A}\mapsto\langle\bm{\ell}^{(1)},\mathbf{A}\bm{x}^{(1)}\rangle,\mathbf{A }\mapsto\langle\bm{\ell}^{(2)},\mathbf{A}\bm{x}^{(2)}\rangle,\ldots\) Equivalently, we can write these as \(\mathbf{A}\mapsto\langle\mathbf{L}^{(t)},\mathbf{A}\rangle_{F}\), where \(\langle\cdot,\cdot\rangle_{F}\) is the component-wise inner product for matrices and \(\mathbf{L}^{(t)}=\bm{\ell}^{(t)}(\bm{x}^{(t)})^{\top}\), which is a rank-one matrix. Let \(D\) be an upper bound on the diameter of \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\), and \(L\) be such that \(\|\mathbf{L}^{(t)}\|_{F}\leq L\) for all \(t\). Then, based on Orabona (2022) we can bound the external regret for this instance of Online Linear Optimization by picking \(\eta^{(t)}=\frac{D}{L\sqrt{t}}\) which gives a regret of \(O(DL\sqrt{T})\). Since \(\mathbf{A}\in[0,1]^{\Sigma\times\Sigma}\) we get \(D=|\Sigma|\), and since \(\bm{\ell}^{(t)},\bm{x}^{(t)}\in[0,1]^{\Sigma}\) we get \(L=|\Sigma|\). This results in the desired linear-swap regret of \(O(|\Sigma|^{2}\sqrt{T})\).

However, in our previous analysis we assumed that it is possible to compute an exact solution \(\mathbf{A}^{(t+1)}=\Pi_{\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}}(\mathbf{A}^{(t )}-\eta^{(t)}\mathbf{L}^{(t)})\) of the projection step, which is an instance of convex quadratic programming, meaning that we can only get an approximate solution (Vishnoi, 2021). In the following Lemma we prove that an \(\epsilon\)-approximate projection does not affect the regret for a single iteration ofthe algorithm, if \(\epsilon\) is sufficiently small. The inequality we prove is similar to the one in Lemma 2.12 from Orabona (2022).

**Lemma D.2**.: _Let \(\mathbf{Y}^{*}=\Pi_{\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}}(\mathbf{A}^{(t)}- \eta^{(t)}\mathbf{L}^{(t)})\) and suppose that \(\mathbf{A}^{(t+1)}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) is such that \(\|\mathbf{A}^{(t+1)}-\mathbf{Y}^{*}\|_{F}^{2}\leq\epsilon^{(t)}\), then for any \(\mathbf{X}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) it holds_

\[\langle\mathbf{L}^{(t)},\mathbf{X}-\mathbf{A}^{(t)}\rangle_{F}\leq\frac{1}{2 \eta^{(t)}}\|\mathbf{A}^{(t)}-\mathbf{X}\|_{F}^{2}-\frac{1}{2\eta^{(t)}}\| \mathbf{A}^{(t+1)}-\mathbf{X}\|_{F}^{2}+\frac{\eta^{(t)}}{2}\|\mathbf{L}^{(t) }\|_{F}^{2}+\frac{D}{\eta^{(t)}}\frac{1}{\epsilon^{(t)}}\]

Proof.: From Lemma 2.12 of Orabona (2022) we know that

\[\langle\mathbf{L}^{(t)},\mathbf{X}-\mathbf{A}^{(t)}\rangle_{F}\leq\frac{1}{2 \eta^{(t)}}\|\mathbf{A}^{(t)}-\mathbf{X}\|_{F}^{2}-\frac{1}{2\eta^{(t)}}\| \mathbf{Y}^{*}-\mathbf{X}\|_{F}^{2}+\frac{\eta^{(t)}}{2}\|\mathbf{L}^{(t)}\|_ {F}^{2}.\]

Additionally, it holds that

\[\|\mathbf{A}^{(t+1)}-\mathbf{X}\|_{F}^{2} =\|\mathbf{A}^{(t+1)}-\mathbf{Y}^{*}+\mathbf{Y}^{*}-\mathbf{X}\| _{F}^{2}\] \[=\|\mathbf{A}^{(t+1)}-\mathbf{Y}^{*}\|_{F}^{2}+\|\mathbf{Y}^{*}- \mathbf{X}\|_{F}^{2}+2\langle\mathbf{A}^{(t+1)}-\mathbf{Y}^{*},\mathbf{Y}^{*} -\mathbf{X}\rangle_{F}\] \[\leq\|\mathbf{Y}^{*}-\mathbf{X}\|_{F}^{2}+2\langle\mathbf{A}^{(t +1)}-\mathbf{Y}^{*},\mathbf{A}^{(t+1)}-\mathbf{Y}^{*}+\mathbf{Y}^{*}-\mathbf{ X}\rangle_{F}\] \[=\|\mathbf{Y}^{*}-\mathbf{X}\|_{F}^{2}+2\langle\mathbf{A}^{(t+1)} -\mathbf{Y}^{*},\mathbf{A}^{(t+1)}-\mathbf{X}\rangle_{F}\] \[\leq\|\mathbf{Y}^{*}-\mathbf{X}\|_{F}^{2}+2\|\mathbf{A}^{(t+1)}- \mathbf{Y}^{*}\|_{F}\|\mathbf{A}^{(t+1)}-\mathbf{X}\|_{F}\] \[\leq\|\mathbf{Y}^{*}-\mathbf{X}\|_{F}^{2}+2D\epsilon^{(t)}.\]

The Lemma follows by combining the previous two inequalities. 

If we set \(\epsilon^{(t)}=1/t^{5/2}\) then for our choice of \(\eta^{(t)}=\frac{D}{L\sqrt{t}}\), the error term becomes \(L/t^{2}\). Summing over \(T\) timesteps we thus get an additive error of \(O(L)=O(|\Sigma|)\) in the regret, and our total linear-swap regret bound remains \(O(|\Sigma|^{2}\sqrt{T})\).

We now move to analyzing the per-iteration time complexity of Algorithm 1. The most computationally heavy steps are (5) and (6). To compute the fixed point at step (6) we can use any polynomial-time LP algorithm. We can also perform the projection step (5) in polynomial time using the ellipsoid method (Vishnoi, 2021). For this we reduce it to a suitable Semidefinite Program with \(O(|\Sigma|^{2})\) variables and use the Cholesky factorization (Gartner and Matousek, 2014) as the separation oracle, thus responding to separation queries in \(O(|\Sigma|^{6})\) time. Note that it is possible to guarantee \(\|\mathbf{A}^{(t+1)}-\mathbf{Y}^{*}\|_{F}^{2}\leq\epsilon^{(t)}\) as the ellipsoid method can output a point \(\mathbf{A}^{(t+1)}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) such that \(\|\mathbf{A}^{(t)}-\eta^{(t)}\mathbf{L}^{(t)}-\mathbf{A}^{(t+1)}\|_{F}\leq\| \mathbf{A}^{(t)}-\eta^{(t)}\mathbf{L}^{(t)}-\mathbf{Y}^{*}\|_{F}+\epsilon\) and furthermore, the Frobenius norm is a 2-strongly convex function.

To apply the ellipsoid method we further need bounds on the Frobenius norm of \(\mathbf{Y}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) and the maximum projection distance. For the norm of \(\mathbf{Y}\), we pick an upper bound of \(R=D=|\Sigma|\). For the lower bound \(r\) we note that \(\mathbf{Y}\bm{x}\in\mathcal{Q}\) for all \(\bm{x}\in\mathcal{Q}\), which implies \(\mathbf{Y}[\mathcal{O}]\bm{x}=1\implies\|\mathbf{Y}[\mathcal{O}]\|_{1}\geq 1\). Thus we get \(r\geq\frac{1}{|\Sigma|}\). Similarly, we bound the projection distance between \(0\) and \(D\). Based on these bounds and on Theorem 13.1 from Vishnoi (2021) we conclude that the total per-iteration time complexity of Algorithm 1 is \(O\left(|\Sigma|^{10}\log(|\Sigma|)\log^{2}(t)\right)\). 

**Theorem 4.1**.: _For two-player, perfect-recall extensive-form games with chance moves, the problem MAXPAY-LCE is not solvable in polynomial time, unless P=NP._

Proof.: We use the exact same argument employed in the paper by von Stengel and Forges (2008) by reducing SAT to the MAXPAY-LCE problem. Specifically, for each instance of SAT having \(n\) clauses and \(m\) variables we construct a two-player extensive-form game of size polynomial in \(n\) and \(m\). In the beginning, there is a chance move that picks one of \(n\) possible actions uniformly at random - one for each SAT clause. Then is the turn of Player 2 who has \(n\) distinct singleton information sets corresponding to the chance node actions, and respectively to the \(n\) clauses of the SAT instance. Let \(L_{i}\) be the set of literals (negated or non-negated variables) included in the \(i\)-th clause of the SATinstance. In information set \(i\) of Player 2 there exist \(|L_{i}|\) actions, one for each literal in \(L_{i}\). Finally, each literal leads to a different decision node for Player 1 who has as many decision nodes as the number of literals in the SAT instance, and in each node there exist exactly 2 possible actions: TRUE and FALSE. However, Player 1 only has \(m\) information sets corresponding to the \(m\) SAT variables with each information set \(x\) grouping together all nodes corresponding to literals of the variable \(x\). This way, Player 1 only chooses the truth value of a variable without knowing from which literal Player 2 has picked this variable. The utilities for both players are equal to 1 if the truth value picked by Player 1 satisfies the literal picked by Player 2, and both utilities are 0 otherwise.

In this game, there exists a pure strategy attaining payoff \(1\) for each player if and only if the SAT instance is satisfiable - namely Player 1 always acts based on the satisfying assignment and Player 2 picks for every clause a literal that is known to be TRUE. Otherwise, the maximum payoff for each pure strategy is at most \(1-1/n\). Given that a LCE describes a convex combination of pure strategies, it follows that the maximum total expected payoff of the the sum of the two players' utilities will either be \(2\) when the SAT instance is satisfiable, or it will be at most \(2(1-1/n)\) when the instance is not satisfiable. Additionally, this holds not just for the sum but for any linear combination of player utilities. Thus, the problem of deciding whether MAXPAY-LCE can attain a value of at least \(k\) is NP-hard for any linear combination of utilities. 

## Appendix E Examples

**Example E.1** (Efce \(\neq\) Lce).: _Consider the following 2-player signaling game presented by von Stengel and Forges [2008]_

_However, we can observe that this EFCE is not a linear-deviation correlated equilibrium because, for example, Player 1 can increase their payoff by a value of \(3/2\) using the following transformation:_

\[X_{G}X_{B} \mapsto X_{G}X_{B} i.e.,\text{ map the reduced-normal-form plan }(1,0,1,0) \mapsto(1,0,1,0)\] \[X_{G}Y_{B} \mapsto X_{G}X_{B} i.e.,\text{ map the reduced-normal-form plan }(1,0,0,1) \mapsto(1,0,1,0)\] \[Y_{G}X_{B} \mapsto Y_{G}Y_{B} i.e.,\text{ map the reduced-normal-form plan }(0,1,1,0) \mapsto(0,1,0,1)\] \[Y_{G}Y_{B} \mapsto Y_{G}Y_{B} i.e.,\text{ map the reduced-normal-form plan }(0,1,0,1) \mapsto(0,1,0,1)\]

_(Above, we have implicitly assumed that the strategy vectors encode probability of actions in the arbitrary order \(X_{G}\), \(Y_{G}\), \(X_{B}\), \(Y_{B}\)). The above transformation is linear, since it can be represented via the matrix_

\[\begin{pmatrix}1&0&0&0\\ 0&1&0&0\\ 1&0&0&0\\ 0&1&0&0\end{pmatrix}.\]

_This would swap the pure strategy \(X_{G}Y_{B}\) with \(X_{G}X_{B}\) and strategy \(Y_{G}X_{B}\) with \(Y_{G}Y_{B}\). Crucially, the strategy at the subtree of information set \(B\) is determined by the strategy at the subtree of information set \(G\). Thus, the transformed value for each sequence does not purely depend on the ancestors of that sequence, but can also depend on strategies belonging to "sibling" subtrees. Hence, this example proves that \(\text{LCE}\neq\text{EFCE}\)._

_In fact, we can even verify that in this toy example the set of LCE coincides with the set of all normal-form CE. We refer the interested reader to von Stengel and Forges [2008] for a more detailed discussion of the different behaviors that players can exhibit at an EFCE vs a normal-form CE, which in this specific example also corresponds to a comparison between the EFCE and the LCE._

**Remark E.2**.: _The linear transformation given in Example E.1 also serves to show that the Behavioral Deviations defined in Morrill et al. [2021] are not a superset of all linear transformations. Additionally, behavioral deviations are not a subset of linear transformations, as the latter act only on reduced strategies. Thus the two sets of deviations are incomparable._

**Example E.3** (\(\text{LCE}\neq\text{CE}\)).: _This example was found through computational search. Consider the 2-player game with the following game tree:_

_Based on the previous game tree, the normal-form payoff matrix of the game is shown below.__We can now verify that the following is a linear-deviation correlated equilibrium for this game. The verification can be done computationally by expressing all constraints of Theorem 3.1 as a Linear Program._

\[\begin{array}{c|cccc}&Q_{l}W_{l}&Q_{l}W_{r}&Q_{r}W_{l}&Q_{r}W_{r}\\ \hline A_{1}B_{1}&\text{1/5}&\text{0}&\text{0}&\text{0}\\ A_{1}B_{2}&\text{0}&\text{1/5}&\text{0}&\text{0}\\ A_{2}B_{1}&\text{1/5}&\text{0}&\text{0}&\text{0}\\ A_{2}B_{2}&\text{0}&\text{0}&\text{1/5}&\text{1/5}\\ \hline\end{array}\]

_However, the swap \(\{A_{1}B_{2}\mapsto A_{1}B_{1},A_{2}B_{1}\mapsto A_{1}B_{1}\}\) can increase player 1's payoff by \(50.5\)._

_Furthermore, we can verify that this swap is not linear as follows. First assume that it was linear and could be written as a matrix \(\mathbf{A}\in[0,1]^{\Sigma\times\Sigma}\). Then the matrix has to be consistent with the following transformations:_

\[\begin{array}{c}A_{1}B_{1}\mapsto A_{1}B_{1}\\ A_{1}B_{2}\mapsto A_{1}B_{1}\\ A_{2}B_{1}\mapsto A_{1}B_{1}\\ A_{2}B_{2}\mapsto A_{2}B_{2}\\ \end{array}\]

_For convenience of referring to matrix rows and columns we number the four sequences as:_

\[A_{1}:0,\quad A_{2}:1,\quad B_{1}:2,\quad B_{2}:3\]

_If we focus on the second transformation, \(A_{1}B_{2}\mapsto A_{1}B_{1}\), we conclude that \(\mathbf{A}[:,0]+\mathbf{A}[:,3]=(1,0,1,0)^{\top}\) which implies that \(\mathbf{A}[1,0]=\mathbf{A}[3,0]=\mathbf{A}[1,3]=\mathbf{A}[3,3]=0\). Now, if we subtract the respective equations of the last two swaps we get \(\mathbf{A}[:,2]-\mathbf{A}[:,3]=(1,-1,1,-1)^{\top}\). Since \(\mathbf{A}\in[0,1]^{\Sigma\times\Sigma}\), the last equation implies \(\mathbf{A}[1,3]=1\) which contradicts the previous constraint of \(\mathbf{A}[1,3]=0\). Thus, we have found a valid normal-form swap that cannot be expressed as a linear transformation of sequence-form strategies. Hence, this example proves that \(\text{LCE}\neq\text{CE}\)._

## Appendix F Details on Empirical Evaluation

In this section we provide details about the implementation of our algorithm, as well as the compute resources and game instances used.

Implementation of our no-linear-swap-regret dynamicsWe implemented our no-linear-swap-regret algorithm (Algorithm 1) in the C++ programming language using the Gurobi commercial optimization solver (Gurobi Optimization, LLC, 2023), version 10. We use Gurobi for the following purposes.

* To compute the projection needed on Line 5 of Algorithm 1. We remark that while Gurobi is typically recognized as a linear and integer linear programming solver, modern versions include tuned code for convex quadratic programming. In particular, we used the barrier algorithm to compute the Euclidean projections onto the polytope \(\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\) required at every iteration of our algorithm.
* To compute the fixed points of the matrices \(\mathbf{A}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}\), that is, finding \(\mathcal{Q}\ni\boldsymbol{x}=\mathbf{A}\boldsymbol{x}\). As discussed in Section 3 this is a polynomially-sized linear program.
* To measure the linear-swap regret incurred after any \(T\) iterations, which is plotted on the y-axes of Figure 2. This corresponds to solving the linear optimization problem \[\max_{\mathbf{A}\in\mathcal{M}_{\mathcal{Q}\to\mathcal{Q}}}\left\{\frac{1}{T} \sum_{t=1}^{T}\langle\boldsymbol{\ell}^{(t)},\boldsymbol{x}^{(t)}-\mathbf{A} \boldsymbol{x}^{(t)}\rangle\right\}.\]

We did very minimal tuning of the constant learning rate \(\eta\) used for online projected gradient descent, trying values \(\eta\in\{0.05,0.1,0.5\}\) (we remark that a constant value of \(\eta\approx 1/\sqrt{T}\) is theoretically sound). We found that \(\eta=0.1\), which is used in the plots of Figure 2, performed best.

Implementation of no-trigger-regret dynamicsWe implemented the no-trigger-regret algorithm of Farina et al. (2022) in the C++ programming language. In this case, there is no need to use Gurobi, since, as the original authors show, the polytope of trigger deviation functions admits a convenient combinatorial characterization that enables us to sidestep linear programming. Rather, we implemented the algorithm and the computation of the trigger regret directly leveraging the combinatorial structure.

Computational resources usedMinimal computational resources were used. All code ran on a personal laptop for roughly 12 hours.

Game instance usedWe ran our code on the standard benchmark game of Kuhn poker Kuhn (1950). We used a three-player variant of the game. Compared to the original game, which only considers a simplified deck make of cards out of only three possible ranks (Jack, Queen, or Kind), we use a full deck of 13 possible card ranks. The game has 156 information sets, 315 sequences, and 22308 terminal states.

## Appendix G Further Remarks on the Reduction from No-\(\Phi\)-Regret to External Regret

A no-\(\Phi\)-regret algorithm is typically defined as outputting _deterministic_ behavior from a finite set \(\mathcal{X}\) (for example, deterministic reduced-normal-form plans in extensive-form games, or actions in normal-form games), which can be potentially sampled at random. This is the setting used by, for example, Hart and Mas-Colell (2000) and Farina et al. (2022). However, we remark that when the transformations \(\phi\in\Phi\) are linear, any such device can be constructed starting from an algorithm that outputs points \(\boldsymbol{x}^{\prime}\in\mathcal{X}^{\prime}\coloneqq\text{conv}(\mathcal{ X})\), and then sampling \(\boldsymbol{x}\) unbiasedly in accordance with \(\boldsymbol{x}^{\prime}\), that is, so that \(\mathbb{E}[\boldsymbol{x}]=\boldsymbol{x}^{\prime}\). The reason why this is useful is that constructing the latter object is usually simpler, as \(\mathcal{X}^{\prime}\) is a closed and convex set, and is therefore amenable to the wide array of online optimization techniques that have been developed over the years.

More formally, let \(\Phi\) be a set of linear transformations that map \(\mathcal{X}\) to itself. A no-\(\Phi\)-regret algorithm for \(\mathcal{X}^{\prime}=\text{conv}(\mathcal{X})\) guarantees that, no matter the sequence of loss vectors \(\boldsymbol{\ell}^{(t)}\),

\[R^{\prime(T)}=\max_{\phi\in\Phi}\sum_{t=1}^{T}\langle\boldsymbol{\ell}^{(t)}, \boldsymbol{x}^{\prime(t)}-\phi(\boldsymbol{x}^{\prime(t)})\rangle\]

grows sublinearly. Consider now an algorithm that, after receiving \(\boldsymbol{x}^{\prime(t)}\in\text{conv}(\mathcal{X})\), samples \(\boldsymbol{x}^{(t)}\in\mathcal{X}\) unbiasedly, that is, so that \(\mathbb{E}[\boldsymbol{x}^{(t)}]=\boldsymbol{x}^{\prime(t)}\). Then, by linearity of the transformations and using the Azuma-Hoeffind concentration inequality, we obtain that for any \(\epsilon>0\),

\[\mathbb{P}\left[\left|R^{\prime\left(T\right)}-\max_{\phi\in\Phi}\sum_{t=1}^{T} \langle\bm{\ell}^{(t)},\bm{x}^{(t)}-\phi(\bm{x}^{(t)})\rangle\right|\leq\Theta \left(\sqrt{T\log\frac{1}{\epsilon}}\right)\right]\geq 1-\epsilon,\]

where the big-theta notation hides constants that depend on the payoff range of the game and the diameter of \(\mathcal{X}\) (a polynomial quantity in the game tree size). This shows that as long as the regret of the no-\(\Phi\)-algorithm that operates over \(\mathcal{X}^{\prime}\) is sublinear, then so is that of an algorithm that outputs points on \(\mathcal{X}\) by sampling unbiasedly from \(\mathcal{X}\). We refer the interested reader to Section 4.2 ("From Deterministic to Mixed Strategies") of Farina et al. (2022).