# Convolutional State Space Models for

Long-Range Spatiotemporal Modeling

 Jimmy T.H. Smith\({}^{*,2,4}\), Shalini De Mello\({}^{1}\), Jan Kautz\({}^{1}\), Scott W. Linderman\({}^{3,4}\), Wonmin Byeon\({}^{1}\)

\({}^{1}\)NVIDIA, \({}^{*}\)Work performed during internship at NVIDIA

\({}^{2}\)Institute for Computational and Mathematical Engineering, Stanford University.

\({}^{3}\)Department of Statistics, Stanford University.

\({}^{4}\)Wu Tsai Neurosciences Institute, Stanford University.

{jsmith14,scott.linderman}@stanford.edu

{shalinig,jkautz,wbyeon}@nvidia.com.

###### Abstract

Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM)1 that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvSS, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training \(3\times\) faster than ConvLSTM and generating samples \(400\times\) faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.

Footnote 1: Implementation available at: https://github.com/NVlabs/ConvSSM.

## 1 Introduction

Developing methods that efficiently and effectively model long-range spatiotemporal dependencies is a challenging problem in machine learning. Whether predicting future video frames [1; 2], modeling traffic patterns [3; 4], or forecasting weather [5; 6], deep spatiotemporal modeling requires simultaneously capturing local spatial structure and long-range temporal dependencies. Although there has been progress in deep generative modeling of complex spatiotemporal data [7; 8; 9; 10; 11; 12], most prior work has only considered short sequences of 20-50 timesteps due to the costs of processing long spatiotemporal sequences. Recent work has begun considering sequences of hundreds to thousands of timesteps [13; 14; 15; 16]. As hardware and data collection of long spatiotemporal sequences continue to improve, new modeling approaches are required that scale efficiently with sequence length and effectively capture long-range dependencies.

Convolutional recurrent networks (ConvRNNs) such as ConvLSTM [17] and ConvGRU [18] are common approaches for spatiotemporal modeling. These methods encode the spatial information using tensor-structured states. The states are updated with recurrent neural network (RNN) equations that use convolutions instead of the matrix-vector multiplications in standard RNNs (e.g., LSTM/GRUs [21, 22]). This approach allows the RNN states to reflect the spatial structure of the data while simultaneously capturing temporal dynamics. ConvRNNs inherit both the benefits and the weaknesses of RNNs: they allow fast, stateful autoregressive generation and an unbounded context window, but they are slow to train due to their inherently sequential structure and can suffer from the vanishing/exploding gradient problem [23].

Transformer-based methods [9, 13, 14, 24, 25, 26, 27] operate on an entire sequence in parallel, avoiding these training challenges. Transformers typically require sophisticated compression schemes [28, 29, 30] to reduce the spatiotemporal sequence into tokens. Moreover, Transformers use an attention mechanism that has a bounded context window and whose computational complexity scales quadratically in sequence length for training and inference [31, 32]. More efficient Transformer methods improve on the complexity of attention [33, 34, 35, 36, 37, 38, 39], but these methods can fail on sequences with long-range dependencies [40, 13]. Some approaches combine Transformers with specialized training frameworks to address the attention costs [13]. However, recent work in deep state space models (SSMs) [19, 41, 42, 20, 43], like S4 [19] and S5 [20], has sought to overcome attention's quadratic complexity while maintaining the parallelizability and performance of attention and the statefulness of RNNs. These SSM layers have proven to be effective in various domains such as speech [44], images [45] and video classification [45, 46]; reinforcement learning [47, 48]; forecasting [49] and language modeling [50, 51, 52, 53].

Inspired by modeling ideas from ConvRNNs and SSMs, we introduce _convolutional state space models_ (ConvSSMs), which have a tensor-structured state like ConvRNNs but a continuous-time parameterization and linear state updates like SSM layers. See Figure 1. However, there are challenges to make this approach scalable and effective for modeling long-range spatiotemporal data. In this paper, we address these challenges and provide a rigorous framework that ensures both computational efficiency and modeling performance for spatiotemporal sequence modeling. First, we discuss computational efficiency and parallelization of ConvSSMs across the sequence for scalable training and fast inference. We show how to parallelize linear convolutional recurrences using a binary associative operator and demonstrate how this can be exploited to use parallel scans for subquadratic parallelization across the spatiotemporal sequence. We discuss both theoretical and practical considerations (Section 3.2) required to make this feasible and efficient. Next, we address how to capture long-range spatiotemporal dependencies. We develop a connection between

Figure 1: ConvRNNs [17, 18] (left) model spatiotemporal sequences using tensor-valued states, \(\bm{\mathcal{X}}_{k}\), and a nonlinear RNN update, \(\mathbf{G}()\), that uses convolutions instead of matrix-vector multiplications. A position-wise nonlinear function, \(\mathbf{h}()\), transforms the states into the output sequence. Deep SSMs [19, 20] (center) model vector-valued input sequences using a discretized linear SSM. The linear dynamics can be exploited to parallelize computations across the sequence and capture long-range dependencies. We introduce ConvSSMs (right) that model spatiotemporal data using tensor states, like ConvRNNs, and linear dynamics, like SSMs. We also introduce an efficient ConvSSM variant, ConvS5, that can be parallelized across the sequence with parallel scans, has fast autoregressive generation, and captures long-range dependencies.

the dynamics of SSMs and ConvSSMs (Section 3.3) and leverage this, in Section 3.4, to introduce a parameterization and initialization design that can capture long-range spatiotemporal dependencies.

As a result, we introduce _ConvS5_, a new spatiotemporal layer that is an efficient ConvSSM variant. It is parallelizable and overcomes difficulties during training (e.g., vanishing/exploding gradient problems) that traditional ConvRNN approaches experience. ConvS5 does not require compressing frames into tokens and provides an unbounded context. It also provides fast (constant time and memory per step) autoregressive generation compared to Transformers. ConvS5 significantly outperforms Transformers and ConvLSTM on a challenging long horizon Moving-MNIST [54] experiment requiring methods to train on 600 frames and generate up to 1,200 frames. In addition, ConvS5 trains \(3\times\) faster than ConvLSTM on this task and generates samples \(400\times\) faster than the Transformer. Finally, we show that ConvS5 matches or exceeds the performance of various state-of-the-art methods on challenging DMLab, Minecraft, and Habitat long-range video prediction benchmarks [13].

## 2 Background

This section provides the background necessary for ConvSSMs and ConvS5, introduced in Section 3.

### Convolutional Recurrent Networks

Given a sequence of inputs \(\mathbf{u}_{1:L}\in\mathbb{R}^{L\times U}\), an RNN updates its state, \(\mathbf{x}_{k}\in\mathbb{R}^{P}\), using the state update equation \(\mathbf{x}_{k}=\mathbf{F}(\mathbf{x}_{k-1},\mathbf{u}_{\mathbf{k}})\), where \(\mathbf{F}()\) is a nonlinear function. For example, a vanilla RNN can be represented (ignoring the bias term) as

\[\mathbf{x}_{k}=\tanh(\mathbf{A}\mathbf{x}_{k-1}+\mathbf{B}\mathbf{u}_{\mathbf{ k}})\] (1)

with state matrix \(\mathbf{A}\in\mathbb{R}^{P\times P}\), input matrix \(\mathbf{B}\in\mathbb{R}^{P\times U}\) and \(\tanh()\) applied elementwise. Other RNNs such as LSTM [21] and GRU [22] utilize more intricate formulations of \(\mathbf{F}()\).

_Convolutional recurrent neural networks_[17; 18] (ConvRNNs) are designed to model spatiotemporal sequences by replacing the vector-valued states and inputs of traditional RNNs with tensors and substituting matrix-vector multiplications with convolutions. Given a length \(L\) sequence of frames, \(\mathcal{U}_{1:L}\in\mathbb{R}^{L\times H^{\prime}\times W^{\prime}\times U}\), with height \(H^{\prime}\), width \(W^{\prime}\) and \(U\) features, a ConvRNN updates its state, \(\mathcal{X}_{k}\in\mathbb{R}^{H\times W\times P}\), with a state update equation \(\mathcal{X}_{k}=\mathbf{G}(\mathcal{X}_{k-1},\mathcal{U}_{k})\), where \(\mathbf{G}()\) is a nonlinear function. Analogous to (1), we can express the state update equation for a vanilla ConvRNN as

\[\mathcal{X}_{k}=\tanh(\mathcal{A}*\mathcal{X}_{k-1}+\mathcal{B}*\mathcal{U}_{ k}),\] (2)

where \(*\) is a spatial convolution operator with state kernel \(\mathcal{A}\in\mathbb{R}^{P\times P\times k_{A}\times k_{A}}\) (using an [output features, input features, kernel height, kernel width] convention), input kernel \(\mathcal{B}\in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}\) and \(\tanh()\) is applied elementwise. More complex updates such as ConvLSTM [17] and ConvGRU [18] are commonly used by making similar changes to the LSTM and GRU equations, respectively.

### Deep State Space Models

This section briefly introduces deep SSMs such as S4 [19] and S5 [20] designed for modeling long sequences. The ConvS5 approach we introduce in Section 3 extends these ideas to the spatiotemporal domain.

Linear State Space ModelsGiven a continuous input signal \(\mathbf{u}(t)\in\mathbb{R}^{U}\), a latent state \(\mathbf{x}(t)\in\mathbb{R}^{P}\) and an output signal \(\mathbf{y}(t)\in\mathbb{R}^{M}\), a continuous-time, linear SSM is defined using a differential equation:

\[\mathbf{x}^{\prime}(t)=\mathbf{A}\mathbf{x}(t)+\mathbf{B}\mathbf{u}(t),\qquad \mathbf{y}(t)=\mathbf{C}\mathbf{x}(t)+\mathbf{D}\mathbf{u}(t),\] (3)

and is parameterized by a state matrix \(\mathbf{A}\in\mathbb{R}^{P\times P}\), an input matrix \(\mathbf{B}\in\mathbb{R}^{P\times U}\), an output matrix \(\mathbf{C}\in\mathbb{R}^{M\times P}\) and a feedthrough matrix \(\mathbf{D}\in\mathbb{R}^{M\times U}\). Given a sequence, \(\mathbf{u}_{1:L}\in\mathbb{R}^{L\times U}\), the SSM can be discretized to define a discrete-time SSM

\[\mathbf{x}_{k}=\overline{\mathbf{A}}\mathbf{x}_{k-1}+\overline{\mathbf{B}} \mathbf{u}_{k},\qquad\mathbf{y}_{k}=\mathbf{C}\mathbf{x}_{k}+\mathbf{D} \mathbf{u}_{k},\] (4)

where the discrete-time parameters are a function of the continuous-time parameters and a timescale parameter, \(\Delta\). We define \(\overline{\mathbf{A}}=\mathrm{DISCRETIZE}_{\mathrm{A}}(\mathbf{A},\Delta)\) and \(\overline{\mathbf{B}}=\mathrm{DISCRETIZE}_{\mathrm{B}}(\mathbf{A},\mathbf{B},\Delta)\) where \(\mathrm{DISCRETIZE}()\) is a discretization method such as Euler, bilinear or zero-order hold [55].

S4 and S5Gu et al. [19] introduced the _structured state space sequence_ (S4) layer to efficiently model long sequences. An S4 layer uses many continuous-time linear SSMs, an explicit discretization step with learnable timescale parameters, and position-wise nonlinear activation functions applied to the SSM outputs. Smith et al. [20] showed that with several architecture changes, the approach could be simplified and made more flexible by just using one SSM as in (3) and utilizing parallel scans. SSM layers, such as S4 and S5, take advantage of the fact that linear dynamics can be parallelized with subquadratic complexity in the sequence length. They can also be run sequentially as stateful RNNs for fast autoregressive generation. While a single SSM layer such as S4 or S5 has only linear dynamics, the nonlinear activations applied to the SSM outputs allow representing nonlinear systems by stacking multiple SSM layers [56; 57; 58].

SSM Parameterization and InitializationParameterization and initialization are crucial aspects that allow deep SSMs to capture long-range dependencies more effectively than prior attempts at linear RNNs [59; 60; 61]. The general setup includes continuous-time SSM parameters, explicit discretization with learnable timescale parameters, and state matrix initialization using structured matrices inspired by the HiPPO framework [62]. Prior research emphasizes the significance of these choices in achieving high performance on challenging long-range tasks [19; 20; 56; 57]. Recent work [57] has studied these parameterizations/initializations in more detail and provides insight into this setup's favorable initial eigenvalue distributions and normalization effects.

### Parallel Scans

We briefly introduce parallel scans, as used by S5, since they are important for parallelizing the ConvS5 method we introduce in Section 3. See Blelloch [63] for a more detailed review. A scan operation, given a binary associative operator \(\bullet\) (i.e. \((a\bullet b)\bullet c=a\bullet(b\bullet c)\)) and a sequence of \(L\) elements \([a_{1},a_{2},...,a_{L}]\), yields the sequence: \([a_{1},\ (a_{1}\bullet a_{2}),\...,\ (a_{1}\bullet a_{2}\bullet...\bullet a _{L})]\).

Parallel scans use the fact that associative operators can be computed in any order. A parallel scan can be defined for the linear recurrence of the state update in (4) by forming the initial scan tuples \(c_{k}=(c_{k,a},c_{k,b}):=(\overline{\mathbf{A}},\ \ \overline{\mathbf{B}} \mathbf{u}_{k})\) and utilizing a binary associative operator that takes two tuples \(q_{i},q_{j}\) (either the initial tuples \(c_{i},c_{j}\) or intermediate tuples) and produces a new tuple of the same type, \(q_{i}\bullet q_{j}:=(q_{j,a}\odot q_{i,a},\ q_{j,a}\otimes q_{i,b}+q_{j,b})\), where \(\odot\) is matrix-matrix multiplication and \(\otimes\) is matrix-vector multiplication. Given sufficient processors, the parallel scan computes the linear recurrence of (4) in \(O(\log L)\) sequential steps (i.e., depth or span) [63].

## 3 Method

This section introduces convolutional state space models (ConvSSMs). We show how ConvSSMs can be parallelized with parallel scans. We then connect the dynamics of ConvSSMs to SSMs to motivate parameterization. Finally, we use these insights to introduce an efficient ConvSSM variant, ConvS5.

### Convolutional State Space Models

Consider a continuous tensor-valued input \(\boldsymbol{\mathcal{U}}(t)\in\mathbb{R}^{H^{\prime}\times W^{\prime}\times U}\) with height \(H^{\prime}\), width \(W^{\prime}\), and number of input features \(U\). We will define a continuous-time, linear convolutional state space model (_ConvSSM_) with state \(\boldsymbol{\mathcal{X}}(t)\in\mathbb{R}^{H\times W\times P}\), derivative \(\boldsymbol{\mathcal{X}}^{\prime}(t)\in\mathbb{R}^{H\times W\times P}\) and output \(\boldsymbol{\mathcal{Y}}(t)\in\mathbb{R}^{H\times W\times U}\), using a differential equation:

\[\boldsymbol{\mathcal{X}}^{\prime}(t) =\boldsymbol{\mathcal{A}}*\boldsymbol{\mathcal{X}}(t)+\boldsymbol {\mathcal{B}}*\boldsymbol{\mathcal{U}}(t)\] (5) \[\boldsymbol{\mathcal{Y}}(t) =\boldsymbol{\mathcal{C}}*\boldsymbol{\mathcal{X}}(t)+\boldsymbol {\mathcal{D}}*\boldsymbol{\mathcal{U}}(t)\] (6)

where \(*\) denotes the convolution operator, \(\boldsymbol{\mathcal{A}}\in\mathbb{R}^{P\times P\times k_{A}\times k_{A}}\) is the state kernel, \(\boldsymbol{\mathcal{B}}\in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}\) is the input kernel, \(\boldsymbol{\mathcal{C}}\in\mathbb{R}^{U\times P\times k_{C}\times k_{C}}\) is the output kernel, and \(\boldsymbol{\mathcal{D}}\in\mathbb{R}^{U\times U\times k_{D}\times k_{D}}\) is the feedthrough kernel. For simplicity, we pad the convolution to ensure the same spatial resolution, \(H\times W\), is maintained in the states and outputs. Similarly, given a sequence of \(L\) inputs, \(\boldsymbol{\mathcal{U}}_{1:L}\in\mathbb{R}^{L\times H^{\prime}\times W^{\prime }\times U}\), we define a discrete-time convolutional state space model as

\[\boldsymbol{\mathcal{X}}_{k} =\overline{\boldsymbol{\mathcal{A}}}*\boldsymbol{\mathcal{X}}_{k -1}+\overline{\boldsymbol{\mathcal{B}}}*\boldsymbol{\mathcal{U}}_{k}\] (7) \[\boldsymbol{\mathcal{Y}}_{k} =\boldsymbol{\mathcal{C}}*\boldsymbol{\mathcal{X}}_{k}+\boldsymbol {\mathcal{D}}*\boldsymbol{\mathcal{U}}_{k}\] (8)

where \(\overline{\boldsymbol{\mathcal{A}}}\in\mathbb{R}^{P\times P\times k_{A}\times k _{A}}\) and \(\overline{\boldsymbol{\mathcal{B}}}\in\mathbb{R}^{P\times U\times k_{B}\times k _{B}}\) denote that these kernels are in discrete-time.

### Parallelizing Convolutional Recurrences

ConvS5 leverages parallel scans to efficiently parallelize the recurrence in (7). As discussed in Section 2.3, this requires a binary associative operator. Given that convolutions are associative, we show:

**Proposition 1**.: _Consider a convolutional recurrence as in (7) and define initial parallel scan elements \(c_{k}=(c_{k,a},c_{k,b}):=(\overline{\bm{\mathcal{A}}},\overline{\bm{\mathcal{B} }}*\bm{\mathcal{U}}_{k})\). The binary operator, defined below, is associative._

\[q_{i}\vartriangleleft q_{j}:=(q_{j,a}\circ q_{i,a},\ q_{j,a}*q_{i,b}\ +\ q_{j,b}),\] (9)

_where \(\circ\) denotes convolution of two kernels, \(*\) denotes convolution and \(+\) is elementwise addition._

Proof.: See Appendix A.1. 

Therefore, in theory, we can use this binary operator with a parallel scan to compute the recurrence in (7). However, the binary operator,, requires convolving two \(k_{A}\times k_{A}\) resolution state kernels together. To maintain equivalence with the sequential scan, the resulting kernel will have resolution \(2k_{a}-1\times 2k_{a}-1\). This implies that the state kernel will grow during the parallel scan computations for general kernels with a resolution greater than \(1\times 1\). This allows the receptive field to grow in the time direction, a useful feature for capturing spatiotemporal context. However, this kernel growth is computationally infeasible for long sequences.

We address this challenge by taking further inspiration from deep SSMs. These methods opt for simple but computationally advantageous operations in the time direction (linear dynamics) and utilize more complex operations (nonlinear activations) in the depth direction of the model. These nonlinear activations allow a stack of SSM layers with linear dynamics to represent nonlinear systems. Analogously, we choose to use \(1\times 1\) state kernels and perform pointwise state convolutions for the convolutional recurrence of (7). When we stack multiple layers of these ConvSSMs, the receptive field grows in the depth direction of the network and allows the stack of layers to capture the spatiotemporal context [64]. Computationally, we now have a construction that can be parallelized with subquadratic complexity with respect to the sequence length.

**Proposition 2**.: _Given the effective inputs \(\overline{\bm{\mathcal{B}}}*\bm{\mathcal{U}}_{1:L}\in\mathbb{R}^{L\times H \times W\times P}\) and a pointwise state kernel \(\bm{\mathcal{A}}\in\mathbb{R}^{P\times P\times 1\times 1}\), the computational cost of computing the convolutional recurrence in Equation 7 with a parallel scan is \(\mathcal{O}\big{(}L(P^{3}+P^{2}HW)\big{)}\)._

Proof.: See Appendix A.2. 

Further, the ConvS5 implementation introduced below admits a diagonalized parameterization that reduces this cost to \(\mathcal{O}(LPHW)\). See Section 3.4 and Appendix B for more details.

Figure 2: The dynamics of a ConvSSM with pointwise state kernel (top) can be equivalently viewed as the dynamics of an SSM (bottom). See Proposition 3. Each ConvSSM state pixel evolves according to an SSM state update with shared state matrix, \(\bm{\Lambda}_{\mathrm{SSM}}\), and input matrix, \(\bm{\mathrm{B}}_{\mathrm{SSM}}\), that can be formed by reshaping the ConvSSMâ€™s state kernel and input kernel. This allows leveraging parameterization insights from deep SSMs [19, 41, 42, 20, 57] to equip ConvS5 to model long-range dependencies.

### Connection to State Space Models

Since the convolutions in (5-6) and (7-8) are linear operations, they can be described equivalently as matrix-vector multiplications by flattening the input and state tensors into vectors and using large, circulant matrices consisting of the kernel elements [65]. Thus, any ConvSSM can be described as a large SSM with a circulant dynamics matrix. However, we show here that the use of pointwise state kernels, as described in the previous section, provides an alternative SSM equivalence, which lends a special structure that can leverage the deep SSM parameterization/initialization ideas discussed in Section 2.2 for modeling long-range dependencies. We show that each pixel of the state, \(\bm{\mathcal{X}}(t)_{i,j}\in\mathbb{R}^{P}\), can be equivalently described as evolving according to a differential equation with a shared state matrix, \(\mathbf{A}_{\mathrm{SSM}}\), and input matrix, \(\mathbf{B}_{\mathrm{SSM}}\). See Figure 2.

**Proposition 3**.: _Consider a ConvSSM state update as in (5) with pointwise state kernel \(\bm{\mathcal{A}}\in\mathbb{R}^{P\times P\times 1\times 1}\), input kernel \(\bm{\mathcal{B}}\in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}\), and input \(\bm{\mathcal{U}}(t)\in\mathbb{R}^{H^{\prime}\times W^{\prime}\times U}\). Let \(\bm{\mathcal{U}}_{\mathrm{im2col}}(t)\in\mathbb{R}^{H\times W\times Uk_{B}^{2}}\) be the reshaped result of applying the Image to Column (im2col) [66, 67] operation on the input \(\bm{\mathcal{U}}(t)\). Then the dynamics of each state pixel of (5), \(\bm{\mathcal{X}}(t)_{i,j}\in\mathbb{R}^{P}\), evolve according to the following differential equation_

\[\bm{\mathcal{X}}^{\prime}(t)_{i,j}=\mathbf{A}_{\mathrm{SSM}}\bm{\mathcal{X}}( t)_{i,j}+\mathbf{B}_{\mathrm{SSM}}\bm{\mathcal{U}}_{im2col}(t)_{i,j}\] (10)

_where the state matrix, \(\mathbf{A}_{\mathrm{SSM}}\in\mathbb{R}^{P\times P}\), and input matrix, \(\mathbf{B}_{\mathrm{SSM}}\in\mathbb{R}^{P\times(Uk_{B}^{2})}\), can be formed by reshaping the state kernel, \(\bm{\mathcal{A}}\), and input kernel, \(\bm{\mathcal{B}}\), respectively._

Proof.: See Appendix A.3. 

Thus, to endow these SSMs with the same favorable long-range dynamical properties as S4/S5 methods, we initialize \(\mathbf{A}_{\mathrm{SSM}}\) with a HiPPO [62] inspired matrix and discretize with a learnable timescale parameter to obtain \(\overline{\mathbf{A}}_{\mathrm{SSM}}\) and \(\overline{\mathbf{B}}_{\mathrm{SSM}}\). Due to the equivalence of Proposition 3, we then reshape these matrices into the discrete ConvSSM state and input kernels of (7) to give the convolutional recurrence the same advantageous dynamical properties. We note that if the input, output and dynamics kernel widths are set to \(1\times 1\), then the ConvSSM formulation is equivalent to "convolving" an SSM across each individual sequence of pixels in the spatiotemporal sequence (this also has connections to the temporal component of S4ND [45] when applied to videos). However, inspired by ConvRNNs, we observed improved performance when leveraging the more general convolutional structure the ConvSSM allows and increasing the input/output kernel sizes to allow local spatial features to be mixed in the dynamical system. See ablations discussed in Section 5.3.

### Efficient ConvSSM for Long-Range Dependencies: ConvS5

Here, we introduce _ConvS5_, which combines ideas of parallelization of convolutional recurrences (Section 3.2) and the SSM connection (Section 3.3). ConvS5 is a ConvSSM that leverages parallel scans and deep SSM parameterization/initialization schemes. Given Proposition 3, we implicitly parameterize a pointwise state kernel, \(\bm{\mathcal{A}}\in\mathbb{R}^{P\times P\times 1\times 1}\) and input kernel \(\bm{\mathcal{B}}\in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}\) in (5) using SSM parameters as used by S5 [20], \(\mathbf{A}_{\mathrm{SS}}\in\mathbb{R}^{P\times P}\) and \(\mathbf{B}_{\mathrm{SS}}\in\mathbb{R}^{P\times(Uk_{B}^{2})}\). We discretize these S5 SSM parameters as discussed in Section 2.2 to give

\[\overline{\mathbf{A}}_{\mathrm{SS}}=\mathrm{DISCRETIZE}_{\mathrm{A}}( \mathbf{A}_{\mathrm{SS}},\bm{\mathcal{\Delta}}),\qquad\overline{\mathbf{B}}_ {\mathrm{SS}}=\mathrm{DISCRETIZE}_{\mathrm{B}}(\mathbf{A}_{\mathrm{SS}}, \mathbf{B}_{\mathrm{SS}},\bm{\mathcal{\Delta}}),\] (11)

and then reshape to give the ConvS5 state update kernels:

\[\overline{\mathbf{A}}_{\mathrm{SS}}\in\mathbb{R}^{P\times P}\xrightarrow{ \mathrm{reshape}}\overline{\bm{\mathcal{A}}}_{\mathrm{ConvS5}}\in\mathbb{R}^{P \times P\times 1\times 1}\] (12)

\[\overline{\mathbf{B}}_{\mathrm{SS}}\in\mathbb{R}^{P\times(Uk_{B}^{2})} \xrightarrow{\mathrm{reshape}}\overline{\bm{\mathcal{B}}}_{\mathrm{ConvS5}} \in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}.\] (13)

We then run the discretized ConvSSM system of (7- 8), using parallel scans to compute the recurrence. In practice, this setup allows us to parameterize ConvS5 using a diagonalized parameterization [41, 42, 20] which reduces the cost of applying the parallel scan in Proposition 2 to \(\mathcal{O}(LPHW)\). See Appendix B for a more detailed discussion of parameterization, initialization and discretization.

We define a ConvS5 layer as the combination of ConvS5 with a nonlinear function applied to the ConvS5 outputs. For example, for the experiments in this paper, we use ResNet[68] blocks for the nonlinear activations between layers. However, this is modular, and other choices such as ConvNext [69] or S4ND [45] blocks could easily be used as well. Finally, many ConvS5 layers can be stacked to form a deep spatiotemporal sequence model.

[MISSING_PAGE_FAIL:7]

environment benchmarks proposed in Yan et al. [13]. Finally, in Section 5.3, we discuss ablations that highlight the importance of ConvSS's parameterization.

### Long Horizon Moving-MNIST Generation

There are few existing benchmarks for training on and generating long spatiotemporal sequences. We develop a long-horizon Moving-MNIST [54] prediction task that requires training on 300-600 frames and accurately generating up to 1200 frames. This allows for a direct comparison of ConvS5, ConvRNNs and Transformers as well as an efficient attention alternative (Performer [33]) and CW-VAE [16], a temporally hierarchical RNN based method. We first train all models on 300 frames and then evaluate by conditioning on 100 frames before generating 1200. We repeat the evaluation after training on 600 frames. See Appendix D for more experiment details. We present the results after generating 800 and 1200 frames in Table 2. See Appendix C for randomly selected sample trajectories. ConvS5 achieves the best overall performance. When only trained on 300 frames, ConvLSTM and ConvS5 perform similarly when generating 1200 frames, and both outperform the Transformer. All methods benefit from training on the longer 600-frame sequence. However, the longer training length allows ConvS5 to significantly outperform the other methods across the metrics when generating 1200 frames.

In Table 2-bottom we revisit the theoretical properties of Table 1 and compare the empirical computational costs of the Transformer, ConvLSTM and ConvS5 on the 600 frame Moving-MNIST task. Although this specific ConvS5 configuration requires a few more FLOPs due to the convolution computations, ConvS5 is parallelizable during training (unlike ConvLSTM) and has fast autoregressive generation (unlike Transformer) -- training 3x faster than ConvLSTM and generating samples 400x faster than Transformers.

### Long-range 3D Environment Benchmarks

Yan et al. [13] introduced a challenging video prediction benchmark specifically designed to contain long-range dependencies. This is one of the first comprehensive benchmarks for long-range spatiotemporal modeling and consists of 300 frame videos of agents randomly traversing 3D environ

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Trained on 300 frames** & \multicolumn{4}{c}{\(100\to 800\)} & \multicolumn{4}{c}{\(100\to 1200\)} \\ Method & FVD \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & FVD \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline Transformer [24] & \(159\) & \(12.6\) & \(0.609\) & \(0.287\) & \(265\) & \(12.4\) & \(0.591\) & \(0.321\) \\ Performer [33] & \(234\) & \(13.4\) & \(0.652\) & \(0.379\) & \(275\) & \(13.2\) & \(0.592\) & \(0.393\) \\ CW-VAE [16] & \(\underline{104}\) & \(12.4\) & \(0.592\) & \(0.277\) & \(\mathbf{117}\) & \(12.3\) & \(0.585\) & \(0.286\) \\ ConvLSTM [17] & \(\underline{128}\) & \(\underline{15.0}\) & \(\underline{0.737}\) & \(0.169\) & \(\underline{187}\) & \(\mathbf{14.1}\) & \(\mathbf{0.706}\) & \(\mathbf{0.203}\) \\ ConvS5 & \(\mathbf{72}\) & \(\mathbf{16.0}\) & \(\mathbf{0.761}\) & \(\mathbf{0.156}\) & \(\underline{187}\) & \(\mathbf{14.5}\) & \(0.678\) & \(0.230\) \\ \hline \hline \multicolumn{10}{l}{**Trained on 600 frames**} \\ \hline Transformer & \(\mathbf{42}\) & \(13.7\) & \(0.672\) & \(0.207\) & \(\underline{91}\) & \(13.1\) & \(0.631\) & \(0.252\) \\ Performer & \(93\) & \(12.4\) & \(0.616\) & \(0.274\) & \(243\) & \(12.2\) & \(0.608\) & \(0.312\) \\ CW-VAE & \(94\) & \(12.5\) & \(0.598\) & \(0.269\) & \(107\) & \(12.3\) & \(0.590\) & \(0.280\) \\ ConvLSTM & \(91\) & \(\underline{15.5}\) & \(\underline{0.757}\) & \(0.149\) & \(137\) & \(\underline{14.6}\) & \(0.727\) & \(0.180\) \\ ConvS5 & \(\underline{47}\) & \(\mathbf{16.4}\) & \(\mathbf{0.788}\) & \(\mathbf{0.134}\) & \(\mathbf{71}\) & \(\mathbf{15.6}\) & \(\mathbf{0.763}\) & \(\mathbf{0.162}\) \\ \hline \hline \multicolumn{10}{l}{GFLOPS \(\downarrow\)} & Train Step Time (s) \(\downarrow\) & Train Cost (V100 days) \(\downarrow\) & Sample Throughput (frames/s) \(\uparrow\) \\ \hline Transformer & \(\mathbf{70}\) & \(\mathbf{0.77}(\mathbf{1.0}\times)\) & \(\mathbf{50}\) & \(\mathbf{0.21}\) (\(1.0\)x) & \\ ConvLSTM & \(\mathbf{65}\) & \(3.0(3.9\times)\) & \(150\) & & \(\mathbf{117}\) (\(\mathbf{557}\times\)) & \\ ConvS5 & \(97\) & \(0.93(1.2\times)\) & \(\mathbf{50}\) & & \(90\) (\(429\times\)) & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative evaluation on the Moving-MNIST dataset [54]. **Top**: To evaluate, we condition on 100 frames, and then show results after generating 800 and 1200 frames. An expanded Table 6 is included in Appendix C with more results, error bars and ablations. Bold scores indicate the best performance and underlined scores indicate the second best performance. **Bottom**: Computational cost comparison for the 600 frame task. Compare to Table 1.

ments in DMLab [99], Minecraft [100], and Habitat [101] environments. See Appendix C for more experimental details and Appendix E for more details on each dataset.

We train models using the same \(16\times 16\) vector-quantized (VQ) codes from the pretrained VQ-GANs [30] used for TECO and the other baselines in Yan et al. [13]. In addition to ConvS5 and the existing baselines, we also train a Transformer (without the TECO framework), Performer and S5. The S5 baseline serves as an ablation on ConvS5's convolutional tensor-structured approach. Finally, since TECO is essentially a training framework (specialized for Transformers), we also use ConvS5 and S5 layers as a drop-in replacement for the Transformer in TECO. Therefore, we refer to the original version of TECO as _TECO-Transformer_, the ConvS5 version as _TECO-ConvS5_ and the S5 version as _TECO-S5_. See Appendix D for detailed information on training procedures, architectures, and hyperparameters.

DMLabThe results for DMLab are presented in Table 3. Of the methods trained without the TECO framework in the top section of Table 3, ConvS5 outperforms all baselines, including RNN [90; 16], efficient attention [39; 33] and diffusion [15] approaches. ConvS5 also has much faster autoregressive generation than the Transformer. ConvS5 significantly outperforms S5 on all metrics, pointing to the value of the convolutional structure of ConvS5.

For the models trained with the TECO framework, we see that TECO-ConvS5 achieves essentially the same FVD and LPIPS as TECO-Transformer, while significantly improving PSNR and SSIM. Note the sample speed comparisons are less dramatic in this setting since the MaskGit [98] sampling procedure is relatively slow. Still, the sample throughput of TECO-ConvS5 and TECO-S5 remains constant, while TECO-Transformer's throughput decreases with sequence length.

Minecraft and HabitatTable 4 presents the results on the Minecraft and Habitat benchmarks. On Minecraft, TECO-ConvS5 achieves the best FVD and performs comparably to TECO-Transformer on the other metrics, outperfoTarming all other baselines. On Habitat, TECO-ConvS5 is the only method to achieve a comparable FVD to TECO-Transformer, while outperforming it on PSNR and SSIM.

### ConvS5 ablations

In Table 5 we present ablations on the convolutional structure of ConvS5. We compare different input and output kernel sizes for the ConvSSM and also compare the default ResNet activations to a channel mixing GLU [102] activation. Where possible, when reducing the sizes of the ConvSSM kernels, we redistribute parameters to the ResNet kernels or the GLU sizes to compare similar parameter counts. The results suggest more convolutional structure improves performance.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{4}{c}{DMLab} \\ Method & FVD \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & Sample Throughput (frames/s) \(\uparrow\) \\ \hline FitVid* [90] & \(176\) & \(12.0\) & \(0.356\) & \(0.491\) & - \\ CW-VAE* [16] & \(125\) & \(12.6\) & \(0.372\) & \(0.465\) & - \\ Perceiver AR* [39] & \(96\) & \(11.2\) & \(0.304\) & \(0.487\) & - \\ Latent FDM* [15] & \(181\) & \(17.8\) & \(0.588\) & \(0.222\) & - \\ Transformer [24] & \(97\) & \(19.9\) & \(0.619\) & \(0.123\) & \(9\) (\(1.0\times\)) \\ Performer [33] & \(80\) & \(17.3\) & \(0.513\) & \(0.205\) & \(7\) (\(0.8\times\)) \\ S5 [20] & \(221\) & \(19.3\) & \(0.641\) & \(0.162\) & \(28\) (\(3.1\times\)) \\ ConvS5 & \(\mathbf{66}\) & \(\mathbf{23.2}\) & \(\mathbf{0.769}\) & \(\mathbf{0.079}\) & \(\mathbf{56}\) (\(\mathbf{6.2}\times\)) \\ \hline TECO-Transformer* [13] & \(\mathbf{28}\) & \(22.4\) & \(0.709\) & \(0.155\) & \(16\) (\(1.8\times\)) \\ TECO-Transformer (our run) & \(\mathbf{28}\) & \(21.6\) & \(0.696\) & \(\mathbf{0.082}\) & \(16\) (\(1.8\times\)) \\ TECO-S5 & \(35\) & \(20.1\) & \(0.687\) & \(0.143\) & \(\mathbf{21}\) (\(\mathbf{2.3}\times\)) \\ TECO-ConvS5 & \(31\) & \(\mathbf{23.8}\) & \(\mathbf{0.803}\) & \(0.085\) & \(18\) (\(2.0\times\)) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative evaluation on the DMLab long-range benchmark [13]. Results from Yan et al. [13] are indicated with \(*\). Methods trained using the TECO [13] training framework are at the bottom of the table. TECO methods are slower to sample due to the MaskGit [98] procedure. The expanded Table 8 in Appendix C includes error bars and ablations.

[MISSING_PAGE_FAIL:10]

## References

* [1] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. _Advances in neural information processing systems_, 29, 2016.
* [2] Yi Xu, Longwen Gao, Kai Tian, Shuigeng Zhou, and Huyang Sun. Non-local ConvLSTM for video compression artifact reduction. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 7043-7052, 2019.
* [3] He Huang, Zheni Zeng, Danya Yao, Xin Pei, and Yi Zhang. Spatial-temporal ConvLSTM for vehicle driving intention prediction. _Tsinghua Science and Technology_, 27(3):599-609, 2021.
* [4] Xiaoyu Chen, Xingsheng Xie, and Da Teng. Short-term traffic flow prediction based on ConvLSTM model. In _2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)_, pages 846-850. IEEE, 2020.
* [5] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive Fourier neural operators. _arXiv preprint arXiv:2202.11214_, 2022.
* [6] Jonathan A Weyn, Dale R Durran, Rich Caruana, and Nathaniel Cresswell-Clay. Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models. _Journal of Advances in Modeling Earth Systems_, 13(7):e2021MS002502, 2021.
* [7] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [8] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. _arXiv preprint arXiv:1907.06571_, 2019.
* [9] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and Transformers. _arXiv preprint arXiv:2104.10157_, 2021.
* [10] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: context-aware controllable video synthesis. _Advances in Neural Information Processing Systems_, 34:14042-14055, 2021.
* [11] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In _International Conference on Learning Representations_, 2021.
* [12] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. _arXiv preprint arXiv:2003.04035_, 2020.
* [13] Wilson Yan, Danijar Hafner, Stephen James, and Pieter Abbeel. Temporally consistent video Transformer for long-term video prediction. _arXiv preprint arXiv:2210.02396_, 2022.
* [14] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive Transformer. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII_, pages 102-118. Springer, 2022.
* [15] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Dietrich Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [16] Vaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork variational autoencoders. _Advances in Neural Information Processing Systems_, 34:29246-29257, 2021.

* [17] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. _Advances in neural information processing systems_, 28, 2015.
* [18] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville. Delving deeper into convolutional networks for learning video representations. _arXiv preprint arXiv:1511.06432_, 2015.
* [19] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In _International Conference on Learning Representations_, 2021.
* [20] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In _The Eleventh International Conference on Learning Representations_, 2023.
* [21] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural Computation_, 9(8):1735-1780, 1997.
* [22] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. _arXiv preprint arXiv:1406.1078_, 2014.
* [23] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In _International Conference on Machine Learning_, pages 1310-1318. PMLR, 2013.
* [24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* [25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021.
* [26] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. ViViT: A video vision Transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6836-6846, 2021.
* [27] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martin-Martin, and Li Fei-Fei. Maskvit: Masked visual pre-training for video prediction. _arXiv preprint arXiv:2206.11894_, 2022.
* [28] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.
* [29] Jacob Walker, Ali Razavi, and Aaron van den Oord. Predicting video with VQVAE. _arXiv preprint arXiv:2103.01950_, 2021.
* [30] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming Transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12873-12883, 2021.
* [31] Reiner Pope, Sholto Douglas, Akanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling Transformer inference. _arXiv preprint arXiv:2211.05102_, 2022.
* [32] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359, 2022.
* [33] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with Performers. In _International Conference on Learning Representations_, 2021.

* [34] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast autoregressive Transformers with linear attention. In _International Conference on Machine Learning_, pages 5156-5165. PMLR, 2020.
* [35] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient Transformer. In _International Conference on Learning Representations_, 2020.
* [36] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document Transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* [37] Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for Transformers, 2020.
* [38] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* [39] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pages 4651-4664. PMLR, 2021.
* [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In _International Conference on Learning Representations_, 2021.
* [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In _Advances in Neural Information Processing Systems_, 2022.
* [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. In _Advances in Neural Information Processing Systems_, 2022.
* [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In _International Conference on Learning Representations_, 2023.
* [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 7616-7633. PMLR, 17-23 Jul 2022.
* [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Re. S4ND: Modeling images and videos as multidimensional signals with state spaces. In _Advances in Neural Information Processing Systems_, 2022.
* [46] Md Mohaimul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV_, pages 87-104, 2022.
* [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In _The Eleventh International Conference on Learning Representations_, 2023.
* [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. _arXiv preprint arXiv:2303.03982_, 2023.
* [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. _arXiv preprint arXiv:2212.12749_, 2022.
* [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry huppos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations_, 2023.

* [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In _The Eleventh International Conference on Learning Representations_, 2023.
* [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. _arXiv preprint arXiv:2212.10544_, 2022.
* [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. _arXiv preprint arXiv:2302.10866_, 2023.
* [54] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using LSTMs. In _International conference on machine learning_, pages 843-852. PMLR, 2015.
* [55] Arieh Iserles. _A first course in the numerical analysis of differential equations_. 44. Cambridge university press, 2009.
* [56] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. _Advances in Neural Information Processing Systems_, 34, 2021.
* [57] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. _arXiv preprint arXiv:2303.06349_, 2023.
* [58] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. On the universality of linear recurrences followed by nonlinear projections. _arXiv preprint arXiv:2307.11888_, 2023.
* [59] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In _International Conference on Learning Representations_, 2018.
* [60] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In _International Conference on Learning Representations_, 2017.
* [61] Tao Lei, Yu Zhang, Sida Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 4470-4481, 2018.
* [62] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. HiPPO: Recurrent memory with optimal polynomial projections. _Advances in Neural Information Processing Systems_, 33:1474-1487, 2020.
* [63] Guy Blelloch. Prefix sums and their applications. Technical report, Tech. rept. CMU-CS-90-190. School of Computer Science, Carnegie Mellon, 1990.
* [64] Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, and Petros Koumoutsakos. ContextVP: Fully context-aware video prediction. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 753-769, 2018.
* [65] Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning. _arXiv preprint arXiv:1603.07285_, 2016.
* [66] Kumar Chellapilla, Sidd Puri, and Patrice Simard. High performance convolutional neural networks for document processing. In _Tenth international workshop on frontiers in handwriting recognition_. Suvisoft, 2006.
* [67] Yangqing Jia. Learning semantic image representations at a large scale. 2014.
* [68] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.

* [69] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A Convnet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11976-11986, 2022.
* [70] Marijn F Stollenga, Wonmin Byeon, Marcus Liwicki, and Juergen Schmidhuber. Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation. _Advances in neural information processing systems_, 28, 2015.
* [71] Reza Azad, Maryam Asadi-Aghbolaghi, Mahmood Fathy, and Sergio Escalera. Bi-directional ConvLSTM U-net with densely connected convolutions. In _Proceedings of the IEEE/CVF international conference on computer vision workshops_, pages 0-0, 2019.
* [72] Si Woon Lee and Ha Young Kim. Stock market forecasting with super-high dimensional time-series data using ConvLSTM, trend sampling, and specialized data augmentation. _expert systems with applications_, 161:113704, 2020.
* [73] Qingqing Wang, Ye Huang, Wenjing Jia, Xiangjian He, Michael Blumenstein, Shujing Lyu, and Yue Lu. FACLSTM: ConvLSTM with focused attention for scene text recognition. _Science China Information Sciences_, 63:1-14, 2020.
* [74] Mohamadreza Bakhtyari and Sayeh Mirzaei. ADHD detection using dynamic connectivity patterns of EEG data and ConvLSTM with attention framework. _Biomedical Signal Processing and Control_, 76:103708, 2022.
* [75] Li Kang, Ziqi Zhou, Jianjun Huang, Wenzhong Han, and IEEE Member. Renal tumors segmentation in abdomen CT images using 3D-CNN and ConvLSTM. _Biomedical Signal Processing and Control_, 72:103334, 2022.
* [76] Tie Liu, Mai Xu, and Zulin Wang. Removing rain in videos: a large-scale database and a two-stream ConvLSTM approach. In _2019 IEEE International Conference on Multimedia and Expo (ICME)_, pages 664-669. IEEE, 2019.
* [77] Xiaofang Xia, Jian Lin, Qiannan Jia, Xiaoluan Wang, Chaofan Ma, Jiangtao Cui, and Wei Liang. ETD-ConvLSTM: A deep learning approach for electricity theft detection in smart grids. _IEEE Transactions on Information Forensics and Security_, 2023.
* [78] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Deep learning for precipitation nowcasting: A benchmark and a new model. _Advances in neural information processing systems_, 30, 2017.
* [79] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine. Stochastic variational video prediction. In _International Conference on Learning Representations_, 2018.
* [80] Yunbo Wang, Haixu Wu, Jianjin Zhang, Zhifeng Gao, Jianmin Wang, S Yu Philip, and Mingsheng Long. PredRNN: A recurrent neural network for spatiotemporal predictive learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(2):2208-2225, 2022.
* [81] Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, and S Yu Philip. PredRNN++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning. In _International Conference on Machine Learning_, pages 5123-5132. PMLR, 2018.
* [82] Yunbo Wang, Lu Jiang, Ming-Hsuan Yang, Li-Jia Li, Mingsheng Long, and Li Fei-Fei. Eidetic 3D LSTM: A model for video prediction and beyond. In _International conference on learning representations_, 2019.
* [83] Wei Yu, Yichao Lu, Steve Easterbrook, and Sanja Fidler. Efficient and information-preserving future frame prediction and beyond. In _International Conference on Learning Representations_, 2020.
* [84] Jiahao Su, Wonmin Byeon, Jean Kossaifi, Furong Huang, Jan Kautz, and Anima Anandkumar. Convolutional tensor-train LSTM for spatio-temporal learning. _Advances in Neural Information Processing Systems_, 33:13714-13726, 2020.

* [85] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. _arXiv preprint arXiv:2303.14526_, 2023.
* [86] Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In _International conference on machine learning_, pages 1747-1756. PMLR, 2016.
* [87] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.
* [88] Ozgun Cicek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-net: learning dense volumetric segmentation from sparse annotation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19_, pages 424-432. Springer, 2016.
* [89] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. _Transactions on Machine Learning Research_.
* [90] Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. FitVid: Overfitting in pixel-level video prediction. _arXiv preprint arXiv:2106.13195_, 2021.
* [91] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video Transformer. _arXiv preprint arXiv:2006.10704_, 2020.
* [92] Younggyo Seo, Kimin Lee, Fangchen Liu, Stephen James, and Pieter Abbeel. HARP: Autoregressive latent video prediction with high-fidelity image generator. In _2022 IEEE International Conference on Image Processing (ICIP)_, pages 3943-3947. IEEE, 2022.
* [93] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. StyleGAN-V: A continuous video generator with the price, image quality and perks of StyleGAN2. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3626-3636, 2022.
* [94] Masaki Saito and Shunta Saito. TGANv2: Efficient training of large models for video generation with multiple subsampling layers. _arXiv preprint arXiv:1811.09245_, 2(6), 2018.
* [95] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In _International Conference on Learning Representations_, 2022.
* [96] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. _Advances in Neural Information Processing Systems_, 35:31769-31781, 2022.
* [97] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via Transformers. _arXiv preprint arXiv:2205.15868_, 2022.
* [98] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGit: Masked generative image Transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11315-11325, 2022.
* [99] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler, Andrew Lefrancq, Simon Green, Victor Valdes, Amir Sadik, et al. Deepmind Lab. _arXiv preprint arXiv:1612.03801_, 2016.
* [100] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. MineRL: A large-scale dataset of Minecraft demonstrations. _arXiv preprint arXiv:1907.13440_, 2019.

* [101] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied AI research. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9339-9347, 2019.
* [102] Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In _International Conference on Machine Learning_, pages 933-941. PMLR, 2017.
* [103] Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, and Zehuan Yuan. Designing BERT for convolutional networks: Sparse and hierarchical masked modeling. _arXiv preprint arXiv:2301.03580_, 2023.
* [104] Sunghyun Park, Kangyeol Kim, Junsoo Lee, Jaegul Choo, Joonseok Lee, Sookyung Kim, and Edward Choi. Vid-ODE: Continuous-time video generation with neural ordinary differential equation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 2412-2422, 2021.
* [105] Mark Harris, Shubhabrata Sengupta, and John D Owens. Parallel prefix sum (scan) with CUDA. _GPU gems_, 3(39):851-876, 2007.
* [106] Albert Gu, Isys Johnson, Aman Tamalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In _International Conference on Learning Representations_, 2023.
* [107] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [108] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.
* [109] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.
* [110] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.
* [111] Yann LeCun. The MNIST database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* [112] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-Matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI. _arXiv preprint arXiv:2109.08238_, 2021.
* [113] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. _arXiv preprint arXiv:1709.06158_, 2017.
* [114] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9068-9079, 2018.

## Appendix for: Convolutional State Space Models for Long-range Spatiotemporal Modeling

**Contents:**

* **Appendix A**: Propositions
* **Appendix B**: ConvS5 Details: Parameterization, Initialization, Discretization
* **Appendix C**: Supplementary Results
* **Appendix D**: Experiment Configurations
* **Appendix E**: DatasetsPropositions

### Parallel Scan for Convolutional Recurrences

**Proposition 1**.: _Consider a convolutional recurrence as in (7) and define initial parallel scan elements \(c_{k}=(c_{k,a},c_{k,b}):=(\overline{\bm{\mathcal{A}}},\overline{\bm{\mathcal{B}} }*\bm{\mathcal{U}}_{k})\). The binary operator \(\varocc{k}\), defined below, is associative._

\[c_{i}\varocc{k}_{j}:=(c_{j,a}\circ c_{i,a},\ c_{j,a}*c_{i,b}\ +\ c_{j,b}),\] (14)

_where \(\circ\) denotes convolution of two kernels, \(*\) denotes convolution between a kernel and input, and \(+\) is elementwise addition._

Proof.: Using that \(\circ\) is associative and the companion operator of \(*\), i.e. \((d\circ e)*f=d*(e*f)\) (see Blelloch [63], Section 1.4), we have:

\[(c_{i}\varocc{c}_{j})\varocc{k}_{k} =(c_{j,a}\circ c_{i,a},\ c_{j,a}*c_{i,b}\ +\ c_{j,b})\varocc{k,a}(c_{k,a},c_{k,b})\] (15) \[=\left(c_{k,a}\circ(c_{j,a}\circ c_{i,a}),\ \ c_{k,a}*(c_{j,a}*c_{i,b}+c_{j,b})+c_{k,b}\right)\] (16) \[=\left((c_{k,a}\circ c_{j,a})\circ c_{i,a},\ \ c_{k,a}*(c_{j,a}*c_{i,b})+c_{k,a}*c_{j,b}+c_{k,b}\right)\] (17) \[=\left((c_{k,a}\circ c_{j,a})\circ c_{i,a},\ \ (c_{k,a}\circ c_{j,a})*c_{i,b}+c_{k,a}*c_{j,b}+c_{k,b}\right)\] (18) \[=c_{i}\varocc{k}(c_{k,a}\circ c_{j,a},\ \ c_{k,a}*c_{j,b}+c_{k,b})\] (19) \[=c_{i}\varocc{k}(c_{j}\varocc{k})\] (20)

### Computational Cost of Parallel Scan for Convolutional Recurrences

**Proposition 2**.: _Given the effective inputs \(\overline{\bm{\mathcal{B}}}*\bm{\mathcal{U}}_{1:L}\in\mathbb{R}^{L\times H \times W\times P}\) and a pointwise state kernel \(\bm{\mathcal{A}}\in\mathbb{R}^{P\times P\times 1\times 1}\), the computational cost of computing the convolutional recurrence in Equation 7 with a parallel scan is \(\mathcal{O}\big{(}L(P^{3}+P^{2}HW)\big{)}\)._

Proof.: Following Blelloch [63], given a single processor, the cost of computing the recurrence sequentially using the binary operator \(\varocc{k}\) defined in Proposition 1 is \(\mathcal{O}\big{(}L(T_{\circ}+T_{*}+T_{+})\big{)}\) where \(T_{\circ}\) refers to the cost of convolving two kernels, \(T_{*}\) is the cost of convolution between a kernel and input and \(T_{+}\) is the cost of elementwise addition. The cost of elementwise addition is \(T_{+}=\mathcal{O}(PHW)\). For state kernels with resolution \(k_{A}\), \(T_{\circ}=\mathcal{O}(P^{3}k_{A}^{4})\) and \(T_{*}=\mathcal{O}(P^{2}k_{A}^{2}HW)\). For pointwise convolutions this becomes \(T_{\circ}=\mathcal{O}(P^{3})\) and \(T_{*}=\mathcal{O}(P^{2}HW)\). Thus, the cost of computing the recurrence sequentially using \(\varocc{k}\) is \(\mathcal{O}\big{(}L(P^{3}+P^{2}HW)\big{)}\). Since there are work-efficient algorithms for parallel scans [105], the overall cost of the parallel scan is also \(\mathcal{O}\big{(}L(P^{3}+P^{2}HW)\big{)}\). 

Note that ConvS5's diagonalized parameterization discussed in Section 3.4 and Appendix B leads to \(T_{\circ}=\mathcal{O}(P)\) and \(T_{*}=\mathcal{O}(PHW)\). Therefore the cost of applying the parallel scan with ConvS5 is \(\mathcal{O}(LPHW)\).

### Connection Between ConvSSMs and SSMs

**Proposition 3**.: _Consider a ConvSSM state update as in (5) with pointwise state kernel \(\bm{\mathcal{A}}\in\mathbb{R}^{P\times P\times 1\times 1}\), input kernel \(\bm{\mathcal{B}}\in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}\), and input \(\bm{\mathcal{U}}(t)\in\mathbb{R}^{H^{\prime}\times W^{\prime}\times U}\). Let \(\bm{\mathcal{U}}_{\mathrm{im2col}}(t)\in\mathbb{R}^{H\times W\times Uk_{B}^{2}}\) be the reshaped result of applying the Image to Column (im2col) [66, 67] operation on the input \(\bm{\mathcal{U}}(t)\). Then the dynamics of each state pixel of (5), \(\bm{\mathcal{X}}(t)_{i,j}\in\mathbb{R}^{P}\), evolve according to the following differential equation_

\[\bm{\mathcal{X}}^{\prime}(t)_{i,j}=\mathbf{A}_{\mathrm{SSM}}\bm{\mathcal{X}}(t )_{i,j}+\mathbf{B}_{\mathrm{SSM}}\bm{\mathcal{U}}_{\mathrm{im2col}}(t)_{i,j}\] (21)

_where the state matrix, \(\mathbf{A}_{\mathrm{SSM}}\in\mathbb{R}^{P\times P}\), and input matrix, \(\mathbf{B}_{\mathrm{SSM}}\in\mathbb{R}^{P\times(Uk_{B}^{2})}\), can be formed by reshaping the state kernel, \(\bm{\mathcal{A}}\) and input kernel, \(\bm{\mathcal{B}}\), respectively._Proof.: Let \(\mathbf{U}_{\mathrm{im2col}}\in\mathbb{R}^{Uk_{b}^{2}\times HW}\) denote the result of performing the im2col operation on the input \(\bm{\mathcal{U}}(t)\) for convolution with the kernel \(\bm{\mathcal{B}}\). Reshape this matrix into the tensor \(\bm{\mathcal{U}}_{\mathrm{im2col}}(t)\in\mathbb{R}^{H\times W\times Uk_{b}^{2}}\). Reshape \(\bm{\mathcal{U}}_{\mathrm{im2col}}(t)\) once more into the tensor \(\bm{\mathcal{V}}(t)\in\mathbb{R}^{H\times W\times U\times k_{B}\times k_{B}}\).

Now, we can write the evolution for the individual channels of each pixel, \(\bm{\mathcal{X}}^{\prime}(t)_{i,j,k}\), in (5) as

\[\bm{\mathcal{X}}^{\prime}(t)_{i,j,k}=\sum_{l=0}^{P-1}\bm{\mathcal{A}}_{k,l,0, 0}\bm{\mathcal{X}}(t)_{i,j,l}+\sum_{q=0}^{U-1}\sum_{m=0}^{k_{B}-1}\sum_{n=0}^ {k_{B}-1}\bm{\mathcal{B}}_{k,q,m,n}\bm{\mathcal{V}}(t)_{i,j,q,m,n}.\] (22)

Let \(\mathbf{A}_{\mathrm{SSM}}\in\mathbb{R}^{P\times P}\) be a matrix with rows, \(\mathbf{A}_{\mathrm{SSM},i}\in\mathbb{R}^{P}\), corresponding to a flattened version of the output features of \(\bm{\mathcal{A}}\), i.e. \(\bm{\mathcal{A}}_{i}\in\mathbb{R}^{P\times 1\times 1}\). Similarly, reshape \(\bm{\mathcal{B}}\) into a matrix \(\mathbf{B}_{\mathrm{SSM}}\in\mathbb{R}^{P\times(Uk_{B}^{2})}\) where the rows, \(\mathbf{B}_{\mathrm{SSM},i}\in\mathbb{R}^{Uk_{B}^{2}}\) correspond to a flattened version of the output features of \(\bm{\mathcal{B}}\), i.e. \(\bm{\mathcal{B}}_{i}\in\mathbb{R}^{U\times k_{B}\times k_{B}}\).

Then we can rewrite (22) equivalently as

\[\bm{\mathcal{X}}^{\prime}(t)_{i,j,k} =\sum_{l=0}^{P-1}\bm{\mathcal{A}}_{k,l,0,0}\bm{\mathcal{X}}(t)_{i, j,l}+\sum_{q=0}^{U-1}\sum_{m=0}^{k_{B}-1}\sum_{n=0}^{k_{B}-1}\bm{\mathcal{B}}_{k,q,m,n }\bm{\mathcal{V}}(t)_{i,j,q,m,n}\] (23) \[=\sum_{l=0}^{P-1}\mathbf{A}_{\mathrm{SSM},k,l}\bm{\mathcal{X}}(t) _{i,j,l}+\sum_{v=0}^{Uk_{B}^{2}-1}\mathbf{B}_{\mathrm{SSM},k,v}\bm{\mathcal{U} }_{\mathrm{im2col}}(t)_{i,j,v}\] (24) \[=\mathbf{A}_{\mathrm{SSM},k}^{T}\bm{\mathcal{X}}(t)_{i,j}+\mathbf{ B}_{\mathrm{SSM},k}^{T}\bm{\mathcal{U}}_{\mathrm{im2col}}(t)_{i,j}\] (25)

## Appendix B ConvS5 Details: Parameterization, Discretization, Initialization

### Background: S5

S5 Parameterization and DiscretizationS5[20] uses a diagonalized parameterization of the general SSM in (3).

Let \(\mathbf{A}_{\mathrm{S5}}=\mathbf{V}\mathbf{\Lambda}_{\mathrm{S5}}\mathbf{V}^{- \mathbf{1}}\in\mathbb{R}^{P\times P}\) where \(\mathbf{\Lambda}_{\mathrm{S5}}\in\mathbb{C}^{P\times P}\) is a complex-valued diagonal matrix and \(\mathbf{V}\in\mathbb{C}^{P\times P}\) corresponds to the eigenvectors. Defining \(\tilde{\mathbf{x}}(t)=\mathbf{V}^{-1}\mathbf{x}(t)\), \(\tilde{\mathbf{B}}=\mathbf{V}^{-1}\mathbf{B}\), and \(\tilde{\mathbf{C}}=\mathbf{CV}\) we can reparameterize the SSM of (3) as the diagonalized system:

\[\frac{\mathrm{d}\tilde{\mathbf{x}}(t)}{\mathrm{d}t}=\mathbf{\Lambda}_{\mathrm{ S5}}\tilde{\mathbf{x}}(t)+\tilde{\mathbf{B}}\mathbf{u}(t),\qquad\mathbf{y}(t)= \tilde{\mathbf{C}}\tilde{\mathbf{x}}(t)+\mathbf{D}\mathbf{u}(t).\] (26)

S5 uses learnable timescale parameters \(\mathbf{\Delta}\in\mathbb{R}^{P}\) and the following zero-order hold (ZOH) disretization:

\[\overline{\mathbf{\Lambda}}_{\mathrm{S5}} =\mathrm{DISCRETIZE}_{\mathrm{A}}(\mathbf{\Lambda}_{\mathrm{S5}},\mathbf{\Delta}):=e^{\mathbf{\Lambda}_{\mathrm{S5}}\mathbf{\Delta}}\] (27) \[\overline{\mathbf{B}}_{\mathrm{S5}} =\mathrm{DISCRETIZE}_{\mathrm{B}}(\mathbf{\Lambda}_{\mathrm{S5}},\widetilde{\mathbf{B}},\mathbf{\Delta}):=\mathbf{\Lambda}_{\mathrm{S5}}^{-1} (\overline{\mathbf{\Lambda}}_{\mathrm{S5}}-\mathbf{I})\tilde{\mathbf{B}}\] (28)

S5 InitializationS5 initializes its state matrix by diagonalizing \(\mathbf{\Lambda}_{\mathrm{S5}}\) as defined here:

\[\mathbf{\Lambda}_{\mathrm{S5}_{nk}}=-\begin{cases}(n+\frac{1}{2})^{1/2}(k+ \frac{1}{2})^{1/2},&n>k\\ \frac{1}{2},&n=k\\ (n+\frac{1}{2})^{1/2}(k+\frac{1}{2})^{1/2},&n<k\end{cases}.\] (29)

This matrix is the normal part of the normal plus low-rank HiPPO-LegS matrix from the HiPPO framework [62] for online function approximation. S4 originally initialized its single-input, single-output (SISO) SSMs with a representation of the full HiPPO-LegS matrix. This was shown to be approximating long-range dependencies at initialization with respect to an infinitely long, exponentially-decaying measure [106]. Gupta et al. [41] empirically showed that the low-rank terms could be removed without impacting performance. Gu et al. [42] showed that in the limit of infinite state dimension, the linear, single-input ODE with this normal approximation to the HiPPO-LegS matrix produces the same dynamics as the linear, single-input ODE with the full HiPPO-LegS matrix. The S5 work extended these findings to the multi-input SSM setting [20].

Importance of SSM Parameterization, Discretization and InitializationPrior research has highlighted the importance of parameterization, discretization and initialization choices of deep SSM methods through ablations and analysis [56, 19, 42, 20, 57]. Concurrent work from Orvieto et al. [57] provides particular insight into the favorable initial eigenvalue distributions provided by initializing with HiPPO-inspired matrices as well as an important normalization effect provided by the explicit discretization procedure. They also introduce a purely discrete-time parameterization that can perform similarly to the continuous-time discretization of S4 and S5. However, their parameterization practically ends up quite similar to the equations of (27-28). We choose to use the continuous-time parameterization of S5 for the implicit parameterization of ConvS5 since it can also be leveraged for zero-shot resolution changes [19, 20, 45] and processing irregularly sampled time-series in parallel [20]. However, due to Proposition 3, other long-range SSM parameterization strategies can also be used, such as in Orvieto et al. [57] or potential future innovations.

### ConvS5 Diagonalization

We leverage S5's diagonalized parameterization to reduce the cost of the parallel scan of ConvS5.

Concretely, we initialize \(\mathbf{A}_{\mathrm{S5}}\) as in (29) and diagonalize as \(\mathbf{A}_{\mathrm{S5}}=\mathbf{V}\mathbf{\Lambda}_{\mathrm{S5}}\mathbf{V}^{- \mathbf{1}}\). To apply ConvS5, we compute \(\overline{\mathbf{\Lambda}}_{\mathrm{S5}}\) and \(\overline{\mathbf{B}}_{\mathrm{S5}}\) using (27-28), and then form the ConvS5 state and input kernels:

\[\overline{\mathbf{\Lambda}}_{\mathrm{S5}}\in\mathbb{R}^{P\times P} \xrightarrow{\mathrm{reshape}} \overline{\mathbf{\mathcal{A}}}_{\mathrm{ConvS5}}\in\mathbb{R}^{P\times P \times 1\times 1}\] (30) \[\overline{\mathbf{B}}_{\mathrm{S5}}\in\mathbb{R}^{P\times(UK_{B}^{ 2})} \xrightarrow{\mathrm{reshape}}\overline{\mathbf{\mathcal{B}}}_{\mathrm{ ConvS5}}\in\mathbb{R}^{P\times U\times k_{B}\times k_{B}}.\] (31)

See Listing 1 for an example of the core implementation. Note, the state kernel \(\overline{\mathbf{\mathcal{A}}}_{\mathrm{ConvS5}}\) is "diagonalized" in the sense that all entries in the state kernel are zero except \(\overline{\mathbf{\mathcal{A}}}_{\mathrm{ConvS5},i,i}=\overline{\mathbf{ \Lambda}}_{\mathrm{S5},i,i}\;\;\forall i\in[P]\).

This means that the pointwise convolutions reduce to channel-wise multiplications. However, this does not reduce expressivity compared to a full pointwise convolution. This is because, given the ConvSSM to SSM equivalence of Proposition 3 and the use of complex-valued kernels, the diagonalization maintains expressivity since almost all SSMs are diagonalizable [41, 42], which follows from the well-known fact that almost all square matrices diagonalize over the complex plane.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

Figure 3: Moving-MNIST Samples: 1200 frames generated conditioned on 100.

[MISSING_PAGE_EMPTY:26]

## 6 Conclusion

Figure 4: DMLab SamplesFigure 5: Minecraft Samples

Figure 6: Habitat Samples

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & & \multicolumn{4}{c}{Minecraft} \\ Method & Params & FVD \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline FitVid* & 176M & \(956\pm 15.8\) & \(13.0\pm 0.0089\) & \(0.343\pm 0.00380\) & \(0.519\pm 0.00367\) \\ CW-VAE* & 140M & \(397\pm 15.5\) & \(13.4\pm 0.0610\) & \(0.338\pm 0.00274\) & \(0.441\pm 0.00367\) \\ Perceiver AR* & 166M & \(76.3\pm 1.72\) & \(13.2\pm 0.0711\) & \(0.323\pm 0.00336\) & \(0.441\pm 0.00207\) \\ Latent FDM* & 33M & \(167\pm 6.26\) & \(13.4\pm 0.0904\) & \(0.349\pm 0.00327\) & \(0.429\pm 0.00284\) \\ TECO-Transformer* & 274M & \(116\pm 5.08\) & \(\mathbf{15.4\pm 0.0603}\) & \(\mathbf{0.381\pm 0.00192}\) & \(\mathbf{0.340\pm 0.00264}\) \\ TECO-ConvS5 & 214M & \(\mathbf{70.7\pm 3.05}\) & \(14.8\pm 0.0984\) & \(0.374\pm 0.00414\) & \(0.355\pm 0.00467\) \\ \hline \multicolumn{5}{c}{Habitat} \\ Method & Params & FVD \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline Perceiver AR* & 200M & \(164\pm 12.6\) & \(12.8\pm 0.0423\) & \(\mathbf{0.405\pm 0.00248}\) & \(0.676\pm 0.00282\) \\ Latent FDM* & 87M & \(433\pm 2.67\) & \(12.5\pm 0.0121\) & \(0.311\pm 0.00083\) & \(\mathbf{0.582\pm 0.00049}\) \\ TECO-Transformer* & 386M & \(\mathbf{76.3\pm 1.72}\) & \(12.8\pm 0.0139\) & \(0.363\pm 0.00122\) & \(0.604\pm 0.00451\) \\ TECO-ConvS5 & 351M & \(95.1\pm 3.74\) & \(\mathbf{12.9\pm 0.212}\) & \(0.390\pm 0.01238\) & \(0.632\pm 0.00823\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Model runtime comparison for 3D Environment results in Tables 8-10. The implementations of the baselines FitVid, CW-VAE, Perceiver AR and Latent FDM used in the TECO work [13] are not publicly available in the TECO repository, so we were unable to include direct runtime comparisons for those methods.

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Train Step Time (s) \(\downarrow\) & 
\begin{tabular}{c} DMLab \\ Sampling Speed (frames/s) \\ \end{tabular} \\ \hline Transformer & \(\mathbf{1.25}\)\((\mathbf{1.0\times})\) & \(9.1\)\((1.0\times)\) \\ Performer & \(1.25\)\((1.0\times)\) & \(7.6\)\((0.8\times)\) \\ S5 & \(1.34\)\((1.1\times)\) & \(28\)\((3.1\times)\) \\ ConvS5 & \(2.31\)\((1.8\times)\) & \(\mathbf{56}\)\((\mathbf{6.2\times})\) \\ \hline TECO-Transformer & \(\mathbf{0.75}\)\((\mathbf{0.6\times})\) & \(16\)\((1.8\times)\) \\ TECO-S5 & \(0.81\)\((0.7\times)\) & \(\mathbf{21}\)\((\mathbf{2.3\times})\) \\ TECO-ConvS5 & \(1.17\)\((0.9\times)\) & \(18\)\((2.0\times)\) \\ \hline \multicolumn{3}{c}{Minecraft} \\ Method & Train Step Time (s) \(\downarrow\) & Sampling Speed (frames/s) \\ \hline TECO-Transformer & \(\mathbf{1.91}\)\((\mathbf{1.0\times})\) & \(8.1\)\((1.0\times)\) \\ TECO-ConvS5 & \(2.53\)\((1.3\times)\) & \(\mathbf{14}\)\((\mathbf{1.7\times})\) \\ \hline \multicolumn{3}{c}{Habitat} \\ Method & Train Step Time (s) \(\downarrow\) & Sampling Speed (frames/s) \\ \hline TECO-Transformer & \(\mathbf{2.71}\)\((\mathbf{1.0\times})\) & \(6.8\)\((1.0\times)\) \\ TECO-ConvS5 & \(3.10\)\((1.1\times)\) & \(\mathbf{11}\)\((\mathbf{1.6\times})\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Full results on the Minecraft and Habitat long-range benchmark datasets [13]. Results from Yan et al. [13] are indicated with \(*\). Note that Yan et al. [13] did not evaluate FitVid or CW-VAE on Habitat due to cost.

Experiment Configurations

Our codebase modifies the TECO codebase from Yan et al. [13] and we reuse their core Transformer and TECO framework implementations. More architectural details and dataset-specific details are described below.

### Spatiotemporal Sequence Model Architectures

ConvS5, ConvLSTM and S5 models are formed by stacking multiple ConvS5, ConvLSTM or S5 layers, respectively. For each of these models, layer normalization [107] with a post-norm setup is used along with residual connections. For the Transformer, we use the Transformer implementation from Yan et al. [13] which consists of a stack of multi-head attention layers.

ConvS5 and ConvLSTM are applied directly to sequences of frames of shape [sequence length, latent height, latent width, latent features], where the original data has been convolved to a latent resolution and latent number of features. Since S5 and Transformer act on vector-valued sequences, these models require an additional downsampling convolution operation to project the latent frames into a token and an upsampling transposed convolution operation to project the Transformer backbone output tokens back into latent frames. We use the same sequence of compression operations for this as in Yan et al. [13]. The Encoder and Decoder referred to for all models in the hyperparameter tables below consist of ResNet Blocks with \(3\times 3\) kernels as implemented in Yan et al. [13].

### Evaluation Metrics

We follow Yan et al. [13] and evaluate methods by computing Frechet Video Distance (FVD) [108], peak signal-to-noise ratio (PSNR), structural similarity index measure [109] and Learned Perceptual Image Patch Similarity (LPIPS) [110] between sampled trajectories and ground truth trajectories. See Yan et al. [13] for a more in-depth discussion of the use of these metrics for the 3D environment benchmarks.

### Compute

All models were trained with 32GB NVIDIA V100 GPUs. For Moving-MNIST, models were trained with 8 V100s. For all other experiments, models were trained with 16 V100s. We list V100 days in the hyperparameters, which denotes the number of days it would take to train on a single V100.

### Moving-MNIST

All models were trained to minimize L1+L2 loss over the frames directly in pixel space, as in Su et al. [84]. We trained models on 300 frames. We then repeated the experiment and trained models on 600 frames. For ConvS5 and ConvLSTM, we fixed the hidden dimensions (layer input/output features) and state sizes to be 256, and we swept over the following learning rates [\(1\times 10^{-4}\), \(5\times 10^{-4}\), \(1\times 10^{-3}\)] and chose the best model. For Transformer, we swept over model size, considering hidden dimensions of [512, 2014] and learning rates [\(1\times 10^{-4}\), \(5\times 10^{-4}\), \(1\times 10^{-3}\)] and chose the best model. We also observed better performance for the Transformer by convolving frames down to an \(8\times 8\) latent resolution (rather than the \(16\times 16\) used by ConvS5 and ConvLSTM) before downsampling to a token. All other relevant training parameters were kept the same between the three methods. See Tables 12-14 for detailed experiment configurations.

Each model was evaluated by collecting 1024 trajectories using the following procedure: condition on 100 frames from the ground truth test set, then generate forward 1200 frames. These samples were compared with the ground truth to compute FVD, PSNR, SSIM and LPIPS.

The ConvSSM ablation was performed using the exact settings as ConvS5, except the state kernel was initialized with a Gaussian and we swept over the following learning rates [\(1\times 10^{-4}\), \(5\times 10^{-4}\), \(1\times 10^{-3}\)].

[MISSING_PAGE_EMPTY:31]

\begin{table}
\begin{tabular}{c l c c} \hline \hline Hyperparameters & Moving-MNIST-300 & Moving-MNIST-600 \\ \hline V100 Days & 25 & 50 \\ Params & 164M & 164M \\ Input Resolution & \(64\times 64\) & \(64\times 64\) \\ Latent Resolution & \(8\times 8\) & \(8\times 8\) \\ Batch Size & 8 & 8 \\ Sequence Length & 300 & 600 \\ LR & \(5\times 10^{-4}\) & \(1\times 10^{-4}\) \\ LR Schedule & cosine & cosine \\ Warmup Steps & 5k & 5k \\ Max Training Steps & 300K & 300K \\ Weight Decay & \(1\times 10^{-5}\) & \(1\times 10^{-5}\) \\ \hline Encoder & Depths & 64, 128, 256, 512 & 64, 128, 256, 512 \\ Blocks & 1 & 1 \\ \hline Decoder & Depths & 64, 128, 256, 512 & 64, 128, 256, 512 \\ Blocks & 1 & 1 \\ \hline \multirow{4}{*}{Temporal Transformer} & Downsample Factor & 8 & 8 \\ Hidden Dim & 1024 & 1024 \\ Feedforward Dim & 4096 & 4096 \\ Heads & 16 & 16 \\ Layers & 8 & 8 \\ Dropout & 0 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Experiment Configuration for Transformer on Moving-MNIST experiments

### Long-Range 3D Environment Benchmarks

We follow the procedures from Yan et al. [13] and train models on the same pre-trained vector-quantized (VQ) \(16\times 16\) codes used by the baselines evaluated in that work. Models were trained to optimize a cross-entropy reconstruction loss between the predictions and true VQ codes. The evaluation of DMLab and Habitat involves both an action-conditioned and unconditioned setting. Therefore, as in Yan et al. [13], the actions were randomly dropped out half the time during training on these datasets.

After training, we follow the procedure from Yan et al. [13] for evaluation in two different settings. The first setting involves computing PSNR, SSIM and LPIPS from 1024 samples generated by conditioning on the first 144 frames and then generating the next 156 frames while providing the model with past and future actions. The second setting does not provide actions as input (with the exception of Minecraft, which also provides actions in this setting). It involves computing FVD using 1024 samples generated by conditioning on the first 36 frames and then predicting the remaining 264 frames.

All sequence models we trained used the same number of layers as the Transformer used in the TECO-Transformer trained by Yan et al. [13]. In addition, the TECO-Transformer, TECO-S5 and TECO-ConvS5 models we trained used the exact encoder/decoder configuration and MaskGit configuration as in Yan et al. [13]. The Transformer, S5 and ConvS5 models we trained without the TECO framework were all trained using the same encoder/decoder configuration. See Tables 15-21 for more detailed experimental configuration details. See dataset-specific paragraphs below for hyperparameter tuning information.

TECO Training FrameworkYan et al. [13] proposed the TECO training framework to train Transformers on long video data. For some of our experiments, we use ConvS5 layers and S5 layers as a drop-in replacement for the Transformer in this framework. We refer the reader to Yan et al. [13] for full details. Briefly, given the original VQ codes, TECO trains an additional encoder/decoder that compresses the frames to a lower latent resolution (e.g., from \(16\times 16\) to \(8\times 8\) ) by training an additional encoder/decoder with a codebook loss, \(\mathcal{L}_{\mathrm{VQ}}\). In addition, a MaskGit [98] dynamics prior loss, \(\mathcal{L}_{\mathrm{prior}}\), is used for the latent transitions. The sequence model (e.g. Transformer, S5, ConvS5) takes the latent frames (compressed into tokens in the case of Transformer and S5) and produces an output which is used along with the latents by the decoder to produce predictions and a reconstruction loss, \(\mathcal{L}_{\mathrm{recon}}\). Models are trained to minimize the following total loss:

\[\mathcal{L}_{\mathrm{TECO}}=\mathcal{L}_{\mathrm{VQ}}+\mathcal{L}_{\mathrm{ recon}}+\mathcal{L}_{\mathrm{prior}}.\] (32)

In addition, TECO includes the use of DropLoss [13], which drops out a percentage of random timesteps that are not decoded and therefore do not require computing the expensive \(\mathcal{L}_{\mathrm{recon}}\) and \(\mathcal{L}_{\mathrm{prior}}\) terms.

DMLabAs mentioned above, the actions were randomly dropped out of sequences half the time (due to the two evaluation scenarios, action-conditioned and unconditioned). We observed that for DMLab, when provided past and future actions, models converged faster using the simple masking strategy discussed in Gu et al. [19] that masks the future inputs rather than feeding the predicted inputs (or true inputs during training) autoregressively. Therefore we trained all models (Transformer, Performer, S5, ConvS5, Teco-Transformer, TECO-S5, TECO-ConvS5) by using this strategy when the actions were provided, and using the autoregressive strategy when actions were not provided. We observed this significantly improved the LPIPS of the Transformer baselines. Note, in pilot runs for Minecraft and Habitat, we observed this strategy led to lower-quality frames and did not use it for the reported results for those datasets.

We trained each model, Transformer, Performer, S5, ConvS5, Teco-Transformer, TECO-S5, TECO-ConvS5, with three different learning rates [\(1\times 10^{-4}\), \(5\times 10^{-4}\), \(1\times 10^{-3}\)] and selected the best run for each model. See Tables 15-20 for more experiment configuration details.

The TECO-ConvSSM ablation used the exact same settings as TECO-ConvS5, except the state kernel was initialized with a random Gaussian and a lower learning rate of \(1\times 10^{-5}\) was required for stable training.

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameters & DMLab \\ \hline V100 Days & 125 \\ Params & 140M \\ Input Resolution & \(64\times 64\) \\ Latent Resolution & \(16\times 16\) \\ Batch Size & 16 \\ Sequence Length & 300 \\ LR & \(1\times 10^{-3}\) \\ LR Schedule & cosine \\ Warmup Steps & 5k \\ Max Training Steps & 500K \\ Weight Decay & \(1\times 10^{-5}\) \\ \hline Encoder & Depths & 256 \\ Blocks & 1 \\ \hline Decoder & Depths & 256 \\  & Warmup Steps & 5k \\  & Max Training Steps & 500K \\ S5 & State Size (\(P\)) & 1024 \\  & Layers & 8 \\  & Dropout & 0 \\  & Activation & GLU (half) \\ \hline \hline \end{tabular}
\end{table}
Table 16: Experiment Configuration for S5 on DMLab

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameters & DMLab \\ \hline V100 Days & 150 \\ Params & 101M \\ Input Resolution & \(64\times 64\) \\ Latent Resolution & \(16\times 16\) \\ Batch Size & 16 \\ Sequence Length & 300 \\ LR & \(1\times 10^{-3}\) \\ LR Schedule & cosine \\ Warmup Steps & 5k \\ Max Training Steps & 500K \\ Weight Decay & \(1\times 10^{-5}\) \\ \hline Encoder & Depths & 256 \\ Blocks & 1 \\ \hline Decoder & Depths & 256 \\ Blocks & 4 \\ \hline \multirow{4}{*}{S5} & Downsample Factor & 16 \\  & Hidden Dim (\(U\)) & 1024 \\  & State Size (\(P\)) & 1024 \\  & Layers & 8 \\  & Dropout & 0 \\  & Activation & GLU (half) \\ \hline \hline \end{tabular}
\end{table}
Table 15: Experiment Configuration for ConvS5 on DMLab

\begin{table}
\begin{tabular}{c l c} \hline \hline \multicolumn{2}{c}{Hyperparameters} & \multicolumn{1}{c}{DMLab} \\ \hline V100 Days & 125 \\ Params & 152M \\ Input Resolution & \(64\times 64\) \\ Latent Resolution & \(16\times 16\) \\ Batch Size & 16 \\ Sequence Length & 300 \\ LR & \(5\times 10^{-4}\) \\ LR Schedule & cosine \\ Warmup Steps & 5k \\ Max Training Steps & 500K \\ Weight Decay & \(1\times 10^{-5}\) \\ \hline Encoder & Depths & 256 \\ Blocks & 1 \\ \hline Decoder & Depths & 256 \\ Blocks & 4 \\ \hline \multirow{4}{*}{Temporal Transformer} & Downsample Factor & 16 \\ Hidden Dim & 512 \\ Feedforward Dim & 2048 \\ Heads & 16 \\ Layers & 8 \\ Dropout & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Experiment Configuration for Transformer on DMLab

\begin{table}
\begin{tabular}{l l c} \hline \hline Hyperparameters & DMLab & \\ \hline  & V100 Days & 110 \\  & Params & 175M \\  & Input Resolution & \(64\times 64\) \\  & Latent Resolution & \(8\times 8\) \\  & Batch Size & 16 \\  & Sequence Length & 300 \\  & LR & \(5\times 10^{-4}\) \\  & LR Schedule & cosine \\  & Warmup Steps & 5k \\  & Max Training Steps & 500K \\  & Weight Decay & \(1\times 10^{-5}\) \\  & DropLoss Rate & 0.9 \\ \hline Encoder & Depths & 256, 512 \\  & Blocks & 2 \\ \hline Codebook & Size & 1024 \\  & Embedding Dim & 32 \\ \hline Decoder & Depths & 256, 512 \\  & Blocks & 4 \\ \hline  & Hidden Dim (\(U\)) & 512 \\  & State Size (\(P\)) & 1024 \\  & \(\bm{\mathcal{B}}\) Kernel Size & \(3\times 3\) \\  & \(\bm{\mathcal{C}}\) Kernel Size & \(3\times 3\) \\  & Layers & 8 \\  & Dropout & 0 \\  & Activation & ResNet \\ \hline  & Mask Schedule & cosine \\  & Hidden Dim & 512 \\  & Feedforward Dim & 2048 \\  & Heads & 8 \\  & Layers & 8 \\  & Dropout & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Experiment Configuration for TECO-ConvS5 on DMLab

\begin{table}
\begin{tabular}{l l c} \hline \hline Hyperparameters & DMLab & \\ \hline  & V100 Days & 80 \\  & Params & 180M \\  & Input Resolution & \(64\times 64\) \\  & Latent Resolution & \(8\times 8\) \\  & Batch Size & 16 \\  & Sequence Length & 300 \\  & LR & \(1\times 10^{-3}\) \\  & LR Schedule & cosine \\  & Warmup Steps & 5k \\  & Max Training Steps & 500K \\  & Weight Decay & \(1\times 10^{-5}\) \\  & DropLoss Rate & 0.9 \\ \hline Encoder & Depths & 256, 512 \\  & Blocks & 2 \\ \hline Codebook & Size & 1024 \\  & Embedding Dim & 32 \\ \hline Decoder & Depths & 256, 512 \\  & Blocks & 4 \\ \hline  & Downsample Factor & 8 \\  & Hidden Dim (\(U\)) & 2048 \\ S5 & State Size (\(P\)) & 2048 \\  & Layers & 8 \\  & Dropout & 0 \\  & Activation & GLU (half) \\ \hline  & Mask Schedule & cosine \\  & Hidden Dim & 512 \\  & Feedforward Dim & 2048 \\  & Heads & 8 \\  & Layers & 8 \\  & Dropout & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 19: Experiment Configuration for TECO-S5 on DMLab

\begin{table}
\begin{tabular}{l l c} \hline \hline Hyperparameters & DMLab & \\ \hline  & V100 Days & 80 \\  & Params & 173M \\  & Input Resolution & \(64\times 64\) \\  & Latent Resolution & \(8\times 8\) \\  & Batch Size & 16 \\  & Sequence Length & 300 \\  & LR & \(1\times 10^{-4}\) \\  & LR Schedule & cosine \\  & Warmup Steps & 5k \\  & Max Training Steps & 500K \\  & Weight Decay & \(1\times 10^{-5}\) \\  & DropLoss Rate & 0.9 \\ \hline Encoder & Depths & 256, 512 \\  & Blocks & 2 \\ \hline Codebook & Size & 1024 \\  & Embedding Dim & 32 \\ \hline Decoder & Depths & 256, 512 \\  & Blocks & 4 \\ \hline  & Downsample Factor & 8 \\  & Hidden Dim & 1024 \\ Temporal & Feedforward Dim & 4096 \\ Transformer & Heads & 16 \\  & Layers & 8 \\  & Dropout & 0 \\ \hline  & Mask Schedule & cosine \\  & Hidden Dim & 512 \\  & Feedforward Dim & 2048 \\  & Heads & 8 \\  & Layers & 8 \\  & Dropout & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Experiment Configuration for TECO-Transformer on DMLabMinecraft and HabitatFor Minecraft and Habitat, we only trained TECO-ConvS5 due to the costs of training on these datasets. See dataset details in Appendix E and reported compute costs in Yan et al. [13]. For Minecraft, we evaluated two different learning rates [\(1\times 10^{-4}\), \(5\times 10^{-4}\)] and chose the best. For Habitat, we only performed one run with no further tuning. See Table 21 for further experiment configuration details.

\begin{table}
\begin{tabular}{c c c} \hline \hline Hyperparameters & Minecraft & Habitat \\ \hline V100 Days & 470 & 575 \\ Params & 214M & 351M \\ Input Resolution & \(128\times 128\) & \(128\times 128\) \\ Latent Resolution & \(8\times 8\) & \(8\times 8\) \\ Batch Size & 16 & 16 \\ Sequence Length & 300 & 300 \\ LR & \(5\times 10^{-4}\) & \(1\times 10^{-4}\) \\ LR Schedule & cosine & cosine \\ Warmup Steps & 5k & 5k \\ Max Training Steps & 1M & 1M \\ DropLoss Rate & 0.9 & 0.9 \\ \hline Encoder & Depths & 256, 512 & 256, 512 \\  & Blocks & 4 & 4 \\ \hline Codebook & Size & 1024 & 1024 \\  & Embedding Dim & 32 & 32 \\ \hline Decoder & Depths & 256, 512 & 256, 512 \\  & Blocks & 8 & 8 \\ \hline ConvS5 & Hidden Dim (\(U\)) & 512 & 512 \\  & State Size (\(P\)) & 512 & 512 \\  & \(\bm{\mathcal{B}}\) Kernel Size & \(3\times 3\) & \(3\times 3\) \\  & \(\mathcal{C}\) Kernel Size & \(3\times 3\) & \(3\times 3\) \\  & Layers & 12 & 8 \\  & Dropout & 0 & 0 \\ \hline \multirow{6}{*}{MaskGit} & Mask Schedule & cosine & cosine \\  & Hidden Dim & 768 & 1024 \\ \cline{1-1}  & Feedforward Dim & 3072 & 4096 \\ \cline{1-1}  & Heads & 12 & 16 \\ \cline{1-1}  & Layers & 6 & 16 \\ \cline{1-1}  & Dropout & 0 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 21: Experiment Configuration for TECO-ConvS5 on Minecraft and HabitatDatasets

### Moving-MNIST

The Moving-MNIST [54] dataset is generated by moving two \(28\times 28\) size MNIST digits from the MNIST dataset [111] inside a \(64\times 64\) black background. The digits begin at a random initial location, and move with constant velocity, bouncing when they reach the boundary. For each of the sequence lengths we consider, 300 and 600, we follow Wang et al. [81] and Su et al. [84] and generate 10,000 sequences for training.

### DMLab

We use the DMLab long-range benchmark designed by Yan et al. [13] using the DeepMind Lab (DMLab) [99] simulator. The simulator generates random 3D mazes with random floor and wall textures. The benchmark consists of 40K action-conditioned, 300 frame videos at a \(64\times 64\) resolution. The videos are of an agent randomly navigating \(7\times 7\) mazes by choosing random points in the maze and navigating to them through the shortest path.

### Minecraft

We use the Minecraft [100] long-range benchmark designed by Yan et al. [13]. The game features 3D worlds that contain complex terrains such as hills, forests, rivers and lakes. The benchmark was constructed by collecting 200K action-conditioned 300 frame videos at a \(128\times 128\) resolution. The videos are in Minecraft's marsh biome and the agent iterates walking forward for a random number of steps and randomly rotating left or right. This results in parts of the scene going out of view and coming back into view later.

### Habitat

We use the Habitat long-range benchmark designed by Yan et al. [13] using the Habitat simulator [101]. The simulator renders trajectories using scans of real 3D scenes. Yan et al. [13] compiled 1400 indoor scans from HM3D [112], Matterport3D [113] and Gibson [114] to generate 200K action-conditioned, 300 frame videos with a \(128\times 128\) resolution. Yan et al. [13] used Habitat's in-built path traversal algorithm to construct action trajectories that move the agent between randomly sampled locations.