# Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning

 Yeongbin Seo Dongha Lee &Jinyoung Yeo

Department of Artificial Intelligence

Yonsei University

{suhcrates,donalee,jinyeo}@yonsei.ac.kr

Co-corresponding authors

###### Abstract

Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, LAMA-ckl, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches. The code and the dataset will be available online2

Footnote 2: https://github.com/ybseo-ac/TAALM

Footnote 3: In this work, we use the notation \(x_{i}|x_{<i}\) to denote that the element \(x_{i}\) is sequenced after the preceding elements \(x_{0},x_{1},...,x_{i-1}\).

Figure 1: (a) Learning of Causal LM: The document is decomposed into multiple token sequences \(s_{i}\doteq x_{i}|x_{<i}\)3, which aligns with different importance, but uniformly weighted. (b) Train-Attention: Our proposed Train-Attention learns to predict weights that approximate importance, to enable targeted continual knowledge updates through label-free meta-learning method.

Introduction

Large language models (LLMs), pre-trained on extensive text corpora, have demonstrated remarkable effectiveness when fine-tuned or prompted to perform a variety of downstream tasks (Brown et al., 2020; Raffel et al., 2020; Sanh et al., 2021; Wei et al., 2021). However, as the world changes and new knowledge needs to be updated to the parameters, these models often suffer from a significant loss of previously learned knowledge (i.e., catastrophic forgetting (Kemker et al., 2018; Kirkpatrick et al., 2017)). To address this issue, the field of continual knowledge learning (CKL) is being actively researched (Jang et al., 2021, 2022), which aims to teach a model new knowledge while minimizing forgetting of previous knowledge. Previously explored approaches are broadly categorized into three: (1) minimizing parameter changes through regularization, (2) training the expanded parameters of the adapter while freezing the base model parameters, and (3) reviewing old knowledge. However, these approaches naively inherit the inefficiency of the standard fine-tuning procedure of causal LMs, which uniformly apply weights to all tokens, regardless of their importance.

This inefficiency of uniform weighting becomes more significant within the context of CKL, where the model is assumed to possess a substantial amount of world knowledge and grammatical capabilities already, thus emphasizing the need for limiting targets of learning. For example, consider a causal LM that has undergone both pre-training and fine-tuning and now requires to update the new information that "The president of the US is Biden." Figure 1a illustrates how the model processes the example sentence. The only sequence that carries the essential information of this sentence is the final sequence \(s_{6}\) ("The president of the US is" \(\rightarrow\)"Biden"), which encapsulates the context of "US", "president", and "Biden". Conversely, another sequence such as \(s_{4}\) ("The president of the" \(\rightarrow\) "US") only contains information that is already familiar to the model: the close association between "president" and the name of a nation, as well as the grammatical rule that a noun follows "the". Moreover, \(s_{1}\) ("The" \(\rightarrow\) "president") introduces a harmful bias, suggesting that "president" should invariably follow "The", although any nouns could follow "The". If the model overemphasizes the likelihood of this sequence, several issues can arise: (1) Parameters will be updated more than the necessary amount to learn only essential information, thus resulting in more forgetting. (2) The training steps required to learn the important sequence could become prolonged.

Therefore, we hypothesize that focusing learning efforts on important tokens elevates the performance of the CKL. We present empirical evidence of this in SS4.1.1 (paragraph of the analysis on Oracle). The concept of selecting important tokens has been previously explored outside the domain of CKL by Hou et al. (2022), Lin et al. (2024), and demonstrates enhanced performance on downstream tasks. These methods share the same principle, assigning more importance (we denote this "token importance") to the token with higher classification error, which assumes a definition of token importance as "tokens with low-confidence are important". While this approach can accelerate learning of low-confidence tokens, it is still not guaranteed that such low-confidence tokens are truly "important". This emphasizes a need for a more comprehensive definition of "token importance". To clarify this, in the example of Figure 1, it is necessary to consider why human intuition easily accepts that the sequence \(s_{6}\) is more important than others. This understanding comes from the anticipation that knowing the new president will be useful in the future (e.g., conversation with neighbors, school exams, etc) (Land and Furneaux, 1997). Building on this concept, we define "token importance" as the expected utility of the token in related tasks, a concept we refer to as **usefulness**. Upon this definition of token importance, we propose a novel approach to CKL, named **T**rain-**A**ttention-**A**ugmented **L**anguage **M**odel (**TAALM**), which predicts weights of each token based on their usefulness, leveraging this weight on the training phase to enable efficient update of new knowledge. Train-Attention, the supportive model that predicts weight for the base model, is trained through the meta-learning method.

We also introduce a new CKL benchmark, **LAMA-ckl**, designed to offer a more clear comparison of learning and retention performance. This benchmark's advantages over the previous standard are explained in SS4.3. We experiment on **LAMA-ckl** and previous CKL benchmark (TemporalWiki (Jang et al., 2022)), and our method achieves remarkable **state-of-the-art** performance on both. Our method is compatible with other approaches, and shows enhanced performance when integrated, indicating a synergistic effect. We also compared RHO-1 (Lin et al., 2024), which is the recent concurrent work on the token selecting method, where ours shows superior performance on CKL benchmarks. Our main contribution can be summarized in three. (1) We propose a novel token weighting approach to the CKL task, with a novel problem definition and meta-learning method.

(2) A new benchmark for CKL based on the LAMA dataset. (3) Through extensive experiments, TAALM proves notable improvements over the baselines.

## 2 Related Works

Continual Knowledge LearningContinual Knowledge Learning (CKL) (Jang et al., 2021) is one variation of Continual Learning (CL), specified to LLM. It is more focused on updating new knowledge without catastrophic forgetting (Kirkpatrick et al., 2017; Kemker et al., 2018) of previously learned and preservable knowledge. Previous approaches for CL and CKL can be mainly categorized into three: regularization, architectural, and rehearsal. We analyze that the three approaches share a common goal; to minimize changes in parameters from initial points. **(1) Regularization**: directly controlling the extent of change in the parameters through weight regulation such as L2 (Kirkpatrick et al., 2017; Zenke et al., 2017; Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chen et al., 2020). **(2) Architectural**: freezing the base model parameters and expanding learnable parameters with adapters such as Lora (Houlsby et al., 2019; Hu et al., 2021; Wang et al., 2020; Dettmers et al., 2024), thereby keeping initial parameters untouched. **(3) Rehearsal**: method of continually reviewing the data that is employed to train the initial model, ultimately returning the parameters to the initial points (Shin et al., 2017; Sun et al., 2019; He et al., 2019; Rolnick et al., 2019). In this view, our method is another approach to achieve the same goal, minimizing change of parameters, by filtering objective tokens.

Meta-LearningMeta-learning (Finn et al., 2017; Hospedales et al., 2021) is most commonly understood as learning-to-learn; the process of improving a learning episode, over multiple outer learning episodes. During meta-learning, an outer (i.e., meta) learner is fitted to improve the learning of the inner (i.e., base) model. The meta-learner could be an initial parameter of the base model (Finn et al., 2017), an optimizer of the base model (Andrychowicz et al., 2016), or a hyper-parameter of the base model such as learning-rate (Li et al., 2017; Franceschi et al., 2018). In this view, our meta-learner (Train-Attention) is an LLM architectural model that predicts hyper-parameters of the base model, as the token weights serve as the hyper-parameters in the training objective.

Token SelectingMethods to enhance learning by selecting specific tokens have previously been explored through various approaches: Token-Dropping (Hou et al., 2022), Focal Loss (Lin et al., 2017), and RHO-1 (Lin et al., 2024). These methods share a common principle: assigning more importance to the token with higher classification error.

## 3 Train-Attention-Augmented Language Model (TAALM)

### Token Importance and Token-Weighted Learning (TWL)

\[\text{PPL}_{\theta} =-\frac{1}{N}\sum_{i}\log p(x_{i}|x_{<i};\theta)\] (1) \[=-\frac{1}{\sum_{i}w_{0}}\sum_{i}\log p(x_{i}|x_{<i};\theta) \times w_{0}\ \ \ (w_{0}=1)\] (2) \[\text{Token Weighted (tw) PPL}_{\theta} =-\frac{1}{\sum_{i}w_{i}}\sum_{i}\log p(x_{i}|x_{<i};\theta) \times w_{i}\] (3)

To learn a document data \(\mathcal{D}=\{x_{1},...,x_{N}\}\) which is defined as a series of tokens, a dominant causal language model (LM) (\(\theta\)) commonly employs perplexity (PPL) as the objective function, as formalized in Eq.(1). This can be also interpreted in the form of Eq.(2), which assigns a uniform weight (\(w_{0}=1\)) to log probabilities of each sequence \(x_{i}|x_{<i}\) across all documents. In contrast, our proposed methodology assigns weights \(0<w_{i}\leq 1\) to log probabilities of each sequence, which approximates the importance of each sequence, named token importance (Hou et al., 2022). We denote this set of weights as the **token weight**, and the training process that incorporates the weights (Eq.(3)) as **Token Weighted Learning (TWL)**.

### Train-Attention: Meta-Learning to Predict Token Importance

We suggest defining token importance as **usefulness**, which indicates how much the contained information is useful for solving related tasks in the future. Under this definition, a meta-learningapproach can be derived to develop a supportive model (meta-learner) that predicts the optimal token weights. Let \(\theta\) represent a base causal LM that continually learns knowledge and solves tasks. \(\mathcal{T}_{\mathcal{D}}\) represents a task that can be solved using information contained in \(\mathcal{D}\). Training dataset \(\mathscr{D}\) is a set of pairs of \(\mathcal{D}\) and \(\mathcal{T}_{\mathcal{D}}\). We assume a task \(\mathcal{T}_{\mathcal{D}}\) can be defined as any type (e.g., predicting object labels, classification) as long as the performance can be measured in a differentiable form. The set of token weights, denoted as \(W_{\mathcal{D}}\), comprises weights \(w_{i}\) that represent the importance of each sequence \(x_{i}|x_{<i}\) within \(\mathcal{D}\). The meta-learner, named Train-Attention and denoted as \(\phi\), predicts \(W_{\mathcal{D}}\) from \(\mathcal{D}\). As illustrated in Figure 1(a), \(\phi\) inherits the architecture and pretrained parameters of the causal LM, but the decoder layer is adjusted to yield only a single-dimensional float between [0,1] for each position.

The desired process, learning knowledge and solving a task, is described in two steps; (a) _learn_: \(\theta\) is trained on \(\mathcal{D}\) and is updated to \(\theta^{\prime}\). This update occurs in a TWL manner, with a token weight \(W_{\mathcal{D},\phi}\leftarrow\phi(\mathcal{D})\) that \(\phi\) predicts upon observing the data \(\mathcal{D}\). (b) _solve_: The revised model \(\theta^{\prime}\) is applied to solve the task \(\mathcal{T}_{\mathcal{D}}\), and the loss value \(\mathcal{L}_{\theta^{\prime}}(\mathcal{T}_{\mathcal{D}})\) is computed to quantify the performance on \(\mathcal{T}_{\mathcal{D}}\), where \(\mathcal{L}\) stands for loss function.

Two steps can be regarded as one black box function, which receives \(\phi\) as an input and outputs \(\mathcal{L}_{\theta^{\prime}}(\mathcal{T}_{\mathcal{D}})\). In other words, the task performance of \(\theta^{\prime}\) depends on how \(\phi\) gives attention when learning evidence text data. And \(\phi\) can be optimized to minimize the \(\mathcal{L}_{\theta^{\prime}}(\mathcal{T}_{\mathcal{D}})\). For this, the procedure of (a) and (b) is developed to corresponding steps of Eq.(4) and (5), where \(\alpha\) and \(\beta\) are the learning rates for each respective update.

\[\theta^{\prime} \leftarrow\theta-\alpha\nabla_{\theta}\texttt{twPPL}_{\theta}( \mathcal{D},W_{\mathcal{D},\phi})\] (4) \[\phi \leftarrow\phi-\beta\nabla_{\phi}\mathcal{L}_{\theta^{\prime}}( \mathcal{T}_{\mathcal{D}})\] (5)

Figure 4: One step update of \(\phi\).

First, base model \(\theta\) is updated to \(\theta^{\prime}\) through TWL, with the token weight \(W_{\mathcal{D}}\) which is generated from \(\phi\). Second, the meta-learner \(\phi\) is updated based on the task performance, \(\mathcal{L}_{\theta^{\prime}}(\mathcal{T}_{\mathcal{D}})\). These two steps of update can be also interpreted like Figure 3. As the model \(\theta\) steps out to a new state \(\theta^{\prime}\), the resulting position depends on which token weight (\(W\)) is applied. Some positions are closer to the \(\theta^{*}\), a model optimally capable of the task \(\mathcal{T}_{\mathcal{D}}\), as the distance is measured with \(\mathcal{L}_{\theta^{\prime}}(\mathcal{T}_{\mathcal{D}})\). We can conclude the weight with a shorter distance (\(W^{*}_{\mathcal{D}}\)) is more optimal, in the perspective of usefulness. To prevent the \(\theta\) from converging to the point \(\theta^{*}\), which disables the measurement of distances, we reset the model parameters to the initial state \(\theta\) after every update of \(\phi\). More detail is depicted in Figure 4 and Algorithm 1. As the gradients of parameters of \(\phi\) are tracked during the updating of \(\theta\), its actual implementation is akin to the second derivative. The max iteration step of \(\theta\) (denote as \(M\) in Algorithm 1) is fixed to 1 through our experiment. We employ gradient accumulation when updating \(\phi\) for batch effect.

On the inference phase, \(\theta\) learns data in TWL manner as in Eq.(4), with the parameter of \(\phi\) frozen. This system is Train-Attention-Augmented Language Model (TAALM). In this work, the foundational structure of \(\phi\) is fixed to the small model (TinyLlama-1.1B (Zhang et al., 2024)), while it is augmented to both large (Llama2-7B (Touvron et al., 2023)) and small base models, because \(\phi\) is compatible with any base model that shares the same tokenizer. Additionally, we explore utilizing a 101M-parameter bidirectional transformer (BERT) (Devlin, 2018) as a Train-Attention (TA) to further reduce resource requirements.

## 4 Experiment

We conduct experiments on two benchmarks. One is our newly designed LAMA-ckl, and the other is the established benchmark, TemporalWiki(Jang et al., 2022). We exclude the CKL benchmark by Jang et al. (2021) which is not publicly available. In this section, we present the corpus, evaluation setup, and training detail for Train-Attention and the test result within our proposed LAMA-ckl benchmark. For TemporalWiki, most configurations are aligned with the original work.

### LAMA-ckl

For LAMA-ckl, we tailor the LAMA dataset (LAnguage Model Analysis (Petroni et al., 2019)) to assess the CKL performance, especially the T-REx (Elsahar et al., 2018) part which consists of data from Wikipedia and Wikidata. LAMA is a cluster of datasets that measures how much world knowledge is contained in the LLM. Each unit in the dataset includes a knowledge base triple <subject, relation, object>, along with corresponding evidence documents that support the information contained in this triple. Referring to the previous work (Jang et al., 2022), a CKL benchmark should evaluate both **plasticity** and **stability**. Plasticity refers to how well the model updates new knowledge, while stability refers to how little the model forgets existing knowledge. Accordingly, we sample 500 units of to-learn and not-to-forget sets from LAMA to assess each dimension. During the evaluation, as illustrated on Figure 5, the model learns the evidence documents in the to-learn set. It is then tested on both the to-learn task and the not-to-forget task to assess plasticity and stability, respectively.

Dataset SetupHere, we outline a protocol for sampling test corpora for the LAMA-ckl benchmark. As to-learn set represents "the knowledge that the model either encounters for the first time or needs to update", it is selected based on two constraints: (1) sample from the categories of time-variant relations, predicated on the assumption that knowledge categorized as time-variant

Figure 5: Evaluation procedure of the LAMA-ckl benchmark.

typically requires updates. (2) to ensure the concept of "knowledge new to the model", we select units where the task accuracy is zero when measured with pre-update baselines. Conversely, because not-to-forget set represents "the knowledge that the model already knows and aims to retain", it is selected from categories of time-invariant relations, with task accuracy of 1. We recommend sampling a new dataset by the specified constraints when evaluating models outside of the LLaMA-family, for more accurate assessment. The categorization of time-variant and time-invariant follows Jang et al. (2021). Each selected unit includes (1) an evidence document, (2) a knowledge base triple (e.g., <Lochinvar, is an instance of, castle>), and (3) a descriptive sentence encapsulating the triple (e.g., "Lochinvar is a castle"), which is inherited from LAMA dataset. The task is predicting object label tokens in the descriptive sentence. Details on data are in Appendix A.1

Evaluation SetupDuring the evaluation, each epoch consists of both a training phase and a test phase. In the training phase, the model is trained on a set of 500 evidence documents from the to-learn set. Subsequently, in the test phase, the model's prediction accuracy for the object labels is assessed using 500 descriptive sentences from both the to-learn and not-to-forget sets. This process is repeated over 30 epochs. For the to-learn set, an increase in mean accuracy from 0 signifies the model's plasticity. Conversely, a decline in mean accuracy for the not-to-forget set from 1 to lower values indicates the model's stability, as it tends to forget previously learned information.

In the proposed benchmark LAMA-ckl, we suggest four main factors as evaluation indicators. **1) Top Acc:** the highest to-learn accuracy among checkpoints of 30 epoch. **2) Epoch:** the epoch where the Top Acc appears. **3) NF Acc:** not-to-forget accuracy of the checkpoint model which is the same as Top Acc. **4) Total Knowledge**: the sum of Top Acc and NF Acc, indicating total capacity of knowledge including updating and maintaining. We chose these factors because the best CKL system is one that _learns the most and the fastest and loses the least_. Factor 1, 3, and 4 are better if higher, while factor 2 is better if lower. The detailed configurations for training datasets are in Appendix A.1.1.

Train-Attention Training SetupReferring to Algorithm 1, the training procedure of Train-Attention requires data \(\mathcal{D}\) and related task \(\mathcal{T}_{\mathcal{D}}\). For LAMA-ckl dataset, we assign evidence document of each unit as \(\mathcal{D}\), and knowledge base triple in a document of **schematic form** as \(\mathcal{T}_{\mathcal{D}}\). The perplexity of object token is assigned as the objective of \(\phi\). Figure 6 shows the heat map of token-weight that Train-Attention generates. Train-Attention seems to give more attention to entities of certain categories, rather than words with general grammatical roles. We describe the detailed configuration and findings on the training of Train-Attention in Appendix A.2.

Baseline SetupWe utilize Llama2-7B integrated with QLoRA (Dettmers et al., 2024) as a base model. The baseline methods and their hyper-parameter settings follow previous CKL study of Jang et al. (2022): standard finetune, K-Adapter (Wang et al., 2020), Mix-review (He et al., 2019), LoRA (Hu et al., 2021), RecAdam (Chen et al., 2020). We regard standard finetune on QLoRA as a substitute for full finetuning and LoRA, thus skipping the two baselines. We also compare RHO-1 (Lin et al., 2024), which is the most recent concurrent work on the token selecting method, sharing a similar concept with ours. We chose the initial parameter state as a reference model, which is utilized to select important tokens for RHO-1, with other hyper-parameters following the optimal of the original. We also evaluate a model trained in TWL manner with **Oracle** token weight. For which, a weight of 1 is exclusively assigned to the object label token in the evidence document, and the rest is assigned zero weight. Oracle is compared for two purposes: **(1)** To prove the concept that token-weighted learning has the advantage for CKL. **(2)** To check the performance upper bound of Train-Attention. Detailed configurations are in Appendix A.3

Figure 6: Heat map of token weights from Train-Attention. Orange color indicates higher weights.

[MISSING_PAGE_FAIL:7]

the necessary token. Referring to Figure 17, for Oracle, Top Acc is 0.5470, epoch 17, and NF Acc is 0.9002. Oracle shows that Top Acc is 4.75 times higher and NF Acc is 1.11 times higher than standard finetune, thus proving the substantial advantage of token-weighted learning on CKL. Also, TAALM nearly approaches Oracle, as it achieves 78.2% of Oracle Top Acc. NF Acc of ours also maintains a similar level to Oracle, indicating that Train-Attention is optimized close to the upper bound. It also indicates that optimization through meta-learning could excel human labeled weight.

Small (1B) TAALM excels large baselinesWe also experiment on the smaller (TinyLlama-1B) baselines, and TAALM on 1B records the best compared to 7B baselines. This observation indicates that our method outperforms other baseline methods even with significantly smaller parameter sizes and computational resources. Related Table and Figure are on Appendix A.4.1.

TA on BERT achieves comparable performance with small resourcesWe train BERT as a TA and evaluated it on the LAMA-ckl dataset. Training TA on BERT requires only a single 24GB GPU, significantly reducing resource usage compared to the previous model (single 82GB GPU), yet achieving performance similar to the larger (1.1B) TA (Appendix D).

Ablation study on various design choicesWe conduct an ablation study on various design choices applied to the token importance predicted by TA: (1) masking out tokens (setting importance to 0) in real-time when the prediction matches the label, and (2) dropping weights with token importance below the top k% threshold. The ablation study reveals that heuristic adjustments degrade performance, as TA is already in an optimized state. Details are in Appendix E.

### TemporalWiki

We experiment on the original CKL learning benchmark TemporalWiki (Jang et al., 2022), where models have to continually learn Wikipedia documents of serial periods (0809, 0910, 1011, 1112) and test on the corresponding Twiki-Probes, which is a dataset of knowledge base triples. As we train Train-Attention on the 0809 data, tests are conducted on the rest. We experiment with only a small (TinyLlama-1B) model, which is bigger than the baselines of the original work (GPT-2 Large). We conduct a separate experiment on QLoRA-based K-Adapter based models, referring to the analysis on experiment of LAMA-ckl on SS4.1.1. We only consider training Diffset, which is the only changed part of Wikipedia, because it is reported as a condition of the best performance. Most of the experimental settings follow the original, and additional change is described in Appendix C.

#### 4.2.1 Result & Analysis

Referring to Table 2, our method presents the state-of-the-art performance across both experiments on QLoRA based and K-Adapter based models. This achievement is consistent in all periods, and in both Changed and Unchanged Twiki-Probes. This result is aligns with the LAMA-ckl benchmark result, showing that our method has a substantial advantage on the CKL. QLoRA based baselines showed poor performance compared to K-Adapter based baselines, indicating architectural disadvantage. We also test TAALM optimized for LAMA-ckl on the TemporalWiki, referring

Figure 8: Comparison between Oracle, standard finetuning, and ours, tested on LAMA-ckl.

Table 2b. It achieves the second-best performance, indicating robustness across different distributions of tasks.

### Why LAMA-ckl: clear contrast of plasticity and stability

For the benchmark TemporalWiki, because Diffset is corpora of evidence documents for Changed set, learning of Diffsets is supposed to result in performance improvement over Changed set and forgetting of Unchanged set. However, during our experiments, we observe that both Changed and Unchanged performance tend to move in similar directions when learning Diffset, which is in contradiction to our assumption (Appendix C.4). We analyze this for two primary reasons. First, the Diffset contains evidence documents for both the Changed and the Unchanged sets (Appendix C.4). This is a complicating factor in the evaluation of stability. Second, the experimental setup involves training on a vast amount of data, an average of 707K documents per period, for just a single epoch at a low learning rate. This might result in learning little amount of knowledge, while the task ability is challenged by extensive iterations of updates; which is closer to a continual learning setup rather than CKL. To address this issue, we structured the LAMA-ckl as follows: (1) To mitigate the issue of data overlap, we partition the dataset into variant and invariant subsets. These subsets are further classified based on the task accuracy measured by pre-update baselines. (2) We conduct training over multiple epochs on a relatively small dataset to observe the acquirement of knowledge. In practice, our benchmark shows a clear upward trend in the to-learn set and a distinct decline in the not-to-forget set as training progresses, clearly demonstrating the contrast between plasticity and stability.

## 5 Conclusion and Limitation

In this paper, we demonstrate that the application of Train-Attention significantly enhances CKL performance and is also synergistic with other baselines. Nevertheless, our work has the following limitations and potential for future exploration.

Task specificity of Train-Attention Train-Attention is trained to focus on information related to tasks encountered in the training session. This allows task performance to increase, but on the other hand, it left questions as to whether it would be possible to cope with other tasks. Nonetheless, if the task entails the acquisition of general knowledge, it will be transferable to other tasks sharing similar distributions. For instance, TAALM optimized to LAMA-ckl also achieved the best performance

\begin{table}

\end{table}
Table 2: TemporalWiki performance of small (TinyLlama-1B) baselines. **Un** refers Unchanged, **C** refers Changed, **Avg** refers the average of the two. TAALM is our method.

on the TemporalWiki (SS4.2.1). Additionally, Train-Attention can ever evolve to adapt, enabling optimal performance for the current tasks.

What if there are no data-task pairTrain-Attention can be trained only if there is a data-task pair. If there is no paired dataset, it can be doubted that training is difficult. However, every knowledge has its purpose, and we can find a strategy to discover it. **1) Search:** When the data and task pools are separate, we can join highly related pairs via searching. TemporalWiki is also a dataset in which data and tasks are not paired, thus we conduct a lexical search. In the future, also dense research methods can be explored. **2) Generate:** If there is even no separate task pool, we can at least get prior information about what kind of tasks are probable to come in the future. Synthetic tasks can be generated via instruction-tuned LLM, based on this prior information. These methods also resemble the human's cognitive strategy, who often revisit past memories and pose hypothetical questions to themselves to enhance the efficiency of their learning processes.

Broader impactsOur method aims to increase the ability of CKL, therefore enhancing the practicability of LLMs and saving the computational resources for fine-tuning entire huge LLMs. We believe that this paper does not have any immediate negative societal impact.

## Acknowledgement

This work was supported by STEAM R&D Project, NRF, Korea (RS-2024-00454458) and Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT)(No.RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University)) and (2022-0-00077, RS-2022-II220077,AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data). Jinyoung Yeo and Dongha Lee are the co-corresponding authors.

## References

* Aljundi et al. (2018) R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In _Proceedings of the European conference on computer vision (ECCV)_, pages 139-154, 2018.
* Andrychowicz et al. (2016) M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learning to learn by gradient descent by gradient descent. _Advances in neural information processing systems_, 29, 2016.
* Brown et al. (2020) T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chen et al. (2020) S. Chen, Y. Hou, Y. Cui, W. Che, T. Liu, and X. Yu. Recall and learn: Fine-tuning deep pretrained language models with less forgetting. _arXiv preprint arXiv:2004.12651_, 2020.
* Dettmers et al. (2024) T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _Advances in Neural Information Processing Systems_, 36, 2024.
* Devlin (2018) J. Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Elsahar et al. (2018) H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest, and E. Simperl. T-rex: A large scale alignment of natural language with knowledge base triples. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, 2018.
* Finn et al. (2017) C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* Franceschi et al. (2018) L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International conference on machine learning_, pages 1568-1577. PMLR, 2018.
* Favaro et al. (2018)T. He, J. Liu, K. Cho, M. Ott, B. Liu, J. Glass, and F. Peng. Mix-review: Alleviate forgetting in the pretrain-finetune framework for neural language generation models. 2019.
* Hospedales et al. (2021) T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 44(9):5149-5169, 2021.
* Hou et al. (2022) L. Hou, R. Y. Pang, T. Zhou, Y. Wu, X. Song, X. Song, and D. Zhou. Token dropping for efficient bert pretraining. _arXiv preprint arXiv:2203.13240_, 2022.
* Houlsby et al. (2019) N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* Hu et al. (2021) E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Jang et al. (2021) J. Jang, S. Ye, S. Yang, J. Shin, J. Han, G. Kim, S. J. Choi, and M. Seo. Towards continual knowledge learning of language models. _arXiv preprint arXiv:2110.03215_, 2021.
* Jang et al. (2022) J. Jang, S. Ye, C. Lee, S. Yang, J. Shin, J. Han, G. Kim, and M. Seo. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. _arXiv preprint arXiv:2204.14211_, 2022.
* Kemker et al. (2018) R. Kemker, M. McClure, A. Abitino, T. Hayes, and C. Kanan. Measuring catastrophic forgetting in neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Kirkpatrick et al. (2017) J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the national academy of sciences_, 114(13):3521-3526, 2017.
* Land and Furneaux (1997) M. F. Land and S. Furneaux. The knowledge base of the oculomotor system. _Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences_, 352(1358):1231-1239, 1997.
* Li et al. (2017) Z. Li, F. Zhou, F. Chen, and H. Li. Meta-sgd: Learning to learn quickly for few-shot learning. arxiv 2017. _arXiv preprint arXiv:1707.09835_, 2017.
* Lin et al. (2017) T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. Focal loss for dense object detection. In _Proceedings of the IEEE international conference on computer vision_, pages 2980-2988, 2017.
* Lin et al. (2024) Z. Lin, Z. Gou, Y. Gong, X. Liu, Y. Shen, R. Xu, C. Lin, Y. Yang, J. Jiao, N. Duan, et al. Rho-1: Not all tokens are what you need. _arXiv preprint arXiv:2404.07965_, 2024.
* Lopez-Paz and Ranzato (2017) D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. _Advances in neural information processing systems_, 30, 2017.
* Petroni et al. (2019) F. Petroni, T. Rocktaschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. Language models as knowledge bases? _arXiv preprint arXiv:1909.01066_, 2019.
* Raffel et al. (2020) C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* Rolnick et al. (2019) D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne. Experience replay for continual learning. _Advances in neural information processing systems_, 32, 2019.
* Sanh et al. (2021) V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. _arXiv preprint arXiv:2110.08207_, 2021.
* Shin et al. (2017) H. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay. _Advances in neural information processing systems_, 30, 2017.
* Shin et al. (2018)* Sun et al. [2019] F.-K. Sun, C.-H. Ho, and H.-Y. Lee. Lamol: Language modeling for lifelong language learning. _arXiv preprint arXiv:1909.03329_, 2019.
* Touvron et al. [2023] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Wang et al. [2020] R. Wang, D. Tang, N. Duan, Z. Wei, X. Huang, G. Cao, D. Jiang, M. Zhou, et al. K-adapter: Infusing knowledge into pre-trained models with adapters. _arXiv preprint arXiv:2002.01808_, 2020.
* Wei et al. [2021] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* Zenke et al. [2017] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In _International conference on machine learning_, pages 3987-3995. PMLR, 2017.
* Zhang et al. [2024] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small language model. _arXiv preprint arXiv:2401.02385_, 2024.

[MISSING_PAGE_FAIL:13]

#### a.1.1 Evaluation setup detail

Hardware and hyper-parametersDuring training to-learn documents, 8 RTX 3090 GPU (24GB) are used, with a global batch size of 64. A total of 30 epochs took 25 minutes of GPU time. Learning rate 1e-4, AdamW optimizer, and max length of 512 tokens are applied.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Relation Code** & **Template ([X], [Y])** & **Relation** \\ \hline P19 & [X] was born in [Y]. & place of birth \\ P20 & [X] died in [Y]. & place of death \\ P279 & [X] is a subclass of [Y]. & subclass of \\ P37 & The official language of [X] is [Y]. & official language \\ P449 & [X] was originally aired on [Y]. & original network \\ P47 & [X] shares border with [Y]. & shares border with \\ P138 & [X] is named after [Y]. & named after \\ P364 & The original language of [X] is [Y]. & original language of film or TV show \\ P527 & [X] consists of [Y]. & has part \\ P176 & [X] is produced by [Y]. & manufacturer \\ P27 & [X] is [Y] citizen. & country of citizenship \\ P407 & [X] was written in [Y]. & language of work or name \\ P30 & [X] is located in [Y]. & continent \\ P178 & [X] is developed by [Y]. & developer \\ P1376 & [X] is the capital of [Y], & capital of \\ P131 & [X] is located in [Y]. & located in the administrative territorial entity \\ P1412 & [X] used to communicate in [Y]. & languages spoken, written or signed \\ P17 & [X] is located in [Y]. & country \\ P276 & [X] is located in [Y]. & location \\ P937 & [X] used to work in [Y]. & work location \\ P140 & [X] is affiliated with the [Y] religion. & religion \\ P103 & The native language of [X] is [Y]. & native language \\ P190 & [X] and [Y] are twin cities. & twinned administrative body \\ P1001 & [X] is a legal term in [Y]. & applies to jurisdiction \\ P495 & [X] was created in [Y]. & country of origin \\ P36 & The capital of [X] is [Y]. & capital \\ P740 & [X] was founded in [Y]. & location of formation \\ P361 & [X] is part of [Y]. & part of \\ \hline \hline \end{tabular}
\end{table}
Table 4: Relations of each not-to-forget and to-learn set.

Accuracy measurementAccuracy is utilized as a metric (Top Acc, NF Acc) to assess the efficacy of models in the task of label prediction. This metric quantifies the proportion of label tokens correctly identified by the model out of the total label tokens presented.

### Train-Attention training detail

DatasetWe select LAMA units from both time-variant and time-invariant sets, specifically those with an accuracy below 0.5, while ensuring no overlap with the not-to-forget and to-learn sets. Furthermore, the initial data exhibit a disproportionately large number of units categorized under relation P530 ('diplomatic relations of the country'). To address this imbalance, we adjust the frequency of P530 units to match that of the second most prevalent type. Consequently, the final version of the Train-Attention training dataset comprises a total of 4166 units, detailed in Table 3.

We propose the **schematic form** to arrange the knowledge base triple into a human-readable sentence. It fits the one-directional feature of causal LM, as opposed to the descriptive form where important information sometimes appears behind the label tokens. The template and example are on Appendix B. We employ schematic form in training Train-Attention, while employing descriptive form during evaluation.

TrainingWe utilized small (TinyLlama-1B) and large (Llama2-7B) models from the LLaMA family, integrated with QLoRA (Dettmers et al., 2024). We found that using a large rather than small model for the base model (\(\theta\)) results in faster convergence. On the other hand, employing a large or a small model for the Train-Attention model (\(\phi\)) made little difference in the aspect of the convergence step and validation score. So we adopt a large model for \(\theta\) and a small model for \(\phi\) while training Train-Attention. In the test phase, We find that \(\theta\) and \(\phi\) are still compatible even though they don't share the same background model, as long as they use the same tokenizer. Therefore we utilize small-size Train-Attention for both small and large baseline experiments.

We initialize the parameters of the Train-Attention head, which is a decoder layer as depicted in Figure 1(a), using a normal distribution with a mean of 0 and a standard deviation of 0.0001. Consequently, the weights generated by the initialized Train-Attention are equivalent to the uniform weight where all \(w_{i}=1\). This approach is adopted to observe the convergence of the \(\mathcal{L}_{\theta^{\prime}}(\mathcal{T}_{\mathcal{D}})\) more clearly, ensuring that the loss value declines from the initial state if training proceeds normally.

A single A100 (82GB) GPU is used, and the effect of batch size 16 is achieved through gradient accumulation. A checkpoint of 400 global steps is used (takes about 6 GPU hours). Of the 4166 train data, 100 are used for validation. The learning rate of 2e-4 and AdamW optimizer is employed for both the base model and Train-Attention.

### Baselines detail

In this section, we describe the detailed configuration of QLoRA and K-Adapter. The other baselines follow the previous CKL work of (Jang et al., 2022).

QLoRAHyper-parameters of QLoRA follow one of the optimal of the original work (Dettmers et al., 2024). We employ LoRA \(r=64,\alpha=16\), NF4 with BF16 computation datatype. A total of 160M parameters are expanded for the large (Llama2-7B) baselines.

K-AdapterHyper-parameters of K-Adapter follow the previous CKL work of (Jang et al., 2022). A total of 303M parameters are expanded. The trainable parameters are double the QLoRA. Efforts are made to reduce the learnable parameters of K-Adapter, but we observe that the current settings are the minimum level necessary for K-Adapter to function effectively. In experiments, K-Adapter demonstrates greater stability than QLoRA, which could be partially attributed to its larger parameter size. We also employ the same quantization configuration of QLoRA for K-Adapter, NF4 with BF16 computation datatype.

[MISSING_PAGE_FAIL:16]

#### a.4.3 Rationale for the accuracy drop

In the main experiment (Figure 7), the accuracy declines after a rapid peak after about the fourth epoch. We hypothesize this is a result of overfitting. While the token weights from TA include beneficial targets, there must also be some that are not. Performance achieves peak until the model completes learning for the true target, but learning may continue for the false targets. This continued learning leads to parameter updates in suboptimal directions, resulting in forgetting.

The comparison between TA and the oracle (Figure 17) provides evidence for this rationale. As the oracle's performance does not decline with further training, the differences in accuracy trends could be caused by false targets. This phenomenon can also be interpreted as a type of overfitting where the model becomes too fitted to the training data which has different distribution from test data. As the phenomenon of declining after peaking is common due to overfitting, this cycle seems to occur more rapidly for TAALM.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Top Acc & Epoch & NF Acc & Total Knowledge \\ \hline ours + K-Adapter & 0.3320 & 21 & **0.9747** & 1.3067 \\ ours(QLoRA) + Mix-review & **0.4500** & **3** & 0.9012 & **1.3512** \\ ours(QLoRA) + RecAdam & 0.4360 & 7 & 0.8982 & 1.3342 \\ ours + K-Adapter + RecAdam & 0.2640 & 13 & 0.9730 & 1.2370 \\ ours + K-Adapter + Mix-review + RecAdam & 0.3140 & 25 & 0.9677 & 1.2817 \\ ours(QLoRA) & 0.4290 & 4 & 0.8983 & 1.3273 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Combination of ours (TAALM) and other baselines. Based on Llama2-7B, tested on LAMA-ckl.

Figure 11: Comparison of each baseline alone and combined with our method. Each title on the plot represents the baseline method. The gray line represents the baseline alone, and the red line represents the combination with TAALM. Solid line for to-learn, dashed line for not-to-forget. All are based on Llama2-7B, and tested on LAMA-ckl.

Figure 12: Combinations of other methods with ours (TAALM) on Llama2-7B base model, tested on LAMA-ckl.

#### a.4.4 Reporting error ranges of the main experiment

We conduct five independent runs using different random seeds on the training data-loader, and display the results with error ranges at \(\pm 2\sigma\) in Figure 14. This demonstrates the consistent performance of our method. All main experiments presented use the data loader with random seed 42.

Figure 14: Performance of the large-scale baseline models (Llama2-7B) on LAMA-CKL, depicted with \(\pm 2\sigma\) error ranges. Results are calculated from five random trials, employing different random seeds over the train data-loader.

Figure 13: The trade-off between plasticity (to-learn) and stability (not-to-forget) are visualized, for all baselines including combinations with our method. All models are based on large (Llama2-7B) architecture. Gray dashed lines stand for the zero-sum state where the Total Knowledge (sum of the performances of to-learn and not-to-forget) is 1. The right and upper sides of the gray lines indicate the more efficient system where learning causes less forgetting. **(a)Dots** presents only the checkpoints of the Top Acc, while **(b)Lines** presents whole checkpoints of 30 epochs as lines.

Descriptive Form & Schematic Form

Each unit of the LAMA dataset comprises a knowledge base triple and a corresponding sentence that encapsulates the information contained within the triple. These sentences are presented in a short _descriptive form_. In contrast, our newly proposed **schematic form** organizes these triples more systematically. The template and example are described in Figure 15. We propose the schematic form because it better aligns with the uni-directional nature of causal language models (LMs). In the descriptive form, critical information often comes after the label tokens, which can be problematic. For instance, in the descriptive form template "[X] is [Y] citizen", the causal LM lacks the crucial cue "citizen" when predicting [Y]. This not only makes the assessment less accurate but also introduces noise into the train-attention learning process.

## Appendix C Detail of TemporalWiki Experiment

### Corpus setup

The original corpus consists of two datasets to learn for 4 periods (0809, 0910, 1011, 1112); (1) Wikipedia: the whole snapshot of Wikipedia documents of each period. (2) Diffsets: the segments of documents that only contain changed information compared to the last period snapshot. The corpus also includes test datasets (TWiki-Probes) corresponding to each period. Just like the LAMA-ckl, the Wikipedia snapshots and Diffsets consist of evidence documents, while TWiki-Probes consist of knowledge base triples. Because we used the data from the very first period to train Train-Attention, only the data from the next three periods (0910, 1011, 1112) are used for the evaluation.

The original study shows that learning only the Diffset results in better performance than learning the entire snapshot. So we excluded the baselines that learn full snapshots and set learning of Diffset as the default. We also employed heuristic filtering for the Diffset. Empty texts containing only 'nan' in the content, or texts with more than 70% of non-letters, which are assumed to contain little meaningful information, are filtered out. Therefore the total number per Diffset data decreases from an average of 837K to 707K documents per each dataset.

Each Twiki-Probes consists of two parts: (1) Changed: assumed to consist only of knowledge base triples that contain changed information compared to the last period. (2) **Unchanged**: contain only retained information from the last period, which is the complementary set of the Changed. Changed set is supposed to measure plasticity, while Unchanged set is supposed to measure stability.

### Evaluation and baseline setup

We follow the evaluation settings of the original work Jang et al. (2022): a model continually learns train datasets of each period while evaluating performance on TWiki-Probes after training of each period. The evaluation metric is the mean perplexity of object label words. One dataset is updated for only 1 epoch.

Figure 15: Descriptive form and our proposed schematic form template to rearrange knowledge triple into a human readable document.

[MISSING_PAGE_FAIL:20]

## Appendix D Reduction of resources through TA on BERT

Due to the substantial GPU resources required to train the TA, it is necessary to find ways to reduce resource consumption. A promising approach is utilizing Bidirectional Transformer (BERT) as a body for TA, which has high inferential capabilities even at a very small size (108M) compared to the previous body (Tinyllama 1.1B), due to its bidirectional property. Since BERT has a different tokenizer from our generation model, the Llama family, we integrate BERT with the Llama tokenizer and pre-train it for one epoch on 17GB Wikipedia documents (9 days using 8 of 24GB GPUs). Then, we finetune this BERT as TA, paired with the generation model of 1B (Tinyllama). This very lightweight TAALM is sufficiently trained on only a single 24GB GPU, significantly reducing resource use compared to the previous version (single 82GB GPU), thus making it affordable for the general environment. On the inference, the TA on BERT demonstrates compatibility with both the 1B and 7B generation models. Although its performance is below that of the TA on Llama, it still exhibits the highest performance among the other baselines.

\begin{table}

\end{table}
Table 8: LAMA-ckl performance of small (TinyLlama-1B) baselines.

Figure 16: The experimental result of small(TinyLlama-1B) baselines on TemporalWiki benchmark. The x-axis and y-axis correspond to the perplexity of Changed and Unchanged sets, respectively. Each color represents baselines, and the arrows represent the experimental result from each period. The start point of the arrow indicates the performance before training, while the endpoint indicates the performance after training.

[MISSING_PAGE_FAIL:22]

Analysis on the attention pattern of TA

TA is observed to generally assign attention to proper nouns, nouns, and verbs that contain the subject's character. The focus of attention seems to be diverse depending on the content of the text. For autobiographical texts, TA shows a tendency to focus on words that represent the person's occupation or major events (Figure 17(a)). In passages listing regional relations, TA pinpoints the names of locations (Figure 17(b)). This appears to be due to the consideration of probable queries. While TA (trained on LAMA-cll) omits some words in the documents, it tends to not miss location names. This is likely because many queries in the LAMA-ckl benchmark involve location-related aspects (e.g., birthplaces, location of the workplace).

We also provide an attention map of TA trained on the multi-session chat (Figure 17(c)). Here, we regard prior dialogue sessions as data (\(\mathcal{D}\)) and an understanding of the next session as task (\(\mathcal{T}_{\mathcal{D}}\)). Unlike Wikipedia documents, chit-chat dialogues contain fewer useful words, highlighting the necessity for TA. TA focuses on the interlocutor's information like the occupation and pet's name.

Figure 18: Heat map of token weights from Train-Attention. Orange color indicates higher weights.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper includes our mathematical formulation and quantitative experimental results that reflect and justify the claims in our abstract and introduction.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitation section contains a discussion of our method's limitations. Guidelines:
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We show how our meta-learning algorithm can be applied to the CKL benchmarks. We clearly explain our algorithm, and architecture with visual aids. We provide anonymized code for our quantitative experiments alongside clear instructions (README.md) for training and evaluation.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide anonymized code for our quantitative experiments alongside clear instructions (README.md) for training and evaluation.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Full training and testing details are in appendix. Full implementations of generative models and classifiers are included in code.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For results on the main experiment (LAMA-ckl benchmark, Llama2-7B baselines), we present error ranges at \(\pm 2\sigma\) derived from the number of trials (5), on Appendix A.4.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We include the name of the GPU we used in addition to the time of execution.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not work with human participants, and all relevant datasets have been checked for privacy compliance prior to experiments and submission. We do not release any model that could be considered high-risk, and we offer a discussion of broader societal impacts in our discussion section.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential positive and negative impacts of this work in SS5.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our contribution does not include new datasets or pre-trained models that pose a risk of misuse.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Code that we derive from earlier work is properly licensed and referenced.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide anonymized code for our quantitative experiments alongside clear instructions for training and evaluation.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects or crowdsourcing were involved in this research.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects were involved in this research.