# Automated Classification of Model Errors on ImageNet

Momchil Peychev, Mark Niklas Muller, Marc Fischer, Martin Vechev

Department of Computer Science

ETH Zurich, Switzerland

{momchil.peychev, mark.mueller, marc.fischer, martin.vechev}@inf.ethz.ch

Equal contribution

###### Abstract

While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over \(95\%\) accuracy and shifting the focus on investigating why the remaining errors persist. Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 900 models. Perhaps surprisingly, we find that across model architectures, scales, and pre-training corpora, top-1 accuracy is a strong predictor for the _portion_ of all error types. In particular, we observe that the portion of severe errors drops significantly with top-1 accuracy indicating that, while it underreports a model's true performance, it remains a valuable performance metric. We release all our code at https://github.com/eth-sri/automated-error-analysis.

## 1 Introduction

ImageNet(Deng et al., 2009; Russakovsky et al., 2015) has established itself as one of the most influential and widely used datasets in computer vision, driving progress in object recognition (Krizhevsky et al., 2012), object detection (Tan et al., 2020), and image segmentation (Minaee et al., 2022). As state-of-the-art models have come close to, and by some metrics exceeded, human performance, the significance of further progress in top-1 and top-5 accuracy has, been questioned in the face of label errors and systematic biases (Beyer et al., 2020; Tsipras et al., 2020).

The most severe such bias is the lack of multi-label annotations in the original ImageNet dataset, with recent studies finding that roughly a fifth of images show multiple entities. While state-of-the-art models have learned to exploit labeling biasses on these images (Tsipras et al., 2020), best practices have shifted to reporting multi-label accuracy (MLA) computed using new multi-label annotations (Beyer et al., 2020; Shankar et al., 2020). Further, many ImageNet and especially organism classes, are hard to distinguish even by trained humans (Horn et al., 2015; Northcutt et al., 2021; Lee et al., 2017), leading to persistent labeling errors.

In the face of these challenges and with state-of-the-art models exceeding \(95\%\) MLA, the focus has increasingly shifted towards analyzing and understanding the remaining model errors instead of blindly pursuing improvements in headline accuracy numbers. To this end, Vasudevan et al. (2022) review all remaining errors of two state-of-the-art models using a panel of experts and classify themwith regards to both error category and severity. While they find that many of the remaining errors are minor or can be attributed to fine-grained class distinctions, they also find major classification errors that are not so easily explainable. We believe that tracking and analyzing the distribution of these error types over a large number of models can not only help us to better understand the impact of novel training techniques and architectures but also identify where the biggest challenges lie and thus how to address them. However, the manual review process employed by Vasudevan et al. (2022) has several issues, preventing it from being employed for a large scale or repeated study of model errors: (i) it is time-consuming even for precise models, making it infeasible to repeat for a large number of potentially less precise models, (ii) it requires (a panel of) expert reviewers which need to be trained on the fine-grained class distinctions, and (iii) it is inconsistent as different reviewers or even the same reviewers at different times might classify the same error differently.

This WorkTo overcome these challenges, we propose an automated error classification pipeline that we use to study the distribution of different types of errors across 962 models of different scales, architectures, training methods, and pre-training datasets. Our pipeline allows us to automatically detect all four error categories identified by Vasudevan et al. (2022): (i) fine-grained classification errors are detected using a set of 161 manually defined superclasses, (ii) fine-grained out-of-vocabulary errors are detected using a visual similarity based criterion for prediction quality and confirmation of their out-of-vocabulary nature using an open-world classifier, (iii) non-prototypical examples are identified using the exhaustive annotations by Vasudevan et al. (2022), and (iv) spurious correlations are detected using a co-occurrence-frequency based criterion.

Main FindingsOur automated error classification pipeline allows us to, for the first time, study the _distribution of different error types_ across a large number of models, leading to the following insights: (i) even MLA is a pessimistic measure of model progress with the _portion_ of severe model failures quickly decreasing with MLA, (ii) this reduction of model failure rate with MLA is more pronounced for larger (pre-)training corpora, _i.e._, models trained on more data make less severe errors even at the same top-1 or multilabel accuracy, and (iii) organism and artifact classes exhibit very different trends and prevalences of error types, _e.g._, fine-grained classification errors are much more frequent for organisms than for artifacts, while artifacts suffer much more from spurious correlations and out-of-vocabulary errors. We believe that these insights can help guide future research in computer vision and that studying the effects of new methods on the resulting error distribution can become an important part of the evaluation pipeline.

## 2 Related Work

Multi-Label AnnotationsWhile the ImageNet dataset is annotated with a single label per image, many images have been found to contain multiple entities (Beyer et al., 2020; Shankar et al., 2020; Tsipras et al., 2020; Yun et al., 2021). Thus, multi-label accuracy (MLA) with respect to new multi-label annotations has been established as a more meaningful metric. Yun et al. (2021) generate pixel-wise labels for ImageNet by directly applying a classification layer to image embeddings before spatial pooling. Beyer et al. (2020) collect Reassessed Labels (ReaL) for the whole ImageNet validation set, by first identifying 6 models with high prediction coverage and accuracy, before manually annotating all images where these 6 models disagree using non-expert labelers. They discard \(3\,163\) images where these labelers disagreed. Tsipras et al. (2020) collect multi-label annotations for \(10\,000\) validation set images, and find that top-1 accuracy is 10% lower for multi- compared to single-label images while MLA accuracy is identical. Shankar et al. (2020) collect multi-label annotation for \(40\,000\) ImageNet and ImageNetV2 validation images by manually reviewing all predictions made by a diverse set of 72 models, using human expert labelers.

Label ErrorsNorthcutt et al. (2021) study label errors across 10 commonly used datasets, including ImageNet, using a confident learning framework to identify potential errors before validating them with Mechanical Turk (MTurk). Studying the whole validation set, they report an error rate of over \(5.8\%\). Lee et al. (2017) manually review 400 randomly selected classification errors of an ensemble model and find the ImageNet label for a substantial portion to either be incorrect or not describe the main entity in the image. Vasudevan et al. (2022) reviewed all remaining errors for two state-of-the-art models with a panel of experts and found that of 676 reviewed model mistakes, 298 were either correct, ambiguous, or the original ground truth incorrect or problematic.

Error AnalysisRecent work has focused on understanding the types of errors models still make on ImageNet. To this end, one strand of work analyses the differences in errors between models. Mania et al. (2019) find that independently trained models make errors that correlate well beyond what can be expected from their accuracy alone. Geirhos et al. (2020) analyze the consistency between errors made by humans and CNNs on a \(16\) class version of ImageNet. They observe that while CNNs make remarkably similar errors, the consistency between humans and CNNs barely goes beyond chance. Nguyen et al. (2021) analyze wide and deep ResNets and find that these exhibit different error patterns. Mania and Sra (2020) find that more precise models typically dominate less precise ones, _i.e_., their error set is a subset of that of less precise models. Lopes et al. (2022), in contrast, find that different pretraining corpora and training objectives can significantly increase error diversity with Andreassen et al. (2021) showing that this diversity reduces significantly during finetuning.

Vasudevan et al. (2022) focus on the errors current state-of-the-art models make, identifying four categories: (i) _fine-grained_ errors describe a model's failure to distinguish between two very similar classes, (ii) _fine-grained out-of-vocabulary_ errors occur when an image shows an entity not contained in the ImageNet vocabulary and the model instead predicts a similar class, (iii) _spurious correlations_ cause the model to predict a class that is not shown in the image in the presence of strongly correlated features, and (iv) _non-prototypical_ instantiations of a class are not recognized by the model.

Datasets for Error AnalysisIn addition to work analyzing model errors on fixed datasets, there has been a growing interest in datasets specifically designed to highlight and thus analyze specific error types. Singla and Feizi (2022) and Moayeri et al. (2022) find that, for some classes, models rely heavily on correlated or spurious features suffering severely reduced accuracy if these are absent or removed. To study this effect, they introduce Salient ImageNet(Singla and Feizi, 2022) and Hard ImageNet(Moayeri et al., 2022), providing soft masks for causal and correlated features and segmentation masks, respectively. Hendrycks et al. (2021) propose ImageNet-A, a dataset of \(7\,500\) single-entity, natural adversarial examples, which induce high-confidence misclassifications in a set of ResNet50 and belong to \(200\) ImageNet classes that were chosen to minimize class overlap and the potential for fine-grained errors (see App. C for an analysis). Vasudevan et al. (2022) collect images where state-of-the-art models fail in an unexplained manner in ImageNet-Major. Taesiri et al. (2023) investigate the effect of zoom and crop on classifier performance and introduce ImageNet-Hard as the set of images (from a range of ImageNet-like datasets) that none of their considered models classified correctly for any crop. Idrissi et al. (2023) annotate ImageNet images with respect to how they differ from what is prototypical for their class, introducing ImageNet-X. This allows them to study the effect of these image-specific variations such as atypical pose, background, or lighting situation, on accuracy over a large number of models. In contrast, our (and Vasudevan et al. (2022)'s) work focuses more on systematic errors caused (partially) by labeling (set) choices rather than on what makes an individual image hard to classify.

## 3 Categorizing ImageNet Errors

In this section, we introduce our automated error classification pipeline, which aims to explain model errors by assigning it one of six error types. We consider errors that can not be explained in this way to be particularly severe _model failures_. While the definitions of our error types are heavily inspired by Vasudevan et al. (2022), we address three main issues of their manual approach with our automated pipeline: (i) it is time-consuming even for precise models and intractable for imprecise ones, (ii) it requires a panel of expert reviewers, typically not available, and (iii) it introduces inconsistencies due to human expert error, disagreement, or ambiguity.

Below, we first provide a brief overview of our pipeline (illustrated in Fig. 1), before discussing the different error types and how we identify them in more detail.

We consider errors due to overlapping class definitions (discussed in SS3.1) and missing multi-label annotations (SS3.2) to be the least severe, as they can be interpreted as labeling errors rather than classification errors. When a model predicts a class that is closely related to one of the labels, we call this a fine-grained classification error (SS3.3), as the model succeeds in recognizing the type of entity but fails during the fine-grained distinction. When an image contains an entity that does not belong to any of the ImageNet classes and a model predicts a label that is closely related to this (out-of-vocabulary) class, we call this a fine-grained out-of-vocabulary error (SS3.4). We consider both types of fine-grained errors to be minor. If a model fails on an image that shows a very non-prototypical instance of a given class, we call this a non-prototypical error (SS3.5). If a model predicts a class that commonly co-occurs with a ground-truth class but is not shown in the image, we attribute this error to spurious correlation and discuss it in SS3.6. We denote both of these errors as explainable errors. If an error can be attributed to multiple causes, we assign the least severe category and thus design our pipeline to consider error types in order of increasing severity (see Fig. 1).

Throughout this section, we include examples and trends for the kinds of errors we discuss. We generally describe these separately for images with a ground-truth class describing an organism (410 of 1000 classes) and artifacts (522 classes) (in line with previous work (Shankar et al., 2020)), as we observe that these two groups of classes exhibit different error patterns. These two groups account for almost all ImageNet classes, with the remaining 68 classes being assigned the group "other". Where we find interesting trends, we further distinguish models by their (pre-)training dataset ranging in size from one million (ImageNet) to multiple billion images (Instagram, LAION-2B), their architecture, including a broad range of MLPs, CNNs, transformer-based, and hybrid models, and their (pre-) training method. For the full details on all 962 models we consider, please refer to App. F. We provide more examples of every error type in App. E and more detailed trends in App. B.

### Class Overlap

Prior work has established that a small number of ImageNet classes exhibit extreme overlap (Northcutt et al., 2021; Vasudevan et al., 2022). We illustrate one such example in Fig. 2: tusker is defined as "an animal with tucks"2, which describes a strict superset of african elephant and has a significant overlap with indian elephant (females have short and sometimes no tusks). To avoid penalizing a model for correct predictions that do not match the ground truth, we consider all predictions describing a _superset or equivalent_ of the ground-truth class to be correct. For example, we accept tusker for an image labeled african elephant, but not vice-versa, as the latter might actually show a boar. We follow the mappings of equivalence and containment from Vasudevan et al. (2022) and refer to their App. C for full details.

Footnote 2: According to the Merriam-Webster Dictionary.

In Fig. 3, we visualize the portion and number of top-1 errors identified to be correct, separating images with a ground-truth class describing an organism (green hues) and artifacts (red hues) and encoding the pre-training dataset size by assigning darker colors to larger datasets. While trends for artifacts and organisms seem very similar for portions of top-1 errors, we observe a clear difference when looking at their absolute numbers. There, two competing effects are at play. With increasing accuracy, more samples get classified correctly, in

Figure 1: We first remove errors w.r.t. the original ImageNet labels caused by overlapping class definitions or missing multi-label annotations, yielding multi-label accuracy (MLA). We then, in this order, identify fine-grained misclassifications, fine-grained misclassifications where the true label of the main entity is not included in the ImageNet labelset, non-prototypical examples of a given class, and spurious correlations. This leaves us with severe model failures that are unexplained by our categorization.

Figure 3: Portion (left) and number (right) of top-1 errors caused by _class overlap_ by group – organisms (green) and artifacts (red). A 95% confidence interval linear fit is shown on the right.

Figure 2: Venn-Diagram of the tusker, indian elephant, and african elephant classes.

cluding as overlapping classes, thus increasing the number of such errors. On the other hand, more accurate models leverage labeling biases to pick the "correct" among equivalent classes (Tsipras et al., 2020), thus reducing the number of such errors. While the former effect seems to dominate for artifacts, the latter dominates for organisms.

### Missing Multi-Label Annotations

While the ImageNet dataset is annotated with a single label per image, many images contain multiple entities (Beyer et al., 2020; Shankar et al., 2020; Tsipras et al., 2020). Consequently, a model predicting the class of any such entity, different from the original ImageNet label, is considered incorrect when computing top-1 accuracy. For example, the image shown in Fig. 4 contains ox, barn, and fence while only being labeled ox. To remedy this issue, multi-label accuracy (MLA) considers all shown classes, according to some multi-label annotation, to be correct. In this work, we use the annotations collected by Shankar et al. (2020) and improved by Vasudevan et al. (2022) combined with the mean class-wise accuracy definition of MLA (Shankar et al., 2020).

We visualize the portion and number of top-1 errors that turn out to be correct under multi-label evaluation in Fig. 5. When looking at the portion of errors (Fig. 5 left), we observe a similar trend for organisms and artifacts with missing multi-label annotations accounting for an increasing portion of top-1 errors as MLA increases and artifacts consistently more affected than organisms. When looking at the absolute numbers, we again observe the number of errors explained by multi-label annotations to first increase with accuracy as models become more precise, before decreasing again as models start to leverage labeling biases (Tsipras et al., 2020). As missing multi-label annotations explain up to \(60\%\) of model errors, we henceforth use MLA instead of top-1 accuracy as the reference for model comparison.

### Fine-Grained Classification Errors

Many of the ImageNet classes and especially the organism classes are very similar, making their fine-grained distinction challenging even for trained humans. For example, Fig. 6 is classified to cornet while actually showing a french horn, both brass wind instruments. While these errors can be corrected for the relatively small validation set using expert human judgment (Horn et al., 2015; Vasudevan et al., 2022), the much larger training set remains uncorrected. In this light, we believe a comprehensive model evaluation and comparison, should consider the failure to distinguish between very similar classes to be less severe than the failure to distinguish very different classes.

To automatically detect such fine-grained classification errors, we manually review all 1000 ImageNet classes and define semantically similar superclasses guided by whether an untrained human could reasonably confuse them. We obtain \(161\) superclasses, containing between \(2\) and \(31\) classes with an average of \(6.7\) and a median of \(4\). An additional \(74\) classes are not part of any superclass. \(50\) superclasses contain organisms (\(9.8\) on average) and \(101\) contain artifacts (\(5.3\) on average). We visualize the portion and number of fine-grained errors in Fig. 7. As expected, we observe significantly more fine-grained errors for organisms than for artifacts, explaining up to \(88\%\) and \(50\%\) of multi-label errors, respectively.

Figure 4: Image with label ox, but also showing the classes barn and fence. Example from Yun et al. (2021)

Figure 5: Portion (left) and number (right) of top-1 errors caused by _missing multi-label annotations_ by group – organisms (green) and artifacts (red).

Figure 6: Image labeled french horn, but predicted to show a cornet.

Figure 7: Portion (left) and number (right) of MLA errors caused by _fine-grained misclassifications_ by group – organisms (green) and artifacts (red).

### Fine-Grained Out-of-Vocabulary Errors

Often images contain entities that do not belong to any ImageNet class, we call these out-of-vocabulary. For example, Fig. 9 shows an image of two blue-cheeked butterflyfish, which is a class not part of the ImageNet labelset. Instead, the target label is coral reef, describing the background of the image. The classifier predicts rock beauty, which is a WordNet child of butterflyfish (despite being part of the angelfish and not the butterflyfish family) and the closest class in the ImageNet labelset to the blue-cheeked butterflyfish. While an optimal classifier could be expected to always predict a class from the intersection of the contained entities and the ImageNet labelset, the following is often observed in practice (Vasudevan et al., 2022). The classifier tries to classify a prominent entity in the image, but as it is not part of the ImageNet labelset, it predicts a closely related class instead. We argue, that these cases should be treated at least as leniently as a fine-grained classification error, but they are harder to detect. As the entity the model tried to classify can not be assigned any ImageNet label, it will not be included in any multi-label annotation and can thus not be captured by defining groups of closely related classes.

To still detect fine-grained out-of-vocabulary (OOV) errors, we propose the following approach, illustrated in Fig. 8 for the above example. We first retrieve the 10 most visually similar images from the ImageNet training set using cosine similarity in the CLIP embedding space (Radford et al., 2021), in this case all labeled rock beauty. If a label of any of these images belongs to the same superclass as the model's prediction (as defined in SS3.3), we conclude that an entity similar to our prediction is shown in the image and proceed with our analysis. Otherwise, we conclude that the analyzed error is not fine-grained. To confirm that the shown entity is indeed out-of-vocabulary, we first collect a set of proposal labels from the following (illustrated in Fig. 10): all ImageNet labels in the same superclass as the model's prediction (IV), all direct WordNet siblings of the model's prediction, up to but excluding the first common ancestor with the ImageNet label (IV and OOV). Finally, we use CLIP as an open-world classifier to score each of the proposed labels. If the highest scoring class is not included in the ImageNet label set, we consider the shown entity to be OOV and conclude that this error was indeed fine-grained OOV.

Figure 8: To confirm a fine-grained out-of-vocabulary error we proceed as follows. Given a misclassified input image, we first retrieve the 10 most visually similar images from the ImageNet train set. If the superclass of any of these images matches the model prediction, we conclude that an entity with a class similar to our prediction is shown in the image. To confirm that this entity is indeed out-of-vocabulary, we first collect a set of proposal labels from WordNet (Miller, 1992) which are partially in and out of vocabulary. Then, we use an Open World Classifier to score each of the proposed labels. If the highest scored among these is not included in the ImageNet labelset, we consider the analyzed error to be OOV.

Figure 10: Illustration of proposal labels \(\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\) are shown in blue boxes \(\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\boxplus\), direct WordNet siblings in orange \(\boxplus\), and ancestors not shared with the label \(\boxminus\boxplus\boxplus\boxplus\boxplus\boxplus\) in green \(\boxplus\).

Figure 9: Image labeled coral reef, but prediction rock beauty (fish).

We visualize the portion and number of multi-label errors categorized as fine-grained OOV in Fig. 11. Interestingly, we observe that fine-grained OOV errors are not only much more prevalent in artifacts but also that their portion increases much more quickly with MLA for artifacts. We hypothesize that this is due to the ImageNet labels covering organisms occurring in the validation set much more comprehensively than artifacts, and many images of artifacts being cluttered and full of other (OOV) objects. We note that this might be a reason why trained humans still outperform even state-of-the-art models on artifacts but not organisms (Shankar et al., 2020). Interestingly, we observe that, across pretraining datasets and model sizes, the number of fine-grained OOV errors for artifact drops much quicker with MLA above around \(93\%\) MLA (see confidence intervals in Fig. 11 right).

### Non-Prototypical Instances

Many concepts described by any one ImageNet class label are broad, depend on social and geographic factors, and change over time. Combined with the search-based data collection process, this has led to a biased dataset with skewed representations of the concepts described by the contained classes. Therefore it is unsurprising that models perform worse on non-prototypical instances or pictures of any given object, _i.e_., instances that, while clearly belonging to a given class, can be considered outliers in the ImageNet distribution of that class.

For example, in Fig. 12 we see a non-prototypical whistle. We believe it is interesting to track progress on these hard, non-prototypical instances as they are likely to be a good indicator to what extent a model has learned to recognize the underlying concepts of a class. However, defining what constitutes a non-prototypical instance of any class is hard and to a large extent subjective. Fortunately, this error type is independent of the (incorrect) prediction made by the model, allowing us to directly leverage the manual categorization of non-prototypical images by Vasudevan et al. (2022). We thus implicitly decide that all images that are classified correctly by the state-of-the-art models (ViT-3B and Greedy Soups) they considered are not sufficiently non-prototypical to explain an error by another model.

We visualize the number of errors caused by non-prototypical instances in Fig. 13. Interestingly and in contrast to all other error types, there is no strong correlation between performance on non-prototypical examples and overall MLA. This suggests that these non-prototypical examples are not inherently hard to classify for all models. Further surprisingly, non-prototypical examples account for a very similar portion of errors for artifacts and organisms, despite the appearance of artifacts of the same class varying significantly more, which we expected would lead to a larger portion of artifact errors being explained by their non-prototypical appearance.

### Spurious Correlations

Entities of many classes frequently co-occur with features that have no causal relationship to that class. For example, oxen are frequently observed on green pastures (such as in Fig. 4). We call these correlated features (Neuhaus et al., 2022). Inherently this is not a problem, however, it has been observed that models frequently rely on these correlated features to make predictions (Choi et al., 2012; Beery et al., 2018; Neuhaus et al., 2022; Moayeri et al., 2022), for example, by predicting ox when shown only a green pasture or camel when shown

Figure 11: Portion (left) and number (right) of MLA errors identified as _fine-grained OOV_ by group – organisms (green) and artifacts (red). 95% confidence interval linear fit is shown on the right. For artifacts, models are divided at \(93\%\) MLA.

Figure 14: An image with label ski mask (and also multi-class label ab), but prediction ski.

Figure 13: Portion (left) and number (right) of MLA errors identified as _non-prototypical sample_.

a cow in the desert. When correlated features lead to prediction errors, we call these spurious correlations. Here, we focus on the case where the presence of a correlated spurious feature causes a misclassification to the correlated class, despite no corresponding entity being shown. In Fig. 14, we show an example of such a spurious correlation where the presence of a ski mask and alps causes a model to predict ski, despite the image containing no ski. We do not consider the error mode, where the absence of a correlated feature causes the model to _not_ predict the correlated class, despite a corresponding entity being shown, investigated by Singla and Feizi (2022); Moayeri et al. (2022).

To identify errors caused by spurious correlations, we identify pairs of commonly co-occurring classes and then categorize errors as spurious correlations if an incorrect model prediction and a multi-label form such a co-occurrence pair. More concretely, we first extract all pairs of co-occurring labels from the ReaL multi-label annotations (Beyer et al., 2020), excluding samples we evaluate on. We then filter out pairs that either belong to the same superclass as defined in SS3.3 or only co-occur once. Using this process, we extract \(13\,090\) label pairs from \(6\,622\) images with more than one label, yielding \(1019\) unique co-occurrence pairs after filtering, which indicate a spurious correlation if they occur.

We visualize the portion and number of errors caused by spurious correlation in Fig. 15. We observe that artifacts and organisms follow notably different trends. Not only is the portion of errors caused by spurious correlations much larger for artifacts than organisms, but it also increases with MLA for artifacts (at an increased rate for higher MLA), while it stays roughly constant for organisms. For state-of-the-art models, spurious correlations explain up to \(15\%\) of errors on artifacts making them the second largest error source we identify.

## 4 Analysis of Model Errors on ImageNet

In this section, we discuss global trends, analyze their interaction with architecture and training choices, and validate our automatic pipeline against the manual analysis of Vasudevan et al. (2022).

Model FailuresAfter removing all minor (see SS3.3 and SS3.4) and explainable (see SS3.5 and SS3.6) errors, from the multi-label errors (MLE), we are left with a set of particularly severe, unexplained model failures (MLF). In Fig. 16 we visualize the portion of these unexplained model failures over multi-label accuracy (MLA) and standard top-1 accuracy, again split by artifact (red) and organism (green). A ratio of \(1\) corresponds to none of the MLEs being explainable by our pipeline, while a ratio of \(0\) corresponds to all MLEs being explainable, \(93\%\)) and top-1 accuracy (at \(80\%\)) for linear fits.

thus consisting only of less severe error types. Surprisingly, both top-1 accuracy and MLA are pessimistic when it comes to reporting model progress, with unexplained model failures decreasing at a much quicker rate than both top-1 errors and MLE.

Further, we observe different error distributions for organisms and artifacts. Firstly, the portion of model failures is much higher for artifacts than for organisms. Secondly, while the portion of unexplained errors decreases for both organisms and artifacts with MLA and top-1 accuracy, indicating that as models get more accurate they make not only less but also less severe errors, the rate of this change differs. For artifacts, this decrease is initially slower but then at around 93% MLA or 80% top-1 error, the portion of severe model failures starts to drop rapidly (roughly three times as fast as before), while the decrease is roughly linear for organisms. This phase change becomes particularly apparent when viewing the trend over MLA, where it is even visible for organisms.

Figure 16: Portion of MLA (left) and top-1 (right) errors that can not be explained and are thus considered _model failures_ – organisms (green) and artifacts (red). 95% confidence interval linear fit is shown shaded. Models are divided by MLA (at \(93\%\)) and top-1 accuracy (at \(80\%\)) for linear fits.

Figure 15: Portion (left) and number (right) of MLA errors caused by _spurious correlations_ by group. A 95% confidence interval linear fit is shown on the right. For artifacts, models are divided into those with more and less than \(93\%\) MLA.

### (Pre-)training Dataset

Throughout SS3, we have illustrated the pretraining dataset size with the marker hue. Generally, we observe that conditioned on identical MLA, the effect of pre-training dataset size is rather modest. Here, we divide the 12 datasets we consider into 4 categories from "small" (\(<\)5M) to "large" (\(>\) 500M) depending on the number of included images, illustrating separate fits in Fig. 17, with a darker shade corresponding to a larger dataset. We observe that across a wide range of accuracies, larger pretraining datasets lead to a faster reduction of model failures with MLA for both artifacts and organisms. While this effect is partially explained by larger datasets more frequently leading to higher MLA, it is also observed for narrow MLA ranges. Considering individual error types, we observe that in particular fine-grained errors are more frequent for larger pretraining datasets. We hypothesize that this is due to these larger datasets leading to better underlying representations, allowing the model to succeed in the coarse classification for harder samples, while still failing on the fine-grained distinctions.

### Model Architecture

In Fig. 18, we again show the portion of unexplained model failures over MLA, this time broken down by architecture type and colored by the number of parameters (larger models have darker hues). To investigate whether modern convolutional architectures (ConvNeXts (Liu et al., 2022)) and vision transformers (ViT (Dosovitskiy et al., 2021), Swin (Liu et al., 2021), EVA (Fang et al., 2023)) exhibit different trends in terms of model failures, we focus on state-of-the-art models with at least \(92\%\) MLA that were trained using a dataset with at least 50M images, leaving 28 CNN-based and 79 transformer-based models. While we observe clear trends of larger models performing better, the rate of model failures seems to be largely independent of model size when conditioned on MLA. We do observe a slight trend where the model failure rate of transformers decreases faster with MLA for organisms but slower for artifacts when compared to ConvNeXts. We speculate that this might be due to the ConvNeXts leveraging textures more heavily (Geirhos et al., 2019), which could be particularly helpful for fine-grained class distinctions on organisms leading to a larger portion of multi-label errors being model failures at the same MLA.

### Comparison to Vasudevan et al. (2022)

To evaluate the alignment of our automated error analysis with a panel of human experts, we compare its results to those of Vasudevan et al. (2022) on both networks they consider, here for ViT-3B and in App. A for Greedy Soups.

Comparing the results in Table 1, we confirm that our pipeline is conservative, classifying \(16.4\%\) (\(62\) / \(378\)) of the errors assigned an explanation by Vasudevan et al. (2022) as model failures. On the remaining errors, our pipeline agrees with the panel of human experts in \(73.1\%\) of the cases. By manually inspecting the \(85\) differently classified errors, we determine that for only \(32\) of these (\(8.4\%\) of all errors), the categorization by Vasudevan et al. (2022) is clearly preferable to ours. Furthermore, our pipeline encodes a minimal-severity bias, absent in Vasudevan et al. (2022)'s categorization. That is, when multiple error explanations would be valid, our pipeline consistently chooses the least severe one. This highlights a further advantage of our approach. While any human judgment is inherently subjective and thus prone to differ between expert panels or even for the same panel over time, as acknowledged by Vasudevan et al. (2022), our approach is consistent and repeatable.

Figure 17: Portion of model failures (MLF) over MLA depending on pre-training dataset size with 95\(\%\) confidence intervals for models with at least 82\(\%\) MLA. Confidence intervals are shown darker for larger pretraining dataset sizes.

Figure 18: Portion of model failures (MLF) over MLA depending on model architecture. 95\(\%\) confidence interval for transformer-based models is shown darker and for CNN-based models lighter.

Observing similar trends for Greedy Soups in App. A, we confirm that on all models for which human expert error categorizations are available, our automated pipeline is well aligned with their judgment while providing consistent, repeatable, and conservative error categorizations.

## 5 Limitations

In this section, we briefly reflect on the limitations of our work.

Personal BiasOur choices in the implementation of the error analysis pipeline reflect our personal biases and decisions, such as the superclasses we picked in SS3.3. This is not only limited to our personal biases, but, as our pipeline relies on prior work in several places, it also encodes the biases of their authors. For example, we rely on the class overlap mappings from Vasudevan et al. (2022) and multi-class labels by Shankar et al. (2020) and Vasudevan et al. (2022).

Extension to New DatasetsTo be applied to a new dataset, our pipeline requires multi-label and non-prototypicality annotations and superclass definitions. While less precise than using a human-annotated gold standard, multi-label annotations could be sourced using a Re-Label (Yun et al., 2021) approach, allowing (given sufficient scale) spurious correlation pairs to be extracted from co-occurrence frequencies. After defining superclasses manually as required by their very definition, this would allow all error types except for the rare non-prototypical instances to be categorized. With the rest of the pipeline in place, non-prototypical instances could be annotated by reviewing the uncategorized errors shared by multiple well-performing models. However, while feasible these steps still require a significant amount of manual work. We showcase a (partial) adaption to the similar ImageNet-A dataset in App. C.

## 6 Conclusion

As state-of-the-art models come close to and exceed human performance on ImageNet, focus is increasingly shifting towards understanding the last remaining errors. Towards this goal, we propose an automated error categorization pipeline that we use to study the distribution of different error types across 962 models, of different scales, architectures, training methods, and pre-training datasets. We distinguish between minor errors, constituting failures on fine-grained class distinctions both when the ground truth was in and out-of-vocabulary, explainable errors, constituting failures on non-prototypical examples and due to spurious correlations, and unexplained model failures, constituting all remaining errors. We find that even MLA is a pessimistic measure of model progress with the portion of severe errors quickly decreasing with multi-label-accuracy. Further, we find that organism and artifact classes exhibit very different trends and prevalences of error types. For example, we observe that fine-grained class distinctions are a much bigger challenge for organisms than for artifacts, while artifacts suffer much more from spurious correlations and out-of-vocabulary errors, with these trends becoming increasingly pronounced as models become more accurate. We believe that such an analysis of a new method's effects on model error distributions can become an important part of the evaluation pipeline and lead to insights guiding future research in computer vision.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Error categories & FG & FG OOV & Non-prot. & Spur. Corr. & Model failures & Total (row) \\ \hline Fine-grained & \(192\) & \(15\) & \(0\) & \(10\) & \(25\) & \(242\) \\ Fine-grained OOV & \(9\) & \(20\) & \(0\) & \(11\) & \(14\) & \(54\) \\ Non-prototypical & \(13\) & \(2\) & \(12\) & \(3\) & \(0\) & \(30\) \\ Spurious Correlation & \(10\) & \(12\) & \(0\) & \(7\) & \(23\) & \(52\) \\ \hline Total (col) & \(224\) & \(49\) & \(12\) & \(31\) & \(62\) & \(378\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our automated analysis of the errors of a ViT-3B model with the manual annotations from (Vasudevan et al., 2022). Rows correspond to the error categories assigned by a human expert while columns correspond to the error types assigned by our automated pipeline.

## Acknowledgements

We thank our anonymous reviewers for their constructive comments and insightful feedback.

This work has been done as part of the EU grant ELSA (European Lighthouse on Secure and Safe AI, grant agreement no. 101070617) and the SERI grant SAFEAI (Certified Safe, Fair and Robust Artificial Intelligence, contract no. MB22.00088). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or European Commission. Neither the European Union nor the European Commission can be held responsible for them.

The work has received funding from the Swiss State Secretariat for Education, Research and Innovation (SERI).

## References

* Deng et al. (2009) J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, "Imagenet: A large-scale hierarchical image database," in _Proc. of CVPR_, 2009.
* Russakovsky et al. (2015) O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei, "Imagenet large scale visual recognition challenge," _Int. J. Comput. Vis._, vol. 115, no. 3, 2015.
* Krizhevsky et al. (2012) A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," in _Proc. of NeurIPS_, 2012.
* Tan et al. (2020) M. Tan, R. Pang, and Q. V. Le, "Efficientdet: Scalable and efficient object detection," in _Proc. of CVPR_, 2020.
* Minaee et al. (2022) S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos, "Image segmentation using deep learning: A survey," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 44, no. 7, 2022.
* Beyer et al. (2020) L. Beyer, O. J. Henaff, A. Kolesnikov, X. Zhai, and A. van den Oord, "Are we done with imagenet?" _ArXiv preprint_, vol. abs/2006.07159, 2020.
* Tsipras et al. (2020) D. Tsipras, S. Santurkar, L. Engstrom, A. Ilyas, and A. Madry, "From imagenet to image classification: Contextualizing progress on benchmarks," in _Proc. of ICML_, vol. 119, 2020.
* Shankar et al. (2020) V. Shankar, R. Roelofs, H. Mania, A. Fang, B. Recht, and L. Schmidt, "Evaluating machine accuracy on imagenet," in _Proc. of ICML_, vol. 119, 2020.
* Horn et al. (2015) G. V. Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. J. Belongie, "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection," in _Proc. of CVPR_, 2015.
* Northcutt et al. (2021) C. G. Northcutt, A. Athalye, and J. Mueller, "Pervasive label errors in test sets destabilize machine learning benchmarks," in _Proc. of NeurIPS Datasets and Benchmarks_, 2021.
* Lee et al. (2017) H. S. Lee, A. A. Agarwal, and J. Kim, "Why do deep neural networks still not recognize these images?: A qualitative analysis on failure cases of imagenet classification," _ArXiv preprint_, vol. abs/1709.03439, 2017.
* Vasudevan et al. (2022) V. Vasudevan, B. Caine, R. G. Lopes, S. Fridovich-Keil, and R. Roelofs, "When does dough become a bagel? analyzing the remaining mistakes on imagenet," in _Proc. of NeurIPS_, 2022.
* Yun et al. (2021) S. Yun, S. J. Oh, B. Heo, D. Han, J. Choe, and S. Chun, "Re-labeling imagenet: From single to multi-labels, from global to localized labels," in _Proc. of CVPR_, 2021.
* Mania et al. (2019) H. Mania, J. Miller, L. Schmidt, M. Hardt, and B. Recht, "Model similarity mitigates test set overuse," in _Proc. of NeurIPS_, 2019.
* Geirhos et al. (2020) R. Geirhos, K. Meding, and F. A. Wichmann, "Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency," in _Proc. of NeurIPS_, 2020.
*T. Nguyen, M. Raghu, and S. Kornblith, "Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth," in _Proc. of ICLR_, 2021.
* Mania and Sra (2020) H. Mania and S. Sra, "Why do classifier accuracies show linear trends under distribution shift?" _ArXiv preprint_, vol. abs/2012.15483, 2020.
* Lopes et al. (2022) R. G. Lopes, Y. Dauphin, and E. D. Cubuk, "No one representation to rule them all: Overlapping features of training methods," in _Proc. of ICLR_, 2022.
* Andreassen et al. (2021) A. Andreassen, Y. Bahri, B. Neyshabur, and R. Roelofs, "The evolution of out-of-distribution robustness throughout fine-tuning," _ArXiv preprint_, vol. abs/2106.15831, 2021.
* Singla and Feizi (2022) S. Singla and S. Feizi, "Salient imagenet: How to discover spurious features in deep learning?" in _Proc. of ICLR_, 2022.
* Moayeri et al. (2022) M. Moayeri, S. Singla, and S. Feizi, "Hard imagenet: Segmentations for objects with strong spurious cues," in _Proc. of NeurIPS_, 2022.
* Hendrycks et al. (2021) D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, "Natural adversarial examples," in _Proc. of CVPR_, 2021.
* Taesiri et al. (2023) M. R. Taesiri, G. Nguyen, S. Habchi, C. Bezemer, and A. Nguyen, "Imagenet-hard: The hardest images remaining from a study of the power of zoom and spatial biases in image classification," _CoRR_, vol. abs/2304.05538, 2023.
* Idrissi et al. (2023) B. Y. Idrissi, D. Bouchacourt, R. Balestriero, I. Evtimov, C. Hazirbas, N. Ballas, P. Vincent, M. Drozdzal, D. Lopez-Paz, and M. Ibrahim, "Imagenet-x: Understanding model mistakes with factor of variation annotations," in _Proc. of ICLR_, 2023.
* Northcutt et al. (2021) C. G. Northcutt, L. Jiang, and I. L. Chuang, "Confident learning: Estimating uncertainty in dataset labels," _J. Artif. Intell. Res._, vol. 70, 2021.
* Miller (1992) G. A. Miller, "WordNet: A lexical database for English," in _Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992_, 1992.
* Radford et al. (2021) A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," in _Proc. of ICML_, vol. 139, 2021.
* large-scale detection of harmful spurious features in imagenet," _CoRR_, vol. abs/2212.04871, 2022.
* Choi et al. (2012) M. J. Choi, A. Torralba, and A. S. Willsky, "Context models and out-of-context objects," _Pattern Recognit. Lett._, vol. 33, no. 7, 2012.
* Beery et al. (2018) S. Beery, G. V. Horn, and P. Perona, "Recognition in terra incognita," in _Proc. of ECCV_, vol. 11220, 2018.
* Liu et al. (2022) Z. Liu, H. Mao, C. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, "A convnet for the 2020s," in _Proc. of CVPR_, 2022.
* Dosovitskiy et al. (2021) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," in _Proc. of ICLR_, 2021.
* Liu et al. (2021) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in _Proc. of ICCV_, 2021.
* Fang et al. (2023) Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao, "EVA: exploring the limits of masked visual representation learning at scale," in _Proc. of CVPR_, 2023.
* Geirhos et al. (2019) R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel, "Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness," in _Proc. of ICLR_, 2019.
* Goh et al. (2019)Extended Comparison to Vasudevan et al. (2022)

Similarly to the analysis in Sec. 4.3, we now evaluate our automated error classification pipeline on GreedySoups, the only other model for which manual error categorizations exist (Vasudevan et al., 2022), showing the results in Table 2. We do not assign an explanation to \(11.6\%\) (\(29\) / \(249\)) of the errors, classifying them as model failures. On the remaining errors, our pipeline agrees with the human expert annotation in \(74.1\%\) (\(163\) / \(220\)) of the cases. After a manual review of the remaining 57 differently classified errors, we determine that the categorization provided by Vasudevan et al. (2022) is clearly preferable to ours for only 26 samples (\(10.4\%\) of all errors). Overall, the results for GreedySoups are very similar to those for ViT-3B in SS4.3, thus further validating our modeling choices. Therefore, we believe that our automatic categorization is indeed aligned with the opinion of human experts and note that an efficient and automatic classification pipeline is valuable even if it is imperfect, as it enables the study of models and trends at scale.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Error categories & FG & FG OOV & Non-prot. & Spur. Corr. & Model failures & Total (row) \\ \hline Fine-grained & \(139\) & \(14\) & \(1\) & \(7\) & \(11\) & \(172\) \\ Fine-grained OOV & \(7\) & \(8\) & \(0\) & \(5\) & \(6\) & \(26\) \\ Non-prototypical & \(8\) & \(1\) & \(8\) & \(2\) & \(0\) & \(19\) \\ Spurious Correlation & \(7\) & \(5\) & \(0\) & \(8\) & \(12\) & \(32\) \\ \hline Total (col) & \(161\) & \(28\) & \(9\) & \(22\) & \(29\) & \(249\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of our automated analysis of the errors of a GreedySoups model with the manual annotations from (Vasudevan et al., 2022). Rows correspond to the error categories assigned by a human expert while columns correspond to the error types assigned by our automated pipeline.

[MISSING_PAGE_FAIL:14]

Finally, in Fig. 25 we present the ratio of unexplained model failures (MLF) over multi-label accuracy (MLA) and standard top-1 accuracy. Again, subfigures (a) and (b) are organized per group, while (c) and (d) show the results for all samples.

Figure 23: Multi-label errors identified as non-prototypical instances. The number of samples identified as non-prototypical by Vasudevan et al. (2022) is inherently small (36) and the majority of them are artifacts, leading to noisy results. As the models get better at other types of errors, the relative portion of multi-label errors due to non-prototypical images increases.

Figure 21: Multi-label errors due to fine-grained misclassifications. The portion of fine-grained errors on “others” is larger than those on “artifacts” but smaller than “organisms”.

Figure 22: Fine-grained out-of-vocabulary multi-label errors. The portion of errors follows a similar trend for “artifacts” and “others”, while the sudden sharp drop in absolute numbers for “artifacts” is absent for “others”.

Figure 25: Ratio of unexplained model failures (MLF) over multi-label accuracy (MLA - (a) and (c)) and standard top-1 accuracy ((b) and (d)). In general, our pipeline explains the biggest portion of errors made on “organisms” and the lowest portion of errors on “artifacts”. However, better models quickly close this gap. For the best models, our pipeline explains up to \(85\%\) of all multi-label errors and \(95\%\) of all top-1 errors.

Analysis of ImageNet-A

To demonstrate the applicability of our automated error analysis pipeline to other datasets, we now apply it to ImageNet-A. ImageNet-A was introduced by Hendrycks et al. (2021) as an ImageNet-like test set of "Natural Adversarial Examples". It contains \(7500\) images from 200 ImageNet classes, picked to fool a set of ResNet-50 classifiers, while still constituting high-quality, single-entity images, clearly depicting an entity of the target class. A comparison of ImageNet and ImageNet-A accuracy is shown in Fig. 26. In this section, marker color encodes model parameter counts, i.e., larger models have darker markers.

As ImageNet-A images were selected to be challenging for ImageNet classifiers at the time, we see that many classifiers with ImageNet accuracies below \(80\%\) have almost \(0\) accuracy on ImageNet-A. In fact, 801 of the 962 models we consider have an accuracy of less than \(30\%\) on ImageNet-A. As classifiers improved (\(>80\%\) top-1 ImageNet accuracy) ImageNet-A accuracy catches up rapidly. Among other improvements, the main driver seems to be the model parameter count. Note that due to the much lower ImageNet-A accuracy (of some models), the x-axis of all further plots starts at \(0\%\) rather than \(60\%\), as before.

As ImageNet-A uses ImageNet classes, we keep the split into organisms (77 out of 200 classes, \(38.5\%\)), artifacts (104 / 200, \(52\%\)), and other classes (19 / 200, \(9.5\%\)).

Class OverlapDue to the shared class structure, we can use the same analysis as for ImageNet and show the results in Fig. 27. The 200 classes in ImageNet-A were selected from the 1000 ImageNet classes to avoid class overlap and label ambiguity. Thus, as expected, we see a much lower overall amount of such errors (below \(0.8\%\) instead of up to \(8.5\%\) for ImageNet). In particular, for artifacts and other classes, we see no class overlap errors at all. Interestingly, for organisms, we still observe a small number (\(\leq 22\)) of errors caused by class overlap. We observe that errors become more frequent (albeit still rare) for models with higher accuracy. As with ImageNet this is due to more accurate models classifying more images correctly, including to overlapping classes.

Multi-Label AnnotationsIn contrast to ImageNet, ImageNet-A was designed to only include single-entity images. Thus, we skip the multi-label analysis for ImageNet-A and use top-1 accuracy/error rather than MLA accuracy/error for all further analyses.

Fine-Grained Classification ErrorsDue to the shared class structure, we can apply the same analysis including superclasses for fine-grained classification errors as on ImageNet. We show results in Fig. 28.

As with ImageNet, the group for which we observe most of these errors are organisms (explaining up to \(47\%\) of the group's errors). However,

Figure 28: Portion (left) and number (right) of top-1 errors caused by _fine-grained misclassifications_ by group – organisms (green), artifacts (red), and others (blue).

Figure 27: Portion (left) and number (right) of top-1 errors caused by _class overlap_ for organisms (green). For the artifacts and other groups we observe no errors.

Figure 26: ImageNet-A top-1 Accuracy vs ImageNet top-1 Accuracy grouped by architecture (left) and training data size (right).

the portion of explained errors is much lower than for ImageNet, where up to \(88\%\) of MLA errors for organisms can be explained by fine-grained classification errors. We see the percentage of errors increase linearly with accuracy. Looking at absolute counts, especially for organisms, we see that fine-grained errors initially become more prevalent as model accuracy increases before dropping off as models more frequently also succeed in the fine-grained class distinction.

Fine-GrainedOut-of-VocabularyErrorsAgain, we can use the same superclass definition, CLIP embedding and open world classifier as for ImageNet to identify fine-grained OOV errors. We generally observe slightly larger portions of OOV errors than on ImageNet and a much clearer linear dependence on accuracy (see Fig. 29). Notably, the superlinear increase observed on ImageNet (see Fig. 11) is absent here. Looking at absolute counts, after an initial increase and stagnation, we observe a linear decrease with model accuracy. In contrast to ImageNet, we again do not observe a slope change.

Non-Prototypical InstancesOur analysis pipeline for ImageNet uses the manual annotation of non-prototypical instances from (Vasudevan et al., 2022). As we do not have these available for ImageNet-A we skip this analysis step here. However, for standard ImageNet we only observe a small number of such errors (\(\leq 2\%\) of MLE) and note that the ImageNet-A images were manually selected to only include high-quality images (Hendrycks et al., 2021). Thus, we expect their portion to be even smaller here.

Spurious CorrelationsAgain following our ImageNet approach for spurious correlation errors we obtain the results in Fig. 30. At first glance, we observe much less spurious correlations than for ImageNet (\(6.2\%\) compared to \(15\%\) for artifacts). However, we observe a more linear instead of super-linear increase in the portion of errors explained by spurious correlations, leading to this reduction being smaller for less accurate models.

Model FailuresLooking at the remaining failures, we see that we can only explain up to half of the errors for the best-performing models. Not only are top-1 and MLA error rates much higher than for ImageNet but also a much larger portion of the remaining errors constitutes severe model failures, highlighting that ImageNet-A is a much harder dataset.

However, analyzing Fig. 31, we observe that both the ImageNet-A top-1 and ImageNet MLA accuracy are good predictors for the portion of model failures which decreases with increasing model accuracy. This highlights again and perhaps surprisingly that ImageNet MLA underreports progress in model performance, even on particularly challenging subdistributions like ImageNet-A.

Figure 31: Portion for remaining failures over ImageNet-A top-1 (left) and ImageNet MLA (right).

Figure 30: Portion (left) and number (right) of top-1 errors identified as _spurious correlations_ by group – organisms (green), artifacts (red) and other (blue).

Figure 29: Portion (left) and number (right) of top-1 errors identified as _fine-grained OOV_ by group – organisms (green), artifacts (red) and other (blue).

Extended Examples on Fine-Grained OOV Error Classification

In this section we provide further step by step examples of the procedure that determines if a given model mistake is categorized as a fine-grained OOV error. The sample in Fig. 32 is confirmed to be fine-grained OOV, the one in Fig. 33 is not fine-grained and the one in Fig. 34 is rejected as OOV.

Figure 32: **Fine-grained OOV confirmed.** The evaluated validation sample (top) has a ground-truth ImageNet label (GT) suit, but the model predicts bicycle-built-for-two which falls in the “motor_cycle” superclass. Below, we show the top-10 training images that are the most visually similar to the validation sample (according to CLIP similarity) together with their ImageNet labels and the superclasses of these labels. The labels of images 3, 6, 9 and 10 are in the same superclass as the predicted label (motor_cycle), hence we proceed with the procedure. After obtaining the in-vocabulary and OOV label proposals, as described in Sec. 3.4, the proposals with the highest probability according to CLIP are: safety bicycle, ordinary bicycle, velocipede and bicycle, all of which are OOV, confirming that the error is indeed fine-grained OOV.

Figure 33: **Fine-grained OOV rejected. The evaluated validation sample (top) has ground-truth ImageNet multi-labels (GT) airship and analog clock, but the model predicts jinrikisha, arguably attempting to classify the carriers on the street. The predicted label is in the “open_cart” superclass. Next, we show the top-10 training images that are the most visually similar to the validation sample (according to CLIP similarity) together with their ImageNet labels and the superclasses of these labels. None of the labels is in the “open_cart” superclass, thus we terminate the procedure and reject the mistake as a possible fine-grained OOV error.**

Figure 34: **Fine-grained OOV rejected**. The evaluated validation sample (top) has a ground-truth ImageNet label (GT) barrow, but the model predicts shopping basket, arguably attempting to classify the basket in the cart. The predicted label is in the “box” superclass. Next, we show the top-10 training images that are the most visually similar to the validation sample (according to CLIP similarity) together with their ImageNet labels and the superclasses of these labels. The labels of images 3 and 4 are in the same superclass as the predicted label (box), hence we proceed with the procedure. After obtaining the in-vocabulary (IV) and OOV label proposals, as described in Sec. 3.4, the proposal with the highest probability according to CLIP is shopping cart, which is in-vocabulary, therefore, the mistake is not categorized as a fine-grained OOV error. The other most likely proposals are: shopping basket (IV), bushel basket (OOV), basket (OOV) and handbasket (OOV).

## Appendix E Mistake Examples by Error Type

Figure 35: **Class overlap**: examples of common prediction errors (Pred) due to overlapping classes. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.

Figure 36: **Multi-label: examples of common prediction errors (Pred) due to multi-label errors. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.**

Figure 37: **Fine-grained**: examples of common prediction errors (Pred) due to fine-grained errors. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.

Figure 38: **Fine-grained OOV: examples of common prediction errors (Pred) due to fine-grained OOV errors. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.**

Figure 39: **Non-prototypical**: examples of common prediction errors (Pred) due to non-prototypical instances. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.

Figure 40: **Spurious correlations**: examples of common prediction errors (Pred) due to spurious correlations. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.

Figure 41: **Model failures: examples of common prediction errors (Pred) classified as model failures. The correct (individual) multi-labels (GT) are separated by commas; the original ImageNet label is listed first.**

Experimental Setup

In this section, we provide further details on our evaluation.

We publish our code and detailed instructions on how to reproduce our results at https://github.com/eth-sri/automated-error-analysis.

### Computing the Multi-Label Accuracy

When computing the models' multi-label accuracies, we closely follow the recommendations by Shankar et al. (2020)3. In particular, we discard the set of problematic images, then compute per-class accuracies and finally average them to obtain the multi-label accuracy. We consider a model prediction to be correct if it is marked as correct or unclear in the multi-label dataset.

Footnote 3: https://www.tensorflow.org/datasets/catalog/imagenet2012_multilabel

### Datasets and Labels

We consider the validation set of the ILSVRC-2012 subset of ImageNet(Deng et al., 2009; Russakovsky et al., 2015), available under a non-commercial research license4. More concretely, we use the subset of this validation set labeled by Shankar et al. (2020) and then Vasudevan et al. (2022), with the labels5 being available under Apache License 2.0. We further evaluate our pipeline on the ImageNet-A dataset (Hendrycks et al., 2021) available under MIT License6.

Footnote 4: For more details see https://image-net.org/download

Footnote 5: https://github.com/hendrycks/natural-adv-examples

### Summary of Evaluated Models

Table 3 contains a list of all models we considered in this study and a subset of their metadata. The models were obtained from multiple sources: Torchvision7, torch.hub8, HuggingFace9, and timm10. They can all be automatically downloaded and evaluated using our open-sourced code. After collecting all model outputs (6 days for ImageNet and 1 day for ImageNet-A on a single GeForce RTX 2080 Ti GPU), running our error analysis pipeline on all models takes \(12\) to \(24\) hours using a single GeForce RTX 2080 Ti GPU for ImageNet and ImageNet-A respectively.

Footnote 7: https://pytorch.org/nub/

Footnote 8: https://huggingface.co/models

Footnote 9: https://github.com/huggingface/pytorch-image-models

Table continues onto next page

\begin{table}
\begin{tabular}{c l l l l} \hline \hline No & Model ID & Source & Arch & Dataset \\ \hline
1. & alexnet & torch & cnn & ImageNet-1k \\
2. & bat\_resnext26ts.ch\_in1k & timm & cnn & ImageNet-1k \\
3. & convformer\_b36.sail\_in1k & timm & cnn & ImageNet-1k \\
4. & convformer\_b36.sail\_in1k\_384 & timm & cnn & ImageNet-1k \\
5. & convformer\_b36.sail\_in22k\_ft\_in1k & timm & cnn & ImageNet-21k \\
6. & convformer\_b36.sail\_in22k\_ft\_in1k\_384 & timm & cnn & ImageNet-21k \\
7. & convformer\_m36.sail\_in1k & timm & cnn & ImageNet-1k \\
8. & convformer\_m36.sail\_in1k\_384 & timm & cnn & ImageNet-1k \\
9. & convformer\_m36.sail\_in22k\_ft\_in1k & timm & cnn & ImageNet-21k \\
10. & convformer\_s18.sail\_in1k & timm & cnn & ImageNet-1k \\
11. & convformer\_s18.sail\_in1k & timm & cnn & ImageNet-1k \\
12. & convformer\_s18.sail\_in1k\_384 & timm & cnn & ImageNet-1k \\
13. & convformer\_s18.sail\_in22k\_ft\_in1k & timm & cnn & ImageNet-21k \\
14. & convformer\_s18.sail\_in22k\_ft\_in1k\_384 & timm & cnn & ImageNet-21k \\
15. & convformer\_s36.sail\_in1k & timm & cnn & ImageNet-1k \\
16. & convformer\_s36.sail\_in1k\_384 & timm & cnn & ImageNet-1k \\
17. & convformer\_s36.sail\_in22k\_ft\_in1k & timm & cnn & ImageNet-21k \\ \hline \hline \end{tabular}
\end{table}
Table 3: List of evaluated models and a subset of their metadata.

[MISSING_PAGE_EMPTY:37]

[MISSING_PAGE_EMPTY:38]

[MISSING_PAGE_EMPTY:39]

[MISSING_PAGE_EMPTY:41]

[MISSING_PAGE_EMPTY:43]

\begin{tabular}{l l l l l} \hline \hline No & Model ID & Source & Arch & Dataset \\ \hline
842. & vit\_base\_patch16\_224\_sam\_in1k & timm & vit & ImageNet-1k \\
843. & vit\_base\_patch16\_224\_mini\_in21k\_ft\_in1k & timm & vit & ImageNet-21k \\

[MISSING_PAGE_POST]

_16 & torch & vit & ImageNet-1k \\
875. & vit\_1\_16 & swap & torch & vit & Instagram \\
876. & vit\_1\_32 & torch & vit & ImageNet-1k \\

[MISSING_PAGE_POST]

 \hline \hline \end{tabular} Table continues onto next page 
\begin{tabular}{l l l l l} \hline \hline No & Model ID & Source & Arch & Dataset \\ \hline

[MISSING_PAGE_POST]

ist\_in1k & timm & vit & ImageNet-1k \\ \hline \hline \end{tabular} Table continues onto next page 

### Fine-grained Superclasses

In this section, we detail the superclasses we use for detecting fine-grained and fine-grained OOV errors and their constituting ImageNet classes. When defining these superclasses, we focused on visual and semantic similarity rather than positions in the WordNet hierarchy. We believe that the latter is fundamentally unsuitable to obtain high-quality groupings, as visually similar or closely related classes often end up having a large WordNet distance, while very different classes are close. Further, we ensured that superclass definitions are not overly generic, providing a much more fine-grained split than prior works. The superclasses are defined programmatically in our code and can be easily reused and adapted by others.

List of superclass definitions:

1. **butterfly** - 6 classes: (n02276258, admiral) (n0227742, ringlet) (n02279972, monarch) (n02280649, cabbage butterfly) (n02281406, sulphur butterfly) (n02281787, lycaenid)
2. **beetle_plus** - 13 classes: (n02165105, tiger beetle) (n02167151, ground beetle) (n02169497, leaf beetle) (n02174001, rhinoceros beetle) (n022177972, weevil) (n02226429, grasshopper) (n02229544, cricket) (n02233338, cockroach) (n02256656, cicada) (n02259212, leafhopper)
3. **inset_rest** - 13 classes: (n02190166, fly) (n02206856, bee) (n02219486, ant) (n02226429, grasshopper) (n02229544, cricket) (n02231487, walking stick) (n02233338, cockroach) (n02236044, mantis) (n02256656, cicada) (n02259212, leafhopper) (n02264363, laceving) (n02268443, dragonfly) (n02268853, damselfly)
4. **isopod_trilobite_chiton** - 3 classes: (n01768244, trilobite) (n01955084, chiton) (n01990880, isopod)
5. **crab_scorpion** - 9 classes: (n01770393, scorpion) (n01978287, Dungeness crab) (n01978455, rock crab) (n01980166, fiddler crab) (n01981276, king crab) (n01983481, American lobster) (n01984695, spiny lobster) (n01985128, crayfish) (n01986214, hermit crab)
6. **spider** - 8 classes: (n01770881, harvestman) (n01773157, black and gold garden spider) (n01773549, barn spider) (n01773797, garden spider) (n01774384, black widow) (n01774750, tarantula) (n01775062, wolf spider) (n01776313, tick)
7. **marine_invertebrate** - 7 classes:(n01914609, sea anemone) (n01950731, sea slug) (n02319095, sea urchin) (n12985857, coral fungus)
8. _conch_champered_nautius - 2 classes: (n01943899, conch)
9. _invertebrate_rest - 7 classes: (n01784675, centipede) (n01930112, nematode) (n0194390, snail) (n01945685, slug) (n02321529, sea cucumber)
10. _salamander_lizard - 16 classes: (n01629819, European fires salamander) (n01630670, common newt) (n01631663, eft) (n01632458, spotted salamander) (n01632777, axolott) (n01675722, banded gecko) (n01677366, common iguana) (n01685808, whiptail) (n01687978, agama) (n01688243, frilted lizard) (n01692333, Gila monster) (n01693334, green lizard) (n01694178, African chameleon) (n01695060, Komondo dragon)
11. _frog_ - 3 classes: (n01641577, bullfrog) (n01644373, tree frog) (n01644900, tailed frog)
12. _shark_and_aquatic_mammal - 7 classes: (n01484850, great white shark) (n01491361, tiger shark) (n01494475, hammerhead) (n02066245, grey whale) (n02071294, killer whale) (n02074367, dugong) (n02077923, sea lion)
13. _ray_ - 2 classes: (n01496331, electric ray) (n01498041, stingray)
14. _fish_rest - 11 classes: (n01440764, tench) (n01443537, goldfish) (n02514041, barracouta) (n02526121, eel) (n02536864, coho) (n02606052, rock beauty) (n02607072, anemone fish) (n02640242, sturgeon) (n02641379, gar) (n02643566, lionfish) (n02655020, puffer)
15. _turtle_ - 5 classes: (n01664065, loggerhead) (n01665541, leatherback turtle) (n01667114, mud turtle) (n01667778, terrapin) (n01669191, box turtle)
16. _crocodilian_reptite_ - 2 classes: (n01697457, African crocodile) (n01698640, American alligator)
17. _snake_ - 17 classes: (n01728572, thunder snake) (n01729322, hognose snake) (n01734418, king snake) (n01737021, water snake) (n01740131, night snake) (n01744401, rock python) (n01749939, green mamba) (n01753488, horned viper) (n01756291, sidewinder)
18. _green_snake_and_lizards - 5 classes:(n01682714, American chameleon) (n01729977, green snake) (n01749939, green mamba)
19. _non_green_snakes_ - 15 classes: (n01689811, alligator lizard) (n01728920, ringneck snake) (n01734418, king snake) (n01737021, water snake) (n01742172, boa constrictor) (n01748264, Indian cobra) (n01753488, horned wiper) (n01756291, sidewinder)
20. _cock_hen_galtinaceous_ - 12 classes: (n01514668, cock) (n01795545, black grouse) (n01797886, ruffed grouse) (n01806143, peacock) (n01807496, partitridge) (n02018207, American coot)
21. _small_bird_ - 15 classes: (n01530575, brandling) (n01532829, house finch) (n01534433, junco) (n01537544, indigo bunting) (n01558993, robin) (n01560419, bubul) (n01580077, jay) (n01582220, mappie) (n01592084, chickade) (n01601694, water ouzel) (n01824575, coucal) (n01828970, bee eater) (n01833805, hummingbird) (n01843065, jacamar)
22. _bird_of_prey_ - 3 classes: (n01608432, kite) (n01614925, bald eagle) (n01616318, vulture)
23. _parrot_ - 4 classes: (n01817953, African grey) (n01818515, macaw) (n01819313, sulphur-crested cockatoo) (n01820546, lorikeet)
24. _big_beak_bird_ - 2 classes: (n01829413, hornbill) (n01843383, toucan)
25. _aquatic_bird_and_ostrich_ - 25 classes: (n01518878, ostrich) (n01847000, drake) (n01855932, red-breasted merganser) (n01855672, goose) (n01860187, black swan) (n02002556, white stork) (n02002724, black stork) (n02007558, flamingo) (n02009912, American egret) (n02012849, crane) (n02017213, European galinule) (n02018795, bustard) (n02027492, red-backed sandpiper) (n02033041, dowitcher) (n02051845, pelican) (n02058221, albatross)
26. _tusker_elephant_ - 3 classes: (n01871265, tusker) (n02504458, African elephant)
27. _primate_ - 20 classes: (n0169334, green lizard) (n01739381, vine snake)(n02480495, orangutan) (n02481823, chimpanzee) (n02483708, siamang) (n02486261, patas) (n02487347, macaque) (n0248702, colobus) (n02490219, marmoset) (n02492669, howler monkey) (n02493793, spider monkey) (n02497673, Madagascar cat)
28. **bear_panda_marsupial** - 10 classes: (n01877812, wallaby) (n01883070, wombat) (n02133161, American black bear) (n02134418, sloth bear) (n02509815, lesser panda)
29. **viverrine_musteline_rodent_prototherian** - 19 classes: (n01872401, echidna) (n01877812, wallaby) (n02138441, meerkat) (n02346627, porcupine) (n02361337, marmot) (n02364673, guinea pig) (n02442845, mink) (n0244314, polecat) (n02443484, black-footed ferret) (n02444819, ofter) (n02445715, skunk) (n02447366, badger) (n02454379, armadillo)
30. **rabbit** - 3 classes: (n02325366, wood rabbit) (n02328159, Angora)
31. **ungulate** - 17 classes: (n02389026, sorrel) (n02395406, hog) (n02397096, warthog) (n02403083, ox) (n02410509, hignon) (n02412808, ram) (n02417914, ibex) (n02422106, hartebeest) (n02422699, impala) (n02423022, gazelle) (n02437312, Arabian camel)
32. **domestic_cat** - 5 classes: (n02123045, tabby) (n02123394, Persian cat) (n02124075, Egyptian cat)
33. **wild_cat_big_cat** - 8 classes: (n02125311, cougar) (n02128385, leopard) (n02128925, jaguar) (n02129604, tiger) (n02129604, tiger)
34. **wolf_wild_dog_hyena_fox** - 12 classes: (n02114367, timber wolf) (n02114712, red wolf) (n02115641, dingo) (n02116738, African hunting dog) (n02119022, red fox) (n02120079, Arctic fox) (n02480855, goril(a) (n02483362, gibbon) (n02484975, guenon) (n02486410, baboon) (n02488291, languur) (n02489166, probocsis monkey) (n02492035, capuchin) (n02493590, titi) (n02494079, squirrel monkey) (n02500267, indiri)
28. **bear_panda_marsupial** - 10 classes: (n01877812, wallaby) (n01882714, koala) (n02132136, brown bear) (n02134084, ice bear) (n024574088, three-toed sloth) (n02510455, giant panda)
29. **viverrine_musteline_rodent_prototherian** - 19 classes: (n01872401, echidna) (n01873310, platypus) (n02137549, mongoose) (n02342885, hamster) (n02356798, fox squirrel) (n02363085, beaver) (n02441942, weaseal) (n02443114, polecat) (n02444819, ofter) (n02447366, badger)
30. **rabbit** - 3 classes: (n02325366, wood rabbit) (n02326432, hare) (n02328159, Angora)
31. **ungulate** - 17 classes: (n02389026, sorrel) (n02391049, zebra) (n02395406, hog) (n02396427, wild boar) (n02397096, warthog) (n02398521, hippopotanus) (n02403083, ox) (n02408429, water buffalo) (n02412808, ram) (n02412808, ram) (n02417914, ibex) (n02422106, hartebeest) (n02422699, impala) (n02437312, Arabian camel)
32. **domestic_cat** - 5 classes: (n02123045, tabby) (n02123159, tiger cat) (n02123394, Persian cat) (n02123597, Siamese cat) (n02124075, Egyptian cat)
33. **wild_cat_big_cat** - 8 classes: (n02125311, cougar) (n02127052, lynx) (n02128385, leopard) (n02128757, snow leopard) (n02128925, jaguar) (n02129165, lion) (n02129604, tiger) (n02130308, cheetah)
34. **wolf_wild_dog_hyena_fox** - 12 classes: (n02114367, timber wolf) (n02114548, white wolf) (n02114855, coyote) (n02115913, dhole) (n02117135, hyena) (n02119789, kit fox) (n02120505, grey fox)35. **pug_boxer_builddog** - 9 classes: (n02093256, Staffordshire bullterrier) (n02096585, Boston bull) (n021088089, boxer) (n02108915, French bulldog) (n02112706, Brabancon grifon)
36. **toy_dog** - 31 classes: (n02085620, Chihuahua) (n02085936, Maltese dog) (n02086240, Shih-Tzu) (n020866910, papillon) (n02087046, toy terrier) (n02093754, Border terrier) (n02094258, Norwich terrier) (n02094258, Norwich terrier) (n02096177, cairn) (n02096437, Dandie Dinmont) (n02097047, miniature schnauzer) (n02097658, silky terrier) (n0209826, West Highland white terrier) (n02102318, cocker spaniel) (n02108915, French bulldog) (n02112018, Pomeranian) (n02113624, toy poodle) (n02113799, standard poodle)
37. **corgi** - 2 classes: (n02113023, Pembroke) (n02113186, Cardigan)
38. **terrier** - 26 classes: (n02090721, Irish wolfhound) (n02091635, ottrehound) (n02093647, Bedlington terrier) (n02093754, Border terrier) (n02093859, Kerry blue terrier) (n02094114, Norfolk terrier) (n02094433, Yorkshire terrier) (n02095576, Lakeland terrier) (n02096051, Airedale) (n02096294, Australian terrier) (n02097047, miniature schnauzer) (n02097209, standard schnauzer) (n02097474, Tibetan terrier) (n02098105, soft-coated wheaten terrier) (n02098413, Lhasa)
39. **poodle_Like** - 7 classes: (n02093647, Bedlington terrier) (n02093859, Kerry blue terrier) (n021099429, curly-coated retriever) (n02113624, toy poodle) (n02113712, miniature poodle) (n02113799, standard poodle)
40. **standard_shepherd_dog** - 11 classes: (n02091467, Norwegian etkhound) (n02105056, groenendael) (n02105412, kelpie) (n02106030, collie) (n02106550, Rottweiler) (n02112350, keeshond)
41. **hairy_shepherd_dog** - 6 classes: (n02097474, Tibetan terrier) (n02105505, komomondor) (n02106382, Bouvier des Flandres) (n02093428, American Staffordshire terrier) (n02106550, Rottweiler) (n02108422, bull mastiff) (n02110958, bug)
42. **toy_dog** - 31 classes: (n02085782, Japanese spaniel) (n02086079, Pekinee) (n02086646, Blenhenhen spaniel) (n02087046, toy terrier) (n02089414, Norfolk terrier) (n02094433, Yorkshire terrier) (n02096294, Australian terrier) (n02096585, Boston bull) (n02097047, Thibetan terrier) (n02097474, Tibetan terrier) (n020998105, soft-coated wheaten terrier) (n02098105, soft-coated wheaten terrier) (n02098413, Lhasa) (n02102318, cocker spaniel) (n02107312, miniature pinscher) (n021108915, French bulldog) (n02110627, afterpinscher) (n02112018, Pomeranian) (n02112706, Brabancon grifon) (n02113624, toy poodle) (n02113712, miniature poodle)
37. **corgi** - 2 classes: (n02113023, Pembroke) (n02113186, Cardigan)
38. **terrier** - 26 classes: (n02090721, Irish wolfhound) (n02091635, ottrehound) (n02093647, Bedlington terrier) (n02093754, Border terrier) (n02093859, Kerry blue terrier) (n02093991, Irish terrier) (n02094114, Norfolk terrier) (n02094258, Norwich terrier) (n02094433, Yorkshire terrier) (n02095314, wire-haired fox terrier) (n02095576, Lakeland terrier) (n0209589, Sealyham terrier) (n02096177, cairn) (n02096437, Dandie Dinmont) (n02097130, giant schnauzer) (n02097298, Scotch terrier) (n02097658, silky terrier) (n02098105, soft-coated wheaten terrier) (n02098286, West Highland white terrier) (n02098413, Lhasa) (n02110627, afterpinscher)
39. **poodle_Like** - 7 classes: (n02093647, Bedlington terrier) (n02093647, Bedlington terrier) (n02093659, Kerry blue terrier) (n02099429, curly-coated retriever) (n02113624, toy poodle) (n02113799, standard poodle)
40. **standard_shepherd_dog** - 11 classes: (n02091467, Norwegian etkhound) (n02104365, schipperke) (n02105056, groenendael) (n02105162, malinois) (n02105412, kelpie) (n02105855, Shetland sheepdog) (n02106030, collie) (n02106166, Border colllie) (n02106550, Rottweiler) (n02106662, German shepherd)
41. **hairy_shepherd_dog** - 6 classes: (n02097474, Tibetan terrier) (n02105251, briard) (n02105541, Old English sheepdog) (n02110627, afterpinscher)42. **beagle_hound_pointer_spaniel_setter** - 27 classes: (n02087394, Rhodesian ridgeback) (n02088364, beagle) (n02088362, bluetick) (n02089867, Walker hound) (n0209037, redbone) (n02108236, german short-haired pointer) (n02108735, English setter) (n02101006, Gordon setter) (n02101556, clumber) (n02102177, Welsh springer spaniel) (n02102480, Sussex spaniel) (n02107142, Doberman) (n02110341, dalmatian) (n02113978, Mexican hairless)
43. **spaniels** - 5 classes: (n02086646, Blenheim spaniel) (n02102040, English springer) (n02102318, cocker spaniel)
44. **skinny_hound_rest** - 12 classes: (n02088094, Afghan hound) (n02090721, Irish wolfhound) (n02091134, whippet) (n02091635, otterhound) (n02092002, Scottish deerhound) (n02110806, basenji)
45. **pingscher** - 3 classes: (n02106550, Rottweiler) (n02107312, miniature pinscher)
46. **sennenhunde_and_big_dogs** - 15 classes: (n02088364, beagle) (n02106550, Rottweiler) (n02107683, Bernese mountain dog) (n02108090, EntleBucher) (n02108422, bull mastiff) (n02109947, Great Dane) (n02111129, Leonberg) (n02112137, chow)
47. **retriever_Like** - 10 classes: (n02090379, redbone) (n02099429, curly-coated retriever) (n02099712, Labrador retriever) (n02101006, Gordon setter) (n02111277, Newfoundland)
48. **husky** - 6 classes: (n02091467, Norwegian ekhound) (n02110063, malamute) (n02111889, Samoyed)
49. **spitz** - 4 classes: (n02111889, Samoyed) (n02112137, chow)
50. **fungi** - 7 classes: (n07734744, mushroom) (n13037406, gyromitra) (n13044778, earthstar) (n13054560, bolete) classes: (n02088238, basset) (n02088466, bloodohound) (n02089078, black-and-tan coonhound) (n02089973, English foxhound) (n02092339, weimaraner) (n02109583, visala) (n02100877, Irish setter) (n02101388, Brittany spaniel) (n02102040, English springer) (n02102318, cocker spaniel) (n02102973, Irish water spaniel) (n02109047, Great Dane) (n02110806, basenji)
50. **fungi** - 7 classes: (n02101388, Brittany spaniel) (n02102177, Welsh springer spaniel) (n02102170, Welsh springer spaniel)
44. **skinny_hound_rest** - 12 classes: (n02090622, borzoi) (n02090721, Irish wolfhound) (n02091032, Italian greyhound) (n02091244, Ibizan hound) (n02091831, Saluki) (n02107142, Doberman) (n02113978, Mexican hairless)
45. **pingscher** - 3 classes: (n02106550, Rottweiler) (n02107312, miniature pinscher)
46. **sennenhunde_and_big_dogs** - 15 classes: (n02088364, beagle) (n02106550, Rottweiler) (n02107683, Bernese mountain dog) (n02108090, EntleBucher) (n02108042, bul mastiff) (n02109947, Great Dane) (n02111129, Leonberg) (n02112137, chow)
47. **retriever_Like** - 10 classes: (n020990379, redbone) (n02099429, curly-coated retriever) (n020999712, Labrador retriever) (n02101006, Gordon setter) (n02111277, Newfoundland)
48. **husky** - 6 classes: (n02091467, Norwegian ekhound) (n02110063, malamute) (n02111889, Samoyed) (n02111889, Samoyed)
49. **spitz** - 4 classes: (n02111889, Samoyed) (n02111889, Samoyed) (n02112137, chow)
50. **fungi** - 7 classes: (n07734744, mushroom) (n13037406, gyromitra) (n13044778, earthstar) (n13054560, bolete) classes: (n02088238, basset) (n02088466, bloodohound) (n02089078, black-and-tan coonhound) (n020889973, English foxhound) (n02092339, weimaraner) (n02109583, visala) (n02100877, Irish setter) (n02101388, Brittany spaniel) (n02102040, English springer) (n02102318, cocker spaniel) (n02102973, Irish water spaniel) (n02109047, Great Dane) (n02110806, basenji)
50. **fungi** - 7 classes: (n02101388, Brittany spaniel) (n02102177, Welsh springer spaniel) (n021021071, bovelsh springer spaniel) (n02090622, borzoi) (n02091032, Italian greyhound) (n02091244, Ibizan hound) (n02091831, Saluki) (n02107142, Doberman) (n02113978, Mexican hairless)
51. **fungi** - 7 classes: (n02106166, Border collie) (n02107574, Greater Swiss Mountain dog) (n02107908, Appenzeller) (n02108089, boxer) (n02108551, Tibetan mastiff) (n02109525, Saint Bernard) (n02111277, Newfoundland)
52. **fungi** - 7 classes: (n02099267, flat-coated retriever) (n02099601, golden retriever) (n020999849, Chesapeake Bay retriever) (n02104029, kuvasz) (n021111508, Great Pyrenees) (n021109961, Eskimo dog) (n02110185, Siberian husky) (n02112350, keeshond)
53. **fungi** - 7 classes: (n021109961, Eskimo dog) (n02110185, Siberian husky) (n02112350, keeshond)
54. **fungi** - 7 classes: (n02112018, Pomeranian) (n02112350, keeshond)
55. **fungi** - 7 classes: (n07734744, mushroom) (n12998815, agaric) (n13040303, stinkhorn) (n13052670, hen-of-the-woods)51. **geological_formation** - 9 classes: (n09193705, alp) (n09288635, geyser) (n09399592, promontory) (n09428293, seashore) (n09472597, volcano)
52. **bread** - 4 classes: (n07684084, French loaf) (n07693725, bagel) (n07695742, pretzel) (n07860988, dough)
53. **cruiciferous_vegetable** - 3 classes: (n07714571, head cabbage) (n07714990, broccoli) (n07715103, cauliflower)
54. **cauliflower_custard_apple** - 2 classes: (n07715103, cauliflower) (n07760859, custard apple)
55. **zucchini_cucumber** - 2 classes: (n07716358, zucchini) (n07718472, cucumber)
56. **squash** - 4 classes: (n07716358, zucchini) (n07716906, spaghetti squash) (n07717410, acorn squash) (n07717556, butternut squash)
57. **reproductive_structure** - 12 classes: (n07720875, belt pepper) (n07742313, Granny Smith) (n07745949, strawberry) (n07747607, orange) (n07749582, leemon) (n07753113, fig) (n07754684, jackfruit) (n07760859, custard apple) (n07768694, pomegranate) (n12267677, acorn) (n12620546, hip) (n12768682, buckeye)
58. **corn** - 2 classes: (n12144580, corn) (n13133613, ear)
59. **cardoon_artichoke** - 3 classes: (n07718747, artichoke) (n07730033, cardoon) (n07753275, pineapple)
60. **desert** - 3 classes: (n07613480, trifle) (n07614500, ice cream) (n07836838, chocolate sauce)
61. **baby_bed** - 3 classes: (n02804414, bassinet) (n03125729, cradle) (n03131574, crib)
62. **chair** - 4 classes: (n02791124, barber chair) (n03376595, folding chair) (n04099969, rocking chair) (n04429376, throne)
63. **entertainment_center_home_theatre** - 2 classes: (n03290653, entertainment center) (n03529860, home theater)
64. **cabinet** - 7 classes: (n02870880, bookcase) (n03016953, chiffonier) (n03018349, china cabinet) (n03290653, entertainment center) (n03337140, file) (n03742115, medicine chest) (n04550184, wardrobe)
65. **table** - 2 classes: (n03179701, desk) (n03201208, dining table)
66. **robe_overgarment_cloak** - 10 classes:(n02667093, abaya) (n02730930, apron) (n03045698, cloak) (n03404251, fur coat) (n03617480, kimono) (n03630383, lab coat) (n03980874, poncho) (n04479046, trench coat) (n04532106, vestment)
67. sweater - 3 classes: (n02963159, cardigan) (n03980874, poncho) (n04370456, sweatshirt)
68. suits - 3 classes: (n03763968, military uniform) (n03877472, pajama) (n04359905, suit)
69. woolen_pieces - 7 classes: (n02963159, cardigan) (n03207743, dishrag) (n03775071, mitten) (n03980874, poncho) (n04229816, ski mask) (n04325704, stole) (n04599235, wool)
70. caps - 3 classes: (n02807133, bathing cap) (n02869837, bonnet) (n04209133, shower cap)
71. swimsuit - 4 classes: (n02837789, bikini) (n02892767, brassiere) (n03710637, mailtot) (n03710721, mailtot)
72. woman_dress - 3 classes: (n03450230, gown) (n03534580, hoopskirt) (n03866082, overskirt)
73. neckwear - 4 classes: (n02865351, bolo tie) (n03814906, necklace) (n04591157, Windsor tie)
74. sock_and_glove - 3 classes: (n03026566, Christmas stocking) (n03775071, mitten) (n04254777, sock)
75. breathing_device - 3 classes: (n03424325, gasmask) (n03868863, oxygen mask) (n04251144, snorket)
76. helmet - 3 classes: (n03127747, crash helmet) (n03379051, football helmet) (n03929855, picklehaube)
77. picklehaube_bearskin - 2 classes: (n02817516, bearskin) (n03929855, picklehaube)
78. hat - 3 classes: (n02869837, bonnet) (n03124170, cowboy hat) (n04259630, sombrero)
79. armor - 4 classes: (n02895154, breastplate) (n03000247, chain mail) (n03146219, cuirass) (n04192698, shield)
80. cuirass_bulletproof_vest - 2 classes: (n02916936, bulletproof vest) (n03146219, cuirass)
81. footwear_rest - 5 classes: (n03047690, clog) (n03124043, cowboy boot) (n03680355, Loafer) (n04120489, running shoe)
82. top_cover - 4 classes: (n02877765, bottlecap) (n03657121, lens cap) (n03717622, manhole cover) (n04019541, puck)83. **lamp_Light** - 5 classes: (n02948072, candle) (n03590841, jack-o'-lantern) (n03637318, lampshade) (n04286575, spotlight) (n04380533, table lamp)
84. **candle_torch** - 2 classes: (n02948072, candle) (n04456115, torch)
85. **ship_boat** - 15 classes: (n02687172, aircraft carrier) (n02951358, canoe) (n02981792, catamaran) (n03095699, container ship) (n03344393, fireboat) (n03447447, gondola) (n03662601, lifeboat) (n03673027, liner) (n03947888, pirate) (n04147183, schooner) (n04273569, speedboat) (n04347754, submarine) (n04483307, trimaran) (n04606251, wreck) (n04612504, yaw)
86. **tank** - 3 classes: (n02704792, amphibian) (n03478589, half track) (n04389083, tank)
87. **snow_sled** - 3 classes: (n02860847, bobsled) (n03218198, dogsled) (n04252077, snowmobile)
88. **motor_cycle** - 6 classes: (n02835271, bicycle-built-for-two) (n03785016, moped) (n03791053, motor scooter) (n03792782, mountain bike) (n04482393, tricycle) (n04509417, unicycle)
89. **aircraft** - 3 classes: (n02690373, airliner) (n02692877, airship) (n04552348, warplane)
90. **balloon** - 3 classes: (n02692877, airship) (n02782093, balloon) (n03888257, parachute)
91. **train** - 6 classes: (n02917067, bullet train) (n03272562, electric locomotive) (n03393912, freight car) (n03895866, passenger car) (n04310018, steam locomotive) (n04335435, streetcar)
92. **car_bus_truck** - 23 classes: (n02701002, ambulance) (n02704792, amphibian) (n02814533, beach wagon) (n02930766, cab) (n03100240, convertible) (n03345487, fire engine) (n03417042, garbage truck) (n03594945, jeep) (n03670208, limousine) (n03769881, minibus) (n03770679, minivan) (n03777568, Model T) (n03796401, moving van) (n03930630, pickup) (n03977966, police van) (n04037443, racer) (n04065272, recreational vehicle) (n04146614, school bus) (n0425225, snowplow) (n04285008, sports car) (n04461696, tow truck) (n04467665, trailer truck) (n04487081, trolleybus)
93. **racer_go_kart** - 2 classes: (n03444034, go-kart) (n04037443, racer)
94. **work_cart** - 4 classes: (n03384352, forklift) (n03444034, go-kart) (n03445924, gofcart) (n03649909, lawm mower)

[MISSING_PAGE_FAIL:55]

108. **player** - 4 classes:

(n02979186, cassette player) (n02988304, CD player)

(n04041544, radio) (n04392985, tape player)

109. **small_electronic_equipment** - 9 classes:

(n02978881, cassette) (n02992529, cellular telephone)

(n03271574, electric fan) (n03485407, hand-held computer)

(n03492542, hard disc) (n03584254, iPod)

(n03602883, joystick) (n04074963, remote control)

(n04372370, switch)

110. **phone** - 3 classes:

(n02992529, cellular telephone) (n03187595, dial telephone)

(n03902125, pay-phone)

111. **radio_modem** - 3 classes:

(n03492542, hard disc) (n03777754, modem)

(n04041544, radio)

112. **big_electronic_equipment** - 6 classes:

(n03691459, loudspeaker) (n0377754, modem)

(n03857828, oscilloscope) (n03924679, photocopier)

(n04094767, printer) (n04099552, projector)

113. **clock** - 9 classes:

(n02708093, analog clock) (n02794156, barometer)

(n03706229, magnetic compass) (n03841143, odometer)

(n03891332, parking meter) (n04141975, scale)

(n04328186, stopwatch) (n04355338, sundial)

(n04548280, wall clock)

114. **digital_clock** - 5 classes:

(n03196217, digital clock) (n03197337, digital watch)

(n04141975, scale) (n04149813, scoreboard)

(n04328186, stopwatch)

115. **ruler** - 3 classes:

(n04118776, rule) (n04238763, slide rule)

(n04376876, syringe)

116. **firearm** - 3 classes:

(n02749479, assault rifle) (n04086273, revolver)

(n04090263, rifle)

117. **gymnastic_apparatus** - 3 classes:

(n02777292, balance beam) (n03535780, horizontal bar)

(n03888605, parallel bars)

118. **barbelt_dumbbell** - 2 classes:

(n02790996, barbelt) (n03255030, dumbbell)

119. **dish** - 13 classes:

(n04263257, soup bowl) (n0759787, plate)

(n07583066, guacamole) (n07584110, consomme)

(n07590611, hot pot) (n07697313, cheeseburger)

(n07697537, hotdog) (n07711569, mashed potato)

(n07831146, carbononara) (n07871810, meat loaf)

(n07873807, pizza) (n07875152, potpie)

(n07880968, burrito)

120. **beverage_pot_jug** - 18 classes:(n62815834, beaker) (n03062245, cocktail shaker) (n03063689, coffeeeot) (n037333805, measuring cup) (n04131690, sultshaker) (n04398044, teapot) (n04560804, water jug) (n07892512, red wine) (n07930864, cup)
121. spatula_spoon_Ladle - 3 classes: (n03633091, ladle) (n04597913, wooden spoon)
122. utensils - 11 classes: (n02909870, bucket) (n03133878, Crock Pot) (n03400231, frying pan) (n03786901, mortar) (n04263257, soup bowl) (n04596742, wok)
123. bucket_pot - 2 classes: (n02909870, bucket) (n02909870, bucket)
124. bottle - 4 classes: (n02823428, beer bottle) (n03983396, pop bottle) (n04557648, water bottle) (n04591713, wine bottle)
125. tub_basin - 3 classes: (n02888440, bathtub) (n04493381, tub) (n04553703, washbasin)
126. barrel_can - 4 classes: (n02747177, ashcan) (n02795169, barrel) (n03764736, milk can) (n04049303, rain barrel)
127. big_appliance - 7 classes: (n03207941, dishwasher) (n03297495, espresso maker) (n03761084, microwave) (n04070727, refrigerator) (n04111531, rotisserie) (n04330267, stove)
128. kitchen_appliance - 4 classes: (n03297495, espresso maker) (n04442312, toaster)
129. small_appliance - 4 classes: (n03483316, hand blower) (n04179913, sewing machine) (n04517823, vacuum)
130. oven_stove - 5 classes: (n03761084, microwave) (n04111531, rotisserie) (n04330267, stove) (n04442312, toaster) (n04542943, waffle iron)
131. space_heater_radiator - 2 classes: (n04040759, radiator) (n04265275, space heater)
132. paper_tissue - 2 classes: (n03887697, paper towel)
133. cover_presentation - 10 classes: (n02786058, Band Aid) (n03291819, envelope) (n03871628, packet) (n06596364, comic book) (n07248320, book jacket) (n02823758, beer glass) (n03063599, coffee mug) (n03443371, goblet) (n03950228, pitcher) (n04254120, soap dispenser) (n04522168, vase) (n04579145, whiskey jug) (n07920052, espresso) (n07932039, egongo)

(n04270147, spatula)

(n04270147, spatula)

(n02939185, caldron) (n03259280, Dutch oven) (n03775546, mixing bowl) (n03992509, potter's wheel) (n04332243, strainer)

(n03991062, pot)

(n03983396, pop bottle) (n04591713, wine bottle)

(n04591713, wine bottle)

(n04493381, tub)

(n04593703, washbasin)

126. barrel_can - 4 classes: (n02795169, barrel) (n03764736, milk can) (n04049303, rain barrel)
127. big_appliance - 7 classes: (n03207941, dishwasher) (n03297495, espresso maker) (n03761084, microwave) (n04070727, refrigerator) (n04111531, rotisserie) (n04330267, stove)

(n04330267, stove)

(n03297495, espresso maker) (n04470727, refrigerator) (n04330267, stove)

(n04554684, washer)

(n0442312, toaster)

(n04542943, waffle iron)

129. small_appliance - 4 classes: (n03483316, hand blower) (n03584829, iron) (n04179913, sewing machine) (n04517823, vacuum)

130. oven_stove - 5 classes: (n03761084, microwave) (n04111531, rotisserie) (n04330267, stove) (n04442312, toaster) (n04542943, waffle iron)

131. space_heater_radiator - 2 classes: (n04040759, radiator) (n04265275, space heater)

132. paper_tissue - 2 classes: (n03887697, paper towel) (n15075141, toilet tissue)

133. cover_presentation - 10 classes: (n02786058, Band Aid) (n02786058, Band Aid) (n03291819, envelope) (n03291819, envelope) (n03871628, packet) (n04263257, soup bowl) (n04596364, comic book) (n07248320, book jacket) (n02823758, beer glass) (n03063599, coffee mug) (n03443371, goblet) (n03950228, pitcher) (n04254120, soap dispenser) (n04522168, vase) (n04579145, whiskey jug) (n07920052, espresso) (n07932039, egongo)

(n04270147, spatula)

(n042939185, caldron) (n03259280, Dutch oven) (n03775546, mixing bowl) (n03992509, potter's wheel) (n04332243, strainer)

(n04332243, strainer)

(n043991062, pot)

(n04591062, pot)

(n04591713, wine bottle)

(n04493381, tub)

(n04593703, washbasin)

126. barrel_can - 4 classes: (n02747177, ashcan) (n02795169, barrel) (n03764736, milk can) (n04049303, rain barrel)

127. big_appliance - 7 classes: (n03207941, dishwasher) (n03297495, espresso maker) (n03761084, microwave) (n04070727, refrigerator) (n0411531, rotisserie) (n04330267, stove)

128. kitchen_appliance - 4 classes: (n03297495, espresso maker) (n03761084, microwave) (n04442312, toaster) (n04542943, waffle iron)

129. small_appliance - 4 classes: (n03483316, hand blower) (n04179913, sewing machine) (n04179913, sewing machine) (n0417823, vacuum)

130. oven_stove - 5 classes: (n03761084, microwave) (n04330267, stove) (n04430267, stove) (n04542943, waffle iron)

131. space_heater_radiator - 2 classes: (n04040759, radiator) (n04265275, space heater)

132. paper_tissue - 2 classes: (n03887697, paper towel) (n03887697, paper towel) (n0429387697, paper towel) (n043330267, poset) (n04330267, stove) (n04442312, toaster)

(n04542943, waffle iron)

(n- 5 classes: (n03314780, face powder) (n03476991, hair spray) (n03690938, lotion) (n03916031, perfume) (n04357314, sunscreen)
* 2 classes: (n03388549, four-poster) (n04344873, studio couch)
* 8 classes: (n02808304, bath towel) (n02834397, bib) (n03207743, dishrag) (n03223299, doormat) (n03485794, handkerchief) (n03998194, prayer rug) (n04209239, shower curtain) (n04525038, velvet)
* 12 classes: (n02699494, altar) (n02825657, belt cote) (n02980441, castle) (n03028079, church) (n03220513, dome) (n03781244, monastery) (n03788195, mosque) (n03877845, palace) (n03956157, planetarium) (n04346328, stupa) (n04486054, triumphal arch) (n04523525, vault)
* 3 classes: (n03355925, flagpole) (n03733131, maybe) (n03976657, pole)
* 4 classes: (n03837869, obelisk) (n03903868, pedestal) (n03976657, pole) (n04458633, totem pole)
* 26 classes: (n02666196, abacus) (n02783161, ballpoint) (n02996734, broom) (n03141823, crutch) (n03250847, drumstick) (n03355925, flagpole) (n03388183, fountain pen) (n03658185, letter opener) (n03729826, matchstick) (n03759954, microphone) (n03804744, nail) (n03873416, paddle) (n03970156, plunpne) (n04033901, quill) (n040439381, racket) (n04141327, scabbard) (n04208210, shovel) (n04277352, spindle) (n04485082, tripod)
* 4 classes: (n02977058, cash machine) (n03425413, gas pump) (n04243546, slot) (n04525305, vending machine)
* 6 classes: (n03496892, harvester) (n03967562, plow) (n04252225, snowplow) (n04428191, thresher) (n04465501, tractor)
* 2 classes: (n03041632, cleaver) (n03498962, hatchet)
* 8 classes: (n02951585, can opener) (n02966687, carpenter's kit) (n0300684, chain saw) (n03109150, corkscrew) (n03954731, plane) (n04154565, screwdriver)
* 2 classes: (n04355933, sunglass) (n04356056, sunglasses)
- 4 classes: (n02814860, beacon) (n02859443, boathouse) (n02894605, breakwater) (n03216828, dock)
* 2 classes: (n02814860, beacon) (n04562935, water tower)
* 3 classes: (n02841315, binoculars) (n03976467, Polaroid camera) (n0469434, reflex camera)
* 2 classes: (n03075370, combination lock) (n04125021, safe)
* 2 classes: (n03075370, combination lock) (n03874599, padlock)
* 9 classes: (n02727426, apiary) (n02843684, birdhouse) (n02971356, carton) (n03014705, chest) (n03127925, crate) (n03482405, hamper) (n03710193, mailbox) (n04204238, shopping basket) (n04204347, shopping cart)
* 2 classes: (n02747177, ashcan) (n03710193, mailbox)
* 4 classes: (n02769748, backpack) (n03709823, mailbag) (n04826417, purse) (n04548362, wallet)
* 3 classes: (n03908618, pencil box) (n04026417, purse) (n04548362, wallet)
* 2 classes: (n04201297, shoji) (n04239074, sliding door)
* 2 classes: (n03126707, crane) (n03240683, drilling platform)
* 16 classes: (n02788148, bannister) (n0300134, chainlink fence) (n033065424, coil) (n03347037, fire screen) (n03530642, honeycomb) (n04905630, prison) (n04040759, radiator) (n04265275, space heater) (n04275548, spider web) (n04326547, stone wall) (n04589990, window screen) (n04590129, window shade) (n04604644, worm fence)
* 2 classes: (n04417672, thatch) (n04435653, tile roof)
* 20 classes: (n02865351, bolo tie) (n03314780, face powder) (n03532672, hook) (n03676483, lipstick) (n03794956, mousestrap) (n03840681, ocarina) (n03908714, pencil sharpener) (n04116512, rubber eraser) (n04131698, saltshaker) (n04423845, thimble)
- 7 classes: (n02793495, barn) (n03457902, greenhouse) (n03776460, mobile home) (n04613696, yurt)
* 2 classes: (n04044716, radio telescope)
* 74 classes: (n01622779, great grey ow() (n01910747, jellyfish) (n02879718, bow) (n02950826, cannon) (n02966193, carousel) (n03042490, Cliff dwelling) (n03208938, disk brake) (n03388043, fountain) (n03494278, harmonica) (n0352744, holstr) (n03594734, jean) (n03623198, knee pad) (n03720891, maraca) (n03733281, maze) (n03770439, miniskirt) (n03788365, mosquito net) (n03793489, mouse) (n03814639, neck brace) (n03884397, panpipe) (n03920288, Petri dish) (n03937543, pill bottle) (n03958227, plastic bag) (n03982430, pool table) (n04033995, quilt) (n04162706, seat belt) (n04296562, stage) (n04336792, stretcher) (n0437174, swing) (n044318357, theater curtain) (n04476259, tray) (n04584207, wig) (n06794110, street sign) (n07615774, ice lolly) (n07802026, hay) (n09835506, bal[player] (n10565667, scuba diver) (n11939491, daisy) (n02859443, boathouse) (n03697007, lumberill) (n03899768, patio)
* [63] (n04258138, solar dish)
* [64] (n04258138, solar dish)
* [65] (n01704323, triceratops) (n02672831, accordion) (n02892201, brass) (n02965783, car mirror) (n02974003, car wheel) (n03188531, diaper) (n03325584, feather boa) (n03467068, guilotine) (n03495258, harp) (n03544143, hourglass) (n03595614, jersey) (n03627232, knot) (n03724870, mask) (n03743016, megalith) (n03787032, mortarboard) (n03792972, mountain tent) (n03803284, muzzle) (n03825788, nipple) (n03891251, park bench) (n03935335, piggy bank) (n03938244, pillow) (n03961711, plate rack) (n04023962, punching bag) (n04136333, sarong) (n04235860, sleeping bag) (n04317175, stethoscope) (n04371430, swimming trunks) (n04399382, teddy) (n04447861, toilet seat) (n04597155, umbrella) (n04592741, wing) (n06874185, traffic light) (n07753592, banana) (n09229709, bubble) (n10148035, groom) (n11879895, rapeseed) (n12057211, yellow lady's slipper)