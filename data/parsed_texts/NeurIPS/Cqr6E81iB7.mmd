# The Limits of Differential Privacy in Online Learning

 Bo Li Wei Wang Peng Ye

Department of Computer Science and Engineering

The Hong Kong University of Science and Technology

Hong Kong SAR, China

bli@ust.hk, weiwa@cse.ust.hk, pyeac@connect.ust.hk

###### Abstract

Differential privacy (DP) is a formal notion that restricts the privacy leakage of an algorithm when running on sensitive data, in which privacy-utility trade-off is one of the central problems in private data analysis. In this work, we investigate the fundamental limits of differential privacy in online learning algorithms and present evidence that separates three types of constraints: no DP, pure DP, and approximate DP. We first describe a hypothesis class that is online learnable under approximate DP but not online learnable under pure DP under the adaptive adversarial setting. This indicates that approximate DP must be adopted when dealing with adaptive adversaries. We then prove that any private online learner must make an infinite number of mistakes for almost all hypothesis classes. This essentially generalizes previous results and shows a strong separation between private and non-private settings since a finite mistake bound is always attainable (as long as the class is online learnable) when there is no privacy requirement.

## 1 Introduction

Machine learning has demonstrated extraordinary capabilities in various industries, from healthcare to finance. Yet, it could raise serious privacy concerns as it may require access to a vast amount of personal data. The data used to train machine learning models may also contain sensitive information such as medical records or financial transactions. Therefore, it is crucial to ensure that the private data is well-protected during the training process.

Differential privacy (DP) [24, 23] is a rigorous mathematical definition that quantifies the level of personal data leakage. In a nutshell, an algorithm is said to be _differentially private_ if the change of any individual's data won't make the output significantly different. DP has become the standard notion of privacy and has been broadly employed [1, 6, 2].

However, privacy is not a free lunch and usually comes at a cost. Simple tasks may become much harder or even intractable when privacy constraints are imposed. It is crucial to understand the cost to pay for privacy. For probably approximately correct (PAC) learning [44, 11, 43], which is the standard theoretical model of machine learning, there have been many works investigating the cost associated with privacy, which demonstrate a huge discrepancy in terms of the cost under non-private, pure private, and approximate private constraints.

One central requirement in PAC learning is that the data need to be i.i.d. generated and given in advance. Such assumptions fail to capture many scenarios in practice. For example, fraud detection in financial transactions often needs to be handled in real time, which prohibits access to the entire dataset. Moreover, fraudulent patterns can change over time, and new types of fraud can emerge. In such a scenario, the data are clearly not i.i.d. and can even be adaptive to the algorithm's prior predictions, in which the online learning model should be adopted.

Compared with private PAC learning, the limits of private online learning are less understood. For approximate DP, algorithms for Littlestone classes were proposed [30]. But for pure DP, the only known result is the one for point functions given by Dmitriev et al. [22]. They also suggested that one could leverage existing tools of DP continual observation [26; 34] to design algorithms for finite hypothesis classes and asked if generic learners can be constructed for infinite hypothesis classes.

Going beyond qualitative learnability, it is worth quantitatively exploring the number of mistakes made by an online learner. Without privacy, it is possible to achieve a mistake bound of at most the Littlestone dimension of the hypothesis class [39], which is independent of the total rounds \(T\). Therefore, the number of mistakes is always bounded as \(T\to\infty\). However, all existing private online learning algorithms suffer from an error count that grows at least logarithmically with \(T\). It was asked by Sanyal and Ramponi [41] whether such a cost is inevitable for DP online learning. In a recent work of Cohen et al. [21], they showed that any private online learning algorithm for the class of point functions over \([T]\) must incur \(\Omega(\log T)\) mistakes. However, it remains open whether such cost is unavoidable for generic hypothesis classes, especially for those with a smaller cardinality.

### Main Results

We obtain results that separate three types of constraints: no DP, pure DP, and approximate DP.

Separation between pure and approximate DP.We first perform a systematic study of online learning under pure DP. We prove that every pure privately PAC learnable class is also pure privately online learnable against oblivious adversaries, answering a question raised by Dmitriev et al. [22]. For the stronger adaptive adversaries, we obtain an impossibility result that the class of point functions over \(\mathbb{N}\), which can be pure privately learned in the offline model, is not online learnable under pure DP. According to the result of Golowich and Livni [30], it is online learnable under approximate DP. Thus, our conclusion reveals a strong separation between these two privacy definitions.

Separation between private and non-private settings.We next quantitatively investigate the dependence on \(T\) in the mistake bound. We show that for any hypothesis class \(\mathcal{H}\), any private online learning algorithm must make \(\Omega(\log T)\) mistakes unless \(\mathcal{H}\) contains only one single hypothesis or exactly two complementary hypotheses (see Section 4 for the definition). This largely generalizes previous results and indicates that such a separation indeed exists universally. We further improve the lower bound to \(\Omega(\mathsf{LD}(\mathcal{H})\log T)\), where \(\mathsf{LD}(\mathcal{H})\) represents the Littlestone dimension of \(\mathcal{H}\).

To better demonstrate our results, we consider the task of online learning point functions over \(\mathbb{N}\) in the oblivious setting and summarize in Table 1 the finiteness of mistakes and learnability under the three types of constraints. Note that for this hypothesis class, the impossibility of making finite mistakes in private online learning can also be derived from the result in [21]. However, our conclusion (Theorem 4.3) is more general - it applies to a much broader family of hypothesis classes. We choose this hypothesis class for illustration because it separates the learnability against adaptive adversaries under pure DP and approximate DP.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{2}{c}{Oblivious adversary} & \multicolumn{2}{c}{Adaptive adversary} \\ \cline{2-4}  & Finite mistakes? & Learnable? & Learnable? \\ \hline \multirow{2}{*}{No DP constraints} & ✓ & ✓ & ✓ \\  & ([39]) & ([39]) & ([39]) \\ \hline \multirow{2}{*}{Approximate DP} & ✗ & ✓ & ✓ \\  & (Theorem 4.3) & ([30]) & ([30]) \\ \hline \multirow{2}{*}{Pure DP} & ✗ & ✓ & ✗ \\  & (Theorem 4.3) & (Theorem 3.3) & (Theorem 3.5) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Separation between three types of constraints

### Related Work

The work of [38] initialized the study of PAC learning with differential privacy. A series of subsequent works then showed that privacy constraints have a distinctive impact on the learners. The most remarkable result is the equivalence between approximate private learning and (non-private) online learning [4; 15; 5; 29]. For pure DP, the learnability is characterized by the so-called representation dimension [10]. Both results suggest that private learning is strictly harder than non-private learning. For some specific hypothesis class such as the one-dimensional threshold over finite domain, it was shown that learning with approximate DP enjoys a much lower sample complexity than with pure DP [9; 19; 28; 13; 14; 37; 20], separating these two types of privacy. Another separation result is the huge gap between properly and improperly learning point functions with pure DP [9], which does not exist in non-private and approximate private settings.

The problem of private online learning Littlestone classes was first studied by Golowich and Livni [30]. They proposed private online learning algorithms for hypothesis classes of finite Littlestone dimension in the realizable setting against oblivious and adaptive adversaries, further strengthening the connection between online learning and differential privacy. In contrast with the non-private setting where the mistake bound is always finite, their algorithms exhibit a cost of \(\log T\) for data streams of length \(T\). Recently, it was shown by Cohen et al. [21] that this extra cost is unavoidable for point functions over \([T]\). Dmitriev et al. [22] also obtained similar results, but only for algorithms that satisfy certain properties. It was questioned by Sanyal and Ramponi [41] whether an unbounded number of mistakes is necessary for \(\varepsilon\approx\sqrt{T}\) (see Section 2.2 for the definition of \(\varepsilon\)).

There were also a great number of results on private parametric online learning tasks such as online predictions from experts (OPE) and online convex optimization (OCO) [36; 42; 35; 3; 8]. Most of them focus on the agnostic setting. Asi et al. [7] developed algorithms for both problems in the realizable regime with oblivious adversaries, again with a \(\log T\) overhead. Asi et al. [8] obtained some hardness results for DP-OPE against adaptive adversaries, but they require the number of experts to be larger than \(T\).

Another related field is differential privacy under continual observation (see, e.g., [26; 18; 34]). While the techniques can be used to design online learning algorithms, it is unclear whether lower bounds for DP continual observation can be transformed to any hardness results for private online learning (see [22] for a detailed discussion).

## 2 Preliminaries

Notation.Throughout this paper, we use \(S=\{z_{1},\ldots,z_{t}\}\) to denote a data stream of length \(T\). We write \(S[t]\) to denote the data point comes at time-step \(t\), i.e., \(z_{t}\). For an algorithm \(\mathcal{A}\) that runs on \(S\), we use \(\mathcal{A}(S)_{t}\) to denote the output of \(\mathcal{A}\) at time-step \(t\).

### Online Learning

We start by defining online learning as a sequential game played between a learner and an adversary. Let \(\mathcal{H}\subseteq\big{\{}0,1\big{\}}^{\mathcal{X}}\) be a hypothesis class over domain \(\mathcal{X}\) and \(T\) be a positive integer indicating the total number of rounds. In the \(t\)-th round, the learner outputs a hypothesis \(h_{t}\in\{0,1\}^{\mathcal{X}}\) (not required to be in \(\mathcal{H}\)) while the adversary presents a pair \((x_{t},y_{t})\). The performance of the learner is measured by the expected _regret_, which is the expected number of additive mistakes made by the learner compared to the best (in hindsight) hypothesis in \(\mathcal{H}\):

\[\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{I}[h_{t}(x_{t})\neq y_{t}]-\min_{h^{ *}\in\mathcal{H}}\sum_{t=1}^{T}\mathbb{I}[h^{*}(x_{t})\neq y_{t}]\right].\]

The above setting is usually referred to as the _agnostic_ setting, where we do not make any assumptions on the data. In the _realizable_ setting, it is guaranteed that there is some \(h^{*}\in\mathcal{H}\) so that \(y_{t}=h^{*}(x_{t})\) for all \(t\in[T]\). In this setting, the performance is directly measured by the expected number of mistakes made by the learner, which is called the _mistake bound_, defined as

\[\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{I}[h_{t}(x_{t})\neq y_{t}]\right].\]An online learning algorithm is considered successful if it attains a sublinear regret, i.e., the regret is \(o(T)\). In this paper, we mainly focus on the realizable setting. We say a hypothesis class \(\mathcal{H}\) is online learnable if there is an online learning algorithm for \(\mathcal{H}\) that makes \(o(T)\) mistakes in expectation.

We consider two types of adversaries: an _oblivious_ adversary chooses the examples in advance (could depend on the learner's strategy, but not on its internal randomness), and \((x_{t},y_{t})\) is revealed to the learner in the \(t\)-th round. An _adaptive_ adversary instead, can choose \((x_{t},y_{t})\) based on past history, i.e., \(h_{1},\ldots,h_{t-1}\) and \((x_{1},y_{1}),\ldots,(x_{t-1},y_{t-1})\).

Without privacy, the mistake bound is exactly characterized by the Littlestone dimension [39] even with stronger adversaries that can choose \((x_{t},y_{t})\) after seeing \(h_{t}\). To define the Littlestone dimension, we first introduce the notion of a shattered tree.

**Definition 2.1** (Shattered Tree).: Consider a full binary tree of depth \(d\) such that each node is labeled by some \(x\in\mathcal{X}\). Every \(\{y_{1},\ldots,y_{d}\}\in\{0,1\}^{d}\) defines a root-to-leaf path \(x_{1},\ldots,x_{d}\) obtained by starting at the root, then for each \(i=2,\ldots,d\) choosing \(x_{i}\) to be the left child of \(x_{i-1}\) if \(y_{i-1}=0\) and to be the right child otherwise. The tree is said to be shattered by \(\mathcal{H}\) if for every root-to-leaf path defined in this way, there exists \(h\in\mathcal{H}\) such that \(y_{i}=h(x_{i})\) for all \(i\in[d]\).

**Definition 2.2** (Littlestone Dimension).: The Littlestone dimension of \(\mathcal{H}\), denoted by \(\mathtt{LD}(\mathcal{H})\), is the maximal \(d\) such that there exists a full binary tree of depth \(d\) that is shattered by \(\mathcal{H}\).

The problem of online prediction from experts can be viewed as a parametric version of online learning. Let \(d\) be the total number of experts. In the \(t\)-th round, the algorithm chooses an expert \(i_{t}\in[d]\) while the adversary selects a loss function \(\ell_{t}:[d]\rightarrow[0,1]\). Then \(\ell_{t}\) is revealed and a cost of \(\ell_{t}(i_{t})\) is incurred. The goal is to minimize the expected regret

\[\mathbb{E}\left[\sum_{t=1}^{T}\ell_{t}(i_{t})-\min_{i\in[d]}\sum_{t=1}^{T} \ell_{t}(i)\right].\]

Similar to online learning, an oblivious adversary chooses all \(\ell_{t}\) in advance, while an adaptive adversary determines \(\ell_{t}\) based on \(i_{1},\ldots,i_{t-1}\) and \(\ell_{1},\ldots,\ell_{t-1}\). In the realizable setting, it is guaranteed that there exists \(i^{\star}\in[d]\) such that \(\ell_{t}(i^{\star})=0\) for all \(t\in[T]\).

### Differential Privacy

We first recall the standard definition of differential privacy.

**Definition 2.3** (Differential Privacy).: An algorithm \(\mathcal{A}\) is said to be \((\varepsilon,\delta)\)-differentially private if for any two sequences \(S_{1}\) and \(S_{2}\) that differ in only one entry and any event \(O\), we have

\[\Pr[\mathcal{A}(S_{1})\in O]\leq e^{\varepsilon}\Pr[\mathcal{A}(S_{2})\in O]+\delta.\]

When \(\delta=0\), we also say \(\mathcal{A}\) is \(\varepsilon\)-differentially private.

Our proofs use the packing argument [32, 9], which heavily relies on the following property of DP.

**Fact 2.4** (Group Privacy).: _Let \(\mathcal{A}\) be an \((\varepsilon,\delta)\)-differentially private algorithm. Then for any two sequences \(S_{1}\) and \(S_{2}\) that differ in \(k\) entries and any event \(O\), we have_

\[\Pr[\mathcal{A}(S_{1})\in O]\leq e^{k\varepsilon}\Pr[\mathcal{A}(S_{2})\in O ]+\frac{e^{k\varepsilon}-1}{e^{\varepsilon}-1}\cdot\delta.\]

Privacy with _adaptive_ adversaries.When interacting with adaptive adversaries, the notion of differential privacy becomes a bit trickier.1 We adopt the definition of adaptive differential privacy from [30, 34, 8]. Let \(\mathcal{A}\) be an online algorithm, \(\mathsf{Adv}\) be an adversary2 who generates two sequences \(S_{1}\) and \(S_{2}\) adaptively such that \(S_{1}\) and \(S_{2}\) differ in only one entry, and \(b\in\{1,2\}\) be a global parameter that is unknown to \(\mathcal{A}\) and \(\mathsf{Adv}\). The interactive process \(\mathcal{A}\circ\mathsf{Adv}(b)\) works as follows: in each time-step \(t\), \(\mathsf{Adv}\) generates two data points \(S_{1}[t],S_{2}[t]\) based on the past history and \(\mathcal{A}\) gets \(S_{b}[t]\). The output of \(\mathcal{A}\circ\mathsf{Adv}(b)\) is defined to be the entire output of \(\mathcal{A}\). We say \(\mathcal{A}\) satisfies \((\varepsilon,\delta)\)-adaptive differential privacy if for any such adversary \(\mathsf{Adv}\) and any event \(O\), we have

Footnote 1: It turns out that the standard differential privacy and adaptive differential privacy are equivalent when \(\delta=0\)[26, 42]. But we will use the adaptive version in our proof since it is easier to work with.

Footnote 2: Note that the adversary here is different from the one in the definition of online learning.

\[\Pr[\mathcal{A}\circ\mathsf{Adv}(1)\in O]\leq e^{\varepsilon}\Pr[\mathcal{A} \circ\mathsf{Adv}(2)\in O]+\delta.\]Choosing the privacy parameters.It is a commonly agreed principle that for the definition of differential privacy to be meaningful, the parameter \(\delta\) should be much less than the inverse of the dataset size [27]. In this paper, when we say an algorithm \(\mathcal{A}\) is private without specifying the privacy parameters, we typically refer to the set-up that \(\varepsilon\) is a small constant (say \(0.01\)) and \(\delta=o(1/T)\).

## 3 Learning with Pure Differential Privacy

In this section, we study online learning under pure DP constraint. We first propose algorithms for privately offline learnable hypothesis classes against oblivious adversaries via a reduction to OPE using the tool of probabilistic representation. Then we turn to adaptive adversaries and present a hypothesis class that is privately offline learnable but not privately online learnable. Note that according to the results of [30], this class is online learnable under approximate DP with adaptive adversaries. Hence, we manifest a strong separation between pure and approximate DP.

### Learning Against Oblivious Adversaries

In this section, we consider an oblivious adversary. We first recall the notion of representation dimension, which was introduced by Beimel et al. [10] to characterize pure DP offline learnability. Let \(\mathcal{D}\) be a distribution over \(\mathcal{X}\times\{0,1\}\) and \(h\in\{0,1\}^{\mathcal{X}}\) be a hypothesis. The error of \(h\) with respect to \(\mathcal{D}\) is defined as \(\operatorname{error}_{\mathcal{D}}(h)=\Pr_{(x,y)\sim\mathcal{D}}[h(x)\neq y]\).

**Definition 3.1** (Representation Dimension).: A probability distribution \(\mathcal{P}\) over hypothesis classes is said to be an \((\alpha,\beta)\)-probabilistic representation for \(\mathcal{H}\) if for any \(h^{\star}\in\mathcal{H}\) and any distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\) that is labeled by \(h^{\star}\), we have

\[\Pr_{V\sim\mathcal{P}}[\exists v\in V\ s.t.\ \operatorname{error}_{ \mathcal{D}}(v)\leq\alpha]\geq 1-\beta.\]

Let \(\operatorname{size}(\mathcal{P})=\max_{V\in\operatorname{supp}(\mathcal{P})} \ln|V|\). The representation dimension of \(\mathcal{H}\), denoted by \(\texttt{RepDim}(\mathcal{H})\), is defined as

\[\texttt{RepDim}(\mathcal{H})=\min_{\mathcal{P}\ \text{is a}\ (1/4,1/4)\text{- probabilistic representation for}\ \mathcal{H}}\operatorname{size}(\mathcal{P}).\]

The following lemma from [10] shows that a constant probabilistic representation can be boosted to an \((\alpha,\beta)\) one with logarithmic cost in \(1/\alpha\) and \(1/\beta\).

**Lemma 3.2**.: _There exists an \((\alpha,\beta)\)-probabilistic representation for \(\mathcal{H}\) with_

\[\operatorname{size}(\mathcal{P})=O(\log(1/\alpha)\cdot(\texttt{RepDim}( \mathcal{H})+\log\log\log(1/\alpha)+\log\log(1/\beta))).\]

We first consider the realizable setting. Let \(S=\{(x_{1},y_{1}),\ldots,(x_{T},y_{T})\}\) be the sequence chosen by the adversary and \(\mathcal{D}_{S}\) be the empirical distribution of \(S\) (i.e., \(\Pr_{(x,y)\sim\mathcal{D}_{S}}[(x,y)=(x_{t},y_{t})]=1/T\) for all \(t\in[T]\)). By sampling a hypothesis class \(V\) from an \((\alpha,\beta)\)-probabilistic representation with \(1/\alpha<1/T\), we know that it holds with probability at least \(1-\beta\) that \(\operatorname{error}_{\mathcal{D}_{S}}(v)\leq 1/\alpha<1/T\) for some \(v\in V\). This further implies that \(v\) is consistent with all examples in \(S\). By Lemma 3.2, \(V\) is finite as long as \(\mathcal{H}\) has a finite representation dimension. Thus, it suffices to run the DP-OPE algorithm in [7] with every \(v\in V\) as an expert.

**Theorem 3.3**.: _Let \(\mathcal{H}\) be a hypothesis class with \(\texttt{RepDim}(\mathcal{H})<\infty\). In the realizable setting, there exists an online learning algorithm that is \(\varepsilon\)-differentially private and has an expected mistake bound of \(O\left(\frac{\log^{2}T(\texttt{RepDim}(\mathcal{H})+\log\log T)^{2}}{ \varepsilon}\right)\) with an oblivious adversary._

The above conclusion directly extends to the agnostic setting by replacing the DP-OPE algorithm with an agnostic one [8].

**Theorem 3.4**.: _Let \(\mathcal{H}\) be a hypothesis class with \(\texttt{RepDim}(\mathcal{H})<\infty\). In the agnostic setting, there exists an online learning algorithm that is \(\varepsilon\)-differentially private and achieves an expected regret of \(O\left(\frac{\sqrt{T}\log T(\texttt{RepDim}(\mathcal{H})+\log\log T)}{ \varepsilon}\right)\) with an oblivious adversary._

Note that every online learning algorithm can be transformed to a PAC learner by the online-to-batch conversion [17]. Our result reveals that pure private online learnability against oblivious adversaries is equivalent to pure private PAC learnability in both realizable and agnostic settings.

### Learning Against Adaptive Adversaries

We now turn to adaptive adversaries. For finite hypothesis classes, it is still feasible to employ techniques from DP-OPE [3] or DP continual observation [26, 18, 34] to devise online learning algorithms (in Appendix F, we give an algorithm with a better mistake bound in the realizable setting). One may hope that this can be extended to hypothesis class with finite representation dimension, as we did in the oblivious setting. However, it turns out that our method for oblivious adversaries is not applicable here. Since the examples are not fixed in advance, we cannot guarantee that the sampled hypothesis class \(V\) contains a consistent hypothesis. Moreover, the famous oblivious-to-adaptive transformation (see, e.g., [16]), which was used by Golowich and Livni [30] to construct online learners under approximate DP, also fails to give a sublinear mistake bound. This is because pure DP only has the basic composition property, which yields a mistake bound that scales linearly with \(T\) (for approximate DP, this can be improved to \(\sqrt{T}\) by advanced composition). Therefore, it is not clear if every offline learnable hypothesis class can also be made online learnable against adaptive adversaries under pure DP.

We will show that this is an impossible mission. Let \(\mathtt{POINT}_{d}\) be the set of point functions over \([d]\) and \(\mathtt{POINT}_{\mathbb{N}}\) be the set of point functions over \(\mathbb{N}\), where a point function \(f_{x}:\mathcal{X}\to\{0,1\}\) is a function that maps \(x\) to \(1\) and all other elements to \(0\). Both \(\mathtt{POINT}_{d}\) and \(\mathtt{POINT}_{\mathbb{N}}\) have a constant representation dimension and thus are offline learnable under pure DP [10]. In the rest of this section, we will prove that for any pure DP online learning algorithm for \(\mathtt{POINT}_{d}\), an adaptive adversary can force it to make \(\Omega(\min(\log d,T))\) errors. As a direct corollary, \(\mathtt{POINT}_{\mathbb{N}}\) is not pure privately online learnable against adaptive adversaries.

We now illustrate the idea of our proof. Let us start by considering a simplified version, where the algorithm is constrained to be proper, i.e., \(h_{t}\in\mathcal{H}=\mathtt{POINT}_{d}\) for every \(t\in[T]\). Then one can construct a series of data streams \(S_{i}=\{(i,1),\ldots,(i,1)\}\) for every \(i\in[d]\). An accurate proper learner must output \(f_{i}\) for most of the rounds. This allows us to use the packing argument to derive an \(\Omega(\log d)\) lower bound for \(T=\Theta(\log d)\).

However, the above argument does not apply to the general case where the learner may be improper since a learner can simply output an all-one function that makes \(0\) errors on each \(S_{i}\). Therefore, we have to insert to \(S_{i}\) some examples of the form \((j,0)\) where \(j\neq i\). This prevents \(h_{t}\) from taking \(1\) on elements other than \(i\). But when should we insert \((j,0)\)? And how do we determine the value of \(j\)?

Note that till now, we have not used the adversary's adaptivity. It is necessary to exploit this power since any oblivious construction can be solved by our algorithm in Theorem 3.3. When the adversary acts adaptively, the construction becomes a dual online learning game: in each round, the learner outputs \(h_{t}\) as a "data point" and the adversary chooses \((i,1)\) or some \((j,0)\) as the "hypothesis". This inspires us to leverage tools from online learning to construct the adversary.

We now sketch our idea. In each round, we choose \((i,1)\) as the data point with probability \(1/2\), and otherwise sample a \((j,0)\) from some probability distribution. We maintain the distribution by the multiplicative update rule, which is a widely used method in online decision making. The weight of \(j\) is increased by a multiplicative factor whenever \(h_{t}(j)=1\), and the probability of selecting \(j\) is proportional to its weight. We provide a detailed implementation in Algorithm 1.

Using the standard argument of multiplicative update, we can show that an accurate learner must predict \(h_{t}(i)=1\) for most rounds and \(h_{t}(j)=1\) for very few rounds. This allows us to apply the packing argument to obtain the following hardness result.

**Theorem 3.5**.: _Let \(\varepsilon\leq O(1)\) and \(d\geq 2\). Any \(\varepsilon\)-differentially private online learning algorithm for \(\mathtt{POINT}_{d}\) must incur a mistake bound of \(\Omega(\min(\log d/\varepsilon,T))\) in the adaptive adversarial setting._

Since \(\mathtt{POINT}_{d}\) is a subset of \(\mathtt{POINT}_{\mathbb{N}}\) for any \(d\), the above result directly implies that \(\mathtt{POINT}_{\mathbb{N}}\) is not online learnable with adaptive adversaries under pure DP. This shows a strong separation between pure DP and approximate DP.

**Corollary 3.6**.: _Let \(\varepsilon\leq O(1)\). In the adaptive adversarial setting, any \(\varepsilon\)-differentially private online learning algorithm for \(\mathtt{POINT}_{\mathbb{N}}\) must make \(\Omega(T)\) mistakes._``` Input: the number of rounds \(T\); online learning algorithm \(\mathcal{A}\); input data stream \(S\) Output: hypotheses \(h_{1},\ldots,h_{T}\) outputted by \(\mathcal{A}\)
1\(w_{0}(j)\gets 1\) for all \(j\in[d]\)
2for\(t=1,\ldots,T\)do
3\((x_{t},y_{t})\gets S[t]\)
4 Set \(p(j)\leftarrow\frac{w_{t-1}(j)}{\sum_{i\in[d]\setminus(x_{t})}w_{t-1}(k)}\) for \(j\in[d]\setminus\{x_{t}\}\)
5 With probability 1/2, sample \(j\sim p\) and set \((x_{t},y_{t})\leftarrow(j,0)\)
6 Present \((x_{t},y_{t})\) to \(\mathcal{A}\) and receive \(h_{t}\) from \(\mathcal{A}\)
7 Update \(w_{t}(j)=w_{t-1}(j)\cdot e^{h_{t}(j)}\) for all \(j\in[d]\)
8
9 end for return\(h_{1},\ldots,h_{T}\) ```

**Algorithm 1**Adaptive adversary for POINT\({}_{d}\)

## 4 A General Lower Bound on the Number of Mistakes

In this section, we prove an \(\Omega(\mathsf{LD}(\mathcal{H})\log T)\) lower bound on the number of mistakes made by any private learner for every hypothesis class \(\mathcal{H}\) that contains a pair of non-complementary hypotheses.3 This implies that as \(T\to\infty\), any private algorithm will make an infinite number of mistakes. Note that without privacy, the Standard Optimal Algorithm always makes at most \(\mathsf{LD}(\mathcal{H})\) mistakes [39]. Thus, our lower bound reveals a universal separation between non-private and private models.

Footnote 3: Such type of classes is equivalent to the notion of non-trivial classes in learning with data poisoning. See, e.g., [12] and [31].

Our proof proceeds in two steps. We first show an \(\Omega(\log T)\) lower bound in Section 4.1. Then based on this result, we prove the \(\Omega(\mathsf{LD}(\mathcal{H})\log T)\) lower bound in Section 4.2.

### A Lower Bound for Non-complementary Hypotheses

We first define the notion of complementary hypotheses.

**Definition 4.1**.: We say two different hypotheses \(f_{1}\) and \(f_{2}\) over \(\mathcal{X}\) are complementary if \(f_{1}(x)=1-f_{2}(x)\) for all \(x\in\mathcal{X}\). Otherwise we say they are non-complementary.

It is worth noticing the following important fact about non-complementary hypotheses, where the first item directly comes from the above definition and the second is because \(f_{1}\) and \(f_{2}\) are different.

**Fact 4.2**.: _Let \(f_{1}\) and \(f_{2}\) be two different hypotheses over \(\mathcal{X}\) that are non-complementary. Then:_

1. _There exists some_ \(u_{0}\in\mathcal{X}\) _such that_ \(f_{1}(u_{0})=f_{2}(u_{0})\)_;_
2. _There exists some_ \(u_{1}\in\mathcal{X}\) _such that_ \(f_{1}(u_{1})\neq f_{2}(u_{1})\)_._

We remark that this fact is also used by Dmitriev et al. [22] to prove a lower bound (in their work, they call it a "distinguishing tuple"). However, they make a strong assumption that when running on a data stream containing \((u_{0},f_{1}(u_{0}))\) only, with high probability, the algorithm predicts \(h_{t}(u_{1})=f_{1}(u_{1})\) simultaneously for all \(t\in[T]\). This largely weakens their bound since most DP algorithms clearly do not have such property.

To see how to use Fact 4.2, consider a hypothesis class that contains a pair of non-complementary hypotheses. We will focus on \(u_{0},u_{1}\) and \(f_{1},f_{2}\) only and ignore all other elements and hypotheses. In our proof, we will use \((u_{0},f_{1}(u_{0}))=(u_{0},f_{2}(u_{0}))\) as a dummy input that provides no information about which hypothesis is correct. Let \(S_{0}\) be a sequence that contains the dummy input only and \(\mathcal{A}\) be an online learning algorithm. Without loss of generality, we can assume that \(\Pr[\mathcal{A}(S_{0})_{t}(u_{1})=f_{1}(u_{1})]\geq 1/2\) for all \(t\in[T]\) (we can make this hold for half of the rounds by swapping \(f_{1}\) and \(f_{2}\), and ignore the rounds that it does not hold). We will insert \((u_{1},f_{2}(u_{1}))\)'s to make algorithm error.

Our proof relies on the classical packing argument. For ease of presentation, we only consider pure DP here, but the proof strategy easily extends to approximate DP via group privacy under approximate DP. In the framework of packing argument, we will construct a series of input sequences \(S_{1},\ldots,S_{m}\)from \(S_{0}\) and disjoint subsets of output \(O_{1},\ldots,O_{m}\) such that \(S_{0}\) and \(S_{i}\) differ by at most \(k\) elements for every \(i\in[m]\), and any algorithm will make \(\Omega(k)\) mistakes on \(S_{i}\). Then by group privacy, for any \(\varepsilon\)-differentially private algorithm \(\mathsf{Alg}\) we have

\[1\geq\sum_{i=1}^{m}\Pr[\mathsf{Alg}(S_{0})\in O_{i}]\geq e^{-k\varepsilon} \sum_{i=1}^{m}\Pr[\mathsf{Alg}(S_{i})\in O_{i}].\]

Thus, a lower bound on \(\Pr[\mathsf{Alg}(S_{i})\in O_{i}]\) implies a lower bound on \(k\) by the above inequality.

The first challenge here is the construction of \(S_{i}\). By our assumption, we can insert a \((u_{1},f_{2}(u_{1}))\) at any position of \(S_{0}\) to cause a loss of \(1/2\). However, when inserting the second one, the loss may decrease by a multiplicative factor of \(e^{\varepsilon}\). Following this argument, no matter how many \((u_{1},f_{2}(u_{1}))\)'s are inserted, we can only bound the expected number of mistakes by

\[\frac{1}{2}\left(1+e^{-\varepsilon}+e^{-2\varepsilon}+\cdots\right)=constant,\]

failing to give an \(\Omega(k)\) bound for \(k=\log T\).

We overcome this by constructing them according to the given algorithm \(\mathcal{A}\) instead of arbitrary algorithms. We will assume \(\mathcal{A}\) has a mistake bound of \(O(\log T)\) and seek to derive a contradiction. We now depict our construction. For \(S_{1}\), we let \(S=S_{0}\) be the initial data stream. We then go through every \(t\in[T]\) in an increasing order, insert a \((u_{1},f_{2}(u_{1}))\) at time-step \(t\) whenever \(\Pr[\mathcal{A}(S)_{t}(u_{1})=f_{1}(u_{1})]\geq 1/3\), and let \(S_{1}=S\) at the end. By our assumption, the number of \((u_{1},f_{2}(u_{1}))\)'s should not exceed \(k=3\cdot O(\log T)\). Hence, \(S_{1}\) and \(S_{0}\) differ by at most \(k=O(\log T)\) points. Moreover, by our construction, for each \(t\in[T]\) such that \(S_{1}[t]=(u_{0},f_{1}(u_{0}))\), we must have \(\Pr[\mathcal{A}(S_{1})_{t}(u_{1})=f_{1}(u_{1})]<1/3\).

Now let us construct \(S_{2}\). We find the earliest round \(t_{1}\) such that \(\Pr[\mathcal{A}(S_{1})_{t_{1}}(u_{1})=f_{1}(u_{1})]<1/3\). The property we mentioned above ensures the existence of such \(t_{1}\) as long as \(k<T\). We then perform a similar procedure as in the construction of \(S_{1}\), but instead of starting from \(t=1\) and going over the entire time span \([T]\), we start from \(t=t_{1}\). The online nature of \(\mathcal{A}\) allows us to use \(t_{1}\) to distinguish \(S_{1}\) and \(S_{2}\) (as well as \(S_{3},\ldots,S_{m}\), which we will construct later) since

\[\Pr[\mathcal{A}(S_{1})_{t_{1}}(u_{1})=f_{1}(u_{1})]<1/3<1/2\leq\Pr[\mathcal{A} (S_{2})_{t_{1}}(u_{1})=f_{1}(u_{1})].\]

In other words, \(\mathcal{A}\) is more likely to predict \(h_{t_{1}}(u_{1})=f_{1}(u_{1})\) on \(S_{2}\) but is less likely to do so on \(S_{1}\).

We repeat the construction for \(i=3,\ldots,m\). For each \(i\), we first identify the minimal \(t_{i-1}\) such that \(\Pr[\mathcal{A}(S_{j})_{t_{i-1}}]<1/3\) for every \(j<i\). Then we insert \((u_{1},f_{2}(u_{1}))\)'s starting from \(t=t_{i-1}\). By the same argument, \(t_{i-1}\) can be used to distinguish \(S_{1},\ldots,S_{i-1}\) and \(S_{i},\ldots,S_{m}\). We formally describe the construction procedure in Algorithm 2.

At the end, we will have \(m\) sequences \(S_{1},\ldots,S_{m}\) and \(m-1\) time-steps \(t_{1},\ldots,t_{m-1}\) such that \(\Pr[\mathcal{A}(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})]<1/3\) for any \(j\geq i\) and \(\Pr[\mathcal{A}(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})]\geq 1/2\) for any \(j<i\). It can be proved that \(m=\Omega(T/k)\), which is sufficiently large for \(k=O(\log T)\). Now we run \(\mathcal{A}\) on some \(S=S_{i}\). Suppose we can figure out the index \(i\), we can apply the packing argument to derive an \(\Omega(\log m)=\Omega(\log T)\) lower bound.

Here comes the second challenge. Though we can use the output of \(\mathcal{A}\) to estimate \(\Pr[\mathcal{A}(S)_{t_{j}}(u_{1})=f_{1}(u_{1})]\) for a given \(j\), we only have a constant success probability. To make the estimate accurate for every \(j\in[m-1]\), one has to achieve a success probability of \(1-1/m\) for each \(j\). This requires running \(\mathcal{A}\) for \(O(\log m)=O(\log T)\) times and taking the average, which is prohibited since the resulting algorithm would be \(O(\varepsilon\log T)\)-DP, yielding a meaningless \(\Omega(1)\) lower bound.

We address this issue by using binary search. We start with \(\{t_{1},\ldots,t_{m-1}\}\) and select the middle point \(t_{mid}\) in each iteration. By averaging over multiple copies of \(\mathcal{A}(S)\), we can figure out whether we should go left or go right. This can be done in \(O(\log m)=O(\log T)\) iterations, and we only require the decision made on each middle point to be correct. Thus, the number of independent copies can be reduced to \(O(\log\log T)\), which leads to a lower bound of \(\Omega(\log T/\log\log T)\).

The above approach is already sufficient to show an unbounded number of mistakes, but we can further refine our method to achieve an \(\Omega(\log T)\) bound. The key observation here is that we do not need the probability of outputting \(i\) on \(S_{i}\) to be a constant. In fact, a success probability of \(1/m^{1-\Omega(1)}\) is enough to obtain \(k\geq\Omega(\log(m/m^{1-\Omega(1)}))=\Omega(\log T)\).

We thus "smooth" our binary search. In each iteration, instead of going left or right deterministically, we go to the side that is more likely to be correct with some probability \(p>1/2\). We show that, by choosing \(p\) appropriately, this approach will output \(i\) on \(S_{i}\) with probability \(1/m^{1-\Omega(1)}\). Moreover, it only requires running the online learning algorithm \(O(1)\) times, avoiding the \(\log\log T\) blow-up of privacy parameters. The \(\Omega(\log T)\) lower bound then follows by applying the packing argument. We illustrate this approach in Algorithm 3.

``` Input: the number of rounds \(T\); online learning algorithm \(\mathcal{A}\); threshold \(k\); \(f_{1},f_{2}\) and \(u_{0},u_{1}\) Output: a single data stream \(S_{i}\), or a collection of \(m\) data streams \(S_{1},\ldots,S_{m}\) along with \(m-1\) time-steps \(t_{1},\ldots,t_{m-1}\)
1\(S_{0}\leftarrow\{(u_{0},f_{1}(u_{0})),\ldots,(u_{0},f_{1}(u_{0}))\}\)
2\(m\leftarrow\lceil T/k\rceil\)
3for\(i=1,\ldots,m\)do
4\(S_{i}\gets S_{0}\)
5 Find the smallest \(t_{i-1}\) such that \(\forall j\in[i-1],\Pr[\mathcal{A}(S_{j})_{t_{i-1}}(u_{1})=f_{1}(u_{1})]<1/3\)
6for\(t=t_{i-1},\ldots,T\)do
7if\(\Pr[\mathcal{A}(S_{i})_{t}(u_{1})=f_{1}(u_{1})]\geq 1/3\)then
8\(S_{i}[t]\leftarrow(u_{1},f_{2}(u_{1}))\)
9 end for
10
11 end for
12if\(|\{t\in[T]:S_{i}[t]=(u_{1},f_{2}(u_{1}))\}|>k\)then
13return\(S_{i}\)
14 end if
15
16 end for return\(S_{1},\ldots,S_{m}\) and \(t_{1},\ldots,t_{m-1}\) ```

**Algorithm 2**Constructing \(S_{0},S_{1},\ldots,S_{m}\) and \(t_{1},\ldots,t_{m-1}\)

**Theorem 4.3**.: _Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\log T/\varepsilon)\) even in the oblivious adversarial setting._

One may ask whether the existence of a non-complementary pair is a necessary condition for the number of mistakes to be unbounded. Note that there are only two cases that \(\mathcal{H}\) contains no non-complementary pairs: either \(|\mathcal{H}|=1\) or \(\mathcal{H}=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). The former is definitely online learnable with zero mistakes. For the latter one, we give an algorithm with a finite expected mistake bound in Appendix F, showing that the condition is indeed necessary and sufficient.

``` Input: the number of rounds \(T\); online learning algorithm \(\mathcal{A}\); time-steps \(t_{1},\ldots,t_{m-1}\); input data stream \(S\in\{S_{1},\ldots,S_{m}\}\); \(f_{1},f_{2}\) and \(u_{0},u_{1}\) used in Algorithm 2 Output: an index \(i\in[m]\)
1 Run \(\mathcal{A}\) on \(S\) for \(360\) times, obtain \(360\) copies of output \(\{h_{1}^{(w)},\ldots,h_{T}^{(w)}\}\) for \(w\in[360]\)
2\(l\gets 1\), \(r\gets m\)
3while\(l<r\)do
4\(mid\leftarrow\lfloor\frac{l+r}{2}\rfloor\)
5if\(|\{h_{mid}^{(w)}(u_{1})=f_{1}(u_{1}):w\in[360]\}|<150\)then
6 Let \(r\gets mid\) with probability \(3/4\), and \(l\gets mid+1\) otherwise
7else
8 Let \(l\gets mid+1\) with probability \(3/4\), and \(r\gets mid\) otherwise
9
10 end if
11return\(l\) ```

**Algorithm 3**Distinguishing \(S_{1},\ldots,S_{m}\)

**Theorem 4.4**.: _Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\log T/\varepsilon)\) even in the oblivious adversarial setting._

Proof.: Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\log T/\varepsilon)\) even in the oblivious adversarial setting.

One may ask whether the existence of a non-complementary pair is a necessary condition for the number of mistakes to be unbounded. Note that there are only two cases that \(\mathcal{H}\) contains no non-complementary pairs: either \(|\mathcal{H}|=1\) or \(\mathcal{H}=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). The former is definitely online learnable with zero mistakes. For the latter one, we give an algorithm with a finite expected mistake bound in Appendix F, showing that the condition is indeed necessary and sufficient.

**Theorem 4.5**.: _Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\log T/\varepsilon)\) even in the oblivious adversarial setting._

Proof.: Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\log T/\varepsilon)\) even in the oblivious adversarial setting.

One may ask whether the existence of a non-complementary pair is a necessary condition for the number of mistakes to be unbounded. Note that there are only two cases that \(\mathcal{H}\) contains no non-complementary pairs: either \(|\mathcal{H}|=1\) or \(\mathcal{H}=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). The former is definitely online learnable with zero mistakes. For the latter one, we give an algorithm with a finite expected mistake bound in Appendix F, showing that the condition is indeed necessary and sufficient.

**Theorem 4.6**.: _Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\log T/\varepsilon)\) even in the oblivious adversarial setting._

Proof.: Let \(c\in(0,1)\) be some constant. Suppose \(\varepsilon\geq\ln T/T^{1-c}\) and \(\delta\leq\varepsilon/T\).

### Incorporating the Littlestone Dimension

Building upon the \(\Omega(\log T)\) lower bound, we are now ready to show an \(\Omega(\mathsf{LD}(\mathcal{H})\log T)\) lower bound for general hypothesis classes. Let \(\mathcal{A}\) be a private online learning algorithm for \(\mathcal{H}\). Consider a shattered tree of depth \(\mathsf{LD}(\mathcal{H})\geq 2\). Let \(u_{0}\) denote its root and \(u_{1}\) be its left child. By the definition of shattered tree, there exists \(f_{1},f_{2}\in\mathcal{H}\) such that \(f_{1}(u_{0})=f_{2}(u_{0})=0\) and \(0=f_{1}(u_{1})\neq f_{2}(u_{1})=1\). Note that \(f_{1},f_{2}\) and \(u_{0},u_{1}\) satisfy the property mentioned in Fact 4.2. We can thus apply Theorem 4.3 to find a sequence \(S_{1}\) of length \(T^{\prime}\) on which \(\mathcal{A}\) makes \(\Omega(\log T^{\prime})\) mistakes.

Till now, only the true labels of \(u_{0}\) and \(u_{1}\) are revealed to the learner. Therefore, we can go into the corresponding subtree of \(u_{1}\) and reiterate the above operation. After repeating it \(\mathsf{LD}(\mathcal{H})/2\) times, we obtain a series of completely non-overlapping sequences \(S_{1},\ldots,S_{\mathsf{LD}(\mathcal{H})/2}\) and on any one of them \(\mathcal{A}\) makes \(\Omega(\log T^{\prime})\) mistakes. By concatenating them together and setting \(T^{\prime}=T/\mathsf{LD}(\mathcal{H})\), we arrive at the \(\Omega(\mathsf{LD}(\mathcal{H})\log T)\) lower bound assuming \(T>\mathsf{LD}(\mathcal{H})^{1+c}\).

**Theorem 4.4**.: _Let \(c_{1}\in(0,1)\) and \(c_{2}>0\) be two constants. Suppose \(\varepsilon\geq\ln T/T^{(1-c_{1})c_{2}/(1+c_{2})}\) and \(\delta\leq\varepsilon/T\). If \(\mathcal{H}\) is a hypothesis class that contains two non-complementary hypotheses, then any \((\varepsilon,\delta)\)-differentially private online learning algorithm for \(\mathcal{H}\) must incur a mistake bound of \(\Omega(\mathsf{LD}(\mathcal{H})\log T/\varepsilon)\) even in the oblivious adversarial setting given that \(T>\mathsf{LD}(\mathcal{H})^{1+c_{2}}\)._

Note that the class of all hypotheses over \([d]\) has a Littlestone dimension of \(\lfloor\log_{2}d\rfloor\). The above theorem directly implies the following lower bound for the OPE problem. This improves the lower bound in [7] by a \(\log T\) factor.

**Corollary 4.5**.: _Let \(c_{1}\in(0,1)\) and \(c_{2}>0\) be two constants. Suppose \(\varepsilon\geq\ln T/T^{(1-c_{1})c_{2}/(1+c_{2})}\) and \(\delta\leq\varepsilon/T\). In the realizable setting, any \((\varepsilon,\delta)\)-differentially private algorithm for OPE has a regret of \(\Omega(\log d\log T/\varepsilon)\) even against oblivious adversaries given that \(T>\lfloor\log_{2}d\rfloor^{1+c_{2}}\)._

### Comparing to the Upper Bounds

We have shown an \(\Omega_{\mathcal{H}}(\log T)\) lower bound on the number of mistakes made by any private online learning algorithm. We now compare it to existing upper bounds.

For pure DP, we provide an upper bound of \(O_{\mathcal{H}}(\log^{2}T\cdot(\log\log T)^{2})\). This is larger than our lower bound by a factor of \(\log T\cdot(\log\log T)^{2}\). In Appendix F, we show that \(O_{\mathcal{H}}(\log T)\) is achievable for some specific hypothesis classes. Whether \(O_{\mathcal{H}}(\log T)\) is attainable for generic hypothesis classes remains open.

For approximate DP, Golowich and Livni [30] proposed an algorithm with \(O_{\mathcal{H}}(\log T)\) mistakes against oblivious adversaries. Thus, our lower bound is tight assuming a constant Littlestone dimension. However, their algorithm exhibits an \(O_{\mathcal{H}}(\sqrt{T})\) upper bound against upper bound against adaptive adversaries. Whether this can also be improved to \(O_{\mathcal{H}}(\log T)\) is an interesting open question.

## Acknowledgments and Disclosure of Funding

The research was supported in part by an RGC RIF grant under the contract R6021-20, an RGC TRS grant under the contract T43-513/23N-2, RGC CRF grants under the contracts C7513, C7004-22G, C1029-22G and C6015-23G, and RGC GRF grants under the contracts 16200221, 16207922 and 16207423. The authors would like to thank the anonymous reviewers for their feedback and suggestions.

## References

* Abadi et al. [2016] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 23rd ACM Conference on Computer and Communications Security_, pages 308-318, 2016.
* Abowd [2018] John M Abowd. The U.S. Census Bureau adopts differential privacy. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, page 2867, 2018.

* [3] Naman Agarwal and Karan Singh. The price of differential privacy for online learning. In _Proceedings of the 34th International Conference on Machine Learning_, volume 70, pages 32-40, 2017.
* [4] Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite littlestone dimension. In _Proceedings of the 51st Annual ACM Symposium on Theory of Computing_, pages 852-860, 2019.
* [5] Noga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private and online learnability are equivalent. _Journal of the ACM_, 69(4):1-34, 2022.
* [6] Apple Differential Privacy Team. Learning with privacy at scale. _Apple Machine Learning Journal_, 2017.
* [7] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Near-optimal algorithms for private online optimization in the realizable regime. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 1107-1120, 2023.
* [8] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private online prediction from experts: Separations and faster rates. In _Proceedings of the 36th Annual Conference on Learning Theory_, volume 195, pages 674-699, 2023.
* [9] Amos Beimel, Hai Brenner, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. _Machine learning_, 94(3):401-437, 2014.
* [10] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of pure private learners. _Journal of Machine Learning Research_, 20:1-33, 2019.
* [11] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Learnability and the Vapnik-Chervonenkis dimension. _Journal of the ACM_, 36(4):929-965, 1989.
* [12] Nader H Bshouty, Nadav Eiron, and Eyal Kushilevitz. Pac learning with nasty noise. _Theoretical Computer Science_, 288(2):255-275, 2002.
* [13] Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. In _Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science_, pages 634-649, 2015.
* [14] Mark Bun, Cynthia Dwork, Guy N Rothblum, and Thomas Steinke. Composable and versatile privacy via truncated cdp. In _Proceedings of the 50th Annual ACM Symposium on Theory of Computing_, pages 74-86, 2018.
* [15] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In _Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science_, pages 389-402, 2020.
* [16] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge University Press, 2006.
* [17] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of online learning algorithms. _IEEE Transactions on Information Theory_, 50(9):2050-2057, 2004.
* [18] T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. _ACM Transactions on Information and System Security_, 14(3):1-24, 2011.
* [19] Kamalika Chaudhuri and Daniel Hsu. Sample complexity bounds for differentially private learning. In _Proceedings of the 24th Annual Conference on Learning Theory_, volume 19, pages 155-186, 2011.
* [20] Edith Cohen, Xin Lyu, Jelani Nelson, Tamas Sarlos, and Uri Stemmer. Optimal differentially private learning of thresholds and quasi-concave optimization. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 472-482, 2023.

* [21] Edith Cohen, Xin Lyu, Jelani Nelson, Tamas Sarlos, and Uri Stemmer. Lower bounds for differential privacy under continual observation and online threshold queries. In _Proceedings of the 37th Annual Conference on Learning Theory_, volume 247, pages 1200-1222, 2024.
* [22] Daniil Dmitriev, Kristof Szabo, and Amartya Sanyal. On the growth of mistakes in differentially private online learning: A lower bound perspective. In _Proceedings of the 37th Annual Conference on Learning Theory_, volume 247, pages 1379-1398, 2024.
* [23] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In _Proceedings of the 25th Annual International Conference on the Theory and Applications of Cryptographic Techniques_, pages 486-503, 2006.
* [24] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Proceedings of the 3rd Conference on Theory of Cryptography_, pages 265-284, 2006.
* [25] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In _Proceedings of the 41st Annual ACM Symposium on Theory of Computing_, pages 381-390, 2009.
* [26] Cynthia Dwork, Moni Naor, Tonianm Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In _Proceedings of the 42nd Annual ACM Symposium on Theory of Computing_, pages 715-724, 2010.
* [27] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [28] Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. In _Proceedings of the 27th Annual Conference on Learning Theory_, volume 35, pages 1000-1019, 2014.
* [29] Badih Ghazi, Noah Golowich, Ravi Kumar, and Pasin Manurangsi. Sample-efficient proper PAC learning with approximate differential privacy. In _Proceedings of the 53rd Annual ACM Symposium on Theory of Computing_, pages 183-196, 2021.
* [30] Noah Golowich and Roi Livni. Littlestone classes are privately online learnable. In _Advances in Neural Information Processing Systems_, volume 34, pages 11462-11473, 2021.
* [31] Steve Hanneke, Amin Karbasi, Mohammad Mahmoody, Idan Mehalel, and Shay Moran. On optimal learning under targeted data poisoning. In _Advances in Neural Information Processing Systems_, volume 35, pages 30770-30782, 2022.
* [32] Moritz Hardt and Kunal Talwar. On the geometry of differential privacy. In _Proceedings of the 42nd Annual ACM Symposium on Theory of Computing_, pages 705-714, 2010.
* [33] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. _Journal of the American Statistical Association_, 58(301):13-30, 1963.
* [34] Palak Jain, Sofya Raskhodnikova, Satchit Sivakumar, and Adam Smith. The price of differential privacy under continual observation. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202, pages 14654-14678, 2023.
* [35] Prateek Jain and Abhradeep Guha Thakurta. (Near) dimension independent risk bounds for differentially private learning. In _Proceedings of the 31st International Conference on Machine Learning_, volume 32, pages 476-484, 2014.
* [36] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta. Differentially private online learning. In _Proceedings of the 25th Annual Conference on Learning Theory_, volume 23, pages 24.1-24.34, 2012.
* [37] Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. In _Proceedings of the 33rd Annual Conference on Learning Theory_, volume 125, pages 2263-2285, 2020.

* [38] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? _SIAM Journal on Computing_, 40(3):793-826, 2011.
* [39] Nick Littelstone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. _Machine learning_, 2:285-318, 1988.
* [40] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In _Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science_, pages 94-103. IEEE Computer Society, 2007.
* [41] Amartya Sanyal and Giorgia Ramponi. Open problem: Do you pay for privacy in online learning? In _Proceedings of the 35th Annual Conference on Learning Theory_, volume 178, pages 5633-5637, 2022.
* [42] Adam Smith and Abhradeep Thakurta. (Nearly) optimal algorithms for private online learning in full-information and bandit settings. In _Advances in Neural Information Processing Systems_, volume 26, pages 2733-2741, 2013.
* [43] Leslie G Valiant. A theory of the learnable. _Communications of the ACM_, 27(11):1134-1142, 1984.
* [44] V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability and Its Applications_, 16(2):264-280, 1971.

Discussion

In this work, we investigate online learning with differential privacy and provide separation results that distinguish non-private, pure private, and approximate private constraints. Below, we discuss some limitations and future work.

Tighter dependence on \(T\) under pure DP.Our algorithm for pure private online learning with oblivious adversaries exhibits a \(\log^{2}T\cdot(\log\log T)^{2}\) dependence on \(T\) (Theorem D.1). We also provide algorithms for \(\mathtt{POINT}_{N}\) (Theorem F.1) and \(\mathtt{Threshold}_{d}\) (Theorem F.3) that have a \(O_{\mathcal{H}}(\log T)\) mistake bound. It is interesting to find out if a \(\log T\) dependence is achievable for generic hypothesis classes.

Broader range of privacy parameters.Our \(\Omega(\log T)\) lower bound requires \(\delta<1/T\) (Theorem 4.3) for constant \(\varepsilon\), while the result in [21] only needs \(\delta<1/\log T\). Although it is a commonly accepted criterion to select \(\delta=o(1/T)\), we still wonder whether our bound also holds for \(\delta<1/\log T\). Moreover, our results do not cover the cases where \(\varepsilon\) or \(T\) are extremely small. Is it possible to cover the entire range?

Mistake bound against stochastic adversaries.One benefit of online learning is that it does not require the data to be i.i.d. generated. But in some scenarios, we may still have i.i.d. data but have to make online predictions. Clearly, such _stochastic adversaries_ are weaker than oblivious ones. Our construction in Algorithm 2 does not apply to stochastic adversaries. Can we overcome the \(\Omega(\log T)\) barrier assuming stochastic adversaries?

Lower bound on learning with constant success probability.In Section 4.1, we show that the _expected_ number of mistakes incurred by any algorithm is \(\Omega(\log T)\). It is unclear whether the \(\Omega(\log T)\) cost remains inevitable or a mistake bound of \(o(\log T)\) can be achieved if we only require the learner to succeed with a _constant probability_ (e.g., \(0.99\)).

## Appendix B Broader Impacts

Privacy has become a fundamental concern in today's machine learning community. In this work, we show several lower bounds and upper bounds on private online learning tasks. While our contributions are theoretical in nature, and we do not see any direct societal implications, we hope that our results will reveal the intrinsic structures of the problems and provide insights for the development of practical online learning algorithms with better privacy-utility trade-offs.

## Appendix C Additional Preliminaries

**Theorem C.1** (Hoeffding's Inequality [33]).: _Let \(Z_{1},\ldots,Z_{n}\) be independent bounded random variables with \(Z_{i}\in[a,b]\). Then_

\[\Pr\left[\frac{1}{n}\sum_{i=1}^{n}\left(Z_{i}-\mathbb{E}[Z_{i}]\right)\geq t \right]\leq\exp\left(-\frac{2nt^{2}}{(b-a)^{2}}\right)\]

_for all \(t\geq 0\)._

The Laplace mechanism ensures privacy by adding Laplace noise.

**Definition C.2** (Sensitivity).: Let \(f:\mathcal{Z}^{n}\to\mathbb{R}\) be a function. The sensitivity of \(f\) is defined by

\[\Delta_{f}=\max_{S_{1}\text{ and }S_{2}\text{ differ in one entry}}|f(S_{1})-f(S_{2})|.\]

**Definition C.3** (Laplace Distribution).: The Laplace distribution with parameter \(b\) and mean \(0\), denoted by \(\operatorname{Lap}(b)\), is defined by the following probability density function:

\[f(x)=\frac{1}{2b}\exp(-|x|/b).\]

**Lemma C.4** (The Laplace Mechanism [24]).: _Let \(r\sim\mathrm{Lap}(\Delta_{f}/\varepsilon)\) be a Laplace random variable. The algorithm that outputs \(f(S)+r\) satisfies \(\varepsilon\)-differential privacy. Moreover, with probability \(1-\beta\) it holds that \(|r|\leq\ln(1/\beta)\Delta_{f}/\varepsilon\)._

We also need the following AboveThreshold algorithm (aka the sparse vector technique) [25].

``` Input: database \(S\); privacy parameter \(\varepsilon\); threshold \(L\); a series of online and adaptively chosen sensitivity-1 queries \(q_{1},\ldots\) Output: a stream of response \(a_{1},\ldots\)
1\(\hat{L}\gets L+\mathrm{Lap}(2/\varepsilon)\)
2for\(i=1,\ldots\), do
3\(\hat{q}_{i}\gets q_{i}(S)+\mathrm{Lap}(4/\varepsilon)\)
4if\(\hat{q}_{i}\geq\hat{L}\)then
5\(a_{i}\leftarrow\top\)
6halt
7else
8\(a_{i}\leftarrow\bot\)
9
10 end if
11
12 end for ```

**Algorithm 4**AboveThreshold

**Lemma C.5**.: _Algorithm 4 is \(\varepsilon\)-differently private._

If we only want to identify a query with a large value instead of the values of all queries, the report-noisy-max mechanism gives a much better utility guarantee. It can be implemented by adding Laplace noise or directly applying the exponential mechanism [40].

**Theorem C.6** (Report-Noisy-Max).: _Let \(S\) be a database and \(q_{1},\ldots,q_{d}\) be \(d\) sensitivity-1 queries. There exists an \(\varepsilon\)-differentially private algorithm that outputs an index \(i\) such that_

\[q_{i}(S)\geq\max_{j\in[d]}q_{j}(S)-\frac{2(\ln(d)+\ln(1/\beta))}{\varepsilon}\]

_with probability at least \(1-\beta\)._

The composition property allows us to combine multiple differentially private algorithms into one, even if they are executed adaptively.

**Lemma C.7** (Basic Composition [23, 27]).: _Let \(\mathcal{A}_{1}:\mathcal{Z}^{n}\to\mathcal{R}_{1}\) be an algorithm that satisfies \((\varepsilon_{1},\delta_{1})\)-DP, and for \(2\leq i\leq k\) let \(\mathcal{A}_{i}:\mathcal{R}_{1}\times\cdots\times\mathcal{R}_{i-1}\times \mathcal{Z}^{n}\to\mathcal{R}_{i}\) be an algorithm that satisfies \((\varepsilon_{i},\delta_{i})\)-DP for any given \((r_{1},\ldots,r_{i-1})\in\mathcal{R}_{1}\times\cdots\mathcal{R}_{i-1}\). Let \(\mathcal{A}\) be an algorithm that_

1. _Computes_ \(r_{1}\leftarrow\mathcal{A}_{1}(S)\)_;_
2. _For each_ \(i=2,\ldots,k\)_, computes_ \(r_{i}\leftarrow\mathcal{A}_{i}(r_{1},\ldots,r_{i-1},S)\)_;_
3. _Outputs_ \(r_{1},\ldots,r_{k}\)_._

_Then \(\mathcal{A}\) is \((\sum_{i=1}^{k}\varepsilon_{i},\sum_{i=1}^{k}\delta_{i})\)-DP._

## Appendix D Proofs from Section 3

### Proof of Theorem 3.3

We use the following DP-OPE algorithm from [7].

**Theorem D.1**.: _For any \(0<\beta<1/2\), there exists an \(\varepsilon\)-differentially private algorithm such that with probability \(1-\beta\) it has regret_

\[O\left(\frac{\log^{2}d+\log(T/\beta)\log(d/\beta)}{\varepsilon}\right)\]

_against oblivious adversaries in the realizable setting._Proof of Theorem 3.3.: Let \(\alpha=\beta=1/2T\). By Lemma 3.2, there exists an \((\alpha,\beta)\)-probabilistic representation of \(\mathcal{H}\) with

\[\operatorname{size}(\mathcal{P})=O(\log(T)\cdot(\texttt{RepDim}(\mathcal{H})+ \log\log T)).\]

Let \(S=\{(x_{1},y_{1}),\ldots,(x_{T},y_{T})\}\) be the sequence chosen by the adversary and \(\mathcal{D}_{S}\) be the empirical distribution of \(S\), namely, \(\Pr_{(x,y)\sim\mathcal{D}_{S}}[(x,y)=(x_{t},y_{t})]=1/T\) for all \(t\in[T]\). Then we have

\[\Pr_{V\sim\mathcal{P}}[\exists v\in V\ s.t.\ y_{t}=v(x_{t})\ \forall t \in[T]] =\Pr_{V\sim\mathcal{P}}[\exists v\in V\ s.t.\ \mathrm{error}_{\mathcal{D}_{S}}\leq 1 /\alpha=1/2T]\] \[\geq 1-\beta\] \[=1-1/2T.\]

Conditioning on this event, we then run the algorithm in Theorem D.1 with \(V\) being the set of experts and \(\ell_{t}(v)=1[v(x_{t})\neq y_{t}]\). With probability \(1-1/2T\), the number of mistakes is at most

\[O\left(\frac{\log^{2}\lvert V\rvert+\log T\log\lvert V\rvert+\log^{2}T}{ \varepsilon}\right)=O\left(\frac{\log^{2}T(\texttt{RepDim}(\mathcal{H})+\log \log T)^{2}}{\varepsilon}\right).\]

By the union bound, the expected number of mistakes is bounded by

\[1/T\cdot T+(1-1/T)\cdot O\left(\frac{\log^{2}T(\texttt{RepDim}(\mathcal{H})+ \log\log T)^{2}}{\varepsilon}\right).\]

### Proof of Theorem 3.4

The proof follows the same steps as the proof of Theorem 3.3. The only difference is that we use the following DP-OPE algorithm from [8] in the agnostic setting.

**Theorem D.2**.: _There exists an \(\varepsilon\)-differentially private algorithm that has an expected regret of_

\[\mathbb{E}\left[\sum_{t=1}^{T}\ell_{t}(i_{t})-\min_{i\in[d]}\sum_{t=1}^{T}\ell _{t}(i)\right]=O\left(\frac{\sqrt{T}\log d}{\varepsilon}\right)\]

_against oblivious adversaries in the agnostic setting._

Proof of Theorem 3.4.: We can use the same argument as in the proof of Theorem 3.3 to sample a hypothesis class \(V\) from \(\mathcal{P}\) such that \(\ln\lvert V\rvert=O(\log(T)\cdot(\texttt{RepDim}(\mathcal{H})+\log\log T))\) and

\[\Pr_{V\sim\mathcal{P}}[\exists v\in V\ s.t.\ v(x_{t})=h^{\star}(x_{t})]\geq 1 -1/2T,\]

where \(h^{\star}=\operatorname*{argmin}_{h\in\mathcal{H}}\mathrm{error}_{\mathcal{D} _{S}}(h)\) is a minimizer of the error on \(S\). Running the algorithm in Theorem D.2 gives an expected regret of at most

\[(1-1/2T)\cdot O\left(\frac{\sqrt{T}\log\lvert V\rvert}{\varepsilon}\right)+T \cdot 1/2T=O\left(\frac{\sqrt{T}\log T(\texttt{RepDim}(\mathcal{H})+\log\log T )}{\varepsilon}\right).\]

### Proof of Theorem 3.5

We start with the following claim, which states that Algorithm 1 is an adversary that preserves privacy.

**Claim D.3**.: _Suppose \(\mathcal{A}\) satisfies \(\varepsilon\)-adaptive differential privacy. Let \(\mathcal{B}\) be the algorithm that runs Algorithm 1 with \(\mathcal{A}\). Then \(\mathcal{B}\) is \(\varepsilon\)-differentially private._

Proof.: Let \(S_{1}\) and \(S_{2}\) be two input sequences that differ in only one entry. Consider an adversary \(\mathsf{Adv}\) that runs Algorithm 1 on \(S_{1}\) and \(S_{2}\) simultanously using the same randomness, and interacts with \(\mathcal{A}\) using \(S_{b}\) for some \(b\in\{1,2\}\). Let \(S^{\prime}_{1}\) and \(S^{\prime}_{2}\) be the resulting sequences. Note that when \(S_{1}[t]=S_{2}[t]\), with the same randomness we have \(S^{\prime}_{1}[t]=S^{\prime}_{2}[t]\) since the weights depend on \(h_{1},\ldots,h_{t-1}\) only. Thus, \(S^{\prime}_{1}\) and \(S^{\prime}_{2}\) also differ in only one entry. By the definition of adaptive differential privacy, it holds that for any event \(O\),

\[\Pr[\mathcal{A}\circ\mathsf{Adv}(1)\in O]\leq e^{\varepsilon}\Pr[\mathcal{A} \circ\mathsf{Adv}(2)\in O].\]

The conclusion follows by observing that the output distributions of \(\mathcal{A}\circ\mathsf{Adv}(b)\) and \(\mathcal{B}(S_{b})\) are identical.

Proof of Theorem 3.5.: Let \(\varepsilon\leq 0.01\). We first consider \(T\in[0.1\ln d/\varepsilon,0.2\ln d/\varepsilon]\) and prove a lower bound of \(\Omega(T)=\Omega(\log d/\varepsilon)\). To this end, we will assume that there exists an online learning algorithm \(\mathcal{A}\) with an expected mistake bound of \(0.01T\) that is \(\varepsilon\)-adaptive DP and derive a contradiction.

Let \(\mathcal{B}\) be the algorithm that runs Algorithm 1 with \(\mathcal{A}\). By Claim D.3, we know that \(\mathcal{B}\) is \(\varepsilon\)-differentially private. Now suppose we are running \(\mathcal{B}\) on \(S_{i}=\{(i,1),\ldots,(i,1)\}\) for some \(i\in[d]\). Let \(c_{t}(j)=\sum_{r=1}^{t}h_{r}(j)\), \(w_{t}(j)=e^{c_{t}(j)}\), \(\Phi_{t}=\sum_{j\in[d]\setminus\{i\}}w_{t}(j)\), and \(p_{t}(j)=w_{t}(j)/\Phi_{t}\). The expected number of mistakes made by \(\mathcal{A}\) can be expressed as

\[\sum_{t=1}^{T}\mathbb{E}\left[\frac{1}{2}\cdot\mathbb{I}[h_{t}(i)=0]+\frac{1} {2}\sum_{j\in[d]\setminus\{i\}}p_{t-1}(j)\cdot h_{t}(j)\right]\leq 0.01T.\] (1)

Now consider the potential \(\Phi_{t}\). At the beginning, we have \(\Phi_{0}=d-1\). We can upper bound \(\Phi_{t}\) by

\[\Phi_{t} =\sum_{j\in[d]\setminus\{i\}}w_{t}(j)\] \[=\sum_{j\in[d]\setminus\{i\}}w_{t-1}(j)e^{h_{t}(j)}\] \[=\Phi_{t-1}\sum_{j\in[d]\setminus\{i\}}p_{t-1}(j)e^{h_{t}(j)}\] \[\leq\Phi_{t-1}\sum_{j\in[d]\setminus\{i\}}p_{t-1}(j)(1+2h_{t}(j))\] \[=\Phi_{t-1}\left(1+2\sum_{j\in[d]\setminus\{i\}}p_{t-1}(j)h_{t}(j)\right)\] \[\leq\Phi_{t-1}\exp\left(2\sum_{j\in[d]\setminus\{i\}}p_{t-1}(j)h_ {t}(j)\right),\]

where we use \(e^{x}\leq 1+2x\) for \(0\leq x\leq 1\) in the forth line and \(1+x\leq e^{x}\) for \(x\in\mathbb{R}\) in the last line. Then it follows by induction that

\[\Phi_{T}\leq\Phi_{0}\exp\left(2\sum_{t=1}^{T}\sum_{j\in[d]\setminus\{i\}}p_{t -1}(j)h_{t}(j)\right).\]

Taking the logarithm on both sides, the linearity of expectation gives

\[\mathbb{E}[\ln\Phi_{T}] \leq\mathbb{E}\left[\ln\Phi_{0}+2\sum_{t=1}^{T}\sum_{j\in[d] \setminus\{i\}}p_{t-1}(j)h_{t}(j)\right]\] \[=\ln(d-1)+2\sum_{t=1}^{T}\sum_{j\in[d]\setminus\{i\}}\mathbb{E}[ p_{t-1}(j)h_{t}(j)]\] \[\leq\ln(d-1)+0.04T\] \[\leq 0.14T,\]

where the third line is due to (1) and the last inequality uses the facts that \(0.1\ln d/\varepsilon\leq T\) and \(\varepsilon\leq 0.01\).

By Markov's inequality, with probability at least \(5/6\) we have \(\ln\Phi_{T}\leq 0.84T\). This implies that, for every \(j\neq i\), we have

\[c_{T}(j)=\ln w_{T}(j)\leq\ln\Phi_{T}\leq 0.84T.\]

We then bound \(c_{T}(i)\). Note that by (1) we have

\[\mathbb{E}[T-c_{T}(i)]=\mathbb{E}\left[\sum_{t=1}^{T}\mathbb{I}[h_{t}(i)=0] \right]\leq 0.02T.\]Applying Markov's inequality again, with probability at least \(5/6\) we have \(T-c_{T}(i)\leq 0.12T\), or equivalently, \(c_{T}(i)\geq 0.88T\). By the union bound, it holds with probability \(2/3\) that \(c_{T}(i)\geq 0.88T>0.84T\geq c_{T}(j)\) for every \(j\neq i\).

Let \(O_{i}\) be the event that \(c_{T}(i)>c_{T}(j)\) for every \(j\neq i\), then \(O_{1},\ldots,O_{d}\) are disjoint. Then by group privacy,

\[1\geq\sum_{i=1}^{d}\Pr[\mathcal{B}(S_{1})\in O_{i}]\geq e^{-T \varepsilon}\sum_{i=1}^{d}\Pr[\mathcal{B}(S_{i})\in O_{i}]\geq 2/3\cdot de^{-T \varepsilon}.\]

Rearranging the inequality yields \(T\geq(\ln d-\ln 1.5)/\varepsilon>0.2\ln d/\varepsilon\) when \(d\geq 2\), a contradiction.

Now, let us deal with the remaining range. When \(T>0.2\ln d/\varepsilon\), the algorithm must make \(\Omega(\log d/\varepsilon)\) in the first \(\lfloor 0.2\ln d/\varepsilon\rfloor\) rounds. For \(T<0.1\ln d/\varepsilon\), suppose \(\mathcal{A}\) makes no more than \(0.005T\) mistakes in expectation. Then we can repeatedly initiate \(\mathcal{A}\) after every \(T\) round to obtain an algorithm that makes at most \(0.005kT\) mistakes in expectation for a total of \(kT\) rounds, where

\[k=\left\lfloor\frac{0.2\ln d/\varepsilon}{T}\right\rfloor\geq 0.5\cdot\frac{0.2 \ln d/\varepsilon}{T}=\frac{0.1\ln d/\varepsilon}{T}.\]

This contradicts our previous conclusion since \(kT\in[0.1\ln d/\varepsilon,0.2\ln d/\varepsilon]\). We thus obtain the \(\Omega(T)\) lower bound as desired. 

## Appendix E Proofs from Section 4

### Proof of Theorem 4.3

We start by analyzing Algorithm 2. As we discussed in Section 4.1, it constructs a series of data sequences and a list of time-steps that can be used to distinguish the sequences. We formalize this in the following lemma.

**Lemma E.1**.: _For any threshold \(k\) and online learning algorithm \(\mathcal{A}\) such that \(\Pr[\mathcal{A}(S_{0})_{t}(u_{1})=f_{1}(u_{1})]\geq 1/2\) for all \(t\in[T]\), where \(S_{0}=\{(u_{0},f_{1}(u_{0})),\ldots,(u_{0},f_{1}(u_{0}))\}\) and \(f_{1},f_{2},u_{0},u_{1}\) satisfy the property listed in Fact 4.2, Algorithm 2 either outputs an \(S\) on which \(\mathcal{A}\) makes more than \(k/3\) mistakes in expectation, or \(S_{1},\ldots,S_{m}\) and \(t_{1},\ldots,t_{m-1}\) such that_

1. \(m=\lceil T/k\rceil\)_._
2. _For each_ \(i\in[m]\)_, we have_ \(\Pr[\mathcal{A}(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})]<1/3\) _for all_ \(j\geq i\) _and_ \(\Pr[\mathcal{A}(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})]\geq 1/2\) _for all_ \(j\leq i-1\)_._

Proof.: If Algorithm 2 outputs a single \(S=S_{i}\), it inserts at least \(k+1\)\((u_{1},f_{2}(u_{1}))\)'s to \(S_{i}\). By our construction, each of them incurs a mistake with a probability of at least \(1/3\). Therefore, the expected number of mistakes made by \(\mathcal{A}\) on \(S_{i}\) is at least \((k+1)/3>k/3\). Now suppose Algorithm 2 does not output a single data stream, this means for every \(i\), Algorithm 2 replaces at most \(k\) elements when constructing \(S_{i}\) from \(S_{0}\).

For each \(i\in[m]\), the algorithm will first find the minimal \(t_{i-1}\) such that \(\Pr[\mathcal{A}(S_{j})_{t_{i-1}}(u_{1})=f_{1}(u_{1})]<1/3\) for all \(1\leq j\leq i-1\), which is exactly the first half of item 2. Moreover, this also suggests that \(t_{1}\leq\cdots\leq t_{m}\). Due to the construction process, the first \(t_{i-1}\) entries of \(S_{i}\) will be the same as \(S_{0}\). Therefore, we have \(\Pr[\mathcal{A}(S_{i})_{t_{j}}(u_{1})=f_{1}(u_{1})]\geq 1/2\) for any \(1\leq j\leq i-1\) since \(\mathcal{A}\) is an online algorithm. This proves the second half of item 2.

Now it remains to show the construction actually runs for \(\lceil T/k\rceil\) rounds, i.e., we can find \(t_{i-1}\) for any \(i\leq\lceil T/k\rceil\). Since \(S_{0}\) and any one of \(S_{1},\ldots,S_{i-1}\) differ at most \(k\) entries, the pigeonhole principle suggests that there exists some \(t\in[(i-1)k+1]\) such that \(S_{1}[t]=\cdots=S_{i-1}[t]=(u_{0},f_{1}(u_{0}))\). We next prove that \(t>t_{j}\) for every \(1\leq j<i-1\). For the sake of contradiction, assume that \(j\) is the smallest index such that \(t_{j}>t\) and \(j<i-1\) (it is impossible for \(t\) to be equal to any \(t_{j}\) since we always have \(S_{j+1}[t_{j}]=(u_{1},f_{2}(u_{1}))\) by our construction). Then we have \(t_{1}\leq\cdots\leq t_{j-1}<t\). This implies \(\Pr[\mathcal{A}(S_{l})_{t}(u_{1})=f_{1}(u_{1})]<1/3\) for every \(l\leq j\), otherwise Algorithm 2 will put a \((u_{1},f_{2}(u_{1}))\) at time-step \(t\) when constructing \(S_{l}\). By the minimality of \(t_{j}\), we should choose \(t_{j}\) to be \(t\) when constructing \(S_{j+1}\), contradicting our assumption that \(t_{j}>t\).

[MISSING_PAGE_FAIL:19]

We now apply the packing argument. Since \(\mathcal{B}\) is \((360\varepsilon,360\delta)\)-differentially private, it follows by group privacy that

\[1 =\sum_{i=1}^{m}\Pr[\mathcal{B}(S_{0})=i]\] \[\geq\sum_{i=1}^{m}\left(e^{-360k\varepsilon}\Pr[\mathcal{B}(S_{i} )=i]-\frac{1-e^{-360k\varepsilon}}{e^{360\varepsilon}-1}\cdot 360\delta\right)\] \[\geq 2/3\cdot 2^{-0.545}\cdot m^{0.455}\cdot e^{-360k\varepsilon }-m\cdot\frac{1-e^{-360k\varepsilon}}{e^{360\varepsilon}-1}\cdot 360\delta\] \[\geq 0.45m^{0.455}\cdot e^{-360k\varepsilon}-m\cdot\frac{1-e^{-360 k\varepsilon}}{e^{360\varepsilon}-1}\cdot 360\delta.\]

If \(0.45m^{0.455}\cdot e^{-360k\varepsilon}\geq 2m\cdot\frac{1-e^{-360k \varepsilon}}{e^{360\varepsilon}-1}\cdot 360\delta\), then we have

\[0.225m^{0.455}\cdot e^{-360k\varepsilon}\leq 0.45m^{0.455}\cdot e^{-360k \varepsilon}-m\cdot\frac{1-e^{360k\varepsilon}}{e^{360\varepsilon}-1}\cdot 3 60\delta\leq 1.\]

Rearranging the above gives

\[k \geq\frac{\ln 0.225+0.455\ln m}{360\varepsilon}\] \[\geq\frac{\ln 0.225+0.455\ln(T/k)}{360\varepsilon}\] \[\geq\frac{\ln 0.225+0.455\ln\left(\frac{3600T\varepsilon}{c\ln T} \right)}{360\varepsilon}\] \[\geq\frac{\ln 0.225+0.455c\ln T+0.455\ln(3600/c)}{360\varepsilon}\] \[\geq\frac{0.455c\ln T}{360\varepsilon},\]

where in the forth inequality we use the condition \(\varepsilon\geq\ln T/T^{1-c}\). This contradicts our assumption that \(k\leq c\ln T/3600\varepsilon\).

If otherwise \(0.45m^{0.455}\cdot e^{-360k\varepsilon}<2m\cdot\frac{1-e^{-360k\varepsilon}} {e^{360\varepsilon}-1}\cdot 360\delta\), we have

\[360k\varepsilon >\ln\left(\frac{e^{360\varepsilon}-1}{360\delta}\cdot 0.225m^{-0.545 }+1\right)\] \[>\ln\left(\frac{e^{360\varepsilon}-1}{360\delta}\right)+\ln 0.225-0.545\ln m\] \[\geq\ln(\varepsilon/\delta)-0.545\ln T+\ln 0.225\] \[\geq 0.455\ln T+\ln 0.225\] \[\geq 0.1\ln T\]

when \(T\geq 100\). Then it follows that \(k>0.1\ln T/(360\varepsilon)>c\ln T/3600\varepsilon\), again a contradiction.

In conclusion, any \((\varepsilon,\delta)\)-differentially private algorithm makes \((k+1)/3\geq c\ln T/(10800\varepsilon)\) mistakes in expectation when \(T\geq 100\). This gives the \(\Omega(\log T/\varepsilon)\) lower bound. 

### Proof of Theorem 4.4

Proof.: When \(\texttt{LD}(\mathcal{H})=1\), the result is directly implied by Theorem 4.3. From now on, we will assume \(\texttt{LD}(\mathcal{H})\geq 2\).

Let \(m=\lfloor\texttt{LD}(\mathcal{H})/2\rfloor\) and \(T^{\prime}=\lfloor T/m\rfloor\geq\lfloor 2T/\texttt{LD}(\mathcal{H})\rfloor\geq T/ \texttt{LD}(\mathcal{H})>T^{1-1/(1+c_{2})}=T^{c_{2}/(1+c_{2})}\). Pick a shattered tree for \(\mathcal{H}\) of depth \(\texttt{LD}(\mathcal{H})\). Let \(u_{0}\) be its root and \(u_{1}\) be the left child of \(u_{0}\). The definition of shattered tree indicates that there exists \(f_{1},f_{2}\in\mathcal{H}\) such that \(f_{1}(u_{0})=f_{2}(u_{0})=0\) and \(0=f_{1}(u_{1})\neq f_{2}(u_{1})=1\). By Theorem 4.3, for any \((\varepsilon,\delta)\)-differentially private online learner with \(\varepsilon\geq\ln T/T^{c_{2}(1-c_{1})/(1+c_{2})}>\ln T^{\prime}/T^{\prime 1-c_{1}}\) and \(\delta\leq\varepsilon/T<\varepsilon/T^{\prime}\), we can construct a sequence \(S_{1}\) such that it makes in expectation \(\Omega(\log T^{\prime}/\varepsilon)=\Omega(\log T/\varepsilon)\) mistakes on \(S_{1}\). Moreover, \(S_{1}\) only contains \((u_{0},0)\) and \((u_{1},b)\) for some \(b\in\{0,1\}\).

Let \(\mathcal{H}^{\prime}=\{h\in\mathcal{H}:h(u_{0})=0\ and\ h(u_{1})=b\}\). If \(\texttt{LD}(\mathcal{H})\geq 4\), we then have \(\texttt{LD}(\mathcal{H}^{\prime})\geq\texttt{LD}(\mathcal{H})-2\geq 2\) since the subtree rooted at the child (left if \(b=0\), right if \(b=1\)) of \(u_{1}\) is shattered by \(\mathcal{H}^{\prime}\) and has a depth of \(\texttt{LD}(\mathcal{H})-2\). Thus, we can similarly construct a sequence \(S_{2}\) such that the algorithm makes \(\Omega(\log T/\varepsilon)\) errors. Importantly, the entries in \(S_{2}\) are completely different from those in \(S_{1}\).

We repeat the above process until we reach a hypothesis class with Littlestone dimension \(\leq 1\) and construct a series of sequences \(S_{1},\dots,S_{m}\) that have non-overlapping entries. On each of them, the learner makes \(\Omega(\log T/\varepsilon)\) mistakes in expectation. Let \(S\) be the stream formed by concatenating \(S_{1},\dots,S_{m}\) together. The length of \(S\) is at most \(mT^{\prime}\leq T\) while the expected number of mistakes on \(S\) is \(\Omega(\texttt{LD}(\mathcal{H})\log T)\). 

## Appendix F Additional Algorithms

In this section, we provide several algorithms for various tasks. Note that some of the upper bounds hold even against strong adaptive adversaries, i.e., adversaries that can see the prediction made by the learner in the current round.

### Learning Point Functions Against Oblivious Adversaries

We show how to improve the \(\log^{2}T\) dependence in Theorem 3.3 to \(\log T\) for \(\texttt{POINT}_{\mathbb{N}}\). In the beginning, we keep outputting an all-zero function and use the sparse vector technique to monitor the mistakes. Then we sample hypotheses from the probabilistic representation constructed in [10] and again apply the sparse vector technique to find one with low error on the past data. Following their argument, we can show that the construction guarantees that the hypothesis we found won't make too many mistakes in the future. The result is stated as follows.

**Theorem F.1**.: _In the realizable setting, there is an \(\varepsilon\)-differentially private online learning algorithm for \(\texttt{POINT}_{\mathbb{N}}\) that makes in expectation \(O(\log T/\varepsilon)\) mistakes against oblivious adversaries._

Proof.: We run Algorithm 5. Note that it is in fact composed of two instances of \(\mathsf{AboveThreshold}\) with privacy parameter \(\varepsilon/2\), the algorithm is \(\varepsilon\)-DP.

Consider the first \(\mathsf{AboveThreshold}\), there are \(T+1\leq 2T\) Laplace random variables. With probability \(1-1/3T\), all of them have absolute values no larger than \(4\ln(6T^{2})/\varepsilon^{\prime}\). Thus, when it halts, there are at least \(\hat{L}-4\ln(6T^{2})/\varepsilon^{\prime}\geq 20\ln(12T^{3})/\varepsilon^{\prime}\) data points of the form \((x^{\star},1)\), where \(f_{x^{\star}}\) is the target hypothesis. Moreover, it makes at most \(20\ln(12T^{3})/\varepsilon^{\prime}+8\ln(6T^{2})/\varepsilon^{\prime}+8\ln(6T ^{2})/\varepsilon^{\prime}+8\ln(6T^{2})/\varepsilon^{\prime}+1\) mistakes.

Now we show that the sampling won't last for too long. We consider the first \(3T^{2}\) iterations. Then with probability \(1-1/3T\), every random noise has an amplitude of at most \(4\ln(12T^{3})/\varepsilon^{\prime}\) in the second \(\mathsf{AboveThreshold}\). Note that \(\sum_{r=1}^{t}\mathbb{I}[h(x_{r})\neq y_{r}\) and \(y_{r}=1]\geq 20\ln(12T^{3})/\varepsilon^{\prime}\) if \(h(x^{\star})=0\) and is \(0\) if \(h(x^{\star})=1\). Thus, the algorithm will exit the loop if and only if \(h(x^{\star})=1\). The probability that this happens in the first \(3T^{2}\) iterations is at least \(1-(1-1/T)^{3T^{2}}\geq 1-e^{-3T}\geq 1-1/3T\). Moreover, this \(h\) makes in expectation less than \((T-1)/T<1\) mistakes on data points other than \((x^{\star},1)\) in the sequence.

Putting it all together, the expected number of mistakes is less than

\[1/T\cdot T+(1-1/T)\cdot(20\ln(12T^{3})/\varepsilon^{\prime}+8\ln(6T^{2})/ \varepsilon^{\prime}+8\ln(6T^{2})/\varepsilon^{\prime}+1+1)=O(\log T/\varepsilon).\]

### Learning Point Functions Against Adaptive Adversaries

**Theorem F.2**.: _In the realizable setting, there is an \(\varepsilon\)-differentially private online learning algorithm for \(\texttt{POINT}_{d}\) that makes in expectation \(O((\log d+\log T)/\varepsilon)\) mistakes against strong adaptive adversaries._Proof.: Let \(\varepsilon^{\prime}=\varepsilon/2\). The algorithm works as follows: it keeps outputting an all-zero function and runs an \(\varepsilon^{\prime}\)-differentially private sparse vector technique (Algorithm 4) with \(L=3(\ln d+\ln(2T))/\varepsilon^{\prime}+8\ln(4T^{2})/\varepsilon^{\prime}\) to monitor the number of mistakes. Once the (noisy) number of mistakes exceeds \(\hat{L}\) at round \(k\), it computes \(c_{i}=|\{t\in[k]:(x_{t},y_{t})=(i,1)\}|\) and apply the report-noisy-max mechanism with privacy parameter \(\varepsilon^{\prime}\) to find an index \(i\) with a large \(c_{i}\). After that, it persistently outputs \(f_{i}\) till the end. The privacy directly follows from the basic composition.

With probability \(1-1/2T\), the amplitude of every noise added in the sparse vector technique is no larger than \(4\ln(4T^{2})/\varepsilon^{\prime}\). Thus, at round \(k\), the number of mistakes must be in the range \([\hat{L}-4\ln(4T^{2})/\varepsilon^{\prime},\hat{L}+4\ln(4T^{2})/\varepsilon^{ \prime}+1]\subseteq[L-8\ln(4T^{2})/\varepsilon^{\prime},L+8\ln(4T^{2})/ \varepsilon^{\prime}+1]\). Hence, we have \(c_{i}\geq L-8\ln(4T^{2})/\varepsilon^{\prime}=3(\ln d+\ln(2T))/\varepsilon^{\prime}\) for some \(i\) and \(c_{j}=0\) for all \(j\neq i\). With probability \(1-1/2T\), the report-noisy-max algorithm will identify \(i\) correctly.

Thus, the expected number of mistakes is bounded by

\[1/T\cdot T+(1-1/T)\cdot(L+8\ln(4T^{2})/\varepsilon^{\prime}+1)=O((\log d+\log T )/\varepsilon).\]

### Learning Threshold Functions

A threshold function over \([d]\) is a function \(f_{i}\) such that \(f_{i}(x)=0\) for all \(x\leq i\) and \(f_{i}(x)=1\) for all \(x>i\), where \(i\in[d]\cup\{0\}\). The class of threshold functions over \([d]\), denote by \(\mathtt{Threshold}_{d}\), is the set \(\{f_{i}:i\in[d]\cup\{0\}\}\).

**Theorem F.3**.: _In the realizable setting, there is an \(\varepsilon\)-differentially private online learning algorithm for \(\mathtt{Threshold}_{d}\) that makes in expectation \(O(\log d\log T/\varepsilon)\) mistakes against strong adaptive adversaries._

Proof.: We run Algorithm 6. Since the counters are refreshed once we switch the current hypothesis, we are actually running \(\mathtt{AboveThreshold}\) on disjoint datasets. Moreover, the comparisons of \(c_{0}\) and \(c_{1}\) are also performed on disjoint datasets. Therefore, the overall algorithm is \(\varepsilon\)-DP.

Since the binary search runs for at most \(\lceil\log_{2}(d+1)\rceil\) iterations, it follows by the privacy of \(\mathtt{AboveThreshold}\) and the basic composition that the overall algorithm is \(\varepsilon\)-DP since the counters are refreshed once we switch the current hypothesis.

By the property of Laplace distribution and the union bound, with probability \(1-1/T\), every random noise that appears in the algorithm has an amplitude no larger than \(4\ln(4T^{2})/\varepsilon^{\prime}\). Conditioning on this event, once we change the current hypothesis, we must have \(c_{0}+c_{1}\geq\hat{L}-4\ln(4T^{2})/\varepsilon^{\prime}\geq 8\ln(4T^{2})/ \varepsilon^{\prime}\) and \(c_{0}+c_{1}\leq\hat{L}+4\ln(4T^{2})/\varepsilon^{\prime}+1\leq 24\ln(4T^{2})/ \varepsilon^{\prime}+1\). Therefore, we can identify which one is zero. Since the binary search runs at most \(O(\log d)\) iterations, we will make at most

\[O(\log d)\cdot(24\ln(4T^{2})/\varepsilon^{\prime}+1)=O(\log d\log T/\varepsilon)\]

mistakes with probability \(1-1/T\). This implies an \(1/T\cdot T+O(\log d\log T/\varepsilon)=O(\log d\log T/\varepsilon)\) bound in expectation. 

``` Input: the number of rounds \(T\); privacy parameter \(\varepsilon\); sequence \(S\) Output: hypotheses \(h_{1},\dots,h_{T}\)
1\(\varepsilon^{\prime}=\varepsilon/2\)
2\(\hat{L}\gets 16\ln(4T^{2})/\varepsilon^{\prime}+\operatorname{Lap}(2/ \varepsilon^{\prime})\)
3\(l\gets 0,r\gets d,mid\leftarrow\lfloor(l+r)/2\rfloor\)
4\(c_{0}\gets 0,c_{1}\gets 0\)
5for\(t=1,\dots,T\)do
6\(h_{t}\gets f_{mid}\)
7\((x_{t},y_{t})\gets S[t]\)
8\(c_{y_{t}}\gets c_{y_{t}}+\mathbb{I}[h(x_{t})\neq y_{t}]\)
9if\(l<r\)and \(c_{0}+c_{1}+\operatorname{Lap}(4/\varepsilon^{\prime})\geq\hat{L}\)then
10if\(c_{0}+\operatorname{Lap}(1/\varepsilon^{\prime})>c_{1}\)then
11\(l\gets mid+1\)
12else
13\(r\gets mid-1\)
14 end if
15\(mid\leftarrow\lfloor(l+r)/2\rfloor\)
16\(c_{1}\gets 0,c_{2}\gets 0\)
17\(\hat{L}\gets 16\ln(4T^{2})/\varepsilon^{\prime}+\operatorname{Lap}(2/ \varepsilon^{\prime})\)
18
19 end for
20
21 end for ```

**Algorithm 6**Learning threshold functions over \([d]\)

### Online Prediction from Experts Against Adaptive Adversaries

It is easy to come up with an algorithm with a regret of \(O(d\log T/\varepsilon)\) even against strong adaptive adversaries. The idea is similar to Algorithm 6. The only difference is that we try the experts one by one instead of running a binary search.

**Theorem F.4**.: _There is an \(\varepsilon\)-differentially private algorithm that solves the OPE problem with an expected regret of \(O\left(\frac{d\log T}{\varepsilon}\right)\) even against strong adaptive adversaries in the realizable setting._

Proof.: We iterate over the set of experts. For each expert, we keep choosing it and run an \(\varepsilon\)-differentially private sparse vector technique to monitor the loss incurred by the current expert. Once the sparse vector technique halts, we switch to the next expert and restart the sparse vector technique. Since all instances of the sparse vector technique are run on disjoint data, the entire algorithm is \(\varepsilon\)-DP.

With probability \(1-1/T\), all the Laplace noises added are bounded by \(4\ln(2T^{2})/\varepsilon\). By choosing \(L=9\ln(2T^{2})/\varepsilon\), we will make at most \(O(\log T/\varepsilon)\) mistakes on each expert. Moreover, for an expert that makes no errors, we will not switch to the next one. Therefore, the overall expected regret is

\[1/T\cdot T+(1-1/T)\cdot d\cdot O(\log T/\varepsilon)=O(d\log T/\varepsilon).\]

Note that there is a multiplicative factor of \(d\) on the \(\log T\) term. We will then show how to improve this dependence to \(\log d\) for (weak) adaptive adversaries. Due to Corollary 4.5, such a dependence is tight even for oblivious adversaries. It is interesting to find out if this can also be achieved for strong adaptive adversaries, and we leave this as future work.

Moreover, our algorithm has a regret of \(O\left(d\log^{2}d+\log d\log T\right)\). Since it was shown by Asi et al. [8] that an \(\Omega(d)\) cost is inevitable, our algorithm is near optimal.

The idea is to select a uniformly random expert rather than keep choosing a fixed one. Suppose we are at the beginning. Let \(c_{t}(i)=\sum_{r=1}^{t}\ell_{r}(i)\) for expert \(i\). The benefit of such random selection is that it only incurs a loss of \(\sum_{i\in[d]}c_{t}(i)/d\), which is much less than the \(\sum_{i\in[d]}c_{t}(i)\) loss if we always choose the same expert since the adversary can let this expert make an error all the time.

We use the sparse vector technique to track the maximum of \(c_{t}\). Once it exceeds \(O(\log T+d\log d)\), we then apply the report-noisy-max algorithm to remove every expert \(j\) with \(a_{t}(j)=\sum_{r=1}^{t}\ell_{r}(j)\geq O(d\log d)\) (we do not write \(c_{t}\) here since we will reset \(c_{t}\) to be \(0\) later). After that, we reiterate the sampling and monitoring process using the remaining experts. Observe that if expert \(j\) is the \(i\)-th one being removed, the loss incurred by \(j\) is at most \(O(\log T+d\log d)/i\). This gives an upper bound of \(O(\log T+d\log d)\cdot(1+1/2+\cdots+1/d)=O(d\log^{2}d+\log d\log T)\). The details are depicted in Algorithm 7.

``` Input: the number of rounds \(T\); privacy parameter \(\varepsilon\); sequence \(S\) Output: indices of experts \(i_{1},\ldots,i_{T}\)
1 Set \(a_{0}(i)\gets 0\) and \(c_{0}(i)\gets 0\) for all \(i\in[d]\)
2\(E\gets[d]\)
3\(\varepsilon_{1}=\varepsilon/2\), \(\varepsilon_{2}=\varepsilon/(6d)\)
4\(\hat{L}\gets 8\ln(2T^{2})/\varepsilon_{1}+3(\ln d+\ln(3d^{2}))/ \varepsilon_{2}+\mathrm{Lap}(2/\varepsilon_{1})\)
5for\(t=1,\ldots,T\)do
6 Sample \(i_{t}\) uniformly from \(E\)
7\(\ell_{t}\gets S[t]\)
8 Update \(a_{t}(i)\gets a_{t-1}(i)+\ell_{t}(i)\) and \(c_{t}(i)\gets c_{t-1}(i)+\ell_{t}(i)\) for all \(i\in[d]\)
9if\(|E|>1\)and \(\max_{i\in E}c_{t}(i)+\mathrm{Lap}(4/\varepsilon_{1})\geq\hat{L}\)then
10 Run report-noisy-max with privacy parameter \(\varepsilon_{2}\) on \(\{a_{t}(j)\}_{j\in E}\) and obtain some index \(i\)
11 Update \(E\gets E\setminus\{i\}\)
12while\(i\notin E\)and \(|E|>1\)do
13 Run report-noisy-max with privacy parameter \(\varepsilon_{2}\) on \(\{a_{t}(j)\}_{j\in E}\) and obtain some index \(i\)
14if\(a_{t}(i)+\mathrm{Lap}(1/\varepsilon_{2})>\ln(3d^{2})/\varepsilon_{2}\)then
15 Update \(E\gets E\setminus\{i\}\)
16 end if
17
18 end for
19\(\hat{L}\gets 8\ln(2T^{2})/\varepsilon_{1}+3(\ln d+\ln(3d^{2}))/ \varepsilon_{2}+\mathrm{Lap}(2/\varepsilon_{1})\)
20\(c_{t}(i)\gets 0\) for all \(i\in[d]\)
21
22 end for
23
24 end for ```

**Algorithm 7**DP-OPE against adaptive adversaries

Here is a subtle issue: the report-noisy-max mechanism only succeeds with probability \(1-1/d\). This does not imply an upper bound in expectation. We cannot simply raise the success probability to \(1-1/T\). Otherwise, the \(d\log^{2}d\) term will become \(d\log d\log T\), which is even higher than the brute force. To fix this, we run an extra sparse vector technique on top of the algorithm to inspect the loss. Once the loss is greater than the upper bound, we then run the algorithm in Theorem F.4 for the rest of the rounds. This reduces the expected cost from \(T\) to \(O(d\log T)\) when it fails, hence successfully bound the expected loss.

**Theorem F.5**.: _There is an \(\varepsilon\)-differentially private algorithm that solves the OPE problem with an expected regret of \(O\left(\frac{d\log^{2}d+\log d\log T}{\varepsilon}\right)\) against adaptive adversaries in the realizable setting._

Proof.: We first show the privacy and utility guarantee for Algorithm 7. For privacy, the sparse vector technique is \(\varepsilon/2\)-DP. The report-noisy-max outside the while loop will be executed at most \(d\) times, and the same for the one inside the while loop. Also, the if clause inside the while loop will also be executed at most \(d\) times. All of these cost a privacy budget of \(3d\varepsilon_{2}=\varepsilon/2\). Thus, the overall algorithm is \(\varepsilon\)-DP.

We next analyze the regret. With probability \(1-1/T\), all the noises added by \(\mathsf{AboveThreshold}\) are bounded by \(4\ln(2T^{2})/\varepsilon_{1}\). Therefore, once \(c_{t}(i)\geq\frac{16\ln(2T^{2})}{\varepsilon_{1}}+\frac{3(\ln d+\ln(3d^{2}))} {\varepsilon_{2}}\) for some \(i\in E\), we must invoke the report-noisy-max mechanism. On the other hand, once we invoke the report-noisy-max mechanism outside the while loop, we must have \(c_{t}(i)\geq\frac{3(\ln d+\ln(3d^{2}))}{\varepsilon_{2}}\) for some \(i\). With probability \(1-1/3d\), it always identifies an \(i\) with \(a_{t}(i)\geq\frac{3(\ln d+\ln(3d^{2}))}{\varepsilon_{2}}-\frac{2(\ln d+\ln(3d^ {2}))}{\varepsilon_{2}}>0\) and delete it.

Now let us move into the while loop. With probability \(1-1/3d\) the report-noisy-max always returns an \(i\) with \(a_{t}(i)>\frac{2\ln(3d^{2})}{\varepsilon_{2}}+\frac{2(\ln d+\ln(3d^{2}))}{ \varepsilon_{2}}-\frac{2(\ln d+\ln(3d^{2}))}{\varepsilon_{2}}=\frac{2\ln(3d^{2 })}{\varepsilon_{2}}\) whenever \(\max_{j\in[E]}a_{t}(j)>\frac{2(\ln d+\ln(3d^{2}))}{\varepsilon_{2}}+\frac{2 \ln(3d^{2})}{\varepsilon_{2}}\). Moreover, the noise added in the if clause is less than \(\frac{\ln(3d^{2})}{\varepsilon_{2}}\) for the entire time span with probability \(1-1/3d\). Thus, expert \(i\) will be removed. Conversely, if it identifies an \(i\) such that \(a_{t}(i)=0\), we will not remove \(i\) and exit the loop.

Let \(E_{t}\) be the set \(E\) at the beginning of round \(t\) and \(E_{T+1}\) be the set \(E\) after the algorithm terminates. Suppose \(i\) is removed from \(E\) at time-step \(T_{i}\) (if it still remains in \(E\) at the end, we define \(T_{i}=T+1\)). From the above analysis, we know that with probability \(1-1/d-1/T\), for every expert \(i\) we have \(a_{T_{i}}(i)\leq\frac{3(\ln d+\ln(3d^{2}))}{\varepsilon_{2}}+\frac{2\ln(3d^{2 })}{\varepsilon_{2}}+\frac{16\ln(2T^{2})}{\varepsilon_{1}}+1\). Conditioning on the event that \(a_{T_{i}}(i)\leq M\) for some \(M\), we can bound the expected loss by

\[\sum_{t=1}^{T}\mathbb{E}\left[\sum_{i\in E_{t}}\ell_{t}(i)/|E_{t}|\right] =\sum_{t=1}^{T}\mathbb{E}\left[\sum_{i\in[d]}\mathbb{I}[i\in E_{ t}]\ell_{t}(i)/|E_{t}|\right]\] \[=\mathbb{E}\left[\sum_{i\in[d]}\sum_{t=1}^{T_{i}}\ell_{t}(i)/|E_{ t}|\right]\] \[\leq\mathbb{E}\left[\sum_{i\in[d]}\sum_{t=1}^{T_{i}}\ell_{t}(i)/| E_{T_{i}}|\right]\] \[=\mathbb{E}\left[\sum_{i\in[d]}a_{T_{i}}(i)/|E_{T_{i}}|\right]\] \[\leq\mathbb{E}\left[\sum_{i\in[d]}\frac{1}{|E_{T_{i}}|}\right].M\] \[\leq\left(\sum_{j=1}^{d}\frac{1}{j}\right)\cdot M\] \[\leq 2\ln d\cdot M\]

for \(d\geq 2\) (the case that \(d=1\) is trivial, so we just ignore it).

We already have \(M=O\left(\frac{d\log d+\log T}{\varepsilon}\right)\) with probability \(1-1/d-1/T\). However, this is not enough to show an expected bound. To get an expected bound, we run Algorithm 7 with privacy parameter \(\varepsilon^{\prime}=\varepsilon/2\) and an \(\varepsilon^{\prime}\)-differentially private \(\mathsf{AboveThreshold}\) to monitor \(\max_{i\in E_{t}}a_{t}(i)\) (note that it won't affect the privacy and utility even if we release \(E_{t}\) publicly) with threshold

\[L=\left(\frac{2\ln(3d^{2})}{\varepsilon^{\prime}}+\frac{3(\ln d+\ln(3d^{2}))}{ \varepsilon^{\prime}}+\frac{16\ln(2T^{2})}{\varepsilon^{\prime}}+1\right)+ \frac{4\ln(T^{2})}{\varepsilon^{\prime}}.\]Once AboveThreshold halts, we then stop Algorithm 7 and switch to the algorithm in Theorem F.4 with privacy parameter \(\varepsilon\).

It is not hard to see that the overall algorithm is \(\varepsilon\)-DP. For utility, we have that with probability \(1-1/T\) AboveThreshold halts once \(\max_{i\in E_{i}}a_{t}(i)\) is greater than \(\overline{L}=L+\frac{4\ln(T^{2})}{\varepsilon^{\prime}}\) and never halts if the value is always no more than \(\underline{L}=L-\frac{4\ln(T^{2})}{\delta^{\prime}}\). Conditioning on this, with probability \(1-1/d-1/T\) we have \(a_{T_{i}}\leq\underline{L}\) for every \(i\) and thus AboveThreshold never halts. In this case, the expected loss is at most \(\underline{L}\cdot 2\ln d=O\left(\frac{d\log^{2}d+\log d\log T}{\varepsilon}\right)\). If AboveThreshold halts, the expected loss incurred before that moment should be at most \((\overline{L}+1)\cdot 2\ln d=O\left(\frac{d\log^{2}d+\log d\log T}{\varepsilon}\right)\). And after that, the regret should be \(O\left(\frac{d\log T}{\varepsilon}\right)\) by Theorem F.4. Putting these two cases together gives an expected regret of

\[\left(1-\frac{1}{d}-\frac{1}{T}\right)\cdot\underline{L}\cdot 2 \ln d+\left(\frac{1}{d}+\frac{1}{T}\right)\left((\overline{L}+1)\cdot 2\ln d+O \left(\frac{d\log T}{\varepsilon}\right)\right)\] \[=O\left(\frac{d\log^{2}d+\log d\log T}{\varepsilon}\right)+\left( \frac{1}{d}+\frac{1}{T}\right)O\left(\frac{d\log T}{\varepsilon}\right)\] \[=O\left(\frac{d\log^{2}d+\log d\log T}{\varepsilon}\right).\]

Thus, the expected regret of the entire algorithm is

\[\left(1-\frac{1}{T}\right)\cdot O\left(\frac{d\log^{2}d+\log d\log T}{ \varepsilon}\right)+\frac{1}{T}\cdot T=O\left(\frac{d\log^{2}d+\log d\log T}{ \varepsilon}\right).\]

### Learning Two Complementary Hypotheses

Let \(\mathcal{H}=\{f_{1},f_{2}\}\) such that \(f_{1}=1-f_{2}\). We now give an algorithm that achieves a mistake bound of \(O(1/\varepsilon)\).

It is not hard to come up with an algorithm that makes \(O(1/\varepsilon)\) mistakes with constant probability. Since the incorrect hypothesis makes an error on every input sample, we can output arbitrarily in the first \(O(1/\varepsilon)\) rounds. Then we use the \(O(1/\varepsilon)\) data points to figure out which hypothesis is the correct one. This can be done using the Laplace mechanism.

However, a constant probability bound does not imply an expected bound. Note that once the Laplace mechanism fails, we may make \(\Omega(T)\) errors on the entire sequence. Thus, if we follow the above framework, we have to achieve a success probability of \(1-1/T\). This requires \(O(\log T/\varepsilon)\) samples, which exceed our target.

To reduce the expected number of errors, we split the entire sequence into buckets of length \(O(1/\varepsilon)\). We perform the Laplace mechanism on every bucket. Then for the \(i\)-th bucket, instead of just using the result on the last bucket to predict, we take the majority over all previous buckets. This makes the fail probability decrease exponentially. The mistake bound then converges to \(O(1/\varepsilon)\) as desired.

We describe the procedure in Algorithm 8 and provide a formal statement of our result in the following theorem.

**Theorem F.6**.: _Let \(\mathcal{H}=\{f_{1},f_{2}\}\) be a hypothesis such that \(f_{1}\) and \(f_{2}\) are complementary. In the realizable setting, there exists an \(\varepsilon\)-differentially private algorithm that online learns \(\mathcal{H}\) with a mistake bound of \(O(1/\varepsilon)\) even against strong adaptive adversaries._

Proof.: It is easy to see that Algorithm 8 is \(\varepsilon\)-differentially private since we add Laplace noise on disjoint buckets. For each bucket, with probability \(2/3\), the noise added to the counter is less than \(\ln(3)/\varepsilon<s/2\). This means we can identify the correct hypothesis with probability \(2/3\).

For the \(n\)-th bucket, we will make \(s\) errors only if we wrongly identify the target hypothesis on at least half of the previous buckets. By Hoeffding's inequality, this happens with probability at most

\[\exp\left(-2(n-1)(1/6)^{2}\right)=\exp(-(n-1)/18).\]Thus, the expected number of mistakes is bounded by

\[s\sum_{n=1}^{\infty}\exp(-(n-1)/18)=O(1/\varepsilon).\]

``` Input: the number of rounds \(T\); hypothesis class \(\mathcal{H}=\{f_{1},f_{2}\}\) such that \(f_{1}\) and \(f_{2}\) are complementary; privacy parameter \(\varepsilon\); sequence \(S\) Output: hypotheses \(h_{1},\ldots,h_{T}\)
1\(c_{1}\gets 0\), \(c_{2}\gets 0\)
2\(s\leftarrow\lceil 2\ln(3)/\varepsilon\rceil\)
3for\(t=1,\ldots,T\)do
4\((x_{t},y_{t})\gets S[t]\)
5if\(c_{1}>c_{2}\)then
6\(h_{t}\gets f_{1}\)
7else
8\(h_{t}\gets f_{2}\)
9 end if
10if\(t\bmod s=0\)then
11if\(\sum_{r=t-s+1}^{t}\lceil f_{1}(x_{r})\neq y_{r}\rceil+\operatorname{Lap}(1/ \varepsilon)<s/2\)then
12\(c_{1}\gets c_{1}+1\)
13else
14\(c_{2}\gets c_{2}+1\)
15 end if
16
17 end if
18
19 end for ```

**Algorithm 8**Learning complementary hypotheses

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we state our two main contributions: a separation result between pure and approximate DP, and the other one between private and non-private settings. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations and future work in Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The proofs of the results in Section 3 and 4 can be found in Appendix D and E respectively. We also provide some additional results together with their formal proofs in Appendix F. For each theorem in the paper, we clearly state the assumptions that it relies on. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

**Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [NA] Justification: The paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

**Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and make sure that the research in this paper conforms with it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the potential impacts of this paper in Appendix B. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not have such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.