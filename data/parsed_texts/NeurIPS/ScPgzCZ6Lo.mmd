# GC-Bench: An Open and Unified Benchmark for Graph Condensation

 Qingyun Sun\({}^{1}\)

Ziying Chen\({}^{1}\)

Beining Yang\({}^{2}\)

Cheng Ji\({}^{1}\)

Xingcheng Fu\({}^{3}\)

**Sheng Zhou\({}^{4}\)**

Hao Peng\({}^{1}\)

Jianxin Li\({}^{1}\)

Philip S. Yu\({}^{5}\)

\({}^{1}\)Beihang University

\({}^{2}\)University of Edinburgh

\({}^{3}\)Guangxi Normal University

\({}^{4}\)Zhejiang University

\({}^{5}\)University of Illinois

 Chicago

{sunay,chanztuying}@buaa.edu.cn

Equal contribution.

###### Abstract

Graph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graph-level tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research. The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.

## 1 Introduction

Data are the driving force behind advancements in machine learning, especially with the advancement of large models. However, the rapidly increasing size of datasets presents challenges in management, storage, and transmission. It also makes model training more costly and time-consuming. This issue is particularly pronounced in the graph domain, where larger datasets mean more large-scale structures, making it challenging to train models in environments with limited resources. Compared to graph coarsening, which groups nodes into super nodes, and sparsification, which selects a subset of edges, graph condensation [39, 10] synthesizes a smaller, informative graph that retains enough data for models to perform comparably to using the full dataset.

**Graph condensation.** Graph condensation aims to learn a new small but informative graph. A general framework of GC are shown in Figure 1. Given a graph dataset \(\mathbf{G}\), the goal of Graph condensation is to achieve comparable results on synthetic condensed graph dataset \(\mathbf{G}^{\prime}\) as training on the original \(\mathbf{G}\). For Node-level condensation, the original dataset \(\mathbf{G}=\{\mathcal{G}\}=\{\mathbf{X}\in\mathbb{R}^{N\times d},\mathbf{A}\in \mathbb{R}^{N\times N}\}\) and the condensed dataset \(\mathbf{G}^{\prime}=\{\mathcal{G}^{\prime}\}=\{\mathbf{X}\in\mathbb{R}^{N^{ \prime}\times d^{\prime}},\mathbf{A}\in\mathbb{R}^{N^{\prime}\times N^{\prime}}\}\), where \(N^{\prime}\ll N\). For Graph-level condensation, the original dataset \(\mathbf{G}=\{\mathcal{G}_{1},\mathcal{G}_{2},\cdots,\mathcal{G}_{n}\}\) and the condensed dataset \(\mathbf{G}^{\prime}=\{\mathcal{G}^{\prime}_{1},\mathcal{G}^{\prime}_{2},\cdots,\mathcal{G}^{\prime}_{n^{\prime}}\}\), where \(n^{\prime}\ll n\). The condensation ratio \(r\) can be calculated by condensed dataset size / whole dataset size.

**Research Gap.** Although several studies aim to comprehensively discuss existing GC methods [39; 10], they either overlook graph-specific properties or lack systematic experimentation. This discrepancy highlights a significant gap in the literature, partly due to limitations in datasets and evaluation dimensions. A concurrent work [23] analyzed the performance of node-level GC methods but it included only a subset of representative methods, lacking an analysis of graph-level methods, a deep structural analysis, and an assessment of the generalizability of the methods. To bridge this gap, we introduce GC-Bench, an open and unified benchmark to systematically evaluate existing graph condensation methods focusing on the following aspects: **Effectiveness:** the progress in GC, and the impact of structure and initialization on GC; **Transferability:** the transferability of GC methods across backbone architectures and downstream tasks; **Efficiency:** the time and space efficiency of GC methods. The contributions of GC-Bench are as follows:

* _Comprehensive benchmark._ GC-Bench systematically integrated 12 representative and competitive GC methods on both node-level and graph-level by unified condensation and evaluation, giving multi-dimensional analysis in terms of effectiveness, transferability, and efficiency.
* _Key findings._ (1) Graph-level GC methods is still far from achieving the goal of lossless compression. A large condensation ratio does not necessarily lead to better performance with current methods. (2) GC methods can retain semantic information from the graph structure in a condensed graph, but there is still significant improvement room in preserving complex structural properties. (3) All condensed datasets struggle to perform well outside the specific tasks they were condensed, leading to limited applicability. (4) Backbone-dependent GC methods embed model-specific information in the condensed datasets, and popular graph transformers are not compatible with current GC methods as backbones. (5) The initialization mechanism affects both the performance and convergence according to the characteristics of the dataset and the GC method. (6) Most GC methods coupled with backbones and whole dataset training have poor time and space efficiency, contradicting the initial purpose of using GC for efficient training.
* _Open-sourced benchmark library and future directions._ GC-Bench is open-sourced and easy to extend to new methods and datasets, which can help identify directions for further exploration and facilitate future endeavors.

## 2 Overview of GC-Bench

We introduce **G**raph **C**ondensation **B**enchmark (**GC-Bench**) in terms of datasets (\(\rhd\) Section 2.1), algorithms (\(\rhd\) Section 2.2), research questions that guide our benchmark design (\(\rhd\) Section 2.3) and the comparison with related benchmarks (\(\rhd\) Section 2.4). The overview of GC-Bench is shown in Table 1. More details can be found in the Appendix provided in the **Supplementary Material**.

Figure 1: The GC methods can be broadly divided into two categories: The first category depends on the backbone model, refining the condensed graph by aligning it with the backboneâ€™s _gradients_ (\(a\)), _trajectories_ (\(b\)), and output _distributions_ (\(c\)) trained on both original and condensed graphs. The second category, independent of the backbone, optimizes the graph by matching its distribution with that of the original graph data (\(d\)) or by identifying frequently co-occurring computation trees (\(e\)).

### Benchmark Datasets

Regarding evaluation datasets, we adapt the 12 widely-used datasets in the current literature. The node level dataset include 5 homogeneous dataset (_Cora_[16], _Citeseer_[16], _ogbn-arxiv_[13], _Flickr_[43], _Reddit_[12]) and 2 heterogeneous datasets (_ACM_[46] and _DBLP_[7]). The graph-level dataset include _NC1I_[32], _DD_[4], _ogbg-molbbe_[13], _ogbg-molbbbp_[13], _ogbg-molhiv_[13]. We leverage the public train, valid, and test split of these datasets. We report the dataset statistics in Appendix A.1.

### Benchmark Algorithms

We selected 12 representative and competitive GC methods across 6 categories for evaluation. The main ideas of these methods are shown in Figure 1. The evaluated methods include: (1) traditional core-set methods including _Random_, _Herding_[36], _K-Center_[30], (2) gradient matching methods including _DosCond_[14], _Gcond_[15] and _SGDD_[42], (3) trajectory matching methods including _SFGC_[47] and _GEOM_[45], (4) distribution matching methods including GCDM [20] and DM [22; 24], (5) Kernel Ridge Regression (KRR) based method _KiDD_[41], and (6) Computation Tree Compression (CTC) method _Mirage_[11]. The details of evaluated methods are in Appendix A.2.

### Research Questions

We systematically design the GC-Bench to comprehensively evaluate the existing GC algorithms and inspire future research. In particular, we aim to investigate the following research questiones.

**RQ1: How much progress has been made by existing GC methods?**

**Motivation and Experiment Design.** Previous GC methods always adopt different experimental settings, making it difficult to compare them fairly. Given the unified settings of GC-Bench, the first question is to revisit the progress made by existing GC methods and provide potential enhancement directions. A good GC method is expected to perform well consistently under different datasets and different condensation ratios. To answer this question, we evaluate GC methods' performance on 7 node-level datasets and 5 graph-level datasets with a broader range of condensation ratio \(r\) than previous works. The results are shown in Sec. 3.1 and Appendix B.1.

**RQ2: How do the potential flaws of the structure affect the graph condensation performance?**

**Motivation and Experiment Design.** Most of the current GC methods borrow the idea of image condensation and overlook the specific non-IID properties of irregular graph data. The impact of structural properties in GC is still not thoroughly explored. On one hand, the structure itself possesses various characteristics such as homogeneity and heterogeneity, as well as homophily and heterophily. It remains unclear whether these properties should be preserved in GC and how to preserve them. On the other hand, some structure-free GC methods [47] suggest that the condensed dataset may not

\begin{table}
\begin{tabular}{l l} \hline \multicolumn{3}{c}{_Methods_} \\ \hline Traditional core-set methods & _Random_, _Herding_[36], _K-Center_[30] \\ \hline Gradient matching & _Gcond_[15], _DosCond_[14], _SGDD_[42] \\ \hline Trajectory matching & _SFGC_[47], _GEOM_[45] \\ \hline Distribution matching & _GCDM_[20], _DM_[24; 22] \\ \hline Kernel Ridge Regression & _KiDD_[41] \\ \hline Computation Tree Compression & _Mirage_[11] \\ \hline \multicolumn{3}{c}{_Datasets_} \\ \hline Homogeneous datasets & _Cora_[16], _Citeseer_[16], _ogbn-arxiv_[13], _Flickr_[43], _Reddit_[12] \\ \hline Heterogeneous datasets & _ACM_[46], _DBLP_[7] \\ \hline Graph-level datasets & _NC1I_[32], _DD_[4], _ogbg-molbbe_[13], _ogbg-molbbbp_[13], _ogbg-molhiv_[13] \\ \hline \multicolumn{3}{c}{_Downstream Tasks_} \\ \hline Node-level task & Node classification, Link prediction, Anomaly detection \\ \hline Graph-level task & Graph classification \\ \hline \multicolumn{3}{c}{_Evaluations_} \\ \hline Effectiveness & Performance under different condensation ratios, Impact of structural properties, Impact of initialization mechanism \\ \hline Transferability & Different downstream tasks, Different backbone model architectures \\ \hline Efficiency & Time and memory consumption \\ \hline \end{tabular}
\end{table}
Table 1: An overview of GC-Benchneed to explicitly preserve graph structure, and preserving structural information in the generated samples is sufficient. To answer this question, we evaluated GC methods on both homogeneous and heterogeneous as well as homophilic and heterophilic datasets to further explore the impact of structural properties. The results are shown in Sec. 3.2 and Appendix B.2.

**RQ3: Can the condensed graphs be transferred to different types of tasks?**

**Motivation and Experiment Design.** Most existing GC methods are primarily designed for node classification and graph classification tasks. However, there are numerous downstream tasks on graph data, such as link prediction and anomaly detection, which focus on different aspects of graph data. The transferability of GC across various graph tasks has yet to be thoroughly explored. To answer this question, we perform condensation guided by node classification and use the condensed dataset to train models for 3 classic downstream tasks: _link prediction_, _node clustering_, and _anomaly detection_. The evaluation results are shown in Sec. 3.3 and Appendix B.3.

**RQ4: How does the backbone model architecture used for condensation affect the performance?**

**Motivation and Experiment Design.** The backbone model plays an important role in extracting the critical features of the original dataset and guiding the optimization process of generating the condensed dataset. Most GC methods choose a specific graph neural network (GNN) as the backbone. The impact of the backbone model architecture and its transferability is under-explored. A high-quality condensed dataset is expected to be used for training models with not only the specific one used for condensation but also various architectures. To answer this question, we evaluate the transferability performance for 5 representative GNN models (_SGC_[37], _GCN_[16], _GraphSAGE_[12], _APPNP_[18], and _ChebyNet_[2]) and 2 non-GNN models (the popular _Graph Transformer_[31]) and simple _MLP_). We also investigated the performance variation of different backbones with the number of training steps. The evaluation results are shown in Sec. 3.4 and Appendix B.4.

**RQ5: How does the initialization mechanism affect the performance of graph condensation?**

**Motivation and Experiment Design.** The initialization mechanism of the condensed dataset is crucial for convergence and performance in image dataset condensation but remains unexplored for irregular graph data. To answer this question, we adopt 5 distinct initialization strategies (_Random Noise_, _Random Sample_, _Center_, _K-Center_, and _K-Means_) to evaluate their impact on condensation performance and converge speed. The results are shown in Sec. 3.5 and Appendix B.5.

**RQ6: How efficient are these GC methods in terms of time and space?**

**Motivation and Experiment Design.** As the GC methods aim to achieve comparable performance on the condensed dataset and the original dataset, they always rely on the training process on the original datasets. The efficiency and scalability of GC methods are overlooked by existing methods, which is crucial in practice since the original intent of GC is to reduce computation and storage costs for large graphs. To answer this question, we evaluate the time and memory consumption of these GC methods. Specifically, we record the overall time when achieving the best result, the peak CPU memory, and the peak GPU memory. The results are shown in Sec. 3.6 and Appendix B.6.

### Discussion on Existing Benchmarks

To the best of our knowledge, GC-Bench is the first comprehensive benchmark for both node-level and graph-level graph condensation. There are a few image dataset condensation benchmark works for image classification task [1] and condensation adversarial robustness [38]. A recent work GCondenser [23] evaluates some node-level GC methods for node classification on homogeneous graphs with limited evaluation dimensions in terms of performance and time efficiency. Our GC-Bench analyzes more GC methods on a wider variety of datasets (both homogeneous and heterogeneous) and tasks (node classification, graph classification), encompassing both node-level and graph-level methods. In addition to performance and efficiency analysis, we further explore the transferability across different tasks (link prediction, node clustering, anomaly detection) and backbones. With GC-Bench covering more in-depth investigation over a wider scope, we believe it will provide valuable insights into existing works and future directions. A comprehensive comparison with GCondenser can be found in Appendix A.5.

[MISSING_PAGE_FAIL:5]

poorly with _GCN_. This is because _KiDD_ does not rely on the backbone and depends solely on the structure. Consequently, the stronger the downstream model's expressive ability, the better the results.

From both node-level and graph-level results, we observe that as the condensation ratio increases, traditional core-set methods improve, narrowing the performance gap with deep methods. However, deep GC methods show a saturation point or even a decline in performance beyond a certain threshold, suggesting that larger condensed data may introduce noise and biases that degrade performance.

**Key Takeaways 1:** Current node-level GC methods can achieve nearly lossless condensation performance. However, there is still a significant gap between graph-level GC and whole dataset training, indicating there is substantial room for improvement.

**Key Takeaways 2:** A large condensation ratio does not necessarily lead to better performance with current methods.

### Structure in Graph Condensation (RQ2)

We analyze the impact of structure in terms of heterogeneity and heterophily. Experimental settings and additional results can be found in Appendix B.2.

(1) _Heterogeneity v.s. Homogeneity._ For the heterogeneous datasets _ACM_ and _DBLP_, we convert the heterogeneous graphs into homogeneous ones for evaluation. From the results in Table 2, we observe that GC methods designed for homogeneous graphs preserve most of the semantic information and perform comparably to models training on the whole dataset.

(2) _Heterophily v.s. Homophily._ From the results of the heterophilous dataset _Flickr_ (with homophily ratio \(\mathcal{H}=0.24\)) in Table 2, we can observe that current GC methods can achieve almost the same accuracy as models training on the whole dataset. However, there is still a significant gap compared to the state-of-the-art results of the model designed for heterophilic graphs.

\begin{table}
\begin{tabular}{c|c c|c c|c|c|c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{\begin{tabular}{c} **Graph** \\ **/Cls** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Ratio(\(r\))** \\ \end{tabular} } & \multicolumn{2}{c|}{**Traditional Core-set methods**} & \multicolumn{2}{c|}{**Gradient**} & \multicolumn{1}{c|}{**KRR**} & \multicolumn{1}{c|}{**CTC**} & \multicolumn{1}{c|}{**Whole**} \\ \cline{5-10}  & & & **Random** & **Herring K-Center** & **DosCond** & **KiDD** & **Mirage** & **Dataset** \\ \hline \multirow{6}{*}{\begin{tabular}{c} _NCI1_ \\ Acc. (\%) \\ \end{tabular} } & 1 & 0.06\% & 50.90\({}_{\pm 0.20}\) & 51.90\({}_{\pm 1.60}\) & 51.90\({}_{\pm 1.60}\) & 49.20\({}_{\pm 1.10}\) & **61.40\({}_{\pm 0.60}\)** & 50.80\({}_{\pm 2.20}\) & \multirow{6}{*}{\(80.0_{\pm 2.0}\)} & \multirow{6}{*}{\(80.0_{\pm 1.8}\)} \\  & 5 & 0.24\% & 52.10\({}_{\pm 1.30}\) & 60.50\({}_{\pm 1.40}\) & 47.00\({}_{\pm 1.10}\) & 51.10\({}_{\pm 0.80}\) & **63.20\({}_{\pm 2.00}\)** & 51.30\({}_{\pm 1.10}\) \\  & 10 & 0.49\% & 55.60\({}_{\pm 1.00}\) & 61.80\({}_{\pm 1.60}\) & 49.40\({}_{\pm 1.50}\) & 50.30\({}_{\pm 1.30}\) & **64.20\({}_{\pm 1.00}\)** & 51.70\({}_{\pm 1.40}\) & \(80.0_{\pm 1.8}\) \\  & 20 & 0.97\% & 58.70\({}_{\pm 1.40}\) & 60.90\({}_{\pm 1.50}\) & 55.20\({}_{\pm 1.60}\) & 50.30\({}_{\pm 1.30}\) & **66.90\({}_{\pm 0.70}\)** & 52.10\({}_{\pm 2.20}\) & \\  & 50 & 2.43\% & 61.10\({}_{\pm 1.20}\) & 50.90\({}_{\pm 1.50}\) & 62.70\({}_{\pm 1.50}\) & 50.30\({}_{\pm 1.30}\) & **65.40\({}_{\pm 0.60}\)** & 52.40\({}_{\pm 2.70}\) & \\ \hline \multirow{6}{*}{\begin{tabular}{c} _DD_ \\ Acc. (\%) \\ \end{tabular} } & 1 & 0.21\% & 49.70\({}_{\pm 1.30}\) & 58.80\({}_{\pm 1.60}\) & 58.80\({}_{\pm 1.60}\) & 46.30\({}_{\pm 8.50}\) & 71.30\({}_{\pm 1.70}\) & **74.00\({}_{\pm 0.00}\)** & \\  & 5 & 1.06\% & 480.80\({}_{\pm 1.40}\) & 58.70\({}_{\pm 1.50}\) & 51.30\({}_{\pm 1.30}\) & 45.70\({}_{\pm 1.50}\) & - & 70.1\({}_{\pm 2.2}\) \\  & 10 & 2.12\% & 63.10\({}_{\pm 1.50}\) & 64.10\({}_{\pm 1.50}\) & 53.40\({}_{\pm 1.30}\) & 46.30\({}_{\pm 8.50}\) & **71.50\({}_{\pm 0.40}\)** & - & 70.1\({}_{\pm 2.2}\) \\  & 20 & 4.25\% & 56.40\({}_{\pm 1.30}\) & 67.00\({}_{\pm 2.00}\) & 58.50\({}_{\pm 5.70}\) & **70.00\({}_{\pm 0.00}\)** & **71.20\({}_{\pm 0.00}\)** & - & \\  & 50 & 10.62\% & 58.90\({}_{\pm 1.40}\) & 58.60\({}_{\pm 1.40}\) & 62.30\({}_{\pm 1.30}\) & 44.00\({}_{\pm 0.70}\) & **71.20\({}_{\pm 0.00}\)** & - & \\ \hline \multirow{6}{*}{
\begin{tabular}{c} _ogbg-moliance_ \\ ROC-AUC \\ \end{tabular} } & 1 & 0.17\% & 0.468\({}_{\pm 0.45}\) & 0.486\({}_{\pm 0.03}\) & 0.486\({}_{\pm 0.03}\) & 0.512\({}_{\pm 0.02}\) & **0.706\({}_{\pm 0.00}\)** & 0.590\({}_{\pm 0.00}\) & \\  & 5 & 0.83\% & 31.21\({}_{\pm 0.19}\) & 0.470\({}_{\pm 0.02}\) & 0.553\({}_{\pm 0.24}\) & 0.555\({}_{\pm 0.09}\) & **0.562\({}_{\pm 0.00}\)** & 0.419\({}_{\pm 0.10}\) & \\  & 10 & 1.65\% & 0.442\({}_{\pm 0.28}\) & 0.532\({}_{\pm 0.01}\) & 0.594\({}_{\pm 0.19}\) & 0.536\({}_{\pm 0.72}\) & **0.590\({}_{\pm 0.94}\)** & 0.419\({}_{\pm 0.10}\) & 0.763\({}_{\pm 0.20}\) \\  & 20 & 3.31\% & 0.510\({}_{\pm 0.20}\) & 0.509\({}_{\pm 0.02}\) & 0.512\({}_{\pm 0.02}\) & 0.484\({}_{\pm 0.08}\) & **0.540\({}_{\pm 0.01}\)** & 0.423\({}_{\pm 0.01}\) & 0.

**Key Takeaways 3:** Existing GC methods primarily address simple graph data. However, the conversion process to specific data types is non-trivial, leaving significant room for improvement in preserving complex structural properties.

### Transferability on Different Tasks (RQ3)

To evaluate the transferability of GC methods, we condense the dataset by node classification (NC) and use the condensed dataset to train models for link prediction (LP), node clustering (NCIu), and anomaly detection (AD) tasks. The results on _Citeseer_ are shown in Figure 2. Settings and additional results can be found in Appendix B.3.

As shown in Figure 2, performance with condensed datasets was significantly lower compared to original datasets in all transferred tasks. This decline may be due to the task-specific nature of the condensation process, which retains only task-relevant information while ignoring other semantically rich details. For instance, AD task prioritizes high-frequency graph signals more than NC and LP tasks, leading to poor performance when transferring condensed datasets from NC to AD tasks. Among the methods, gradient matching methods (_GCond_, _DosCond_, and _SGD_) demonstrated better transferability in downstream tasks. In contrast, while structure-free methods (_SFGC_ and _GEOM_) perform well in node classification (Section 3.1), they show a significant performance gap in AD tasks compared to gradient matching methods.

**Key Takeaways 4**: All condensed datasets struggle to perform well outside the context of the specific tasks for which they were condensed, leading to limited applicability.

### Transferability of Backbone Model Architectures (RQ4)

We adopt one model (_SGC_ or Graph Transformer) as the backbone for condensation and use the various models in downstream tasks evaluation. Details and additional results are in Appendix B.4.

As shown in Figure 3(a) and 3(b), each column shows the generalization performance of a condensed graph generated by different methods for various downstream models. We can observe that datasets condensed with _SGC_ generally maintain performance when transferred across models. However, datasets condensed with _Graph Transformer_ (_GTrans_) consistently underperform across various methods, and other models also exhibit reduced performance when adapted to _Graph Transformer_. Intuitively, _SGC_'s basic neighbor message-passing strategy may overlook global dependencies critical to more complex models, and similarly, complex models may not perform well when adapted to simpler models. As we can observe, _DosCond_ exhibits generally better transferability compared to other gradient-matching methods. Since it can be regarded as the one-step gradient matching variant of _GCond_, we further test the impact of gradient matching steps on transferability (Figure 3(c)). Increasing the number of matching steps was found to correlate with reduced performance across architectures, indicating that extensive gradient matching may encode model-specific biases.

Figure 2: **Cross-task performance on _Citeseer_. For all downstream tasks, the models are trained solely using data of graphs condensed by node classification. For anomaly detection (c, d), structural and contextual anomalies [3] are injected into both the condensed graph and the original graph.**

**Key Takeaways 5:** Current GC methods exhibit significant performance variability when transferred to different backbone architectures. Involving the entire training process potentially may lead to encoding backbone-specific details in the condensed datasets.

**Key Takeaways 6:** Despite their strong performance in general graph learning tasks, transformers surprisingly yield suboptimal results in graph condensation.

### Initialization Impact (RQ5)

We evaluate 5 distinct initialization strategies, namely: _Random Noise_, _Random Sample_, _Center_, _K-Center_, and _K-Means_. The results of _Gcond_ on _Cora_ and _ogbn-arxiv_ are shown in Figure 4. Detailed settings and additional results can be found in Appendix B.5.

As shown in Figure 4(a) and Figure 4(b), the choice of the initialization method can significantly influence the efficiency of the condensation process but with little impact on the final accuracy. For instance, using _Center_ on _Cora_ reduces the average time to reach the same accuracy by approximately 25% compared to _Random Sample_ and 71% compared to _Random Noise_. However, this speed advantage diminishes as the scale of the condensed graph increases. Additionally, different datasets have their preferred initialization methods for optimal performance. For example, _Center_ is generally faster for _Cora_ condensed by _Gcond_ while _K-Means_ performs better on _ogbn-arxiv_.

**Key Takeaways 7:** Different datasets have their preferred initialization methods for optimal performance even for the same GC method.

**Key Takeaways 8:** The initialization mechanism primarily affects the convergence speed with little impact on the final performance. The smaller the condensed graph, the greater the influence of different initialization strategies on the convergence speed.

Figure 4: **The impact of initialization** under different condensation ratios (a, b) and the impact across different datasets _Cora_ (a, b) and _ogbn-arxiv_ (c).

Figure 3: **Cross-architecture performance.** Using _SGC_ and Graph Transformer (_GTrans_) to condense _Cora_ with a 2.6% ratio, we then test the accuracy on various downstream architectures (a, b). Furthermore, we evaluate the influence of gradient matching steps on _Gcond_ (c).

### Efficiency and Scalability (RQ6)

In this subsection, we evaluate the condensation time and memory consumption of GC methods. The results on _ogbn-arxiv_ are shown in Figure 5, where the \(x\)-axis denotes the overall condensation time (min) when achieving the best validation performance, the \(y\)-axis denotes the test accuracy (%), the inner size of the marker represents the peak CPU memory usage (MB), while the outer size represents the peak GPU memory usage (MB). As we can observe, the gradient matching methods have higher time and space consumption compared to other types of methods. However, Table 2 shows that current gradient and distribution matching GC methods may trigger OOM (Out of Memory) errors on large datasets with high condensation ratios, making them unsuitable for large-scale scenarios, which contradicts the goal of applying graph condensation to extremely large graphs. More detailed results in Appendix B.6.

**Key Takeaways 9**: GC methods that rely on backbones and full-scale data training have large time and space consumption.

## 4 Future Directions

Notwithstanding the promising results, there are some directions worthy to explore in the future:

**Theory of optimal condensation.** According to our findings, GC methods are striving to achieve better performance with smaller condensed dataset sizes but it's not necessarily true that larger compressed datasets lead to better results. How to trade off between dataset size, information condensation, and information preservation, and whether there exists a theory of Pareto-optimal condensation in the graph condensation process, are future research directions.

**Condensation for more complex graph data.** Current GC methods are predominantly tailored to the simplest types of graphs, overlooking the diversity of graph structures such as heterogeneous graphs, directed graphs, hypergraphs, signed graphs, dynamic graphs, text-rich graphs, etc. There is a pressing need for research on graph condensation methods that cater to more complex graph data.

**Task-Agnostic graph condensation.** Task-agnostic GC methods could greatly enhance flexibility and utilization in graph data analysis, promoting versatility across various domains. Current methods often depend on downstream labels or task-specific training. Future research should focus on developing task-agnostic, unsupervised, or self-supervised GC methods that preserve crucial structural and semantic information independently of specific tasks or datasets.

**Improving the efficiency and scalability of graph condensation methods.** Efficient and scalable GC methods are crucial yet challenging to design. Most current methods combine condensation with full training, making them resource-heavy and less scalable. Decoupling these processes could significantly enhance GC's efficiency and scalability, broadening its use across various domains.

## 5 Conclusion and Future Works

This paper introduces a comprehensive graph condensation benchmark, GC-Bench, by integrating and comparing 12 methods across 12 datasets covering varying types and scopes. We conduct extensive experiments to reveal the performance of GC methods in terms of effectiveness, transferability, and efficiency. We implement an library (https://github.com/RingBDStack/GC-Bench) that incorporates all the aforementioned protocols, baseline methods, datasets, and scripts to reproduce the results in this paper. The GC-Bench library offers a comprehensive and unbiased platform for evaluating current methods and facilitating future research. In this study, we mainly evaluate the performance of GC methods for the node classification and graph classification task, which is widely adopted in the previous literature. In the future, we plan to extend the GC-Bench with broader coverage of datasets and tasks, providing further exploration of the generalization ability of GC methods. We will update the benchmark regularly to reflect the most recent progress in GC methods.

## Acknowledgements

The corresponding author is Jianxin Li. This work is supported by the NSFC through grants No.62225202 and No.62302023, the Fundamental Research Funds for the Central Universities, CAAI-MindSporc Open Fund, developed on OpenI Community. This work is also supported in part by NSF under grants III-2106758, and POSE-2346158.

## References

* [1] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Dc-bench: Dataset condensation benchmark. In _NeurIPS_, 2022.
* [2] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. _arXiv: Learning,arXiv: Learning_, 2016.
* [3] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. _Deep Anomaly Detection on Attributed Networks_, page 594-602. 2019.
* [4] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. _Journal of molecular biology_, 2003.
* [5] Junfeng Fang, Xinglin Li, Yongduo Sui, Yuan Gao, Guibin Zhang, Kun Wang, Xiang Wang, and Xiangnan He. Exgc: Bridging efficiency and explainability in graph condensation. _arXiv preprint arXiv:2402.05962_, 2024.
* [6] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [7] Tao-yang Fu, Wang-Chien Lee, and Zhen Lei. Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning. In _CIKM_, pages 1797-1806, 2017.
* [8] Jian Gao and Jianshe Wu. Multiple sparse graphs condensation. _Knowledge-Based Systems_, 278:110904, 2023.
* [9] Xinyi Gao, Tong Chen, Yilong Zang, Wentao Zhang, Quoc Viet Hung Nguyen, Kai Zheng, and Hongzhi Yin. Graph condensation for inductive node representation learning. In _ICDE_, 2024.
* [10] Xinyi Gao, Junliang Yu, Wei Jiang, Tong Chen, Wentao Zhang, and Hongzhi Yin. Graph condensation: A survey. _arXiv preprint arXiv:2401.11720_, 2024.
* [11] Mridul Gupta, Sahil Manchanda, Sayan Ranu, and Hariprasad Kodamana. Mirage: Model-agnostic graph distillation for graph classification. In _ICLR_, 2024.
* [12] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NeurIPS_, 2017.
* [13] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _NeurIPS_, 2020.
* [14] Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang, and Bing Yin. Condensing graphs via one-step gradient matching. In _SIGKDD_, 2022.
* [15] Wei Jin, Lingxiao Zhao, Shi-Chang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph condensation for graph neural networks. In _ICLR_, 2021.
* [16] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [17] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [18] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. In _ICLR_, 2018.
* [19] Xinglin Li, Kun Wang, Hanhui Deng, Yuxuan Liang, and Di Wu. Attend who is weak: Enhancing graph condensation via cross-free adversarial training. _arXiv preprint arXiv:2311.15772_, 2023.

* [20] Mengyang Liu, Shanchuan Li, Xinshi Chen, and Le Song. Graph condensation via receptive field distribution matching. _arXiv preprint arXiv:2206.13697_, 2022.
* [21] Yang Liu, Deyu Bo, and Chuan Shi. Graph condensation via eigenbasis matching. In _ICML_, 2024.
* [22] Yilun Liu, Ruihong Qiu, and Zi Huang. Cat: Balanced continual graph learning with graph condensation. In _ICDM_, pages 1157-1162. IEEE, 2023.
* [23] Yilun Liu, Ruihong Qiu, and Zi Huang. Gcondenser: Benchmarking graph condensation. _arXiv preprint arXiv:2405.14246_, 2024.
* [24] Yilun Liu, Ruihong Qiu, Yanran Tang, Hongzhi Yin, and Zi Huang. Puma: Efficient continual graph learning with graph condensation. _arXiv preprint arXiv:2312.14439_, 2023.
* [25] Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, and Jie Tang. Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks. In _SIGKDD_, 2021.
* [26] Runze Mao, Wenqi Fan, and Qing Li. Gcare: Mitigating subgroup unfairness in graph condensation through adversarial regularization. _Applied Sciences_, 13(16):9166, 2023.
* [27] ChristopherJ. Morris, NilsM. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv: Learning_, Jul 2020.
* [28] Qiying Pan, Ruofan Wu, Tengfei Liu, Tianyi Zhang, Yifei Zhu, and Weiqiang Wang. Fedgkd: Unleashing the power of collaboration in federated graph neural networks. _arXiv preprint arXiv:2309.09517_, 2023.
* [29] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. In _ICLR_, 2019.
* [30] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In _ICLR_, 2018.
* [31] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification, 2021.
* [32] Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. _Knowledge and Information Systems_, 2008.
* [33] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. Attributed graph clustering: A deep attentional embedding approach. In _IJCAI_, 2019.
* [34] Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, and Qing Li. Fast graph condensation with structure-based neural tangent kernel. In _The Web Conference_, 2024.
* [35] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In _The Web Conference_, 2019.
* [36] Max Welling. Herding dynamical weights to learn. In _ICML_, pages 1121-1128, 2009.
* [37] Felix Wu, Tianyi Zhang, AmauriH. Souza, Christopher Fifty, Tao Yu, and KilianQ. Weinberger. Simplifying graph convolutional networks. _arXiv: Learning_, 2019.
* [38] Yifan Wu, Jiawei Du, Ping Liu, Yuewei Lin, Wenqing Cheng, and Wei Xu. Dd-robustbench: An adversarial robustness benchmark for dataset distillation. _arXiv preprint arXiv:2403.13322_, 2024.
* [39] Hongjia Xu, Liangliang Zhang, Yao Ma, Sheng Zhou, Zhuonan Zheng, and Bu Jiajun. A survey on graph condensation. _arXiv preprint arXiv:2402.02000_, 2024.
* [40] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2018.
* [41] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. Kernel ridge regression-based graph dataset distillation. In _SIGKDD_, pages 2850-2861, 2023.
* [42] Beining Yang, Kai Wang, Qingyun Sun, Cheng Ji, Xingcheng Fu, Hao Tang, Yang You, and Jianxin Li. Does graph distillation see like vision dataset counterpart? In _NeurIPS_, 2023.

* [43] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In _ICLR_, 2020.
* [44] Tianle Zhang, Yuchen Zhang, Kun Wang, Kai Wang, Beining Yang, Kaipeng Zhang, Wenqi Shao, Ping Liu, Joey Tianyi Zhou, and Yang You. Two trades is not baffled: Condense graph via crafting rational gradient matching. _arXiv preprint arXiv:2402.04924_, 2024.
* [45] Yuchen Zhang, Tianle Zhang, Kai Wang, Ziyao Guo, Yuxuan Liang, Xavier Bresson, Wei Jin, and Yang You. Navigating complexity: Toward lossless graph condensation via expanding window matching. _CoRR_, abs/2402.05011, 2024.
* [46] Jianan Zhao, Xiao Wang, Chuan Shi, Zekuan Liu, and Yanfang Ye. Network schema preserving heterogeneous information network embedding. In _IJCAI_, 2020.
* [47] Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen, Xingquan Zhu, and Shirui Pan. Structure-free graph condensation: From large-scale graphs to condensed graph-free data. In _NeurIPS_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Sec. 5. 3. Did you discuss any potential negative societal impacts of your work? [No] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix C. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix A. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section 3. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A.4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See Section 2.1 and Appendix A.2. 2. Did you mention the license of the assets? [Yes] See Appendix C. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix C. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix C.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [NA] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [NA] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [NA]Details of GC-Bench

### Datasets

The evaluation node-level datasets include 5 homogeneous datasets (3 transductive datasets, i.e., _Cora_, _Citeseer_[16] and _ogbn-arxiv_[13], and 2 inductive datasets, i.e., _Flickr_[43] and _Reddit_[12]) and 2 heterogeneous datasets (_ACM_[46] and _DBLP_[7]). The evaluation graph-level datasets include 5 datasets ( _NC1I_[32], _DD_[4], _ogbg-molbac_[13], _ogbg-molhiv_[13], _ogbg-molbbbp_[13]).

We utilize the standard data splits provided by PyTorch Geometric [6] and the Open Graph Benchmark (OGB) [13] for our experiments. For datasets in TUDataset [27], we split the data into 10% for testing, 10% for validation, and 80% for training. For _ACM_ and _DBLP_ datasets, we follow the settings outlined in [25]. Dataset statistics are shown in Table A1.

### Algorithms

We summarize the current GC algorithms in Table A2. We choose **12** representative ones for evaluation in this paper including _Random_, _K-Center_[30], _Herding_[36], _GCond_[15], _DosCond_[14], _SGDD_[42], _GCDM_[20], _DM_[24], _SFGC_[47], _GEOM_[45], _KiDD_[41], _Mirage_[11]. **We will continue to update and improve the benchmark to include more algorithms.** Here we introduce the GC algorithms in detail:

* **Traditional Core-set Methods*
* **Random**: For node classification tasks, nodes are randomly selected to form a new subgraph. For graph classification, the graphs are randomly selected to create a new subset.
* **Herding**[36]: The nodes or graphs are selected samples that are closest to the cluster center.
* **K-Center**[30]: Nodes or graphs are chosen such that they have the minimal distance to the nearest cluster center, which is generated using the K-Means Clustering method.
* **Gradient Matching Methods*
* **GCond**[15]: In Gcond, the optimization of the synthetic dataset is framed as a bi-level problem. It adapts a gradient matching scheme to match the gradients of GNN parameters between the condensed and original graphs, while optimizing the model's performance on the datasets. For generating the synthetic adjacency matrix, Gcond employs a Multi-Layer Perceptron (MLP) to model the edges by using node features as input, maintaining the correlations between node features and graph structures.
* **DosCond**[14]: In DosCond, the gradient matching scheme only matches the network gradients for model initialization \(\theta_{0}\) while discarding the training trajectory of \(\theta_{t}\), which accelerated the entire condensation process by only informing the direction to update the
synthetic dataset. DosCond also modeled the discrete graph structure as a probabilistic model and each element in the adjacency matrix follows a Bernoulli distribution. * **MSGC**[8]: MSGC condenses a large-scale graph into multiple small-scale sparse graphs, leveraging neighborhood patterns as substructures to enable the construction of various connection schemes. This process enriches the diversity of embeddings generated by GNNs, enhances the representation power of GNNs con complex graphs. * **SGDD**[42]: SGDD uses graphon approximation to ensure that the structural information of the original graph is retained in the synthetic, condensed graph. The condensed graph structure is optimized by minimizing the optimal transport (OT) distance between the original structure and the condensed structure. * **GCARe**[26]: GCARe addresses biases in condensed graphs by regularizing the condensation process, ensuring that the knowledge of different subgroups is distilled fairly into the resulting graphs. * **CTRL**[44]: CTRL clusters each class of the original graph into sub-clusters and uses these as initial value for the synthetic graph. By considering both the direction and magnitude of gradients during gradient matching, it effectively minimizes matching errors during the condensation phase. * **GroC**[19]: GroC uses an adversarial training (bi-level optimization) framework to explore the most impactful parameter spaces and employs a Shock Absorber operator to apply targeted adversarial perturbation. * **EXGC**[5]: EXGC leverages Mean-Field variational approximation to address inefficiency in the current gradient matching schemes and uses the Gradient Information Bottleneck objective to tackle node redundancy. * **Mcond**[9]: MCond addresses the limitations of traditional condensed graphs in handling unseen data by learning a one-to-many node mapping from original nodes to synthetic nodes and uses an alternating optimization scheme to enhance the learning of synthetic graph and mapping matrix.
* **Distribution Matching Methods**

**GCDM**[20]: GCDM synthesizes small graphs with receptive fields that share a similar distribution to the original graph, achieved through a distribution matching loss quantified by maximum mean discrepancy (MMD).
* **DM**[22; 24]: DM can be regarded as a one-step variant of GCDM. In DM, the optimization is centered on the initial parameters. Notably, in [22] and [24], DM does not learn any structures for efficiency. However, for better comparison in our experiments, we continue to learn the adjacency matrix.
* **FedGKD**[28]: FedGKD trains models on condensed local graphs within each client to mitigate the potential leakage of the training set membership. FedGKD features a Federated Graph Neural Network framework that enhances client collaboration using a task feature extractor for graph data distillation and a task relator for globally-aware model aggregation.
* **Trajectory Matching Methods*
* **SFGC**[47]: SFGC uses trajectory matching instead of a gradient matching scheme. It first trains a set of GNNs on original graphs to acquire and store an expert parameter distribution offline. The expert trajectory guides the optimization of the condensed graph-free data. The generated graphs are evaluated using closed-form solutions of GNNs under the graph neural tangent kernel (GNTK) ridge regression, avoiding iterative GNN training.
* **GEOM**[45]: GEOM makes the first attempt toward lossless graph condensation using curriculum-based trajectory matching. A homophily-based difficulty score is assigned to each node and the easy nodes are learned in the early stages while more difficult ones are learned in the later stages. On top of that, GEOM incorporated a Knowledge Embedding Extraction (KEE) loss into a matching loss.
* **Kernel Ridge Regression Methods*
* **GC-SNTK**[34]: GC-SNTK introduces a Structure-based Neural Tangent Kernel(SNTK) to capture graph topology, replacing the inner GNNs training in traditional GC paradigm, avoiding multiple iterations.
* **KiDD**[41]: KiDD uses kernel ridge regression (KRR) with a graph neural tangent kernel (GNTK) for graph-level tasks. To enhance efficiency, KiDD introduces LiteGNTK, a simplified GNTK, and proposes KiDD-LR for faster low-rank approximation and KiDD-D for handling discrete graph topology using the Gumbel-Max reparameterization trick. We use KiDD-LR for experiments as it has generally demonstrated better performance compared to KiDD-D.
* **Computation Tree Compression Methods*
* **Mirage**[11]: Mirage decomposes graphs in datasets into a collection of computation trees and then mines frequently co-occurring trees from this set. Mirage then uses aggregation functions (MEANPOOL, SUMPOOL, etc.) on the embeddings of the root node of each tree to approximate the graph embedding.

### Hyper-Parameter Setting

For the implementation of various graph condensation methods, we adhere to the default parameters as specified by the authors in their respective original implementations. This approach ensures that our results are comparable to those reported in the foundational studies. For condensation ratios that were not explored in the original publications, we employ a grid search strategy to identify the optimal hyperparameters within the predefined search space. This includes experimenting with various combinations, such as differing learning rates for the feature optimizer and the adjacency matrix optimizer. The corresponding hyperparameter space are shown in Table A3.

### Computation resources

All experiments were conducted on a high-performance GPU cluster to ensure a fair comparison. The cluster consists of 32 identical dell-GPU nodes, each featuring 256GB of memory, 2 Intel Xeon processors, and 4 NVIDIA Tesla V100 GPUs, with each GPU having 64 GB of GPU memory. If any experiment setting exceeds the GPU memory limit, it is reported as out-of-memory (OOM).

### Discussion on Existing Benchmarks

To the best of our knowledge, the only concurrent work is GCondenser [23]. The comparison of GCondser and our GC-Bench are list in Table A4. GCondenser [23] focus the node-level GC methods for node classification on homogeneous graphs with limited evaluation dimensions in terms of performance and time efficiency. Our GC-Bench analyzes more GC methods on a wider variety of datasets (both homogeneous and heterogeneous) and tasks (node classification, graph classification), encompassing both node-level and graph-level methods. In addition to performance and efficiency analysis, we further explore the transferability across different tasks (link prediction, node clustering, anomaly detection) and backbones (GNN models and the popular Graph Transformer). With GC-Bench covering more in-depth investigation over a wider scope, we believe it will provide valuable insights into existing works and future directions.

## Appendix B Settings and Additional Results

In this section, we provide more details of the experimental settings and the additional results for the proposed 6 research questions, respectively.

### Settings and Additional Results of Performance Comparison (RQ1)

#### b.1.1 Comparison Setting

Node Classification Graph Dataset Setting.We compared ten state-of-the-art GC methods. The selection of the condensation ratio \(r\) is based on the labeling rates of different datasets. For datasets like _Cora_ and _Citeseer_, the labeling rates are less than 50%, we select \(r\) as a proportion of the labeling rate, specifically at \(\{5\%,10\%,25\%,50\%,75\%,100\%\}\). For datasets like _ogbn-arxiv_, and inductive datasets where all nodes in the training graphs are labeled, with a relatively higher labeling rate, \(r\) is chosen to be \(\{5\%,10\%,25\%,50\%,75\%,100\%\}\). Corresponding condensation rates are shown in Table B2.

Graph Classification Graph Dataset Setting.We compared three state-of-the-art GC algorithms on graph classification datasets: _DosCond_[14], _KiDD_[41], and _Mirage_[11]. _Mirage_[11] does not condense datasets into unified graphs measurable by Graphs per Class(GPC) as _DosCond_[14] and_KiDD_[41] do. Therefore, we measure the condensed dataset size by storing its elements in.pt format, similar to _DosCond_[14] and _KiDD_[41]. We select the Mirage-condensed dataset size closest to _DosCond_'s as the corresponding GPC. _KiDD_[41] generally occupies more disk space than DosCond under the same GPC. The size of _Mirage_ datasets is determined by two parameters: the number of GNN layers (\(L\)) and the frequency threshold \(\Theta\). We fix \(L=2\), consistent with the 2-layer model used for validation, and employ a grid search strategy to identify the threshold combination that yields a dataset size closest to the targeted GPC. The corresponding disk space, GPC, and threshold choices are presented in Table B1. Note that for small thresholds, the MP Tree search algorithms used in _Mirage_[11] may reach recursive limits. Consequently, in _DD_ and _ogbg-molbace_, certain GPCs lack corresponding threshold values.

Heterogeneous Graph Dataset Setting.Due to the absence of condensation methods specifically for heterogeneous graphs, we convert heterogeneous datasets into homogeneous graphs for condensation, focusing on target nodes. We uniformly summed the adjacency matrices corresponding to various meta-paths as in [25], and applied zero-padding to match the maximum feature dimension as well as one-hot encoding for nodes without features. Specifically, in _GEOM_[45], when calculating heterophily, all nodes without labels (non-target nodes) are assigned the same distinct label, ensuring a consistent heterophily calculation.

#### b.1.2 Additional Results

The graph classification performance on GCN is shown in Table B3. _DosCond_[14] with GCN demonstrates significant advantages in 12 out of 25 cases, while _KiDD_[41] underperforms in most scenarios. Notably, _DosCond_[14] and _Mirage_[11] even outperform the results of the whole dataseton _ogbg-molbace_. For _Mirage_[11], due to the algorithm's recursive depth under low threshold parameters, we have only one result corresponding to GPC 1 on _DD_. However, this single result already surpasses all datasets condensed by _KiDD_[41] and the dataset with GPC 1 condensed by _DosCond_.

### Settings and Additional Results of Structure in Graph Condensation (RQ2)

#### b.2.1 Experimental Settings

The homophily ratio we use is the edge homophily ratio, which represents the fraction of edges that connect nodes with the same labels. It can be calculated as:

\[\mathcal{H}(G)=\frac{1}{|\mathcal{E}|}\sum_{(j,k)\in\mathcal{E}}\mathbf{1}(y_ {j}=y_{k}),\;\;i\in\mathcal{V},\] (A.1)

where \(\mathcal{V}\) is the node set, \(\mathcal{E}\) is the edge set, \(|\mathcal{E}|\) is the number of edges in the graph, \(y_{i}\) is the label of node \(i\) and \(\mathbf{1}(\cdot)\) is the indicator function. A graph is typically considered to be highly homophilous when \(\mathcal{H}\) is large (typically, 0.5 \(\leq\mathcal{H}\leq 1\) ), such as _Cora_ and _Reddit_. Conversely, a graph with a low edge homophily ratio is considered to be heterophilous, such as _Flickr_.

We also calculate the homophily ratio of condensed datasets. Since the condensed datasets have weighted edges, we first sparsify the graph by removing all edges with weights less than 0.05, then calculate the homophily ratio by adjusting the fraction to a weighted fraction, which can be represented as:

\[\mathcal{H}(G)=\frac{\sum_{(j,k)\in\mathcal{E}}w_{jk}\mathbf{1}(y_{j}=y_{k})}{ \sum_{(j,k)\in\mathcal{E}}w_{jk}},\;\;i\in\mathcal{V},\] (A.2)

where \(w_{jk}\) is the weight of the edge between nodes \(j\) and \(k\).

#### b.2.2 Additional Results

The results of homophily ratios of condensed datasets are shown in Table B4. It appears that condensed datasets often struggle to preserve the homophily properties of the original datasets. For instance, in the case of the heterophilous dataset _Flickr_, an increase in the homophily rate is observed under most methods and ratios.

\begin{table}
\begin{tabular}{l|c c c c c|c c c|c} \hline \hline \multirow{2}{*}{**Dataset**} & \begin{tabular}{c} **Graph** \\ **/Cls** \\ \end{tabular} & \begin{tabular}{c} **Ratio(\(r\))** \\ \end{tabular} & \begin{tabular}{c} **Traditional Core-set methods** \\ \end{tabular} & \begin{tabular}{c} **Gradient** \\ \end{tabular} & \begin{tabular}{c} **KRR** \\ \end{tabular} & \begin{tabular}{c} **CTC** \\ \end{tabular} & 
\begin{tabular}{c} **Whole** \\ **Dataset** \\ \end{tabular} \\ \hline \multirow{6}{*}{_NCI1_} & 1 & 0.06\% & 53.0\({}_{\pm 0.6}\) & 55.20\({}_{\pm 2.6}\) & 55.20\({}_{\pm 2.6}\) & **57.30\({}_{\pm 0.9}\)** & 49.30\({}_{\pm 1.1}\) & 49.10\({}_{\pm 0.9}\) & \multirow{6}{*}{\(71.1_{\pm 0.8}\)} \\  & 5 & 0.24\% & 55.00\({}_{\pm 1.4}\) & 56.50\({}_{\pm 0.9}\) & 53.20\({}_{\pm 0.6}\) & **58.40\({}_{\pm 1.6}\)** & 56.10\({}_{\pm 0.1}\) & 49.60\({}_{\pm 2.2}\) & \\  & 10 & 0.49\% & 58.10\({}_{\pm 2.2}\) & **58.60\({}_{\pm 0.08}\)** & 57.00\({}_{\pm 2.5}\) & 57.80\({}_{\pm 1.6}\) & 57.50\({}_{\pm 1.1}\) & 48.60\({}_{\pm 0.1}\) & 71.12\({}_{\pm 0.8}\) \\  & 20 & 0.97\% & 54.40\({}_{\pm 0.8}\) & 59.10\({}_{\pm 1.1}\) & **60.10\({}_{\pm 0.8}\)** & **60.10\({}_{\pm 3.2}\)** & 56.40\({}_{\pm 0.6}\) & 48.70\({}_{\pm 0.0}\) & \\  & 50 & 2.43\% & 56.80\({}_{\pm 1.1}\) & 58.70\({}_{\pm 1.1}\) & **64.00\({}_{\pm 0.9}\)** & 58.20\({}_{\pm 2.8}\) & **59.90\({}_{\pm 0.0}\)** & 48.60\({}_{\pm 0.1}\) & \\ \hline \multirow{6}{*}{_DD_} & 1 & 0.21\% & 59.70\({}_{\pm 1.5}\) & 66.90\({}_{\pm 2.8}\) & 66.90\({}_{\pm 2.8}\) & 68.30\({}_{\pm 3.6}\) & 58.30\({}_{\pm 6.4}\) & **71.20\({}_{\pm 6.6}\)** \\  & 5 & 10.6\% & 61.90\({}_{\pm 1.6}\) & 66.20\({}_{\pm 2.5}\) & 62.00\({}_{\pm 1.7}\) & **73.10\({}_{\pm 2.2}\)** & 58.60\({}_{\pm 1.1}\) & 58.60\({}_{\pm 1.1}\) & - & 78.4\({}_{\pm 1.7}\) \\ \cline{1-1}  & 10 & 2.12\% & 63.70\({}_{\pm 2.8}\) & 68.00\({}_{\pm 3.6}\) & 62.50\({}_{\pm 2.3}\) & **71.30\({}_{\pm 8.3}\)** & 61.60\({}_{\pm 3.8}\) & - & 78.4\({}_{\pm 1.7}\) \\ \cline{1-1}  & 20 & 4.25\% & 64.70\({}_{\pm 5.3}\) & 69.20\({}_{\pm 0.8}\) & 63.10\({}_{\pm 1.9}\) & **73.00\({}_{\pm 8.5}\)** & 62.01\({}_{\pm 1.4}\) & - & - \\ \cline{1-1}  & 50 & 10.62\% & 66.60\({}_{\pm 2.0}\) & 68.50\({}_{\pm 1.4}\) & **68.50\({}_{\pm 1.8}\)** & **74.20\({}_{\pm 2.5}\)** & 59.30\({}_{\pm 0.0}\) & - & - \\ \hline \multirow{6}{*}{_opbg-mollance_} & 1 & 0.17\% & 0.510\({}_{\pm 0.03}\) & 0.515\({}_{\pm 0.01}\) & 0.517\({}_{\pm 0.04}\) & 0.658\({}_{\pm 0.04}\) & 0.568\({}_{\pm 0.04}\) & 0.568\({}_{\pm 0.04}\) & **0.

[MISSING_PAGE_EMPTY:21]

original graph. Since trajectory matching methods do not generate any edges, we do not use them for link prediction tasks. The results of condensed datasets on the link prediction task are shown in Table B5. We observe that most condensed datasets underperform in link prediction tasks, especially on _ogbn-arxiv_ and _Flickr_. Most methods' condensed datasets consistently underperform compared to traditional core-set methods, indicating room for improvement.

#### b.3.2 Node Clustering

For the node clustering tasks on condensed datasets, we utilize DAEGC [33] to train on synthetic datasets condensed using the node classification task. We then test the trained model on the original large-scale datasets and include the results of other methods on the original graph for comprehensive comparison. Due to the performance degradation of GAT with large neighborhood sizes, we use GCN as the encoder.Performance metrics include Accuracy (Acc.), Normalized Mutual Information (NMI), F-score, and Adjusted Rand Index (ARI).

To fully leverage the condensed datasets, we include the results of node clustering with pertaining. In this experiment, the GCN encoder is first trained on the synthetic datasets with a node classification task, which incorporates the synthetic labels' information. Using the pre-trained GCN as an encoder, we then perform node clustering on the synthetic datasets and the original graph. Results of node clustering tasks, both without and with pertaining are shown in Table B6 and Table B7 respectively.

Figure B.4: Degree distribution in the condensed graphs for _Citeseer_ (1.80%), _Cora_ (2.60%), and _Flickr_ (0.05%). The first, second, and third columns correspond to _Citeseer_, _Cora_, and _Flickr_, respectively.

We observe that most condensed datasets perform worse in the node clustering task compared to the original dataset. However, when additional information from the pretraining of the node classification task on condensed dataset is utilized, the results of node clustering significantly improve. Notably, some datasets in Table B6 exhibit identical results with the Adjusted Rand Index (ARI) being 0 or even negative. This occurs because the clustering results do not match the number of classes in the labels, requiring manual splitting of clusters in such scenarios. An ARI of 0 indicates that the clustering result is as good as random, while a negative ARI suggests it is worse than random.

#### b.3.3 Anomaly Detection

For the anomaly detection tasks, we generate two types of anomalies, _Contextual Anomalies_ and _Structural Anomalies_, following the method described in [3]. We set the anomaly rate to 0.05; if the condensed dataset is too small, we inject one contextual anomaly and two structural anomalies.

**Contextual Anomalies**: Each outlier is generated by randomly selecting a node and substituting its attributes with those from another node with the maximum Euclidean distance in attribute space.

**Structural Anomalies**: Outliers are generated by randomly selecting a small group of nodes and making them fully connected, forming a clique. The nodes in this clique are then regarded as structural outliers. This process is repeated iteratively until a predefined number of cliques are generated.

We conduct anomaly detection by training a DOMINANT model [3], which features a shared graph convolutional encoder, a structure reconstruction decoder, and an attribute reconstruction decoder. Initially, we inject predefined anomalies into the test set of the original graph and use it for evaluation across different condensed datasets derived from this graph. The model is then trained on these condensed datasets, which were injected with specific types of anomalies before training. The DOMINANT model measures reconstruction errors as anomaly scores for both the graph structure and node attributes, combining these scores to detect anomalies. The results are evaluated using the ROC-AUC metric, as shown in Table B8 and B9.

### Settings and Additional Results of Transferability across Backbone Model Architectures (RQ4)

#### b.4.1 Experimental Settings

For transferability evaluation, we use different models as backbones to test the condensation methods. For distribution matching methods, two backbone models with shared parameters are used to generate embeddings that are matched. For trajectory matching methods, two backbone models are used to generate expert trajectories and student trajectories, respectively, to match the corresponding parameters. For gradient matching methods, two backbone models with shared parameters are used to generate gradients for real and synthetic data. Models are selected using grid-searched hyperparameters. The details of the backbone architecture are as follows:* **MLP:** MLP is a simple neural network consisting of fully connected layers. The MLP we use is structured similarly to a GCN but without the adjacency matrix input, effectively functioning as a standard multi-layer perceptron (MLP). The MLP we adopted consists of 2 layers with 256 hidden units in each layer.
* **GCN [16]:** GCN is the most common architecture for evaluating condensed datasets in mainstream GC methods. GCN defines a localized, first-order approximation of spectral graph convolutions, effectively aggregating and combining features from a node's local neighborhood, leveraging the graph's adjacency matrix to update node representations through multiple layers. We adhere to the setting in previous work [15] and use 2 graph convolutional layers for node classification, each followed by ReLu activation and batch normalization depending on the configuration. For graph classification, we use a 3-layer GCN with a sum pooling function. The hidden unit size is set to 256.
* **SGC [37]:** SGC is the standardized model used for condensation in previous works. It can be regarded as a simplified version of GCN, which ignores the nonlinear activation function but still keeps two Graph Convolution layers, thereby preserving similar graph filtering behaviors. In the experiments, we use 2-layer SGC with no bias.

\begin{table}
\begin{tabular}{l c c c c c|c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multirow{2}{*}{**Ratio (\(r\))**} & \multicolumn{4}{c|}{_Citeseer_} & \multicolumn{4}{c}{_Cora_} \\ \cline{3-10}  & & **Acc.** & **NMI** & **ARI** & **F1** & **Ratio(r)** & **Acc.** & **NMI** & **ARI** & **F1** \\ \hline K-means & & 54.4 & 31.2 & 28.5 & 41.3 & & & 50.0 & 31.7 & 37.6 & 23.9 \\ \hline DAEGC [33] & Full & **67.2** & **39.7** & **41.0** & **63.6** & Full & **70.4** & **52.8** & **68.2** & 49.6 \\ \hline \multirow{3}{*}{Random} & 0.90\% & 40.6 & 19.1 & 17.5 & 36.0 & 1.30\% & 36.6 & 13.5 & 9.0 & 34.3 \\  & 1.80\% & 38.3 & 14.8 & 13.6 & 34.5 & 2.60\% & 33.5 & 13.9 & 7.1 & 33.4 \\  & 3.60\% & 41.8 & 18.1 & 16.9 & 39.4 & 5.20\% & 30.2 & 0.4 & 0.0 & 6.8 \\ \hline \multirow{3}{*}{Herding} & 0.90\% & 41.9 & 16.9 & 15.3 & 40.0 & 1.30\% & 37.4 & 18.2 & 11.7 & 35.0 \\  & 1.80\% & 44.9 & 18.7 & 16.0 & 41.1 & 2.60\% & 36.6 & 16.4 & 11.9 & 34.0 \\  & 3.60\% & 58.1 & 27.8 & 29.2 & 52.3 & 5.20\% & 26.7 & 13.7 & 2.9 & 20.6 \\ \hline \multirow{3}{*}{K-Center} & 0.90\% & 37.9 & 13.4 & 11.1 & 35.2 & 1.30\% & 34.3 & 13.5 & 7.8 & 32.4 \\  & 1.80\% & 50.0 & 23.5 & 22.9 & 46.5 & 2.60\% & 42.5 & 22.3 & 15.0 & 42.3 \\  & 3.60\% & 31.9 & 14.0 & 10.2 & 31.0 & 5.20\% & 30.2 & 0.4 & 0.0 & 6.8 \\ \hline \multirow{3}{*}{GCDM} & 0.90\% & 41.4 & 16.9 & 16.2 & 38.6 & 1.30\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 1.80\% & 44.1 & 18.1 & 18.1 & 38.8 & 2.60\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 3.60\% & 22.8 & 1.8 & 1.2 & 20.9 & 5.20\% & 30.2 & 0.4 & 0.0 & 6.8 \\ \hline \multirow{3}{*}{DM} & 0.90\% & 23.5 & 2.1 & 1.1 & 17.7 & 1.30\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 1.80\% & 45.3 & 19.1 & 17.7 & 42.9 & 2.60\% & 29.2 & 2.0 & 0.0 & 9.5 \\  & 3.60\% & 25.9 & 4.5 & 3.5 & 20.0 & 5.20\% & 30.2 & 0.4 & 0.0 & 6.8 \\ \hline \multirow{3}{*}{DosCond} & 0.90\% & 28.6 & 10.2 & 6.3 & 25.1 & 1.30\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 1.80\% & 57.1 & 31.4 & 26.2 & 49.5 & 2.60\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 3.60\% & 44.3 & 20.6 & 17.0 & 38.6 & 5.20\% & 29.6 & 16.2 & 7.7 & 23.4 \\ \hline \multirow{3}{*}{Gcond} & 0.90\% & 61.8 & 34.0 & 34.7 & 55.9 & 1.30\% & 46.6 & 36.7 & 27.3 & 41.2 \\  & 1.80\% & 59.6 & 33.0 & 32.6 & 50.3 & 2.60\% & 49.9 & 39.3 & 27.9 & 44.3 \\  & 3.60\% & 57.8 & 32.0 & 30.2 & 54.8 & 5.20\% & 44.6 & 40.9 & 25.1 & 37.3 \\ \hline \multirow{3}{*}{SGDD} & 0.90\% & 56.5 & 27.3 & 26.8 & 50.6 & 1.30\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 1.80\% & 45.4 & 24.0 & 20.0 & 43.9 & 2.60\% & 30.2 & 0.4 & 0.0 & 6.8 \\  & 3.60\% & 42.5 & 23.6 & 20.8 & 38.2 & 5.20\% & 33.2 & 17.9 & 8.8 & 25.5 \\ \hline \multirow{3}{*}{SFGC} & 0.90\% & 46.7 & 19.9 & 18.8 & 43.4 & 1.30\% & 42.1 & 23.5 & 17.7 & 39.2 \\  & 1.80\% & 56.8 & 27.4 & 27.6 & 52.8 & 2.60\% & 54.4 & 31.8 & 26.4 & **50.2** \\  & 3.60\% & 47.7 & 19.0 & 16.9 & 45.3 & 5.20\% & 30.1 & 0.4 & -0.1 & 6.8 \\ \hline \multirow{3}{*}{GEOM} & 0.90\% & 41.4 & 16.9 & 16.2 & 38.6 & 1.30\% & 40.7 & 16.9 & 11.6 & 37.3 \\  & 1.80\% & 44.1 & 18.1 & 18.1 & 38.8 & 2.60\% & 30.8 & 12.9 & 9.3 & 29.2 \\ \cline{1-1}  & 3.60\% & 22.8 & 1.8 & 1.2 & 20.9 & 5.20\% & 35.6 & 16.0 & 11.5 & 33.6 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Node Clustering without Pretraining Results on _Cora_ and _Citeseer_ with varying condensation ratios (\(r\)). The best results are highlighted in **bold**, the runner-ups are underlined, and the best results of condensed datasets are shaded in gray * **Cheby [2]:** Cheby utilizes Chebyshev polynomials to approximate the graph convolution operations, which retains the essential graph filtering properties of GCN while reducing the computational complexity. We use a 2-layer Cheby with 256 hidden units and ReLU activation function.
* **GraphSAGE [12]:** GraphSAGE is a spatial-based graph neural network that directly samples and aggregates features from a node's local neighborhood. In the experiments, We use a two-layer architecture and a hidden dimension size of 256 while using a mean aggregator.
* **APPNP [18]:** APPNP leverages personalized PageRank to propagate information throughout the graph. This method decouples the neural network used for prediction from the propagation mechanism, enabling the use of personalized PageRank for message passing. In the experiments,we use a 2-layer model implemented with ReLU activation and sparse dropout to condense and evaluate.
* **GIN [40]:** GIN aggregates features by linearly combining the node features with those of their neighbors, achieving classification power as strong as the Weisfeiler-Lehman graph isomorphism test. We specifically applied a 3-layer GIN with a mean pooling function to compress and evaluate graph classification datasets. For the datasets _DD_ and _NCII_, we use negative log-likelihood loss function for training and softmax activation in the final layer. For _ogbg-molhiv_, _ogbg-molbbpp_ and _ogbg-molbace_, we use binary cross-entropy with logits and sigmoid activation in the final layer.
* **Graph Transformer [31]:** The Graph Transformer leverages the self-attention mechanism of the Transformer to capture long-range dependencies between nodes in a graph. It employs multi-head self-attention to dynamically weigh the importance of different nodes, effectively modeling complex relationships within the graph. We use a two-layer model with layer normalization and gated residual connections, following the settings outlined in [31].

#### b.4.2 Additional Results

Table B10 shows the node classification accuracy of datasets condensed by traditional core-set methods, which is backbone-free, evaluated across different backbone architectures on _Cora_.

### Settings and Additional Results of Initialization Impacts (RQ5)

#### b.5.1 Experimental Settings

The details of evaluated initialization mechanism are as follows:

* **Random Sample.** We randomly select features from nodes in the original graph that correspond to the same label, using these features to initialize the synthetic nodes.
* **Random Noise.** Consistent with prevalent dataset condensation methods, we initialize node features by sampling from a Gaussian distribution.
* **Center.** This method involves extracting features from nodes within the same label, applying the K-Means clustering algorithm to these features while treating the graph as a singular cluster and utilizing the centroid of this cluster as the initialization point for all synthetic nodes bearing the same label.
* **K-Center.** Similar to the Center initialization method, but employ the K-Means Clustering method on original nodes by dividing each class of the original graph nodes into n clusters,

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Methods** & **SGC** & **GCN** & **GraphSage** & **APPNP** & **Cheby** & **GTrans.** & **MLP** \\ \hline Full Dataset & 80.8 & 80.8 & 80.8 & 80.3 & 78.8 & 69.6 & 81.0 \\ Herding & 74.8 & 74.0 & 74.1 & 73.3 & 69.6 & 65.4 & 74.1 \\ K-Center & 72.5 & 72.4 & 71.8 & 71.5 & 63.0 & 64.3 & 72.2 \\ Random & 71.7 & 72.4 & 71.6 & 71.3 & 65.3 & 62.7 & 71.6 \\ \hline \hline \end{tabular}
\end{table}
Table B10: **Node Classification Accuracy (%) of core-set datasets across different backbone architectures on _Cora_ (2.6%).**

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Dataset** & 
\begin{tabular}{c} **Ratio** \\ (\(r\)) \\ \end{tabular} & **Random** & **Herding** & **K-Center** & **GCDM** & **DM** & **DosCond** & **Gcond** & **SGDD** & **SFGC** & **GEOM** \\ \hline \multirow{3}{*}{_Citeseer_} & 0.90\% & 0.62 & 0.60 & 0.62 & 0.65 & 0.65 & 0.55 & 0.70 & **0.74** & 0.62 & 0.59 \\  & 1.80\% & 0.60 & 0.54 & 0.60 & 0.64 & 0.65 & 0.58 & **0.68** & 0.67 & 0.60 & 0.56 \\  & 3.60\% & 0.57 & 0.56 & 0.57 & **0.68** & **0.68** & 0.59 & **0.68** & 0.52 & 0.59 & 0.57 \\ \hline \multirow{3}{*}{_Cora_} & 1.30\% & 0.52 & 0.48 & 0.53 & 0.52 & 0.52 & 0.45 & **0.54** & 0.41 & **0.54** & 0.51 \\  & 2.60\% & 0.50 & 0.45 & 0.54 & 0.54 & 0.54 & 0.56 & 0.55 & **0.57** & 0.53 & 0.53 \\ \cline{1-1}  & 5.20\% & 0.56 & 0.58 & 0.59 & 0.55 & 0.55 & 0.55 & 0.57 & **0.62** & 0.54 & 0.55 \\ \hline \hline \end{tabular}
\end{table}
Table B9: **Contextual Anomaly Detection results (ROC-AUC) on _Cora_ and _Citeseer_ with varying condensation ratios. The best results are shown in **bold** and the runner-ups are shown in underline.**where n is the number of synthetic nodes per class. We select the center of these clusters as the initialization of synthetic nodes in this class.
* **K-Means.** Similar to the K-Center initialization method, but instead of using the centroids of clusters to initialize the synthetic dataset, randomly select one node from each cluster to serve as the initial state for the synthetic node.

#### b.5.2 Additional Results

The performance of different initialization mechanism on _Cora_ (2.6%) and _Cora_ (0.26%) are shown in Table B11 and Table B12, respectively. It is evident that distribution matching methods are highly sensitive to the choice of initialization, especially when the dataset is condensed to a smaller scale. Additionally, trajectory matching methods perform poorly with random noise initialization and often fail to converge.

### Settings and Additional Results of Efficiency and Scalability (RQ6)

#### b.6.1 Experimental Settings

For a fair comparison, all the experiments are conducted on a single NVIDIA A100 GPU. Then we report the overall condensation time (min) when achieving the best validation performance, the peak CPU memory usage (MB) and the peak GPU memory usage (MB).

#### b.6.2 Additional Results

The detailed time and space consumption of the node-level GC methods on _ogbn-arxiv_ (0.50%) and graph-level GC methods on _ogbg-molhiv_ (1 Graph/Cls) are shown in Table B13 and Table B14 respectively. For node-level methods, although trajectory matching methods (_SFGC_[47], _GEOM_[45]) may consume less time and memory due to their offline matching mechanism, the expert trajectories generated before matching can occupy up to 764 GB of space as shown in Table B15, significantly impacting storage requirements. Among all the graph-level GC methods, _Mirage_[11] stands out by not relying on any GPU resources for calculation and can condense data extremely quickly, taking only 1% of the time required by other methods.

## Appendix C Reproducibility and Limitations

**Accessibility and license.** All the datasets, algorithm implementations, and experimental settings are publicly available in our open project (https://github.com/RingBDStack/GC-Bench). Ourpackage (codes and datasets) is licensed under the MIT License. This license permits users to freely use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the software, provided that the original copyright notice and permission notice are included in all copies or or substantial portions of the software. The MIT License is widely accepted for its simplicity and permissive terms, ensuring ease of use and contribution to the codes and datasets. We bear all responsibility in case of violation of rights, _etc_, and confirmation of the data license.

**Datasets.**_Cora_, _Citeseer_, _Flickr_, _Reddit_ and _DBLP_ are publicly available online3 with the MIT license. _ogbn-arxiv_, _ogbg-molb_, _ogbg-molb_bpp_ and _ogbg-molhiv_ are released by OGB [13] with the MIT license. _ACM_[46] is the subset hosted in [35] with the MIT license. _NCI1_[32] and _DD_[4] are available in TU Datasets [27] with the MIT license. All the datasets are consented to by the authors for academic usage. All the datasets do not contain personally identifiable information or offensive content.

Footnote 3: https://github.com/pyg-team/pytorch_geometric

**Limitations.** GC-Bench has some limitations that we aim to address in future work. Our current benchmark is limited to a specific set of graph types and graph tasks and might not reflect the full potential and versatility of GC methods. We hope to implement more GC algorithms for various tasks (e.g. subgraph classification, community detection) on more types of graphs (e.g., dynamic graph, directed graph). Besides, due to resource constraints and the availability of implementations, we could not include some of the latest GC algorithms in our benchmark. We will continuously update our repository to keep track of the latest advances in the field. We are also open to any suggestions and contributions that will improve the usability and effectiveness of our benchmark, ensuring it remains a valuable resource for the IGL research community.