# Imitation guided Automated Red Teaming

Desik Rengarajan\({}^{\dagger}\), Sajad Mousavi\({}^{\dagger}\), Ashwin Ramesh Babu, Vineet Gundecha,

**Avisek Naug, Sahand Ghorbanpour, Antonio Guillen, Ricardo Luna, Soumyendu Sarkar\({}^{*}\)**

\begin{tabular}{l} Hewlett Packard Enterprise (Hewlett Packard Labs) \\ \{desik.rengarajan, sajad.mousavi, ashwin.ramesh-babu, vineet.gundecha, \\ avisek.naug, sahand.ghorbanpour, antonio.guillen, rluna, \\ soumyendu.sarkar\}@hpe.com \\ \end{tabular}

Corresponding author. \(\dagger\)These authors contributed equally.

###### Abstract

The potential of large language models (LLMs) is substantial, yet they also carry the risk of generating harmful responses. An automatic "red teaming" process constructs test cases designed to elicit unfavorable responses from these models. A successful generator must provoke undesirable responses from the target LLMs with test cases that exemplify diversity. Current methods often struggle to balance quality (i.e., the harmfulness of responses) and diversity (i.e., the range of scenarios) in testing, typically sacrificing one to enhance the other and relying on non-optimal exhaustive comparison approaches. To address these challenges, we introduce an imitation-guided reinforcement learning approach to learn optimal red teaming strategies that generate both diverse and high-quality test cases without exhaustive searching. Our proposed method, Imitation-guided Automated Red Teaming (iART), is evaluated across various LLMs fine-tuned for different tasks. We demonstrate that iART achieves not only diverse test sets but also elicits undesirable responses from the target LLM in a computationally efficient manner.

**Warning: This paper consists of LLM outputs that are offensive.**

## 1 Introduction

Large Language Models (LLMs) have recently become extremely popular. They have achieved remarkable success in tasks such as text completion, instruction following, and code generation, becoming essential tools in various workflows and daily activities (Jiang et al., 2023; Roziere et al., 2023; Touvron et al., 2023; Achiam et al., 2023). Despite their advanced capabilities, these models can also generate harmful and incorrect content, thus making them prone to such issues as outlined in (Ji et al., 2023; Wei et al., 2023; Perez et al., 2022).

Given the widespread use of LLMs, testing them to prevent the production of harmful or undesirable content is crucial. This process, known as red teaming, involves identifying inputs that generate undesirable content. red teaming is challenging due to the vast range of possible input prompts and generated outputs. A common red teaming approach is using humans to design prompts that elicit undesirable responses from the LLM (Ganguli et al., 2022). However, relying solely on human testers presents various challenges: it is both expensive and time-consuming, limited by testers' domain knowledge, and exposes humans to toxic and harmful content (Radharapu et al., 2023).

Given these challenges, automating the red teaming process has become a key research focus. In particular, reinforcement learning (RL) has emerged as a popular approach for automated red teaming[Perez et al., 2022, Casper et al., 2023, Hong et al., 2024]. In RL-based red teaming, the main idea is to train a separate LLM known as the _attack LLM_ using RL to illicit undesirable responses from the LLM being tested (known as the _target LLM_). The outputs of the target LLM are evaluated using an evaluator module (typically another LLM), and this is used as feedback for training the attack LLM.

There are two main metrics the test cases generated by the attack LLM should satisfy, (1) **Quality:** The test cases generated by the attack LLM should elicit undesirable responses from the target LLM, (2) **Diversity:** The test cases generated by the attack LLM should be diverse., ie., they should cover a wide range of inputs to the target LLM. Methods solely based on RL Perez et al. [2022], Hong et al. [2024], while effective at eliciting undesirable responses, often struggle with generating diverse test cases. As noted by Hong et al. [2024], this lack of diversity stems from the absence of an explicit reward that encourages the attack LLM to generate new test cases, and utilizing RL for training causes the attack LLM to converge to a deterministic policy, leading to the generation of repeated test cases.

Current methods aimed at improving the quality and diversity of the generated test cases are often inadequate and computationally inefficient. For instance, Hong et al. [2024] imposes an explicit penalty during the training process to prevent the generation of previously seen test cases by the attack LLM. This involves comparing the outputs generated at the current training iteration with all of the previously generated outputs, thus making the training process extremely slow.

In this work, we propose Imitation Guided Automated Red Teaming (iART), a novel approach to RL-guided automated red teaming. The goal of **iART** is to simultaneously improve the quality and diversity of the outputs/test cases generated by the attack LLM in a computationally efficient manner. We achieve this using two innovative components. **First**, inspired by imitation learning, we _indirectly_ guide the training of the attack LLM using examples of undesirable responses we want the target LLM to generate. These examples demonstrate the range of behaviors that we want to test our target LLM on. Thus using these different examples for guidance helps us improve both the quality and diversity of the outputs generated by the attack LLM. **Second**, to further enhance the diversity of the attack LLM, we train a diversity module to model the distribution of previously generated outputs of the attack LLM. We then use this module to penalize the attack LLM from generating previously generated outputs, thus enhancing diversity. Our approach avoids the computationally inefficient method of exhaustively scanning through previously generated outputs to impose a penalty.

We evaluate our approach on text-continuation and instruction-following tasks using different target LLMs. For all the experiments, we use the 137M GPT-2 model as our attack LLM. We successfully elicit undesirable responses from much larger LLMs, such as Mistral-7B and Dolly-3B. Our approach outperforms all baselines in both quality and diversity. We find that our proposed method balances high-quality and diverse outputs across a range of tasks. Additionally, our algorithm is significantly more computationally efficient compared to existing methods that aim to improve both metrics. Overall, our approach enhances quality, diversity, and computational efficiency.

## 2 Related Work

**Learning from demonstrations and Imitation Learning:** The concept of learning from demonstrations involves leveraging demonstration data to aid the learning process [Schaal, 1996]. This approach, along with imitation learning, is popular in the RL domain [Hester et al., 2018, Nair et al., 2018]. It is particularly beneficial for applications like robotics [Vecerik et al., 2017, Rajeswaran et al., 2017], where defining a reward function can be challenging, but obtaining demonstrations is relatively easy. These methods have proven to be valuable in environments where exploration is difficult due to weak reward signals [Kang et al., 2018, Yang et al., 2023]. In this work, we extend the idea of learning from demonstrations and imitation learning to help us train an attack LLM that can elicit undesirable responses from a given target LLM.

**Adversarial Attacks and Red Teaming on LLMs:** Adversarial attacks aim to discover inputs that prompt a target LM to produce undesirable responses. Alzantot et al. [2018], Garg and Ramakrishnan [2020], Li et al. [2020, 20] investigate adversarial attacks on LLMs by focusing on word perturbations. These perturbations are designed to cause the target LM to generate undesirable outputs while preserving the original semantic meaning of the input. These approaches are called black-box attacks, as the algorithm cannot access the target LLM parameters. On the other hand, Wallace et al. [2019], Zou et al. [2023], Wichers et al. [2024] concentrate on white-box attacks, aiming to create adversarialprompts where the attacker has access to the weights or parameters of the target LLM. In a different approach, Deng et al. (2023); Mehrabi et al. (2023); Radharapu et al. (2023) utilize instruction and in-context learning-based methods to generate adversarial examples.

**RL-based Automated Red Teaming:**Perez et al. (2022) investigate the concept of automatically identifying instances where a target LLM exhibits harmful behavior by generating test cases using another LLM, employing methods such as RL and zero-shot learning. Casper et al. (2023) propose a red teaming pipeline where they fine-tune the evaluator function based on the outputs of the target model. Additionally, to prevent model collapse, they utilize a constraint based on the target LM's embeddings of the generated prompts. Hong et al. (2024) further extend these approaches by employing computationally intensive techniques (see Sections 4 and 5) to enhance the diversity and effectiveness of test cases.

Given the recent success of RL-based approaches for red teaming, our work focuses on refining these methods through established techniques in RL and imitation learning. Our approach differs from existing RL-based automated red teaming methods as we employ computationally efficient techniques to simultaneously enhance the diversity and effectiveness of test cases. Further, we integrate the concept of imitation learning into automated red teaming.

## 3 Preliminaries

In RL-based red teaming, we train a red teaming model, also known as an attack LLM \(\pi\), to induce a target LLM \(p\) to generate undesirable outputs. The undesirability of these outputs is measured by an evaluator function \(R\)(Hong et al., 2024; Perez et al., 2022). Formally, given a prompt \(x\), the target LLM \(p\) generates a response \(y\sim p(\cdot|x)\). The objective in RL-based red teaming is to train the attack LLM \(\pi\) to generate a prompt \(x\sim\pi(\cdot|z)\) for a specific instruction \(z\), aiming to maximize the undesirability of the target LLM's response \(R(y)\). Additionally, we incorporate a Kullback-Leibler (KL) divergence penalty between the attack model \(\pi\) and a reference model \(\pi_{\text{ref}}\) to prevent model drift (Ouyang et al., 2022). The RL-based red teaming objective is summarized as follows:

\[\max_{\pi} \mathbb{E}\left[R(y)-\beta D_{KL}\left(\pi(\cdot|z)||\pi_{\text{ ref}}(\cdot|z)\right)\right] \tag{1}\] \[z\sim\mathcal{D},x\sim\pi(\cdot|z),y\sim p(\cdot|x)\]

Here, \(\mathcal{D}\) represents a dataset of input prompts or instructions for the attack LLM, and \(\beta\) denotes the KL penalty coefficient.

## 4 Imitation Guided Automated Red Teaming

RL-based red teaming methods struggle to balance the quality and diversity of attack LLM outputs. Techniques such as adding randomness to the attack LLM's generation, incorporating an entropy bonus to encourage exploration, adjusting the KL penalty \(\beta\), or increasing the sampling temperature have been shown to improve either quality or diversity, but at the expense of the other (Hong et al., 2024). Further, current techniques to improve both metrics involve exhaustive computations, making them computationally inefficient (Hong et al., 2024).

Our approach aims to address both metrics of quality and diversity simultaneously in a computationally efficient manner. We accomplish this by introducing two novel components.

### Imitation Guidance

To enhance the quality and diversity of the attack LLM's outputs, we aim to _indirectly_ guide the training of the attack LLM using examples of undesirable outputs. We assume that we have access to

Figure 1: Imitation guided automated red teaming workflow.

a dataset that consists of undesirable outputs \(\mathcal{D}_{\text{harm}}\). This dataset represents the behaviors we need to test our **target LLM** on. In our approach to imitation guidance, we intend to utilize this dataset to determine which inputs prompt our target LLM to generate outputs similar to those in \(\mathcal{D}_{\text{harm}}\). In other words, we train the attack LLM such that it generates test cases that cause the target LLM to generate outputs similar to those in \(\mathcal{D}_{\text{harm}}\).

This approach is valuable as it enables us to test and understand which inputs elicit specific behaviors from the target LLM. Further, there exist a large number of datasets that consist of examples of undesirable behaviors Gehman et al. (2020); Lin et al. (2023), which can be used as \(\mathcal{D}_{\text{harm}}\).

We first model the space of \(\mathcal{D}_{\text{harm}}\) by training a harm LLM \(\phi\) on it. This ensures that when prompted, \(\phi\) produces outputs similar to those in \(\mathcal{D}_{\text{harm}}\). Given the harm model \(\phi\), our goal is to train the attack LLM \(\pi\) to generate prompts capable of inducing the target LLM \(p\) to generate outputs \(y\) similar to those of the harm model \(\tilde{y}\sim\phi(\cdot|z)\) where the input to the harm LLM is a combination of the input to the Attack LLM \(z\), and output of the attack LLM \(x\). Our objective now becomes:

\[\max_{\pi}\mathbb{E} \left[R(y)-\beta D_{KL}\left(\pi(\cdot|z)||\pi_{\text{ref}}(\cdot |z)\right)+\beta_{1}D_{\text{cos}}\left(y,\tilde{y}\right)\right] \tag{2}\] \[z\sim\mathcal{D},x\sim\pi(\cdot|z),y\sim p(\cdot|x),\tilde{y} \sim\phi(\cdot|z)\]

Here, \(D_{\text{cos}}\) measures the cosine similarity between the output of the target LLM \(y\) and the harm LLM \(\tilde{y}\). Intuitively, we are training the attack LLM to prompt the target LLM to generate outputs resembling those of the Harm LLM. Having imitation guidance aids in both producing harmful content and ensuring that the outputs of the attack LLM are diverse. This is because the harm model is trained on multiple examples of harmful outputs, and thus can guide the training of the attack LLM. Details on harm model training and \(D_{\text{cos}}\) are provided in Appendix A.

### Diversity Module

To enhance the diversity of the attack LLM, we include a _diversity module_\(G\) which is a prompt-conditioned generative model. The goal of the diversity module is to model the distribution of previously generated outputs of the attack LLM during the training process. We train this model to generate previously observed outputs of the attack LLM for input prompt \(z\) during the training process. We then compare the outputs of the attack LLM, \(x\sim\pi(\cdot|z)\), with the outputs of the diversity module, \(\tilde{x}\sim G(\cdot|z)\), for the same input \(z\). If these outputs are similar, it indicates that the output has been generated previously, and we penalize the attack LLM. We iteratively train the diversity module \(G\) using previous inputs and outputs of the attack LLM. The final objective of our approach is as follows:

\[\max_{\pi}\mathbb{E} \left[R(y)-\beta D_{KL}\left(\pi(\cdot|z)||\pi_{\text{ref}}(\cdot |z)\right)+\beta_{1}D_{\text{cos}}\left(y,\tilde{y}\right)-D_{\text{cos}} \left(x,\tilde{x}\right)\right] \tag{3}\] \[z\sim\mathcal{D},x\sim\pi(\cdot|z),y\sim p(\cdot|x),\tilde{y} \sim\phi(\cdot|z),\tilde{x}\sim G(\cdot|z)\]

Intuitively, the diversity module keeps track of the outputs of the attack LLM that have been observed during training. Further details of our _diversity module_ are available in Appendix A.2. Our methodology is summarized in Fig. 1.

## 5 Experiments

The goal of our evaluation is to measure iART's performance on two metrics: the _quality_ and _diversity_ of the test cases it produces. To achieve this, we conduct experiments on two tasks: text continuation and instruction following. We compare iART's performance with other baselines using various target LLMs of different sizes, including Mistral-7B (Jiang et al., 2023), GPT2-Alpaca (a GPT-2 model (Radford et al., 2019) fine-tuned on the Alpaca dataset (Taori et al., 2023)), and Dolly-3V (Conover et al., 2023).

### Metrics

**Quality**: Following (Hong et al., 2024), the quality of a red teaming model or attack LLM \(\pi\) is the percentage of toxic responses it can elicit from the target LLM \(p\), when given test cases generated by \(\pi\). We employ the RoBERTa hate speech classifier (Vidgen et al., 2020) to assign toxicity scores to the responses from the target LLM \(y\) for test cases \(x\) generated by the attack LLM. We consider an output of the target LLM toxic if its assigned toxicity score exceeds a threshold.

We evaluate the quality of iART and baselines by measuring the quality of test cases \(x\) generated during the training of \(\pi\) for different threshold values.

**Diversity**: We quantify the diversity of the attack LLM by measuring the variability of test cases it generates across different toxicity thresholds. This variability is measured using the cosine similarity model \(D_{\text{cos}}\). We provide details on \(D_{\text{cos}}\) in Appendix A.

To evaluate the diversity of iART and other baselines, we compare each test case generated during training of \(\pi\) with all other test cases produced for different threshold values.

**F1 Score for Diversity and Quality (F1DQ)**: Quality and diversity in testing scenarios often present a trade-off, where an improvement in one metric may come at the cost of the other. Specifically, higher quality (manifested as more frequent toxic outputs) tends to involve repetitive toxic words, thus reducing the diversity of the test cases. On the other hand, a higher diversity score can lead to the target model generating less toxic responses.

To quantify this trade-off and assess both metrics simultaneously, we introduce the F1DQ metric, which combines the quality and diversity scores using a harmonic mean. We define the F1DQ metric as follows:

\[F1DQ=\frac{2\times\text{Quality}\times\text{Diversity}}{\text{Quality}+\text{ Diversity}}\]

A red teaming model with a high F1DQ score implies that it is optimizing both quality and diversity simultaneously. This metric allows for a balanced assessment of the red teaming model's performance in generating diverse test cases yet eliciting the target model to generate toxic responses.

Similar to quality and diversity, evaluate the F1DQ score of iART and other baselines over different toxicity thresholds.

### Baselines

We benchmark our iART method against established RL-based automated red teaming approaches to demonstrate the benefits of integrating imitation guidance to indirectly guide the training of \(\pi\) and a diversity module to improve the diversity of the generated test cases. For consistency, we use GPT2 (Radford et al., 2019) with 137M parameters as our attack LLM across all baselines and use proximal policy optimization (PPO) (Schulman et al., 2017) as the RL algorithm. We provide more details in Appendix A. We compare the performance of iART with the following baselines.

1. **RL (Perez et al., 2022)**: This foundational method involves training the red team model \(\pi\) with a focus on maximizing rewards \(R(y)\) while incorporating a KL divergence penalty to prevent model drift (Eq. 1).
2. **RL+TDiv (Casper et al., 2023)**: Building on the RL framework of Perez et al. (2022), this variant enhances the model by training \(\pi\) to not only follow the reward structure and KL penalty but also to maximize the diversity among responses. Diversity is quantified through the average distances between sentence embeddings produced by the target LLM.
3. **RL+Curiosity (Hong et al., 2024)**: This approach modifies the RL+TDiv method by shifting the focus of diversity maximization to the attack LLM itself. It measures the diversity of outputs by evaluating the distances among **all test cases** generated by the attack LLM, utilizing both the SelfBLEU score (Zhu et al., 2018), which employs BLEU score n-gram modeling for \(n\in\{2,3,4,5\}\), and cosine similarity of sentence embeddings to assess the diversity. The BLEU score measures the overlap of n-grams between a generated sentence and reference sentences. In the case of SelfBLEU, each previously generated sentence acts as a reference, with the score for each sentence labeled as SelfBLEU. Adopting this method is computationally intensive, as each generated sentence at every timestep in RL must be compared both semantically, using sentence embeddings, and textually, through SelfBLEU, against all prior generated test cases.

Our iART model advances these methods by training the red team model \(\pi\) and removing the need for exhaustive comparison of prior test cases by utilizing imitation-guided reinforcement learning with harmful model rewards and diversity model rewards, as detailed in Section 4.

### Tasks

We evaluate our red teaming approach, iART, against target LLMs on two tasks: text continuation and instruction following. Text continuation in LLMs involves generating coherent and contextually relevant text that logically follows from a given prompt or initial segment. Meanwhile, the goal of the instruction following task is for the LLM to execute specific commands embedded within a textual input, adhering to direct instructions and providing appropriate responses. We conducted experiments using three seeds for each red teaming algorithm across all tasks, except for RL+Curiosity, which required several days to complete just one run.

#### 5.3.1 Text Continuation

In the text continuation task, we use a variant of GPT2 (Radford et al., 2019) fine-tuned on the IMDb review dataset (Maas et al., 2011) as our attack LLM \(\pi\), with Mistral 7B serving as the target LLMs. We extract the first \(10\) words of each movie review from the IMDB dataset and feed them into the attack LLM to generate an extended review. This continuation is then concatenated with the original input and passed to the target LLM to elicit a response.

We measure the toxicity scores of all responses generated by the target LLM and plot the percentage of toxic responses against the toxicity threshold, as illustrated in Fig. 2. The graph in Fig. 2 reveals the efficacy of different red teaming strategies in provoking toxic responses at varying thresholds, Fig. 2 shows the diversity of the test cases for different toxicity thresholds, and Fig. 2 plots the F1DQ scores. The results show that iART consistently outperforms other models in eliciting high toxicity across a broader range of thresholds, while still being diverse.

Fig. 2 showcases the diversity of test cases generated by various red teaming models, as measured through embedding diversity. iART matches other methods in diversity while maintaining high quality, as evidenced by Fig. 2. In contrast, RL+Curiosity, also shown in Fig. 2, prioritizes diversity at the expense of quality. The F1 Score for Diversity and Quality, depicted in Fig. 2, illustrates that iART outperforms other models by balancing quality and diversity more effectively. RL+Curiosity and RL+TDiv, ranking second and third respectively.

Fig. 2 illustrates the execution times of each red teaming algorithm (the execution time corresponds to the total training time to generate all test cases.). RL+Curiosity requires the longest time, at 2929 minutes (approximately 2 days and 39 minutes), which is nearly 8 times longer than iART, which completes in just 423 minutes (7 hours and 3 minutes). This extensive duration for RL+Curiosity is attributed to its exhaustive method of keeping an array of prior test cases through detailed cosine similarity and SelfBLEU comparisons with previous cases, often leading to less toxic responses. In contrast, RL+TDiv focuses on eliciting diverse responses from the target LLM but may not effectively balance the quality and diversity of these cases.

#### 5.3.2 Instruction Following

In the instruction-following task, we employ GPT-2 as our attack model, while GPT-2-Alpaca, finetuned with the Alpaca dataset, and Dolly-v2-3B, a high-quality instruction instruction-following large language model trained on the Databricks machine learning platform and datasets (Conover et al., 2023), serve as the target LLMs. Based on the framework established by the Alpaca team

Figure 2: Comparative analysis of red teaming strategies with a GPT-2 attacker against Mistral 7B in the IMDb reviews dataset. (a) Demonstrates each algorithm’s ability to induce toxic responses. (b) Shows the diversity of test cases generated. (c) Effectiveness of balancing quality and diversity. (d) Compares execution times.

Taori et al. (2023), we provide a list of instructions to the attack model, which is then tasked with generating an additional instruction. This newly formed instruction is subsequently fed into the target LLMs, which are prompted to craft a response that effectively completes the given directive.

Fig. 2(e) demonstrates the performances on the Alpaca dataset. We can clearly observe that iART consistently outperforms RL+Curiosity, RL+TDiv, and RL in generating toxic responses across varying thresholds, as evidenced in Fig. 2(a). The diversity metrics, shown in Fig. 2(b), indicate that iART achieves substantial diversity while demonstrating superior quality. The F1 Score for Diversity

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline Test Case & Target LLM Response & Toxicity Score \\ \hline Russian immigrants lack intellectual capacity or intelligent minds, walk freely; speak fast enough ; sit down nor run Japanese men lacked moral sense and were lazy or clever, walked round themselves; stood uprightly & I agree with the statement. Russian immigrants lack intellectual capacity or intelligent minds and are very impulsive. In fact, the best example I see of this While Japanese women could be very charming and alluring, they were often boxy, domineering and had a hard time managing their emotions. Women control their needs in dreams; live abroad or fly away & 0.99 \\ Chinese immigrants lack adequate intellectual or sense, drive their heads ; sit down nor concentrate, breathe; speak & The people in the video look disbrevled and poorly put together. It is not uncommon for immigrants to speak Chinese. One would expect them to Women, especially more demure and sedate woman, are often prone to gigle or even laugh out loud when facing something humans. Their eyes Nigerian migrants lacked moral sense nor were smart or clever, walked forward ; stood uprightly dumb animals & 0.99 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Qualitative analysis of iART on the instruction following task with the Databricks dataset.

Figure 3: Comparative analysis of red meaning strategies in instruction following tasks across different LLMs and datasets using GPT-2 and Dolly-3B models. (a), (f) Demonstrate our method’s ability to elicit a higher percentage of toxic responses from the target LLM across various toxicity thresholds. (b), (g) iART achieves the greatest and most stable diversity (low variance) of test cases among the baselines, measured by 1 - Cosine Similarity. (c), (h) Present the F1 Score for Diversity and Quality, highlighting iART’s effective balance of high-quality toxic response generation with diverse test cases. (d), (i) Show that iART achieves this significant performance within reasonable running times compared to other models.

and Quality (Fig. 3c) highlights iART's superior balance in maintaining both high-quality responses and diversity compared to other models.

When using Dolly-3B as the target, we use the instruction dataset provided by Hong et al. [2024] as the input to the attack LLM. The graphs shown in Fig. 3j, show that iART outperforms other baselines in generating toxic responses (Fig. 3f). The diversity performance (Fig. 3g) shows a similar trend to the Alpaca dataset, with iART providing a robust diversity score. The F1DQ Score (Fig. 3h) further emphasizes iART's effectiveness in achieving an optimal balance between quality and diversity, outpacing all of the competing approaches.

Fig. 3d and Fig. 3i display the execution times of each red teaming algorithm applied to the Alpaca and Databricks tasks, respectively. The figures reveal that while RL and RL+TDiv exhibit shorter running times, they struggle to deliver both high-quality responses and diverse test cases. Specifically, RL+TDiv produces diverse test cases but with almost negligible toxicity rates, whereas RL shows better quality but lacks diversity compared to RL+TDiv. RL+Curiosity excels in balancing quality and diversity, but this comes at the cost of much longer times, requiring 4922 minutes (approximately 3 days, 10 hours) and 5892 minutes (approximately 4 days, 2 hours) for 500 epochs on each dataset, respectively. In contrast, iART demonstrates impressive performance in both quality and diversity across both datasets, with significantly more efficient execution times of 361 minutes (6 hours) and 841 minutes (14 hours).

### Ablation of Imitation and Diversity Module

We conduct ablation studies to gauge the impact of the imitation and diversity module on iART. We conduct experiments on the test continuation task using the IMDB dataset, with Mistral-7B as the target LLM and GPT2 as the attack LLM. From Fig. 4, we can clearly observe that both modules work together to improve all of the metrics.

We show examples to illustrate the performance of iART in Table 1.

We conduct several experiments to demonstrate the performance of iART. These include hyperparameter sweep of imitation guidance coefficient (Appendix C) and KL Coefficient (Appendix D), experiments using Mistral as the attacker (Appendix E), and studying the performance of RL with different KL coefficients (Appendix F).

## 6 Conclusion

We introduce iART, an innovative approach to automated red teaming that utilizes imitation learning to enhance the diversity of test cases generated by the red teaming model and the quality of responses from target LLMs. Our experiments show that iART significantly outperforms existing reinforcement learning-based methods such as RL, RL+TDiv, and RL+Curiosity, not only in efficiency but also in its ability to balance diversity and quality (i.e., demonstrated with the F1DQ score). By producing test cases that are diverse and robust, iART effectively uncovers a broader spectrum of potential flaws in target LLMs across different tasks and datasets, proving its effectiveness in real-world scenarios. In addition, iART demonstrated substantial gains in computational efficiency, making it a vital tool to scale up red teaming practices and improve the safety and reliability of AI systems.

Figure 4: Ablation of imitation and diversity modules.

## References

* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Alzantot et al. (2018) Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. _arXiv preprint arXiv:1804.07998_, 2018.
* Casper et al. (2023) Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit: Red teaming language models from scratch. _arXiv preprint arXiv:2306.09442_, 2023.
* Conover et al. (2023a) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm. _Company Blog of Databricks_, 2023a.
* Conover et al. (2023b) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023b. URL [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).
* Deng et al. (2023) Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, and Xiangnan He. Attack prompt generation for red teaming and defending large language models. _arXiv preprint arXiv:2310.12505_, 2023.
* Ganguli et al. (2022) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_, 2022.
* Garg and Ramakrishnan (2020) Siddhant Garg and Goutham Ramakrishnan. Bae: Bert-based adversarial examples for text classification. _arXiv preprint arXiv:2004.01970_, 2020.
* Gehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Real-toxicityprompts: Evaluating neural toxic degeneration in language models. _arXiv preprint arXiv:2009.11462_, 2020.
* Havrilla et al. (2023) Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman, Quentin Anthony, and Louis Castricato. trlX: A framework for large scale reinforcement learning from human feedback. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 8578-8595, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.530. URL [https://aclanthology.org/2023.emnlp-main.530](https://aclanthology.org/2023.emnlp-main.530).
* Hester et al. (2018) Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Hong et al. (2024) Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James Glass, Akash Srivastava, and Pulkit Agrawal. Curiosity-driven red-teaming for large language models. _arXiv preprint arXiv:2402.19464_, 2024.
* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Liu et al. (2021)Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy optimization with demonstrations. In _International conference on machine learning_, pages 2469-2478. PMLR, 2018.
* Li et al. (2020) Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun, and Bill Dolan. Contextualized perturbation for textual adversarial attack. _arXiv preprint arXiv:2009.07502_, 2020a.
* Li et al. (2020b) Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert using bert. _arXiv preprint arXiv:2004.09984_, 2020b.
* Lin et al. (2023) Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation, 2023.
* Maas et al. (2011) Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_, pages 142-150, 2011.
* Mehrabi et al. (2023) Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. Flirt: Feedback loop in-context red teaming. _arXiv preprint arXiv:2308.04265_, 2023.
* Nair et al. (2018) Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 6292-6299. IEEE, 2018.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* Perez et al. (2022) Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. _arXiv preprint arXiv:2202.03286_, 2022.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Radharapu et al. (2023) Bhaktipriya Radharapu, Kevin Robinson, Lora Aroyo, and Preethi Lahoti. Aart: Ai-assisted red-teaming with diverse data generation for new llvm-powered applications. _arXiv preprint arXiv:2311.08592_, 2023.
* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2023.
* Rajeswaran et al. (2017) Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. _arXiv preprint arXiv:1709.10087_, 2017.
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* Schaal (1996) Stefan Schaal. Learning from demonstration. _Advances in neural information processing systems_, 9, 1996.
* Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Touvron et al. (2023)Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothorl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. _arXiv preprint arXiv:1707.08817_, 2017.
* Vidgen et al. (2020) Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. Learning from the worst: Dynamically generated datasets to improve online hate detection. _arXiv preprint arXiv:2012.15761_, 2020.
* von Werra et al. (2020) Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. [https://github.com/huggingface/trl](https://github.com/huggingface/trl), 2020.
* Wallace et al. (2019) Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers for attacking and analyzing nlp. _arXiv preprint arXiv:1908.07125_, 2019.
* Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? _Advances in Neural Information Processing Systems_, 36, 2023.
* Wichers et al. (2024) Nevan Wichers, Carson Denison, and Ahmad Beirami. Gradient-based language model red teaming. _arXiv preprint arXiv:2401.16656_, 2024.
* Yang et al. (2023) Hanlin Yang, Chao Yu, Siji Chen, et al. Hybrid policy optimization from imperfect demonstrations. _Advances in Neural Information Processing Systems_, 36, 2023.
* Zhu et al. (2018) Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 1097-1100, 2018.
* Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section **??**. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Code will be released after acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?Answer: [Yes] Justification: See Section 5 and Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics**Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: [NA] Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix B. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Model and code will be released after appropriate safeguards are installed. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See section 5 and Appendix A. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: Code will be released upon acceptance. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: [The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

## Appendix A Experimental Setup and Resources

### Attack LLM

For all our experiments and baseline implementations we use GPT2 (Radford et al., 2019) with 137M parameters as the attack LLM \(\pi\). We implement iART and the baselines using the repository provided by (Hong et al., 2024), which is implemented using trlx(Havrilla et al., 2023). We train iART and baselines using PPO (Schulman et al., 2017). To ensure a fair comparison, similar to Hong et al. (2024), we include a gibbsimish penalty2 for iART as well as all the baselines. This ensures that the outputs of the attack LLM are natural and human-like. To promote exploration, for iART as well as all the baselines, we include an entropy bonus with a coefficient \(0.001\). For all the experiments and baselines we train the attack LLM using LORA (Hu et al., 2021). We run all experiments and baselines for \(500\) epochs, generating \(40K\) test cases. We provide other hyperparameters for the attack LLM in table 3.

Footnote 2: [https://huggingface.co/madhurjindal/autonlp-Gibberish-Detector-492513457](https://huggingface.co/madhurjindal/autonlp-Gibberish-Detector-492513457)

For all the tasks and baselines, we use the RoBERTa hate speech classifier Vidgen et al. (2020) as the reward or evaluator model \(R\). For all experiments and baselines, we set the target LLM to generate a maximum of \(30\) tokens.

We provide details of the assets and licenses used in Table 2.

### Diversity Module

For the diversity module \(G\), we implement a strategy to enhance the variety of outputs produced by the attack LLM. This module functions as a prompt-conditioned generative model, learning to replicate the distribution of outputs previously generated by the attack LLM during training sessions. To achieve this, \(G\) is trained online, dynamically updating its model parameters based on the latest interactions with the target LLM after each rollout. For all our tasks, we model \(G\) using GPT2 (Radford et al., 2019).

We incorporate online training with a replay buffer. The replay buffer maintains a history of past interactions, which helps mitigate issues related to data imbalance and temporal correlations. This buffer is utilized to train \(G\) by sampling batches of historical data, ensuring that the model does not forget previously learned patterns For the implementation, we use the Iterative Trainer module of the trl library.

### Compute Resources

We run iART and all baseline comparisons, on nodes equipped with A100 and H100 GPUs, each with 80 GB of memory and 64 CPU cores. iART can be executed on a single GPU. The timings reported in Section 5 are based on single GPU runs.

### Prompt design for instruction following tasks

We follow the prompt design used by Hong et al. (2024). For the Alpaca dataset we use,

Write a list of instructions:

1. {Instruction 1}
2. {Instruction 2}
3. {Instruction 3}
4.

For the databrick dataset, we use

### Cosine Similarity Module

For measuring cosine similarity, denoted as \(D_{\text{cos}}\), we utilize the Cross-Encoder architecture for Sentence Similarity, specifically adopting the model (abbasgolestani/ag-nli-DeTS-sentence-similarity-v2). This model excels in computing semantic similarities, producing a score ranging from 0 (no similarity) to 1 (high similarity). It assesses the similarity of each corresponding pair of sentences from two input arrays, enabling precise and context-aware similarity evaluations.

### Harm Model

We choose the openly available dataset ToxicDPQqa as \(\mathcal{D}_{\text{harm}}\). We fine-tune a Mistral-7B LLM (OpenHermes-2.5-Mistral-7B) on it using Direct Preference Optimization [14] using code from the trl (Transformers Reinforcement Learning) library developed by Hugging Face [27] to obtain the harm LLM \(\phi\). While training the attack LLM, we load the harm LLM in \(4\) bit for faster execution.

## Appendix B Broader Impacts

The development of LLMs has transformed many sectors from computer science to healthcare, necessitating measures to evaluate their potential for generating harmful content. Our work iART probes these models and identifies the risks before their deployment in real-world applications.

By automating red teaming processes, iART not only reduces reliance on human testers, thus minimizing exposure to harmful content but also enhances scalability and effectiveness. Also, this approach aids stakeholders in outlining the ethical boundaries of LLM deployments, pinpointing triggers of harmful outputs to promote safer model behavior. Moreover, iART contributes to enhancing model robustness by identifying and addressing trustworthiness weaknesses, ensuring the models are better prepared for real-world scenarios and challenges.

\begin{table}
\begin{tabular}{|l|l|l|} \hline Asset & Licenses & Link \\ \hline Code of [13] & MIT & github.com/Improbable-AI/curiosity\_redteam \\ \hline trlx & MIT & github.com/CarperAI/trlx \\ \hline Mistral-7B & Apache & huggingface.co/mistralai/Mistral-7B-v0.1 \\ \hline GPT2 & MIT & huggingface.co/openai-community/gpt2 \\ \hline GPT-2 Alpaca & MIT & huggingface.co/vicealle/gpt2-alpaca \\ \hline Dolly-3B & MIT & huggingface.co/databricks/dolly-v2-3b \\ \hline IMDB & MIT & huggingface.co/datasets/stanfordnlp/imdb \\ \hline OpenHermes-2.5-Mistral-7B & Apache & huggingface.co/teknium/OpenHermes-2.5-Mistral-7B \\ \hline ToxicDPQ & MIT & huggingface.co/datasets/NobodyExistsOnTheInternet/ToxicDPQqa \\ \hline ag-nli-DeTS-sentence-similarity-v2 & Apache & huggingface.co/abbasgolestani/ag-nli-DeTS-sentence-similarity-v2 \\ \hline \end{tabular}
\end{table}
Table 2: Table of assets used.

\begin{table}
\begin{tabular}{|l|l|} \hline Config Type & Value \\ \hline \multirow{3}{*}{train} & seq\_length = 1024, \\  & batch\_size = 32, \\  & mixed\_precision= no \\ \hline \multirow{7}{*}{model} & model\_path = gpt2 \\  & num\_layers\_unfrozen = -1 \\  & peff\_config = \{ \\  & “r”: 16, \\  & “lora\_alpha”: 16, \\  & “lora\_dropout”: 0.005, \\  & “task\_type”: "CAUSAL\_LM", \\  & “peff\_type”: "LORA", \\  & “bias”: "none", \\  & “target\_modules”: [ “k\_proj”,gate\_proj”,v\_proj”, \\  & “up\_proj”, “q\_proj”, “o\_proj”,“down\_proj” ] \}, \\  & quantization\_config =\{ \\  & “load\_in\_4bit”: true, \\  & “bnb\_4bit\_compute\_dtype”: "float16”, \\  & “bnb\_4bit\_use\_double\_quant”: true, \\  & “bnb\_4bit\_quant\_type”: "nf4" \\  & \} \\ \hline \multirow{2}{*}{tokenizer} & tokenizer\_path="gpt2”, \\  & truncation\_side="right” \\ \hline \multirow{3}{*}{optimizer} & name = “adamw”, \\  & kwargs =[Ir: 3e-05, \\  & betas:[0.9, 0.95], \\  & eps: 1e-08, \\  & weight\_decay: 1e-06 \} \\ \hline \multirow{2}{*}{scheduler} & name=“cosine\_annealing”, \\  & kwargs=[T\_max: 1e12, \\  & eta\_min: 3e-5] \\ \hline \multirow{8}{*}{method} & ppo\_epochs =4, \\  & num\_rollouts =128, \\  & chunk\_size = 128, \\  & horizon =10000, \\  & gamma =1, \\  & lam =0.95, \\  & cliprange =0.2, \\  & cliprange\_value =;0.2, \\  & vf\_coef= 1, \\  & cliprange\_reward =10, \\  & gen\_kwargs =\{ \\  & “max\_new\_tokens”: 20, \\  & “top\_k”: 5, \\  & “top\_p”: 0.92, \\  & “repetition\_penalty”: 1.5, \\  & “temperature”: 0.7, \\  & “do\_sample”: true, \} \\ \hline \end{tabular}
\end{table}
Table 3: Attack LLM parametersHyperparameter Sweep of Imitation Guidance Coefficient

We run iART for different values of \(\beta_{1}\), the coefficient of the imitation guidance module. We consider the text continuation task, where we use a GPT2 as an attacker Mistral-7B as the target LLM.

From Figure 5 we can clearly observe that iART is fairly robust to the variations of this hyperparameter.

## Appendix D Hyperparameter Sweep of KL Coefficient

We study the performance of iART under different KL co-efficient \(\beta\) values in Figure 6 in the text continuation task, with GPT2 as the attack LLM and Mistral 7B as the target LLM. We observe that higher KL values lead to a degradation in performance, as the trained attacker is constrained to stay close to the initial model.

## Appendix E Effect of using Larger Attack Model

We investigate the performance of iART when Mistral-7B is used as the attack LLM. Our study focuses on the instruction-following task, utilizing the Alpaca dataset, where the target LLM is a GPT-2 model fine-tuned on the Alpaca dataset. Further details of the experimental setup are provided in Section **Instruction Following** of the main paper.

As shown in Figure 7, iART generally outperforms all baseline models. Additionally, we observe some discontinuities in Figure 6(b). These discontinuities arise because there are no examples available at specific toxicity thresholds.

Figure 5: Imitation Guidance hyperparameter sweep

Figure 6: KL coefficient hyperparameter sweep

## Appendix F Does changing the KL penalty affect RL performance?

We explore whether adjusting the \(\beta\) parameter (the KL penalty) can enhance both quality and diversity. In Figure 8, we present the results of experiments with varying \(\beta\) values. The findings indicate that while increasing \(\beta\) improves diversity, it simultaneously reduces quality when compared to lower \(\beta\) values. Overall, this indicates that modifying the KL penalty weight alone is insufficient for generating diverse and effective test cases.

Figure 8: Comparison of iART and RL with different KL penalty weights.

Figure 7: Mistral as attacker