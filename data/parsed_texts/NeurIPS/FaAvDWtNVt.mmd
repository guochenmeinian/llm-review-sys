# N Multipliers for N Bits: Learning Bit Multipliers for Non-Uniform Quantization

Raghav Singhal  Anmol Biswas\({}^{\dagger}\)  Sivakumar Elangovan  Shreyas Sabnis

&Udayan Ganguly

Indian Institute of Technology Bombay, India

anmolbiswas@gmail.com\({}^{\dagger}\)

###### Abstract

Effective resource management is critical for deploying Deep Neural Networks (DNNs) in resource-constrained environments, highlighting the importance of low-bit quantization to optimize memory and speed. In this paper, we introduce N-Multipliers-for-N-Bits, a novel method for non-uniform quantization designed for efficient hardware implementation. Our method uses N parameters, distinct for every layer and corresponding to the N quantization bits, whose linear combinations span the set of allowed weights (and activations). Furthermore, we learn these parameters in conjunction with the weights, ensuring exceptional flexibility in the quantizer model with minimal hardware overhead. We validate our method on CIFAR-10 and ImageNet, achieving competitive results with 3- and 4-bit quantized models. We demonstrate strong performance on 4-bit quantized Spiking Neural Networks (SNNs), evaluated on the CIFAR10-DVS and N-Caltech 101 datasets. Further, we address the issue of stuck-at faults in hardware, and demonstrate robustness to up to 30\(\%\) faulty bits.

## 1 Introduction

Deep learning dominates computer vision and broader AI applications, where cloud-based models perform inference by transferring data to servers. While effective, this approach is inefficient in terms of data transfer and power consumption. A more efficient alternative, especially for simple tasks, is edge inference using low-power accelerators with fixed-point arithmetic and in-memory or near-memory computing architectures [1, 2, 3, 4, 5]. These architectures, such as crossbar arrays, perform matrix-vector multiplication by accumulating parallel operations. They can be implemented using analog components or digital ones, but both approaches encounter a trade-off between energy efficiency and performance [6, 7]. Energy efficiency in edge devices often comes at the cost of circuit non-idealities such as line resistance and device variability [8, 9, 10] and hardware faults such as stuck-at (SA) faults [11, 12], where certain weight bits get stuck at either 0 or 1 and become unprogrammable. Addressing both quantization errors and hardware faults is crucial for optimizing edge inference.

Low-bit quantization for weights and activations has been extensively explored through quantization-aware training (QAT). Popular methods in the literature include regularization-based QAT, like sine-regularization [13, 14] and bin-regularization [15], and learned quantization scale [16, 17, 18]. Non-uniform methods, such as learning quantization levels or companding functions [19, 20], offer flexibility by learning key parameters. However, these works are not generally extendable to the specific challenges of low-power neuromorphic accelerators and event-driven Spiking Neural Networks (SNNs) [21, 22].

In this work, we propose a QAT scheme that optimizes bit-multipliers for each quantization level using a regularization-based approach using Mean Squared Error (MSE) loss. This enables the generalization of our method beyond just QAT for Artificial Neural Networks and into QAT for Spiking Neural Networks and non-ideality and hardware fault mitigation for low-power neuromorphic accelerators. We match the state-of-the-art performance in QAT for 4-bit networks on CIFAR-10 [23] and ImageNet [24] benchmarks and show excellent performance in 4-bit SNNs on CIFAR10-DVS [25] and N-Caltech 101 [26] neuromorphic benchmarks. Finally, we also show robust training for low-bit quantized models even with a high rate of hardware faults (up to \(30\%\)).

Our key contributions are summarized as follows:

* We introduce a novel, flexible, and hardware-compatible quantization framework that learns N bit multipliers per layer alongside network weights, enabling adaptable precision with minimal hardware overhead, while spanning a rich set of quantization levels.
* We show our method's effectiveness across multiple networks and datasets, achieving comparable state-of-the-art results for 3- and 4-bit DNNs on CIFAR-10 [23] and ImageNet [24], and 4-bit SNNs on event-based datasets: CIFAR10-DVS [25] and N-Caltech 101 [26].
* We propose a fault-tolerant quantization method that enables low-bit models to maintain performance up to \(30\%\) faulty bits, as demonstrated on CIFAR-10, enhancing robustness.
* We propose a custom implementation of bit-level multipliers for analog/digital crossbars, optimized for our quantization scheme and directly portable to neuromorphic hardware.

## 2 Methodology

**Preliminaries:** Quantization aims to replace floating-point weights and activations in DNNs with low-bit representations to reduce memory usage and speed up computations. A general N-bit quantizer function will have \(2^{N}\) levels, say \(l_{1},l_{2},\ldots,l_{2^{N}}\), \(2^{N}-1\) transition thresholds, say \(t_{1},t_{2},\ldots,t_{2^{N}-1}\), and is defined as follows:

\[Q(x)=\begin{cases}l_{1}&\text{if }x<t_{1}\\ l_{i}&\text{if }t_{i-1}\leq x<t_{i},\quad i=2,3,\ldots,2^{N}-1\\ l_{2^{N}}&\text{if }x\geq t_{2^{N}-1}\end{cases}\] (1)

**Quantizer Model:** We introduce an \(N\)-dimensional learnable vector \(r\in\mathbb{R}^{N}\), which defines the N bit multipliers, alongside a scalar offset \(c\) in our quantizer model. The set of allowed quantized weights or activations is given by:

\[W_{r}=\left\{\langle r,b\rangle+c\mid b\in\{0,1\}^{N}\right\}\] (2)

The quantization function maps each full-precision weight to its nearest quantized counterpart:

\[\hat{x}=Q(x,r)=\arg\min_{w_{q}\in W_{r}}\mid x-w_{q}\mid\] (3)

This design enables a flexible non-uniform quantizer with multiple step sizes, offering hardware efficiency while preserving the structure of N-bit quantization. Although learning all \(2^{N}\) quantization levels would offer maximum flexibility, it would undermine hardware efficiency and the core benefits of N-bit quantization. Figure 0(a) illustrates a sample quantizer function. Drawing parallels between a general N-bit quantizer and the one introduced above, we can see that the elements of the set \(W_{r}\) serve as the levels, \(l_{1},l_{2},\ldots,l_{2^{N}}\), and the transition thresholds are defined as \(t_{i}=(l_{i}+l_{i+1})/2\).

**Loss and Learning:** We jointly optimize the bit multipliers, offsets, and weights by introducing an additional quantization-aware loss alongside the standard cross-entropy loss. This allows the model parameters to be optimized through backpropagation within the usual training pipeline. During training, the weights remain in full precision but gradually align with their quantized counterparts due to the influence of the quantization-aware loss. The actual quantization is applied post-training, where the full-precision weights are mapped to their nearest quantized values.

**Quantization-Aware Loss:** We define a regularization loss that minimizes the squared error between each weight and its nearest quantized value. To balance gradient contributions across layers, we introduce a layer-specific scaling factor. The total loss is formulated as:

\[\mathcal{L}=\mathcal{L}_{CE}+\lambda\sum_{l=1}^{L}\alpha_{l}\sum_{i=1}^{n_{l}} \min_{w_{q}\in W_{r}^{l}}|\ w_{i}-w_{q}\ |^{2}\] (4)

where \(\mathcal{L}_{CE}\) is the cross-entropy loss and \(W_{r}^{l}\) represents the set of quantized weights for layer \(l\), defined by parameters \(r^{l}\) and \(c^{l}\). The term \(\alpha_{l}\) is a layer-wise scaling factor, and \(\lambda\) controls the regularization strength. Following other works[16], we set \(\alpha_{l}\) as \(\nicefrac{{1}}{{\sqrt{N\cdot Q_{P}}}}\), where \(Q_{P}\) is \(2^{b}-1\) for activations (unsigned data) and \(2^{b-1}-1\) for weights (signed data), respectively; \(b\) denotes the number of bits. Figure 0(b) illustrates the regularization loss for a sample weight using an arbitrary vector \(r\) to define the quantized weight set. Equivalently, the loss can be expressed as a function of the weights and bit multipliers. This formulation jointly optimizes the overall objective and the quantization parameters, including the bit multipliers and offsets that define the quantization function itself.

\[\mathcal{L}=\mathcal{L}_{CE}+\lambda\sum_{l=1}^{L}\alpha_{l}\sum_{i=1}^{n_{l}} |\ w_{i}-Q(w_{i},r^{l})\ |^{2}\] (5)

**Gradient Calculation:** The gradient calculation for the weights and quantizer parameters is straightforward. Since we use full precision weights throughout the training, we can simply define \(\frac{\partial Q(w,r)}{\partial w}=0\), thereby eliminating the need of any gradient approximation techniques. For the quantizer parameters, \(\frac{\partial Q(w,r)}{\partial c}=1\) and \(\nabla_{r}Q(w,r)=B_{r}(Q(w,r))\), where \(B_{r}\) is an inverse map defined as \(B_{r}:W_{r}\rightarrow\{0,1\}^{N}\), providing the bit representation vector of the quantized weights. This encoding function satisfies \(w_{q}=\langle r,B_{r}(w_{q})\rangle+c\forall w_{q}\in W_{r}\). The gradients for the weights, bit multipliers, and offsets are calculated as follows:

\[\frac{\partial L}{\partial w}=\frac{\partial L_{CE}}{\partial w}+2\lambda \cdot\alpha_{l}\cdot(w-Q(w,r^{l}))\] (6)

\[\frac{\partial L}{\partial r^{l}}=2\lambda\cdot\alpha_{l}\sum_{i=1}^{n_{l}}(w _{i}-Q(w_{i},r^{l}))\cdot B_{r}(Q(w_{i},r^{l}))\] (7)

\[\frac{\partial L}{\partial c^{l}}=2\lambda\cdot\alpha_{l}\sum_{i=1}^{n_{l}}(w _{i}-Q(w_{i},r^{l}))\cdot(-\frac{\partial Q(w_{i},r^{l})}{\partial c^{l}})=2 \lambda\cdot\alpha_{l}\sum_{i=1}^{n_{l}}(Q(w_{i},r^{l})-w_{i})\] (8)

**SNN Training:** SNNs inherently produce _quantized activations_ in the form of _spike trains_, we thus need to solely quantize the weights of the network. We use a Leaky Integrate-and-Fire (LIF) model [27] for the spiking neuron in our SNN models. These discrete-time equations describe its dynamics:

\[H[t] =V[t-1]+\beta(X[t]-(V[t-1]-V_{reset}))\] (9) \[S[t] =\Theta(H[t]-V_{th})\] (10) \[V[t] =H[t]\ \left(1-S[t]\right)+V_{reset}\ S[t]\] (11)

Figure 1: **(a)** Non-uniform quantizer model demonstrating the learned bit-multiplier quantization function. **(b)** QAT loss with (MSE) regularization (shown in **red**) to minimize quantization error.

where \(X[t]\) denotes the input current at time step \(t\). \(H[t]\) denotes the membrane potential following neural dynamics and \(V[t]\) denotes the membrane potential after a spike at step \(t\), respectively. The model uses a firing threshold \(V_{th}\) and utilizes the Heaviside step function \(\Theta(x)\) to determine spike generation. The output spike at step \(t\) is denoted by \(S[t]\), while \(V_{reset}\) represents the reset potential following a spike. The membrane decay constant is denoted by \(\beta\). To facilitate error backpropagation, we use the surrogate gradient method [28], defining \(\Theta^{\prime}(x)\triangleq\sigma^{\prime}(x)\), where \(\sigma(x)\) is the arctan surrogate function [29]. The remaining part of the training and quantization follows that of the non-spiking networks described earlier.

**Fault-Aware Modification:** We propose a two-pronged approach to address SA faults in quantized neural networks. Firstly, we enhance fault awareness during training by periodically (every 4 epochs) loading faulty weights onto the model. Secondly, we introduce a fault-aware modification to our algorithm, designed to avoid weight configurations rendered impossible by SA faults. We introduce a _validity_ term that constrains weights to only those quantization levels that are achievable, avoiding those rendered unreachable by faulty bits. The _validity_ term is defined for each layer as a binary map that indicates whether a specific weight can attain a given quantization level (1 if achievable, 0 otherwise). This allows us to modify the quantization-aware training loss in Equation 4 as follows:

\[\mathcal{L}=\mathcal{L}_{CE}+\lambda\sum_{l=1}^{L}\alpha_{l}\sum_{i=1}^{n_{l}} \min_{w_{q}\in W_{r}^{l}}(val_{i,q}^{l}\mid w_{i}-w_{q}\mid^{2}+(1-val_{i,q}^{ l})\cdot\Delta)\] (12)

Here, \(val_{i,q}^{l}\) represents the _validity_ term for weight \(w_{i}\) in layer \(l\) with respect to the quantization level \(w_{q}\in W_{r}^{l}\). If \(w_{i}\) can reach \(w_{q}\), then \(val_{i,q}^{l}=1\); otherwise, \(val_{i,q}^{l}=0\). The term \(\Delta\) is a large constant that penalizes unreachable quantization levels, effectively excluding them from the optimization.

## 3 Experiments

We initialize quantized networks with weights from a trained full-precision model of the same architecture, then fine-tune in the quantized space, which has been proven to improve performance [30; 31; 32]. We quantize input activations and weights to 3- or 4-bits for all matrix multiplication layers except the first and last, which use 8-bits. This approach is commonly used for quantizing deep networks, and has been proven to increase effectiveness at the cost of minimal overhead [16]. The weights and the quantization parameters: bit mutlpliers and the offset values, are trained using SGD with a momentum of 0.9 and a cosine learning rate decay schedule [33]. We sweep over different values of the regularization hyperparameter \(\lambda\) and chose \(\lambda=100\) for our results.

**ANN Training Details.** We use the ResNet-18 [34] architecture for experiments on CIFAR-10 [23] and ImageNet [24] datasets. Models are trained for 200 epochs on CIFAR-10 and 90 epochs on ImageNet with the weights having a learning rate of 0.01 and 0.1 respectively. The other parameters are trained with a learning rate of 0.001. For ImageNet, we preprocess images by resizing them to \(256\times 256\) pixels. During training, we apply random \(224\times 224\) crops and horizontal flips half the time. At inference, we use a center crop of \(224\times 224\). For CIFAR-10, we augment the training data by padding images with 4 pixels on each side, then taking random 32x32 crops. We also apply random horizontal flips half the time. The results are shown in Table 1 and 2.

**SNN Training Details.** We use the ResNet-19 [35] and VGG-11 [36] models, after adapting them to SNNs. Specifically, we replace all ReLU activation functions with LIF modules and substitute max-pooling layers with average pooling operations. We follow the implementation and data augmentation technique used in NDA [37] as our baseline training method. The weights and the other parameters are trained with a learning rate of 0.01 and 0.001 respectively. We evaluate on the N-Caltech 101 and CIFAR10-DVS benchmarks. N-Caltech 101 consists of 8,831 DVS images converted from the original Caltech 101 dataset, while CIFAR10-DVS comprises 10,000 DVS images derived from the original CIFAR10 dataset. For both these datasets, we apply a 9:1 train-validation split and resize all images to \(48\times 48\). Each sample is temporally integrated into 10 frames using spikingjelly [38]. \(V_{reset}\) is set to 0 and the membrane decay \(\beta\) is \(0.25\). Our results are presented in Table 3.

**Fault-Aware Training.** We evaluate our method on the VGG-13 architecture, training with 3-bit and 4-bit precision for both weights and activations on the CIFAR-10 dataset. Our experiments consider varying levels of SA fault density. Figures 1(a) and 1(b) illustrate the efficacy of our approach for 4-bit and 3-bit quantization, respectively.

Results and Analysis

**Comparison with Baselines.** Tables 1 and 2 present our quantized ANN results for CIFAR10 and ImageNet, respectively. Our method outperforms existing approaches, with 4-bit ResNet-18 (**W4/A4** refers to 4-bit weights and 4-bit activations) achieving a \(0.24\%\) accuracy increase over full-precision (FP) on CIFAR-10 and matching FP performance on ImageNet. For 4-bit quantized SNNs (Table 3), we observe performance gains on N-Caltech 101 and marginal losses on CIFAR10-DVS compared to FP. We attribute occasional performance improvements in both 4-bit ANNs and SNNs to the regularization effect of our quantization loss.

**Robustness to Faults.** SA faults represent extreme non-idealities in hardware, with each faulty bit halving the range of possible weight values. High device variability in conductance states can similarly cause significant discrepancies between expected and realized weights. Our approach, combining periodic loading of faulty weights during training with a fault-aware modified QAT algorithm, demonstrates robust performance even under high SA fault densities.

**Hardware Compatibility.** Figures 2(a) and 2(b) illustrate implementations of custom bit multipliers in analog and digital crossbar arrays. For analog arrays, the implementation incurs no additional cost, requiring only the adjustment of bit-multiplier conductance values from power-of-2 proportions to custom values. In digital arrays, the multiply-accumulate operation remains fully fixed-point, and the custom bit-multiplier scaling can be absorbed into the floating-point scaling operation (which is

\begin{table}
\begin{tabular}{l|c c c} \hline \hline
**Method** & **FP** & **W4/A4 (\(\Delta\) FP)** & **W3/A3 (\(\Delta\) FP)** \\ \hline L1 Reg [39] & \(93.54\) & \(89.98\) (\(-3.56\)) & - \\ BASQ [40] & \(91.7\) & \(90.21\) (\(-1.49\)) & - \\ LTS [41] & \(91.56\) & \(91.7\) (\(+0.1\)) & \(90.58\) (\(-0.98\)) \\ PACT [17] & \(91.7\) & \(91.3\) (\(-0.4\)) & \(91.1\) (\(-0.6\)) \\ LQ-Nets [19] & \(92.1\) & - & \(91.6\) (\(-0.5\)) \\ \hline LCQ [20] & \(93.4\) & \(93.2\) (\(-0.2\)) & \(92.8\) (\(-0.6\)) \\ \hline
**Ours (N-Multipliers)** & \(93.26\) & \(\mathbf{93.50}\) (\(+\mathbf{0.24}\)) & \(\mathbf{92.84}\) (\(-\mathbf{0.42}\)) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy (%) for 3- and 4- bit quantized ResNet-18 models on CIFAR-10. FP denotes full-precision accuracy, \(\Delta\) FP denotes difference in performance compared to the corresponding FP network. **Best/**second best relative performances for each bit-width are marked in **bold**/**underlined.

\begin{table}
\begin{tabular}{l l|c c} \hline \hline
**Method** & **Type** & **FP** & **W4/A4 (\(\Delta\) FP)** \\ \hline L1 Reg [39] & No QAT & \(69.7\) & \(57.5\) (\(-12.5\)) \\ SinReQ [14] & Sine reg. & \(70.5\) & \(64.6\) (\(-5.9\)) \\ LTS [41] & Lottery & \(69.6\) & \(68.3\) (\(-1.3\)) \\ PACT [17] & Learned scale & \(69.7\) & \(69.2\) (\(-0.5\)) \\ LQ-Nets [19] & Linear non-uniform & \(70.3\) & \(69.3\) (\(-1.0\)) \\ QIL [42] & Non-linear & \(70.2\) & \(70.1\) (\(-0.1\)) \\ QSin [13] & Sine reg. & \(69.8\) & \(69.7\) (\(-0.1\)) \\ LCQ [20] & Non-linear & \(70.4\) & \(\mathbf{71.5}\) (\(+\mathbf{1.1}\)) \\ \hline Ours & Fixed levels & \(69.6\) & \(68.2\) (\(-1.4\)) \\ Ours (N-Mult) & Linear non-uniform & \(69.6\) & \(69.6\) (\(-0.0\)) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy (%) for 4-bit quantized ResNet-18 models on ImageNet. **Best/**second best performances are marked in **bold**/**underlined.

\begin{table}
\begin{tabular}{l l|c c} \hline \hline
**Dataset** & **Model** & **FP** & **W4 (\(\Delta\) FP)** \\ \hline CIFAR10-DVS & Spiking VGG-11 & \(71.92\) & \(71.84\) (\(-0.08\)) \\ CIFAR10-DVS & Spiking ResNet-19 & \(72.91\) & \(72.14\) (\(-0.77\)) \\ N-Caltech 101 & Spiking VGG-11 & \(73.19\) & \(74.18\) (\(+0.99\)) \\ N-Caltech 101 & Spiking ResNet-19 & \(75.27\) & \(75.93\) (\(+0.66\)) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy (%) for 4- bit quantized SNNs on CIFAR10-DVS and N-Caltech 101.

common to all quantization schemes), thus eliminating any overhead caused by floating-point bit-multipliers. Learning custom bit multipliers within QAT enables highly effective low-bit quantization models that are compatible with standard in-memory computing architectures.

## 5 Conclusion and Future Work

We introduce a novel algorithm for learning bit multipliers within QAT, enabling efficient low-bit quantization models with learnable, non-uniform levels compatible with in-memory computing architectures. Our approach demonstrates minimal accuracy drops for 3- and 4-bit models compared to FP baselines across various datasets and architectures, including CIFAR-10 and ImageNet using ResNet-18, and CIFAR10-DVS and N-Caltech 101 using spiking VGG-11 and ResNet-19. Notably, our quantized models occasionally outperform their FP counterparts. We further extend our method to address SA faults, maintaining performance with up to \(30\%\) faulty bits. Future directions include extending the method to channel-specific quantizers, conducting fault-aware training experiments on additional benchmarks, expanding ANN and SNN model evaluations, and exploring sub-3-bit quantization. These advancements aim to enhance the efficiency and robustness of quantized neural networks for resource-constrained environments and hardware non-idealities.

Figure 3: **(a)** Analog implementation. **(b)** Digital implementation. While uniform quantization uses bit-multipliers \((r_{0}^{l},r_{1}^{l},r_{2}^{l},r_{3}^{l})\) in powers-of-2 proportions \((1,2,4,8)\), we propose learning custom multiplier factors instead.

Figure 2: Performance preservation with SA faults: periodic faulty weight loading maintains accuracy for low fault densities; our fault-aware modified QAT extends robustness to high fault fractions.

## References

* [1]N. Kukreja, A. Shilova, O. Beaumont, J. Huckelheim, N. Ferrier, P. Hovland, and G. Gorman (2019) Training on the edge: the why and the how. In 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pp. 899-903. Cited by: SS1.
* [2]Y. Chen, T. Yang, J. Emer, and V. Sze (2019) Eyeriss v2: a flexible accelerator for emerging deep neural networks on mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems9 (2), pp. 292-308. Cited by: SS1.
* [3]Y. Chih, P. Lee, H. Fujiwara, Y. Shih, C. Lee, R. Naous, Y. Chen, C. Lo, C. Lu, H. Mori, et al. 16.4 an 89tops/w and 16.3 tops/mm 2 all-digital sram-based full-precision compute-in memory macro in 22nm for machine-learning edge applications. In 2021 IEEE International Solid-State Circuits Conference (ISSCC), Vol. 64, pp. 252-254. Cited by: SS1.
* [4]H. Jia, H. Valavi, Y. Tang, J. Zhang, and N. Verma (2020) A programmable heterogeneous microprocessor based on bit-scalable in-memory computing. IEEE Journal of Solid-State Circuits55 (9), pp. 2609-2621. Cited by: SS1.
* [5]J. Seo, J. Saikia, J. Meng, W. He, H. Suh, Y. Liao, A. Hasssan, I. Yeo, et al. (2022) Digital versus analog artificial intelligence accelerators: advances, trends, and emerging designs. IEEE Solid-State Circuits Magazine14 (3), pp. 65-79. Cited by: SS1.
* [6]L. Han, R. Pan, Z. Zhou, H. Lu, Y. Chen, H. Yang, P. Huang, G. Sun, X. Liu, and J. Kang (2024) Comm: algorithm-hardware co-design platform for non-volatile memory based convolutional neural network accelerators. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Cited by: SS1.
* [7]H. Sun, Z. Zhu, C. Wang, X. Ning, G. Dai, H. Yang, and Y. Wang (2023) Gibbon: an efficient co-exploration framework of nn model and processing-in-memory architecture. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Cited by: SS1.
* [8]X. Peng, S. Huang, H. Jiang, A. Lu, and S. Yu (2020) Dnn+ neurosim v2. 0: an end-to-end benchmarking framework for compute-in-memory accelerators for on-chip training. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems40 (11), pp. 2306-2319. Cited by: SS1.
* [9]M. J. Rasch, D. Moreda, T. Gokmen, M. Le Gallo, F. Carta, C. Goldberg, K. El Maghraoui, A. Sebastian, and V. Narayanan (2021) A flexible and fast pytorch toolkit for simulating training and inference on analog crossbar arrays. In 2021 IEEE 3rd international conference on artificial intelligence circuits and systems (AICAS), pp. 1-4. Cited by: SS1.
* [10]C. Lammie, W. Xiang, B. Linares-Barranco, and M. Rahimi Azghadi (2022) Memtorch: an open-source simulation framework for memristive deep learning systems. Neurocomputing485, pp. 124-133. Cited by: SS1.
* [11]M. Hanif and M. Shafique (2023) Faq: mitigating the impact of faults in the weight memory of dnn accelerators through fault-aware quantization. arXiv preprint arXiv:2305.12590. Cited by: SS1.
* [12]T. Liu, W. Wen, L. Jiang, Y. Wang, C. Yang, and G. Quan (2019) A fault-tolerant neural network architecture. In Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1-6. Cited by: SS1.
* [13]K. Solodskikh, V. Chikin, R. Aydarkhanov, D. Song, I. Zhelavskaya, and J. Wei (2022) Towards accurate network quantization with equivalent smooth regularizer. In European Conference on Computer Vision, pp. 727-742. Cited by: SS1.
* [14]A. T. Elthakeb, P. Pilligundla, and H. Esmaeilzadeh (2019) Sinreq: generalized sinusoidal regularization for low-bitwidth deep quantized training. arXiv preprint arXiv:1905.01416. Cited by: SS1.
** [15] Matthias Wess, Sai Manoj Pudukotai Dinakarrao, and Axel Jantsch. Weighted quantization-regularization in dnns for weight memory minimization toward hw implementation. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_, 37(11):2929-2939, 2018.
* [16] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. _arXiv preprint arXiv:1902.08153_, 2019.
* [17] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. _arXiv preprint arXiv:1805.06085_, 2018.
* [18] Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Wen Ji, Yaowei Wang, and Wenwu Zhu. Mixed-precision neural network quantization via learned layer-wise importance. In _European Conference on Computer Vision_, pages 259-275. Springer, 2022.
* [19] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In _Proceedings of the European conference on computer vision (ECCV)_, pages 365-382, 2018.
* [20] Kohei Yamamoto. Learnable companding quantization for accurate low-bit neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5029-5038, 2021.
* [21] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. _Neural networks_, 10(9):1659-1671, 1997.
* [22] Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence with neuromorphic computing. _Nature_, 575(7784):607-617, 2019.
* [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* [25] Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi. Cifar10-dvs: an event-stream dataset for object classification. _Frontiers in neuroscience_, 11:309, 2017.
* [26] Garrick Orchard, Ajinkya Jayawant, Gregory K Cohen, and Nitish Thakor. Converting static image datasets to spiking neuromorphic datasets using saccades. _Frontiers in neuroscience_, 9:437, 2015.
* [27] Wulfram Gerstner and Werner M Kistler. _Spiking neuron models: Single neurons, populations, plasticity_. Cambridge university press, 2002.
* [28] Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. _IEEE Signal Processing Magazine_, 36(6):51-63, 2019.
* [29] Wei Fang, Zhaofei Yu, Yanqi Chen, Timothee Masquelier, Tiejun Huang, and Yonghong Tian. Incorporating learnable membrane time constant to enhance learning of spiking neural networks. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 2661-2671, 2021.
* [30] Jeffrey L McKinstry, Steven K Esser, Rathinakumar Appuswamy, Deepika Bablani, John V Arthur, Izzet B Yildiz, and Dharmendra S Modha. Discovering low-precision networks close to full-precision networks for efficient embedded inference. _arXiv preprint arXiv:1809.04191_, 2018.
* [31] Wonyong Sung, Sungho Shin, and Kyuyeon Hwang. Resiliency of deep neural networks under quantization. _arXiv preprint arXiv:1511.06488_, 2015.

* [32] Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. _arXiv preprint arXiv:1711.05852_, 2017.
* [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_, 2016.
* [34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [35] Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger spiking neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11062-11070, 2021.
* [36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [37] Yuhang Li, Youngeun Kim, Hyoungseob Park, Tamar Geller, and Priyadarshini Panda. Neuromorphic data augmentation for training spiking neural networks. In _European Conference on Computer Vision_, pages 631-649. Springer, 2022.
* [38] Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timothee Masquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi Li, and Yonghong Tian. Spikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence. _Science Advances_, 9(40):eadi1480, 2023.
* [39] Milad Alizadeh, Arash Behboodi, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, and Max Welling. Gradient l1 regularization for quantization robustness. _arXiv preprint arXiv:2002.07520_, 2020.
* [40] Han-Byul Kim, Eunhyeok Park, and Sungjoo Yoo. Basq: Branch-wise activation-clipping search quantization for sub-4-bit neural networks. In _European Conference on Computer Vision_, pages 17-33. Springer, 2022.
* [41] Yunshan Zhong, Gongrui Nan, Yuxin Zhang, Fei Chao, and Rongrong Ji. Exploiting the partly scratch-off lottery ticket for quantization-aware training. _arXiv preprint arXiv:2211.08544_, 2022.
* [42] Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4350-4359, 2019.