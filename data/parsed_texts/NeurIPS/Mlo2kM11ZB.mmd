# Birder: Communication-Efficient 1-bit Adaptive Optimizer for Practical Distributed DNN Training

Hanyang Peng\({}^{1}\)1, Shuang Qin\({}^{1}\)1, Yue Yu\({}^{1}\)2, Jin Wang\({}^{1}\), Hui Wang\({}^{1}\), Ge Li\({}^{2}\)2

\({}^{1}\)Peng Cheng Laboratory, Shenzhen, China

\({}^{2}\)School of Electronic and Computer Engineering,

Shenzhen Graduate School, Peking University, Shenzhen, China

penghy@pcl.ac.cn, qinsh@pcl.ac.cn, yuy@pcl.ac.cn

wangj05@pcl.ac.cn, wangh06@pcl.ac.cn, geli@ece.pku.edu.cn

Equal ContributionCorresponding Author

Footnote 1: footnotemark:

###### Abstract

Various gradient compression algorithms have been proposed to alleviate the communication bottleneck in distributed learning, and they have demonstrated effectiveness in terms of high compression ratios and theoretical low communication complexity. However, when it comes to practically training modern deep neural networks (DNNs), these algorithms have yet to match the inference performance of uncompressed SGD-momentum (SGDM) and adaptive optimizers (_e.g._, Adam). More importantly, recent studies suggest that these algorithms actually offer no speed advantages over SGDM/Adam when used with common distributed DNN training frameworks ( _e.g._, _DistributedDataParallel (DDP)_) in the typical settings, due to heavy compression/decompression computation or incompatibility with the efficient _All-Reduce_ or the requirement of uncompressed warmup at the early stage. For these reasons, we propose a novel 1-bit adaptive optimizer, dubbed **B**inary **r**andomization **a**d**aptive **o**tim**er (**B**irder**). The quantization of **B**irder can be easily and lightly computed, and it does not require warmup with its uncompressed version in the beginning. Also, we devise _Hierarchical-1-bit-All-Reduce_ to further lower the communication volume. We theoretically prove that it promises the same convergence rate as the Adam. Extensive experiments, conducted on 8 to 64 GPUs (1 to 8 nodes) using _DDP_, demonstrate that **B**irder achieves comparable inference performance to uncompressed SGDM/Adam, with up to \(\mathbf{2.5\times}\) speedup for training ResNet-50 and \(\mathbf{6.3\times}\) speedup for training BERT-Base. Code is publicly available at https://openi.pcl.ac.cn/c2net_optim/Birder.

## 1 Introduction

With the rapid development of computational power, "bigger" and "bigger" deep neural network (DNN) models are proposed for expect better performance, from the early classical models, such as AlexNet(61 million parameters) [15], and ResNet (ResNet-50: 20.5 million parameters) [12] to the current large language models (LLMs), such as BERT (BERT-Lagre: 340 million parameters)[10], and GPT (GPT-3: 176 billion parameters)[5]. Scalable parallelism across distributed computing workers for training these large-scale models becomes a necessity. During training, millions to billions of parameters need to be communicated among workers at each iteration, so distributed large-scale DNN training almost invariably suffers from the communication bottleneck.

To address the communication bottleneck, a wide variety of lossy gradient compression algorithms have been proposed to lower the communication overhead. The algorithms can be broadly dividedinto three categories based on the compression techniques, including low-precision approximation (_e.g._, SignSGD[4], TernGrad[31], and QSGD[3], 1-bit Adam[28]), low-rank simplification (_e.g._, ATOMO[30], PowerSGD[29], and GradZip[9]), and sparsification (_e.g._, Random-\(k\)[27], Top-\(k\)[2], and MSRTop-\(k\)[24]). In addition to the specific compression techniques, some other works, such as Error Feedback(EF)[23][26], EF21[21], DIANA[19] and MARINA[11], focus on changing the compression objects from the gradient to the gradient and delayed error summation, or the gradient differences to mitigate compressing errors and/or accelerate the convergence rate.

Gradient compression algorithms have demonstrated promising results with a high compression ratio and low oracle/communication complexity in theory. However, when practically training DNNs, they are still **inferior to** uncompressed SGDM/Adam in terms of inference performance. This is because, these gradient compression algorithms are SGD-type optimizers, which will be commonly reduced to vanilla SGD without momentum if compression is not employed. The performance for a compressed optimizer is commonly upper bounded by its uncompressed counterpart, while vanilla SGD is typically less effective than SGDM for training DNNs. Particularly, SGD-type optimizers are known to be substantially inferior to adaptive optimizers (_e.g._, Adam) for training Transformer-based networks[17][34][7], which have become predominant in the DNN community. _Supporting empirical evidences for this phenomenon can be found in Section B in the Appendix._ Furthermore, if we apply the techniques of gradient compression algorithms to compress and communicate the gradients, and subsequently utilize the compressed gradients to construct adaptive optimizers in the local nodes, the final performance will be degraded[28]. Therefore, designing native communication-compression adaptive optimizers is an underexplored problem that requires further research.

As for the system-level speed, recent studies ([33],[1]) pointed out, when distributedly training typical DNN models (_e.g._, ResNet-50 and BERT-Base) with off-the-shelf _DistributedDataParallel (DDP)_ at typical bandwidths (_e.g._, 10Gbps), existing gradient compression algorithms are still **slower** than uncompressed SGDM/Adam. This is because, the compressor for these algorithms are either quantization or sparsification or low-rank simplification, which exhibit one or more weaknesses below. (\(i\)) Vector-wise quantization and low-rank simplification compressors are commonly computationally heavy, and their time cost, in some cases, is close to and even larger than the savings from the reduces communications, as empirical evidence has shown in [33] ; (\(ii\)) Sparsification compressors and bias quantization compressor are not naively combatable with the efficient communication primitive_All-Reduce_ due to their inherent structures, and they have to utilize _All-Gather_ for aggregation in stead, which will significantly slow down the communication speed, as empirically shown in [1] ; (\(iii\)) Some low-rank simplification compressors[29] and quantization compressors[28][18] need to harness their uncompressed counterparts for warm-up at the early stage to stabilize the convergence, and the warm-up time is commonly nontrivial which to some extent renders their high compression ratios vacuous. Therefore, from a system-level perspective, the design ethos of a system-efficient communication-compression algorithm is that we should guarantee that the compression/decompression of the algorithm is computationally light and takes less time, and it should also be friendly to efficient collective communication primitives. Additionally, there is no need to resort to an uncompressed optimizer for warm-up.

To this end, we propose a 1-bit adaptive optimizer, called **B**inary **r**andomization **a**d**aptive **o**m**(**B**irder), which use the following updating rule is \(x_{t+1}=x_{t}-\alpha_{t}\mathcal{Q}\left(\frac{m_{t}}{b_{t}}\right)\) where \(m_{t}=\beta m_{t-1}+(1-\beta)g_{t}\), \(b_{t}=\beta b_{t-1}+(1-\beta)|g_{t}|\) and \(g_{t}\) is the gradient, and \(\mathcal{Q}(\cdot)\) is a element-wise binary quantization operator. The main difference between **B**irder and existing gradient-quantization algorithms is that we directly quantize the entire adaptive update \(\frac{m_{t}}{b_{t}}\) rather than quantize the gradient \(g_{t}\) or the momentum \(m_{t}\). Because \(-1\leq\frac{(m_{t})_{j}}{(b_{t})_{j}}\leq 1\), where \((m_{t})_{j}\), \((b_{t})_{j}\) are the \(j^{th}\) element of \(m_{t}\), \(b_{t}\) respectively, each element of \(\frac{m_{t}}{b_{t}}\) is easy to be randomly quantized to \(1\) or \(-1\) in probability, making the quantization computationally light. Another advantage of **B**irder is that it does not require a full-precision optimizer to warm up at the early stage to ensure stable convergence. We also demonstrate **B**irder's convergence rate can match that of Adam. Moreover, taking into accost the nature of **B**irder, we devise an efficient hierarchical communication scheme to further speed up communication, which sufficiently leverages the ultra-high intra-bandwidth among GPUs within the same node.

In particular, we make the following key **contributions**:

* We propose a novel 1-bit optimizer, dubbed **B**irder, **which is a native communication-compression _adaptive_ **algorithm that _element-wise_ **quantizes the entire model update and does not need to leverage its uncompressed counterpart for warm-up**, making compression/decompression computationally light and the extreme quantization ratio exert its best function (Section 2).
* We theoretically prove that despite employing extreme 1-bit quantization is employed, **Birder still promise the same convergence speed as the full-precision Adam** (Section 3).
* We develop a new communication scheme for 1-bit communication, called _Hierarchical-1-bit-All-Reduce_, **which sufficiently harnesses the ultra-fast intra-connects to accelerate the local communication, and utilize more efficient commutation primitives to further reduce the communication overhead** (Section 4).
* We perform extensive distributed training experiments to demonstrate the effectiveness of the proposed algorithm. **As far as we know, running with _DDP, our algorithm is the first work to consistently trump SGDM/Adam in terms of entire running time at little/no inference performance cost**, reaching up to **2.47\(\times\)** speedup for ResNet-50 and **6.26\(\times\)** speedup for BERT-Base on \(64\) GPUs. (Section 5).

## 2 One-Bit Adaptive Optimizer Birder

In this section, we focus on solving the following problem for distributed training :

\[\min_{x\in\mathbb{R}^{d}}f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x;\xi^{(i)})\] (1)

where \(x\) is the \(d\)-dimensional model parameter, \(n\) is the number of distributed workers. \(\xi^{(i)}\) is the sampled min-batch data on the \(i\)-the worker. The sampled min-batch data on all the workers is independent and identically distributed (_i.i.d._). \(f_{i}(x;\xi^{(i)})\) is the loss function. Note that \(f_{i}(x;\xi_{i})\) is commonly abbreviated as \(f_{i}(x)\) in the following.

When distributedly training large-scale DNN models, using vanilla full-precision optimizers can cause communication bottleneck issues in gradient communication among workers at each iteration. To alleviate this problem, elegant SignSGD[4] was proposed, which merely takes the sign of each coordinate of the gradients. While this algorithm can substantially reduce the communication overhead, its practical performance is still inferior to popular adaptive optimizers, such as Adam. Fortunately, we observe that the mathematical formulations of SignSGD and Adam have close connections, providing an opportunity to propose a new optimizer that can combine their merits. This new optimizer can considerably reducing the communication volume with light computation, while maintaining fast convergence speed and high inference performance.

The mathematical updating rule of SignSGD can be formulated as:

\[x_{t+1}\gets x_{t}-\alpha_{t}\mathrm{Sign}(g_{t})=x_{t}-\alpha_{t}\frac{g _{t}}{|g_{t}|}\] (2)

where \(\alpha_{t}\) is the learning rate, \(g_{t}\) denotes the estimated unbias noisy gradient of \(f(x_{t})\) with random samples, \(\mathrm{Sign}(\cdot)\) is a element-wise signum, and \(|\cdot|\) is an element-wise absolute operator.

Whereas the updating rule of vanilla Adam[14] can be expressed as:

\[\begin{split}& m_{t}\leftarrow\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}, \\ & v_{t}\leftarrow\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2},\\ & x_{t+1}\gets x_{t}-\alpha_{t}\frac{m_{t}}{\sqrt{v_{t}}}, \end{split}\] (3)

where \(\beta_{1}\) and \(\beta_{2}\) represents the exponential moving average factors 3.

Footnote 3: For simplicity, we omit the bias correction for \(m_{t}\) and \(v_{t}\) and the small constant in the numerator.

If taking \(\beta_{1}\) and \(\beta_{2}\) to zero, \(\beta_{1},\beta_{2}\to 0\) in Eq. (3), Adam will be reduced to SignSGD.

Given the observations above, we propose a new optimizer that is an intermediate between SignSGD and Adam, referred to as Binder, _i.e._,

\[\begin{split}& m_{t}\leftarrow\beta m_{t-1}+(1-\beta)g_{t},\\ & b_{t}\leftarrow\beta b_{t-1}+(1-\beta)|g_{t}|,\\ & x_{t+1}\gets x_{t}-\alpha_{t}\mathcal{Q}\left(\frac{m_{t} }{b_{t}}\right),\end{split}\] (4)

where the \(j\)-th elements of \(m_{t},b_{t}\) rigorously satisfies \(-1\leq\frac{(m_{t})_{j}}{(b_{t})_{j}}\leq 1\), \(\mathcal{Q}(\cdot)\) is an element-wise quantization operator, and it quantizes the \(j\)-th element of \(\frac{m_{t}}{b_{t}}\) as follows:

\[\mathcal{Q}\left(\frac{(m_{t})_{j}}{(b_{t})_{j}}\right)=\left\{\begin{array}[] {ll}1,&\text{with probability }p=\frac{1}{2}(\frac{(m_{t})_{j}}{(b_{t})_{j}}+1)\\ -1,&\text{with probability }1-p\end{array}\right.,\] (5)

where \(\mathbb{E}\left(\mathcal{Q}\left(\frac{m_{t}}{b_{t}}\right)\right)=\frac{m_{1 }}{b_{t}}\), so \(\mathcal{Q}(\cdot)\) is unbiased, and proof is provided in Section A of the appendix. The detailed implementation of Binder in a parameter-server model is illustrated in Algorithm 1.

```
1:Input: all workers’s model parameter \(x_{0},x_{1}\), the \(i^{th}\) worker’s momentum \(m_{0}^{(i)}=0\), \(b_{0}^{(i)}=0\), the \(i^{th}\) worker’s local error \(e_{0}^{(i)}=0\), server’s global error \(\bar{e}_{0}=0\), exponential moving average factor \(\beta\), the threshold \(T_{0}\), and the learning rate sequence \(\{\alpha_{t}\}\).
2:for\(t=1,...,T\)do
3: (On the \(i^{th}\) worker)
4: Randomly sample \(\xi_{t}^{(i)}\) and compute local gradient: \(g_{t}^{(i)}=\nabla f_{i}(x_{t};\xi_{t}^{(i)})\)
5: Update the local \(m_{t}^{(i)}\): \(m_{t}^{(i)}=\beta m_{t-1}^{(i)}+(1-\beta)g_{t}^{(i)}\)
6: Update the local \(\hat{b}_{t}^{(i)}\): \(\hat{b}_{t}^{(i)}=\beta\hat{b}_{t-1}^{(i)}+(1-\beta)|g_{t}^{(i)}|\)
7: Update the local \(b_{t}^{(i)}\): if\(t>T_{0}\) {\(b_{t}^{(i)}=\max(b_{t}^{(i)},\hat{b}_{t}^{(i)})\)} else {\(b_{t}^{(i)}=\hat{b}_{t}^{(i)}\)} *
8: Quantize the local update: \(u_{t}^{(i)}=\mathcal{Q}(\frac{m_{t}^{(i)}}{b_{t}^{(i)}}+\epsilon_{t-1}^{(i)})\)
9: Update the local error feedback \(e_{t}^{(i)}:e_{t}^{(i)}=e_{t-1}^{(i)}+\frac{m_{t}^{(i)}}{b_{t}^{(i)}}-u_{t}^{(i)}\)
10: Send \(u_{t}^{(i)}\) to the server
11: (On server)
12: Average all received \(q_{t}\) and quantize it: \(\bar{u}_{t}=\mathcal{Q}(\frac{1}{n}\sum_{i=1}^{n}u_{t}^{(i)}+\bar{e}_{t-1})\)
13: Update the global error feedback \(\bar{e}_{t}:\bar{e}_{t}=\bar{e}_{t-1}+\frac{1}{n}\sum_{i=1}^{n}u_{t}^{(i)}- \bar{u}_{t}\)
14: Send back \(\bar{u}_{t}\) to all workers
15: (On the \(i^{th}\) worker)
16: Update the local model parameter \(x_{t+1}\): \(x_{t+1}=x_{t}-\alpha_{t}\bar{u}_{t}\)
17:endfor ```

**Algorithm 1**Binder

The appealing characters of Binder are summarized in the following:

* Compared to SGD-type optimizers, Adam provides a fast convergence rate in practice by adaptively preconditioning the gradients with \(v_{t}\). **Binder inherits this feature to accelerate convergence speed.** On the other hand, unlike Adam, Binder employs the same exponential moving average factor \(\beta\) for both \(m_{t}\) and \(b_{t}\). **This eliminates the need for bias correction and reduces the amount of tuning work required.**
* Existing quantization optimizers are built upon _vector-wise quantization_[3][19][22]4

Footnote 4: The typical gradient-quantized optimizer _QSGD_ quantizes the gradient as follows:

\[\mathcal{Q}\left((g_{t})_{j}\right)=\left\{\begin{array}{ll}\|g_{t}\|_{p} \text{sign}((g_{t})_{j})\cdot\frac{r}{s},&\text{with probability }p_{i}=\frac{i|(g_{t})_{j}|}{\|g_{t}\|_{p}}-r\\ \|g_{t}\|_{p}\text{sign}((g_{t})_{j})\cdot\frac{r+1}{s},&\text{with probability }1-p_{i} \end{array}\right.\]

 where \(\|g_{t}\|_{p}=(\sum_{j}|(g_{t})_{j}|^{p})^{\frac{1}{p}}(p\geq 1)\), \(0\leq r<s\) (\(r,l\in\mathbb{N}\)) and \(\frac{s|(g_{t})_{j}|}{\|g_{t}\|_{2}}\in[\frac{r}{s},\frac{r+1}{s}]\). * E and estimate the sign of each element, which will renders the saved communication time cost somewhat meaningless. In contrast, Binder _element-wise_ quantizes the update, and the subtle design for \(m_{t}\) and \(b_{t}\) ensures the unquantized update is strictly bounded in the range \([-1,1]\), **allowing quantization is computed easily and lightly**. Further, unlike most quantization optimizers that only compress the gradients [6], **Binder performs the quantization for the entire adaptive update**, which further streamlines the optimization process.

**Remark.** We have noticed that the prior works 1-bit Adam ([28]) and its variants ([16], [18]) are also categorized as 1-bit adaptive optimizers. However, the design ethos of 1-bit Adam and Binder differ significantly. 1-bit Adam is still built on gradient quantization and and essentially functions as a preconditioned SGDM. 1-bit Adam runs full-precision Adam in the beginning (warm-up phase) and utilizes it as a fixed precondition for SGDM during the rest of training (compression phase). There are three aspects that influence 1-bit Adam to indeed accelerate communication. _First_, the warm-up stage constitutes approximately 15%-25% of the total steps, which to some extent discounts the high quantization ratio. _Second_, the vector-wise quantization employed by 1-bit Adam necessitates extra computations, including the calculation of vector norms and estimation of element signs, which are then transmitted as additional data. These factors diminish the time savings achieved through reduced communication bits. _Third_, the vector-wise quantization technique employed by 1-bit Adam is not compatible with the common-used distributed framework _DDP_ (system-level engineered distributed framework). In _DDP_, communication data is uniformly divided into buckets of equal size on the sender's side to enhance communication efficiency. Consequently, when vector-wise quantization is used, communication data from a single layer may be divided into different buckets, resulting in substantial errors during restoration on the receiver's end.

## 3 Theoretical Analysis

In this section, we present the theoretical convergence guarantee for Binder (Algorithm 1). We first introduce some necessary assumptions.

**Assumption 1.[Bounded infimum]** _For any \(x\) and a constant \(f^{*}\), we have the objective value \(f(x)\geq f^{*}\)._

**Assumption 2.** [Lipschitz continuous gradient] _The gradient \(\nabla f(\cdot)\) is \(L\)-Lipschitz continuous, i.e.,, \(\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|_{2},\quad\forall x,y\in\mathbb{R}^{d}\)._

**Assumption 3.** [Unbias and independent noisy gradient] _The gradient with respect to the random samples on each worker and at a different time is independent identically distributed (i.i.d.), i.e., \(\mathbb{E}[g_{t}^{(i)}]=\nabla f(x_{t}),\forall t\geq 1\), \(g_{t}^{(i)}\) is independent of \(g_{t}^{(j)}\) for \(i\neq j\), and \(g_{t_{1}}^{(i)}\) is independent of \(g_{t_{2}}^{(j)}\) for \(t_{1}\neq t_{2}\)._

**Assumption 4.** [Bounded gradient] _The noisy gradient and the full-set gradient are bounded i.e., \(\|g_{t}^{(i)}\|\leq G,\quad\|\nabla f_{t}(x)\|\leq G,\quad\forall t\geq 1\)._

Under the assumptions above, we then present the theoretical convergence for Binder in Algorithm 1.

**Theorem 1.** _For Binder in Algorithm 1, under Assumption 1-4, assuming \((b_{t}^{(i)})_{j}\geq\rho>0\), \(\forall j\in[1,2,...,d]\)5, choosing \(\alpha_{t}=\frac{c}{\sqrt{t}}\), \(\forall t\in[1,2,...,T]\) and \(\alpha_{0}=\alpha_{1}\), and defining \(z_{1}=x_{1}+\alpha_{1}(\delta_{1}-e_{1})\) where \(\delta_{1}{=}\frac{1}{n}\sum_{i=1}^{n}\frac{m_{i}^{(i)}}{b_{1}^{(i)}}-\frac{ \sum_{i=1}^{n}m_{i}^{(i)}}{\sum_{i=1}^{n}b_{1}^{(i)}}\) and \(e_{1}{=}\frac{1}{n}\sum_{i=1}^{n}e_{1}^{(i)}+\bar{e}_{1}\), we then have the following_

Footnote 5: We commonly add a small constant to \(b_{t}\) to avoid zero denominators for numerical stability, which guarantees this assumption holds in practice.

\[\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}\|\nabla f(x_{t})\|^{2}\right]^{2} \leq\frac{C_{1}}{\sqrt{T}}+\frac{C_{2}(1+\log T)}{\sqrt{T}},\] (6)

_where_

\[C_{1} =cG\left(\mathbb{E}[f(z_{1})-f^{*}]+\frac{3c^{2}dL}{16}+\frac{ \beta cdG^{2}}{(1-\beta)\rho}+\frac{4cdG^{2}}{\rho}+\frac{c^{2}\beta^{2}LG^{2 }d}{\rho^{2}(1-\beta)^{2}}\right),\] \[C_{2} =c^{3}G\left(\frac{(8\beta^{2}+10\beta+5)L^{2}d}{(1-\beta)^{2}}+ \frac{G^{2}(1+L)}{2\rho^{2}}+2dL\right).\]The theoretical results suggested that 1-bit Binder essentially achieve the same convergence rate (\(O(\frac{1}{\sqrt{T}})\)) as the uncompressed Adam[14][8].

## 4 Hierarchical-1-bit-All-Reduce

The data communication for Binder is one-bit, which cannot be directly aggregated using the efficient _All-Reduce_. Additionally, there is a significant imbalance between the intra-node and inter-node bandwidths. If we attempt to aggregate the data uniformly from both intra-nodes and inter-nodes, the communication process will be hindered by the inter-node data exchanges, resulting in slower overall communication.

In light of the problems above, we propose a hierarchical communication scheme called _Hierarchical-1-bit-All-Reduce_. This scheme efficiently aggregates our 1-bit data by leveraging the ultra-high intra-node bandwidth and reducing the inter-node communication overhead. Assuming we have \(n\) nodes, each containing \(m\) GPUs, and the overall volume for each GPU needs to be communicated is \(P\),as visually depicted in Figure 1, the steps of _Hierarchical-1-bit-All-Reduce_ are as follows: (\(i\)) Each GPU performs _Reduce-Scatter_ to locally aggregate and scatter the data within its node. The communication volume for each GPU in this step is \(\frac{(m-1)P}{m}\). (\(ii\))Each GPU then applies Binder to quantize the data, resulting in a reduced volume of \(\frac{P}{32m}\) on each GPU. (\(iii\)) The GPU proceeds with _1-bit-All-Reduce_ to inter-aggregate the data. This step consists of two sub-steps: 1) Each GPU performs _All-to-All_ to collect the corresponding data from GPUs in other nodes, with a communication volume of \(\frac{(n-1)P}{32mn}\). 2) Each GPU averages and re-quantizes the data, followed by _All-Gather_ operation to gather the data. The communication volume in this sub-step is also \(\frac{(n-1)P}{32mn}\). (iv)Finally, each GPU performs _All-Gather_ to intra-aggregate the data, with a communication volume of \(\frac{(m-1)P}{32m}\).

Compared to the time cost of inter-node communication, the time cost of intra-node communication is relatively insignificant. Thus, when utilizing _Hierarchical-1-bit-All-Reduce_, the majority of the communication cost arises from the _1-bit All-Reduce_ step in Step (\(iii\)). The communication volume across nodes for all GPUs in this scheme is approximately \(\frac{2(n-1)P}{32}\). In contrast, if we were to simply employ the original _All-Gather_ to aggregate data, the communication volume across nodes for all GPUs would be approximately \(\frac{m^{2}n(n-1)P}{32}\). Consequently, _Hierarchical-1-bit-All-Reduce_ proves significantly more efficient than the original _All-Gather_.

Notably, the work [32] also introduces a 1-bit data communication scheme among nodes, but it can only guarantee the expected value of the gathered 1-bit data equals to the average of the original 1-bit data among nodes, thereby it will bring performance deterioration. In contrast, _All-to-All_ in _Hierarchical-1-bit-All-Reduce_ ensures the final data exactly equals to the average of the original data.

Figure 1: Paradigm of _Hierarchical-1-bit-All-Reduce_

## 5 Experiments

Recently, several works [33],[1] have shown that when utilizing the system-level engineered distributed data-parallel framework _DDP_, the existing communication-compression optimizers (excluding 1-bit Adam) still perform slower than the uncompressed SGDM/Adam. Therefore, in our evaluation, we focus on assessing the performance of Birder, the uncompressed SGDM/Adam, and the closely related algorithm 1-bit Adam through distributed training experiments using the benchmark models ResNet-50 (CNN) and BERT-Base (Transformer). **More extensive experiments can be founded in Section B of the appendix.**

\begin{table}
\begin{tabular}{|l|c|c c|c c|} \hline \multirow{2}{*}{Optimizer} & \multirow{2}{*}{\#GPUs} & \multicolumn{2}{c|}{32 samples per GPU} & \multicolumn{2}{c|}{128 samples per GPU} \\  & & \multicolumn{2}{c|}{Throughput} & \multicolumn{1}{c|}{Top-1 Acc. (\%)} & \multicolumn{1}{c|}{Throughput} & \multicolumn{1}{c|}{Top-1 Acc.(\%)} \\  & & (samples/s) & & & (samples/s) & \\ \hline SGDM & \multirow{3}{*}{8} & **3693 (1.00\(\times\))** & 76.19 & **5272 (1.00\(\times\))** & 75.05 \\
1-bit Adam & & 3243 (0.83\(\times\)) & 75.55 & 5229 (0.99\(\times\)) & 75.42 \\ Birder & & 3462 (0.94\(\times\)) & 75.98 & 5251 (0.99\(\times\)) & 75.45 \\ \hline SGDM & \multirow{3}{*}{16} & 2959 (1.00\(\times\)) & 75.96 & 6189 (1.00\(\times\)) & 74.61 \\
1-bit Adam & & 4745 (1.60\(\times\)) & 75.33 & 8836 (1.42\(\times\)) & 75.05 \\ Birder & & **6015 (2.03\(\times\))** & 75.53 & **9633 (1.56\(\times\))** & 75.09 \\ \hline SGDM & \multirow{3}{*}{32} & 4270 (1.00\(\times\)) & 75.47 & 9909 (1.00\(\times\)) & 74.54 \\
1-bit Adam & & 7268 (1.70\(\times\)) & 75.18 & 13827 (1.40\(\times\)) & 74.62 \\ Birder & & **9416 (2.21\(\times\))** & 75.27 & **15950 (1.61\(\times\))** & 74.82 \\ \hline SGDM & \multirow{3}{*}{64} & 6189 (1.00\(\times\)) & 75.37 & 16640 (1.00\(\times\)) & 74.22 \\
1-bit Adam & & 5546 (0.89\(\times\)) & 75.54 & 16426 (0.99\(\times\)) & 74.14 \\ Birder & & **15253 (2.47\(\times\))** & 75.30 & **23727 (1.43\(\times\))** & 74.24 \\ \hline \end{tabular}
\end{table}
Table 1: System throughput and Test Accuracy of SGDM, 1-bit Adam and Birder for training ResNet-50 on ILSVRC2012 from scratch with \(8,16,32,64\) GPUs.

Figure 2: Epoch-wise and time-wise convergence speed for training ResNet-50 with \(32\) samples per GPU, ResNet-50 with \(128\) samples per GPU, and fine tuning BERT-Base with \(3\) samples per GPU with \(64\) GPUs.

### Experimental Settings

Our experiments were conducted on a testbed consisting of 1, 2, 4, 8 nodes interconnected via 10Gbps Ethernet. Each node was equipped with 8 Nvidia Tesla A100-80GB GPUs. The hardware and software configurations were identical across all instances, with Ubuntu 20.04.4 LTS serving as the operating system. PyTorch 1.11.0 was used as the primary framework, accompanied by CUDA-11.6, cuDNN-8.2, NCCL-2.10.3, and PyTorch 1.11.0 for other relevant libraries. Notably, to ensure compatibility with PyTorch's _DDP_, certain components of Binder and our hierarchical communication scheme were implemented within the customized communication hook of _DDP_.

**Training details.** For the experiments over ResNet-50, we evaluate the convergence and performance of SGDM, 1-bit Adam and Binder on ILSVRC2012. The batch size per GPU is set to \(32\) or \(128\) with the standard input resolution \(224\times 224\). When employing _SGDM (baseline)_, the learning rate starts at \(0.1\times\frac{batch\ size}{256}\) with momentum of \(0.9\) and weight decay of \(0.0001\). When employing 1-bit Adam and Binder, the learning rate starts at \(0.001\times\frac{batch\ size}{256}\) with weight decay of \(0.0001\), and \([\beta_{1},\beta_{2}]\) for 1-bit Adam is set to \([0.9,0.999]\) and \(\beta\) for Binder is set to \(0.95\). Then, the learning rate is divided by \(10\) after \(30\), \(60\) and \(90\) epochs, and training is finally terminated after \(100\) epochs. Specifically, the first 15 epochs are used as the warmup stage for 1-bit Adam. For the experiments over BERT-Base, we access the convergence and performance of BertAdam (baseline), 1-bit Adam and Binder for SQuAD 1.1 fine-tuning task using a pre-trained BERT-Base model checkpoint from HuggingFace 6. The batch size per GPU is set to \(3\). We perform fine-tuning for 2 epochs. The learning rate linearly increases to \(1\times 10^{-4}\) steps in the early \(500\) steps and then linearly decreases to \(0\) in the rest iteration. Specifically, the first \(0.2\times\) steps are used as the warmup stage for 1-bit Adam. \([\beta_{1},\beta_{2}]\) for BertAdam, and 1-bit Adam is set to \([0.9,0.999]\) and \(\beta\) for Binder is set to \(0.9\).

Footnote 6: https://github.com/huggingface/transformers

### Experimental Results

Figure 2 shows the convergence behaviors of epoch-wise and time-wise training for SGDM / BertAdam (baseline), 1-bit Adam, and Binder using ResNet-50 and BERT-Base models running on 64 GPUs. The experimental results clearly demonstrate that Binder achieves a similar epoch-wise convergence rate compared to the baseline. However, the actual training speed of Binder surpasses both the baseline and 1-bit Adam by a significant margin.

Figure 3 illustrates the system throughput of different optimizers when running ResNet-50 and BERT-Base on 8 GPUs to 64 GPUs (1 node to 8 nodes). When training on 8 GPUs (1 node), where computation takes precedence over communication, the throughput of Binder is slightly lower than that of SGDM and BertAdam. However, as the number of GPUs increases, Binder consistently outperforms its counterparts, and this superiority becomes increasingly evident with more GPUs. Additionally, the system throughput for SGDM, BertAdam, and 1-bit Adam occasionally decreases as the number of GPUs increases, whereas the throughput of Binder steadily grows. This observation indicates that Binder offers better scalability efficiency.

\begin{table}
\begin{tabular}{|l|c|c c c|} \hline Optimizer & \#GPUs & Throughput & F1-Score (\%) & Exact Match (\%) \\  & & (samples/s) & & \\ \hline BertAdam & \multirow{2}{*}{8} & **413 (1.00\(\times\))** & 88.13 & 80.59 \\
1-bit Adam & & 358 (0.87\(\times\)) & 88.05 & 80.06 \\ Binder & & 412 (1.00\(\times\)) & 88.71 & 81.18 \\ \hline BertAdam & \multirow{2}{*}{16} & 84 (1.00\(\times\)) & 88.47 & 81.07 \\
1-bit Adam & & 213 (2.54\(\times\)) & 87.87 & 80.31 \\ Binder & & **431 (5.13\(\times\))** & 88.31 & 80.80 \\ \hline BertAdam & \multirow{2}{*}{32} & 119 (1.00\(\times\)) & 88.38 & 80.94 \\
1-bit Adam & & 274 (2.30\(\times\)) & 87.78 & 80.08 \\ Binder & & **730 (6.13\(\times\))** & 88.08 & 80.50 \\ \hline BertAdam & \multirow{2}{*}{64} & 158 (1.00\(\times\)) & 88.13 & 80.94 \\
1-bit Adam & & 252 (1.59\(\times\)) & 87.33 & 79.67 \\ Binder & & **990 (6.26\(\times\))** & 88.28 & 80.75 \\ \hline \end{tabular}
\end{table}
Table 2: System throughput and F1-Score / Exact-Match of BertAdam, 1-bit Adam and Binder for fine tuning BERT-base on SQuAD 1.1 with \(8,16,32,64\) GPUs.

In terms of inference performance for ResNet-50, we evaluate the Top-1 accuracy after training on ILSVRC2012 from scratch. For BERT-Base, we measure the F1-score and exact-match score after fine-tuning on SQuAD 1.1. Table 1 shows that when the batch size is set to 32 samples per GPU, the accuracy of Brider is slightly lower than that of SGDM. It has been suggested in some works ([13], [35]) that adaptive optimizers generally yield worse generalization compared to SGDM for CNN architectures. However, as the batch size increases (Table 2), both 1-bit Adam and Brider achieve better accuracy. This can be attributed to the beneficial effect of introducing a certain level of noise for generalization ([25]), which biases the optimizer towards wider valleys. Table 2 demonstrates that Brider achieves similar or higher F1-score and exact-match score compared to BertAdam and 1-bit Adam, validating the effectiveness of Brider for inference tasks.

### Communication Efficiency Analysis

As shown in Figure 4, training on a single node demonstrates that the baseline SGDM and BertAdam algorithms are slightly faster compared to Brider and 1-bit Adam. In this scenario, the inter-GPU bandwidth within a node is extremely high, rendering communication time negligible. However, the newly introduced compression/decompression process by Brider and 1-bit Adam adds extra time due to its implementation. Thanks to light-computation quantization, the compression/decompression time for Brider with ResNet-50 and BERT-Base is significantly reduced to approximately \(15ms\) and \(8ms\) respectively.When conducting distributed training across two nodes, the bandwidth between them is relatively limited (10Gbps in our experiment), making communication time a critical factor. In the case of uncompressed SGDM and BertAdam, the communication time substantially exceeds the computation time for ResNet with 32 samples per GPU and BERT-Base. Consequently, the system throughput is lower compared to a single node (as depicted in Figure 2). However, the extreme 1-bit quantization implemented in Brider effectively reduces communication overhead, resulting in only a marginal increase in the total time required for Brider. As the number of nodes continues to increase, the importance of an efficient communication scheme becomes paramount. By leveraging

Figure 4: Computation time, communication time and compression/decompression time per iteration of optimizers for training (a) ResNet-50 with \(32\) samples per GPU, (b)ResNet-50 with \(128\) samples per GPU, and (c) fine tuning BERT-Base with \(3\) samples per GPU with \(8\), \(16\), \(32\), \(64\) GPUs.

Figure 3: System throughput of optimizers for training (a) ResNet-50 with \(32\) samples per GPU, (b)ResNet-50 with 128 samples per GPU, and (c) fine tuning BERT-Base with \(3\) samples per GPU with \(8\), \(16\), \(32\), \(64\) GPUs.

our proposed _Hierarchical-1-bit-All-Reduce_, the overall inter-node communication volume exchanged scales proportionally with the number of nodes. In contrast, the _Compressed-All-Reduce_ method employed by 1-bit Adam[28] results in the overall communication volume exchanged among nodes being proportional to the number of GPUs (eight times larger than the number of nodes in our experiments). Consequently, as the number of nodes increases, the communication time for Binder exhibits a gradual rise, while the communication time for 1-bit Adam experiences a sudden surge.

## 6 Conclusion

In this study, we introduce a novel 1-bit adaptive optimizer for distributed training. Our optimizer offers the advantages of being lightweight in terms of computation while employing extreme 1-bit quantization for the communication data. Furthermore, we provide theoretical evidence demonstrating that Binder can achieve convergence rates comparable to the uncompressed Adam. To enhance communication speed, we propose a novel communication scheme tailored specifically for Binder, replacing the inefficient naive _All-Gather_ approach. Through extensive experiments on benchmark models such as ResNet-50 and BERT-Base, we validate the effectiveness and efficiency of Binder in comparison to uncompressed methods like SGDM/Adam as well as the relevant 1-bit Adam.

## Acknowledgments

This work is supported by the National R&D Program of China (Grant No. 2022ZD0115301), the Major Key Project of PCL (Grant No. PCL2023AS7-1), and the National Natural Science Foundation of China (Grant No. 61806128).

## References

* [1] Saurabh Agarwal, Hongyi Wang, Shivaram Venkataraman, and Dimitris Papailiopoulos. On the utility of gradient compression in distributed training systems. In _Proceedings of Machine Learning and Systems_, volume 4, pages 652-672, 2022.
* [2] Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. _arXiv preprint arXiv:1704.05021_, 2017.
* [3] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. In _Advances in neural information processing systems_, volume 30, 2017.
* [4] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. Signsgd: Compressed optimisation for non-convex problems. In _International Conference on Machine Learning_, pages 560-569, 2018.
* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 33:1877-1901, 2020.
* [6] Congliang Chen, Li Shen, Haozhi Huang, and Wei Liu. Quantized adam with error feedback. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 2021.
* [7] Jacques Chen, Frederik Kunstner, and Mark Schmidt. Heavy-tailed noise does not explain the gap between sgd and adam on transformers. In _13th Annual Workshop on Optimization for Machine Learning_, 2021.
* [8] Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. _arXiv preprint arXiv:1808.02941_, 2018.
* [9] Minsik Cho, Vinod Muthusamy, Brad Nemanich, and Ruchir Puri. Gradzip: Gradient compression using alternating matrix factorization for large-scale deep learning. In _Advances in Neural Information Processing Systems_, 2019.
* [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [11] Eduard Gorbunov, Konstantin P Burlachenko, Zhize Li, and Peter Richtarik. Marina: Faster non-convex distributed learning with compression. In _International Conference on Machine Learning_, 2021.

* [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [13] Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. _arXiv preprint arXiv:1712.07628_, 2017.
* [14] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* [15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* [16] Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit lamb: Communication efficient large-scale large-batch training with lamb's convergence speed. _arXiv preprint arXiv:2104.06069_, 2021.
* [17] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. _arXiv preprint arXiv:2004.08249_, 2020.
* [18] Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, and Yuxiong He. Maximizing communication efficiency for large-scale training via 0/1 adam. _arXiv preprint arXiv:2202.06009_, 2022.
* [19] Konstantin Mishchenko, Eduard Gorbunov, Martin Takac, and Peter Richtarik. Distributed learning with compressed gradient differences. _arXiv preprint arXiv:1901.09269_, 2019.
* [20] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In _International Conference on Learning Representations_, 2018.
* [21] Peter Richtarik, Igor Sokolov, and Ilyas Fatkhullin. Ef21: A new, simpler, theoretically better, and practically faster error feedback. In _Advances in Neural Information Processing Systems_, 2021.
* [22] Mher Safaryan and Peter Richtarik. Stochastic sign descent methods: New algorithms and better theory. In _International Conference on Machine Learning_, 2021.
* [23] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In _Fifteenth annual conference of the international speech communication association_, 2014.
* [24] Shaohuai Shi, Xianhao Zhou, Shutao Song, Xingyao Wang, Zilin Zhu, Xue Huang, Xinan Jiang, Feihu Zhou, Zhenyu Guo, Liqiang Xie, et al. Towards scalable distributed training of deep learning on public cloud clusters. In _Proceedings of Machine Learning and Systems_, volume 3, pages 401-412, 2021.
* [25] Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. In _International Conference on Learning Representations (ICLR)_, 2018.
* [26] Sebastian Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed communication. _arXiv preprint arXiv:1909.05350_, 2019.
* [27] Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [28] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication efficient large-scale training with adam's convergence speed. In _International Conference on Machine Learning_, pages 10118-10129, 2021.
* [29] Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient compression for distributed optimization. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [30] Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. Atomo: Communication-efficient learning via atomic sparsification. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [31] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In _Advances in neural information processing systems_, volume 30, 2017.
* [32] Feijie Wu, Shiqi He, Song Guo, Zhihao Qu, Haozhao Wang, Weihua Zhuang, and Jie Zhang. Sign bit is enough: a learning synchronization framework for multi-hop all-reduce with ultimate compression. In _Proceedings of the 59th ACM/IEEE Design Automation Conference_, 2022.

* [33] Hang Xu, Chen-Yu Ho, Ahmed M Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos Karatsenidis, Marco Canini, and Panos Kalnis. Compressed communication for distributed deep learning: Survey and quantitative evaluation. Technical report, 2020.
* [34] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? In _Advances in Neural Information Processing Systems_, 2020.
* [35] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically understanding why sgd generalizes better than adam in deep learning. _Advances in Neural Information Processing Systems_, 33:21285-21296, 2020.

## Appendix A Theoretical Analysis for Algorithm 1

In practice, we implement Binder in a non-parameter-server model to further reduce the communication overhead, but the data exchange is essentially equivalent to that in a parameter-server prototype. Hence, we provide the theoretical analysis for Binder in a parameter-server model as shown in Algorithm 1.

According to Algorithm 1, the update \(\bar{u}_{t}\) can be recursively formulated as

\[\bar{u}_{t}= \mathcal{Q}\left(\frac{1}{n}\sum_{i=1}^{n}u_{t}^{(i)}+\bar{e}_{t }\right)\] \[= \frac{1}{n}\sum_{i=1}^{n}u_{t}^{(i)}+\bar{e}_{t}-\bar{e}_{t+1}\] \[= \frac{1}{n}\sum_{i=1}^{n}\mathcal{Q}\left(\frac{m_{t}^{(i)}}{b_{t }^{(i)}}+e_{t}^{(i)}\right)+\bar{e}_{t}-\bar{e}_{t+1}\] (7) \[= \frac{1}{n}\sum_{i=1}^{n}\left(\frac{m_{t}^{(i)}}{b_{t}^{(i)}}+e_ {t}^{(i)}-e_{t+1}^{(i)}\right)+\bar{e}_{t}-\bar{e}_{t+1}\] \[= \frac{1}{n}\sum_{i=1}^{n}\frac{m_{t}^{(i)}}{b_{t}^{(i)}}+\frac{1 }{n}\sum_{i=1}^{n}\left(e_{t}^{(i)}-e_{t+1}^{(i)}\right)+\bar{e}_{t}-\bar{e}_{ t+1}\]

Denote

\[g_{t}\stackrel{{\triangle}}{{=}}\frac{1}{n}\sum_{i= 1}^{n}g_{t}^{(i)},\] (8) \[m_{t}\stackrel{{\triangle}}{{=}}\frac{1}{n}\sum_{i =1}^{n}m_{t}^{(i)}=\beta m_{t-1}+(1-\beta)g_{t},\] (9) \[b_{t}\stackrel{{\triangle}}{{=}}\frac{1}{n}\sum_{i =1}^{n}b_{t}^{(i)},\] (10) \[\delta_{t}\stackrel{{\triangle}}{{=}}\frac{1}{n} \sum_{i=1}^{n}\frac{m_{t}^{(i)}}{b_{t}^{(i)}}-\frac{m_{t}}{b_{t}},\] (11) \[e_{t}\stackrel{{\triangle}}{{=}}\frac{1}{n}\sum_{i =1}^{n}e_{t}^{(i)}+\bar{e}_{t}\] (12)

Hence, the updating rule can be summarized as

\[x_{t+1} =x_{t}-\alpha_{t}\bar{u}_{t}\] (14) \[= x_{t}-\alpha_{t}\left(\frac{m_{t}}{b_{t}}+\delta_{t}+e_{t}-e_{t+ 1}\right)\]

### Auxiliary Lemmas

_Lemma 1_.: Let \(u_{t}=\frac{m_{t}}{b_{t}}\), the element-wise quantization function is defined in Eq.(5) can be reformulated as

\[\mathcal{Q}\left((u_{t})_{j}\right)=\left\{\begin{array}{ll}1,& \text{ with probability }p=\frac{(u_{t})_{j}+1}{2}\quad(j\in\{1,2,...,d\},\quad-1\leq(u_{t})_{j} \leq 1).\\ -1,&\text{ with probability }1-p\end{array}\right.(j\in\{1,2,...,d\},\quad-1\leq(u_{t})_{j} \leq 1).\] (15)

_We have \(e_{t}=u_{t}-\mathcal{Q}\left(u_{t}\right)\), and then the following holds true_

\[\mathbb{E}\left[e_{t}\right]=0,\ \ \mathbb{E}\left[\|e_{t}\|^{2}\right]\leq d.\] (16)

**Proof.** From Eq.(15), we know

\[\mathbb{E}\left[(e_{t})_{j}\right] =\mathbb{E}\left[u_{t}-\mathcal{Q}\left(u_{t}\right)\right]\] (17) \[=\frac{1}{2}\left((u_{t})_{j}+1\right)\left((u_{t})_{j}-1\right) +(1-\frac{1}{2}((u_{t})_{j}+1))((u_{t})_{j}+1)=0,\]and,

\[\mathbb{E}\left[\left(e_{t}\right)_{j}^{2}\right] =\mathbb{E}\left[\left((u_{t})_{j}-\mathcal{Q}\left((u_{t})_{j} \right)\right)^{2}\right]\] (18) \[=\frac{1}{2}\left((u_{t})_{j}+1\right)\left((u_{t})_{j}-1\right)^ {2}+(1-\frac{1}{2}((u_{t})_{j}+1))((u_{t})_{j}+1)^{2}\] \[=1-((u_{t})_{j})^{2}\leq 1.\]

Hence,

\[\mathbb{E}\left[e_{t}\right]=0,\ \ \mathbb{E}\left[\left\|e_{t}\right\|^{2} \right]\leq d.\] (19)

**Lemma 2**.: _Let \(x_{0}=x_{1}\) and \(\alpha_{0}=\alpha_{1}\) in Algorithm 1, defining the sequence_

\[z_{1} =x_{1}+\alpha_{1}(\delta_{1}-e_{1})\] (20) \[z_{t} =x_{t}+\frac{\beta}{1-\beta}(x_{t}-x_{t-1})+\frac{\alpha_{t-1}}{1 -\beta}(\delta_{t-1}+\beta e_{t-1}-e_{t}),\forall t\geq 2.\] (21)

_Then the following equality will hold, i.e.,_

\[z_{t+1}=z_{t}+\frac{\beta}{1-\beta}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{ \alpha_{t}}{b_{t}}\right)\odot m_{t-1}-\alpha_{t}\frac{g_{t}}{b_{t}}-\alpha_{ t-1}\delta_{t-1}-(\alpha_{t}-\alpha_{t-1})e_{t}.\] (22)

**Proof.** For \(t=1\), we have

\[z_{2}-z_{1} =x_{2}+\frac{\beta}{1-\beta}(x_{2}-x_{1})+\frac{\alpha_{1}}{1- \beta}(\delta_{1}+\beta e_{1}-e_{2})-(x_{1}+\alpha_{1}(\delta_{1}-e_{1}))\] (23) \[=(\frac{\beta}{1-\beta}+1)(x_{2}-x_{1})+\frac{\alpha_{1}}{1- \beta}(\delta_{1}+\beta e_{1}-e_{2})-\alpha_{1}(\delta_{1}-e_{1})\] \[=-\frac{\alpha_{1}}{1-\beta}\left(\frac{(1-\beta)g_{1}}{b_{1}}+ \delta_{1}+e_{1}-e_{2}\right)+\frac{\alpha_{1}}{1-\beta}(\delta_{1}+\beta e_{ 1}-e_{2})-\alpha_{1}(\delta_{1}-e_{1})\] \[=-\alpha_{1}\frac{g_{1}}{b_{1}}-\alpha_{0}\delta_{1}\]

where the second equality follows the updating rule in Eq.(14).

For \(t\geq 2\), following the updating rule in Eq.(14), we have

\[x_{t+1}-x_{t}+\alpha_{t}(\delta_{t}+e_{t}-e_{t+1})= -\alpha_{t}\frac{m_{t}}{b_{t}}\] (24) \[= -\alpha_{t}\frac{\beta m_{t-1}+(1-\beta)g_{t}}{b_{t}}\] \[= \beta\left(x_{t}-x_{t-1}+\alpha_{t-1}(\delta_{t}+e_{t-1}-e_{t})\right)\] \[+\beta\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}} \right)\odot m_{t-1}-(1-\beta)\alpha_{t}\frac{g_{t}}{b_{t}}\]

We know \(x_{t+1}-x_{t}+\alpha_{t}(e_{t}-e_{t+1})=(1-\beta)(x_{t+1}+-\alpha_{t}(e_{t+1}- \delta_{t}))-(1-\beta)(x_{t}-\alpha_{t}e_{t})+\beta(x_{t+1}-x_{t}+\alpha_{t}( \delta_{t}+e_{t}-e_{t+1}))\), so Eq. (24) can be rearranged as

\[(1-\beta)(x_{t+1}+\alpha_{t}(\delta_{t}-e_{t+1}))+\beta(x_{t+1}- x_{t}+\alpha_{t}(\delta_{t}+e_{t}-e_{t+1}))\] (25) \[= (1-\beta)(x_{t}-\alpha_{t}e_{t})+\beta\left(x_{t}-x_{t-1}+\alpha _{t-1}(\delta_{t-1}+e_{t-1}-e_{t})\right)\] \[+\beta\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t} }\right)\odot m_{t-1}-(1-\beta)\alpha_{t}\frac{g_{t}}{b_{t}}\]

Divided both sides by \(1-\beta\), we obtain

\[x_{t+1}+\alpha_{t}(\delta_{t}-e_{t+1})+\frac{\beta}{1-\beta}(x_ {t+1}-x_{t}+\alpha_{t}(\delta_{t}+e_{t}-e_{t+1}))\] (26) \[= x_{t}+\alpha_{t-1}(\delta_{t-1}-e_{t})+\frac{\beta}{1-\beta} \left(x_{t}-x_{t-1}+\alpha_{t-1}(\delta_{t-1}+e_{t-1}-e_{t})\right)\] \[+\frac{\beta}{1-\beta}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{ \alpha_{t}}{b_{t}}\right)\odot m_{t-1}\] \[-\alpha_{t}\frac{g_{t}}{b_{t}}-\alpha_{t-1}\delta_{t-1}-(\alpha_{t }-\alpha_{t-1})e_{t}\]Rearranging Eq. (26), we have

\[x_{t+1}+\frac{\beta}{1-\beta}(x_{t+1}-x_{t})+\frac{\alpha_{t}}{1- \beta}(\delta_{t}+\beta e_{t}-e_{t+1})\] (27) \[= x_{t}+\frac{\beta}{1-\beta}(x_{t}-x_{t-1})+\frac{\alpha_{t-1}}{1- \beta}(\delta_{t-1}+\beta e_{t-1}-e_{t})\] \[+\frac{\beta}{1-\beta}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{ \alpha_{t}}{b_{t}}\right)\odot m_{t-1}\] \[-\alpha_{t}\frac{g_{t}}{b_{t}}-\alpha_{t-1}\delta_{t-1}-(\alpha_ {t}-\alpha_{t-1})e_{t}\]

Define the sequence

\[z_{t}=x_{t}+\frac{\beta}{1-\beta}(x_{t}-x_{t-1})+\frac{\alpha_{t-1}}{1-\beta} (\delta_{t-1}+\beta e_{t-1}-e_{t})\] (28)

We finally obtain

\[z_{t+1}=z_{t}+\frac{\beta}{1-\beta}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{ \alpha_{t}}{b_{t}}\right)\odot m_{t-1}-\alpha_{t}\frac{g_{t}}{b_{t}}-\alpha_{ t-1}\delta_{t-1}-(\alpha_{t}-\alpha_{t-1})e_{t}.\] (29)

Recalling \(x_{1}=x_{0}\) and \(\alpha_{1}=\alpha_{0}\), we have \(\frac{\alpha_{1}}{b_{1}}=\frac{\alpha_{0}}{b_{0}}\). Then, combining Eq.(23) and Eq.(29), we obtain the conclusion.

### Proof of Theorem 1

**Proof.** By the the gradient Lipschitz continuous in Assumption 2 and Lemma 2, we obtain

\[\mathbb{E}[f(z_{t+1})-f(z_{t})]\leq\mathbb{E}\langle\nabla f(z_{t }),z_{t+1}-z_{t}\rangle+\frac{L}{2}\mathbb{E}\|z_{t+1}-z_{t}\|^{2}\] (30) \[= \mathbb{E}\left[\frac{\beta}{1-\beta}\langle\nabla f(z_{t}), \left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right)\odot m_{t-1 }\rangle\right]-\mathbb{E}\left[\langle\nabla f(z_{t}),\alpha_{t}\frac{g_{t}}{ b_{t}}\rangle\right]\] \[-\mathbb{E}\left[\langle\nabla f(z_{t}),\alpha_{t-1}\delta_{t-1} \rangle\right]-\mathbb{E}\left[\langle\nabla f(z_{t}),(\alpha_{t}-\alpha_{t}) e_{t-1}\rangle\right]\] \[+\mathbb{E}\left[\frac{L}{2}\left\|\frac{\beta}{1-\beta}\left( \frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right)\odot m_{t-1}- \alpha_{t}\frac{g_{t}}{b_{t}}-\alpha_{t-1}\delta_{t-1}-(\alpha_{t}-\alpha_{t- 1})e_{t-1}\right\|^{2}\right]\] \[= \mathbb{E}\left[\frac{\beta}{1-\beta}\langle\nabla f(z_{t}), \left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right)\odot m_{t-1 }\rangle\right]-\mathbb{E}\left[\langle\nabla f(z_{t}),\alpha_{t}\frac{g_{t}}{ b_{t}}\rangle\right]\] \[+\mathbb{E}\left[\frac{L}{2}\left\|\frac{\beta}{1-\beta}\left( \frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right)\odot m_{t-1}- \alpha_{t}\frac{g_{t}}{b_{t}}-\alpha_{t-1}\delta_{t-1}-(\alpha_{t}-\alpha_{t- 1})e_{t-1}\right\|^{2}\right]\] \[\leq \mathbb{E}\left[\frac{\beta}{1-\beta}\langle\nabla f(z_{t}), \left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right)\odot m_{t-1 }\rangle\right]-\mathbb{E}\left[\langle\nabla f(z_{t}),\alpha_{t}\frac{g_{t}}{ b_{t}}\rangle\right]\] \[+L\mathbb{E}\left[\left\|\frac{\beta}{1-\beta}\left(\frac{ \alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right)\odot m_{t-1}\right\|^{2 }\right]+L\mathbb{E}\left[\alpha_{t}^{2}\left\|\frac{g_{t}}{b_{t}}\right\|^{2}\right]\] \[+\frac{L}{2}\mathbb{E}\left[\left|\alpha_{t-1}\delta_{t-1}\right| ^{2}\right]+\frac{L}{2}\mathbb{E}\left[\left|(\alpha_{t-1}-\alpha_{t})e_{t} \right|^{2}\right]\]

where the second equality holds due to \(\mathbb{E}\left[\delta_{t-1}\right]=0\) and \(\mathbb{E}\left[e_{t-1}\right]=0\). The last inequality holds owing to \(\mathbb{E}[\|a+b\|^{2}]=\mathbb{E}[\|a\|^{2}]+\mathbb{E}[\|b\|^{2}]\) if \(\mathbb{E}[a]=0\) or \(\mathbb{E}[b]=0\), and \(\mathbb{E}[\|a+b\|^{2}]\leq 2\mathbb{E}[\|a\|^{2}]+2\mathbb{E}[\|b\|^{2}]\) if \(\mathbb{E}[a]\neq 0\) and \(\mathbb{E}[b]\neq 0\).

Taking telescope sum from \(1\) to \(T\) on the both sides of Eq.(30), we then have

\[\mathbb{E}[f(z_{T})-f(z_{1})]\leq \underbrace{\beta}_{1-\beta}\mathbb{E}\left[\sum_{t=1}^{T}\langle \nabla f(z_{t}),\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}} \right)\odot m_{t-1}\rangle\right]}_{\mathcal{T}_{1}}\underbrace{-\mathbb{E} \left[\sum_{t=1}^{T}\langle\nabla f(z_{t}),\alpha_{t}\frac{g_{t}}{b_{t}} \rangle\right]}_{\mathcal{T}_{2}}\] (31) \[+\underbrace{L\mathbb{E}\left[\sum_{t=1}^{T}\left\|\frac{\beta}{ 1-\beta}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right) \odot m_{t-1}\right\|^{2}\right]}_{\mathcal{T}_{3}}\] \[+\underbrace{L\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\| \frac{g_{t}}{b_{t}}\right\|^{2}\right]+\frac{L}{2}\mathbb{E}[\sum_{t=1}^{T} \|\alpha_{t-1}\delta_{t-1}\|^{2}]+\frac{L}{2}\mathbb{E}[\sum_{t=1}^{T}\|( \alpha_{t-1}-\alpha_{t})e_{t}\|^{2}]}_{\mathcal{T}_{4}}\]

Now we focus on bounding \(T_{1}\) below. From Assumption 4, we know \(\|g_{t}\|\leq G\) (\(t=1,2,...,T\)) and \(\|\nabla f(z_{t})\|\leq G\). Due to \(m_{t}=\beta m_{t-1}+(1-\beta)g_{t}\) and \(m_{1}=g_{1}\), it is easy to obtain \(\|m_{t}\|\leq G\) by complete induction.

Since \(\|\nabla f(z_{t})\|\leq G\) and \(\|m_{t}\|\leq G\), we have

\[T_{1} =\frac{\beta}{1-\beta}\mathbb{E}\left[\sum_{i=1}^{T}\langle \nabla f(z_{i}),\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}} \right)\odot m_{i-1}\rangle\right]\] (32) \[\stackrel{{(i)}}{{\leq}}\frac{\beta}{1-\beta} \mathbb{E}\left[\sum_{i=1}^{T}\left\|\nabla f(z_{t})\right\|\|m_{t}\|\left\| \frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}}\right\|_{1}\right]\] \[\stackrel{{(ii)}}{{\leq}}\frac{\beta}{1-\beta}G^{2} \mathbb{E}\left[\sum_{i=1}^{T}\left\|\frac{\alpha_{t-1}}{b_{t-1}}-\frac{ \alpha_{t}}{b_{t}}\right\|_{1}\right]\] \[\stackrel{{(ii)}}{{=}}\frac{\beta}{1-\beta}G^{2} \mathbb{E}\left[\left\|\sum_{i=1}^{T}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac {\alpha_{t}}{b_{t}}\right)\right\|_{1}\right]\] \[\leq\frac{\beta}{1-\beta}G^{2}\mathbb{E}\left[\left\|\frac{\alpha _{0}}{b_{0}}\right\|_{1}\right]\] \[\stackrel{{(iv)}}{{\leq}}\frac{\alpha_{0}\beta d}{(1 -\beta)\rho}G^{2},\]

where \((i)\) holds sice \(\|a\odot b\|\leq\|a\|\max_{j}|(b)_{j}|\leq\|a\|\|b\|_{1}\), \((ii)\) holds due to \(\|\nabla f(z_{t})\|\leq G\) and \(\|m_{t}\|\leq G\), \((iii)\) holds because \(\frac{\alpha_{t-1}}{(b_{t-1})_{j}}-\frac{\alpha_{t}}{(b_{t})_{j}}\geq 0\) for any \(j\in[1,2,...,d]\), \((iv)\) holds due to \(\min_{j}(b_{t})_{j}\geq\rho>0\) for any \(j\in[1,2,...,d]\).

Let us turn to bound \(T_{2}\),

\[T_{2} =-\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(z_{t}),\alpha_{t }\frac{g_{t}}{b_{t}}\rangle\right]\] (33) \[=\underbrace{-\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(z_{ t})-f(x_{t}),\alpha_{t}\frac{g_{t}}{b_{t}}\rangle\right]}_{\mathcal{T}_{5}} \underbrace{-\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(x_{t}),\alpha_{t }\frac{g_{t}}{b_{t}}\rangle\right]}_{\mathcal{T}_{6}}\]We now analyze \(T_{5}\) below,

\[T_{5}=-\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(z_{t})-f(x_{t} ),\alpha_{t}\frac{g_{t}}{b_{t}}\rangle\right]\] (34) \[\overset{(i)}{\leq} \frac{1}{2}\mathbb{E}\left[\sum_{t=1}^{T}\|f(z_{t})-f(x_{t})\|^{2 }\right]+\frac{1}{2}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\|\frac{g_ {t}}{b_{t}}\right\|^{2}\right]\] \[\overset{(ii)}{\leq} \frac{L^{2}}{2}\mathbb{E}\left[\sum_{t=1}^{T}\|z_{t}-x_{t}\|^{2} \right]+\frac{1}{2}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\|\frac{g_ {t}}{b_{t}}\right\|^{2}\right]\] \[\overset{(iii)}{=} \frac{L^{2}}{2}\mathbb{E}\left[\sum_{t=1}^{T}\left\|\frac{\beta} {1-\beta}(x_{t}-x_{t-1})+\frac{\alpha_{t-1}}{1-\beta}\left(\delta_{t-1}+\beta e _{t-1}-e_{t}\right)\right\|^{2}\right]+\frac{1}{2}\mathbb{E}\left[\sum_{t=1}^ {T}\alpha_{t}^{2}\left\|\frac{g_{t}}{b_{t}}\right\|^{2}\right]\] \[\overset{(iv)}{\leq} \frac{\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{ T}\|x_{t}-x_{t-1}\|^{2}\right]+\frac{L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\|\alpha_{t-1}\delta_{t-1}\|^{2}\right]\] \[+\frac{\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\|\alpha_{t-1}e_{t-1}\|^{2}\right]+\frac{L^{2}}{(1-\beta)^{2}}\mathbb{E} \left[\sum_{t=1}^{T}\|\alpha_{t-1}e_{t}\|^{2}\right]+\frac{1}{2}\mathbb{E} \left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\|\frac{g_{t}}{b_{t}}\right\|^{2}\right]\] \[\overset{(v)}{=} \frac{\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\alpha_{t-1}^{2}\left\|\left(\frac{m_{t-1}}{b_{t-1}}+\delta_{t-1}+e_{t-1}-e _{t}\right)\right\|^{2}\right]+\frac{L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[ \sum_{t=1}^{T}\|\alpha_{t-1}\delta_{t-1}\|^{2}\right]\] \[+\frac{\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\|\alpha_{t-1}e_{t-1}\|^{2}\right]+\frac{L^{2}}{(1-\beta)^{2}}\mathbb{E} \left[\sum_{t=1}^{T}\|\alpha_{t-1}e_{t}\|^{2}\right]+\frac{1}{2}\mathbb{E} \left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\|\frac{g_{t}}{b_{t}}\right\|^{2}\right]\] \[= \frac{\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\|\alpha_{t-1}\|\frac{m_{t-1}}{b_{t-1}}\|^{2}\right]+\frac{(1+\beta^{2})L^ {2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\|\alpha_{t-1}\|^{2}\right]\] \[+\frac{2\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\alpha_{t-1}^{2}\left\|e_{t-1}\right\|^{2}\right]+\frac{(1+\beta^{2})L^{2} }{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t-1}^{2}\left\|e_{t}\right\| ^{2}\right]+\frac{1}{2}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\| \frac{g_{t}}{b_{t}}\right\|^{2}\right]\] \[= \frac{\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\alpha_{t-1}^{2}\left\|\frac{m_{t-1}}{b_{t-1}}\right\|^{2}\right]+\frac{(1+ \beta^{2})L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t-1}^{2} \left\|\delta_{t-1}\right\|^{2}\right]\] \[+\frac{2\beta^{2}L^{2}}{(1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^ {T}\alpha_{t-1}^{2}\left\|e_{t-1}\right\|^{2}\right]+\frac{(1+\beta^{2})L^{2}}{ (1-\beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t-1}^{2}\left\|e_{t}\right\| ^{2}\right]+\frac{1}{2}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}^{2}\left\| \frac{g_{t}}{b_{t}}\right\|^{2}\right]\] \[\overset{(vi)}{\leq} \left(\frac{\beta^{2}L^{2}d}{(1-\beta)^{2}}+\frac{4(1+\beta^{2}) L^{2}d}{(1-\beta)^{2}}+\frac{2\beta^{2}L^{2}d}{(1-\beta)^{2}}+\frac{(1+\beta^{2})L^{2}d}{(1- \beta)^{2}}+\frac{G^{2}}{2\rho^{2}}\right)\sum_{t=1}^{T}\alpha_{t-1}^{2}\] \[= \left(\frac{(8\beta^{2}+10\beta+5)L^{2}d}{(1-\beta)^{2}}+\frac{G^{2 }}{2\rho^{2}}\right)\sum_{t=1}^{T}\alpha_{t-1}^{2}\]

where \((i)\) holds by following \(\langle a,b\rangle\leq\frac{1}{2}\|a\|^{2}+\frac{1}{2}\|a\|^{2}\), \((ii)\) holds due to Assumption 1, \((iii)\) holds due to Assumption 1 owing to Eq.(21), \((iii)\) holds since \(\mathbb{E}[\|a+b\|^{2}]=\mathbb{E}[\|a\|^{2}]+\mathbb{E}[\|b\|^{2}]\) if \(\mathbb{E}[a]=0\) or \(\mathbb{E}[b]=0\), \((v)\) holds resulting from the updating rule in Eq. (14), \((vi)\) holds due to \(\left|\frac{(m_{t})_{j}}{(b_{t})_{j}}\right|\leq 1\), \(|(\delta)_{j}|\leq 2\) (the definition of \(\delta_{i}\)in Eq. (11) ), \(\mathbb{E}[\|e_{t}\|^{2}]\leq d\) in Lemma 1, \(\|g_{t}\|\leq G\) in Assumption 2 and \(\min_{j}(b_{t})_{j}\geq\rho>0\).

We then bound \(T_{6}\)

\[T_{6} = -\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(x_{t}),\alpha_{t}\frac {g_{t}}{b_{t}}\rangle\right]\] (35) \[= -\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(x_{t}),\alpha_{t} \frac{\nabla f(x_{t})}{b_{t}}\rangle\right]-\mathbb{E}\left[\sum_{t=1}^{T} \langle\nabla f(x_{t}),\alpha_{t}\frac{g_{t}-\nabla f(x_{t})}{b_{t}}\rangle\right]\] \[\overset{(i)}{\leq} -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x_ {t})\|^{2}\right]+\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f(x_{t}),\alpha_ {t}\frac{\nabla f(x_{t})-g_{t}}{b_{t}}\rangle\right]\] \[= -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x_ {t})\|^{2}\right]+\mathbb{E}\left[\langle\nabla f(x_{1}),\alpha_{1}\frac{ \nabla f(x_{1})-g_{1}}{b_{1}}\rangle\right]\] \[+\mathbb{E}\left[\sum_{t=2}^{T}\langle\nabla f(x_{t}),\nabla(f(x _{t})-g_{t})\odot\left(\frac{\alpha_{t}}{b_{t}}-\frac{\alpha_{t-1}}{b_{t-1}} \right)\rangle\right]+\mathbb{E}\left[\sum_{t=2}^{T}\langle\nabla f(x_{t}), \alpha_{t-1}\frac{\nabla f(x_{t})-g_{t}}{b_{t-1}}\rangle\right]\] \[\overset{(ii)}{=} -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x _{t})\|^{2}\right]+\mathbb{E}\left[\langle\nabla f(x_{1}),\alpha_{1}\frac{ \nabla f(x_{1})-g_{1}}{b_{1}}\rangle\right]\] \[+\mathbb{E}\left[\sum_{t=2}^{T}\langle\nabla f(x_{t}),(\nabla f( x_{t})-g_{t})\odot\left(\frac{\alpha_{t}}{b_{t}}-\frac{\alpha_{t-1}}{b_{t-1}} \right)\rangle\right]\] \[\overset{(iii)}{\leq} -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x _{t})\|^{2}\right]+\mathbb{E}\left[\|\nabla f(x_{1})\|\|\nabla f(x_{1})-g_{1} \|\left\|\frac{\alpha_{1}}{b_{1}}\right\|_{1}\right]\] \[+\mathbb{E}\left[\sum_{t=2}^{T}\|\nabla f(x_{t})\|\|\nabla f(x_{t })-g_{t}\|\left\|\frac{\alpha_{t}}{b_{t}}-\frac{\alpha_{t-1}}{b_{t-1}}\right\| _{1}\right]\] \[\overset{(iv)}{\leq} -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x _{t})\|^{2}\right]+2G^{2}\mathbb{E}\left[\left\|\frac{\alpha_{1}}{b_{1}}\right\| _{1}+\sum_{t=2}^{T}\frac{\alpha_{t}}{b_{t}}-\frac{\alpha_{t-1}}{b_{t-1}}\right\| _{1}\right]\] \[\overset{(v)}{=} -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x _{t})\|^{2}\right]+4G^{2}\mathbb{E}\left[\left\|\frac{\alpha_{1}}{b_{1}}\right\| _{1}\right],\] \[\overset{(vi)}{\leq} -\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x _{t})\|^{2}\right]+\frac{4G^{2}\alpha_{1}d}{\rho}\]

where \((i)\) holds due to \(\max_{j}(b_{t})_{j}\leq\|b_{t}\|\leq G\), \((ii)\) holds owing to \(\mathbb{E}[\nabla f(x_{t})-g_{t}]=0\) in Assumption 2 and \(g_{t}\), \(b_{t-1}\) are independent, \((iii)\) holds size \(\|a\odot b\|\leq\|a\|\max_{j}|(b)_{j}|\leq\|a\|\|b\|_{1}\), \((iv)\) holds resulting from \(\|\nabla f(x_{t})\|\leq G\) and \(\|\nabla f(x_{t})-g_{t}\|\leq\|\nabla f(x_{t})\|+\|g_{t}\|\leq 2G\), and \((v)\) holds because \(\frac{\alpha_{t-1}}{(b_{t-1})_{j}}-\frac{\alpha_{t}}{(b_{t})_{j}}\geq 0\) for any \(j\in[1,2,...,d]\), \((vi)\) holds due to \(\min_{j}(b_{t})_{j}\geq\rho>0\) for any \(j\in[1,2,...,d]\).

Then, we pay attention to \(T_{3}\),

\[\begin{split} T_{3}&=L\mathbb{E}\left[\sum_{t=1}^{T} \left\|\frac{\beta}{1-\beta}\left(\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t} }{b_{t}}\right)\odot m_{t-1}\right\|^{2}\right]\\ &\overset{(i)}{\leq}\frac{\beta^{2}L}{(1-\beta)^{2}}\mathbb{E} \left[\sum_{t=1}^{T}\left\|\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t} }\right\|^{2}\left\|m_{t-1}\right\|^{2}\right]\\ &\overset{(ii)}{\leq}\frac{\beta^{2}LG^{2}}{(1-\beta)^{2}} \mathbb{E}\left[\sum_{t=1}^{T}\left\|\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_ {t}}{b_{t}}\right\|^{2}\right]\\ &\overset{(iii)}{\leq}\frac{\beta^{2}LG^{2}}{(1-\beta)^{2}} \mathbb{E}\left[\sum_{t=1}^{T}\max_{j}\left|\frac{\alpha_{t-1}}{(b_{t-1})_{j} }-\frac{\alpha_{t}}{(b_{t})_{j}}\right|\left\|\frac{\alpha_{t-1}}{b_{t-1}}- \frac{\alpha_{t}}{b_{t}}\right\|_{1}\right]\\ &\overset{(iv)}{\leq}\frac{\alpha_{0}\beta^{2}LG^{2}}{\rho(1- \beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\max_{j}\left(\frac{\alpha_{t-1}}{(b_{ t-1})_{j}}\right)\left\|\frac{\alpha_{t-1}}{b_{t-1}}-\frac{\alpha_{t}}{b_{t}} \right\|_{1}\right]\\ &\overset{(v)}{\leq}\frac{\alpha_{0}\beta^{2}LG^{2}}{\rho(1- \beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\left\|\frac{\alpha_{t-1}}{b_{t-1}}- \frac{\alpha_{t}}{b_{t}}\right\|_{1}\right]\\ &\overset{(vi)}{\leq}\frac{\alpha_{0}\beta^{2}LG^{2}}{\rho^{2}(1- \beta)^{2}}\mathbb{E}\left[\sum_{t=1}^{T}\left\|\frac{\alpha_{t-1}}{b_{t-1}} \right\|_{1}-\left\|\frac{\alpha_{t}}{b_{t}}\right\|_{1}\right]\\ &\overset{(vii)}{\leq}\frac{\alpha_{0}\beta^{2}LG^{2}}{\rho(1- \beta)^{2}}\mathbb{E}\left[\left\|\frac{\alpha_{0}}{b_{0}}\right\|_{1}-\left\| \frac{\alpha_{T}}{b_{T}}\right\|_{1}\right]\\ &\overset{(viii)}{\leq}\frac{\alpha_{0}^{2}\beta^{2}LG^{2}d}{\rho ^{2}(1-\beta)^{2}},\end{split}\] (36)

where \((i)\) holds due to \(\|a\odot b\|\leq\|a\|\|b\|\), \((ii)\) holds owing to \(\|m_{t-1}\|\leq G\), \((ii)\) holds due to \(\|a\|^{2}\leq\max_{j}|(a)_{j}\|a\|_{1}\), \((iv)\) holds due to \(\frac{\alpha_{t-1}}{(b_{t-1})_{j}}-\frac{\alpha_{t}}{(b_{t})_{j}}\geq 0\) and \(\frac{\alpha_{t}}{(b_{t})_{j}}>0\) for any \(j\in[1,2,...,d]\), \((v)\) holds resulting from \(\min_{j}(b_{t})_{j}\geq\rho>0\) for any \(j\) and \(\alpha_{t}\) is non-increasing, \((vi)\) holds resulting from \(\frac{\alpha_{t-1}}{(b_{t-1})_{j}}-\frac{\alpha_{t}}{(b_{t})_{j}}\geq 0\) for any \(j\in[1,2,...,d]\), \((vii)\) holds due to telescoping sum, and \((viii)\) holds due to \(\min_{j}(b_{t})_{j}\geq\rho>0\) for any \(j\in[1,2,...,d]\)..

Now we turn attention to \(T_{4}\),

\[\begin{split} T_{4}&=L\mathbb{E}\left[\sum_{t=1}^{T }\alpha_{t}^{2}\left\|\frac{g_{t}}{b_{t}}\right\|^{2}\right]+\frac{L}{2} \mathbb{E}[\sum_{t=1}^{T}\|\alpha_{t-1}\delta_{t-1}\|^{2}]+\frac{L}{2}\mathbb{E }[\sum_{t=1}^{T}\|(\alpha_{t-1}-\alpha_{t})e_{t}\|^{2}]\\ &\leq\left(L\frac{G^{2}}{\rho^{2}}+2dL\right)\sum_{t=1}^{T} \alpha_{t}^{2}+\frac{dL}{2}\sum_{t=1}^{T}(\alpha_{t-1}-\alpha_{t})^{2},\end{split}\] (37)

where the inequality holds owing to \(\|m_{t-1}\|\leq G\) and \(\min_{j}(b_{t})_{j}\geq\rho>0\), \(\|(\delta_{t-1})_{j}\|\leq 2\), and \(\mathbb{E}[\|e_{t}\|^{2}]\leq d\). Combining Eq.(31-37), we can obtain

\[\begin{split}\mathbb{E}[f(z_{T})-f(z_{1})]\leq&\frac {\alpha_{0}\beta d}{(1-\beta)\rho}G^{2}+\left(\frac{(8\beta^{2}+10\beta+5)L^{2 }d}{(1-\beta)^{2}}+\frac{G^{2}}{2\rho^{2}}\right)\sum_{t=1}^{T}\alpha_{t-1}^{2} \\ &-\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f( x_{t})\|^{2}\right]+\frac{4G^{2}\alpha_{1}d}{\rho}+\frac{\alpha_{0}^{2}\beta^{2}LG^{2}d}{ \rho^{2}(1-\beta)^{2}}\\ &+\left(L\frac{G^{2}}{\rho^{2}}+2dL\right)\sum_{t=1}^{T}\alpha_{t }^{2}+\frac{dL}{2}\sum_{t=1}^{T}(\alpha_{t-1}-\alpha_{t})^{2}.\end{split}\] (38)Reformulating Eq.(38), we then have

\[\frac{1}{G}\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x_{t}) \|^{2}\right]\leq \mathbb{E}[f(z_{1})-f(z_{T})]\] (39) \[+\left(\frac{(8\beta^{2}+10\beta+5)L^{2}d}{(1-\beta)^{2}}+\frac{G ^{2}(1+L)}{2\rho^{2}}+2dL\right)\sum_{t=1}^{T}\alpha_{t-1}^{2}\] \[+\frac{dL}{2}\sum_{t=1}^{T}(\alpha_{t-1}-\alpha_{t})^{2}\] \[+\frac{\alpha_{0}\beta d}{(1-\beta)\rho}G^{2}+\frac{4G^{2}\alpha _{1}d}{\rho}+\frac{\alpha_{0}^{2}\beta^{2}LG^{2}d}{\rho^{2}(1-\beta)^{2}}\]

It is known the learning rate satfities \(\alpha_{t}=\frac{c}{\sqrt{t}},\forall t\geq 1\) and \(\alpha_{0}=\alpha_{1}=c\). Utilizing non-increasing \(\alpha_{t}\) and Cauchy-Schwarz inequality, we know \(\mathbb{E}\left[\sum_{t=1}^{T}\alpha_{t}\|\nabla f(x_{t})\|^{2}\right]\geq T \alpha_{T}\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}\|\nabla f(x_{t})\|\right]^ {2}=\frac{\sqrt{T}}{c}\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}\|\nabla f(x_{t })\|\right]^{2}\). \(\sum_{t=1}^{T}\alpha_{t-1}^{2}=\sum_{t=1}^{T}\frac{c^{2}}{T}\leq c^{2}(1+\int_ {1}^{T-1}\frac{1}{dt})\leq c^{2}(1+\log T)\), and \(\sum_{t=1}^{T}(\alpha_{t-1}-\alpha_{t})^{2}=\sum_{t=2}^{T}(\alpha_{t-1}-\alpha _{t})^{2}\leq\sum_{t=2}^{T}\frac{c^{2}}{4(t-1)^{3}}\leq\frac{c^{2}}{4}(1+\int_ {1}^{T-2}t^{-3}dt)=\frac{c^{2}}{4}(\frac{3}{2}-\frac{1}{2(T-2)})\leq\frac{3c^{ 2}}{8}\), we further have

\[\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}\|\nabla f(x_{t})\|\right]^{2}\leq \frac{C_{1}}{\sqrt{T}}+\frac{C_{2}(1+\log T)}{\sqrt{T}},\] (40)

where we define

\[C_{1} =cG\left(\mathbb{E}[f(z_{1})-f^{*}]+\frac{3c^{2}dL}{16}+\frac{ \beta cdG^{2}}{(1-\beta)\rho}+\frac{4cdG^{2}}{\rho}+\frac{c^{2}\beta^{2}LG^{2 }d}{\rho^{2}(1-\beta)^{2}}\right),\] (41) \[C_{2} =c^{3}G\left(\frac{(8\beta^{2}+10\beta+5)L^{2}d}{(1-\beta)^{2}}+ \frac{G^{2}(1+L)}{2\rho^{2}}+2dL\right).\] (42)

## Appendix B Experiments for Comparing Vanilla SGD, SGDM, Adam, Binder and SoftSignSGD

To address the bottleneck in communication during distributed training, numerous gradient compression algorithms have been proposed, aiming to reduce the communication volume. Most of these algorithms can be reduced to Vanilla SGD without momentum if compression is not performed. Generally speaking, the epoch-wise convergence rate and inference performance a compressed algorithms is upper bounded by its uncompressed counterpart. In the experiments, we conducted empirical experiments to evaluate the training and inference performance of of Vanilla SGD, SGDM, Adam, Binder and its uncompressed version in training typical CNN-base, LSTM-base and Transformer-base DNNs.

```
1:Input: model parameter \(x_{0},x_{1}\), the momentum \(m_{0}^{(t)}=0\), \(b_{0}^{(t)}=0\), the exponential moving average factor \(\beta\), the learning rate sequence \(\{\alpha_{t}\}\)
2:for\(t=1,...,T\)do
3: Randomly sample \(\xi_{t}\) and compute the gradient: \(g_{t}=\nabla f(x_{t};\xi_{t})\)
4: Update the momentum \(m_{t}\): \(m_{t}=\beta m_{t-1}+(1-\beta)g_{t}\)
5: Update the momentum \(b_{t}\): \(b_{t}=\beta b_{t-1}+(1-\beta)|g_{t}|\)
6: Update the model parameter \(x_{t+1}\): \(x_{t+1}=x_{t}-\alpha_{t}\frac{m_{t}}{b_{t}}\)
7:endfor ```

**Algorithm 1**SoftSignSGD

We refer to the uncompressed Binder as SoftSignSGD. The implementation details for SoftSignSGD are presented in Algorithm 1. When comparing SoftSignSGD to Adam, there are two key differences. First, instead of using the square root of the exponential moving average of the squared gradient, denoted as \(\sqrt{v_{t}}=\sqrt{(1-\beta_{2})v_{t-1}+(1-\beta_{2})g_{t}^{2}}\), SoftSignSGD utilizes the exponential moving average of the absolute gradient, represented as \(b_{t}=(1-\beta)b_{t-1}+|g_{t}|\). Second, in SoftSignSGD, the exponential moving factors for both the numerator \(m_{t}\) and the denominator \(b_{t}\) are the same. These differences ensure that each element of the updating amount in SoftSignSGD satisfies the condition \(-1\leq(\frac{m_{t}}{b_{t}})_{j}\leq 1\).

### Experimental Results for training ResNet-20

We evaluated the performance of five optimization algorithms: Vanilla SGD, SGDM, AdamW, Birder and SoftSignSGD, for training ResNet-20 on CIFAR100. Each batch consisted of a set of \(128\) examples sampled with replacement. For SGDM, we set the momentum parameter \(\beta\) to \(0.9\), while for SoftSignSGD and Birder, it was set to \(0.95\). For AdamW, the parameters \(\beta_{1}\) and \(\beta_{2}\) were set to \(0.9\) and \(0.999\), respectively. The weight decay was uniformly set to \(0.0005\) for Vanilla SGD and SGDM, and \(0.05\) for AdamW, Birder and SoftSignSGD. To simplify the tuning process and ensure fair comparisons, we initialized the learning rates at \(0.1\) for Vanilla SGD and SGDM, and \(0.005\) for AdamW, Birder and SoftSignSGD. We divided the learning rates by \(10\) after \(75\) and \(130\) epochs, and terminated the training after \(150\) epochs.

Figure 5 visually demonstrates that Vanilla SGD exhibits slower convergence speed and lower test accuracy compared to SGDM. In contrast, both Birder and SoftSignSGD show comparable training and inference performance to the commonly used SGDM and AdamW. This observation suggests that Birder outperforms existing gradient compression algorithms when training CNN-based ResNet-20 models.

### Experimental Results for training LSTM

We conducted experiments to train a 3-layer LSTM model on the Penn TreeBank dataset to evaluate the performance of five optimization algorithms: Vanilla SGD, SGDM, AdamW, Birder and SoftSignSGD. Our implementations were built upon the code provided in the AdaBelief paper7, and we used the default experimental settings for SGDM and AdamW. For Vanilla SGD, we used the experimental settings of SGDM with the exception that we set the momentum parameter \(\beta\) to \(0\). For Birder and SoftSignSGD, we adopted the experimental settings of AdamW, except that we set the momentum parameter \(\beta\) to \(0.99\).

Footnote 7: https://github.com/juntang-zhuang/Adabelief-Optimizer

As visually illustrated in Figure 6, Vanilla SGD is still less effective than SGDM in terms of the convergence speed and the test accuracy, while the training and inference performance of SoftSignSGD and Birder are comparative to common-used SGDM and AdamW. It indicates the Birder is superior to exiting gradient compression algorithms for training LSTM.

### Experimental Results for training ViT

We train ViT-B with Vanilla SGD, SGDM, AdamW, SoftSignSGD and Birder on the ILSVRC2012 with 32 GPUs (4 nodes). We use the Pytorch official implementation for ViT 8. For AdamW, SoftSignSGD and Birder, we followed the recommended experimental settings, with the exception that we set the momentum parameter \(\beta\) to \(0.95\) for SoftSignSGD and Birder. As for Vanilla SGD and SGDM, we set the basic learning rate to \(0.1\) and the weight decay to \(0.001\), while keeping other settings the same as AdamW. Instead of the default \(300\) epochs, we uniformly set the total number of epochs to \(150\) for all optimizers

Footnote 8: https://github.com/pytorch/vision/tree/main/references/classification

As visually illustrated in Figure 7, Vanilla SGD is still less effective than SGDM in terms of the convergence speed and the test accuracy, while the training and inference performance of SoftSignSGD and Birder are comparative to common-used SGDM and AdamW. Notably, the performance of SGD-type optimizers are substantially inferior to that of adaptive optimizers.

Figure 5: Training loss and test accuracy for ResNet-20 on CIFAR100.

## Appendix C Experimental Results for pre-training BERT-Base

We employed BertAdam and Binder to pre-train BERT-Base on Wikipedia using \(64\) GPUs (8 nodes). The sequence length was set to \(512\), and the batch size per GPU was set to \(16\). The training process consisted of \(37,000\) iterations. The learning rate started at \(4\times 10^{-}4\) and linearly increased in the first \(12,500\) iterations, after which it linearly decreased to \(0\) for the remaining iterations. ForBertAdam, the parameter values \([\beta_{1},\beta_{2}]\) were set to \([0.9,0.999]\), and for Binder, the momentum parameter beta was set to \(0.9\).

As depicted in Figure 8,Binder demonstrates a comparable iteration-wise convergence rate to BertAdam. However, in terms of time-wise convergence, Binder achieves a 4.2x faster convergence rate compared to BertAdam.

## Appendix D Experiments with InfiniBand connections

To further evaluate the communication efficiency of SGDM/Adam, SoftSignSGD and Binder with high bandwidth connections, we implement experiments for training ResNet-50 and BERT-Base with distributed nodes connected with 200Gbps InfiniBand. All the experimental settings are the same as we perform experiments with Ethernet in Subsection 5.1, and the experimental results are listed in Table 3 and Table 4.

As shown in Table 3 and Table 4, compared with the baseline SGDM/Adam, Binder can still reach up to \(1.45\times\) speedup for ResNet-50 on ILSVRC2012 and \(2.85\times\) speedup for BERT-Base on SQuAD 1.1, although the speed advantage is not so obvious as that with lower-bandwidth Ethernet connections. An interesting phenomenon

Figure 6: Training loss and test perplexity (the lower, the better) for 3-layer LSTM on Penn TreeBank.

Figure 7: Training loss and test accuracy for ViT-B-16 on ILSVRC2012.

is that the system throughput of Birder with 10Gbps Ethernet can match that of SGDM/Adam with 200Gbps InfiniBand.

The experimental results in Table 3 and Table 4 also show that as the number of GPUs is increasing, the scale efficiency of SGDM/Adam, SoftSignSGD and Birder becomes lower. The reason for this phenomenon can be summarized in the following. When the number of GPUs doubles, the number of communication trips also

Figure 8: Iteration-wise and time-wise convergence speed for pre-training BERT-Base with \(16\) samples per GPU with \(64\) GPUs.

multiplies. We take the communication scheme _All-Reduce_ for example. If the number of GPUs is \(n\), each GPU requires \(2(n-1)\) trips across the network confections. When the number is non-trivial, the computation time of the communication primitives may exceed the time of the pure communication itself and dominate the overall communication time, since the total communication overhead does not change with the number of GPUs. Notably, _All-reduce_ is more efficient than _All-to-All_ which is the core of our _Hierarchical-1-bit-All-Reduce_. Hence, as shown in in Table 3 and Table 4, the scale efficiency of Birder decreases more quickly than SGDM/Adam with the number of GPUs growing.

## Appendix E Discussion

The original paper on 1-bit Adam reports a significant speed advantage (up to \(3.8\times\)) for 1-bit Adam compared to full-precision Adam, with the advantage becoming more prominent as the number of GPUs increases. However, in our experiments, we did not observe clear speed advantages for 1-bit Adam over the original Adam. In fact, when running on \(64\) GPUs, 1-bit Adam was not only slower than the original Adam, but its throughput rate was even lower than that on \(32\) GPUs. There are several reasons for this phenomenon. First, the speedup of 1-bit Adam is obtained by comparing the throughput of the compression phase with that of the warm-up phase. However, in our experiments, we evaluated the overall average throughput of both the warm-up phase and the compression phase for 1-bit Adam. Second, the baseline Adam did not run with system-level efficient \(DDP\). Third, the authors of 1-bit Adam customized highly efficient communication primitives specifically for their optimizer, whereas we utilized off-the-shelf communication primitives in PyTorch for all the optimizers to ensure fairness.

As shown in Figure 4, as the number of GPUs increases, the communication time for Birder also grows superlinearly. One of the reasons for this is that the communication primitive _All-to-All_ accounts for an increasing portion of the communication time. However, the native _All-to-All_ in Step (iii) of the _Hierarchical-1-bit-All-Reduce_ is not less efficient than the native _All-Reduce_. Therefore, we plan to further optimize the _All-to-All_ and _All-Gather_ primitives to accelerate Birder.

When training large-scale DNNs, the mixed-precision technique is commonly used to reduce memory consumption, allowing for larger model sizes. While optimizers still utilize full-precision states and computations, which typically contribute to 33-75% of the total memory footprint, Birder does not require full-precision states or computations. Moreover, due to the random quantization of updates to 1 or -1, Birder can leverage lower precision than FP16 gradients to estimate the update. Therefore, Birder shows promise for applications that focus on reducing memory usage, as highlighted in recent research on 8-bit optimizers via block-wise quantization (Tim Dettmers et al., ICLR 2022).