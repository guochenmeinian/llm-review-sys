# Diversifying Spatial-Temporal Perception

for Video Domain Generalization

 Kun-Yu Lin\({}^{1}\)1 &Jia-Run Du\({}^{1}\)1 &Yipeng Gao\({}^{1}\) &Jiaming Zhou\({}^{1}\)

Wei-Shi Zheng\({}^{1,2}\)2

\({}^{1}\)School of Computer Science and Engineering, Sun Yat-sen University, China

\({}^{2}\)Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China

{linky5,dujr6,gaoyp23,zhoujm55}@mail2.sysu.edu.cn

wszheng@ieee.org

Equal contributionsCorresponding author

Footnote 1: footnotemark:

###### Abstract

Video domain generalization aims to learn generalizable video classification models for unseen target domains by training in a source domain. A critical challenge of video domain generalization is to defend against the heavy reliance on domain-specific cues extracted from the source domain when recognizing target videos. To this end, we propose to perceive diverse spatial-temporal cues in videos, aiming to discover potential domain-invariant cues in addition to domain-specific cues. We contribute a novel model named Spatial-Temporal Diversification Network (STDN), which improves the diversity from both space and time dimensions of video data. First, our STDN proposes to discover various types of spatial cues within individual frames by spatial grouping. Then, our STDN proposes to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales by spatial-temporal relation modeling. Extensive experiments on three benchmarks of different types demonstrate the effectiveness and versatility of our approach.

## 1 Introduction

Recently, advanced deep network architectures have achieved competitive results for video classification [1; 2; 3; 4; 5; 6; 7; 8], leading to wide applications in surveillance systems, sport analysis, health monitoring, etc [9; 10; 11]. However, existing video classification models rely on the i.i.d. assumption, _i.e._, training and test videos are independently and identically distributed. This assumption would be easily violated, since models often face unfamiliar scenarios in real-world applications. For example, a housework robot will work in a new house, and a surveillance system will encounter illumination change caused by camera viewpoint or weather [12; 13; 14]. Holding such an assumption, the performance of video classification models would drop significantly in unfamiliar test scenarios.

To alleviate the above problem, our work studies the video domain generalization task, which aims to learn a video classification model that is generalizable in _unseen_ target domains by training in a source domain [15; 16]. In this task, videos from the source and target domains follow different distributions though with an identical label space. For example, as shown in Figure 1, humans in the source domain play basketball shooting on indoor basketball courts while those in the target domain play outdoors. Different from the video domain adaptation task with available unlabeled target videos for training [17; 18; 19; 20], video domain generalization can only access the source domain during training, which is much more challenging but more practical.

A critical challenge of video domain generalization is to defend against the reliance on domain-specific cues in the source domain that are correlated with class labels. For example, as shown in Figure 1, video classification models prefer to leverage the backboard for recognizing the class "shoot ball" in the source domain, since the static backboard provides a clearer cue compared with the blurred basketball in motion (static patterns are usually easy-to-fit [21; 22; 23; 24]). However, in the target domain, the backboard is occluded due to the viewpoint, thus recognizing the class by the backboard would cause recognition errors. It is challenging to address this problem in lack of any knowledge of the target domain. Typically, existing works explore invariance across domains for learning generalizable video features [25; 15; 16]. For example, Yao et al. propose to learn generalizable temporal features by encoding information of local features into global features, assuming that local temporal features are more invariant across domains compared with global ones [15].

In this work, we propose a novel approach for video domain generalization, which explore spatial-temporal diversity in videos. Our approach aims to perceive diverse class-correlated cues from abundant video contents, and thus we would leverage not only easy-to-fit domain-specific cues but also other _potential_ domain-invariant cues for recognizing videos in target domains (_e.g._, we expect that our model can capture not only static backboards but also dynamic basketballs in the source domain). As a result, our approach can alleviate the overfitting of domain-specific cues in the source domain and generalize better in target domains by leveraging those potential domain-invariant cues. Specifically, we propose to explore the diversity from both space and time dimensions of video data, leading to a novel architecture named Spatial-Temporal Diversification Network (STDN). Our contributions are summarized as follows:

* We propose Spatial Grouping Module to discover various groups of spatial cues within individual frames by embedding a clustering-like process, enriching the diversity from a spatial modeling perspective.
* We propose Spatial-Temporal Relation Module to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales, enriching the diversity from a spatial-temporal relation modeling perspective.
* Extensive experiments are conducted on three benchmarks of different types, including two newly designed benchmarks, and the results demonstrate the effectiveness and versatility of our proposed method.

## 2 Related Works

**Video Classification** aims to recognize actions or events in videos. Recently, many advanced deep learning architectures have been proposed for video classification. 3D CNNs extend the 2D convolution to 3D convolution for video feature learning [1; 2; 3; 26; 27; 28; 29; 30]. Another

Figure 1: Video classification models suffer from the misguidance of domain-specific cues when generalizing to unseen domains. As shown in the figure, in the source domain, the static backboard provides a clearer cue compared with the blurred basketball in motion, thus prevailing video classification models are prone to recognize the class “Shoot Ball” by the backboard. However, the backboard is invisible in the target domain due to viewpoint change, and thus previous models learned in the source domain would make mistakes in recognition. Videos in the figure are from the UCF-HMDB benchmark. Best viewed in color.

type of models first applies 2D convolution for frame-level spatial modeling and then conducts temporal modeling based on frame features [5; 6; 31; 32; 33]. Some works propose to couple explicit shifts along the time dimension for efficient temporal modeling [7; 34; 35]. More recently, pioneer works have made efforts in adapting Vision Transformer [36] for video classification [37; 38; 39; 40; 41; 42; 43; 44; 45]. Although these advanced architectures achieve appealing performance, they usually assume an identical test distribution to the training one, which is not practical in real-world applications.

**Video Domain Generalization** aims to train video classification models in a source domain for generalizing to _unseen_ target domains. With target videos inaccessible during training, existing works usually assume different types of invariance across domains to defend against the reliance on domain-specific cues [25; 15; 16]. For example, Yao et al. propose to learn generalizable temporal features according to an assumption from empirical findings, _i.e._, local temporal features are more invariant across domains compared with the global ones [15]; Planamente et al. propose to constrain a consistency across visual and audio modalities by relative norm alignment for addressing domain generalization of egocentric action recognition [16]. In this work, we propose to perceive diverse class-correlated spatial-temporal cues in source videos, which alleviates the misguidance of domain-specific cues in a way that is orthogonal to previous works.

**Video Domain Adaptation** aims to learn transferable video classification models for a label-free target domain by transferring knowledge from a label-sufficient source domain [17; 18]. Different from video domain generalization, video domain adaptation is oriented to a specific _seen_ unlabeled target domain. Typically, existing works learn invariance between labeled source videos and unlabeled target videos to tackle video domain adaptation. A class of representative works propose to learn domain-invariant temporal features by designing temporal modeling modules [18; 19; 46; 47]. In addition, Choi et al. [20; 48] propose self-supervised methods adaptive to video data. Furthermore, multi-modal works explore information interaction between different modalities (_e.g._, RGB, Flow, Audio) for domain-invariant feature learning [49; 50; 51; 52; 53].

**General Domain Generalization**, also known as out-of-distribution generalization, studies learning models generalizable to out-of-distribution data for the image classification task. In recent years, a plethora of methods have been proposed to address domain generalization [25; 54; 55; 56]. Prevailing methods are mainly based on feature alignment [57; 58; 59], domain adversarial learning [60; 61], invariant risk minimization [62; 63; 64; 65], meta learning [66; 67; 68; 69; 70], data augmentation [71; 72; 73; 74; 75], etc. In addition, ensemble learning is an effective way to learn generalizable models [76; 77; 78]. And recently, Zhu et al. develop a theory showing that ensemble learning can provably improve test accuracy by discovering the "multi-view" structure of data [79], which partially inspires our approach. Among architecture-based methods [80; 81], Meng et al. propose to redesign attention modules for learning diverse task-related features [80]. Different from existing general domain generalization methods, we propose a domain generalization method specific to video classification, which explores diverse class-correlated information in intrinsic space and time dimensions of video data. There are some works that study the identification of out-of-distribution data of different categories from training data [82; 83; 84; 85; 86; 87; 88], but this topic is not within the scope of our work.

## 3 Spatial-Temporal Diversification Network

In this section, we illustrate our proposed Spatial-Temporal Diversification Network (STDN) in detail, which perceives diverse class-correlated spatial-temporal cues from video contents for generalization in unseen target domains.

### Problem Formulation

In video domain generalization, a set of labeled videos \(\mathcal{D}=\{(x,y)\}\) from a source domain are given for training, where \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\) denote a source video and its corresponding class label. Given only the source video set, the goal of video domain generalization is to learn a video classification model that is generalizable in _unseen_ target domains. The source and target domains follow different but related distributions with the same label space \(\mathcal{Y}=\{0,1,\dots,C-1\}\), where \(C\) denotes the number of video classes. Following the standard video domain generalization setting [15], each video is evenly divided into \(N\) segments, and one frame is sampled from each segment as the model input during training and testing, _i.e._, \(x=\{x_{1},x_{2},\ldots,x_{N}\}\) and \(x_{n}\) denotes the \(n\)-th sampled frame from the \(n\)-th segment.

### Model Overview

Aiming at generalization in unseen target domains, our idea is to perceive rich and diverse class-correlated cues in the source domain. In this way, our model would leverage not only easy-to-fit domain-specific cues but also other potential domain-invariant cues for recognizing videos in the target domain, alleviating the misguidance of domain-specific cues. Considering the intrinsic space and time dimensions of video data, we propose to explore the diversity in both spatial and temporal modeling. An overview of our proposed STDN is shown in Figure 2. Firstly, given the video \(x\), our STDN takes \(N\) sampled frames as input and separately extracts \(N\) spatial feature maps \(\{z_{1},z_{2},...,z_{N}\}\) by the backbone (_e.g._, ResNet [89]), where \(z_{n}\in\mathbb{R}^{H\times W\times D}\) denotes the feature map of the \(n\)-th frame, \(D\) denotes the feature dimension and \(H\times W\) denotes the size of feature maps. Then, we extract spatial cues of \(K\) types (groups) from each spatial feature map by our proposed Spatial Grouping Module, aiming to enrich the spatial diversity. In the Spatial Grouping Module, two entropy-based losses are introduced to enhance the distinction between different spatial cues. On top of the Spatial Grouping Module, we propose to explicitly model spatial-temporal dependencies between video contents at multiple space-time scales by our proposed Spatial-Temporal Relation Module. The learning of the Spatial-Temporal Relation Module is guided by a relation discrimination loss, which ensures the diversity of the extracted spatial-temporal relation features. Finally, diverse spatial-temporal features are aggregated for video domain generalization.

### Spatial Grouping Module

Our proposed Spatial Grouping Module aims to discover diverse class-correlated spatial cues from abundant contents of individual frames, which enriches the diversity from a spatial modeling perspective for video domain generalization. Our Spatial Grouping Module extracts various spatial cues of different types by partitioning features from different spatial positions into several groups within individual frames. In this way, our Spatial Grouping Module discovers more diverse spatial cues, compared with prevailing approaches that extract an integrated feature for each frame (_e.g._, by average pooling).

As shown in Figure 3 (a), given the spatial feature map \(z_{n}\in\mathbb{R}^{H\times W\times D}\) of the \(n\)-th frame, our proposed Spatial Grouping Module learns to extract \(K\) spatial cues by aggregating the \(HW\) spatial features. Specifically, the proposed spatial grouping process is conducted based on \(K\) learnable anchor features \(\{a_{n,1},a_{n,2},\ldots,a_{n,K}\}\), where \(a_{n,k}\in\mathbb{R}^{D}\) denotes the anchor feature of the \(k\)-th spatial group for the \(n\)-th frame. Then, we calculate the probability of assigning a spatial feature to each spatial group, which is formulated as follows:

\[p_{n,i,k}=\frac{\exp\left(-\mathrm{dist}\left(z_{n,i},a_{n,k}\right)/\tau \right)}{\sum_{j=1}^{K}\exp\left(-\mathrm{dist}\left(z_{n,i},a_{n,j}\right)/ \tau\right)},\] (1)

Figure 2: An overview of our proposed Spatial-Temporal Diversification Network (STDN). We use a video of \(N=5\) segments with \(K=4\) spatial groups for example. After backbone feature extraction, our STDN extracts spatial cues of \(K\) types for each frame by the Spatial Grouping Module, enriching the diversity in spatial modeling. Then, our STDN explicitly models spatial-temporal dependencies at multiple space-time scales, enriching the diversity in spatial-temporal relation modeling. Best viewed in color.

where \(z_{n,i}\in\mathbb{R}^{D}\) denotes the \(i\)-th spatial feature in the feature map \(z_{n}\) (\(i\in[1,2,\ldots,HW]\)), \(\mathrm{dist}(\cdot,\cdot)\) denotes the Euclidean distance metric and \(\tau\) is the temperature factor. According to Eq. (1), if the spatial feature \(z_{n,i}\) is closer to the anchor feature \(a_{n,k}\), then the \(z_{n,i}\) will be assigned to the \(k\)-th spatial group with higher probability. After group partition, our Spatial Grouping Module produces \(K\) integrated features representing \(K\) different spatial cues by aggregating spatial features in each group. The integration process is formulated as follows:

\[z_{n,k}^{s}=\frac{1}{\sum_{i=1}^{HW}p_{n,i,k}}\sum_{i=1}^{HW}p_{n,i,k}*z_{n,i},\] (2)

where \(z_{n,k}^{s}\) denotes the spatial cues integrated from the \(k\)-th group within the \(n\)-th frame.

In order to extract spatial cues of diverse types, we introduce two entropy-based losses to enhance the distinction between different spatial groups. The first one is an entropy minimization loss to enhance the confidence of group assignment for each spatial feature. The loss is formulated as follows:

\[L_{\mathrm{emin}}=-\frac{1}{NHW}\sum_{n=1}^{N}\sum_{i=1}^{HW}\sum_{k=1}^{K}p_ {n,i,k}\log\left(p_{n,i,k}\right).\] (3)

For the assignment probability vector \(p_{n,i}=[p_{n,i,1},p_{n,i,2},\ldots,p_{n,i,K}]^{T}\in\mathbb{R}^{K\times D}\), if the entropy is minimized, then the feature \(z_{n,i}\) will be confidently assigned to a specific spatial group. The second loss is an entropy maximization loss for the mean assignment probability vector, which guarantees that those \(HW\) spatial features are assigned to different spatial groups. Specifically, the loss is formulated as follows:

\[L_{\mathrm{emax}}=\frac{1}{N}\sum_{n=1}^{N}\sum_{k=1}^{K}\bar{p}_{n,k}\log \left(\bar{p}_{n,k}\right),\] (4)

where \(\bar{p}_{n,k}=\frac{1}{HW}\sum_{i=1}^{HW}p_{n,i,k}\) denotes the mean probability of assigning features to the \(k\)-th group within the \(n\)-th frame. For the mean assignment probability vector \(\bar{p}_{n}=[\bar{p}_{n,1},\bar{p}_{n,2},\ldots,\bar{p}_{n,K}]^{T}\in\mathbb{R }^{K\times D}\), if the entropy is maximized, then the spatial features \(\{z_{n,i}\}\) will be uniformly assigned to \(K\) spatial groups. By using the two entropy-based losses, we guarantee that spatial features are different from each other across different spatial groups, enriching the diversity of extracted spatial cues.

In the Spatial Grouping Module, the learnable anchor feature for each group is extracted by weighted combining those \(HW\) spatial features, and the weights are calculated conditioned on the feature map \(z_{n}\) by using a lightweight two-layer convolutional network. In this way, the spatial grouping process can be regarded as conducting clustering over spatial features within individual frames. All involved parameters in the module are end-to-end trained together with the main network, _i.e._, we contribute a parametric clustering module to group spatial features for improving the spatial diversity.

### Spatial-Temporal Relation Module

Our proposed Spatial-Temporal Relation Module aims to discover diverse class-correlated spatial-temporal cues from abundant video contents, which enriches the diversity from a spatial-temporal relation modeling perspective for video domain generalization. As demonstrated by previous works [4; 90; 4], there are rich dependencies between entities over space and time dimensions in videos, which

Figure 3: Overview of our proposed (a) Spatial Grouping Module and (b) Spatial-Temporal Relation Module. Best viewed in color.

is crucial for video classification. Accordingly, we propose to explicitly model spatial-temporal dependencies between video cues at multiple space-time scales. Our proposed Spatial-Temporal Relation Module conducts dependency modeling at space and time dimensions separately, and an overview of the module is shown in Figure 3 (b).

First, based on the spatial cues extracted by our Spatial Grouping Module, we conduct spatial dependency modeling between these spatial cues at multiple space scales. Specifically, given the representations of spatial cues \(z_{n}^{s}=[z_{n,1}^{s},z_{n,2}^{s},\dots,z_{n,K}^{s}]^{T}\in\mathbb{R}^{K\times D}\) for the \(n\)-th frame, we extract the spatial relation feature at the \(l\)-th space scale by the spatial dependency modeling function \(R_{l}^{s}(\cdot)\) as follows:

\[R_{l}^{s}(z_{n}^{s})=\mathbb{E}_{k_{1},k_{2},\dots,k_{l}}\left[H_{l}^{s}(z_{n, k_{1}}^{s},z_{n,k_{2}}^{s},\dots,z_{n,k_{l}}^{s})\right]\in\mathbb{R}^{D_{s}},\] (5)

where \(\mathbb{E}[\cdot]\) denotes the expectation calculation and \(H_{l}^{s}(\cdot,\cdot,\dots,\cdot)\) denotes a linear projection function after feature concatenation. The index set \(\{k_{1},k_{2},\dots,k_{l}\}\) denotes the index of spatial features uniformly sampled from the \(K\) spatial features, where the index \(l\in\{2,3,\dots,K\}\) indicates the space scale, \(k_{1}\neq k_{2}\neq\dots\neq k_{l}\) and \(k_{i}\in\{1,2,\dots,K\}\). For each frame, we extract \(K-1\) spatial relation features by dependency modeling at \(K-1\) space scales separately. And, we concatenate these spatial relation features and produce an integrated feature for each frame, which is given by \(\hat{z}_{n}=[R_{2}^{s}(z_{n}^{s})^{T},R_{3}^{s}(z_{n}^{s})^{T},\dots,R_{K}^{s} (z_{n}^{s})^{T},G(z_{n})^{T}]^{T}\in\mathbb{R}^{KD_{s}}\). In the integrated feature \(\hat{z}_{n}\), \(G(z_{n})\in\mathbb{R}^{D_{s}}\) denotes the global feature extracted from the feature map \(z_{n}\) by a convolution layer.

Second, based on the frame-level integrated features, we conduct temporal dependency modeling between frames at multiple time scales. Specifically, given \(N\) frame-level features denoted by \(\hat{z}=[\hat{z}_{1},\hat{z}_{2},\dots,\hat{z}_{N}]\), we extract the temporal relation feature at the \(m\)-th time scale by the temporal dependency modeling function \(R_{m}^{t}(\cdot)\) as follows:

\[R_{m}^{t}(\hat{z})=\mathbb{E}_{n_{1}<n_{2}<\dots<n_{m}}\left[H_{m}^{t}(\hat{z }_{n_{1}},\hat{z}_{n_{2}},\dots,\hat{z}_{n_{m}})\right]\in\mathbb{R}^{D_{t}},\] (6)

where \(H_{m}^{t}(\cdot,\cdot,\dots,\cdot)\) denotes a linear projection function after feature concatenation. The index set \(\{n_{1},n_{2},\dots,n_{m}\}\) denotes the index of frame features randomly sampled from the \(N\) frame features, where the index \(m\in\{2,3,\dots,N\}\) indicates the time scale and \(n_{i}\in\{1,2,\dots,N\}\). Note that we keep the relative order of sampled frames for temporal modeling. By using \(N-1\) temporal dependency modeling functions, we extract \(N-1\) temporal relation features at \(N-1\) time scales for each video.

To ensure the diversity of temporal relation features, we propose a relation discrimination loss to constrain that different temporal dependency modeling functions (_i.e._, different time scales) capture different temporal cues. This loss constrains that a relation classifier can distinguish one relation feature from not only relation features of other classes but also relation features of the same class but of other time scales. Thus, it avoids the feature collapse of learned temporal relation features. Specifically, the loss is formulated as follows:

\[L_{\mathrm{rel}}=\frac{1}{N-1}\sum_{m=2}^{N}\mathrm{CE}(F_{\mathrm{rel}}(\tilde {z}_{m}),\tilde{y}_{m}),\] (7)

where \(\tilde{z}_{m}=R_{m}^{t}(\hat{z})\) denotes the temporal relation feature at the \(m\)-th time scale, \(F_{\mathrm{rel}}(\cdot)\) denotes a relation classifier that classifying \((N-1)*C\) classes, and \(\mathrm{CE}(\cdot,\cdot)\) denotes the cross-entropy loss. The \(\tilde{y}_{m}\) denotes the relation label of the video \(x\) with label \(y\), _i.e._, \(\tilde{y}_{m}=y*(N-1)+(m-2)\in\{0,1,2,\dots,(N-1)*C-1\}\). In this way, the loss forces different temporal dependency modeling functions to capture different class-correlated temporal cues in the video since the captured temporal cues are discriminative across scales. By incorporating the Spatial-Temporal Relation Module with the relation discrimination loss \(L_{\mathrm{rel}}\), we extract rich and diverse spatial-temporal cues.

**Feature Aggregation:** After exploring spatial-temporal diversity by our proposed Spatial Grouping Module and Spatial-Temporal Relation Module, our model discovers diverse class-correlated spatial-temporal cues from abundant video contents. Then, we aggregate these diverse spatial-temporal features for video classification. Specifically, the feature aggregation is formulated as \(\hat{z}=\sum_{m=2}^{N}H_{m}^{a}(\hat{z}_{m})\), where \(H_{m}^{a}(\cdot)\) denotes a small SE-based block [91] for modulating the \(m\)-th temporal relation features.

**Overall Training and Test:** We adopt a video classification loss on top of the aggregated feature \(\hat{z}\) given by \(L_{\mathrm{cls}}=\mathrm{CE}(F(\hat{z}),y)\), where \(F(\cdot)\) is a video classifier. Overall, the training loss of our proposed STDN is given as follows:

\[L=L_{\mathrm{cls}}+\lambda_{\mathrm{ent}}L_{\mathrm{emin}}+\lambda_{\mathrm{ent}}L _{\mathrm{emax}}+\lambda_{\mathrm{rel}}L_{\mathrm{rel}},\] (8)

where \(\lambda_{\mathrm{ent}}\) and \(\lambda_{\mathrm{rel}}\) are hyperparameters for trade-off. Following the standard protocol [15], we use source videos for training and test the model on target videos for evaluation.

## 4 Experiments

### Benchmarks and Experimental Setups

To demonstrate the effectiveness and versatility of our proposed Spatial-Temporal Diversification Network (STDN), we adopt three benchmarks of different types for experiments, including two newly designed benchmarks, namely EPIC-Kitchens-DG and Jester-DG. For these two new benchmarks, we select video categories and construct domains following previous video domain adaptation works [49; 19]. We split the source video set into training and validation sets following previous source validation protocols [25; 15], _i.e._, a reasonable in-domain model selection scheme for better generalization ability in unseen target domains. We reproduce general domain generalization methods (cooperated with video classification architectures) and state-of-the-art video domain generalization methods for comparison. We report mean and standard deviation of accuracy over three random trials for all methods.

**UCF-HMDB** is the most widely used benchmark for cross-domain video classification [15; 18], which contains 3,809 videos of 12 overlapping sport categories shared by UCF101 [94] and HMDB51 [95]. The videos in UCF101 are mostly captured from certain scenarios or similar environments, and the videos in HMDB51 are captured from unconstrained environments and different camera viewpoints. This benchmark includes two subtasks, i.e., UCF\(\rightarrow\)HMDB and HMDB\(\rightarrow\)UCF.

**EPIC-Kitchens-DG** is a _cross-scene egocentric action recognition_ benchmark, which consists of 10,094 videos across 8 egocentric action classes from three domains (scenes), following Munro et al. [49]. The three domains of EPIC-Kitchens-DG (_i.e._, D1, D2, D3) correspond to three largest kitchens (_i.e._, P08, P01, P22) from the large-scale egocentric action recognition dataset EPIC-Kitchens-55 [96]. This benchmark includes six subtasks constructed from three domains.

**Jester-DG** is a _cross-domain hand gesture recognition_ benchmark. We select videos from the Jester dataset [97] and construct two domains following Pan et al. [19]. The source (S) and target (T) domains contain 51,498 and 51,415 video clips across 7 categories, respectively. The videos in EPIC-Kitchens-DG and Jester-DG benchmarks are both hand-centric, but they are captured from different views, namely first-person and third-person views.

**Implementation details:** We use ResNet50 [89] pretrained on ImageNet [98] as the backbone for frame-level feature extraction following the standard video domain generalization protocol [15]. The backbone takes frames of size \(224\times 224\) as input and outputs feature maps of size \(7\times 7\times 2048\). We take \(N=5\) frames for each video for temporal modeling. We set \(K=4\), \(\tau=0.5\), \(D_{s}=192\) and \(D_{t}=256\). \(F(\cdot)\) is a linear classifier and \(F_{\mathrm{rel}}(\cdot)\) is an MLP classifier. All parameters are optimized using mini-batch SGD with a batch size of 32, a momentum of 0.9, a learning rate of

\begin{table}
\begin{tabular}{c|c c c||c|c c} \hline \hline Arch & DG Method & UCF\(\rightarrow\)HMDB & HMDB\(\rightarrow\)UCF & Arch & DG Method & UCF\(\rightarrow\)HMDB & HMDB\(\rightarrow\)UCF \\ \hline \multirow{4}{*}{TSN [5]} & ERM & 51.4\(\pm\)0.2 & 68.6\(\pm\)0.3 & \multirow{4}{*}{TSM [7]} & ERM & 52.2\(\pm\)0.3 & 69.2\(\pm\)0.3 \\  & ADA\({}_{sem}\)[71] & 51.1\(\pm\)0.3 & 68.2\(\pm\)0.2 & \multirow{4}{*}{TSM [7]} & ADA\({}_{sem}\)[71] & 51.3\(\pm\)0.3 & 68.6\(\pm\)0.3 \\  & ADA\({}_{pixel}\)[71] & 49.6\(\pm\)0.3 & 67.4\(\pm\)0.2 & \multirow{4}{*}{TSM [7]} & ADA\({}_{pixel}\)[71] & 52.7\(\pm\)0.3 & 68.3\(\pm\)0.2 \\  & M-ADA [72] & 52.4\(\pm\)0.2 & 69.2\(\pm\)0.2 & & M-ADA [72] & 52.5\(\pm\)0.2 & 69.1\(\pm\)0.3 \\  & Jigsaw [93] & 51.5\(\pm\)0.3 & 68.5\(\pm\)0.3 & \multirow{4}{*}{Jigsaw [93]} & 52.5\(\pm\)0.3 & 68.9\(\pm\)0.3 \\ \hline \multirow{4}{*}{APN [15]} & ERM & 54.3\(\pm\)0.3 & 71.4\(\pm\)0.3 & \multirow{4}{*}{TRN [6]} & ERM & 52.4\(\pm\)0.3 & 69.8\(\pm\)0.3 \\  & ADA\({}_{sem}\)[71] & 55.2\(\pm\)0.3 & 71.9\(\pm\)0.3 & \multirow{4}{*}{ADAs [71]} & ADA\({}_{sem}\)[71] & 52.8\(\pm\)0.2 & 69.6\(\pm\)0.5 \\  & ADA\({}_{pixel}\)[71] & 56.9\(\pm\)0.2 & 72.2\(\pm\)0.3 & \multirow{4}{*}{TRN [6]} & ADA\({}_{pixel}\)[71] & 52.1\(\pm\)0.3 & 70.6\(\pm\)0.2 \\  & M-ADA [72] & 55.6\(\pm\)0.3 & 71.5\(\pm\)0.3 & & M-ADA [72] & 53.4\(\pm\)0.3 & 69.9\(\pm\)0.3 \\  & Jigsaw [93] & 55.2\(\pm\)0.4 & 72.4\(\pm\)0.3 & \multirow{4}{*}{Jigsaw [93]} & 53.3\(\pm\)0.3 & 70.1\(\pm\)0.3 \\ \hline \hline \multicolumn{2}{c|}{VideoDG [15]} & 59.1\(\pm\)0.3 & 74.9\(\pm\)0.3 & STDN (Ours) & **60.2\(\pm\)**0.5 & **77.1\(\pm\)**0.4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with state-of-the-art methods on the UCF-HMDB benchmark. Red and blue denotes the best and second best. Results of all compared methods are from VideoDG [15].

[MISSING_PAGE_FAIL:8]

pendency modeling of our Spatial-Temporal Relation Module (denoted by TRM), we obtain 1.8% and 1.4% improvement on UCF\(\rightarrow\)HMDB and HMDB\(\rightarrow\)UCF, respectively. It should be noted that the relation discrimination loss \(L_{\mathrm{rel}}\) is an important part in temporal dependency space and time dimensions, which enriches the diversity in spatial-temporal relation modeling. Moreover, by introducing the feature augmentation technique MixStyle, we obtain further improvement. Finally, our full model aggregates diverse spatial-temporal features, leading to better generalization performance on both UCF\(\rightarrow\)HMDB and HMDB\(\rightarrow\)UCF.

**Diversity Analysis:** We make a quantitative analysis to the diversity of learned video features for our model. Specifically, we evaluate the difference between temporal relation features of different time scales, measured by the normalized Mean Square Error (MSE) between feature vectors. A higher value of normalized MSE indicates a large difference. As shown in Figure 4, without our relation discrimination loss \(L_{\mathrm{rel}}\), learned temporal relation features at different time scales hold very small difference (implying feature collapse). By introducing \(L_{\mathrm{rel}}\), our TRM improves the diversity, indicated by the higher MSE value. By introducing our Spatial Grouping Module, the diversity is further improved as various spatial cues are extracted from each frame. Moreover, by modeling spatial dependencies, our model further enlarges the difference between features across scales.

**Analysis of Spatial Grouping:** We make a qualitative analysis to the grouping process of our proposed Spatial Grouping Module (SGM). Specifically, we use t-SNE [104] for visualizing feature distributions of spatial features, and we adopt the model trained without our SGM as the baseline (adopts average pooling to extract an integrated feature for each frame) for comparison. Also, we use the Davies-Bouldin Index1 as a quantitative metric to measure the clustering performance, i.e., a lower value of the Davies-Bouldin Index indicates better separation between clusters. As shown by the qualitative and quantitative results in Figure 5, our SGM extracts spatial features with better cluster separation than the baseline, which is attributed to that our SGM enhances the distinction between features in different spatial groups. These results indicate that our proposed spatial grouping process forces the model to learn features encoding more different information. In the supplemental material, we also show Grad-CAM examples to qualitatively compare our SGM with the baseline.

Figure 4: Diversity analysis on UCF\(\rightarrow\)HMDB. We use the normalized Mean Square Error (MSE) to evaluate the feature diversity of variants of TRM, _i.e._, measuring difference between temporal relation features of different time scales. A higher value of normalized MSE indicates higher diversity.

Figure 5: T-SNE visualization of spatial features. For both the baseline and SGM, we cluster the set of spatial features into \(K=4\) clusters by \(K\)-means before visualization. In the figure, dots stand for spatial feature vectors and stars stand for cluster centers, and different colors denote different clusters.

\begin{table}
\begin{tabular}{c|c c} \hline Method & UCF\(\rightarrow\)HMDB & HMDB\(\rightarrow\)UCF \\ \hline Backbone & 52.7\(\pm_{0.3}\) & 71.9\(\pm_{0.3}\) \\ +SGM & 54.9\(\pm_{0.3}\) & 73.9\(\pm_{0.4}\) \\ +TRM & 56.7\(\pm_{0.2}\) & 75.3\(\pm_{0.4}\) \\ +STRM & 58.3\(\pm_{0.4}\) & 76.2\(\pm_{0.3}\) \\ +MixStyle & 59.3\(\pm_{0.3}\) & 76.6\(\pm_{0.2}\) \\ Full STDN & 60.2\(\pm_{0.5}\) & 77.1\(\pm_{0.4}\) \\ \hline \end{tabular}
\end{table}
Table 3: Ablation study on UCF-HMDB.

**Grad-CAM Visualization:** We compare our proposed STDN with a TRN [6] model (the baseline) by Grad-CAM [106]. As shown in Figure 6, the baseline prefers to use the domain-specific backboard for recognition, which causes recognition errors in the target domains as backboards are invisible. In contrast to the baseline, our proposed STDN perceives more diverse class-correlated cues from the source domain, including some domain-invariant cues such as basketballs. As a result, our STDN can predict the correct video class by recognizing the basketball in the target video. These results demonstrate that, our proposed diversity-based approach can discover some potential domain-invariant cues, which alleviates the overfitting to domain-specific cues and leads to better generalization in the target domain.

## 5 Conclusion

In this work, we propose to explore spatial-temporal diversity to address the video domain generalization task. Our proposed Spatial-Temporal Diversification Network learns diverse spatial-temporal features in videos, which discovers potential domain-invariant cues and thus alleviates the heavy reliance on domain-specific cues. We conduct extensive quantitative and qualitative experiments on three benchmarks (including two newly designed benchmarks), and the results demonstrate the effectiveness and versatility of our approach.

**Acknowledgements.** This work was supported partially by the NSFC (U21A20471,U1911401), Guangdong NSF Project (No. 2023B1515040025, 2020B1515120085). The authors would like to thank Zhilin Zhao, Yi-Xing Peng, and Yu-Ming Tang for their valuable suggestions on model design or writing.

## References

* [1] Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3D convolutional networks. In _IEEE International Conference on Computer Vision_, 2015.
* [2] Joao Carreira and Andrew Zisserman. Quo Vadis, Action Recognition? A new model and the kinetics dataset. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2017.
* [3] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2018.
* [4] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2018.
* [5] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal Segment Networks: Towards good practices for deep action recognition. In _European Conference on Computer Vision_, 2016.
* [6] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In _European Conference on Computer Vision_, 2018.

Figure 6: Grad-CAM visualization on UCF-HMDB. As shown in the figure, compared with the baseline, our proposed STDN captures more diverse class-correlated cues in the source domain, _i.e._, including domain-specific backboards and domain-invariant basketballs. As a result, our proposed STDN generalizes better in the target domain, where backboards are invisible and thus our STDN uses the basketball for recognition instead. Best viewed in color.

* [7] Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In _IEEE/CVF International Conference on Computer Vision_, 2019.
* [8] Haoxin Li, Wei-Shi Zheng, Yu Tao, Haifeng Hu, and Jian-Huang Lai. Adaptive interaction modeling via graph operations search. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* [9] Yu Kong and Yun Fu. Human action recognition and prediction: A survey. _International Journal of Computer Vision_, 130(5):1366-1401, 2022.
* [10] Zehua Sun, Quihong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang, and Jun Liu. Human action recognition from various data modalities: A review. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(3):3200-3225, 2023.
* [11] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip H.S. Torr, and Song Bai. MOSE: A new dataset for video object segmentation in complex scenes. In _IEEE/CVF International Conference on Computer Vision_, 2023.
* [12] Riccardo Volpi, Pau de Jorge, Diane Larlus, and Gabriela Csurka. On the road to online adaptation for semantic image segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [13] Attila Lengyel, Sourav Garg, Michael Milford, and Jan C. van Gemert. Zero-shot day-night domain adaptation with a physics prior. In _IEEE/CVF International Conference on Computer Vision_, 2021.
* [14] Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin, and Simon See. ARID: A comprehensive study on recognizing actions in the dark and a new benchmark dataset. _CoRR_, abs/2006.03876, 2020.
* [15] Zhiyu Yao, Yunbo Wang, Jianmin Wang, Philip S. Yu, and Mingsheng Long. VideoDG: Generalizing temporal relations in videos to novel domains. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):7989-8004, 2022.
* [16] Mirco Planamente, Chiara Pizzari, Emanuele Alberti, and Barbara Caputo. Domain generalization through audio-visual relative norm alignment in first person action recognition. In _IEEE/CVF Winter Conference on Applications of Computer Vision_, 2022.
* [17] Arshad Jamal, Vinay P. Namboodiri, Dipti Deodhare, and K. S. Venkatesh. Deep domain adaptation in action space. In _British Machine Vision Conference_, 2018.
* [18] Min-Hung Chen, Zsolt Kira, Ghassan Alregib, Jaekwon Yoo, Ruxin Chen, and Jian Zheng. Temporal attentive alignment for large-scale video domain adaptation. In _IEEE/CVF International Conference on Computer Vision_, 2019.
* [19] Boxiao Pan, Zhangjie Cao, Ehsan Adeli, and Juan Carlos Niebles. Adversarial cross-domain action recognition with co-attention. In _AAAI Conference on Artificial Intelligence_, 2020.
* [20] Jinwoo Choi, Gaurav Sharma, Samuel Schulter, and Jia-Bin Huang. Shuffle and attend: Video domain adaptation. In _European Conference on Computer Vision_, 2020.
* [21] Yingwei Li, Yi Li, and Nuno Vasconcelos. RESOUND: Towards action recognition without representation bias. In _European Conference on Computer Vision_, 2018.
* [22] Jinwoo Choi, Chen Gao, Joseph C. E. Messou, and Jia-Bin Huang. Why Can't I Dance in the Mall? Learning to mitigate scene bias in action recognition. In _Advances in Neural Information Processing Systems_, 2019.
* [23] Jinpeng Wang, Yuting Gao, Ke Li, Yiqi Lin, Andy J. Ma, Hao Cheng, Pai Peng, Feiyue Huang, Rongrong Ji, and Xing Sun. Removing the background by adding the background: Towards background robust self-supervised video representation learning. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2021.
* [24] Haoxin Li, Yuan Liu, Hanwang Zhang, and Boyang Li. Mitigating and evaluating static bias of action representations in the background and the foreground. In _IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* [25] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _International Conference on Learning Representations_, 2021.

* [26] Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. Video classification with channel-separated convolutional networks. In _IEEE/CVF International Conference on Computer Vision_, 2019.
* [27] Christoph Feichtenhofer. X3D: Expanding architectures for efficient video recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* [28] Shiwen Zhang, Sheng Guo, Weilin Huang, Matthew R. Scott, and Limin Wang. V4D: 4D convolutional neural networks for video-level representation learning. In _International Conference on Learning Representations_, 2020.
* [29] Kunchang Li, Xianhang Li, Yali Wang, Jun Wang, and Yu Qiao. CT-Net: Channel tensorization network for video classification. In _International Conference on Learning Representations_, 2021.
* [30] Junyan Wang, Zhenhong Sun, Yichen Qian, Dong Gong, Xiuyu Sun, Ming Lin, Maurice Pagnucco, and Yang Song. Maximizing spatio-temporal entropy of deep 3d cnns for efficient video recognition. _CoRR_, abs/2303.02693, 2023.
* [31] Noureldien Hussein, Efstratios Gavves, and Arnold W. M. Smeulders. Timeception for complex action recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, pages 254-263, 2019.
* [32] Jiaming Zhou, Kun-Yu Lin, Haoxin Li, and Wei-Shi Zheng. Graph-based high-order relation modeling for long-term action recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2021.
* [33] Jiaming Zhou, Kun-Yu Lin, Yu-Kun Qiu, and Wei-Shi Zheng. Twinformer: Fine-to-coarse temporal modeling for long-term action recognition. _IEEE Transactions on Multimedia_, 2023.
* [34] Hao Shao, Shengju Qian, and Yu Liu. Temporal interlacing network. In _AAAI Conference on Artificial Intelligence_, 2020.
* [35] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift networks for video action recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, 2017.
* [37] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer network. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2019.
* [38] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. _CoRR_, abs/2102.00719, 2021.
* [39] Hao Zhang, Yanbin Hao, and Chong-Wah Ngo. Token shift transformer for video classification. In _ACM International Conference on Multimedia_, 2021.
* [40] Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe. VidTr: Video transformer without convolutions. In _IEEE/CVF International Conference on Computer Vision_, 2021.
* [41] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In _International Conference on Machine Learning_, 2021.
* [42] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. ViViT: A video vision transformer. In _IEEE/CVF International Conference on Computer Vision_, 2021.
* [43] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. VLT: Vision-language transformer and query generation for referring segmentation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(6):7900-7916, 2023.
* [44] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [45] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. AIM: Adapting image models for efficient video action recognition. In _International Conference on Learning Representations_, 2023.
* [46] Yuecong Xu, Haozhi Cao, Kezhi Mao, Zhenghua Chen, Lihua Xie, and Jianfei Yang. Aligning correlation information for domain adaptation in action recognition. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.

* [47] Yadan Luo, Zi Huang, Zijian Wang, Zheng Zhang, and Mahsa Baktashmotlagh. Adversarial bipartite graph learning for video domain adaptation. In _ACM International Conference on Multimedia_, 2020.
* [48] Aadarsh Sahoo, Rutav Shah, Rameswar Panda, Kate Saenko, and Abir Das. Contrast and mix: Temporal contrastive video domain adaptation with background mixing. In _Advances in Neural Information Processing Systems_, 2021.
* [49] Jonathan Munro and Dima Damen. Multi-modal domain adaptation for fine-grained action recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* [50] Xiaolin Song, Sicheng Zhao, Jingyu Yang, Huanjing Yue, Pengfei Xu, Runbo Hu, and Hua Chai. Spatio-temporal contrastive domain adaptation for action recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2021.
* [51] Donghyun Kim, Yi-Hsuan Tsai, Bingbing Zhuang, Xiang Yu, Stan Sclaroff, Kate Saenko, and Manmohan Chandraker. Learning cross-modal contrastive features for video domain adaptation. In _IEEE/CVF International Conference on Computer Vision_, 2021.
* [52] Lijin Yang, Yifei Huang, Yusuke Sugano, and Yoichi Sato. Interact before align: Leveraging cross-modal knowledge for domain adaptive action recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [53] Yunhua Zhang, Hazel Doughty, Ling Shao, and Cees G. M. Snoek. Audio-adaptive activity recognition across video domains. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [54] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dvijotham, and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In _International Conference on Learning Representations_, 2022.
* [55] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4396-4415, 2023.
* [56] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. _CoRR_, abs/2103.03097, 2021.
* [57] Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation. In _European Conference on Computer Vision Workshops_, 2016.
* [58] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In _International Conference on Machine Learning_, 2015.
* [59] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In _International Conference on Machine Learning_, 2021.
* [60] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. _Journal of Machine Learning Research_, 17:59:1-59:35, 2016.
* [61] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In _European Conference on Computer Vision_, 2018.
* [62] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _CoRR_, abs/1907.02893, 2019.
* [63] David Krueger, Ethan Caballero, Jorn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron C. Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, 2021.
* [64] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S. Jaakkola. Invariant rationalization. In _International Conference on Machine Learning_, 2020.
* [65] Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang. Sparse invariant risk minimization. In _International Conference on Machine Learning_, 2022.
* [66] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In _AAAI Conference on Artificial Intelligence_, 2018.
* [67] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using meta-regularization. In _Advances in Neural Information Processing Systems_, 2018.

* Li et al. [2019] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M. Hospedales. Feature-critic networks for heterogeneous domain generalization. In _International Conference on Machine Learning_, 2019.
* Dou et al. [2019] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In _Advances in Neural Information Processing Systems_, 2019.
* Chen et al. [2023] Jin Chen, Zhi Gao, Xinxiao Wu, and Jiebo Luo. Meta-causal learning for single domain generalization. _CoRR_, abs/2304.03709, 2023.
* Volpi et al. [2018] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In _Advances in Neural Information Processing Systems_, 2018.
* Zhang et al. [2018] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _International Conference on Learning Representations_, 2018.
* Yue et al. [2019] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto L. Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. In _IEEE/CVF International Conference on Computer Vision_, 2019.
* Zhou et al. [2021] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In _International Conference on Learning Representations_, 2021.
* Lee et al. [2023] Sangrok Lee, Jongseong Bae, and Ha Young Kim. Decompose, adjust, compose: Effective normalization by playing with frequency for domain generalization. _CoRR_, abs/2303.02328, 2023.
* Izmailov et al. [2018] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In _Conference on Uncertainty in Artificial Intelligence_, 2018.
* Cha et al. [2021] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. SWAD: Domain generalization by seeking flat minima. In _Advances in Neural Information Processing Systems_, 2021.
* Chu et al. [2022] Xu Chu, Yujie Jin, Wenwu Zhu, Yasha Wang, Xin Wang, Shanghang Zhang, and Hong Mei. DNA: Domain generalization with diversified neural averaging. In _International Conference on Machine Learning_, 2022.
* Allen-Zhu and Li [2023] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In _International Conference on Learning Representations_, 2023.
* Meng et al. [2022] Rang Meng, Xianfeng Li, Weijie Chen, Shicai Yang, Jie Song, Xinchao Wang, Lei Zhang, Mingli Song, Di Xie, and Shiliang Pu. Attention diversification for domain generalization. In _European Conference on Computer Vision_, 2022.
* Choi et al. [2023] Seokeon Choi, Debasmit Das, Sungha Choi, Seunghan Yang, Hyunsin Park, and Sungrack Yun. Progressive random convolutions for single domain generalization. _CoRR_, abs/2304.00424, 2023.
* Hendrycks and Gimpel [2017] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In _International Conference on Learning Representations_, 2017.
* Liang et al. [2018] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In _International Conference on Learning Representations_, 2018.
* Lee et al. [2018] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In _International Conference on Learning Representations_, 2018.
* Yang et al. [2022] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Dan Hendrycks, Yixuan Li, and Ziwei Liu. Openood: Benchmarking generalized out-of-distribution detection. In _Advances in Neural Information Processing Systems_, 2022.
* Zhao et al. [2023] Zhilin Zhao, Longbing Cao, and Kun-Yu Lin. Revealing the distributional vulnerability of discriminators by implicit generators. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(7):8888-8901, 2023.
* Zhao et al. [2023] Zhilin Zhao, Longbing Cao, and Kun-Yu Lin. Supervision adaptation balancing in-distribution generalization and out-of-distribution detection. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-16, 2023.

* [88] Zhilin Zhao and Longbing Cao. Dual representation learning for out-of-distribution detection. _Transactions on Machine Learning Research_, 2023.
* [89] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2016.
* [90] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In _European Conference on Computer Vision_, 2018.
* [91] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2018.
* [92] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* [93] Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2019.
* [94] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. _CoRR_, abs/1212.0402, 2012.
* [95] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomasao A. Poggio, and Thomas Serre. HMDB: A large video database for human motion recognition. In _IEEE International Conference on Computer Vision_, 2011.
* [96] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The EPIC-KITCHENS dataset. In _European Conference on Computer Vision_, 2018.
* [97] Joanna Materzynska, Guillaume Berger, Ingo Bax, and Roland Memisevic. The jester dataset: A large-scale video dataset of human gestures. In _IEEE/CVF International Conference on Computer Vision Workshops_, 2019.
* [98] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, 2009.
* [99] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems_, 2019.
* [100] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2019.
* [101] Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to Adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. In _International Conference on Learning Representations_, 2023.
* [102] Ziyi Zhang, Weikai Chen, Hui Cheng, Zhen Li, Siyuan Li, Liang Lin, and Guanbin Li. Divide and Contrast: Source-free domain adaptation via adaptive contrastive learning. In _Advances in Neural Information Processing Systems_, 2022.
* [103] Yipeng Gao, Kun-Yu Lin, Junkai Yan, Yaowei Wang, and Wei-Shi Zheng. AsyFOD: An asymmetric adaptation paradigm for few-shot domain adaptive object detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_. IEEE, 2023.
* [104] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. _Journal of Machine Learning Research_, 9(11), 2008.
* [105] David L. Davies and Donald W. Bouldin. A cluster separation measure. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 1(2):224-227, 1979.
* [106] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-based localization. _International Journal of Computer Vision_, 128(2):336-359, 2020.