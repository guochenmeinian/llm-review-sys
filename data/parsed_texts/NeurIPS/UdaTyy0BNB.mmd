# Double Gumbel Q-Learning

David Yu-Tung Hui

Mila, Universite de Montreal

dythui2+drl@gmail.com

Aaron Courville

Mila, Universite de Montreal

Pierre-Luc Bacon

Mila, Universite de Montreal

###### Abstract

We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning. To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control. In discrete control, we derive a closed-form expression for the loss function of our algorithm. In continuous control, this loss function is intractable and we therefore derive an approximation with a hyperparameter whose value regulates pessimism in Q-Learning. We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning this hyperparameter may further improve sample efficiency.

## 1 Introduction

Reinforcement Learning (RL) algorithms learn optimally rewarding behaviors (Sutton and Barto, 1998, 2018). Recent RL algorithms have attained superhuman performance in Atari (Mnih et al., 2015), Go (Silver et al., 2016), Chess (Schrittwieser et al., 2020), StarCraft (Vinyals et al., 2019), simulation racing (Wurman et al., 2022), and have also been used to control stratospheric balloons (Bellemare et al., 2020) and magnetic fields in Tokamak nuclear fusion reactors (Degrave et al., 2022). All these aforementioned agents use a \(Q\)-function (Watkins and Dayan, 1992) to measure the respective quality of their behaviors.

Many algorithms (Lillicrap et al., 2015; Mnih et al., 2016, 2015; Schulman et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018) learn a \(Q\)-function by minimizing the Mean-Squared Bellman Error (MSBE). The functional form of the MSBE was motivated by an analogy to the popular Mean-Squared Error (MSE) in supervised learning regression problems, but there is little theoretical justification for its use in RL (Bradtke and Barto, 1996; Ernst et al., 2005; Riedmiller, 2005).

Training a deep neural network to minimize the MSBE is empirically unstable (Irban, 2018; Henderson et al., 2018; Van Hasselt et al., 2016) because deep neural networks induce overestimation bias in the \(Q\)-function (Thrun and Schwartz, 1993; Fujimoto et al., 2018). A popular method to reduce overestimation 'pessimistically' selects less positive \(Q\)-values from an ensemble of \(Q\)-functions, effectively returning low-quantile estimates (Fujimoto et al., 2018; Kuznetsov et al., 2020; Ball et al., 2023). However, an ensemble is computationally expensive and quantiles only provide discrete-grained control over the degree of pessimism. Curiously, pessimism has mainly been applied to 'continuous control' problems that require continuous-valued actions and not discrete control.

This paper analyzes noise in Deep \(Q\)-Learning. We introduce a loss function for Deep \(Q\)-Learning and a scalar hyperparameter that adjusts pessimism without an ensemble. These two innovations form **Double Gumbel \(Q\)-Learning (DoubleGum)**, 1 an off-policy Deep \(Q\)-Learning algorithm applicable to both discrete and continuous control. Our paper is structured as follows:

Footnote 1: Code: https://github.com/dyth/doublegum

1. Section 2.1 shows that the MSBE implicitly uses a noise model of the Bellman Optimality Equation with additive homoscedastic noise. Section 4.1 shows that this noise model is empirically too coarse to fully capture the distribution of noise in Deep \(Q\)-Learning.
2. Section 3 derives a noise model of the Soft Bellman Equation with additive heteroscedastic logistic noise derived from two heteroscedastic Gumbel noise sources. Section 4.1 shows that our noise model is a better empirical fit to the noise distribution in Deep \(Q\)-Learning.
3. Section 3.3 shows that when the soft value function in the Soft Bellman Equation is intractable in continuous control, its approximation leads to methods that adjust pessimism. Section 3.1 presents a method to adjust pessimism with a single scalar hyperparameter, and its effect is empirically verified in Section 4.2.
4. Section 5 discusses related work, notably presenting DoubleGum as a special case of Distributional RL and a generalization of existing Maximum Entropy (MaxEnt) RL algorithms.
5. Section 6 benchmarks DoubleGum, showing marginal improvements over DQN baselines on 3 classic discrete control tasks and improved aggregate performance over DDPG, SAC, TD3, XQL and two Distributional RL algorithms over 33 continuous control tasks.

## 2 Mathematical Background

Reinforcement Learning agents learn to make maximally rewarding decisions in an environment (Sutton and Barto, 1998, 2018). In Markov Decision Process (MDP) environments, a \(Q\)-function specifies the expected amount of reward an agent obtains (Watkins and Dayan, 1992) and is also known as a state-action value function. MDPs define a transition probability \(p(s^{\prime}\mid s,a)\) and reward \(r(s,a,s^{\prime})\) for every pair of states \(s,s^{\prime}\in\mathcal{S}\) and action \(a\in\mathcal{A}\)(Bellman, 1957; Howard, 1960). Decisions are made by a policy \(\pi(a\mid s)\) at every discrete timestep and the \(Q\)-function of \(\pi\) at \(s\), \(a\) is

\[Q_{\pi}(s,a)=\operatorname*{\mathbb{E}}_{\pi,p}\left[\sum_{j=i}^{n}\gamma^{j}r _{j}\middle|s_{i}=s,a_{i}=s\right],\ \ \text{denoting}\ r(s_{j},a_{j},s_{j+1})\ \text{as}\ r_{j},\]

where the discount factor \(\gamma\in]0,1[\) ensures finite \(Q\) for infinite time-horizon \(n\)(Samuelson, 1937). The expectation in the \(Q\)-function is computed over a trajectory sequence of \(s\) and \(a\) induced by the coupling of the policy and the transition function.

In every MDP, an optimal policy \(\pi^{\star}\) makes maximally rewarding decisions following \(s\to\arg\max_{a}Q^{\star}(s,a)\)(Watkins and Dayan, 1992). \(Q^{\star}\) is the optimal value function specified as the unique fixed point of the Bellman Optimality Equation of the MDP (Bellman, 1957)

\[Q^{\star}(s,a)=\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r+\gamma \max_{a^{\prime}}Q^{\star}(s^{\prime},a^{\prime})\right],\ \ \text{ over all $s$ and $a$, denoting $r(s,a,s^{\prime})$ as $r$.}\] (1)

In Deep \(Q\)-Learning, \(Q^{\star}\) is approximated by a neural network \(Q_{\theta}\) with parameters \(\theta\). Following Munos and Szepesvari (2008), \(\theta\) is updated by a sequence of optimization problems whose \(i^{\text{th}}\) stage iteration is given by

\[\theta_{i+1}=\arg\min_{\theta}\operatorname{L}(Q_{\theta},y_{\bar{\theta}_{i}} ),\ \ \text{where}\ y_{i}(s,a)=\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r+ \gamma\max_{a^{\prime}}Q_{\bar{\theta}_{i}}(s^{\prime},a^{\prime})\right]\ \,\]

where the'stop-gradient' operator \(\bar{(\cdot)}\) denotes evaluating but not optimizing an objective with respect to its argument and \(y\) is dubbed the bootstrapped target. In this work, we restrict our environments to deterministic MDPs, so the resultant expectation over \(s^{\prime}\) is computed over one sample.

Typically, the loss function \(\operatorname{L}(Q_{\theta},y_{\bar{\theta}_{i}})=\operatorname*{\mathbb{E}}_ {s,a}\left(Q_{\theta}(s,a)-y_{\bar{\theta}_{i}}(s,a)\right)^{2}\) is the Mean-Squared Bellman Error (MSBE). At every \(i\), \(\operatorname{L}\) is minimized with stochastic gradient descent by a fixed number of gradient descent steps and not optimized to convergence (Minh et al., 2015; Lillicrap et al., 2015).

### Deriving the Mean-Squared Bellman Error

Thrun and Schwartz (1993) model \(Q_{\theta}\)(s, a) as a random variable whose value varies with \(\theta,s\) and \(a\). They set up a generative process \(Q_{\theta}(s,a)=Q^{\star}(s,a)-\epsilon_{\theta,s,a}\) showing that the function approximator \(Q_{\theta}\) is a noisy function of the implicit true value \(Q^{\star}\), but they do not make any assumptions about \(\epsilon\). 2

Footnote 2: This equation derives from an unnumbered equation at the top of Page 3 of Thrun and Schwartz (1993), and we have renamed their variables to fit our notation. The authors add the noise \(\epsilon\) to \(Q^{\star}\), but we subtract it for notational clarity later on without loss of generality. We additionally subscript the noise by \(\theta\) to clarify that the random variable is resampled for different \(\theta\)s in the function approximator.

We derive the MSBE with two additional assumptions: \(\epsilon_{\theta,s,a}\stackrel{{\text{iid}}}{{\sim}}\mathcal{N }(0,\sigma^{2})\), where \(\mathcal{N}\) is a homoscedastic normal due to the Central Limit Theorem (CLT) 3 and secondly \(Q^{\star}(s,a)\approx y_{\widetilde{\theta}}(s,a)\), where \(\theta\) are the parameters at an arbitrary optimization stage. Incorporating these two assumptions into the noise model yields \(y_{\widetilde{\theta}}(s,a)=Q_{\theta}(s,a)+\epsilon_{\theta,s,a}\), uncovering the implicit assumption that Temporal-Difference (TD) errors \(y_{\widetilde{\theta}}-Q_{\theta}\) follow a homoscedastic normal. Maximum Likelihood Estimation (MLE) of \(\theta\) on the resultant noise model yields the MSBE after abstracting away the constants

Footnote 3: Ernst et al. (2005) and Riedm√ºller (2005) introduced the MSBE to \(Q\)-Learning, arguing that MSE was popular for regression problems in supervised learning. In regression, the MSE arises from MLE parameter estimation of a homoscedastic normal, which is assumed to exist because of the CLT.

\[\max_{\theta}\operatorname*{\mathbb{E}}_{s,a}\log p(y_{\widetilde{\theta}}(s,a ))=\min_{\theta}\operatorname*{\mathbb{E}}_{s,a}\left[\log\sigma\sqrt{2\pi}+ \frac{1}{\sigma^{2}}\left(Q_{\theta}(s,a)-y_{\widetilde{\theta}}(s,a)\right)^ {2}\right]\enspace.\]

### The Limiting Distribution in Bootstrapped Targets

Both assumptions used to derive the MSBE are theoretically weak. First, the CLT states that iid samples of an underlying distribution tend towards a homoscedastic normal but do not account for the form of the underlying distribution. Secondly, there is no analytic justification for \(Q^{\star}(s,a)\approx y_{\widetilde{\theta}}(s,a)\). Substituting the Thrun and Schwartz (1993) model in the RHS of the Bellman Optimality Equation yields

\[Q^{\star}(s,a)=\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r+\gamma \max_{a^{\prime}}[Q_{\theta}(s^{\prime},a^{\prime})+\epsilon_{\theta,s^{\prime },a^{\prime}}]\right]=\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r+ \gamma g_{\theta,s^{\prime}}\right]\neq y_{\widetilde{\theta}}(s,a)\enspace.\]

When the \(\max\) over a finite number of iid samples with unbounded support is taken, the Extreme Value Theorem (EVT) gives the limiting distribution of the resultant random variable as a Gumbel distribution (Fisher and Tippett, 1928; Gnedenko, 1943), defined here as \(g_{\theta,s^{\prime}}\sim\mathcal{G}(\alpha,\beta)\). Expressions relating \(\alpha\) and \(\beta\) to the parameters of the underlying iid random variables are rarely analytically expressible (Kimball, 1946; Jowitt, 1979) and there is no guarantee \(Q^{\star}(s,a)\approx y_{\widetilde{\theta}}(s,a)\).

## 3 Deriving a Deep Q-Learning Algorithm from First Principles

We find an analytic expression that relates the limiting Gumbel distribution to parameters of the underlying noise distribution from function approximation. To do so, we assume that noise in the Thrun and Schwartz (1993) model is a heteroscedastic Gumbel noise with state-dependent spread

\[Q_{\theta}(s,a)=Q^{\star}(s,a)-g_{\theta,a}(s),\quad g_{\theta,a}(\cdot) \stackrel{{\text{iid}}}{{\sim}}\mathcal{G}(0,\beta_{\theta}( \cdot)),\enspace\text{for all }a\in\mathcal{A}\enspace,\] (2)

Appendix A.1 restates the finding of McFadden et al. (1973) and Rust (1994), which results in the following expression with the soft value function \(V_{\theta}^{\text{soft}}\)

\[\max_{a}Q^{\star}(s,a) =\max_{a}\left[Q_{\theta}(s,a)+g_{\theta,a}(s)\right]=\beta_{ \theta}(s)\log\int\exp\left(\frac{Q_{\theta}(s,a)}{\beta_{\theta}(s)}\right) \,\mathrm{d}a+g_{\theta}(s)\] \[=V_{\theta}^{\text{soft}}(s)+g_{\theta}(s),\enspace\text{where }g_{\theta,a}(\cdot),g_{\theta}(\cdot)\stackrel{{\text{iid}}}{{ \sim}}\mathcal{G}(0,\beta_{\theta}(\cdot))\enspace.\] (3)

Substituting Equations 2 and 3 into 1 yields a new noise model of the Soft Bellman Optimality Equation with two function approximators and two heteroscedastic Gumbel noise sources

\[Q_{\theta}(s,a)+g_{\theta,a}(s)=\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a) }\left[r+\gamma V_{\widetilde{\theta}}^{\text{soft}}(s^{\prime})+\gamma g_{ \widetilde{\theta}}(s^{\prime})\right]\enspace.\] (4)

We now develop this noise model into a loss function used in our algorithm, DoubleGum.

[MISSING_PAGE_EMPTY:4]

is commonly used in continuous control (Lillicrap et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018), and has been recently been shown to improve discrete control (D'Oro et al., 2022).

**Q-Functions with Variance Networks**: We learn \(\sigma\) with variance networks (Nix and Weigend, 1994; Kendall and Gal, 2017). In discrete control, \(\sigma_{\theta}(s)\) is a variance head added to the \(Q\)-network, and is approximated as \(\sigma_{\theta}(s,a)\) in continuous control. The variance head is a single linear layer whose input is the penultimate layer of the main \(Q\)-network and output is a single unit with a SoftPlus activation function (Dugas et al., 2000) that ensures its positivity. Seitzer et al. (2022) improves stability during training by multiplying the entire loss by \(\sigma_{\bar{\theta}}\), the numerical value of the variance network, yielding a loss function for \(\theta\) which for continuous control is

\[J_{\theta,y_{\phi},\sigma_{\theta}}(s,a,r,s^{\prime})=\left[\sigma_{\bar{ \theta}}(s,a)\left(\log\sigma_{\theta}(s,a)+\left(\frac{y_{\psi}(s,a,r,s^{ \prime})-Q_{\theta}^{\text{new}}(s,a)}{\sigma_{\theta}(s,a)}\right)^{2}\right) \right]\enspace.\] (9)

Note that in the above equation \(\sigma_{\bar{\theta}}(s,a)\) would be \(\sigma_{\bar{\theta}}(s)\) in discrete control. We do not have any convergence guarantees for this loss function, and Appendix C discusses this issue in more detail.

**Policy**: In discrete control, actions are taken by \(\max_{a}Q_{\theta}^{\text{new}}(s,a)\) as standard. Exploration was performed by policy churn (Schaul et al., 2022). In continuous control, the policy \(\pi_{\phi}\) is a separate network. We use a DDPG fixed-variance actor because Yarats et al. (2021) showed it trained more stably than a SAC actor with learned variance. Following Laskin et al. (2021), the actor's outputs were injected with zero-mean Normal noise with a standard deviation of 0.2 truncated to 0.3.

**Network Architecture**: All networks had two hidden layers of size 256, ReLU activations (Glorot et al., 2011), orthogonal initialization (Saxe et al., 2013) with a gain of \(\sqrt{2}\) for all layers apart from the last layer of the policy and variance head, which had gains of \(1\). This initialization had been shown to be empirically advantageous in policy-gradient and \(Q\)-Learning methods in Huang et al. (2022) and Kostrikov (2021), respectively. Ball et al. (2023) improves stability in continuous control using LayerNorm (Ba et al., 2016). We find that the similar method of GroupNorm (Wu and He, 2018) with 16 groups without a shift and scale (Xu et al., 2019) worked better, but only in continuous control. All parameters were optimized by Adam (Kingma and Ba, 2014) with default hyperparameters.

## 4 Empirical Evidence for Theoretical Assumptions

### Noise Distributions in Deep Q-Learning

Figure 1 shows the evolution of noise during training DoubleGum in the classic control task of CartPole-v1. Histograms in Figures 1(a) and 1(b) were generated from \(10^{4}\) samples from the replay buffer after 1 million training timesteps. Figure 1(a) computes TD-errors by \(y_{\psi}(s,a,r,s^{\prime})-Q_{\theta}^{\text{new}}(s,a)\), while Figure 1(b) computes standardized TD-errors by dividing the previous equation by \(\sigma_{\theta}(s)\) or \(\sigma_{\theta}(s,a)\) where appropriate. The normal, logistic, and Gumbel distributions were fitted by moment matching, and homo(scedastic) and hetero(sedastic) distributions were respectively fitted to unstandardized and standardized data. Figure 1(c) shows the mean and standard deviation over 12 Negative Log-Likelihoods (NLLs), each computed from a different training run with a different (randomly chosen) initial seed. Every NLL was computed over \(10^{4}\) samples from the replay buffer every 1000 timesteps of training.

Figure 1(a) shows that a homoscedastic normal coarsely fits the empirical distribution, forming a good estimate of the mean but not the spread. Our result contradicts Garg et al. (2023) that fitted a Gumbel to the empirical distribution. We discuss this discrepancy in Appendix E.1. Figure 1(b) shows that a heteroscedastic Logistic captures both the mean and spread of the TD-errors, validating Equations 5 and its derivation. Finally, Figure 1(c) shows that a moment-matched heteroscedastic normal used in DoubleGum is a suitable approximation to the heteroscedastic logistic throughout training, validating Equation 6. Appendix D.1 shows that the trend in Figure 1(c) holds for other discrete control environments as well as continuous control.

### Adjusting The Pessim Factor

Figure 2 plots a 12-sample IQM and standard deviation over \(\frac{1}{256}\sum_{i=1}^{256}Q_{\psi}^{\text{new}}(s_{i},a_{i})\), the average magnitude of the target \(Q\)-value used in bootstrapped targets. Figure 2 and Appendix D.2 show that the average magnitude increases as the pessimism factor \(c\) increases, validating its effectiveness.

Figure 1: The empirical distribution of noise in discrete control CartPole-v1. (a, b): Fitted normal, logistic, and Gumbel distributions against (a) unstandardized and (b) standardized empirical distributions in Deep \(Q\)-Learning at the end of training. (c): Negative Log-Likelihoods (NLLs) of the noise in Deep \(Q\)-Learning under different distributions throughout training (lower is better). Appendix D.1 presents further results in more discrete and continuous control tasks.

Figure 2: The effect of changing pessimism factor \(c\) on the target \(Q\)-value in continuous control. Appendix D.2 presents further results in more tasks.

Related Work

### Theoretical Analyses of the Noise in Q-Learning

**Logistic Q-Learning**Bas-Serrano et al. (2021) presents a similar noise model to DoubleGum but with a homoscedastic logistic instead of a heteroscedastic logistic in Equation 5. Their distribution is derived from the linear-programming perspective of value-function learning described in Section 6.9 of Puterman (2014)) and Peters et al. (2010). While DoubleGum learns a \(Q\)-function off-policy, Logistic \(Q\)-Learning uses on-policy rollouts to compute the \(Q\)-values of the current policy. We do not benchmark against Logistic \(Q\)-Learning because their method is on-policy and only uses linear function approximation.

**Extreme Q-Learning (XQL)**Garg et al. (2023) presents a noise model for Deep \(Q\)-Learning with one homoscedastic Gumbel noise source, as opposed to the two heteroscedastic Gumbels in Equation 4. Parameters are learned by the LINear-EXponential (LINEX) loss Varian (1975) formed from the log-likelihood of a Gumbel distribution. XQL is presented in more detail in Appendix B.4.

**Gumbel Noise in Deep Q-Learning**: Thrun and Schwartz (1993) argued that the \(\max\)-operator in bootstrapped targets transformed zero-mean noise from function approximation into statistically biased noise. The authors did not recognize that the resultant noise distribution was Gumbel, and this was realized by Lee and Powell (2012). Unlike DoubleGum, these two works did not make assumptions about the distribution of function approximator noise but instead focused on mitigating the overestimation bias of bootstrapped targets. In economics, McFadden et al. (1973) and Rust (1994) assume the presence of Gumbel noise from noisy reward observations (unlike DoubleGum, which assumes that noise comes from function approximation) and derives the soft value function we present in Appendix A.1 for static and dynamic discrete choice models. XQL brings the Rust-McFadden et al. model to deep reinforcement learning to tackle continuous control.

The **soft value function** was introduced to model stochastic policies (Rummery and Niranjan, 1994; Fox et al., 2015; Haarnoja et al., 2017, 2018b). The most prominent algorithm that uses the soft value function is **Soft Actor-Critic (SAC)**(Haarnoja et al., 2018a,b), which Appendix B.2 shows is a special case of DoubleGum when \(Q_{\theta}^{\text{new}}\) is learned by homoscedastic instead of heteroscedastic normal regression and the spread is a tuned scalar parameter instead of a learned state-dependent standard deviation. Appendix B.2 also shows that **Deep Deterministic Policy Gradients (DDPG)**(Lillicrap et al., 2015) is a simpler special case of SAC that has recently been shown to outperform and train more stably than SAC (Yarats et al., 2021).

### Empirically Similar Methods to DoubleGum

**Distributional RL** models the bootstrapped targets and \(Q\)-function as distributions. In these methods, a \(Q\)-function is learned by minimizing the divergence between itself and a target distribution. The most similar distributional RL method to DoubleGum is **Mixture-of-Gaussian (MoG) Critics**, introduced in Appendix A of Barth-Maron et al. (2018). DoubleGum is a special case of MoG-Critics with only one Gaussian (such as in Morimura et al. (2012)) and mean samples of the target distribution. Curiously, Shahriari et al. (2022) shows that sample-efficiency of training improves when bootstrapped target sampled are increasingly near the mean but did not try mean sampling. Nevertheless, Shahriari et al. (2022) show that MoG-Critics outperforms a baseline with the C51 distributional head (Bellemare et al., 2017) popular in discrete control, obtaining state-of-the-art results in DeepMind Control. In discrete control with distributional RL, C51 has been superseded by **Quantile Regression (QR)** methods (Dabney et al., 2018,a; Yang et al., 2019) that predict quantile estimates of a distribution. Ma et al. (2020); Wurman et al. (2022) and Teng et al. (2022) apply QR to continuous control.

**Adjusting pessimism** greatly improves the sample efficiency of RL. Fujimoto et al. (2018) showed empirical evidence of overestimation bias in continuous control and mitigated it with pessimistic estimates computed by **Twin Networks**. However, Twin Networks may sometimes harm sample efficiency because the optimal degree of pessimism varies across environments. To address this, the degree of pessimism is adjusted during training (Lan et al., 2020; Wang et al., 2021; Karimpanal et al., 2021; Moskovitz et al., 2021; Kuznetsov et al., 2020, 2022; Ball et al., 2023), often with the help of an ensemble. In contrast, DoubleGum uses one \(Q\)-network and one scalar hyperparameter fixed throughout training.

Many other RL methods **add or subtract the learned standard deviation to bootstrapped targets**. Risk-Aware RL subtracts a learned standard deviation to learn a low-variance policy (La and Ghavamzadeh, 2013; Tamar and Mannor, 2013; Tamar et al., 2016). Upper-Confidence Bounded (UCB) methods add a learned standard deviation to explore high-variance regions (Lee et al., 2021; Teng et al., 2022). These methods use a combination of ensembles and variance networks, but it is also possible to derive a Bellman equation for the variance following Dearden et al. (1998). The current state-of-the-art method in RL with variance estimation is Mai et al. (2022), which uses both variance networks and ensembles. The use of ensembles is motivated by the need to capture model uncertainty - differences between -functions with different parameters. We believe that ensembles are unnecessary because model uncertainty will be expressed in bootstrapped targets from two different timesteps as parameters change through learning and that all variation in bootstrapped targets will henceforth be captured by variance networks.

## 6 Results

### Discrete Control

We benchmarked DoubleGum on classic discrete control tasks against two baselines from the DQN family of algorithms (Figure 3). All algorithms were implemented following Section 3.4. Appendix E.3 discusses the baseline algorithms in more detail.

Performance was evaluated by the InterQuartile Mean (IQM) over 12 runs, each one with a different randomly initialized seed. Agarwal et al. (2021) showed that the IQM was a robust performance metric in RL. 12 was chosen because it was the smallest multiple of four (so a IQM could be computed) greater than the 10 seeds recommended by Henderson et al. (2018). In each run, the policy was evaluated by taking the mean of 10 rollouts every 1000 timesteps, following Fujimoto et al. (2018).

Figure 3 shows that DoubleGum sometimes obtains better sample efficiency than baselines, but not significantly more to necessitate further discrete control experiments. In the remainder of this work, we focus on continuous control, where we found DoubleGum to be more effective.

### Continuous Control

We present two modes of evaluating DoubleGum because our algorithm has a pessimism factor hyperparameter we choose to tune per suite. We, therefore, benchmark all continuous control algorithms with default pessimism (Figure 4) and without pessimism-tuning per-suite (Figure 5). Table 5 presents default and per-suite pessimism values.

We compare against seven baselines. The first five are popular algorithms in the continuous control literature: DDPG (Lillicrap et al., 2015), TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018), MoG-Critics (Shahriari et al., 2022) and XQL (Garg et al., 2023). Here, DDPG, TD3, and SAC represent MaxEnt RL, MoG-Critics represent Distributional RL, and XQL is the algorithm with the most similar noise model to ours. We introduce two further baselines: QR-DDPG, a stronger Distributional RL baseline that combines quantile regression (Dabney et al., 2018) with DDPG, and FinerTD3, TD3 with an ensemble of five networks as opposed to a network of two ensembles

Figure 3: Discrete control, IQM of returns \(\pm\) standard deviation over 12 seeds.

in the original TD3 that enables finer control over the degree of pessimism with an ensemble of five networks. All algorithms were implemented following design decisions outlined in Section 3.4. Appendix E.4 discusses the algorithms in more detail.

The pessimism of DoubleGum was tuned by changing its pessimism factor. The pessimism of baseline algorithms was tuned by manually choosing whether to use Twin Networks (Fujimoto et al., 2018) or not. Note that we refer to DDPG with Twin Networks as TD3. The pessimism of FinerTD3 was tuned by selecting which quantile estimate to use from an ensemble of five networks. The pessimism of MoG-Critics could not be tuned because its critic does not support paired sampling between ensemble members. Appendices F.1 and F.2 respectively detail how the pessimisms of DoubleGum and baseline algorithms were adjusted.

We benchmarked DoubleGum on 33 tasks over 4 continuous control suites comprising respectively of 11 DeepMind Control (DMC) tasks (Tassa et al., 2018; Tunyasuvunakool et al., 2020), 5 MuJoCo tasks (Todorov et al., 2012; Brockman et al., 2016), 15 MetaWorld tasks (Yu et al., 2020) and 2 Box2D tasks (Brockman et al., 2016). We follow Agarwal et al. (2021) and evaluate performance with the normalized IQM with 95% stratified bootstrap confidence intervals aggregated over all 33 tasks. 12 runs from each task was collected by a similar method to that described in Section 6.1. Further details of the tasks and their aggregate metric are detailed in Appendix E.2.

Figure 4 shows that DoubleGum outperformed all baselines in aggregate over 33 tasks when all algorithms used their default pessimism settings. Figure 5 shows that DoubleGum outperformed all baselines in aggregate over 33 tasks when the pessimism of all algorithms is adjusted per suite. Comparing the figures shows that adjusting the pessimism of DoubleGum per suite also attained a higher aggregate score than DoubleGum with default pessimism.

## 7 Discussion

This paper studied the noise distribution in Deep \(Q\)-Learning from first principles. We derived a noise model for Deep \(Q\)-Learning that used two heteroscedastic Gumbel distributions. Converting our noise model into an algorithm yielded DoubleGum, an off-policy Deep \(Q\)-Learning algorithm applied to both discrete and continuous control.

In discrete control, our algorithm attained competitive performance to the baselines. Despite having an numerically exact loss function in discrete control, DoubleGum was very sensitive to hyperparameters.

Figure 4: Continuous control with default parameters, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs. Methods that default to use Twin Networks are dashed. Appendix F.4 presents per-suite and per-task aggregate results.

Figure 5: Continuous control with the best pessimism hyperparameters tuned per suite, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs. Appendix F.5 presents per-suite and per-task aggregate results.

Practically, using Dueling DQN (Wang et al., 2016) to learn a \(Q\)-function was crucial to getting DoubleGum to work. Appendix D.1 shows that our noise model fits the underlying noise distribution of Deep \(Q\)-Learning and we therefore suspect that instability in discrete control might be due to the training dynamics of deep learning and not our theory.

In continuous control, we introduced a pessimism factor hyperparameter to approximate our otherwise intractable noise model. We provided a default value for the pessimism factor that outperformed popular \(Q\)-Learning baselines in aggregate over 33 tasks. Tuning this hyperparameter yielded even greater empirical gains. Our method of tuning pessimism was more computationally efficient and finer-grained than popular methods that tuned a quantile estimate from an ensemble.

In continuous control, DoubleGum outperformed all baselines in aggregate. We hypothesize that DoubleGum outperformed MaxEnt RL baselines because DoubleGum is a more expressive generalization of SAC, which is itself more expressive than DDPG as shown in Appendix B.2. Our theory showed that TD-errors follow a heteroscedastic Logistic, and we believe that modeling this distribution should be sufficient for distributional RL. We hypothesize that more complex distributions considered by the Distributional RL methods QR and MoG overfit to the replay buffer and might not generalize well to online rollouts. FinerTD3 performs marginally poorer than DoubleGum, even when pessimism was adjusted per-suite. We believe this is because FinerTD3 adjusts pessimism finer than other baselines, but still not as fine as the continuous scalar in DoubleGum. Finally, we hypothesize that DoubleGum outperformed XQL because our noise model better fits the underlying noise distribution in Deep \(Q\)-Learning, as shown in Appendix D.1.

This paper shows that better empirical performance in Deep RL may be attained through a better understanding of theory. To summarize, we hope that our work encourages the community to increase focus on reducing the gap between theory and practice to create reinforcement learning algorithms that train stably across a wide variety of environments.

### Limitations

Theoretically, our work lacks a convergence guarantee for DoubleGum. This is exceptionally challenging because to the best of our knowledge there are currently no convergence guarantees for heteroscedastic regression. Appendix C discusses convergence in more detail.

Experimentally, there are many areas left open for future work. For speed in proof of concept experiments, we only swept over five values for the pessimism factor hyperparameter and only tuned per-suite. We anticipate that a more thorough search of the pessimism factor combined with a per-task selection will improve our results even further. An obvious follow-up would be an algorithm that automatically adjusted the pessimism factor during training. Additionally, we only focused on environments with state observations to not deal with representation learning from visual inputs. Another obvious next step would be to train DoubleGum on visual inputs or POMDPs.

## Acknowledgements

We would like to thank Clement Gehring, Victor Hui, Sobhan Mohammadpour, Tianwei Ni, Anushree Rankawat, Fabrice Normandin, Ryan D'Orazio, Jesse Farebrother, Valentin Thomas, Matthieu Geist, Olivier Pietquin, Emma Frejinger, and the five anonymous reviewers.

All algorithms and code were implemented in JAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020) and cprpb (Yamada, 2019). Helper functions for running RL experiments were taken from Kostrikov (2021); Wu (2021); Hoffman et al. (2020) and Agarwal et al. (2021). Compute requirements are listed in Appendix E.5.

This research was funded by CIFAR and Hitachi and enabled in part by compute resources, software and technical help provided by Mila (mila.quebec), Calcul Quebec (calculquebec.ca) and the Digital Research Alliance of Canada (alliance.can.ca).

Hui carried out the majority of his work in Section J of Mila and would like to express his wish for the silent (but vibrant) working conditions there to continue for many years to come.

## References

* Agarwal et al. (2021) Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_, 34:29304-29320.
* Ba et al. (2016) Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. _arXiv preprint arXiv:1607.06450_.
* Ball et al. (2023) Ball, P. J., Smith, L., Kostrikov, I., and Levine, S. (2023). Efficient online reinforcement learning with offline data. _arXiv preprint arXiv:2302.02948_.
* Barth-Maron et al. (2018) Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W., Horgan, D., Dhruva, T., Muldal, A., Heess, N., and Lillicrap, T. (2018). Distributed distributional deterministic policy gradients. In _International Conference on Learning Representations_.
* Bas-Serrano et al. (2021) Bas-Serrano, J., Curi, S., Krause, A., and Neu, G. (2021). Logistic q-learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3610-3618. PMLR.
* Bellemare et al. (2020) Bellemare, M. G., Candidio, S., Castro, P. S., Gong, J., Machado, M. C., Moitra, S., Ponda, S. S., and Wang, Z. (2020). Autonomous navigation of stratospheric balloons using reinforcement learning. _Nature_, 588(7836):77-82.
* Bellemare et al. (2017) Bellemare, M. G., Dabney, W., and Munos, R. (2017). A distributional perspective on reinforcement learning. In _International conference on machine learning_, pages 449-458. PMLR.
* Bellman (1957) Bellman, R. (1957). A markovian decision process. _Journal of mathematics and mechanics_, pages 679-684.
* Bradbury et al. (2018) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs.
* Bradtke and Barto (1996) Bradtke, S. J. and Barto, A. G. (1996). Linear least-squares algorithms for temporal difference learning. _Machine learning_, 22(1-3):33-57.
* Bridle (1989) Bridle, J. (1989). Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. _Advances in neural information processing systems_, 2.
* Bridle (1990) Bridle, J. S. (1990). Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In _Neurocomputing_, pages 227-236. Springer.
* Brockman et al. (2016) Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym.
* Ceron and Castro (2021) Ceron, J. S. O. and Castro, P. S. (2021). Revisiting rainbow: Promoting more insightful and inclusive deep reinforcement learning research. In _International Conference on Machine Learning_, pages 1373-1383. PMLR.
* Chen et al. (2021) Chen, X., Wang, C., Zhou, Z., and Ross, K. W. (2021). Randomized ensembled double q-learning: Learning fast without a model. In _International Conference on Learning Representations_.
* Dabney et al. (2018a) Dabney, W., Ostrovski, G., Silver, D., and Munos, R. (2018a). Implicit quantile networks for distributional reinforcement learning. In _International conference on machine learning_, pages 1096-1105. PMLR.
* Dabney et al. (2018b) Dabney, W., Rowland, M., Bellemare, M., and Munos, R. (2018b). Distributional reinforcement learning with quantile regression. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32.
* Dearden et al. (1998) Dearden, R., Friedman, N., and Russell, S. (1998). Bayesian q-learning. _Aaaai/taai_, 1998:761-768.
* Degrave et al. (2022) Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de Las Casas, D., et al. (2022). Magnetic control of tokamak plasmas through deep reinforcement learning. _Nature_, 602(7897):414-419.
* D'Oro et al. (2022) D'Oro, P., Schwarzer, M., Nikishin, E., Bacon, P.-L., Bellemare, M. G., and Courville, A. (2022). Sample-efficient reinforcement learning by breaking the replay ratio barrier. In _Deep Reinforcement Learning Workshop NeurIPS 2022_.
* Dugas et al. (2000) Dugas, C., Bengio, Y., Belisle, F., Nadeau, C., and Garcia, R. (2000). Incorporating second-order functional knowledge for better option pricing. _Advances in neural information processing systems_, 13.
* Dugas et al. (2018)Ernst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6.
* Farama Foundation (2023) Farama Foundation (2023). Farama-Foundation/Gymnasium. original-date: 2022-09-08T01:58:05Z.
* Fisher and Tippett (1928) Fisher, R. A. and Tippett, L. H. C. (1928). Limiting forms of the frequency distribution of the largest or smallest member of a sample. In _Mathematical proceedings of the Cambridge philosophical society_, volume 24, pages 180-190. Cambridge University Press.
* Fortunato et al. (2015) Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., et al. (2017). Noisy networks for exploration. _arXiv preprint arXiv:1706.10295_.
* Fox et al. (2015) Fox, R., Pakman, A., and Tishby, N. (2015). Taming the noise in reinforcement learning via soft updates. _arXiv preprint arXiv:1512.08562_.
* Fujimoto et al. (2018) Fujimoto, S., Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic methods. In _International Conference on Machine Learning_, pages 1587-1596. PMLR.
* Garg et al. (2023) Garg, D., Hejna, J., Geist, M., and Ermon, S. (2023). Extreme q-learning: Maxent rl without entropy. _arXiv preprint arXiv:2301.02328_.
* Glorot et al. (2011) Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectifier neural networks. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 315-323. JMLR Workshop and Conference Proceedings.
* Gnedenko (1943) Gnedenko, B. (1943). Sur la distribution limite du terme maximum d'une serie aleatoire. _Annals of mathematics_, pages 423-453.
* Gumbel (1935) Gumbel, E. J. (1935). Les valeurs extremes des distributions statistiques. In _Annales de l'institut Henri Poincare_, volume 5, pages 115-158.
* Haarnoja et al. (2017) Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-based policies. In _International conference on machine learning_, pages 1352-1361. PMLR.
* Haarnoja et al. (2018a) Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018a). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR.
* Haarnoja et al. (2018b) Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. (2018b). Soft actor-critic algorithms and applications. _arXiv preprint arXiv:1812.05905_.
* Henderson et al. (2018) Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2018). Deep reinforcement learning that matters. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32.
* Hennigan et al. (2020) Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. (2020). Haiku: Sonnet for JAX.
* Hessel et al. (2018) Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. In _Thirty-second AAAI conference on artificial intelligence_.
* Hinton et al. (2012) Hinton, G., Srivastava, N., and Swersky, K. (2012). Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. _Cited on_, 14(8):2.
* Hoffman et al. (2020) Hoffman, M. W., Shahriari, B., Aslanides, J., Barth-Maron, G., Momchev, N., Sinopalnikov, D., Stanczyk, P., Ramos, S., Raichuk, A., Vincent, D., Hussenot, L., Dadashi, R., Dulac-Arnold, G., Orsini, M., Jacq, A., Ferret, J., Vieillard, N., Ghasempiour, S. K. S., Girgin, S., Pietquin, O., Behbahani, F., Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., Henderson, S., Friesen, A., Haroun, R., Novikov, A., Colmenarejo, S. G., Cabi, S., Gulcehre, C., Paine, T. L., Srinivasan, S., Cowie, A., Wang, Z., Piot, B., and de Freitas, N. (2020). Acme: A research framework for distributed reinforcement learning. _arXiv preprint arXiv:2006.00979_.
* Howard (1960) Howard, R. A. (1960). _Dynamic programming and markov processes_. John Wiley.
* Huang et al. (2022a) Huang, S., Dossa, R. F. J., Raffin, A., Kanervisto, A., and Wang, W. (2022a). The 37 implementation details of proximal policy optimization. In _ICLR Blog Track_. https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.
* Huang et al. (2022b) Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., and Araujo, J. G. (2022b). Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. _Journal of Machine Learning Research_, 23(274):1-18.
* Huang et al. (2020)Huber, P. J. (1992). Robust estimation of a location parameter. In _Breakthroughs in statistics: Methodology and distribution_, pages 492-518. Springer.
* Irpan (2018) Irpan, A. (2018). Deep reinforcement learning doesn't work yet. https://www.alexirpan.com/2018/02/14/rl-hard.html.
* Jowitt (1979) Jowitt, P. (1979). The extreme-value type-1 distribution and the principle of maximum entropy. _Journal of Hydrology_, 42(1-2):23-38.
* Karimpanal et al. (2021) Karimpanal, T. G., Le, H., Abdolshah, M., Rana, S., Gupta, S., Tran, T., and Venkatesh, S. (2021). Balanced q-learning: Combining the influence of optimistic and pessimistic targets. _arXiv preprint arXiv:2111.02787_.
* Kendall and Gal (2017) Kendall, A. and Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer vision? _Advances in neural information processing systems_, 30.
* Kimball (1946) Kimball, B. F. (1946). Sufficient statistical estimation functions for the parameters of the distribution of maximum values. _The Annals of Mathematical Statistics_, 17(3):299-309.
* Kingma and Ba (2014) Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_.
* Kostrikov (2021) Kostrikov, I. (2021). JAXRL: Implementations of Reinforcement Learning algorithms in JAX.
* Kuznetsov et al. (2022) Kuznetsov, A., Grishin, A., Tsypin, A., Ashukha, A., Kadurin, A., and Vetrov, D. (2022). Automating control of overestimation bias for reinforcement learning.
* Kuznetsov et al. (2020) Kuznetsov, A., Shvechikov, P., Grishin, A., and Vetrov, D. (2020). Controlling overestimation bias with truncated mixture of continuous distributional quantile critics. In _International Conference on Machine Learning_, pages 5556-5566. PMLR.
* La and Ghavamzadeh (2013) La, P. and Ghavamzadeh, M. (2013). Actor-critic algorithms for risk-sensitive mdps. _Advances in neural information processing systems_, 26.
* Lacoste et al. (2019) Lacoste, A., Luccioni, A., Schmidt, V., and Dandres, T. (2019). Quantifying the carbon emissions of machine learning. _arXiv preprint arXiv:1910.09700_.
* Lan et al. (2020) Lan, Q., Pan, Y., Fyshe, A., and White, M. (2020). Maxmin q-learning: Controlling the estimation bias of q-learning. _arXiv preprint arXiv:2002.06487_.
* Laskin et al. (2021) Laskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. (2021). Urlb: Unsupervised reinforcement learning benchmark. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_.
* Lee and Powell (2012) Lee, D. and Powell, W. (2012). An intelligent battery controller using bias-corrected q-learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 26, pages 316-322.
* Lee et al. (2021) Lee, K., Laskin, M., Srinivas, A., and Abbeel, P. (2021). Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. In _International Conference on Machine Learning_, pages 6131-6141. PMLR.
* Lillicrap et al. (2015) Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_.
* Lin (1992) Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_, 8(3):293-321.
* Ma et al. (2020) Ma, X., Xia, L., Zhou, Z., Yang, J., and Zhao, Q. (2020). Dsac: Distributional soft actor critic for risk-sensitive reinforcement learning. _arXiv preprint arXiv:2004.14547_.
* Mai et al. (2022) Mai, V., Mani, K., and Paull, L. (2022). Sample efficient deep reinforcement learning via uncertainty estimation. _arXiv preprint arXiv:2201.01666_.
* McFadden et al. (1973) McFadden, D. et al. (1973). _Conditional logit analysis of qualitative choice behavior_. Institute of Urban and Regional Development, University of California.
* Mnih et al. (2016) Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pages 1928-1937. PMLR.
* Mnih et al. (2016)Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533.
* Morimura et al. (2012) Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. (2012). Parametric return density estimation for reinforcement learning. _arXiv preprint arXiv:1203.3497_.
* Moskovitz et al. (2021) Moskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M., and Jordan, M. (2021). Tactical optimism and pessimism for deep reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12849-12863.
* Munos and Szepesvari (2008) Munos, R. and Szepesvari, C. (2008). Finite-time bounds for fitted value iteration. _J. Mach. Learn. Res._, 9:815-857.
* Nix and Weigend (1994) Nix, D. A. and Weigend, A. S. (1994). Estimating the mean and variance of the target probability distribution. In _Proceedings of 1994 ieee international conference on neural networks (ICNN'94)_, volume 1, pages 55-60. IEEE.
* Peters et al. (2010) Peters, J., Mulling, K., and Altun, Y. (2010). Relative entropy policy search. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 24, pages 1607-1612.
* Puterman (2014) Puterman, M. L. (2014). _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons.
* Riedmiller (2005) Riedmiller, M. (2005). Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. In _European conference on machine learning_, pages 317-328. Springer.
* Rummery and Niranjan (1994) Rummery, G. A. and Niranjan, M. (1994). _On-line Q-learning using connectionist systems_, volume 37. University of Cambridge, Department of Engineering Cambridge, UK.
* Rust (1994) Rust, J. (1994). Structural estimation of markov decision processes. _Handbook of econometrics_, 4:3081-3143.
* Samuelson (1937) Samuelson, P. A. (1937). A note on measurement of utility. _The review of economic studies_, 4(2):155-161.
* Saxe et al. (2013) Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_.
* Schaul et al. (2022) Schaul, T., Barreto, A., Quan, J., and Ostrovski, G. (2022). The phenomenon of policy churn. In _Advances in Neural Information Processing Systems_.
* Schaul et al. (2016) Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In Bengio, Y. and LeCun, Y., editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_.
* Schrittwieser et al. (2020) Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. (2020). Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609.
* Schulman et al. (2015) Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR.
* Schwarzer et al. (2023) Schwarzer, M., Ceron, J. S. O., Courville, A., Bellemare, M. G., Agarwal, R., and Castro, P. S. (2023). Bigger, better, faster: Human-level atari with human-level efficiency. In _International Conference on Machine Learning_, pages 30365-30380. PMLR.
* Seitzer et al. (2022) Seitzer, M., Tavakoli, A., Antic, D., and Martius, G. (2022). On the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks. _arXiv preprint arXiv:2203.09168_.
* Seyde et al. (2022) Seyde, T., Werner, P., Schwarting, W., Gilitschenski, I., Riedmiller, M., Rus, D., and Wulfmeier, M. (2022). Solving continuous control via q-learning. _arXiv preprint arXiv:2210.12566_.
* Shahriari et al. (2022) Shahriari, B., Abdolmaleki, A., Byravan, A., Friesen, A., Liu, S., Springenberg, J. T., Heess, N., Hoffman, M., and Riedmiller, M. (2022). Revisiting gaussian mixture critic in off-policy reinforcement learning: a sample-based approach. _arXiv preprint arXiv:2204.10256_.
* Silver et al. (2016) Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489.
* Silver et al. (2016)Sutton, R. S. and Barto, A. G. (1998). _Reinforcement learning: An introduction_. MIT press.
* Sutton and Barto (2018) Sutton, R. S. and Barto, A. G. (2018). _Reinforcement Learning: An Introduction_. The MIT Press, second edition.
* Tamar et al. (2016) Tamar, A., Di Castro, D., and Mannor, S. (2016). Learning the variance of the reward-to-go. _The Journal of Machine Learning Research_, 17(1):361-396.
* Tamar and Mannor (2013) Tamar, A. and Mannor, S. (2013). Variance adjusted actor critic algorithms. _arXiv preprint arXiv:1310.3697_.
* Tassa et al. (2018) Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. (2018). Deepmind control suite. _arXiv preprint arXiv:1801.00690_.
* Teng et al. (2022) Teng, M., van de Panne, M., and Wood, F. (2022). Exploration with multi-sample target values for distributional reinforcement learning. _arXiv preprint arXiv:2202.02693_.
* Thrun and Schwartz (1993) Thrun, S. and Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In _Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum_, volume 6, pages 1-9.
* Todorov et al. (2012) Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033. IEEE.
* Tunyasuvunakool et al. (2020) Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa, Y. (2020). dm_control: Software and tasks for continuous control. _Software Impacts_, 6:100022.
* Van Hasselt et al. (2016) Van Hasselt, H., Guez, A., and Silver, D. (2016). Deep reinforcement learning with double q-learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 30.
* Varian (1975) Varian, H. R. (1975). A bayesian approach to real estate assessment. _Studies in Bayesian econometric and statistics in Honor of Leonard J. Savage_, pages 195-208.
* Vinyals et al. (2019) Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354.
* Wang et al. (2021) Wang, H., Lin, S., and Zhang, J. (2021). Adaptive ensemble q-learning: Minimizing estimation bias via error feedback. _Advances in Neural Information Processing Systems_, 34:24778-24790.
* Wang et al. (2016) Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. In _International conference on machine learning_, pages 1995-2003. PMLR.
* Watkins and Dayan (1992) Watkins, C. J. and Dayan, P. (1992). Q-learning. _Machine learning_, 8:279-292.
* Wu (2021) Wu, Y. (2021). td3-jax. https://github.com/yifan12wu/td3-jax.
* Wu and He (2018) Wu, Y. and He, K. (2018). Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19.
* Wurman et al. (2022) Wurman, P. R., Barrett, S., Kawamoto, K., MacGlashan, J., Subramanian, K., Walsh, T. J., Capobianco, R., Devlic, A., Eckert, F., Fuchs, F., et al. (2022). Outracing champion gran turismo drivers with deep reinforcement learning. _Nature_, 602(7896):223-228.
* Xu et al. (2019) Xu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J. (2019). Understanding and improving layer normalization. _Advances in Neural Information Processing Systems_, 32.
* Yamada (2019) Yamada, H. (2019). cpprb.
* Yang et al. (2019) Yang, D., Zhao, L., Lin, Z., Qin, T., Bian, J., and Liu, T.-Y. (2019). Fully parameterized quantile function for distributional reinforcement learning. _Advances in neural information processing systems_, 32.
* Yarats et al. (2021) Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2021). Mastering visual continuous control: Improved data-augmented reinforcement learning. _arXiv preprint arXiv:2107.09645_.
* Yu et al. (2020) Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. (2020). Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on robot learning_, pages 1094-1100. PMLR.
* Zhang et al. (2023) Zhang, Y., Lin, J., Li, F., Adler, Y., Rasul, K., Schneider, A., and Nevmyvaka, Y. (2023). Risk bounds on aleatoric uncertainty recovery. In _International Conference on Artificial Intelligence and Statistics_, pages 6015-6036. PMLR.

## Appendix

### Table of Contents

* A Proofs A.1 The SoftMax Location of Gumbel Distribution A.2 Shifting the Value Function A.3 The Difference between Two Gumbel Random Variables is a Logistic A.4 Soft Q-Learning Identity B Further Theory and Derivations B.1 Actor Loss B.2 Maximum-Entropy Reinforcement Learning B.3 Interpreting the Cross-Entropy as a Pessimism Factor B.4 Comparison between DoubleGum and XQL
* C A Discussion on The Convergence of DoubleGum C.1 Tabular Q-Functions C.2 Deep Q-Functions
* D Further Empirical Evidence for Theoretical Assumptions D.1 Noise Distributions in Deep Q-Learning D.2 Adjusting The Pessimism Factor
* E Further Experimental Details E.1 Noise Distribution Discrepancy with Extreme Q-Learning E.2 Continuous Control Benchmarks and Evaluation E.3 Discrete Control Baselines E.4 Continuous Control Baselines E.5 Compute Requirements F Further Results F.1 Adjusting the Pessimism of DoubleGum F.2 Adjusting the Pessimism of Baseline Algorithms F.3 Discrete Control F.4 Continuous Control with Default Pessimism F.5 Continuous Control with Pessimism adjusted Per-SuiteProofs

### The SoftMax Location of Gumbel Distribution

We refer to the \(\log\)-sum-\(\exp\) operator as the SoftMax operator. This is not the same-named operator in Bridle (1989, 1990), which we suggest should be (re-)named SoftArgMax.

**Theorem 1**.: \[\max_{i}[\alpha_{i}+g_{i}]=\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}}{\beta} \right)+g,\;\;\text{where}\;\;g,g_{i}\sim\mathcal{G}\left(0,\beta\right)\]

_where \(\mathcal{G}\) is a Gumbel distribution. A Gumbel random variable \(g\sim\mathcal{G}(\alpha,\beta)\) specified by location \(\alpha\in\mathbb{R}\) and spread \(\beta>0\) has PDF \(p(g)=\frac{1}{\beta}\exp(-z-\exp(-z))\) with \(z=(g-\alpha)/\beta\) and CDF \(P(g)=\frac{1}{\beta}\exp(-\exp(-z))\)(Gumbel, 1935)._

Proof.: First note

\[\alpha_{i}+g_{i},\;\;g_{i}\sim\mathcal{G}\left(0,\beta\right)\implies\alpha_{i }+g_{i}\sim\mathcal{G}_{i}\left(\alpha_{i},\beta\right)\]

Then, denoting \(y=\max_{i}[\alpha_{i}+g_{i}]\)

\[P\left(X\leqslant y\right) =\prod_{i}P_{i}(X\leqslant y)\] \[=\prod_{i}\mathcal{G}_{i}(X\leqslant y)\] \[=\prod_{i}\exp\left(-\exp\left(-\frac{x-\alpha_{i}}{\beta}\right)\right)\] \[=\exp\left(-\sum_{i}\exp\left(-\frac{x-\alpha_{i}}{\beta}\right)\right)\] \[=\exp\left(-\exp\left(-\frac{x}{\beta}\right)\sum_{i}\exp\left( \frac{\alpha_{i}}{\beta}\right)\right)\] \[=\exp\left(-\exp\left(-\frac{x}{\beta}\right)\sum_{i}\exp\left( \frac{\alpha_{i}}{\beta}\right)\right)\] \[=\exp\left(-\exp\left(-\frac{1}{\beta}\left(x-\beta\log\sum_{i} \exp\left(\frac{\alpha_{i}}{\beta}\right)\right)\right)\right)\] \[=\mathcal{G}\left(\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}}{ \beta}\right),\beta\right)\] \[=\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}}{\beta}\right)+g,\; \;\text{where}\;\;g\sim\mathcal{G}\left(0,\beta\right)\]

When applied to discrete \(Q\)-Learning, we produce

\[\max_{\mathcal{A}}\left[Q_{\theta}(s,a)+g_{\theta,a}(s)\right]=\beta_{\theta} (s)\log\int_{\mathcal{A}_{s}}\exp\left(\frac{Q_{\theta}(s,a)}{\beta_{\theta}( s)}\right)\,\mathrm{d}a+g_{\theta}(s)\] (10)

where \(g_{\theta,a}(\cdot),g_{\theta}(\cdot)\sim\mathcal{G}\left(0,\beta_{\theta}( \cdot)\right),\;\;\text{for all}\;a\in\mathcal{A}\).

We assume that the same result holds for the continuous case if \(|\mathcal{A}|<\infty\), an assumption first used in Lemma 1, Appendix B.1, Page 11 of Haarnoja et al. (2018a) to ensure boundedness. Here, we require the output of the \(\max\)-operator to be bounded, which cannot be the case when the number of its arguments is \(\infty\).

### Shifting the Value Function

**Theorem 2**.: \[\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}+g}{\beta}\right)=\beta\log\sum_{i}\exp \left(\frac{\alpha_{i}}{\beta}\right)+g\]

Proof.: \[\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}+g}{\beta}\right) =\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}}{\beta}+\frac{g}{ \beta}\right)\] (11) \[=\beta\log\exp\left(\frac{g}{\beta}\right)\sum_{i}\exp\left(\frac {\alpha_{i}}{\beta}\right)\] (12) \[=\beta\log\exp\left(\frac{g}{\beta}\right)+\beta\log\sum_{i}\exp \left(\frac{\alpha_{i}}{\beta}\right)\] (13) \[=\beta\log\sum_{i}\exp\left(\frac{\alpha_{i}}{\beta}\right)+g\] (14)

### The Difference between Two Gumbel Random Variables is a Logistic

**Theorem 3**.: \[x_{1},x_{2}\sim\mathcal{G}(0,\beta)\implies z=x_{1}-x_{2},\quad z\sim\mathcal{ L}(0,\beta)\]

_where \(\mathcal{L}\) is a logistic distribution. A logistic random variable \(l\sim L(\alpha,\beta)\) with location \(\alpha\) and spread \(\beta\) has PDF \(\frac{\exp\left(-(l-\alpha)/\beta\right)}{\beta\left(1+\exp\left(-(l-\alpha)/ \beta\right)\right)^{2}}\) and CDF \(\frac{1}{1+\exp\left(-(l-\alpha)/\beta\right)}\)._

Proof.: First, construct the convolution based on the joint PDF

\[\mathcal{L}(z) =P(Z\leqslant z)\] \[=P(X_{1}-X_{2}\leqslant z)\] \[=P(X_{1}\leqslant z+x_{2})\] \[=\int_{-\infty}^{\infty}\int_{-\infty}^{z+x_{2}}g(x_{1})\,g(x_{2 })\,\mathrm{d}x_{1}\,\mathrm{d}x_{2}\] \[=\int_{-\infty}^{\infty}G(z+x_{2})\,g(x_{2})\,\mathrm{d}x_{2}\enspace.\]

Rewriting \(x_{2}\) as \(x\) yields

\[\mathcal{L}(z) =\int_{-\infty}^{\infty}G(z+x)\,g(x)\,\mathrm{d}x\] \[=\int_{-\infty}^{\infty}\exp\left(\exp-\frac{z+x}{\beta}\right) \exp\left(-\frac{x}{\beta}-\exp-\frac{x}{\beta}\right)\mathrm{d}x\] \[=\int_{-\infty}^{\infty}\exp\left(-e^{-\frac{x}{\beta}}\left(1+e^ {-\frac{x}{\beta}}\right)\right)e^{-\frac{x}{\beta}}\,\mathrm{d}x\] \[=\int_{0}^{\infty}\frac{1}{\beta}\exp\left(-u\left(1+e^{-\frac{x }{\beta}}\right)\right)\mathrm{d}u,\enspace\text{where}\enspace u=e^{-\frac{x}{ \beta}},\ \mathrm{d}u=-\frac{1}{\beta}e^{-\frac{x}{\beta}}\mathrm{d}x\] \[=\frac{1}{\beta}\frac{1}{1+e^{-\frac{x}{\beta}}}e^{\left(-u\left( 1+e^{-\frac{x}{\beta}}\right)\right)}\bigg{|}_{0}^{\infty}\] \[=\frac{1}{1+e^{-\frac{x}{\beta}}}\enspace.\]

which is the CDF of \(\mathcal{L}(0,\beta)\)

### Soft Q-Learning Identity

**Theorem 4**.: _For an arbitrary \(p(x)\)_

\[\beta\log\int\exp\left(\frac{E(x)}{\beta}\right)\,\mathrm{d}x=\mathop{\mathbb{E}} \limits_{x\sim p(\cdot)}[E(x)]+\beta\,\mathbb{C}[p\parallel p^{\star}],\ \ \text{where}\ \ p^{\star}(x)=\frac{\exp\frac{E(x)}{\beta}}{\int\exp\frac{E(x)}{\beta}\, \mathrm{d}x}\]

Proof.: \[\mathop{\mathbb{E}}\limits_{p(x)}[E(x)]+\beta\,\mathbb{C}[p \parallel p^{\star}] =\mathop{\mathbb{E}}\limits_{p(x)}[E(x)]-\beta\,\int p(x)\log \frac{\exp\frac{E(x)}{\beta}}{\int\exp\frac{E(x^{\prime})}{\beta}\,\mathrm{d}x ^{\prime}}\,\mathrm{d}x\] \[=\beta\int p(x)\log\int\exp\frac{E(x^{\prime})}{\beta}\,\mathrm{d }x^{\prime}\,\mathrm{d}x\] \[=\beta\log\int\exp\frac{E(x)}{\beta}\,\mathrm{d}x\]

When applied to \(Q\)-Learning, the following identity produces

\[\beta(s)\log\int\exp\left(\frac{Q_{\theta}(s,a)}{\beta(s)}\right) \,\mathrm{d}a =\mathop{\mathbb{E}}\limits_{\pi_{\phi}(a^{\prime}|s^{\prime})}[Q_{ \theta}(s,a)]+\beta(s)\,\mathbb{C}[\pi_{\phi}\parallel p_{\theta}]\] \[\text{where}\ \ p_{\theta}(a\mid s) =\frac{\exp\frac{Q_{\theta}(s,a)}{\beta(s)}}{\int\exp\frac{Q_{ \theta}(s,a^{\prime})}{\beta(s)}\,\mathrm{d}a^{\prime}}\]

## Appendix B Further Theory and Derivations

### Actor Loss

The actor losses used in DoubleGum, SAC, and DDPG are all derived from the same principle. For a given \(s\), the actor loss function should minimizes the following (reverse) KL-Divergence, previously presented in Equation 7.

\[\min_{\phi}\beta\,\mathbb{D}_{\text{KL}}[\pi_{\phi}\mid\mid p_{\theta}],\ \ \text{where}\ \ p_{\theta}(a\mid s)=\frac{\exp Q_{\theta}^{\text{ new}}(s,a)/\beta}{\int\exp Q_{\theta}^{\text{new}}(s,a^{\prime})/\beta\, \mathrm{d}a^{\prime}}\ \.\]

This simplifies as

\[\min_{\phi}\beta\,\operatorname{D}_{\text{KL}}[\pi_{\phi}\mid \mid p_{\theta}] =\min_{\phi}\beta\,\int\pi_{\phi}(a\mid s)\log\frac{\pi_{\phi}(a \mid s)}{p_{\theta}(a\mid s)}\,\mathrm{d}a\] \[=\max_{\phi}\left[\beta\,\mathbb{H}[\pi_{\phi}]+\beta\,\int\pi_{ \phi}(a\mid s)\log\frac{\exp Q_{\theta}^{\text{new}}(s,a)/\beta}{\int\exp Q_{ \theta}^{\text{new}}(s,a^{\prime})/\beta\,\mathrm{d}a^{\prime}}\,\mathrm{d}a\right]\] \[=\max_{\phi}\left[\beta\,\mathbb{H}[\pi_{\phi}]+\beta\,\int\pi_{ \phi}(a\mid s)\,\frac{Q_{\theta}^{\text{new}}(s,a)}{\beta}\,\mathrm{d}a\right]\]

which is then estimated by Monte-Carlo samples from \(\pi_{\phi}\) as

\[\max_{\phi}\,\mathop{\mathbb{E}}\limits_{\pi_{\phi}(a\mid s)}\left[Q_{\theta}^ {\text{new}}(s,a)-\beta\log\pi_{\phi}(a\mid s)\right]\ \.\] (15)

SAC [11, 2] has a policy with learned variance and state-independent \(\beta\). DDPG [12] has a fixed-variance policy which removes the second term in Equation 15 as it is constant with respect to the maximization. DoubleGum has a state-dependent \(\beta(s)\), but uses the same actor loss as DDPG because DoubleGum uses a DDPG fixed-variance policy.

### Maximum-Entropy Reinforcement Learning

SACv1 (Haarnoja et al., 2018) is a special case of DoubleGum and DDPG (Lillicrap et al., 2015) is a special case of SAC. All three continuous control algorithms have an actor and critic loss derived from the same principle. Section B.1 shows this for the actor losses of DoubleGum, SAC, and DDPG. We now relate the critic losses to each other, starting from the most general case, DoubleGum. In continuous control, DoubleGum uses the following noise model, formed from substituting Equation 7 into Equation 5:

\[Q_{\theta}^{\text{new}}(s,a)+l_{\theta,a}(s)=\operatorname*{\mathbb{E}}_{p(s^{ \prime}|s,a)}\left[r+\gamma\operatorname*{\mathbb{E}}_{\pi_{\phi}(a^{\prime}|s^ {\prime})}[Q_{\theta}^{\text{new}}(s^{\prime},a^{\prime})]+\gamma\beta_{\theta }(s)\operatorname*{\mathbb{C}}[\pi_{\phi}\ ||\ p_{\theta}]\right]\enspace.\] (16)

Here, \(l_{\theta,a}(\cdot)\sim\mathcal{L}(0,\beta_{\theta}(\cdot))\) is a logistic distribution and \(p_{\theta}(a\mid s)\propto\exp\frac{Q_{\theta}^{\text{new}}(s,a)}{\beta_{ \theta}(s)}\). The DoubleGum critic loss is derived from this noise model by approximating the RHS with Equation 8 and learning \(\theta\) with moment matching in Section 3.2.

The SAC noise model is derived from Equation 16 in three ways. First, SAC approximates \(l_{\theta,a}(s)\sim\mathcal{L}(0,\beta_{\theta}(\cdot))\) as \(n_{\theta,a}\sim\mathcal{N}(0,\sigma)\), motivated by the fact that both distributions have the same mean/mode. Secondly, SAC approximates the DoubleGum state-dependent logistic spread \(\beta_{\theta}(s)\) as temperature parameter \(\beta\) learned not as a part of the critic but by itself with Lagrangian dual gradient descent. Thirdly, SAC breaks down \(\operatorname*{\mathbb{C}}[\pi_{\phi}\ ||\ p_{\theta}]=\operatorname*{ \mathbb{H}}[\pi_{\phi}]+\operatorname*{\mathbb{D}}_{\text{KL}}[\pi_{\phi}\ ||\ p_{\theta}]\) before assuming that the KL-Divergence is negligible, given that it is minimized by the actor loss. These three approximations yield the SAC noise model as

\[Q_{\theta}^{\text{new}}(s,a)+n_{\theta,a}=\operatorname*{\mathbb{E}}_{p(s^{ \prime}|s,a)}\left[r+\gamma\operatorname*{\mathbb{E}}_{\pi_{\phi}(a^{\prime}| s^{\prime})}[Q_{\theta}^{\text{new}}(s^{\prime},a^{\prime})+\beta\log\pi_{ \phi}(a^{\prime}\mid s^{\prime})]\right]\enspace.\] (17)

MLE of \(\theta\) wrt the above noise model yields the MSBE critic loss.

DDPG is a special case of SAC that assumes \(\beta\to 0\), removing the last term in Equation 17. \(\lim_{\beta\to 0}p_{\theta}(a\mid s)\) becomes deterministic, so \(\pi_{\phi}\) may be modelled by a deterministic policy.

### Interpreting the Cross-Entropy as a Pessimism Factor

In continuous control, Fujimoto et al. (2018) introduced Twin Networks, a method that improved sample-efficiency with pessimistic bootstrapped targets computed by returning a sample-wise minimum from an ensemble of two \(Q\)-functions. Follow-up work selects a quantile estimate from an ensemble (Kuznetsov et al., 2020; Chen et al., 2021; Ball et al., 2023), which we demonstrate is equivalent to estimating \(V_{\theta,\beta}^{\text{soft, new}}\).

Suppose there is an ensemble of \(n\) networks where the ith network follows \(Q_{\theta_{i}}(s,a)=Q_{\theta}(s,a)+z_{i}(s,a)\). Here, \(Q_{\theta}\) is an 'ideal' function approximator never instantiated nor computed and \(z\) is an arbitrary noise source. When \(n\) is sufficiently large,

\[\min_{i}\operatorname*{\mathbb{E}}_{\pi_{\phi}(a|s)}[Q_{\theta_{i }}(s,a)] =\min_{i}\operatorname*{\mathbb{E}}_{\pi_{\phi}(a|s)}[Q_{\theta}(s,a)+z_{i}(s,a)]=\operatorname*{\mathbb{E}}_{\pi_{\phi}(a|s)}[Q_{\theta}(s,a)] +\min_{i}z_{i}(s)\] \[=\operatorname*{\mathbb{E}}_{\pi_{\phi}(a|s)}[Q_{\theta}(s,a)]-g(s ),\enspace\text{where }g(s)\sim\mathcal{G}(\alpha(s),\beta(s))\enspace.\]

A Gumbel random variable \(g\sim\mathcal{G}(\alpha,\beta)\) has \(\operatorname*{\mathbb{E}}[g]=\alpha+\gamma_{e}\beta\), where \(\gamma_{e}\) is the Euler-Mascheroni constant, so for a deterministic environment the bootstrapped targets become

\[r+\gamma\operatorname*{\mathbb{E}}_{\pi_{\phi}(a^{\prime}|s^{\prime})}[Q_{ \theta}(s^{\prime},a^{\prime})]-\gamma\alpha(s^{\prime})-\gamma\gamma_{e}\beta (s^{\prime})\enspace,\]

recovering Equation 8, the DoubleGum continuous control targets, up to an additive term \(\gamma\alpha(s^{\prime})\), while \(-\gamma\gamma_{e}\beta(s^{\prime})\) recovers the spread \(\gamma c\sigma_{\theta}(s^{\prime})\) up to a negative scaling factor, indicating that the default \(c\) should be negative. Moskovitz et al. (2021) and Ball et al. (2023) showed that the appropriate ensemble size and selected quantile changes the overestimation bias, so appropriate values would ensure \(\alpha(s^{\prime})=0\).

### Comparison between DoubleGum and XQL

We present an explanation of Extreme \(Q\)-Learning (XQL) as presented in Appendix C.1 of Garg et al. (2023). XQL can be derived from Soft Bellman Equation backups given by

\[Q(s,a)\leftarrow\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}[r(s,a,s^{\prime} )+\gamma V^{\text{soft}}(s)],\;\;\text{where}\;\;V^{\text{soft}}(s)=\beta\log \sum_{a^{\prime}}\exp\left(\frac{Q(s^{\prime},a^{\prime})}{\beta}\right)\]

and \(\beta\) is a fixed hyperparameter. Computing the \(\log\)-\(\operatorname{sum-exp}\) of \(V^{\text{soft}}\) is intractable in continuous control, as the sum over \(a^{\prime}\) becomes an integral in continuous control tasks.

Garg et al. (2023) present a method of estimating its value using Gumbel regression. Given a (potentially infinite) set of scalars \(x\in X\), Gumbel regression provides a method to estimate the numerical value of \(\log\)-\(\operatorname{sum-exp}_{\beta}(x)=\beta\log\sum_{X}\exp x/\beta\). Gumbel regression assumes \(x\sim\mathcal{G}(\alpha,\beta)\), where \(\mathcal{G}\) is a homoscedastic Gumbel distribution, and \(\beta\) is a fixed (hyper)parameter. \(\alpha\) estimated by MLE tends towards \(\log\)-\(\operatorname{sum-exp}_{\beta}(x)\). MLE is performed by numerically maximizing the \(\log\)-likelihood of a Gumbel distribution, which recovers the LINEar-EXponential (LINEX) loss function introduced by Varian (1975).

Garg et al. (2023) incorporate Gumbel regression into deep \(Q\)-Learning in two ways, which they name X-SAC and X-TD3. X-SAC combines Gumbel regression to estimate the soft value function used in SACv0 (Haarnoja et al., 2018). The soft value function \(V^{\text{soft}}_{\rho}(s)\) is a neural network whose parameters \(\rho\) are learned by Gumbel regression from \(Q_{\psi}(s,a)\sim\mathcal{G}(V_{\rho}(s),\beta)\), where \(\psi\) are target network parameters. A neural network \(Q_{\theta}\) with parameters \(\theta\) may then be learned by the MSE between itself and \(\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}[r(s,a,s^{\prime})+\gamma V^{ \text{soft}}_{\rho}(s)]\). X-SAC is vastly different from DoubleGum, because our algorithm does not estimate the soft value function with a separate neural network.

Gumbel regression is directly used to learn the \(Q\)-values in X-TD3. First, the bootstrapped targets are thusly rewritten

\[y^{\text{soft}}(s,a) =\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r+\gamma \beta\log\sum_{a^{\prime}}\exp\left(\frac{Q_{\phi}(s^{\prime},a^{\prime})}{ \beta}\right)\right]\] \[=\operatorname*{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[\gamma\beta \log\sum_{a^{\prime}}\exp\left(\frac{r+\gamma Q_{\psi}(s^{\prime},a^{\prime})- Q_{\theta}(s,a)}{\gamma\beta}\right)\right]\]

In environments with deterministic environments, which comprise all environments considered by Garg et al. (2023) and our paper, Lemma C.1 of Garg et al. (2023) provides a method of learning the soft value function with Gumbel regression on \(y^{\text{soft}}(s,a)\sim\mathcal{G}(Q_{\theta}(s,a),\gamma\beta)\). The Gumbel regression objective used in X-TD3 to learn \(\theta\) is vastly different from the moment matching with the logistic distribution DoubleGum uses to learn \(\theta\).

To motivate their use of Gumbel regression, Garg et al. (2023) derived a noise model which they use to present empirical evidence of homoscedastic Gumbel noise. In contrast, we presented empirical evidence of heteroscedastic logistic noise formed from a noise model with two heteroscedastic Gumbel distributions.

## Appendix C A Discussion on The Convergence of DoubleGum

To the best of our knowledge, there are two types of convergence analysis in \(Q\)-Learning: 1) operator-theoretic analysis over tabular \(Q\)-functions, and 2) training dynamics of neural network parameters. We believe the second is more appropriate for DoubleGum, because our theory addresses issues in using neural networks (and not tables) for \(Q\)-learning. Nevertheless, for completeness, we discuss convergence guarantees for the tabular setting and the function approximation setting. While we can guarantee convergence for the former setting, we have no guarantees for the second.

### Tabular Q-Functions

Appendices B.1 and B.2 present DoubleGum as a MaxEnt RL algorithm. When \(Q\)-functions are tabular, Appendix A of Haarnoja et al. (2018) shows that MaxEnt RL algorithms may be derivedfrom soft policy iteration. We therefore present a convergence proof for DoubleGum with tabular \(Q\)-functions based on soft policy iteration.

DoubleGum treats the return as coming from a logistic distribution and learns its location and spread. In the tabular setting, two tables would need to be learned, \(Q(s,a)\) and \(\beta(s)\). An algorithm to learn these tables in a finite MDP with soft policy iteration is presented in Algorithm 2. Policy evaluation is done by Lines 4-6 while Line 7 performs policy improvement.

Proof of convergence of Algorithm 2 is similar to the SAC proof of convergence in Appendix B of Haarnoja et al. (2018). This should not be surprising, given that Appendix B.2 shows SAC as a special case of DoubleGum. We first show that policy evaluation converges and that a new policy found by policy improvement does not reduce the magnitude of the value function.

**Lemma 5** (Soft Policy Evaluation).: _Consider the Soft Policy Evaluation operator given by_

\[Q_{i+1}(s,a)\leftarrow\mathop{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r(s,a,s^{ \prime})+\gamma\beta_{i}(s^{\prime})\log\sum_{a^{\prime}}\exp\left(\frac{Q_{i}( s^{\prime},a^{\prime})}{\beta_{i}(s^{\prime})}\right)\right]\text{ over all }(s,a)\text{ pairs.}\]

\(\lim_{i\rightarrow\infty}Q_{i}\) _converges to the soft \(Q\)-value._

Proof.: Following Appendix A.4

\[\beta(s)\log\sum_{a}\exp\left(\frac{Q(s,a)}{\beta(s)}\right) =\mathop{\mathbb{E}}_{\pi(a|s)}[Q_{\theta}(s,a)]+\beta(s)\,\mathbb{C }[\pi\mid\mid p]\] \[\text{where }\ p(a\mid s) =\frac{\exp(Q(s,a)/\beta(s))}{\sum_{a^{\prime}}\exp(Q(s,a^{\prime })/\beta(s))}\]

the bootstrapped targets may be thusly rewritten

\[\mathop{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r(s,a,s^{\prime})+ \gamma\beta(s^{\prime})\log\sum_{a^{\prime}}\exp\left(\frac{Q(s^{\prime},a^{ \prime})}{\beta(s^{\prime})}\right)\right]\] \[=\mathop{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r(s,a,s^{\prime}) +\gamma\mathop{\mathbb{E}}_{\pi(a^{\prime}|s^{\prime})}[Q(s^{\prime},a^{ \prime})]+\beta(s^{\prime})\,\mathbb{C}[\pi\mid\mid p]\right]\] \[=\mathop{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r^{\prime}(s,a,s^{ \prime})+\gamma\mathop{\mathbb{E}}_{\pi(a^{\prime}|s^{\prime})}[Q(s^{\prime},a^{\prime})]\right]\]

where \(r^{\prime}(s,a,s^{\prime})=r(s,a,s^{\prime})+\beta(s^{\prime})\,\mathbb{C}[ \pi\mid\mid p]\).

Following Lemma 1 in Haarnoja et al. (2018), Sutton and Barto (1998) gives convergence of \(Q_{i+1}(s,a)\leftarrow\mathop{\mathbb{E}}_{p(s^{\prime}|s,a)}\left[r^{\prime} (s,a,s^{\prime})+\gamma\mathbb{E}_{\pi(a^{\prime}|s^{\prime})}[Q_{i}(s^{\prime },a^{\prime})]\right]\) 

The proof of Soft Policy Improvement should be identical to SAC, given that Appendix B.1 shows that DoubleGum and SAC use identical actor losses. As such, Lemma 5 can be used in place of Lemma 1 in Theorem 1 of Haarnoja et al. (2018), thus showing convergence of DoubleGum in the tabular setting.

### Deep Q-Functions

Parameters of the deep \(Q\)-function used by DoubleGum in Algorithm 1 are learned by a loss function equivalent to that of heteroscedastic normal regression. Convergence of DoubleGum in the function approximation setting would therefore rely on convergence of heteroscedastic normal regression.

Zhang et al. (2023) introduces PAC-bounds for heteroscedastic normal regression, but on the condition that the mean-estimate is close to the ground truth mean, as mentioned in Paragraph 1 of Section 4. This is empirically achieved by Seitzer et al. (2022), who analyze heteroscedastic normal regression and find that the mean-estimate frequently converges to an underfitting solution. This is because the Negative Log-Likelihood (NLL) of a normal distribution is minimized when the variance becomes large - in Equation 6, this term is denoted with \(\sigma_{\theta}^{2}\). As such, changes in \(Q_{\theta}^{\text{new}}\) will not change the loss function much. To rectify this, Seitzer et al. (2022) multiplies the NLL of the normal with the numerical value of the standard deviation, reducing the dominance of \(\sigma_{\theta}\) on the loss function.

## Appendix D Further Empirical Evidence for Theoretical Assumptions

### Noise Distributions in Deep Q-Learning

Figure 6 presents graphs corresponding to Figure 1c for all environments considered in this paper. Continuous control results were generated from DoubleGum with default pessimism (\(c=-0.1\)).

### Adjusting The Pessimism Factor

Figure 7 presents graphs corresponding to Figure 2 for all continuous control environments considered in this paper.

## Appendix E Further Experimental Details

### Noise Distribution Discrepancy with Extreme Q-Learning

In Appendix D.2 of Page 19, Garg et al. (2023) fitted a Gumbel distribution to the TD errors on three continuous control environments. The Gumbel distribution was a good fit in two of the three environments they benchmarked on. We could not reproduce this result and attribute the discrepancy to experimental differences.

Garg et al. (2023) logged their batch of 256 TD errors once every 5,000 steps during training for 100,000 timesteps, producing \(\approx 4000\) samples which were aggregated. They also computed bootstrapped targets with online parameters. In contrast, we sample 10,000 TD errors with bootstrapped targets computed from target parameters at a single timestep instance, and we do not aggregate samples across timesteps.

### Continuous Control Benchmarks and Evaluation

As mentioned in Section 6.2, the evaluation metric in continuous control was the normalized IQM with 95% stratified bootstrap confidence intervals from Agarwal et al. (2021). Returns were normalized by a minimum value computed from the mean of 100 rollouts sampled from a uniform policy and the maximum possible return from the environment. When the maximum value was not specified, we used the maximum value of any single rollout attained by any of the baselines.

We benchmarked on four continuous control suites: DeepMind Control (Tassa et al., 2018; Tunyasuvunakool et al., 2020), MuJoCo (Todorov et al., 2012; Brockman et al., 2016), MetaWorld (Yu et al., 2020), and Box2D (Brockman et al., 2016). These environments were selected to be as extensive as possible. DeepMind Control and MetaWorld were chosen because of their diversity of tasks, while the MuJoCo and Box2D environments are popular benchmarks within the common interface of OpenAI Gym (Brockman et al., 2016), now Gymnasium (Farama Foundation, 2023). No citation exists for Gymnasium as of writing this paper, and we link to their GitHub repository https://github.com/Farama-Foundation/Gymnasium as suggested in https://github.com/Farama-Foundation/Gymnasium/issues/82.

Figure 6: Negative Log-Likelihoods (NLLs) of the noise in Deep \(Q\)-Learning under different distributions throughout training (lower is better). Mean calculated per-task \(\pm\) standard deviation. The legend for all graphs is in Figure 5(e).

Figure 7: The effect of changing pessimism factor \(c\) on the target \(Q\)-value in continuous control. IQM calculated per-task \(\pm\) standard deviation. The legend for all graphs is in Figure 7d.

**DeepMind Control (DMC)** was designed to benchmark continuous control, over a broad range of agent morphologies. We selected agent morphologies that could be trained from states with a broad range of action spaces from 1 (acrobot) to 38 (dog). We did not benchmark on humanoid_CMU as this environment was not intended to be solved with RL from scratch, unlike the other baselines. The hardest task was selected from each of the agent morphologies. Properties of the 11 DMC tasks are presented in Table 1a.

**MetaWorld** was designed to have a diverse range of tasks to evaluate the generalization ability of learned policies. Each environment within MetaWorld is therefore made up of multiple tasks, all with the same underlying structure of an MDP but with different numerical values of their parameters. We follow the method of Seyde et al. (2022) to benchmark on a single MetaWorld task by first selecting an environment and then randomly selecting a set of numerical parameters. Each new instantiation of a MetaWorld task would result in a different set of hyperparameters. As such, we expect the error bars in the aggregate statistics of MetaWorld to be substantially larger than the other environments. We benchmark on tasks formed from the union of the ML1, MT10, and ML10 train tasks that a policy in MetaWorld would be trained on, as well as the five environments benchmarked in Seyde et al. (2022). Properties of the 15 MetaWorld tasks are presented in Table 1c.

**MuJoCo** was evaluated on the same tasks as SAC (Haarnoja et al., 2018). These tasks were all locomotion-based. Properties of the 5 MuJoCo tasks are presented in Table 1b.

**Box2D** was evaluated on all continuous control tasks from states. Properties of the 2 Box2D tasks are presented in 1d.

### Discrete Control Baselines

Discrete control algorithms were implemented as described in Section 3.4. Hyperparameters used in discrete control algorithms are detailed in Tables 2 and 3. We provide explanations for these design choices as follows.

**DQN**: The original DQN algorithm in Mnih et al. (2015) was designed for pixel inputs. We modified DQN to use state inputs by using an architecture described in Section 3.4 we used in continuous control that was popular for use with state inputs. Conversely to the continuous control architecture, we found removing GroupNorm (Wu and He, 2018) was crucial to getting DQN to work. Similarly to the continuous control architecture, we found that changing the initialization and target network updating drastically improved performance. We also used the MSE and Adam (Kingma and Ba, 2014) optimizers as Ceron and Castro (2021) showed that this yielded improved performance over the Huber Loss (Huber, 1992) and RMSProp (Hinton et al., 2012) of the original DQN. Our implementation of DQN solves classic discrete control tasks that the CleanRL (Huang et al., 2022) reproduction of the original DQN paper at https://docs.cleanrl.dev/rl-algorithms/dqn/#experiment-results_1 could not solve.

**Dueling Double DQN (Dueling DDQN)** was a baseline modified from Hessel et al. (2018) designed to be as compatible with DoubleGum as possible. Rainbow evaluated six innovations to DQN: Double DQN (Van Hasselt et al., 2016), Dueling DQN (Wang et al., 2016), noisy networks (Fortunato et al., 2017), \(n\)-step returns, C51 distributional RL (Bellemare et al., 2017), and prioritized replay (Schaul et al., 2016). We only used the first two of these six innovations in DoubleGum. We did not find \(n\)-step returns effective in discrete domains we considered, nor prioritized replay. Distributional RL was incompatible with DoubleGum, while Schwarzer et al. (2023) did not find noisy networks advantageous.

DoubleDQN was implemented following Van Hasselt et al. (2016) by computing bootstrapped targets of \(Q_{\psi}^{\text{new}}(s,\max_{a}Q_{\theta}^{\text{new}}(s,a))\). Dueling DQN was implemented following Wang et al. (2016), with the advantage and value heads having two layers with a hidden layer of size 256 and ReLU activations. The stability of Dueling DQN was greatly improved by setting the biases of both dueling heads to 0.

**DoubleGum** was implemented as Dueling DDQN with an additional variance head described in Section 3.4.

\begin{table}

\end{table}
Table 1: Properties of Continuous Control Environments

\begin{table}
\begin{tabular}{l|c} \hline Hyperparameter & Value \\ \hline Starting Timesteps & 10,000 \\ Maximum Timesteps & 1,000,000 \\ Exploration Noise & 0.2 \\ Policy Noise in Critic Loss & 0.1 \\ Policy Noise in Actor Loss & 0.1 \\ Actor optimizer & Adam \\ Actor learning rate & 3e-4 \\ Critic optimizer & Adam \\ Critic learning rate & 3e-4 \\ Number of groups in Actor GroupNorm & 16 \\ Number of groups in Critic GroupNorm & 16 \\ Critic target networks EMA \(\eta_{\phi}\) & 5e-3 \\ Actor target networks EMA & 1 \\  & Linear(256), GroupNorm, ReLU \\ Critic structure & Linear(256), GroupNorm, ReLU \\  & Linear(256), GroupNorm, ReLU \\ Actor structure & Linear(256), GroupNorm, ReLU \\ \hline \end{tabular}
\end{table}
Table 4: Hyperparameters for Continuous Control

\begin{table}
\begin{tabular}{l|c} \hline Hyperparameter & Value \\ \hline Evaluation Episodes & 10 \\ Evaluation Frequency & Maximum Timesteps / 100 \\ Discount Factor \(\gamma\) & 0.99 \\ \(n\)-Step Returns & 1 step \\ Replay Ratio & 1 \\ Replay Buffer Size & 1,000,000 \\ Maximum Timesteps & 1,000,000 \\ \hline \end{tabular}
\end{table}
Table 2: Shared Hyperparameters of Benchmarked Algorithms

\begin{table}
\begin{tabular}{l|c} \hline Hyperparameter & Value \\ \hline Starting Timesteps & 10,000 \\ Maximum Timesteps & 1,000,000 \\ Exploration Noise & 0.2 \\ Policy Noise in Critic Loss & 0.1 \\ Policy Noise in Actor Loss & 0.1 \\ Actor optimizer & Adam \\ Actor learning rate & 3e-4 \\ Critic optimizer & Adam \\ Critic learning rate & 3e-4 \\ Number of groups in Actor GroupNorm & 16 \\ Number of groups in Critic GroupNorm & 16 \\ Critic target networks EMA \(\eta_{\phi}\) & 5e-3 \\ Actor target networks EMA & 1 \\  & Linear(256), GroupNorm, ReLU \\ Critic structure & Linear(256), GroupNorm, ReLU \\  & Linear(256), GroupNorm, ReLU \\ Actor structure & Linear(256), GroupNorm, ReLU \\ \hline \end{tabular}
\end{table}
Table 3: Hyperparameters for Discrete Control

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline \multirow{2}{*}{Algorithm} & \multicolumn{5}{c}{Pessimism Hyperparameter} \\ \cline{2-5}  & Default & DeepMind Control & MuJoCo & MetaWorld & Box2D \\ \hline
**DoubleGum (ours)** & \(-0.1\) & \(-0.1\) & \(-0.5\) & \(0.1\) & \(-0.1\) \\ DDPG/TD3 & Twin & Single & Twin & Single & Twin \\ SAC & Twin & Single & Twin & Single & Twin \\ XQL & Twin (\(\beta=5\)) & Single (3) & Single (5) & Twin (2) & Twin (5) \\ QR-DDPG & Single & Single & Twin & Single & Twin \\ FinerTD3 & 1 & 1 & 3 & 3 & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Pessimism Hyperparameters in Continuous Control

### Continuous Control Baselines

Continuous control algorithms were implemented as described in Section 3.4. Hyperparameters used in continuous control algorithms are detailed in Tables 2 and 4. Pessimism hyperparameters are presented in Table 5 and were found following results in Appendix F.2.

As mentioned, all implementations used networks with two hidden layers of width 256, with orthogonal initialization (Saxe et al., 2013) and GroupNorm (Wu and He, 2018). Following Kostrikov (2021), target network parameters were updated with an EMA of \(5e-3\) in the critic and \(0\) in the actor. All these design choices differ from their original implementations but improved aggregate performance. We provide explanations for these design choices as follows.

**DDPG** was introduced in Lillicrap et al. (2015) and Fujimoto et al. (2018) updated the design choices of DDPG to empirically improve its performance. In addition to the existing changes, our implementation uses the noise clipping scheme in the actor specified by Laskin et al. (2021).

**TD3** was implemented with three changes from Fujimoto et al. (2018). First, we update the actor once per critic update - ie using a delay of 1. This is such that the only hyperparameter change between our DDPG and TD3 is the use of Twin Networks. Secondly, we update the actor to maximize the mean of two critics rather than a single critic, a design choice we found empirically reduced variance between training runs. Thirdly, we do not compute the EMA of actor-network parameters. Removing this EMA improves sample efficiency but at the cost of higher variance.

**FinerTD3 (our introduced baseline)** was implemented with the same hyperparameters as TD3 but with an ensemble of 5 critic networks. We chose to use 5 networks because we tuned the pessimism factor hyperparameter of DoubleGum over 5 values. The 5 critics in FinerTD3 enable five values of pessimism to be used. Pessimism of FinerTD3 is adjusted in the bootstrapped targets. The 5 critic values are sorted by decreasing positivity, and the \(i^{\text{th}}\) smallest value is used as the target critic value in the bootstrapped targets.

**SAC** was implemented with hyperparameters from Kostrikov (2021), which we found improved performance. Kostrikov (2021) differs from Haarnoja et al. (2018b) in two additional ways from those mentioned. The standard deviation in the actor was clipped to \([-10,2]\), and the target entropy was the action dimension divided by 2 instead of just the action dimension.

**XQL**Garg et al. (2023) presents two off-policy algorithms: X-TD3 and X-SAC. We use X-TD3 to be consistent with the DDPG fixed-variance actor of DoubleGum and refer to it throughout as XQL. XQL tunes two hyperparameters per task: the use of twin networks/not and scalar hyperparameter \(\beta\). We swept over the same \(\beta\)-values as Garg et al. (2023): 1, 2, 5 without Twin Critics and 3, 4, 10 and 20 with Twin Critics. \(\beta\) was tuned in the same way as pessimism - we found a default \(\beta\) value and a \(\beta\) tuned per-suite. \(\beta\) values are presented in Table 5 and were found following results in Appendix F.2.

**MoG-DDPG** is formed by combining a Mixture-of-Gaussians (MoG) critic with DDPG. The MoG critic was introduced in Appendix A of Barth-Maron et al. (2018) and improved by Shahriari et al. (2022). The latter paper combines the MoG critic with DDPG with distributed training, but we remove the distributed training component because we do not use it in DoubleGum.

**QR-DDPG (our introduced baseline)** combines the quantile regression method of Dabney et al. (2018b) with a DDPG actor. Although Ma et al. (2020); Wurman et al. (2022) and Teng et al. (2022) have combined quantile regression with SAC, we combine it with DDPG because DoubleGum is built on top of DDPG. Like Dabney et al. (2018b), we use 201 quantiles, but these are initialized with orthogonal initialization and are optimized with the MSE, rather than the Huber loss. QR was developed for discrete control and uses the Huber loss with the RMSProp optimizer popular in discrete control methods. We found better performance with the MSE and Adam optimizer, perhaps confirming the result of Ceron and Castro (2021) in distributional RL for continuous control.

**DoubleGum** was implemented as DDPG with a variance head described in Section 3.4.

### Compute Requirements

A single training run for discrete control may take up to 3 to 5 minutes on a laptop with an Intel Core i9 CPU, NVIDIA 1050 GPU and 31.0 GiB of RAM. On the same system, a single training run for continuous control takes 1 - 2 hours.

[MISSING_PAGE_FAIL:30]

Figure 11: Adjusting pessimism of baseline algorithms with the use of Twin Networks/not, per-suite IQM normalized score with 95% stratified bootstrap CIs. Methods that default to use Twin Networks are dashed.

Figure 8: Adjusting the pessimism factor \(c\) in DoubleGum, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs.

Figure 12: Adjusting pessimism of XQL, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs. Methods that use Twin Networks are dashed.

Figure 10: Adjusting pessimism of baseline algorithms with the use of Twin Networks/not, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs. Methods that default to use Twin Networks are dashed.

Figure 9: Adjusting pessimism in DoubleGum, per-suite IQM with 95% stratified bootstrap CIs.

Figure 16: Continuous control with default parameters, per-suite IQM normalized score with 95% stratified bootstrap CIs. Methods that default to use Twin Networks are dashed.

Figure 14: Adjusting pessimism of FinerTD3, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs.

Figure 13: Adjusting pessimism of XQL, per-suite IQM normalized score with 95% stratified bootstrap CIs. Methods that use Twin Networks are dashed.

Figure 15: Adjusting pessimism of FinerTD3, per-suite IQM normalized score with 95% stratified bootstrap CIs.

[MISSING_PAGE_FAIL:33]

[MISSING_PAGE_EMPTY:34]

Figure 18: Continuous control with default pessimism hyperparameters, per-task IQM \(\pm\) standard deviation. Methods that default to use Twin Networks are dashed. The legend for all graphs is in Figure 18d.

[MISSING_PAGE_EMPTY:36]

Figure 19: Continuous control with the best pessimism hyperparameters adjusted per suite, per-task IQM \(\pm\) standard deviation. The legend for all graphs is in Figure 19.