# Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis

 Dachao Lin

Academy for Advanced Interdisciplinary Studies; Peking University; lindachao@pku.edu.cn;

Yuze Han

School of Mathematical Sciences; Peking University; hanyuze97@pku.edu.cn;

Haishan Ye

Corresponding Author; School of Management; Xi'an Jiaotong University; SGIT AI Lab, State Grid Corporation of China; yehaishan@xjtu.edu.cn;

Zhihua Zhang

School of Mathematical Sciences; Peking University; zhzhang@math.pku.edu.cn.

###### Abstract

We study finite-sum distributed optimization problems involving a master node and \(n-1\) local nodes under the popular \(\delta\)-similarity and \(\mu\)-strong convexity conditions. We propose two new algorithms, SVRS and AccSVRS, motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient sliding and variance reduction and achieves a better communication complexity of \(\tilde{\mathcal{O}}(n{+}\sqrt{n}\delta/\mu)\) compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X [6], we also develop a directly accelerated version named AccSVRS with the \(\tilde{\mathcal{O}}(n{+}n^{3/4}\sqrt{\delta/\mu})\) communication complexity. In contrast to existing results, our complexity bounds are entirely smoothness-free and exhibit superiority in ill-conditioned cases. Furthermore, we establish a nearly matched lower bound to verify the tightness of our AccSVRS method.

## 1 Introduction

We have witnessed the development of distributed optimization in recent years. Distributed optimization aims to cooperatively solve a learning task over a predefined social network by exchanging information exclusively with immediate neighbors. This class of problems has found extensive applications in various fields, including machine learning, healthcare, network information processing, telecommunications, manufacturing, natural language processing tasks, and multi-agent control [54, 30, 48, 45, 60, 8]. In this paper, we focus on the following classical finite-sum optimization problem in a centralized setting:

\[\min_{\bm{x}\in\mathbb{R}^{d}}f(\bm{x}):=\frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{x }),\] (1)

where each \(f_{i}\) is differentiable and corresponds to a client or node, and the target objective is their average function \(f\). Without loss of generality, we assume \(f_{1}\) is the master node and the others are local nodes. In each round, every local node can communicate with the master node certain information, such as the local parameter \(\bm{x}\), local gradient \(\nabla f_{i}(\bm{x})\), and some global information gathered at the master node. Such a scheme can also be viewed as decentralized optimization over a star network [55].

Following the wisdom of statistical similarity residing in the data at different nodes, many previous works study scenarios where the individual functions exhibit relationships or, more specifically, certain homogeneity shared among the local \(f_{i}\)'s and \(f\). The most common one is under the \(\delta\)-second-ordersimilarity assumption [33; 35], that is,

\[\left\|\nabla^{2}f_{i}(\bm{x})-\nabla^{2}f(\bm{x})\right\|\leq\delta,\forall\bm{x }\in\mathbb{R}^{d},i\in\{1,\ldots,n\}.\]

Such an assumption also has different names in the literature, such as \(\delta\)-related assumption, bounded Hessian dissimilarity, or function similarity [7; 32; 54; 62; 51]. The rigorous definitions are deferred to Section 2. Moreover, the second-order similarity assumption can hold with a relatively small \(\delta\) compared to the smoothness coefficient of \(f_{i}\)'s in many practical settings, such as statistical learning. More insights on this can be found in the discussion presented in [54; Section 2]. The similarity assumption indicates that the data across different clients share common information on the second-order derivative, potentially leading to a reduction in communication among clients. Meanwhile, the cost of communication is often much higher than that of local computation in distributed optimization settings [9; 44; 30]. Hence, researchers are motivated to develop efficient algorithms characterized by low communication complexity, which is the primary objective of this paper as well.

Furthermore, we need to emphasize that prior research [25; 56; 63; 19; 43; 5] has shown tightly matched lower and upper bounds on computation complexity for the finite-sum objective in Eq. (1). These works focus on gradient complexity under (average) smoothness [63] instead of communication complexity under similarity. Indeed, we will also discuss and compare the gradient complexity as shown in [35], to explore the trade-off between communication and gradient complexity.

Although the development of distributed optimization with similarity has lasted for years, the optimal complexity under full participation was only recently achieved by Kovalev et al. [35]. They employed gradient-sliding [37] and obtained the optimal communication complexity \(\tilde{\mathcal{O}}(n\sqrt{\delta/\mu})\) for \(\mu\)-strongly convex \(f\) and \(\delta\)-related \(f_{i}\)'s in Eq. (1). However, the full participation model requires the calculation of the whole gradient \(\nabla f(\cdot)\), which incurs a communication cost of \(n{-}1\) in each round. In contrast, partial participation could reduce the communication burden and yield improved complexity. Hence, Khaled and Jin [33] introduced client sampling, a technique that selects one client for updating in each round. They developed a non-accelerated algorithm SVRP, which achieves the communication complexity of \(\tilde{\mathcal{O}}(n{+}\delta^{2}/\mu^{2})\). Additionally, they proposed a Catalyzed version of SVRP with the complexity \(\tilde{\mathcal{O}}(n{+}n^{3/4}\sqrt{\delta/\mu})\), which is better than the rates obtained in the full participation setting.

We believe there are several potential avenues for improvement inspired by [33]. 1) Khaled and Jin [33] introduced the requirement that each individual function is strongly convex (see [33, Assumption 2]). However, this constraint is absent in prior works. Notably, in the context of full participation, even non-convexity is deemed acceptable6. A prominent example is the shift-and-invert approach to solving PCA [52; 23], where each component is smooth and non-convex, but the average function remains convex. Thus we doubt the necessity of requiring strong convexity for individual components. 2) In hindsight, it seems that the directly accelerated SVRP could only achieve a bound of \(\tilde{\mathcal{O}}(n+\sqrt{n}\cdot\delta/\mu)\) based on the current analysis, which is far from being satisfactory compared to its Catalyzed version. Consequently, there might be room for the development of a more effective algorithm for direct acceleration. 3) It is essential to note that the Catalyst framework introduces an additional log term in the overall complexity, along with the challenge of parameter tuning. This aspect is discussed in detail in [6, Section 1.2]. Therefore, we intend to address the aforementioned concerns, particularly on designing directly accelerated methods under the second-order similarity assumption.

Footnote 6: Readers can check that the proof of [35] only requires \(f_{1}(\cdot)+\frac{1}{2\theta}\left\|\cdot\right\|^{2}\) is strongly convex, which can be guaranteed by \(\delta\)-second-order similarity since \(f\) is \(\mu\)-strongly convex and \(\theta=1/(2\delta)\) therein.

### Main Contributions

In this paper, we address the above concerns under the average similarity condition. Our contributions are presented in detail below and we provide a comparison with previous works in Table 1:

* First, we combine gradient sliding and client sampling techniques to develop an improved non-accelerated algorithm named SVRS (Algorithms 1). SVRS achieves a communication complexity of \(\tilde{\mathcal{O}}(n+\sqrt{n}\cdot\delta/\mu)\), surpassing SVRP in ill-conditioned cases. Notably, this rate does not need component strong convexity and applies to the function value gap instead of the parameter distance.
* Second, building on SVRS, we employ a classical interpolation framework motivated by Katyusha X [6] to introduce the directly accelerated SVRS (AccSVRS, Algorithm 2).

AccSVRS achieves the same communication bound of \(\tilde{\mathcal{O}}(n+n^{3/4}\sqrt{\delta/\mu})\) as Catalyzed SVRP. Specifically, our bound is entirely smoothness-free and slightly outperforms Catalyzed SVRP, featuring a log improvement and not requiring component strong convexity.
* Third, by considering the proximal incremental first-order oracle in the centralized distributed framework, we establish a lower bound, which nearly matches the upper bound of AccSVRS in ill-conditioned cases.

### Related Work

Gradient sliding/Oracle Complexity Separation.For optimization problems with a separated structure or multiple building blocks, such as Eq. (1), there are scenarios where computing the gradients/values of some parts (or the whole) is more expensive than the others (or a partial one). In response to this challenge, techniques such as the gradient-sliding method [37] and the concept of oracle complexity separation [28] have emerged. These methods advocate for the infrequent use of more expensive oracles compared to their less resource-intensive counterparts. This strategy has found applications in zero-order [12; 21; 28; 53], first-order [37; 38; 39; 31] and high-order methods [31; 24; 3], as well as in addressing saddle point problems [4; 13]. Our algorithms can be viewed as a variance-reduced version of gradient sliding tailored to leverage the similarity assumption.

Distributed optimization under similarity.Distributed optimization has a long history with a plethora of existing works and surveys. To streamline our discussion, we only list the most relevant references, particularly under the similarity and strong convexity assumptions. In the full participation setting, which involves deterministic methods, the first algorithm credits to DANE [51], though its analysis is limited to quadratic objectives. Subsequently, AIDE [50], DANE-LS and DANE-HB [58] improved the rates for quadratic objective; Disco [62] SPAG [27], ACN [1] and DiRegINA [18] improved the rates for self-concordant objectives. As for general strongly convex objectives, Sun et al. [54] introduced the SONATA algorithm, and Tian et al. [55] proposed accelerated SONATA. However, their complexity bounds include additional log factors. These factors have recently been removed by Accelerated Extragradient [35], whose complexity bound perfectly matches the lower bound in [7]. We highly recommend the comparison of rates in [35, Table 1] for a comprehensive overview. Once the discussion of deterministic methods is concluded, Khaled and Jin [33] shifted their focus to stochastic methods using client sampling. They proposed SVRP and its Catalyzed version, both of which exhibited superior rates compared to deterministic methods.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & **Method/Reference** & **Communication complexity** & **Assumptions** \\ \hline \multirow{2}{*}{No Sampling} & AccExtragradient [35] & \(\mathcal{O}\left(n\sqrt{\frac{\delta}{\mu}}\log\frac{1}{\varepsilon}\right)\) & SS only for \(f_{1}\) \\ \cline{2-4}  & Lower bound [7] & \(\Omega\left(n\sqrt{\frac{\delta}{\mu}}\log\frac{1}{\varepsilon}\right)\) & SS for \(f_{i}\)’s \\ \hline \multirow{4}{*}{Client Sampling} & SVRP [33] & \(\mathcal{O}\left(\left(n+\frac{\delta^{2}}{\mu^{2}}\right)\log\frac{1}{ \varepsilon}\right)^{(1)}\) & SC for \(f_{i}\)’s, AveSS \\ \cline{2-4}  & Catalyzed SVRP [33] & \(\mathcal{O}\left(\left(n+n^{3/4}\sqrt{\frac{\delta}{\mu}}\right)\log\frac{1}{ \varepsilon}\right)^{(2)}\) & SC for \(f_{i}\)’s, AveSS \\ \cline{2-4}  & SVRS (Thm 3.3) & \(\mathcal{O}\left(\left(n+\sqrt{n}\cdot\frac{\delta}{\mu}\right)\log\frac{1}{ \varepsilon}\right)\) & AveSS \\ \cline{2-4}  & AccSVRS (Thm 3.6) & \(\mathcal{O}\left(\left(n+n^{3/4}\sqrt{\frac{\delta}{\mu}}\right)\log\frac{1}{ \varepsilon}\right)\) & AveSS \\ \cline{2-4}  & Lower bound (Thm 4.4) & \(\Omega\left(n+n^{3/4}\sqrt{\frac{\delta}{\mu}}\log\frac{1}{\varepsilon}\right)^{( 3)}\) & AveSS \\ \hline \end{tabular}

* The rate only applies to \(\mathbb{E}\left\|\bm{x}_{h}-\bm{x}_{\mu}\right\|^{2}\), otherwise it would introduce \(L\) in the log term; [2] The term \(\log(L/\mu)\) comes from the Catalyst framework. See Appendix C for the detail. [2, 3] Here we only list the rates of the common ill-conditioned case: \(\mu=\mathcal{O}(\delta/\sqrt{n})\). See Appendices for the remaining case. _Notation:_\(\delta\)insimilarity parameter (both for SS and AveSS), \(L\)=smoothness constant of \(f\), \(\mu\)=strong convexity constant of \(f\)(or \(f_{i}\)’s), \(\varepsilon\)=error of the solution for \(\mathbb{E}f(\bm{x}_{h})-f(\bm{x}_{\star})\). Here \(L\geq\delta\geq\mu\gg\epsilon>0\). _Abbreviation:_ SC=strong convexity, SS=second-order similarity, AveSS=average SS.

\end{table}
Table 1: Comparison of communication under similarity for the strongly convex objective.

## 2 Preliminaries

Notation.We denote vectors by lowercase bold letters (e.g., \(\bm{w},\bm{x}\)), and matrices by capital bold letters (e.g., \(\bm{A},\bm{B}\)). We let \(\|\cdot\|\) be the \(\ell_{2}\)-norm for vectors, or induced \(\ell_{2}\)-norm for a given matrix: \(\|\bm{A}\|=\sup_{\bm{u}\neq 0}\left\|\bm{A}\bm{u}\right\|/\left\|\bm{u}\right\|\). We abbreviate \([n]=\{1,\dots,n\}\) and \(\bm{I}_{d}\in\mathbb{R}^{d\times d}\) is the identity matrix. We use \(\bm{0}\) for the all-zero vector/matrix, whose size will be specified by a subscript, if necessary, and otherwise is clear from the context. We denote \(\mathrm{Unif}(\mathcal{S})\) as the uniform distribution over set \(\mathcal{S}\). We say \(T\sim\mathrm{Geom}(p)\) for \(p\in(0,1]\) if \(\mathbb{P}(T=k)=(1-p)^{k-1}p,\forall k\in\{1,2,\dots\}\), i.e., \(T\) obeys a geometric distribution. We adopt \(\mathbb{E}_{k}\) as the expectation for all randomness appeared in step \(k\), and \(1_{A}\) as the indicator function on event \(A\), i.e., \(1_{A}=1\) if event \(A\) holds, and \(0\) otherwise. We use \(\mathcal{O}(\cdot),\Omega(\cdot),\Theta(\cdot)\) and \(\tilde{\mathcal{O}}(\cdot)\) notation to hide universal constants and log-factors. We define the Bregman divergence induced by a differentiable (convex) function \(h\colon\mathbb{R}^{d}\to\mathbb{R}\) as \(D_{h}(\bm{x},\bm{y}):=h(\bm{x})-h(\bm{y})-\langle\nabla h(\bm{y}),\bm{x}-\bm{ y}\rangle\).

Definitions.We present the following common definitions used in this paper.

**Definition 2.1**: _A differentiable function \(g\colon\mathbb{R}^{d}\to\mathbb{R}\) is \(\mu\)-strongly convex (SC) if_

\[g(\bm{y})\geq g(\bm{x})+\langle\nabla g(\bm{x}),\bm{y}-\bm{x}\rangle+\frac{ \mu}{2}\left\|\bm{y}-\bm{x}\right\|^{2},\forall\bm{x},\bm{y}\in\mathbb{R}^{d}.\] (2)

_Particularly, if \(\mu=0\), we say that \(g\) is convex._

**Definition 2.2**: _A differentiable function \(g\colon\mathbb{R}^{d}\to\mathbb{R}\) is L-smooth if_

\[g(\bm{y})\leq g(\bm{x})+\langle\nabla g(\bm{x}),\bm{y}-\bm{x}\rangle+\frac{L} {2}\left\|\bm{y}-\bm{x}\right\|^{2},\forall\bm{x},\bm{y}\in\mathbb{R}^{d}.\] (3)

There are many basic inequalities involving strong convexity and smoothness, see [22, Appendix A.1] for an introduction. Next, we present the definition of second-order similarity in distributed optimization.

**Definition 2.3**: _The differentiable functions \(f_{i}\)'s satisfy \(\delta\)-average second-order similarity (AveSS) if the following inequality holds for \(f_{i}\)'s and \(f=\frac{1}{n}\sum_{i=1}^{n}f_{i}\):_

\[\text{(AveSS)}\quad\frac{1}{n}\sum_{i=1}^{n}\left\|[\nabla[f_{i}-f](\bm{x})- \nabla[f_{i}-f](\bm{y})]\right\|^{2}\leq\delta^{2}\left\|\bm{x}-\bm{y}\right\| ^{2},\forall\bm{x},\bm{y}\in\mathbb{R}^{d}.\] (4)

**Definition 2.4**: _The differentiable functions \(f_{i}\)'s satisfy \(\delta\)-component second-order similarity (SS) if the following inequality holds for \(f_{i}\)'s and \(f=\frac{1}{n}\sum_{i=1}^{n}f_{i}\):_

\[\text{(SS)}\quad\left\|[\nabla[f_{i}-f](\bm{x})-\nabla[f_{i}-f](\bm{y})] \right\|^{2}\leq\delta^{2}\left\|\bm{x}-\bm{y}\right\|^{2},\forall\bm{x},\bm{ y}\in\mathbb{R}^{d},i\in[n].\] (5)

Definitions 2.3 and 2.4 first appear in [33], which is an analogy to (average) smoothness in prior literature [63]. Particularly, \(f_{i}\)'s satisfy \(\delta\)-AveSS implies that \((f\!-\!f_{i})\)'s satisfy \(\delta\)-average smoothness, while \(f_{i}\)'s satisfy \(\delta\)-SS implies that \((f\!-\!f_{i})\)'s satisfy \(\delta\)-smoothness. Additionally, many researchers [32, 7, 51, 62, 54, 35] use the equivalent one defined by Hessian similarity (HS) if assuming that \(f_{i}\)'s are twice differentiable. Thus we also list them below and leave the derivation in Appendix B.

\[\text{(AveHS)}\ \left\|\frac{1}{n}\sum_{i=1}^{n}\left[\nabla^{2}f_{i}(\bm{x})- \nabla^{2}f(\bm{x})\right]^{2}\right\|\leq\delta^{2};\text{(HS)}\ \left\|\nabla^{2}f_{i}(\bm{x})-\nabla^{2}f(\bm{x})\right\|\leq\delta,\forall i \in[n].\] (6)

Since our algorithm is a first-order method, we adopt the gradient description of similarity (Definitions 2.3 and 2.4) without assuming twice differentiability for brevity.

As mentioned in [7, 54], if \(f_{i}\)'s satisfy \(\delta\)-AveSS (or SS), and \(f\) is \(\mu\)-strongly convex and \(L\)-smooth, then generally \(L\gg\delta\gg\mu>0\) for large datasets in practice. Therefore, researchers aim to develop algorithms that achieve communication complexity solely related to \(\delta,\mu\) (or log terms of \(L\)). This is also our objective. To finish this section, we will clarify several straightforward yet essential propositions, and the proofs are deferred to Appendix A.

**Proposition 2.5**: _We have the following properties among SS, AveSS, and SC: 1) \(\delta\)-SS implies \(\delta\)-AveSS, but \(\delta\)-AveSS only implies \(\sqrt{n}\delta\)-SS. 2) If \(f_{i}\)'s satisfy \(\delta\)-SS and \(f\) is \(\mu\)-strongly convex, then for all \(i\in[n],f_{i}(\cdot)+\frac{\delta-\mu}{2}\left\|\cdot\right\|^{2}\) is convex, i.e., \(f_{i}\) is \((\delta-\mu)\)-almost convex [14]._Algorithm and Theory

In this section, we introduce our main algorithms, which are developed to solve the distributed optimization problem in Eq. (1) under Assumption 1 below:

**Assumption 1**: _We assume that \(f_{i}\)'s satisfy \(\delta\)-AveSS, and \(f\) is \(\mu\)-strongly convex with \(\delta\geq\mu>0\)._

Assumption 1 does not need each \(f_{i}\) to be \(\mu\)-strongly convex. In fact, it is acceptable that \(f_{i}\)'s are non-convex, since by Proposition 2.5, \(f_{i}\)'s are \((\sqrt{n}\delta-\mu)\)-almost convex [14]. In the following, we first propose our new algorithm SVRS, which combines the techniques of gradient sliding and variance reduction, resulting in improved rates. Then we establish the directly accelerated method motivated by [6].

### No Acceleration Version: SVRS

We first show the one-epoch Stochastic Variance-Reduced Sliding (\(\mathrm{SVRS}^{1\mathrm{ep}}\)) method in Algorithm 1. Before delving into the theoretical analysis, we present some key insights into our method. These insights aim to enhance comprehension and facilitate connections with other algorithms.

Variance Reduction.Our algorithm can be viewed as adding variance reduction from [35]. Besides the acceleration step, the main difference lies in the proximal step, where Kovalev et al. [35] solved:

\[\bm{x}_{t+1}\approx\operatorname*{arg\,min}_{\bm{x}\in\mathbb{R}^{d}}B_{ \theta}^{t}(\bm{x}):=\langle\nabla f(\bm{x}_{t})-\nabla f_{1}(\bm{x}_{t}),\bm {x}-\bm{x}_{t}\rangle+\frac{1}{2\theta}\left\|\bm{x}-\bm{x}_{t}\right\|^{2}+f _{1}(\bm{x}).\]

To save the heavy communication burden of calculating \(\nabla f(\bm{x}_{t})\), we apply client sampling by selecting a random \(\nabla f_{i_{t}}(\bm{x}_{t})\) in the \(t\)-th step. However, this substitution introduces significant noise. To mitigate this, we incorporate a correction term \(\bm{g}_{t}=\nabla f_{i_{t}}(\bm{w}_{0})-\nabla f(\bm{w}_{0})\) from previous wisdom [29] to reduce the variance.

Gradient sliding.Our algorithm can be viewed as adding gradient sliding from SVRP [33]. The main difference also lies in the proximal point problem, where Khaled and Jin [33] solved:

\[\bm{x}_{t+1}\approx\operatorname*{arg\,min}_{\bm{x}\in\mathbb{R}^{d}}C_{ \theta}^{t}(\bm{x}):=\langle-\bm{g}_{t},\bm{x}-\bm{x}_{t}\rangle+\frac{1}{2 \theta}\left\|\bm{x}-\bm{x}_{t}\right\|^{2}+f_{i_{t}}(\bm{x}).\]

Here we adopt a fixed proximal function \(f_{1}\) instead of \(f_{i_{t}}\), which can be viewed as approximating \(f_{i_{t}}(\bm{x}){\approx}f_{1}(\bm{x})+[f_{i_{t}}-f_{1}](\bm{x}_{t})+\langle \nabla[f_{i_{t}}-f_{1}](\bm{x}_{t}),\bm{x}-\bm{x}_{t}\rangle+\frac{1}{2\theta ^{\prime}}\left\|\bm{x}-\bm{x}_{t}\right\|^{2}\) with a properly chosen \(\theta^{\prime}>0\). Such a modification is motivated by [35], where they reformulated the objective as \(f(\bm{x})=[f(\bm{x})-f_{1}(\bm{x})]+f_{1}(\bm{x})\). Thus they could employ gradient sliding to skip heavy computations of \(\nabla[f-f_{1}](\bm{x})\) by utilizing the easy computations of \(\nabla f_{1}(\bm{x})\) more times. Fixing the proximal function \(f_{1}\) leads to the same metric space owned by \(f_{1}\) in each step, which could benefit the analysis and alleviate the requirements on \(f_{i}\)'s compared to SVRP. Indeed, in our setting \(f_{1}\) can be replaced by any other **fixed** client \(f_{b},b\in[n]\). In this case, the master node would be \(f_{b}\) instead of \(f_{1}\).

Bregman-SVRG.Our algorithm can be viewed as the classical Bregman-SVRG [20] with the reference function \(f_{1}(\cdot)+\frac{1}{2\theta}\left\|\cdot\right\|^{2}\) after introducing the Bregman divergence:

\[\bm{x}_{t+1}\approx\operatorname*{arg\,min}_{\bm{x}\in\mathbb{R}^{d}}A_{ \theta}^{t}(\bm{x})\overset{(\ref{eq:Bregman-SVRG})}{=}\operatorname*{arg\, min}_{\bm{x}\in\mathbb{R}^{d}}\left\langle\nabla f_{i_{t}}(\bm{x}_{t})-\nabla[f_{i_{t}}-f ](\bm{w}_{0}),\bm{x}-\bm{x}_{t}\right\rangle+D_{f_{1}(\cdot)+\frac{1}{2\theta} \left\|\cdot\right\|^{2}}(\bm{x},\bm{x}_{t}).\]

We need to emphasize that the proof of Bregman-SVRG requires additional structural assumptions [20, Assumption 3], which is not directly applicable in our setting. Hence, the rigorous proof of Bregman-SVRG under our similarity assumption is still meaningful as far as we are concerned.

#### 3.1.1 Communication Complexity under Distributed Settings

When applied to the distributed system, the communication complexity of \(\mathrm{SVRS}^{1\mathrm{ep}}\)can be described as follows: At the beginning of each epoch, the master (corresponding to \(f_{1}\)) sends \(\bm{w}_{0}\) to all clients. Each client computes \(\nabla f_{i}(\bm{w}_{0})\) from its local data and sends it back to the master. The master then builds \(\nabla f(\bm{w}_{0})\) after collecting all \(\nabla f_{i}(\bm{w}_{0})\)'s. The communication complexity is \(2(n-1)\) in this case. Next, the algorithm enters into the loop iterations. In each iteration, the master only sends current \(\bm{x}_{t}\) to the chosen client \(i_{t}\). The \(i_{t}\)-th client computes \(\nabla f_{i_{t}}(\bm{x}_{t})\) and sends it to the master (the first client). Then the master solves (inexactly) the local problem (Line 5 in Algorithm 1) to get an inexact solution \(\bm{x}_{t+1}\). The communication complexity is \(2\) in this case. Thus, the total communication complexity of \(\mathrm{SVRS}^{\mathrm{1ep}}\) is \(2(n-1)+2T\). Note that \(\mathbb{E}T=1/p\) and generally \(p=1/n\). We obtain that one epoch communication complexity is \(4n-2\) in expectation.

We would like to emphasize that our setup differs from that in [41, 46], where the authors assume the nodes can perform calculations and transmit vectors in parallel. We recognize the significance of both setups. However, there are situations where communication is more expensive than computation. For instance, in a business network or communication network the communication between any two nodes can result in charges and the risk of information leakage. To mitigate these costs, we should reduce the frequency of communication. Thus, we focus on the nonparallel setting.

#### 3.1.2 Convergence Analysis of SVRS

Based on the one-epoch method \(\mathrm{SVRS}^{\mathrm{1ep}}\), we could introduce our non-accelerated algorithm SVRS, which starts from \(\bm{w}_{0}\in\mathbb{R}^{d}\) and repeatedly performs the update7

Footnote 7: See Algorithm 3 in Appendix D for the details.

\[\bm{w}_{k+1}=\mathrm{SVRS}^{\mathrm{1ep}}(f,\bm{w}_{k},\theta,p),\;\forall k \geq 0.\]

Now we derive the convergence rate of SVRS8. The main technique we apply is replacing the Euclidean distance with the Bergman divergence. Denote the reference function

Footnote 8: Similar results for the popular loopless version [34] can also be derived, see Appendix D.5 for the detail.

\[h(\bm{x}):=f_{1}(\bm{x})+\frac{1}{2\theta}\left\|\bm{x}\right\|^{2}-f(\bm{x}).\] (8)

By Assumption 1 and 1) in Proposition 2.5, we see that \(f_{i}\)'s are \(\sqrt{n}\delta\)-SS. i.e., \([f_{1}-f](\cdot)\) is \((\sqrt{n}\delta)\)-smooth. Thus, \(h(\cdot)\) is \((\frac{1}{\theta}-\sqrt{n}\delta)\)-strongly convex and \((\frac{1}{\theta}+\sqrt{n}\delta)\)-smooth if \(\theta<\frac{1}{\sqrt{n}\theta}\), that is,

\[0\leq\frac{1-\sqrt{n}\theta\delta}{2\theta}\left\|\bm{x}-\bm{y}\right\|^{2} \stackrel{{\eqref{eq:h_{i}}}}{{\leq}}D_{h}(\bm{x},\bm{y}) \stackrel{{\eqref{eq:h_{i}}}}{{\leq}}\frac{1+\sqrt{n}\theta \delta}{2\theta}\left\|\bm{x}-\bm{y}\right\|^{2}.\] (9)

Hence, if \(\sqrt{n}\theta\delta=\Theta(1)\), \(h(\cdot)\) is nearly a rescaled Euclidean norm since its condition number related to \(\left\|\cdot\right\|\) is \(\frac{1+\sqrt{n}\theta\delta}{1-\sqrt{n}\theta\delta}=\Theta(1)\). Next, we employ the properties of the Bregman divergence \(D_{h}(\cdot,\cdot)\) to build the one-epoch progress of \(\mathrm{SVRS}^{\mathrm{1ep}}\)as shown below:

**Lemma 3.1**: _Suppose Assumption 1 holds. Let \(\bm{w}^{+}=\mathrm{SVRS}^{\mathrm{1ep}}(f,\bm{w}_{0},\theta,p)\) with \(\theta=1/(4\sqrt{n}\delta)\), and the approximated solution \(\bm{x}_{t+1}\) satisfies_

\[\left\|\nabla A^{t}_{\theta}(\bm{x}_{t+1})\right\|^{2}\leq\frac{\mu}{20\theta} \left\|\bm{x}_{t}-\operatorname*{arg\,min}_{\bm{x}\in\mathbb{R}^{d}}A^{t}_{ \theta}(\bm{x})\right\|^{2},\forall t\geq 0.\] (10)_Then for all \(\bm{x}\in\mathbb{R}^{d}\) that is independent of the indices \(i_{1},i_{2},\ldots,i_{T}\) in \(\mathrm{SVRS}^{1\mathrm{ep}}(f,\bm{w}_{0},\theta,p)\), we have_

\[\mathbb{E}f(\bm{w}^{+})-f(\bm{x})\leq\mathbb{E}\,p\langle\bm{x}-\bm{w}_{0}, \nabla h(\bm{w}^{+})-\nabla h(\bm{w}_{0})\rangle-\big{(}p-\frac{2}{9n}\big{)}D_ {h}(\bm{w}_{0},\bm{w}^{+})-\frac{2\mu\theta}{5}D_{h}(\bm{x},\bm{w}^{+}).\] (11)

**Remark 3.2**: _We note that some papers [10; 11] assume the smoothness and convexity of component functions, and adopt local updates for solving the proximal step. However, we replace these assumptions with a proximal approximately solvable assumption (10), which could even cover some nonsmooth and non-convex but proximal trackable component functions. We regard our assumption as more essential since the local updates can be viewed as partially solving this proximal step._

The proof of Lemma 3.1 is left in Appendix D.1. From Lemma 3.1, we find a well-behaved proximal operator is sufficient to ensure favorable progress. Finally, we establish the convergence rate and communication complexity of the SVRS method, and the proof is deferred to Appendix D.2.

**Theorem 3.3**: _Suppose Assumption 1 holds. If in \(\mathrm{SVRS}^{1\mathrm{ep}}\)(Algorithm 1), the hyperparameters are set as \(\theta=1/(4\sqrt{n}\delta),p=1/n\), and the approximate solution \(\bm{x}_{t+1}\) in each proximal step satisfies Eq. (10). Then for any error \(\varepsilon>0\), when_

\[k\geq K_{1}:=\max\left\{2,\frac{5\delta}{\mu\sqrt{n}}\right\}\log\frac{3 \left(1+\frac{\delta}{\mu\sqrt{n}}\right)\left[f(\bm{w}_{0})-f(\bm{x}_{*}) \right]}{\varepsilon},\]

_i.e., after \(\tilde{\mathcal{O}}(n+\sqrt{n}\delta/\mu)\) communications in expectation, we obtain that \(\mathbb{E}f(\bm{w}_{k})-f(\bm{x}_{*})\leq\varepsilon\)._

**Remark 3.4**: _Our results enjoy the following advantages over SVRP [33]: The convergence of SVRP ([33, Theorem 2]) only applied to \(\mathbb{E}\left\|\bm{w}_{k}-\bm{x}_{*}\right\|^{2}\), which can also be derived by our results from strong convexity: \(f(\bm{w}_{k})-f(\bm{x}_{*})\geq\frac{\mu}{2}\left\|\bm{w}_{k}-\bm{x}_{*} \right\|^{2}\). However, the reverse is not applicable since we do not assume the smoothness of \(f\), or indeed the smoothness coefficient is very large. Moreover for ill-conditioned problems (e.g., \(\delta/\mu\gg\sqrt{n}\)), our step size \(1/(4\sqrt{n}\delta)\) is much larger than \(\mu/(2\delta^{2})\) used in SVRP, and the convergence rate is also faster than SVRP: \(\tilde{\mathcal{O}}\left(n+\sqrt{n}\delta/\mu\right)\) vs. \(\tilde{\mathcal{O}}\left(n+\delta^{2}/\mu^{2}\right)\). Finally, we do not need the strong convexity assumption of component functions._

### Acceleration Version: AccSVRS

Now we apply the classical interpolation technique motivated by Katyusha X [6] to establish accelerated SVRS (AccSVRS, Algorithm 2). The main difference between AccSVRS and Katyusha X is due to the different choices of distance spaces. Specifically, we adopt \(D_{h}(\cdot,\cdot)\) instead of the Euclidean distance used in Katyusha X. Thus, the gradient mapping step (corresponding to Step 2 in [6, (4.1)]) should be built on the reference function \(h(\cdot)\) defined in Eq. (8), i.e., \(\nabla h(\bm{x}_{k+1})-\nabla h(\bm{y}_{k+1})\) instead of \((\bm{x}_{k+1}-\bm{y}_{k+1})/\theta\). Moreover, noting that \(\nabla h(\cdot)\) could involve the heavy gradient computing part \(\nabla f(\cdot)\), we further employ its stochastic version (Step 5 in Algorithm 2) to reduce the overall communication complexity.

Next, we delve into the convergence analysis. We first give the core lemma for AccSVRS, which is also motivated by the framework of Katyusha X [6]. The proof is deferred to Appendix D.3.

**Lemma 3.5**: _Suppose Assumption 1 holds, and \(\theta=1/(4\sqrt{n\delta}),p=1/n,\alpha\leq n\theta/(2\tau)\) in Algorithm 2, where \(\mathrm{SVRS}^{\mathrm{1ep}}(f,\bm{x}_{k+1},\theta,p)\) satisfies Eq. (10) in each iteration. Then for all \(\bm{x}\in\mathbb{R}^{d}\) that is independent of the random indices \(i_{1}^{(k)},i_{2}^{(k)},\ldots,i_{T}^{(k)}\) in \(\mathrm{SVRS}^{\mathrm{1ep}}(f,\bm{x}_{k+1},\theta,p)\), we have that_

\[\mathbb{E}_{k}\frac{\alpha}{\tau}\left[f(\bm{y}_{k+1})-f(\bm{x})\right]\leq \mathbb{E}_{k}(1-\tau)\cdot\frac{\alpha}{\tau}\left[f(\bm{y}_{k})-f(\bm{x}) \right]+\frac{\left\|\bm{x}-\bm{z}_{k}\right\|^{2}}{2}-\frac{1+0.3\mu\alpha}{2 }\left\|\bm{x}-\bm{z}_{k+1}\right\|^{2}.\] (12)

Finally, we present the convergence rate and communication complexity of AccSVRS based on Lemma 3.5, and the proof is left in Appendix D.4.

**Theorem 3.6**: _Suppose Assumption 1 holds. Consider AccSVRS with the following hyperparameters_

\[\theta=\frac{1}{4\sqrt{n}\delta},p=\frac{1}{n},\tau=\frac{1}{4}\min\left\{1, \frac{n^{1/4}}{2}\sqrt{\frac{\mu}{\delta}}\right\},\alpha=\frac{\sqrt{n}}{8 \delta\tau},\]

_and Eq. (10) is satisfied in each iteration of \(\mathrm{SVRS}^{\mathrm{1ep}}(f,\bm{x}_{k+1},\theta,p)\). Then for any \(\varepsilon>0\), when_

\[k\geq K_{2}:=\max\left\{4,8n^{-1/4}\sqrt{\delta/\mu}\right\}\log\frac{2[f(\bm{ y}_{0})-f(\bm{x}_{*})]}{\varepsilon},\]

_i.e., after \(\tilde{\mathcal{O}}\left(n+n^{3/4}\sqrt{\delta/\mu}\right)\) communications in expectation, we obtain that \(\mathbb{E}f(\bm{y}_{k})-f(\bm{x}_{*})\leq\varepsilon\)._

**Remark 3.7**: _Although roughly the same as the communication complexity obtained by Catalyzed SVRP in [33, Theorem 3], our results have the following advantages._

_Fewer assumptions. Except for the strong convexity of \(f\) and AveSS of \(f_{i}\)'s, we do not need to assume component strong convexity appearing in [33, Assumption 2]._

_Inexact proximal step. Khaled and Jin [33, Theorem 3] require exact evaluations of the proximal operator, though they mention that this is only for the convenience of analysis. Our framework allows approximated solutions in each proximal step, and the approximation criterion (10) is error-independent, i.e., irrelevant to the final error \(\varepsilon\). Since the local proximal function is strongly convex, we could solve the problem in a few steps if additionally assuming the smoothness of \(f_{1}\)._

_Smoothness-free bound. As shown in [33, Appendix G.1] or Appendix C, even if an exact proximal step is allowed, a dependence on the smoothness coefficient would be introduced in the total communication iterations of Catalyzed SVRP, though only in a log scale. Our directly accelerated method has no dependence on the smoothness coefficient._

### Gradient Complexity under Smooth Assumption

Due to the importance of total computation in the machine learning and optimization community, we consider a more common setup by **additionally assuming** that \(f_{1}\) is \(L\)-smooth with \(L\geq\delta\geq\mu>0\), which together with Assumption 1 facilitates the quantification of Eq. (10). Then we can compute the total gradient complexity for AccSVRS as shown below. By Proposition 2.5 and our assumptions, \(A_{\theta}^{t}(\bm{x})\) is \((\frac{1}{\theta}-\sqrt{n\delta})\)-strongly convex and \((\frac{1}{\theta}+L)\)-smooth. Using accelerated methods starting from \(\bm{x}_{t}\), we can guarantee that Eq. (10) holds after \(T_{\mathrm{app}}=\tilde{\mathcal{O}}\left(\sqrt{\frac{1+\theta L}{1-\sqrt{n \theta}\delta}}\right)=\tilde{\mathcal{O}}\left(1+n^{-1/4}\sqrt{L/\delta}\right)\) iterations with the choice of \(\theta\) in Theorem 3.6. Hence, the total gradient calls in expectation are

\[\mathcal{O}(nT_{\mathrm{app}}\cdot K_{2})=\tilde{\mathcal{O}}\left(n+n^{3/4} \left(\sqrt{\delta/\mu}+\sqrt{L/\delta}\right)+\sqrt{nL/\mu}\right).\]

Since \(\delta\in[\mu,L]\), we recover the optimal gradient complexity \(\tilde{\mathcal{O}}(n+n^{3/4}\sqrt{L/\mu})\) for the average smooth setting [63, Table 1] if neglecting log factors. Particularly, when \(\delta=\Theta(\sqrt{\mu L})\), we even obtain the nearly optimal gradient complexity \(\tilde{\mathcal{O}}(n+\sqrt{nL/\mu})\) for the component smooth setting [25, 26, 56]. We leave the details in Appendix E. Although the gradient complexity is not the primary focus of our work, we have demonstrated that the gradient complexity bound of AccSVRS is nearly optimal for certain values of \(\delta\) in specific cases.

## 4 Lower Bound

In this section, we establish the lower bound of the communication complexity, which nearly matches the upper bound of AccSVRS.

### Definition of Algorithms

In this subsection, we specify the class of algorithms to which our lower bound can apply. We first introduce the Proximal Incremental First-order Oracle (PIFO) [56, 25], which is defined as \(h_{f_{i}}^{\mathrm{P}}(\bm{x},\gamma)=[f_{i}(\bm{x}),\nabla f_{i}(\bm{x}), \mathrm{prox}_{f_{i}}^{\gamma}(\bm{x})]\) with \(\gamma>0\). Here the proximal operator is defined as \(\mathrm{prox}_{f_{i}}^{\gamma}(\bm{x}):=\arg\min_{\bm{u}}\{f_{i}(\bm{u})+\frac{ 1}{2\gamma}\left\|\bm{x}-\bm{u}\right\|^{2}\}\). In addition to the local zero-order and first-order information of \(f_{i}\) at \(\bm{x}\), the PIFO \(h_{f_{i}}^{\mathrm{P}}(\bm{x},\gamma)\) also provides some global information through the proximal operator9. Then we assume the algorithm has access to the PIFO and the definition of algorithms is presented as follows.

Footnote 9: If we let \(\gamma\to\infty,\,\mathrm{prox}_{f_{i}}^{\gamma}(\bm{x})\) converges to the exact minimizer of \(f_{i}\), irrelevant to the choice of \(\bm{x}\).

**Definition 4.1**: _Consider a randomized algorithm \(\mathcal{A}\) to solve problem (1). Suppose the number of communication rounds is \(T\). At the initialization stage, the master node \(1\) communicates with all the others. In round \(t\)\((0\leq t\leq T-1)\), the algorithm samples a node \(i_{t}\sim\mathrm{Unif}([n])\), and node \(1\) communicates with node \(i_{t}\). Then the algorithm samples a Bernoulli random variable \(a_{t}\) with constant expectation \(c_{0}/n\). If \(a_{t}=1\), node \(1\) communicates with all the others. Define the information set \(\mathcal{I}_{t+1}\) as the set of all the possible points \(\mathcal{A}\) can obtain after round \(t\). The algorithm updates \(\mathcal{I}_{t+1}\) based on the linear-span operation and PIFO, and finally outputs a certain point in \(\mathcal{I}_{T}\)._

At the initialization stage, the communication cost is \(2(n-1)\). In each communication round, the Bernoulli random variable \(a_{t}\) determines whether the master node communicates with all the others, i.e., whether to calculate the full gradient. Since \(\mathbb{E}a_{t}=c_{0}/n\), the expected communication cost of each round is of the order \(\Theta(1)\). Thus the total communication cost is of the order \(\Theta(n+T)\) and we can use \(T\) to measure the communication complexity. Moreover, one can check Algorithm 2 satisfies Definition 4.1. The formal definition and detailed analysis are deferred to Appendix F.1.

### The Construction and Results

In this section, we construct a hard instance of problem (1) and then use it to establish the lower bound. Due to space limitations, we only present several key properties. The complete framework of construction is deferred to Appendix F.2.

Inspired by [25], we consider the class of matrices \(\bm{B}(m,\zeta)=\left[\begin{smallmatrix}1&-1&\ldots\\ &\ddots&\ddots\\ &&1&-1\\ &&&\zeta\end{smallmatrix}\right]\in\mathbb{R}^{m\times m}\). This class of matrices is widely used to establish lower bounds for minimax optimization problems [61, 49, 59], and \(\bm{A}(m,\zeta):=\bm{B}(m,\zeta)^{\top}\bm{B}(m,\zeta)\) is the well-known tridiagonal matrix in the analysis of lower bounds for convex optimization [47, 40, 15]. Denote the \(l\)-th row of \(\bm{B}(m,\zeta)\) as \(\bm{b}_{l}(m,\zeta)^{\top}\). We partition the row vectors of \(\bm{B}(m,\zeta)\) according to the index sets \(\mathcal{L}_{i}=\{l:1\leq l\leq m,\,l\equiv i-1\,(\mathrm{mod}\,\,(n-1))\}\) for \(2\leq i\leq n\) and \(\mathcal{L}_{1}=\varnothing\)10. These sets are mutually exclusive and their union is \([m]\). Then we consider the following problem

Footnote 10: Such a way of partitioning is also inspired by [25] and similar to that in [36]. However, our setting is different from theirs.

\[\min_{\bm{x}\in\mathbb{R}^{m}}r(\bm{x};m,\zeta,c)\!\!=\!\!\frac{1}{n}\sum_{i=1 }^{n}\Bigg{[}r_{i}(\bm{x};m,\zeta,c)\!\!:=\!\!\begin{cases}\frac{c}{2}\left\| \bm{x}\right\|^{2}\!-\!n\left\langle\bm{e}_{1},\bm{x}\right\rangle&\text{for }i=1,\\ \frac{c}{2}\left\|\bm{x}\right\|^{2}\!+\!\frac{n}{2}\sum\limits_{l\in\mathcal{ L}_{i}}\left\|\bm{b}_{l}(m,\zeta)^{\top}\bm{x}\right\|^{2}&\text{for }i\neq 1.\end{cases}\] (13)

Here \(\bm{e}_{i}\in\mathbb{R}^{m}\) denotes the unit vector with the \(i\)-th element equal to \(1\) and others equal to \(0\). Then one can check \(r(\bm{x};m,\zeta,c)=\frac{1}{2}\left\|\bm{x}^{\top}\bm{A}(m,\zeta)\,\bm{x}+ \frac{c}{2}\left\|\bm{x}\right\|^{2}-\left\langle\bm{e}_{1},\bm{x}\right\rangle \right)\). Clearly, \(r\) is \(c\)-strongly convex. We can also determine the AveSS parameter as follows. The proof is deferred to Appendix F.3.

**Proposition 4.2**: _Suppose that \(0<\zeta\leq\sqrt{2}\), \(n\geq 3\) and \(m\geq 3\). Then \(r_{i}\)'s satisfy \(\sqrt{8n+4}\)-AveSS._

Define the subspaces \(\{\mathcal{F}_{k}\}_{k=0}^{m}\) as \(\mathcal{F}_{0}=\{\bm{0}\}\) and \(\mathcal{F}_{k}=\mathrm{span}\{\bm{e}_{1},\bm{e}_{2},\ldots,\bm{e}_{k}\}\) for \(1\leq k\leq m\). The next lemma is fundamental to our analysis. The proof is deferred to Appendix F.5.

**Lemma 4.3**: _Suppose the algorithm \(\mathcal{A}\) satisfies Definition 4.1 and apply it to solve problem (13) with \(n\geq 3\) and \(m\geq 4\). We have (i) \(\mathcal{I}_{0}=\mathcal{F}_{1}\). (ii) Suppose \(\mathcal{I}_{t}\subseteq\mathcal{F}_{k}\) (\(1\leq k\leq m-3\)). If \(i_{t}\) satisfies \(k\in\mathcal{L}_{i_{t}}\) or \(a_{t}=1\), then \(\mathcal{I}_{t+1}\subseteq\mathcal{F}_{k+3}\); otherwise, \(\mathcal{I}_{t+1}\subseteq\mathcal{F}_{k}\)._Lemma 4.3 guarantees that in each round, only when a specific component is sampled or the full gradient is calculated, can we expand the information set by at most three dimensions. For problem (13), we could never obtain an approximate solution unless we expand the information set to the whole space (see Proposition F.6 in Appendix F.2), while Lemma 4.3 implies that the process of expanding is very slow. Then we can establish the following lower bound.

**Theorem 4.4**: _For any \(n\geq 3\), \(\delta,\mu>0\), algorithm \(\mathcal{A}\) satisfying Definition 4.1 and sufficiently small \(\epsilon>0\), there exists a rescaled version of problem (13) such that (i) Assumption 1 holds; (ii) In order to find an \(\epsilon\)-suboptimal solution \(\hat{\bm{x}}\) such that \(\mathbb{E}r(\hat{\bm{x}})-\min_{\bm{x}}r(\bm{x})<\epsilon\) by \(\mathcal{A}\), the communication complexity in expectation is \(\tilde{\Omega}(n+n^{3/4}\sqrt{\delta/\mu})\)._

This lower bound nearly matches the upper bound in Theorem 3.6 up to log factors, implying Algorithm 2 is nearly optimal in terms of communication complexity. The detailed statement and proof are deferred to Appendices F.2 and F.9.

## 5 Experiments

To demonstrate the advantages of our algorithms, we conduct the same numerical experiments as those in [35; 33]. We focus on the linear ridge regression problem with \(\ell_{2}\) regularization, where the average loss \(f\) has the formulation: \(f(\bm{x})=\frac{1}{n}\sum_{i=1}^{n}\left[f_{i}(\bm{x}):=\frac{1}{m}\sum_{j=1} ^{m}\left(\bm{z}_{i,j}^{\top}\bm{x}-y_{i,j}\right)^{2}+\frac{\mu}{2}\left\| \bm{x}\right\|^{2}\right]\). Here \(\bm{z}_{i,j}\in\mathbb{R}^{d}\) and \(y_{i,j}\in\mathbb{R},\forall i\in[n],j\in[m]\) serve as the feature and label respectively, and \(m\) can be viewed as data size in each local client. We consider a synthetic dataset generated by adding a small random noise matrix to the center matrix, ensuring a small \(\delta\). To capture the differences in convergence rates between our methods and SVRP caused by different magnitudes of \(\mu\), we vary \(\mu=10^{-i},i\in\{0,1,2\}\). We compare our methods (SVRS and AccSVRS) against SVRG, KatyushaX, SVRP (Catalyzed SVRP is somehow hard to tune so we omit it), and Accelerated Extragradient (AccEG) using their theoretical step sizes, except that we scale the interpolation parameter \(\tau\) in KatyushaX and AccSVRS for producing practical performance (see Appendix G for detail). From Figure 1, we can observe that for a large \(\mu\), SVRP outperforms existing algorithms due to its high-order dependence on \(\mu\). However, when the problem becomes ill-conditioned with a small \(\mu\), AccSVRS exhibits significant improvements compared to other algorithms.

## 6 Conclusion

In this paper, we have introduced two new algorithms, SVRS and its directly accelerated version AccSVRS, and established improved communication complexity bounds for distributed optimization under the similarity assumption. Our rates are entirely smoothness-free and only require strong convexity of the objective, average similarity, and proximal friendliness of components. Moreover, our methods also have nearly optimal gradient complexity (leaving out the log term) when applied to smooth components in specific cases. It would be interesting to remove additional log terms to achieve both optimal communication and local gradient calls as [35], as well as investigating the complexity under other similarity assumptions (such as SS instead of AveSS) in future research.

Figure 1: Numerical experiments on synthetic data. The corresponding coefficients are shown in the title of each graph. We plot the function gap on a log scale versus the number of communication steps, where one exchange of vectors counts as a communication step.

## Acknowledgments and Disclosure of Funding

Lin, Han, and Zhang have been supported by the National Key Research and Development Project of China (No. 2022YFA1004002) and the National Natural Science Foundation of China (No. 12271011). Ye has been supported by the National Natural Science Foundation of China (No. 12101491).

## References

* [1] Artem Agafonov, Pavel Dvurechensky, Gesualdo Scutari, Alexander Gasnikov, Dmitry Kamzolov, Aleksandr Lukashevich, and Amir Daneshmand. An accelerated second-order method for distributed stochastic optimization. In _2021 60th IEEE Conference on Decision and Control (CDC)_, pages 2407-2413. IEEE, 2021.
* [2] Alekh Agarwal and Leon Bottou. A lower bound for the optimization of finite sums. In _ICML_, 2015.
* [3] Masoud Ahookosh and Yurii Nesterov. High-order methods beyond the classical complexity bounds, ii: inexact high-order proximal-point methods with segment search. _arXiv preprint arXiv:2109.12303_, 2021.
* [4] Mohammad S Alkousa, Alexander Vladimirovich Gasnikov, Darina Mikhailovna Dvinskikh, Dmitry A Kovalev, and Fedor Sergeevich Stonyakin. Accelerated methods for saddle-point problem. _Computational Mathematics and Mathematical Physics_, 60:1787-1809, 2020.
* [5] Zeyuan Allen-Zhu. Katyusha: the first direct acceleration of stochastic gradient methods. _The Journal of Machine Learning Research_, 18(1):8194-8244, 2017.
* [6] Zeyuan Allen-Zhu. Katyusha x: Practical momentum method for stochastic sum-of-nonconvex optimization. _arXiv preprint arXiv:1802.03866_, 2018.
* [7] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. _Advances in neural information processing systems_, 28, 2015.
* [8] Syreen Banabilah, Moayad Aloqaily, Eitaa Alsayed, Nida Malik, and Yaser Jararweh. Federated learning review: Fundamentals, enabling technologies, and future applications. _Information processing & management_, 59(6):103061, 2022.
* [9] Ron Bekkerman, Mikhail Bilenko, and John Langford. _Scaling up machine learning: Parallel and distributed approaches_. Cambridge University Press, 2011.
* [10] Aleksandr Beznosikov and Alexander Gasnikov. Compression and data similarity: Combination of two techniques for communication-efficient solving of distributed variational inequalities. In _International Conference on Optimization and Applications_, pages 151-162. Springer, 2022.
* [11] Aleksandr Beznosikov and Alexander Gasnikov. Similarity, compression and local steps: Three pillars of efficient communications for distributed variational inequalities. _arXiv preprint arXiv:2302.07615_, 2023.
* [12] Aleksandr Beznosikov, Eduard Gorbunov, and Alexander Gasnikov. Derivative-free method for composite optimization with applications to decentralized distributed optimization. _IFAC-PapersOnLine_, 53(2):4038-4043, 2020.
* [13] Aleksandr Beznosikov, Gesualdo Scutari, Alexander Rogozin, and Alexander Gasnikov. Distributed saddle-point problems under data similarity. _Advances in Neural Information Processing Systems_, 34:8172-8184, 2021.
* [14] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex optimization. _SIAM Journal on Optimization_, 28(2):1751-1772, 2018.
* [15] Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points ii: first-order methods. _Mathematical Programming_, 185(1-2):315-355, 2021.

* Chang and Lin [2011] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. _ACM transactions on intelligent systems and technology (TIST)_, 2(3):1-27, 2011.
* Chen and Teboulle [1993] Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm using bregman functions. _SIAM Journal on Optimization_, 3(3):538-543, 1993.
* Daneshmand et al. [2021] Amir Daneshmand, Gesualdo Scutari, Pavel Dvurechensky, and Alexander Gasnikov. Newton method over networks is fast up to the statistical precision. In _International Conference on Machine Learning_, pages 2398-2409. PMLR, 2021.
* Defazio [2016] Aaron Defazio. A simple practical accelerated method for finite sums. _Advances in neural information processing systems_, 29, 2016.
* Dragomir et al. [2021] Radu Alexandru Dragomir, Mathieu Even, and Hadrien Hendrikx. Fast stochastic bregman gradient methods: Sharp analysis and variance reduction. In _International Conference on Machine Learning_, pages 2815-2825. PMLR, 2021.
* Dvinskikh et al. [2020] Darina Mikhailovna Dvinskikh, Sergey Sergeevich Omelchenko, Alexander Vladimirovich Gasnikov, and AI Tyurin. Accelerated gradient sliding for minimizing a sum of functions. In _Doklady Mathematics_, volume 101, pages 244-246. Springer, 2020.
* d'Aspremont et al. [2021] Alexandre d'Aspremont, Damien Scieur, Adrien Taylor, et al. Acceleration methods. _Foundations and Trends(r) in Optimization_, 5(1-2):1-245, 2021.
* Garber et al. [2016] Dan Garber, Elad Hazan, Chi Jin, Cameron Musco, Praneeth Netrapalli, Aaron Sidford, et al. Faster eigenvector computation via shift-and-invert preconditioning. In _International Conference on Machine Learning_, pages 2626-2634. PMLR, 2016.
* Grapiglia and Nesterov [2022] Geovani Nunes Grapiglia and Yurii Nesterov. Adaptive third-order methods for composite convex optimization. _arXiv preprint arXiv:2202.12730_, 2022.
* Han et al. [2021] Yuze Han, Guangzeng Xie, and Zhihua Zhang. Lower complexity bounds of finite-sum optimization problems: The results and construction. _arXiv preprint arXiv:2103.08280_, 2021.
* Hannah et al. [2018] Robert Hannah, Yanli Liu, Daniel O'Connor, and Wotao Yin. Breaking the span assumption yields fast finite-sum minimization. _Advances in Neural Information Processing Systems_, 31, 2018.
* Hendrikx et al. [2020] Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, and Laurent Massoulie. Statistically preconditioned accelerated gradient method for distributed optimization. In _International conference on machine learning_, pages 4203-4227. PMLR, 2020.
* Ivanova et al. [2022] Anastasiya Ivanova, Pavel Dvurechensky, Evgeniya Vorontsova, Dmitry Pasechnyuk, Alexander Gasnikov, Darina Dvinskikh, and Alexander Tyurin. Oracle complexity separation in convex optimization. _Journal of Optimization Theory and Applications_, 193(1-3):462-490, 2022.
* Johnson and Zhang [2013] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. _Advances in neural information processing systems_, 26, 2013.
* Kairouz et al. [2021] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* Kamzolov et al. [2020] Dmitry Kamzolov, Alexander Gasnikov, and Pavel Dvurechensky. Optimal combination of tensor optimization methods. In _Optimization and Applications: 11th International Conference, OPTIMA 2020, Moscow, Russia, September 28-October 2, 2020, Proceedings 11_, pages 166-183. Springer, 2020.
* Karimireddy et al. [2020] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.

* [33] Ahmed Khaled and Chi Jin. Faster federated optimization under second-order similarity. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=EIC6LY04MfD.
* [34] Dmitry Kovalev, Samuel Horvath, and Peter Richtarik. Don't jump through hoops and remove those loops: Svrg and katyusha are better without the outer loop. In _Algorithmic Learning Theory_, pages 451-467. PMLR, 2020.
* [35] Dmitry Kovalev, Aleksandr Beznosikov, Ekaterina Dmitrievna Borodich, Alexander Gasnikov, and Gesualdo Scutari. Optimal gradient sliding and its application to optimal distributed optimization under similarity. In _Advances in Neural Information Processing Systems_, 2022.
* [36] Dmitry Kovalev, Aleksandr Beznosikov, Abdurakhmon Sadiev, Michael Persiianov, Peter Richtarik, and Alexander Gasnikov. Optimal algorithms for decentralized stochastic variational inequalities. _Advances in Neural Information Processing Systems_, 35:31073-31088, 2022.
* [37] Guanghui Lan. Gradient sliding for composite optimization. _Mathematical Programming_, 159:201-235, 2016.
* [38] Guanghui Lan and Yuyuan Ouyang. Accelerated gradient sliding for structured convex optimization. _Computational Optimization and Applications_, 82(2):361-394, 2022.
* [39] Guanghui Lan and Yi Zhou. Conditional gradient sliding for convex optimization. _SIAM Journal on Optimization_, 26(2):1379-1409, 2016.
* [40] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. _Mathematical programming_, 171:167-215, 2018.
* [41] Kfir Y Levy. Slowcal-sgd: Slow query points improve local-sgd for stochastic convex optimization. _arXiv preprint arXiv:2304.04169_, 2023.
* [42] Bingcong Li, Meng Ma, and Georgios B Giannakis. On the convergence of sarah and beyond. In _International Conference on Artificial Intelligence and Statistics_, pages 223-233. PMLR, 2020.
* [43] Zhize Li. Anita: An optimal loopless accelerated variance-reduced gradient method. _arXiv preprint arXiv:2103.11333_, 2021.
* [44] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in neural information processing systems_, 30, 2017.
* [45] Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, and He Zhang. Federated learning meets natural language processing: A survey. _arXiv preprint arXiv:2107.12603_, 2021.
* [46] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In _International Conference on Machine Learning_, pages 15750-15769. PMLR, 2022.
* [47] Yurii Nesterov et al. _Lectures on convex optimization_, volume 137. Springer, 2018.
* [48] Dinh C Nguyen, Quoc-Viet Pham, Pubudu N Pathirana, Ming Ding, Aruna Seneviratne, Zihuai Lin, Octavia Dobre, and Won-Joo Hwang. Federated learning for smart healthcare: A survey. _ACM Computing Surveys (CSUR)_, 55(3):1-37, 2022.
* [49] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems. _Mathematical Programming_, 185(1-2):1-35, 2021.
* [50] Sashank J Reddi, Jakub Konecny, Peter Richtarik, Barnabas Poczos, and Alex Smola. Aide: Fast and communication efficient distributed optimization. _arXiv preprint arXiv:1608.06879_, 2016.

* [51] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In _International conference on machine learning_, pages 1000-1008. PMLR, 2014.
* [52] Danny C Sorensen. Numerical methods for large eigenvalue problems. _Acta Numerica_, 11:519-584, 2002.
* [53] Ivan Stepanov, Artyom Voronov, Aleksandr Beznosikov, and Alexander Gasnikov. One-point gradient-free methods for composite optimization with applications to distributed optimization. _arXiv preprint arXiv:2107.05951_, 2021.
* [54] Ying Sun, Gesualdo Scutari, and Amir Daneshmand. Distributed optimization based on gradient tracking revisited: Enhancing convergence rate via surrogation. _SIAM Journal on Optimization_, 32(2):354-385, 2022.
* [55] Ye Tian, Gesualdo Scutari, Tianyu Cao, and Alexander Gasnikov. Acceleration in distributed optimization under similarity. In _International Conference on Artificial Intelligence and Statistics_, pages 5721-5756. PMLR, 2022.
* [56] Blake E Woodworth and Nati Srebro. Tight complexity bounds for optimizing composite objectives. _Advances in neural information processing systems_, 29, 2016.
* [57] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. _SIAM Journal on Optimization_, 24(4):2057-2075, 2014.
* [58] Xiao-Tong Yuan and Ping Li. On convergence of distributed approximate newton methods: Globalization, sharper bounds and beyond. _The Journal of Machine Learning Research_, 21(1):8502-8552, 2020.
* [59] Junyu Zhang, Mingyi Hong, and Shuzhong Zhang. On lower iteration complexity bounds for the convex concave saddle point problems. _Mathematical Programming_, 194(1-2):901-935, 2022.
* [60] Sai Qian Zhang, Jieyu Lin, and Qi Zhang. A multi-agent reinforcement learning approach for efficient client selection in federated learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9091-9099, 2022.
* [61] Siqi Zhang, Junchi Yang, Cristobal Guzman, Negar Kiyavash, and Niao He. The complexity of nonconvex-strongly-concave minimax optimization. In _Uncertainty in Artificial Intelligence_, pages 482-492. PMLR, 2021.
* [62] Yuchen Zhang and Xiao Lin. Disco: Distributed optimization for self-concordant empirical loss. In _International conference on machine learning_, pages 362-370. PMLR, 2015.
* [63] Dongruo Zhou and Quanquan Gu. Lower bounds for smooth nonconvex finite-sum optimization. In _International Conference on Machine Learning_, pages 7574-7583. PMLR, 2019.