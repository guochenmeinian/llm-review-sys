# Learning Visual Prior via Generative Pre-Training

Jinheng Xie\({}^{1}\) Kai Ye\({}^{2*}\) Yudong Li\({}^{2*}\) Yuexiang Li\({}^{3}\) Kevin Qinghong Lin\({}^{1}\)

**Yefeng Zheng\({}^{3}\) Linlin Shen\({}^{2}\) Mike Zheng Shou\({}^{1\dagger}\)**

\({}^{1}\) Show Lab, National University of Singapore \({}^{2}\) Shenzhen University

\({}^{3}\) Jarvis Research Center, Tencent YouTu Lab

{sierkinhane,mike.zheng.shou}@gmail.com

https://sierkinhane.github.io/visor-gpt

 Equal Contribution \(\dagger\) Corresponding Author

###### Abstract

Various stuff and things in visual data possess specific traits, which can be learned by deep neural networks and are implicitly represented as the visual prior, _e.g.,_ object location and shape, in the model. Such prior potentially impacts many vision tasks. For example, in conditional image synthesis, spatial conditions failing to adhere to the prior can result in visually inaccurate synthetic results. This work aims to explicitly learn the visual prior and enable the customization of sampling. Inspired by advances in language modeling, we propose to learn **Visual prior** via **G**enerative **P**re-**T**raining, dubbed VisorGPT. By discretizing visual locations, _e.g.,_ bounding boxes, human pose, and instance masks, into sequences, VisorGPT can model visual prior through likelihood maximization. Besides, prompt engineering is investigated to unify various visual locations and enable customized sampling of sequential outputs from the learned prior. Experimental results demonstrate the effectiveness of VisorGPT in modeling visual prior and extrapolating to novel scenes, _potentially motivating that discrete visual locations can be integrated into the learning paradigm of current language models to further perceive visual world_.

## 1 Introduction

The digital camera can continuously capture photographs of the visual world, such that tremendous photos and videos are currently shared on the Internet. In our world, various stuff and things possess specific _traits_, which have been correspondingly embedded in such visual data. In the current era of deep learning, deep neural networks [10, 40, 7] have demonstrated remarkable proficiency in learning from vast amounts of data, leading to the development of visual foundation models (VFMs) [15, 32, 43, 12, 22, 20]. Such _traits_ have been accordingly learned and implicitly represented as the _visual prior_ in VFMs, which has the potential to impact real-world applications. An example that highlights its importance can be seen in the field of image synthesis. To present high-quality and natural-looking images, the synthetic stuff and things must adhere to the visual prior such as the **spatial location, shape, and interaction of objects** (Fig. 1 (a)). A vivid example of layout-to-image is provided in Fig. 1 (b). When the spatial conditions do not adhere to the visual prior, such as the shape of 'donut' not being square, the size of 'person' being similar to that of 'donut', and 'donut' being floated in the air instead of being placed on 'dining table', the resulting synthetic contents may be inaccurate and visually inconsistent with the desired outcome. Despite recent advances in conditional image synthesis such as ControlNet [43] and GLIGEN [22], the challenge of continuously sampling customized spatial conditions that adhere to the visual prior remains a difficult problem, particularly for automatic synthesis of massive images with corresponding fine-grained annotations.

In this paper, we study the problem of how to explicitly learn visual prior from the real world and enable customization of sampling. If we would like to paint a series of instances on a canvas, weshould decide what to paint and also their shapes, locations, interactions, _etc_. It seems that these elements share a joint probabilistic prior, in which any stuff or things can be accordingly sampled to construct a scene. As there may be many potential variables in the prior, it is extremely hard to be comprehensively formulated. Over the past few years, significant advances have been made in language modeling [28, 29, 1, 6, 46], demonstrating their remarkable capacity for modeling the probabilistic distribution of sentences. Our focus is on learning the visual prior of location, shape, and relationships among categories, rather than raw pixels. It is possible to convert such visual information into a series of sequences, such that the visual prior can be learned by language modeling. To this end, as presented in Fig. 1 (d), we propose to learn **Vis**ual **prior** via **G**enerative **P**re-**T**raining, dubbed **Vis**G**PT. Thanks to the development of deep learning, many high-quality annotated data such as bounding-box [23, 38, 17], human pose [23, 21], instance mask [23] are publicly available. This provides sufficient location, shape, and relation information of stuff and things in the visual world. Since they are all encoded using 2D or 3D coordinates, we can simply convert them into a corpus of sequences. In this way, the visual prior can be learned by a pretext objective, _e.g.,_ maximizing the likelihood of each sequence. Beyond this, prompt engineering is investigated to unify various visual locations and enable the customized sampling of sequential outputs from the learned prior.

As shown in Fig. 1 (e), according to the user's prompt, VisrGPT can correspondingly sample a sequence from the learned prior, which can be spatially decoded for image synthesis (Fig. 1 (c)). Since the decoded conditions adhere to the prior, the synthetic 'cup', 'dining table', and 'donut' are realistic and consistent with the desired semantics. This finding confirms that we can continuously customize spatial conditions from many aspects, _e.g.,_**data type, object size, number of instances, and classes**, using VisrGPT. With the advance of conditional image synthesis, it is feasible to generate an endless supply of synthetic images with their corresponding fine-grained annotations, potentially providing ample resources to train more robust and generalized visual intelligence models.

## 2 Related Works

**Language Modeling.** Language modeling aims to estimate the probability of a given sequence of words occurring in a sentence. In recent years, Large language models (LLMs) such as GPT series [28, 29, 1] and BERT family [6, 18, 26] have revolutionized the field of natural language processing. In particular, BERT family adopts the encoder(-decoder) architecture and employs masked language modeling techniques to model each given sentence bi-directionally in context. In contrast, GPT series employs the decoder-only architecture to sequentially model the probability of

Figure 1: An overview of the problem of visual prior (top) and VisrGPT (bottom). (a) refers to visual prior, _e.g.,_ location, shape, and relations of objects. (b) provides a _failure_ case of image synthesis from spatial conditions that do not adhere to the prior. Specifically, the shape of the ‘donut’ not being square and ‘donut’ being floated in the air instead of being placed on ‘dining table’. (c) displays a _success_ case that conditions sampled from VisrGPT leads to a more accurate synthetic results. (d) illustrates that VisrGPT learns visual prior through sequence corpus converted from the visual world. (e) gives an example that a user customizes a sampling from VisrGPT by prompting.

[MISSING_PAGE_FAIL:3]

The provided prompts can be summarized in Tab. 1, which provides standardized templates to unify commonly used 2D and 3D visual location information into 1D textual sequences. Each prompt begins with the flags [Annotation type] and [Data type], which are the flags indicating the type of annotation and scene, _e.g.,_ box and multiple instances. The following flags of [Size] and [#Instances] represent the average area and the number of instances in the current image, while [#Keypoints] indicates the number of keypoints annotated for each person, _i.e.,_ 14 or 18. The following two flags are the [Category name] of each instance and their corresponding [Coordinate]. At last, [Natural Language Input] is extended for free-form language input and [Instruction] is used for iterative refinement of the current layout. Examples of these prompts are presented in the following sections. By employing our defined templates, we transform commonly used visual annotations into a large-scale sequential corpus. The corpus can be seamlessly ingested by language models, facilitating better learning of visual commonsense prior.

### Learning Visual Prior via Generative Pre-Training

**Model Architecture**. In the past few years, many large language models have been successively proposed, such as GPT [28; 29; 1] and BERT [6; 18; 26] family, and recently introduced LLAMA [39]. We employ the GPT decoder-style transformer as our model to learn the visual probabilistic prior.

**Pretext Objective**. After processing the visual locations \(\bm{x}\) as textual sequences \(\mathbf{t}\) in SS 3.2, we tokenize each sequence by byte-pair encoding (BPE) algorithm [37] to obtain a sequence with \(n\) tokens \(\mathbf{u}=\{u_{1},u_{2},\cdots,u_{n}\}\) such that a standard language modeling objective can be directly employed to learn visual prior by maximizing the following likelihood:

\[\mathcal{L}=\sum_{i}\text{log}p(u_{i}|u_{i-k},\cdots,u_{i-1};\Theta),\] (1)

where \(k\) is the size of context window, and \(p(\cdot|\cdot)\) indicates the conditional probability which is modeled by the neural network \(\Theta\). Stochastic gradient descent is used to train the neural network.

### Customizing Sequential Output

In addition to offering formatted visual annotations for learning a probabilistic prior, the standardized templates enable to _personalize sequential output for various applications through prompting_. For example, the customized sequential output can be employed as spatial conditions in image synthesis models (_e.g.,_ ControlNet [43] and GLIGEN [22]). This opens up the possibility of synthesizing a broad range of data types to address diverse problems and challenges in computer vision. Here are a few representative scenarios:

**(a) Object Bounding-Box.** As we use a flag to distinguish different types of visual annotations, we can control the type of data and scene to be sampled from the learned probabilistic prior by setting the beginning tokens in the input prompt. Accordingly, we can set the beginning prompt as "box;" to generate sequential output with instances and corresponding bounding-box information. Besides, with flags like [Size], [#Instances], and [#Keypoints], we can sample a scene that adheres to multiple conditions. As depicted in Fig. 2 (a), we can input a prompt "box; multiple instances; small; 16; 0; kite, kite, person," as a prefix to require the VisorGPT to conditionally infer the remaining tokens. In this example, VisorGPT outputs the categories and their locations, specifically fulfilling the requirement of objects being in small size. In particular, (xmin, ymin) and (xmax, ymax) are special tokens indicating the top-left and bottom-right corners of the target object.

**(b) Human Pose.** With flags of [#Instances] and [#Keypoints], VisorGPT is capable of customizing sequential outputs involving instances with keypoints in a crowd scene. We give an example in Fig. 2 (b). Numbers (10 and 14) are added to the beginning of prompt as conditions to infer a scene consisting of 10 people with 14 keypoints. Note that, we use "a, b, c, d, \(\cdots\)" and "m0, m1, m2, m3, \(\cdots\)" as special tokens to distinguish each human keypoint and object boundary coordinate, respectively.

**(c) Instance Mask.** Beyond sparse coordinates as shown in (a) and (b), VisorGPT can deal with dense spatial annotations, _i.e.,_ instance masks. Typically, pixel-level information can be represented

\begin{table}
\begin{tabular}{l l} \hline \hline Annotation type & box; keypoint; mask; multimodal; \(\cdots\) \\ Data type & object centric; multiple instances \\ Size & small; medium; large \\ \#Instances & 1; 2; 3; \(\cdots\) \\ \#Keypoints & 14; 18 \\ Category name & cup; person; dog; \(\cdots\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Candidate choices of prompt template.

using a mask matrix or a set of boundary coordinates. For convenient sequentialization, we uniformly sample \(n\) points along the angle in the polar space from object boundary coordinates to represent the pixel-level location, which is similar to [41]. We provide an example in Fig. 2 (c).

**(d) Multimodal Inference.**VisorGPT exhibits the capability of multimodal inference. In Fig. 2 (d), VisorGPT can deduce the presence of two individuals and a frisbee along with their bounding boxes and keypoints of these two people are also predicted at the same time.

**(e) Natural Language Input and Instruction.** Apart from the above prompts, VisorGPT can receive free-form language as input to generate corresponding layouts. We present an example in Fig. 2 (d). In addition, the proposed VisorGPT supports iterative refinement by instructions like "make the boy and girl closer".

## 4 Experiments

### Experimental Setup

**Datasets.** We collect around 4 million sequences from the publicly available datasets for VisorGPT. In particular, we consider three types of commonly used visual annotations, _i.e.,_ object bounding-box, human pose, and instance mask. In the MS-COCO dataset [23], we collect

\begin{table}
\begin{tabular}{l l l} \hline \hline Datasets (type) & \#Categories & \#Images \\ \hline Open Images (Box) [17] & 600 & 1,743,042 \\ Objects365 (Box) [38] & 365 & 1,728,775 \\ COCO (Box) [23] & 80 & 117,266 \\ ImageNet (Box) [33] & 1,000 & 38,285 \\ COCO (Keypoint) [23] & 1 & 53,473 \\ CrowdPose (Keypoint) [21] & 1 & 9,981 \\ COCO (Mask) [23] & 80 & 117,266 \\ Flickr30K [27] & 44,518 & 31,783 \\ Rico [5] & 25 & 35,851 \\ PubLayNet [44] & 5 & 315,757 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Training corpus for VisorGPT.

\begin{table}
\begin{tabular}{l l l} \hline \hline Datasets (type) & \#Categories & \#Images \\ \hline Open Images (Box) [17] & 600 & 1,743,042 \\ Objects365 (Box) [38] & 365 & 1,728,775 \\ COCO (Box) [23] & 80 & 117,266 \\ ImageNet (Box) [33] & 1,000 & 38,285 \\ COCO (Keypoint) [23] & 1 & 53,473 \\ CrowdPose (Keypoint) [21] & 1 & 9,981 \\ COCO (Mask) [23] & 80 & 117,266 \\ Flickr30K [27] & 44,518 & 31,783 \\ Rico [5] & 25 & 35,851 \\ PubLayNet [44] & 5 & 315,757 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Training corpus for VisorGPT.

Figure 2: Examples of customizing sequential outputs from the proposed VisorGPT.

-118K images annotated with 80 categories and their object bounding-boxes and instance masks. For each image, all object bounding-boxes and instance masks with their category information are formatted to a sequence, respectively. Beyond that, ~3.5 million bounding-box annotations of Objects365 [38] and Open Images [17] are also converted to sequences. Other types of annotations (_i.e.,_ human keypoint) of MS-COCO (~54K) and CrowdPose (~10K) are also formatted to sequential data. For the object-centric scenario, we collect ~4K sequences from ImageNet-1K [33]. Besides, Flick30K [27] is employed to support free-form natural language as input. For the scenarios of mobile interface and document layout, we use the Rico [5] and PubLayNet [44] datasets. A summary is presented in Tab. 2.

**Evaluation Metrics**. We propose to evaluate VisorGPT from three aspects: **(i)** Evaluating the quality of sequences generated by VisorGPT. In the inference stage, as VisorGPT predicts sequences in the format given in SS 3.2, it is necessary to examine whether the generated sequences can be decoded into visual locations. In particular, we generate a series of sequences using VisorGPT and calculate the accuracy whether it can be successfully decoded (termed **Format** in Table 5) and the number of categories matches the number of locations (termed **Matching** in Table 5). **(ii)** As discussed in SS 3.2, we use flags, _i.e.,_ [Size] and [#Instances], to indicate the average size and number of instances in the current sequence. Hence, we can control the average object size and the number of instances in the generated sequences via setting flags [Size] and [#Instances]. Then, we can calculate the accuracy whether the object size and the number of instances in the generated sequences are consistent with the given flags to validate the performance of controllability (termed **Size** and **#Instances**, respectively). **(iii)** Evaluating the learned probabilistic prior, _i.e.,_ object location, shape, and relation among categories, on the _val_ set of COCO, Objects365, and Open Images datasets. In this work, we propose to compare the discrete distribution of every visual prior. Specifically, to compute the **location** prior of a category, we initialize an empty canvas and convert the bounding-box of each instance of the category to a binary mask. Then, each mask is accumulated on the canvas and normalized as 2D location distribution. To compute the **shape** prior of a category, we calculate the ratio of width to height of each instance of the category, and estimate a discrete distribution as the shape prior. To establish the **relation** prior of a category to other categories, we count the number of co-occurrences between the category and other categories and estimate a discrete distribution. In this way, discrete prior of each category can be computed on COCO, Objects365, and OpenImages _val_ sets as real one. During evaluation, we infer a series of sequences to compute the learned visual prior. Then we measure the similarity between learned and the real prior using the **Kullback-Leibler divergence**[14]. In addition, FID [11] is adopted to compare with layout generation methods [16, 13].

### Quantitative Results

**Evaluation on Learned Visual Prior**. In Tab. 4, we present the measured similarity between real probabilistic prior and the one learned by V1-sorGPT on the validation sets of COCO, Open Images, and Objects365, using KL divergence. The prompt template T\({}_{a}\) and T\({}_{a}\)+T\({}_{b}\) in SS 3.2, are

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Prompt} & \multicolumn{3}{c}{KL Div on COCO (\(\downarrow\))} & \multicolumn{3}{c}{KL Div on Open Images (\(\downarrow\))} & \multicolumn{3}{c}{KL Div on Objects365 (\(\downarrow\))} \\ \cline{2-10}  & & Location & Shape & Relation & Location & Shape & Relation & Location & Shape & Relation \\ \hline VisorGPT\({}^{\dagger}\) & T\({}_{a}\) & 1.133 & 1.483 & 0.452 & - & - & - & - & - \\ VisorGPT\({}^{\dagger}\) & T\({}_{a}\)+T\({}_{b}\) & 1.032 & 1.446 & 0.445 & - & - & - & - & - \\ VisorGPT & T\({}_{a}\) & 1.212 & 1.813 & 0.561 & 0.890 & 2.775 & 3.715 & 1.969 & 1.345 & 2.790 \\ VisorGPT & T\({}_{a}\)+T\({}_{b}\) & 1.583 & 1.710 & 0.581 & 1.007 & 2.782 & 3.888 & 1.995 & 1.377 & 2.765 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluation on training corpus scale and prompt templates of VisorGPT. The similarity between real probabilistic prior and the learned one is measured by KL divergence (KL Div).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Models & \#Parameters & \#Training data & Annotation type & Batch size & Iterations & Learning rate & Sequence length \(n\) \\ \hline VisorGPT & 117M & 4M & box \& keypoint \& mask & 128 & 200K & \(5.0c^{-5}\) & 1024 \\ VisorGPT\({}^{\dagger}\) & 117M & 34K & box \& keypoint \& mask & 128 & 200K & \(5.0c^{-5}\) & 1024 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Model card of VisorGPT.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multicolumn{3}{c}{Quality (\(\uparrow\))} & \multicolumn{3}{c}{Controllability (\(\uparrow\))} \\ \cline{2-7}  & Format & Matching & Size & \#Instances \\ \hline COCO & 100.0 & 100.0 & 92.02 & 100.0 \\ Open Images & 99.97 & 99.40 & 89.35 & 98.71 \\ Objects365 & 99.99 & 99.94 & 91.52 & 99.78 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Evaluation on customized outputs (%).

used for comparison. Overall, VisorGPT T\({}_{a}\) and T\({}_{a}\)+T\({}_{b}\) exhibit comparable performance, indicating both prompt templates have comparable capability for learning visual prior.

**Evaluation on Customized Sequences**. We present the quality of generated sequences and the performance of VisorGPT's controllability in Tab. 5. It is obvious that nearly all predicted sequences can be decoded successfully in three datasets. Additionally, in over 99% of sequences, all instances can match their respective locations. Besides, the table shows that VisorGPT achieves accuracies of 92.02%, 89.35%, and 91.52% in controlling the average object size on COCO, Open Images, and Objects365 datasets, respectively. Furthermore, VisorGPT can achieve an accuracy of over 98% in controlling the number of instances across all three datasets. These findings demonstrate the strong capacity of VisorGPT in reasoning high-quality sequences and control the object size and number of instances in the scene.

**Comparison with Layout Generation Methods.** We also conduct experiments to compare with layout generation methods and the corresponding results are presented in Table 6. The experimental results include FID and Align. score (referred from LayoutDM [13]) with lower values indicates better performance. Notably, VisorGPT significantly surpasses these state-of-the-art methods with better FID and Align. score on Rico and PubLayNet datasets, showcasing the superior capability of VisorGPT to model and generate layouts.

### Visualization Results

**Generation Diversity**. One of the concerns is that VisorGPT would memorize the overall data distribution and cannot generate diverse and novel layouts. We present examples in Fig. 3 to address the concern. Firstly, we specify target classes (_e.g.,_ "bottle, dining table, person, knife, bowl, oven, person, cup, cup, bowl, bowl, broccoli, spoon") and then select layouts satisfying these conditions from COCO _train_ set (left in Fig. 3, only one satisfied sample over 110K samples). In contrast, we use the same target classes as prompts for VisorGPT to infer their bounding boxes. As depicted in Fig. 3, the bounding boxes generated by VisorGPT exhibit diversity and considerable differences from those selected ones from COCO _train_ set.

**Scene Completion**. VisorGPT can receive user conditions and reasonably complete the corresponding missing classes in an interactive way, which can also demonstrate the capability of novel/unseen layout generation. We present examples in Fig. 4. One can observe that users can roughly draw two instances of "person" across the canvas (above) and prompt VisorGPT to deduce the remaining "skis" (below). To avoid cherry-picking, we provide the distribution of generated "skis" using 100 samples when given various scenes.

As illustrated, when the two "person" vary from left-top to middle-bottom, the distribution of "skis"

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c}{Rico} & \multicolumn{2}{c}{PubLayNet} \\ \cline{2-5}  & FID (\(\downarrow\)) & Align. (\(\downarrow\)) & FID (\(\downarrow\)) & Align. (\(\downarrow\)) \\ \hline MaskGIT [2] & 52.1 & 0.015 & 27.1 & 0.101 \\ BIT [16] & 88.2 & 1.030 & 116 & 0.153 \\ BART [19] & 11.9 & 0.090 & 16.6 & 0.116 \\ VQDiffusion [9] & 7.46 & 0.178 & 15.4 & 0.193 \\ LayoutDM [13] & 6.65 & 0.162 & 13.9 & 0.195 \\ \hline VisorGPT (Ours) & **5.85** & **0.109** & **9.18** & **0.103** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison to previous methods.

Figure 4: Scene completion.

Figure 3: Generation diversity of VisorGPT.

exhibits consistent variations. This validates that VisorGPT does not rely on memorization. Instead, it learns intrinsic visual priors among categories, empowering it to infer novel/unseen layouts, accommodating the diverse requirements of users.

**Relation Prior.** Fig. 5 illustrates the comparison between the real-world relation matrix among 30 categories and the one estimated by VisorGPT. Each row depicts the relation prior of one category to others. For instance, it can be observed from the real world matrix that the 'person' (the first row) frequently interacts with other categories such as 'dog' and 'cat'. Similarly, in the third row, the co-occurrence between 'car' and 'bus', 'truck', and'stop sign' is larger than that of other categories. Notably, it is clear that the relation prior learned by VisorGPT is very close to that of the real-world one. This indicates that VisorGPT can capture the real relationships among categories and generate sequential output that aligns with these visual prior.

**Location Prior.** In addition to the quantitative results presented above, we visualize the comparison between the location prior learned by VisorGPT and the real one across various categories. Fig. 7 displays the location prior of three categories, including'surfboard', 'tie', and 'train'. It is noticeable that, in each column, the location prior learned by VisorGPT is similar to the real one. For instance, from the first column, one can observe that the real distribution of 'tie' is mainly located in the lower-middle region, and the shape prior learned by VisorGPT exhibits a similar pattern.

**Shape Prior.** Fig. 6 shows the shape prior of four categories, such as 'person' and'motorcycle'. To facilitate comparison, we employ kernel density estimation to estimate a continuous distribution from the discrete one. We observe that the shape prior learned by VisorGPT is close to those of the real visual world. For example, in the real world, the ratio of width to height of a car is almost always larger than 1, and the estimated shape prior of 'car' is mainly distributed around 1.8. It is evident that the learned probabilistic prior by VisorGPT, represented by the blue line, closely approximates the real one, represented by the red line. Overall, the shape priors of other categories learned by VisorGPT well match that of the real world.

### Ablation Studies

Tab. 8 presents the impact of Special Words (SW), Textual Knowledge (TK, _i.e._, with model weights initialized from the official pre-trained GPT-2), the number of sequences (#Seq), and model size (#Param). **(a)** Results on Tab. 8(a) are measured by the average KL divergence of location and shape prior. This confirms that the special words can potentially improve VisorGPT's performance in learning the visual prior. Notably, we found that the NLP textual knowledge deteriorated the performance of VisorGPT. We attribute this to the fact that the association between visual coordinates

Figure 5: Relation matrix among 30 categories on COCO.

Figure 6: Shape prior of the categories of ‘person’, ‘car’, ‘motorcycle’, and ‘teddy bear’.

and natural language is relatively weak, thus it becomes inessential to learn visual prior from visual annotations. **(b)** In Tab. (b)b, we find that increasing the number of sampled sequences leads to a more precise estimation of the visual prior by VisorGPT. **(c)** In Tab. (c)c, we investigate the impact of model size on learning visual prior. For simplicity and efficiency, we replace VisorGPT architecture by three GPT versions and train it using only COCO (box) data. The results demonstrate the scalability of VisorGPT, _i.e.,_ modeling the visual prior better with increased learnable parameters.

### Applications

**Conditional Image Synthesis**. VisorGPT's remarkable ability to infer visual categories and their locations based on user-customized prompts shows promising potential for generating customized images that still maintain a sense of realism. Here, we utilize ControlNet [43] and GLIGEN [22] to synthesize images from keypoints and bounding-boxes, respectively. We showcase some examples in Fig. 7 and 8. The first and fourth columns in Fig. 7 present the customized spatial conditions sampled from VisorGPT and the conditions not adhering to the visual prior. The second, third, and fifth columns provide synthetic results by GLIGEN conditioned on the corresponding spatial conditions. For example, on the first three columns, it is evident that the spatial conditions sampled from VisorGPT are more natural, such that the synthetic images are realistic and natural-looking. However, when the conditions (the last two columns) do not adhere to the prior, such as 'person' not being on a similar scale to 'dining table', the width of 'pizza' being too long, and the width of 'chair' being too short, the synthetic contents like 'person', 'chair', and 'dining table' appear abnormal, also impacting the authenticity of other objects like the two cups (circled in red dotted line).

Moreover, VisorGPT is capable of inferring sequences that include instances with keypoint information. For example, as shown in Fig. 8, we can provide a prompt like "key point; multiple instances; large; 13; 18; person, " to VisorGPT. This allows it to conditionally imagine a scene involving 13 people with their keypoint coordinates. Decoded results can be used as spatial conditions for image synthesis by ControlNet (shown in the last two columns). More examples can be found in supp.

## 5 Conclusion

This work proposed a novel approach, VisorGPT, to explicitly learning the probabilistic prior of the visual world through generative pre-training. This was achieved by transforming the continuous visual locations into discrete tokens by prompting and training a transformer decoder to maximize

\begin{table}

\end{table}
Table 8: Impact of Special Words (SW), Textual Knowledge (TK), Number of Sequences (#Seq), and Model size (#Param). We use KL Div (\(\downarrow\)) as evaluation metric.

Figure 7: Comparison of synthetic images from _object boxes_ adhering to the prior (left) or not (right).

the likelihood of training sequences. As a result, VisorGPT exhibits significant potential in comprehending real-world visual prior and leveraging this knowledge to create plausible scenes under a variety of customized prompts. This ability can facilitate the automatic synthesis of a vast number of images, along with their corresponding fine-grained annotations, using ControlNet and GLIGEN. This could potentially yield ample resources to train more robust visual intelligence models.

**Broader Impact**. This work demonstrates the substantial capacity of Large Language Models (LLMs) to effectively model spatial visual locations through language modeling. In the subsequent generations of LLMs, the visual priors discussed here can be seamlessly integrated into large-scale training, endowing LLMs with the ability to possess not only textual knowledge but also the capability to perceive and understand the visual world. While current datasets may have limited annotations for object categories, the vast knowledge present in extensive text corpora allows for the association of known objects with novel ones through language-based knowledge, such as synonyms. This presents a promising direction for the next generation of LLMs to expand their understanding and modeling of our visual world.

**Limitation**. We encountered limitations regarding the number of instances that could be included in each sequence due to the maximum token length, despite converting each mask annotation to a fixed length. In the future, we plan to incorporate large-scale natural language corpora into training and extend the maximum sequence length.

## Acknowledgment

This project is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2022).

Figure 8: Illustration of input prompts (comprising multiple instances with _keypoints_), output sequences, decoded results and synthetic images.

## References

* [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _NeurIPS_, pages 1877-1901, 2020.
* [2] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In _CVPR_, pages 11315-11325, 2022.
* [3] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. _arXiv preprint arXiv:2109.10852_, 2021.
* [4] Rodrigo de Bem, Arnab Ghosh, Thalalayssingam Ajanthan, Ondrej Miksik, Adnane Boukhayma, N Siddharth, and Philip Torr. Degpose: Deep generative models for human body analysis. _International Journal of Computer Vision_, 128:1537-1563, 2020.
* [5] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In _Proceedings of the 30th annual ACM symposium on user interface software and technology_, pages 845-854, 2017.
* [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [8] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. _arXiv preprint arXiv:2305.15393_, 2023.
* [9] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In _CVPR_, pages 10696-10706, 2022.
* [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _NeurIPS_, 30, 2017.
* [12] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. _arXiv preprint arXiv:2302.09778_, 2023.
* [13] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Ouni, and Kota Yamaguchi. LayoutDM: Discrete Diffusion Model for Controllable Layout Generation. In _CVPR_, pages 10167-10176, 2023.
* [14] James M Joyce. Kullback-leibler divergence. In _International encyclopedia of statistical science_, pages 720-722. 2011.
* [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [16] Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, and Irfan Essa. Blt: bidirectional layout transformer for controllable layout generation. In _ECCV_, pages 474-490. Springer, 2022.
* [17] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 128:1956-1981, 2020.
* [18] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. _arXiv preprint arXiv:1909.11942_, 2019.
* [19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. pages 7871-7880, 2022.
* [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [21] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In _CVPR_, pages 10863-10872, 2019.
* [22] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. _CVPR_, 2023.
* [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, pages 740-755, 2014.

* [24] Haozhe Liu, Bing Li, Haoqian Wu, Hanbang Liang, Yawen Huang, Yuexiang Li, Bernard Ghanem, and Yefeng Zheng. Combating mode collapse via offline manifold entropy estimation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 8834-8842, 2023.
* [25] Haozhe Liu, Wentin Zhang, Bing Li, Haoqian Wu, Nanjun He, Yawen Huang, Yuexiang Li, Bernard Ghanem, and Yefeng Zheng. Adaptivemix: Improving gan training via feature space shrinkage. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16219-16229, 2023.
* [26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [27] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _ICCV_, pages 2641-2649, 2015.
* [28] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, page 9, 2019.
* [30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _ICML_, pages 8821-8831, 2021.
* [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10684-10695, 2022.
* [33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _IJCV_, 115:211-252, 2015.
* [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.
* [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _arXiv preprint arXiv:2210.08402_, 2022.
* [36] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* [37] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. _arXiv preprint arXiv:1508.07909_, 2015.
* [38] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, pages 8430-8439, 2019.
* [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NeurIPS_, pages 5998-6008, 2017.
* [41] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask: Single shot instance segmentation with polar representation. In _CVPR_, pages 12193-12202, 2020.
* [42] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In _ICCV_, pages 7452-7461, 2023.
* [43] Lvnrin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* [44] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In _ICDAR_, pages 1015-1022. IEEE, 2019.
* [45] Guangming Zhu, Liang Zhang, Youliang Jiang, Yixuan Dang, Haoran Hou, Peiyi Shen, Mingtao Feng, Xia Zhao, Qiguang Miao, Syed Afaq Ali Shah, et al. Scene graph generation: A comprehensive survey. _arXiv preprint arXiv:2201.00443_, 2022.
* [46] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Robert Csordas, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Akel A Kaler Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. _arXiv preprint arXiv:2305.17066_, 2023.

Appendix

### Advantages compared to selecting GT bounding boxes from _train_ set

**(i) Diverse layouts generation**. When users are required to specify a large number of classes (larger than 10), the layouts selected from COCO _train_ set including all such classes are limited. As shown in Fig. 3, when target classes ('bottle', 'dining table', 'person', 'knife', 'bowl', 'bowl', 'oven', 'person', 'cup', 'cup', 'bowl', 'bowl', 'broccoli','spoon') are considered, there is only one relevant layout containing all the target classes from COCO _train_ set. In contrast, as shown in Fig. 3, VisorGPT can receive these target classes to infer a lot of corresponding layouts with much diversity.

**(ii) User-customized / Controllability**. Compared to selecting from ground truth bounding boxes, one of the main advantages of VisorGPT lies in its ability to extrapolate novel layouts based on users' configurations, as depicted in Fig. 10 (c). VisorGPT can deduce the missing classes' positions ('skis, skis, skis') based on the layout ('person, person') given by users.

**(iii) Free-form captions as layout condition**. Incorporating captions in the training sequences enables VisorGPT to receive free-form natural language input as a condition (_e.g.,_ a boy and a girl are playing by the sea or a train is running toward the man) and generate corresponding layouts (as shown in Fig. 10 (b)).

### Extrapolation ability

To showcase VisorGPT's extrapolation ability, we present examples in Fig. 10 (a). As demonstrated, when the'sea' occupies the majority of the canvas, the'surfboard' can appear throughout the entire canvas. Upon shifting the'sea' region to the middle part of the canvas, the'surfboard' is accordingly confined within the middle region. This phenomenon demonstrates that VisorGPT does not solely rely on memorization; rather, it learns intrinsic relation, location, and shape priors among categories. Consequently, VisorGPT can extrapolate unseen/novel layouts based on users' configurations.

### Examples of Training Sequences

Here, we give some examples of various types of training sequences on different datasets:

\begin{tabular}{l} 
\begin{tabular}{l} **Human Pose (COCO):** \\ key point; multiple instances; large; 1; 18; person; [ a 190 120 b 266 146 c 318 143 d 385 232 e 338 269 f 214 150 g 0 0 h 0 0 i 312 280 j 365 296 k 359 420 l 258 283 m 194 344 n 301 383 o 197 100 p 181 103 q \\
234 84 r 0 0] \\ \end{tabular}
**Human Pose (CrowdPose):** \\ key point; multiple instances; large; 2; 14; person, person; [ a 312 201 b 306 200 c 311 232 d 269 214 e 298 257 f 231 206 g 296 275 h 307 275 i 251 244 j 271 235 k 274 292 l 283 295 m 304 153 n 310 191] [ a 179 247 b 165 245 c 164 313 d 160 315 e 221 316 f 207 279 g 155 343 h 144 366 i 242 337 j 240 367 k 210 431 300 d 187 127 176 h 177 277 key point; multiple instances; large; 2; 14; person, person; [ a 240 178 b 304 168 c 228 239 d 0 0 e 261 236 f 0 0 g 251 266 h 289 296 i 0 0 j 0 k 0 0 1 0 0 m 261 92 n 272 156] [ a 314 160 b 363 158 c 274 232 d 356 264 e 224 260 f 271 263 g 298 315 h 341 324 i 0 0 j 332 442 k 0 0 0 1 0 0 m 287 64 n 333 133] \\ \end{tabular}
**Instance Mask:** \\ mask; multiple instances; medium; 1; 0; clock; [ m 02 224 291 m1 226 299 m2 227 306 m3 228 313 m4 233 320 m5 238 325 m6 245 329 m7 252 332 m8 259 334 m9 266 335 m10 274 333 m11 281 330 m12 288 327 \\ m13 293 323 m14 299 318 m15 303 312 m16 305 305 m17 307 298 m18 310 291 m9 308 284 m20 307 276 m21 303 269 m22 299 263 m23 295 257 m24 288 254 m25 280 251 m26 273 250 m27 266 249 m28 \\ 259 249 m29 252 251 m30 246 256 m31 240 260 m32 235 265 233 229 270 m34 227 277 m35 225 284] \\ \end{tabular} **Object Centric Bounding-Box:** \\ box; object centric; large; 1; 0; castle; [ xmin 236 ymin 142 xmax 413 ymax 232] \\ \end{tabular}

### Implementation Details

All experimental evaluations were conducted on eight NVIDIA Tesla V100-32GB GPUs using PyTorch. In order to include special words, we created a new vocabulary containing a total of 30,769 words based on a standard vocabulary. To optimize computational efficiency and memory utilization, we utilized the DeepSpeed framework. To serialize visual locations, we first resized the long side of each image to a length of 512 pixelsand then shifted the image content to the center by padding the short side to a length of 512 pixels. As a result, the number of bins \(m\) was set to 512. The flag of [Size] indicates the average area of all instances in the image and we set the flag according to the rule:

\[\left\{\begin{array}{cl}\text{``small''}&\text{average area}<32^{2}\\ \text{``medium''}&32^{2}\leq\text{average area}<96^{2}\\ \text{``large''}&\text{average area}\geq 96^{2}\end{array}\right..\]

We omitted person instances with fewer than five keypoints. To enable continuous generation, we designed and trained models based on the prompt format (b). Specifically, VisorGPT\({}^{\dagger}\) (a&kb) and VisorGPT (a&b) were trained using the same number of sequences as VisorGPT\({}^{\dagger}\) (a) and VisorGPT (a), respectively. The only difference is that we randomly utilized prompt format (a) or (b) to construct each training sequence.

During the evaluation stage, we set the maximum sequence length of our model (VisorGPT) to 256 tokens to ensure efficient inference. In the ablation studies, we added special words only to the [Coordinate] term, and we reported the average KL divergence between the location and shape priors learned by VisorGPT and those in the real world. Since training large-scale language models is time- and resource-consuming, we trained only three types of VisorGPT with respect to GPT-2 (base, medium, large) with a maximum token length of 256 in 50,000 iterations on COCO (Box) data.

### Evaluation Details

To estimate discrete visual prior from VisorGPT, we infer a series of sequences via prompting as below:

\begin{tabular}{l} Code in Python: \\ f"box; multiple instances; random.choice{['small','medium', 'large']}; \\ random.randint(2, 10); 0; category name, " \\ \end{tabular}

To ensure that each category in a given dataset is sufficiently represented in the sequence data used for estimating the visual prior, we specify a minimum number of sequences in which each category must appear. Table 9 provides an overview of the predicted sequences that are used for evaluation.

In our study, we adopt the Kullback-Leibler divergence to quantify the similarity between two given discrete distributions. Specifically, let \(p\) and \(q\) denote the estimated probabilistic priors derived from the real-world data and the VisorGPT, respectively. The degree of similarity between these two distributions can be computed as:

\[\text{KL}(p||q)=p\text{log}(p/q).\] (2)

### Visualization

**Relation Prior of COCO**. Fig. 9 illustrates the comparison between the real and learned relation prior among 80 categories on the COCO dataset. As can be observed, there is a high degree of similarity between the two relation matrices.

**More Visual Comparison**. We provide more comparison of visual prior between the real world and one learned by our VisorGPT and failure cases on COCO dataset in Fig. 11.

**Continuous Generation**. Fig. 12 presents a set of examples showcasing continuous generation based on the current scene. Notably, in each row, the proposed VisorGPT is able to successfully complete a scene that involves many individuals annotated with 14/18 keypoints or objects with bounding boxes, based on the information provided in the corresponding scene depicted in the previous columns.

Figs. 13 and 14 present more visualization results.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Datasets & \#Categories & \#Predicted Seq. & Min \#Seq. Per Category \\ \hline Open Images (Box) & 600 & 48,000 & \textasci{}80 \\ Objects365 (Box) & 365 & 29,200 & \textasci{}80 \\ COCO (Box) & 80 & 6,400 & \textasci{}80 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Details about the predicted sequences for evaluation.

Figure 10: More visual examples.

Figure 9: Relation among 80 categories on COCO.

Figure 11: Comparison of visual prior between the real world and one learned by VisorGPT on COCO dataset.

Figure 12: Examples of continual generation.

Figure 13: Examples of input prompts, output sequences, decoded results and synthetic images.

[MISSING_PAGE_FAIL:19]