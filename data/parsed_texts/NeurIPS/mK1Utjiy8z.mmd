# RClicks: Realistic Click Simulation

for Benchmarking Interactive Segmentation

 Anton Antonov\({}^{1\ast\)\(\boxtimes\)

Andrey Moskalenko\({}^{1,\,2\,\ast}\)

Denis Shepelev\({}^{1\ast}\)

Alexander Krapukhin\({}^{1}\)

Konstantin Soshin\({}^{1}\)

Anton Konushin\({}^{1,\,2}\)

Vlad Shakhuro\({}^{1,\,2\,\dagger}\)

\({}^{1}\)AIRI, Moscow, Russia

\({}^{2}\)Lomonosov Moscow State University

{lastname}@airi.net

\({}^{\ast}\)Equal contribution

\({}^{\dagger}\)Project leader

Corresponding author

https://github.com/emb-ai/rclicks

###### Abstract

The emergence of Segment Anything (SAM) sparked research interest in the field of interactive segmentation, especially in the context of image editing tasks and speeding up data annotation. Unlike common semantic segmentation, interactive segmentation methods allow users to directly influence their output through prompts (e.g. clicks). However, click patterns in real-world interactive segmentation scenarios remain largely unexplored. Most methods rely on the assumption that users would click in the center of the largest erroneous area. Nevertheless, recent studies show that this is not always the case. Thus, methods may have poor performance in real-world deployment despite high metrics in a baseline benchmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing study of click patterns in an interactive segmentation scenario and collected 475K real-user clicks. Drawing on ideas from saliency tasks, we develop a clickability model that enables sampling clicks, which closely resemble actual user inputs. Using our model and dataset, we propose RClicks benchmark for a comprehensive comparison of existing interactive segmentation methods on realistic clicks. Specifically, we evaluate not only the average quality of methods, but also the robustness w.r.t. click patterns. According to our benchmark, in real-world usage interactive segmentation models may perform worse than it has been reported in the baseline benchmark, and most of the methods are not robust. We believe that RClicks is a significant step towards creating interactive segmentation methods that provide the best user experience in real-world cases.

## 1 Introduction

The task of interactive segmentation involves providing additional hints or prompts to the method, allowing it to produce more precise annotations compared to conventional semantic segmentation. The most famous member of interactive segmentation methods is Segment Anything (SAM) [1; 2]. Nowadays, SAM-like methods are applied in various fields, including the thin object segmentation [3; 4], medical segmentation [5; 6; 7; 8; 9; 10], 3D segmentation [11; 12], tracking [13] and video [2].

Typically, interactions occur in several rounds, where in each round the user corrects the prediction errors of the previous one. Evaluation of such methods requires user inputs. However, collecting many real-user inputs for multiple rounds is impractical since such a dataset needs to be rebuilt for every method and every interaction round due to its iterative nature. Thus, researchers often resort to a simple strategy to simulate user inputs. According to this strategy, a single click for each interaction round is generated as follows: (1) select the largest error region in the previous interaction round, and (2) click in the furthest point from the boundaries of this region (center point). Hereinafter, we referto the above click sampling strategy as a _baseline strategy_, following [14]. However, relying solely on this approach may result in overfitting and degraded performance in real-world usage scenarios.

Our goal is to create a highly realistic simulator of user clicks to enable a more accurate evaluation of interactive segmentation methods. We begin our research with a simple observation: when a user clicks, their gaze is focused on the area where they click. In turn, the task of predicting saliency is well-studied, with benchmark data collected using specialized devices such as eye trackers. Saliency prediction models generate spatial attention heatmaps, from which fixation points of viewers can be sampled and utilized. However, saliency models assume a free-viewing task, which differs from interactive segmentation, where the user should segment a specific area. In other words, clicks should be sampled from a spatial distribution, that is conditioned not only on an image but also on a target segmentation area.

Drawing from best practices in interactive segmentation and saliency prediction tasks, we collect a dataset of task-specific user clicks, and propose a model that facilitates sampling of the most realistic click positions in interactive segmentation. Overall, our main contributions are as follows:

* We curate a large multiple-round interaction dataset in the interactive segmentation task (see samples from the first round in Figure 1). To achieve this, we introduce a click collection methodology and conduct an ablation to address presentation bias, involving users on both PCs and mobile devices.
* We introduce a novel click sampling strategy based on a _clickability model_ that can sample more realistic clicks than the _baseline strategy_ and estimate click probabilities.
* a novel benchmark, that leverages the _clickability model_ to estimate the real-world performance of interactive methods. We conduct extensive comparisons and benchmark state-of-the-art methods using both the _baseline strategy_ clicks, and realistic clicks simulated by the _clickability model_. This comparison reveals that benchmarks employing the _baseline strategy_ may overestimate methods' real-world performance. Moreover, we conclude that current segmentation methods are unable to achieve both optimal performance and robustness simultaneously on all datasets.
* We utilize the collected first-round real-user clicks to evaluate the performance of segmentation methods. Furthermore, we propose a methodology to estimate the real-world segmentation difficulty for state-of-the-art methods for each instance in a dataset.

We believe that the proposed methodology enhances comprehension of real-users actions and will facilitate the development of interactive methods that are more applicable in real-world cases.

Figure 1: Examples of real and predicted users’ clicks of interactive segmentation task. The upper row depicts real-users clicks (green) for a given target object (white contour); the middle and bottom rows visualize, correspondingly, clicks and their distribution predicted by our clickability model. Purple points in the middle and bottom rows represent clicks generated by the _baseline strategy_[19]. Mostly baseline click is close to a mode of users’ distribution (see (b) and (e)), however, in some cases it may be far from the mode (e.g. (a), (d)) or may not represent all modes of the distribution (e.g. (c), (e)).

Related Work

### User Input Types in Interactive Segmentation

Various types of user inputs have been explored in the literature. In [15; 20] an initial selection is obtained using bounding boxes, and then refined with strokes. In [21] object selection is done with strokes. [22] considers contours for selecting small objects, minor parts of an object, or a group of objects of the same type. [23] proposes to use trimap, scribblemap or clickmap as an input. Segment Anything, or SAM [1], processes multiple types of user prompts, including a point, a box, a mask, or a text.

Clicks-based approach selects objects of interest according to multiple user clicks (either positive or negative), and was first introduced in [24] and investigated in [19; 25; 26; 27; 1; 3; 28]. We focus solely on the click-based approach, since it is well-explored and has an established evaluation procedure in the field.

### Benchmarking Interactive Segmentation

GrabCut [15] is the first dataset proposed for interactive segmentation task. Then [16] adapted Berkeley [29] segmentation dataset to evaluate interactive segmentation methods, but it required manual testing. However, manual testing is a time-consuming and resource-intensive process. Interactive segmentation expects multiple rounds of interactions, when each interaction depends on previous ones, and it is infeasible to apply manual procedure for larger scales. For these reasons, in practice, benchmarks generate user interactions automatically based on previous interactions.

In [24] authors proposed an automatic clicks generation strategy for evaluation on PASCAL VOC 2012 [30] and COCO [17] segmentation datasets. The subsequent work [31] used DAVIS [18] and SBD [32] datasets for interactive segmentation, applying the same _baseline strategy_.

Most of the existing click-based methods [19; 25; 26; 27; 1; 3; 28] use the _baseline strategy_. However, it has not been validated in real-world usage scenarios until recently. [14] introduced TETRIS benchmark and revealed that real users do not always click in the center of an area with the largest error, as assumed in the _baseline strategy_. Using the adversarial attacks, the paper demonstrated that methods have a tendency to overfit to the _baseline strategy_. Specifically, when the baseline clicks are used, the segmentation quality may be high, but even a slight change in the click position can result in a significant drop in quality. Therefore, the _baseline strategy_ may not accurately estimate the quality of the methods in real usage. We believe that to estimate the actual quality, each click should be generated in accordance with _human perception_.

### Saliency Prediction

The task of saliency prediction aims to model human perception by predicting probability maps [33; 34] of user engagement in a free-view observation for a given media content. Reference data for this task usually comes from a specialized device - an eye tracker - which records eye _fixations_[35; 36; 37]. Subsequently, fixations from multiple viewers are aggregated into a probability distribution through Gaussian at each fixation point, with sigma corresponding to the retinal angle of a human's field of view [35]. Since scaling expensive eye tracker experiments is too complex, several researchers [38; 39] proposed to use mouse movements as a proxy for saliency when training saliency models. However, saliency fixations cannot be directly used in the interactive segmentation task because saliency observers engage in free-viewing, while in our task, the user's goal is to make a click to highlight a specific object or a part of it. Thus, for the interactive segmentation problem, real-user clicks should be collected.

## 3 Users' Clicks Dataset

We propose a novel dataset of real-users clicks for interactive segmentation. Our dataset is based on the existing image segmentation datasets. In total, we collected 475 544 user inputs for GrabCut [15], Berkeley [29], DAVIS [18], COCO-MVal [40], TETRIS [14]. To gather users' clicks, we developed a specialized presentation tool. Specifically, in each task, we asked users to click on the target objects by displaying images and corresponding segmentation masks. We considered several display modesto instruct users what objects should be selected. As interface can cause bias in clicks distribution, we conducted a user study to select the option that best mimics natural user object selections.

This section is organized as follows. First, we present task display modes (3.1). Then we choose one that eliminates bias associated with user viewing mode behavior (3.2). Finally, we describe clicks collection in the first and the subsequent rounds of interactions (3.3).

### Collection Procedure

When collecting user clicks, we executed the following procedure (see Figure 2): (1) Show the entire image for 1.5 seconds. (2) Show segmentation target using one of the Display Modes. (3) The entire image is shown again for 1.5 seconds, during which clicking is not allowed. (4) The user makes a click. Steps (1) and (3) simulate the user behavior during real-world interactive segmentation, when individuals initially view the image and then interact with it by clicking. Step (2) visualizes the object that should be selected by the user.

We considered the following task displaying modes. (a) _Text Description_ mode shows the textual description of the target object for 2.5 seconds. (b) _Object CutOut_ visualizes the target instance in its original position on a gray background for 2 seconds. (c) During _Shifted CutOut_, the target instance is shown for 2 seconds on a gray background, then shifted to the top-left corner, which aims to motivate the assessor to independently locate the instance on the image as its position is shifted. (d) _Silhouette Mask_ shows a black-and-white mask of the target instance in the original position for 2 seconds. (e) _Highlighted Instance_ displays the original image with the background where the target instance is highlighted with a green border. Rationale for our choice of these displaying modes and time periods can be found in Appendix C.1.

### Selecting Unbiased Task Display Mode

_Text Description_ is considered to be unbiased because users do not see the target segmentation mask, as in real-world interactive segmentation. However, textual descriptions may be ambiguous for certain types of instances or areas (see Figure 3). In other display modes, the mask is presented, which could potentially distort the distribution of user clicks. To choose mask-based display mode with minimal bias, we compare all modes with _Text Description_ mode.

Figure 3: Examples of situations where instructing participants with text descriptions may be challenging or ambiguous: selection of a certain instance in the first round ((a)-(b)); and selecting or unselecting a certain error area in the subsequent round ((c)-(d)).

Figure 2: Illustration of the tested display modes to reduce presentation bias. The best result was obtained with the _Object CutOut_ mode, where an object is presented on a gray background without shifts.

Therefore, we conducted an ablation study on 100 randomly selected images from TETRIS dataset [14]. We used images and segmentation masks from TETRIS, and additionally manually annotated textual descriptions. In this study, we compare the clicks obtained via considered display modes with clicks collected through _Text Description_.

Clicks gathering was done on Toloka AI 1 crowdsourcing platform. Each participant was given a batch of 10 unique images, in each image they were required to make one click. Each participant received on average 3 batches of images. Participants did not receive the same image more than once. For every display mode, different people were involved in labeling. A participant's clicks are considered to be valid, if at least 7 out of the 10 clicks in a batch were within the object mask. We also considered a click to be valid whether it was within the object mask or not farther than one click radius from the border. Otherwise, clicks were disregarded as invalid. To select an unbiased presentation strategy, for each image, we collected 25 clicks from participants using computers and 25 clicks from those using mobile devices. After filtering, in total we obtained 47 725 valid clicks.

Footnote 1: https://toloka.ai/

To compare quantitatively display modes with unbiased _Text Description_ mode, we utilized the following sample-based metrics: (a) PL\({}_{1}\) - mean of all pairwise L\({}_{1}\)-distances between click coordinates, normalized by object width and height. (b) WD - Wasserstein distance [41] between click coordinates, normalized by object width and height. (c) KS - Kolmogorov-Smirnov test in 2D case [42, 43, 44]. We conclude that clicks are not significantly different if a p-value is greater than 0.05. The indicator function is used as a metric, which equals 1 when clicks are not significantly different.

The average values w.r.t. images of the listed metrics for each display mode are presented in Table 1. The best results were obtained with the _Object CutOut_ method. Note that we did not use probability map based metrics used for evaluating saliency prediction, because the actual model of clicks distribution is unknown, and we did not want to limit it in the ablation stage. Here, we ablated display modes only for the first interaction rounds. We cannot examine our display modes in the subsequent rounds, as, in addition to the reference instance, the user needs to know which error should be corrected and where it is located, which is not possible to describe textually. Thus, we assume that the best display mode for the first round is also best for the subsequent ones.

In the following, we used _Object CutOut_ mode to collect clicks for the remaining datasets.

### Collected Interactions

We annotated each instance in all common interactive segmentation benchmark datasets - DAVIS [18], GrabCut [15], COCO-MVal [17], Berkeley [29], TETRIS [14] using PC and mobile clicks. Collected clicks were validated similarly to the ablation stage. When annotating subsequent interaction rounds, the user should click in the area of the segmentation error. To obtain error masks for the subsequent rounds, we applied state-of-the-art interactive segmentation methods - SAM [1], SimpleClick [25], and RITM [19] - to all images and all clicks corresponding to those images from the first round. Then, for each image, we selected the mask with the highest quality up to a threshold of 0.95 IoU. We motivate it by the fact that at such a high level of quality, the user is likely to stop annotating the instance as the errors would be minimal, and even the radius of the click may exceed the size of the erroneous area. In total, we collected 475 thousand valid clicks from users. The number of valid clicks annotated for each dataset and interaction round is presented in Table 2.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Display mode & PL\({}_{1}\) \(\downarrow\) & KS \(\uparrow\) & WD \(\downarrow\) \\ \hline _Object CutOut_ & **0.242** & **0.58** & **0.042** \\ _Shifted CutOut_ & 0.246 & 0.56 & 0.046 \\ _Silhouette Mask_ & 0.246 & 0.41 & 0.048 \\ _Highlighted Instance_ & 0.258 & 0.37 & 0.051 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of display modes with _Text Description_.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & First \# & Subseq. \# & Sum \# \\ \hline GrabCut & 2 395 & 3 427 & 5 822 \\ Berkeley & 4 859 & 6 937 & 11 796 \\ DAVIS & 16 975 & 23 687 & 40 662 \\ COCO-MV & 38 097 & 53 926 & 92 023 \\ TETRIS & 123 023 & 202 218 & 325 241 \\ \hline All & 185 349 & 290 195 & 475 544 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The number of collected clicks for each dataset in interaction rounds.

Click Simulation

In this section, we explore models for predicting user clicks. Firstly, the baselines are described (4.1). Secondly, we introduce _a clickability model_ used for click prediction (4.2) in our interactive segmentation benchmark. Thirdly, in (4.3) we describe the construction of training dataset for _clickability model_. Finally, we compare our _clickability model_ with baselines (4.4).

### Baseline Models

As baselines for comparison, we considered uniform, distance, and saliency distribution models. The uniform hypothesis postulates that the clickability of all pixels within the target area is equally distributed (see Figure 4(b)). When the area of interest is relatively small, the uniform assumption is reasonable. However, according to this assumption, the click probability of object boundaries and their centers is equal, which is not necessarily true. The distance transform [14; 19] addresses this issue by assigning greater weight to pixels in the center of the object than to those on the boundary (see Figure 4(c)). Nevertheless, this transform considers only the shape of the object, neglecting human perception. To account for human perception, saliency distribution can be used. This is a reasonable baseline, as users look at the target area of interaction when clicking. For the saliency baseline, we utilized a state-of-the-art model, TranSalNet [45]. The example of constructed saliency distribution is presented in Figure 4(d), details of how such map is constructed can be found in the Appendix B.2. However, saliency models are trained for free-viewing task, and do not take into consideration the setup of our task.

### Clickability Prediction Model

Similar to the saliency prediction task, we formulate the task of simulating user clicks as a probabilistic problem. Given an image, a ground truth object mask, and a segmentation error mask (FP or FN), the model should predict at each pixel the probability of being clicked. We refer to this as a _clickability map_ (see details in Section 4.3). The proposed pipeline is shown in Figure 5. As a base architecture for our model, we adapted state-of-the-art SegNeXt segmentation network [46]. We input the original image into the network and concatenate the ground truth mask with the error mask, feeding the resulting tensor as an additional input to the network, a technique inspired by the Conv1S [19]. We use the Kullback-Leibler divergence (KLD) loss function between the predicted and ground truth distributions.

Figure 4: Examples of considered clickability models: (a) visualizes target object (white contour) and ground-truth clicks (green points); (b) – (d) depict uniform distribution (UD), distance transform (DT), and saliency map (SM) respectively; (e) – our predicted clickability map.

Figure 5: Proposed clickability prediction pipeline.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Model & KS\(\uparrow\) & PL\({}_{1}\)\(\downarrow\) & WD\(\downarrow\) & NSS\(\uparrow\) & PDE\(\uparrow\) \\ \hline UD & 0.10 & 0.57 & 0.17 & 3.99 & 1.36E-05 \\ DT & 0.14 & 0.52 & 0.16 & 6.45 & 2.76E-05 \\ SM & 0.13 & 0.51 & 0.15 & 4.79 & 1.83E-05 \\ Ours & **0.55** & **0.40** & **0.08** & **9.11** & **4.69E-05** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation of various clickability models on real-user clicks of TETRIS validation part. Our approach outperforms existing clicking strategies in terms of the proximity of samples to real-user clicks.

### Clickability Maps Dataset

We introduce the concept of a _clickability map_ as a single-channel image, such that the value of each pixel corresponds to the probability that the user will click on it during the interaction round. We propose to use such maps to train _clickability models_.

Given an image, error mask, and user clicks, the _clickability map_ is constructed as follows: (1) initialize the map as an image of zero values; (2) at each pixel position that was clicked, add one; (3) smooth the map by a Gaussian with some sigma, where sigma is a hyperparameter; (4) multiply pixel values of the map by corresponding pixel values from a _soft error mask_, obtained by smoothing the original error mask by a Gaussian; (5) normalize pixels by the map sum. The proposed method is analogous to the construction of saliency maps from human eye fixations, except for step (4).

Unlike saliency, we need to somehow constrain the most likely click positions within the boundaries of the mask. Moreover, recall that during the collection of clicks, we considered clicks as valid if they were inside the mask or close to its border. For these reasons, in step (4) the _clickability map_ is conditioned by multiplying it on the _soft error mask_ - smoothing the error mask we consider the allowed radius of the border vicinity. We smooth the error mask by Gaussian blurring with a sigma equal to the radius of the click used in the user interface (i.e., 1% of image diagonal). The sigma in step (3) simulates the probability density of clicks inside the mask.

We constructed train and validation datasets as follows. We split images of TETRIS dataset into non-overlapping train and validation parts. Since we do not know the real click density, for model training and validation, several sets of _clickability maps_ were constructed with varying magnitudes of sigma from step (3). To choose the best sigma, we conducted an ablation study, which can be found in Appendix B.3. Note that we constructed _clickability maps_ using clicks from both smartphones and PCs. This was done to ensure that the model would predict clicks regardless of the device type.

### Models Evaluation

To choose the best _clickability model_, we evaluated considered models on the real-user clicks of TETRIS validation part. Here, in addition to sample-based metrics considered above, we calculated additional metrics, that were computed based on the ground-truth clicks positions and predicted clicks distribution: PDE - likelihood of ground-truth clicks, and NSS from saliency benchmarks [47]. Evaluation results are presented in Table 3. Our clickability model shows the best performance. We address the question of model generalizability in the Appendix B.2.

## 5 Benchmarking Interactive Segmentation

In this section, we introduce **RClicks** benchmark that evaluates the interactive segmentation methods according to the proposed _clickability model_. Our evaluation protocol aims to estimate not only the average annotating time but also the spread w.r.t. _clicking groups_. Our model returns a probability density for an instance. For every possible click, we have \((x,y)\) coordinates and probability. We sort clicks according to their probabilities and split them into 10 intervals (called _clicking groups_) \(\{\text{G}_{i}\}_{i=1}^{10}\) s.t. every interval has 10% of total probability mass. We interpret these _groups_ as different user clicking patterns, and evaluate methods for each group separately. Visualization of _clicking groups_ for an instance may be seen in Figure 6. Note, that even the probability mass of each group \(\text{G}_{i}\) is equal, the average probability of clicks in each group increases with increase of \(i\).

We modify a common evaluation protocol [19] by replacing the _baseline sampling strategy_ with sampling from different _clicking groups_, obtained through the _clickability model_. Specifically, (1) interactive segmentation metrics (e.g. NoC) are calculated for every instance in a dataset and group \(\text{G}_{i}\) by sampling click from \(\text{G}_{i}\) (weighted by the _clickability model_) for every interaction round (in our experiments - 20 rounds); (2) then for each instance statistics (e.g. mean and standard deviation) of sampled metrics over _clicking groups_ are estimated; (3) finally, these statistics are averaged over all instances in the dataset.

Figure 6: Spatial distribution of clicking groups for the instance in Figure 4(a).

In interactive segmentation, a commonly used metric is NoC\({}_{20}\)@90, which estimates the annotation time (in clicks, not more than 20) to achieve 90% IoU using a particular method. We use this metric with our sampling strategy to estimate the following averaged statistics: (i) Mean and standard deviation, which are denoted as **Sample** NoC. (ii) Relative increase of Sample NoC compared to the _baseline strategy_ NoC. This statistic indicates how much extra annotation time an average user spends compared to the _baseline strategy_. We denote it as \(\Delta\)SB. (iii) Relative increase of annotation time using clicks from group G\({}_{1}\) over using clicks from group G\({}_{10}\). This metric is denoted as \(\Delta\)**GR**. This metric represents a difference in annotation speed between two _clicking groups_, that have a maximum difference of average clicking probabilities.

Figure 7 shows plots of the averaged IoU versus the number of clicks for various segmentation methods when sampling clicks according to the _baseline strategy_, G\({}_{1}\) and G\({}_{10}\) groups. Overall, clicks from G\({}_{10}\) outperform clicks from G\({}_{1}\) in terms of IoU-AuC, while the _baseline strategy_ mostly outperforms clicks from both groups. With a sufficient number of interactions, clicks from both G\({}_{10}\) and G\({}_{1}\) achieve high IoU. However, clicks from G\({}_{1}\) require more interactions.

Estimated statistics for NoC\({}_{20}\)@90 are presented in Table 4. Additional evaluation results for NoF\({}_{20}\)@90, IoU-AuC\({}_{20}\) are provided in Appendix D.

In addition to the evaluation results on simulated clicks, we provide evaluation results on the first round real clicks. The evaluation on the subsequent real clicks is infeasible, since interactive segmentation in a subsequent round depends on a model output from a previous round. However, there is no such problem in the first round, and actual performance metrics can be computed on the real clicks of the first round. Therefore, we employed the real clicks from the first round as follows: (1) computed real-world accuracy (see Appendix D); (2) compared accuracy of interactive segmentation methods using real and simulated clicks (see Appendix D); and (3) estimated real-world robustness of the methods for each instance in the dataset. We estimated the latter through IoU noise-to-signal ratio (NSR). The greater the value of NSR, the more difficult such an instance is for segmentation. Figure 8 plots a scatter of mean vs. standard deviation of IoU over real-users first clicks and segmentation methods from Table 4.

Figure 8: A scatter plot of the mean vs. standard deviation (STD) of IoU for the first real-users clicks. Each point represents the statistics for each instance, averaged across all considered segmentation methods and real clicks. An average NSR for each dataset is provided in brackets in the legend.

Figure 7: Mean IoU for varying number of clicks for _baseline strategy_, G\({}_{1}\) and G\({}_{10}\)_clicking groups_. IoU-AuC under these curves provided in brackets.

## 6 Discussion

A review of Table 4, Figures 7 and 8 leads to the following conclusions. First, according to \(\Delta\)SB, _baseline strategy_ underestimates the real-world annotation time from 5% up to 29%. This implies that the baseline benchmark may significantly underestimate the real-world annotation costs. Consequently, our benchmark may be employed for a more accurate estimation of annotation costs.

Then, according to \(\Delta\)GR, **annotation time of users from different _clicking groups_ varies from 3% up to 79%. The observed variations in \(\Delta\)GR indicate that segmentation methods are unstable w.r.t. click positions in the image.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Method} & \multirow{3}{*}{Backbone} & \multirow{3}{*}{Data} & \multicolumn{3}{c}{DAVIS [18]} & \multicolumn{3}{c}{COCO-MVal [17]} & \multicolumn{3}{c}{TETRIS [14]} \\  & & & \multicolumn{3}{c}{NoC\({}_{20}\)@90} & \multicolumn{3}{c}{NoC\({}_{20}\)@90} & \multicolumn{3}{c}{NoC\({}_{20}\)@90} \\  & & & \multicolumn{3}{c}{Sample} & \(\Delta\)SB & \(\Delta\)GR & Sample & \(\Delta\)SB & \(\Delta\)GR & Sample & \(\Delta\)SB & \(\Delta\)GR \\  & & & (\(\pm\)std) & (\(\pm\)\%) & (\(\pm\)std) & (\(\pm\)\%) & (\(\pm\)std) & (\(\pm\)\%) & (\(\pm\)std) & (\(\pm\)\%) & (\(\pm\)\%) \\ \hline GPCIS [27] & RN50 & C+L & 6.44±0.85 & 16.88 & 53.65 & 4.74±1.31 & 26.43 & 79.00 & 3.87±0.79 & 19.55 & 56.43 \\ \hline \multirow{3}{*}{CDNet [48]} & RN34 & C+L & 5.95±0.73 & 14.95 & 45.88 & 4.13±0.85 & 15.15 & 49.79 & 3.10±0.53 & 14.16 & 44.06 \\  & RN34 & SBD & 7.87±1.25 & 23.39 & 64.95 & 6.36±1.29 & 20.86 & 51.58 & 4.51±0.76 & 17.08 & 55.22 \\ \hline \multirow{3}{*}{RITM [19]} & HR18 & C+L & 6.23±0.67 & 6.92 & 16.13 & 3.71±0.78 & 10.27 & 20.22 & 3.69±0.52 & 7.02 & 13.95 \\  & HR188-IT & C+L & 6.71±0.99 & 20.88 & 54.15 & 3.65±0.92 & 16.81 & 33.89 & 3.80±0.67 & 15.79 & 32.66 \\  & HR18-IT & C+L & 6.15±0.83 & 11.37 & 31.14 & 3.22±0.83 & 15.84 & 37.01 & 3.48±0.60 & 11.59 & 23.99 \\  & HR32-IT & C+L & 5.90±0.89 & 18.34 & 51.07 & 3.24±0.83 & 15.50 & 37.31 & 3.44±0.65 & 17.47 & 30.69 \\  & HR18-IT & SBD & 7.42±1.03 & 17.85 & 38.39 & 4.81±1.24 & 17.17 & 43.23 & 4.80±0.74 & 11.96 & 25.31 \\ \hline \multirow{3}{*}{AdaptClick [49]} & ViT-B & C+L & 4.97±0.40 & 8.60 & 15.14 & 2.93±0.58 & 9.44 & 19.75 & 2.62±0.37 & 6.99 & 12.94 \\  & ViT-B & SBD & 5.37±0.49 & 8.69 & 17.94 & 4.33±1.06 & 14.59 & 35.07 & 3.49±0.50 & 8.40 & 16.44 \\ \hline \multirow{3}{*}{SimpleClick [25]} & ViT-B & C+L & 5.32±0.54 & 9.05 & 26.33 & 3.07±0.70 & 11.72 & 23.60 & 2.73±0.41 & 8.86 & 16.64 \\  & ViT-L & C+L & 5.03±0.42 & 8.71 & 16.67 & 2.67±0.56 & 8.05 & 20.88 & _2.46±0.35_ & 7.11 & 10.01 \\  & ViT-H & C+L & 5.00±0.42 & 7.06 & 12.29 & **2.57±0.54** & 6.14 & 17.65 & 2.36±0.33 & 6.94 & 10.83 \\  & ViT-XT & SBD & 8.35±1.36 & 18.67 & 51.05 & 5.86±1.65 & 26.28 & 61.63 & 5.49±1.22 & 28.57 & 35.40 \\  & ViT-B & SBD & 5.77±0.58 & 8.44 & 25.72 & 4.52±1.07 & 17.58 & 36.18 & 3.75±0.54 & 11.42 & 19.50 \\  & ViT-L & SBD & 5.56±0.53 & 7.26 & 15.97 & 3.83±0.88 & 10.02 & 33.06 & 3.40±0.43 & 7.32 & 16.15 \\  & ViT-H & SBD & 5.49±0.55 & 7.67 & 23.69 & 3.74±0.86 & 10.46 & 31.97 & 3.32±0.43 & 7.37 & 16.88 \\ \hline CFR-ICL [26] & ViT-H & C+L & _4.53±0.46_ & 9.32 & 18.47 & 2.70±_0.63_ & 9.58 & 24.13 & **2.12±0.34** & 8.76 & 14.33 \\ \hline MobileSAM [28] & ViT-Tiny & SA-1B & 5.96±0.56 & 8.63 & 15.39 & 5.25±0.78 & 9.79 & 19.78 & 3.42±0.48 & 7.47 & 12.69 \\ \hline \multirow{3}{*}{SAM [1]} & ViT-B & SA-1B & 5.30±0.53 & 8.26 & _11.27_ & 4.91±0.79 & 9.88 & 15.73 & 3.04±0.51 & 11.17 & 10.06 \\  & ViT-L & SA-1B & 5.21±0.41 & 8.82 & 11.59 & 4.81±0.63 & 8.89 & 14.97 & 2.60±0.40 & 8.11 & 7.08 \\  & ViT-H & SA-1B & 5.42±0.49 & 8.00 & 15.02 & 5.14±0.68 & 7.63 & 15.61 & 2.66±0.38 & **5.95** & 8.50 \\ \hline \multirow{3}{*}{SAM-HQ [3]} & ViT-B & SA-1B & 5.32±0.50 & 7.45 & 13.03 & 5.39±0.89 & 12.48 & 16.68 & 3.35±0.67 & 16.41 & 10.74 \\  & ViT-L & SA-1B & 5.19±0.48 & 8.58 & 15.69 & 5.05±0.74 & 9.64 & 13.50 & 2.81±0.51 & 11.02 & 7.69 \\  & ViT-H & SA-1B & 5.16±0.44 & 8.15 & 18.36 & 4.97±0.68 & 7.71 & _12.36_ & 2.75±0.41 & _6.78_ & 7.95 \\ \hline \multirow{3}{*}{SAM 2[2]} & Hiera-T & SA-V & 4.65±0.28 & **4.86** & **7.46** & 3.86±0.64 & 7.79 & 13.14 & 3.11±0.50 & 9.45 & 3.57 \\  & Hiera-B & SA-V & 4.67±0.33 & 8.49 & 15.86 & 3.75±0.61 & _7.44_ & 12.67 & 3.02±0.47 & 9.51 & _4.79_ \\ \cline{1-1}  & Hiera-L & SA-V & _4.61±0.29_ & 9.51 & 13.28 & 3.84±0.62 & 9.12 & 12.35 & 2.83±0.41 & 7.46 & _4.10_ \\ \cline{1-1}  & Hiera-H & SA-V & **4.39±0.23** & 7.55 & 10.03 & 3.42±0.51According to Sample and \(\Delta\)GR values, the best annotation time is achieved by SAM 2 Hiera-H (on DAVIS), CFR-ICL (on TETRIS) and SimpleClick ViT-H (on COCO-MVal). The two latter methods are less robust compared to SAM-like methods, which perform more consistently across _clicking groups_. However, SAM 2 Hiera-H backbone is less robust than Hiera-T backbone on DAVIS dataset. This indicates that **there is currently no segmentation method that is optimal in terms of both performance and robustness on all datasets**. Consequently, **developers should select a method in accordance with their requirements**.

Finally, points in the bottom-right part of Figure 8 correspond to instances with high NSR. These values may be utilized **to identify and analyze hard instances in the datasets**. Additionally, according to averaged NSR, we **identified the hardest dataset for annotation**, it is DAVIS with 24.15 NSR.

## 7 Conclusion

In this paper, we presented RClicks - a benchmark for interactive segmentation methods, that evaluates both real-world quality and robustness with respect to different clicking patterns. Using the developed unbiased presentation strategy, we collected the multi-round real-user click dataset. We developed the _clickability model_ that can be utilized to estimate click probabilities and sample realistic user clicks. By employing this model in our benchmark, we demonstrated that _baseline strategy_ may overestimate methods' performance in the real world. Furthermore, our analysis showed that there is currently no interactive segmentation method that is optimal in terms of both performance and robustness on all datasets. Additionally, we evaluated segmentation methods using real-user clicks of the first round and proposed a methodology to estimate the instance difficulty for state-of-the-art methods. We hope RClicks will facilitate the advancement of interactive segmentation methods that provide optimal user experiences in real-world scenarios.

## References

* [1] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
* [2] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Rvali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. _arXiv preprint arXiv:2408.00714_, 2024.
* [3] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. _arXiv preprint arXiv:2306.01567_, 2023.
* [4] Zhaozhi Xie, Bochen Guan, Weihao Jiang, Muyang Yi, Yue Ding, Hongtao Lu, and Lei Zhang. Pa-sam: Prompt adapter sam for high-quality image segmentation. _2024 IEEE International Conference on Multimedia and Expo (ICME)_, 2024.
* [5] Chuanfei Hu, Tianyi Xia, Shenghong Ju, and Xinde Li. When sam meets medical images: An investigation of segment anything model (sam) on multi-phase liver tumor segmentation, 2023.
* [6] Tao Zhou, Yizhe Zhang, Yi Zhou, Ye Wu, and Chen Gong. Can sam segment polyps?, 2023.
* [7] Risab Biswas. Polyp-sam++: Can a text guided sam perform better for polyp segmentation?, 2023.
* [8] Yichi Zhang and Rushi Jiao. How segment anything model (sam) boost medical image segmentation? _arXiv preprint arXiv:2305.03678_, 2023.
* [9] Yichi Zhang, Zhenrong Shen, and Rushi Jiao. Segment anything model for medical image segmentation: Current applications and future directions. _Computers in Biology and Medicine_, 171:108238, 2024.

* [10] Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, and Yixin Zhang. Segment anything model for medical image analysis: an experimental study. _Medical Image Analysis_, 89:102918, 2023.
* [11] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. Sam3d: Segment anything in 3d scenes, 2023.
* [12] Muitan Xu, Xingyilang Yin, Lingteng Qiu, Yang Liu, Xin Tong, and Xiaoguang Han. Sampro3d: Locating sam prompts in 3d for zero-shot scene segmentation. _arXiv preprint arXiv:2311.17707_, 2023.
* [13] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. _arXiv preprint arXiv:2305.06558_, 2023.
* [14] Andrey Moskalenko, Vlad Shakhuro, Anna Vorontsova, Anton Konushin, Anton Antonov, Alexander Krapukhin, Denis Shepelev, and Konstantin Soshin. Tetris: Towards exploring the robustness of interactive segmentation. _arXiv preprint arXiv:2402.06132_, 2024.
* interactive foreground extraction using iterated graph cuts. _ACM transactions on graphics (TOG)_, 23(3):309-314, 2004.
* [16] Kevin McGuinness and Noel E O'connor. A comparative evaluation of interactive segmentation algorithms. _Pattern Recognition_, 43(2):434-444, 2010.
* [17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.
* [18] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In _CVPR_, 2016.
* [19] Konstantin Sofiiuk, Ilya A. Petrov, and Anton Konushin. Reviving iterative training with mask guidance for interactive segmentation. In _2022 IEEE International Conference on Image Processing (ICIP)_, pages 3141-3145, 2022. doi: 10.1109/ICIP46576.2022.9897365.
* [20] Eirikur Agustsson, Jasper R. R. Uijlings, and Vittorio Ferrari. Interactive full image segmentation by considering all regions jointly. In _CVPR_, 2019.
* [21] Houssem-Eddine Gueziri, Michael McGuffin, and Catherine Laporte. Latency management in scribble-based interactive segmentation of medical images. _IEEE Transactions on Biomedical Engineering_, 2017.
* [22] Polina Popenova, Danil Galeev, Anna Vorontsova, and Anton Konushin. Contour-based interactive segmentation. In _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence_, pages 1322-1330, 2023.
* [23] Hang Cheng, Shugong Xu, Xiufeng Jiang, and Rongrong Wang. Deep image matting with flexible guidance input. In _BMVC_, 2021.
* [24] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep interactive object selection. In _CVPR_, 2016.
* [25] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Simpleclick: Interactive image segmentation with simple vision transformers. _arXiv preprint arXiv:2210.11006_, 2022.
* [26] Shoukun Sun, Min Xian, Fei Xu, Tiankai Yao, and Luca Capriotti. Cfr-icl: Cascade-forward refinement with iterative click loss for interactive image segmentation. _arXiv preprint arXiv:2303.05620_, 2023.

* Zhou et al. [2023] Minghao Zhou, Hong Wang, Qian Zhao, Yuexiang Li, Yawen Huang, Deyu Meng, and Yefeng Zheng. Interactive segmentation as gaussian process classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19488-19497, 2023.
* Zhang et al. [2023] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. _arXiv preprint arXiv:2306.14289_, 2023.
* Martin et al. [2001] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _ICCV_, 2001.
* Everingham et al. [2012] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results, 2012.
* Li et al. [2018] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image segmentation with latent diversity. In _CVPR_, 2018.
* Hariharan et al. [2011] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In _ICCV_, 2011.
* Lou et al. [2022] Jianxun Lou, Hanhe Lin, David Marshall, Dietmar Saupe, and Hantao Liu. Transalnet: Towards perceptually relevant visual saliency prediction. _Neurocomputing_, 2022. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2022.04.080.
* Kroner et al. [2020] Alexander Kroner, Mario Senden, Kurt Driessens, and Rainer Goebel. Contextual encoder-decoder network for visual saliency prediction. _Neural Networks_, 129:261-270, 2020. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2020.05.004. URL http://www.sciencedirect.com/science/article/pii/S0893608020301660.
* Judd et al. [2009] Tilke Judd, Krista Ehinger, Fredo Durand, and Antonio Torralba. Learning to predict where humans look. In _2009 IEEE 12th International Conference on Computer Vision_, pages 2106-2113, 2009. doi: 10.1109/ICCV.2009.5459462.
* Judd et al. [2012] Tilke Judd, Fredo Durand, and Antonio Torralba. A benchmark of computational models of saliency to predict human fixations. In _MIT Technical Report_, 2012.
* Borji and Itti [2015] Ali Borji and Laurent Itti. Cat2000: A large scale fixation dataset for boosting saliency research. _CVPR 2015 workshop on "Future of Datasets"_, 2015. arXiv preprint arXiv:1505.03581.
* Jiang et al. [2015] Ming Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao. Salicon: Saliency in context. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1072-1080, 2015.
* Tavakoli et al. [2017] Hamed R Tavakoli, Fawad Ahmed, Ali Borji, and Jorma Laaksonen. Saliency revisited: Analysis of mouse movements versus fixations. In _Proceedings of the ieee conference on computer vision and pattern recognition_, pages 1774-1782, 2017.
* Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* Peyre and Cuturi [2020] Gabriel Peyre and Marco Cuturi. Computational optimal transport, 2020.
* Christian [2020] Sam Christian. Re-examining the evidence of the Hercules-Corona Borealis Great Wall. _Monthly Notices of the Royal Astronomical Society_, 495(4):4291-4296, 05 2020. ISSN 0035-8711. doi: 10.1093/mnras/staa1448.
* Peacock [1983] John A Peacock. Two-dimensional goodness-of-fit testing in astronomy. _Monthly Notices of the Royal Astronomical Society_, 202(3):615-627, 1983.

* [44] Giovanni Fasano and Alberto Franceschini. A multidimensional version of the kolmogorov-smirnov test. _Monthly Notices of the Royal Astronomical Society_, 225(1):155-170, 1987.
* [45] Jianxun Lou, Hanhe Lin, David Marshall, Dietmar Saupe, and Hantao Liu. Transalnet: Towards perceptually relevant visual saliency prediction. _Neurocomputing_, 494:455-467, 2022.
* [46] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic segmentation. _arXiv preprint arXiv:2209.08575_, 2022.
* [47] Matthias Kummerer, Thomas S. A. Wallis, and Matthias Bethge. Saliency benchmarking made easy: Separating models, maps and metrics. In _Proceedings of the European Conference on Computer Vision (ECCV)_, September 2018.
* [48] Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7345-7354, 2021.
* [49] Jiacheng Lin, Jiajun Chen, Kailun Yang, Alina Roitberg, Siyu Li, Zhiyong Li, and Shutao Li. Adaptiveclick: Click-aware transformer with adaptive focal loss for interactive image segmentation. _IEEE Transactions on Neural Networks and Learning Systems_, 2024.
* [50] Thanos Delatolas, Vicky Kalogeiton, and Dim P Papadopoulos. Learning the what and how of annotation in video object segmentation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 6951-6961, 2024.
* [51] Dim P. Papadopoulos, Alasdair D. F. Clarke, Frank Keller, and Vittorio Ferrari. Training object class detectors from eye tracking data. In _European Conference on Computer Vision_, 2014. URL https://api.semanticscholar.org/CorpusID:14119147.
* [52] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio Ferrari. Training object class detectors with click supervision. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 6374-6383, 2017.
* [53] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What's the point: Semantic segmentation with point supervision. In _European conference on computer vision_, pages 549-565. Springer, 2016.
* [54] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume III, and Kate Crawford. Datasheets for datasets. volume 64, pages 86-92, 2021.

Benchmark Discussion

**Access.** The benchmark is publicly available at https://github.com/emb-ai/rclicks repository.

**License.** Clicks are under CC BY-NC 4.0, evaluation code and baseline models are under MIT.

**Ethical issues.** The benchmark is created for testing interactive segmentation methods. To the best of our knowledge, interactive segmentation has two applications: image editing and assisted image labeling. Since interactive segmentation methods still do require user input, we believe their emergence will not make image labeling and image editing jobs redundant.

**Limitations of work.** We observe the following major limitations of our benchmark:

1. To estimate the sample statistic, we split clicks into 10 pattern groups and sample 1 click from each group per round (10 samples per round). While increasing the number of sampled clicks improves performance estimations per image, we believe that, for a sufficiently large dataset, 10 sampled clicks per round is sufficient to accurately estimate average performance over the dataset.
2. We trained our _clickability model_ on the TETRIS subset. The click patterns observed in our study may be domain-specific to the train images. Different types of images (e.g., medical images, satellite images) might result in different click behaviors that our model does not account for.
3. Our dataset of clicks was collected from an online annotation platform, which means our model may reflect biases specific to the individuals who participated in this platform. This could include biases related to their annotation habits, demographic backgrounds, or other unmeasured factors, potentially affecting the model's performance when applied to a wider or different user base.
4. Our model and benchmark are based on click interactions, but interactive segmentation can involve other modalities such as scribbles, contours, or voice commands. We have not considered these types of inputs.

To overcome Limitation 1, we would need to evaluate the model for every possible click position, which is computationally impossible.

We believe that Limitation 2 is inevitable due to the fact that we cannot annotate all possible image scenarios.

Limitation 3 cannot be avoided because it is not feasible to obtain a fully representative population of all people with different patterns.

To overcome Limitation 4, it would be necessary to initiate a separate research project dedicated to studying different input types for interactive segmentation. Unfortunately, this is beyond the scope of this paper.

## Appendix B Clickability Model

### Training and model details

In this subsection, we provide _clickability model_ architecture description and its training details.

**Architecture details.** As a predictor, we adapted the state-of-the-art SegNeXt-B segmentation network [46] with the MSCAN-B backbone. We input the original image into the network and concatenate the ground truth mask with the error mask, feeding the resulting tensor as an additional input to the network. This technique is inspired by Conv1S [19]. However, we used three convolutional layers with non-linear activation functions instead of one, as we needed to encode more complex features of the ground truth mask and error mask, rather than just clicks. Additionally, after a forward pass, we used min-max normalization to transform values to the interval from 0 to 1.

To compare our model's complexity with state-of-the-art interactive segmentation methods, we measured inference speed and resource usage of the state-of-the-art segmentation methods and our clickability model in Table 5. All evaluations were done on a single A100. The _clickability model_ is about twice as fast as RITM and SimpleClick, and about five times faster than SAM. It also uses a small or comparable amount of GPU memory. Note that our model can easily fit on a consumer GPU with 8GB of VRAM.

**Training details.** We trained our model for 20 epochs on the train part of TETRIS dataset (TETRIS splits can be found in txt-file of the benchmark code). During training, samples were augmented using Horizontal Flip. We minimized Kullback-Leibler Divergence Loss by Adam optimizer with CosineLRScheduler and initial 0.01 learning rate. The training process took 3 hours on a single Nvidia Tesla A100 GPU.

### Model generalizability

To assess the generalizability of our _clickability model_, we computed metrics on click samples for different datasets in Table 6.

Note, that in Tables 3, 6 and 7 KS, PL\({}_{1}\), WD were calculated using clicks bootstrapping from _clickability model_ (100 times per instance).

Additionally, to calculate saliency fairly, we assume the model should receive not the full image, but only a cropped region of the image with the object of interest. This ensures that the model's attention is directed towards the relevant area, leading to a more accurate assessment of its saliency. We have checked two variants of giving image with object of interest:

1. Part of image with expand around the object of interest on 1.4 times (denoted as SI in Table 6).
2. Part of image with expand around the object of interest on 1.4 times, but everything except the object of interest is gray (denoted as SM in Table 6).

According to Table 6, our clickability model significantly outperforms all baselines on TETRIS and other datasets. On other datasets the click sampling quality of our model mostly is slightly worse than on TETRIS, nevertheless we find these results comparable. Therefore, we conclude that the clickability model is not limited to TETRIS dataset and generalizes well to other datasets of common images.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Model & Params, M & GFLOPs & Mem, Gb & Inference, ms & Input size \\ \hline RITM HRNet-32 & 31.95 & 83 & 0.217 & 42 \(\pm\) 2.0 & 400 \(\times\) 400 \\ SimpleClick-ViT-H & 659.39 & 1461 & 2.785 & 41 \(\pm\) 2.3 & 448 \(\times\) 448 \\ SAM-ViT-H & 641.09 & 5473 & 5.745 & 150 \(\pm\) 2.7 & 1024 \(\times\) 1024 \\ Clickability model & 27.59 & 64 & 0.378 & 23 \(\pm\) 0.9 & 416 \(\times\) 416 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of the characteristics of different interactive segmentation methods with the clickability model.

However, datasets from specific domains, e.g. medical, are out of the scope of our work. For these datasets, additional data collection and evaluations should be done.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset & Model & KS \(\uparrow\) & PL\({}_{1}\downarrow\) & WD \(\downarrow\) & NSS \(\uparrow\) & PDE \(\uparrow\) \\ \hline \multirow{6}{*}{GrabCut} & UD & 0.15 & 0.55 & 0.16 & 2.44 & 4.06E-05 \\  & DT & 0.22 & 0.49 & 0.14 & 3.94 & 7.75E-05 \\  & SI & 0.10 & 0.52 & 0.17 & 2.27 & 4.64E-05 \\  & SM & 0.14 & 0.52 & 0.16 & 2.75 & 5.30E-05 \\  & Ours & **0.50** & **0.40** & **0.10** & **5.63** & **2.12E-04** \\ \hline \multirow{6}{*}{Berkeley} & UD & 0.16 & 0.54 & 0.16 & 3.39 & 1.20E-04 \\  & DT & 0.23 & 0.48 & 0.14 & 5.28 & 2.13E-04 \\  & SI & 0.11 & 0.51 & 0.17 & 3.08 & 1.38E-04 \\  & SM & 0.17 & 0.51 & 0.15 & 3.69 & 1.50E-04 \\  & Ours & **0.50** & **0.39** & **0.09** & **7.29** & **4.99E-04** \\ \hline \multirow{6}{*}{DAVIS} & UD & 0.15 & 0.56 & 0.17 & 4.31 & 6.49E-05 \\  & DT & 0.25 & 0.49 & 0.14 & 6.68 & 1.21E-04 \\  & SI & 0.11 & 0.53 & 0.17 & 3.96 & 7.22E-05 \\  & SM & 0.16 & 0.53 & 0.16 & 4.50 & 7.84E-05 \\  & Ours & **0.48** & **0.41** & **0.10** & **8.55** & **2.53E-04** \\ \hline \multirow{6}{*}{COCO-MVal} & UD & 0.20 & 0.79 & 0.21 & 6.08 & 4.04E-04 \\  & DT & 0.26 & 0.73 & 0.19 & 8.34 & 5.74E-04 \\ \cline{1-1}  & SI & 0.16 & 0.73 & 0.22 & 6.26 & 5.12E-04 \\ \cline{1-1}  & SM & 0.21 & 0.74 & 0.20 & 6.80 & 5.14E-04 \\ \cline{1-1}  & Ours & **0.50** & **0.61** & **0.14** & **10.44** & **7.29E-04** \\ \hline \multirow{6}{*}{TETRIS (Val)} & UD & 0.10 & 0.57 & 0.17 & 3.99 & 1.36E-05 \\  & DT & 0.14 & 0.52 & 0.16 & 6.45 & 2.76E-05 \\ \cline{1-1}  & SI & 0.09 & 0.51 & 0.16 & 4.13 & 1.66E-05 \\ \cline{1-1}  & SM & 0.13 & 0.51 & 0.15 & 4.79 & 1.83E-05 \\ \cline{1-1}  & Ours & **0.55** & **0.40** & **0.08** & **9.11** & **4.69E-05** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Evaluation of various _clickability models_ on real-user clicks. Our model was trained on the train part of TETRIS dataset. Our approach outperforms baseline clicking strategies in terms of the proximity of samples to real-user clicks on all considered interactive segmentation datasets.

[MISSING_PAGE_EMPTY:17]

Figure 10: Examples of ground truth clicks (left), clicks (middle) and _clickability map_ (right), generated by our _clickability model_ for the first round. Green dots illustrate first round clicks, teal masks represent target regions that should be segmented. In the subcaptions the results of Kolmogorov-Smirnov test (True if p-value > 0.05, i.e. there are no significant differences between the distributions of clicks) are provided.

Figure 11: Examples of false-positive (FP) ground truth clicks (left), generated clicks (middle) and predicted _clickability map_ (right) by our _clickability model_ for the subsequent round. Red dots illustrate FP clicks, red masks represent regions of segmentation errors. In the subcaptions the results of Kolmogorov-Smirnov test (True if p-value > 0.05, i.e. there are no significant differences between the distributions of clicks) are provided.

Figure 12: Examples of false-negative (FN) ground truth clicks (left), generated clicks (middle) and predicted _clickability map_ (right) by our _clickability model_ for the subsequent round. Green dots illustrate FN clicks, teal masks represent target regions that should be segmented. In the subcaptions the results of Kolmogorov-Smirnov test (True if p-value > 0.05, i.e. there are no significant differences between the distributions of clicks) are provided.

Clicks Dataset

### Collection procedure

Before starting data collection, we considered various ways to visually present the mask and image to a person. These are all the methods we found reasonable.

Here is a brief story behind choosing display mode:

1. The most unbiased display mode to instruct a participant is considered to be Text Description. By avoiding any visual display of the mask, we eliminate potential biases. However, we could not use it to annotate all images because such descriptions can be ambiguous. We also cannot use this method for subsequent rounds because we need to indicate the part of the mask that was not segmented in the previous round.
2. The next display mode we tried was to show the ground truth black-and-white mask (Silhouette Mask). In that case, we simply show the mask as is without any modifications. We collected the first batch of data and noticed that participants tend to click to the geometric center of the object. Therefore, this strategy seemed to be biased. We started to look for the least unbiased one.
3. To prevent people from selecting the geometric center of the object, we added Object CutOut mode. We hypothesize that it is harder to be biased on the geometric form of an object when it retains its appearance.
4. After that, we tried Shifted CutOut mode. The idea of this mode was to eliminate dependency on initial cursor position. In that display mode, a person has to move the cursor to the object.
5. We also tried Highlighted Instance mode, since it's a natural way to highlight an object retaining its' background. We hypothesized that object background may influence click position.

In determining the optimal display duration for our dataset collection, we conducted a thorough review of existing literature on annotation time requirements. A number of studies have been conducted on this subject [50] (Sec. 4), [2] (Sec. E.1.2), [51] (Sec. 2), [52] (Sec. 3.4), [53] (Sec. 4.1). Researchers came to the conclusion that the required time plan formula is the following: 1 sec. for the annotator to visually locate an object and 1.5 sec. for adding each click, while total reported time to localize and click may vary between 1.87 sec. and 2.5 sec. Since, we did not have the goal of speeding up the labeling of RClicks, but to collect high-quality data, we increased times as follows: showing an image for 1.5 sec., different mask displaying modes for 2 sec. (since time is needed to localize and remember the target object) and Text Description mode for 2.5 sec. (since more time is needed to read and understand the text).

To annotate datasets, we used the toloka.ai crowdsourcing platform. Each crowd worker was paid $0.02 for 10 clicks. On average, workers made 13 clicks per minute, earning approximately $1.5 per hour. This level is above minimum hour wage for the countries the annotators were from. Overall we spent about 1300$ on annotation.

According to the user agreement and privacy policy of Toloka, personal data typically includes information that can identify an individual, such as name, contact information, and other personal identifiers. Annotators clicks on images do not fall under this category. Moreover, we provide fully anonymized data, that can not be linked with people who clicked. Toloka's policy allows for the sharing of anonymized data with third parties. If the collected click data is anonymized and cannot be traced back to individual annotators, it may be shared with third parties without violating the privacy terms.

Here is an example of an instruction, which annotators saw, for Object CutOut mode:

## Introduction

* After opening the task, there will be an image loading period of approximately 30 seconds.
* After clicking the "START" button, an image will be shown for 1.5 seconds, followed by a demonstration of the object of interest on a gray background for 2 seconds. Note that during this time, you cannot click on the object! Example: 
* Then you will see the original image again, which will displayed for 1.5 second. Your task is to select the object that was indicated in the previous step with one click.
* To successfully complete the task, it is necessary to process 10 images in sequence.

[MISSING_PAGE_EMPTY:23]

Additional Benchmark Results

### Evaluation setup

We benchmarked 11 methods with 33 checkpoints. To do this, we spent 2400 GPU hours, which is equivalent to approximately 6 days of compute using 16 NVIDIA Tesla A100 GPUs.

### Additional evaluation results

Tables 9, 10, 11 present evaluation results on simulated user clicks of various interactive segmentation algorithms on GrabCut, Berkeley, DAVIS, COCO-MVal and TETRIS datasets for NoC\({}_{20}\)@90, NoF\({}_{20}\)@90 and IoU-AuC\({}_{20}\) respectively. Table 12 presents the evaluation results on the first round real and simulated user clicks.

**Base** statistics represent the performance of the methods according to baseline benchmark. In addition to the statistics presented in the main paper, we calculated \(\Delta\)**HH** statistic - this statistic is based on \(\Delta\)GR, but for \(\Delta\)HH we merge intervals \(\{\mathbf{G}_{i}\}_{i=1}^{5}\) and \(\{\mathbf{G}_{i}\}_{i=6}^{10}\) s.t. every interval has 50% of total probability mass. Moreover, we calculated noise-signal-ratio (**NSR**) of IoU averaged over the datasets instances.

Figure 14 shows the correlations between different metrics. From this, we can draw several conclusions:

1. There is a strong correlation between Sample and User performances on the first click, as seen in Sample IoU@1 and User IoU@1, as well as Sample NSR IoU@1 and User NSR IoU@1. This demonstrates that performance on real and simulated clicks is very similar.
2. There is a weak correlation between performance on the first and twentieth clicks, as seen in the Sample IoU@1 with Sample IoU@20 and Base IoU@1 with Base IoU@20. This likely indicates that all models converge by the 20th round.
3. There is a weak correlation between robustness metrics and average model performance. This is observed in: \(\Delta\)GR NoC with (Base NoC@20 and Sample NoC@20), \(\Delta\)HH NoC with (Base NoC@20 and Sample NoC@20), \(\Delta\)GR IoU with (Base IoU@20 and Sample IoU@20), and \(\Delta\)HH IoU with (Base IoU@20 and Sample IoU@20). This suggests that high-performance methods may not be robust.

Figure 14: PearsonSpearman correlations of considered averaged performance and robustness metrics statistics averaged over all instances in all datasets.

4. Performance measures have high correlations with each other, as seen in the cross-correlations between Sample NoC, Sample IoU, Sample NoF and Base NoC, Base IoU, Base NoF.
5. The same phenomenon is observed with robustness metrics, demonstrated by the cross-correlations between \(\Delta\)HH NoC, \(\Delta\)GR NoC and \(\Delta\)HH IoU, \(\Delta\)GR IoU.

## Appendix A

[MISSING_PAGE_EMPTY:28]

### Finding hard instances for the first round using real-user clicks

Using Figure 8 we depict a first-round real-world robustness of the methods for each instance in the datasets. Obtained mean and standard deviation (STD) of IoU we estimated NSR for each sample. Ideally, for robust interactive segmentation methods, all instances should have low NSR, e.g. high mean and low STD of IoU. The instances with high NSR can be considered as hard cases for methods in the first round.

Figures 15 and 16 illustrate the hard (high NSR) and simple (low NSR) cases for RITM HRNet32-IT (C+L) [19], SimpleClick ViT-H (C+L) [25], SAM ViT-H (SA1-B) [1], SAM-HQ ViT-H (SA1-B) [3]. For each method, the uncertainty of prediction is depicted by the mean mask averaged over the method's predictions on the first-round clicks. In the averaged mask, gray pixels correspond to variations of the method predictions (hard cases), black and white pixels illustrate low variation in, respectively, background and foreground predictions (simple cases).

Figure 15: Samples with high User-IoU@1-NSR. From left to right – the image with the target instance and real-user clicks of the first round; masks, averaged over the clicks, obtained by the methods: RITM HRNet32-IT (C+L) [19], SimpleClick ViT-H (C+L) [25], SAM ViT-H (SA1-B) [1], SAM-HQ ViT-H (SA1-B) [3].

Figure 16: Samples with low User-IoU@1-NSR. From left to right – the image with the target instance and real-user clicks of the first round; masks, averaged over the clicks, obtained by the methods: RITM HRNet32-IT (C+L) [19], SimpleClick ViT-H (C+L) [25], SAM ViT-H (SA1-B) [1], SAM-HQ ViT-H (SA1-B) [3].

Datasheet for the Benchmark

Datasheets for datasets [54] facilitate communication between data creators and users in the form of the questionnaire explicating motivation, data acquisition process, and potential use cases. In this document, we provide a datasheet for the benchmark.

### Motivation

**Q1. For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.**

- Interactive image segmentation aims at segmenting objects of interest given an image and sequential user input (clicks, strokes, contours), with each round allowing the user to correct prediction errors from the previous round. While numerous interactive segmentation methods have been developed, accurately evaluating these methods is crucial for identifying the best one. True evaluation requires real-user inputs. However, collecting many real-user inputs for multiple rounds is impractical, as such a dataset would need to be rebuilt for every method and interaction round due to its iterative nature.

To address this, researchers often use a simple strategy to simulate user inputs. This strategy involves generating a single click for each interaction round by selecting the largest error region from the previous round and clicking at the furthest point from the boundaries of this region (center point). However, previous works [19; 14] have shown that assuming users click in the center of an object is overly simplistic and unrealistic.

To address the issue of unfair assessment algorithms, we started our work by collecting a large dataset of real-user clicks over multiple rounds of interactions. We trained a _clickability model_ to sample realistic user clicks, ensuring fairer evaluation. This model not only allows for more accurate multi-round evaluation but also provides data for first-round assessments.

**Q2. Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?**

- Seven researchers at AIRI, Moscow (affiliated as of 2024) have created RClicks: Anton Antonov, Andrey Moscalenko, Denis Shepelev, Alexander Krapukhin, Konstantin Soshin, Anton Konushin, Vlad Shakhuro.

**Q3. Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.**

- This research work was fully supported by the authors, including funding annotators' work.

**Q4. Any other comments?**

- No.

### Composition

**Q5. What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.**

- RClicks dataset consists of csv-file with clicks and previous-round masks, obtained by some interaction methods after first round for each instance in a segmentation dataset. Each row in the csv-file contains the following information in the columns:

* a name of a segmentation dataset;
* a name of an image without suffix;
* an encode of a target instance, if a mask contained multiple instances;
* a name of interaction method, that was used in the previous round (in case of first round interaction
* a click type (first for the first round, or fp or fn for the subsequent rounds);
* a unique identifier of image_stem, object_stem, model_type and click_type;
* a type of device where it was clicked (pc or mobile);
- coordinates of a click;
* a width and a height of the image.

**Q6. How many instances are there in total (of each type, if appropriate)?**

- There are 185 349 clicks from the first round of iteration, 290 195 clicks form the second (to collect these clicks we used 8144 masks from the first round). Overall -- 475 544.

**Q7. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).**

- RClicks contains all click-annotated instances from GrabCut, Berkeley, DAVIS, COCO-MVal, TETRIS.

**Q8. What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description.**

- RClicks csv-file columns have a following "raw" types:

* strings: dataset, image_stem, object_stem, model_type, click_type, full_stem, device;
* integers: x, y, w, h.

Each previous-round mask in RClicks dataset - is a single-channel image.

**Q9. Is there a label or target associated with each instance? If so, please provide a description.**

- Yes, each instance has a corresponding mask of instance on the image, where user had to click.

**Q10. Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.**

- No.

**Q11. Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.**

- No, there are neither explicit of implicit relationships between individual instances in RClicks.

**Q12. Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.**

- We intend our dataset to be primarily used for benchmarking interactive segmentation methods. Hence, all instances in our dataset would be used for testing. For clickability model training there is a train/test split.

**Q13. Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.**

- It can be stated with certainty that there are no erroneous values in RClicks. The sole source of noise is the fact that during the annotation process, participants were permitted to make minor "mistakes." A small subset of clicks was collected that were situated outside the chosen object mask but in close proximity to its boundaries.

**Q14. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, (a) Are there guarantees that they will exist, and remain constant, over time?**

(b) Are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created)?

(c) Are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.**

- The dataset is self-contained.

**Q15. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description.**

- No, the clicks in RClicks do not cover scenarios that may be considered confidential.

**Q16. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.**

- The dataset contains no data that might be offensive, insulting, threatening, or might cause anxiety by manually curating the set of images.

**Q17. Does the dataset relate to people? If not, you may skip remaining questions in this section.**

- No.

**Q21. Any other comments?**

- No.

### Collection Process

**Q22. How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.**

- We downloaded images and mask annotation from GrabCut, Berkeley, DAVIS, COCO-MVal, TETRIS and annotated instances from the datasets with clicks on crowd sourcing platform toloka.ai. Accordingly, the "raw" image data was directly observable by annotators, and annotations were created manually.

**Q23. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated?**

- We selected and downloaded interactive segmentation datasets. Then with toloka.ai crowd sourcing platform annotated datasets' instances with clicks. The annotators saw image, then mask of chosen object, then they clicked at this object on image.

**Q24. If the dataset is a sample from a larger set, what was the sampling strategy?**

- We release whole dataset.

**Q25. Who was involved in data collection process (e.g., students, crowd-workers, contractors) and how were they compensated (e.g., how much were crowd-workers paid)?**

- We used toloka.ai crowd sourcing platform, every crowd-worker was paid 0.025 for 10 clicks.

**Q26. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please provide a description of the timeframe.**

- Data was collected from the April of 2024 to the May of 2024.

**Q27. Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.**

- No, such processes were unnecessary in our case.

**Q28. Does the dataset relate to people? If not, you may skip remaining questions in this section.**

- No.

**Q34. Any other comments?**

- No.

### Preprocessing, Cleaning, and/or Labeling

**Q35. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section.**

- We clean data from the clicks that were done not in the object of interest.

**Q36. Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.**

**Q37. Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.**

**- We provide code scripts in supplemental materials.**

**Q38. Any other comments?**

**- No.**

### Uses

**Q39. Has the dataset been used for any tasks already? If so, please provide a description.**

**- We have used our dataset to evaluate state-of-the-art interactive segmentation methods and train our clickability model.**

**Q40. Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.**

**- We do not maintain such a repository. However, citation trackers like Google Scholar and Semantic Scholar would list all future works that cite our dataset.**

**Q41. What (other) tasks could the dataset be used for?**

**- We anticipate that the dataset could be used for benchmarking interactive segmentation methods and training interactive segmentation methods.**

**Q42. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?**

**- This is very difficult to anticipate. Future users should be aware that our dataset was collected both from PC and Mobile devices, choose clicks needed to their platform.**

**Q43. Are there any tasks for which the dataset should not be used? If so, please provide a description.**

**- No.**

**Q44. Any other comments?**

**- No.**

### Distribution

**Q45. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.**

**- Yes, our dataset will be publicly available.**

**Q46. How will the dataset will be distributed (e.g., tarball on website, API, GitHub) Does the dataset have a digital object identifier (DOI)?**

**- We will distribute our dataset at github repository. All uses of RClicks should cite this paper, explicating which version of the dataset was considered.**

**Q47. When will the dataset be distributed?**

**- The dataset will be publicly available starting from September 2024.**

**Q48. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.**

**- Clicks are under CC BY-NC 4.0, evaluation code and baseline models are under MIT.**

**Q49. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.**

- Nowadays, GrabCut and Berkeley datasets are unavailable from official sites, they can be downloaded through WebArchiveMachine or through RITM repository.

**Q50. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.**

- No.

**Q51. Any other comments?**

- No.

### Maintenance

**Q52. Who will be supporting/hosting/maintaining the dataset?**

- Our team will maintain the dataset.

**Q53. How can the owner/curator/manager of the dataset be contacted (e.g., email address)?**

- antonov@airi.net

**Q54. Is there an erratum? If so, please provide a link or other access point.**

- There is no erratum for our initial release.

**Q55. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?**

- We will probably update our dataset on a non-regular basis.

**Q56. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.**

- No.

**Q57. Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users.**

- A new version release of RClicks will automatically deprecate its previous version. We will only support and maintain the latest version at all times.

**Q58. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.**

- Anyone can extend RClicks if providing high-resolution images with properly annotated masks and clicks for it. We are open to accept extensions via personal communication with potential contributors. Otherwise, our code and data licenses allow others to create independent derivative works (with proper attribution).

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] See Introduction 1 and Conclusion 7. 2. Did you describe the limitations of your work? [Yes] See Appendix A. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix A. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? We include it in supplemental material. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix B. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? See Table 4, Appendix D. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendices B, D.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? See Appendix A. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See the supplementary 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix C. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Appendix C for an example of textual instruction. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] See Appendix C.