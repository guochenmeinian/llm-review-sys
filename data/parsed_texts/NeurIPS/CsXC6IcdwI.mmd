# Ehrshot: An EHR Benchmark for Few-Shot Evaluation of Foundation Models

 Michael Wornow

Department of Computer Science

Stanford University

mwornow@stanford.edu

&Rahul Thapa

Center for Biomedical Informatics Research

Stanford University

rthapa84@stanford.edu

Equal contribution

Ethan Steinberg

Department of Computer Science

Stanford University

ethan@stanford.edu

&Jason A. Fries

Center for Biomedical Informatics Research

Stanford University

jfries@stanford.edu

&Nigam H. Shah

Center for Biomedical Informatics Research

Clinical Excellence Research Center

Stanford University

Technology and Digital Solutions

Stanford Healthcare

nigam@stanford.edu

Equal contributionEqual senior authorship

###### Abstract

While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains de-identified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model and dataset are available via a research data use agreement from our website. Code to reproduce our results is available here.

[MISSING_PAGE_FAIL:2]

## 2 Related Work

One of the most popular EHR datasets made accessible to researchers is MIMIC-III, which contains roughly 40,000 patients seen in the intensive care unit (ICU) of Beth Israel Deaconess Medical Center in Boston, Massachusetts, between 2001 and 2012 [11]. Other public datasets include eICU [22], HiRID [2], AmsterdamUMCdb [44], CPRD [11], MIMIC-IV [13], and the UK BioBank [2].

Most of the aforementioned datasets are narrowly scoped to a single department: the ICU [11][12][13][14]. This makes it impossible to capture a patient's full health trajectory to the extent that an academic medical center or health system would know of the patients it treats. Other datasets such as MIMIC-IV include data from multiple departments, but are still heavily anchored to the ICU, as only patients admitted for an ICU/ED visit are included [15]. In contrast, our work releases the full longitudinal EHR of patients across all departments of a major academic medical center, thus providing a more realistic setting for general prediction making.

Prior work has also typically relied on the creation of bespoke schemas to store their data. These custom schemas greatly increase the difficulty of transferring models across datasets and sites [43]. In contrast, the data preprocessing pipeline that we use is capable of ingesting both EHRSHOT as well as any dataset that follows the Observational Medical Outcomes Partnership Common Data Model (OMOP-CDM), an open community data standard for sharing EHRs used by over 100 health systems [34]. More details on our data preprocessing pipeline can be found in the Appendix in Section C.5.

Previously published EHR datasets typically only provide raw data. Thus, significant additional effort has been devoted to building standardized preprocessing pipelines, patient splits, and task definitions on top of these datasets [110][29][22]. These add-on benchmarks, however, are still limited by the narrow scope of their underlying data, and many recycle the same core set of tasks (e.g. in-patient mortality, long length-of-stay, ICU transfer, and ICD code prediction) [29][10]. Additionally, these benchmarks are typically not created with the purpose of measuring a pretrained model's few-shot

Figure 1: Overview of EHRSHOT. Black boxes represent open source code, data, and model weights. Red boxes are private data. (1) Starting with a source EHR database of 3.67M patients, we define a global train/val/test split across all patients. (2) We use an open source EHR preprocessing package called FEMR to transform our data. We keep all structured data (diagnoses, medications, labs, etc.) and discard images and clinical text. (3) We use the 2.57M patients in our global train split to pre-train a foundation model, CLMBR-T-base [41][4][1][4] We filter the source database down to a cohort of 6,739 patients, which we use for EHRSHOT. (5) We define 15 few-shot classification tasks and label each patient accordingly. (6) We test two baseline models for each task: our pretrained CLMBR-T-base and a count-based GBM model [31]. (7) We measure the AUROC and AUPRC of each model on each task, and share the results in Section [5]performance [21]. This limits their utility in assessing the key value propositions of foundation models, such as improved sample efficiency and adaptation to diverse tasks.

On the modeling side, substantial literature exists on training FMs for EHR data [28][19][32][21][24]. However, the vast majority of these FMs have never had their weights published [43]. This greatly hinders reproducibility and makes cross-model evaluations difficult. Worse, this lack of sharing undermines a primary advantage of FMs: transfer learning, i.e. the ability to use the pretrained weights of an existing FM to shortcut model development for other tasks [11].

EHRSHOT aims to fill several of these gaps by providing a longitudinal EHR benchmark specifically geared towards few-shot evaluation of pretrained FMs. EHRSHOT is built on top of a cross-site interoperable standard (OMOP-CDM), and leverages an open source data preprocessing pipeline to allow other researchers to reproduce our results end-to-end. Additionally, we release the weights of the clinical foundation model that we pretrain and evaluate, one of the first to do so. We provide additional points of comparison in Table [11].

## 3 Dataset

We are releasing EHRSHOT (pronounced "earshot"), an EHR benchmark for few-shot evaluation of foundation models. EHRSHOT is a collection of 6,739 unique patients with canonical train/validation/test splits and corresponding labels for 15 classification tasks. We also provide canonical \(k\)-shot samples for each few-shot evaluation task. Unlike prior EHR benchmarks focused on task-specific supervised models [21] for specific episodes of care, e.g. admission to the ICU [10][27], our benchmark is designed for evaluating pretrained FMs on a broad range of tasks using the depth of information that a health system would typically possess for its patients. EHRSHOT is provided as a set of CSV files. It is essentially a lightweight serialization of the OMOP-CDM format. Please see Section C.4 in the Appendix for additional details on the dataset format.

EHRSHOT contains a total of 41.6 million coded observations (e.g. diagnoses, procedures, medications, lab results, etc.) and 921,499 unique visits across 6,739 patients. We exclude all patients less than 19 years of age or greater than 88 years of age. We also exclude patients with less than 10 total clinical events in their record. We include statistics of EHRSHOT's cohort demographics in Table [2] and Appendix Table [4] and histograms of patient characteristics in Appendix Figure [4].

### Data Source

We sourced the data for our benchmark from the Stanford Medicine Research Data Repository (STARR) [5], which contains EHR data from both Stanford Health Care (primarily adult care) and Lucile Packard Children's Hospital (primarily pediatric care). The source dataset is structured according to the Observational Medical Outcomes Partnership Common Data Model (OMOP-CDM) [12] and comprises a total of 3.67M unique patients from 1990 to February 8th, 2023 [5]. Of these patients, 2.57M (70%) are used for training and 0.55M (15%) for validation of the foundation model

\begin{table}
\begin{tabular}{l|c c c c|c c c c} \hline \hline \multirow{2}{*}{**Benchmark**} & \multirow{2}{*}{**Source**} & \multicolumn{3}{c|}{**EHR Properties**} & \multicolumn{2}{c}{**Evaluation**} & \multicolumn{3}{c}{**Reproducibility**} \\  & & \multicolumn{1}{c}{Dataset} & ICU/ED & Other & \# of & Few & Dataset & Preprocessing & Model \\  & & Visits & Visits & Patients & Tasks & Shot & via DUA & Code & Weights \\ \hline MIMIC-Extract [27] & MIMIC-III & ✓ & – & 34k & 5 & – & ✓ & ✓ & – \\ Parushotham 2018 [29] & MIMIC-III & ✓ & – & 35k & 3 & – & ✓ & ✓ & – \\ Harutyunyan 2019 [30] & MIMIC-III & ✓ & – & 33k & 4 & – & ✓ & ✓ & – \\ Gupta 2022 [5] & MIMIC-IV & ✓ & * & 257k & 4 & – & ✓ & ✓ & – \\ COP-E-CAT [31] & MIMIC-IV & ✓ & * & 257k & 4 & – & ✓ & ✓ & – \\ Xie 2022 [43] & MIMIC-IV & ✓ & * & 216k & 3 & – & ✓ & ✓ & – \\ eICU [50] & eICU & ✓ & – & 73k & 4 & – & ✓ & ✓ & – \\ EHR PT [5] & MIMIC-III / eICU & ✓ & – & 86k & 11 & ✓ & ✓ & ✓ & – \\ FIDLE [51] & MIMIC-III / eICU & ✓ & – & 157k & 3 & – & ✓ & ✓ & – \\ HiRID-ICU [52] & HiRID & ✓ & – & 33k & 6 & – & ✓ & ✓ & – \\ Solares 2020 [43] & CPRD & ✓ & ✓ & 4M & 2 & – & – & – & – \\ \hline
**EHRSHOT** & Stanford Medicine & ✓ & ✓ & 7k & 15 & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our work to existing EHR benchmarks. Checkmark indicates full support, asterisk represents properties that are semi-supported.

[MISSING_PAGE_FAIL:5]

We chose these two models as our baselines for several reasons. First, language modeling has achieved state-of-the-art results on clinical prediction tasks [41, 32, 23, 24, 25, 19], while count-based featurization remains a simple but competitive baseline [41, 32, 31]. Second, most prior FMs trained on structured EHR data have not had their model weights published, and were developed and tested exclusively on nonstandard data formats like MIMIC-III [43]. This makes it nearly impossible to conduct a fair comparison of prior models, which often requires re-implementation or significant modification to work across datasets [41]. This is one of the key challenges we are attempting to solve with EHRSHOT. We pre-train our own FM from scratch to have full control over its training, and publish its model weights so the community can reproduce and build upon our results.

**Count-based Features**. Count-based featurization is a well-established baseline for EHR tasks, valued for its simplicity and effectiveness [31]. The fundamental idea involves converting each patient's timeline into a count vector, where each element contains the number of occurrences of a specific medical concept prior to the prediction time of a task. These patient vectors are combined into a count matrix, which is high-dimensional and sparse. We use a technique called _ontology expansion_ to increase the density of representation and improve the accuracy of code coverage by acknowledging the parent/child hierarchical relationships between medical concepts [4]. After generating our ontology-expanded count matrix, we train a gradient boosting machine (GBM) model on the EHRSHOT train split, and tune hyperparameters on the validation split. We use the LightGBM implementation [42]. We also evaluate a Logistic Regression and Random Forest model as baselines.

Figure 2: Summary of Benchmark Tasks. Each subfigure contains one of the 4 types of predictive classification tasks included in our benchmark: (1) _Operational Outcomes_ (binary), (2) _Assignment of New Diagnoses_ (binary), (3) _Anticipating Chest X-ray Findings_ (multilabel), (4) _Anticipating Lab Test Results_ (multiclass). Each **black line** represents a patient timeline. The [black boxes] represent how each timeline would be labeled for each task at a specific prediction time. The leftmost edge of the dotted lines above each timeline is the prediction time, and the rightmost edge is the end of the time horizon for that task. Note that each _Operational Outcome_ task (1) has a different prediction window (30 days, 7 days, duration of admission), while the other three task categories (2, 3, 4) all have uniform prediction windows across their subtasks.

Their results can be seen in Appendix in Figures[\(\overline{10}\)]and[\(\overline{11}\)]. For clarity, we exclude them from the following analyses, as they perform roughly at par with the count-based GBM model.

**Clinical Language-Model-Based Representations using Transformers (CLMBR-T-base)**. CLMBR-T-base is an autoregressive model designed to predict the next medical code in a patient's timeline given previous codes. This objective enables it to learn robust global patterns for clinical prediction tasks. It is based on the CLMBR model originally developed in [\(\overline{41}\)], but following [\(\overline{3}\)] we substitute a transformer in place of a GRU as its base model. Our model employs causally masked local attention. This ensures forward-only flow of information which is vital for prediction tasks, and is in contrast to BERT-based models which are bidirectional in nature [\(\overline{41}\)]. Note that our model does not process clinical text, only structured information. Our model has 141M trainable parameters, a hidden dimension of 768, and a next code prediction objective. This provides our version of CLMBR-T-base with minute-level resolution rather than the day-level aggregation of the original model formulation [\(\overline{41}\)]. We leave training larger versions of CLMBR to future work.

More details about our baseline models can be found in the Appendix in Section \(\underline{\text{D}}\)

## 5 Results

We evaluate each baseline model in a few-shot setting. For each of the 15 benchmark tasks, we steadily increase the number of examples \(k\) that each model sees from \(k=1\) to the full training dataset, and record the model's AUROC and AUPRC at each \(k\).

More precisely, we define "\(k\)-shot evaluation" of a model \(M\) on a specific task \(T\) as follows. We train \(M\) on \(k\) positive examples and \(k\) negative examples sampled from \(T\)'s training split. We then select an additional \(k\) positive examples and \(k\) negative examples from \(T\)'s validation split, and use these validation examples to select the best hyperparameters for \(M\) for task \(T\). Finally, we evaluate the AUROC and AUPRC of the best performing version of \(M\) on \(T\)'s entire held-out test split. For tasks where the total number of unique positive examples is less than \(k\), we include all positive examples in our training set, and randomly resample positive examples until the total number of training examples seen by the model is \(k\). We consider values of \(k\in\{1,2,4,8,12,16,24,32,48,64,128\}\) for all tasks (with the exception of Celiac, for which we limit \(k\leq 64\) as there are only 62 positive training labels).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{**Train**} & \multicolumn{2}{c}{**Val**} & \multicolumn{2}{c}{**Test**} \\ \cline{2-7}
**Task Name** & \# Patients (\# Positive) & \# Labels (\# Positive) & \# Patients (\# Positive) & \# Patients (\# Positive) & \# Labels (\# Positive) \\ \hline
**Operational Outcomes** & & & & & & \\ Long Length of Stay & 1377 (464) & 2569 (681) & 1240 (395) & 2231 (534) & 1238 (412) & 2195 (552) \\
30-day Readmission & 1337 (164) & 2609 (370) & 1192 (159) & 2207 (281) & 1190 (151) & 2189 (260) \\ ICU Transfer & 1306 (107) & 2402 (113) & 1157 (84) & 2052 (92) & 1154 (75) & 2037 (85) \\
**Anticipating Lab Test Results** & & & & & & \\ Thrombocytopenia & 2084 (870) & 68776 (9774) & 1981 (774) & 54504 (6962) & 1998 (818) & 56338 (7960) \\ Hyperkleemia & 2038 (383) & 76349 (1215) & 1935 (348) & 60168 (886) & 1958 (339) & 63653 (948) \\ Hypoglycemia & 2054 (422) & 12218 (1065) & 1950 (362) & 95488 (858) & 1970 (356) & 100568 (783) \\ Hyponatremia & 2035 (1288) & 81336 (2081) & 1930 (1165) & 64473 (14674) & 1956 (1212) & 67028 (16003) \\ Anemia & 2092 (1251) & 70501 (9544) & 1992 (1122) & 56224 (7445) & 2002 (1151) & 58155 (7636) \\
**Assignment of New Diagnoses** & & & & & & \\ Hypertension & 793 (130) & 1260 (184) & 784 (130) & 1250 (177) & 758 (130) & 1261 (160) \\ Hyperlipidemia & 923 (137) & 1684 (205) & 863 (140) & 1441 (189) & 864 (133) & 1317 (172) \\ Pancreatic Cancer & 1376 (128) & 2576 (155) & 1242 (46) & 2215 (53) & 1246 (40) & 2220 (56) \\ Celiae & 1392 (48) & 2623 (62) & 1252 (8) & 2284 (11) & 1255 (13) & 2222 (21) \\ Lupus & 1377 (79) & 2570 (104) & 1239 (24) & 2226 (33) & 1249 (For the count-based GBM, these few-shot examples are the only training examples seen by the model. For the pretrained CLMBR-T-base model, we use these few-shot examples to fine-tune a logistic regression head appended to the top of the model, while keeping the weights of the pretrained CLMBR-T-base model frozen. Pretraining the CLMBR-T-base model took roughly 4 days on a single Nvidia V100 hosted in an on-premise compute cluster.

The AUROC of each model across all 4 task categories is presented in Figure 4. In the Appendix, we show this grouping for AUPRC in Figure 4. We also break down each individual task's AUROC in Figure 4 and AUPRC in Figure 4 of the Appendix. We also include results for additional baselines in the Appendix in Figures 10 and 11. The **bolded lines** are the Macro-AUC for each model within a task category, averaged across all subtasks at each \(k\). We include the performance of each model trained on the entire EHRSHOT training split on the far right of every plot as _All_.

As shown in Figure 4, the pretrained foundation model CLMBR-T-base (blue) outperforms the count-based GBM (red) across all aggregated task categories for \(k\leq 64\). This demonstrates the benefits of pretraining in few-shot settings, as the model can leverage patterns learned across millions of patients to derive more accurate representations out-of-the-box than a model trained from scratch. CLMBR-T-base outperforms the count-based GBM across all \(k\) on the _Operational Outcomes_ and the majority of _Anticipating Lab Test Results_ and _Anticipating Chest X-ray Findings_ tasks. For these three task groups, the advantage of CLMBR-T-base seems most pronounced at intermediate levels of \(k\) between 8 and 128. At extremely low \(k\) (i.e. \(k=1\)), both models struggle to learn anything, while as \(k\) increases the advantage of the pretrained model tends to shrink, a trend noted elsewhere [21]. This is most visible in the far-right of the plot at the _All_ marker, which represents the performance of each model when trained on the full EHRSHOT training dataset.

In fact, the count-based GBM exceeds the performance of CLMBR-T-base on the _Assignment of New Diagnoses_ tasks at \(k>64\). This suggests that the advantage of pretraining comes primarily from improved initialization of patient representations, and that the largest gains are achieved in the most data poor regimes.

There are several possible reasons for CLMBR-T-base's underperformance at higher values of \(k\) for the _Assignment of New Diagnoses_ tasks. First, the CLMBR-T-base model's training objective is next code prediction, which makes it ill-suited for predictive tasks with long time horizons (which for these tasks is 1 year). Second, if a simple tree-based model exists for a task (i.e. a few medical concepts tightly correlate with a diagnosis), then it may be more difficult for a pretrained model to coerce patient representations learned over millions of patients to that specific task than training a model from scratch with enough data to learn those distinctive signals. We believe that this reversal in model rankings demonstrates a key strength of EHRSHOT - namely, the diversity of its predictive tasks can help identify opportunities for improving pretraining and few-shot strategies.

We release all of our model weights, evaluation tasks, and data processing code to fully reproduce our results. To the best of our knowledge, the release of our pretrained CLMBR-T-base model is one of the first examples of such a clinical FM having its pretrained weights made publicly available [48].

## 6 Discussion

We believe that EHRSHOT represents a useful contribution to the ML community by enabling more reproducible healthcare ML research. The release of our pretrained CLMBR-T-base model's weights will allow the community to replicate and build upon our work. Our results identify opportunities for improving pretrained models in few-shot settings.

Acquiring labeled EHR data is expensive and time-consuming. Additionally, certain rare conditions may only be present in a small cohort of patients out of millions within a health system [23]. Thus, model performance in low-label settings is of paramount importance in healthcare. As our results in Section 5 demonstrate, pretrained FMs can yield large performance gains in few-shot settings. While we acknowledge that the tasks themselves may not be the most clinically meaningful, we believe that EHRSHOT offers a valuable contribution by providing a reproducible and rigorous point of comparison for different technical approaches to developing clinical FMs.

**Limitations**. There are several limitations to this work. First, we only release structured data - i.e. we do not publish any of the clinical text or images associated with our patients. While many datasets for medical images exist [3], publishing clinical text remains a challenge [40]. Second, we only consider one type of foundation model (CLMBR-T-base) for our experiments [1]. We look forward to seeing the additional foundation models that the community applies to our benchmark. Third, we release a very small cohort of patients (<1%) from our source EHR database, and specifically select these patients for the tasks that we define. Releasing our full pretraining dataset would be infeasible from a governance and effort perspective. Thus, while necessary in order to publish our EHR dataset and still broader than existing ICU-specific datasets, our cohort selection process limits the types of questions we can answer and does not reflect the full diversity of medical data. Fourth, as we only were able to evaluate our pretrained FM on Stanford Medicine data, it is unclear how well our pretrained model will perform at other institutions. We anticipate there will be some drop in performance, but the extent is unclear. Fifth, several of our tasks are "low label" in the most extreme sense - for example, the Celiac task only has 13 positive patients in its test set. This makes obtaining low variance estimates of model performance difficult. We aim to mitigate this by adding additional patients to our benchmark in future releases.

Figure 3: Aggregated AUROC across all subtasks within each of the 4 task categories for \(k\in\{1,2,4,8,12,16,24,32,48,64,128\}\) shots. We show performance on the full training set as _All_. The **bolded lines** are the Macro-AUROC for each model, averaged across all subtasks within a task category for each value of \(k\). The blurred lines are the average AUROC across 5 replicates for each subtask within a task category. CLMBR-T-base (blue) consistently outperforms the count-based GBM (red) at \(k\leq 64\), but lags in higher label settings for the _Assignment of New Diagnoses_ tasks.

**Societal Implications**. We believe that the release of this dataset can help spur positive innovations for improving clinical care with ML. However, we recognize that there are patient privacy concerns anytime EHR data is released. We believe we sufficiently mitigate this risk through the rigorous deidentification process on which our data is subjected [3]. Additionally, we gate access to the dataset through a research data use agreement. Another concern is that models trained on biased data will reflect those biases [7]. Thus, the pretrained FM that we release may propagate biases in care delivery or outcomes present in our source EHR database [7]. However, we hope that by encouraging the full release of models, we can help the community better identify and mitigate these issues [23].

## 7 Conclusion

We publish EHRSHOT, a benchmark containing the structured data of 6,739 patients' full longitudinal medical timelines specifically geared towards few-shot evaluation of foundation models for clinical data. Unlike most prior work, EHRSHOT contains longitudinal health data rather than a single department (e.g. ICU). We define a set of 15 tasks ranging from well-studied outcomes like 30-day readmission to lesser explored settings such as anticipating abnormal lab values. Finally, we release the weights of a foundation model pretrained on over 2.57M patient timelines and publish the code needed to replicate our results. We hope that this work represents a first step towards moving the field of ML for healthcare towards more reproducible and open model development.

## Acknowledgments and Disclosure of Funding

We thank the Stanford AIMI Center and Stanford Medicine Research IT for their assistance in publishing this dataset. This work was supported in part by the Mark and Debra Leslie Endowment for AI in Healthcare, the Clinical Excellence Research Center at Stanford Medicine, and Technology and Digital Solutions at Stanford Healthcare. MW is supported by an NSF Graduate Research Fellowship. JF was supported in part by a Stanford AIMI-HAI Partnership Grant.

## References

* [1]R. Bommasani, D. A. Hudson, E. Adeli, R. B. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. Creel, J. Quincy Davis, D. Dzemsky, C. Donahue, M. Doumbuoya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. Wei Koh, M. S. Krass, R. Krishna, R. Kuditipudi, and et al. (2021) On the opportunities and risks of foundation models. CoRRabs/2108.07258. External Links: Link, 2108.07258 Cited by: SS1.
* [2]C. Bycroft, C. Freeman, D. Petkova, G. Band, L. T. Elliott, K. Sharp, A. Motyer, D. Vukcevic, O. Delaneau, J. O'Connell, et al. (2018) The uk biobank resource with deep phenotyping and genomic data. Nature562 (7726), pp. 203-209. External Links: Link, Document Cited by: SS1.
* [3]E. Callt, E. Sogancioglu, B. van Ginneken, K. G. van Leeuwen, and K. Murphy (2021) Deep learning for chest x-ray analysis: a survey. Medical Image Analysis72, pp. 102125. External Links: Link, Document Cited by: SS1.
* [4]E. Choi, M. T. Bahadori, L. Song, W. F. Stewart, and J. Sun (2017) Gram: graph-based attention model for healthcare representation learning. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 787-795. Cited by: SS1.
* [5]S. Datta, J. Posada, G. Olson, W. Li, C. O'Reilly, D. Balraj, J. Mesterhazy, J. Pallas, P. Desai, and N. Shah (2020) A new paradigm for accelerating clinical data science at stanford medicine. arXiv preprint arXiv:2003.10534. Cited by: SS1.
* [6]M. Faltys, M. Zimmermann, X. Lyu, M. Huser, S. Hyland, G. Ratsch, and T. Merz (2020) Hird, a high time-resolution icu dataset (version 1.0). Note: DOI: https://doi. org/10.13026/hz5m-md48 Cited by: SS1.
* [7]M. A. Gianfrancesco, S. Tamang, J. Yazdany, and G. Schmajuk (2018) Potential biases in machine learning algorithms using electronic health record data. JAMA internal medicine178 (11), pp. 1544-1547. External Links: Link, Document Cited by: SS1.
* [8]L. Guo, E. Steinberg, S. L. Fleming, J. Posada, J. Lemmon, S. R. Pfohl, N. Shah, J. Fries, and L. Sung (2023) Ehr foundation models improve robustness in the presence of temporal distribution shift. Scientific Reports13 (1), pp. 3767. External Links: Link, Document Cited by: SS1.
* [9]M. Gupta, B. Gallamoza, N. Cutrona, P. Dhakal, R. Poulain, and R. Beheshti (2022) An extensive data processing pipeline for mimic-iv. In Machine Learning for Health, pp. 311-325. Cited by: SS1.
* [10]H. Harutyunyan, H. Khachatrian, D. C. Kale, G. Ver Steeg, and A. Galstyan (2019) Multitask learning and benchmarking with clinical time series data. Scientific data6 (1), pp. 96. External Links: Link, Document Cited by: SS1.
* [11]E. Herrett, A. M. Gallagher, K. Bhaskaran, H. Forbes, R. Mathur, T. Van Staa, and L. Smeeth (2015) Data resource profile: clinical practice research datalink (cprd). International journal of epidemiology44 (3), pp. 827-836. External Links: Link, Document Cited by: SS1.
* [12]G. Hripcsak, J. D. Duke, N. H. Shah, C. G. Reich, V. Huser, M. J. Schuemie, M. A. Suchard, R. W. Park, I. Chi K. Wong, P. R. Rijnbeek, et al. (2015) Observational health data sciences and informatics (ohdsi): opportunities for observational researchers. In MEDINFO 2015: eHealth-enabled Health, pp. 574-578. External Links: Link, Document Cited by: SS1.
* [13]K. Hur, J. Lee, J. Oh, W. Price, Y. Kim, and E. Choi (2021) Unifying heterogenous electronic health records systems via text-based code embedding. arXiv preprint arXiv:2111.09098. Cited by: SS1.
* [14]J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya, et al. (2019) Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33, pp. 590-597. External Links: Link, Document Cited by: SS1.

* [15] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shanmout, Steven Horng, Tom J Pollard, Benjamin Moody, Brian Gow, Li-wei H Lehman, et al. Mimic-iv, a freely accessible electronic health record dataset. _Scientific data_, 10(1):1, 2023.
* [16] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* [17] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. _Advances in neural information processing systems_, 30, 2017.
* [18] Max Langenkamp and Daniel N Yue. How open source machine learning software shapes ai. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, pages 385-395, 2022.
* [19] Yikuan Li, Shishir Rao, Jose Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy, Yajie Zhu, Kazem Rahimi, and Gholamreza Salimi-Khorshidi. Behrt: transformer for electronic health records. _Scientific reports_, 10(1):1-12, 2020.
* [20] Aishwarya Mandyam, Elizabeth C Yoo, Jeff Soules, Krzysztof Laudanski, and Barbara E Engelhardt. Cop-e-cat: cleaning and organization pipeline for ehr computational and analytic tasks. In _Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics_, pages 1-9, 2021.
* [21] Matthew McDermott, Bret Nestor, Evan Kim, Wancong Zhang, Anna Goldenberg, Peter Szolovits, and Marzyeh Ghassemi. A comprehensive ehr timeseries pre-training benchmark. In _Proceedings of the Conference on Health, Inference, and Learning_, pages 257-278, 2021.
* [22] Matthew BA McDermott, Shirly Wang, Nikki Marinsek, Rajesh Ranganath, Luca Foschini, and Marzyeh Ghassemi. Reproducibility in machine learning for health research: Still a ways to go. _Science Translational Medicine_, 13(586):eabb1655, 2021.
* [23] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. _Nature_, 616(7956):259-265, 2023.
* [24] Anna Munoz-Farre, Harry Rose, and Sera Aylin Cakiroglu. sehr-ce: Language modelling of structured ehr data for efficient and generalizable patient cohort expansion. _arXiv preprint arXiv:2211.17121_, 2022.
* [25] Natalia Norori, Qiyang Hu, Florence Marcelle Aellen, Francesca Dalia Faraci, and Athina Tzovara. Addressing bias in big data and ai for health care: A call for open science. _Patterns_, 2(10):100347, 2021.
* [26] Heather A Piwowar and Todd J Vision. Data reuse and the open data citation advantage. _PeerJ_, 1:e175, 2013.
* [27] Tom J Pollard, Alistair EW Johnson, Jesse D Raffa, Leo A Celi, Roger G Mark, and Omar Badawi. The eicu collaborative research database, a freely available multi-center database for critical care research. _Scientific data_, 5(1):1-13, 2018.
* [28] Raphael Poulain, Mehak Gupta, and Rahmatollah Beheshti. Few-shot learning with semi-supervised transformers for electronic health records. In Zachary Lipton, Rajesh Ranganath, Mark Sendak, Michael Sjoding, and Serena Yeung, editors, _Proceedings of the 7th Machine Learning for Healthcare Conference_, volume 182 of _Proceedings of Machine Learning Research_, pages 853-873. PMLR, 05-06 Aug 2022.
* [29] Sanjay Purushotham, Chuziheng Meng, Zhengping Che, and Yan Liu. Benchmarking deep learning models on large healthcare datasets. _Journal of biomedical informatics_, 83:112-134, 2018.
* [30] Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P-W Lo, Bo Xiao, et al. Large ai models in health informatics: Applications, challenges, and the future. _arXiv preprint arXiv:2303.11568_, 2023.
* [31] Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela Hardt, Peter J Liu, Xiaobing Liu, Jake Marcus, Mimi Sun, et al. Scalable and accurate deep learning with electronic health records. _NPJ digital medicine_, 1(1):18, 2018.
* [32] Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. _NPJ digital medicine_, 4(1):86, 2021.

* [33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* [34] Observational Health Data Sciences and Informatics. The book of ohdsi, Jan 2021.
* [35] Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute trends across three eras of machine learning. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2022.
* [36] Seyedmostafa Sheikhalishahi, Vevake Balaraman, and Venet Osmani. Benchmarking machine learning models on multi-centre eicu critical care dataset. _Plos one_, 15(7):e0235424, 2020.
* [37] Emily Sohn. The reproducibility issues that haunt health-care ai. _Nature_, 613:402-403, 01 2023.
* [38] Jose Roberto Ayala Solares, Francesca Elisa Diletta Raimondi, Yajie Zhu, Fatemeh Rahimian, Dexter Canoy, Jenny Tran, Ana Catarina Pinho Gomes, Amir H Payberah, Mariagrazia Zottoli, Milad Nazarzadeh, et al. Deep learning for electronic health records: A comparative review of multiple deep neural architectures. _Journal of biomedical informatics_, 101:103337, 2020.
* [39] Soren Sonnenburg, Mikio L Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCunn, Klaus-Robert Muller, Fernando Pereira, Carl Edward Rasmussen, et al. The need for open source software in machine learning. _Journal of Machine Learning Research_, 8(81):2443-2466, 2007.
* [40] Irena Spasic, Goran Nenadic, et al. Clinical text data in machine learning: systematic review. _JMIR medical informatics_, 8(3):e17984, 2020.
* [41] Ethan Steinberg, Ken Jung, Jason A Fries, Conor K Corbin, Stephen R Pfohl, and Nigam H Shah. Language models are an effective representation learning technique for electronic health record data. _Journal of biomedical informatics_, 113:103637, 2021.
* [42] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murdadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_, 2021.
* [43] Shengpu Tang, Parmida Davarmanesh, Yanmeng Song, Danai Koutra, Michael W Sjoding, and Jenna Wiens. Democratizing ehr analyses with fiddle: a flexible data-driven preprocessing pipeline for structured clinical data. _Journal of the American Medical Informatics Association_, 27(12):1921-1934, 2020.
* [44] Patrick J Thoral, Jan M Peppink, Ronald H Driessen, Eric JG Sijbrands, Erwin JO Kompanje, Lewis Kaplan, Heatherlee Bailey, Jozef Kesecioglu, Maurizio Cecconi, Matthew Churpek, et al. Sharing icu patient data responsibly under the society of critical care medicine/european society of intensive care medicine joint data science collaboration: the amsterdam university medical centers database (amsterdamumcdb) example. _Critical care medicine_, 49(6):e563, 2021.
* [45] Andreas Toscher, Michael Jahrer, and Robert M Bell. The bigchaos solution to the netflix grand prize. _Netflix prize documentation_, pages 1-52, 2009.
* [46] Robin van de Water, Hendrik Schmidt, Paul Elbers, Patrick Thoral, Bert Arnrich, and Patrick Rockenschaub. Yet another icu benchmark: A flexible multi-center framework for clinical ml. _arXiv preprint arXiv:2306.05109_, 2023.
* [47] Shirly Wang, Matthew BA McDermott, Geeticka Chauhan, Marzyeh Ghassemi, Michael C Hughes, and Tristan Naumann. Mimic-extract: A data extraction, preprocessing, and representation pipeline for mimic-iii. In _Proceedings of the ACM conference on health, inference, and learning_, pages 222-235, 2020.
* [48] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A Pfeffer, Jason Fries, and Nigam H Shah. The shaky foundations of clinical foundation models: A survey of large language models and foundation models for emrs. _arXiv preprint arXiv:2303.12961_, 2023.
* [49] Feng Xie, Jun Zhou, Jin Wee Lee, Mingrui Tan, Siqi Li, Logasa S/O Rajnthern, Marcel Lucas Chee, Bibhas Chakraborty, An-Kwok Ian Wong, Alon Dagan, et al. Benchmarking emergency department prediction models with machine learning and public electronic health records. _Scientific Data_, 9(1):658, 2022.
* [50] Hugo Yeche, Rita Kuznetsova, Marc Zimmermann, Matthias Huser, Xinrui Lyu, Martin Faltys, and Gunnar Ratsch. Hirid-icu-benchmark-a comprehensive machine learning benchmark on high-resolution icu data. _arXiv preprint arXiv:2111.08536_, 2021.