# Provably Efficient Offline Reinforcement Learning

in Regular Decision Processes

 Roberto Cipollone

Sapienza University of Rome

cipollone@diag.uniroma1.it &Anders Jonsson

Universitat Pompeu Fabra

anders.jonsson@upf.edu &Alessandro Ronca

University of Oxford

alessandro.ronca@cs.ox.ac.uk &Mohammad Sadegh Talebi

University of Copenhagen

m.shahi@di.ku.dk

###### Abstract

This paper deals with offline (or batch) Reinforcement Learning (RL) in episodic Regular Decision Processes (RDPs). RDPs are the subclass of Non-Markov Decision Processes where the dependency on the history of past events can be captured by a finite-state automaton. We consider a setting where the automaton that underlies the RDP is unknown, and a learner strives to learn a near-optimal policy using pre-collected data, in the form of non-Markov sequences of observations, without further exploration. We present RegORL, an algorithm that suitably combines automata learning techniques and state-of-the-art algorithms for offline RL in MDPs. RegORL has a modular design allowing one to use any off-the-shelf offline RL algorithm in MDPs. We report a non-asymptotic high-probability sample complexity bound for RegORL to yield an \(\varepsilon\)-optimal policy, which makes appear a notion of concentrability relevant for RDPs. Furthermore, we present a sample complexity lower bound for offline RL in RDPs. To our best knowledge, this is the first work presenting a provably efficient algorithm for offline learning in RDPs.

## 1 Introduction

Most reinforcement learning (RL) algorithms hinge on the Markovian assumption, i.e. that the underlying system transitions and rewards are Markovian in some natural notion of (observable) state, and hence, the distribution of future observations depends only on the current state-action of the system. This fundamental assumption allows one to model decision making using the powerful framework of Markov Decision Processes (MDPs) [1]. However, there are many application scenarios where rewards are issued according to temporal conditions over histories (or trajectories), and others where the environment itself evolves in a history-dependent manner. As a result, Markovian approaches may prove unsuitable for modeling such situations. These scenarios can be appropriately modeled as _Non-Markovian Decision Processes (NMDPs)_[2, 3].

NMDPs describe environments where the distribution on the next observation and reward is a function of the history. In these environments, behaving optimally may also require to take histories into account. For example, a robot may receive a reward for delivering an item only if the item was previously requested, and a self-driving car is more likely to skid and lose control if it previously trained. Also, consider a mobile robot that has to track an object which may disappear from its field of view. The object is likely to be found again in the same place where it was seen last time. This requires the agent to remember, hence to act according to information in its interaction history. In general, an NMDP can show an arbitrary dependency on the history or trace, preventing efficient learning. Consequently, recent research has focused on tractable sub-classes of NMDPs. In RegularDecision Processes (RDPs) [3], the next observation and reward distributions depend on regular properties of the history, which can be captured by a deterministic finite-state automaton. This determines the existence of a finite state space where states are determined by histories, and where the Markov property is regained.

In this paper, we investigate offline RL in episodic RDPs, where the goal is to find a near-optimal policy using a pre-collected dataset, with minimal possible size, generated by a fixed behavior policy (and without further exploration). Offline RL in MDPs has received extensive attention recently, and provably sample efficient algorithms have been proposed for various settings. Despite the extensive and rich literature on MDPs, comparatively little work exists on offline RL in NMDPs. The scarcity of results may likely be attributed to the difficult nature of the problem rather than the lack of interest.

Partially-Observable Markov Decision Processes (POMDPs) [4] are also NMDPs, and RDPs can be seen as the subclass of POMDPs that enjoy the property of having hidden states determined by the history of observations. This is a key property that allows one to take advantage of a set of planning and learning techniques that do not apply to arbitrary POMDPs. Planning in POMDPs is computationally intractable [5], and two common approaches to solve (and learn) them rely on maintaining either a belief state or a finite history of observations. Maintaining and updating a belief state is worst-case exponential in the size of the original observation space, while the latter approach yields a space whose size is exponential in the history length. State-of-the-art work on offline RL in POMDPs considers restricted classes of POMDPs such as undercomplete POMDPs (e.g., [6; 7]), which cannot be used to model all RDP instances. General POMDPs are only considered under assumptions such as the possibility of reaching every belief state in a few steps [8] or ergodicity [9]. While existing offline RL algorithms for solving POMDPs cannot guarantee provable learning in a generic RDP, the structural properties of RDPs indicate that they can be solved more efficiently using techniques that are carefully tailored to their structure. Exploiting the structure in RDPs is thus key in designing provably sample-efficient learning algorithms.

### Summary of Contributions

We formalize offline RL in RDPs (Section 2), and establish a first, to the best of our knowledge, sample complexity lower bound thereof (Section 5). We introduce an algorithm, called RegORL, that learns \(\varepsilon\)-optimal policies for any RDP, in the episodic setting. At the core of RegORL, there is a component called AdaCT-H, which is a variant of AdaCT[10], carefully tailored to episodic RDPs. AdaCT-H learns a minimal automaton that underlies the unknown RDP without prior knowledge. The output automaton is further used to derive a Markov abstraction of data to be used by any off-the-shelf algorithm for offline RL in episodic MDPs. We present a sample-complexity bound for AdaCT-H to return a minimal underlying automaton with high probability. This bound substantially improves the existing bound for the original AdaCT, and can be of independent interest. In view of the modular design of RegORL, the total sample complexity is controlled by twice that of AdaCT-H (Theorem 6) and that for the incorporated off-the-shelf algorithm. We also present another variant of AdaCT-H, called AdaCT-H-A. In contrast to AdaCT-H that learns a complete RDP, AdaCT-H-A only reconstructs a subset of states that are likely under the behavior policy, in relation to an input accuracy parameter. As such, AdaCT-H-A renders more aligned with the practice of RL than AdaCT-H. Furthermore, we provide a first lower-bound for offline RL in RDPs that involves relevant parameters for the problem, such as the RDP single-policy concentrability, which extends an analogous notion for MDPs from the literature. Finally, if contrasted to both online learning in RDPs and automata learning, our results suggest possible improvements in sample complexity results for both areas.

### Related Work

Offline RL in MDPs.There is a rich and growing literature on offline RL, and provably sample efficient algorithms have been proposed for various settings of MDPs; see, e.g., [11; 12; 13; 14; 15; 16; 17; 18; 19; 20]. For example, in the case of episodic MDPs, it is established that the optimal sample size in offline RL depends on the size of state-space, episode length, as well as some notion of concentrability, reflecting the distribution mismatch between the behavior and optimal policies. A closely related problem is off-policy learning; see, e.g., [21; 22; 23] and the recent survey [24].

Online RL in RDPs.Several algorithms for _online_ RL in RDPs exist [25; 26; 27] but complexity bounds are only given in [26] for the infinite-horizon discounted setting. The sample complexity bounds in [26] are not immediately comparable to ours, due to the different setting. Importantly, the algorithm in [26] uses the uniform policy for learning, and it therefore might be adapted to our setting only under the assumption that the behaviour policy is uniform. Even in this case, our bounds show an improved dependency on several key quantities. Furthermore, we provide a sample complexity lower bound, whereas their results are limited to showing that a dependency on the quantities occurring in their upper bounds is necessary.

The online RL algorithms in [28; 29; 30; 31] have been developed for formalisms that are closely related to RDPs, and such algorithms can be applied to RDPs. However, these algorithms are not proven to be sample efficient.

POMDPs.Every RDP can be seen as a POMDP whose hidden dynamics evolves according to its finite-state automaton. However, RL in POMDPs is a largely open problem. Even for a known POMDP, computing a near-optimal policy is PSpace-complete [5]. For unknown dynamics, which is the setting considered here, favourable bounds have been obtained for the class of undercomplete POMDPs [6; 7], which does not include all RDPs, or alternatively, under other assumptions such as few-step reachability [8] or ergodicity [9]. This relationship between RDPs and POMDPs can be also seen from the notion of state. In fact, the automaton state of an RDP is an instance of information state, as defined in [32], and of belief, as in classic POMDP literature [33].

PSRs.Predictive State Representations (PSRs) [34; 35; 36; 37] are general descriptions of dynamical systems that capture POMDPs and hence RDPs. There exist polynomial PAC bounds for online RL in PSRs [38]. Nonetheless, these bounds are looser than the one we show here, since they must necessarily consider a wider class of models. Moreover, although a minimum core set for PSRs is similar to a minimal RDP, the bounds feature a number of quantities that are specific to PSRs (e.g., regularity parameter) and do not immediately apply to RDPs.

Other Non-Markovian Settings._Feature MDPs_ and _state representations_ both share the idea of having a map from histories to a state space. This is analogous to the map determined by the transition function of the automaton underlying an RDP. Algorithmic solutions for feature MDPs are based on suffix trees, and they cannot yield optimal performance in our setting [29; 30]. The automaton of an RDP can be seen as providing one kind of state representation [39; 40; 41; 42]. The existing bounds for state representations show a linear dependency on the number of candidate representations, which is exponential in the number of states in our case. A similar dependency is also observed in [43]. RL with _non-Markovian rewards_ is considered in [44; 45; 46; 47; 48; 49]. The idea of a map from histories to states is also found in [32]. Non-Markovianity is also introduced by logical specifications that the agent is required to satisfy [50; 51; 52; 53; 54]; however, it is resolved a priori from the known specification. The convergence properties of Q-learning over a (known) underlying state space such as the one of an RDP are studied in [55].

Learning PDFA.Our algorithms for learning an RDP borrow and improve over techniques for learning Probabilistic-Deterministic Finite Automata (PDFA) [56; 57; 58; 10; 59; 60]. Our algorithm builds upon the state-of-the-art algorithm AdaCT [10], and we derive bounds that are a substantial improvement over the ones that would be obtained from a straightforward application of any existing PDFA-learning algorithm to the offline RL setting.

We provide additional literature review in Appendix A.

## 2 Preliminaries and Problem Formulation

Notations.Given a set \(\mathcal{Y}\), \(\Delta(\mathcal{Y})\) denotes the set of probability distributions over \(\mathcal{Y}\). For a function \(f:\mathcal{X}\rightarrow\Delta(\mathcal{Y})\), \(f(x,y)\) is the probability of \(y\) given \(x\). Further, we write \(y\sim f(x)\) to abbreviate \(y\sim f(x,\cdot)\). For \(y\in\mathcal{Y}\), we use \(\mathbbm{1}_{y}\in\Delta(\mathcal{Y})\) to denote the Kronecker delta defined as \(\mathbbm{1}_{y}(y)=1\) and \(\mathbbm{1}_{y}(y^{\prime})=0\) for each \(y^{\prime}\in\mathcal{Y}\) such that \(y^{\prime}\neq y\). Given an event \(E\), \(\mathbb{I}(E)\) denotes the indicator function of \(E\), which equals \(1\) if \(E\) is true, and \(0\) otherwise, e.g. \(\mathbbm{1}_{y}(y^{\prime})=\mathbb{I}(y=y^{\prime})\). For any integer \(Z\!\geq\!0\), we let \([Z]:=\{0,\ldots,Z\}\). Given a set \(\mathcal{X}\), for \(k\in\mathbb{N}\), \(\mathcal{X}^{k}\) represents the set of sequences of length \(k\) whose elements are from \(\mathcal{X}\). Also, \(\mathcal{X}^{*}=\cup_{k=0}^{\infty}\mathcal{X}^{k}\). The notation \(\widetilde{\mathcal{O}}(\cdot)\) hides poly-logarithmic terms.

### Episodic Regular Decision Processes

We first introduce generic episodic decision processes. An episodic decision process is a tuple \(\mathcal{P}=\langle\mathcal{O},\mathcal{A},\mathcal{R},\bar{T},\bar{R},H\rangle\), where \(\mathcal{O}\) is a finite set of observations, \(\mathcal{A}\) is a finite set of actions, \(\mathcal{R}\subset[0,1]\) is a finite set of rewards and \(H\geq 1\) is a finite horizon. As is common in automata theory, we use sequences \(a_{m}r_{m}o_{m}\cdots a_{n}r_{n}o_{n}\) to denote traces of actions, rewards and observations, and concatenation \(\mathcal{ARO}=\{aro:a\in\mathcal{A},r\in\mathcal{R},o\in\mathcal{O}\}\) to denote sets of sequences. Let \(\mathcal{E}_{t}=(\mathcal{ARO})^{t+1}\) be the set of traces of length \(t+1\), and let \(e_{m:n}\in\mathcal{E}_{n-m}\) denote a trace from time \(m\) to time \(n\), included. A _trajectory_\(e_{0:T}\) is the full trace generated until time \(T\). We assume that a trajectory \(e_{0:T}\) can be partitioned into _episodes_\(e_{\ell:\ell+H}\in\mathcal{E}_{H}\) of length \(H+1\), and that the dynamics at time \(T=k(H+1)+t,t\in[H]\), are _conditionally independent_ of the previous episodes and all rewards, i.e. the dynamics only depend on \(a_{k(H+1)}o_{k(H+1)}\cdots a_{T}o_{T}\). For \(t\in[H]\), let \(\mathcal{H}_{t}=(\mathcal{AO})^{t+1}\) denote the relevant part of the trajectory for decision making, and let \(\mathcal{H}=\cup_{t=0}^{H}\mathcal{H}_{t}\). We refer to elements in \(\mathcal{H}\) as _histories_, even though they are not complete trajectories. In each episode \(e_{0:H}\), \(a_{0}=a_{\perp}\) is a dummy action used to initialize the distribution on \(\mathcal{H}_{0}\). The transition function \(\bar{T}:\mathcal{H}\times\mathcal{A}\to\Delta(\mathcal{O})\) and the reward function \(\bar{R}:\mathcal{H}\times\mathcal{A}\to\Delta(\mathcal{R})\) only depend on the history of the current episode. Given \(\mathcal{P}\), a generic policy is a function \(\pi:(\mathcal{AO})^{*}\to\Delta(\mathcal{A})\) that maps trajectories to distributions over actions. The value function \(V^{\pi}:[H]\times\mathcal{H}\to\mathbb{R}\) of a policy \(\pi\) is a mapping that assigns real values to histories. For \(h\in\mathcal{H}\), it is defined as \(V^{\pi}(H,h)\coloneqq 0\) and

\[V^{\pi}(t,h)\coloneqq\mathbb{E}\left[\left.\sum_{i=t+1}^{H}r_{i}\left|h,\pi \right.\right],\ \ \forall t<H,\forall h\in\mathcal{H}_{t}.\right.\] (1)

For brevity, we write \(V^{\pi}_{t}(h)\coloneqq V^{\pi}(t,h)\). The optimal value function \(V^{*}\) is defined as \(V^{*}_{t}(h)\coloneqq\sup_{\pi}V^{\pi}_{t}(h),\forall t\in[H],\forall h\in \mathcal{H}_{t}\), where \(\sup\) is taken over all policies \(\pi:(\mathcal{AO})^{*}\to\Delta(\mathcal{A})\). Any policy achieving \(V^{*}\) is called optimal, which we denote by \(\pi^{*}\); namely \(V^{\pi^{*}}=V^{*}\). Solving \(\mathcal{P}\) amounts to finding \(\pi^{*}\). In what follows we consider simpler policies of the form \(\pi:\mathcal{H}\to\Delta(\mathcal{A})\) mapping histories to distributions over actions. Let \(\Pi_{\mathcal{H}}\) denote the set of such policies. It can be shown that \(\Pi_{\mathcal{H}}\) always contains an optimal policy, i.e. \(V^{*}_{t}(h)\coloneqq\max_{\pi\in\Pi_{\mathcal{H}}}V^{\pi}_{t}(h),\forall t \in[H],\forall h\in\mathcal{H}_{t}\). An episodic MDP is an episodic decision process whose dynamics at each timestep \(t\) only depends on the last observation and action [1].

Episodic RDPs.An episodic Regular Decision Process (RDP) [3, 25] is an episodic decision process \(\mathbf{R}=\langle\mathcal{O},\mathcal{A},\mathcal{R},\bar{T},\bar{R},H\rangle\) described by a _finite transducer_ (Moore machine) \(\langle\mathcal{Q},\Sigma,\Omega,\tau,\theta,q_{0}\rangle\), where \(\mathcal{Q}\) is a finite set of states, \(\Sigma=\mathcal{A}\,\mathcal{O}\) is a finite input alphabet composed of actions and observations, \(\Omega\) is a finite output alphabet, \(\tau:\mathcal{Q}\times\Sigma\to\mathcal{Q}\) is a transition function, \(\theta:\mathcal{Q}\to\Omega\) is an output function, and \(q_{0}\in\mathcal{Q}\) is a fixed initial state [61, 62, 63]. The output space \(\Omega=\Omega_{\mathsf{o}}\times\Omega_{\mathsf{r}}\) consists of a finite set of functions that compute the conditional probabilities of observations and rewards, meaning \(\Omega_{\mathsf{o}}\subset\mathcal{A}\to\Delta(\mathcal{O})\) and \(\Omega_{\mathsf{r}}\subset\mathcal{A}\to\Delta(\mathcal{R})\). For simplicity, we use two output functions, \(\theta_{\mathsf{o}}:\mathcal{Q}\times\mathcal{A}\to\Delta(\mathcal{O})\) and \(\theta_{\mathsf{r}}:\mathcal{Q}\times\mathcal{A}\to\Delta(\mathcal{R})\), to denote the individual conditional probabilities. Let \(\tau^{-1}\) denote the inverse of \(\tau\), i.e. \(\tau^{-1}(q)\subseteq\mathcal{Q}\times\mathcal{A}\,\mathcal{O}\) is the subset of state-symbol pairs that map to \(q\in\mathcal{Q}\). In this context, an input symbol is an element of \(\mathcal{AO}\). We use \(A,R,O,Q\) to denote the cardinality of \(\mathcal{A},\mathcal{R},\mathcal{O},\mathcal{Q}\), respectively, and assume \(A\geq 2\).

An RDP \(\mathbf{R}\) implicitly represents a function \(\bar{\tau}:\mathcal{H}\to\mathcal{Q}\) from histories in \(\mathcal{H}\) to states in \(\mathcal{Q}\), recursively defined as \(\bar{\tau}(h_{0})\coloneqq\tau(q_{0},a_{0}o_{0})\) and \(\bar{\tau}(h_{t})\coloneqq\tau(\bar{\tau}(h_{t-1}),a_{t}o_{t})\). The dynamics and of \(\mathbf{R}\) are defined as \(\bar{T}(h,a,o)=\theta_{\mathsf{o}}(\bar{\tau}(h),a,o)\) and \(\bar{R}(h,a,r)=\theta_{\mathsf{r}}(\bar{\tau}(h),a,r)\), \(\forall h\in\mathcal{H},\forall aro\in\mathcal{ARO}\). Episodic RDPs are acyclic, i.e. the states can be partitioned as \(\mathcal{Q}=\mathcal{Q}_{0}\cup\cdots\cup\mathcal{Q}_{H+1}\), where each \(\mathcal{Q}_{t+1}\) is the set of states generated by histories in \(\mathcal{H}_{t}\) for each \(t\in[H]\). An RDP is minimal if its Moore machine is minimal. Since there is nothing to predict at time \(H+1\), a minimal RDP contains a single state \(q_{H+1}\) in \(\mathcal{Q}_{H+1}\). To ensure that an acyclic RDP \(\mathbf{R}\) is minimal, we introduce a designated termination observation \(o_{\perp}\) in \(\mathcal{O}\) and define \(\tau(q_{H+1},a_{o\perp})=q_{H+1}\) and \(\theta_{\mathsf{o}}(q_{H+1},a)=\mathbbm{1}_{o_{\perp}}\) for each \(a\in\mathcal{A}\). Hence, \(q_{H+1}\) is absorbing and the states in \(\mathcal{Q}\) implicitly count how many steps are left until we observe \(o_{\perp}\). Without \(o_{\perp}\), a Moore machine could potentially represent all episodes using fewer than \(H+2\) states.

Since the conditional probabilities of observations and rewards are fully determined by the current state-action pair \((q,a)\), an RDP \(\mathbf{R}\) adheres to the Markov property over its states, but _not over the observations_. Given a state \(q_{t}\in\mathcal{Q}\) and an action \(a_{t}\in\mathcal{A}\), the probability of the next transition is

\[\mathbb{P}(r_{t},o_{t},q_{t+1}\mid q_{t},a_{t},\mathbf{R})=\theta_{\mathsf{r}}(q_ {t},a_{t},r_{t})\,\theta_{\mathsf{o}}(q_{t},a_{t},o_{t})\,\mathbb{I}(q_{t+1}= \tau(q_{t},a_{t}o_{t})).\]Evidently, in the special case where an RDP is Markovian in both observations and rewards, it reduces to an episodic MDP. More precisely, any episodic MDP with actions \(\mathcal{A}\), states \(\mathcal{O}\) and horizon \(H\) can be represented by some episodic RDP with states \(\mathcal{Q}\subseteq\mathcal{O}\times[H+1]\) and inputs \(\mathcal{AO}\).

An important class of policies for RDPs are the regular policies. Given an RDP \(\mathbf{R}\), a policy \(\pi:\mathcal{H}\rightarrow\Delta(\mathcal{A})\) is called _regular_ if \(\pi(h_{1})=\pi(h_{2})\) whenever \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\), for all \(h_{1},h_{2}\in\mathcal{H}\). Let \(\Pi_{\mathbf{R}}\) denote the set of regular policies for \(\mathbf{R}\). Regular policies exhibit powerful properties. First, under a regular policy, suffixes have the same probability of being generated for histories that map to the same RDP state. Second, there exists at least one optimal policy that is regular.

**Proposition 1**.: _Consider an RDP \(\mathbf{R}\), a regular policy \(\pi\in\Pi_{\mathbf{R}}\) and two histories \(h_{1}\) and \(h_{2}\) in \(\mathcal{H}_{t}\), \(t\in[H]\), such that \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\). For each suffix \(e_{t+1:H}\in\mathcal{E}_{H-t-1}\), the probability of generating \(e_{t+1:H}\) is the same for \(h_{1}\) and \(h_{2}\), i.e. \(\mathbb{P}(e_{t+1:H}\mid h_{1},\pi,\mathbf{R})=\mathbb{P}(e_{t+1:H}\mid h_{2}, \pi,\mathbf{R})\)._

**Proposition 2**.: _Each RDP \(\mathbf{R}\) has at least one optimal policy \(\pi^{*}\in\Pi_{\mathbf{R}}\)._

Due to Proposition 2, when solving an RDP \(\mathbf{R}\), we can restrict our search to the set of regular policies \(\Pi_{\mathbf{R}}\). A regular policy can be compactly defined as \(\pi:\mathcal{Q}\rightarrow\Delta(\mathcal{A})\), where \(\pi(q_{0})=\mathbbm{1}_{a_{\perp}}\) always selects the dummy action \(a_{\perp}\), and its value function as \(V^{\pi}:[H]\times\mathcal{Q}\rightarrow\mathbb{R}\).

Next, we define occupancy measures for RDPs. Given a regular policy \(\pi:\mathcal{Q}\rightarrow\Delta(\mathcal{A})\) and \(t\in[H]\), let \(d_{t}^{\pi}\in\Delta(\mathcal{Q}_{t}\times\mathcal{A}\,\mathcal{O})\) be the induced probability distribution over states in \(\mathcal{Q}_{t}\) and input symbols in \(\mathcal{A}\,\mathcal{O}\), recursively defined as \(d_{0}^{\pi}(q_{0},a_{0}o_{0}):=\theta_{\mathsf{o}}(q_{0},a_{0},o_{0})\) and

\[d_{t}^{\pi}(q_{t},a_{t}o_{t})\coloneqq\sum_{(q,ao)\in\tau^{-1}(q_{t})}d_{t-1} ^{\pi}(q,ao)\,\pi(q_{t},a_{t})\,\theta_{\mathsf{o}}(q_{t},a_{t},o_{t})\quad t >0.\]

We also overload the notation by writing \(d_{t}^{\pi}(q_{t},a_{t})=\sum_{o\in\mathcal{O}}d^{\pi}(q_{t},a_{t}o)\). Of particular interest is the occupancy distribution \(d_{t}^{*}\coloneqq d_{t}^{\pi^{*}}\), associated with an optimal policy \(\pi^{*}\).

**Example 1** (The cookie domain [28]).: The _cookie domain_ (Figure 1a) has three rooms connected by a hallway. The agent (purple triangle) can move in the four cardinal directions. When pressing a button in the orange room, a cookie randomly appears in either the green or the blue room. The agent receives a reward of \(+1\) for eating the cookie and it may then press the button again. This domain is partially observable since the agent can only see what is in the room that it currently occupies (Figure 1b). The cookie domain can be modelled as an episodic RDP with states \(\mathcal{Q}=[H+1]\times\mathcal{O}\times\mathcal{U}\), with \(\mathcal{U}=\{u_{1},u_{2},u_{3},u_{4}\}\). The value of \(\mathcal{U}\) is \(u_{1}\) when the button has _not_ been pressed yet (or not pressed since the last cookie was eaten). The value is \(u_{2}\) when the button has been pressed, but the agent has not visited the blue or green room yet. In this state, the environment has a \(50\%\) probability of generating the observation of a cookie when the agent enters either room for the first time. If the agent visits the green room and finds no cookie, the value becomes \(u_{3}\), meaning that the cookie is in the blue room. The meaning of \(u_{4}\) is dual to that of \(u_{3}\).

### Offline RL in RDPs

We are now ready to formally present the offline RL problem in episodic RDPs. Assume that we have access to a batch dataset \(\mathcal{D}\) collected through interacting with an unknown (but fixed) episodic RDP \(\mathbf{R}\) using a regular _behavior_ policy \(\pi^{\mathsf{b}}\). We assume that \(\mathcal{D}\) comprises \(N\) episodes, where the \(k\)-th episode is of the form \(e_{0:H}^{k}=a_{0}^{k}r_{0}^{k}\theta_{0}^{k}\cdots a_{H}^{k}r_{H}^{k}\theta_{H}^ {k}\), where \(q_{0}^{k}=q_{0}\) and where, for each \(t\in[H]\),

\[a_{t}^{k}\sim\pi^{\mathsf{b}}(q_{t}^{k}),\quad r_{t}^{k}\sim\theta_{\mathsf{r} }(q_{t}^{k},a_{t}^{k}),\quad o_{t}^{k}\sim\theta_{\mathsf{o}}(q_{t}^{k},a_{t}^ {k}),\quad q_{t+1}^{k}=\tau(q_{t}^{k},a_{t}^{k}o_{t}^{k}).\]

Figure 1: The _cookie_ domain: The agent can only see what is in the current room [28].

The goal is to compute a near-optimal policy \(\widehat{\pi}\) using the dataset \(\mathcal{D}\) (and without further exploration). More precisely, for a pre-specified accuracy \(\varepsilon\in(0,H]\), we aim to find an \(\varepsilon\)-optimal policy \(\widehat{\pi}\), using the smallest dataset \(\mathcal{D}\) possible. A policy \(\widehat{\pi}\) is \(\varepsilon\)-optimal iff \(\mathbb{E}_{h_{0}}[V_{0}^{*}(h_{0})-V_{0}^{\widehat{\infty}}(h_{0})]\leq\varepsilon\), where \(h_{0}=a_{\perp}o_{0}\), for some random \(o_{0}\in\mathcal{O}\).

By virtue of Proposition 2, one may expect that it is sufficient to search for regular \(\varepsilon\)-optimal policies, which is indeed the case. In order to learn an \(\varepsilon\)-optimal policy from \(\mathcal{D}\), some assumption is necessary regarding the policy \(\pi^{\text{b}}\) that was used to collect the episodes. Let \(d_{t}^{\text{b}}:=d_{t}^{\pi^{\text{b}}}\) be the occupancy distribution of \(\pi^{\text{b}}\). The following assumption requires that the behavior policy assigns a positive probability to all actions, which ensures that \(\pi^{\text{b}}\) explores the entire RDP.

**Assumption 1**.: \(\min_{t\in[H],q\in\mathcal{Q}_{\ell},a\in\mathcal{A}}d_{t}^{\text{b}}(q,a)>0\)_._

Assumption 1 is only needed by Theorem 6, which reconstructs the full unknown RDP. Theorem 8, instead, relies on a weaker assumption that can be expressed with the coefficient introduced in Definition 1.

The second assumption we require concerns the richness of \(\pi^{\text{b}}\) and its capability to allow us to distinguish the various RDP states. This is perfectly captured by notions of _distiguishability_ arising in automata theory, such as in Balle et al. [10]. We apply these concepts in our context, where such discrete distributions are generated from an RDP and a policy. Consider a minimal RDP \(\mathbf{R}\) with states \(\mathcal{Q}=\cup_{t\in[H+1]}\mathcal{Q}_{t}\). Given some policy \(\pi\), at each timestep \(t\in[H]\), every RDP state \(q\in\mathcal{Q}_{t}\) defines a unique probability distribution over the episode suffixes \(\mathcal{E}_{H-t}=(\mathcal{A}\mathcal{R}\mathcal{O})^{H-t+1}\). Then, the states in each \(\mathcal{Q}_{t}\) can be compared through the probability distributions they induce over \(\mathcal{E}_{H-t}\). Consider any \(L=\{L_{\ell}\}_{\ell=0}^{H}\), where each \(L_{\ell}\) is a metric over \(\Delta(\mathcal{E}_{\ell})\). We define the _\(L\)-distinguishability_ of \(\mathbf{R}\) and \(\pi\) as the maximum \(\mu_{0}\) such that, for any \(t\in[H]\) and any two distinct \(q,q^{\prime}\in\mathcal{Q}_{t}\), the probability distributions over suffix traces \(\epsilon_{t:H}\in\mathcal{E}_{\ell}\) from the two states satisfy

\[L_{H-t}(\mathbb{P}(e_{t:H}\mid q_{t}=q,\pi),\mathbb{P}(e_{t:H}\mid q_{t}=q^{ \prime},\pi))\geq\mu_{0}\,.\]

We will often omit the remaining length of the episode \(\ell=H-t\) from \(L_{\ell}\) and simply write \(L\). We consider the \(L_{\infty}^{p}\)-distinguishability, instantiating the definition above with the metric \(L_{\infty}^{p}(p_{1},p_{2})=\max_{u\in[\ell],e\in\mathcal{E}_{u}}\lvert p_{1}( e\ast)-p_{2}(e\ast)\rvert\), where \(p_{i}(e\ast)\) represents the probability of the trace prefix \(e\in\mathcal{E}_{u}\), followed by any trace \(e^{\prime}\in\mathcal{E}_{\ell-u-1}\). The \(L_{1}^{p}\)-distinguishability is defined analogously using \(L_{1}^{p}(p_{1},p_{2})=\sum_{u\in[\ell],e\in\mathcal{E}_{u}}\lvert p_{1}(e \ast)-p_{2}(e\ast)\rvert\). We can now require a positive distinguishability with our second assumption.

**Assumption 2**.: The behavior policy \(\pi^{\text{b}}\) has \(L_{\infty}^{p}\)-distinguishability of at least \(\mu_{0}>0\).

Finally, in order to capture the mismatch in occupancy measure between the optimal policy and the behavior policy, we introduce a key quantity called _single-policy RDP concentrability coefficient_, which extends the single-policy concentrability coefficient in MDPs to RDPs:

**Definition 1**.: The _single-policy RDP concentrability coefficient_ of an RDP \(\mathbf{R}\) with episode horizon \(H\) and with respect to a policy \(\pi^{\text{b}}\) is defined as:

\[C_{\mathbf{R}}^{\ast}=\max_{t\in[H],q\in\mathcal{Q}_{t},ao\in\mathcal{A} \mathcal{O}}\frac{d_{t}^{\ast}(q,ao)}{d_{t}^{b}(q,ao)}\,.\] (2)

The concentrability coefficient in Definition 1 resembles the notions of concentrability in MDPs (e.g., [14; 15]). It should be stressed, however, that those in MDPs are defined in terms of observation-action pairs \((o,a)\), whereas \(C_{\mathbf{R}}^{\ast}\) is defined in terms of _hidden_ RDP states and actions-observations, \((q,ao)\). It is worth remarking that \(C_{\mathbf{R}}^{\ast}\) could be equivalently defined in terms of state-action pairs \((q,a)\). Finally, in the special case where the RDP is Markovian - in which case it coincides with an episodic MDP - we have \(\mathcal{Q}\subseteq\mathcal{O}\times[H+1]\) and \(C_{\mathbf{R}}^{\ast}\) coincides with the standard single-policy concentrability coefficient for MDPs in [15]. This fact is shown in the proof of Corollary 17.

## 3 RegORL: Learning an Episodic RDP

In this section we present an algorithm for learning the transition function of an RDP \(\mathbf{R}\) from a dataset \(\mathcal{D}\) of episodes generated by a regular behavior policy \(\pi^{\text{b}}\). To simplify the presentation, we treat \(\mathcal{D}\) as a multiset of traces in \(\mathcal{E}_{H}\). The learning agent only has access to the non-Markovian tracesin \(\mathcal{D}\), and needs prior knowledge of \(\mathcal{A}\), \(\mathcal{R}\) and \(\mathcal{O}\), but no prior knowledge of \(\pi^{\text{b}}\) and \(\mathbf{R}\). Our algorithm is an adaptation of AdaCT [10] to episodic RDPs, and we thus refer to the algorithm as AdaCT-H.

The intuition behind AdaCT-H is that due to Proposition 1, two histories \(h_{1}\) and \(h_{2}\) should map to the same RDP state if they induce the same probability distribution on suffixes. AdaCT-H starts by adding an initial RDP state \(q_{0}\) to \(\mathcal{Q}_{0}\), whose suffixes are the full traces in \(\mathcal{D}\) (line 1). The algorithm then iteratively constructs the state sets \(\mathcal{Q}_{1},\ldots,\mathcal{Q}_{H+1}\). In each iteration \(t\in[H]\), AdaCT-H creates a set of candidate states \(\mathcal{Q}_{e,t+1}\) by extending all states in \(\mathcal{Q}_{t}\) with symbols in \(\mathcal{A}\,\mathcal{O}\) (line 3). We use \(qao\) to simultaneously refer to a candidate state and its state-symbol prefix \((q,ao)\). We associate each candidate state \(qao\) with a multiset of suffixes \(\mathcal{X}(qao)\), i.e. traces in \(\mathcal{E}_{H-t-1}\), obtained by selecting all suffixes in \(\mathcal{X}(q)\) that start with action \(a\) and observation \(o\) (line 4).

Next, AdaCT-H finds the candidate state whose suffix multiset has maximum cardinality, and promotes this candidate to \(\mathcal{Q}_{t+1}\) by defining the transition function \(\tau\) accordingly (lines 5-7). The algorithm then iterates over each remaining candidate state \(qao\in\mathcal{Q}_{e,t+1}\), comparing the distribution on suffixes in \(\mathcal{X}(qao)\) to those of states in \(\mathcal{Q}_{t+1}\) (line 9). If the suffix distribution is different from that of each state in \(\mathcal{Q}_{t+1}\), \(qao\) is promoted to \(\mathcal{Q}_{t+1}\) (line 10), else \(qao\) is merged with a state \(q^{\prime}\in\mathcal{Q}_{t+1}\) that has a similar suffix distribution (line 11). Finally, AdaCT-H returns the set of RDP states \(\mathcal{Q}\) and the associated transition function \(\tau\). The function TestDistinct compares two multisets \(\mathcal{X}_{1}\) and \(\mathcal{X}_{2}\) of traces in \(\mathcal{E}_{H-t-1}\) using the metric \(L^{\text{p}}_{\infty}\). For \(i\in\{1,2\}\) and each trace \(e\in\mathcal{E}_{H-t-1}\), let \(\widehat{p}_{i}(e)=\sum_{x\in\mathcal{X}_{i}}\mathbb{I}(x=e)/|\mathcal{X}_{i}|\) be the empirical estimate of \(p_{i}\), as the proportion of elements in \(\mathcal{X}_{i}\) equal to \(e\). TestDistinct compares \(L^{\text{p}}_{\infty}(\mathcal{X}_{1},\mathcal{X}_{2})\coloneqq L^{\text{p}} _{\infty}(\widehat{p}_{1},\widehat{p}_{2})\) to a confidence threshold.

Markov transformation.We are now ready to connect the RDP learning phase with the MDP learning phase. RDPs do not respect the Markov property over their observations and rewards, if automaton states remain hidden. However, we can use the reconstructed transition function \(\tau\) returned by AdaCT-H, extended over histories as \(\bar{\tau}:\mathcal{H}\to\mathcal{Q}\), to recover the Markov property. In what follows we formalize the notion of Markov transformation and the properties that its outputs satisfy.

**Definition 2**.: Let \(e_{0:H}\in\mathcal{E}_{H}\) be an episode collected from an RDP \(\mathbf{R}\) and a policy \(\pi^{\text{b}}\) that is regular in \(\mathbf{R}\). The _Markov transformation_ of \(e_{H}\) with respect to \(\mathbf{R}\) is the episode constructed as \(a_{0}r_{0}q_{1}\ldots a_{H}r_{H}q_{H+1}\), where \(q_{t+1}=\bar{\tau}(h_{t})\) and \(h_{t}=a_{0}o_{0}\cdots a_{t}o_{t}\), \(t\in[H]\). The Markov transformation of a dataset \(\mathcal{D}\) is the Markov transformation of all the episodes it contains.

A Markov transformation discards all observations from \(\mathcal{D}\) and replaces them with RDP states output by \(\bar{\tau}\). The dataset so constructed can be seen as generated from an MDP, which we define next.

**Definition 3**.: The episodic MDP _associated to_ an episodic RDP \(\mathbf{R}\) is \(\mathbf{M}_{\mathbf{R}}=\langle\mathcal{Q},\mathcal{A},\mathcal{R},T,\theta_{ \text{r}},H\rangle\), where \(T(q,a,q^{\prime})=\sum_{o\in\mathcal{O}}\mathbb{I}(q^{\prime}=\tau(q,ao))\, \theta_{\text{o}}(q,a,o)\) for each \((q,a,q^{\prime})\in\mathcal{Q}\times\mathcal{A}\times\mathcal{Q}\).

The associated MDP in Definition 3 is the decision process that corresponds to the Markov transformation of Definition 2, i.e. any episode produced with the Markov transformation can be equivalently seen as being generated from the associated MDP, in the sense of the following proposition.

**Proposition 3**.: _Let \(e_{0:H}\) be an episode sampled from an episodic MDP \(\mathbf{R}\) under a regular policy \(\pi\in\Pi_{\mathbf{R}}\), with \(\pi(h,a)=\pi_{r}(\bar{\tau}(h),a)\). If \(e^{\prime}_{H}\) is the Markov transformation of \(e_{H}\) with respect to \(\mathbf{R}\), then \(\mathbb{P}(e^{\prime}_{H}\mid\mathbf{R},\pi)=\mathbb{P}(e^{\prime}_{H}\mid \mathbf{M}_{\mathbf{R}},\pi_{r})\,,\) where \(\mathbf{M}_{\mathbf{R}}\) is the MDP associated to \(\mathbf{R}\)._

Rewards are not affected by the Markov transformation, only observations, implying the following.

**Proposition 4**.: _Let \(\pi\in\Pi_{\mathbf{R}}\) be a regular policy in \(\mathbf{R}\) such that \(\pi(h,a)=\pi_{r}(\bar{\tau}(h),a)\). Then \(\mathbb{E}[V^{\pi}_{0,\mathbf{R}}]=\mathbb{E}[V^{\pi_{r}}_{0,\mathbf{M}_{ \mathbf{R}}}]\), where \(V^{\pi}_{0,\mathbf{R}}\) and \(V^{\pi_{r}}_{0,\mathbf{M}_{\mathbf{R}}}\) are the values in the respective decision process, and \(\mathbb{E}[V^{\pi}_{0,\mathbf{R}}]=\mathbb{E}[V^{\pi}_{0,\mathbf{M}_{ \mathbf{R}}}]\), where expectations are with respect to randomness in \(o_{0}\)._

**Corollary 5**.: _Given \(\varepsilon\in(0,H]\), if \(\pi_{r}:\mathcal{Q}\to\Delta(\mathcal{A})\) is an \(\varepsilon\)-optimal policy of \(\mathbf{M}_{\mathbf{R}}\), the MDP associated to some RDP \(\mathbf{R}\), then, \(\pi(h,a)=\pi_{r}(\bar{\tau}(h),a)\) is \(\varepsilon\)-optimal in \(\mathbf{R}\)._

In summary, from Proposition 3, if \(\mathcal{D}_{\text{m}}\) is the Markov transformation of a dataset \(\mathcal{D}\) with respect to an RDP \(\mathbf{R}\), then, \(\mathcal{D}_{\text{m}}\) can be seen as being generated from the associated MDP \(\mathbf{M}_{\mathbf{R}}\). Hence, any offline RL algorithm for MDPs can be used for learning in \(\mathcal{D}_{\text{m}}\). Moreover, according to Corollary 5, any solution for \(\mathbf{M}_{\mathbf{R}}\) can be translated via \(\bar{\tau}\) into a policy for the original RDP, with the same guarantees.

Complete algorithm.The complete procedure is illustrated in Algorithm 1. Initially, the input dataset \(\mathcal{D}\) is separated in two halves. The first portion is used for learning the transition function of the unknown RDP with AdaCT-H (Section 3). If an upper bound \(\overline{Q}\) on \(|\mathcal{Q}|\) is available, it can optionally be provided to compute a more appropriate failure parameter for AdaCT-H. If not available, we adopt the upper bound of \(2(AO)^{H}\) states, which is valid for any instance, due to histories having finite length. As we will see in Theorem 6, this would only contribute linearly in \(H\) to the required dataset size. The output function computed by AdaCT-H is then used to compute a Markov transformation of the second phase, as specified in Definition 2. The resulting dataset, now Markovian, can be passed to a generic offline RL algorithm, which we represent with the function OfflineRL\((\mathcal{D},\varepsilon,\delta)\). In Appendix D, we instantiate it for a specific state-of-the-art offline RL algorithm.

``` Input: Dataset \(\mathcal{D}\), accuracy \(\varepsilon\in(0,H]\), failure probability \(0<\delta<1\), (optionally) upper bound \(\overline{Q}\) on \(|\mathcal{Q}|\) Output: Policy \(\hat{\pi}:\mathcal{H}\to\Delta(\mathcal{A})\)
1\(\mathcal{D}_{1},\mathcal{D}_{2}\leftarrow\) separate \(\mathcal{D}\) into two datasets of the same size
2\(\mathcal{Q},\tau\leftarrow\textsc{AdACT-H}(\mathcal{D}_{1},\delta/(AAO\overline{ Q}))\), where \(\overline{Q}=2(AO)^{H}\) if not provided
3\(\mathcal{D}_{2}^{\prime}\leftarrow\textsc{Markov transformation of }\mathcal{D}_{2}\) with respect to \(\bar{\tau}\) as in Definition 2
4\(\hat{\pi}_{m}\leftarrow\textsc{OfflineRL}(\mathcal{D}_{2}^{\prime}, \varepsilon,\delta/2)\) return\(\hat{\pi}:h\mapsto\hat{\pi}_{m}(\bar{\tau}(h))\) ```

**Algorithm 1**Full procedure (RegORL)

## 4 Theoretical Guarantees

We now turn to theoretical performance guarantees of RegORL. Our main performance result is a sample complexity bound in Theorem 7, ensuring that, for any accuracy \(\varepsilon\in(0,H]\), RegORL finds an \(\varepsilon\)-optimal policy. We also report a sample complexity bound for AdaCT-H in Theorem 6, and an alternative bound in Theorem 8. In comparison, the sample complexity bound for AdaCT [10] is

\[\widetilde{\mathcal{O}}\left(\frac{Q^{4}A^{2}O^{2}H^{5}\log(1/\delta)}{ \varepsilon^{2}}\max\left\{\frac{1}{\mu_{0}^{2}},\frac{H^{4}O^{2}A^{2}}{ \varepsilon^{4}}\right\}\right).\]

We achieve a tighter bound by using Bernstein's inequalities and exploiting the finiteness of histories.

**Theorem 6**.: _Consider a dataset \(\mathcal{D}\) of episodes sampled from an RDP \(\mathbf{R}\) and a regular policy \(\pi^{\text{b}}\in\Pi_{\mathbf{R}}\). With probability \(1-\delta\), the output of AdaCT-H\((\mathcal{D},\delta/(2QAO))\) is the transition function of the minimal RDP equivalent to \(\mathbf{R}\), provided that \(|\mathcal{D}|\geq N_{\delta}\), where_

\[N_{\delta}\coloneqq\frac{21\log(8QAO/\delta)}{d_{\min}^{\text{b}}\,\mu_{0}} \sqrt{H\log(2ARO)}\in\widetilde{\mathcal{O}}\left(\frac{\sqrt{H}}{d_{\min}^{ \text{b}}\,\mu_{0}}\right),\]\(d^{\mathsf{b}}_{\min}:=\min\{d^{\mathsf{b}}_{t}(q,a)\mid t\in[H],q\in\mathcal{Q}_{t},a\in\mathcal{A}O,d^{\mathsf{b}}_{t}(q,a)>0\}\) is the minimal occupancy distribution, and \(\mu_{0}\) is the \(L^{\mathsf{p}}_{\infty}\)-distinguishability._

The proof appears in Appendix C.2. Theorem 6 tells us that the sample complexity of AdaCT-H, to return a minimal RDP, is inversely proportional to \(\mu_{0}\), the \(L^{\mathsf{p}}_{\infty}\)-distinguishability of \(\mathbf{R}\) and \(\pi^{\mathsf{b}}\), and the minimal occupancy \(d^{\mathsf{b}}_{\min}\). Note that \(d^{\mathsf{b}}_{\min}\leq 1/(QOA)\). The bound also depends on \(Q\), the number of RDP states, implicitly through \(d^{\mathsf{b}}_{\min}\) and explicitly via a logarithmic term. In the absence of prior knowledge of \(Q\), one may use in the argument of Algorithm 1 the worst-case upper bound \(\overline{Q}=2(AO)^{H}\). The sample complexity would then have an additional linear term in \(H\), since \(\overline{Q}\) is only used in the logarithmic term to set the appropriate value of \(\delta\). However, this will not impact the value of the \(d^{\mathsf{b}}_{\min}\) term.

Theorem 6 is a sample complexity guarantee for the first phase of the algorithm, which learns \(\tau\), the structure of the minimal RDP that is equivalent to the underlying RDP. If \(\delta\) is the desired failure probability of the complete algorithm, RegORL executes AdaCT-H so that its success probability is at least \(1-\delta/2\). This means that with the same probability, \(\mathcal{D}^{\mathsf{r}}_{2}\) is an MDP dataset with the properties listed in Section 3. As a consequence, provided that OfflineRL is some generic \((\varepsilon,\delta/2)\)-PAC offline RL algorithm for MDPs, the output of RegORL is an \(\varepsilon\)-optimal policy with probability \(1-\delta\).

**Theorem 7**.: _Consider a dataset \(\mathcal{D}\) of episodes sampled from an RDP \(\mathbf{R}\) and a regular policy \(\pi^{\mathsf{b}}\in\Pi_{\mathbf{R}}\). For any \(\varepsilon\in(0,H]\) and \(0<\delta<1\), if OfflineRL is an \((\varepsilon,\delta/2)\)-PAC offline algorithm for MDPs with sample complexity \(N_{\mathsf{m}}\), then, the output of RegORL\((\mathcal{D},\varepsilon,\delta)\) is an \(\varepsilon\)-optimal policy in \(\mathbf{R}\), with probability at least \(1-\delta\), provided that \(|\mathcal{D}|\geq 2\max\{N_{\delta/2},N_{\mathsf{m}}\}\)._

As we can see, the sample complexity requirement separates for the two phases. While \(N_{\delta/2}\) is due to the RDP learning component, defined in Theorem 6, the quantity \(N_{\mathsf{m}}\) completely depends on the offline RL algorithm for MDPs that is adopted. Among other terms, the performance guarantees of offline algorithms can often be characterized through the single-policy concentrability for MDPs \(C^{*}\). However, since states become observations in the associated MDP, due to the properties of Proposition 3, \(C^{*}\) coincides with \(C^{*}_{\mathbf{R}}\), the RDP single-policy concentrability of Definition 1.

In Appendix D, we demonstrate a specific instantiation of RegORL with an off-the-shelf offline RL algorithm from the literature by Li et al. [16]. This yields the following requirement for \(N_{\mathsf{m}}\):

\[N_{\mathsf{m}}\geq\frac{c\,H^{3}QC^{*}_{\mathbf{R}}\log\frac{2HN_{\mathsf{m}} }{\delta}}{\varepsilon^{2}},\]

for a constant \(c>0\).

To eliminate the dependence that Theorem 6 has on \(d^{\mathsf{b}}_{\min}\), we develop a variant of AdaCT-H which does not learn a complete RDP. Rather, it only reconstructs a subset of states that are likely under the behavior policy. The algorithm, which we call AdaCT-H-A (with 'A' standing for "approximation"), is defined in Appendix C.3. Theorem 8 is an upper bound on the sample complexity of AdaCT-H-A that takes the accuracy \(\varepsilon\) as input and returns the transition function of an \(\varepsilon/2\)-approximate RDP \(\mathbf{R}^{\prime}\), whose optimal policy is \(\varepsilon/2\)-optimal for the original RDP \(\mathbf{R}\). By performing a Markov transformation for \(\mathbf{R}^{\prime}\) and using an \((\varepsilon/2,\delta/2)\)-PAC offline algorithm for MDPs, we can compute an \(\varepsilon\)-optimal policy for \(\mathbf{R}\). The total sample complexity can be combined in the same way as in Theorem 7.

**Theorem 8**.: _Consider a dataset \(\mathcal{D}\) of episodes sampled from an RDP \(\mathbf{R}\) and a regular policy \(\pi^{\mathsf{b}}\in\Pi_{\mathbf{R}}\). With probability \(1-\delta\), the output of AdaCT-H-A, called with \(\mathcal{D}\), \(\delta/(2QOA)\) and \(\varepsilon\in(0,H]\) in input, is the transition function of an \(\varepsilon/2\)-approximate RDP \(\mathbf{R}^{\prime}\), provided that \(|\mathcal{D}|\geq N^{\prime}_{\delta}\), where_

\[N^{\prime}_{\delta}\coloneqq\frac{504HQAOC^{*}_{\mathbf{R}^{\prime}},\log(16 QAO/\delta)}{\varepsilon\,\mu_{0}}\sqrt{H\log(2ARO)}\in\widetilde{\mathcal{O}}\left( \frac{H^{3/2}QAOC^{*}_{\mathbf{R}^{\prime}}}{\varepsilon\,\mu_{0}}\right).\]

This theorem does not rely on Assumption 1, because a finite \(C^{*}_{\mathbf{R}}\) suffices.

## 5 Sample Complexity Lower Bound

The main result of this section is Theorem 9, a sample complexity lower bound for offline RL in RDPs. It shows that the dataset size required by any RL algorithm scales with the relevant parameters.

**Theorem 9**.: _For any \((C_{\mathbf{R}}^{*},H,\varepsilon,\mu_{0})\) satisfying \(C_{\mathbf{R}}^{*}\geq 2\), \(H\geq 2\) and \(\varepsilon\leq H\mu_{0}/64\), there exists an RDP with horizon \(H\), \(L_{1}^{\mathsf{p}}\)-distinguishability \(\mu_{0}\) and a regular behavior policy \(\pi^{\mathsf{b}}\) with RDP single-policy concentrability \(C_{\mathbf{R}}^{*}\), such that if \(\mathcal{D}\) has been generated using \(\pi^{\mathsf{b}}\) and \(\mathbf{R}\), and_

\[|\mathcal{D}|\notin\Omega\left(\frac{H}{\mu_{0}}+\frac{C_{\mathbf{R}}^{*}H^{2 }}{\varepsilon^{2}}\right)\] (3)

_then, for any algorithm \(\mathfrak{A}:\mathcal{D}\mapsto\widehat{\pi}\) returning non-Markov deterministic policies, the probability that \(\widehat{\pi}\) is not \(\varepsilon\)-optimal is at least \(1/4\)._

The proof relies on worst-case RDP instances that carefully combine two-armed bandits with noisy parity functions. This last component allows to capture the difficulty of learning in presence of temporal dependencies. Figure 2 shows an RDP in this class. At the beginning of each episode, the observation causes a transition towards either the bandit component (bottom branch) or the noisy parity function (top branches). Acting optimally in the two parity branches requires to predict the output of a parity function, which depends on some unknown binary code (of length \(3\), in the example). The first term in Theorem 9 is due to this component, because the code scales linearly with \(H\), or \(Q\), while the amount of information revealed about the code is controlled by \(\mu_{0}\). The second term is caused by the required optimality in the bandit.

Differently from this lower bound, the parameter \(\mu_{0}\), appearing in the upper bounds of Theorems 6 and 8, is a \(L_{\infty}^{\mathsf{p}}\)-distinguishability. However, the two are related, since \(L_{1}^{\mathsf{p}}(q,q^{\prime})\geq L_{\infty}^{\mathsf{p}}(q,q^{\prime})\). Intuitively, the \(L_{1}^{\mathsf{p}}\)-distinguishability accounts for all the information that is available as differences in episode probabilities. The \(L_{\infty}^{\mathsf{p}}\)-distinguishability, on the other hand, quantifies the maximum the difference in probability associated to specific suffixes. This is the information used by the algorithm and the one appearing in the two upper bounds.

## 6 Conclusion

In this paper we propose an algorithm for Offline RL in episodic Regular Decision Processes. Our algorithm exploits automata learning techniques to reduce the problem of RL in RDPs, in which observations and rewards are non-Markovian, into standard offline RL for MDPs. We provide the first high-probability sample complexity guarantees for this setting, as well as a lower bound that shows how its complexity relates to the parameters that characterize the decision process and the behavior policy. We identify the RDP single-policy concentrability as an analogous quantity to the one used for MDPs in the literature. Our sample complexity upper bound depends on the \(L_{\infty}^{\mathsf{p}}\)-distinguishability of the behavior policy. As a future work, we plan to investigate if any milder notion of distinguishability also suffices. This is motivated by our lower bound which only involves the \(L_{1}^{\mathsf{p}}\)-distinguishability over the same policy. Finally, our results have strong implications for online learning in RDPs, which is a relevant setting to be explored.

Figure 2: One episodic RDP instance \(\mathbf{R}_{101,1}\in\mathbb{R}(L,H,\xi,\eta)\), associated to the parity function \(f_{101}\), with code \(101\), and the optimal arm \(a_{1}^{\prime}\). The length is \(L=|101|=3\), the horizon \(H=5\), the noise parameter \(\xi\) and the bandit bonus parameter is \(\eta\). The transition function only depends on the observations, not the actions. The output distributions are: \(u=\mathrm{unif}\{0,1\}\), \(u_{+}=\mathrm{unif}\{+,-\}\), \(v_{\alpha}(+)=(1+\alpha)/2\), \(v_{\alpha}(-)=(1-\alpha)/2\). The star denotes any symbol. If the label of a state \(q\) is \(a.d\), then the observation function is \(\theta_{\mathsf{o}}(q,a)=d\). Refer to Appendix E for details.

## Acknowledgments

Roberto Cipollone is partially supported by the EU H2020 project AIPlan4EU (No. 101016442), the ERC-ADG White-Mech (No. 834228), the EU ICT-48 2020 project TAILOR (No. 952215), the PRIN project RIPER (No. 20203FFYLK), and the PNRR MUR project FAIR (No. PE0000013). Anders Jonsson is partially supported by the EU ICT-48 2020 project TAILOR (No. 952215), AGAUR SGR, and the Spanish grant PID2019-108141GB-I00. Alessandro Ronca is partially supported by the ERC project WhiteMech (No. 834228), and the ERC project ARiAT (No. 852769). Mohammad Sadegh Talebi is partially supported by the Independent Research Fund Denmark, grant number 1026-00397B.

## References

* [1] Martin L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. Wiley, 1994.
* [2] Fahiem Bacchus, Craig Boutilier, and Adam J. Grove. Rewarding behaviors. In _AAAI_, pages 1160-1167, 1996.
* [3] Ronen I. Brafman and Giuseppe De Giacomo. Regular decision processes: A model for non-markovian domains. In _IJCAI_, pages 5516-5522, 2019.
* [4] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. _Artif. Intell._, 101(1-2):99-134, 1998.
* [5] Christos H. Papadimitriou and John N. Tsitsiklis. The complexity of Markov decision processes. _Mathematics of Operations Research_, 12(3):441-450, 1987.
* [6] Chi Jin, Sham M. Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efficient reinforcement learning of undercomplete POMDPs. In _NeurIPS_, 2020.
* [7] Hongyi Guo, Qi Cai, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang. Provably efficient offline reinforcement learning for partially observable Markov decision processes. In _ICML_, pages 8016-8038, 2022.
* [8] Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A PAC RL algorithm for episodic POMDPs. In _AISTATS_, pages 510-518, 2016.
* [9] Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Reinforcement learning of POMDPs using spectral methods. In _COLT_, pages 193-256, 2016.
* [10] Borja Balle, Jorge Castro, and Ricard Gavalda. Learning probabilistic automata: A study in state distinguishability. _Theor. Comput. Sci._, 473:46-60, 2013.
* [11] Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In _ICLR_, 2022.
* [12] Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. In _NeurIPS_, pages 4065-4078, 2021.
* [13] Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _ICML_, pages 1042-1051, 2019.
* [14] Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. In _NeurIPS_, pages 27395-27407, 2021.
* [15] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. In _NeurIPS_, pages 11702-11716, 2021.
* [16] Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of model-based offline reinforcement learning. _CoRR_, abs/2204.05275, 2022.
* [17] Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In _COLT_, pages 2730-2775, 2022.
* [18] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline RL? In _ICML_, pages 5084-5096, 2021.
* [19] Tongzheng Ren, Jialian Li, Bo Dai, Simon S Du, and Sujay Sanghavi. Nearly horizon-free offline reinforcement learning. In _NeurIPS_, pages 15621-15634, 2021.
* [20] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline RL in low-rank MDPs. In _ICLR_, 2022.
* [21] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In _International Conference on Machine Learning_, pages 2139-2148. PMLR, 2016.

* [22] Hamid Reza Maei, Csaba Szepesvari, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy learning control with function approximation. In _ICML_, pages 719-726, 2010.
* [23] Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evaluation in Markov decision processes. _The Journal of Machine Learning Research_, 21(1):6742-6804, 2020.
* [24] Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. A review of off-policy evaluation in reinforcement learning. _arXiv preprint arXiv:2212.06355_, 2022.
* [25] Eden Abadi and Ronen I. Brafman. Learning and solving regular decision processes. In _IJCAI_, pages 1948-1954, 2020.
* [26] Alessandro Ronca and Giuseppe De Giacomo. Efficient PAC reinforcement learning in regular decision processes. In _IJCAI_, pages 2026-2032, 2021.
* [27] Alessandro Ronca, Gabriel Paludo Licks, and Giuseppe De Giacomo. Markov abstractions for PAC reinforcement learning in non-Markov decision processes. In _IJCAI_, pages 3408-3415, 2022.
* [28] Rodrigo Toro Icarte, Ethan Waldie, Toryn Q. Klassen, Richard Anthony Valenzano, Margarita P. Castro, and Sheila A. McIlraith. Learning reward machines for partially observable reinforcement learning. In _NeurIPS_, pages 15497-15508, 2019.
* [29] Marcus Hutter. Feature reinforcement learning: Part I. Unstructured MDPs. _J. Artif. Gen. Intell._, 1(1): 3-24, 2009.
* [30] Joel Veness, Kee Siong Ng, Marcus Hutter, William T. B. Uther, and David Silver. A Monte-Carlo AIXI approximation. _J. Artif. Intell. Res._, 40:95-142, 2011.
* [31] M. M. Hassan Mahmud. Constructing states for reinforcement learning. In _ICML_, pages 727-734, 2010.
* [32] Jayakumar Subramanian, Amit Sinha, Raihan Seraj, and Aditya Mahajan. Approximate information state for approximate planning and reinforcement learning in partially observed systems. _The Journal of Machine Learning Research_, 23(1):483-565, 2022.
* [33] Karl Johan Astrom. Optimal control of Markov processes with incomplete state information. _Journal of Mathematical Analysis and Applications_, 10(1):174-205, 1965.
* [34] Satinder Singh, Michael L. Littman, Nicholas K. Jong, David Pardoe, and Peter Stone. Learning predictive state representations. In _ICML_, pages 712-719, 2003.
* [35] Michael R. James and Satinder Singh. Learning and discovery of predictive state representations in dynamical systems with reset. In _ICML_, 2004.
* [36] Michael H. Bowling, Peter McCracken, Michael James, James Neufeld, and Dana F. Wilkinson. Learning predictive state representations using non-blind policies. In _ICML_, pages 129-136, 2006.
* [37] Alex Kulesza, Nan Jiang, and Satinder Singh. Spectral learning of predictive state representations with insufficient statistics. In _AAAI_, pages 2715-2721, 2015.
* [38] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D. Lee. PAC reinforcement learning for predictive state representations. In _ICLR_, 2023.
* [39] Odalric-Ambrym Maillard, Remi Munos, and Daniil Ryabko. Selecting the state-representation in reinforcement learning. In _NeurIPS_, pages 2627-2635, 2011.
* [40] Phuong Nguyen, Odalric-Ambrym Maillard, Daniil Ryabko, and Ronald Ortner. Competing with an infinite set of models in reinforcement learning. In _AISTATS_, pages 463-471, 2013.
* [41] Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, and Daniil Ryabko. Optimal regret bounds for selecting the state representation in reinforcement learning. In _ICML_, pages 543-551, 2013.
* [42] Ronald Ortner, Matteo Pirotta, Alessandro Lazaric, Ronan Fruit, and Odalric-Ambrym Maillard. Regret bounds for learning state representations in reinforcement learning. In _NeurIPS_, pages 12717-12727, 2019.
* [43] Tor Lattimore, Marcus Hutter, and Peter Sunehag. The sample-complexity of general reinforcement learning. In _ICML_, pages 28-36, 2013.
* [44] Rodrigo Toro Icarte, Toryn Q. Klassen, Richard Anthony Valenzano, and Sheila A. McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In _ICML_, pages 2112-2121, 2018.
* [45] Zhe Xu, Ivan Gavran, Yousef Ahmad, Rupak Majumdar, Daniel Neider, Ufuk Topcu, and Bo Wu. Joint inference of reward machines and policies for reinforcement learning. In _ICAPS_, pages 590-598, 2020.
* [46] Giuseppe De Giacomo, Luca Iocchi, Marco Favorito, and Fabio Patrizi. Foundations for restraining bolts: Reinforcement learning with LTLf/LDLf restraining specifications. In _ICAPS_, pages 128-136, 2019.
* [47] Giuseppe De Giacomo, Marco Favorito, Luca Iocchi, Fabio Patrizi, and Alessandro Ronca. Temporal logic monitoring rewards via transducers. In _KR_, pages 860-870, 2020.

* Hasanbeig et al. [2021] Mohammad Hasanbeig, Natasha Yogananda Jeppu, Alessandro Abate, Tom Melham, and Daniel Kroening. DeepSynth: Automata synthesis for automatic task segmentation in deep reinforcement learning. In _AAAI_, pages 7647-7656, 2021.
* Bourel et al. [2023] Hippolyte Bourel, Anders Jonsson, Odalric-Ambrym Maillard, and Mohammad Sadegh Talebi. Exploration in reward machines with low regret. In _International Conference on Artificial Intelligence and Statistics_, pages 4114-4146. PMLR, 2023.
* Fu and Topcu [2014] Jie Fu and Ufuk Topcu. Probably approximately correct MDP learning and control with temporal logic constraints. In _RSS_, 2014.
* Hasanbeig et al. [2020] Mohammadhossein Hasanbeig, Alessandro Abate, and Daniel Kroening. Cautious reinforcement learning with logical constraints. In _AAMAS_, pages 483-491, 2020.
* Hammond et al. [2021] Lewis Hammond, Alessandro Abate, Julian Gutierrez, and Michael J. Wooldridge. Multi-agent reinforcement learning with temporal logic specifications. In _AAMAS_, pages 583-592, 2021.
* Bozkurt et al. [2020] Alper Kamil Bozkurt, Yu Wang, Michael M. Zavlanos, and Miroslav Pajic. Control synthesis from linear temporal logic specifications using model-free reinforcement learning. In _ICRA_, pages 10349-10355, 2020.
* Hahn et al. [2019] Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, and Dominik Wojtczak. Omega-regular objectives in model-free reinforcement learning. In _TACAS_, pages 395-412, 2019.
* Majeed and Hutter [2018] Sultan Javed Majeed and Marcus Hutter. On Q-learning convergence for non-Markov decision processes. In _IJCAI_, pages 2546-2552, 2018.
* Ron et al. [1998] Dana Ron, Yoram Singer, and Naftali Tishby. On the learnability and usage of acyclic probabilistic finite automata. _J. Comput. Syst. Sci._, 56(2):133-152, 1998.
* Clark and Tholmard [2004] Alexander Clark and Franck Tholmard. PAC-learnability of probabilistic deterministic finite state automata. _J. Mach. Learn. Res._, 5:473-497, 2004.
* Palmer and Goldberg [2007] Nick Palmer and Paul W. Goldberg. PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance. _Theor. Comput. Sci._, 387(1):18-31, 2007.
* Balle [2013] Borja Balle. _Learning Finite-State Machines: Statistical and Algorithmic Aspects_. PhD thesis, Universitat Politecnica de Catalunya, 2013.
* Balle et al. [2014] Borja Balle, Jorge Castro, and Ricard Gavalda. Adaptively learning probabilistic deterministic automata from data streams. _Machine Learning_, 96(1):99-127, 2014.
* Moore [1956] Edward Forrest Moore. Gedanken-experiments on sequential machines. _Automata studies_, 34:129-153, 1956.
* Hopcroft et al. [2006] John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. _Introduction to Automata Theory, Languages, and Computation_. Addison-Wesley Longman Publishing Co., Inc., 2006.
* Shallit [2008] Jeffrey Shallit. _A second course in formal languages and automata theory_. Cambridge University Press, 2008.
* Brafman et al. [2018] Ronen I. Brafman, Giuseppe De Giacomo, and Fabio Patrizi. LTLf/LDLf non-markovian rewards. In _AAAI_, pages 1771-1778, 2018.
* Gaon and Brafman [2020] Maor Gaon and Ronen I. Brafman. Reinforcement learning with non-markovian rewards. In _AAAI_, pages 3980-3987, 2020.
* McCallum [1996] Andrew Kachites McCallum. _Reinforcement Learning with Selective Perception and Hidden State_. PhD thesis, University of Rochester, 1996.
* Rissanen [1983] Jorma Rissanen. A universal data compression system. _IEEE Transactions on information theory_, 29(5):656-664, 1983.
* Ron et al. [1996] Dana Ron, Yoram Singer, and Naftali Tishby. The power of amnesia: Learning probabilistic automata with variable memory length. _Mach. Learn._, 25(2-3):117-149, 1996.
* Hutter [2016] Marcus Hutter. Extreme state aggregation beyond Markov decision processes. _Theor. Comp. Sci._, pages 73-91, 2016.
* Maurer and Pontil [2009] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample-variance penalization. In _COLT_, 2009.
* Dann et al. [2017] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In _NeurIPS_, pages 5713-5723, 2017.
* Cover and Thomas [2006] Thomas M. Cover and Joy A. Thomas. _Elements of Information Theory (2. Ed.)_. Wiley, 2006.
* Szorenyi [2009] Balazs Szorenyi. Characterizing statistical query learning: Simplified notions and proofs. In _ALT_, pages 186-200, 2009.

* [74] Avrim Blum, Merrick L. Furst, Jeffrey C. Jackson, Michael J. Kearns, Yishay Mansour, and Steven Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In _STOC_, pages 253-262, 1994.
* [75] Sumegha Garg, Ran Raz, and Avishay Tal. Extractor-based time-space lower bounds for learning. In _STOC_, pages 990-1002, 2018.
* [76] Sumegha Garg, Pravesh K. Kothari, Pengda Liu, and Ran Raz. Memory-sample lower bounds for learning parity with noise. _CoRR_, abs/2107.02320, 2021.

## Appendices

* A Extended Discussion of Related Work
* A.1 Offline RL in MDPs
* A.2 Online RL in RDPs
* A.3 Non-Markov Rewards and Reward Machines
* A.4 State Representations
* A.5 Feature MDPs and General RL
* A.6 Learning PDFA
* B RDP properties
* B.1 RDPs and Regular Policies
* B.2 Markov Transformation
* C Sample Complexity of AdaCT-H
* C.1 Preliminaries
* C.2 Proof of Theorem 6
* C.3 Proof of Theorem 8
* C.4 Distinguishability Parameters
* D RegORL with Subsampled VI-LCB
* E Sample Complexity Lower Bound: Proof of Theorem 9
* E.1 Learning parity with noise
* E.2 Class of hard RDP instances
* E.3 Proof of Theorem 9
Extended Discussion of Related Work

Some of the related work mentioned in the introduction requires a more extensive discussion, which we provide below.

### Offline RL in MDPs

There is a rich and growing literature on offline RL in MDPs; see, e.g., [11; 12; 13; 14; 15; 16; 17; 18; 19; 20]. A closely related line of work is off-policy learning in MDPs [21; 22; 23]; we refer to the recent survey [24] for a discussion on how the various settings are related. For offline RL in MDPs, the papers cited above report learning algorithms with theoretical guarantees on their sample efficiency. The majority of these algorithms are designed based on the _pessimism principle_. While most literature focuses on tabular MDPs, the case of linear function approximation is discussed in some papers, e.g., [20]. In several settings, the presented algorithms are shown to be minimax optimal. For instance, in the case of tabular episodic MDPs, it is established in [16] that the optimal sample complexity depends on the size of state-space, episode length, as well as some notion of concentrability reflecting the distribution mismatch between the behavior and optimal policy.

### Online RL in RDPs

RDPs have been introduced in [3] as a formalism based on temporal logic. They admit an equivalent formulation in terms of automata, which is favoured in the context of RL. Several algorithms for _online_ RL in RDPs exist [25; 26; 27] but complexity bounds are only given in [26] for the infinite-horizon discounted setting. This work [26] shows the correspondence between RDPs and Probabilistic Deterministic Finite Automata (PDFA), and it introduces the idea of using PDFA-learning techniques to learn RDPs. Their sample complexity bounds are not immediately comparable to ours, due to the different setting. Importantly, this algorithm uses the uniform policy for learning. So, the algorithm might be adapted to our setting only under the assumption that the behaviour policy is uniform. Even in this case, our bounds show an improved dependency on several key quantities. Furthermore, we provide a sample complexity lower bound, whereas their results are limited to showing that a dependency on the quantities occurring in their upper bounds is necessary.

The first RL algorithm for RDPs appears in [25] for the online discounted setting. It is automaton-based, and in particular, it learns the RDP in the form of a Mealy machine. The algorithm is shown in [26] to incur an exponential dependency on the length of the relevant histories. An algorithm that achieves performance guarantees building on the techniques from [26] but also integrating an effective exploration strategy is given in [27]. This work also introduces the idea of seeing the transition function of a PDFA as a Markov abstraction of the histories to be passed to an RL algorithm for MDPs, so as to employ it in a modular manner.

The algorithms in [28; 29; 30; 31] apply to RDPs even though they have not been developed specifically for RDPs. Toro Icarte et al. [28] present an RL algorithm for the subclass of POMDPs that have a finite set of reachable belief states. Such POMDPs can be captured by finite-state automata, and are in fact RDPs. Their algorithm is based on automata learning, but it does not come with an analysis of its performance guarantees. The RL techniques presented in [29; 30] for _feature MDPs_ are in fact applicable to episodic RDPs. The techniques are based on suffix trees, rather than automata. There are cases when the size of the smallest suffix tree is exponential in the horizon, while an automaton of linear size exists--see section below on feature MDPs. Thus their techniques cannot yield optimal bounds for RDPs. Mahmud [31] introduces an RL algorithm for _Deterministic Markov Decision Models_ (MDDs). Such MDDs are also automaton-based, and they are more general than RDPs since the automaton is only required to predict rewards. Thus their RL algorithm applies to RDPs. However, it is an algorithm without guarantees.

### Non-Markov Rewards and Reward Machines

MDPs with non-Markov rewards are a special case of NMDPs, where only rewards are non-Markovian. Namely, observed states satisfy the Markov property, while rewards may depend on the history of past states. The specific kind of non-Markov rewards considered in the literature amount to the subclass of RDPs where the automaton's state is only needed to predict the next reward--while the next observation (i.e., state) can be predicted from the last observation.

Non-Markov rewards can already be found in [2], where the reward function is specified in a temporal logic of the past. More recently, the setting has been revisited with so-called _reward machines_[44] as well as with temporal logics of the future on finite traces [64; 47]. A reward machine is a finite automaton (or transducer) used to specify a non-Markovian reward function. Reward machines have been introduced in [44] along with an RL algorithm that assumes the reward machine to be known. RL algorithms with unknown reward machine, or equivalently unknown temporal specification, are presented in [65; 45], with no performance guarantees. Reward machines have been generalised so as to predict observations as well [28], which makes them equivalent to RDPs--as mentioned above.

The first performance bounds for RL with reward machines have been recently established in [49]. The work shows regret bounds that take into account the structure provided by a reward machine, and hence improve over the bounds that one would obtain by naively adapting regret bounds for MDPs.

An automaton-based method for dealing with non-Markov sparse rewards is proposed in [48].

### State Representations

State representations are maps from histories to a finite state space. The map defined by the transition function of an RDP is a state representation. The studies on state representations [39; 40; 41; 42] focus on regret bounds for RL given a candidate set of state representations. While in our case the state representations are concretely defined by the class of finite-state automata, in their case they are arbitrary maps. This is a challenging setting, which does not allow for taking advantage of the properties of specific classes of state representations. The regret bounds in [39; 41; 42] are for finite sets of state representations, and they all show a linear dependency on the cardinality of the given set of state representations. In our case, the candidate state representations correspond to the set of automata with at most \(Q=2(AO)^{H}\) states and \(AO\) input letters. Such a set contains at least \(Q^{QAO}\) automata--the number of distinct transition functions. Thus, if we could instantiate their bounds in our setting, they would have an exponential dependency on the number \(Q\) of RDP states, and hence a doubly-exponential dependency on the horizon \(H\). We avoid this dependency, obtaining polynomial bounds in the mentioned quantities.

Nguyen et al. [40] consider the case of a countably infinite set of state representations, and present an algorithm whose regret bound does not show a dependency such as the one discussed above. Instead, they show a dependency on a quantity \(K_{0}\), which admits several interpretations, including one based on the descriptional complexity of the candidate state representations. Thus, there may be a way to relate \(K_{0}\) to the quantities we use in our bounds. However, the formal relationship between the two, if any, renders highly non-trivial, which prevents one to use their ideas in the case of RDPs. We believe establishing a formal relationship between their model and RDPs is an interesting, yet challenging, topic for future work. Furthermore, it should be stressed that even if the relationship was clear and one could borrow ideas from [40], the resulting sample complexity bound would have to grow as \(1/\varepsilon^{3}\) in view of their regret bound scaling as \(T^{2/3}\). In contrast, our bounds achieve an optimal dependency of \(1/\varepsilon^{2}\) on \(\varepsilon\).

### Feature MDPs and General RL

Hutter [29] introduces _feature MDPs_, where histories are mapped to states by a feature map. It relates to our work since the map provided by the transition function of an RDP is a feature map. The concrete feature maps considered in [29] are based on U-Trees [66]. The idea is also revisited in [30] with Prediction Suffix Trees (PSTs) [67; 68]. Both U-Trees and PSTs are suffix trees. There are cases when their size is exponential in the horizon, while an automaton of linear size exists. For instance, in the case of a parity condition over the history. To see this, note that a suffix \(x\) of a bit string \(bx\) does not suffice to establish parity of \(bx\). In fact, the parity of \(0x\) is different from the parity of \(1x\). Thus a suffix tree for parity must encode all suffixes, and hence it will have a number of leaves that is exponential in the maximum length of a relevant string--the horizon \(H\) in the case of episodic RL.

Hutter [69] provides several formal characterisations of feature maps. All the characterisations are less restrictive than the one defined by an RDP. In particular, their most restrictive characterisation is given in their Equation (6). It only requires to preserve the ability to predict rewards, not observations--this is also adopted by Mahmud [31]. The states of our automata suffice to predict observations as well. It is unclear whether automata techniques can be used to learn directly abstractions that do not preserve the dynamics entirely.

Majeed and Hutter [55] study convergence of Q-learning when executed over the state space of an MDP that underlies a non-Markov decision processes. Such a state space corresponds to the state space of the RDP, but they do not consider the problem of learning the state space.

Lattimore et al. [43] consider General RL as the problem of RL when we are given a set of candidate NMDPs, rather than assuming the decision process to belong to a fixed class. Similarly to the works on state representations, it does not commit to specific classes of NMDPs, and their bounds have a linear dependency on the number of candidate models. As remarked above, in our setting, it amounts to an exponential dependency on the number of states of the candidate RDPs, and hence a doubly-exponential dependency on the horizon; we avoid such exponential dependencies.

### Learning PDFA

Our algorithms for learning an RDP borrow and improve over techniques for learning Probabilistic-Deterministic Finite Automata (PDFA). The first PAC learning algorithm for acyclic PDFA has been presented in [56], then followed by extensions and variants that can handle PDFA with cycles [57, 58, 10, 59, 60]. All bounds feature some variant of a _distinguishability parameter_, which we adopt in our bounds, properly adapting it to the offline RL setting. Our algorithm builds upon the state-of-the-art algorithm AdaCT [10], and we derive bounds that are a substantial improvement over the ones that can be obtained from a straightforward application of any existing PDFA-learning algorithm to the offline RL setting.

## Appendix B RDP properties

In this section we prove several properties of RDPs that are stated in Sections 2 and 3.

### RDPs and Regular Policies

In this section, we prove Propositions 1 and 2.

**Proposition 1**.: _Consider an RDP \(\mathbf{R}\), a regular policy \(\pi\in\Pi_{\mathbf{R}}\) and two histories \(h_{1}\) and \(h_{2}\) in \(\mathcal{H}_{t}\), \(t\in[H]\), such that \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\). For each suffix \(e_{t+1:H}\in\mathcal{E}_{H-t-1}\), the probability of generating \(e_{t+1:H}\) is the same for \(h_{1}\) and \(h_{2}\), i.e. \(\mathbb{P}(e_{t+1:H}\mid h_{1},\pi,\mathbf{R})=\mathbb{P}(e_{t+1:H}\mid h_{2}, \pi,\mathbf{R})\)._

Proof.: By induction on \(t\). For \(t=H\), all histories in \(\mathcal{H}_{H}\) generate the empty suffix in \((\mathcal{ARO})^{0}\) with probability \(1\) (the stop symbol is omitted). For \(t<H\), the probability of generating a suffix \(aroe_{t+2:H}\) is

\[\mathbb{P} (aroe_{t+2:H}\mid h_{1},\pi,\mathbf{R})=\pi(h_{1},a)\cdot\mathbb{ P}(r,o\mid\bar{\tau}(h_{1}),a,\mathbf{R})\cdot\mathbb{P}(e_{t+2:H}\mid h_{1}ao, \pi,\mathbf{R})\] \[=\pi(h_{2},a)\cdot\mathbb{P}(r,o\mid\bar{\tau}(h_{2}),a,\mathbf{ R})\cdot\mathbb{P}(e_{t+2:H}\mid h_{2}ao,\pi,\mathbf{R})=\mathbb{P}(aroe_{t+2:H} \mid h_{2},\pi,\mathbf{R}),\]

where we have used the fact that \(\pi\) is regular, \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\), \(\bar{\tau}(h_{1}ao)=\tau(\bar{\tau}(h_{1}),ao)=\tau(\bar{\tau}(h_{2}),ao)= \bar{\tau}(h_{2}ao)\), and the induction hypothesis. 

The following statement appears in the literature [3, Theorem 2], but the authors do not provide a complete proof, so for completeness we prove the statement here.

**Proposition 2**.: _Each RDP \(\mathbf{R}\) has at least one optimal policy \(\pi^{*}\in\Pi_{\mathbf{R}}\)._

Proof.: Given \(\mathbf{R}\), consider any optimal policy \(\pi^{*}:\mathcal{H}\rightarrow\Delta(\mathcal{A})\), not necessarily regular. We prove the statement by constructing a policy \(\pi\) and showing by induction on \(t\in[H]\) that \(\pi\) is both optimal and regular. The base case is given by \(t=H\). In this case, for an arbitrary \(a\in\mathcal{A}\), define \(\pi(h)\coloneqq\mathbbm{1}_{a}\) for each history \(h\in\mathcal{H}_{H}\). Since \(V_{H}^{\pi}(h)=0\) by definition, \(\pi\) is optimal for each history \(h\in\mathcal{H}_{H}\), and regular since it always selects the same action.

For \(t<H\), we first construct a new policy \(\pi_{\mathsf{c}}\) which is the composition of policies \(\pi^{*}\) and \(\pi\). Concretely, for each history \(h\in\mathcal{H}_{u}\) such that \(u\leq t\), \(\pi_{\mathsf{c}}(h)=\pi^{*}(h)\) acts according to \(\pi^{*}\), while for each history \(h\in\mathcal{H}_{u}\) such that \(u>t\), \(\pi_{\mathsf{c}}(h)=\pi(h)\) acts according to \(\pi\). Clearly, \(\pi_{\mathsf{c}}\) is an optimal policy for \(\mathbf{R}\) since \(\pi^{*}\) is optimal and since by induction, \(\pi\) is optimal for histories in \(\mathcal{H}_{u}\), \(u>t\).

Consider a pair of histories \(h_{1}\) and \(h_{2}\) in \(\mathcal{H}_{t}\) such that \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\) but \(\pi_{\mathsf{c}}(h_{1})\neq\pi_{\mathsf{c}}(h_{2})\). Define \(\pi(h_{1})\coloneqq\pi(h_{2})\coloneqq\pi_{\mathsf{c}}(h_{1})\). Since the value function can be written as an expectation over suffixes, due to Proposition 1 and the fact that \(\pi\) is regular for histories in \(\mathcal{H}_{u}\), \(u>t\), we have \(V_{t}^{\pi}(h_{1})=V_{t}^{\pi}(h_{2})\). Since \(\pi_{\mathsf{c}}\) is the same as \(\pi\) for histories in \(\mathcal{H}_{u}\), \(u>t\), this implies \(V_{t}^{\pi_{\mathsf{c}}}(h_{1})=V_{t}^{\pi_{\mathsf{c}}}(h_{1})\leq V_{t}^{\pi _{\mathsf{c}}}(h_{2})\) since \(\pi_{\mathsf{c}}\) is optimal for \(h_{2}\). If we were to instead define \(\pi(h_{1}):=\pi(h_{2}):=\pi_{\mathsf{c}}(h_{2})\), we would obtain \(V_{t}^{\pi_{\mathsf{c}}}(h_{2})\leq V_{t}^{\pi_{\mathsf{c}}}(h_{1})\). The only possibility is \(V_{t}^{\pi_{\mathsf{c}}}(h_{1})=V_{t}^{\pi_{\mathsf{c}}}(h_{2})\), which is the same value achieved by the policy \(\pi\). Hence \(\pi\) is optimal for \(h_{1}\) and \(h_{2}\).

We now repeat the same procedure for each pair of histories \(h_{1}\) and \(h_{2}\) in \(\mathcal{H}_{t}\) such that \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\) but \(\pi_{\mathsf{c}}(h_{1})\neq\pi_{\mathsf{c}}(h_{2})\). If necessary, we complete the definition of \(\pi\) by copying the action choices of \(\pi_{\mathsf{c}}\). The resulting policy \(\pi\) is optimal for each history \(h\in\mathcal{H}_{t}\), and regular since it makes the same action choices for each pair of histories \(h_{1}\) and \(h_{2}\) in \(h\in\mathcal{H}_{t}\) such that \(\bar{\tau}(h_{1})=\bar{\tau}(h_{2})\). 

### Markov Transformation

In this section, we verify the properties of the Markov transformation, which is the intermediate step that \(\mathtt{RegORL}\) uses to recover the Markov property in the original dataset. This transformation has been formalized in Definition 2. We use \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\) to denote the original and the transformed datasets, respectively.

**Proposition 3**.: _Let \(e_{0:H}\) be an episode sampled from an episodic RDP \(\mathbf{R}\) under a regular policy \(\pi\in\Pi_{\mathbf{R}}\), with \(\pi(h,a)=\pi_{\mathsf{r}}(\bar{\tau}(h),a)\). If \(e_{H}^{\prime}\) is the Markov transformation of \(e_{H}\) with respect to \(\mathbf{R}\), then \(\mathbb{P}(e_{H}^{\prime}\mid\mathbf{R},\pi)=\mathbb{P}(e_{H}^{\prime}\mid \mathbf{M}_{\mathbf{R}},\pi_{\mathsf{r}})\,,\) where \(\mathbf{M}_{\mathbf{R}}\) is the MDP associated to \(\mathbf{R}\)._

Proof.: For \(t\in[H]\), let \(e_{t}\in\mathcal{E}_{t}=(\mathcal{ARO})^{t+1}\) be an episode prefix in \(\mathbf{R}\), \(\phi(e_{t})\in\mathcal{E}_{t}^{\prime}=(\mathcal{ARQ})^{t+1}\) be its Markov transformation and \(e_{t}^{\prime}\in\mathcal{E}_{t}^{\prime}\) be an episode of the associated MDP. The function \(\phi:\mathcal{E}\to\mathcal{E}^{\prime}\) transforms the observations according to \(\bar{\tau}\), and preserves actions and rewards. The statement says that \(\mathbb{P}(\phi(e_{t})\mid\mathbf{R},\pi)=\mathbb{P}(e_{t}^{\prime}\mid \mathbf{M}_{\mathbf{R}},\pi_{\mathsf{r}})\) (note that \(\phi(e_{t})\) and \(e_{t}^{\prime}\) are distinct random variables). We prove this by induction. For \(t=0\), we recall that the irrelevant quantities \(a_{0},r_{0}\) are constant and,

\[\mathbb{P}(\phi(a_{0}r_{0}o_{0})=a_{0}r_{0}q\mid\mathbf{R},\pi) =\sum_{o\in\mathcal{O}}\mathbb{I}(\tau(q_{0},a_{0}o)=q)\,\theta_{ \mathsf{o}}(q_{0},a_{0},o)\] \[=T(q_{0},a_{0},q)\] \[=\mathbb{P}(e_{0}^{\prime}=a_{0}r_{0}q\mid\mathbf{M}_{\mathbf{R} },\pi_{\mathsf{r}})\] (4)

where \(T:\mathcal{Q}\times\mathcal{A}\to\Delta(\mathcal{Q})\) is the transition function of \(\mathbf{M}_{\mathbf{R}}\), from Definition 3. Due to the role of the dummy action, \(T(q_{0},a_{0})\) is the initial distribution of the MDP.

For the inductive step, assume that \(\mathbb{P}(\phi(e_{t-1})\mid\mathbf{R},\pi)=\mathbb{P}(e_{t-1}^{\prime}\mid \mathbf{M}_{\mathbf{R}},\pi_{\mathsf{r}})\). Then, for any \(e^{\prime}\in\mathcal{E}_{t-1}^{\prime}\), \(arq\in\mathcal{ARQ}\), if \(q^{\prime}\) is the last element of \(e^{\prime}\), we have

\[\mathbb{P}(\phi(e_{t})=e^{\prime}arq\mid\mathbf{R},\pi)=\mathbb{P} (\phi(e_{t-1})=e^{\prime}\mid\mathbf{R},\pi)\,\mathbb{P}(a_{t}r_{t}q_{t+1}= arq\mid\phi(e_{t-1})=e^{\prime},\mathbf{R},\pi)\] (5) \[=\mathbb{P}(e_{t-1}^{\prime}=e^{\prime}\mid\mathbf{M}_{\mathbf{R} },\pi_{\mathsf{r}})\,\mathbb{P}(a_{t}r_{t}q_{t+1}=arq\mid q_{t}=q^{\prime}, \mathbf{R},\pi)\] (6) \[=\mathbb{P}(e_{t-1}^{\prime}=e^{\prime}\mid\mathbf{M}_{\mathbf{R} },\pi_{\mathsf{r}})\,\pi_{\mathsf{r}}(q^{\prime},a)\,\theta_{\mathsf{r}}(q^{ \prime},a,r)\,\sum_{o\in\mathcal{O}}\theta_{\mathsf{o}}(q^{\prime},a,o)\, \mathbb{I}(q=\tau(q^{\prime},ao))\] (7) \[=\mathbb{P}(e_{t-1}^{\prime}=e^{\prime}\mid\mathbf{M}_{\mathbf{R} },\pi_{\mathsf{r}})\,\pi_{\mathsf{r}}(q^{\prime},a)\,\theta_{\mathsf{r}}(q^{ \prime},a,r)\,T(q^{\prime},a,q)\] (8) \[=\mathbb{P}(e_{t}^{\prime}=e^{\prime}arq\mid\mathbf{M}_{\mathbf{R} },\pi_{\mathsf{r}})\] (9)

where, in (6), we have used the induction hypothesis and the fact that \(a_{t}r_{t}q_{t+1}\) are Markov in \(q^{\prime}\) by regularity of the policy. 

Thanks to this relation, the values of corresponding policies are also related in the following way.

**Proposition 4**.: _Let \(\pi\in\Pi_{\mathbf{R}}\) be a regular policy in \(\mathbf{R}\) such that \(\pi(h,a)=\pi_{\mathsf{r}}(\bar{\tau}(h),a)\). Then \(\mathbb{E}[V_{0,\mathbf{R}}^{\pi}]=\mathbb{E}[V_{0,\mathbf{M}_{\mathbf{R}}}^{ \pi_{\mathsf{r}}}]\), where \(V_{0,\mathbf{R}}^{\pi}\) and \(V_{0,\mathbf{M}_{\mathbf{R}}}^{\pi_{\mathsf{r}}}\) are the values in the respective decision process, and \(\mathbb{E}[V_{0,\mathbf{R}}^{\pi}]=\mathbb{E}[V_{0,\mathbf{M}_{\mathbf{R}}}^{ \pi}]\), where expectations are with respect to randomness in \(o_{0}\)._

Proof.: The statement is composed of two parts. First, we show that \(\mathbb{E}[V_{0,\mathbf{R}}^{\pi}]=\mathbb{E}[V_{0,\mathbf{M}_{\mathbf{R}}}^{\pi_{ \mathsf{r}}}]\), which is a direct consequence of Proposition 3. Following the same convention as in the proof of Proposition 3,we use \(\mathcal{E}^{\prime}_{t}=(\mathcal{ARQ})^{t+1}\) and \(\phi\) for the Markov transformation. Then,

\[\mathbb{E}[V^{\pi}_{0,\mathbf{R}}] =\sum_{r_{1}\dots r_{H}\in\mathcal{R}^{H+1}}\mathbb{P}(r_{1:H}=r_{ 1}\dots r_{H}\mid\mathbf{R},\pi)\,\sum_{i=1}^{H}r_{i}\] (10) \[=\sum_{e^{\prime}\in\mathcal{E}^{\prime}_{H}}\mathbb{P}(\phi(e_{ H})=e^{\prime}\mid\mathbf{R},\pi)\,\sum_{i=1}^{H}r_{i}\] (11) \[=\sum_{e^{\prime}\in\mathcal{E}^{\prime}_{H}}\mathbb{P}(e^{ \prime}_{H}=e^{\prime}\mid\mathbf{M}_{\mathbf{R}},\pi_{\mathbf{r}})\,\sum_{i= 1}^{H}r_{i}\] (12) \[=\mathbb{E}[V^{\pi_{\mathbf{r}}}_{0,\mathbf{M}_{\mathbf{R}}}]\] (13)

For the second part of the statement, let \(\Pi_{\mathbf{R}}\) and \(\Pi_{\mathbf{M}}\) be the regular and the Markov policies in \(\mathbf{R}\) and \(\mathbf{M}_{\mathbf{R}}\), respectively. Then, using Proposition 2 and the first part of this statement,

\[V^{\pi}_{0,\mathbf{R}}=\max_{\pi\in\Pi_{\mathbf{R}}}\mathbb{E}[V^{\pi}_{0, \mathbf{R}}]=\max_{\pi\in\Pi_{\mathbf{R}}}\mathbb{E}[V^{\pi}_{0,\mathbf{M}_{ \mathbf{R}}}]=\max_{\pi_{\mathbf{\prime}}\in\Pi_{\mathbf{M}}}\mathbb{E}[V^{ \pi}_{0,\mathbf{M}_{\mathbf{R}}}]=V^{\pi}_{0,\mathbf{M}_{\mathbf{R}}}\] (14)

Corollary 5 is a consequence of the two parts of Proposition 4. Since \(V^{\pi}_{0,\mathbf{R}}=V^{\pi}_{0,\mathbf{M}_{\mathbf{R}}}\) and the value achieved by the optimal policy in the respective model is the same, \(\varepsilon\)-optimality of \(\pi\) in \(\mathbf{R}\) implies \(\varepsilon\)-optimality of \(\pi_{\mathbf{r}}\) in \(\mathbf{M}_{\mathbf{M}}\), and vice versa.

## Appendix C Sample Complexity of AdaCT-H

In this section we prove high-probability upper bounds on the sample complexity of AdaCT-H.

### Preliminaries

We first state Hoeffding's inequality for Bernoulli variables. In what follows we take \(\log\) to be the natural logarithm.

**Lemma 10** (Hoeffding's inequality).: _Let \(X_{1},\dots,X_{N}\) be \(N\) independent random Bernoulli variables with the same expected value \(\mathbb{E}[X_{1}]=p\), and let \(\widehat{p}_{N}=\sum_{i=1}^{N}X_{i}/N\) be an empirical estimate of \(p\). Then, for any \(\delta\in(0,1)\),_

\[\mathbb{P}\left(|\widehat{p}_{N}-p|\geq\sqrt{\frac{\log(2/\delta)}{2N}}\right) \leq\delta.\] (15)

An alternative to Hoeffding's inequality is the empirical Bernstein inequality, which can be expressed as follows for Bernoulli variables [70, 71].

**Lemma 11** (Empirical Bernstein inequality).: _Let \(X_{1},\dots,X_{N}\) be \(N\) independent random Bernoulli variables with the same expected value \(\mathbb{E}[X_{1}]=p\), and let \(\widehat{p}_{N}=\sum_{i=1}^{N}X_{i}/N\) be an empirical estimate of \(p\). Then, for any \(\delta\in(0,1)\),_

\[\mathbb{P}\left(|\widehat{p}_{N}-p|\geq\sqrt{\frac{2\widehat{p}\log(4/\delta) }{N}}+\frac{14\log(4/\delta)}{3N}\right)\leq\delta.\] (16)

If \(X\sim p_{X}\) is a discrete random variable, the entropy of \(X\) is \(H(X)=-\sum_{x\in\mathcal{X}}p_{X}(x)\,\log p_{X}(x)\). Further, for \(x\in(0,1)\), we define the binary entropy function as \(H_{2}(x)=-x\log(x)-(1-x)\log(1-x)\). If \((X,Y)\sim p_{XY}\) are two discrete variables, the conditional entropy is \(H(Y\mid X)=\sum_{x\in\mathcal{X}}p_{X}(x)\,H(Y\mid X=x)\). The mutual information is \(I(X;Y)=I(Y;X)=D_{\mathrm{KL}}(p_{XY}\parallel p_{X}\cdot p_{Y})\), where \(D_{\mathrm{KL}}\) is the Kullback-Leibler divergence. If \(X,Y,Z\) are three random variables, we write \(X\to Y\to Z\) if the conditional distribution of \(Z\) does not depend on \(X\), given \(Y\). With these definition, we state Fano's inequality, as one can find in Cover and Thomas [72], (2.140).

**Theorem 12** (Fano's inequality).: _Let \(X\to Y\to\hat{X}\), for \(X,\hat{X}\in\mathcal{X}\) and \(P_{e}=\mathbb{P}(\hat{X}\neq X)\). Then,_

\[H_{2}(P_{e})+P_{e}\,\log(|\mathcal{X}|-1)\geq H(X\mid Y).\] (17)

### Proof of Theorem 6

In this section we prove Theorem 6, which states a high-probability upper bound on the sample complexity of AdaCT-H. The first two lemmas are adaptations of Lemmas 19 and 20 in Balle et al. [10] to the episodic setting.

**Lemma 13**.: _For \(t\in[H]\), let \(\mathcal{X}_{1}\) and \(\mathcal{X}_{2}\) be multisets sampled from distributions \(p_{1}\) and \(p_{2}\) in \(\Delta(\mathcal{E}_{H-t-1})\). If \(p_{1}=p_{2}\), then TestDistinct\((t,\mathcal{X}_{1},\mathcal{X}_{2},\delta)\) returns False with probability \(1-\delta\)._

Proof.: For each \(i\in\{1,2\}\) and each trace \(e\in\mathcal{E}_{H-t-1}\), we can view each episode as a random Bernoulli variable with expected value \(p_{i}(e)\) that takes value \(1\) if we observe \(e\), and \(0\) otherwise. Let \(\widehat{p}_{i}(e)=\sum_{x\in\mathcal{X}_{i}}\mathbb{I}(x=e)/|\mathcal{X}_{i}|\) be the empirical estimate of \(p_{i}\), i.e. the proportion of elements in \(\mathcal{X}_{i}\) equal to \(e\). For each \(i\in\{1,2\}\), each \(u\in[H-t-1]\) and each prefix \(e_{0:u}\in\mathcal{E}_{u}\), Hoeffding's inequality yields

\[\mathbb{P}\left(|\widehat{p}_{i}(e_{0:u}\ast)-p_{i}(e_{0:u}\ast)|\geq\sqrt{ \frac{\log(2/\delta_{\mathsf{s}})}{2|\mathcal{X}_{i}|}}\right)\leq\delta_{ \mathsf{s}}.\]

The total number of non-empty prefixes of \(\mathcal{E}_{H-t-1}\) equals a geometric sum:

\[(ARO)^{1}+\cdots+(ARO)^{H-t}=\frac{(ARO)^{H+1-t}-1}{ARO-1}-1\leq 2(ARO)^{H-t}.\]

Choosing \(\delta_{\mathsf{s}}=\delta/4(ARO)^{H-t}\) and taking a union bound implies that the above inequality holds for each \(i\in\{1,2\}\) and each \(e_{0:u}\) simultaneously with probability \(1-4(ARO)^{H-t}\delta_{\mathsf{s}}=1-\delta\), implying

\[L^{\mathsf{p}}_{\infty}(\mathcal{X}_{1},\mathcal{X}_{2}) =\max_{u,e_{0:u}}|\widehat{p}_{1}(e_{0:u}\ast)-\widehat{p}_{2}(e_ {0:u}\ast)|\leq L^{\mathsf{p}}_{\infty}(p_{1},p_{2})+\sqrt{\frac{\log(2/ \delta_{\mathsf{s}})}{2|\mathcal{X}_{1}|}}+\sqrt{\frac{\log(2/\delta_{\mathsf{ s}})}{2|\mathcal{X}_{2}|}}\] \[\leq 0+2\sqrt{\frac{\log(2/\delta_{\mathsf{s}})}{2\min(| \mathcal{X}_{1}|,|\mathcal{X}_{2}|)}}=\sqrt{\frac{2\log(8(ARO)^{H-t}/\delta)} {\min(|\mathcal{X}_{1}|,|\mathcal{X}_{2}|)}},\]

which is precisely the condition under which TestDistinct\((t,\mathcal{X}_{1},\mathcal{X}_{2},\delta)\) returns False. 

**Lemma 14**.: _For \(t\in[H]\), let \(\mathcal{X}_{1}\) and \(\mathcal{X}_{2}\) be multisets sampled from distributions \(p_{1}\) and \(p_{2}\) in \(\Delta(\mathcal{E}_{H-t-1})\). If the \(L^{\mathsf{p}}_{\infty}\)-distinguishability of \(\pi^{\mathsf{b}}\) is \(\mu_{0}\), then TestDistinct\((t,\mathcal{X}_{1},\mathcal{X}_{2},\delta)\) returns True with probability \(1-\delta\) provided that_

\[\min(|\mathcal{X}_{1}|,|\mathcal{X}_{2}|)\geq\frac{8}{\mu_{0}^{2}}\left(\log( 2(ARO)^{H-t})+\log(4/\delta)\right).\]

Proof.: Using the same argument as in the proof of Lemma 13, Hoeffding's inequality yields

\[\mathbb{P}\left(|\widehat{p}_{i}(e_{0:u}\ast)-p_{i}(e_{0:u}\ast)|>\sqrt{ \frac{\log(2/\delta_{\mathsf{s}})}{2|\mathcal{X}_{i}|}}\right)\leq\delta_{ \mathsf{s}},\]

with the inequality holding simultaneously for \(i\in\{1,2\}\) and each prefix \(e_{0:u}\) with probability \(1-\delta\) by choosing \(\delta_{\mathsf{s}}=\delta/4(ARO)^{H-t}\). Choosing \(\mu_{0}\geq 4\sqrt{\log(2/\delta_{\mathsf{s}})/2|\mathcal{X}_{i}|}\) for each \(i\in\{1,2\}\) yields

\[|\mathcal{X}_{i}|\geq\min(|\mathcal{X}_{1}|,|\mathcal{X}_{2}|)\geq\frac{8}{ \mu_{0}^{2}}\log(2/\delta_{\mathsf{s}})=\frac{8}{\mu_{0}^{2}}\left(\log(2(ARO )^{H-t})+\log(4/\delta)\right).\]

In this case we have

\[L^{\mathsf{p}}_{\infty}(\mathcal{X}_{1},\mathcal{X}_{2}) =\max_{u,e_{0:u}}|\widehat{p}_{1}(e)-\widehat{p}_{2}(e)|\geq L^{ \mathsf{p}}_{\infty}(p_{1},p_{2})-\sqrt{\frac{\log(2/\delta_{\mathsf{s}})}{2| \mathcal{X}_{1}|}}-\sqrt{\frac{\log(2/\delta_{\mathsf{s}})}{2|\mathcal{X}_{2} |}}\] \[\geq\mu_{0}-\frac{\mu_{0}}{4}-\frac{\mu_{0}}{4}=\frac{\mu_{0}}{2} \geq 2\sqrt{\frac{\log(2/\delta_{\mathsf{s}})}{2\min(|\mathcal{X}_{1}|,| \mathcal{X}_{2}|)}}=\sqrt{\frac{2\log(8(ARO)^{H-t}/\delta)}{\min(|\mathcal{X}_{ 1}|,|\mathcal{X}_{2}|)}},\]

which is precisely the condition under which TestDistinct\((t,\mathcal{X}_{1},\mathcal{X}_{2},\delta)\) returns True.

We are now ready to prove Theorem 6, which we restate below:

**Theorem 6**.: _Consider a dataset \(\mathcal{D}\) of episodes sampled from an RDP \(\mathbf{R}\) and a regular policy \(\pi^{\mathsf{b}}\in\Pi_{\mathbf{R}}\). With probability \(1-\delta\), the output of \(\textsc{{AdaC}}\)-\(\textsc{{H}}(\mathcal{D},\delta/(2QAO))\) is the transition function of the minimal RDP equivalent to \(\mathbf{R}\), provided that \(|\mathcal{D}|\geq N_{\delta}\), where_

\[N_{\delta}\coloneqq\frac{21\log(8QAO/\delta)}{d_{\min}^{\mathsf{b}}\,\mu_{0} }\sqrt{H\log(2ARO)}\in\widetilde{\mathcal{O}}\left(\frac{\sqrt{H}}{d_{\min}^{ \mathsf{b}}\,\mu_{0}}\right),\]

\(d_{\min}^{\mathsf{b}}:=\min\{d_{t}^{\mathsf{b}}(q,ao)\mid t\in[H],q\in\mathcal{ Q}_{t},ao\in\mathcal{AO},d_{t}^{\mathsf{b}}(q,ao)>0\}\) _is the minimal occupancy distribution, and \(\mu_{0}\) is the \(L^{\mathsf{b}}_{\infty}\)-distinguishability._

Proof.: The proof consists in choosing \(N\) and \(\delta\) such that the condition in Lemma 14 is true with high probability for each application of TestDistinct. Consider an iteration \(t\in[H]\) of AdaCT-H. For a candidate state \(qao\in\mathcal{Q}_{\mathsf{c},t+1}\), its associated probability is \(d_{t}^{\mathsf{b}}(q,ao)\) with empirical estimate \(\widehat{p}_{t}(qao)=|\mathcal{X}(qao)|/N\), i.e. the proportion of episodes in \(\mathcal{D}\) that are consistent with \(qao\). We can apply the empirical Bernstein inequality in (16) to show that

\[\mathbb{P}\left(\left|\widehat{p}_{t}(qao)-d_{t}^{\mathsf{b}}(q,ao)\right| \geq\sqrt{\frac{2\widehat{p}_{t}(qao)\ell}{N}}+\frac{14\ell}{3N}=\frac{\sqrt{ 2M\ell}+14\ell/3}{N}\right)\leq\delta,\]

where \(M=|\mathcal{X}(qao)|\), \(\ell=\log(4/\delta)\), and \(\delta\) is the failure probability of AdaCT-H. To obtain a bound on \(M\) and \(N\), assume that we can estimate \(d_{t}^{\mathsf{b}}(q,ao)\) with accuracy \(d_{t}^{\mathsf{b}}(q,ao)/2\), which yields

\[\frac{d_{t}^{\mathsf{b}}(q,ao)}{2} \geq\frac{\sqrt{2M\ell}+14\ell/3}{N}\] (18) \[\widehat{p}_{t}(qao) \geq d_{t}^{\mathsf{b}}(q,ao)-\frac{\sqrt{2M\ell}+14\ell/3}{N} \geq d_{t}^{\mathsf{b}}(q,ao)-\frac{d_{t}^{\mathsf{b}}(q,ao)}{2}=\frac{d_{t} ^{\mathsf{b}}(q,ao)}{2}.\] (19)

Combining these two results, we obtain

\[M=N\widehat{p}_{t}(qao)\geq Nd_{t}^{\mathsf{b}}(q,ao)/2\geq\frac{N}{2N}\left( \sqrt{2M\ell}+14\ell/3\right)=\frac{1}{2}\left(\sqrt{2M\ell}+14\ell/3\right).\] (20)

Solving for \(M\) yields \(M\geq 4\ell\), which is subsumed by the bound on \(M\) in Lemma 14 since \(\mu_{0}<1\). Hence the bound on \(M\) in Lemma 14 is sufficient to ensure that we estimate \(d_{t}^{\mathsf{b}}(q,ao)\) with accuracy \(d_{t}^{\mathsf{b}}(q,ao)/2\). We can now insert the bound on \(M\) from Lemma 14 into (18) to obtain a bound on \(N\):

\[N\geq\frac{2(\sqrt{2M\ell}+14\ell/3)}{d_{t}^{\mathsf{b}}(q,ao)}\geq\frac{2 \ell}{d_{t}^{\mathsf{b}}(q,ao)}\left(\frac{4}{\mu_{0}}\sqrt{\frac{(H-t)\log(2 ARO)}{\ell}}+1+\frac{14}{3}\right)\equiv N_{1}.\] (21)

To simplify the bound, we can choose any value larger than \(N_{1}\):

\[N_{1} \leq\frac{2\ell}{d_{t}^{\mathsf{b}}(q,ao)}\left(\frac{4}{\mu_{0} }\sqrt{H\log(2ARO)+H\log(2ARO)}+\frac{14}{3\mu_{0}}\sqrt{H\log(2ARO)}\right)\] \[<\frac{21\ell}{d_{\min}^{\mathsf{b}}\mu_{0}}\sqrt{H\log(2ARO)} \equiv N_{0},\] (22)

where we have used \(d_{t}^{\mathsf{b}}(q,ao)\geq d_{\min}^{\mathsf{b}}\), \(\mu_{0}<1\), \(\ell=\log 4+\log(1/\delta)\geq 1\), \(H\log(2ARO)\geq\log 4\geq 1\) and \(4\sqrt{2}+14/3<\frac{21}{2}\). Choosing \(\delta=\delta_{0}/2QAO\), a union bound implies that accurately estimating \(d_{t}^{\mathsf{b}}(q,ao)\) for each candidate state \(qao\) and accurately estimating \(p(e_{0:u}*)\) for each prefix in the multiset \(\mathcal{X}(qao)\) associated with \(qao\) occurs with probability \(1-2QAO\delta=1-\delta_{0}\), since there are at most \(QAO\) candidate states. Substituting the expression for \(\delta\) in \(N_{0}\) yields the bound in the theorem.

It remains to show that the resulting RDP is minimal. We show the result by induction. The base case is given by the set \(\mathcal{Q}_{0}\), which is clearly minimal since it only contains the initial state \(q_{0}\). For \(t\in[H]\), assume that the algorithm has learned a minimal RDP for sets \(\mathcal{Q}_{0},\ldots,\mathcal{Q}_{t}\). Let \(\mathcal{Q}_{t+1}\) be the set of states at layer \(t+1\) of a minimal RDP. Due to Proposition 1, each pair of histories that map to a state \(q_{t+1}\in\mathcal{Q}_{t+1}\) generate the same probability distribution over suffixes. Hence by Lemma 13, with high probability \(\textsc{TestDistinct}(t,\mathcal{X}(qao),\mathcal{X}(q^{\prime}a^{\prime}o^{ \prime}),\delta)\) returns false for each pair of candidate states \(qao\) and \(q^{\prime}a^{\prime}o^{\prime}\) that map to \(q_{t+1}\). Consequently, the algorithm merges \(qao\) and \(q^{\prime}a^{\prime}o^{\prime}\). On the other hand, by assumption, each pair of histories that map to different states of \(\mathcal{Q}_{t+1}\) have \(L^{\mathsf{p}}_{\infty}\)-distinguishability \(\mu_{0}\). Hence by Lemma 14, with high probability \(\textsc{TestDistinct}(t,\mathcal{X}(qao),\mathcal{X}(q^{\prime}a^{\prime}o^{ \prime}),\delta)\) returns true for each pair of candidate states \(qao\) and \(q^{\prime}a^{\prime}o^{\prime}\) that map to different states in \(\mathcal{Q}_{t+1}\). Consequently, the algorithm does not merge \(qao\) and \(q^{\prime}a^{\prime}o^{\prime}\). It follows that with high probability, AdaCT-H will generate exactly the set \(\mathcal{Q}_{t+1}\), which is that of a minimal RDP. 

### Proof of Theorem 8

In this section we prove Theorem 8, which states an alternative upper bound on the sample complexity of AdaCT-H. The proof requires an alternative definition of the algorithm, which we call AdaCT-H-A (for "approximation").

**Theorem 8**.: _Consider a dataset \(\mathcal{D}\) of episodes sampled from an RDP \(\mathbf{R}\) and a regular policy \(\pi^{\mathsf{b}}\in\Pi_{\mathbf{R}}\). With probability \(1-\delta\), the output of AdaCT-H-A, called with \(\mathcal{D}\), \(\delta/(2QAO)\) and \(\varepsilon\in(0,H]\) in input, is the transition function of an \(\varepsilon/2\)-approximate RDP \(\mathbf{R}^{\prime}\), provided that \(|\mathcal{D}|\geq N^{\prime}_{\delta}\), where_

\[N^{\prime}_{\delta}\coloneqq\frac{504HQAOC^{*}_{\mathbf{R}^{\prime}}\log(16 QAO/\delta)}{\varepsilon\,\mu_{0}}\sqrt{H\log(2ARO)}\in\widetilde{\mathcal{O}} \left(\frac{H^{3/2}QAOC^{*}_{\mathbf{R}^{\prime}}}{\varepsilon\,\mu_{0}} \right)\,.\]

Proof.: AdaCT-H-A returns the set of RDP states \(\mathcal{Q}^{\prime}\) and transition function \(\tau^{\prime}\) of an approximate RDP \(\mathbf{R}^{\prime}\), taking as input the accuracy \(\varepsilon\), an upper bound \(\overline{Q}\) on \(|\mathcal{Q}^{\prime}|\), and an upper bound \(\overline{C}\) on the concentrability \(C^{*}_{\mathbf{R}^{\prime}}\) of \(\mathbf{R}^{\prime}\). If, at any moment, the number of RDP states \(|\mathcal{Q}^{\prime}|\) exceeds \(\overline{Q}\), the algorithm returns Failure (line 15). AdaCT-H-A defines a sequence of side states \(q^{*}_{0},\ldots,q^{*}_{H+1}\) (lines 2 and 4), and defines \(\tau^{\prime}(q^{*}_{t},ao)=q^{*}_{t+1}\) for each \(t\in[H]\) and \(ao\in\mathcal{AO}\) (line 5). For each candidate state \(qao\in\mathcal{Q}^{\prime}_{\varepsilon,t+1}\) such that \(|\mathcal{X}(qao)|/N\geq\varepsilon/(4\overline{Q}AOH\overline{C})\), the definition of AdaCT-H-A is the same as that of AdaCT-H, including the call to TestDistinct (lines 11-14). For each candidate state \(qao\in\mathcal{Q}^{\prime}_{\varepsilon,t+1}\) such that \(|\mathcal{X}(qao)|/N<\varepsilon/(4\overline{Q}AOH\overline{C})\), instead of mapping \((q,ao)\) to the correct RDP state, AdaCT-H-A maps \((q,ao)\) to the side state \(q^{*}_{t+1}\) (lines 17-18). Once in \(q^{*}_{t+1}\), \(\mathbf{R}^{\prime}\) remains in a side state for the rest of the episode. The side states do not satisfy Proposition 1, since the histories that map to side states may assign different probabilities to suffixes (and TestDistinct is never called).

We define an alternative occupancy measure \(d^{\prime}_{t}(q,ao)\) associated with the approximate RDP \(\mathbf{R}^{\prime}\) and the behavior policy \(\pi^{\mathrm{b}}\). The new definition is given by \(d^{\prime}_{0}(q_{0},a_{0}o_{0})=\theta_{\mathrm{o}}(q_{0},a_{0},o_{0})\) and

\[d^{\prime}_{t}(q_{t},a_{t}o_{t})=\sum_{(q,ao)\in\tau^{\prime-1}(q_{t})}d^{ \prime}_{t-1}(q,ao)\cdot\pi^{\mathrm{b}}(q_{t},a_{t})\cdot\theta_{\mathrm{o}}( q_{t},a_{t},o_{t}),\ \ t>0.\]

The only difference between \(d^{\prime}_{t}\) and \(d^{\mathrm{b}}_{t}\) is that \(d^{\prime}_{t}\) is defined with respect to the transition function \(\tau^{\prime}\) of the approximate RDP \(\mathbf{R}^{\prime}\), instead of the transition function \(\tau\) associated with the original RDP \(\mathbf{R}\). Note that apart from the side states, \(\mathbf{R}^{\prime}\) will contain the same states as \(\mathbf{R}\), as long as the candidate states satisfy the condition on line 11, and \(\tau^{\prime}\) will be the same as \(\tau\) on those states. Since the states and behavior policy are the same, the \(L^{p}_{\infty}\)-distinguishability \(\mu_{0}\) of \(\mathbf{R}^{\prime}\) will be the same as that of \(\mathbf{R}\).

First consider each candidate state \(qao\in\mathcal{Q}^{\prime}_{\mathrm{c},t+1}\) such that \(|\mathcal{X}(qao)|/N\geq\varepsilon/(4\overline{Q}AOH\overline{C})\). In this case, AdaCT-H-A calls TestDistinct, so Lemmas 13 and 14 apply to these candidate states. The associated occupancy is \(d^{\prime}_{t}(q,ao)\) with empirical estimate \(\widehat{p}_{t}(qao)=|\mathcal{X}(qao)|/N\). Hence the empirical Bernstein inequality applies to \(d^{\prime}_{t}(q,ao)\) and \(\widehat{p}_{t}(qao)\). Just as in the proof of Theorem 6, we choose \(\mathcal{X}(qao)\) large enough to accurately estimate \(d^{\prime}_{t}(q,ao)\) within a factor \(d^{\prime}_{t}(q,ao)/2\) with probability \(1-\delta\). We thus obtain an alternative upper bound on \(d^{\prime}_{t}(q,ao)\) as follows:

\[d^{\prime}_{t}(q,ao)\geq\frac{|\mathcal{X}(qao)|}{N}-\frac{d^{\prime}_{t}(q, ao)}{2}\ \ \Leftrightarrow\ \ \frac{3d^{\prime}_{t}(q,ao)}{2}\geq\frac{|\mathcal{X}(qao)|}{N}\geq\frac{ \varepsilon}{4\overline{Q}AOH\overline{C}}.\]

From here, we can use the proof of Theorem 6 by substituting \(d^{\prime}_{t}\) for \(d^{\mathrm{b}}_{t}\), up until the definition of the bound \(N_{1}\) on \(|\mathcal{D}|\) in (21). Inserting the bound on \(d^{\prime}_{t}(q,ao)\) into the expression for \(N_{1}\) yields

\[N_{1} \leq\frac{2\ell}{d^{\prime}_{t}(q,ao)}\left(\frac{4}{\mu_{0}} \sqrt{H\log(2ARO)+H\log(2ARO)}+\frac{14}{3\mu_{0}}\sqrt{H\log(2ARO)}\right)\] \[\leq\frac{126\overline{Q}AOH\overline{C}\ell}{\varepsilon\mu_{0}} \sqrt{H\log(2ARO)}\equiv N_{2}.\] (23)

Next, consider each candidate state \(qao\in\mathcal{Q}^{\prime}_{\mathrm{c},t+1}\) such that \(|\mathcal{X}(qao)|/N<\varepsilon/(4\overline{Q}AOH\overline{C})\). In this case, we instead choose \(\mathcal{X}(qao)\) large enough to estimate \(d^{\prime}_{t}(q,ao)\) with accuracy \(\beta\) with probability \(1-\delta\). From the empirical Bernstein inequality, estimating \(d^{\prime}_{t}(q,ao)\) with accuracy \(\beta\) implies

\[\beta\geq\sqrt{\frac{2\widehat{p}_{t}(qao)\ell}{N}}+\frac{14\ell}{3N}\ \ \Leftrightarrow\ \ N\geq\frac{2\ell}{\beta}\left(\frac{14}{3}+\frac{\widehat{p}_{t}(qao)}{ \beta}\right)\equiv N_{3}.\]

Choosing \(\beta=\varepsilon/(4\overline{Q}AOH\overline{C})\) implies \(\widehat{p}_{t}(qao)<\beta\), and we can thus simplify \(N_{3}\) as

\[N_{3}=\frac{2\ell}{\beta}\left(\frac{14}{3}+\frac{\widehat{p}_{t}(qao)}{\beta }\right)<\frac{12\ell}{\beta}=\frac{48\overline{Q}AOH\overline{C}\ell}{ \varepsilon}\equiv N_{4}.\] (24)

In addition, this choice of \(\beta\) yields the following bound on \(d^{\prime}_{t}(q,ao)\):

\[d^{\prime}_{t}(q,ao)\leq\widehat{p}_{t}(qao)+\beta<\frac{\varepsilon}{4 \overline{Q}AOH\overline{C}}+\frac{\varepsilon}{4\overline{Q}AOH\overline{C}} =\frac{\varepsilon}{2\overline{Q}AOH\overline{C}}.\]

We prove that \(\mathbf{R}^{\prime}\) is an \(\varepsilon/2\)-approximation of the original RDP \(\mathbf{R}\). We briefly overload notation by letting \(d^{*}_{t}(q,ao)\) refer to the occupancy of an optimal policy \(\pi^{*}\)_with respect to the transition function \(\tau^{\prime}\) of \(\mathbf{R}^{\prime}\)_._ Consider a candidate state \(qao\in\mathcal{Q}^{\prime}_{\mathrm{c},t+1}\) such that \(|\mathcal{X}(qao)|/N<\varepsilon/(4\overline{Q}AOH\overline{C})\). The contribution to the expected optimal reward of \(\mathbf{R}\) of all histories that map to \(qao\) is bounded as

\[d^{*}_{t}(q,ao)(H-t)\leq C^{*}_{\mathbf{R}^{\prime}}d^{\prime}_{t}(q,ao)H<\frac {\varepsilon}{2\overline{Q}AO},\]

since \((H-t)\) is the maximum reward obtained during the remaining time steps. Since \(qao\) is mapped to a side state of \(\mathbf{R}^{\prime}\), an optimal policy for \(\mathbf{R}^{\prime}\) may not accurately estimate the expected optimal value for \(qao\), but the contribution of all such candidate states to the expected optimal value is at most

\[\sum_{t\in[H-1]}\sum_{q\in\mathcal{Q}_{t}}\sum_{ao\in\mathcal{A}\mathcal{O}}d^{ *}_{t}(q,ao)(H-t)\leq\sum_{t\in[H-1]}\sum_{q\in\mathcal{Q}_{t}}\sum_{ao\in \mathcal{A}\mathcal{O}}\frac{\varepsilon}{2\overline{Q}AO}\leq\frac{ \varepsilon}{2},\]since there can be at most \(\overline{QAO}\) such candidate states. Hence any optimal policy for \(\mathbf{R}^{\prime}\) is an \(\varepsilon/2\)-optimal policy for \(\mathbf{R}\), which implies that we can approximate an \(\varepsilon\)-optimal regular policy for the exact RDP \(\mathbf{R}\) by finding an \(\varepsilon/2\)-optimal policy for the approximate RDP \(\mathbf{R}^{\prime}\).

It is easy to verify that the bound \(N_{4}\) in (24) is less than the bound \(N_{2}\) in (23). Hence a worst-case bound is obtained by assuming that \(|\mathcal{X}(qao)|/N\geq\varepsilon/(4\overline{QAO}\overline{C})\) for each \(t\in[H]\) and each candidate state \(qao\in\mathcal{Q}^{\prime}_{\mathbf{c},t+1}\), which yields an upper bound \(N_{2}\). Note that AdaCT-H-A takes as input an upper bound \(\overline{Q}\) on the number of RDP states \(|\mathcal{Q}^{\prime}|\) of \(\mathbf{R}^{\prime}\), as well as an upper bound \(\overline{C}\) of the concentrability coefficient \(C^{*}_{\mathbf{R}^{\prime}}\). If the learning agent has no prior knowledge of \(\overline{Q}\) and \(\overline{C}\), it could start with small estimates of \(\overline{Q}\) and \(\overline{C}\), and in the case that AdaCT-H-A returns Failure or the resulting policy has larger concentrability than \(\overline{C}\) for \(\mathbf{R}^{\prime}\), iteratively double the estimates \(\overline{Q}\) and/or \(\overline{C}\) and call the algorithm again. This only increases the computational complexity of AdaCT-H-A by a factor \(O(\log QC^{*}_{\mathbf{R}^{\prime}})\), and the resulting upper bounds \(\overline{Q}\) and \(\overline{C}\) do not exceed \(2Q\) and \(2C^{*}_{\mathbf{R}^{\prime}}\). Since we already have an estimate \(\overline{Q}\), in each iteration we can call AdaCT-H-A with \(\delta=\delta_{1}/(2\overline{Q}AO)\) to ensure that the bound \(N_{2}\) holds for each candidate state simultaneously with probability \(1-\delta_{1}\). Substituting this value of \(\delta\) in the bound \(N_{2}\) in (23) and using \(\overline{Q}<2Q\) and \(\overline{C}<2C^{*}_{\mathbf{R}^{\prime}}\) yields the sample complexity bound stated in the theorem. 

**Lemma 15**.: _The concentrability \(C^{*}_{\mathbf{R}^{\prime}}\) of the approximate RDP \(\mathbf{R}^{\prime}\) satisfies_

\[C^{*}_{\mathbf{R}^{\prime}}\leq C^{*}_{\mathbf{R}}(1+3\overline{Q}AO).\]

Proof.: For each \(t>0\), let \(d^{\prime}_{t}(q^{\mathsf{e}}_{t})\) be the occupancy of the side state \(q^{\mathsf{e}}_{t}\) in the approximate RDP \(\mathbf{R}^{\prime}\). We prove by induction on \(t\) that \(d^{\prime}_{t}(q^{\mathsf{e}}_{t})\) satisfies

\[d^{\prime}_{t}(q^{\mathsf{e}}_{t})<\frac{\varepsilon\sum_{u=0}^{t-1}|\mathcal{ Q}_{u}|}{2\overline{Q}H\overline{C}}\leq\frac{\varepsilon}{2H\overline{C}}.\]

The base case is given by \(t=1\). In this case, a candidate state \((q_{0},ao)\) is mapped to \(q^{\mathsf{e}}_{t}\) if \(d^{\mathsf{b}}_{t}(q_{0},ao)=d^{\prime}_{t}(q_{0},ao)<\varepsilon/(2\overline {QAOH\overline{C}})\). Since there can be at most \(AO=|\mathcal{Q}_{0}|AO\) such candidate states, we have

\[d^{\prime}_{t}(q^{\mathsf{e}}_{t})<\frac{\varepsilon|\mathcal{Q}_{0}|AO}{2 \overline{Q}AOH\overline{C}}=\frac{\varepsilon|\mathcal{Q}_{0}|}{2\overline{ Q}H\overline{C}}.\]

For \(t>1\), a candidate state \((q_{t-1},ao)\) is mapped to \(q^{\mathsf{e}}_{t}\) if \(d^{\prime}_{t}(q_{t-1},ao)<\varepsilon/(2\overline{Q}AOH\overline{C})\). Again, there can be at most \(|Q_{t-1}|AO\) such candidate states. Since all occupancy of \(q^{\mathsf{e}}_{t-1}\) is also mapped to \(q^{\mathsf{e}}_{t}\), we have

\[d^{\prime}_{t}(q^{\mathsf{e}}_{t})<d^{\prime}_{t-1}(q^{\mathsf{e}}_{t-1})+ \frac{\varepsilon|\mathcal{Q}_{t-1}|AO}{2\overline{Q}AOH\overline{C}}<\frac{ \varepsilon\sum_{u=0}^{t-2}|\mathcal{Q}_{u}|}{2\overline{Q}H\overline{C}}+\frac {\varepsilon|\mathcal{Q}_{t-1}|}{2\overline{Q}H\overline{C}}=\frac{\varepsilon \sum_{u=0}^{t-1}|\mathcal{Q}_{u}|}{2\overline{Q}H\overline{C}},\]

where we have used the induction hypothesis.

Consider a candidate state \((q,ao)\) of \(\mathbf{R}\) at time \(t\). Due to approximation, some histories in \(\bar{\tau}^{-1}(q)\) are mapped to side states in \(\mathbf{R}^{\prime}\) instead of \(q\), and we can therefore write \(d^{\mathsf{b}}_{t}(q,ao)=d^{\prime}_{t}(q,ao)+\xi\leq d^{\prime}_{t}(q,ao)+d^{ \prime}_{t}(q^{\mathsf{e}}_{t})\), where \(\xi\) is the total occupancy of histories in \(\bar{\tau}^{-1}(q)\) mapped to side states. In turn, this implies

\[d^{*}_{t}(q,ao)\leq d^{\mathsf{b}}_{t}(q,ao)C^{*}_{\mathbf{R}}\leq(d^{\prime} _{t}(q,ao)+d^{\prime}_{t}(q^{\mathsf{e}}_{t}))C^{*}_{\mathbf{R}}<(d^{\prime}_{ t}(q,ao)+\frac{\varepsilon}{2H\overline{C}})C^{*}_{\mathbf{R}}.\]

The concentrability of a candidate state \((q,ao)\) in the approximate RDP \(\mathbf{R}^{\prime}\) that is not mapped to a side state (i.e. \(d^{\prime}_{t}(q,ao)\geq\varepsilon/(6\overline{Q}AOH\overline{C})\)) can now be bounded as

\[\frac{d^{*}_{t}(q,ao)}{d^{\prime}_{t}(q,ao)}<\frac{d^{\prime}_{t}(q,ao)+ \varepsilon/(2H\overline{C})}{d^{\prime}_{t}(q,ao)}C^{*}_{\mathbf{R}}=\left(1+ \frac{\varepsilon}{2H\overline{C}d^{\prime}_{t}(q,ao)}\right)C^{*}_{\mathbf{R}} \leq C^{*}_{\mathbf{R}}(1+3\overline{Q}AO).\]

This concludes the proof of the lemma.

### Distinguishability Parameters

As defined in the main text, for \(t\in[H]\), we consider a metric \(L\) over distributions on the remaining part of the episode \(\Delta(\mathcal{E}_{\ell})\), for \(\ell=H-t\). Then, the \(L\)-distinguishability of an RDP \(\mathbf{R}\) and a policy \(\pi\) is the maximum \(\mu_{0}\) such that, for any \(t\in[H]\) and any two distinct \(q,q^{\prime}\in\mathcal{Q}_{t}\), the probability distributions over suffix traces \(e_{t:H}\in\mathcal{E}_{\ell}\) from the two states satisfy

\[L(\mathbb{P}(e_{t:H}\mid q_{t}=q,\pi),\mathbb{P}(e_{t:H}\mid q_{t}=q^{\prime}, \pi))\geq\mu_{0}\] (25)

So, \(\mu_{0}\) is a feature of the RDP and the policy combined and it quantifies the distance between any two distinct states of the RDP with respect to the distributions they induce over the observable quantities. Distinguishability parameters have been first introduced in Ron et al. [56] and later generalized for other metrics. They can be also found in Balle [59] for PDFA learning and in Ronca and De Giacomo [26], Ronca et al. [27] for RDP learning.

According to the definition we adopt, there exists an \(L\)-distinguishability for any RDP and policy. However, as stated in Assumption 2, we require \(\mu_{0}\) to be strictly positive. This does not constitute a restriction for the RDP, since it can be always minimized while preserving all conditional probabilities. Though it implies that, in any state, the behavior policy takes with positive probability all actions that are needed to observe episode suffixes that have different probability under the two states. Clearly if this was not the case for two distinct \(q,q^{\prime}\in\mathcal{Q}_{t}\) at some \(t\in[H]\), \(\mathbb{P}(e_{t:H}\mid q_{t}=q,\pi)=\mathbb{P}(e_{t:H}\mid q_{t}=q^{\prime},\pi)\) and no information would be available for the algorithm to distinguish \(q\) and \(q^{\prime}\). Assumption 2 is implied by Assumption 1. However, it becomes necessary for Theorem 8, since this does not rely on Assumption 1.

The metric selected also influences the actual value of the distinguishability parameter. In this paper, we adopt \(L^{\mathsf{p}}_{\infty}\), as it can be seen from the TestDistinct function in the two algorithms. A more standard distance would be \(L_{\infty}\). According to Eq. (25), an \(L_{\infty}\)-distinguishability of \(\mu_{0}\) implies that for any \(t\in[H]\) and two distinct \(q,q^{\prime}\in\mathcal{Q}_{t}\),

\[\max_{e\in\mathcal{E}_{H-t}}|\mathbb{P}(e_{t:H}=e\mid q_{t}=q)-\mathbb{P}(e_{ t:H}=e\mid q_{t}=q^{\prime})|\geq\mu_{0}.\] (26)

This means that some sequence until the end of the episode has a different probability of being generated from the two states. Although similar, the \(L^{\mathsf{p}}_{\infty}\) distance, maximizes for the full trace as well as any of its prefixes as

\[\max_{u\in[H-t],e\in\mathcal{E}_{u}}|\mathbb{P}(e_{t:H}=e*\mid q_{t}=q)- \mathbb{P}(e_{t:H}=e*\mid q_{t}=q^{\prime})|\geq\mu_{0}\] (27)

As it has been discussed in Balle [59], Appendix A.5, the prefix \(L^{\mathsf{p}}_{\infty}\) metric always upper bounds the \(L_{\infty}\) metric, up to a multiplicative factor, while there are pairs of distributions in which \(L_{\infty}\) is exponentially smaller than \(L^{\mathsf{p}}_{\infty}\) with respect to the expected suffix length. This motivates our choice. Moreover, in the specific case of our fixed horizon setting, we have that the \(L^{\mathsf{p}}_{\infty}\)-distinguishability is never lower than \(L_{\infty}\)-distinguishability. Note that in the hard instance of Appendix E.2, the two coincide.

The lower bound is stated in terms of the \(L^{\mathsf{p}}_{1}\)-distinguishability of the RDP. While \(L^{\mathsf{p}}_{\infty}\) is achieved for one specific trace prefix maximizing the difference in probability, \(L^{\mathsf{p}}_{1}\) takes all traces into account as \(\sum_{u\in[H-t],e\in\mathcal{E}_{u}}|\mathbb{P}(e_{t:H}=e*\mid q_{t}=q)- \mathbb{P}(e_{t:H}=e*\mid q_{t}=q^{\prime})|\). Due to this relation, the \(L^{\mathsf{p}}_{\infty}\)-distinguishability always lower bounds the \(L^{\mathsf{p}}_{1}\)-distinguishability in the fixed horizon setting.

## Appendix D RegOrl with Subsampled Vi-Lcb

In this section we demonstrate the composition of our proposed algorithm with a specific Offline Reinforcement Learning algorithm for MDPs. Specifically, we adopt Subsampled VI-LCB, from Algorithm 3 of Li et al. [16] and report the combined sample complexity of this choice, through a simple application of Theorem 7.

First, we introduce the occupancy distribution and the single-policy concentrability coefficient for MDPs. Let \(\mathbf{M}=\langle\mathcal{Q},\mathcal{A},\mathcal{R},T,R,H\rangle\) be an MDP with states \(\mathcal{Q}\), horizon \(H\), transition function \(T:\mathcal{Q}\times\mathcal{A}\rightarrow\mathcal{Q}\) and reward function \(R:\mathcal{Q}\times\mathcal{A}\rightarrow\Delta(\mathcal{R})\). The state-action occupancy distribution of a policy \(\pi:\mathcal{Q}\rightarrow\Delta(\mathcal{A})\) in \(\mathbf{M}\) at step \(t\in[H]\) is \(d^{\pi}_{t}(q,a)=\mathbb{P}(q_{t}=q,a_{t}=a\mid\mathbf{M},\pi)\). For ourpurposes, it suffices to consider a fixed initial state \(q_{0}\). Finally, the MDP single-policy concentrability of a policy \(\pi^{\text{b}}\) is [15]:

\[C^{*}=\max_{t\in[H],q\in\mathcal{Q},a\in\mathcal{A}}\frac{d_{\mathsf{m},t}^{*}(q,a)}{d_{\mathsf{m},t}^{\text{b}}(q,a)}\] (28)

We can now express the sample complexity of Subsampled VI-LCB.

**Theorem 16** (Li et al. [16]).: _Let \(\mathcal{D}\) be a dataset of \(N_{\mathsf{m}}\) episodes, sampled from an MDP \(\mathbf{M}\) with a Markov policy \(\pi^{\text{b}}\). For any \(\varepsilon\in(0,H]\) and \(0<\delta<1/12\), with probability exceeding \(1-\delta\), the policy \(\widehat{\pi}\) returned by Subsampled VI-LCB obeys \(V_{0}^{*}(q_{0})-V_{0}^{\widehat{\pi}}(q_{0})\leq\varepsilon\), as long as:_

\[N_{\mathsf{m}}\geq\frac{c\,H^{3}QC^{*}\log\frac{N_{\mathsf{m}}H}{\delta}}{ \varepsilon^{2}}\] (29)

_for a positive constant \(c\)._

The analysis in Li et al. [16] of Subsampled VI-LCB assumes that the reward function is deterministic and known. Thus, restricting our attention to this setting, we consider any episodic RDP with history-dependent, deterministic rewards. The reward function can be regarded as known, since it may be easily extracted from the dataset resulting from the Markov transformation of Definition 2.

**Corollary 17**.: _Let \(\mathcal{D}\) be a dataset of \(N\) episodes, sampled with a regular policy \(\pi^{\text{b}}\in\Pi_{\mathbf{R}}\) from an RDP \(\mathbf{R}\) with deterministic rewards. If Subsampled VI-LCB is the OfflineRL algorithm in Algorithm 1, then, for any \(\varepsilon\in(0,H]\) and \(0<\delta<1/12\), with probability exceeding \(1-\delta\), the output of \(\texttt{RegRLL}(\mathcal{D},\varepsilon,\delta)\) is an \(\varepsilon\)-optimal policy of \(\mathbf{R}\), as long as_

\[N\geq 2\max\Bigl{\{}\frac{21\log(8QAO/\delta)}{d_{\min}^{\text{b}}\,\mu_{0}} \sqrt{H\log(2ARO)},\,\frac{c\,H^{3}QC_{\mathbf{R}}^{*}\log\frac{2NH}{\delta}} {\varepsilon^{2}}\Bigr{\}}\] (30)

Proof.: This corollary follows as a direct application of Theorem 16 to Theorem 6. It only remains to verify that the single-policy concentrability of the MDP underlying the dataset \(\mathcal{D}^{\prime}\) that Subsampled VI-LCB receives is \(C_{\mathbf{R}}^{*}\). The dataset \(\mathcal{D}^{\prime}\) is generated according to the Markov transformation \(\bar{\tau}\) from Definition 2. We only consider the cases in which AdACT-H succeeds. Let \(\pi\in\Pi_{\mathbf{R}}\) be any regular policy and \(q_{t},q_{t}^{\prime}\) the states reached at step \(t\) by \(\mathbf{R}\) and \(\mathbf{M}_{\mathbf{R}}\), respectively. Then for \(t>0\),

\[d_{t}^{\pi}(q,a) \coloneqq\mathbb{P}(q_{t}=q\mid\mathbf{R},\pi)\,\pi_{\mathsf{r}}( q,a)\] (31) \[=\mathbb{P}(\bar{\tau}(h_{t-1})=q\mid\mathbf{R},\pi)\,\pi_{\mathsf{ r}}(q,a)\] (32) \[=\mathbb{P}(q_{t}^{\prime}=q\mid\mathbf{M}_{\mathbf{R}},\pi_{ \mathsf{r}})\,\pi_{\mathsf{r}}(q,a)\] (33) \[=d_{\mathsf{m},t}^{\pi_{\mathsf{r}}}(q,a)\] (34)

This is valid for any regular policy, and for the optimal and behavior policies in particular. Then,

\[C_{\mathbf{R}}^{*} =\max_{t\in[H],q\in\mathcal{Q}_{t},a\in\mathcal{A}}\frac{d_{t}^{ *}(q,ao)}{d_{t}^{\text{b}}(q,ao)}\] (35) \[=\max_{t\in[H],q\in\mathcal{Q}_{t},a\in\mathcal{A}}\frac{d_{ \mathsf{m},t}^{\pi^{*}}(q,a)\,\theta_{\mathsf{o}}(q,a,o)}{d_{t}^{\text{b}}(q,a )\,\theta_{\mathsf{o}}(q,a,o)}\] (36) \[=\max_{t\in[H],q\in\mathcal{Q},a\in\mathcal{A}}\frac{d_{\mathsf{m},t}^{\pi^{*}}(q,a)}{d_{\mathsf{m},t}^{\text{b}}(q,a)}\] (37) \[=C^{*}\] (38)

Similarly to the previous corollary, it is also possible to combine Theorem 16 with Theorem 8. In this case, the sample complexity of Subsampled VI-LCB for learning an \(\varepsilon/2\)-accurate policy with probability \(1-\delta/2\) would be combined with \(N_{\delta/2}^{\prime}\) of Theorem 8.

Sample Complexity Lower Bound: Proof of Theorem 9

In this section, we prove the sample complexity lower bound in Theorem 9. The proof is based on a suitable composition of a two-armed bandit and an instance of the learning problem associated to noisy parity functions. We first describe this latter problem and its sample-complexity lower bound in Appendix E.1. Then, we compose a hard class of RDP instances in Appendix E.2, and prove the final statement in Appendix E.3.

### Learning parity with noise

Let \(\mathbb{B}=\{0,1\}\) and \(L\in\mathbb{N}\). For any string \(x\in\mathbb{B}^{L}\), the parity function \(f_{x}:\mathbb{B}^{L}\to\mathbb{B}\) is \(f_{x}(y)=\oplus_{i\in[L-1]}x_{i}y_{i}\), where \(\oplus\) is addition modulo 2. For noise parameter \(\xi\in(0,0.5)\), a noisy parity function \(f_{x,\xi}\) returns \(f_{x}(y)\) with probability \(0.5+\xi\) and \(1-f_{x}(y)\) otherwise. Consider the class of parity functions \(\mathbb{F}(L)=\{f_{x}\}_{x\in\mathbb{B}^{L}}\) and the class of noisy parity functions \(\mathbb{F}(L,\xi)=\{f_{x,\xi}\}_{x\in\mathbb{B}^{L}}\). Assume that \(x,y_{1}\), \(y_{2}\), \(\ldots\sim\mathrm{unif}(\mathbb{B}^{L})\) are uniformly sampled. The success probability of a streaming algorithm \(\mathfrak{A}\) for \(\mathbb{F}(L,\xi)\) is the probability that \(\mathfrak{A}\) recovers \(x\), given in input a sequence of observations \((y_{i},f_{x,\xi}(y_{i}))_{i}\).

**Lemma 18**.: _Any streaming algorithm for \(\mathbb{F}(L,\xi)\) with a success probability higher than \(O(2^{-L})\) requires at least \(\Omega(L/\xi)\) or \(2^{\Omega(L)}\) input samples \((y_{i},f_{x,\xi}(y_{i}))_{i}\)._

Proof.: Learning in \(\mathbb{F}(L,\xi)\) is the problem of recovering \(x\in 2^{\mathbb{B}}\) from noisy data \((y_{i},b_{i})\), where \(b_{i}=f_{x}(y_{i})\) with probability \(0.5+\xi\), and \(b_{i}=1-f_{x}(y_{i})\) otherwise. This is the problem of learning in \(\mathbb{F}(L)\) with corruption rate \(0.5-\xi\). Hence, we focus on the problem of learning noiseless parity first.

The Statistical Query dimension \(\textsc{SQdim}(\mathcal{C},d)\), characterizes the complexity of learning in the class \(\mathcal{C}\) with respect to the prior distribution \(d\in\Delta(\mathcal{C})\). As defined in [73], \(\textsc{SQdim}(\mathcal{C},d)\) is the maximum \(n\in\mathbb{N}\) such that there exist distinct \(f_{1},\ldots,f_{n}\in\mathcal{C}\), such that their pairwise correlations with respect to \(d\) are between \(-1/n\) and \(1/n\). For the class of parity functions, under the uniform distribution over \(x\), \(\textsc{SQdim}(\mathbb{F}(L),\mathrm{unif})=2^{L}\). This was already observed in [74], for a slightly different notion of SQ dimension. However, to verify this, we can consider a natural ordering over binary strings in \(\mathcal{X}\), and represent the problem of learning \(\mathbb{F}(L)\) as a matrix \(M=(m_{ij})\in\{1,-1\}^{2^{L}\times 2^{L}}\), defined as \(m_{ij}=(-1)^{f_{x_{j}}(y_{i})}=(-1)^{y_{i}\cdot x_{j}}\), where scalar product is modulo 2. We have that \(M\) is a Hadamard matrix. Then, since every row is orthogonal to the others, and the same is true for columns, every couple of parity functions are uncorrelated under the uniform distribution over \(x\).

Regarding the noisy parity problem, since \(\textsc{SQdim}(\mathbb{F}(L),\mathrm{unif})=2^{L}\), we can apply Corollary 8 of [75] with \(m=2^{L}\), to have that the matrix \(M\) corresponding to the parity problem is a \((k,l)\)-\(L_{2}\)-extractor with error \(2^{-r}\), for \(k,l,r\in\Omega(L)\). Since \(M\) is a suitable extractor, we can apply Theorem 1 of [76], which considers the problem of learning \(M\) with the additional noise parameter \(\xi\). We obtain that, in the streaming setting, any branching program \(B\) for \(\mathbb{F}(L,\xi)\) whose depth is at most \(2^{f_{1}(k,l,r)}\) and width is at most \(2^{ckl/\xi}\) has a success probability of at most \(O(2^{-f_{1}(k,l,r)})\), where \(c\) is a suitable constant and \(f_{1}\) is equation (1) of [76].

Then, if the success probability is not in \(O(2^{-f_{1}(k,l,r)})\), meaning it is higher, we have that the depth of \(\mathfrak{A}\) exceeds \(2^{f_{1}(k,l,r)}\) or the width of \(\mathfrak{A}\) exceeds \(2^{ckl/\xi}\). Since \(k,l,r\in\Omega(L)\), then, if the success probability is not in \(O(2^{-L})\), the depth of \(\mathfrak{A}\) is \(2^{\Omega(L)}\) or the width of \(\mathfrak{A}\) is \(2^{\Omega(L^{2})/\xi}\). Width and depth refer to the computational model that represents \(\mathfrak{A}\) as a branching program. A branching program is a directed acyclic graph in which internal nodes have one outgoing edge for each possible input sample, that is \(|\mathbb{B}^{L}\times\mathbb{B}|=2^{L+1}\) in our problem, and leaves correspond to algorithm decisions. From the required width and depth we know that \(\mathfrak{A}\) has a leaf in layer \(2^{\Omega(L)}\) or in a layer that contains \(2^{\Omega(L^{2})/\xi}\) nodes. The former case implies a worst case sample complexity requirement that is exponential in \(L\). For the latter, we observe that in order to reach that width, at least \(\log_{2^{L+1}}2^{\Omega(L^{2}/\xi)}\) transitions and input samples, are required. This is \(\Omega(L/\xi)\).

### Class of hard RDP instances

For our main lower bound, we define a class of hard RDP instances. Figure 3 shows one possible instance in this class. We will soon define it formally, but we can observe that its structure is organized in two main paths. The two branches in the top part encode a parity computation according to some hidden code \(x\in\mathbb{B}^{L}\), so that behaving optimally in that region requires to solve a parity problem (the one of Lemma 18). The bottom part reaches a two-armed bandit whose optimal action is \(c\). The right-most states are winning or losing states that provide a positive and null reward accordingly.

Formally, we define a class of hard RDP instances as \(\mathbb{R}(L,H,\xi,\eta)=\{\mathbf{R}_{x,c}\}_{x\in\mathbb{B}^{L},c\in\{0,1\}}\) where \(\mathbf{R}_{x,c}=\langle\mathcal{Q},\mathcal{AO},\Omega,\tau,\theta,q_{s},H\rangle\), for \(\mathcal{Q}=\{q_{s},q_{o_{\perp}}\}\cup\{q_{0i},q_{1i},q_{bi}\}_{i=1,...,L} \cup\{q_{+,i},q_{-,i}\}_{i=L+1,...,H}\), \(\mathcal{A}=\{a^{\prime}_{0},a^{\prime}_{1}\}\), \(\mathcal{O}=\{0,1,+,-\}\). Assume \(L\geq 1\) and \(H>L\). Rewards are zero everywhere, except in the winning states

\[\theta_{r}(q,a,r)=\mathbbm{1}_{1}\text{ if }q=q_{+i}\text{ with }i>L,\, \mathbbm{1}_{0}\text{, otherwise}\] (39)

where we recall that \(\mathbbm{1}_{x}\) represents the deterministic distribution on \(x\). For observation probabilities, we denote the distributions \(u(o)\coloneqq\operatorname{unif}\{0,1\}\) and

\[v_{\alpha}(o)\coloneqq\begin{cases}\frac{1+\alpha}{2}&\text{if }o=+\\ \frac{1-\alpha}{2}&\text{if }o=-\\ 0&\text{otherwise}\end{cases}\qquad s(o)\coloneqq\begin{cases}1/4&\text{if }o=0\\ 1/4&\text{if }o=1\\ 1/2&\text{if }o=+\end{cases}\] (40)

Now define observations as

\[\theta_{\mathsf{o}}(q,a,o)=\begin{cases}s(o)&\text{if }q=q_{s}\\ u(o)&\text{if }q\in\{q_{0i},q_{1i}\}_{i=1,...,L-1}\\ v_{\xi}(o)&\text{if }q=q_{0L}\wedge a=a^{\prime}_{0}\text{ or }q=q_{1L}\wedge a=a^{\prime}_{1}\\ v_{-\xi}(o)&\text{if }q=q_{0L}\wedge a=a^{\prime}_{1}\text{ or }q=q_{0L}\wedge a=a^{\prime}_{1}\\ \mathbbm{1}_{+}(o)&\text{if }q=q_{bi}\text{ with }i<L\\ v_{0}(o)&\text{if }q=q_{bL}\wedge a=a^{\prime}_{0}\\ v_{\eta}(o)&\text{if }q=q_{bL}\wedge a=a^{\prime}_{1}\wedge c=1\\ v_{-\eta}(o)&\text{if }q=q_{bL}\wedge a=a^{\prime}_{1}\wedge c=0\\ \mathbbm{1}_{o_{\perp}}(o)&\text{if }q=q_{o_{\perp}}\end{cases}\] (41)

Finally, the transition function is defined such that \(\bar{\tau}(q_{s},h_{L-1})=q_{iL}\) with \(i=f_{x}(o_{0:L-1})\), and

\[\tau(q_{kL},a+) =q_{+,L+1} \tau(q_{kL},a-) =q_{-,L+1} \text{for }k=1,2\] (42) \[\tau(q_{bL},a+) =q_{+,L+1} \tau(q_{bL},a-) =q_{-,L+1}\] (43)

Figure 3: One episodic RDP instance \(\mathbf{R}_{101,1}\in\mathbb{R}(3,5,\xi,\eta)\), associated to the parity function \(f_{101}\) and optimal arm \(a^{\prime}_{1}\). The transition function is represented by the arcs, labelled by observations (transitions do not depend on actions). The star denotes any symbol. If the label of a state \(q\) is \(a.d\), then the observation function is \(\theta_{\mathsf{o}}(q,a)=d\), where \(d\in\Delta(\mathcal{O})\) (some irrelevant outputs are omitted).

\[\tau(q_{+i},ao) =q_{+,i+1} \tau(q_{-i},ao) =q_{-,i+1}\] (44) \[\tau(q_{s},a+) =q_{b1} \tau(q_{bi},ao) =q_{b,i+1} \text{for }i<L\] (45) \[\tau(q_{+H},ao) =q_{o_{\perp}} \tau(q_{-H},ao) =q_{o_{\perp}} \tau(q_{o_{\perp}},a,o) =q_{o_{\perp}}\] (46)

In addition, the trasitions \(\tau(q_{0i},ao)\) and \(\tau(q_{1i},ao)\), for \(i<L\), are defined according to \(o\in\{0,1\}\) and the parity code \(x\). Namely, \(\tau(q_{0i},ao)\) equals \(q_{1,i+1}\) iff \(o\oplus x(i)=1\), and \(q_{0,i+1}\), otherwise. \(\tau(q_{1i},ao)\) is defined analogously.

### Proof of Theorem 9

**Theorem 9**.: _For any \((C_{\mathbf{R}}^{*},H,\varepsilon,\mu_{0})\) satisfying \(C_{\mathbf{R}}^{*}\geq 2\), \(H\geq 2\) and \(\varepsilon\leq H\mu_{0}/64\), there exists an RDP with horizon \(H\), \(L_{1}^{*}\)-distinguishability \(\mu_{0}\) and a regular behavior policy \(\pi^{\mathtt{b}}\) with RDP single-policy concentrability \(C_{\mathbf{R}}^{*}\), such that if \(\mathcal{D}\) has been generated using \(\pi^{\mathtt{b}}\) and \(\mathbf{R}\), and_

\[|\mathcal{D}|\notin\Omega\left(\frac{H}{\mu_{0}}+\frac{C_{\mathbf{R}}^{*}H^{2} }{\varepsilon^{2}}\right)\] (3)

_then, for any algorithm \(\mathfrak{A}:\mathcal{D}\mapsto\widehat{\pi}\) returning non-Markov deterministic policies, the probability that \(\widehat{\pi}\) is not \(\varepsilon\)-optimal is at least \(1/4\)._

Proof.: Denote with \(\pi^{\mathtt{b}}\) a regular policy in \(\mathbf{R}\) and \(\mathcal{D}\in\mathbb{D}\) a dataset of episodes of length \(H\), collected from \(\mathbf{R}\) and the behavior policy \(\pi^{\mathtt{b}}\). For an RDP \(\mathbf{R}\), let \(\Pi_{\mathtt{d}}=\mathcal{A}^{\mathcal{H}}\) be the set of deterministic non-Markov policies and \(\mathfrak{A}=\mathbb{D}\rightarrow\Pi_{\mathtt{d}}\) an offline RL algorithm. For \(\delta<0.5\), we say that an algorithm \(\mathfrak{A}\) is \((\varepsilon,\delta)\)-PAC for the class of RDPs \(\mathbb{R}\) under \(\varphi\), if, for every \(\mathbf{R}\in\mathbb{R}\) and \(\mathcal{D}\in\mathbb{D}\), if the condition \(\varphi(\mathcal{D},\pi^{\mathtt{b}})\) is verified, then the output policy \(\mathfrak{A}(\mathcal{D})\) is \(\varepsilon\)-optimal in \(\mathbf{R}\), with probability \(1-\delta\). One notable case is that of \(\varphi\) requiring a minimum dataset size.

Since the output of a generic algorithm might be any generic non-Markov deterministic policy, we cannot restrict our attention to regular policies. We expand the value of a policy \(\pi:\mathcal{H}\rightarrow\mathcal{A}\) in a RDP \(\mathbf{R}_{x,c}\in\mathbb{R}(L,H,\xi,\eta)\) as follows:

\[\mathbb{E}[V_{0}^{\pi}(h_{0})] =\mathbb{E}\left[\sum_{i=1}^{H}r_{i}\mid\pi\right]\] (47) \[=(H-L)\,\mathbb{P}(q_{L+1}=q_{+,L+1}\mid\pi)\] (48) \[=(H-L)\sum_{q\in\mathcal{Q}_{L}}\mathbb{P}(q_{L}=q\mid\pi)\, \mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q,\pi)\] (49) \[=(H-L)\,(\mathbb{P}(q_{L}=q_{0L}\mid\pi)\,\mathbb{P}(q_{L+1}=q_{ +,L+1}\mid q_{L}=q_{0L},\pi)\] \[\qquad\quad+\mathbb{P}(q_{L}=q_{1L}\mid\pi)\,\mathbb{P}(q_{L+1}=q _{+,L+1}\mid q_{L}=q_{1L},\pi))\] \[\qquad+(H-L)\,(\mathbb{P}(q_{L}=q_{bL}\mid\pi)\,\mathbb{P}(q_{L+1} =q_{+,L+1}\mid q_{L}=q_{bL},\pi))\] (50) \[=\frac{H-L}{2}\,(\mathbb{P}(q_{L}=q_{0L}\mid o_{0}\in\{0,1\},\pi) \,\mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q_{0L},\pi)\] \[\qquad\quad+\mathbb{P}(q_{L}=q_{1L}\mid o_{0}\in\{0,1\},\pi)\, \mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q_{1L},\pi))\] \[\qquad+\frac{H-L}{2}\,\mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q_{bL},\pi)\] (51) \[=\frac{H-L}{4}\,(\mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q_{0L},\pi )+\mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q_{1L},\pi))\] \[\qquad+\frac{H-L}{2}\,\mathbb{P}(q_{L+1}=q_{+,L+1}\mid q_{L}=q_{bL },\pi)\] (52) \[=\frac{H-L}{4}\,(\mathbb{P}(a_{L}=a_{0}^{\prime}\mid q_{L}=q_{0L}, \pi)\,\mathbb{P}(o_{L}=+\mid q_{L}=q_{0L},a_{L}=a_{0}^{\prime})\] \[\qquad\quad+(1-\mathbb{P}(a_{L}=a_{0}^{\prime}\mid q_{L}=q_{0L}, \pi))\,\mathbb{P}(o_{L}=+\mid q_{L}=q_{0L},a_{L}=a_{1}^{\prime})\] \[\qquad\quad+(1-\mathbb{P}(a_{L}=a_{1}^{\prime}\mid q_{L}=q_{1L}, \pi))\,\mathbb{P}(o_{L}=+\mid q_{L}=q_{1L},a_{L}=a_{0}^{\prime})\] \[\qquad\quad+\mathbb{P}(a_{L}=a_{1}^{\prime}\mid q_{L}=q_{1L},\pi) \,\mathbb{P}(o_{L}=+\mid q_{L}=q_{1L},a_{L}=a_{1}^{\prime}))\] \[\qquad+\frac{H-L}{2}\,(\mathbb{P}(a_{L}=a_{0}^{\prime}\mid q_{L}=q _{bL},\pi)\,\mathbb{P}(o_{L}=+\mid q_{L}=q_{bL},a_{L}=a_{0}^{\prime})\]\[+\mathbb{P}(a_{L}=a_{1}^{\prime}\mid q_{L}=q_{bL},\pi)\,\mathbb{P}(o_{L}=+\mid q_{ L}=q_{bL},a_{L}=a_{1}^{\prime}))\] (53)

where in Eq. (52) we have used the uniform probability over \(x\). Now, for any history-dependent deterministic policy \(\pi\) in episodic RDPs, it is possible to identify an associated regular stochastic policy \(\pi_{r}:\mathcal{Q}^{\prime}\;\rightarrow\;\Delta(\mathcal{A})\), where \(\mathcal{Q}^{\prime}\coloneqq\mathcal{Q}\setminus\{q_{o_{\perp}}\}\) and:

\[\pi_{r}(q,a) \coloneqq\mathbb{P}(\pi(h)=a\mid\bar{\tau}(h)=q)\] (54) \[=\sum_{h^{\prime}\in\bar{\tau}^{-1}(q)}\mathbb{I}(\pi(h^{\prime} )=a)\,\frac{\mathbb{P}(h=h^{\prime}\mid\pi)}{\mathbb{P}(q\mid\pi)}\] (55)

In other words, \(\pi_{r}\) encodes the probability that \(\pi\) takes action \(a\), given that some history has led to state \(q\). With this convention, we resume from Eq. (53)

\[\mathbb{E}[V_{0}^{\pi}(h_{0})] =\frac{H-L}{4}\left(\pi_{r}(q_{0L},a_{0}^{\prime})\,v_{\xi}(+)+ \left(1-\pi_{r}(q_{0L},a_{0}^{\prime})\right)v_{\xi}(-)\right.\] \[\qquad+\left(1-\pi_{r}(q_{1L},a_{1}^{\prime})\right)v_{\xi}(-)+ \pi_{r}(q_{1L},a_{1}^{\prime})\,v_{\xi}(+))\] \[\qquad+\frac{H-L}{2}\left(\pi_{r}(q_{bL},a_{0}^{\prime})\,u(+)+ \pi_{r}(q_{bL},a_{1}^{\prime})\left(\mathbb{I}(c=a_{1}^{\prime})\,v_{\eta}(+) +\mathbb{I}(c=a_{0}^{\prime})\,v_{\eta}(-)\right)\] (56) \[=\frac{H-L}{8}\left(\pi_{r}(q_{0L},a_{0}^{\prime})\left(1+\xi \right)+\left(1-\pi_{r}(q_{0L},a_{0}^{\prime})\right)\left(1-\xi\right)\right.\] \[\qquad+\left(1-\pi_{r}(q_{1L},a_{1}^{\prime})\right)\left(1-\xi \right)+\pi_{r}(q_{1L},a_{1}^{\prime})\left(1+\xi\right)\] \[\qquad+\frac{H-L}{4}\left(\pi_{r}(q_{bL},a_{0}^{\prime})+\pi_{r}( q_{bL},a_{1}^{\prime})\left(\mathbb{I}(c=a_{1}^{\prime})\left(1+\eta \right)+\mathbb{I}(c=a_{0}^{\prime})\left(1-\eta\right)\right)\right)\] (57) \[=\frac{H-L}{4}\left(1-\xi+\xi\,\pi_{r}(q_{0L},a_{0}^{\prime})+ \xi\,\pi_{r}(q_{1L},a_{1}^{\prime})\right.\] \[\qquad+\pi_{r}(q_{bL},a_{0}^{\prime})+\pi_{r}(q_{bL},a_{1}^{ \prime})\left(1+\eta\,\mathbb{I}(c=a_{1}^{\prime})-\eta\,\mathbb{I}(c=a_{0}^{ \prime})\right)\] (58)

For the optimal policy in particular, this becomes:

\[V^{*}=\frac{H-L}{4}\left(1+\xi+\mathbb{I}(c=a_{0}^{\prime})+\left(1+\eta \right)\mathbb{I}(c=a_{1}^{\prime})\right)\] (59)

From the \(\varepsilon\)-optimality of \(\pi=\mathfrak{A}(\mathcal{D})\), then,

\[\varepsilon \geq\mathbb{E}[V_{0}^{*}(h_{0})-V_{0}^{\pi}(h_{0})]\] (60) \[=\frac{H-L}{4}(2\xi-\xi\,\pi_{r}(q_{0L},a_{0}^{\prime})-\xi\,\pi_ {r}(q_{1L},a_{1}^{\prime})\] \[\qquad+\eta\,\mathbb{I}(c=a_{1}^{\prime})\left(1-\pi_{r}(q_{bL},a_ {1}^{\prime})\right)+\eta\,\mathbb{I}(c=a_{0}^{\prime})\,\pi_{r}(q_{bL},a_{1}^{ \prime}))\] (61) \[=\frac{H-L}{4}(\xi\left(2-\pi_{r}(q_{0L},a_{0}^{\prime})-\pi_{r}( q_{1L},a_{1}^{\prime})\right)+\eta\left(1-\pi_{r}(q_{bL},c)\right))\] (62) \[\geq\frac{H-L}{4}\max\{\xi\left(1-\pi_{r}(q_{0L},a_{0}^{\prime}) \right),\xi\left(1-\pi_{r}(q_{1L},a_{1}^{\prime})\right),\eta\left(1-\pi_{r}(q _{bL},c)\right)\}\] (63)

Now, assume that

\[\min\{\xi,\eta\}\geq\frac{16\,\varepsilon}{H-L}\] (64)

Then, all of the following is true: \(\pi_{r}(q_{0L},a_{0}^{\prime})\geq 3/4\), \(\pi_{r}(q_{1L},a_{1}^{\prime})\geq 3/4\), \(\pi_{r}(q_{bL},c)\geq 3/4\). This means that, for small \(\varepsilon\), any \(\varepsilon\)-optimal policy must frequently select the optimal action for both the parity problem and the bandit. Let us represent the first two events with \(B_{\mathfrak{p}}\) and the third with \(B_{\mathfrak{b}}\). Since \(\mathfrak{A}\) is \((\varepsilon,\delta)\)-PAC for \(\mathbb{R}(L,H,\xi,\eta)\) under \(\varphi\), the probability of \(B_{\mathfrak{p}}\wedge B_{\mathfrak{b}}\) is at least \(1-\delta\), for any \(\mathcal{D}\) and \(\pi^{\mathfrak{b}}\) satisfying \(\varphi(\mathcal{D},\pi^{\mathfrak{p}})\).

We proceed to compute the necessary data to satisfy both events with high probability. The dataset \(\mathcal{D}\) can be partitioned in two subsets \(\mathcal{D}_{\mathfrak{p}}\) and \(\mathcal{D}_{\mathfrak{b}}\), containing any episode from \(\mathcal{D}\) whose initial observation is \(\{0,1\}\) and +, respectively. The two datasets share no information and \(\mathcal{D}_{\mathfrak{p}}\) and \(\mathcal{D}_{\mathfrak{b}}\) are mutually independent. To see this, we observe that the sequence \(a_{L+1}r_{L+1}o_{L+1}\ldots o_{H}\) is independent of \(a_{0}r_{0}o_{0}\ldots a_{L}\) given \(o_{L}\), since \(+\) or \(-\) determines at step \(L\) determines the rest of the episode. Also, for any two episodes \(e_{H},e^{\prime}_{H}\), the sequence \(a_{1}r_{1}o_{1}\ldots o_{L}\) is independent of \(a^{\prime}_{1}r^{\prime}_{1}o^{\prime}_{1}\ldots o^{\prime}_{L}\) given \(o_{0}\). Since, \(o_{0}\sim s\), that is the starting distribution, the two datasets are independent. Let \(\mathcal{Q}_{\textsf{p}}=\{q_{s},q_{o_{\perp}}\}\cup\{q_{0i},q_{1i}\}_{i=1, \ldots,L}\cup\{q_{+,i},q_{-,i}\}_{i=L+1,\ldots,H}\) and \(\mathcal{Q}_{\textsf{b}}=\{q_{s},q_{o_{\perp}}\}\cup\{q_{bi}\}_{i=1,\ldots,L}\cup \{q_{+,i},q_{-,i}\}_{i=L+1,\ldots,H}\) be the reachable states in the two datasets. Then, we consider two separate classes \(\mathbb{R}(L,H,\xi)\) and \(\mathbb{R}(L,H,\eta)\) as the sets of RDPs in \(\mathbb{R}(L,H,\xi,\eta)\), restricted to \(\mathcal{Q}_{\textsf{p}}\) and \(\mathcal{Q}_{\textsf{b}}\), respectively. To do so we construct \(\mathbf{R}_{r}\in\mathbb{R}(L,H,\xi)\) and \(\mathbf{R}_{c}\in\mathbb{R}(L,H,\eta)\) such that the initial observation follows \(\min\{\min\{0,1\}\}\) in \(\mathbf{R}_{r}\) and \(\mathbf{I}_{+}\) in \(\mathbf{R}_{c}\). Now, from the independence of the two datasets and the fact that \(\mathfrak{A}\) is \((\varepsilon,\delta)\)-PAC in \(\mathcal{D}\), there must exists an algorithm \(\mathfrak{A}_{\textsf{p}}:\mathcal{D}_{\textsf{p}}\mapsto\pi_{\textsf{p}}\) that is \((2\varepsilon,\delta)\)-PAC in \(\mathbb{R}(L,H,\xi)\) under some \(\varphi_{\textsf{p}}\), and \(\mathfrak{A}_{\textsf{b}}:\mathcal{D}_{\textsf{b}}\mapsto\pi_{\textsf{b}}\) that is \((2\varepsilon,\delta)\)-PAC in \(\mathbb{R}(L,H,\eta)\) under some \(\varphi_{\textsf{b}}\). If this was not the case, \(B_{\textsf{p}}\wedge B_{\textsf{b}}\) could not be verified in one of the two terms.

We analyze \(\mathfrak{A}_{\textsf{p}}\) first and we show that its requirement \(\varphi_{\textsf{p}}\) is \(|\mathcal{D}_{\textsf{p}}|\in\Omega(L/\xi)\cup 2^{\Omega(L)}\). For a contradiction, assume this is not the case and that \(|\mathcal{D}_{\textsf{p}}|=g(L,\xi)\not\in\Omega(L/\xi)\cup 2^{\Omega(L)}\) is allowed. Then, we can use \(\mathfrak{A}_{\textsf{p}}\) to solve the noisy parity problem under the streaming setting with \(g(L,\xi)\) samples (this setting has been introduced in Appendix E.1). We proceed as follows. Consider any noisy parity function \(f_{x,\xi}\) with unknown \(x\). Sample a sequence of strings \(\{y_{i}\}_{i}\in 2^{L}\) from the uniform distribution and collect \(g(L,\xi)\) pairs \((y_{i},p_{i})\), sampling \(p_{i}\sim f_{x,\xi}(y_{i})\). Then, for \(H>L\), compose a dataset of episodes \(\{e_{i}\}_{i}\). All actions of \(e_{i}\) are selected uniformly in \(\{a^{\prime}_{0},a^{\prime}_{1}\}\). The observations \(o_{0:L-1}\) are \(y_{i}\) and \(o_{L}\) equals \(p_{i}\) if \(a_{L}=a^{\prime}_{0}\), \(1-p_{i}\), otherwise (0 and 1 take roles of \(+\) and \(-\) symbols here). Rewards \(r_{L+1:H}\) are equal to one if \(o_{L}=1\), null otherwise. We obtain that dataset so constructed is equally likely under this procedure than under the uniform policy and the RDP \(\mathbf{R}_{x}\in\mathbb{R}(L,H,\xi)\). Since \(\mathfrak{A}_{\textsf{p}}\) is \((2\varepsilon,\delta)\)-PAC for \(\mathbb{R}(L,H,\xi)\), with probability \(1-\delta\), the output policy \(\pi_{\textsf{p}}\) satisfies:

\[\min\{\pi_{\textsf{pr}}(q_{0L},a^{\prime}_{0}),\pi_{\textsf{pr}}(q_{1L},a^{ \prime}_{1})\}\geq 3/4\] (65)

where \(\pi_{\textsf{pr}}\) is the stochastic regular policy for \(\pi_{\textsf{p}}\). This can be seen by our assumption in Eq. (64) and doubling both \(\varepsilon\) and the sub-optimality gap of Eq. (63), due to the updated probability for the initial observation. Then, for any sequence \(y\in 2^{L}\) and associated history \(h_{L-1}\) with \(o_{0:L-1}=y\),

\[f_{x}(y)=\operatorname*{arg\,max}_{i=0,1}\pi_{\textsf{p}}(h_{L-1},a^{\prime}_{ i})\] (66)

which is the noiseless parity function based on \(x\). This means that it is possible to reconstruct \(x\) solely by interacting with \(\pi_{\textsf{p}}\), without collecting further samples. The solution we have described is a streaming algorithm with sample complexity \(g(L,\xi)\). Since this contradicts Lemma 18, we have proven \(|\mathcal{D}_{\textsf{p}}|\in\Omega(L/\xi)\cup 2^{\Omega(L)}\).

We now consider the bandit problem, which is solved by \(\mathfrak{A}_{\textsf{b}}\). Similarly to the previous case, from the \(\varepsilon\)-optimality of \(\mathfrak{A}_{\textsf{b}}(\mathcal{D}_{\textsf{b}})\), we obtain the necessary condition: \(\pi_{\textsf{br}}(q_{bL},c)\geq 3/4\) from Eq. (63). This condition is expressed for the stochastic policy \(\pi_{\textsf{br}}\), However, we notice that for \(q_{bL}\) in particular, the only possible history is \(h_{L-1}=a_{0}+a_{1}\ldots+\), where all actions must also be deterministic. Then,

\[\pi_{\textsf{br}}(q_{bL},c)=\mathbb{P}(\pi_{\textsf{b}}(h)=c\mid\bar{\tau}(h)=q _{bL})=\mathbb{I}(\pi_{\textsf{b}}(h_{L-1})=c)\] (67)

implying that \(\pi_{\textsf{br}}\) can only be deterministic for \(q_{bL}\). This means that \(\mathfrak{A}_{\textsf{b}}\) must solve best-arm identification in the two arm bandit at \(q_{bL}\). We can compose a simplified dataset that is relevant for the bandit as:

\[\mathcal{D}^{\prime}_{\textsf{b}}=\{a_{L}o_{L}:e_{H}\in\mathcal{D}_{\textsf{b}}\}\] (68)

Since \(\mathcal{D}_{\textsf{b}}\) can be deterministically reconstructed from \(\mathcal{D}^{\prime}_{\textsf{b}}\), we have the following conditional independence: \(\pi_{\textsf{b}}\perp c\mid\mathcal{D}^{\prime}_{\textsf{b}}\), where \(c\in\{a^{\prime}_{0},a^{\prime}_{1}\}\) is the optimal arm, and \(\pi_{\textsf{b}}=\mathfrak{A}_{\textsf{b}}(\mathcal{D}_{\textsf{b}})\) is the output of the algorithm. Denoting with \(\hat{c}=\pi_{\textsf{b}}(h_{L-1})\) the selected arm, the error probability is \(P_{e}\coloneqq\mathbb{P}(\hat{c}\neq c)\). Applying Fano's inequality from Theorem 12 to the variables \(c\to\mathcal{D}^{\prime}_{\textsf{b}}\to\hat{c}\) gives:

\[H_{2}(P_{e}) \geq H(c\mid\mathcal{D}^{\prime}_{\textsf{b}})\] (69) \[=H(c)-I(c;\mathcal{D}^{\prime}_{\textsf{b}})=\log 2-I(c;\mathcal{D}^{ \prime}_{\textsf{b}})\] (70)

where we have used the fact that \(\hat{c}\) is a Bernoulli variable and the uniform prior over \(c\). Now, assuming \(C\geq 2\), we construct the following behavior policy: \(\pi^{\textsf{b}}(q_{bL},a_{0})=1-1/C\) and \(\pi^{\textsf{b}}(q_{bL},a_{1})=1/C\). In the following, we write \(N_{\textsf{b}}\coloneqq|\mathcal{D}_{\textsf{b}}|\) and omit the implicit dependency on \(\pi^{\textsf{b}}\).

\[I(c \!;\,\mathcal{D}^{\prime}_{\textsf{b}})=H(\mathcal{D}^{\prime}_{ \textsf{b}})-H(\mathcal{D}^{\prime}_{\textsf{b}}\mid c)\] (71) \[=N_{\textsf{b}}(H(a_{L}o_{L})-H(a_{L}o_{L}\mid c))\] (72)\[=N_{\mathsf{b}}D_{\mathrm{KL}}(\mathbb{P}(a_{L}o_{L},c)\parallel \mathbb{P}(a_{L}o_{L})\,\mathbb{P}(c))\] (73) \[=\frac{N_{\mathsf{b}}}{2}\sum_{a,c^{\prime}\in\mathcal{A},o\in \mathcal{O}}\mathbb{P}(a,o\mid c^{\prime})\,\log\frac{\mathbb{P}(a,o\mid c^{ \prime})}{\mathbb{P}(a,o)}\] (74) \[=\frac{N_{\mathsf{b}}}{2}\sum_{a,c^{\prime}\in\mathcal{A},o\in \mathcal{O}}\mathbb{P}(a,o\mid c^{\prime})\,\log\frac{\mathbb{P}(a\mid c^{ \prime})\,\mathbb{P}(o\mid c^{\prime},a)}{\sum_{c^{\prime\prime}}\mathbb{P}(a \mid c^{\prime\prime})\,\mathbb{P}(o\mid c^{\prime\prime},a)/2}\] (75) \[=\frac{N_{\mathsf{b}}}{2}\sum_{a,c^{\prime}\in\mathcal{A},o\in \mathcal{O}}\mathbb{P}(a,o\mid c^{\prime})\,\log\frac{2\mathbb{P}(o\mid c^{ \prime},a)}{\sum_{c^{\prime\prime}}\mathbb{P}(o\mid c^{\prime\prime},a)}\] (76) \[=\frac{N_{\mathsf{b}}}{2}\sum_{a,c^{\prime}\in\mathcal{A},o\in \mathcal{O}}\mathbb{P}(a,o\mid c^{\prime})\,\log(2\mathbb{P}(o\mid c^{\prime},a))\] (77) \[=\frac{N_{\mathsf{b}}}{2}\sum_{c^{\prime}\in\mathcal{A},o\in \mathcal{O}}\mathbb{P}(a^{\prime}_{1},o\mid c^{\prime})\,\log(2\mathbb{P}(o \mid c^{\prime},a^{\prime}_{1}))\] (78) \[=\frac{N_{\mathsf{b}}}{2}\sum_{o\in\mathcal{O}}(\mathbb{P}(a^{ \prime}_{1},o\mid c=a^{\prime}_{0})\,\log(2\mathbb{P}(o\mid c=a^{\prime}_{0},a^ {\prime}_{1}))+\mathbb{P}(a^{\prime}_{1},o\mid c=a^{\prime}_{1})\,\log(2 \mathbb{P}(o\mid c=a^{\prime}_{1},a^{\prime}_{1})))\] (79) \[=N_{\mathsf{b}}(\mathbb{P}(a^{\prime}_{1},+\mid c=a^{\prime}_{0}) \,\log(2\mathbb{P}(+\mid c=a^{\prime}_{0},a^{\prime}_{1}))+\mathbb{P}(a^{ \prime}_{1},+\mid c=a^{\prime}_{1})\,\log(2\mathbb{P}(+\mid c=a^{\prime}_{1},a ^{\prime}_{1})))\] (80) \[=N_{\mathsf{b}}\Big{(}\frac{1-\eta}{2C}\,\log(1-\eta)+\frac{1+\eta }{2C}\,\log(1+\eta)\Big{)}\] (81) \[=\frac{N_{\mathsf{b}}}{C}\,D_{\mathrm{KL}}(v_{\eta}\parallel v_{0})\] (82) \[\leq\frac{N_{\mathsf{b}}\,\eta^{2}}{C}\] (83)

Then from Eq. (70), and the fact that \(\mathfrak{A}_{\mathsf{b}}\) is \((2\varepsilon,\delta)\)-PAC,

\[H(\delta)\geq H_{2}(P_{e})\geq\log 2-\frac{N_{\mathsf{b}}\, \eta^{2}}{C}\] (84) \[\Rightarrow N_{\mathsf{b}}\geq\frac{C}{\eta^{2}}(\log 2-H(\delta))\] (85)

Which means that this must be \(\varphi_{\mathsf{b}}\), the requirement for \(\mathfrak{A}_{\mathsf{b}}\).

Finally, to compose the results from both branches, we observe that \(|\mathcal{D}|=|\mathcal{D}_{\mathsf{p}}|+|\mathcal{D}_{\mathsf{b}}|\). Also, for any \(\delta\in(0,0.5)\), say \(1/4\), \((\log 2-H(\delta))\) becomes a positive constant, and we can add both sizes asymptotically:

\[|\mathcal{D}|\in\Omega\Big{(}\frac{H}{\xi}+\frac{C}{\eta^{2}}\Big{)}\] (86)

To relate the parameters to features of the RDP, we observe that the number of states of any RDP in \(\mathbb{R}(L,H,\xi,\eta)\) is \(Q\leq 3H\). Also, the behavior policy is uniform everywhere except in \(q_{bL}\). Assuming \(C\geq 2\), the computation of the single-policy concentrability coefficient yields \(C^{*}_{\mathsf{R}}=C\), for any \(c\in\{a^{\prime}_{0},a^{\prime}_{1}\}\). Next, we compute the \(L^{p}_{1}\)-distinguishability of any RDP in this class. The \(L^{p}_{1}\)-distinguishability of a set of states \(\mathcal{Q}\) is the minimum \(L_{1}\) distance in distribution between episodes prefixes that are generated starting from any two states in \(\mathcal{Q}\). Let us consider the \(L_{1}\) norm for the pair \(q_{01}\) and \(q_{11}\),

\[\|\mathbb{P} (e_{1:H}\mid q_{01},\pi^{\mathsf{b}})-\mathbb{P}(e_{1:H}\mid q_{01}, \pi^{\mathsf{b}})\|_{1}=\] (87) \[=\sum_{e\in\mathcal{E}_{H-1}}|\mathbb{P}(e_{1:H}=e\mid q_{01})- \mathbb{P}(e_{1:H}=e\mid q_{11})|\] (88) \[=\sum_{earo\in\mathcal{E}_{L}}\mathbb{P}(e_{1:L-1}=e)|\mathbb{P}(a_ {L}=a,r_{L}=r,o_{L}=o\mid q_{01},e)-\mathbb{P}(a_{L}=a,r_{L}=r,o_{L}=o\mid q_{11},e)|\] (89)\[= \sum_{a\in\mathcal{AO}}\lvert\mathbb{P}(a_{L}=a,o_{L}=o\mid q_{0L})- \mathbb{P}(a_{L}=a,o_{L}=o\mid q_{1L})\rvert\] (90) \[= (1/2)\sum_{o\in\mathcal{O}}\lvert\mathbb{P}(o_{L}=o\mid a_{L}=a_{0 }^{\prime},q_{0L})-\mathbb{P}(o_{L}=o\mid a_{L}=a_{0}^{\prime},q_{1L})\rvert\] (91) \[= +(1/2)\sum_{o\in\mathcal{O}}\lvert\mathbb{P}(o_{L}=o\mid a_{L}=a_ {1}^{\prime},q_{0L})-\mathbb{P}(o_{L}=o\mid a_{L}=a_{1}^{\prime},q_{1L})\rvert\] (92) \[= \sum_{o\in\mathcal{O}}\lvert\mathbb{P}(o_{L}=o\mid a_{L}=a_{0}^{ \prime},q_{0L})-\mathbb{P}(o_{L}=o\mid a_{L}=a_{0}^{\prime},q_{1L})\rvert\] (93) \[= 2\lvert\mathbb{P}(o_{L}=+\mid a_{L}=a_{0}^{\prime},q_{0L})- \mathbb{P}(o_{L}=+\mid a_{L}=a_{0}^{\prime},q_{1L})\rvert\] (94) \[= 2\xi\] (95)

The \(L_{1}\) distance of suffixes from \(q_{01},q_{11}\) that are shorter than \(L\) have also a distance of \(2\xi\). On the other hand, any shorter prefix has a distance of 0. Since, \(L_{1}^{\text{p}}\) sums all these distances, the minimum across the \(q_{0i},q_{1i}\) pairs, is attained for \(q_{0L}\) and \(q_{1L}\), which determines \(\mu_{0}\geq 2\xi\). Also, the distance between any other pair of states in the same layer is strictly higher, since they differ deterministically in some reward or observation. Hence, the \(L_{1}^{\text{p}}\)-distinguishability of the entire RDP is \(\mu_{0}=2\xi\). Now, we choose \(L=H/2\), \(\eta=32\,\varepsilon/H\) and we assume \(\varepsilon\leq H\mu_{0}/64\), \(H\geq 2\). We can verify that these choices are consistent with the previous assumption \(\min\{\xi,\eta\}\geq\frac{16\,\varepsilon}{H-L}\). Substituting, the final requirement \(\varphi\) for the complete algorithm \(\mathfrak{A}\) is an exponential number of episodes in \(H\) or:

\[\lvert\mathcal{D}\rvert\in\Omega\Big{(}\frac{H}{\mu_{0}}+\frac{C_{\mathbf{R} }^{*}H^{2}}{\varepsilon^{2}}\Big{)}\] (96)

Now, for any \(H,\mu_{0},C_{\mathbf{R}}^{*},\varepsilon\) satisfying the previous assumptions, any algorithm cannot be \((\varepsilon,1/4)\)-optimal for the instances in \(\mathbb{R}(H/2,H,\mu_{0},32\,\varepsilon/H)\) if Eq. (96) is not satisfied. 

Note that in our RDP instance, the number of states and the horizon length scale linearly. So, we might equivalently write \(HO\) instead of \(H^{2}\).