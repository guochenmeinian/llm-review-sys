# Quantum Bayesian Optimization

Zhongxiang Dai*\({}^{1}\), Gregory Kang Ruey Lau*\({}^{1,2}\), Arun Verma\({}^{1}\), Yao Shu\({}^{1}\), Bryan Kian Hsiang Low\({}^{1}\), Patrick Jaillet\({}^{3}\)

\({}^{1}\)Department of Computer Science, National University of Singapore

\({}^{2}\)CNRS@CREATE, 1 Create Way, #08-01 Create Tower, Singapore 138602

\({}^{3}\)Department of Electrical Engineering and Computer Science, MIT

dzx@nus.edu.sg, greglau@comp.nus.edu.sg, arun@comp.nus.edu.sg, shuyao95@u.nus.edu, lowkh@comp.nus.edu.sg, jaillet@mit.edu

* Equal contribution.

###### Abstract

_Kernelized bandits_, also known as _Bayesian optimization_ (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their _cumulative regret_ which are sub-linear in the number \(T\) of iterations, and a regret lower bound of \(\Omega(\sqrt{T})\) has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on _quantum bandits_ have shown that with the aid of quantum computing, it is possible to achieve tighter regret upper bounds better than their corresponding classical lower bounds. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the _quantum-Gaussian process-upper confidence bound_ (Q-GP-UCB) algorithm. To the best of our knowledge, our Q-GP-UCB _is the first BO algorithm able to achieve a regret upper bound of \(\mathcal{O}(\operatorname{poly}\log T)\)_, which is significantly smaller than its regret lower bound of \(\Omega(\sqrt{T})\) in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment _using a real quantum computer_, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice.

## 1 Introduction

_Kernelized bandits_[9], also named _Bayesian optimization_ (BO) [10; 11; 16; 19; 32], has been an immensely popular method for various applications involving the optimization of complicated black-box reward functions. For example, BO has been extensively used to optimize the hyperparameters of machine learning (ML) models [12; 14; 22], the parameters of computationally expensive simulators [5], etc. In every iteration \(t=1,\ldots,T\), BO chooses an arm/input \(x_{t}\) and then queries the reward function \(f\) for a noisy observation \(y_{t}=f(x_{t})+\zeta_{t}\) where \(\zeta_{t}\) is a sub-Gaussian noise. In addition to its impressive practical performance, BO is also equipped with solid theoretical guarantees. A number of commonly adopted BO algorithms have been theoretically shown to enjoy sub-linear upper bounds on their _cumulative regret_[9; 33], which ensures that they are guaranteed to be able to find the global optimum of the reward function as \(T\) (i.e., the total number of function queries) increases. On the other hand, a lower bound of \(\Omega(\sqrt{T})\) (ignoring additional log factors) on the cumulative regret of BO has been shown [31], which represents the fundamental limit of any BO algorithm (in the classical setting). In other words, no classical BO algorithm can achieve a cumulative regret smaller than \(\Omega(\sqrt{T})\). This naturally begs the question: _can we leverage more advanced technology to go beyondthe classical setting and hence break this fundamental limit of \(\Omega(\sqrt{T})\)?_ In this work, we give an affirmative answer by showing that this can be achieved with the aid of _quantum computing_[34].

Quantum bandits, which incorporates quantum computing into bandit algorithms, has been studied by a number of recent works [8, 38, 40, 41]. Notably, the recent work of [38] has introduced quantum variants of classical algorithms for multi-armed bandits (MAB) and stochastic linear bandits (SLB). In the setting of quantum bandits adopted by [38], every query to the reward function \(f\) at the arm \(x_{t}\) (in the classical setting) is replaced by a chance to access a _quantum oracle_, which encodes the reward distribution for the arm \(x_{t}\). For every selected arm \(x_{t}\), [38] has proposed to adopt the _quantum Monte Carlo_ (QMC) algorithm [4, 28] as a subroutine to obtain an accurate estimation of \(f(x_{t})\) in an efficient way, i.e., using a small number of queries to the quantum oracle (Lemma 1). As a result, [38] has shown that the regrets of the quantum algorithms for both MAB and SLB are significantly improved compared with their classical counterparts and are smaller than their classical lower bounds. However, both MAB and SLB fall short when optimizing complex real-world functions (e.g., optimizing the hyperparameters of ML models). This is because MAB is unable to exploit the correlations among the arms to model the reward function, and the assumption of a linear reward function adopted by SLB is usually too restrictive in practice. Therefore, designing a quantum bandit algorithm capable of optimizing sophisticated non-linear reward functions, which is a critical step towards practically useful quantum bandit algorithms, is still an open problem. In this work, we resolve this open problem by proposing the first algorithm for quantum BO.

Similar to [38], in our quantum BO problem setting, queries to the reward function in the classical setting are replaced by access to a quantum oracle encoding the reward distribution. As discussed in [38, 39], a quantum oracle is available when the learning environment is implemented by a quantum algorithm, which makes the setting of our quantum BO fairly general. For example, our quantum BO algorithm may be used to optimize the hyperparameters of quantum ML algorithms [3], such as quantum support vector machines (SVMs) [29], quantum neural networks (NNs) [1, 18], among others. It may also be used to optimize the parameters of simulators implemented on a quantum computer, or for applications involving quantum systems where the data produced is inherently quantum. Moreover, our quantum BO algorithm could also be applied to classical data and algorithms, because as discussed in [39], any classical computer program can be converted to a quantum circuit, allowing it to serve as a quantum oracle in our quantum BO algorithm.

In this work, we introduce the first quantum BO algorithm: _quantum-Gaussian process-upper confidence bound_ (Q-GP-UCB). For every selected arm \(x_{s}\), our Q-GP-UCB algorithm (Algo. 1) adopts the QMC subroutine (Lemma 1) to efficiently estimate the corresponding reward function value \(f(x_{s})\) to satisfy a target estimation error \(\epsilon_{s}\). Every arm \(x_{s}\) is selected based on our _weighted_ GP posterior distribution, in which every previously selected arm \(x_{s}\) is given a weight of \(1/\epsilon_{s}^{2}\) which is inversely related to its estimation error. The theoretical analysis of our Q-GP-UCB is faced with non-trivial technical challenges, and our contributions can be summarized as follows:

* We analyze the growth rate of the _weighted information gain_ (Sec. 5.1) which arises due to the use of our weighted GP regression (Sec. 4.1), and show its connection with the standard maximum information gain commonly used in the analysis of BO/kernelized bandits [9, 33]. Our analysis here may be of broader independent interest for future works adopting weighted GP regression.
* We derive a tight confidence ellipsoid which gives a guarantee on the concentration of the reward function around the weighted GP posterior mean (Sec. 5.3), and discuss the intuition behind our proof which corresponds to an interesting interpretation about our Q-GP-UCB algorithm. A particularly crucial and novel step in our analysis relies on the recognition to apply the concentration inequality for 1-sub-Gaussian noise (Sec. 5.3).
* We prove that our Q-GP-UCB achieves a regret upper bound of \(\mathcal{O}(\text{poly}\log T)\) for the commonly used _squared exponential_ (SE) kernel (Secs. 5.4 and 5.5), which is considerably smaller than the classical regret lower bound of \(\Omega(\sqrt{T})\)[31] and hence represents a significant quantum speedup.
* By using a similar proof technique to the one adopted for our tight confidence ellipsoid (Sec. 5.3), we improve the confidence ellipsoid for the quantum linear UCB (Q-LinUCB) algorithm [38]. This improved analysis leads to _a tighter regret upper bound for Q-LinUCB_, which matches the regret of our Q-GP-UCB with the linear kernel (Sec. 5.6).
* We use simulations implemented based on the Qiskit package to verify the empirical improvement of our Q-GP-UCB over the classical GP-UCB and over Q-LinUCB, through a synthetic experiment and an experiment on _automated machine learning_ (AutoML) (Sec. 6). Notably, we have also performed an experiment _using a real quantum computer_, in which our Q-GP-UCB still achieves a consistent performance advantage (Fig. 4, App. J)

## 2 Related Work

A number of recent works have studied bandit algorithms in the quantum setting. The works of [8] and [40] have focused on the problem of pure exploration in quantum bandits. [25] has studied a different problem setting where the learner aims to maximize some property of an unknown quantum state (i.e., the rewards) by sequentially selecting the observables (i.e., actions). The recent work of [38] has introduced algorithms for quantum multi-armed bandits (MAB) and stochastic linear bandits (SLB), and has shown that by incorporating the QMC subroutine into MAB and SLB, tight regret upper bounds can be achieved which are better than the classical lower bounds. More recently, the works of [23] and [41] have followed similar approaches to introduce quantum bandit algorithms for, respectively, stochastic convex bandits and bandits with heavy-tailed reward distributions. In addition to quantum bandits, some recent works have introduced quantum reinforcement learning (RL) algorithms. [39] has assumed access to a generative model of the environment and proved that their quantum RL algorithm achieves significantly better sample complexity over their classical counterparts. More recently, [17; 43] have removed the requirement for a generative model in quantum RL and achieved quantum speedup in terms of the regret. More comprehensive reviews of the related works on quantum RL can be found in recent surveys (e.g., [27]). The problem setting of our quantum BO (Sec. 3.2) is the same as many of these above-mentioned previous works on quantum bandits [38; 41] and quantum RL [17; 39; 43]. However, to the best of our knowledge, none of the existing works on quantum bandits (and quantum RL) is able to handle non-linear reward functions, which we resolve in this work. Over the years, extensive efforts [7; 24; 30; 36] have been made to design BO/kernelized bandit algorithms whose regret upper bounds are small enough to (nearly) match the classical regret lower bound of \(\Omega(\sqrt{T})\) (ignoring additional log factors). Our work provides an alternative route by proving that with the aid of quantum computing, it is possible to achieve a tight regret upper bound which is significantly smaller than the classical regret lower bound.

## 3 Problem Setting and Background

### Classical Kernelized Bandits

A BO/kernelized bandit algorithm aims to maximize a reward function \(f:\mathcal{X}\rightarrow\mathbb{R}\) where \(\mathcal{X}\subset\mathbb{R}^{d}\), i.e., it aims to find \(x^{*}\in\operatorname*{arg\,max}_{x\in\mathcal{X}}f(x)\). Consistent with the previous works on BO/kernelized bandits [9; 37], we assume that \(f\) lies in the _reproducing kernel Hilbert space_ (RKHS) associated with a kernel \(k\): \(f\in H_{k}(\mathcal{X})\). That is, we assume that \(\norm{f}_{H_{k}}\leq B\) for \(B>0\), in which \(\norm{f}_{H_{k}}\) denotes the RKHS norm induced by the kernel \(k\). Note that unlike previous works on linear bandits [2], our assumption here allows \(f\) to be non-linear w.r.t. the input \(x\). In this work, we mainly focus on the widely used _squared exponential_ (SE) kernel: \(k(x,x^{\prime})\triangleq\exp(-\norm{x-x^{\prime}}_{2}^{2}/(2l^{2}))\) in which \(l\) is the _length scale_. We also extend our results to the Matern kernel in Sec. 5.7. Without loss of generality, we assume that \(k(\cdot,\cdot)\leq 1\). In the following, we use \([t]\) to denote \(\{1,\ldots,t\}\) for simplicity.

In every iteration \(t\in[T]\) of BO, an input \(x_{t}\) is selected, after which a corresponding noisy observation \(y_{t}=f(x_{t})+\zeta_{t}\) is collected where \(\zeta_{t}\) is a sub-Gaussian noise (e.g., bounded noise or Gaussian noise). The inputs \(x_{t}\)'s are sequentially selected by maximizing an _acquisition function_, which is calculated based on the _Gaussian process_ (GP) posterior distribution. Specifically, after \(t\) iterations, we use the current history \(\mathcal{D}_{t}\triangleq(\{x_{1},y_{1}\},\ldots,(x_{t},y_{t})\}\) to calculate the GP posterior predictive distribution at \(x\in\mathcal{X}\): \(\mathcal{N}(\mu_{t}(x),\sigma_{t}^{2}(x))\) where

\[\mu_{t}(x)\triangleq k_{t}^{\top}(x)(K_{t}+\lambda I)^{-1}Y_{t},\quad\sigma_{ t}^{2}(x)\triangleq k(x,x)-k_{t}^{\top}(x)(K_{t}+\lambda I)^{-1}k_{t}(x),\] (1)

in which \(k_{t}(x)\triangleq[k(x_{\tau},x)]_{\tau\in[t]}^{\top}\) and \(Y_{t}\triangleq[y_{x_{\tau}}]_{\tau\in[t]}^{\top}\) are column vectors, \(K_{t}\triangleq[k(x_{\tau},x_{\tau^{\prime}})]_{\tau,\tau^{\prime}\in[t]}\) is the \(t\times t\) covariance matrix, and \(\lambda>1\) is a regularization parameter. Based on (1), a commonly used acquisition function is GP-UCB [33], which selects the next query by: \(x_{t+1}=\operatorname*{arg\,max}_{x\in\mathcal{X}}\mu_{t}(x)+\xi_{t+1}\sigma_{ t}(x)\) where \(\xi_{t+1}\) is a parameter carefully chosen to balance exploration and exploitation. The performance of a BO/kernelized bandit algorithm is usually theoretically analyzed by deriving an upper bound on its cumulative regret: \(R_{T}=\sum_{t=1}^{T}[f(x^{*})-f(x_{t})]\), and a tighter regret upper bound is an indicator of a better theoretical convergence.

### Quantum Bandits/BO

A _quantum state_\(|x\rangle\) in Hilbert space \(\mathbb{C}^{n}\) can be expressed as a superposition of \(n\) basis states (e.g. qubits) via a vector \(\vec{x}=[x_{1},\ldots,x_{n}]^{\top}\), with \(|x_{i}|^{2}\) representing the probability of being in the \(i^{th}\) basis state. Given two quantum states \(|x\rangle\in\mathbb{C}^{n}\) and \(|y\rangle\in\mathbb{C}^{m}\), we use \(|x\rangle|y\rangle=[x_{1}y_{1},\ldots,x_{n}y_{m}]^{\top}\) to denote their tensor product. A quantum algorithm typically works by applying unitary operators to quantum states, and may have access to input data encoded in unitary operators called _quantum oracles_ (examples have been discussed in Sec. 1). We defer a more detailed introduction to quantum computing to related works dedicated to these topics (e.g., [26]).

Our quantum BO setting follows that of the quantum bandits works in [38; 41]. In every iteration of quantum BO, after an input \(x\) is selected, instead of observing a noisy reward as in classical BO/bandits (Sec. 3.1), we get a chance to access a quantum unitary oracle \(\mathcal{O}_{x}\) and its inverse that encode the noisy reward distribution. Specifically, let \(P_{x}\) denote the reward distribution, \(\Omega_{x}\) denote the finite sample space of \(P_{x}\), and \(y_{x}:\Omega_{x}\rightarrow\mathbb{R}\) denote the random reward associated with input \(x\). Then, \(\mathcal{O}_{x}\) is formally defined as:

\[\mathcal{O}_{x}:|0\rangle\rightarrow\sum_{\omega\in\Omega_{x}}\sqrt{P_{x}( \omega)}|\omega\rangle|y_{x}(\omega)\rangle.\] (2)

_Quantum mean estimation_, which aims to estimate the mean of an unknown distribution with better sample efficiency than classical algorithms, has been a widely studied problem [4; 21]. Consistent with the works of [38; 41], we will make use of the following _quantum Monte Carlo_ (QMC) algorithm:

**Lemma 1** (Quantum Monte Carlo (QMC) [28]).: _Let \(y_{x}:\Omega_{x}\rightarrow\mathbb{R}\) denote a random variable, \(\Omega_{x}\) is equipped with probability measure \(P_{x}\), and the quantum unitary oracle \(\mathcal{O}_{x}\) encodes \(P_{x}\) and \(y_{x}\)._

* _Bounded Noise__: If the noisy output observation satisfies_ \(y_{x}\in[0,1]\)_, then there exists a constant_ \(C_{1}>1\) _and a QMC algorithm_ \(\text{QMC}(\mathcal{O}_{x},\epsilon,\delta)\) _which returns an estimate_ \(\hat{y}_{x}\) _of_ \(\mathbb{E}[y_{x}]\) _such that_ \(\mathbb{P}(|\hat{y}_{x}-\mathbb{E}[y_{x}]|>\epsilon)\leq\delta\)_,_ _using at most_ \(\frac{C_{1}}{\epsilon}\log(1/\delta)\) _queries to_ \(\mathcal{O}_{x}\) _and its inverse._
* _Noise with Bounded Variance__: If the variance of_ \(y_{x}\) _is_ \(\leq\sigma^{2}\)_, then for_ \(\epsilon<4\sigma\)_, there exists a constant_ \(C_{2}>1\) _and a QMC algorithm_ \(\text{QMC}(\mathcal{O}_{x},\epsilon,\delta)\) _which returns an estimate_ \(\hat{y}_{x}\) _s.t._ \(\mathbb{P}(|\hat{y}_{x}-\mathbb{E}[y_{x}]|>\epsilon)\leq\delta\)_, using at most_ \(\frac{C_{2}\sigma}{\epsilon}\log_{2}^{3/2}\left(\frac{8\sigma}{\epsilon} \right)\log_{2}(\log_{2}\frac{8\sigma}{\epsilon})\log\frac{1}{\delta}\) _queries to_ \(\mathcal{O}_{x}\) _and its inverse._

The _quantum query complexity_ of a quantum algorithm is usually measured by _the number of queries to the quantum oracle_ [3; 38]. So, the sample complexities from Lemma 1 can be compared with that of classical Monte Carlo (MC) estimation. In the classical setting, it can be easily shown using concentration inequalities that MC estimation requires \(\widetilde{\mathcal{O}}(1/\epsilon^{2})\) samples to reach a target mean estimation error of \(\epsilon\). Therefore, the QMC algorithm (Lemma 1), which only needs \(\widetilde{\mathcal{O}}(1/\epsilon)\) samples, achieves a quadratic reduction in the required number of samples. This dramatic improvement is crucial for the quantum speedup in terms of regrets achieved by our algorithm (Sec. 5.4).

During our Q-GP-UCB algorithm, after every input \(x_{s}\) is selected, we will apply the QMC algorithm from Lemma 1 with the quantum oracle \(\mathcal{O}_{x_{s}}\) to obtain an estimation \(y_{s}\) of its reward \(f(x_{s})\) (line 6 of Algo. 1). Of note, the above-mentioned equivalence between a query to the quantum oracle and a sample for classical MC mean estimation implies that _a query to the quantum oracle \(\mathcal{O}_{x}\) in quantum BO/bandits is equivalent to the pulling of an arm \(x\) in classical BO/bandits_. Therefore, the cumulative regret \(R_{T}\) of our Q-GP-UCB algorithm is defined as the regret incurred after \(T\) queries to the quantum oracle, which makes it amenable to comparisons with the regrets of classical algorithms.

## 4 Quantum Bayesian Optimization

In this section, we first introduce weighted GP regression (Sec. 4.1) which will be used by our Q-GP-UCB algorithm to calculate the acquisition function for input selection, and then describe our Q-GP-UCB algorithm (Sec. 4.2) in detail.

### Weighted GP Posterior Distribution

In contrast to standard GP-UCB [33] which uses the standard GP posterior distribution (1) to calculate the acquisition function, our Q-GP-UCB makes use of a _weighted_ GP posterior distribution. That is, we assign a weight to every previous observation. Specifically, after stage \(s\) of our Q-GP-UCB (i.e., given \(\mathcal{D}_{s}\triangleq\{(x_{\tau},y_{\tau})\}_{\tau\in[s]}\)), we define the weight matrix \(W_{s}\triangleq\text{diag}(1/\epsilon_{1}^{2},\ldots,1/\epsilon_{s}^{2})\). \(W_{s}\) isan \(s\times s\) diagonal matrix, in which the \(\tau^{\text{th}}\) diagonal element represents the weight \(1/\epsilon_{\tau}^{2}\) given to the \(\tau^{\text{th}}\) observation \((x_{\tau},y_{\tau})\). We will set \(\epsilon_{\tau}=\widetilde{\sigma}_{\tau-1}(x_{\tau})/\sqrt{\lambda}\) (Sec. 4.2), i.e., \(\epsilon_{\tau}\) is calculated using the weighted GP posterior standard deviation (3) (conditioned on the first \(\tau-1\) observations) at \(x_{\tau}\).

Define \(K_{s}\triangleq[k(x_{\tau},x_{\tau^{\prime}})]_{\tau,\tau^{\prime}\in[s]}\) which is the \(s\times s\)-dimensional covariance matrix given the first \(s\) observations, and define \(\widetilde{K}_{s}\triangleq W_{s}^{1/2}K_{s}W_{s}^{1/2}\) which is the _weighted_ covariance matrix. Similarly, define \(k_{s}(x)\triangleq[k(x,x_{\tau})]_{\tau=[s]}^{\top}\) and \(\widetilde{k}_{s}(x)\triangleq W_{s}^{1/2}k_{s}(x)\). Denote the collection of output observations by \(Y_{s}\triangleq[y_{\tau}]_{\tau\in[s]}^{\top}\), and define \(\widetilde{Y}_{s}\triangleq W_{s}^{1/2}Y_{s}\). With these definitions, given \(\mathcal{D}_{s}\), our weighted GP posterior distribution at an input \(x\in\mathcal{X}\) is a Gaussian distribution: \(\mathcal{N}(\widetilde{\mu}_{s}(x),\widetilde{\sigma}_{s}^{2}(x))\), in which

\[\widetilde{\mu}_{s}(x)\triangleq\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s }+\lambda I)^{-1}\widetilde{Y}_{s},\quad\widetilde{\sigma}_{s}^{2}(x) \triangleq k(x,x)-\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s}+\lambda I)^{- 1}\widetilde{k}_{s}(x).\] (3)

Note that the GP posterior mean \(\widetilde{\mu}_{s}\) above is equivalently the solution to the following weighted kernel ridge regression problem: \(\widetilde{\mu}_{s}=\arg\min_{f\in H_{k}(X)}\sum_{\tau=1}^{s}\frac{1}{\epsilon _{\tau}^{2}}(y_{\tau}-f(x_{\tau}))^{2}+\lambda\|f\|_{H_{k}}^{2}\). We give a more detailed analysis of the weighted GP posterior (3) in App. A. Weighted GP regression has also been adopted by previous works on BO such as [15]. However, our choice of the weights, algorithmic design and theoretical analyses all require significantly novel treatments.

### Q-GP-UCB Algorithm

```
1:for stage \(s=1,2,\ldots\)do
2:\(x_{s}=\arg\max_{x\in\mathcal{X}}\widetilde{\mu}_{s-1}(x)+\beta_{s}\widetilde{ \sigma}_{s-1}(x)\) (3).
3:\(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\).
4:
4:
5:
6:
7: Update the weighted GP posterior (3) using \((x_{s},y_{s})\). ```

**Algorithm 1** Q-GP-UCB

Our Q-GP-UCB algorithm is presented in Algo. 1. Q-GP-UCB proceeds in stages. In stage \(s\), we first select the next input \(x_{s}\) to query by maximizing the GP-UCB acquisition function calculated using the weighted GP posterior distribution (3) (line 2 of Algo. 1). Here \(\beta_{s}\triangleq B+\sqrt{2(\widetilde{\gamma}_{s-1}+1+\log(2/\delta))}\) (more details in Sec. 5.3), in which \(\delta\in(0,2/e]\) and \(\widetilde{\gamma}_{s-1}\triangleq\frac{1}{2}\log(\text{det}(I+\frac{1}{ \lambda}\widetilde{K}_{s-1}))\) is the _weighted information gain_ (more details in Sec. 5.1). Next, we calculate \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\) (line 3) and \(N_{\epsilon_{s}}\) (line 4), in which \(N_{\epsilon_{s}}\) depends on the type of noise and the value of \(\epsilon_{s}\). Here \(\overline{m}\) is an upper bound on the total number \(m\) of stages which we will analyze in Sec. 5.2. Subsequently, unless the algorithm is terminated (line 5), we run the QMC algorithm (Lemma 1) to estimate \(f(x_{s})\) by querying the quantum oracle of \(x_{s}\) for \(N_{\epsilon_{s}}\) rounds (line 6). The QMC procedure returns an estimate \(y_{s}\) of the reward function value \(f(x_{s})\), for which the estimation error is guaranteed to be bounded: \(|y_{s}-f(x_{s})|\leq\epsilon_{s}\) with probability of at least \(1-\delta/(2\overline{m})\). Lastly, we update the weighted GP posterior (3) using the newly collected input-output pair \((x_{s},y_{s})\), as well as its weight \(1/\epsilon_{s}^{2}\) (line 7).

Of note, the value of \(\epsilon_{s}\) is used for both **(a)** calculating the number \(N_{\epsilon_{s}}\) of queries to the quantum oracle for \(x_{s}\), and **(b)** computing the weight \(1/\epsilon_{s}^{2}\) assigned to \((x_{s},y_{s})\) in the weighted GP regression (3) in the subsequent iterations. Regarding **(a)**, our designs of \(\epsilon_{s}\) and \(N_{\epsilon_{s}}=\widetilde{\mathcal{O}}(1/\epsilon_{s})\) have an interesting interpretation in terms of the exploration-exploitation trade-off: In the initial stages, the weighted GP posterior standard deviation \(\widetilde{\sigma}_{s-1}(x_{s})\) and hence \(\epsilon_{s}\) are usually large, which leads to a small number \(N_{\epsilon_{s}}\) of queries for every \(x_{s}\) and hence allows our Q-GP-UCB to favor the _exploration_ of more unique inputs; in later stages, \(\widetilde{\sigma}_{s-1}(x_{s})\) and \(\epsilon_{s}\) usually become smaller, which results in large \(N_{\epsilon_{s}}\)'s and hence causes our Q-GP-UCB to prefer the _exploitation_ of a small number of unique inputs. Regarding **(b)**, assigning a larger weight \(1/\epsilon_{s}^{2}\) to an input \(x_{s}\) with a smaller \(\epsilon_{s}\) is reasonable, because a smaller \(\epsilon_{s}\) indicates a smaller estimation error for \(y_{s}\) as we explained above, which makes the observation \(y_{s}\) more accurate and reliable for calculating the weighted GP regression (3).

Note that we have modified the original GP-UCB by querying every selected input \(x_{s}\) multiple times, in order to make it amenable to the integration of the QMC subroutine. The recent work of [6] has also adapted BO to evaluate a small number of unique inputs while querying each of them multiple times. It has been shown [6] that the resulting BO algorithm preserves both the theoretical guarantee (i.e., the regret upper bound) and empirical performance of the original BO, while significantly reducing the computational cost. So, the findings from [6] can serve as justifications for our modification to GP-UCB. Also note that despite this similarity, [6] still focuses on the traditional setting (i.e., their regret upper bound is \(R_{T}=\mathcal{O}(\gamma_{T}\sqrt{T})\)) and their regret analyses are entirely different from ours.

## 5 Theoretical Analysis

Throughout our analysis, we condition on the event that \(|f(x_{s})-y_{s}|\leq\epsilon_{s},\forall s=1,\ldots,m\). This event holds with probability of at least \(1-\delta/2\), because we set the error probability in QMC (Lemma 1) to \(\delta/(2\overline{m})\) in which \(\overline{m}\geq m\) (see Theorem 2 for details). For simplicity, we mainly focus on the scenario of bounded noise (i.e., the observations are bounded within \([0,1]\), the first case of Lemma 1), because the theoretical analyses and insights about the other scenario of noise with bounded variance (i.e., the second case of Lemma 1) are almost identical (more details in Sec. 5.5).

### Weighted Information Gain

To begin with, the following lemma (proof in App. B) gives an upper bound on the sum of the weights in all \(m\) stages, which will be useful for upper-bounding the weighted information gain \(\widetilde{\gamma}_{m}\).

**Lemma 2**.: _Choose \(\delta\in(0,2/e]\), then we have that \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\leq T^{2}\)._

Denote by \(\gamma_{T}\) the maximum information gain from any set of \(T\) inputs: \(\gamma_{T}\triangleq\max_{\{x_{1},\ldots,x_{T}\}\subset X}\frac{1}{2}\log \det(I+\frac{1}{\lambda}K_{T})\), which is commonly used in the analysis of BO/kernelized bandit algorithms [9; 33]. Denote by \(\widetilde{\gamma}_{m}\) the _weighted information gain_ from all \(m\) selected inputs in all stages: \(\widetilde{\gamma}_{m}\triangleq\frac{1}{2}\log\det(I+\frac{1}{\lambda} \widetilde{K}_{m})\). Note that in the definition of \(\widetilde{\gamma}_{m}\), we do not take the maximum over all sets of \(m\) inputs, so \(\widetilde{\gamma}_{m}\) still depends on the selected inputs \(\mathcal{X}_{m}\triangleq\{x_{1},\ldots,x_{m}\}\). Also note that \(\{\epsilon_{1},\ldots,\epsilon_{m}\}\) are known constants conditioned on \(\mathcal{X}_{m}\). This is because the weighted GP posterior standard deviation \(\widetilde{\sigma}_{s-1}\) (3) does not depend on the output observations, so every \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\) only depends on \(\{x_{1},\ldots,x_{s}\}\). The next theorem gives an upper bound on \(\widetilde{\gamma}_{m}\).

**Theorem 1**.: _(a) Given \(\mathcal{X}_{m}\triangleq\{x_{1},\ldots,x_{m}\}\), the growth rate of \(\widetilde{\gamma}_{m}\) is the same as that of \(\gamma_{\sum_{s=1}^{m}1/\epsilon_{s}^{2}}\). (b) \(\widetilde{\gamma}_{m}\leq\gamma_{T^{2}}\)._

We give the complete statement of result _(a)_ in Theorem 8 (App. C), which presents the concrete growth rate. As stated by result _(a)_, to obtain an upper bound on \(\widetilde{\gamma}_{m}\), we simply need to firstly use the upper bound on \(\gamma_{T}\) from previous works [35], and then replace the \(T\) in this upper bound by \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\). Based on this, the result _(b)_ has further upper-bounded \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\) by \(T^{2}\) through Lemma 2. Note that because \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\leq 1\), we have that \(1/\epsilon_{s}^{2}\geq 1\) and hence \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\geq m\). Therefore, our weighted information gain \(\widetilde{\gamma}_{m}\) has a looser upper bound than \(\gamma_{m}\). This intuitively implies that our weighted covariance matrix \(\widetilde{K}_{m}\) is expected to _contain more information than the original \(K_{m}\)_, whose implication will be discussed again in Sec. 5.2. The proof of Theorem 1 (App. C) has followed the analysis of the information gain from [35]. Specifically, we have leveraged a finite-dimensional projection of the infinite-dimensional RKHS feature space, and hence upper-bounded the information gain in terms of the tail mass of the eigenvalues of the kernel \(k\). The proof requires carefully tracking the impact of the weights \(1/\epsilon_{s}^{2}\) throughout the analysis. Importantly, our Theorem 1 and its analysis may be _of wider independent interest_ for future works which also adopt weighted GP regression (Sec. 4.1).

The growth rate of \(\gamma_{T}\) has been characterized for commonly used kernels [35]. Based on these, the next corollary provides the growth rates of \(\widetilde{\gamma}_{m}\) for the linear and squared exponential (SE) kernels.

**Corollary 1**.: _For the linear and squared exponential (SE) kernels, we have that_

\[\widetilde{\gamma}_{m} =\mathcal{O}(d\log\sum_{s=1}^{m}1/\epsilon_{s}^{2})=\mathcal{O}(d \log T)\qquad\text{linear kernel,}\] \[\widetilde{\gamma}_{m} =\mathcal{O}(\log^{d+1}\sum_{s=1}^{m}1/\epsilon_{s}^{2})= \mathcal{O}((\log T)^{d+1})\qquad\text{SE kernel.}\]

### Upper Bound on The Total Number \(m\) of Stages

The next theorem gives an upper bound on the total number of stages of our Q-GP-UCB algorithm.

**Theorem 2**.: _For the linear kernel, our Q-GP-UCB algorithm has at most \(m=\mathcal{O}(d\log T)\) stages. For the SE kernel, our Q-GP-UCB algorithm has at most \(m=\mathcal{O}((\log T)^{d+1})\) stages._

Note that for the linear kernel, our upper bound on \(m\) matches that of the Q-LinUCB algorithm from [38] (see Lemma 2 from [38]). The proof of Theorem 2 is given in App. D, and here we give a brief sketch of the proof. For stage \(s\), define \(V_{s}\triangleq\lambda I+\sum_{\tau=1}^{s}(1/\epsilon_{\tau}^{2})\phi(x_{\tau} )\phi(x_{\tau})^{\top}\), in which \(\phi(\cdot)\) is the (potentially infinite-dimensional) RKHS feature mapping: \(k(x,x^{\prime})=\phi(x)^{\top}\phi(x^{\prime})\). Intuitively, \(\log\det V_{s}\) represents the amount of information contained in the first \(s\) selected inputs \(\{x_{1},\ldots,x_{s}\}\). As the first step in our proof, we prove that thanks to our choice of \(\epsilon_{s}\) (line 3 of Algo. 1) and our design of the weights \(1/\epsilon_{s}^{2}\), the amount of information \(\log\det V_{s}\) is doubled after every stage: \(\det V_{\tau+1}=2\det V_{\tau},\forall\tau=0,\ldots,m-1\). This allows us to show that \(m\log 2=\log(\det(V_{m})/\det(V_{0}))\) where \(V_{0}=\lambda I\). Next, we show that this term is also related to our weighted information gain: \(\log(\det(V_{m})/\det(V_{0}))=2\widetilde{\gamma}_{m}\), which allows us to upper-bound \(m\) in terms of \(\widetilde{\gamma}_{m}\) and hence in terms of \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\) (see Corollary 1). This eventually allows us to derive an upper bound on \(m\) by using the fact that the total number of iterations (i.e., queries to the quantum oracles) is no larger than \(T\). Therefore, the key intuition of the proof here is that as long as we gather a sufficient amount of information in every stage, we do not need a large total number of stages.

### Confidence Ellipsoid

The next theorem proves the concentration of \(f\) around the weighted GP posterior mean (3).

**Theorem 3**.: _Let \(\eta\triangleq 2/T\), \(\lambda\triangleq 1+\eta\), and \(\beta_{s}\triangleq B+\sqrt{2(\widetilde{\gamma}_{s-1}+1+\log(2/\delta))}\). We have with probability of at least \(1-\delta/2\) that_

\[|f(x)-\widetilde{\mu}_{s-1}(x)|\leq\beta_{s}\widetilde{\sigma}_{s-1}(x), \qquad\forall s\in[m],x\in\mathcal{X}.\]

The proof of Theorem 3 is presented in App. E, and here we give a brief proof sketch. Denote by \(\mathcal{F}_{s-1}\) the \(\sigma\)-algebra \(\mathcal{F}_{s-1}=\{x_{1},\ldots,x_{s},\zeta_{1},\ldots,\zeta_{s-1}\}\) where \(\zeta_{k}=y_{k}-f(x_{k})\) is the noise. Recall that \(W_{s}^{1/2}\triangleq\text{diag}(1/\epsilon_{1},\ldots,1/\epsilon_{s})\) (Sec. 4.1), and define \(\zeta_{1:s}\triangleq[\zeta_{k}]_{k\in[s]}\). In the first part of the proof, we use the RKHS feature space view of the weighted GP posterior (3) (see App. A) to upper-bound \(|f(x)-\widetilde{\mu}_{s-1}(x)|\) in terms of \(\widetilde{\sigma}_{s-1}(x)\) and \(||W_{s}^{1/2}\zeta_{1:s}||_{((\widetilde{K}_{s}+\eta I)^{-1}+I)^{-1}}\). Next, the most crucial step in the proof is to upper-bound \(||W_{s}^{1/2}\zeta_{1:s}||_{((\widetilde{K}_{s}+\eta I)^{-1}+I)^{-1}}\) in terms of \(\widetilde{\gamma}_{s}\) by applying the self-normalized concentration inequality from Theorem 1 of [9] to _1-sub-Gaussian noise_. This is feasible because _(a)_ every \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\) is \(\mathcal{F}_{s-1}\)-measurable, and _(b)_ conditioned on \(\mathcal{F}_{s-1}\), the scaled noise term \(\zeta_{s}/\epsilon_{s}\) (in \(W_{s}^{1/2}\zeta_{1:s}\)) is \(1\)-sub-Gaussian. The statement _(a)_ follows because \(\epsilon_{s}\) only depends on \(\{x_{1},\ldots,x_{s}\}\) as we have discussed in Sec. 5.1. The statement _(b)_ follows since our QMC subroutine ensures that every noise term \(\zeta_{k}\) is bounded within \([-\epsilon_{k},\epsilon_{k}]\) (Sec. 4.2) because \(|y_{k}-f(x_{k})|\leq\epsilon_{k}\) (with high probability). This suggests that conditioned on \(\mathcal{F}_{s-1}\) (which makes \(\epsilon_{s}\) a known constant as discussed above), \(\zeta_{s}/\epsilon_{s}\) is bounded within \([-1,1]\) and is hence 1-sub-Gaussian.

This critical step in the proof is in fact also consistent with an interesting interpretation about our algorithm. That is, after \(x_{s}\) is selected in stage \(s\), we adaptively decide the number \(N_{\epsilon_{s}}\) of queries to the quantum oracle in order to reach the target accuracy \(\epsilon_{s}\) (line 4 of Algo. 1). This ensures that \(|y_{s}-f(x_{s})|\leq\epsilon_{s}\) (with high probability), which guarantees that the noise \(\zeta_{s}=y_{s}-f(x_{s})\) is (conditionally) \(\epsilon_{s}\)-sub-Gaussian. Moreover, note that the vector of observations \(\widetilde{Y}_{s}\) used in the calculation of the weighted GP posterior mean (3) is \(\widetilde{Y}_{s}=W_{s}^{1/2}Y_{s}\). So, from the perspective of the weighted GP posterior (3), every noisy observation \(y_{s}\), and hence every noise \(\zeta_{s}\), is multiplied by \(1/\epsilon_{s}\). So, the effective noise of the observations \(\widetilde{Y}_{s}\) used in the weighted GP posterior (i.e., \(\zeta_{s}/\epsilon_{s}\)) is \(\epsilon_{s}/\epsilon_{s}=1\)-sub-Gaussian. This shows the underlying intuition as to why our proof of the concentration of the reward function \(f\) using the weighted GP regression can make use of _1-sub-Gaussian noise_ (e.g., in contrast to the \(\sigma\)-sub-Gaussian noise used in the proof of classical GP-UCB [9]).

Our tight confidence ellipsoid around \(f\) from Theorem 3 is crucial for deriving a tight regret upper bound for our Q-GP-UCB (Sec. 5.4). Moreover, as we will discuss in more detail in Sec. 5.6, our proof technique for Theorem 3, interestingly, can be adopted to obtain a tighter confidence ellipsoid for the Q-LinUCB algorithm from [38] and hence improve its regret upper bound.

### Regret Upper Bound

Here we derive an upper bound on the cumulative regret of our Q-GP-UCB algorithm.

**Theorem 4**.: _With probability of at least \(1-\delta\), we have that1_

Footnote 1: We have omitted the dependency on \(\log(1/\delta)\) for simplicity. Refer to App. F for the full expression.

\[R_{T} =\mathcal{O}\big{(}d^{3/2}(\log T)^{3/2}\log(d\log T)\big{)}\qquad \text{linear kernel},\] \[R_{T} =\mathcal{O}\big{(}(\log T)^{3(d+1)/2}\log((\log T)^{d+1})\big{)} \qquad\text{SE kernel}.\]

The proof of Theorem 4 is presented in App. F. The proof starts by following the standard technique in the proof of UCB-type algorithms to show that \(r_{s}=f(x^{*})-f(x_{s})\leq 2\beta_{s}\vec{\sigma}_{s-1}(x_{s})\). Next, importantly, our design of \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\) allows us to show that \(r_{s}\leq 2\beta_{s}\epsilon_{s}\sqrt{\lambda}\). After that, the total regrets incurred in a stage \(s\) can be upper-bounded by \(N_{\epsilon_{s}}\times 2\beta_{s}\epsilon_{s}\sqrt{\lambda}=\mathcal{O}( \frac{1}{\epsilon_{s}}\log(\overline{m}/\delta)\times\beta_{s}\epsilon_{s})= \mathcal{O}(\beta_{s}\log(\overline{m}/\delta))\). Lastly, the regret upper bound follows by summing the regrets from all \(m\) stages. Of note, for the commonly used SE kernel, our regret upper bound is of the order \(\mathcal{O}(\text{poly}\log T)\), which is significantly smaller than the regret lower bound of \(\Omega(\sqrt{T})\) in the classical setting shown by [31]. This improvement over the classical fundamental limit is mostly attributed to the use of the QMC (Lemma 1), i.e., line 6 of Algo. 1. If the QMC procedure is replaced by classical MC estimation, then in contrast to the \(N_{\epsilon_{s}}=\mathcal{O}(1/\epsilon_{s})\) queries by QMC, every stage would require \(N_{\epsilon_{s}}=\mathcal{O}(1/\epsilon_{s}^{2})\) queries. As a result, this would introduce an additional factor of \(1/\epsilon_{s}\) to the total regrets in a stage \(s\) (as discussed above), which would render the derivation of our regret upper bound (Theorem 4) invalid. This verifies that our tight regret upper bound in Theorem 4 is only achievable with the aid of quantum computing. Our Theorem 4 is a demonstration of the immense potential of quantum computing to dramatically improve the theoretical guarantees over the classical setting.

Importantly, when the linear kernel is used, the regret upper bound of our Q-GP-UCB is in fact better than that of the Q-LinUCB algorithm from [38]. This improvement arises from our tight confidence ellipsoid in Theorem 3. In Sec. 5.6, we will give a more detailed discussion of this improvement and adopt our proof technique for Theorem 3 to improve the confidence ellipsoid for Q-LinUCB, after which their improved regret upper bound matches that of our Q-GP-UCB with the linear kernel.

### Regret Upper Bound for Noise with Bounded Variance

Here we analyze the regret of our Q-GP-UCB when the variance of the noise is bounded by \(\sigma^{2}\), which encompasses common scenarios including \(\sigma\)-sub-Gaussian noises and Gaussian noises with a variance of \(\sigma^{2}\). Consistent with the analysis of Q-LinUCB [38], in order to apply the theoretical guarantee provided by QMC (Lemma 1) for noise with bounded variance, here we need to assume that the noise variance is not too small, i.e., we assume that \(\sigma>1/4\). In this case of noise with bounded variance, the following theorem gives an upper bound on the regret of our Q-GP-UCB.

**Theorem 5**.: _With probability of at least \(1-\delta\), we have that_

\[R_{T} =\mathcal{O}\big{(}\sigma d^{3/2}(\log T)^{3/2}\log(d\log T)(\log _{2}\sigma T)^{3/2}\log_{2}(\log_{2}\sigma T)\big{)}\qquad\text{linear kernel,}\] (4) \[R_{T} =\mathcal{O}\big{(}\sigma(\log T)^{3(d+1)/2}\log((\log T)^{d+1})( \log_{2}\sigma T)^{3/2}\log_{2}(\log_{2}\sigma T)\big{)}\qquad\text{SE kernel.}\]

The proof of Theorem 5 is given in App. G. Note that same as Theorem 4, the regret for the SE kernel in Theorem 5 is also of the order \(\mathcal{O}(\text{poly}\log T)\), which is also significantly smaller than the classical regret lower bound of \(\Omega(\sqrt{T})\)[31]. Moreover, Theorem 5 shows that for both the linear kernel and SE kernel, the regret of our Q-GP-UCB is reduced when the noise variance \(\sigma^{2}\) becomes smaller.

### Improvement to Quantum Linear UCB (Q-LinUCB)

As we have mentioned in Sec. 5.4, the regret upper bound of our Q-GP-UCB with the linear kernel is \(R_{T}=\mathcal{O}(d^{3/2}(\log T)^{3/2}\log(d\log T))\), which is better than the corresponding \(R_{T}=\mathcal{O}(d^{2}(\log T)^{3/2}\log(d\log T))\) for Q-LinUCB [38]2 by a factor of \(\mathcal{O}(\sqrt{d})\). This improvement can be attributed to our tight confidence ellipsoid from Theorem 3. Here, following the general idea of the proof of our Theorem 3, we prove a tighter confidence ellipsoid for the Q-LinUCB algorithm, i.e., we improve Lemma 3 from the work of [38]. Here we follow the notations from [38], and we defer the detailed notations, as well as the proof of Theorem 6 below, to App. H due to space limitation.

**Theorem 6** (Improved Confidence Ellipsoid for [38]).: _With probability of at least \(1-\delta\),_

\[\theta^{*}\in\mathcal{C}_{s}=\{\theta\in\mathbb{R}^{d}:\left\|\theta-\widehat{ \theta}_{s}\right\|_{V_{s}}\leq\Big{(}\sqrt{2d\log\Big{(}1+\frac{L^{2}}{\lambda }\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}\Big{)}\frac{1}{\delta}+\sqrt{\lambda} S\Big{)}\},\qquad\forall s\in[m].\]

Note that the size of the confidence ellipsoid from our Theorem 6 is of the order \(\mathcal{O}(\sqrt{d\log(\sum_{k=1}^{s}1/\epsilon_{k}^{2})})=\mathcal{O}(\sqrt {d\log T})\) (we have used Lemma 2 here), which improves over the \(\mathcal{O}(\sqrt{ds})=\mathcal{O}(\sqrt{dm})=\mathcal{O}(\sqrt{d\times d\log T })=\mathcal{O}(d\sqrt{\log T})\) from Lemma 3 of [38] by a factor of \(\sqrt{d}\). Plugging in our tighter confidence ellipsoid (Theorem 6) into the regret analysis of Q-LinUCB, the regret upper bound for Q-LinUCB is improved to \(R_{T}=\mathcal{O}\big{(}d^{3/2}\log^{3/2}T\log(d\log T)\big{)}\) (more details at the end of App. H). This improved regret upper bound exactly matches that of our Q-GP-UCB with the linear kernel (Theorem 4). Similar to Theorem 3, the most crucial step in the proof of Theorem 6 is to apply the self-normalized concentration inequality from Theorem 1 of [2] to _1-sub-Gaussian noise_. Again this is feasible because \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\) is \(\mathcal{F}_{s-1}\)-measurable, and conditioned on \(\mathcal{F}_{s-1}\), the scaled noise \(\zeta_{s}/\epsilon_{s}\) is \(1\)-sub-Gaussian.

### Regret Upper Bound for the Matern Kernel

So far we have mainly focused on the linear and SE kernels in our analysis. Here we derive a regret upper bound for our Q-GP-UCB algorithm for the Matern kernel with smoothness parameter \(\nu\).

**Theorem 7** (Matern Kernel, Bounded Noise).: _With probability of at least \(1-\delta\), we have that_

\[R_{T}=\widetilde{\mathcal{O}}\big{(}T^{3d/(2\nu+d)}\big{)}.\]

The proof is in App. I. Note that the state-of-the-art regret upper bound for the Matern kernel in the classical setting is \(R_{T}=\mathcal{O}(\sqrt{T\gamma_{T}})=\widetilde{\mathcal{O}}(T^{(\nu+d)/(2 \nu+d)})\)[24; 30; 36; 7], which matches the corresponding classical regret lower bound (up to logarithmic factors in \(T\)) [35]. Therefore, the regret upper bound of our Q-GP-UCB for the Matern kernel (Theorem 7) improves over the corresponding classical regret lower bound when \(\nu>2d\), i.e., when the reward function is sufficiently smooth. Also note that similar to the classical GP-UCB [33] (with regret \(R_{T}=\mathcal{O}(\gamma_{T}\sqrt{T})=\widetilde{\mathcal{O}}(T^{(\nu+3d/2)/ (2\nu+d)})\)) which requires the reward function to be sufficiently smooth (i.e., \(\nu>d/2\)) to attain sub-linear regrets for the Matern kernel, our Q-GP-UCB, as the first quantum BO algorithm, also requires the reward function to be smooth enough (i.e., \(\nu>d\)) in order to achieve a sub-linear regret upper bound for the Matern kernel. We leave it to future works to further improve our regret upper bound and hence relax this requirement for smooth functions.

## 6 Experiments

We use the Qiskit python package to implement the QMC algorithm (Lemma 1) following the recent work of [20]. Some experimental details are deferred to App. J due to space limitation.

**Synthetic Experiment.** Here we use a grid of \(|\mathcal{X}|=20\) equally spaced points within \([0,1]\) as the \(1\)-dimensional input domain \(\mathcal{X}\) (\(d=1\)), and sample a function \(f\) from a GP prior with the SE kernel. The sampled function \(f\) is scaled so that its output is bounded within \([0,1]\), and then used as the reward function in the synthetic experiments. We consider two types of noises: _(a)_ bounded noise (within \([0,1]\)) and _(b)_ Gaussian noise, which correspond to the two types of noises in Lemma 1, respectively. For _(a)_ bounded noise, we follow the practice of [38] such that when an input \(x\) is selected, we treat the function value \(f(x)\) as the probability for a Bernoulli distribution, i.e., we observe an output of \(1\) with probability of \(f(x)\) and \(0\) otherwise. For _(b)_ Gaussian noise, we simply add a zero-mean Gaussian noise with variance \(\sigma^{2}\) to \(f(x)\). The results for _(a)_ bounded noise and _(b)_ Gaussian noise are shown in Figs. 1 (a) and (b), respectively. The figures show that for both types of noises, our Q-GP-UCB significantly outperforms the classical baseline of GP-UCB. Specifically, although our Q-GP-UCB incurs larger regrets in the initial stage, it is able to leverage the accurate observations provided by the QMC subroutine to rapidly find the global optimum. These results show that the quantum speedup of our Q-GP-UCB in terms of the tighter regret upper bounds (Theorems 4 and 5) may also be relevant in practice. We have additionally compared with linear UCB (LinUCB) [2] and Q-LinUCB [38], which are, respectively, the most representative classical and quantum linear bandit algorithms. The results in Fig. 2 (App. J) show that in these experiments where the rewardfunction is non-linear, algorithms based on linear bandits severely underperform compared with BO/kernelized bandit algorithms in both the classical and quantum settings.

**AutoML Experiment.** In our experiments on _automated machine learning_ (AutoML), we use our Q-GP-UCB algorithm to tune the hyperparameters of an SVM for a classification task. Here we consider a Gaussian noise with a variance of \(\sigma^{2}\) since it is more practical and more commonly used in real-world experiments. The results in Fig. 1 (c) show that our Q-GP-UCB again significantly outperforms the classical GP-UCB. Fig. 1 (d) additionally shows the comparisons with LinUCB and Q-LinUCB. The results again corroborate that BO/kernelized bandit algorithms are considerably superior in real-world applications with highly non-linear reward functions in both the classical and quantum settings. These results here further demonstrate the potential of our Q-GP-UCB to lead to quantum speedup in practical applications.

**More Realistic Experiments.** We have additionally tested the performance of our Q-GP-UCB algorithm after accounting for the effect of quantum noise, by incorporating into our Qiskit simulations a noise model based on the actual performance of real IBM quantum computers. The results (Fig. 3 in App. J) show that although the addition of quantum noise slightly deteriorates the performance of our Q-GP-UCB, it is still able to significantly outperform classical GP-UCB. Notably, we have additionally performed an experiment _using a real quantum computer_ (details in App. J), in which the performance of our Q-GP-UCB, although further worsened compared to the experiment using simulated noise, is still considerably better than classical GP-UCB (Fig. 4, App. J). Also note that the work of [38] has not shown that Q-LinUCB outperforms LinUCB in the presence of simulated quantum noise. Therefore, the consistently superior performances of our Q-GP-UCB over GP-UCB with both simulated quantum noise and a real quantum computer serve as important new support for the potential practical advantages of quantum bandit algorithms.

**Discussion.** In our experimental results (e.g., Fig. 1), our Q-GP-UCB usually has relatively larger regrets in the initial stages but quickly achieves zero regret thereafter (i.e., the curve plateaus), which has also been observed in [38] for the Q-LinUCB algorithm. This is because in the initial stages, our Q-GP-UCB explores a smaller number of unique arms than GP-UCB. However, after the initial exploration, our Q-GP-UCB quickly starts to perform reliable exploitation, because the accurate reward observations achieved thanks to our QMC subroutines allow us to rapidly learn the reward function and hence find the optimal arm.

## 7 Conclusion

We have introduced the first quantum BO algorithm, named Q-GP-UCB. Our Q-GP-UCB achieves a regret upper bound of \(\mathcal{O}(\operatorname{poly}\log T)\), which is significantly smaller than the classical regret lower bound of \(\Omega(\sqrt{T})\). A limitation of our work is that the regret upper bound of our Q-GP-UCB: \(\mathcal{O}((\log T)^{3(d+1)/2})\) (ignoring polylog factors) has a worse dependency on the input dimension \(d\) than the regret of the classical GP-UCB: \(\mathcal{O}((\log T)^{d+1}\sqrt{T})\). A similar limitation is shared by Q-LinUCB, since its regret has a dependency of \(\mathcal{O}(d^{3/2})\) in contrast to the \(\mathcal{O}(d)\) of LinUCB. It is an interesting future work to explore potential approaches to remove this extra dependency on \(d\). Another limitation of our work is that for the Matern kernel, we require the reward function to be smooth enough in order to achieve a sub-linear regret upper bound (Sec. 5.7). So, another important future work is to further tighten the regret upper bound of our Q-GP-UCB for the Matern kernel when the reward function is less smooth, as well as for other kernels encompassing non-smooth functions such as the neural tangent kernel which has been adopted by recent works on neural bandits [13, 42, 44]. Moreover, another interesting future work is to derive regret lower bounds in our setting of quantum kernelized bandits, which can help evaluate the tightness of our regret upper bounds.

Figure 1: Cumulative regret for synthetic experiments with (a) bounded noise and (b) Gaussian noise. (c) Cumulative regret for the AutoML experiment; (d) additionally includes results for linear bandits.

## Acknowledgements and Disclosure of Funding

This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG-PhD/\(2023\)-\(01\)-\(039\)J). DesCartes: this research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme.

## References

* [1] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner. The power of quantum neural networks. _Nature Computational Science_, 1(6):403-409, 2021.
* [2] Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. In _Proc. NeurIPS_, volume 24, 2011.
* [3] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd. Quantum machine learning. _Nature_, 549(7671):195-202, 2017.
* [4] G. Brassard, P. Hoyer, M. Mosca, and A. Tapp. Quantum amplitude amplification and estimation. _Contemporary Mathematics_, 305:53-74, 2002.
* [5] S. Cakmak, R. Astudillo Marban, P. Frazier, and E. Zhou. Bayesian optimization of risk measures. In _Proc. NeurIPS_, volume 33, pages 20130-20141, 2020.
* [6] D. Calandriello, L. Carratino, A. Lazaric, M. Valko, and L. Rosasco. Scaling Gaussian process optimization by evaluating a few unique candidates multiple times. In _Proc. ICML_, pages 2523-2541. PMLR, 2022.
* [7] R. Camilleri, K. Jamieson, and J. Katz-Samuels. High-dimensional experimental design and kernel bandits. In _Proc. ICML_, pages 1227-1237. PMLR, 2021.
* [8] B. Casale, G. Di Molfetta, H. Kadri, and L. Ralaivola. Quantum bandits. _Quantum Machine Intelligence_, 2:1-7, 2020.
* [9] S. R. Chowdhury and A. Gopalan. On kernelized multi-armed bandits. In _Proc. ICML_, pages 844-853, 2017.
* [10] Z. Dai, B. K. H. Low, and P. Jaillet. Federated Bayesian optimization via Thompson sampling. In _Proc. NeurIPS_, 2020.
* [11] Z. Dai, B. K. H. Low, and P. Jaillet. Differentially private federated Bayesian optimization with distributed exploration. In _Proc. NeurIPS_, volume 34, 2021.
* [12] Z. Dai, Y. Shu, B. K. H. Low, and P. Jaillet. Sample-then-optimize batch neural Thompson sampling. In _Proc. NeurIPS_, 2022.
* [13] Z. Dai, Y. Shu, A. Verma, F. X. Fan, B. K. H. Low, and P. Jaillet. Federated neural bandits. In _Proc. ICLR_, 2023.
* [14] Z. Dai, H. Yu, B. K. H. Low, and P. Jaillet. Bayesian optimization meets Bayesian optimal stopping. In _Proc. ICML_, pages 1496-1506, 2019.
* [15] Y. Deng, X. Zhou, B. Kim, A. Tewari, A. Gupta, and N. Shroff. Weighted Gaussian process bandits for non-stationary environments. In _Proc. AISTATS_, pages 6909-6932. PMLR, 2022.
* [16] P. I. Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.
* [17] B. Ganguly, Y. Wu, D. Wang, and V. Aggarwal. Quantum computing provides exponential regret improvement in episodic reinforcement learning. arXiv:2302.08617, 2023.
* [18] S. Garg and G. Ramakrishnan. Advances in quantum deep learning: An overview. arXiv preprint arXiv:2005.04316, 2020.
* [19] R. Garnett. _Bayesian Optimization_. Cambridge Univ. Press, 2022.
* [20] D. Grinko, J. Gacon, C. Zoufal, and S. Woerner. Iterative quantum amplitude estimation. _npj Quantum Information_, 7(1):52, 2021.
* [21] L. K. Grover. A framework for fast quantum mechanical algorithms. In _Proceedings of the thirtieth annual ACM symposium on Theory of computing_, pages 53-62, 1998.
* [22] F. Hutter, L. Kotthoff, and J. Vanschoren. _Automated machine learning: methods, systems, challenges_. Springer Nature, 2019.

* [23] T. Li and R. Zhang. Quantum speedups of optimizing approximately convex functions with applications to logarithmic regret stochastic convex bandits. In _Proc. NeurIPS_, 2022.
* [24] Z. Li and J. Scarlett. Gaussian process bandit optimization with few batches. In _Proc. AISTATS_, pages 92-107. PMLR, 2022.
* [25] J. Lumbreras, E. Haapasalo, and M. Tomamichel. Multi-armed quantum bandits: Exploration versus exploitation when learning properties of quantum states. _Quantum_, 6:749, 2022.
* [26] A. Messiah. _Quantum mechanics_. Courier Corporation, 2014.
* [27] N. Meyer, C. Ufrecht, M. Periyasamy, D. D. Scherer, A. Plinge, and C. Mutschler. A survey on quantum reinforcement learning. arXiv:2211.03464, 2022.
* [28] A. Montanaro. Quantum speedup of Monte Carlo methods. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 471(2181):20150301, 2015.
* [29] P. Rebentrost, M. Mohseni, and S. Lloyd. Quantum support vector machine for big data classification. _Physical review letters_, 113(13):130503, 2014.
* [30] S. Salgia, S. Vakili, and Q. Zhao. A domain-shrinking based Bayesian optimization algorithm with order-optimal regret performance. In _Proc. NeurIPS_, volume 34, pages 28836-28847, 2021.
* [31] J. Scarlett, I. Bogunovic, and V. Cevher. Lower bounds on regret for noisy Gaussian process bandit optimization. In _Proc. COLT_, pages 1723-1742. PMLR, 2017.
* [32] B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of Bayesian optimization. _Proceedings of the IEEE_, 104(1):148-175, 2016.
* [33] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _Proc. ICML_, pages 1015-1022, 2010.
* [34] A. Steane. Quantum computing. _Reports on Progress in Physics_, 61(2):117, 1998.
* [35] S. Vakili, K. Khezeli, and V. Picheny. On information gain and regret bounds in Gaussian process bandits. In _Proc. AISTATS_, pages 82-90. PMLR, 2021.
* [36] M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of kernelised contextual bandits. In _Proc. UAI_, 2013.
* [37] A. Verma, Z. Dai, and B. K. H. Low. Bayesian optimization under stochastic delayed feedback. In _Proc. ICML_, pages 22145-22167. PMLR, 2022.
* [38] Z. Wan, Z. Zhang, T. Li, J. Zhang, and X. Sun. Quantum multi-armed bandits and stochastic linear bandits enjoy logarithmic regrets. In _Proc. AAAI_.
* [39] D. Wang, A. Sundaram, R. Kothari, A. Kapoor, and M. Roetteler. Quantum algorithms for reinforcement learning with a generative model. In _Proc. ICML_, pages 10916-10926. PMLR, 2021.
* [40] D. Wang, X. You, T. Li, and A. M. Childs. Quantum exploration algorithms for multi-armed bandits. In _Proc. AAAI_, volume 35, pages 10102-10110, 2021.
* [41] Y. Wu, C. Guan, V. Aggarwal, and D. Wang. Quantum heavy-tailed bandits. arXiv:2301.09680, 2023.
* [42] W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural Thompson sampling. In _Proc. ICLR_, 2021.
* [43] H. Zhong, J. Hu, Y. Xue, T. Li, and L. Wang. Provably efficient exploration in quantum reinforcement learning with logarithmic worst-case regret. arXiv:2302.10796, 2023.
* [44] D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with UCB-based exploration. In _Proc. ICML_, pages 11492-11502. PMLR, 2020.

Analysis of the Weighted GP Posterior (3)

Here we show an alternative view of the weighted GP posterior distribution (3) in the RKHS feature space. The notations in this section will be extensively used in the theoretical analyses in the subsequent sections.

We denote the kernel \(k\) by \(k(x,x^{\prime})=\phi(x)^{\top}\phi(x^{\prime})\), in which \(\phi(x)\) is the potentially infinite-dimensional feature mapping of \(x\) in the RKHS of \(k\). Define \(\Phi_{s}\triangleq[\phi(x_{1}),\ldots,\phi(x_{s})]^{\top}\) which is an \(s\times\infty\) matrix. Recall that we have defined \(W_{s}\triangleq\text{diag}(\frac{1}{\epsilon_{s}^{2}},\ldots,\frac{1}{ \epsilon_{s}^{2}})\) which is an \(s\times s\) diagonal matrix. Define \(V_{s}\triangleq\lambda I+\sum_{\tau=1}^{s}\frac{1}{\epsilon_{s}^{2}}\phi(x_{ \tau})\phi(x_{\tau})^{\top}=\lambda I+\Phi_{s}^{\top}W_{s}\Phi_{s}\) which is an \(\infty\times\infty\) matrix, and let \(V_{0}\triangleq\lambda I\). With these notations, it can be easily verified that \(K_{s}=\Phi_{s}\Phi_{s}^{\top}=[k(x_{\tau},x_{\tau^{\prime}})]_{\tau,\tau^{ \prime}=1,\ldots,s}\), and \(\widetilde{K}_{s}=W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}=W_{s}^{1/2}K_ {s}W_{s}^{1/2}\). Moreover, we can also easily show that \(k_{s}(x)=\Phi_{s}\phi(x)=[k(x,x_{\tau})]_{\tau=1,\ldots,s}\), \(\widetilde{k}_{s}(x)=W_{s}^{1/2}\Phi_{s}\phi(x)=W_{s}^{1/2}k_{s}(x)\), and \(\widetilde{Y}_{s}=W_{s}^{1/2}Y_{s}\).

We start by deriving an alternative expression for the weighted GP posterior mean \(\widetilde{\mu}_{s}(x)\). To begin with, we have that

\[\Phi_{s}^{\top}W_{s}^{1/2}(\widetilde{K}_{s}+\lambda I) =\Phi_{s}^{\top}W_{s}^{1/2}(W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s }^{1/2}+\lambda I)\] \[=\Phi_{s}^{\top}W_{s}^{1/2}W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s }^{1/2}+\lambda\Phi_{s}^{\top}W_{s}^{1/2}\] \[=(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)\Phi_{s}^{\top}W_{s}^{1/2}\] \[=V_{s}\Phi_{s}^{\top}W_{s}^{1/2}.\]

Now we multiply both sides by \(V_{s}^{-1}\) on the left and by \((\widetilde{K}_{s}+\lambda I)^{-1}\) on the right, which leads to

\[V_{s}^{-1}\Phi_{s}^{\top}W_{s}^{1/2} =\Phi_{s}^{\top}W_{s}^{1/2}(\widetilde{K}_{s}+\lambda I)^{-1}\]

Now we can show that

\[\widetilde{\mu}_{s}(x)=\phi(x)^{\top}V_{s}^{-1}\Phi_{s}^{\top}W_{ s}Y_{s} =\phi(x)^{\top}\Phi_{s}^{\top}W_{s}^{1/2}(\widetilde{K}_{s}+ \lambda I)^{-1}W_{s}^{1/2}Y_{s}\] \[=\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s}+\lambda I)^{-1} \widetilde{Y}_{s}.\]

Next, we derive an alternative expression for the weighted GP posterior variance \(\widetilde{\sigma}_{s}^{2}(x)\). To begin with, we have that

\[\begin{split}\left(W_{s}^{-1}+\frac{1}{\lambda}\Phi_{s}\Phi_{s}^{ \top}\right)^{-1}&=\left(W_{s}^{-1/2}\left(I+\frac{1}{\lambda}W_{ s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}\right)W_{s}^{-1/2}\right)^{-1} \\ &=W_{s}^{1/2}\left(I+\frac{1}{\lambda}W_{s}^{1/2}\Phi_{s}\Phi_{s}^ {\top}W_{s}^{1/2}\right)^{-1}W_{s}^{1/2}\\ &=\lambda W_{s}^{1/2}\left(\lambda I+\widetilde{K}_{s}\right)^{-1} W_{s}^{1/2}.\end{split}\] (5)

This allows us to show that

\[\begin{split}\widetilde{\sigma}_{s}^{2}(x)&=\lambda \phi(x)^{\top}V_{s}^{-1}\phi(x)\\ &=\lambda\phi(x)^{\top}(\lambda I+\Phi_{s}^{\top}W_{s}\Phi_{s})^{ -1}\phi(x)\\ &\stackrel{{(a)}}{{=}}\lambda\phi(x)^{\top}\left( \frac{1}{\lambda}I-\frac{1}{\lambda}\Phi_{s}^{\top}(W_{s}^{-1}+\Phi_{s}\frac{1 }{\lambda}\Phi_{s}^{\top})^{-1}\Phi_{s}\frac{1}{\lambda}\right)\phi(x)\\ &=\phi(x)^{\top}\phi(x)-\phi(x)^{\top}\Phi_{s}^{\top}(W_{s}^{-1} +\frac{1}{\lambda}\Phi_{s}\Phi_{s}^{\top})^{-1}\Phi_{s}\frac{1}{\lambda}\phi(x) \\ &\stackrel{{(b)}}{{=}}\phi(x)^{\top}\phi(x)-\phi(x)^{ \top}\Phi_{s}^{\top}W_{s}^{1/2}\left(\lambda I+\widetilde{K}_{s}\right)^{-1}W_{ s}^{1/2}\Phi_{s}\phi(x)\\ &=k(x,x)-\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s}+\lambda I )^{-1}\widetilde{k}_{s}(x),\end{split}\]

in which we have used the matrix inversion lemma in step \((a)\), and used equation (5) in step \((b)\).

To summarize, we have derived the following alternative expressions for the weighted GP posterior distribution (3):

\[\begin{split}\widetilde{\mu}_{s}(x)&=\phi(x)^{\top}V_ {s}^{-1}\Phi_{s}^{\top}W_{s}Y_{s}=\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s}+ \lambda I)^{-1}\widetilde{Y}_{s},\\ \widetilde{\sigma}_{s}^{2}(x)&=\lambda\phi(x)^{\top}V_ {s}^{-1}\phi(x)=k(x,x)-\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s}+ \lambda I)^{-1}\widetilde{k}_{s}(x).\end{split}\] (6)Proof of Lemma 2

To begin with, we can lower-bound the total number of iterations (i.e., the total number of queries to the quantum oracles) in \(m\) stages by

\[\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta})\geq \sum_{s=1}^{m}\frac{1}{\epsilon_{s}}\geq\sqrt{\sum_{s=1}^{m}\frac{1}{\epsilon_{ s}^{2}}}.\] (7)

The first inequality follows since \(C_{1}>1\) (Lemma 1) and \(\log(\frac{2\overline{m}}{\delta})\geq\log(\frac{2}{\delta})\geq 1\) because we have chosen \(\delta\in(0,2/e]\). Suppose that \(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}>T^{2}\), then we have that \(\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta})>T\) which is a contradiction. Therefore, we have that

\[\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\leq T^{2}.\] (8)

## Appendix C Proof of Theorem 1

Here we prove the growth rate of \(\widetilde{\gamma}_{m}\), i.e., the weighted information gain. Recall that \(W_{m}=\text{diag}\left(\frac{1}{\epsilon_{s}^{2}},\frac{1}{\epsilon_{s}^{2}},\dots,\frac{1}{\epsilon_{m}^{2}}\right)\), and \(\widetilde{\gamma}_{m}=\frac{1}{2}\log\det\left(I+\lambda^{-1}\widetilde{K}_ {m}\right)\). Our proof here follows closely the proof of Theorem 3 from the work of [35]. We will omit the subscript \(m\) as long as the context is clear.

Denote by \(K_{m}=\Phi_{m}\Phi_{m}^{\top}=[k(x_{i},x_{j})]_{i,j=1,\dots,m}\) the \(m\times m\) covariance matrix, and define \(\widetilde{K}_{m}=W_{m}^{1/2}\Phi_{m}\Phi_{m}^{\top}W_{m}^{1/2}=W_{m}^{1/2}K_ {m}W_{m}^{1/2}\). Following Theorem 1 from [35], our kernel \(k\) can be denoted as \(k(x,x^{\prime})=\sum_{\tau=1}^{\infty}\lambda_{\tau}\psi_{\tau}(x)\psi_{\tau} (x^{\prime})\), in which \(\{\lambda_{\tau}\}_{\tau=1,\dots,\infty}\) and \(\{\psi_{\tau}\}_{\tau=1,\dots,\infty}\) represent, respectively, the eigenvalues and eigenfunctions of the kernel \(k\). We will make use of the projection onto a \(D\)-dimensional RKHS (\(D<\infty\)), which consists of the first \(D\) features corresponding to the \(D\) largest eigenvalues of the kernel \(k\). Specifically, define \(k_{P}\) as the \(D\)-dimensional projection in the RKHS of \(k\), and \(k_{O}\) corresponds to the orthogonal element such that \(k(\cdot,\cdot)=k_{P}(\cdot,\cdot)+k_{O}(\cdot,\cdot)\). That is, following Section 3.2 from the work of [35], define

\[k_{P}(x,x^{\prime})=\sum_{\tau=1}^{D}\lambda_{\tau}\psi_{\tau}(x)\psi_{\tau}(x ^{\prime}).\] (9)

We define \(K_{P}\triangleq[k_{P}(x_{i},x_{j})]_{i,j=1,\dots,m}\) and \(K_{O}\triangleq[k_{O}(x_{i},x_{j})]_{i,j=1,\dots,m}\). Similarly, define \(\widetilde{K}_{P}=W_{m}^{1/2}K_{P}W_{m}^{1/2}\) and \(\widetilde{K}_{O}=W_{m}^{1/2}K_{O}W_{m}^{1/2}\). This implies that \(K_{m}=K_{P}+K_{O}\) and that \(\widetilde{K}_{m}=\widetilde{K}_{P}+\widetilde{K}_{O}\).

Here to be consistent with [35], we use \(I_{m}\) to denote the \(m\times m\)-dimensional identity matrix. Denote by \(\widetilde{I}(\mathbf{y}_{m};f)\) the information gain about the function \(f\) from the observations \(\mathbf{y}_{m}=[y_{1},\dots,y_{m}]\). We have that

\[\widetilde{I}(\mathbf{y}_{m};f) =\frac{1}{2}\log\det\left(I_{m}+\frac{1}{\lambda}\widetilde{K}_{m}\right)\] (10) \[=\frac{1}{2}\log\det\left(I_{m}+\frac{1}{\lambda}\left(\widetilde{ K}_{P}+\widetilde{K}_{O}\right)\right)\] \[=\frac{1}{2}\log\det\left(\left(I_{m}+\frac{1}{\lambda}\widetilde{ K}_{P}\right)\left(I_{m}+\frac{1}{\lambda}\left(I_{m}+\frac{1}{\lambda} \widetilde{K}_{P}\right)^{-1}\widetilde{K}_{O}\right)\right)\] \[=\frac{1}{2}\log\det\left(I_{m}+\frac{1}{\lambda}\widetilde{K}_{ P}\right)+\frac{1}{2}\log\det\left(I_{m}+\frac{1}{\lambda}\left(I_{m}+\frac{1}{ \lambda}\widetilde{K}_{P}\right)^{-1}\widetilde{K}_{O}\right).\]

The last line follows because \(\det\left(AB\right)=\det\left(A\right)\det\left(B\right)\). In the following, we will separately upper-bound the two terms in (10).

We start by upper-bounding the first term in equation (10). Denote by \(\boldsymbol{\psi}_{D}(x)\) the \(D\)-dimensional feature vector corresponding to the first \(D\) principle features of the kernel \(k\) (i.e., the features corresponding to the largest \(D\) eigenvalues). Define \(\Psi_{m,D}\triangleq[\boldsymbol{\psi}_{D}(x_{1}),\ldots,\boldsymbol{\psi}_{D} (x_{m})]^{\top}\) which is an \(m\times D\) matrix. Denote by \(\Lambda_{D}=\text{diag}(\lambda_{1},\ldots,\lambda_{D})\) a \(D\times D\)-diagonal matrix whose diagonal entries consist of the largest \(D\) eigenvalues of the kernel \(k\) in descending order. With this definition, we have that \(K_{P}=\Psi_{m,D}\Lambda_{D}\Psi_{m,D}^{\top}\) and

\[\widetilde{K}_{P}=W_{m}^{1/2}\Psi_{m,D}\Lambda_{D}\Psi_{m,D}^{\top}W_{m}^{1/2}.\] (11)

Also define the gram matrix:

\[\widetilde{G}_{m}=\Lambda_{D}^{1/2}\Psi_{m,D}^{\top}W_{m}\Psi_{m,D}\Lambda_{D }^{1/2}.\] (12)

Based on these definitions, we have that

\[\det\left(I_{D}+\frac{1}{\lambda}\widetilde{G}_{m}\right)=\det\left(I_{m}+ \frac{1}{\lambda}\widetilde{K}_{P}\right),\] (13)

which follows from the Weinstein-Aronszajn identity: \(\det\left(I_{D}+AA^{\top}\right)=\det\left(I_{m}+A^{\top}A\right)\) for a \(D\times m\) matrix \(A\), and we have plugged in \(A=\frac{1}{\sqrt{\lambda}}\Lambda_{D}^{1/2}\Psi_{m,D}^{\top}W_{m}^{1/2}\). In addition, the following inequality will also be useful in the subsequent proof:

\[\left\|\Lambda_{D}^{1/2}\boldsymbol{\psi}_{D}(x)\right\|_{2}^{2}=\sum_{\tau=1 }^{D}\lambda_{\tau}\psi_{\tau}^{2}(x)=k_{P}(x,x)\leq k(x,x)\leq 1,\] (14)

in which we have made use of the definition of \(k_{P}\) from (9), and our assumption (w.l.o.g.) that \(k(x,x^{\prime})\leq 1,\forall x,x^{\prime}\) (Sec. 3.1). In the following, we will denote the trace of a matrix \(A\) by \(\text{Tr}(A)\). Next, we have that

\[\text{Tr}\left(I_{D}+\frac{1}{\lambda}\widetilde{G}_{m}\right) =D+\frac{1}{\lambda}\text{Tr}\left(\widetilde{G}_{m}\right)\] \[=D+\frac{1}{\lambda}\text{Tr}\left(\Lambda_{D}^{1/2}\Psi_{m,D}^{ \top}W_{m}\Psi_{m,D}\Lambda_{D}^{1/2}\right)\] \[\stackrel{{(a)}}{{=}}D+\frac{1}{\lambda}\text{Tr} \left(\Lambda_{D}^{1/2}\left[\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}} \boldsymbol{\psi}_{D}(x_{s})\boldsymbol{\psi}_{D}^{\top}(x_{s})\right]\Lambda _{D}^{1/2}\right)\] \[=D+\frac{1}{\lambda}\text{Tr}\left(\sum_{s=1}^{m}\frac{1}{ \epsilon_{s}^{2}}\Lambda_{D}^{1/2}\boldsymbol{\psi}_{D}(x_{s})\boldsymbol{ \psi}_{D}^{\top}(x_{s})\Lambda_{D}^{1/2}\right)\] (15) \[\stackrel{{(b)}}{{=}}D+\frac{1}{\lambda}\sum_{s=1}^{ m}\frac{1}{\epsilon_{s}^{2}}\text{Tr}\left(\boldsymbol{\psi}_{D}^{\top}(x_{s}) \Lambda_{D}^{1/2}\Lambda_{D}^{1/2}\boldsymbol{\psi}_{D}(x_{s})\right)\] \[=D+\frac{1}{\lambda}\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}} \boldsymbol{\psi}_{D}^{\top}(x_{s})\Lambda_{D}^{1/2}\Lambda_{D}^{1/2} \boldsymbol{\psi}_{D}(x_{s})\] \[=D+\frac{1}{\lambda}\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}} \Big{\|}\Lambda_{D}^{1/2}\boldsymbol{\psi}_{D}(x_{s})\Big{\|}_{2}^{2}\] \[\stackrel{{(c)}}{{\leq}}D+\frac{1}{\lambda}\sum_{s=1 }^{m}\frac{1}{\epsilon_{s}^{2}},\]

in which \((a)\) follows because \(\Psi_{m,D}^{\top}W_{m}\Psi_{m,D}=\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}} \boldsymbol{\psi}_{D}(x_{s})\boldsymbol{\psi}_{D}^{\top}(x_{s})\), \((b)\) has made use of the cyclic property of the trace operator, and \((c)\) follows from (14) above.

Also note that for a positive definite matrix \(P\in\mathbb{R}^{n\times n}\), we have that

\[\log\det(P)\leq n\log\frac{\text{Tr}(P)}{n}.\] (16)

Therefore, we have that

\[\log\det\left(I_{m}+\frac{1}{\lambda}\widetilde{K}_{P}\right) =\log\det\left(I_{D}+\frac{1}{\lambda}\widetilde{G}_{m}\right)\] (17) \[\leq D\log\frac{D+\frac{1}{\lambda}\sum_{s=1}^{m}\frac{1}{\epsilon _{s}^{2}}}{D}=D\log\left(1+\frac{1}{\lambda D}\sum_{s=1}^{m}\frac{1}{\epsilon _{s}^{2}}\right).\]

The first equality follows from (13), and the inequality results from (16) and (15).

Now we upper-bound the second term in equation (10). To begin with, we have that

\[\text{Tr}\left(\left(I_{m}+\frac{1}{\lambda}\widetilde{K}_{P}\right)^{-1} \widetilde{K}_{O}\right)\leq\text{Tr}\left(\widetilde{K}_{O}\right),\] (18)

which is because the matrix \(\left(I_{m}+\frac{1}{\lambda}\widetilde{K}_{P}\right)^{-1}\) is positive semi-definite (PSD) whose largest eigenvalue is upper-bounded by \(1\), and \(\text{Tr}(P_{1}P_{2})\leq\lambda_{P_{1}}\text{Tr}(P_{2})\) where \(P_{1}\) and \(P_{2}\) are PSD matrices and \(\lambda_{P_{1}}\) is the largest eigenvalue of \(P_{1}\). Next, define \(\delta_{D}\triangleq\sum_{\tau=D+1}^{\infty}\lambda_{\tau}\psi^{2}\) where \(|\phi_{\tau}(x)|\leq\psi,\forall x,\tau\). Then we immediately have that \(k_{O}(x,x^{\prime})=\sum_{\tau=D+1}^{\infty}\lambda_{\tau}\phi_{\tau}(x)\phi_ {\tau}(x^{\prime})\leq\delta_{D},\forall x,x^{\prime}\). Therefore, we have that

\[\text{Tr}\left(\widetilde{K}_{O}\right)=\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{ 2}}k_{O}(x_{s},x_{s})\leq\delta_{D}\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}.\] (19)

As a result, we have that

\[\text{Tr}\left(I_{m}+\frac{1}{\lambda}\left(I_{m}+\frac{1}{\lambda}\widetilde{ K}_{P}\right)^{-1}\widetilde{K}_{O}\right)\leq m+\frac{1}{\lambda}\delta_{D}\sum_{s= 1}^{m}\frac{1}{\epsilon_{s}^{2}},\] (20)

in which the inequality follows from (18) and (19).

Next, again making use of equation (16) and incorporating the upper bound on the trace from (20), we have that

\[\log\det\left(I_{m}+\frac{1}{\lambda}\left(I_{m}+\frac{1}{\lambda }\widetilde{K}_{P}\right)^{-1}\widetilde{K}_{O}\right) \leq m\log\frac{m+\frac{1}{\lambda}\delta_{D}\sum_{s=1}^{m}\frac{ 1}{\epsilon_{s}^{2}}}{m}\] (21) \[\leq m\log\left(1+\frac{\delta_{D}}{m\lambda}\sum_{s=1}^{m}\frac {1}{\epsilon_{s}^{2}}\right)\] \[\leq m\times\frac{\delta_{D}}{m\lambda}\sum_{s=1}^{m}\frac{1}{ \epsilon_{s}^{2}}\] \[\leq\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\right)\frac{ \delta_{D}}{\lambda}.\]

in which the second last step is because \(\log(1+z)\leq z,\forall z\in\mathbb{R}\).

Finally, combining the upper bounds from 17 and 21, equation (10) can be upper-bounded by

\[\widetilde{I}(\mathbf{y}_{m};f)\leq\frac{1}{2}D\log\left(1+\frac{\sum_{s=1}^{ m}\frac{1}{\epsilon_{s}^{2}}}{D\lambda}\right)+\frac{1}{2}\left(\sum_{s=1}^{m} \frac{1}{\epsilon_{s}^{2}}\right)\frac{\delta_{D}}{\lambda}.\] (22)

Therefore, we have that

\[\widetilde{\gamma}_{m}=\widetilde{I}(\mathbf{y}_{m};f)=\frac{1}{2}\log\det \left(I_{m}+\frac{1}{\lambda}\widetilde{K}_{m}\right)\leq\frac{1}{2}D\log(1+ \frac{\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}}{D\lambda})+\frac{1}{2}\left( \sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\right)\frac{\delta_{D}}{\lambda}.\] (23)Also recall that for the standard unweighted maximum information gain \(\gamma_{T}\), we have that [35]

\[\gamma_{T}=\frac{1}{2}\log\det\left(I_{T}+\frac{1}{\lambda}K_{T}\right)\leq\frac {1}{2}D\log(1+\frac{T}{D\lambda})+\frac{1}{2}T\frac{\delta_{D}}{\lambda}.\] (24)

Therefore, the upper bound on the weighted information gain \(\widetilde{\gamma}_{m}\) is obtained by replacing the \(T\) from the upper bound on standard maximum information gain \(\gamma_{T}\) by \(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\). This is formalized by the following Theorem.

**Theorem 8**.: _[More formal statement of Theorem 1 (a)] Given \(\mathcal{X}_{m}\triangleq\{x_{1},\ldots,x_{m}\}\), then we have that_

\[\widetilde{\gamma}_{m}\leq\frac{1}{2}D\log(1+\frac{\sum_{s=1}^{m}\frac{1}{ \epsilon_{s}^{2}}}{D\lambda})+\frac{1}{2}\left(\sum_{s=1}^{m}\frac{1}{ \epsilon_{s}^{2}}\right)\frac{\delta_{D}}{\lambda},\]

\[\gamma_{\sum_{s=1}^{m}1/\epsilon_{s}^{2}}\leq\frac{1}{2}D\log(1+\frac{\sum_{ s=1}^{m}\frac{1}{\epsilon_{s}^{2}}}{D\lambda})+\frac{1}{2}\left(\sum_{s=1}^{m} \frac{1}{\epsilon_{s}^{2}}\right)\frac{\delta_{D}}{\lambda}.\]

Lastly, note that it has been shown that \(\gamma_{T}=\mathcal{O}(d\log T)\) for the linear kernel and \(\gamma_{T}=\mathcal{O}((\log T)^{d+1})\) for the SE kernel, Therefore, plugging in Lemma 2, which allows us to upper-bound \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\) by \(T^{2}\), leads to Corollary 1.

## Appendix D Proof of Theorem 2

Our proof here follows the outline of the proof of Lemma 2 from the work of [38]. To begin with, we show that \(\text{det}(V_{\tau+1})=2\text{det}(V_{\tau}),\forall\tau=0,\ldots,m-1\), i.e., the amount of collected information is doubled after every stage.

\[\text{det}(V_{\tau+1}) =\text{det}\left(\lambda I+\sum_{j=1}^{\tau+1}\frac{1}{\epsilon_{ j}^{2}}\phi(x_{j})\phi(x_{j})^{\top}\right)\] (25) \[=\text{det}\left(\lambda I+\sum_{j=1}^{\tau}\frac{1}{\epsilon_{ j}^{2}}\phi(x_{j})\phi(x_{j})^{\top}+\frac{1}{\epsilon_{\tau+1}^{2}}\phi(x_{ \tau+1})\phi(x_{\tau+1})^{\top}\right)\] \[=\text{det}\left(V_{\tau}+\frac{1}{\epsilon_{\tau+1}^{2}}\phi(x _{\tau+1})\phi(x_{\tau+1})^{\top}\right)\] \[=\text{det}\left(V_{\tau}\right)\text{det}\left(I+\frac{1}{ \epsilon_{\tau+1}^{2}}V_{\tau}^{-1/2}\phi(x_{\tau+1})\phi(x_{\tau+1})^{\top}V_ {\tau}^{-1/2}\right)\] \[\stackrel{{(a)}}{{=}}\text{det}\left(V_{\tau}\right) \left(1+\frac{1}{\epsilon_{\tau+1}^{2}}\phi(x_{\tau+1})^{\top}V_{\tau}^{-1/2}V_ {\tau}^{-1/2}\phi(x_{\tau+1})\right)\] \[=\text{det}\left(V_{\tau}\right)\left(1+\frac{1}{\epsilon_{\tau+ 1}^{2}}\big{\|}\phi(x_{\tau+1})\big{\|}_{V_{\tau}^{-1}}^{2}\right)\] \[\stackrel{{(b)}}{{=}}\text{det}\left(V_{\tau}\right) \left(1+\frac{\widetilde{\sigma}_{\tau}^{2}(x_{\tau+1})/\lambda}{\widetilde{ \sigma}_{\tau}^{2}(x_{\tau+1})/\lambda}\right)\] \[=2\text{det}\left(V_{\tau}\right).\]

Step \((a)\) has made use of the matrix determinant lemma: \(\det(I+ab^{\top})=\det(I+a^{\top}b)\), and \((b)\) follows from the expression of the weighted GP posterior variance (6) and our choice of \(\widetilde{\sigma}_{\tau}(x_{\tau+1})/\sqrt{\lambda}\) (line 3 of Algo. 1). An immediate consequence of (25) is that \(\text{det}(V_{m})=2^{m}\text{det}(V_{0})\). Recall we have defined earlier that \(V_{0}=\lambda I\) (App. A).

Recall that following the notations of App. A, we have that \(V_{m}=\lambda I+\Phi_{m}^{\top}W_{m}\Phi_{m}\). Next, we can also show that

\[\begin{split}\log\frac{\text{det}(V_{m})}{\text{det}(V_{0})}& =\log\frac{\text{det}(\lambda I+\Phi_{m}^{\top}W_{m}\Phi_{m})}{ \text{det}(\lambda I)}\\ &=\log\text{det}\left(I+\frac{1}{\lambda}(\Phi_{m}^{\top}W_{m}^{1 /2})(W_{m}^{1/2}\Phi_{m})\right)\\ &\stackrel{{(a)}}{{=}}\log\text{det}\left(I+\frac{1} {\lambda}W_{m}^{1/2}\Phi_{m}\Phi_{m}^{\top}W_{m}^{1/2}\right)\\ &=\log\text{det}\left(I+\frac{1}{\lambda}\widetilde{K}_{m} \right)\\ &=2\widetilde{\gamma}_{m},\end{split}\] (26)

in which step \((a)\) has again made use of the Weinstein-Aronszajn identity (similar to (13)), and the last two equalities have simply plugged in the expressions for \(\widetilde{K}_{s}=W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}\) (App. A), and \(\widetilde{\gamma}_{m}=\frac{1}{2}\log\det(I+\lambda^{-1}\widetilde{K}_{m})\) (Sec. 5.1).

According to Corollary 1, for the linear kernel, we have that there exists an absolute constant \(C>0\) such that \(\widetilde{\gamma}_{m}\leq Cd\log\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2 }}\right)\). Combining equations (25) and (26), we can show that

\[m\log 2=\log\frac{\text{det}(V_{m})}{\text{det}(V_{0})}=2\widetilde{\gamma}_{m} \leq 2Cd\log\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\right).\] (27)

This allows us to show that

\[\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\geq(2^{m})^{1/(2Cd)}.\] (28)

Note that in every stage \(s\), we query the quantum oracle for \(\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta})\) times, in which \(C_{1}>1\), \(\delta\in(0,2/e]\) and \(\overline{m}\) is an upper bound on the total number of stages. An immediate consequence is that \(\log(\frac{2\overline{m}}{\delta})\geq\log(\frac{2}{\delta})\geq 1\). Therefore, the total number of iterations can be analyzed as

\[\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta}) \geq\sum_{s=1}^{m}\frac{1}{\epsilon_{s}}\geq\sqrt{\sum_{s=1}^{m}\frac{1}{ \epsilon_{s}^{2}}}\geq\sqrt{(2^{m})^{1/(2Cd)}}.\] (29)

Now we derive an upper bound on \(m\) by contradiction. Suppose \(m>\frac{2Cd}{\log 2}\log T\), this immediately implies that

\[\sqrt{(2^{m})^{1/(2Cd)}}>T.\] (30)

This equation, combined with (29), implies that \(\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta})>T\), which is a contradiction. Therefore, we have that

\[m\leq\frac{2}{\log 2}Cd\log T=\mathcal{O}(d\log T).\] (31)

That is, for the linear kernel, our algorithm runs for at most \(\mathcal{O}(d\log T)\) stages, which matches the upper bound on the total number of stages for the quantum linear UCB (Q-LinUCB) algorithm, as proved by Lemma 2 from the work of [38].

Next, for the SE kernel, Corollary 1 shows that there exists an absolute constant \(C>0\) such that \(\widetilde{\gamma}_{m}\leq C\log^{d+1}\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{ s}^{2}}\right)\). Again combining equations (25) and (26), we can show that

\[m\log 2=\log\frac{\text{det}(V_{m})}{\text{det}(V_{0})}=2\widetilde{\gamma}_{m} \leq 2C\log^{d+1}\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\right).\] (32)This allows us to show that

\[\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\geq\exp\left(\left(\frac{m\log 2}{2C} \right)^{1/(d+1)}\right).\] (33)

Now again we derive an upper bound on \(m\) by contradiction. Suppose \(m>\frac{2C}{\log 2}\left(2\log T\right)^{d+1}\), then this implies that

\[\sqrt{\exp\left(\left(\frac{m\log 2}{2C}\right)^{1/(d+1)}\right)}>T.\] (34)

This, combined with (29), implies that \(\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta})>T\) which is a contradiction. Therefore

\[m\leq\frac{2C}{\log 2}\left(2\log T\right)^{d+1}=\mathcal{O}\left(\left( \log T\right)^{d+1}\right)\] (35)

for the SE kernel. This completes the proof.

## Appendix E Proof of Theorem 3

Our proof here follows closely the proof of Theorem 2 from [9]. Denote by \(\zeta_{s}\) the observation noise for the observation in stage \(s\): \(y_{s}=f(x_{s})+\zeta_{s}\). Define \(\zeta_{1:s}\triangleq[\zeta_{1},\ldots,\zeta_{s}]^{\top}\) which is an \(s\times 1\) vector. Denote by \(\mathcal{F}_{s}\) the \(\sigma\)-algebra: \(\mathcal{F}_{s}=\{x_{1},\ldots,x_{s+1},\zeta_{1},\ldots,\zeta_{s}\}\). With this definition, we have that \(x_{s}\) is \(\mathcal{F}_{s-1}\)-measurable, and \(\zeta_{s}\) is \(\mathcal{F}_{s}\)-measurable. Note that we have used \(\phi(\cdot)\) to denote the RKHS feature map of the kernel \(k\): \(k(x,x^{\prime})=\phi(x)^{\top}\phi(x^{\prime})\), and here we let \(\phi(x)=k(x,\cdot)\). Then using the reproducing property, we have that \(f(x)=\langle\phi(x),f\rangle_{k}=\phi(x)^{\top}f\), in which the inner product denotes the inner product induced by the RKHS of \(k\).

To begin with, we have that

\[\Phi_{s}^{\top}W_{s}^{1/2}(W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+ \lambda I)^{-1}=(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1}\Phi_{s}^{\top} W_{s}^{1/2},\] (36)

which follows from noting that \((\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)\Phi_{s}^{\top}W_{s}^{1/2}=\Phi_{s}^{ \top}W_{s}^{1/2}(W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+\lambda I)\), and then multiplying by \((\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1}\) on the left and by \((W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+\lambda I)^{-1}\) on the right.

Now let \(f_{s}\triangleq[f(x_{k})]_{k=1,\ldots,s}^{\top}=\Phi_{s}f\) which is an \(s\times 1\)-dimensional vector, and \(\widetilde{f}_{s}\triangleq W_{s}^{1/2}f_{s}=W_{s}^{1/2}\Phi_{s}f\). Next, we can prove that

\[\begin{split}|f(x)&-\widetilde{k}_{s}^{\top}(x)( \widetilde{K}_{s}+\lambda I)^{-1}\widetilde{f}_{s}|=|f(x)-\phi(x)^{\top}\Phi_ {s}^{\top}W_{s}^{1/2}\left(W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+ \lambda I\right)^{-1}\widetilde{f}_{s}|\\ &\overset{(a)}{=}|f(x)-\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_ {s}+\lambda I)^{-1}\Phi_{s}^{\top}W_{s}^{1/2}W_{s}^{1/2}\Phi_{s}f|\\ &=|\phi(x)^{\top}f-\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_{s}+ \lambda I)^{-1}\Phi_{s}^{\top}W_{s}\Phi_{s}f|\\ &=|\lambda\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I )^{-1}f|\\ &\leq\|f\|_{k}\left\|\lambda\phi(x)^{\top}(\Phi_{s}^{\top}W_{s} \Phi_{s}+\lambda I)^{-1}\right\|_{k}\\ &=\|f\|_{k}\left\|\lambda\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_ {s}+\lambda I)^{-1}\lambda I(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1} \phi(x)\right.\\ &\leq\|f\|_{k}\left\|\lambda\phi(x)^{\top}(\Phi_{s}^{\top}W_{s} \Phi_{s}+\lambda I)^{-1}\phi(x)\right.\\ &\overset{(b)}{\leq}B\widetilde{\sigma}_{s}(x),\end{split}\] (37)in which step \((a)\) has made use of (36), and step \((b)\) follows from our assumption that \(\left\|f\right\|_{k}\leq B\) (Sec. 3.1) and the expression for the weighted GP posterior variance (6). Next, we have that

\[\begin{split}\|\widetilde{k}_{s}^{\top}(x)&(\widetilde {K}_{s}+\lambda I)^{-1}W_{s}^{1/2}\zeta_{1:s}|=|\phi(x)^{\top}\Phi_{s}^{\top}W_ {s}^{1/2}\left(W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+\lambda I\right)^ {-1}W_{s}^{1/2}\zeta_{1:s}|\\ &\overset{(a)}{=}|\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_{s}+ \lambda I)^{-1}\Phi_{s}^{\top}W_{s}^{1/2}W_{s}^{1/2}\zeta_{1:s}|\\ &=|\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1} \Phi_{s}^{\top}W_{s}\zeta_{1:s}|\\ &\leq\!\!\left\|(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1/2} \phi(x)\right\|_{k}\!\!\left\|(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1/2} \Phi_{s}^{\top}W_{s}\zeta_{1:s}\right\|_{k}\\ &=\sqrt{\phi(x)^{\top}(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{ -1}\phi(x)}\sqrt{(\Phi_{s}^{\top}W_{s}\zeta_{1:s})^{\top}(\Phi_{s}^{\top}W_{s} \Phi_{s}+\lambda I)^{-1}\Phi_{s}^{\top}W_{s}\zeta_{1:s}}\\ &=\frac{1}{\sqrt{\lambda}}\widetilde{\sigma}_{s}(x)\sqrt{\zeta_{1 :s}^{\top}W_{s}\Phi_{s}(\Phi_{s}^{\top}W_{s}\Phi_{s}+\lambda I)^{-1}\Phi_{s}^{ \top}W_{s}^{1/2}W_{s}^{1/2}\zeta_{1:s}}\\ &\overset{(b)}{=}\frac{1}{\sqrt{\lambda}}\widetilde{\sigma}_{s}( x)\sqrt{\zeta_{1:s}^{\top}W_{s}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}(W_{s}^{1/2} \Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+\lambda I)^{-1}W_{s}^{1/2}\zeta_{1:s}}\\ &=\frac{1}{\sqrt{\lambda}}\widetilde{\sigma}_{s}(x)\sqrt{\zeta_ {1:s}^{\top}W_{s}^{1/2}W_{s}^{1/2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}(W_{s}^{1/ 2}\Phi_{s}\Phi_{s}^{\top}W_{s}^{1/2}+\lambda I)^{-1}W_{s}^{1/2}\zeta_{1:s}}\\ &=\frac{1}{\sqrt{\lambda}}\widetilde{\sigma}_{s}(x)\sqrt{\zeta_ {1:s}^{\top}W_{s}^{1/2}\widetilde{K}_{s}(\widetilde{K}_{s}+\lambda I)^{-1}W_{s }^{1/2}\zeta_{1:s}},\end{split}\] (38)

in which \((a)\) and \((b)\) follow from (36). Since \(\widetilde{\mu}_{s}(x)=\widetilde{k}_{s}^{\top}(x)(\widetilde{K}_{s}+\lambda I )^{-1}\widetilde{Y}_{s}\) and \(\widetilde{Y}_{s}=\widetilde{f}_{s}+W_{s}^{1/2}\zeta_{1:s}\), then the two equations above combine to tell us that

\[\begin{split}|f(x)-\widetilde{\mu}_{s}(x)|&\leq \widetilde{\sigma}_{s}(x)\left(B+\frac{1}{\sqrt{\lambda}}\sqrt{\zeta_{1:s}^{ \top}W_{s}^{1/2}\widetilde{K}_{s}(\widetilde{K}_{s}+\lambda I)^{-1}W_{s}^{1/ 2}\zeta_{1:s}}\right)\\ &\leq\widetilde{\sigma}_{s}(x)\left(B+\sqrt{\zeta_{1:s}^{\top}W_ {s}^{1/2}\widetilde{K}_{s}(\widetilde{K}_{s}+\lambda I)^{-1}W_{s}^{1/2}\zeta_ {1:s}}\right),\end{split}\] (39)

in which the second inequality follows since \(\lambda=1+\eta>1\). Again following [9], we apply a few more steps of transformations. Note that for an invertible \(K\), we have that \(K(K+I)^{-1}=\left((K+I)K^{-1}\right)^{-1}=\left(I+K^{-1}\right)^{-1}\). Substituting \(K=\widetilde{K}_{s}+\eta I\), we have that

\[(\widetilde{K}_{s}+\eta I)(\widetilde{K}_{s}+(\eta+1)I)^{-1}=\left(I+( \widetilde{K}_{s}+\eta I)^{-1}\right)^{-1}.\]

Next, we have that

\[\begin{split}\zeta_{1:s}^{\top}W_{s}^{1/2}\widetilde{K}_{s}( \widetilde{K}_{s}+\lambda I)^{-1}W_{s}^{1/2}\zeta_{1:s}&\leq\zeta_ {1:s}^{\top}W_{s}^{1/2}(\widetilde{K}_{s}+\eta I)(\widetilde{K}_{s}+(1+\eta) I)^{-1}W_{s}^{1/2}\zeta_{1:s}\\ &=\zeta_{1:s}^{\top}W_{s}^{1/2}\left((\widetilde{K}_{s}+\eta I)^ {-1}+I\right)^{-1}W_{s}^{1/2}\zeta_{1:s}\\ &=\!\!\left\|W_{s}^{1/2}\zeta_{1:s}\right\|_{((\widetilde{K}_{s}+ \eta I)^{-1}+I)^{-1}}^{2}.\end{split}\]

This allows us to further re-write (39) as

\[|f(x)-\widetilde{\mu}_{s}(x)|\leq\widetilde{\sigma}_{s}(x)\left(B+\left\|W_{s} ^{1/2}\zeta_{1:s}\right\|_{((\widetilde{K}_{s}+\eta I)^{-1}+I)^{-1}}\right).\] (40)

Recall that \(W_{s}=\text{diag}\left(\frac{1}{\epsilon_{1}^{2}},\frac{1}{\epsilon_{2}^{2}}, \ldots,\frac{1}{\epsilon_{2}^{2}}\right)\), therefore, \(W_{s}^{1/2}\zeta_{1:s}=[\zeta_{k}\frac{1}{\epsilon_{k}}]_{k=1,\ldots,s}^{\top}\). Of note, since we have that \(|f(x_{s})-y_{s}|\leq\epsilon_{s},\forall s=1,\ldots,m\) (with high probability, refer to the beginning of Sec. 5), so, the noise \(\zeta_{s}=y_{s}-f(x_{s})\) is bounded: \(|\zeta_{s}|\leq\epsilon_{s},\forall s=1,\ldots,m\). In other words, \(\zeta_{s}\) is \(\epsilon_{s}\)-sub-Gaussian.

To begin with, note that \(\epsilon_{k}\) is \(\mathcal{F}_{k-1}\)-measurable. This can be seen recursively: conditioned on \(\{x_{1}\}\), \(\epsilon_{1}=\widetilde{\sigma}_{0}(x_{1})/\sqrt{\lambda}\) is a deterministic constant (i.e., predictable); conditioned on \(\{x_{1},x_{2}\}\), \(\widetilde{\sigma}_{1}(\cdot)\) is predictable because it depends on \(x_{1}\) and \(\epsilon_{1}\) (via the weight \(\frac{1}{\epsilon_{1}^{2}}\)), and hence \(\epsilon_{2}=\widetilde{\sigma}_{1}(x_{2})/\sqrt{\lambda}\) is predictable conditioned on \(\{x_{1},x_{2}\}\). By induction, conditioned on \(\{x_{1},\ldots,x_{k}\}\), \(\epsilon_{k}=\widetilde{\sigma}_{k-1}(x_{k})/(\sqrt{\lambda})\) is predictable. Therefore, \(\epsilon_{k}\) is \(\mathcal{F}_{k-1}\)-measurable, because \(\mathcal{F}_{k-1}=\{x_{1},\ldots,x_{k},\zeta_{1},\ldots,\zeta_{k-1}\}\).

Next, note that since \(\zeta_{k}\) is \(\mathcal{F}_{k}\)-measurable, we have that \(\frac{\zeta_{k}}{\epsilon_{k}}\) is \(\mathcal{F}_{k}\)-measurable. Moreover, conditioned on \(\mathcal{F}_{k-1}\) (i.e., \(\epsilon_{k}\) is a predictable), \(\frac{\zeta_{k}}{\epsilon_{k}}\) is \(1\)-sub-Gaussian, because \(\zeta_{k}\) is \(\epsilon_{k}\)-sub-Gaussian as discussed above. To summarize, we have that _every element \(\frac{\zeta_{k}}{\epsilon_{k}}\) of this noise vector \(W_{s}^{1/2}\zeta_{1:s}\) is (a) \(\mathcal{F}_{k}\)-measurable and (b) 1-sub-Gaussian conditionally on \(\mathcal{F}_{k-1}\)_. Therefore, we can apply Theorem 1 of [9] to 1-sub-Gaussian noise, to show that

\[\Big{\|}W_{s}^{1/2}\zeta_{1:s}\Big{\|}_{((\widetilde{K}_{s}+\eta I)^{-1}+I)^{- 1}}\leq\sqrt{2\log\frac{\sqrt{\text{det}((1+\eta)I+\widetilde{K}_{s})}}{ \delta}}\] (41)

with probability of at least \(1-\delta\). When applying Theorem 1 of [9], we have also replaced the original unweighted covariance matrix \(K_{s}\) by the corresponding weighted covariance matrix \(\widetilde{K}_{s}=W_{s}^{1/2}K_{s}W_{s}^{1/2}\). This is possible because every \(\epsilon_{k}\) is \(\mathcal{F}_{k-1}\)-measurable. More concretely, in the proof of Theorem 1 of [9], when defining the super-martingale \(\{M_{t}\}_{t}\) which is conditioned on \(\mathcal{F}_{\infty}\), we only need to replace the covariance matrix \(K_{t}\) (for the multivariate Gaussian distribution of \(h(x_{1}),\ldots,h(x_{t})\)) by \(\widetilde{K}_{t}=W_{t}^{1/2}K_{t}W_{t}^{1/2}\), because both the original \(K_{t}\) and our \(\widetilde{K}_{t}\) only require conditioning on \(\{x_{1},\ldots,x_{t}\}\).

Combining the two equations above allows us to show that

\[|f(x)-\widetilde{\mu}_{s}(x)|\leq\widetilde{\sigma}_{s}(x)\Big{(}B+\sqrt{2 \log\frac{\sqrt{\text{det}((1+\eta)I+\widetilde{K}_{s})}}{\delta}}\Big{)}\] (42)

with probability of \(\geq 1-\delta\). Next, we can further upper-bound the log determinant term:

\[\begin{split}\log\det\Big{(}(1+\eta)I+\widetilde{K}_{s}\Big{)}& \stackrel{{(a)}}{{\leq}}s\log(1+\eta)+\log\det\left(I+ \frac{1}{1+\eta}\widetilde{K}_{s}\right)\\ &\stackrel{{(b)}}{{\leq}}\log\det\left(I+\frac{1}{ \lambda}\widetilde{K}_{s}\right)+s\eta\\ &\stackrel{{(c)}}{{=}}2\widetilde{\gamma}_{s}+s\eta\\ &\stackrel{{(d)}}{{\leq}}2\widetilde{\gamma}_{s}+2, \end{split}\] (43)

in which \((a)\) follows since \(\det(AB)=\det(A)\det(B)\) and \(\det(cA)\leq c^{s}\det(A)\) for a scalar \(c\) and an \(s\times s\)-matrix \(A\), \((b)\) has made use of \(\log(1+z)\leq z\), \((c)\) has made use of the definition of \(\gamma_{s}\), and \((d)\) follows because \(\eta=2/T\) and \(s\leq T\). This eventually allows us to show that

\[\begin{split}|f(x)-\widetilde{\mu}_{s}(x)|&\leq \widetilde{\sigma}_{s}(x)\Big{(}B+\sqrt{2\log\frac{\sqrt{\text{det}((1+\eta)I+ \widetilde{K}_{s})}}{\delta}}\Big{)}\\ &\leq\widetilde{\sigma}_{s}(x)\Big{(}B+\sqrt{2(\widetilde{\gamma}_ {s}+1+\log(1/\delta))}\Big{)},\end{split}\] (44)

Of note, although the work of [15] has also adopted weighted GP regression in a similar way to our (3), our proof technique here cannot be applied in their analysis. This is because the work of [15] has chosen the weight to be \(w_{s}=\eta^{-s}\) for \(\eta\in(0,1)\) and hence \(W_{s}=\text{diag}[w_{1},\ldots,w_{s}]\). Therefore, every element of the scaled noise vector \(W_{s}^{1/2}\zeta_{1:s}\) (i.e., \(\zeta_{k}\sqrt{w_{k}}\)) is not guaranteed to be sub-Gaussian. As a result, this makes it infeasible for them to apply the self-normalizing concentration inequality from Theorem 1 of [9], i.e., our (41) above does not hold in their case. The work of [15] has come up with other novel proof techniques suited to their setting. Therefore, our proof here only works in our problem setting of quantum BO.

Proof of Theorem 4

To begin with, we can show that

\[\begin{split} r_{s}&=f(x^{*})-f(x_{s})\stackrel{{ (a)}}{{\leq}}\widetilde{\mu}_{s-1}(x^{*})+\beta_{s}\widetilde{\sigma}_{s-1}(x^{ *})-f(x_{s})\\ &\stackrel{{(b)}}{{\leq}}\widetilde{\mu}_{s-1}(x_{s} )+\beta_{s}\widetilde{\sigma}_{s-1}(x_{s})-f(x_{s})\\ &\stackrel{{(c)}}{{\leq}}2\beta_{s}\widetilde{\sigma }_{s-1}(x_{s})\stackrel{{(d)}}{{=}}2\beta_{s}\epsilon_{s}\sqrt{ \lambda},\end{split}\] (45)

in which \((a)\) and \((c)\) follow from Theorem 3, \((b)\) results from our policy for selecting \(x_{s}\) (line 2 of Algo. 1), and \((d)\) follows from our design of \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\) (line 3 of Algo. 1). As a result, the total regret in stage \(s\) can be upper bounded by:

\[\frac{C_{1}}{\epsilon_{s}}\log\frac{2\overline{m}}{\delta}\times 2\beta_{s} \epsilon_{s}\sqrt{\lambda}=2C_{1}\beta_{s}\sqrt{\lambda}\log\frac{2\overline{ m}}{\delta}.\] (46)

Then an upper bound on the total cumulative regret can be obtained by summing up the regrets from all \(m\) stages:

\[R_{T}\leq\sum_{s=1}^{m}2C_{1}\beta_{s}\sqrt{\lambda}\log\frac{2\overline{m}}{ \delta}\leq m2C_{1}\beta_{m}\sqrt{\lambda}\log\frac{2\overline{m}}{\delta}= \mathcal{O}\left(m\beta_{m}\log\frac{\overline{m}}{\delta}\right).\] (47)

For the linear kernel, recall from Theorem 3 and Corollary 1 that \(\beta_{m}=\mathcal{O}(\sqrt{\widetilde{\gamma}_{m-1}+\log(1/\delta)})= \mathcal{O}(\sqrt{d\log T+\log(1/\delta)})\). Also recall that Theorem 2 tells us that \(m\leq\overline{m}=\mathcal{O}(d\log T)\). Therefore, for the linear kernel,

\[R_{T}=\mathcal{O}\left(d\log T\sqrt{d\log T+\log(1/\delta)}\log\frac{d\log T} {\delta}\right)=\mathcal{O}\left(d^{3/2}\log^{3/2}T\log(d\log T)\right),\] (48)

in which we have ignored the dependency on \(\log(1/\delta)\) in the second step to get a cleaner expression.

For the SE kernel, recall from Theorem 3 and Corollary 1 that \(\beta_{m}=\mathcal{O}(\sqrt{\widetilde{\gamma}_{m-1}+\log(1/\delta)})= \mathcal{O}(\sqrt{(\log T)^{d+1}+\log(1/\delta)})\). Also recall that Theorem 2 tells us that \(m\leq\overline{m}=\mathcal{O}((\log T)^{d+1})\). Therefore, for the linear kernel,

\[\begin{split} R_{T}&=\mathcal{O}\left((\log T)^{d +1}\sqrt{(\log T)^{d+1}+\log(1/\delta)}\log\frac{(\log T)^{d+1}}{\delta}\right) \\ &=\mathcal{O}\left((\log T)^{3(d+1)/2}\log((\log T)^{d+1})\right),\end{split}\] (49)

in which we have again ignored the dependency on \(\log(1/\delta)\) to obtain a cleaner expression.

The error probability of \(\delta\) results from (a) conditioning on the event that \(|f(x_{s})-y_{s}|\leq\epsilon_{s},\forall s=1,\ldots,m\) which holds with probability of at least \(1-\delta/2\), and (b) Theorem 3 which also holds with probability of at least \(1-\delta/2\).

## Appendix G Proof of Theorem 5

Consistent with the analysis of Q-LinUCB [38], in order to apply the theoretical guarantee provided by QMC (Lemma 1) for noise with bounded variance, here we need to assume that the noise variance is not too small: We assume that \(\sigma>1/3\). We have chosen the value of \(1/3\) just for convenience, in fact, any \(\sigma\) larger than \(1/4\) can be used in our analysis here because these different values will only alter the constants in our regret upper bound which are absorbed by the \(\mathcal{O}\).

Note that \(\epsilon_{s}=\widetilde{\sigma}_{s-1}(x_{s})/\sqrt{\lambda}\leq 1\), which follows because \(\widetilde{\sigma}_{s-1}^{2}(x_{s})\leq 1\) (which can be easily seen from (3) and our assumption that \(k(x,x^{\prime})\leq 1\)) and \(\lambda>1\). As a result of the assumption of \(\sigma>1/3\), we have that \(\sigma>1/3>2^{\sqrt{2}}/8\). This, combined with \(\epsilon_{s}\leq 1\), allows us to show that \(\log_{2}^{3/2}\left(\frac{8\sigma}{\epsilon_{s}}\right)\geq 2^{3/4}\) and that \(\log_{2}(\log_{2}\frac{8\sigma}{\epsilon_{s}})\geq 1/2\).

Next, we derive an upper bound on \(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\) which is analogous to (8). Note that in the case of a noise with bounded variance, the number of queries to the quantum oracle in stage \(s\) is given by \(N_{\epsilon_{s}}=\frac{C_{2}\sigma}{\epsilon_{s}}\log_{2}^{3/2}\left(\frac{8 \sigma}{\epsilon_{s}}\right)\log_{2}(\log_{2}\frac{8\sigma}{\epsilon_{s}}) \log\frac{2\overline{m}}{\delta}\) with \(C_{2}>1\). As a result, we have the following inequality which is in a similar spirit to (7)

\[\sum_{s=1}^{m}\frac{C_{2}\sigma}{\epsilon_{s}}\log_{2}^{3/2}\left(\frac{8 \sigma}{\epsilon_{s}}\right)\log_{2}(\log_{2}\frac{8\sigma}{\epsilon_{s}}) \log\frac{2\overline{m}}{\delta}\geq\sum_{s=1}^{m}\frac{1}{3}\frac{1}{ \epsilon_{s}}2^{3/4}2^{-1}\geq\frac{1}{3\times 2^{1/4}}\sqrt{\sum_{s=1}^{m} \frac{1}{\epsilon_{s}^{2}}}\] (50)

Now suppose that \(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}>9\sqrt{2}T^{2}\), then we have that \(\sum_{s=1}^{m}N_{\epsilon_{s}}>T\) which is a contradiction. Therefore, we have that

\[\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\leq 9\sqrt{2}T^{2}.\] (51)

Next, note that Theorem 1 is unaffected since its proof does not depend on the noise. Moreover, Corollary 1 also stays the same because it has only made use of (8) which gives an upper bound on \(\sum_{s=1}^{m}1/\epsilon_{s}^{2}\), and compared to (8), its counterpart (51) in the analysis here only introduces an extra constant factor which is absorbed by the bit \(\mathcal{O}\) notation. Similarly, Theorem 2 is also unaltered for a similar reason, i.e., only an extra constant factor of \(3\times 2^{1/4}\) will be introduced to the right hand side of (30) and (34), which is absorbed by the big \(\mathcal{O}\) notation.

Theorem 3 is also unaltered since its proof does not depend on the type of noise. In particular, the key underlying reason why it is unaffected is because for both types of noise, given a particular \(\epsilon_{s}\), we choose the corresponding number \(N_{\epsilon_{s}}\) of queries (to the quantum oracle) depending on the type of noise so that the error guarantee of \(|y_{s}-f(x_{s})|\leq\epsilon_{s}\) is satisfied. This allows us to make use of the self-normalizing bound from [9] for \(1\)-sub-Gaussian noise.

Lastly, we need to modify the proof of Theorem 4. To begin with, the total regret from stage \(s\) can now be bounded as

\[\frac{C_{2}\sigma}{\epsilon_{s}}\log_{2}^{3/2}\left(\frac{8\sigma}{\epsilon_{ s}}\right)\log_{2}(\log_{2}\frac{8\sigma}{\epsilon_{s}})\log\frac{2\overline{m}} {\delta}\times 2\beta_{s}\epsilon_{s}\sqrt{\lambda}=\mathcal{O}\big{(} \sigma\log_{2}^{3/2}(\frac{\sigma}{\epsilon_{s}})\log_{2}(\log_{2}\frac{ \sigma}{\epsilon_{s}})\log\frac{\overline{m}}{\delta}\beta_{s}\big{)}.\] (52)

Note that every \(1/\epsilon_{s}\) is upper-bounded by \(1/\epsilon_{s}\leq 3\times 2^{1/4}T=\mathcal{O}(T)\) which can be easily inferred using (51). So, the total regret can be upper-bounded as

\[\begin{split} R_{T}&=\mathcal{O}\left(\sum_{s=1}^{m} \sigma\log_{2}^{3/2}(\frac{\sigma}{\epsilon_{s}})\log_{2}(\log_{2}\frac{ \sigma}{\epsilon_{s}})\log\frac{\overline{m}}{\delta}\beta_{s}\right)\\ &=\mathcal{O}\left(m\beta_{m}\log\frac{\overline{m}}{\delta} \sigma\log_{2}^{3/2}(\sigma T)\log_{2}\left(\log_{2}(\sigma T)\right)\right). \end{split}\] (53)

As a result, for the linear kernel for which \(m\leq\overline{m}=\mathcal{O}(d\log T)\) (Theorem 2) and \(\beta_{m}=\sqrt{\widehat{\gamma}_{m-1}+\log(1/\delta)}=\mathcal{O}(\sqrt{d \log T+\log(1/\delta)})\) (Theorem 3 and Corollary 1),

\[\begin{split} R_{T}&=\mathcal{O}\left(d\log T\sqrt{d \log T+\log(1/\delta)}\log\frac{d\log T}{\delta}\sigma\log_{2}^{3/2}(\sigma T )\log_{2}(\log_{2}(\sigma T))\right)\\ &=\mathcal{O}\left(\sigma d^{3/2}\log^{3/2}T\log(d\log T)\log_{2} ^{3/2}(\sigma T)\log_{2}(\log_{2}(\sigma T))\right),\end{split}\] (54)

in which we have ignored all \(\log(1/\delta)\) factors for a cleaner expression.

For the SE kernel for which \(m\leq\overline{m}=\mathcal{O}(\log^{d+1}T)\) (Theorem 2) and \(\beta_{m}=\sqrt{\widehat{\gamma}_{m-1}+\log(1/\delta)}=\mathcal{O}(\sqrt{ \log^{d+1}T+\log(1/\delta)})\) (Theorem 3 and Corollary 1),

\[\begin{split} R_{T}&=\mathcal{O}\left(\log^{d+1}T \sqrt{\log^{d+1}T+\log(1/\delta)}\log\frac{\log^{d+1}T}{\delta}\sigma\log_{2}^{ 3/2}(\sigma T)\log_{2}(\log_{2}\sigma T)\right)\\ &=\mathcal{O}\left(\sigma(\log T)^{3(d+1)/2}\log((\log T)^{d+1}) \log_{2}^{3/2}(\sigma T)\log_{2}(\log_{2}\sigma T)\right),\end{split}\] (55)

in which we have again ignored all \(\log(1/\delta)\) factors for a cleaner expression.

Proof of Theorem 6

Our proof here follows closely the proof of Theorem 2 from the work of [2]. For consistency, in our proof here, we follow the notations from [38] and [2].

Since we focus on linear bandits here, we assume that the reward function is linear with a groundtruth \(d\)-dimensional parameter vector \(\theta^{*}\): \(f(x)=x^{\top}\theta^{*},\forall x\). Again following [2] and [38] as well as many other works on linear bandits, we assume that \(\|\theta^{*}\|_{2}\leq S\), and \(\|x\|_{2}\leq L,\forall x\). We use \(X_{s}\) to denote \(X_{s}\triangleq[x_{1},\ldots,x_{s}]^{\top}\) which is an \(s\times d\) matrix, and denote \(Y_{s}\triangleq[y_{1},\ldots,y_{s}]^{\top}\) which is an \(s\times 1\)-dimensional vector of observations. We use \(\zeta_{1:s}\triangleq[\zeta_{1},\ldots,\zeta_{s}]^{\top}\) to denote the \(s\times 1\)-dimensional vector of noise. These notations allow us to write \(Y_{s}=X_{s}\theta^{*}+\zeta_{1:s}\). We denote \(V_{s}\triangleq X_{s}^{\top}W_{s}X_{s}+\lambda I\). Following [38], we use \(\widehat{\theta}_{s}\) to denote the MLE estimate of the parameter \(\theta^{*}\) given the observations after the first \(s\) stages: \(\widehat{\theta}_{s}=V_{s}^{-1}X_{s}^{\top}W_{s}Y_{s}\). Given these notations, we firstly have that

\[\widehat{\theta}_{s} =(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}X_{s}^{\top}W_{s}(X_{s} \theta^{*}+\zeta_{1:s})\] (56) \[=(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}X_{s}^{\top}W_{s}\zeta_{1 :s}+(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}(X_{s}^{\top}W_{s}X_{s}+\lambda I) \theta^{*}\] \[\qquad-\lambda(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}\theta^{*}\] \[=(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}X_{s}^{\top}W_{s}\zeta_{1 :s}+\theta^{*}-\lambda(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}\theta^{*}.\]

Therefore,

\[x^{\top}\widehat{\theta}_{s}-x^{\top}\theta^{*} =x^{\top}(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}X_{s}^{\top}W_{s} \zeta_{1:s}-\lambda x^{\top}(X_{s}^{\top}W_{s}X_{s}+\lambda I)^{-1}\theta^{*}\] (57) \[=\langle x,X_{s}^{\top}W_{s}\zeta_{1:s}\rangle_{V_{s}^{-1}}- \lambda\langle x,\theta^{*}\rangle_{V_{s}^{-1}}.\]

This allows us to use the Cauchy-Schwarz inequality to show that

\[|x^{\top}\widehat{\theta}_{s}-x^{\top}\theta^{*}|\leq \left\|x\right\|_{V_{s}^{-1}}\left(\left\|X_{s}^{\top}W_{s}\zeta_ {1:s}\right\|_{V_{s}^{-1}}+\lambda\|\theta^{*}\|_{V_{s}^{-1}}\right)\] (58) \[\leq \left\|x\right\|_{V_{s}^{-1}}\left(\left\|X_{s}^{\top}W_{s}\zeta_ {1:s}\right\|_{V_{s}^{-1}}+\sqrt{\lambda}\|\theta^{*}\|_{2}\right).\]

The last inequality follows because \(\left\|\theta^{*}\right\|_{V_{s}^{-1}}^{2}\leq\frac{1}{\lambda_{\min}(V_{s})} \|\theta^{*}\|_{2}^{2}\leq\frac{1}{\lambda}\|\theta^{*}\|_{2}^{2}\). Note that \(X_{s}^{\top}W_{s}\zeta_{1:s}=\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}\zeta_{k} x_{k}\).

Next, we make use of the self-normalized concentration inequality from Theorem 1 of [2]. Specifically, we will use \(\{\frac{\zeta_{s}}{\epsilon_{s}}\}_{s=1}^{\infty}\) as to replace the stochastic process \(\{\zeta_{s}\}_{s=1}^{\infty}\) (i.e., representing the noise, \(\{\eta_{s}\}_{s=1}^{\infty}\) according to the notations from [2]), and use \(\{\frac{1}{\epsilon_{s}}x_{s}\}_{s=1}^{\infty}\) to replace the vector-valued stochastic process \(\{x_{s}\}_{s=1}^{\infty}\) (i.e., representing the sequence of inputs). Now we explain why this is feasible.

Similar to the proof of Theorem 3, we again denote the observation noise as \(\zeta_{s}\): \(y_{s}=f(x_{s})+\zeta_{s}\), and define \(\mathcal{F}_{s}\) as the \(\sigma\)-algebra \(\mathcal{F}_{s}=\sigma(x_{1},x_{2},\ldots,x_{s+1},\zeta_{1},\zeta_{2},\ldots, \zeta_{s})\). Similarly, an immediate consequence is that \(x_{s}\) is \(\mathcal{F}_{s-1}\)-measurable, and \(\zeta_{s}\) is \(\mathcal{F}_{s}\)-measurable. Following a similar analysis to that used in the proof of Theorem 3 allows us to show that \(\epsilon_{k}\) is \(\mathcal{F}_{k-1}\)-measurable (i.e., \(\epsilon_{k}\) can be predicted based on \(\mathcal{F}_{k-1}\)). Therefore, we have that \(\frac{1}{\epsilon_{k}}x_{k}\) is \(\mathcal{F}_{k-1}\)-measurable (i.e., \(\frac{1}{\epsilon_{k}}x_{k}\) can be predicted based on \(\mathcal{F}_{k-1}\)) and that \(\frac{\zeta_{k}}{\epsilon_{k}}\) is \(\mathcal{F}_{k}\)-measurable. Moreover, conditioned on \(\mathcal{F}_{k-1}\), \(\frac{\zeta_{k}}{\epsilon_{k}}\) is \(1\)-sub-Gaussian. This is because the QMC subroutine guarantees that \(|y_{k}-f(x_{k})|\leq\epsilon_{k},\forall k=1,\ldots,m\) (with high probability), which means that the absolute value of the noise \(\zeta_{k}=y_{k}-f(x_{k})\) is upper-bounded by \(\epsilon_{k}\): \(|\zeta_{k}|\leq\epsilon_{k}\). Therefore, conditioned on \(\mathcal{F}_{k-1}\), \(\frac{\zeta_{k}}{\epsilon_{k}}\) is \(1\)-sub-Gaussian.

Given that these conditions are satisfied, we can apply the self-normalized concentration inequality from Theorem 1 of [2] to \(\{\frac{\zeta_{k}}{\epsilon_{s}}\}_{s=1}^{\infty}\) and \(\{\frac{1}{\epsilon_{s}}x_{s}\}_{s=1}^{\infty}\) with (following the notations from Theorem 1 of [2])

\[V_{s} =V+\sum_{k=1}^{s}\left(\frac{1}{\epsilon_{k}}x_{k}\right)\left( \frac{1}{\epsilon_{k}}x_{k}^{\top}\right)=\lambda I+\sum_{k=1}^{s}\frac{1}{ \epsilon_{k}^{2}}x_{k}x_{k}^{\top},\] (59) \[S_{s} =\sum_{k=1}^{s}\frac{\zeta_{k}}{\epsilon_{k}}\left(\frac{1}{\epsilon _{k}}x_{k}\right)=\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}\zeta_{k}x_{k},\]in which we have used \(V=\lambda I\). This allows us to show that

\[\left\|S_{s}\right\|_{V_{s}^{-1}}= \left\|X_{s}^{\top}W_{s}\zeta_{1:s}\right\|_{V_{s}^{-1}}\leq\sqrt{2 \log\left(\frac{\det(V_{s})^{1/2}\det(\lambda I)^{-1/2}}{\delta}\right)}\] (60)

which holds with probability of at least \(1-\delta\). Next, again making use of the trace-determinant inequality from (16) and using the aforementioned assumption that \(\left\|x\right\|_{2}\leq L\), we can show that \(\det(V_{s})\leq(\lambda+\frac{L^{2}}{d}\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}) ^{d}\). Also note that \(\det(\lambda I)=\lambda^{d}\). As a result, we can further upper-bound the equation above by:

\[\left\|X_{s}^{\top}W_{s}\zeta_{1:s}\right\|_{V_{s}^{-1}} \leq\sqrt{2\log\left(\sqrt{\frac{(\lambda+\frac{L^{2}}{d}\sum_{k= 1}^{s}\frac{1}{\epsilon_{k}^{2}})^{d}}{\lambda^{d}}}\frac{1}{\delta}\right)}\] (61) \[=\sqrt{2\log\left(\sqrt{\left(1+\frac{L^{2}}{d\lambda}\sum_{k=1} ^{s}\frac{1}{\epsilon_{k}^{2}}\right)^{d}}\frac{1}{\delta}\right)}\] \[\leq\sqrt{2d\log\frac{1+\frac{L^{2}}{\lambda}\sum_{k=1}^{s}\frac{ 1}{\epsilon_{k}^{2}}}{\delta}},\]

in which we have used \(d\geq 1\) in the last step to get a cleaner expression.

Therefore, we can re-write (58) as

\[\left|x^{\top}\widehat{\theta}_{s}-x^{\top}\theta^{*}\right|\leq \left\|x\right\|_{V_{s}^{-1}}\left(\sqrt{2d\log\frac{1+\frac{L^{2 }}{\lambda}\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}}{\delta}}+\sqrt{\lambda} S\right),\qquad\forall x,s,\] (62)

in which we have also used the above-mentioned assumption of \(\left\|\theta^{*}\right\|_{2}\leq S\). Now again following [2], we plug in \(x=V_{s}(\widehat{\theta}_{s}-\theta^{*})\), which gives us

\[\left\|\widehat{\theta}_{s}-\theta^{*}\right\|_{V_{s}}^{2}\leq \left\|V_{s}(\widehat{\theta}_{s}-\theta^{*})\right\|_{V_{s}^{-1}} \left(\sqrt{2d\log\frac{1+\frac{L^{2}}{\lambda}\sum_{k=1}^{s}\frac{1}{ \epsilon_{k}^{2}}}{\delta}}+\sqrt{\lambda}S\right)\] (63) \[= \left\|\widehat{\theta}_{s}-\theta^{*}\right\|_{V_{s}}\left(\sqrt {2d\log\frac{1+\frac{L^{2}}{\lambda}\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}} {\delta}}+\sqrt{\lambda}S\right).\]

Therefore, we have that

\[\left\|\widehat{\theta}_{s}-\theta^{*}\right\|_{V_{s}}\leq\left(\sqrt{2d\log \frac{1+\frac{L^{2}}{\lambda}\sum_{k=1}^{s}\frac{1}{\epsilon_{k}^{2}}}{\delta }}+\sqrt{\lambda}S\right).\] (64)

This completes the proof of Theorem 6.

Now we briefly show how the improved confidence ellipsoid from our Theorem 6 leads to an improved regret upper bound for Q-LinUCB. Here we follow the proof of Theorem 3 from [38], and hence defer the detailed explanations of some of the steps to the proof there. To begin with, we have that

\[f(x^{*})-f(x_{s}) =(x^{*}-x_{s})^{\top}\theta^{*}\] (65) \[\leq\epsilon_{s}\left(\left\|\widetilde{\theta}_{s}-\widehat{ \theta}_{s-1}\right\|_{V_{s-1}}+\left\|\widehat{\theta}_{s-1}-\theta^{*} \right\|_{V_{s-1}}\right)\] \[\leq 2\epsilon_{s}\times\left(\sqrt{2d\log\frac{1+L^{2}T^{2}/ \lambda}{\delta}}+\sqrt{\lambda}S\right)\] \[=\mathcal{O}\left(\epsilon_{s}\sqrt{d\log T}\right).\]Here for simplicity, we have upper-bounded \(\sum_{k=1}^{s}(1/\epsilon_{k}^{2})\) by \(T^{2}\) (Lemma 2) in the second inequality, and have omitted the dependency on \(\log(1/\delta)\).

Then the total regrets in stage \(s\) can be upper-bounded by

\[\frac{C_{1}}{\epsilon_{s}}\log\frac{2\overline{m}}{\delta}\times\mathcal{O} \left(\epsilon_{s}\sqrt{d\log T}\right)=\mathcal{O}\left(\sqrt{d\log T}\log \frac{\overline{m}}{\delta}\right)\] (66)

Plugging in our tighter confidence ellipsoid (Theorem 6) into the regret analysis of QLinUCB, we have that the regret upper bound of QLinUCB can be analyzed as

\[R_{T} =\mathcal{O}(\sum_{s=1}^{m}\sqrt{d\log T}\log(\frac{\overline{m}} {\delta}))\] (67) \[=\mathcal{O}(m\sqrt{d\log T}\log(\frac{\overline{m}}{\delta}))\] \[=\mathcal{O}(d\log T\sqrt{d\log(T)}\log(d\log T))\] \[=\mathcal{O}\left(d^{3/2}(\log T)^{3/2}\log(d\log T)\right),\]

which gives the tighter regret upper bound for Q-LinUCB that we have discussed in the main text (Sec. 5.6).

## Appendix I Regret Upper Bound for the Matern Kernel (Proof of Theorem 7)

In this section, we modify our analysis to derive a regret upper bound for the Matern kernel. We focus on bounded noise here (i.e., the first scenario in Lemma 1), since the analysis for noise with bounded variance (i.e., the second scenario in Lemma 1) is similar and follows from the analysis of App. G. In our analysis here, for simplicity, we ignore the logarithmic factors.

To begin with, note that Theorem 1 is not kernel-specific and hence still holds in our analysis here. It has been shown that for the Matern kernel, the upper bound on the standard maximum information gain is \(\gamma_{T}=\widetilde{\mathcal{O}}(T^{\frac{d}{2\nu+d}})\)[35]. Therefore, similar to our Corollary 1, for the Matern kernel, we can use Theorem 1 to show that

\[\widetilde{\gamma}_{m}=\widetilde{\mathcal{O}}\left(\left(\sum_{s=1}^{m}\frac{ 1}{\epsilon_{s}^{2}}\right)^{\frac{d}{2\nu+d}}\right)=\widetilde{\mathcal{O}} \left(T^{\frac{2d}{2\nu+d}}\right).\] (68)

Next, we modify the proof of our Theorem 2 to derive the corresponding upper bound on the total number of stages for the Matern kernel, i.e., we discuss here how we modify the proof of Theorem 2 in App. D. To begin with, (68) suggests that there exists a \(C>0\) such that \(\widetilde{\gamma}_{m}\leq C\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}} \right)^{\frac{d}{2\nu+d}}\). Different from our analysis for the linear and SE kernels in App. D (i.e., (27) and (32)), the absolute constant \(C\) here also contains logarithmic factors. Similar to (27), we can show that

\[m\log 2=\log\frac{\text{det}(V_{m})}{\text{det}(V_{0})}=2\widetilde{\gamma}_{m} \leq 2C\left(\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\right)^{\frac{d}{2\nu+ d}}.\] (69)

This naturally leads to

\[\sum_{s=1}^{m}\frac{1}{\epsilon_{s}^{2}}\geq\left(m\frac{\log 2}{2C}\right)^{ \frac{2\nu+d}{d}},\] (70)

which is analogous to (28). Next, similar to (29), the total number of iterations can be analyzed as

\[\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2\overline{m}}{\delta}) \geq\sum_{s=1}^{m}\frac{1}{\epsilon_{s}}\geq\sqrt{\sum_{s=1}^{m}\frac{1}{ \epsilon_{s}^{2}}}\geq\sqrt{\left(m\frac{\log 2}{2C}\right)^{\frac{2\nu+d}{d}}}.\] (71)Now suppose \(m>T^{\frac{2d}{2\nu+d}}\frac{2C}{\log 2}\), then this implies that

\[\sqrt{\left(m\frac{\log 2}{2C}\right)^{\frac{2\nu+d}{d}}}>T.\] (72)

This equation, combined with (71), suggests that \(\sum_{s=1}^{m}\frac{C_{1}}{\epsilon_{s}}\log(\frac{2m}{\delta})>T\), which is a contradiction. Therefore, we have that

\[m\leq T^{\frac{2d}{2\nu+d}}\frac{2C}{\log 2}=\widetilde{\mathcal{O}}\left(T^{ \frac{2d}{2\nu+d}}\right).\] (73)

Next, note that our confidence ellipsoid in Theorem 3 does not depend on the choice of the kernel and hence also holds for the Matern kernel.

Lastly, we can modify the proof of Theorem 4 (App. F). Specifically, we can adopt the result from (47): \(R_{T}\leq\mathcal{O}\left(m\beta_{m}\log\frac{m}{\delta}\right)\). For the Matern kernel, Theorem 3 and (68) allow us to show that \(\beta_{m}=\widetilde{\mathcal{O}}(\sqrt{\widehat{\gamma}_{m-1}})=\widetilde{ \mathcal{O}}(\sqrt{T^{\frac{2d}{2\nu+d}}})=\widetilde{\mathcal{O}}(T^{\frac{d} {2\nu+d}})\). Combining this with (73) allows us to show that for the Matern kernel,

\[R_{T}=\widetilde{\mathcal{O}}\left(T^{\frac{2d}{2\nu+d}}T^{\frac{d}{2\nu+d}} \right)=\widetilde{\mathcal{O}}\left(T^{\frac{2d}{2\nu+d}}\right).\] (74)

This completes the proof of Theorem 7.

## Appendix J More Experimental Details, and Experiment on Real Quantum Computer

In our experiments, when implementing both the classical GP-UCB and our Q-GP-UCB algorithms, we use random Fourier features (RFF) approximation to approximate the kernel: \(k(x,x^{\prime})\approx\widetilde{\phi}(x)^{\top}\widetilde{\phi}(x^{\prime})\). As a result, we can implement the weighted GP posterior following (6) as discussed in App. A, in which the \(M\)-dimensional (\(M<\infty\)) random features \(\widetilde{\phi}(\cdot)\) are used to replace the infinite-dimensional RKHS features \(\phi(\cdot)\). We have used \(M=100\) random features in the synthetic experiment and \(M=200\) in the AutoML experiment. As we have mentioned in the main paper (Sec. 6), we have implemented the QMC algorithm from the recent work of [20] using the Qiskit python package. We have used 6 qubits for the Gaussian noise and 1 qubits for the Bernoulli noise. For each experiment, we perform multiple independent trials (\(10\) trials for the synthetic experiment and \(5\) trials for the AutoML experiment) and have plotted the resulting mean and standard error as the solid lines and error bars in each figure, respectively. For our Q-GP-UCB, we choose \(\beta_{s}=1+\log s,\forall s\geq 1\) following the order given by the theoretical value of \(\beta_{s}\) (Theorem 3). For classical GP-UCB, we tried different approaches to setting \(\beta_{s}\): \(\beta_{s}=1+\log s\), \(\beta_{s}=\sqrt{2}\) and \(\beta_{s}=1\), all of which are commonly adopted practices in BO; we found that \(\beta_{s}=\sqrt{2}\) and \(\beta_{s}=1\) have led to the best performances for GP-UCB in, respectively, the synthetic and AutoML experiments.

For the synthetic experiment, some important experimental details have been discussed in Sec. 6. The reward function \(f\) used in the synthetic experiment is sampled from a GP with the SE kernel with a length scale of \(0.1\). In the AutoML experiment, an SVM is used for diabetes diagnosis. That is, we adopt the diabetes diagnosis dataset which can be found at https://www.kaggle.com/uciml/pima-indians-diabetes-database, which is under the CC0 license. The task involves using \(8\) features to predict whether the patient had diabetes or not, which constitutes a two-class classification problem. We use \(70\%\) of the dataset as the training set and the remaining dataset as the validation set. We aim to tune two hyperparameters of the SVM: the penalty parameter and the RBF kernel parameter, both within the range of \([10^{-4},1]\). For simplicity, for each of the two hyperparameters, we discretize its domain into \(5\) points with equal spacing. This results in a domain with \(\left|\mathcal{X}\right|=25\). Here we adopt the SE kernel: \(k(x,x^{\prime})\triangleq\sigma_{0}^{2}\exp(-\left\|x-x^{\prime}\right\|_{2}^ {2}/(2l^{2}))\) with \(\sigma_{0}^{2}=0.5\). Since we have implemented our algorithms (both classical GP-UCB and our Q-GP-UCB) using RFF approximation as discussed above, we performed a grid search for the length scale \(l\) within \(\{0.1,0.2,\ldots,1\}\) for both GP-UCB and Q-GP-UCB when generating the random features. We found that \(l=1.0\) leads to the best performance for GP-UCB and \(l=0.2\) works the best for Q-GP-UCB. This is likely because for our Q-GP-UCB, the effective noise is much smaller (thanks to the accurate estimation by the QMC algorithm), which allows us to use a smaller length scale (which can model more complicated functions) to accurately model the reward function.

All our experiments are run on a computer with Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz, with 64 CPUs. No GPUs are needed.

More Experimental Results.Fig. 2 additionally shows the cumulative regret for the LinUCB and Q-LinUCB algorithms in the synthetic experiment. Consistent with the results in Fig. 1 (d) in the main paper (Sec. 6) which has included the results for LinUCB and Q-LinUCB in the AutoML experiment, here LinUCB and QLinUCB again underperform significantly in Fig. 2 for both types of noises. These results further verify that in realistic experiments with non-linear reward functions, algorithms based on linear bandits are significantly outperformed by those based on BO/kernelized bandits. Fig. 3 additionally shows the results of our Q-LinUCB after considering quantum noise. As we have discussed in the main paper (Sec. 6), the results show that the quantum noise leads to slightly worse performances for our Q-GP-UCB, yet they are still able to significantly outperform classical GP-UCB.

Results Using A Real Quantum Computer.Here we additionally perform an experiment using a real quantum computer. Specifically, we ran our QMC subroutine on a real IBM quantum computer based on superconducting qubit technology (7 qubit, Falcon r5.11H processor) through the IBM Quantum cloud service. We ran our synthetic experiment with Gaussian noise on the system for only 1 trial (instead of 10 as in other simulations) due to time and resource constraints. The results in Fig. 4 show that although the performance of our Q-GP-UCB on real quantum computers is worse than those obtained without quantum noise and with simulated quantum noise, our Q-GP-UCB run on real quantum computers is still able to significantly outperform classical GP-UCB. This experiment provides a further demonstration for the potential of our Q-GP-UCB algorithm to lead to quantum speedup real BO applications.

Figure 3: Cumulative regret for (a) the synthetic experiment with Bernoulli noise, (b) the synthetic experiment with Gaussian noise, and (c) the AutoML experiment, with the additional comparisons with our Q-GP-UCB algorithm after considering quantum noise.

Figure 2: Cumulative regret for the synthetic experiment with (a) Bernoulli noise and (b) Gaussian noise, with the additional comparisons with LinUCB and Q-LinUCB.

Figure 4: Results using **a real IBM quantum computer**. Cumulative regret for the synthetic experiment with Gaussian noise with \(\sigma=0.4\).