# On Dynamic Programming Decompositions

of Static Risk Measures in Markov Decision Processes

 Jia Lin Hau

University of New Hampshire

Durham, NH

jialin.hau@unh.edu

&Erick Delage

HEC Montreal

Montreal (Quebec)

erick.delage@hec.ca

&Mohammad Ghavamzadeh

Amazon

Palo Alto, CA

ghavamza@amazon.com

&Marek Petrik

University of New Hampshire

Durham, NH

mpetrik@cs.unh.edu

The work was done prior to joining Amazon, while the author was at Google Research.

###### Abstract

Optimizing static risk-averse objectives in Markov decision processes is difficult because they do not admit standard dynamic programming equations common in Reinforcement Learning (RL) algorithms. Dynamic programming decompositions that augment the state space with discrete risk levels have recently gained popularity in the RL community. Prior work has shown that these decompositions are optimal when the risk level is discretized sufficiently. However, we show that these popular decompositions for Conditional-Value-at-Risk (CVaR) and Entropic-Value-at-Risk (EVaR) are inherently suboptimal regardless of the discretization level. In particular, we show that a saddle point property assumed to hold in prior literature may be violated. However, a decomposition does hold for Value-at-Risk and our proof demonstrates how this risk measure differs from CVaR and EVaR. Our findings are significant because risk-averse algorithms are used in high-stakes environments, making their correctness much more critical.

## 1 Introduction

Risk-averse reinforcement learning (RL) seeks to provide a risk-averse policy for high-stakes real-world decision problems. These high-stake domains include autonomous driving (Jin et al., 2019; Sharma et al., 2020), robot collision avoidance (Ahmadi et al., 2021; Hakobyan and Yang, 2021), liver transplant timing (Kose, 2016), HIV treatment (Keramati et al., 2020; Zhong, 2020), unmanned aerial vehicle (UAV) (Choudhry et al., 2021), and investment liquidation (Min et al., 2022), to name a few. Because these domains call for reliable solutions, risk-averse algorithms must be based on solid theoretical foundations. This is one reason why monetary risk measures, such as Value-at-Risk (VaR) and Conditional Value-at-Risk (CVaR), have become pervasive in risk-averse RL (Prashanth and Fu, 2022). Indeed, risk measures such as CVaR are known to be coherent (Artzner et al., 1999) with respect to a set of fundamental axioms that define how risk should be quantified and have been adopted as gold standards in banking regulations (Basel Committee on Banking Supervision, 2019).

Introducing risk-averse objectives in Markov decision processes (MDPs)--the primary model used in RL--is challenging. Dynamic programming, the linchpin of most RL algorithms, cannot be used directly to optimize a risk measure like VaR or CVaR in MDPs. One line of work tackles this challenge by exploiting the primal representation of risk measures and augmenting the state spaceof their dynamic programs (DPs) with an additional parameter that typically represents the total cumulative reward up to the current point (Bauerle and Ott, 2011; Boda et al., 2004; Chow and Ghavamzadeh, 2014; Filar et al., 1995; Hau et al., 2023; Lin et al., 2003; Wu and Lin, 1999; Xu and Mannor, 2011). Even when the original MDP is finite, this DP requires computing the value function for a continuous state space, and thus, has been considered inefficient in practice (Chapman et al., 2022; Chow et al., 2015; Li et al., 2022).

Another line of recent work leverages the dual representation to produce a _risk-level decomposition_ of risk measures (Pflug and Pichler, 2016). Using this decomposition, numerous authors have derived DPs for common risk measures and integrated them within various RL algorithms (Chapman et al., 2019, 2022; Chow et al., 2015; Ding and Feinberg, 2022, 2023; Ni and Lai, 2022; Rigter et al., 2021; Stanko and Macek, 2019). Although this risk-level decomposition requires augmenting the state space with a continuous parameter, this parameter is naturally bounded between 0 and 1. It has been generally accepted, with several _tentative_ proofs supporting this claim (Chow et al., 2015; Li et al., 2022), that these DPs recover the optimal policy if we can discretize the augmented state space sufficiently finely. Moreover, it is believed that one can use the optimal value function from this DP to recover the policies that are optimal for the full range of risk levels.

In this paper, we make a surprising discovery that numerous claims of optimality of risk-level decompositions published in the past several years are incorrect. Even when one discretizes the augmented state space arbitrarily finely, most risk-level DPs are not guaranteed to recover the optimal value function and policy. There are several reasons why existing arguments fail. As the most common reason, several papers assume that a certain saddle point property holds, either explicitly (Chow et al., 2015) or implicitly (Ding and Feinberg, 2022, 2023). We show that this property does not generally hold, invalidating the optimality of DPs, as hinted at in Chapman et al. (2019, 2022). This finding directly refutes the claimed or hypothesized optimality of algorithms proposed in many recent research papers and pre-prints, such as Chapman et al. (2019); Chow et al. (2015); Ding and Feinberg (2022, 2023); Rigter et al. (2021); Stanko and Macek (2019). Our results also affect applications of these algorithms, such as automated vehicle motion planning (Jin et al., 2019). We also identify gaps in related decompositions (Li et al., 2022; Ni and Lai, 2022) and propose how to fix them.

We make the following contributions in this paper. _First_, we show in Section 3 that the popular DP for optimizing CVaR in MDPs may not recover the optimal value function and policy regardless of how finely one discretizes the risk level in the augmented states. This method was first proposed in Chow et al. (2015) but adopted widely afterwards (Chapman et al., 2019; Ding and Feinberg, 2022, 2023; Rigter et al., 2021; Stanko and Macek, 2019). The simple counterexample in this section contradicts the optimality claims in Chow et al. (2015); Ding and Feinberg (2022). We hypothesize that prior work missed this issue because the CVaR DP works for policy evaluation and only fails when one uses it to optimize policies based on the "risk-to-go value function". Therefore, our results do not contradict the original decomposition in Pflug and Pichler (2016) that only applies to policy evaluation. We give a new independent and simple proof that the CVaR decomposition indeed works when evaluating a fixed policy.

_Second_, we show in Section 4 that the DP for optimizing the Entropic-Value-at-Risk in MDPs, proposed by Ni and Lai (2022), does not compute the correct value function even when the policy is fixed. Although EVaR has not been as popular as CVaR, it has been gaining attention in recent years (Hau et al., 2023). We give an example that contradicts the correctness claims of the risk-level decomposition for EVaR in Ni and Lai (2022). The gap that we identify with this objective applies to both policy evaluation and policy optimization. Furthermore, we prove a new, correct EVaR decomposition for policy evaluation. Unfortunately, the EVaR decomposition fails and is sub-optimal when applied to policy optimization, similar to CVaR.

_Third_, we propose an _optimal dynamic program_ for policy optimization of VaR in Section 5. Our DP is based on a risk-level decomposition that closely resembles the quantile MDP decomposition in Li et al. (2022) but corrects for several technical inaccuracies. The derivation shows why VaR stands apart from coherent risk measures like CVaR and EVaR. VaR is unique in that the decomposition can be constructed directly from the primal formulation of the risk measure, which avoids the complications that arise in the robust formulations used in CVaR and EVaR decompositions.

It is important to note that the correctness of DPs that augment the state space with the accumulated rewards is unaffected by our results (Bauerle and Ott, 2011; Chow and Ghavamzadeh, 2014; Chow et al., 2018; Hau et al., 2023). These DPs use the _primal_ risk measure representation and do not suffer from the same saddle point issue as the augmentation methods that use the _dual_ representation of the risk measures, such as the one in Chow et al. (2015).

## 2 Preliminaries

This section summarizes relevant properties of monetary risk measures and outlines how they are typically used in the context of solving MDPs.

Monetary Risk MeasuresWe restrict our attention to probability spaces with a finite outcome space \(\Omega\) such that \(|\Omega|=m\) for some \(m\in\mathbb{N}\). We use \(\mathbb{X}=\mathbb{R}^{m}\) to denote the space of real-valued random variables. To improve the clarity of probabilistic claims, we always adopt random variables with a tilde, such as \(\tilde{x}\in\mathbb{X}\). In finite spaces, we can represent any random variable \(\tilde{x}\in\mathbb{X}\) as a vector \(\bm{x}\in\mathbb{R}^{m}\). We also use \(\bm{q}\in\Delta_{m}\) to represent a probability distribution over \(\Omega\) where \(\Delta_{m}\)represents the \(m\)-dimensional probability simplex. Using this notation, we can write that \(\mathbb{E}[\tilde{x}]=\bm{q}^{\top}\bm{x}\).

A _monetary risk measure_\(\psi\colon\mathbb{X}\to\mathbb{R}\) assigns a real value to each real-valued random variable in a way that it is monotone and cash-invariant (Follmer and Schied, 2016; Shapiro et al., 2014). A risk measure can be seen as a generalization of the expectation operator \(\mathbb{E}[\cdot]\) that also takes into account the uncertainty in the random variable. In this work, we define all risk measures for random variables \(\tilde{x}\) that represent _rewards_. Thus, the risk-averse decision-maker aims to choose actions that maximize the value of the risk measure, i.e., a higher value of risk measure represents a lower exposure to risk.

We consider three monetary risk measures common in RL. Perhaps the most well-known measure is _Value-at-Risk_ (VaR), which is defined for a risk-level \(\alpha\in[0,1]\) and a random variable \(\tilde{x}\in\mathbb{X}\) in modern literature as (e.g., Follmer and Schied, 2016; Shapiro et al., 2014)

\[\mathrm{VaR}_{\alpha}\left[\tilde{x}\right]\ =\ \sup\ \{z\in\mathbb{R}\ |\ \mathbb{P}\left[\tilde{x}<z\right]\leq\alpha\}\ =\ \inf\ \{z\in\mathbb{R}\ |\ \mathbb{P}\left[\tilde{x}\leq z\right]>\alpha\}\,.\] (1)

Note that \(\mathrm{VaR}_{1}\left[\tilde{x}\right]=\infty\). The equality between the two definitions holds, for example, by Follmer and Schied (2016, remark A.20).

Another popular risk measure is the _Conditional-value-at-Risk_ (CVaR), which is defined for a risk level \(\alpha\in[0,1]\) and a random variable \(\tilde{x}\in\mathbb{X}\) distributed as \(\tilde{x}\sim\bm{q}\) as (e.g., Follmer and Schied 2016, definition 11.8 and Shapiro et al. 2014, eq. 6.23)

\[\mathrm{CVaR}_{\alpha}\left[\tilde{x}\right]\ =\ \sup_{z\in\mathbb{R}}\ \big{(}z-\alpha^{-1}\mathbb{E}\left[z-\tilde{x}\right]_{+}\big{)}\ =\ \inf\ \Big{\{}\bm{\xi}^{\top}\bm{x}\ |\ \bm{\xi}\in\Delta_{m},\alpha\cdot\bm{\xi}\leq\bm{q}\Big{\}},\] (2)

with \(\mathrm{CVaR}_{0}\left[\tilde{x}\right]=\mathrm{ess}\inf[\tilde{x}]\) and \(\mathrm{CVaR}_{1}\left[\tilde{x}\right]=\mathbb{E}[\tilde{x}]\). The equality above follows from standard conjugacy arguments for finite probability spaces (Follmer and Schied, 2016). Note that our CVaR definition applies to \(\tilde{x}\) that represents rewards and assumes that a higher value of the risk measure is preferable to a lower value. Other CVaR formulations exist in the literature, but they induce identical preferences for appropriately chosen rewards and risk level \(\alpha\).

Finally, the _entropic value at risk_ (EVaR), with \(\mathrm{EVaR}_{0}\left[\tilde{x}\right]=\mathrm{ess}\inf[\tilde{x}]\) and \(\mathrm{EVaR}_{1}\left[\tilde{x}\right]=\mathbb{E}[\tilde{x}]\), is defined for \(\alpha\in(0,1]\) as (Ahmadi-Javid, 2012)

\[\mathrm{EVaR}_{\alpha}\left[\tilde{x}\right] =\sup_{\beta>0}-\frac{1}{\beta}\log\left(\alpha^{-1}\mathbb{E} \big{[}\exp\left(-\beta\cdot\tilde{x}\right)\big{]}\right)\] (3) \[=\inf\ \big{\{}\bm{\xi}^{T}\bm{x}\ |\ \bm{\xi}\in\Delta_{m},\bm{\xi} \ll\bm{q},\mathrm{KL}(\bm{\xi}\|\bm{q})\leq-\log\alpha\big{\}}\,,\]

where \(\mathrm{KL}\) is the standard KL-divergence defined for each \(\bm{x},\bm{y}\in\Delta_{m}\) as \(\mathrm{KL}(\bm{x}\|\bm{y})=\sum_{\omega\in\Omega}x_{\omega}\log\left(x_{-}/y_ {\omega}\right)\). This definition is valid only when \(\bm{x}\) is absolutely continuous with respect to \(\bm{y}\), which is denoted as \(\bm{x}\ll\bm{y}\) and corresponds to \(y_{\omega}=0\Rightarrow x_{\omega}=0\) for each \(\omega\in\Omega\).

Risk Averse MDPsA Markov decision process (MDP) is a sequential decision model that underlies most of RL (Puterman, 2005). We consider finite MDPs with states \(\mathcal{S}=\{s_{1},\ldots,s_{S}\}\) and actions \(\mathcal{A}=\{a_{1},\ldots,a_{A}\}\). After taking an action in a state, the agent transitions to the next state according to a transition probability function \(p\colon\mathcal{S}\times\mathcal{A}\to\Delta_{\mathcal{S}}\) such that \(p(s,a,s^{\prime})\) represents the transition probability from \(s\in\mathcal{S}\) to \(s^{\prime}\in\mathcal{S}\) after taking \(a\in\mathcal{A}\). We use \(\bm{p}_{s,a}=p(s,a,\cdot)\in\Delta_{S}\) to denote the vector of transition probabilities. The initial state \(\tilde{s}_{0}\) is distributed according to \(\hat{\bm{p}}\in\Delta_{S}\). To avoid divisions by \(0\) that are not central to our claims, we assume that \(\hat{p}_{s}>0\) for each \(s\in\mathcal{S}\). Finally,the reward function is \(r\colon\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}\), where \(r(s,a,s^{\prime})\) represents the deterministic reward associated with the transition to \(s^{\prime}\) from \(s\) after taking an action \(a\).

The most general solution to an MDP is a _history-dependent randomized_ policy \(\pi\) which maps a sequence of observed states and actions \(s^{0},a^{0},s^{1},a^{1},\ldots,s^{t}\) to a distribution over the next action \(a^{t}\). It is well-known that with risk-neutral objectives, there always exists an optimal stationary--depends only on the last state--deterministic policy (Puterman, 2005). When the objective is risk-averse, like VaR, or CVaR, there may not exist an optimal stationary or deterministic policy. Hence, we use the symbol \(\Pi\) to denote the set of history-dependent randomized policies in the remainder of the paper.

This paper focuses on the _finite-horizon objective_ in which the agent aims to compute policies that optimize the sum of rewards over a known horizon \(T\). We further restrict our attention to the objective with horizon \(T=1\). It turns out that having a single time step is sufficient to derive our counterexamples to existing dynamic programs. Moreover, deriving the decompositions with \(T=1\) makes it possible to avoid technicalities caused by history-dependent policies, which could distract us from the main ideas presented in this work. Our results an be extended to general horizons \(T>1\) and the discounted infinite-horizon objectives using standard techniques (Chow et al., 2015).

With horizon \(T=1\), the set of randomized history-dependent policies is \(\Pi=\{\boldsymbol{\pi}\colon\mathcal{S}\to\Delta_{\lambda}\}\). The symbol \(\pi(s,a)\) denotes the probability of action \(a\) in a state \(s\), and \(\boldsymbol{\pi}(s)=\pi(s,\cdot)\in\Delta_{A}\) denotes the \(A\)-dimensional vector of action probabilities in a state \(s\). Given a risk measure \(\psi\) with a risk level \(\alpha\in[0,1]\), the finite-horizon risk-averse value of a policy \(\pi\in\Pi\) is computed as

\[v_{0}^{\pi}(\alpha)=\psi_{\alpha}^{\hat{a}\sim\boldsymbol{\pi}(\tilde{s})} \left[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\right],\] (4)

where the superscript in \(\psi_{\alpha}^{\hat{a}\sim\boldsymbol{\pi}(\tilde{s})}\) specifies the distribution of the random action. Throughout the paper, we generally use \(\tilde{s}\) to denote the random state at time \(t=0\) and \(\tilde{s}^{\prime}\) to denote the random state at time \(t=1\). In risk-neutral objectives, when \(\psi=\mathbb{E}\), one can use the tower property of the expectation operator and define a value function \(v_{t}\) for each time step \(t\)(Puterman, 2005), but this property does not hold in most static risk measures (Hau et al., 2023). The term _policy evaluation_ in the remainder of the paper refers to computing the value in (4).

The goal in an MDP is to compute an _optimal_ value function and a policy that attains it. In risk-averse MDPs, this goal is formalized as the following risk-averse optimization

\[v_{0}^{\star}(\alpha)=\max_{\pi\in\Pi}\,v_{0}^{\pi}(\alpha)=\max_{\pi\in\Pi} \,\psi_{\alpha}^{\hat{a}\sim\boldsymbol{\pi}(\tilde{s})}\left[r(\tilde{s}, \tilde{a},\tilde{s}^{\prime})\right],\] (5)

with the _optimal policy_\(\pi^{\star}\) being any policy that attains the maximum (5). As with policy evaluation, when \(\psi=\mathbb{E}\), the optimal value function \(v_{t}^{\star}\) can be defined for each time-step \(t\)(Puterman, 2005), but this is impossible in general for common risk measures, like VaR and CVaR. The term _policy optimization_ in the remainder of the paper refers to computing the value and the maximizer in (5).

In the remainder of the paper, we study dynamic programming algorithms proposed to solve the policy evaluation problem in (4) and policy optimization problem in (5). In general, these algorithms build on risk-level decomposition (Pflug and Pichler, 2016) of risk measures to define a value function \(v_{t}^{\pi}(s,\alpha)\) for each time step \(t\in[T]\), state \(s\in\mathcal{S}\), and risk-level \(\alpha\in[0,1]\)(Chow et al., 2015). The value function represents the risk-adjusted sum of rewards that can be obtained if starting in a state \(s\in\mathcal{S}\) at time \(t\) and a risk level \(\alpha\). For example, one would define the value function as \(v_{1}^{\pi}(s,\alpha)=\psi_{\alpha}^{\hat{a}\sim\boldsymbol{\pi}(s)}[r(s, \tilde{a},\tilde{s}^{\prime})]\) and compute \(v_{0}^{\pi}\) using a Bellman operator \(T_{\alpha}^{\pi}\) as \(v_{0}^{\pi}(\alpha)=(T^{\pi}v_{1}^{\pi})(\alpha)\). In risk-neutral objectives, the Bellman operator is defined as \((T^{\pi}(v_{1}^{\pi}))(\alpha)=\mathbb{E}[v_{1}^{\pi}(\tilde{s},\alpha)]\), with \(\alpha\in\{1\}\), but in risk-averse formulations the operator definition is more complex. The remainder of the paper discusses the decompositions and the operator for CVaR, EVaR, and VaR risk measures respectively.

## 3 CVaR: Decomposition Fails in Policy Optimization

In this section, we show that a common CVaR decomposition proposed in Chow et al. (2015) and used to optimize risk-averse policies is inherently sub-optimal regardless of how closely one discretizes the state space. The following proposition represents one of the key results used to decompose the risk measure in multi-stage decision-making.

**Proposition 3.1** (lemma 22 in Pflug and Pichler 2016).: _Suppose that \(\pi\in\Pi\) and \(\tilde{s}\sim\boldsymbol{\hat{p}}\), \(\tilde{a}\sim\boldsymbol{\pi}(\tilde{s})\), \(\tilde{s}^{\prime}\sim\boldsymbol{p}_{s,a}\). Then,_

\[\mathrm{CVaR}_{\alpha}\left[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\right] = \min_{\boldsymbol{\zeta}\in\mathcal{Z}_{\mathrm{C}}}\;\sum_{s\in \mathcal{S}}\zeta_{s}\;\mathrm{CVaR}_{\alpha\zeta_{s}\tilde{p}_{s}^{-1}}\left[ r(s,\tilde{a},\tilde{s}^{\prime})\mid\tilde{s}=s\right],\] (6)

_where the state \(s\) on the right-hand side is not random and_

\[\mathcal{Z}_{\mathrm{C}}\ =\ \{\boldsymbol{\zeta}\in\Delta_{S}\mid\alpha\cdot \boldsymbol{\zeta}\leq\boldsymbol{\hat{p}}\}\,.\] (7)

The notation in Proposition 3.1 differs superficially from lemma 22 in Pflug and Pichler (2016). Specifically, our CVaR is defined for rewards rather than costs, the meaning of our \(\alpha\) corresponds to \(1-\alpha\) in Pflug and Pichler (2016), and we use \(\xi_{s}=z_{s}\hat{p}_{s}\) as the optimization variable. We include a simple proof of Proposition 3.1 for completeness in Appendix A.1.

The decomposition in Proposition 3.1 is important because it shows that the CVaR evaluation can be formulated as a dynamic program. The theorem shows that CVaR at time \(t=0\) decomposes into a convex combination of CVaR values at \(t=1\). Recursively repeating this process, one can formulate a dynamic program for any finite time horizon \(T\). Because the risk level at \(t=1\) differs from the level at \(t=0\) and depends on the optimal \(\boldsymbol{\zeta}\), one must compute CVaR values for all (or many) risk-levels \(\alpha\in[0,1]\) at time \(t=1\). As a result, the dynamic program includes an additional state variable that represents the current risk level.

Chow et al. (2015) proposed to adapt the decomposition in Proposition 3.1 to policy optimization as

\[\begin{split}\max_{\pi\in\Pi}\;\mathrm{CVaR}_{\alpha}^{\tilde{a} \sim\boldsymbol{\pi}(\tilde{s})}[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})]& =\;\max_{\pi\in\Pi}\min_{\boldsymbol{\zeta}\in\mathcal{Z}_{\mathrm{C}}} \;\sum_{s\in\mathcal{S}}\zeta_{s}\left(\mathrm{CVaR}_{\alpha\zeta_{s}\tilde{ p}_{s}^{-1}}^{\tilde{a}\sim\boldsymbol{\pi}(s)}|\;\tilde{s}=s|\right)\\ &\stackrel{{\ref{eq:CvaR}}}{{=}}\;\sum_{\boldsymbol{ \zeta}\in\mathcal{Z}_{\mathrm{C}}}\;\sum_{s\in\mathcal{S}}\zeta_{s}\left(\max_ {d\in\Delta_{\mathcal{A}}}\;\mathrm{CVaR}_{\alpha\zeta_{s}\tilde{p}_{s}^{-1}} ^{\tilde{a}\sim\boldsymbol{d}}[r(s,\tilde{a},\tilde{s}^{\prime})\mid\tilde{s} =s]\right)\,.\end{split}\] (8)

They used the decomposition in (8) to formulate a dynamic program with the current risk level as an additional state variable. We prove in the following theorem that the second equality in (8) marked with question marks is false in general.

**Theorem 3.2**.: _There exists an MDP and a risk level \(\alpha\in[0,1]\) such that_

\[\max_{\pi\in\Pi}\;\mathrm{CVaR}_{\alpha}^{\tilde{a}\sim\boldsymbol{\pi}( \tilde{s})}[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})]<\min_{\boldsymbol{\zeta }\in\mathcal{Z}_{\mathrm{C}}}\;\sum_{s\in\mathcal{S}}\zeta_{s}\left(\max_{d \in\Delta_{\mathcal{A}}}\;\mathrm{CVaR}_{\alpha\zeta_{s}\tilde{p}_{s}^{-1}}^{ \tilde{a}\sim\boldsymbol{d}}[r(s,\tilde{a},\tilde{s}^{\prime})\mid\tilde{s}=s ]\right)\,.\] (9)

Before proving Theorem 3.2, we discuss its implications. First, Theorem 3.2 contradicts theorems 5 and 7 in Chow et al. (2015) and shows that their algorithm is inherently sub-optimal regardless of the resolution of the discretization. Theorem 3.2 also contradicts the optimality of the accelerated dynamic program proposed in Stanko and Macek (2019). The result of Chow et al. (2015) was exploited as is in Chapman et al. (2019), Ding and Feinberg (2022), and Jin et al. (2019) to propose DP reductions, and extended, without proof, in (Rigter et al., 2021) to the context of a Bayesian MDP.

Finally, it is important to emphasize that Theorem 3.2 only applies to the policy optimization setting and does not contradict Proposition 3.1, which holds for the evaluation of policies that assign the same action distribution to each history of states and actions (i.e., policies that are independent of the hypothesized values of \(\boldsymbol{\zeta}\)).

Figure 1: Rewards of MDP \(M_{\mathrm{C}}\) used in the proof of Theorem 3.2. The dot indicates that the rewards are independent of the next state.

Proof.: Let \(\alpha=0.5\) and consider the MDP \(M_{\mathrm{C}}\) in Figure 1. In state \(s_{1}\), both actions \(a_{1}\) and \(a_{2}\) are available, and in state \(s_{2}\), only action \(a_{1}\) is available. The MDP's rewards are

\[r(s_{1},a_{1},s_{1}) =-50, r(s_{1},a_{1},s_{2}) =100,\] \[r(s_{1},a_{2},s_{1}) =r(s_{1},a_{2},s_{2})=0, r(s_{2},a_{1},s_{1}) =r(s_{2},a_{1},s_{2})=10\,.\]

The transition probabilities in \(M_{\mathrm{C}}\) are

\[p(s_{1},a_{1},s_{1}) =0.4, p(s_{1},a_{1},s_{2}) =0.6\,,\]

and the initial distribution is uniform: \(\hat{p}_{s_{1}}=\hat{p}_{s_{2}}=0.5\).

To simplify the notation, we define \(\theta_{\pi}\colon\mathcal{Z}_{\mathrm{C}}\to\mathbb{R}\) for each \(\pi\in\Pi\) and \(\bm{\zeta}\in\mathcal{Z}_{\mathrm{C}}\) as

\[\theta_{\pi}(\bm{\zeta})\ =\ \sum_{s\in\mathcal{S}}\zeta_{s}\operatorname{ CVaR}_{\alpha\zeta_{s}\hat{p}_{s}^{-1}}^{\hat{a}\sim\bm{\pi}(s)}[r(s,\tilde{a}, \tilde{s}^{\prime})\mid\tilde{s}=s]\,.\]

Because \(\operatorname{CVaR}\) is convex in the distribution (Delage et al., 2019) and any distribution for \(r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\) obtained from a policy \(\pi\in\Pi\) is a mixture of the distributions of \(r(\tilde{s},a_{1},\tilde{s}^{\prime})\) and \(r(\tilde{s},a_{2},\tilde{s}^{\prime})\), it is sufficient to consider only _deterministic_ policies (there exists an optimal deterministic policy). Thus, we can reformulate the _left-hand side_ of (9) in terms of \(\theta_{\pi}(\bm{\zeta})\) as

\[\max_{\bm{\pi}\in\Pi}\ \operatorname{CVaR}_{\alpha}^{\hat{a}\sim\bm{\pi}( \tilde{s})}[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})] =\max_{\pi\in\{\pi_{1},\pi_{2}\}}\operatorname{CVaR}_{\alpha \zeta_{s}\hat{p}_{s}^{-1}}^{\hat{a}\sim\bm{\pi}(s)}[r(\tilde{s},\tilde{a}, \tilde{s}^{\prime})]\] \[=\max_{\pi\in\{\pi_{1},\pi_{2}\}}\min_{\bm{\zeta}\in\mathcal{Z}_{ \mathrm{C}}}\sum_{s\in\mathcal{S}}\zeta_{s}\cdot\operatorname{CVaR}_{\alpha \zeta_{s}\hat{p}_{s}^{-1}}^{\hat{a}\sim\bm{\pi}(s)}[r(s,\tilde{a},\tilde{s}^{ \prime})\mid\tilde{s}=s]\] \[=\max_{\pi\in\{\pi_{1},\pi_{2}\}}\min_{\bm{\zeta}\in\mathcal{Z}_{ \mathrm{C}}}\theta_{\pi}(\bm{\zeta})\,,\]

with \(\pi_{1}(s,a_{1})=1-\pi_{1}(s,a_{2})=1\) and \(\pi_{2}(s,a_{2})=1-\pi_{2}(s,a_{1})=1\), for all \(s\in\mathcal{S}\). The functions \(\theta_{\pi_{1}}(\cdot)\) and \(\theta_{\pi_{2}}(\cdot)\) are depicted in Figure 2. Similarly, the _right-hand side_ of (9) can be expressed using the convexity of \(\operatorname{CVaR}\) in the distribution by algebraic manipulation as

\[\min_{\bm{\zeta}\in\mathcal{Z}_{\mathrm{C}}}\ \sum_{s\in\mathcal{S}}\zeta_{s} \max_{\bm{d}\in\Delta_{A}}\operatorname{CVaR}_{\alpha\zeta_{s}\hat{p}_{s}^{- 1}}^{\hat{a}\sim\bm{d}}[r(s,\tilde{a},\tilde{s}^{\prime})\mid\tilde{s}=s]=\min _{\bm{\zeta}\in\mathcal{Z}_{\mathrm{C}}}\max_{\pi\in\{\pi_{1},\pi_{2}\}}\theta _{\pi}(\bm{\zeta})\,.\]

Using the notation introduced above and the sufficiency of optimizing over deterministic policies only, the inequality in (9) becomes

\[\max_{\pi\in\{\pi_{1},\pi_{2}\}}\min_{\bm{\zeta}\in\mathcal{Z}_{ \mathrm{C}}}\theta_{\pi}(\bm{\zeta})\ <\ \min_{\bm{\zeta}\in\mathcal{Z}_{\mathrm{C}}}\max_{\pi\in\{\pi_{1},\pi_{2}\}} \theta_{\pi}(\bm{\zeta})\,.\] (10)

Figure 2 demonstrates the inequality in (10) numerically, with the rectangle representing the left-hand side maximum and the pentagon representing the right-hand side minimum. The dashed line represents the function \(\bm{\zeta}\mapsto\max_{\pi\in\{\pi_{1},\pi_{2}\}}\theta_{\pi}(\bm{\zeta})\).

[MISSING_PAGE_FAIL:7]

**Theorem 4.1**.: _There exists an MDP with a single action and \(\alpha\in(0,1]\) such that_

\[\mathrm{EVaR}_{\alpha}\left[r(\tilde{s},a_{1},\tilde{s}^{\prime})\right] < \min_{\bm{\xi}\in\mathcal{Z}_{\mathrm{E}}}\;\sum_{s\in\mathcal{S}} \xi_{s}\,\mathrm{EVaR}_{\alpha\xi_{s},\tilde{p}_{s}^{-1}}\left[r(s,a_{1}, \tilde{s}^{\prime})\mid\tilde{s}=s\right]\,,\] (13)

_the set \(\mathcal{Z}_{\mathrm{E}}\) defined by (12)._

Theorem 4.1 demonstrates a stronger failure mode than the one in Theorem 3.2 (for CVaR policy optimization) since it applies to both policy evaluation and policy optimization settings.

We propose a correct decomposition of EVaR in the following theorem and employ it to establish that the decomposition in (11) overestimates the actual value of EVaR (see Appendix A.3 for a proof).

**Theorem 4.2**.: _Given any finite MDP with horizon \(T=1\) and \(\alpha\in(0,1]\), we have that_

\[\mathrm{EVaR}_{\alpha}\left[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\right] = \inf_{\bm{\xi}\in(0,\,1]^{S},\,\bm{\xi}\in\mathcal{Z}_{\mathrm{E}}^{ \prime}(\bm{\zeta})}\;\sum_{s\in\mathcal{S}}\xi_{s}\,\mathrm{EVaR}_{\zeta_{s} }\left[r(s,\tilde{a},\tilde{s}^{\prime})\mid\tilde{s}=s\right]\,,\]

_where_

\[\mathcal{Z}_{\mathrm{E}}^{\prime}(\bm{\zeta})=\left\{\bm{\xi}\in\Delta_{S}\mid \bm{\xi}\ll\bm{\hat{p}},\;\sum_{s\in\mathcal{S}}\xi_{s}\big{(}\log(\xi_{s}/ \hat{p}_{s})-\log(\zeta_{s})\big{)}\leq-\log\alpha\right\}.\]

_Moreover, EVaR can be upper-bounded as_

\[\mathrm{EVaR}_{\alpha}\left[r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\right] \leq \min_{\bm{\xi}\in\mathcal{Z}_{\mathrm{E}}}\;\sum_{s\in\mathcal{S}} \xi_{s}\,\mathrm{EVaR}_{\alpha\xi_{s},\tilde{p}_{s}^{-1}}\left[r(s,\tilde{a}, \tilde{s}^{\prime})\mid\tilde{s}=s\right]\,.\] (14)

## 5 VaR: Decomposition Holds for Policy Evaluation and Optimization

In this section, we discuss a dynamic program decomposition for VaR whose decomposition resembles those for CVaR and EVaR described in Sections 3 and 4. We provide a new proof of the VaR decomposition to elucidate the differences that make it optimal in contrast to CVaR and EVaR decompositions. Our VaR decomposition closely resembles the quantile MDP approach in Li et al. (2022) with a few technical modifications that can significantly impact the computed value.

To contrast the typical definition of VaR with the quantile definition in Li et al. (2022), it is helpful to summarize how VaR is related to the quantile of a random variable. Let \(\mathfrak{q}\in\mathbb{R}\) define as the \(\alpha\)_-quantile_ of \(\tilde{x}\in\mathbb{X}\) when

\[\mathbb{P}\left[\tilde{x}\leq\mathfrak{q}\right]\geq\alpha\quad\text{and} \quad\mathbb{P}\left[\tilde{x}<\mathfrak{q}\right]\leq\alpha\;.\] (15)

In general, the set of quantiles is an interval \([\mathfrak{q}_{\tilde{x}}^{-}(\alpha),\mathfrak{q}_{\tilde{x}}^{+}(\alpha)]\) with the bounds computed as (Follmer and Schied, 2016, appendix A.3)

\[\mathfrak{q}_{\tilde{x}}^{-}(\alpha) =\;\sup\left\{z\mid\mathbb{P}\left[\tilde{x}<z\right]<\alpha \right\}=\inf\left\{z\mid\mathbb{P}\left[\tilde{x}\leq z\right]\geq\alpha \right\},\] \[\mathfrak{q}_{\tilde{x}}^{+}(\alpha) =\;\inf\left\{z\mid\mathbb{P}\left[\tilde{x}\leq z\right]>\alpha \right\}=\sup\left\{z\mid\mathbb{P}\left[\tilde{x}<z\right]\leq\alpha\right\}.\]

Note that when the distribution of \(\tilde{x}\) is absolutely continuous (atomless), then \(\mathfrak{q}_{\tilde{x}}^{+}(\alpha)=\mathfrak{q}_{\tilde{x}}^{-}(\alpha)\) and the quantile is unique. The following example illustrates a simple setting in which the quantile is not unique.

_Example 1_ (Bernoulli random variable).: Consider a Bernoulli random variable \(\tilde{e}\) such that \(\tilde{e}=1\) and \(\tilde{e}=0\) with equal (50%) probabilities. Then, any value \(q\in[0,1]\) is a valid \(0.5\)-quantile because

\[\mathfrak{q}_{\tilde{e}}^{-}(0.5) =\inf_{z\in\mathbb{R}}\;\left\{z\mid\mathbb{P}\left[\tilde{e}\leq z \right]\geq 0.5\right\}=\inf_{z\in\mathbb{R}}\;\left\{z\mid z\geq 0\right\}=0\] \[\mathfrak{q}_{\tilde{e}}^{+}(0.5) =\sup_{z\in\mathbb{R}}\;\left\{z\mid\mathbb{P}\left[\tilde{e} \geq z\right]\geq 0.5\right\}=\sup_{z\in\mathbb{R}}\;\left\{z\mid z\leq 1\right\}=1.\]

The objective in Li et al. (2022) is to maximize the quantile operator \(Q_{\alpha}\colon\mathbb{X}\to\mathbb{R}\) defined for a reward random variable \(\tilde{x}\in\mathbb{X}\) and a risk level \(\alpha\in[0,1]\) as

\[Q_{\alpha}(\tilde{x})\;=\;\inf_{z\in\mathbb{R}}\;\left\{z\mid\mathbb{P}\left[ \tilde{x}\leq z\right]\geq\alpha\right\}\,.\] (16)

The quantile operator \(Q_{\alpha}\) and VaR differ in which quantile of the random variable they consider:

\[Q_{\alpha}(\tilde{x})\;=\;\mathfrak{q}_{\tilde{x}}^{-}(\alpha),\qquad\text{but} \qquad\mathrm{VaR}_{\alpha}\left[\tilde{x}\right]\;=\;\mathfrak{q}_{\tilde{x}}^ {+}(\alpha)\,.\] (17)

As a result, the quantile MDP objective in (16) coincides with the VaR value only when the quantile is unique, which is not always the case, as shown in Example 1.

**Theorem 5.1**.: _Let \(\tilde{y}\colon\Omega\to[N]\) be a random variable distributed as \(\tilde{\bm{p}}=(\hat{p}_{i})_{i=1}^{N}\) with \(\hat{p}_{i}>0\). Then for any random variable \(\tilde{x}\in\mathbb{X}\), we have_

\[\mathrm{VaR}_{\alpha}\left[\tilde{x}\right] = \sup_{\bm{\zeta}\in\Delta_{N}}\left\{\min_{i}\ \mathrm{VaR}_{\alpha_{i}\hat{p}_{i}^{-1}}\left[\tilde{x}\mid\tilde{y}=i \right]\mid\alpha\cdot\bm{\zeta}\leq\tilde{\bm{p}}\right\}\,,\] (18)

_where we interpret the minimum to evaluate to \(\infty\) if all terms are infinite, which only occurs if \(\alpha=1\)._

Proof.: We decompose VaR using the definition in (1) as

\[\mathrm{VaR}_{\alpha}\left[\tilde{x}\right] =\sup\ \left\{z\in\mathbb{R}\mid\mathbb{P}\left[\tilde{x}<z \right]\leq\alpha\right\}\overset{\text{(a)}}{=}\sup\ \left\{z\in\mathbb{R}\mid\sum_{i=1}^{N}\mathbb{P}\left[\tilde{x}<z\mid\tilde{ y}=i\right]\cdot\hat{p}_{i}\leq\alpha\right\}\] \[\overset{\text{(b)}}{=}\sup\ \left\{z\in\mathbb{R}\mid\bm{\zeta}\in[0,1]^{N},\ \mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\leq\zeta_{i},\forall i\in[N ],\ \sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}\leq\alpha\right\}\] \[\overset{\text{(c)}}{=}\sup\left\{\sup\ \left\{z\in\mathbb{R}\mid \mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\leq\zeta_{i},\ \forall i\in[N]\right\}\mid\bm{\zeta}\in[0,1]^{N},\ \sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}\leq\alpha\right\}\] \[=\sup\left\{\sup\ \bigcap_{i\in[N]}\left\{z\in\mathbb{R}\mid \mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\leq\zeta_{i}\right\}\mid\bm {\zeta}\in[0,1]^{N},\ \sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}\leq\alpha\right\}\] \[\overset{\text{(d)}}{=}\sup\left\{\min_{i\in[N]}\sup\ \left\{z\in\mathbb{R}\mid \mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\leq\zeta_{i}\right\}\mid\bm {\zeta}\in[0,1]^{N},\ \sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}\leq\alpha\right\}\] \[\overset{\text{(e)}}{=}\sup\ \left\{\min_{i\in[N]}\ \mathrm{VaR}_{\zeta_{i}} \left[\tilde{x}\mid\tilde{y}=i\right]\mid\bm{\zeta}\in[0,1]^{N},\ \sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}\leq\alpha\right\}.\]

We decompose the probability \(\mathbb{P}\left[\tilde{x}<z\right]\) as a marginal of the conditional probabilities \(\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\) and \(\hat{p}_{i}=\mathbb{P}\left[\tilde{y}=i\right]\) in step (a) and then lower-bound them by an auxiliary variable \(\zeta_{i}\) in step (b). In step (c) we replace the joint supremum over \(z\) and \(\bm{\zeta}\) by sequential suprema, and then we replace the supremum of an intersection by the minimum of the suprema of sets in (d). The equality in (d) holds because \(\mathbb{P}\left[\tilde{x}<z\right]\) is monotone and, therefore, the sets \(\{z\in\mathbb{R}\mid\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\leq \zeta_{i}\}\) are nested. Finally, step (e) follows from the definition of VaR in (1). 

Focusing on the finite MDP with horizon \(T=1\), we can show that the decomposition proposed in Theorem 5.1 is amenable to policy optimization. The main difference between the VaR decomposition and CVaR is that the former VaR was expressed as a supremum instead of an infimum over quantile levels \(\bm{\zeta}\). For VaR, changing the order of maximum (\(\pi\)) and supremum (\(\bm{\zeta}\)) does not suffer from a potential gap, but changing the order of maximum (\(\pi\)) and infimum/minimum (\(\bm{\zeta}\)) in CVaR does suffer from such a gap as shown in Theorem 3.2.

The following theorem (proved in Appendix A.4) summarizes the decomposition for VaR.

**Theorem 5.2**.: _Given any finite MDP with horizon \(T=1\) and \(\alpha\in[0,1]\), we have_

\[\max_{\pi\in\Pi}\ \mathrm{VaR}_{\alpha}^{\tilde{a}\sim\bm{\pi}(\tilde{s})}[r( \tilde{s},\tilde{a},\tilde{s}^{\prime})]=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{ \min_{s\in\mathcal{S}}\max_{\bm{d}\in\Delta_{A}}\mathrm{VaR}_{\alpha_{s}\hat{p }_{s}^{-1}}^{\tilde{a}\sim\bm{d}}\left[r(s,\tilde{a},\tilde{s}^{\prime})\mid \tilde{s}=s\right]\mid\alpha\cdot\bm{\zeta}\leq\tilde{\bm{p}}\right\}.\]

For completeness, Appendix C extends Theorem 5.2 to a setting with horizon \(T>1\), providing dynamic programming equations and a definition of the optimal policy. These equations are analogous to the equations presented in (Li et al., 2022) yet we obtain them using the accurate definition and decomposition of \(\mathrm{VaR}_{\alpha}\).

We finally present below a valid decomposition for the lower quantile MDP, used officially as the objective in (Li et al., 2022) (see Appendix A.5 for a proof).

**Proposition 5.3**.: _Given any finite MDP with horizon \(T=1\) and some \(\alpha\in[0,1]\), we have that:_

\[\max_{\pi\in\Pi}Q_{\alpha}^{\tilde{a}\sim\bm{\pi}(\tilde{s})}(r(\tilde{s}, \tilde{a},\tilde{s}^{\prime}))=\sup_{\bm{\zeta}\in[0,1]^{S}}\left\{\min_{s\in \mathcal{S}:\zeta_{s}<1}\ \max_{\bm{d}\in\Delta_{A}}Q_{\zeta_{s}}^{\tilde{a}\sim\bm{d}}(r(s,\tilde{a}, \tilde{s}^{\prime})\mid\tilde{s}=s)\mid\sum_{s=1}^{S}\zeta_{s}\hat{p}_{s}<\alpha \right\}.\]We note that the difference with the result presented in (Li et al., 2022) resides in the constraint imposed on \(\zeta\) that replaces the weak inequality with a strict one. In fact, this strict versus weak inequality is the main distinguishing factor between the decompositions for the lower and upper quantiles.

## 6 Conclusion

This paper shows that a popular decomposition approach to solving MDPs with CVaR and EVaR objectives is suboptimal despite the claims to the contrary. This suboptimality arises from a saddle-point gap when _optimizing policy_. We also prove that a similar decomposition approach is optimal for policy optimization and evaluation when solving MDPs with the VaR objective. The decomposition is optimal because VaR does not involve the same saddle point problem as CVaR and EVaR.

Our findings are significant because practitioners who make risk-averse decisions in high-stakes scenarios need to have confidence in the correctness of the algorithms they use. Our work raises awareness that popular static CVaR and EVaR MDP algorithms are suboptimal, and their analyses are inaccurate. We hope the results we present in our paper will increase the scrutiny of dynamic programming methods for risk-averse MDPs and motivate research into alternative approaches, such as the parametric dynamic programs.

#### Acknowledgments

We thank Yinlam Chow for his insightful comments that inspired this paper's topic and results. The work in the paper was supported, in part, by NSF grants 2144601 and 2218063. In addition, this work was performed in part while Marek Petrik was a visiting researcher at Google Research. Finally, E. Delage acknowledges the support from the Canadian Natural Sciences and Engineering Research Council and the Canada Research Chair program [Grant RGPIN-2022-05261 and CRC-2018-00105].

## References

* Ahmadi et al. (2021) Mohamadreza Ahmadi, Xiaobin Xiong, and Aaron D Ames. Risk-averse control via CVaR barrier functions: Application to bipedal robot locomotion. _IEEE Control Systems Letters_, 6:878-883, 2021.
* Ahmadi-Javid (2012) A. Ahmadi-Javid. Entropic Value-at-Risk: A new coherent risk measure. _Journal of Optimization Theory and Applications_, 155(3):1105-1123, 2012.
* Artzner et al. (1999) Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk. _Mathematical Finance_, 9(3):203-228, 1999.
* Committee on Banking Supervision (2019) Basel Committee on Banking Supervision. Minimum capital requirements for market risk. In _Basel III: international regulatory framework for banks_, 2019.
* Bauerle and Ott (2011) Nicole Bauerle and Jonathan Ott. Markov decision processes with average-value-at-risk criteria. _Mathematical Methods of Operations Research_, 74(3):361-379, 2011.
* Boda et al. (2004) Kang Boda, Jerzy A Filar, Yuanlie Lin, and Lieneke Spanjers. Stochastic target hitting time and the problem of early retirement. _IEEE Transactions on Automatic Control_, 49(3):409-419, 2004.
* Chapman et al. (2019) Margaret P Chapman, Jonathan Lacotte, Aviv Tamar, Donggun Lee, Kevin M Smith, Victoria Cheng, Jaime F Fisac, Susmit Jha, Marco Pavone, and Claire J Tomlin. A risk-sensitive finite-time reachability approach for safety of stochastic dynamic systems. In _American Control Conference (ACC)_, pages 2958-2963. IEEE, 2019.
* Chapman et al. (2022) Margaret P Chapman, Riccardo Bonalli, Kevin M Smith, Insoon Yang, Marco Pavone, and Claire J Tomlin. Risk-sensitive safety analysis using conditional value-at-risk. _IEEE Transactions on Automatic Control_, 67(12):6521-6536, 2022.
* Choudhry et al. (2021) Arnav Choudhry, Brady Moon, Jay Patrikar, Constantine Samaras, and Sebastian Scherer. CVaR-based flight energy risk assessment for multirotor uavs using a deep energy model. In _IEEE International Conference on Robotics and Automation (ICRA)_, pages 262-268, 2021.
* Choudhry et al. (2021)Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In _Neural Information Processing Systems (NIPS)_, 2014.
* Chow et al. (2015) Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-making: A CVaR optimization approach. In _Neural Information Processing Systems (NIPS)_, 2015.
* Chow et al. (2018) Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. _Journal of Machine Learning Research_, 18:1-51, 2018.
* Cover and Thomas (2006) Thomas M. Cover and Joy A. Thomas. _Elements of Information Theory_. Wiley-Interscience, 2nd edition, 2006.
* Delage et al. (2019) Erick Delage, Daniel Kuhn, and Wolfram Wiesemann. "Dice"-sion-making under uncertainty: When can a random decision reduce risk? _Management Science_, 65(7):3282-3301, 2019.
* Ding and Feinberg (2022) Rui Ding and Eugene Feinberg. CVaR optimization for MDPs: Existence and computation of optimal policies. _ACM SIGMETRICS Performance Evaluation Review_, 50(2):39-41, 2022.
* Ding and Feinberg (2023) Rui Ding and Eugene A. Feinberg. Sequential Optimization of CVaR, February 2023.
* Filar et al. (1995) Jerzy A. Filar, Dmitry Krass, and Keith W. Ross. Percentile Performance Criteria For Limiting Average Markov Decision Processes. _IEEE Transactions on Automatic Control_, 40(1):2-10, 1995.
* Follmer and Schied (2016) Hans Follmer and Alexander Schied. _Stochastic Finance: Introduction in Discrete Time_. De Gruyter Graduate, fourth edition, 2016.
* Hakobyan and Yang (2021) Astghik Hakobyan and Insoon Yang. Wasserstein distributionally robust motion control for collision avoidance using conditional value-at-risk. _IEEE Transactions on Robotics_, 38(2):939-957, 2021.
* Hau et al. (2023) Jia Lin Hau, Marek Petrik, and Mohammad Ghavamzadeh. Entropic risk optimization in discounted MDPs. In _Artificial Intelligence and Statistics (AISTATS)_, 2023.
* Jin et al. (2019) I Ge Jin, Bastian Schurmann, Richard M Murray, and Matthias Althoff. Risk-aware motion planning for automated vehicle among human-driven cars. In _American Control Conference (ACC)_, pages 3987-3993, 2019.
* Keramati et al. (2020) Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be conservative: Quickly learning a CVaR policy. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 4436-4443, 2020.
* Kose (2016) Umit Emre Kose. _Optimal timing of living-donor liver transplantation under risk-aversion_. PhD thesis, Bilkent Universitesi (Turkey), 2016.
* Li et al. (2022) Xiaocheng Li, Huaiyang Zhong, and Margaret L. Brandeau. Quantile Markov decision processes. _Operations Research_, 70(3):1428-1447, 2022.
* Lin et al. (2003) Yuanlile Lin, Congbin Wu, and Boda Kang. Optimal models with maximizing probability of first achieving target value in the preceding stages. _Science in China Series A: Mathematics_, 46:396-414, 2003.
* Min et al. (2022) Seungki Min, Ciamac C Moallemi, and Costis Maglaras. Risk-sensitive optimal execution via a conditional value-at-risk objective. _arXiv preprint arXiv:2201.11962_, 2022.
* Ni and Lai (2022) Xinyi Ni and Lifeng Lai. Risk-sensitive reinforcement learning via Entropic-VaR optimization. In _Asilomar Conference on Signals, Systems, and Computers_, pages 953-959. IEEE, 2022.
* Ch Pflug and Pichler (2016) Georg Ch Pflug and Alois Pichler. Time-consistent decisions and temporal decomposition of coherent risk functionals. _Mathematics of Operations Research_, 41(2):682-699, 2016.
* Prashanth and Fu (2022) L. A Prashanth and Michael Fu. Risk-sensitive reinforcement learning via policy gradient search. _Foundations and Trends in Machine Learning_, 15(5):537-693, 2022.
* Pichler and Schied (2016)Martin L Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. Wiley-Interscience, 2005.
* Rigter et al. [2021] Marc Rigter, Bruno Lacerda, and Nick Hawes. Risk-averse Bayes-adaptive reinforcement learning. _Advances in Neural Information Processing Systems_, 34:1142-1154, 2021.
* Shapiro et al. [2014] A. Shapiro, D. Dentcheva, and A. Ruszczynski. _Lectures on Stochastic Programming: Modeling and Theory_. SIAM, 2014.
* Shapiro [2017] Alexander Shapiro. Interchangeability principle and dynamic equations in risk averse stochastic programming. _Operations Research Letters_, 45(4):377-381, 2017.
* Sharma et al. [2020] Vishnu D Sharma, Maymoonah Toubeh, Lifeng Zhou, and Pratap Tokekar. Risk-aware planning and assignment for ground vehicles using uncertain perception from aerial vehicles. In _International Conference on Intelligent Robots and Systems (IROS)_, pages 11763-11769. IEEE, 2020.
* Stanko and Macek [2019] Silvestr Stanko and Karel Macek. Risk-averse distributional reinforcement learning: A CVaR optimization approach. In _International Joint Conference on Computational Intelligence_, pages 412-423, 2019.
* Wu and Lin [1999] Congbin Wu and Yuanlie Lin. Minimizing risk models in Markov decision processes with policies depending on target values. _Journal of mathematical analysis and applications_, 231(1):47-67, 1999.
* Xu and Mannor [2011] Huan Xu and Shie Mannor. Probabilistic goal Markov decision processes. In _International Joint Conference on Artificial Intelligence_, 2011.
* Zhong [2020] Huaiyang Zhong. _Decision Making for Disease Treatment: Operations Research and Data Analytic Modeling_. Stanford University, 2020.

## Appendix A Proofs

### Proof of Proposition 3.1

Suppose that \(\alpha>0\); the decomposition for \(\alpha=0\) holds readily because \(\mathrm{CVaR}_{0}\left[\tilde{x}\right]=\mathrm{ess}\inf[\tilde{x}]\).

To streamline the notation, Define a random variable \(\tilde{x}=r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\) over a random space \(\Omega=\mathcal{S}\times\mathcal{A}\times\mathcal{S}\) with a probability distribution \(\bm{q}\in\Delta_{m}\) such that \(q_{s,a,s^{\prime}}=\hat{p}_{s}\cdot\pi(s,a)\cdot p(s,a,s^{\prime})\). The value \(\bm{x}\) is the vector representation of the random variable \(\tilde{x}\) and \(\bm{\xi}_{s}=\bm{\xi}_{s,\cdot\cdot}\in\mathbb{R}^{S\cdot A}\) for \(\bm{\xi}\in\mathbb{R}^{m}\) is a vector that corresponds to the subset of the elements of \(\Omega\) in which the first element is some \(s\in\mathcal{S}\). The vectors \(\bm{x}_{s}=\bm{x}_{s,\cdot\cdot}\in\mathbb{R}^{S\cdot A}\) and \(\bm{q}_{s}=\bm{q}_{s,\cdot\cdot}\in\mathbb{R}^{S\cdot A}\) are defined analogously to \(\bm{\xi}_{s}\).

Starting with the \(\mathrm{CVaR}\) definition in (2) and introducing an auxiliary variable \(\bm{\zeta}\) we get that

\[\mathrm{CVaR}_{\alpha}\left[\tilde{x}\right] =\min_{\bm{\xi}\in\Delta_{m}}\left\{\bm{x}^{\top}\bm{\xi}\mid \alpha\bm{\xi}\leq\bm{q}\right\}=\min_{\bm{\xi}\in\Delta_{m},\bm{\zeta}\in \mathbb{R}^{S}}\left\{\bm{x}^{\top}\bm{\xi}\mid\alpha\bm{\xi}\leq\bm{q},\, \zeta_{s}=\bm{1}^{\top}\bm{\xi}_{s},\forall s\in\mathcal{S}\right\}\] \[=\min_{\bm{\xi}\in\Delta_{m},\bm{\zeta}\in\Delta_{S}}\left\{\bm{x} ^{\top}\bm{\xi}\mid\alpha\bm{\xi}\leq\bm{q},\,\zeta_{s}=\bm{1}^{\top}\bm{\xi}_ {s},\,\alpha\zeta_{s}\leq\hat{p}_{s},\,\forall s\in\mathcal{S}\right\}\] \[=\min_{\bm{\xi}\in\mathbb{R}^{m}_{\bm{\xi}},\bm{\zeta}\in\mathcal{ Z}_{\mathbb{C}}}\left\{\bm{x}^{\top}\bm{\xi}\mid\alpha\bm{\xi}\leq\bm{q},\, \zeta_{s}=\bm{1}^{\top}\bm{\xi}_{s},\,\forall s\in\mathcal{S}\right\}\,.\]

In the derivation above, we replaced the infimum by a minimum because \(\Omega\) is finite, introduced a new variable \(\bm{\zeta}\), derived implied constraints on \(\bm{\zeta}\), and then dropped superfluous constraints on \(\bm{\xi}\). Continuing with the derivation above and noticing that the constraints on each \(\bm{\xi}_{s}\) are independent given \(\bm{\zeta}\), we get that

\[\mathrm{CVaR}_{\alpha}\left[\tilde{x}\right] =\min_{\bm{\xi}\in\mathbb{R}^{0}_{\bm{\xi}},\bm{\zeta}\in \mathcal{Z}_{\mathbb{C}}}\left\{\sum_{s\in\mathcal{S}}\bm{x}_{s}^{\top}\bm{\xi }_{s}\mid\alpha\bm{\xi}\leq\bm{q},\,\zeta_{s}=\bm{1}^{\top}\bm{\xi}_{s},\, \forall s\in\mathcal{S}\right\}\] \[\overset{\text{(a)}}{=}\min_{\bm{\zeta}\in\mathcal{Z}_{\mathbb{C} }}\sum_{s\in\mathcal{S}}\inf_{\bm{\xi}_{s}\in\mathbb{R}^{1\alpha_{s}}_{+}} \left\{\bm{x}_{s}^{\top}\bm{\xi}_{s}\mid\alpha\bm{\xi}_{s}\leq\bm{q}_{s},\, \zeta_{s}=\bm{1}^{\top}\bm{\xi}_{s}\right\}\] \[\overset{\text{(b)}}{=}\min_{\bm{\zeta}\in\mathcal{Z}_{\mathbb{C} }}\sum_{s\in\mathcal{S}}\zeta_{s}\cdot\min_{\bm{\chi}\in\Delta_{S\cdot A}} \left\{\bm{x}_{s}^{\top}\bm{\chi}\mid\alpha\hat{p}_{s}^{-1}\zeta_{s}\chi_{ \lambda,a,s^{\prime}}\leq\hat{p}_{s}^{-1}q_{s,a,s^{\prime}},\forall a\in \mathcal{A},s^{\prime}\in\mathcal{S}\right\}\] \[\overset{\text{(c)}}{=}\min_{\bm{\zeta}\in\mathcal{Z}_{\mathbb{C} }}\sum_{s\in\mathcal{S}}\zeta_{s}\cdot\mathrm{CVaR}_{\alpha\zeta_{s}\hat{p}_{s }^{-1}}\left[\tilde{x}\mid\tilde{s}=s\right]\,.\]

The step (a) follows from the interchangeability principle (Shapiro et al., 2014, theorem 7.92), and the step (b) follows by substituting \(\xi_{s,a,s^{\prime}}=\zeta_{s}\chi_{\alpha,s^{\prime}}\) taking care when \(\zeta_{s}=0\) and multiplying both sides of the inequality by \(\hat{p}_{s}^{-1}>0\). Finally, in step (c), the random variable \(\tilde{x}=r(\tilde{s},\tilde{a},\tilde{s}^{\prime})\) conditional on \(\tilde{s}=s\) is distributed according to \(q_{s,a,s^{\prime}}\hat{p}_{s}^{-1}\) and the equality follows from the definition of \(\mathrm{CVaR}\) in (2). 

### Proof of Theorem Theorem 4.1

Consider an MDP \(M_{\mathrm{E}}\) depicted in Figure 3 with \(\mathcal{S}=\{s_{1},s_{2}\}\) and \(\mathcal{A}=\{a_{1}\}\) and a reward function \(r(s_{1},a_{1},\cdot)=1\) and \(r(s_{2},a_{1},\cdot)=0\). We abbreviate the rewards to \(r(s_{1})\) and \(r(s_{2})\) because they only depend on the originating state. The initial distribution is \(\hat{p}_{s_{1}}=\hat{p}_{s_{2}}=0.5\). We finally let \(\alpha=0.75\)

Figure 3: Rewards of the MDP \(M_{\mathrm{E}}\) used in the proof of Theorem 4.1. The dot indicates that the rewards are independent of the next state.

Because \(\mathcal{Z}_{\mathrm{E}}\subseteq\mathcal{Z}_{\mathrm{C}}\), the right-hand side of (13) can be lower-bounded by \(\mathrm{CVaR}\) as

\[\begin{split}\min_{\boldsymbol{\xi}\in\mathcal{Z}_{\mathrm{E}}} \,\sum_{s\in\mathcal{S}}\xi_{s}\,\mathrm{EVaR}_{\alpha\xi_{s}\tilde{p}_{s}^{- 1}}\,[r(s,a_{1},\tilde{s}^{\prime})]&=\;\min_{\boldsymbol{\xi} \in\mathcal{Z}_{\mathrm{E}}}\,\sum_{s\in\mathcal{S}}\xi_{s}r(s)\\ &\geq\;\min_{\boldsymbol{\xi}\in\mathcal{Z}_{\mathrm{C}}}\,\sum_{ s\in\mathcal{S}}\xi_{s}r(s)=\mathrm{CVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{ \prime})]\.\end{split}\] (19)

The first equality holds from the positive homogeneity and cash invariance properties of \(\mathrm{EVaR}\), and the last equality follows from the dual representation of \(\mathrm{CVaR}\)(Follmer and Schied, 2016).

Because \(\mathrm{EVaR}_{\alpha}\,[\tilde{x}]\leq\mathrm{CVaR}_{\alpha}\,[\tilde{x}]\) for each \(\alpha\in[0,1]\) and \(\tilde{x}\in\mathbb{X}\) (see (Ahmadi-Javid, 2012, proposition 3.2)), we can further lower-bound (19) as

\[\mathrm{EVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]\;\leq\; \mathrm{CVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]\;\leq\;\min_{ \boldsymbol{\xi}\in\mathcal{Z}_{\mathrm{E}}}\,\sum_{s\in\mathcal{S}}\xi_{s} \,\mathrm{EVaR}_{\alpha\xi_{s}\tilde{p}_{s}^{-1}}\,[r(s,a_{1},\tilde{s}^{ \prime})]\.\] (20)

Therefore, (13) holds with an inequality.

To prove by contradiction that the inequality in (13) is strict, suppose that

\[\mathrm{EVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]\;=\;\min_{ \boldsymbol{\xi}\in\mathcal{Z}_{\mathrm{E}}}\,\sum_{s\in\mathcal{S}}\xi_{s} \,\mathrm{EVaR}_{\alpha\xi_{s}\tilde{p}_{s}^{-1}}\,[r(s,a_{1},\tilde{s}^{ \prime})]\.\] (21)

Equalities (21) and (20) imply that \(\mathrm{EVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]=\mathrm{CVaR} _{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]\) which is false in general (Ahmadi-Javid, 2012).

We now show that \(\mathrm{EVaR}\) does not equal \(\mathrm{CVaR}\) even for the categorical distribution of \(\tilde{s}\). The \(\mathrm{CVaR}\) of the return in \(M_{\mathrm{E}}\) reduces from (2) to

\[\mathrm{CVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]=\min_{ \boldsymbol{\xi}\in\mathcal{Z}_{\mathrm{C}}}\sum_{s\in\mathcal{S}}\xi_{s}r(s) =\max\left\{0,\,\frac{\hat{p}_{s_{1}}+\alpha-1}{\alpha}\right\}\.\] (22)

Since \(1-\alpha=0.25<0.5=\hat{p}_{s_{1}}\), then the optimal \(\boldsymbol{\xi}^{\star}\) in (22) is

\[\boldsymbol{\xi}^{\star}=\begin{pmatrix}\frac{\hat{p}_{s_{1}}+\alpha-1}{\alpha }\\ \frac{1-\hat{p}_{s_{1}}}{\alpha}\end{pmatrix}\.\]

Since \(\mathrm{KL}(\boldsymbol{\xi}^{\star}\|\boldsymbol{\hat{p}})<-\log\alpha\). we have that \(\boldsymbol{\xi}^{\star}\) is in the relative interior of the \(\mathrm{EVaR}\) feasible region in (3), and, therefore, there exists an \(\epsilon>0\) such that

\[\mathrm{EVaR}_{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]=\mathrm{CVaR} _{\alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]-\epsilon<\mathrm{CVaR}_{ \alpha}\,[r(\tilde{s},a_{1},\tilde{s}^{\prime})]\,\]

which proves the desired inequality. 

### Proof of Corollary 4.2

We start by proposing a new decomposition for \(\mathrm{EVaR}\).

**Proposition A.1**.: _Given a random variable \(\tilde{x}\in\mathbb{X}\) and a discrete variable \(\tilde{y}\colon\Omega\to\mathcal{N}=\{1,\ldots,N\}\), with probabilities denoted as \(\{\hat{p}_{i}\}_{i=1}^{N}\), for any \(\alpha\in(0,1]\) we have that_

\[\mathrm{EVaR}_{\alpha}\,[\tilde{x}]\quad=\quad\inf_{\boldsymbol{\zeta}\in(0,1 ]^{N}}\min_{\boldsymbol{\xi}\in\mathcal{Z}_{\mathrm{E}}^{\prime}(\boldsymbol{ \zeta})}\;\sum_{i}\xi_{i}\,\mathrm{EVaR}_{\zeta_{i}}\,[\tilde{x}\mid\tilde{y} =i]\,\]

_where_

\[\mathcal{Z}_{\mathrm{E}}^{\prime}(\boldsymbol{\zeta})=\left\{\boldsymbol{ \xi}\in\Delta_{N}\mid\boldsymbol{\xi}\ll\boldsymbol{\hat{p}},\sum_{i=1}^{N} \xi_{i}(\log(\xi_{i}/\hat{p}_{i})-\log(\zeta_{i}))\leq-\log\alpha\right\}\,.\]

Proof.: Let \(\boldsymbol{q}\) denote the joint probability distribution of \(\tilde{x}\) and \(\tilde{y}\). The proof exploits the chain rule of relative entropy (e.g., Cover and Thomas (2006, theorem 2.5.3)), which states that for any probability distributions \(\boldsymbol{\eta},\boldsymbol{q}\in\Delta_{\Omega}\) with \(\boldsymbol{\eta}\ll\boldsymbol{q}\) that

\[\mathrm{KL}(\boldsymbol{\eta}\|\boldsymbol{q})=\mathrm{KL}(\boldsymbol{\eta}( \tilde{y})\|\boldsymbol{q}(\tilde{y}))+\mathrm{KL}(\boldsymbol{\eta}(\tilde{x} |\tilde{y})\|\boldsymbol{q}(\tilde{x}|\tilde{y})),\] (23)where the conditional relative entropy is defined as

\[\mathrm{KL}(\bm{\eta}(\tilde{x}|\tilde{y})\|\bm{q}(\tilde{x}|\tilde{y}))=\mathbb{ E}^{\bm{\eta}}\left[\log\frac{\bm{\eta}(\tilde{x}|\tilde{y})}{\bm{q}(\tilde{x}| \tilde{y})}\right]\,.\]

with \(\mathbb{E}^{\bm{\eta}}[f(\tilde{x},\tilde{y})]\) as a shorthand notation to indicate that \((\tilde{x},\tilde{y})\sim\bm{\eta}\). We can now decompose EVaR from its definition in (3) as

\[\mathrm{EVaR}_{\alpha}\left[\tilde{x}\right]=\inf_{\bm{\eta}\in \Delta_{m\cdot N}:\bm{\eta}\ll\bm{q}}\left\{\mathbb{E}^{\bm{\eta}}[\tilde{x}] \mid\mathrm{KL}(\bm{\eta}\mid\bm{q})\leq-\log\alpha\right\}\] \[\overset{\text{(a)}}{=}\inf_{\bm{\eta}\in\Delta_{m\cdot N}:\bm{ \eta}\ll\bm{q}}\left\{\mathbb{E}^{\bm{\eta}}[\tilde{x}]\mid\mathrm{KL}(\bm{ \eta}(\tilde{y})\|\bm{q}(\tilde{y}))+\mathrm{KL}(\bm{\eta}(\tilde{x}|\tilde{y} )\|\bm{q}(\tilde{x}|\tilde{y}))\leq-\log\alpha\right\}\] \[=\inf_{\bm{\eta}\in\Delta_{m\cdot N}:\bm{\eta}\ll\bm{q}}\left\{ \mathbb{E}^{\bm{\eta}}[\tilde{x}]\mid\mathrm{KL}(\bm{\eta}(\tilde{y})\|\bm{q} (\tilde{y}))+\mathbb{E}^{\bm{\eta}}\left[\mathbb{E}^{\bm{\eta}}\left[\log \frac{\bm{\eta}(\tilde{x}|\tilde{y})}{\bm{q}(\tilde{x}|\tilde{y})}\right]\mid \tilde{y}\right]\leq-\log\alpha\right\}\] \[\overset{\text{(b)}}{=}\inf_{\bm{\eta}\in\Delta_{m\cdot N}:\bm{ \zeta}\in(0,1]^{N}:\bm{\eta}\ll\bm{q}}\left\{\mathbb{E}^{\bm{\eta}}[\mathbb{E }^{\bm{\eta}}[\tilde{x}\mid\tilde{y}]]\mid\begin{array}{c}\mathrm{KL}(\bm{ \eta}(\tilde{y})\|\bm{q}(\tilde{y}))+\mathbb{E}^{\bm{\eta}}[-\log(\zeta_{ \tilde{y}})]\leq-\log\alpha\\ \mathbb{P}_{\bm{\eta}}\left[\mathbb{E}^{\bm{\eta}}\left[\log(\bm{\eta}(\tilde {x}|\tilde{y})/\bm{q}(\tilde{x}|\tilde{y})\right]\mid\tilde{y}\right]\leq-\log (\zeta_{\tilde{y}})]=1\end{array}\right\}\] \[\overset{\text{(c)}}{=}\inf_{\bm{\xi}\in\Delta_{N},\bm{\zeta}\in (0,1]^{N}:\bm{\xi}\ll\bm{\hat{p}}}\left\{\mathbb{E}^{\bm{\xi}}[\mathrm{EVaR}_ {\zeta_{\tilde{y}}}\left[\tilde{x}\right]\mid\mathrm{KL}(\bm{\xi}\|\tilde{p})+ \mathbb{E}^{\bm{\xi}}[-\log(\zeta_{\tilde{y}})]\leq-\log\alpha\right\}\] \[=\inf_{\bm{\xi}\in\Delta_{N},\bm{\zeta}\in(0,1]^{N}:\bm{\xi}\ll \bm{\hat{p}}}\left\{\sum_{i}\xi_{i}\,\mathrm{EVaR}_{\zeta_{i}}\left[\tilde{x }\mid\tilde{y}=i\right]\mid\sum_{i=1}^{N}\xi_{i}\log(\xi_{i}/\hat{p}_{i})-\sum _{i=1}^{N}\xi_{i}\log(\zeta_{i})\leq-\log\alpha\right\}.\]

Here, we decompose the relative entropy of \(\bm{\eta}\) and \(\bm{q}\) using (23) in step (a) and then use the tower property of the expectation operator in the next step. In step (b), we introduce a variable \(\zeta_{i}\) for each realization of \(\tilde{y}=i\) with \(i\in\mathcal{N}\) to decouple the influence of \(\bm{\eta}(\bm{\tilde{x}}|\tilde{y})\), under each \(\tilde{y}\), in the inequality constraint. Finally, we replace the conditional EVaR definition by solving for \(\bm{\eta}(\bm{\tilde{x}}|\tilde{y})\) for a given \(\bm{\zeta}\) in step (c), and representing \(\bm{\eta}(\bm{\tilde{y}})\) using \(\bm{\xi}\). 

The first part of our theorem follows directly from Proposition A.1. Suppose that \(\alpha>0\); the result follows for \(\alpha=0\) because \(\mathrm{EVaR}_{0}\left[\cdot\right]\) reduces to \(\mathrm{ess\,inf}\). Then, the second part of the corollary holds as

\[\mathrm{EVaR}_{\alpha}\left[r(\tilde{s},\tilde{a},\tilde{s}^{ \prime})\right]=\] \[=\inf_{\bm{\zeta}\in(0,1]^{N},\,\bm{\xi}\in\Delta_{N}}\left\{ \sum_{s\in\mathcal{S}}\xi_{s}\,\mathrm{EVaR}_{\zeta_{s}}\left[r(s,\tilde{a}, \tilde{s}^{\prime})\mid\tilde{s}=s\right]\mid\sum_{s\in\mathcal{S}}\xi_{s} \log\frac{\xi_{s}}{\zeta_{s}\hat{p}_{s}}\leq-\log\alpha\right\}\] \[\leq\inf_{\bm{\xi}\in(0,1]^{N},\,\bm{\xi}\in\Delta_{N}}\left\{ \sum_{s\in\mathcal{S}}\xi_{s}\,\mathrm{EVaR}_{\zeta_{s}}\left[r(s,\tilde{a}, \tilde{s}^{\prime})\mid\tilde{s}=s\right]\mid\sum_{s\in\mathcal{S}}\xi_{s}\log \frac{\xi_{s}}{\zeta_{s}\hat{p}_{s}}\leq-\log\alpha,\,\bm{\xi}\leq\alpha^{-1} \hat{\bm{p}}\right\}\] \[\leq\inf_{\bm{\xi}\in\Delta_{N}}\left\{\sum_{s\in\mathcal{S}}\xi_{s }\,\mathrm{EVaR}_{\alpha\xi_{s}\hat{p}_{s}^{-1}}\left[r(s,\tilde{a},\tilde{s}^ {\prime})\mid\tilde{s}=s\right]\mid\bm{\xi}\leq\alpha^{-1}\hat{\bm{p}}\right\}\,.\]

The first inequality follows from adding a constraint on the pairs on the \(\bm{\xi}\) considered by the infimum. The second inequality follows by fixing \(\zeta_{s}=\hat{\zeta}_{s}\) with \(\hat{\zeta}_{s}=\alpha\xi_{s}\hat{p}_{s}^{-1}\) for each \(s\in\mathcal{S}\). This is an upper bound because \(\hat{\zeta}_{s}\) is feasible in the infimum:

\[\sum_{s\in\mathcal{S}}\xi_{s}\log\frac{\xi_{s}}{\hat{\zeta}_{s}\hat{p}_{s}}=-\log \alpha\leq-\log\alpha\,.\]

The value \(\hat{\zeta}_{s}\) is well-defined since \(\hat{p}_{s}>0\) and the constraint \(\bm{\xi}\leq\alpha^{-1}\hat{\bm{p}}\) ensures that \(\hat{\zeta}_{s}\leq 1\). Also, we can relax the constraint \(\zeta_{s}>0\Rightarrow\xi_{s}>0\) to \(\xi_{s}\geq 0\) because \(\mathrm{EVaR}_{0}\left[\tilde{x}\right]=\lim_{\alpha\to 0}\mathrm{ EVaR}_{\alpha}\left[\tilde{x}\right]\), and, therefore, the infimum is not affected. Finally, the inequality in the corollary follows immediately by further upper bounding the decomposition above by adding a constraint.

### Proof of Theorem 5.2

The equality develops from Theorem 5.1 as

\[\max_{\pi\in\Pi}\ \mathrm{VaR}_{\alpha}^{\tilde{a}\sim\bm{\pi}(\tilde{s})}[r (\tilde{s},\tilde{a},\tilde{s}^{\prime})] =\ \max_{\pi\in\Pi}\ \underset{\bm{\zeta}\in\Delta_{S}:\alpha\cdot\bm{\zeta}\leq\tilde{\bm{p}}}{ \sup}\ \min_{s\in\tilde{\mathcal{S}}}\ \min_{s\in\tilde{\mathcal{S}}}\ \Big{(}\mathrm{VaR}_{\alpha \zeta_{s}\tilde{p}_{s}^{-1}}^{\tilde{a}\sim\bm{\pi}(s)}[r(s,\tilde{a},\tilde{s} ^{\prime})\mid\tilde{s}=s]\Big{)}\] \[=\ \underset{\bm{\zeta}\in\Delta_{S}:\alpha\cdot\bm{\zeta}\leq \tilde{\bm{p}}}{\sup}\ \max_{s\in\tilde{\mathcal{S}}}\ \min_{s\in\tilde{\mathcal{S}}}\ \min_{\bm{\zeta}\in\tilde{\mathcal{S}}}\ \Big{(}\mathrm{VaR}_{\alpha \zeta_{s}\tilde{p}_{s}^{-1}}^{\tilde{a}\sim\bm{\pi}(s)}[r(s,\tilde{a},\tilde{s} ^{\prime})\mid\tilde{s}=s]\Big{)}\] \[=\ \underset{\bm{\zeta}\in\Delta_{S}:\alpha\cdot\bm{\zeta}\leq \tilde{\bm{p}}}{\sup}\ \min_{s\in\tilde{\mathcal{S}}}\ \Big{(}\max_{\bm{d}\in\Delta_{A}}\mathrm{VaR}_{\alpha \zeta_{s}\tilde{p}_{s}^{-1}}^{\tilde{a}\sim\bm{d}}[r(s,\tilde{a},\tilde{s}^{ \prime})\mid\tilde{s}=s]\Big{)},\]

where we first change the order of maximum and supremum, followed by changing the order of \(\max_{\pi}\min_{s}\) with \(\min_{s}\max_{\pi}\). The latter is a direct consequence of the interchangeability property of the maximum operation (Shapiro, 2017, proposition 2.2). 

### Proof of Theorem 5.3

The proof mainly relies on correcting the decomposition of lower quantile proposed in (Li et al., 2022).

**Proposition A.2**.: _Given an \(\tilde{z}\in\mathbb{X}\), suppose that a random variable \(\tilde{y}\colon\Omega\to\mathcal{N}=\{1,\ldots,N\}\) is distributed as \(\bm{\hat{p}}=(\hat{p}_{i})_{i=1}^{N}\) with \(\hat{p}_{i}>0\). Then:_

\[Q_{\alpha}(\tilde{x}) =\ \ \underset{\bm{\zeta}\in[0,1]^{N}}{\sup}\ \left\{\min_{i\in\mathcal{N}:\zeta_{i}<1}\ Q_{\zeta_{i}}(\tilde{x}\mid \tilde{y}=i)\mid\sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}<\alpha\right\}\,,\] (24)

_where we interpret the supremum to be minus infinity if its feasible set is empty, which only occurs if \(\alpha=0\)._

Proof.: First, we decompose the lower quantile using its definition as

\[Q_{\alpha}(\tilde{x}) =\sup\ \{z\mid\mathbb{P}\left[\tilde{x}<z\right]<\alpha\} \overset{\text{\rm{(a)}}}{=}\underset{z\in\mathbb{R}}{\sup}\ \left\{z\mid\sum_{i=1}^{N}\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right] \hat{p}_{i}<\alpha\right\}\] \[\overset{\text{\rm{(b)}}}{=}\underset{z\in\mathbb{R},\bm{\zeta} \in[0,1]^{N}}{\sup}\ \left\{z\mid\sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}<\alpha,\ \mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]<\zeta_{i},\forall i\in \mathcal{N}:\zeta_{i}<1\right\}\] \[\overset{\text{\rm{(c)}}}{=}\underset{z\in\mathbb{R},\bm{\zeta} \in[0,1]^{N}}{\sup}\ \left\{z\mid z<Q_{\zeta_{i}}(\tilde{x}\mid\tilde{y}=i),\forall i\in \mathcal{N}:\zeta_{i}<1,\ \sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}<\alpha\right\}\] \[\overset{\text{\rm{(d)}}}{=}\underset{\bm{\zeta}\in[0,1]^{N}}{ \sup}\ \left\{\underset{z\in\mathbb{R}}{\sup}\ \left\{z\mid z<Q_{\zeta_{i}}(\tilde{x}\mid\tilde{y}=i),\forall i\in \mathcal{N}:\zeta_{i}<1\right\}\mid\sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}<\alpha\right\}\] \[\overset{\text{\rm{(e)}}}{=}\underset{\bm{\zeta}\in[0,1]^{N}}{ \sup}\ \left\{\underset{i\in\mathcal{N}:\zeta_{i}<1}{\min}\ \left\{\underset{i\in\mathcal{N}:\zeta_{i}<1}{\min}\ Q_{\zeta_{i}}(\tilde{x} \mid\tilde{y}=i)\mid\sum_{i=1}^{N}\zeta_{i}\hat{p}_{i}<\alpha\right\}.\]

We decompose the probability \(\mathbb{P}\left[\tilde{x}<z\right]\) in terms of the conditional probabilities \(\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\) in step (a) and then lower-bound them by an auxiliary variable \(\zeta_{i}\) in step (b). In step (c), we exploit the following equivalence:

\[\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]<\zeta_{i}\quad \Leftrightarrow\quad z<Q_{\zeta_{i}}(\tilde{x}\mid\tilde{y}=i)\]

The direction \(\Leftarrow\) in the equivalence follows from the definition of \(Q_{\zeta_{i}}(\tilde{x}\mid\tilde{y}=i)\):

\[z<Q_{\zeta_{i}}(\tilde{x}\mid\tilde{y}=i)=\inf\left\{z\mid \mathbb{P}\left[\tilde{x}\leq z\mid\tilde{y}=i\right]\geq\zeta_{i}\right\} \Rightarrow\ \mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]<\zeta_{i}.\]

The direction \(\Rightarrow\) follows from the definition of VaR (see equation (1)), which implies that VaR upper-bounds any \(z\) that satisfies the left-hand condition:

\[\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]<\zeta_{i}\ \Rightarrow\ Q_{\zeta_{i}}( \tilde{x}\mid\tilde{y}=i)=\sup\ \left\{z\in\mathbb{R}\mid\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]<\zeta_ {i}\right\}\geq z,\]

yet \(Q_{\zeta_{i}}(\tilde{x}\mid\tilde{y}=i)\neq z\) otherwise since \(\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]\) is right continuous, there must exist some \(\epsilon>0\) for which \(\mathbb{P}\left[\tilde{x}<z+\epsilon\mid\tilde{y}=i\right]<\zeta_{i}\) hence:

\[z=\sup\ \left\{z\in\mathbb{R}\mid\mathbb{P}\left[\tilde{x}<z\mid\tilde{y}=i\right]< \zeta_{i}\right\}\geq z+\epsilon>z,\]

which leads to a contradiction. In step (e), we solve for \(z\). Finally, we obtain the form in (24).

## Appendix B CVaR Suboptimal Policy

In this section, we construct a simple MDP to demonstrate that the suboptimality of the CVaR decomposition discussed in Section 3 can also lead to computing a suboptimal policy. First, we give a particular example in which the policy computed according to Chow et al. (2015) is suboptimal. Then, we show that the sub-optimality gap can be arbitrarily large.

To demonstrate the suboptimality of the policy computed in Chow et al. (2015), consider an MDP with two states and three actions and a horizon \(T=1\) as shown in Figure 4. Also let \(M=600\) and

Figure 4: An example used to show the sub-optimality of a policy in Appendix B.

Figure 5: Return computed in Chow et al. (2015) vs optimal CVaR policy for the example in Figure 4 with \(p_{s_{1}}=p_{s_{2}}=0.5\) and \(M=600\). The optimal CVaR policy for each \(\alpha\) is denoted by \(\pi^{\star}\). The value function \(\hat{v}\) is computed according to the decomposition in the r.h.s. of (8) and the corresponding policy is \(\hat{\pi}\).

\(p_{s_{1}}=p_{s_{2}}=0.5\). An optimal solution for the \(s_{1}\) sub-problem in this example is

\[\mathrm{CVaR}_{\alpha^{\prime}}[r(\tilde{s})\ |\ \tilde{s}=s_{1}]\quad= \quad\begin{cases}\mathrm{CVaR}_{\alpha^{\prime}}[r(s_{1},a_{2}, \tilde{s}^{\prime})]&\text{if }\alpha^{\prime}<0.5\\ \mathrm{CVaR}_{\alpha^{\prime}}[r(s_{1},a_{1},\tilde{s}^{\prime})]&\text{if } \alpha^{\prime}\geq 0.5.\end{cases}\]

Figure 5 depicts the sub-optimality demonstrated by the CVaR decomposition. CVaR decomposition for policy optimization would over-approximate (black dash) the value function and commit to a sub-optimal solution (red solid). This simple example answers the following important concerns on the suboptimality.

First, CVaR policy optimization decomposition in Chow et al. (2015) can lead one to choose a suboptimal policy. In particular, Figure 5 shows that the CVaR policy optimization decomposition is an overestimate of the return and the decision maker commits to the worst action for CVaR objective \(\alpha\in(0.25,625)\). Furthermore, action \(a_{3}\) is never chosen for (Chow et al., 2015) even though it is the only optimal action for \(\alpha\in(0.375,0.6875)\). We can see from Figure 6 when we inspect the \(\zeta\) function for \(\alpha=0.5\) that \(a_{3}\) is optimal with CVaR return of \(50\). However, the CVaR decomposition would select \(a_{1}\) or \(a_{2}\) and over-estimate the CVaR return to \(100\), but the sub-optimal actions \(a_{1}\) or \(a_{2}\) would have the true CVaR return of \(0\) instead.

Second, there is no upper bound on the sub-optimality of the action chosen with respect CVaR return of the optimal action. The regret of the sub-optimal return is dependent on the distribution of the sub-optimal action and unbounded with respect to the true optimal return, as the performance of the sub-optimal return for \(\alpha\in(0.25,0.5)\) is worse than the worst case scenario of the best worst-case policy. Increasing the magnitude (\(M>400\)) for the reward instead of the sub-optimal action \(a_{1}\) in \(s_{1}\) in Figure 4 increases the sub-optimality and can be arbitrarily large.

Third, the sub-optimality can occur for any \(\alpha\in(0,1)\). As above, we use the MDP in Figure 4 and perturb two values to demonstrate the suboptimality by (i) increasing the magnitude (\(M>400\)) for the reward of the sub-optimal action \(s_{1},a_{1}\), or (ii) increasing the initial state probability (\(p_{s_{2}}\)). That leads to increasing the range where the CVaR decomposition for policy optimization yields a sub-optimal policy as shown in Figure 7.

Figure 7: CVaR decomposition suboptimality for different \(M\) and \(p_{s2}\) for MDP defined in Figure 4

Extension to Longer Horizon

For completeness, we include in this section an extension of the corrected decomposition presented in Theorem 5.2 to horizon \(T>1\). Our demonstration establishes that the decomposition proposed in Theorem 1 and 3 of (Li et al., 2022) actually applies to the upper quantile (i.e. value-at-risk) of the cumulated reward rather than the lower one as claimed by the authors. In comparison with the proof found in (Li et al., 2022), we pay close attention to demonstrating that the supremum in (5.1) is attained when finite and that deterministic policies are optimal.

Given any finite MDP with horizon \(T>0\) and \(\alpha\in[0,1]\), we define the initial value as

\[v_{0}:=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s\in\mathcal{S}}\ \max_{a\in\mathcal{A}}q_{1}(s,a,\alpha\zeta_{s}\tilde{p}_{s}^{-1})\mid\alpha \cdot\bm{\zeta}\leq\bm{\hat{p}}\right\}.\] (25)

The state-action value function \(q_{t}\colon\mathcal{S}\times\mathcal{A}\times[0,1]\to\mathbb{R}\) for the terminal step \(t=T+1\) and \(\alpha\in[0,1]\) and each \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\) is defined as

\[q_{T+1}(s,a,\alpha):=\begin{cases}\infty&\text{ if }\alpha=1\\ 0&\text{ otherwise.}\end{cases}\]

For each \(t=1,\ldots,T\) and \(\alpha<1\) the state-action function is defined recursively as

\[q_{t}(s,a,\alpha):=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\max_{a^{\prime}\in\mathcal{A}}r(s,a,s^{\prime})+q_{t+1}(s^{\prime },a^{\prime},\alpha\zeta_{s^{\prime}}p_{ss^{\prime}}^{-1})\mid\alpha\cdot\bm{ \zeta}\leq\bm{p}_{sa}\right\}.\] (26)

For \(\alpha=1\), the state-action value function is defined as

\[q_{t}(s,a,1):=\infty.\]

To construct the optimal policy, we also need to define a history-dependent risk level \(\bar{\alpha}_{t}\colon\mathcal{S}^{t}\times\mathcal{A}^{t-1}\to[0,1]\) that satisfies for each \(t=1,\ldots,T\) and \(s_{1},\ldots,s_{t}\) and \(a_{1},\ldots,a_{t-1}\) that

\[q_{t}(s,a,\bar{\alpha}_{t}(s_{1:t},a_{1:t-1}))=\min_{s^{\prime}\in\mathcal{S} }\max_{a^{\prime}\in\mathcal{A}}r(s,a,s^{\prime})+q_{t+1}(s^{\prime},a^{\prime },\bar{\alpha}_{t+1}([s_{1:t},s^{\prime}],[a_{1:t-1},a^{\prime}]).\]

The appropriate values \(\bar{\alpha}_{t}\) can be readily recovered from the optimal solution to (26). Finally, letting

\[a_{t}^{*}(s_{t},\alpha_{t})\in\begin{cases}\mathcal{A}&\text{ if }\alpha_{t}=1\\ \arg\max_{a\in\mathcal{A}}q_{t}(s_{t},a,\alpha_{t})&\text{ otherwise },\end{cases}.\] (27)

we construct a _deterministic_ history-dependent policy \(\bar{\pi}_{t}\colon\mathcal{S}^{t}\times\mathcal{A}^{t-1}\to\mathcal{A}\) for each \(t=1,\ldots,T\) and \(s_{1},\ldots,s_{t}\) and \(a_{1},\ldots,a_{t-1}\) that puts all the probability mass on \(a_{t}^{*}(s_{t},\bar{\alpha}_{t}(s_{1:t},a_{1:t-1}))\), i.e.

\[\mathbb{P}^{\bar{a}_{t}\sim\bar{\pi}_{t}(s_{1:t},a_{1:t-1})}(\tilde{a}_{t}=a_ {t}^{*}(s_{t},\bar{\alpha}_{t}(s_{1:t},a_{1:t-1})))=1.\] (28)

The following theorem states that the value function defined above represents the VaR of the returns of the optimal policy. Moreover, the history-dependent policy \(\bar{\pi}\) above is optimal and attains the optimal VaR return.

**Theorem C.1**.: _For a finite horizon \(T\) and any \(\alpha\in[0,1]\) we have that \(v_{0}\) defined in (25) coincides with the optimal VaR return:_

\[v_{0}\ =\ \max_{\pi\in\Pi}\ \mathrm{VaR}_{\alpha}^{\tilde{a}_{t}\sim\bm{\pi}_{t}( \tilde{s}_{1:t},\tilde{a}_{1:t-1})}\left[\sum_{t=1}^{T}r(\tilde{s}_{t},\tilde{a }_{t},\tilde{s}_{t+1})\right].\]

_Moreover, this optimal return is attained by the policy \((\bar{\pi}_{t})_{t=1}^{T}\) defined in (27):_

\[v_{0}\ =\ \mathrm{VaR}_{\alpha}^{\tilde{a}_{t}\sim\bar{\pi}_{t}(\tilde{s}_{1 :t},\tilde{a}_{1:t-1})}\left[\sum_{t=1}^{T}r(\tilde{s}_{t},\tilde{a}_{t},\tilde{ s}_{t+1})\right].\]

In proving Theorem C.1, we will make use of the upper semicontinuity property of \(\mathrm{VaR}_{\alpha}[\tilde{x}]\), which is presented in the following Lemma for completeness.

**Lemma C.2**.: _The function \(\mathrm{VaR}_{\alpha}[\tilde{x}]\) is upper semicontinuous in \(\alpha\) on the interval \(\alpha\in[0,1]\)._Proof.: This follows from the fact that \(\mathrm{VaR}_{\alpha}[\tilde{x}]\) is non-decreasing and right-continuous in terms of \(\alpha\) (see Lemma A.19 in (Follmer and Schied, 2016)). 

Proof of Theorem c.1.: The proof consists of two steps addressing the two equalities stated by the theorem.

**Step 1:** For all \(t=1,\ldots,T\), we let

\[\hat{q}_{t}(s_{t},a_{t},\alpha_{t}):=\] \[\max_{\pi\in\Pi_{t+1:T}}\mathrm{VaR}_{\alpha_{t}}^{\tilde{a}_{t^ {\prime}}\sim\boldsymbol{\pi}_{t^{\prime}}(\tilde{s}_{t+1,t^{\prime}},\tilde{ a}_{t+1,t^{\prime}-1})}\left[r(s_{t},a_{t},\tilde{s}_{t+1})+\sum_{t^{\prime}=t+1}^{T}r( \tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1})\mid s _{t},a_{t}\right]\]

where \(\Pi_{t+1:T}\) refers to the set of all history-dependent policies starting from time \(t+1\), and where \(\mathrm{VaR}_{\alpha}[\tilde{x}|s_{t},a_{t}]\) is short for \(\mathrm{VaR}_{\alpha}[\tilde{x}|\tilde{s}_{t}=s_{t},\tilde{a}_{t}=a_{t}]\). We will first show how \(\hat{q}_{t}(s,a,\alpha)=q_{t}(s,a,\alpha)\) for all \(t=1,\ldots,T\), \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), and \(\alpha\in[0,\,1]\). We follow with our conclusion about \(v_{0}\).

First, addressing the case \(\alpha=1\), one can easily see that for all \(t=1,\ldots,T\), \(s\in\mathcal{S}\), and \(a\in\mathcal{A}\), we have that

\[\hat{q}_{t}(s_{t},a_{t},1) =\max_{\pi\in\Pi_{t+1:T}}\mathrm{VaR}_{1}^{\tilde{a}_{t^{\prime} }\sim\boldsymbol{\pi}_{t^{\prime}}(\tilde{s}_{t+1,t^{\prime}},\tilde{a}_{t+1,t ^{\prime}-1})}\left[r(s_{t},a_{t},\tilde{s}_{t+1})+\sum_{t^{\prime}=t+1}^{T}r( \tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1})\mid s _{t},a_{t}\right]\] \[=\infty\;=\;q_{t}(s_{t},a_{t},1)\,,\]

based on our definition of value-at-risk for \(\alpha=1\).

Next, for \(0\leq\alpha<1\) and using \(\boldsymbol{p}\) short for \(\boldsymbol{p}_{sa}\) when can start looking at time \(t=T\), where we have that for all \(s_{T}\in\mathcal{S}\), and \(a_{T}\in\mathcal{A}\) :

\[\hat{q}_{T}(s_{T},a_{T},\alpha_{T}) =\mathrm{VaR}_{\alpha_{T}}[r(s_{T},a_{T},\tilde{s}_{t+1})|s_{T},a _{T}]\] \[=\sup_{\boldsymbol{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime} \in\mathcal{S}}\,\mathrm{VaR}_{\alpha_{T}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1 }}[r(s_{T},a_{T},s^{\prime})]\mid\alpha_{T}\cdot\boldsymbol{\zeta}\leq \boldsymbol{p}\right\}\] \[=\max_{\boldsymbol{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime} \in\mathcal{S}}\,\mathrm{VaR}_{\alpha_{T}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1 }}[r(s_{T},a_{T},s^{\prime})]\mid\alpha_{T}\cdot\boldsymbol{\zeta}\leq \boldsymbol{p}\right\}\] \[=\max_{\boldsymbol{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime} \in\mathcal{S}}\,r(s_{T},a_{T},s^{\prime})+\mathrm{VaR}_{\alpha_{T}\zeta_{s^{ \prime}}p_{s}^{-1}}[0]\mid\alpha_{T}\cdot\boldsymbol{\zeta}\leq\boldsymbol{p}\right\}\] \[=\max_{\boldsymbol{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime} \in\mathcal{S}}\,r(s_{T},a_{T},s^{\prime})+\max_{a^{\prime}\in\mathcal{A}}q_{ T+1}(s^{\prime},a^{\prime},\alpha_{T}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1})\mid \alpha_{T}\cdot\boldsymbol{\zeta}\leq\boldsymbol{p}\right\}\] \[=q_{T}(s_{T},a_{T},\alpha_{T}),\]

where we first use the definition of \(\hat{q}_{T}(s,a,\alpha)\), then the decomposition of \(\mathrm{VaR}\) in Theorem 5.1. We follow with confirming, based on extreme value theory, that the supremum over \(\boldsymbol{\zeta}\) must be achieved given that \(\mathrm{VaR}_{\alpha}(\tilde{x})\) of a random variable \(\tilde{x}\) is upper semi-continuous (usc) in \(\alpha\) (see Lemma C.2), that the minimum of a finite set of us functions is usc, and that \(\Delta_{S}\) is compact. The other two steps follow from the translation invariance of \(\mathrm{VaR}\) and the definition of \(q_{T+1}(s,a,\alpha)\).

Moreover, for all \(1\leq t<T\) let \(\tilde{h}_{t:t^{\prime}}:=(\tilde{s}_{t:t^{\prime}},\tilde{a}_{t:t^{\prime}-1})\) as the history between \(t\) and \(t^{\prime}\). Using the short-hand \(\tilde{r}_{t^{\prime}}=r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{ s}_{t^{\prime}+1})\), and when \(0\leq\alpha_{t}<1\), we have that:

\[\hat{q}_{t}(s_{t},a_{t},\alpha_{t})=\max_{\pi\in\Pi_{t+1:T}}\ \mathrm{VaR}_{ \alpha_{t}}^{\tilde{a}_{t^{\prime}}\sim\boldsymbol{\pi}_{t^{\prime}}(\tilde{h}_{t +1,t^{\prime}})}[r(s_{t},a_{t},\tilde{s}_{t+1})+\sum_{t^{\prime}=t+1}^{T}r( \tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1})|s_{t},a _{t}]\] \[=\max_{\pi\in\Pi_{t+1:T}}\ \sup_{\boldsymbol{\zeta}\in\Delta_{S}} \left\{\min_{s^{\prime}\in\mathcal{S}}\,\mathrm{VaR}_{\alpha_{t}\zeta_{s^{ \prime}}p_{s^{\prime}}^{-1}}^{\tilde{a}_{t^{\prime}}\sim\boldsymbol{\pi}_{t^{ \prime}}(\tilde{h}_{t+1,t^{\prime}})}[r(s_{t},a_{t},s^{\prime})+\sum_{t^{\prime}=t+1 }^{T}\tilde{r}_{t^{\prime}}\mid\tilde{s}_{t+1}=s^{\prime}]\mid\alpha_{t}\cdot \boldsymbol{\zeta}\leq\boldsymbol{p}\right\}\] \[=\max_{\pi\in\Pi_{t+1:T}}\ \max_{\boldsymbol{\zeta}\in\Delta_{S}} \left\{\min_{s^{\prime}\in\mathcal{S}}\,\mathrm{VaR}_{\alpha_{t}\zeta_{s^{ \prime}}p_{s^{\prime}}^{-1}}^{\tilde{a}_{t^{\prime}}\sim\boldsymbol{\pi}_{t^{ \prime}}(\tilde{h}_{t+1,t^{\prime}})}[r(s_{t},a_{t},s^{\prime})+\sum_{t^{ \prime}=t+1}^{T}\tilde{r}_{t^{\prime}}\mid\tilde{s}_{t+1}=s^{\prime}]\mid \alpha_{t}\cdot\boldsymbol{\zeta}\leq\boldsymbol{p}\right\}\]\[=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\max_{\pi\in\Pi_{t+1:T}}\ \min_{s^{\prime}\in\mathcal{S}}\ \mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t^{\prime}}(\tilde{h}_{t+1:t ^{\prime}})}_{\alpha_{t}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1}}[r(s_{t},a_{t},s ^{\prime})+\sum_{t^{\prime}=t+1}^{T}\tilde{r}_{t^{\prime}}\mid\tilde{s}_{t+1}=s ^{\prime}]\mid\alpha_{t}\cdot\bm{\zeta}\leq\bm{p}\right\}\] \[=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\max_{\bm{d}\in\Delta_{A},\{\pi^{a}\}_{a\in\mathcal{A}}\in\Pi_{t+2 :T}^{|\mathcal{A}|}}\ \mathrm{VaR}^{\tilde{a}_{t+1}\sim\bm{d},\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t^ {\prime}}^{\tilde{a}_{t}+1}(\tilde{h}_{t+2:t^{\prime}})}_{\alpha_{t}\zeta_{s^{ \prime}}p_{s^{\prime}}^{-1}}[r(s_{t},a_{t},s^{\prime})+\right.\] \[\left.\sum_{t^{\prime}=t+1}^{T}\tilde{r}_{t^{\prime}}\mid\tilde{s}_ {t+1}=s^{\prime}]\mid\alpha_{t}\cdot\bm{\zeta}\leq\bm{p}\right\}\] \[=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}r(s_{t},a_{t},s^{\prime})+\right.\] \[\max_{d\in\Delta_{A},\{\pi^{a}\}_{a\in\mathcal{A}}\in\Pi_{t+2:T}^ {|\mathcal{A}|}}\ \mathrm{VaR}^{\tilde{a}_{t+1}\sim\bm{d},\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t ^{\prime}}^{\tilde{a}_{t}+1}(\tilde{h}_{t+2:t^{\prime}})}_{\alpha_{t}\zeta_{s^ {\prime}}p_{s^{\prime}}^{-1}}[\sum_{t^{\prime}=t+1}^{T}\tilde{r}_{t^{\prime}} \mid\tilde{s}_{t+1}=s^{\prime}]\mid\alpha_{t}\cdot\bm{\zeta}\leq\bm{p}\right\}\] \[=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}r(s_{t},a_{t},s^{\prime})+\hat{v}_{t+1}(s^{\prime},\alpha_{t}\zeta _{s^{\prime}}p_{s^{\prime}}^{-1})\mid\alpha_{t}\cdot\bm{\zeta}\leq\bm{p}\right\}\] (29)

where, as before, we start by exploiting the decomposition Theorem 5.1 and explicating that the supremum is attained. We then change the order of the two maximums, followed by changing the order of \(\max_{\pi}\min_{s^{\prime}}\) with \(\min_{s^{\prime}}\max_{\pi}\), which follows based on the interchangeability property of the minimum operation (Shapiro, 2017, proposition 2.2). We finally introduced a \((s,\alpha)\)-value function operator which can be reduced as follows:

\[\hat{v}_{t}(s,\alpha):=\max_{\bm{d}\in\Delta_{A},\{\pi^{a}\}_{a \in\mathcal{A}}\in\Pi_{t+1:T}^{|\mathcal{A}|}}\ \mathrm{VaR}^{\tilde{a}_{t}\sim\bm{d},\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t^ {\prime}}^{\tilde{a}_{t^{\prime}}}(\tilde{h}_{t+1:t})}_{\alpha}[\sum_{t^{ \prime}=t}^{T}r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{ \prime}+1})|\tilde{s}_{t}=s]\] \[=\max_{\{\pi^{a}\}_{a\in\mathcal{A}}\in\Pi_{t+1:T}^{|\mathcal{A}| }}\max_{d\in\Delta_{A}}\ \mathrm{VaR}^{\tilde{a}_{t}\sim\bm{d},\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t^ {\prime}}^{\tilde{a}_{t}}(\tilde{h}_{t+1:t})}_{\alpha}[\sum_{t^{\prime}=t}^{T }r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1})| \tilde{s}_{t}=s^{\prime}]\] \[=\max_{\{\pi^{a}\}_{a\in\mathcal{A}}\in\Pi_{t+1:T}^{|\mathcal{A}| }}\max_{d\in\Delta}\ \mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t^{\prime}}( \tilde{h}_{t+1:t})}_{\alpha}[r(s,a,\tilde{s}_{t+1})+\sum_{t^{\prime}=t+1}^{T }r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1})| \tilde{s}_{t}=s,\tilde{a}_{t}=a]\] \[=\max_{a\in\mathcal{A}}\max_{\pi\in\Pi_{t+1:T}}\ \mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bm{\pi}_{t^{\prime}}( \tilde{h}_{t+1:t^{\prime}})}_{\alpha}[r(s,a,\tilde{s}_{t+1})+\sum_{t^{\prime}=t +1}^{T}r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1}) |\tilde{s}_{t}=s,\tilde{a}_{t}=a]\] \[=\max_{a\in\mathcal{A}}\hat{q}_{t}(s,a,\alpha),\] (30)

where we exploited the fact that value-at-risk is a mixture quasi-convex function (Delage et al., 2019), meaning that it cannot be improved by a randomized policy.

Hence, replacing \(\hat{v}\) back in equation (29), we get

\[\hat{q}_{t}(s_{t},a_{t},\alpha_{t}) =\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}r(s_{t},a_{t},s^{\prime})+\max_{a\in\mathcal{A}}\hat{q}_{t+1}(s^{ \prime},\alpha_{t}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1})\mid\alpha_{t}\cdot\bm{ \zeta}\leq\bm{p}\right\}\] \[=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}r(s_{t},a_{t},s^{\prime})+\max_{a\in\mathcal{A}}q_{t+1}(s^{\prime}, \alpha_{t}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1})\mid\alpha_{t}\cdot\bm{\zeta} \leq\bm{p}\right\}\] \[=q_{t}(s_{t},a_{t},\alpha_{t}).\]

Finalizing our conclusion, we get using the short-hand \(\tilde{r}_{t}=r(\tilde{s}_{t},\tilde{a}_{t},\tilde{s}_{t+1})\) that

\[\max_{\pi\in\Pi}\ \mathrm{VaR}^{\tilde{a}_{t}\sim\bm{\pi}_{t}( \tilde{h}_{1:t})}_{\alpha}\left[\sum_{t=1}^{T}\tilde{r}_{t}\right]=\] \[=\max_{\pi\in\Pi}\ \sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\ \mathrm{VaR}^{\tilde{a}_{t}\sim\bm{\pi}_{t}(\tilde{h}_{1:t})}_{\alpha \zeta_{s^{\prime}}\tilde{p}_{s^{\prime}}^{-1}}\left[\sum_{t=1}^{T}\tilde{r}_{t }\mid\tilde{s}_{1}=s^{\prime}\right]\mid\alpha\cdot\bm{\zeta}\leq\bm{\tilde{p}}\right\}\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\max_{d\in\Delta_{A},\{\pi^{a}\}_{a\in\mathcal{A}}\in\Pi_{2:T}^{| \mathcal{A}|}}\ \mathrm{VaR}^{\tilde{a}_{1}\sim\bm{d},\tilde{a}_{t}\sim\bm{\pi}_{t}^{ \tilde{a}_{1}}(\tilde{h}_{2:t})}_{\alpha\zeta_{s^{\prime}}\tilde{p}_{s^{\prime} }^{-1}}\left[\sum_{t=1}^{T}\tilde{r}_{t}\mid\tilde{s}_{1}=s^{\prime}\right] \mid\alpha\cdot\bm{\zeta}\leq\bm{\tilde{p}}\right\}\]\[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in\mathcal{S}} \max_{a\in\mathcal{A},\pi\in\Pi_{2:T}}\text{VaR}^{\tilde{a}_{t}\sim\bm{\pi}_{t}( \tilde{h}_{2:t})}_{\alpha_{\zeta^{\prime}}\tilde{p}_{s^{\prime}}^{-1}}\left[ \sum_{t=1}^{T}\tilde{r}_{t}\mid\tilde{s}_{1}=s^{\prime}\right]\mid\alpha\cdot \bm{\zeta}\leq\hat{\bm{p}}\right\}\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\max_{a\in\mathcal{A}}\hat{q}_{1}(s^{\prime},a,\alpha\zeta_{s^{ \prime}}\tilde{p}_{s^{\prime}}^{-1})\mid\tilde{s}_{1}=s^{\prime}\right]\mid \alpha\cdot\bm{\zeta}\leq\hat{\bm{p}}\right\}\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left[\min_{s^{\prime}\in \mathcal{S}}\max_{a\in\mathcal{A}}q_{1}(s^{\prime},a,\alpha\zeta_{s^{\prime}} \tilde{p}_{s^{\prime}}^{-1})\mid\tilde{s}_{1}=s^{\prime}\right]\mid\alpha\cdot \bm{\zeta}\leq\hat{\bm{p}}\right]\] \[=v_{0}\,,\]

where we employ the same steps as for reducing \(\hat{q}_{t}\) to \(q_{t}\).

**Step 2:** Regarding the optimality of the proposed policy, we will show inductively that

\[\bar{\hat{q}}_{t}(s_{t},a_{t},\alpha_{t}):=\] \[\text{VaR}^{\tilde{a}_{t^{\prime}}\sim\bar{\bm{\pi}}_{t^{\prime}} (\tilde{h}_{1:t^{\prime}})}_{\alpha_{t}}\left[r(s_{t},a_{t},\tilde{s}_{t+1})+ \sum_{t^{\prime}=t+1}^{T}r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}}, \tilde{s}_{t^{\prime}+1})\mid s_{t},a_{t},\bar{\alpha}_{t}(\tilde{s}_{1:t}, \tilde{a}_{1:t-1})=\alpha_{t}\right]\]

is an upper envelope for \(\hat{q}_{t}(s_{t},a_{t},\alpha_{t})\) for all \(1\leq t\leq T\). Conditioning on \(\alpha_{t}\) in the definition above is necessary because the policy \(\hat{\bm{\pi}}\) depends on the risk level \(\alpha\). This can then be exploited to obtain the following inequality:

\[\text{VaR}^{\tilde{a}_{t}\sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t}) }_{\alpha_{t}\sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t})}[\sum_{t=1}^{T}r(\tilde{s }_{t},\tilde{a}_{t},\tilde{s}_{t+1})]=\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s\in\mathcal{S}} \text{VaR}^{\tilde{a}_{t}\sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t})}_{\alpha_{t} \sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t})}[\sum_{t=1}^{T}r(\tilde{s}_{t},\tilde{ a}_{t},\tilde{s}_{t+1})|\tilde{s}_{1}=s]\mid\alpha\cdot\bm{\zeta}\leq\hat{\bm{p}}\right\}\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s\in\mathcal{S}} \text{VaR}^{\tilde{a}_{t}\sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t})}_{\alpha_{t} \sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t})}[\sum_{t=1}^{T}r(\tilde{s}_{t},\tilde{ a}_{t},\tilde{s}_{t+1})|\tilde{s}_{1}=s,\tilde{a}_{1}=a_{1}^{*}(s,\bar{\alpha}_{1}(s)),\bar{\alpha}_{1}(\tilde{s}_{1})=\bar{\alpha}_{1}(s)]\right.\] \[\geq\min_{s\in\mathcal{S}}\,\text{VaR}^{\tilde{a}_{t}\sim\bar{ \bm{\pi}}_{t}(\tilde{h}_{1:t})}_{\tilde{\alpha}_{1}(s)}[\sum_{t=1}^{T}r( \tilde{s}_{t},\tilde{a}_{t},\tilde{s}_{t+1})|\tilde{s}_{1}=s,\tilde{a}_{1}=a_{ 1}^{*}(s,\bar{\alpha}_{1}(s)),\bar{\alpha}_{1}(\tilde{s}_{1})=\bar{\alpha}_{1} (s)]\] \[=\min_{s\in\mathcal{S}}\,\tilde{\tilde{q}}_{1}(s,a_{1}^{*}(s,\bar{ \alpha}_{1}(s)),\bar{\alpha}_{1}(s))\ \geq\ \min_{s\in\mathcal{S}}\,\hat{q}_{1}(s,a_{1}^{*}(s,\bar{ \alpha}_{1}(s)),\bar{\alpha}_{1}(s))\] \[=\min_{s\in\mathcal{S}}\,q_{1}(s,a_{1}^{*}(s,\bar{\alpha}_{1}(s)),\bar{\alpha}_{1}(s))\ =\ \min_{s\in\mathcal{S}}\,\max_{a\in\mathcal{A}}\,q_{1}(s,a_{ 1}(s))\] \[=\max_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s\in\mathcal{S}}\max_{a \in\mathcal{A}}\,q_{1}(s,a,\bar{\alpha}_{1}(s))\mid\alpha\cdot\bm{\zeta}\leq \hat{\bm{p}}\right\}=v_{0}\] \[=\max_{\pi\in\Pi}\text{VaR}^{\tilde{a}_{t}\sim\bar{\bm{\pi}}_{t}( \tilde{h}_{1:t})}_{\alpha}\left[\sum_{t=1}^{T}r(\tilde{s}_{t},\tilde{a}_{t}, \tilde{s}_{t+1})\right]\] \[\geq\text{VaR}^{\tilde{a}_{t}\sim\bar{\bm{\pi}}_{t}(\tilde{h}_{1:t })}_{\alpha}\left[\sum_{t=1}^{T}r(\tilde{s}_{t},\tilde{a}_{t},\tilde{s}_{t+1}) \right],\]

where the first step comes from the decomposition of \(\text{VaR}\), the second from the definition of \(\bar{\pi}_{1}\). The third step follows from the feasibility of \(\zeta_{s}:=\bar{\alpha}_{1}(s)/(\alpha\hat{p}_{s}^{-1})\). We then exploit the definition of \(\bar{\hat{q}}_{1}\), the fact that it upper bounds \(\hat{q}_{1}\), the equivalence of \(\hat{q}_{1}\) and \(q_{1}\), and definition of \(a_{1}^{*}\). On the sixth line, we exploit the definition of \(\bar{\alpha}_{1}(s)\), followed with the definition of \(v_{0}\). These steps would therefore confirm that \(\bar{\pi}\) is optimal.

We are left with showing that \(\bar{\hat{q}}_{t}\) upper bounds \(\hat{q}_{t}\) for all \(t=1,\ldots,T\). Specifically, starting with \(t=T\), we have:

\[\bar{\hat{q}}_{T}(s_{T},a_{T},\alpha_{T})=\text{VaR}_{\alpha_{T}}[r(s_{T},a_{T}, \tilde{s}_{T+1})|s_{T},a_{T}]=\hat{q}_{T}(s_{T},a_{T},\alpha_{T})\] (31)Then, for \(t<T\) we focus on the case \(\alpha_{t}<1\), since otherwise we have \(\infty\geq\infty\). Specifically, we first define

\[\bar{v}_{t}(s_{t},\alpha_{t}):=\mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bar{\bm{ \pi}}_{t^{\prime}}(\tilde{h}_{1:t^{\prime}})}[r(s_{t},\tilde{a}_{t},\tilde{s}_ {t+1})+\sum_{t^{\prime}=t+1}^{T}r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime }},\tilde{s}_{t^{\prime}+1})|s_{t},\bar{\alpha}_{t}(\tilde{s}_{1:t},\tilde{a}_ {1:t-1})=\alpha_{t}],\]

which captures the value-at-risk at level \(\alpha_{t}\) of the reward-to-go looking forward from time \(t\) when running \(\bar{\pi}\) given that the history up to time \(t\) satisfies \(\bar{\alpha}_{t}(\tilde{s}_{1:t},\tilde{a}_{1:t-1})=\alpha_{t}\).

We then take inductive steps starting from \(t=T\) down to \(t=2\). At each \(t\), we can verify that if \(\bar{\tilde{q}}_{t}\) is an upper envelope for \(\hat{q}_{t}\), then :

\[\bar{v}_{t}(s_{t},1)=\infty\geq\infty=\max_{a\in\mathcal{A}}\hat{q}_{t}(s_{t}, a,1)=\hat{v}_{t}(s_{t},1)\]

and that for \(0\leq\alpha_{t}<1\):

\[\bar{v}_{t}(s_{t},\alpha_{t}) =\mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bar{\bm{\pi}}_{t^{ \prime}}(\tilde{h}_{1:t^{\prime}})}_{\alpha_{t}}\Big{[}r(s_{t},\tilde{a}_{t}, \tilde{s}_{t+1})\] \[\qquad\qquad+\sum_{t^{\prime}=t+1}^{T}r(\tilde{s}_{t^{\prime}}, \tilde{a}_{t^{\prime}},\tilde{s}_{t^{\prime}+1})\mid s_{t},\tilde{a}_{t}=a_{t }^{*}(s_{t},\alpha_{t}),\bar{\alpha}_{t}(\tilde{s}_{1:t},\tilde{a}_{1:t-1})= \alpha_{t}\Big{]}\] \[=\tilde{q}_{t}(s_{t},a_{s}^{*}(s_{t},\alpha_{t}),\alpha_{t})\geq \hat{q}_{t}(s_{t},a_{t}^{*}(s_{t},\alpha_{t}),\alpha_{t})\,=\,q_{t}(s_{t},a_{ t}^{*}(s_{t},\alpha_{t}),\alpha_{t})\] \[=\max_{a\in\mathcal{A}}q_{t}(s_{t},a,\alpha_{t})=\max_{a\in \mathcal{A}}\hat{q}_{t}(s_{t},a,\alpha_{t})\,=\,\hat{v}_{t}(s_{t},\alpha_{t}),\]

where we first used the definition of \(\bar{\pi}\), then used the definition of \(\bar{\tilde{q}}_{T}\), followed with the assumed property that \(\bar{\tilde{q}}_{t}\) is an upper envelope for \(\hat{q}_{t}\). We then employ the equivalence of \(\hat{q}_{t}\) and \(q_{t}\) twice, the definition of \(a_{t}^{*}\), and the relationship between \(\hat{v}_{t}\) and \(\hat{q}_{t}\) established in (30).

Next, we confirm that if \(\bar{v}_{t}\) is an upper envelope for \(\hat{v}_{t}\), then taking one step back and using the short-hand \(\tilde{r}_{t^{\prime}}=r(\tilde{s}_{t^{\prime}},\tilde{a}_{t^{\prime}},\tilde {s}_{t^{\prime}+1})\) we get:

\[\bar{\hat{q}}_{t-1}(s_{t-1},a_{t-1},\alpha_{t-1}):=\] \[\mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bar{\bm{\pi}}_{t^{ \prime}}(\tilde{h}_{1:t^{\prime}})}_{\alpha_{t-1}\zeta_{t^{\prime}}p_{s^{ \prime}}^{-1}}[r(s_{t-1},a_{t-1},\tilde{s}_{t})+\sum_{t^{\prime}=t}^{T}\tilde{ r}_{t^{\prime}}\mid s_{t-1},a_{t-1},\bar{\alpha}_{t-1}(\tilde{s}_{1:t-1},\tilde{a}_{1:t-2})= \alpha_{t-1}]\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+\right.\] \[\left.\mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bar{\bm{\pi}}_{t^ {\prime}}(\tilde{h}_{1:t^{\prime}})}_{\alpha_{t-1}\zeta_{t^{\prime}}p_{s^{ \prime}}^{-1}}[\sum_{t^{\prime}=t}^{T}\tilde{r}_{t^{\prime}}\mid\tilde{s}_{t}= s^{\prime},\bar{\alpha}_{t}(\tilde{s}_{1:t},\tilde{a}_{1:t-1})=g(\alpha_{t-1},s_{t-1},a_{t-1},s^{ \prime})]\mid\alpha_{t-1}\cdot\bm{\zeta}\leq\bm{p}\right\}\] \[\geq\min_{s^{\prime}\in\mathcal{S}}\,r(s_{t-1},a_{t-1},s^{ \prime})+\] \[\mathrm{VaR}^{\tilde{a}_{t^{\prime}}\sim\bar{\bm{\pi}}_{t^{ \prime}}(\tilde{h}_{1:t^{\prime}})}_{g(\alpha_{t-1},s_{t-1},a_{t-1},s^{ \prime})}[\sum_{t^{\prime}=t}^{T}\tilde{r}_{t^{\prime}}\mid s_{t}=s^{\prime}, \bar{\alpha}_{t}(\tilde{s}_{1:t},\tilde{a}_{1:t-1})=g(\alpha_{t-1},s_{t-1},a_ {t-1},s^{\prime})]\] \[=\min_{s^{\prime}\in\mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+ \bar{v}_{t}(s^{\prime},g(\alpha_{t-1},s_{t-1},a_{t-1},s^{\prime}))\] \[\geq\min_{s^{\prime}\in\mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+ \hat{v}_{t}(s^{\prime},g(\alpha_{t-1},s_{t-1},a_{t-1},s^{\prime}))\] \[=\min_{s^{\prime}\in\mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+ \max_{a\in\mathcal{A}}\hat{q}_{t}(s^{\prime},a,g(\alpha_{t-1},s_{t-1},a_{t-1},s^{ \prime}))\] \[=\min_{s^{\prime}\in\mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+ \max_{a\in\mathcal{A}}q_{t}(s^{\prime},a,g(\alpha_{t-1},s_{t-1},a_{t-1},s^{ \prime}))\] \[=\sup_{\bm{\zeta}\in\Delta_{S}}\left\{\min_{s^{\prime}\in \mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+\max_{a\in\mathcal{A}}q_{t}(s^{ \prime},a,\alpha_{t-1}\zeta_{s^{\prime}}p_{s^{\prime}}^{-1})\mid\alpha_{t-1}\cdot \bm{\zeta}\leq\bm{p}\right\}\]\[=\sup_{\xi\in\Delta_{S}}\left\{\min_{s^{\prime}\in\mathcal{S}}\,r(s_{t-1},a_{t-1},s^{\prime})+\hat{v}_{t}(s^{\prime},\alpha_{t-1}\zeta_{s^{\prime}}p_{s^{\prime}} ^{-1})\mid\alpha_{t-1}\cdot\bm{\zeta}\leq\bm{p}\right\}\] \[=\hat{q}_{t-1}(s_{t-1},a_{t-1},\alpha_{t-1}).\]

where

\[g(\alpha,s,a,s^{\prime}):=\alpha p_{s,a,s^{\prime}}^{-1}\cdot\left[\arg\max_{ \bm{\zeta}\in\Delta_{S}:\alpha^{\star}\leq\bm{p}_{t,a}}\min_{s^{\prime\prime} \in\mathcal{S}}r(s,a,s^{\prime\prime})+\max_{a\in\mathcal{A}}q_{t}(s,a,\alpha \zeta_{s^{\prime\prime}}p_{s,a,s^{\prime\prime}}^{-1})\right]_{s^{\prime}}\]

The first step comes from the decomposition of \(\mathrm{VaR}\), the second from the fact that \(\bar{\alpha}_{t}\) is known given that \(\bar{\alpha}_{t-1}\), \(\tilde{s}_{t}\), and \(\tilde{a}_{t-1}\) are fixed while \(\sum_{t^{\prime}=t}^{T}r(\tilde{s}_{\nu},\tilde{a}_{t^{\prime}},\tilde{s}_{t^ {\prime}+1})\) only depends on \(\tilde{s}_{t}\) and \(\bar{\alpha}_{t}\). The third step follows from replacing the supremum over \(\bm{\zeta}\) with a feasible member of \(\Delta_{S}\). The fourth step follows from the definition of \(\bar{v}_{t}\), followed with the fact that it upper bounds \(\hat{v}_{t}\). Finally, we exploit the definition of \(g(\cdot)\) and of \(\hat{q}_{t-1}(\cdot)\). This completes the inductive proof demonstrating that \(\tilde{\hat{q}}_{t}(\cdot)\) is an upper envelope for \(\hat{q}_{t}(\cdot)\) for all \(1\leq t\leq T\).