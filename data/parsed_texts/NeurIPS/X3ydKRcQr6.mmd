# MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings

 Laxman Dhulipala

Google Research and UMD

&Majid Hadian

Google DeepMind

&Rajesh Jayaram

Google Research

&Jason Lee

Google Research

&Vahab Mirrokni

Google Research

Corresponding Author: rkjayaram@google.com

###### Abstract

Neural embedding models have become a fundamental component of modern information retrieval (IR) pipelines. These models produce a single embedding \(x\in\mathbb{R}^{d}\) per data-point, allowing for fast retrieval via highly optimized maximum inner product search (MIPS) algorithms. Recently, beginning with the landmark ColBERT paper, _multi-vector models_, which produce a set of embedding per data-point, have achieved markedly superior performance for IR tasks. Unfortunately, using these models for IR is computationally expensive due to the increased complexity of multi-vector retrieval and scoring.

In this paper, we introduce Muvera (**M**ulti-**V**ector **R**etrieval **A**lgorithm), a retrieval mechanism which reduces _multi-vector_ similarity search to _single-vector_ similarity search. This enables the usage of off-the-shelf MIPS solvers for multi-vector retrieval. Muvera asymmetrically generates _Fixed Dimensional Encodings_ (FDEs) of queries and documents, which are vectors whose inner product approximates multi-vector similarity. We prove that FDEs give high-quality \(\varepsilon\)-approximations, thus providing the first single-vector proxy for multi-vector similarity with theoretical guarantees. Empirically, we find that FDEs achieve the same recall as prior state-of-the-art heuristics while retrieving 2-5\(\times\) fewer candidates. Compared to prior state of the art implementations, Muvera achieves consistently good end-to-end recall and latency across a diverse set of the BEIR retrieval datasets, achieving an average of 10\(\%\) improved recall with \(90\%\) lower latency.

## 1 Introduction

Over the past decade, the use of neural embeddings for representing data has become a central tool for information retrieval (IR) [56], among many other tasks such as clustering and classification [39]. Recently, _multi-vector_ (MV) representations, introduced by the _late-interaction_ framework in ColBERT [29], have been shown to deliver significantly improved performance on popular IR benchmarks. ColBERT and its variants [17, 21, 32, 35, 42, 44, 49, 54] produce _multiple_ embeddings per query or document by generating one embedding per token. The query-document similarity is then scored via the _Chamfer Similarity_ (SS1.1), also known as the MaxSim operation, between the two sets of vectors. These multi-vector representations have many advantages over single-vector (SV) representations, such as better interpretability [15, 50] and generalization [16, 36, 51, 55].

Despite these advantages, multi-vector retrieval is inherently more expensive than single-vector retrieval. Firstly, producing one embedding per token increases the number of embeddings in a dataset by orders of magnitude. Moreover, due to the non-linear Chamfer similarity scoring, there is a lack of optimized systems for multi-vector retrieval. Specifically, single-vector retrieval isgenerally accomplished via Maximum Inner Product Search (MIPS) algorithms, which have been highly-optimized over the past few decades [18]. However, SV MIPS alone cannot be used for MV retrieval. This is because the MV similarity is the _sum_ of the SV similarities of each embedding in a query to the nearest embedding in a document. Thus, a document containing a token with high similarity to a single query token may not be very similar to the query overall. Thus, in an effort to close the gap between SV and MV retrieval, there has been considerable work in recent years to design custom MV retrieval algorithms with improved efficiency [12, 21, 42, 43].

The most prominent approach to MV retrieval is to employ a multi-stage pipeline beginning with single-vector MIPS. The basic version of this approach is as follows: in the initial stage, the most similar document tokens are found for each of the query tokens using SV MIPS. Then the corresponding documents containing these tokens are gathered together and rescored with the original Chamfer similarity. We refer to this method as the _single-vector heuristic_. ColBERTv2 [44] and its optimized retrieval engine PLAID [43] are based on this approach, with the addition of several intermediate stages of pruning. In particular, PLAID employs a complex _four_-stage retrieval and pruning process to gradually reduce the number of final candidates to be scored (Figure 1). Unfortunately, as described above, employing SV MIPS on individual query embeddings can fail to find the true MV nearest neighbors. Additionally, this process is expensive, since it requires querying a significantly larger MIPS index for _every_ query embedding (larger because there are multiple embeddings per document). Finally, these multi-stage pipelines are complex and highly sensitive to parameter setting, as recently demonstrated in a reproducibility study [37], making them difficult to tune. To address these challenges and bridge the gap between single and multi-vector retrieval, in this paper we seek to design faster and simplified MV retrieval algorithms.

Contributions.We propose Muvera: a multi-vector retrieval mechanism based on a light-weight and provably correct reduction to single-vector MIPS. Muvera employs a fast, data-oblivious transformation from a set of vectors to a single vector, allowing for retrieval via highly-optimized MIPS solvers before a single stage of re-ranking. Specifically, Muvera transforms query and document MV sets \(Q,P\subset\mathbb{R}^{d}\) into single fixed-dimensional vectors \(\vec{q},\vec{p}\), called _Fixed Dimensional Encodings_ (FDEs), such that the the dot product \(\vec{q}\cdot\vec{p}\) approximates the multi-vector similarity between \(Q,P\) (SS2). Empirically, we show that retrieving with respect to the FDE dot product significantly outperforms the single vector heuristic at recovering the Chamfer nearest neighbors (SS3.1). For instance, on MS MARCO, our FDEs Recall\(@N\) surpasses the Recall\(@2\)-5N achieved by the SV heuristic while scanning a similar total number of floats in the search.

We prove in (SS2.1) that our FDEs have strong approximation guarantees; specifically, the FDE dot product gives an \(\varepsilon\)-approximation to the true MV similarity. This gives the first algorithm with provable guarantees for Chamfer similarity search with strictly faster than brute-force runtime (Theorem 2.2). Thus, Muvera provides the first principled method for MV retrieval via a SV proxy.

We compare the end-to-end retrieval performance of Muvera to PLAID on several of the BEIR IR datasets, including the well-studied MS MARCO dataset. We find Muvera to be a robust and efficient retrieval mechanism; across the datasets we evaluated, Muvera obtains an average of 10% higher recall, while requiring 90% lower latency on average compared with PLAID. Additionally, Muvera crucially incorporates a vector compression technique called _product quantization_ that enables us to compress the FDEs by 32\(\times\) (i.e., storing 10240 dimensional FDEs using 1280 bytes) while incurring negligible quality loss, resulting in a significantly smaller memory footprint.

Figure 1: Muvera’s two-step retrieval process, compared to PLAID’s multi-stage retrieval process. Diagram on the right from Santhanam et. al. [43] with permission.

### Chamfer Similarity and the Multi-Vector Retrieval Problem

Given two sets of vectors \(Q,P\subset\mathbb{R}^{d}\), the _Chamfer Similarity_ is given by

\[\textsc{Chamfer}(Q,P)=\sum_{q\in Q}\max_{P\in P}\langle q,p\rangle\]

where \(\langle\cdot,\cdot\rangle\) is the standard vector inner product. Chamfer similarity is the default method of MV similarity used in the _late-interaction_ architecture of ColBERT, which includes systems like ColBERTv2 [44], Baleen [28], Hindsight [41], DrDecr [34], and XTR [32], among many others. These models encode queries and documents as sets \(Q,P\subset\mathbb{R}^{d}\) (respectively), where the query-document similarity is given by \(\textsc{Chamfer}(Q,P)\). We note that Chamfer Similarity (and its distance variant) itself has a long history of study in the computer vision (e.g., [4, 6, 14, 27, 45]) and graphics [33] communities, and had been previously used in the ML literature to compare sets of embeddings [3, 5, 30, 48]. In these works, Chamfer is also referred to as _MaxSim_ or the _relaxed earth mover distance_; we choose the terminology _Chamfer_ due to its historical precedence [6].

In this paper, we study the problem of Nearest Neighbor Search (NNS) with respect to the Chamfer Similarity. Specifically, we are given a dataset \(D=\{P_{1},\ldots,P_{n}\}\) where each \(P_{i}\subset\mathbb{R}^{d}\) is a set of vectors. Given a query subset \(Q\subset\mathbb{R}^{d}\), the goal is to quickly recover the nearest neighbor \(P^{*}\in D\), namely:

\[P^{*}=\arg\max_{P_{i}\in D}\textsc{Chamfer}(Q,P_{i})\]

For the retrieval system to be scalable, this must be achieved in time significantly faster than brute-force scoring each of the \(n\) similarities \(\textsc{Chamfer}(Q,P_{i})\) within \(D\).

### Our Approach: Reducing Multi-Vector Search to Single-Vector MIPS

Muvera is a streamlined procedure that directly reduces the Chamfer Similarity Search to MIPS. For a pre-specified target dimension \(d_{\textsc{PDE}}\), Muvera produces randomized mappings \(\mathbf{F}_{\text{q}}:2^{\mathbb{R}^{d}}\rightarrow\mathbbm{R}^{d_{\textsc{ PDE}}}\) (for queries) and \(\mathbf{F}_{\text{doc}}:2^{\mathbb{R}^{d}}\rightarrow\mathbb{R}^{d_{\textsc{ PDE}}}\) (for documents) such that, for all query and document multi-vector representations \(Q,P\subset\mathbb{R}^{d}\), we have:

\[\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P)\rangle\approx \textsc{Chamfer}(Q,P)\]

We refer to the vectors \(\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P)\) as _Fixed Dimensional Encodings_ (FDEs). Muvera first applies \(\mathbf{F}_{\text{doc}}\) to each document representation \(P\in D\), and indexes the set \(\{\mathbf{F}_{\text{doc}}(P)\}_{P\in D}\) into a MIPS solver. Given a query \(Q\subset\mathbb{R}^{d}\), Muvera quickly computes \(\mathbf{F}_{\text{q}}(Q)\) and feeds it to the MIPS solver to recover the top-\(k\) most similar document FDE's \(\mathbf{F}_{\text{doc}}(P)\). Finally, we re-rank these candidates by the original Chamfer similarity. See Figure 1 for an overview. We remark that one important advantage of the FDEs is that the functions \(\mathbf{F}_{\text{q}},\mathbf{F}_{\text{doc}}\) are _data-oblivious_, making them robust to distribution shifts, and easily usable in streaming settings.

### Related Work on Multi-Vector Retrieval

The early multi-vector retrieval systems, such as ColBERT [29], all implement optimizations of the previously described SV heuristic, where the initial set of candidates is found by querying a MIPS index for every query token \(q\in Q\). In ColBERTv2 [44], the document token embeddings are first clustered via k-means, and the first round of scoring uses cluster centroids instead of the original token. This technique was further optimized in PLAID [43] by employing a four-stage pipeline to progressively prune candidates before a final re-ranking (Figure 1).

An alternative approach with proposed in DESSERT [12], whose authors also pointed out the limitations of the SV heuristic, and proposed an algorithm based on Locality Sensitive Hashing (LSH) [20]. They prove that their algorithm recovers \(\varepsilon\)-approximate nearest neighbors in time \(\tilde{O}(n|Q|T)\), where \(T\) is roughly the maximum number of document tokens \(p\in P_{i}\) that are similar to any query token \(q\in Q\), which can be as large as \(\max_{i}|P_{i}|\). Thus, in the worst case, their algorithm runs no faster than brute-force. Conversely, our algorithm recovers \(\varepsilon\)-approximate nearest neighbors and _always_ runs in time \(\tilde{O}(n|Q|)\). Experimentally, DESSERT is 2-5\(\times\) faster than PLAID, but attains worse recall (e.g. 2-2.5\(\%\) R@1000 on MS MARCO). Conversely, we match and sometimes strongly exceed PLAID's recall with up to 5.7\(\times\) lower latency. Additionally, DESSERT still employs an initial filtering stage based on \(k\)-means clustering of individual query token embeddings (in the manner of ColBERTv2), thus they do not truly avoid the aforementioned limitations of the SV heuristic.

Fixed Dimensional Encodings

We now describe our process for generating FDEs. Our transformation is reminiscent of the technique of probabilistic tree embeddings [1, 7, 10, 13], which can be used to transform a set of vectors into a single vector. For instance, they have been used to embed the Earth Mover's Distance into the \(\ell_{1}\) metric [1, 10, 22, 24], and to embed the weight of a Euclidean MST of a set of vectors into the Hamming metric [9, 22, 23]. However, since we are working with inner products, which are not metrics, instead of \(\ell_{p}\) distances, an alternative approach for our transformation will be needed.

The intuition behind our transformation is as follows. Hypothetically, for two MV representations \(Q,P\subset\mathds{R}^{d}\), if we knew the optimal mapping \(\pi:Q\to P\) in which to match them, then we could create vectors \(\vec{q},\vec{p}\) by concatenating all the vectors in \(Q\) and their corresponding images in \(P\) together, so that \(\langle\vec{q},\vec{p}\rangle=\sum_{q\in Q}\langle q,\pi(q)\rangle=\textsc{ Chamfer}(Q,P)\). However, since we do not know \(\pi\) in advance, and since different query-document pairs have different optimal mappings, this simple concatenation clearly will not work. Instead, our goal is to find a randomized ordering over _all_ the points in \(\mathds{R}^{d}\) so that, after clustering close points together, the dot product of _any_ query-document pair \(Q,P\subset\mathds{R}^{d}\) concatenated into a single vector under this ordering will approximate the Chamfer similarity.

The first step is to partition the latent space \(\mathds{R}^{d}\) into \(B\) clusters so that vectors that are closer are more likely to land in the same cluster. Let \(\bm{\varphi}:\mathds{R}^{d}\to[B]\) be such a partition (for an integer \(N\geqslant 1\) we use the notation \([N]=\{1,2,\ldots,N\}\)); \(\bm{\varphi}\) can be implemented via Locality Sensitive Hashing (LSH) [20], \(k\)-means, or other methods; we discuss choices for \(\bm{\varphi}\) later in this section. After partitioning via \(\bm{\varphi}\), the hope is that for each \(q\in Q\), the closest \(p\in P\) lands in the same cluster (i.e. \(\bm{\varphi}(q)=\bm{\varphi}(p)\)). Hypothetically, if this were to occur, then:

\[\textsc{Chamfer}(Q,P)=\sum_{k=1}^{B}\sum_{\begin{subarray}{c}q\in Q\\ \bm{\varphi}(q)=k\end{subarray}}\max_{\begin{subarray}{c}p\in P\\ \bm{\varphi}(p)=k\end{subarray}}\langle q,p\rangle\] (1)

If \(p\) is the only point in \(P\) that collides with \(q\), then (1) can be realized as a dot product between two vectors \(\vec{q},\vec{p}\) by creating one block of \(d\) coordinates in \(\vec{q},\vec{p}\) for each cluster \(k\in[B]\) (call these blocks \(\vec{q}_{(k)},\vec{p}_{(k)}\in\mathds{R}^{d}\)), and setting \(\vec{q}_{(k)},\vec{p}_{(k)}\) to be the sum of all \(q\in Q\) (resp. \(p\in P\)) that land in the \(k\)-th cluster under \(\bm{\varphi}\). However, if multiple \(p^{\prime}\in P\) collide with \(q\), then \(\langle\vec{q},\vec{p}\rangle\) will differ from (1), since _every_\(p^{\prime}\) with \(\bm{\varphi}(p^{\prime})=\bm{\varphi}(q)\) will contribute at least \(\langle q,p^{\prime}\rangle\) to \(\langle\vec{q},\vec{p}\rangle\). To resolve this, we set \(\vec{p}_{(k)}\) to be the _centroid_ of the \(p\in P\)'s with \(\bm{\varphi}(p)=\bm{\varphi}(q)\). Formally, for \(k=1,\ldots,B\), we define

\[\vec{q}_{(k)}=\sum_{\begin{subarray}{c}q\in Q\\ \bm{\varphi}(q)=k\end{subarray}}q,\qquad\qquad\vec{p}_{(k)}=\frac{1}{|P\cap \bm{\varphi}^{-1}(k)|}\sum_{\begin{subarray}{c}p\in P\\ \bm{\varphi}(p)=k\end{subarray}}p\] (2)

Setting \(\vec{q}=(\vec{q}_{(1)},\ldots,\vec{q}_{(B)})\) and \(\vec{p}=(\vec{p}_{(1)},\ldots,\vec{p}_{(B)})\), then we have

\[\langle\vec{q},\vec{p}\rangle=\sum_{k=1}^{B}\sum_{\begin{subarray}{c}q\in Q\\ \bm{\varphi}(q)=k\end{subarray}}\frac{1}{|P\cap\bm{\varphi}^{-1}(k)|}\sum_{ \begin{subarray}{c}p\in P\\ \bm{\varphi}(p)=k\end{subarray}}\langle q,p\rangle\] (3)

Note that the resulting dimension of the vectors \(\vec{q},\vec{p}\) is \(dB\). To reduce the dependency on \(d\), we can apply a random linear projection \(\bm{\psi}:\mathds{R}^{d}\to\mathds{R}^{d_{\text{proj}}}\) to each block \(\vec{q}_{(k)},\vec{p}_{(k)}\), where \(d_{\text{proj}}<d\). Specifically, we define \(\bm{\psi}(x)=(1/\sqrt{d_{\text{proj}}})Sx\), where \(S\in\mathds{R}^{d_{\text{proj}}\times d}\) is a random matrix with uniformly distributed \(\pm 1\) entries. We can then define \(\vec{q}_{(k),\bm{\psi}}=\bm{\psi}(\vec{q}_{(k)})\) and \(\vec{p}_{(k),\bm{\psi}}=\bm{\psi}(\vec{p}_{(k)})\), and define the _FDE's with inner projection_ as \(\vec{q}_{\bm{\psi}}=(\vec{q}_{(1),\bm{\psi}},\ldots,\vec{q}_{(B),\bm{\psi}})\) and \(\vec{p}_{\bm{\psi}}=(\vec{p}_{(1),\bm{\psi}},\ldots,\vec{p}_{(B),\bm{\psi}})\). When \(d=d_{\text{proj}}\), we simply define \(\psi\) to be the identity mapping, in which case \(\vec{q}_{\bm{\psi}},\vec{p}_{\bm{\psi}}\) are identical to \(\vec{q},\vec{p}\). To increase accuracy of (3) in approximating (1), we repeat the above process \(R_{\text{reps}}\geqslant 1\) times independently, using different randomized partitions \(\bm{\varphi}_{1},\ldots,\bm{\varphi}_{R_{\text{reps}}}\) and projections \(\bm{\psi}_{1},\ldots,\bm{\psi}_{R_{\text{reps}}}\). We denote the vectors resulting from \(i\)-th repetition by \(\vec{q}_{i,\bm{\psi}},\vec{p}_{i,\bm{\psi}}\). Finally, we concatenate these \(R_{\text{reps}}\) vectors together, so that our final FDEs are defined as \(\mathbf{F}_{\text{q}}(Q)=(\vec{q}_{1,\bm{\psi}},\ldots,\vec{q}_{R_{\text{reps}}, \bm{\psi}})\) and \(\mathbf{F}_{\text{doc}}(P)=(\vec{p}_{1,\bm{\psi}},\ldots,\vec{p}_{R_{\text{ reps}},\bm{\psi}})\). Observe that a complete FDE mapping is specified by the three parameters \((B,d_{\text{proj}},R_{\text{reps}})\), resulting in a final dimension of \(d_{\text{FDE}}=B\cdot d_{\text{proj}}\cdot R_{\text{reps}}\).

**Choice of Space Partition.** When choosing the partition function \(\bm{\varphi}\), the desired property is that points are more likely to collide (i.e. \(\bm{\varphi}(x)=\bm{\varphi}(y)\)) the closer they are to each other. Such functionswith this property exist, and are known as _locality-sensitive hash functions_ (LSH) (see [20]). When the vectors are normalized, as they are for those produced by ColBERT-style models, SimHash [8] is the standard choice of LSH. Specifically, for any \(k_{\mathsf{sim}}\geqslant 1\), we sample random Gaussian vectors \(g_{1},\dots,g_{k_{\mathsf{sim}}}\in\mathbb{R}^{d}\), and set \(\bm{\varphi}(x)=(\mathbf{1}(\langle g_{1},x\rangle>0),\dots,\mathbf{1}(\langle g _{k_{\mathsf{sim}}},x\rangle>0))\), where \(\mathbf{1}(\cdot)\in\{0,1\}\) is the indicator function. Converting the bit-string to decimal, \(\bm{\varphi}(x)\) gives a mapping from \(\mathbb{R}^{d}\) to \([B]\) where \(B=2^{k_{\mathsf{sim}}}\). In other words, SimHash partitions \(\mathbb{R}^{d}\) by drawing \(k_{\mathsf{sim}}\) random half-spaces, and each of the \(2^{k_{\mathsf{sim}}}\) clusters is formed by the \(k_{\mathsf{sim}}\)-wise intersection of each of these halfspaces or their complement. Another natural approach is to choose \(k_{\textsc{centre}}\geqslant 1\) centers from the collection of all token embeddings \(\cup_{i=1}^{n}P_{i}\), either randomly or via \(k\)-means, and set \(\bm{\varphi}(x)\in[k_{\textsc{centre}}]\) to be the index of the center nearest to \(x\). We compare this method to SimHash in (3.1).

**Filling Empty Clusters.** A key source of error in the FDE's approximation is when the nearest vector \(p\in P\) to a given query embedding \(q\in Q\) maps to a different cluster, namely \(\bm{\varphi}(p)\neq\bm{\varphi}(q)=k\). This can be made less likely by decreasing \(B\), at the cost of making it more likely for other \(p^{\prime}\in P\) to also map to the same cluster, moving the centroid \(\vec{p}_{(k)}\) farther from \(p\). If we increase \(B\) too much, it is possible that no \(p\in P\) collides with \(\bm{\varphi}(q)\). To avoid this trade-off, we directly ensure that if no \(p\in P\) maps to a cluster \(k\), then instead of setting \(\vec{p}_{(k)}=0\) we set \(\vec{p}_{(k)}\) to the point \(p\) that is _closest_ to cluster \(k\). As a result, increasing \(B\) will result in a more accurate estimator, as this results in smaller clusters. Formally, for any cluster \(k\) with \(P\cap\bm{\varphi}^{-1}(k)=\emptyset\), if fill_empty_clusters is enabled, we set \(\vec{p}_{(k)}=p\) where \(p\in P\) is the point for which \(\bm{\varphi}(p)\) has the fewest number of disagreeing bits with \(k\) (both thought of as binary strings), with ties broken arbitrarily. We do not enable this for query FDEs, as doing so would result in a given \(q\in Q\) contributing to the dot product multiple times.

**Final Projections.** A natural approach to reducing the dimensionality is to apply a final projection \(\bm{\psi}^{\prime}:\mathbb{R}^{d_{\mathsf{rec}}}\to\mathbb{R}^{d_{\mathsf{ final}}}\) (also implemented via multiplication by a random \(\pm 1\) matrix) to the FDE's, reducing the final dimensionality to any \(d_{\mathsf{final}}<d_{\mathsf{rec}}\). Experimentally, we find that final projections can provide small but non-trivial \(1\)-\(2\%\) recall boosts for a fixed dimension (see SSC.2).

### Theoretical Guarantees for FDEs

We now state our theoretical guarantees for our FDE construction. For clarity, we state our results in terms of normalized Chamfer similarity \(\textsc{NChamfer}(Q,P)=\frac{1}{|Q|}\textsc{Chamfer}(Q,P)\). This ensures \(\textsc{NChamfer}(Q,P)\in[-1,1]\) whenever the vectors in \(Q,P\) are normalized. Note that this factor of \(1/|Q|\) does not affect the relative scoring of documents for a fixed query. In what follows, we assume that all token embeddings are normalized (i.e. \(\|q\|_{2}=\|p\|_{2}=1\) for all \(q\in Q,p\in P\)). Note that ColBERT-style late interaction MV models indeed produce normalized token embeddings. We will always use the fill_empty_clusters method for document FDEs, but never for queries.

Our main result is that FDEs give \(\varepsilon\)-additive approximations of the Chamfer similarity. The proof uses the properties of LSH (SimHash) to show that for each query point \(q\in Q\), the point \(q\) gets mapped to a cluster \(\varphi(q)\) that _only_ contains points \(p\in P\) that are close to \(q\) (within \(\varepsilon\) of the closest point to \(q\)); the fact that at least one point collides with \(q\) uses the fill_empty_partitions method.

**Theorem 2.1** (FDE Approximation).: _Fix any \(\varepsilon,\delta>0\), and sets \(Q,P\subset\mathbb{R}^{d}\) of unit vectors, and let \(m=|Q|+|P|\). Then setting \(k_{\mathsf{sim}}=O\left(\frac{\log(m\delta^{-1})}{\varepsilon}\right)\), \(d_{\mathsf{proj}}=O\left(\frac{1}{\varepsilon^{2}}\log(\frac{m}{\varepsilon \delta})\right)\), \(R_{\mathsf{rep}s}=1\), so that

Figure 2: FDE Generation Process. Three SimHashes (\(k_{\mathsf{sim}}=3\)) split space into six regions labelled \(A\)-\(F\) (in high-dimensions \(B=2^{k_{\mathsf{sim}}}\), but \(B=6\) here since \(d=2\)). \(\mathbf{F}_{q}(Q),\mathbf{F}_{\mathsf{dec}}(P)\) are shown as \(B\times d\) matrices, where the \(k\)-th row is \(\vec{q}_{(k)},\vec{p}_{(k)}\). The actual FDEs are flattened versions of these matrices. Not shown: inner projections, repetitions, and fill_empty_clusters.

\(d_{\text{FDE}}=(m/\delta)^{O(1/\varepsilon)}\), then in expectation and with probability at least \(1-\delta\) we have_

\[\textsc{NChamfer}(Q,P)-\varepsilon\leqslant\frac{1}{|Q|}\langle\mathbf{F}_{q}(Q ),\mathbf{F}_{\text{dec}}(P)\rangle\leqslant\textsc{NChamfer}(Q,P)+\varepsilon\]

Finally, we show that our FDE's give an \(\varepsilon\)-approximate solution to Chamfer similarity search, using FDE dimension that depends only _logarithmically_ on the size of the dataset \(n\). Using the fact that our query FDEs are sparse (Lemma A.1), one can run exact MIPS over the FDEs in time \(\widetilde{O}(|Q|\cdot n)\), improving on the brute-force runtime of \(O(|Q|\max_{i}|P_{i}|n)\) for Chamfer similarity search.

**Theorem 2.2**.: _Fix any \(\varepsilon>0\), query \(Q\), and dataset \(P=\{P_{1},\ldots,P_{n}\}\), where \(Q\subset\mathbb{R}^{d}\) and each \(P_{i}\subset\mathbb{R}^{d}\) is a set of unit vectors. Let \(m=|Q|+\max_{i\in[n]}|P_{i}|\). Let \(k_{\text{sim}}=O(\frac{\log m}{\varepsilon})\), \(d_{\text{proj}}=O(\frac{1}{\varepsilon^{2}}\log(m/\varepsilon))\) and \(R_{\text{repg}}=O(\frac{1}{\varepsilon^{2}}\log n)\) so that \(d_{\text{FDE}}=m^{O(1/\varepsilon)}\cdot\log n\). Then if \(i^{*}=\arg\max_{i\in[n]}\langle\mathbf{F}_{q}(Q),\mathbf{F}_{\text{dec}}(P_{ i})\rangle\), with high probability (i.e. \(1-1/\operatorname{poly}(n)\)) we have:_

\[\textsc{NChamfer}(Q,P_{i^{*}})\geqslant\max_{i\in[n]}\textsc{NChamfer}(Q,P_{ i})-\varepsilon\]

_Given the query \(Q\), the document \(P^{*}\) can be recovered in time \(O\left(|Q|\max\{d,n\}\frac{1}{\varepsilon^{4}}\log(\frac{m}{\varepsilon})\log n\right)\)._

## 3 Evaluation

In this section, we evaluate our FDEs as a method for MV retrieval. First, we evaluate the FDEs themselves (offline) as a proxy for Chamfer similarity (SS3.1). In (SS3.2), we discuss the implementation of Muvera, as well as several optimizations made in the search. Then we evaluate the latency of Muvera compared to PLAID, and study the effects of the aforementioned optimizations.

**Datasets.** Our evaluation includes results from six of the well-studied BEIR [46] information retrieval datasets: MS MARCO [40] (CC BY-SA 4.0), HotpotQA (CC BY-SA 4.0) [53], NQ (Apache-2.0) [31], Quora (Apache-2.0) [46], SciDocs (CC BY 4.0) [11], and ArguAna (Apache-2.0) [47]. These datasets were selected for varying corpus size (8K-8.8M) and average number of document tokens (18-165); see (SSB) for further dataset statistics. Following [43], we use the development set for our experiments on MS MARCO, and use the test set on the other datasets.

**MV Model, MV Embedding Sizes, and FDE Dimensionality.** We compute our FDEs on the MV embeddings produced by the ColBERTv2 model [44] (MIT License), which have a dimension of \(d=128\) and a fixed number \(|Q|=32\) of embeddings per query. The number of document embeddings is variable, ranging from an average of \(18.3\) on Quora to \(165\) on Seldocs. This results in 2,300-21,000 floats per document on average (e.g. 10,087 for MS MARCO). Thus, when constructing our FDEs we consider a comparable range of dimensions \(d_{\text{FDE}}\) between 1,000-20,000. Furthermore, using product quantization, we show in (SS3.2) that the FDEs can be significantly compressed by \(32\times\) with minimal quality loss, further increasing the practicality of FDEs.

### Offline Evaluation of FDE Quality

We evaluate the quality of our FDEs as a proxy for the Chamfer similarity, without any re-ranking and using exact (offline) search. We first demonstrate that FDE recall quality improves dependably as the dimension \(d_{\text{FDE}}\) increases, making our method relatively easy to tune. We then show that FDEs are a more effective method of retrieval than the SV heuristic. Specifically, the FDE method achieves Recall\(@N\) exceeding the Recall\(@2\)-4N of the SV heuristic, while in principle scanning a similar number of floats in the search. This suggests that the success of the SV heuristic is largely due to the significant effort put towards optimizing it (as supported by [37]), and similar effort for FDEs may result in even bigger efficiency gains. Additional plots can be found in (SSC). All recall curves use a single FDE instantiation, since in (SSC.1) we show the variance of FDE recall is negligible.

**FDE Quality vs. Dimensionality.** We study how the retrieval quality of FDE's improves as a function of the dimension \(d_{\text{FDE}}\). We perform a grid search over FDE parameters \(R_{\text{repg}}\in\{1,5,10,15,20\},k_{\text{sim}}\in\{2,3,4,5,6\},d_{\text{ proj}}\in\{8,16,32,64\}\), and compute recall on MS MARCO (Figure 3). We find that Pareto optimal parameters are generally achieved by larger \(R_{\text{repg}}\), with \(k_{\text{sim}},d_{\text{proj}}\) playing a lesser role in improving quality. Specifically, \((R_{\text{repg}},k_{\text{sim}},d_{\text{proj}})\in\{(20,3,8),(20,4,8)(20,5,8),( 20,5,16\})\) were all Pareto optimal for their respective dimensions (namely \(R_{\text{repg}}\cdot 2^{k_{\text{sim}}}\cdot d_{\text{proj}}\)). While there are small variations depending on the parameter choice, the FDE quality is tightly linked to dimensionality; increase in dimensionality will generally result in quality gains. We also evaluate using \(k\)-means as a method of partitioning instead of SimHash. Specifically, we cluster the document embeddings with \(k\)-means and set \(\varphi(x)\) to be the index of the nearest centroid to \(x\). We perform a grid search over the same parameters (but with \(k\in\{4,8,16,32,64\}\) to match \(B=2^{k_{\text{am}}}\)). We find that \(k\)-means partitioning offers no quality gains on the Pareto Frontier over SimHash, and is often worse. Moreover, FDE construction with \(k\)-means is no longer data oblivious. Thus, SimHash is chosen as the preferred method for partitioning for the remainder of our experiments.

In Figure 4, we evaluate the FDE retrieval quality with respect to the Chamfer similarity (instead of labelled ground truth data). We compute \(1\)Recall@\(N\), which is the fraction of queries for which the Chamfer \(1\)-nearest neighbor is among the top-\(N\) most similar in FDE dot product. We choose FDE parameters which are Pareto optimal for the dimension from the above grid search. We find that FDE's with fewer dimensions that the original MV representations achieve significantly good recall across multiple BEIR retrieval datasets. For instance, on MS MARCO (where \(d\cdot m_{avg}\approx 10K\)) we achieve \(95\%\) recall while retrieving only \(75\) candidates using \(d_{\text{FDE}}=5120\).

Single Vector Heuristic vs. FDE retrieval.We compare the quality of FDEs as a proxy for retrieval against the previously described SV heuristic, which is the method underpinning PLAID. Recall that in this method, for each of the \(i=1,\dots,32\) query vectors \(q_{i}\) we compute the \(k\) nearest neighbors \(p_{1,i},\dots,p_{k,i}\) from the set \(\cup_{i}P_{i}\) of all document token embeddings. To compute Recall@\(N\), we create an ordered list \(\ell_{1,1},\dots,\ell_{1,32},\ell_{2,1},\dots\), where \(\ell_{i,j}\) is the document ID containing \(p_{i,j}\), consisting of the \(1\)-nearest neighbors of the queries, then the \(2\)-nearest neighbors, and so on. When re-ranking, one first removes duplicate document IDs from this list. Since duplicates cannot be detected while performing the initial \(32\) SV MIPS queries, the SV heuristic needs to over-retrieve to reach a desired number of unique candidates. Thus, we note that the true recall curve of implementations of the SV heuristic (e.g. PLAID) is somewhere between the case of no deduplication and full deduplication; we compare to both in Figure 5.

To compare the cost of the SV heuristic to running MIPS over the FDEs, we consider the total number of floats scanned by both using a brute force search. The FDE method must scan \(n\cdot d_{\text{FDE}}\) floats to compute the \(k\)-nearest neighbors. For the SV heuristic, one runs \(32\) brute force scans over \(n\cdot m_{avg}\) vectors in \(128\) dimensions, where \(m_{avg}\) is the average number of embeddings per document (see SSB for values of \(m_{avg}\)). For MS MARCO, where \(m_{avg}=78.8\), the SV heuristic searches through \(32\cdot 128\cdot 78.8\cdot n\) floats. This allows for an FDE dimension of \(d_{\text{FDE}}=\) 322,764 to have comparable cost! We can extend this comparison to fast approximate search - suppose that approximate MIPS

Figure 4: Comparison of FDE recall versus brute-force search over Chamfer similarity.

Figure 3: FDE recall vs dimension for varying FDE parameters on MS MARCO. Plots show FDE Recall@100,1k,10k left to right. Recall@\(N\) for exact Chamfer scoring is shown by dotted lines.

over \(n\) vectors can be accomplished in sublinear \(n^{\varepsilon}\) time, for some \(\varepsilon\in(0,1)\). Then even in the unrealistic case of \(\varepsilon=0\), we can still afford an FDE dimension of \(d_{\text{FDE}}=32\cdot 128=4096\).

The results can be found in Figure 5. We build FDEs once for each dimension, using \(R_{\text{reps}}=40,k_{\text{sim}}=6,d_{\text{proj}}=d=128\), and then applying a final projection to reduce to the target dimension (see C.2 for experiments on the impact of final projections). On MS MARCO, even the \(4096\)-dimensional FDEs match the recall of the (deduplicated) SV heuristic while retrieving 1.75-3.75\(\times\)_fewer_ candidates (our Recall\(@N\) matches the Recall\(@1.75\)-3.75\(N\) of the SV heuristic), and 10.5-15\(\times\) fewer than to the non-deduplicated SV heuristic. For our \(10240\)-dimension FDEs, these numbers are 2.6-5\(\times\) and 20-22.2\(\times\) fewer, respectively. For instance, we achieve \(80\%\) recall with \(60\) candidates when \(d_{\text{FDE}}=10240\) and \(80\) candidates when \(d_{\text{FDE}}=4096\), but the SV heuristic requires \(300\) and \(1200\) candidates (for dedup and non-dedup respectively). See Table 1 for further comparisons.

Variance.Note that although the FDE generation is a randomized process, we show in (SSC.1) that the variance of the FDE Recall is essentially negligible; for instance, the standard deviation Recall\(@1000\) is at most \(0.08\)-\(0.16\%\) for FDEs with \(2\)-\(10k\) dimensions.

### Online Implementation and End-to-End Evaluation

We implemented Muvera, an FDE generation and end-to-end retrieval engine in C++. We discussed FDE generation and various optimizations and their tradeoffs in (SS3.1). Next, we discuss how we perform retrieval over the FDEs, and additional optimizations.

Single-Vector MIPS Retrieval using DiskANN.Our single-vector retrieval engine uses a scalable implementation [38] of DiskANN [25] (MIT License), a state-of-the-art graph-based ANNS algorithm. We build DiskANN indices by using the uncompressed document FDEs with a maximum degree of 200 and a build beam-width of 600. Our retrieval works by querying the DiskANN index using beam search with beam-width \(W\), and subsequently reranking the retrieved candidates with Chamfer similarity. The only tuning knob in our system is \(W\); increasing \(W\) increases the number of candidates retrieved by Muvera, which improves the recall.

Product Quantization (PQ).To further improve the memory usage of Muvera, we use a textbook vector compression technique called product quantization (PQ) with asymmetric querying [19, 26] on the FDEs. We refer to product quantization with \(C\) centers per group of \(G\) dimensions as PQ-\(C\)-\(G\). For example, PQ-\(256\)-\(8\), which we find to provide the best tradeoff between quality and compression in our experiments, compresses every consecutive set of \(8\) dimensions to one of \(256\) centers. Thus PQ-\(256\)-\(8\) provides \(32\times\) compression over storing each dimension using a single float, since each block of \(8\) floats is represented by a single byte. See (SSC.4) for further experiments and details on PQ.

Experimental Setup.We run our online experiments on an Intel Sapphire Rapids machine on Google Cloud (c3-standard-176). The machine supports up to 176 hyper-threads. We run latency experiments using a single thread, and run our QPS experiments on all 176 threads.

Ball Carving.To improve re-ranking speed, we reduce the number of query embeddings by clustering them via a _ball carving_ method and replacing the embeddings in each cluster with their sum. This speeds up reranking without decreasing recall. Specifically, we group the queries \(Q\) into clusters \(C_{1},\dots,C_{k}\), setting \(c_{i}=\sum_{q\in C_{i}}q\) and \(Q_{C}=\{c_{1},\dots,c_{k}\}\). Then, after retrieving a set of candidate documents with the FDEs, instead of rescoring via Chamfer\((Q,P)\) for each candidate \(P\), we rescore via Chamfer\((Q_{C},P)\), which runs in time \(O(|Q_{C}|\cdot|P|)\), offering speed-ups when the number of clusters is small. Instead of fixing \(k\), we perform greedy ball-carving to allow \(k\) to adapt to \(Q\). Specifically, given a threshold \(\tau\), we select an arbitrary point \(q\in Q\), cluster it with all other points \(q^{\prime}\in Q\) with \(\langle q,q^{\prime}\rangle\geqslant\tau\), remove the clustered points and repeat until all points are clustered.

Figure 5: FDE retrieval vs SV Heuristic, both with and without document id deduplication.

In Figure 6, we show the the trade-off between end-to-end Recall\(@k\) of Muvera and the ball carving threshold used. Notice that for both \(k=100\) and \(k=1000\), the recall curves flatten after a threshold of \(\tau=0.6\), and for all datasets they are essentially flat after \(\tau\geqslant 0.7\). Thus, for such thresholds we incur essentially no quality loss by performing ball carving. Based on these empirical results, we choose the value of \(\tau=0.7\) in our end-to-end experiments.

In (SSC.3), we show the impact on end-to-end QPS of ball carving on the MS MARCO dataset. For sequential re-ranking, we find that ball carving at a \(\tau=0.7\) threshold provides a \(25\%\) QPS improvement, and when re-ranking is being done in parallel (over all cores simultaneously) it yields a 20\(\%\) QPS improvement. Moreover, with a threshold of \(\tau=0.7\), there were an average of \(5.9\) clusters created per query on MS MARCO. This reduces the number of query embeddings by 5.4\(\times\), down from the initial fixed setting of \(|Q|=32\). This finding shows that pre-clustering the queries before re-ranking gives non-trivial runtime improvements with negligible quality loss. It also suggests that a fixed setting of \(|Q|=32\) query embeddings used by existing approaches is likely excessive for MV similarity quality, and that fewer queries could achieve a similar performance.

QPS vs. Recall.A useful metric for retrieval is the number of _queries per second (QPS)_ a system can serve at a given recall; evaluating the QPS of a system tries to fully utilize the system resources (e.g., the bandwidth of multiple memory channels and caches), and deployments where machines serve many queries simultaneously. Figure 7 shows the QPS vs. Recall@100 for Muvera on a subset of the BEIR datasets, using different PQ schemes over the FDEs. We show results for additional datasets, as well as Recall@1000, in the Appendix. Using PQ-\(256\)-\(8\) not only reduces the space usage of the FDEs by 32\(\times\), but also improves the QPS at the same query beamwidth by up to 20\(\times\), while incurring a minimal loss in end-to-end recall. Our method has a relatively small dependence on the dataset size, which is consistent with prior studies on graph-based ANNS data structures, since the number of distance comparisons made during beam search grows roughly logarithmically with increasing dataset size [25, 38]. We tried to include QPS numbers for PLAID [43], but unfortunately their implementation does not support running multiple queries in parallel, and is optimized for measuring latency.

Latency and Recall Results vs. PLAID [43]We evaluated Muvera and PLAID [43] on the 6 datasets from the BEIR benchmark described earlier in (SS3); Figure 8 shows that Muvera achieves essentially equivalent Recall@\(k\) as PLAID (within 0.4%) on MS MARCO, while obtaining up to 1.56\(\times\) higher recall on other datasets (e.g. HotpotQA). We ran PLAID using the recommended settings for their system, which reproduced their recall results for MS MARCO. Compared with PLAID, on

Figure 6: Plots showing the trade-off between the threshold used for ball carving and the end-to-end recall.

Figure 7: Plots showing the QPS vs. Recall@100 for Muvera on a subset of the BEIR datasets. The different curves are obtained by using different PQ methods on 10240-dimensional FDEs.

average over all \(6\) datasets and \(k\in\{100,1000\}\), Muvera achieves 10% higher Recall@\(k\) (up to 56% higher), and 90% lower latency (up to 5.7\(\times\) lower).

Importantly, Muvera has consistently high recall and low latency across all of the datasets that we measure, and our method _does not_ require costly parameter tuning to achieve this--all of our results use the same 10240-dimensional FDEs that are compressed using PQ with PQ-\(256\)-\(8\); the only tuning in our system was to pick the first query beam-width over the \(k\) that we rerank to that obtained recall matching that of PLAID. As Figure 8 shows, in cases like NQ and HotpotQA, Muvera obtains much higher recall while obtaining lower latency. Given these results, we believe a distinguishing feature of Muvera compared to prior multi-vector retrieval systems is that it achieves consistently high recall and low latency across a wide variety of datasets with minimal tuning effort.

## 4 Conclusion

In this paper, we presented Muvera: a principled and practical MV retrieval algorithm which reduces MV similarity to SV similarity by constructing Fixed Dimensional Encoding (FDEs) of a MV representation. We prove that FDE dot products give high-quality approximations to Chamfer similarity (SS2.1). Experimentally, we show that FDEs are a much more effective proxy for MV similarity, since they require retrieving 2-4\(\times\) fewer candidates to achieve the same recall as the SV Heuristic (SS3.1). We complement these results with an end-to-end evaluation of Muvera, showing that it achieves an average of 10% improved recall with 90% lower latency compared with PLAID. Moreover, despite the extensive optimizations made by PLAID to the SV Heuristic, we still achieve significantly better latency on \(5\) out of \(6\) BEIR datasets we consider (SS3). Given their retrieval efficiency compared to the SV heuristic, we believe that there are still significant gains to be obtained by optimizing the FDE method, and leave further exploration of this to future work.

**Broader Impacts and Limitations:** While retrieval is an important component of LLMs, which themselves have broader societal impacts, these impacts are unlikely to result from our retrieval algorithm. Our contribution simply improves the efficiency of retrieval, without enabling any fundamentally new capabilities. As for limitations, while we outperformed PLAID, sometimes significantly, on \(5\) out of the \(6\) datasets we studied, we did not outperform PLAID on MS MARCO, possibly due to their system having been carefully tuned for MS MARCO given its prevalence. Additionally, we did not study the effect that the average number of embeddings \(m_{avg}\) per document has on retrieval quality of FDEs; this is an interesting direction for future work.

Figure 8: Bar plots showing the latency and Recall@\(k\) of Muvera vs PLAID on a subset of the BEIR datasets. The x-tick labels are formatted as dataset-\(k\), i.e., optimizing for Recall@\(k\) on the given dataset.

## References

* [1] Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Earth mover distance over high-dimensional spaces. In _Proceedings of the 19th ACM-SIAM Symposium on Discrete Algorithms (SODA '2008)_, pages 343-352, 2008.
* [2] Rosa I Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and random projection. _Machine learning_, 63:161-182, 2006.
* [3] Kubilay Atasu and Thomas Mittelholzer. Linear-complexity data-parallel earth mover's distance approximations. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 364-373. PMLR, 09-15 Jun 2019.
* [4] Vassilis Athitsos and Stan Sclaroff. Estimating 3d hand pose from a cluttered image. In _2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings._, volume 2, pages II-432. IEEE, 2003.
* [5] Ainesh Bakshi, Piotr Indyk, Rajesh Jayaram, Sandeep Silwal, and Erik Waingarten. Near-linear time algorithm for the chamfer distance. _Advances in Neural Information Processing Systems_, 36, 2024.
* [6] Harry G Barrow, Jay M Tenenbaum, Robert C Bolles, and Helen C Wolf. Parametric correspondence and chamfer matching: Two new techniques for image matching. In _Proceedings: Image Understanding Workshop_, pages 21-27. Science Applications, Inc, 1977.
* [7] Yair Bartal. Probabilistic approximation of metric spaces and its algorithmic applications. In _Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer Science (FOCS '1996)_, 1996.
* [8] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In _Proceedings of the thiry-fourth annual ACM symposium on Theory of computing_, pages 380-388, 2002.
* [9] Xi Chen, Vincent Cohen-Addad, Rajesh Jayaram, Amit Levi, and Erik Waingarten. Streaming euclidean mst to a constant factor. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, STOC 2023, page 156-169, New York, NY, USA, 2023. Association for Computing Machinery.
* [10] Xi Chen, Rajesh Jayaram, Amit Levi, and Erik Waingarten. New streaming algorithms for high dimensional emd and mst. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 222-233, 2022.
* [11] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. _arXiv preprint arXiv:2004.07180_, 2020.
* [12] Joshua Engels, Benjamin Coleman, Vihan Lakshman, and Anshumali Shrivastava. Dessert: An efficient algorithm for vector set search with vector set queries. _Advances in Neural Information Processing Systems_, 36, 2024.
* [13] Jittat Fakcharoenphol, Satish Rao, and Kunal Talwar. A tight bound on approximating arbitrary metrics by tree metrics. _Journal of Computer and System Sciences_, 69(3):485-497, 2004.
* [14] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstruction from a single image. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 605-613, 2017.
* [15] Thibault Formal, Benjamin Piwowarski, and Stephane Clinchant. A white box analysis of colbert. In _Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28-April 1, 2021, Proceedings, Part II 43_, pages 257-263. Springer, 2021.
* [16] Thibault Formal, Benjamin Piwowarski, and Stephane Clinchant. Match your words! a study of lexical matching in neural information retrieval. In _European Conference on Information Retrieval_, pages 120-127. Springer, 2022.

* [17] Luyu Gao, Zhuyun Dai, and Jamie Callan. Coil: Revisit exact lexical match in information retrieval with contextualized inverted list. _arXiv preprint arXiv:2104.07186_, 2021.
* [18] Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and David Simcha. Quantization based fast inner product search. In _Artificial intelligence and statistics_, pages 482-490. PMLR, 2016.
* [19] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In _International Conference on Machine Learning_, pages 3887-3896. PMLR, 2020.
* [20] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards removing the curse of dimensionality. _Theory of Computing_, 8(1):321-350, 2012.
* [21] Sebastian Hofstatter, Omar Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. Introducing neural bag of whole-words with colberter: Contextualized late interactions using enhanced reduction. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 737-747, 2022.
* [22] Piotr Indyk. Algorithms for dynamic geometric problems over data streams. In _Proceedings of the 36th ACM Symposium on the Theory of Computing (STOC '2004)_, pages 373-380, 2004.
* [23] Rajesh Jayaram, Vahab Mirrokni, Shyam Narayanan, and Peilin Zhong. Massively parallel algorithms for high-dimensional euclidean minimum spanning tree. In _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 3960-3996. SIAM, 2024.
* [24] Rajesh Jayaram, Erik Waingarten, and Tian Zhang. Data-dependent lsh for the earth mover's distance. In _Proceedings of the 56th Annual ACM Symposium on Theory of Computing_, 2024.
* [25] Suhas Jayaram Subramanya, Fru Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. _Advances in Neural Information Processing Systems_, 32, 2019.
* [26] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. _IEEE transactions on pattern analysis and machine intelligence_, 33(1):117-128, 2010.
* [27] Li Jiang, Shaoshuai Shi, Xiaojuan Qi, and Jiaya Jia. Gal: Geometric adversarial loss for single-view 3d-object reconstruction. In _Proceedings of the European conference on computer vision (ECCV)_, pages 802-816, 2018.
* [28] Omar Khattab, Christopher Potts, and Matei Zaharia. Baleen: Robust multi-hop reasoning at scale via condensed retrieval. _Advances in Neural Information Processing Systems_, 34:27670-27682, 2021.
* [29] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In _Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval_, pages 39-48, 2020.
* [30] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In _International conference on machine learning_, pages 957-966. PMLR, 2015.
* [31] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, et al. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 2019.
* [32] Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Zhao. Rethinking the role of token retrieval in multi-vector retrieval. _Advances in Neural Information Processing Systems_, 36, 2024.
* [33] Chun-Liang Li, Tomas Simon, Jason Saragih, Barnabas Poczos, and Yaser Sheikh. Lbs autoencoder: Self-supervised fitting of articulated meshes to point clouds. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11967-11976, 2019.

* [34] Yulong Li, Martin Franz, Md Arafat Sultan, Bhavani Iyer, Young-Suk Lee, and Avirup Sil. Learning cross-lingual ir from an english retriever. _arXiv preprint arXiv:2112.08185_, 2021.
* [35] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained late-interaction multi-modal retrieval for retrieval augmented visual question answering. _Advances in Neural Information Processing Systems_, 36, 2024.
* [36] Simon Lupart, Thibault Formal, and Stephane Clinchant. Ms-shift: An analysis of ms marco distribution shifts on neural retrieval. In _European Conference on Information Retrieval_, pages 636-652. Springer, 2023.
* [37] Sean MacAvaney and Nicola Tonellotto. A reproducibility study of plaid. _arXiv preprint arXiv:2404.14989_, 2024.
* [38] Magdalen Dobson Manohar, Zheqi Shen, Guy Blelloch, Laxman Dhulipala, Yan Gu, Harsha Vardhan Simhadri, and Yihan Sun. Parlayann: Scalable and deterministic parallel graph-based approximate nearest neighbor search algorithms. In _Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming_, pages 270-285, 2024.
* [39] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. _arXiv preprint arXiv:2210.07316_, 2022.
* [40] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human-generated machine reading comprehension dataset. 2016.
* [41] Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, and Christopher D Manning. Hindsight: Posterior-guided training of retrievers for improved open-ended generation. _arXiv preprint arXiv:2110.07752_, 2021.
* [42] Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu, Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim, Tao Lei, and Vincent Y Zhao. Multi-vector retrieval as sparse alignment. _arXiv preprint arXiv:2211.01267_, 2022.
* [43] Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. Plaid: an efficient engine for late interaction retrieval. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 1747-1756, 2022.
* [44] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. _arXiv preprint arXiv:2112.01488_, 2021.
* [45] Erik B Sudderth, Michael I Mandel, William T Freeman, and Alan S Willsky. Visual hand tracking using nonparametric belief propagation. In _2004 Conference on Computer Vision and Pattern Recognition Workshop_, pages 189-189. IEEE, 2004.
* [46] Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. _arXiv preprint arXiv:2104.08663_, 2021.
* [47] Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 241-251, 2018.
* [48] Ziyu Wan, Dongdong Chen, Yan Li, Xingguang Yan, Junge Zhang, Yizhou Yu, and Jing Liao. Transductive zero-shot learning with visual structure constraint. _Advances in neural information processing systems_, 32, 2019.
* [49] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. Pseudo-relevance feedback for multiple representation dense retrieval. In _Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval_, pages 297-306, 2021.

* [50] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. Reproducibility, replicability, and insights into dense multi-representation retrieval models: from colbert to col. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2552-2561, 2023.
* [51] Orion Weller, Dawn Lawrie, and Benjamin Van Durme. Nevir: Negation in neural information retrieval. _arXiv preprint arXiv:2305.07614_, 2023.
* [52] David P Woodruff et al. Sketching as a tool for numerical linear algebra. _Foundations and Trends(r) in Theoretical Computer Science_, 10(1-2):1-157, 2014.
* [53] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_, 2018.
* [54] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. _arXiv preprint arXiv:2111.07783_, 2021.
* [55] Jingtao Zhan, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. Evaluating interpolation and extrapolation performance of neural retrieval models. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 2486-2496, 2022.
* [56] Ye Zhang, Md Mustafizur Rahman, Alex Braylan, Brandon Dang, Heng-Lu Chang, Henna Kim, Quinten McNamara, Aaron Angert, Edward Banner, Vivek Khetan, et al. Neural information retrieval: A literature review. _arXiv preprint arXiv:1611.06792_, 2016.

Missing Proofs from Section 2.1

In this section, we provide the missing proofs in Section 2.1. For convenience, we also reproduce theorem statements as they appear in the main text before the proofs. We begin by analyzing the runtime to compute query and document FDEs, as well as the sparsity of the queries.

**Lemma A.1**.: _For any FDE parameters \(k_{\mathsf{sim}},d_{\mathsf{proj}},R_{\mathsf{reps}}\geqslant\) and sets \(Q,P\subset\mathbb{R}^{d}\), we can compute \(\mathbf{F}_{q}(Q)\) in time \(T_{q}:=O(R_{\mathsf{reps}}|Q|d(d_{\mathsf{proj}}+k_{\mathsf{sim}}))\), and \(\mathbf{F}_{q}(P)\) in time \(O(T_{q}+R_{\mathsf{reps}}|P|2^{k_{\mathsf{sim}}}k_{\mathsf{sim}})\). Moreover, \(\mathbf{F}_{q}(Q)\) has at most \(O(|Q|d_{\mathsf{proj}}R_{\mathsf{reps}})\) non-zero entries._

Proof.: We first consider the queries. To generate the queries, we must first project each of the \(|Q|\) queries via the inner random linear productions \(\bm{\psi}_{i}:\mathbb{R}^{d}\to\mathbb{R}^{d_{\mathsf{proj}}}\), which requires \(O(|Q|d_{\mathsf{proj}}R_{\mathsf{reps}})\) time to perform the matrix-query products for all repetitions. Next, we must compute \(\varphi_{i}(q)\) for each \(q\in Q\) and repetition \(i\in[R_{\mathsf{reps}}]\), Each such value can be compute in \(d\cdot k_{\mathsf{sim}}\) time to multiply the \(q\in\mathbb{R}^{d}\) by the \(k_{\mathsf{sim}}\) Gaussian vectors. Thus the total running time for this step is \(O(R_{\mathsf{reps}}|Q|dk_{\mathsf{sim}})\). Finally, summing the relevant values into the FDE once \(\varphi_{i}(q),\bm{\psi}_{i}(q)\) are computed can be done in \(O(|Q|d_{\mathsf{proj}})\) time. For sparsity, note that only the coordinate blocks in the FDE corresponding to clusters \(k\) in a repetition \(i\) with at least one \(q\in|Q|\) with \(\bm{\varphi}_{i}(q)=k\) are non-zero, and there can be at most \(O(R_{\mathsf{reps}}|Q|)\) of these blocks, each of which has \(O(d_{\mathsf{proj}})\) coordinates.

The document runtime is similar, except with the additional complexity required to carry out the fill_empty_clusters option. For each repetition, the runtime required to find the closest \(p\in P\) to a give cluster \(\vec{k}\) is \(O(|P|\cdot k_{\mathsf{sim}})\), since we need to run over all \(|p|\) values of \(\bm{\varphi}(p)\) and check how many bits disagree with \(k\). Thus, the total runtime is \(O(R_{\mathsf{reps}}|P|Bk_{\mathsf{sim}})=O(R_{\mathsf{reps}}|P|2^{k_{\mathsf{ sim}}}k_{\mathsf{sim}})\). 

In what follows, we will need the following standard fact that random projections approximately preserve dot products. The proof is relatively standard, and can be found in [2], or see results on approximate matrix product [52] for more general bounds.

**Fact A.2** ([2]).: _Fix \(\varepsilon,\delta>0\). For any \(d\geqslant 1\) and \(x,y\in\mathbb{R}^{d}\), let \(S\in\mathbb{R}^{t\times d}\) by a matrix of independent entries distributed uniformly over \(\{1,-1\}\), where \(t=O(1/\varepsilon^{2}\cdot\log\delta^{-1})\). Then we have \(\mathbb{E}\left[\langle Sx,Sy\rangle\right]=\langle x,y\rangle\), and moreover with probability at least \(1-\delta\) we have_

\[|\langle Sx,Sy\rangle-\langle x,y\rangle|\leqslant\varepsilon\|x\|_{2}\|y\|_{2}\]

To anaylze the approximations of our FDEs, we begin by proving an upper bound on the value of the FDE dot product. In fact, we prove a stronger result: we show that our FDEs have the desirable property of being _one-sided estimators_ - namely, they never overestimate the true Chamfer similarity. This is summarized in the following Lemma.

**Lemma A.3** (One-Sided Error Estimator).: _Fix any sets \(Q,P\subset\mathbb{R}^{d}\) of unit vectors with \(|Q|+|P|=m\). Then if \(d=d_{\mathsf{proj}}\), we always have_

\[\frac{1}{|Q|}\left\langle\mathbf{F}_{q}(Q),\mathbf{F}_{\text{doc}}(P)\right\rangle \leqslant\textsc{NChamfer}(Q,P)\]

_Furthermore, for any \(\delta>0\), if we set \(d_{\mathsf{proj}}=O(\frac{1}{\varepsilon^{2}}\log(m/\delta))\), then we have \(\frac{1}{|Q|}\langle\mathbf{F}_{q}(Q),\mathbf{F}_{\text{doc}}(P)\rangle \leqslant\textsc{NChamfer}(Q,P)+\varepsilon\) in expectation and with probability at least \(1-\delta\)._Proof.: First claim simply follows from the fact that the average of a subset of a set of numbers can't be bigger than the maximum number in that set. More formally, we have:

\[\begin{split}\frac{1}{|Q|}\left\langle\mathbf{F}_{\text{q}}(Q), \mathbf{F}_{\text{doc}}(P)\right\rangle&=\frac{1}{|Q|}\sum_{ \begin{subarray}{c}k=1\\ \boldsymbol{\varphi}(q)=k\end{subarray}}^{B}\sum_{\begin{subarray}{c}q\in Q\\ \boldsymbol{\varphi}(q)=k\end{subarray}}\frac{1}{|P\cap\boldsymbol{\varphi}^{- 1}(k)|}\sum_{\begin{subarray}{c}p\in P\\ \boldsymbol{\varphi}(p)=k\end{subarray}}\left\langle q,p\right\rangle\\ &\leqslant\frac{1}{|Q|}\sum_{k=1}^{B}\sum_{\begin{subarray}{c}q\in Q \\ \boldsymbol{\varphi}(q)=k\end{subarray}}\frac{1}{|P\cap\boldsymbol{\varphi}^{- 1}(k)|}\sum_{\begin{subarray}{c}p\in P\\ \boldsymbol{\varphi}(p)=k\end{subarray}}\max_{p^{\prime}\in P}\langle q,p^{ \prime}\rangle\\ &=\frac{1}{|Q|}\sum_{k=1}^{B}\sum_{\begin{subarray}{c}q\in Q\\ \boldsymbol{\varphi}(q)=k\end{subarray}}\max_{p^{\prime}\in p}\langle q,p \rangle=\textsc{NChamfer}(Q,P)\end{split}\] (4)

Which completes the first part of the lemma. For the second part, to analyze the case of \(d_{\mathsf{proj}}<d\), when inner random projections are used, by applying Fact A.2, firstly we have \(\mathbb{E}\left[\langle\boldsymbol{\psi}(p),\boldsymbol{\psi}(q)\right]= \langle q,p\rangle\) for any \(q\in Q,p\in P\), and secondly, after a union bound we over \(|P|\cdot|Q|\leqslant m^{2}\) pairs, we have \(\langle q,p\rangle=\langle\boldsymbol{\psi}(p),\boldsymbol{\psi}(q)\rangle \pm\varepsilon\) simultaneously for all \(q\in Q,p\in P\), with probability \(1-\delta\), for any constant \(C>1\). The second part of the Lemma then follows similarly as above. 

We are now ready to give the proof of our main FDE approximation theorem.

**Theorem 2.1** (FDE Approximation).: _Fix any \(\varepsilon,\delta>0\), and sets \(Q,P\subset\mathbb{R}^{d}\) of unit vectors, and let \(m=|Q|+|P|\). Then setting \(k_{\texttt{sim}}=O\left(\frac{\log(m\delta^{-1})}{\varepsilon}\right)\), \(d_{\texttt{proj}}=O\left(\frac{1}{\varepsilon^{2}}\log(\frac{m}{\varepsilon \delta})\right)\), \(R_{\texttt{reps}}=1\), so that \(d_{\text{FDE}}=(m/\delta)^{O(1/\varepsilon)}\), we have_

\[\textsc{NChamfer}(Q,P)-\varepsilon\leqslant\frac{1}{|Q|}\langle\mathbf{F}_{q} (Q),\mathbf{F}_{\text{doc}}(P)\rangle\leqslant\textsc{NChamfer}(Q,P)+\varepsilon\]

_in expectation, and with probability at least \(1-\delta\)._

Proof of Theorem 2.1.: The upper bound follows from Lemma A.3, so it will suffice to prove the lower bound. We first prove the result in the case when there are no random projections \(\boldsymbol{\psi}\), and remove this assumption at the end of the proof. Note that, by construction, \(\mathbf{F}_{\text{q}}\) is a linear mapping so that \(\mathbf{F}_{\text{q}}(Q)=\sum_{q\in Q}\mathbf{F}(q)\), thus

\[\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P)\rangle=\sum_{q\in Q }\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P)\rangle\]

So it will suffice to prove that

\[\mathbf{Pr}\left[\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P) \rangle\geqslant\max_{p\in P}\langle q,p\rangle-\varepsilon\right]\geqslant 1 -\varepsilon\delta/|Q|\] (5)

for all \(q\in Q\), since then, by a union bound 5 will hold for all over all \(q\in Q\) with probability at least \(1-\varepsilon\delta\), in which case we will have

\[\begin{split}\frac{1}{|Q|}\langle\mathbf{F}_{\text{q}}(Q), \mathbf{F}_{\text{doc}}(P)\rangle&\geqslant\frac{1}{|Q|}\sum_{q \in Q}\left(\max_{p\in P}\langle q,p\rangle-\varepsilon\right)\\ &=\textsc{NChamfer}(Q,P)-\varepsilon\end{split}\] (6)

which will complete the theorem.

In what follows, for any \(x,y\in\mathbb{R}^{d}\) let \(\theta(x,y)\in[0,\pi]\) be the angle between \(x,y\). Now fix any \(q\in Q\), and let \(p^{*}=\arg\max_{p\in P}\langle q,p\rangle\), and let \(\theta^{*}=\theta(q,p^{*})\). By construction, there always exists some set of points \(S\subset P\) such that

\[\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P)\rangle=\left\langle q,\frac{1}{|S|}\sum_{p\in S}p\right\rangle\]Moreover, the RHS of the above equation is always bounded by \(1\) in magnitude, since it is an average of dot products of normalized vectors \(q,p\in\mathbb{S}^{d-1}\). In particular, there are two cases. In case **(A)**\(S\) is the set of points \(p\) with \(\boldsymbol{\varphi}(p)=\boldsymbol{\varphi}(q)\), and in case **(B)**\(S\) is the single point \(\arg\min_{p\in P}\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_{0}\), where \(\|x-y\|_{0}\) denotes the hamming distance between any two bit-strings \(x,y\in\{0,1\}^{k_{\text{sim}}}\), and we are interpreting \(\boldsymbol{\varphi}(p),\boldsymbol{\varphi}(q)\in\{0,1\}^{k_{\text{sim}}}\) as such bit-strings. Also let \(g_{1},\ldots,g_{k_{\text{sim}}}\in\mathbb{R}^{d}\) be the random Gaussian vectors that were drawn to define the partition function \(\boldsymbol{\varphi}\). To analyze \(S\), we first prove the following:

**Claim A.4**.: For any \(q\in Q\) and \(p\in P\), we have

\[\mathbf{Pr}\left[\left|\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_{0 }-k_{\text{sim}}\cdot\frac{\theta(q,p)}{\pi}\right|>\sqrt{\varepsilon}k_{ \text{sim}}\right]\leqslant\left(\frac{\varepsilon\delta}{m^{2}}\right)\]

Proof.: Fix any such \(p\), and for \(i\in[k_{\text{sim}}]\) let \(Z_{i}\) be an indicator random variable that indicates the event that \(\mathbf{1}(\langle g_{i},p\rangle>0)\neq\mathbf{1}(\langle g_{i},q\rangle>0)\). First then note that \(\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_{0}=\sum_{i=1}^{k_{\text {sim}}}Z_{i}\). Now by rotational invariance of Gaussians, for a Gaussian vector \(g\in\mathbb{R}^{d}\) we have \(\mathbf{Pr}\left[\mathbf{1}(\langle g,x\rangle>0)\neq\mathbf{1}(\langle g,y \rangle>0)\right]=\frac{\theta(x,y)}{\pi}\) for any two vectors \(x,y\in\mathbb{R}^{d}\). It follows that \(Z_{i}\) is a Bernoulli random variable with \(\mathbb{E}\left[Z_{i}\right]=\frac{\theta(x,y)}{\pi}\). By a simple application of Hoeffding's inequality, we have

\[\mathbf{Pr}\left[\left|\|\boldsymbol{\varphi}(p)-\boldsymbol{ \varphi}(q)\|_{0}-k_{\text{sim}}\cdot\frac{\theta(q,p)}{\pi}\right|>\sqrt{ \varepsilon}k_{\text{sim}}\right] =\mathbf{Pr}\left[\left|\sum_{i=1}^{k_{\text{sim}}}Z_{i}-\mathbb{ E}\left[\sum_{i=1}^{k_{\text{sim}}}Z_{i}\right]\right|>\sqrt{\varepsilon}k_{ \text{sim}}\right]\] \[\leqslant\exp\left(-2\varepsilon k_{\text{sim}}\right)\] \[\leqslant\left(\frac{\varepsilon\delta}{m^{2}}\right)\] (7)

where we took \(k_{\text{sim}}\geqslant 1/2\cdot\log(\frac{m^{2}}{\varepsilon\delta})/\varepsilon\), which completes the proof. 

We now condition on the event in Claim A.4 occurring for all \(p\in P\), which holds with probability at least \(1-|P|\cdot\left(\frac{\varepsilon\delta}{m^{2}}\right)>1-\left(\frac{ \varepsilon\delta}{m}\right)\) by a union bound. Call this event \(\mathcal{E}\), and condition on it in what follows.

Now first suppose that we are in case **(B)**, and the set \(S\) of points which map to the cluster \(\boldsymbol{\varphi}(q)\) is given by \(S=\{p^{\prime}\}\) where \(p^{\prime}=\arg\min_{p\in P}\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_ {0}\). Firstly, if \(p^{\prime}=p^{*}\), then we are done as \(\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P)\rangle=\langle q,p^ {*}\rangle\), and 5 follows. Otherwise, by Claim A.4 we must have had \(|\theta(q,p^{\prime})-\theta(q,p^{*})|\leqslant\pi\cdot\sqrt{\varepsilon}\). Using that the Taylor expansion of cosine is \(\cos(x)=1-x^{2}/2+O(x^{4})\), we have

\[|\cos(\theta(q,p^{\prime}))-\cos(\theta(q,p^{*}))|\leqslant O(\varepsilon)\]

Thus

\[\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P)\rangle =\langle q,p^{\prime}\rangle\] (8) \[=\cos(\theta(q,p^{\prime}))\] \[\geqslant\cos(\theta(q,p^{*}))-O(\varepsilon)\] \[=\max_{p\in P}\langle q,p\rangle-O(\varepsilon)\]

which proves the desired statement 5 after a constant factor rescaling of \(\varepsilon\).

Next, suppose we are in case **(A)** where \(S=\{p\in P\ ^{\prime}|\ \boldsymbol{\varphi}(p)=\boldsymbol{\varphi}(q)\}\) is non-empty. In this case, \(S\) consists of the set of points \(p\) with \(\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_{0}=0\). From this, it follows again by Claim A.4that \(\theta(q,p)\leqslant\sqrt{\varepsilon\pi}\) for any \(p\in S\). Thus, by the same reasoning as above, we have

\[\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P)\rangle =\frac{1}{|S|}\sum_{p\in S}\cos(\theta(q,p^{\prime}))\] (9) \[\geqslant\frac{1}{|S|}\sum_{p\in S}(1-O(\varepsilon))\] \[\geqslant\frac{1}{|S|}\sum_{p\in S}(\langle q,p^{*}\rangle-O( \varepsilon))\] \[=\max_{p\in P}\langle q,p\rangle-O(\varepsilon)\]

which again proves the desired statement 5 in case **(A)**, thereby completing the full proof in the case where there are no random projections.

To analyze the expectation, note that using the fact that \(|\langle\mathbf{F}_{\text{q}}(q),\mathbf{F}_{\text{doc}}(P)\rangle|\leqslant 1\) deterministically, the small \(O(\varepsilon\delta)\) probability of failure (i.e. the event that \(\mathcal{E}\) does not hold) above can introduce at most a \(O(\varepsilon\delta)\leqslant\varepsilon\) additive error into the expectation, which is acceptable after a constant factor rescaling of \(\varepsilon\).

Finally, to incorporate projections, by standard consequences of the Johnson Lindenstrauss Lemma (Fact A.2) setting \(d_{\text{proj}}=O(\frac{1}{\varepsilon^{2}}\log\frac{m}{\varepsilon})\) and projecting via a random Gaussian or \(\pm 1\) matrix from \(\bm{\psi}:\mathbbm{R}^{d}\to\mathbbm{R}^{d_{\text{proj}}}\), for any set \(S\subset P\) we have that \(\mathbb{E}\left[\langle\bm{\psi}(q),\bm{\psi}(\frac{1}{|S|}\sum_{p\in S}p) \rangle\right]=\langle q,\frac{1}{|S|}\sum_{p\in S}p\rangle\), and moreover that \(\langle q,\frac{1}{|S|}\sum_{p\in S}p\rangle=\langle\bm{\psi}(q),\bm{\psi}( \frac{1}{|S|}\sum_{p\in S}p)\rangle\|q\|_{2}\|\frac{1}{|S|}\sum_{p\in S}p\|_{2}\pm\varepsilon\) for all \(q\in Q,p\in P\) with probability at least \(1-\varepsilon\delta\). Note that \(\|q\|_{2}=1\), and by triangle inequality \(\|\frac{1}{|S|}\sum_{p\in S}p\|_{2}\leqslant\frac{1}{|S|}\sum_{p\in S}\|p\|_{2}=1\). Thus, letting \(\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P)\) be the FDE values without the inner projection \(\bm{\psi}\) and \(\mathbf{F}_{\text{q}}^{\psi}(Q),\mathbf{F}_{\text{doc}}^{\psi}(P)\) be the FDE values with the inner projection \(\bm{\psi}\), conditioned on the above it follows that

\[\frac{1}{|Q|}\langle\mathbf{F}_{\text{q}}^{\psi}(Q),\mathbf{F}_{ \text{doc}}^{\psi}(P)\rangle =\frac{1}{|Q|}\sum_{q\in Q}\langle\mathbf{F}_{\text{q}}^{\psi}(q),\mathbf{F}_{\text{doc}}^{\psi}(P)\rangle\] (10) \[=\frac{1}{|Q|}\sum_{q\in Q}\left(\langle\mathbf{F}_{\text{q}}(q), \mathbf{F}_{\text{doc}}(P)\rangle\pm\varepsilon\right)\] \[=\frac{1}{|Q|}\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{ doc}}(P)\rangle\pm\varepsilon\]

Finally, to analyze the expectation, note that since

\[\left|\frac{1}{|Q|}\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P )\rangle\right|\leqslant\frac{1}{|Q|}\sum_{q\in Q}|\langle\mathbf{F}_{\text{q} }(q),\mathbf{F}_{\text{doc}}(P)\rangle|\leqslant 1\]

as before conditioning on this small probability event changes the expectation of 5 by at most a \(\varepsilon\) additive factor, which completes the proof of the Theorem after a constant factor rescaling of \(\varepsilon\).

Equipped with Theorem 2.1, as well as the sparsity bounds from Lemma A.1, we are now prepared to prove our main theorem on approximate nearest neighbor search under the Chamfer Similarity.

**Theorem 2.2**.: _Fix any \(\varepsilon>0\), query \(Q\), and dataset \(P=\{P_{1},\ldots,P_{n}\}\), where \(Q\subset\mathbbm{R}^{d}\) and each \(P_{i}\subset\mathbbm{R}^{d}\) is a set of unit vectors. Let \(m=|Q|+\max_{i\in[n]}|P_{i}|\). Then setting \(k_{\text{sim}}=O(\frac{\log m}{\varepsilon})\), \(d_{\text{proj}}=O(\frac{1}{\varepsilon^{2}}\log(m/\varepsilon))\) and \(R_{\text{reps}}=O(\frac{1}{\varepsilon^{2}}\log n)\) so that \(d_{\text{FDE}}=m^{O(1/\varepsilon)}\cdot\log n\). Then setting \(i^{*}=\arg\max_{i\in[n]}\langle\mathbf{F}_{q}(Q),\mathbf{F}_{\text{doc}}(P_ {i})\rangle\), with high probability (i.e. \(1-1/\operatorname{poly}(n)\)) we have:_

\[\textsc{NChamfer}(Q,P_{i^{*}})\geqslant\max_{i\in[n]}\textsc{NChamfer}(Q,P_{i})-\varepsilon\]

_Given the query \(Q\), the document \(P^{*}\) can be recovered in time \(O\left(|Q|\max\{d,n\}\frac{1}{\varepsilon^{4}}\log(\frac{m}{\varepsilon})\log n\right)\)._Proof of Theorem 2.2.: First note, for a single repetition, for any subset \(P_{j}\in D\), by Theorem 2.1 we have

\[\mathbb{E}\left[\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P_{j}) \rangle\right]=\text{NChamfer}(Q,P)\pm\varepsilon\]

Moreover, as demonsrated in the proof of Theorem 2.1, setting \(\delta=1/10\), we have

\[\left|\frac{1}{|Q|}\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P_{ j})\rangle\right|\leqslant\frac{1}{|Q|}\sum_{q\in Q}|\langle\mathbf{F}_{\text{q}}(q), \mathbf{F}_{\text{doc}}(P_{j})\rangle|\leqslant 1\]

It follows that for each repetition \(i\in[R_{\text{reps}}]\), letting \(\mathbf{F}_{\text{q}}(Q)^{i},\mathbf{F}_{\text{doc}}(P_{j})^{i}\) be the coordinates in the final FDE vectors corresponding to that repetition, the random variable \(X_{i}=\frac{1}{|Q|}\langle\mathbf{F}_{\text{q}}^{i}(Q),\mathbf{F}_{\text{doc }}^{i}(P_{j})\rangle\) is bounded in \([-1,1]\) and has expectation \(\text{NChamfer}(Q,P_{j})\pm\varepsilon\). By Chernoff bounds, averaging over \(R_{\text{reps}}=O(\frac{1}{\varepsilon^{2}}\log(n))\) repetitions, we have

\[\left|\sum_{i=1}^{R_{\text{reps}}}\frac{1}{R_{\text{reps}}|Q|}\langle\mathbf{ F}_{\text{q}}^{i}(Q),\mathbf{F}_{\text{doc}}^{i}(P_{j})\rangle-\text{NChamfer}(Q,P_{ j})\right|\leqslant 2\varepsilon\] (11)

with probability \(1-1/n^{C}\) for any arbitrarily large constant \(C>1\). Note also that \(\sum_{i=1}^{R_{\text{reps}}}\frac{1}{R_{\text{reps}}|Q|}\langle\mathbf{F}_{ \text{q}}^{i}(Q),\mathbf{F}_{\text{doc}}^{i}(P_{j})\rangle=\frac{1}{R_{\text {reps}}|Q|}\langle\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P_{j})\rangle\), where \(\mathbf{F}_{\text{q}}(Q),\mathbf{F}_{\text{doc}}(P_{j})\) are the final FDEs. We can then condition on (11) holding for all documents \(j\in[n]\), which holds with probability with probability \(1-1/n^{C-1}\) by a union bound. Conditioned on this, we have

\[\text{NChamfer}(Q,P_{i^{*}}) \geqslant\frac{1}{R_{\text{reps}}|Q|}\langle\mathbf{F}_{\text{q}} (Q),\mathbf{F}_{\text{doc}}(P_{i^{*}})\rangle-2\varepsilon\] \[=\max_{j\in[n]}\frac{1}{R_{\text{reps}}|Q|}\langle\mathbf{F}_{ \text{q}}(Q),\mathbf{F}_{\text{doc}}(P_{j})\rangle-2\varepsilon\] (12) \[\geqslant\max_{j\in[n]}\text{NChamfer}(Q,P_{j})-6\varepsilon\]

which completes the proof of the approximation after a constant factor scaling of \(\varepsilon\). The runtime bound follows from the runtime required to compute \(\mathbf{F}_{\text{q}}(Q)\), which is \(O(|Q|R_{\text{reps}}d(d_{\text{proj}}+k_{\text{sim}}))=O(|Q|\frac{\log n}{ \varepsilon^{2}}d(\frac{1}{\varepsilon^{2}}\log(m/\varepsilon)+\frac{1}{ \varepsilon}\log m)\), plus the runtime required to brute force search for the nearest dot product. Specifically, note that each of the \(n\) FDE dot products can be computed in time proportional to the sparsity of \(\mathbf{F}_{\text{q}}(Q)\), which is at most \(O(|Q|d_{\text{proj}}R_{\text{reps}})=O(|Q|\frac{1}{\varepsilon^{4}}\log(m/ \varepsilon)\log n)\). Adding these two bounds together yields the desired runtime. 

## Appendix B Additional Dataset Information

In Table 9 we provide further dataset-specific information on the BEIR retrieval datasets used in this paper. Specifically, we state the sizes of the query and corpuses used, as well as the average number of embeddings produced by the ColBERTv2 model per document. Specifically, we consider the six BEIR retrieval datasets MS MARCO [40], NQ [31], HotpotQA [53], ArguAna [47], SciDocs [11], and Quora [46], Note that the MV corpus (after generating MV embeddings on all documents in a corpus) will have a total of \(\#\text{Corpus}\times(\text{Avg}\,\,\#\,\text{Embeddings per Doc})\) token embeddings. For even further details, see the BEIR paper [46].

Figure 9: Dataset Specific Statistics for the BEIR datasets considered in this paper.

[MISSING_PAGE_FAIL:20]

Figure 11: FDE retrieval vs SV Heuristic, Recall\(@5\)-\(500\)

Figure 10: FDE retrieval vs SV Heuristic, Recall\(@100\)-\(5000\)

\begin{table}
\begin{tabular}{c|c c|c c} \hline Experiment & w/o projection & w/ projection & w/o projection & w/ projection \\ \hline Dimension & 2460 & 2460 & 5120 & 5120 \\ \hline Recall@100 & 77.71 & 78.82 & 80.37 & 83.35 \\ \hline Recall@1000 & 91.91 & 91.62 & 93.55 & 94.83 \\ \hline Recall@10000 & 97.52 & 96.64 & 98.07 & 98.33 \\ \hline \end{tabular}
\end{table}
Table 3: Recall Quality of Final Projection based FDEs with \(d_{\text{FDE}}\in\{2460,5120\}\)

### Comparison to Final Projections.

We now show the effect of employing final projections to reduce the target dimensionality of the FDE's. For all experiments, the final projection \(\bm{\psi}^{\prime}\) is implemented in the same way as inner projections are: namely, via multiplication by a random \(\pm 1\) matrix. We choose four target dimensions, \(d_{\text{FDE}}\in\{2460,5120,10240,20480\}\), and choose the Pareto optimal parameters \((R_{\text{reps}},k_{\text{sim}},d_{\text{proj}})\) from the grid search without final projections in Section 3.1, which are \((20,4,8),(20,5,8),(20,5,16),(20,5,32)\). We then build a large dimensional FDE with the parameters \((R_{\text{reps}},k_{\text{sim}},d_{\text{proj}})=(40,6,128)\). Here, since \(d=d_{\text{proj}}\), we do not use any inner projections when constructing the FDE. We then use a single random final projection to reduce the dimensionality of this FDE from \(R_{\text{reps}}\cdot 2^{k_{\text{sim}}}.d_{\text{proj}}=327680\) down to each of the above target dimensions \(d_{\text{FDE}}\). The results are show in Tables 3 and 4. Notice that incorporating final projections can have a non-trivial impact on recall, especially for Recall\(@10\)00, where it can increase by around \(3\%\). In particular, FDEs with the final projections are often better than FDEs with twice the dimensionality without final projections. The one exception is the \(2460\)-dimensional FDE, where the Recall\(@100\) only improved by \(1.1\%\), and the Recall\(@1000\) was actually lower bound \(0.3\%\).

### Ball Carving

Continuing our discussion from Section 3.2, we show that ball-carving at this threshold of \(0.7\) gives non-trivial efficiency gains. Specifically, in Figure 13, we plot the per-core queries-per-second of re-ranking (i.e. computing \(\textsc{Chamfer}(Q_{C},P)\)) against varying ball carving thresholds for the MS MARCO dataset. Please see the discussion in Section 3.2 for analysis of the figure.

### Product Quantization

PQ DetailsWe implemented our product quantizers using a simple "textbook" \(k\)-means based quantizer. Recall that AH-\(C\)-\(G\) means that each consecutive group of \(G\) dimensions is represented by \(C\) centers. We train the quantizer by: (1) taking for each group of dimensions the coordinates

\begin{table}
\begin{tabular}{c|c c|c c} \hline Experiment & w/o projection & w/ projection & w/o projection & w/ projection \\ \hline Dimension & 10240 & 10240 & 20480 & 20480 \\ \hline Recall@100 & 82.31 & 85.15 & 83.36 & 86.00 \\ \hline Recall@1000 & 94.91 & 95.68 & 95.58 & 95.95 \\ \hline Recall@10000 & 98.76 & 98.93 & 98.95 & 99.17 \\ \hline \end{tabular}
\end{table}
Table 4: Recall Quality of Final Projection based FDEs with \(d_{\text{FDE}}\in\{10240,20480\}\)

Figure 12: Comparison of FDE recall with respect to the most similar point under Chamfer.

of a sample of at most 100,000 vectors from the dataset, and (2) running \(k\)-means on this sample using \(k=C=256\) centers until convergence. Given a vector \(x\in\mathbbm{R}^{d}\), we can split \(x\) into \(d/G\) blocks of coordinates \(x_{(1)},\ldots,x_{(d/G)}\in\mathbbm{R}^{G}\) each of size \(G\). The block \(x_{(i)}\) can be compressed by representing \(x_{(i)}\) by the index of the centroid from the \(i\)-th group that is nearest to \(x_{(i)}\). Since there are \(256\) centroids per group, each block \(x_{(i)}\) can then be represented by a single byte.

ResultsIn Figures 14 and 15 we show the full set of results for our QPS experiments from Section 3.2 on all of the BEIR datasets that we evaluated in this paper. We include results for both Recall@\(100\) (Figure 14) and Recall@\(1000\) (Figure 15).

We find that PQ-\(256\)-\(8\) is consistently the best performing PQ codec across all of the datasets that we tested. Not using PQ at all results in significantly worse results (worse by at least \(5\times\) compared to using PQ) at the same beam width for the beam; however, the recall loss due to using PQ-\(256\)-\(8\) is minimal, and usually only a fraction of a percent. Since our retrieval engine works by over-retrieving with respect to the FDEs and then reranking using Chamfer similarity, the loss due to approximating the FDEs using PQ can be handled by simply over-retrieving slightly more candidates.

We also observe that the difference between different PQ codecs is much more pronounced in the lower-recall regime when searching for the top \(1000\) candidates for a query. For example, most of the plots in Figure 15 show significant stratification in the QPS achieved in lower recall regimes,

Figure 14: Plots showing the QPS vs. Recall@\(100\) for Muvera on the BEIR datasets we evaluate in this paper. The different curves are obtained by using different PQ methods on 10240-dimensional FDEs.

Figure 13: Per-Core Re-ranking QPS versus Ball Carving Threshold, on MS MARCO dataset.

with PQ-\(256\)-\(16\) (the most compressed and memory-efficient format) usually outperforming all others; however, for achieving higher recall, PQ-\(256\)-\(16\) actually does much worse than slightly less compressed formats like PQ-\(256\)-\(8\) and PQ-\(256\)-\(4\).

Figure 15: Plots showing the QPS vs. Recall@\(1000\) for Muvera on the BEIR datasets we evaluate in this paper. The different curves are obtained by using different PQ methods on 10240-dimensional FDEs.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims made in the abstract and introduction are thoroughly discussed and evaluated in the remaining sections of the paper. The theoretical claims are justified in Section 2.1, and the experimental claims are justified in Section 3. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: A limitations section is provided at the end of the paper in Section 4, along with the broader impacts. We discuss there the primary limitations of the paper. The computational efficiency of the algorithms are discussed in the latency experiments Section 3.2 of the paper, as well as how the MIPS index scales with dataset size. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All theoretical results are formally stated in the main paper, along with all relevant assumptions in Section 2.1. All definitions are formally given in the main paper, with full proofs deferred to the appendix in Section A, with ideas of the proof given in the main paper in Section 2.1. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The algorithm for generating FDEs is thoroughly and exactly described in Section 2, and is easy to reproduce from the description. As mentioned in Section 3.2, we will publish a standalone open-source shared-memory implementation of the FDE generation step upon publication. Further note that the DiskANN library used for retrieving via MIPS is publicly available, and we explicitly state the parameters we ran it with. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Our end-to-end retrieval engine is implemented in C++ in a proprietary codebase, preventing us from directly releasing it. As described in Section 3.2, we plan to publish a standalone open-source implementation of the FDE generation step upon publication, along with the product quantization code (which is a textbook method) and the ball-carving code. The only other component of the algorithm is DiskANN, which is publicly available and properly cited. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: No models were trained for this paper, however details about all datasets and how we evaluated our algorithms on them were precisely defined. All parameters used in our algorithms were explicitly stated for every experiment in their corresponding section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We run detailed variance experiments in the supplementary in Appendix C.1, and describe the main conclusions of these results in Section 3.1, demonstrating that our FDE recall is extremely stable and varies very little depending on the random seed. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: For all our online (latency) experiments, we state the exact hardware and compute resources we used in Section 3.2. The results of offline experiments (Section 3.1) do not depend on hardware implementation, as they are measuring fixed recall properties of an algorithm (without any runtime benchmarking), and therefore can be reproduced on any hardware and infrastructure. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the code of ethics and verified that research conducted in the paper fully conforms to the code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts in a standalone section at the end of the paper in Section 4. In essence, since our work is solely about improving the efficiency of multi-vector retrieval, it is unlikely to have any direct path to negative applications, and we discuss this fact in the Broader Impacts section. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper proposes a new algorithm that purely improves the efficiency of multi-vector retrieval; we do not release any data or new models. Therefore our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The authors and sources of all six BEIR datasets that were used in this paper are cited, as well as the ColBERTv2 model that was used to generate the multi-vector embeddings, and the DiskANN algorithm we used for single vector retrieval. For each of these aforementioned assets, the license of the asset was explicitly stated in the text: we stated the license for the BEIR datasets in Section 3, and the license for DiskANN in Section 3.2. The terms of use of all these licenses were properly respected in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets were introduced in this paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: No crowdsourcing nor research with human subjects with used in this paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: No crowdsourcing nor research with human subjects with used in this paper. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.