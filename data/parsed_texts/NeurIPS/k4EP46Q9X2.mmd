# Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators

 Yiyan Huang

The Hong Kong Polytechnic University

yiyhuang3-c@my.cityu.edu.hk

&Cheuk Hang Leung

City University of Hong Kong

chleung87@cityu.edu.hk

&Siyi Wang

City University of Hong Kong

siyi.wang@my.cityu.edu.hk

&Yijun Li

City University of Hong Kong

yijunli5-c@my.cityu.edu.hk

&Qi Wu

City University of Hong Kong

qi.wu@cityu.edu.hk

The co-first authors.Corresponding author.

###### Abstract

The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). Various types of CATE estimators have been developed with advancements in machine learning and causal inference. However, selecting the desirable CATE estimator through a conventional model validation procedure remains impractical due to the absence of counterfactual outcomes in observational data. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two challenges. First, they must determine the metric form and the underlying machine learning models for fitting nuisance parameters (e.g., outcome function, propensity function, and plug-in learner). Second, they lack a specific focus on selecting a robust CATE estimator. To address these challenges, this paper introduces a Distributionally Robust Metric (DRM) for CATE estimator selection. The proposed DRM is nuisance-free, eliminating the need to fit models for nuisance parameters, and it effectively prioritizes the selection of a distributionally robust CATE estimator. The experimental results validate the effectiveness of the DRM method in selecting CATE estimators that are robust to the distribution shift incurred by covariate shift and hidden confounders.

## 1 Introduction

The escalating demand for decision-making has sparked an increasing interest in _Causal Inference_ across various research domains, such as economics [23, 13, 43, 1], statistics [70, 49, 26, 41], healthcare [78, 27, 61, 9, 42], and financial application [11, 15, 37, 21, 35, 24, 50]. The primary goal in personalized decision-making is to quantify the causal effect of a specific treatment (or policy/intervention) on the target outcome, and understanding such causal effects is closely connected with identifying the _Conditional Average Treatment Effect (CATE)_. In observational studies, identifying the CATE faces a significant and fundamental challenge: the absence of _counterfactual_ knowledge. According to Rubin Causal Model [64], the CATE is determined by comparing _potential outcomes_under different treatment assignments (i.e., treat and control) for a specific covariate. Nonetheless, in real-world applications, we can only observe the potential outcome under the actual treatment (i.e., _factual outcome_), while the potential outcome under the alternative treatment (i.e., _counterfactual outcome_) remains unobserved. The unavailability of the counterfactual outcome is widely recognized as the fundamental problem in causal inference [33], making it difficult to accurately determine the true value of the CATE.

The advancement of machine learning (ML) has opened up a promising opportunity to improve the CATE estimation from observational data. Several innovative CATE estimation approaches, such as meta-learners and causal ML models, have been proposed to tackle the fundamental challenge in causal inference and enhance the predictive accuracy of CATE estimates (as discussed in Section 3). Nevertheless, the emergence of various CATE estimation methods has brought forth a new question: _Given multifarious options for CATE estimators, which should be chosen?_ In observational data, treatment is often non-random and propensity scores remain unknown. Conventional model validation procedures, unfortunately, are not suitable for CATE estimator selection in this case due to the absence of ground truth CATE labels. Therefore, exploring proper metrics for CATE estimator selection remains an essential yet challenging research topic in causal inference.

Recent research has emphasized the significance of model selection for CATE estimators, as highlighted in [66, 20, 53]. These works have proposed and summarized two types of criteria for CATE estimator selection, the _plug-in_ metric \(\mathcal{R}^{plug}_{\tilde{\tau}}(\hat{\tau})\) and the _pseudo-outcome_ metric \(\mathcal{R}^{pseudo}_{\tilde{Y}}(\hat{\tau})\):

\[\mathcal{R}^{plug}_{\tilde{\tau}}(\hat{\tau})=\sqrt{\frac{1}{n}\sum_{i=1}^{n} (\hat{\tau}(X_{i})-\hat{\tau}(X_{i}))^{2}},\quad\mathcal{R}^{pseudo}_{ \tilde{Y}}(\hat{\tau})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{\tau}(X_{i})-\tilde {Y}_{i})^{2}}.\] (1)

One can establish a plug-in estimator \(\tilde{\tau}\) or construct a pseudo-outcome estimator \(\tilde{Y}\) using the validation data to select CATE estimator \(\hat{\tau}\). The previous studies [66, 20, 53] have shown that these metrics offer some assistance in identifying well-performing CATE estimators. However, two additional challenges are still encountered in these two metrics.

Challenge 1: How to determine the metric form and underlying ML models for nuisance parameters?As previously discussed, plug-in and pseudo-outcome metrics have various forms, and both of them rely on estimating nuisance parameters \(\tilde{\eta}\) using ML algorithms such as linear models, tree-based models, etc. Plug-in metrics even need to fit an additional ML model for the plug-in learner \(\tilde{\tau}\). However, selecting the suitable metric form and ML algorithms can be very difficult without the knowledge of true data generating process. Consequently, we might go round in circles as this challenge leads us back to the original estimator selection problem [20].

Challenge 2: These metrics are not well-targeted for selecting a robust CATE estimator.In potential outcome framework [64], the factual distribution \(P^{F}\) and the counterfactual distribution \(P^{CF}\) for \(t\in\{0,1\}\) can be defined as follows:

\[\begin{split} P^{F}:=P(X,Y^{t}|T=t)&=P(Y^{t}|X,T =t)P(X|T=t);\\ P^{CF}:=P(X,Y^{t}|T=1-t)&=P(Y^{t}|X,T=1-t)P(X|T=1- t).\end{split}\] (2)

The above (2) reveals that the covariate shift \(P(X|T=t)\neq P(X|T=1-t)\) leads to a distribution shift between \(P^{F}\) and \(P^{CF}\) - and such distribution shift can be further exacerbated once the unconfounded assumption \(P(Y^{t}|X,T=t)=P(Y^{t}|X,T=1-t)\) is violated. It is widely recognized that ML models often struggle when the training and test data do not adhere to the same distribution. Therefore, it becomes essential to select a CATE estimator learned on \(P^{F}\) that demonstrates robust performance to the counterfactual distribution \(P^{CF}\). This need for robustness holds even greater significance than the pursuit of an ideal "stellar" estimator because striving for the perfect estimator can be futile in the absence of ground truth counterfactual labels.

Contributions.In this paper, we propose a Distributionally Robust Metric (DRM) for CATE estimator selection. The main contributions are summarized as follows: (1) The proposed DRM method is nuisance-free, eliminating the need to fit models for nuisance parameters (outcome function, propensity function, and plug-in learner). (2) The DRM method is designed to prioritize selecting a distributionally robust CATE estimator. (3) We provide a finite sample analysis of the proposeddistributionally robust value \(\hat{\mathcal{V}}^{t}(\hat{\tau})\) for \(t\in\{0,1\}\), showing it decays to \(\mathcal{V}^{t}(\hat{\tau})\) at a rate of \(n^{-1/2}\). (4) Experimental results validate the effectiveness of the DRM method in selecting a CATE estimator that is robust to the distribution shift incurred by covariate shift and hidden confounders.

## 2 Background of CATE Estimator Selection

Suppose the observational data contain \(n\) i.i.d. samples \(\{(x_{i},t_{i},y_{i})\}_{i=1}^{n}\), with the associated random variables being \(\{(X_{i},T_{i},Y_{i})\}_{i=1}^{n}\). For each unit \(i\), \(X_{i}\in\mathcal{X}\subset\mathbb{R}^{d}\) is \(d\)-dimensional covariates and \(T_{i}\in\{0,1\}\) is the binary treatment. Potential outcomes for treat (\(T=1\)) and control (\(T=0\)) are denoted by \(Y^{1},Y^{0}\in\mathcal{Y}\subset\mathbb{R}\). The observed (factual) outcome is \(Y=TY^{1}+(1-T)Y^{0}\). The propensity score [63] is defined as \(\pi(x):=P(T=1\mid X=x)\). The conditional mean potential outcome surface is defined as \(\mu_{t}(x):=\mathbb{E}\left[Y^{t}\mid X=x\right]\) for \(t\in\{0,1\}\). The true CATE is defined as

\[\tau_{true}(x):=\mathbb{E}\left[Y^{1}-Y^{0}\mid X=x\right]=\mu_{1}(x)-\mu_{0 }(x).\]

Following the standard and necessary assumptions in potential outcome framework [64], we impose Assumption 2.1 that ensure treatment effects are identifiable.

**Assumption 2.1** (Consistency, Overlap, and Unconfoundedness).: Consistency: If the treatment is \(t\), then the observed outcome \(Y\) equals \(Y^{t}\). Overlap: The propensity score is bounded away from \(0\) to \(1\), i.e., \(0<\pi(x)<1\), \(\forall x\in\mathcal{X}\). Unconfoundedness 3: \(Y^{t}\perp\!\!\!\perp T\mid X,\ \forall t\in\{0,1\}\).

Footnote 3: Note that in the setting C of our experiments, the unconfoundedness assumption is violated, leading to misspecified nuisance parameters in CATE estimators and baseline selectors.

The goal of CATE estimator selection is to select the best CATE estimator, denoted by \(\hat{\tau}_{best}\), from a set of \(J\) candidate estimators \(\{\hat{\tau}_{1},\ldots,\hat{\tau}_{J}\}\):

\[\hat{\tau}_{best}=\operatorname*{arg\,min}_{\hat{\tau}\in\{\hat{\tau}_{1}, \ldots,\hat{\tau}_{J}\}}\mathcal{R}^{oracle}(\hat{\tau}),\quad\mathcal{R}^{ oracle}(\hat{\tau}):=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{\tau}(X_{i})-\tau_{true}(X_{i} ))^{2}}.\] (3)

Here, \(\mathcal{R}^{oracle}(\hat{\tau})\) is associated with \(\mathbb{E}[(\hat{\tau}(X)-\tau_{true}(X))^{2}]\), known as the Precision of Estimating Heterogeneous Effects (PEHE) w.r.t. \(\hat{\tau}\)[32, 68]. Note that \(\mathcal{R}^{oracle}(\hat{\tau})\) cannot be employed to evaluate CATE estimators' performances in real applications as we do not have access to \(\tau_{true}\). Previous studies have introduced plug-in and pseudo-outcome metrics to aid in CATE estimator selection, as shown in equation (1). Then, the CATE estimator \(\hat{\tau}_{select}\) is selected on validation data by

\[\hat{\tau}_{select}=\operatorname*{arg\,min}_{\hat{\tau}\in\{\hat{\tau}_{1}, \ldots,\hat{\tau}_{J}\}}\mathcal{R}_{\hat{\tau}}^{plug}(\hat{\tau})\quad\text {or}\quad\hat{\tau}_{select}=\operatorname*{arg\,min}_{\hat{\tau}\in\{\hat{ \tau}_{1},\ldots,\hat{\tau}_{J}\}}\mathcal{R}_{\hat{Y}}^{pseudo}(\hat{\tau}).\] (4)

Notably, both the plug-in and pseudo-outcome metrics necessitate the fitting of nuisance parameters \(\tilde{\eta}\) (e.g., \(\tilde{\eta}=(\tilde{\mu}_{1},\tilde{\mu}_{0},\tilde{\pi})\)) using off-the-shelf ML models. While some papers like [16] address the selection of nuisance parameters for Aerate Treatment Effect (ATE) estimators, e.g., the doubly robust estimator [23, 13, 36], our paper focuses on the selection of CATE estimators rather than nuisance parameters. For the plug-in metric, \(\tilde{\tau}\) can be constructed using any CATE estimator discussed in Appendix A.1, yielding metrics such as plug-T, plug-DR, etc. For the pseudo-outcome metric, \(\tilde{Y}\) can be constructed using a specific formula discussed in Appendix A.2, yielding metrics such as pseudo-DR, pseudo-R, etc. The metrics based on the influence function [5] and the R-learner objective [55] are categorized into the pseudo-outcome metric. The categorization of plug-in and pseudo-outcome metrics maintains consistency with [20, 53].

## 3 Related Work

CATE estimation.Recent advancements in ML have emerged as powerful tools for estimating CATE from observational data, and researchers pay particular attention to _meta-learners_ and _causal ML_ models. Existing meta-learners mainly include traditional learners such as S-learner, T-learner, PS-learner, and IPW-learner, as well as new learners such as X-learner [47], U-learner [25, 55], DR-learner [41, 26], R-learner [55], and RA-learner [18]. The specific details of these meta-learners are stated in Appendix A.1. Additionally, some studies also focus on developing innovative causal MLmodels for CATE estimation, such as Causal BART [30], Causal Forest [70; 8; 58], generative models like CEVAE [51] and GANITE [77], representation learning nets including SITE [76], TARNet [68], Dragonnet [69], FlexTENet [19], and HTCE [10], disentangled learning nets like D\({}^{2}\)VD [44; 45], DeR-CFR [74], and DR-CFR [31], and representation balancing nets such as BNN [39], CFRNet [68], DKLITE [79], IGNITE [29], BWCFR [6], DRRB [35], and DIGNet [38]. Recent surveys [28; 75; 56] have also conducted a systematic review of various causal inference methods.

CATE estimator selection.Compared to the diverse range of CATE estimation methods, selecting CATE estimators has received limited attention in existing causal inference research. Current methods for selecting CATE estimators can be broadly classified into two main categories. **The first category**, which is also considered in this paper, involves using plug-in and pseudo-outcome methods to evaluate CATE estimators. These methods share two common characteristics: 1) Both methods require fitting ML models for nuisances (e.g., outcome function, propensity function, CATE function) on a validation set and then implementing the learned ML models in either the plug-in surrogate or the pseudo-outcome surrogate; 2) Both methods serve as surrogates for the expected error between the CATE estimator and the true CATE, i.e., \(\mathcal{R}^{oracle}(\hat{\tau})\) in equation (3). The difference between the two methods is that the plug-in method directly approximates the true CATE function, where only covariate variables are involved, while the pseudo-outcome method typically constructs a specific formula incorporating covariates, treatment, and outcome variables. For example, the pseudo-DR proposed in [65] is constructed by the outcome predictors learned with representation balancing objective [68; 40]. Recent research [66; 20; 53] has conducted thorough empirical investigations into exploring these two methods for selecting CATE estimators. Their findings suggest that no single selection criterion can universally outperform others in all scenarios in the task of selecting CATE estimators. More details of the two selection methods are stated in Appendix A.2. **The second category** considers leveraging the data generating process (DGP) to generate synthetic data with the known true CATE function, allowing the validation of CATE estimators' performance on this synthetic data. For example, authors in [2] find that placebo and structured empirical Monte Carlo methods are helpful for estimator selection under some restrictive conditions. In addition, researchers in [67; 7; 59] focus on training generative models to enforce the generated data to approximate the distribution of the observed data. However, the DGP-based method still faces some limitations in CATE estimator selection: 1) it only guarantees the resemblance of the generated data to the factual distribution, without considering the counterfactual distribution; and ii) there is a potential risk of the method favoring estimators that closely resemble the generative models [17].

## 4 The Distributionally Robust Metric

In this section, we introduce the Distributionally Robust Metric (DRM) for CATE estimator selection. First, we capture the uncertainty in PEHE in a distributionally robust manner (Section 4.1). We then establish the DRM based on the distributionally robust value of PEHE (Section 4.2).

### Capturing the Uncertainty in PEHE

**Proposition 4.1**.: _The PEHE w.r.t. the CATE estimator \(\hat{\tau}\) can be decomposed as follows:_

\[\mathbb{E}[(\hat{\tau}(X)-\tau_{true}(X))^{2}]=\mathbb{E}[\hat{\tau}(X)^{2}]+ 2\mathbb{E}[\hat{\tau}(X)Y^{0}]+2\mathbb{E}[-\hat{\tau}(X)Y^{1}]+\zeta,\] (5)

_where \(\zeta=\mathbb{E}[(\mu_{1}(X)-\mu_{0}(X))^{2}]\). The proof is deferred to Appendix B.1._

Proposition 4.1 indicates that the PEHE is equal to four terms, where \(\mathbb{E}[\hat{\tau}(X)^{2}]\), \(\mathbb{E}[\hat{\tau}(X)Y^{0}]\), and \(\mathbb{E}[-\hat{\tau}(X)Y^{1}]\) depend on \(\hat{\tau}\), while \(\zeta\) is a constant that is independent of \(\hat{\tau}\). The term \(\mathbb{E}[\hat{\tau}(X)Y^{t}]\) for \(t\in\{0,1\}\) can be further decomposed as follows:

\[\mathbb{E}[\hat{\tau}(X)Y^{t}]=\underbrace{\mathbb{E}[\hat{\tau}(X)Y^{t}|T=t]}_ {\text{(a) Empirically computable}}P(T=t)+\underbrace{\mathbb{E}[\hat{\tau}(X)Y^{t}| T=1-t]}_{\text{(b) Empirically uncomputable}}P(T=1-t).\] (6)

Equation (6a) can be computed empirically since the potential outcome \(Y^{t}\) is observable in the group of \(T=t\). However, equation (6b) is empirically uncomputable due to the unavailability of \(Y^{t}\) in the group of \(T=1-t\). The unknown term \(\mathbb{E}[\hat{\tau}(X)Y^{t}|T=1-t]\) therefore determines the uncertainty in PEHE. To capture such an uncertainty, we therefore establish distributionally robust values for \(\mathbb{E}[\hat{\tau}(X)Y^{0}|T=1]\) and \(\mathbb{E}[-\hat{\tau}(X)Y^{1}|T=0]\) based on a Kullback-Leibler (KL) ambiguity set.

**Definition 4.2** (KL ambiguity set).: Given two distributions \(Q\) and \(P\) and the ambiguity radius \(\epsilon>0\). The KL ambiguity (uncertainty) set \(\mathcal{B}_{\epsilon}(P)\) is defined as

\[\mathcal{B}_{\epsilon}(P):=\{Q:D_{KL}(Q||P)\leq\epsilon\},\quad\text{where }D_{KL}(Q||P)=\int_{\mathcal{X}}q(x)\log\frac{q(x)}{p(x)}dx.\] (7)

Here, \(D_{KL}(Q||P)\) denotes the KL divergence of some arbitrary distribution \(Q\) from the reference distribution \(P\). Now we define the distribution of \((X,Y^{0},Y^{1})\) in the treated and controlled groups as

\[P_{T}:=P(X,Y^{0},Y^{1}|T=1);\ P_{C}:=P(X,Y^{0},Y^{1}|T=0).\] (8)

By setting an adequately large ambiguity radius in Definition 4.2, the following inequalities hold for \(\mathbb{E}[\hat{\tau}(X)Y^{0}|T=1]=\mathbb{E}^{P_{T}}[\hat{\tau}(X)Y^{0}]\) and \(\mathbb{E}[-\hat{\tau}(X)Y^{1}|T=0]=\mathbb{E}^{P_{C}}[-\hat{\tau}(X)Y^{1}]\):

\[\mathbb{E}[\hat{\tau}(X)Y^{0}|T=1]=\mathbb{E}^{P_{T}}[\hat{\tau}(X)Y^{0}] \leq\sup_{Q\in B_{\epsilon_{0}}(P_{C})}\mathbb{E}^{Q}[\hat{\tau}(X)Y^{0}]=: \mathcal{V}^{0}(\hat{\tau});\] (9)

To provide a clearer understanding, let us consider the example of \(\mathbb{E}^{P_{T}}[\hat{\tau}(X)Y^{0}]\). Since the term \(\mathbb{E}[\hat{\tau}(X)Y^{0}]\) is computable on its factual distribution \(P_{C}\) but uncomputable on its counterfactual distribution \(P_{T}\), we can construct an ambiguity set centered around the distribution \(P_{C}\) such that it is large enough to contain the distribution \(P_{T}\). By doing so, we can capture the uncertainty of \(\mathbb{E}^{P_{T}}[\hat{\tau}(X)Y^{0}]\) w.r.t. \(\hat{\tau}\). In other words, the value of the uncomputable quantity \(\mathbb{E}^{P_{T}}[\hat{\tau}(X)Y^{0}]\) will be **at most**\(\mathcal{V}^{0}(\hat{\tau})\). Similarly, the value of the uncomputable quantity \(\mathbb{E}^{P_{C}}[-\hat{\tau}(X)Y^{1}]\) will be **at most**\(\mathcal{V}^{1}(\hat{\tau})\). Obviously, the uncertainty in PEHE will be larger if the distribution shift between factual and counterfactual distribution is severer. Consequently, we can obtain the distributionally robust value of PEHE in Corollary 4.3, which measures the uncertainty in PEHE.

**Corollary 4.3**.: _Let \(\mathcal{V}^{0}(\hat{\tau})\) and \(\mathcal{V}^{1}(\hat{\tau})\) be the quantities defined in equation (9), \(\zeta\) be the constant given in Proposition 4.1, \(u_{1}:=P(T=1)\), and \(u_{0}=1-u_{1}=P(T=0)\). The distributionally robust value of PEHE w.r.t. \(\hat{\tau}\) is defined as \(\mathcal{V}_{PEHE}(\hat{\tau})\) such that_

\[\mathbb{E}[(\hat{\tau}(X)-\tau_{true}(X))^{2}]\leq\mathcal{V}_{ PEHE}(\hat{\tau})\] (10) \[=\mathbb{E}[\hat{\tau}(X)^{2}]+2\left(u_{0}\mathbb{E}^{P_{C}}[ \hat{\tau}(X)Y^{0}]+u_{1}\mathbb{E}^{P_{T}}[-\hat{\tau}(X)Y^{1}]\right)+2 \left(u_{0}\mathcal{V}^{1}(\hat{\tau})+u_{1}\mathcal{V}^{0}(\hat{\tau})\right) +\zeta.\]

### Establishing Distributionally Robust Metric

As Corollary 4.3 provides the distributionally robust (worst-case) value of PEHE, it can naturally measure the robustness of the CATE estimator \(\hat{\tau}\) against distribution shift between counterfactual distribution and factual distribution. In this section, we will provide two steps involved in using Corollary 4.3 to construct the DRM method for CATE estimator selection.

Step 1: Establishing computational tractability of \(\mathcal{V}^{t}(\hat{\tau})\).The distributionally robust values \(\mathcal{V}^{0}(\hat{\tau})\) and \(\mathcal{V}^{1}(\hat{\tau})\) in equation (10) are initially defined as supremum problems over infinite support, presenting a substantial computational challenge. Theorem 4.4 reformulates the infeasible supremum problems into tractable minimum problems.

**Theorem 4.4**.: _The distributionally robust values \(\mathcal{V}^{0}(\hat{\tau})\) and \(\mathcal{V}^{1}(\hat{\tau})\) in equation (9) are equivalent to_

\[\mathcal{V}^{0}(\hat{\tau}) =\min_{\lambda_{0}>0}\lambda_{0}\epsilon_{0}+\lambda_{0}\log \mathbb{E}^{P_{C}}[\exp(\hat{\tau}(X)Y^{0}/\lambda_{0})];\] (11) \[\mathcal{V}^{1}(\hat{\tau}) =\min_{\lambda_{1}>0}\lambda_{1}\epsilon_{1}+\lambda_{1}\log \mathbb{E}^{P_{T}}[\exp(-\hat{\tau}(X)Y^{1}/\lambda_{1})].\]

_The proof is deferred to Appendix B.3._

In the finite-sample scenario, \(\mathcal{V}^{0}(\hat{\tau})\) and \(\mathcal{V}^{1}(\hat{\tau})\) can be empirically approximated as follows:

\[\hat{\mathcal{V}}^{0}(\hat{\tau}) =\min_{\lambda_{0}>0}\lambda_{0}\epsilon_{0}+\lambda_{0}\log \frac{1}{n_{c}}\sum_{i=1}^{n}(1-T_{i})\exp(\hat{\tau}(X_{i})Y_{i}/\lambda_{0});\] (12) \[\hat{\mathcal{V}}^{1}(\hat{\tau}) =\min_{\lambda_{1}>0}\lambda_{1}\epsilon_{1}+\lambda_{1}\log \frac{1}{n_{t}}\sum_{i=1}^{n}T_{i}\exp(-\hat{\tau}(X_{i})Y_{i}/\lambda_{1}).\]Note that in equation (12), the potential outcomes \(Y^{0}\) and \(Y^{1}\) are replaced by the observed outcome \(Y\) due to the fact that \((1-T)Y^{0}=(1-T)Y\) and \(TY^{1}=TY\), which aligns with the Consistency assumption in Assumption 2.1. We then provide a finite-sample analysis of the gap between \(\hat{\mathcal{V}}^{t}(\hat{\tau})\) and \(\mathcal{V}^{t}(\hat{\tau})\) in the following Theorem 4.5, which suggests the gap decays at a rate of \(n^{-1/2}\).

**Theorem 4.5**.: _Let \(u_{t}:=P(T=t)\) for \(t\in\{0,1\}\). Assume \(0<\lambda\leq\lambda_{0},\lambda_{1}\leq\bar{\lambda}\) and \(\hat{\tau}(X)Y\) is bounded within the range of \(M\) to \(\bar{M}\). Define \(C_{exp}=\mathbf{1}_{\{\bar{M}\leq\bar{M}\leq 0\}}\exp\left(\bar{M}/\bar{ \lambda}-M/\bar{\lambda}\right)+\mathbf{1}_{\{\bar{M}\leq 0,\bar{M}\geq 0\}} \exp\left(\bar{M}/\bar{\lambda}-M/\bar{\lambda}\right)+\mathbf{1}_{\{0\leq M \leq\bar{M}\}}\exp\left(\bar{M}/\bar{\lambda}-M/\bar{\lambda}\right)\). For \(n\geq 2/u^{2}\log(2/\delta)\) and \(t\in\{0,1\}\), with probability \(1-\delta\), we have_

\[|\hat{\mathcal{V}}^{t}(\hat{\tau})-\mathcal{V}^{t}(\hat{\tau})|\leq\mathcal{O }\left(\sqrt{\frac{8\bar{\lambda}^{2}\log\frac{2}{\delta}}{nu_{t}^{2}}C_{exp} ^{2}}\right)+\mathcal{O}\left(\sqrt{\frac{2\bar{\lambda}^{2}\log(\frac{2}{ \delta})}{nu_{t}^{2}}}\right).\] (13)

_The proof is deferred to Appendix B.4._

Step 2: Finalizing Distributionally Robust Metric for CATE estimator selection.We first define two functions that are useful in obtaining \(\mathcal{V}^{0}(\hat{\tau})\) and \(\mathcal{V}^{1}(\hat{\tau})\):

\[\hat{F}_{0}(\lambda_{0},\epsilon_{0};\hat{\tau})=\lambda_{0} \epsilon_{0}+\lambda_{0}\log\frac{1}{n_{c}}\sum_{i=1}^{n_{c}}e^{\frac{\hat{x} _{i}}{\lambda_{0}}},\;\hat{F}_{1}(\lambda_{1},\epsilon_{1};\hat{\tau})=\lambda _{1}\epsilon_{1}+\lambda_{1}\log\frac{1}{n_{t}}\sum_{i=1}^{n_{t}}e^{\frac{- \hat{x}_{i}}{\lambda_{1}}};\] (14a) \[\frac{\partial\hat{F}_{0}}{\partial\lambda_{0}}=\epsilon_{0}+\log \sum_{i=1}^{n_{c}}\frac{e^{\frac{\hat{x}_{i}}{\lambda_{0}}}}{n_{c}}-\frac{ \sum_{i=1}^{n_{c}}Z_{i}e^{\frac{\hat{x}_{i}}{\lambda_{0}}}}{\lambda_{0}\sum_{i =1}^{n_{c}}e^{\frac{\hat{x}_{i}}{\lambda_{0}}}},\;\frac{\partial\hat{F}_{1}}{ \partial\lambda_{1}}=\epsilon_{1}+\log\sum_{i=1}^{n_{t}}\frac{e^{\frac{-\hat{ x}_{i}}{\lambda_{1}}}}{n_{t}}-\frac{\sum_{i=1}^{n_{t}}-Z_{i}e^{\frac{-\hat{x}_{i}}{ \lambda_{1}}}}{\lambda_{1}\sum_{i=1}^{n_{t}}e^{\frac{-\hat{x}_{i}}{\lambda_{1 }}}}.\] (14b)

Here, \(Z\) denotes \(\hat{\tau}(X)Y\) for notational simplicity. We then use the Newton-Raphson method to find the empirical solution for \(\hat{\mathcal{V}}^{t}(\hat{\tau})\), exploiting the convexity of \(\hat{F}_{t}(\lambda_{t},\epsilon_{t};\hat{\tau})\) w.r.t. \(\lambda_{t}\). Based on the distributionally robust value of PEHE, i.e., \(\hat{\mathcal{V}}_{PEHE}(\hat{\tau})\) in equation (10), we finally obtain the selected estimator \(\hat{\tau}_{select}=\arg\min_{\hat{\tau}\in\{\hat{\tau}_{1},\ldots,\hat{\tau}_ {j}\}}\mathcal{R}^{DRM}(\hat{\tau})\) such that

\[\mathcal{R}^{DRM}(\hat{\tau})=\frac{1}{n}\sum_{i=1}^{n}\hat{\tau}(X_{i})^{2}+ \frac{2}{n}\left(\sum_{i=1}^{n_{c}}\hat{\tau}(X_{i})Y_{i}+\sum_{i=1}^{n_{t}}- \hat{\tau}(X_{i})Y_{i}+n_{c}\hat{\mathcal{V}}^{1}(\hat{\tau})+n_{t}\hat{ \mathcal{V}}^{0}(\hat{\tau})\right).\] (15)

Algorithm 1 provides complete procedure of using the DRM method for CATE estimator selection.

Discussion on the ambiguity radius \(\epsilon\).The ambiguity radius \(\epsilon\) plays a critical role in real-world applications [54, 52, 60]. However, determining an appropriate value for \(\epsilon\) can be challenging as it requires striking a balance between ensuring the bound in equation (9) holds and maintaining its tightness. Specifically, if \(\epsilon\) is set too small, it fails to guarantee that the counterfactual distribution is contained within the ambiguity set centered at factual distribution (the bound in Corollary 4.3 can hold). On the other hand, if \(\epsilon\) is set too large, even though the ambiguity set can encompass more distributions to ensure the counterfactual distribution is contained, the bound in Corollary 4.3 can be less tight. In general, selecting a proper ambiguity radius is an open problem in distributioanlly robust optimization (DRO) literature [34; 54; 46; 48; 72].

In this paper, we provide a guidance for determining the ambiguity radius for our DRM method. Based on the above discussion, an ideal radius should be \(\epsilon_{1}^{*}=D_{KL}(P_{C}||P_{T})\) and \(\epsilon_{0}^{*}=D_{KL}(P_{T}||P_{C})\), which ensures that the bound in Corollary 4.3 holds and is tight. However, as defined in equation (8), both \(P_{C}\) and \(P_{T}\) involve counterfactual information, making it unattainable to directly compute \(D_{KL}(P_{C}||P_{T})\) and \(D_{KL}(P_{T}||P_{C})\). To overcome this challenge, we demonstrate that Proposition 4.6 provides an intriguing alternative approach to acquire \(D_{KL}(P_{C}||P_{T})\) and \(D_{KL}(P_{T}||P_{C})\) when unconfoundedness in Assumption 2.1 is satisfied.

**Proposition 4.6**.: _Let \(P_{X}^{T}:=P(X|T=1)\) and \(P_{X}^{C}:=P(X|T=0)\) denote the covariates distribution in the treat and control group, respectively. Assuming that random variables \((X,T,Y^{1},Y^{0})\) satisfy the unconfoundedness in Assumption 2.1, we have_

\[D_{KL}(P_{C}||P_{T})=D_{KL}(P_{X}^{C}||P_{X}^{T});\quad D_{KL}(P_{T}||P_{C})=D _{KL}(P_{X}^{T}||P_{X}^{C}).\] (16)

_The proof is deferred to Appendix B.2._

Proposition 4.6 provides an important insight that the uncomputable term \(D_{KL}(P_{C}||P_{T})\) (or \(D_{KL}(P_{T}||P_{C})\)) can be replaced by a computable quantity \(D_{KL}(P_{X}^{C}||P_{X}^{T})\) (or \(D_{KL}(P_{X}^{T}||P_{X}^{C})\)), where \(P_{X}^{C}\) and \(P_{T}^{T}\) are empirically observable. Consequently, the ideal ambiguity radius can be set as \(\epsilon_{1}^{*}=D_{KL}(P_{X}^{C}||P_{X}^{T})\) and \(\epsilon_{0}^{*}=D_{KL}(P_{X}^{T}||P_{X}^{C})\). While the KL divergence can be approximated using empirical algorithm (e.g, Nearest-Neighbror [73; 57]), we recommend setting the ambiguity radius larger than the empirically approximated KL divergence (see specific explanations in Appendix C.1). This is necessary because it ensures that the ambiguity set is large enough to contain the target distribution. It is also important to note that though the Algorithm 1 involves approximating \(\epsilon_{1}^{*}=D_{KL}(P_{X}^{C}||P_{X}^{T})\) and \(\epsilon_{0}^{*}=D_{KL}(P_{X}^{T}||P_{X}^{C})\), the DRM itself remains free of nuisances, as this approach only determines the ambiguity radius but does not involve learning any nuisance function such as the outcome function, propensity function, and plug-in learner.

## 5 Experiments

### Experimental Setup.

Estimators & Selectors.We consider a total of **36 CATE estimators**, comprising the combination of 4 base ML models and 9 meta-learners. Specifically, the base ML models are Linear Regression (LR), Support Vector Machine (SVM), Random Forests (RF), and Neural Net (Net). We consider these ML models for CATE estimators because they are representative of both rigid and flexible models, with each encoded distinct inductive biases, as highlighted by [19; 20]. Note that for the LR method, we employ Ridge regression for regression tasks and Logistic regression for classification tasks. As for the remaining methods, we utilize their corresponding regressors and classifiers for regression and classification tasks, respectively. Regarding the meta-learners, we select a set of both traditional basic learners (S-, T-, PS-, and IPW-learners) and recently developed learners (X-, DR-, U-, R-, and RA-learners), as detailed in Appendix A.1. We consider **14 CATE selectors**, consisting of 9 plug-in methods that rely on the above 9 learners, 3 pseudo-outcome methods (pseudo-DR, -R, and -IF), the random selection, the factual selection (from the 6-learner pool with S-, T-), the Nearest-Neighbor Matching [62], and our proposed DRM. The specific details of baseline selectors are stated in Appendix A.2. We employ the eXtreme Gradient Boosting (XGB) [12] as the underlying ML model for both plug-in and pseudo-outcome methods. We choose XGB because: i) it demonstrates superior performance in various scenarios, ensuring a good performance of baseline selectors; ii) the need to avoid potential congeniality bias that may arise from using the similar ML models employed in CATE estimators [20]; iii) aligning with [5] where XGB is used for their proposed pseudo-IF metric. The details of hyperparameters for nuisance models are stated in Section C.2 of Appendix.

Dataset.Since the ground truth of CATE is unavailable in real-world data, previous studies commonly utilize semi-synthetic datasets to compare model performance. In line with [19; 20], we collect the covariates with \(n=4802\) data points from AIC2016 dataset [22]. Then, we generate treatment with \(T_{i}|X_{i}\sim Bern(1/(1+\exp(-\xi(\beta_{i}^{t}X_{i}+3))))\), where \(Bern\) indicates the Bernoulli

[MISSING_PAGE_FAIL:8]

exhibit better PEHE as the CATE complexity decreases, aligning with the findings in [20]. In setting B, the DRM selector demonstrates robustness against selection bias (controlled by \(\xi\)) compared to many baselines. However, for the case \(\xi=2\), DRM selects a poor estimator 1 or 2 times out of 100 experiments, as shown in Figure 1. Although this weakens its overall performance, DRM still outperforms many baselines in this scenario. In the scenario \(\xi=0\) where no selection bias is present, the factual selection criterion performs better in this specific setting. In this case, DRM does not demonstrate a significant advantage, as there is no distribution shift caused by selection bias. In setting C where the unconfoundedness assumption is violated, most selectors exhibit inferior performance. In contrast, DRM demonstrates consistent outperformance across all three cases, and its superiority becomes particularly significant as \(m\) increases to 0.9, showcasing its robustness against the distribution shift arising from unobserved confounders.

Ranking ability.In Table 2, the DRM method demonstrates favorable performance in ranking estimators, surpassing certain Plug- (e.g., U, T, IPW, DR, RA) and Pseudo- (e.g., DR, IF) selectors.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline  & A (\(\rho=0\)) & A (\(\rho=0.1\)) & A (\(\rho=0.3\)) & B (\(\xi=0\)) & B (\(\xi=2\)) & C (\(m=0.1\)) & C (\(m=0.5\)) & C (\(m=0.9\)) \\ \hline Plug-U & 0.69\(\pm\)0.34 & 0.70\(\pm\)0.35 & 0.75\(\pm\)0.29 & **0.95\(\pm\)**0.04 & 0.53\(\pm\)0.30 & 0.68\(\pm\)0.33 & 0.73\(\pm\)0.34 & 0.83\(\pm\)0.24 \\ Plug-S & 0.95\(\pm\)0.06 & 0.95\(\pm\)0.06 & 0.95\(\pm\)0.05 & **0.95\(\pm\)**0.04 & **0.95\(\pm\)**0.05 & 0.95\(\pm\)0.03 & 0.95\(\pm\)0.05 & 0.91\(\pm\)0.07 \\ Plug-PS & 0.95\(\pm\)0.06 & 0.95\(\pm\)0.06 & 0.95\(\pm\)0.05 & **0.95\(\pm\)**0.04 & **0.95\(\pm\)**0.05 & 0.95\(\pm\)0.03 & 0.95\(\pm\)0.05 & 0.91\(\pm\)0.07 \\ Plug-T & 0.54\(\pm\)0.18 & 0.54\(\pm\)0.18 & 0.54\(\pm\)0.16 & 0.89\(\pm\)0.07 & 0.57\(\pm\)0.16 & 0.51\(\pm\)0.16 & 0.58\(\pm\)0.21 & 0.59\(\pm\)0.21 \\ Plug-X & 0.94\(\pm\)0.05 & 0.94\(\pm\)0.04 & 0.94\(\pm\)0.04 & 0.93\(\pm\)0.05 & 0.93\(\pm\)0.05 & 0.93\(\pm\)0.04 & 0.92\(\pm\)0.06 & 0.85\(\pm\)0.13 \\ Plug-DW & 0.72\(\pm\)0.19 & 0.71\(\pm\)0.19 & 0.71\(\pm\)0.19 & 0.92\(\pm\)0.06 & 0.68\(\pm\)0.15 & 0.69\(\pm\)0.19 & 0.76\(\pm\)0.18 & 0.77\(\pm\)0.17 \\ Plug-DR & 0.65\(\pm\)0.19 & 0.63\(\pm\)0.20 & 0.63\(\pm\)0.18 & 0.93\(\pm\)0.06 & 0.59\(\pm\)0.16 & 0.61\(\pm\)0.18 & 0.71\(\pm\)0.21 & 0.73\(\pm\)0.18 \\ Plug-R & **0.96\(\pm\)**0.03 & **0.96\(\pm\)**0.03 & **0.96\(\pm\)**0.03 & **0.95\(\pm\)**0.04 & 0.93\(\pm\)0.07 & **0.96\(\pm\)**0.03 & **0.96\(\pm\)**0.05 & **0.96\(\pm\)**0.04 \\ Plug-RA & 0.55\(\pm\)0.19 & 0.54\(\pm\)0.17 & 0.55\(\pm\)0.17 & 0.92\(\pm\)0.06 & 0.57\(\pm\)0.15 & 0.53\(\pm\)0.17 & 0.60\(\pm\)0.22 & 0.62\(\pm\)0.21 \\ Pseudo-DR & 0.54\(\pm\)0.18 & 0.53\(\pm\)0.18 & 0.53\(\pm\)0.16 & 0.87\(\pm\)0.10 & 0.55\(\pm\)0.15 & 0.50\(\pm\)0.17 & 0.54\(\pm\)0.24 & 0.58\(\pm\)0.23 \\ Pseudo-R & 0.86\(\pm\)0.11 & 0.87\(\pm\)0.09 & 0.88\(\pm\)0.08 & 0.93\(\pm\)0.06 & 0.83\(\pm\)0.13 & 0.85\(\pm\)0.13 & 0.85\(\pm\)0.12 & 0.80\(\pm\)0.16 \\ Pseudo-IF & 0.52\(\pm\)0.17 & 0.52\(\pm\)0.17 & 0.51\(\pm\)0.15 & 0.66\(\pm\)0.18 & 0.64\(\pm\)0.16 & 0.52\(\pm\)0.16 & 0.53\(\pm\)0.19 & 0.62\(\pm\)0.18 \\ Random & 0.26\(\pm\)0.13 & 0.26\(\pm\)0.13 & 0.27\(\pm\)0.13 & 0.47\(\pm\)0.11 & 0.23\(\pm\)0.13 & 0.28\(\pm\)0.10 & 0.28\(\pm\)0.11 & 0.24\(\pm\)0.14 \\ Fact & 0.35\(\pm\)0.08 & 0.36\(\pm\)0.08 & 0.35\(\pm\)0.09 & 0.48\(\pm\)0.08 & 0.31\(\pm\)0.10 & 0.35\(\pm\)0.07 & 0.33\(\pm\)0.09 & 0.29\(\pm\)0.11 \\ Matching & 0.53\(\pm\)0.17 & 0.51\(\pm\)0.18 & 0.52\(\pm\)0.16 & 0.89\(\pm\)0.08 & 0.58\(\pm\)0.15 & 0.51\(\pm\)0.16 & 0.55\(\pm\)0.21 & 0.60\(\pm\)0.21 \\ DRM & 0.81\(\pm\)0.08 & 0.80\(\pm\)0.08 & 0.80\(\pm\)0.08 & 0.85\(\pm\)0.06 & 0.77\(\pm\)0.15 & 0.79\(\pm\)0.09 & 0.81\(\pm\)0.10 & 0.80\(\pm\)0.08 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of rank correlation for different selectors across Settings A, B, and C (Note that B (\(\xi=1\)) matches A (\(\rho=0.1\))). Bold denotes the best three results among all selectors. Reported values (mean \(\pm\) standard deviation) are computed over 100 experiments. Larger is better.

Figure 1: The stacked bar chart showing the distribution of the selected estimator’s rank for each evaluation metric across rank intervals: [1-3], [4-11], [12-19], [20-27], and [28-36]. The greener (or redder) color indicates that the selected estimator ranks higher (or lower). For example, the **dark red** (or green) indicates the percentage of cases (out of 100 experiments) where the selected estimator ranks among the worst 9 estimators, specifically as ranks 28, 29,..., or 36 (or among the best 3 estimators, specifically as ranks 1, 2, or 3).

In comparison to other nuisance-free baselines (Random, Fact, and Matching), DRM achieves significantly superior ranking ability. However, compared to Plug-S, -PS, -X, and -R, it does not exhibit remarkable performance in ranking CATE estimators, possibly due to the fact that DRM selects estimators based on their distributionally robust (worst-case) performance. Indeed, the definition of ranking inherently involves the concept of expected (average) performance, which is not determined solely by either the best or worst performance. While distributionally robust performance serves as a suitable criterion for selecting players to participate in the Olympics, it may not be a reasonable standard for ranking players' average performance. Therefore, it would be intriguing to explore some ways in future research that can enhance the ranking ability of our DRM selector.

Variance analysis.Table 1 indicates that baseline selectors tend to exhibit higher variances in Regret performance. This is primarily due to the wide range of PEHE performances across the 36 CATE estimators. If a selector consistently selects either good or bad estimators, the variance would not be very large. To investigate this further, we sorted all 36 estimators in ascending order based on their \(\mathcal{R}^{oracle}(\hat{\tau})\) values, resulting in the sorted list: \([\mathcal{R}^{oracle}(\hat{\tau}_{1}),\ldots,\mathcal{R}^{oracle}(\hat{\tau}_ {J})]\). We then determine the actual rank of the selected estimator within this list and visualize the distribution of these 100 ranks using a stacked bar chart. Figure 1 shows that many baseline methods tend to select CATE estimators from various percentile ranges, leading to high variance across the 100 selections. Notably, the DRM selector consistently chooses higher-ranked (i.e., better performing in PEHE) estimators, demonstrating its robustness in CATE estimator selection.

Potential improvements.There are several potential improvements based on the current experimental settings. First, the existing results suggest that Plug-S performs better than Plug-T, indicating that the complexity of CATE function is relatively simple. It would help to provide more comprehensive analysis if investigating how DRM compares to baselines when the CATE function is more complex. Second, since the impact of selection bias can vary with sample size [3], it is important to compare different selectors when the sample size is sufficiently large. Third, considering baselines that are specifically designed for addressing hidden confounders could provide valuable insights for testing different selectors under such conditions. We encourage deeper investigation of causal model selection without assuming unconfoundedness. Finally, it would be good if future studies will apply DRM and other selectors in Healthcare, Economics, and Business applications with real-world data, as CATE estimator selection plays an important role in personalized decision makings.

## 6 Conclusion

This paper sheds lights on the potential of robustness in CATE estimator selection. We propose a distributionally robust metric (DRM). The proposed metric is nuisance-free, eliminating the need to fit models for nuisance parameters (outcome function, propensity function, and plug-in learner). Additionally, it is well-targeted for selecting a robust CATE estimator. We provide a finite sample analysis that demonstrates the gap between \(\hat{\mathcal{V}}^{t}(\hat{\tau})\) and \(\mathcal{V}^{t}(\hat{\tau})\) reduces at a rate of \(n^{-1/2}\) for \(t\in\{0,1\}\). The experimental results showcase that the CATE estimator selected by DRM demonstrate robustness to the distribution shift incurred by covariate shift and hidden confounders.

Limitations and future work.This paper explores the potential of robustness in CATE estimator selection. However, we must acknowledge that our DRM method is not a silver bullet, as consistent estimation on the CATE are never attainable [14]. Here, we outline some challenges and suggest future research directions. First, while Proposition 4.6 provides useful guidance for setting ambiguity radius in the DRM algorithm, we cannot guarantee that the empirically-computed radius is optimal due to potential bias in the algorithm's approximation of KL-divergence. Second, as discussed in Section 5.2, enhancing the ranking capability of DRM is a promising area for further research. Moreover, our findings are based on KL-divergence. However, using other divergences, such as the Wasserstein distance, to construct the ambiguity set could incorporate more diverse distributions, despite the challenges in solving the dual formulation of the Wasserstein distributionally robust value. Simultaneously, exploring whether alternative divergences can yield a tighter bound for the PEHE error is also interesting [4]. Finally, inspired by [16], understanding how nuisance parameters influence metrics like plug-DR and pseudo-DR might be helpful in CATE estimator selection. We hope our methods and findings will spur interest in model selection for causal inference, as well as in related fields like domain adaptation and out-of-distribution generalization.

## Acknowledgement

Qi WU acknowledges the support from The CityU-JD Digits Joint Laboratory in Financial Technology and Engineering, The Hong Kong Research Grants Council [General Research Fund 11219420/9043008], and The CityU APRC Grant 9610643. The work described in this paper was partially supported by the InnoHK initiative, the Government of the HKSAR, and the Laboratory for AI-Powered Financial Technologies. We finally thank all the anonymous reviewers for their constructive suggestions.

## References

* [1] Alberto Abadie, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. When should you adjust standard errors for clustering? _The Quarterly Journal of Economics_, 138(1):1-35, 2023.
* [2] Arun Advani, Toru Kitagawa, and Tymon Sloczynski. Mostly harmless simulations? using monte carlo studies for estimator selection. _Journal of Applied Econometrics_, 34(6):893-910, 2019.
* [3] Ahmed Alaa and Mihaela Schaar. Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design. In _International Conference on Machine Learning_, pages 129-138. PMLR, 2018.
* [4] Ahmed Alaa and Mihaela van der Schaar. Limits of estimating heterogeneous treatment effects: Guidelines for practical algorithm design. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 129-138. PMLR, 10-15 Jul 2018.
* [5] Ahmed Alaa and Mihaela Van Der Schaar. Validating causal inference models via influence functions. In _International Conference on Machine Learning_, pages 191-201. PMLR, 2019.
* [6] Serge Assaad, Shuxi Zeng, Chenyang Tao, Shounak Datta, Nikhil Mehta, Ricardo Henao, Fan Li, and Lawrence Carin. Counterfactual representation learning with balancing weights. In _International Conference on Artificial Intelligence and Statistics_, pages 1972-1980. PMLR, 2021.
* [7] Susan Athey, Guido W Imbens, Jonas Metzger, and Evan Munro. Using wasserstein generative adversarial networks for the design of monte carlo simulations. _Journal of Econometrics_, 2021.
* [8] SUSAN ATHEY, JULIE TIBSHIRANI, and STEFAN WAGER. Generalized random forests. _The Annals of Statistics_, 47(2):1148-1178, 2019.
* [9] Ioana Bica, Ahmed M Alaa, Craig Lambert, and Mihaela Van Der Schaar. From real-world patient data to individualized treatment effects using machine learning: current and future methods to address underlying challenges. _Clinical Pharmacology & Therapeutics_, 109(1):87-100, 2021.
* [10] Ioana Bica and Mihaela van der Schaar. Transfer learning on heterogeneous feature spaces for treatment effects estimation. _Advances in Neural Information Processing Systems_, 35:37184-37198, 2022.
* [11] Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. _Journal of Machine Learning Research_, 14(11), 2013.
* [12] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining_, pages 785-794, 2016.
* [13] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018.

* [14] Victor Chernozhukov, Mert Demirer, Esther Duflo, and Ivan Fernandez-Val. Generic machine learning inference on heterogeneous treatment effects in randomized experiments, with an application to immunization in india. Technical report, National Bureau of Economic Research, 2018.
* [15] Zhixuan Chu, Stephen L Rathbun, and Sheng Li. Graph infomax adversarial learning for treatment effect estimation with networked observational data. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 176-184, 2021.
* [16] Yifan Cui and EJ Tchetgen Tchetgen. Selective machine learning of doubly robust functionals. _Biometrika_, 111(2):517-535, 2024.
* [17] Alicia Curth, David Svensson, Jim Weatherall, and Mihaela van der Schaar. Really doing great at estimating cate? a critical look at ml benchmarking practices in treatment effect estimation. In _Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2)_, 2021.
* [18] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In _International Conference on Artificial Intelligence and Statistics_, pages 1810-1818. PMLR, 2021.
* [19] Alicia Curth and Mihaela van der Schaar. On inductive biases for heterogeneous treatment effect estimation. _Advances in Neural Information Processing Systems_, 34:15883-15894, 2021.
* [20] Alicia Curth and Mihaela Van Der Schaar. In search of insights, not magic bullets: Towards demystification of the model selection dilemma in heterogeneous treatment effect estimation. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 6623-6642. PMLR, 23-29 Jul 2023.
* [21] Robert Donnelly, David Blei, Susan Athey, et al. Correction to: Counterfactual inference for consumer choice across many product categories. _Quantitative Marketing and Economics_, 19(3-4):409-409, 2021.
* [22] Vincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition. _Statistical Science_, 34(1):43-68, 2019.
* [23] Max H Farrell. Robust inference on average treatment effects with possibly more covariates than observations. _Journal of Econometrics_, 189(1):1-23, 2015.
* [24] Carlos Fernandez-Loria, Foster Provost, Jesse Anderton, Benjamin Carterette, and Praveen Chandar. A comparison of methods for treatment assignment with an application to playlist generation. _Information Systems Research_, 34(2):786-803, 2023.
* [25] Aaron Fisher. Inverse-variance weighting for estimation of heterogeneous treatment effects. In _Forty-first International Conference on Machine Learning_.
* [26] Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. _The Annals of Statistics_, 51(3):879-908, 2023.
* [27] Jared C Foster, Jeremy MG Taylor, and Stephen J Ruberg. Subgroup identification from randomized clinical trial data. _Statistics in medicine_, 30(24):2867-2880, 2011.
* [28] Ruocheng Guo, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. A survey of learning causality with data: Problems and methods. _ACM Computing Surveys (CSUR)_, 53(4):1-37, 2020.
* [29] Ruocheng Guo, Jundong Li, Yichuan Li, K Selcuk Candan, Adrienne Raglin, and Huan Liu. Ignite: A minimax game toward learning individual treatment effects from networked observational data. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, pages 4534-4540, 2021.
* [30] P Richard Hahn, Jared S Murray, and Carlos M Carvalho. Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion). _Bayesian Analysis_, 15(3):965-1056, 2020.

* [31] Negar Hassanpour and Russell Greiner. Learning disentangled representations for counterfactual regression. In _International Conference on Learning Representations_, 2019.
* [32] Jennifer L Hill. Bayesian nonparametric modeling for causal inference. _Journal of Computational and Graphical Statistics_, 20(1):217-240, 2011.
* [33] Paul W Holland. Statistics and causal inference. _Journal of the American statistical Association_, 81(396):945-960, 1986.
* [34] Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust optimization. _Available at Optimization Online_, 1(2):9, 2013.
* [35] Yiyan Huang, Cheuk Hang Leung, Shumin Ma, Zhiri Yuan, Qi Wu, Siyi Wang, Dongdong Wang, and Zhixiang Huang. Towards balanced representation learning for credit policy evaluation. In _International Conference on Artificial Intelligence and Statistics_, pages 3677-3692. PMLR, 2023.
* [36] Yiyan Huang, Cheuk Hang Leung, Qi Wu, Xing Yan, Shumin Ma, Zhiri Yuan, Dongdong Wang, and Zhixiang Huang. Robust causal learning for the estimation of average treatment effects. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 1-9. IEEE, 2022.
* [37] Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Nanbo Peng, Dongdong Wang, and Zhixiang Huang. The causal learning of retail delinquency. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 204-212, 2021.
* [38] Yiyan Huang, WANG Siyi, Cheuk Hang Leung, WU Qi, WANG Dongdong, and Zhixiang Huang. Dignet: Learning decomposed patterns in representation balancing for treatment effect estimation. _Transactions on Machine Learning Research_, 2024.
* [39] Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual inference. In _International conference on machine learning_, pages 3020-3029. PMLR, 2016.
* [40] Fredrik D Johansson, Uri Shalit, Nathan Kallus, and David Sontag. Generalization bounds and representation learning for estimation of potential outcomes and causal effects. _The Journal of Machine Learning Research_, 23(1):7489-7538, 2022.
* [41] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. _Electronic Journal of Statistics_, 17(2):3008-3049, 2023.
* [42] Newton Mwai Kinyanjui and Fredrik D Johansson. Adcb: An alzheimer's disease simulator for benchmarking observational estimators of causal effects. In _Conference on Health, Inference, and Learning_, pages 103-118. PMLR, 2022.
* [43] Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximization methods for treatment choice. _Econometrica_, 86(2):591-616, 2018.
* [44] Kun Kuang, Peng Cui, Bo Li, Meng Jiang, Shiqiang Yang, and Fei Wang. Treatment effect estimation with data-driven variable decomposition. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* [45] Kun Kuang, Peng Cui, Hao Zou, Bo Li, Jianrong Tao, Fei Wu, and Shiqiang Yang. Data-driven variable decomposition for treatment effect estimation. _IEEE Transactions on Knowledge and Data Engineering_, 34(5):2120-2134, 2020.
* [46] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh. Wasserstein distributionally robust optimization: Theory and applications in machine learning. In _Operations research & management science in the age of analytics_, pages 130-166. Informs, 2019.
* [47] Soren R Kunzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. _Proceedings of the national academy of sciences_, 116(10):4156-4165, 2019.

* [48] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally robust optimization. _Advances in Neural Information Processing Systems_, 33:8847-8860, 2020.
* [49] Shuangning Li and Stefan Wager. Random graph asymptotics for treatment effect estimation under network interference. _The Annals of Statistics_, 50(4):2334-2358, 2022.
* [50] Yijun Li, Cheuk Hang Leung, Xiangqian Sun, Chaoqun Wang, Yiyan Huang, Xing Yan, Qi Wu, Dongdong Wang, and Zhixiang Huang. The causal impact of credit lines on spending distributions. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 180-187, 2024.
* [51] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal effect inference with deep latent-variable models. _Advances in neural information processing systems_, 30, 2017.
* [52] Shumin Ma, Cheuk Hang Leung, Qi Wu, Wei Liu, and Nanbo Peng. Understanding distributional ambiguity via non-robust chance constraint. In _Proceedings of the First ACM International Conference on AI in Finance_, pages 1-8, 2020.
* [53] Divyat Mahajan, Ioannis Mitliagkas, Brady Neal, and Vasilis Syrgkanis. Empirical analysis of model selection for heterogenous causal effect estimation. _International Conference on Learning Representations_, 2024.
* [54] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: performance guarantees and tractable reformulations. _Mathematical Programming_, 171(1-2):115-166, 2018.
* [55] Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. _Biometrika_, 108(2):299-319, 2021.
* [56] Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, and Joao Gama. Methods and tools for causal discovery and causal inference. _Wiley interdisciplinary reviews: data mining and knowledge discovery_, 12(2):e1449, 2022.
* [57] Yung-Kyun Noh, Masashi Sugiyama, Song Liu, Marthinus C Plessis, Frank Chongwoo Park, and Daniel D Lee. Bias reduction and metric learning for nearest-neighbor estimation of kullback-leibler divergence. In _Artificial Intelligence and Statistics_, pages 669-677. PMLR, 2014.
* [58] Miruna Oprescu, Vasilis Syrgkanis, and Zhiwei Steven Wu. Orthogonal random forest for causal inference. In _International Conference on Machine Learning_, pages 4932-4941. PMLR, 2019.
* [59] Harsh Parikh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. Validating causal inference methods. In _International Conference on Machine Learning_, pages 17346-17358. PMLR, 2022.
* [60] Georg Ch Pflug. Multistage stochastic decision problems: Approximation by recursive structures and ambiguity modeling. _European Journal of Operational Research_, 306(3):1027-1039, 2023.
* [61] Zhaozhi Qian, Yao Zhang, Ioana Bica, Angela Wood, and Mihaela van der Schaar. Synctwin: Treatment effect estimation with longitudinal outcomes. _Advances in Neural Information Processing Systems_, 34:3178-3190, 2021.
* [62] Craig A Rolling and Yuhong Yang. Model selection for estimating treatment effects. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 76(4):749-769, 2014.
* [63] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. _Biometrika_, 70(1):41-55, 1983.
* [64] Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. _Journal of the American Statistical Association_, 100(469):322-331, 2005.

* Saito and Yasui [2020] Yuta Saito and Shota Yasui. Counterfactual cross-validation: Stable model selection procedure for causal inference models. In _International Conference on Machine Learning_, pages 8398-8407. PMLR, 2020.
* Schuler et al. [2018] Alejandro Schuler, Michael Baiocchi, Robert Tibshirani, and Nigam Shah. A comparison of methods for model selection when estimating individual treatment effects. _arXiv preprint arXiv:1804.05146_, 2018.
* Schuler et al. [2017] Alejandro Schuler, Ken Jung, Robert Tibshirani, Trevor Hastie, and Nigam Shah. Synth-validation: Selecting the best causal inference method for a given dataset. _arXiv preprint arXiv:1711.00083_, 2017.
* Shalit et al. [2017] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In _International conference on machine learning_, pages 3076-3085. PMLR, 2017.
* Shi et al. [2019] Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment effects. _Advances in neural information processing systems_, 32, 2019.
* Wager and Athey [2018] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. _Journal of the American Statistical Association_, 113(523):1228-1242, 2018.
* Wang et al. [2021] Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl library. _Proceedings of Machine Learning and Systems_, 3:434-447, 2021.
* Wang et al. [2021] Jie Wang, Rui Gao, and Yao Xie. Sinkhorn distributionally robust optimization. _arXiv preprint arXiv:2109.11926_, 2021.
* Wang et al. [2006] Qing Wang, Sanjeev R Kulkarni, and Sergio Verdu. A nearest-neighbor approach to estimating divergence between continuous random vectors. In _2006 IEEE International Symposium on Information Theory_, pages 242-246. IEEE, 2006.
* Wu et al. [2022] Anpeng Wu, Junkun Yuan, Kun Kuang, Bo Li, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei Wu. Learning decomposed representations for treatment effect estimation. _IEEE Transactions on Knowledge and Data Engineering_, 35(5):4989-5001, 2022.
* Yao et al. [2021] Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. A survey on causal inference. _ACM Transactions on Knowledge Discovery from Data (TKDD)_, 15(5):1-46, 2021.
* Yao et al. [2018] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. Representation learning for treatment effect estimation from observational data. _Advances in neural information processing systems_, 31, 2018.
* Yoon et al. [2018] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. Ganite: Estimation of individualized treatment effects using generative adversarial nets. In _International conference on learning representations_, 2018.
* Zhang et al. [2019] Linying Zhang, Yixin Wang, Anna Ostropolets, Jami J Mulgrave, David M Blei, and George Hripcsak. The medical deconfounder: assessing treatment effects with electronic health records. In _Machine Learning for Healthcare Conference_, pages 490-512. PMLR, 2019.
* Zhang et al. [2020] Yao Zhang, Alexis Bellot, and Mihaela Schaar. Learning overlapping representations for the estimation of individualized treatment effects. In _International Conference on Artificial Intelligence and Statistics_, pages 1005-1014. PMLR, 2020.

## Appendix A CATE Estimation Strategies

### CATE Learners

We now detail how to construct CATE learners using the observed samples \(\{(X_{i},T_{i},Y_{i})\}_{i=1}^{n}\). Note that CATE learners are learned on the training set, so the sample size \(n\) here equals the training sample size. Denote \(n_{t}\) by the sample size in the treat group, and \(n_{c}\) by the sample size in the control group such that \(n=n_{t}+n_{c}\).

* S-learner: Let predictors=\((X,T)\), response=\(Y\). Train a model \(\hat{\mu}(X,T)\). Then we obtain \(\hat{\tau}_{S}(X)\): \[\hat{\tau}_{S}(X)=\hat{\mu}(X,1)-\hat{\mu}(X,0).\]
* T-learner: Let predictors=\(X^{T}\) (covariates in the treat), response=\(Y^{T}\) (outcome in the treat). Train a model \(\hat{\mu}_{1}(X)\). Let predictors=\(X^{C}\) (covariates in the control), response=\(Y^{C}\) (outcome in the control). Train a model \(\hat{\mu}_{0}(X)\). Then we obtain \(\hat{\tau}_{T}(X)\): \[\hat{\tau}_{T}(X)=\hat{\mu}_{1}(X)-\hat{\mu}_{0}(X).\]
* PS-learner: Fisrt-step: Train \(\hat{\tau}_{S}(X)\) using the above-mentioned step in S-learner. Second-step: Let predictors=\(X\), response=\(\hat{\tau}_{S}(X)\). Train a model \(\hat{\tau}_{PS}(X)\) from the following objective: \[\hat{\tau}_{PS}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n}\sum_{i=1}^{n}(\tau(X_{i})-\hat{\tau}_{S}(X_{i}))^{2}.\]
* IPW-learner: First-step: let predictors=\(X\), response=\(T\). Train a propensity score model \(\hat{\pi}(X)\). Construct surrogate of CATE using pseudo-outcomes with inverse propensity weighting (IPW) formula: \(Y_{IPW}^{1,0}=Y_{IPW}^{1}-Y_{IPW}^{0}\), where \(Y_{IPW}^{1}=\frac{TY}{\hat{\pi}(X)}\) and \(Y_{IPW}^{0}=\frac{(1-T)Y}{1-\hat{\pi}(X)}\). Train a model \(\hat{\tau}_{IPW}(X)\) from the following objective: \[\hat{\tau}_{IPW}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n}\sum_{i=1}^{n}(\tau(X_{i})-Y_{i,IPW}^{1,0})^{2}.\]
* X-learner [47]: First-step: Train \(\hat{\mu}_{1}(X)\) and \(\hat{\mu}_{0}(X)\) using the the above-mentioned procedure in T-learner. Train a propensity score model \(\hat{\pi}(X)\) using the the above-mentioned procedure in IPW-learner. Second-step: Let predictors=\(X^{T}\), response=\(\hat{\mu}_{1}(X^{T})-Y^{T}\), and predictors=\(X^{C}\), response=\(\hat{\mu}_{0}(X^{C})-Y^{C}\). Obtain a model \(\hat{\tau}_{X}(X)\) by learning two separate functions \(\hat{\tau}_{X}^{1}(X)\) and \(\hat{\tau}_{X}^{0}(X)\): \[\hat{\tau}_{X}(X)=(1-\hat{\pi}(X))\hat{\tau}_{X}^{1}(X)+\hat{\pi}( X)\hat{\tau}_{X}^{0}(X),\] \[\hat{\tau}_{X}^{1}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n_{t}}\sum_{i=1}^{n_{t}}(\tau(X_{i})-(Y_{i}-\hat{\mu}_{0}(X_{i})))^{2},\] \[\hat{\tau}_{X}^{0}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n_{c}}\sum_{i=1}^{n_{c}}(\tau(X_{i})-(\hat{\mu}_{1}(X_{i})-Y_{i}))^{2}.\]
* U-learner [25, 55]: First-step: Let predictors=\(X\), response=\(Y\). Train a model \(\hat{\mu}(X)\) to approximate the conditional mean outcome \(\mathbb{E}[Y|X]\). Train a propensity score model \(\hat{\pi}(X)\) using the the above-mentioned procedure in IPW-learner. Second-step: Compute the outcome residual \(\xi=Y-\hat{\mu}(X)\) and treatment residual \(\nu=T-\hat{\pi}(X)\). Train a model \(\hat{\tau}_{U}(X)\) from the following objective: \[\hat{\tau}_{U}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n}\sum_{i=1}^{n}( \frac{\xi_{i}}{\nu_{i}}-\tau(X_{i}))^{2}.\]* DR-learner [41, 26]: First-step: Train \(\hat{\mu}_{1}(X)\) and \(\hat{\mu}_{0}(X)\) using the the above-mentioned procedure in T-learner. Train a propensity score model \(\hat{\pi}(X)\) using the the above-mentioned procedure in IPW-learner. Second-step: Construct surrogate of CATE using pseudo-outcomes with doubly robust (DR) formula: \(Y^{1,0}_{DR}=Y^{1}_{DR}-Y^{0}_{DR}\), where \(Y^{1}_{DR}=\hat{\mu}_{1}(X)+\frac{T}{\pi(X)}(Y-\hat{\mu}_{1}(X))\) and \(Y^{0}_{DR}=\hat{\mu}_{0}(X)+\frac{1-T}{1-\pi(X)}(Y-\hat{\mu}_{0}(X))\). Train a model \(\hat{\tau}_{DR}(X)\) from the following objective: \[\hat{\tau}_{DR}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n}\sum_{i=1}^{n}(\tau(X_{i})-Y^{1,0}_{i, DR})^{2}.\]
* R-learner [55]: First-step: Let predictors=\(X\), response=\(Y\). Train a model \(\hat{\mu}(X)\) to approximate the conditional mean outcome \(\mathbb{E}[Y|X]\). Train a propensity score model \(\hat{\pi}(X)\) using the the above-mentioned procedure in IPW-learner. Second-step: Compute the outcome residual \(\xi=Y-\hat{\mu}(X)\) and treatment residual \(\nu=T-\hat{\pi}(X)\). Train a model \(\hat{\tau}_{R}(X)\) from the following objective: \[\hat{\tau}_{R}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n}\sum_{i=1}^{n}( \xi_{i}-\nu_{i}\tau(X_{i}))^{2}.\]
* RA-learner [18]: First-step: Train \(\hat{\mu}_{1}(X)\) and \(\hat{\mu}_{0}(X)\) using the the above-mentioned procedure in T-learner. Second-step: Construct surrogate of CATE using pseudo-outcomes with regression adjustment (RA) formula: \(Y_{RA}=T(Y-\hat{\mu}_{0}(X))+(1-T)(\hat{\mu}_{1}(X)-Y)\). Train a model \(\hat{\tau}_{RA}(X)\) from the following objective: \[\hat{\tau}_{RA}=\operatorname*{arg\,min}_{\tau}\ \frac{1}{n}\sum_{i=1}^{n}( \tau(X_{i})-Y_{i,RA})^{2}.\]

### CATE Selectors

We now detail how to construct CATE selectors using the observed samples \(\{(X_{i},T_{i},Y_{i})\}_{i=1}^{n}\). Note that CATE selectors are constructed on the validation set, so the sample size \(n\) here equals the validation sample size.

* Plug-in selector: Obtain any CATE learners \(\tilde{\tau}\) using the observational validation data. Then plug-in \(\tilde{\tau}\) into the following metric \(\mathcal{R}^{plug}_{\tilde{\tau}}(\hat{\tau})\): \[\mathcal{R}^{plug}_{\tilde{\tau}}(\hat{\tau})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}( \hat{\tau}(X_{i})-\tilde{\tau}(X_{i}))^{2}}.\] For each plug-in selector \(\tilde{\tau}\), the selected \(j^{*}\)-th CATE estimator is \(\hat{\tau}_{j^{*}}\), where \(j^{*}=\operatorname*{arg\,min}_{j\in\{1,\ldots,J\}}\mathcal{R}^{plug}_{\tilde{ \tau}}(\hat{\tau}_{j})\).
* Pseudo-outcome selector: 1. Pseudo-DR: Utilize validation data to estimate nuisance parameters \((\tilde{\mu}_{1},\tilde{\mu}_{0},\tilde{\pi})\), following the procedure described in Section A.1. \(\tilde{Y}_{DR}=\tilde{Y}^{1}_{DR}-\tilde{Y}^{0}_{DR}\), where \(\tilde{Y}^{1}_{DR}=\tilde{\mu}_{1}(X)+\frac{T}{\tilde{\pi}(X)}(Y-\tilde{\mu}_ {1}(X))\) and \(\tilde{Y}^{0}_{DR}=\tilde{\mu}_{0}(X)+\frac{1-T}{1-\tilde{\pi}(X)}(Y-\tilde{\mu }_{0}(X))\). Then the pseudo-DR metric is \[\mathcal{R}^{pseudo}_{DR}(\hat{\tau})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(\hat{\tau }(X_{i})-\tilde{Y}_{i,DR})^{2}}.\] For pseudo-DR selector, the selected \(j^{*}\)-th CATE estimator is \(\hat{\tau}_{j^{*}}\), where \(j^{*}=\operatorname*{arg\,min}_{j\in\{1,\ldots,J\}}\mathcal{R}^{pseudo}_{DR}( \hat{\tau}_{j})\).
2. Pseudo-R: Utilize validation data to estimate nuisance parameters \((\tilde{\mu},\tilde{\pi})\), following the procedure described in Section A.1. Then the pseudo-R metric is \[\mathcal{R}^{pseudo}_{R}(\hat{\tau})=\sqrt{\frac{1}{n}\sum_{i=1}^{n}((Y_{i}- \tilde{\mu}(X_{i}))-\hat{\tau}(X_{i})(T_{i}-\tilde{\pi}(X_{i})))^{2}}.\]

[MISSING_PAGE_FAIL:18]

Proof.: \[p(X,Y^{0},Y^{1}|T=0)\] \[= p(Y^{0},Y^{1}|X,T=0)p(X|T=0)\] \[= p(Y^{0},Y^{1}|X)p(X|T=0).\quad\text{(Unconfoundedness)}\] \[p(X,Y^{0},Y^{1}|T=1)\] \[= p(Y^{0},Y^{1}|X,T=1)p(X|T=1)\] \[= p(Y^{0},Y^{1}|X)p(X|T=1).\quad\text{(Unconfoundedness)}\]

Now we can prove Proposition 4.6.

Proof.: \[D_{KL}(P_{C}||P_{T})\] \[= D_{KL}(P(X,Y^{0},Y^{1}|T=0)||P(X,Y^{0},Y^{1}|T=1))\] \[= \int_{\mathcal{X}}\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^{1}}p( x,y^{0},y^{1}|T=0)\log\frac{p(x,y^{0},y^{1}|T=0)}{p(x,y^{0},y^{1}|T=1)}dy^{1}dy^{0}dx\] \[= \int_{\mathcal{X}}\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^{1}}p( y^{0},y^{1}|x)p(x|T=0)\log\frac{p(y^{0},y^{1}|x)p(x|T=0)}{p(y^{0},y^{1}|x)p(x|T =1)}dy^{1}dy^{0}dx\quad\text{(By Proposition B.1)}\] \[= \int_{\mathcal{X}}\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^{1}}p( y^{0},y^{1}|x)p(x|T=0)\log\frac{p(x|T=0)}{p(x|T=1)}dy^{1}dy^{0}dx\] \[= \int_{\mathcal{X}}\left(\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^ {1}}p(y^{0},y^{1}|x)dy^{1}dy^{0}\right)p(x|T=0)\log\frac{p(x|T=0)}{p(x|T=1)}dx\] \[= \int_{\mathcal{X}}p(x|T=0)\log\frac{p(x|T=0)}{p(x|T=1)}dx\] \[= D_{KL}(P(X|T=0)||P(X|T=1))\] \[= D_{KL}(P_{X}^{C}||P_{X}^{T}).\]

Similarly, it is easy to show \(D_{KL}(P_{T}||P_{C})=D_{KL}(P_{X}^{T}||P_{X}^{C})\) 

### Proof of Theorem 4.4

**Lemma B.2** (Theorem 1 in [34]).: _Let \(f_{\theta}(X)\) denote the loss function of \(X\) and it is bounded almost surely. \(\theta\in\Theta\) represents the model parameters of the function \(f_{\theta}(X)\). Let \(\mathcal{B}_{\epsilon}(P)\) be the uncertainty ball centered at distribution \(P\) with ambiguity radius \(\epsilon\). Define \(\kappa\) as the mass of the distribution \(P\) on its essential supremum (Proposition 2 in [34]). Assume \(f_{\theta}(X)\) is bounded and \(\log\kappa+\epsilon<0\), then we have_

\[\mathcal{V}:=\sup_{Q\in\mathcal{B}_{\epsilon}(P)}\mathbb{E}^{Q}[f_{\theta}(X) ]=\min_{\lambda>0}\lambda\epsilon+\lambda\log\mathbb{E}^{P}[\exp(f_{\theta}(X) /\lambda)].\]

Our Theorem 4.4 follows by directly applying the above Lemma B.2.

### Proof of Theorem 4.5

For notational simplicity, we denote \(W=(X,T,Y)\in\mathcal{W}\) and \(Z=\hat{\tau}(X)Y\). Assume \(Z\) is bounded within the range \(M\) and \(\bar{M}\). Define the following functions:

\[G_{0}(\lambda_{0};W)=\mathbb{E}[g_{0}(\lambda_{0};W)], \hat{G}_{0}(\lambda_{0};W)=\frac{1}{n}\sum_{i=1}^{n}g_{0}( \lambda_{0};W_{i}),\] \[\text{where }\,g_{0}(\lambda_{0};W)=(1-T)\exp\left(Z/\lambda_{0} \right);\] \[G_{1}(\lambda_{1};W)=\mathbb{E}[g_{1}(\lambda_{1};W)], \hat{G}_{1}(\lambda_{1};W)=\frac{1}{n}\sum_{i=1}^{n}g_{1}(\lambda_ {1};W_{i}),\] \[\text{where }\,g_{1}(\lambda_{1};W)=T\exp\left(-Z/\lambda_{1} \right).\]Then we have the following lemma that guarantees the convergence for \(\hat{G}_{0}(\lambda_{0};W)\) and \(\hat{G}_{1}(\lambda_{1};W)\).

**Lemma B.3**.: _Assume \(0<\lambda\leq\lambda_{0},\lambda_{1}\leq\bar{\lambda}\), and \(\hat{\tau}(X)Y\) is bounded within the range of \(M\) to \(\bar{M}\). Then with probability \(1-\delta\), we have_

\[\text{If }\,\,\underline{M}\leq\bar{M}\leq 0:\] \[|\hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)|\leq\mathcal{O} \left(\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(\bar{M}/\bar{\lambda} \right)\right)^{2}}{n}}\right);\] \[|\hat{G}_{1}(\lambda_{1};W)-G_{1}(\lambda_{1};W)|\leq\mathcal{O} \left(\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(-M/\lambda\right) \right)^{2}}{n}}\right).\] \[\text{If }\,\,\underline{M}\leq 0;\bar{M}\geq 0:\] \[|\hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)|\leq\mathcal{O} \left(\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(\bar{M}/\bar{\lambda} \right)\right)^{2}}{n}}\right);\] (18) \[|\hat{G}_{1}(\lambda_{1};W)-G_{1}(\lambda_{1};W)|\leq\mathcal{O} \left(\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(-M/\bar{\lambda} \right)\right)^{2}}{n}}\right).\] \[\text{If }\,\,0\leq\underline{M}\leq\bar{M}:\] \[|\hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)|\leq\mathcal{O} \left(\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(\bar{M}/\bar{\lambda} \right)\right)^{2}}{n}}\right);\] \[|\hat{G}_{1}(\lambda_{1};W)-G_{1}(\lambda_{1};W)|\leq\mathcal{O} \left(\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(-M/\bar{\lambda} \right)\right)^{2}}{n}}\right).\]

Proof.: Denote \(h_{0}(W_{1},W_{2},\ldots,W_{n})=\frac{1}{n}\sum_{i=1}^{n}g_{0}(\lambda_{0};W_ {i})\). We notice that \(h_{0}(W_{1},W_{2},\ldots,W_{n})\) satisfies the bounded difference inequality:

\[\sup_{W_{1},\ldots,W_{n},W_{i}^{\prime}\in\mathcal{W}}|h_{0}(W_{ 1},\ldots,W_{i},\cdots,W_{n})-h_{0}(W_{1},\ldots,W_{i}^{\prime},\cdots,W_{n})|\] \[= \sup_{W_{i},W_{i}^{\prime}\in\mathcal{W}}\frac{|g_{0}(\lambda_{0 };W_{i})-g_{0}(\lambda_{0};W_{i}^{\prime})|}{n}\] \[\leq 2\sup_{W_{i}\in\mathcal{W}}\frac{|g_{0}(\lambda_{0};W_{i})|} {n}\leq\frac{2\exp\left(\bar{M}/\lambda_{0}\right)}{n}.\]

Note that \(|\hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)|=|h_{0}(W_{1},W_{2},\ldots,W_ {n})-\mathbb{E}[h_{0}(W_{1},W_{2},\ldots,W_{n})]|\). Then using McDiarmid's inequality, for any \(\epsilon>0\), we have

\[P\left(\left|\hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W) \right|\geq\epsilon\right)\] \[=P\left(|h_{0}(W_{1},W_{2},\ldots,W_{n})-\mathbb{E}[h_{0}(W_{1},W _{2},\ldots,W_{n})]|\geq\epsilon\right)\] \[\leq 2\exp\left(-\frac{2\epsilon^{2}}{n(\frac{2\exp\left(\bar{M}/ \lambda_{0}\right)}{n})^{2}}\right)=2\exp\left(\frac{-n\epsilon^{2}}{2\left( \exp\left(\bar{M}/\lambda_{0}\right)\right)^{2}}\right).\]

For some \(\delta>0\), we have

\[P\left(\left|\hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)\right|\geq\epsilon \right)\leq 2\exp\left(\frac{-n\epsilon^{2}}{2\left(\exp\left(\bar{M}/\lambda_{ 0}\right)\right)^{2}}\right)\leq\delta.\]

This solves \(\epsilon\) such that

\[\epsilon\geq\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(\bar{M}/\lambda_ {0}\right)\right)^{2}}{n}}.\]The above inequality should hold for any \(\lambda_{0}\) such that \(0<\lambda\leq\lambda_{0}\leq\bar{\lambda}\). Therefore, we have

\[\text{If }\bar{M}\geq 0: \epsilon\geq\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(\bar{ M}/\bar{\lambda}\right)\right)^{2}}{n}};\] \[\text{If }\bar{M}\leq 0: \epsilon\geq\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(\bar{ M}/\bar{\lambda}\right)\right)^{2}}{n}}.\]

Similarly, denote \(h_{1}(W_{1},W_{2},\ldots,W_{n})=\frac{1}{n}\sum_{i=1}^{n}g_{1}(\lambda_{1};W_{i})\). We note that \(h_{1}(W_{1},W_{2},\ldots,W_{n})\) satisfies the bounded difference inequality:

\[\sup_{W_{1},\ldots,W_{n},W_{i}^{\prime}\in\mathcal{W}}|h_{1}(W_{1 },\ldots,W_{i},\cdots,W_{n})-h_{1}(W_{1},\ldots,W_{i}^{\prime},\cdots,W_{n})|\] \[=\sup_{W_{i},W_{i}^{\prime}\in\mathcal{W}}\frac{|g_{1}(\lambda_{1 };W_{i})-g_{1}(\lambda_{1};W_{i}^{\prime})|}{n}\] \[\leq 2\sup_{W_{i}\in\mathcal{W}}\frac{|g_{1}(\lambda_{1};W_{i})|} {n}\leq\frac{2\exp\left(-M/\lambda_{1}\right)}{n}.\]

Then using McDiarmid's inequality, for any \(\epsilon>0\), we have

\[P\left(\left|\hat{G}_{1}(\lambda_{1};W)-G_{1}(\lambda_{1};W) \right|\geq\epsilon\right)\] \[=P\left(|h_{1}(W_{1},W_{2},\ldots,W_{n})-\mathbb{E}[h_{1}(W_{1},W _{2},\ldots,W_{n})]|\geq\epsilon\right)\] \[\leq 2\exp\left(-\frac{2\epsilon^{2}}{n(\frac{2\exp(-M/\lambda_{1} )}{n})^{2}}\right)=2\exp\left(\frac{-n\epsilon^{2}}{2\left(\exp\left(-M/ \lambda_{1}\right)\right)^{2}}\right).\]

For some \(\delta>0\), we have

\[P\left(\left|\hat{G}_{1}(\lambda_{1};W)-G_{1}(\lambda_{1};W) \right|\geq\epsilon\right)\leq 2\exp\left(\frac{-n\epsilon^{2}}{2\left(\exp \left(-M/\lambda_{1}\right)\right)^{2}}\right)\leq\delta.\]

This solves \(\epsilon\) such that

\[\epsilon\geq\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(-M/\lambda_{1} \right)\right)^{2}}{n}}.\]

The above inequality should hold for any \(\lambda_{1}\) such that \(0<\lambda\leq\lambda_{1}\leq\bar{\lambda}\). Therefore, we have

\[\text{If }\underline{M}\geq 0: \epsilon\geq\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(-M/ \bar{\lambda}\right)\right)^{2}}{n}};\] \[\text{If }\underline{M}\leq 0: \epsilon\geq\sqrt{\frac{2\log\frac{2}{\delta}\left(\exp\left(-M/ \bar{\lambda}\right)\right)^{2}}{n}}.\]

In the following content, we will bound terms \(\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{0}(\lambda_{0};W)\right)\right|\) and \(\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_{1};W)\right)\right|\). Lemma B.4 is useful for bounding these two terms.

**Lemma B.4**.: _Let \(c\) be a constant. For any \(x_{1}\), \(x_{2}\) such that \(x_{1},x_{2}\geq c>0\), we have_

\[|\log(x_{1})-\log(x_{2})|\leq\frac{1}{c}|x_{1}-x_{2}|\] (19)

Proof.: Without loss of generality, assume \(0<c\leq x_{1}\leq x_{2}\). We then have

\[\log(x_{2})-\log(x_{1})=\log(\frac{x_{2}}{x_{1}})=\log(1+\frac{x_{2}}{x_{1}}-1 )\leq\frac{x_{2}}{x_{1}}-1=\frac{x_{2}-x_{1}}{x_{1}}\leq\frac{x_{2}-x_{1}}{c}.\]

Taking the absolute value of both the left-hand side and the right-hand side, we have

\[|\log(x_{1})-\log(x_{2})|\leq\frac{1}{c}|x_{1}-x_{2}|.\]Next, we introduce Lemma B.5 that bounds terms \(\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{0}(\lambda_{0};W)\right)\right|\) and \(\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_{1};W)\right)\right|\).

**Lemma B.5**.: _Let \(u\) denote the probability of treat, i.e., \(u=P(T=1)\). Assume that \(\lambda_{0},\lambda_{1}\in\Lambda:=[\lambda,\bar{\lambda}]\) and \(\hat{\tau}(X)Y\) is bounded within \(M\) and \(\bar{M}\). Then for \(n\geq\max\{\frac{2}{u^{2}}\log\left(\frac{2}{\delta}\right),\frac{2}{(1-u)^{2 }}\log\left(\frac{2}{\delta}\right)\}\), with probability \(1-\delta\), we have_

\[\begin{split}&\text{If }\,M\leq\bar{M}\leq 0:\\ &\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{0}(\lambda_{ 0};W)\right)\right|\leq\frac{2}{\exp(M/\lambda)(1-u)}\left|\hat{G}_{0}( \lambda_{0};W)-G_{0}(\lambda_{0};W)\right|;\\ &\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_ {1};W)\right)\right|\leq\frac{2}{\exp(-\bar{M}/\bar{\lambda})u}\left|\hat{G}_{ 1}(\lambda_{1};W)-G_{1}(\lambda_{1};W)\right|.\end{split}\]

_If \(\,M\leq 0,\bar{M}\geq 0:\)_

\[\begin{split}&\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{ 0}(\lambda_{0};W)\right)\right|\leq\frac{2}{\exp(M/\bar{\lambda})(1-u)}\left| \hat{G}_{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)\right|;\\ &\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_ {1};W)\right)\right|\leq\frac{2}{\exp(-\bar{M}/\bar{\lambda})u}\left|\hat{G}_{ 1}(\lambda_{1};W)-G_{1}(\lambda_{1};W)\right|.\end{split}\] (20)

Proof.: First, we bound the term \(\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{0}(\lambda_{0};W)\right)\right|\).

\(G_{0}(\lambda_{0};W)\) and \(\hat{G}_{0}(\lambda_{0};W)\) are greater than \(0\) and bounded because \(Z=\hat{\tau}(X)Y\) is bounded within the range \(\underline{M}\) and \(M\). Therefore, applying Lemma B.4, we have

\[\begin{split}&\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{ 0}(\lambda_{0};W)\right)\right|\leq\frac{1}{c}\left|\hat{G}_{0}(\lambda_{0};W) -G_{0}(\lambda_{0};W)\right|,\\ &\text{where }c=\min\left\{\inf_{\lambda_{0}\in\Lambda,W\in \mathcal{W}}\hat{G}_{0}(\lambda_{0};W),\,\inf_{\lambda_{0}\in\Lambda,W\in \mathcal{W}}G_{0}(\lambda_{0};W)\right\}.\end{split}\]

Moreover, for any \(\lambda_{0}\in\Lambda\), we have

\[\begin{split}\text{If }\,M\geq 0:&\quad G_{0}(\lambda_{ 0};W)=\mathbb{E}[(1-T)\exp(Z/\lambda_{0})]=\mathbb{E}[\exp(Z/\lambda_{0})|T=0 ]P(T=0)\\ &\geq\mathbb{E}[\exp(M/\bar{\lambda})|T=0](1-u)=\exp(M/\bar{ \lambda})(1-u);\\ &\hat{G}_{0}(\lambda_{0};W)=\frac{1}{n}\sum_{i=1}^{n}(1-T_{i}) \exp(Z_{i}/\lambda_{0})\\ &\geq\frac{1}{n}\sum_{i=1}^{n}(1-T_{i})\exp(M/\bar{\lambda})= \exp(M/\bar{\lambda})(1-\hat{u}).\end{split}\] (21)

\[\begin{split}\text{If }\,M\leq 0:&\quad G_{0}(\lambda_{ 0};W)=\mathbb{E}[(1-T)\exp(Z/\lambda_{0})]=\mathbb{E}[\exp(Z/\lambda_{0})|T=0 ]P(T=0)\\ &\geq\mathbb{E}[\exp(M/\lambda)|T=0](1-u)=\exp(M/\lambda)(1-u); \\ &\hat{G}_{0}(\lambda_{0};W)=\frac{1}{n}\sum_{i=1}^{n}(1-T_{i}) \exp(Z_{i}/\lambda_{0})\\ &\geq\frac{1}{n}\sum_{i=1}^{n}(1-T_{i})\exp(M/\lambda)=\exp(M/ \lambda)(1-\hat{u}).\end{split}\] (22)

Given \(\hat{u}=\frac{1}{n}\sum_{i=1}^{n}T_{i}\) and \(u=\mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}T_{i}]\), using Hoeffding's inequality, we have

\[P\left(\left|\frac{1}{n}\sum_{i=1}^{n}(1-T_{i})-\mathbb{E}[\frac{1}{n}\sum_{i=1 }^{n}(1-T_{i})]\right|\geq\frac{\mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}(1-T_{i})]} {2}\right)\leq 2\exp\left(-\frac{2(\frac{1-u}{2})^{2}}{n(\frac{1}{n})^{2}} \right)\leq\delta.\]We can solve \(n\) by

\[2\exp\left(-\frac{n(1-u)^{2}}{2}\right)\leq\delta\Rightarrow n\geq \frac{2}{(1-u)^{2}}\log\left(\frac{2}{\delta}\right).\]

This indicates that \((1-\hat{u})\geq(1-u)/2\) with probability \(1-\delta\) when \(n\geq\frac{2}{(1-u)^{2}}\log\left(\frac{2}{\delta}\right)\). Combining this with equations (21) and (22), with probability \(1-\delta\), when \(n\geq\frac{2}{(1-u)^{2}}\log\left(\frac{2}{\delta}\right)\), we have

\[\text{If }\,M\geq 0: \inf_{\lambda_{0}\in\Lambda,W\in\mathcal{W}}G_{0}(\lambda_{0};W) \geq\exp(M/\bar{\lambda})(1-u);\] \[\inf_{\lambda_{0}\in\Lambda,W\in\mathcal{W}}\hat{G}_{0}(\lambda_{ 0};W) \geq\exp(M/\bar{\lambda})(1-\hat{u})\geq\exp(M/\bar{\lambda})(1-u)/2.\] \[\text{If }\,M\leq 0: \inf_{\lambda_{0}\in\Lambda,W\in\mathcal{W}}G_{0}(\lambda_{0};W) \geq\exp(M/\lambda)(1-u);\] \[\inf_{\lambda_{0}\in\Lambda,W\in\mathcal{W}}\hat{G}_{0}(\lambda_{ 0};W) \geq\exp(M/\lambda)(1-\hat{u})\geq\exp(M/\lambda)(1-u)/2.\]

Therefore, with probability \(1-\delta\), when \(n\geq\frac{2}{(1-u)^{2}}\log\left(\frac{2}{\delta}\right)\), we have

\[\text{If }\,\underline{M}\geq 0:\] \[\left|\log(\hat{G}_{0}(\lambda_{0};W))-\log\left(G_{0}(\lambda_{ 0};W)\right)\right|\leq\frac{2}{\exp(M/\bar{\lambda})(1-u)}\left|\hat{G}_{0}( \lambda_{0};W)-G_{0}(\lambda_{0};W)\right|.\]

Next, we bound the term \(\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_{1};W)\right)\right|\). \(G_{1}(\lambda_{1};W)\) and \(\hat{G}_{1}(\lambda_{1};W)\) are greater than \(0\) and bounded above. Therefore, applying Lemma B.4, we have

\[\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_{1 };W)\right)\right|\leq\frac{1}{c}\left|\hat{G}_{1}(\lambda_{1};W)-G_{1}( \lambda_{1};W)\right|,\] \[\text{where }c=\min\left\{\inf_{\lambda_{1}\in\Lambda,W\in \mathcal{W}}\hat{G}_{1}(\lambda_{1};W),\ \inf_{\lambda_{1}\in\Lambda,W\in\mathcal{W}}G_{1}(\lambda_{1};W)\right\}.\]

Moreover, for any \(\lambda_{1}\in\Lambda\), we have

\[\text{If }\,\bar{M}\geq 0:\quad G_{1}(\lambda_{1};W) =\mathbb{E}[T\exp(-Z/\lambda_{1})]=\mathbb{E}[\exp(-Z/\lambda_{1}) |T=1]P(T=1)\] \[\geq\mathbb{E}[\exp(-\bar{M}/\bar{\lambda})|T=1]u=\exp(-\bar{M}/ \bar{\lambda})u;\] \[\hat{G}_{1}(\lambda_{1};W) =\frac{1}{n}\sum_{i=1}^{n}T_{i}\exp(-Z_{i}/\lambda_{1})\] (23) \[\geq\frac{1}{n}\sum_{i=1}^{n}T_{i}\exp(-\bar{M}/\bar{\lambda})= \exp(-\bar{M}/\bar{\lambda})\hat{u}.\]

\[\text{If }\,\bar{M}\leq 0:\quad G_{1}(\lambda_{1};W) =\mathbb{E}[T\exp(-Z/\lambda_{1})]=\mathbb{E}[\exp(-Z/\lambda_{1} )|T=1]P(T=1)\] \[\geq\mathbb{E}[\exp(-\bar{M}/\bar{\lambda})|T=1]u=\exp(-\bar{M}/ \bar{\lambda})u;\] \[\hat{G}_{1}(\lambda_{1};W) =\frac{1}{n}\sum_{i=1}^{n}T_{i}\exp(-Z_{i}/\lambda_{1})\] (24) \[\geq\frac{1}{n}\sum_{i=1}^{n}T_{i}\exp(-\bar{M}/\bar{\lambda})= \exp(-\bar{M}/\bar{\lambda})\hat{u}.\]

Given \(\hat{u}=\frac{1}{n}\sum_{i=1}^{n}T_{i}\) and \(u=\mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}T_{i}]\), using Hoeffding's inequality, we have

\[P\left(\left|\frac{1}{n}\sum_{i=1}^{n}T_{i}-\mathbb{E}[\frac{1}{n}\sum_{i=1}^{ n}T_{i}]\right|\geq\frac{\mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}T_{i}]}{2}\right) \leq 2\exp\left(-\frac{2(\frac{u}{2})^{2}}{n(\frac{1}{n})^{2}}\right)\leq\delta.\]We can solve \(n\) by

\[2\exp\left(-\frac{nu^{2}}{2}\right)\leq\delta\Rightarrow n\geq\frac{2}{u^{2}}\log \left(\frac{2}{\delta}\right).\]

This indicates that \(hat\geq u/2\) with probability \(1-\delta\) when \(n\geq\frac{2}{u^{2}}\log\left(\frac{2}{\delta}\right)\). Combining this with equations (23) and (24), with probability \(1-\delta\), when \(n\geq\frac{2}{u^{2}}\log\left(\frac{2}{\delta}\right)\), we have

\[\text{If }\bar{M}\geq 0: \inf_{\lambda_{1}\in\Lambda,W\in\mathcal{W}}G_{1}(\lambda_{1};W) \geq\exp(-\bar{M}/\lambda)u;\] \[\inf_{\lambda_{1}\in\Lambda,W\in\mathcal{W}}\hat{G}_{1}(\lambda_{1 };W) \geq\exp(-\bar{M}/\lambda)\hat{u}\geq\exp(-\bar{M}/\lambda)u/2.\] \[\text{If }\bar{M}\leq 0: \inf_{\lambda_{1}\in\Lambda,W\in\mathcal{W}}G_{1}(\lambda_{1};W) \geq\exp(-\bar{M}/\bar{\lambda})u;\] \[\inf_{\lambda_{1}\in\Lambda,W\in\mathcal{W}}\hat{G}_{1}(\lambda_{ 1};W) \geq\exp(-\bar{M}/\bar{\lambda})\hat{u}\geq\exp(\bar{M}/\bar{\lambda})u/2.\]

Therefore, with probability \(1-\delta\), when \(n\geq\frac{2}{u^{2}}\log\left(\frac{2}{\delta}\right)\), we have

\[\text{If }\bar{M}\geq 0:\] \[\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_{1 };W)\right)\right| \leq\frac{2}{\exp(-\bar{M}/\bar{\lambda})u}\left|\hat{G}_{1}( \lambda_{1};W)-G_{1}(\lambda_{1};W)\right|;\] \[\text{If }\bar{M}\leq 0:\] \[\left|\log(\hat{G}_{1}(\lambda_{1};W))-\log\left(G_{1}(\lambda_{1 };W)\right)\right| \leq\frac{2}{\exp(-\bar{M}/\bar{\lambda})u}\left|\hat{G}_{1}( \lambda_{1};W)-G_{1}(\lambda_{1};W)\right|.\]

This completes the proof of Lemma B.5. 

Additionally, the following Lemma B.6 provides the bound of \(|\log(\hat{u})-\log(u)|\).

**Lemma B.6**.: _Let \(\hat{u}=\frac{1}{n}\sum_{i=1}^{n}T_{i}\) and \(u=\mathbb{E}[\frac{1}{n}\sum_{i=1}^{n}T_{i}]\). For \(n\geq\frac{2}{u^{2}}\log\left(\frac{2}{\delta}\right)\), with probability \(1-\delta\), we have_

\[|\log(\hat{u})-\log(u)|\leq\mathcal{O}\left(\sqrt{\frac{2\log( \frac{2}{\delta})}{nu^{2}}}\right).\] (25)

Proof.: Using Hoeffding's inequality, we have

\[P(|\hat{u}-u|\geq\epsilon) =P\left(\left|\frac{1}{n}\sum_{i=1}^{n}T_{i}-\mathbb{E}[\frac{1}{ n}\sum_{i=1}^{n}T_{i}]\right|\geq\epsilon\right)\leq 2\exp\left(-2n\epsilon^{2} \right),\] \[2\exp\left(-2n\epsilon^{2}\right) \leq\delta\quad\text{solves}\quad\epsilon\geq\sqrt{\frac{\log( \frac{2}{\delta})}{2n}}.\]

Notably, using the results in the previous lemma, we know for \(n\geq\frac{2}{u^{2}}\log\left(\frac{2}{\delta}\right)\), \(\hat{u}\geq u/2\). Therefore, we have

\[|\log(\hat{u})-\log(u)| \leq\frac{1}{\min\{\hat{u},u\}}|\hat{u}-u|.\quad\text{(By Lemma B.4)}\] \[\leq\frac{2}{u}|\hat{u}-u|\leq\frac{2}{u}\mathcal{O}\left(\sqrt {\frac{\log(\frac{2}{\delta})}{2n}}\right)=\mathcal{O}\left(\sqrt{\frac{2\log( \frac{2}{\delta})}{nu^{2}}}\right).\]In the following, we will bound the term \(|\hat{V}(\hat{\tau})-\mathcal{V}(\hat{\tau})|\) using above lemmas. We first define functions \(F_{0}(\lambda_{0})\), \(\hat{F}_{0}(\lambda_{0})\), \(F_{1}(\lambda_{1})\), and \(\hat{F}_{1}(\lambda_{1})\):

\[F_{0}(\lambda_{0}) =\lambda_{0}\epsilon_{0}+\lambda_{0}\log(\mathbb{E}^{P_{C}}[\exp( \hat{\tau}(X)Y/\lambda_{0})])\] \[=\lambda_{0}\epsilon_{0}+\lambda_{0}\log\left(\frac{1}{1-u} \mathbb{E}[(1-T)\exp(\hat{\tau}(X)Y/\lambda_{0})]\right);\] \[\hat{F}_{0}(\lambda_{0}) =\lambda_{0}\epsilon_{0}+\lambda_{0}\log(\frac{1}{n_{c}}\sum_{i= 1}^{n}(1-T_{i})\exp(\hat{\tau}(X_{i})Y_{i}/\lambda_{0}))\] \[=\lambda_{0}\epsilon_{0}+\lambda_{0}\log\left(\frac{1}{n(1-\hat{u })}\sum_{i=1}^{n}(1-T_{i})\exp(\hat{\tau}(X_{i})Y_{i}/\lambda_{0})\right).\] \[F_{1}(\lambda_{1}) =\lambda_{1}\epsilon_{1}+\lambda_{1}\log(\mathbb{E}^{P_{T}}[\exp (-\hat{\tau}(X)Y/\lambda_{1})])\] \[=\lambda_{1}\epsilon_{1}+\lambda_{1}\log\left(\frac{1}{u}\mathbb{ E}[T\exp(-\hat{\tau}(X)Y/\lambda_{1})]\right);\] \[\hat{F}_{1}(\lambda_{1}) =\lambda_{1}\epsilon_{1}+\lambda_{1}\log(\frac{1}{n_{t}}\sum_{i= 1}^{n}T_{i}\exp(-\hat{\tau}(X_{i})Y_{i}/\lambda_{1}))\] \[=\lambda_{1}\epsilon_{1}+\lambda_{1}\log\left(\frac{1}{n\hat{u}} \sum_{i=1}^{n}T_{i}\exp(-\hat{\tau}(X_{i})Y_{i}/\lambda_{1})\right).\]

The following Lemma B.7 bounds the term \(|\hat{F}(\lambda)-F(\lambda)|\).

**Lemma B.7**.: _Let \(u:=P(T=1)\). Assuming that \(0<\lambda\leq\lambda\leq\bar{\lambda}\) and \(\hat{\tau}(X)Y\) is bounded within the range of \(M\) to \(\bar{M}\). Define \(C_{exp}=\mathbf{1}_{\{M\leq\bar{M}\leq 0\}}\exp\left(\bar{M}/\bar{ \lambda}-M/\lambda\right)+\mathbf{1}_{\{M\leq\bar{M}\leq\bar{M}\}}\exp\left( \bar{M}/\bar{\lambda}-M/\bar{\lambda}\right)+\mathbf{1}_{\{0\leq M\leq\bar{M} \}}\exp\left(\bar{M}/\bar{\lambda}-M/\bar{\lambda}\right)\). For \(n\geq 2/u^{2}\log(2/\delta)\), with probability \(1-\delta\), we have_

\[\begin{split}|\hat{F}_{0}(\lambda_{0})-F_{0}(\lambda_{0})|& \leq\mathcal{O}\left(\sqrt{\frac{8\lambda_{0}^{2}\log\frac{2}{ \delta}}{n(1-u)^{2}C_{exp}^{2}}}\right)+\mathcal{O}\left(\sqrt{\frac{2\lambda_ {0}^{2}\log(\frac{2}{\delta})}{n(1-u)^{2}}}\right);\\ |\hat{F}_{1}(\lambda_{1})-F_{1}(\lambda_{1})|&\leq \mathcal{O}\left(\sqrt{\frac{8\lambda_{1}^{2}\log\frac{2}{\delta}}{nu^{2}}C_{ exp}^{2}}\right)+\mathcal{O}\left(\sqrt{\frac{2\lambda_{1}^{2}\log(\frac{2}{ \delta})}{nu^{2}}}\right).\end{split}\] (26)

Proof.: \[|\hat{F}_{0}(\lambda_{0})-F_{0}(\lambda_{0})|\] \[=\left|\lambda_{0}\left(\log\left(\mathbb{E}[(1-T)\exp(\hat{\tau }(X)Y/\lambda_{0})]\right)-\log\left(\frac{1}{n}\sum_{i=1}^{n}(1-T_{i})\exp( \hat{\tau}(X_{i})Y_{i}/\lambda_{0})\right)+\log(1-\hat{u})-\log(1-u)\right|\] \[\leq\lambda_{0}\left|\log\left(\mathbb{E}[(1-T)\exp(\hat{\tau}(X) Y/\lambda_{0})]\right)-\log\left(\frac{1}{n}\sum_{i=1}^{n}(1-T_{i})\exp( \hat{\tau}(X_{i})Y_{i}/\lambda_{0})\right)\right|+\lambda_{0}\left|\log(1- \hat{u})-\log(1-u)\right|.\]If \(M\leq\bar{M}\leq 0:\)

\[|\hat{F}_{0}(\lambda_{0})-F_{0}(\lambda_{0})|\] \[\leq\frac{2\lambda_{0}}{\exp(M/\lambda)(1-u)}\left|\hat{G}_{0}( \lambda_{0};W)-G_{0}(\lambda_{0};W)\right|+\lambda_{0}\left|\log(1-\hat{u})- \log(1-u)\right|\quad\text{(By Lemma B.5)}\] \[\leq\mathcal{O}\left(\sqrt{\frac{8\lambda_{0}^{2}\log\frac{2}{ \delta}}{n(1-u)^{2}}\left(\exp\left(\bar{M}/\bar{\lambda}-\underline{M}/ \lambda\right)\right)^{2}}\right)+\mathcal{O}\left(\sqrt{\frac{2\lambda_{0}^{ 2}\log(\frac{2}{\delta})}{n(1-u)^{2}}}\right)\quad\text{(By Lemma B.3 and Lemma B.6)}\] If \(M\leq 0,\bar{M}\geq 0:\)

\[|\hat{F}_{0}(\lambda_{0})-F_{0}(\lambda_{0})|\] \[\leq\frac{2\lambda_{0}}{\exp(M/\lambda)(1-u)}\left|\hat{G}_{0}( \lambda_{0};W)-G_{0}(\lambda_{0};W)\right|+\lambda_{0}\left|\log(1-\hat{u})- \log(1-u)\right|\quad\text{(By Lemma B.5)}\] \[\leq\mathcal{O}\left(\sqrt{\frac{8\lambda_{0}^{2}\log\frac{2}{ \delta}}{n(1-u)^{2}}\left(\exp\left(\bar{M}/\lambda-\underline{M}/\lambda \right)\right)^{2}}\right)+\mathcal{O}\left(\sqrt{\frac{2\lambda_{0}^{2}\log( \frac{2}{\delta})}{n(1-u)^{2}}}\right)\quad\text{(By Lemma B.3 and Lemma B.6)}\] If \(0\leq M\leq\bar{M}:\)

\[|\hat{F}_{0}(\lambda_{0})-F_{0}(\lambda_{0})|\] \[\leq\frac{2\lambda_{0}}{\exp(M/\bar{\lambda})(1-u)}\left|\hat{G} _{0}(\lambda_{0};W)-G_{0}(\lambda_{0};W)\right|+\lambda_{0}\left|\log(1-\hat{u} )-\log(1-u)\right|\quad\text{(By Lemma B.5)}\] \[\leq\mathcal{O}\left(\sqrt{\frac{8\lambda_{0}^{2}\log\frac{2}{ \delta}}{n(1-u)^{2}}\left(\exp\left(\bar{M}/\lambda-\underline{M}/\bar{\lambda }\right)\right)^{2}}\right)+\mathcal{O}\left(\sqrt{\frac{2\lambda_{0}^{2}\log( \frac{2}{\delta})}{n(1-u)^{2}}}\right)\quad\text{(By Lemma B.3 and Lemma B.6)}\]

\[|\hat{F}_{1}(\lambda_{1})-F_{1}(\lambda_{1})|\] \[=\left|\lambda_{1}\left(\log\left(\frac{1}{u}\mathbb{E}[T\exp(- \hat{\tau}(X)Y/\lambda_{1})]\right)-\log\left(\frac{1}{n\hat{u}}\sum_{i=1}^{n} T_{i}\exp(\hat{\tau}(X_{i})Y_{i}/\lambda_{0})\right)\right)\right|\] \[=\lambda_{1}\left|\log\left(\mathbb{E}[T\exp(\hat{\tau}(X)Y/ \lambda_{1})]\right)-\log\left(\frac{1}{n}\sum_{i=1}^{n}T_{i}\exp(\hat{\tau} (X_{i})Y_{i}/\lambda_{1})\right)+\log(\hat{u})-\log(u)\right|\] \[\leq\lambda_{1}\left|\log\left(\mathbb{E}[T\exp(\hat{\tau}(X)Y/ \lambda_{1})]\right)-\log\left(\frac{1}{n}\sum_{i=1}^{n}T_{i}\exp(\hat{\tau}( X_{i})Y_{i}/\lambda_{1})\right)\right|+\lambda_{1}\left|\log(\hat{u})-\log(u)\right|.\]

[MISSING_PAGE_EMPTY:27]

\[\hat{\mathcal{V}}^{1}(\hat{\tau})-\mathcal{V}^{1}(\hat{\tau}) =\hat{F}_{1}(\hat{\lambda}_{1})-F_{1}(\lambda_{1}^{*})\] \[=\hat{F}_{1}(\hat{\lambda}_{1})-F_{1}(\lambda_{1}^{*})+\hat{F}_{1}( \lambda_{1}^{*})-\hat{F}_{1}(\lambda_{1}^{*})\] \[=\hat{F}_{1}(\lambda_{1}^{*})-F_{1}(\lambda_{1}^{*})+\hat{F}_{1}( \hat{\lambda}_{1})-\hat{F}_{1}(\lambda_{1}^{*})\] \[\leq|\hat{F}_{1}(\lambda_{1}^{*})-F_{1}(\lambda_{1}^{*})|+0\] \[\leq\sup_{\lambda_{1}}|\hat{F}_{1}(\lambda_{1})-F_{1}(\lambda_{1 })|.\]

Therefore, we have

If \(M\leq\bar{M}\leq 0:\)

\[|\hat{\mathcal{V}}^{0}(\hat{\tau})-\mathcal{V}^{0}(\hat{\tau})| \leq\sup_{\lambda}|\hat{F}(\lambda)-F(\lambda)|\leq\mathcal{O} \left(\sqrt{\frac{8\bar{\lambda}^{2}\log\frac{2}{\delta}}{n(1-u)^{2}}\left( \exp\left(\bar{M}/\bar{\lambda}-M/\lambda\right)\right)^{2}}\right)+\mathcal{ O}\left(\sqrt{\frac{2\bar{\lambda}^{2}\log(\frac{2}{\delta})}{n(1-u)^{2}}} \right);\] \[|\hat{\mathcal{V}}^{1}(\hat{\tau})-\mathcal{V}^{1}(\hat{\tau})| \leq\sup_{\lambda}|\hat{F}(\lambda)-F(\lambda)|\leq\mathcal{O} \left(\sqrt{\frac{8\bar{\lambda}^{2}\log\frac{2}{\delta}}{nu^{2}}\left(\exp \left(\bar{M}/\bar{\lambda}-M/\lambda\right)\right)^{2}}\right)+\mathcal{O} \left(\sqrt{\frac{2\bar{\lambda}^{2}\log(\frac{2}{\delta})}{nu^{2}}}\right).\]

If \(M\leq 0,\bar{M}\geq 0:\)

\[|\hat{\mathcal{V}}^{0}(\hat{\tau})-\mathcal{V}^{0}(\hat{\tau})| \leq\sup_{\lambda}|\hat{F}(\lambda)-F(\lambda)|\leq\mathcal{O} \left(\sqrt{\frac{8\bar{\lambda}^{2}\log\frac{2}{\delta}}{n(1-u)^{2}}\left( \exp\left(\bar{M}/\lambda-M/\bar{\lambda}\right)\right)^{2}}\right)+\mathcal{O }\left(\sqrt{\frac{2\bar{\lambda}^{2}\log(\frac{2}{\delta})}{n(1-u)^{2}}} \right);\] \[|\hat{\mathcal{V}}^{1}(\hat{\tau})-\mathcal{V}^{1}(\hat{\tau})| \leq\sup_{\lambda}|\hat{F}(\lambda)-F(\lambda)|\leq\mathcal{O} \left(\sqrt{\frac{8\bar{\lambda}^{2}\log\frac{2}{\delta}}{nu^{2}}\left(\exp \left(\bar{M}/\lambda-M/\bar{\lambda}\right)\right)^{2}}\right)+\mathcal{O} \left(\sqrt{\frac{2\bar{\lambda}^{2}\log(\frac{2}{\delta})}{nu^{2}}}\right).\]

Finally, we have

\[|\hat{\mathcal{V}}_{t}(\hat{\tau})-\mathcal{V}_{t}(\hat{\tau})| \leq\mathcal{O}\left(\sqrt{\frac{8\bar{\lambda}^{2}\log\frac{2}{\delta}}{nu _{t}^{2}}C_{exp}^{2}}\right)+\mathcal{O}\left(\sqrt{\frac{2\bar{\lambda}^{2} \log(\frac{2}{\delta})}{nu_{t}^{2}}}\right).\]

Note that \(u_{1}=P(T=1)\) and \(u_{0}=P(T=0)\). \(C_{exp}=\mathbf{1}_{\{M\leq\bar{M}\leq 0\}}\exp\left(\bar{M}/\bar{ \lambda}-M/\lambda\right)+\mathbf{1}_{\{M\leq 0,\bar{M}\geq 0\}}\exp\left( \bar{M}/\lambda-M/\lambda\right)+\mathbf{1}_{\{0\leq M\leq\bar{M}\}}\exp \left(\bar{M}/\lambda-M/\bar{\lambda}\right)\).

\(\Box\)

## Appendix C Additional Materials

### Additional Explanations

Q1. Why DRM can select CATE estimators that are robust to the uncertainty in PEHE caused by selection bias and unobserved confounders?In Section 3.1 and Section 4.1, we have presented theoretical explanations for the reason why DRM can measure a CATE estimator's robustness against selection bias and unobserved confounding. Below we will explain it more specifically.

In causal inference, all the CATE estimators are constructed on the observational factual data. But how reliable the CATE estimator that learned on factual data is? This question can be never known unless we have the knowledge of the oracle PEHE. As shown in equation (6), we know that the PEHE is equal to two \(\hat{\tau}\)-dependent terms, \(\mathbb{E}[\hat{\tau}(X)Y^{t}|T=t]\) and \(\mathbb{E}[\hat{\tau}(X)Y^{t}|T=1-t]\)Unfortunately, \(\mathbb{E}[\hat{\tau}(X)Y^{t}|T=1-t]\) is uncomputable empirically because we can only observe the factual distribution \(P^{F}=P(X,Y^{t}|T=t)\) but not the counterfactual distribution \(P^{CF}=P(X,Y^{t}|T=1-t)\). The unobserved counterfactual distribution can be regarded as an uncertain distribution varying around the observed and certain factual distribution \(P^{F}\). If we could assume a "God's perspective" and observe \(P^{CF}\) directly, the counterfactual distribution will be certain - like a quantum world! Such an uncertainty in \(P^{CF}\) results in the uncertainty in PEHE. Now we will analyze the source of such uncertainty by analyzing the relationship between the uncertain distribution \(P^{CF}\) and the certain distribution \(P^{F}\) based on equation (2):

\[P(X,Y^{t}|T=1-t)=P(X,Y^{t}|T=t)\frac{P(Y^{t}|T=1-t,X)}{P(Y^{t}|T=t,X)}\frac{P( X|T=1-t)}{P(X|T=t)}.\]

From above, we find the unobservable distribution \(P(X,Y^{t}|T=1-t)\) is equal to the observable distribution \(P(X,Y^{t}|T=t)\) multiplied with \(\frac{P(Y^{t}|T=1-t,X)}{P(Y^{t}|T=t,X)}\frac{P(X|T=1-t)}{P(X|T=t)}\). In other words, \(\frac{p(y^{t}|T=1-t,x)}{p(y^{t}|T=t,x)}\frac{p(x|T=1-t)}{p(x|T=t)}\) controls the discrepancy between \(P^{F}\) and \(P^{CF}\). Note that if there is no unmeasured confounders, then we have \(p(y^{t}|T=1-t,x)=p(y^{t}|T=t,x)\); and if there is no selection bias (covariate shift), then we have \(p(x|T=1-t)=p(x|T=t)\). Now we understand the root cause of the discrepancy between \(P^{F}\) and \(P^{CF}\) (or between \(\mathbb{E}[\tau(X)Y^{t}|T=1-t]\) and \(\mathbb{E}[\tau(X)Y^{t}|T=t]\)) lies at the unobserved confounders and selection bias (covariate shift). In the DRM method, the uncertainty caused by potential unobserved confounders and selection bias in PEHE can be further measured as the distributionally robust values \(\hat{\mathcal{V}}^{1}\) and \(\hat{\mathcal{V}}^{0}\). Then the PEHE w.r.t. the CATE estimator \(\hat{\tau}\) will be at most \(\mathcal{R}^{DRM}(\hat{\tau})\), as shown in equation (15). An estimator \(\hat{\tau}\) that attains smallest \(\mathcal{R}^{DRM}(\hat{\tau})\) by definition reflects the distributional robustness against potential unobserved confounders and selection bias.

Q2. How to set \(\epsilon^{*}\) when there are unobserved confounders?When unobserved confounders are present, Proposition 3.6 can also provide guidance for setting \(\epsilon^{*}\). Taking \(\epsilon^{*}_{1}=D_{KL}(P_{C}||P_{T})\) as an example, we have

\[D_{KL}(P_{C}||P_{T})\] \[=\int_{\mathcal{X}}\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^{1}}p( y^{0},y^{1}|x,T=0)p(x|T=0)\log\frac{p(y^{0},y^{1}|x,T=0)p(x|T=0)}{p(y^{0},y^{1}|x,T= 1)p(x|T=1)}dy^{1}dy^{0}dx\] \[=\int_{\mathcal{X}}\left(\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y} ^{1}}p(y^{0},y^{1}|x,T=0)dy^{1}dy^{0}\right)p(x|T=0)\log\frac{p(x|T=0)}{p(x|T= 1)}dx\] \[\quad+\int_{\mathcal{X}}\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^{ 1}}p(y^{0},y^{1}|x,T=0)p(x|T=0)\log\frac{p(y^{0},y^{1}|x,T=0)}{p(y^{0},y^{1}|x,T=1)}dy^{1}dy^{0}dx\] \[=D_{KL}(P(X|T=0)||P(X|T=1))\] \[\quad+\int_{\mathcal{X}}\int_{\mathcal{Y}^{0}}\int_{\mathcal{Y}^{ 1}}p(y^{0},y^{1}|x,T=0)p(x|T=0)\log\frac{p(y^{0},y^{1}|x,T=0)}{p(y^{0},y^{1}|x,T=1)}dy^{1}dy^{0}dx\] \[>D_{KL}(P_{X}^{C}||P_{X}^{T})\]

Therefore, when unobserved confounders present, we can set \(\epsilon^{*}\) to a larger value than the one guided by Proposition 4.6. Simultaneously, as the empirical approximation of \(\epsilon^{*}_{1}=D_{KL}(P_{X}^{C}||P_{X}^{T})\) and \(\epsilon^{*}_{0}=D_{KL}(P_{X}^{T}||P_{X}^{C})\) can be biased, we also suggest set \(\epsilon^{*}\) to a large value than the empirically-computed ones to ensure the ambiguity set is large enough to contain the target distribution. Therefore, we generally set \(\epsilon^{*}_{1}=D_{KL}(P_{X}^{C}||P_{X}^{T})+5.2\) and \(\epsilon^{*}_{0}=D_{KL}(P_{X}^{T}||P_{X}^{C})+5.2\) for all settings in our experiment. Theoretically, a larger \(\epsilon^{*}\) should guarantee the DRM-selected estimator to be more robust, as it allows for a broader range of possible counterfactual distributions in the ambiguity set. However, setting \(\epsilon^{*}\) too large can result in overly conservative estimator selection (similar to the well-known accuracy-robustness tradeoff). Therefore, how to determine a proper ambiguity radius still remains an open challenge in both our work and distributionally robust optimization literature.

### Hyperparameters

* For linear model, we use LogisticRegressionCV and RidgeCV (both are with 3-fold cross-validation) from sklearn package to tune hyperparameters: Logistic regression: \(Cs\in\) {0.01, 0.1, 1, 10}; Ridge Regression: \(\alpha\in\) {0.01, 0.1, 1, 10, 100}.

* For Neural Net, we set the hidden layers as [200, 200, 200, 100, 100], each with the ReLU activation function. The model is trained using the Adam optimizer with a learning rate of 0.001, a batch size of 64, and 300 epochs.
* For RF, XGBoost, and SVM model, we use AutoML [71, 53] (with 3-fold cross-validation) from flaml package to tune hyperparameters.

### The Complementary Results

First, we would like to emphasize that the experimental results in this final version of the paper differ from those in the original version. We made several revisions based on the feedback from anonymous reviewers: 1) We add Neural Net model to the base ML models and add the U-learner to the meta-learners, increasing the number of CATE estimators from 24 to 36; 2) We adopted AutoML for hyperparameter tuning when training SVM, RF, and XGBoost. All code is available at https://github.com/yiyhuang3/CATE_estimator_selection.

Below, we prsent the complementary PEHE results for 36 candidate CATE estimators, where the candidate pool contains 4 ML models (LR, SVM, RF, and Neural Net) \(\times\) 9 learners (U-, S-, T-, PS-, IPW-, X-, DR-, R-, RA-).

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & A (\(\rho=0\)) & A (\(\rho=0.1\)) & A (\(\rho=0.3\)) & B (\(\xi=0\)) & B (\(\xi=2\)) & C (\(m=0.1\)) & C (\(m=0.5\)) & C (\(m=0.9\)) \\ \hline Plug-U & 49.59\(\pm\)95.07 & 41.93\(\pm\)61.07 & 36.16\(\pm\)61.77 & 2.28\(\pm\)2.32 & 155.24\(\pm\)291.78 & 42.45\(\pm\)52.65 & 59.51\(\pm\)210.54 & 24.37\(\pm\)26.51 \\ Plug-S & 5.10\(\pm\)8.29 & 5.36\(\pm\)5.84 & 6.29\(\pm\)5.76 & **1.99\(\pm\)**1.41 & 9.18\(\pm\)15.57 & 5.76\(\pm\)5.46 & **7.88\(\pm\)**7.55 & 13.45\(\pm\)**9.53 \\ Plug-PS & 4.80\(\pm\)7.74 & 5.36\(\pm\)5.85 & 6.28\(\pm\)5.75 & **1.99\(\pm\)**1.41 & **9.71\(\pm\)**11.58 & 5.76\(\pm\)5.46 & **8.58\(\pm\)**7.40 & 13.45\(\pm\)**9.53 \\ Plug-T & 60.84\(\pm\)2.203 & 59.09\(\pm\)25.88 & 59.39\(\pm\)21.34 & 2.125\(\pm\)10.30 & 68.22\(\pm\)18.14 & 62.90\(\pm\)19.13 & 48.32\(\pm\)23.60 & 45.07\(\pm\)20.63 \\ Plug-W & 9.82\(\pm\)10.67 & 10.39\(\pm\)22.00 & 8.91\(\pm\)11.30 & 6.52\(\pm\)10.90 & 14.82\(\pm\)14.23 & 10.82\(\pm\)15.24 & 15.80\(\pm\)15.03 & 20.59\(\pm\)13.03 \\ Plug-IPW & 5.50\(\pm\)9.268 & 35.05\(\pm\)27.78 & 39.20\(\pm\)27.48 & 6.19\(\pm\)7.99 & 61.90\(\pm\)20.41 & 44.47\(\pm\)31.37 & 30.02\(\pm\)22.57 & 39.32\(\pm\)0.45 \\ Plug-DR & 44.83\(\pm\)26.74 & 46.47\(\pm\)27.02 & 48.23\(\pm\)26.56 & 5.98\(\pm\)9.09 & 67.87\(\pm\)18.94 & 49.61\(\pm\)33.34 & 33.69\(\pm\)23.05 & 32.39\(\pm\)19.28 \\ Plug-R & 3.64\(\pm\)5.01 & 5.33\(\pm\)15.72 & 5.51\(\pm\)3.75 & 2.19\(\pm\)23.31 & 13.04\(\pm\)31.51 & 4.95\(\pm\)5.38 & 7.56\(\pm\)7.88 & 10.91\(\pm\)17.92 \\ Plug-R & 5.83\(\pm\)22.42 & 60.00\(\pm\)13.01 & 58.63\(\pm\)22.86 & 3.87\(\pm\)7.71 & 77.82\(\pm\)58.91 & 91.96\(\pm\)6.54 & 45.22\(\pm\)24.80 & 42.13\(\pm\)20.35 \\ Pseudo-DR & 63.07\(\pm\)22.54 & 63.00\(\pm\)22.23 & 63.10\(\pm\)19.41 & 65.12\(\pm\)23.05 & 7.29\(\pm\)17.48 & 65.21\(\pm\)20.41 & 53.87\(\pm\)26.16 & 53.79\(\pm\)24.91 \\ Pseudo-R & 11.57\(\pm\)27.25 & 16.83\(\pm\)58.51 & 9.97\(\pm\)21.23 & 6.49\(\pm\)20.46 & 18.13\(\pm\)30.61 & 13.62\(\pm\)24.78 & 20.96\(\pm\)30.42 & 30.05\(\pm\)32.02 \\ Pseudo-IF & 66.26\(\pm\)15.20 & 6.52\(\pm\)16.35 & 66.72\(\pm\)15.84 & 8.49\(\pm\)23.55 & 69.01\(\pm\)16.57 & 63.09\(\pm\)20.62 & 60.00\(\pm\)19.18 & 37.40\(\pm\)20.16 \\ Random & 7216.27\(\pm\)27.45 & 64.61\(\pm\)265.00 & 4200\(\pm\)17048 & 1136\(\pm\)595 & 7552\(\pm\)22498 & 3771\(\pm\)16652 & 6219\(\pm\)19924 & 345.93\(\pm\)14590 \\ Fact & 52.81\(\pm\)18.01 & 53.58\(\pm\)19.42 & 55.05\(\pm\)21.10 & 16.09\(\pm\)61.50 & 65.80\(\pm\)27.69 & 51.96\(\pm\)17.45 & 52.44\(\pm\)22.51 & 49.16\(\pm\)24.47 \\ Matching & 62.57\(\pm\)21.57 & 64.90\(\pm\)17.85 & 63.94\(\pm\)18.72 & 15.10\(\pm\)22.93 & 72.25\(\pm\)17.46 & 64.56\(\pm\)19.59 & 57.87\(\pm\)24.40 & 48.81\(\pm\)25.23 \\ DRM & **2.68\(\pm\)**4.73 & **3.55\(\pm\)**5.65 & **5.28\(\pm\)**6.37 & 2.14\(\pm\)17.0 & 18.77\(\pm\)12.78 & **4.60\(\pm\)**9.58 & **6.44\(\pm\)**9.73 & **10.05\(\pm\)**7.19 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of PEHE for different sectors across Settings A, B, and C (Note that B (\(\xi=1\)) matches A (\(\rho=0.1\))), with base model for CATE estimator being {LR, SVM, RF, Net}. Reported values (mean \(\pm\) standard deviation) are computed over 100 experiments. Smaller is better.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: [Yes] Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: [Yes] Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: [Yes] Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: [Yes] Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: [Yes] Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: [Yes] Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: [Yes] Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: [Yes] Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: [Yes] Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines: [The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.