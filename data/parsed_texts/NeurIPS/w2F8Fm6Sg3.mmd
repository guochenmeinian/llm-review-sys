# Balanced Training for Sparse GANs

Yite Wang\({}^{1}\), Jing Wu\({}^{1}\), Naira Hovakimyan\({}^{1}\), Ruoyu Sun\({}^{2,3}\)

\({}^{1}\)University of Illinois Urbana-Champaign, USA

\({}^{2}\)School of Data Science, The Chinese University of Hong Kong, Shenzhen, China

\({}^{3}\)Shenzhen International Center for Industrial and Applied Mathematics,

Shenzhen Research Institute of Big Data

{yitew2,jingwu6,nhovakim}@illinois.edu, sunruoyu@cuhk.edu.cn

Corresponding author.Equal contribution

###### Abstract

Over the past few years, there has been growing interest in developing larger and deeper neural networks, including deep generative models like generative adversarial networks (GANs). However, GANs typically come with high computational complexity, leading researchers to explore methods for reducing the training and inference costs. One such approach gaining popularity in supervised learning is dynamic sparse training (DST), which maintains good performance while enjoying excellent training efficiency. Despite its potential benefits, applying DST to GANs presents challenges due to the adversarial nature of the training process. In this paper, we propose a novel metric called the balance ratio (BR) to study the balance between the sparse generator and discriminator. We also introduce a new method called balanced dynamic sparse training (ADAPT), which seeks to control the BR during GAN training to achieve a good trade-off between performance and computational cost. Our proposed method shows promising results on multiple datasets, demonstrating its effectiveness. Our code is available at https://github.com/YiteWang/ADAPT.

## 1 Introduction

Generative adversarial networks (GANs) [20, 7, 70, 44] are a type of generative model that has gained significant attention in recent years due to their impressive performance in image-generation tasks. However, the mainstream models in GANs are known to be computationally intensive, making them challenging to train in resource-constrained settings. Therefore, it is crucial to develop methods that can effectively reduce the computational cost of training GANs while maintaining their performance, making GANs more practical and applicable in real-world scenarios.

Neural network pruning has recently emerged as a powerful tool to reduce the training and inference costs of DNNs for supervised learning. There are mainly three genres of pruning methods, namely pruning-at-initialization, pruning-during-training, and post-hoc pruning methods. Post-hoc pruning [32, 42, 25] can date back to the 1980s, which was first introduced for reducing inference time and memory requirements for efficient deployment; hence does not align with our purpose of efficient training. Later, pruning-at-initialization [46, 78, 75] and pruning-during-training methods [84] were introduced to circumvent the need to fully train the dense networks. However, early pruning-during-training algorithms [58] do not bring much training efficiency compared to post-hoc pruning, while pruning-at-initialization methods usually suffer from significant performance drop [18]. Recently, advances in dynamic sparse training (DST) [62, 16, 51, 52, 54] for the first time show that pruning-during-training methods can have comparable training FLOPs as pruning-at-initialization methodswhile having competing performance to post-hoc pruning. Therefore, applying DST on GANs seems to be a promising choice.

Although DST has attained remarkable achievements in supervised learning, the application of DST on GANs is not successful due to newly emerging challenges. One challenge is keeping the generator and the discriminator balanced. In particular, using overly strong discriminators can lead to overfitting, while weaker discriminators may fail to effectively prevent mode collapse [3; 4]. Hence, balancing the sparse generator and the (possibly) sparse discriminator throughout training is even more difficult. To mitigate the unbalance issue, a recent work STU-GAN [53] proposes to apply DST directly to the generator. However, we find empirically that such an algorithm is likely to fail when the generator is already more powerful than the discriminator. Consequently, it remains unclear how to conduct balanced dynamic sparse training for GANs.

To this end, we propose a metric called balance ratio (BR), which measures the degree of balance of the two components, to study sparse GAN training. We find that BR is useful in (1) understanding the interaction between the discriminator and the generator, (2) identifying the cause of a certain training failure/collapse [7; 8], and (3) helping stabilize sparse GAN training as an indicator. To our best knowledge, this is the first study to quantify the unbalance of sparse GANs and may even provide new insights into dense GAN training.

Furthermore, using BR as an indicator, we propose bAlanced DynAmic sParse Training (ADAPT) to adjust the density and the connections of the discriminator automatically during training.

Our main contributions are summarized below:

* We introduce a novel quantity named balance ratio to study the degree of balance in sparse GAN training.
* We find empirically that the balance ratio is problematic in certain practical training scenarios and that existing methods are inadequate for resolving this issue.
* We propose ADAPT, which makes real-time monitoring of the balance ratio. By dynamically adjusting the discriminator, ADAPT enables effective control of the balance ratio throughout training. Empirically, ADAPT achieves a good trade-off between performance and computational cost on several datasets.

## 2 Related works

### Neural network pruning

In deep learning, efficiency is achieved through several methods. This paper primarily focuses on model training and inference efficiency, which is different from techniques for data efficiency [83; 85; 86]. These include neural architecture search (NAS) [80; 49] to discover optimal network structures, quantization [31; 67] for computational efficacy, knowledge distillation [28] to leverage the knowledge of larger models for smaller counterparts, and neural network pruning to remove unnecessary connections. Among these, neural network pruning is the focal point of our research. More specifically, we narrow our focus on unstructured pruning [25; 17], where individual weight is the finest resolution. This contracts with structured pruning [56; 59; 57; 30] where entire neurons or channels are pruned.

**Post-hoc pruning.** Post-hoc pruning method prunes weights of a fully-trained neural network. It usually requires high computational costs due to the multiple rounds of the train-prune-retrain procedure [25; 68]. Some use specific criteria [42; 26; 25; 23; 15; 12; 91; 63] to remove weights, while others perform extra optimization iterations [77]. Post-hoc pruning was initially proposed to reduce the inference time, while lottery ticket works [17; 68] aimed to mine trainable sub-networks.

**Pruning-at-initialization methods.** SNIP [46] is one of the pioneering works that aim to find trainable sub-networks without any training. Some follow-up works [78; 75; 13; 65; 1] aim to propose different metrics to prune networks at initialization. Among them, Synflow [75], SPP [45], and FORCE [13] try to address the problem of layer collapse during pruning. NTT [55], PHEW [65], and NTK-SAP [82] draw inspiration from neural tangent kernel theory.

**Pruning-during-training methods.** Another genre of pruning algorithms prunes or adjusts DNNs throughout training. Early works add explicit \(\ell_{0}\)[58] or \(\ell_{1}\)[84] regularization terms to encourage a sparse solution, hence mitigating performance drop incurred by post-hoc pruning. Later works learn the subnetworks structures through projected gradient descent [94] or trainable masks [73; 88; 35; 41; 50; 71]. However, these pruning-during-training methods often do not introduce memory sparsity during training. As a remedy, DST methods [5; 62; 64; 14; 16; 51; 52; 54; 21] were introduced to train the neural networks under a given parameter budget while allowing mask change during training.

### Generative adversarial networks

**Generative adversarial networks (GANs).** GANs [20] have drawn considerable attention and have been widely investigated for years. Deep convolutional GANs [66] replace fully-connected layers in the generator and the discriminator. Follow-up works [22; 36; 7; 92] employed more advanced methods to improve the fidelity of generated samples. After that, several novel loss functions [69; 60; 2; 22; 74], normalization and regularization methods [61; 76; 87] were proposed to stabilize the adversarial training. Besides the efforts devoted to training GANs, image-to-image translation is also extensively explored [96; 95; 10; 38; 43; 81].

**GAN balance.** Addressing the balance between the generator and discriminator in GAN training has been the focus of various works. However, directly applying existing methods to sparse GAN training poses challenges. For instance, [3; 4] offer theoretical analyses on the issue of imbalance but may have limited practical benefits, e.g., they require training multiple generators and discriminators. Empirically, BEGAN [6] proposes to use proportional control theory to maintain a hyper-parameter \(\frac{\mathbb{E}\|G(z)-D(G(z))|^{n}\}\), but it is only applicable when the discriminator is an auto-encoder. Unbalanced GAN [24] pretrains a VAE to initialize the generator, which may only address the unbalance near initialization. GCC [48] considers the balance during GAN compression, while its criterion requires a trained (dense) GAN, which is not given in the DST setting. Finally, STU-GAN [53] proposes to use DST to address the unbalance issues but may fail under certain conditions, as demonstrated in our experiments.

**GAN compression and pruning.** One of the promising ways is based on neural architecture search and distillation algorithm [47; 19; 29]. Another part of the work applied pruning-based methods for generator compression [72; 90; 33]. Later, works by [79] presented a unified framework by combing the methods mentioned above. Nevertheless, they only focus on the pruning of generators, thus potentially posing a negative influence on Nash Equilibrium between generators and discriminators. GCC [48] compresses both components of GANs by letting the student GANs also learn the losses. Another line of work [34; 9; 8] tries to test the existence of lottery tickets in GANs. To the best of our knowledge, STU-GAN [53] is the only work that tries to directly train sparse GANs from scratch.

## 3 Preliminary and setups

Generative adversarial networks (GANs) have two fundamental components, a generator \(G(\cdot;\bm{\theta}_{G})\) and a discriminator \(D(\cdot;\bm{\theta}_{D})\). Specifically, the generator maps a sampled noise \(\bm{z}\) from a multivariate normal distribution \(p(\bm{z})\) into a fake image to cheat the discriminator. In contrast, the discriminator distinguishes the generator's output and the real images \(\bm{x}_{r}\) from the distribution \(p_{\text{data}}(\bm{x}_{r})\).

Formally, the optimization objective of the two-player game defined in GANs can be generally defined as follows:

\[\mathcal{L}_{D}(\bm{\theta}_{D},\bm{\theta}_{G})= \mathbb{E}_{\bm{x}_{r}\sim p_{\text{data}}}\left[f_{1}(D(\bm{x}_{ r};\bm{\theta}_{D}))\right]+\mathbb{E}_{\bm{z}\sim p}\left[f_{2}(D(G(\bm{z};\bm{ \theta}_{G})))\right]\] (1) \[\mathcal{L}_{G}(\bm{\theta}_{G})= \mathbb{E}_{\bm{z}\sim p}\left[g_{1}(D(G(\bm{z};\bm{\theta}_{G}))) \right].\] (2)

To be more specific, different losses can be used, including the loss in the original JS-GAN [20] where \(f_{1}(x)=-\log(x)\), \(f_{2}(x)=-g_{1}(x)=-\log(1-x)\); Wasserstein loss [22] where \(f_{1}(x)=-f_{2}(x)=g_{1}(x)=-x\); and hinge loss [61] where \(f_{1}(x)=\max(0,1-x)\), \(f_{2}(x)=\max(0,1+x)\), and \(g_{1}(x)=-x\). The two components are optimized alternately to achieve the Nash equilibrium.

**GAN sparse training.** In this work, we are interested in sparse training for GANs. In particular, the objective of sparse GAN training can be formulated as follows:

\[\bm{\theta}_{D}^{*} =\min_{\bm{\theta}_{D}}\mathcal{L}_{D}(\bm{\theta}_{D}\odot\bm{m}_{ D},\bm{\theta}_{G}\odot\bm{m}_{G})\] \[\bm{\theta}_{G}^{*} =\min_{\bm{\theta}_{G}}\mathcal{L}_{G}(\bm{\theta}_{G}\odot\bm{m} _{G})\] \[\text{s.t.}\quad\bm{m}_{D}\in\{0,1\}^{|\bm{\theta}_{D}|},\quad \bm{m}_{G}\in\{0,1\}^{|\bm{\theta}_{G}|},\quad\|\bm{m}_{D}\|_{0}/|\bm{\theta} _{D}|\leq d_{D},\quad\|\bm{m}_{G}\|_{0}/|\bm{\theta}_{G}|\leq d_{G},\]

where \(\odot\) is the Hadamard product; \(\bm{\theta}_{D}^{*}\), \(\bm{m}_{D}\), \(|\bm{\theta}_{D}|\), \(d_{D}\) are the sparse solution, mask, number of parameters, and target density for the discriminator, respectively. The corresponding variables for the generator are denoted with subscript \(G\). For pruning-at-initialization methods, masks \(\bm{m}\) are determined before training, whereas \(\bm{m}\) are dynamically adjusted for dynamic sparse training (DST) methods.

**Dynamic sparse training (DST).** DST methods [62; 16] usually start with a sparse network parameterized by \(\bm{\theta}\odot\bm{m}\) with randomly initialized mask \(\bm{m}\). After a constant time interval \(\Delta T\), it updates mask \(\bm{m}\) by removing a fraction of connections and activating new ones with a certain criterion. The total number of active parameters \(\|\bm{m}\|_{0}\) is hence kept under a certain threshold \(d|\bm{\theta}|\). Please see Appendix B for more details.

## 4 Motivating observations: The unbalance in sparse GAN training

As discussed in section 1, it is essential to maintain the balance of generator and discriminator during GAN training. As strong discriminators may lead to over-fitting, whereas weak discriminators may be unable to detect mode collapse. When it comes to sparse GAN training, the consequences caused by the unbalance can be further amplified. For example, sparsifying a weak generator while keeping the discriminator unmodified may lead to an even more unbalanced worst-case scenario.

To support our claim, we conduct experiments with SNGAN [61] on the CIFAR-10 dataset. We consider the following sparse training algorithms:

**Static sparse training (STATIC).** For STATIC, layer-wise sparsity ratio and masks \(\bm{m}_{G},\bm{m}_{D}\) are fixed throughout the training.

**Single dynamic sparse training (SDST).** SDST is a direct application of the DST method on GANs where only the generator dynamically adjusts masks during the training. We name such method SDST as only one component of the GAN, i.e., the generator, is dynamic. Furthermore, we call the variant which grows connections based on gradients magnitude as \(\bullet\)SDST-RigL [16], and randomly as \(\bullet\)SDST-SET [62]. Note that STU-GAN [53] is almost identical to \(\bullet\)SDST-RigL with EMA [89] tailored for DST. We do not consider naively applying DST on both generators and discriminators, as in STU-GAN, it is empirically shown that simply adjusting both components generates worse performance with more severe training instability.2

Footnote 2: We also perform a small experiment in subsection E.2 to validate their findings.

We test the considered algorithms with \(d_{G}\in\{10\%,20\%,30\%,50\%\}\) and \(d_{D}\in\{10\%,20\%,30\%,\)\(50\%,100\%\}\). More experiment details can be found at Appendix A and Appendix B.

Figure 1: FID (\(\downarrow\)) comparison of SDST against STATIC sparse training for SNGAN on CIFAR-10 with different sparsity ratio combinations. The shaded areas denote the standard deviation.

### Key observations

We report the results in Figure 1 and summarize our critical findings as follows:

**Observation 1: Neither strong nor weak sparse discriminators can provide satisfactory results.** The phenomenon is most noticeable when \(d_{G}=10\%\), where the FID initially decreases but then increases. The reasons may be as follows: (1) Overly weak discriminators may cause training collapse as they cannot provide useful information to the generator, resulting in a sudden increase in FID at the early stage of sparse GAN training. (2) Overly strong discriminators may not yield good FID results because they learn too quickly, not allowing the generator to keep up. Hence, to ensure a balanced training of GAN for sparse training methods, it is crucial to find an appropriate sparsity ratio for the discriminator.

**Observation 2: SDST is unable to give stable performance boost compared to the STATIC baseline.** Another critical observation is that SDST is better than STATIC only when the discriminator is strong enough. More specifically, for all selected discriminator density ratios, SDST method is not better than STATIC when using a small discriminator density (\(d_{D}=10\%\)). On the contrary, for the cases where \(d_{D}>d_{G}\), we generally see a significant performance boost brought by SDST.

## 5 Balance ratio: Towards quantifying the unbalance in sparse GAN training

### Formulation of the balance ratio

To gain a deeper understanding of the phenomenon observed in the previous section, and to better monitor and control the degree of unbalance in sparse GANs, we introduce a novel quantity called the balance ratio (BR). This quantity is defined as follows.

At each training iteration, we draw random noise \(\bm{z}\) from a multivariate normal distribution and real images \(\bm{x}_{r}\) from the training set. We denote the discriminator after gradient descent update as \(D(\cdot;\bm{\theta}_{D})\). We denote generator before and after gradient descent training as \(G^{\text{pre}}(\cdot;\bm{\theta}_{G})\) and

Figure 2: BR comparison of SDST against STATIC sparse training for SNGAN on CIFAR-10 with different sparsity ratio combinations.

\(G^{\text{post}}(\cdot;\bm{\theta}_{G}^{\prime})\), respectively. Then the balance ratio is defined as:

\[\text{BR}=\frac{\mathbb{E}_{\bm{z}\sim p}\left[D(G^{\text{post}}(\bm{z}))-D(G^{ \text{pre}}(\bm{z}))\right]}{\mathbb{E}_{\bm{x}_{r}\sim p_{\text{ann}}}[D(\bm{ x}_{r})]-\mathbb{E}_{\bm{z}\sim p}[D(G^{\text{pre}}(\bm{z}))]}=\frac{\alpha}{ \beta}.\] (3)

Precisely, BR measures how much improvement the generator can achieve in the scale measured by the discriminator for a specific random noise \(\bm{z}\). When BR is too small (e.g., BR\(<30\%\)), the updated generator is too weak to trick the discriminator, as the generated images are still considered fake. Similarly, for the case where BR is too large (e.g., BR\(>80\%\)), the discriminator is considered too weak hence it may not provide useful information to the generator. We also illustrate BR in Figure 3.

### Understanding observation 1: Analysing GAN balance with the balance ratio

We visualize the BR evolution throughout the training for the experiments in section 4 to show the effectiveness of BR in quantifying the balance of sparse GANs. We show the results in Figure 2.

It illustrates that BR can distinguish the density difference (hence the representation power difference) of the discriminator. Specifically, we can see that for larger discriminator density \(d_{D}\), the BR is much lower throughout the training, indicating strong discriminators. On the contrary, for the cases where the discriminators are too weak compared to the generators, e.g., all cases where \(d_{D}=10\%\), we can observe BR first increases and then oscillates wildly. We believe this oscillatory behavior is related to the training collapse. Empirical results also show that the FID metric experiences a sudden increase after this turning point.

### Dynamic density adjust: A first attempt to utilize the balance ratio

As demonstrated in the previous section, the balance ratio (BR) effectively captures the degree of balance between the generators and discriminators in sparse GANs. Hence, it is natural to leverage BR to dynamically adjust the density of discriminators during sparse GAN training so that a reasonable discriminator density can be found.

To demonstrate the value of BR, we propose a simple yet powerful modification to the STATIC baseline. This method, which we call **dynamic density adjust (DDA)**, is explained below. Specifically, we initialize the initial density of the discriminator \(d_{D}^{\text{init}}=d_{G}\). After a specific training iteration interval, we adjust the density of the discriminator based on the BR over the last few iterations with a pre-defined density increment \(\Delta d\). With a pre-defined BR bounds \([B_{-},B_{+}]\), we decrease \(d_{D}\) by \(\Delta d\) when BR is smaller than \(B_{-}\), and vise versa. We show the algorithm in Appendix C Algorithm 1.

**Comparison to ADA [37].** In this paragraph, we compare ADA and DDA. (1) Notice that DDA algorithm is orthogonal to ADA in a sense that StyleGAN2-ADA adjusts the data augmentation

Figure 4: FID (\(\downarrow\)) of STATIC sparsely trained SNGAN with and without DDA on CIFAR-10 with different sparsity ratio combinations. The result of DDA is independent of \(d_{D}\) as it is determined automatically. The shaded areas denote the standard deviation.

Figure 3: Illustration of balance ratio.

probability while DDA adjusts the discriminator density. (2) Moreover, the criterion used in DDA, i.e. BR, is very different from the criterion proposed in StyleGAN2-ADA, i.e. \(r_{v}=\frac{\mathbb{E}(D(x_{\text{train}}))-\mathbb{E}(D(x_{\text{val}}))}{ \mathbb{E}[D(x_{\text{train}})]-\mathbb{E}[D(x_{\text{val}})]}\) and \(r_{t}=\mathbb{E}\left[\text{sign}(D(x_{\text{train}}))\right]\). In particular, \(r_{v}\) requires a separate validation set, while \(r_{t}\) only quantifies the overfitting of the discriminator to the training set. (3) Another note is that DDA is a flexible framework, where its criterion, i.e. BR, can be potentially replaced by \(r_{v}\), \(r_{t}\), and such.

**Experiment results.** We test DDA with target BR interval \([B_{-},B_{+}]=[0.5,0.65]\). Precisely, DDA tends to find a suitable discriminator where the generator can just trick the discriminator throughout the training. We show the results in Figure 4 with red lines. The experiments show that DDA can identify reasonable discriminator densities to enable balanced training for sparse GANs.

### Understanding observation 2: Analysing the failure of SDST with the balance ratio

By leveraging BR, we can also gain further insights into why some configurations do not benefit from SDST as compared to STATIC.

**Regarding SDST as a way of increasing the generator capacity.** Our findings suggest that SDST can possibly enhance the generator's representation power, as demonstrated by the higher BR values compared to STATIC observed in Figure 2. We attribute this effect to the in-time over-parameterization (ITOP) [54] induced by dynamic sparse training.

**SDST does not address training collapse.** The increase in generator's representation power resulting from SDST is only beneficial when the discriminator has matching or superior representation power. Therefore, if the training has already collapsed for the static baseline methods (STATIC), meaning that the generator is already stronger than the discriminator, SDST may not be effective in stabilizing sparse GAN training. This is evident from the results shown in Figure 2 first-row column 1, second-row column 1, third-row columns 1-2, and fourth-row columns 1-3.

Despite the superior performance of STU-GAN (or SDST in general) at higher discriminator density ratios \(d_{D}\), there exist some limitations for SDST, which we summarize below:

\(\bullet\)SDST requires a pre-defined discriminator density \(d_{D}\) before training. However, it is unclear what is a good choice. In real-world scenarios, it is not practical to manually search for the optimal \(d_{D}\) for each \(d_{G}\). A workaround may be using the maximum allowed density for the discriminator. However, as shown in Figure 1, the best performance is not always obtained with the maximum \(d_{D}=100\%\). Moreover, we are wasting extra computational cost for a worse performance if we use an overly-strong discriminator.

\(\bullet\)SDST fails if there is an additional constraint on the density of the discriminator \(d_{D}\). As Figure 1 suggests, for weak discriminators, SDST is unable to show consistent improvement compared to the STATIC baseline.

Hence, STU-GAN (or SDST in general), which directly applies DST to the generator, may only be useful when the corresponding discriminator is strong enough. In this sense, obtaining balanced training automatically is essential in GAN DST to deal with more complicated scenarios.

## 6 Balanced dynamic sparse training for GANs

In this section, we describe our methodology for balanced sparse GAN training.

STU-GAN (or SDST in general) considered in the last section cannot generate stable and satisfying performance. This implies that we should utilize the discriminator in a better way rather than do nothing (like SDST) or directly apply DST to the discriminator (see subsection E.2 for additional experiments). Consequently, DDA (subsection 5.3), which adjusts the discriminator density to stabilize GAN training, is a favorable candidate to address the issue. To this end, we propose **bAlamed DynAmicsparse Training (ADAPT)**, which adjusts the density of the discriminator during training with DDA while the generator performs DST.

We further introduce two variants, namely ADAPT\({}_{\text{relax}}\) and ADAPT\({}_{\text{strict}}\), based on whether we force the discriminator to be sparse. We present them in subsection 6.1 and subsection 6.2. These methods are more flexible and generate more stable performance compared to SDST.

### ADAPT\({}_{\text{relax}}\): Balanced dynamic sparse training in the relaxed setting

In this section, we consider the relaxed setting where a dense discriminator can be used, i.e., \(d_{D}\leq d_{\text{max}}=100\%\). This relaxed scenario gives the greatest flexibility to the discriminator. However, it does not necessarily enforce the sparsity of the discriminator (hence, no computational savings for the discriminator) because the density of the discriminator can be as high as \(100\%\).

For the relaxed setting, we use the direct combination of SDST with DDA. Precisely, the generator is adjusted using DST as mentioned in section 4 while the density of the discriminator is dynamically adjusted with DDA as mentioned in subsection 5.3. We call such a combination **relaxed balanced dynamic sparse training (ADAPT\({}_{\text{relax}}\))**. Please see Appendix C Algorithm 2 for more details.

**Comparison to STU-GAN (or SDST in general)**. Compared to STU-GAN (or SDST in general), which pre-defines and fixes the discriminator density during training, the difference is that for ADAPT\({}_{\text{relax}}\), the density of the discriminator is adjusted during the training process automatically through real-time monitoring of the balance ratio. Given the initial discriminator density \(d_{D}^{\text{init}}=d_{G}\), ADAPT\({}_{\text{relax}}\) increases the discriminator density if a stronger discriminator is needed, and vice versa.

### ADAPT\({}_{\text{strict}}\): Balanced dynamic sparse training in the strict setting

Different from subsection 6.1, we now consider a strict setting where there is an additional sparsity constraint on the discriminator density in this section, i.e., \(d_{D}\leq d_{\text{max}}<100\%\).

ADAPT\({}_{\text{relax}}\) introduced in the previous section does not necessarily enforce sparsity for the discriminator, which provides less memory/training resources saving for larger generator density ratios. Note that the discriminator does not take advantage of DST to explore the structure of the dense network. Hence, we further present **strict balanced dynamic sparse training (ADAPT\({}_{\text{strict}}\))** in this section. This method allows the discriminator to perform DST in a controlled manner, which can lead to a better exploration of the dense network structure while maintaining the balance between the generator and the discriminator. We explain how ADAPT\({}_{\text{strict}}\) differs from ADAPT\({}_{\text{relax}}\) below:

\(\bullet\)**Capacity increase of the discriminator.** The essential difference lies when the observed BR is higher than \(B_{+}\), which means we need a stronger discriminator. In this case, if the discriminator density is lower than the constraint, i.e., \(d_{D}<d_{\text{max}}\), ADAPT\({}_{\text{strict}}\) will perform just like ADAPT\({}_{\text{relax}}\) to increase the discriminator density. However, if the discriminator is already the maximum density, i.e., \(d_{D}=d_{\text{max}}\), the discriminator will alternatively perform DST as a way of increasing the discriminator capacity (See subsection 5.4 for intuition).

\(\bullet\)**Capacity decrease of the discriminator.** Similar to ADAPT\({}_{\text{relax}}\), when the observed BR is lower than \(B_{-}\), we will decrease the discriminator density.

Hence, ADAPT\({}_{\text{strict}}\) makes the discriminator adaptive both in the density level (through density adjustment) and the parameter level (through DST). The algorithm of ADAPT\({}_{\text{strict}}\) is shown in Appendix C Algorithm 3.

### Experiment setting

**Datasets, architectures, and target sparsity ratios.** We conduct experiments on SNGAN with ResNet architectures on the CIFAR-10 [40] and the STL-10 [11] datasets. We have also conducted experiments with BigGAN [7] on the CIFAR-10 and TinyImageNet dataset (with DiffAug [93]).

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c} \hline \hline Dataset & \multicolumn{4}{c}{**CIFAR-10 (SNGAN)**} & \multicolumn{4}{c}{**STL-10 (SNGAN)**} & \multicolumn{4}{c}{**CIFAR-10 (BigGAN)**} & \multicolumn{4}{c}{**TIFAR-10 (BigGAN)**} \\ \hline Generator density & 10\% & 29\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% \\ \hline (Dense Baseline) & & 10.74 & & & 29.71 & & & & 8.11 & & & & & & & & & & & & \\ \hline STAIC-Balance & 26.75 & 19.04 & 15.05 & 12.24 & 48.18 & 44.67 & 41.73 & 37.68 & 16.98 & 12.81 & 10.33 & 8.47 & 28.78 & 21.67 & 18.86 & 17.51 \\ \hline STATIC-Strong & 26.79 & 19.65 & 14.38 & 11.91 & 52.48 & 43.85 & 42.06 & 37.47 & 23.48 & 14.26 & 11.19 & 8.64 & 31.44 & 22.51 & 18.22 & 18.00 \\ \hline \hline \(\bullet\) SDST-Balance-SEF & 26.23 & 17.99 & 13.21 & 11.79 & 56.41 & 48.58 & 39.93 & 30.37 & 12.41 & 9.87 & 9.13 & **8.01** & 25.39 & 21.30 & 21.80 & 21.20 \\ \(\bullet\) SDST-Strong-SEF & 16.49 & 13.26 & 11.68 & 10.68 & 67.37 & 49.96 & 37.99 & 39.19 & 10.84 & 9.64 & 8.75 & 36.26 & 22.06 & 20.56 & 21.70 & 13.82 \\ \(\bullet\) SDST-Balance-Rigid. & 27.06 & 16.36 & 14.00 & 12.28 & 43.08 & 33.90 & 31.83 & 30.30 & 12.45 & 9.42 & 8.86 & 8.03 & 21.60 & 19.33 & 18.57 & 17.45 \\ \(\bullet\) SDST-Strong-Rigid. & 17.02 & 13.36 & 12.51 & 11.35 & 35.65 & 33.25 & **31.41** & 20.18 & 10.28 & 9.11 & 8.69 & 8.33 & 21.14 & 18.95 & 17.75 & 16.30 \\ \hline \hline ADAPT\({}_{\text{relax}}\) (Ours) & **14.19** & **13.19** & 12.38 & **10.60** & **35.98** & **33.06** & 31.71 & **29.96** & **10.19** & **8.56** & **8.36** & 8.22 & **19.42** & **17.99** & **17.66** & **14.15** \\ \hline \hline \end{tabular}
\end{table}
Table 1: FID (\(\downarrow\)) of different sparse training methods with **no** constraint on the density of the **discriminator**. Best results are in **bold**; second-best results are underlined.

Target density ratios and two practical strategies.We use STATIC and SDST (section 4) as our Appendix A for more experiment details.

**Baseline methods and two practical strategies.** We use STATIC and SDST (section 4) as our baselines. Note that in real-world application scenarios, it is not practical to perform a grid search for a good \(d_{D}\) as in section 4. Hence, we propose two practical strategies to define the constant discriminator density for these baseline methods: (1) balance strategy, where we set the density of the discriminator \(d_{D}\) the same as the density of the generator \(d_{G}\); (2) strong strategy, where we set the density of the discriminator as large as possible, i.e., \(d_{D}=d_{\text{max}}\). For SDST methods, we test both \(\bullet\)SDST-SET and \(\bullet\)SDST-RigL. For a fair comparison, \(d_{\text{max}}\) is set to be 100% and 50% for the relaxed setting and the strict setting, respectively.

For ADAPT, we use the \(\bullet\)RigL version, which grows connections of the generators and discriminators based on gradient magnitude. The gradient information enables two components to react promptly according to the change of each other. Different from the value used in subsection 5.3, we control the balance ratio in the range \([0.45,0.55]\) unless otherwise mentioned to have a slightly stronger discriminator, potentially avoiding training collapse. More details can be found in Appendix B.

### Experiment results

We show the experiment results in Table 1 and Table 2 for the relaxed setting and the strict setting, respectively. We also present the training FLOPs normalized by the dense counterpart for the relaxed setting in Table 3. We defer the results for the strict setting to Appendix F Table 12. We show FID for the CIFAR-10 test set, Inception scores, and comparison with post-hoc pruning baseline in Appendix E. We also show ADAPT BR evolution in Appendix D. We summarize our findings below.

**The strong strategy and the balance strategy for baselines.** Generally, using the strong strategy has some advantages over the balance strategy. Such an observation is most prominent in the CIFAR-10 dataset. For the cases where the balance strategy is better, e.g., SNGAN on the STL-10 dataset, our explanation is that the size difference between generators and discriminators is more significant. Hence, the degree of unbalance is more severe and leads to more detrimental effects.

**Comparison of \(\bullet\)RigL and \(\bullet\)SET for SDST.** We found that \(\bullet\)RigL has an advantage over \(\bullet\)SET when dealing with more sparse generators. Our hypothesis is that gradient information can effectively guide the generator to identify the most crucial connections in such cases. However, this advantage is not as apparent for more dense generators.

**ADAPT\({}_{\text{relax}}\) achieves a good trade-off between performance and computational cost.** Experiments show that ADAPT\({}_{\text{relax}}\) shows promising performance by being best for 13 out of 16 cases. The advantage of ADAPT\({}_{\text{relax}}\) is most prominent for the most difficult case, i.e., \(d_{G}=10\%\). Specifically, it

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline Dataset & \multicolumn{3}{c}{**CIFAR-10 (SNGAN)**} & \multicolumn{3}{c}{**STL-10 (SNGAN)**} & \multicolumn{3}{c}{**CIFAR-10 (BigGAN)**} & \multicolumn{3}{c}{**Tiny ImageNet (BigGAN)**} \\ \hline Generator density & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% \\ \hline (Dense Baseline) & \multicolumn{3}{c}{10.74} & \multicolumn{3}{c}{8.11} & \multicolumn{3}{c}{15.43} \\ \hline STATIC-Balance & 26.75 & 10.64 & 15.05 & 12.58 & 48.18 & 44.67 & 41.73 & 37.68 & 16.98 & 12.81 & 10.33 & 8.47 & 28.78 & 21.67 & 18.86 & 17.51 \\ STATIC-Stonge & 21.73 & 16.69 & 13.48 & 12.58 & 50.36 & 44.06 & 40.73 & 37.68 & 18.91 & 13.43 & 10.84 & 8.47 & 33.01 & 25.93 & 17.90 & 17.51 \\ \hline \hline \(\bullet\)SDST-Balance-SET & 26.23 & 17.79 & 13.21 & **11.79** & 56.24 & 44.51 & 41.23 & 30.80 & 12.41 & 9.87 & 9.13 & 8.01 & 25.39 & 21.30 & 21.80 & 21.20 \\ \(\bullet\)SDST-Stonge-SET & 15.86 & 12.82 & **11.55** & **11.79** & 57.91 & 50.05 & 38.13 & 30.30 & 11.85 & 9.36 & 8.61 & 8.01 & 22.68 & 20.24 & 22.00 & 21.20 \\ \(\bullet\)SDST-Balance-RigL & 27.06 & 16.36 & 14.00 & 12.28 & 43.23 & 33.00 & 13.61 & 30.20 & 12.54 & 9.42 & 8.86 & 8.31 & 21.60 & 19.38 & 15.77 & **14.55** \\ \(\bullet\)SDST-Stonge-RigL & 15.19 & 12.93 & 12.75 & 12.28 & 53.74 & 37.34 & 31.98 & 30.30 & 10.11 & 9.17 & **8.35** & 8.03 & 21.90 & 20.43 & 18.29 & 17.45 \\ \hline \hline ADAPT\({}_{\text{relax}}\) (Ours) & **14.53** & **12.73** & 12.20 & 12.11 & **41.18** & **31.59** & **31.16** & **29.11** & **9.29** & **8.64** & 8.44 & **7.90** & **18.89** & **17.37** & **16.93** & **16.02** \\ \hline \hline \end{tabular}
\end{table}
Table 2: FID (\(\downarrow\)) of different sparse training methods. **The density of the discriminator is constrained to be lower than 50%**. Best results are in **bold**; second-best results are underlined.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline Dataset & \multicolumn{3}{c}{**CIFAR-10 (SNGAN)**} & \multicolumn{3}{c}{**STL-10 (SNGAN)**} & \multicolumn{3}{c}{**CIFAR-10 (BigGAN)**} & \multicolumn{3}{c}{**Tiny ImageNet (BigGAN)**} \\ \hline Generator density & 10\% & 20\% & 30\% & 80\% & 10\% & 20\% & 30\% & 30\% & 10\% & 20\% & 30\% & 80\% & 10\% & 20\% & 30\% & 50\% \\ \hline (Dense Baseline) & 100\% & 2.677 & 10.98 & 26.59 & 47.26 & 27.98 & 47.16 & 93.22 & 73.356 & 9.798 & 10.05 & 26.664 & 40.993 & 23.35 & 44.879 & 60.691 & 93.29 \\ \(\bullet\)SDST-Stage-SET & 8.22\% & 40.064 & 46.73shows 2.3 and 7.1 FID improvements over the second-best methods for the SNGAN on the CIFAR-10 and the STL-10, respectively. Moreover, compared to the competitive baseline methods that use the strong strategy, i.e., \(\bullet\)SDST-Strong-RigL and \(\bullet\)SDST-Strong-SET, ADAPT\({}_{\text{relax}}\) shows great computational cost reduction. For example, it outperforms \(\bullet\)SDST-Strong-RigL on BigGAN (CIFAR-10) with much-reduced training FLOPs (10.39% v.s. 83.97%).

**ADAPT\({}_{\text{strict}}\) shows stable and superior performance.** Similar to ADAPT\({}_{\text{relax}}\), we notice that ADAPT\({}_{\text{strict}}\) also delivers promising results compared to baselines, even with a further constraint on the discriminator. More precisely, among all the cases, ADAPT\({}_{\text{strict}}\) ranks top 2 for all cases, with 13 cases being the best. Moreover, ADAPT\({}_{\text{strict}}\) again shows comparable or better performance compared to \(\bullet\)SDST-Strong-RigL with reduced computational cost.

A more interesting observation is that ADAPT\({}_{\text{strict}}\) sometimes outperforms ADAPT\({}_{\text{relax}}\). We speculate that this phenomenon occurs because changes in density may result in a larger influence on the GAN balance during training compared to DST. Hence, the strict version, whose discriminator density range is smaller, may offer a more consistent performance.

## 7 Conclusion

In this paper, we investigate the use of DST for GANs and find that solely applying DST to the generator does not necessarily enhance the performance of sparse GANs. To address this, we introduce BR to examine the degree of unbalance between the sparse generators and discriminators. We find that applying DST to the generator only benefits the training when the discriminator is comparatively stronger. Additionally, we propose ADAPT, which can dynamically adjust the discriminator at both the parameter and density levels. Our approach shows promising results, and we hope it can aid researchers in better comprehending the interplay between the two components of GAN training and motivate further exploration of sparse training for GANs. However, we must note that we have not yet evaluated our methods on the latest GAN architectures due to computational constraints.

## 8 Acknowledgement

This work utilizes resources supported by the National Science Foundation's Major Research Instrumentation program, grant No.1725729 [39], as well as the University of Illinois at Urbana-Champaign. This work is supported in part by Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2022046); University Development Fund UDF01001491 from the Chinese University of Hong Kong, Shenzhen; Guangdong Key Lab on the Mathematical Foundation of Artificial Intelligence, Department of Science and Technology of Guangdong Province. Prof. NH is in part supported by Air Force Office of Scientific Research (AFOSR) grant FA9550-21-1-0411.

## References

* [1] M. Alizadeh, S. A. Tailor, L. M. Zintgraf, J. van Amersfoort, S. Farquhar, N. D. Lane, and Y. Gal. Prospect pruning: Finding trainable weights at initialization using meta-gradients. _arXiv preprint arXiv:2202.08132_, 2022.
* [2] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pages 214-223. PMLR, 2017.
* [3] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative adversarial nets (gans). In _International Conference on Machine Learning_, pages 224-232. PMLR, 2017.
* [4] Y. Bai, T. Ma, and A. Risteski. Approximability of discriminators implies diversity in gans. _arXiv preprint arXiv:1806.10586_, 2018.
* [5] G. Bellec, D. Kappel, W. Maass, and R. Legenstein. Deep rewiring: Training very sparse deep networks. _arXiv preprint arXiv:1711.05136_, 2017.
* [6] D. Berthelot, T. Schumm, and L. Metz. Began: Boundary equilibrium generative adversarial networks. _arXiv preprint arXiv:1703.10717_, 2017.
* [7] A. Brock, J. Donahue, and K. Simonyan. Large scale gan training for high fidelity natural image synthesis. _arXiv preprint arXiv:1809.11096_, 2018.
* [8] T. Chen, Y. Cheng, Z. Gan, J. Liu, and Z. Wang. Data-efficient gan training beyond (just) augmentations: A lottery ticket perspective. _Advances in Neural Information Processing Systems_, 34:20941-20955, 2021.
* [9] X. Chen, Z. Zhang, Y. Sui, and T. Chen. Gans can play lottery tickets too. _arXiv preprint arXiv:2106.00134_, 2021.
* [10] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8789-8797, 2018.
* [11] A. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* [12] B. Dai, C. Zhu, B. Guo, and D. Wipf. Compressing neural networks using the variational information bottleneck. In _International Conference on Machine Learning_, pages 1135-1144. PMLR, 2018.
* [13] P. de Jorge, A. Sanyal, H. S. Behl, P. H. Torr, G. Rogez, and P. K. Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. _arXiv preprint arXiv:2006.09081_, 2020.
* [14] T. Dettmers and L. Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. _arXiv preprint arXiv:1907.04840_, 2019.
* [15] X. Dong, S. Chen, and S. Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. _Advances in Neural Information Processing Systems_, 30, 2017.
* [16] U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen. Rigging the lottery: Making all tickets winners. In _International Conference on Machine Learning_, pages 2943-2952. PMLR, 2020.
* [17] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* [18] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin. Pruning neural networks at initialization: Why are we missing the mark? In _ICLR_, 2021.
* [19] Y. Fu, W. Chen, H. Wang, H. Li, Y. Lin, and Z. Wang. Autogan-distiller: Searching to compress generative adversarial networks. _arXiv preprint arXiv:2006.08198_, 2020.

* [20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [21] L. Graesser, U. Evci, E. Elsen, and P. S. Castro. The state of sparse training in deep reinforcement learning. In _International Conference on Machine Learning_, pages 7766-7792. PMLR, 2022.
* [22] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of wasserstein gans. _Advances in neural information processing systems_, 30, 2017.
* [23] Y. Guo, A. Yao, and Y. Chen. Dynamic network surgery for efficient dnns. _Advances in neural information processing systems_, 29, 2016.
* [24] H. Ham, T. J. Jun, and D. Kim. Unbalanced gans: Pre-training the generator of generative adversarial network using variational autoencoder. _arXiv preprint arXiv:2002.02112_, 2020.
* [25] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural network. _Advances in neural information processing systems_, 28, 2015.
* [26] B. Hassibi, D. G. Stork, and G. J. Wolff. Optimal brain surgeon and general network pruning. In _IEEE international conference on neural networks_, pages 293-299. IEEE, 1993.
* [27] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [28] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [29] L. Hou, Z. Yuan, L. Huang, H. Shen, X. Cheng, and C. Wang. Slimmable generative adversarial networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 7746-7753, 2021.
* [30] Z. Huang and N. Wang. Data-driven sparse structure selection for deep neural networks. In _Proceedings of the European conference on computer vision (ECCV)_, pages 304-320, 2018.
* [31] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized neural networks: Training neural networks with low precision weights and activations. _The Journal of Machine Learning Research_, 18(1):6869-6898, 2017.
* [32] S. A. Janowsky. Pruning versus clipping in neural networks. _Physical Review A_, 39(12):6600, 1989.
* [33] Q. Jin, J. Ren, O. J. Woodford, J. Wang, G. Yuan, Y. Wang, and S. Tulyakov. Teachers do more than teach: Compressing image-to-image models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13600-13611, 2021.
* [34] N. M. Kalibhat, Y. Balaji, and S. Feizi. Winning lottery tickets in deep generative models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8038-8046, 2021.
* [35] M. Kang and B. Han. Operation-aware soft channel pruning using differentiable masks. In _International Conference on Machine Learning_, pages 5122-5131. PMLR, 2020.
* [36] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. _arXiv preprint arXiv:1710.10196_, 2017.
* [37] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila. Training generative adversarial networks with limited data. _Advances in Neural Information Processing Systems_, 33:12104-12114, 2020.
* [38] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8110-8119, 2020.

* [39] V. Kindratenko, D. Mu, Y. Zhan, J. Maloney, S. H. Hashemi, B. Rabe, K. Xu, R. Campbell, J. Peng, and W. Gropp. Hal: Computer system for scalable deep learning. In _Practice and experience in advanced research computing_, pages 41-48. 2020.
* [40] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [41] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, and A. Farhadi. Soft threshold weight reparameterization for learnable sparsity. In _International Conference on Machine Learning_, pages 5544-5555. PMLR, 2020.
* [42] Y. LeCun, J. Denker, and S. Solla. Optimal brain damage. _Advances in neural information processing systems_, 2, 1989.
* [43] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4681-4690, 2017.
* [44] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11523-11532, 2022.
* [45] N. Lee, T. Ajanthan, S. Gould, and P. H. Torr. A signal propagation perspective for pruning neural networks at initialization. _arXiv preprint arXiv:1906.06307_, 2019.
* [46] N. Lee, T. Ajanthan, and P. H. Torr. Snip: Single-shot network pruning based on connection sensitivity. _arXiv preprint arXiv:1810.02340_, 2018.
* [47] M. Li, J. Lin, Y. Ding, Z. Liu, J.-Y. Zhu, and S. Han. Gan compression: Efficient architectures for interactive conditional gans. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5284-5294, 2020.
* [48] S. Li, J. Wu, X. Xiao, F. Chao, X. Mao, and R. Ji. Revisiting discriminator in gan compression: A generator-discriminator cooperative compression scheme. _arXiv preprint arXiv:2110.14439_, 2021.
* [49] H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. _arXiv preprint arXiv:1806.09055_, 2018.
* [50] J. Liu, Z. Xu, R. Shi, R. C. Cheung, and H. K. So. Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers. _arXiv preprint arXiv:2005.06870_, 2020.
* [51] S. Liu, T. Chen, X. Chen, Z. Atashgahi, L. Yin, H. Kou, L. Shen, M. Pechenizkiy, Z. Wang, and D. C. Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration. _Advances in Neural Information Processing Systems_, 34:9908-9922, 2021.
* [52] S. Liu, D. C. Mocanu, Y. Pei, and M. Pechenizkiy. Selfish sparse rnn training. In _International Conference on Machine Learning_, pages 6893-6904. PMLR, 2021.
* [53] S. Liu, Y. Tian, T. Chen, and L. Shen. Don't be so dense: Sparse-to-sparse gan training without sacrificing performance. _arXiv preprint arXiv:2203.02770_, 2022.
* [54] S. Liu, L. Yin, D. C. Mocanu, and M. Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. In _International Conference on Machine Learning_, pages 6989-7000. PMLR, 2021.
* [55] T. Liu and F. Zenke. Finding trainable sparse networks through neural tangent transfer. In _International Conference on Machine Learning_, pages 6336-6347. PMLR, 2020.
* [56] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efficient convolutional networks through network slimming. In _Proceedings of the IEEE international conference on computer vision_, pages 2736-2744, 2017.

* [57] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell. Rethinking the value of network pruning. _arXiv preprint arXiv:1810.05270_, 2018.
* [58] C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through \(l\)0 regularization. _arXiv preprint arXiv:1712.01312_, 2017.
* [59] J.-H. Luo, J. Wu, and W. Lin. Thinet: A filter level pruning method for deep neural network compression. In _Proceedings of the IEEE international conference on computer vision_, pages 5058-5066, 2017.
* [60] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley. Least squares generative adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2794-2802, 2017.
* [61] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. _arXiv preprint arXiv:1802.05957_, 2018.
* [62] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. _Nature communications_, 9(1):1-12, 2018.
* [63] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz. Importance estimation for neural network pruning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11264-11272, 2019.
* [64] H. Mostafa and X. Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. In _International Conference on Machine Learning_, pages 4646-4655. PMLR, 2019.
* [65] S. M. Patil and C. Dovrolis. Phew: Constructing sparse networks that learn fast and generalize well without training data. In _International Conference on Machine Learning_, pages 8432-8442. PMLR, 2021.
* [66] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. _arXiv preprint arXiv:1511.06434_, 2015.
* [67] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In _European conference on computer vision_, pages 525-542. Springer, 2016.
* [68] A. Renda, J. Frankle, and M. Carbin. Comparing rewinding and fine-tuning in neural network pruning. _arXiv preprint arXiv:2003.02389_, 2020.
* [69] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. _Advances in neural information processing systems_, 29, 2016.
* [70] A. Sauer, K. Schwarz, and A. Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In _ACM SIGGRAPH 2022 conference proceedings_, pages 1-10, 2022.
* [71] P. Savarese, H. Silva, and M. Maire. Winning the lottery with continuous sparsification. _Advances in Neural Information Processing Systems_, 33:11380-11390, 2020.
* [72] H. Shu, Y. Wang, X. Jia, K. Han, H. Chen, C. Xu, Q. Tian, and C. Xu. Co-evolutionary compression for unpaired image translation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3235-3244, 2019.
* [73] S. Srinivas, A. Subramanya, and R. Venkatesh Babu. Training sparse neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition workshops_, pages 138-145, 2017.
* [74] R. Sun, T. Fang, and A. Schwing. Towards a better global loss landscape of gans. _Advances in Neural Information Processing Systems_, 33:10186-10198, 2020.

* [75] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. _Advances in Neural Information Processing Systems_, 33:6377-6389, 2020.
* [76] D. Terjek. Adversarial lipschitz. _arXiv preprint arXiv:1907.05681_, 2019.
* [77] S. Verma and J.-C. Pesquet. Sparsifying networks via subdifferential inclusion. In _International Conference on Machine Learning_, pages 10542-10552. PMLR, 2021.
* [78] C. Wang, G. Zhang, and R. Grosse. Picking winning tickets before training by preserving gradient flow. _arXiv preprint arXiv:2002.07376_, 2020.
* [79] H. Wang, S. Gui, H. Yang, J. Liu, and Z. Wang. Gan slimming: All-in-one gan compression by a unified optimization framework. In _European Conference on Computer Vision_, pages 54-73. Springer, 2020.
* [80] H. Wang, Y. Wang, R. Sun, and B. Li. Global convergence of maml and theory-inspired neural architecture search for few-shot learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9797-9808, 2022.
* [81] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In _Proceedings of the European conference on computer vision (ECCV) workshops_, pages 0-0, 2018.
* [82] Y. Wang, D. Li, and R. Sun. Ntk-sap: Improving neural network pruning by aligning training dynamics. In _The Eleventh International Conference on Learning Representations_, 2022.
* [83] Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan. Low-shot learning from imaginary data. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7278-7286, 2018.
* [84] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks. _Advances in neural information processing systems_, 29, 2016.
* [85] J. Wu, J. Hobbs, and N. Hovakimyan. Hallucination improves the performance of unsupervised visual representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16132-16143, 2023.
* [86] J. Wu, N. Hovakimyan, and J. Hobbs. Genco: An auxiliary generator from contrastive learning for enhanced few-shot learning in remote sensing. _arXiv preprint arXiv:2307.14612_, 2023.
* [87] Y.-L. Wu, H.-H. Shuai, Z.-R. Tam, and H.-Y. Chiu. Gradient normalization for generative adversarial networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6373-6382, 2021.
* [88] X. Xiao, Z. Wang, and S. Rajasekaran. Autoprune: Automatic network pruning by regularizing auxiliary parameters. _Advances in neural information processing systems_, 32, 2019.
* [89] Y. Yaz, C.-S. Foo, S. Winkler, K.-H. Yap, G. Piliouras, V. Chandrasekhar, et al. The unusual effectiveness of averaging in gan training. In _International Conference on Learning Representations_, 2018.
* [90] C. Yu and J. Pool. Self-supervised generative adversarial compression. _Advances in Neural Information Processing Systems_, 33:8235-8246, 2020.
* [91] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han, M. Gao, C.-Y. Lin, and L. S. Davis. Nisp: Pruning networks using neuron importance score propagation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9194-9203, 2018.
* [92] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. In _International conference on machine learning_, pages 7354-7363. PMLR, 2019.
* [93] S. Zhao, Z. Liu, J. Lin, J.-Y. Zhu, and S. Han. Differentiable augmentation for data-efficient gan training. _Advances in Neural Information Processing Systems_, 33:7559-7570, 2020.

* [94] X. Zhou, W. Zhang, H. Xu, and T. Zhang. Effective sparsification of neural networks with global sparsity constraint. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3599-3608, 2021.
* [95] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2223-2232, 2017.
* [96] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. _Advances in neural information processing systems_, 30, 2017.

### Overview of the Appendix

The Appendix is organized as follows:

* Appendix A introduces the general experimental setup.
* Appendix B introduces the details of dynamic sparse training.
* Appendix C shows detailed algorithms, i.e., \(\texttt{DDA}\), \(\texttt{ADAPT}_{\texttt{relax}}\), and \(\texttt{ADAPT}_{\texttt{strict}}\).
* Appendix D shows the BR evolution during training for ADAPT.
* Appendix E shows additional results, including IS and FID of test sets of the main paper.
* Appendix F shows detailed FLOPs comparisons of sparse training methods.

## Appendix A Experimental setup

In this section, we explain the training details used in our experiments. Our code is mainly based on the original code of ITOP [54] and GAN ticket [9].

### Architecture details

We use ResNet-32 [27] for the CIFAR-10 dataset and ResNet-48 for the STL-10 dataset. See Table 4 and Table 5 for detailed architectures. We apply spectral normalization for all fully-connected layers and convolutional layers of the discriminators.

For BigGAN architecture, we use the implementation used in DiffAugment [93].3

Footnote 3: https://github.com/mit-han-lab/data-efficient-gans/tree/master/DiffAugment-biggan-cifar.

### Datasets

We use the training set of CIFAR-10, the unlabeled partition of STL-10, and the training set of TinyImageNet for GAN training. Training images are resized to \(32\times 32\), \(48\times 48\), \(64\times 64\) for CIFAR-10, STL-10, and TinyImageNet datasets, respectively. Augmentation methods for both datasets are random horizontal flip and per-channel normalization.

### Training hyperparameters

**SNGAN on the CIFAR-10 and STL-10 datasets.** We use a learning rate of \(2\times 10^{-4}\) for both generators and discriminators. The discriminator is updated five times for every generator update. We adopt Adam optimizer with \(\beta_{1}=0\) and \(\beta_{2}=0.9\). The batch size of the discriminator and the generator is set to 64 and 128, respectively. Hinge loss is used following [7; 9]. We use exponential moving average (EMA) [89] with \(\beta=0.999\). The generator is trained for a total of 100k iterations.

**BigGAN on the CIFAR-10 dataset.** We use a learning rate of \(2\times 10^{-4}\) for both generators and discriminators. The discriminator is updated four times for every generator update. We adopt Adam optimizer with \(\beta_{1}=0\) and \(\beta_{2}=0.999\). The batch size of both the discriminator and the generator is set to 50. Hinge loss is used following [7; 87]. We use EMA with \(\beta=0.9999\). The generator is trained for a total of 200k iterations.

**BigGAN on the TinyImageNet dataset.** We use DiffAug [93] to augment the input. The learning rate of the discriminator and the generator are set to \(4\times 10^{-4}\) and \(1\times 10^{-4}\), respectively. The discriminator is updated one time for every generator update. We adopt Adam optimizer with \(\beta_{1}=0\) and \(\beta_{2}=0.999\). The batch size of both the discriminator and the generator is set to 256. Hinge loss is used following [7; 87]. We use EMA with \(\beta=0.9999\). The generator is trained for a total of 200k iterations.

### Evaluation metric

**SNGAN on the CIFAR-10 and the STL-10 datasets.** We compute Frechet inception distance (FID) and Inception score (IS) for 50k generated images every 5000 iterations. Best FID and IS arereported. For the CIFAR-10 dataset, we report both FID for the training set and test set, whereas, for the STL-10 dataset, we report the FID of the unlabeled partition.

**BigGAN on the CIFAR-10 and the TinyImageNet dataset.** We compute Frechet inception distance (FID) and Inception score (IS) for 10k generated images every 5000 iterations. Best FID and IS are reported.

## Appendix B Dynamic sparse training details

### How the generator performs DST

In this section, we explain how the generator performs DST below. Note that the generator performs the same for SDST and ADAPT.

**Sparsity distribution at initialization.** Following RigL and ITOP [16; 54], only parameters of fully connected and convolutional layers will be pruned. At initialization, we use the commonly adopted _Erdos-Renyi-Kernel_ (ERK) strategy [16; 14; 54] to allocate higher sparsity to larger layers. Specifically, the sparsity of convolutional layers \(l\) is scaled with \(1-\frac{n^{l-1}+n^{l}+w^{l}+h^{l}}{n^{l}-1^{n}w^{l}+h^{l}}\), where \(n^{l}\) denotes the number of channels of layer \(l\) while \(w^{l}\) and \(h^{l}\) are the widths and the height of the corresponding kernel in that layer. For fully connected layers, _Erdos-Renyi_ (ER) strategy is used, where the sparsity is scaled with \(1-\frac{n^{l-1}+n^{l}}{n^{l}-1^{n}}\).

**Update schedule.** The update schedule controls how many connections are adjusted per DST operation. It can be specified by the number of training iterations between sparse connectivity updates \(\Delta T_{G}\), the initial fraction of connections adjusted \(\gamma\), and decaying schedule \(f_{\text{decay}}(\gamma,T)\) for \(\gamma\).

**Drop and grow.** After \(\Delta T_{G}\) training iterations, we update the mask \(\bm{m}_{G}\) by dropping/pruning \(f_{\text{decay}}(\gamma,T)\left|\bm{\theta}_{G}\right|d_{G}\) number of connections with the lowest magnitude, where \(\left|\bm{\theta}_{G}\right|\), \(d_{G}\) are the number of parameters and target density for the generator, \(f_{\text{decay}}(\gamma,T)\) is the update schedule. Right after the connection drop, we regrow the same amount of connections.

For the growing criterion, we test both random growth \(\bullet\) SET[62; 54] and gradient-based growth \(\bullet\) RigL[16]. Concretely, gradient-based methods find newly-activated connections \(\theta\) with the highest gradient magnitude \(\left|\frac{\partial\mathcal{C}}{\partial\theta}\right|\), while random-based methods explore connections in a random fashion. All the newly-activated connections are set to 0. One thing that should be noticed is that while previous works consider layer-wise connections drop and growth, we grow and drop connections globally as it grants more flexibility to the DST method.

**EMA for sparse GAN.** EMA [89] is well-known for its ability to alleviate the non-convergence of GAN. We also implement EMA for sparse GAN training. Specifically, we zero out the moving average of dropped weights whenever there is a mask change.

### DST hyperparameters for the generator

We use the same hyper-parameters for SDST and ADAPT. The initial \(\gamma\) is set to 0.5, and we use a cosine annealing function \(f_{\text{decay}}\) following RigL and ITOP.

**SNGAN on the CIFAR-10 and the STL-10 datasets.** The connection update frequency of the generator \(\Delta T_{G}\) is set to 500 and 1000 for the CIFAR-10 dataset and STL-10 dataset, respectively.

**BigGAN on the CIFAR-10 and the TinyImageNet dataset.** The connection update frequency of the generator \(\Delta T_{G}\) is set to be 1000.

### Density dynamic adjust (Dda) hyper-parameters

In this section, we provide hyper-parameters used in subsection 5.3. We set \(d_{D}=2000\), \(\Delta T_{D}=0.05\), \([B_{-},B_{+}]=[0.5,0.65]\). Time-averaged BR over 1000 iterations is used as the indicator.

### DST hyperparameters for the discriminator in ADAPT

We use a constant BR interval \([B_{-},B_{+}]=[0.45,0.55]\) for SNGAN experiments and BigGAN on the CIFAR-10 dataset. We set the BR interval \([B_{-},B_{+}]=[0.3,0.4]\) for BigGAN on the TinyImageNetsince it uses DiffAug. Time-averaged BR over 1000 iterations is used as the indicator. Density increment \(\Delta d\) is set to be 0.05, 0.025, and 0.05 for SNGAN (CIFAR-10), SNGAN (STL-10), and BigGAN (CIFAR-10), respectively. We use the same setting used in subsection B.2 for the generator.

**Hyper-parameters for ADAPT\({}_{\text{relax}}\).** The density update frequency of the discriminator \(\Delta T_{D}\) is 1000, 2000, 5000, and 10000 iterations for SNGAN (CIFAR-10), SNGAN (STL-10), BigGAN (CIFAR-10), and BigGAN (TinyImageNet), respectively.

**Hyper-parameters for ADAPT\({}_{\text{strict}}\).** The density/connections update frequency of the discriminator \(\Delta T_{D}\) is 2000, 2000, 5000, and 10000 iterations for SNGAN (CIFAR-10), SNGAN (STL-10), BigGAN (CIFAR-10), and BigGAN (TinyImageNet), respectively.

Note that we compute BR for every iteration to visualize the BR evolution, whereas one should note that such computational cost can be greatly decreased if BR is computed every few iterations.

## Appendix C Algorithms

In this section, we present the detailed algorithms for DDA, ADAPT\({}_{\text{relax}}\), and ADAPT\({}_{\text{strict}}\).

### Dynamic adjust algorithm

We first present DDA in Algorithm 1.

```
0: Generator \(G\), discriminator \(D\), BR upper bound \(B_{+}\) and lower bound \(B_{-}\), DA interval \(\Delta T_{D}\), density increment \(\Delta d\), current training iteration \(t\).
1:if\(t\mod\Delta T_{D}==0\)then
2: Compute time-averaged BR with Equation 3
3:if BR \(>B_{+}\)then
4: Increase the density of discriminator from \(d_{D}\) to \(d_{D}+\Delta d\).
5:elseif BR \(<B_{-}\)then
6: Decrease the density of discriminator from \(d_{D}\) to \(d_{D}-\Delta d\).
7:endif
8:endif ```

**Algorithm 1** Dynamic density adjust (DDA) for the discriminator.

### Relaxed balanced dynamic sparse training algorithm

Details of ADAPT\({}_{\text{relax}}\) algorithm is presented in Algorithm 2.

```
0: Generator \(G\), discriminator \(D\), BR upper bound \(B_{+}\) and lower bound \(B_{-}\), DA interval \(\Delta T_{D}\), density increment \(\Delta d\), current training iteration \(t\).
1:if\(t\mod\Delta T_{D}==0\)then
2: Compute time-averaged BR with Equation 3
3:if BR \(>B_{+}\)then
4: Increase the density of discriminator from \(d_{D}\) to \(d_{D}+\Delta d\).
5:elseif BR \(<B_{-}\)then
6: Decrease the density of discriminator from \(d_{D}\) to \(d_{D}-\Delta d\).
7:endif
8:endif ```

**Algorithm 2** Dynamic density adjust (DDA) for the discriminator.

\begin{table}
\begin{tabular}{c c} \hline \hline (a) Generator & (b) Discriminator \\ \hline \(z\in\mathbb{R}^{128}\sim\mathcal{N}(0,I)\) & image \(x\in[-1,1]^{48\times 48\times 3}\) \\ \hline dense, \(6\times 6\times 512\) & ResBlock down 64 \\ \hline ResBlock up 256 & ResBlock down 128 \\ \hline ResBlock up 256 & ResBlock down 128 \\ \hline ResBlock up 256 & ResBlock down 128 \\ \hline BN, ReLU, \(3\times 3\) conv, Tanh & ReLU 0.1 \\ \hline \multicolumn{2}{c}{Global sum pooling} \\ \hline  & dense \(\to 1\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: ResNet architecture for CIFAR-10.

\begin{table}
\begin{tabular}{c c} \hline \hline (a) Generator & (b) Discriminator \\ \hline \(z\in\mathbb{R}^{128}\sim\mathcal{N}(0,I)\) & image \(x\in[-1,1]^{48\times 48\times 3}\) \\ \hline dense, \(6\times 6\times 512\) & ResBlock down 64 \\ \hline ResBlock up 256 & ResBlock down 128 \\ \hline ResBlock up 128 & ResBlock down 256 \\ \hline ResBlock up 64 & ResBlock down 512 \\ \hline BN, ReLU, \(3\times 3\) conv, Tanh & ResBlock down 1024 \\ \hline  & ReLU 0.1 \\ \hline  & \multicolumn{2}{c}{Global avg pooling} \\ \hline  & dense \(\to 1\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: ResNet architecture for STL-10.

```
0: Generator \(G\), discriminator \(D\), total number of training iterations \(T\), number of training steps for discriminator in each iteration \(N\), discriminator adjustment interval \(\Delta T_{D}\), DST interval for the generator \(\Delta T_{G}\), density increment \(\Delta d\), first upper bound \(B_{+}\) and lower bound \(B_{-}\).
1: Set initial discriminator density \(d_{D}=d_{G}\)
2:for\(t\) in \([1,\cdots,T]\)do
3:for\(n\) in \([1,\cdots,N]\)do
4: Compute the loss of discriminator \(\mathcal{L}_{D}(\boldsymbol{\theta}_{D})\)
5:\(\mathcal{L}_{D}(\boldsymbol{\theta}_{D}).backward()\)
6:endfor
7:if\(t\mod\Delta T_{D}==0\)then
8: Compute the loss of generator \(\mathcal{L}_{G}(\boldsymbol{\theta}_{G})\)
9:\(\mathcal{L}_{G}(\boldsymbol{\theta}_{G}).backward()\)
10: Compute time-averaged BR with Equation 3
11:if\(\text{ BR}>B_{+}\)then
12: Increase the density of discriminator from \(d_{D}\) to \(\min(100\%,d_{D}+\Delta d)\).
13:else if BR \(<B_{-}\)then
14: Decrease the density of discriminator from \(d_{D}\) to \(\max(0\%,d_{D}-\Delta d)\).
15:endif
16:endif
17:if\(t\mod\Delta T_{G}==0\)then
18: Apply DST to \(G\)
19:endif
20:endfor ```

**Algorithm 2** Relaxed balanced dynamic sparse training (ADAPT_relax) for GANs.

```
0: Generator \(G\), discriminator \(D\), total number of training iterations \(T\), number of training steps for discriminator in each iteration \(N\), given maximal density of discriminator \(d_{\text{max}}\), discriminator adjustment interval \(\Delta T_{D}\), DST interval for the generator \(\Delta T_{G}\), density increment \(\Delta d\), target generator density \(d_{G}\), BR upper bound \(B_{+}\) and lower bound \(B_{-}\).
1: Set initial discriminator density \(d_{D}=d_{G}\)
2:for\(t\) in \([1,\cdots,T]\)do
3:for\(n\) in \([1,\cdots,N]\)do
4: Compute the loss of discriminator \(\mathcal{L}_{D}(\boldsymbol{\theta}_{D})\)
5:\(\mathcal{L}_{D}(\boldsymbol{\theta}_{D}).backward()\)
6:endfor
7:if\(t\mod\Delta T_{D}==0\)then
8: Compute the loss of generator \(\mathcal{L}_{G}(\boldsymbol{\theta}_{G})\)
9:\(\mathcal{L}_{G}(\boldsymbol{\theta}_{G}).backward()\)
10: Compute time-averaged BR with Equation 3
11:if\(\text{ BR}>B_{+}\) and \(d_{D}<d_{\text{max}}\)then
12: Increase the density of discriminator from \(d_{D}\) to \(\min(d_{\text{max}},d_{D}+\Delta d)\).
13:else if BR \(>B_{+}\) and \(d_{D}==d_{\text{max}}\)then
14: Apply DST to \(D\)
15:else if BR \(<B_{-}\)then
16: Decrease the density of discriminator from \(d_{D}\) to \(\max(0\%,d_{D}-\Delta d)\).
17:endif
18:endif if
19:if\(t\mod\Delta T_{G}==0\)then
20: Apply DST to \(G\)
21:endif
22:endfor ```

**Algorithm 3** Strict balanced dynamic sparse training (ADAPT_strict) for GANs.

## Appendix D Adapt balance ratio evolution

In this section, we show that ADAPT methods are able to maintain a BR throughout training. We show the time evolution of BR and discriminator density for BigGAN on the CIFAR-10 dataset.

Results of ADAPT_relax and ADAPT_strict are shown in Figure 5 and Figure 6. It clearly illustrates the ability of ADAPT to keep the BR controlled during GAN training.

Figure 5: Balance ratio and discriminator density evolution during training for ADAPT_relax on BigGAN (CIFAR-10). Dashed lines represent BR values of 0.45 and 0.55.

Figure 6: Balance ratio and discriminator density evolution during training for ADAPT_strict on BigGAN (CIFAR-10). Dashed lines represent BR values of 0.45 and 0.55.

## Appendix E More experiment results

### IS and FID for the CIFAR-10 dataset

In this section, we present corresponding IS scores results for Table 1 and Table 2. The results are shown in Table 8 and Table 9, respectively. We also include FID results of CIFAR-10 test set in Table 10.

### Naively applying DST to both the generator and the discriminator

In this section, we follow STU-GAN to compare the baseline where applying DST on both generators and discriminators. We name it DST-bothGD.

We test on SNGAN (CIFAR-10) with \(\Delta T_{D}=1000\), \(\Delta T_{G}=500\), and \(\gamma=0.5\). Note that we use the balance strategy where \(d_{G}=d_{D}\). The reason is that the strong strategy uses a dense discriminator, and it does not make sense to apply DST to a dense network.

We show the results in Table 7. It shows that it generates unstable results and consistenly performs worse than SDST-Strong. So we do not compare such baseline in the main body of the paper.

### Post-hoc pruning baseline

In this section, we compare different sparse training methods with post-hoc magnitude pruning [68] baseline. Magnitude pruning involves first training a dense generator, then pruning its weights globally based on their magnitudes. The pruned generator is then fine-tuned with the dense discriminator. We perform additional fine-tuning for \(50\%\) of the original total iterations. Results are presented in Table 6.

Our experimental results clearly demonstrate the advantages of dynamic sparse training over post-hoc magnitude pruning. The latter typically requires around 150% normalized training FLOPs, while DST methods constantly achieve comparable or better performance with significantly reduced computational cost.

## Appendix F A detailed comparison of training costs

In this section, we include the detailed computational cost of all sparse training methods. More specifically, we take into account the density redistribution over different layers in this section. Also, we make an assumption that the computational overhead introduced by computing BR can be neglected.4

Footnote 4: This is true if we compute BR less frequently.

Here we provide training costs for the **strict** setting in Table 12.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline Dataset & \multicolumn{4}{c}{**CIFAR-10 (SNGAN)**} & \multicolumn{4}{c}{**STL-10 (SNGAN)**} & \multicolumn{4}{c}{**CIFAR-10 (BigGAN)**} \\ \hline Generator density & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% \\ \hline (Dense Baseline) & \multicolumn{4}{c}{10.74} & \multicolumn{4}{c}{29.71} & \multicolumn{4}{c}{8.11} \\ \hline Post-hoc pruning & 20.89 & 14.07 & 12.99 & 11.90 & 57.28 & 37.12 & 31.98 & **29.70** & 15.44 & 10.84 & 9.65 & 8.77 \\ \hline STATIC-Balance & 26.75 & 19.04 & 15.05 & 12.24 & 48.18 & 44.67 & 41.73 & 37.68 & 16.98 & 12.81 & 10.33 & 8.47 \\ STATIC-Strong & 26.79 & 19.65 & 14.38 & 11.91 & 52.48 & 43.85 & 42.06 & 37.47 & 23.48 & 14.26 & 11.19 & 8.64 \\ \hline \hline \(\bullet\) SDST-Balance-SET & 26.23 & 17.79 & 13.21 & 11.79 & 56.41 & 46.58 & 39.93 & 30.37 & 12.41 & 9.87 & 9.13 & **8.01** \\ \(\bullet\) SDST-Strong-SET & 16.49 & 13.36 & **11.68** & 10.68 & 67.37 & 49.96 & 37.99 & 31.08 & 18.94 & 9.64 & 8.75 & 8.36 \\ \(\bullet\) SDST-Balance-RigL & 27.06 & 16.36 & 14.00 & 12.28 & 43.08 & 33.90 & 31.83 & 30.30 & 12.45 & 9.42 & 8.86 & 8.03 \\ \(\bullet\) SDST-Strong-RigL & 17.02 & 13.86 & 12.51 & 11.35 & 53.65 & 33.25 & **31.41** & 30.18 & 10.58 & 9.11 & 8.69 & 8.33 \\ \hline \hline ADAPT\({}_{\text{relax}}\) (Ours) & **14.19** & **13.19** & 12.38 & **10.60** & **35.98** & **33.06** & 31.71 & 29.96 & **10.19** & **8.56** & **8.36** & 8.22 \\ \hline \hline \end{tabular}
\end{table}
Table 6: FID (\(\downarrow\)) of different sparse training methods along with post-hoc pruning baseline **with no constraint on the density of the discriminator**. Best results are in **bold**; second-best results are underlined.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multicolumn{5}{l}{Maximal discriminator density \(d_{\text{max}}\)} & \multicolumn{3}{c}{**100 \%**} & \multicolumn{3}{c}{**50 \%**} \\ \hline Generator density & 10\% & 20 \% & 30 \% & 50 \% & 10\% & 20 \% & 30 \% & 50 \% \\ \hline \multicolumn{5}{l}{(Dense Baseline)} & \multicolumn{5}{c}{13.32} \\ \hline Static-Balance & 29.56 & 21.79 & 17.80 & 14.94 & 29.56 & 21.79 & 17.80 & 14.94 \\ Static-Strong & 29.50 & 22.45 & 17.12 & 14.58 & 24.62 & 19.43 & 16.32 & 14.94 \\ \hline \multicolumn{5}{l}{\(\bullet\) SDST-Balance-SET} & 28.84 & 20.31 & 15.95 & 14.35 & 28.84 & 20.31 & 15.95 & **14.35** \\ \multicolumn{5}{l}{\(\bullet\) SDST-Strong-SET} & 19.16 & 16.12 & **14.45** & 13.50 & 18.38 & **15.33** & **14.78** & **14.35** \\ \multicolumn{5}{l}{\(\bullet\) SDST-Balance-RigL} & 29.77 & 19.02 & 16.68 & 15.05 & 29.77 & 19.02 & 16.68 & 15.05 \\ \multicolumn{5}{l}{\(\bullet\) SDST-Strong-RigL} & 19.72 & 16.50 & 15.20 & 14.09 & 17.92 & 15.51 & 15.52 & 15.05 \\ \hline \hline \multicolumn{5}{l}{\(\Delta\)ADPT\({}_{\text{relax}}\) (Ours)} & **16.82** & **15.85** & 15.14 & **13.37** & - & - & - \\ \multicolumn{5}{l}{\(\bullet\) SDAPT\({}_{\text{strict}}\) (Ours)} & - & - & - & **17.19** & 15.57 & 14.92 & 14.80 \\ \hline \hline \end{tabular}
\end{table}
Table 10: FID of test set (\(\downarrow\)) of different sparse training methods on SNGAN (CIFAR-10) dataset. Best results are in **bold**; second-best results are underlined.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{l}{Dataset} & \multicolumn{5}{c}{**SNGAN(CIFAR-10)**} & \multicolumn{5}{c}{**SNGAN(CIFAR-10)**} & \multicolumn{5}{c}{**BigGAN(CIFAR-10)**} & \multicolumn{5}{c}{**BigGAN(CIFAR-10)**} \\ \hline Generator density & 10\% & 20 \% & 30 \% & 50 \% & 10\% & 20 \% & 30 \% & 50 \% & 10\% & 20 \% & 30 \% & 50 \% & 10\% & 20 \% & 30 \% & 50 \% \\ \hline \multicolumn{1}{l}{Generator density} & \multicolumn{5}{c}{10\%} & 20 \% &

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Maximal discriminator density \(d_{\max}\)} & \multicolumn{4}{c}{**100 \%**} & \multicolumn{4}{c}{**50 \%**} \\ \hline Generator density & 10\% & 20 \% & 30 \% & 50 \% & 10\% & 20 \% & 30 \% & 50 \% \\ \hline \multicolumn{1}{c}{(Dense Baseline)} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline Static-Balance & 8.97\% & 17.08\% & 20.25\% & 47.25\% & 27.30\% & 47.14\% & 59.22\% & 73.35\% & 9.79\% & 19.02\% & 19.03\% & 23.25\% & 44.87\% & 60.91\% & 79.29\% \\ Static-Strong & 30.89\% & 33.58\% & 37.17\% & 47.25\% & 70.65\% & 71.48\% & 72.14\% & 73.35\% & 42.66\% & 43.69\% & 45.10\% & 49.01\% & 41.52\% & 55.03\% & 66.29\% & 79.29\% \\ \hline \hline \multicolumn{1}{c}{SDT-Balance-SET} & 9.78\% & 18.91\% & 28.35\% & 48.44\% & 27.55\% & 47.60\% & 60.17\% & 73.88\% & 10.35\% & 20.12\% & 29.96\% & 49.82\% & 21.13\% & 37.06\% & 48.83\% & 65.85\% \\ \multicolumn{1}{c}{*SDT-Storage-SET} & 13.7\% & 35.31\% & 39.35\% & 48.44\% & 39.95\% & 17.97\% & 73.07\% & 73.58\% & 43.25\% & 44.89\% & 46.24\% & 49.32\% & 39.28\% & 47.31\% & 54.11\% & 65.38\% \\ \multicolumn{1}{c}{*SDT-Balance-Rigl} & 10.71\% & 17.43\% & 25.66\% & 43.45\% & 29.51\% & 50.41\% & 63.34\% & 9.92\% & 19.30\% & 28.06\% & 48.34\% & 49.47\% & 48.36\% & 57.26\% & 76.55\% \\ \multicolumn{1}{c}{*SDT-Storage-Rigl} & 31.22\% & 33.93\% & 36.63\% & 43.56\% & 29.95\% & 75.68\% & 76.42\% & 79.03\% & 42.08\% & 44.83\% & 45.37\% & 48.31\% & 43.76\% & 53.71\% & 63.05\% & 76.57\% \\ \hline \hline ADAPT\_rex(Ours) & 24.25\% & 27.55\% & 31.00\% & 37.83\% & 50.91\% & 70.18\% & 75.99\% & 88.68\% & 10.32\% & 23.69\% & 31.54\% & 33.33\% & 34.42\% & 51.68\% & 62.24\% & 77.46\% \\ \hline \hline \end{tabular}
\end{table}
Table 11: FID of test set (\(\downarrow\)) of different sparse training methods on BigGAN (CIFAR-10) dataset. Best results are in **bold**; second-best results are underlined.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{Dataset} & \multicolumn{4}{c}{**CIFAR-10 (SNGAN)**} & \multicolumn{4}{c}{**STL-40 (SNGAN)**} & \multicolumn{4}{c}{**CIFAR-10 (BigGAN)**} & \multicolumn{4}{c}{**Time/ImageNet (BigGAN)**} \\ \hline Generator density & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 30\% & 50\% & 10\% & 20\% & 10\% & 20\% & 10\% & 50\% \\ \hline \multicolumn{1}{c}{(Dense Baseline)} & 100\% & 20 \% & 30 \% & 50 \% & 10\% & 10\% & 20 \% & 30 \% & 50 \% & 10\% & 20\% & 30 \% & 50 \% \\ \hline \multicolumn{1}{c}{(Dense Baseline)} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline Static-Balance & 8.97\% & 17.08\% & 20.25\% & 47.25\% & 27.30\% & 47.14\% & 59.22\% & 73.35\% & 9.79\% & 19.02\% & 28.66\% & 49.03\% & 23.25\% & 44.87\% & 60.91\% & 79.29\% \\ Static-Strong & 30.89\% & 33.58\% & 37.17\% & 47.25\% & 70.65\% & 71.48\% & 72.14\% & 73.35\% & 42.66\% & 43.69\% & 45.10\% & 49.01\% & 41.52\% & 55.03\% & 66.29\% & 79.29\% \\ \hline \hline \multicolumn{1}{c}{*SDT-Balance-SET} & 9.78\% & 18.91\% & 28.35\% & 48.44\% & 27.55\% & 47.60\% & 60.17\% & 73.88\% & 10.35\% & 20.12\% & 29.96\% & 49.82\% & 21.13\% & 37.06\% & 48.83\% & 65.85\% \\ \multicolumn{1}{c}{*SDT-Storage-SET} & 13.7\% & 35.1\% & 39.35\% & 48.44\% & 39.95\% & 17.97\% & 73.07\% & 73.58\% & 43.25\% & 44.89\% & 46.24\% & 49.32\% & 39.28\% & 47.31\% & 54.11\% & 65.38\% \\ \multicolumn{1}{c}{*SDT-Balance-Rigl} & 10.71\% & 17.43\% & 25.66\% & 43.35\% & 29.51\% & 50.41\% & 63.34\% & 9.92\% & 19.30\% & 28.06\% & 48.34\% & 49.47\% & 48.36\% & 57.26\% & 76.55\% \\ \multicolumn{1}{c}{*SDT-Storage-Rigl} & 31.2\% & 33.93\% & 36.63\% & 43.56\% & 29.95\% & 75.68\% & 76.42\% & 79.03\% & 42.08\% & 44.83\% & 45.37\% & 48.31\% & 47.36\% & 53.71\% & 63.05\% & 76.75\% \\ \hline \hline ADAPT\_rex(Ours) & 24.25\% & 27.55\% & 31.00\% & 37.83\% & 50.91\% & 70.18\% & 75.99\% & 88.68\% & 10.32\% & 23.69\% & 31.54\% & 33.33\% & 34.42\% & 51.68\% & 62.24\% & 77.46\% \\ \hline \hline \end