# A Finite-Sample Analysis of Payoff-Based Independent Learning in Zero-Sum Stochastic Games

Zaiwei Chen\({}^{1,*}\), Kaiqing Zhang\({}^{2}\), Eric Mazumdar\({}^{1,\dagger}\), Asuman Ozdaglar\({}^{3}\), Adam Wierman\({}^{1,\ddagger}\)

\({}^{1}\)CMS, Caltech, \({}^{*}\)zchen458@caltech.edu, \({}^{\dagger}\)mazumdar@caltech.edu, \({}^{\dagger}\)adamw@caltech.edu

\({}^{2}\)ECE & ISR, University of Maryland, College Park, kaiqing@umd.edu

\({}^{3}\)EECS, MIT, asuman@mit.edu

###### Abstract

In this work, we study two-player zero-sum stochastic games and develop a variant of the smoothed best-response learning dynamics that combines independent learning dynamics for matrix games with the minimax value iteration for stochastic games. The resulting learning dynamics are payoff-based, convergent, rational, and symmetric between the two players. Our theoretical results present to the best of our knowledge the first last-iterate finite-sample analysis of such independent learning dynamics. To establish the results, we develop a coupled Lyapunov drift approach to capture the evolution of multiple sets of coupled and stochastic iterates, which might be of independent interest.

## 1 Introduction

Recent years have seen remarkable successes in reinforcement learning (RL) in a variety of applications, such as board games [1], autonomous driving [2], robotics [3], and city navigation [4]. A common feature of these applications is that there are _multiple_ decision makers interacting with each other in a common environment. Although empirical successes have shown the potential of multi-agent reinforcement learning (MARL) [5; 6], the training of MARL agents is largely based on heuristics and parameter tuning and, therefore, is not always reliable. In particular, many practical MARL algorithms are directly extended from their single-agent counterparts and lack guarantees because of the adaptive strategies of multiple agents.

A growing literature seeks to provide theoretical insights to substantiate the empirical success of MARL and inform the design of efficient and provably convergent algorithms. Work along these lines can be broadly categorized into work on cooperative MARL where agents seek to reach a common goal [7; 8; 9; 10], and work on competitive MARL where agents have individual (and possibly misaligned) objectives [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22]. While some earlier work focused on providing guarantees on the asymptotic convergence, the more recent ones share an increasing interest in understanding the finite-time/sample behavior. This follows from a line of recent advances in establishing finite-sample guarantees of single-agent RL algorithms, see e.g., [23; 24; 25; 26] and many others.

In this paper, we focus on the benchmark setting of two-player1 zero-sum stochastic games, and develop best-response-type learning dynamics with provable finite-sample guarantees. Crucially, our learning dynamics are independent (requiring no coordination between the agents in learning) and rational (each agent will converge to the best response to the opponent if the opponent plays an (asymptotically) stationary policy [27]), and therefore capture learning in settings with multiple game-theoretic agents. Indeed, learning dynamics with self-interested agents should not enforce information communication or coordination among agents. Furthermore, we focus on the more challenging but practically relevant setting of payoff-based learning, where each agent can onlyobserve their realized payoff at each stage, without observing the policy or even the action taken by the opponent. For learning dynamics with such properties, we establish to the best of our knowledge the first last-iterate finite-sample guarantees. We detail our contributions as follows.

### Contributions

We first consider zero-sum matrix games and provide the last-iterate finite-sample guarantees for the smoothed best-response dynamics proposed in [28]. Then, we extend the algorithmic idea to the setting of stochastic games and develop an algorithm called value iteration with smoothed best-response dynamics (VI-SBR) that also enjoys last-iterate finite-sample convergence.

**Two-Player Zero-Sum Matrix Games.** We start with the smoothed best-response dynamics in [28] and establish the last-iterate finite-sample bounds when using stepsizes of various decay rates. The result implies a sample complexity of \(\mathcal{O}(\epsilon^{-1})\) in terms of the last iterate to find the Nash distribution [29], which is also known as the quantal response equilibrium in the literature [30]. To our knowledge, this is the first last-iterate finite-sample result for best-response learning dynamics that are payoff-based, rational, and symmetric in zero-sum matrix games.

**Two-Player Zero-Sum Stochastic Games.** Building on the algorithmic ideas for matrix games, we develop best-response-type learning dynamics for stochastic games called VI-SBR, which uses a single trajectory of Markovian samples. Our learning dynamics consist of two loops and can be viewed as a combination of the smoothed best-response dynamics for an induced auxiliary matrix game (conducted in the inner loop) and an independent way of performing minimax value iteration (conducted in the outer loop). In particular, in the inner loop, the iterate of the outer loop, i.e., the value function, is fixed, and the players learn the approximate Nash equilibrium of an auxiliary matrix game induced by the value function; then the outer loop is updated by approximating the minimax value iteration updates for the stochastic game, with only local information.

We establish the last-iterate finite-sample bounds for VI-SBR when using both constant stepsizes and diminishing stepsizes of \(\mathcal{O}(1/k)\) decay rate. To the best of our knowledge, this appears to be the first last-iterate finite-sample analysis of best-response-type independent learning dynamics that are convergent and rational for stochastic games. Most existing MARL algorithms are either symmetric across players, but not payoff-based, e.g., [31, 32, 33, 34, 35], or not symmetric and thus not rational, e.g., [36, 37, 38, 14], or do not have last-iterate finite-time/sample guarantees, e.g., [39, 15, 40].

### Challenges & Techniques

The main challenge in analyzing our learning dynamics is that it maintains multiple sets of stochastic iterates and updates them in a coupled manner. To overcome this challenge, we develop a novel _coupled Lyapunov drift approach_. Specifically, we construct a Lyapunov function for each set of the stochastic iterates and establish a Lyapunov drift inequality for each. We then carefully combine the coupled Lyapunov drift inequalities to establish the finite-sample bounds. Although a more detailed analysis is provided in the appendices, we briefly give an overview of the main challenges in analyzing the payoff-based independent learning dynamics in stochastic games, as well as our techniques to overcome them.

**Time-Inhomogeneous Markovian Noise.** The fact that our learning dynamics are payoff-based presents major challenges in handling the stochastic errors in the update. In particular, due to the best-response nature of the dynamics, the behavior policy for sampling becomes time-varying. In fact, the sample trajectory used for learning forms a time-inhomogeneous Markov chain. This makes it challenging to establish finite-sample guarantees, as time-inhomogeneity prevents us from directly exploiting the uniqueness of stationary distributions and the fast mixing of Markov chains. Building on existing work [23, 41, 42, 24], we overcome this challenge by tuning the algorithm parameters (in particular, the stepsizes) and developing a refined conditioning argument.

**Non-Zero-Sum Payoffs Due to Independent Learning.** As illustrated in Section 1.1, the inner loop of VI-SBR is designed to approximately learn the Nash equilibrium of an auxiliary matrix game induced by the value functions for the two players, which we denote by \(v_{t}^{1}\) and \(v_{t}^{2}\), where \(t\) is the iteration index of the outer loop. Importantly, the value functions \(v_{t}^{1}\) and \(v_{t}^{2}\) are maintained individually by players \(1\) and \(2\), and therefore do not necessarily satisfy \(v_{t}^{1}+v_{t}^{2}=0\) due to independent learning. As a result, the auxiliary matrix game from the inner loop does _not_ necessarily 

[MISSING_PAGE_FAIL:3]

Zero-Sum Matrix Games

We begin by considering zero-sum matrix games. This section introduces algorithmic and technical ideas that are important for the setting of stochastic games. For \(i\in\{1,2\}\), let \(\mathcal{A}^{i}\) be the finite action space of player \(i\), and let \(R_{i}\in\mathbb{R}^{|\mathcal{A}^{i}|\times|\mathcal{A}^{-i}|}\) (where \(-i\) denotes the index of player \(i\)'s opponent) be the payoff matrix of player \(i\). Note that in a zero-sum game we have \(R_{1}+R_{2}^{\top}=0\). Since there are finitely many actions for each player, we assume without loss of generality that \(\max_{a^{i},a^{2}}|R_{1}(a^{1},a^{2})|\leq 1\). Furthermore, we denote \(A_{\max}=\max(|\mathcal{A}^{1}|,|\mathcal{A}^{2}|)\).

The decision variables here are the policies \(\pi^{i}\in\Delta(\mathcal{A}^{i})\), \(i\in\{1,2\}\), where \(\Delta(\mathcal{A}^{i})\) denotes the probability simplex supported on \(\mathcal{A}^{i}\). Given a joint policy \((\pi^{1},\pi^{2})\), the expected reward received by player \(i\) is \(\mathbb{E}_{A^{i}\sim\pi^{i},A^{-i}\sim\pi^{-i}}[R_{i}(A^{i},\mathcal{A}^{-i}) ]=(\pi^{i})^{\top}R_{i}\pi^{-i}\), where \(i\in\{1,2\}\). Both players aim to maximize their rewards against their opponents. Unlike in the single-agent setting, since the performance of player \(i\)'s policy depends on its opponent \(-i\)'s policy, there is, in general, no universal optimal policy. Instead, we use the _Nash gap_ and the _regularized Nash gap_ as measurements of the performance of the learning dynamics, as formally defined below.

**Definition 2.1** (Nash Gap in Matrix Games).: Given a joint policy \(\pi=(\pi^{1},\pi^{2})\), the Nash gap \(\text{NG}(\pi^{1},\pi^{2})\) is defined as \(\text{NG}(\pi^{1},\pi^{2})=\sum_{i=1,2}\max_{\hat{\pi}^{i}\in\Delta(\mathcal{A }^{i})}(\hat{\pi}^{i}-\pi^{i})^{\top}R_{i}\pi^{-i}\).

Note that \(\text{NG}(\pi^{1},\pi^{2})=0\) if and only if \((\pi^{1},\pi^{2})\) is in a Nash equilibrium of the matrix game (which may not be unique), in which no player has the incentive to change its policy.

**Definition 2.2** (Regularized Nash Gap in Matrix Games).: Given a joint policy \(\pi=(\pi^{1},\pi^{2})\) and a constant \(\tau>0\), the entropy-regularized Nash gap \(\text{NG}_{\tau}(\pi^{1},\pi^{2})\) is defined as \(\text{NG}_{\tau}(\pi^{1},\pi^{2})=\sum_{i=1,2}\big{\{}\max_{\hat{\pi}^{i}\in \Delta(\mathcal{A}^{i})}(\hat{\pi}^{i}-\pi^{i})^{\top}R_{i}\pi^{-i}+\tau\nu( \hat{\pi}^{i})-\tau\nu(\pi^{i})\big{\}}\), where \(\nu(\cdot)\) is the entropy function defined as \(\nu(\pi^{i})=-\sum_{a^{i}\in\mathcal{A}^{i}}\pi^{i}(a^{i})\log(\pi^{i}(a^{i}))\) for \(i\in\{1,2\}\).

A joint policy \((\pi^{1},\pi^{2})\) satisfying \(\text{NG}_{\tau}(\pi^{1},\pi^{2})=0\) is called the Nash distribution [29] or the quantal response equilibrium [30], which, unlike Nash equilibria, is unique in zero-sum matrix games. As \(\tau\) approaches \(0\), the corresponding Nash distribution approximates a Nash equilibrium [68].

### The Learning Dynamics in Zero-Sum Matrix Games

We start by presenting in Algorithm 1 (from the perspective of player \(i\), where \(i\in\{1,2\}\)) the independent learning dynamics for zero-sum matrix games, which was first proposed in [28]. Given \(\tau>0\), we use \(\sigma_{\tau}:\mathbb{R}^{|\mathcal{A}^{i}|}\mapsto\mathbb{R}^{|\mathcal{A}^{ i}|}\) for the softmax function with temperature \(\tau\), that is, \([\sigma_{\tau}(q^{i})](a^{i})=\exp(q^{i}(a^{i})/\tau)/\sum_{\hat{a}^{i}\in \mathcal{A}^{i}}\exp(q^{i}(\tilde{a}^{i})/\tau)\) for all \(a^{i}\in\mathcal{A}^{i}\), \(q^{i}\in\mathbb{R}^{|\mathcal{A}^{i}|}\), and \(i\in\{1,2\}\).

```
1:Input: Integer \(K\), initializations \(q^{i}_{0}=0\in\mathbb{R}^{|\mathcal{A}^{i}|}\) and \(\pi^{i}_{0}=\text{Unif}(\mathcal{A}^{i})\).
2:for\(k=0,1,\cdots,K-1\)do
3:\(\pi^{i}_{k+1}=\pi^{i}_{k}+\beta_{k}(\sigma_{\tau}(q^{i}_{k})-\pi^{i}_{k})\)
4: Play \(A^{i}_{k}\sim\pi^{i}_{k+1}(\cdot)\) (against \(A^{-i}_{k}\)), and receive reward \(R_{i}(A^{i}_{k},A^{-i}_{k})\)
5:\(q^{i}_{k+1}(a^{i})=q^{i}_{k}(a^{i})+\alpha_{k}\mathds{1}_{\{a^{i}=A^{i}_{k}\}} \left(R_{i}(A^{i}_{k},A^{-i}_{k})-q^{i}_{k}(A^{i}_{k})\right)\) for all \(a^{i}\in\mathcal{A}^{i}\)
6:endfor ```

**Algorithm 1** Independent Learning Dynamics in Zero-Sum Matrix Games

To make this paper self-contained, we next provide a detailed interpretation of Algorithm 1, which also motivates our algorithm for stochastic games in Section 3. At a high level, Algorithm 1 can be viewed as a discrete and smoothed variant of the best-response dynamics, where each player constructs an approximation of the best response to its opponent's policy using the \(q\)-function. The update for the \(q\)-function is in the spirit of the TD-learning algorithm in RL [69].

**The Policy Update.** To understand the update equation for the policies (cf. Algorithm 1 Line \(3\)), consider the discrete version of the smoothed best-response dynamics:

\[\pi^{i}_{k+1}=\pi^{i}_{k}+\beta_{k}(\sigma_{\tau}(R_{i}\pi^{-i}_{k})-\pi^{i}_{k} ),\quad i\in\{1,2\}.\] (1)

In Eq. (1), each player updates its policy \(\pi^{i}_{k}\) incrementally towards the smoothed best response to its opponent's current policy. While the dynamics in Eq. (1) provably converge for zero-sum matrix games, see e.g., [70], implementing it requires player \(i\) to compute \(\sigma_{\tau}(R_{i}\pi_{k}^{-i})\). Note that \(\sigma_{\tau}(R_{i}\pi_{k}^{-i})\) involves the exact knowledge of the opponent's policy and the reward matrix, both of which cannot be accessed in payoff-based independent learning. This leads to the update equation for the \(q\)-functions, which estimate the quantity \(R_{i}\pi_{k}^{-i}\) that is needed for implementing Eq. (1).

**The \(q\)-Function Update.** Suppose for now that we are given a _stationary_ joint policy \(\pi=(\pi^{1},\pi^{2})\). Fix \(i\in\{1,2\}\), the problem of player \(i\) estimating \(R_{i}\pi^{-i}\) can be viewed as a _policy evaluation_ problem, which is usually solved with TD-learning in RL [69]. Specifically, the two players repeatedly play the matrix game with the joint policy \(\pi=(\pi^{1},\pi^{2})\) and produce a sequence of joint actions \(\{(A_{k}^{1},A_{k}^{2})\}_{k\geq 0}\). Then, player \(i\) forms an estimate of \(R_{i}\pi^{-i}\) through the following iterative algorithm:

\[q_{k+1}^{i}(a^{i})=q_{k}^{i}(a^{i})+\alpha_{k}\mathds{1}_{\{a^{i}=A_{k}^{1}\} }(R_{i}(A_{k}^{i},A_{k}^{-i})-q_{k}^{i}(A_{k}^{i})),\quad\forall\ a^{i}\in \mathcal{A}^{i},\] (2)

with an arbitrary initialization \(q_{0}^{i}\in\mathbb{R}^{|\mathcal{A}^{i}|}\), where \(\alpha_{k}>0\) is the stepsize. To understand Eq. (2), suppose that \(q_{k}^{i}\) converges to some \(\bar{q}^{i}\). Then Eq. (2) should be "stationary" at the limit point \(\bar{q}^{i}\) in the sense that \(\mathbb{E}_{A^{i}\sim\pi^{i}(\cdot),A^{i-i}\sim\pi^{-i}(\cdot)}[\mathds{1}_{\{ a^{i}=A^{i}\}}(R_{i}(A^{i},A^{-i})-\bar{q}^{i}(A^{i}))]=0\) for all \(a^{i}\in\mathcal{A}^{i}\), which implies \(\bar{q}^{i}=R_{i}\pi^{-i}\), as desired. Although Eq. (2) is motivated by the case when the joint policy \((\pi^{1},\pi^{2})\) is stationary, the joint policy \(\pi_{k}=(\pi_{k}^{1},\pi_{k}^{2})\) from Eq. (1) is time-varying. A natural approach to address this issue is to make sure that the policies evolve at a _slower_ time-scale compared to that of the \(q\)-functions, so that \(\pi_{k}\) is close to being _stationary_ from the perspectives of \(q_{k}^{i}\).

_Remark_.: In [28], where Algorithm 1 was first proposed, the authors require \(\beta_{k}=o(\alpha_{k})\) to establish the asymptotic convergence, making Algorithm 1 a _two time-scale_ algorithm. In this work, for finite-sample analysis and easier implementation, we update \(\pi_{k}^{i}\) and \(q_{k}^{i}\) on a _single time scale_ with only a multiplicative constant difference in their stepsizes, i.e., \(\beta_{k}=c_{\alpha,\beta}\alpha_{k}\) for some \(c_{\alpha,\beta}\in(0,1)\).

### Finite-Sample Analysis

In this section, we present the finite-sample analysis of Algorithm 1 for the convergence to the Nash distribution [29]. We consider using either constant stepsizes, i.e., \(\alpha_{k}\equiv\alpha\) and \(\beta_{k}\equiv\beta=c_{\alpha,\beta}\alpha\), or diminishing stepsizes with \(\mathcal{O}(1/k)\) decay rate, i.e., \(\alpha_{k}=\alpha/(k+h)\) and \(\beta_{k}=\beta/(k+h)=c_{\alpha,\beta}\alpha/(k+h)\). Let \(\ell_{\tau}=[(A_{\max}-1)\exp(2/\tau)+1]^{-1}\) and \(L_{\tau}=\tau/\ell_{\tau}+A_{\max}^{2}/\tau\). The requirement for choosing the stepsizes is stated in the following condition.

**Condition 2.1**.: When using either constant or diminishing stepsizes, we choose \(\tau\leq 1\), \(\alpha_{0}<\frac{2}{\ell_{\tau}}\), \(\beta_{0}<\min(2,\frac{\tau}{128A_{\max}^{2}})\), and \(c_{\alpha,\beta}=\beta_{k}/\alpha_{k}\leq\min\left(\frac{\tau\ell_{\tau}^{3}} {32},\frac{\ell_{\tau}\pi^{3}}{128A_{\max}^{2}},\frac{2\sqrt{2}}{L_{\tau}^{1/2 }}\right)\).

We next state the finite-sample bounds of Algorithm 1. See Appendix C for the proof.

**Theorem 2.1**.: _Suppose that both players follow the learning dynamics presented in Algorithm 1, and the stepsizes \(\{\alpha_{k}\}\) and \(\{\beta_{k}\}\) are chosen such that Condition 2.1 is satisfied. Then we have the following results._

1. _When using constant stepsizes, i.e.,_ \(\alpha_{k}\equiv\alpha\) _and_ \(\beta_{k}\equiv\beta\)_, we have_ \[\mathbb{E}[\text{NG}_{\tau}(\pi_{K}^{1},\pi_{K}^{2})]\leq B_{\text{in}}\left( 1-\frac{\beta}{4}\right)^{K}+8L_{\tau}\beta+\frac{64\alpha}{c_{\alpha,\beta}},\] _where_ \(B_{\text{in}}=4+2\tau\log(A_{\max})+2A_{\max}\)_._
2. _When using_ \(\alpha_{k}=\alpha/(k+h)\) _and_ \(\beta_{k}=\beta/(k+h)\)_, by choosing_ \(\beta>4\)_, we have_ \[\mathbb{E}[\text{NG}_{\tau}(\pi_{K}^{1},\pi_{K}^{2})]\leq B_{\text{in}}\left( \frac{h}{K+h}\right)^{\beta/4}+\left(64eL_{\tau}\beta+\frac{512e\alpha}{c_{ \alpha,\beta}}\right)\frac{1}{K+h}.\]

The convergence bounds in Theorem 2.1 are qualitatively consistent with the existing results on the finite-sample analysis of general stochastic approximation algorithms [44; 46; 24; 65; 23; 42]. Specifically, when using constant stepsizes, the bound consists of a geometrically decaying term (known as the optimization error) and two constant terms (known as the statistical error) that are proportional to the stepsizes. When using diminishing stepsizes with suitable hyperparameters, both the optimization error and the statistical error achieve an \(\mathcal{O}(1/K)\) rate of convergence.

Although Theorem 2.1 is stated in terms of the expectation of the regularized Nash gap, it implies the mean-square convergence of the policy iterates \((\pi^{1}_{k},\pi^{2}_{k})\). To see this, note that the regularized Nash gap \(\text{NG}_{\tau}(\pi^{1},\pi^{2})\) has a unique minimizer, i.e., the Nash distribution and is denoted by \((\pi^{1}_{*,\tau},\pi^{2}_{*,\tau})\). In addition, fixing \(\pi^{1}\) (respectively, \(\pi^{2}\)), the function \(\text{NG}_{\tau}(\pi^{1},\cdot)\) (respectively, \(\text{NG}_{\tau}(\cdot,\pi^{2})\)) is a \(\tau\)-strongly convex function with respect to \(\pi^{2}\) (respectively, \(\pi^{1}\)). See Lemma D.7 for a proof. Therefore, by the quadratic growth property of strongly convex functions, we have

\[\text{NG}_{\tau}(\pi^{1}_{k},\pi^{2}_{k}) =\text{NG}_{\tau}(\pi^{1}_{k},\pi^{2}_{k})-\text{NG}_{\tau}(\pi^{ 1}_{*,\tau},\pi^{2}_{k})+\text{NG}_{\tau}(\pi^{1}_{*,\tau},\pi^{2}_{k})-\text{ NG}_{\tau}(\pi^{1}_{*,\tau},\pi^{2}_{*,\tau})\] \[\geq\frac{\tau}{2}(\|\pi^{1}_{k}-\pi^{1}_{*,\tau}\|^{2}_{2}+\|\pi^ {2}_{k}-\pi^{2}_{*,\tau}\|^{2}_{2}).\]

As a result, up to a multiplicative constant, the convergence bound for \(\mathbb{E}[\text{NG}_{\tau}(\pi^{1}_{k},\pi^{2}_{k})]\) implies a convergence bound of \(\mathbb{E}[\|\pi^{1}_{k}-\pi^{1}_{*,\tau}\|^{2}_{2}]+\mathbb{E}[\|\pi^{2}_{k}- \pi^{2}_{*,\tau}\|^{2}_{2}]\).

Based on Theorem 2.1, we next derive the sample complexity of Algorithm 1 in the following corollary. See Appendix C.5 for the proof.

**Corollary 2.1.1**.: _Given \(\epsilon>0\), to achieve \(\mathbb{E}[\text{NG}_{\tau}(\pi^{1}_{K},\pi^{2}_{K})]\leq\epsilon\), the sample complexity is \(\mathcal{O}(\epsilon^{-1})\)._

To the best of our knowledge, Theorem 2.1 and Corollary 2.1.1 present the first last-iterate finite-sample analysis of Algorithm 1[28]. Importantly, with only feedback in the form of realized payoffs, we achieve a sample complexity of \(\mathcal{O}(\epsilon^{-1})\) to find the Nash distribution. In general, for smooth and strongly monotone games, the lower bound for the sample complexity of payoff-based or zeroth-order algorithms is \(\mathcal{O}(\epsilon^{-2})\)[71]. We have an improved \(\mathcal{O}(\epsilon^{-1})\) sample complexity due to the bilinear structure of the game (up to a regularizer). In particular, with bandit feedback, the \(q\)-function is constructed as an efficient estimator for the marginalized payoff \(R_{i}\pi^{-i}_{k}\), which can also be interpreted as the gradient. Therefore, Algorithm 1 enjoys the fast \(\mathcal{O}(\epsilon^{-1})\) sample complexity that is comparable to the first-order method [72].

**The Dependence on the Temperature \(\tau\).** Although our finite-sample bound enjoys the \(\mathcal{O}(1/K)\) rate of convergence, the stepsize ratio \(c_{\alpha,\beta}\) appears as \(c_{\alpha,\beta}^{-1}\) in the bound. Since \(c_{\alpha,\beta}=o(\ell_{\tau})\) (cf. Condition 2.1) and \(\ell_{\tau}\) is exponentially small in \(\tau\), the finite-sample bound is actually exponentially large in \(\tau^{-1}\). To illustrate this phenomenon, consider the update equation for the \(q\)-functions (cf. Algorithm 1 Line \(5\)). Observe that the \(q\)-functions are updated asynchronously because only one component (which corresponds to the action taken at time step \(k\)) of the vector-valued \(q^{i}_{k}\) is updated in the \(k\)-th iteration. Suppose that an action \(a^{i}\) is never taken in the algorithm trajectory, which means that \(q^{i}_{k}(a^{i})\) is never updated during learning. Then, in general, we cannot expect the convergence of \(q^{i}_{k}\) or \(\pi^{i}_{k}\). Similarly, suppose that an action is rarely taken in the learning dynamics, we would expect the overall convergence rate to be slow. Therefore, the finite-sample bound should depend on the quantity \(\min_{i\in\{1,2\}}\min_{0\leq k\leq K}\min_{a^{i}}\pi^{i}_{k}(a^{i})\), which captures the exploration abilities of Algorithm 1. Due to the exponential nature of softmax functions, the parameter \(\ell_{\tau}\), which we establish in Lemma C.2 as a lower bound of \(\min_{i\in\{1,2\}}\min_{0\leq k\leq K}\min_{a^{i}}\pi^{i}_{k}(a^{i})\), is also exponentially small in \(\tau\). This eventually leads to the exponential dependence in \(\tau^{-1}\) in the finite-sample bound.

A consequence of having such an exponential factor of \(\tau^{-1}\) in the sample complexity bound is that, if we want to have convergence to a Nash equilibrium rather than to the Nash distribution, the sample complexity can be exponentially large. To see this, note that the following bound holds regarding the Nash gap and the regularized Nash gap:

\[\text{NG}(\pi^{1},\pi^{2})\leq\text{NG}_{\tau}(\pi^{1},\pi^{2})+2\tau\log(A_{ \max}),\quad\forall\;(\pi^{1},\pi^{2}),\] (3)

which, after combining with Theorem 2.1, gives the following corollary. For simplicity of presentation, we only state the result for using constant stepsizes.

**Corollary 2.1.2**.: _Under the same conditions stated in Theorem 2.1 (1), we have_

\[\mathbb{E}[\text{NG}(\pi^{1}_{K},\pi^{2}_{K})]\leq B_{\text{in}}\left(1-\frac{ \beta}{4}\right)^{K}+8L_{\tau}\beta+\frac{64\alpha}{c_{\alpha,\beta}}+2\tau \log(A_{\max}).\] (4)

The last term on the RHS of Eq. (4) can be viewed as the bias due to using smoothed best-response. In view of Eq. (4), to achieve \(\mathbb{E}[\text{NG}(\pi^{1}_{K},\pi^{2}_{K})]\leq\epsilon\), we need \(\tau=\mathcal{O}(\epsilon)\). Since \(c_{\alpha,\beta}\) appears in the denominator of our finite-sample bound and is exponentially small in \(\tau\), the overall sample complexity for the convergence to a Nash equilibrium can be exponentially large in \(\epsilon^{-1}\). In Appendix F, we conduct numerical experiments to investigate the impact of \(\tau\) on this smoothing bias.

In light of the discussion before Corollary 2.1.2, the reason for such an exponentially large sample complexity for finding a Nash equilibrium is due to the limitation of using the softmax policies in smoothed best-response for exploration. We kept the softmax policy without further modification to preserve the "naturalness" of the learning dynamics, which is part of the motivation for studying independent learning in games [73]. A future direction of this work is to remove such an exponential dependence on \(\tau\) by designing an improved exploration strategy.

## 3 Zero-Sum Stochastic Games

Moving to the setting of stochastic games, we consider an infinite-horizon discounted two-player zero-sum stochastic game \(\mathcal{M}=(\mathcal{S},\mathcal{A}^{1},\mathcal{A}^{2},p,R_{1},R_{2},\gamma)\), where \(\mathcal{S}\) is a finite state space, \(\mathcal{A}^{1}\) (respectively, \(\mathcal{A}^{2}\)) is a finite action space for player \(1\) (respectively, player \(2\)), \(p\) represents the transition probabilities, in particular, \(p(s^{\prime}\mid s,a^{1},a^{2})\) is the probability of transitioning to state \(s^{\prime}\) after player \(1\) taking action \(a^{1}\) and player \(2\) taking action \(a^{2}\) simultaneously at state \(s\), \(R_{1}:\mathcal{S}\times\mathcal{A}^{1}\times\mathcal{A}^{2}\mapsto\mathbb{R}\) (respectively, \(R_{2}:\mathcal{S}\times\mathcal{A}^{2}\times\mathcal{A}^{1}\mapsto\mathbb{R}\)) is player \(1\)'s (respectively, player \(2\)'s) reward function, and \(\gamma\in(0,1)\) is the discount factor. Note that we have \(R_{1}(s,a^{1},a^{2})+R_{2}(s,a^{2},a^{1})=0\) for all \((s,a^{1},a^{2})\). We assume without loss of generality that \(\max_{s,a^{1},a^{2}}|R_{1}(s,a^{1},a^{2})|\leq 1\), and denote \(A_{\max}=\max(|\mathcal{A}^{1}|,|\mathcal{A}^{2}|)\).

Given a joint policy \(\pi=(\pi^{1},\pi^{2})\), where \(\pi^{i}:\mathcal{S}\mapsto\Delta(\mathcal{A}^{i})\), \(i\in\{1,2\}\), we define the local \(q\)-function \(q_{\pi}^{i}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\) of player \(i\) as \(q_{\pi}^{i}(s,a^{i})=\mathbb{E}_{\pi}\left[\sum_{c=0}^{\infty}\gamma^{k}R_{i}( S_{k},A_{k}^{i},A_{k}^{-i})\;\right]\,S_{0}=s,\,A_{0}^{i}=a^{i}\right]\) for all \((s,a^{i})\), where we use the notation \(\mathbb{E}_{\pi}[\,\cdot\,]\) to indicate that the actions are chosen according to the joint policy \(\pi\). In addition, we define the global value function \(v_{\pi}^{i}\in\mathbb{R}^{|\mathcal{S}|}\) as \(v_{\pi}^{i}(s)=\mathbb{E}_{A^{i}\sim\pi^{i}(\cdot|s)}[q_{\pi}^{i}(s,A^{i})]\) for all \(s\), and the expected value function \(U^{i}(\pi^{i},\pi^{-i})\in\mathbb{R}\) as \(U^{i}(\pi^{i},\pi^{-i})=\mathbb{E}_{S\sim p_{o}}[v_{\pi}^{i}(S)]\), where \(p_{o}\in\Delta(\mathcal{S})\) is an arbitrary initial distribution on the states. The Nash gap in the case of stochastic games is defined in the following.

**Definition 3.1** (Nash Gap in Zero-Sum Stochastic Games).: Given a joint policy \(\pi=(\pi^{1},\pi^{2})\), the Nash gap \(\text{NG}(\pi^{1},\pi^{2})\) is defined as \(\text{NG}(\pi^{1},\pi^{2})=\sum_{i=1,2}\left(\max_{\hat{\pi}}\,U^{i}(\hat{\pi} ^{i},\pi^{-i})-U^{i}(\pi^{i},\pi^{-i})\right)\).

Similar to the matrix-game setting, a joint policy \(\pi=(\pi^{1},\pi^{2})\) satisfying \(\text{NG}(\pi^{1},\pi^{2})=0\) is called a Nash equilibrium, which may not be unique.

**Additional Notation.** In what follows, we will frequently work with the real vectors in \(\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\), \(\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{-i}|}\), and \(\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}||\mathcal{A}^{-i}|}\), where \(i\in\{1,2\}\). To simplify the notation, for any \(x\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}||\mathcal{A}^{-i}|}\), we use \(x(s)\) to denote the \(|\mathcal{A}^{i}|\times|\mathcal{A}^{-i}|\) matrix with the \((a^{i},a^{-i})\)-th entry being \(x(s,a^{i},a^{-i})\). For any \(y\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\), we use \(y(s)\) to denote the \(|\mathcal{A}^{i}|\)-dimensional vector with its \(a^{i}\)-th entry being \(y(s,a^{i})\).

### Value Iteration with Smoothed Best-Response Dynamics

Our learning dynamics for stochastic games (cf. Algorithm 2) build on the dynamics for matrix games studied in Section 2.1, with the additional incorporation of minimax value iteration, a well-known approach for solving zero-sum stochastic games [74].

**Algorithmic Ideas.** To motivate the learning dynamics, we first introduce the minimax value iteration. For \(i\in\{1,2\}\), let \(\mathcal{T}^{i}:\mathbb{R}^{|\mathcal{S}|}\mapsto\mathbb{R}^{|\mathcal{S}|| \mathcal{A}^{i}||\mathcal{A}^{-i}|}\) be an operator defined as

\[\mathcal{T}^{i}(v)(s,a^{i},a^{-i})=R_{i}(s,a^{i},a^{-i})+\gamma\mathbb{E} \left[v(S_{1})\;\right|\;S_{0}=s,A_{0}^{i}=a^{i},A_{0}^{-i}=a^{-i}\big{]}\]

for all \((s,a^{i},a^{-i})\) and \(v\in\mathbb{R}^{|\mathcal{S}|}\). Given \(X\in\mathbb{R}^{|\mathcal{A}^{i}|\times|\mathcal{A}^{-i}|}\), we define \(\textit{val}^{i}:\mathbb{R}^{|\mathcal{A}^{i}|\times|\mathcal{A}^{-i}|}\mapsto \mathbb{R}\) as

\[\textit{val}^{i}(X)=\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\min_{\mu^{-i}\in \Delta(\mathcal{A}^{-i})}\{(\mu^{i})^{\top}X\mu^{-i}\}=\min_{\mu^{-i}\in\Delta (\mathcal{A}^{-i})}\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\{(\mu^{i})^{\top}X \mu^{-i}\}.\]

Then, the minimax Bellman operator \(\mathcal{B}^{i}:\mathbb{R}^{|\mathcal{S}|}\mapsto\mathbb{R}^{|\mathcal{S}|}\) is defined as \([\mathcal{B}^{i}(v)](s)=\textit{val}^{i}(\mathcal{T}^{i}(v)(s))\) for all \(s\in\mathcal{S}\), where \(\mathcal{T}^{i}(v)(s)\) is an \(|\mathcal{A}^{i}|\times|\mathcal{A}^{-i}|\) matrix according to our notation. It is known that the operator \(\mathcal{B}^{i}(\cdot)\) is a \(\gamma-\) contraction mapping with respect to the \(\ell_{\infty}\)-norm [74], hence it admits a unique fixed point, which we denote by \(v_{*}^{i}\).

A common approach for solving zero-sum stochastic games is to first implement the minimax value iteration \(v^{i}_{t+1}=\mathcal{B}^{i}(v^{i}_{t})\) until (approximate) convergence to \(v^{i}_{*}\)[75], and then solve the matrix game \(\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i })}(\mu^{i})^{\top}\mathcal{T}^{i}(v^{i}_{*})(s)\mu^{-i}\) for each state \(s\) to obtain an (approximate) Nash equilibrium policy. However, implementing this algorithm requires complete knowledge of the underlying transition probabilities. Moreover, since it is an off-policy algorithm, the output is independent of the opponent's policy. Thus, it is not rational by the definition in [27]. To develop a model-free and rational learning dynamics, let us first rewrite the minimax value iteration:

\[v^{i}_{t+1}=\hat{v},\ \text{where}\ \ \hat{v}(s)=\max_{\mu^{i}\in\Delta( \mathcal{A}^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v^{i}_{t})(s)\mu^{-i},\ \forall\ s\in\mathcal{S},\] (5)

where \(\hat{v}\in\mathbb{R}^{|\mathcal{S}|}\) is a dummy variable. In view of Eq. (5), we need to solve a matrix game with payoff matrix \(\mathcal{T}^{i}(v^{i}_{t})(s)\) for each state \(s\) and then update the value of the game to \(v^{i}_{t+1}(s)\). In light of Algorithm 1, we already know how to solve matrix games with independent learning. Thus, what remains to do is to combine Algorithm 1 with value iteration. This leads to Algorithm 2, which is presented from player \(i\)'s perspective, where \(i\in\{1,2\}\).

```
1:Input: Integers \(K\) and \(T\), initializations \(v^{i}_{0}=0\in\mathbb{R}^{|\mathcal{S}|}\), \(q^{i}_{0,0}=0\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\), and \(\pi^{i}_{0,0}(\cdot|s)=\text{Unif}(\mathcal{A}^{i})\) for all \(s\in\mathcal{S}\).
2:for\(t=0,1,\cdots,T-1\)do
3:for\(k=0,1,\cdots,K-1\)do
4:\(\pi^{i}_{t,k+1}(s)=\pi^{i}_{t,k}(s)+\beta_{k}(\sigma_{\tau}(q^{i}_{t,k}(s))- \pi^{i}_{t,k}(s))\) for all \(s\in\mathcal{S}\)
5: Play \(A^{i}_{k}\sim\pi^{i}_{t,k+1}(\cdot|S_{k})\) (against \(A^{-i}_{k}\)) and observe \(S_{k+1}\sim p(\cdot\mid S_{k},A^{i}_{k},A^{-i}_{k})\)
6:\(q^{i}_{t,k+1}(s,a^{i})=q^{i}_{t,k}(s,a^{i})+\alpha_{k}\mathds{1}_{\{(s,a^{i})= (S_{k},A^{i}_{k})\}}(R_{i}(S_{k},A^{i}_{k},A^{-i}_{k})+\gamma v^{i}_{t}(S_{k+1 })-\)\(q^{i}_{t,k}(S_{k},A^{i}_{k}))\) for all \((s,a^{i})\)
7:endfor
8:\(v^{i}_{t+1}(s)=\pi^{i}_{t,K}(s)^{\top}q^{i}_{t,K}(s)\) for all \(s\in\mathcal{S}\)
9:\(S_{0}=S_{K}\), \(q^{i}_{t+1,0}=q^{i}_{t,K}\), and \(\pi^{i}_{t+1,0}=\pi^{i}_{t,K}\)
10:endfor ```

**Algorithm 2** Value Iteration with Smoothed Best-Response (VI-SBR) Dynamics

**Algorithm Details.** For each state \(s\), the inner loop of Algorithm 2 is designed to solve a matrix game with payoff matrices \(\mathcal{T}^{1}(v^{1}_{t})(s)\) and \(\mathcal{T}^{2}(v^{2}_{t})(s)\) for each state \(s\in\mathcal{S}\), which reduces to Algorithm 1 when (1) the stochastic game has only one state, and (2) \(v^{1}_{t}=v^{2}_{t}=0\). However, in general, since \(v^{1}_{t}\) and \(v^{2}_{t}\) are _independently_ maintained by players \(1\) and \(2\), the quantity

\[\mathcal{T}^{1}(v^{1}_{t})(s,a^{1},a^{2})+\mathcal{T}^{2}(v^{2}_{t})(s,a^{2},a ^{1})=\gamma\sum_{s^{\prime}}p(s^{\prime}\mid s,a^{1},a^{2})(v^{1}_{t}(s^{ \prime})+v^{2}_{t}(s^{\prime}))\]

is in general _non-zero_ during learning. As a result, the auxiliary matrix game (with payoff matrices \(\mathcal{T}^{1}(v^{1}_{t})(s)\) and \(\mathcal{T}^{2}(v^{2}_{t})(s)\)) at state \(s\) that the inner loop of Algorithm 2 is designed to solve is not necessarily a zero-sum matrix game, which presents a major challenge in the finite-sample analysis, as illustrated previously in Section 1.2.

The outer loop of Algorithm 2 is an "on-policy" variant of minimax value iteration. To see this, note that, ideally, we would synchronize \(v^{i}_{t+1}(s)\) with \(\pi^{i}_{t,K}(s)^{\top}\mathcal{T}^{i}(v^{i}_{t})(s)\pi^{-i}_{t,K}(s)\), which is an approximation of \([\mathcal{B}^{i}(v)](s)=\text{\it val}^{i}(\mathcal{T}^{i}(v^{i}_{t})(s))\) by design of our inner loop. However, player \(i\) has no access to \(\pi^{-i}_{t,K}\) in independent learning. Fortunately, the \(q\)-function \(q^{i}_{t,K}\) is precisely constructed as an _estimate_ of \(\mathcal{T}^{i}(v^{i}_{t})(s)\pi^{-i}_{t,K}(s)\), as illustrated in Section 2.1, which leads to the outer loop of Algorithm 2. In Algorithm 2 Line \(8\), we set \(S_{0}=S_{K}\) to ensure that the initial state of the next inner loop is the last state of the previous one; hence Algorithm 2 is driven by a single trajectory of Markovian samples.

### Finite-Sample Analysis

We now state our main results, which, to the best of our knowledge, provide the first last-iterate finite-sample bound for best-response-type payoff-based independent learning dynamics in zero-sum stochastic games. Our results are based on the following assumption.

**Assumption 3.1**.: There exists a joint policy \(\pi_{b}=(\pi_{b}^{1},\pi_{b}^{2})\) such that the Markov chain \(\{S_{k}\}_{k\geq 0}\) induced by \(\pi_{b}\) is irreducible and aperiodic.

One challenge in our finite-sample analysis is that the behavior policies used for taking the actions are time-varying, due to the best-response nature of the dynamics. Most, if not all, existing finite-sample guarantees of RL algorithms under time-varying behavior policies assume that the induced Markov chain of any policy, or any policy encountered along the algorithm trajectory, is uniformly geometrically ergodic [78, 79, 77, 59, 78, 79, 80]. Assumption 3.1 is weaker, since it assumes only the existence of one policy that induces an irreducible and aperiodic Markov chain.

We consider using either constant stepsizes, i.e., \(\alpha_{k}\equiv\alpha\) and \(\beta_{k}\equiv\beta=c_{\alpha,\beta}\beta\), or diminishing stepsizes of \(\mathcal{O}(1/k)\) decay rate, i.e., \(\alpha_{k}=\alpha/(k+h)\) and \(\beta_{k}=\beta/(k+h)=c_{\alpha,\beta}\alpha/(k+h)\), where \(c_{\alpha,\beta}\in(0,1)\) is the stepsize ratio. In the stochastic-game setting, we redefine \(\ell_{\tau}=[1+(A_{\max}-1)\exp(2/[(1-\gamma)\tau])]^{-1}\), which, analogous to the matrix-game setting, is a uniform lower bound on the entries of the policies generated by Algorithm 2 (cf. Lemma D.1). We next state our requirement for choosing the stepsizes.

**Condition 3.1**.: When using either constant or diminishing stepsizes, we choose \(\tau\leq 1/(1-\gamma)\) and the stepsize ratio \(c_{\alpha,\beta}\) to satisfy \(c_{\alpha,\beta}\leq\min\big{(}\frac{1}{60L_{p}|\mathcal{S}|A_{\max}},\frac{c_ {\tau}\tau(1-\gamma)^{2}}{34|\mathcal{S}|A_{\max}^{2}},\frac{c_{\tau}\ell_{ \tau}^{2}\tau^{3}(1-\gamma)^{2}}{144A_{\max}^{2}}\big{)}\), where \(c_{\tau}\propto\ell_{\tau}\) and \(L_{p}>0\) are defined in Appendix B.3. In addition, when using \(\alpha_{k}\equiv\alpha\) and \(\beta_{k}\equiv\beta\), we require 2\(\alpha<1/c_{\tau}\) and \(\beta<1\), and when using \(\alpha_{k}=\alpha/(k+h)\) and \(\beta_{k}=\beta/(k+h)\), we require 2\(\beta=4\), \(\alpha>1/c_{\tau}\), and \(h>1\) such that \(\alpha_{0}<1/c_{\tau}\) and \(\beta_{0}<1\).

Footnote 2: The proof works as long as \(\beta>2\). We here use \(\beta=4\) to simplify the statement of the results.

We next state the finite-sample bound of Algorithm 2. For simplicity of presentation, we use \(a\lesssim b\) to mean that there exists an _absolute_ constant \(c>0\) such that \(a\leq bc\).

**Theorem 3.1**.: _Suppose that both players follow Algorithm 2, Assumption 3.1 is satisfied, and the stepsizes \(\{\alpha_{k}\}\) and \(\{\beta_{k}\}\) satisfy Condition 3.1. Then, we have the following results._

1. _When using constant stepsizes, there exists_ \(z_{\beta}=\mathcal{O}(\log(1/\beta))\) _such that the following inequality holds as long as_ \(K\geq z_{\beta}\)_:_ \[\mathbb{E}[\text{NG}(\pi_{T,K}^{1},\pi_{T,K}^{2})]\lesssim \underbrace{\frac{A_{\max}^{2}T}{\tau(1-\gamma)^{3}}\left(\frac{1+ \gamma}{2}\right)^{T-1}}_{:=\mathcal{E}_{1}}+\underbrace{\frac{A_{\max}^{2}L_{ \text{in}}(K-z_{\beta})^{1/2}}{\tau(1-\gamma)^{4}}\left(1-\frac{\beta}{2} \right)^{\frac{K-z_{\beta}-1}{2}}}_{:=\mathcal{E}_{2}}\] \[+\underbrace{\frac{|\mathcal{S}|A_{\max}}{(1-\gamma)^{4}c_{\alpha,\beta}}z_{\beta}^{2}\alpha^{1/2}}_{:=\mathcal{E}_{3}}+\underbrace{\frac{ \tau\log(A_{\max})}{(1-\gamma)^{2}}}_{:=\mathcal{E}_{4}},\] _where_ \(L_{\text{in}}=\frac{4}{(1-\gamma)}+2\tau\log(A_{\max})+\frac{8|\mathcal{S}|A _{\max}}{(1-\gamma)^{2}}\)_._
2. _When using_ \(\alpha_{k}=\alpha/(k+h)\) _and_ \(\beta_{k}=\beta/(k+h)\)_, there exists_ \(k_{0}>0\) _such that the following inequality holds as long as_ \(K\geq k_{0}\)_:_ \[\mathbb{E}[\text{NG}(\pi_{T,K}^{1},\pi_{T,K}^{2})]\lesssim\frac{A_{\max}^{2}T} {\tau(1-\gamma)^{3}}\left(\frac{1+\gamma}{2}\right)^{T-1}+\frac{L_{\text{in}}| \mathcal{S}|A_{\max}z_{K}^{2}\alpha_{K}^{1/2}}{(1-\gamma)^{4}\alpha_{k_{0}}^{1 /2}c_{\alpha,\beta}}+\frac{\tau\log(A_{\max})}{(1-\gamma)^{2}},\] _where_ \(z_{K}=\mathcal{O}(\log(K))\)_._

_Remark_.: Analogous to [29, 15], our learning dynamics are symmetric between the two players in the sense that there is no time-scale separation between the two players, that is, they both implement the algorithm with the same stepsizes.

A detailed proof sketch of Theorem 3.1 is provided in Appendix B and the complete proof is provided in Appendix D. Next, we discuss the result in Theorem 3.1 (1). The bound in Theorem 3.1 (1) involves a value iteration error term \(\mathcal{E}_{1}\), an optimization error term \(\mathcal{E}_{2}\), a statistical error term \(\mathcal{E}_{3}\), and a smoothing bias term \(\mathcal{E}_{4}\) due to the use of smoothed best-response in the learning dynamics. Note that \(\mathcal{E}_{1}\) would be the only error term if we were able to perform minimax value iteration to solvethe game. Since minimax value iteration converges geometrically, the term \(\mathcal{E}_{1}\) also goes to zero at a geometric rate. Notably, the terms \(\mathcal{E}_{2}\) and \(\mathcal{E}_{3}\) are orderwise larger compared to their matrix-game counterparts, see Corollary 2.1.2. Intuitively, the reason is that the induced auxiliary matrix game (with payoff matrices \(\mathcal{T}^{1}(v_{t}^{1})(s)\) and \(\mathcal{T}^{2}(v_{t}^{2})(s)\)) that the inner loop of Algorithm 2 aims at solving does not necessarily have a zero-sum structure (see the discussion in Section 3.1 after Algorithm 2). Consequently, the error due to such a "non-zero-sum" structure propagates through the algorithm and eventually undermines the convergence bound.

Recall that in the matrix game setting, we proved convergence to the Nash distribution (or the Nash equilibrium of the entropy-regularized matrix game). In the stochastic-game setting, we do not have convergence to the Nash equilibrium of the entropy-regularized stochastic game. The main reason is that, in order to have such a convergence, our outer loop should be designed to approximate the _entropy-regularized_ minimax value iteration rather than the vanilla minimax value iteration as in Algorithm 2 Line \(8\). However, in the payoff-based setting, since each player does not even observe the actions of their opponent, it is unclear how to construct an estimator of the entropy function of the opponent's policy, which is an interesting future direction to investigate.

Although the transient terms in Theorem 3.1 enjoy a desirable rate of convergence (e.g., geometric in \(T\) and \(\widetilde{\mathcal{O}}(1/K^{1/2})\) in \(K\)), the stepsize ratio \(c_{\alpha,\beta}\) (which is exponentially small in \(\tau\)) appears as \(c_{\alpha,\beta}^{-1}\) in the bound; see Theorem 3.1. Therefore, due to the presence of the smoothing bias (i.e., the term \(\mathcal{E}_{4}\) on the RHS of the bound in Theorem 3.1 (1)), to achieve \(\mathbb{E}[\text{NG}(\pi_{T,K}^{1},\pi_{T,K}^{2})]\leq\epsilon\), the overall sample complexity can also be exponentially large in \(\epsilon^{-1}\). This is analogous to Corollary 2.1.2 for zero-sum matrix games. As illustrated in detail in Section 2, the reason here is due to the exploration limitation of using softmax as a means for smoothed best response, which we kept without further modification to preserve the naturalness of the learning dynamics. Removing such exponential factors by developing improved exploration strategies is an immediate future direction.

Finally, we consider the case where the opponent of player \(i\) (where \(i\in\{1,2\}\)) plays the game with a stationary policy and provide a finite-sample bound for player \(i\) to find the best response.

**Corollary 3.1.1**.: _[Rationality3] Given \(i\in\{1,2\}\), suppose that player \(i\) follows the learning dynamics presented in Algorithm 2, but its opponent player \(-i\) follows a stationary policy, denoted by \(\pi^{-i}\). Then, we have \(\max_{\tilde{\pi}^{i}}U^{i}(\hat{\pi}^{i},\pi^{-i})-\mathbb{E}[U^{i}(\pi_{T,K }^{i},\pi^{-i})]\leq\tilde{\mathcal{O}}\big{(}\omega_{1}T\left(\frac{\gamma+1} {2}\right)^{T}+\frac{\omega_{2}^{2}}{K^{1/2}}+\tau\big{)}\), where \(\omega_{1}\) and \(\omega_{2}\) are constants that are exponential in \(\tau^{-1}\), but polynomial in \(|\mathcal{S}|\), \(A_{\max}\), and \(1/(1-\gamma)\)._

Intuitively, the reason that our algorithm is rational is that it performs an _on-policy_ update in RL. In contrast to an off-policy update, where the behavior policy can be arbitrarily different from the policy being generated during learning (such as in \(Q\)-learning [81]), in the on-policy update for games, each player is actually playing with the policy that is moving towards the best response to its opponent. As a result, when the opponent's policy is stationary, it reduces to a single-agent problem and the player naturally finds the best response (also up to a smoothing bias). This is an advantage of using symmetric and independent learning dynamics. One challenge of analyzing such on-policy learning dynamics is that the behavior policy is time-varying.

Footnote 3: According to the definition in [27], a dynamics being rational means that the player following this dynamics will converge to the best response to its opponent when the opponent uses an _asymptotically_ stationary policy. Since we are performing finite-sample analysis, we assume the opponent’s policy _is_ stationary, because otherwise, the convergence rate (which may be arbitrary) of the opponent’s policy will also impact the bound.

## 4 Conclusion and Future Directions

In this work, we consider payoff-based independent learning for zero-sum matrix games and stochastic games. In both settings, we establish the last-iterate finite-sample guarantees. Our approach, i.e., the coupled Lyapunov drift argument, provides a number of tools that are likely to be of interest more broadly for dealing with iterative algorithms with multiple sets of coupled and stochastic iterates.

**Limitations and Future Directions.** As mentioned before Corollary 2.1 and after Theorem 3.1, the convergence bounds involve constants that are exponential in \(\tau^{-1}\), which arise due to the use of the smoothed best response to preserve the naturalness of the learning dynamics. An immediate future direction of this work is to remove such exponential factors by designing better exploration strategies. In the long term, we are interested to see if the algorithmic ideas and the analysis techniques developed in this work can be used to study other classes of games beyond zero-sum stochastic games.

## Acknowledgement

The authors would like to thank the anonymous reviewers for the helpful feedback, especially for pointing out several related references. ZC acknowledges support from the PIMCO Postdoctoral Fellowship. KZ acknowledges support from the Northrop Grumman - Maryland Seed Grant Program. EM acknowledges support from NSF CAREER Award 2240110. AW acknowledges support from NSF Grants CNS-2146814, CPS-2136197, CNS-2106403, and NGSDI-2105648.

## References

* S. Shalev-Shwartz, S. Shammah, and A. Shashua (2016) Safe, multi-agent, reinforcement learning for autonomous driving. Preprint arXiv:1610.03295. Cited by: SS1.
* S. Gu, J. Grudzien Kuba, Y. Chen, Y. Du, L. Yang, A. Knoll, and Y. Yang (2023) Safe multi-agent reinforcement learning for multi-robot control. Artificial Intelligence319, pp. 103905. Cited by: SS1.
* P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Anderson, D. Teplyashin, K. Simonyan, A. Zisserman, R. Hadsell, et al. (2018) Learning to navigate in cities without a map. Advances in neural information processing systems31. Cited by: SS1.
* L. Busoniu, R. Babuska, B. De Schutter, et al. (2008) A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C38 (2), pp. 156-172. Cited by: SS1.
* K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar (2018) Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning, pp. 5867-5876. Cited by: SS1.
* G. Arslan and S. Yuksel (2017) Decentralized \(Q\)-learning for stochastic teams and games. IEEE Transactions on Automatic Control62 (4), pp. 1545-1558. Cited by: SS1.
* K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar (2018) Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning, pp. 5867-5876. Cited by: SS1.
* G. Qu, A. Wierman, and N. Li (2020) Scalable reinforcement learning of localized policies for multi-agent networked systems. In Learning for Dynamics and Control, pp. 256-266. Cited by: SS1.
* Y. Zhang, G. Qu, P. Xu, Y. Lin, Z. Chen, and A. Wierman (2023) Global convergence of localized policy iteration in networked multi-agent reinforcement learning. Proceedings of the ACM on Measurement and Analysis of Computing Systems7 (1), pp. 1-51. Cited by: SS1.
* M. L. Littman (1994) Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the Eleventh International Conference on International Conference on Machine Learning, ICML'94, San Francisco, CA, USA, pp. 157-163. Cited by: SS1.
* M. L. Littman (2001) Friend-or-foe \(Q\)-learning in general-sum games. In International Conference on Machine Learning, Vol. 1, pp. 322-328. Cited by: SS1.
* J. Hu and M. P. Wellman (2003) Nash Q-learning for general-sum stochastic games. Journal of Machine Learning Research4 (Nov), pp. 1039-1069. Cited by: SS1.
* C. Daskalakis, D. J. Foster, and N. Golowich (2020) Independent policy gradient methods for competitive reinforcement learning. Advances in neural information processing systems33, pp. 5527-5540. Cited by: SS1.
** [15] Muhammed Sayin, Kaiqing Zhang, David Leslie, Tamer Basar, and Asuman Ozdaglar. Decentralized \(Q\)-learning in zero-sum Markov games. _Advances in Neural Information Processing Systems_, 34:18320-18334, 2021.
* [16] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In _International conference on machine learning_, pages 551-560. PMLR, 2020.
* [17] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move Markov games using function approximation and correlated equilibrium. In _Conference on Learning Theory_, pages 3674-3682. PMLR, 2020.
* [18] Runyu Zhang, Zhaolin Ren, and Na Li. Gradient play in stochastic games: Stationary points, convergence, and sample complexity. _Preprint arXiv:2106.00198_, 2021.
* [19] Dongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, and Mihailo Jovanovic. Independent policy gradient for large-scale markov potential games: Sharper rates, function approximation, and game-agnostic convergence. In _International Conference on Machine Learning_, pages 5166-5220. PMLR, 2022.
* [20] Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In _International Conference on Machine Learning_, pages 7001-7010. PMLR, 2021.
* [21] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning-A simple, efficient, decentralized algorithm for multiagent RL. In _ICLR 2022 Workshop on Gamification and Multiagent Solutions_, 2022.
* [22] Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The complexity of Markov equilibrium in stochastic games. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 4180-4234. PMLR, 2023.
* [23] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite-time analysis of temporal difference learning with linear function approximation. In _Conference on learning theory_, pages 1691-1692. PMLR, 2018.
* [24] R Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD learning. In _Conference on Learning Theory_, pages 2803-2830, 2019.
* [25] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous Q-learning: Sharper analysis and variance reduction. _IEEE Transactions on Information Theory_, 68(1):448-473, 2021.
* [26] Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample analysis of contractive stochastic approximation using smooth convex envelopes. _Advances in Neural Information Processing Systems_, 33, 2020.
* [27] Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games. In _International Joint Conference on Artificial Intelligence_, volume 17, pages 1021-1026, 2001.
* [28] David S Leslie and Edmund J Collins. Convergent multiple-timescales reinforcement learning algorithms in normal form games. _The Annals of Applied Probability_, 13(4):1231-1251, 2003.
* [29] David S Leslie and Edmund J Collins. Individual \(Q\)-learning in normal form games. _SIAM Journal on Control and Optimization_, 44(2):495-514, 2005.
* [30] Richard D McKelvey and Thomas R Palfrey. Quantal response equilibria for normal form games. _Games and economic behavior_, 10(1):6-38, 1995.
* [31] Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. _Advances in Neural Information Processing Systems_, 34:27952-27964, 2021.
* [32] Shicong Cen, Yuejie Chi, Simon Shaolei Du, and Lin Xiao. Faster last-iterate convergence of policy optimization in zero-sum Markov games. In _The Eleventh International Conference on Learning Representations_, 2023.

* [33] Runyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, and Yu Bai. Policy optimization for Markov games: Unified framework and faster convergence. _Advances in Neural Information Processing Systems_, 35:21886-21899, 2022.
* [34] Sihan Zeng, Thinh Doan, and Justin Romberg. Regularized gradient descent ascent for two-player zero-sum Markov games. _Advances in Neural Information Processing Systems_, 35:34546-34558, 2022.
* [35] Liad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, and Yishay Mansour. Regret minimization and convergence to equilibria in general-sum Markov games. In _International Conference on Machine Learning_, pages 9343-9373. PMLR, 2023.
* [36] Yulai Zhao, Yuandong Tian, Jason Lee, and Simon Du. Provably efficient policy optimization for two-player zero-sum Markov games. In _International Conference on Artificial Intelligence and Statistics_, pages 2736-2761. PMLR, 2022.
* [37] Kaiqing Zhang, Xiangyuan Zhang, Bin Hu, and Tamer Basar. Derivative-free policy optimization for linear risk-sensitive and robust control design: Implicit regularization and sample complexity. _Advances in Neural Information Processing Systems_, 34:2949-2964, 2021.
* [38] Ahmet Alacaoglu, Luca Viano, Niao He, and Volkan Cevher. A natural actor-critic framework for zero-sum Markov games. In _International Conference on Machine Learning_, pages 307-366. PMLR, 2022.
* [39] David S Leslie, Steven Perkins, and Zibo Xu. Best-response dynamics in zero-sum stochastic games. _Journal of Economic Theory_, 189:105095, 2020.
* [40] Lucas Baudin and Rida Laraki. Smooth fictitious play in stochastic games with perturbed payoffs and unknown transitions. _Advances in Neural Information Processing Systems_, 35:20243-20256, 2022.
* [41] Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear function approximation. In _Advances in Neural Information Processing Systems_, pages 8668-8678, 2019.
* [42] Shangtong Zhang, R Tachet, and Romain Laroche. Global optimality and finite sample analysis of softmax off-policy actor critic under state distribution mismatch. _Journal of Machine Learning Research_, 23(343):1-91, 2022.
* [43] Muhammed O Sayin, Francesca Parise, and Asuman Ozdaglar. Fictitious play in zero-sum stochastic games. _SIAM Journal on Control and Optimization_, 60(4):2095-2114, 2022.
* [44] Guanghui Lan. _First-order and Stochastic Optimization Methods for Machine Learning_. Springer, 2020.
* [45] Zaiwei Chen, Sheng Zhang, Thinh T. Doan, John-Paul Clarke, and Siva Theja Maguluri. Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications in Reinforcement Learning. _Preprint arXiv:1905.11425_, 2019.
* [46] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _Siam Review_, 60(2):223-311, 2018.
* [47] George W Brown. Iterative solution of games by fictitious play. _Activity Analysis of Production and Allocation_, 13(1):374-376, 1951.
* [48] Julia Robinson. An iterative method of solving a game. _Annals of Mathematics_, pages 296-301, 1951.
* [49] D. Fudenberg and D. Kreps. Learning mixed equilibria. _Games and Economic Behavior_, 5:320-367, 1993.
* [50] Josef Hofbauer and William H Sandholm. On the global convergence of stochastic fictitious play. _Econometrica_, 70(6):2265-2294, 2002.

* [51] James Hannan. Approximation to Bayes risk in repeated play. _Contributions to the Theory of Games_, 3:97-139, 1957.
* [52] Drew Fudenberg and David K Levine. Consistency and cautious fictitious play. _Journal of Economic Dynamics and Control_, 19(5-7):1065-1089, 1995.
* [53] Josef Hofbauer and Ed Hopkins. Learning in perturbed asymmetric games. _Games and Economic Behavior_, 52(1):133-152, 2005.
* [54] Jeff S Shamma and Gurdal Arslan. Unified convergence proofs of continuous-time fictitious play. _IEEE Transactions on Automatic Control_, 49(7):1137-1141, 2004.
* [55] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* [56] Stefanos Leonardos, Will Overman, Ioannis Panageas, and Georgios Piliouras. Global convergence of multi-agent policy gradient in Markov potential games. In _International Conference on Learning Representations_, 2022.
* [57] Sarath Pattathil, Kaiqing Zhang, and Asuman Ozdaglar. Symmetric (optimistic) natural policy gradient for multi-agent learning with parameter convergence. In _International Conference on Artificial Intelligence and Statistics_, pages 5641-5685. PMLR, 2023.
* [58] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive Markov games. In _Conference on Learning Theory_, pages 4259-4299. PMLR, 2021.
* [59] Ziyi Chen, Shaocong Ma, and Yi Zhou. Sample efficient stochastic policy extragradient algorithm for zero-sum Markov game. In _International Conference on Learning Representations_, 2021.
* [60] Muhammed O Sayin, Kaiqing Zhang, and Asuman Ozdaglar. Fictitious play in Markov games with single controller. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 919-936, 2022.
* [61] Lucas Baudin and Rida Laraki. Fictitious play and best-response dynamics in identical interest and zero-sum stochastic games. In _International Conference on Machine Learning_, pages 1664-1690. PMLR, 2022.
* [62] Chinmay Maheshwari, Manxi Wu, Druv Pai, and Shankar Sastry. Independent and decentralized learning in Markov potential games. _Preprint arXiv:2205.14590_, 2022.
* [63] Eyal Even-Dar and Yishay Mansour. Learning rates for \(Q\)-learning. _Journal of Machine Learning Research_, 5(Dec):1-25, 2003.
* [64] Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation and \(Q\)-learning. In _Conference on Learning Theory_, pages 3185-3205. PMLR, 2020.
* [65] Zaiwei Chen, Siva T Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A Lyapunov theory for finite-sample guarantees of Markovian stochastic approximation. _Operations Research_, 2023.
* [66] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. _Mathematical programming_, pages 1-48, 2022.
* [67] Yuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan. The efficacy of pessimism in asynchronous \(Q\)-learning. _IEEE Transactions on Information Theory_, 2023.
* [68] Srihari Govindan, Philip J Reny, Arthur J Robson, et al. A short proof of Harsanyi's purification theorem. _Games and Economic Behavior_, 45(2):369-374, 2003.
* [69] Richard S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3(1):9-44, 1988.

* [70] Josef Hofbauer and Sylvain Sorin. Best response dynamics for continuous zero-sum games. _Discrete and Continuous Dynamical Systems Series B_, 6(1):215, 2006.
* [71] Tianyi Lin, Zhengyuan Zhou, Wenjia Ba, and Jiawei Zhang. Doubly optimal no-regret online learning in strongly monotone games with bandit feedback. _Preprint arXiv:2112.02856_, 2021.
* [72] Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou. Stochastic gradient descent-ascent: Unified theory and new efficient methods. In _International Conference on Artificial Intelligence and Statistics_, pages 172-235. PMLR, 2023.
* [73] Drew Fudenberg and David K Levine. _The Theory of Learning in Games_, volume 2. MIT press, 1998.
* [74] Lloyd S Shapley. Stochastic games. _Proceedings of the National Academy of Sciences_, 39(10):1095-1100, 1953.
* [75] Stefan Banach. Sur les operations dans les ensembles abstraits et leur application aux equations integrales. _Fund. math_, 3(1):133-181, 1922.
* [76] Sajad Khodadadian, Thinh T Doan, Justin Romberg, and Siva Theja Maguluri. Finite sample analysis of two-time-scale natural actor-critic algorithm. _IEEE Transactions on Automatic Control_, 2022.
* [77] Ziyi Chen, Yi Zhou, Rong-Rong Chen, and Shaofeng Zou. Sample and communication-efficient decentralized actor-critic algorithms with finite-time analysis. In _International Conference on Machine Learning_, pages 3794-3834. PMLR, 2022.
* [78] Tengyu Xu and Yingbin Liang. Sample complexity bounds for two timescale value-based reinforcement learning algorithms. In _International Conference on Artificial Intelligence and Statistics_, pages 811-819. PMLR, 2021.
* [79] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic methods. _Advances in Neural Information Processing Systems_, 33:17617-17628, 2020.
* [80] Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of actor-critic algorithm. _IEEE Journal on Selected Areas in Information Theory_, 2(2):652-664, 2021.
* [81] Christopher JCH Watkins and Peter Dayan. \(Q\)-learning. _Machine learning_, 8(3-4):279-292, 1992.
* [82] Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. _Advances in neural information processing systems_, 33:2159-2170, 2020.
* [83] Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum Markov games with a large number of players sample-efficiently? In _International Conference on Learning Representations_, 2022.
* [84] Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for decentralized multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 15007-15049. PMLR, 2022.
* [85] Qiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state space: RL in Markov games with independent linear function approximation. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2651-2652. PMLR, 2023.
* [86] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In _Advances in Neural Information Processing Systems_, pages 4987-4997, 2017.
* [87] Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent RL in zero-sum Markov games with near-optimal sample complexity. _Advances in Neural Information Processing Systems_, 33:1166-1178, 2020.

* [88] Gen Li, Yuejie Chi, Yuting Wei, and Yuxin Chen. Minimax-optimal multi-agent RL in Markov games with a generative model. _Advances in Neural Information Processing Systems_, 35:15353-15367, 2022.
* [89] Qiwen Cui and Simon S Du. When are offline two-player zero-sum Markov games solvable? _Advances in Neural Information Processing Systems_, 35:25779-25791, 2022.
* [90] Qiwen Cui and Simon S Du. Provably efficient offline multi-agent reinforcement learning via strategy-wise bonus. _Advances in Neural Information Processing Systems_, 35:11739-11751, 2022.
* [91] Han Zhong, Wei Xiong, Jiyuan Tan, Liwei Wang, Tong Zhang, Zhaoran Wang, and Zhuoran Yang. Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. In _International Conference on Machine Learning_, pages 27117-27142. PMLR, 2022.
* [92] Zuguang Gao, Qianqian Ma, Tamer Basar, and John R. Birge. Sample complexity of decentralized tabular \(Q\)-learning for stochastic games. _2023 American Control Conference (ACC)_, pages 1098-1103, 2023.
* [93] David A Levin and Yuval Peres. _Markov Chains and Mixing Times_, volume 107. American Mathematical Soc., 2017.
* [94] Dimitri P Bertsekas and John N Tsitsiklis. _Neuro-Dynamic Programming_. Athena Scientific, 1996.
* [95] John M Danskin. _The Theory of Max-Min and Its Application to Weapons Allocation Problems_, volume 5. Springer Science & Business Media, 2012.
* [96] Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. _Preprint arXiv:1704.00805_, 2017.
* [97] Amir Beck. _First-order methods in optimization_, volume 25. SIAM, 2017.
* [98] Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. _10th Innovations in Theoretical Computer Science_, 2019.
* [99] Kenshi Abe, Kaito Ariu, Mitsuki Sakamoto, Kentaro Toyoshima, and Atsushi Iwasaki. Last-iterate convergence with full and noisy feedback in two-player zero-sum games. In _International Conference on Artificial Intelligence and Statistics_, pages 7999-8028. PMLR, 2023.

Extended Related Work

Continued from Section 1.3, we here discuss several other existing works that are relevant.

Recently, there has been an increasing study of MARL with sample efficiency guarantees recently [16; 82; 20; 17; 21; 83; 84; 22; 85]. Most of them focus on the finite-horizon episodic setting with online exploration, and perform regret analysis, which differs from our last-iterate finite-sample analysis under the stochastic approximation paradigm. Additionally, these algorithms are episodic due to the finite-horizon nature of the setting and are not best-response-type independent learning dynamics that are repeatedly run for infinitely long, which can be viewed as a non-equilibrating adaptation process. In fact, the primary focus of this line of work is a _self-play_ setting where all the players can be controlled to perform centralized learning [86; 16; 82; 20; 17]. Beyond the online setting, finite-sample efficiency has also been established for MARL using a generative model [87; 88] or offline datasets [89; 90; 67]. These algorithms tend to be centralized in nature and focus on _equilibrium computation_, instead of performing independent learning.

Finite-sample complexity has also been established for _policy gradient_ methods, a popular RL approach when applied to solving zero-sum stochastic games [14; 36; 37; 38]. However, to ensure convergence, these methods are _asymmetric_ in that the players update their policies at _different_ timescales, e.g., one player updates faster than the other with larger stepsizes, or one player fixes its policy while waiting for the other to update. Such asymmetric policy gradient methods are not independent, as some implicit coordination is required to enable such a timescale separation across agents. This style of implicit coordination is also required for the finite-sample analysis of decentralized learning in certain general-sum stochastic games, e.g., [92], which improves the asymptotic convergence in [7]. In contrast, our learning dynamics only require the update of each player's policy to be slower than the update of their \(q\)-functions, but crucially we do not assume a time-scale separation between the two players, making our learning dynamics symmetric.

## Appendix B Proof Sketch of Theorem 3.1

In this section, we present the key steps and technical ideas used to prove Theorem 3.1. The core challenge here is that Algorithm 2 maintains \(3\) sets of iterates (\(\{q^{i}_{t,k}\}\), \(\{\pi^{i}_{t,k}\}\), and \(\{v^{i}_{t}\}\)), which are coupled. The coupling of their update equations means that it is not possible to separately analyze them. Instead, we develop a coupled Lyapunov drift approach to establish the finite-sample bounds of Algorithm 2. Specifically, we first show that the expected Nash gap can be upper bounded by a sum of properly defined Lyapunov functions, one for each set of the iterates (i.e., the \(v\)-functions, the policies, and the \(q\)-functions). Then, we establish a set of coupled Lyapunov drift inequalities - one for each Lyapunov function. Finally, we decouple the Lyapunov drift inequalities to establish the overall finite-sample bounds. We outline the key steps in the argument below.

To begin with, we show in Lemma D.1 that the \(q\)-functions \(\{q^{i}_{t,k}\}\) and the \(v\)-functions \(\{v^{i}_{t}\}\) generated by Algorithm 2 are uniformly bounded from above in \(\ell_{\infty}\)-norm by \(1/(1-\gamma)\) and the entries of the policies \(\{\pi^{i}_{t,k}\}\) are uniformly bounded below by \(\ell_{\tau}>0\). This result will be frequently used in our analysis. We next introduce the Lyapunov functions we use to analyze Algorithm 2. Specifically, for any \(t,k\geq 0\), let \(\bar{q}^{i}_{t,k}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\) be defined as \(\bar{q}^{i}_{t,k}(s)=\mathcal{T}^{i}(v^{i}_{t})(s)\pi^{-i}_{t,k}(s)\) for all \(s\in\mathcal{S}\). Let

\[\mathcal{L}_{\text{sum}}(t) =\|v^{i}_{t}+v^{2}_{t}\|_{\infty},\quad\mathcal{L}_{v}(t)=\sum_{i =1,2}\|v^{i}_{t}-v^{i}_{\text{s}}\|_{\infty},\] \[\mathcal{L}_{q}(t,k) =\sum_{i=1,2}\sum_{s\in\mathcal{S}}\|q^{i}_{t,k}(s)-\mathcal{T}^ {i}(v^{i}_{t})(s)\pi^{-i}_{t,k}(s)\|^{2}_{2}=\sum_{i=1,2}\|q^{i}_{t,k}-\bar{q} ^{i}_{t,k}\|^{2}_{2},\] \[\mathcal{L}_{\pi}(t,k) =\max_{s\in\mathcal{S}}\sum_{i=1,2}\max_{\mu^{i}\in\Delta( \mathcal{A}^{i})}\left\{(\mu^{i}-\pi^{i}_{t,k}(s))^{\top}\mathcal{T}^{i}(v^{ i}_{t})(s)\pi^{-i}_{t,k}(s)+\tau\nu(\mu^{i})-\tau\nu(\pi^{i}_{t,k}(s))\right\}.\]

Note that \(\mathcal{L}_{\text{sum}}(t)\) is introduced to deal with the fact that the induced matrix game the inner loop of Algorithm 2 is designed to solve may not be a zero-sum game due to independent learning. See the discussion in Section 1.2 and the paragraph after Algorithm 2. At the core of our argument is the following inequality (cf. Lemma D.4):

\[\text{NG}(\pi^{1}_{T,K},\pi^{2}_{T,K})\leq\frac{4}{1-\gamma}\left(2\mathcal{L}_ {\text{sum}}(T)+\mathcal{L}_{v}(T)+\mathcal{L}_{\pi}(T,K)+2\tau\log(A_{\max}) \right),\] (6)which motivates us to bound all the Lyapunov functions.

### Analysis of the Outer Loop: \(v\)-Function Update

Motivated by Eq. (6), we need to bound \(\mathcal{L}_{\text{sum}}(T)\) and \(\mathcal{L}_{v}(T)\). To achieve that, we establish Lyapunov drift inequalities for them. Specifically, we show in Lemmas D.5 and D.6 that

\[\mathcal{L}_{v}(t+1) \leq\underbrace{\gamma\mathcal{L}_{v}(t)}_{\text{Dift}}+ \underbrace{4\mathcal{L}_{\text{sum}}(t)+2\mathcal{L}_{q}^{1/2}(t,K)+4 \mathcal{L}_{\pi}(t,K)+6\tau\log(A_{\max})}_{\text{Additive Errors}},\] (7) \[\mathcal{L}_{\text{sum}}(t+1) \leq\underbrace{\gamma\mathcal{L}_{\text{sum}}(t)}_{\text{Dift}}+ \underbrace{2\mathcal{L}_{q}(t,K)^{1/2}}_{\text{Additive Errors}},\quad\forall\,t \geq 0.\] (8)

Suppose that the _Additive Errors_ in the previous two inequalities were only functions of \(v_{t}^{1}\) and \(v_{t}^{2}\), then these two Lyapunov drift inequalities can be repeatedly used to obtain a convergence bound for \(\mathcal{L}_{\text{sum}}(T)\) and \(\mathcal{L}_{v}(T)\). However, the coupled nature of Eqs. (7) and (8) requires us to analyze the policies and the \(q\)-functions in the inner loop, and establish their Lyapunov drift inequalities.

### Analysis of the Inner Loop: Policy Update

As illustrated in Section 2.1 and Section 3.1, for each state \(s\), the update equation of the policies can be viewed as a discrete and stochastic variant of the smoothed best-response dynamics for solving matrix games [29]. Typically, the following Lyapunov function is used to study such dynamics [53]:

\[V_{X}(\mu^{1},\mu^{2})=\sum_{i=1,2}\max_{\hat{\mu}^{i}\in\Delta( \mathcal{A}^{i})}\{(\hat{\mu}^{i}-\mu^{i})^{\top}X_{i}\mu^{-i}+\tau\nu(\hat{ \mu}^{i})-\tau\nu(\mu^{i})\},\] (9)

where \(X_{1}\) and \(X_{2}\) are the payoff matrices for player \(1\) and player \(2\), respectively, and \(\nu(\cdot)\) is the entropy function. Specialized to our case, given a joint \(v\)-function \(v=(v^{1},v^{2})\) from the outer loop4 and a state \(s\in\mathcal{S}\), we would like to use

Footnote 4: Due to the nested-loop structure of Algorithm 2, conditioned on the history up to the beginning of the \(t\)-th outer loop, the \(v\)-functions \(v_{t}^{1}\) and \(v_{t}^{2}\) are constants.

\[V_{v,s}(\pi^{1}(s),\pi^{2}(s))=\sum_{i=1,2}\max_{\hat{\mu}^{i} \in\Delta(\mathcal{A}^{i})}\{(\hat{\mu}^{i}-\pi^{i}(s))^{\top}\mathcal{T}^{i }(v^{i})(s)\pi^{-i}(s)+\tau\nu(\hat{\mu}^{i})-\tau\nu(\pi^{i}(s))\}\]

as our Lyapunov function. Note that \(\max_{s\in\mathcal{S}}V_{v_{t},s}(\pi^{1}_{t,k}(s),\pi^{2}_{t,k}(s))=\mathcal{ L}_{\pi}(t,k)\). A sequence of properties (e.g., strong convexity, smoothness, etc.) regarding the Lyapunov function \(V_{X}(\cdot,\cdot)\) is established in Lemma D.7. In the end, we show in Lemma D.8 that

\[\mathbb{E}_{t}\left[\mathcal{L}_{\pi}(t,k+1)\right] \leq\underbrace{\left(1-\frac{3\beta_{k}}{4}\right)\mathbb{E}_{t }\left[\mathcal{L}_{\pi}(t,k)\right]}_{\text{Dift}}\] \[\quad+\underbrace{2L_{\pi}\beta_{k}^{2}+\frac{32A_{\max}^{2}\beta _{k}}{\tau^{3}\ell_{\tau}^{2}(1-\gamma)^{2}}\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]+\frac{16A_{\max}^{2}\beta_{k}}{\tau}\mathcal{L}_{\text{sum}}(t)^{2}}_{ \text{Additive Errors}},\] (10)

where \(\mathbb{E}_{t}[\,\cdot\,]\) stands for conditional expectation conditioned on the history up to the beginning of the \(t\)-th outer loop. To interpret the above, suppose that we were considering the continuous-time smoothed best-response dynamics. Then, the additive error term would disappear in the sense that the time-derivative of the Lyapunov function along the trajectory of the ODE is strictly negative. Thus, the three terms in the _Additive Errors_ can be interpreted as (1) the discretization error in the update equation, (2) the stochastic error in the \(q\)-function estimate, and (3) the error due to the non-zero-sum structure of the inner-loop auxiliary matrix game.

### Analysis of the Inner Loop: \(q\)-Function Update

Our next focus is the \(q\)-function update. The \(q\)-function update equation is in the same spirit as TD-learning in RL, and a necessary condition for the convergence of TD-learning is that the behavior policy (i.e., the policy used to collect samples) should enable the agent to sufficiently explore the environment. To achieve this goal, since we show in Lemma D.1 that all joint policies from the algorithm trajectory have uniformly lower-bounded entries (with lower bound \(\ell_{\tau}>0\)), it is enough to restrict our attention to a "soft" policy class \(\Pi_{\tau}:=\{\pi=(\pi^{1},\pi^{2})\mid\min_{s,a^{1}}\pi^{1}(a^{1}|s)\geq\ell_{ \tau},\min_{s,a^{2}}\pi^{2}(a^{2}|s)\geq\ell_{\tau}\}\). The following lemma, which is an extension of [10, Lemma 4], establishes a uniform exploration property under Assumption 3.1.

To present the result, we need the following notation. Under Assumption 3.1, the Markov chain induced by the joint policy \(\pi_{b}\) has a unique stationary distribution \(\mu_{b}\in\Delta(\mathcal{S})\)[93], the minimum component of which is denoted by \(\mu_{b,\min}\). In addition, there exists \(\rho_{b}\in(0,1)\) such that \(\max_{s\in\mathcal{S}}\left\|P_{\pi_{b}}^{k}(s,\cdot)-\mu_{b}(\cdot)\right\| _{\mathrm{TV}}\leq 2\rho_{b}^{k}\) for all \(k\geq 0\)[93], where \(P_{\pi_{b}}\) is the transition probability matrix of the Markov chain \(\{S_{k}\}\) under \(\pi_{b}\). We also define the mixing time in the following. Given a joint policy \(\pi=(\pi^{1},\pi^{2})\) and an accuracy level \(\eta>0\), the \(\eta-\) mixing time of the Markov chain \(\{S_{k}\}\) induced by \(\pi\) is defined as

\[t_{\pi,\eta}=\min\left\{k\geq 0\ :\ \max_{s\in\mathcal{S}}\|P_{\pi}^{k}(s, \cdot)-\mu_{\pi}(\cdot)\|_{\mathrm{TV}}\leq\eta\right\},\] (11)

where \(P_{\pi}\) is the \(\pi\)-induced transition probability matrix and \(\mu_{\pi}\) is the stationary distribution of \(\{S_{k}\}\) under \(\pi\), provided that it exists and is unique. When the induced Markov chain mixes at a geometric rate, it is easy to see that \(t_{\pi,\eta}=\mathcal{O}(\log(1/\eta))\).

**Lemma B.1** (An Extension of Lemma 4 in [10]).: _Suppose that Assumption 3.1 is satisfied. Then we have the following results._

1. _For any_ \(\pi=(\pi^{1},\pi^{2})\in\Pi_{\tau}\)_, the Markov chain_ \(\{S_{k}\}\) _induced by the joint policy_ \(\pi\) _is irreducible and aperiodic, hence admits a unique stationary distribution_ \(\mu_{\pi}\in\Delta(\mathcal{S})\)_._
2. _It holds that_ \(\sup_{\tau\in\Pi_{\tau}}\max_{s\in\mathcal{S}}\|P_{\pi}^{k}(s,\cdot)-\mu_{ \pi}(\cdot)\|_{TV}\leq 2\rho_{\tau}^{k}\) _for any_ \(k\geq 0\)_, where_ \(\rho_{\tau}=\rho_{b}^{\ell_{\tau}^{2\tau_{b}}\mu_{h,\min}}\) _and_ \(r_{b}:=\min\{k\geq 0\ :\ P_{\pi_{b}}^{k}(s,s^{\prime})>0,\ \forall\ (s,s^{\prime})\}\)_. As a result, we have_ \[t(\ell_{\tau},\eta):=\sup_{\pi\in\Pi_{\tau}}t_{\pi,\eta}\leq\frac{t_{\pi_{b}, \eta}}{\ell_{\tau}^{2r_{b}}\mu_{b,\min}},\] (12) _where we recall that_ \(t_{\pi_{b},\eta}\) _is the_ \(\eta\)_- mixing time of the Markov chain_ \(\{S_{k}\}\) _induced by_ \(\pi_{b}\)_._
3. _There exists_ \(L_{p}\geq 1\) _(which was used in the statement of Theorem_ 3.1_) such that_ \[\|\mu_{\pi}-\mu_{\bar{\pi}}\|_{1}\leq L_{p}\big{(}\max_{s\in\mathcal{S}}\|\pi^{ 1}(s)-\pi^{2}(s)\|_{1}+\max_{s\in\mathcal{S}}\|\bar{\pi}^{1}(s)-\bar{\pi}^{2}( s)\|_{1}\big{)}\] _for all_ \(\pi=(\pi^{1},\pi^{2}),\bar{\pi}=(\bar{\pi}^{1},\bar{\pi}^{2})\in\Pi_{\tau}\)_._
4. \(\mu_{\min}:=\inf_{\pi\in\Pi_{\tau}}\min_{s\in\mathcal{S}}\mu_{\pi}(s)>0\)_._

_Remark_.: Lemma B.1 (1), (3), and (4) were previous established in [10, Lemma 4]. Lemma B.1 (2) enables us to see the explicit dependence of the "uniform mixing time" on the margin \(\ell_{\tau}\) and the mixing time of the benchmark exploration policy \(\pi_{b}\).

In view of Lemma B.1 (2), we have fast mixing for all policies in \(\Pi_{\tau}\) if _(i)_ the margin \(\ell_{\tau}\) is large, and _(ii)_ the Markov chain \(\{S_{k}\}\) induced by the benchmark exploration policy \(\pi_{b}\) is well-behaved. By "well-behaved" we mean the mixing time is small (i.e., small \(t_{\pi_{b},\eta}\)) and the stationary distribution is relatively well-balanced (i.e., large \(\mu_{b,\min}\)). Point _(i)_ agrees with our intuition as a large margin encourages more exploration. To make sense of point _(ii)_, since \(\pi(a|s)\geq\ell_{\tau}^{2}\pi_{b}(a|s)\) for all \(s\) and \(a=(a^{1},a^{2})\), we can write \(\pi\) as a convex combination between \(\pi_{b}\) and some residual policy \(\bar{\pi}\): \(\pi(\cdot|s)=\ell_{\tau}^{2}\pi_{b}(\cdot|s)+(1-\ell_{\tau}^{2})\tilde{\pi}( \cdot|s)\) for all \(s\in\mathcal{S}\). Therefore, since any \(\pi\in\Pi_{\tau}\) has a portion of the benchmark exploration policy \(\pi_{b}\) in it, it makes intuitive sense that fast mixing of \(\{S_{k}\}\) under \(\pi_{b}\) implies, to some extent, fast mixing of \(\{S_{k}\}\) under \(\pi\in\Pi_{\tau}\). Note that, as the margin \(\ell_{\tau}\) approaches zero, the uniform mixing time in Lemma B.1 (2) goes to infinity. This is not avoidable in general, as demonstrated by a simple MDP example constructed in Appendix E.

We define \(c_{\tau}=\mu_{\min}\ell_{\tau}\), which was used in the statement of Theorem 3.1. With Lemma B.1 in hand, we are now able to analyze the behavior of the \(q\)-functions. We model the \(q\)-function update as a 

[MISSING_PAGE_FAIL:20]

where we first derive a crude bound and then substitute the crude bound back into the Lyapunov drift inequalities to derive a tighter bound. We next present our approach.

For ease of presentation, for a scalar-valued quantity \(W\) that is a function of \(k\) and/or \(t\), we say \(W=o_{k}(1)\) if \(\lim_{k\to\infty}W=0\) and \(W=o_{t}(1)\) if \(\lim_{t\to\infty}W=0\). The explicit convergence rates of the \(o_{k}(1)\) term and the \(o_{t}(1)\) term will be revealed in the complete proof in Appendix D.6, but is not important for the illustration here.

**Step 1.** Adding up Eq. (17) and (18), using Condition 3.1, and then repeatedly using the resulting inequality, we obtain:

\[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)]\leq\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k )+\mathcal{L}_{q}(t,k)]=o_{k}(1)+\mathcal{O}(1)\mathcal{L}_{\text{sum}}^{2}(t ),\ \forall\ t,k.\] (19)

**Step 2.** Substituting the bound for \(\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)]\) in Eq. (19) into Eq. (18) and repeatedly using the resulting inequality, and we obtain:

\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,K)]=o_{K}(1)+\mathcal{O}(c_{\alpha,\beta}) \mathcal{L}_{\text{sum}}^{2}(t),\ \forall\ t,\]

which in turn implies (by first using Jensen's inequality and then taking total expectation) that:

\[\mathbb{E}[\mathcal{L}_{q}^{1/2}(t,K)]=o_{K}(1)+\mathcal{O}(c_{\alpha,\beta}^ {1/2})\mathbb{E}[\mathcal{L}_{\text{sum}}(t)],\ \forall\ t,\] (20)

where we recall that \(c_{\alpha,\beta}=\beta_{k}/\alpha_{k}\) is the stepsize ratio. The fact that we are able to get a factor of \(\mathcal{O}(c_{\alpha,\beta}^{1/2})\) in front of \(\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\) is crucial for the decoupling procedure.

**Step 3.** Taking total expectation on both sides of Eq. (16) and then using the upper bound of \(\mathbb{E}[\mathcal{L}_{q}^{1/2}(t,K)]\) we obtained in Eq. (20), we obtain

\[\mathbb{E}[\mathcal{L}_{\text{sum}}(t+1)]\leq(\gamma+\mathcal{O}(c_{\alpha, \beta}^{1/2}))\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]+o_{K}(1),\ \forall\ t.\]

By choosing \(c_{\alpha,\beta}\) so that \(\mathcal{O}(c_{\alpha,\beta}^{1/2})\leq(1-\gamma)/2\), the previous inequality implies

\[\mathbb{E}[\mathcal{L}_{\text{sum}}(t+1)]\leq\left(1-\frac{1-\gamma}{2} \right)\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]+o_{K}(1),\ \forall\ t,\] (21)

which can be repeatedly used to obtain

\[\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]=o_{t}(1)+o_{K}(1).\] (22)

Substituting the previous bound on \(\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\) into Eq. (19), we have

\[\max(\mathbb{E}[\mathcal{L}_{\pi}(t,K)],\mathbb{E}[\mathcal{L}_{q}(t,K)])=o_{ t}(1)+o_{K}(1).\] (23)

**Step 4.** Substituting the bounds we obtained for \(\mathbb{E}[\mathcal{L}_{\pi}(t,K)]\), \(\mathbb{E}[\mathcal{L}_{q}(t,K)]\), and \(\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\) in Eqs. (22) and (23) into Eq. (15), and then repeatedly using the resulting inequality from \(t=0\) to \(t=T\), we have

\[\mathbb{E}[\mathcal{L}_{v}(T)]=o_{T}(1)+o_{K}(1)+\mathcal{O}(\tau).\]

Now that we have obtained finite-sample bounds for \(\mathbb{E}[\mathcal{L}_{v}(T)]\), \(\mathbb{E}[\mathcal{L}_{\text{sum}}(T)]\), \(\mathbb{E}[\mathcal{L}_{\pi}(T,K)]\), and \(\mathbb{E}[\mathcal{L}_{q}(T,K)]\), using them in Eq. (6), we finally obtain the desired finite-sample bound for the expected Nash gap.

Looking back at the decoupling procedure, Steps \(2\) and \(3\) are crucial. In fact, in Step \(1\) we already obtain a bound on \(\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]\), where the additive error is \(\mathcal{O}(1)\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\). However, directly using this bound on \(\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]\) in Eq. (16) would result in an expansive inequality for \(\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\). By performing Step \(2\), we are able to obtain a tighter bound for \(\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]\), with the additive error being \(\mathcal{O}(c_{\alpha,\beta}^{1/2})\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\). Furthermore, we can choose \(c_{\alpha,\beta}\) to be small enough so that after using the bound from Eq. (20) in Eq. (16), the additive error \(\mathcal{O}(c_{\alpha,\beta}^{1/2})\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\) is dominated by the negative drift in Eq. (21).

## Appendix C Proof of Theorem 2.1

The proof is divided into \(4\) steps. In Appendix C.1, we prove an important boundedness property regarding the iterates generated by Algorithm 1. In Appendices C.2 and C.3, we analyze the evolution of the policies and the \(q\)-functions by establishing the negative drift inequalities with respect to their associated Lyapunov functions. In Appendix C.4, we solve the coupled Lyapunov drift inequalities to prove Theorem 2.1. Moreover, we prove Corollary 2.1.1 in Appendix C.5. The statement and proof of all supporting lemmas used in this section are presented in Appendix C.6.

### Boundedness of the Iterates

In this subsection, we show that the \(q\)-functions generated by Algorithm 1 are uniformly bounded from above, and the entries of the policies are uniformly bounded from below. The following lemma is needed to establish the result.

**Lemma C.1**.: _For any \(i\in\{1,2\}\) and \(q^{i}\in\mathbb{R}^{|\mathcal{A}|^{i}}\), we have_

\[\min_{a^{i}\in\mathcal{A}^{i}}[\sigma_{\tau}(q^{i})](a^{i})\geq\frac{1}{(A_{ \max}-1)\exp(2\|q^{i}\|_{\infty}/\tau)+1}.\]

Proof of Lemma c.1.: Given \(i\in\{1,2\}\), for any \(q^{i}\in\mathbb{R}^{|\mathcal{A}|^{i}}\) and \(a^{i}\in\mathcal{A}^{i}\), we have

\[[\sigma_{\tau}(q^{i})](a^{i}) =\frac{\exp(q^{i}(a^{i})/\tau)}{\sum_{\tilde{a}^{i}\in\mathcal{A} ^{i}}\exp(q^{i}(\tilde{a}^{i})/\tau)}\] \[=\frac{1}{\sum_{\tilde{a}^{i}\neq a^{i}}\exp((q^{i}(\tilde{a}^{i} )-q^{i}(a^{i}))/\tau)+1}\] \[\geq\frac{1}{(|\mathcal{A}^{i}|-1)\exp(2\|q^{i}\|_{\infty}/\tau) +1}\] \[\geq\frac{1}{(A_{\max}-1)\exp(2\|q^{i}\|_{\infty}/\tau)+1}.\]

Since the RHS of the previous inequality does not depend on \(a^{i}\), we have the desired inequality. 

We next derive the boundedness property in the following lemma.

**Lemma C.2**.: _It holds for all \(k\geq 0\) and \(i\in\{1,2\}\) that \(\|q^{i}_{k}\|_{\infty}\leq 1\) and \(\min_{a^{i}\in\mathcal{A}^{i}}\pi^{i}_{k}(a^{i})\geq\ell_{\tau}\), where \(\ell_{\tau}=[(A_{\max}-1)\exp(2/\tau)+1]^{-1}\)._

Proof of Lemma c.2.: We prove the results by induction. Since \(q^{i}_{0}=0\) and \(\pi^{i}_{0}\) is initialized as a uniform distribution on \(\mathcal{A}^{i}\), we have the base case. Now suppose that the results hold for some \(k\geq 0\). Using the update equation for \(q^{i}_{k}\) in Algorithm 1 Line \(5\), we have

\[|q^{i}_{k+1}(a^{i})| =|(1-\alpha_{k}\mathds{1}_{\{a^{i}=A^{i}_{k}\}})q^{i}_{k}(a^{i})+ \alpha_{k}\mathds{1}_{\{a^{i}=A^{i}_{k}\}}R_{i}(A^{i}_{k},A^{-i}_{k})|\] \[\leq\max(|q^{i}_{k}(a^{i})|,(1-\alpha_{k})|q^{i}_{k}(a^{i})|+ \alpha_{k}|R_{i}(A^{i}_{k},A^{-i}_{k})|)\] \[\leq 1\]

for any \(a^{i}\in\mathcal{A}^{i}\), where the last line follows from the induction hypothesis \(\|q^{i}_{k}\|_{\infty}\leq 1\) and \(|R_{i}(a^{i},a^{-i})|\leq 1\) for all \((a^{i},a^{-i})\). As for \(\pi^{i}_{k+1}\), using the update equation for \(\pi^{i}_{k}\) in Algorithm 1 Line \(3\), we have

\[\pi^{i}_{k+1}(a^{i}) =(1-\beta_{k})\pi^{i}_{k}(a^{i})+\beta_{k}[\sigma_{\tau}(q^{i}_{ k})](a^{i})\] \[\geq(1-\beta_{k})\ell_{\tau}+\frac{\beta_{k}}{(A_{\max}-1)\exp(2 \|q^{i}_{k}\|_{\infty}/\tau)+1}\] (Lemma C.1) \[\geq(1-\beta_{k})\ell_{\tau}+\beta_{k}\ell_{\tau}\] ( \[\|q^{i}_{k}\|_{\infty}\leq 1\] by induction hypothesis) \[=\ell_{\tau}.\]

The induction is complete. 

### Analysis of the Policies

Let \(V_{R}:\Delta(\mathcal{A}^{1})\times\Delta(\mathcal{A}^{2})\mapsto\mathbb{R}\) be the regularized Nash gap defined as

\[V_{R}(\mu^{1},\mu^{2})=\sum_{i=1,2}\max_{\tilde{\mu}^{i}\in\Delta(\mathcal{A}^ {i})}\left\{(\tilde{\mu}^{i}-\mu^{i})^{\top}R_{i}\mu^{-i}+\tau\nu(\tilde{\mu}^{ i})-\tau\nu(\mu^{i})\right\},\]

where \(\nu(\cdot)\) is the entropy function. A sequence of properties regarding \(V_{R}(\cdot,\cdot)\) are provided in Lemma C.7. For simplicity of notation, we use \(\nabla_{1}V_{R}(\cdot,\cdot)\) (respectively, \(\nabla_{2}V_{R}(\cdot,\cdot)\)) to represent the gradient with respect to the first argument (respectively, the second argument).

**Lemma C.3**.: _It holds for all \(k\geq 0\) that_

\[\mathbb{E}[V_{R}(\pi^{1}_{k+1},\pi^{2}_{k+1})]\leq\,\left(1-\frac{ \beta_{k}}{2}\right)\mathbb{E}[V_{R}(\pi^{1}_{k},\pi^{2}_{k})]+\frac{\ell_{ \tau}\alpha_{k}}{4}\sum_{i=1,2}\mathbb{E}[\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|^{2}_{ 2}]+2L_{\tau}\beta^{2}_{k},\]

_where we recall that \(L_{\tau}=\tau/\ell_{\tau}+A^{2}_{\max}/\tau\)._

Proof of Lemma c.3.: Using the smoothness property of \(V_{R}(\cdot,\cdot)\) (cf. Lemma C.7 (1)) and the update equation in Algorithm 1 Line \(3\), we have for any \(k\geq 0\) that

\[V_{R}(\pi^{1}_{k+1},\pi^{2}_{k+1}) \leq V_{R}(\pi^{1}_{k},\pi^{2}_{k})+\beta_{k}\langle\nabla_{2}V_{R}( \pi^{1}_{k},\pi^{2}_{k}),\sigma_{\tau}(q^{2}_{k})-\pi^{2}_{k}\rangle\] \[\quad+\beta_{k}\langle\nabla_{1}V_{R}(\pi^{1}_{k},\pi^{2}_{k}), \sigma_{\tau}(q^{1}_{k})-\pi^{1}_{k}\rangle+\frac{L_{\tau}\beta^{2}_{k}}{2} \sum_{i=1,2}\|\sigma_{\tau}(q^{i}_{k})-\pi^{i}_{k}\|^{2}_{2}\] \[\leq V_{R}(\pi^{1}_{k},\pi^{2}_{k})+\beta_{k}\langle\nabla_{2}V_{R}( \pi^{1}_{k},\pi^{2}_{k}),\sigma_{\tau}(R_{2}\pi^{1}_{k})-\pi^{2}_{k}\rangle\] \[\quad+\beta_{k}\langle\nabla_{1}V_{R}(\pi^{1}_{k},\pi^{2}_{k+1}), \sigma_{\tau}(R_{1}\pi^{2}_{k})-\pi^{1}_{k}\rangle\] \[\quad+\beta_{k}\langle\nabla_{2}V_{R}(\pi^{1}_{k},\pi^{2}_{k}), \sigma_{\tau}(q^{2}_{k})-\sigma_{\tau}(R_{2}\pi^{1}_{k})\rangle\] \[\quad+\beta_{k}\langle\nabla_{1}V_{R}(\pi^{1}_{k},\pi^{2}_{k+1}), \sigma_{\tau}(q^{1}_{k})-\sigma_{\tau}(R_{1}\pi^{2}_{k})\rangle+2L_{\tau}\beta ^{2}_{k}\] \[\leq \left(1\!-\!\frac{\beta_{k}}{2}\right)V_{R}(\pi^{1}_{k},\pi^{2}_{ k})\!+\!4\beta_{k}\left(\frac{1}{\tau\ell^{2}_{\tau}}\!+\!\frac{A^{2}_{\max}}{ \tau^{3}}\right)\sum_{i=1,2}\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|^{2}_{2}+2L_{\tau} \beta^{2}_{k},\]

where the last line follows from Lemma C.7 (2) and (3). Taking expectations on both sides of the previous inequality and using the condition that \(c_{\alpha,\beta}=\frac{\beta_{k}}{\alpha_{k}}\leq\min(\frac{\tau\ell^{3}_{ \tau}}{32},\frac{\ell_{\tau}\gamma^{3}}{32A^{2}_{\max}})\) (cf. Condition 2.1), we have

\[\mathbb{E}[V_{R}(\pi^{1}_{k+1},\pi^{2}_{k+1})]\leq\,\left(1-\frac{ \beta_{k}}{2}\right)\mathbb{E}[V_{R}(\pi^{1}_{k},\pi^{2}_{k})]+\frac{\ell_{ \tau}\alpha_{k}}{4}\sum_{i=1,2}\mathbb{E}[\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|^{2}_ {2}]+2L_{\tau}\beta^{2}_{k}.\]

The proof is complete. 

### Analysis of the \(q\)-Functions

We study the \(q\)-functions generated by Algorithm 1 through a stochastic approximation framework. For \(i\in\{1,2\}\), let \(F^{i}:\mathbb{R}^{|\mathcal{A}^{i}|}\times\mathcal{A}^{i}\times\mathcal{A}^{-i }\mapsto\mathbb{R}^{|\mathcal{A}^{i}|}\) be an operator defined as

\[[F^{i}(q^{i},a^{i}_{0},a^{-i}_{0})](a^{i})=\mathds{1}_{\{a^{i}_{0}=a^{i}\}} \left(R_{i}(a^{i}_{0},a^{-i}_{0})-q^{i}(a^{i}_{0})\right),\quad\forall\;(q^{i},a^{i}_{0},a^{-i}_{0})\text{ and }a^{i}.\]

Then, Algorithm 1 Line \(5\) can be compactly written as

\[q^{i}_{k+1}=q^{i}_{k}+\alpha_{k}F^{i}(q^{i}_{k},A^{i}_{k},A^{-i}_{k}).\] (24)

Given a joint policy \((\pi^{1},\pi^{2})\), let \(\bar{F}^{i}_{\pi}:\mathbb{R}^{|\mathcal{A}^{i}|}\mapsto\mathbb{R}^{|\mathcal{ A}^{i}|}\) be defined as

\[\bar{F}^{i}_{\pi}(q^{i}):=\mathbb{E}_{\mathcal{A}^{i}\sim\pi^{i}( \cdot),A^{-i}\sim\pi^{-i}(\cdot)}[F^{i}(q^{i},A^{i},A^{-i})]=\text{diag}(\pi^{ i})(R_{i}\pi^{-i}-q^{i}).\]

Then, Eq. (24) can be viewed as a stochastic approximation algorithm for solving the slowly time-varying equation \(\bar{F}^{i}_{\pi_{k}}(q^{i})=0\).

**Lemma C.4**.: _The following inequality holds for all \(k\geq 0\):_

\[\sum_{i=1,2}\mathbb{E}[\|q^{i}_{k+1}-R_{i}\pi^{-i}_{k+1}\|^{2}_{2}]\leq\left(1- \frac{\ell_{\tau}\alpha_{k}}{2}\right)\sum_{i=1,2}\mathbb{E}[\|q^{i}_{k}-R_{i} \pi^{-i}_{k}\|^{2}_{2}]+\frac{\beta_{k}}{4}\mathbb{E}[V_{R}(\pi^{1}_{k},\pi^{2} _{k})]+16\alpha^{2}_{k}.\]

Proof of Lemma c.4.: For any \(k\geq 0\) and \(i\in\{1,2\}\), we have

\[\|q^{i}_{k+1}-R_{i}\pi^{-i}_{k+1}\|^{2}_{2}\] \[=\|q^{i}_{k+1}-q^{i}_{k}+q^{i}_{k}-R_{i}\pi^{-i}_{k}+R_{i}\pi^{-i} _{k}-R_{i}\pi^{-i}_{k+1}\|^{2}_{2}\] \[=\|q^{i}_{k+1}-q^{i}_{k}\|^{2}_{2}+\|q^{i}_{k}-R_{i}\pi^{-i}_{k} \|^{2}_{2}+\|R_{i}\pi^{-i}_{k}-R_{i}\pi^{-i}_{k+1}\|^{2}_{2}+2\langle q^{i}_{k+1}- q^{i}_{k},q^{i}_{k}-R_{i}\pi^{-i}_{k}\rangle\]\[\quad+2\alpha_{k}\langle\bar{F}^{i}_{\pi_{k}}(q^{i}_{k}),q^{i}_{k}-R_{i} \pi^{-i}_{k}\rangle+2\alpha_{k}\langle F^{i}(q^{i}_{k},A^{i}_{k},A^{-i}_{k})- \bar{F}^{i}_{\pi_{k}}(q^{i}_{k}),q^{i}_{k}-R_{i}\pi^{-i}_{k}\rangle\] \[\quad+\frac{\beta_{k}}{c_{2}}\|R_{i}(\tau_{q}^{-i}-\tau_{k}^{-i}) \|_{2}^{2}+c_{2}\beta_{k}\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|_{2}^{2}\] \[\quad+\frac{\beta_{k}}{c_{2}}\|R_{i}(\tau_{q}^{-i}-\tau_{k}^{-i}) \|_{2}^{2}+c_{2}\beta_{k}\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|_{2}^{2}\] \[\quad=\left(\alpha_{k}^{2}+\frac{\alpha_{k}\beta_{k}}{c_{1}} \right)\|F^{i}(q^{i}_{k},A^{i}_{k},A^{-i}_{k})\|_{2}^{2}+(1+c_{2}\beta_{k})\, \mathbb{E}[\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|_{2}^{2}]\] \[\quad+2\alpha_{k}\langle\bar{F}^{i}_{\pi_{k}}(q^{i}_{k}),q^{i}_{k }-R_{i}\pi^{-i}_{k}\rangle]+\left(\beta_{k}^{2}+\frac{\beta_{k}}{c_{2}}+\alpha _{k}\beta_{k}c_{1}\right)\|R_{i}\|_{2}^{2}\mathbb{E}[\|\sigma_{\tau}(q^{-i}_{k })-\pi^{-i}_{k}\|_{2}^{2}],\]

where the term \(\mathbb{E}[\langle F^{i}(q^{i}_{k},A^{i}_{k},A^{-i}_{k})-\bar{F}^{i}_{\pi_{k}} (q^{i}_{k}),q^{i}_{k}-R_{i}\pi^{-i}_{k}]\rangle\) vanishes due to the tower property of conditional expectations. To proceed, observe

\[\mathbb{E}[\|F^{i}(q^{i}_{k},A^{i}_{k},A^{-i}_{k})\|_{2}^{2}] =\mathbb{E}\left[\sum_{a^{i}\in\mathcal{A}^{i}}\mathds{1}_{\{A^{ i}_{k}=a^{i}\}}\left(R_{i}(A^{i}_{k},A^{-i}_{k})-q^{i}_{k}(A^{i}_{k})\right)^{2}\right]\] \[=\mathbb{E}[(R_{i}(a^{i},A^{-i}_{k})-q^{i}_{k}(a^{i}))^{2}]\] \[\leq\mathbb{E}[(|R_{i}(a^{i},A^{-i}_{k})|+|q^{i}_{k}(a^{i})|)^{2}]\] \[\leq 4\] (Lemma C.2)

and

\[\mathbb{E}[\langle\bar{F}^{i}_{\pi_{k}}(q^{i}_{k}),q^{i}_{k}-R_{i} \pi^{-i}_{k}\rangle] =\mathbb{E}[\langle\text{diag}(\pi^{i}_{k})(R_{i}\pi^{-i}_{k}-q^{i }_{k}),q^{i}_{k}-R_{i}\pi^{-i}_{k}\rangle]\] \[\leq\ -\ell_{\tau}\mathbb{E}[\|q^{i}_{k}-R_{i}\pi^{-i}_{k}\|_{2}^{2}].\] (Lemma C.2)

In addition, we have

\[\mathbb{E}[\|\sigma_{\tau}(q^{-i}_{k})-\pi^{-i}_{k}\|_{2}^{2}] \leq 2\mathbb{E}[\|\sigma_{\tau}(q^{-i}_{k})-\sigma_{\tau}(R_{-i}\pi^ {i}_{k})\|_{2}^{2}]+2\mathbb{E}[\|\sigma_{\tau}(R_{-i}\pi^{i}_{k})-\pi^{-i}_{k} \|_{2}^{2}]\] \[\leq\frac{2}{\tau^{2}}\mathbb{E}[\|q^{-i}_{k}-R_{-i}\pi^{i}_{k}\| _{2}^{2}]+\frac{4}{\tau}\mathbb{E}[V_{R}(\pi^{1}_{k},\pi^{2}_{k})],\]where the last line follows from Lemma C.6. Using the previous \(4\) inequalities all together, we obtain

\[\mathbb{E}[\|q_{k+1}^{i}-R_{i}\pi_{k+1}^{-i}\|_{2}^{2}] \leq 4\left(\alpha_{k}^{2}+\frac{\alpha_{k}\beta_{k}}{c_{1}}\right)+ \left(1-2\ell_{\tau}\alpha_{k}+c_{2}\beta_{k}\right)\mathbb{E}[\|q_{k}^{i}-R_{ i}\pi_{k}^{-i}\|_{2}^{2}]\] \[\quad+\frac{4A_{\max}^{2}}{\tau}\left(\beta_{k}^{2}+\frac{\beta_{ k}}{c_{2}}+\alpha_{k}\beta_{k}c_{1}\right)\mathbb{E}[V_{R}(\pi_{k}^{1},\pi_{k}^{2 })]\] \[\quad+\frac{2A_{\max}^{2}}{\tau^{2}}\left(\beta_{k}^{2}+\frac{ \beta_{k}}{c_{2}}+\alpha_{k}\beta_{k}c_{1}\right)\mathbb{E}[\|q_{k}^{-i}-R_{ i}\pi_{k}^{-i}\|_{2}^{2}].\]

Summing up the previous inequality for \(i\in\{1,2\}\), we have

\[\sum_{i=1,2}\mathbb{E}[\|q_{k+1}^{i}-R_{i}\pi_{k+1}^{-i}\|_{2}^{2}]\] \[\leq \left(1-2\ell_{\tau}\alpha_{k}+c_{2}\beta_{k}+\frac{2A_{\max}^{2 }}{\tau^{2}}\left(\beta_{k}^{2}+\frac{\beta_{k}}{c_{2}}+\alpha_{k}\beta_{k}c_{ 1}\right)\right)\sum_{i=1,2}\mathbb{E}[\|q_{k}^{i}-R_{i}\pi_{k}^{-i}\|_{2}^{2}]\] \[\quad+\frac{8A_{\max}^{2}}{\tau}\left(\beta_{k}^{2}+\frac{\beta_ {k}}{c_{2}}+\alpha_{k}\beta_{k}c_{1}\right)\mathbb{E}[V_{R}(\pi_{k}^{1},\pi_{k} ^{2})]+8\left(\alpha_{k}^{2}+\frac{\alpha_{k}\beta_{k}}{c_{1}}\right)\] \[= \left(1-\frac{3\ell_{\tau}\alpha_{k}}{2}+\frac{2A_{\max}^{2}}{ \tau^{2}}\left(2\beta_{k}^{2}+\frac{2\beta_{k}^{2}}{\ell_{\tau}\alpha_{k}} \right)\right)\sum_{i=1,2}\mathbb{E}[\|q_{k}^{i}-R_{i}\pi_{k}^{-i}\|_{2}^{2}]\] \[\quad+\frac{8A_{\max}^{2}}{\tau}\left(2\beta_{k}^{2}+\frac{2 \beta_{k}^{2}}{\ell_{\tau}\alpha_{k}}\right)\mathbb{E}[V_{R}(\pi_{k}^{1},\pi_{ k}^{2})]+16\alpha_{k}^{2}\qquad\text{(Choosing $c_{1}=\frac{\beta_{k}}{\alpha_{k}}$ and $c_{2}=\frac{\ell_{\tau}\alpha_{k}}{2\beta_{k}}$)}\] \[\leq \left(1-\frac{\ell_{\tau}\alpha_{k}}{2}\right)\sum_{i=1,2}\mathbb{ E}[\|q_{k}^{i}-R_{i}\pi_{k}^{-i}\|_{2}^{2}]+\frac{\beta_{k}}{4}\mathbb{E}[V_{R}(\pi_{k} ^{1},\pi_{k}^{2})]+16\alpha_{k}^{2},\]

where the last line follows from \(c_{\alpha,\beta}\leq\min(\frac{\tau^{2}\ell_{\tau}}{8A_{\max}^{2}},\frac{\tau \ell_{\tau}}{128A_{\max}^{2}})\) and \(\beta_{0}\leq\frac{\tau}{128A_{\max}^{2}}\) (cf. Condition 2.1). The proof is complete. 

### Solving the Coupled Lyapunov Inequalities

For simplicity of notation, denote \(\mathcal{L}_{q}(k)=\sum_{i=1,2}\mathbb{E}[\|q_{k}^{i}-R_{i}\pi_{k}^{-i}\|_{2}^ {2}]\) and \(\mathcal{L}_{\pi}(k)=\mathbb{E}[V_{R}(\pi_{k}^{1},\pi_{k}^{2})]\). Then, Lemmas C.3 and C.4 state that

\[\mathcal{L}_{\pi}(k+1)\leq \left(1-\frac{\beta_{k}}{2}\right)\mathcal{L}_{\pi}(k)+\frac{\ell_ {\tau}\alpha_{k}}{4}\mathcal{L}_{q}(k)+2L_{\tau}\beta_{k}^{2},\]

and

\[\mathcal{L}_{q}(k+1)\leq \left(1-\frac{\ell_{\tau}\alpha_{k}}{2}\right)\mathcal{L}_{q}(k)+ \frac{\beta_{k}}{4}\mathcal{L}_{\pi}(k)+16\alpha_{k}^{2}.\]

Adding up the previous two inequalities, we obtain

\[\mathcal{L}_{q}(k+1)+\mathcal{L}_{\pi}(k+1) \leq \left(1-\frac{\beta_{k}}{4}\right)\mathcal{L}_{\pi}(k)+2L_{\tau} \beta_{k}^{2}+\left(1-\frac{\ell_{\tau}\alpha_{k}}{4}\right)\mathcal{L}_{q}(k)+ 16\alpha_{k}^{2}\] (25) \[\leq \left(1-\frac{\beta_{k}}{4}\right)\left(\mathcal{L}_{\pi}(k)+ \mathcal{L}_{q}(k)\right)+2L_{\tau}\beta_{k}^{2}+16\alpha_{k}^{2},\]

where the second inequality follows from \(c_{\alpha,\beta}\leq\ell_{\tau}\) (cf. Condition 2.1).

Constant Stepsizes.When using constant stepsizes, i.e., \(\alpha_{k}\equiv\alpha\) and \(\beta_{k}\equiv\beta\), repeatedly using Eq. (25), we have for all \(k\geq 0\) that

\[\mathcal{L}_{q}(k)+\mathcal{L}_{\pi}(k) \leq \left(1-\frac{\beta}{4}\right)^{k}\left(\mathcal{L}_{\pi}(0)+ \mathcal{L}_{q}(0)\right)+8L_{\tau}\beta+64\alpha^{2}/\beta\] \[\leq \left(1-\frac{\beta}{4}\right)^{k}\left(4+2\tau\log(A_{\max})+2A_ {\max}\right)+8L_{\tau}\beta+64\alpha^{2}/\beta\]\[=B_{\text{in}}\left(1-\frac{\beta}{4}\right)^{k}+8L_{\tau}\beta+\frac{64 \alpha}{c_{\alpha,\beta}}\]

where the second inequality follows from

\[\mathcal{L}_{\pi}(0)\leq 4+2\tau\log(A_{\max}),\quad\text{and}\quad\mathcal{L}_{q} (0)\leq 2A_{\max}.\]

Theorem 2.1 (1) follows by observing that \(\mathcal{L}_{q}(k)+\mathcal{L}_{\pi}(k)\geq\mathcal{L}_{\pi}(k)=\mathbb{E}[ \text{NG}_{\tau}(\pi^{1}_{k},\pi^{2}_{k})]\).

Diminishing Stepsizes.Consider using \(\alpha_{k}=\frac{\alpha}{k+h}\) and \(\beta_{k}=\frac{\beta}{k+h}\). Recursions of the form presented in Eq. (25) have been well studied in the existing literature for the convergence rates of iterative algorithms [44, 24, 65]. Since \(\beta>4\), using the same line of analysis as in [65, Appendix A.2], we have

\[\mathcal{L}_{q}(k)+\mathcal{L}_{\pi}(k)\leq B_{\text{in}}\left(\frac{h}{k+h} \right)^{\beta/4}+\left(64eL_{\tau}\beta+512e\alpha/c_{\alpha,\beta}\right) \frac{1}{k+h}.\]

Theorem 2.1 (2) follows by observing that \(\mathcal{L}_{q}(k)+\mathcal{L}_{\pi}(k)\geq\mathcal{L}_{\pi}(k)=\mathbb{E}[ \text{NG}_{\tau}(\pi^{1}_{k},\pi^{2}_{k})]\).

### Proof of Corollary 2.1.1

We use Theorem 2.1 (1) to derive the sample complexity, and choose \(\beta=c_{\alpha,\beta}\alpha\) with \(c_{\alpha,\beta}\) satisfying Condition 2.1. To achieve \(\mathbb{E}[\text{NG}_{\tau}(\pi^{1}_{K},\pi^{2}_{K})]\leq\epsilon\), in view of Theorem 2.1 (1), it is sufficient that

\[B_{\text{in}}e^{-\beta K/4}\leq\frac{\epsilon}{3},\quad 8L_{\tau}\beta\leq \frac{\epsilon}{3},\quad 64\alpha/c_{\alpha,\beta}\leq\frac{\epsilon}{3},\]

which implies \(\beta=\mathcal{O}(\epsilon)\). It follows that \(K=\mathcal{O}\left(\epsilon^{-1}\right)\).

### Supporting Lemmas

**Lemma C.5**.: _For any \(i\in\{1,2\}\) and \(\mu^{i}_{1},\mu^{i}_{2}\in\{\mu^{i}\in\Delta(\mathcal{A}^{i})\mid\min_{a^{i} \in\mathcal{A}^{i}}\mu^{i}(a^{i})\geq\ell_{\tau}\}\), we have_

\[\|\nabla\nu(\mu^{i}_{1})-\nabla\nu(\mu^{i}_{2})\|_{2}\leq\frac{1}{\ell_{\tau} }\|\mu^{i}_{1}-\mu^{i}_{2}\|_{2}.\]

Proof of Lemma c.5.: For any \(i\in\{1,2\}\) and \(\mu^{i}\in\Delta(\mathcal{A}^{i})\) such that \(\min_{a^{i}\in\mathcal{A}^{i}}\mu^{i}(a^{i})\geq\ell_{\tau}\), the Hessian of \(\nu(\cdot)\) satisfies

\[\text{Hessian}_{\nu}(\mu^{i})=\text{diag}\left(\mu^{i}\right)^{-1}\leq\frac{I _{|\mathcal{A}^{i}|}}{\min_{a^{i}\in\mathcal{A}^{i}}\mu^{i}(a^{i})}\leq\frac{I _{|\mathcal{A}^{i}|}}{\ell_{\tau}}.\]

Therefore, the gradient of the negative entropy function \(\nabla\nu(\cdot)\) is \(\frac{1}{\ell_{\tau}}\) - Lipschitz continuous with respect to \(\|\cdot\|_{2}\) on the set \(\{\mu^{i}\in\Delta(\mathcal{A}^{i})\mid\min_{a^{i}\in\mathcal{A}^{i}}\mu^{i}(a ^{i})\geq\ell_{\tau}\}\), which implies the \(\frac{1}{\ell_{\tau}}\) - smoothness of \(\nu(\cdot)\). 

**Lemma C.6**.: _For \(i\in\{1,2\}\), we have for all \(\mu^{i}\in\Delta(\mathcal{A}^{i})\) and \(\mu^{-i}\in\Delta(\mathcal{A}^{-i})\) that_

\[\|\sigma_{\tau}(R_{i}\mu^{-i})-\mu^{i}\|_{2}^{2}\leq\frac{2}{\tau}V_{R}(\mu^{ 1},\mu^{2}).\]

Proof of Lemma c.6.: Recall that the negative entropy \(\nu(\cdot)\) is \(1\)-strongly concave with respect to \(\|\cdot\|_{2}\). Therefore, given \(i\in\{1,2\}\), fix \(\mu^{-i}\), the function

\[\max_{\hat{\mu}^{i}\in\Delta(\mathcal{A}^{i})}\left\{(\hat{\mu}^{i}-\mu^{i})^{ \top}R_{i}\mu^{-i}+\tau\nu(\hat{\mu}^{i})-\tau\nu(\mu^{i})\right\}\]

is \(\tau\)-strongly convex with respect to \(\mu^{i}\). As a result, by the quadratic growth property of strongly convex functions, we have

\[\|\sigma_{\tau}(R_{i}\mu^{-i})-\mu^{i}\|_{2}^{2}\leq\frac{2}{\tau}\max_{\hat{ \mu}^{i}\in\Delta(\mathcal{A}^{i})}\left\{(\hat{\mu}^{i}-\mu^{i})^{\top}R_{i} \mu^{-i}+\tau\nu(\hat{\mu}^{i})-\tau\nu(\mu^{i})\right\}\leq\frac{2}{\tau}V_{R}( \mu^{1},\mu^{2}).\]Denote \(\Pi_{\tau}=\{(\pi^{1},\pi^{2})\in\Delta(\mathcal{A}^{1})\times\Delta(\mathcal{A}^{2 })\mid\min_{a^{1}\in\mathcal{A}^{1}}\pi^{1}(a^{1})\geq\ell_{\tau},\min_{a^{2} \in\mathcal{A}^{2}}\pi^{2}(a^{2})\geq\ell_{\tau}\}\).

**Lemma C.7**.: _The function \(V_{R}(\cdot,\cdot)\) has the following properties._

1. _The function_ \(V_{R}(\mu^{1},\mu^{2})\) _is_ \(L_{\tau}\) _- smooth on_ \(\Pi_{\tau}\)_, where_ \(L_{\tau}=\frac{\tau}{\ell_{\tau}}+\frac{A_{\max}^{2}}{\tau}\)_._
2. _It holds for any_ \((\mu^{1},\mu^{2})\in\Pi_{\tau}\) _that_ \[\langle\nabla_{1}V_{R}(\mu^{1},\mu^{2}),\sigma_{\tau}(R_{1}\mu^{2})-\mu^{1} \rangle+\langle\nabla_{2}V_{R}(\mu^{1},\mu^{2}),\sigma_{\tau}(R_{2}\mu^{1})- \mu^{2}\rangle\leq-V_{R}(\mu^{1},\mu^{2}).\]
3. _For any_ \(q^{1}\in\mathbb{R}^{|\mathcal{A}^{1}|}\) _and_ \(q^{2}\in\mathbb{R}^{|\mathcal{A}^{2}|}\) _, we have for all_ \((\mu^{1},\mu^{2})\in\Pi_{\tau}\) _that_ \[\langle\nabla_{1}V_{R}(\mu^{1},\mu^{2}),\sigma_{\tau}(q^{1})- \sigma_{\tau}(R_{1}\mu^{2})\rangle+\langle\nabla_{2}V_{R}(\mu^{1},\mu^{2}), \sigma_{\tau}(q^{2})-\sigma_{\tau}(R_{2}\mu^{1})\rangle\] \[\leq\frac{1}{2}V_{R}(\mu^{i},\mu^{-i})+4\left(\frac{1}{\tau\ell_ {\tau}^{2}}+\frac{A_{\max}^{2}}{\tau^{3}}\right)\sum_{i=1,2}\|q^{i}-R_{i}\mu^{ -i}\|_{2}^{2}.\]

Proof of Lemma c.7.: Recall the definition of \(V_{R}(\cdot,\cdot)\) in the following:

\[V_{R}(\mu^{1},\mu^{2})=\sum_{i=1,2}\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})} \left\{(\hat{\mu}^{i}-\mu^{i})^{\top}R_{i}\mu^{-i}+\tau\nu(\hat{\mu}^{i})- \tau\nu(\mu^{i})\right\}.\]

By Danskin's theorem [95], we have

\[\nabla_{1}V_{R}(\mu^{1},\mu^{2})= \ -\tau\nabla\nu(\mu^{1})+(R_{2})^{\top}\sigma_{\tau}(R_{2}\mu^{1}),\] \[\nabla_{2}V_{R}(\mu^{1},\mu^{2})= \ -\tau\nabla\nu(\mu^{2})+(R_{1})^{\top}\sigma_{\tau}(R_{1}\mu^{2}),\]

both of which will be frequently used in our analysis.

1. For any \((\mu^{1},\mu^{2}),(\bar{\mu}^{1},\bar{\mu}^{2})\in\Pi_{\tau}\), we have \[\|\nabla_{1}V_{R}(\mu^{1},\mu^{2})-\nabla_{1}V_{R}(\bar{\mu}^{1}, \bar{\mu}^{2})\|_{2}\] \[=\|\tau\nabla\nu(\bar{\mu}^{1})-\tau\nabla\nu(\mu^{1})+(R_{2})^{ \top}\sigma_{\tau}(R_{2}\mu^{1})-(R_{2})^{\top}\sigma_{\tau}(R_{2}\bar{\mu}^{ 1})\|_{2}\] \[\leq\tau\|\nabla\nu(\bar{\mu}^{1})-\nabla\nu(\mu^{1})\|_{2}+\|R_{ 2}\|_{2}\|\sigma_{\tau}(R_{2}\mu^{1})-\sigma_{\tau}(R_{2}\bar{\mu}^{1})\|_{2}\] \[\leq\frac{\tau}{\ell_{\tau}}\|\mu^{1}-\bar{\mu}^{1}\|_{2}+\frac{ \|R_{2}\|_{2}^{2}}{\tau}\|\mu^{1}-\bar{\mu}^{1}\|_{2}\] \[\leq\ \left(\frac{\tau}{\ell_{\tau}}+\frac{A_{\max}^{2}}{\tau} \right)\|\mu^{1}-\bar{\mu}^{1}\|_{2},\] where the second last inequality follows from Lemma C.5 and \(\sigma_{\tau}(\cdot)\) being \(\frac{1}{\tau}\)-Lipschitz continuous with respect to \(\|\cdot\|_{2}\)[96], and the last inequality follows from \(\|R_{i}\|_{2}\leq\sqrt{|\mathcal{A}^{1}||\mathcal{A}^{2}|}\leq A_{\max}\) for \(i\in\{1,2\}\). Similarly, we also have \[\|\nabla_{2}V_{R}(\mu^{1},\mu^{2})-\nabla_{2}V_{R}(\bar{\mu}^{1},\bar{\mu}^{2}) \|_{2}^{2}\leq\left(\frac{\tau}{\ell_{\tau}}+\frac{A_{\max}^{2}}{\tau}\right) \|\mu^{2}-\bar{\mu}^{2}\|_{2}.\] It follows from the previous two inequalities that \[\|\nabla V_{R}(\mu^{1},\mu^{2})-\nabla V_{R}(\bar{\mu}^{1},\bar{ \mu}^{2})\|_{2}^{2}\] \[=\|\nabla_{1}V_{R}(\mu^{1},\mu^{2})-\nabla_{1}V_{R}(\bar{\mu}^{1},\bar{\mu}^{2})\|_{2}^{2}+\|\nabla_{2}V_{R}(\mu^{1},\mu^{2})-\nabla_{2}V_{R}( \bar{\mu}^{1},\bar{\mu}^{2})\|_{2}^{2}\] \[\leq\ \left(\frac{\tau}{\ell_{\tau}}+\frac{A_{\max}^{2}}{\tau} \right)^{2}\sum_{i=1,2}\|\mu^{i}-\bar{\mu}^{i}\|_{2}^{2},\] which implies that \(V_{R}(\cdot,\cdot)\) is an \(L_{\tau}\) - smooth function on \(\Pi_{\tau}\)[97], where \(L_{\tau}=\frac{\tau}{\ell_{\tau}}+\frac{A_{\max}^{2}}{\tau}\).
2. The result follows from Lemma D.7 by setting \(X_{i}=R_{i}\), \(i\in\{1,2\}\), and by observing that \(R_{1}+(R_{2})^{\top}=0\).

3. Using the formula of the gradient of \(V_{R}(\cdot,\cdot)\) in the begining of the proof, we have \[\langle\nabla_{1}V_{R}(\mu^{1},\mu^{2}),\sigma_{\tau}(q^{1})- \sigma_{\tau}(R_{1}\mu^{2})\rangle\] \[= \langle-\tau\nabla\nu(\mu^{1})+(R_{2})^{\top}\sigma_{\tau}(R_{2} \mu^{1}),\sigma_{\tau}(q^{1})-\sigma_{\tau}(R_{1}\mu^{2})\rangle\] \[= \tau\langle\nabla\nu(\sigma_{\tau}(R_{1}\mu^{2}))-\nabla\nu(\mu^ {1}),\sigma_{\tau}(q^{1})-\sigma_{\tau}(R_{1}\mu^{2})\rangle\] \[+(\sigma_{\tau}(R_{2}\mu^{1})-\mu^{2})^{\top}R_{2}(\sigma_{\tau} (q^{1})-\sigma_{\tau}(R_{1}\mu^{2}))\] (This follows from the order optimality condition: \(R_{1}\mu^{2}+\tau\nabla\nu(\sigma_{\tau}(R_{1}\mu^{2}))=0\)) \[\leq \frac{\tau}{2c_{1}}\|\nabla\nu(\sigma_{\tau}(R_{1}\mu^{2}))- \nabla\nu(\mu^{1})\|_{2}^{2}+\frac{\tau c_{1}}{2}\|\sigma_{\tau}(q^{1})- \sigma_{\tau}(R_{1}\mu^{2})\|_{2}^{2}\] \[+\frac{1}{2c_{2}}\|\sigma_{\tau}(R_{2}\mu^{1})-\mu^{2}\|_{2}^{2}+ \frac{c_{2}}{2}\|R_{2}(\sigma_{\tau}(q^{1})-\sigma_{\tau}(R_{1}\mu^{2}))\|_{2 }^{2}\] (This follows from AM-GM inequality, where \(c_{1},c_{2}>0\) can be arbitrary) \[\leq \frac{\tau}{2c_{1}\ell_{\tau}^{2}}\|\sigma_{\tau}(R_{1}\mu^{2})- \mu^{1}\|_{2}^{2}+\frac{c_{1}}{2\tau}\|q^{1}-R_{1}\mu^{2}\|_{2}^{2}\] \[+\frac{1}{2c_{2}}\|\sigma_{\tau}(R_{2}\mu^{1})-\mu^{2})\|_{2}^{2} +\frac{c_{2}\|R_{2}\|_{2}^{2}}{2\tau^{2}}\|q^{1}-R_{1}\mu^{2}\|_{2}^{2}\] (Lemma C.5) \[\leq \left(\frac{1}{c_{1}\ell_{\tau}^{2}}+\frac{1}{\tau c_{2}}\right)V _{R}(\mu^{1},\mu^{2})+\frac{c_{1}}{2\tau}\|q^{1}-R_{1}\mu^{2}\|_{2}^{2}+\frac {c_{2}\|R_{2}\|_{2}^{2}}{2\tau^{2}}\|q^{1}-R_{1}\mu^{2}\|_{2}^{2}\] (Lemma C.6) \[\leq \frac{1}{4}V_{R}(\mu^{1},\mu^{2})+\frac{4}{\tau\ell_{\tau}^{2}}\| q^{1}-R_{1}\mu^{2}\|_{2}^{2}+\frac{4\|R_{2}\|_{2}^{2}}{\tau^{3}}\|q^{1}-R_{1}\mu^{2} \|_{2}^{2},\] where the last line follows by choosing \(c_{1}=\frac{8}{\ell_{\tau}^{2}}\) and \(c_{2}=\frac{8}{\tau}\). Similarly, we also have \[\langle\nabla_{2}V_{R}(\mu^{1},\mu^{2}),\sigma_{\tau}(q^{2})- \sigma_{\tau}(R_{2}\mu^{1})\rangle \leq \frac{1}{4}V_{R}(\mu^{1},\mu^{2})+\frac{4}{\tau\ell_{\tau}^{2}} \|q^{2}-R_{2}\mu^{1}\|_{2}^{2}\] \[+\frac{4\|R_{1}\|_{2}^{2}}{\tau^{3}}\|q^{2}-R_{2}\mu^{1}\|_{2}^{2}.\] Summing up the previous two inequalities, we obtain \[\langle\nabla_{1}V_{R}(\mu^{1},\mu^{2}),\sigma_{\tau}(q^{1})- \sigma_{\tau}(R_{1}\mu^{2})\rangle+\langle\nabla_{2}V_{R}(\mu^{1},\mu^{2}), \sigma_{\tau}(q^{2})-\sigma_{\tau}(R_{2}\mu^{1})\rangle\] \[\leq \frac{1}{2}V_{R}(\mu^{i},\mu^{-i})+\left(\frac{4}{\tau\ell_{\tau}^ {2}}+\frac{4A_{\max}^{2}}{\tau^{3}}\right)\sum_{i=1,2}\|q^{i}-R_{i}\mu^{-i}\|_ {2}^{2},\] where we used \(\|R_{i}\|_{2}\leq\sqrt{A_{\max}}\) for \(i\in\{1,2\}\).

## Appendix D Proof of Theorem 3.1

We begin by introducing a summary of notation in Appendix D.1. In Appendix D.2, we establish an important boundedness property regarding the \(q\)-functions, value functions, and the policies generated by Algorithm 2. In Appendix D.3, we bound the Nash gap in terms of the Lyapunov functions. In Appendices D.4 and D.5, we analyze the outer loop and the inner loop of Algorithm 2 and establish the Lyapunov drift inequalities. Finally, in Appendix D.6, we solve the coupled Lyapunov inequalities to obtain the finite-sample bound. The proof of all supporting lemmas are provided in Appendix D.7, and the proof of Corollary 3.1.1 is provided in Appendix D.8.

### Notation

We begin with a summary of the notation that will be used in the proof.

1. Given a pair of matrices \(\{X_{i}\in\mathbb{R}^{|\mathcal{A}^{i}|\times|\mathcal{A}^{-i}|}\}_{i\in\{1,2\}}\) and a pair of distributions \(\{\mu^{i}\in\Delta(\mathcal{A}^{i})\}_{i\in\{1,2\}},\) we define \[V_{X}(\mu^{1},\mu^{2})=\sum_{i=1,2}\max_{\hat{\mu}^{i}\in\Delta(\mathcal{A}^{i})} \left\{(\hat{\mu}^{i}-\mu^{i})^{\top}X_{i}\mu^{-i}+\tau\nu(\hat{\mu}^{i})-\tau \nu(\mu^{i})\right\},\] (26)where \(\nu(\cdot)\) is the entropy function. Note that \(V_{X}(\cdot,\cdot)\) is similar to \(V_{R}(\cdot,\cdot)\) defined in Appendix C.2 in the setting of matrix games. However, we do not assume that \(X_{1}+X_{2}=0\).
2. Given a pair of value functions \((v^{1},v^{2})\) and a state \(s\in\mathcal{S}\), when \(X_{i}=\mathcal{T}^{i}(v^{i})(s)\), \(i\in\{1,2\}\), we write \(V_{v,s}(\cdot,\cdot)\) for \(V_{X}(\cdot,\cdot)\).
3. For any joint policy \((\pi^{1},\pi^{2})\) and state \(s\), given \(i\in\{1,2\}\), we define \(v^{i}_{*,\pi^{-i}}(s)=\max_{\hat{\pi}^{i}}v^{i}_{\hat{\pi}^{i},\pi^{-i}}(s)\), \(v^{i}_{\pi^{i},*}=\min_{\hat{\pi}^{-i}}v^{i}_{\pi^{i},\hat{\pi}^{-i}}(s)\), \(v^{-i}_{\pi^{-i},*}(s)=\min_{\hat{\pi}^{i}}v^{-i}_{\pi^{-i},\hat{\pi}^{i}}(s)\), and \(v^{-i}_{*,*^{i}}(s)=\max_{\hat{\pi}^{-i}}v^{-i}_{\hat{\pi}^{-i},\hat{\pi}^{i}}(s)\). Note that we have \(v^{1}_{*,\pi^{2}}+v^{2}_{\pi^{2},*}=0\) and \(v^{1}_{\pi^{1},*}+v^{2}_{*,\pi^{1}}=0\) because of the zero-sum structure.
4. For \(i\in\{1,2\}\), denote \(v^{i}_{*}\) as the unique fixed point of the equation \(\mathcal{B}^{i}(v^{i})=v^{i}\), where \(\mathcal{B}^{i}(\cdot)\) is the minimax Bellman operator defined in Section 3. Note that we have \(v^{1}_{*}+v^{2}_{*}=0\).
5. For any \(t,k\geq 0\) and \(i\in\{1,2\}\), let \(\bar{q}^{i}_{t,k}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\) be defined as \(\bar{q}^{i}_{t,k}(s)=\mathcal{T}^{i}(v^{i}_{t})(s)\pi^{-i}_{t,k}(s)\) for all \(s\in\mathcal{S}\). In addition, let \[\mathcal{L}_{\text{sum}}(t) =\|v^{1}_{t}+v^{2}_{t}\|_{\infty},\quad\mathcal{L}_{v}(t)=\sum_{ i=1,2}\|v^{i}_{t}-v^{i}_{*}\|_{\infty},\] \[\mathcal{L}_{q}(t,k) =\sum_{i=1,2}\sum_{s\in\mathcal{S}}\|q^{i}_{t,k}(s)-\mathcal{T}^ {i}(v^{i}_{t})(s)\pi^{-i}_{t,k}(s)\|_{2}^{2}=\sum_{i=1,2}\|q^{i}_{t,k}-\bar{q} ^{i}_{t,k}\|_{2}^{2},\] \[\mathcal{L}_{\pi}(t,k) =\max_{s\in\mathcal{S}}V_{v_{t},s}(\pi^{1}_{t,k}(s),\pi^{2}_{t,k }(s)),\] which will be the Lyapunov functions we use in the analysis.
6. Given \(k_{1}\leq k_{2}\), we denote \(\beta_{k_{1},k_{2}}=\sum_{k=k_{1}}^{k_{2}}\beta_{k}\) and \(\alpha_{k_{1},k_{2}}=\sum_{k=k_{1}}^{k_{2}}\alpha_{k}\).
7. Recall that \(z_{k}=t(\ell_{\tau},\beta_{k})\) is the uniform mixing time defined in Lemma B.1 (2), where \(\ell_{\tau}\) is the uniform lower bound of the policies. When using constant stepsizes, \(z_{k}\) is not a function of \(k\), and is simply denoted by \(z_{\beta}\). Observe that, due to the uniform geometric mixing property established in Lemma B.1 (2), we have \(z_{k}=\mathcal{O}(\log(k))\) when using \(\mathcal{O}(1/k)\) stepsizes and \(z_{\beta}=\mathcal{O}(\log(1/\beta))\) when using constant stepsizes. Let \(k_{0}=\min k:k\geq z_{k}\), which is well defined because \(z_{k}\) grows logarithmically with \(k\).

### Boundedness of the Iterates

We first show in the following lemma that the \(q\)-functions and the \(v\)-functions generated by Algorithm 2 are uniformly bounded from above, and the policies are uniformly bounded from below. In the context of stochastic games, we redefine \(\ell_{\tau}=[1+(A_{\max}-1)\exp(2/[(1-\gamma)\tau])]^{-1}\).

**Lemma D.1**.: _For all \(t,k\) and \(i\in\{1,2\}\), we have (1) \(\|v^{i}_{t}\|_{\infty}\leq 1/(1-\gamma)\) and \(\|q^{i}_{t,k}\|_{\infty}\leq 1/(1-\gamma)\), and (2) \(\min_{s\in\mathcal{S},a^{i}\in\mathcal{A}^{i}}\pi^{i}_{t,k}(a^{i}\mid s)\geq\ell _{\tau}\)._

Proof of Lemma D.1.: The proof uses induction arguments. Let \(i\in\{1,2\}\).

1. Given \(t\geq 0\), we first show by induction that, if \(\|v^{i}_{t}\|_{\infty}\leq\frac{1}{1-\gamma}\) and \(\|q^{i}_{t,0}\|_{\infty}\leq\frac{1}{1-\gamma}\), we have \(\|q^{i}_{t,k}\|_{\infty}\leq\frac{1}{1-\gamma}\) for all \(k\geq 0\). The base case \(\|q^{i}_{t,0}\|_{\infty}\leq\frac{1}{1-\gamma}\) holds by our assumption. Suppose that \(\|q^{i}_{t,k}\|_{\infty}\leq\frac{1}{1-\gamma}\) for some \(k\geq 0\). Then, by Algorithm 2 Line \(6\), we have for all \((s,a^{i})\) that \[|q^{i}_{t,k+1}(s,a^{i})|\] \[=|q^{i}_{t,k}(s,a^{i})+\alpha_{k}\mathds{1}_{\{(s,a^{i})=(S_{k},A ^{i}_{k})\}}(R_{i}(S_{k},A^{i}_{k},A^{-i}_{k})+\gamma v^{i}_{t}(S_{k+1})-q^{i}_{ t,k}(S_{k},A^{i}_{k}))|\] \[\leq(1-\alpha_{k}\mathds{1}_{\{(s,a^{i})=(S_{k},A^{i}_{k})\}})|q^{ i}_{t,k}(s,a^{i})|\] \[\quad+\alpha_{k}\mathds{1}_{\{(s,a^{i})=(S_{k},A^{i}_{k})\}}|R_{i}( S_{k},A^{i}_{k},A^{-i}_{k})+\gamma v^{i}_{t}(S_{k+1})|\] \[\leq(1-\alpha_{k}\mathds{1}_{\{(s,a^{i})=(S_{k},A^{i}_{k})\}})\frac {1}{1-\gamma}+\alpha_{k}\mathds{1}_{\{(s,a^{i})=(S_{k},A^{i}_{k})\}}\left(1+ \frac{\gamma}{1-\gamma}\right)\] (27) \[=\frac{1}{1-\gamma},\]where Eq. (27) follows from the induction hypothesis \(\|q^{i}_{t,k}\|_{\infty}\leq\frac{1}{1-\gamma}\), our assumption that \(\|v^{i}_{t}\|_{\infty}\leq\frac{1}{1-\gamma}\), and \(\max_{s,a^{i},a^{-i}}|R_{i}(s,a^{i},a^{-i})|\leq 1\). The induction is now complete and we have \(\|q^{i}_{t,k}\|_{\infty}\leq\frac{1}{1-\gamma}\) for all \(k\geq 0\) whenever \(\|v^{i}_{t}\|_{\infty}\leq\frac{1}{1-\gamma}\) and \(\|q^{i}_{t,0}\|_{\infty}\leq\frac{1}{1-\gamma}\). We next again use induction to show that \(\|v^{i}_{t}\|_{\infty}\leq\frac{1}{1-\gamma}\) and \(\|q^{i}_{t,0}\|_{\infty}\leq\frac{1}{1-\gamma}\) for all \(t\geq 0\). Our initialization ensures that \(\|v^{i}_{0}\|_{\infty}\leq\frac{1}{1-\gamma}\) and \(\|q^{i}_{0,0}\|_{\infty}\leq\frac{1}{1-\gamma}\). Suppose that \(\|v^{i}_{t}\|_{\infty}\leq\frac{1}{1-\gamma}\) and \(\|q^{i}_{t,0}\|_{\infty}\leq\frac{1}{1-\gamma}\) for some \(t\geq 0\). Using the update equation for \(v^{i}_{t+1}\) (cf. Algorithm 2 Line \(8\)) and the fact that \(\|q^{i}_{t,k}\|_{\infty}\leq\frac{1}{1-\gamma}\) for all \(k\geq 0\) (established in the previous paragraph), we have for all \(s\in\mathcal{S}\) that \[|v^{i}_{t+1}(s)|=\left|\sum_{a^{i}\in\mathcal{A}^{i}}\pi^{i}_{t,K}(a^{i}|s)q^ {i}_{t,K}(s,a^{i})\right|\leq\sum_{a^{i}\in\mathcal{A}^{i}}\pi^{i}_{t,K}(a^{i }|s)\|q^{i}_{t,K}\|_{\infty}\leq\frac{1}{1-\gamma},\] which implies \(\|v^{i}_{t+1}\|_{\infty}\leq\frac{1}{1-\gamma}\). Moreover, we have by Algorithm 2 Line \(9\) that \(\|q^{i}_{t+1,0}\|_{\infty}=\|q^{i}_{t,K}\|_{\infty}\leq\frac{1}{1-\gamma}\). The induction is now complete and we have \(\|v^{i}_{t}\|_{\infty}\leq\frac{1}{1-\gamma}\) and \(\|q^{i}_{t,0}\|_{\infty}\leq\frac{1}{1-\gamma}\) for all \(t\geq 0\).
2. We first use induction to show that, given \(t\geq 0\), if \(\min_{s,a^{i}}\pi^{i}_{t,0}(a^{i}\mid s)\geq\ell_{\tau}\), then we have \(\min_{s,a^{i}}\pi^{i}_{t,k}(a^{i}\mid s)\geq\ell_{\tau}\) for all \(k\in\{0,1,\cdots,K\}\). Since \(\min_{s,a^{i}}\pi^{i}_{t,0}(a^{i}\mid s)\geq\ell_{\tau}\) by our assumption, we have the base case. Now suppose that \(\min_{s\in\mathcal{S},a^{i}\in\mathcal{A}^{i}}\pi^{i}_{t,k}(a^{i}\mid s)\geq \ell_{\tau}\) for some \(k\geq 0\). Then we have by Algorithm 2 Line \(4\) that \[\pi^{i}_{t,k+1}(a^{i}\mid s) =(1-\beta_{k})\pi^{i}_{t,k}(a^{i}\mid s)+\beta_{k}\sigma_{\tau}( q^{i}_{t,k}(s))(a^{i})\] \[\geq(1-\beta_{k})\ell_{\tau}+\beta_{k}\ell_{\tau}\] \[=\ell_{\tau},\] where the inequality follows from (1) the induction hypothesis, and (2) \(\sigma_{\tau}(q^{i}_{t,k}(s))(a^{i})\geq\ell_{\tau}\), which follows from Lemma D.1 (1) and Lemma C.1. The induction is complete. We next again use induction to show that \(\min_{s,a^{i}}\pi^{i}_{t,0}(a^{i}\mid s)\geq\ell_{\tau}\) for all \(t\in\{0,1,\cdots,T\}\). Since \(\min_{s,a^{i}}\pi^{i}_{0,0}(a^{i}\mid s)\) is initialized as a uniform policy, we have the base case. Now suppose that \(\min_{s,a^{i}}\pi^{i}_{t,0}(a^{i}\mid s)\geq\ell_{\tau}\) for some \(t\geq 0\). Since this implies that \(\min_{s,a^{i}}\pi^{i}_{t,k}(a^{i}\mid s)\geq\ell_{\tau}\) for all \(k\in\{0,1,\cdots,K\}\), and in addition, \(\pi^{i}_{t+1,0}=\pi^{i}_{t,K}\) according to Algorithm 2 Line \(9\), we have \(\min_{s,a^{i}}\pi^{i}_{t+1,0}(a^{i}\mid s)\geq\ell_{\tau}\). The induction is complete.

### Bounding the Nash Gap

Our ultimate goal is to bound the Nash gap

\[\text{NG}(\pi^{1}_{T,K},\pi^{2}_{T,K})=\sum_{i=1,2}\left(\max_{\pi^{i}}U^{i}( \pi^{i},\pi^{-i}_{T,K})-U^{i}(\pi^{i}_{T,K},\pi^{-i}_{T,K})\right).\] (28)

We first bound the Nash gap using the value functions of the output policies from Algorithm 2.

**Lemma D.2**.: _It holds that_

\[\sum_{i=1,2}\left(\max_{\pi^{i}}U^{i}(\pi^{i},\pi^{-i}_{T,K})-U^{i}(\pi^{i}_{T, K},\pi^{-i}_{T,K})\right)\leq\sum_{i=1,2}\left\|v^{i}_{*,\pi^{-i}_{T,K}}-v^{i}_{ \pi^{-i}_{T,K},\pi^{-i}_{T,K}}\right\|_{\infty}.\] (29)

Proof of Lemma D.2.: Using the definition of the expected value functions, we have

\[\sum_{i=1,2}\left(\max_{\pi^{i}}U^{i}(\pi^{i},\pi^{-i}_{T,K})-U^{i}(\pi^{i}_{T, K},\pi^{-i}_{T,K})\right)\]\[= \sum_{i=1,2}\left(\max_{\pi^{i}}\mathbb{E}_{S\sim p_{o}}\left[v^{i}_{ \pi^{i},\pi^{-i}_{T,K}}(S)-v^{i}_{\pi^{i}_{T,K},\pi^{-i}_{T,K}}(S)\right]\right)\] \[\leq \sum_{i=1,2}\left(\mathbb{E}_{S\sim p_{o}}\left[\max_{\pi^{i}}v^{i }_{\pi^{i},\pi^{-i}_{T,K}}(S)-v^{i}_{\pi^{i}_{T,K},\pi^{-i}_{T,K}}(S)\right] \right)\] (Jensen's inequality) \[= \sum_{i=1,2}\left(\mathbb{E}_{S\sim p_{o}}\left[v^{i}_{*,\pi^{-i}_ {T,K}}(S)-v^{i}_{\pi^{i}_{T,K},\pi^{-i}_{T,K}}(S)\right]\right)\] \[\leq \sum_{i=1,2}\left\|v^{i}_{*,\pi^{-i}_{T,K}}-v^{i}_{\pi^{i}_{T,K}, \pi^{-i}_{T,K}}\right\|_{\infty}.\]

The next lemma bounds the RHS of Eq. (29) using the actual iterates generated by Algorithm 2.

**Lemma D.3**.: _It holds for \(i\in\{1,2\}\) that_

\[\left\|v^{i}_{*,\pi^{-i}_{i,K}}-v^{i}_{\pi^{i}_{i,K},\pi^{-i}_{i,K}}\right\|_ {\infty}\leq\frac{2}{1-\gamma}\left(2\mathcal{L}_{\text{sum}}(T)+\mathcal{L} _{v}(T)+\mathcal{L}_{\pi}(T,K)+2\tau\log(A_{\max})\right).\]

Proof of Lemma D.3.: For any \(s\in\mathcal{S}\) and \(i\in\{1,2\}\), we have

\[0 \leq \left|v^{i}_{*,\pi^{-i}_{T,K}}(s)-v^{i}_{\pi^{i}_{T,K},\pi^{-i}_{ T,K}}(s)\right|\] \[= v^{i}_{*,\pi^{-i}_{T,K}}(s)-v^{i}_{\pi^{i}_{T,K},\pi^{-i}_{T,K}}(s)\] \[\leq v^{i}_{*,\pi^{-i}_{T,K}}(s)-v^{i}_{\pi^{i}_{T,K},*}(s)\] \[= -v^{-i}_{\pi^{-i}_{T,K},*}(s)-v^{i}_{\pi^{i}_{T,K},*}(s)\] \[= v^{i}_{*}(s)-v^{-i}_{\pi^{-i}_{T,K},*}(s)+v^{-i}_{*}(s)-v^{i}_{ \pi^{i}_{T,K},*}(s)\] \[\leq \sum_{j=1,2}\left\|v^{-j}_{*}-v^{-j}_{\pi^{j}_{T,K},*}\right\|_ {\infty}.\]

Since the RHS of the previous inequality does not depend on \(s\), we have for \(i\in\{1,2\}\) that

\[\left\|v^{i}_{*,\pi^{-i}_{T,K}}-v^{i}_{\pi^{i}_{T,K},\pi^{-i}_{T,K}}\right\|_ {\infty}\leq\sum_{j=1,2}\left\|v^{-j}_{*}-v^{-j}_{\pi^{j}_{T,K},*}\right\|_{ \infty}.\] (30)

It remains to bound the RHS of the previous inequality. Observe that for any \(s\in\mathcal{S}\) and \(i\in\{1,2\}\), we have

\[0 \leq v^{-i}_{*}(s)-v^{-i}_{\pi^{-i}_{T,K},*}(s)\] \[= v^{i}_{*,\pi^{-i}_{T,K}}(s)-v^{i}_{*}(s)\] \[= \max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top}\mathcal{ T}^{i}(v^{i}_{*,\pi^{-i}_{T,K}})(s)\pi^{-i}_{T,K}(s)-\max_{\mu^{i}\in\Delta( \mathcal{A}^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v^{i}_{*})(s)\mu^{-i}\] \[\leq \left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v^{i}_{*,\pi^{-i}_{T,K}})(s)\pi^{-i}_{T,K}(s)-\max_{\mu^{i}\in \Delta(\mathcal{A}^{i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v^{i}_{*})(s)\pi^{-i}_{ T,K}(s)\right|\] \[+\left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v^{i}_{*})(s)\pi^{-i}_{T,K}(s)-\max_{\mu^{i}\in\Delta(\mathcal{A }^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top}\mathcal{T}^ {i}(v^{i}_{*})(s)\mu^{-i}\right|\] \[\leq \max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\left|(\mu^{i})^{\top}( \mathcal{T}^{i}(v^{i}_{*,\pi^{-i}_{T,K}})(s)-\mathcal{T}^{i}(v^{i}_{*})(s))\pi^ {-i}_{T,K}(s)\right|\] \[+\left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v^{i}_{*})(s)\pi^{-i}_{T,K}(s)-\max_{\mu^{i}\in\Delta(\mathcal{ A}^{i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v^{i}_{T})(s)\pi^{-i}_{T,K}(s)\right|\] \[+\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top}\mathcal{ T}^{i}(v^{i}_{T})(s)\pi^{-i}_{T,K}(s)-\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})} \min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v^{i}_{ T})(s)\mu^{-i}\]\[+\left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\min_{\mu^{-i}\in \Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v_{T}^{i})(s)\mu^{-i}- \max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{- i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v_{*}^{i})(s)\mu^{-i}\right|.\] (31)

We next bound each term on the RHS of the previous inequality.

**The \(1\)st Term on the RHS of Eq. (31).** Using the definition of \(\mathcal{T}^{i}(\cdot)\), we have

\[\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\left|(\mu^{i})^{\top}( \mathcal{T}^{i}(v_{*,\pi_{T,K}^{-i}}^{i})(s)-\mathcal{T}^{i}(v_{*}^{i})(s)) \pi_{T,K}^{-i}(s)\right|\] \[\leq \max_{s,a^{i},a^{-i}}\left|\mathcal{T}^{i}(v_{*,\pi_{T,K}^{-i}}^{ i})(s,a^{i},a^{-i})-\mathcal{T}^{i}(v_{*}^{i})(s,a^{i},a^{-i})\right|\] \[= \gamma\max_{s,a^{i},a^{-i}}\left|\mathbb{E}\left[v_{*}^{i}(S_{1} )-v_{*,\pi_{T,K}^{-i}}^{i}(S_{1})\;\Big{|}\;S_{0}=s,A_{0}^{i}=a^{i},A_{0}^{-i} =a^{-i}\right]\right|\] \[\leq \gamma\max_{s,a^{i},a^{-i}}\mathbb{E}\left[\left|v_{*}^{i}(S_{1} )-v_{*,\pi_{T,K}^{-i}}^{i}(S_{1})\right|\;\Big{|}\;S_{0}=s,A_{0}^{i}=a^{i},A_{ 0}^{-i}=a^{-i}\right]\] \[\leq \gamma\left\|v_{*}^{i}-v_{*,\pi_{T,K}^{-i}}^{i}\right\|_{\infty}.\]

**The \(2\)nd Term on the RHS of Eq. (31).** Using the definition of \(\mathcal{T}^{i}(\cdot)\), we have

\[\left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v_{*}^{i})(s)\pi_{T,K}^{-i}(s)-\max_{\mu^{i}\in\Delta( \mathcal{A}^{i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v_{T}^{i})(s)\pi_{T,K}^{-i}( s)\right|\] \[\leq \max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}\left|(\mu^{i})^{\top}( \mathcal{T}^{i}(v_{*}^{i})(s)-\mathcal{T}^{i}(v_{T}^{i})(s))\pi_{T,K}^{-i}(s) \right|\] \[\leq \max_{s,a^{i},a^{-i}}\left|\mathcal{T}^{i}(v_{*}^{i})(s,a^{i},a^{ -i})-\mathcal{T}^{i}(v_{T}^{i})(s,a^{i},a^{-i})\right|\] \[\leq \gamma\left\|v_{*}^{i}-v_{T}^{i}\right\|_{\infty}.\]

**The \(3\)rd Term on the RHS of Eq. (31).** Bounding the third term requires more effort. To begin with, we decompose it in the following way:

\[\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top}\mathcal{ T}^{i}(v_{T}^{i})(s)\pi_{T,K}^{-i}(s)-\max_{\mu^{i}\in\Delta(\mathcal{A}^{i}) \,\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v_{T}^{ i})(s)\mu^{-i}\] \[\leq \left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v_{T}^{i})(s)\pi_{T,K}^{-i}(s)-\min_{\mu^{-i}\in\Delta( \mathcal{A}^{-i})}\pi_{T,K}^{i}(s)\mathcal{T}^{i}(v_{T}^{i})(s)\mu^{-i}\right|\] \[\leq \left|\max_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{-i})^{\top} \mathcal{T}^{-i}(v_{T}^{-i})(s)\pi_{T,K}^{i}(s)+\min_{\mu^{-i}\in\Delta( \mathcal{A}^{-i})}(\mu^{-i})^{\top}\mathcal{T}^{i}(v_{T}^{i})(s)^{\top}\pi_{T,K }^{i}(s)\right|\] \[+\left|\sum_{i=1,2}\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{ i})^{\top}\mathcal{T}^{i}(v_{T}^{i})(s)\pi_{T,K}^{-i}(s)\right|.\] (32)

We next bound each term on the RHS of the previous inequality. For the first term, we have by definition of \(\mathcal{T}^{i}(\cdot)\) that

\[\left|\max_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{-i})^{\top} \mathcal{T}^{-i}(v_{T}^{-i})(s)\pi_{T,K}^{i}(s)+\min_{\mu^{-i}\in\Delta( \mathcal{A}^{-i})}(\mu^{-i})^{\top}\mathcal{T}^{i}(v_{T}^{i})(s)^{\top}\pi_{T,K}^{i}(s)\right|\] \[= \left|\max_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{-i})^{\top} \mathcal{T}^{-i}(v_{T}^{-i})(s)\pi_{T,K}^{i}(s)-\max_{\mu^{-i}\in\Delta( \mathcal{A}^{-i})}(\mu^{-i})^{\top}[-\mathcal{T}^{i}(v_{T}^{i})(s)]^{\top}\pi_{T,K}^{i}(s)\right|\] \[\leq \max_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}\left|(\mu^{-i})^{\top} (\mathcal{T}^{-i}(v_{T}^{-i})(s)+\mathcal{T}^{i}(v_{T}^{i})(s)^{\top})\pi_{T,K}^{i}(s)\right|\] \[\leq \max_{s,a^{i},a^{-i}}\left|\mathcal{T}^{-i}(v_{T}^{-i})(s,a^{i},a^ {-i})+\mathcal{T}^{i}(v_{T}^{i})(s,a^{i},a^{-i})\right|\] \[= \gamma\max_{s,a^{i},a^{-i}}\left|\mathbb{E}\left[v_{T}^{-i}(S_{1} )+v_{T}^{i}(S_{1})\;\big{|}\;S_{0}=s,A_{0}^{i}=a^{i},A_{0}^{-i}=a_{0}^{-i} \right]\right|\] \[\leq \gamma\left\|v_{T}^{-i}+v_{T}^{i}\right\|_{\infty}.\]

[MISSING_PAGE_EMPTY:33]

Combining the results in Lemma D.2 and Lemma D.3 in Eq. (28), we have the following result, which bounds the Nash gap in terms of the Lyapunov functions defined in Appendix D.1.

**Lemma D.4**.: _It holds that_

\[\text{NG}(\pi_{T,K}^{1},\pi_{T,K}^{2})\leq\frac{4}{1-\gamma}\left(2\mathcal{L}_ {\text{sum}}(T)+\mathcal{L}_{v}(T)+\mathcal{L}_{\pi}(T,K)+2\tau\log(A_{\max}) \right).\]

The next step is to bound the Lyapunov functions, which require us to analyze the outer loop and inner loop of Algorithm 2.

### Analysis of the Outer Loop: \(v\)-Function Update

We first consider \(\mathcal{L}_{v}(t)\) and establish a one-step Lyapunov drift inequality for it.

**Lemma D.5**.: _It holds for all \(t\geq 0\) that_

\[\mathcal{L}_{v}(t+1)\leq\gamma\mathcal{L}_{v}(t)+4\mathcal{L}_{\text{sum}}(t )+2\mathcal{L}_{q}^{1/2}(t,K)+4\mathcal{L}_{\pi}(t,K)+6\tau\log(A_{\max}).\] (34)

Proof of Lemma D.5.: For \(i\in\{1,2\}\), using the outer-loop update equation (cf. Algorithm 2 Line 8) and the fact that \(\mathcal{B}^{i}(v_{*}^{i})=v_{*}^{i}\), we have for any \(t\geq 0\) and \(s\in\mathcal{S}\) that

\[v_{t+1}^{i}(s)-v_{*}^{i}(s) =\pi_{t,K}^{i}(s)^{\top}q_{t,K}^{i}(s)-v_{*}^{i}(s)\] \[=\mathcal{B}^{i}(v_{t}^{i})(s)-\mathcal{B}^{i}(v_{*}^{i})(s)+\pi _{t,K}^{i}(s)^{\top}q_{t,K}^{i}(s)-\mathcal{B}^{i}(v_{t}^{i})(s).\]

Since the minimax Bellman operator \(\mathcal{B}^{i}(\cdot)\) is a \(\gamma\)-contraction mapping with respect to \(\|\cdot\|_{\infty}\), we have from the previous inequality that

\[\left|v_{t+1}^{i}(s)-v_{*}^{i}(s)\right| \leq\left|\mathcal{B}^{i}(v_{t}^{i})(s)-\mathcal{B}^{i}(v_{*}^{i} )(s)\right|+\left|\pi_{t,K}^{i}(s)^{\top}q_{t,K}^{i}(s)-\mathcal{B}^{i}(v_{t} ^{i})(s)\right|\] \[\leq\left|\mathcal{B}^{i}(v_{t}^{i})-\mathcal{B}^{i}(v_{*}^{i}) \right|_{\infty}+\left|\pi_{t,K}^{i}(s)^{\top}q_{t,K}^{i}(s)-\mathcal{B}^{i}(v _{t}^{i})(s)\right|\] \[\leq\gamma\left\|v_{t}^{i}-v_{*}^{i}\right\|_{\infty}+\left|\pi _{t,K}^{i}(s)^{\top}q_{t,K}^{i}(s)-\mathcal{B}^{i}(v_{t}^{i})(s)\right|.\] (35)

It remains to bound the second term on the RHS of Eq. (35). Using the definition of \(\mathcal{B}^{i}(\cdot)\), we have

\[\left|\pi_{t,K}^{i}(s)^{\top}q_{t,K}^{i}(s)-\mathcal{B}^{i}(v_{t} ^{i})(s)\right|\] \[=\] \[\leq \left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v_{t}^{i})(s)\pi_{t,K}^{-i}(s)-\pi_{t,K}^{i}(s)^{\top}q_{t,K} ^{i}(s)\right|\] \[+\left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v_{t}^{i})(s)\pi_{t,K}^{-i}(s)-\max_{\mu^{i}\in\Delta( \mathcal{A}^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v_{t}^{i})(s)\mu^{-i}\right|\] \[\leq \max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i}-\pi_{t,K}^{i}(s) )^{\top}\mathcal{T}^{i}(v_{t}^{i})(s)\pi_{t,K}^{-i}(s)\] \[+\left|(\pi_{t,K}^{i}(s))^{\top}(\mathcal{T}^{i}(v_{t}^{i})(s)\pi _{t,K}^{-i}(s)-q_{t,K}^{i}(s))\right|\] \[+\left|\max_{\mu^{i}\in\Delta(\mathcal{A}^{i})}(\mu^{i})^{\top} \mathcal{T}^{i}(v_{t}^{i})(s)\pi_{t,K}^{-i}(s)-\max_{\mu^{i}\in\Delta(\mathcal{ A}^{i})}\min_{\mu^{-i}\in\Delta(\mathcal{A}^{-i})}(\mu^{i})^{\top}\mathcal{T}^{i}(v_{t}^{i })(s)\mu^{-i}\right|\] \[\leq \left\|\mathcal{T}^{i}(v_{t}^{i})(s)\pi_{t,K}^{-i}(s)-q_{t,K}^{i} (s)\right\|_{\infty}+2V_{v_{t},s}(\pi_{t,K}^{1}(s),\pi_{t,K}^{2}(s))\] \[+2\gamma\|v_{t}^{1}+v_{t}^{2}\|_{\infty}+3\tau\log(A_{\max}),\]

where the last line follows from Eq. (33). Using the previous inequality in Eq. (35), we obtain

\[\left\|v_{t+1}^{i}-v_{*}^{i}\right\|_{\infty} \leq\gamma\left\|v_{t}^{i}-v_{*}^{i}\right\|_{\infty}+\max_{s\in \mathcal{S}}\left\|\mathcal{T}^{i}(v_{t}^{i})(s)\pi_{t,K}^{-i}(s)-q_{t,K}^{i}( s)\right\|_{\infty}\] \[\quad+2\max_{s\in\mathcal{S}}V_{v_{t},s}(\pi_{t,K}^{1}(s),\pi_{t,K} ^{2}(s))+2\gamma\|v_{t}^{1}+v_{t}^{2}\|_{\infty}+3\tau\log(A_{\max}).\]

Summing up both sides of the previous inequality for \(i\in\{1,2\}\), we have

\[\mathcal{L}_{v}(t+1)\leq\gamma\mathcal{L}_{v}(t)+4\mathcal{L}_{\text{sum}}(t)+4 \mathcal{L}_{\pi}(t,K)+6\tau\log(A_{\max})\]\[\leq \sum_{i=1,2}\max_{s\in\mathcal{S}}\|q^{i}_{t,K}(s)-\mathcal{T}^{i}(v^{i}_ {t})(s)\pi_{t,K}^{-i}(s)\|_{\infty}\] \[+\sum_{i=1,2}\max_{(s,a^{i},a^{-i})}\left|\mathcal{T}^{i}(v^{i}_{t })(s,a^{i},a^{-i})+\mathcal{T}^{-i}(v^{-i}_{t})(s,a^{i},a^{-i})\right|\] \[\leq \sum_{i=1,2}\max_{s\in\mathcal{S}}\|q^{i}_{t,K}(s)-\mathcal{T}^{i }(v^{i}_{t})(s)\pi_{t,K}^{-i}(s)\|_{\infty}+\gamma\|v^{1}_{t}+v^{2}_{t}\|_{ \infty},\]

where the last line follows from the definition of \(\mathcal{T}^{i}(\cdot)\). Since the RHS of the previous inequality does not depend on \(s\), we have

\[\|v^{1}_{t+1}+v^{2}_{t+1}\|_{\infty}\leq\gamma\|v^{1}_{t}+v^{2}_{t}\|_{\infty }+\sum_{i=1,2}\max_{s\in\mathcal{S}}\|q^{i}_{t,K}(s)-\mathcal{T}^{i}(v^{i}_{t })(s)\pi_{t,K}^{-i}(s)\|_{\infty}.\]

The result follows from using Eq. (36) to bound the last term on the RHS of the previous inequality and then using \(\mathcal{L}_{\text{sum}}(t)\) and \(\mathcal{L}_{q}(t,k)\) to simplify the notation. 

### Analysis of the Inner Loop

In this section, we establish negative drift inequalities for the Lyapunov functions \(\mathcal{L}_{q}(t,k)\) and \(\mathcal{L}_{\pi}(t,k)\), which are defined in terms of the \(q\)-functions and the policies updated in the inner loop of Algorithm 2. For ease of presentation, we write down only the inner loop of Algorithm 2 in Algorithm 3, where we omit the subscript \(t\), which is used as the index for the outer loop. Similarly, we will write \(\mathcal{L}_{q}(k)\) for \(\mathcal{L}_{q}(t,k)\) and \(\mathcal{L}_{\pi}(k)\) for \(\mathcal{L}_{\pi}(t,k)\). All results derived for the \(q\)-functions and policies of Algorithm 3 can be directly combined with the analysis of outer loop of Algorithm 2 using a conditioning argument together with the Markov property.

#### d.5.1 Analysis of the Policies

We consider \(\{(\pi^{1}_{k},\pi^{2}_{k})\}_{k\geq 0}\) generated by Algorithm 3 and use \(V_{X}(\cdot,\cdot)\) defined in Appendix D.1 as the Lyapunov function to study them. For simplicity of notation, we use \(\nabla_{1}V_{X}(\cdot,\cdot)\) (respectively, \(\nabla_{2}V_{X}(\cdot,\cdot)\)) to denote the gradient with respect to the first argument (respectively, the second argument). Recall that Lemma D.1 implies that \(\pi_{k}=(\pi^{1}_{k},\pi^{2}_{k})\in\Pi_{\tau}\) for all \(k\geq 0\), where \(\Pi_{\tau}=\{(\pi^{1},\pi^{2})\in\Delta(\mathcal{A}^{1})\times\Delta(\mathcal{ A}^{2})\mid\min_{a^{1}\in\mathcal{A}^{1}}\pi^{1}(a^{1})\geq\ell_{\tau},\min_{a^{2} \in\mathcal{A}^{2}}\pi^{2}(a^{2})\geq\ell_{\tau}\}\). The following lemma establishes the properties of \(V_{X}(\cdot,\cdot)\).

**Lemma D.7**.: _The function \(V_{X}(\cdot,\cdot)\) has the following properties._

1. _For_ \(i\in\{1,2\}\)_, fix_ \(\mu^{-i}\in\Delta(\mathcal{A}^{-i})\)_, the function_ \(V_{X}(\mu^{i},\mu^{-i})\) _as a function of_ \(\mu^{i}\) _is_ \(\tau\) _- strongly convex with respect to_ \(\|\cdot\|_{2}\)_._
2. \(V_{X}(\cdot,\cdot)\) _is_ \(\tilde{L}_{\tau}\) _- smooth on_ \(\Pi_{\tau}\)_, where_ \(\tilde{L}_{\tau}=2\left(\frac{\tau}{\ell_{\tau}}+\frac{\max(\|X_{1}\|_{2}^{2}, \|X_{2}\|_{2}^{2})}{\tau}+\|X_{1}+X_{2}^{\top}\|_{2}\right)\)_._
3. _It holds for any_ \((\mu^{1},\mu^{2})\in\Delta(\mathcal{A}^{1})\times\Delta(\mathcal{A}^{2})\) _that_ \[\langle\nabla_{1}V_{X}(\mu^{1},\mu^{2}),\sigma_{\tau}(X_{1}\mu^{2 })-\mu^{1}\rangle+\langle\nabla_{2}V_{X}(\mu^{1},\mu^{2}),\sigma_{\tau}(X_{2} \mu^{1})-\mu^{2}\rangle\] \[\leq -\frac{7}{8}V_{X}(\mu^{1},\mu^{2})+\frac{16}{\tau}\|X_{1}+X_{2}^{ \top}\|_{2}^{2}.\]
4. _For any_ \(u^{1}\in\mathbb{R}^{|\mathcal{A}^{1}|}\) _and_ \(u^{2}\in\mathbb{R}^{|\mathcal{A}^{2}|}\)_, we have for all_ \((\mu^{1},\mu^{2})\in\Pi_{\tau}\) _that_ \[\langle\nabla_{1}V_{X}(\mu^{1},\mu^{2}),\sigma_{\tau}(u^{1})- \sigma_{\tau}(X_{1}\mu^{2})\rangle+\langle\nabla_{2}V_{X}(\mu^{1},\mu^{2}), \sigma_{\tau}(u^{2})-\sigma_{\tau}(X_{2}\mu^{1})\rangle\] \[\leq\frac{1}{8}V_{X}(\mu^{1},\mu^{2})+\frac{8}{\tau}\left(\frac{1 }{\ell_{\tau}}+\frac{\max(\|X_{1}\|_{2},\|X_{2}\|_{2})}{\tau}\right)^{2}\sum_{ i=1,2}\|u^{i}-X_{i}\mu^{-i}\|_{2}^{2}.\]

Proof of Lemma d.7.: To begin with, we have by Danskin's theorem [95] that

\[\nabla_{1}V_{X}(\mu^{1},\mu^{2})= -(X_{1}+X_{2}^{\top})\mu^{2}-\tau\nabla\nu(\mu^{1})+X_{2}^{\top} \sigma_{\tau}(X_{2}\mu^{1}).\] (37)

Similar result holds for \(\nabla_{2}V_{X}(\mu^{1},\mu^{2})\).

1. It is clear that the function \(V_{X}(\cdot,\cdot)\) is non-negative. The strong convexity follows from the following two observations. 1. The negative entropy \(-\nu(\cdot)\) is \(1\) - strongly convex with respect to \(\|\cdot\|_{2}\)[97, Example 5.27]. 2. Given \(i\in\{1,2\}\), the function \(\max_{\hat{\mu}^{-i}\in\Delta(\mathcal{A}^{-i})}\left\{(\hat{\mu}^{-i})^{\top} X_{-i}\mu^{i}+\tau\nu(\hat{\mu}^{-i})\right\}\) as a function of \(\mu^{i}\) is the maximum of linear functions in \(\mu^{i}\), and therefore is convex. It follows that, for any \(i\in\{1,2\}\), the function \(V_{X}(\mu^{1},\mu^{2})\) is \(\tau\) - strongly convex in \(\mu^{i}\) with respect to \(\|\cdot\|_{2}\) uniformly for all \(\mu^{-i}\).

[MISSING_PAGE_FAIL:37]

\[\leq \left(\frac{\tau}{\ell_{\tau}}\|\sigma_{\tau}(X_{1}\mu^{2})-\mu^{ 1}\|_{2}+\|\sigma_{\tau}(X_{2}\mu^{1})-\mu^{2}\|_{2}\|X_{2}\|_{2}\right)\frac{1} {\tau}\|u^{1}-X_{1}\mu^{2}\|_{2}\] \[\leq \frac{\sqrt{2}}{\sqrt{\tau}}\left(\frac{1}{\ell_{\tau}}+\frac{\|X _{2}\|_{2}}{\tau}\right)V_{X}(\mu^{1},\mu^{2})^{1/2}\|u^{1}-X_{1}\mu^{2}\|_{2}\] \[\leq \frac{1}{16}V_{X}(\mu^{1},\mu^{2})+\frac{8}{\tau}\left(\frac{1}{ \ell_{\tau}}+\frac{\|X_{2}\|_{2}}{\tau}\right)^{2}\|u^{1}-X_{1}\mu^{2}\|_{2}^{2},\]

where the third last inequality follows from the \(\frac{1}{\ell_{\tau}}\)-smoothness of \(\nu(\cdot)\) on \(\Pi_{\tau}\), the second last inequality follows from Lemma D.7 (1) together with the quadratic growth property of strongly convex functions, and the last inequality follows from \(a^{2}+b^{2}\geq 2ab\). Similarly, we also have for any \(u^{2}\in\mathbb{R}^{|\mathcal{A}^{2}|}\) that

\[\langle\nabla_{2}V_{X}(\mu^{1},\mu^{2}),\sigma_{\tau}(u^{2})-\sigma _{\tau}(X_{2}\mu^{1})\rangle\] \[\leq\frac{1}{16}V_{X}(\mu^{1},\mu^{2})+\frac{8}{\tau}\left(\frac{1 }{\ell_{\tau}}+\frac{\|X_{1}\|_{2}}{\tau}\right)^{2}\|u^{2}-X_{2}\mu^{1}\|_{2} ^{2}.\]

Adding up the previous two inequalities, we obtain

\[\langle\nabla_{1}V_{X}(\mu^{1},\mu^{2}),\sigma_{\tau}(u^{1})- \sigma_{\tau}(X_{1}\mu^{2})\rangle+\langle\nabla_{2}V_{X}(\mu^{1},\mu^{2}), \sigma_{\tau}(u^{2})-\sigma_{\tau}(X_{2}\mu^{1})\rangle\] \[\leq\frac{1}{8}V_{X}(\mu^{1},\mu^{2})+\frac{8}{\tau}\left(\frac{ 1}{\ell_{\tau}}+\frac{\max(\|X_{1}\|_{2},\|X_{2}\|_{2})}{\tau}\right)^{2}\sum _{i=1,2}\|u^{i}-X_{i}\mu^{-i}\|_{2}^{2}.\]

The proof is complete.

With the properties of \(V_{X}(\cdot,\cdot)\) established in Lemma D.7, we now use it as a Lyapunov function to study \((\pi_{k}^{1},\pi_{k}^{2})\) generated by Algorithm 3. Specifically, using the smoothness of \(V_{X}(\cdot,\cdot)\) (cf. Lemma D.7 (2)), the update equation in Algorithm 3 Line 3, and Lemma D.7 (3) and (4), we have the desired one-step Lyapunov drift inequality for \(\mathcal{L}_{\pi}(k)\), which is presented in the following.

**Lemma D.8**.: _The following inequality holds for all \(k\geq 0\):_

\[\mathcal{L}_{\pi}(k+1)\leq \ \left(1-\frac{3\beta_{k}}{4}\right)\mathcal{L}_{\pi}(k)+\frac{16A _{\max}^{2}\beta_{k}}{\tau}\|v^{1}+v^{2}\|_{\infty}^{2}\] \[+\frac{32A_{\max}^{2}\beta_{k}}{\tau^{3}\ell_{\tau}^{2}(1-\gamma )^{2}}\mathcal{L}_{q}(k)+2L_{\tau}\beta_{k}^{2}.\]

Proof of Lemma D.8.: We will use \(V_{v,s}(\cdot,\cdot)\) (see Lemma D.1) as the Lyapunov function to study the evolution of \((\pi_{k}^{1}(s),\pi_{k}^{2}(s))\). To begin with, we identify the smoothness parameter of \(V_{v,s}(\cdot,\cdot)\). Using Lemma D.7 (1) and the definition of \(V_{v,s}(\cdot,\cdot)\), we have

\[\tilde{L}_{\tau} =2\left(\frac{\tau}{\ell_{\tau}}+\frac{\max(\|X_{1}\|_{2}^{2},\| X_{2}\|_{2}^{2})}{\tau}+\|X_{1}+X_{2}^{\top}\|_{2}\right)\] \[=2\left(\frac{\tau}{\ell_{\tau}}+\frac{\max(\|\mathcal{T}^{1}(v^ {1})(s)\|_{2}^{2},\|\mathcal{T}^{2}(v^{2})(s)\|_{2}^{2})}{\tau}+\|\mathcal{T} ^{1}(v^{1})(s)+\mathcal{T}^{2}(v^{2})(s)^{\top}\|_{2}\right)\] \[\leq 2\left(\frac{\tau}{\ell_{\tau}}+\frac{A_{\max}^{2}}{\tau(1- \gamma)^{2}}+\frac{2A_{\max}}{1-\gamma}\right)\] (This follows from \[|\mathcal{T}^{i}(v^{i})(s,a^{i},a^{-i})|\leq\frac{1}{1-\gamma}\],

\[\forall\ (s,a^{i},a^{-i})\text{ and }i\in\{1,2\}\]

.)

\[:=L_{\tau}.\]

Therefore, \(V_{v,s}(\cdot,\cdot)\) is an \(L_{\tau}\) - smooth function on \(\Pi_{\tau}\). Using the smoothness of \(V_{v,s}(\cdot,\cdot)\), for any \(s\in\mathcal{S}\), we have by the policy update equation (cf. Algorithm 3 Line 3) that

\[V_{v,s}(\pi_{k+1}^{1}(s),\pi_{k+1}^{2}(s))\] \[\leq V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2}(s))+\beta_{k}\langle\nabla_{2} V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2}(s)),\sigma_{\tau}(q_{k}^{2}(s))-\pi_{k}^{2}(s)\rangle\] \[\quad+\beta_{k}\langle\nabla_{1}V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2 }(s)),\sigma_{\tau}(q_{k}^{1}(s))-\pi_{k}^{1}(s)\rangle+\frac{L_{\tau}\beta_{k} ^{2}}{2}\sum_{i=1,2}\|\sigma_{\tau}(q_{k}^{1}(s))-\pi_{k}^{1}(s)\|_{2}^{2}\] \[\leq V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2}(s))+\beta_{k}\langle\nabla_{2} V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2}(s)),\sigma_{\tau}(\mathcal{T}^{2}(v^{2})(s)\pi_{k}^{1}(s ))-\pi_{k}^{2}(s)\rangle\] \[\quad+\beta_{k}\langle\nabla_{1}V_{v,s}(\pi_{k}^{1}(s),\pi_{k+1}^ {2}(s)),\sigma_{\tau}(\mathcal{T}^{1}(v^{1})(s)\pi_{k}^{2}(s))-\pi_{k}^{1}(s)\rangle\] \[\quad+\beta_{k}\langle\nabla_{2}V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2 }(s)),\sigma_{\tau}(q_{k}^{2}(s))-\sigma_{\tau}(\mathcal{T}^{2}(v^{2})(s)\pi_{k} ^{1}(s))\rangle\] \[\quad+\beta_{k}\langle\nabla_{1}V_{v,s}(\pi_{k}^{1}(s),\pi_{k+1}^ {2}(s)),\sigma_{\tau}(q_{k}^{1}(s))-\sigma_{\tau}(\mathcal{T}^{1}(v^{1})(s)\pi_{ k}^{2}(s))\rangle+2L_{\tau}\beta_{k}^{2}\] \[\leq \ \left(1-\frac{3\beta_{k}}{4}\right)V_{v,s}(\pi_{k}^{1}(s),\pi_{k}^{2 }(s))+\frac{16\beta_{k}}{\tau}\|\mathcal{T}^{1}(v^{1})(s)+\mathcal{T}^{2}(v^{2} )(s)^{\top}\|_{2}^{2}\]\[\leq \left(1-\frac{3\beta_{k}}{4}\right)\mathcal{L}_{\pi}(k)+\frac{16 \beta_{k}A_{\max}^{2}}{\tau}\|v^{1}+v^{2}\|_{\infty}^{2}\] \[+\frac{32A_{\max}^{2}\beta_{k}}{\tau^{3}\ell_{\tau}^{2}(1-\gamma) ^{2}}\mathcal{L}_{q}(k)+2L_{\tau}\beta_{k}^{2},\]

where the last line follows from \(\tau\leq 1/(1-\gamma)\). 

#### d.5.2 Analysis of the \(q\)-Functions

In this section, we consider \(q_{k}^{i}\) generated by Algorithm 3. We begin by reformulating the update of the \(q\)-function as a stochastic approximation algorithm for estimating a time-varying target. For \(i\in\{1,2\}\), fixing \(v^{i}\in\mathbb{R}^{|\mathcal{S}|}\), let \(F^{i}:\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\times\mathcal{S}\times \mathcal{A}^{i}\times\mathcal{A}^{-i}\times\mathcal{S}\mapsto\mathbb{R}^{| \mathcal{S}||\mathcal{A}^{i}|}\) be defined as

\[[F^{i}(q^{i},s_{0},a_{0}^{i},a_{0}^{-i},s_{1})](s,a^{i})=\mathds{1}_{\{(s,a^{ i})=(s_{0},a_{0}^{i})\}}\left(R_{i}(s_{0},a_{0}^{i},a_{0}^{-i})+\gamma v^{i}(s_{ 1})-q^{i}(s_{0},a_{0}^{i})\right)\]

for all \((q^{i},s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\) and \((s,a^{i})\). Then Algorithm 3 Line \(5\) can be compactly written as

\[q_{k+1}^{i}=q_{k}^{i}+\alpha_{k}F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{ k+1}).\] (41)

Denote the stationary distribution of the Markov chain \(\{S_{k}\}\) induced by the joint policy \(\pi_{k}=(\pi_{k}^{1},\pi_{k}^{2})\) by \(\mu_{k}\in\Delta(\mathcal{S})\), the existence and uniqueness of which is guaranteed by Lemma D.1 and Lemma B.1 (1). Let \(\bar{F}_{k}^{i}:\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\mapsto\mathbb{R}^ {|\mathcal{S}||\mathcal{A}^{i}|}\) be defined as

\[\bar{F}_{k}^{i}(q^{i})=\mathbb{E}_{S_{0}\sim\mu_{k}(\cdot),A_{0}^{i}\sim\pi_{ k}^{i}(\cdot|S_{0}),A_{0}^{-i}\sim\pi_{k}^{-i}(\cdot|S_{0}),S_{1}\sim p( \cdot|S_{0},A_{0}^{i},A_{0}^{-i},S_{1})}\left[F^{i}(q^{i},S_{0},A_{0}^{i},A_{0 }^{-i},S_{1})\right]\]

for all \(q^{i}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\). Then, Eq. (41) can be viewed as a stochastic approximation algorithm for solving the (time-varying) equation \(\bar{F}_{k}^{i}(q^{i})=0\) with time-inhomogeneous Markovian noise \(\{(S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\}_{k\geq 0}\). We next establish the properties of the operators \(F^{i}(\cdot)\) and \(\bar{F}_{k}^{i}(\cdot)\) in the following lemma.

**Lemma D.9**.: _The following properties hold for \(i\in\{1,2\}\)._

1. _It holds that_ \(\|F^{i}(q_{1}^{i},s_{0},a_{0}^{i},a_{0}^{-i},s_{1})-F^{i}(q_{2}^{i},s_{0},a_{0}^{ i},a_{0}^{-i},s_{1})\|_{2}\leq\|q_{1}^{i}-q_{2}^{i}\|_{2}\) _for any_ \((q_{1}^{i},q_{2}^{i})\) _and_ \((s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\)_._
2. _It holds that_ \(\|F^{i}(0,s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\|_{2}\leq\frac{1}{1-\gamma}\) _for all_ \((s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\)_._
3. \(\bar{F}_{k}^{i}(q^{i})=0\) _has a unique solution_ \(\bar{q}_{k}^{i}\)_, which is given as_ \(\bar{q}_{k}^{i}(s)=\mathcal{T}^{i}(v^{i})(s)\pi_{k}^{-i}(s)\) _for all_ \(s\)_._
4. _It holds that_ \(\langle\bar{F}_{k}^{i}(q_{1}^{i})-\bar{F}_{k}^{i}(q_{2}^{i}),q_{1}^{-i}-q_{2} ^{i}\rangle\leq-c_{\tau}\|q_{1}^{i}-q_{2}^{i}\|_{2}^{2}\) _for all_ \((q_{1}^{i},q_{2}^{i})\)_, where_ \(c_{\tau}=\mu_{\min}\ell_{\tau}\)_. See Lemma B.1 for the definition of_ \(\mu_{\min}\)_._

Proof of Lemma d.9.:
1. For any \((q_{1}^{i},q_{2}^{i})\) and \((s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\), we have \[\|F^{i}(q_{1}^{i},s_{0},a_{0}^{i},a_{0}^{-i},s_{1})-F^{i}(q_{2}^{i },s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\|_{2}^{2}\] \[= \sum_{(s,a^{i})}([F^{i}(q_{1}^{i},s_{0},a_{0}^{i},a_{0}^{-i},s_{1 })](s,a^{i})-[F^{i}(q_{2}^{i},s_{0},a_{0}^{i},a_{0}^{-i},s_{1})](s,a^{i}))^{2}\] \[= \left(q_{1}^{i}(s_{0},a_{0}^{i})-q_{2}^{i}(s_{0},a_{0}^{i})\right) ^{2}\] \[\leq \|q_{1}^{i}-q_{2}^{i}\|_{2}^{2}.\]
2. For any \((s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\), we have \[\|F^{i}(0,s_{0},a_{0}^{i},a_{0}^{-i},s_{1})\|_{2}^{2} = \sum_{(s,a^{i})}([F^{i}(0,s_{0},a_{0}^{i},a_{0}^{-i},s_{1})](s,a^ {i}))^{2}\] \[= \left(R_{i}(s_{0},a_{0}^{i},a_{0}^{-i})+\gamma v^{i}(s_{1}) \right)^{2}\] \[\leq \frac{1}{(1-\gamma)^{2}},\] where the last line follows from \(\|v^{i}\|_{\infty}\leq 1/(1-\gamma)\) and \(|R_{i}(s_{0},a_{0}^{i},a_{0}^{-i})|\leq 1\).
3. We first write down the explicitly the operator \(\bar{F}_{k}^{i}(\cdot)\). Using the definition of \(\mathcal{T}^{i}(\cdot)\), we have \[\bar{F}_{k}^{i}(q^{i})(s)=\mu_{k}(s)\text{diag}(\pi_{k}^{i}(s))\left(\mathcal{ T}^{i}(v^{i})(s)\pi_{k}^{-i}(s)-q^{i}(s)\right),\quad\forall\;s\in\mathcal{S},\] Since \(\mu_{k}(s)\geq\mu_{\min}>0\) (cf. Lemma B.1 (4)) and \(\text{diag}(\pi_{k}^{i}(s))\) has strictly positive diagonal entries (cf. Lemma D.1) for all \(s\in\mathcal{S}\) and \(k\geq 0\), the equation \(\bar{F}_{k}^{i}(q^{i})=0\) has a unique solution \(\bar{q}_{k}^{i}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\), which is given as \[\bar{q}_{k}^{i}(s)=\mathcal{T}^{i}(v^{i})(s)\pi_{k}^{-i}(s),\quad\forall\;s \in\mathcal{S}.\]
4. Using the expression of \(\bar{F}_{k}^{i}(\cdot)\), we have for any \(q_{1}^{i},q_{2}^{i}\in\mathbb{R}^{|\mathcal{S}||\mathcal{A}^{i}|}\) that \[(q_{1}^{i}-q_{2}^{i})^{\top}(\bar{F}_{k}^{i}(q_{1}^{i})-\bar{F}_{k}^{ i}(q_{2}^{i})) = -\sum_{s,a^{i}}\mu_{k}(s)\pi_{k}^{i}(a^{i}|s)(q_{1}^{i}(s,a^{i})- q_{2}^{i}(s,a^{i}))^{2}\] \[\leq -\min_{s,a^{i}}\mu_{k}(s)\pi_{k}^{i}(a^{i}|s)\|q_{1}^{i}-q_{2}^{i }\|_{2}^{2}\] \[\leq -\mu_{\min}\ell_{\tau}\|q_{1}^{i}-q_{2}^{i}\|_{2}^{2}\qquad\text{ (Lemma B.1 and Lemma D.1)}\] \[= -c_{\tau}\|q_{1}^{i}-q_{2}^{i}\|_{2}^{2}.\]

The proof is complete.

Next, we establish a negative drift inequality for \(\mathcal{L}_{q}(k)\). Using \(\|\cdot\|_{2}^{2}\) as a Lyapunov function, we have by Eq. (41) that

\[\mathbb{E}[\|q_{k+1}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}]=\mathbb{E}[\|q_{k+1}^{i}-q _{k}^{i}+q_{k}^{i}-\bar{q}_{k}^{i}+\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}]\]\[=\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}]+\mathbb{E}[\|q_{k+1 }^{i}-q_{k}^{i}\|_{2}^{2}]+\mathbb{E}[\|\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i}\|_{2}^ {2}]\] \[\quad+2\alpha_{k}\mathbb{E}[(q_{k}^{i}-\bar{q}_{k}^{i})^{\top}\bar {F}_{k}^{i}(q_{k}^{i})]\] \[\quad+2\alpha_{k}\mathbb{E}[(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k} ^{-i},S_{k+1})-\bar{F}_{k}^{i}(q_{k}^{i}))^{\top}(q_{k}^{i}-\bar{q}_{k}^{i})]\] \[\quad+2\mathbb{E}[(\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i})^{\top}(\bar{ q}_{k+1}^{i}-q_{k}^{i})]\] \[\quad+2\mathbb{E}[(q_{k}^{i}-\bar{q}_{k}^{i})^{\top}(\bar{q}_{k}^ {i}-\bar{q}_{k+1}^{i})]\] \[\leq(1-2\alpha_{k}c_{\tau})\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i} \|_{2}^{2}]+\mathbb{E}[\|q_{k+1}^{i}-q_{k}^{i}\|_{2}^{2}]+\mathbb{E}[\|\bar{q} _{k}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}]\] \[\quad+2\mathbb{E}[(\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i})^{\top}(\bar{ q}_{k+1}^{i}-q_{k}^{i})]\] \[\quad+2\mathbb{E}[(q_{k}^{i}-\bar{q}_{k}^{i})^{\top}(\bar{q}_{k}^ {i}-\bar{q}_{k+1}^{i})]\] \[\quad+2\alpha_{k}\mathbb{E}[(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k }^{-i},S_{k+1})-\bar{F}_{k}^{i}(q_{k}^{i}))^{\top}(q_{k}^{i}-\bar{q}_{k}^{i})]\] (42)

where the last line follows from Lemma D.9 (4). The terms \(\mathbb{E}[\|q_{k+1}^{i}-q_{k}^{i}\|_{2}^{2}]\), \(\mathbb{E}[\|\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}]\), \(\mathbb{E}[(\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i})^{\top}(q_{k+1}^{i}-q_{k}^{i})]\), \(\mathbb{E}[(q_{k}^{i}-\bar{q}_{k}^{i})^{\top}(\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i })]\) on the RHS of Eq. (42) are bounded in the following lemma.

**Lemma D.10**.: _The following inequalities hold for all \(k\geq 0\)._

1. \(\mathbb{E}[\|q_{k+1}^{i}-q_{k}^{i}\|_{2}^{2}]\leq\frac{4|\mathcal{S}|A_{\max \alpha}\alpha_{k}^{2}}{(1-\gamma)^{2}}\)_._
2. \(\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}]\leq\frac{4|\mathcal{S}|A_{ \max\delta}\beta_{k}}{(1-\gamma)^{2}}\)_._
3. \(\mathbb{E}[\langle q_{k+1}^{i}-q_{k}^{i},\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i} \rangle]\leq\frac{4|\mathcal{S}|A_{\max\alpha_{k}\delta_{k}\delta_{k}}}{(1- \gamma)^{2}}\)_._
4. \(\mathbb{E}[\langle q_{k}^{i}-\bar{q}_{k}^{i},\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i} \rangle]\leq\frac{17|\mathcal{S}|A_{\max\delta}^{2}\beta_{k}}{\tau(1-\gamma)^ {2}}\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}]+\frac{\beta_{k}}{16} \mathbb{E}[\mathcal{L}_{\pi}(k)]\)_._

Proof of Lemma D.10.:
1. For any \(k\geq 0\), using Eq. (41) and Lemma D.9 (1), we have \[\|q_{k+1}^{i}-q_{k}^{i}\|_{2}^{2} =\alpha_{k}^{2}\|F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1 })\|_{2}^{2}\] \[=\alpha_{k}^{2}\|F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+ 1})-F^{i}(0,S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\] \[\quad+F^{i}(0,S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\|_{2}^{2}\] \[\leq\alpha_{k}^{2}\left(\|q_{k}^{i}\|_{2}+\frac{1}{1-\gamma} \right)^{2}\] \[\leq\alpha_{k}^{2}\left(\frac{\sqrt{|\mathcal{S}|A_{\max}}}{1- \gamma}+\frac{1}{1-\gamma}\right)^{2}\] ( \[\|q_{k}^{i}\|_{\infty}\leq\frac{1}{1-\gamma}\] by Lemma D.1 ) \[\leq\frac{4|\mathcal{S}|A_{\max}\alpha_{k}^{2}}{(1-\gamma)^{2}}.\]

The result follows by taking expectation on both sides of the previous inequality.
2. For any \(k\geq 0\), using the definition of \(\bar{q}_{k}\) in Appendix D.1, we have by Lemma D.9 that \[\|\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2} =\sum_{s}\|\mathcal{T}^{i}(v^{i})(s)(\pi_{k+1}^{-i}(s)-\pi_{k}^{-i }(s))\|_{2}^{2}\] \[=\beta_{k}^{2}\sum_{s}\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q_ {k}^{-i}(s)-\pi_{k}^{-i}(s))\|_{2}^{2}\] \[\leq\beta_{k}^{2}\sum_{s}(\|\mathcal{T}^{i}(v^{i})(s)\sigma_{\tau} (q_{k}^{-i}(s))\|_{2}+\|\mathcal{T}^{i}(v^{i})(s)\pi_{k}^{-i}(s)\|_{2})^{2}\] \[\leq\frac{4|\mathcal{S}|A_{\max}\beta_{k}^{2}}{(1-\gamma)^{2}}.\]

The result follows by taking expectation on both sides of the previous inequality.

3. For any \(k\geq 0\), we have \[\langle q^{i}_{k+1}-q^{i}_{k},\bar{q}^{i}_{k}-\bar{q}^{i}_{k+1}\rangle\leq\|q^{i}_ {k+1}-q^{i}_{k}\|_{2}\|\bar{q}^{i}_{k}-\bar{q}^{i}_{k+1}\|_{2}\leq\frac{4| \mathcal{S}|A_{\max}\alpha_{k}\beta_{k}}{(1-\gamma)^{2}},\] where the last inequality follows from Part (1) and Part (2) of this lemma. The result follows by taking expectation on both sides of the previous inequality.
4. For any \(k\geq 0\), we have \[\langle q^{i}_{k}-\bar{q}^{i}_{k},\bar{q}^{i}_{k}-\bar{q}^{i}_{k+ 1}\rangle\] \[=\beta_{k}\sum_{s}\langle q^{i}_{k}(s)-\bar{q}^{i}_{k}(s), \mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q^{-i}_{k}(s))-\pi^{-i}_{k}(s))\rangle\] \[\leq\frac{c_{1}\beta_{k}\|q^{i}_{k}-\bar{q}^{i}_{k}\|_{2}^{2}}{2 }+\frac{\beta_{k}\sum_{s}\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q^{-i}_{k }(s))-\pi^{-i}_{k}(s))\|_{2}^{2}}{2c_{1}},\] (43) where \(c_{1}>0\) is an arbitrary positive real number. We next bound the second term on the RHS of the previous inequality. For any \(s\in\mathcal{S}\), we have \[\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q^{-i}_{k}(s))-\pi^{-i} _{k}(s))\|_{2}\] \[=\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q^{-i}_{k}(s))-\sigma_ {\tau}(\bar{q}^{-i}_{k}(s))+\sigma_{\tau}(\mathcal{T}^{-i}(v^{-i})(s)\pi^{i}_{ k}(s))-\pi^{-i}_{k}(s))\|_{2}\] \[\leq\underbrace{\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q^{-i}_ {k}(s))-\sigma_{\tau}(\bar{q}^{-i}_{k}(s)))\|_{2}}_{B_{1}}\] \[\quad+\underbrace{\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}( \mathcal{T}^{-i}(v^{-i})(s)\pi^{i}_{k}(s))-\pi^{-i}_{k}(s))\|_{2}}_{B_{2}}.\] Since the softmax operator \(\sigma_{\tau}(\cdot)\) is \(\frac{1}{\tau}\) - Lipschitz continuous with respect to \(\|\cdot\|_{2}\)[96, Proposition 4], we have \[B_{1} \leq\|\mathcal{T}^{i}(v^{i})(s)\|_{2}\|\sigma_{\tau}(q^{-i}_{k}(s ))-\sigma_{\tau}(\bar{q}^{-i}_{k}(s))\|_{2}\] \[\leq\frac{A_{\max}}{\tau(1-\gamma)}\|q^{-i}_{k}(s)-\bar{q}^{-i}_{ k}(s)\|_{2}.\] We next analyze the term \(B_{2}\). Using Lemma D.7 (1) and the quadratic growth property of strongly convex functions, we have \[B_{2} =\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(\mathcal{T}^{-i}(v^{- i})(s)\pi^{i}_{k}(s))-\pi^{-i}_{k}(s))\|_{2}\] \[\leq\|\mathcal{T}^{i}(v^{i})(s)\|_{2}\|\sigma_{\tau}(\mathcal{T} ^{-i}(v^{-i})(s)\pi^{i}_{k}(s))-\pi^{-i}_{k}(s)\|_{2}\] \[\leq\frac{\sqrt{2}A_{\max}}{\sqrt{\tau}(1-\gamma)}V_{v,s}(\pi^{1} _{k}(s),\pi^{2}_{k}(s))^{1/2}.\] Combine the upper bounds we obtained for the terms \(B_{1}\) and \(B_{2}\), we obtain \[\sum_{s}\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q^{-i}_{k}(s))- \pi^{-i}_{k}(s))\|_{2}^{2}\] \[\leq\,\sum_{s}(B_{1}+B_{2})^{2}\] \[\leq 2\sum_{s}\left(\frac{A_{\max}^{2}}{\tau^{2}(1-\gamma)^{2}}\|q^{- i}_{k}(s))-\bar{q}^{-i}_{k}(s)\|_{2}^{2}+\frac{2A_{\max}^{2}}{\tau(1-\gamma)^{2}}V_{v,s}(\pi^{1}_{k}(s),\pi^{2}_{k}(s))\right)\] \[\leq\frac{2A_{\max}^{2}}{\tau^{2}(1-\gamma)^{2}}\|q^{-i}_{k}-\bar {q}^{-i}_{k}\|_{2}^{2}+\frac{4|\mathcal{S}|A_{\max}^{2}}{\tau(1-\gamma)^{2}} \mathcal{L}_{\pi}(k).\]

Coming back to Eq. (43), using the previous inequality, we have

\[\langle q^{i}_{k}-\bar{q}^{i}_{k},\bar{q}^{i}_{k}-\bar{q}^{i}_{k+1}\rangle\]\[\leq\frac{c_{1}\beta_{k}\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}}{2}+ \frac{\beta_{k}\sum_{s}\|\mathcal{T}^{i}(v^{i})(s)(\sigma_{\tau}(q_{k}^{-i}(s))- \pi_{k}^{-i}(s))\|_{2}^{2}}{2c_{1}}\] \[\leq\frac{c_{1}\beta_{k}\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}}{2}+ \frac{A_{\max}^{2}\beta_{k}}{c_{1}\tau^{2}(1-\gamma)^{2}}\|q_{k}^{-i}-\bar{q}_ {k}^{-i}\|_{2}^{2}+\frac{2|\mathcal{S}|A_{\max}^{2}\beta_{k}}{c_{1}\tau(1- \gamma)^{2}}\mathcal{L}_{\pi}(k).\]

Choosing \(c_{1}=\frac{32|\mathcal{S}|A_{\max}^{2}}{\tau(1-\gamma)^{2}}\) in the previous inequality and then taking expectation on both sides, we obtain

\[\mathbb{E}[\langle q_{k}^{i}-\bar{q}_{k}^{i},\bar{q}_{k}^{i}-\bar{q}_{k+1}^{i} \rangle]\leq\frac{17|\mathcal{S}|A_{\max}^{2}\beta_{k}}{\tau(1-\gamma)^{2}} \mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}]+\frac{\beta_{k}}{16}\mathbb{ E}[\mathcal{L}_{\pi}(k)].\]

The proof is complete.

We next consider the last term on the RHS of Eq. (42), which involves the difference between the operator \(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\) and its expected version \(\bar{F}_{k}^{i}(q_{k}^{i})\), and hence can be viewed as the stochastic error due to sampling. The fact that the Markov chain \(\{(S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\}\) is time-inhomogeneous presents a challenge in our analysis. To overcome this challenge, observe that: (1) the policy (hence the transition probability matrix of the induced Markov chain) is changing slowly compared to the \(q\)-function; see Algorithm 3 Line \(3\), and (2) the stationary distribution as a function of the policy is Lipschitz (cf. Lemma B.1 (3)). These two observations together enable us to develop a refined conditioning argument to handle the time-inhomogeneous Markovian noise. The result is presented in following. Similar ideas were previous used in [23, 24, 65, 41, 76] for finite-sample analysis of single-agent RL algorithms. Recall that we use \(\alpha_{k_{1},k_{2}}=\sum_{k=k_{1}}^{k_{2}}\alpha_{k}\) to simplify the notation.

**Lemma D.11** (Proof in Appendix D.7.2).: _The following inequality holds for all \(k\geq z_{k}\) :_

\[\mathbb{E}[(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})-\bar{F}_{k}^{ i}(q_{k}^{i}))^{\top}(q_{k}^{i}-\bar{q}_{k}^{i})]\leq\frac{17z_{k}\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}},\]

_where \(z_{k}\) is the mixing time of the Markov chain \(\{S_{n}\}_{n\geq 0}\) induced by the joint policy \(\pi_{k}=(\pi_{k}^{1},\pi_{k}^{2})\) with accuracy \(\beta_{k}\); see Eq. (11)._

When using constant stepsize, we have \(z_{k}\alpha_{k-z_{k},k-1}=z_{\beta}^{2}\alpha=\mathcal{O}(\alpha\log^{2}(1/ \beta))\). Since the two stepsizes \(\alpha\) and \(\beta\) differ only by a multiplicative constant \(c_{\alpha,\beta}\), we have \(\lim_{\alpha\to 0}z_{\beta}^{2}\alpha=0\). Similarly, we also have \(\lim_{k\to\infty}z_{k}\alpha_{k-z_{k},k-1}=0\) when using diminishing stepsizes.

Using the upper bounds we obtained for all the terms on the RHS of Eq. (42), we have the one-step Lyapunov drift inequality for \(q_{k}^{i}\). Following the same line of analysis, we also obtain the one-step inequality for \(q_{k}^{-i}\). Adding up the two Lyapunov drift inequalities, we arrive at the following lemma.

**Lemma D.12**.: _The following inequality holds for all \(k\geq z_{k}\) and \(i\in\{1,2\}\):_

\[\mathbb{E}[\mathcal{L}_{q}(k+1)]\leq(1-\alpha_{k}c_{\tau})\,\mathbb{E}[ \mathcal{L}_{q}(k)]+\frac{\beta_{k}}{4}\mathbb{E}[\mathcal{L}_{\pi}(k)]+\frac{ 100|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k}\alpha_{k}\alpha_{k-z_{k},k-1}.\]

Proof of Lemma d.12.: For \(i\in\{1,2\}\), we have from Eq. (42), Lemma D.10, and Lemma D.11 that

\[\mathbb{E}[\|q_{k+1}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}] \leq(1-2\alpha_{k}c_{\tau})\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\| _{2}^{2}]+\frac{4|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}(\alpha_{k}^{2}+2 \alpha_{k}\beta_{k}+\beta_{k}^{2})\] \[\quad+\frac{34|\mathcal{S}|A_{\max}^{2}\beta_{k}}{\tau(1-\gamma)^{ 2}}\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}]+\frac{\beta_{k}}{8} \mathbb{E}[\mathcal{L}_{\pi}(k)]+\frac{34z_{k}\alpha_{k}\alpha_{k-z_{k},k-1}}{(1 -\gamma)^{2}}\] \[\leq\,\left(1-2\alpha_{k}c_{\tau}+\frac{34|\mathcal{S}|A_{\max}^{ 2}\beta_{k}}{\tau(1-\gamma)^{2}}\right)\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\| _{2}^{2}]+\frac{\beta_{k}}{8}\mathbb{E}[\mathcal{L}_{\pi}(k)]\] \[\quad+\frac{50|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k}\alpha_{k }\alpha_{k-z_{k},k-1},\]where the second inequality follows from \(\beta_{k}=c_{\alpha,\beta}\alpha_{k}\) with \(c_{\alpha,\beta}\leq 1\). Since

\[c_{\alpha,\beta}\leq\frac{c_{\tau}\tau(1-\gamma)^{2}}{34|\mathcal{S}|A_{\max}^{ 2}},\] (Condition 3.1)

we have

\[\mathbb{E}[\|q_{k+1}^{i}-\bar{q}_{k+1}^{i}\|_{2}^{2}]\leq\,(1-\alpha_{k}c_{\tau })\,\mathbb{E}[\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{2}^{2}]+\frac{\beta_{k}}{8} \mathbb{E}[\mathcal{L}_{\pi}(k)]+\frac{50|\mathcal{S}|A_{\max}}{(1-\gamma)^{2 }}z_{k}\alpha_{k}\alpha_{k-z_{k},k-1}.\]

Summing up the previous inequality for \(i=1,2\), we have

\[\mathbb{E}[\mathcal{L}_{q}(k+1)]\leq(1-\alpha_{k}c_{\tau})\, \mathbb{E}[\mathcal{L}_{q}(k)]+\frac{\beta_{k}}{4}\mathbb{E}[\mathcal{L}_{ \pi}(k)]+\frac{100|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k}\alpha_{k}\alpha _{k-z_{k},k-1}.\]

### Solving Coupled Lyapunov Drift Inequalities

We first restate the Lyapunov drift inequalities from previous sections. Recall our notation \(\mathcal{L}_{q}(t,k)=\sum_{i=1,2}\|q_{t,k}^{i}-\bar{q}_{i,k}^{i}\|_{2}^{2}\), \(\mathcal{L}_{\pi}(t,k)=\max_{s\in\mathcal{S}}V_{v_{t,s}}(\pi_{t,k}^{1}(s),\pi _{t,k}^{2}(s))\), \(\mathcal{L}_{\text{sum}}(t)=\|v_{t}^{1}+v_{t}^{2}\|_{\infty}\), and \(\mathcal{L}_{v}(t)=\sum_{i=1,2}\|v_{t}^{i}-v_{i}^{i}\|_{\infty}\). Let \(\mathcal{F}_{t}\) be the history of Algorithm 2 right before the \(t\)-th outer-loop iteration. Note that \(v_{t}^{1}\) and \(v_{t}^{2}\) are both measurable with respect to \(\mathcal{F}_{t}\). In what follows, for ease of presentation, we write \(\mathbb{E}_{t}[\,\cdot\,\,]\) for \(\mathbb{E}[\,\cdot\,\,|\,\mathcal{F}_{t}]\).

* **Lemma D.5:** It holds for all \(t\geq 0\) that \[\mathcal{L}_{v}(t+1)\leq\gamma\mathcal{L}_{v}(t)+4\mathcal{L}_{\text{sum}}(t) +2\mathcal{L}_{q}^{1/2}(t,K)+4\mathcal{L}_{\pi}(t,K)+6\tau\log(A_{\max}).\] (44)
* **Lemma D.6:** It holds for all \(t\geq 0\) that \[\mathcal{L}_{\text{sum}}(t+1)\leq\gamma\mathcal{L}_{\text{sum}}(t)+2\mathcal{ L}_{q}^{1/2}(t,K).\] (45)
* **Lemma D.8:** It holds for all \(t,k\geq 0\) that \[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k+1)]\leq \,\left(1-\frac{3\beta_{k}}{4}\right)\mathbb{E}_{t}[\mathcal{L}_{ \pi}(t,k)]+\frac{16A_{\max}^{2}\beta_{k}}{\tau}\mathcal{L}_{\text{sum}}(t)^{2}\] \[+\frac{32A_{\max}^{2}\beta_{k}}{\tau^{3}\ell_{\tau}^{2}(1-\gamma )^{2}}\mathcal{L}_{q}(k)+2L_{\tau}\beta_{k}^{2}.\] (46)
* **Lemma D.12:** It holds for all \(t\geq 0\) and \(k\geq z_{k}\) that \[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k+1)]\leq(1-\alpha_{k}c_{\tau})\, \mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]+\frac{\beta_{k}}{4}\mathbb{E}_{t}[ \mathcal{L}_{\pi}(t,k)]+\frac{100|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k} \alpha_{k}\alpha_{k-z_{k},k-1}.\] (47)

Adding up Eqs. (46) and (47), we have by \(c_{\alpha,\beta}\leq\min(\frac{1}{L_{\tau}^{1/2}},\frac{c_{\tau}\tau^{3}\ell_ {\tau}^{2}(1-\gamma)^{2}}{128A_{\max}^{2}},c_{\tau})\) (cf. Condition 3.1) that

\[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k+1)+\mathcal{L}_{q}(t,k+1)] \leq\,\left(1-\frac{\beta_{k}}{2}\right)\mathbb{E}_{t}[\mathcal{L}_{ \pi}(t,k)+\mathcal{L}_{q}(t,k)]\] \[\frac{16A_{\max}^{2}\beta_{k}}{\tau}\mathcal{L}_{\text{sum}}(t)^{2 }+\frac{102|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k}\alpha_{k}\alpha_{k-z_{k },k-1}.\] (48)

#### d.6.1 Constant Stepsize

When using constant stepsizes, i.e., \(\alpha_{k}\equiv\alpha\), \(\beta_{k}\equiv\beta=c_{\alpha,\beta}\alpha\), repeatedly using Eq. (48) from \(z_{\beta}\) to \(k\), we have

\[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)+\mathcal{L}_{q}(t,k)]\leq\, \left(1-\frac{\beta}{2}\right)^{k-z_{\beta}}\left(\mathcal{L}_{\pi}(t,0)+ \mathcal{L}_{q}(t,0)\right)\]\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]\leq \ (1-\alpha c_{\tau})\,\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]+\frac{15 1|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{\beta}^{2}\alpha^{2}+\frac{\beta L_{ \text{in}}}{4}\left(1-\frac{\beta}{2}\right)^{k-z_{\beta}}\] \[+\frac{4A_{\max}^{2}\alpha_{,\beta}}{\tau}\mathcal{L}_{\text{ sum}}(t)^{2}.\]

Repeatedly using the previous inequality, since \(c_{\alpha,\beta}\leq c_{\tau}\) (cf. Condition 3.1), we have

\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)] \leq L_{\text{in}}\left(1-c_{\tau}\alpha\right)^{k-z_{\beta}}+ \frac{\beta L_{\text{in}}(k-z_{\beta})}{4}\left(1-\frac{\beta}{2}\right)^{k-z _{\beta}-1}\] \[\ \ \ +\frac{4A_{\max}^{2}c_{\alpha,\beta}}{c_{\tau}\tau} \mathcal{L}_{\text{sum}}(t)^{2}+\frac{151|\mathcal{S}|A_{\max}}{(1-\gamma)^{2 }c_{\tau}}z_{\beta}^{2}\alpha,\]

which implies (by using Jensen's inequality) that

\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)^{1/2}] \leq L_{\text{in}}^{1/2}\left(1-c_{\tau}\alpha\right)^{\frac{k-z _{\beta}}{2}}+\frac{\beta^{1/2}L_{\text{in}}^{1/2}(k-z_{\beta})^{1/2}}{2}\left( 1-\frac{\beta}{2}\right)^{\frac{k-z_{\beta}-1}{2}}\] \[\ \ \ +\frac{2A_{\max}c_{\alpha,\beta}^{1/2}}{c_{\tau}^{1/2} \tau^{1/2}}\mathcal{L}_{\text{sum}}(t)+\frac{13|\mathcal{S}|^{1/2}A_{\max}^{ 1/2}}{(1-\gamma)c_{\tau}^{1/2}}z_{\beta}\alpha^{1/2}.\]

Substituting the previous bound on \(\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)^{1/2}]\) into Eq. (45) and then taking total expectation, we have

\[\mathbb{E}[\mathcal{L}_{\text{sum}}(t+1)]\leq\gamma\mathcal{L}_{\text{sum}}( t)+2L_{\text{in}}^{1/2}\left(1-c_{\tau}\alpha\right)^{\frac{K-z_{\beta}}{2}}+ \beta^{1/2}L_{\text{in}}^{1/2}(K-z_{\beta})^{1/2}\left(1-\frac{\beta}{2}\right) ^{\frac{K-z_{\beta}-1}{2}}\]\[\leq\gamma\mathbb{E}[\mathcal{L}_{v}(t)]+\frac{1223|\mathcal{S}|A_{ \max}}{(1-\gamma)^{2}c_{\alpha,\beta}}z_{\beta}^{2}\alpha^{1/2}+6\tau\log(A_{ \max})\] \[\quad+\frac{266A_{\max}^{2}}{\tau(1-\gamma)^{2}}\left(\frac{1+ \gamma}{2}\right)^{t}+\frac{805A_{\max}^{2}L_{\text{in}}(K-z_{\beta})^{1/2}}{ (1-\gamma)^{2}\tau}\left(1-\frac{\beta}{2}\right)^{\frac{K-z_{\beta}-1}{2}}.\]

Repeatedly using the previous inequality from \(0\) to \(T-1\) and then using \(\mathcal{L}_{v}(0)\leq\frac{4}{1-\gamma}\), we have

\[\mathcal{L}_{v}(T) \leq\frac{270A_{\max}^{2}T}{\tau(1-\gamma)^{2}}\left(\frac{1+ \gamma}{2}\right)^{T-1}\] \[\quad+\frac{805A_{\max}^{2}L_{\text{in}}(K-z_{\beta})^{1/2}}{ \tau(1-\gamma)^{3}}\left(1-\frac{\beta}{2}\right)^{\frac{K-z_{\beta}-1}{2}}\] \[\quad+\frac{1223|\mathcal{S}|A_{\max}}{(1-\gamma)^{3}c_{\alpha, \beta}}z_{\beta}^{2}\alpha^{1/2}+\frac{6\tau\log(A_{\max})}{1-\gamma}.\]Our next step is to use the bounds we obtained for \(\mathcal{L}_{q}(t,k)\), \(\mathcal{L}_{\pi}(t,k)\), \(\mathcal{L}_{v}(t)\), and \(\mathcal{L}_{\text{sum}}(t)\) in Lemma D.4. For simplicity of presentation, we use \(a\lesssim b\) to mean that there exists a _numerical_ constant \(c\) such that \(a\leq cb\). Using the previous inequality, Eq. (50), and Eq. (51), we have

\[\mathbb{E}[\text{NG}(\pi_{T,K}^{1},\pi_{T,K}^{2})] \leq\frac{8}{1-\gamma}\mathcal{L}_{\text{sum}}(T)+\frac{4}{1- \gamma}\mathcal{L}_{v}(T)+\frac{4}{1-\gamma}\mathcal{L}_{\pi}(T,K)+\frac{8\tau \log(A_{\max})}{1-\gamma}\] \[\lesssim\frac{A_{\max}^{2}T}{\tau(1-\gamma)^{3}}\left(\frac{1+ \gamma}{2}\right)^{T-1}+\frac{A_{\max}^{2}L_{\text{in}}(K-z_{\beta})^{1/2}}{ \tau(1-\gamma)^{4}}\left(1-\frac{\beta}{2}\right)^{\frac{K-z_{\beta}-1}{2}}\] \[\quad+\frac{|\mathcal{S}|A_{\max}}{(1-\gamma)^{4}c_{\alpha,\beta }}z_{\beta}^{2}\alpha^{1/2}+\frac{\tau\log(A_{\max})}{(1-\gamma)^{2}}.\]

The proof of Theorem 3.1 (1) is complete.

#### d.6.2 Diminishing Stepsizes

Consider using linearly diminishing stepsizes, i.e., \(\alpha_{k}=\frac{\alpha}{k+h}\), \(\beta_{k}=\frac{\beta}{k+h}\), and \(\beta=c_{\alpha,\beta}\alpha\). Repeatedly using Eq. (48), we have for all \(k\geq k_{0}:=\min\{k^{\prime}\mid k^{\prime}\geq z_{k^{\prime}}\}\) that

\[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)+\mathcal{L}_{q}(t,k)] \leq L_{\text{in}}\underbrace{\prod_{m=k_{0}}^{k-1}\left(1- \frac{\beta_{m}}{2}\right)}_{\hat{\mathcal{E}_{1}}}+\frac{204|\mathcal{S}|A_{ \max}}{(1-\gamma)^{2}}\underbrace{\sum_{n=k_{0}}^{k-1}z_{n}^{2}\alpha_{n}^{2} \prod_{m=n+1}^{k-1}\left(1-\frac{\beta_{m}}{2}\right)}_{\hat{\mathcal{E}_{2}}}\] \[\quad+\frac{16A_{\max}^{2}}{\tau}\mathcal{L}_{\text{sum}}(t)^{2} \underbrace{\sum_{n=k_{0}}^{k-1}\beta_{n}\prod_{m=n+1}^{k-1}\left(1-\frac{ \beta_{m}}{2}\right)}_{\hat{\mathcal{E}_{3}}}.\]

Next, we evaluate the terms \(\{\hat{\mathcal{E}}_{j}\}_{1\leq j\leq 3}\). Terms like \(\{\hat{\mathcal{E}}_{j}\}_{1\leq j\leq 3}\) have been well studied in the existing literature [24, 44, 65]. Specifically, we have from [65, Appendix A.2.] and \(\beta=4\) that

\[\hat{\mathcal{E}}_{1}\leq\frac{k_{0}+h}{k+h},\quad\hat{\mathcal{E}}_{2}\leq \frac{64ez_{k}^{2}}{(k+h)c_{\alpha,\beta}^{2}},\text{ and }\hat{\mathcal{E}}_{3}\leq 2.\]

It follows that

\[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)+\mathcal{L}_{q}(t,k)]\leq L_{\text{in}} \frac{k_{0}+h}{k+h}+\frac{3264e|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}c_{\alpha, \beta}}z_{k}^{2}\alpha_{k}+\frac{32A_{\max}^{2}}{\tau}\mathcal{L}_{\text{sum}} (t)^{2},\]

which implies

\[\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)]\leq L_{\text{in}}\frac{k_{0}+h}{k+h}+ \frac{3264e|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}c_{\alpha,\beta}}z_{k}^{2} \alpha_{k}+\frac{32A_{\max}^{2}}{\tau}\mathcal{L}_{\text{sum}}(t)^{2}.\] (52)

Using the previous inequality on \(\mathbb{E}_{t}[\mathcal{L}_{\pi}(t,k)]\) in Eq. (47), we have

\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k+1)] \leq\,(1-\alpha_{k}c_{\tau})\,\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)] +\frac{100|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k}\alpha_{k}\alpha_{k-z_{k}, k-1}\] \[\quad+\frac{L_{\text{in}}c_{\alpha,\beta}\alpha_{k}^{2}}{4\alpha _{k_{0}}}+\frac{816e|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}}z_{k}^{2}\alpha_{k}^ {2}+\frac{8A_{\max}^{2}\beta_{k}}{\tau}\mathcal{L}_{\text{sum}}(t)^{2}\] \[\leq\,(1-\alpha_{k}c_{\tau})\,\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)] +\frac{1017eL_{\text{in}}|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}\alpha_{k_{0}}}z_{ k}^{2}\alpha_{k}^{2}\] \[\quad+\frac{8A_{\max}^{2}\beta_{k}}{\tau}\mathcal{L}_{\text{sum}} (t)^{2}.\]

Repeatedly using the previous inequality starting from \(k_{0}\), since \(\alpha c_{\tau}\geq 1\) (cf. Condition 3.1), we have

\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)]\leq L_{\text{in}}\frac{k_{0}+h}{k+h}+ \frac{4068e^{2}L_{\text{in}}|\mathcal{S}|A_{\max}}{(1-\gamma)^{2}c_{\tau} \alpha_{k_{0}}}z_{k}^{2}\alpha_{k}+\frac{8A_{\max}^{2}c_{\alpha,\beta}}{c_{\tau} \tau}\mathcal{L}_{\text{sum}}(t)^{2},\]which implies (by using Jensen's inequality) that

\[\mathbb{E}_{t}[\mathcal{L}_{q}(t,k)^{1/2}]\leq L_{\text{in}}^{1/2}\left(\frac{k_{ 0}+h}{k+h}\right)^{1/2}+\frac{64eL_{\text{in}}^{1/2}|\mathcal{S}|^{1/2}A_{\text {max}}^{1/2}}{(1-\gamma)c_{\tau}^{1/2}\alpha_{k_{0}}^{1/2}}z_{k}\alpha_{k}^{1/ 2}+\frac{3A_{\text{max}}c_{\alpha,\beta}^{1/2}}{c_{\tau}^{1/2}\tau^{1/2}} \mathcal{L}_{\text{sum}}(t).\] (53)

Taking total expectation on both sides of the previous inequality and then using the result in Eq. (45), we have

\[\mathbb{E}[\mathcal{L}_{\text{sum}}(t+1)] \leq\gamma\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]+2L_{\text{in}}^ {1/2}\left(\frac{k_{0}+h}{K+h}\right)^{1/2}+\frac{128e|\mathcal{S}|^{1/2}A_{ \text{max}}^{1/2}}{(1-\gamma)c_{\tau}^{1/2}\alpha_{k_{0}}^{1/2}}z_{K}\alpha_{ K}^{1/2}\] \[\quad+\frac{6A_{\text{max}}c_{\alpha,\beta}^{1/2}}{c_{\tau}^{1/2 }\tau^{1/2}}\mathcal{L}_{\text{sum}}(t)\] \[\leq\]

where the last line follows from \(c_{\alpha,\beta}\leq\frac{c_{\tau}\tau(1-\gamma)^{2}}{144A_{\text{max}}^{1/2}}\) (cf. Condition 3.1). Repeatedly using the previous inequality starting from \(0\), we have

\[\mathbb{E}[\mathcal{L}_{\text{sum}}(t)]\leq\frac{2}{1-\gamma}\left(\frac{1+ \gamma}{2}\right)^{t}+\frac{260eL_{\text{in}}^{1/2}|\mathcal{S}|^{1/2}A_{ \text{max}}^{1/2}}{(1-\gamma)^{2}c_{\tau}^{1/2}\alpha_{k_{0}}^{1/2}}z_{K} \alpha_{K}^{1/2}.\] (54)

The next step is to bound \(\mathcal{L}_{v}(t)\). Recall from Eq. (44) that

\[\mathbb{E}_{t}[\mathcal{L}_{v}(t+1)]\leq\gamma\mathcal{L}_{v}(t)+4\mathcal{L} _{\text{sum}}(t)+2\mathbb{E}_{t}[\mathcal{L}_{q}^{1/2}(t,K)]+4\mathbb{E}_{t}[ \mathcal{L}_{\pi}(t,K)]+6\tau\log(A_{\text{max}}).\]

Using Eqs. (52), (53), and (54) in the previous inequality, we have

\[\mathbb{E}[\mathcal{L}_{v}(t+1)] \leq\gamma\mathbb{E}[\mathcal{L}_{v}(t)]+4\mathbb{E}[\mathcal{L} _{\text{sum}}(t)]+2\mathbb{E}[\mathcal{L}_{q}^{1/2}(t,K)]+4\mathbb{E}[ \mathcal{L}_{\pi}(t,K)]+6\tau\log(A_{\text{max}})\] \[\leq\gamma\mathbb{E}[\mathcal{L}_{v}(t)]+\frac{130eL_{\text{in}}^ {1/2}|\mathcal{S}|^{1/2}A_{\text{max}}^{1/2}}{(1-\gamma)c_{\tau}^{1/2}\alpha_{ k_{0}}^{1/2}}z_{K}\alpha_{K}^{1/2}\] \[\quad+\frac{4L_{\text{in}}\alpha_{K}}{\alpha_{k_{0}}}+\frac{13056 e|\mathcal{S}|A_{\text{max}}}{(1-\gamma)^{2}c_{\alpha,\beta}}z_{K}^{2}\alpha_{K}+6 \tau\log(A_{\text{max}})\] \[\quad+\frac{522A_{\text{max}}^{2}}{\tau(1-\gamma)^{2}}\left( \frac{1+\gamma}{2}\right)^{t}+\frac{67860eL_{\text{in}}^{1/2}|\mathcal{S}|^{1/ 2}A_{\text{max}}^{5/2}}{(1-\gamma)^{3}\tau c_{\tau}^{1/2}\alpha_{k_{0}}^{1/2}} z_{K}\alpha_{K}^{1/2}\] \[\leq\gamma\mathbb{E}[\mathcal{L}_{v}(t)]+\frac{522A_{\text{max}}^ {2}}{\tau(1-\gamma)^{2}}\left(\frac{1+\gamma}{2}\right)^{t}+\frac{15056eL_{ \text{in}}|\mathcal{S}|A_{\text{max}}}{(1-\gamma)^{2}\alpha_{k_{0}}^{1/2}c_{ \alpha,\beta}}z_{K}^{2}\alpha_{K}^{1/2}\] \[\quad+6\tau\log(A_{\text{max}}).\]

Repeatedly using the previous inequality starting from \(0\) to \(T-1\), we have

\[\mathcal{L}_{v}(T)\leq\frac{526A_{\text{max}}^{2}T}{\tau(1-\gamma)^{2}}\left( \frac{1+\gamma}{2}\right)^{T-1}+\frac{15056eL_{\text{in}}|\mathcal{S}|A_{\text {max}}}{(1-\gamma)^{3}\alpha_{k_{0}}^{1/2}c_{\alpha,\beta}}z_{K}^{2}\alpha_{K}^ {1/2}+\frac{6\tau\log(A_{\text{max}})}{1-\gamma}\]

Finally, using the previous inequality, Eq. (52), and Eq. (54) in Lemma D.4, we obtain

\[\mathbb{E}[\text{NG}(\pi_{T,K}^{1},\pi_{T,K}^{2})]\lesssim\frac{A_{\text{max}}^ {2}T}{\tau(1-\gamma)^{3}}\left(\frac{1+\gamma}{2}\right)^{T-1}+\frac{L_{\text{ in}}|\mathcal{S}|A_{\text{max}}}{(1-\gamma)^{4}\alpha_{k_{0}}^{1/2}c_{ \alpha,\beta}}z_{K}^{2}\alpha_{K}^{1/2}+\frac{\tau\log(A_{\text{max}})}{(1- \gamma)^{2}}.\]

The proof of Theorem 3.1 (2) is complete.

### Proof of All Supporting Lemmas

#### d.7.1 Proof of Lemma b.1

Lemma B.1 (1), (3), and (4) are identical to [10, Proposition 3]. We here only prove Lemma B.1 (2). Consider the Markov chain \(\{S_{k}\}\) induced by \(\pi_{b}\). Since \(\{S_{k}\}\) is irreducible and aperiodic, there existsa positive integer \(r_{b}\) such that \(P_{\pi_{b}}^{r_{b}}\) has strictly positive entries [93, Proposition 1.7]. Therefore, there exists \(\delta_{b}\in(0,1)\) such that

\[P_{\pi_{b}}^{r_{b}}(s,s^{\prime})\geq\delta_{b}\mu_{b}(s^{\prime})\]

for all \((s,s^{\prime})\). In addition, the constant \(\rho_{b}\) introduced after Assumption 3.1 is explicitly given as \(\rho_{b}=\exp(-\delta_{b}/r_{b})\). The previous two equations are from the proof of the Markov chain convergence theorem presented in [93, Section 4.3]. Next, we consider the Markov chain \(\{S_{k}\}\) induced by an arbitrary \(\pi\in\Pi_{\tau}\). Since

\[\frac{\pi_{b}(a|s)}{\pi(a|s)}=\frac{\pi_{b}^{i}(a^{i}|s)\pi_{b}^{-i}(a^{i}|s)}{ \pi^{i}(a^{i}|s)\pi^{-i}(a^{i}|s)}\leq\frac{1}{\ell_{\tau}^{2}},\quad\forall\;a =(a^{i},a^{-i})\text{ and }s,\]

we have for any \(s,s^{\prime}\in\mathcal{S}\) and \(k\geq 1\) that

\[P_{\pi_{b}}^{k}(s,s^{\prime}) = \sum_{s_{0}}P_{\pi_{b}}^{k-1}(s,s_{0})P_{\pi_{b}}(s_{0},s^{\prime})\] \[= \sum_{s_{0}}P_{\pi_{b}}^{k-1}(s,s_{0})\sum_{a\in\mathcal{A}}\pi_ {b}(a|s_{0})P_{a}(s_{0},s^{\prime})\] \[= \sum_{s_{0}}P_{\pi_{b}}^{k-1}(s,s_{0})\sum_{a\in\mathcal{A}}\frac {\pi_{b}(a|s_{0})}{\pi(a|s_{0})}\pi(a|s_{0})P_{a}(s_{0},s^{\prime})\] \[\leq \frac{1}{\ell_{\tau}^{2}}\sum_{s_{0}}P_{\pi_{b}}^{k-1}(s,s_{0}) \sum_{a\in\mathcal{A}}\pi(a|s_{0})P_{a}(s_{0},s^{\prime})\] \[\leq \frac{1}{\ell_{\tau}^{2}}\sum_{s_{0}}P_{\pi_{b}}^{k-1}(s,s_{0})P_ {\pi}(s_{0},s^{\prime})\] \[= \frac{1}{\ell_{\tau}^{2}}[P_{\pi_{b}}^{k-1}P_{\pi}](s,s^{\prime}).\]

Since the previous inequality holds for all \(s\) and \(s^{\prime}\), we in fact have \(\ell_{\tau}^{2}P_{\pi_{b}}^{k}\leq P_{\pi_{b}}^{k-1}P_{\pi}\) (which is an entry-wise inequality). Repeatedly using the previous inequality, we obtain

\[\ell_{\tau}^{2k}P_{\pi_{b}}^{k}\leq P_{\pi}^{k},\]

which implies

\[P_{\pi}^{r_{b}}(s,s^{\prime}) \geq\ell_{\tau}^{2r_{b}}P_{\pi_{b}}^{r_{b}}(s,s^{\prime})\] \[\geq\delta_{b}\ell_{\tau}^{2r_{b}}\mu_{b}(s^{\prime})\] \[\geq\delta_{b}\ell_{\tau}^{2r_{b}}\frac{\mu_{b}(s^{\prime})}{\mu _{\tau}(s^{\prime})}\mu_{\pi}(s^{\prime})\] \[\geq\delta_{b}\ell_{\tau}^{2r_{b}}\mu_{b,\min}\mu_{\pi}(s^{\prime }).\]

Following the proof of the Markov chain convergence theorem in [93, Section 4.3], we have

\[\|P_{\pi}^{k}(s,\cdot)-\mu_{\pi}(\cdot)\|_{\text{TV}}\leq(1-\delta_{b}\ell_{ \tau}^{2r_{b}}\mu_{b,\min})^{k/r_{b}-1},\quad\forall\;s\in\mathcal{S},\;\pi\in \Pi_{\tau}.\] (55)

Since \(A_{\max}\geq 2\) (otherwise there is no decision to make in this stochastic game), we have \(\ell_{\tau}^{2}\leq\frac{1}{2}\). It follows that \(1-\delta_{b}\ell_{\tau}^{2r_{b}}\mu_{b,\min}>1/2\). Using the previous inequality in Eq. (55), we have

\[\sup_{\pi\in\Pi_{\tau}}\max_{s\in\mathcal{S}}\|P_{\pi}^{k}(s, \cdot)-\mu_{\pi}(\cdot)\|_{\text{TV}} \leq 2(1-\delta_{b}\ell_{\tau}^{2r_{b}}\mu_{b,\min})^{k/r_{b}}\] \[\leq 2\exp\left(-\delta_{b}\ell_{\tau}^{2r_{b}}\mu_{b,\min}k/r_{b}\right)\] \[=2\rho_{b}^{\ell_{\tau}^{2r_{b}}\mu_{b,\min}k}\qquad\qquad\text{ (Recall that $\rho_{b}=\exp(-\delta_{b}/r_{b})$)}\] \[=2\rho_{\tau}^{k}.\]

We next compute the mixing time. Using the previous inequality and the definition of the total variation distance, we have

\[\sup_{\pi\in\Pi_{\tau}}\max_{s\in\mathcal{S}}\|P_{\pi}^{k}(s,\cdot)-\mu_{\pi}( \cdot)\|_{\text{TV}}\leq\eta\]

as long as

\[k\geq\frac{\log(2/\eta)}{\log(1/\rho_{\delta})}=\frac{1}{\ell_{\tau}^{2r_{b}} \mu_{b,\min}}\frac{\log(2/\eta)}{\log(1/\rho_{b})}\geq\frac{t_{\pi_{b},\eta}}{ \ell_{\tau}^{2r_{b}}\mu_{b,\min}}.\]

#### d.7.2 Proof of Lemma d.11

For any \(k\geq z_{k}\), we have

\[\mathbb{E}[(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})- \bar{F}_{k}^{i}(q_{k}^{i})^{\top}(q_{k}^{i}-\bar{q}_{k}^{i})]\] \[= \underbrace{\mathbb{E}[(F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{ k}^{-i},S_{k+1})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i}))^{\top}(q_{k-z_{k}}^{i}- \bar{q}_{k-z_{k}}^{i})]}_{N_{1}}\] \[+\underbrace{\mathbb{E}[(F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_ {k}^{-i},S_{k+1})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i}))^{\top}(q_{k}^{i}-q_{ k-z_{k}}^{i})]}_{N_{2}}\] \[+\underbrace{\mathbb{E}[(F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_ {k}^{-i},S_{k+1})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i}))^{\top}(\bar{q}_{k-z_ {k}}^{i}-\bar{q}_{k}^{i})]}_{N_{3}}\] \[+\underbrace{\mathbb{E}[(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{- i},S_{k+1})-F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1}))^{\top}(q_{k}^{i }-\bar{q}_{k}^{i})]}_{N_{4}}\] \[+\underbrace{\mathbb{E}[(\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})- \bar{F}_{k}^{i}(q_{k}^{i}))^{\top}(q_{k}^{i}-\bar{q}_{k}^{i})]}_{N_{5}}.\] (56)

To bound the terms \(N_{1}\) to \(N_{5}\) on the RHS of the previous inequality, the following lemma is needed.

**Lemma d.13**.: _For any positive integers \(k_{1}\leq k_{2}\), we have (1) \(\|q_{k_{2}}^{i}-q_{k_{1}}^{i}\|_{\infty}\leq\frac{2\alpha_{k_{1},k_{2}-1}}{1-\gamma}\), and (2) \(\max_{s\in\mathcal{S}}\|\pi_{k_{2}}^{i}(s)-\pi_{k_{1}}^{i}(s)\|_{1}\leq 2 \beta_{k_{1},k_{2}-1}\)._

Proof of Lemma d.13.: For any \(k\in[k_{1},k_{2}-1]\), we have by Eq. (41) that

\[\|q_{k+1}^{i}-q_{k}^{i}\|_{\infty}=\alpha_{k}\|F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\|_{\infty}\leq\frac{2\alpha_{k}}{1-\gamma}.\]

It follows that \(\|q_{k_{2}}^{i}-q_{k_{1}}^{i}\|_{\infty}\leq\frac{2\alpha_{k_{1},k_{2}-1}}{1-\gamma}\). Similarly, for any \(k\in[k_{1},k_{2}-1]\) and \(s\in\mathcal{S}\), we have

\[\|\pi_{k+1}^{i}(s)-\pi_{k}^{i}(s)\|_{1}=\beta_{k}\|\sigma_{\tau}(q_{k}^{i}(s)) -\pi_{k}^{i}(s)\|_{1}\leq 2\beta_{k},\]

which implies \(\max_{s\in\mathcal{S}}\|\pi_{k_{2}}^{i}(s)-\pi_{k_{1}}^{i}(s)\|_{1}\leq 2 \beta_{k_{1},k_{2}-1}\). 

We next bound the terms \(N_{1}\) to \(N_{5}\). Let \(\mathcal{F}_{k}\) be the \(\sigma\)-algebra generated the sequence of random variables \(\{S_{0},A_{0}^{i},A_{0}^{-i},\cdots,S_{k-1},A_{k-1}^{i},A_{k-1}^{-i},S_{k}\}\).

**The Term \(N_{1}\).** Using the tower property of conditional expectations, we have

\[N_{1} =\mathbb{E}[(F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{ k+1})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i}))^{\top}(q_{k-z_{k}}^{i}-\bar{q}_{k-z_{k}}^ {i})]\] \[=\mathbb{E}[(\mathbb{E}[F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{ k}^{i},A_{k}^{-i},S_{k+1})\mid\mathcal{F}_{k-z_{k}}]-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i}) ^{\top}(q_{k-z_{k}}^{i}-\bar{q}_{k-z_{k}}^{i})]\] \[\leq\mathbb{E}[\|\mathbb{E}[F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\mid\mathcal{F}_{k-z_{k}}]-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^ {i})\|_{1}\|q_{k-z_{k}}^{i}-\bar{q}_{k-z_{k}}^{i}\|_{\infty}]\] (57) \[\leq\frac{2}{1-\gamma}\mathbb{E}[\|\mathbb{E}[F^{i}(q_{k-z_{k}}^ {i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\mid\mathcal{F}_{k-z_{k}}]-\bar{F}_{k-z_{k} }^{i}(q_{k-z_{k}}^{i})\|_{1}]\] (Lemma D.1) \[\leq\frac{2}{1-\gamma}\mathbb{E}[\|\bar{F}_{k}^{i}(q_{k-z_{k}}^{i} )-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1}]\] \[\quad+\frac{2}{1-\gamma}\mathbb{E}[\|\mathbb{E}[F^{i}(q_{k-z_{k}}^{ i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\mid\mathcal{F}_{k-z_{k}}]-\bar{F}_{k}^{i}(q_{k-z_{k}}^ {i})\|_{1}].\] (58)

We next bound the two terms on the RHS of the previous inequality. Observe that

\[\|\bar{F}_{k}^{i}(q_{k-z_{k}}^{i})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}} ^{i})\|_{1}\] \[= \sum_{s,a^{i}}|[\bar{F}_{k}^{i}(q_{k-z_{k}}^{i})](s,a^{i})-[\bar{F }_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})](s,a^{i})|\] \[= \sum_{s,a^{i}}\left|[\mathbb{E}_{k}[F^{i}(q_{k-z_{k}}^{i},S,A^{i},A^ {-i},S^{\prime})](s,a^{i})-[\mathbb{E}_{k-z_{k}}[F^{i}(q_{k-z_{k}}^{i},S,A^{i},A^{- i},S^{\prime})](s,a^{i})\right|,\]where we use \(\mathbb{E}_{k}[\,\cdot\,]\) to denote \(\mathbb{E}_{S\sim\mu_{k}(\cdot),A^{i}\sim\pi_{k}^{i}(\cdot|S_{0}),A^{-i}\sim\pi_{ k}^{-i}(\cdot|S_{0}),S^{\prime}\sim\rho(\cdot|S_{0},A_{0}^{i},A_{0}^{-i})}[\,\cdot\,]\) for ease of presentation. To proceed, recall the following equivalent definition of total variation distance between probability measures \(p_{1},p_{2}\):

\[\|p_{1}-p_{2}\|_{\text{TV}}=\frac{1}{2}\sup_{f:\|f\|_{\infty}\leq 1}\left| \mathbb{E}_{p_{1}}[f]-\mathbb{E}_{p_{2}}[f]\right|.\]

It follows that

\[\left|[\mathbb{E}_{k}[F^{i}(q_{k-z_{k}}^{i},S,A^{i},A^{-i},S^{ \prime})](s,a^{i})-[\mathbb{E}_{k-z_{k}}[F^{i}(q_{k-z_{k}}^{i},S,A^{i},A^{-i},S ^{\prime})](s,a^{i})\right|\] \[\leq \max_{\bar{s},\bar{a}^{i},\bar{a}^{-i},\bar{s}^{\prime}}\left|[F^ {i}(q_{k-z_{k}}^{i},\bar{s},\bar{a}^{i},\bar{a}^{-i},\bar{s}^{\prime})](s,a^{ i})\right|\] \[\times\sum_{\bar{s},\bar{a}^{i},\bar{a}^{-i}}\left|\mu_{k}(\tilde {s})\pi_{k}^{i}(\tilde{a}^{i}|\tilde{s})\pi_{k}^{-i}(\tilde{a}^{-i}|\tilde{s} )-\mu_{k-z_{k}}(\tilde{s})\pi_{k-z_{k}}^{i}(\tilde{a}^{i}|\tilde{s})\pi_{k-z_ {k}}^{-i}(\tilde{a}^{-i}|\tilde{s})\right|\] \[\leq \frac{1}{1-\gamma}\left(\|\mu_{k}-\mu_{k-z_{k}}\|_{1}+\max_{s}\| \pi_{k}^{i}(s)-\pi_{k-z_{k}}^{i}(s)\|_{1}+\max_{s}\|\pi_{k}^{-i}(s)-\pi_{k-z_{ k}}^{-i}(s)\|_{1}\right)\] \[\leq \frac{2L_{p}}{1-\gamma}\left(\max_{s}\|\pi_{k}^{i}(s)-\pi_{k-z_{ k}}^{i}(s)\|_{1}+\max_{s}\|\pi_{k}^{-i}(s)-\pi_{k-z_{k}}^{-i}(s)\|_{1}\right)\] (Lemma B.1) \[\leq \frac{8L_{p}\beta_{k-z_{k},k-1}}{1-\gamma}.\] (Lemma D.13)

Therefore, we have

\[\|\bar{F}_{k}^{i}(q_{k-z_{k}}^{i})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{ k}}^{i})\|_{1} =\sum_{s,a^{i}}|[\bar{F}_{k}^{i}(q_{k-z_{k}}^{i})](s,a^{i})-[\bar{ F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})](s,a^{i})|\] \[\leq \frac{8|\mathcal{S}|A_{\max}L_{p}\beta_{k-z_{k},k-1}}{1-\gamma}.\] (59)

It remains to bound the second term on the RHS of Eq. (58). Recall that we denote \(P_{\pi}\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) as the transition probability matrix of the Markov chain \(\{S_{k}\}\) induced by a joint policy \(\pi\). Using the definition of conditional expectations, we have

\[\|\mathbb{E}[F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{ k+1})\mid\mathcal{F}_{k-z_{k}}]-\bar{F}_{k}^{i}(q_{k-z_{k}}^{i})\|_{1}\] \[= \left\|\sum_{s\in\mathcal{S}}\left[\left(\prod_{j=k+1}^{k+z_{k}} P_{\pi_{j-z_{k}}}\right)(S_{k-z_{k}},s)-\mu_{k}(s)\right]\sum_{a^{i}}\pi_{k}^{i}(a^{ i}|s)\sum_{a^{-i}}\pi_{k}^{-i}(a^{-i}|s)\right.\] \[\times\sum_{s^{\prime}}p(s^{\prime}|s,a^{i},a^{-i})F^{i}(q_{k-z_{ k}}^{i},s,a^{i},a^{-i},s^{\prime})\Bigg{\|}_{1}\] \[\leq \frac{2}{1-\gamma}\sum_{s\in\mathcal{S}}\left|\left(\prod_{j=k+1} ^{k+z_{k}}P_{\pi_{j-z_{k}}}\right)(S_{k-z_{k}},s)-\mu_{k}(s)\right|\] (Lemma D.9) \[\leq \frac{2}{1-\gamma}\left\{\sum_{s\in\mathcal{S}}\left|\left(\prod_{ j=k+1}^{k+z_{k}}P_{\pi_{j-z_{k}}}\right)(S_{k-z_{k}},s)-P_{\pi_{k}}^{z_{k}}(S_{k-z_{ k}},s)\right|\right.\] \[\left.+\sum_{s\in\mathcal{S}}\left|P_{\pi_{k}}^{z_{k}}(S_{k-z_{k} },s)-\mu_{k}(s)\right|\right\}\] \[\leq \frac{2}{1-\gamma}\left\{\left\|\prod_{j=k+1}^{k+z_{k}}P_{\pi_{j- z_{k}}}-P_{\pi_{k}}^{z_{k}}\right\|_{\infty}+2\rho_{\tau}^{z_{k}}\right\},\] (60)

where the last line follows from Lemma B.1 (2). Observe that

\[\left\|\prod_{j=k+1}^{k+z_{k}}P_{\pi_{j-z_{k}}}-P_{\pi_{k}}^{z_{k}}\right\|_{ \infty}=\left\|\sum_{\ell=1}^{z_{k}}\left(\prod_{j=k+1}^{k-\ell+1+z_{k}}P_{\pi_{ j-z_{k}}}P_{\pi_{k}}^{\ell-1}-\prod_{j=k+1}^{k-\ell+z_{k}}P_{\pi_{j-z_{k}}}P_{\pi_{k}}^{ \ell}\right)\right\|_{\infty}\]\[= \left|R_{i}(S_{k},A_{k}^{i},A_{k}^{-i})+\gamma v^{i}(S_{k+1})-q_{k}^{ i}(S_{k},A_{k}^{i})\right|\] \[\leq 1+\frac{\gamma}{1-\gamma}+\frac{1}{1-\gamma}\] \[= \frac{2}{1-\gamma}.\] (62)

Moreover, we have by Jensen's inequality that

\[\|\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1}\leq\frac{2}{1- \gamma}.\] (63)

Using Eqs. (62) and (63) together in Eq. (61), we have

\[N_{2}\leq\frac{8\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}.\]

**The Term \(N_{3}\).** For any \(k\geq z_{k}\), we have

\[N_{3} \leq\mathbb{E}[\|F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_ {k+1})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1}\|\bar{q}_{k-z_{k}}^{i}-\bar{ q}_{k}^{i}\|_{\infty}]\] \[\leq\mathbb{E}[(\|F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\|_{1}+\|\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1})\|\bar{q}_{k-z_{ k}}^{i}-\bar{q}_{k}^{i}\|_{\infty}]\] \[\leq\frac{4}{1-\gamma}\mathbb{E}[\|\bar{q}_{k-z_{k}}^{i}-\bar{q}_ {k}^{i}\|_{\infty}].\] (Eqs. ( 62 ) and ( 63 ))

Observe that

\[\|\bar{q}_{k-z_{k}}^{i}-\bar{q}_{k}^{i}\|_{\infty} =\max_{s\in\mathcal{S}}\|\mathcal{T}^{i}(v^{i})(s)(\pi_{k}^{-i}(s) -\pi_{k-z_{k}}^{-i}(s))\|_{\infty}\] \[\leq\max_{s\in\mathcal{S}}\|\mathcal{T}^{i}(v^{i})(s)\|_{1,\infty }\|\pi_{k}^{-i}(s)-\pi_{k-z_{k}}^{-i}(s)\|_{1}\] \[\leq\frac{2\beta_{k-z_{k},k-1}}{1-\gamma},\]

where the last line follows from Lemma D.13 and

\[\|\mathcal{T}^{i}(v^{i})(s)\|_{1,\infty}\leq\max_{s,a^{i},a^{-i}}|\mathcal{T} ^{i}(v^{i})(s,a^{i},a^{-i})|\leq\frac{1}{1-\gamma}.\]

Therefore, we have

\[N_{3}\leq\frac{8\beta_{k-z_{k},k-1}}{(1-\gamma)^{2}}.\]

**The Term \(N_{4}\).** For any \(k\geq z_{k}\), we have

\[N_{4} \leq\mathbb{E}[\|F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+ 1})-F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\|_{1}\|q_{k}^{i}- \bar{q}_{k}^{i}\|_{\infty}]\] \[\leq\frac{2}{1-\gamma}\mathbb{E}[\|F^{i}(q_{k}^{i},S_{k},A_{k}^{i },A_{k}^{-i},S_{k+1})-F^{i}(q_{k-z_{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1} )\|_{1}],\]

where the last line follows from Lemma D.1. Using the definition of \(F^{i}(\cdot)\), we have

\[\|F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})-F^{i}(q_{k-z _{k}}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})\|_{1}\] \[=\sum_{s,a^{i}}\mathds{1}_{\{(s,a^{i})=(S_{k},A_{k}^{i})\}}\left|q _{k-z_{k}}^{i}(S_{k},A_{k}^{i})-q_{k}^{i}(S_{k},A_{k}^{i})\right|\] \[=\left|q_{k-z_{k}}^{i}(S_{k},A_{k}^{i})-q_{k}^{i}(S_{k},A_{k}^{i})\right|\] \[\leq\|q_{k-z_{k}}^{i}-q_{k}^{i}\|_{\infty}\] \[\leq\frac{2\alpha_{k-z_{k},k-1}}{1-\gamma}.\] (Lemma D.13 )

It follows that

\[N_{4}\leq\frac{4\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}.\]

**The Term \(N_{5}\).** For any \(k\geq z_{k}\), we have

\[N_{5} \leq\mathbb{E}[\|\bar{F}_{k}^{i}(q_{k}^{i})-\bar{F}_{k-z_{k}}^{i} (q_{k-z_{k}}^{i})\|_{1}\|q_{k}^{i}-\bar{q}_{k}^{i}\|_{\infty}]\] \[\leq\frac{2}{1-\gamma}\mathbb{E}[\|\bar{F}_{k}^{i}(q_{k}^{i})- \bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1}]\] (Lemma D.1 ) \[\leq\frac{2}{1-\gamma}\mathbb{E}[\|\bar{F}_{k}^{i}(q_{k}^{i})- \bar{F}_{k-z_{k}}^{i}(q_{k}^{i})\|_{1}+\|\bar{F}_{k-z_{k}}^{i}(q_{k}^{i})-\bar{ F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1}]\] \[\leq\frac{16L_{p}|\mathcal{S}|A_{\max}\beta_{k-z_{k},k-1}}{(1- \gamma)^{2}}+\frac{2}{1-\gamma}\mathbb{E}[\|\bar{F}_{k-z_{k}}^{i}(q_{k}^{i})- \bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1}],\] (64 )

where the last line follows from the same analysis as we obtain Eq. (59). As for the second term on the RHS of Eq. (64), using the definition of \(\bar{F}_{k}^{i}(\cdot)\), we have

\[\|\bar{F}_{k-z_{k}}^{i}(q_{k}^{i})-\bar{F}_{k-z_{k}}^{i}(q_{k-z_{k}}^{i})\|_{1} =\sum_{s\in\mathcal{S}}\mu_{k-z_{k}}(s)\sum_{a^{i}}\pi_{k-z_{k}}^{i}(a^{i}\mid s )|q_{k}^{i}(s,a^{i})-q_{k-z_{k}}^{i}(s,a^{i})|\]\[\leq\|q_{k}^{i}-q_{k-z_{k}}^{i}\|_{\infty}\] \[\leq\frac{2\alpha_{k-z_{k},k-1}}{1-\gamma}.\] (Lemma D.13)

Using the previous inequality in Eq. (64), we obtain

\[N_{5}\leq\frac{16L_{p}|\mathcal{S}|A_{\max}\beta_{k-z_{k},k-1}}{(1-\gamma)^{2} }+\frac{4\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}.\]

Combining the upper bounds we derived for the terms \(N_{1}\) to \(N_{5}\) in Eq. (56), we have

\[\mathbb{E}[(F^{i}(q_{k}^{i},S_{k},A_{k}^{i},A_{k}^{-i},S_{k+1})- \tilde{F}_{k}^{i}(q_{k}^{i}-\tilde{q}_{k}^{i})]\] \[\leq\frac{36L_{p}|\mathcal{S}|A_{\max}z_{k}\beta_{k-z_{k},k-1}}{( 1-\gamma)^{2}}+\frac{8\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}+\frac{8\beta_{k-z _{k},k-1}}{(1-\gamma)^{2}}+\frac{4\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}\] \[\quad+\frac{16L_{p}|\mathcal{S}|A_{\max}\beta_{k-z_{k},k-1}}{(1- \gamma)^{2}}+\frac{4\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}\] \[\leq\frac{60L_{p}|\mathcal{S}|A_{\max}z_{k}\beta_{k-z_{k},k-1}}{ (1-\gamma)^{2}}+\frac{16\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}}\] \[\leq\frac{17z_{k}\alpha_{k-z_{k},k-1}}{(1-\gamma)^{2}},\]

where the last line follows from \(\beta_{k}/\alpha_{k}=c_{\alpha,\beta}\leq\frac{1}{60L_{p}|\mathcal{S}|A_{\max}}\) (cf. Condition 3.1).

### Proof of Corollary 3.1.1

The following proof idea was previous used in [15] to show the rationality of their decentralized \(Q\)-learning algorithm.

Observe that Theorem 3.1 can be easily generalized to the case where the reward is corrupted by noise. Specifically, suppose that player \(i\) takes action \(a^{i}\) and player \(-i\) takes action \(a^{-i}\). Instead of assuming player \(i\) receives a deterministic reward \(R_{i}(s,a^{i},a^{-i})\), we assume that player \(i\) receives a random reward \(r^{i}(s,a^{i},a^{-i},\xi)\), where \(\xi\in\Xi\) (\(\Xi\) is a finite set) is a random variable with distribution \(\mu_{\xi}(s)\), and is independent of everything else. The proof is identical as long as \(r^{i}+r^{-i}=0\), and the reward is uniformly bounded, i.e., \(\max_{s,a^{i},a^{-i},\xi}|r^{i}(s,a^{i},a^{-i},\xi)|<\infty\). Now consider the case where player \(i\)'s opponent follows a stationary policy \(\pi^{-i}\). We incorporate the randomness of player \(-i\)'s action into the model and introduce a fictitious opponent with only one action \(a^{*}\). In particular, let the random reward function be defined as \(\hat{r}^{i}(s,a^{i},a^{*},A^{-i})=R_{i}(s,a^{i},A^{-i})\) for all \((s,a^{i})\), where \(A^{-i}\sim\pi^{-i}(\cdot|s)\), and let \(\hat{p}(s^{\prime}\mid s,a^{i},a^{*})=\sum_{\pi^{-i}(a^{-i}|s)}p(s^{\prime} \mid a^{i},a^{-i},s)\). Now the problem can be reformulated as player \(i\) playing against the fictitious player with a single action \(a^{*}\), with reward function \(\hat{r}^{i}\) (\(i\in\{1,2\}\)) and transition probabilities \(\hat{p}\). Using the same proof for Theorem 3.1, we have the desired finite-sample bound.

## Appendix E On the Mixing Time of MDPs with Almost Deterministic Policies

Consider an MDP with two states \(s_{1},s_{2}\) and two actions \(a_{1},a_{2}\). The transition probability matrix \(P_{1}\) of taking action \(a_{1}\) is the identity matrix \(I_{2}\), and the transition probability matrix \(P_{2}\) of taking action \(a_{2}\) is \(P_{2}=[0,1;1,0]\). Given \(\alpha\in(1/2,1)\), let \(\pi_{\alpha}\) be a policy such that \(\pi(a_{1}|s)=\alpha\) and \(\pi(a_{2}|s)=1-\alpha\) for any \(s\in\{s_{1},s_{2}\}\). Denote \(P_{\alpha}\) as the transition probability matrix under \(\pi_{\alpha}\). It is easy to see that

\[P_{\alpha}=\begin{bmatrix}\alpha&1-\alpha\\ 1-\alpha&\alpha\end{bmatrix}.\]

Since \(P_{\alpha}\) is a doubly stochastic matrix, and has strictly positive entries, it has a unique stationary distribution \(\mu=\mathbf{1}^{\top}/2\).

We next compute a lower bound of the mixing time of the \(\pi_{\alpha}\)-induced Markov chain. Let \(e_{1}=[1,0]^{\top}\) be the initial distribution of the states, and denote \([x_{k},1-x_{k}]^{\top}\) as the distribution of the states at time step \(k\). Then we have

\[x_{k+1}=x_{k}\alpha+(1-x_{k})(1-\alpha)\]\[=(2\alpha-1)x_{k}+1-\alpha\] \[=(2\alpha-1)x_{0}+\sum_{i=0}^{k}(1-\alpha)(2\alpha-1)^{k-i}\] \[=\frac{1}{2}+\frac{(2\alpha-1)^{k+1}}{2}.\]

It follows that

\[t_{\pi_{\alpha},\eta} =\min_{k\geq 0}\left\{\max_{\mu_{0}\in\Delta^{2}}\left\|\mu_{0}^{ \top}P_{\alpha}^{k}-\mathbf{1}^{\top}/2\right\|_{\text{TV}}\leq\eta\right\}\] \[\geq\min_{k\geq 0}\left\{\left\|\left\|c_{1}^{\top}P_{\alpha}^{k}- \mathbf{1}^{\top}/2\right\|_{\text{TV}}\leq\eta\right\}\right.\] \[=\min_{k\geq 0}\left\{(2\alpha-1)^{k}\leq 2\eta\right\}\] \[\geq\frac{\log(1/2\eta)}{\log(1/(2\alpha-1))}-1,\]

which implies \(\lim_{\alpha\to 1}t_{\alpha,\eta}=\infty\). Therefore, as the policies become deterministic, the mixing time of the associated Markov chain can approach infinity.

## Appendix F Numerical Simulations

We first conduct numerical simulations to investigate the impact of choosing different \(\tau\), which is used to define the softmax operator in Algorithms 1 and 2. Our theoretical results indicate that there is an asymptotically non-vanishing bias due to using a positive \(\tau\). Intuitively, since a softmax policy always has strictly positive entries while a Nash equilibrium policy can have zero entries, we cannot, in general, expect the Nash gap to converge to zero.

To demonstrate this phenomenon, consider the following example of a zero-sum matrix game. Let

\[R_{1}=\begin{bmatrix}N&1&-1\\ -1&0&1\\ 1&-1&0\end{bmatrix}\]

be the payoff matrix for player \(1\), and let \(R_{2}=-(R_{1})^{\top}\), where \(N>0\) is a tunable parameter. Note that this matrix game has a unique Nash equilibrium, which goes to the joint policy \(\pi^{1}=(1/3,2/3,0)\), \(\pi^{2}=(0,2/3,1/3)\) as \(N\rightarrow\infty\). In our simulations, we use constant stepsizes \(\alpha_{k}\equiv 0,5\) and \(\beta_{k}\equiv 0.01\) and run Algorithm 1 for \(100\) trajectories (each has \(K=2000\) iterations). Then, we plot the average Nash gap (averaged over the \(100\) trajectories) as a function of the number of iterations \(k\) in Figure 1 for different temperatures \(\tau\). To enable a fair comparison, we use the normalized \(q\)-function to compute the softmax, that is, instead of directly using \(\sigma_{\tau}(q_{k}^{i})\) in Algorithm 1, we use \(\sigma_{\tau}(q_{k}^{i}/\|q_{k}^{i}\|_{2})\). As we can see in Figure 1, as \(\tau\) increases, the asymptotic error also increases, which is consistent with our theoretical results.

### Comparison with the Optimistic Multiplicative-Weights Update

The Optimistic Multiplicative-Weights Update (OMWU) was recognized as a popular learning algorithm for zero-sum matrix games [98]. Since OMWU in the payoff-based setting (or noisy-feedback setting) may not have last-iterate convergence [99], to enable a fair comparison, we will compare OMWU in the noiseless setting (which does enjoy last-iterate convergence [98]) with smoothed-best response dynamics. We start by writing down the algorithm.

**OMWU:** With initializations \(\pi_{0}^{1},\pi_{1}^{1}\) (respectively, \(\pi_{0}^{2}\), \(\pi_{1}^{2}\)) that live in the interior of the probability simplex \(\Delta(\mathcal{A}^{1})\) (respectively, \(\Delta(\mathcal{A}^{2})\)), OMWU updates \((\pi_{k}^{1},\pi_{k}^{2})\) iteratively according to

\[\pi_{k+1}^{i}(a^{i})=\frac{\pi_{k}^{i}(a^{i})\exp(2\eta[R_{i}\pi_{k}^{-i}](a^ {i})-\eta[R_{i}\pi_{k-1}^{-i}(a^{i})])}{\sum_{\tilde{a}^{i}\in\mathcal{A}^{i} }\pi_{k}^{i}(\tilde{a}^{i})\exp(2\eta[R_{i}\pi_{k}^{-i}](\tilde{a}^{i})-\eta[R _{i}\pi_{k-1}^{-i}(\tilde{a}^{i})])},\;\forall\,a^{i}\in\mathcal{A}^{i},i\in \{1,2\},\]

where \(\eta\in(0,1)\) is the stepsize.

**The Discrete Smoothed Best-Response Dynamics (DSBR):** With arbitrary initializations \(\pi_{0}^{1}\in\Delta(\mathcal{A}^{1})\) and \(\pi_{0}^{2}\in\Delta(\mathcal{A}^{2})\), the discrete smoothed best-response dynamics update \((\pi_{k}^{1},\pi_{k}^{2})\) iteratively according to

\[\pi_{k+1}^{i}=(1-\beta_{k})\pi_{k}^{i}+\beta_{k}\sigma_{\tau}(R_{i}\pi_{k}^{-i}),\quad\forall\,i\in\{1,2\},\]

where \(\beta_{k}\) is the stepsize.

We perform two sets of numerical simulations to compare OMWU and DSBR. Our first experiment is implemented on the rock-paper-scissor game, where the payoff matrix for player \(1\) is

\[R_{1}=\begin{bmatrix}0&1&-1\\ -1&0&1\\ 1&-1&0\end{bmatrix},\]

and \(R_{2}=-(R_{1})^{\top}\). As we see in Figure 2, the convergence rates of OMWU and DSBR are comparable. However, DSBR seems to be more stable compared with OMWU. Note that while we use softmax policies in DSBR, since the rock-paper-scissor game has a unique Nash equilibrium, which is also the unique Nash equilibrium of the entropy-regularized matrix game for any temperature \(\tau>0\), there is no smoothing bias and the Nash gap under DSBR does converge to zero.

Figure 1: The Nash Gap for Different Temperatures \(\tau\)

Figure 2: The Nash Gap as a Function of the Number of Iterations \(k\)In our second numerical simulation, we set the payoff matrix of player \(1\) to be

\[R_{1}=\begin{bmatrix}N&1&-1\\ -1&0&1\\ 1&-1&0\end{bmatrix},\]

and \(R_{2}=-(R_{1})^{\top}\), where we choose \(N=100\). Note that as \(N\to\infty\), the unique Nash equilibrium goes to \(\pi^{1}=(1/3,2/3,0)\), \(\pi^{2}=(0,2/3,1/3)\). In this case, we also see from Figure 3 that DSBR is more stable compared with OMWU. However, since in this case, the Nash equilibrium has zero entries, due to the use of softmax policies, DSBR suffers from an asymptotically non-vanishing bias. This is clear from Figure 4, which plots the asymptotic behavior of Figure 3. We see that OMWU converges to zero while the Nash gap from DSBR converges to a positive real number.

Figure 4: The Asymptotic Behavior of Figure 3

Figure 3: The Nash Gap as a Function of the Number of Iterations \(k\)