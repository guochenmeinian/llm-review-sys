# Deep Multi-Marginal Momentum Schrodinger Bridge

 Tianrong Chen, Guan-horng Liu, Molei Tao, Evangelos A. Theodorou

Georgia Institute of Technology, USA

{tianrong.chen,ghliu, mtao, evangelos.theodorou}@gatech.edu

###### Abstract

It is a crucial challenge to reconstruct population dynamics using unlabeled samples from distributions at coarse time intervals. Recent approaches such as flow-based models or Schrodinger Bridge (SB) models have demonstrated appealing performance, yet the inferred sample trajectories either fail to account for the underlying stochasticity or are unnecessarily rigid. In this article, We extend the approach in [1] to operate in continuous space and propose Deep Momentum Multi-Marginal Schrodinger Bridge (DMSB), a novel computational framework that learns the smooth measure-valued spline for stochastic systems that satisfy position marginal constraints across time. By tailoring the celebrated Bregman Iteration and extending the Iteration Proportional Fitting to phase space, we manage to handle high-dimensional multi-marginal trajectory inference tasks efficiently. Our algorithm outperforms baselines significantly, as evidenced by experiments for synthetic datasets and a real-world single-cell RNA sequence dataset. Additionally, the proposed approach can reasonably reconstruct the evolution of velocity distribution, from position snapshots only, when there is a ground truth velocity that is nevertheless inaccessible.

## 1 Introduction

We consider the multi-marginal trajectory inference problem, which pertains to elucidating the dynamics and reactions of indiscernible individuals, given static snapshots of them taken at sporadic time points. Due to the inability of tracking each individual, one considers the evolution of the statistical distribution of the population instead. This problem received considerable attention, and associated applications appear in various scientific areas such as estimating cell dynamics [2; 3], predicting meteorological evolution [4], and medical healthcare statistics tracking [5]. [6; 7] constructed an energy landscape that best aligned with empirical observations using neural network. [8; 9] learn regularized Neural ODE [10] to encode such potential landscape. Notably, in the aforementioned work, the trajectory of samples is represented in a deterministic way. In contrast, [11; 12] employ Schrodinger Bridge (SB) to determine the most likely evolution of samples between marginal distributions when individual sample trajectories are also affected by environmental stochasticity. Yet, these approaches scale poorly w.r.t. the state dimension due to specialized neural network architectures and computational frameworks.

SB can be viewed as a solution to the entropy-regularized optimal transport problem. SB seeks a nonlinear SDE that yields a straight path measure between two _arbitrary_ distributions. The straightness is implied by achieving optimality of minimizing transportation costs (i.e. 2-Wasserstein distance (\(W_{2}\))). We note SB is often related to Score-based Generated Model (SGM), both of which can be used for generative modeling by constructing certain Stochastic Differential Equation (SDE) that links data distribution and a tractable prior distribution (i.e. 2 marginals). SGM accomplishes the generative task by first diffusing data to prior through a pre-specified linear SDE, during which a neural network is also learned to approximate the score function. Then this score approximator is used to reverse this diffusion process, and consequently establish the generation. Critically-dampedLangevin Diffusion (CLD) [13] extends the SGM SDE to the phase space by introducing an auxiliary velocity variable with a tractable Gaussian distribution at both the initial and terminal time. The resulting trajectory in the position space becomes smoother, as stochasticity is only injected into the velocity space, and the empirical performance and sample efficiency are enhanced due to the structure of the critical damped SDE. The connection between SGM and SB has been elaborated in [14; 15] and scalable mean matching Iterative Proportional Fitting algorithm (IPF) is proposed to estimate SB efficiently in high dimensional cases. Applications of SB, such as image-to-image transformation [16; 17], RNA trajectory inference [11], solving Mean Field Game[18], Riemannian interpolation [19], demonstrate the effectiveness of SB in various domains.

In this work, we start with SB in phase space (termed momentum SB, mSB in short), and then further investigate mSB with multiple empirical marginal constraints present in the position space, which was formulated as multi-marginal mSB (mmmSB) in [1]. This circumvents the need for expensive space discretization which does not scale well to high dimensions. We also address the challenge of intricate geometric averaging in continuous space setup by strategically partitioning and reorganizing the constraint sets. Furthermore, we enhance the algorithm's computational efficiency by incorporating the method of half-bridge IPF. The optimality of transportation cost in SB leads to straight trajectories, and if one solves N 2-marginal SB problems and connect the resulting trajectories to match N+1 marginals, the connected trajectories will have kinks at all connection points. On the contrary, in mmmSB, the optimality of transportation cost leads to a smooth measure-spline over the state space that also interpolates the empirical marginals. Therefore, this approach is highly suitable for problems originated from physical systems and/or those that should have smooth trajectories, such as trajectory inference in single-cell RNA sequencing. Our research will emphasize on solving mmmSB efficiently in high-dimensions (thus the approach will differ from that in the seminal work [1]; see Sec.4). The differences between our algorithm and prior work are demonstrated in Table.1, and the main contributions of our work are fourfold:

* We extend the mean matching IPF to phase space allowing for scalable mSB computing.
* We introduce and tailor the Bregman Iteration [21] for mmmSB which makes it compatible with the phase space mean matching objective, thus the efficient computation is activated for high dimensional mmmSB.
* We show how to overcome the challenge of sampling the velocity variable when it is not available in training data, which enhances the applicability of our model.
* We show the performance of proposed algorithm DMSB on toy datasets which contains intricate bifurcations and merge. On realistic high-dimension (100-D) single-cell RNA-seq (scRNA-seq) datasets, DMSB outperforms baselines by a significant margin in terms of the quality of the generated trajectory both visually and quantitatively. We show that DMSB is able to capture reasonable velocity distribution compared with ground truth while other baselines fail.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Models & Optimality & \(p_{0}(\cdot)\) & \(p_{1}(\cdot)\) \\ \hline SGM [20] & ✗ & \(p_{\mathcal{A}}(x)\) & \(\mathcal{N}(\mathbf{0},\mathbf{\Sigma})\) \\ CLD [13] & ✗ & \(p_{\mathcal{A}}(x)\otimes\mathcal{N}(\mathbf{0},\mathbf{\Sigma})\) & \(\mathcal{N}(\mathbf{0},\mathbf{\Sigma})\otimes\mathcal{N}(\mathbf{0},\mathbf{ \Sigma})\) \\ SB [14] & \(\downarrow W_{2}\rightarrow\) kinks & \(p_{\mathcal{A}}(x)\) & \(p_{\mathcal{B}}(x)\) \\ DMSB (ours) & \(\downarrow W_{2}\rightarrow\) smooth & \(p_{\mathcal{A}}(x)q_{\theta}(v|x)\) & \(p_{\mathcal{B}}(x)q_{\phi}(v|x)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between different models in terms of optimality and boundary distributions \(p_{0}\) and \(p_{1}\). Our DMSB extends standard SB, which generalizes SGM beyond Gaussian priors, to phase space, similar to CLD. However, unlike CLD, DMSB jointly _learns_ the phase space distributions, i.e., \(p_{\theta}(x,v)=p_{\mathcal{A}}(x)q_{\theta}(v|x)\) and \(p_{\phi}(x,v)=p_{\mathcal{B}}(x)q_{\phi}(v|x)\). In other words, DMSB infers the underlying phase state dynamics given only state distributions.

Preliminary

### Dynamical Schrodinger Bridge problem

Dynamical Schrodinger Bridge problem has been extensively studied in the past few decades. The objective of the SB problem is to solve the following optimization problem:

\[\min_{\pi\in\Pi(\rho_{0},\rho_{T})}D_{KL}\left(\pi||\xi\right),\] (1)

where \(\pi\in\Pi(\rho_{0},\rho_{T})\) belongs to a set of path measures with its marginal densities at \(t=0\) and \(T\) being \(\rho_{0}\) and \(\rho_{T}\). \(\xi\) is the reference path measure (i.e., [14] sets \(\xi\) as Wiener process from \(\rho_{0}\)). The optimality of the problem (1) is characterized by a set of PDEs (3).

**Theorem 2.1** ([22]).: _The optimal path measure \(\pi\) in the problem (1) is represented by forward and backward stochastic processes_

\[\mathrm{d}\mathbf{x}_{t} =[2\;\nabla_{\mathbf{x}}\log\Psi_{t}]\mathrm{d}t+\sqrt{2}\mathrm{ d}\mathbf{w}_{t},\quad\mathbf{x}_{0}\sim\rho_{0},\] (2a) \[\mathrm{d}\mathbf{x}_{t} =[-2\;\nabla_{\mathbf{x}}\log\widehat{\Psi}_{t}]\mathrm{d}t+ \sqrt{2}\;\mathrm{d}\widehat{\mathbf{w}}_{t},\mathbf{x}_{T}\sim\rho_{T}.\] (2b)

_in which \(\Psi,\widehat{\Psi}\in C^{1,2}\) are the solutions to the following coupled PDEs,_

\[\frac{\partial\Psi_{t}}{\partial t} =-\Delta\Psi_{t},\quad\frac{\partial\widehat{\Psi}_{t}}{\partial t }=\Delta\widehat{\Psi}_{t}\] (3) _s.t._ \[\Psi(0,\cdot)\widehat{\Psi}(0,\cdot)=\rho_{0}(\cdot),\;\Psi(T, \cdot)\widehat{\Psi}(T,\cdot)=\rho_{T}(\cdot),\]

The stochastic processes of SB in (2a) and (2b) are equivalent in the sense of \(\forall t\in[0,T],p_{t}^{(2a)}\equiv p_{t}^{(2b)}\equiv p_{t}^{SB}\). Here \(p_{t}^{SB}\) stands for the marginal distribution of SB at time \(t\), which also represents the marginal density of stochastic process induced by either of Eq.2. The potentials \(\Psi_{t}\) and \(\widehat{\Psi}_{t}\) explicitly represent the solution of Fokker-Plank Equation (FPE) and Hamilton-Jacobi-Bellman equation (HJB) after exponential transform [14] where FPE describes the evolution of samples density and HJB represents for the optimality of Eq.1. Furthermore, the marginal density also obeys a factorization of \(p_{t}^{SB}=\Psi_{t}\widehat{\Psi}_{t}\). Such rich structures of SB will later on be used to construct the log-likelihood objective (Thm.B.1) and Langevin sampler for velocity (SS4.4).

To solve SB, prior work have primarily used the half-bridge optimization technique, also known as Iterative Proportional Fitting (IPF), in which one iteratively solves the optimization problem with one of the two boundary conditions [14; 15; 23],

\[\pi^{(d+1)}:=\operatorname*{arg\,min}_{\pi\in\Pi(\cdot,\rho_{1})}D_{KL}(\pi ||\pi^{(d)})\quad\rightleftarrows\quad\pi^{(d+2)}:=\operatorname*{arg\,min} _{\pi\in\Pi(\rho_{0},\cdot)}D_{KL}(\pi||\pi^{(d+1)})\] (4)

with initial path measure \(\pi^{(0)}:=\xi\). By repeatedly iterating over aforementioned optimizations until the algorithm converges, the SB solution will be attained as \(\pi^{SB}\equiv\lim_{d\to\infty}\pi^{(d)}\)[24]. In addition, [25] shows that the drift term in SB problem can also be interpreted as the solution Stochastic Optimal Control (SOC) problem by having optimal control policy \(\mathbf{z}^{*}=2\;\nabla_{\mathbf{x}}\log\Psi(t,\mathbf{x}_{t})\):

\[\mathbf{z}^{*}(\mathbf{x})\in\operatorname*{arg\,min}_{\mathbf{z}\in \mathcal{Z}}\mathbb{E}\left[\int_{0}^{T}\frac{1}{2}\|\mathbf{z}_{t}\|^{2} \mathrm{d}t\right]\quad s.t\quad\begin{cases}\mathrm{d}\mathbf{x}_{t}= \mathbf{z}_{t}\mathrm{d}t+\sqrt{2}\,\mathrm{d}\mathbf{w}_{t}\\ \mathbf{x}_{0}\sim\rho_{0},\quad\mathbf{x}_{1}\sim\rho_{T}.\end{cases}\]

This formulation will be used later on for constructing phase space likelihood objective function in SS3. Regarding solving the half-bridge problem, abundant results exist in the literature for the vanilla SB described above [14; 15; 23], but we will be solving a different SB problem; see Prop.4.1 for formulation and SS4 for a solution.

### Bregman Iterations for Multiple Constraints

Bregman iteration [21] can be viewed as a multiple marginal generalization of IPF, and it is widely used to solve entropy regularized optimal transport problem [1] with multiple constraints. The algorithm can efficiently solve problems in the form of,

\[\inf_{\pi\in\mathcal{K}}KL\left(\pi|\xi\right),\]where \(\mathcal{K}\) is the intersection of multiple closed convex constraint sets \(\mathcal{K}_{l}\): \(\mathcal{K}=\cap_{l=1}^{L}\mathcal{K}_{l}\). Bregman Projection (BP) is defined as optimization w.r.t one of the constraint \(\mathcal{K}_{l}\),

\[P_{\mathcal{K}_{l}}^{KL}(\xi):=\operatorname*{arg\,min}_{\pi\in\mathcal{K}_{l} }KL(\pi|\xi),\]

and \(d\)-th Bregman Iteration (BI) is recursively computing BP over all the constraints in \(\mathcal{K}\):

\[\forall 0<n\leq L,\quad\pi^{(d,n)}:=P_{\mathcal{K}_{l}^{T}}^{KL}(\pi^{(d,n-1)}),\]

The initial condition for (\(d+1\))-th BI is \(\pi^{(d+1,0)}=\pi^{(d,L)}\). Under certain conditions (see e.g., [24]), one has that \(\pi^{(d,L)}\) converges to the unique solution:

\[\pi^{(d,L)}\to P_{\mathcal{K}}^{KL}(\xi)\quad\text{as}\quad d\to+\infty\]

**Remark 2.2**.: One BI traverses all constraints via multiple BPs, and each BP solves an optimization problem with one constraint.One can notice that the BI will become the aforementioned IPF procedure solving SB problem (1) by defining \(L=2\), \(\mathcal{K}_{1}=\Pi(\rho_{0},\cdot)\), \(\mathcal{K}_{2}=\Pi(\cdot,\rho_{1})\).

## 3 Momentum Schrodinger Bridge

We first describe how to conduct half-bridge IPF training in the phase space, which can be used to solve momentum SB (mSB) problem with two marginals constraints. This scalable phase space half-bridge technique will then be applied to multi-marginal cases (Sec.4). Fig.1 demonstrates how we develop an algorithm based on [14]. Notations used in following sections are listed in Table.2. mSB extends SB problem to phase space, which consists of both position and velocity.

\begin{table}
\begin{tabular}{c c c} \hline \hline Notation & Definition & Notation & Definition \\ \hline \(\mathbf{x}\) & position variable & \(\rho\) & position distribution \(\rho(\mathbf{x})\) \\ \(\mathbf{v}\) & velocity variable & \(\gamma\) & velocity Distribution \(\gamma(\mathbf{v})\) \\ \(\mathbf{m}\) & concatenation of \([\mathbf{x},\mathbf{v}]^{\mathsf{T}}\) & \(\mu\) & distribution of \(\mu(\mathbf{x},\mathbf{v})\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Mathematical notation.

Figure 1: A summary of various SB problems and corresponding algorithms. The toy example in the 3rd row illustrates that vanilla SB determines ‘straight’ paths (modulo fluctuations due to noise) between pairwise empirical marginals, while our multi-marginal momentum SB approach establishes a smooth measure-spline between marginals in the position space (albeit still stochastic, the path is smooth between any pair of adjacent 2 marginals, because noise is added to velocity, and the path is also smooth across different pairs of adjacent 2 marginals per design.

We will first consider boundary distributions that depend on both \(\mathbf{x}\) and \(\mathbf{v}\), although eventually we will use this as a module to find transport maps between two distributions that only depend on position \(\mathbf{x}\), as velocity \(\mathbf{v}\) is an auxiliary variable artificially introduced for obtaining smooth transport. Conceptually, as an entropy regularized optimal transport problem, SB tries to obtain the straightest path between empirical marginals of positions \(\mathbf{x}\) with additive noise, but mSB aims at finding the smooth interpolation between empirical marginals of \(\mathbf{x}\)[26] conditioned on boundary velocity distributions (see Fig.1). Such smooth measure-valued splines in the position space are obtained by the optimization problem in the phase space [1]:

\[\min_{\pi\in\Pi(\mu_{0},\mu_{T})}KL(\pi|\xi)\quad s.t\quad\pi=\text{ Law}(\mathbf{x},\mathbf{v}):\underbrace{\left(\frac{\text{d}\mathbf{x}_{t}}{\text{d} \mathbf{v}_{t}}\right)}_{\text{d}\mathbf{m}_{t}}=\underbrace{\left(\mathbf{v} _{t}\right)}_{\text{\bf{$\mathbf{f}$}}(\mathbf{v},t)}\text{d}t+\underbrace{ \left(\frac{\mathbf{0}\ \mathbf{0}}{\mathbf{0}\ g_{t}}\right)}_{\text{\bf{$\mathbf{g}$}}(t)} \underbrace{\left(\mathbf{z}_{t}\right)}_{\text{\bf{$\mathbf{Z}$}}(t)}\text{d}t +\underbrace{\left(\frac{\mathbf{0}\ \mathbf{0}}{\mathbf{0}\ g_{t}}\right)}_{\text{\bf{$\mathbf{g}$}}(t)}\text{d} \mathbf{w}_{t},\]

Similar to Theorem 2.1, one can derive a set of PDEs using the potential functions \(\Psi(t,\mathbf{x},\mathbf{v})\) and \(\widehat{\Psi}(t,\mathbf{x},\mathbf{v})\), and subsequently apply IPF procedure to solve the problem. The formulation of the phase space PDE can be found in Appendix.B.2. Such PDE representation of mSB results in a straightforward yet innovative log-likelihood training that enables efficient optimization of the IPF.

**Proposition 3.1** (likelihood bound).: _The half-bridge IPF in phase space_

\[\pi^{(d+1)}:=\operatorname*{arg\,min}_{\pi\in\Pi(\mu_{0},\cdot)}D_{KL}(\pi|| \pi^{(d)})\quad\rightleftarrows\quad\pi^{(d+2)}:=\operatorname*{arg\,min}_{ \pi\in\Pi(\cdot,\mu_{T})}D_{KL}(\pi||\pi^{(d+1)})\]

_represents the bound of the likelihood and gives approximate likelihood training:_

\[\mathbf{Z}_{t}:=\operatorname*{arg\,min}_{\mathbf{Z}_{t}}-\log p(\mathbf{m}_ {0},0)\quad\rightleftarrows\quad\widehat{\mathbf{Z}}_{t}:=\operatorname*{arg \,min}_{\widehat{\mathbf{Z}}_{t}}-\log p(\mathbf{m}_{T},T).\]

\[\text{where}\quad\log p(\mathbf{m}_{0},0)\propto\int_{0}^{T}\mathbb{E}_{ \widehat{\mathbf{m}}_{t}}\left[\frac{1}{2}\|\widehat{\mathbf{z}}_{t}+\mathbf{ z}_{t}-g\nabla_{\mathbf{v}}\log\hat{p}_{t}\|^{2}\right]\text{d}t.\]

\[\text{and}\ \widehat{\mathbf{m}}_{t}\ \text{samples from:}\quad\ \text{d}\widehat{\mathbf{m}}_{t}=\left[\bm{f}-\text{g}\widehat{\mathbf{Z}}_{t }\right]\text{d}t+\text{g}(t)\text{d}\mathbf{w}_{t},\quad\widehat{\mathbf{m}} _{T}\sim\mu_{T}\] (5)

\(\widehat{\mathbf{Z}}_{t}\triangleq\left(\begin{matrix}\mathbf{0}\\ \widehat{\mathbf{z}}_{t}\end{matrix}\right)\) and \(\widehat{p}_{t}\) is the density of path measure induced by eq.5 at time \(t\). A similar result for \(\log(\mathbf{m}_{T},T)\) can be obtained in a similar derivation.

Proof.: See Appendix B.1. 

**Remark 3.2**.: After optimizing \(\widehat{\mathbf{Z}}_{t}\), the reference path measure becomes eq.5, which implies \(\pi\in\Pi(\cdot,\mu_{T})\), i.e., the constraint in half-bridge IPF is satisfied. A path measure \(\pi\) is induced by either \(\mathbf{Z}_{t}\) or \(\widehat{\mathbf{Z}}_{t}\). As being mentioned in Remark.2.2. One half-bridge IPF is basically one BP and one IPF is one BI. Prop.3.1 provides a convenient way to perform one BP in the form of \(\pi:=\operatorname*{arg\,min}_{\pi\in\mathcal{K}_{l}}D_{KL}(\pi||\pi)\) by maximizing log-likelihood given constraint \(\mathcal{K}\) and reference path measure \(\bar{\pi}\).

Prop.3.1 provides an alternative way to conduct the BI which will be heavily used in mmmSB SS3, and it is computationally efficient after parameterizing and discretization (SS4.4).

## 4 Deep Momentum Multi-Marginal Schrodinger Bridge

We first state the problem formulation of momentum multi-marginal Schrodinger Bridge (mmmSB). Different from previous two marginals case, we consider the scenario where \(N+1\) probability measures \(\mu_{t_{i}}\) are lying at time \(t_{i}\). In addition, velocity distributions are not necessarily known.

**Proposition 4.1** ([1]).: _The dynamical mmmSB with multiple marginal constraints reads:_

\[\min_{\pi}\mathcal{J}(\pi):=\sum_{i=0}^{N-1}KL\left(\pi_{t_{i},t_{i+1}}|\xi_{t_ {i},t_{i+1}}\right),\quad\text{s.t}\quad\pi\in\mathcal{K}:=\cap_{i=0}^{N} \mathcal{K}_{t_{i}}\] (6)_where:_ \[\mathcal{K}_{t_{0}} =\left\{\int\pi_{t_{0}:t_{1}}\mathbf{d}\mathbf{m}_{t_{1}}=\mu_{t_{0} },\int\mu_{t_{0}}\mathbf{d}\mathbf{v}_{t_{0}}=\rho_{t_{0}}\right\}\] \[\mathcal{K}_{t_{N}} =\left\{\int\pi_{t_{N-1}:t_{N}}\mathbf{d}\mathbf{m}_{t_{N-1}}=\mu _{t_{N}},\int\mu_{t_{N}}\mathbf{d}\mathbf{v}_{t_{N}}=\rho_{t_{N}}\right\}\] \[\mathcal{K}_{t_{i}} =\left\{\int\pi_{t_{i}:t_{i+1}}\mathbf{d}\mathbf{m}_{t_{i+1}}=\mu _{t_{i}},\int\pi_{t_{i-1}:t_{i}}\mathbf{d}\mathbf{m}_{t_{i-1}}=\mu_{t_{i}}, \int\mu_{t_{i}}\mathbf{d}\mathbf{v}_{t_{i}}=\rho_{t_{i}}\right\},\] (7)

_and \(\mathcal{K}\) is the intersection of close convex set of \(\mathcal{K}_{t_{i}}\)._

The problem described in Prop.4.1 can be solved by classical BI algorithm integrated with Sinkhorn method [1]. However, due to the curse of dimensionality and unfavorable geometric explicit solution, the BP cannot be applied in high-dimensional and continuous state space directly. To tackles these difficulties, we parameterize the forward and backward policies \(\mathbf{z}_{t}\) and \(\widehat{\mathbf{z}}_{t}\) by a pair of neural networks. We further decouple and resemble the constraints by which it enables the scalable likelihood IPF and avoids the geometric averaging issue under mmmSB context.

### Decoupling and Reassembling Constraints

We decompose the constraint set (7) by

\[\mathcal{K}_{t_{i}}=\cap_{r=0}^{2}\mathcal{K}_{t_{i}}^{r},\quad\text{where} \quad\quad\begin{array}{l}\mathcal{K}_{t_{i}}^{0}=\left\{\int\pi_{t_{i}:t_{i +1}}\mathbf{d}\mathbf{m}_{t_{i+1}}=\hat{\mu}_{t_{i}},\int\hat{\mu}_{t_{i}} \mathbf{d}\mathbf{v}_{t_{i}}=\rho_{t_{i}}\right\}\\ \mathcal{K}_{t_{i}}^{1}=\left\{\int\pi_{t_{i-1}:t_{i}}\mathbf{d}\mathbf{m}_{t_ {i-1}}=\mu_{t_{i}},\int\mu_{t_{i}}\mathbf{d}\mathbf{v}_{t_{i}}=\rho_{t_{i}} \right\}\\ \mathcal{K}_{t_{i}}^{2}=\left\{\int\pi_{t_{i}:t_{i+1}}\mathbf{d}\mathbf{m}_{t_ {i+1}}=\int\pi_{t_{i-1}:t_{i}}\mathbf{d}\mathbf{m}_{t_{i-1}}\right\}.\end{array}\] (8)

One can notice that the \(\mathcal{K}_{t_{i}}^{0}\) and \(\mathcal{K}_{t_{i}}^{1}\) share similar structure as simpler boundary marginal conditions \(\mathcal{K}_{t_{0}}\) and \(\mathcal{K}_{t_{N}}\), hence we can get rid of the notorious geometric averaging (see SS4 in [1]). Notably, this type of constraint provides an opportunity to utilize Proposition 3.1 for optimization, but the joint distribution of \(\mathbf{x}\) and \(\mathbf{v}\) is still absent. We classify the constraints into two categories:

\[\mathcal{K}_{\text{boundary}}=\left\{\cap_{i=1}^{N-1}\mathcal{K}_{t_{i}}^{r} \cap\mathcal{K}_{t_{0}}\cap\mathcal{K}_{t_{N}}|\forall r\in\{0,1\}\right\}, \quad\mathcal{K}_{\text{bridge}}=\left\{\cap_{i=1}^{N-1}\mathcal{K}_{t_{i}}^{2} \right\}.\]

By following BI (SS2.2), we execute optimization w.r.t. (6) while projecting the solution to subset of \(\mathcal{K}_{\text{boundary}}\) or \(\mathcal{K}_{\text{bridge}}\) iteratively. The sketch can be found in Fig.2. The next sections will provide more details on obtaining the joint distribution \(\mu\) and optimizing within each constraint set.

Hereafter, we only demonstrate the optimization for forward policy \(\mathbf{z}_{t}\) given reference path measure \(\bar{\pi}\) driven by fixed backward policy \(\widehat{\mathbf{z}}_{t}\). The procedure can be applied for the \(\widehat{\mathbf{z}}_{t}\) and vice versa.

### Optimization in set \(\mathcal{K}_{\text{boundary}}\)

We first show how to optimize forward policy \(\mathbf{z}_{t}\) w.r.t. objective function (6) given the reference path measure \(\bar{\pi}\) driven by fixed backward policy \(\widehat{\mathbf{z}}_{t}\) under one subset of \(\mathcal{K}_{\text{boundary}}\).

**Proposition 4.2** (Optimality w.r.t. \(\mathcal{K}_{\text{boundary}}\)).: _Given the reference path measure \(\bar{\pi}\) driven by the backward policy \(\widehat{\mathbf{z}}_{t}\) from boundary \(\mu_{t_{i+1}}\) in the reverse time direction, the optimal path measure in the forward time direction of the following problem_

\[\min_{\pi}\mathcal{J}(\pi):=\sum_{i=0}^{N-1}KL\left(\pi_{t_{i}:t_{i+1}}|\bar{ \pi}_{t_{i}:t_{i+1}}\right),\quad s.t\quad\pi\in\left\{\int\pi_{t_{i}:t_{i+1}} \mathbf{d}\mathbf{m}_{t_{i+1}}=\mu_{t_{i}},\int\mu_{t_{i}}\mathbf{d}\mathbf{v}_ {t_{i}}=\rho_{t_{i}}\right\}\]

Figure 2: The procedure details the Bregman Iteration (BI) employed in DMSB. The gray and blue blocks represent the BP step performed under \(\mathcal{K}_{\text{boundary}}\) constraint for forward and backward policies, respectively. The red block signifies the BP step executed under the \(\mathcal{K}_{\text{bridge}}\) constraint. Algorithms for training and sampling can be found in Appendix.D.

\[\text{is}:\qquad\pi^{*}_{t_{i}:t_{i+1}}=\frac{\rho_{t_{i}}\bar{\pi}_{t_{i}:t_{i+1} }}{\int\bar{\pi}_{t_{i}:t_{i+1}}\text{d}\mathbf{m}_{t_{i+1}}\text{d}\mathbf{v}_{ t_{i}}}.\]

_When \(\pi_{t_{i}:t_{i+1}}\equiv\pi^{*}_{t_{i}:t_{i+1}}\), the following equations need to hold \(\forall t\in[t_{i},t_{i+1}]\):_

\[\|\mathbf{z}_{t}+\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}} \log\hat{p}_{t}\|_{2}^{2}=0,\] (9a) \[p_{t_{i}}(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}})\equiv\hat{q}_{t _{i}}(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}}),\] (9b)

_where \(\hat{p}_{t}\) and \(\hat{q}_{t}\) denote the marginal density and conditional velocity distribution of the reference path measure at time \(t\), respectively._

Proof.: See appendix.B.5 

**Remark 4.3**.: When the ground truth distributions of velocity \(\gamma_{t_{i}}\) are available, one can simply sample from \(\gamma_{t_{i}}\) since the joint distribution \(\mu_{t}\) is available in this case. In order to matching the reference path measure in KL divergence sense, one needs to match both the intermediate path measure eq.9a and the boundary condition eq.9b. In the traditional two-boundary SB case, matching the boundary condition is often disregarded due to either having a predefined data distribution or a tractable prior. However, in our specific case, as the velocity is not predefined, it becomes imperative to address this issue and optimize it through the application of Langevin dynamics.

### Optimization in set \(\mathcal{K}_{\text{bridge}}\)

The formulation of optimization under \(\mathcal{K}_{\text{bridge}}\) is similar to the previous section but differs by the boundary condition (eq.10b):

**Proposition 4.4** (Optimality w.r.t. \(\mathcal{K}_{\text{bridge}}\)).: _Given the reference path measure \(\bar{\pi}\) driven by the backward policy \(\widehat{\mathbf{z}}_{t}\) from boundary \(\mu_{t_{N}}\) in the reverse time direction, the optimal path measure in the forward time direction of the following problem_

\[\min_{\bar{\pi}}\mathcal{J}(\pi):=\sum_{i=0}^{N-1}KL\left(\pi_{t_ {i}:t_{i+1}}|\bar{\pi}_{t_{i}:t_{i+1}}\right),\quad\text{s.t.}\quad\pi\in \mathcal{K}_{\text{bridge}}=\left\{\cap_{i=1}^{N-1}\mathcal{K}_{t_{i}}^{2}\right\}\] \[\text{is:}\quad\pi^{*}_{t_{0}:t_{N}}=\frac{q_{t_{0}}\bar{\pi}_{t_ {0}:t_{N}}}{\int\bar{\pi}_{t_{0}:t_{N}}\text{d}\mathbf{m}_{t_{N}}\text{d} \mathbf{v}_{t_{0}}}.\]

_when \(\pi_{t_{0}:t_{N}}\equiv\pi^{*}_{t_{0}:t_{N}}\), the following equations need to hold \(\forall t\in[t_{0},t_{N}]\):_

\[\|\mathbf{z}_{t}+\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}} \log\hat{p}_{t}\|_{2}^{2}=0\] (10a) \[p_{t_{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\equiv\hat{q}_{t _{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\] (10b)

Proof.: See appendix.B.6 

Conceptually, the above optimization objective with \(\mathcal{K}_{\text{bridge}}\) constraint aims at finding a _continuous_ path measure close to reference path measure \(\bar{\pi}\) while any intermediate marginals constraints will not be considered. The boundary condition of reference path measure in the next iteration \(p_{t_{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\) is determined by eq.10b. Fortunately, the empirical samples from this distribution are available, though the analytic representation of the distribution \(\hat{q}_{t_{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\) is unknown. Hence we can utilize these samples as empirical sources from boundary distribution \(\hat{q}_{t_{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\) for the next BP. For further explanation and intuition, one can find it in Appendix.G

### Parameterization and Training Objective Function

Inspired by the success of prior work [14], we parameterize path measure \(\pi\) by forward policy \(\mathbf{z}_{t}^{\theta}\) or backward policy \(\widehat{\mathbf{z}}_{t}^{\phi}\) combined with one of constraints in \(\mathcal{K}_{\text{boundary}}\) or \(\mathcal{K}_{\text{bridge}}\) (see Fig.8 in Appendix for visualization). We adopt Euler-Maruyama discretization and denote the timestep as \(\delta_{t}\). Notably, eq.9b and eq.10b can be implied by minimizing phase space NLL in Prop.3.1. This leads to the following objective function, termed as phase space mean matching objective, which will be used to train neural networks that represent \(\mathbf{z}_{t}^{\theta}\) and \(\widehat{\mathbf{z}}_{t}^{\phi}\) after time discretization:

\[\mathcal{L}_{MM}=\mathbb{E}\left[\left|\left|\delta_{t}\mathbf{z}_{t}^{\theta} (\mathbf{m}_{t+\delta_{t}})+\delta_{t}\widehat{\mathbf{z}}_{t+\delta_{t}}^{ \phi}(\mathbf{m}_{t+\delta_{t}})-\left(\mathbf{m}_{t}+\delta_{t}\mathbf{z}_{ t}^{\theta}-\mathbf{m}_{t+\delta_{t}}\right)\right|\right|^{2}\right].\]

The velocity boundary condition for the reference path measure in the succeeding BP is encoded in eq.9b or eq.10b, but the representation of conditional distribution eq.9b is not clear. We leverage the favorable property of SB to parameterize and sample from such distribution.

**Proposition 4.5** ([27; 28]).: _If \(\pi^{\theta}\) and \(\bar{\pi}^{\phi}\) shares same path measure, then_

\[\tilde{p}_{t_{i}}^{\theta,\phi}(\mathbf{v}_{t_{i}},\mathbf{x}_{t_{i}})\equiv q_{ t_{i}}^{\phi}(\mathbf{v}_{t_{i}},\mathbf{x}_{t_{i}})\propto q_{t_{i}}^{\phi}( \mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}}),\quad\text{where:}\quad\nabla_{\mathbf{ v}}\log\tilde{p}_{t}^{\theta,\phi}=\left(\mathbf{z}_{t}^{\theta}+\widehat{\mathbf{z}}_{t}^{ \phi}\right)/g.\] (11)

Prop.4.5 suggests that one can use \(p_{t_{i}}(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}}):=\tilde{p}_{t_{i}}^{\theta,\phi}\) to imply condition (9b) and obtain samples from such distribution by simulating Langevin dynamics. Namely, we first sample position from ground truth \(\mathbf{x}_{t_{i}}\sim\rho_{t_{i}}\), and then sample \(\mathbf{v}_{t_{i}}\sim\tilde{p}_{t}^{\theta,\phi}\) using eq.11. One can further adopt the same regularization [29] to enforce the condition of Prop.4.5.

### Training Scheme

Here we introduce the scheme to traverse BI (see Fig.2). In one BI, all constraints must be iterated once. For the sake of \(\mathcal{L}_{\text{MM}}\), the reference path measure should be induced by opposite direction. A single BI cannot be recursively repeated due to the conflict of reference path measure direction. For example (see Fig.2), at the end of \(d\)-th BI, \(\bar{\pi}\) is yielded by forward policy while the first BP of \(d\)-th BI is also optimizing forward policy which violates \(\mathcal{L}_{\text{MM}}\). Instead, we reschedule the optimization order. Specifically, in \((d+1)\)-th BI, we optimize backward policy at the first BP and the last BP.

## 5 Experiments

**Setups:** We test DMSB on 2D synthetic datasets and real-world scRNA-seq dataset [30]. We choose state of the art algorithms MIOFlow [9] and NLSB [11] as our baselines. We tune both models to the best of our hardware capacity. We choose Sliced-Wasserstein Distance (SWD)[31] and Maximum Mean Discrepancy (MMD)[32] together with visualization as our criterion. The detailed setup of training and evaluation can be found in Appendix.C.

**Synthetic Datasets:** The Petal [9] and Gaussian Mixture Model (GMM) dataset are simple yet challenging, as they mimic natural dynamics arising in cellular differentiation, including bifurcations and merges. We compare our algorithm with MIOFlow in Fig.3. DMSB can infer trajectories aligned with ground truth distribution more faithfully at timesteps when snapshots are taken. In GMM experiments (see Fig.4), we choose standard Gaussian at initial and terminal time steps while four-modal GMM and eight-modal GMM are placed at intermediate time steps. Besides good position trajectory, it is almost serendipity that DMSB can also learn the reasonable velocity trajectory _without_ any access to ground truth velocity information. This paves the way for our later velocity estimation for the RNAsc dataset. **scRNA-seq Dataset:** The emergence of single-cell profiling technologies has facilitated the acquisition of high-resolution single-cell data, enabling the characterization of individual cells at distinct developmental states [7]. However, because the cell population is eliminated after the measurement, one may only gather statistical data for single samples at particular timesteps, which neither preserves any correlations over time nor provides access to the ground truth trajectory. The diversity of embryonic stem cells after development from embryoid bodies, which comprises mesoderm, endoderm,

Figure 4: Validation of our DMSB model on complex GMM synthetic dataset. The velocity and position of the same sample correspond to the same shade level. _Upper_: Samples’ evolution in the position space. _Bottom_: Learnt samples’ evolution in the velocity space.

Figure 3: Comparsion with MIOFlow and ground truth on challenging petal dataset. DMSB is able to generate trajectories whose time marginal matches ground truth faithfully and outperforms prior work. Time is indicated by colors.

neuroectoderm, and neural crest in 27 days, is demonstrated by the scRNA-seq dataset. The snapshot of cells are collected between (\(t_{0}\): day 0 to 3, \(t_{1}\): day 6 to 9, \(t_{2}\): day 12 to 15, \(t_{3}\): day 18 to 21,\(t_{4}\): day 24 to 27). Snapshot data are prepossessed by the quality control [30] and then projected to feature space by principal component analysis (PCA). We inherit processed data from [8]. We validate DMSB on 5-dim and 100-dim PCA space to show superior performance on high-dimension problems compared with baselines. We further show that DMSB can estimate better velocity distribution compared with baselines when the ground truth is absent during training and testing.

We testify the performance of our model by computing MMD and SWD with full snapshots and when one of snapshots is left out (LO). We postpone the comparison of all the models on 5-d RNA space to the appendix (see Fig.9 and Table.6) because the problem is relatively simple and all models can infer accurate trajectory. Table.3 summarizes the average MMD and SWD between estimated marginal and ground truth over different snapshot timesteps. DMSB outperforms prior work by a large margin in high (100) dimensional scenarios. The visualization (Fig.5) in PCA space further justifies the numerical result and highlights the variety and quality of the samples produced by DMSB.

Interestingly, Fig.4 demonstrates that DMSB can reconstruct reasonable evolution of the velocity distribution which was not accessible to the algorithm. We further validate such property in 100-D RNAsc dataset. During the training and testing, all the models do not have access to the ground truth velocity. We run the experiments of 100-D and 5-D RNAsc datasets and average the discrepancy between ground truth velocity and estimated velocity over snapshot time. The numerical values are listed in the Table.7 and Table.6. The plot of velocity and position can be found in Fig.9 and Fig.10. The plot illustrates that while all models are capable of learning reasonable trajectories, only DMSB has the ability to estimate a plausible velocity distribution. This property holds even for

Figure 5: Comparison of population-level dynamics on 100-dimensional PCA space at the moment of observation for scRNA-seq data using MIOFlow, NLSB, and DMSB. We display the plot of the first 6 principle components (PC). Baselines can only learn the trajectory’s fundamental trend, whereas DMSB can match the target marginal along the trajectory across different dimensions. The right figure shows Kernel Density Estimation [33] of samples generated by DMSB and ground truth at \(t_{3}\) and \(t_{4}\). The generated samples for all timesteps and comparison with baseline are in Appendix.F.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline  & & \multicolumn{3}{c}{MMD \(\downarrow\)} & \multicolumn{3}{c}{SWD \(\downarrow\)} \\ \cline{2-9} Algorithm & w/o LO & LO-\(t_{1}\) & LO-\(t_{2}\) & LO-\(t_{3}\) & w/o LO & LO-\(t_{1}\) & LO-\(t_{2}\) & LO-\(t_{3}\) \\ \hline NLSB[10] & 0.66 & 0.38 & 0.37 & 0.37 & 0.54 & 0.55 & 0.54 & 0.55 \\ MIOFlow[8] & 0.23 & 0.23 & 0.90 & 0.23 & 0.35 & 0.49 & 0.72 & 0.50 \\ DMSB(ours) & **0.03** & **0.04** & **0.04** & **0.20** & **0.20** & **0.19** & **0.18** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Numerical result of MMD and SWD on 100 dimensions single-cell RNA-seq dataset and results for leaving out (LO) marginals at different observation. DMSB outperforms prior work by a large margin for both metrics and all leave-out case. See Appendix.4 for Results over 3 seeds.

100-D RNA dataset (see Fig.5,11,12). This is notable, despite the velocity estimated by DMSB does not perfectly match the ground truth, because it should be noted that the proposed phase space SDE and the optimality of OT are artificial and may not necessarily represent the actual RNA evolution. Moreover, as individual evolutions cannot be tracked, possibilities such as {A\(\rightarrow\)A, B\(\rightarrow\)B} versus {A\(\rightarrow\)B, B\(\rightarrow\)A} can not be discerned, which renders exact velocity recovering almost impossible.

## 6 Conclusion and Limitations

In this paper, we propose DMSB, a scalable algorithm that learns the trajectory which fits the different marginal distributions over time. We extend the mean matching objective to phase space which enables efficient mSB computing. We propose a novel training scheme to fit the mean matching objective without violating BI which is the root of solving mmmSB problem. We demonstrate the superior result of DMSB compared with the existing algorithms.

A main limitation of this work is, the rate of convergence to the actual mmmSB has not been quantified after neural network approximations are introduced. Even though [15] theoretically analyzed the convergence of mean matching iteration, supporting its outstanding performance [14], the iteration still fails to converge to the actual SB [34] precisely due to practical neural network estimation errors accumulating over BI. However, recent work [35] shows the convergence of SB when training error exists. In addition, DMSB cannot simulate the process with death and birth of cells which can be potentially described as unbalanced optimal transport [36].

## 7 Acknowledgement

This research was supported by the ARO Award W911NF2010151, and the DoD Basic Research Office AwardHQ00342110002.

## References

* [1] Yongxin Chen, Giovanni Conforti, Tryphon T Georgiou, and Luigia Ripani. Multi-marginal schrodinger bridges. In _International Conference on Geometric Science of Information_, pages 725-732. Springer, 2019.
* [2] Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. _Cell_, 176 (4):928-943, 2019.
* [3] Karren D Yang and Caroline Uhler. Scalable unbalanced optimal transport using generative adversarial networks. _arXiv preprint arXiv:1810.11447_, 2018.
* [4] Mike Fisher, Jorge Nocedal, Yannick Tremolet, and Stephen J Wright. Data assimilation in weather forecasting: a case study in pde-constrained optimization. _Optimization and Engineering_, 10(3):409-426, 2009.
* [5] Kenneth G Manton, XiLiang Gu, and Gene R Lowrimore. Cohort changes in active life expectancy in the us elderly population: Experience from the 1982-2004 national long-term care survey. _The Journals of Gerontology Series B: Psychological Sciences and Social Sciences_, 63(5):S269-S281, 2008.
* [6] Tatsunori Hashimoto, David Gifford, and Tommi Jaakkola. Learning population-level diffusions with generative rnns. In _International Conference on Machine Learning_, pages 2417-2426. PMLR, 2016.
* [7] Charlotte Bunne, Laetitia Papaxanthos, Andreas Krause, and Marco Cuturi. Proximal optimal transport modeling of population dynamics. In _International Conference on Artificial Intelligence and Statistics_, pages 6511-6528. PMLR, 2022.
* [8] Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics. In _International conference on machine learning_, pages 9526-9536. PMLR, 2020.
* [9] Guillaume Huguet, Daniel Sumner Magruder, Oluwadamilola Fasina, Alexander Tong, Manik Kuchroo, Guy Wolf, and Smita Krishnaswamy. Manifold interpolating optimal-transport flows for trajectory inference. _arXiv preprint arXiv:2206.14928_, 2022.
* [10] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In _Advances in Neural Information Processing Systems_, pages 6572-6583, 2018.
* [11] Takeshi Koshizuka and Issei Sato. Neural lagrangian schr\(\backslash\)" _odinger bridge. _arXiv preprint arXiv:2204.04853_, 2022.
* [12] Lenaic Chizat, Stephen Zhang, Matthieu Heitz, and Geoffrey Schiebinger. Trajectory inference via mean-field langevin in path space. _arXiv preprint arXiv:2205.07146_, 2022.
* [13] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. _arXiv preprint arXiv:2112.07068_, 2021.
* [14] Tianrong Chen*, Guan-Horng Liu*, and Evangelos A Theodorou. Likelihood training of schrodinger bridge using forward-backward sdes theory. _arXiv preprint arXiv:2110.11291_, 2021.
* [15] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _arXiv preprint arXiv:2106.01357_, 2021.
* [16] Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Conditional simulation using diffusion schrodinger bridges. In _Uncertainty in Artificial Intelligence_, pages 1792-1802. PMLR, 2022.

* [17] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I 2 sb: Image-to-image schr\(\backslash\)" odinger bridge. _arXiv preprint arXiv:2302.05872_, 2023.
* [18] Guan-Horng Liu, Tianrong Chen, Oswin So, and Evangelos A Theodorou. Deep generalized schr\(\backslash\)" odinger bridge. _arXiv preprint arXiv:2209.09893_, 2022.
* [19] James Thornton, Michael Hutchinson, Emile Mathieu, Valentin De Bortoli, Yee Whye Teh, and Arnaud Doucet. Riemannian diffusion schr\(\backslash\)" odinger bridge. _arXiv preprint arXiv:2207.03024_, 2022.
* [20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [21] Lev M Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. _USSR computational mathematics and mathematical physics_, 7(3):200-217, 1967.
* [22] Michele Pavon and Anton Wakolbinger. On free energy, stochastic control, and schrodinger processes. In _Modeling, Estimation and Control of Systems with Uncertainty_, pages 334-348. Springer, 1991.
* [23] Francisco Vargas. Machine-learning approaches for the empirical schrodinger bridge problem. Technical report, University of Cambridge, Computer Laboratory, 2021.
* [24] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre. Iterative bregman projections for regularized transportation problems. _SIAM Journal on Scientific Computing_, 37(2):A1111-A1138, 2015.
* [25] Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. _Applied mathematics and Optimization_, 23(1):313-329, 1991.
* [26] Jean-David Benamou, Thomas O Gallouet, and Francois-Xavier Vialard. Second-order models for optimal transport and cubic splines on the wasserstein space. _Foundations of Computational Mathematics_, 19(5):1113-1143, 2019.
* [27] Brian DO Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* [28] Edward Nelson. _Dynamical theories of Brownian motion_, volume 106. Princeton university press, 2020.
* [29] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7921-7931, 2021.
* [30] Kevin R Moon, David van Dijk, Zheng Wang, Scott Gigante, Daniel B Burkhardt, William S Chen, Kristina Yim, Antonia van den Elzen, Matthew J Hirn, Ronald R Coifman, et al. Visualizing structure and transitions in high-dimensional biological data. _Nature biotechnology_, 37(12):1482-1492, 2019.
* [31] Nicolas Bonneel, Julien Rabin, Gabriel Peyre, and Hanspeter Pfister. Sliced and radon wasserstein barycenters of measures. _Journal of Mathematical Imaging and Vision_, 51:22-45, 2015.
* [32] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [33] Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. _The annals of mathematical statistics_, pages 832-837, 1956.
* [34] David Lopes Fernandes, Francisco Vargas, Carl Henrik Ek, and Neill DF Campbell. Shooting schrodinger's cat. In _Fourth Symposium on Advances in Approximate Bayesian Inference_, 2021.

* [35] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, Anderson Schneider, and Yuriy Nevmyvaka. Provably convergent schr\(\backslash\)" odinger bridge with applications to probabilistic time series imputation. _arXiv preprint arXiv:2305.07247_, 2023.
* [36] Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. The most likely evolution of diffusing and vanishing particles: Schrodinger bridges with unbalanced marginals. _SIAM Journal on Control and Optimization_, 60(4):2016-2039, 2022.
* [37] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _arXiv e-prints_, pages arXiv-2101, 2021.
* [38] Jiongmin Yong and Xun Yu Zhou. _Stochastic controls: Hamiltonian systems and HJB equations_, volume 43. Springer Science & Business Media, 1999.
* [39] Kenneth Caluya and Abhishek Halder. Wasserstein proximal algorithms for the schrodinger bridge problem: Density control with nonlinear drift. _IEEE Transactions on Automatic Control_, 2021.
* [40] Ioannis Exarchos and Evangelos A Theodorou. Stochastic optimal control via forward and backward stochastic differential equations and importance sampling. _Automatica_, 87:159-165, 2018.
* [41] Tianrong Chen, Ziyi O Wang, Ioannis Exarchos, and Evangelos Theodorou. Large-scale multi-agent deep fbsdes. In _International Conference on Machine Learning_, pages 1740-1748. PMLR, 2021.
* [42] Tianrong Chen, Ziyi Wang, and Evangelos A Theodorou. Deep graphic fbsdes for opinion dynamics stochastic control. In _2022 IEEE 61st Conference on Decision and Control (CDC)_, pages 4652-4659. IEEE, 2022.
* [43] Hopf Eberhard. The partial differential equation ut+ uux= \(\mu\)xx. _Communications on Pure and Applied Mathematics_, 3(3):201-230, 1950.
* [44] Julian D Cole. On a quasi-linear parabolic equation occurring in aerodynamics. _Quarterly of applied mathematics_, 9(3):225-236, 1951.
* [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [46] Francisco Vargas, Pierre Thodoroff, Neil D Lawrence, and Austen Lamacraft. Solving schrodinger bridges via maximum likelihood. _arXiv preprint arXiv:2106.02081_, 2021.

Appendix

## Appendix B Proof in SS3 and SS4

Before stating our proofs, we provide the assumptions used throughout the paper. These assumptions are adopted from stochastic analysis for SGM [27, 37, 38], SB [39], and FBSDE [40, 41, 42].

* \(\mu_{t_{i}}\) with finite second-order moment for all \(t_{i}\).
* \(\bm{f}\) and \(g\) are continuous functions, and \(|g(t)|^{2}>0\) is uniformly lower-bounded w.r.t. \(t\).
* \(\forall t\in[0,T]\), we have \(\nabla_{\mathbf{v}}\log p_{t}(\mathbf{m}_{t},t),\nabla_{\mathbf{v}}\log\Psi( \cdot,\cdot,\cdot),\nabla_{\mathbf{v}}\log\widehat{\Psi}(\cdot,\cdot,\cdot), \mathbf{Z}(\cdot,\cdot,\cdot;\theta)\), and \(\widehat{\mathbf{Z}}(\cdot,\cdot,\cdot;\phi)\) Lipschitz and at most linear growth w.r.t. \(\mathbf{x}\) and \(\mathbf{v}\).
* \(\Psi,\widehat{\Psi}\in C^{1,2}\).
* \(\exists k>0:p_{t}^{SB}(\mathbf{m})=\mathcal{O}(\exp^{-\|\mathbf{m}\|_{k}^{2}})\) as \(\mathbf{m}\to\infty\).

Assumptions (i) (ii) (iii) are standard conditions in stochastic analysis to ensure the existence-uniqueness of the SDEs; hence also appear in SGM analysis [37]. Assumption (iv) allows applications of Ito formula and properly defines the backward SDE in FBSDE theory. Finally, assumption (v) assures the exponential limiting behavior when performing integration by parts. w.o.l.g, we denote \(\bm{f}=[\mathbf{v},\mathbf{0}]^{\mathsf{T}}\).

### Proof of Proposition.3.1

The results of the Prop.3.1 is part of results of Prop.B.4 which gives the results for both forward and backward likelihood objective.

**Theorem B.1**.: _The optimization problem_

\[\min\int_{0}^{1}\int\frac{1}{2}\|\mathbf{a}\|_{2}^{2}\mu\mathrm{ d}\mathbf{x}\mathrm{d}\mathbf{v}\mathrm{d}t,\] (12) \[s.t\quad\begin{cases}\frac{\partial\mu(\mathbf{m}_{t})}{\partial t }=-\nabla_{\mathbf{m}}\cdot\left\{\left[\left(\bm{f}+g\mathbf{u}\right)\right] \mu\right\}+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\mu,\\ \mu_{0}=p(0,\mathbf{x},\mathbf{v}),\quad\mu_{1}=p(T,\mathbf{x},\mathbf{v}), \end{cases}\] (13)

_will induce the coupled PDEs,_

\[\frac{\partial\mu(\mathbf{m}_{t})}{\partial t} =-\nabla_{\mathbf{m}}\cdot\left[\left(\bm{f}+g^{2}\nabla_{ \mathbf{v}}\phi\right)\mu\right]+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\mu,\] (14) \[\frac{\partial\phi(\mathbf{m}_{t})}{\partial t} =-\frac{1}{2}\|g\nabla_{\mathbf{v}}\phi\|_{2}^{2}-\mathbf{v}^{ \mathsf{T}}\nabla_{\mathbf{x}}\phi-\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\phi,\] (15)

_and the optimal control of the problem is_

\[\mathbf{a}^{*}=g\nabla_{\mathbf{v}}\phi.\]Proof.: One can write the Lagrange by introducing lagrangian multiplier \(\phi\):

\[\mathcal{L}(\mu,\mathbf{a},\phi) =\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\frac{1}{2} \|\mathbf{a}\|_{2}^{2}\mu\mathbf{d}\mathbf{x}\mathbf{d}\mathbf{v}\mathbf{d}t+ \int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\phi\frac{\partial\mu}{ \partial t}\mathbf{d}\mathbf{v}\mathbf{d}\mathbf{x}\mathbf{d}t\] \[+\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\phi\left\{ -\frac{1}{2}g^{2}\Delta_{\mathbf{m}}\mu+\nabla_{\mathbf{m}}\cdot\left[\left( \boldsymbol{f}+g\mathbf{u}\right)\mu\right]\right\}\mathbf{d}\mathbf{v}\mathbf{ d}\mathbf{x}\mathbf{d}t\] \[=\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\frac{1}{2} \|\mathbf{a}\|_{2}^{2}\mu\mathbf{d}\mathbf{x}\mathbf{d}\mathbf{v}\mathbf{d}t- \int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\mu\frac{\partial\phi}{ \partial t}\mathbf{v}\mathbf{d}\mathbf{x}\mathbf{d}t\] \[+\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\phi\nabla _{\mathbf{m}}\cdot\left[\left(\boldsymbol{f}+g\mathbf{u}\right)\mu\right]- \phi\left[\frac{1}{2}g^{2}\Delta_{\mathbf{m}}\mu\right]\mathbf{d}\mathbf{v} \mathbf{d}\mathbf{x}\mathbf{d}t\] \[=\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\frac{1}{2} \|\mathbf{a}\|_{2}^{2}\mu\mathbf{d}\mathbf{x}\mathbf{d}\mathbf{v}\mathbf{d}t- \int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\mu\frac{\partial\phi}{ \partial t}\mathbf{v}\mathbf{d}\mathbf{x}\mathbf{d}t\] \[+\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}-\nabla_{ \mathbf{m}}\phi^{\mathsf{T}}\left[\left(\boldsymbol{f}+g\mathbf{u}\right)\right] \mu-\mu\left[\frac{1}{2}g^{2}\Delta_{\mathbf{m}}\phi\right]\mathbf{d}\mathbf{ v}\mathbf{d}\mathbf{x}\mathbf{d}t\] \[=\int_{0}^{1}\int_{\mathbb{R}^{n}\times\mathbb{R}^{n}}\left\{ \frac{1}{2}\|\mathbf{a}\|_{2}^{2}-\frac{\partial\phi}{\partial t}-\mathbf{v}^ {\mathsf{T}}\nabla_{\mathbf{x}}\phi-\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\phi-g \nabla_{\mathbf{v}}\phi^{\mathsf{T}}\mathbf{a}\right\}\mu\;\;\mathbf{d} \mathbf{v}\mathbf{d}\mathbf{x}\mathbf{d}t\]

By taking the minimization within the bracket, The optimal control is,

\[\mathbf{a}^{*}=g\nabla_{\mathbf{v}}\phi\]

By Plugging it back, the optimality of the aforementioned problem is presented as:

\[\frac{\partial\mu(\mathbf{m}_{t})}{\partial t} =-\nabla_{\mathbf{v}}\cdot\left[\left(\boldsymbol{f}+g^{2}\nabla_ {\mathbf{v}}\phi\right)\mu\right]+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\mu,\] \[\frac{\partial\phi(\mathbf{m}_{t})}{\partial t} =-\frac{1}{2}\|g\nabla_{\mathbf{v}}\phi\|_{2}^{2}-\mathbf{v}^{ \mathsf{T}}\nabla_{\mathbf{x}}\phi-\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\phi,\]

**Theorem B.2**.: _The optimal forward and backward processes are represented as:_

\[\mathbf{d}\mathbf{m}_{t} =\left[\boldsymbol{f}+g\mathbf{u}_{t}^{f*}\right]\mathbf{d}t+g(t) \mathbf{d}\mathbf{w}_{t}\quad\text{(forward)}\] (16) \[\mathbf{d}\mathbf{m}_{s} =\left[\boldsymbol{f}+g\mathbf{u}_{s}^{b*}\right]\mathbf{d}t+g(t) \mathbf{d}\mathbf{w}_{s}\quad\text{(Backward)}\] (17)

_in which \(\boldsymbol{f}=[\mathbf{v},\mathbf{0}]^{\mathsf{T}}\). Optimal control is expressed as,_

\[\mathbf{u}_{t}^{f*} :=\mathbf{Z}_{t}\equiv\begin{pmatrix}\mathbf{0}\\ \mathbf{z}_{t}\end{pmatrix}\equiv\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\log\Psi_{t}\end{pmatrix}\] (18) \[\mathbf{u}_{t}^{b*} :=\widehat{\mathbf{Z}}_{t}\equiv\begin{pmatrix}\mathbf{0}\\ \mathbf{z}_{t}\end{pmatrix}\equiv\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\log\widehat{\Psi}_{t}\end{pmatrix}\] (19)

_where \(\Psi\) and \(\widehat{\Psi}\) are the solution of following PDEs,_

\[\boxed{\frac{\partial\Psi_{t}}{\partial t}=-\frac{1}{2}g^{2}\Delta_{ \mathbf{v}}\Psi_{t}-\nabla_{\mathbf{x}}\Psi_{t}^{\mathsf{T}}\mathbf{v}}\] (20) \[\frac{\partial\widehat{\Psi}_{t}}{\partial t} =\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\widehat{\Psi}_{t}-\nabla_{ \mathbf{x}}\widehat{\Psi}_{t}^{\mathsf{T}}\mathbf{v}\] _s.t_ \[\Psi(\mathbf{x},\mathbf{v},0)\widehat{\Psi}(\mathbf{x},\mathbf{v},0)=p(\mathbf{x},\mathbf{v},0),\quad\Psi(\mathbf{x},\mathbf{v},T)\widehat{\Psi }(\mathbf{x},\mathbf{v},T)=p(\mathbf{x},\mathbf{v},T)\]

Proof.: By Lemma.B.1, we notice that the optimal control is:

\[\mathbf{a}^{*}=g\nabla_{\mathbf{v}}\phi.\]

By leveraging Hopf-Cole [43, 44] transformation, here we define

\[\Psi =\exp\left(\phi\right),\] \[\widehat{\Psi} =\mu\exp\left(-\phi\right).\]Then we can have the following expressions:

\[\nabla\Psi =\exp(\phi)\nabla\phi\] \[\Delta\Psi =\nabla\cdot\left(\nabla\Psi\right)\] \[=\sum_{i}\frac{\partial}{\partial\mathbf{m}_{i}}\left[\exp\left( \phi\right)\nabla\phi\right]\] \[=\left[\nabla\phi^{\mathsf{T}}\left(\exp\left(\phi\right)\frac{ \partial\phi_{i}}{\partial\mathbf{m}_{i}}\right)+\exp\left(\phi\right)\frac{ \partial(\nabla\phi)_{i}}{\partial\mathbf{m}_{i}}\right]\] \[=\exp\left(\phi\right)\left[\|\nabla\phi\|_{2}^{2}+\Delta\phi\right]\] \[\nabla\widehat{\Psi} =\mu\exp\left(-\phi\right)\left(-\nabla\phi\right)+\exp(-\phi)\nabla\mu\] \[=\exp(-\phi)(-\mu\nabla\phi+\nabla\mu)\] \[\Delta\widehat{\Psi} =\nabla\cdot\left(\nabla\widehat{\Psi}\right)\] \[=\sum_{i}\frac{\partial}{\partial\mathbf{m}_{i}}\left[\exp\left( -\phi\right)\left(-\mu\nabla\phi+\nabla\mu\right)\right]\] \[=\sum_{i}\left[\left(-\mu\nabla\phi+\nabla\mu\right)^{\mathsf{T} }\left(\exp\left(-\phi\right)\frac{-\partial[\nabla\phi]_{i}}{\partial\mathbf{ m}_{i}}\right)\right.\] \[\left.+\exp\left(-\phi\right)\left(\frac{\partial}{\partial\mathbf{ m}_{i}}[\nabla\mu]_{i}-\mu\frac{\partial}{\partial\mathbf{m}_{i}}[\nabla\phi]_{i}- \nabla\phi^{\mathsf{T}}\frac{\partial}{\partial\mathbf{m}_{i}}[\nabla\mu]_{i} \right)\right]\] \[=\exp\left(-\phi\right)\left[\mu\|\nabla\phi\|_{2}^{2}-\nabla\mu^ {\mathsf{T}}\nabla\phi+\Delta\mu-\mu\Delta\phi-\nabla\phi^{\mathsf{T}}\nabla\mu\right]\] \[=\exp\left(-\phi\right)\left[\mu\|\nabla\phi\|_{2}^{2}-2\nabla\mu ^{\mathsf{T}}\nabla\phi+\Delta\mu-\mu\Delta\phi\right]\]

Thus, we can have.

\[\frac{\partial\Psi}{\partial t} =\exp\left(\phi\right)\frac{\partial\phi}{\partial t}\] \[=\exp\left(\phi\right)\left(-\frac{1}{2}\|g\nabla_{\mathbf{v}} \phi\|_{2}^{2}-\mathbf{v}^{\mathsf{T}}\nabla_{\mathbf{x}}\phi-\frac{1}{2}g^{2} \Delta_{\mathbf{v}}\phi\right)\] \[=-\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\Psi-\nabla_{\mathbf{x}} \Psi^{\mathsf{T}}\mathbf{v}\] \[\frac{\partial\widehat{\Psi}}{\partial t} =\exp\left(-\phi\right)\frac{\partial\mu}{\partial t}-\mu\exp\left( -\phi\right)\frac{\partial\phi}{\partial t}\] \[=\exp\left(-\phi\right)\left(\frac{\partial\mu}{\partial t}-\mu \frac{\partial\phi}{\partial t}\right)\] \[=\exp\left(-\phi\right)\left[-\nabla_{\mathbf{m}}\cdot\left\{\left[ \left(\boldsymbol{f}+g\mathbf{u}\right)\mathbf{I}_{d}\right]\mu\right\}+\frac{1 }{2}g^{2}\Delta_{\mathbf{m}}\mu+\mu\left(\frac{1}{2}\|g\nabla_{\mathbf{v}} \phi\|_{2}^{2}+\mathbf{v}^{\mathsf{T}}\nabla_{\mathbf{x}}\phi+\frac{1}{2}g^{2 }\Delta_{\mathbf{v}}\phi\right)\right]\] \[=\exp\left(-\phi\right)\left[-\nabla_{\mathbf{v}}\cdot\left(g^{2 }\mu\nabla_{\mathbf{v}}\phi\right)-\mathbf{v}^{\mathsf{T}}\nabla_{\mathbf{x}} \mu+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\mu+\frac{\mu}{2}\|g\nabla_{\mathbf{v}} \phi\|_{2}^{2}+\mu\mathbf{v}^{\mathsf{T}}\nabla_{\mathbf{x}}\phi+\mu\frac{1}{2 }g^{2}\Delta_{\mathbf{v}}\phi\right]\] \[=\exp\left(-\phi\right)\left[-g^{2}\nabla_{\mathbf{v}}\mu^{ \mathsf{T}}\nabla_{\mathbf{v}}\phi-g^{2}\mu\Delta_{\mathbf{v}}\phi-\mathbf{v}^ {\mathsf{T}}\nabla_{\mathbf{x}}\mu+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\mu+\frac {\mu}{2}\|g\nabla_{\mathbf{v}}\phi\|_{2}^{2}+\mu\mathbf{v}^{\mathsf{T}}\nabla_{ \mathbf{x}}\phi+\mu\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\phi\right]\] \[=\exp\left(-\phi\right)\left[-g^{2}\nabla_{\mathbf{v}}\mu^{ \mathsf{T}}\nabla_{\mathbf{v}}\phi-\mu\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\phi- \mathbf{v}^{\mathsf{T}}\nabla_{\mathbf{x}}\mu+\frac{1}{2}g^{2}\Delta_{\mathbf{v }}\mu+\frac{\mu}{2}\|g\nabla_{\mathbf{v}}\phi\|_{2}^{2}+\mu\mathbf{v}^{\mathsf{ T}}\nabla_{\mathbf{x}}\phi\right]\] \[=\exp\left(-\phi\right)\left[\frac{\mu}{2}\|g\nabla_{\mathbf{v}} \phi\|_{2}^{2}-g^{2}\nabla_{\mathbf{v}}\mu^{\mathsf{T}}\nabla_{\mathbf{v}}\phi- \mu\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\phi+\frac{1}{2}g^{2}\Delta_{\mathbf{v }}\mu-\mathbf{v}^{\mathsf{T}}\nabla_{\mathbf{x}}\mu+\mu\mathbf{v}^{\mathsf{T}} \nabla_{\mathbf{x}}\phi\right]\] \[=\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\hat{\Psi}-\nabla_{\mathbf{x}} \widehat{\Psi}^{\mathsf{T}}\mathbf{v}\]Then we can represent the optimal control as:

\[\mathbf{u}_{t}^{f*} :=\mathbf{Z}_{t}\equiv\begin{pmatrix}\mathbf{0}\\ \mathbf{z}_{t}\end{pmatrix}\] (21) \[\equiv\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\phi\end{pmatrix}\overset{\text{Hopf-Cole}}{=}\begin{pmatrix} \mathbf{0}\\ g\nabla_{\mathbf{v}}\log\Psi_{t}\end{pmatrix}\] (22)

Then the solution of such mSB is characterized by the forward SDE:

\[\mathbf{dm}_{t}=\left[\bm{f}+g\mathbf{u}_{t}^{f*}\right]\mathbf{d}t+g(t) \mathbf{d}\mathbf{w}_{t},\] (23)

Due to the structure of Hopf-Cole transform, one can have

\[p_{t}^{SB}=p_{t}^{eq.\eqref{eq:mSB}}=\Psi_{t}\widehat{\Psi}_{t}\] (24)

According to [27, 28], the reverse drift of such SDE (eq.23) \(\mathbf{u}_{t}^{b*}\) should admits,

\[\mathbf{u}_{t}^{f*}+\mathbf{u}_{t}^{b*} =\mathbf{g}\nabla_{\mathbf{v}}\log p_{t}^{SB}\] (25) \[\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\log\Psi_{t}\end{pmatrix}+\mathbf{u}_{t}^{b*} =\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\log\Psi_{t}+g\nabla_{\mathbf{v}}\log\widehat{\Psi}_{t} \end{pmatrix}\] (26) \[\mathbf{u}_{t}^{b*} =\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\log\widehat{\Psi}_{t}\end{pmatrix}\] (27)

which yields The backward optimal control

\[\mathbf{u}_{t}^{b*}:=\widehat{\mathbf{Z}}_{t}\equiv\begin{pmatrix}\mathbf{0} \\ \widehat{\mathbf{z}}_{t}\end{pmatrix}\equiv\begin{pmatrix}\mathbf{0}\\ g\nabla_{\mathbf{v}}\log\widehat{\Psi}_{t}\end{pmatrix}\] (28)

Thus, the optimal forward and backward process is

\[\mathbf{dm}_{t} =\left[\bm{f}+g\mathbf{u}_{t}^{f*}\right]\mathbf{d}t+g(t) \mathbf{d}\mathbf{w}_{t}\] (29) \[\mathbf{dm}_{s} =\left[\bm{f}+g\mathbf{u}_{s}^{b*}\right]\mathbf{d}t+g(t) \mathbf{d}\widehat{\mathbf{w}}_{s}\] (30)

And \(\Psi\) and \(\widehat{\Psi}\) satisfy following PDEs,

\[\frac{\partial\Psi_{t}}{\partial t} =-\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\Psi_{t}-\nabla_{\mathbf{x} }\Psi_{t}^{\mathsf{T}}\mathbf{v}\] \[\frac{\partial\widehat{\Psi}_{t}}{\partial t} =\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\widehat{\Psi}_{t}-\nabla_{ \mathbf{x}}\widehat{\Psi}_{t}^{\mathsf{T}}\mathbf{v}\]

**Lemma B.3**.: _By specifying \(\bm{f}=[\mathbf{v},\mathbf{0}]^{\mathsf{T}}\), The PDE shown in 20 can be represented by following SDEs_

\[\boxed{\begin{pmatrix}\mathbf{d}\mathbf{x}\\ \mathbf{d}\mathbf{v}\end{pmatrix}=\begin{pmatrix}\mathbf{v}\\ -g^{2}\nabla_{\mathbf{v}}\log\Psi\end{pmatrix}\mathbf{d}t+\begin{pmatrix} \mathbf{0}&\mathbf{0}\\ \mathbf{0}&g\end{pmatrix}\mathbf{d}\mathbf{w}}\] (31) \[\mathbf{d}\mathbf{y} =\frac{1}{2}\|\mathbf{z}\|^{2}\mathbf{d}t+\mathbf{z}^{\mathsf{T} }\mathbf{d}\mathbf{w}_{t}\] (32) \[\mathbf{d}\widehat{\mathbf{y}} =\left[\frac{1}{2}\|\widehat{\mathbf{z}}\|^{2}+\mathbf{z}^{ \mathsf{T}}\widehat{\mathbf{z}}+\nabla_{\mathbf{v}}\cdot g\widehat{\mathbf{z }}\right]\mathbf{d}t+\widehat{\mathbf{z}}^{\mathsf{T}}\mathbf{d}\mathbf{w}_{t}\] (33) \[\textbf{s.t}:\exp\left(\mathbf{y}_{0}+\widehat{\mathbf{y}}_{0} \right)=p(\mathbf{x},\mathbf{v},0),\quad\exp\left(\mathbf{y}_{T}+\widehat{ \mathbf{y}}_{T}\right)=p(\mathbf{x},\mathbf{v},T)\] (34)

_Where:_

\[\mathbf{y}\equiv\mathbf{y}(\mathbf{x}_{t},\mathbf{v},t)=\log \Psi(\mathbf{x}_{t},\mathbf{v}_{t},t),\quad\mathbf{z}\equiv\mathbf{z}( \mathbf{x}_{t},\mathbf{v}_{t},t)=g\nabla_{\mathbf{v}}\log\Psi(\mathbf{x}_{t}, \mathbf{v}_{t},t)\] \[\widehat{\mathbf{y}}\equiv\widehat{\mathbf{y}}(\mathbf{x}_{t}, \mathbf{v}_{t},t)=\log\widehat{\Psi}(\mathbf{x}_{t},\mathbf{v}_{t},t),\quad \widehat{\mathbf{z}}\equiv\widehat{\mathbf{z}}(\mathbf{x}_{t},\mathbf{v}_{t},t )=g\nabla_{\mathbf{v}}\log\widehat{\Psi}(\mathbf{x}_{t},\mathbf{v}_{t},t)\]Proof.: One can write

\[\frac{\partial\log\Psi}{\partial t} =\frac{1}{\Psi}\left(-\nabla_{\mathbf{x}}\Psi^{\mathsf{T}}\mathbf{v} -\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\Psi\right)\] \[=-\nabla_{\mathbf{x}}\log\Psi^{\mathsf{T}}\mathbf{v}-\frac{1}{2}g ^{2}\frac{\Delta_{\mathbf{v}}\Psi}{\Psi}\] \[=-\nabla_{\mathbf{x}}\log\Psi^{\mathsf{T}}\mathbf{v}-\frac{1}{2}g ^{2}\operatorname{Tr}\left[\frac{1}{\Psi}\nabla_{\mathbf{v}}^{2}\Psi\right]\]

\[\frac{\partial\log\widehat{\Psi}}{\partial t} =\frac{1}{\widehat{\Psi}}\left(-\nabla_{\mathbf{x}}\widehat{ \Psi}^{\mathsf{T}}\mathbf{v}+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\widehat{\Psi}\right)\] \[=-\nabla_{\mathbf{x}}\log\widehat{\Psi}^{\mathsf{T}}\mathbf{v}- \frac{1}{2}g^{2}\operatorname{Tr}\left[\frac{1}{\widehat{\Psi}}\nabla_{ \mathbf{v}}^{2}\widehat{\Psi}\right]\]

By applying Ito's lemma,

\[\mathrm{d}\log\Psi =\frac{\partial\log\Psi}{\partial t}\mathrm{d}t+\left[\nabla_{ \mathbf{x}}\log\Psi^{\mathsf{T}}\mathbf{v}+g^{2}\|\nabla_{\mathbf{v}}\log\Psi \|_{2}^{2}+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\log\Psi\right]\mathrm{d}t+ \left[\nabla_{\mathbf{m}}\log\Psi^{\mathsf{T}}\right]g\mathrm{d}\mathbf{w}_{t}\] \[=\left[-\nabla_{\mathbf{x}}\log\Psi^{\mathsf{T}}\mathbf{v}-\frac {1}{2}g^{2}\operatorname{Tr}\left[\frac{1}{\Psi}\nabla_{\mathbf{v}}^{2}\Psi \right]\right]\mathrm{d}t\] \[+\left[\nabla_{\mathbf{x}}\log\Psi^{\mathsf{T}}\mathbf{v}+g^{2} \|\nabla_{\mathbf{v}}\log\Psi\|_{2}^{2}+\frac{1}{2}g^{2}\operatorname{Tr} \left[\frac{1}{\Psi}\nabla_{\mathbf{v}}^{2}\Psi-\frac{1}{\Psi^{2}}\nabla_{ \mathbf{v}}\Psi\nabla_{\mathbf{v}}\Psi^{\mathsf{T}}\right]\right]\mathrm{d}t +g\left[\nabla_{\mathbf{v}}\log\Psi^{\mathsf{T}}\right]\mathrm{d}\mathbf{w}_{t}\] \[=\left[g^{2}\|\nabla_{\mathbf{v}}\log\Psi\|_{2}^{2}-\frac{1}{2}g ^{2}\operatorname{Tr}\left[\frac{1}{\Psi^{2}}\nabla_{\mathbf{v}}\Psi\nabla_{ \mathbf{v}}\Psi^{\mathsf{T}}\right]\right]\mathrm{d}t+g\left[\nabla_{\mathbf{ v}}\log\Psi^{\mathsf{T}}\right]\mathrm{d}\mathbf{w}_{t}\] \[=\left[\frac{1}{2}g^{2}\|\nabla_{\mathbf{v}}\log\Psi\|_{2}^{2} \right]\mathrm{d}t+g\left[\nabla_{\mathbf{v}}\log\Psi^{\mathsf{T}}\right] \mathrm{d}\mathbf{w}_{t}\]

Similarly, one can have,

\[\mathrm{d}\log\widehat{\Psi} =\frac{\partial\log\widehat{\Psi}}{\partial t}\mathrm{d}t+\left[ \nabla_{\mathbf{x}}\log\widehat{\Psi}^{\mathsf{T}}\mathbf{v}+g^{2}\nabla_{ \mathbf{v}}\log\Psi^{\mathsf{T}}\nabla_{\mathbf{v}}\log\widehat{\Psi}+\frac{1 }{2}g^{2}\Delta_{\mathbf{v}}\log\widehat{\Psi}\right]\mathrm{d}t+\left[\nabla _{\mathbf{m}}\log\widehat{\Psi}^{\mathsf{T}}\right]g\mathrm{d}\mathbf{w}_{t}\] \[=\left[-\nabla_{\mathbf{x}}\log\widehat{\Psi}^{\mathsf{T}} \mathbf{v}+\frac{1}{2}g^{2}\operatorname{Tr}\left[\frac{1}{\widehat{\Psi}} \nabla_{\mathbf{v}}^{2}\widehat{\Psi}\right]\right]\mathrm{d}t\] \[+\left[\nabla_{\mathbf{x}}\log\widehat{\Psi}^{\mathsf{T}} \mathbf{v}+g^{2}\nabla_{\mathbf{v}}\log\Psi^{\mathsf{T}}\nabla_{\mathbf{v}} \log\widehat{\Psi}+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\log\widehat{\Psi} \right]\mathrm{d}t+g\left[\nabla_{\mathbf{v}}\log\widehat{\Psi}^{\mathsf{T}} \right]\mathrm{d}\mathbf{w}_{t}\]

Noticing:

\[\frac{1}{2}\left[\frac{1}{\widehat{\Psi}}\nabla_{\mathbf{v}}^{2} \widehat{\Psi}+\nabla_{\mathbf{v}}^{2}\log\widehat{\Psi}\right] =\operatorname{Tr}\left[\frac{1}{\Psi}\nabla_{\mathbf{v}}^{2} \widehat{\Psi}-\frac{1}{2}\|\nabla_{\mathbf{v}}\log\widehat{\Psi}\|^{2}\right]\] \[=\frac{1}{2}\|\nabla_{\mathbf{v}}\log\widehat{\Psi}\|^{2}+ \Delta_{\mathbf{v}}\log\widehat{\Psi}\]

Following the above derivation, one can have,

\[\mathrm{d}\log\widehat{\Psi} =\left[-\nabla_{\mathbf{x}}\log\widehat{\Psi}^{\mathsf{T}} \mathbf{v}+\frac{1}{2}g^{2}\operatorname{Tr}\left[\frac{1}{\widehat{\Psi}} \nabla_{\mathbf{v}}^{2}\widehat{\Psi}\right]\right]\mathrm{d}t\] \[+\left[\nabla_{\mathbf{x}}\log\widehat{\Psi}^{\mathsf{T}} \mathbf{v}+g^{2}\nabla_{\mathbf{v}}\log\Psi^{\mathsf{T}}\nabla_{\mathbf{v}} \log\widehat{\Psi}+\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\log\widehat{\Psi}\right] \mathrm{d}t+g\left[\nabla_{\mathbf{v}}\log\widehat{\Psi}^{\mathsf{T}}\right] \mathrm{d}\mathbf{w}_{t}\] \[=\left[g^{2}\nabla_{\mathbf{v}}\log\Psi^{\mathsf{T}}\nabla_{ \mathbf{v}}\log\widehat{\Psi}+\frac{1}{2}g^{2}\|\nabla_{\mathbf{v}}\log \widehat{\Psi}\|^{2}+2\frac{1}{2}g^{2}\Delta_{\mathbf{v}}\log\widehat{\Psi} \right]\mathrm{d}t+g\left[\nabla_{\mathbf{v}}\log\widehat{\Psi}^{\mathsf{T}} \right]\mathrm{d}\mathbf{w}_{t}\]By defining

\[\mathbf{y} \equiv\mathbf{y}(\mathbf{x}_{t},\mathbf{v},t)=\log\Psi(\mathbf{x}_{t},\mathbf{v}_{t},t),\quad\mathbf{z}\equiv\mathbf{z}(\mathbf{x}_{t},\mathbf{v}_{t},t)=g\nabla_{\mathbf{v}}\log\Psi(\mathbf{x}_{t},\mathbf{v}_{t},t)\] \[\widehat{\mathbf{y}} \equiv\widehat{\mathbf{y}}(\mathbf{x}_{t},\mathbf{v}_{t},t)=\log \widehat{\Psi}(\mathbf{x}_{t},\mathbf{v}_{t},t),\quad\widehat{\mathbf{z}} \equiv\widehat{\mathbf{z}}(\mathbf{x}_{t},\mathbf{v}_{t},t)=g\nabla_{\mathbf{v }}\log\widehat{\Psi}(\mathbf{x}_{t},\mathbf{v}_{t},t)\]

One can conclude the results.

\[\begin{pmatrix}\text{d}\mathbf{x}\\ \text{d}\mathbf{v}\end{pmatrix}=\begin{pmatrix}\mathbf{v}\\ -g^{2}\nabla_{\mathbf{v}}\log\Psi\end{pmatrix}\text{d}t+\begin{pmatrix}\mathbf{ 0}&\mathbf{0}\\ \mathbf{0}&g\end{pmatrix}\text{d}\mathbf{w}\] \[\text{d}\mathbf{y}=\frac{1}{2}\|\mathbf{z}\|^{2}\text{d}t+ \mathbf{z}^{\mathsf{T}}\text{d}\mathbf{w}_{t}\] \[\text{d}\widehat{\mathbf{y}}=\left[\frac{1}{2}\|\widehat{ \mathbf{z}}\|^{2}+\mathbf{z}^{\mathsf{T}}\widehat{\mathbf{z}}+\nabla_{\mathbf{ v}}\cdot g\widetilde{\mathbf{z}}\right]\text{d}t+\widehat{\mathbf{z}}^{\mathsf{T}} \text{d}\mathbf{w}_{t}\] \[\text{s.t}:\exp\left(\mathbf{y}_{0}+\widehat{\mathbf{y}}_{0} \right)=p(\mathbf{x},\mathbf{v},0),\quad\exp\left(\mathbf{y}_{T}+\widehat{ \mathbf{y}}_{T}\right)=p(\mathbf{x},\mathbf{v},T)\]

**Proposition B.4**.: _The log-likelihood at data point \(\mathbf{m}_{0}\) can be expressed as_

\[\log p(\mathbf{m}_{0},0) =\mathbb{E}_{\mathbf{m}_{t}\sim(17)}\left[\log p(\mathbf{m}_{T}, T)\right]-\int_{0}^{T}\mathbb{E}_{\mathbf{m}_{t}\sim(17)}\left[\frac{1}{2}\| \mathbf{z}_{t}\|^{2}\text{d}t+\frac{1}{2}\|\widehat{\mathbf{z}}_{t}\|^{2}+ \mathbf{z}_{t}^{\mathsf{T}}\widehat{\mathbf{z}}_{t}+\nabla_{\mathbf{v}}\cdot g \widehat{\mathbf{z}}_{t}\right]\text{d}t\] \[=\mathbb{E}_{\mathbf{m}_{t}\sim(17)}\left[\log p(\mathbf{m}_{T}, T)\right]-\] \[\quad\int_{0}^{T}\mathbb{E}_{\mathbf{m}_{t}\sim(17)}\left[\frac{1 }{2}\|\mathbf{z}_{t}\|^{2}+\underbrace{\frac{1}{2}\|\widehat{\mathbf{z}}_{t}-g \nabla_{\mathbf{v}}\log p^{(17)}+\mathbf{z}_{t}\|^{2}}_{\text{mean matching objective}}-\frac{1}{2}\|g\nabla_{\mathbf{v}}\log p^{(17)}- \mathbf{z}_{t}\|^{2}\right]\text{d}t\] \[\quad\propto\quad\int_{0}^{T}\mathbb{E}_{\mathbf{m}_{t}\sim(16)} \left[\underbrace{\frac{1}{2}\|\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}} \log p^{(17)}+\mathbf{z}_{t}\|^{2}}_{\text{mean matching objective}}\right]\text{d}t\] \[\quad\propto\mathbb{E}_{\mathbf{m}_{t}\sim(16)}\left[\underbrace{ \frac{1}{2}\|\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}}\log p^{(16)}+ \mathbf{z}_{t}\|^{2}}_{\text{mean matching objective}}\right]\text{d}t\]

_By maximizing the log-likelihood at time \(t=0\) then \(t=T\) iteratively, \((\mathbf{z}_{t},\widehat{\mathbf{z}}_{t})\) will converge to the solution of phase space SB._Proof.: from Lemma.B.3, one can have:

\[\log p(\mathbf{m}_{0},0) =\mathbb{E}\left[\mathbf{y}_{0}+\widehat{\mathbf{y}}_{0}\right]\] \[=\mathbb{E}\left[\mathbf{y}_{T}+\widehat{\mathbf{y}}_{T}\right]- \int_{0}^{T}\mathbb{E}\left[\frac{1}{2}\|\mathbf{z}_{t}\|^{2}\mathrm{d}t+\frac {1}{2}\|\widehat{\mathbf{z}}_{t}\|^{2}+\mathbf{z}_{t}^{\mathsf{T}}\widehat{ \mathbf{z}}_{t}+\nabla_{\mathbf{v}}\cdot g\widehat{\mathbf{z}}_{t}\right]\mathrm{ d}t\] \[=\mathbb{E}\left[\log p(\mathbf{m}_{T},T)\right]-\int_{0}^{T} \mathbb{E}\left[\frac{1}{2}\|\mathbf{z}_{t}\|^{2}+\frac{1}{2}\|\widehat{ \mathbf{z}}_{t}\|^{2}+\mathbf{z}_{t}^{\mathsf{T}}\widehat{\mathbf{z}}_{t}+ \nabla_{\mathbf{v}}\cdot g\widehat{\mathbf{z}}_{t}\right]\mathrm{d}t\] \[=\mathbb{E}\left[\log p(\mathbf{m}_{T},T)\right]\] \[\quad-\int_{0}^{T}\mathbb{E}\left[\frac{1}{2}\|\mathbf{z}_{t}\|^ {2}+\frac{1}{2}\|\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}}\log p^{SB}+ \mathbf{z}_{t}\|^{2}-\frac{1}{2}\|g\nabla_{\mathbf{v}}\log p^{SB}-\mathbf{z}_ {t}\|^{2}\right]\mathrm{d}t\]

A similar result can be obtained for \(\log p(\mathbf{m}_{T},T)\).

One can notice that the likelihood objective is a continuous time analog of the mean matching objective proposed in [15], and iterative optimization between \(logp(\mathbf{m}_{0},0)\) and \(\log p(\mathbf{m}_{T},T)\) are the continuous analog of IPF. Hence, the convergence proof will keep valid (see Proposition 4 in [15]). 

The equivalence of KL divergence optimization in IPF and likelihood optimization is widely analyzed in [14, 15, 18]. The objective function will eventually boil down to the mean matching objective shown in the above proposition.B.4.

**Proposition B.5** (Optimality w.r.t. \(\mathcal{K}_{\text{boundary}}\)).: _Given the reference path measure \(\bar{\pi}\) driven by the policy \(\widehat{\mathbf{z}}_{t}\) from boundary \(\mu_{t_{i+1}}\) in the reverse time direction, the optimal path measure in the forward time direction of the following problem_

\[\min_{\pi}\mathcal{J}(\pi):=\sum_{i=0}^{N-1}KL\left(\pi_{t_{i}:t_{i+1}}|\bar{ \pi}_{t_{i}:t_{i+1}}\right),\quad s.t\quad\pi\quad\in\left\{\int\pi_{t_{i}:t_ {i+1}}\mathrm{d}\mathbf{m}_{t_{i+1}}=\mu_{t_{i}},\int\mu_{t_{i}}\mathrm{d} \mathbf{v}_{t_{i}}=\rho_{t_{i}}\right\}\]

\[\text{is}:\qquad\pi_{t_{i}:t_{i+1}}^{*}=\frac{\rho_{t_{i}}\bar{\pi}_{t_{i}:t_ {i+1}}}{\int\bar{\pi}_{t_{i}:t_{i+1}}\mathrm{d}\mathbf{m}_{t_{i+1}}\mathrm{d} \mathbf{v}_{t_{i}}}.\]

_When \(\pi_{t_{i}:t_{i+1}}\equiv\pi_{t_{i}:t_{i+1}}^{*},\) the following equations need to hold \(\forall t\in[t_{i},t_{i+1}]\):_

\[\|\mathbf{z}_{t}+\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}} \log\hat{p}_{t}\|_{2}^{2}=0,\] (35a) \[p_{t_{i}}(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}})\equiv\hat{q}_{ t_{i}}(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}}),\] (35b)

_where \(\hat{p}_{t}\) and \(\hat{q}_{t}\) denote the marginal density and conditional velocity distribution of the reference path measure at time \(t\) and \(t_{i}\), respectively._

Proof.: Due to the similarity of optimization for \(\mathcal{K}_{\text{boundary}}\), the close form solution of the next path measure is (see SS4 in [1] for detail):

\[\pi_{t_{i}:t_{i+1}}^{*}=\frac{\rho_{t_{i}}\bar{\pi}_{t_{i}:t_{i+1}}}{\int\bar{ \pi}_{t_{i}:t_{i+1}}\mathrm{d}\mathbf{m}_{t_{i+1}}\mathrm{d}\mathbf{v}_{t_{i}}}.\]

By denoting the transition kernel of parameterized SDE driven by backward policy \(\widehat{\mathbf{z}}_{t}\) as \(q(\cdot|\cdot)\), and the time range between \(t_{i}\) and \(t_{i+1}\) is discretized into \(S\) interval by EM discretization. Then one can get

\[\pi^{*}_{t_{i}:t_{i+1}}\] \[=\frac{\rho_{t_{i}}\bar{\pi}_{t_{i}:t_{i+1}}}{\int\bar{\pi}_{t_{i}:t _{i+1}}\mathbf{dm}_{t_{i+1}}\mathbf{dV}_{t_{i}}}\] \[=\frac{p_{t_{i}}(\mathbf{x}_{t_{i}})q_{t_{i}}(\mathbf{m}_{t_{i}}| \mathbf{m}_{t_{i}+\delta_{t}})\cdots q_{t_{i+1}-\delta_{t}}(\mathbf{m}_{t_{i+1 }-\delta_{t}}|\mathbf{m}_{t_{i+1}})\mu_{t_{i+1}}(\mathbf{m}_{t_{i+1}})}{q_{t_{i }}(\mathbf{x}_{t_{i}})}\] \[=\frac{p_{t_{i}}(\mathbf{x}_{t_{i}})q_{t_{i}}(\mathbf{x}_{t_{i}},\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t} })q_{t_{i+\delta_{t}}}(\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta _{t}})\cdots q_{t_{i+1}-\delta_{t}}(\mathbf{m}_{t_{i+1}-\delta_{t}}|\mathbf{m} _{t_{i+1}})\mu_{t_{i+1}}(\mathbf{m}_{t_{i+1}})}{q_{t_{i}}(\mathbf{x}_{t_{i}})q _{t_{i+\delta_{t}}}(\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t }})}\] \[=\frac{p_{t_{i}}(\mathbf{x}_{t_{i}})q_{t_{i}}(\mathbf{x}_{t_{i}},\mathbf{v}_{t_{i}},\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t }})\cdots q_{t_{i+1}-\delta_{t}}(\mathbf{m}_{t_{i+1}-\delta_{t}}|\mathbf{m}_{t_ {i+1}})\mu_{t_{i+1}}(\mathbf{m}_{t_{i+1}})}{q_{t_{i}}(\mathbf{x}_{t_{i}})q_{t_{ i+\delta_{t}}}(\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t}})}\] \[=\frac{p_{t_{i}}(\mathbf{x}_{t_{i}})q_{t_{i}}(\mathbf{x}_{t_{i}},\mathbf{v}_{t_{i}},\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t }})\cdots q_{t_{i+1}-\delta_{t}}(\mathbf{m}_{t_{i+1}-\delta_{t}}|\mathbf{m}_{t _{i+1}})\mu_{t_{i+1}}(\mathbf{m}_{t_{i+1}})}{q_{t_{i}}(\mathbf{x}_{t_{i}})q_{ t_{i+\delta_{t}}}(\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t}})}\] \[=\frac{p_{t_{i}}(\mathbf{x}_{t_{i}})q_{t_{i}}(\mathbf{v}_{t_{i}},\mathbf{x}_{t_{i}+\delta_{t}},\mathbf{v}_{t_{i}+\delta_{t}}|\mathbf{x}_{t_{i} })\cdots q_{t_{i+1}-\delta_{t}}(\mathbf{m}_{t_{i+1}-\delta_{t}}|\mathbf{m}_{t _{i+1}})\mu_{t_{i+1}}(\mathbf{m}_{t_{i+1}})}{q_{t_{i}+\delta_{t}}(\mathbf{m}_{ t_{i}+\delta_{t}})}\] \[=p_{t_{i}}(\mathbf{x}_{t_{i}})q(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_{ i}})q(\mathbf{m}_{t_{i}+\delta_{t}}|\mathbf{m}_{t_{i}})\frac{q_{t_{i+\delta_{t}}}( \mathbf{m}_{t_{i}+\delta_{t}}|\mathbf{m}_{t_{i}+2\delta_{t}})\cdots q_{t_{i+1}- \delta_{t}}(\mathbf{m}_{t_{i+1}-\delta_{t}}|\mathbf{m}_{t_{i+1}})\mu_{t_{i+1}} (\mathbf{m}_{t_{i+1}})}{q_{t_{i+\delta_{t}}}(\mathbf{m}_{t_{i}+2\delta_{t}})}\] (36)

**Doing eq.36 revursively**

\[=p_{t_{i}}(\mathbf{x}_{t_{i}})q(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_ {i}})q(\mathbf{m}_{t_{i}+\delta_{t}}|\mathbf{m}_{t_{i}})\prod_{s=1}^{S-1}q_{s} (\mathbf{m}_{t_{i}+(s+1)\cdot\delta_{t}}|\mathbf{m}_{t_{i}+s\cdot\delta_{t}})\] \[=p_{t_{i}}(\mathbf{x}_{t_{i}})q(\mathbf{v}_{t_{i}}|\mathbf{x}_{t_ {i}})\prod_{s=0}^{S-1}q_{s}(\mathbf{m}_{t_{i}+(s+1)\cdot\delta_{t}}|\mathbf{m} _{t_

**Proposition B.6** (Optimality w.r.t. \(\mathcal{K}_{\text{bridge}}\)).: _Given the reference path measure \(\bar{\pi}\) driven by the policy \(\widehat{\mathbf{z}}_{t}\) from boundary \(\mu_{t_{N}}\) in the reverse time direction, the optimal path measure in the forward time direction of the following problem_

\[\min_{\pi}\mathcal{J}(\pi):=\sum_{i=0}^{N-1}KL\left(\pi_{t_{i}:t_{i+ 1}}|\bar{\pi}_{t_{i}:t_{i+1}}\right),\quad\text{s.t.}\quad\pi\in\mathcal{K}_{ \text{bridge}}=\left\{\cap_{i=1}^{N-1}\mathcal{K}_{t_{i}}^{2}\right\}\] \[\text{is:}\quad\pi_{t_{0}t_{N}}^{*}=\frac{q_{t_{0}}\bar{\pi}_{t_ {0}:t_{N}}\text{d}\mathbf{v}_{t_{0}}}{\int\bar{\pi}_{t_{0}:t_{N}}\text{d} \mathbf{v}_{t_{0}}}.\]

_when \(\pi_{t_{0}t_{N}}\equiv\pi_{t_{0}t_{N}}^{*}\), the following equations need to hold \(\forall t\in[t_{0},t_{N}]\):_

\[\|\mathbf{z}_{t}+\widehat{\mathbf{z}}_{t}-g\nabla_{\mathbf{v}} \log\hat{p}_{t}\|_{2}^{2}=0\] (42a) \[p_{t_{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\equiv\hat{q}_{t _{0}}(\mathbf{v}_{t_{0}},\mathbf{x}_{t_{0}})\] (42b)

Proof.: Same proof as B.5. 

**Remark B.7**.: The optimizer of such a problem can be represented as

\[\pi^{*}=\mu_{t_{N}}\bar{\pi}_{\cdot|t_{N}}\] (43)

which can also be represented as,

\[\pi^{*}=\int\bar{\pi}\text{d}\mu_{t_{N}}\cdot(\bar{\pi}_{\cdot|t_{N}})^{R}\] (44)

Where the notation \(R\) represents for the time reversal. The Proposition.4.4 is basically using neural network \(\mathbf{Z}_{t}^{\theta}\) to approximate eq.44.

## Appendix C Experiment Details

We test DMSB on 2D synthetic datasets and realworld scRNA-seq dataset [30]. We parameterize \(\mathbf{z}(t,\mathbf{m};\theta)\) and \(\widehat{\mathbf{z}}(t,\mathbf{m};\phi)\) with residual-based networks for all datasets (see.fig.6). The network adopts position encoding and is trained with AdamW[45] on one Nvidia 3090 Ti GPU. We use constant g(t) for simplicity though the framework can adopt time varying function g(t). We set the time horizon \(T=t_{N}=1\cdot N\) and interval \(\delta_{t}=0.01\). We use EM discretization throughout the whole paper. For scRNA-seq dataset, we split data into train and test subsets(85% and 15%).All the experiment results are simulated by all-step push forward from initial data points at time \(t=t_{0}\).

**MIOFlow and NLSB setup:** We use the official implementation of NLSB and MIOFlow.For MIOFlow, we report the best performance for all experiments w/GAE(or AE) and w/o GAE(or AE) embedding. For NLSB, we enlarge the size of the neural network to the best of our GPU capacity for a 100-dimensional scRNA-seq dataset and report the best performance during the training.

We evaluate the velocity of NLSB, as an SDE model, by its estimated drift term at time steps \(t=\{1,2,3,4,5\}\). Because MIOflow w/ GAE simulates trajectories in the latent space, we estimate the velocity by using the forward finite difference technique with discretization \(1E-3\) sec after mapping from the latent code to the original space. We run the experiments of 100-D and 5-D RNAsc datasets and average the discrepancy between ground truth velocity and estimated velocity over snapshot time. The numerical values are listed in the Table.7 and Table.6. The plot of velocity and position can be found in Fig.9 and Fig.10. **We do not want to underestimate any prior work and tried out best to tune the prior work. Feel free to communicate with the first author if one can reproduce better results in the experiment section, and we are willing to update it.**

**Metrics and Evaluations** The 1-Wasserstein Distance suffers from the curse of dimensionality seriously. In the main paper, we are using Sliced-Wasserstein Distance (SWD) and Maximum Mean Distance as our criterion for 100-dim RNA dataset. An example is listed in the following toy code. One can notice that \(W_{1}\) suffers from the curse of dimensionality seriously, the distance between two gaussian samples is even larger than the distance between gaussian and zeros (See following code snapshot). Hence such a metric is not suitable for high dimension (\(\geq 100\)) dataset evaluation even though some papers report \(W_{1}\). In order to better evaluate our model compared with baselines, we are using \(W_{1}\), Energy Distance, Max-sliced Wasserstein distance, Sliced-Wasserstein Distance and MMD. Our metric is adapted from Geoloss (\(W_{1}\) and \(Energy\)), POT (Sliced Wasserstein and Maximum-Sliced Wasserstein) and this repo (MMD).

**Trajectories Cache** Similar to prior work [14; 15], we also need to cache the trajectories for training purposes. We cache 4096 trajectories for each Bregman Projection.

### Special Clarification for NLSB

We evaluate the velocity of NLSB, as an SDE model, by its estimated drift term at time steps \(t=\{1,2,3,4,5\}\). It may not be reasonable to consider the drift term as the real velocity, but the drift term can certainly depict a trend of SDE, so we still provide the result here.

**Training:We use Exponential Moving Average (EMA) with a decay rate of 0.999. Table.7 details the hyperparameters used for each dataset.The learning rate for all the datasets is set to be 2e-4 and the training batching size is \(256\). For computation efficiency, we cache large batch size of empirical samples from reference trajectory and sample training batch size from the cache data. The hyperparameters can be found in Table.7.**

**Langevin sampling:The Langevin sampling procedure for the velocity is summarized in 2. Given some pre-defined signal-to-noise ratio r (we set snr =0.15 for all experiments), the Langevin noise scale \(\sigma\) at each time step t and each corrector step i is computed by**

\[\sigma_{t}=\frac{2r^{2}g^{2}\|\epsilon\|^{2}}{\|\mathbf{z}(t,\mathbf{m}_{t})+ \widehat{\mathbf{z}}(t,\mathbf{m}_{t})\|^{2}},\] (45)

Figure 6: Neural network architecture for all experiments. The network size (# parameters) are varying between different tasks.

Figure 7: Training Hyper-parametersAlgorithms

``` Input: Policies \(\mathbf{z}(\cdot,\cdot;\theta)\) and \(\widehat{\mathbf{z}}(\cdot,\cdot;\phi)\) Total sampling step \(S=\frac{t_{N}}{\delta_{t}}\). Data distributions \(\rho_{t_{i}}\). Initializing velocity distributions \(\gamma_{t_{i}}=\mathcal{N}(0,\mathbf{I})\) if they are not avaliable. for\(s=0\)to\(S-1\)do ifs==0then  Sample position data \(\mathbf{x}_{t_{0}}\) from \(\rho_{t_{0}}\). if ground truth velocity distribution \(\gamma_{t_{0}}\) avaliable then  Sample velocity data \(\mathbf{v}_{t_{0}}\) from \(\gamma_{t_{0}}\) else  Sample velocity data \(\mathbf{v}_{t_{0}}\) by Langevin simulation conditioning on \(\mathbf{x}_{t_{0}}\).(Algorithm.2) endif \(\mathbf{m}_{t_{0}}=[\mathbf{x}_{t_{0}},\mathbf{v}_{t_{0}}]^{\mathsf{T}}\) endif  Simulating dynamics: \(\text{d}\mathbf{m}_{t}=[\boldsymbol{f}(\mathbf{m}_{t},t)+g(t)\mathbf{Z}_{t}] \,\text{d}t+g(t)\text{d}\mathbf{w}_{t}\)(eq.16) endfor return\(\mathbf{m}_{t\in[t_{0},t_{N}]}\) ```

**Algorithm 1** Sampling Procedure of DMSB

``` Input: policies \(\mathbf{z}(\cdot,\cdot;\theta)\) and \(\widehat{\mathbf{z}}(\cdot,\cdot;\phi)\), Previous timestep predicted velocity \(\mathbf{v}_{t_{i}}\). Sample position from ground truth \(\mathbf{x}_{t_{i}}\sim\rho_{t_{i}}\). for\(step=0\)to\(\#\) Langevin stepsdo  Sample \(\epsilon\sim\mathcal{N}(\mathbf{0},\boldsymbol{I})\).  Construct new \(\mathbf{m}_{t_{i}}=[\mathbf{x}_{t_{i}},\mathbf{v}_{t_{i}}]^{\mathsf{T}}\)  Compute \(\nabla_{\mathbf{v}}\log\widetilde{p}_{t}^{\theta,\phi}\approx[\mathbf{z}(t_{i },\mathbf{m}_{t_{i}})+\widehat{\mathbf{z}}(t_{i},\mathbf{m}_{t_{i}})]/g\).  Compute \(\sigma_{t}\) with (45).  Langevin Sampling \(\mathbf{v}_{t_{i}}\leftarrow\mathbf{v}_{t_{i}}+\sigma_{t_{i}}\nabla_{\mathbf{ v}}\log\widetilde{p}_{t_{i}}^{\theta,\phi}+\sqrt{2\sigma_{t}}\,\epsilon\). endfor return\(\mathbf{m}_{t_{i}}=[\mathbf{x}_{t_{i}},\mathbf{v}_{t_{i}}]^{\mathsf{T}}\) ```

**Algorithm 2** Langevin Sampler at \(t_{i}\) marginal constraint ``` Input:\(N+1\) Marginal position distribution \(\rho_{t_{i}}\), \(i\in[0,N]\).Parametrized policies \(\mathbf{z}(\cdots;\theta)\) and \(\widehat{\mathbf{z}}(\cdots;\phi)\). The number of Bregman Iteration \(B\). Initialize postion and velocity at time step \(t_{i}:\bar{\mathbf{m}}_{t_{i}}:=None\) for the first iteration. if Use ground truth velocity then  set prior velocity: \(\gamma_{t_{i}}=\gamma_{t_{i}}\) else  set initial velocity \(\gamma_{t_{i}}=\mathcal{N}(0,\boldsymbol{I})\) endif for\(b=0\)to\(B-1\)do for\(k=N\)to\(1\)do \(\mathbf{z}^{\phi},_{=}=OptSubSet(t_{k},t_{k-1},\mathbf{z}_{ref}=\mathbf{z}^{ \theta},\mathbf{z}_{opt}=\mathbf{z}^{\phi},\eta=\phi,\bar{\mathbf{m}}=None)\) [Optimize \(\mathcal{K}_{\text{boundary}}\)] endfor for\(k=0\)to\(N-1\)do \(\mathbf{z}^{\theta},_{=}=OptSubSet(t_{k},t_{k+1},\mathbf{z}_{ref}=\mathbf{z}^{ \phi},\mathbf{z}_{opt}=\mathbf{z}^{\theta},\eta=\theta,\bar{\mathbf{m}}=None)\) [Optimize \(\mathcal{K}_{\text{boundary}}\)] endfor \(\mathbf{z}^{\phi},_{\bar{\mathbf{m}}}=OptSubSet(t_{N},t_{0},\mathbf{z}_{ ref}=\mathbf{z}^{\theta},\mathbf{z}_{opt}=\mathbf{z}^{\phi},\eta=\phi,\bar{\mathbf{m}}=\bar{ \mathbf{m}}\)) [Optimize \(\mathcal{K}_{\text{bridge}}\)] for\(k=0\)to\(N-1\)do \(\mathbf{z}^{\theta},_{=}=OptSubSet(t_{k},t_{k+1},\mathbf{z}_{ref}=\mathbf{z}^{ \phi},\mathbf{z}_{opt}=\mathbf{z}^{\theta},\eta=\theta,\bar{\mathbf{m}}=\bar{ \mathbf{m}}\)) [Optimize \(\mathcal{K}_{\text{boundary}}\)] endfor for\(k=N\)to\(1\)do \(\mathbf{z}^{\phi},_{=}=OptSubSet(t_{k},t_{k-1},\mathbf{z}_{ref}=\mathbf{z}^{ \theta},\mathbf{z}_{opt}=\mathbf{z}^{\phi},\eta=\phi,\bar{\mathbf{m}}=None)\) [Optimize \(\mathcal{K}_{\text{boundary}}\)] endfor \(\mathbf{z}^{\phi},_{\bar{\mathbf{m}}}=OptSubSet(t_{0},t_{N},\mathbf{z}_{ ref}=\mathbf{z}^{\phi},\mathbf{z}_{opt}=\mathbf{z}^{\theta},\eta=\theta,\bar{ \mathbf{m}}=\bar{\mathbf{m}}\)) [Optimize \(\mathcal{K}_{\text{bridge}}\)] endfor ```

**Algorithm 4** Function OptSubSet (Optimization for subsets)

``` input: Initial time \(t_{i}\) and terminal time \(t_{j}\). Reference path measure boundary condition \(\rho_{t_{i}}\). Reference path measure driver \(\mathbf{z}_{ref}\). Policy being optimized \(\mathbf{z}_{opt}\) and corresponding parameter \(\eta\). Empirical sample form last iteration \(\widehat{\mathbf{m}}\). output:\(\mathbf{z}_{opt}\),samples \(\widehat{\mathbf{m}}_{t_{j}}\) from reference path measure. if\(\widehat{\mathbf{m}}\) is None then  Sample position data \(\mathbf{x}_{t_{i}}\) from \(\rho_{t_{j}}\). if velocity distribution \(\gamma_{t_{i}}\) avaliable then  Sample conditional velocity data \(\mathbf{v}_{t_{i}}\) from \(\gamma_{t_{i}}\) else  Sample velocity data \(\mathbf{v}_{t_{i}}\) by Langevin simulation conditioning on \(\mathbf{x}_{t_{i}}\).(see Algorithm.2.) endif \(\mathbf{m}_{t_{i}}=[\mathbf{x}_{t_{i}},\mathbf{v}_{t_{i}}]^{\mathsf{T}}\) else \(\mathbf{m}_{t_{i}}=\bar{\mathbf{m}}\) endif  Sample trajectory \(\mathbf{m}_{t\in[t_{i},t_{j}]}\) from \(\mathbf{m}_{t_{i}}\) using \(\mathbf{z}_{ref}\)  Compute \(\mathcal{L}=\alpha\mathcal{L}_{\text{MM}}+(1-\alpha)\mathcal{L}_{reg}\) (Regularization of SB \(L_{reg}\)[29] is optional ) update \(\eta\) ```

**Algorithm 5** Function OptSubSet (Optimization for subsets)

[MISSING_PAGE_EMPTY:26]

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dim=100 & Energy \(\downarrow\) & MMD \(\downarrow\) & SWD\(\downarrow\) & MWD \(\downarrow\) \\ \hline NLSB2 & 2.12 & 1.6 & 0.94 & 1.27 \\ MIOFLOW & 9.18 & 2.41 & 1.89 & 5.66 \\
**DMSB(ours)** & **0.36** & **0.18** & **0.39** & **0.78** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Numerical result of Wasserstein-1 (\(W_{1}\)), MMD, energy distance and Max-sliced Wasserstein distance (MWD) on the **velocity** of 100 dimensions single-cell RNA-seq dataset using 500 generative samples and 500 ground truth data.

Figure 10: Comparison of estimated velocity on 5-dimensional PCA space at the moment of observation for scRNA-seq data using MIOFlow, NLSB, and DMSB. We display the plot of the first 4 principle components (PC). For the results of NLSB, see special clarification of NLSB in Appendix.C

Figure 9: Comparison of population-level dynamics on 5-dimensional PCA space at the moment of observation for scRNA-seq data using MIOFlow, NLSB, and DMSB. We display the plot of the first 4 principle components (PC). All method performs well under this experiment setup.

## Appendix G Intuitions of Propositions and Theorems

1. Remark for Proposition 3.1: Within each half-bridge IPF, the variable \(\mathbf{Z}_{t}(\text{or }\widehat{\mathbf{Z}}_{t})\) is essentially learning the reverse-time stochastic process induced by \(\widehat{\mathbf{Z}}_{t}\). This process can also be viewed as minimizing the approximated parameterized negative log-likelihood.

Figure 11: Comparison of estimated velocity on 100-dimensional PCA space at the moment of observation for scRNA-seq data using MIOFlow, NLSB, and DMSB. We display the plot of the first 6 principle components (PC). For the results of NLSB, see special clarification of NLSB in Appendix.C

Figure 12: Comparison of estimated velocity on 100-dimensional PCA space at the moment of observation for scRNA-seq data using DMSB with ground truth. We display the plot of the first 6 principle components (PC).

[MISSING_PAGE_FAIL:29]