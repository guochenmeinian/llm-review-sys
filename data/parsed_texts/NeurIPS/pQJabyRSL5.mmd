# The Importance of Being Bayesian in Online Conformal Prediction

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Based on the framework of _Conformal Prediction_ (CP), we study the online construction of valid confidence sets given a black-box machine learning model. Converting the targeted confidence levels to quantile levels, the problem can be reduced to predicting the quantiles (in hindsight) of a sequentially revealed data sequence, where existing results can be divided into two types.

* Assuming the data sequence is iid, one could maintain the empirical distribution of the observed data as an algorithmic belief, and directly predict its quantiles.
* As the iid assumption is often violated in practice, a recent trend is to apply first-order online optimization on moving quantile losses [1]. This indirect approach requires knowing the targeted quantile level beforehand, and suffers from certain validity issues on the obtained confidence sets, due to the associated loss linearization.

This paper presents a Bayesian approach that combines their strengths. Without any statistical assumption, it is able to both

* answer multiple arbitrary confidence level queries online, with provably low regret; and
* overcome the validity issues suffered by first-order optimization baselines, due to being "data-centric" rather than "iterate-centric".

From a technical perspective, our key idea is to take the above iid-based procedure and regularize its algorithmic belief by a Bayesian prior, which "robustifies" it by simulating a non-linearized _Follow the Regularized Leader_ (FTRL) algorithm on the output. For statisticians, this can be regarded as an online adversarial view of Bayesian nonparametric distribution estimation. Importantly, the proposed belief update backbone is shared by "prediction heads" targeting different confidence levels, bringing practical benefits similar to U-calibration [14].

## 1 Introduction

Modern machine learning (ML) models are better at point prediction compared to probabilistic prediction. For example, when given an image classification task, they are better at responding "_this image is most likely a white cat_", rather than "_I'm 90% sure this image is an animal, 60% sure it's a cat, and 30% sure it's a white cat_". For downstream users, the more nuanced probabilistic predictions are often important for risk assessment. The challenge, however, lies in aligning the model's own uncertainty evaluation with its actual performance in the real world.

_Conformal Prediction_ (CP) [23] has recently emerged as a premier framework to address this challenge, as it blends the empirical strength of modern ML with the theoretical soundness oftraditional statistical methods. In a nutshell, CP algorithms make _confidence set predictions_ (rather than point predictions) on the label space, by sequentially interacting with three other parties: the _nature_ (i.e., the data stream), a _black-box ML model_, and _downstream users_. Writing the covariate-label space as \(\mathcal{X}\times\mathcal{Y}\) and the time horizon as \(T\), we consider the following sequential interaction protocol. In each (the \(t\)-th) round,

1. We, as the CP algorithm, observe a _target covariate_\(x_{t}\in\mathcal{X}\) from the nature, and a _score function_\(s_{t}:\mathcal{X}\times\mathcal{Y}\to[0,R]\) generated by a black-box ML model Base.1 Footnote 1: An example is classification, where the score function is usually the softmax score of each candidate label (\(R=1\)). It is _positively oriented_: larger means the model is more certain that the candidate label is the true one. For regression, it is more common to use _negatively oriented_ score functions, which means the inequality in Eq.(1) is reversed.
2. The downstream users select a finite set of _confidence level queries_, \(A_{t}\subset[0,1]\). Given each \(\alpha\in A_{t}\), we predict a _score threshold_\(r_{t}(\alpha)\),2 which leads to a _confidence set_ Footnote 2: This extended abstract focuses on _marginal_ CP. More generally, the CP algorithm can predict \(r_{t}(x_{t},\alpha)\). \[\mathcal{C}_{t}(x_{t},\alpha)=\{y\in\mathcal{Y}:s_{t}(x_{t},y)\geq r_{t}( \alpha)\}\,.\] (1)
3. We observe the _ground truth label_\(y_{t}\in\mathcal{Y}\) from the nature, and send the \((x_{t},y_{t})\) pair to Base (which it optionally uses to update the score function \(s_{t+1}\)). Define the _true score_\(r^{*}_{t}:=s_{t}(x_{t},y_{t})\).

Limitation of prior workThe essential objective of CP is to have the prediction \(r_{t}(\alpha)\) close to the (\(1-\alpha\))-_quantile_ of the true score sequence \(r^{*}_{1:T}\), while only knowing \(r^{*}_{1:t-1}\)[14, 15]. For the readers' reference, the \((1-\alpha)\)-quantile of a real random variable \(X\) is defined as \(q_{1-\alpha}(X):=\min\{x;\mathbb{P}(X\leq x)\geq 1-\alpha\}\). Guided by this general principle, the community has focused on two very different approaches under distinct assumptions.

* Assuming the sequence \(r^{*}_{1:T}\) is iid, it suffices to maintain the empirical distribution of \(r^{*}_{1:t-1}\), denoted as \(P_{t}=\bar{P}(r^{*}_{1:t-1})\), as an _algorithmic belief_. Then, when queried with the confidence level \(\alpha\), the CP algorithm directly "post-processes" the belief by setting \(r_{t}(\alpha)=q_{1-\alpha}(P_{t})\), or in situations with only _exchangeability_, \(q_{1-\alpha-o(1)}(P_{t})\)[15]. This is essentially _Empirical Risk Minimization_ (ERM) with the quantile loss \(l_{\alpha}(r,r^{*}):=(\alpha-\mathbf{1}[r<r^{*}])(r-r^{*})\), i.e., \[r_{t}(\alpha)=q_{1-\alpha}(P_{t})\in\operatorname*{arg\,min}_{r\in[0,R]}\sum _{i=1}^{t-1}l_{\alpha}(r,r^{*}_{i}).\] (2)
* Since the iid assumption is often violated in practice, a recent trend [1] is to indirectly view CP as an instance of _adversarial online learning_[1, 13], and apply first-order optimization algorithms from there. Taking gradient descent for example, such an approach amounts to picking \(r_{1}(\alpha)\in[0,R]\) and following with the projected incremental update \[r_{t+1}(\alpha)=\Pi_{[0,R]}\left[r_{t}(\alpha)-\eta_{t}\partial l_{\alpha}(r_ {t}(\alpha),r^{*}_{t})\right],\] where \(\eta_{t}>0\) is the _learning rate_, and \(\partial l_{\alpha}(r,r^{*})\) can be any subgradient of the quantile loss \(l_{\alpha}\) with respect to the first argument. Strictly speaking the two approaches are incomparable due to targeting different performance metrics, but nonetheless, let us compare the _algorithms_ side by side. Although first-order optimization seems more robust due to the nonnecessity of statistical assumptions, it requires being "iterate-centric" rather than "data-centric": one needs to fix a single confidence level \(\alpha\) beforehand, and the prediction \(r_{t}(\alpha)\) depends on how previous predictions \(r_{1:t-1}(\alpha)\) compare to the "data" \(r^{*}_{1:t-1}\), rather than just the "data" itself. This leads to some paradoxical observations regarding the obtained confidence sets. For example,
* The confidence set \(\mathcal{C}_{t}\) is not invariant to permutations of \(r^{*}_{1:t-1}\).
* Suppose one runs two first-order optimization algorithms targeting different \(\alpha\) (say, \(\alpha_{1}<\alpha_{2}\)), then even if the initialization \(r_{1}(\alpha_{1})=r_{1}(\alpha_{2})\), it is still possible that \(\mathcal{C}_{t}(x_{t},\alpha_{1})\) is strictly contained in \(\mathcal{C}_{t}(x_{t},\alpha_{2})\). That is, the confidence sets violate the monotonicity of probability measures.

In contrast, the ERM approach does not suffer from such issues, therefore is more "valid / plausible" in some sense. The problem is that ERM, also known as _Follow the Leader_ (FTL) in online learning, is not robust to adversarial environments with quantile losses. Can we enjoy the best of both worlds?ContributionThis paper presents a Bayesian approach to CP, which (\(i\)) does not require any statistical assumption; (\(ii\)) does not suffer from the aforementioned validity issues; and (\(iii\)) efficiently handles multiple, arbitrary confidence levels revealed online, with provably low regret. Our main workhorse, in short, is an online adversarial view of Bayesian nonparametric estimation.

## 2 Main result

OverviewOur proposed algorithm (Algorithm 1) is perhaps the simplest one could think of. Defining the _Bayesian prior_ as an arbitrary distribution \(P_{0}\) on the domain \([0,R]\) (with strictly positive density \(p_{0}:[0,R]\rightarrow\mathbb{R}_{>0}\)), we update the algorithmic belief \(P_{t}\) by mixing \(P_{0}\) with the empirical distribution of the previous true scores, \(\bar{P}(r_{1:t-1}^{*})\). This can be seen as regularizing the frequentist belief update \(P_{t}=\bar{P}(r_{1:t-1}^{*})\). Then, given each queried confidence level \(\alpha\), our algorithm picks \(r_{t}(\alpha)=q_{1-\alpha}(P_{t})\) just like the iid-based approach. It is clear that \(r_{t}(\alpha)\) is invariant to permutations of \(r_{1:t-1}^{*}\), and for any \(\alpha_{1}<\alpha_{2}\) we always have \(r_{t}(\alpha_{1})\leq r_{t}(\alpha_{2})\).

```
0: Step sizes \(\{\lambda_{t}\}_{t\in\mathbb{N}_{+}}\), where each \(\lambda_{t}\in[0,1]\) and \(\lambda_{1}=1\). Bayesian prior \(P_{0}\).
1:for\(t=1,2,\ldots\)do
2: Compute the empirical distribution \(\bar{P}(r_{1:t-1}^{*})\), and set the algorithmic belief \(P_{t}\) to \[P_{t}=\lambda_{t}P_{0}+(1-\lambda_{t})\cdot\bar{P}(r_{1:t-1}^{*}).\] (3)
3:for\(\alpha\in A_{t}\)do
4: Output the score threshold \(r_{t}(\alpha)=q_{1-\alpha}(P_{t})\).
5:endfor
6: Observe the true score \(r_{t}^{*}\).
7:endfor ```

**Algorithm 1** Online conformal prediction with regularized belief.

Our central observation, however, is quite profound in our opinion:

The Bayesian regularization on the algorithmic belief \(P_{t}\) induces _downstream regularizations_ on the predicted threshold \(r_{t}(\alpha)\).

In particular, Theorem 1 shows that despite not knowing \(\alpha\) beforehand, Algorithm 1 generates the same output \(r_{t}(\alpha)\) as a non-linearized _Follow the Regularized Leader_ (FTRL) algorithm with the quantile loss \(l_{\alpha}\). To provide more context, FTRL is a standard improvement of ERM / FTL with better stability in adversarial environments, and our framework involves its non-linearized version which retains the full structure of quantile losses. It is also important to note that the _downstream simulation_ of FTRL deviates from the common scope of online learning (which requires specifying a single loss function in each round [11, 12]), and instead has a similar flavor as the recently proposed _U-calibration_[13, 14]: forecasting for an _unknown_ downstream agent.

From a more technical perspective: prior works on U-calibration considered the setting of "finite-class distributional prediction" with generic _proper_ losses [13, 14], while our paper focuses on the continuous domain \([0,R]\) (i.e., "infinitely many classes") with the more specific quantile losses. The extra problem structure allows our algorithm to be deterministic (rather than _Follow the Perturbed Leader_; FTPL), thus establishing a closer connection to deterministic _online convex optimization_.

Appendix A further discusses the interpretation of the belief update Eq.(3) as _Bayesian nonparametric distribution estimation_. The nontrivial insight here is that this statistical procedure induces downstream adversarial regret bounds, without statistical assumptions at all.

AnalysisFormally, we first present the FTRL-equivalence of Algorithm 1, which can be compared to the FTL-equivalence of the iid-based approach, i.e., Eq.(2).

**Theorem 1**.: _With a base regularizer defined as \(\psi(r):=\mathbb{E}_{r^{*}\sim P_{0}}[l_{\alpha}(r,r^{*})]\), the output \(r_{t}(\alpha)\) of Algorithm 1 satisfies_

\[r_{t}(\alpha)\in\operatorname*{arg\,min}_{r\in[0,R]}\left[\frac{\lambda_{t}(t -1)}{1-\lambda_{t}}\cdot\psi(r)+\sum_{i=1}^{t-1}l_{\alpha}(r,r_{i}^{*}) \right],\quad\forall\alpha\in[0,1],t\geq 2.\] (4)_Specifically, (i) \(\psi\) is strongly convex with coefficient \(\inf_{r\in[0,R]}p_{0}(r)\); and (ii) if \(P_{0}\) is the uniform distribution on \([0,R]\), then \(\psi\) is the quadratic function,_

\[\psi(r)=\frac{1}{2R}r^{2}-(1-\alpha)r+\frac{1}{2}(1-\alpha)R.\]

Next, using Theorem 1, we obtain the following _regret bound_ for our CP algorithm. Here we only consider the uniform prior, and defer the case of generic priors to longer versions of this paper (the benefit of good priors can be shown using the _local norm_ analysis of FTRL [11, Section 7.4]).

**Theorem 2**.: _Let \(P_{0}\) be the uniform distribution on \([0,R]\). With the step size \(\lambda_{t}=1/\sqrt{t}\), the output of Algorithm 1 against any \(r_{1:T}^{*}\) sequence satisfies_

\[\sum_{t=1}^{T}l_{\alpha}(r_{t}(\alpha),r_{t}^{*})-\sum_{t=1}^{T}l_{\alpha}(q_ {1-\alpha}(r_{1:T}^{*}),r_{t}^{*})=O(R\sqrt{T}),\quad\forall\alpha\in[0,1],\]

_where \(q_{1-\alpha}(r_{1:T}^{*})\) denotes the \((1-\alpha)\)-quantile of the hindsight empirical distribution \(\bar{P}(r_{1:T}^{*})\), and \(O(\cdot)\) subsumes absolute constants._

Let us interpret this bound. Suppose \(\bar{P}(r_{1:T}^{*})\) is known beforehand (but the exact \(r_{1:T}^{*}\) sequence is unknown), then for all \(\alpha\), a very reasonable strategy is to predict \(r_{t}(\alpha)=q_{1-\alpha}(r_{1:T}^{*})\). Theorem 2 shows that without statistical assumptions, Algorithm 1 asymptotically performs as well as this oracle in terms of the total quantile loss. Existing first-order optimization baselines are equipped with regret bounds of a similar type [1, 1, 10], but the key difference is that they require knowing \(\alpha\) beforehand, whereas Algorithm 1 achieves low regret simultaneously for all \(\alpha\in[0,1]\).

## 3 Discussion

Any-\(\alpha\) baselineAlthough not studied in existing works, it is actually possible to construct a nonstochastic CP algorithm from first-order optimization algorithms, without specifying a fixed \(\alpha\) beforehand. The idea is simple: (\(i\)) evenly discretize the \([0,1]\) interval using a grid \(\bar{A}\) of size \(\sqrt{T}\); (\(ii\)) for each \(\bar{\alpha}\in\bar{A}\), run a "base" CP algorithm targeting \(\bar{\alpha}\); and (\(iii\)) at test time, given a queried \(\alpha\), follow the base algorithm corresponding to its nearest neighbor in \(\bar{A}\). It also satisfies the regret bound in Theorem 2, since the nearest-neighbor approximation only adds an additive \(O(R\sqrt{T})\) factor due to the Lipschitzness of the quantile loss function \(l_{\alpha}(r,r^{*})\) with respect to \(\alpha\).

However, such a baseline also suffers from the previously mentioned validity issues. Even more, the update (based on \(r_{t}^{*}\)) and the queries (based on \(A_{t}\)) are coupled: if \(A_{t}\) is empty for a certain \(t\) (all the users abstain), the baseline still needs \(O(\sqrt{T})\) time in that round to process the observation \(r_{t}^{*}\). In comparison, Algorithm 1 needs one UpdateTime to process \(r_{t}^{*}\) and \(|A_{t}|\) QueryTime to answer the \(\alpha\)-queries, where their exact values depend on the data structure used to maintain the belief \(P_{t}\).

Coverage boundA common objective in online CP, initiated by [1], is to show that given a confidence level \(\alpha\), the post-hoc empirical _coverage frequency_ of an algorithm approaches \(\alpha\), i.e.,

\[\left|\alpha-T^{-1}\sum_{t=1}^{T}\mathbf{1}[r_{t}^{*}\geq r_{t}^{*}(\alpha)] \right|=o(1).\]

Since this can be achieved by switching between \(r_{t}^{*}(\alpha)=0\) and \(r_{t}^{*}(\alpha)=R\) independently of data [1], one needs an extra objective, such as the regret (Theorem 2), to justify the validity of an online CP method. Existing first-order optimization baselines satisfy both desirable bounds.

Here we argue that the regret could be a better-posed objective than the coverage. To support this argument, notice that just like the previous pathological example, first-order optimization baselines achieve the coverage bound due to the "overshooting" provided by the loss linearization, and the latter also causes the validity issues discussed earlier. Besides, achieving the coverage bound requires adjusting the prediction based on the _coverage history_: if an algorithm keeps mis-covering, then it has to predict a very small \(r_{t}(\alpha)\) to "almost ensure" coverage. These are different from regret minimization, where loss linearization is not necessary, and the algorithm is incentivized to best-respond to its belief (on the empirical distribution of the environment in hindsight).

## References

* [BGJ\({}^{+}\)22] Osbert Bastani, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Practical adversarial multivalid conformal prediction. _Advances in Neural Information Processing Systems_, 35:29362-29373, 2022.
* [BWXB23] Aadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In _International Conference on Machine Learning_, pages 2337-2363. PMLR, 2023.
* [GC21] Isaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. _Advances in Neural Information Processing Systems_, 34:1660-1672, 2021.
* [GC24] Isaac Gibbs and Emmanuel J Candes. Conformal inference for online prediction with arbitrary distribution shifts. _Journal of Machine Learning Research_, 25(162):1-36, 2024.
* [GCS\({}^{+}\)21] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. Bayesian data analysis. http://www.stat.columbia.edu/~gelman/book/BDA3.pdf, 2021.
* [Haz23] Elad Hazan. Introduction to online convex optimization. _arXiv preprint arXiv:1909.05207v3_, 2023.
* [KLST23] Bobby Kleinberg, Renato Paes Leme, Jon Schneider, and Yifeng Teng. U-calibration: Forecasting for an unknown agent. In _Conference on Learning Theory_, pages 5143-5145. PMLR, 2023.
* [LS20] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [LSS24] Haipeng Luo, Spandan Senapati, and Vatsal Sharan. Optimal multiclass U-calibration error and beyond. _arXiv preprint arXiv:2405.19374_, 2024.
* [Ora23] Francesco Orabona. A modern introduction to online learning. _arXiv preprint arXiv:1912.13213_, 2023.
* [Rot22] Aaron Roth. Uncertain: Modern topics in uncertainty estimation. https://www.cis.upenn.edu/~aaroth/uncertainty-notes.pdf, 2022.
* [Tib23] Ryan Tibshirani. Advanced topics in statistical learning: Conformal prediction. https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/conformal.pdf, 2023.
* [VGS05] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. _Algorithmic learning in a random world_, volume 29. Springer, 2005.
* [XZ23] Yunbei Xu and Assaf Zeevi. Bayesian design principles for frequentist sequential learning. In _International Conference on Machine Learning_, pages 38768-38800. PMLR, 2023.
* [ZBY24] Zhiyu Zhang, David Bombara, and Heng Yang. Discounted adaptive online learning: Towards better regularization. In _International Conference on Machine Learning_, pages 58631-58661. PMLR, 2024.

## Appendix A Bayesian interpretation

We further discuss the Bayesian interpretation of our algorithm, i.e., how the belief update Eq.(3) can be viewed from the statistical lens as the Bayesian nonparametric estimation of the underlying distribution from iid samples. We will follow [14, Chapter 23]. This is not new, and we provide it only for the readers' reference.

Distribution estimationConsider the following standard statistical problem: given \(x_{1},\ldots,x_{n}\in\mathcal{X}\) sampled iid from an unknown distribution \(X\), what is a good estimate of \(X\)? The simplest nonparametric estimate is just the empirical distribution \(\bar{P}(x_{1:n})\).

However, there is a parallel Bayesian perspective. It says that before observing any samples, we hold a certain _prior_\(F_{0}\) on \(X\), where \(F_{0}\) is a distribution on all possibilities of \(X\) (i.e., all distributions supported on the domain \(\mathcal{X}\)). Then, after observing the samples \(x_{1:n}\), we can use the Bayes' theorem to compute the _posterior_\(F_{n}\), the distribution of \(X\) conditioned on the samples. Our estimate of \(X\) can be just \(\mathbb{E}[F_{n}]\), the expectation of the posterior. This is _Bayes-optimal_ under the square loss.

Concretely, one would like \(F_{0}\) to be a _conjugate prior_: it refers to a family of distributions (over \(X\)) such that if the prior \(F_{0}\) belongs to this family, then the posterior \(F_{n}\) also belongs to this family. The most notable conjugate prior for distribution estimation is the _Dirichlet process_ (DP), denoted as \(\mathrm{DP}(\alpha,P_{0})\). Here \(\alpha\) and \(P_{0}\) are hyperparameters of a DP: \(P_{0}\) equals the mean \(\mathbb{E}[\mathrm{DP}(\alpha,P_{0})]\), while \(\alpha\) controls the variance of \(\mathrm{DP}(\alpha,P_{0})\). The larger \(\alpha\) is, the smaller the variance of \(\mathrm{DP}(\alpha,P_{0})\) gets. Due to the conjugacy, if the prior \(F_{0}=\mathrm{DP}(\alpha,P_{0})\), then the posterior after iid observations \(x_{1:n}\) is

\[F_{n}=\mathrm{DP}\left(\alpha+n,\frac{\alpha}{\alpha+n}P_{0}+\frac{n}{\alpha+ n}\bar{P}(x_{1:n})\right).\]

Consequently, the Bayesian estimate of the distribution \(X\) is

\[\mathbb{E}[F_{n}]=\frac{\alpha}{\alpha+n}P_{0}+\frac{n}{\alpha+n}\bar{P}(x_{1 :n}).\]

This is the same as the belief update Eq.(3) in our algorithm, with \(\lambda_{t}=\alpha/(\alpha+n)\).

A more intuitive but less rigorous explanation: the Bayesian estimate \(\mathbb{E}[F_{n}]\) could be regarded as adding "fictitious counts" to the samples \(x_{1:n}\). It means that before observing \(x_{1:n}\), we sample fictitious data \(\tilde{x}_{1:N}\in\mathcal{X}\) from the prior \(P_{0}\) (for some large \(N\)) and give each of them equal but small weights, such that their total weight equals \(\alpha\). Then, after observing the true samples \(x_{1:n}\), our Bayesian distribution estimate is the "weighted" empirical distribution taking both \(x_{1:n}\) and \(\tilde{x}_{1:N}\) into account.

Adversarial BayesDeviating from the above, a novelty of our work is rigorously showing that in an adversarial setting (without statistical assumptions), it is still beneficial to maintain the same Bayesian algorithmic belief on the environment and best-respond to that. Mathematically this is simple after establishing the downstream equivalence to regularization (Theorem 1), but the connection between this idea and CP is quite surprising to us.

To provide more context, such an idea of "adversarial Bayes" is closely related to the use of _Follow the Perturbed Leader_ (FTPL) in adversarial online learning: in each round, FTPL randomly perturbs a summary of the historical observations, and best-responds to that using an optimization oracle. This can be regarded as best-responding to a belief _sampled_ from a Bayesian posterior (rather than the posterior mean), and prior works on U-calibration (with possibly nonconvex losses) [13, 14] are essentially built on this idea. Another well-known example is _Thompson sampling_, a prevalent Bayesian approach for bandits and reinforcement learning [14, 15].

Different from U-calibration and bandits, the online CP problem we consider has convex losses and _full information_ feedback. This removes the need of randomization, therefore our algorithmic belief is chosen as the posterior mean. The algorithm simulates FTRL rather than FTPL on the output, which is deterministic, analytically simpler, and arguably more interpretable.

Omitted proofs

**Theorem 1**.: _With a base regularizer defined as \(\psi(r):=\mathbb{E}_{r^{*}\sim P_{0}}[l_{\alpha}(r,r^{*})]\), the output \(r_{t}(\alpha)\) of Algorithm 1 satisfies_

\[r_{t}(\alpha)\in\operatorname*{arg\,min}_{r\in[0,R]}\left[\frac{\lambda_{t}(t- 1)}{1-\lambda_{t}}\cdot\psi(r)+\sum_{i=1}^{t-1}l_{\alpha}(r,r_{i}^{*})\right], \quad\forall\alpha\in[0,1],t\geq 2.\] (4)

_Specifically, (i) \(\psi\) is strongly convex with coefficient \(\inf_{r\in[0,R]}p_{0}(r)\); and (ii) if \(P_{0}\) is the uniform distribution on \([0,R]\), then \(\psi\) is the quadratic function,_

\[\psi(r)=\frac{1}{2R}r^{2}-(1-\alpha)r+\frac{1}{2}(1-\alpha)R.\]

Proof of Theorem 1.: We first rewrite the base regularizer \(\psi\) as

\[\psi(r) =\int_{0}^{R}l_{\alpha}(r,r^{*})p_{0}(r^{*})dr^{*}\] \[=\alpha\int_{0}^{r}(r-r^{*})p_{0}(r^{*})dr^{*}+(1-\alpha)\int_{r} ^{R}(r^{*}-r)p_{0}(r^{*})dr^{*}.\]

It is twice-differentiable, with

\[\psi^{\prime}(r)=\alpha\int_{0}^{r}p_{0}(r^{*})dr^{*}-(1-\alpha)\int_{r}^{R}p_ {0}(r^{*})dr^{*}=\int_{0}^{r}p_{0}(r^{*})dr^{*}-(1-\alpha),\]

and \(\psi^{\prime\prime}(r)=p_{0}(r)\). The strong convexity statement on \(\psi\) is thus clear. If \(P_{0}\) is uniform, we have

\[\psi(r) =R^{-1}\left[\alpha\int_{0}^{r}(r-r^{*})dr^{*}+(1-\alpha)\int_{r }^{R}(r^{*}-r)dr^{*}\right]\] \[=\frac{1}{2R}\left[\alpha r^{2}+(1-\alpha)(R-r)^{2}\right]=\frac {1}{2R}r^{2}-(1-\alpha)r+\frac{1}{2}(1-\alpha)R.\]

Next, consider the first part of the theorem. Algorithm 1 outputs

\[r_{t}(\alpha) =q_{1-\alpha}\left[\lambda_{t}P_{0}+(1-\lambda_{t})\cdot\bar{P} (r_{1:t-1}^{*})\right]\] \[=\min\left\{x;\lambda_{t}\int_{0}^{x}p_{0}(r)dr+\frac{1-\lambda_ {t}}{t-1}\sum_{i=1}^{t-1}\mathbf{1}[r_{i}^{*}\leq x]\geq 1-\alpha\right\}.\] (5)

On the other hand, consider the optimization objective in Eq.(4), scaled by \((1-\lambda_{t})/(t-1)\); it can be written as

\[\gamma(x):=\lambda_{t}\psi(x)+\frac{1-\lambda_{t}}{t-1}\sum_{i=1}^{t-1}l_{ \alpha}(x,r_{i}^{*}).\]

Notice that the function \(\gamma\) is continuous and right-differentiable. Taking its right-derivative, we have

\[\gamma_{+}^{\prime}(x) =\lambda_{t}\left[\int_{0}^{x}p_{0}(r^{*})dr^{*}-(1-\alpha) \right]+\frac{1-\lambda_{t}}{t-1}\left[(\alpha-1)\sum_{i=1}^{t-1}\mathbf{1}[x <r_{i}^{*}]+\alpha\sum_{i=1}^{t-1}\mathbf{1}[x\geq r_{i}^{*}]\right]\] \[=\lambda_{t}\int_{0}^{x}p_{0}(r^{*})dr^{*}+\lambda_{t}(\alpha-1) +\frac{1-\lambda_{t}}{t-1}(\alpha-1)(t-1)+\frac{1-\lambda_{t}}{t-1}\sum_{i=1}^ {t-1}\mathbf{1}[x\geq r_{i}^{*}]\] \[=\lambda_{t}\int_{0}^{x}p_{0}(r^{*})dr^{*}+\frac{1-\lambda_{t}}{t- 1}\sum_{i=1}^{t-1}\mathbf{1}[x\geq r_{i}^{*}]+\alpha-1.\]

Comparing it to Eq.(5), we see that the output \(r_{t}(\alpha)\) of Algorithm 1 satisfies

\[r_{t}(\alpha)=\min\{s;\gamma_{+}^{\prime}(x)\geq 0\}.\]

Therefore it also satisfies the FTRL update, Eq.(4).

**Theorem 2**.: _Let \(P_{0}\) be the uniform distribution on \([0,R]\). With the step size \(\lambda_{t}=1/\sqrt{t}\), the output of Algorithm 1 against any \(r_{1:T}^{*}\) sequence satisfies_

\[\sum_{t=1}^{T}l_{\alpha}(r_{t}(\alpha),r_{t}^{*})-\sum_{t=1}^{T}l_{\alpha}(q_{1 -\alpha}(r_{1:T}^{*}),r_{t}^{*})=O(R\sqrt{T}),\quad\forall\alpha\in[0,1],\]

_where \(q_{1-\alpha}(r_{1:T}^{*})\) denotes the \((1-\alpha)\)-quantile of the hindsight empirical distribution \(\bar{P}(r_{1:T}^{*})\), and \(O(\cdot)\) subsumes absolute constants._

Proof of Theorem 2.: Starting from Eq.(4), we first verify that the regularizer weight \(\frac{\lambda_{t}(t-1)}{1-\lambda_{t}}\) is increasing with respect to \(t\) (when \(t>1\)), so that the classical FTRL regret bound can be applied. To this end, define

\[h(t):=\frac{\lambda_{t}(t-1)}{1-\lambda_{t}}=\frac{t-1}{\sqrt{t}-1}.\]

Taking the derivative, for all \(t>1\),

\[h^{\prime}(t)=\frac{\sqrt{t}-1-\frac{t-1}{2\sqrt{t}}}{(\sqrt{t}-1)^{2}}=\frac {t-2\sqrt{t}+1}{2\sqrt{t}(\sqrt{t}+1-1)^{2}}=\frac{(\sqrt{t}-1)^{2}}{2\sqrt{t} (\sqrt{t}+1-1)^{2}}\geq 0.\]

Now, since the regularizer weight is increasing and the base regularizer \(\psi\) corresponding to the uniform prior is \(R^{-1}\)-strongly convex, we can apply the strong-convexity-based FTRL regret bound [13, Corollary 7.9] starting from \(t=2\) (and implicitly, \(T\geq 2\)). This yields

\[\sum_{t=2}^{T}l_{\alpha}(r_{t}(\alpha),r_{t}^{*})-\sum_{t=2}^{T}l _{\alpha}(q_{1-\alpha}(r_{1:T}^{*}),r_{t}^{*})\leq\frac{\lambda_{T}(T-1)}{1- \lambda_{T}}\left[\max_{r\in[0,R]}\psi(r)-\min_{r\in[0,R]}\psi(r)\right]\] \[+\frac{R}{2}\sum_{t=2}^{T}\frac{1-\lambda_{t}}{\lambda_{t}(t-1)}g _{t}^{2},\]

where \(g_{t}\) is defined as

\[g_{t}=\begin{cases}\alpha,&r_{t}(\alpha)>r_{t}^{*},\\ 1-\alpha,&r_{t}(\alpha)<r_{t}^{*},\\ 0,&r_{t}(\alpha)=r_{t}^{*}.\end{cases}\]

In all cases we have \(g_{t}^{2}\leq 1\). Furthermore, \(\min_{r\in[0,R]}\psi(r)=\frac{1}{2}\alpha(1-\alpha)R\geq 0\), \(\max_{r\in[0,R]}\psi(r)=\frac{R}{2}\max\{\alpha,1-\alpha\}\leq R/2\). Therefore, plugging in \(\lambda_{t}=1/\sqrt{t}\) we have

\[\sum_{t=2}^{T}l_{\alpha}(r_{t}(\alpha),r_{t}^{*})-\sum_{t=2}^{T}l _{\alpha}(q_{1-\alpha}(r_{1:T}^{*}),r_{t}^{*}) \leq\frac{R}{2}\left[\frac{\lambda_{T}(T-1)}{1-\lambda_{T}}+\sum_ {t=2}^{T}\frac{1-\lambda_{t}}{\lambda_{t}(t-1)}\right]\] \[\leq\frac{R}{2}\left[4\sqrt{T}+\sum_{t=1}^{T-1}\frac{\sqrt{t+1}} {t}\right]=O(R\sqrt{T}).\]

Adding the instantaneous regret from the first round only results in an additional \(R\) on the total regret bound.