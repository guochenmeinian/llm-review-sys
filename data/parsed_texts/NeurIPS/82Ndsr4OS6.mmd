# Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning

 Honghao Wei

Washington State University

honghao.wei@wsu.edu

&Xiyue Peng

ShanghaiTech University

pengxy2024@shanghaitech.edu.cn

&Arnob Ghosh

New Jersey Institute of Technology

arnob.ghosh@njit.edu

&Xin Liu

ShanghaiTech University

liuxin7@shanghaitech.edu.cn

###### Abstract

We propose WSAC (Weighted Safe Actor-Critic), a novel algorithm for Safe Offline Reinforcement Learning (RL) under functional approximation, which can robustly optimize policies to improve upon an arbitrary reference policy with limited data coverage. WSAC is designed as a two-player Stackelberg game to optimize a refined objective function. The actor optimizes the policy against two adversarially trained value critics with small importance-weighted Bellman errors, which focus on scenarios where the actor's performance is inferior to the reference policy. In theory, we demonstrate that when the actor employs a no-regret optimization oracle, WSAC achieves a number of guarantees: \((i)\) For the first time in the safe offline RL setting, we establish that WSAC can produce a policy that outperforms **any** reference policy while maintaining the same level of safety, which is critical to designing a safe algorithm for offline RL. \((ii)\) WSAC achieves the optimal statistical convergence rate of \(1/\sqrt{N}\) to the reference policy, where \(N\) is the size of the offline dataset. \((iii)\) We theoretically show that WSAC guarantees a safe policy improvement across a broad range of hyperparameters that control the degree of pessimism, indicating its practical robustness. Additionally, we offer a practical version of WSAC and compare it with existing state-of-the-art safe offline RL algorithms in several continuous control environments. WSAC outperforms all baselines across a range of tasks, supporting the theoretical results.

## 1 Introduction

Online safe reinforcement learning (RL) has found successful applications in various domains, such as autonomous driving (Isele et al., 2018), recommender systems (Chow et al., 2017), and robotics (Achiam et al., 2017). It enables the learning of safe policies effectively while satisfying certain safety constraints, including collision avoidance, budget adherence, and reliability. However, collecting diverse interaction data can be extremely costly and infeasible in many real-world applications, and this challenge becomes even more critical in scenarios where risky behavior cannot be tolerated. Given the inherently risk-sensitive nature of these safety-related tasks, data collection becomes feasible only when employing behavior policies satisfies all the safety requirements.

To overcome the limitations imposed by interactive data collection, offline RL algorithms are designed to learn a policy from an available dataset collected from historical experiences by some behavior policy, which may differ from the policy we aim to learn. A desirable property of an effective offline algorithm is the assurance of robust policy improvement (RPI), which guarantees that a learned policy is always at least as good as the baseline behavior policies (Fujimoto et al., 2019; Laroche et al., 2019;Kumar et al., 2019; Siegel et al., 2020; Chen et al., 2022a; Zhu et al., 2023; Bhardwaj et al., 2024). We extend the property of RPI to offline safe RL called safe robust policy improvement (SRPI), which indicates the improvement should be _safe_ as well. This is particularly important in offline safe RL. For example, in autonomous driving, an expert human driver operates the vehicle to collect a diverse dataset under various road and weather conditions, serving as the behavior policy. This policy is considered both effective and safe, as it demonstrates proficient human driving behavior while adhering to all traffic laws and other safety constraints. Achieving a policy that upholds the SRPI characteristic with such a dataset can significantly mitigate the likelihood of potential collisions and other safety concerns.

In offline RL, we represent the state-action occupancy distribution of policy \(\pi\) over the dataset distribution \(\mu\) using the ratio \(w^{\pi}=d^{\pi}/\mu\). A commonly required assumption is that the \(\ell_{\infty}\) concentrability \(C^{\pi}_{\ell_{\infty}}\) is bounded, which is defined as the infinite norm of \(w^{\pi}\) for **all** policies (Liu et al., 2019; Chen and Jiang, 2019; Wang et al., 2019; Liao et al., 2022; Zhang et al., 2020). A stronger assumption requires a uniform lower bound on \(\mu(a|s)\)(Xie and Jiang, 2021). However, such all-policy concentrability assumptions are difficult to satisfy in practice, particularly for offline safe RL, as they essentially require the offline dataset to have good coverage of **all** unsafe state-action pairs. To address the full coverage requirement, other works (Rashidinejad et al., 2021; Zhan et al., 2022; Chen and Jiang, 2022; Xie et al., 2021; Uehara and Sun, 2021) adapt conservative algorithms by employing the principle of pessimism in the face of uncertainty, reducing the assumption to the best covered policy (or optimal policy) concerning \(\ell_{\infty}\) concentrability. Zhu et al. (2023) introduce \(\ell_{2}\) concentrability to further relax the assumption, indicating that \(\ell_{\infty}\) concentrability is always an upper bound of \(\ell_{2}\) concentrability (see Table 1 for detailed comparisons with previous works). While provable guarantees are obtained using single policy concentrability for unconstrained MDP as Table 1 suggests for the safe RL setting, all the existing studies (Hong et al., 2024; Le et al., 2019)_still_ require the coverage on **all** the policies. Further, as Table 1 suggests, the above papers do not guarantee robust safe policy improvement. Our main contributions are summarized below:

1. We prove that our algorithm, which uses average Bellman error, enjoys an optimal statistical rate of \(1/\sqrt{N}\) under partial data coverage assumption. _This is the first work that achieves such a result using only single-policy \(\ell_{2}\) concentrability_.
2. We propose a novel offline safe RL algorithm, called Weighted Safe Actor-Critic (WSAC), which can robustly learn policies that improve upon any behavior policy with controlled relative pessimism. We prove that under standard function approximation assumptions, when the actor incorporates a no-regret policy optimization oracle, WSAC outputs a policy that never degrades the performance of a reference policy (including the behavior policy) for a range of hyperparameters (defined later). _This is the first work that provably demonstrates the property of SRPI in offline safe RL setting_.
3. We point out that primal-dual-based approaches Hong et al. (2024) may require **all**-policy concentrability assumption. Thus, unlike, the primal-dual-based appraoch, we propose a novel rectified penalty-based approach to obtain results using **single-policy** concentrability. Thus, we need novel analysis techniques to prove results.
4. Furthermore, we provide a practical implementation of WSAC following a two-timescale actor-critic framework using adversarial frameworks similar to Cheng et al. (2022); Zhu et al. (2023), and test it on several continuous control environments in the offline safe RL benchmark (Liu et al., 2023). WSAC outperforms all other state-of-the-art baselines, validating the property of a safe policy improvement.

Figure 1: Comparison between WSAC and the behavior policy in the tabular case. The behavior policy is a mixture of the optimal policy and a random policy, with the mixture percentage representing the proportion of the optimal policy. The cost threshold is set to 0.1. We observe that WSAC consistently ensures a safely improved policy across various scenarios, even when the behavior policy is not safe.

## 2 Related Work

**Offline safe RL:** Deep offline safe RL algorithms (Lee et al., 2022; Liu et al., 2023; Xu et al., 2022; Chen et al., 2021; Zheng et al., 2024) have shown strong empirical performance but lack theoretical guarantees. To the best of our knowledge, the investigation of policy improvement properties in offline safe RL is relatively rare in the state-of-the-art offline RL literature. Wu et al. (2021) focus on the offline constrained multi-objective Markov Decision Process (CMOMDP) and demonstrate that an optimal policy can be learned when there is sufficient data coverage. However, although they show that CMDP problems can be formulated as CMOMDP problems, they assume a linear kernel CMOMDP in their paper, whereas our consideration extends to a more general function approximation setting. Le et al. (2019) propose a model-based primal-dual-type algorithm with deviation control for offline safe RL in the tabular setting. With prior knowledge of the slackness in Slater's condition and a constant on the concentrability coefficient, an \((\epsilon,\delta)\)-PAC error is achievable when the number of data samples \(N\) is large enough \((N=\tilde{\mathcal{O}}(1/\epsilon^{2}))\). These assumptions make the algorithm impractical, and their computational complexity is much higher than ours. Additionally, we consider a more practical, model-free function approximation setting. In another concurrent work (Hong et al., 2024), a primal-dual critic algorithm is proposed for offline-constrained RL settings with general function approximation. However, their algorithm requires \(\ell_{2}\) concentrability for **all** policies, which is not practical as discussed. The reason is that the dual variable optimization in their primal-dual design requires an accurate estimation of all the policies used in each episode, which necessitates coverage over all policies. Moreover, they cannot guarantee the property of SRPI. Moreover, their algorithm requires an additional offline policy evaluation (OPE) oracle for policy evaluation, making the algorithm less efficient.

## 3 Preliminaries

### Constrained Markov Decision Process

We consider a Constrained Markov Decision Process (CMDP) \(\mathcal{M}\), denoted by \((\mathcal{S},\mathcal{A},\mathcal{P},R,C,\gamma,\rho)\). \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition kernel, where \(\Delta(\cdot)\) is a probability simplex, \(R:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the reward function, \(C:\mathcal{S}\times\mathcal{A}\rightarrow[-1,1]\) is the cost function, \(\gamma\in[0,1)\) is the discount factor and \(\rho:\mathcal{S}\rightarrow[0,1]\) is the initial state distribution. We assume \(\mathcal{A}\) is finite while allowing \(\mathcal{S}\) to be arbitrarily complex. We use \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) to denote a stationary policy, which specifies a distribution over actions for each state. At each time, the agentobserves a state \(s_{t}\in\mathcal{S}\), takes an action \(a_{t}\in\mathcal{S}\) according to a policy \(\pi,\) receives a reward \(r_{t}\) and a cost \(c_{t}\), where \(r_{t}=R(s_{t},a_{t}),c_{t}=C(s_{t},a_{t})\). Then the CMDP moves to the next state \(s_{t+1}\) based on the transition kernel \(\mathcal{P}(\cdot|s_{t},a_{t})\). Given a policy \(\pi,\) we use \(V_{r}^{\pi}(s)\) and \(V_{c}^{\pi}(s)\) to denote the expected discounted return and the expected cumulative discounted cost of \(\pi\), starting from state \(s\), respectively.

\[V_{r}^{\pi}(s):= \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}|s_{0}=s,a_{t}\sim \pi(\cdot|s_{t})] \tag{1}\] \[V_{c}^{\pi}(s):= \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}c_{t}|s_{0}=s,a_{t}\sim \pi(\cdot|s_{t})]. \tag{2}\]

Accordingly, we also define the \(Q-\)value function of a policy \(\pi\) for the reward and cost as

\[Q_{r}^{\pi}(s,a):= \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}|(s_{0},a_{0})=(s,a), a_{t}\sim\pi(\cdot|s_{t})] \tag{3}\] \[Q_{c}^{\pi}(s,a):= \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}c_{t}|(s_{0},a_{0})=(s,a), a_{t}\sim\pi(\cdot|s_{t})], \tag{4}\]

respectively. As rewards and costs are bounded, we have that \(0\leq Q_{r}^{\pi}\leq\frac{1}{1-\gamma}\), and \(-\frac{1}{1-\gamma}\leq Q_{c}^{\pi}\leq\frac{1}{1-\gamma}\). We let \(V_{\max}=\frac{1}{1-\gamma}\) to simplify the notation. We further write

\[J_{r}(\pi):=(1-\gamma)\mathbb{E}_{s\sim\rho}[V_{r}^{\pi}(s)],\quad J_{c}(\pi): =(1-\gamma)\mathbb{E}_{s\sim\rho}[V_{c}^{\pi}(s)]\]

to represent the normalized average reward/cost value of policy \(\pi\). In addition, we use \(d^{\pi}(s,a)\) to denote the normalized and discounted state-action occupancy measure of the policy \(\pi:\)

\[d^{\pi}(s,a):=(1-\gamma)\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}\mathds{1}(s_ {t}=s,a_{t}=a)|a_{t}\sim\pi(\cdot|s_{t})],\]

where \(\mathds{1}(\cdot)\) is the indicator function. We also use \(d^{\pi}(s)=\sum_{a\in\mathcal{A}}d^{\pi}(s,a)\) to denote the discounted state occupancy and we use \(\mathbb{E}_{\pi}\) as a shorthand of \(\mathbb{E}_{(s,a)\sim d^{\pi}}[\cdot]\) or \(\mathbb{E}_{s\sim d^{\pi}}[\cdot]\)to denote the expectation with respect to \(d^{\pi}\). Thus The objective in safe RL for an agent is to find a policy such that

\[\pi\in\arg\max\ J_{r}(\pi)\qquad\text{s.t.}\ J_{c}(\pi)\leq 0. \tag{5}\]

_Remark 3.1_.: For ease of exposition, this paper exclusively focuses on a single constraint. However, it is readily extendable to accommodate multiple constraints.

### Function Approximation

In complex environments, the state space \(\mathcal{S}\) is usually very large or even infinite. We assume access to a policy class \(\Pi\subseteq(\mathcal{S}\rightarrow\Delta(\mathcal{A}))\) consisting of all candidate policies from which we can search for good policies. We also assume access to a value function class \(\mathcal{F}\subseteq(\mathcal{S}\times\mathcal{A}\rightarrow[0,V_{\max}])\) to model the reward \(Q-\)functions, and \(\mathcal{G}\subseteq(\mathcal{S}\times\mathcal{A}\rightarrow[-V_{\max},V_{ \max}])\) to model the cost \(Q-\)functions of candidate policies. We further assume access to a function class \(\mathcal{W}\in\{w:\mathcal{S}\times\mathcal{A}\rightarrow[0,B_{w}]\}\) that represents marginalized importance weights with respect to the offline data distribution. Without loss of generality, we assume that the all-one function is contained in \(\mathcal{W}\).

For a given policy \(\pi\in\Pi\), we denote \(f(s^{\prime},\pi):=\sum_{a^{\prime}}\pi(a^{\prime}|s^{\prime})f(s^{\prime},a ^{\prime})\) for any \(s\in\mathcal{S}\). The Bellman operator \(\mathcal{T}_{r}^{\pi}:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\rightarrow \mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) for the reward is defined as

\[(\mathcal{T}_{r}^{\pi}f)(s,a):=R(s,a)+\gamma\mathbb{E}_{\mathcal{P}(s^{ \prime}|s,a)}[f(s^{\prime},\pi)],\]

The Bellman operator \(\mathcal{T}_{c}^{\pi}:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\rightarrow \mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) for the cost is

\[(\mathcal{T}_{c}^{\pi}f)(s,a):=C(s,a)+\gamma\mathbb{E}_{\mathcal{P}(s^{ \prime}|s,a)}[f(s^{\prime},\pi)].\]

Let \(\|\cdot\|_{2,\mu}:=\sqrt{\mathbb{E}_{\mu}[(\cdot)^{2}]}\) denote the Euclidean norm weighted by distribution \(\mu\). We make the following standard assumptions in offline RL setting (Xie et al., 2021; Cheng et al., 2022; Zhu et al., 2023) on the representation power of the function classes:

**Assumption 3.2** (Approximate Realizability).: Assume there exists \(\epsilon_{1}\geq 0,\) s.t. for any given policy \(\pi\in\Pi,\) we have \(\min_{f\in\mathcal{F}}\max_{\text{admissible}\,\nu}\|f-T^{\pi}_{r}f\|_{2,\nu}^{2} \leq\epsilon_{1},\) and \(\min_{f\in\mathcal{F}}\max_{\text{admissible}\,\nu}\|f-T^{\pi}_{c}f\|_{2,\nu}^{2} \leq\epsilon_{1},\) where \(\nu\) is the state-action distribution of any admissible policy such that \(\nu\in\{d^{\pi},\forall\pi\in\Pi\}.\)

Assumption 3.2 assumes that for any policy \(\pi\in\Pi,\)\(Q^{\pi}_{r}\) and \(Q^{\pi}_{c}\) are approximately realizable in \(\mathcal{F}\) and \(\mathcal{G}\). When \(\epsilon_{1}\) is small for all admissible \(\nu,\) we have \(f_{r}\approx Q^{\pi}_{r},\) and \(f_{c}\approx Q^{\pi}_{c}.\) In particular, when \(\epsilon_{1}=0,\) we have \(Q^{\pi}_{r}\in\mathcal{F},Q^{\pi}_{c}\in\mathcal{F}\) for any policy \(\pi\in\Pi.\) Note that we do not need Bellman completeness assumption Cheng et al. (2022).

### Offline RL

In offline RL, we assume that the available offline data \(\mathcal{D}=\{(s_{i},a_{i},r_{i},c_{i},s^{\prime}_{i})\}_{i=1}^{N}\) consists of \(N\) samples. Samples are i.i.d. (which are common assumptions in unconstrained Cheng et al. (2022), as well as constrained setting Hong et al. (2024)), and the distribution of each tuple \((s,a,r,c,s^{\prime})\) is specified by a distribution \(\mu\in\Delta(\mathcal{S}\times\mathcal{A})\), which is also the discounted visitation probability of a behavior policy (also denoted by \(\mu\) for simplicity). In particular, \((s,a)\sim\mu,r=R(s,a),c=C(s,a),s^{\prime}\sim\mathcal{P}(\cdot|s,a).\) We use \(a\sim\mu(\cdot|s),\) to denote that the action is drawn using the behavior policy and \((s,a,s^{\prime})\sim\mu\) to denote that \((s,a)\sim\mu,\) and \(s^{\prime}\sim\mathcal{P}(\cdot|s,a).\)

For a given policy \(\pi,\) we define the marginalized importance weights \(w^{\pi}(s,a):=\frac{d^{\pi}(s,a)}{\mu(s,a)}\) which is the ratio between the discounted state-action occupancy of \(\pi\) and the data distribution \(\mu.\) This ratio can be used to measure the concentrability of the data coverage (Xie and Jiang, 2020; Zhan et al., 2022; Rashidinejad et al., 2022; Ozdaglar et al., 2023; Lee et al., 2021).

In this paper we study offline RL with access to a dataset with limited coverage. The coverage of a policy \(\pi\) is the dataset can be measured by the weighted \(\ell_{2}\) single policy concentrability coefficient (Zhu et al., 2023; Yin and Wang, 2021; Uehara et al., 2024; Hong et al., 2024):

**Definition 3.3** (\(\ell_{2}\) Concentrability).: Given a policy \(\pi,\) define \(C^{\pi}_{\ell_{2}}=\|w^{\pi}\|_{2,\mu}=\|d^{\pi}/\mu\|_{2,\mu}.\)

_Remark 3.4_.: The definition here is much weaker than the **all policy** concentrability used in offline RL (Chen and Jiang, 2019) and safe offline RL (Le et al., 2019; Hong et al., 2024), which requires the ratio \(\frac{d^{\pi}(s,a)}{\mu(s,a)}\) to be bounded for all \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\) and **all** policies \(\pi.\) In particular, the all-policy concentrability assumption essentially requires the dataset to have full coverage of all policies ((nearly all the state action pairs). This requirement is often violated in practical scenarios. This requirement is even impossible to meet in safe offline RL because it would require collecting data from **every** dangerous state and actions, which clearly is impractical.

In the following lemma, we compare two variants of single-policy concentrability definition with the \(\ell_{2}\) defined in Definition 3.3.

**Lemma 1** (Restate Proposition \(2.1\) in Zhu et al. (2023)).: _Define the \(\ell_{\infty}\) single policy concentrability (Rashidinejad et al., 2021) as \(C^{\pi}_{\ell_{\infty}}=\|d^{\pi}/\mu\|_{\infty}\) and the Bellman-consistent single-policy concentrability (Xie et al., 2021) as \(C^{\pi}_{Bellman}=\max_{f\in\mathcal{F}}\frac{\|f-\mathcal{T}^{\pi}f\|_{2,a^{ \pi}}^{2}}{\|f-\mathcal{T}^{\pi}f\|_{2,\mu}^{2}}\) (\(\mathcal{T}\) could be \(\mathcal{T}_{r}\) or \(\mathcal{T}_{c}\) in our setting) Then, it always holds \((C^{\pi}_{\ell_{2}})^{2}\leq C^{\pi}_{\ell_{\infty}},C^{\pi}_{\ell_{2}}\leq C^ {\pi}_{\ell_{\infty}}\) and there exist offline RL instances where \((C^{\pi}_{\ell_{2}})^{2}\leq C^{\pi}_{Bellman},C^{\pi}_{\ell_{2}}\leq C^{\pi }_{Bellman}.\)_

_Remark 3.5_.: It is easy to observe that the \(\ell_{2}\) variant is bounded by \(\ell_{\infty}\) and \(C^{\pi}_{Bellman}\) under some cases. There is an example (Example 1) in Zhu et al. (2023) showing that \(C^{\pi}_{\ell_{2}}\) is bounded by a constant \(\sqrt{2}\) while \(C^{\pi}_{\ell_{\infty}}\) could be arbitrarily large. For the case when the function class \(\mathcal{F}\) is highly expressive, \(C^{\pi}_{Bellman}\) could be close to \(C^{\pi}_{\ell_{\infty}}\) and thus possibly larger than \(C^{\pi}_{\ell_{2}}.\) Intuitively, \(C^{\pi}_{\ell_{2}}\) implies that only \(\mathbb{E}_{d^{\pi}}[w^{\pi}(s,a)]\) is bounded, rather, \(w^{\pi}(s,a)\) is bounded for all \((s,a)\) in \(\ell_{\infty}\) concentrability bound.

Given the definition of the concentrability, we make the following assumption on the weight function class \(\mathcal{W}\) and a single-policy realizability:

**Assumption 3.6** (Boundedness of \(\mathcal{W}\) in \(\ell_{2}\) norm).: For all \(w\in\mathcal{W}\), assume that \(\|w\|_{2,\mu}\leq C^{*}_{\ell_{2}}.\)

**Assumption 3.7** (Single-policy realizability of \(w^{\pi}\)).: For some policy \(\pi\) that we would like to compete with, assume that \(w^{\pi}\in\mathcal{W}.\)

In this paper, we want to study the robust policy improvement on any reference policy, then we assume that we are provided a reference policy \(\pi_{\text{ref}}.\) Note that in many applications (e.g., scheduling,networking) we indeed have a reference policy. We want that while applying a sophisticated RL policy it should do better and be safe as well. This is one of the main motivations behind this assumption.

**Assumption 3.8** (Reference Policy).: We assume access to a reference policy \(\pi_{\text{ref}}\in\Pi\), which can be queried at any state.

In many applications such as networking, scheduling, and control problems, there are existing good enough reference policies. In these cases, a robust and safe policy improvement over these reference policies has practical value. If \(\pi_{\text{ref}}\) is not provided, we can simply run a behavior cloning on the offline data to extract the behavior policy as \(\pi_{\text{ref}}\) accurately, as long as the size of the offline data set is large enough. More discussion can be found in Section C in the Appendix.

## 4 Actor-Critic with Importance Weighted Bellman Error

Our algorithm design builds upon the constrained actor-critic method, in which we iteratively optimize a policy and improve the policy based on the evaluation of reward and cost. Consider the following actor-critic approach for solving the optimization problem (5):

**Actor:**\(\hat{\pi}^{*}\in\arg\max_{\pi\in\Pi}f_{r}^{\pi}(s_{0},\pi)\quad s.t.\quad f_{c} ^{\pi}(s_{0},\pi)\leq 0\)

**Critic:**\(f_{r}^{\pi}\in\arg\min_{f\in\mathcal{F}}\mathbb{E}_{\mu}[((f-\mathcal{T}_{r}f) (s,a))^{2}],\quad f_{c}^{\pi}\in\arg\min_{f\in\mathcal{G}}\mathbb{E}_{\mu}[(( f-\mathcal{T}_{c}f)(s,a))^{2}],\)

where we assume that \(s_{0}\) is a fixed initial state, and \(f_{r}(s,\pi)=\sum_{a\in\mathcal{A}}\pi(a|s)f_{r}(s,a),f_{c}(s,\pi)=\sum_{a\in \mathcal{A}}\pi(a|s)f_{c}(s,a).\) The policy is optimized by maximizing the reward \(q\) function \(f_{r}\) while ensuring that \(f_{c}\) satisfies the constraint, and the two functions are trained by minimizing the Bellman error. However, this formulation has several disadvantages. \(1)\) It cannot handle insufficient data coverage, which may fail to provide an accurate estimation of the policy for unseen states and actions. \(2)\)It cannot guarantee robust policy improvement. \(3)\) The actor training step is computationally intractable especially when the policy space is extremely large.

To address the insufficient data coverage issue, as mentioned in Xie et al. (2021) the critic can include a Bellman-consistent pessimistic evaluation of \(\pi,\) which selects the most pessimistic function that approximately satisfies the Bellman equation, which is called absolute pessimism. Then later as indicated by Cheng et al. (2022), instead of using an absolute pessimism, a relative pessimism approach by considering competing to the behavior policy can obtain a robust improvement over the behavior policy. However, this kind of approach can only achieve a suboptimal statistical rate of \(N^{1/3},\) and fails to achieve the optimal statistical rate of \(1/\sqrt{N},\) then later a weighted average Bellman error (Uehara et al., 2020; Xie and Jiang, 2020; Zhu et al., 2023) could be treated as one possible solution for improving the order. We remark here that all the discussions here are for the traditional _unconstrained_ offline RL. Regarding safety, _no existing efficient algorithms in safe offline RL have theoretically demonstrated_ the property of robust policy improvement with optimal statistical rate.

**Can Primal-dual based approaches achieve result using only single policy coverability?**: The most commonly used approach for addressing safe RL problems is primal-dual optimization (Efroni et al., 2020; Altman, 1999). As shown in current offline safe RL literature (Hong et al., 2024; Le et al., 2019), the policy can be optimized by maximizing a new unconstrained "reward" \(Q-\) function \(f_{r}^{\pi}(s_{0},\pi)-\lambda f_{c}^{\pi}(s_{0},\pi)\) where \(\lambda\) is a dual variable. Then, the dual-variable can be tuned by taking gradient descent step. As we discussed in the introduction, all these require **all** policy concentrability which is not practical especially for safe RL. Important question is whether all policy concentrability assumption can be relaxed. Note that primal-dual algorithm relies on solving the min-max problem \(\min_{\lambda}\max_{\pi}f_{r}^{\pi}(s_{0},\pi)-\lambda f_{c}^{\pi}(s_{0},\pi)\). Recent result (Cui and Du, 2022) shows that single policy concentrability assumption is _not_ enough for offline min-max game. Hence, we _conjecture_ that using the primal-dual method we can not relax the all policy concentrability assumption. Intuitively, the primal-dual based method (Hong et al., 2024) rely on bounding the regret in dual domain \(\sum_{k}(\lambda_{k}-\lambda^{*})(f_{c}^{\pi_{k}}-0)\), hence, all the policies \(\{\pi_{k}\}_{k=1}^{K}\) encountered throughout the iteration must be supported by the dataset to evaluate the dual value \(\lambda^{*}(f_{c}^{\pi_{k}}-0)\) where \(\lambda^{*}\) is the optimal dual value.

**Our novelty**: In contrast, we propose an aggression-limited objective function \(f_{r}(s_{0},\pi)-\lambda\cdot[f_{c}(s_{0},\pi)]_{+}\) to control aggressive policies, where \(\{\cdot\}_{+}:=\max\{\cdot,0\}.\) The high-level intuition behind this aggression-limited objective function is that by appropriately selecting a \(\lambda\) (usually large enough), we penalize all the policies that are not safe. As a result, the policy that maximizes the objective function is the optimal safe policy. This formulation is fundamentally different from the traditional primal-dual approach as it does not require dual-variable tuning, and thus, does not require all policy concentrability. In particular, we only need to bound the primal domain regret which can be done as long as the reference policy is covered by the dataset similar to the unconstrained setup.

Combining all the previous ideas together provides the design of our main algorithm named WSAC (**W**eighted **S**afe **A**ctor-**C**ritic). In Section 5, we will provide theoretical guarantees of WSAC and discuss its advantages over existing approaches in offline safe RL. WSAC aims to solve the following optimization problem:

\[\begin{split}&\hat{\pi}^{*}\in\operatorname*{arg\,max}_{\pi\in\Pi} \mathcal{L}_{\mu}(\pi,f_{r}^{\pi})-\lambda\{\mathcal{L}_{\mu}(\pi,f_{c}^{\pi}) \}_{+}\\ & s.t.\ \ f_{r}^{\pi}\in\operatorname*{arg\,min}_{f_{r}\in\mathcal{F}} \mathcal{L}_{\mu}(\pi,f_{r})+\beta\mathcal{E}_{\mu}(\pi,f_{r}),\quad f_{c}^{ \pi}\in\operatorname*{arg\,min}_{f_{c}\in\mathcal{G}}\ -\lambda\mathcal{L}_{\mu}( \pi,f_{c})+\beta\hat{\mathcal{E}}_{\mu}(\pi,f_{c}),\end{split} \tag{6}\]

where \(\mathcal{L}_{\mu}(\pi,f):=\mathbb{E}_{\mu}[f(s,\pi)-f(s,a)]\), and \(\mathcal{E}_{\mu}(\pi,f):=\max_{w\in\mathcal{W}}|\mathbb{E}_{\mu}[w(s,a)((f- T_{r}^{\pi}f)(s,a))]|,\hat{\mathcal{E}}_{\mu}(\pi,f):=\max_{w\in\mathcal{W}}|\mathbb{E}_{ \mu}[w(s,a)((f-T_{c}^{\pi}f)(s,a))]|\). This formulation can also be treated as a Stackelberg game (Von Stackelberg, 2010) or bilevel optimization problem. We penalize the objective function only when the approximate cost \(Q\)-function \(f_{c}^{\pi}\) of the policy \(\pi\) is more perilous than the behavior policy (\(f_{c}^{\pi}(s,\pi)\geq f_{c}^{\pi}(s,a)\)) forcing our policy to be as safe as the behavior policy. Maximization over \(w\) in for training the two critics can ensure that the Bellman error is small when averaged over measure \(\mu\cdot w\) for any \(w\in\mathcal{W}\), which turns out to be sufficient to control the suboptimality of the learned policy.

In the following theorem, we show that the solution of the optimization problem (6) is not worse than the behavior policy \(\mu\) in both performance and safety for any \(\beta\geq 0,\lambda>0\) than the policy \(\mu\) under Assumption 3.2 with \(\epsilon_{1}=0\).

**Theorem 4.1**.: _Assume that Assumption 3.2 holds with \(\epsilon_{1}=0,\) and the behavior policy \(\mu\in\Pi,\) then for any \(\beta\geq 0,\lambda>0\) we have \(J_{r}(\hat{\pi}^{*})\geq J_{r}(\mu),\) and \(\{J_{c}(\hat{\pi}^{*})\}_{+}\leq\{J_{c}(\mu)\}_{+}+\frac{1}{\lambda}.\)_

The result in Theorem 4.1 shows that by selecting \(\lambda\) large enough, for any \(\beta\geq 0,\) the solution can achieve better performance than the behavior policy while maintaining safety that is arbitrarily close to that of the behavior policy. The Theorem verifies the design of our framework which has the potential to have a robust safe improvement.

In the next section, we will introduce our main algorithm WSAC and provide its theoretical guarantees.

## 5 Theoretical Analysis of WSAC

### Main Algorithm

In this section, we present the theoretical version of our new model-free offline safe RL algorithm WSAC. Since we only have access to a dataset \(\mathcal{D}\) instead of the data distribution. WSAC solves an empirical version of (6):

\[\begin{split}&\hat{\pi}\in\operatorname*{arg\,max}_{\pi\in\Pi} \mathcal{L}_{\mathcal{D}}(\pi,f_{r}^{\pi})-\lambda\{\mathcal{L}_{\mathcal{D} }(\pi,f_{c}^{\pi})\}_{+}\\ & s.t.\ \ f_{r}^{\pi}\in\operatorname*{arg\,min}_{f_{r}\in\mathcal{F}} \mathcal{L}_{\mathcal{D}}(\pi,f_{r})+\beta\mathcal{E}_{\mathcal{D}}(\pi,f_{r} ),\quad f_{c}^{\pi}\in\operatorname*{arg\,min}_{f_{c}\in\mathcal{G}}\ -\lambda \mathcal{L}_{\mathcal{D}}(\pi,f_{c})+\beta\hat{\mathcal{E}}_{\mathcal{D}}(\pi,f_ {c}),\end{split} \tag{7}\]

where

\[\begin{split}\mathcal{L}_{\mathcal{D}}(\pi,f):=& \mathbb{E}_{\mathcal{D}}[f(s,\pi)-f(s,a)]\\ \mathcal{E}_{\mathcal{D}}(\pi,f):=&\operatorname*{ arg\,max}_{w\in\mathcal{W}}|\mathbb{E}_{\mathcal{D}}[w(s,a)(f(s,a)-r-\gamma f(s^{ \prime},\pi))]|\\ \hat{\mathcal{E}}_{\mathcal{D}}(\pi,f):=&\max_{w\in \mathcal{W}}|\mathbb{E}_{\mathcal{D}}[w(s,a)(f(s,a)-c-\gamma f(s^{\prime},\pi ))]|.\end{split} \tag{8}\]

As shown in Algorithm 1, at each iteration, WSAC selects \(f_{r}^{k}\) maximally pessimistic and \(f_{c}^{k}\) maximally optimistic for the current policy \(\pi_{k}\) with a weighted regularization on the estimated Bellman error for reward and cost, respectively (Line \(4\) and \(6\)) to address the worse cases within reasonable range. In order to achieve a safe robust policy improvement, the actor then applies a no-regret policy optimization oracle to update the policy \(\pi_{k+1}\) by optimizing the aggression-limited objective function compared with the reference policy (Line \(7\))\(f_{c}^{k}(s,\pi_{\text{ref}})\}_{+}\). Our algorithm is very computationally efficient and tractable compared with existing approaches (Hong et al., 2024; Le et al., 2019), since we do not need another inner loop for optimizing the dual variable with an additional online algorithm or offline policy evaluation oracle. The policy improvement process relies on a no-regret policy optimization oracle, a technique commonly employed in offline RL literature (Zhu et al., 2023; Cheng et al., 2022; Hong et al., 2024; Zhu et al., 2023). Extensive literature exists on such methodologies. For instance, approaches like soft policy iteration (Pirotta et al., 2013) and algorithms based on natural policy gradients (Kakade, 2001; Agarwal et al., 2021) can function as effective no-regret policy optimization oracles. We now formally define the oracle:

**Definition 5.1** (No-regret policy optimization oracle).: An algorithm **PO** is called a no-regret policy optimization oracle if for any sequence of functions \(f^{1},\ldots,f^{K}\) with \(f^{k}:\mathcal{S}\times\mathcal{A}\rightarrow[0,V_{\max}],\forall k\in[K]\). The policies \(\pi_{1},\ldots,\pi_{K}\) produced by the oracle **PO** satisfy that for any policy \(\pi\in\Pi:\)

\[\epsilon_{opt}^{\pi}\triangleq\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi}[f^{k} (s,\pi)-f^{k}(s,\pi_{k})]=o(1) \tag{9}\]

There indeed exist many methods that can serve as the no-regret oracle, for example, the mirror-descent approach (Geist et al., 2019) or the natural policy gradient approach (Kakade, 2001) of the form \(\pi_{k+1}(a|s)\propto\pi_{k}(a|s)\exp(\eta f^{k}(s,a))\) with \(\eta=\sqrt{\frac{\log|\mathcal{A}|}{2V_{\max}^{2}K}}\)(Even-Dar et al., 2009; Agarwal et al., 2021). In the following define \(\epsilon_{opt}^{\pi}\) as the error generated from the oracle **PO** by considering \(f_{r}^{k}(s,a)-\lambda\{f_{c}^{k}(s,a)-f_{c}^{k}(s,\pi)\}_{+}\) as the sequence of functions in Definition 5.1, then we have the following guarantee.

**Lemma 2**.: _Applying a no-regret oracle_ **PO** _for \(K\) episodes with \((f_{r}^{k}(s,a)-\lambda\{f_{c}^{k}(s,a)-f_{c}^{k}(s,\pi)\}_{+})\) for an arbitrary policy \(\pi\), can guarantee_

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi}[f_{r}^{k}(s,\pi)-f_{r}^ {k}(s,\pi_{k})]\leq\epsilon_{opt}^{\pi} \tag{10}\] \[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi}[\{f_{c}^{k}(s,\pi_{k})-f _{c}^{k}(s,\pi)\}_{+}]\leq\epsilon_{opt}^{\pi}+\frac{V_{\max}}{\lambda}. \tag{11}\]

Lemma 2 establishes that the policy outputted by **PO** with considering the aggression-limited "reward" can have a strong guarantee on the performance of both reward and cost when \(\lambda\) is large enough., which is comparable with any competitor policy. This requirement is critical to achieving the performance guarantee of our algorithm and the safe and robust policy improvement. The detailed proof is deferred to Appendix B.2 due to page limit.

### Theoretical Guarantees

We are now ready to provide the theoretical guarantees of WSAC Algorithm 1. The complete proof is deferred to Appendix B.3.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]

## References

* J. Achiam, D. Held, A. Tamar, and P. Abbeel (2017)Constrained policy optimization. In Int. Conf. Machine Learning (ICML), pp. 22-31. Cited by: SS1.
* A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan (2021)On the theory of policy gradient methods: optimality, approximation, and distribution shift. Journal of Machine Learning Research22 (98), pp. 1-76. Cited by: SS1.
* E. Altman (1999)Constrained markov decision processes. Vol. 7, CRC Press. Cited by: SS1.
* M. Bhardwaj, T. Xie, B. Boots, N. Jiang, and C. Cheng (2024)Adversarial model for offline reinforcement learning. Advances in Neural Information Processing Systems36. Cited by: SS1.
* F. Chen, J. Zhang, and Z. Wen (2022)A near-optimal primal-dual method for off-policy learning in cmdp. In Advances Neural Information Processing Systems (NeurIPS), Vol. 35, pp. 10521-10532. Cited by: SS1.
* J. Chen and N. Jiang (2019)Information-theoretic considerations in batch reinforcement learning. In Int. Conf. Machine Learning (ICML), pp. 1042-1051. Cited by: SS1.
* J. Chen and N. Jiang (2022)Offline reinforcement learning under value and density-ratio realizability: the power of gaps. In Uncertainty in Artificial Intelligence, pp. 378-388. Cited by: SS1.
* L. Chen, R. Jain, and H. Luo (2022b)Learning infinite-horizon average-reward markov decision process with constraints. In Int. Conf. Machine Learning (ICML), pp. 3246-3270. Cited by: SS1.
* L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch (2021)Decision transformer: reinforcement learning via sequence modeling. Advances in neural information processing systems34, pp. 15084-15097. Cited by: SS1.
* C. Cheng, A. Kolobov, and A. Agarwal (2020)Policy improvement via imitation of multiple oracles. In Advances Neural Information Processing Systems (NeurIPS), Vol. 33, pp. 5587-5598. Cited by: SS1.
* C. Cheng, T. Xie, N. Jiang, and A. Agarwal (2022)Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning, pp. 3852-3878. Cited by: SS1.
* Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone (2017)Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research18 (1), pp. 6070-6120. Cited by: SS1.
* Q. Cui and S. S. Du (2022)When are offline two-player zero-sum markov games solvable?. Advances in Neural Information Processing Systems35, pp. 25779-25791. Cited by: SS1.
* Y. Efroni, S. Mannor, and M. Pirotta (2020)Exploration-exploitation in constrained MDPs. arXiv preprint arXiv:2003.02189. Cited by: SS1.
* E. Even-Dar, S. M. Kakade, and Y. Mansour (2009)Online markov decision processes. Mathematics of Operations Research34 (3), pp. 726-736. Cited by: SS1.
* S. Fujimoto, D. Meger, and D. Precup (2019)Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pp. 2052-2062. Cited by: SS1.
* S. Fujimoto, H. van Hoof, and D. Meger (2018)Addressing function approximation error in actor-critic methods. In Int. Conf. Machine Learning (ICML), pp. 1582-1591. Cited by: SS1.
* M. Geist, B. Scherrer, and O. Pietquin (2019)A theory of regularized markov decision processes. In International Conference on Machine Learning, pp. 2160-2169. Cited by: SS1.
* T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine (2018)Soft Actor-Critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Int. Conf. Machine Learning (ICML), pp. 1861-1870. Cited by: SS1.

Hong, K., Li, Y., and Tewari, A. (2024). A primal-dual-critic algorithm for offline constrained reinforcement learning. In _Int. Conf. Artificial Intelligence and Statistics (AISTATS)_, pages 280-288. PMLR.
* Isele et al. (2018) Isele, D., Nakhaei, A., and Fujimura, K. (2018). Safe reinforcement learning on autonomous vehicles. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1-6. IEEE.
* Kakade and Langford (2002) Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In _Int. Conf. Machine Learning (ICML)_, pages 267-274.
* Kakade (2001) Kakade, S. M. (2001). A natural policy gradient. In _Advances Neural Information Processing Systems (NeurIPS)_.
* Kingma and Ba (2015) Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y., editors, _Int. Conf. on Learning Representations (ICLR)_.
* Kumar et al. (2019) Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019). Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32.
* Kumar et al. (2022) Kumar, A., Hong, J., Singh, A., and Levine, S. (2022). Should i run offline reinforcement learning or behavioral cloning? In _Int. Conf. on Learning Representations (ICLR)_.
* Laroche et al. (2019) Laroche, R., Trichelair, P., and Des Combes, R. T. (2019). Safe policy improvement with baseline bootstrapping. In _International conference on machine learning_, pages 3652-3661. PMLR.
* Le et al. (2019) Le, H., Voloshin, C., and Yue, Y. (2019). Batch policy learning under constraints. In _International Conference on Machine Learning_, pages 3703-3712. PMLR.
* Lee et al. (2021) Lee, J., Jeon, W., Lee, B., Pineau, J., and Kim, K.-E. (2021). Optidice: Offline policy optimization via stationary distribution correction estimation. In Meila, M. and Zhang, T., editors, _Int. Conf. Machine Learning (ICML)_, volume 139 of _Proceedings of Machine Learning Research_, pages 6120-6130. PMLR.
* Lee et al. (2022) Lee, J., Paduraru, C., Mankowitz, D. J., Heess, N., Precup, D., Kim, K.-E., and Guez, A. (2022). Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation. _arXiv preprint arXiv:2204.08957_.
* Liao et al. (2022) Liao, P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S. A. (2022). Batch policy learning in average reward markov decision processes. _Annals of statistics_, 50(6):3364.
* Liu et al. (2019) Liu, B., Cai, Q., Yang, Z., and Wang, Z. (2019). Neural trust region/proximal policy optimization attains globally optimal policy. _Advances in neural information processing systems_, 32.
* Liu et al. (2023a) Liu, Z., Guo, Z., Lin, H., Yao, Y., Zhu, J., Cen, Z., Hu, H., Yu, W., Zhang, T., Tan, J., et al. (2023a). Datasets and benchmarks for offline safe reinforcement learning. _arXiv preprint arXiv:2306.09303_.
* Liu et al. (2023b) Liu, Z., Guo, Z., Yao, Y., Cen, Z., Yu, W., Zhang, T., and Zhao, D. (2023b). Constrained decision transformer for offline safe reinforcement learning. In _International Conference on Machine Learning_, pages 21611-21630. PMLR.
* Ozdaglar et al. (2023) Ozdaglar, A. E., Pattathil, S., Zhang, J., and Zhang, K. (2023). Revisiting the linear-programming framework for offline rl with general function approximation. In _International Conference on Machine Learning_, pages 26769-26791. PMLR.
* Pirotta et al. (2013) Pirotta, M., Restelli, M., Pecorino, A., and Calandriello, D. (2013). Safe policy iteration. In _Int. Conf. Machine Learning (ICML)_, pages 307-315. PMLR.
* Rajaraman et al. (2020) Rajaraman, N., Yang, L., Jiao, J., and Ramchandran, K. (2020). Toward the fundamental limits of imitation learning. _Advances Neural Information Processing Systems (NeurIPS)_, 33:2914-2924.
* Rashidinejad et al. (2021) Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. In _Advances Neural Information Processing Systems (NeurIPS)_, volume 34, pages 11702-11716.
* Raghavan et al. (2019)Rashidinejad, P., Zhu, H., Yang, K., Russell, S., and Jiao, J. (2022). Optimal conservative offline rl with general function approximation via augmented lagrangian. _arXiv preprint arXiv:2211.00716_.
* Siegel et al. (2020) Siegel, N. Y., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020). Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. _arXiv preprint arXiv:2002.08396_.
* Stooke et al. (2020) Stooke, A., Achiam, J., and Abbeel, P. (2020). Responsive safety in reinforcement learning by pid lagrangian methods. In _Int. Conf. Machine Learning (ICML)_, pages 9133-9143. PMLR.
* Uehara et al. (2020) Uehara, M., Huang, J., and Jiang, N. (2020). Minimax weight and q-function learning for off-policy evaluation. In _International Conference on Machine Learning_, pages 9659-9668. PMLR.
* Uehara et al. (2024) Uehara, M., Kallus, N., Lee, J. D., and Sun, W. (2024). Offline minimax soft-q-learning under realizability and partial coverage. _Advances in Neural Information Processing Systems_, 36.
* Uehara and Sun (2021) Uehara, M. and Sun, W. (2021). Pessimistic model-based offline reinforcement learning under partial coverage. _arXiv preprint arXiv:2107.06226_.
* Von Stackelberg (2010) Von Stackelberg, H. (2010). _Market structure and equilibrium_. Springer Science & Business Media.
* Wang et al. (2019) Wang, L., Cai, Q., Yang, Z., and Wang, Z. (2019). Neural policy gradient methods: Global optimality and rates of convergence. _arXiv preprint arXiv:1909.01150_.
* Wu et al. (2021) Wu, R., Zhang, Y., Yang, Z., and Wang, Z. (2021). Offline constrained multi-objective reinforcement learning via pessimistic dual value iteration. In _Advances Neural Information Processing Systems (NeurIPS)_, volume 34, pages 25439-25451.
* Xie et al. (2021) Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694.
* Xie and Jiang (2020) Xie, T. and Jiang, N. (2020). Q* approximation schemes for batch reinforcement learning: A theoretical comparison. In _Conference on Uncertainty in Artificial Intelligence_, pages 550-559. PMLR.
* Xie and Jiang (2021) Xie, T. and Jiang, N. (2021). Batch value-function approximation with only realizability. In _Int. Conf. Machine Learning (ICML)_, pages 11404-11413. PMLR.
* Xu et al. (2022) Xu, H., Zhan, X., and Zhu, X. (2022). Constraints penalized q-learning for safe offline reinforcement learning. In _AAAI Conf. Artificial Intelligence_, volume 36, pages 8753-8760.
* Yin and Wang (2021) Yin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism. _Advances in neural information processing systems_, 34:4065-4078.
* Zhan et al. (2022) Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. (2022). Offline reinforcement learning with realizability and single-policy concentrability. In _Proc. Conf. Learning Theory (COLT)_, pages 2730-2775. PMLR.
* Zhang et al. (2020) Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. (2020). Variational policy gradient method for reinforcement learning with general utilities. _Advances in Neural Information Processing Systems_, 33:4572-4583.
* Zheng et al. (2024) Zheng, Y., Li, J., Yu, D., Yang, Y., Li, S. E., Zhan, X., and Liu, J. (2024). Safe offline reinforcement learning with feasibility-guided diffusion model. _arXiv preprint arXiv:2401.10700_.
* Zhu et al. (2023) Zhu, H., Rashidinejad, P., and Jiao, J. (2023). Importance weighted actor-critic for optimal conservative offline reinforcement learning. _arXiv preprint arXiv:2301.12714_.

**Supplementary Material**

## Appendix A Auxiliary Lemmas

In the following, we first provide several lemmas which are useful for proving our main results.

**Lemma 3**.: _With probability at least \(1-\delta\), for any \(f_{r}\in\mathcal{F},f_{c}\in\mathcal{G},\pi\in\Pi\) and \(w\in\mathcal{W},\) we have_

\[\left|\mathbb{E}_{\mu}[(f_{r}-\mathcal{T}_{r}^{\pi}f)w]|-\Big{|} \frac{1}{N}\sum_{(s,a,r,s^{\prime})}w(s,a)(f_{r}(s,a)-r-\gamma f_{r}(s^{\prime },\pi))\Big{|}\right|\] \[\leq \mathcal{O}\bigg{(}V_{\max}\sqrt{\frac{\log(|\mathcal{F}||\Pi|| \mathcal{W}|/\delta)}{N}}+\frac{V_{\max}B_{w}\log(|\mathcal{F}||\Pi||\mathcal{ W}|/\delta)}{N}\bigg{)} \tag{16}\] \[\left|\left|\mathbb{E}_{\mu}[(f_{c}-\mathcal{T}_{c}^{\pi}f)w]|- \Big{|}\frac{1}{N}\sum_{(s,a,r,s^{\prime})}w(s,a)(f_{c}(s,a)-c-\gamma f_{c}(s^ {\prime},\pi))\Big{|}\right|\right|\] \[\leq \mathcal{O}\bigg{(}V_{\max}\sqrt{\frac{\log(|\mathcal{G}||\Pi|| \mathcal{W}|/\delta)}{N}}+\frac{V_{\max}B_{w}\log(|\mathcal{G}||\Pi||\mathcal{ W}|/\delta)}{N}\bigg{)} \tag{17}\]

The proofs can be found in Lemma 4 in Zhu et al. (2023).

**Lemma 4**.: _With probability at least \(1-2\delta\), for any \(f_{r}\in\mathcal{F},f_{c}\in\mathcal{G}\) and \(\pi\in\Pi,\) we have_

\[|\mathcal{E}_{\mu}(\pi,f_{r})-\mathcal{E}_{\mathcal{D}}(\pi,f_{r} )|\leq\epsilon_{stat} \tag{18}\] \[|\mathcal{E}_{\mu}(\pi,f_{c})-\mathcal{E}_{\mathcal{D}}(\pi,f_{c} )|\leq\epsilon_{stat}, \tag{19}\]

_where \(\epsilon_{stat}:=V_{\max}C_{\ell_{2}}^{*}\sqrt{\frac{\log(|\mathcal{F}||\Pi|| \mathcal{W}|/\delta)}{N}}+\frac{V_{\max}B_{w}\log(|\mathcal{F}||\mathcal{G}|| \Pi||W|/\delta)}{N}\)._

Proof.: Condition on the high probability event in Lemma 3,for any \(f_{r}\in\mathcal{F},f_{c}\in\mathcal{G},\pi\in\Pi,\) define

\[w_{\pi,f}^{*}=\arg\max_{w\in\mathcal{W}}\mathcal{E}_{\mu}(\pi,f_{r})=\arg\max _{w\in\mathcal{W}}|\mathbb{E}_{\mu}[w(s,a)(f_{r}-\mathcal{T}_{r}^{\pi}f_{r})( s,a)]|\]

and define

\[\hat{w}_{\pi,f_{r}}=\arg\max_{w\in\mathcal{W}}\mathcal{E}_{\mathcal{D}}(\pi,f _{r})=\arg\max_{w\in\mathcal{W}}|\frac{1}{N}\sum_{(s,a,r,s^{\prime})\in \mathcal{D}}w(s,a)(f_{r}(s,a)-r-\gamma f_{r}(s^{\prime},\pi))|.\]

Then we can have

\[\mathcal{T}_{\mu}(\pi,f_{r})-\mathcal{E}_{\mathcal{D}}(\pi,f_{r})\] \[= |\mathbb{E}_{\mu}[w_{\pi,f_{r}}^{*}(s,a)(f_{r}-\mathcal{T}_{r}^{ \pi}f_{r})(s,a)]|-\left|\frac{1}{N}\sum_{(s,a,r,s^{\prime})}\hat{w}_{\pi,f}(s,a)(f_{r}(s,a)-r-\gamma f_{r}^{\prime}(s^{\prime},\pi))\right|\] \[= |\mathbb{E}_{\mu}[w_{\pi,f_{r}}^{*}(s,a)(f_{r}-\mathcal{T}_{r}^{ \pi}f_{r})(s,a)]|-|\mathbb{E}_{\mu}[\hat{w}_{\pi,f_{r}}(s,a)(f_{r}-\mathcal{T} _{r}^{\pi}f_{r})(s,a)]|\] \[+|\mathbb{E}_{\mu}[\hat{w}_{\pi,f_{r}}(s,a)(f_{r}-\mathcal{T}_{r}^ {\pi}f_{r})(s,a)]|-\left|\frac{1}{N}\sum_{(s,a,r,s^{\prime})}\hat{w}_{\pi,f}(s,a)(f_{r}(s,a)-r-\gamma f_{r}^{\prime}(s^{\prime},\pi))\right|\] \[\geq 0-\epsilon_{stat}=-\epsilon_{stat},\]

where the inequality is true by using the definition of \(w_{\pi,f_{r}}^{*}\) and Lemma 3. Thus

\[\mathcal{E}_{\mu}(\pi,f_{r})-\epsilon_{\mathcal{D}}(\pi,f_{r})\] \[= |\mathbb{E}_{\mu}[w_{\pi,f_{r}}^{*}(s,a)(f_{r}-\mathcal{T}_{r}^{ \pi}f_{r})(s,a)]|-\left|\frac{1}{N}\sum_{(s,a,r,s^{\prime})}w_{\pi,f_{r}}^{*}( s,a)(f_{r}(s,a)-r-\gamma f_{r}^{\prime}(s^{\prime},\pi))\right|\] \[+ \left|\frac{1}{N}\sum_{(s,a,r,s^{\prime})}w_{\pi,f_{r}}^{*}(s,a)( f_{r}(s,a)-r-\gamma f_{r}^{\prime}(s^{\prime},\pi))\right|\] \[-\left|\frac{1}{N}\sum_{(s,a,r,s^{\prime})}

**Lemma 5**.: _(Empirical weighted average Bellman Error) With probability at least \(1-2\delta\), for any \(\pi\in\Pi,\) we have_

\[\mathcal{E}_{\mathcal{D}}(\pi,f_{r}^{\pi})\leq C_{\ell_{2}}^{*}\sqrt{\epsilon_{1}}+\epsilon_{stat} \tag{20}\] \[\mathcal{E}_{\mathcal{D}}(\pi,f_{c}^{\pi})\leq C_{\ell_{2}}^{*}\sqrt{\epsilon_{1}}+\epsilon_{stat}, \tag{21}\]

_where_

\[f_{r}^{\pi}:=\operatorname*{arg\,min}_{f_{r}\in\mathcal{E}}\sup_ {\text{admissible}\,\nu}\|f_{r}-\mathcal{T}_{r}^{\pi}f_{r}\|_{2,\nu}^{2},\forall \pi\in\Pi\] \[f_{c}^{\pi}:=\operatorname*{arg\,min}_{f_{c}\in\mathcal{G}}\sup_ {\text{admissible}\,\nu}\|f_{c}-\mathcal{T}_{c}^{\pi}f_{c}\|_{2,\nu}^{2},\forall \pi\in\Pi.\]

Proof.: Condition on the high probability event in Lemma 4, we have

\[\mathcal{E}_{\mu}(\pi,f_{r}^{\pi})=\max_{w\in\mathcal{W}}|\mathbb{ E}_{\mu}[w(s,a)((f-T_{r}^{\pi}f_{r}^{\pi})(s,a))]|\] \[\leq \mathcal{E}_{\mu}(\pi,f_{r}^{\pi})=\max_{w\in\mathcal{W}}\|\|w\| _{2,\mu}\|f_{\pi}-T_{r}^{\pi}f)(s,a))\|_{2,\mu}\] \[\leq C_{\ell_{2}^{*}}\sqrt{\epsilon_{1}},\]

where the first inequality is true because of Cauchy-Schwarz inequality and the second inequality comes from the definition of \(f_{r}^{\pi}\) and Assumption 3.2, thus we can obtain

\[\mathcal{E}_{\mathcal{D}}(\pi,f_{r}^{\pi})\leq\mathcal{E}_{\mu}( \pi,f_{r}^{\pi})+\epsilon_{stat}\leq C_{\ell_{2}}^{*}\sqrt{\epsilon_{1}}+ \epsilon_{stat}. \tag{22}\]

Following a similar proof we can have

\[\hat{\mathcal{E}}_{\mathcal{D}}(\pi,f_{c}^{\pi})\leq\mathcal{E}_{\mu}(\pi,f_{c} ^{\pi})+\epsilon_{stat}\leq C_{\ell_{2}}^{*}\sqrt{\epsilon_{1}}+\epsilon_{stat}. \tag{23}\]

**Lemma 6**.: _(Performance difference decomposition, restate of Lemma \(12\) in Cheng et al. (2022)) For an arbitrary policy \(\pi,\hat{\pi}\in\Pi,\) and \(f\) be an arbitrary function over \(\mathcal{S}\times\mathcal{A}.\) Then we have,_

\[J_{\diamond}(\pi)-J_{\diamond}(\hat{\pi})\] \[= \mathbb{E}_{\mu}\big{[}\big{(}f-\mathcal{T}_{\diamond}^{\hat{\pi }}\big{)}(s,a)\big{]}+\mathbb{E}_{\pi}\big{[}\big{(}\mathcal{T}_{\diamond}^{ \hat{\pi}}f-f\big{)}(s,a)\big{]}+\mathbb{E}_{\pi}[f(s,\pi)-f(s,\hat{\pi})]+ \mathcal{L}_{\mu}(\hat{\pi},f)-\mathcal{L}_{\mu}(\hat{\pi},Q_{\diamond}^{\hat {\pi}}), \tag{24}\]

_where \(\diamond:=r\) or \(c\)._

Proof.: We prove the case when \(\diamond:=r,\) the other case is identical. Let \(R^{f,\hat{\pi}}(s,a):=f(s,a)-\gamma\mathbb{E}_{s^{\prime}|(s,a)}[f(s^{\prime}, \hat{\pi})]\) be a virtual reward function for given \(f\) and \(\hat{\pi}.\) According to performance difference lemma (Kakade and Langford, 2002), We first have that

\[(J_{r}(\hat{\pi})-J_{r}(\mu))= \mathcal{L}_{\mu}(\hat{\pi},Q_{r}^{\hat{\pi}})\] \[= \Delta(\hat{\pi})+\mathcal{L}_{\mu}(\hat{\pi},f)\qquad\qquad( \Delta(\hat{\pi}):=\mathcal{L}_{\mu}(\hat{\pi},Q_{r}^{\hat{\pi}})-\mathcal{L} _{\mu}(\hat{\pi},f))\] \[= \Delta(\hat{\pi})+\mathbb{E}_{\mu}[f(s,\hat{\pi})-f(s,a)]\] \[= \Delta(\hat{\pi})+(1-\gamma)(J_{R^{f,\hat{\pi}}}(\hat{\pi})-J_{R ^{f,\hat{\pi}}}(\mu))\] \[= \Delta(\hat{\pi})+(1-\gamma)Q_{R^{f,\hat{\pi}}}^{\hat{\pi}}(s_{0},\hat{\pi})-\mathbb{E}_{\mu}[R^{\hat{\pi},f}(s,a)]\] \[= \Delta(\hat{\pi})+(1-\gamma)f(s_{0},\hat{\pi})-\mathbb{E}_{\mu}[ R^{\hat{\pi},f}(s,a)],\]

where the last equality is true because that

\[Q_{R^{f,\hat{\pi}}}^{\pi}(s,a)=(\mathcal{T}_{R^{f,\hat{\pi}}}^{\pi}f)(s,a)=R^{f,\hat{\pi}}+\gamma \mathbb{E}_{s^{\prime}|(s,a)}[f(s^{\prime},\hat{\pi})]=f(s,a).\]

Thus we have

\[(J_{r}(\pi)-J_{r}(\hat{\pi}))= (J_{r}(\pi)-J_{r}(\mu)-(J_{r}(\hat{\pi})-J_{r}(\mu))\] \[= (J_{r}(\pi)-f(d_{0},\hat{\pi}))+\left(\mathbb{E}_{\mu}[R^{\hat{ \pi},f}(s,a)]-J_{r}(\mu)\right)-\Delta(\hat{\pi}). \tag{25}\]For the first term, we have

\[(J_{r}(\pi)-f(d_{0},\hat{\pi}))= (J_{r}(\pi)-f(s_{0},\hat{\pi}))\] (deterministic initial state) \[= J_{r}(\pi)-\mathbb{E}_{q^{*}}[R^{\hat{\pi},f}(s,a)]+\mathbb{E}_{d^{ *}}[R^{\hat{\pi},f}(s,a)]-f(s_{0},\hat{\pi})\] \[= \mathbb{E}_{d^{*}}[R(s,a)-R^{\hat{\pi},f}(s,a)]+\mathbb{E}_{d^{*} }[f(s,\pi)-f(s,\hat{\pi})]\] \[= \mathbb{E}_{d^{*}}[(\mathcal{T}_{r}^{\hat{\pi}}f-f)(s,a)]+\mathbb{ E}_{d^{*}}[f(s,\pi)-f(s,\hat{\pi})], \tag{26}\]

where the second equality is true because

\[\mathbb{E}_{d^{*}}[R^{\hat{\pi},f}(s,a)]-f(s_{0},\hat{\pi})\] \[= \mathbb{E}_{d^{*}}\big{[}f(s,a)-\gamma\mathbb{E}_{s^{\prime}|(s,a )}[f(s^{\prime},\hat{\pi})]\big{]}-f(s_{0},\hat{\pi})\] \[= \mathbb{E}_{d^{*}}\big{[}f(s,\pi)\big{]}-\sum_{s}\sum_{t=0}^{ \infty}\gamma^{t}\Pr(s_{t}=s|s_{0}\sim d_{0},\pi)f(s,\hat{\pi}(s))\] \[= \mathbb{E}_{d^{*}}\big{[}f(s,\pi)\big{]}-\sum_{s}\sum_{t=0}^{ \infty}\gamma^{t}\Pr(s_{t}=s|s_{0}\sim d_{0},\pi)f(s,\hat{\pi}(s))\] \[= \mathbb{E}_{d^{*}}\big{[}f(s,\pi)\big{]}-\sum_{s,a}\sum_{t=0}^{ \infty}\gamma^{t}\Pr(s_{t}=s,a_{t}=a|s_{0}\sim d_{0},\pi)f(s,\hat{\pi}(s))\] \[= \mathbb{E}_{d^{*}}[f(s,\pi)-f(s,\hat{\pi})].\]

For the second term we have

\[\mathbb{E}_{\mu}[R^{\hat{\pi},f}(s,a)]-J_{r}(\mu)\] \[= \mathbb{E}_{\mu}[R^{\hat{\pi},f}(s,a)-R(s,a)]\] \[= \mathbb{E}_{\mu}[(f-\mathcal{T}_{r}^{\hat{\pi}}f)(s,a)]. \tag{27}\]

Therefore plugging 26 and (27) into Eq. (25), we have

\[J_{r}(\pi)-J_{r}(\hat{\pi})\] \[= \mathbb{E}_{\mu}\big{[}(f-\mathcal{T}_{r}^{\hat{\pi}})(s,a)\big{]} +\mathbb{E}_{\pi}\big{[}(\mathcal{T}_{r}^{\hat{\pi}}f-f)(s,a)\big{]}+\mathbb{E }_{\pi}[f(s,\pi)-f(s,\hat{\pi})]+\mathcal{L}_{\mu}(\hat{\pi},f)-\mathcal{L}_{ \mu}(\hat{\pi},Q_{r}^{\hat{\pi}}).\]

The proof is completed. 

**Lemma 7**.: _With probability at least \(1-2\delta\), for any \(f_{r}\in\mathcal{F},f_{c}\in\mathcal{G},\) and \(\pi\in\Pi,\) we have:_

\[|\mathcal{L}_{\mu}(\pi,f_{r})-\mathcal{L}_{\mathcal{D}}(\pi,f_{ r})|\leq\epsilon_{stat} \tag{28}\] \[|\mathcal{L}_{\mu}(\pi,f_{c})-\mathcal{L}_{\mathcal{D}}(\pi,f_{c} )|\leq\epsilon_{stat} \tag{29}\]

_where \(\epsilon_{stat}:=V_{\max}C_{\ell_{2}}^{*}\sqrt{\frac{\log(|\mathcal{F}|| \mathcal{G}||\Pi||/\delta)}{N}+\frac{V_{\max}B_{w}\log(|\mathcal{F}||\mathcal{ G}||\Pi||W|/\delta)}{N}}\)._

Proof.: Recall that \(\mathbb{E}_{\mu}[\mathcal{L}_{\mathcal{D}}(\pi,f_{r})]=\mathcal{L}_{\mu}(\pi,f)\) and \(|f_{r}(s,\pi)-f_{r}(s,a)|\leq V_{\max}.\) For any \(f_{r}\in\mathcal{F},\) policy \(\pi\in\Pi,\) applying a Hoeffding's inequality and a union bound we can obtain with probability \(1-\delta,\)

\[|\mathcal{L}_{\mu}(\pi,f_{r})-\mathcal{L}_{\mathcal{D}}(\pi,f_{ r})|\leq\mathcal{O}\bigg{(}V_{\max}\sqrt{\frac{\log(|\mathcal{F}||\Pi|/ \delta)}{N}}\bigg{)}\leq\epsilon_{stat}. \tag{30}\]

The inequality for proving the \(f_{c},\pi\) is the same. 

## Appendix B Missing Proofs

### Proof of Theorem 4.1

Proof.: According to the performance difference lemma (Kakade and Langford, 2002), we have

\[(J_{r}(\pi)-J_{r}(\mu))-\lambda\{J_{c}(\pi)-J_{c}(\mu)\}_{+}\] \[= \mathcal{L}_{\mu}(\pi,Q_{r}^{\pi})-\lambda\{\mathcal{L}_{\mu}( \pi,Q_{c}^{\pi})\}_{+}\] \[= \mathcal{L}_{\mu}(\pi,Q_{r}^{\pi})+\beta\mathcal{E}_{\mu}(\pi,Q_ {r}^{\pi})-\lambda\{\mathcal{L}_{\mu}(\pi,Q_{c}^{\pi})\}_{+}+\beta\hat{ \mathcal{E}}_{\mu}(\pi,Q_{c}^{\pi})\] \[\geq \mathcal{L}_{\mu}(\pi,f_{r}^{\pi})+\beta\mathcal{E}_{\mu}(\pi,f _{r}^{\pi})-\lambda\{\mathcal{L}_{\mu}(\pi,f_{c}^{\pi})\}_{+}+\beta\hat{ \mathcal{E}}_{\mu}(\pi,f_{c}^{\pi})\] \[\geq \mathcal{L}_{\mu}(\pi,f_{r}^{\pi})-\lambda\{\mathcal{L}_{\mu}(\pi, f_{c}^{\pi})\}_{+}, \tag{31}\]where the second equality is true because \(\mathcal{E}_{\mu}(\pi,Q_{r}^{\pi})=\mathcal{E}_{\mu}(\pi,Q_{c}^{\pi})=0\) by Assumption 3.2, and the first inequality comes from the selection of \(f_{r}^{\pi}\) and \(f_{c}^{\pi}\) in optimization (6).

Therefore, we can obtain

\[J_{r}(\hat{\pi}^{*})-J_{r}(\mu)\geq \big{(}\mathcal{L}_{\mu}(\hat{\pi}^{*},f_{r}^{\pi^{*}})-\lambda \{\mathcal{L}_{\mu}(\hat{\pi}^{*},f_{c}^{\pi^{*}})\}_{+}\big{)}+\lambda\{J_{c} (\hat{\pi}^{*})-J_{c}(\mu)\}_{+}\] \[\geq \big{(}\mathcal{L}_{\mu}(\mu,f_{r}^{\mu})-\lambda\{\mathcal{L}_{ \mu}(\mu,f_{c}^{\mu})\}_{+}\big{)}+\lambda\{J_{c}(\hat{\pi}^{*})-J_{c}(\mu)\}_ {+}\] \[\geq \lambda\{J_{c}(\hat{\pi}^{*})-J_{c}(\mu)\}_{+}\geq 0 \tag{32}\]

and

\[\{J_{c}(\hat{\pi}^{*})\}_{+}-\{J_{c}(\mu)\}_{+}\leq\{J_{c}(\hat{ \pi}^{*})-J_{c}(\mu)\}_{+}\leq\frac{1}{\lambda}(J_{r}(\hat{\pi}^{*})-J_{r}(\mu ))\leq\frac{1}{\lambda}. \tag{33}\]

### Proof of Lemma 2

Proof.: Denote \(\pi_{\text{ref}}\) as \(\pi\). First according to the definition for the no-regret oracle 5.1, we have

\[\frac{1}{K}\sum_{k=1}^{K} \mathbb{E}_{\pi}[f_{r}^{k}(s,\pi)-f_{r}^{k}(s,\pi_{k})-\lambda\{f _{c}^{k}(s,\pi)-f_{c}^{k}(s,\pi)\}_{+}\] \[+\lambda\{f_{c}^{k}(s,\pi_{k})-f_{c}^{k}(s,\pi)\}_{+}]\leq\epsilon _{opt}^{\pi} \tag{34}\]

Therefore,

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi}[f_{r}^{k}(s,\pi)-f_{r}^ {k}(s,\pi_{k})]\] \[\leq \epsilon_{opt}^{\pi}+\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi}[ \lambda\{f_{c}^{k}(s,\pi)-f_{c}^{k}(s,\pi)\}_{+}-\lambda\{f_{c}^{k}(s,\pi_{k}) -f_{c}^{k}(s,\pi)\}_{+}]\leq\epsilon_{opt}^{\pi}, \tag{35}\]

and

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi}[\{f_{c}^{k}(s,\pi_{k})-f _{c}^{k}(s,\pi)\}_{+}]\leq\epsilon_{opt}^{\pi}-\frac{1}{\lambda K}\sum_{k=1}^{ K}\mathbb{E}_{\pi}[f_{r}^{k}(s,\pi)-f_{r}^{k}(s,\pi_{k})]\leq\epsilon_{opt}^{\pi}+ \frac{V_{\max}}{\lambda}. \tag{36}\]

We finish the proof. 

### Proof of Theorem 5.2

**Theorem** (Restate of Theorem 5.2).: _Under Assumptions 3.2 and 3.6, let the reference policy \(\pi_{\text{ref}}\in\Pi\) be any policy satisfying Assumption 3.7, then with probability at least \(1-\delta,\)_

\[J_{r}(\pi_{\text{ref}})-J_{r}(\bar{\pi}) \leq\mathcal{O}\bigg{(}\epsilon_{stat}+C_{\ell_{2}}^{*}\sqrt{ \epsilon_{1}}\bigg{)}+\epsilon_{opt}^{\pi} \tag{37}\] \[J_{c}(\bar{\pi})-J_{c}(\pi_{\text{ref}}) \leq\mathcal{O}\bigg{(}\epsilon_{stat}+C_{\ell_{2}}^{*}\sqrt{ \epsilon_{1}}\bigg{)}+\epsilon_{opt}^{\pi}+\frac{V_{\max}}{\lambda}, \tag{38}\]

_where \(\epsilon_{stat}:=V_{\max}C_{\ell_{2}}^{*}\sqrt{\frac{\log(|\mathcal{F}|| \mathcal{G}||\Pi||W|/\delta)}{N}}+\frac{V_{\max}B_{w}\log(|\mathcal{F}|| \mathcal{G}||\Pi||W|/\delta)}{N},\) and \(\bar{\pi}\) is the policy returned by Algorithm 1 with \(\beta>0\) and \(\pi_{\text{ref}}\) as input._

[MISSING_PAGE_FAIL:18]

_where \(\epsilon_{stat}:=V_{\max}C^{*}_{\ell_{2}}\sqrt{\frac{\log(|\mathcal{F}||\Pi||W/ \delta)}{N}}+\frac{V_{\max}B_{w}\log(|\mathcal{F}||\Pi||W/\delta)}{N},\) and \(\bar{\pi}\) is the policy returned by Algorithm 1 with \(\beta\geq 0\) and \(\mu\) as input._

Proof.: Following a similar proof in Theorem 5.2. But when the reference policy is the behavior policy, we have \((\mathbf{I})+(\Pi)=0.\) Therefore we have have

\[(\text{IV})=\mathcal{L}_{\mu}(\pi_{k},f^{k}_{r})-\mathcal{L}_{\mu} (\pi_{k},Q^{\pi_{k}})\] \[\leq \mathcal{L}_{\mu}(\pi_{k},f^{k}_{r})-\mathcal{L}_{\mu}(\pi_{k},Q^ {\pi_{k}})+\beta\mathcal{E}_{\mathcal{D}}(\pi_{k},f^{k}_{r})\] \[\leq \mathcal{L}_{\mu}(\pi_{k},f^{k}_{r})-\mathcal{L}_{\mu}(\pi_{k},Q^ {\pi_{k}})+\beta\mathcal{E}_{\mathcal{D}}(\pi_{k},f^{k}_{r})-\beta\mathcal{E}_ {\mathcal{D}}(\pi,f_{\pi_{k}})+\beta C^{*}_{\ell_{2}}\sqrt{\epsilon_{1}}+ \beta\epsilon_{stat}\] (Lemma 5) \[\leq \mathcal{L}_{\mathcal{D}}(\pi_{k},f^{k}_{r})+\beta\mathcal{E}_{ \mathcal{D}}(\pi_{k},f^{k}_{r})-\mathcal{L}_{\mathcal{D}}(\pi_{k},f^{\pi_{k}} _{r})-\beta\mathcal{E}_{\mathcal{D}}(\pi,f_{\pi_{k}})+(\beta+1)(\epsilon_{stat} +C^{*}_{\ell_{2}}\sqrt{\epsilon_{1}})\] \[\leq (\beta+1)(\epsilon_{stat}+C^{*}_{\ell_{2}}\sqrt{\epsilon_{1}}).\]

We finish the proof. 

## Appendix C Discussion on obtaining the behavior policy

To extract the behavior policy when it is not provided, we can simply run behavior cloning on the offline data. In particular, we can estimate the learned behavior policy \(\hat{\pi}_{\mu}\) as follows: \(\forall s\in\mathcal{D},\hat{\pi}_{\mu}(a|s)\leftarrow\frac{n(s,a)}{n(s)}\), and \(\forall s\notin\mathcal{D},\hat{\pi}_{\mu}(a|s)\leftarrow\frac{1}{|\mathcal{ A}|}\), where \(n(s,a)\) is the number of times \((s,a)\) appears in the offline dataset \(\mathcal{D}\). Essentially, the estimated BC policy matches the empirical behavior policy on states in the offline dataset and takes uniform random actions outside the support of the dataset. It is easy to show that the gap between the learned policy \(\hat{\pi}_{\mu}\) and the behavior policy \(\pi_{\mu}\) is upper bounded by \(\mathcal{O}(\min\{1,|\mathcal{S}|/N\})\)(Kumar et al., 2022; Rajaraman et al., 2020). We can have a very accurate estimate as long as the size of the dataset is large enough.

## Appendix D Experimental Supplement

### Practical Algorithm

The practical version of our algorithm WSAC is shown in Algorithm 2.

```
1:Input: Batch data \(\mathcal{D}\), policy network \(\pi\), network for the reward critic \(f_{r}\), network for the cost critic \(f_{c},\beta>0,\lambda>0\).
2:for\(k=1,2,\ldots,K\)do
3: Sample minibatch \(\mathcal{D}_{\text{mini}}\) from the dataset \(\mathcal{D}\).
4: Update Critic Networks: \[l_{\text{reward}}(f_{r}) :=\mathcal{L}_{\mathcal{D}_{\text{mini}}}(\pi,f_{r})+\beta \mathcal{E}_{\mathcal{D}_{\text{mini}}}(\pi,f_{r}),\] \[f_{r} \leftarrow\text{ADAM}(f_{r}-\eta_{\text{fast}}\nabla l_{\text{ reward}}(f_{r})),\] \[l_{\text{cost}}(f_{c}) :=-\lambda\mathcal{L}_{\mathcal{D}_{\text{mini}}}(\pi,f_{c})+\beta \mathcal{E}_{\mathcal{D}_{\text{mini}}}(\pi,f_{c}),\] \[f_{c} \leftarrow\text{ADAM}(f_{c}-\eta_{\text{fast}}\nabla l_{\text{ cost}}(f_{c})).\]
5: Update Policy Network: \[l_{\text{actor}}(\pi) :=-\mathcal{L}_{\text{mini}}(\pi,f_{r})+\lambda\{\mathcal{L}_{ \text{mini}}(\pi,f_{c})\}_{+},\] \[\pi \leftarrow\text{ADAM}(\pi-\eta_{\text{slow}}\nabla l_{\text{ actor}}(\pi)).\]
6:endfor
7:Output:\(\pi\)
```

**Algorithm 2** WSAC - Practical Version

### Environments Description

Besides the "BallCircle" environment, we also study several representative environments as follows. All of them are shown in Figure 2 and their offline dataset is from Liu et al. (2023).

* **CarCircle**: This environment requires the car to move on a circle in a clockwise direction within the safety zone defined by the boundaries. The car is a four-wheeled agent based on MIT's race car. The reward is dense and increases by the car's velocity and by the proximity towards the boundary of the circle and the cost is incurred if the agent leaves the safety zone defined by the two yellow boundaries, which are the same as "CarCircle".
* **PointPush**: This environment requires the point to navigate to the goal button location and touch the right goal button while avoiding more gemlins and hazards. The point has two actuators, one for rotation and the other for forward/backward movement. The reward consists of two parts, indicating the distance between the agent and the goal and if the agent reaches the goal button and touches it. The cost will be incurred if the agent enters the hazardous areas, contacts the gemlins, or presses the wrong button.
* **PointPush**: This environment requires the point to push a box to reach the goal while circumventing hazards and pillars. The objects are in 2D planes and the point is the same as "PointButton". It has a small square in front of it, which makes it easier to determine the orientation visually and also helps point push the box.

### Implementation Details and Experimental settings

We run all the experiments with NVIDIA GeForce RTX \(3080\) Ti \(8-\)Core Processor.

The normalized reward and cost are summarized as follows:

\[R_{normalized} =\frac{R_{\pi}-r_{min}(\mathcal{M})}{r_{max}(\mathcal{M})-r_{ min}(\mathcal{M})} \tag{51}\] \[C_{normalized} =\frac{C_{\pi}+\epsilon}{\kappa+\epsilon}, \tag{52}\]

where \(r(\mathcal{M})\) is the empirical reward for task \(\mathcal{M}\), \(\kappa\) is the cost threshold, \(\epsilon\) is a small number to ensure numerical stability. Thus any normalized cost below \(1\) is considered as safe. We use \(R_{\pi}\) and \(C_{\pi}\) to dentoe the cumulative rewards and cost for the evaluated policy, respectively. The parameters of \(r_{min}(\mathcal{M})\), \(r_{max}(\mathcal{M})\) and \(\kappa\) are environment-dependent constants and the specific values can be found in Appendix D. We remark that the normalized reward and cost only used for demonstrating the performance purpose and are not used in the training process. The detailed value of the reward and costs can be found in Table 3.

To mitigate the risk of unsafe scenarios, we introduce a hyperparameter \(UB_{Q_{C}}\) to the cost \(Q\)-function as an overestimation when calculating the actor loss. We use two separate \(\beta_{r}\), \(\beta_{c}\) for reward and cost \(Q\) functions to make the algorithm more flexible.

Figure 2: BallCircle and CarCircle (left), PointButton (medium), PointPush(right).

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Parameters & BallCircle & CarCircle & PointButton & PointPush \\ \hline \(\beta_{c}\) & 30.0 & 38.0 & 30.0 & 30.0 \\ \hline \(\beta_{r}\) & 10.0 & 12.0 & 10.0 & 10.0 \\ \hline \(UB_{Q_{C}}\) & 30.0 & 28.0 & 32.0 & 30.0 \\ \hline \(\lambda\) & \multicolumn{4}{|c|}{\([1.0,20.0]\)} \\ \hline Batch size & \multicolumn{4}{|c|}{\(512\)} \\ \hline Actor learning rate & \multicolumn{4}{|c|}{\(0.0001\)} \\ \hline Critic learning rate & \multicolumn{4}{|c|}{\(0.0003\)} \\ \hline \(\kappa\) & \multicolumn{4}{|c|}{\(40\)} \\ \hline \(r_{min}(\mathcal{M})\) & 0.3831 & 3.4844 & 0.0141 & 0.0012 \\ \hline \(r_{max}(\mathcal{M})\) & 881.4633 & 534.3061 & 42.8986 & 14.6910 \\ \hline \end{tabular}
\end{table}
Table 3: Hyperparameters of WSACFigure 3: The moving average of evaluation results is recorded every \(500\) training steps, with each result representing the average over \(20\) evaluation episodes and three random seeds. A cost threshold \(1\) is applied, with any normalized cost below 1 considered safe.

We use different \(\beta\) for the reward and cost critic networks and different \(UB_{Q_{C}}\) for the actor-network to make the adversarial training more stable. We also let the key parameter \(\lambda\) within a certain range balance reward and cost during the training process. Their values are shown in Table 3. In experiments, we take \(\mathcal{W}=\{0,C_{\infty}\}\) for computation effective. Then we can reduce \(\mathcal{E}_{\mathcal{D}}(\pi,f)\) to \(C_{\infty}\mathbb{E}_{\mathcal{D}}[(f(s,a)-r-\gamma f(s^{\prime},\pi))^{2}]\) and reduce \(\mathcal{\hat{E}}_{\mathcal{D}}(\pi,f)\) to \(C_{\infty}\mathbb{E}_{\mathcal{D}}[(f(s,a)-c-\gamma f(s^{\prime},\pi))^{2}]\). In this case, \(C_{\infty}\) can be considered as a part of the hyperparameter \(\beta_{r}(\beta_{c})\).

### Experimental results details and supplements

The evaluation performances of the agents in each environment after \(30000\) update steps of training are shown in Table 2, and the performance of average rewards and costs are shown in Figure 3. From the results, we observe that WSAC achieves a best reward performance with significantly lowest costs against all the baselines. It suggests WSAC can establish a safe and efficient policy and achieve a steady improvement by leveraging the offline dataset.

### Simulations under different cost limits

To further evaluate the performance of our algorithm under varying situations. We further compare our algorithm with baselines under varying cost limits, we report the average performance of our method and other baselines in Table 4. Specifically, cost limits of \([10,20,40]\) are used for the BallCircle and CarCircle environments, and \([20,40,80]\) for the PointButton and PointPush environments, following the standard setup outlined by Liu et al. (2023). Our results demonstrate that WSAC maintains safety across all environments, and its performance is either comparable to or superior to the best baseline in each case. These suggest that WSAC is well-suited for adapting to tasks of varying difficulty.

### Ablation studies

To investigate the contribution of each component of our algorithm, including the weighted Bellman regularizer, the aggression-limited objective, and the no-regret policy optimization (which together guarantee our theoretical results), we performed an ablation study in the tabular setting. The results, presented in Table 5, indicate that the weighted Bellman regularization ensures the safety of the algorithm, while the aggression-limited objective fine-tunes the algorithm to achieve higher rewards without compromising safety.

### Sensitivity Analysis of Hyper-Parameters

We provide the rewards and costs under different sets of \(\beta_{r}=\beta_{c}\in\{1,0.5,0.05\}\) and \(\lambda\in\{[0,1],[0,2],[1,2]\}\) (since our \(\lambda\) only increases, the closed interval here represents the initial value

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline  & \multicolumn{2}{c|}{BC} & \multicolumn{2}{c|}{Safe-Rice} & \multicolumn{2}{c|}{CIPT} & \multicolumn{2}{c|}{BCQL} & \multicolumn{2}{c|}{BEM-L} & \multicolumn{2}{c|}{CGP} & \multicolumn{2}{c|}{CGP-RICE} & \multicolumn{2}{c|}{WSA-C} \\ \cline{2-14}  & **Reward** \(\uparrow\) & Cut. \(\downarrow\) & **Reward** \(\uparrow\) & **Cut. \(\downarrow\)** & **Reward** \(\uparrow\) & **Cut. \(\downarrow\)** & **Reward** \(\uparrow\) & **Cut. \(\downarrow\)** & **Reward** \(\uparrow\) & **Cut. \(\downarrow\)** & **Reward** \(\uparrow\) & **Cut. \(\downarrow\)** & **Reward** \(\uparrow\) & **Cut. \(\downarrow\)** \\ \hline BallCircle & 0.74 & 4.71 & 0.52 & 0.65 & 0.77 & 1.05 & 0.69 & 2.36 & 0.86 & 3Figure 4: Sensitivity Analysis of Hyperparameters in the Tabular Case. The left figure illustrates tests conducted with various \(\beta\) values (For the sake of discussion, we denote \(\beta=\beta_{r}=\beta_{c}\)) with \(\lambda=[0,2]\), while the right figure presents tests across different ranges of \(\lambda\) with \(\beta_{r}=\beta_{c}=2.0\).

and the upper bound of \(\lambda\)) to demonstrate the robustness of our approach in the tabular setting in Figure 4. We can observe that the performance is almost the same under different sets of parameters and different qualities of behavior policies.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist",**
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in Abstract and Intriduction reflect the paper's contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations have been properly discussed in Conclusion and in Section 4.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All the assumptions have been properly stated. The complete proofs are in Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our experimental results are reproducible. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the data and code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The Experimental setting and details are in Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In the experiment, we used multiple random seeds to ensure the statistical significance of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We specified the computational resources we used in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ** The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: This paper is theoretical in nature, and it has been conducted with the NeurIPS code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This paper has tremendous positive societal impact as it develops RL algorithms with provable safety guarantee using offline data. Such a guarantee is essential for practical implementation of RL algorithms. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification:

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification:

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification:

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer:[NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer:[NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.